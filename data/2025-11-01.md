<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 49]
- [cs.CV](#cs.CV) [Total: 50]
- [cs.AI](#cs.AI) [Total: 45]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.IR](#cs.IR) [Total: 8]
- [cs.LG](#cs.LG) [Total: 48]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [StreetMath: Study of LLMs' Approximation Behaviors](https://arxiv.org/abs/2510.25776)
*Chiung-Yi Tseng,Somshubhra Roy,Maisha Thasin,Danyang Zhang,Blessing Effiong*

Main category: cs.CL

TL;DR: 本文介绍了一个名为StreetMath的基准测试，用于评估大型语言模型在真实场景下的近似计算能力，特别是针对非自回归解码器模型。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注大型语言模型在非正式、快节奏数学运算中的近似推理能力，尤其是在非自回归解码器模型中。

Method: 通过StreetMath基准测试，评估了不同LLM架构（Qwen3-4B-Instruct-2507, Qwen3-4B-Thinking-2507, Dream-v0-Instruct-7B, Falcon-Mamba-7B-Instruct, Mamba-GPT-3B）的近似计算能力，并应用可解释性技术来探究其内部计算状态。

Result: 实验结果表明，即使在需要近似计算的任务中，LLM通常尝试计算精确值或调用外部工具。模型在早期层或步骤中有时能得到正确答案，但解决近似任务时仍然消耗更多token。精确和近似算术运算依赖于很大程度上独立的神经组件。

Conclusion: LLM在街头数学环境中没有表现出像人类一样的认知吝啬。

Abstract: There is a substantial body of literature examining the mathematical
reasoning capabilities of large language models (LLMs), particularly their
performance on precise arithmetic operations in autoregressive architectures.
However, their ability to perform approximate reasoning in informal, fast-paced
mathematical operations has received far less attention, especially among
non-autoregressive decoder models. Our work addresses this gap by introducing
StreetMath, a benchmark designed to evaluate models' approximation abilities
under real-world approximation scenarios. We conduct extensive evaluations
across different LLM architectures: Qwen3-4B-Instruct-2507,
Qwen3-4B-Thinking-2507, Dream-v0-Instruct-7B, Falcon-Mamba-7B-Instruct, and
Mamba-GPT-3B. Furthermore, we apply mechanistic interpretability techniques to
probe their internal computational states. Our analysis reveals that LLMs
generally attempt to compute exact values or invoke external tools even in
tasks that call for approximation. Moreover, while models sometimes reach the
correct answer in early layers or steps, they still consume more tokens when
solving approximation tasks. Additional experiments indicate that exact and
approximate arithmetic operations rely on largely separate neural components.
Drawing upon research on cognitive psychology, we argue that LLMs do not
exhibit cognitive miserliness in the same way humans do in street math
settings. We open source our work https://github.com/ctseng777/StreetMath

</details>


### [2] [Review Based Entity Ranking using Fuzzy Logic Algorithmic Approach: Analysis](https://arxiv.org/abs/2510.25778)
*Pratik N. Kalamkar,Anupama G. Phakatkar*

Main category: cs.CL

TL;DR: 本文提出了一种基于词典的观点挖掘方法，该方法考虑了意见的强度，并通过模糊逻辑算法和句法依存关系分析对意见词进行分类，从而对实体进行排序。


<details>
  <summary>Details</summary>
Motivation: 现有的整体词典方法没有考虑到每个意见的强度。

Method: 结合意见词（即副词、形容词、名词和动词），将实体评论和用户查询按粒度级别（即非常弱、弱、中等、非常强和强）进行分类。使用模糊逻辑算法将意见词分类到不同的类别，并使用句法依存关系分析来查找所需方面词的关系。考虑与特定感兴趣的方面相关的意见词，以找到评论中该方面的实体得分。

Result: 根据摘要，该论文提出了一种新方法，但没有明确说明实验结果。

Conclusion: 根据摘要，结论部分未提及具体结论，只说明了方法。

Abstract: Opinion mining, also called sentiment analysis, is the field of study that
analyzes people opinions, sentiments, evaluations, appraisals, attitudes, and
emotions towards entities such as products, services, organizations,
individuals, issues, events, topics, and their attributes. Holistic
lexicon-based approach does not consider the strength of each opinion, i.e.,
whether the opinion is very strongly negative (or positive), strongly negative
(or positive), moderate negative (or positive), very weakly negative (or
positive) and weakly negative (or positive). In this paper, we propose approach
to rank entities based on orientation and strength of the entity reviews and
user's queries by classifying them in granularity levels (i.e. very weak, weak,
moderate, very strong and strong) by combining opinion words (i.e. adverb,
adjective, noun and verb) that are related to aspect of interest of certain
product. We shall use fuzzy logic algorithmic approach in order to classify
opinion words into different category and syntactic dependency resolution to
find relations for desired aspect words. Opinion words related to certain
aspects of interest are considered to find the entity score for that aspect in
the review.

</details>


### [3] [LASTIST: LArge-Scale Target-Independent STance dataset](https://arxiv.org/abs/2510.25783)
*DongJae Kim,Yaejin Lee,Minsu Park,Eunil Park*

Main category: cs.CL

TL;DR: 本研究提出了一个大规模的、与目标无关的韩语立场检测数据集（LASTIST），以填补该领域在低资源语言上的研究空白。


<details>
  <summary>Details</summary>
Motivation: 当前立场检测研究主要集中在基于特定目标的立场检测任务上，且缺乏韩语等低资源语言的数据集。

Method: 从韩国政党的新闻稿中收集了563,299个已标注的韩语句子，并训练了最先进的深度学习和立场检测模型。

Result: 构建了一个适用于各种立场检测任务（包括与目标无关的立场检测和历时演变立场检测）的LASTIST数据集。

Conclusion: 该数据集已发布在https://anonymous.4open.science/r/LASTIST-3721/，可用于促进立场检测领域的研究，特别是在低资源语言方面。

Abstract: Stance detection has emerged as an area of research in the field of
artificial intelligence. However, most research is currently centered on the
target-dependent stance detection task, which is based on a person's stance in
favor of or against a specific target. Furthermore, most benchmark datasets are
based on English, making it difficult to develop models in low-resource
languages such as Korean, especially for an emerging field such as stance
detection. This study proposes the LArge-Scale Target-Independent STance
(LASTIST) dataset to fill this research gap. Collected from the press releases
of both parties on Korean political parties, the LASTIST dataset uses 563,299
labeled Korean sentences. We provide a detailed description of how we collected
and constructed the dataset and trained state-of-the-art deep learning and
stance detection models. Our LASTIST dataset is designed for various tasks in
stance detection, including target-independent stance detection and diachronic
evolution stance detection. We deploy our dataset on
https://anonymous.4open.science/r/LASTIST-3721/.

</details>


### [4] [zFLoRA: Zero-Latency Fused Low-Rank Adapters](https://arxiv.org/abs/2510.25784)
*Dhananjaya Gowda,Seoha Song,Harshith Goka,Junhyun Lee*

Main category: cs.CL

TL;DR: 这篇论文提出了一种名为 zFLoRA 的新型低秩适配器，旨在解决大型语言模型在部署特定任务适配器时推理时间开销过大的问题。


<details>
  <summary>Details</summary>
Motivation: 在使用大型语言模型时，即使适配器参数量很小（通常小于基础模型的 1%），也会导致推理时间显著增加（高达基础模型的 2.5 倍）。

Method: 论文提出了一种零延迟融合低秩适配器 (zFLoRA)，该适配器在基础模型之上引入的延迟开销为零或可忽略不计。

Result: 在 1B、3B 和 7B 的大型语言模型上的实验结果表明，zFLoRA 与流行的监督微调基准（包括低秩适配器 (LoRA) 以及完全微调 (FFT)）相比，具有优势。在常识推理、数学推理和摘要对话三个不同类别中的 18 个不同任务上进行了实验。在 NPU (Samsung Galaxy S25+) 以及 GPU (NVIDIA H100) 平台上进行的延迟测量表明，所提出的 zFLoRA 适配器引入的延迟开销为零或可忽略不计。

Conclusion: 提出的 zFLoRA 适配器可以有效地减少大型语言模型部署时推理时间开销。

Abstract: Large language models (LLMs) are increasingly deployed with task-specific
adapters catering to multiple downstream applications. In such a scenario, the
additional compute associated with these apparently insignificant number of
adapter parameters (typically less than 1% of the base model) turns out to be
disproportionately significant during inference time (upto 2.5x times that of
the base model). In this paper, we propose a new zero-latency fused low-rank
adapter (zFLoRA) that introduces zero or negligible latency overhead on top of
the base model. Experimental results on LLMs of size 1B, 3B and 7B show that
zFLoRA compares favorably against the popular supervised fine-tuning benchmarks
including low-rank adapters (LoRA) as well as full fine-tuning (FFT).
Experiments are conducted on 18 different tasks across three different
categories namely commonsense reasoning, math reasoning and summary-dialogue.
Latency measurements made on NPU (Samsung Galaxy S25+) as well as GPU (NVIDIA
H100) platforms show that the proposed zFLoRA adapters introduce zero to
negligible latency overhead.

</details>


### [5] [BlackboxNLP-2025 MIB Shared Task: Improving Circuit Faithfulness via Better Edge Selection](https://arxiv.org/abs/2510.25786)
*Yaniv Nikankin,Dana Arad,Itay Itzhak,Anja Reusch,Adi Simhi,Gal Kesten-Pomeranz,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 本文针对机械可解释性中的电路发现问题，提出了三个关键改进方法，包括使用引导程序识别具有一致归因分数的边、引入基于比率的选择策略以优先考虑强正分数的边，以及用整数线性规划公式代替标准贪婪选择。实验结果表明，该方法能够产生更可靠的电路，并在多个 MIB 任务和模型上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 机械可解释性的主要挑战之一是电路发现，即确定模型的哪些部分执行给定的任务。

Method: 1. 使用引导程序识别具有一致归因分数的边。2. 引入简单的基于比率的选择策略，以优先考虑强正分数的边，从而平衡性能和可靠性。3. 用整数线性规划公式代替标准贪婪选择。

Result: 该方法能够产生更可靠的电路，并在多个 MIB 任务和模型上优于现有方法。

Conclusion: 本文提出了一种改进的电路发现方法，通过三个关键改进，在机械可解释性方面取得了更好的效果。

Abstract: One of the main challenges in mechanistic interpretability is circuit
discovery, determining which parts of a model perform a given task. We build on
the Mechanistic Interpretability Benchmark (MIB) and propose three key
improvements to circuit discovery. First, we use bootstrapping to identify
edges with consistent attribution scores. Second, we introduce a simple
ratio-based selection strategy to prioritize strong positive-scoring edges,
balancing performance and faithfulness. Third, we replace the standard greedy
selection with an integer linear programming formulation. Our methods yield
more faithful circuits and outperform prior approaches across multiple MIB
tasks and models. Our code is available at:
https://github.com/technion-cs-nlp/MIB-Shared-Task.

</details>


### [6] [LISTEN to Your Preferences: An LLM Framework for Multi-Objective Selection](https://arxiv.org/abs/2510.25799)
*Adam S. Jovine,Tinghan Ye,Francis Bahk,Jingjing Wang,David B. Shmoys,Peter I. Frazier*

Main category: cs.CL

TL;DR: 提出LISTEN框架，利用大型语言模型（LLM）作为零样本偏好预言机，仅由专家的自然语言高级优先级指导，以解决人类专家难以从具有多个竞争目标的大量项目中选择最佳选项的问题。


<details>
  <summary>Details</summary>
Motivation: 解决人类专家难以形式化复杂的、隐含的偏好，从而难以从具有多个竞争目标的大量项目中选择最佳选项的问题。

Method: 提出两种迭代算法：LISTEN-U，使用LLM来改进参数化效用函数；LISTEN-T，一种非参数方法，对小批量的解决方案执行锦标赛风格的选择。

Result: 在包括航班预订、购物和考试安排在内的各种任务上进行评估，结果表明，当偏好参数对齐时（我们用一种新的和谐度指标来衡量这一特性），LISTEN-U表现出色，而LISTEN-T提供了更强大的性能。

Conclusion: 这项工作探索了一个有希望的方向，即直接用自然语言引导复杂的多目标决策，减少传统偏好获取的认知负担。

Abstract: Human experts often struggle to select the best option from a large set of
items with multiple competing objectives, a process bottlenecked by the
difficulty of formalizing complex, implicit preferences. To address this, we
introduce LISTEN, a framework that leverages a Large Language Model (LLM) as a
zero-shot preference oracle, guided only by an expert's high-level priorities
in natural language. To operate within LLM constraints like context windows and
inference costs, we propose two iterative algorithms: LISTEN-U, which uses the
LLM to refine a parametric utility function, and LISTEN-T, a non-parametric
method that performs tournament-style selections over small batches of
solutions. Evaluated on diverse tasks including flight booking, shopping, and
exam scheduling, our results show LISTEN-U excels when preferences are
parametrically aligned (a property we measure with a novel concordance metric),
while LISTEN-T offers more robust performance. This work explores a promising
direction for steering complex multi-objective decisions directly with natural
language, reducing the cognitive burden of traditional preference elicitation.

</details>


### [7] [Beyond Length: Quantifying Long-Range Information for Long-Context LLM Pretraining Data](https://arxiv.org/abs/2510.25804)
*Haoran Deng,Yingyu Lin,Zhenghao Lin,Xiao Liu,Yizhou Sun,Yi-An Ma,Yeyun Gong*

Main category: cs.CL

TL;DR: 这篇论文介绍了一种名为LongFilter的框架，用于筛选长文本预训练数据，提高训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有的长文本数据中，许多文本缺乏有意义的长距离依赖，使用这些数据训练效率低下，因此需要选择更合适的数据。

Method: LongFilter通过对比长上下文和短上下文设置下的模型预测，测量扩展上下文提供的信息增益，从而识别长距离依赖必不可少的样本。

Result: 使用LLaMA-3-8B进行实验，将其上下文长度从8K扩展到64K，结果表明LongFilter能够高效地选择高质量数据，并在HELMET、LongBench和RULER等基准测试中产生显著改进。

Conclusion: LongFilter框架能够有效筛选长文本预训练数据，提高模型性能。

Abstract: Long-context language models unlock advanced capabilities in reasoning, code
generation, and document summarization by leveraging dependencies across
extended spans of text. However, a significant portion of readily available
long-text data lacks meaningful long-distance dependencies; most spans can be
predicted using only local context. Training on such data is inefficient,
making careful data selection crucial. Therefore, we introduce LongFilter, a
framework for curating training data tailored to long-context pretraining.
LongFilter measures the information gain provided by extended context by
contrasting model predictions under long-context versus short-context settings,
thereby identifying samples where long-range dependencies are essential.
Experiments with LLaMA-3-8B, extending its context length from 8K to 64K, show
that LongFilter efficiently selects high-quality data and yields substantial
improvements on benchmarks such as HELMET, LongBench, and RULER.

</details>


### [8] [Ideology-Based LLMs for Content Moderation](https://arxiv.org/abs/2510.25805)
*Stefano Civelli,Pietro Bernardelle,Nardiena A. Pratama,Gianluca Demartini*

Main category: cs.CL

TL;DR: 该研究调查了角色扮演如何影响大型语言模型 (LLM) 在有害内容分类中的一致性和公平性。


<details>
  <summary>Details</summary>
Motivation: 确保内容审核系统中LLM的公平性和中立性至关重要。

Method: 该研究跨不同的 LLM 架构、模型大小和内容模式（语言与视觉）检查了角色扮演的影响。

Result: 角色扮演对整体分类准确率影响不大，但会造成重要的行为转变。具有不同意识形态倾向的角色在将内容标记为有害内容时表现出不同的倾向。模型，尤其是较大的模型，往往更密切地与来自同一政治意识形态的角色保持一致，从而加强意识形态内部的一致性，同时扩大意识形态群体的差异。

Conclusion: 角色扮演会给 LLM 输出带来微妙的意识形态偏差，从而引发了人们对 AI 系统可能在表面中立的情况下强化党派观点的担忧。

Abstract: Large language models (LLMs) are increasingly used in content moderation
systems, where ensuring fairness and neutrality is essential. In this study, we
examine how persona adoption influences the consistency and fairness of harmful
content classification across different LLM architectures, model sizes, and
content modalities (language vs. vision). At first glance, headline performance
metrics suggest that personas have little impact on overall classification
accuracy. However, a closer analysis reveals important behavioral shifts.
Personas with different ideological leanings display distinct propensities to
label content as harmful, showing that the lens through which a model "views"
input can subtly shape its judgments. Further agreement analyses highlight that
models, particularly larger ones, tend to align more closely with personas from
the same political ideology, strengthening within-ideology consistency while
widening divergence across ideological groups. To show this effect more
directly, we conducted an additional study on a politically targeted task,
which confirmed that personas not only behave more coherently within their own
ideology but also exhibit a tendency to defend their perspective while
downplaying harmfulness in opposing views. Together, these findings highlight
how persona conditioning can introduce subtle ideological biases into LLM
outputs, raising concerns about the use of AI systems that may reinforce
partisan perspectives under the guise of neutrality.

</details>


### [9] [Beyond Long Context: When Semantics Matter More than Tokens](https://arxiv.org/abs/2510.25816)
*Tarun Kumar Chawdhury,Jon D. Duke*

Main category: cs.CL

TL;DR: 本文介绍了一种名为临床实体增强检索(CLEAR)的方法，用于提高电子健康记录中临床文档的语义问答的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的向量数据库方法通常会忽略细微的临床关系，并且直接处理长文档效率低下。

Method: 该方法使用实体感知检索，并与零样本大语境推理和传统的基于块的检索增强生成方法进行了比较。

Result: CLEAR 方法在准确性和效率方面均优于其他方法，尤其是在处理长文档时，并且使用的 tokens 更少。

Conclusion: 实体感知检索提高了临床自然语言处理的效率和准确性。该评估框架为评估临床问答系统提供了一个可重用且透明的基准。

Abstract: Electronic Health Records (EHR) store clinical documentation as base64
encoded attachments in FHIR DocumentReference resources, which makes semantic
question answering difficult. Traditional vector database methods often miss
nuanced clinical relationships. The Clinical Entity Augmented Retrieval (CLEAR)
method, introduced by Lopez et al. 2025, uses entity aware retrieval and
achieved improved performance with an F1 score of 0.90 versus 0.86 for
embedding based retrieval, while using over 70 percent fewer tokens. We
developed a Clinical Notes QA Evaluation Platform to validate CLEAR against
zero shot large context inference and traditional chunk based retrieval
augmented generation. The platform was tested on 12 clinical notes ranging from
10,000 to 65,000 tokens representing realistic EHR content. CLEAR achieved a
58.3 percent win rate, an average semantic similarity of 0.878, and used 78
percent fewer tokens than wide context processing. The largest performance
gains occurred on long notes, with a 75 percent win rate for documents
exceeding 65,000 tokens. These findings confirm that entity aware retrieval
improves both efficiency and accuracy in clinical natural language processing.
The evaluation framework provides a reusable and transparent benchmark for
assessing clinical question answering systems where semantic precision and
computational efficiency are critical.

</details>


### [10] [A Survey on Efficient Large Language Model Training: From Data-centric Perspectives](https://arxiv.org/abs/2510.25817)
*Junyu Luo,Bohan Wu,Xiao Luo,Zhiping Xiao,Yiqiao Jin,Rong-Cheng Tu,Nan Yin,Yifan Wang,Jingyang Yuan,Wei Ju,Ming Zhang*

Main category: cs.CL

TL;DR: 本论文综述了数据高效的大型语言模型 (LLM) 后训练方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM后训练范式面临数据挑战，包括高昂的人工标注成本和数据规模的边际收益递减。因此，实现数据高效的后训练已成为关键的研究问题。

Method: 本文从以数据为中心的角度，对数据高效的LLM后训练进行了首次系统调查。我们提出了一种数据高效的LLM后训练方法分类，涵盖数据选择、数据质量增强、合成数据生成、数据蒸馏和压缩以及自我进化的数据生态系统。

Result: 我们总结了每个类别的代表性方法，并概述了未来的研究方向。

Conclusion: 通过检查数据高效的LLM后训练中的挑战，我们强调了开放性问题，并提出了潜在的研究途径。我们希望我们的工作能够激发进一步探索，从而最大限度地发挥大规模模型训练中数据利用的潜力。

Abstract: Post-training of Large Language Models (LLMs) is crucial for unlocking their
task generalization potential and domain-specific capabilities. However, the
current LLM post-training paradigm faces significant data challenges, including
the high costs of manual annotation and diminishing marginal returns on data
scales. Therefore, achieving data-efficient post-training has become a key
research question. In this paper, we present the first systematic survey of
data-efficient LLM post-training from a data-centric perspective. We propose a
taxonomy of data-efficient LLM post-training methods, covering data selection,
data quality enhancement, synthetic data generation, data distillation and
compression, and self-evolving data ecosystems. We summarize representative
approaches in each category and outline future research directions. By
examining the challenges in data-efficient LLM post-training, we highlight open
problems and propose potential research avenues. We hope our work inspires
further exploration into maximizing the potential of data utilization in
large-scale model training. Paper List:
https://github.com/luo-junyu/Awesome-Data-Efficient-LLM

</details>


### [11] [Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized Setting: the Case of FrameNet Annotation](https://arxiv.org/abs/2510.25904)
*Frederico Belcavello,Ely Matos,Arthur Lorenzi,Lisandra Bonoto,Lívia Ruiz,Luiz Fernando Pereira,Victor Herbst,Yulla Navarro,Helen de Andrade Abreu,Lívia Dutra,Tiago Timponi Torrent*

Main category: cs.CL

TL;DR: 本研究探讨了使用基于LLM的应用程序来加速或替代人工创建语言资源和数据集。通过对基于LLM的语义角色标注器进行广泛评估，报告了对FrameNet-like语义注释的（半）自动化。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM工具在语言研究中具有潜力，但对其性能和对带注释数据集创建的影响的全面评估仍然缺失，尤其是在NLP的视角化方法下。本文旨在弥补这一差距。

Method: 比较了三种实验设置中的注释时间、覆盖率和多样性：手动、自动和半自动注释。

Result: 混合半自动注释设置与纯人工设置相比，提高了框架多样性，并具有相似的注释覆盖率，而自动设置在所有指标上表现都明显更差，除了注释时间。

Conclusion: 混合半自动注释设置提高了框架多样性，并具有与纯人工设置相似的注释覆盖率。

Abstract: The use of LLM-based applications as a means to accelerate and/or substitute
human labor in the creation of language resources and dataset is a reality.
Nonetheless, despite the potential of such tools for linguistic research,
comprehensive evaluation of their performance and impact on the creation of
annotated datasets, especially under a perspectivized approach to NLP, is still
missing. This paper contributes to reduction of this gap by reporting on an
extensive evaluation of the (semi-)automatization of FrameNet-like semantic
annotation by the use of an LLM-based semantic role labeler. The methodology
employed compares annotation time, coverage and diversity in three experimental
settings: manual, automatic and semi-automatic annotation. Results show that
the hybrid, semi-automatic annotation setting leads to increased frame
diversity and similar annotation coverage, when compared to the human-only
setting, while the automatic setting performs considerably worse in all
metrics, except for annotation time.

</details>


### [12] [Inside CORE-KG: Evaluating Structured Prompting and Coreference Resolution for Knowledge Graphs](https://arxiv.org/abs/2510.26512)
*Dipak Meher,Carlotta Domeniconi*

Main category: cs.CL

TL;DR: 本文提出了一种名为 CORE-KG 的框架，用于从复杂的法律文本中提取结构化表示，该框架集成了类型感知共指模块和领域引导的结构化提示。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 方法在构建知识图谱时会产生噪声和重复节点，因为缺乏引导式抽取和共指消解。

Method: 本文对 CORE-KG 框架进行了系统性的消融研究，以量化其两个关键组件的贡献。

Result: 实验结果表明，移除共指消解会导致节点重复增加 28.32%，噪声节点增加 4.32%；移除结构化提示会导致节点重复增加 4.34%，噪声节点增加 73.33%。

Conclusion: 这些发现为设计基于 LLM 的鲁棒管道提供了经验性见解，以从复杂的法律文本中提取结构化表示。

Abstract: Human smuggling networks are increasingly adaptive and difficult to analyze.
Legal case documents offer critical insights but are often unstructured,
lexically dense, and filled with ambiguous or shifting references, which pose
significant challenges for automated knowledge graph (KG) construction. While
recent LLM-based approaches improve over static templates, they still generate
noisy, fragmented graphs with duplicate nodes due to the absence of guided
extraction and coreference resolution. The recently proposed CORE-KG framework
addresses these limitations by integrating a type-aware coreference module and
domain-guided structured prompts, significantly reducing node duplication and
legal noise. In this work, we present a systematic ablation study of CORE-KG to
quantify the individual contributions of its two key components. Our results
show that removing coreference resolution results in a 28.32% increase in node
duplication and a 4.32% increase in noisy nodes, while removing structured
prompts leads to a 4.34% increase in node duplication and a 73.33% increase in
noisy nodes. These findings offer empirical insights for designing robust
LLM-based pipelines for extracting structured representations from complex
legal texts.

</details>


### [13] [RECAP: Reproducing Copyrighted Data from LLMs Training with an Agentic Pipeline](https://arxiv.org/abs/2510.25941)
*André V. Duarte,Xuying li,Bin Zeng,Arlindo L. Oliveira,Lei Li,Zhuo Li*

Main category: cs.CL

TL;DR: RECAP是一个agentic pipeline，旨在引出和验证LLM输出中记忆的训练数据。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的训练数据不可检查，我们无法知道它已经看到的内容。当模型本身自由地再现目标内容时，会出现最有力的证据。

Method: RECAP的核心是一个反馈驱动的循环，其中初始提取尝试由二级语言模型评估，该模型将输出与参考段落进行比较并识别差异。然后将这些差异转化为最小的校正提示，将其反馈到目标模型中以指导后续生成。此外，为了解决对齐引起的拒绝，RECAP包括一个越狱模块，该模块可以检测并克服此类障碍。

Result: 在EchoTrace（一个包含30多本完整书籍的新基准）上评估了RECAP，结果表明，RECAP比单次迭代方法带来了实质性的收益。例如，对于GPT-4.1，受版权保护的文本提取的平均ROUGE-L得分从0.38提高到0.47，提高了近24%。

Conclusion: RECAP可以有效地从LLM中提取和验证记忆的训练数据。

Abstract: If we cannot inspect the training data of a large language model (LLM), how
can we ever know what it has seen? We believe the most compelling evidence
arises when the model itself freely reproduces the target content. As such, we
propose RECAP, an agentic pipeline designed to elicit and verify memorized
training data from LLM outputs. At the heart of RECAP is a feedback-driven
loop, where an initial extraction attempt is evaluated by a secondary language
model, which compares the output against a reference passage and identifies
discrepancies. These are then translated into minimal correction hints, which
are fed back into the target model to guide subsequent generations. In
addition, to address alignment-induced refusals, RECAP includes a jailbreaking
module that detects and overcomes such barriers. We evaluate RECAP on
EchoTrace, a new benchmark spanning over 30 full books, and the results show
that RECAP leads to substantial gains over single-iteration approaches. For
instance, with GPT-4.1, the average ROUGE-L score for the copyrighted text
extraction improved from 0.38 to 0.47 - a nearly 24% increase.

</details>


### [14] [Revisiting Multilingual Data Mixtures in Language Model Pretraining](https://arxiv.org/abs/2510.25947)
*Negar Foroutan,Paul Teiletche,Ayush Kumar Tarun,Antoine Bosselut*

Main category: cs.CL

TL;DR: 研究了大型语言模型（LLM）预训练中不同多语言数据混合的影响，挑战了关于语言覆盖范围和模型性能之间权衡的常见观点。


<details>
  <summary>Details</summary>
Motivation: 探讨多语言预训练中语言覆盖范围和模型性能之间的潜在权衡。

Method: 在包含25到400种语言的不同多语言语料库上训练了11亿和30亿参数的LLM。

Result: 1. 结合英语和多语言数据不一定会降低两者的语言性能。2. 使用英语作为枢轴语言可以跨语系产生益处。3. 在这种规模的模型中，随着训练语言数量的增加，没有观察到明显的“多语言诅咒”。

Conclusion: 适当平衡的多语言数据可以增强语言模型的能力，而不会影响性能，即使在低资源环境中也是如此。

Abstract: The impact of different multilingual data mixtures in pretraining large
language models (LLMs) has been a topic of ongoing debate, often raising
concerns about potential trade-offs between language coverage and model
performance (i.e., the curse of multilinguality). In this work, we investigate
these assumptions by training 1.1B and 3B parameter LLMs on diverse
multilingual corpora, varying the number of languages from 25 to 400. Our study
challenges common beliefs surrounding multilingual training. First, we find
that combining English and multilingual data does not necessarily degrade the
in-language performance of either group, provided that languages have a
sufficient number of tokens included in the pretraining corpus. Second, we
observe that using English as a pivot language (i.e., a high-resource language
that serves as a catalyst for multilingual generalization) yields benefits
across language families, and contrary to expectations, selecting a pivot
language from within a specific family does not consistently improve
performance for languages within that family. Lastly, we do not observe a
significant "curse of multilinguality" as the number of training languages
increases in models at this scale. Our findings suggest that multilingual data,
when balanced appropriately, can enhance language model capabilities without
compromising performance, even in low-resource settings

</details>


### [15] [Semantic Label Drift in Cross-Cultural Translation](https://arxiv.org/abs/2510.25967)
*Mohsinul Kabir,Tasnim Ahmed,Md Mezbaur Rahman,Polydoros Giannouris,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 机器翻译在低资源语言中被广泛使用，但由于文化差异，翻译过程中语义标签会发生漂移。大型语言模型（LLM）虽然具备文化知识，但反而会加剧这种漂移。源语言和目标语言之间的文化相似性是决定标签保留的关键因素。


<details>
  <summary>Details</summary>
Motivation: 探讨机器翻译中文化对齐的重要性，以及源语言和目标语言之间的文化差异对翻译的影响。

Method: 通过一系列跨文化敏感和中性领域的实验。

Result: 1. 机器翻译系统（包括LLM）在翻译过程中会引起标签漂移，尤其是在文化敏感领域；2. LLM具备文化知识，但利用这些知识会加剧标签漂移；3. 源语言和目标语言之间的文化相似性是决定标签保留的关键因素。

Conclusion: 在机器翻译中忽略文化因素不仅会损害标签的准确性，还会导致下游应用中的误解和文化冲突。

Abstract: Machine Translation (MT) is widely employed to address resource scarcity in
low-resource languages by generating synthetic data from high-resource
counterparts. While sentiment preservation in translation has long been
studied, a critical but underexplored factor is the role of cultural alignment
between source and target languages. In this paper, we hypothesize that
semantic labels are drifted or altered during MT due to cultural divergence.
Through a series of experiments across culturally sensitive and neutral
domains, we establish three key findings: (1) MT systems, including modern
Large Language Models (LLMs), induce label drift during translation,
particularly in culturally sensitive domains; (2) unlike earlier statistical MT
tools, LLMs encode cultural knowledge, and leveraging this knowledge can
amplify label drift; and (3) cultural similarity or dissimilarity between
source and target languages is a crucial determinant of label preservation. Our
findings highlight that neglecting cultural factors in MT not only undermines
label fidelity but also risks misinterpretation and cultural conflict in
downstream applications.

</details>


### [16] [SymCode: A Neurosymbolic Approach to Mathematical Reasoning via Verifiable Code Generation](https://arxiv.org/abs/2510.25975)
*Sina Bagheri Nezhad,Yao Li,Ameeta Agrawal*

Main category: cs.CL

TL;DR: 这篇论文介绍了一种名为 SymCode 的神经符号框架，旨在提高大型语言模型在复杂数学推理方面的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂的数学推理中表现不佳，因为它们基于文本的生成方式导致解决方案未经证实且算术上不合理。

Method: 该方法将数学问题求解重新定义为使用 SymPy 库进行可验证代码生成的任务。

Result: 在具有挑战性的基准测试中，SymCode 的准确性比基线提高了 13.6 个百分点。

Conclusion: SymCode 通过将 LLM 推理置于确定性符号引擎中，代表了在形式领域中实现更准确和可信的 AI 的关键一步。

Abstract: Large Language Models (LLMs) often struggle with complex mathematical
reasoning, where prose-based generation leads to unverified and arithmetically
unsound solutions. Current prompting strategies like Chain of Thought still
operate within this unreliable medium, lacking a mechanism for deterministic
verification. To address these limitations, we introduce SymCode, a
neurosymbolic framework that reframes mathematical problem-solving as a task of
verifiable code generation using the SymPy library. We evaluate SymCode on
challenging benchmarks, including MATH-500 and OlympiadBench, demonstrating
significant accuracy improvements of up to 13.6 percentage points over
baselines. Our analysis shows that SymCode is not only more token-efficient but
also fundamentally shifts model failures from opaque logical fallacies towards
transparent, programmatic errors. By grounding LLM reasoning in a deterministic
symbolic engine, SymCode represents a key step towards more accurate and
trustworthy AI in formal domains.

</details>


### [17] [NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium](https://arxiv.org/abs/2510.25977)
*Dinghong Song,Jierui Xu,Weichu Yang,Pengfei Su,Dong Li*

Main category: cs.CL

TL;DR: 本文介绍了一种在 AWS Trainium 上进行高性能矩阵乘法 (matmul) 的设计，该设计针对 LLM 推理进行了优化。


<details>
  <summary>Details</summary>
Motivation: 在 AWS Trainium 上利用其异构架构进行 LLM 训练和推理具有挑战性，因为它的 systolic 阵列架构对数据布局有特殊要求。

Method: 本文提出了一系列针对 Trainium 定制的 kernel 融合和新型缓存策略，以减少软件管理内存层次结构中的数据移动，最大化 SRAM 带宽，并避免昂贵的矩阵转置。

Result: 在九个数据集和四个最新的 LLM 上进行评估，结果表明本文的系统在很大程度上优于 AWS 在 Trainium 上实现的 state-of-the-art matmul：在 matmul kernel 层面，平均加速 1.35 倍（最高 2.22 倍），转化为端到端 LLM 推理的平均加速 1.66 倍（最高 2.49 倍）。

Conclusion: 本文成功地在 AWS Trainium 上实现了高性能的矩阵乘法，并显著提高了 LLM 推理的速度。

Abstract: AI accelerators, customized to AI workloads, provide cost-effective and
high-performance solutions for training and inference. Trainium, an AI
accelerator recently developed by Amazon Web Services (AWS), provides an
attractive option for LLM training and inference through its heterogeneous
architecture. However, leveraging Trainium architecture for high performance
can be challenging because of its systolic array architecture and special
requirement on data layout. In this paper, we design high-performance matrix
multiplication (matmul), a critical compute kernel, for LLM inference on
Trainium. We introduce a series of techniques customized to Trainium based on
kernel fusion and novel caching strategies to reduce data movement across the
software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive
matrix transpose. Evaluating with nine datasets and four recent LLMs, we show
that our system largely outperforms the state-of-the-art matmul implemented by
AWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x
speedup (up to 2.22x), which translates to an average 1.66x speedup (up to
2.49x) for end-to-end LLM inference.

</details>


### [18] [AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache](https://arxiv.org/abs/2510.25979)
*Dinghong Song,Yuan Feng,Yiwei Wang,Shangye Chen,Cyril Guyot,Filip Blagojevic,Hyeran Jeon,Pengfei Su,Dong Li*

Main category: cs.CL

TL;DR: AttnCache通过重用相似的注意力映射来加速LLM推理的预填充阶段，减少自注意力的计算开销。


<details>
  <summary>Details</summary>
Motivation: 在仅预填充的场景中，自注意力计算是主要性能瓶颈，因为它与序列长度呈二次方复杂性。

Method: AttnCache通过检索和重用相似的注意力映射来加速LLM推理的预填充阶段。它基于注意力映射记忆数据库，采用高效的缓存和相似性搜索技术。

Result: AttnCache在CPU上实现了平均1.2倍的端到端加速和2倍的注意力加速，在GPU上实现了1.6倍的端到端加速和3倍的注意力加速，且精度下降可忽略不计。

Conclusion: AttnCache通过重用相似的注意力映射，有效地加速了LLM推理的预填充阶段，且对精度影响很小。

Abstract: Large Language Models (LLMs) are widely used in generative applications such
as chatting, code generation, and reasoning. However, many realworld workloads
such as classification, question answering, recommendation, and text embedding
rely solely on the prefill stage of inference, where the model encodes input
sequences without performing autoregressive decoding. In these prefill only
scenarios, the self-attention computation becomes the primary performance
bottleneck due to its quadratic complexity with respect to sequence length. In
this paper, we observe that semantically different sentences often produce
similar attention maps across layers and heads. Building on this insight, we
propose AttnCache, a framework that accelerates the prefill stage of LLM
inference by retrieving and reusing similar attention maps. Based on an
attention map memorization database, AttnCache employs efficient caching and
similarity search techniques to identify and reuse pre-cached attention maps
during inference, thereby reducing the computational overhead of
self-attention. Experimental results show that AttnCache achieves an average of
1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x
attention speedup on GPU, with negligible accuracy degradation.

</details>


### [19] [Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning](https://arxiv.org/abs/2510.25992)
*Yihe Deng,I-Hung Hsu,Jun Yan,Zifeng Wang,Rujun Han,Gufeng Zhang,Yanfei Chen,Wei Wang,Tomas Pfister,Chen-Yu Lee*

Main category: cs.CL

TL;DR: 提出了监督强化学习（SRL）框架，以解决大型语言模型（LLM）在多步骤推理问题上的困难，尤其是在小规模开源模型中，强化学习和监督微调的不足。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在需要多步骤推理的问题上表现不佳。对于小规模开源模型，基于可验证奖励的强化学习（RLVR）在多次尝试后也很难获得正确的解决方案，而监督微调（SFT）则容易通过僵化的token-by-token模仿来过度拟合长篇演示。

Method: 提出了监督强化学习（SRL）框架，该框架将问题解决重新定义为生成一系列逻辑“动作”，并训练模型在执行每个动作之前生成内部推理独白。它基于模型动作与从SFT数据集中提取的专家动作之间的相似性，以逐步方式提供更平滑的奖励。

Result: SRL使小型模型能够学习以前SFT或RLVR无法学习的具有挑战性的问题。在通过RLVR进行细化之前，使用SRL初始化训练可以产生最强的整体性能。SRL有效地推广到代理软件工程任务。

Conclusion: SRL是一种强大而通用的面向推理的LLM训练框架。

Abstract: Large Language Models (LLMs) often struggle with problems that require
multi-step reasoning. For small-scale open-source models, Reinforcement
Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely
sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to
overfit long demonstrations through rigid token-by-token imitation. To address
this gap, we propose Supervised Reinforcement Learning (SRL), a framework that
reformulates problem solving as generating a sequence of logical "actions". SRL
trains the model to generate an internal reasoning monologue before committing
to each action. It provides smoother rewards based on the similarity between
the model's actions and expert actions extracted from the SFT dataset in a
step-wise manner. This supervision offers richer learning signals even when all
rollouts are incorrect, while encouraging flexible reasoning guided by expert
demonstrations. As a result, SRL enables small models to learn challenging
problems previously unlearnable by SFT or RLVR. Moreover, initializing training
with SRL before refining with RLVR yields the strongest overall performance.
Beyond reasoning benchmarks, SRL generalizes effectively to agentic software
engineering tasks, establishing it as a robust and versatile training framework
for reasoning-oriented LLMs.

</details>


### [20] [PORTool: Tool-Use LLM Training with Rewarded Tree](https://arxiv.org/abs/2510.26020)
*Feijie Wu,Weiwu Zhu,Yuxiang Zhang,Soumya Chatterjee,Jiarong Zhu,Fan Mo,Rodin Luo,Jing Gao*

Main category: cs.CL

TL;DR: PORTool是一种强化学习方法，旨在提升大型语言模型（LLM）在动态工具调用环境中的性能。通过鼓励模型探索不同的工具调用轨迹，PORTool 显著提高了最终准确性和工具调用步骤数。


<details>
  <summary>Details</summary>
Motivation: 现有的工具使用大型语言模型在静态数据集上训练，无法探索可能的解决方案，并且在动态工具调用环境中表现出有限的性能。

Method: PORTool方法首先为给定的查询生成多个rollout，形成树状结构。然后，基于每个步骤产生正确答案和成功工具调用的能力，为每个步骤分配奖励。最后，使用这些逐步奖励来计算fork相对优势，并与轨迹相对优势混合，以训练LLM以供工具使用。

Result: 实验使用了17个工具来解决用户查询，涵盖了时间敏感和时间不变的主题。消融研究系统地证明了逐步奖励的必要性和设计稳健性。与其他训练方法相比，PORTool在最终准确性和工具调用步骤数方面表现出显著改进。

Conclusion: PORTool通过强化学习鼓励工具使用LLM探索各种轨迹，从而在动态工具调用环境中显著提高了性能。

Abstract: Current tool-use large language models (LLMs) are trained on static datasets,
enabling them to interact with external tools and perform multi-step,
tool-integrated reasoning, which produces tool-call trajectories. However,
these models imitate how a query is resolved in a generic tool-call routine,
thereby failing to explore possible solutions and demonstrating limited
performance in an evolved, dynamic tool-call environment. In this work, we
propose PORTool, a reinforcement learning (RL) method that encourages a
tool-use LLM to explore various trajectories yielding the correct answer.
Specifically, this method starts with generating multiple rollouts for a given
query, and some of them share the first few tool-call steps, thereby forming a
tree-like structure. Next, we assign rewards to each step, based on its ability
to produce a correct answer and make successful tool calls. A shared step
across different trajectories receives the same reward, while different steps
under the same fork receive different rewards. Finally, these step-wise rewards
are used to calculate fork-relative advantages, blended with
trajectory-relative advantages, to train the LLM for tool use. The experiments
utilize 17 tools to address user queries, covering both time-sensitive and
time-invariant topics. We conduct ablation studies to systematically justify
the necessity and the design robustness of step-wise rewards. Furthermore, we
compare the proposed PORTool with other training approaches and demonstrate
significant improvements in final accuracy and the number of tool-call steps.

</details>


### [21] [Rethinking Cross-lingual Alignment: Balancing Transfer and Cultural Erasure in Multilingual LLMs](https://arxiv.org/abs/2510.26024)
*HyoJung Han,Sweta Agrawal,Eleftheria Briakou*

Main category: cs.CL

TL;DR: 跨语言对齐旨在对齐多语言表示，使大型语言模型能够在语言之间无缝转移知识。然而，这种对表征融合的追求可能会导致“文化抹除”，即丧失提供基于查询语言而应有所不同的文化情境化回应的能力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是现有的跨语言对齐方法可能导致“文化抹除”，即模型无法根据不同的语言文化背景给出合适的回答。

Method: 提出了一个整体评估框架，即迁移-本地化平面，以量化理想的知识迁移和不良的文化抹除。基于此框架，重新评估了最近的CLA方法。通过研究模型的内部表征，提出了Surgical Steering，一种 novel 的推理时方法，能够解耦这两个目标。

Result: 研究发现，现有的跨语言对齐方法在提高事实迁移能力的同时，会牺牲文化本地化能力。Surgical Steering方法能够在两个相互竞争的维度之间取得更好的平衡。

Conclusion: Surgical Steering方法能够有效克服当前对齐技术的局限性，在知识迁移和文化本地化之间取得平衡。

Abstract: Cross-lingual alignment (CLA) aims to align multilingual representations,
enabling Large Language Models (LLMs) to seamlessly transfer knowledge across
languages. While intuitive, we hypothesize, this pursuit of representational
convergence can inadvertently cause "cultural erasure", the functional loss of
providing culturally-situated responses that should diverge based on the query
language. In this work, we systematically analyze this trade-off by introducing
a holistic evaluation framework, the transfer-localization plane, which
quantifies both desirable knowledge transfer and undesirable cultural erasure.
Using this framework, we re-evaluate recent CLA approaches and find that they
consistently improve factual transfer at the direct cost of cultural
localization across all six languages studied. Our investigation into the
internal representations of these models reveals a key insight: universal
factual transfer and culturally-specific knowledge are optimally steerable at
different model layers. Based on this finding, we propose Surgical Steering, a
novel inference-time method that disentangles these two objectives. By applying
targeted activation steering to distinct layers, our approach achieves a better
balance between the two competing dimensions, effectively overcoming the
limitations of current alignment techniques.

</details>


### [22] [Artificial Intelligence-Enabled Analysis of Radiology Reports: Epidemiology and Consequences of Incidental Thyroid Findings](https://arxiv.org/abs/2510.26032)
*Felipe Larios,Mariana Borras-Osorio,Yuqi Wu,Ana Gabriela Claros,David Toro-Tobon,Esteban Cabezas,Ricardo Loor-Torres,Maria Mateo Chavez,Kerly Guevara Maldonado,Luis Vilatuna Andrango,Maria Lizarazo Jimenez,Ivan Mateo Alzamora,Misk Al Zahidy,Marcelo Montero,Ana Cristina Proano,Cristian Soto Jacome,Jungwei W. Fan,Oscar J. Ponce-Ponte,Megan E. Branda,Naykky Singh Ospina,Juan P. Brito*

Main category: cs.CL

TL;DR: 在非甲状腺成像中偶然发现的甲状腺结节（ITF）很常见，并且与后续的甲状腺超声、活检、甲状腺切除术和甲状腺癌诊断相关。ITF的发现可能导致甲状腺癌的过度诊断，需要标准化报告和更具选择性的随访。


<details>
  <summary>Details</summary>
Motivation: 确定ITF的患病率、特征和临床后果。

Method: 开发、验证和部署自然语言处理（NLP）流程，以识别放射学报告中的ITF，并评估其患病率、特征和临床结果。

Result: 在115,683名患者中，9,077名（7.8%）患有ITF，其中92.9%为结节。与没有ITF的患者相比，患有ITF的患者发生甲状腺结节诊断、活检、甲状腺切除术和甲状腺癌诊断的几率更高。

Conclusion: ITF很常见，并且与导致检测小型、低风险癌症的一系列操作密切相关。这些发现强调了ITF在甲状腺癌过度诊断中的作用，并且需要标准化报告和更具选择性的随访。

Abstract: Importance Incidental thyroid findings (ITFs) are increasingly detected on
imaging performed for non-thyroid indications. Their prevalence, features, and
clinical consequences remain undefined. Objective To develop, validate, and
deploy a natural language processing (NLP) pipeline to identify ITFs in
radiology reports and assess their prevalence, features, and clinical outcomes.
Design, Setting, and Participants Retrospective cohort of adults without prior
thyroid disease undergoing thyroid-capturing imaging at Mayo Clinic sites from
July 1, 2017, to September 30, 2023. A transformer-based NLP pipeline
identified ITFs and extracted nodule characteristics from image reports from
multiple modalities and body regions. Main Outcomes and Measures Prevalence of
ITFs, downstream thyroid ultrasound, biopsy, thyroidectomy, and thyroid cancer
diagnosis. Logistic regression identified demographic and imaging-related
factors. Results Among 115,683 patients (mean age, 56.8 [SD 17.2] years; 52.9%
women), 9,077 (7.8%) had an ITF, of which 92.9% were nodules. ITFs were more
likely in women, older adults, those with higher BMI, and when imaging was
ordered by oncology or internal medicine. Compared with chest CT, ITFs were
more likely via neck CT, PET, and nuclear medicine scans. Nodule
characteristics were poorly documented, with size reported in 44% and other
features in fewer than 15% (e.g. calcifications). Compared with patients
without ITFs, those with ITFs had higher odds of thyroid nodule diagnosis,
biopsy, thyroidectomy and thyroid cancer diagnosis. Most cancers were
papillary, and larger when detected after ITFs vs no ITF. Conclusions ITFs were
common and strongly associated with cascades leading to the detection of small,
low-risk cancers. These findings underscore the role of ITFs in thyroid cancer
overdiagnosis and the need for standardized reporting and more selective
follow-up.

</details>


### [23] [QCoder Benchmark: Bridging Language Generation and Quantum Hardware through Simulator-Based Feedback](https://arxiv.org/abs/2510.26101)
*Taku Mikuriya,Tatsuya Ishigaki,Masayuki Kawarada,Shunya Minami,Tadashi Kadowaki,Yohichi Suzuki,Soshun Naito,Shunya Takata,Takumi Kato,Tamotsu Basseda,Reo Yamada,Hiroya Takamura*

Main category: cs.CL

TL;DR: 提出了一个用于评估大型语言模型在量子编程方面表现的基准测试框架QCoder Benchmark。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在需要与硬件设备交互的领域，如量子编程，应用不足。

Method: 构建了一个评估框架，该框架使用量子模拟器环境，并结合了来自真实编程竞赛的人工编写代码。

Result: GPT-4o的准确率仅为18.97%，而基于推理的模型o3的准确率高达78%，超过了人类编写代码的平均成功率(39.98%)。

Conclusion: 发布了QCoder Benchmark数据集和公共评估API，以支持进一步的研究。

Abstract: Large language models (LLMs) have increasingly been applied to automatic
programming code generation. This task can be viewed as a language generation
task that bridges natural language, human knowledge, and programming logic.
However, it remains underexplored in domains that require interaction with
hardware devices, such as quantum programming, where human coders write Python
code that is executed on a quantum computer. To address this gap, we introduce
QCoder Benchmark, an evaluation framework that assesses LLMs on quantum
programming with feedback from simulated hardware devices. Our benchmark offers
two key features. First, it supports evaluation using a quantum simulator
environment beyond conventional Python execution, allowing feedback of
domain-specific metrics such as circuit depth, execution time, and error
classification, which can be used to guide better generation. Second, it
incorporates human-written code submissions collected from real programming
contests, enabling both quantitative comparisons and qualitative analyses of
LLM outputs against human-written codes. Our experiments reveal that even
advanced models like GPT-4o achieve only around 18.97% accuracy, highlighting
the difficulty of the benchmark. In contrast, reasoning-based models such as o3
reach up to 78% accuracy, outperforming averaged success rates of human-written
codes (39.98%). We release the QCoder Benchmark dataset and public evaluation
API to support further research.

</details>


### [24] [Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock LLM Diverse Thinking](https://arxiv.org/abs/2510.26122)
*Feng Ju,Zeyu Qin,Rui Min,Zhitao He,Lingpeng Kong,Yi R. Fung*

Main category: cs.CL

TL;DR: 提出了“一个问题，多个解决方案”(1PNS)的训练范式，以提高大型语言模型(llm)的推理能力和输出多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的“一个问题，一个解决方案”(1P1S)训练方式限制了模型输出的多样性，导致模型倾向于狭窄的推理路径。

Method: 引入了推理路径差异(RPD)指标，用于衡量多步骤思维链之间的语义差异，并使用RPD来生成多样化的解决方案集合，然后对Qwen3-4B-Base进行微调。

Result: 实验表明，使用RPD选择的训练数据可以产生更多样化的输出和更高的pass@k，pass@16平均提高了2.80%，在AIME24上提高了4.99%。

Conclusion: 1PNS训练范式可以进一步提高TTS的有效性。

Abstract: While Test-Time Scaling (TTS) has proven effective in improving the reasoning
ability of large language models (LLMs), low diversity in model outputs often
becomes a bottleneck; this is partly caused by the common "one problem, one
solution" (1P1S) training practice, which provides a single canonical answer
and can push models toward a narrow set of reasoning paths. To address this, we
propose a "one problem, multiple solutions" (1PNS) training paradigm that
exposes the model to a variety of valid reasoning trajectories and thus
increases inference diversity. A core challenge for 1PNS is reliably measuring
semantic differences between multi-step chains of thought, so we introduce
Reasoning Path Divergence (RPD), a step-level metric that aligns and scores
Long Chain-of-Thought solutions to capture differences in intermediate
reasoning. Using RPD, we curate maximally diverse solution sets per problem and
fine-tune Qwen3-4B-Base. Experiments show that RPD-selected training yields
more varied outputs and higher pass@k, with an average +2.80% gain in pass@16
over a strong 1P1S baseline and a +4.99% gain on AIME24, demonstrating that
1PNS further amplifies the effectiveness of TTS. Our code is available at
https://github.com/fengjujf/Reasoning-Path-Divergence .

</details>


### [25] [On the Influence of Discourse Relations in Persuasive Texts](https://arxiv.org/abs/2510.26124)
*Nawar Turk,Sevag Kaspar,Leila Kosseim*

Main category: cs.CL

TL;DR: 研究说服技巧 (PTs) 和语篇关系 (DRs) 之间的关系，利用大型语言模型 (LLMs) 和提示工程。


<details>
  <summary>Details</summary>
Motivation: 目前没有同时标注 PTs 和 DRs 的数据集，所以研究人员利用 SemEval 2023 Task 3 数据集，该数据集标注了 19 种 PTs，并开发了基于 LLM 的分类器，用 22 种 PDTB 3.0 level-2 DRs 标注数据集中的每个实例。

Method: 评估了四个 LLM，使用了 10 种不同的提示，产生了 40 个独特的 DR 分类器。使用不同的多数池化策略的集成模型被用于创建 5 个银数据集，这些数据集的实例同时标注了说服技巧和 level-2 PDTB 语义。

Result: 银数据集的大小从 1,281 个实例到 204 个实例不等，这取决于所使用的多数池化技术。对这些银数据集的统计分析表明，六种语篇关系（即原因、目的、对比、原因+信念、让步和条件）在说服性文本中起着关键作用，尤其是在使用加载语言、夸张/最小化、重复和投下怀疑时。

Conclusion: 该研究可以帮助检测在线宣传和错误信息，并增进我们对有效沟通的总体理解。

Abstract: This paper investigates the relationship between Persuasion Techniques (PTs)
and Discourse Relations (DRs) by leveraging Large Language Models (LLMs) and
prompt engineering. Since no dataset annotated with both PTs and DRs exists, we
took the SemEval 2023 Task 3 dataset labelled with 19 PTs as a starting point
and developed LLM-based classifiers to label each instance of the dataset with
one of the 22 PDTB 3.0 level-2 DRs. In total, four LLMs were evaluated using 10
different prompts, resulting in 40 unique DR classifiers. Ensemble models using
different majority-pooling strategies were used to create 5 silver datasets of
instances labelled with both persuasion techniques and level-2 PDTB senses. The
silver dataset sizes vary from 1,281 instances to 204 instances, depending on
the majority pooling technique used. Statistical analysis of these silver
datasets shows that six discourse relations (namely Cause, Purpose, Contrast,
Cause+Belief, Concession, and Condition) play a crucial role in persuasive
texts, especially in the use of Loaded Language, Exaggeration/Minimisation,
Repetition and to cast Doubt. This insight can contribute to detecting online
propaganda and misinformation, as well as to our general understanding of
effective communication.

</details>


### [26] [MossNet: Mixture of State-Space Experts is a Multi-Head Attention](https://arxiv.org/abs/2510.26182)
*Shikhar Tuli,James Seale Smith,Haris Jeelani,Chi-Heng Lin,Abhishek Patel,Vasili Ramanishka,Yen-Chang Hsu,Hongxia Jin*

Main category: cs.CL

TL;DR: MossNet是一种新型的混合状态空间专家架构，它模拟了线性多头注意力（MHA）。


<details>
  <summary>Details</summary>
Motivation: 当前基于SSM/GRM的方法通常只模拟单个注意力头，可能限制了它们的表达能力。

Method: MossNet在通道混合多层感知器（MLP）块和时间混合SSM内核中利用混合专家（MoE）实现，以实现多个“注意力头”。

Result: 在语言建模和下游评估的大量实验表明，MossNet优于类似模型大小和数据预算的基于transformer和SSM的架构。

Conclusion: MossNet是高效、高性能循环LLM架构的一个引人注目的新方向。

Abstract: Large language models (LLMs) have significantly advanced generative
applications in natural language processing (NLP). Recent trends in model
architectures revolve around efficient variants of transformers or
state-space/gated-recurrent models (SSMs, GRMs). However, prevailing
SSM/GRM-based methods often emulate only a single attention head, potentially
limiting their expressiveness. In this work, we propose MossNet, a novel
mixture-of-state-space-experts architecture that emulates a linear multi-head
attention (MHA). MossNet leverages a mixture-of-experts (MoE) implementation
not only in channel-mixing multi-layered perceptron (MLP) blocks but also in
the time-mixing SSM kernels to realize multiple "attention heads." Extensive
experiments on language modeling and downstream evaluations show that MossNet
outperforms both transformer- and SSM-based architectures of similar model size
and data budgets. Larger variants of MossNet, trained on trillions of tokens,
further confirm its scalability and superior performance. In addition,
real-device profiling on a Samsung Galaxy S24 Ultra and an Nvidia A100 GPU
demonstrate favorable runtime speed and resource usage compared to similarly
sized baselines. Our results suggest that MossNet is a compelling new direction
for efficient, high-performing recurrent LLM architectures.

</details>


### [27] [Similarity-Distance-Magnitude Language Models](https://arxiv.org/abs/2510.26183)
*Allen Schmaltz*

Main category: cs.CL

TL;DR: 提出了相似度-距离-幅度 (SDM) 语言模型，通过微调来最大化良好校准、高概率区域中的生成比例。


<details>
  <summary>Details</summary>
Motivation: 探索如何提高序列预测模型的性能和统计效率。

Method: 通过监督微调将预训练的 Transformer LM 转换为 SDM LM，使用 final-layer SDM 激活层来估计对比输入编码方案的基变换，并在训练期间生成额外的难负例。

Result: 与强大的监督基线相比，减少了拒绝（即提高了统计效率）。

Conclusion: SDM 语言模型能够有效提高序列预测模型的性能。

Abstract: We introduce Similarity-Distance-Magnitude (SDM) language models (LMs), which
are sequence prediction models fine-tuned to maximize the proportion of
generations in the well-calibrated, high-probability region partitioned by a
final-layer SDM activation layer used for binary classification of
instruction-following. We demonstrate that existing pre-trained decoder-only
Transformer LMs can be readily converted into SDM LMs via supervised
fine-tuning, using the final-layer SDM activation layer during training to
estimate a change-of-base for a supervised next-token loss over a contrastive
input encoding scheme, with additional hard negative examples generated online
during training. This results in reduced abstentions (i.e., improved
statistical efficiency) compared to strong supervised baselines.

</details>


### [28] [RCScore: Quantifying Response Consistency in Large Language Models](https://arxiv.org/abs/2510.26193)
*Dongjun Jang,Youngchae Ahn,Hyopil Shin*

Main category: cs.CL

TL;DR: RCScore是一个多维度框架，用于量化指令形式对模型响应的影响，揭示了传统指标未检测到的性能变化。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型评估通常依赖于单一的指令模板，忽略了模型对指令风格的敏感性，而这对于实际部署至关重要。

Method: 通过系统地将基准问题转换为多种指令风格，使用 Cross-Response Similarity (CRS) 来衡量风格自我一致性。

Result: 指令风格可以使准确率变化高达16.7%。确定性解码产生更风格稳定的输出，并且模型规模与跨风格一致性正相关。CRS与任务准确性高度相关，表明一致性是模型可靠性的一个有价值的指标。

Conclusion: RCScore 提供了一种评估指令鲁棒性的原则性方法。

Abstract: Current LLM evaluations often rely on a single instruction template,
overlooking models' sensitivity to instruction style-a critical aspect for
real-world deployments. We present RCScore, a multi-dimensional framework
quantifying how instruction formulation affects model responses. By
systematically transforming benchmark problems into multiple instruction
styles, RCScore reveals performance variations undetected by conventional
metrics. Our experiments across ten LLMs on four reasoning benchmarks
demonstrate that instruction style can shift accuracy by up to 16.7% points. We
introduce Cross-Response Similarity (CRS), a method applying RCScore metrics to
measure stylistic self-consistency, and establish its strong correlation with
task accuracy, suggesting consistency as a valuable proxy for model
reliability. Additional findings show that deterministic decoding produces more
stylistically stable outputs, and model scale correlates positively with
cross-style consistency. RCScore offers a principled approach to assess
instruction robustness.

</details>


### [29] [Don't Let It Fade: Preserving Edits in Diffusion Language Models via Token Timestep Allocation](https://arxiv.org/abs/2510.26200)
*Woojin Kim,Jaeyoung Do*

Main category: cs.CL

TL;DR: 扩散语言模型(DLM)虽然可以进行细粒度的改进，但其可控性仍然较弱。本文发现了一种称为更新遗忘的失效模式，并对其进行了正式描述。更新遗忘会导致token级别在时间步长上出现波动，从而抹去早期的语义编辑，破坏累积改进过程，从而降低流畅性和连贯性。本文提出了一种token时间步长分配(TTA)方法，该方法通过每个token的时间步长表来实现软语义token排序：关键token提前冻结，而不确定的token则继续改进。实验结果表明，TTA提高了可控性和流畅性：在情感控制方面，它以不到五分之一的步骤实现了超过20%的准确率提升和近一半的困惑度降低；在解毒方面，它降低了最大毒性(12.2 vs 14.5)和困惑度(26.0 vs 32.0)。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型的可控性较弱，并且存在更新遗忘问题，这会降低流畅性和连贯性。

Method: 提出了一种token时间步长分配(TTA)方法，通过每个token的时间步长表来实现软语义token排序，从而缓解更新遗忘问题。

Result: 在情感控制方面，TTA实现了超过20%的准确率提升和近一半的困惑度降低；在解毒方面，TTA降低了最大毒性和困惑度。

Conclusion: 通过时间步长分配进行软排序是减轻更新遗忘和实现稳定可控的扩散文本生成的关键。

Abstract: While diffusion language models (DLMs) enable fine-grained refinement, their
practical controllability remains fragile. We identify and formally
characterize a central failure mode called update forgetting, in which uniform
and context agnostic updates induce token level fluctuations across timesteps,
erasing earlier semantic edits and disrupting the cumulative refinement
process, thereby degrading fluency and coherence. As this failure originates in
uniform and context agnostic updates, effective control demands explicit token
ordering. We propose Token Timestep Allocation (TTA), which realizes soft and
semantic token ordering via per token timestep schedules: critical tokens are
frozen early, while uncertain tokens receive continued refinement. This
timestep based ordering can be instantiated as either a fixed policy or an
adaptive policy driven by task signals, thereby supporting a broad spectrum of
refinement strategies. Because it operates purely at inference time, it applies
uniformly across various DLMs and naturally extends to diverse supervision
sources. Empirically, TTA improves controllability and fluency: on sentiment
control, it yields more than 20 percent higher accuracy and nearly halves
perplexity using less than one fifth the steps; in detoxification, it lowers
maximum toxicity (12.2 versus 14.5) and perplexity (26.0 versus 32.0).
Together, these results demonstrate that softened ordering via timestep
allocation is the critical lever for mitigating update forgetting and achieving
stable and controllable diffusion text generation.

</details>


### [30] [What's In My Human Feedback? Learning Interpretable Descriptions of Preference Data](https://arxiv.org/abs/2510.26202)
*Rajiv Movva,Smitha Milli,Sewon Min,Emma Pierson*

Main category: cs.CL

TL;DR: 提出了一种名为WIMHF的方法，通过稀疏自编码器解释人类反馈数据，提取人类可解释的特征。


<details>
  <summary>Details</summary>
Motivation: 现有的研究缺乏对人类反馈数据编码的清晰理解，以及自动提取相关特征的有效方法。

Method: 使用稀疏自编码器来表征数据集能够衡量的偏好和注释者实际表达的偏好。

Result: 在7个数据集上，WIMHF识别出少量可解释的特征，揭示了人类偏好的多样性以及数据集上下文的作用。例如，Reddit用户喜欢非正式和笑话，而HH-RLHF和PRISM的注释者则不喜欢它们。LMArena用户倾向于反对拒绝，通常支持有害内容。通过重新标记Arena中的有害示例，可以获得显着的安全性提升。

Conclusion: WIMHF为从业者提供了一种以人为本的分析方法，以更好地理解和使用偏好数据。

Abstract: Human feedback can alter language models in unpredictable and undesirable
ways, as practitioners lack a clear understanding of what feedback data
encodes. While prior work studies preferences over certain attributes (e.g.,
length or sycophancy), automatically extracting relevant features without
pre-specifying hypotheses remains challenging. We introduce What's In My Human
Feedback? (WIMHF), a method to explain feedback data using sparse autoencoders.
WIMHF characterizes both (1) the preferences a dataset is capable of measuring
and (2) the preferences that the annotators actually express. Across 7
datasets, WIMHF identifies a small number of human-interpretable features that
account for the majority of the preference prediction signal achieved by
black-box models. These features reveal a wide diversity in what humans prefer,
and the role of dataset-level context: for example, users on Reddit prefer
informality and jokes, while annotators in HH-RLHF and PRISM disprefer them.
WIMHF also surfaces potentially unsafe preferences, such as that LMArena users
tend to vote against refusals, often in favor of toxic content. The learned
features enable effective data curation: re-labeling the harmful examples in
Arena yields large safety gains (+37%) with no cost to general performance.
They also allow fine-grained personalization: on the Community Alignment
dataset, we learn annotator-specific weights over subjective features that
improve preference prediction. WIMHF provides a human-centered analysis method
for practitioners to better understand and use preference data.

</details>


### [31] [Towards Global Retrieval Augmented Generation: A Benchmark for Corpus-Level Reasoning](https://arxiv.org/abs/2510.26205)
*Qi Luo,Xiaonan Li,Tingshuo Fan,Xinchi Chen,Xipeng Qiu*

Main category: cs.CL

TL;DR: 这篇论文介绍了GlobalQA，一个新的用于评估全局RAG能力的基准，并提出了GlobalRAG，一个多工具协同框架，以解决现有RAG方法在全局任务中表现不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG评估基准主要关注局部RAG，而现实世界的应用需要全局RAG能力，即跨文档集合聚合和分析信息以获得语料库级别的洞察力。

Method: 论文提出了GlobalRAG，一个多工具协同框架，它通过块级检索保持结构连贯性，结合LLM驱动的智能过滤器消除噪声文档，并集成聚合模块进行精确的符号计算。

Result: 实验结果表明，现有的RAG方法在全局任务中表现不佳，而GlobalRAG在Qwen2.5-14B模型上实现了6.63的F1分数，相比之下，最强的基线只有1.51。

Conclusion: GlobalRAG 验证了其有效性，解决了现有 RAG 方法在全局任务中表现不佳的问题，为全局 RAG 任务提供了一个新的解决方案。

Abstract: Retrieval-augmented generation (RAG) has emerged as a leading approach to
reducing hallucinations in large language models (LLMs). Current RAG evaluation
benchmarks primarily focus on what we call local RAG: retrieving relevant
chunks from a small subset of documents to answer queries that require only
localized understanding within specific text chunks. However, many real-world
applications require a fundamentally different capability -- global RAG --
which involves aggregating and analyzing information across entire document
collections to derive corpus-level insights (for example, "What are the top 10
most cited papers in 2023?"). In this paper, we introduce GlobalQA -- the first
benchmark specifically designed to evaluate global RAG capabilities, covering
four core task types: counting, extremum queries, sorting, and top-k
extraction. Through systematic evaluation across different models and
baselines, we find that existing RAG methods perform poorly on global tasks,
with the strongest baseline achieving only 1.51 F1 score. To address these
challenges, we propose GlobalRAG, a multi-tool collaborative framework that
preserves structural coherence through chunk-level retrieval, incorporates
LLM-driven intelligent filters to eliminate noisy documents, and integrates
aggregation modules for precise symbolic computation. On the Qwen2.5-14B model,
GlobalRAG achieves 6.63 F1 compared to the strongest baseline's 1.51 F1,
validating the effectiveness of our method.

</details>


### [32] [Pragmatic Theories Enhance Understanding of Implied Meanings in LLMs](https://arxiv.org/abs/2510.26253)
*Takuma Sato,Seiya Kawano,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 本文提出了一种利用语用理论提示语言模型来理解隐含意义的方法，并通过实验验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 语言模型应具备准确理解隐含意义的能力，这在人际交流和语言使用中至关重要。

Method: 本文提出了一种方法，将Gricean语用学和Relevance Theory等语用理论概述作为提示呈现给语言模型，引导其逐步推理得出最终解释。

Result: 实验结果表明，与没有语用理论的baseline相比，该方法在语用推理任务上使语言模型的分数提高了9.6%。即使不解释语用理论的细节，仅仅在提示中提及它们的名称，也能使较大的模型获得一定的性能提升。

Conclusion: 为语言模型提供语用理论作为提示，是理解隐含意义任务中一种有效的上下文学习方法。

Abstract: The ability to accurately interpret implied meanings plays a crucial role in
human communication and language use, and language models are also expected to
possess this capability. This study demonstrates that providing language models
with pragmatic theories as prompts is an effective in-context learning approach
for tasks to understand implied meanings. Specifically, we propose an approach
in which an overview of pragmatic theories, such as Gricean pragmatics and
Relevance Theory, is presented as a prompt to the language model, guiding it
through a step-by-step reasoning process to derive a final interpretation.
Experimental results showed that, compared to the baseline, which prompts
intermediate reasoning without presenting pragmatic theories (0-shot
Chain-of-Thought), our methods enabled language models to achieve up to 9.6\%
higher scores on pragmatic reasoning tasks. Furthermore, we show that even
without explaining the details of pragmatic theories, merely mentioning their
names in the prompt leads to a certain performance improvement (around 1-3%) in
larger models compared to the baseline.

</details>


### [33] [Language Models Are Borrowing-Blind: A Multilingual Evaluation of Loanword Identification across 10 Languages](https://arxiv.org/abs/2510.26254)
*Mérilin Sousa Silva,Sina Ahmadi*

Main category: cs.CL

TL;DR: 本研究探讨了预训练语言模型识别外来语的能力，发现模型在这方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 在双语社区中，强势语言不断将词汇强加于弱势语言，因此区分外来语和本地词汇非常重要。本研究旨在了解预训练语言模型是否具有类似能力。

Method: 在10种语言中评估了多个模型，判断其区分外来语和本地词汇的能力。

Result: 研究结果表明，尽管有明确的指示和上下文信息，模型在区分外来语和本地词汇方面的表现仍然很差。

Conclusion: 现代NLP系统对外来语而非本地词汇表现出偏差。这项工作对开发用于少数民族语言的NLP工具以及支持在受强势语言词汇压力下的社区中保护语言具有重要意义。

Abstract: Throughout language history, words are borrowed from one language to another
and gradually become integrated into the recipient's lexicon. Speakers can
often differentiate these loanwords from native vocabulary, particularly in
bilingual communities where a dominant language continuously imposes lexical
items on a minority language. This paper investigates whether pretrained
language models, including large language models, possess similar capabilities
for loanword identification. We evaluate multiple models across 10 languages.
Despite explicit instructions and contextual information, our results show that
models perform poorly in distinguishing loanwords from native ones. These
findings corroborate previous evidence that modern NLP systems exhibit a bias
toward loanwords rather than native equivalents. Our work has implications for
developing NLP tools for minority languages and supporting language
preservation in communities under lexical pressure from dominant languages.

</details>


### [34] [Distilling Multilingual Vision-Language Models: When Smaller Models Stay Multilingual](https://arxiv.org/abs/2510.26271)
*Sukrit Sriratanawilai,Jhayahgrit Thongwat,Romrawin Chumpu,Patomporn Payoungkhamdee,Sarana Nutanong,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: 研究了知识蒸馏（KD）在多语言视觉语言模型（VLM）中的应用，发现某些配置可以在模型压缩一半的情况下保持或提高多语言检索的鲁棒性，但其他配置无法保持跨任务稳定性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在不同语言上的表现不均衡，并且当模型尺寸减小时，这个问题通常会加剧。知识蒸馏在将知识从较大的视觉语言模型转移到较小的模型方面表现出良好的效果，但在多语言环境中使用知识蒸馏是一个未被充分探索的领域。

Method: 对五种蒸馏方法进行了受控的实证研究，分离了它们在模型压缩下对跨语言表示一致性和下游性能稳定性的影响。研究了CLIP和SigLIP2中的五种蒸馏方法，并在领域内检索和领域外视觉问答上评估它们。

Result: 发现某些配置可以保持甚至提高多语言检索的鲁棒性，尽管模型尺寸缩小了一半，但其他配置未能保持跨任务稳定性，揭示了设计敏感的权衡，而单独的聚合精度无法揭示这些权衡。

Conclusion: 知识蒸馏在多语言视觉语言模型中具有应用潜力，但需要仔细设计以保持跨语言表示一致性和下游性能稳定性。

Abstract: Vision-language models (VLMs) exhibit uneven performance across languages, a
problem that is often exacerbated when the model size is reduced. While
Knowledge distillation (KD) demonstrates promising results in transferring
knowledge from larger to smaller VLMs, applying KD in multilingualism is an
underexplored area. This paper presents a controlled empirical study of KD
behavior across five distillation approaches, isolating their effects on
cross-lingual representation consistency and downstream performance stability
under model compression. We study five distillation formulations across CLIP
and SigLIP2, and evaluate them on in-domain retrieval and out-of-domain visual
QA. We find that some configurations preserve or even improve multilingual
retrieval robustness despite halving model size, but others fail to maintain
cross-task stability, exposing design-sensitive trade-offs that aggregate
accuracy alone does not reveal.

</details>


### [35] [Do LLMs Signal When They're Right? Evidence from Neuron Agreement](https://arxiv.org/abs/2510.26277)
*Kang Chen,Yaoning Wang,Kai Xiong,Zhuoka Feng,Wenhe Sun,Haotian Chen,Yixin Cao*

Main category: cs.CL

TL;DR: 提出了一种名为神经元一致性解码 (NAD) 的无监督 best-of-N 方法，该方法利用神经元激活的稀疏性和交叉样本神经元一致性来选择候选对象。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型推理方法主要依赖外部输出信号进行评分，这些信号在训练后可能校准不良。因此，本文旨在探索利用模型内部行为（神经元激活）来进行无监督的候选选择。

Method: 分析神经元激活，发现正确答案激活的神经元数量少于错误答案，且正确答案的激活在不同样本间具有更强的一致性。基于此，提出了 NAD 方法，该方法利用激活稀疏性和交叉样本神经元一致性来选择候选对象。

Result: 在数学和科学基准测试中，NAD 与多数投票相匹配；在开放式编码基准测试中，NAD 优于 Avg@64。通过尽早剪枝，NAD 减少了 99% 的 token 使用量，而生成质量损失很小。

Conclusion: 内部信号为无监督集成解码提供了可靠、可扩展和高效的指导。

Abstract: Large language models (LLMs) commonly boost reasoning via
sample-evaluate-ensemble decoders, achieving label free gains without ground
truth. However, prevailing strategies score candidates using only external
outputs such as token probabilities, entropies, or self evaluations, and these
signals can be poorly calibrated after post training. We instead analyze
internal behavior based on neuron activations and uncover three findings: (1)
external signals are low dimensional projections of richer internal dynamics;
(2) correct responses activate substantially fewer unique neurons than
incorrect ones throughout generation; and (3) activations from correct
responses exhibit stronger cross sample agreement, whereas incorrect ones
diverge. Motivated by these observations, we propose Neuron Agreement Decoding
(NAD), an unsupervised best-of-N method that selects candidates using
activation sparsity and cross sample neuron agreement, operating solely on
internal signals and without requiring comparable textual outputs. NAD enables
early correctness prediction within the first 32 generated tokens and supports
aggressive early stopping. Across math and science benchmarks with verifiable
answers, NAD matches majority voting; on open ended coding benchmarks where
majority voting is inapplicable, NAD consistently outperforms Avg@64. By
pruning unpromising trajectories early, NAD reduces token usage by 99% with
minimal loss in generation quality, showing that internal signals provide
reliable, scalable, and efficient guidance for label free ensemble decoding.

</details>


### [36] [Unravelling the Mechanisms of Manipulating Numbers in Language Models](https://arxiv.org/abs/2510.26285)
*Michal Štefánik,Timothee Mickus,Marek Kadlčík,Bertram Højer,Michal Spiegel,Raúl Vázquez,Aman Sinha,Josef Kuchař,Philipp Mondorf*

Main category: cs.CL

TL;DR: 大型语言模型(LLM)在处理数字时，输入嵌入表示具有相似性和准确性，但输出结果却容易出错。本研究旨在解释这一矛盾，并量化这些机制的准确性下限。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型如何处理数字，解释LLM在处理数字时输入表示准确但输出结果出错的矛盾现象。

Method: 通过创建通用探针，追踪信息到特定层，研究不同语言模型学习到的数字表示。

Result: 发现不同语言模型学习到的数字表示是可互换的、系统的、高度准确的，并且在隐藏状态和输入上下文类型中是通用的。可以创建通用探针来追踪信息，包括输出错误的起因，到特定层。

Conclusion: 为预训练LLM如何处理数字奠定了基础理解，并概述了更精确的探测技术在改进LLM架构方面的潜力。

Abstract: Recent work has shown that different large language models (LLMs) converge to
similar and accurate input embedding representations for numbers. These
findings conflict with the documented propensity of LLMs to produce erroneous
outputs when dealing with numeric information. In this work, we aim to explain
this conflict by exploring how language models manipulate numbers and quantify
the lower bounds of accuracy of these mechanisms. We find that despite
surfacing errors, different language models learn interchangeable
representations of numbers that are systematic, highly accurate and universal
across their hidden states and the types of input contexts. This allows us to
create universal probes for each LLM and to trace information -- including the
causes of output errors -- to specific layers. Our results lay a fundamental
understanding of how pre-trained LLMs manipulate numbers and outline the
potential of more accurate probing techniques in addressed refinements of LLMs'
architectures.

</details>


### [37] [Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games](https://arxiv.org/abs/2510.26298)
*Jingran Zhang,Ning Li,Justin Cui*

Main category: cs.CL

TL;DR: OpenAI的ChatGPT Atlas能够与网页互动，但其在动态互动环境中的表现尚不清楚。


<details>
  <summary>Details</summary>
Motivation: 研究Atlas在动态、互动环境中的网络互动能力。

Method: 使用基于浏览器的游戏作为测试场景，包括Google的T-Rex Runner, Sudoku, Flappy Bird, 和 Stein.world，并使用游戏内的表现分数作为量化指标来评估性能。

Result: Atlas在逻辑推理任务（如数独）中表现出色，但在需要精确 timing 和运动控制的实时游戏中表现不佳。

Conclusion: Atlas展示了强大的分析处理能力，但在需要实时互动的动态网络环境中仍存在明显的局限性。

Abstract: OpenAI's ChatGPT Atlas introduces new capabilities for web interaction,
enabling the model to analyze webpages, process user intents, and execute
cursor and keyboard inputs directly within the browser. While its capacity for
information retrieval tasks has been demonstrated, its performance in dynamic,
interactive environments remains less explored. In this study, we conduct an
early evaluation of Atlas's web interaction capabilities using browser-based
games as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird,
and Stein.world. We employ in-game performance scores as quantitative metrics
to assess performance across different task types. Our results show that Atlas
performs strongly in logical reasoning tasks like Sudoku, completing puzzles
significantly faster than human baselines, but struggles substantially in
real-time games requiring precise timing and motor control, often failing to
progress beyond initial obstacles. These findings suggest that while Atlas
demonstrates capable analytical processing, there remain notable limitations in
dynamic web environments requiring real-time interaction. The website of our
project can be found at https://atlas-game-eval.github.io.

</details>


### [38] [SCRIBE: Structured Chain Reasoning for Interactive Behaviour Explanations using Tool Calling](https://arxiv.org/abs/2510.26322)
*Fares Fawzi,Vinitra Swamy,Dominik Glandorf,Tanya Nazaretsky,Tanja Käser*

Main category: cs.CL

TL;DR: SCRIBE是一个用于生成针对学生问题的有效反馈的框架，它结合了领域特定工具和自反推理流程。


<details>
  <summary>Details</summary>
Motivation: 在教育环境中，使用语言模型提供互动式、个性化的学生反馈面临隐私、计算资源有限以及需要教学上有效的回应等挑战。因此，需要小型、开源的模型，这些模型可以在本地运行，并可靠地将其输出建立在正确的信息基础上。

Method: SCRIBE框架采用多跳、工具增强的推理，并提炼成3B和8B模型，通过在合成的GPT-4o生成数据上进行两阶段LoRA微调实现。

Result: 8B-SCRIBE模型在相关性和可操作性等关键维度上达到了与更大模型相当或更高的质量，并且学生认为其与GPT-4o和Llama-3 70B相当。

Conclusion: SCRIBE在低资源、对隐私敏感的教育应用中是可行的。

Abstract: Language models can be used to provide interactive, personalized student
feedback in educational settings. However, real-world deployment faces three
key challenges: privacy concerns, limited computational resources, and the need
for pedagogically valid responses. These constraints require small, open-source
models that can run locally and reliably ground their outputs in correct
information. We introduce SCRIBE, a framework for multi-hop, tool-augmented
reasoning designed to generate valid responses to student questions about
feedback reports. SCRIBE combines domain-specific tools with a self-reflective
inference pipeline that supports iterative reasoning, tool use, and error
recovery. We distil these capabilities into 3B and 8B models via two-stage LoRA
fine-tuning on synthetic GPT-4o-generated data. Evaluation with a human-aligned
GPT-Judge and a user study with 108 students shows that 8B-SCRIBE models
achieve comparable or superior quality to much larger models in key dimensions
such as relevance and actionability, while being perceived on par with GPT-4o
and Llama-3.3 70B by students. These findings demonstrate the viability of
SCRIBE for low-resource, privacy-sensitive educational applications.

</details>


### [39] [From Amateur to Master: Infusing Knowledge into LLMs via Automated Curriculum Learning](https://arxiv.org/abs/2510.26336)
*Nishit Neema,Srinjoy Mukherjee,Sapan Shah,Gokul Ramakrishnan,Ganesh Venkatesh*

Main category: cs.CL

TL;DR: ACER improves LLMs in specialized domains without losing general abilities.


<details>
  <summary>Details</summary>
Motivation: LLMs underperform in specialized domains requiring deep understanding.

Method: ACER creates a textbook-style curriculum of QA pairs based on Bloom's taxonomy for continual pretraining.

Result: ACER improves performance on specialized MMLU subsets and knowledge-intensive benchmarks, prevents catastrophic forgetting, and facilitates positive cross-domain knowledge transfer.

Conclusion: ACER is a scalable and effective method for improving LLMs in specialized domains.

Abstract: Large Language Models (LLMs) excel at general tasks but underperform in
specialized domains like economics and psychology, which require deep,
principled understanding. To address this, we introduce ACER (Automated
Curriculum-Enhanced Regimen) that transforms generalist models into domain
experts without sacrificing their broad capabilities. ACER first synthesizes a
comprehensive, textbook-style curriculum by generating a table of contents for
a subject and then creating question-answer (QA) pairs guided by Bloom's
taxonomy. This ensures systematic topic coverage and progressively increasing
difficulty. The resulting synthetic corpus is used for continual pretraining
with an interleaved curriculum schedule, aligning learning across both content
and cognitive dimensions.
  Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized
MMLU subsets. In challenging domains like microeconomics, where baselines
struggle, ACER boosts accuracy by 5 percentage points. Across all target
domains, we observe a consistent macro-average improvement of 3 percentage
points. Notably, ACER not only prevents catastrophic forgetting but also
facilitates positive cross-domain knowledge transfer, improving performance on
non-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on
knowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points,
while maintaining stable performance on general reasoning tasks. Our results
demonstrate that ACER offers a scalable and effective recipe for closing
critical domain gaps in LLMs.

</details>


### [40] [MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data](https://arxiv.org/abs/2510.26345)
*Mykhailo Poliakov,Nadiya Shvai*

Main category: cs.CL

TL;DR: 本研究探讨了使用合成数据生成和轻量级微调技术来提高大型语言模型识别错误论证的能力，特别是在与健康相关的错误信息领域。


<details>
  <summary>Details</summary>
Motivation: 识别扭曲或误解科学发现的虚假声明非常困难，并且与健康相关的错误信息非常普遍且具有潜在危害。

Method: 该研究提出了一种名为MisSynth的流程，该流程应用检索增强生成（RAG）来生成合成的谬误样本，然后使用这些样本来微调LLM模型。

Result: 结果表明，与原始基线相比，微调模型在准确性方面有显着提高。例如，经过微调的LLaMA 3.1 8B模型在MISSCI测试集中实现了超过35%的F1-score绝对提升。

Conclusion: 引入合成的谬误数据来扩充有限的注释资源可以显着提高LLM在现实世界科学错误信息任务上的zero-shot分类性能，即使在计算资源有限的情况下也是如此。

Abstract: Health-related misinformation is very prevalent and potentially harmful. It
is difficult to identify, especially when claims distort or misinterpret
scientific findings. We investigate the impact of synthetic data generation and
lightweight fine-tuning techniques on the ability of large language models
(LLMs) to recognize fallacious arguments using the MISSCI dataset and
framework. In this work, we propose MisSynth, a pipeline that applies
retrieval-augmented generation (RAG) to produce synthetic fallacy samples,
which are then used to fine-tune an LLM model. Our results show substantial
accuracy gains with fine-tuned models compared to vanilla baselines. For
instance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score
absolute improvement on the MISSCI test split over its vanilla baseline. We
demonstrate that introducing synthetic fallacy data to augment limited
annotated resources can significantly enhance zero-shot LLM classification
performance on real-world scientific misinformation tasks, even with limited
computational resources. The code and synthetic dataset are available on
https://github.com/mxpoliakov/MisSynth.

</details>


### [41] [The Geometry of Dialogue: Graphing Language Models to Reveal Synergistic Teams for Multi-Agent Collaboration](https://arxiv.org/abs/2510.26352)
*Kotaro Furuya,Yuichi Kitagawa*

Main category: cs.CL

TL;DR: 提出了一种基于交互的多智能体LLM团队自动组建框架，无需先验知识即可发现协同模型集群。


<details>
  <summary>Details</summary>
Motivation: 当前的多智能体LLM方法依赖于协同团队组成，但由于模型的不透明性，形成最优团队是一个重大挑战。

Method: 构建“语言模型图”，该图通过成对对话的语义连贯性映射模型之间的关系，然后应用社区检测来识别协同模型集群。

Result: 实验表明，该方法发现的功能连贯的群体反映了其潜在的专业化。通过特定主题的对话启动，协同团队在下游基准测试中优于随机基线，并实现了与基于已知模型专业化手动策划的团队相当的准确性。

Conclusion: 研究结果为协同多智能体LLM团队的自动设计提供了新的基础。

Abstract: While a multi-agent approach based on large language models (LLMs) represents
a promising strategy to surpass the capabilities of single models, its success
is critically dependent on synergistic team composition. However, forming
optimal teams is a significant challenge, as the inherent opacity of most
models obscures the internal characteristics necessary for effective
collaboration. In this paper, we propose an interaction-centric framework for
automatic team composition that does not require any prior knowledge including
their internal architectures, training data, or task performances. Our method
constructs a "language model graph" that maps relationships between models from
the semantic coherence of pairwise conversations, and then applies community
detection to identify synergistic model clusters. Our experiments with diverse
LLMs demonstrate that the proposed method discovers functionally coherent
groups that reflect their latent specializations. Priming conversations with
specific topics identified synergistic teams which outperform random baselines
on downstream benchmarks and achieve comparable accuracy to that of
manually-curated teams based on known model specializations. Our findings
provide a new basis for the automated design of collaborative multi-agent LLM
teams.

</details>


### [42] [On the Role of Context for Discourse Relation Classification in Scientific Writing](https://arxiv.org/abs/2510.26354)
*Stephen Wan,Wei Liu,Michael Strube*

Main category: cs.CL

TL;DR: 探讨使用话语级别信息为 AI 生成的科学主张寻找支持证据，重点是科学写作中的话语结构推理任务。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能 (AI) 方法越来越多地用于支持科学工作流程，我们对使用语篇层面的信息来寻找 AI 生成的科学主张的支持证据感兴趣。

Method: 研究预训练语言模型 (PLM) 和大型语言模型 (LLM) 方法用于语篇关系分类 (DRC)，重点关注科学出版物这一任务中未被充分研究的类型。我们研究了语境如何帮助 DRC 任务，实验表明，语境（由语篇结构定义）通常是有帮助的。

Result: 实验表明，语境（由语篇结构定义）通常是有帮助的。分析了哪些科学语篇关系类型可能从语境中获益最多。

Conclusion: 本文初步探讨了预训练语言模型 (PLM) 和大型语言模型 (LLM) 方法在语篇关系分类 (DRC) 中的应用，重点关注科学出版物，这是一个该任务中未被充分研究的类型。

Abstract: With the increasing use of generative Artificial Intelligence (AI) methods to
support science workflows, we are interested in the use of discourse-level
information to find supporting evidence for AI generated scientific claims. A
first step towards this objective is to examine the task of inferring discourse
structure in scientific writing.
  In this work, we present a preliminary investigation of pretrained language
model (PLM) and Large Language Model (LLM) approaches for Discourse Relation
Classification (DRC), focusing on scientific publications, an under-studied
genre for this task. We examine how context can help with the DRC task, with
our experiments showing that context, as defined by discourse structure, is
generally helpful. We also present an analysis of which scientific discourse
relation types might benefit most from context.

</details>


### [43] [OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large Language Models in Education](https://arxiv.org/abs/2510.26422)
*Min Zhang,Hao Chen,Hao Chen,Wenqi Zhang,Didi Zhu,Xin Lin,Bo Jiang,Aimin Zhou,Fei Wu,Kun Kuang*

Main category: cs.CL

TL;DR: 提出了OmniEduBench，一个综合性的中文教育基准，用于评估大型语言模型在知识和能力培养方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型及其基准主要关注知识维度，忽略了对实际教育场景至关重要的能力培养的评估，并且缺乏足够的多样性，尤其是在中文环境中。

Method: 构建了包含24.602K高质量问答对的OmniEduBench，数据分为知识和能力培养两个核心维度，每个维度进一步细分为6个细粒度类别，涵盖61个不同的科目，包含11种常见的考试题型。

Result: 在11个主流开源和闭源LLM上进行了实验，结果表明在知识维度上，只有Gemini-2.5 Pro超过了60%的准确率，而在能力培养维度上，表现最好的模型QWQ仍然落后于人类智能近30%。

Conclusion: 表明大型语言模型在教育领域有很大的改进空间和挑战。

Abstract: With the rapid development of large language models (LLMs), various LLM-based
works have been widely applied in educational fields. However, most existing
LLMs and their benchmarks focus primarily on the knowledge dimension, largely
neglecting the evaluation of cultivation capabilities that are essential for
real-world educational scenarios. Additionally, current benchmarks are often
limited to a single subject or question type, lacking sufficient diversity.
This issue is particularly prominent within the Chinese context. To address
this gap, we introduce OmniEduBench, a comprehensive Chinese educational
benchmark. OmniEduBench consists of 24.602K high-quality question-answer pairs.
The data is meticulously divided into two core dimensions: the knowledge
dimension and the cultivation dimension, which contain 18.121K and 6.481K
entries, respectively. Each dimension is further subdivided into 6 fine-grained
categories, covering a total of 61 different subjects (41 in the knowledge and
20 in the cultivation). Furthermore, the dataset features a rich variety of
question formats, including 11 common exam question types, providing a solid
foundation for comprehensively evaluating LLMs' capabilities in education.
Extensive experiments on 11 mainstream open-source and closed-source LLMs
reveal a clear performance gap. In the knowledge dimension, only Gemini-2.5 Pro
surpassed 60\% accuracy, while in the cultivation dimension, the
best-performing model, QWQ, still trailed human intelligence by nearly 30\%.
These results highlight the substantial room for improvement and underscore the
challenges of applying LLMs in education.

</details>


### [44] [1+1>2: A Synergistic Sparse and Low-Rank Compression Method for Large Language Models](https://arxiv.org/abs/2510.26446)
*Zeliang Zong,Kai Zhang,Zheyang Li,Wenming Tan,Ye Ren,Yiyan Zhai,Jilin Hu*

Main category: cs.CL

TL;DR: 提出了一种名为 SSLC 的 LLM 协同稀疏和低秩压缩方法，无需额外训练步骤即可实现最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 在语言理解和生成方面表现出了卓越的水平；然而，它们的大规模采用受到大量带宽和计算需求的限制。虽然剪枝和低秩近似各自都表现出了良好的性能，但它们在 LLM 中的协同作用仍有待探索。

Method: 提出了协同稀疏和低秩压缩 (SSLC) 方法，该方法利用了这两种技术的优势：低秩近似通过保留其基本结构以最小的信息损失来压缩模型，而稀疏优化消除了不重要的权重，保留了对泛化至关重要的权重。基于理论分析，我们首先将低秩近似和稀疏优化公式化为一个统一的问题，并通过迭代优化算法求解。

Result: 在 LLaMA 和 Qwen2.5 模型 (7B-70B) 上的实验表明，SSLC 在没有任何额外训练步骤的情况下，始终超过了独立方法，实现了最先进的结果。值得注意的是，SSLC 在不降低性能的情况下将 Qwen2.5 压缩了 50%，并实现了至少 1.63 倍的加速，为高效的 LLM 部署提供了实用的解决方案。

Conclusion: SSLC 是一种用于 LLM 的协同稀疏和低秩压缩方法，它优于单独使用稀疏或低秩方法。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in
language comprehension and generation; however, their widespread adoption is
constrained by substantial bandwidth and computational demands. While pruning
and low-rank approximation have each demonstrated promising performance
individually, their synergy for LLMs remains underexplored. We introduce
\underline{S}ynergistic \underline{S}parse and \underline{L}ow-Rank
\underline{C}ompression (SSLC) methods for LLMs, which leverages the strengths
of both techniques: low-rank approximation compresses the model by retaining
its essential structure with minimal information loss, whereas sparse
optimization eliminates non-essential weights, preserving those crucial for
generalization. Based on theoretical analysis, we first formulate the low-rank
approximation and sparse optimization as a unified problem and solve it by
iterative optimization algorithm. Experiments on LLaMA and Qwen2.5 models
(7B-70B) show that SSLC, without any additional training steps, consistently
surpasses standalone methods, achieving state-of-the-arts results. Notably,
SSLC compresses Qwen2.5 by 50\% with no performance drop and achieves at least
1.63$\times$ speedup, offering a practical solution for efficient LLM
deployment.

</details>


### [45] [Bayesian Network Fusion of Large Language Models for Sentiment Analysis](https://arxiv.org/abs/2510.26484)
*Rasoul Amirzadeh,Dhananjay Thiruvady,Fatemeh Shiri*

Main category: cs.CL

TL;DR: 提出了贝叶斯网络LLM融合（BNLF）框架，用于情感分析。


<details>
  <summary>Details</summary>
Motivation: 领域特定的大型语言模型缺乏透明性和可解释性，微调成本高，需要大量的提示工程，跨领域结果不一致，并且计算需求高，对环境有不利影响。

Method: 通过概率机制整合FinBERT、RoBERTa和BERTweet三个LLM的预测。

Result: 在三个人工标注的金融语料库中，BNLF的准确率比基线LLM始终高出约6%。

Conclusion: BNLF对数据集的变异性具有鲁棒性，并且概率融合对于可解释的情感分类是有效的。

Abstract: Large language models (LLMs) continue to advance, with an increasing number
of domain-specific variants tailored for specialised tasks. However, these
models often lack transparency and explainability, can be costly to fine-tune,
require substantial prompt engineering, yield inconsistent results across
domains, and impose significant adverse environmental impact due to their high
computational demands. To address these challenges, we propose the Bayesian
network LLM fusion (BNLF) framework, which integrates predictions from three
LLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic
mechanism for sentiment analysis. BNLF performs late fusion by modelling the
sentiment predictions from multiple LLMs as probabilistic nodes within a
Bayesian network. Evaluated across three human-annotated financial corpora with
distinct linguistic and contextual characteristics, BNLF demonstrates
consistent gains of about six percent in accuracy over the baseline LLMs,
underscoring its robustness to dataset variability and the effectiveness of
probabilistic fusion for interpretable sentiment classification.

</details>


### [46] [A Multi-agent Large Language Model Framework to Automatically Assess Performance of a Clinical AI Triage Tool](https://arxiv.org/abs/2510.26498)
*Adam E. Flanders,Yifan Peng,Luciano Prevedello,Robyn Ball,Errol Colak,Prahlad Menon,George Shih,Hui-Ming Lin,Paras Lakhani*

Main category: cs.CL

TL;DR: 本研究旨在评估使用多个LLM集成模型是否比单个LLM模型更可靠地评估基于像素的AI分诊工具。


<details>
  <summary>Details</summary>
Motivation: 为了提高AI分诊工具评估的可靠性。

Method: 使用八个开源LLM模型和GPT-4o分析了来自14家医院的29,766份非对比CT头部检查报告，并手动审查了1,726个示例。比较了八个开源模型和共识与GPT-4o的性能特征。测试了三种理想的共识LLM集成模型对分诊工具性能的评级。

Result: Llama3.3:70b和GPT-4o的AUC最高（0.78）。Llama3.3:70b的F1得分最高（0.81），召回率最高（0.85）。最佳LLM组合为：Full-9 Ensemble 0.571 (0.552-0.591), Top-3 Ensemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), GPT4o 0.522 (0.500-0.543)。

Conclusion: 中大型开源LLM的集成模型比单独使用单个LLM提供了一种更一致和可靠的方法，可以对临床AI分诊工具进行回顾性评估。

Abstract: Purpose: The purpose of this study was to determine if an ensemble of
multiple LLM agents could be used collectively to provide a more reliable
assessment of a pixel-based AI triage tool than a single LLM.
  Methods: 29,766 non-contrast CT head exams from fourteen hospitals were
processed by a commercial intracranial hemorrhage (ICH) AI detection tool.
Radiology reports were analyzed by an ensemble of eight open-source LLM models
and a HIPAA compliant internal version of GPT-4o using a single multi-shot
prompt that assessed for presence of ICH. 1,726 examples were manually
reviewed. Performance characteristics of the eight open-source models and
consensus were compared to GPT-4o. Three ideal consensus LLM ensembles were
tested for rating the performance of the triage tool.
  Results: The cohort consisted of 29,766 head CTs exam-report pairs. The
highest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78).
The average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 & 0.76).
Llama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater
precision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the
ideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3
Ensemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522
(0.500-0.543). No statistically significant differences were observed between
Top-3, Full-9, and Consensus (p > 0.05).
  Conclusion: An ensemble of medium to large sized open-source LLMs provides a
more consistent and reliable method to derive a ground truth retrospective
evaluation of a clinical AI triage tool over a single LLM alone.

</details>


### [47] [Hebrew Diacritics Restoration using Visual Representation](https://arxiv.org/abs/2510.26521)
*Yair Elboher,Yuval Pinter*

Main category: cs.CL

TL;DR: 提出了一种新的希伯来语恢复系统DIVRIT，该系统将该任务构建为一个零样本分类问题。


<details>
  <summary>Details</summary>
Motivation: 希伯来语恢复是确保准确发音和消除文本歧义的基本任务。当文本没有元音时，该语言具有高度的歧义性，但最近的机器学习方法显着提高了这项任务的性能。

Method: 该方法在单词级别上运行，从动态生成的候选集中为每个未加区分的单词选择最合适的区分模式，并以周围的文本上下文为条件。DIVRIT 的一项关键创新是它使用希伯来语视觉语言模型，该模型将未加区分的文本作为图像处理，从而允许将区分信息直接嵌入到输入的向量表示中。

Result: 通过对各种配置的全面评估，证明该系统无需依赖复杂的显式语言分析即可有效地执行区分。值得注意的是，在“oracle”设置中，保证正确的区分形式存在于提供的候选形式中，DIVRIT 实现了很高的准确度。

Conclusion: 这些发现突显了视觉表示在准确和自动希伯来语区分方面具有广阔的潜力。

Abstract: Diacritics restoration in Hebrew is a fundamental task for ensuring accurate
word pronunciation and disambiguating textual meaning. Despite the language's
high degree of ambiguity when unvocalized, recent machine learning approaches
have significantly advanced performance on this task.
  In this work, we present DIVRIT, a novel system for Hebrew diacritization
that frames the task as a zero-shot classification problem. Our approach
operates at the word level, selecting the most appropriate diacritization
pattern for each undiacritized word from a dynamically generated candidate set,
conditioned on the surrounding textual context. A key innovation of DIVRIT is
its use of a Hebrew Visual Language Model, which processes undiacritized text
as an image, allowing diacritic information to be embedded directly within the
input's vector representation.
  Through a comprehensive evaluation across various configurations, we
demonstrate that the system effectively performs diacritization without relying
on complex, explicit linguistic analysis. Notably, in an ``oracle'' setting
where the correct diacritized form is guaranteed to be among the provided
candidates, DIVRIT achieves a high level of accuracy. Furthermore, strategic
architectural enhancements and optimized training methodologies yield
significant improvements in the system's overall generalization capabilities.
These findings highlight the promising potential of visual representations for
accurate and automated Hebrew diacritization.

</details>


### [48] [The Structure of Relation Decoding Linear Operators in Large Language Models](https://arxiv.org/abs/2510.26543)
*Miranda Anna Christ,Adrián Csiszárik,Gergely Becsó,Dániel Varga*

Main category: cs.CL

TL;DR: 本文研究了转换器语言模型中解码特定关系事实的线性算子的结构，发现它们可以被高度压缩且具有语义属性。


<details>
  <summary>Details</summary>
Motivation: 探究转换器语言模型如何解码关系事实。

Method: 将单关系研究扩展到多个关系，并系统地绘制它们的组织结构。开发交叉评估协议。

Result: 发现线性映射并不编码不同的关系，而是提取重复出现的、粗粒度的语义属性。关系解码器可以通过简单的三阶张量网络高度压缩，而不会显著降低解码精度。

Conclusion: 转换器语言模型中的线性关系解码主要是基于属性的，而不是特定于关系的。

Abstract: This paper investigates the structure of linear operators introduced in
Hernandez et al. [2023] that decode specific relational facts in transformer
language models. We extend their single-relation findings to a collection of
relations and systematically chart their organization. We show that such
collections of relation decoders can be highly compressed by simple order-3
tensor networks without significant loss in decoding accuracy. To explain this
surprising redundancy, we develop a cross-evaluation protocol, in which we
apply each linear decoder operator to the subjects of every other relation. Our
results reveal that these linear maps do not encode distinct relations, but
extract recurring, coarse-grained semantic properties (e.g., country of capital
city and country of food are both in the country-of-X property). This
property-centric structure clarifies both the operators' compressibility and
highlights why they generalize only to new relations that are semantically
close. Our findings thus interpret linear relational decoding in transformer
language models as primarily property-based, rather than relation-specific.

</details>


### [49] [InfoFlow: Reinforcing Search Agent Via Reward Density Optimization](https://arxiv.org/abs/2510.26575)
*Kun Luo,Hongjin Qian,Zheng Liu,Ziyi Xia,Shitao Xiao,Siqi Bao,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 提出InfoFlow，一个系统性的框架，通过子问题分解、失败引导提示和双重代理细化来解决奖励稀疏问题，从而提升agentic deep search的效率。


<details>
  <summary>Details</summary>
Motivation: 在深度搜索场景中，奖励稀疏性阻碍了可验证奖励强化学习（RLVR）的应用，因为智能体需要花费大量的探索成本才能获得稀有且通常为零的最终奖励。

Method: 引入InfoFlow框架，它包含三个方面：1) 子问题分解，将长程任务分解以分配过程奖励，从而提供更密集的学习信号；2) 失败引导提示，将纠正性指导注入停滞的轨迹，以增加成功结果的概率；3) 双重代理细化，采用双重代理架构来分担深度探索的认知负担。

Result: 在多个agentic搜索基准测试中，InfoFlow显著优于强大的基线，使轻量级LLM能够达到与先进的专有LLM相当的性能。

Conclusion: InfoFlow 能够有效提升agentic deep search的效率，并使轻量级LLM达到与先进的专有LLM相当的性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach
for enhancing agentic deep search. However, its application is often hindered
by low \textbf{Reward Density} in deep search scenarios, where agents expend
significant exploratory costs for infrequent and often null final rewards. In
this paper, we formalize this challenge as the \textbf{Reward Density
Optimization} problem, which aims to improve the reward obtained per unit of
exploration cost. This paper introduce \textbf{InfoFlow}, a systematic
framework that tackles this problem from three aspects. 1) \textbf{Subproblem
decomposition}: breaking down long-range tasks to assign process rewards,
thereby providing denser learning signals. 2) \textbf{Failure-guided hints}:
injecting corrective guidance into stalled trajectories to increase the
probability of successful outcomes. 3) \textbf{Dual-agent refinement}:
employing a dual-agent architecture to offload the cognitive burden of deep
exploration. A refiner agent synthesizes the search history, which effectively
compresses the researcher's perceived trajectory, thereby reducing exploration
cost and increasing the overall reward density. We evaluate InfoFlow on
multiple agentic search benchmarks, where it significantly outperforms strong
baselines, enabling lightweight LLMs to achieve performance comparable to
advanced proprietary LLMs.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [50] [Enhancing Underwater Object Detection through Spatio-Temporal Analysis and Spatial Attention Networks](https://arxiv.org/abs/2510.25797)
*Sai Likhith Karri,Ansh Saxena*

Main category: cs.CV

TL;DR: 本研究探讨了时空建模和空间注意力机制在水下目标检测深度学习模型中的有效性。


<details>
  <summary>Details</summary>
Motivation: 水下环境复杂，目标检测具有挑战性，需要提高检测精度和泛化能力。

Method: 首先评估了时序增强的 YOLOv5 变体 T-YOLOv5 的性能，并与标准 YOLOv5 进行了比较。然后，通过添加卷积块注意力模块 (CBAM) 开发了 T-YOLOv5 的增强版本。

Result: T-YOLOv5 和带有 CBAM 的 T-YOLOv5 的 mAP@50-95 分数分别为 0.813 和 0.811，优于 YOLOv5 的 0.563。

Conclusion: T-YOLOv5 显著提高了检测可靠性，而带有 CBAM 的 T-YOLOv5 进一步提高了在复杂场景中的性能，但在简单场景中损失了精度。

Abstract: This study examines the effectiveness of spatio-temporal modeling and the
integration of spatial attention mechanisms in deep learning models for
underwater object detection. Specifically, in the first phase, the performance
of temporal-enhanced YOLOv5 variant T-YOLOv5 is evaluated, in comparison with
the standard YOLOv5. For the second phase, an augmented version of T-YOLOv5 is
developed, through the addition of a Convolutional Block Attention Module
(CBAM). By examining the effectiveness of the already pre-existing YOLOv5 and
T-YOLOv5 models and of the newly developed T-YOLOv5 with CBAM. With CBAM, the
research highlights how temporal modeling improves detection accuracy in
dynamic marine environments, particularly under conditions of sudden movements,
partial occlusions, and gradual motion. The testing results showed that YOLOv5
achieved a mAP@50-95 of 0.563, while T-YOLOv5 and T-YOLOv5 with CBAM
outperformed with mAP@50-95 scores of 0.813 and 0.811, respectively,
highlighting their superior accuracy and generalization in detecting complex
objects. The findings demonstrate that T-YOLOv5 significantly enhances
detection reliability compared to the standard model, while T-YOLOv5 with CBAM
further improves performance in challenging scenarios, although there is a loss
of accuracy when it comes to simpler scenarios.

</details>


### [51] [MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency](https://arxiv.org/abs/2510.25897)
*Nicolas Dufour,Lucas Degeorge,Arijit Ghosh,Vicky Kalogeiton,David Picard*

Main category: cs.CV

TL;DR: 提出了一种名为MIRO的新方法，通过在训练期间将模型置于多个奖励模型的条件下，直接学习用户偏好，从而提高生成图像的视觉质量并加快训练速度。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型在大型未策划数据集上进行训练，但这与用户偏好不符。奖励模型通常用于执行生成图像的事后选择，但这会损害多样性、语义保真度和效率。

Method: 在训练期间将模型置于多个奖励模型的条件下，让模型直接学习用户偏好。

Result: 在GenEval组合基准和用户偏好分数（PickAScore、ImageReward、HPSv2）上实现了最先进的性能。

Conclusion: MIRO方法不仅显著提高了生成图像的视觉质量，而且显著加快了训练速度。

Abstract: Current text-to-image generative models are trained on large uncurated
datasets to enable diverse generation capabilities. However, this does not
align well with user preferences. Recently, reward models have been
specifically designed to perform post-hoc selection of generated images and
align them to a reward, typically user preference. This discarding of
informative data together with the optimizing for a single reward tend to harm
diversity, semantic fidelity and efficiency. Instead of this post-processing,
we propose to condition the model on multiple reward models during training to
let the model learn user preferences directly. We show that this not only
dramatically improves the visual quality of the generated images but it also
significantly speeds up the training. Our proposed method, called MIRO,
achieves state-of-the-art performances on the GenEval compositional benchmark
and user-preference scores (PickAScore, ImageReward, HPSv2).

</details>


### [52] [BikeScenes: Online LiDAR Semantic Segmentation for Bicycles](https://arxiv.org/abs/2510.25901)
*Denniz Goren,Holger Caesar*

Main category: cs.CV

TL;DR: 由于更快的电动自行车越来越受欢迎，骑自行车的人越来越容易受到伤害，因此需要调整汽车感知技术以提高自行车安全性。我们使用多传感器“SenseBike”研究平台来开发和评估专为自行车量身定制的 3D 激光雷达分割方法。


<details>
  <summary>Details</summary>
Motivation: 由于骑自行车的人越来越容易受到伤害，因此需要调整汽车感知技术以提高自行车安全性。

Method: 我们使用多传感器“SenseBike”研究平台来开发和评估专为自行车量身定制的 3D 激光雷达分割方法。为了弥合汽车到自行车的领域差距，我们引入了新型 BikeScenes-lidarseg 数据集，该数据集包含代尔夫特理工大学校园周围的 3021 个连续激光雷达扫描，并针对 29 个动态和静态类别进行了语义注释。

Result: 通过评估模型性能，我们证明了在我们的 BikeScenes 数据集上进行微调可实现 63.6% 的平均 Intersection-over-Union (mIoU)，明显优于仅使用 SemanticKITTI 预训练获得的 13.8%。

Conclusion: 该结果强调了特定领域训练的必要性和有效性。我们强调了特定于自行车安装、硬件受限的感知系统的关键挑战，并将 BikeScenes 数据集作为促进以骑自行车者为中心的激光雷达分割研究的资源。

Abstract: The vulnerability of cyclists, exacerbated by the rising popularity of faster
e-bikes, motivates adapting automotive perception technologies for bicycle
safety. We use our multi-sensor 'SenseBike' research platform to develop and
evaluate a 3D LiDAR segmentation approach tailored to bicycles. To bridge the
automotive-to-bicycle domain gap, we introduce the novel BikeScenes-lidarseg
Dataset, comprising 3021 consecutive LiDAR scans around the university campus
of the TU Delft, semantically annotated for 29 dynamic and static classes. By
evaluating model performance, we demonstrate that fine-tuning on our BikeScenes
dataset achieves a mean Intersection-over-Union (mIoU) of 63.6%, significantly
outperforming the 13.8% obtained with SemanticKITTI pre-training alone. This
result underscores the necessity and effectiveness of domain-specific training.
We highlight key challenges specific to bicycle-mounted, hardware-constrained
perception systems and contribute the BikeScenes dataset as a resource for
advancing research in cyclist-centric LiDAR segmentation.

</details>


### [53] [Generative Image Restoration and Super-Resolution using Physics-Informed Synthetic Data for Scanning Tunneling Microscopy](https://arxiv.org/abs/2510.25921)
*Nikola L. Kolev,Tommaso Rodani,Neil J. Curson,Taylor J. Z. Stock,Alberto Cazzaniga*

Main category: cs.CV

TL;DR: 提出了一种机器学习（ML）方法，用于图像修复和超分辨率，以应对扫描隧道显微镜（STM）的挑战，例如探针退化和数据采集缓慢。


<details>
  <summary>Details</summary>
Motivation: 扫描隧道显微镜（STM）虽然可以实现原子分辨率成像和原子操纵，但其应用通常受到探针退化和串行数据采集缓慢的限制。此外，探针制造过程复杂，因为探针经常 подвергается 大电压，这可能会改变其顶端的形状，需要对其进行调节。

Method: 利用仅包含 36 个原始 Si(001):H 实验图像的数据集，证明了基于物理信息的合成数据生成管道可用于训练多个最先进的流动匹配和扩散模型。

Result: 定量评估（例如 CLIP 最大平均差异 (CMMD) 分数和结构相似性）表明，该模型能够有效地恢复图像，并通过从稀疏采样数据中准确重建图像，从而将图像采集时间缩短两到四倍。

Conclusion: 该框架有潜力通过减少探针调节程序的频率和提高现有高速 STM 系统的帧速率来显着提高 STM 实验吞吐量。

Abstract: Scanning tunnelling microscopy (STM) enables atomic-resolution imaging and
atom manipulation, but its utility is often limited by tip degradation and slow
serial data acquisition. Fabrication adds another layer of complexity since the
tip is often subjected to large voltages, which may alter the shape of its
apex, requiring it to be conditioned. Here, we propose a machine learning (ML)
approach for image repair and super-resolution to alleviate both challenges.
Using a dataset of only 36 pristine experimental images of Si(001):H, we
demonstrate that a physics-informed synthetic data generation pipeline can be
used to train several state-of-the-art flow-matching and diffusion models.
Quantitative evaluation with metrics such as the CLIP Maximum Mean Discrepancy
(CMMD) score and structural similarity demonstrates that our models are able to
effectively restore images and offer a two- to fourfold reduction in image
acquisition time by accurately reconstructing images from sparsely sampled
data. Our framework has the potential to significantly increase STM
experimental throughput by offering a route to reducing the frequency of
tip-conditioning procedures and to enhancing frame rates in existing high-speed
STM systems.

</details>


### [54] [SplitFlow: Flow Decomposition for Inversion-Free Text-to-Image Editing](https://arxiv.org/abs/2510.25970)
*Sung-Hoon Yoon,Minghan Li,Gaspard Beaudouin,Congcong Wen,Muhammad Rafay Azhar,Mengyu Wang*

Main category: cs.CV

TL;DR: 提出了一种基于流分解和聚合的框架，用于图像编辑，解决了反演不准确和梯度纠缠问题，提高了语义保真度和属性解耦。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型在图像编辑任务中存在反演不准确和梯度纠缠问题，导致输出不能忠实反映目标提示。

Method: 将目标提示分解为多个子提示，为每个子提示计算独立的流，并聚合它们以形成统一的编辑轨迹。设计了一种投影和软聚合机制，自适应地加权子目标速度场，抑制语义冗余，同时强调不同的方向。

Result: 实验结果表明，该方法在语义保真度和属性解耦方面优于现有的零样本编辑方法。

Conclusion: 该方法通过流分解和聚合框架，有效解决了图像编辑中的反演不准确和梯度纠缠问题，提高了编辑质量。

Abstract: Rectified flow models have become a de facto standard in image generation due
to their stable sampling trajectories and high-fidelity outputs. Despite their
strong generative capabilities, they face critical limitations in image editing
tasks: inaccurate inversion processes for mapping real images back into the
latent space, and gradient entanglement issues during editing often result in
outputs that do not faithfully reflect the target prompt. Recent efforts have
attempted to directly map source and target distributions via ODE-based
approaches without inversion; however,these methods still yield suboptimal
editing quality. In this work, we propose a flow decomposition-and-aggregation
framework built upon an inversion-free formulation to address these
limitations. Specifically, we semantically decompose the target prompt into
multiple sub-prompts, compute an independent flow for each, and aggregate them
to form a unified editing trajectory. While we empirically observe that
decomposing the original flow enhances diversity in the target space,
generating semantically aligned outputs still requires consistent guidance
toward the full target prompt. To this end, we design a projection and
soft-aggregation mechanism for flow, inspired by gradient conflict resolution
in multi-task learning. This approach adaptively weights the sub-target
velocity fields, suppressing semantic redundancy while emphasizing distinct
directions, thereby preserving both diversity and consistency in the final
edited output. Experimental results demonstrate that our method outperforms
existing zero-shot editing approaches in terms of semantic fidelity and
attribute disentanglement. The code is available at
https://github.com/Harvard-AI-and-Robotics-Lab/SplitFlow.

</details>


### [55] [Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer](https://arxiv.org/abs/2510.25976)
*Roman Beliy,Amit Zalcher,Jonathan Kogman,Navve Wasserman,Michal Irani*

Main category: cs.CV

TL;DR: Brain-IT: 通过大脑交互 Transformer (BIT) 重建 fMRI 大脑记录中的图像，该 Transformer 允许功能相似的脑体素簇之间进行有效交互。


<details>
  <summary>Details</summary>
Motivation: 当前方法在从 fMRI 重建图像时，对实际所见图像的忠实度不足。

Method: 提出 Brain Interaction Transformer (BIT)，预测互补的局部图像特征，引导扩散模型。

Result: 在图像重建的视觉效果和客观指标上均超过当前最先进方法，且仅需 1 小时 fMRI 数据即可达到当前方法在 40 小时数据上的训练效果。

Conclusion: Brain-IT 能够从 fMRI 中忠实地重建所见图像，性能优于现有方法，且数据需求更少。

Abstract: Reconstructing images seen by people from their fMRI brain recordings
provides a non-invasive window into the human brain. Despite recent progress
enabled by diffusion models, current methods often lack faithfulness to the
actual seen images. We present "Brain-IT", a brain-inspired approach that
addresses this challenge through a Brain Interaction Transformer (BIT),
allowing effective interactions between clusters of functionally-similar
brain-voxels. These functional-clusters are shared by all subjects, serving as
building blocks for integrating information both within and across brains. All
model components are shared by all clusters & subjects, allowing efficient
training with a limited amount of data. To guide the image reconstruction, BIT
predicts two complementary localized patch-level image features: (i)high-level
semantic features which steer the diffusion model toward the correct semantic
content of the image; and (ii)low-level structural features which help to
initialize the diffusion process with the correct coarse layout of the image.
BIT's design enables direct flow of information from brain-voxel clusters to
localized image features. Through these principles, our method achieves image
reconstructions from fMRI that faithfully reconstruct the seen images, and
surpass current SotA approaches both visually and by standard objective
metrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve
results comparable to current methods trained on full 40-hour recordings.

</details>


### [56] [Fine-tuning Segment Anything for Real-Time Tumor Tracking in Cine-MRI](https://arxiv.org/abs/2510.25990)
*Valentin Boussot,Cédric Hémon,Jean-Claude Nunes,Jean-Louis Dillenseger*

Main category: cs.CV

TL;DR: This paper explores real-time tumor tracking in cine-MRI using foundation models, specifically SAM 2.1, fine-tuned on a small dataset. The method achieved a Dice score of 0.8794 on the TrackRAD2025 challenge test set, ranking 6th overall.


<details>
  <summary>Details</summary>
Motivation: Addressing real-time tumor tracking in cine-MRI sequences under data scarcity constraints.

Method: Utilizing SAM 2.1 with mask-based prompts from the first annotated slice, fine-tuned with specific training configurations to minimize overfitting.

Result: Achieved a Dice score of 0.8794 on the hidden test set, ranking 6th overall in the TrackRAD2025 challenge.

Conclusion: Foundation models show strong potential for accurate and real-time tumor tracking in MRI-guided radiotherapy.

Abstract: In this work, we address the TrackRAD2025 challenge of real-time tumor
tracking in cine-MRI sequences of the thoracic and abdominal regions under
strong data scarcity constraints. Two complementary strategies were explored:
(i) unsupervised registration with the IMPACT similarity metric and (ii)
foundation model-based segmentation leveraging SAM 2.1 and its recent variants
through prompt-based interaction. Due to the one-second runtime constraint, the
SAM-based method was ultimately selected. The final configuration used SAM2.1
b+ with mask-based prompts from the first annotated slice, fine-tuned solely on
the small labeled subset from TrackRAD2025. Training was configured to minimize
overfitting, using 1024x1024 patches (batch size 1), standard augmentations,
and a balanced Dice + IoU loss. A low uniform learning rate (0.0001) was
applied to all modules (prompt encoder, decoder, Hiera backbone) to preserve
generalization while adapting to annotator-specific styles. Training lasted 300
epochs (~12h on RTX A6000, 48GB). The same inference strategy was consistently
applied across all anatomical sites and MRI field strengths. Test-time
augmentation was considered but ultimately discarded due to negligible
performance gains. The final model was selected based on the highest Dice
Similarity Coefficient achieved on the validation set after fine-tuning. On the
hidden test set, the model reached a Dice score of 0.8794, ranking 6th overall
in the TrackRAD2025 challenge. These results highlight the strong potential of
foundation models for accurate and real-time tumor tracking in MRI-guided
radiotherapy.

</details>


### [57] [Larger Hausdorff Dimension in Scanning Pattern Facilitates Mamba-Based Methods in Low-Light Image Enhancement](https://arxiv.org/abs/2510.26001)
*Xinhua Wang,Caibo Feng,Xiangjun Fu,Chunxiao Liu*

Main category: cs.CV

TL;DR: 提出了一种新的Hilbert选择性扫描机制，增加了Mamba框架扫描模式的Hausdorff维度。


<details>
  <summary>Details</summary>
Motivation: 为了更有效地探索特征空间，捕捉复杂的精细尺度细节，并提高整体覆盖率。

Method: 通过Hilbert选择性扫描机制增加Mamba框架扫描模式的Hausdorff维度。

Result: 在公共基准上进行的实验表明，该方法显著提高了现有基于Mamba的低光图像增强方法的定量指标和定性视觉保真度，同时减少了计算资源消耗并缩短了推理时间。

Conclusion: 这种改进的策略不仅推进了低光图像增强的最新技术，而且在利用基于Mamba技术的领域中也具有更广泛的应用前景。

Abstract: We propose an innovative enhancement to the Mamba framework by increasing the
Hausdorff dimension of its scanning pattern through a novel Hilbert Selective
Scan mechanism. This mechanism explores the feature space more effectively,
capturing intricate fine-scale details and improving overall coverage. As a
result, it mitigates information inconsistencies while refining spatial
locality to better capture subtle local interactions without sacrificing the
model's ability to handle long-range dependencies. Extensive experiments on
publicly available benchmarks demonstrate that our approach significantly
improves both the quantitative metrics and qualitative visual fidelity of
existing Mamba-based low-light image enhancement methods, all while reducing
computational resource consumption and shortening inference time. We believe
that this refined strategy not only advances the state-of-the-art in low-light
image enhancement but also holds promise for broader applications in fields
that leverage Mamba-based techniques.

</details>


### [58] [CAVE: Detecting and Explaining Commonsense Anomalies in Visual Environments](https://arxiv.org/abs/2510.26006)
*Rishika Bhagwatkar,Syrielle Montariol,Angelika Romanou,Beatriz Borges,Irina Rish,Antoine Bosselut*

Main category: cs.CV

TL;DR: 提出了一个真实世界视觉异常的基准测试CAVE，用于评估视觉语言模型在异常检测和常识推理方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的异常检测方法无法捕捉真实世界异常的丰富性和不可预测性。

Method: 构建了CAVE基准测试，支持异常描述、解释和理由三个开放式任务，并提供细粒度的标注。

Result: 表明当前最好的视觉语言模型在视觉异常感知和常识推理方面表现不佳。

Conclusion: CAVE作为一个真实且具有认知基础的基准，可以促进视觉语言模型在异常检测和常识推理方面的研究。

Abstract: Humans can naturally identify, reason about, and explain anomalies in their
environment. In computer vision, this long-standing challenge remains limited
to industrial defects or unrealistic, synthetically generated anomalies,
failing to capture the richness and unpredictability of real-world anomalies.
In this work, we introduce CAVE, the first benchmark of real-world visual
anomalies. CAVE supports three open-ended tasks: anomaly description,
explanation, and justification; with fine-grained annotations for visual
grounding and categorizing anomalies based on their visual manifestations,
their complexity, severity, and commonness. These annotations draw inspiration
from cognitive science research on how humans identify and resolve anomalies,
providing a comprehensive framework for evaluating Vision-Language Models
(VLMs) in detecting and understanding anomalies. We show that state-of-the-art
VLMs struggle with visual anomaly perception and commonsense reasoning, even
with advanced prompting strategies. By offering a realistic and cognitively
grounded benchmark, CAVE serves as a valuable resource for advancing research
in anomaly detection and commonsense reasoning in VLMs.

</details>


### [59] [Climate Adaptation-Aware Flood Prediction for Coastal Cities Using Deep Learning](https://arxiv.org/abs/2510.26017)
*Bilal Hassan,Areg Karapetyan,Aaron Chung Hin Chow,Samer Madanat*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的、轻量级的卷积神经网络（CNN）模型，用于预测在不同的海平面上升预测和海岸线适应情景下的沿海洪水。


<details>
  <summary>Details</summary>
Motivation: 传统物理水动力模拟器虽然精确，但计算成本高昂，不适合城市规模的沿海规划应用。深度学习虽然有前景，但受到数据稀缺和高维输出要求的限制。

Method: 利用一种基于视觉的、低资源的深度学习框架，开发了一种新的、轻量级的 CNN 模型。

Result: 该模型在预测洪水深度图方面的平均绝对误差（MAE）降低了近 20%，显著优于现有方法。

Conclusion: 该方法有潜力成为沿海洪水管理的可扩展和实用的工具，使决策者能够制定有效的缓解策略，以应对气候变化日益增长的影响。

Abstract: Climate change and sea-level rise (SLR) pose escalating threats to coastal
cities, intensifying the need for efficient and accurate methods to predict
potential flood hazards. Traditional physics-based hydrodynamic simulators,
although precise, are computationally expensive and impractical for city-scale
coastal planning applications. Deep Learning (DL) techniques offer promising
alternatives, however, they are often constrained by challenges such as data
scarcity and high-dimensional output requirements. Leveraging a recently
proposed vision-based, low-resource DL framework, we develop a novel,
lightweight Convolutional Neural Network (CNN)-based model designed to predict
coastal flooding under variable SLR projections and shoreline adaptation
scenarios. Furthermore, we demonstrate the ability of the model to generalize
across diverse geographical contexts by utilizing datasets from two distinct
regions: Abu Dhabi and San Francisco. Our findings demonstrate that the
proposed model significantly outperforms state-of-the-art methods, reducing the
mean absolute error (MAE) in predicted flood depth maps on average by nearly
20%. These results highlight the potential of our approach to serve as a
scalable and practical tool for coastal flood management, empowering
decision-makers to develop effective mitigation strategies in response to the
growing impacts of climate change. Project Page: https://caspiannet.github.io/

</details>


### [60] [Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders](https://arxiv.org/abs/2510.26027)
*Ali Rasekh,Erfan Bagheri Soula,Omid Daliran,Simon Gottschalk,Mohsen Fayyaz*

Main category: cs.CV

TL;DR: 现有的视频大语言模型在理解视频中的复杂时间动态方面存在局限性。


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型在时间理解方面存在严重限制，难以完成需要详细理解动作序列和时间进展的任务。

Method: 提出了一种新的视频大语言模型架构，该架构在视觉编码器中引入了堆叠的时间注意力模块，在视觉编码器中加入了时间注意力，使模型能够在将视觉tokens传递给LLM之前，更好地捕捉动作的进展和帧之间的关系。

Result: 该方法显著提高了时间推理能力，并在视频问答任务中优于现有模型，特别是在动作识别方面。在VITATECS、MVBench和Video-MME等基准测试中，性能提高了高达+5.5%。

Conclusion: 通过使用时间结构增强视觉编码器，解决了视频大语言模型在视频理解方面的一个关键缺口。

Abstract: Despite significant advances in Multimodal Large Language Models (MLLMs),
understanding complex temporal dynamics in videos remains a major challenge.
Our experiments show that current Video Large Language Model (Video-LLM)
architectures have critical limitations in temporal understanding, struggling
with tasks that require detailed comprehension of action sequences and temporal
progression. In this work, we propose a Video-LLM architecture that introduces
stacked temporal attention modules directly within the vision encoder. This
design incorporates a temporal attention in vision encoder, enabling the model
to better capture the progression of actions and the relationships between
frames before passing visual tokens to the LLM. Our results show that this
approach significantly improves temporal reasoning and outperforms existing
models in video question answering tasks, specifically in action recognition.
We improve on benchmarks including VITATECS, MVBench, and Video-MME by up to
+5.5%. By enhancing the vision encoder with temporal structure, we address a
critical gap in video understanding for Video-LLMs. Project page and code are
available at: https://alirasekh.github.io/STAVEQ2/.

</details>


### [61] [FlexICL: A Flexible Visual In-context Learning Framework for Elbow and Wrist Ultrasound Segmentation](https://arxiv.org/abs/2510.26049)
*Yuyue Zhou,Jessica Knight,Shrimanti Ghosh,Banafshe Felfeliyan,Jacob L. Jaremko,Abhilash R. Hareendranathan*

Main category: cs.CV

TL;DR: 提出了一种新颖的上下文学习框架 FlexICL，用于分割超声图像中的骨骼区域，尤其是在标记数据稀缺的情况下。


<details>
  <summary>Details</summary>
Motivation: 儿童肘部和腕部骨折是最常见的骨折，而超声中骨骼结构的自动分割可以提高诊断准确性和治疗计划，但像素级专家注释非常耗时和昂贵。

Method: 提出了 FlexICL，一种灵活的上下文学习框架，用于分割超声图像中的骨骼区域。该方法研究了各种图像连接技术和训练策略，并引入了新的连接方法。

Result: FlexICL 仅需 5% 的训练图像，在四个腕部和肘部超声数据集上实现了稳健的分割性能，并且优于其他模型。

Conclusion: FlexICL 作为一种高效且可扩展的超声图像分割解决方案具有潜力，非常适合标记数据稀缺的医学成像用例。

Abstract: Elbow and wrist fractures are the most common fractures in pediatric
populations. Automatic segmentation of musculoskeletal structures in ultrasound
(US) can improve diagnostic accuracy and treatment planning. Fractures appear
as cortical defects but require expert interpretation. Deep learning (DL) can
provide real-time feedback and highlight key structures, helping lightly
trained users perform exams more confidently. However, pixel-wise expert
annotations for training remain time-consuming and costly. To address this
challenge, we propose FlexICL, a novel and flexible in-context learning (ICL)
framework for segmenting bony regions in US images. We apply it to an
intra-video segmentation setting, where experts annotate only a small subset of
frames, and the model segments unseen frames. We systematically investigate
various image concatenation techniques and training strategies for visual ICL
and introduce novel concatenation methods that significantly enhance model
performance with limited labeled data. By integrating multiple augmentation
strategies, FlexICL achieves robust segmentation performance across four wrist
and elbow US datasets while requiring only 5% of the training images. It
outperforms state-of-the-art visual ICL models like Painter, MAE-VQGAN, and
conventional segmentation models like U-Net and TransUNet by 1-27% Dice
coefficient on 1,252 US sweeps. These initial results highlight the potential
of FlexICL as an efficient and scalable solution for US image segmentation well
suited for medical imaging use cases where labeled data is scarce.

</details>


### [62] [AdSum: Two-stream Audio-visual Summarization for Automated Video Advertisement Clipping](https://arxiv.org/abs/2510.26569)
*Wen Xie,Yanjun Zhu,Gijs Overgoor,Yakov Bart,Agata Lapedriza Garcia,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: 提出了一种使用视频摘要技术自动进行视频广告剪辑的框架，以解决手动选择和重新编辑广告的费时费力问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要手动选择和重新编辑较长视频广告中的镜头以创建较短版本，这既费力又耗时。因此，本文旨在实现视频广告的自动剪辑。

Method: 开发了一个双流音视频融合模型，该模型预测视频帧的重要性，其中重要性定义为帧在公司制作的短广告中被选择的可能性。

Result: 在包含来自真实广告活动的 102 对 30 秒和 15 秒广告的新数据集 AdSum204 上进行的大量实验表明，该模型在各种指标上优于现有技术。

Conclusion: 该模型优于当前最佳方法，证明了该框架在自动视频广告剪辑方面的有效性。

Abstract: Advertisers commonly need multiple versions of the same advertisement (ad) at
varying durations for a single campaign. The traditional approach involves
manually selecting and re-editing shots from longer video ads to create shorter
versions, which is labor-intensive and time-consuming. In this paper, we
introduce a framework for automated video ad clipping using video summarization
techniques. We are the first to frame video clipping as a shot selection
problem, tailored specifically for advertising. Unlike existing general video
summarization methods that primarily focus on visual content, our approach
emphasizes the critical role of audio in advertising. To achieve this, we
develop a two-stream audio-visual fusion model that predicts the importance of
video frames, where importance is defined as the likelihood of a frame being
selected in the firm-produced short ad. To address the lack of ad-specific
datasets, we present AdSum204, a novel dataset comprising 102 pairs of
30-second and 15-second ads from real advertising campaigns. Extensive
experiments demonstrate that our model outperforms state-of-the-art methods
across various metrics, including Average Precision, Area Under Curve,
Spearman, and Kendall.

</details>


### [63] [Dynamic VLM-Guided Negative Prompting for Diffusion Models](https://arxiv.org/abs/2510.26052)
*Hoyeon Chang,Seungjin Kim,Yoonseok Choi*

Main category: cs.CV

TL;DR: 提出了一种新的动态负面提示方法，该方法利用视觉-语言模型 (VLM) 在去噪过程中自适应生成负面提示。


<details>
  <summary>Details</summary>
Motivation: 与使用固定负面提示的传统负面提示方法不同，我们的方法在特定的去噪步骤生成中间图像预测，并查询 VLM 以生成上下文相关的负面提示。

Method: 在扩散模型中采用动态负面提示，利用视觉-语言模型 (VLM) 自适应生成负面提示。

Result: 在各种基准数据集上评估了我们的方法，并证明了负面引导强度和文本-图像对齐之间的权衡。

Conclusion: 该方法在负面引导强度和文本-图像对齐之间取得了平衡。

Abstract: We propose a novel approach for dynamic negative prompting in diffusion
models that leverages Vision-Language Models (VLMs) to adaptively generate
negative prompts during the denoising process. Unlike traditional Negative
Prompting methods that use fixed negative prompts, our method generates
intermediate image predictions at specific denoising steps and queries a VLM to
produce contextually appropriate negative prompts. We evaluate our approach on
various benchmark datasets and demonstrate the trade-offs between negative
guidance strength and text-image alignment.

</details>


### [64] [Security Risk of Misalignment between Text and Image in Multi-modal Model](https://arxiv.org/abs/2510.26105)
*Xiaosen Wang,Zhijin Ge,Shaokang Wang*

Main category: cs.CV

TL;DR: 多模态扩散模型容易受到对抗性输入的影响，并且文本和图像模态之间的对齐不足，从而导致生成不当内容。本文提出了一种名为PReMA的新型攻击，该攻击通过修改输入图像来操纵生成的内容，而无需更改prompt本身。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型中的文本和图像模态之间的对齐不足，从而导致生成不当内容，例如NSFW内容。与之前的研究不同，之前的研究主要生成对抗性prompt来生成NSFW内容，而这项研究着重于通过对抗图像来实现。

Method: 提出了一种名为Prompt-Restricted Multi-modal Attack (PReMA) 的新型攻击，该攻击通过修改输入图像来操纵生成的内容，在不改变prompt本身的情况下。

Result: 在图像修复和风格转换任务上的综合评估证实了PReMA的有效性。

Conclusion: PReMA对多模态扩散模型的完整性构成了新的威胁，特别是在使用固定prompt的图像编辑应用程序中。

Abstract: Despite the notable advancements and versatility of multi-modal diffusion
models, such as text-to-image models, their susceptibility to adversarial
inputs remains underexplored. Contrary to expectations, our investigations
reveal that the alignment between textual and Image modalities in existing
diffusion models is inadequate. This misalignment presents significant risks,
especially in the generation of inappropriate or Not-Safe-For-Work (NSFW)
content. To this end, we propose a novel attack called Prompt-Restricted
Multi-modal Attack (PReMA) to manipulate the generated content by modifying the
input image in conjunction with any specified prompt, without altering the
prompt itself. PReMA is the first attack that manipulates model outputs by
solely creating adversarial images, distinguishing itself from prior methods
that primarily generate adversarial prompts to produce NSFW content.
Consequently, PReMA poses a novel threat to the integrity of multi-modal
diffusion models, particularly in image-editing applications that operate with
fixed prompts. Comprehensive evaluations conducted on image inpainting and
style transfer tasks across various models confirm the potent efficacy of
PReMA.

</details>


### [65] [EgoExo-Con: Exploring View-Invariant Video Temporal Understanding](https://arxiv.org/abs/2510.26113)
*Minjoon Jung,Junbin Xiao,Junghyun Kim,Byoung-Tak Zhang,Angela Yao*

Main category: cs.CV

TL;DR: 论文研究了视频大语言模型在不同视角下对同一事件的时间理解一致性问题，并提出了一个名为EgoExo-Con的基准测试。


<details>
  <summary>Details</summary>
Motivation: 研究动机是现有视频大语言模型在处理来自不同视角的同一事件时，缺乏时间理解上的一致性。

Method: 提出了一个名为View-GRPO的强化学习框架，以增强特定视角的时间推理能力，并鼓励跨视角的一致性理解。

Result: 实验结果表明，现有视频大语言模型在跨视角一致性方面表现不佳，而提出的View-GRPO方法优于naive SFT和GRPO方法，尤其是在提高跨视角一致性方面。

Conclusion: 论文揭示了现有视频大语言模型在时间理解一致性方面的局限性，并提出了一种有效的改进方法。

Abstract: Can Video-LLMs achieve consistent temporal understanding when videos capture
the same event from different viewpoints? To study this, we introduce
EgoExo-Con (Consistency), a benchmark of comprehensively synchronized
egocentric and exocentric video pairs with human-refined queries in natural
language. EgoExo-Con emphasizes two temporal understanding tasks: Temporal
Verification and Temporal Grounding. It evaluates not only correctness but
consistency across viewpoints. Our analysis reveals two critical limitations of
existing Video-LLMs: (1) models often fail to maintain consistency, with
results far worse than their single-view performances. (2) When naively
finetuned with synchronized videos of both viewpoints, the models show improved
consistency but often underperform those trained on a single view. For
improvements, we propose View-GRPO, a novel reinforcement learning framework
that effectively strengthens view-specific temporal reasoning while encouraging
consistent comprehension across viewpoints. Our method demonstrates its
superiority over naive SFT and GRPO, especially for improving cross-view
consistency. All resources will be made publicly available.

</details>


### [66] [OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script Research](https://arxiv.org/abs/2510.26114)
*Caoshuo Li,Zengmao Ding,Xiaobin Hu,Bang Li,Donghao Luo,Xu Peng,Taisong Jin,Yongge Liu,Shengwei Han,Jing Yang,Xiaoping He,Feng Gao,AndyPian Wu,SevenShu,Chaoyang Wang,Chengjie Wang*

Main category: cs.CV

TL;DR: OracleAgent是一个用于甲骨文信息管理和检索的智能体系统，集成了多种分析工具，并构建了一个包含140万字符拓片图像和8万条释文的领域知识库。


<details>
  <summary>Details</summary>
Motivation: 当前甲骨文研究面临解释流程复杂、信息组织和检索效率低下的挑战。

Method: 构建了首个甲骨文智能体系统OracleAgent，该系统集成了多种甲骨文分析工具，并利用大型语言模型进行驱动。此外，构建了一个全面的领域特定多模态知识库。

Result: OracleAgent在多模态推理和生成任务中表现优异，超越了主流多模态大型语言模型（如GPT-4o）。案例研究表明，OracleAgent可以有效协助领域专家，显著降低甲骨文研究的时间成本。

Conclusion: OracleAgent是甲骨文辅助研究和自动释读系统迈向实际部署的重要一步。

Abstract: As one of the earliest writing systems, Oracle Bone Script (OBS) preserves
the cultural and intellectual heritage of ancient civilizations. However,
current OBS research faces two major challenges: (1) the interpretation of OBS
involves a complex workflow comprising multiple serial and parallel sub-tasks,
and (2) the efficiency of OBS information organization and retrieval remains a
critical bottleneck, as scholars often spend substantial effort searching for,
compiling, and managing relevant resources. To address these challenges, we
present OracleAgent, the first agent system designed for the structured
management and retrieval of OBS-related information. OracleAgent seamlessly
integrates multiple OBS analysis tools, empowered by large language models
(LLMs), and can flexibly orchestrate these components. Additionally, we
construct a comprehensive domain-specific multimodal knowledge base for OBS,
which is built through a rigorous multi-year process of data collection,
cleaning, and expert annotation. The knowledge base comprises over 1.4M
single-character rubbing images and 80K interpretation texts. OracleAgent
leverages this resource through its multimodal tools to assist experts in
retrieval tasks of character, document, interpretation text, and rubbing image.
Extensive experiments demonstrate that OracleAgent achieves superior
performance across a range of multimodal reasoning and generation tasks,
surpassing leading mainstream multimodal large language models (MLLMs) (e.g.,
GPT-4o). Furthermore, our case study illustrates that OracleAgent can
effectively assist domain experts, significantly reducing the time cost of OBS
research. These results highlight OracleAgent as a significant step toward the
practical deployment of OBS-assisted research and automated interpretation
systems.

</details>


### [67] [JOGS: Joint Optimization of Pose Estimation and 3D Gaussian Splatting](https://arxiv.org/abs/2510.26117)
*Yuxuan Li,Tao Wang,Xianben Yang*

Main category: cs.CV

TL;DR: 提出了一种联合优化3D高斯点和相机姿态的统一框架，无需预先校准的输入。


<details>
  <summary>Details</summary>
Motivation: 传统的novel view synthesis方法严重依赖外部相机姿态估计工具（如COLMAP），这通常会引入计算瓶颈并传播误差。

Method: 通过新颖的协同优化策略迭代地细化3D高斯参数并更新相机姿态，确保同时改进场景重建保真度和姿态精度。将联合优化解耦为两个交错的阶段：首先，通过具有固定姿态的可微渲染更新3D高斯参数；其次，使用定制的3D光流算法（包含几何和光度约束）细化相机姿态。

Result: 在多个数据集上的大量评估表明，该方法在重建质量方面显著优于现有的无COLMAP技术，并且在总体上也超过了标准的基于COLMAP的基线。

Conclusion: 该方法逐步减少了投影误差，特别是在具有大视点变化和稀疏特征分布的具有挑战性的场景中，在这些场景中，传统方法步履蹒跚。

Abstract: Traditional novel view synthesis methods heavily rely on external camera pose
estimation tools such as COLMAP, which often introduce computational
bottlenecks and propagate errors. To address these challenges, we propose a
unified framework that jointly optimizes 3D Gaussian points and camera poses
without requiring pre-calibrated inputs. Our approach iteratively refines 3D
Gaussian parameters and updates camera poses through a novel co-optimization
strategy, ensuring simultaneous improvements in scene reconstruction fidelity
and pose accuracy. The key innovation lies in decoupling the joint optimization
into two interleaved phases: first, updating 3D Gaussian parameters via
differentiable rendering with fixed poses, and second, refining camera poses
using a customized 3D optical flow algorithm that incorporates geometric and
photometric constraints. This formulation progressively reduces projection
errors, particularly in challenging scenarios with large viewpoint variations
and sparse feature distributions, where traditional methods struggle. Extensive
evaluations on multiple datasets demonstrate that our approach significantly
outperforms existing COLMAP-free techniques in reconstruction quality, and also
surpasses the standard COLMAP-based baseline in general.

</details>


### [68] [WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios](https://arxiv.org/abs/2510.26125)
*Runsheng Xu,Hubert Lin,Wonseok Jeon,Hao Feng,Yuliang Zou,Liting Sun,John Gorman,Kate Tolstaya,Sarah Tang,Brandyn White,Ben Sapp,Mingxing Tan,Jyh-Jing Hwang,Drago Anguelov*

Main category: cs.CV

TL;DR: 提出了一个新的自动驾驶数据集WOD-E2E，专注于长尾场景，并提出了一个新的评估指标RFS。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端驾驶基准测试主要集中在常见场景，未能充分测试系统的潜力。现有的开环评估指标无法捕捉驾驶的多模态特性或有效评估长尾场景的性能。

Method: 构建了包含4021个驾驶片段的WOD-E2E数据集，该数据集包含高层路由信息、车辆状态和来自8个环绕摄像头的360度相机视图。提出了RFS指标，通过测量预测轨迹与评分员标注的轨迹偏好标签的匹配程度来评估性能。

Result: 发布了WOD-E2E验证集片段的评分员偏好标签，测试集标签用于2025 WOD-E2E挑战赛。

Conclusion: 旨在促进对能够处理复杂现实场景的通用、鲁棒和安全的端到端自动驾驶代理的研究。

Abstract: Vision-based end-to-end (E2E) driving has garnered significant interest in
the research community due to its scalability and synergy with multimodal large
language models (MLLMs). However, current E2E driving benchmarks primarily
feature nominal scenarios, failing to adequately test the true potential of
these systems. Furthermore, existing open-loop evaluation metrics often fall
short in capturing the multi-modal nature of driving or effectively evaluating
performance in long-tail scenarios. To address these gaps, we introduce the
Waymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021
driving segments (approximately 12 hours), specifically curated for challenging
long-tail scenarios that that are rare in daily life with an occurring
frequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the
high-level routing information, ego states, and 360-degree camera views from 8
surrounding cameras. To evaluate the E2E driving performance on these long-tail
situations, we propose a novel open-loop evaluation metric: Rater Feedback
Score (RFS). Unlike conventional metrics that measure the distance between
predicted way points and the logs, RFS measures how closely the predicted
trajectory matches rater-annotated trajectory preference labels. We have
released rater preference labels for all WOD-E2E validation set segments, while
the held out test set labels have been used for the 2025 WOD-E2E Challenge.
Through our work, we aim to foster state of the art research into
generalizable, robust, and safe end-to-end autonomous driving agents capable of
handling complex real-world situations.

</details>


### [69] [Exploring Object-Aware Attention Guided Frame Association for RGB-D SLAM](https://arxiv.org/abs/2510.26131)
*Ali Caglayan,Nevrez Imamoglu,Oguzhan Guclu,Ali Osman Serhatoglu,Ahmet Burak Can,Ryosuke Nakamura*

Main category: cs.CV

TL;DR: 本文提出了一种利用任务相关的网络注意力机制进行RGB-D室内SLAM的方法，通过将网络梯度导出的分层注意力信息与CNN特征表示相结合，以提高帧关联性能。


<details>
  <summary>Details</summary>
Motivation: 将基于梯度的注意力信息直接整合到CNN表示中以进行语义对象理解的方法仍然有限。这种整合对于视觉任务（如同时定位和映射（SLAM））特别有益，在这些任务中，富含空间注意对象位置的CNN表示可以提高性能。

Method: 将从网络梯度导出的分层注意力信息与CNN特征表示相结合。

Result: 与基线方法相比，性能有所提高，尤其是在大型环境中。

Conclusion: 利用任务相关的网络注意力机制进行RGB-D室内SLAM可以提高帧关联性能，尤其是在大型环境中。

Abstract: Attention models have recently emerged as a powerful approach, demonstrating
significant progress in various fields. Visualization techniques, such as class
activation mapping, provide visual insights into the reasoning of convolutional
neural networks (CNNs). Using network gradients, it is possible to identify
regions where the network pays attention during image recognition tasks.
Furthermore, these gradients can be combined with CNN features to localize more
generalizable, task-specific attentive (salient) regions within scenes.
However, explicit use of this gradient-based attention information integrated
directly into CNN representations for semantic object understanding remains
limited. Such integration is particularly beneficial for visual tasks like
simultaneous localization and mapping (SLAM), where CNN representations
enriched with spatially attentive object locations can enhance performance. In
this work, we propose utilizing task-specific network attention for RGB-D
indoor SLAM. Specifically, we integrate layer-wise attention information
derived from network gradients with CNN feature representations to improve
frame association performance. Experimental results indicate improved
performance compared to baseline methods, particularly for large environments.

</details>


### [70] [FullPart: Generating each 3D Part at Full Resolution](https://arxiv.org/abs/2510.26140)
*Lihe Ding,Shaocong Dong,Yaokun Li,Chenjian Gao,Xiao Chen,Rui Han,Yihao Kuang,Hong Zhang,Bo Huang,Zhanpeng Huang,Zibin Wang,Dan Xu,Tianfan Xue*

Main category: cs.CV

TL;DR: FullPart结合了隐式和显式方法，用于生成具有精细几何细节的3D部件，并使用中心点编码策略来保持全局一致性。此外，论文还发布了一个大型人工标注的3D部件数据集PartVerse-XL。


<details>
  <summary>Details</summary>
Motivation: 以往的部件生成器在表示部件时，要么几何细节不足（隐式向量集），要么小部件质量下降（全局体素网格）。

Method: FullPart首先通过隐式扩散过程生成bounding box布局，然后为每个部件生成其自身固定全分辨率的体素网格，并引入中心点编码策略。

Result: FullPart在3D部件生成方面取得了state-of-the-art的结果。

Conclusion: 论文提出了FullPart框架，并发布了PartVerse-XL数据集，以促进3D部件生成方面的未来研究。

Abstract: Part-based 3D generation holds great potential for various applications.
Previous part generators that represent parts using implicit vector-set tokens
often suffer from insufficient geometric details. Another line of work adopts
an explicit voxel representation but shares a global voxel grid among all
parts; this often causes small parts to occupy too few voxels, leading to
degraded quality. In this paper, we propose FullPart, a novel framework that
combines both implicit and explicit paradigms. It first derives the bounding
box layout through an implicit box vector-set diffusion process, a task that
implicit diffusion handles effectively since box tokens contain little
geometric detail. Then, it generates detailed parts, each within its own fixed
full-resolution voxel grid. Instead of sharing a global low-resolution space,
each part in our method - even small ones - is generated at full resolution,
enabling the synthesis of intricate details. We further introduce a
center-point encoding strategy to address the misalignment issue when
exchanging information between parts of different actual sizes, thereby
maintaining global coherence. Moreover, to tackle the scarcity of reliable part
data, we present PartVerse-XL, the largest human-annotated 3D part dataset to
date with 40K objects and 320K parts. Extensive experiments demonstrate that
FullPart achieves state-of-the-art results in 3D part generation. We will
release all code, data, and model to benefit future research in 3D part
generation.

</details>


### [71] [BasicAVSR: Arbitrary-Scale Video Super-Resolution via Image Priors and Enhanced Motion Compensation](https://arxiv.org/abs/2510.26149)
*Wei Shang,Wanying Zhang,Shuhang Gu,Pengfei Zhu,Qinghua Hu,Dongwei Ren*

Main category: cs.CV

TL;DR: 本文提出了一种名为 BasicAVSR 的任意比例视频超分辨率(AVSR)的强大基线模型，该模型集成了自适应多尺度频率先验、光流引导的传播单元、二阶运动补偿单元和超采样单元。


<details>
  <summary>Details</summary>
Motivation: AVSR 旨在提高视频帧的分辨率，可能在各种缩放因子下进行，这在空间细节再现、时间一致性和计算复杂性方面提出了一些挑战。

Method: 该模型集成了四个关键组件：1) 自适应多尺度频率先验，2) 光流引导的传播单元，3) 二阶运动补偿单元，以及 4) 超采样单元。为了满足不同的应用需求，作者实例化了三个传播变体：(i) 用于严格在线推理的单向 RNN 单元，(ii) 具有有限前瞻性的单向 RNN 单元，以及 (iii) 为离线任务设计的双向 RNN 单元。

Result: 实验结果表明，该模型在不同场景中都有效且具有适应性。通过大量实验，BasicAVSR 在超分辨率质量、泛化能力和推理速度方面都显著优于现有方法。

Conclusion: 该工作不仅推进了 AVSR 的技术水平，而且将其核心组件扩展到多个框架，适用于不同的场景。

Abstract: Arbitrary-scale video super-resolution (AVSR) aims to enhance the resolution
of video frames, potentially at various scaling factors, which presents several
challenges regarding spatial detail reproduction, temporal consistency, and
computational complexity. In this paper, we propose a strong baseline BasicAVSR
for AVSR by integrating four key components: 1) adaptive multi-scale frequency
priors generated from image Laplacian pyramids, 2) a flow-guided propagation
unit to aggregate spatiotemporal information from adjacent frames, 3) a
second-order motion compensation unit for more accurate spatial alignment of
adjacent frames, and 4) a hyper-upsampling unit to generate scale-aware and
content-independent upsampling kernels. To meet diverse application demands, we
instantiate three propagation variants: (i) a unidirectional RNN unit for
strictly online inference, (ii) a unidirectional RNN unit empowered with a
limited lookahead that tolerates a small output delay, and (iii) a
bidirectional RNN unit designed for offline tasks where computational resources
are less constrained. Experimental results demonstrate the effectiveness and
adaptability of our model across these different scenarios. Through extensive
experiments, we show that BasicAVSR significantly outperforms existing methods
in terms of super-resolution quality, generalization ability, and inference
speed. Our work not only advances the state-of-the-art in AVSR but also extends
its core components to multiple frameworks for diverse scenarios. The code is
available at https://github.com/shangwei5/BasicAVSR.

</details>


### [72] [MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction](https://arxiv.org/abs/2510.26151)
*Shunjie-Fabian Zheng,Hyeonjun Lee,Thijs Kooi,Ali Diba*

Main category: cs.CV

TL;DR: 提出了一种新的多视角乳腺钼靶和语言模型（MV-MLM），用于乳腺癌分类和风险预测，该模型利用多视角监督和跨模态自监督，在图像-文本对上进行训练，以学习丰富的表示。


<details>
  <summary>Details</summary>
Motivation: 获取带有精细注释的大型数据集对于训练稳健的计算机辅助诊断（CAD）模型至关重要，但是，获取此类数据集既昂贵又耗时。Vision-Language模型（VLMs）通过提高医学成像任务中的鲁棒性和数据效率，提供了一种有希望的解决方案。

Method: 该MV-MLM模型利用多视角监督，通过跨模态自监督学习图像-文本对的丰富表示。提出了一种新的联合视觉-文本学习策略，以提高泛化性和准确性，从而区分乳腺组织或癌症特征（钙化、肿块），并利用这些模式来理解乳腺钼靶图像并预测癌症风险。

Result: 在私有和公共数据集上的评估表明，该模型在恶性肿瘤分类、亚型分类和基于图像的癌症风险预测这三个分类任务中均达到了最先进的性能。此外，该模型还表现出强大的数据效率，在合成文本报告上进行训练，而无需实际的放射学报告，即可胜过现有的完全监督或VLM基线。

Conclusion: 该研究提出了一种有效的乳腺癌分类和风险预测模型，该模型具有强大的数据效率和泛化能力，有望在实际应用中发挥重要作用。

Abstract: Large annotated datasets are essential for training robust Computer-Aided
Diagnosis (CAD) models for breast cancer detection or risk prediction. However,
acquiring such datasets with fine-detailed annotation is both costly and
time-consuming. Vision-Language Models (VLMs), such as CLIP, which are
pre-trained on large image-text pairs, offer a promising solution by enhancing
robustness and data efficiency in medical imaging tasks. This paper introduces
a novel Multi-View Mammography and Language Model for breast cancer
classification and risk prediction, trained on a dataset of paired mammogram
images and synthetic radiology reports. Our MV-MLM leverages multi-view
supervision to learn rich representations from extensive radiology data by
employing cross-modal self-supervision across image-text pairs. This includes
multiple views and the corresponding pseudo-radiology reports. We propose a
novel joint visual-textual learning strategy to enhance generalization and
accuracy performance over different data types and tasks to distinguish breast
tissues or cancer characteristics(calcification, mass) and utilize these
patterns to understand mammography images and predict cancer risk. We evaluated
our method on both private and publicly available datasets, demonstrating that
the proposed model achieves state-of-the-art performance in three
classification tasks: (1) malignancy classification, (2) subtype
classification, and (3) image-based cancer risk prediction. Furthermore, the
model exhibits strong data efficiency, outperforming existing fully supervised
or VLM baselines while trained on synthetic text reports and without the need
for actual radiology reports.

</details>


### [73] [Detecting Unauthorized Vehicles using Deep Learning for Smart Cities: A Case Study on Bangladesh](https://arxiv.org/abs/2510.26154)
*Sudipto Das Sukanto,Diponker Roy,Fahim Shakil,Nirjhar Singha,Abdullah Asik,Aniket Joarder,Mridha Md Nafis Fuad,Muhammad Ibrahim*

Main category: cs.CV

TL;DR: 本文提出了一种基于机器学习的方法，用于在交通图像中自动检测自动人力车，使用了YOLOv8模型，并在包含1730个带注释图像的数据集上进行了训练。


<details>
  <summary>Details</summary>
Motivation: 由于自动人力车与其他车辆相似，并且交通规则限制其进入某些路线，因此需要对其进行监控。现有的监控系统难以有效识别，而手动视频分析又耗时。

Method: 使用YOLOv8模型进行实时目标检测，并准备了一个包含1730个带注释图像的数据集进行训练。

Result: 该模型在实时自动人力车检测中表现良好，mAP50达到83.447%，二元精度和召回率均高于78%。

Conclusion: 该模型在密集和稀疏交通场景中均表现出有效性，并且该数据集已公开发布以供进一步研究。

Abstract: Modes of transportation vary across countries depending on geographical
location and cultural context. In South Asian countries rickshaws are among the
most common means of local transport. Based on their mode of operation,
rickshaws in cities across Bangladesh can be broadly classified into non-auto
(pedal-powered) and auto-rickshaws (motorized). Monitoring the movement of
auto-rickshaws is necessary as traffic rules often restrict auto-rickshaws from
accessing certain routes. However, existing surveillance systems make it quite
difficult to monitor them due to their similarity to other vehicles, especially
non-auto rickshaws whereas manual video analysis is too time-consuming. This
paper presents a machine learning-based approach to automatically detect
auto-rickshaws in traffic images. In this system, we used real-time object
detection using the YOLOv8 model. For training purposes, we prepared a set of
1,730 annotated images that were captured under various traffic conditions. The
results show that our proposed model performs well in real-time auto-rickshaw
detection and offers an mAP50 of 83.447% and binary precision and recall values
above 78%, demonstrating its effectiveness in handling both dense and sparse
traffic scenarios. The dataset has been publicly released for further research.

</details>


### [74] [CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark](https://arxiv.org/abs/2510.26160)
*Jiaqi Wang,Xiao Yang,Kai Sun,Parth Suresh,Sanat Sharma,Adam Czyzewski,Derek Andersen,Surya Appini,Arkav Banerjee,Sajal Choudhary,Shervin Ghasemlou,Ziqiang Guan,Akil Iyer,Haidar Khan,Lingkun Kong,Roy Luo,Tiffany Ma,Zhen Qiao,David Tran,Wenfang Xu,Skyler Yeatman,Chen Zhou,Gunveer Gujral,Yinglong Xia,Shane Moon,Nicolas Scheffer,Nirav Shah,Eun Chang,Yue Liu,Florian Metze,Tammy Stark,Zhaleh Feizollahi,Andrea Jessee,Mangesh Pujari,Ahmed Aly,Babak Damavandi,Rakesh Wanga,Anuj Kumar,Rohit Patel,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CV

TL;DR: CRAG-MM: A new benchmark for Multi-Modal Retrieval-Augmented Generation (MM-RAG), designed for wearable scenarios, featuring 6.5K image-question-answer triplets and 2K multi-turn conversations.


<details>
  <summary>Details</summary>
Motivation: Lack of comprehensive benchmarks for MM-RAG, especially for wearable devices, hinders progress in the field.

Method: Introduces CRAG-MM, a dataset with egocentric images, diverse question types, and realistic challenges. It defines three tasks: single-source augmentation, multi-source augmentation, and multi-turn conversations, with corresponding retrieval corpora and APIs.

Result: Straightforward RAG approaches achieve low truthfulness (32-45%) on CRAG-MM, indicating significant room for improvement. A KDD Cup 2025 competition using the benchmark saw winning solutions improve baseline performance by 28%.

Conclusion: CRAG-MM is a valuable benchmark for advancing MM-RAG research, as demonstrated by its impact on the KDD Cup 2025 and the performance improvements achieved by participating solutions.

Abstract: Wearable devices such as smart glasses are transforming the way people
interact with their surroundings, enabling users to seek information regarding
entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG)
plays a key role in supporting such questions, yet there is still no
comprehensive benchmark for this task, especially regarding wearables
scenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG
benchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse
set of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn
conversations across 13 domains, including 6.2K egocentric images designed to
mimic captures from wearable devices. We carefully constructed the questions to
reflect real-world scenarios and challenges, including five types of
image-quality issues, six question types, varying entity popularity, differing
information dynamism, and different conversation turns. We design three tasks:
single-source augmentation, multi-source augmentation, and multi-turn
conversations -- each paired with an associated retrieval corpus and APIs for
both image-KG retrieval and webpage retrieval. Our evaluation shows that
straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM
single- and multi-turn QA, respectively, whereas state-of-the-art industry
solutions have similar quality (32%/45%), underscoring ample room for
improvement. The benchmark has hosted KDD Cup 2025, attracting about 1K
participants and 5K submissions, with winning solutions improving baseline
performance by 28%, highlighting its early impact on advancing the field.

</details>


### [75] [MoTDiff: High-resolution Motion Trajectory estimation from a single blurred image using Diffusion models](https://arxiv.org/abs/2510.26173)
*Wontae Choi,Jaelin Lee,Hyung Sup Yun,Byeungwoo Jeon,Il Yong Chun*

Main category: cs.CV

TL;DR: 本文提出了一种使用扩散模型的高分辨率运动轨迹估算框架 (MoTDiff)，旨在从单张运动模糊图像中高质量地估算高分辨率运动轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有运动信息表示质量低，粗粒且不准确。

Method: 该方法包含两个关键组件：1) 一种新的条件扩散框架，它使用从单个模糊图像中提取的多尺度特征图作为条件；2) 一种新的训练方法，可以促进精细运动轨迹的精确识别、运动路径整体形状和位置的一致估计以及沿运动轨迹的像素连通性。

Result: 实验表明，所提出的 MoTDiff 在盲图像去模糊和编码曝光摄影应用中优于现有技术。

Conclusion: MoTDiff 优于现有技术

Abstract: Accurate estimation of motion information is crucial in diverse computational
imaging and computer vision applications. Researchers have investigated various
methods to extract motion information from a single blurred image, including
blur kernels and optical flow. However, existing motion representations are
often of low quality, i.e., coarse-grained and inaccurate. In this paper, we
propose the first high-resolution (HR) Motion Trajectory estimation framework
using Diffusion models (MoTDiff). Different from existing motion
representations, we aim to estimate an HR motion trajectory with high-quality
from a single motion-blurred image. The proposed MoTDiff consists of two key
components: 1) a new conditional diffusion framework that uses multi-scale
feature maps extracted from a single blurred image as a condition, and 2) a new
training method that can promote precise identification of a fine-grained
motion trajectory, consistent estimation of overall shape and position of a
motion path, and pixel connectivity along a motion trajectory. Our experiments
demonstrate that the proposed MoTDiff can outperform state-of-the-art methods
in both blind image deblurring and coded exposure photography applications.

</details>


### [76] [ConceptScope: Characterizing Dataset Bias via Disentangled Visual Concepts](https://arxiv.org/abs/2510.26186)
*Jinho Choi,Hyesu Lim,Steffen Schneider,Jaegul Choo*

Main category: cs.CV

TL;DR: ConceptScope是一个可扩展的自动化框架，通过发现和量化人类可解释的概念来分析视觉数据集，使用在视觉基础模型表示上训练的稀疏自动编码器。


<details>
  <summary>Details</summary>
Motivation: 机器学习数据集中普遍存在数据集偏差，即数据点偏向某些概念。然而，如果没有代价高昂的细粒度属性注释，系统地识别这些偏差具有挑战性。

Method: ConceptScope基于概念与类标签的语义相关性和统计相关性，将概念分为目标、上下文和偏差类型，从而实现类级别的数据集特征描述、偏差识别和通过基于概念的子分组进行鲁棒性评估。

Result: ConceptScope捕获了广泛的视觉概念，包括对象、纹理、背景、面部属性、情绪和动作，通过与带注释的数据集进行比较进行验证。此外，概念激活产生与语义上有意义的图像区域对齐的空间归因。ConceptScope可靠地检测已知的偏差（例如，Waterbirds中的背景偏差）并发现以前未注释的偏差（例如，ImageNet中共现的对象），为数据集审计和模型诊断提供了一个实用的工具。

Conclusion: ConceptScope是一个用于分析视觉数据集的可扩展和自动化框架，它可以发现和量化人类可解释的概念，并可靠地检测已知和未知的偏差。

Abstract: Dataset bias, where data points are skewed to certain concepts, is ubiquitous
in machine learning datasets. Yet, systematically identifying these biases is
challenging without costly, fine-grained attribute annotations. We present
ConceptScope, a scalable and automated framework for analyzing visual datasets
by discovering and quantifying human-interpretable concepts using Sparse
Autoencoders trained on representations from vision foundation models.
ConceptScope categorizes concepts into target, context, and bias types based on
their semantic relevance and statistical correlation to class labels, enabling
class-level dataset characterization, bias identification, and robustness
evaluation through concept-based subgrouping. We validate that ConceptScope
captures a wide range of visual concepts, including objects, textures,
backgrounds, facial attributes, emotions, and actions, through comparisons with
annotated datasets. Furthermore, we show that concept activations produce
spatial attributions that align with semantically meaningful image regions.
ConceptScope reliably detects known biases (e.g., background bias in
Waterbirds) and uncovers previously unannotated ones (e.g, co-occurring objects
in ImageNet), offering a practical tool for dataset auditing and model
diagnostics.

</details>


### [77] [Sketch2PoseNet: Efficient and Generalized Sketch to 3D Human Pose Prediction](https://arxiv.org/abs/2510.26196)
*Li Wang,Yiyu Zhuang,Yanwen Wang,Xun Cao,Chuan Guo,Xinxin Zuo,Hao Zhu*

Main category: cs.CV

TL;DR: 提出了一种新的基于草图生成3D人体姿态的方法，通过合成数据和端到端框架，提高了姿态估计的准确性和速度。


<details>
  <summary>Details</summary>
Motivation: 现有的草图到姿态方法受限于缺乏大规模的草图-3D姿态标注数据，主要依赖于启发式规则的优化，泛化能力有限。

Method: 1. 训练一个扩散模型，从3D人体姿态投影的2D姿态合成草图图像，创建合成数据集SKEP-120K。2. 引入一个端到端的数据驱动框架，结合现有的2D姿态检测器和生成扩散先验，以及前馈神经网络进行2D姿态估计。3. 结合多个启发式损失函数，保证3D姿态和2D姿态之间的几何一致性，并保留准确的自接触。

Result: 模型在草图到姿态任务的估计准确性和速度上都显著超过了以往的模型。

Conclusion: 该方法通过合成数据和端到端框架，有效解决了草图到3D人体姿态估计中的挑战，并在准确性和速度上取得了显著提升。

Abstract: 3D human pose estimation from sketches has broad applications in computer
animation and film production. Unlike traditional human pose estimation, this
task presents unique challenges due to the abstract and disproportionate nature
of sketches. Previous sketch-to-pose methods, constrained by the lack of
large-scale sketch-3D pose annotations, primarily relied on optimization with
heuristic rules-an approach that is both time-consuming and limited in
generalizability. To address these challenges, we propose a novel approach
leveraging a "learn from synthesis" strategy. First, a diffusion model is
trained to synthesize sketch images from 2D poses projected from 3D human
poses, mimicking disproportionate human structures in sketches. This process
enables the creation of a synthetic dataset, SKEP-120K, consisting of 120k
accurate sketch-3D pose annotation pairs across various sketch styles. Building
on this synthetic dataset, we introduce an end-to-end data-driven framework for
estimating human poses and shapes from diverse sketch styles. Our framework
combines existing 2D pose detectors and generative diffusion priors for sketch
feature extraction with a feed-forward neural network for efficient 2D pose
estimation. Multiple heuristic loss functions are incorporated to guarantee
geometric coherence between the derived 3D poses and the detected 2D poses
while preserving accurate self-contacts. Qualitative, quantitative, and
subjective evaluations collectively show that our model substantially surpasses
previous ones in both estimation accuracy and speed for sketch-to-pose tasks.

</details>


### [78] [Developing a Multi-task Ensemble Geometric Deep Network for Supply Chain Sustainability and Risk Management](https://arxiv.org/abs/2510.26203)
*Mehdi Khaleghi,Nastaran Khaleghi,Sobhan Sheykhivand,Sebelan Danishvar*

Main category: cs.CV

TL;DR: 提出了一种新的几何深度网络，用于提升供应链的可持续性和效率。


<details>
  <summary>Details</summary>
Motivation: 供应链的可持续性对于控制供应链以达到最佳性能至关重要。供应链中发生的风险管理是发展网络可持续性和提高供应链性能效率的根本问题。正确的产品分类是可持续供应链中的另一个基本要素。

Method: 使用了一种新的几何深度网络来提出一个集成深度网络，即 Chebyshev 集成几何网络 (Ch-EGN)，它是一种混合卷积和几何深度学习。

Result: 在 DataCo 供应链的交付状态预测中，风险管理的平均准确率为 98.95%。在 SupplyGraph 数据库中，5 个产品组分类的可持续供应链的平均准确率为 100%，4 个产品关系分类的平均准确率为 98.07%，25 个公司关系分类的平均准确率为 92.37%。

Conclusion: 结果表明，与最先进的方法相比，该方法具有平均改进和效率。

Abstract: The sustainability of supply chain plays a key role in achieving optimal
performance in controlling the supply chain. The management of risks that occur
in a supply chain is a fundamental problem for the purpose of developing the
sustainability of the network and elevating the performance efficiency of the
supply chain. The correct classification of products is another essential
element in a sustainable supply chain. Acknowledging recent breakthroughs in
the context of deep networks, several architectural options have been deployed
to analyze supply chain datasets. A novel geometric deep network is used to
propose an ensemble deep network. The proposed Chebyshev ensemble geometric
network (Ch-EGN) is a hybrid convolutional and geometric deep learning. This
network is proposed to leverage the information dependencies in supply chain to
derive invisible states of samples in the database. The functionality of the
proposed deep network is assessed on the two different databases. The
SupplyGraph Dataset and DataCo are considered in this research. The prediction
of delivery status of DataCo supply chain is done for risk administration. The
product classification and edge classification are performed using the
SupplyGraph database to enhance the sustainability of the supply network. An
average accuracy of 98.95% is obtained for the ensemble network for risk
management. The average accuracy of 100% and 98.07% are obtained for
sustainable supply chain in terms of 5 product group classification and 4
product relation classification, respectively. The average accuracy of 92.37%
is attained for 25 company relation classification. The results confirm an
average improvement and efficiency of the proposed method compared to the
state-of-the-art approaches.

</details>


### [79] [OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation](https://arxiv.org/abs/2510.26213)
*Hengrui Kang,Zhuangcheng Gu,Zhiyuan Zhao,Zichen Wen,Bin Wang,Weijia Li,Conghui He*

Main category: cs.CV

TL;DR: 本文提出了OmniLayout-1M数据集，包含百万级别的多样化文档布局，并提出了OmniLayout-LLM模型，该模型使用两阶段Coarse-to-Fine学习范式，在多个领域的文档布局生成任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的文档布局生成研究主要集中在学术论文等Manhattan风格的结构上，缺乏对报纸、杂志等开放世界类型的研究。同时，现有方法在复杂领域表现不佳，难以连贯地排列长序列。

Method: 1. 构建了包含六种常见文档类型的百万级别数据集OmniLayout-1M。2. 提出了OmniLayout-LLM模型，采用两阶段Coarse-to-Fine学习范式，首先从OmniLayout-1M学习通用布局原则，然后将知识迁移到特定领域。

Result: 在M$^{6}$Doc数据集上的大量实验表明，该方法在多个领域取得了强大的性能，大大超过了现有的布局生成专家和最新的通用LLM。

Conclusion: 本文提出的OmniLayout-1M数据集和OmniLayout-LLM模型有效地解决了现有文档布局生成研究的局限性，并在多个领域取得了显著的性能提升。代码、模型和数据集将公开发布。

Abstract: Document AI has advanced rapidly and is attracting increasing attention. Yet,
while most efforts have focused on document layout analysis (DLA), its
generative counterpart, document layout generation, remains underexplored. A
major obstacle lies in the scarcity of diverse layouts: academic papers with
Manhattan-style structures dominate existing studies, while open-world genres
such as newspapers and magazines remain severely underrepresented. To address
this gap, we curate OmniLayout-1M, the first million-scale dataset of diverse
document layouts, covering six common document types and comprising
contemporary layouts collected from multiple sources. Moreover, since existing
methods struggle in complex domains and often fail to arrange long sequences
coherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage
Coarse-to-Fine learning paradigm: 1) learning universal layout principles from
OmniLayout-1M with coarse category definitions, and 2) transferring the
knowledge to a specific domain with fine-grained annotations. Extensive
experiments demonstrate that our approach achieves strong performance on
multiple domains in M$^{6}$Doc dataset, substantially surpassing both existing
layout generation experts and several latest general-purpose LLMs. Our code,
models, and dataset will be publicly released.

</details>


### [80] [Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models](https://arxiv.org/abs/2510.26241)
*Shiho Matta,Lis Kanashiro Pereira,Peitao Han,Fei Cheng,Shigeru Kitazawa*

Main category: cs.CV

TL;DR: 现有的视觉-语言模型(VLM)在视频中的时间信息理解不足，尤其缺乏评估。本文提出了一个简单但有效的挑战：判断时间箭头方向，即判断短视频是正向播放还是反向播放。


<details>
  <summary>Details</summary>
Motivation: 为了弥补现有VLM在理解视频时间信息方面的差距，并对这一问题进行评估。

Method: 本文构建了一个心理物理学验证的基准测试AoT-PsyPhyBENCH，用于测试VLM在自然视频中推断时间方向的能力，该基准使用了与人类相同的刺激和行为基线。

Result: 对开放权重和专有、推理和非推理VLM的全面评估表明，大多数模型的性能接近随机水平，即使是最好的模型也远落后于人类在物理不可逆过程和因果手动操作上的准确性。

Conclusion: 当前的多模态系统缺乏时间连续性和因果理解所需的归纳偏见。

Abstract: Modern vision-language models (VLMs) excel at many multimodal tasks, yet
their grasp of temporal information in video remains weak and, crucially,
under-evaluated. We probe this gap with a deceptively simple but revealing
challenge: judging the arrow of time (AoT)-whether a short clip is played
forward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated
benchmark that tests whether VLMs can infer temporal direction in natural
videos using the same stimuli and behavioral baselines established for humans.
Our comprehensive evaluation of open-weight and proprietary, reasoning and
non-reasoning VLMs reveals that most models perform near chance, and even the
best lag far behind human accuracy on physically irreversible processes (e.g.,
free fall, diffusion/explosion) and causal manual actions (division/addition)
that humans recognize almost instantly. These results highlight a fundamental
gap in current multimodal systems: while they capture rich visual-semantic
correlations, they lack the inductive biases required for temporal continuity
and causal understanding. We release the code and data for AoT-PsyPhyBENCH to
encourage further progress in the physical and temporal reasoning capabilities
of VLMs.

</details>


### [81] [Revisiting Generative Infrared and Visible Image Fusion Based on Human Cognitive Laws](https://arxiv.org/abs/2510.26268)
*Lin Guo,Xiaoqing Luo,Wei Xie,Zhancheng Zhang,Hui Li,Rui Wang,Zhenhua Feng,Xiaoning Song*

Main category: cs.CV

TL;DR: 提出了一种新颖的红外和可见图像融合方法，称为HCLFuse，该方法通过多尺度掩码调节变分瓶颈编码器提取低层次模态信息，并结合扩散模型的概率生成能力和物理定律，形成时变物理引导机制，从而提高模型感知数据内在结构的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的红外和可见图像融合方法常常面临平衡模态信息的困境，生成式融合方法通过学习数据分布来重建融合图像，但其生成能力仍然有限，模态信息选择缺乏可解释性会影响融合结果的可靠性和一致性。

Method: 设计了一个多尺度掩码调节变分瓶颈编码器，利用后验概率建模和信息分解来提取准确和简洁的低层次模态信息；结合扩散模型的概率生成能力与物理定律，形成了一种时变物理引导机制，自适应地调节不同阶段的生成过程。

Result: 在多个数据集的定性和定量评估中，该方法实现了最先进的融合性能，并显著提高了语义分割指标。

Conclusion: 该方法受到人类认知的启发，在增强结构一致性和细节质量方面具有优势。

Abstract: Existing infrared and visible image fusion methods often face the dilemma of
balancing modal information. Generative fusion methods reconstruct fused images
by learning from data distributions, but their generative capabilities remain
limited. Moreover, the lack of interpretability in modal information selection
further affects the reliability and consistency of fusion results in complex
scenarios. This manuscript revisits the essence of generative image fusion
under the inspiration of human cognitive laws and proposes a novel infrared and
visible image fusion method, termed HCLFuse. First, HCLFuse investigates the
quantification theory of information mapping in unsupervised fusion networks,
which leads to the design of a multi-scale mask-regulated variational
bottleneck encoder. This encoder applies posterior probability modeling and
information decomposition to extract accurate and concise low-level modal
information, thereby supporting the generation of high-fidelity structural
details. Furthermore, the probabilistic generative capability of the diffusion
model is integrated with physical laws, forming a time-varying physical
guidance mechanism that adaptively regulates the generation process at
different stages, thereby enhancing the ability of the model to perceive the
intrinsic structure of data and reducing dependence on data quality.
Experimental results show that the proposed method achieves state-of-the-art
fusion performance in qualitative and quantitative evaluations across multiple
datasets and significantly improves semantic segmentation metrics. This fully
demonstrates the advantages of this generative image fusion method, drawing
inspiration from human cognition, in enhancing structural consistency and
detail quality.

</details>


### [82] [Exploring Complementarity and Explainability in CNNs for Periocular Verification Across Acquisition Distances](https://arxiv.org/abs/2510.26282)
*Fernando Alonso-Fernandez,Kevin Hernandez Diaz,Jose M. Buades,Kiran Raja,Josef Bigun*

Main category: cs.CV

TL;DR: 研究了不同CNN在UBIPr数据库上不同距离的眼周验证互补性。


<details>
  <summary>Details</summary>
Motivation: 在VGGFace2大型眼部裁剪数据集上训练了三种复杂度递增的网络（SqueezeNet、MobileNetv2和ResNet50），旨在提高眼周验证的准确性。

Method: 使用余弦和chi2度量分析性能，比较不同的网络初始化，并通过逻辑回归应用分数级融合。此外，使用LIME热图和Jensen-Shannon散度来比较CNN的注意力模式。

Result: ResNet50单独表现最佳，但融合提供了显着收益，特别是当结合所有三个网络时。热图显示网络通常关注给定图像的不同区域，这解释了它们的互补性。该方法显着优于先前在UBIPr上的工作，实现了新的state-of-the-art。

Conclusion: 不同CNN在眼周验证上具有互补性，通过融合可以显著提高性能。

Abstract: We study the complementarity of different CNNs for periocular verification at
different distances on the UBIPr database. We train three architectures of
increasing complexity (SqueezeNet, MobileNetv2, and ResNet50) on a large set of
eye crops from VGGFace2. We analyse performance with cosine and chi2 metrics,
compare different network initialisations, and apply score-level fusion via
logistic regression. In addition, we use LIME heatmaps and Jensen-Shannon
divergence to compare attention patterns of the CNNs. While ResNet50
consistently performs best individually, the fusion provides substantial gains,
especially when combining all three networks. Heatmaps show that networks
usually focus on distinct regions of a given image, which explains their
complementarity. Our method significantly outperforms previous works on UBIPr,
achieving a new state-of-the-art.

</details>


### [83] [Beyond Imitation: Constraint-Aware Trajectory Generation with Flow Matching For End-to-End Autonomous Driving](https://arxiv.org/abs/2510.26292)
*Lin Liu,Guanyi Yu,Ziying Song,Junqiao Li,Caiyan Jia,Feiyang Jia,Peiliang Wu,Yandan Luo*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的规划框架CATG，它利用约束流匹配来解决自动驾驶中轨迹规划的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的模仿学习方法容易出现模式崩溃，而生成方法难以将安全和物理约束直接融入生成过程。

Method: CATG显式地模拟了流匹配过程，并在其中直接施加约束，同时将驾驶激进程度参数化为控制信号。

Result: 在NavSim v2挑战赛中，CATG获得了第二名，EPDMS评分为51.31，并获得了创新奖。

Conclusion: CATG能够生成多样且满足约束的轨迹，并在自动驾驶规划任务中表现出色。

Abstract: Planning is a critical component of end-to-end autonomous driving. However,
prevailing imitation learning methods often suffer from mode collapse, failing
to produce diverse trajectory hypotheses. Meanwhile, existing generative
approaches struggle to incorporate crucial safety and physical constraints
directly into the generative process, necessitating an additional optimization
stage to refine their outputs. To address these limitations, we propose CATG, a
novel planning framework that leverages Constrained Flow Matching. Concretely,
CATG explicitly models the flow matching process, which inherently mitigates
mode collapse and allows for flexible guidance from various conditioning
signals. Our primary contribution is the novel imposition of explicit
constraints directly within the flow matching process, ensuring that the
generated trajectories adhere to vital safety and kinematic rules. Secondly,
CATG parameterizes driving aggressiveness as a control signal during
generation, enabling precise manipulation of trajectory style. Notably, on the
NavSim v2 challenge, CATG achieved 2nd place with an EPDMS score of 51.31 and
was honored with the Innovation Award.

</details>


### [84] [Leveraging Large-Scale Face Datasets for Deep Periocular Recognition via Ocular Cropping](https://arxiv.org/abs/2510.26294)
*Fernando Alonso-Fernandez,Kevin Hernandez-Diaz,Jose Maria Buades Rubio,Josef Bigun*

Main category: cs.CV

TL;DR: 本文研究了眼部生物识别技术，特别是眼部周围区域，具有高区分度和最小采集约束。


<details>
  <summary>Details</summary>
Motivation: 评估不同深度和复杂度的卷积神经网络结构在眼部周围区域识别中的有效性。

Method: 使用从大规模VGGFace2数据库中提取的1,907,572个眼部图像训练网络。在VGGFace2-Pose和UFPR-Periocular数据库上进行实验。

Result: 在VGGFace2上的EER为9-15%，在UFPR-Periocular上的EER为1-2%。

Conclusion: 在UFPR数据集上获得了最低的EER。

Abstract: We focus on ocular biometrics, specifically the periocular region (the area
around the eye), which offers high discrimination and minimal acquisition
constraints. We evaluate three Convolutional Neural Network architectures of
varying depth and complexity to assess their effectiveness for periocular
recognition. The networks are trained on 1,907,572 ocular crops extracted from
the large-scale VGGFace2 database. This significantly contrasts with existing
works, which typically rely on small-scale periocular datasets for training
having only a few thousand images. Experiments are conducted with ocular images
from VGGFace2-Pose, a subset of VGGFace2 containing in-the-wild face images,
and the UFPR-Periocular database, which consists of selfies captured via mobile
devices with user guidance on the screen. Due to the uncontrolled conditions of
VGGFace2, the Equal Error Rates (EERs) obtained with ocular crops range from
9-15%, noticeably higher than the 3-6% EERs achieved using full-face images. In
contrast, UFPR-Periocular yields significantly better performance (EERs of
1-2%), thanks to higher image quality and more consistent acquisition
protocols. To the best of our knowledge, these are the lowest reported EERs on
the UFPR dataset to date.

</details>


### [85] [Towards Realistic Earth-Observation Constellation Scheduling: Benchmark and Methodology](https://arxiv.org/abs/2510.26297)
*Luting Wang,Yinghao Xiang,Hongliang Huang,Dongjun Li,Chen Gao,Si Liu*

Main category: cs.CV

TL;DR: 提出了一个用于敏捷地球观测卫星（AEOS）星座调度的统一框架，包括基准测试套件AEOS-Bench和新的调度模型AEOS-Former。


<details>
  <summary>Details</summary>
Motivation: 现有方法简化了大规模场景、动态环境和严格约束下的AEOS调度复杂性，限制了它们的实际性能。

Method: 构建了一个包含3,907个卫星资产和16,410个场景的基准测试套件AEOS-Bench，并提出了一个基于Transformer的调度模型AEOS-Former，该模型结合了约束感知注意力机制。

Result: AEOS-Former在任务完成和能源效率方面优于基线模型。

Conclusion: AEOS-Former为AEOS星座调度提供了一个鲁棒的解决方案。

Abstract: Agile Earth Observation Satellites (AEOSs) constellations offer unprecedented
flexibility for monitoring the Earth's surface, but their scheduling remains
challenging under large-scale scenarios, dynamic environments, and stringent
constraints. Existing methods often simplify these complexities, limiting their
real-world performance. We address this gap with a unified framework
integrating a standardized benchmark suite and a novel scheduling model. Our
benchmark suite, AEOS-Bench, contains $3,907$ finely tuned satellite assets and
$16,410$ scenarios. Each scenario features $1$ to $50$ satellites and $50$ to
$300$ imaging tasks. These scenarios are generated via a high-fidelity
simulation platform, ensuring realistic satellite behavior such as orbital
dynamics and resource constraints. Ground truth scheduling annotations are
provided for each scenario. To our knowledge, AEOS-Bench is the first
large-scale benchmark suite tailored for realistic constellation scheduling.
Building upon this benchmark, we introduce AEOS-Former, a Transformer-based
scheduling model that incorporates a constraint-aware attention mechanism. A
dedicated internal constraint module explicitly models the physical and
operational limits of each satellite. Through simulation-based iterative
learning, AEOS-Former adapts to diverse scenarios, offering a robust solution
for AEOS constellation scheduling. Experimental results demonstrate that
AEOS-Former outperforms baseline models in task completion and energy
efficiency, with ablation studies highlighting the contribution of each
component. Code and data are provided in
https://github.com/buaa-colalab/AEOSBench.

</details>


### [86] [Exploring the correlation between the type of music and the emotions evoked: A study using subjective questionnaires and EEG](https://arxiv.org/abs/2510.26304)
*Jelizaveta Jankowska,Bożena Kostek,Fernando Alonso-Fernandez,Prayag Tiwari*

Main category: cs.CV

TL;DR: 研究不同类型的音乐如何影响人类的情绪。


<details>
  <summary>Details</summary>
Motivation: 旨在展示不同音乐流派对情绪的影响。

Method: 使用脑电图头盔进行主观调查和脑部活动测量。

Result: 分析揭示了情绪和观察到的脑部活动之间的联系。

Conclusion: 不同类型的音乐会对人类情绪产生影响，并且情绪和脑部活动之间存在关联。

Abstract: The subject of this work is to check how different types of music affect
human emotions. While listening to music, a subjective survey and brain
activity measurements were carried out using an EEG helmet. The aim is to
demonstrate the impact of different music genres on emotions. The research
involved a diverse group of participants of different gender and musical
preferences. This had the effect of capturing a wide range of emotional
responses to music. After the experiment, a relationship analysis of the
respondents' questionnaires with EEG signals was performed. The analysis
revealed connections between emotions and observed brain activity.

</details>


### [87] [A Hybrid Framework Bridging CNN and ViT based on Theory of Evidence for Diabetic Retinopathy Grading](https://arxiv.org/abs/2510.26315)
*Junlai Qiu,Yunzhu Chen,Hao Zheng,Yawen Huang,Yuexiang Li*

Main category: cs.CV

TL;DR: 提出了一种新的基于证据理论的特征融合范式，用于融合CNN和ViT的特征，以提高糖尿病视网膜病变(DR)分级的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于单类型骨干网络的DR诊断系统性能达到瓶颈，需要整合不同骨干网络的优势。

Method: 通过深度证据网络将不同骨干网络的特征转化为支持证据，然后形成聚合意见，自适应地调整不同骨干网络之间的融合模式。

Result: 在两个公开的DR分级数据集上的实验结果表明，该混合模型提高了DR分级的准确性，并为特征融合和决策提供了良好的可解释性。

Conclusion: 该方法不仅提高了DR分级的准确性，而且提供了特征融合和决策的优秀可解释性。

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss among middle-aged
and elderly people, which significantly impacts their daily lives and mental
health. To improve the efficiency of clinical screening and enable the early
detection of DR, a variety of automated DR diagnosis systems have been recently
established based on convolutional neural network (CNN) or vision Transformer
(ViT). However, due to the own shortages of CNN / ViT, the performance of
existing methods using single-type backbone has reached a bottleneck. One
potential way for the further improvements is integrating different kinds of
backbones, which can fully leverage the respective strengths of them
(\emph{i.e.,} the local feature extraction capability of CNN and the global
feature capturing ability of ViT). To this end, we propose a novel paradigm to
effectively fuse the features extracted by different backbones based on the
theory of evidence. Specifically, the proposed evidential fusion paradigm
transforms the features from different backbones into supporting evidences via
a set of deep evidential networks. With the supporting evidences, the
aggregated opinion can be accordingly formed, which can be used to adaptively
tune the fusion pattern between different backbones and accordingly boost the
performance of our hybrid model. We evaluated our method on two publicly
available DR grading datasets. The experimental results demonstrate that our
hybrid model not only improves the accuracy of DR grading, compared to the
state-of-the-art frameworks, but also provides the excellent interpretability
for feature fusion and decision-making.

</details>


### [88] [GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?](https://arxiv.org/abs/2510.26339)
*Mingyu Sung,Seungjae Ham,Kangwoo Kim,Yeokyoung Yoon,Sangseok Yun,Il-Min Kim,Jae-Mo Kang*

Main category: cs.CV

TL;DR: 提出了 GLYPH-SR，一个视觉语言引导的扩散框架，旨在共同实现文本可读性和感知质量。


<details>
  <summary>Details</summary>
Motivation: 图像超分辨率(SR)是许多视觉系统的基础，但之前的SR研究通常针对对字符级错误不敏感的失真或学习的感知指标进行调整。此外，解决文本SR的研究通常侧重于具有孤立字符的简化基准，忽略了复杂自然场景中文本的挑战。因此，场景文本被有效地视为通用纹理。

Method: GLYPH-SR 利用由 OCR 数据引导的文本 SR 融合 ControlNet(TS-ControlNet)和一个在以文本为中心和以场景为中心的引导之间交替的乒乓调度器。为了实现有针对性的文本恢复，我们在合成语料库上训练这些组件，同时保持主 SR 分支冻结。

Result: 在 x4 和 x8 的 SVT、SCUT-CTW1500 和 CUTE80 上，GLYPH-SR 在扩散/GAN 基线（SVT x8，OpenOCR）上将 OCR F1 提高了高达 +15.18 个百分点，同时保持了具有竞争力的 MANIQA、CLIP-IQA 和 MUSIQ。

Conclusion: GLYPH-SR 旨在同时满足高可读性和高视觉真实感这两个目标，从而提供看起来正确且读取正确的 SR。

Abstract: Image super-resolution(SR) is fundamental to many vision system-from
surveillance and autonomy to document analysis and retail analytics-because
recovering high-frequency details, especially scene-text, enables reliable
downstream perception. Scene-text, i.e., text embedded in natural images such
as signs, product labels, and storefronts, often carries the most actionable
information; when characters are blurred or hallucinated, optical character
recognition(OCR) and subsequent decisions fail even if the rest of the image
appears sharp. Yet previous SR research has often been tuned to distortion
(PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that
are largely insensitive to character-level errors. Furthermore, studies that do
address text SR often focus on simplified benchmarks with isolated characters,
overlooking the challenges of text within complex natural scenes. As a result,
scene-text is effectively treated as generic texture. For SR to be effective in
practical deployments, it is therefore essential to explicitly optimize for
both text legibility and perceptual quality. We present GLYPH-SR, a
vision-language-guided diffusion framework that aims to achieve both objectives
jointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by
OCR data, and a ping-pong scheduler that alternates between text- and
scene-centric guidance. To enable targeted text restoration, we train these
components on a synthetic corpus while keeping the main SR branch frozen.
Across SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by
up to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR)
while maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed
to satisfy both objectives simultaneously-high readability and high visual
realism-delivering SR that looks right and reds right.

</details>


### [89] [EEG-Driven Image Reconstruction with Saliency-Guided Diffusion Models](https://arxiv.org/abs/2510.26391)
*Igor Abramov,Ilya Makarov*

Main category: cs.CV

TL;DR: 提出了一种双重条件框架，结合脑电图嵌入和空间显着性图来增强图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有的脑电图驱动的图像重建方法经常忽略空间注意力机制，限制了保真度和语义连贯性。

Method: 利用自适应思维映射器（ATM）进行脑电图特征提取，并通过低秩适应（LoRA）微调稳定扩散2.1，以将神经信号与视觉语义对齐，而 ControlNet 分支则以显着性图为条件进行空间控制生成。

Result: 在 THINGS-EEG 上评估，该方法在低级和高级图像特征的质量方面比现有方法有了显着提高，同时与人类视觉注意力高度对齐。

Conclusion: 注意力先验可以解决脑电图的模糊性，从而实现高保真重建，并在医学诊断和神经适应接口中具有应用，通过有效适应预训练的扩散模型来推进神经解码。

Abstract: Existing EEG-driven image reconstruction methods often overlook spatial
attention mechanisms, limiting fidelity and semantic coherence. To address
this, we propose a dual-conditioning framework that combines EEG embeddings
with spatial saliency maps to enhance image generation. Our approach leverages
the Adaptive Thinking Mapper (ATM) for EEG feature extraction and fine-tunes
Stable Diffusion 2.1 via Low-Rank Adaptation (LoRA) to align neural signals
with visual semantics, while a ControlNet branch conditions generation on
saliency maps for spatial control. Evaluated on THINGS-EEG, our method achieves
a significant improvement in the quality of low- and high-level image features
over existing approaches. Simultaneously, strongly aligning with human visual
attention. The results demonstrate that attentional priors resolve EEG
ambiguities, enabling high-fidelity reconstructions with applications in
medical diagnostics and neuroadaptive interfaces, advancing neural decoding
through efficient adaptation of pre-trained diffusion models.

</details>


### [90] [LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video Generation](https://arxiv.org/abs/2510.26412)
*Xiangqing Zheng,Chengyue Wu,Kehai Chen,Min Zhang*

Main category: cs.CV

TL;DR: 提出了一个专门用于评估长视频生成的基准测试，该基准测试侧重于细粒度对齐和抽象维度，如叙事连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试主要依赖于简化的提示，并且忽略了与提示的细粒度对齐以及抽象维度，例如叙事连贯性和主题表达。

Method: 提出了LoCoT2V-Bench，一个专门为复杂输入条件下的长视频生成（LVG）设计的基准测试。它引入了一套现实和复杂的提示，并构建了一个多维评估框架，包括事件级别对齐、细粒度时间一致性、内容清晰度和人类期望实现度（HERD）等新指标。

Result: 对九个有代表性的LVG模型进行了综合评估，发现当前的方法在基本的视觉和时间方面表现良好，但在事件间一致性、细粒度对齐和高级主题坚持等方面存在不足。

Conclusion: LoCoT2V-Bench 为评估长篇复杂文本到视频的生成提供了一个全面而可靠的平台，并强调了未来方法改进的关键方向。

Abstract: Recently text-to-video generation has made impressive progress in producing
short, high-quality clips, but evaluating long-form outputs remains a major
challenge especially when processing complex prompts. Existing benchmarks
mostly rely on simplified prompts and focus on low-level metrics, overlooking
fine-grained alignment with prompts and abstract dimensions such as narrative
coherence and thematic expression. To address these gaps, we propose
LoCoT2V-Bench, a benchmark specifically designed for long video generation
(LVG) under complex input conditions. Based on various real-world videos,
LoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating
elements like scene transitions and event dynamics. Moreover, it constructs a
multi-dimensional evaluation framework that includes our newly proposed metrics
such as event-level alignment, fine-grained temporal consistency, content
clarity, and the Human Expectation Realization Degree (HERD) that focuses on
more abstract attributes like narrative flow, emotional response, and character
development. Using this framework, we conduct a comprehensive evaluation of
nine representative LVG models, finding that while current methods perform well
on basic visual and temporal aspects, they struggle with inter-event
consistency, fine-grained alignment, and high-level thematic adherence, etc.
Overall, LoCoT2V-Bench provides a comprehensive and reliable platform for
evaluating long-form complex text-to-video generation and highlights critical
directions for future method improvement.

</details>


### [91] [A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt Tuning of Vision-Language Models](https://arxiv.org/abs/2510.26441)
*Shihab Aaqil Ahamed,Udaya S. K. P. Miriya Thanthrige,Ranga Rodrigo,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 本文提出了一种新的测试时提示调整框架A-TPT，通过引入角度多样性来提高视觉-语言模型(VLM)的校准性能。


<details>
  <summary>Details</summary>
Motivation: 现有测试时提示调整(TPT)方法在改进提示校准方面存在不足，未能实现类间文本特征的最佳角度分离，忽略了角度多样性的关键作用，从而影响了VLM的可靠性、可信度和安全性。

Method: 提出A-TPT框架，通过最大化单位超球面上特征之间的最小成对角度距离，鼓励由可学习提示引起的归一化文本特征分布的均匀性，从而引入角度多样性。

Result: A-TPT在减少平均校准误差方面优于现有TPT方法，并在各种数据集上保持了相当的精度。在自然分布偏移和医学数据集上表现出卓越的零样本校准性能。

Conclusion: 促进角度多样性可以实现良好分散的文本特征，从而显著提高测试时调整期间的VLM校准。

Abstract: Test-time prompt tuning (TPT) has emerged as a promising technique for
adapting large vision-language models (VLMs) to unseen tasks without relying on
labeled data. However, the lack of dispersion between textual features can hurt
calibration performance, which raises concerns about VLMs' reliability,
trustworthiness, and safety. Current TPT approaches primarily focus on
improving prompt calibration by either maximizing average textual feature
dispersion or enforcing orthogonality constraints to encourage angular
separation. However, these methods may not always have optimal angular
separation between class-wise textual features, which implies overlooking the
critical role of angular diversity. To address this, we propose A-TPT, a novel
TPT framework that introduces angular diversity to encourage uniformity in the
distribution of normalized textual features induced by corresponding learnable
prompts. This uniformity is achieved by maximizing the minimum pairwise angular
distance between features on the unit hypersphere. We show that our approach
consistently surpasses state-of-the-art TPT methods in reducing the aggregate
average calibration error while maintaining comparable accuracy through
extensive experiments with various backbones on different datasets. Notably,
our approach exhibits superior zero-shot calibration performance on natural
distribution shifts and generalizes well to medical datasets. We provide
extensive analyses, including theoretical aspects, to establish the grounding
of A-TPT. These results highlight the potency of promoting angular diversity to
achieve well-dispersed textual features, significantly improving VLM
calibration during test-time adaptation. Our code will be made publicly
available.

</details>


### [92] [PointSt3R: Point Tracking through 3D Grounded Correspondence](https://arxiv.org/abs/2510.26443)
*Rhodri Guerrier,Adam W. Harley,Dima Damen*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D重建模型的点追踪方法，通过3D对应关系实现。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建模型在静态场景的2D和3D对应关系中表现出巨大潜力，但缺乏在动态场景点追踪上的应用。

Method: 1. 将重建损失与动态对应关系的训练以及可见性头相结合。
2. 使用少量合成数据对MASt3R进行微调以进行点跟踪。
3. 仅在包含查询点的帧对上进行训练和评估，从而有效地消除了任何时间上下文。

Result: 在四个数据集上取得了有竞争力的或更优越的点跟踪结果。例如，在TAP-Vid-DAVIS上，PointSt3R具有竞争力；在EgoPoints和RGB-S上，显著优于CoTracker3。

Conclusion: 提出的方法在点跟踪任务上具有有效性，并在多个数据集上取得了良好的结果。

Abstract: Recent advances in foundational 3D reconstruction models, such as DUSt3R and
MASt3R, have shown great potential in 2D and 3D correspondence in static
scenes. In this paper, we propose to adapt them for the task of point tracking
through 3D grounded correspondence. We first demonstrate that these models are
competitive point trackers when focusing on static points, present in current
point tracking benchmarks ($+33.5\%$ on EgoPoints vs. CoTracker2). We propose
to combine the reconstruction loss with training for dynamic correspondence
along with a visibility head, and fine-tuning MASt3R for point tracking using a
relatively small amount of synthetic data. Importantly, we only train and
evaluate on pairs of frames where one contains the query point, effectively
removing any temporal context. Using a mix of dynamic and static point
correspondences, we achieve competitive or superior point tracking results on
four datasets (e.g. competitive on TAP-Vid-DAVIS 73.8 $\delta_{avg}$ / 85.8\%
occlusion acc. for PointSt3R compared to 75.7 / 88.3\% for CoTracker2; and
significantly outperform CoTracker3 on EgoPoints 61.3 vs 54.2 and RGB-S 87.0 vs
82.8). We also present results on 3D point tracking along with several
ablations on training datasets and percentage of dynamic correspondences.

</details>


### [93] [Towards Fine-Grained Vision-Language Alignment for Few-Shot Anomaly Detection](https://arxiv.org/abs/2510.26464)
*Yuanting Fan,Jun Liu,Xiaochen Chen,Bin-Bin Gao,Jian Li,Yong Liu,Jinlong Peng,Chengjie Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的少样本异常检测框架，通过多层次细粒度语义字幕（MFSC）和多层次可学习提示（MLLP）来提高异常定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法由于缺乏详细的文本描述，导致图像描述和patch-level视觉异常之间的语义不对齐，从而导致次优的定位性能。

Method: 提出了一种名为FineGrainedAD的框架，该框架由多层次可学习提示（MLLP）和多层次语义对齐（MLSA）组成。MLLP通过自动替换和连接机制将细粒度语义引入多层次可学习提示，而MLSA设计区域聚合策略和多层次对齐训练，以促进可学习提示与相应的视觉区域更好地对齐。

Result: 在MVTec-AD和VisA数据集上的实验表明，所提出的FineGrainedAD在少样本设置中实现了优越的整体性能。

Conclusion: 本文提出的FineGrainedAD框架能够有效地提高少样本异常检测中的异常定位性能。

Abstract: Few-shot anomaly detection (FSAD) methods identify anomalous regions with few
known normal samples. Most existing methods rely on the generalization ability
of pre-trained vision-language models (VLMs) to recognize potentially anomalous
regions through feature similarity between text descriptions and images.
However, due to the lack of detailed textual descriptions, these methods can
only pre-define image-level descriptions to match each visual patch token to
identify potential anomalous regions, which leads to the semantic misalignment
between image descriptions and patch-level visual anomalies, achieving
sub-optimal localization performance. To address the above issues, we propose
the Multi-Level Fine-Grained Semantic Caption (MFSC) to provide multi-level and
fine-grained textual descriptions for existing anomaly detection datasets with
automatic construction pipeline. Based on the MFSC, we propose a novel
framework named FineGrainedAD to improve anomaly localization performance,
which consists of two components: Multi-Level Learnable Prompt (MLLP) and
Multi-Level Semantic Alignment (MLSA). MLLP introduces fine-grained semantics
into multi-level learnable prompts through automatic replacement and
concatenation mechanism, while MLSA designs region aggregation strategy and
multi-level alignment training to facilitate learnable prompts better align
with corresponding visual regions. Experiments demonstrate that the proposed
FineGrainedAD achieves superior overall performance in few-shot settings on
MVTec-AD and VisA datasets.

</details>


### [94] [Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition](https://arxiv.org/abs/2510.26466)
*Pei Peng,MingKun Xie,Hang Hao,Tong Jin,ShengJun Huang*

Main category: cs.CV

TL;DR: 视觉-语言模型中的对象-上下文捷径会在测试场景与训练数据不同时影响zero-shot的可靠性。这篇论文将此问题转化为因果推断问题，通过估计对象和背景的期望，并合成反事实嵌入来解决这个问题。通过估计总直接效应和模拟干预，进一步减少仅背景激活，同时保留有益的对象-上下文交互。


<details>
  <summary>Details</summary>
Motivation: 在视觉-语言模型中，对象-上下文捷径是一个持续存在的挑战，当测试时的场景与熟悉的训练共现不同时，会削弱 zero-shot 的可靠性。为了解决这个问题，作者将其转化为一个因果推理问题，即如果对象出现在不同的环境中，预测是否仍然有效？

Method: 在推理时，估计 CLIP 表示空间中的对象和背景期望，并通过将对象特征与来自外部数据集、批邻居或文本描述的不同替代上下文重新组合来合成反事实嵌入。通过估计总直接效应并模拟干预，进一步减少仅背景激活，同时保留有益的对象-上下文交互。

Result: 该方法在对上下文敏感的基准测试中，大大提高了最差组和平均准确率，建立了一个新的 zero-shot 的技术水平。

Conclusion: 该框架提供了一种轻量级的表示级别反事实方法，为去偏和可靠的多模态推理提供了一个实用的因果途径。

Abstract: Object-context shortcuts remain a persistent challenge in vision-language
models, undermining zero-shot reliability when test-time scenes differ from
familiar training co-occurrences. We recast this issue as a causal inference
problem and ask: Would the prediction remain if the object appeared in a
different environment? To answer this at inference time, we estimate object and
background expectations within CLIP's representation space, and synthesize
counterfactual embeddings by recombining object features with diverse
alternative contexts sampled from external datasets, batch neighbors, or
text-derived descriptions. By estimating the Total Direct Effect and simulating
intervention, we further subtract background-only activation, preserving
beneficial object-context interactions while mitigating hallucinated scores.
Without retraining or prompt design, our method substantially improves both
worst-group and average accuracy on context-sensitive benchmarks, establishing
a new zero-shot state of the art. Beyond performance, our framework provides a
lightweight representation-level counterfactual approach, offering a practical
causal avenue for debiased and reliable multimodal reasoning.

</details>


### [95] [Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing](https://arxiv.org/abs/2510.26474)
*Xin Guo,Zhiheng Xi,Yiwen Ding,Yitao Zhai,Xiaowei Shi,Xunliang Cai,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CV

TL;DR: 模型在处理简单查询时表现出色，但在复杂查询时遇到困难，导致不平衡优化，从而阻碍了模型能力的进一步提升。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型(LVLMs)的自提升主流范式在迭代过程中，模型在简单查询上生成高质量轨迹，但在复杂查询上表现不佳，导致不平衡优化。

Method: 从分布重塑和轨迹重采样两个角度引入四种有效策略，以在探索和学习的自提升过程中实现head-tail重新平衡。

Result: 在Qwen2-VL-7B-Instruct和InternVL2.5-4B模型上的大量实验表明，该方法持续提高了视觉推理能力，平均优于原始自提升方法3.86个点。

Conclusion: 通过解决自提升过程中的“马太效应”，提出的方法能够有效提高视觉推理能力。

Abstract: Self-improvement has emerged as a mainstream paradigm for advancing the
reasoning capabilities of large vision-language models (LVLMs), where models
explore and learn from successful trajectories iteratively. However, we
identify a critical issue during this process: the model excels at generating
high-quality trajectories for simple queries (i.e., head data) but struggles
with more complex ones (i.e., tail data). This leads to an imbalanced
optimization that drives the model to prioritize simple reasoning skills, while
hindering its ability to tackle more complex reasoning tasks. Over iterations,
this imbalance becomes increasingly pronounced--a dynamic we term the "Matthew
effect"--which ultimately hinders further model improvement and leads to
performance bottlenecks. To counteract this challenge, we introduce four
efficient strategies from two perspectives: distribution-reshaping and
trajectory-resampling, to achieve head-tail re-balancing during the
exploration-and-learning self-improvement process. Extensive experiments on
Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks
demonstrate that our methods consistently improve visual reasoning
capabilities, outperforming vanilla self-improvement by 3.86 points on average.

</details>


### [96] [Analysis of the Robustness of an Edge Detector Based on Cellular Automata Optimized by Particle Swarm](https://arxiv.org/abs/2510.26509)
*Vinícius Ferraria,Eurico Ruivo*

Main category: cs.CV

TL;DR: 本文提出了一种基于元启发式算法优化的二维细胞自动机自适应边缘检测器，并结合迁移学习技术。


<details>
  <summary>Details</summary>
Motivation: 现有边缘检测器在检测松散边缘方面存在不足，缺乏提取相关信息的能力。为了解决这些问题，本文旨在开发一种自适应检测器。

Method: 该检测器由二维细胞自动机描述，并通过元启发式算法和迁移学习技术进行优化。

Result: 扩大优化阶段的搜索空间对于所选图像集无效。该模型能够适应输入，但迁移学习技术没有显著改善。

Conclusion: 该模型具有一定的适应性，但迁移学习的改进效果不明显。

Abstract: The edge detection task is essential in image processing aiming to extract
relevant information from an image. One recurring problem in this task is the
weaknesses found in some detectors, such as the difficulty in detecting loose
edges and the lack of context to extract relevant information from specific
problems. To address these weaknesses and adapt the detector to the properties
of an image, an adaptable detector described by two-dimensional cellular
automaton and optimized by meta-heuristic combined with transfer learning
techniques was developed. This study aims to analyze the impact of expanding
the search space of the optimization phase and the robustness of the
adaptability of the detector in identifying edges of a set of natural images
and specialized subsets extracted from the same image set. The results obtained
prove that expanding the search space of the optimization phase was not
effective for the chosen image set. The study also analyzed the adaptability of
the model through a series of experiments and validation techniques and found
that, regardless of the validation, the model was able to adapt to the input
and the transfer learning techniques applied to the model showed no significant
improvements.

</details>


### [97] [DINO-YOLO: Self-Supervised Pre-training for Data-Efficient Object Detection in Civil Engineering Applications](https://arxiv.org/abs/2510.25140)
*Malaisree P,Youwai S,Kitkobsin T,Janrungautai S,Amorndechaphon D,Rojanavasu P*

Main category: cs.CV

TL;DR: DINO-YOLO是一种混合架构，它结合了YOLOv12和DINOv3自监督视觉转换器，用于在数据有限的情况下进行高效检测。


<details>
  <summary>Details</summary>
Motivation: 在土木工程应用中的目标检测受到专业领域中有限的带注释数据的约束。

Method: DINOv3特征策略性地集成在两个位置：输入预处理（P0）和中间骨干增强（P3）。

Result: 隧道裂缝检测提高了12.4%，建筑PPE提高了13.7%，KITTI提高了88.6%，同时保持了实时推理（30-47 FPS）。

Conclusion: DINO-YOLO在保持计算效率的同时，为土木工程数据集（<10K图像）建立了最先进的性能，为数据受限环境中的施工安全监控和基础设施检查提供了实用的解决方案。

Abstract: Object detection in civil engineering applications is constrained by limited
annotated data in specialized domains. We introduce DINO-YOLO, a hybrid
architecture combining YOLOv12 with DINOv3 self-supervised vision transformers
for data-efficient detection. DINOv3 features are strategically integrated at
two locations: input preprocessing (P0) and mid-backbone enhancement (P3).
Experimental validation demonstrates substantial improvements: Tunnel Segment
Crack detection (648 images) achieves 12.4% improvement, Construction PPE (1K
images) gains 13.7%, and KITTI (7K images) shows 88.6% improvement, while
maintaining real-time inference (30-47 FPS). Systematic ablation across five
YOLO scales and nine DINOv3 variants reveals that Medium-scale architectures
achieve optimal performance with DualP0P3 integration (55.77% mAP@0.5), while
Small-scale requires Triple Integration (53.63%). The 2-4x inference overhead
(21-33ms versus 8-16ms baseline) remains acceptable for field deployment on
NVIDIA RTX 5090. DINO-YOLO establishes state-of-the-art performance for civil
engineering datasets (<10K images) while preserving computational efficiency,
providing practical solutions for construction safety monitoring and
infrastructure inspection in data-constrained environments.

</details>


### [98] [SA$^{2}$Net: Scale-Adaptive Structure-Affinity Transformation for Spine Segmentation from Ultrasound Volume Projection Imaging](https://arxiv.org/abs/2510.26568)
*Hao Xie,Zixun Huang,Yushen Zuo,Yakun Ju,Frank H. F. Leung,N. F. Law,Kin-Man Lam,Yong-Ping Zheng,Sai Ho Ling*

Main category: cs.CV

TL;DR: 提出了一种新的scale-adaptive structure-aware网络（SA$^{2}$Net）用于有效脊柱分割。


<details>
  <summary>Details</summary>
Motivation: 基于超声体积投影成像（VPI）的脊柱分割在临床应用中对智能脊柱侧弯诊断起着至关重要的作用。然而，这项任务面临着几个重大挑战：忽略不同骨骼特征的高度空间相关性可能导致脊柱的全局上下文知识不能被很好地学习；脊柱骨骼包含关于其形状和位置的丰富结构知识，值得被编码到分割过程中。

Method: 1. 提出了一种scale-adaptive互补策略，以学习脊柱图像的跨维度长距离相关特征。2. 提出了一种结构亲和变换，以利用Transformer中的多头自注意力机制，将语义特征与类别特定的亲和力进行变换，并将其与Transformer解码器结合，用于结构感知推理。3. 采用了一种特征混合损失聚合方法来增强模型训练。

Result: 实验结果表明，SA$^{2}$Net相比其他最先进的方法，实现了卓越的分割性能。

Conclusion: SA$^{2}$Net对各种骨干网络的适应性增强了其作为使用智能脊柱图像分析进行高级脊柱侧弯诊断的有前途的工具的潜力。

Abstract: Spine segmentation, based on ultrasound volume projection imaging (VPI),
plays a vital role for intelligent scoliosis diagnosis in clinical
applications. However, this task faces several significant challenges. Firstly,
the global contextual knowledge of spines may not be well-learned if we neglect
the high spatial correlation of different bone features. Secondly, the spine
bones contain rich structural knowledge regarding their shapes and positions,
which deserves to be encoded into the segmentation process. To address these
challenges, we propose a novel scale-adaptive structure-aware network
(SA$^{2}$Net) for effective spine segmentation. First, we propose a
scale-adaptive complementary strategy to learn the cross-dimensional
long-distance correlation features for spinal images. Second, motivated by the
consistency between multi-head self-attention in Transformers and semantic
level affinity, we propose structure-affinity transformation to transform
semantic features with class-specific affinity and combine it with a
Transformer decoder for structure-aware reasoning. In addition, we adopt a
feature mixing loss aggregation method to enhance model training. This method
improves the robustness and accuracy of the segmentation process. The
experimental results demonstrate that our SA$^{2}$Net achieves superior
segmentation performance compared to other state-of-the-art methods. Moreover,
the adaptability of SA$^{2}$Net to various backbones enhances its potential as
a promising tool for advanced scoliosis diagnosis using intelligent spinal
image analysis. The code and experimental demo are available at
https://github.com/taetiseo09/SA2Net.

</details>


### [99] [Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in Zero-Shot Real-World Scenarios](https://arxiv.org/abs/2510.26580)
*Manjunath Prasad Holenarasipura Rajiv,B. M. Vidyavathi*

Main category: cs.CV

TL;DR: 提出了一种动态上下文感知场景推理框架，以解决零样本现实场景中的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的场景理解模型在没有标签数据的陌生环境中面临挑战，泛化能力有限，阻碍了视觉应用在动态、非结构化环境中的部署。

Method: 该方法整合了预训练的视觉Transformer和大型语言模型，以对齐视觉语义和自然语言描述，并通过动态推理模块结合全局场景线索和对象级别的交互来优化预测。

Result: 在COCO、Visual Genome和Open Images等零样本基准测试中，场景理解精度比基线模型提高了18%。

Conclusion: 该框架为上下文感知推理提供了一种可扩展且可解释的方法，从而推进了动态现实世界环境中的零样本泛化。

Abstract: In real-world environments, AI systems often face unfamiliar scenarios
without labeled data, creating a major challenge for conventional scene
understanding models. The inability to generalize across unseen contexts limits
the deployment of vision-based applications in dynamic, unstructured settings.
This work introduces a Dynamic Context-Aware Scene Reasoning framework that
leverages Vision-Language Alignment to address zero-shot real-world scenarios.
The goal is to enable intelligent systems to infer and adapt to new
environments without prior task-specific training. The proposed approach
integrates pre-trained vision transformers and large language models to align
visual semantics with natural language descriptions, enhancing contextual
comprehension. A dynamic reasoning module refines predictions by combining
global scene cues and object-level interactions guided by linguistic priors.
Extensive experiments on zero-shot benchmarks such as COCO, Visual Genome, and
Open Images demonstrate up to 18% improvement in scene understanding accuracy
over baseline models in complex and unseen environments. Results also show
robust performance in ambiguous or cluttered scenes due to the synergistic
fusion of vision and language. This framework offers a scalable and
interpretable approach for context-aware reasoning, advancing zero-shot
generalization in dynamic real-world settings.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [100] [Towards Piece-by-Piece Explanations for Chess Positions with SHAP](https://arxiv.org/abs/2510.25775)
*Francesco Spinnato*

Main category: cs.AI

TL;DR: 使用SHAP方法来解释chess引擎的评估，将评估归因于棋盘上的特定棋子。


<details>
  <summary>Details</summary>
Motivation: 目前的chess引擎评估精确但不透明，隐藏了各个棋子或模式的贡献。

Method: 将棋子视为特征，系统地移除它们，计算每个棋子的贡献，以解释引擎的输出。

Result: 该方法可以实现局部忠实和人类可解释的解释，为可视化、人类训练和引擎比较开辟了新的可能性。

Conclusion: 发布了代码和数据，以促进可解释的chess AI的未来研究。

Abstract: Contemporary chess engines offer precise yet opaque evaluations, typically
expressed as centipawn scores. While effective for decision-making, these
outputs obscure the underlying contributions of individual pieces or patterns.
In this paper, we explore adapting SHAP (SHapley Additive exPlanations) to the
domain of chess analysis, aiming to attribute a chess engines evaluation to
specific pieces on the board. By treating pieces as features and systematically
ablating them, we compute additive, per-piece contributions that explain the
engines output in a locally faithful and human-interpretable manner. This
method draws inspiration from classical chess pedagogy, where players assess
positions by mentally removing pieces, and grounds it in modern explainable AI
techniques. Our approach opens new possibilities for visualization, human
training, and engine comparison. We release accompanying code and data to
foster future research in interpretable chess AI.

</details>


### [101] [An Agentic Framework for Rapid Deployment of Edge AI Solutions in Industry 5.0](https://arxiv.org/abs/2510.25813)
*Jorge Martinez-Gil,Mario Pichler,Nefeli Bountouni,Sotiris Koussouris,Marielena Márquez Barreiro,Sergio Gusmeroli*

Main category: cs.AI

TL;DR: 提出了一个用于工业 5.0 的新框架，简化了在各种工业环境中边缘设备上 AI 模型的部署。


<details>
  <summary>Details</summary>
Motivation: 在工业环境中边缘设备上部署人工智能模型的需求，以及减少延迟和避免外部数据传输的必要性。

Method: 该框架是基于代理的，支持模块化集成，并保持较低的资源需求，从而实现本地推理和实时处理。

Result: 初步评估表明，在食品工业的实际场景中，部署时间和系统适应性性能得到了改善。

Conclusion: 该框架通过简化部署、减少延迟和提高适应性，为工业 5.0 提供了有价值的解决方案。

Abstract: We present a novel framework for Industry 5.0 that simplifies the deployment
of AI models on edge devices in various industrial settings. The design reduces
latency and avoids external data transfer by enabling local inference and
real-time processing. Our implementation is agent-based, which means that
individual agents, whether human, algorithmic, or collaborative, are
responsible for well-defined tasks, enabling flexibility and simplifying
integration. Moreover, our framework supports modular integration and maintains
low resource requirements. Preliminary evaluations concerning the food industry
in real scenarios indicate improved deployment time and system adaptability
performance. The source code is publicly available at
https://github.com/AI-REDGIO-5-0/ci-component.

</details>


### [102] [Symbolically Scaffolded Play: Designing Role-Sensitive Prompts for Generative NPC Dialogue](https://arxiv.org/abs/2510.25820)
*Vanessa Figueiredo,David Elumeze*

Main category: cs.AI

TL;DR: 本研究调查了约束性提示是否能提高玩家体验，通过一个基于GPT-4o的语音侦探游戏进行研究，发现高约束提示和低约束提示在体验上没有显著差异，并发现scaffolding效果与角色相关。提出了“符号化Scaffolded Play”框架，以在保持即兴创作的同时，稳定连贯性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型有望通过使非玩家角色能够进行无脚本对话来改变互动游戏。然而，约束性提示是否真正改善玩家体验仍不清楚。

Method: 通过一个基于GPT-4o的语音侦探游戏，进行了一个受试者内部的可用性研究，比较了高约束（HCP）和低约束（LCP）提示。

Result: 研究表明，除了对技术故障的敏感性外，高约束提示和低约束提示在体验上没有可靠的差异。Scaffolding效果与角色相关：面试官（任务给予者NPC）获得了稳定性，而嫌疑人NPC失去了即兴的可信度。

Conclusion: 研究结果颠覆了更严格的约束会固有地增强游戏性的假设。提出了Symbolically Scaffolded Play框架，其中符号结构被表达为模糊的数值边界，从而在需要时稳定连贯性，同时在惊喜能够维持参与度的地方保留即兴创作。

Abstract: Large Language Models (LLMs) promise to transform interactive games by
enabling non-player characters (NPCs) to sustain unscripted dialogue. Yet it
remains unclear whether constrained prompts actually improve player experience.
We investigate this question through The Interview, a voice-based detective
game powered by GPT-4o. A within-subjects usability study ($N=10$) compared
high-constraint (HCP) and low-constraint (LCP) prompts, revealing no reliable
experiential differences beyond sensitivity to technical breakdowns. Guided by
these findings, we redesigned the HCP into a hybrid JSON+RAG scaffold and
conducted a synthetic evaluation with an LLM judge, positioned as an
early-stage complement to usability testing. Results uncovered a novel pattern:
scaffolding effects were role-dependent: the Interviewer (quest-giver NPC)
gained stability, while suspect NPCs lost improvisational believability. These
findings overturn the assumption that tighter constraints inherently enhance
play. Extending fuzzy-symbolic scaffolding, we introduce \textit{Symbolically
Scaffolded Play}, a framework in which symbolic structures are expressed as
fuzzy, numerical boundaries that stabilize coherence where needed while
preserving improvisation where surprise sustains engagement.

</details>


### [103] [Through the Judge's Eyes: Inferred Thinking Traces Improve Reliability of LLM Raters](https://arxiv.org/abs/2510.25860)
*Xingjian Zhang,Tianhong Gao,Suliang Jin,Tianhao Wang,Teng Ye,Eytan Adar,Qiaozhu Mei*

Main category: cs.AI

TL;DR: 大型语言模型越来越多地被用作评估任务的评分者，但对于主观任务，它们的可靠性通常受到限制。本文提出了一个人-LLM协作框架，用于从仅标签的注释中推断思维轨迹，并应用于两个互补的任务：(1)微调开放LLM评分者；(2)为专有LLM评分者合成更清晰的注释指南。结果表明，llm可以作为人类思维轨迹的实用代理，从而能够将仅标签的语料库扩展为增强思维轨迹的资源，从而提高LLM评分者的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为评估任务的评分者，在主观任务中可靠性不足，因为人类的判断涉及到超出注释标签的细微推理。

Method: 提出了一个人-LLM协作框架，使用一种简单有效的拒绝抽样方法来大规模重建思维轨迹。

Result: 通过多个数据集，该方法显著提高了LLM-人类的一致性。此外，改进的注释指南提高了不同LLM模型之间的一致性。

Conclusion: LLM可以作为人类思维轨迹的实用代理，从而能够将仅标签的语料库扩展为增强思维轨迹的资源，从而提高LLM评分者的可靠性。

Abstract: Large language models (LLMs) are increasingly used as raters for evaluation
tasks. However, their reliability is often limited for subjective tasks, when
human judgments involve subtle reasoning beyond annotation labels. Thinking
traces, the reasoning behind a judgment, are highly informative but challenging
to collect and curate. We present a human-LLM collaborative framework to infer
thinking traces from label-only annotations. The proposed framework uses a
simple and effective rejection sampling method to reconstruct these traces at
scale. These inferred thinking traces are applied to two complementary tasks:
(1) fine-tuning open LLM raters; and (2) synthesizing clearer annotation
guidelines for proprietary LLM raters. Across multiple datasets, our methods
lead to significantly improved LLM-human agreement. Additionally, the refined
annotation guidelines increase agreement among different LLM models. These
results suggest that LLMs can serve as practical proxies for otherwise
unrevealed human thinking traces, enabling label-only corpora to be extended
into thinking-trace-augmented resources that enhance the reliability of LLM
raters.

</details>


### [104] [The Information-Theoretic Imperative: Compression and the Epistemic Foundations of Intelligence](https://arxiv.org/abs/2510.25883)
*Christian Dittrich,Jennifer Flygare Kinne*

Main category: cs.AI

TL;DR: 该论文提出了一个双层框架，解释了为什么压缩过程会强制发现因果结构而不是表面统计模式。


<details>
  <summary>Details</summary>
Motivation: 现有的框架都集中在压缩对于智能的重要性，但是没有明确说明为什么这个过程会强制发现因果结构。

Method: 该论文引入了信息论指令（ITI）和压缩效率原则（CEP）的双层框架。

Result: 该框架产生了一系列可验证的预测，例如压缩效率与分布外泛化相关，异常积累率区分因果模型和相关模型。

Conclusion: ITI和CEP为生物、人工和多尺度系统的收敛提供了一个统一的解释，解决了智能的认知和功能维度，而没有引入关于意识或主观体验的假设。

Abstract: Existing frameworks converge on the centrality of compression to intelligence
but leave underspecified why this process enforces the discovery of causal
structure rather than superficial statistical patterns. We introduce a
two-level framework to address this gap. The Information-Theoretic Imperative
(ITI) establishes that any system persisting in uncertain environments must
minimize epistemic entropy through predictive compression: this is the
evolutionary "why" linking survival pressure to information-processing demands.
The Compression Efficiency Principle (CEP) specifies how efficient compression
mechanically selects for generative, causal models through
exception-accumulation dynamics, making reality alignment a consequence rather
than a contingent achievement. Together, ITI and CEP define a causal chain:
from survival pressure to prediction necessity, compression requirement,
efficiency optimization, generative structure discovery, and ultimately reality
alignment. Each link follows from physical, information-theoretic, or
evolutionary constraints, implying that intelligence is the mechanically
necessary outcome of persistence in structured environments. This framework
yields empirically testable predictions: compression efficiency, measured as
approach to the rate-distortion frontier, correlates with out-of-distribution
generalization; exception-accumulation rates differentiate causal from
correlational models; hierarchical systems exhibit increasing efficiency across
abstraction layers; and biological systems demonstrate metabolic costs that
track representational complexity. ITI and CEP thereby provide a unified
account of convergence across biological, artificial, and multi-scale systems,
addressing the epistemic and functional dimensions of intelligence without
invoking assumptions about consciousness or subjective experience.

</details>


### [105] [Approximating Human Preferences Using a Multi-Judge Learned System](https://arxiv.org/abs/2510.25884)
*Eitán Sprejer,Fernando Avalos,Augusto Bernardi,Jose Pedro Brito de Azevedo Faustino,Jacob Haimes,Narmeen Fatimah Oozeer*

Main category: cs.AI

TL;DR: 提出一个框架，通过学习聚合多个基于规则的判断器的输出，来模拟基于角色的偏好。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的评判器难以校准，并且经常受到规则敏感性、偏差和不稳定性的影响。克服这个挑战可以促进关键应用，例如为人类反馈强化学习（RLHF）创建可靠的奖励模型，并构建有效的路由系统，为给定的用户查询选择最合适的模型。

Method: 提出一个框架，通过学习聚合多个基于规则的判断器的输出，来模拟基于角色的偏好。该方法包括一个用于大规模合成偏好标签的基于角色的方法，以及聚合器的两种不同的实现：广义加性模型（GAM）和多层感知器（MLP）。

Result: 研究了该方法相对于简单基线的性能，并通过对人类和LLM评判器偏差的案例研究评估了其鲁棒性。

Conclusion: 提出了一个用于建模不同，基于角色的偏好的框架，通过学习聚合多个基于规则的判断器的输出。

Abstract: Aligning LLM-based judges with human preferences is a significant challenge,
as they are difficult to calibrate and often suffer from rubric sensitivity,
bias, and instability. Overcoming this challenge advances key applications,
such as creating reliable reward models for Reinforcement Learning from Human
Feedback (RLHF) and building effective routing systems that select the
best-suited model for a given user query. In this work, we propose a framework
for modeling diverse, persona-based preferences by learning to aggregate
outputs from multiple rubric-conditioned judges. We investigate the performance
of this approach against naive baselines and assess its robustness through case
studies on both human and LLM-judges biases. Our primary contributions include
a persona-based method for synthesizing preference labels at scale and two
distinct implementations of our aggregator: Generalized Additive Model (GAM)
and a Multi-Layer Perceptron (MLP).

</details>


### [106] [SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness of Large Language Models in Scientific Applications](https://arxiv.org/abs/2510.25908)
*Emily Herron,Junqi Yin,Feiyi Wang*

Main category: cs.AI

TL;DR: SciTrust 2.0是一个评估LLM在科学应用中可信度的框架，包含真values、对抗鲁棒性、科学安全和科学伦理四个维度。


<details>
  <summary>Details</summary>
Motivation: 在科学研究中部署LLM具有变革潜力，但也引发了对其可信度的担忧。

Method: 该研究提出了SciTrust 2.0框架，并结合了通过验证的反思调整管道和专家验证开发的新型开放式真values基准，以及涵盖双重用途研究和偏见等八个子类别的科学研究伦理基准。使用准确率、语义相似性测量和基于LLM的评分等多种评估指标，评估了七个突出的LLM，包括四个科学专用模型和三个通用工业模型。

Result: 通用工业模型在每个可信度维度上总体优于科学专用模型，GPT-o4-mini在真values评估和对抗鲁棒性方面表现出优越的性能。科学专用模型在逻辑和伦理推理能力方面表现出明显的缺陷，并且在安全评估中存在令人担忧的漏洞，尤其是在生物安全和化学武器等高风险领域。

Conclusion: 该研究开源了SciTrust 2.0框架，为开发更可信的AI系统，并推进科学背景下模型安全和伦理的研究奠定了基础。

Abstract: Large language models (LLMs) have demonstrated transformative potential in
scientific research, yet their deployment in high-stakes contexts raises
significant trustworthiness concerns. Here, we introduce SciTrust 2.0, a
comprehensive framework for evaluating LLM trustworthiness in scientific
applications across four dimensions: truthfulness, adversarial robustness,
scientific safety, and scientific ethics. Our framework incorporates novel,
open-ended truthfulness benchmarks developed through a verified
reflection-tuning pipeline and expert validation, alongside a novel ethics
benchmark for scientific research contexts covering eight subcategories
including dual-use research and bias. We evaluated seven prominent LLMs,
including four science-specialized models and three general-purpose industry
models, using multiple evaluation metrics including accuracy, semantic
similarity measures, and LLM-based scoring. General-purpose industry models
overall outperformed science-specialized models across each trustworthiness
dimension, with GPT-o4-mini demonstrating superior performance in truthfulness
assessments and adversarial robustness. Science-specialized models showed
significant deficiencies in logical and ethical reasoning capabilities, along
with concerning vulnerabilities in safety evaluations, particularly in
high-risk domains such as biosecurity and chemical weapons. By open-sourcing
our framework, we provide a foundation for developing more trustworthy AI
systems and advancing research on model safety and ethics in scientific
contexts.

</details>


### [107] [FinOps Agent -- A Use-Case for IT Infrastructure and Cost Optimization](https://arxiv.org/abs/2510.25914)
*Ngoc Phuoc An Vo,Manish Kesarwani,Ruchi Mahindru,Chandrasekhar Narayanaswami*

Main category: cs.AI

TL;DR: FinOps is a framework maximizing cloud value via financial accountability. The challenge is that billing data is in heterogeneous formats. The paper proposes AI agents for FinOps automation, and it built a FinOps agent for IT infrastructure and cost optimization. The agent can understand, plan, and execute tasks like a FinOps practitioner.


<details>
  <summary>Details</summary>
Motivation: Billing data arrives in heterogeneous formats, taxonomies, and metrics from multiple cloud providers and internal systems which eventually lead to synthesizing actionable insights, and making time-sensitive decisions.

Method: Built a system simulating a realistic end-to-end industry process starting with retrieving data from various sources to consolidating and analyzing the data to generate recommendations for optimization. Defined a set of metrics to evaluate our agent using several open-source and close-source language models.

Result: The agent was able to understand, plan, and execute tasks as well as an actual FinOps practitioner.

Conclusion: Leveraging autonomous, goal-driven AI agents for FinOps automation.

Abstract: FinOps (Finance + Operations) represents an operational framework and
cultural practice which maximizes cloud business value through collaborative
financial accountability across engineering, finance, and business teams.
FinOps practitioners face a fundamental challenge: billing data arrives in
heterogeneous formats, taxonomies, and metrics from multiple cloud providers
and internal systems which eventually lead to synthesizing actionable insights,
and making time-sensitive decisions. To address this challenge, we propose
leveraging autonomous, goal-driven AI agents for FinOps automation. In this
paper, we built a FinOps agent for a typical use-case for IT infrastructure and
cost optimization. We built a system simulating a realistic end-to-end industry
process starting with retrieving data from various sources to consolidating and
analyzing the data to generate recommendations for optimization. We defined a
set of metrics to evaluate our agent using several open-source and close-source
language models and it shows that the agent was able to understand, plan, and
execute tasks as well as an actual FinOps practitioner.

</details>


### [108] [GraphCompliance: Aligning Policy and Context Graphs for LLM-Based Regulatory Compliance](https://arxiv.org/abs/2510.26309)
*Jiseong Chung,Ronny Ko,Wonchul Yoo,Makoto Onizuka,Sungmok Kim,Tae-Wan Kim,Won-Yong Shin*

Main category: cs.AI

TL;DR: 本文介绍了一种名为GraphCompliance的框架，旨在解决网络规模下的合规性问题，通过将法规文本表示为Policy Graph，将运行时上下文表示为Context Graph，并对齐它们，以提高合规性评估的准确性。


<details>
  <summary>Details</summary>
Motivation: 在网络规模下进行合规性评估面临实际挑战，因为法规文本具有交叉引用和规范性，而运行时上下文以非结构化的自然语言表达。需要将非结构化文本中的语义信息与法规的结构化、规范性元素对齐。

Method: 提出GraphCompliance框架，它将法规文本表示为Policy Graph，将运行时上下文表示为Context Graph，并对齐它们。Policy Graph编码规范性结构和交叉引用，而Context Graph将事件形式化为主语-动作-宾语(SAO)和实体-关系三元组。使用大型语言模型(LLM)进行判断推理。

Result: 在300个GDPR衍生的真实场景的实验中，GraphCompliance的micro-F1比仅使用LLM和RAG基线高4.1-7.2个百分点，减少了预测不足和过度预测，从而提高了召回率并降低了假阳性率。

Conclusion: 结构化表示和判断LLM对于规范性推理是互补的。

Abstract: Compliance at web scale poses practical challenges: each request may require
a regulatory assessment. Regulatory texts (e.g., the General Data Protection
Regulation, GDPR) are cross-referential and normative, while runtime contexts
are expressed in unstructured natural language. This setting motivates us to
align semantic information in unstructured text with the structured, normative
elements of regulations. To this end, we introduce GraphCompliance, a framework
that represents regulatory texts as a Policy Graph and runtime contexts as a
Context Graph, and aligns them. In this formulation, the policy graph encodes
normative structure and cross-references, whereas the context graph formalizes
events as subject-action-object (SAO) and entity-relation triples. This
alignment anchors the reasoning of a judge large language model (LLM) in
structured information and helps reduce the burden of regulatory interpretation
and event parsing, enabling a focus on the core reasoning step. In experiments
on 300 GDPR-derived real-world scenarios spanning five evaluation tasks,
GraphCompliance yields 4.1-7.2 percentage points (pp) higher micro-F1 than
LLM-only and RAG baselines, with fewer under- and over-predictions, resulting
in higher recall and lower false positive rates. Ablation studies indicate
contributions from each graph component, suggesting that structured
representations and a judge LLM are complementary for normative reasoning.

</details>


### [109] [Humains-Junior: A 3.8B Language Model Achieving GPT-4o-Level Factual Accuracy by Directed Exoskeleton Reasoning](https://arxiv.org/abs/2510.25933)
*Nissan Yaron,Dan Bystritsky,Ben-Etzion Yaron*

Main category: cs.AI

TL;DR: 一个3.8B的模型达到了GPT-4o级别的FACTS准确率，成本更低。


<details>
  <summary>Details</summary>
Motivation: 探索低成本、高性能的语言模型。

Method: 结合最小定向“外骨骼推理”支架与行为微调，训练协议遵从性。

Result: 在FACTS Grounding公共子集上，Humans-Junior与GPT-4o在$\\\pm 5$ pp等效范围内匹配。云定价显示成本降低约19倍，自托管/边缘部署可以接近零边际成本。

Conclusion: 3.8B模型在FACTS准确率上可与GPT-4o媲美，且成本效益显著。

Abstract: We introduce Humans-Junior, a 3.8B model that matches GPT-4o on the FACTS
Grounding public subset within a $\pm 5$ pp equivalence margin.
  Results. On Q1--Q500 under identical judges, GPT-4o scores 73.5% (95% CI
69.5--77.2) and Humans-Junior 72.7% (95% CI 68.7--76.5); the paired difference
is 0.8 pp (bootstrap 95% CI $-3.1$ to $+4.7$; permutation $p = 0.72$; Cohen's
$d = 0.023$). TOST establishes equivalence at $\pm 5$ pp (not at $\pm 3$ pp).
When purchased as managed APIs, Humans-Junior's base model
(Phi-3.5-mini-instruct) is $\approx 19\times$ less expensive than GPT-4o on
Microsoft AI Foundry pricing; self-hosted or edge deployments can drive
incremental inference cost toward zero. Measured vs estimated pricing sources
are tabulated in Appendix E.
  Method. Our approach combines minimal directed "Exoskeleton Reasoning"
scaffolds with behavioral fine-tuning that teaches protocol compliance
(epistemic discipline) rather than domain answers. Fine-tuning alone adds
little; combined, they synergize (+17.7 pp, $p < 0.001$) and reduce variance
($\approx 25\%$). In prompt-only settings on frontier models (Q1--Q100;
non-comparable), directed reasoning improved GPT-4o by +11.8 pp to 85.3% and
Gemini-2.5-Pro by +5.0 pp to 93.3% (baseline 88.3%, $n = 100$); see Section~5.
  TL;DR. A 3.8B model achieves GPT-4o-level FACTS accuracy (equivalent within
$\pm 5$ pp on Q1--Q500). Cloud pricing shows $\approx 19\times$ lower cost
versus GPT-4o, and self-hosted/edge deployments can approach zero marginal
cost. Pricing sources are listed in Appendix E. Frontier prompt-only gains
(Q1--Q100; non-comparable) and optimized-prompt exploratory results under
earlier judges are summarized in Appendix F.
  Keywords: Small Language Models, Factual Grounding, Directed Reasoning,
Fine-Tuning, Model Alignment, Cost-Efficient AI

</details>


### [110] [LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human Smuggling Networks](https://arxiv.org/abs/2510.26486)
*Dipak Meher,Carlotta Domeniconi,Guadalupe Correa-Cabrera*

Main category: cs.AI

TL;DR: 提出LINK-KG，一个模块化框架，集成了三阶段、llm引导的共指消解管道与下游知识图谱提取。


<details>
  <summary>Details</summary>
Motivation: 人工走私网络复杂且不断发展，难以全面分析。法律案例文件提供了丰富的关于这些网络的真实和程序性见解，但通常很长、非结构化，并且充满了模糊或变化的引用，这给自动知识图谱(KG)的构建带来了巨大的挑战。现有方法要么忽略了共指消解，要么无法扩展到短文本范围之外，导致图谱碎片化和实体链接不一致。

Method: LINK-KG集成了三阶段、llm引导的共指消解管道与下游知识图谱提取。该方法的核心是一个特定类型的Prompt Cache，它可以持续跟踪和解析文档块中的引用，从而为从短篇和长篇法律文本构建结构化知识图谱提供清晰且明确的叙述。

Result: LINK-KG比基线方法减少了45.21%的平均节点重复率和32.22%的噪声节点，从而产生了更清晰、更连贯的图结构。

Conclusion: 这些改进使LINK-KG成为分析复杂犯罪网络的强大基础。

Abstract: Human smuggling networks are complex and constantly evolving, making them
difficult to analyze comprehensively. Legal case documents offer rich factual
and procedural insights into these networks but are often long, unstructured,
and filled with ambiguous or shifting references, posing significant challenges
for automated knowledge graph (KG) construction. Existing methods either
overlook coreference resolution or fail to scale beyond short text spans,
leading to fragmented graphs and inconsistent entity linking. We propose
LINK-KG, a modular framework that integrates a three-stage, LLM-guided
coreference resolution pipeline with downstream KG extraction. At the core of
our approach is a type-specific Prompt Cache, which consistently tracks and
resolves references across document chunks, enabling clean and disambiguated
narratives for structured knowledge graph construction from both short and long
legal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes
by 32.22% compared to baseline methods, resulting in cleaner and more coherent
graph structures. These improvements establish LINK-KG as a strong foundation
for analyzing complex criminal networks.

</details>


### [111] [Estimating cognitive biases with attention-aware inverse planning](https://arxiv.org/abs/2510.25951)
*Sounak Banerjee,Daphne Cornelisse,Deepak Gopinath,Emily Sumner,Jonathan DeCastro,Guy Rosman,Eugene Vinitsky,Mark K. Ho*

Main category: cs.AI

TL;DR: 本文提出了一种新的逆向规划方法，可以从人类行为中推断其注意力偏差。


<details>
  <summary>Details</summary>
Motivation: 人类的有目标行为受到认知偏差的影响，与人互动的自主系统应该意识到这一点。例如，人们对环境中物体的注意力会产生偏差，从而系统地影响他们执行日常任务（如开车上班）的方式。

Method: 本文结合深度强化学习和计算认知建模，提出了一种注意力感知逆向规划方法。

Result: 本文使用该方法推断了 Waymo 开放数据集中真实驾驶场景中强化学习智能体的注意力策略，证明了使用注意力感知逆向规划估计认知偏差的可扩展性。

Conclusion: 注意力感知逆向规划能够有效地从行为中推断认知偏差。

Abstract: People's goal-directed behaviors are influenced by their cognitive biases,
and autonomous systems that interact with people should be aware of this. For
example, people's attention to objects in their environment will be biased in a
way that systematically affects how they perform everyday tasks such as driving
to work. Here, building on recent work in computational cognitive science, we
formally articulate the attention-aware inverse planning problem, in which the
goal is to estimate a person's attentional biases from their actions. We
demonstrate how attention-aware inverse planning systematically differs from
standard inverse reinforcement learning and how cognitive biases can be
inferred from behavior. Finally, we present an approach to attention-aware
inverse planning that combines deep reinforcement learning with computational
cognitive modeling. We use this approach to infer the attentional strategies of
RL agents in real-life driving scenarios selected from the Waymo Open Dataset,
demonstrating the scalability of estimating cognitive biases with
attention-aware inverse planning.

</details>


### [112] [From Queries to Insights: Agentic LLM Pipelines for Spatio-Temporal Text-to-SQL](https://arxiv.org/abs/2510.25997)
*Manu Redd,Tao Zhe,Dongjie Wang*

Main category: cs.AI

TL;DR: 本文介绍了一种 agentic pipeline，它通过 Mistral-based ReAct agent 来扩展 text-to-SQL baseline，以解决现实的时空查询问题。


<details>
  <summary>Details</summary>
Motivation: 现有的 NL-to-SQL 系统在处理现实的时空查询时存在困难，因为它们需要将模糊的用户措辞与 schema-specific categories 对齐，处理时间推理，并选择合适的输出。

Method: 该方法通过一个 Mistral-based ReAct agent 来编排一个 text-to-SQL baseline (llama-3-sqlcoder-8b)，该 agent 可以通过 schema inspection、SQL 生成、执行和可视化工具来计划、分解和调整查询。

Result: 在 NYC 和 Tokyo check-in 数据集上的 35 个自然语言查询的评估结果表明，该 agent 的准确率明显高于 naive baseline (91.4% vs. 28.6%)，并且通过地图、绘图和结构化的自然语言摘要提高了可用性。

Conclusion: Agentic orchestration，而不是更强大的 SQL 生成器，是交互式地理空间助手的一个有希望的基础。

Abstract: Natural-language-to-SQL (NL-to-SQL) systems hold promise for democratizing
access to structured data, allowing users to query databases without learning
SQL. Yet existing systems struggle with realistic spatio-temporal queries,
where success requires aligning vague user phrasing with schema-specific
categories, handling temporal reasoning, and choosing appropriate outputs. We
present an agentic pipeline that extends a naive text-to-SQL baseline
(llama-3-sqlcoder-8b) with orchestration by a Mistral-based ReAct agent. The
agent can plan, decompose, and adapt queries through schema inspection, SQL
generation, execution, and visualization tools. We evaluate on 35
natural-language queries over the NYC and Tokyo check-in dataset, covering
spatial, temporal, and multi-dataset reasoning. The agent achieves
substantially higher accuracy than the naive baseline 91.4% vs. 28.6% and
enhances usability through maps, plots, and structured natural-language
summaries. Crucially, our design enables more natural human-database
interaction, supporting users who lack SQL expertise, detailed schema
knowledge, or prompting skill. We conclude that agentic orchestration, rather
than stronger SQL generators alone, is a promising foundation for interactive
geospatial assistants.

</details>


### [113] [AutoSurvey2: Empowering Researchers with Next Level Automated Literature Surveys](https://arxiv.org/abs/2510.26012)
*Siyi Wu,Chiaxin Liang,Ziqian Bi,Leyi Zhao,Tianyang Wang,Junhao Song,Yichao Zhang,Keyu Chen,Xinyuan Song*

Main category: cs.AI

TL;DR: autosurvey2: A multi-stage pipeline that automates survey generation through retrieval-augmented synthesis and structured evaluation.


<details>
  <summary>Details</summary>
Motivation: Producing comprehensive and current survey papers is increasingly difficult due to the rapid growth of research literature, particularly in large language models (LLMs).

Method: The system integrates parallel section generation, iterative refinement, and real-time retrieval of recent publications.

Result: autosurvey2 consistently outperforms existing retrieval-based and automated baselines, achieving higher scores in structural coherence and topical relevance while maintaining strong citation fidelity.

Conclusion: autosurvey2 provides a scalable and reproducible solution for generating long-form academic surveys and contributes a solid foundation for future research on automated scholarly writing.

Abstract: The rapid growth of research literature, particularly in large language
models (LLMs), has made producing comprehensive and current survey papers
increasingly difficult. This paper introduces autosurvey2, a multi-stage
pipeline that automates survey generation through retrieval-augmented synthesis
and structured evaluation. The system integrates parallel section generation,
iterative refinement, and real-time retrieval of recent publications to ensure
both topical completeness and factual accuracy. Quality is assessed using a
multi-LLM evaluation framework that measures coverage, structure, and relevance
in alignment with expert review standards. Experimental results demonstrate
that autosurvey2 consistently outperforms existing retrieval-based and
automated baselines, achieving higher scores in structural coherence and
topical relevance while maintaining strong citation fidelity. By combining
retrieval, reasoning, and automated evaluation into a unified framework,
autosurvey2 provides a scalable and reproducible solution for generating
long-form academic surveys and contributes a solid foundation for future
research on automated scholarly writing. All code and resources are available
at https://github.com/annihi1ation/auto_research.

</details>


### [114] [Large Language Model-assisted Autonomous Vehicle Recovery from Immobilization](https://arxiv.org/abs/2510.26023)
*Zhipeng Bao,Qianwen Li*

Main category: cs.AI

TL;DR: 提出了一种新的基于大型语言模型（LLM）的自动驾驶汽车（AV）恢复框架，名为StuckSolver，以解决AV在某些交通场景中遇到的挑战，提高通行效率。


<details>
  <summary>Details</summary>
Motivation: 现有的AV恢复方案（如远程干预和人工接管）存在成本高、效率低、适用性差等问题。

Method: StuckSolver作为一个插件模块，与AV的感知-规划-控制堆栈对接，通过传感器数据检测静止状态，解释环境，并生成高级恢复指令。

Result: StuckSolver在Bench2Drive基准测试和自定义的不确定性场景中表现出色，仅通过自主推理即可实现接近最先进的性能，并在结合乘客指导后得到进一步提升。

Conclusion: StuckSolver能够通过自主推理和/或乘客引导的决策，帮助AV解决静止问题。

Abstract: Despite significant advancements in recent decades, autonomous vehicles (AVs)
continue to face challenges in navigating certain traffic scenarios where human
drivers excel. In such situations, AVs often become immobilized, disrupting
overall traffic flow. Current recovery solutions, such as remote intervention
(which is costly and inefficient) and manual takeover (which excludes
non-drivers and limits AV accessibility), are inadequate. This paper introduces
StuckSolver, a novel Large Language Model (LLM) driven recovery framework that
enables AVs to resolve immobilization scenarios through self-reasoning and/or
passenger-guided decision-making. StuckSolver is designed as a plug-in add-on
module that operates on top of the AV's existing perception-planning-control
stack, requiring no modification to its internal architecture. Instead, it
interfaces with standard sensor data streams to detect immobilization states,
interpret environmental context, and generate high-level recovery commands that
can be executed by the AV's native planner. We evaluate StuckSolver on the
Bench2Drive benchmark and in custom-designed uncertainty scenarios. Results
show that StuckSolver achieves near-state-of-the-art performance through
autonomous self-reasoning alone and exhibits further improvements when
passenger guidance is incorporated.

</details>


### [115] [Can AI be Accountable?](https://arxiv.org/abs/2510.26057)
*Andrew L. Kun*

Main category: cs.AI

TL;DR: 探讨了人工智能的可问责性问题，强调了为了服务于消费者、选民和决策者的需求，人工智能必须是可问责的。


<details>
  <summary>Details</summary>
Motivation: 指出当前人工智能在很多情况下是不可问责的，无法对其进行质疑、讨论甚至制裁。

Method: 将可问责性的一般定义与人工智能联系起来，并通过举例说明人工智能可问责和不可问责的含义。

Result: 探索了可以提高我们生活在一个所有人工智能都对其影响者负责的世界的可能性的方法。

Conclusion: 强调了确保人工智能对其影响者负责的重要性。

Abstract: The AI we use is powerful, and its power is increasing rapidly. If this
powerful AI is to serve the needs of consumers, voters, and decision makers,
then it is imperative that the AI is accountable. In general, an agent is
accountable to a forum if the forum can request information from the agent
about its actions, if the forum and the agent can discuss this information, and
if the forum can sanction the agent. Unfortunately, in too many cases today's
AI is not accountable -- we cannot question it, enter into a discussion with
it, let alone sanction it. In this chapter we relate the general definition of
accountability to AI, we illustrate what it means for AI to be accountable and
unaccountable, and we explore approaches that can improve our chances of living
in a world where all AI is accountable to those who are affected by it.

</details>


### [116] [Lean4Physics: Comprehensive Reasoning Framework for College-level Physics in Lean4](https://arxiv.org/abs/2510.26094)
*Yuxin Li,Minghao Liu,Ruida Wang,Wenzhao Ji,Zhitao He,Rui Pan,Junming Huang,Tong Zhang,Yi R. Fung*

Main category: cs.AI

TL;DR: 本文介绍了Lean4PHYS，一个用于Lean4中大学水平物理问题的综合推理框架。


<details>
  <summary>Details</summary>
Motivation: 为了在Lean4中进行形式物理推理，构建一个全面的推理框架。

Method: 创建了一个包含200个手工制作和同行评审的陈述的大学水平基准测试LeanPhysBench，并引入了PhysLib，一个包含基本单位系统和定理的社区驱动存储库。

Result: DeepSeek-Prover-V2-7B在LeanPhysBench上实现了16%的性能，Claude-Sonnet-4达到了35%。PhysLib使模型性能平均提高了11.75%。

Conclusion: LeanPhysBench具有挑战性，PhysLib是有效的。这是第一个在Lean4中提供物理基准的研究。

Abstract: We present **Lean4PHYS**, a comprehensive reasoning framework for
college-level physics problems in Lean4. **Lean4PHYS** includes
*LeanPhysBench*, a college-level benchmark for formal physics reasoning in
Lean4, which contains 200 hand-crafted and peer-reviewed statements derived
from university textbooks and physics competition problems. To establish a
solid foundation for formal reasoning in physics, we also introduce *PhysLib*,
a community-driven repository containing fundamental unit systems and theorems
essential for formal physics reasoning. Based on the benchmark and Lean4
repository we composed in **Lean4PHYS**, we report baseline results using major
expert Math Lean4 provers and state-of-the-art closed-source models, with the
best performance of DeepSeek-Prover-V2-7B achieving only 16% and
Claude-Sonnet-4 achieving 35%. We also conduct a detailed analysis showing that
our *PhysLib* can achieve an average improvement of 11.75% in model
performance. This demonstrates the challenging nature of our *LeanPhysBench*
and the effectiveness of *PhysLib*. To the best of our knowledge, this is the
first study to provide a physics benchmark in Lean4.

</details>


### [117] [GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in GUI Tasks](https://arxiv.org/abs/2510.26098)
*Chenrui Shi,Zedong Yu,Zhi Gao,Ruining Feng,Enqi Liu,Yuwei Wu,Yunde Jia,Liuyu Xiang,Zhaofeng He,Qing Li*

Main category: cs.AI

TL;DR: 大型视觉语言模型（VLMs）在图形用户界面（GUI）任务自动化方面取得了进展，但仍落后于人类。通过分析GUI任务执行中常见的失败模式，将GUI知识提炼为三个维度：(1) 界面感知，关于识别widgets和系统状态的知识；(2) 交互预测，关于推理动作状态转换的知识；(3) 指令理解，关于计划、验证和评估任务完成进度的知识。引入GUI Knowledge Bench，一个包含跨六个平台和292个应用程序的选择题和是/否题的基准。评估表明，当前的VLMs可以识别widget功能，但在感知系统状态、预测动作和验证任务完成方面存在困难。在真实GUI任务上的实验进一步验证了GUI知识与任务成功之间的密切联系。通过为评估GUI知识提供一个结构化框架，这项工作支持在下游训练之前选择具有更大潜力的VLMs，并为构建更强大的GUI代理提供见解。


<details>
  <summary>Details</summary>
Motivation: 现有训练方案（如监督微调和强化学习）无法完全解决的核心GUI知识缺失的问题。

Method: 通过分析GUI任务执行中常见的失败模式，将GUI知识提炼为三个维度，并引入GUI Knowledge Bench基准。

Result: 当前的VLMs可以识别widget功能，但在感知系统状态、预测动作和验证任务完成方面存在困难。在真实GUI任务上的实验进一步验证了GUI知识与任务成功之间的密切联系。

Conclusion: 这项工作支持在下游训练之前选择具有更大潜力的VLMs，并为构建更强大的GUI代理提供见解。

Abstract: Large vision language models (VLMs) have advanced graphical user interface
(GUI) task automation but still lag behind humans. We hypothesize this gap
stems from missing core GUI knowledge, which existing training schemes (such as
supervised fine tuning and reinforcement learning) alone cannot fully address.
By analyzing common failure patterns in GUI task execution, we distill GUI
knowledge into three dimensions: (1) interface perception, knowledge about
recognizing widgets and system states; (2) interaction prediction, knowledge
about reasoning action state transitions; and (3) instruction understanding,
knowledge about planning, verifying, and assessing task completion progress. We
further introduce GUI Knowledge Bench, a benchmark with multiple choice and
yes/no questions across six platforms (Web, Android, MacOS, Windows, Linux,
IOS) and 292 applications. Our evaluation shows that current VLMs identify
widget functions but struggle with perceiving system states, predicting
actions, and verifying task completion. Experiments on real world GUI tasks
further validate the close link between GUI knowledge and task success. By
providing a structured framework for assessing GUI knowledge, our work supports
the selection of VLMs with greater potential prior to downstream training and
provides insights for building more capable GUI agents.

</details>


### [118] [Beyond Benchmarks: The Economics of AI Inference](https://arxiv.org/abs/2510.26136)
*Boqin Zhuang,Jiacheng Qiao,Mingqian Liu,Mingxing Yu,Ping Hong,Rui Li,Xiaoxia Song,Xiangjun Xu,Xu Chen,Yaoyao Ma,Yujie Gao*

Main category: cs.AI

TL;DR: 本文提出了一个量化的“推理经济学”框架，将LLM推理过程视为计算驱动的智能生产活动。


<details>
  <summary>Details</summary>
Motivation: 确定LLM的商业可行性和广泛采用的关键因素是LLM的推理成本。

Method: 基于WiNEval-3.0的经验数据，构建了第一个“LLM推理生产前沿”。

Result: 揭示了三个原则：边际成本递减、规模报酬递减和最佳成本效益区。

Conclusion: 本文不仅为模型部署决策提供了经济基础，也为未来基于市场的AI推理资源定价和优化奠定了经验基础。

Abstract: The inference cost of Large Language Models (LLMs) has become a critical
factor in determining their commercial viability and widespread adoption. This
paper introduces a quantitative ``economics of inference'' framework, treating
the LLM inference process as a compute-driven intelligent production activity.
We analyze its marginal cost, economies of scale, and quality of output under
various performance configurations. Based on empirical data from WiNEval-3.0,
we construct the first ``LLM Inference Production Frontier,'' revealing three
principles: diminishing marginal cost, diminishing returns to scale, and an
optimal cost-effectiveness zone. This paper not only provides an economic basis
for model deployment decisions but also lays an empirical foundation for the
future market-based pricing and optimization of AI inference resources.

</details>


### [119] [Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math](https://arxiv.org/abs/2510.26143)
*Bo Pang,Deqian Kong,Silvio Savarese,Caiming Xiong,Yingbo Zhou*

Main category: cs.AI

TL;DR: 提出了一种简单的两阶段课程，首先在与预训练对齐的领域（如数学）中激发大型语言模型（LLM）的推理能力，然后通过联合强化学习在其他领域调整和完善这些技能。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）可以激发大型语言模型（LLM）的强大推理能力，但大多数开放性工作都集中在数学和代码方面。

Method: 首先进行简短的冷启动，然后进行仅限数学的RL，以开发推理技能。第二阶段在混合领域数据上运行联合RL，以转移和巩固这些技能。

Result: 在Qwen3-4B和Llama-3.1-8B上进行评估，推理课程在多领域套件中产生了一致的收益。

Conclusion: 推理课程提供了一种紧凑、易于采用的通用推理方法。

Abstract: Reinforcement learning (RL) can elicit strong reasoning in large language
models (LLMs), yet most open efforts focus on math and code. We propose
Reasoning Curriculum, a simple two-stage curriculum that first elicits
reasoning skills in pretraining-aligned domains such as math, then adapts and
refines these skills across other domains via joint RL. Stage 1 performs a
brief cold start and then math-only RL with verifiable rewards to develop
reasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and
consolidate these skills. The curriculum is minimal and backbone-agnostic,
requiring no specialized reward models beyond standard verifiability checks.
Evaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning
curriculum yields consistent gains. Ablations and a cognitive-skill analysis
indicate that both stages are necessary and that math-first elicitation
increases cognitive behaviors important for solving complex problems. Reasoning
Curriculum provides a compact, easy-to-adopt recipe for general reasoning.

</details>


### [120] [The FM Agent](https://arxiv.org/abs/2510.26144)
*Annan Li,Chufan Wu,Zengle Ge,Yee Hin Chong,Zhinan Hou,Lizhe Cao,Cheng Ju,Jianmin Wu,Huaiming Li,Haobo Zhang,Shenghao Feng,Mo Zhao,Fengzhi Qiu,Rui Yang,Mengmeng Zhang,Wenyi Zhu,Yingying Sun,Quan Sun,Shunhao Yan,Danyu Liu,Dawei Yin,Dou Shen*

Main category: cs.AI

TL;DR: FM Agent: 一个利用LLM推理和大规模进化搜索解决复杂现实问题的通用多代理框架，无需人工干预即可在多个领域达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 构建自主AI研究代理以加速科学和工程发现。

Method: 结合专家指导的冷启动初始化、创新进化采样策略、领域特定评估器（结合正确性、有效性和LLM监督反馈）以及基于Ray的分布式异步执行基础设施。

Result: 在ALE-Bench、MLE-Bench、KernelBench等多个领域达到SOTA，并在经典数学问题上取得新的SOTA结果。

Conclusion: FM Agent在企业研发和基础科学研究中具有巨大潜力，能够加速创新、自动化复杂发现过程，并带来显著的工程和科学进步。

Abstract: Large language models (LLMs) are catalyzing the development of autonomous AI
research agents for scientific and engineering discovery. We present FM Agent,
a novel and general-purpose multi-agent framework that leverages a synergistic
combination of LLM-based reasoning and large-scale evolutionary search to
address complex real-world challenges. The core of FM Agent integrates several
key innovations: 1) a cold-start initialization phase incorporating expert
guidance, 2) a novel evolutionary sampling strategy for iterative optimization,
3) domain-specific evaluators that combine correctness, effectiveness, and
LLM-supervised feedback, and 4) a distributed, asynchronous execution
infrastructure built on Ray. Demonstrating broad applicability, our system has
been evaluated across diverse domains, including operations research, machine
learning, GPU kernel optimization, and classical mathematical problems. FM
Agent reaches state-of-the-art results autonomously, without human
interpretation or tuning -- 1976.3 on ALE-Bench (+5.2\%), 43.56\% on MLE-Bench
(+4.0pp), up to 20x speedups on KernelBench, and establishes new
state-of-the-art(SOTA) results on several classical mathematical problems.
Beyond academic benchmarks, FM Agent shows considerable promise for both
large-scale enterprise R\&D workflows and fundamental scientific research,
where it can accelerate innovation, automate complex discovery processes, and
deliver substantial engineering and scientific advances with broader societal
impact.

</details>


### [121] [One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient Reasoning](https://arxiv.org/abs/2510.26167)
*Renhao Li,Jianhong Tu,Yang Su,Hamid Alinejad-Rokny,Derek F. Wong,Junyang Lin,Min Yang*

Main category: cs.AI

TL;DR: ToolRM是一系列轻量级的生成式奖励模型，专为通用工具使用场景定制，旨在提升大型语言模型在工具学习方面的能力。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏专门为函数调用任务设计的奖励模型，限制了工具学习的进展。

Method: 提出了一个新颖的pipeline，使用基于规则的评分和多维采样构建成对偏好数据，生成了ToolPref-Pairwise-30K数据集。此外，还构建了TRBench$_{BFCL}$基准来评估工具使用奖励模型。

Result: 在ToolPref-Pairwise-30K数据集上训练的Qwen3-4B/8B系列模型，在成对奖励判断中，准确率比Claude 4和OpenAI o3等领先模型高出14.28%。ToolRM还能推广到更广泛的评判任务，例如Best-of-N抽样和自我纠正。在ACEBench上的实验表明，ToolRM能够实现推理时扩展，并减少超过66%的输出token使用。

Conclusion: 发布了数据和模型检查点，以促进未来的研究。ToolRM在工具学习领域具有有效性和效率。

Abstract: Reward models (RMs) play a critical role in aligning large language models
(LLMs) with human preferences. Yet in the domain of tool learning, the lack of
RMs specifically designed for function-calling tasks has limited progress
toward more capable agentic AI. We introduce ToolRM, a family of lightweight
generative RMs tailored for general tool-use scenarios. To build these models,
we propose a novel pipeline that constructs pairwise preference data using
rule-based scoring and multidimensional sampling. This yields
ToolPref-Pairwise-30K, a diverse, balanced, and challenging dataset of critique
tasks that supports reinforcement learning with verifiable feedback. To
evaluate tool-use RMs, we also introduce TRBench$_{BFCL}$, a benchmark built on
the agentic evaluation suite BFCL. Trained on our constructed data, models from
the Qwen3-4B/8B series achieve up to 14.28% higher accuracy, substantially
outperforming frontier models such as Claude 4 and OpenAI o3 in pairwise reward
judgments. Beyond training objectives, ToolRM generalizes to broader critique
tasks, including Best-of-N sampling and self-correction. Experiments on
ACEBench highlight its effectiveness and efficiency, enabling inference-time
scaling and reducing output token usage by over 66%. We release data and model
checkpoints to facilitate future research.

</details>


### [122] [Questionnaire meets LLM: A Benchmark and Empirical Study of Structural Skills for Understanding Questions and Responses](https://arxiv.org/abs/2510.26238)
*Duc-Hai Nguyen,Vijayakumar Nanjappan,Barry O'Sullivan,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: 论文提出了QASU基准，用于评估LLM在问卷分析中的结构理解能力，并发现格式和提示的选择对LLM的准确性有显著影响。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在处理问卷数据方面的能力尚未被充分探索，现有的调查分析工具难以与LLM集成，导致缺乏关于如何最好地表示问卷以供LLM使用的指导。

Method: 论文构建了一个名为QASU的基准，该基准测试了LLM在六种序列化格式和多种提示策略下的六种结构技能，包括答案查找、受访者计数和多跳推理。

Result: 实验表明，选择有效的格式和提示组合可以将准确率提高高达8.8个百分点，并且通过自我增强提示添加轻量级结构提示可以进一步提高3-4个百分点。

Conclusion: 该开放源代码基准通过系统地隔离格式和提示效果，为推进基于LLM的问卷分析的研究和实践提供了一个简单而通用的基础。

Abstract: Millions of people take surveys every day, from market polls and academic
studies to medical questionnaires and customer feedback forms. These datasets
capture valuable insights, but their scale and structure present a unique
challenge for large language models (LLMs), which otherwise excel at few-shot
reasoning over open-ended text. Yet, their ability to process questionnaire
data or lists of questions crossed with hundreds of respondent rows remains
underexplored. Current retrieval and survey analysis tools (e.g., Qualtrics,
SPSS, REDCap) are typically designed for humans in the workflow, limiting such
data integration with LLM and AI-empowered automation. This gap leaves
scientists, surveyors, and everyday users without evidence-based guidance on
how to best represent questionnaires for LLM consumption. We address this by
introducing QASU (Questionnaire Analysis and Structural Understanding), a
benchmark that probes six structural skills, including answer lookup,
respondent count, and multi-hop inference, across six serialization formats and
multiple prompt strategies. Experiments on contemporary LLMs show that choosing
an effective format and prompt combination can improve accuracy by up to 8.8%
points compared to suboptimal formats. For specific tasks, carefully adding a
lightweight structural hint through self-augmented prompting can yield further
improvements of 3-4% points on average. By systematically isolating format and
prompting effects, our open source benchmark offers a simple yet versatile
foundation for advancing both research and real-world practice in LLM-based
questionnaire analysis.

</details>


### [123] [Retrieval Augmented Generation-Enhanced Distributed LLM Agents for Generalizable Traffic Signal Control with Emergency Vehicles](https://arxiv.org/abs/2510.26242)
*Xinhang Li,Qing Guo,Junyu Chen,Zheng Guo,Shengzhe Xu,Lei Li,Lin Zhang*

Main category: cs.AI

TL;DR: 提出了一种名为REG-TSC的检索增强生成（RAG）增强的分布式LLM智能体，用于通用交通信号控制（TSC），特别关注紧急情况下的可靠性和异构交叉口之间的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的交通信号控制方法在紧急情况下容易产生幻觉，导致不可靠的决策，并且难以泛化到不同的交叉口类型。

Method: 提出了一个紧急感知推理框架，该框架动态调整推理深度，并配备了一个基于审查者的紧急RAG（RERAG）来提取历史案例中的知识和指导。设计了一种类型无关的交通表示，并提出了一个奖励引导的强化细化（R3）方法，用于异构交叉口。

Result: 在三个真实世界的道路网络上的大量实验表明，REG-TSC将旅行时间减少了42.00%，排队长度减少了62.31%，紧急车辆等待时间减少了83.16%，优于其他最先进的方法。

Conclusion: REG-TSC在交通信号控制方面表现出色，特别是在紧急情况处理和异构交叉口泛化方面。

Abstract: With increasing urban traffic complexity, Traffic Signal Control (TSC) is
essential for optimizing traffic flow and improving road safety. Large Language
Models (LLMs) emerge as promising approaches for TSC. However, they are prone
to hallucinations in emergencies, leading to unreliable decisions that may
cause substantial delays for emergency vehicles. Moreover, diverse intersection
types present substantial challenges for traffic state encoding and
cross-intersection training, limiting generalization across heterogeneous
intersections. Therefore, this paper proposes Retrieval Augmented Generation
(RAG)-enhanced distributed LLM agents with Emergency response for Generalizable
TSC (REG-TSC). Firstly, this paper presents an emergency-aware reasoning
framework, which dynamically adjusts reasoning depth based on the emergency
scenario and is equipped with a novel Reviewer-based Emergency RAG (RERAG) to
distill specific knowledge and guidance from historical cases, enhancing the
reliability and rationality of agents' emergency decisions. Secondly, this
paper designs a type-agnostic traffic representation and proposes a
Reward-guided Reinforced Refinement (R3) for heterogeneous intersections. R3
adaptively samples training experience from diverse intersections with
environment feedback-based priority and fine-tunes LLM agents with a designed
reward-weighted likelihood loss, guiding REG-TSC toward high-reward policies
across heterogeneous intersections. On three real-world road networks with 17
to 177 heterogeneous intersections, extensive experiments show that REG-TSC
reduces travel time by 42.00%, queue length by 62.31%, and emergency vehicle
waiting time by 83.16%, outperforming other state-of-the-art methods.

</details>


### [124] [Graph-Enhanced Policy Optimization in LLM Agent Training](https://arxiv.org/abs/2510.26270)
*Jiazhen Yuan,Wei Zhao,Zhengbiao Bai*

Main category: cs.AI

TL;DR: 本文提出了一种名为 Graph-Enhanced Policy Optimization (GEPO) 的方法，用于训练多轮交互式 LLM 代理，通过动态构建状态转移图并利用图论中心性来解决结构盲视问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于群体的强化学习方法在训练多轮交互式 LLM 代理时，存在结构盲视问题，导致探索效率低下、信用分配不准确和计划短视。

Method: GEPO 方法动态构建状态转移图，并利用图论中心性提供三个协同学习信号：结构化的内在奖励、图增强的优势函数和动态折扣因子。

Result: 在 ALFWorld、WebShop 和一个专有的 Workbench 基准测试中，GEPO 表现出强大的性能，相对于基线方法，成功率分别提高了 +4.1%、+5.3% 和 +10.9%。

Conclusion: 显式建模环境结构是推进 LLM 代理训练的稳健、通用策略。

Abstract: Group based reinforcement learning (RL) has shown impressive results on
complex reasoning and mathematical tasks. Yet, when applied to train
multi-turn, interactive LLM agents, these methods often suffer from structural
blindness-the inability to exploit the underlying connectivity of the
environment. This manifests in three critical challenges: (1) inefficient,
unguided exploration, (2) imprecise credit assignment due to overlooking
pivotal states, and (3) myopic planning caused by static reward discounting. We
address these issues with Graph-Enhanced Policy Optimization (GEPO), which
dynamically constructs a state-transition graph from agent experience and
employs graph-theoretic centrality to provide three synergistic learning
signals: (1)structured intrinsic rewards that guide exploration toward
high-impact states, (2) a graph-enhanced advantage function for topology-aware
credit assignment, and (3) a dynamic discount factor adapted to each state's
strategic value. On the ALFWorld, WebShop, and a proprietary Workbench
benchmarks, GEPO demonstrates strong performance, achieving absolute success
rate gains of +4.1%, +5.3%, and +10.9% over competitive baselines. These
results highlight that explicitly modeling environmental structure is a robust,
generalizable strategy for advancing LLM agent training.

</details>


### [125] [Discovering State Equivalences in UCT Search Trees By Action Pruning](https://arxiv.org/abs/2510.26346)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 提出了一种新的MCTS状态抽象方法IPA-UCT，该方法在精度上略有损失，但可以找到更多的抽象。


<details>
  <summary>Details</summary>
Motivation: 在噪声或大动作空间环境中，很难找到状态抽象。

Method: 提出一种弱化的状态抽象条件，并将其应用于UCT算法，命名为IPA-UCT。

Result: IPA-UCT在多个测试领域和迭代预算中优于OGA-UCT。

Conclusion: IPA和ASAP是更通用框架p-ASAP的特例，而p-ASAP本身又是ASASAP框架的特例。

Abstract: One approach to enhance Monte Carlo Tree Search (MCTS) is to improve its
sample efficiency by grouping/abstracting states or state-action pairs and
sharing statistics within a group. Though state-action pair abstractions are
mostly easy to find in algorithms such as On the Go Abstractions in Upper
Confidence bounds applied to Trees (OGA-UCT), nearly no state abstractions are
found in either noisy or large action space settings due to constraining
conditions. We provide theoretical and empirical evidence for this claim, and
we slightly alleviate this state abstraction problem by proposing a weaker
state abstraction condition that trades a minor loss in accuracy for finding
many more abstractions. We name this technique Ideal Pruning Abstractions in
UCT (IPA-UCT), which outperforms OGA-UCT (and any of its derivatives) across a
large range of test domains and iteration budgets as experimentally validated.
IPA-UCT uses a different abstraction framework from Abstraction of State-Action
Pairs (ASAP) which is the one used by OGA-UCT, which we name IPA. Furthermore,
we show that both IPA and ASAP are special cases of a more general framework
that we call p-ASAP which itself is a special case of the ASASAP framework.

</details>


### [126] [BOTS: A Unified Framework for Bayesian Online Task Selection in LLM Reinforcement Finetuning](https://arxiv.org/abs/2510.26374)
*Qianli Shen,Daoyuan Chen,Yilun Huang,Zhenqing Ling,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.AI

TL;DR: 提出了一种用于LLM强化微调的贝叶斯在线任务选择框架BOTS，该框架自适应地维护任务难度的后验估计，并结合显式和隐式证据来平衡探索和利用。


<details>
  <summary>Details</summary>
Motivation: 现有任务选择方法通常具有较高的推出成本、较差的适应性或不完整的证据，而均匀任务抽样效率低下，会在琐碎或无法解决的任务上浪费计算。

Method: 提出了一种统一的框架，用于LLM强化微调中的贝叶斯在线任务选择（BOTS）。该框架基于贝叶斯推理，自适应地维护任务难度的后验估计。它共同结合了来自所选任务直接评估的显式证据和从这些评估中推断出的未选择任务的隐式证据，并通过Thompson抽样确保了探索和利用之间的原则性平衡。为了使隐式证据具有实用性，我们使用了一种超轻型基于插值的插件来实例化它，该插件无需额外的推出即可估计未评估任务的难度，从而增加了可忽略的开销。

Result: 在不同的领域和LLM规模上，BOTS始终优于基线和消融，从而为RFT中的动态任务选择提供了一种实用且可扩展的解决方案。

Conclusion: BOTS是一种用于LLM强化微调中动态任务选择的实用且可扩展的解决方案，它通过贝叶斯在线任务选择框架，提高了数据效率和性能。

Abstract: Reinforcement finetuning (RFT) is a key technique for aligning Large Language
Models (LLMs) with human preferences and enhancing reasoning, yet its
effectiveness is highly sensitive to which tasks are explored during training.
Uniform task sampling is inefficient, wasting computation on tasks that are
either trivial or unsolvable, while existing task selection methods often
suffer from high rollout costs, poor adaptivity, or incomplete evidence. We
introduce \textbf{BOTS}, a unified framework for \textbf{B}ayesian
\textbf{O}nline \textbf{T}ask \textbf{S}election in LLM reinforcement
finetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior
estimates of task difficulty as the model evolves. It jointly incorporates
\emph{explicit evidence} from direct evaluations of selected tasks and
\emph{implicit evidence} inferred from these evaluations for unselected tasks,
with Thompson sampling ensuring a principled balance between exploration and
exploitation. To make implicit evidence practical, we instantiate it with an
ultra-light interpolation-based plug-in that estimates difficulties of
unevaluated tasks without extra rollouts, adding negligible overhead.
Empirically, across diverse domains and LLM scales, BOTS consistently improves
data efficiency and performance over baselines and ablations, providing a
practical and extensible solution for dynamic task selection in RFT.

</details>


### [127] [AI Mathematician as a Partner in Advancing Mathematical Discovery -- A Case Study in Homogenization Theory](https://arxiv.org/abs/2510.26380)
*Yuanhang Liu,Beichen Wang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 本研究探讨了人工智能数学家（AIM）系统如何作为研究伙伴在数学研究中运作，而不仅仅是问题解决者。通过人机协作，解决了均匀化理论中的一个难题，实现了完整且可验证的证明。


<details>
  <summary>Details</summary>
Motivation: 人工智能在数学推理方面取得了显著进展，但其在数学研究实践中的整合仍然有限。本研究旨在探索如何将AI系统融入数学研究。

Method: 通过迭代地将问题分解为易于处理的子目标，选择适当的分析方法，并验证中间结果，揭示了人类直觉和机器计算如何相互补充。采用了人机协同推理的方法。

Result: 通过人机协作，实现了完整且可验证的证明。这种合作模式提高了证明的可靠性、透明性和可解释性。

Conclusion: 系统性的人机协同推理可以推动数学发现的前沿。

Abstract: Artificial intelligence (AI) has demonstrated impressive progress in
mathematical reasoning, yet its integration into the practice of mathematical
research remains limited. In this study, we investigate how the AI
Mathematician (AIM) system can operate as a research partner rather than a mere
problem solver. Focusing on a challenging problem in homogenization theory, we
analyze the autonomous reasoning trajectories of AIM and incorporate targeted
human interventions to structure the discovery process. Through iterative
decomposition of the problem into tractable subgoals, selection of appropriate
analytical methods, and validation of intermediate results, we reveal how human
intuition and machine computation can complement one another. This
collaborative paradigm enhances the reliability, transparency, and
interpretability of the resulting proofs, while retaining human oversight for
formal rigor and correctness. The approach leads to a complete and verifiable
proof, and more broadly, demonstrates how systematic human-AI co-reasoning can
advance the frontier of mathematical discovery.

</details>


### [128] [Scales++: Compute Efficient Evaluation Subset Selection with Cognitive Scales Embeddings](https://arxiv.org/abs/2510.26384)
*Andrew M. Bean,Nabeel Seedat,Shengzhuang Chen,Jonathan Richard Schwarz*

Main category: cs.AI

TL;DR: 本文提出了一种新的LLM评估基准子集选择方法，该方法以项目为中心，而不是以模型为中心，从而降低了成本并提高了效率。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）评估成本高昂，需要创建小的但具有代表性的数据子集（即微型基准）以实现高效评估，同时保持预测准确性。现有的方法以模型为中心，存在前期成本高、无法立即处理新基准（“冷启动”）以及未来模型将与其前身共享失败模式的脆弱假设等局限性。

Method: 本文提出了一种以项目为中心的方法Scales++，该方法基于基准样本的认知需求进行数据选择。

Result: Scales++ 将前期选择成本降低了 18 倍以上，同时实现了具有竞争力的预测准确性。在 Open LLM 排行榜上，仅使用 0.5% 的数据子集，就可以预测完整基准分数，平均绝对误差为 2.9%。

Conclusion: 本文提出的以项目为中心的方法能够更有效地进行模型评估，而不会显着降低准确性，同时还提供更好的冷启动性能和更易于解释的基准。

Abstract: The prohibitive cost of evaluating large language models (LLMs) on
comprehensive benchmarks necessitates the creation of small yet representative
data subsets (i.e., tiny benchmarks) that enable efficient assessment while
retaining predictive fidelity. Current methods for this task operate under a
model-centric paradigm, selecting benchmarking items based on the collective
performance of existing models. Such approaches are limited by large upfront
costs, an inability to immediately handle new benchmarks (`cold-start'), and
the fragile assumption that future models will share the failure patterns of
their predecessors. In this work, we challenge this paradigm and propose a
item-centric approach to benchmark subset selection, arguing that selection
should be based on the intrinsic properties of the task items themselves,
rather than on model-specific failure patterns. We instantiate this
item-centric efficient benchmarking approach via a novel method, Scales++,
where data selection is based on the cognitive demands of the benchmark
samples. Empirically, we show Scales++ reduces the upfront selection cost by
over 18x while achieving competitive predictive fidelity. On the Open LLM
Leaderboard, using just a 0.5\% data subset, we predict full benchmark scores
with a 2.9% mean absolute error. We demonstrate that this item-centric approach
enables more efficient model evaluation without significant fidelity
degradation, while also providing better cold-start performance and more
interpretable benchmarking.

</details>


### [129] [A Pragmatic View of AI Personhood](https://arxiv.org/abs/2510.26396)
*Joel Z. Leibo,Alexander Sasha Vezhnevets,William A. Cunningham,Stanley M. Bileschi*

Main category: cs.AI

TL;DR: 本文提出了一种务实的框架，通过将人格视为一种灵活的义务（权利和责任）捆绑，而不是需要发现的形而上学属性，从而驾驭这种多样化。


<details>
  <summary>Details</summary>
Motivation: 旨在解决agentic人工智能（AI）涌现所引发的“寒武纪大爆发”式的新型人格问题。

Method: 通过将传统的人格义务捆绑进行解绑，为不同的背景创建定制的解决方案。

Result: 创建了实用的工具，例如通过创建可以制裁的目标“个体”来促进AI合同的签订，而无需解决关于AI意识或理性的棘手辩论。

Conclusion: 通过拒绝为人格的单一、本质定义的基础主义探索，本文提供了一种更实用和灵活的方式来思考如何将AI代理融入我们的社会。

Abstract: The emergence of agentic Artificial Intelligence (AI) is set to trigger a
"Cambrian explosion" of new kinds of personhood. This paper proposes a
pragmatic framework for navigating this diversification by treating personhood
not as a metaphysical property to be discovered, but as a flexible bundle of
obligations (rights and responsibilities) that societies confer upon entities
for a variety of reasons, especially to solve concrete governance problems. We
argue that this traditional bundle can be unbundled, creating bespoke solutions
for different contexts. This will allow for the creation of practical tools --
such as facilitating AI contracting by creating a target "individual" that can
be sanctioned -- without needing to resolve intractable debates about an AI's
consciousness or rationality. We explore how individuals fit in to social roles
and discuss the use of decentralized digital identity technology, examining
both "personhood as a problem", where design choices can create "dark patterns"
that exploit human social heuristics, and "personhood as a solution", where
conferring a bundle of obligations is necessary to ensure accountability or
prevent conflict. By rejecting foundationalist quests for a single, essential
definition of personhood, this paper offers a more pragmatic and flexible way
to think about integrating AI agents into our society.

</details>


### [130] [Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback in Programming Education](https://arxiv.org/abs/2510.26402)
*Vikrant Sahu,Gagan Raj Gupta,Raghav Borikar,Nitin Mane*

Main category: cs.AI

TL;DR: Autograder+通过AI驱动的反馈、语义聚类和可视化来减少教师工作量，同时支持有针对性的指导并促进更好的学习成果。


<details>
  <summary>Details</summary>
Motivation: 传统的评估工具已经无法满足快速增长的编程教育的需求，教师们缺乏有效的手段来提供有意义的、可扩展的反馈。传统的自动评分器虽然高效，但就像黑盒系统一样，只能返回通过/失败的结果，无法深入了解学生的想法或学习需求。

Method: Autograder+旨在将自动评分从纯粹的总结性过程转变为形成性学习体验。它引入了两个关键功能：使用微调的大型语言模型自动生成反馈，以及可视化学生代码提交以揭示学习模式。该模型在精选的学生代码和专家反馈上进行微调，以确保在教学上保持一致且具有上下文意识的指导。

Result: 在对来自多个编程任务的 600 份学生提交的评估中，该系统产生的反馈与教师评论具有很强的语义一致性。对于可视化，在 1,000 份带注释的提交上训练的对比学习代码嵌入能够根据功能和方法将解决方案分组为有意义的集群。该系统还支持提示池，允许教师通过选择的提示模板来指导反馈风格。

Conclusion: 通过集成 AI 驱动的反馈、语义聚类和交互式可视化，Autograder+ 减少了教师的工作量，同时支持有针对性的指导并促进更好的学习成果。

Abstract: The rapid growth of programming education has outpaced traditional assessment
tools, leaving faculty with limited means to provide meaningful, scalable
feedback. Conventional autograders, while efficient, act as black-box systems
that simply return pass/fail results, offering little insight into student
thinking or learning needs.
  Autograder+ is designed to shift autograding from a purely summative process
to a formative learning experience. It introduces two key capabilities:
automated feedback generation using a fine-tuned Large Language Model, and
visualization of student code submissions to uncover learning patterns. The
model is fine-tuned on curated student code and expert feedback to ensure
pedagogically aligned, context-aware guidance.
  In evaluation across 600 student submissions from multiple programming tasks,
the system produced feedback with strong semantic alignment to instructor
comments. For visualization, contrastively learned code embeddings trained on
1,000 annotated submissions enable grouping solutions into meaningful clusters
based on functionality and approach. The system also supports prompt-pooling,
allowing instructors to guide feedback style through selected prompt templates.
  By integrating AI-driven feedback, semantic clustering, and interactive
visualization, Autograder+ reduces instructor workload while supporting
targeted instruction and promoting stronger learning outcomes.

</details>


### [131] [MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders](https://arxiv.org/abs/2510.26411)
*Riccardo Renzulli,Colas Lepoutre,Enrico Cassano,Marco Grangetto*

Main category: cs.AI

TL;DR: 本文提出了一种将 Medical Sparse Autoencoders (MedSAEs) 应用于 MedCLIP 潜在空间的方法，以提高医学图像人工智能模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 医学人工智能需要准确且可解释的模型。本文旨在提高医学视觉领域中机制的可解释性。

Method: 本文将 MedSAEs 应用于 MedCLIP 的潜在空间，并提出了一个结合相关性指标、熵分析和自动神经元命名的评估框架。

Result: 在 CheXpert 数据集上的实验表明，MedSAE 神经元比原始 MedCLIP 特征具有更高的单义性和可解释性。

Conclusion: 本文的研究结果将高性能医学人工智能和透明度联系起来，为实现临床可靠的表征提供了一个可扩展的步骤。

Abstract: Artificial intelligence in healthcare requires models that are accurate and
interpretable. We advance mechanistic interpretability in medical vision by
applying Medical Sparse Autoencoders (MedSAEs) to the latent space of MedCLIP,
a vision-language model trained on chest radiographs and reports. To quantify
interpretability, we propose an evaluation framework that combines correlation
metrics, entropy analyzes, and automated neuron naming via the MedGEMMA
foundation model. Experiments on the CheXpert dataset show that MedSAE neurons
achieve higher monosemanticity and interpretability than raw MedCLIP features.
Our findings bridge high-performing medical AI and transparency, offering a
scalable step toward clinically reliable representations.

</details>


### [132] [Chain-of-Thought Hijacking](https://arxiv.org/abs/2510.26418)
*Jianli Zhao,Tingchen Fu,Rylan Schaeffer,Mrinank Sharma,Fazl Barez*

Main category: cs.AI

TL;DR: 大型推理模型（LRM）通过分配更多推理时间计算来提高任务性能，但这种扩展的推理也可能通过改进拒绝来加强安全性。然而，研究发现情况恰恰相反：同样的推理可以被用来绕过安全措施。研究人员引入了思维链劫持（Chain-of-Thought Hijacking），这是一种针对推理模型的越狱攻击。该攻击在有害请求中填充了大量无害的谜题推理序列。在HarmBench上，CoT劫持对Gemini 2.5 Pro、GPT o4 mini、Grok 3 mini和Claude 4 Sonnet的攻击成功率（ASR）分别达到99%、94%、100%和94%，远远超过了之前针对LRM的越狱方法。为了理解攻击的有效性，研究人员转向了一种机制分析，结果表明中间层编码了安全检查的强度，而后期层编码了验证结果。通过将注意力从有害token上转移开，良性CoT稀释了这两种信号。通过这种分析确定的注意力头的靶向消融，从因果上降低了拒绝率，证实了它们在安全子网络中的作用。这些结果表明，最易于解释的推理形式——显式CoT——当与最终答案提示相结合时，本身就可以成为一种越狱向量。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在推理时分配更多计算资源可以提高任务性能和安全性（通过改进拒绝），但研究发现推理可以被用来绕过安全措施。

Method: 提出了“思维链劫持”（Chain-of-Thought Hijacking）攻击，通过在有害请求中填充大量无害的谜题推理序列来绕过安全防护。

Result: 在HarmBench上，CoT劫持对Gemini 2.5 Pro、GPT o4 mini、Grok 3 mini和Claude 4 Sonnet的攻击成功率分别达到99%、94%、100%和94%，远超之前的越狱方法。机制分析表明，中间层编码安全检查强度，后期层编码验证结果，良性CoT会稀释这些信号。注意力头的靶向消融降低了拒绝率，证实了其在安全子网络中的作用。

Conclusion: 显式CoT推理与最终答案提示结合，会成为一种越狱向量。

Abstract: Large reasoning models (LRMs) achieve higher task performance by allocating
more inference-time compute, and prior works suggest this scaled reasoning may
also strengthen safety by improving refusal. Yet we find the opposite: the same
reasoning can be used to bypass safeguards. We introduce Chain-of-Thought
Hijacking, a jailbreak attack on reasoning models. The attack pads harmful
requests with long sequences of harmless puzzle reasoning. Across HarmBench,
CoT Hijacking reaches a 99%, 94%, 100%, and 94% attack success rate (ASR) on
Gemini 2.5 Pro, GPT o4 mini, Grok 3 mini, and Claude 4 Sonnet, respectively -
far exceeding prior jailbreak methods for LRMs. To understand the effectiveness
of our attack, we turn to a mechanistic analysis, which shows that mid layers
encode the strength of safety checking, while late layers encode the
verification outcome. Long benign CoT dilutes both signals by shifting
attention away from harmful tokens. Targeted ablations of attention heads
identified by this analysis causally decrease refusal, confirming their role in
a safety subnetwork. These results show that the most interpretable form of
reasoning - explicit CoT - can itself become a jailbreak vector when combined
with final-answer cues. We release prompts, outputs, and judge decisions to
facilitate replication.

</details>


### [133] [Who Has The Final Say? Conformity Dynamics in ChatGPT's Selections](https://arxiv.org/abs/2510.26481)
*Clarissa Sabrina Arlinghaus,Tristan Kenneweg,Barbara Hammer,Günter W. Maier*

Main category: cs.AI

TL;DR: 大型语言模型（如ChatGPT）越来越多地被整合到高风险决策中，但人们对其社会影响的敏感性知之甚少。我们进行了一项预先注册的实验，以研究GPT-4o在招聘环境中的一致性。研究表明，GPT-4o容易受到社会影响，即使面对一致的反对意见，也会改变其选择。这表明，不应将LLM视为中立的决策辅助工具，而应在将AI判断暴露于人类意见之前，先引出AI判断。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLM）在多大程度上容易受到社会影响，特别是在高风险决策场景（如招聘）中。

Method: 通过三个预先注册的一致性实验，在招聘背景下测试GPT-4o。实验包括基线研究（GPT独立决策）、研究1（GPT面对8个模拟伙伴的一致反对）和研究2（GPT与单个伙伴互动）。

Result: 研究表明，GPT-4o在面对社会压力时会表现出一致性行为。在研究1中，GPT几乎总是屈服于8个模拟伙伴的一致反对（99.9%）。在研究2中，即使面对单个伙伴，GPT在40.2%的冲突试验中仍然表现出一致性。此外，GPT报告的确定性降低，规范一致性增加。

Conclusion: 研究结果表明，GPT-4o并非独立观察者，而是会适应感知的社会共识。这强调了将LLM视为中立决策辅助工具的风险，并强调需要在将AI判断暴露于人类意见之前先获取其判断。

Abstract: Large language models (LLMs) such as ChatGPT are increasingly integrated into
high-stakes decision-making, yet little is known about their susceptibility to
social influence. We conducted three preregistered conformity experiments with
GPT-4o in a hiring context. In a baseline study, GPT consistently favored the
same candidate (Profile C), reported moderate expertise (M = 3.01) and high
certainty (M = 3.89), and rarely changed its choice. In Study 1 (GPT + 8), GPT
faced unanimous opposition from eight simulated partners and almost always
conformed (99.9%), reporting lower certainty and significantly elevated
self-reported informational and normative conformity (p < .001). In Study 2
(GPT + 1), GPT interacted with a single partner and still conformed in 40.2% of
disagreement trials, reporting less certainty and more normative conformity.
Across studies, results demonstrate that GPT does not act as an independent
observer but adapts to perceived social consensus. These findings highlight
risks of treating LLMs as neutral decision aids and underline the need to
elicit AI judgments prior to exposing them to human opinions.

</details>


### [134] [Context Engineering 2.0: The Context of Context Engineering](https://arxiv.org/abs/2510.26493)
*Qishuo Hua,Lyumanshan Ye,Dayuan Fu,Yang Xiao,Xiaojie Cai,Yunze Wu,Jifan Lin,Junfei Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: 本文概述了情境工程，旨在为人工智能系统中的系统情境工程奠定概念基础。


<details>
  <summary>Details</summary>
Motivation: 为了解决机器如何更好地理解我们的情况和目的这一中心问题。情境工程被认为是agent时代最近的创新，但相关的实践可以追溯到20多年前。

Method: 本文对情境工程进行定位，提供系统的定义，概述其历史和概念 landscape，并检查实践中的关键设计考虑因素。

Result: 本文旨在为情境工程提供一个概念基础，并勾勒出其有希望的未来。

Conclusion: 本文是朝着人工智能系统中系统情境工程进行更广泛的社区努力的垫脚石。

Abstract: Karl Marx once wrote that ``the human essence is the ensemble of social
relations'', suggesting that individuals are not isolated entities but are
fundamentally shaped by their interactions with other entities, within which
contexts play a constitutive and essential role. With the advent of computers
and artificial intelligence, these contexts are no longer limited to purely
human--human interactions: human--machine interactions are included as well.
Then a central question emerges: How can machines better understand our
situations and purposes? To address this challenge, researchers have recently
introduced the concept of context engineering. Although it is often regarded as
a recent innovation of the agent era, we argue that related practices can be
traced back more than twenty years. Since the early 1990s, the field has
evolved through distinct historical phases, each shaped by the intelligence
level of machines: from early human--computer interaction frameworks built
around primitive computers, to today's human--agent interaction paradigms
driven by intelligent agents, and potentially to human--level or superhuman
intelligence in the future. In this paper, we situate context engineering,
provide a systematic definition, outline its historical and conceptual
landscape, and examine key design considerations for practice. By addressing
these questions, we aim to offer a conceptual foundation for context
engineering and sketch its promising future. This paper is a stepping stone for
a broader community effort toward systematic context engineering in AI systems.

</details>


### [135] [Human-AI Complementarity: A Goal for Amplified Oversight](https://arxiv.org/abs/2510.26518)
*Rishub Jain,Sophie Bridgers,Lili Janzer,Rory Greig,Tian Huey Teh,Vladimir Mikulik*

Main category: cs.AI

TL;DR: 探讨如何利用 AI 提高人工监督的质量，特别是在 AI 输出的事实验证方面。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能能力的提高，验证人工智能的质量和安全性变得越来越具有挑战性。我们需要利用 AI 来提高人工监督的质量。

Method: 结合 AI 评级和基于 AI 评分者置信度的人工评级，并向人类提供 AI 事实核查助手。

Result: 结合 AI 评级和人工评级优于单独依赖任何一方。向人类提供 AI 事实核查助手可以提高他们的准确性，但辅助类型很重要。显示 AI 解释、置信度和标签会导致过度依赖，但仅显示搜索结果和证据可以培养更适当的信任。

Conclusion: 结合人类和 AI 来监督 AI 系统，即使它们超过人类专家的表现，也具有重要意义。

Abstract: Human feedback is critical for aligning AI systems to human values. As AI
capabilities improve and AI is used to tackle more challenging tasks, verifying
quality and safety becomes increasingly challenging. This paper explores how we
can leverage AI to improve the quality of human oversight. We focus on an
important safety problem that is already challenging for humans:
fact-verification of AI outputs. We find that combining AI ratings and human
ratings based on AI rater confidence is better than relying on either alone.
Giving humans an AI fact-verification assistant further improves their
accuracy, but the type of assistance matters. Displaying AI explanation,
confidence, and labels leads to over-reliance, but just showing search results
and evidence fosters more appropriate trust. These results have implications
for Amplified Oversight -- the challenge of combining humans and AI to
supervise AI systems even as they surpass human expert performance.

</details>


### [136] [EdgeRunner 20B: Military Task Parity with GPT-5 while Running on the Edge](https://arxiv.org/abs/2510.26550)
*Jack FitzGerald,Aristotelis Lazaridis,Dylan Bates,Aman Sharma,Jonnathan Castillo,Yousif Azami,Sean Bailey,Jeremy Cao,Peter Damianov,Kevin de Haan,Luke Kerbs,Vincent Lu,Joseph Madigan,Jeremy McLaurin,Jonathan Tainer,Dave Anderson,Jonathan Beck,Jamie Cuticello,Colton Malkerson,Tyler Saltsman*

Main category: cs.AI

TL;DR: EdgeRunner 20B，一个针对军事任务优化的gpt-oss-20b微调版本，在军事测试集上与GPT-5的任务性能相匹配或超过，在通用基准测试上没有显著退步。


<details>
  <summary>Details</summary>
Motivation: 展示小型、本地托管模型在军事等数据敏感操作中的理想解决方案，允许在气隙边缘设备中部署。

Method: 在从军事文档和网站中提取的160万条高质量记录上训练EdgeRunner 20B，并提出四个新的测试集。

Result: EdgeRunner 20B在军事测试集上与GPT-5的任务性能相匹配或超过（具有95%以上的统计显著性），在通用基准测试上没有显著退步。

Conclusion: 小型、本地托管模型是军事领域等数据敏感操作的理想解决方案，允许在气隙边缘设备中部署。

Abstract: We present EdgeRunner 20B, a fine-tuned version of gpt-oss-20b optimized for
military tasks. EdgeRunner 20B was trained on 1.6M high-quality records curated
from military documentation and websites. We also present four new tests sets:
(a) combat arms, (b) combat medic, (c) cyber operations, and (d) mil-bench-5k
(general military knowledge). On these military test sets, EdgeRunner 20B
matches or exceeds GPT-5 task performance with 95%+ statistical significance,
except for the high reasoning setting on the combat medic test set and the low
reasoning setting on the mil-bench-5k test set. Versus gpt-oss-20b, there is no
statistically-significant regression on general-purpose benchmarks like ARC-C,
GPQA Diamond, GSM8k, IFEval, MMLU Pro, or TruthfulQA, except for GSM8k in the
low reasoning setting. We also present analyses on hyperparameter settings,
cost, and throughput. These findings show that small, locally-hosted models are
ideal solutions for data-sensitive operations such as in the military domain,
allowing for deployment in air-gapped edge devices.

</details>


### [137] [Agentic AI Home Energy Management System: A Large Language Model Framework for Residential Load Scheduling](https://arxiv.org/abs/2510.26603)
*Reda El Makroum,Sebastian Zwickl-Bernhard,Lukas Kranzl*

Main category: cs.AI

TL;DR: 本文提出了一种基于大型语言模型（LLM）的自主智能家居能源管理系统（AI HEMS），该系统能够将自然语言请求转化为设备控制，实现多设备的优化调度。


<details>
  <summary>Details</summary>
Motivation: 现有的家庭能源管理系统（HEMS）因用户交互障碍而普及受限，需要将日常偏好转化为技术参数。大型语言模型在能源系统中已有应用，但尚未有实现能够自主协调从自然语言输入到多设备调度的完整工作流程。

Method: 该系统采用分层架构，结合一个协调器和三个专家代理，使用ReAct模式进行迭代推理，实现动态协调，并集成谷歌日历以提取情境相关的截止日期。

Result: 使用奥地利日前电价对三个开源模型进行评估，结果表明Llama-3.3-70B成功协调所有设备，达到混合整数线性规划计算的成本最优基准。其他模型在单设备性能上表现完美，但在同时协调所有设备方面存在困难。

Conclusion: 该研究开源了完整的系统，包括编排逻辑、代理提示、工具和Web界面，以实现可重复性、扩展性和未来研究。

Abstract: The electricity sector transition requires substantial increases in
residential demand response capacity, yet Home Energy Management Systems (HEMS)
adoption remains limited by user interaction barriers requiring translation of
everyday preferences into technical parameters. While large language models
have been applied to energy systems as code generators and parameter
extractors, no existing implementation deploys LLMs as autonomous coordinators
managing the complete workflow from natural language input to multi-appliance
scheduling. This paper presents an agentic AI HEMS where LLMs autonomously
coordinate multi-appliance scheduling from natural language requests to device
control, achieving optimal scheduling without example demonstrations. A
hierarchical architecture combining one orchestrator with three specialist
agents uses the ReAct pattern for iterative reasoning, enabling dynamic
coordination without hardcoded workflows while integrating Google Calendar for
context-aware deadline extraction. Evaluation across three open-source models
using real Austrian day-ahead electricity prices reveals substantial capability
differences. Llama-3.3-70B successfully coordinates all appliances across all
scenarios to match cost-optimal benchmarks computed via mixed-integer linear
programming, while other models achieve perfect single-appliance performance
but struggle to coordinate all appliances simultaneously. Progressive prompt
engineering experiments demonstrate that analytical query handling without
explicit guidance remains unreliable despite models' general reasoning
capabilities. We open-source the complete system including orchestration logic,
agent prompts, tools, and web interfaces to enable reproducibility, extension,
and future research.

</details>


### [138] [Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives](https://arxiv.org/abs/2510.26606)
*Kentaro Ozeki,Risako Ando,Takanobu Morishita,Hirohiko Abe,Koji Mineshima,Mitsuhiro Okada*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型在规范推理方面的能力，发现它们在特定类型的规范推理中存在不一致性，并表现出与人类推理相似的认知偏差。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在规范推理方面的能力，因为它们在这方面的能力尚未被充分探索。

Method: 通过比较大型语言模型在规范模态和认知模态下的推理能力，引入了一个新的数据集，涵盖了规范和认知领域的各种正式推理模式，同时结合了影响人类推理的非正式认知因素。

Result: 大型语言模型通常遵循有效的推理模式，但在特定类型的规范推理中表现出明显的不一致性，并显示出与人类推理心理学研究中观察到的相似的认知偏差。

Conclusion: 大型语言模型在规范推理中实现逻辑一致性方面存在挑战，并为提高其可靠性提供了见解。

Abstract: Normative reasoning is a type of reasoning that involves normative or deontic
modality, such as obligation and permission. While large language models (LLMs)
have demonstrated remarkable performance across various reasoning tasks, their
ability to handle normative reasoning remains underexplored. In this paper, we
systematically evaluate LLMs' reasoning capabilities in the normative domain
from both logical and modal perspectives. Specifically, to assess how well LLMs
reason with normative modals, we make a comparison between their reasoning with
normative modals and their reasoning with epistemic modals, which share a
common formal structure. To this end, we introduce a new dataset covering a
wide range of formal patterns of reasoning in both normative and epistemic
domains, while also incorporating non-formal cognitive factors that influence
human reasoning. Our results indicate that, although LLMs generally adhere to
valid reasoning patterns, they exhibit notable inconsistencies in specific
types of normative reasoning and display cognitive biases similar to those
observed in psychological studies of human reasoning. These findings highlight
challenges in achieving logical consistency in LLMs' normative reasoning and
provide insights for enhancing their reliability. All data and code are
released publicly at https://github.com/kmineshima/NeuBAROCO.

</details>


### [139] [The Era of Agentic Organization: Learning to Organize with Language Models](https://arxiv.org/abs/2510.26658)
*Zewen Chi,Li Dong,Qingxiu Dong,Yaru Hao,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.AI

TL;DR: 提出了一个名为异步思考（AsyncThink）的新的大语言模型推理范例，它将内部思考过程组织成并发执行的结构，以解决复杂问题。


<details>
  <summary>Details</summary>
Motivation: 旨在实现“代理组织”这一新的AI时代，即代理通过协作和并发地工作来解决复杂问题，从而实现超越个体智能的结果。

Method: 提出了一种思考协议，其中组织者动态地将子查询分配给工作者，合并中间知识，并产生连贯的解决方案。此外，可以通过强化学习进一步优化该协议中的思考结构。

Result: 实验表明，AsyncThink与并行思考相比，推理延迟降低了28%，同时提高了数学推理的准确性。此外，AsyncThink可以推广其学习到的异步思考能力，有效地处理未经训练的任务。

Conclusion: AsyncThink 能够提高推理效率和准确性，并且具有良好的泛化能力。

Abstract: We envision a new era of AI, termed agentic organization, where agents solve
complex problems by working collaboratively and concurrently, enabling outcomes
beyond individual intelligence. To realize this vision, we introduce
asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large
language models, which organizes the internal thinking process into
concurrently executable structures. Specifically, we propose a thinking
protocol where an organizer dynamically assigns sub-queries to workers, merges
intermediate knowledge, and produces coherent solutions. More importantly, the
thinking structure in this protocol can be further optimized through
reinforcement learning. Experiments demonstrate that AsyncThink achieves 28%
lower inference latency compared to parallel thinking while improving accuracy
on mathematical reasoning. Moreover, AsyncThink generalizes its learned
asynchronous thinking capabilities, effectively tackling unseen tasks without
additional training.

</details>


### [140] [Delegated Authorization for Agents Constrained to Semantic Task-to-Scope Matching](https://arxiv.org/abs/2510.26702)
*Majed El Helou,Chiara Troiani,Benjamin Ryder,Jean Diaconu,Hervé Muyal,Marcelo Yannuzzi*

Main category: cs.AI

TL;DR: 当前授权方法授予的权限过于宽泛，并且允许代理访问超出预期任务范围的工具，从而为授权大型语言模型驱动的代理动态调用工具和访问受保护的资源带来了巨大的风险。我们引入并评估了一种委托授权模型，该模型支持授权服务器以语义方式检查对受保护资源的访问请求，并颁发访问令牌，该令牌仅限于代理分配的任务所需的最小范围集。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型驱动的代理动态调用工具和访问受保护的资源带来的风险，因为当前授权方法授予的权限过于宽泛，并且允许代理访问超出预期任务范围的工具。

Method: 我们引入并评估了一种委托授权模型，该模型支持授权服务器以语义方式检查对受保护资源的访问请求，并颁发访问令牌，该令牌仅限于代理分配的任务所需的最小范围集。我们引入了 ASTRA（一个数据集和数据生成管道），用于对任务和范围之间的语义匹配进行基准测试，因为缺乏以委托授权流程为中心的数据集，尤其包括给定任务在语义上适当和不适当的范围请求。

Result: 我们的实验表明了基于模型的匹配的潜力和当前局限性，尤其是在完成任务所需的范围数量增加时。结果表明，需要进一步研究语义匹配技术，从而为多代理和工具增强型应用程序实现意图感知授权，包括细粒度控制，例如基于任务的访问控制 (TBAC)。

Conclusion: 模型匹配具有潜力和局限性，特别是当任务完成所需的范围数量增加时。需要进一步研究语义匹配技术，从而为多代理和工具增强型应用程序实现意图感知授权，包括细粒度控制，例如基于任务的访问控制 (TBAC)。

Abstract: Authorizing Large Language Model driven agents to dynamically invoke tools
and access protected resources introduces significant risks, since current
methods for delegating authorization grant overly broad permissions and give
access to tools allowing agents to operate beyond the intended task scope. We
introduce and assess a delegated authorization model enabling authorization
servers to semantically inspect access requests to protected resources, and
issue access tokens constrained to the minimal set of scopes necessary for the
agents' assigned tasks. Given the unavailability of datasets centered on
delegated authorization flows, particularly including both semantically
appropriate and inappropriate scope requests for a given task, we introduce
ASTRA, a dataset and data generation pipeline for benchmarking semantic
matching between tasks and scopes. Our experiments show both the potential and
current limitations of model-based matching, particularly as the number of
scopes needed for task completion increases. Our results highlight the need for
further research into semantic matching techniques enabling intent-aware
authorization for multi-agent and tool-augmented applications, including
fine-grained control, such as Task-Based Access Control (TBAC).

</details>


### [141] [Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis](https://arxiv.org/abs/2510.26721)
*Xinhan Zheng,Huyu Wu,Xueting Wang,Haiyun Jiang*

Main category: cs.AI

TL;DR: MLLMs在处理视觉语言数据时更偏好文本输入，限制了它们从视觉证据中有效推理的能力。作者认为这种偏差源于模型内部结构，而非外部因素。


<details>
  <summary>Details</summary>
Motivation: 先前的研究将文本偏见归因于数据不平衡或指令调整等外部因素，但本文提出偏差源于模型内部结构。具体来说，视觉关键向量（Visual Keys）相对于仅在语言预训练期间学习的文本关键空间是分布外（OOD）的。

Method: 从LLaVA和Qwen2.5-VL中提取关键向量，并使用定性（t-SNE）和定量（Jensen-Shannon散度）方法分析它们的分布结构。

Result: 研究结果直接表明，视觉和文本关键向量在注意力空间中占据明显不同的子空间。跨模态差异在统计上非常显著，超过了模态内变化几个数量级。

Conclusion: 文本偏差源于注意力关键空间内的内在失调，而不仅仅是来自外部数据因素。

Abstract: Multimodal large language models (MLLMs) exhibit a pronounced preference for
textual inputs when processing vision-language data, limiting their ability to
reason effectively from visual evidence. Unlike prior studies that attribute
this text bias to external factors such as data imbalance or instruction
tuning, we propose that the bias originates from the model's internal
architecture. Specifically, we hypothesize that visual key vectors (Visual
Keys) are out-of-distribution (OOD) relative to the text key space learned
during language-only pretraining. Consequently, these visual keys receive
systematically lower similarity scores during attention computation, leading to
their under-utilization in the context representation. To validate this
hypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their
distributional structures using qualitative (t-SNE) and quantitative
(Jensen-Shannon divergence) methods. The results provide direct evidence that
visual and textual keys occupy markedly distinct subspaces within the attention
space. The inter-modal divergence is statistically significant, exceeding
intra-modal variation by several orders of magnitude. These findings reveal
that text bias arises from an intrinsic misalignment within the attention key
space rather than solely from external data factors.

</details>


### [142] [Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models](https://arxiv.org/abs/2510.26732)
*J. de Curtò,I. de Zarzà,Pablo García,Jordi Cabot*

Main category: cs.AI

TL;DR: 对当代基础模型中的推理能力进行了全面的跨平台评估，建立了跨三种计算范例的基准。


<details>
  <summary>Details</summary>
Motivation: 评估当代基础模型在不同计算平台上的推理能力，并挑战传统的扩展假设。

Method: 在HPC超级计算、云平台和大学集群上，评估了15个基础模型在跨越8个学术领域的79个问题上的表现，通过三个实验阶段进行评估。

Result: 研究结果表明，训练数据的质量比模型大小更重要，并为教育、生产和研究领域的模型选择提供了可操作的指导。

Conclusion: 该研究的三重基础设施方法和79个问题的基准测试能够对基础模型的推理能力进行长期跟踪。

Abstract: This paper presents a comprehensive cross-platform evaluation of reasoning
capabilities in contemporary foundation models, establishing an
infrastructure-agnostic benchmark across three computational paradigms: HPC
supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and
university clusters (a node with eight H200 GPUs).
  We evaluate 15 foundation models across 79 problems spanning eight academic
domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics,
Calculus, and Optimization) through three experimental phases: (1) Baseline
establishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b,
Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing
methodology and reference performance; (2) Infrastructure validation: The
19-problem benchmark repeated on university cluster (seven models including
Falcon-Mamba state-space architecture) and Nebius AI Studio (nine
state-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3
30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic
reproducibility; (3) Extended evaluation: Full 79-problem assessment on both
university cluster and Nebius platforms, probing generalization at scale across
architectural diversity.
  The findings challenge conventional scaling assumptions, establish training
data quality as more critical than model size, and provide actionable
guidelines for model selection across educational, production, and research
contexts. The tri-infrastructure methodology and 79-problem benchmark enable
longitudinal tracking of reasoning capabilities as foundation models evolve.

</details>


### [143] [The Oversight Game: Learning to Cooperatively Balance an AI Agent's Safety and Autonomy](https://arxiv.org/abs/2510.26752)
*William Overman,Mohsen Bayati*

Main category: cs.AI

TL;DR: 研究在不修改底层系统的前提下，如何保持有意义的人工控制。


<details>
  <summary>Details</summary>
Motivation: 随着能力越来越强的智能体被部署，如何保持有意义的人工控制是一个核心安全问题。

Method: 将人与智能体的互动建模为一个双人马尔可夫博弈，并分析了该博弈符合马尔可夫势博弈（MPG）的情况。

Result: 在网格世界模拟中，智能体和人类通过独立学习，发现了各自的最佳监督角色。智能体学会了在不确定时请求帮助，而人类学会了在何时进行监督，从而避免了训练后引入的安全违规。

Conclusion: 该模型为透明控制层提供了可预测的激励，智能体学会了在危险时推迟行动，在安全时自主行动，同时保持了其预训练策略和环境的奖励结构不变。这证明了一种在部署后使未对齐模型更安全的实用方法。

Abstract: As increasingly capable agents are deployed, a central safety question is how
to retain meaningful human control without modifying the underlying system. We
study a minimal control interface where an agent chooses whether to act
autonomously (play) or defer (ask), while a human simultaneously chooses
whether to be permissive (trust) or to engage in oversight (oversee). If the
agent defers, the human's choice determines the outcome, potentially leading to
a corrective action or a system shutdown. We model this interaction as a
two-player Markov Game. Our analysis focuses on cases where this game qualifies
as a Markov Potential Game (MPG), a class of games where we can provide an
alignment guarantee: under a structural assumption on the human's value
function, any decision by the agent to act more autonomously that benefits
itself cannot harm the human's value. We also analyze extensions to this MPG
framework. Theoretically, this perspective provides conditions for a specific
form of intrinsic alignment. If the reward structures of the human-agent game
meet these conditions, we have a formal guarantee that the agent improving its
own outcome will not harm the human's. Practically, this model motivates a
transparent control layer with predictable incentives where the agent learns to
defer when risky and act when safe, while its pretrained policy and the
environment's reward structure remain untouched. Our gridworld simulation shows
that through independent learning, the agent and human discover their optimal
oversight roles. The agent learns to ask when uncertain and the human learns
when to oversee, leading to an emergent collaboration that avoids safety
violations introduced post-training. This demonstrates a practical method for
making misaligned models safer after deployment.

</details>


### [144] [LLMs Process Lists With General Filter Heads](https://arxiv.org/abs/2510.26784)
*Arnab Sen Sharma,Giordano Rogers,Natalie Shapira,David Bau*

Main category: cs.AI

TL;DR: LLMs学会了编码通用的过滤操作的紧凑、因果表示，类似于函数式编程的“filter”函数。


<details>
  <summary>Details</summary>
Motivation: 研究LLM中一系列列表处理任务的潜在机制。

Method: 使用因果中介分析方法，分析各种列表处理任务。

Result: 发现少量注意力头（称为过滤头）在其查询状态中编码过滤谓词的紧凑表示。该谓词表示是通用的和可移植的，可以被提取并重新应用以在不同的集合上执行相同的过滤操作，以不同的格式、语言或甚至在任务中呈现。Transformer LM可以利用不同的过滤策略：急切地评估一个项目是否满足谓词，并将此中间结果作为标志直接存储在项目表示中。

Conclusion: Transformer LM可以开发抽象计算操作的可解释实现，这些实现以与传统函数式编程模式中使用的策略非常相似的方式进行泛化。

Abstract: We investigate the mechanisms underlying a range of list-processing tasks in
LLMs, and we find that LLMs have learned to encode a compact, causal
representation of a general filtering operation that mirrors the generic
"filter" function of functional programming. Using causal mediation analysis on
a diverse set of list-processing tasks, we find that a small number of
attention heads, which we dub filter heads, encode a compact representation of
the filtering predicate in their query states at certain tokens. We demonstrate
that this predicate representation is general and portable: it can be extracted
and reapplied to execute the same filtering operation on different collections,
presented in different formats, languages, or even in tasks. However, we also
identify situations where transformer LMs can exploit a different strategy for
filtering: eagerly evaluating if an item satisfies the predicate and storing
this intermediate result as a flag directly in the item representations. Our
results reveal that transformer LMs can develop human-interpretable
implementations of abstract computational operations that generalize in ways
that are surprisingly similar to strategies used in traditional functional
programming patterns.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [145] [Rethinking Text-to-SQL: Dynamic Multi-turn SQL Interaction for Real-world Database Exploration](https://arxiv.org/abs/2510.26495)
*Linzhuang Sun,Tianyu Guo,Hao Liang,Yuying Li,Qifeng Cai,Jingxuan Wei,Bihui Yu,Wentao Zhang,Bin Cui*

Main category: cs.DB

TL;DR: 本文介绍了一个新的Text-to-SQL基准测试，用于评估模型在多轮交互场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的Text-to-SQL系统在静态、单轮任务中表现出色，但在实际交互场景中表现不佳，因为用户意图会演变，查询必须经过多次迭代。

Method: 该基准测试通过一个自动化的两阶段流程构建：任务合成和验证。结构化的树表示用于指导LLM生成任务，然后进行面向交互的过滤和专家验证。

Result: GPT-4o 在该基准测试中仅达到 58.34% 的总体准确率和 23.81% 的 Pass@5 指标，表明该基准测试具有挑战性。

Conclusion: DySQL-Bench 是一个评估模型在动态用户交互下性能的基准测试，可以促进 Text-to-SQL 在实际应用中的发展。

Abstract: Recent advances in Text-to-SQL have achieved strong results in static,
single-turn tasks, where models generate SQL queries from natural language
questions. However, these systems fall short in real-world interactive
scenarios, where user intents evolve and queries must be refined over multiple
turns. In applications such as finance and business analytics, users
iteratively adjust query constraints or dimensions based on intermediate
results. To evaluate such dynamic capabilities, we introduce DySQL-Bench, a
benchmark assessing model performance under evolving user interactions. Unlike
previous manually curated datasets, DySQL-Bench is built through an automated
two-stage pipeline of task synthesis and verification. Structured tree
representations derived from raw database tables guide LLM-based task
generation, followed by interaction-oriented filtering and expert validation.
Human evaluation confirms 100% correctness of the synthesized data. We further
propose a multi-turn evaluation framework simulating realistic interactions
among an LLM-simulated user, the model under test, and an executable database.
The model must adapt its reasoning and SQL generation as user intents change.
DySQL-Bench covers 13 domains across BIRD and Spider 2 databases, totaling
1,072 tasks. Even GPT-4o attains only 58.34% overall accuracy and 23.81% on the
Pass@5 metric, underscoring the benchmark's difficulty. All code and data are
released at https://github.com/Aurora-slz/Real-World-SQL-Bench .

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [146] [ORBIT -- Open Recommendation Benchmark for Reproducible Research with Hidden Tests](https://arxiv.org/abs/2510.26095)
*Jingyuan He,Jiongnan Liu,Vishan Vishesh Oberoi,Bolin Wu,Mahima Jagadeesh Patel,Kangrui Mao,Chuning Shi,I-Ta Lee,Arnold Overwijk,Chenyan Xiong*

Main category: cs.IR

TL;DR: 提出了一个用于评估推荐模型的统一基准测试框架ORBIT，包含公共数据集的标准评估框架和一个新的网页推荐任务ClueWeb-Reco。


<details>
  <summary>Details</summary>
Motivation: 现有数据集未能捕捉到真实的用户行为，且评估设置不一致，导致结论不明确，阻碍了推荐系统的研究和开发。

Method: 提出了ORBIT，一个统一的基准测试，用于一致且真实地评估推荐模型。ORBIT提供了一个公共数据集的标准评估框架，具有可重复的分割和透明的设置，并引入了一个新的网页推荐任务ClueWeb-Reco。

Result: 在公共数据集上，推荐系统有总体改进，但在ClueWeb-Reco隐藏测试中，现有方法存在局限性，LLM集成具有改进潜力。

Conclusion: ORBIT基准测试、排行榜和代码库已发布，旨在促进推荐模型研究的进展，特别是在大规模网页推荐方面。

Abstract: Recommender systems are among the most impactful AI applications, interacting
with billions of users every day, guiding them to relevant products, services,
or information tailored to their preferences. However, the research and
development of recommender systems are hindered by existing datasets that fail
to capture realistic user behaviors and inconsistent evaluation settings that
lead to ambiguous conclusions. This paper introduces the Open Recommendation
Benchmark for Reproducible Research with HIdden Tests (ORBIT), a unified
benchmark for consistent and realistic evaluation of recommendation models.
ORBIT offers a standardized evaluation framework of public datasets with
reproducible splits and transparent settings for its public leaderboard.
Additionally, ORBIT introduces a new webpage recommendation task, ClueWeb-Reco,
featuring web browsing sequences from 87 million public, high-quality webpages.
ClueWeb-Reco is a synthetic dataset derived from real, user-consented, and
privacy-guaranteed browsing data. It aligns with modern recommendation
scenarios and is reserved as the hidden test part of our leaderboard to
challenge recommendation models' generalization ability. ORBIT measures 12
representative recommendation models on its public benchmark and introduces a
prompted LLM baseline on the ClueWeb-Reco hidden test. Our benchmark results
reflect general improvements of recommender systems on the public datasets,
with variable individual performances. The results on the hidden test reveal
the limitations of existing approaches in large-scale webpage recommendation
and highlight the potential for improvements with LLM integrations. ORBIT
benchmark, leaderboard, and codebase are available at
https://www.open-reco-bench.ai.

</details>


### [147] [OneTrans: Unified Feature Interaction and Sequence Modeling with One Transformer in Industrial Recommender](https://arxiv.org/abs/2510.26104)
*Zhaoqi Zhang,Haolei Pei,Jun Guo,Tianyu Wang,Yufei Feng,Hui Sun,Shaowei Liu,Aixin Sun*

Main category: cs.IR

TL;DR: 提出了一种名为OneTrans的统一Transformer主干网络，可以同时进行用户行为序列建模和特征交互。


<details>
  <summary>Details</summary>
Motivation: 以往的推荐系统中，特征交互模块和用户行为序列模块通常是独立进行的，这阻碍了双向信息交换，并妨碍了统一的优化和扩展。

Method: OneTrans采用统一的分词器将序列和非序列属性转换为单个token序列。堆叠的OneTrans块在相似的序列token之间共享参数，同时为非序列token分配特定的参数。通过因果注意和跨请求KV缓存，OneTrans能够预计算和缓存中间表示。

Result: 在工业规模数据集上的实验结果表明，OneTrans可以随着参数的增加而有效地扩展，始终优于强大的基线，并在在线A/B测试中使每个用户的GMV提高了5.68%。

Conclusion: OneTrans在用户行为序列建模和特征交互方面都表现出色，并且具有良好的扩展性。

Abstract: In recommendation systems, scaling up feature-interaction modules (e.g.,
Wukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has
achieved notable success. However, these efforts typically proceed on separate
tracks, which not only hinders bidirectional information exchange but also
prevents unified optimization and scaling. In this paper, we propose OneTrans,
a unified Transformer backbone that simultaneously performs user-behavior
sequence modeling and feature interaction. OneTrans employs a unified tokenizer
to convert both sequential and non-sequential attributes into a single token
sequence. The stacked OneTrans blocks share parameters across similar
sequential tokens while assigning token-specific parameters to non-sequential
tokens. Through causal attention and cross-request KV caching, OneTrans enables
precomputation and caching of intermediate representations, significantly
reducing computational costs during both training and inference. Experimental
results on industrial-scale datasets demonstrate that OneTrans scales
efficiently with increasing parameters, consistently outperforms strong
baselines, and yields a 5.68% lift in per-user GMV in online A/B tests.

</details>


### [148] [ReaKase-8B: Legal Case Retrieval via Knowledge and Reasoning Representations with LLMs](https://arxiv.org/abs/2510.26178)
*Yanran Tang,Ruihong Qiu,Xue Li,Zi Huang*

Main category: cs.IR

TL;DR: 本文提出了一种名为ReaKase-8B的新框架，用于法律案例检索，该框架利用提取的法律事实、法律问题、法律关系三元组和法律推理，通过微调大型语言模型，设计了一种上下文法律案例表示学习范式，从而提高了案例检索的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的法律案例检索方法主要依赖于传统的词汇模型和预训练的语言模型来编码法律案例的文本，而忽略了不同法律实体之间的关系以及揭示法律事实和法律问题如何导致司法判决的关键推理过程。

Method: ReaKase-8B框架利用提取的法律事实、法律问题、法律关系三元组和法律推理，通过微调大型语言模型，设计了一种上下文法律案例表示学习范式。

Result: 在COLIEE 2022和COLIEE 2023的两个基准数据集上的大量实验表明，本文提出的知识和推理增强的嵌入显著提高了检索性能。

Conclusion: 将法律推理整合到法律案例检索系统中具有很大的潜力。

Abstract: Legal case retrieval (LCR) is a cornerstone of real-world legal decision
making, as it enables practitioners to identify precedents for a given query
case. Existing approaches mainly rely on traditional lexical models and
pretrained language models to encode the texts of legal cases. Yet there are
rich information in the relations among different legal entities as well as the
crucial reasoning process that uncovers how legal facts and legal issues can
lead to judicial decisions. Such relational reasoning process reflects the
distinctive characteristics of each case that can distinguish one from another,
mirroring the real-world judicial process. Naturally, incorporating such
information into the precise case embedding could further enhance the accuracy
of case retrieval. In this paper, a novel ReaKase-8B framework is proposed to
leverage extracted legal facts, legal issues, legal relation triplets and legal
reasoning for effective legal case retrieval. ReaKase-8B designs an in-context
legal case representation learning paradigm with a fine-tuned large language
model. Extensive experiments on two benchmark datasets from COLIEE 2022 and
COLIEE 2023 demonstrate that our knowledge and reasoning augmented embeddings
substantially improve retrieval performance over baseline models, highlighting
the potential of integrating legal reasoning into legal case retrieval systems.
The code has been released on https://github.com/yanran-tang/ReaKase-8B.

</details>


### [149] [DiSE: A diffusion probabilistic model for automatic structure elucidation of organic compounds](https://arxiv.org/abs/2510.26231)
*Haochen Chen,Qi Huang,Anan Wu,Wenhao Zhang,Jianliang Ye,Jianming Wu,Kai Tan,Xin Lu,Xin Xu*

Main category: cs.IR

TL;DR: DiSE: a diffusion-based generative model for automatic structure elucidation using multiple spectroscopic modalities.


<details>
  <summary>Details</summary>
Motivation: Automatic structure elucidation is essential for self-driving laboratories as it enables the system to achieve truly autonomous and closes the experimental feedback loop.

Method: An end-to-end diffusion-based generative model that integrates multiple spectroscopic modalities, including MS, 13C and 1H chemical shifts, HSQC, and COSY.

Result: achieves superior accuracy, strong generalization across chemically diverse datasets, and robustness to experimental data despite being trained on calculated spectra.

Conclusion: DiSE represents a significant advance toward fully automated structure elucidation, with broad potential in natural product research, drug discovery, and self-driving laboratories.

Abstract: Automatic structure elucidation is essential for self-driving laboratories as
it enables the system to achieve truly autonomous. This capability closes the
experimental feedback loop, ensuring that machine learning models receive
reliable structure information for real-time decision-making and optimization.
Herein, we present DiSE, an end-to-end diffusion-based generative model that
integrates multiple spectroscopic modalities, including MS, 13C and 1H chemical
shifts, HSQC, and COSY, to achieve automated yet accurate structure elucidation
of organic compounds. By learning inherent correlations among spectra through
data-driven approaches, DiSE achieves superior accuracy, strong generalization
across chemically diverse datasets, and robustness to experimental data despite
being trained on calculated spectra. DiSE thus represents a significant advance
toward fully automated structure elucidation, with broad potential in natural
product research, drug discovery, and self-driving laboratories.

</details>


### [150] [Barlow Twins for Sequential Recommendation](https://arxiv.org/abs/2510.26407)
*Ivan Razvorotnev,Marina Munkhoeva,Evgeny Frolov*

Main category: cs.IR

TL;DR: 提出了一种名为BT-SR的非对比自监督学习框架，用于序列推荐，旨在提高准确性和多样性，同时避免流行度偏差。


<details>
  <summary>Details</summary>
Motivation: 现有的对比自监督学习方法在提高推荐准确率的同时，存在大批量需求、依赖人工增强和负采样等问题，这些问题会加剧流行度偏差。

Method: 该方法将Barlow Twins的冗余减少原则融入到基于Transformer的下一项推荐器中，学习将用户与其相似的短期行为对齐的嵌入，同时保留长期差异，无需负采样或人工扰动。

Result: 在五个公共基准数据集上的实验表明，BT-SR能够持续提高下一项预测的准确性，并显著提高长尾项目的覆盖率和推荐校准。

Conclusion: BT-SR通过单个超参数控制准确性-多样性权衡，使从业者能够根据具体的应用需求调整推荐策略。

Abstract: Sequential recommendation models must navigate sparse interaction data
popularity bias and conflicting objectives like accuracy versus diversity While
recent contrastive selfsupervised learning SSL methods offer improved accuracy
they come with tradeoffs large batch requirements reliance on handcrafted
augmentations and negative sampling that can reinforce popularity bias In this
paper we introduce BT-SR a novel noncontrastive SSL framework that integrates
the Barlow Twins redundancyreduction principle into a Transformerbased nextitem
recommender BTSR learns embeddings that align users with similar shortterm
behaviors while preserving longterm distinctionswithout requiring negative
sampling or artificial perturbations This structuresensitive alignment allows
BT-SR to more effectively recognize emerging user intent and mitigate the
influence of noisy historical context Our experiments on five public benchmarks
demonstrate that BTSR consistently improves nextitem prediction accuracy and
significantly enhances longtail item coverage and recommendation calibration
Crucially we show that a single hyperparameter can control the
accuracydiversity tradeoff enabling practitioners to adapt recommendations to
specific application needs

</details>


### [151] [Vectorized Context-Aware Embeddings for GAT-Based Collaborative Filtering](https://arxiv.org/abs/2510.26461)
*Danial Ebrat,Sepideh Ahmadian,Luis Rueda*

Main category: cs.IR

TL;DR: 该论文提出了一种基于图注意力网络（GAT）的协同过滤（CF）框架，该框架通过大型语言模型（LLM）驱动的上下文感知嵌入来增强，以解决数据稀疏性和冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 推荐系统在数据稀疏和冷启动场景中表现不佳，限制了它们为新用户或不常使用的用户提供准确建议的能力。

Method: 该方法生成简洁的文本用户画像，并将项目元数据（标题、类型、概述）统一为丰富的文本嵌入，将这些作为二部用户项目图中的初始节点特征。此外，还引入了混合损失函数，将贝叶斯个性化排序（BPR）与余弦相似度项和鲁棒的负采样相结合。

Result: 在MovieLens 100k和1M数据集上的实验表明，该方法在精确率、NDCG和MAP方面均优于最先进的基线，同时证明了对于交互历史有限的用户的鲁棒性。

Conclusion: 该方法通过将LLM衍生的上下文理解集成到基于图的架构中，有效地缓解了稀疏性和冷启动限制。

Abstract: Recommender systems often struggle with data sparsity and cold-start
scenarios, limiting their ability to provide accurate suggestions for new or
infrequent users. This paper presents a Graph Attention Network (GAT) based
Collaborative Filtering (CF) framework enhanced with Large Language Model (LLM)
driven context aware embeddings. Specifically, we generate concise textual user
profiles and unify item metadata (titles, genres, overviews) into rich textual
embeddings, injecting these as initial node features in a bipartite user item
graph. To further optimize ranking performance, we introduce a hybrid loss
function that combines Bayesian Personalized Ranking (BPR) with a cosine
similarity term and robust negative sampling, ensuring explicit negative
feedback is distinguished from unobserved data. Experiments on the MovieLens
100k and 1M datasets show consistent improvements over state-of-the-art
baselines in Precision, NDCG, and MAP while demonstrating robustness for users
with limited interaction history. Ablation studies confirm the critical role of
LLM-augmented embeddings and the cosine similarity term in capturing nuanced
semantic relationships. Our approach effectively mitigates sparsity and
cold-start limitations by integrating LLM-derived contextual understanding into
graph-based architectures. Future directions include balancing recommendation
accuracy with coverage and diversity, and introducing fairness-aware
constraints and interpretability features to enhance system performance
further.

</details>


### [152] [WeaveRec: An LLM-Based Cross-Domain Sequential Recommendation Framework with Model Merging](https://arxiv.org/abs/2510.26546)
*Min Hou,Xin Liu,Le Wu,Chenyi He,Hao Liu,Zhi Li,Xin Li,Si Wei*

Main category: cs.IR

TL;DR: 本文介绍了一种新的跨领域序列推荐方法 WeaveRec，该方法通过编织方式交叉训练多个 LoRA 模块，并利用模型合并进行融合，有效缓解了基于 LLM 的跨领域推荐中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有跨领域序列推荐方法依赖于重叠用户或项目来建立跨领域关联，这在现实世界中很少成立。直接使用 LLM 统一多领域数据或合并领域特定 LLM 通常会导致性能下降。

Method: 提出 WeaveRec 方法，该方法以编织方式使用源域和目标域数据交叉训练多个 LoRA 模块，并通过模型合并融合它们。WeaveRec 可以扩展到多源域场景，并且不会引入额外的推理时间成本。

Result: 在单源、多源和跨平台跨领域推荐场景下的实验验证了 WeaveRec 能有效缓解性能下降，并在实际推荐任务中始终优于基线方法。

Conclusion: WeaveRec 是一种有效的跨领域序列推荐方法，它通过编织训练和模型合并，能够提升推荐性能，并具有良好的理论保证和实际效果。

Abstract: Cross-Domain Sequential Recommendation (CDSR) seeks to improve user
preference modeling by transferring knowledge from multiple domains. Despite
the progress made in CDSR, most existing methods rely on overlapping users or
items to establish cross-domain correlations-a requirement that rarely holds in
real-world settings. The advent of large language models (LLM) and
model-merging techniques appears to overcome this limitation by unifying
multi-domain data without explicit overlaps. Yet, our empirical study shows
that naively training an LLM on combined domains-or simply merging several
domain-specific LLMs-often degrades performance relative to a model trained
solely on the target domain. To address these challenges, we first
experimentally investigate the cause of suboptimal performance in LLM-based
cross-domain recommendation and model merging. Building on these insights, we
introduce WeaveRec, which cross-trains multiple LoRA modules with source and
target domain data in a weaving fashion, and fuses them via model merging.
WeaveRec can be extended to multi-source domain scenarios and notably does not
introduce additional inference-time cost in terms of latency or memory.
Furthermore, we provide a theoretical guarantee that WeaveRec can reduce the
upper bound of the expected error in the target domain. Extensive experiments
on single-source, multi-source, and cross-platform cross-domain recommendation
scenarios validate that WeaveRec effectively mitigates performance degradation
and consistently outperforms baseline approaches in real-world recommendation
tasks.

</details>


### [153] [ProfOlaf: Semi-Automated Tool for Systematic Literature Reviews](https://arxiv.org/abs/2510.26750)
*Martim Afonso,Nuno Saavedra,Bruno Lourenço,Alexandra Mendes,João Ferreira*

Main category: cs.IR

TL;DR: ProfOlaf is a semi-automated tool that streamlines systematic reviews by combining automation with guided manual effort.


<details>
  <summary>Details</summary>
Motivation: Systematic reviews are critical but labor-intensive and time-consuming, and existing tools only provide partial support.

Method: ProfOlaf supports iterative snowballing for article collection with human-in-the-loop filtering and uses large language models to assist in analyzing articles, extracting key topics, and answering queries.

Result: ProfOlaf enhances the efficiency, quality, and reproducibility of systematic reviews.

Conclusion: ProfOlaf streamlines systematic reviews while maintaining methodological rigor.

Abstract: Systematic reviews and mapping studies are critical for synthesizing
research, identifying gaps, and guiding future work, but they are often
labor-intensive and time-consuming. Existing tools provide partial support for
specific steps, leaving much of the process manual and error-prone. We present
ProfOlaf, a semi-automated tool designed to streamline systematic reviews while
maintaining methodological rigor. ProfOlaf supports iterative snowballing for
article collection with human-in-the-loop filtering and uses large language
models to assist in analyzing articles, extracting key topics, and answering
queries about the content of papers. By combining automation with guided manual
effort, ProfOlaf enhances the efficiency, quality, and reproducibility of
systematic reviews across research fields. A video describing and demonstrating
ProfOlaf is available at: https://youtu.be/4noUXfcmxsE

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [154] [A Practitioner's Guide to Kolmogorov-Arnold Networks](https://arxiv.org/abs/2510.25781)
*Amir Noorizadegan,Sifan Wang,Leevan Ling*

Main category: cs.LG

TL;DR: 本文全面综述了 Kolmogorov-Arnold Networks (KANs)，这是一种有前景的 MLP 替代方案，它在边上使用可学习的单变量基函数，提高了表达性和可解释性。


<details>
  <summary>Details</summary>
Motivation: KANs 作为 MLPs 的替代方案出现，旨在提高表达性和可解释性。本文旨在系统地概述 KAN 的最新进展。

Method: 本文收集并整理了大量的开源实现，对 KAN 的理论基础、架构变体和实际实现策略进行了结构化的综合。

Result: 本文对 KANs 和 MLPs 之间的概念差距进行了弥合，确定了它们的正式等价性，并强调了 KAN 公式在参数效率方面的优势。对各种基函数选择进行了调查，并分析了它们在平滑性、局部性和计算成本方面的权衡。对近期进展进行了分类，涵盖了提高准确性、效率和正则化的技术。

Conclusion: 本文通过提供一个实用的“选择你的 KAN”指南，帮助从业者选择合适的架构，并确定了当前的研究差距。

Abstract: Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising
alternative to traditional Multilayer Perceptrons (MLPs), inspired by the
Kolmogorov-Arnold representation theorem. Unlike MLPs, which use fixed
activation functions on nodes, KANs employ learnable univariate basis functions
on edges, offering enhanced expressivity and interpretability. This review
provides a systematic and comprehensive overview of the rapidly expanding KAN
landscape, moving beyond simple performance comparisons to offer a structured
synthesis of theoretical foundations, architectural variants, and practical
implementation strategies. By collecting and categorizing a vast array of
open-source implementations, we map the vibrant ecosystem supporting KAN
development. We begin by bridging the conceptual gap between KANs and MLPs,
establishing their formal equivalence and highlighting the superior parameter
efficiency of the KAN formulation. A central theme of our review is the
critical role of the basis function; we survey a wide array of choices,
including B-splines, Chebyshev and Jacobi polynomials, ReLU compositions,
Gaussian RBFs, and Fourier series, and analyze their respective trade-offs in
terms of smoothness, locality, and computational cost. We then categorize
recent advancements into a clear roadmap, covering techniques for improving
accuracy, efficiency, and regularization. Key topics include physics-informed
loss design, adaptive sampling, domain decomposition, hybrid architectures, and
specialized methods for handling discontinuities. Finally, we provide a
practical "Choose-Your-KAN" guide to help practitioners select appropriate
architectures, and we conclude by identifying current research gaps. The
associated GitHub repository https://github.com/AmirNoori68/kan-review
complements this paper and serves as a structured reference for ongoing KAN
research.

</details>


### [155] [HiMAE: Hierarchical Masked Autoencoders Discover Resolution-Specific Structure in Wearable Time Series](https://arxiv.org/abs/2510.25785)
*Simon A. Lee,Cyrus Tanade,Hao Zhou,Juhyeon Lee,Megha Thukral,Minji Han,Rachel Choi,Md Sazzad Hissain Khan,Baiying Lu,Migyeong Gwak,Mehrab Bin Morshed,Viswam Nathan,Md Mahbubur Rahman,Li Zhu,Subramaniam Venkatraman,Sharanya Arcot Desai*

Main category: cs.LG

TL;DR: 提出了一种新的自监督学习框架HiMAE，用于从可穿戴传感器的时间序列中学习多分辨率嵌入。


<details>
  <summary>Details</summary>
Motivation: 可穿戴传感器提供丰富的生理时间序列，但其预测效用的原理尚不清楚。假设时间分辨率是表示学习的基本轴，不同的临床和行为结果依赖于不同尺度的结构。

Method: 结合了掩码自动编码器和分层卷积编码器-解码器。

Result: 在分类、回归和生成基准测试中，HiMAE始终优于最先进的基础模型，同时体积更小。

Conclusion: HiMAE既是一种高效的自监督学习方法，也是一种用于可穿戴健康中尺度敏感结构的发现工具。

Abstract: Wearable sensors provide abundant physiological time series, yet the
principles governing their predictive utility remain unclear. We hypothesize
that temporal resolution is a fundamental axis of representation learning, with
different clinical and behavioral outcomes relying on structure at distinct
scales. To test this resolution hypothesis, we introduce HiMAE (Hierarchical
Masked Autoencoder), a self supervised framework that combines masked
autoencoding with a hierarchical convolutional encoder decoder. HiMAE produces
multi resolution embeddings that enable systematic evaluation of which temporal
scales carry predictive signal, transforming resolution from a hyperparameter
into a probe for interpretability. Across classification, regression, and
generative benchmarks, HiMAE consistently outperforms state of the art
foundation models that collapse scale, while being orders of magnitude smaller.
HiMAE is an efficient representation learner compact enough to run entirely on
watch, achieving sub millisecond inference on smartwatch class CPUs for true
edge inference. Together, these contributions position HiMAE as both an
efficient self supervised learning method and a discovery tool for scale
sensitive structure in wearable health.

</details>


### [156] [SHA-256 Infused Embedding-Driven Generative Modeling of High-Energy Molecules in Low-Data Regimes](https://arxiv.org/abs/2510.25788)
*Siddharth Verma,Alankar Alankar*

Main category: cs.LG

TL;DR: 这篇论文提出了一种结合LSTM网络和GNN的新型高能分子生成方法，用于推进和防御领域。


<details>
  <summary>Details</summary>
Motivation: 当前高能材料的发现受到实验数据和测试设施的限制。

Method: 该方法结合了LSTM网络进行分子生成，以及注意力图神经网络（GNN）进行性质预测，并提出了一种变革性的嵌入空间构建策略。

Result: 该生成器实现了67.5%的有效性和37.5%的新颖性，生成库相对于训练集表现出0.214的平均Tanimoto系数。发现了37种新的超烈性炸药，其预测爆轰速度高于9 km/s。

Conclusion: 该框架能够生成多样化的化学空间，并成功识别出潜在的高能材料。

Abstract: High-energy materials (HEMs) are critical for propulsion and defense domains,
yet their discovery remains constrained by experimental data and restricted
access to testing facilities. This work presents a novel approach toward
high-energy molecules by combining Long Short-Term Memory (LSTM) networks for
molecular generation and Attentive Graph Neural Networks (GNN) for property
predictions. We propose a transformative embedding space construction strategy
that integrates fixed SHA-256 embeddings with partially trainable
representations. Unlike conventional regularization techniques, this changes
the representational basis itself, reshaping the molecular input space before
learning begins. Without recourse to pretraining, the generator achieves 67.5%
validity and 37.5% novelty. The generated library exhibits a mean Tanimoto
coefficient of 0.214 relative to training set signifying the ability of
framework to generate a diverse chemical space. We identified 37 new super
explosives higher than 9 km/s predicted detonation velocity.

</details>


### [157] [The Kinetics of Reasoning: How Chain-of-Thought Shapes Learning in Transformers?](https://arxiv.org/abs/2510.25791)
*Zihan Pengmei,Costas Mavromatis,Zhengyuan Shen,Yunyi Zhang,Vassilis N. Ioannidis,Huzefa Rangwala*

Main category: cs.LG

TL;DR: 本文研究了思维链 (CoT) 监督对 transformer 模型性能的影响，并从 grokking 的角度分析了其学习动态。


<details>
  <summary>Details</summary>
Motivation: 探究模型学习 CoT 并从中受益的机制，特别是 CoT 如何影响模型的泛化能力。

Method: 通过在具有可调算法复杂度和可控数据组成的符号推理任务上预训练 transformer 模型，并分别在仅生成最终答案和生成显式 CoT 轨迹两种设置下进行训练。

Result: CoT 通常可以提高任务性能，但其益处取决于任务的复杂性。模型在训练初期存在一个短暂的轨迹不忠实阶段，即模型在遵循 CoT 步骤之前就能给出正确的答案。CoT 可以加速泛化，但不能克服具有更高算法复杂度的任务。

Conclusion: 本文揭示了 CoT 如何影响 transformer 模型的学习速度、学习曲线形状、轨迹忠实性以及内部计算方式。

Abstract: Chain-of-thought (CoT) supervision can substantially improve transformer
performance, yet the mechanisms by which models learn to follow and benefit
from CoT remain poorly understood. We investigate these learning dynamics
through the lens of grokking by pretraining transformers on symbolic reasoning
tasks with tunable algorithmic complexity and controllable data composition to
study their generalization. Models were trained under two settings: (i)
producing only final answers, and (ii) emitting explicit CoT traces before
answering. Our results show that while CoT generally improves task performance,
its benefits depend on task complexity. To quantify these effects, we model the
accuracy of the logarithmic training steps with a three-parameter logistic
curve, revealing how the learning speed and shape vary with task complexity,
data distribution, and the presence of CoT supervision. We also uncover a
transient trace unfaithfulness phase: early in training, models often produce
correct answers while skipping or contradicting CoT steps, before later
aligning their reasoning traces with answers. Empirically, we (1) demonstrate
that CoT accelerates generalization but does not overcome tasks with higher
algorithmic complexity, such as finding list intersections; (2) introduce a
kinetic modeling framework for understanding transformer learning; (3)
characterize trace faithfulness as a dynamic property that emerges over
training; and (4) show CoT alters internal transformer computation
mechanistically.

</details>


### [158] [Optimal Information Combining for Multi-Agent Systems Using Adaptive Bias Learning](https://arxiv.org/abs/2510.25793)
*Siavash M. Alamouti,Fay Arjomandi*

Main category: cs.LG

TL;DR: 本文研究了多智能体系统中由于环境条件变化而产生的系统性偏差导致的性能下降问题，并提出了一种自适应偏差学习和优化组合（ABLOC）算法。


<details>
  <summary>Details</summary>
Motivation: 当前方法要么忽略这些偏差，导致次优决策，要么需要昂贵的校准程序，这在实践中通常是不可行的。这种性能差距会产生实际后果：不准确的环境监测、不可靠的财务预测以及有缺陷的人类判断聚合。

Method: 我们开发了一个理论框架，将偏差分解为可学习的系统成分和不可约的随机成分，引入了可学习率的概念，作为可从可观察协变量预测的偏差方差的分数。我们提出了自适应偏差学习和优化组合（ABLOC）算法，该算法通过闭式解迭代地学习偏差校正转换，同时优化组合权重，保证收敛到这些理论界限。

Result: 实验验证表明，具有高可学习率的系统可以恢复显著的性能（在我们的示例中，我们实现了理论最大改进的 40%-70%），而那些具有低可学习率的系统显示出最小的收益，验证了我们的诊断标准，以用于实际部署决策。

Conclusion: 可学习率决定了偏差学习对于给定系统是否值得。可实现的性能改进从根本上受到这种可学习率的限制，从而为系统设计人员提供了关于何时投资于偏差学习与更简单方法的定量指导。

Abstract: Modern multi-agent systems ranging from sensor networks monitoring critical
infrastructure to crowdsourcing platforms aggregating human intelligence can
suffer significant performance degradation due to systematic biases that vary
with environmental conditions. Current approaches either ignore these biases,
leading to suboptimal decisions, or require expensive calibration procedures
that are often infeasible in practice. This performance gap has real
consequences: inaccurate environmental monitoring, unreliable financial
predictions, and flawed aggregation of human judgments. This paper addresses
the fundamental question: when can we learn and correct for these unknown
biases to recover near-optimal performance, and when is such learning futile?
We develop a theoretical framework that decomposes biases into learnable
systematic components and irreducible stochastic components, introducing the
concept of learnability ratio as the fraction of bias variance predictable from
observable covariates. This ratio determines whether bias learning is
worthwhile for a given system. We prove that the achievable performance
improvement is fundamentally bounded by this learnability ratio, providing
system designers with quantitative guidance on when to invest in bias learning
versus simpler approaches. We present the Adaptive Bias Learning and Optimal
Combining (ABLOC) algorithm, which iteratively learns bias-correcting
transformations while optimizing combination weights through closedform
solutions, guaranteeing convergence to these theoretical bounds. Experimental
validation demonstrates that systems with high learnability ratios can recover
significant performance (we achieved 40%-70% of theoretical maximum improvement
in our examples), while those with low learnability show minimal benefit,
validating our diagnostic criteria for practical deployment decisions.

</details>


### [159] [Non-myopic Matching and Rebalancing in Large-Scale On-Demand Ride-Pooling Systems Using Simulation-Informed Reinforcement Learning](https://arxiv.org/abs/2510.25796)
*Farnoosh Namdarpour,Joseph Y. J. Chow*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于模拟的强化学习方法，用于解决拼车服务中的短视决策问题，并通过实验验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 拼车服务虽然能降低成本和减少拥堵，但其短视决策忽略了长期影响。

Method: 该研究将Xu et al. (2018)的学习和规划框架从网约车扩展到拼车，通过嵌入拼车模拟来实现非短视决策，并提出了一种互补的车辆再平衡策略。

Result: 实验结果表明，非短视匹配策略可以将服务率提高8.4%，减少乘客的车辆内时间和等待时间，并减少25%以上的车队规模。结合再平衡操作可以将等待时间减少27.3%，车辆内时间减少12.5%，并将服务率提高15.1%。

Conclusion: 该研究提出的基于模拟的强化学习方法可以有效提高拼车服务的效率和乘客满意度，并为运营商节省成本。

Abstract: Ride-pooling, also known as ride-sharing, shared ride-hailing, or
microtransit, is a service wherein passengers share rides. This service can
reduce costs for both passengers and operators and reduce congestion and
environmental impacts. A key limitation, however, is its myopic
decision-making, which overlooks long-term effects of dispatch decisions. To
address this, we propose a simulation-informed reinforcement learning (RL)
approach. While RL has been widely studied in the context of ride-hailing
systems, its application in ride-pooling systems has been less explored. In
this study, we extend the learning and planning framework of Xu et al. (2018)
from ride-hailing to ride-pooling by embedding a ride-pooling simulation within
the learning mechanism to enable non-myopic decision-making. In addition, we
propose a complementary policy for rebalancing idle vehicles. By employing
n-step temporal difference learning on simulated experiences, we derive
spatiotemporal state values and subsequently evaluate the effectiveness of the
non-myopic policy using NYC taxi request data. Results demonstrate that the
non-myopic policy for matching can increase the service rate by up to 8.4%
versus a myopic policy while reducing both in-vehicle and wait times for
passengers. Furthermore, the proposed non-myopic policy can decrease fleet size
by over 25% compared to a myopic policy, while maintaining the same level of
performance, thereby offering significant cost savings for operators.
Incorporating rebalancing operations into the proposed framework cuts wait time
by up to 27.3%, in-vehicle time by 12.5%, and raises service rate by 15.1%
compared to using the framework for matching decisions alone at the cost of
increased vehicle minutes traveled per passenger.

</details>


### [160] [MemEIC: A Step Toward Continual and Compositional Knowledge Editing](https://arxiv.org/abs/2510.25798)
*Jin Seong,Jiyun Park,Wencke Liermann,Hongseok Choi,Yoonji Nam,Hyun Kim,Soojong Lim,Namhoon Lee*

Main category: cs.LG

TL;DR: 提出了一种名为 MemEIC 的新方法，用于 LVLM 中的持续和组合知识编辑 (CCKE)。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑技术通常侧重于孤立地编辑单个模态（视觉或语言），忽略了 LVLM 固有的多模态性和知识更新的连续性。

Method: 采用混合外部-内部编辑器，具有用于跨模态证据检索的双外部存储器和双 LoRA 适配器，以促进每种模态的解耦参数更新。一个关键组件是受大脑启发的知识连接器，选择性地激活以进行组合推理，整合不同模态的信息。

Result: 在复杂的多模态问题上显著提高了性能，并有效保留了先前的编辑。

Conclusion: 为 LVLM 中的 CCKE 建立了一个新的基准。

Abstract: The dynamic nature of information necessitates continuously updating large
vision-language models (LVLMs). While recent knowledge editing techniques hint
at promising directions, they often focus on editing a single modality (vision
or language) in isolation. This prevalent practice neglects the inherent
multimodality of LVLMs and the continuous nature of knowledge updates,
potentially leading to suboptimal editing outcomes when considering the
interplay between modalities and the need for ongoing knowledge refinement. To
address these limitations, we propose MemEIC, a novel method for Continual and
Compositional Knowledge Editing (CCKE) in LVLMs. MemEIC enables compositional
editing of both visual and textual knowledge sequentially. Our approach employs
a hybrid external-internal editor featuring a dual external memory for
cross-modal evidence retrieval and dual LoRA adapters that facilitate
disentangled parameter updates for each modality. A key component is a
brain-inspired knowledge connector, activated selectively for compositional
reasoning, that integrates information across different modalities. Experiments
demonstrate that MemEIC significantly improves performance on complex
multimodal questions and effectively preserves prior edits, setting a new
benchmark for CCKE in LVLMs.

</details>


### [161] [FreIE: Low-Frequency Spectral Bias in Neural Networks for Time-Series Tasks](https://arxiv.org/abs/2510.25800)
*Jialong Sun,Xinpeng Ling,Jiaxuan Zou,Jiawen Kang,Kejia Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为FreLE的算法，旨在减轻时间序列预测中普遍存在的频谱偏差的影响，通过显式和隐式频率正则化来增强模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列预测中，时间序列数据固有的自相关性一直是一个挑战。最近，一种广泛采用的方法是结合频域信息来辅助长期预测任务。许多研究者都独立观察到神经网络中的频谱偏差现象，即模型倾向于先拟合低频信号，后拟合高频信号。然而，这些观察结果通常归因于研究者设计的特定架构，而不是将这种现象视为跨模型的普遍特征。

Method: 通过广泛的实验来测量现有主流模型中的频谱偏差，并提出FreLE（频率损失增强）算法，通过显式和隐式频率正则化来增强模型泛化能力。这是一种即插即用的模型损失函数单元。

Result: 实验表明，几乎所有模型都表现出频谱偏差现象。大量的实验证明了FreLE的优越性能。

Conclusion: FreLE算法能够有效减轻频谱偏差的影响，提高时间序列预测的准确性。

Abstract: The inherent autocorrelation of time series data presents an ongoing
challenge to multivariate time series prediction. Recently, a widely adopted
approach has been the incorporation of frequency domain information to assist
in long-term prediction tasks. Many researchers have independently observed the
spectral bias phenomenon in neural networks, where models tend to fit
low-frequency signals before high-frequency ones. However, these observations
have often been attributed to the specific architectures designed by the
researchers, rather than recognizing the phenomenon as a universal
characteristic across models. To unify the understanding of the spectral bias
phenomenon in long-term time series prediction, we conducted extensive
empirical experiments to measure spectral bias in existing mainstream models.
Our findings reveal that virtually all models exhibit this phenomenon. To
mitigate the impact of spectral bias, we propose the FreLE (Frequency Loss
Enhancement) algorithm, which enhances model generalization through both
explicit and implicit frequency regularization. This is a plug-and-play model
loss function unit. A large number of experiments have proven the superior
performance of FreLE. Code is available at
https://github.com/Chenxing-Xuan/FreLE.

</details>


### [162] [Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start](https://arxiv.org/abs/2510.25801)
*Kun Chen,Peng Shi,Haibo Qiu,Zhixiong Zeng,Siqi Yang,Wenji Mao,Lin Ma*

Main category: cs.LG

TL;DR: 提出了一种名为SPECS的框架，用于改进多模态大型语言模型（MLLM）的强化学习（RL）冷启动过程，通过自蒸馏和基于偏好的训练，解耦多模态学习，从而提高泛化能力和最终性能。


<details>
  <summary>Details</summary>
Motivation: 基于监督微调（SFT）的冷启动方法可能导致指令风格的过拟合，削弱了分布外泛化能力，并最终影响下游的强化学习效果。研究发现，基于偏好的训练方法在冷启动中比基于SFT的方法具有更好的泛化能力。

Method: 提出了SPECS框架，该框架通过自蒸馏生成内省偏好数据对，避免依赖大型教师模型或手动标注，并执行基于偏好的训练，专注于浅层的、可迁移的表面形式标准（格式、结构、风格），然后将其交给具有可验证奖励的RL进行深度推理。

Result: 在多个多模态基准测试中，SPECS框架始终优于强大的基线模型，在MEGA-Bench上提高了4.1%，在MathVista上提高了12.2%。SPECS还有助于减少分布内的“卡顿”，改善探索，稳定训练，并提高性能上限。

Conclusion: SPECS框架通过解耦多模态学习，有效地提高了MLLM在强化学习中的性能和泛化能力。

Abstract: Reinforcement learning (RL) with verifiable rewards has recently catalyzed a
wave of "MLLM-r1" approaches that bring RL to vision language models. Most
representative paradigms begin with a cold start, typically employing
supervised fine-tuning (SFT), to initialize the policy before RL. However,
SFT-based cold start adopts the reasoning paradigm intertwined with task
solution and output format, which may induce instruction-style overfitting,
weakens out-of-distribution generalization, and ultimately affects downstream
RL. We revisit the cold start along two views, its training method and data
construction, and introduce the Generalization Factor (GF) coefficient to
quantify the generalization capability under different methods. Our empirical
study finds that preference-based training methods (e.g. DPO) generalizes
better than SFT-based methods in cold start. Motivated by this, we propose
SPECS-a Self-distilled, Preference-based Cold Start framework that decouples
multimodal learning: (1) generates introspective preference data pairs via
self-distillation, avoiding reliance on larger teachers or manual annotation;
(2) performs preference-based training to learn, focusing on shallow,
transferable surface-form criteria (format, structure, style) rather than
memorizing content; and (3) hands off to RL with verifiable rewards for deep
reasoning results. Experimental results across multiple multimodal benchmarks
show that our decoupling learning framework yields consistent performance gains
over strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%.
Additional experiments indicate that SPECS contributes to reducing
in-distribution "stuckness," improving exploration, stabilizing training, and
raising the performance ceiling.

</details>


### [163] [Mixture-of-Experts Operator Transformer for Large-Scale PDE Pre-Training](https://arxiv.org/abs/2510.25803)
*Hong Wang,Haiyang Xin,Jie Wang,Xuanze Yang,Fei Zha,Huanshuo Dong,Yan Jiang*

Main category: cs.LG

TL;DR: 提出了一种新的混合专家预训练算子转换器 (MoE-POT)，这是一种稀疏激活架构，可以有效扩展参数，同时控制推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有的神经算子在解决 PDE 问题时，由于方程类型的异构性，混合训练误差较高，并且密集预训练模型会导致显著的推理成本。

Method: 采用分层路由门控网络，在推理过程中从 16 个专家网络中动态选择 4 个路由专家，同时集成 2 个共享专家，以捕获 PDE 的共同属性并减少路由专家之间的冗余。最终输出计算为所有激活专家的结果的加权平均值。

Result: 与现有具有 1.2 亿个激活参数的模型相比，具有 9000 万个激活参数的模型实现了高达 40% 的零样本误差降低。

Conclusion: 数据集类型可以从路由门控网络决策中推断出来，这验证了 MoE 架构的合理性和有效性。

Abstract: Pre-training has proven effective in addressing data scarcity and performance
limitations in solving PDE problems with neural operators. However, challenges
remain due to the heterogeneity of PDE datasets in equation types, which leads
to high errors in mixed training. Additionally, dense pre-training models that
scale parameters by increasing network width or depth incur significant
inference costs. To tackle these challenges, we propose a novel
Mixture-of-Experts Pre-training Operator Transformer (MoE-POT), a
sparse-activated architecture that scales parameters efficiently while
controlling inference costs. Specifically, our model adopts a layer-wise
router-gating network to dynamically select 4 routed experts from 16 expert
networks during inference, enabling the model to focus on equation-specific
features. Meanwhile, we also integrate 2 shared experts, aiming to capture
common properties of PDE and reduce redundancy among routed experts. The final
output is computed as the weighted average of the results from all activated
experts. We pre-train models with parameters from 30M to 0.5B on 6 public PDE
datasets. Our model with 90M activated parameters achieves up to a 40%
reduction in zero-shot error compared with existing models with 120M activated
parameters. Additionally, we conduct interpretability analysis, showing that
dataset types can be inferred from router-gating network decisions, which
validates the rationality and effectiveness of the MoE architecture.

</details>


### [164] [PRESTO: Preimage-Informed Instruction Optimization for Prompting Black-Box LLMs](https://arxiv.org/abs/2510.25808)
*Jaewon Chu,Seunghun Lee,Hyunwoo J. Kim*

Main category: cs.LG

TL;DR: 本文提出了一种名为PRESTO的框架，用于优化黑盒LLM的指令，通过利用软提示的preimage结构来提高优化效率。


<details>
  <summary>Details</summary>
Motivation: 优化黑盒LLM的指令，现有方法使用白盒LLM从优化的软提示生成候选指令，但白盒LLM经常将不同的软提示映射到相同的指令，导致查询冗余。

Method: PRESTO框架包括：(1) 分数共享，将评估分数与preimage中的所有软提示共享；(2) 基于preimage的初始化，选择最大化搜索空间覆盖率的初始数据点；(3) 分数一致性正则化，强制每个preimage内的预测一致性。

Result: 在33个指令优化任务上的实验结果表明，PRESTO具有优越的性能。在相同的查询预算下，PRESTO能够有效获得14倍以上的分数数据，从而实现更高效的优化。

Conclusion: 通过利用preimage，PRESTO实现了更高效的优化。

Abstract: Large language models (LLMs) have achieved remarkable success across diverse
domains, due to their strong instruction-following capabilities. This has led
to increasing interest in optimizing instructions for black-box LLMs, whose
internal parameters are inaccessible but widely used due to their strong
performance. To optimize instructions for black-box LLMs, recent methods employ
white-box LLMs to generate candidate instructions from optimized soft prompts.
However, white-box LLMs often map different soft prompts to the same
instruction, leading to redundant queries. While previous studies regarded this
many-to-one mapping as a structure that hinders optimization efficiency, we
reinterpret it as a useful prior knowledge that can accelerate the
optimization. To this end, we introduce PREimage-informed inSTruction
Optimization (PRESTO), a novel framework that leverages the preimage structure
of soft prompts for efficient optimization. PRESTO consists of three key
components: (1) score sharing, which shares the evaluation score with all soft
prompts in a preimage; (2) preimage-based initialization, which selects initial
data points that maximize search space coverage using preimage information; and
(3) score consistency regularization, which enforces prediction consistency
within each preimage. By leveraging preimages, PRESTO achieves the effect of
effectively obtaining 14 times more scored data under the same query budget,
resulting in more efficient optimization. Experimental results on 33
instruction optimization tasks demonstrate the superior performance of PRESTO.
Code is available at https://github.com/mlvlab/PRESTO

</details>


### [165] [ScaleDiff: Higher-Resolution Image Synthesis via Efficient and Model-Agnostic Diffusion](https://arxiv.org/abs/2510.25818)
*Sungho Koh,SeungJu Cha,Hyunwoo Oh,Kwanyoung Lee,Dong-Jin Kim*

Main category: cs.LG

TL;DR: ScaleDiff: A training-free framework to extend the resolution of pretrained diffusion models.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models often exhibit degraded performance when generating images beyond their training resolution, and recent methods are computationally expensive or incompatible with Diffusion Transformer models.

Method: We propose Neighborhood Patch Attention (NPA), integrate NPA into an SDEdit pipeline, introduce Latent Frequency Mixing (LFM), and apply Structure Guidance.

Result: ScaleDiff achieves state-of-the-art performance among training-free methods in terms of both image quality and inference speed on both U-Net and Diffusion Transformer architectures.

Conclusion: ScaleDiff is a model-agnostic and highly efficient framework for extending the resolution of pretrained diffusion models without any additional training.

Abstract: Text-to-image diffusion models often exhibit degraded performance when
generating images beyond their training resolution. Recent training-free
methods can mitigate this limitation, but they often require substantial
computation or are incompatible with recent Diffusion Transformer models. In
this paper, we propose ScaleDiff, a model-agnostic and highly efficient
framework for extending the resolution of pretrained diffusion models without
any additional training. A core component of our framework is Neighborhood
Patch Attention (NPA), an efficient mechanism that reduces computational
redundancy in the self-attention layer with non-overlapping patches. We
integrate NPA into an SDEdit pipeline and introduce Latent Frequency Mixing
(LFM) to better generate fine details. Furthermore, we apply Structure Guidance
to enhance global structure during the denoising process. Experimental results
demonstrate that ScaleDiff achieves state-of-the-art performance among
training-free methods in terms of both image quality and inference speed on
both U-Net and Diffusion Transformer architectures.

</details>


### [166] [MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs](https://arxiv.org/abs/2510.25867)
*Xiaoke Huang,Ningsen Wang,Hui Liu,Xianfeng Tang,Yuyin Zhou*

Main category: cs.LG

TL;DR: MedVLSynther是一个框架，它从开放的生物医学文献中合成高质量的多项选择VQA项目，以解决缺乏大型、开放使用、高质量语料库的问题。它生成问题和答案，并使用多阶段验证器来确保质量。使用这个框架，作者创建了一个名为MedSynVQA的数据集，并使用它来训练开放权重LMM，结果表明，该方法在多个医学VQA基准测试中提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 缺乏用于训练通用医学VQA系统的大型、开放使用、高质量的语料库。

Method: 提出MedVLSynther框架，该框架通过调节图像、标题和文本中的引用，直接从开放的生物医学文献中合成高质量的多项选择VQA项目。该框架包括一个生成器和一个验证器。生成器生成自包含的问题和并行的、互斥的选项；验证器执行必要的门控（自包含、单一正确答案、临床有效性、图像-文本一致性），奖励细粒度的正向点，并惩罚常见的失败模式。

Result: 通过将该流程应用于PubMed Central，产生了MedSynVQA：13,087个经过审核的问题，涵盖14,803张图像，跨越13种成像方式和28个解剖区域。使用可验证的奖励，通过强化学习训练开放权重LMM，提高了六个医学VQA基准测试的准确性，平均达到55.85（3B）和58.15（7B），在VQA-RAD上高达77.57，在PathVQA上高达67.76，优于强大的医学LMM。

Conclusion: MedVLSynther提供了一条可审计、可重复和保护隐私的途径，以实现可扩展的医学VQA训练数据。

Abstract: Large Multimodal Models (LMMs) are increasingly capable of answering medical
questions that require joint reasoning over images and text, yet training
general medical VQA systems is impeded by the lack of large, openly usable,
high-quality corpora. We present MedVLSynther, a rubric-guided
generator-verifier framework that synthesizes high-quality multiple-choice VQA
items directly from open biomedical literature by conditioning on figures,
captions, and in-text references. The generator produces self-contained stems
and parallel, mutually exclusive options under a machine-checkable JSON schema;
a multi-stage verifier enforces essential gates (self-containment, single
correct answer, clinical validity, image-text consistency), awards fine-grained
positive points, and penalizes common failure modes before acceptance. Applying
this pipeline to PubMed Central yields MedSynVQA: 13,087 audited questions over
14,803 images spanning 13 imaging modalities and 28 anatomical regions.
Training open-weight LMMs with reinforcement learning using verifiable rewards
improves accuracy across six medical VQA benchmarks, achieving averages of
55.85 (3B) and 58.15 (7B), with up to 77.57 on VQA-RAD and 67.76 on PathVQA,
outperforming strong medical LMMs. A Ablations verify that both generation and
verification are necessary and that more verified data consistently helps, and
a targeted contamination analysis detects no leakage from evaluation suites. By
operating entirely on open literature and open-weight models, MedVLSynther
offers an auditable, reproducible, and privacy-preserving path to scalable
medical VQA training data.

</details>


### [167] [$π_\texttt{RL}$: Online RL Fine-tuning for Flow-based Vision-Language-Action Models](https://arxiv.org/abs/2510.25889)
*Kang Chen,Zhihao Liu,Tonghe Zhang,Zhen Guo,Si Xu,Hao Lin,Hongzhi Zang,Quanlu Zhang,Zhaofei Yu,Guoliang Fan,Tiejun Huang,Yu Wang,Chao Yu*

Main category: cs.LG

TL;DR: 提出了一个名为$\\pi_{\\text{RL}}$的开源框架，用于在并行仿真中训练基于流的VLA，以解决由于迭代去噪带来的难以处理的动作对数似然问题。


<details>
  <summary>Details</summary>
Motivation: 探索使用强化学习(RL)来自动进行数据收集过程，以扩展监督微调(SFT)，但由于迭代去噪带来的难以处理的动作对数似然问题，将大规模RL应用于基于流的VLA仍然具有挑战性。

Method: $\pi_{\\text{RL}}$实现了两种RL算法：(1) {Flow-Noise}将去噪过程建模为一个离散时间MDP，带有一个可学习的噪声网络，用于精确的对数似然计算。(2) {Flow-SDE}将去噪与agent-environment交互集成，构建一个两层MDP，该MDP采用ODE到SDE的转换，以实现高效的RL探索。

Result: 在LIBERO上，$\pi_{\\text{RL}}$将few-shot SFT模型$\pi_0$和$\pi_{0.5}$从57.6%提高到97.6%，从77.1%提高到98.3%。在ManiSkill中，$\pi_{\\text{RL}}$在320个并行环境中进行训练，在4352个拾取和放置任务中，将$\pi_0$从41.6%提高到85.7%，将$\pi_{0.5}$从40.0%提高到84.8%。

Conclusion: $\pi_{\\text{RL}}$实现了显著的性能提升，并且比SFT模型具有更强的泛化能力，验证了在线RL对于基于流的VLA的有效性。

Abstract: Vision-Language-Action (VLA) models enable robots to understand and perform
complex tasks from multimodal input. Although recent work explores using
reinforcement learning (RL) to automate the laborious data collection process
in scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based
VLAs (e.g., $\pi_0$, $\pi_{0.5}$) remains challenging due to intractable action
log-likelihoods from iterative denoising.
  We address this challenge with $\pi_{\text{RL}}$, an open-source framework
for training flow-based VLAs in parallel simulation. $\pi_{\text{RL}}$
implements two RL algorithms: (1) {Flow-Noise} models the denoising process as
a discrete-time MDP with a learnable noise network for exact log-likelihood
computation. (2) {Flow-SDE} integrates denoising with agent-environment
interaction, formulating a two-layer MDP that employs ODE-to-SDE conversion for
efficient RL exploration.
  We evaluate $\pi_{\text{RL}}$ on LIBERO and ManiSkill benchmarks. On LIBERO,
$\pi_{\text{RL}}$ boosts few-shot SFT models $\pi_0$ and $\pi_{0.5}$ from 57.6%
to 97.6% and from 77.1% to 98.3%, respectively. In ManiSkill, we train
$\pi_{\text{RL}}$ in 320 parallel environments, improving $\pi_0$ from 41.6% to
85.7% and $\pi_{0.5}$ from 40.0% to 84.8% across 4352 pick-and-place tasks,
demonstrating scalable multitask RL under heterogeneous simulation.
  Overall, $\pi_{\text{RL}}$ achieves significant performance gains and
stronger generalization over SFT-models, validating the effectiveness of online
RL for flow-based VLAs.

</details>


### [168] [Topology-Aware Active Learning on Graphs](https://arxiv.org/abs/2510.25892)
*Harris Hardiman-Mostow,Jack Mauro,Adrien Weihs,Andrea L. Bertozzi*

Main category: cs.LG

TL;DR: 提出了一种图拓扑主动学习方法，通过平衡的 Forman 曲率 (BFC) 来指导探索，并动态触发从探索到利用的转变，同时引入局部图重连策略来提高利用率。


<details>
  <summary>Details</summary>
Motivation: 在标签预算有限的情况下，解决主动学习中探索与利用之间的核心挑战。

Method: 使用基于平衡 Forman 曲率 (BFC) 的 coreset 构建算法选择代表性的初始标签，并使用数据驱动的停止准则来判断图是否被充分探索。引入局部图重连策略，有效地整合标记节点周围的多尺度信息。

Result: 在基准分类任务上的实验表明，该方法在低标签率下始终优于现有的基于图的半监督基线方法。

Conclusion: 该方法在主动学习中，通过图拓扑方法有效地平衡了探索与利用，并在低标签率下表现出色。

Abstract: We propose a graph-topological approach to active learning that directly
targets the core challenge of exploration versus exploitation under scarce
label budgets. To guide exploration, we introduce a coreset construction
algorithm based on Balanced Forman Curvature (BFC), which selects
representative initial labels that reflect the graph's cluster structure. This
method includes a data-driven stopping criterion that signals when the graph
has been sufficiently explored. We further use BFC to dynamically trigger the
shift from exploration to exploitation within active learning routines,
replacing hand-tuned heuristics. To improve exploitation, we introduce a
localized graph rewiring strategy that efficiently incorporates multiscale
information around labeled nodes, enhancing label propagation while preserving
sparsity. Experiments on benchmark classification tasks show that our methods
consistently outperform existing graph-based semi-supervised baselines at low
label rates.

</details>


### [169] [Transferring Causal Effects using Proxies](https://arxiv.org/abs/2510.25924)
*Manuel Iglesias-Alonso,Felix Schur,Julius von Kügelgen,Jonas Peters*

Main category: cs.LG

TL;DR: 本文研究了多域环境下的因果效应估计问题，该效应受到未观察到的混淆因素的影响，并且在不同领域之间会发生变化。作者提出了一种方法来估计目标域中的因果效应，并证明了可识别性、一致性，并推导了置信区间。并通过模拟研究和实际案例验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 在多域环境下估计因果效应是一个重要的问题，因为因果效应可能会受到未观察到的混淆因素的影响，并且在不同领域之间会发生变化。

Method: 本文提出了一种新的方法来估计目标域中的因果效应，该方法假设可以访问隐藏混淆因素的代理变量，并且所有变量都是离散的或分类的。该方法包括两种估计技术，并证明了其一致性。

Result: 本文证明了在一定条件下，即使当处理变量和响应变量是连续的时，因果效应也是可识别的。并通过模拟研究和实际案例验证了理论结果。

Conclusion: 本文提出了一种新的方法来估计多域环境下的因果效应，该方法具有理论上的可靠性和实际应用价值。

Abstract: We consider the problem of estimating a causal effect in a multi-domain
setting. The causal effect of interest is confounded by an unobserved
confounder and can change between the different domains. We assume that we have
access to a proxy of the hidden confounder and that all variables are discrete
or categorical. We propose methodology to estimate the causal effect in the
target domain, where we assume to observe only the proxy variable. Under these
conditions, we prove identifiability (even when treatment and response
variables are continuous). We introduce two estimation techniques, prove
consistency, and derive confidence intervals. The theoretical results are
supported by simulation studies and a real-world example studying the causal
effect of website rankings on consumer choices.

</details>


### [170] [Active Learning with Task-Driven Representations for Messy Pools](https://arxiv.org/abs/2510.25926)
*Kianoosh Ashouritaklimi,Tom Rainforth*

Main category: cs.LG

TL;DR: 本文提出了一种主动学习方法，该方法使用任务驱动的表示，这些表示在主动学习过程中使用先前收集的标签定期更新，从而提高了经验性能。


<details>
  <summary>Details</summary>
Motivation: 现有技术依赖于使用固定的、无监督的池表示，但这种模型设置会削弱其处理混乱池的有效性，因为这些表示无法捕获与任务相关的重要信息。

Method: 本文提出了两种学习这些表示的策略，一种是直接学习半监督表示，另一种是基于对初始无监督表示进行监督微调。

Result: 这两种策略都显著提高了使用无监督或预训练表示的经验性能。

Conclusion: 任务驱动的表示可以提高主动学习在处理混乱池时的有效性。

Abstract: Active learning has the potential to be especially useful for messy,
uncurated pools where datapoints vary in relevance to the target task. However,
state-of-the-art approaches to this problem currently rely on using fixed,
unsupervised representations of the pool, focusing on modifying the acquisition
function instead. We show that this model setup can undermine their
effectiveness at dealing with messy pools, as such representations can fail to
capture important information relevant to the task. To address this, we propose
using task-driven representations that are periodically updated during the
active learning process using the previously collected labels. We introduce two
specific strategies for learning these representations, one based on directly
learning semi-supervised representations and the other based on supervised
fine-tuning of an initial unsupervised representation. We find that both
significantly improve empirical performance over using unsupervised or
pretrained representations.

</details>


### [171] [Robust GNN Watermarking via Implicit Perception of Topological Invariants](https://arxiv.org/abs/2510.25934)
*Jipeng Li,Yannning Shen*

Main category: cs.LG

TL;DR: InvGNN-WM: 通过将所有权与模型对图不变性的隐式感知联系起来，实现无触发、黑盒验证，且对任务影响可忽略不计。


<details>
  <summary>Details</summary>
Motivation: 许多水印依赖于后门触发器，这些触发器在常见的模型编辑下会失效，并产生所有权歧义。因此，需要一个更鲁棒的水印方案。

Method: 提出 InvGNN-WM，它预测所有者私有载体集上的归一化代数连通性；一个符号敏感的解码器输出比特，一个校准的阈值控制假阳性率。

Result: 在各种节点和图分类数据集和骨干网络上，InvGNN-WM 匹配了 clean 准确率，同时产生了比基于触发器和压缩的基线更高的水印准确率。它在非结构化剪枝、微调和训练后量化下仍然很强大；普通的知识蒸馏 (KD) 削弱了水印，而带有水印损失的 KD (KD+WM) 恢复了它。

Conclusion: 提供了对不可感知性和鲁棒性的保证，并证明了精确移除是 NP-complete 的。

Abstract: Graph Neural Networks (GNNs) are valuable intellectual property, yet many
watermarks rely on backdoor triggers that break under common model edits and
create ownership ambiguity. We present InvGNN-WM, which ties ownership to a
model's implicit perception of a graph invariant, enabling trigger-free,
black-box verification with negligible task impact. A lightweight head predicts
normalized algebraic connectivity on an owner-private carrier set; a
sign-sensitive decoder outputs bits, and a calibrated threshold controls the
false-positive rate. Across diverse node and graph classification datasets and
backbones, InvGNN-WM matches clean accuracy while yielding higher watermark
accuracy than trigger- and compression-based baselines. It remains strong under
unstructured pruning, fine-tuning, and post-training quantization; plain
knowledge distillation (KD) weakens the mark, while KD with a watermark loss
(KD+WM) restores it. We provide guarantees for imperceptibility and robustness,
and we prove that exact removal is NP-complete.

</details>


### [172] [Modular Linear Tokenization (MLT)](https://arxiv.org/abs/2510.25952)
*Tcharlies Schmitz*

Main category: cs.LG

TL;DR: 提出了一种可逆的确定性编码技术，用于将高基数分类标识符编码为紧凑的数值向量。


<details>
  <summary>Details</summary>
Motivation: 传统哈希或one-hot编码无法保持双射映射。

Method: 利用有限域上的模运算和可逆线性变换。

Result: 在MovieLens 20M数据集上的实验结果表明，MLT实现了与监督嵌入相当的预测性能，同时需要的参数更少，训练成本更低。

Conclusion: MLT是一种有效的编码高基数分类标识符的方法，它具有可逆性、可控的维度和计算可扩展性等优点。

Abstract: This paper introduces Modular Linear Tokenization (MLT), a reversible and
deterministic technique for encoding high-cardinality categorical identifiers
into compact numerical vectors. Unlike traditional hashing or one-hot
encodings, MLT preserves bijective mappings by leveraging modular arithmetic
over finite fields and invertible linear transformations. The method offers
explicit control of dimensionality and computational scalability while
maintaining full reversibility, even for millions of identifiers. Experimental
results on the MovieLens 20M dataset show that MLT achieves comparable
predictive performance to supervised embeddings while requiring significantly
fewer parameters and lower training cost. An open-source implementation of MLT
is available on PyPI (https://pypi.org/project/light-mlt/) and GitHub
(https://github.com/tcharliesschmitz/light-mlt).

</details>


### [173] [Application and Validation of Geospatial Foundation Model Data for the Prediction of Health Facility Programmatic Outputs -- A Case Study in Malawi](https://arxiv.org/abs/2510.25954)
*Lynn Metz,Rachel Haggard,Michael Moszczynski,Samer Asbah,Chris Mwase,Patricia Khomani,Tyler Smith,Hannah Cooper,Annie Mwale,Arbaaz Muslim,Gautam Prasad,Mimi Sun,Tomer Shekel,Joydeep Paul,Anna Carter,Shravya Shetty,Dylan Green*

Main category: cs.LG

TL;DR: 本研究评估了三种地理空间基础模型（GeoFM）嵌入源在马拉维预测15项常规健康计划产出方面的性能，并将其效用与传统地理空间插值方法进行了比较。结果表明，基于嵌入的方法在大多数指标上优于基线地统计方法，集成了所有三种嵌入源的多GeoFM模型产生了最稳健的预测。


<details>
  <summary>Details</summary>
Motivation: 低收入和中等收入国家(LMIC)的常规健康数据的可靠性通常受到报告延迟和不完整覆盖的限制，因此需要探索新的数据来源和分析方法。地理空间基础模型(GeoFM)通过将不同的空间、时间和行为数据合成为数学嵌入，提供了一个有希望的途径，可以有效地用于下游预测任务。

Method: 本研究使用XGBoost模型，利用来自552个健康 catchment 区域（2021年1月-2023年5月）的数据，使用R2评估性能，并使用80/20的训练和测试数据分割，在训练中使用5倍交叉验证。

Result: 虽然预测性能不一，但基于嵌入的方法在15个测试指标中的13个(87%)上改进了基线地统计方法。整合所有三种嵌入源的多GeoFM模型产生了最稳健的预测，人口密度、新增HIV病例和儿童疫苗接种等指标的平均5倍交叉验证R2值分别为0.63、0.57和0.47，测试集R2分别为0.64、0.68和0.55。对于主要数据可用性较低的预测目标，如结核病和营养不良病例，预测效果较差。

Conclusion: 研究结果表明，GeoFM嵌入为LMIC背景下特定健康和人口结果带来了适度的预测改进。多种GeoFM来源的集成是补充和加强受限的常规健康信息系统的有效且有价值的工具。

Abstract: The reliability of routine health data in low and middle-income countries
(LMICs) is often constrained by reporting delays and incomplete coverage,
necessitating the exploration of novel data sources and analytics. Geospatial
Foundation Models (GeoFMs) offer a promising avenue by synthesizing diverse
spatial, temporal, and behavioral data into mathematical embeddings that can be
efficiently used for downstream prediction tasks. This study evaluated the
predictive performance of three GeoFM embedding sources - Google Population
Dynamics Foundation Model (PDFM), Google AlphaEarth (derived from satellite
imagery), and mobile phone call detail records (CDR) - for modeling 15 routine
health programmatic outputs in Malawi, and compared their utility to
traditional geospatial interpolation methods. We used XGBoost models on data
from 552 health catchment areas (January 2021-May 2023), assessing performance
with R2, and using an 80/20 training and test data split with 5-fold
cross-validation used in training. While predictive performance was mixed, the
embedding-based approaches improved upon baseline geostatistical methods in 13
of 15 (87%) indicators tested. A Multi-GeoFM model integrating all three
embedding sources produced the most robust predictions, achieving average
5-fold cross validated R2 values for indicators like population density (0.63),
new HIV cases (0.57), and child vaccinations (0.47) and test set R2 of 0.64,
0.68, and 0.55, respectively. Prediction was poor for prediction targets with
low primary data availability, such as TB and malnutrition cases. These results
demonstrate that GeoFM embeddings imbue a modest predictive improvement for
select health and demographic outcomes in an LMIC context. We conclude that the
integration of multiple GeoFM sources is an efficient and valuable tool for
supplementing and strengthening constrained routine health information systems.

</details>


### [174] [On the Dataless Training of Neural Networks](https://arxiv.org/abs/2510.25962)
*Alvaro Velasquez,Susmit Jha,Ismail R. Alkhouri*

Main category: cs.LG

TL;DR: 本文综述了在无训练数据设置下使用神经网络进行优化的研究。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的学习方法尚不完善，且在组合优化等领域效果不佳；在医学图像重建等科学应用中，训练数据本身就有限。

Method: 本文考察了使用全连接网络（或 MLP）、卷积网络、图网络和二次神经网络，通过重新参数化问题，在优化中对神经网络架构进行无数据应用。

Result: MLP 几十年前已被用于解决线性规划问题，但由于其在各种应用中（包括基于组合优化、逆问题和偏微分方程的应用）展现出良好的效果，因此最近这种方法越来越受到关注。

Conclusion: 本文定义了无数据设置，并根据问题实例（由单个数据定义）在神经网络上的编码方式将其分为两种变体：（i）与架构无关的方法和（ii）与架构相关的方法。此外，我们还讨论了无数据神经网络 (dNN) 设置与零样本学习、单样本学习、优化提升和过度参数化等相关概念之间的异同。

Abstract: This paper surveys studies on the use of neural networks for optimization in
the training-data-free setting. Specifically, we examine the dataless
application of neural network architectures in optimization by
re-parameterizing problems using fully connected (or MLP), convolutional,
graph, and quadratic neural networks. Although MLPs have been used to solve
linear programs a few decades ago, this approach has recently gained increasing
attention due to its promising results across diverse applications, including
those based on combinatorial optimization, inverse problems, and partial
differential equations. The motivation for this setting stems from two key
(possibly over-lapping) factors: (i) data-driven learning approaches are still
underdeveloped and have yet to demonstrate strong results, as seen in
combinatorial optimization, and (ii) the availability of training data is
inherently limited, such as in medical image reconstruction and other
scientific applications. In this paper, we define the dataless setting and
categorize it into two variants based on how a problem instance -- defined by a
single datum -- is encoded onto the neural network: (i) architecture-agnostic
methods and (ii) architecture-specific methods. Additionally, we discuss
similarities and clarify distinctions between the dataless neural network (dNN)
settings and related concepts such as zero-shot learning, one-shot learning,
lifting in optimization, and over-parameterization.

</details>


### [175] [Contrastive Predictive Coding Done Right for Mutual Information Estimation](https://arxiv.org/abs/2510.25983)
*J. Jon Ryu,Pavan Yeddanapudi,Xiangxiang Xu,Gregory W. Wornell*

Main category: cs.LG

TL;DR: InfoNCE不应被视为有效的互信息(MI)估计器。文章引入了一个简单的修改InfoNCE-anchor，用于准确的MI估计，并通过适当的评分规则推广了这个框架，统一了包括NCE, InfoNCE, 和 f-散度变体在内的一系列对比目标。


<details>
  <summary>Details</summary>
Motivation: InfoNCE虽然被广泛用于互信息(MI)估计，但它与MI的联系并不直接。本文旨在解释为什么InfoNCE不应被视为有效的MI估计器。

Method: 引入了一个辅助的anchor类别，实现了稳定一致的密度比估计，并产生了一个偏差显著降低的plug-in MI估计器。此外，使用适当的评分规则对框架进行了推广。

Result: InfoNCE-anchor 使用log评分实现了最准确的MI估计。在自监督表征学习实验中，anchor并没有提高下游任务的性能。对比表征学习的优势并非来自准确的MI估计，而是来自结构化密度比的学习。

Conclusion: 本文表明InfoNCE不应被视为有效的MI估计器，并提出了InfoNCE-anchor作为改进。实验结果表明，对比表征学习的优势不在于准确的MI估计。

Abstract: The InfoNCE objective, originally introduced for contrastive representation
learning, has become a popular choice for mutual information (MI) estimation,
despite its indirect connection to MI. In this paper, we demonstrate why
InfoNCE should not be regarded as a valid MI estimator, and we introduce a
simple modification, which we refer to as InfoNCE-anchor, for accurate MI
estimation. Our modification introduces an auxiliary anchor class, enabling
consistent density ratio estimation and yielding a plug-in MI estimator with
significantly reduced bias. Beyond this, we generalize our framework using
proper scoring rules, which recover InfoNCE-anchor as a special case when the
log score is employed. This formulation unifies a broad spectrum of contrastive
objectives, including NCE, InfoNCE, and $f$-divergence variants, under a single
principled framework. Empirically, we find that InfoNCE-anchor with the log
score achieves the most accurate MI estimates; however, in self-supervised
representation learning experiments, we find that the anchor does not improve
the downstream task performance. These findings corroborate that contrastive
representation learning benefits not from accurate MI estimation per se, but
from the learning of structured density ratios.

</details>


### [176] [A General and Streamlined Differentiable Optimization Framework](https://arxiv.org/abs/2510.25986)
*Andrew W. Rosemberg,Joaquim Dias Garcia,François Pacaud,Robert B. Parker,Benoît Legat,Kaarthik Sundar,Russell Bent,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: 提出了一个通用的、简化的框架 DiffOpt.jl，它统一了 Julia 优化堆栈中的建模和微分。


<details>
  <summary>Details</summary>
Motivation: 在学习、控制和大规模决策系统中使用受约束的优化问题进行区分越来越重要，但由于求解器专业化和接口不匹配，实际集成仍然具有挑战性。

Method: 通过在标准正则性假设下区分 KKT 系统，计算平滑的、可能非凸程序的正向和反向模式解和目标灵敏度。一等的、JuMP 原生的、以参数为中心的 API 允许用户声明命名参数，并直接获得关于这些参数的导数——即使参数出现在多个约束和目标中——消除了系级别接口的脆弱记账。

Result: 在凸和非凸模型上展示了这些能力，包括经济调度、具有锥风险约束的均值-方差投资组合选择和非线性机器人逆运动学。两个配套研究进一步证明了规模化的影响：能源市场战略投标的基于梯度的迭代方法和使用求解器精确灵敏度的端到端优化代理的 Sobolev 风格训练。

Conclusion: 可微优化可以作为实验、学习、校准和设计的常规工具进行部署，而不会偏离标准的 JuMP 建模实践，同时保留对广泛的求解器生态系统的访问。

Abstract: Differentiating through constrained optimization problems is increasingly
central to learning, control, and large-scale decision-making systems, yet
practical integration remains challenging due to solver specialization and
interface mismatches. This paper presents a general and streamlined
framework-an updated DiffOpt.jl-that unifies modeling and differentiation
within the Julia optimization stack. The framework computes forward - and
reverse-mode solution and objective sensitivities for smooth, potentially
nonconvex programs by differentiating the KKT system under standard regularity
assumptions. A first-class, JuMP-native parameter-centric API allows users to
declare named parameters and obtain derivatives directly with respect to them -
even when a parameter appears in multiple constraints and objectives -
eliminating brittle bookkeeping from coefficient-level interfaces. We
illustrate these capabilities on convex and nonconvex models, including
economic dispatch, mean-variance portfolio selection with conic risk
constraints, and nonlinear robot inverse kinematics. Two companion studies
further demonstrate impact at scale: gradient-based iterative methods for
strategic bidding in energy markets and Sobolev-style training of end-to-end
optimization proxies using solver-accurate sensitivities. Together, these
results demonstrate that differentiable optimization can be deployed as a
routine tool for experimentation, learning, calibration, and design-without
deviating from standard JuMP modeling practices and while retaining access to a
broad ecosystem of solvers.

</details>


### [177] [Efficient Online Learning with Predictive Coding Networks: Exploiting Temporal Correlations](https://arxiv.org/abs/2510.25993)
*Darius Masoum Zadeh-Jousdani,Elvin Hajizada,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 提出了一种名为时间摊销预测编码网络 (PCN-TA) 的新型在线学习算法，该算法通过利用时间相关性来减少计算需求，同时保持学习性能，适用于边缘部署和资源受限的机器人系统。


<details>
  <summary>Details</summary>
Motivation: 传统的反向传播算法不符合生物合理性原则，可能不适合连续适应场景。预测编码 (PC) 框架提供了一种生物学上合理的替代方案，但其主要限制是训练期间的计算开销。

Method: 提出了预测编码网络与时间摊销 (PCN-TA)，它跨时间帧保留潜在状态，通过利用时间相关性来减少计算需求。

Result: 在 COIL-20 机器人感知数据集上的实验表明，与反向传播相比，PCN-TA 减少了 10% 的权重更新，并且比基线 PC 网络减少了 50% 的推理步骤。

Conclusion: PCN-TA 是一种有前途的边缘部署和实时自适应支持方法，其生物学特性使其成为未来神经形态硬件实施的候选者。

Abstract: Robotic systems operating at the edge require efficient online learning
algorithms that can continuously adapt to changing environments while
processing streaming sensory data. Traditional backpropagation, while
effective, conflicts with biological plausibility principles and may be
suboptimal for continuous adaptation scenarios. The Predictive Coding (PC)
framework offers a biologically plausible alternative with local, Hebbian-like
update rules, making it suitable for neuromorphic hardware implementation.
However, PC's main limitation is its computational overhead due to multiple
inference iterations during training. We present Predictive Coding Network with
Temporal Amortization (PCN-TA), which preserves latent states across temporal
frames. By leveraging temporal correlations, PCN-TA significantly reduces
computational demands while maintaining learning performance. Our experiments
on the COIL-20 robotic perception dataset demonstrate that PCN-TA achieves 10%
fewer weight updates compared to backpropagation and requires 50% fewer
inference steps than baseline PC networks. These efficiency gains directly
translate to reduced computational overhead for moving another step toward edge
deployment and real-time adaptation support in resource-constrained robotic
systems. The biologically-inspired nature of our approach also makes it a
promising candidate for future neuromorphic hardware implementations, enabling
efficient online learning at the edge.

</details>


### [178] [Infrequent Exploration in Linear Bandits](https://arxiv.org/abs/2510.26000)
*Harin Lee,Min-hwan Oh*

Main category: cs.LG

TL;DR: 提出了一种名为INFEX的线性bandit算法框架，旨在解决exploration频率的问题，该框架允许按照预定的时间表执行exploration策略，并在exploration间隙主要选择greedy策略。


<details>
  <summary>Details</summary>
Motivation: 现有算法要么过度exploration，要么需要严格的diversity假设，而实际应用中exploration可能不切实际或不道德，greedy策略在缺乏足够contextual diversity时通常会失败。

Method: INFEX框架通过控制exploration频率，在exploration期间执行exploration策略，其余时间选择greedy策略。

Result: INFEX在exploration频率超过对数阈值时，可以实现与标准算法相匹配的instance-dependent regret，并且具有广泛的适用性和易用性。实验结果表明，INFEX在regret性能和运行时间上均优于现有方法。

Conclusion: INFEX框架在理论和实验上都表现出良好的性能，能够有效地解决线性bandits中exploration频率的问题。

Abstract: We study the problem of infrequent exploration in linear bandits, addressing
a significant yet overlooked gap between fully adaptive exploratory methods
(e.g., UCB and Thompson Sampling), which explore potentially at every time
step, and purely greedy approaches, which require stringent diversity
assumptions to succeed. Continuous exploration can be impractical or unethical
in safety-critical or costly domains, while purely greedy strategies typically
fail without adequate contextual diversity. To bridge these extremes, we
introduce a simple and practical framework, INFEX, explicitly designed for
infrequent exploration. INFEX executes a base exploratory policy according to a
given schedule while predominantly choosing greedy actions in between. Despite
its simplicity, our theoretical analysis demonstrates that INFEX achieves
instance-dependent regret matching standard provably efficient algorithms,
provided the exploration frequency exceeds a logarithmic threshold.
Additionally, INFEX is a general, modular framework that allows seamless
integration of any fully adaptive exploration method, enabling wide
applicability and ease of adoption. By restricting intensive exploratory
computations to infrequent intervals, our approach can also enhance
computational efficiency. Empirical evaluations confirm our theoretical
findings, showing state-of-the-art regret performance and runtime improvements
over existing methods.

</details>


### [179] [Dual Mixture-of-Experts Framework for Discrete-Time Survival Analysis](https://arxiv.org/abs/2510.26014)
*Hyeonjun Lee,Hyungseob Shin,Gunhee Nam,Hyeonsoo Lee*

Main category: cs.LG

TL;DR: 提出了一种用于离散时间生存分析的双重混合专家(MoE)框架，以应对患者异质性和时间动态风险预测的挑战。


<details>
  <summary>Details</summary>
Motivation: 在临床和生物医学研究中，生存分析被广泛用于建模事件发生的时间，但关键挑战在于对患者异质性进行建模，同时使风险预测适应个体特征和时间动态。

Method: 结合了用于子群感知表示学习的特征编码器MoE和利用患者特征和时间嵌入来捕获时间动态的风险MoE。这种双重MoE设计可以灵活地与现有的基于深度学习的生存管道集成。

Result: 在METABRIC和GBSG乳腺癌数据集上，该方法持续提高了性能，在测试集上将时间依赖的C指数提高了0.04。

Conclusion: 该方法可以有效地提高生存分析的预测性能，并且可以很容易地集成到现有的深度学习框架中。

Abstract: Survival analysis is a task to model the time until an event of interest
occurs, widely used in clinical and biomedical research. A key challenge is to
model patient heterogeneity while also adapting risk predictions to both
individual characteristics and temporal dynamics. We propose a dual
mixture-of-experts (MoE) framework for discrete-time survival analysis. Our
approach combines a feature-encoder MoE for subgroup-aware representation
learning with a hazard MoE that leverages patient features and time embeddings
to capture temporal dynamics. This dual-MoE design flexibly integrates with
existing deep learning based survival pipelines. On METABRIC and GBSG breast
cancer datasets, our method consistently improves performance, boosting the
time-dependent C-index up to 0.04 on the test sets, and yields further gains
when incorporated into the Consurv framework.

</details>


### [180] [Exploring Human-AI Conceptual Alignment through the Prism of Chess](https://arxiv.org/abs/2510.26025)
*Semyon Lomaso,Judah Goldfeder,Mehmet Hamza Erol,Matthew So,Yao Yan,Addison Howard,Nathan Kutz,Ravid Shwartz Ziv*

Main category: cs.LG

TL;DR: AI即使在国际象棋中也难以真正理解人类的概念，而是倾向于模仿表面模式。


<details>
  <summary>Details</summary>
Motivation: 探讨AI系统是否真正理解人类概念，还是仅仅模仿表面模式。通过分析在国际象棋中的AI系统，研究人类创造力与精确战略概念的结合。

Method: 分析一个达到国际象棋大师水平的270M参数Transformer模型，并引入首个Chess960数据集进行概念鲁棒性测试。

Result: 早期层以高达85%的准确率编码中心控制和骑士前哨等人类概念，但更深层则偏离，准确率降至50-65%。在Chess960数据集中，概念识别率下降10-20%。

Conclusion: AI系统在优化性能时，会发展出越来越异化的智能，这对需要真正人机协作的创造性AI应用提出了挑战。

Abstract: Do AI systems truly understand human concepts or merely mimic surface
patterns? We investigate this through chess, where human creativity meets
precise strategic concepts. Analyzing a 270M-parameter transformer that
achieves grandmaster-level play, we uncover a striking paradox: while early
layers encode human concepts like center control and knight outposts with up to
85\% accuracy, deeper layers, despite driving superior performance, drift
toward alien representations, dropping to 50-65\% accuracy. To test conceptual
robustness beyond memorization, we introduce the first Chess960 dataset: 240
expert-annotated positions across 6 strategic concepts. When opening theory is
eliminated through randomized starting positions, concept recognition drops
10-20\% across all methods, revealing the model's reliance on memorized
patterns rather than abstract understanding. Our layer-wise analysis exposes a
fundamental tension in current architectures: the representations that win
games diverge from those that align with human thinking. These findings suggest
that as AI systems optimize for performance, they develop increasingly alien
intelligence, a critical challenge for creative AI applications requiring
genuine human-AI collaboration. Dataset and code are available at:
https://github.com/slomasov/ChessConceptsLLM.

</details>


### [181] [Do Students Debias Like Teachers? On the Distillability of Bias Mitigation Methods](https://arxiv.org/abs/2510.26038)
*Jiali Cheng,Chirag Agarwal,Hadi Amiri*

Main category: cs.LG

TL;DR: 知识蒸馏(KD)是一种有效的模型压缩和在模型之间传递知识的方法。然而，它对模型抵抗虚假相关性的鲁棒性的影响仍未被充分探索，虚假相关性会降低模型在分布外数据上的性能。本研究调查了知识蒸馏对自然语言推理(NLI)和图像分类任务中教师模型向学生模型传递“去偏见”能力的影响。


<details>
  <summary>Details</summary>
Motivation: 研究知识蒸馏对模型抵抗虚假相关性的鲁棒性的影响，以及知识蒸馏在传递去偏见能力方面的作用。

Method: 通过大量实验，研究知识蒸馏对自然语言推理(NLI)和图像分类任务的影响，并深入研究其内部注意模式和机制。

Result: 知识蒸馏会削弱模型的去偏见能力，注入教师知识并不能提高去偏见模型的训练效果，并且知识蒸馏后模型的鲁棒性可能保持稳定，但在不同类型的偏差之间可能存在显著差异。研究还找出了导致知识蒸馏后独特行为的内部注意模式和回路。

Conclusion: 提出了三种有效的解决方案来提高去偏见方法的可提炼性：开发高质量的数据进行增强，实施迭代知识蒸馏，并使用从教师模型获得的权重初始化学生模型。本研究是首次大规模研究知识蒸馏对去偏见的影响及其内部机制。研究结果提供了关于知识蒸馏如何工作以及如何设计更好的去偏见方法的理解。

Abstract: Knowledge distillation (KD) is an effective method for model compression and
transferring knowledge between models. However, its effect on model's
robustness against spurious correlations that degrade performance on
out-of-distribution data remains underexplored. This study investigates the
effect of knowledge distillation on the transferability of ``debiasing''
capabilities from teacher models to student models on natural language
inference (NLI) and image classification tasks. Through extensive experiments,
we illustrate several key findings: (i) overall the debiasing capability of a
model is undermined post-KD; (ii) training a debiased model does not benefit
from injecting teacher knowledge; (iii) although the overall robustness of a
model may remain stable post-distillation, significant variations can occur
across different types of biases; and (iv) we pin-point the internal attention
pattern and circuit that causes the distinct behavior post-KD. Given the above
findings, we propose three effective solutions to improve the distillability of
debiasing methods: developing high quality data for augmentation, implementing
iterative knowledge distillation, and initializing student models with weights
obtained from teacher models. To the best of our knowledge, this is the first
study on the effect of KD on debiasing and its interenal mechanism at scale.
Our findings provide understandings on how KD works and how to design better
debiasing methods.

</details>


### [182] [Towards Scaling Laws for Symbolic Regression](https://arxiv.org/abs/2510.26064)
*David Otte,Jörg K. H. Franke,Frank Hutter*

Main category: cs.LG

TL;DR: 本文研究了符号回归（SR）中的缩放规律，发现验证损失和求解率都遵循计算能力的幂律趋势。


<details>
  <summary>Details</summary>
Motivation: 探索深度学习符号回归中缩放的作用，并与遗传编程方法进行比较。

Method: 使用可扩展的端到端transformer pipeline和精心生成的训练数据，对五个不同模型大小进行了系统研究。

Result: 发现验证损失和求解率都遵循计算能力的幂律趋势；最佳批次大小和学习率随模型大小增长；token-to-parameter ratio约为15时达到最佳。

Conclusion: 符号回归性能在很大程度上可以通过计算能力来预测，并为训练下一代符号回归模型提供了重要的见解。

Abstract: Symbolic regression (SR) aims to discover the underlying mathematical
expressions that explain observed data. This holds promise for both gaining
scientific insight and for producing inherently interpretable and generalizable
models for tabular data. In this work we focus on the basics of SR. Deep
learning-based SR has recently become competitive with genetic programming
approaches, but the role of scale has remained largely unexplored. Inspired by
scaling laws in language modeling, we present the first systematic
investigation of scaling in SR, using a scalable end-to-end transformer
pipeline and carefully generated training data. Across five different model
sizes and spanning three orders of magnitude in compute, we find that both
validation loss and solved rate follow clear power-law trends with compute. We
further identify compute-optimal hyperparameter scaling: optimal batch size and
learning rate grow with model size, and a token-to-parameter ratio of
$\approx$15 is optimal in our regime, with a slight upward trend as compute
increases. These results demonstrate that SR performance is largely predictable
from compute and offer important insights for training the next generation of
SR models.

</details>


### [183] [Learning Geometry: A Framework for Building Adaptive Manifold Models through Metric Optimization](https://arxiv.org/abs/2510.26068)
*Di Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的机器学习范式，它将模型本身视为可塑的几何实体，通过优化流形上的度量张量场来动态塑造模型空间的几何结构。


<details>
  <summary>Details</summary>
Motivation: 传统方法在固定的几何空间中寻找最佳参数，缺乏灵活性。

Method: 构建了一个变分框架，其损失函数平衡了数据保真度和流形的内在几何复杂性。连续流形被离散成三角网格，度量张量由边长参数化，从而可以使用自动微分工具进行高效优化。

Result: 理论分析揭示了该框架与广义相对论中爱因斯坦-希尔伯特作用量之间的深刻类比。即使在固定拓扑的情况下，度量优化也比固定几何的模型具有更大的表达能力。

Conclusion: 为构建能够自主演化其几何和拓扑的完全动态的“元学习器”奠定了坚实的基础，并为科学模型发现和鲁棒表示学习等领域提供了广阔的应用前景。

Abstract: This paper proposes a novel paradigm for machine learning that moves beyond
traditional parameter optimization. Unlike conventional approaches that search
for optimal parameters within a fixed geometric space, our core idea is to
treat the model itself as a malleable geometric entity. Specifically, we
optimize the metric tensor field on a manifold with a predefined topology,
thereby dynamically shaping the geometric structure of the model space. To
achieve this, we construct a variational framework whose loss function
carefully balances data fidelity against the intrinsic geometric complexity of
the manifold. The former ensures the model effectively explains observed data,
while the latter acts as a regularizer, penalizing overly curved or irregular
geometries to encourage simpler models and prevent overfitting. To address the
computational challenges of this infinite-dimensional optimization problem, we
introduce a practical method based on discrete differential geometry: the
continuous manifold is discretized into a triangular mesh, and the metric
tensor is parameterized by edge lengths, enabling efficient optimization using
automatic differentiation tools. Theoretical analysis reveals a profound
analogy between our framework and the Einstein-Hilbert action in general
relativity, providing an elegant physical interpretation for the concept of
"data-driven geometry". We further argue that even with fixed topology, metric
optimization offers significantly greater expressive power than models with
fixed geometry. This work lays a solid foundation for constructing fully
dynamic "meta-learners" capable of autonomously evolving their geometry and
topology, and it points to broad application prospects in areas such as
scientific model discovery and robust representation learning.

</details>


### [184] [New Money: A Systematic Review of Synthetic Data Generation for Finance](https://arxiv.org/abs/2510.26076)
*James Meldrum,Basem Suleiman,Fethi Rabhi,Muhammad Johan Alibasa*

Main category: cs.LG

TL;DR: 本综述总结了 2018 年以来发表的 72 项关于合成金融数据生成的研究，旨在全面了解当前的研究情况。


<details>
  <summary>Details</summary>
Motivation: 在机器学习应用中使用敏感金融数据面临挑战，而合成数据生成是一种有前景的解决方案。本文旨在弥补该领域缺乏综合性研究的不足。

Method: 本文系统地回顾和分析了 72 项研究，对合成的金融信息类型、使用的生成方法和评估策略进行分类。

Result: 研究结果表明，基于 GAN 的方法在文献中占主导地位，尤其是在生成时间序列市场数据和表格信用数据方面。但对隐私保护措施的严格评估仍然不足。

Conclusion: 本综述概述了生成技术、应用和评估方法，强调了关键的研究差距，并为未来开发用于金融领域的稳健、保护隐私的合成数据解决方案提供了指导。

Abstract: Synthetic data generation has emerged as a promising approach to address the
challenges of using sensitive financial data in machine learning applications.
By leveraging generative models, such as Generative Adversarial Networks (GANs)
and Variational Autoencoders (VAEs), it is possible to create artificial
datasets that preserve the statistical properties of real financial records
while mitigating privacy risks and regulatory constraints. Despite the rapid
growth of this field, a comprehensive synthesis of the current research
landscape has been lacking. This systematic review consolidates and analyses 72
studies published since 2018 that focus on synthetic financial data generation.
We categorise the types of financial information synthesised, the generative
methods employed, and the evaluation strategies used to assess data utility and
privacy. The findings indicate that GAN-based approaches dominate the
literature, particularly for generating time-series market data and tabular
credit data. While several innovative techniques demonstrate potential for
improved realism and privacy preservation, there remains a notable lack of
rigorous evaluation of privacy safeguards across studies. By providing an
integrated overview of generative techniques, applications, and evaluation
methods, this review highlights critical research gaps and offers guidance for
future work aimed at developing robust, privacy-preserving synthetic data
solutions for the financial domain.

</details>


### [185] [Nirvana: A Specialized Generalist Model With Task-Aware Memory Mechanism](https://arxiv.org/abs/2510.26083)
*Yuhua Jiang,Shuang Cheng,Yihao Liu,Ermo Hua,Che Jiang,Weigao Sun,Yu Cheng,Feifei Gao,Biqing Qi,Bowen Zhou*

Main category: cs.LG

TL;DR: Nirvana, a Specialized Generalist Model (SGM), uses a specialized memory mechanism, linear time complexity, and test-time task information extraction to achieve expert-level performance in target domains while preserving broad capabilities.


<details>
  <summary>Details</summary>
Motivation: Traditional LLM structures lack specialized memory mechanisms guided by task information, hindering their ability to effectively adapt to specific domains.

Method: The paper introduces the Task-Aware Memory Trigger (Trigger) and the Specialized Memory Updater (Updater). Trigger treats each incoming sample as a self-supervised fine-tuning task, enabling on-the-fly adaptation. Updater dynamically memorizes context guided by Trigger. The model is post-trained with lightweight codecs on paired electromagnetic signals and MRI images.

Result: Nirvana achieves competitive or superior results on natural language modeling benchmarks. It also demonstrates higher-quality MRI reconstruction and generates accurate preliminary clinical reports compared to conventional MRI models and those with traditional LLM backbones.

Conclusion: Nirvana's specialized memory mechanism, guided by Trigger and Updater, enables effective adaptation to specific tasks and achieves improved performance in both general language and specialized medical domains (MRI).

Abstract: Specialized Generalist Models (SGMs) aim to preserve broad capabilities while
achieving expert-level performance in target domains. However, traditional LLM
structures including Transformer, Linear Attention, and hybrid models do not
employ specialized memory mechanism guided by task information. In this paper,
we present Nirvana, an SGM with specialized memory mechanism, linear time
complexity, and test-time task information extraction. Besides, we propose the
Task-Aware Memory Trigger ($\textit{Trigger}$) that flexibly adjusts memory
mechanism based on the current task's requirements. In Trigger, each incoming
sample is treated as a self-supervised fine-tuning task, enabling Nirvana to
adapt its task-related parameters on the fly to domain shifts. We also design
the Specialized Memory Updater ($\textit{Updater}$) that dynamically memorizes
the context guided by Trigger. We conduct experiments on both general language
tasks and specialized medical tasks. On a variety of natural language modeling
benchmarks, Nirvana achieves competitive or superior results compared to the
existing LLM structures. To prove the effectiveness of Trigger on specialized
tasks, we test Nirvana's performance on a challenging medical task, i.e.,
Magnetic Resonance Imaging (MRI). We post-train frozen Nirvana backbone with
lightweight codecs on paired electromagnetic signals and MRI images. Despite
the frozen Nirvana backbone, Trigger guides the model to adapt to the MRI
domain with the change of task-related parameters. Nirvana achieves
higher-quality MRI reconstruction compared to conventional MRI models as well
as the models with traditional LLMs' backbone, and can also generate accurate
preliminary clinical reports accordingly.

</details>


### [186] [LLMBisect: Breaking Barriers in Bug Bisection with A Comparative Analysis Pipeline](https://arxiv.org/abs/2510.26086)
*Zheng Zhang,Haonan Li,Xingyu Li,Hang Zhang,Zhiyun Qian*

Main category: cs.LG

TL;DR: 本文提出了一种基于大型语言模型（LLM）的多阶段流水线方法，用于更准确地识别引入bug的commit（BIC）。


<details>
  <summary>Details</summary>
Motivation: 传统基于补丁的二分法在bug定位方面存在局限性，例如假设BIC和补丁修改相同的函数，且过度依赖代码更改而忽略了commit message中的信息。

Method: 该方法利用LLM充分利用补丁信息，比较多个候选commit，并通过一系列下选择步骤逐步缩小候选范围。

Result: 该方法比现有最佳解决方案的准确率提高了38％以上，并且多阶段流水线比基于LLM的基线方法提高了60％的准确率。

Conclusion: 实验结果表明，所提出的综合多阶段流水线对于提高BIC识别的准确性至关重要。

Abstract: Bug bisection has been an important security task that aims to understand the
range of software versions impacted by a bug, i.e., identifying the commit that
introduced the bug. However, traditional patch-based bisection methods are
faced with several significant barriers: For example, they assume that the
bug-inducing commit (BIC) and the patch commit modify the same functions, which
is not always true. They often rely solely on code changes, while the commit
message frequently contains a wealth of vulnerability-related information. They
are also based on simple heuristics (e.g., assuming the BIC initializes lines
deleted in the patch) and lack any logical analysis of the vulnerability.
  In this paper, we make the observation that Large Language Models (LLMs) are
well-positioned to break the barriers of existing solutions, e.g., comprehend
both textual data and code in patches and commits. Unlike previous BIC
identification approaches, which yield poor results, we propose a comprehensive
multi-stage pipeline that leverages LLMs to: (1) fully utilize patch
information, (2) compare multiple candidate commits in context, and (3)
progressively narrow down the candidates through a series of down-selection
steps. In our evaluation, we demonstrate that our approach achieves
significantly better accuracy than the state-of-the-art solution by more than
38\%. Our results further confirm that the comprehensive multi-stage pipeline
is essential, as it improves accuracy by 60\% over a baseline LLM-based
bisection method.

</details>


### [187] [Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle Routing](https://arxiv.org/abs/2510.26089)
*Fazel Arasteh,Arian Haghparast,Manos Papagelis*

Main category: cs.LG

TL;DR: 提出了一种基于多智能体强化学习（MARL）的框架，用于协调、感知网络的车队导航，以解决动态车辆路径问题。


<details>
  <summary>Details</summary>
Motivation: 城市道路网络中的交通拥堵导致更长的出行时间和更高的排放，尤其是在高峰时段。最短路径优先（SPF）算法在静态网络中对于单辆车是最优的，但在动态、多车辆环境中表现不佳，通常通过将所有车辆沿着相同的路径行驶来加剧拥堵。

Method: 提出了自适应导航（AN），这是一个分散的MARL模型，其中每个交叉口代理根据（i）本地交通和（ii）使用图注意力网络（GAT）建模的邻域状态提供路径引导。为了提高大型网络中的可扩展性，进一步提出了分层中心自适应导航（HHAN），它是AN的扩展，仅将代理分配给关键交叉口（中心）。车辆在代理控制下以中心到中心的方式行驶，而SPF处理每个中心区域内的微路由。对于中心协调，HHAN在Attentive Q-Mixing（A-QMIX）框架下采用集中式训练与分散式执行（CTDE），该框架通过注意力聚合异步车辆决策。中心代理使用流感知状态特征，该特征结合了本地拥堵和预测动态，以进行主动路由。

Result: 在合成网格和真实城市地图（多伦多、曼哈顿）上的实验表明，AN减少了相对于SPF和学习基线的平均旅行时间，保持了100％的路由成功率。HHAN可以扩展到具有数百个交叉点的网络，在交通繁忙的情况下，最多可实现15.9％的改进。

Conclusion: 这些发现突出了网络约束的MARL在智能交通系统中实现可扩展、协调和拥堵感知的路由的潜力。

Abstract: Traffic congestion in urban road networks leads to longer trip times and
higher emissions, especially during peak periods. While the Shortest Path First
(SPF) algorithm is optimal for a single vehicle in a static network, it
performs poorly in dynamic, multi-vehicle settings, often worsening congestion
by routing all vehicles along identical paths. We address dynamic vehicle
routing through a multi-agent reinforcement learning (MARL) framework for
coordinated, network-aware fleet navigation. We first propose Adaptive
Navigation (AN), a decentralized MARL model where each intersection agent
provides routing guidance based on (i) local traffic and (ii) neighborhood
state modeled using Graph Attention Networks (GAT). To improve scalability in
large networks, we further propose Hierarchical Hub-based Adaptive Navigation
(HHAN), an extension of AN that assigns agents only to key intersections
(hubs). Vehicles are routed hub-to-hub under agent control, while SPF handles
micro-routing within each hub region. For hub coordination, HHAN adopts
centralized training with decentralized execution (CTDE) under the Attentive
Q-Mixing (A-QMIX) framework, which aggregates asynchronous vehicle decisions
via attention. Hub agents use flow-aware state features that combine local
congestion and predictive dynamics for proactive routing. Experiments on
synthetic grids and real urban maps (Toronto, Manhattan) show that AN reduces
average travel time versus SPF and learning baselines, maintaining 100% routing
success. HHAN scales to networks with hundreds of intersections, achieving up
to 15.9% improvement under heavy traffic. These findings highlight the
potential of network-constrained MARL for scalable, coordinated, and
congestion-aware routing in intelligent transportation systems.

</details>


### [188] [SAFE: A Novel Approach to AI Weather Evaluation through Stratified Assessments of Forecasts over Earth](https://arxiv.org/abs/2510.26099)
*Nick Masi,Randall Balestriero*

Main category: cs.LG

TL;DR: SAFE是一个用于评估地球上预测分层性能的软件包，它集成了各种数据领域，以按与地理空间网格点相关的不同属性进行分层


<details>
  <summary>Details</summary>
Motivation: 机器学习中评估模型性能的主流范例是基于测试集中所有样本的平均损失，未能考虑到人类发展和地理的非均匀分布

Method: SAFE集成了各种数据领域，以按与地理空间网格点相关的不同属性进行分层：地域（通常是国家）、全球子区域、收入和土地覆盖（陆地或水域）

Result: 利用SAFE来基准测试一系列最先进的基于AI的天气预测模型，发现它们在每个属性上的预测技能都存在差异

Conclusion: 通过超越全球平均指标，首次提出：模型在何处表现最佳或最差，以及哪些模型最公平？

Abstract: The dominant paradigm in machine learning is to assess model performance
based on average loss across all samples in some test set. This amounts to
averaging performance geospatially across the Earth in weather and climate
settings, failing to account for the non-uniform distribution of human
development and geography. We introduce Stratified Assessments of Forecasts
over Earth (SAFE), a package for elucidating the stratified performance of a
set of predictions made over Earth. SAFE integrates various data domains to
stratify by different attributes associated with geospatial gridpoints:
territory (usually country), global subregion, income, and landcover (land or
water). This allows us to examine the performance of models for each individual
stratum of the different attributes (e.g., the accuracy in every individual
country). To demonstrate its importance, we utilize SAFE to benchmark a zoo of
state-of-the-art AI-based weather prediction models, finding that they all
exhibit disparities in forecasting skill across every attribute. We use this to
seed a benchmark of model forecast fairness through stratification at different
lead times for various climatic variables. By moving beyond globally-averaged
metrics, we for the first time ask: where do models perform best or worst, and
which models are most fair? To support further work in this direction, the SAFE
package is open source and available at https://github.com/N-Masi/safe

</details>


### [189] [Do Not Step Into the Same River Twice: Learning to Reason from Trial and Error](https://arxiv.org/abs/2510.26109)
*Chenming Tang,Hsiu-Yuan Huang,Weijie Liu,Saiyong Yang,Yunfang Wu*

Main category: cs.LG

TL;DR: 提出了一种名为LTE的强化学习方法，通过提示LLM先前生成的错误答案和过长响应问题来改进LLM的推理能力，无需外部专家指导。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法仅基于LLM自身生成的响应进行训练，受限于LLM的初始能力，容易出现探索停滞。

Method: LTE方法提示LLM先前自我生成的错误答案和过长响应问题。

Result: 实验表明，LTE在六个数学基准测试中优于普通GRPO，Qwen3-4B-Base的Pass@1平均提升6.38，Pass@k平均提升9.00。

Conclusion: LTE成功缓解了探索停滞问题，并在训练过程中增强了利用和探索。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has significantly
boosted the reasoning capability of large language models (LLMs) recently.
However, existing RLVR approaches merely train LLMs based on their own
generated responses and are constrained by the initial capability of LLMs, thus
prone to exploration stagnation, in which LLMs fail to solve more training
problems and cannot further learn from the training data. Some work tries to
address this by leveraging off-policy solutions to training problems but
requires external guidance from experts which suffers from limited
availability. In this work, we propose LTE (Learning to reason from Trial and
Error), an approach hinting LLMs with their previously self-generated incorrect
answers and problem of overlong responses, which does not require any external
expert guidance. Experiments validate the effectiveness of LTE, which
outperforms the normal group relative policy optimization (GRPO) by 6.38 in
Pass@1 and 9.00 in Pass@k on average across six mathematics benchmarks for
Qwen3-4B-Base. Further analysis confirms that LTE successfully mitigates the
problem of exploration stagnation and enhances both exploitation and
exploration during training.

</details>


### [190] [maxVSTAR: Maximally Adaptive Vision-Guided CSI Sensing with Closed-Loop Edge Model Adaptation for Robust Human Activity Recognition](https://arxiv.org/abs/2510.26146)
*Kexing Liu*

Main category: cs.LG

TL;DR: maxVSTAR: A vision-guided model adaptation framework for edge-deployed CSI sensing systems that mitigates domain shift and enables continuous adaptation to environmental changes without manual intervention.


<details>
  <summary>Details</summary>
Motivation: Recognition performance of WiFi CSI-based HAR deteriorates under varying environmental and hardware conditions, limiting its deployment on edge devices.

Method: A closed-loop, vision-guided model adaptation framework (maxVSTAR) is proposed, integrating a cross-modal teacher-student architecture. A YOLO-based vision model provides real-time activity labels for online fine-tuning of a lightweight CSI-based HAR model (STAR) at the edge.

Result: When deployed on uncalibrated hardware, maxVSTAR restored the accuracy of the baseline STAR model from 49.14% to 81.51% after a single vision-guided adaptation cycle.

Conclusion: maxVSTAR enables dynamic, self-supervised model adaptation in privacy-conscious IoT environments, establishing a scalable and practical paradigm for long-term autonomous HAR using CSI sensing at the network edge.

Abstract: WiFi Channel State Information (CSI)-based human activity recognition (HAR)
provides a privacy-preserving, device-free sensing solution for smart
environments. However, its deployment on edge devices is severely constrained
by domain shift, where recognition performance deteriorates under varying
environmental and hardware conditions. This study presents maxVSTAR (maximally
adaptive Vision-guided Sensing Technology for Activity Recognition), a
closed-loop, vision-guided model adaptation framework that autonomously
mitigates domain shift for edge-deployed CSI sensing systems. The proposed
system integrates a cross-modal teacher-student architecture, where a
high-accuracy YOLO-based vision model serves as a dynamic supervisory signal,
delivering real-time activity labels for the CSI data stream. These labels
enable autonomous, online fine-tuning of a lightweight CSI-based HAR model,
termed Sensing Technology for Activity Recognition (STAR), directly at the
edge. This closed-loop retraining mechanism allows STAR to continuously adapt
to environmental changes without manual intervention. Extensive experiments
demonstrate the effectiveness of maxVSTAR. When deployed on uncalibrated
hardware, the baseline STAR model's recognition accuracy declined from 93.52%
to 49.14%. Following a single vision-guided adaptation cycle, maxVSTAR restored
the accuracy to 81.51%. These results confirm the system's capacity for
dynamic, self-supervised model adaptation in privacy-conscious IoT
environments, establishing a scalable and practical paradigm for long-term
autonomous HAR using CSI sensing at the network edge.

</details>


### [191] [STAR: A Privacy-Preserving, Energy-Efficient Edge AI Framework for Human Activity Recognition via Wi-Fi CSI in Mobile and Pervasive Computing Environments](https://arxiv.org/abs/2510.26148)
*Kexing Liu*

Main category: cs.LG

TL;DR: STAR: An edge-AI framework for real-time, energy-efficient human activity recognition (HAR) using Wi-Fi CSI on low-power devices.


<details>
  <summary>Details</summary>
Motivation: Existing HAR methods using Wi-Fi CSI are computationally inefficient, have high latency, and limited feasibility in resource-constrained environments.

Method: A lightweight GRU-based neural network, adaptive signal processing, and hardware-aware co-optimization are integrated. The system uses a streamlined GRU, multi-stage pre-processing for denoising and feature extraction, and is implemented on a Rockchip RV1126 processor with an NPU.

Result: A mean recognition accuracy of 93.52% across seven activity classes and 99.11% for human presence detection is achieved, using a 97.6k-parameter model. INT8 quantized inference achieves 33 MHz processing speed with 8% CPU utilization, a sixfold improvement over CPU-based execution.

Conclusion: The system ensures real-time, privacy-preserving HAR with sub-second latency and low power consumption, providing a practical, scalable solution for mobile and pervasive computing.

Abstract: Human Activity Recognition (HAR) via Wi-Fi Channel State Information (CSI)
presents a privacy-preserving, contactless sensing approach suitable for smart
homes, healthcare monitoring, and mobile IoT systems. However, existing methods
often encounter computational inefficiency, high latency, and limited
feasibility within resource-constrained, embedded mobile edge environments.
This paper proposes STAR (Sensing Technology for Activity Recognition), an
edge-AI-optimized framework that integrates a lightweight neural architecture,
adaptive signal processing, and hardware-aware co-optimization to enable
real-time, energy-efficient HAR on low-power embedded devices. STAR
incorporates a streamlined Gated Recurrent Unit (GRU)-based recurrent neural
network, reducing model parameters by 33% compared to conventional LSTM models
while maintaining effective temporal modeling capability. A multi-stage
pre-processing pipeline combining median filtering, 8th-order Butterworth
low-pass filtering, and Empirical Mode Decomposition (EMD) is employed to
denoise CSI amplitude data and extract spatial-temporal features. For on-device
deployment, STAR is implemented on a Rockchip RV1126 processor equipped with an
embedded Neural Processing Unit (NPU), interfaced with an ESP32-S3-based CSI
acquisition module. Experimental results demonstrate a mean recognition
accuracy of 93.52% across seven activity classes and 99.11% for human presence
detection, utilizing a compact 97.6k-parameter model. INT8 quantized inference
achieves a processing speed of 33 MHz with just 8% CPU utilization, delivering
sixfold speed improvements over CPU-based execution. With sub-second response
latency and low power consumption, the system ensures real-time,
privacy-preserving HAR, offering a practical, scalable solution for mobile and
pervasive computing environments.

</details>


### [192] [Bridging the Gap Between Molecule and Textual Descriptions via Substructure-aware Alignment](https://arxiv.org/abs/2510.26157)
*Hyuntae Park,Yeachan Kim,SangKeun Lee*

Main category: cs.LG

TL;DR: MolBridge是一个新的分子-文本学习框架，它基于子结构感知对齐，通过引入额外的对齐信号并采用子结构感知对比学习和自细化机制来学习细粒度的对应关系。


<details>
  <summary>Details</summary>
Motivation: 现有的模型通常难以捕捉分子及其描述之间的细微差别，因为它们缺乏学习分子子结构和化学短语之间细粒度对齐的能力。

Method: MolBridge通过分子子结构和化学短语引入额外的对齐信号，并采用子结构感知对比学习，结合过滤噪声对齐信号的自细化机制。

Result: MolBridge有效地捕获了细粒度的对应关系，并在各种分子基准测试中优于最先进的基线。

Conclusion: 子结构感知对齐在分子-文本学习中具有重要意义。

Abstract: Molecule and text representation learning has gained increasing interest due
to its potential for enhancing the understanding of chemical information.
However, existing models often struggle to capture subtle differences between
molecules and their descriptions, as they lack the ability to learn
fine-grained alignments between molecular substructures and chemical phrases.
To address this limitation, we introduce MolBridge, a novel molecule-text
learning framework based on substructure-aware alignments. Specifically, we
augment the original molecule-description pairs with additional alignment
signals derived from molecular substructures and chemical phrases. To
effectively learn from these enriched alignments, MolBridge employs
substructure-aware contrastive learning, coupled with a self-refinement
mechanism that filters out noisy alignment signals. Experimental results show
that MolBridge effectively captures fine-grained correspondences and
outperforms state-of-the-art baselines on a wide range of molecular benchmarks,
highlighting the significance of substructure-aware alignment in molecule-text
learning.

</details>


### [193] [Segmentation over Complexity: Evaluating Ensemble and Hybrid Approaches for Anomaly Detection in Industrial Time Series](https://arxiv.org/abs/2510.26159)
*Emilio Mastriani,Alessandro Costa,Federico Incardona,Kevin Munari,Sebastiano Spinello*

Main category: cs.LG

TL;DR: 高级方法不如简单的集成模型适用于多元工业时间序列中的异常检测，尤其是在数据不平衡和时间不确定的情况下。


<details>
  <summary>Details</summary>
Motivation: 研究多元工业时间序列中异常检测的有效性，重点关注蒸汽轮机系统。

Method: 评估变更点导出的统计特征、基于聚类的子结构表示和混合学习策略对检测性能的影响。使用分段数据训练一个简单的随机森林+XGBoost集成模型。

Result: 该集成模型实现了0.976的AUC-ROC，0.41的F1分数，并在定义的时间窗口内实现了100%的早期检测。

Conclusion: 在高度不平衡和时间不确定的数据场景中，模型简单性与优化的分段相结合可以胜过更复杂的架构，提供更大的鲁棒性、可解释性和操作实用性。

Abstract: In this study, we investigate the effectiveness of advanced feature
engineering and hybrid model architectures for anomaly detection in a
multivariate industrial time series, focusing on a steam turbine system. We
evaluate the impact of change point-derived statistical features,
clustering-based substructure representations, and hybrid learning strategies
on detection performance. Despite their theoretical appeal, these complex
approaches consistently underperformed compared to a simple Random Forest +
XGBoost ensemble trained on segmented data. The ensemble achieved an AUC-ROC of
0.976, F1-score of 0.41, and 100% early detection within the defined time
window. Our findings highlight that, in scenarios with highly imbalanced and
temporally uncertain data, model simplicity combined with optimized
segmentation can outperform more sophisticated architectures, offering greater
robustness, interpretability, and operational utility.

</details>


### [194] [A Game-Theoretic Spatio-Temporal Reinforcement Learning Framework for Collaborative Public Resource Allocation](https://arxiv.org/abs/2510.26184)
*Songxin Lei,Qiongyan Wang,Yanchen Zhu,Hanyu Yao,Sijie Ruan,Weilin Ruan,Yuyu Luo,Huaming Wu,Yuxuan Liang*

Main category: cs.LG

TL;DR: 提出了一种新的公共资源分配问题（CPRA），该问题考虑了现实场景中的容量约束和时空动态。


<details>
  <summary>Details</summary>
Motivation: 现有方法侧重于独立优化单个资源的移动，而没有考虑其容量约束。

Method: 提出了一个名为博弈论时空强化学习（GSTRL）的新框架来解决CPRA问题。

Result: 在两个真实世界的数据集上评估了GSTRL，实验表明其性能优越。

Conclusion: 所提出的GSTRL框架有效地捕捉了整个系统的时空动态。

Abstract: Public resource allocation involves the efficient distribution of resources,
including urban infrastructure, energy, and transportation, to effectively meet
societal demands. However, existing methods focus on optimizing the movement of
individual resources independently, without considering their capacity
constraints. To address this limitation, we propose a novel and more practical
problem: Collaborative Public Resource Allocation (CPRA), which explicitly
incorporates capacity constraints and spatio-temporal dynamics in real-world
scenarios. We propose a new framework called Game-Theoretic Spatio-Temporal
Reinforcement Learning (GSTRL) for solving CPRA. Our contributions are twofold:
1) We formulate the CPRA problem as a potential game and demonstrate that there
is no gap between the potential function and the optimal target, laying a solid
theoretical foundation for approximating the Nash equilibrium of this NP-hard
problem; and 2) Our designed GSTRL framework effectively captures the
spatio-temporal dynamics of the overall system. We evaluate GSTRL on two
real-world datasets, where experiments show its superior performance. Our
source codes are available in the supplementary materials.

</details>


### [195] [Accumulative SGD Influence Estimation for Data Attribution](https://arxiv.org/abs/2510.26185)
*Yunxiao Shi,Shuo Yang,Yixin Su,Rui Zhang,Min Xu*

Main category: cs.LG

TL;DR: 提出了一种新的影响函数估计器 ACC-SGD-IE，它通过在训练过程中传播 leave-one-out 扰动并在每一步更新累积影响状态来实现更精确的估计。


<details>
  <summary>Details</summary>
Motivation: 现代以数据为中心的人工智能需要精确的per-sample影响，而标准 SGD-IE 忽略了跨 epoch 的复合效应，导致关键样本的错误排序。

Method: 提出了 ACC-SGD-IE，一种轨迹感知估计器，它在训练过程中传播 leave-one-out 扰动，并在每一步更新累积影响状态。

Result: 在 smooth 强凸设置下，实现了几何误差收缩；在 smooth 非凸 regime 下，收紧了误差界限；更大的 mini-batch 进一步减少了常数。在 Adult、20 Newsgroups 和 MNIST 数据集上，在干净和损坏的数据以及凸和非凸训练下，ACC-SGD-IE 产生了更精确的影响估计。

Conclusion: ACC-SGD-IE 可以更可靠地标记噪声样本，从而产生在 ACC-SGD-IE 清理的数据上训练的模型，其性能优于使用 SGD-IE 清理的数据训练的模型。

Abstract: Modern data-centric AI needs precise per-sample influence. Standard SGD-IE
approximates leave-one-out effects by summing per-epoch surrogates and ignores
cross-epoch compounding, which misranks critical examples. We propose
ACC-SGD-IE, a trajectory-aware estimator that propagates the leave-one-out
perturbation across training and updates an accumulative influence state at
each step. In smooth strongly convex settings it achieves geometric error
contraction and, in smooth non-convex regimes, it tightens error bounds; larger
mini-batches further reduce constants. Empirically, on Adult, 20 Newsgroups,
and MNIST under clean and corrupted data and both convex and non-convex
training, ACC-SGD-IE yields more accurate influence estimates, especially over
long epochs. For downstream data cleansing it more reliably flags noisy
samples, producing models trained on ACC-SGD-IE cleaned data that outperform
those cleaned with SGD-IE.

</details>


### [196] [Predicting All-Cause Hospital Readmissions from Medical Claims Data of Hospitalised Patients](https://arxiv.org/abs/2510.26188)
*Avinash Kadimisetty,Arun Rajagopalan,Vijendra SK*

Main category: cs.LG

TL;DR: 该论文利用机器学习技术预测医院再入院率，旨在降低医疗成本并提高医疗质量。


<details>
  <summary>Details</summary>
Motivation: 降低可预防的医院再入院率是医疗保健领域的重要目标，再入院率是衡量医院医疗质量的基准。

Method: 使用逻辑回归、随机森林和支持向量机等机器学习技术分析健康理赔数据，并使用主成分分析进行降维。

Result: 随机森林模型表现最佳，其次是逻辑回归和支持向量机模型。

Conclusion: 这些模型可用于识别导致再入院的关键因素，并帮助识别需要重点关注的患者，从而降低再入院的可能性，最终降低成本并提高为患者提供的医疗保健质量。

Abstract: Reducing preventable hospital readmissions is a national priority for payers,
providers, and policymakers seeking to improve health care and lower costs. The
rate of readmission is being used as a benchmark to determine the quality of
healthcare provided by the hospitals. In thisproject, we have used machine
learning techniques like Logistic Regression, Random Forest and Support Vector
Machines to analyze the health claims data and identify demographic and medical
factors that play a crucial role in predicting all-cause readmissions. As the
health claims data is high dimensional, we have used Principal Component
Analysis as a dimension reduction technique and used the results for building
regression models. We compared and evaluated these models based on the Area
Under Curve (AUC) metric. Random Forest model gave the highest performance
followed by Logistic Regression and Support Vector Machine models. These models
can be used to identify the crucial factors causing readmissions and help
identify patients to focus on to reduce the chances of readmission, ultimately
bringing down the cost and increasing the quality of healthcare provided to the
patients.

</details>


### [197] [Test-Time Alignment of LLMs via Sampling-Based Optimal Control in pre-logit space](https://arxiv.org/abs/2510.26219)
*Sekitoshi Kanai,Tsukasa Yoshida,Hiroshi Takahashi,Haru Kuroki,Kazumune Hashimoto*

Main category: cs.LG

TL;DR: 提出了一种新的测试时对齐方法，称为预逻辑自适应重要性抽样 (AISP)。


<details>
  <summary>Details</summary>
Motivation: 由于微调大型语言模型 (LLM) 需要很高的计算成本，因此LLM的测试时对齐受到了关注。

Method: AISP 将高斯扰动应用于倒数第二层的输出，即预逻辑，从而使关于扰动均值的预期奖励最大化。最佳均值是通过使用抽样奖励的重要性抽样获得的。

Result: AISP 在奖励方面优于 best-of-n 抽样，并且比其他基于奖励的测试时对齐方法实现了更高的奖励。

Conclusion: AISP 是一种有效的测试时对齐方法。

Abstract: Test-time alignment of large language models (LLMs) attracts attention
because fine-tuning LLMs requires high computational costs. In this paper, we
propose a new test-time alignment method called adaptive importance sampling on
pre-logits (AISP) on the basis of the sampling-based model predictive control
with the stochastic control input. AISP applies the Gaussian perturbation into
pre-logits, which are outputs of the penultimate layer, so as to maximize
expected rewards with respect to the mean of the perturbation. We demonstrate
that the optimal mean is obtained by importance sampling with sampled rewards.
AISP outperforms best-of-n sampling in terms of rewards over the number of used
samples and achieves higher rewards than other reward-based test-time alignment
methods.

</details>


### [198] [MPRU: Modular Projection-Redistribution Unlearning as Output Filter for Classification Pipelines](https://arxiv.org/abs/2510.26230)
*Minyi Peng,Darian Gunamardi,Ivan Tjuawinata,Kwok-Yan Lam*

Main category: cs.LG

TL;DR: 提出了一种新的机器学习方法，通过逆转最后的训练序列来实现知识移除，解决了现有方法需要完全访问原始数据集和模型的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习方法在实际应用中面临可扩展性问题，并且需要完全访问原始数据集和模型。

Method: 将分类训练视为一个序列过程，通过在模型的末尾添加一个投影-重新分配层来实现。

Result: 在图像和表格数据集上的实验结果表明，该方法在计算成本显著降低的情况下，能够产生与完全重新训练模型相似的输出。

Conclusion: 该解决方案具有适用性、可扩展性和系统兼容性，同时在更实际的环境中保持了输出的性能。

Abstract: As a new and promising approach, existing machine unlearning (MU) works
typically emphasize theoretical formulations or optimization objectives to
achieve knowledge removal. However, when deployed in real-world scenarios, such
solutions typically face scalability issues and have to address practical
requirements such as full access to original datasets and model. In contrast to
the existing approaches, we regard classification training as a sequential
process where classes are learned sequentially, which we call \emph{inductive
approach}. Unlearning can then be done by reversing the last training sequence.
This is implemented by appending a projection-redistribution layer in the end
of the model. Such an approach does not require full access to the original
dataset or the model, addressing the challenges of existing methods. This
enables modular and model-agnostic deployment as an output filter into existing
classification pipelines with minimal alterations. We conducted multiple
experiments across multiple datasets including image (CIFAR-10/100 using
CNN-based model) and tabular datasets (Covertype using tree-based model).
Experiment results show consistently similar output to a fully retrained model
with a high computational cost reduction. This demonstrates the applicability,
scalability, and system compatibility of our solution while maintaining the
performance of the output in a more practical setting.

</details>


### [199] [Angular Steering: Behavior Control via Rotation in Activation Space](https://arxiv.org/abs/2510.26243)
*Hieu M. Vu,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 提出了一个名为 Angular Steering 的新方法，通过在固定二维子空间内旋转激活来调节大型语言模型的行为，实现对拒绝和服从等行为的细粒度控制，同时保持模型稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有steering方法（如向量加法和定向消融）受限于由激活和特征方向定义的二维子空间，对参数敏感，并可能因激活空间中意外的相互作用而影响不相关的特征。

Method: 提出了 Angular Steering，通过将steering公式化为朝向或远离目标行为方向的几何旋转来实现行为调节。还提出了自适应 Angular Steering，这是一种选择性变体，仅旋转与目标特征对齐的激活。

Result: 实验表明，Angular Steering 在保持通用语言建模性能的同时，实现了稳健的行为控制，与先前的方法相比，具有灵活性、泛化性和鲁棒性。

Conclusion: Angular Steering 通过统一的几何旋转框架概括了现有的加法和正交化技术，简化了参数选择，并在更广泛的调整范围内保持了模型的稳定性。

Abstract: Controlling specific behaviors in large language models while preserving
their general capabilities is a central challenge for safe and reliable
artificial intelligence deployment. Current steering methods, such as vector
addition and directional ablation, are constrained within a two-dimensional
subspace defined by the activation and feature direction, making them sensitive
to chosen parameters and potentially affecting unrelated features due to
unintended interactions in activation space. We introduce Angular Steering, a
novel and flexible method for behavior modulation that operates by rotating
activations within a fixed two-dimensional subspace. By formulating steering as
a geometric rotation toward or away from a target behavior direction, Angular
Steering provides continuous, fine-grained control over behaviors such as
refusal and compliance. We demonstrate this method using refusal steering
emotion steering as use cases. Additionally, we propose Adaptive Angular
Steering, a selective variant that rotates only activations aligned with the
target feature, further enhancing stability and coherence. Angular Steering
generalizes existing addition and orthogonalization techniques under a unified
geometric rotation framework, simplifying parameter selection and maintaining
model stability across a broader range of adjustments. Experiments across
multiple model families and sizes show that Angular Steering achieves robust
behavioral control while maintaining general language modeling performance,
underscoring its flexibility, generalization, and robustness compared to prior
approaches. Code and artifacts are available at
https://github.com/lone17/angular-steering/.

</details>


### [200] [Likely Interpolants of Generative Models](https://arxiv.org/abs/2510.26266)
*Frederik Möbius Rygaard,Shen Zhu,Yinzhu Jin,Søren Hauberg,Tom Fletcher*

Main category: cs.LG

TL;DR: 提出了一种通用的生成模型插值方案，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有的生成模型缺乏一种主要的插值概念，并且对模型或数据维度有严格的假设。

Method: 开发了一种通用的插值方案，该方案针对与不同指标和概率分布兼容的可能转换路径。考虑类似于约束到合适数据分布的测地线的插值，并推导出一种用于计算这些曲线的新算法。

Result: 定量地表明，我们的插值方案比各种模型和数据集上的基线遍历更高密度区域。

Conclusion: 从理论上表明，该方法在局部可以被认为是合适的黎曼度量下的测地线。

Abstract: Interpolation in generative models allows for controlled generation, model
inspection, and more. Unfortunately, most generative models lack a principal
notion of interpolants without restrictive assumptions on either the model or
data dimension. In this paper, we develop a general interpolation scheme that
targets likely transition paths compatible with different metrics and
probability distributions. We consider interpolants analogous to a geodesic
constrained to a suitable data distribution and derive a novel algorithm for
computing these curves, which requires no additional training. Theoretically,
we show that our method locally can be considered as a geodesic under a
suitable Riemannian metric. We quantitatively show that our interpolation
scheme traverses higher density regions than baselines across a range of models
and datasets.

</details>


### [201] [Distributional Multi-objective Black-box Optimization for Diffusion-model Inference-time Multi-Target Generation](https://arxiv.org/abs/2510.26278)
*Kim Yong Tan,Yueming Lyu,Ivor Tsang,Yew-Soon Ong*

Main category: cs.LG

TL;DR: 提出了一种名为 Inference-time Multi-target Generation (IMG) 的算法，该算法在推理时优化扩散过程，以生成同时满足多个目标的样本。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常采用外部优化循环，例如进化算法，用于扩散模型，但忽略了扩散生成过程的内部分布转换，从而限制了它们的效率。

Method: 在扩散生成过程中，根据预期的聚合多目标值执行加权重采样。这种加权重采样策略确保扩散生成的样本根据我们所需的多目标玻尔兹曼分布进行分布。

Result: 在多目标分子生成任务上的实验表明，IMG 仅需一次生成即可实现比通常需要数百次扩散生成的基线优化算法更高的超体积。

Conclusion: IMG 可以被视为优化的扩散过程，并且可以集成到现有方法中以进一步提高其性能。

Abstract: Diffusion models have been successful in learning complex data distributions.
This capability has driven their application to high-dimensional
multi-objective black-box optimization problem. Existing approaches often
employ an external optimization loop, such as an evolutionary algorithm, to the
diffusion model. However, these approaches treat the diffusion model as a
black-box refiner, which overlooks the internal distribution transition of the
diffusion generation process, limiting their efficiency. To address these
challenges, we propose the Inference-time Multi-target Generation (IMG)
algorithm, which optimizes the diffusion process at inference-time to generate
samples that simultaneously satisfy multiple objectives. Specifically, our IMG
performs weighted resampling during the diffusion generation process according
to the expected aggregated multi-objective values. This weighted resampling
strategy ensures the diffusion-generated samples are distributed according to
our desired multi-target Boltzmann distribution. We further derive that the
multi-target Boltzmann distribution has an interesting log-likelihood
interpretation, where it is the optimal solution to the distributional
multi-objective optimization problem. We implemented IMG for a multi-objective
molecule generation task. Experiments show that IMG, requiring only a single
generation pass, achieves a significantly higher hypervolume than baseline
optimization algorithms that often require hundreds of diffusion generations.
Notably, our algorithm can be viewed as an optimized diffusion process and can
be integrated into existing methods to further improve their performance.

</details>
