{"id": "2510.23990", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.23990", "abs": "https://arxiv.org/abs/2510.23990", "authors": ["Maruf Ahmed Mridul", "Oshani Seneviratne"], "title": "Resource-Efficient LLM Application for Structured Transformation of Unstructured Financial Contracts", "comment": "5 pages, 1 figure, 2 tables", "summary": "The transformation of unstructured legal contracts into standardized,\nmachine-readable formats is essential for automating financial workflows. The\nCommon Domain Model (CDM) provides a standardized framework for this purpose,\nbut converting complex legal documents like Credit Support Annexes (CSAs) into\nCDM representations remains a significant challenge. In this paper, we present\nan extension of the CDMizer framework, a template-driven solution that ensures\nsyntactic correctness and adherence to the CDM schema during contract-to-CDM\nconversion. We apply this extended framework to a real-world task, comparing\nits performance with a benchmark developed by the International Swaps and\nDerivatives Association (ISDA) for CSA clause extraction. Our results show that\nCDMizer, when integrated with a significantly smaller, open-source Large\nLanguage Model (LLM), achieves competitive performance in terms of accuracy and\nefficiency against larger, proprietary models. This work underscores the\npotential of resource-efficient solutions to automate legal contract\ntransformation, offering a cost-effective and scalable approach that can meet\nthe needs of financial institutions with constrained resources or strict data\nprivacy requirements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd CDMizer \u6846\u67b6\u7684\u6269\u5c55\uff0c\u7528\u4e8e\u5c06\u975e\u7ed3\u6784\u5316\u6cd5\u5f8b\u5408\u540c\u8f6c\u6362\u4e3a\u673a\u5668\u53ef\u8bfb\u7684 CDM \u683c\u5f0f\uff0c\u4ee5\u5b9e\u73b0\u91d1\u878d\u5de5\u4f5c\u6d41\u7a0b\u7684\u81ea\u52a8\u5316\u3002", "motivation": "\u5c06\u975e\u7ed3\u6784\u5316\u6cd5\u5f8b\u5408\u540c\u8f6c\u6362\u4e3a\u6807\u51c6\u5316\u7684\u673a\u5668\u53ef\u8bfb\u683c\u5f0f\u5bf9\u4e8e\u81ea\u52a8\u5316\u91d1\u878d\u5de5\u4f5c\u6d41\u7a0b\u81f3\u5173\u91cd\u8981\u3002\u4fe1\u7528\u652f\u6301\u9644\u4ef6 (CSA) \u7b49\u590d\u6742\u6cd5\u5f8b\u6587\u4ef6\u8f6c\u6362\u4e3a CDM \u8868\u793a\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd CDMizer \u6846\u67b6\u7684\u6269\u5c55\uff0c\u8fd9\u662f\u4e00\u79cd\u6a21\u677f\u9a71\u52a8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u786e\u4fdd\u5408\u540c\u5230 CDM \u8f6c\u6362\u671f\u95f4\u7684\u53e5\u6cd5\u6b63\u786e\u6027\u548c\u7b26\u5408 CDM \u6a21\u5f0f\u3002", "result": "\u672c\u6587\u5c06\u6269\u5c55\u540e\u7684\u6846\u67b6\u5e94\u7528\u4e8e\u5b9e\u9645\u4efb\u52a1\uff0c\u5e76\u5c06\u5176\u6027\u80fd\u4e0e\u56fd\u9645\u6389\u671f\u548c\u884d\u751f\u54c1\u534f\u4f1a (ISDA) \u5f00\u53d1\u7684 CSA \u6761\u6b3e\u63d0\u53d6\u57fa\u51c6\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7ed3\u679c\u8868\u660e\uff0cCDMizer \u5728\u4e0e\u66f4\u5c0f\u7684\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u96c6\u6210\u65f6\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5b9e\u73b0\u4e86\u4e0e\u66f4\u5927\u7684\u4e13\u6709\u6a21\u578b\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u8d44\u6e90\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u5728\u81ea\u52a8\u5316\u6cd5\u5f8b\u5408\u540c\u8f6c\u6362\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6ee1\u8db3\u8d44\u6e90\u6709\u9650\u6216\u6570\u636e\u9690\u79c1\u8981\u6c42\u4e25\u683c\u7684\u91d1\u878d\u673a\u6784\u7684\u9700\u6c42\u3002"}}
{"id": "2510.24369", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.24369", "abs": "https://arxiv.org/abs/2510.24369", "authors": ["Yutian Xiao", "Meng Yuan", "Fuzhen Zhuang", "Wei Chen", "Shukuan Wang", "Shanqi Liu", "Chao Feng", "Wenhui Yu", "Xiang Li", "Lantao Hu", "Han Li", "Zhao Zhang"], "title": "DUET: Dual Model Co-Training for Entire Space CTR Prediction", "comment": null, "summary": "The pre-ranking stage plays a pivotal role in large-scale recommender systems\nbut faces an intrinsic trade-off between model expressiveness and computational\nefficiency. Owing to the massive candidate pool and strict latency constraints,\nindustry systems often rely on lightweight two-tower architectures, which are\ncomputationally efficient yet limited in estimation capability. As a result,\nthey struggle to capture the complex synergistic and suppressive relationships\namong candidate items, which are essential for producing contextually coherent\nand diverse recommendation lists. Moreover, this simplicity further amplifies\nthe Sample Selection Bias (SSB) problem, as coarse-grained models trained on\nbiased exposure data must generalize to a much larger candidate space with\ndistinct distributions.\n  To address these issues, we propose \\textbf{DUET} (\\textbf{DU}al Model\nCo-Training for \\textbf{E}ntire Space C\\textbf{T}R Prediction), a set-wise\npre-ranking framework that achieves expressive modeling under tight\ncomputational budgets. Instead of scoring items independently, DUET performs\nset-level prediction over the entire candidate subset in a single forward pass,\nenabling information-aware interactions among candidates while amortizing the\ncomputational cost across the set. Moreover, a dual model co-training mechanism\nextends supervision to unexposed items via mutual pseudo-label refinement,\neffectively mitigating SSB. Validated through extensive offline experiments and\nonline A/B testing, DUET consistently outperforms state-of-the-art baselines\nand achieves improvements across multiple core business metrics. At present,\nDUET has been fully deployed in Kuaishou and Kuaishou Lite Apps, serving the\nmain traffic for hundreds of millions of users.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDUET\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u9884\u6392\u5e8f\u9636\u6bb5\uff0c\u65e8\u5728\u89e3\u51b3\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u53cc\u5854\u67b6\u6784\u5728\u6355\u83b7\u5019\u9009\u9879\u76ee\u4e4b\u95f4\u590d\u6742\u7684\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5e76\u4e14\u5bb9\u6613\u53d7\u5230\u6837\u672c\u9009\u62e9\u504f\u5dee\uff08SSB\uff09\u95ee\u9898\u7684\u5f71\u54cd\u3002", "method": "DUET\u901a\u8fc7\u5728\u5355\u4e2a\u524d\u5411\u4f20\u9012\u4e2d\u5bf9\u6574\u4e2a\u5019\u9009\u5b50\u96c6\u6267\u884c\u96c6\u5408\u7ea7\u522b\u7684\u9884\u6d4b\uff0c\u4ece\u800c\u5b9e\u73b0\u5019\u9009\u9879\u76ee\u4e4b\u95f4\u7684\u4fe1\u606f\u611f\u77e5\u4ea4\u4e92\uff0c\u5e76\u901a\u8fc7\u53cc\u6a21\u578b\u534f\u540c\u8bad\u7ec3\u673a\u5236\u6269\u5c55\u5bf9\u672a\u66dd\u5149\u9879\u76ee\u7684\u76d1\u7763\uff0c\u4ece\u800c\u6709\u6548\u7f13\u89e3SSB\u3002", "result": "\u79bb\u7ebf\u5b9e\u9a8c\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u8868\u660e\uff0cDUET\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u6c34\u5e73\u7684\u57fa\u7ebf\uff0c\u5e76\u5728\u591a\u4e2a\u6838\u5fc3\u4e1a\u52a1\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u6539\u8fdb\u3002DUET\u5df2\u5728\u5feb\u624b\u548c\u5feb\u624b\u6781\u901f\u7248App\u4e2d\u5168\u9762\u90e8\u7f72\u3002", "conclusion": "DUET\u6846\u67b6\u80fd\u591f\u5728\u4e25\u683c\u7684\u8ba1\u7b97\u9884\u7b97\u4e0b\u5b9e\u73b0\u8868\u8fbe\u6027\u5efa\u6a21\uff0c\u7f13\u89e3\u6837\u672c\u9009\u62e9\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002"}}
{"id": "2510.24402", "categories": ["cs.IR", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.24402", "abs": "https://arxiv.org/abs/2510.24402", "authors": ["Michail Dadopoulos", "Anestis Ladas", "Stratos Moschidis", "Ioannis Negkakis"], "title": "Metadata-Driven Retrieval-Augmented Generation for Financial Question Answering", "comment": "Preprint version submitted to the International Journal of Accounting\n  Information Systems; currently under major revision. 20 pages, 1 figure, 1\n  table", "summary": "Retrieval-Augmented Generation (RAG) struggles on long, structured financial\nfilings where relevant evidence is sparse and cross-referenced. This paper\npresents a systematic investigation of advanced metadata-driven\nRetrieval-Augmented Generation (RAG) techniques, proposing and evaluating a\nnovel, multi-stage RAG architecture that leverages LLM-generated metadata. We\nintroduce a sophisticated indexing pipeline to create contextually rich\ndocument chunks and benchmark a spectrum of enhancements, including\npre-retrieval filtering, post-retrieval reranking, and enriched embeddings,\nbenchmarked on the FinanceBench dataset. Our results reveal that while a\npowerful reranker is essential for precision, the most significant performance\ngains come from embedding chunk metadata directly with text (\"contextual\nchunks\"). Our proposed optimal architecture combines LLM-driven pre-retrieval\noptimizations with these contextual embeddings to achieve superior performance.\nAdditionally, we present a custom metadata reranker that offers a compelling,\ncost-effective alternative to commercial solutions, highlighting a practical\ntrade-off between peak performance and operational efficiency. This study\nprovides a blueprint for building robust, metadata-aware RAG systems for\nfinancial document analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684RAG\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5229\u7528LLM\u751f\u6210\u7684\u5143\u6570\u636e\u6765\u5904\u7406\u957f\u7bc7\u3001\u7ed3\u6784\u5316\u7684\u8d22\u52a1\u6587\u4ef6\u3002", "motivation": "\u73b0\u6709RAG\u6280\u672f\u5728\u5904\u7406\u5305\u542b\u7a00\u758f\u548c\u4ea4\u53c9\u5f15\u7528\u7684\u957f\u7bc7\u7ed3\u6784\u5316\u8d22\u52a1\u6587\u4ef6\u65f6\u5b58\u5728\u56f0\u96be\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u9636\u6bb5RAG\u67b6\u6784\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e00\u7cfb\u5217\u589e\u5f3a\u6280\u672f\uff0c\u5305\u62ec\u9884\u68c0\u7d22\u8fc7\u6ee4\u3001\u540e\u68c0\u7d22\u91cd\u6392\u5e8f\u548c\u4e30\u5bcc\u7684\u5d4c\u5165\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5f3a\u5927\u7684\u91cd\u6392\u5e8f\u5668\u5bf9\u4e8e\u7cbe\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u800c\u6027\u80fd\u63d0\u5347\u4e3b\u8981\u6765\u81ea\u4e8e\u5c06\u5757\u5143\u6570\u636e\u76f4\u63a5\u5d4c\u5165\u6587\u672c\uff08\u201c\u4e0a\u4e0b\u6587\u5757\u201d\uff09\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u4f18\u5316\u67b6\u6784\u7ed3\u5408\u4e86LLM\u9a71\u52a8\u7684\u9884\u68c0\u7d22\u4f18\u5316\u548c\u4e0a\u4e0b\u6587\u5d4c\u5165\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24430", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.24430", "abs": "https://arxiv.org/abs/2510.24430", "authors": ["Yejin Kim", "Shaghayegh Agah", "Mayur Nankani", "Neeraj Sharma", "Feifei Peng", "Maria Peifer", "Sardar Hamidian", "H Howie Huang"], "title": "From Time and Place to Preference: LLM-Driven Geo-Temporal Context in Recommendations", "comment": null, "summary": "Most recommender systems treat timestamps as numeric or cyclical values,\noverlooking real-world context such as holidays, events, and seasonal patterns.\nWe propose a scalable framework that uses large language models (LLMs) to\ngenerate geo-temporal embeddings from only a timestamp and coarse location,\ncapturing holidays, seasonal trends, and local/global events. We then introduce\na geo-temporal embedding informativeness test as a lightweight diagnostic,\ndemonstrating on MovieLens, LastFM, and a production dataset that these\nembeddings provide predictive signal consistent with the outcomes of full model\nintegrations. Geo-temporal embeddings are incorporated into sequential models\nthrough (1) direct feature fusion with metadata embeddings or (2) an auxiliary\nloss that enforces semantic and geo-temporal alignment. Our findings highlight\nthe need for adaptive or hybrid recommendation strategies, and we release a\ncontext-enriched MovieLens dataset to support future research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4ec5\u4ece\u65f6\u95f4\u6233\u548c\u7c97\u7565\u4f4d\u7f6e\u751f\u6210\u5730\u7406\u65f6\u95f4\u5d4c\u5165\uff0c\u4ece\u800c\u6355\u83b7\u5047\u671f\u3001\u5b63\u8282\u6027\u8d8b\u52bf\u548c\u672c\u5730/\u5168\u7403\u4e8b\u4ef6\u3002", "motivation": "\u5927\u591a\u6570\u63a8\u8350\u7cfb\u7edf\u5c06\u65f6\u95f4\u6233\u89c6\u4e3a\u6570\u5b57\u6216\u5faa\u73af\u503c\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u4e16\u754c\u7684\u80cc\u666f\uff0c\u4f8b\u5982\u5047\u671f\u3001\u4e8b\u4ef6\u548c\u5b63\u8282\u6027\u6a21\u5f0f\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u751f\u6210\u5730\u7406\u65f6\u95f4\u5d4c\u5165\uff0c\u7136\u540e\u5f15\u5165\u5730\u7406\u65f6\u95f4\u5d4c\u5165\u4fe1\u606f\u91cf\u6d4b\u8bd5\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u8bca\u65ad\u3002", "result": "\u5728 MovieLens\u3001LastFM \u548c\u751f\u4ea7\u6570\u636e\u96c6\u4e0a\u8bc1\u660e\uff0c\u8fd9\u4e9b\u5d4c\u5165\u63d0\u4f9b\u4e86\u4e0e\u5b8c\u6574\u6a21\u578b\u96c6\u6210\u7ed3\u679c\u4e00\u81f4\u7684\u9884\u6d4b\u4fe1\u53f7\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u5bf9\u81ea\u9002\u5e94\u6216\u6df7\u5408\u63a8\u8350\u7b56\u7565\u7684\u9700\u6c42\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684 MovieLens \u6570\u636e\u96c6\u4ee5\u652f\u6301\u672a\u6765\u7684\u7814\u7a76\u3002"}}
{"id": "2510.24307", "categories": ["cs.DB", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.24307", "abs": "https://arxiv.org/abs/2510.24307", "authors": ["Shyam Jesalpura", "Shengda Zhu", "Amir Shaikhha", "Antonio Barbalace", "Boris Grot"], "title": "Odyssey: An End-to-End System for Pareto-Optimal Serverless Query Processing", "comment": null, "summary": "Running data analytics queries on serverless (FaaS) workers has been shown to\nbe cost- and performance-efficient for a variety of real-world scenarios,\nincluding intermittent query arrival patterns, sudden load spikes and\nmanagement challenges that afflict managed VM clusters. Alas, existing\nserverless data analytics works focus primarily on the serverless execution\nengine and assume the existence of a \"good\" query execution plan or rely on\nuser guidance to construct such a plan. Meanwhile, even simple analytics\nqueries on serverless have a huge space of possible plans, with vast\ndifferences in both performance and cost among plans.\n  This paper introduces Odyssey, an end-to-end serverless-native data analytics\npipeline that integrates a query planner, cost model and execution engine.\nOdyssey automatically generates and evaluates serverless query plans, utilizing\nstate space pruning heuristics and a novel search algorithm to identify\nPareto-optimal plans that balance cost and performance with low latency even\nfor complex queries. Our evaluations demonstrate that Odyssey accurately\npredicts both monetary cost and latency, and consistently outperforms AWS\nAthena on cost and/or latency.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a Odyssey \u7684\u7aef\u5230\u7aef serverless \u6570\u636e\u5206\u6790\u6d41\u6c34\u7ebf\uff0c\u5b83\u96c6\u6210\u4e86\u67e5\u8be2\u89c4\u5212\u5668\u3001\u6210\u672c\u6a21\u578b\u548c\u6267\u884c\u5f15\u64ce\u3002", "motivation": "\u73b0\u6709serverless\u6570\u636e\u5206\u6790\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8serverless\u6267\u884c\u5f15\u64ce\uff0c\u5e76\u5047\u8bbe\u5b58\u5728\u201c\u597d\u7684\u201d\u67e5\u8be2\u6267\u884c\u8ba1\u5212\uff0c\u6216\u8005\u4f9d\u8d56\u7528\u6237\u6307\u5bfc\u6765\u6784\u5efa\u8fd9\u6837\u7684\u8ba1\u5212\u3002\u7136\u800c\uff0c\u5373\u4f7f\u5728serverless\u4e0a\u8fdb\u884c\u7b80\u5355\u7684\u5206\u6790\u67e5\u8be2\uff0c\u4e5f\u5b58\u5728\u5de8\u5927\u7684\u53ef\u80fd\u8ba1\u5212\u7a7a\u95f4\uff0c\u4e0d\u540c\u8ba1\u5212\u5728\u6027\u80fd\u548c\u6210\u672c\u4e0a\u5b58\u5728\u5de8\u5927\u5dee\u5f02\u3002", "method": "Odyssey \u81ea\u52a8\u751f\u6210\u548c\u8bc4\u4f30 serverless \u67e5\u8be2\u8ba1\u5212\uff0c\u5229\u7528\u72b6\u6001\u7a7a\u95f4\u4fee\u526a\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u4e00\u79cd\u65b0\u7684\u641c\u7d22\u7b97\u6cd5\u6765\u8bc6\u522b Pareto \u6700\u4f18\u8ba1\u5212\uff0c\u5373\u4f7f\u5bf9\u4e8e\u590d\u6742\u7684\u67e5\u8be2\uff0c\u4e5f\u80fd\u4ee5\u4f4e\u5ef6\u8fdf\u5e73\u8861\u6210\u672c\u548c\u6027\u80fd\u3002", "result": "\u8bc4\u4f30\u8868\u660e\uff0cOdyssey \u80fd\u591f\u51c6\u786e\u9884\u6d4b\u8d27\u5e01\u6210\u672c\u548c\u5ef6\u8fdf\uff0c\u5e76\u4e14\u5728\u6210\u672c\u548c/\u6216\u5ef6\u8fdf\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e AWS Athena\u3002", "conclusion": "Odyssey \u5728 serverless \u6570\u636e\u5206\u6790\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u548c\u8bc4\u4f30\u67e5\u8be2\u8ba1\u5212\uff0c\u5e76\u5728\u6210\u672c\u548c\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002"}}
{"id": "2510.23730", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23730", "abs": "https://arxiv.org/abs/2510.23730", "authors": ["Alessandra Terranova", "Bj\u00f6rn Ross", "Alexandra Birch"], "title": "Evaluating Long-Term Memory for Long-Context Question Answering", "comment": "14 pages including appendix, 3 figures. Submitted to October ARR and\n  to Metacognition in Generative AI EurIPS workshop (under review for both)", "summary": "In order for large language models to achieve true conversational continuity\nand benefit from experiential learning, they need memory. While research has\nfocused on the development of complex memory systems, it remains unclear which\ntypes of memory are most effective for long-context conversational tasks. We\npresent a systematic evaluation of memory-augmented methods using LoCoMo, a\nbenchmark of synthetic long-context dialogues annotated for question-answering\ntasks that require diverse reasoning strategies. We analyse full-context\nprompting, semantic memory through retrieval-augmented generation and agentic\nmemory, episodic memory through in-context learning, and procedural memory\nthrough prompt optimization. Our findings show that memory-augmented approaches\nreduce token usage by over 90% while maintaining competitive accuracy. Memory\narchitecture complexity should scale with model capability, with small\nfoundation models benefitting most from RAG, and strong instruction-tuned\nreasoning model gaining from episodic learning through reflections and more\ncomplex agentic semantic memory. In particular, episodic memory can help LLMs\nrecognise the limits of their own knowledge.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9700\u8981\u8bb0\u5fc6\u6765\u5b9e\u73b0\u8fde\u7eed\u5bf9\u8bdd\u548c\u7ecf\u9a8c\u5b66\u4e60\u3002\u672c\u6587\u5bf9\u4e0d\u540c\u7684\u8bb0\u5fc6\u589e\u5f3a\u65b9\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\uff0c\u4ee5\u786e\u5b9a\u54ea\u79cd\u8bb0\u5fc6\u7c7b\u578b\u6700\u9002\u5408\u957f\u7a0b\u5bf9\u8bdd\u4efb\u52a1\u3002", "motivation": "\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u590d\u6742\u8bb0\u5fc6\u7cfb\u7edf\u7684\u5f00\u53d1\u4e0a\uff0c\u4f46\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u54ea\u79cd\u8bb0\u5fc6\u7c7b\u578b\u5bf9\u4e8e\u957f\u7a0b\u5bf9\u8bdd\u4efb\u52a1\u6700\u6709\u6548\u3002", "method": "\u4f7f\u7528LoCoMo\uff08\u4e00\u4e2a\u5408\u6210\u7684\u957f\u7a0b\u5bf9\u8bdd\u57fa\u51c6\uff09\u5bf9\u8bb0\u5fc6\u589e\u5f3a\u65b9\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\uff0c\u8be5\u57fa\u51c6\u9488\u5bf9\u9700\u8981\u4e0d\u540c\u63a8\u7406\u7b56\u7565\u7684\u95ee\u7b54\u4efb\u52a1\u8fdb\u884c\u4e86\u6ce8\u91ca\u3002\u5206\u6790\u4e86\u5168\u4e0a\u4e0b\u6587\u63d0\u793a\u3001\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5b9e\u73b0\u7684\u8bed\u4e49\u8bb0\u5fc6\u3001\u4ee3\u7406\u8bb0\u5fc6\u3001\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u5b9e\u73b0\u7684\u4e8b\u4ef6\u8bb0\u5fc6\u4ee5\u53ca\u901a\u8fc7\u63d0\u793a\u4f18\u5316\u5b9e\u73b0\u7684\u8fc7\u7a0b\u8bb0\u5fc6\u3002", "result": "\u8bb0\u5fc6\u589e\u5f3a\u65b9\u6cd5\u5728\u4fdd\u6301\u7ade\u4e89\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u51cf\u5c11\u4e86\u8d85\u8fc790%\u7684token\u4f7f\u7528\u91cf\u3002", "conclusion": "\u8bb0\u5fc6\u67b6\u6784\u7684\u590d\u6742\u6027\u5e94\u968f\u6a21\u578b\u80fd\u529b\u800c\u6269\u5c55\uff0c\u5c0f\u578b\u57fa\u7840\u6a21\u578b\u4eceRAG\u4e2d\u83b7\u76ca\u6700\u591a\uff0c\u800c\u5f3a\u5927\u7684\u6307\u4ee4\u8c03\u6574\u63a8\u7406\u6a21\u578b\u5219\u4ece\u901a\u8fc7\u53cd\u601d\u548c\u66f4\u590d\u6742\u7684\u4ee3\u7406\u8bed\u4e49\u8bb0\u5fc6\u5b9e\u73b0\u7684\u4e8b\u4ef6\u5b66\u4e60\u4e2d\u83b7\u76ca\u3002\u7279\u522b\u662f\uff0c\u60c5\u666f\u8bb0\u5fc6\u53ef\u4ee5\u5e2e\u52a9LLM\u8bc6\u522b\u81ea\u8eab\u77e5\u8bc6\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.23691", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23691", "abs": "https://arxiv.org/abs/2510.23691", "authors": ["Zihao Wang", "Xujing Li", "Yining Ye", "Junjie Fang", "Haoming Wang", "Longxiang Liu", "Shihao Liang", "Junting Lu", "Zhiyong Wu", "Jiazhan Feng", "Wanjun Zhong", "Zili Li", "Yu Wang", "Yu Miao", "Bo Zhou", "Yuanfan Li", "Hao Wang", "Zhongkai Zhao", "Faming Wu", "Zhengxuan Jiang", "Weihao Tan", "Heyuan Yao", "Shi Yan", "Xiangyang Li", "Yitao Liang", "Yujia Qin", "Guang Shi"], "title": "Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents", "comment": null, "summary": "We present Game-TARS, a generalist game agent trained with a unified,\nscalable action space anchored to human-aligned native keyboard-mouse inputs.\nUnlike API- or GUI-based approaches, this paradigm enables large-scale\ncontinual pre-training across heterogeneous domains, including OS, web, and\nsimulation games. Game-TARS is pre-trained on over 500B tokens with diverse\ntrajectories and multimodal data. Key techniques include a decaying continual\nloss to reduce causal confusion and an efficient Sparse-Thinking strategy that\nbalances reasoning depth and inference cost. Experiments show that Game-TARS\nachieves about 2 times the success rate over the previous sota model on\nopen-world Minecraft tasks, is close to the generality of fresh humans in\nunseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet\nin FPS benchmarks. Scaling results on training-time and test-time confirm that\nthe unified action space sustains improvements when scaled to cross-game and\nmultimodal data. Our results demonstrate that simple, scalable action\nrepresentations combined with large-scale pre-training provide a promising path\ntoward generalist agents with broad computer-use abilities.", "AI": {"tldr": "Game-TARS \u662f\u4e00\u4e2a\u901a\u7528\u6e38\u620f\u667a\u80fd\u4f53\uff0c\u5b83\u4f7f\u7528\u7edf\u4e00\u7684\u3001\u53ef\u6269\u5c55\u7684\u52a8\u4f5c\u7a7a\u95f4\u8fdb\u884c\u8bad\u7ec3\uff0c\u8be5\u52a8\u4f5c\u7a7a\u95f4\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684\u539f\u59cb\u952e\u76d8\u9f20\u6807\u8f93\u5165\u76f8\u5173\u8054\u3002", "motivation": "\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u5728\u4e0d\u540c\u9886\u57df\uff08\u5305\u62ec\u64cd\u4f5c\u7cfb\u7edf\u3001\u7f51\u7edc\u548c\u6a21\u62df\u6e38\u620f\uff09\u4e2d\u8fdb\u884c\u5927\u89c4\u6a21\u6301\u7eed\u9884\u8bad\u7ec3\u7684\u901a\u7528\u6e38\u620f\u667a\u80fd\u4f53\u3002", "method": "\u4f7f\u7528\u8d85\u8fc7 500B tokens \u7684\u591a\u6837\u5316\u8f68\u8ff9\u548c\u591a\u6a21\u6001\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u91c7\u7528\u8870\u51cf\u7684\u6301\u7eed\u635f\u5931\u6765\u51cf\u5c11\u56e0\u679c\u6df7\u6dc6\uff0c\u5e76\u91c7\u7528\u6709\u6548\u7684\u7a00\u758f\u601d\u8003\u7b56\u7565\u6765\u5e73\u8861\u63a8\u7406\u6df1\u5ea6\u548c\u63a8\u7406\u6210\u672c\u3002", "result": "\u5728\u5f00\u653e\u4e16\u754c Minecraft \u4efb\u52a1\u4e2d\u7684\u6210\u529f\u7387\u662f\u4e4b\u524d\u6700\u4f73\u6a21\u578b\u7684 2 \u500d\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u7f51\u7edc 3D \u6e38\u620f\u4e2d\u63a5\u8fd1\u4eba\u7c7b\u7684\u901a\u7528\u6027\uff0c\u5e76\u4e14\u5728 FPS \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e GPT-5\u3001Gemini-2.5-Pro \u548c Claude-4-Sonnet\u3002", "conclusion": "\u7b80\u5355\u3001\u53ef\u6269\u5c55\u7684\u52a8\u4f5c\u8868\u793a\u4e0e\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u76f8\u7ed3\u5408\uff0c\u4e3a\u5177\u6709\u5e7f\u6cdb\u8ba1\u7b97\u673a\u4f7f\u7528\u80fd\u529b\u7684\u901a\u7528\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u5e0c\u671b\u7684\u9053\u8def\u3002"}}
{"id": "2510.23775", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.23775", "abs": "https://arxiv.org/abs/2510.23775", "authors": ["Aryan Mathur", "Asaduddin Ahmed", "Pushti Amit Vasoya", "Simeon Kandan Sonar", "Yasir Z", "Madesh Kuppusamy"], "title": "Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices", "comment": null, "summary": "The increasing realism of AI-generated imagery poses challenges for verifying\nvisual authenticity. We present an explainable image authenticity detection\nsystem that combines a lightweight convolutional classifier\n(\"Faster-Than-Lies\") with a Vision-Language Model (Qwen2-VL-7B) to classify,\nlocalize, and explain artifacts in 32x32 images. Our model achieves 96.5%\naccuracy on the extended CiFAKE dataset augmented with adversarial\nperturbations and maintains an inference time of 175ms on 8-core CPUs, enabling\ndeployment on local or edge devices. Using autoencoder-based reconstruction\nerror maps, we generate artifact localization heatmaps, which enhance\ninterpretability for both humans and the VLM. We further categorize 70 visual\nartifact types into eight semantic groups and demonstrate explainable text\ngeneration for each detected anomaly. This work highlights the feasibility of\ncombining visual and linguistic reasoning for interpretable authenticity\ndetection in low-resolution imagery and outlines potential cross-domain\napplications in forensics, industrial inspection, and social media moderation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u56fe\u50cf\u771f\u4f2a\u68c0\u6d4b\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86\u8f7b\u91cf\u7ea7\u5377\u79ef\u5206\u7c7b\u5668\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u53ef\u4ee5\u5bf932x32\u56fe\u50cf\u4e2d\u7684\u4f2a\u5f71\u8fdb\u884c\u5206\u7c7b\u3001\u5b9a\u4f4d\u548c\u89e3\u91ca\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u751f\u6210\u56fe\u50cf\u7684\u65e5\u76ca\u903c\u771f\u5bf9\u9a8c\u8bc1\u89c6\u89c9\u771f\u5b9e\u6027\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u7ed3\u5408\u8f7b\u91cf\u7ea7\u5377\u79ef\u5206\u7c7b\u5668\uff08\u201cFaster-Than-Lies\u201d\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (Qwen2-VL-7B) \u3002\u4f7f\u7528\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u7684\u91cd\u5efa\u8bef\u5dee\u56fe\u751f\u6210\u4f2a\u5f71\u5b9a\u4f4d\u70ed\u56fe\u3002", "result": "\u5728\u6269\u5c55\u7684 CiFAKE \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86 96.5% \u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728 8 \u6838 CPU \u4e0a\u4fdd\u6301 175 \u6beb\u79d2\u7684\u63a8\u7406\u65f6\u95f4\u3002\u5c06 70 \u79cd\u89c6\u89c9\u4f2a\u5f71\u7c7b\u578b\u5206\u4e3a\u516b\u4e2a\u8bed\u4e49\u7ec4\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u68c0\u6d4b\u5230\u7684\u5f02\u5e38\u751f\u6210\u53ef\u89e3\u91ca\u7684\u6587\u672c\u3002", "conclusion": "\u8bc1\u660e\u4e86\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u63a8\u7406\u5728\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u8fdb\u884c\u53ef\u89e3\u91ca\u7684\u771f\u5b9e\u6027\u68c0\u6d4b\u7684\u53ef\u884c\u6027\uff0c\u5e76\u6982\u8ff0\u4e86\u6cd5\u533b\u5b66\u3001\u5de5\u4e1a\u68c0\u6d4b\u548c\u793e\u4ea4\u5a92\u4f53\u5ba1\u6838\u4e2d\u6f5c\u5728\u7684\u8de8\u9886\u57df\u5e94\u7528\u3002"}}
{"id": "2510.23617", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23617", "abs": "https://arxiv.org/abs/2510.23617", "authors": ["Phuong Q. Dao", "Mark Roantree", "Vuong M. Ngo"], "title": "An Enhanced Dual Transformer Contrastive Network for Multimodal Sentiment Analysis", "comment": "The paper has been accepted for presentation at the MEDES 2025\n  conference", "summary": "Multimodal Sentiment Analysis (MSA) seeks to understand human emotions by\njointly analyzing data from multiple modalities typically text and images\noffering a richer and more accurate interpretation than unimodal approaches. In\nthis paper, we first propose BERT-ViT-EF, a novel model that combines powerful\nTransformer-based encoders BERT for textual input and ViT for visual input\nthrough an early fusion strategy. This approach facilitates deeper cross-modal\ninteractions and more effective joint representation learning. To further\nenhance the model's capability, we propose an extension called the Dual\nTransformer Contrastive Network (DTCN), which builds upon BERT-ViT-EF. DTCN\nincorporates an additional Transformer encoder layer after BERT to refine\ntextual context (before fusion) and employs contrastive learning to align text\nand image representations, fostering robust multimodal feature learning.\nEmpirical results on two widely used MSA benchmarks MVSA-Single and TumEmo\ndemonstrate the effectiveness of our approach. DTCN achieves best accuracy\n(78.4%) and F1-score (78.3%) on TumEmo, and delivers competitive performance on\nMVSA-Single, with 76.6% accuracy and 75.9% F1-score. These improvements\nhighlight the benefits of early fusion and deeper contextual modeling in\nTransformer-based multimodal sentiment analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u7684\u65b0\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u7ed3\u5408\u4e86 BERT \u548c ViT\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u6027\u80fd\u3002\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u662f\u6709\u6548\u7684\u3002", "motivation": "\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u65e8\u5728\u901a\u8fc7\u8054\u5408\u5206\u6790\u6765\u81ea\u591a\u4e2a\u6a21\u6001\u7684\u6570\u636e\u6765\u7406\u89e3\u4eba\u7c7b\u7684\u60c5\u611f\uff0c\u4e0e\u5355\u6a21\u6001\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b83\u53ef\u4ee5\u63d0\u4f9b\u66f4\u4e30\u5bcc\u548c\u66f4\u51c6\u786e\u7684\u89e3\u91ca\u3002", "method": "\u672c\u6587\u9996\u5148\u63d0\u51fa\u4e86 BERT-ViT-EF\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u578b\uff0c\u5b83\u901a\u8fc7\u65e9\u671f\u878d\u5408\u7b56\u7565\u7ed3\u5408\u4e86\u5f3a\u5927\u7684\u57fa\u4e8e Transformer \u7684\u7f16\u7801\u5668 BERT\uff08\u7528\u4e8e\u6587\u672c\u8f93\u5165\uff09\u548c ViT\uff08\u7528\u4e8e\u89c6\u89c9\u8f93\u5165\uff09\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u589e\u5f3a\u6a21\u578b\u7684\u80fd\u529b\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u53cc\u91cd Transformer \u5bf9\u6bd4\u7f51\u7edc (DTCN) \u7684\u6269\u5c55\uff0c\u5b83\u5efa\u7acb\u5728 BERT-ViT-EF \u7684\u57fa\u7840\u4e0a\u3002DTCN \u5728 BERT \u4e4b\u540e\u52a0\u5165\u4e86\u4e00\u4e2a\u989d\u5916\u7684 Transformer \u7f16\u7801\u5668\u5c42\uff0c\u4ee5\u6539\u8fdb\u6587\u672c\u4e0a\u4e0b\u6587\uff08\u5728\u878d\u5408\u4e4b\u524d\uff09\uff0c\u5e76\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u6765\u5bf9\u9f50\u6587\u672c\u548c\u56fe\u50cf\u8868\u793a\uff0c\u4ece\u800c\u4fc3\u8fdb\u7a33\u5065\u7684\u591a\u6a21\u6001\u7279\u5f81\u5b66\u4e60\u3002", "result": "\u5728\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684 MSA \u57fa\u51c6 MVSA-Single \u548c TumEmo \u4e0a\u7684\u7ecf\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u672c\u6587\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002DTCN \u5728 TumEmo \u4e0a\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u51c6\u786e\u7387 (78.4%) \u548c F1 \u5206\u6570 (78.3%)\uff0c\u5e76\u5728 MVSA-Single \u4e0a\u63d0\u4f9b\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u51c6\u786e\u7387\u4e3a 76.6%\uff0cF1 \u5206\u6570\u4e3a 75.9%\u3002", "conclusion": "\u8fd9\u4e9b\u6539\u8fdb\u7a81\u51fa\u4e86\u57fa\u4e8e Transformer \u7684\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u65e9\u671f\u878d\u5408\u548c\u66f4\u6df1\u5c42\u6b21\u7684\u4e0a\u4e0b\u6587\u5efa\u6a21\u7684\u4f18\u52bf\u3002"}}
{"id": "2510.24431", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24431", "abs": "https://arxiv.org/abs/2510.24431", "authors": ["Xiaoyu Kong", "Leheng Sheng", "Junfei Tan", "Yuxin Chen", "Jiancan Wu", "An Zhang", "Xiang Wang", "Xiangnan He"], "title": "MiniOneRec: An Open-Source Framework for Scaling Generative Recommendation", "comment": "Technical Report", "summary": "The recent success of large language models (LLMs) has renewed interest in\nwhether recommender systems can achieve similar scaling benefits. Conventional\nrecommenders, dominated by massive embedding tables, tend to plateau as\nembedding dimensions grow. In contrast, the emerging generative paradigm\nreplaces embeddings with compact Semantic ID (SID) sequences produced by\nautoregressive Transformers. Yet most industrial deployments remain\nproprietary, leaving two fundamental questions open: (1) Do the expected\nscaling laws hold on public benchmarks? (2) What is the minimal post-training\nrecipe that enables competitive performance?\n  We present MiniOneRec, to the best of our knowledge, the first fully\nopen-source generative recommendation framework, which provides an end-to-end\nworkflow spanning SID construction, supervised fine-tuning, and\nrecommendation-oriented reinforcement learning. We generate SIDs via a Residual\nQuantized VAE and post-train Qwen backbones ranging from 0.5B to 7B parameters\non the Amazon Review dataset. Our experiments reveal a consistent downward\ntrend in both training and evaluation losses with increasing model size,\nvalidating the parameter efficiency of the generative approach. To further\nenhance performance, we propose a lightweight yet effective post-training\npipeline that (1) enforces full-process SID alignment and (2) applies\nreinforcement learning with constrained decoding and hybrid rewards. Together,\nthese techniques yield significant improvements in both ranking accuracy and\ncandidate diversity.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86 MiniOneRec\uff0c\u8fd9\u662f\u4e00\u4e2a\u5b8c\u5168\u5f00\u6e90\u7684\u751f\u6210\u5f0f\u63a8\u8350\u6846\u67b6\uff0c\u7528\u4e8e\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6269\u5c55\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u63a8\u8350\u7cfb\u7edf\u53d7\u9650\u4e8e\u5d4c\u5165\u7ef4\u5ea6\u6269\u5c55\uff0c\u800c\u65b0\u5174\u7684\u751f\u6210\u5f0f\u65b9\u6cd5\u4f7f\u7528\u7d27\u51d1\u7684\u8bed\u4e49ID\u5e8f\u5217\u3002\u4f46\u5de5\u4e1a\u90e8\u7f72\u901a\u5e38\u662f\u79c1\u6709\u7684\uff0c\u6269\u5c55\u89c4\u5f8b\u548c\u6700\u5c0f\u540e\u8bad\u7ec3\u65b9\u6848\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u901a\u8fc7\u6b8b\u5dee\u91cf\u5316VAE\u751f\u6210SIDs\uff0c\u5e76\u5728Amazon Review\u6570\u636e\u96c6\u4e0a\u5bf90.5B\u52307B\u53c2\u6570\u7684Qwen\u4e3b\u5e72\u7f51\u7edc\u8fdb\u884c\u540e\u8bad\u7ec3\u3002\u63d0\u51fa\u4e86\u8f7b\u91cf\u7ea7\u7684\u540e\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5305\u62ecSID\u5bf9\u9f50\u548c\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u968f\u7740\u6a21\u578b\u89c4\u6a21\u7684\u589e\u52a0\uff0c\u8bad\u7ec3\u548c\u8bc4\u4f30\u635f\u5931\u6301\u7eed\u4e0b\u964d\uff0c\u9a8c\u8bc1\u4e86\u751f\u6210\u65b9\u6cd5\u7684\u53c2\u6570\u6548\u7387\u3002\u6240\u63d0\u51fa\u7684\u540e\u8bad\u7ec3\u6280\u672f\u663e\u8457\u63d0\u9ad8\u4e86\u6392\u5e8f\u51c6\u786e\u6027\u548c\u5019\u9009\u591a\u6837\u6027\u3002", "conclusion": "MiniOneRec \u9a8c\u8bc1\u4e86\u751f\u6210\u5f0f\u63a8\u8350\u65b9\u6cd5\u7684\u6269\u5c55\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u540e\u8bad\u7ec3\u6d41\u7a0b\uff0c\u4e3a\u5f00\u6e90\u793e\u533a\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u3002"}}
{"id": "2510.24599", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.24599", "abs": "https://arxiv.org/abs/2510.24599", "authors": ["Harsha Kokel", "Aamod Khatiwada", "Tejaswini Pedapati", "Haritha Ananthakrishnan", "Oktie Hassanzadeh", "Horst Samulowitz", "Kavitha Srinivas"], "title": "Evaluating Joinable Column Discovery Approaches for Context-Aware Search", "comment": "This is an Experiments and Analysis paper. The source code, data,\n  and/or other artifacts have been made available at\n  https://github.com/IBM/ContextAwareJoin", "summary": "Joinable Column Discovery is a critical challenge in automating enterprise\ndata analysis. While existing approaches focus on syntactic overlap and\nsemantic similarity, there remains limited understanding of which methods\nperform best for different data characteristics and how multiple criteria\ninfluence discovery effectiveness. We present a comprehensive experimental\nevaluation of joinable column discovery methods across diverse scenarios. Our\nstudy compares syntactic and semantic techniques on seven benchmarks covering\nrelational databases and data lakes. We analyze six key criteria -- unique\nvalues, intersection size, join size, reverse join size, value semantics, and\nmetadata semantics -- and examine how combining them through ensemble ranking\naffects performance. Our analysis reveals differences in method behavior across\ndata contexts and highlights the benefits of integrating multiple criteria for\nrobust join discovery. We provide empirical evidence on when each criterion\nmatters, compare pre-trained embedding models for semantic joins, and offer\npractical guidelines for selecting suitable methods based on dataset\ncharacteristics. Our findings show that metadata and value semantics are\ncrucial for data lakes, size-based criteria play a stronger role in relational\ndatabases, and ensemble approaches consistently outperform single-criterion\nmethods.", "AI": {"tldr": "\u672c\u6587\u5bf9\u53ef\u8fde\u63a5\u5217\u53d1\u73b0\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u5e76\u63d0\u4f9b\u4e86\u9009\u62e9\u5408\u9002\u65b9\u6cd5\u7684\u5b9e\u8df5\u6307\u5357\u3002", "motivation": "\u4f01\u4e1a\u6570\u636e\u5206\u6790\u4e2d\uff0c\u53ef\u8fde\u63a5\u5217\u53d1\u73b0\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5bf9\u4e0d\u540c\u6570\u636e\u7279\u5f81\u7684\u6700\u4f73\u65b9\u6cd5\u4ee5\u53ca\u591a\u91cd\u6807\u51c6\u5982\u4f55\u5f71\u54cd\u53d1\u73b0\u6548\u679c\u7684\u7406\u89e3\u6709\u9650\u3002", "method": "\u672c\u6587\u6bd4\u8f83\u4e86\u5173\u7cfb\u6570\u636e\u5e93\u548c\u6570\u636e\u6e56\u4e0a\u4e03\u4e2a\u57fa\u51c6\u7684\u8bed\u6cd5\u548c\u8bed\u4e49\u6280\u672f\uff0c\u5e76\u5206\u6790\u4e86\u516d\u4e2a\u5173\u952e\u6807\u51c6\uff08\u552f\u4e00\u503c\u3001\u4ea4\u96c6\u5927\u5c0f\u3001\u8fde\u63a5\u5927\u5c0f\u3001\u53cd\u5411\u8fde\u63a5\u5927\u5c0f\u3001\u503c\u8bed\u4e49\u548c\u5143\u6570\u636e\u8bed\u4e49\uff09\uff0c\u5e76\u7814\u7a76\u4e86\u901a\u8fc7\u96c6\u6210\u6392\u5e8f\u7ec4\u5408\u5b83\u4eec\u5982\u4f55\u5f71\u54cd\u6027\u80fd\u3002", "result": "\u5143\u6570\u636e\u548c\u503c\u8bed\u4e49\u5bf9\u4e8e\u6570\u636e\u6e56\u81f3\u5173\u91cd\u8981\uff0c\u57fa\u4e8e\u5927\u5c0f\u7684\u6807\u51c6\u5728\u5173\u7cfb\u6570\u636e\u5e93\u4e2d\u8d77\u7740\u66f4\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u5e76\u4e14\u96c6\u6210\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u5355\u6807\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63ed\u793a\u4e86\u8de8\u6570\u636e\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\u884c\u4e3a\u5dee\u5f02\uff0c\u5e76\u5f3a\u8c03\u4e86\u6574\u5408\u591a\u4e2a\u6807\u51c6\u4ee5\u5b9e\u73b0\u53ef\u9760\u8fde\u63a5\u53d1\u73b0\u7684\u597d\u5904\uff0c\u4e3a\u6839\u636e\u6570\u636e\u96c6\u7279\u5f81\u9009\u62e9\u5408\u9002\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5357\u3002"}}
{"id": "2510.23766", "categories": ["cs.CL", "68T05", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.23766", "abs": "https://arxiv.org/abs/2510.23766", "authors": ["Ramshankar Bhuvaneswaran", "Handan Liu"], "title": "BitSkip: An Empirical Analysis of Quantization and Early Exit Composition", "comment": "Submitted to JMLR", "summary": "The pursuit of efficient Large Language Models (LLMs) has led to increasingly\ncomplex techniques like extreme quantization and dynamic routing. While\nindividual benefits of these methods are well-documented, their compositional\neffects remain poorly understood. This paper introduces BitSkip, a hybrid\narchitectural framework for systematically exploring these interactions.\nCounter-intuitively, our findings reveal that a simple 8-bit quantized model\nwithout Hadamard transform (BitSkip-V1) not only outperforms its more complex\n4-bit and Hadamard-enhanced counterparts but also competes the full-precision\nbaseline in quality (perplexity of 1.13 vs 1.19) . The introduction of Hadamard\ntransforms, even at 8-bit precision, catastrophically degraded performance by\nover 37,000%, tracing fundamental training instability. Our BitSkip-V1 recipe\ndemonstrates superior early-exit characteristics, with layer 18 providing\noptimal 32.5% speed gain for minimal 4% quality loss.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u91cf\u5316\u548c\u52a8\u6001\u8def\u7531\u7b49\u590d\u6742\u6280\u672f\u7684\u7ec4\u5408\u6548\u5e94\u3002", "motivation": "\u63a2\u7d22\u9ad8\u6548LLM\uff0c\u8fd9\u4e9b\u6280\u672f\u867d\u7136\u5355\u72ec\u6709\u6548\uff0c\u4f46\u7ec4\u5408\u6548\u679c\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51faBitSkip\u6846\u67b6\uff0c\u7cfb\u7edf\u63a2\u7d22\u8fd9\u4e9b\u4ea4\u4e92\u4f5c\u7528\u3002", "result": "\u4e00\u4e2a\u7b80\u5355\u76848\u4f4d\u91cf\u5316\u6a21\u578bBitSkip-V1\uff0c\u4f18\u4e8e\u66f4\u590d\u6742\u76844\u4f4d\u548cHadamard\u589e\u5f3a\u6a21\u578b\uff0c\u751a\u81f3\u5728\u8d28\u91cf\u4e0a\u4e0e\u5168\u7cbe\u5ea6\u57fa\u7ebf\u7ade\u4e89\u3002", "conclusion": "\u5f15\u5165Hadamard\u53d8\u6362\u5373\u4f7f\u57288\u4f4d\u7cbe\u5ea6\u4e0b\u4e5f\u4f1a\u5bfc\u81f4\u6027\u80fd\u707e\u96be\u6027\u4e0b\u964d\uff0cBitSkip-V1\u5728\u65e9\u671f\u9000\u51fa\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.23734", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23734", "abs": "https://arxiv.org/abs/2510.23734", "authors": ["Eamon Duede"], "title": "AI and the Decentering of Disciplinary Creativity", "comment": null, "summary": "This paper examines the role of artificial intelligence in scientific\nproblem-solving, with a focus on its implications for disciplinary creativity.\nDrawing on recent work in the philosophy of creativity, I distinguish between\ncreative approaches and creative products, and introduce the concept of\ndisciplinary creativity -the creative application of discipline-specific\nexpertise to a valued problem within that field. Through two cases in\nmathematics, I show that while computation can extend disciplinary creativity,\ncertain approaches involving AI can serve to displace it. This displacement has\nthe potential to alter (and, perhaps, diminish) the value of scientific\npursuit.", "AI": {"tldr": "\u63a2\u8ba8\u4eba\u5de5\u667a\u80fd\u5728\u79d1\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u89d2\u8272\u53ca\u5176\u5bf9\u5b66\u79d1\u521b\u9020\u529b\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u4eba\u5de5\u667a\u80fd\u5982\u4f55\u5f71\u54cd\u5b66\u79d1\u521b\u9020\u529b\uff0c\u533a\u5206\u521b\u9020\u6027\u65b9\u6cd5\u548c\u521b\u9020\u6027\u4ea7\u54c1\uff0c\u5e76\u5f15\u5165\u5b66\u79d1\u521b\u9020\u529b\u7684\u6982\u5ff5\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u6570\u5b66\u6848\u4f8b\u8fdb\u884c\u5206\u6790\uff0c\u5c55\u793a\u8ba1\u7b97\u5982\u4f55\u6269\u5c55\u5b66\u79d1\u521b\u9020\u529b\u3002", "result": "\u8ba1\u7b97\u53ef\u4ee5\u6269\u5c55\u5b66\u79d1\u521b\u9020\u529b\uff0c\u4f46\u67d0\u4e9b\u4eba\u5de5\u667a\u80fd\u65b9\u6cd5\u53ef\u80fd\u4f1a\u53d6\u4ee3\u5b83\u3002", "conclusion": "\u4eba\u5de5\u667a\u80fd\u5bf9\u5b66\u79d1\u521b\u9020\u529b\u7684\u53d6\u4ee3\u53ef\u80fd\u4f1a\u6539\u53d8\u751a\u81f3\u964d\u4f4e\u79d1\u5b66\u63a2\u7d22\u7684\u4ef7\u503c\u3002"}}
{"id": "2510.23785", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23785", "abs": "https://arxiv.org/abs/2510.23785", "authors": ["Md Tanvir Hossain", "Akif Islam", "Mohd Ruhul Ameen"], "title": "CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting", "comment": "6 pages, 2 tables, 6 figures. Submitted to IEEE 5th International\n  Conference on Electrical, Computer and Telecommunication Engineering (ICECTE\n  2025)", "summary": "Humans can effortlessly count diverse objects by perceiving visual repetition\nand structural relationships rather than relying on class identity. However,\nmost existing counting models fail to replicate this ability; they often\nmiscount when objects exhibit complex shapes, internal symmetry, or overlapping\ncomponents. In this work, we introduce CountFormer, a transformer-based\nframework that learns to recognize repetition and structural coherence for\nclass-agnostic object counting. Built upon the CounTR architecture, our model\nreplaces its visual encoder with the self-supervised foundation model DINOv2,\nwhich produces richer and spatially consistent feature representations. We\nfurther incorporate positional embedding fusion to preserve geometric\nrelationships before decoding these features into density maps through a\nlightweight convolutional decoder. Evaluated on the FSC-147 dataset, our model\nachieves performance comparable to current state-of-the-art methods while\ndemonstrating superior accuracy on structurally intricate or densely packed\nscenes. Our findings indicate that integrating foundation models such as DINOv2\nenables counting systems to approach human-like structural perception,\nadvancing toward a truly general and exemplar-free counting paradigm.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aCountFormer\u7684\u57fa\u4e8eTransformer\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u7c7b\u522b\u65e0\u5173\u7684\u5bf9\u8c61\u8ba1\u6570\uff0c\u901a\u8fc7\u5b66\u4e60\u8bc6\u522b\u91cd\u590d\u548c\u7ed3\u6784\u8fde\u8d2f\u6027\u6765\u5b9e\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u8ba1\u6570\u6a21\u578b\u5728\u5bf9\u8c61\u5177\u6709\u590d\u6742\u7684\u5f62\u72b6\u3001\u5185\u90e8\u5bf9\u79f0\u6027\u6216\u91cd\u53e0\u7684\u7ec4\u4ef6\u65f6\uff0c\u7ecf\u5e38\u4f1a\u51fa\u73b0\u9519\u8bef\u8ba1\u6570\uff0c\u65e0\u6cd5\u590d\u5236\u4eba\u7c7b\u7684\u8ba1\u6570\u80fd\u529b\u3002", "method": "\u8be5\u6a21\u578b\u5efa\u7acb\u5728CounTR\u67b6\u6784\u4e4b\u4e0a\uff0c\u7528\u81ea\u76d1\u7763\u57fa\u7840\u6a21\u578bDINOv2\u66ff\u6362\u4e86\u5176\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u5e76\u8fdb\u4e00\u6b65\u7ed3\u5408\u4e86\u4f4d\u7f6e\u5d4c\u5165\u878d\u5408\uff0c\u4ee5\u5728\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5377\u79ef\u89e3\u7801\u5668\u5c06\u8fd9\u4e9b\u7279\u5f81\u89e3\u7801\u4e3a\u5bc6\u5ea6\u56fe\u4e4b\u524d\uff0c\u4fdd\u7559\u51e0\u4f55\u5173\u7cfb\u3002", "result": "\u5728FSC-147\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u8be5\u6a21\u578b\u53d6\u5f97\u4e86\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u7ed3\u6784\u590d\u6742\u6216\u5bc6\u96c6\u5806\u79ef\u7684\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u96c6\u6210\u8bf8\u5982DINOv2\u4e4b\u7c7b\u7684\u57fa\u7840\u6a21\u578b\u4f7f\u8ba1\u6570\u7cfb\u7edf\u80fd\u591f\u63a5\u8fd1\u7c7b\u4eba\u7684\u7ed3\u6784\u611f\u77e5\uff0c\u4ece\u800c\u671d\u7740\u771f\u6b63\u901a\u7528\u548c\u65e0\u793a\u4f8b\u7684\u8ba1\u6570\u8303\u4f8b\u8fc8\u8fdb\u3002"}}
{"id": "2510.23621", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23621", "abs": "https://arxiv.org/abs/2510.23621", "authors": ["Alexandre Benoit"], "title": "Speeding Up MACE: Low-Precision Tricks for Equivarient Force Fields", "comment": "78 pages, 21 figures", "summary": "Machine-learning force fields can deliver accurate molecular dynamics (MD) at\nhigh computational cost. For SO(3)-equivariant models such as MACE, there is\nlittle systematic evidence on whether reduced-precision arithmetic and\nGPU-optimized kernels can cut this cost without harming physical fidelity. This\nthesis aims to make MACE cheaper and faster while preserving accuracy by\nidentifying computational bottlenecks and evaluating low-precision execution\npolicies. We profile MACE end-to-end and per block, compare the e3nn and NVIDIA\ncuEquivariance backends, and assess FP64/FP32/BF16/FP16 settings (with FP32\naccumulation) for inference, short NVT and long NPT water simulations, and toy\ntraining runs under reproducible, steady-state timing. cuEquivariance reduces\ninference latency by about $3\\times$. Casting only linear layers to BF16/FP16\nwithin an FP32 model yields roughly 4x additional speedups, while energies and\nthermodynamic observables in NVT/NPT MD remain within run-to-run variability.\nHalf-precision weights during training degrade force RMSE. Mixing e3nn and cuEq\nmodules without explicit adapters causes representation mismatches. Fused\nequivariant kernels and mixed-precision inference can substantially accelerate\nstate-of-the-art force fields with negligible impact on downstream MD. A\npractical policy is to use cuEquivariance with FP32 by default and enable\nBF16/FP16 for linear layers (keeping FP32 accumulations) for maximum\nthroughput, while training remains in FP32. Further gains are expected on\nAmpere/Hopper GPUs (TF32/BF16) and from kernel-level FP16/BF16 paths and\npipeline fusion.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8bc6\u522b\u8ba1\u7b97\u74f6\u9888\u548c\u8bc4\u4f30\u4f4e\u7cbe\u5ea6\u6267\u884c\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u964d\u4f4e MACE \u7684\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u9ad8\u5176\u901f\u5ea6\u3002\u901a\u8fc7\u5206\u6790 MACE \u7684\u7aef\u5230\u7aef\u548c\u6bcf\u4e2a\u6a21\u5757\uff0c\u6bd4\u8f83 e3nn \u548c NVIDIA cuEquivariance \u540e\u7aef\uff0c\u5e76\u8bc4\u4f30 FP64/FP32/BF16/FP16 \u8bbe\u7f6e\uff0c\u7528\u4e8e\u63a8\u7406\u3001NVT \u548c NPT \u6c34\u6a21\u62df\u4ee5\u53ca\u73a9\u5177\u8bad\u7ec3\u3002", "motivation": "SO(3)-\u7b49\u53d8\u6a21\u578b\uff08\u5982 MACE\uff09\u867d\u7136\u80fd\u63d0\u4f9b\u51c6\u786e\u7684\u5206\u5b50\u52a8\u529b\u5b66\uff08MD\uff09\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u8bc1\u636e\u8868\u660e\uff0c\u964d\u4f4e\u7cbe\u5ea6\u7b97\u6cd5\u548c GPU \u4f18\u5316\u5185\u6838\u662f\u5426\u53ef\u4ee5\u5728\u4e0d\u635f\u5bb3\u7269\u7406\u4fdd\u771f\u5ea6\u7684\u60c5\u51b5\u4e0b\u964d\u4f4e\u8fd9\u79cd\u6210\u672c\u3002", "method": "\u901a\u8fc7\u5bf9 MACE \u8fdb\u884c\u6027\u80fd\u5206\u6790\uff0c\u6bd4\u8f83\u4e0d\u540c\u7684\u540e\u7aef\u548c\u7cbe\u5ea6\u8bbe\u7f6e\uff0c\u5e76\u5728\u53ef\u91cd\u73b0\u7684\u7a33\u6001\u8ba1\u65f6\u4e0b\u8fdb\u884c\u63a8\u7406\u3001NVT \u548c NPT \u6c34\u6a21\u62df\u4ee5\u53ca\u73a9\u5177\u8bad\u7ec3\u3002", "result": "cuEquivariance \u5c06\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u4e86\u7ea6 3 \u500d\u3002\u5728 FP32 \u6a21\u578b\u4e2d\u4ec5\u5c06\u7ebf\u6027\u5c42\u8f6c\u6362\u4e3a BF16/FP16 \u53ef\u989d\u5916\u52a0\u901f\u7ea6 4 \u500d\uff0c\u800c NVT/NPT MD \u4e2d\u7684\u80fd\u91cf\u548c\u70ed\u529b\u5b66\u53ef\u89c2\u6d4b\u91cf\u4fdd\u6301\u5728\u8fd0\u884c\u95f4\u53d8\u5f02\u6027\u8303\u56f4\u5185\u3002\u8bad\u7ec3\u671f\u95f4\u7684\u534a\u7cbe\u5ea6\u6743\u91cd\u4f1a\u964d\u4f4e\u529b\u7684 RMSE\u3002\u5728\u6ca1\u6709\u663e\u5f0f\u9002\u914d\u5668\u7684\u60c5\u51b5\u4e0b\u6df7\u5408 e3nn \u548c cuEq \u6a21\u5757\u4f1a\u5bfc\u81f4\u8868\u793a\u4e0d\u5339\u914d\u3002", "conclusion": "\u878d\u5408\u7684\u7b49\u53d8\u5185\u6838\u548c\u6df7\u5408\u7cbe\u5ea6\u63a8\u7406\u53ef\u4ee5\u663e\u7740\u52a0\u901f\u6700\u5148\u8fdb\u7684\u529b\u573a\uff0c\u800c\u5bf9\u4e0b\u6e38 MD \u7684\u5f71\u54cd\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002\u4e00\u79cd\u5b9e\u7528\u7684\u7b56\u7565\u662f\u9ed8\u8ba4\u4f7f\u7528\u5e26\u6709 FP32 \u7684 cuEquivariance\uff0c\u5e76\u4e3a\u7ebf\u6027\u5c42\u542f\u7528 BF16/FP16\uff08\u4fdd\u6301 FP32 \u7d2f\u79ef\uff09\u4ee5\u5b9e\u73b0\u6700\u5927\u541e\u5410\u91cf\uff0c\u800c\u8bad\u7ec3\u4ecd\u4fdd\u6301\u5728 FP32 \u4e2d\u3002\u9884\u8ba1\u5728 Ampere/Hopper GPU\uff08TF32/BF16\uff09\u4e0a\u4ee5\u53ca\u6765\u81ea\u5185\u6838\u7ea7 FP16/BF16 \u8def\u5f84\u548c\u6d41\u6c34\u7ebf\u878d\u5408\u5c06\u83b7\u5f97\u66f4\u591a\u6536\u76ca\u3002"}}
{"id": "2510.24469", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.24469", "abs": "https://arxiv.org/abs/2510.24469", "authors": ["Durga Prasad Maram", "Dhruvin Gandhi", "Zonghai Yao", "Gayathri Akkinapalli", "Franck Dernoncourt", "Yu Wang", "Ryan A. Rossi", "Nesreen K. Ahmed"], "title": "Iterative Critique-Refine Framework for Enhancing LLM Personalization", "comment": null, "summary": "Personalized text generation requires models not only to produce coherent\ntext but also to align with a target user's style, tone, and topical focus.\nExisting retrieval-augmented approaches such as LaMP and PGraphRAG enrich\nprofiles with user and neighbor histories, but they stop at generation and\noften yield outputs that drift in tone, topic, or style. We present PerFine, a\nunified, training-free critique-refine framework that enhances personalization\nthrough iterative, profile-grounded feedback. In each iteration, an LLM\ngenerator produces a draft conditioned on the retrieved profile, and a critic\nLLM - also conditioned on the same profile - provides structured feedback on\ntone, vocabulary, sentence structure, and topicality. The generator then\nrevises, while a novel knockout strategy retains the stronger draft across\niterations. We further study additional inference-time strategies such as\nBest-of-N and Topic Extraction to balance quality and efficiency. Across Yelp,\nGoodreads, and Amazon datasets, PerFine consistently improves personalization\nover PGraphRAG, with GEval gains of +7-13%, steady improvements over 3-5\nrefinement iterations, and scalability with increasing critic size. These\nresults highlight that post-hoc, profile-aware feedback offers a powerful\nparadigm for personalized LLM generation that is both training-free and\nmodel-agnostic.", "AI": {"tldr": "PerFine\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u548c\u57fa\u4e8e\u7528\u6237\u753b\u50cf\u7684\u53cd\u9988\u6765\u589e\u5f3a\u4e2a\u6027\u5316\u6587\u672c\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4e2a\u6027\u5316\u6587\u672c\u751f\u6210\u4e2d\u5b58\u5728\u8bed\u6c14\u3001\u4e3b\u9898\u6216\u98ce\u683c\u6f02\u79fb\u7684\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u4f7f\u7528\u4e00\u4e2aLLM\u751f\u6210\u5668\u751f\u6210\u8349\u7a3f\uff0c\u7136\u540e\u4f7f\u7528\u53e6\u4e00\u4e2aLLM\u8bc4\u8bba\u5458\u63d0\u4f9b\u5173\u4e8e\u8bed\u6c14\u3001\u8bcd\u6c47\u3001\u53e5\u5b50\u7ed3\u6784\u548c\u4e3b\u9898\u7684\u7ed3\u6784\u5316\u53cd\u9988\uff0c\u751f\u6210\u5668\u518d\u8fdb\u884c\u4fee\u6539\u3002\u6b64\u5916\uff0c\u8fd8\u7814\u7a76\u4e86\u8bf8\u5982Best-of-N\u548cTopic Extraction\u7b49\u63a8\u7406\u65f6\u7b56\u7565\u3002", "result": "\u5728Yelp\u3001Goodreads\u548cAmazon\u6570\u636e\u96c6\u4e0a\uff0cPerFine\u59cb\u7ec8\u4f18\u4e8ePGraphRAG\uff0cGEval\u6307\u6807\u63d0\u9ad8\u4e86+7-13%\uff0c\u5e76\u4e14\u968f\u7740\u8bc4\u8bba\u5458\u89c4\u6a21\u7684\u589e\u52a0\u800c\u5177\u6709\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u4e8b\u540e\u3001\u611f\u77e5\u7528\u6237\u753b\u50cf\u7684\u53cd\u9988\u4e3a\u4e2a\u6027\u5316LLM\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u8303\u4f8b\uff0c\u8be5\u8303\u4f8b\u65e0\u9700\u8bad\u7ec3\u4e14\u4e0e\u6a21\u578b\u65e0\u5173\u3002"}}
{"id": "2510.23828", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23828", "abs": "https://arxiv.org/abs/2510.23828", "authors": ["Mena Attia", "Aashiq Muhamed", "Mai Alkhamissi", "Thamar Solorio", "Mona Diab"], "title": "Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural Processing of Figurative Language", "comment": null, "summary": "We present a comprehensive evaluation of the ability of large language models\n(LLMs) to process culturally grounded language, specifically to understand and\npragmatically use figurative expressions that encode local knowledge and\ncultural nuance. Using figurative language as a proxy for cultural nuance and\nlocal knowledge, we design evaluation tasks for contextual understanding,\npragmatic use, and connotation interpretation in Arabic and English. We\nevaluate 22 open- and closed-source LLMs on Egyptian Arabic idioms,\nmultidialectal Arabic proverbs, and English proverbs. Our results show a\nconsistent hierarchy: the average accuracy for Arabic proverbs is 4.29% lower\nthan for English proverbs, and performance for Egyptian idioms is 10.28% lower\nthan for Arabic proverbs. For the pragmatic use task, accuracy drops by 14.07%\nrelative to understanding, though providing contextual idiomatic sentences\nimproves accuracy by 10.66%. Models also struggle with connotative meaning,\nreaching at most 85.58% agreement with human annotators on idioms with 100%\ninter-annotator agreement. These findings demonstrate that figurative language\nserves as an effective diagnostic for cultural reasoning: while LLMs can often\ninterpret figurative meaning, they face challenges in using it appropriately.\nTo support future research, we release Kinayat, the first dataset of Egyptian\nArabic idioms designed for both figurative understanding and pragmatic use\nevaluation.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u548c\u4f7f\u7528\u6587\u5316\u80cc\u666f\u4e0b\u7684\u8bed\u8a00\uff0c\u7279\u522b\u662f\u9690\u55bb\u8868\u8fbe\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u8bc4\u4f30\u4efb\u52a1\uff0c\u5e76\u5728\u963f\u62c9\u4f2f\u8bed\u548c\u82f1\u8bed\u7684\u8bed\u5883\u7406\u89e3\u3001\u8bed\u7528\u548c\u5185\u6db5\u89e3\u91ca\u65b9\u9762\u8bc4\u4f30\u4e86 22 \u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u5728\u7406\u89e3\u963f\u62c9\u4f2f\u8bed\u8c1a\u8bed\u65b9\u9762\u7684\u51c6\u786e\u7387\u4f4e\u4e8e\u82f1\u8bed\u8c1a\u8bed\uff0c\u800c\u7406\u89e3\u57c3\u53ca\u4e60\u8bed\u7684\u51c6\u786e\u7387\u4f4e\u4e8e\u963f\u62c9\u4f2f\u8bed\u8c1a\u8bed\u3002\u6a21\u578b\u5728\u8bed\u7528\u4efb\u52a1\u4e2d\u4e5f\u9762\u4e34\u6311\u6218\u3002\u6211\u4eec\u53d1\u5e03\u4e86 Kinayat\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u57c3\u53ca\u963f\u62c9\u4f2f\u8bed\u4e60\u8bed\u6570\u636e\u96c6\uff0c\u65e8\u5728\u7528\u4e8e\u9690\u55bb\u7406\u89e3\u548c\u8bed\u7528\u8bc4\u4f30\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\u6587\u5316\u80cc\u666f\u4e0b\u7684\u8bed\u8a00\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u7406\u89e3\u548c\u5b9e\u9645\u4f7f\u7528\u7f16\u7801\u672c\u5730\u77e5\u8bc6\u548c\u6587\u5316\u7ec6\u5fae\u5dee\u522b\u7684\u6bd4\u55bb\u8868\u8fbe\u65b9\u9762\u3002", "method": "\u4f7f\u7528\u6bd4\u55bb\u8bed\u8a00\u4f5c\u4e3a\u6587\u5316\u7ec6\u5fae\u5dee\u522b\u548c\u672c\u5730\u77e5\u8bc6\u7684\u4ee3\u8868\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u8bc4\u4f30\u963f\u62c9\u4f2f\u8bed\u548c\u82f1\u8bed\u8bed\u5883\u7406\u89e3\u3001\u8bed\u7528\u548c\u5185\u6db5\u89e3\u91ca\u7684\u8bc4\u4f30\u4efb\u52a1\u3002\u6211\u4eec\u8bc4\u4f30\u4e86 22 \u4e2a\u5f00\u6e90\u548c\u95ed\u6e90\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u57c3\u53ca\u963f\u62c9\u4f2f\u8bed\u4e60\u8bed\u3001\u591a\u65b9\u8a00\u963f\u62c9\u4f2f\u8bed\u8c1a\u8bed\u548c\u82f1\u8bed\u8c1a\u8bed\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u963f\u62c9\u4f2f\u8bed\u8c1a\u8bed\u7684\u5e73\u5747\u51c6\u786e\u7387\u6bd4\u82f1\u8bed\u8c1a\u8bed\u4f4e 4.29%\uff0c\u800c\u57c3\u53ca\u4e60\u8bed\u7684\u51c6\u786e\u7387\u6bd4\u963f\u62c9\u4f2f\u8bed\u8c1a\u8bed\u4f4e 10.28%\u3002\u5bf9\u4e8e\u8bed\u7528\u4efb\u52a1\uff0c\u51c6\u786e\u7387\u76f8\u5bf9\u4e8e\u7406\u89e3\u4e0b\u964d\u4e86 14.07%\uff0c\u4f46\u63d0\u4f9b\u4e0a\u4e0b\u6587\u4e60\u8bed\u53e5\u5b50\u53ef\u5c06\u51c6\u786e\u7387\u63d0\u9ad8 10.66%\u3002\u6a21\u578b\u5728\u5185\u6db5\u610f\u4e49\u65b9\u9762\u4e5f\u5b58\u5728\u56f0\u96be\uff0c\u5bf9\u4e8e\u5177\u6709 100% \u6807\u6ce8\u8005\u95f4\u4e00\u81f4\u6027\u7684\u4e60\u8bed\uff0c\u4e0e\u4eba\u5de5\u6807\u6ce8\u8005\u7684\u4e00\u81f4\u6027\u6700\u9ad8\u8fbe\u5230 85.58%\u3002", "conclusion": "\u6bd4\u55bb\u8bed\u8a00\u53ef\u4ee5\u4f5c\u4e3a\u6587\u5316\u63a8\u7406\u7684\u6709\u6548\u8bca\u65ad\u5de5\u5177\uff1a\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u53ef\u4ee5\u89e3\u91ca\u6bd4\u55bb\u610f\u4e49\uff0c\u4f46\u5b83\u4eec\u5728\u4f7f\u7528\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002"}}
{"id": "2510.23744", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23744", "abs": "https://arxiv.org/abs/2510.23744", "authors": ["Eline M. Bovy", "Caleb Probine", "Marnix Suilen", "Ufuk Topcu", "Nils Jansen"], "title": "Multi-Environment POMDPs: Discrete Model Uncertainty Under Partial Observability", "comment": "Accepted at NeurIPS 2025", "summary": "Multi-environment POMDPs (ME-POMDPs) extend standard POMDPs with discrete\nmodel uncertainty. ME-POMDPs represent a finite set of POMDPs that share the\nsame state, action, and observation spaces, but may arbitrarily vary in their\ntransition, observation, and reward models. Such models arise, for instance,\nwhen multiple domain experts disagree on how to model a problem. The goal is to\nfind a single policy that is robust against any choice of POMDP within the set,\ni.e., a policy that maximizes the worst-case reward across all POMDPs. We\ngeneralize and expand on existing work in the following way. First, we show\nthat ME-POMDPs can be generalized to POMDPs with sets of initial beliefs, which\nwe call adversarial-belief POMDPs (AB-POMDPs). Second, we show that any\narbitrary ME-POMDP can be reduced to a ME-POMDP that only varies in its\ntransition and reward functions or only in its observation and reward\nfunctions, while preserving (optimal) policies. We then devise exact and\napproximate (point-based) algorithms to compute robust policies for AB-POMDPs,\nand thus ME-POMDPs. We demonstrate that we can compute policies for standard\nPOMDP benchmarks extended to the multi-environment setting.", "AI": {"tldr": "ME-POMDPs are POMDPs with discrete model uncertainty, where the goal is to find a policy that maximizes the worst-case reward across all POMDPs.", "motivation": "To address the problem of model uncertainty in POMDPs, where multiple domain experts may disagree on how to model a problem.", "method": "The paper generalizes ME-POMDPs to adversarial-belief POMDPs (AB-POMDPs) and reduces any arbitrary ME-POMDP to a ME-POMDP that only varies in its transition and reward functions or only in its observation and reward functions. The paper then devises exact and approximate algorithms to compute robust policies for AB-POMDPs, and thus ME-POMDPs.", "result": "The paper demonstrates that it can compute policies for standard POMDP benchmarks extended to the multi-environment setting.", "conclusion": "The paper provides a method for finding robust policies in ME-POMDPs by generalizing them to AB-POMDPs and developing exact and approximate algorithms."}}
{"id": "2510.23798", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23798", "abs": "https://arxiv.org/abs/2510.23798", "authors": ["Gauthier Grimmer", "Romain Wenger", "Cl\u00e9ment Flint", "Germain Forestier", "Gilles Rixhon", "Valentin Chardon"], "title": "A geometric and deep learning reproducible pipeline for monitoring floating anthropogenic debris in urban rivers using in situ cameras", "comment": null, "summary": "The proliferation of floating anthropogenic debris in rivers has emerged as a\npressing environmental concern, exerting a detrimental influence on\nbiodiversity, water quality, and human activities such as navigation and\nrecreation. The present study proposes a novel methodological framework for the\nmonitoring the aforementioned waste, utilising fixed, in-situ cameras. This\nstudy provides two key contributions: (i) the continuous quantification and\nmonitoring of floating debris using deep learning and (ii) the identification\nof the most suitable deep learning model in terms of accuracy and inference\nspeed under complex environmental conditions. These models are tested in a\nrange of environmental conditions and learning configurations, including\nexperiments on biases related to data leakage. Furthermore, a geometric model\nis implemented to estimate the actual size of detected objects from a 2D image.\nThis model takes advantage of both intrinsic and extrinsic characteristics of\nthe camera. The findings of this study underscore the significance of the\ndataset constitution protocol, particularly with respect to the integration of\nnegative images and the consideration of temporal leakage. In conclusion, the\nfeasibility of metric object estimation using projective geometry coupled with\nregression corrections is demonstrated. This approach paves the way for the\ndevelopment of robust, low-cost, automated monitoring systems for urban aquatic\nenvironments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u56fa\u5b9a\u6444\u50cf\u5934\u76d1\u6d4b\u6cb3\u6d41\u4e2d\u6f02\u6d6e\u5783\u573e\u7684\u65b9\u6cd5\u3002", "motivation": "\u6cb3\u6d41\u4e2d\u6f02\u6d6e\u7684\u4eba\u9020\u5783\u573e\u5bf9\u751f\u7269\u591a\u6837\u6027\u3001\u6c34\u8d28\u548c\u4eba\u7c7b\u6d3b\u52a8\u4ea7\u751f\u4e0d\u5229\u5f71\u54cd\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u548c\u51e0\u4f55\u6a21\u578b\uff0c\u901a\u8fc7\u56fa\u5b9a\u6444\u50cf\u5934\u8fde\u7eed\u91cf\u5316\u548c\u76d1\u6d4b\u6f02\u6d6e\u5783\u573e\uff0c\u5e76\u4f30\u8ba1\u68c0\u6d4b\u5230\u7684\u7269\u4f53\u7684\u5927\u5c0f\u3002", "result": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6570\u636e\u96c6\u6784\u6210\u534f\u8bae\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u5728\u6574\u5408\u8d1f\u9762\u56fe\u50cf\u548c\u8003\u8651\u65f6\u95f4\u6cc4\u6f0f\u65b9\u9762\u3002\u8bc1\u660e\u4e86\u4f7f\u7528\u6295\u5f71\u51e0\u4f55\u548c\u56de\u5f52\u6821\u6b63\u8fdb\u884c\u5ea6\u91cf\u5bf9\u8c61\u4f30\u8ba1\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5f00\u53d1\u57ce\u5e02\u6c34\u751f\u73af\u5883\u7684\u9c81\u68d2\u3001\u4f4e\u6210\u672c\u3001\u81ea\u52a8\u5316\u76d1\u6d4b\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.23622", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23622", "abs": "https://arxiv.org/abs/2510.23622", "authors": ["Alyssa Gerhart", "Balaji Iyangar"], "title": "Adversarially-Aware Architecture Design for Robust Medical AI Systems", "comment": null, "summary": "Adversarial attacks pose a severe risk to AI systems used in healthcare,\ncapable of misleading models into dangerous misclassifications that can delay\ntreatments or cause misdiagnoses. These attacks, often imperceptible to human\nperception, threaten patient safety, particularly in underserved populations.\nOur study explores these vulnerabilities through empirical experimentation on a\ndermatological dataset, where adversarial methods significantly reduce\nclassification accuracy. Through detailed threat modeling, experimental\nbenchmarking, and model evaluation, we demonstrate both the severity of the\nthreat and the partial success of defenses like adversarial training and\ndistillation. Our results show that while defenses reduce attack success rates,\nthey must be balanced against model performance on clean data. We conclude with\na call for integrated technical, ethical, and policy-based approaches to build\nmore resilient, equitable AI in healthcare.", "AI": {"tldr": "\u5bf9\u6297\u6027\u653b\u51fb\u5bf9\u533b\u7597\u4fdd\u5065\u4e2d\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u6784\u6210\u4e25\u91cd\u98ce\u9669\uff0c\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u4ea7\u751f\u5371\u9669\u7684\u9519\u8bef\u5206\u7c7b\u3002", "motivation": "\u7814\u7a76\u5bf9\u6297\u6027\u653b\u51fb\u5bf9\u533b\u7597\u4fdd\u5065\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u5a01\u80c1\uff0c\u5c24\u5176\u662f\u5728\u670d\u52a1\u4e0d\u8db3\u7684\u4eba\u7fa4\u4e2d\u3002", "method": "\u901a\u8fc7\u5728\u76ae\u80a4\u75c5\u5b66\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u91c7\u7528\u8be6\u7ec6\u7684\u5a01\u80c1\u5efa\u6a21\u3001\u5b9e\u9a8c\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u8bc4\u4f30\u3002", "result": "\u5bf9\u6297\u6027\u653b\u51fb\u663e\u8457\u964d\u4f4e\u4e86\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u9632\u5fa1\u63aa\u65bd\uff08\u5982\u5bf9\u6297\u8bad\u7ec3\u548c\u77e5\u8bc6\u84b8\u998f\uff09\u53d6\u5f97\u4e86\u4e00\u5b9a\u7684\u6210\u529f\uff0c\u4f46\u9700\u8981\u5728\u9632\u5fa1\u6548\u679c\u548c\u6a21\u578b\u5728\u5e72\u51c0\u6570\u636e\u4e0a\u7684\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "conclusion": "\u9700\u8981\u91c7\u53d6\u7efc\u5408\u7684\u6280\u672f\u3001\u4f26\u7406\u548c\u653f\u7b56\u65b9\u6cd5\uff0c\u4ee5\u5728\u533b\u7597\u4fdd\u5065\u9886\u57df\u6784\u5efa\u66f4\u5177\u5f39\u6027\u3001\u66f4\u516c\u5e73\u7684AI\u3002"}}
{"id": "2510.24652", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.24652", "abs": "https://arxiv.org/abs/2510.24652", "authors": ["Jiawei Zhou", "Lei Chen"], "title": "Optimizing Retrieval for RAG via Reinforced Contrastive Learning", "comment": null, "summary": "As retrieval-augmented generation (RAG) becomes increasingly widespread, the\nrole of information retrieval (IR) is shifting from retrieving information for\nhuman users to retrieving contextual knowledge for artificial intelligence (AI)\nsystems, where relevance becomes difficult to define or annotate beforehand. To\naddress this challenge, we propose R3, a Retrieval framework optimized for RAG\nthrough trialand-feedback Reinforced contrastive learning. Unlike prior\napproaches that rely on annotated or synthetic data for supervised fine-tuning,\nR3 enables the retriever to dynamically explore and optimize relevance within\nthe RAG environment. During training, the retrieved results interact with the\nenvironment to produce contrastive signals that automatically guide the\nretriever's self-improvement. Extensive experiments across diverse tasks\ndemonstrate that R3 improves RAG performance by 5.2% over the original\nretriever and surpasses state-of-the-art retrievers by 4.9%, while achieving\ncomparable results to LLM-augmented retrieval and RAG systems built on\npost-trained or instruction-tuned LLMs. It is both efficient and practical,\nrequiring only 4 GPUs and completing training within a single day.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aR3\u7684\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5bf9\u6bd4\u5b66\u4e60\u4f18\u5316RAG\u4e2d\u7684\u68c0\u7d22\u3002", "motivation": "\u4f20\u7edf\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e2d\uff0c\u4fe1\u606f\u68c0\u7d22\uff08IR\uff09\u7684\u89d2\u8272\u8f6c\u53d8\u4e3a\u4e3aAI\u7cfb\u7edf\u68c0\u7d22\u4e0a\u4e0b\u6587\u77e5\u8bc6\uff0c\u4f46\u76f8\u5173\u6027\u96be\u4ee5\u9884\u5148\u5b9a\u4e49\u6216\u6807\u6ce8\u3002", "method": "\u4f7f\u7528trial-and-feedback\u5f3a\u5316\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5141\u8bb8\u68c0\u7d22\u5668\u5728RAG\u73af\u5883\u4e2d\u52a8\u6001\u63a2\u7d22\u548c\u4f18\u5316\u76f8\u5173\u6027\u3002\u901a\u8fc7\u68c0\u7d22\u7ed3\u679c\u4e0e\u73af\u5883\u4e92\u52a8\u4ea7\u751f\u5bf9\u6bd4\u4fe1\u53f7\uff0c\u81ea\u52a8\u5f15\u5bfc\u68c0\u7d22\u5668\u7684\u81ea\u6211\u6539\u8fdb\u3002", "result": "\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\uff0cR3\u76f8\u6bd4\u539f\u59cb\u68c0\u7d22\u5668\u63d0\u9ad8\u4e86RAG\u6027\u80fd5.2%\uff0c\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u68c0\u7d22\u56684.9%\uff0c\u5e76\u4e14\u4e0e\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u589e\u5f3a\u7684\u68c0\u7d22\u548cRAG\u7cfb\u7edf\u53d6\u5f97\u4e86\u76f8\u5f53\u7684\u7ed3\u679c\u3002", "conclusion": "R3\u6846\u67b6\u9ad8\u6548\u4e14\u5b9e\u7528\uff0c\u4ec5\u9700\u5c11\u91cfGPU\u8d44\u6e90\u5373\u53ef\u5b8c\u6210\u8bad\u7ec3\u3002"}}
{"id": "2510.23842", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23842", "abs": "https://arxiv.org/abs/2510.23842", "authors": ["Saki Imai", "Lee Kezar", "Laurel Aichler", "Mert Inan", "Erin Walker", "Alicia Wooten", "Lorna Quandt", "Malihe Alikhani"], "title": "How Pragmatics Shape Articulation: A Computational Case Study in STEM ASL Discourse", "comment": null, "summary": "Most state-of-the-art sign language models are trained on interpreter or\nisolated vocabulary data, which overlooks the variability that characterizes\nnatural dialogue. However, human communication dynamically adapts to contexts\nand interlocutors through spatiotemporal changes and articulation style. This\nspecifically manifests itself in educational settings, where novel vocabularies\nare used by teachers, and students. To address this gap, we collect a motion\ncapture dataset of American Sign Language (ASL) STEM (Science, Technology,\nEngineering, and Mathematics) dialogue that enables quantitative comparison\nbetween dyadic interactive signing, solo signed lecture, and interpreted\narticles. Using continuous kinematic features, we disentangle dialogue-specific\nentrainment from individual effort reduction and show spatiotemporal changes\nacross repeated mentions of STEM terms. On average, dialogue signs are\n24.6%-44.6% shorter in duration than the isolated signs, and show significant\nreductions absent in monologue contexts. Finally, we evaluate sign embedding\nmodels on their ability to recognize STEM signs and approximate how entrained\nthe participants become over time. Our study bridges linguistic analysis and\ncomputational modeling to understand how pragmatics shape sign articulation and\nits representation in sign language technologies.", "AI": {"tldr": "\u6536\u96c6\u4e86\u4e00\u4e2a\u7f8e\u56fd\u624b\u8bed (ASL) STEM\uff08\u79d1\u5b66\u3001\u6280\u672f\u3001\u5de5\u7a0b\u548c\u6570\u5b66\uff09\u5bf9\u8bdd\u7684\u8fd0\u52a8\u6355\u6349\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u80fd\u591f\u5bf9 dyadic \u4ea4\u4e92\u5f0f\u7b7e\u540d\u3001\u5355\u72ec\u7b7e\u540d\u8bb2\u5ea7\u548c\u89e3\u91ca\u6587\u7ae0\u4e4b\u95f4\u8fdb\u884c\u5b9a\u91cf\u6bd4\u8f83\u3002", "motivation": "\u76ee\u524d\u6700\u597d\u7684\u624b\u8bed\u6a21\u578b\u5927\u591a\u5728\u53e3\u8bd1\u5458\u6216\u5b64\u7acb\u7684\u8bcd\u6c47\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5ffd\u7565\u4e86\u81ea\u7136\u5bf9\u8bdd\u7684\u53d8\u5f02\u6027\u3002\u7136\u800c\uff0c\u4eba\u7c7b\u4ea4\u6d41\u901a\u8fc7\u65f6\u7a7a\u53d8\u5316\u548c\u8868\u8fbe\u65b9\u5f0f\u52a8\u6001\u5730\u9002\u5e94\u8bed\u5883\u548c\u5bf9\u8bdd\u8005\u3002\u8fd9\u5c24\u5176\u4f53\u73b0\u5728\u6559\u80b2\u73af\u5883\u4e2d\uff0c\u6559\u5e08\u548c\u5b66\u751f\u4f7f\u7528\u65b0\u7684\u8bcd\u6c47\u3002", "method": "\u4f7f\u7528\u8fde\u7eed\u8fd0\u52a8\u5b66\u7279\u5f81\uff0c\u6211\u4eec\u5c06\u7279\u5b9a\u4e8e\u5bf9\u8bdd\u7684\u5939\u5e26\u4e0e\u4e2a\u4eba\u52aa\u529b\u51cf\u5c11\u533a\u5206\u5f00\u6765\uff0c\u5e76\u663e\u793a STEM \u672f\u8bed\u91cd\u590d\u63d0\u53ca\u7684\u65f6\u7a7a\u53d8\u5316\u3002", "result": "\u5e73\u5747\u800c\u8a00\uff0c\u5bf9\u8bdd\u7b26\u53f7\u7684\u6301\u7eed\u65f6\u95f4\u6bd4\u5b64\u7acb\u7b26\u53f7\u77ed 24.6%-44.6%\uff0c\u5e76\u4e14\u5728\u72ec\u767d\u4e0a\u4e0b\u6587\u4e2d\u6ca1\u6709\u663e\u7740\u51cf\u5c11\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u5c06\u8bed\u8a00\u5206\u6790\u548c\u8ba1\u7b97\u5efa\u6a21\u8054\u7cfb\u8d77\u6765\uff0c\u4ee5\u4e86\u89e3\u8bed\u7528\u5b66\u5982\u4f55\u5f71\u54cd\u7b26\u53f7\u8868\u8fbe\u53ca\u5176\u5728\u624b\u8bed\u6280\u672f\u4e2d\u7684\u8868\u793a\u3002"}}
{"id": "2510.23746", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23746", "abs": "https://arxiv.org/abs/2510.23746", "authors": ["Laura Mismetti", "Marvin Alberts", "Andreas Krause", "Mara Graziani"], "title": "Test-Time Tuned Language Models Enable End-to-end De Novo Molecular Structure Generation from MS/MS Spectra", "comment": null, "summary": "Tandem Mass Spectrometry enables the identification of unknown compounds in\ncrucial fields such as metabolomics, natural product discovery and\nenvironmental analysis. However, current methods rely on database matching from\npreviously observed molecules, or on multi-step pipelines that require\nintermediate fragment or fingerprint prediction. This makes finding the correct\nmolecule highly challenging, particularly for compounds absent from reference\ndatabases. We introduce a framework that, by leveraging test-time tuning,\nenhances the learning of a pre-trained transformer model to address this gap,\nenabling end-to-end de novo molecular structure generation directly from the\ntandem mass spectra and molecular formulae, bypassing manual annotations and\nintermediate steps. We surpass the de-facto state-of-the-art approach DiffMS on\ntwo popular benchmarks NPLIB1 and MassSpecGym by 100% and 20%, respectively.\nTest-time tuning on experimental spectra allows the model to dynamically adapt\nto novel spectra, and the relative performance gain over conventional\nfine-tuning is of 62% on MassSpecGym. When predictions deviate from the ground\ntruth, the generated molecular candidates remain structurally accurate,\nproviding valuable guidance for human interpretation and more reliable\nidentification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6d4b\u8bd5\u65f6\u8c03\u6574\uff0c\u589e\u5f3a\u9884\u8bad\u7ec3 Transformer \u6a21\u578b\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u4ee5\u89e3\u51b3\u4e32\u8054\u8d28\u8c31\u5206\u6790\u4e2d\u4ece\u5934\u5206\u5b50\u7ed3\u6784\u751f\u6210\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5148\u524d\u89c2\u5bdf\u5230\u7684\u5206\u5b50\u7684\u6570\u636e\u5e93\u5339\u914d\uff0c\u6216\u9700\u8981\u4e2d\u95f4\u7247\u6bb5\u6216\u6307\u7eb9\u9884\u6d4b\u7684\u591a\u6b65\u9aa4\u6d41\u7a0b\uff0c\u8fd9\u4f7f\u5f97\u5bfb\u627e\u6b63\u786e\u7684\u5206\u5b50\u975e\u5e38\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u53c2\u8003\u6570\u636e\u5e93\u4e2d\u4e0d\u5b58\u5728\u7684\u5316\u5408\u7269\u3002", "method": "\u8be5\u6846\u67b6\u901a\u8fc7\u5229\u7528\u6d4b\u8bd5\u65f6\u8c03\u6574\uff0c\u589e\u5f3a\u9884\u8bad\u7ec3\u7684 Transformer \u6a21\u578b\uff0c\u5b9e\u73b0\u76f4\u63a5\u4ece\u4e32\u8054\u8d28\u8c31\u548c\u5206\u5b50\u5f0f\u8fdb\u884c\u7aef\u5230\u7aef\u7684\u5206\u5b50\u7ed3\u6784\u751f\u6210\uff0c\u7ed5\u8fc7\u624b\u52a8\u6ce8\u91ca\u548c\u4e2d\u95f4\u6b65\u9aa4\u3002", "result": "\u5728\u4e24\u4e2a\u6d41\u884c\u7684\u57fa\u51c6\u6d4b\u8bd5 NPLIB1 \u548c MassSpecGym \u4e0a\uff0c\u8d85\u8d8a\u4e86\u4e8b\u5b9e\u4e0a\u7684\u6700\u5148\u8fdb\u65b9\u6cd5 DiffMS\uff0c\u5206\u522b\u63d0\u9ad8\u4e86 100% \u548c 20%\u3002\u5728\u5b9e\u9a8c\u5149\u8c31\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u65f6\u8c03\u6574\u5141\u8bb8\u6a21\u578b\u52a8\u6001\u9002\u5e94\u65b0\u7684\u5149\u8c31\uff0c\u5e76\u4e14\u5728 MassSpecGym \u4e0a\uff0c\u76f8\u5bf9\u4e8e\u4f20\u7edf\u5fae\u8c03\u7684\u76f8\u5bf9\u6027\u80fd\u589e\u76ca\u4e3a 62%\u3002", "conclusion": "\u5373\u4f7f\u9884\u6d4b\u504f\u79bb\u4e86\u771f\u5b9e\u60c5\u51b5\uff0c\u751f\u6210\u7684\u5206\u5b50\u5019\u9009\u7269\u4ecd\u7136\u5728\u7ed3\u6784\u4e0a\u662f\u51c6\u786e\u7684\uff0c\u4e3a\u4eba\u5de5\u89e3\u91ca\u548c\u66f4\u53ef\u9760\u7684\u8bc6\u522b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6307\u5bfc\u3002"}}
{"id": "2510.23816", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23816", "abs": "https://arxiv.org/abs/2510.23816", "authors": ["Forouzan Fallah", "Wenwen Li", "Chia-Yu Hsu", "Hyunho Lee", "Yezhou Yang"], "title": "RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution of Rare-Earth Features", "comment": null, "summary": "Super-resolution (SR) for remote sensing imagery often fails under\nout-of-distribution (OOD) conditions, such as rare geomorphic features captured\nby diverse sensors, producing visually plausible but physically inaccurate\nresults. We present RareFlow, a physics-aware SR framework designed for OOD\nrobustness. RareFlow's core is a dual-conditioning architecture. A Gated\nControlNet preserves fine-grained geometric fidelity from the low-resolution\ninput, while textual prompts provide semantic guidance for synthesizing complex\nfeatures. To ensure physically sound outputs, we introduce a multifaceted loss\nfunction that enforces both spectral and radiometric consistency with sensor\nproperties. Furthermore, the framework quantifies its own predictive\nuncertainty by employing a stochastic forward pass approach; the resulting\noutput variance directly identifies unfamiliar inputs, mitigating feature\nhallucination. We validate RareFlow on a new, curated benchmark of multi-sensor\nsatellite imagery. In blind evaluations, geophysical experts rated our model's\noutputs as approaching the fidelity of ground truth imagery, significantly\noutperforming state-of-the-art baselines. This qualitative superiority is\ncorroborated by quantitative gains in perceptual metrics, including a nearly\n40\\% reduction in FID. RareFlow provides a robust framework for high-fidelity\nsynthesis in data-scarce scientific domains and offers a new paradigm for\ncontrolled generation under severe domain shift.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9065\u611f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387(SR)\u6846\u67b6RareFlow\uff0c\u8be5\u6846\u67b6\u5728\u57df\u5916(OOD)\u6761\u4ef6\u4e0b\u5177\u6709\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u53cc\u91cd\u6761\u4ef6\u67b6\u6784\u3001\u591a\u65b9\u9762\u7684\u635f\u5931\u51fd\u6570\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6765\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u9065\u611f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5728OOD\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4ea7\u751f\u89c6\u89c9\u4e0a\u5408\u7406\u4f46\u7269\u7406\u4e0a\u4e0d\u51c6\u786e\u7684\u7ed3\u679c\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u9065\u611f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u5728OOD\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u7684\u6838\u5fc3\u662f\u4e00\u4e2a\u53cc\u91cd\u6761\u4ef6\u67b6\u6784\uff0c\u5229\u7528\u95e8\u63a7ControlNet\u4ece\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u4e2d\u4fdd\u7559\u7ec6\u7c92\u5ea6\u7684\u51e0\u4f55\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u5229\u7528\u6587\u672c\u63d0\u793a\u4e3a\u5408\u6210\u590d\u6742\u7279\u5f81\u63d0\u4f9b\u8bed\u4e49\u6307\u5bfc\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u591a\u65b9\u9762\u7684\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u786e\u4fdd\u8f93\u51fa\u5728\u5149\u8c31\u548c\u8f90\u5c04\u4e0a\u4e0e\u4f20\u611f\u5668\u7279\u6027\u4fdd\u6301\u4e00\u81f4\u3002\u8be5\u6846\u67b6\u8fd8\u901a\u8fc7\u91c7\u7528\u968f\u673a\u524d\u5411\u4f20\u9012\u65b9\u6cd5\u6765\u91cf\u5316\u5176\u81ea\u8eab\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u591a\u4f20\u611f\u5668\u536b\u661f\u56fe\u50cf\u7684\u65b0\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86RareFlow\u3002\u5730\u7403\u7269\u7406\u4e13\u5bb6\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6a21\u578b\u7684\u8f93\u51fa\u63a5\u8fd1\u5730\u9762\u5b9e\u51b5\u56fe\u50cf\u7684\u4fdd\u771f\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\u3002\u611f\u77e5\u6307\u6807\u7684\u5b9a\u91cf\u589e\u76ca\u4e5f\u8bc1\u5b9e\u4e86\u8fd9\u79cd\u5b9a\u6027\u4f18\u52bf\uff0c\u5305\u62ecFID\u964d\u4f4e\u4e86\u8fd140%\u3002", "conclusion": "RareFlow\u4e3a\u6570\u636e\u7a00\u7f3a\u7684\u79d1\u5b66\u9886\u57df\u4e2d\u7684\u9ad8\u4fdd\u771f\u5408\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u7684\u6846\u67b6\uff0c\u5e76\u4e3a\u4e25\u91cd\u9886\u57df\u8f6c\u79fb\u4e0b\u7684\u53d7\u63a7\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u8303\u4f8b\u3002"}}
{"id": "2510.23624", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23624", "abs": "https://arxiv.org/abs/2510.23624", "authors": ["Tiago Mendon\u00e7a dos Santos", "Rafael Izbicki", "Lu\u00eds Gustavo Esteves"], "title": "DiNo and RanBu: Lightweight Predictions from Shallow Random Forests", "comment": null, "summary": "Random Forest ensembles are a strong baseline for tabular prediction tasks,\nbut their reliance on hundreds of deep trees often results in high inference\nlatency and memory demands, limiting deployment in latency-sensitive or\nresource-constrained environments. We introduce DiNo (Distance with Nodes) and\nRanBu (Random Bushes), two shallow-forest methods that convert a small set of\ndepth-limited trees into efficient, distance-weighted predictors. DiNo measures\ncophenetic distances via the most recent common ancestor of observation pairs,\nwhile RanBu applies kernel smoothing to Breiman's classical proximity measure.\nBoth approaches operate entirely after forest training: no additional trees are\ngrown, and tuning of the single bandwidth parameter $h$ requires only\nlightweight matrix-vector operations. Across three synthetic benchmarks and 25\npublic datasets, RanBu matches or exceeds the accuracy of full-depth random\nforests-particularly in high-noise settings-while reducing training plus\ninference time by up to 95\\%. DiNo achieves the best bias-variance trade-off in\nlow-noise regimes at a modest computational cost. Both methods extend directly\nto quantile regression, maintaining accuracy with substantial speed gains. The\nimplementation is available as an open-source R/C++ package at\nhttps://github.com/tiagomendonca/dirf. We focus on structured tabular random\nsamples (i.i.d.), leaving extensions to other modalities for future work.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u6d45\u5c42\u68ee\u6797\u65b9\u6cd5DiNo\u548cRanBu\uff0c\u5c06\u5c11\u91cf\u6df1\u5ea6\u6709\u9650\u7684\u6811\u8f6c\u6362\u4e3a\u6709\u6548\u7684\u8ddd\u79bb\u52a0\u6743\u9884\u6d4b\u5668\u3002", "motivation": "\u968f\u673a\u68ee\u6797\u96c6\u662f\u8868\u683c\u9884\u6d4b\u4efb\u52a1\u7684\u5f3a\u5927\u57fa\u7ebf\uff0c\u4f46\u5b83\u4eec\u5bf9\u6570\u767e\u68f5\u6df1\u5ea6\u6811\u7684\u4f9d\u8d56\u901a\u5e38\u5bfc\u81f4\u9ad8\u63a8\u7406\u5ef6\u8fdf\u548c\u5185\u5b58\u9700\u6c42\uff0c\u9650\u5236\u4e86\u5728\u5ef6\u8fdf\u654f\u611f\u6216\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u3002", "method": "DiNo\u901a\u8fc7\u89c2\u5bdf\u5bf9\u7684\u6700\u8fd1\u516c\u5171\u7956\u5148\u6765\u6d4b\u91cf\u8c31\u7cfb\u8ddd\u79bb\uff0c\u800cRanBu\u5c06\u6838\u5e73\u6ed1\u5e94\u7528\u4e8eBreiman\u7684\u7ecf\u5178\u90bb\u8fd1\u5ea6\u91cf\u3002", "result": "RanBu\u5728\u4e09\u4e2a\u5408\u6210\u57fa\u51c6\u548c25\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e2d\u5339\u914d\u6216\u8d85\u8fc7\u4e86\u5168\u6df1\u5ea6\u968f\u673a\u68ee\u6797\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u566a\u58f0\u73af\u5883\u4e2d\uff0c\u540c\u65f6\u5c06\u8bad\u7ec3\u52a0\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u4e86\u9ad8\u8fbe95%\u3002DiNo\u5728\u4f4e\u566a\u58f0\u72b6\u6001\u4e0b\u4ee5\u9002\u5ea6\u7684\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u3002", "conclusion": "\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u53ef\u4ee5\u76f4\u63a5\u6269\u5c55\u5230\u5206\u4f4d\u6570\u56de\u5f52\uff0c\u5e76\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5927\u5e45\u63d0\u9ad8\u901f\u5ea6\u3002"}}
{"id": "2510.24701", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.24701", "abs": "https://arxiv.org/abs/2510.24701", "authors": ["Tongyi DeepResearch Team", "Baixuan Li", "Bo Zhang", "Dingchu Zhang", "Fei Huang", "Guangyu Li", "Guoxin Chen", "Huifeng Yin", "Jialong Wu", "Jingren Zhou", "Kuan Li", "Liangcai Su", "Litu Ou", "Liwen Zhang", "Pengjun Xie", "Rui Ye", "Wenbiao Yin", "Xinmiao Yu", "Xinyu Wang", "Xixi Wu", "Xuanzhong Chen", "Yida Zhao", "Zhen Zhang", "Zhengwei Tao", "Zhongwang Zhang", "Zile Qiao", "Chenxi Wang", "Donglei Yu", "Gang Fu", "Haiyang Shen", "Jiayin Yang", "Jun Lin", "Junkai Zhang", "Kui Zeng", "Li Yang", "Hailong Yin", "Maojia Song", "Ming Yan", "Peng Xia", "Qian Xiao", "Rui Min", "Ruixue Ding", "Runnan Fang", "Shaowei Chen", "Shen Huang", "Shihang Wang", "Shihao Cai", "Weizhou Shen", "Xiaobin Wang", "Xin Guan", "Xinyu Geng", "Yingcheng Shi", "Yuning Wu", "Zhuo Chen", "Zijian Li", "Yong Jiang"], "title": "Tongyi DeepResearch Technical Report", "comment": "https://tongyi-agent.github.io/blog", "summary": "We present Tongyi DeepResearch, an agentic large language model, which is\nspecifically designed for long-horizon, deep information-seeking research\ntasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is\ndeveloped through an end-to-end training framework that combines agentic\nmid-training and agentic post-training, enabling scalable reasoning and\ninformation seeking across complex tasks. We design a highly scalable data\nsynthesis pipeline that is fully automatic, without relying on costly human\nannotation, and empowers all training stages. By constructing customized\nenvironments for each stage, our system enables stable and consistent\ninteractions throughout. Tongyi DeepResearch, featuring 30.5 billion total\nparameters, with only 3.3 billion activated per token, achieves\nstate-of-the-art performance across a range of agentic deep research\nbenchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,\nWebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We\nopen-source the model, framework, and complete solutions to empower the\ncommunity.", "AI": {"tldr": "Tongyi DeepResearch \u662f\u4e00\u4e2a\u4e13\u4e3a\u957f\u671f\u3001\u6df1\u5ea6\u4fe1\u606f\u68c0\u7d22\u7814\u7a76\u4efb\u52a1\u800c\u8bbe\u8ba1\u7684 agentic \u5927\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u4e3a\u4e86\u6fc0\u52b1\u81ea\u4e3b\u6df1\u5ea6\u7814\u7a76\u80fd\u529b\uff0cTongyi DeepResearch \u901a\u8fc7\u7ed3\u5408 agentic mid-training \u548c agentic post-training \u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u6846\u67b6\u8fdb\u884c\u5f00\u53d1\uff0c\u4ece\u800c\u80fd\u591f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u63a8\u7406\u548c\u4fe1\u606f\u68c0\u7d22\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u9ad8\u5ea6\u53ef\u6269\u5c55\u7684\u6570\u636e\u5408\u6210\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u662f\u5b8c\u5168\u81ea\u52a8\u7684\uff0c\u4e0d\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684\u4eba\u5de5\u6ce8\u91ca\uff0c\u5e76\u652f\u6301\u6240\u6709\u8bad\u7ec3\u9636\u6bb5\u3002\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u9636\u6bb5\u6784\u5efa\u5b9a\u5236\u73af\u5883\uff0c\u6211\u4eec\u7684\u7cfb\u7edf\u80fd\u591f\u5728\u6574\u4e2a\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u7a33\u5b9a\u548c\u4e00\u81f4\u7684\u4ea4\u4e92\u3002", "result": "Tongyi DeepResearch \u5177\u6709 305 \u4ebf\u4e2a\u603b\u53c2\u6570\uff0c\u4f46\u6bcf\u4e2a token \u4ec5\u6fc0\u6d3b 33 \u4ebf\u4e2a\u53c2\u6570\uff0c\u5728\u5305\u62ec Humanity's Last Exam\u3001BrowseComp\u3001BrowseComp-ZH\u3001WebWalkerQA\u3001xbench-DeepSearch\u3001FRAMES \u548c xbench-DeepSearch-2510 \u5728\u5185\u7684\u4e00\u7cfb\u5217 agentic \u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u6211\u4eec\u5f00\u6e90\u4e86\u6a21\u578b\u3001\u6846\u67b6\u548c\u5b8c\u6574\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u589e\u5f3a\u793e\u533a\u80fd\u529b\u3002"}}
{"id": "2510.23845", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23845", "abs": "https://arxiv.org/abs/2510.23845", "authors": ["Grace Byun", "Rebecca Lipschutz", "Sean T. Minton", "Abigail Lott", "Jinho D. Choi"], "title": "CRADLE Bench: A Clinician-Annotated Benchmark for Multi-Faceted Mental Health Crisis and Safety Risk Detection", "comment": null, "summary": "Detecting mental health crisis situations such as suicide ideation, rape,\ndomestic violence, child abuse, and sexual harassment is a critical yet\nunderexplored challenge for language models. When such situations arise during\nuser--model interactions, models must reliably flag them, as failure to do so\ncan have serious consequences. In this work, we introduce CRADLE BENCH, a\nbenchmark for multi-faceted crisis detection. Unlike previous efforts that\nfocus on a limited set of crisis types, our benchmark covers seven types\ndefined in line with clinical standards and is the first to incorporate\ntemporal labels. Our benchmark provides 600 clinician-annotated evaluation\nexamples and 420 development examples, together with a training corpus of\naround 4K examples automatically labeled using a majority-vote ensemble of\nmultiple language models, which significantly outperforms single-model\nannotation. We further fine-tune six crisis detection models on subsets defined\nby consensus and unanimous ensemble agreement, providing complementary models\ntrained under different agreement criteria.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86CRADLE BENCH\uff0c\u4e00\u4e2a\u7528\u4e8e\u591a\u65b9\u9762\u5371\u673a\u68c0\u6d4b\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u4e03\u79cd\u5371\u673a\u7c7b\u578b\u5e76\u5305\u542b\u65f6\u95f4\u6807\u7b7e\u3002", "motivation": "\u68c0\u6d4b\u5fc3\u7406\u5065\u5eb7\u5371\u673a\u60c5\u51b5\uff08\u5982\u81ea\u6740\u610f\u5ff5\u3001\u5f3a\u5978\u3001\u5bb6\u5ead\u66b4\u529b\u3001\u8650\u5f85\u513f\u7ae5\u548c\u6027\u9a9a\u6270\uff09\u5bf9\u4e8e\u8bed\u8a00\u6a21\u578b\u6765\u8bf4\u81f3\u5173\u91cd\u8981\u4f46\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u6a21\u578b\u5fc5\u987b\u53ef\u9760\u5730\u6807\u8bb0\u8fd9\u4e9b\u60c5\u51b5\uff0c\u5426\u5219\u53ef\u80fd\u4f1a\u4ea7\u751f\u4e25\u91cd\u540e\u679c\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b600\u4e2a\u4e34\u5e8a\u533b\u751f\u6ce8\u91ca\u7684\u8bc4\u4f30\u793a\u4f8b\u548c420\u4e2a\u5f00\u53d1\u793a\u4f8b\u7684\u57fa\u51c6\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5305\u542b\u7ea64K\u4e2a\u793a\u4f8b\u7684\u8bad\u7ec3\u8bed\u6599\u5e93\uff0c\u8fd9\u4e9b\u793a\u4f8b\u4f7f\u7528\u591a\u4e2a\u8bed\u8a00\u6a21\u578b\u7684\u5927\u591a\u6570\u6295\u7968\u96c6\u6210\u81ea\u52a8\u6807\u8bb0\u3002", "result": "\u901a\u8fc7\u5728\u5171\u8bc6\u548c\u4e00\u81f4\u96c6\u6210\u534f\u8bae\u5b9a\u4e49\u7684\u5b50\u96c6\u4e0a\u5fae\u8c03\u516d\u4e2a\u5371\u673a\u68c0\u6d4b\u6a21\u578b\uff0c\u63d0\u4f9b\u4e86\u5728\u4e0d\u540c\u534f\u8bae\u6807\u51c6\u4e0b\u8bad\u7ec3\u7684\u4e92\u8865\u6a21\u578b\u3002", "conclusion": "CRADLE BENCH\u57fa\u51c6\u7684\u63d0\u51fa"}}
{"id": "2510.23772", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23772", "abs": "https://arxiv.org/abs/2510.23772", "authors": ["Vivek Veeriah", "Federico Barbero", "Marcus Chiam", "Xidong Feng", "Michael Dennis", "Ryan Pachauri", "Thomas Tumiel", "Johan Obando-Ceron", "Jiaxin Shi", "Shaobo Hou", "Satinder Singh", "Nenad Toma\u0161ev", "Tom Zahavy"], "title": "Evaluating In Silico Creativity: An Expert Review of AI Chess Compositions", "comment": "Accepted at the Creative AI Track, NeurIPS 2025", "summary": "The rapid advancement of Generative AI has raised significant questions\nregarding its ability to produce creative and novel outputs. Our recent work\ninvestigates this question within the domain of chess puzzles and presents an\nAI system designed to generate puzzles characterized by aesthetic appeal,\nnovelty, counter-intuitive and unique solutions. We briefly discuss our method\nbelow and refer the reader to the technical paper for more details. To assess\nour system's creativity, we presented a curated booklet of AI-generated puzzles\nto three world-renowned experts: International Master for chess compositions\nAmatzia Avni, Grandmaster Jonathan Levitt, and Grandmaster Matthew Sadler. All\nthree are noted authors on chess aesthetics and the evolving role of computers\nin the game. They were asked to select their favorites and explain what made\nthem appealing, considering qualities such as their creativity, level of\nchallenge, or aesthetic design.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5728\u521b\u4f5c\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u80fd\u591f\u751f\u6210\u5177\u6709\u5ba1\u7f8e\u5438\u5f15\u529b\u3001\u65b0\u9896\u3001\u8fdd\u53cd\u76f4\u89c9\u548c\u72ec\u7279\u89e3\u6cd5\u7684\u8c1c\u9898\u7684AI\u7cfb\u7edf\u3002", "motivation": "\u63a2\u8ba8\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u662f\u5426\u80fd\u591f\u4ea7\u751f\u521b\u9020\u6027\u548c\u65b0\u9896\u7684\u8f93\u51fa\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2aAI\u7cfb\u7edf\u6765\u751f\u6210\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\uff0c\u5e76\u7531\u56fd\u9645\u8c61\u68cb\u4e13\u5bb6\u8bc4\u4f30\u3002", "result": "\u56fd\u9645\u8c61\u68cb\u4e13\u5bb6\u4eec\u88ab\u8981\u6c42\u9009\u62e9\u4ed6\u4eec\u6700\u559c\u6b22\u7684\u8c1c\u9898\uff0c\u5e76\u89e3\u91ca\u5176\u5438\u5f15\u529b\u3002", "conclusion": "\u901a\u8fc7\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\u7684\u751f\u6210\u548c\u8bc4\u4f30\uff0c\u7814\u7a76\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u521b\u9020\u529b\u3002"}}
{"id": "2510.23880", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.23880", "abs": "https://arxiv.org/abs/2510.23880", "authors": ["Hanke Chen", "Yuan Liu", "Minchen Li"], "title": "TRELLISWorld: Training-Free World Generation from Object Generators", "comment": null, "summary": "Text-driven 3D scene generation holds promise for a wide range of\napplications, from virtual prototyping to AR/VR and simulation. However,\nexisting methods are often constrained to single-object generation, require\ndomain-specific training, or lack support for full 360-degree viewability. In\nthis work, we present a training-free approach to 3D scene synthesis by\nrepurposing general-purpose text-to-3D object diffusion models as modular tile\ngenerators. We reformulate scene generation as a multi-tile denoising problem,\nwhere overlapping 3D regions are independently generated and seamlessly blended\nvia weighted averaging. This enables scalable synthesis of large, coherent\nscenes while preserving local semantic control. Our method eliminates the need\nfor scene-level datasets or retraining, relies on minimal heuristics, and\ninherits the generalization capabilities of object-level priors. We demonstrate\nthat our approach supports diverse scene layouts, efficient generation, and\nflexible editing, establishing a simple yet powerful foundation for\ngeneral-purpose, language-driven 3D scene construction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u5229\u7528\u6587\u672c\u52303D\u5bf9\u8c61\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u6a21\u5757\u5316\u74e6\u7247\u751f\u6210\u5668\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u3001\u8fde\u8d2f\u7684\u573a\u666f\u5408\u6210\uff0c\u540c\u65f6\u4fdd\u7559\u5c40\u90e8\u8bed\u4e49\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u5355\u5bf9\u8c61\u751f\u6210\uff0c\u9700\u8981\u7279\u5b9a\u9886\u57df\u7684\u8bad\u7ec3\uff0c\u6216\u8005\u7f3a\u4e4f\u5bf9\u5b8c\u6574360\u5ea6\u89c6\u89d2\u7684\u652f\u6301\u3002", "method": "\u5c06\u573a\u666f\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u591a\u74e6\u7247\u53bb\u566a\u95ee\u9898\uff0c\u5176\u4e2d\u91cd\u53e0\u76843D\u533a\u57df\u88ab\u72ec\u7acb\u751f\u6210\u5e76\u901a\u8fc7\u52a0\u6743\u5e73\u5747\u65e0\u7f1d\u6df7\u5408\u3002", "result": "\u8be5\u65b9\u6cd5\u652f\u6301\u591a\u6837\u5316\u7684\u573a\u666f\u5e03\u5c40\u3001\u9ad8\u6548\u7684\u751f\u6210\u548c\u7075\u6d3b\u7684\u7f16\u8f91\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u901a\u7528\u3001\u8bed\u8a00\u9a71\u52a8\u76843D\u573a\u666f\u6784\u5efa\u5960\u5b9a\u4e86\u7b80\u5355\u800c\u5f3a\u5927\u7684\u57fa\u7840\u3002"}}
{"id": "2510.23626", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23626", "abs": "https://arxiv.org/abs/2510.23626", "authors": ["Shuang Geng", "Wenli Zhang", "Jiaheng Xie", "Rui Wang", "Sudha Ram"], "title": "From Detection to Discovery: A Closed-Loop Approach for Simultaneous and Continuous Medical Knowledge Expansion and Depression Detection on Social Media", "comment": "Presented at SWAIB2025 and HICSS2026", "summary": "Social media user-generated content (UGC) provides real-time, self-reported\nindicators of mental health conditions such as depression, offering a valuable\nsource for predictive analytics. While prior studies integrate medical\nknowledge to improve prediction accuracy, they overlook the opportunity to\nsimultaneously expand such knowledge through predictive processes. We develop a\nClosed-Loop Large Language Model (LLM)-Knowledge Graph framework that\nintegrates prediction and knowledge expansion in an iterative learning cycle.\nIn the knowledge-aware depression detection phase, the LLM jointly performs\ndepression detection and entity extraction, while the knowledge graph\nrepresents and weights these entities to refine prediction performance. In the\nknowledge refinement and expansion phase, new entities, relationships, and\nentity types extracted by the LLM are incorporated into the knowledge graph\nunder expert supervision, enabling continual knowledge evolution. Using\nlarge-scale UGC, the framework enhances both predictive accuracy and medical\nunderstanding. Expert evaluations confirmed the discovery of clinically\nmeaningful symptoms, comorbidities, and social triggers complementary to\nexisting literature. We conceptualize and operationalize\nprediction-through-learning and learning-through-prediction as mutually\nreinforcing processes, advancing both methodological and theoretical\nunderstanding in predictive analytics. The framework demonstrates the\nco-evolution of computational models and domain knowledge, offering a\nfoundation for adaptive, data-driven knowledge systems applicable to other\ndynamic risk monitoring contexts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u95ed\u73af\u5927\u8bed\u8a00\u6a21\u578b-\u77e5\u8bc6\u56fe\u8c31\u6846\u67b6\uff0c\u7528\u4e8e\u6291\u90c1\u75c7\u68c0\u6d4b\uff0c\u901a\u8fc7\u8fed\u4ee3\u5b66\u4e60\u5faa\u73af\u6574\u5408\u9884\u6d4b\u548c\u77e5\u8bc6\u6269\u5c55\u3002", "motivation": "\u5229\u7528\u793e\u4ea4\u5a92\u4f53\u7528\u6237\u751f\u6210\u5185\u5bb9\u9884\u6d4b\u5fc3\u7406\u5065\u5eb7\u72b6\u51b5\uff0c\u5e76\u6269\u5c55\u533b\u5b66\u77e5\u8bc6\u3002", "method": "\u6784\u5efa\u95ed\u73af\u6846\u67b6\uff0cLLM\u8fdb\u884c\u6291\u90c1\u75c7\u68c0\u6d4b\u548c\u5b9e\u4f53\u62bd\u53d6\uff0c\u77e5\u8bc6\u56fe\u8c31\u8868\u793a\u548c\u52a0\u6743\u5b9e\u4f53\uff0c\u4e13\u5bb6\u76d1\u7763\u4e0b\u5c06\u65b0\u77e5\u8bc6\u52a0\u5165\u77e5\u8bc6\u56fe\u8c31\u3002", "result": "\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u53d1\u73b0\u4e86\u4e34\u5e8a\u4e0a\u6709\u610f\u4e49\u7684\u75c7\u72b6\u3001\u5408\u5e76\u75c7\u548c\u793e\u4f1a\u8bf1\u56e0\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u9884\u6d4b\u5f0f\u5b66\u4e60\u548c\u5b66\u4e60\u5f0f\u9884\u6d4b\u7684\u76f8\u4e92\u4fc3\u8fdb\uff0c\u4e3a\u81ea\u9002\u5e94\u6570\u636e\u9a71\u52a8\u77e5\u8bc6\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.23853", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23853", "abs": "https://arxiv.org/abs/2510.23853", "authors": ["Yize Cheng", "Arshia Soltani Moakhar", "Chenrui Fan", "Kazem Faghih", "Parsa Hosseini", "Wenxiao Wang", "Soheil Feizi"], "title": "Temporal Blindness in Multi-Turn LLM Agents: Misaligned Tool Use vs. Human Time Perception", "comment": "preliminary work in progress", "summary": "Large language model agents are increasingly used in multi-turn\nconversational settings to interact with and execute tasks in dynamic\nenvironments. However, a key limitation is their temporal blindness: they, by\ndefault, operate with a stationary context, failing to account for the\nreal-world time elapsed between messages. This becomes a critical liability\nwhen an agent must decide whether to invoke a tool based on how much time has\npassed since the last observation. Without temporal awareness, agents often\neither over-rely on previous context (skipping necessary tool calls), or\nunder-rely on it (unnecessarily repeating tool calls). To study this challenge,\nwe introduce TicToc-v1, a test set of multi-turn user-agent trajectories across\n34 scenarios with varying time sensitivity. Each trajectory ends with a user\nquestion, where the need for a tool call depends on the amount of time elapsed\nsince the last message. To give LLMs temporal context, we augment dialogue\nmessages with explicit timestamps, bridging the gap between static dialogue and\nevolving environments. We then collected human preferences for these samples,\ncreating two subsets: one where humans preferred relying on the previous\nobservation (prefer-noTool), and another where they preferred a new tool call\n(prefer-Tool). We evaluated how well LLM tool-calling decisions align with\nhuman preferences under varying time intervals on TicToc-v1. Our analysis show\nthat without time information, most models perform only slightly better than\nrandom, with the top alignment rate being just over 60%. While adding\ntimestamps leads to a slight improvement, particularly for larger models, the\nimprovement is modest, peaking at around 65%. We also show that naive,\nprompt-based alignment have limited effectiveness. Our findings highlight the\nneed for specific post-training alignment to align multi-turn LLM tool use with\nhuman temporal perception.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u4e0e\u52a8\u6001\u73af\u5883\u4ea4\u4e92\u65f6\u5b58\u5728\u65f6\u95f4\u76f2\u533a\uff0c\u65e0\u6cd5\u611f\u77e5\u6d88\u606f\u4e4b\u95f4\u7684\u65f6\u95f4\u95f4\u9694\uff0c\u5bfc\u81f4\u5de5\u5177\u8c03\u7528\u4e0d\u51c6\u786e\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578bAgent\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u4e0e\u52a8\u6001\u73af\u5883\u4ea4\u4e92\u65f6\uff0c\u7531\u4e8e\u7f3a\u4e4f\u65f6\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u65e0\u6cd5\u6839\u636e\u65f6\u95f4\u95f4\u9694\u51b3\u5b9a\u662f\u5426\u8c03\u7528\u5de5\u5177\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u6216\u4e0d\u8db3\u5730\u4f9d\u8d56\u5148\u524d\u4fe1\u606f\u3002", "method": "1. \u63d0\u51fa\u4e86TicToc-v1\u6d4b\u8bd5\u96c6\uff0c\u5305\u542b34\u4e2a\u5177\u6709\u4e0d\u540c\u65f6\u95f4\u654f\u611f\u5ea6\u7684\u573a\u666f\u30022. \u901a\u8fc7\u5728\u5bf9\u8bdd\u6d88\u606f\u4e2d\u6dfb\u52a0\u65f6\u95f4\u6233\uff0c\u4e3aLLM\u63d0\u4f9b\u65f6\u95f4\u80cc\u666f\u4fe1\u606f\u30023. \u6536\u96c6\u4eba\u7c7b\u504f\u597d\u6570\u636e\uff0c\u521b\u5efa\u4e24\u4e2a\u5b50\u96c6\uff1aprefer-noTool\u548cprefer-Tool\u3002", "result": "\u5728\u6ca1\u6709\u65f6\u95f4\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u5927\u591a\u6570\u6a21\u578b\u7684\u6027\u80fd\u7565\u9ad8\u4e8e\u968f\u673a\u6c34\u5e73\uff0860%\uff09\uff0c\u6dfb\u52a0\u65f6\u95f4\u6233\u540e\u7565\u6709\u6539\u5584\uff0865%\uff09\uff0c\u4f46\u63d0\u5347\u6709\u9650\u3002", "conclusion": "\u9700\u8981\u8fdb\u884c\u4e13\u95e8\u7684\u540e\u8bad\u7ec3\u5bf9\u9f50\uff0c\u4ee5\u4f7f\u591a\u8f6eLLM\u5de5\u5177\u7684\u4f7f\u7528\u4e0e\u4eba\u7c7b\u7684\u65f6\u95f4\u611f\u77e5\u5bf9\u9f50\u3002"}}
{"id": "2510.23807", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23807", "abs": "https://arxiv.org/abs/2510.23807", "authors": ["Hamid R. Tizhoosh"], "title": "Why Foundation Models in Pathology Are Failing", "comment": null, "summary": "In non-medical domains, foundation models (FMs) have revolutionized computer\nvision and language processing through large-scale self-supervised and\nmultimodal learning. Consequently, their rapid adoption in computational\npathology was expected to deliver comparable breakthroughs in cancer diagnosis,\nprognostication, and multimodal retrieval. However, recent systematic\nevaluations reveal fundamental weaknesses: low diagnostic accuracy, poor\nrobustness, geometric instability, heavy computational demands, and concerning\nsafety vulnerabilities. This short paper examines these shortcomings and argues\nthat they stem from deeper conceptual mismatches between the assumptions\nunderlying generic foundation modeling in mainstream AI and the intrinsic\ncomplexity of human tissue. Seven interrelated causes are identified:\nbiological complexity, ineffective self-supervision, overgeneralization,\nexcessive architectural complexity, lack of domain-specific innovation,\ninsufficient data, and a fundamental design flaw related to tissue patch size.\nThese findings suggest that current pathology foundation models remain\nconceptually misaligned with the nature of tissue morphology and call for a\nfundamental rethinking of the paradigm itself.", "AI": {"tldr": "\u75c5\u7406\u5b66\u4e2d\u7684\u901a\u7528\u57fa\u7840\u6a21\u578b\u672a\u80fd\u8fbe\u5230\u5728\u764c\u75c7\u8bca\u65ad\u7b49\u65b9\u9762\u7684\u9884\u671f\u7a81\u7834\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e0e\u4eba\u7c7b\u7ec4\u7ec7\u7684\u590d\u6742\u6027\u5b58\u5728\u6839\u672c\u7684\u4e0d\u5339\u914d\u3002", "motivation": "\u8bc4\u4f30\u8868\u660e\uff0c\u901a\u7528\u57fa\u7840\u6a21\u578b\u5728\u75c5\u7406\u5b66\u4e2d\u5b58\u5728\u8bca\u65ad\u51c6\u786e\u7387\u4f4e\u3001\u9c81\u68d2\u6027\u5dee\u7b49\u5f31\u70b9\u3002", "method": "\u5206\u6790\u4e86\u901a\u7528\u57fa\u7840\u6a21\u578b\u5728\u75c5\u7406\u5b66\u5e94\u7528\u4e2d\u7684\u4e0d\u8db3\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u6839\u672c\u539f\u56e0\u3002", "result": "\u8bc6\u522b\u4e86\u4e03\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u539f\u56e0\uff0c\u5305\u62ec\u751f\u7269\u590d\u6742\u6027\u3001\u65e0\u6548\u7684\u81ea\u76d1\u7763\u548c\u8fc7\u5ea6\u6cdb\u5316\u7b49\u3002", "conclusion": "\u5f53\u524d\u7684\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u6982\u5ff5\u4e0a\u4e0e\u7ec4\u7ec7\u5f62\u6001\u7684\u672c\u8d28\u4e0d\u7b26\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u8fd9\u4e00\u8303\u4f8b\u3002"}}
{"id": "2510.23894", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23894", "abs": "https://arxiv.org/abs/2510.23894", "authors": ["Jinxin Zhou", "Jiachen Jiang", "Zhihui Zhu"], "title": "Improving Visual Discriminability of CLIP for Training-Free Open-Vocabulary Semantic Segmentation", "comment": "23 pages, 10 figures, 14 tables", "summary": "Extending CLIP models to semantic segmentation remains challenging due to the\nmisalignment between their image-level pre-training objectives and the\npixel-level visual understanding required for dense prediction. While prior\nefforts have achieved encouraging results by reorganizing the final layer and\nfeatures, they often inherit the global alignment bias of preceding layers,\nleading to suboptimal segmentation performance. In this work, we propose\nLHT-CLIP, a novel training-free framework that systematically exploits the\nvisual discriminability of CLIP across layer, head, and token levels. Through\ncomprehensive analysis, we reveal three key insights: (i) the final layers\nprimarily strengthen image-text alignment with sacrifice of visual\ndiscriminability (e.g., last 3 layers in ViT-B/16 and 8 layers in ViT-L/14),\npartly due to the emergence of anomalous tokens; (ii) a subset of attention\nheads (e.g., 10 out of 144 in ViT-B/16) display consistently strong visual\ndiscriminability across datasets; (iii) abnormal tokens display sparse and\nconsistent activation pattern compared to normal tokens. Based on these\nfindings, we propose three complementary techniques: semantic-spatial\nreweighting, selective head enhancement, and abnormal token replacement to\neffectively restore visual discriminability and improve segmentation\nperformance without any additional training, auxiliary pre-trained networks, or\nextensive hyperparameter tuning. Extensive experiments on 8 common semantic\nsegmentation benchmarks demonstrate that LHT-CLIP achieves state-of-the-art\nperformance across diverse scenarios, highlighting its effectiveness and\npracticality for real-world deployment.", "AI": {"tldr": "LHT-CLIP is a training-free framework that improves CLIP models for semantic segmentation by exploiting visual discriminability across different levels.", "motivation": "Extending CLIP models to semantic segmentation is difficult due to the misalignment between image-level pre-training and pixel-level understanding. Existing methods often inherit global alignment bias, leading to suboptimal performance.", "method": "The authors propose LHT-CLIP, which includes semantic-spatial reweighting, selective head enhancement, and abnormal token replacement to restore visual discriminability.", "result": "LHT-CLIP achieves state-of-the-art performance on 8 semantic segmentation benchmarks.", "conclusion": "LHT-CLIP effectively improves segmentation performance without additional training, auxiliary networks, or extensive hyperparameter tuning, making it practical for real-world deployment."}}
{"id": "2510.23629", "categories": ["cs.LG", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.23629", "abs": "https://arxiv.org/abs/2510.23629", "authors": ["Nuo Chen", "Zehua Li", "Keqin Bao", "Junyang Lin", "Dayiheng Liu"], "title": "Chain of Execution Supervision Promotes General Reasoning in Large Language Models", "comment": null, "summary": "Building robust and general reasoning ability is a central goal in the\ndevelopment of large language models (LLMs). Recent efforts increasingly turn\nto code as a rich training source, given its inherent logical structure and\ndiverse reasoning paradigms such as divide-and-conquer, topological ordering,\nand enumeration. However, reasoning in code is often expressed implicitly and\nentangled with syntactic or implementation noise, making direct training on raw\ncode suboptimal.To address this, we introduce TracePile, a large-scale corpus\nof 2.6 million samples that transforms code execution into explicit,\nstep-by-step chain-of-thought-style rationales, which we call Chain of\nExecution (CoE). The corpus spans domains including mathematics, classical\nalgorithms and algorithmic competition, and is enriched with variable-tracing\nquestions and code rewritings to enhance logical granularity and code\ndiversity. We evaluate TracePile using three training setups:\ncontinue-pretraining, instruction tuning after pretraining, and two-stage\nfinetuning. Experiments across four base models (LLaMA 3, LLaMA 3.1, Qwen-2.5,\nand Qwen-2.5 Coder) and 20 benchmarks covering math, code, logic, and\nalgorithms demonstrate consistent improvements. Notably, TracePile boosts\nLLaMA3.1-8B by 7.1\\% on average across nine math datasets and delivers clear\ngains on LiveCodeBench, CRUX, and MMLU under two-stage fine-tuning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86TracePile\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u4ee3\u7801\u6267\u884c\u8f68\u8ff9\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u4ee3\u7801\u8bad\u7ec3\u6570\u636e\u5305\u542b\u9690\u5f0f\u63a8\u7406\uff0c\u5e76\u4e14\u53d7\u5230\u8bed\u6cd5\u548c\u5b9e\u73b0\u566a\u58f0\u7684\u5e72\u6270\uff0c\u76f4\u63a5\u8bad\u7ec3\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u8bba\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b260\u4e07\u6837\u672c\u7684TracePile\u6570\u636e\u96c6\uff0c\u5c06\u4ee3\u7801\u6267\u884c\u8f6c\u5316\u4e3a\u663e\u5f0f\u7684\u3001\u9010\u6b65\u7684\u601d\u7ef4\u94fe\u5f0f\u63a8\u7406\uff08Chain of Execution\uff0cCoE\uff09\u3002\u6570\u636e\u96c6\u6db5\u76d6\u6570\u5b66\u3001\u7ecf\u5178\u7b97\u6cd5\u548c\u7b97\u6cd5\u7ade\u8d5b\u7b49\u9886\u57df\uff0c\u5e76\u901a\u8fc7\u53d8\u91cf\u8ffd\u8e2a\u95ee\u9898\u548c\u4ee3\u7801\u91cd\u5199\u6765\u589e\u5f3a\u903b\u8f91\u7c92\u5ea6\u548c\u4ee3\u7801\u591a\u6837\u6027\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u7840\u6a21\u578b\uff08LLaMA 3, LLaMA 3.1, Qwen-2.5, and Qwen-2.5 Coder\uff09\u548c20\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u6db5\u76d6\u6570\u5b66\u3001\u4ee3\u7801\u3001\u903b\u8f91\u548c\u7b97\u6cd5\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTracePile\u5e26\u6765\u4e86\u6301\u7eed\u7684\u6539\u8fdb\u3002\u4f8b\u5982\uff0c\u5728\u4e5d\u4e2a\u6570\u5b66\u6570\u636e\u96c6\u4e0a\uff0cTracePile\u4f7fLLaMA3.1-8B\u7684\u6027\u80fd\u5e73\u5747\u63d0\u9ad8\u4e867.1%\u3002", "conclusion": "TracePile\u80fd\u591f\u6709\u6548\u63d0\u5347LLMs\u5728\u6570\u5b66\u3001\u4ee3\u7801\u3001\u903b\u8f91\u548c\u7b97\u6cd5\u7b49\u65b9\u9762\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2510.23854", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23854", "abs": "https://arxiv.org/abs/2510.23854", "authors": ["Jyotika Singh", "Weiyi Sun", "Amit Agarwal", "Viji Krishnamurthy", "Yassine Benajiba", "Sujith Ravi", "Dan Roth"], "title": "Can LLMs Narrate Tabular Data? An Evaluation Framework for Natural Language Representations of Text-to-SQL System Outputs", "comment": "Accepted at EMNLP 2025", "summary": "In modern industry systems like multi-turn chat agents, Text-to-SQL\ntechnology bridges natural language (NL) questions and database (DB) querying.\nThe conversion of tabular DB results into NL representations (NLRs) enables the\nchat-based interaction. Currently, NLR generation is typically handled by large\nlanguage models (LLMs), but information loss or errors in presenting tabular\nresults in NL remains largely unexplored. This paper introduces a novel\nevaluation method - Combo-Eval - for judgment of LLM-generated NLRs that\ncombines the benefits of multiple existing methods, optimizing evaluation\nfidelity and achieving a significant reduction in LLM calls by 25-61%.\nAccompanying our method is NLR-BIRD, the first dedicated dataset for NLR\nbenchmarking. Through human evaluations, we demonstrate the superior alignment\nof Combo-Eval with human judgments, applicable across scenarios with and\nwithout ground truth references.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5 Combo-Eval\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u81ea\u7136\u8bed\u8a00\u8868\u793a (NLR)\uff0c\u5e76\u6784\u5efa\u4e86\u9996\u4e2a\u4e13\u7528\u6570\u636e\u96c6 NLR-BIRD\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5c06\u8868\u683c\u6570\u636e\u5e93\u7ed3\u679c\u8f6c\u6362\u4e3a\u81ea\u7136\u8bed\u8a00\u8868\u793a (NLR)\uff0c\u4f46\u8868\u683c\u7ed3\u679c\u5728\u81ea\u7136\u8bed\u8a00\u8868\u793a\u4e2d\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u6216\u9519\u8bef\u7684\u95ee\u9898\uff0c\u8fd9\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u5c1a\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u7ed3\u5408\u4e86\u591a\u79cd\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u4f18\u5316\u8bc4\u4f30\u4fdd\u771f\u5ea6\uff0c\u5e76\u663e\u8457\u51cf\u5c11 LLM \u8c03\u7528\u6b21\u6570\uff0825-61%\uff09\u3002", "result": "\u901a\u8fc7\u4eba\u5de5\u8bc4\u4f30\u8868\u660e\uff0cCombo-Eval \u4e0e\u4eba\u5de5\u5224\u65ad\u5177\u6709\u66f4\u597d\u7684\u4e00\u81f4\u6027\uff0c\u9002\u7528\u4e8e\u6709\u548c\u6ca1\u6709ground truth\u53c2\u8003\u7684\u573a\u666f\u3002", "conclusion": "Combo-Eval \u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u7528\u6765\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u81ea\u7136\u8bed\u8a00\u8868\u793a\u3002"}}
{"id": "2510.23822", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23822", "abs": "https://arxiv.org/abs/2510.23822", "authors": ["Zhenyu Zhang", "Tianyi Chen", "Weiran Xu", "Alex Pentland", "Jiaxin Pei"], "title": "ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents", "comment": null, "summary": "Long-horizon tasks requiring multi-step reasoning and dynamic re-planning\nremain challenging for large language models (LLMs). Sequential prompting\nmethods are prone to context drift, loss of goal information, and recurrent\nfailure cycles, while hierarchical prompting methods often weaken cross-level\ncontinuity or incur substantial runtime overhead. We introduce ReCAP (Recursive\nContext-Aware Reasoning and Planning), a hierarchical framework with shared\ncontext for reasoning and planning in LLMs. ReCAP combines three key\nmechanisms: (i) plan-ahead decomposition, in which the model generates a full\nsubtask list, executes the first item, and refines the remainder; (ii)\nstructured re-injection of parent plans, maintaining consistent multi-level\ncontext during recursive return; and (iii) memory-efficient execution, bounding\nthe active prompt so costs scale linearly with task depth. Together these\nmechanisms align high-level goals with low-level actions, reduce redundant\nprompting, and preserve coherent context updates across recursion. Experiments\ndemonstrate that ReCAP substantially improves subgoal alignment and success\nrates on various long-horizon reasoning benchmarks, achieving a 32% gain on\nsynchronous Robotouille and a 29% improvement on asynchronous Robotouille under\nthe strict pass@1 protocol.", "AI": {"tldr": "ReCAP: A hierarchical framework with shared context for reasoning and planning in LLMs, improving subgoal alignment and success rates on long-horizon reasoning benchmarks.", "motivation": "Long-horizon tasks requiring multi-step reasoning and dynamic re-planning remain challenging for large language models (LLMs). Sequential prompting methods are prone to context drift, loss of goal information, and recurrent failure cycles, while hierarchical prompting methods often weaken cross-level continuity or incur substantial runtime overhead.", "method": "ReCAP combines three key mechanisms: (i) plan-ahead decomposition, in which the model generates a full subtask list, executes the first item, and refines the remainder; (ii) structured re-injection of parent plans, maintaining consistent multi-level context during recursive return; and (iii) memory-efficient execution, bounding the active prompt so costs scale linearly with task depth.", "result": "ReCAP substantially improves subgoal alignment and success rates on various long-horizon reasoning benchmarks, achieving a 32% gain on synchronous Robotouille and a 29% improvement on asynchronous Robotouille under the strict pass@1 protocol.", "conclusion": "ReCAP aligns high-level goals with low-level actions, reduces redundant prompting, and preserve coherent context updates across recursion."}}
{"id": "2510.23907", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23907", "abs": "https://arxiv.org/abs/2510.23907", "authors": ["Eddison Pham", "Prisha Priyadarshini", "Adrian Maliackel", "Kanishk Bandi", "Cristian Meo", "Kevin Zhu"], "title": "DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning", "comment": "16 pages, 15 figures, 5 Tables, submitted to AAAI AI4ED Workshop 2026", "summary": "Scene-level captioning in instructional videos can enhance learning by\nrequiring an understanding of both visual cues and temporal structure. By\naligning visual cues with textual guidance, this understanding supports\nprocedural learning and multimodal reasoning, providing a richer context for\nskill acquisition. However, captions that fail to capture this structure may\nlack coherence and quality, which can create confusion and undermine the\nvideo's educational intent. To address this gap, we introduce DynaStride, a\npipeline to generate coherent, scene-level captions without requiring manual\nscene segmentation. Using the YouCookII dataset's scene annotations, DynaStride\nperforms adaptive frame sampling and multimodal windowing to capture key\ntransitions within each scene. It then employs a multimodal chain-of-thought\nprocess to produce multiple action-object pairs, which are refined and fused\nusing a dynamic stride window selection algorithm that adaptively balances\ntemporal context and redundancy. The final scene-level caption integrates\nvisual semantics and temporal reasoning in a single instructional caption.\nEmpirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,\ndemonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and\nsemantic similarity measures (BERTScore, CLIPScore). Qualitative analyses\nfurther show that DynaStride produces captions that are more temporally\ncoherent and informative, suggesting a promising direction for improving\nAI-powered instructional content generation.", "AI": {"tldr": "DynaStride\u662f\u4e00\u79cd\u751f\u6210\u8fde\u8d2f\u573a\u666f\u7ea7\u522b\u5b57\u5e55\u7684\u6d41\u7a0b\uff0c\u65e0\u9700\u624b\u52a8\u573a\u666f\u5206\u5272\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5e27\u91c7\u6837\u548c\u591a\u6a21\u6001\u7a97\u53e3\u5904\u7406\u6765\u6355\u6349\u5173\u952e\u8f6c\u573a\uff0c\u5e76\u4f7f\u7528\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u8fc7\u7a0b\u751f\u6210\u52a8\u4f5c-\u5bf9\u8c61\u5bf9\uff0c\u6700\u540e\u901a\u8fc7\u52a8\u6001\u6b65\u5e45\u7a97\u53e3\u9009\u62e9\u7b97\u6cd5\u8fdb\u884c\u4f18\u5316\u548c\u878d\u5408\u3002", "motivation": "\u73b0\u6709\u7684\u5b57\u5e55\u53ef\u80fd\u7f3a\u4e4f\u8fde\u8d2f\u6027\u548c\u8d28\u91cf\uff0c\u4ece\u800c\u524a\u5f31\u89c6\u9891\u7684\u6559\u80b2\u610f\u56fe\u3002", "method": "DynaStride\u6d41\u7a0b\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u5e27\u91c7\u6837\u548c\u591a\u6a21\u6001\u7a97\u53e3\u5904\u7406\uff0c\u5e76\u7ed3\u5408\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u548c\u52a8\u6001\u6b65\u5e45\u7a97\u53e3\u9009\u62e9\u7b97\u6cd5\u3002", "result": "\u5728N-gram\u6307\u6807\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u6307\u6807\u4e0a\u5747\u4f18\u4e8eVLLaMA3\u548cGPT-4o\u7b49\u57fa\u7ebf\u6a21\u578b\u3002\u5b9a\u6027\u5206\u6790\u8868\u660e\uff0cDynaStride\u751f\u6210\u7684\u5b57\u5e55\u5728\u65f6\u95f4\u4e0a\u66f4\u8fde\u8d2f\u548c\u4fe1\u606f\u66f4\u4e30\u5bcc\u3002", "conclusion": "DynaStride\u4e3a\u6539\u8fdb\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u6559\u5b66\u5185\u5bb9\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002"}}
{"id": "2510.23630", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23630", "abs": "https://arxiv.org/abs/2510.23630", "authors": ["Ninghui Feng", "Yiyan Qi"], "title": "NUM2EVENT: Interpretable Event Reasoning from Numerical time-series", "comment": null, "summary": "Large language models (LLMs) have recently demonstrated impressive multimodal\nreasoning capabilities, yet their understanding of purely numerical time-series\nsignals remains limited. Existing approaches mainly focus on forecasting or\ntrend description, without uncovering the latent events that drive numerical\nchanges or explaining the reasoning process behind them. In this work, we\nintroduce the task of number-to-event reasoning and decoding, which aims to\ninfer interpretable structured events from numerical inputs, even when current\ntext is unavailable. To address the data scarcity and semantic alignment\nchallenges, we propose a reasoning-aware framework that integrates an\nagent-guided event extractor (AGE), a marked multivariate Hawkes-based\nsynthetic generator (EveDTS), and a two-stage fine-tuning pipeline combining a\ntime-series encoder with a structured decoder. Our model explicitly reasons\nover numerical changes, generates intermediate explanations, and outputs\nstructured event hypotheses. Experiments on multi-domain datasets show that our\nmethod substantially outperforms strong LLM baselines in event-level precision\nand recall. These results suggest a new direction for bridging quantitative\nreasoning and semantic understanding, enabling LLMs to explain and predict\nevents directly from numerical dynamics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u503c\u5230\u4e8b\u4ef6\u7684\u63a8\u7406\u548c\u89e3\u7801\u4efb\u52a1\uff0c\u65e8\u5728\u4ece\u6570\u503c\u8f93\u5165\u4e2d\u63a8\u65ad\u51fa\u53ef\u89e3\u91ca\u7684\u7ed3\u6784\u5316\u4e8b\u4ef6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u9884\u6d4b\u6216\u8d8b\u52bf\u63cf\u8ff0\uff0c\u800c\u6ca1\u6709\u63ed\u793a\u9a71\u52a8\u6570\u503c\u53d8\u5316\u7684\u6f5c\u5728\u4e8b\u4ef6\u6216\u89e3\u91ca\u5176\u80cc\u540e\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u8be5\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a reasoning-aware \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86 agent-guided event extractor (AGE)\u3001marked multivariate Hawkes-based synthetic generator (EveDTS) \u548c\u4e00\u4e2a\u4e24\u9636\u6bb5\u5fae\u8c03 pipeline\uff0c\u8be5 pipeline \u5c06\u65f6\u95f4\u5e8f\u5217\u7f16\u7801\u5668\u4e0e\u7ed3\u6784\u5316\u89e3\u7801\u5668\u76f8\u7ed3\u5408\u3002", "result": "\u5728\u591a\u9886\u57df\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e8b\u4ef6\u7ea7\u522b\u7684\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u5f3a\u5927\u7684 LLM \u57fa\u7ebf\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u5f25\u5408\u5b9a\u91cf\u63a8\u7406\u548c\u8bed\u4e49\u7406\u89e3\u4e4b\u95f4\u7684\u5dee\u8ddd\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u65b9\u5411\uff0c\u4f7f LLM \u80fd\u591f\u76f4\u63a5\u4ece\u6570\u503c\u52a8\u6001\u4e2d\u89e3\u91ca\u548c\u9884\u6d4b\u4e8b\u4ef6\u3002"}}
{"id": "2510.23870", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23870", "abs": "https://arxiv.org/abs/2510.23870", "authors": ["Marianne Menglin Liu", "Sai Ashish Somayajula", "Syed Fahad Allam Shah", "Sujith Ravi", "Dan Roth"], "title": "OraPlan-SQL: A Planning-Centric Framework for Complex Bilingual NL2SQL Reasoning", "comment": null, "summary": "We present OraPlan-SQL, our system for the Archer NL2SQL Evaluation Challenge\n2025, a bilingual benchmark requiring complex reasoning such as arithmetic,\ncommonsense, and hypothetical inference. OraPlan-SQL ranked first, exceeding\nthe second-best system by more than 6% in execution accuracy (EX), with 55.0%\nin English and 56.7% in Chinese, while maintaining over 99% SQL validity (VA).\nOur system follows an agentic framework with two components: Planner agent that\ngenerates stepwise natural language plans, and SQL agent that converts these\nplans into executable SQL. Since SQL agent reliably adheres to the plan, our\nrefinements focus on the planner. Unlike prior methods that rely on multiple\nsub-agents for planning and suffer from orchestration overhead, we introduce a\nfeedback-guided meta-prompting strategy to refine a single planner. Failure\ncases from a held-out set are clustered with human input, and an LLM distills\nthem into corrective guidelines that are integrated into the planner's system\nprompt, improving generalization without added complexity. For the multilingual\nscenario, to address transliteration and entity mismatch issues, we incorporate\nentity-linking guidelines that generate alternative surface forms for entities\nand explicitly include them in the plan. Finally, we enhance reliability\nthrough plan diversification: multiple candidate plans are generated for each\nquery, with the SQL agent producing a query for each plan, and final output\nselected via majority voting over their executions.", "AI": {"tldr": "OraPlan-SQL\u5728Archer NL2SQL Evaluation Challenge 2025\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u5728\u6267\u884c\u51c6\u786e\u7387\uff08EX\uff09\u65b9\u9762\u8d85\u8fc7\u7b2c\u4e8c\u540d\u7cfb\u7edf6%\u4ee5\u4e0a\uff0c\u82f1\u8bed\u4e3a55.0%\uff0c\u4e2d\u6587\u4e3a56.7%\uff0c\u540c\u65f6\u4fdd\u6301\u8d85\u8fc799%\u7684SQL\u6709\u6548\u6027\uff08VA\uff09\u3002", "motivation": "\u8be5\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u590d\u6742\u63a8\u7406\uff08\u5982\u7b97\u672f\u3001\u5e38\u8bc6\u548c\u5047\u8bbe\u63a8\u7406\uff09\u7684NL2SQL\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u53cc\u8bed\u73af\u5883\u4e2d\u9762\u4e34\u7684\u6311\u6218\uff0c\u5305\u62ec\u97f3\u8bd1\u548c\u5b9e\u4f53\u4e0d\u5339\u914d\u95ee\u9898\u3002", "method": "\u8be5\u7cfb\u7edf\u91c7\u7528agentic\u6846\u67b6\uff0c\u5305\u542bPlanner agent\u548cSQL agent\u3002\u901a\u8fc7\u53cd\u9988\u5f15\u5bfc\u7684meta-prompting\u7b56\u7565\u4f18\u5316\u5355\u4e2aplanner\uff0c\u5229\u7528\u4eba\u5de5\u8f93\u5165\u7684failure case\u805a\u7c7b\uff0c\u63d0\u70bc\u51fa\u4fee\u6b63\u6307\u5357\u5e76\u6574\u5408\u5230planner\u7684\u7cfb\u7edf\u63d0\u793a\u4e2d\u3002\u540c\u65f6\uff0c\u52a0\u5165\u5b9e\u4f53\u94fe\u63a5\u6307\u5357\uff0c\u751f\u6210\u5b9e\u4f53\u7684\u66ff\u4ee3\u5f62\u5f0f\uff0c\u5e76\u663e\u5f0f\u5305\u542b\u5728\u8ba1\u5212\u4e2d\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u8ba1\u5212\u591a\u6837\u5316\u6765\u589e\u5f3a\u53ef\u9760\u6027\u3002", "result": "OraPlan-SQL\u5728Archer NL2SQL Evaluation Challenge 2025\u4e2d\uff0c\u82f1\u8bed\u6267\u884c\u51c6\u786e\u7387\u4e3a55.0%\uff0c\u4e2d\u6587\u4e3a56.7%\uff0cSQL\u6709\u6548\u6027\u8d85\u8fc799%\uff0c\u8d85\u8fc7\u7b2c\u4e8c\u540d\u7cfb\u7edf6%\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684NL2SQL\u7cfb\u7edfOraPlan-SQL\uff0c\u901a\u8fc7\u5355planner\u7684\u4f18\u5316\u548c\u8ba1\u5212\u591a\u6837\u5316\uff0c\u5728\u590d\u6742\u63a8\u7406\u548c\u53cc\u8bed\u73af\u5883\u4e0b\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002"}}
{"id": "2510.23824", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23824", "abs": "https://arxiv.org/abs/2510.23824", "authors": ["Murad Ismayilov", "Edwin Meriaux", "Shuo Wen", "Gregory Dudek"], "title": "Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models", "comment": "Accepted at MIT URTC 2025", "summary": "Coordinating multiple autonomous agents in shared environments under\ndecentralized conditions is a long-standing challenge in robotics and\nartificial intelligence. This work addresses the problem of decentralized goal\nassignment for multi-agent path planning, where agents independently generate\nranked preferences over goals based on structured representations of the\nenvironment, including grid visualizations and scenario data. After this\nreasoning phase, agents exchange their goal rankings, and assignments are\ndetermined by a fixed, deterministic conflict-resolution rule (e.g., agent\nindex ordering), without negotiation or iterative coordination. We\nsystematically compare greedy heuristics, optimal assignment, and large\nlanguage model (LLM)-based agents in fully observable grid-world settings. Our\nresults show that LLM-based agents, when provided with well-designed prompts\nand relevant quantitative information, can achieve near-optimal makespans and\nconsistently outperform traditional heuristics. These findings underscore the\npotential of language models for decentralized goal assignment in multi-agent\npath planning and highlight the importance of information structure in such\nsystems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u53bb\u4e2d\u5fc3\u5316\u76ee\u6807\u5206\u914d\u95ee\u9898\uff0c\u5229\u7528LLM\u5728\u7f51\u683c\u4e16\u754c\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684makespan\u3002", "motivation": "\u5728\u53bb\u4e2d\u5fc3\u5316\u6761\u4ef6\u4e0b\uff0c\u534f\u8c03\u5171\u4eab\u73af\u5883\u4e2d\u7684\u591a\u4e2a\u81ea\u4e3b\u667a\u80fd\u4f53\u662f\u4e00\u4e2a\u957f\u671f\u5b58\u5728\u7684\u6311\u6218\u3002\u672c\u6587\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u7684\u53bb\u4e2d\u5fc3\u5316\u76ee\u6807\u5206\u914d\u95ee\u9898\u3002", "method": "\u667a\u80fd\u4f53\u57fa\u4e8e\u73af\u5883\u7684\u7ed3\u6784\u5316\u8868\u793a\uff08\u5305\u62ec\u7f51\u683c\u53ef\u89c6\u5316\u548c\u573a\u666f\u6570\u636e\uff09\u72ec\u7acb\u751f\u6210\u6392\u5e8f\u540e\u7684\u76ee\u6807\u504f\u597d\u3002\u7136\u540e\uff0c\u667a\u80fd\u4f53\u4ea4\u6362\u5176\u76ee\u6807\u6392\u540d\uff0c\u5e76\u901a\u8fc7\u56fa\u5b9a\u7684\u51b2\u7a81\u89e3\u51b3\u89c4\u5219\u6765\u786e\u5b9a\u5206\u914d\u3002", "result": "LLM\u667a\u80fd\u4f53\u5728\u5145\u5206\u89c2\u5bdf\u7684\u7f51\u683c\u4e16\u754c\u73af\u5883\u4e2d\uff0c\u5728\u7ecf\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u548c\u76f8\u5173\u7684\u5b9a\u91cf\u4fe1\u606f\u540e\uff0c\u53ef\u4ee5\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684makespan\uff0c\u5e76\u4e14\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edf\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u4e2d\u53bb\u4e2d\u5fc3\u5316\u76ee\u6807\u5206\u914d\u7684\u6f5c\u529b\uff0c\u5e76\u7a81\u51fa\u4e86\u4fe1\u606f\u7ed3\u6784\u5728\u6b64\u7c7b\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.23929", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23929", "abs": "https://arxiv.org/abs/2510.23929", "authors": ["Emily Kim", "Julieta Martinez", "Timur Bagautdinov", "Jessica Hodgins"], "title": "TurboPortrait3D: Single-step diffusion-based fast portrait novel-view synthesis", "comment": null, "summary": "We introduce TurboPortrait3D: a method for low-latency novel-view synthesis\nof human portraits. Our approach builds on the observation that existing\nimage-to-3D models for portrait generation, while capable of producing\nrenderable 3D representations, are prone to visual artifacts, often lack of\ndetail, and tend to fail at fully preserving the identity of the subject. On\nthe other hand, image diffusion models excel at generating high-quality images,\nbut besides being computationally expensive, are not grounded in 3D and thus\nare not directly capable of producing multi-view consistent outputs. In this\nwork, we demonstrate that image-space diffusion models can be used to\nsignificantly enhance the quality of existing image-to-avatar methods, while\nmaintaining 3D-awareness and running with low-latency. Our method takes a\nsingle frontal image of a subject as input, and applies a feedforward\nimage-to-avatar generation pipeline to obtain an initial 3D representation and\ncorresponding noisy renders. These noisy renders are then fed to a single-step\ndiffusion model which is conditioned on input image(s), and is specifically\ntrained to refine the renders in a multi-view consistent way. Moreover, we\nintroduce a novel effective training strategy that includes pre-training on a\nlarge corpus of synthetic multi-view data, followed by fine-tuning on\nhigh-quality real images. We demonstrate that our approach both qualitatively\nand quantitatively outperforms current state-of-the-art for portrait novel-view\nsynthesis, while being efficient in time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a TurboPortrait3D \u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f4e\u5ef6\u8fdf\u7684\u4eba\u50cf\u65b0\u89c6\u89d2\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u5230 3D \u7684\u4eba\u50cf\u751f\u6210\u6a21\u578b\u5bb9\u6613\u4ea7\u751f\u89c6\u89c9\u4f2a\u5f71\uff0c\u7f3a\u4e4f\u7ec6\u8282\uff0c\u5e76\u4e14\u65e0\u6cd5\u5b8c\u5168\u4fdd\u7559\u5bf9\u8c61\u7684\u8eab\u4efd\u3002\u56fe\u50cf\u6269\u6563\u6a21\u578b\u64c5\u957f\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5e76\u4e14\u6ca1\u6709\u4ee5 3D \u4e3a\u57fa\u7840\uff0c\u56e0\u6b64\u65e0\u6cd5\u76f4\u63a5\u751f\u6210\u591a\u89c6\u56fe\u4e00\u81f4\u7684\u8f93\u51fa\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u5355\u5f20\u4e3b\u4f53\u6b63\u9762\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u5e94\u7528\u524d\u9988\u56fe\u50cf\u5230\u5934\u50cf\u751f\u6210\u7ba1\u9053\u4ee5\u83b7\u5f97\u521d\u59cb 3D \u8868\u793a\u548c\u76f8\u5e94\u7684\u566a\u58f0\u6e32\u67d3\u3002\u7136\u540e\u5c06\u8fd9\u4e9b\u566a\u58f0\u6e32\u67d3\u8f93\u5165\u5230\u5355\u6b65\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u8be5\u6a21\u578b\u4ee5\u8f93\u5165\u56fe\u50cf\u4e3a\u6761\u4ef6\uff0c\u5e76\u4e14\u7ecf\u8fc7\u4e13\u95e8\u8bad\u7ec3\u4ee5\u591a\u89c6\u56fe\u4e00\u81f4\u7684\u65b9\u5f0f\u7ec6\u5316\u6e32\u67d3\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8d28\u91cf\u548c\u6570\u91cf\u4e0a\u5747\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u4eba\u50cf\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u65f6\u95f4\u4e0a\u4e5f\u5f88\u6709\u6548\u7387\u3002", "conclusion": "\u56fe\u50cf\u7a7a\u95f4\u6269\u6563\u6a21\u578b\u53ef\u7528\u4e8e\u663e\u7740\u63d0\u9ad8\u73b0\u6709\u56fe\u50cf\u5230\u5934\u50cf\u65b9\u6cd5\u7684\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301 3D \u611f\u77e5\u5e76\u4ee5\u4f4e\u5ef6\u8fdf\u8fd0\u884c\u3002"}}
{"id": "2510.23631", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23631", "abs": "https://arxiv.org/abs/2510.23631", "authors": ["Yuxuan Tang", "Yifan Feng"], "title": "Beyond Pairwise: Empowering LLM Alignment With Ranked Choice Modeling", "comment": null, "summary": "Alignment of large language models (LLMs) has predominantly relied on\npairwise preference optimization, where annotators select the better of two\nresponses to a prompt. While simple, this approach overlooks the opportunity to\nlearn from richer forms of human feedback, such as multiwise comparisons and\ntop-$k$ rankings. We propose Ranked Choice Preference Optimization (RCPO), a\nunified framework that bridges preference optimization with (ranked) choice\nmodeling via maximum likelihood estimation. The framework is flexible,\nsupporting both utility-based and rank-based choice models. It subsumes several\nexisting pairwise methods (e.g., DPO, SimPO), while providing principled\ntraining objectives for richer feedback formats. We instantiate this framework\nwith two representative ranked choice models (Multinomial Logit and\nMallows-RMJ). Empirical studies on Llama-3-8B-Instruct and Gemma-2-9B-it across\nAlpacaEval 2 and Arena-Hard benchmarks show that RCPO consistently outperforms\ncompetitive baselines. RCPO shows how directly leveraging ranked preference\ndata, combined with the right choice models, yields more effective alignment.\nIt offers a versatile and extensible foundation for incorporating (ranked)\nchoice modeling into LLM training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6392\u5e8f\u9009\u62e9\u504f\u597d\u4f18\u5316 (RCPO) \u7684\u7edf\u4e00\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5c06\u504f\u597d\u4f18\u5316\u4e0e\uff08\u6392\u5e8f\uff09\u9009\u62e9\u5efa\u6a21\u8054\u7cfb\u8d77\u6765\u3002", "motivation": "\u4ee5\u5f80\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5bf9\u9f50\u4e3b\u8981\u4f9d\u8d56\u4e8e\u6210\u5bf9\u504f\u597d\u4f18\u5316\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5ffd\u7565\u4e86\u4ece\u66f4\u4e30\u5bcc\u7684\u4eba\u7c7b\u53cd\u9988\u5f62\u5f0f\uff08\u4f8b\u5982\u591a\u9879\u6bd4\u8f83\u548c top-k \u6392\u540d\uff09\u4e2d\u5b66\u4e60\u7684\u673a\u4f1a\u3002", "method": "\u63d0\u51fa\u4e86\u6392\u5e8f\u9009\u62e9\u504f\u597d\u4f18\u5316 (RCPO)\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5c06\u504f\u597d\u4f18\u5316\u4e0e\uff08\u6392\u5e8f\uff09\u9009\u62e9\u5efa\u6a21\u8054\u7cfb\u8d77\u6765\u3002\u8be5\u6846\u67b6\u7075\u6d3b\uff0c\u652f\u6301\u57fa\u4e8e\u6548\u7528\u548c\u57fa\u4e8e\u6392\u540d\u7684\u9009\u62e9\u6a21\u578b\u3002\u5b83\u5305\u542b\u4e86\u51e0\u79cd\u73b0\u6709\u7684\u6210\u5bf9\u65b9\u6cd5\uff08\u4f8b\u5982\uff0cDPO\u3001SimPO\uff09\uff0c\u540c\u65f6\u4e3a\u66f4\u4e30\u5bcc\u7684\u53cd\u9988\u683c\u5f0f\u63d0\u4f9b\u4e86\u6709\u539f\u5219\u7684\u8bad\u7ec3\u76ee\u6807\u3002", "result": "\u5728 AlpacaEval 2 \u548c Arena-Hard \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5bf9 Llama-3-8B-Instruct \u548c Gemma-2-9B-it \u7684\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\uff0cRCPO \u59cb\u7ec8\u4f18\u4e8e\u6709\u7ade\u4e89\u529b\u7684\u57fa\u7ebf\u3002", "conclusion": "RCPO \u8868\u660e\uff0c\u76f4\u63a5\u5229\u7528\u6392\u5e8f\u504f\u597d\u6570\u636e\uff0c\u7ed3\u5408\u6b63\u786e\u7684\u9009\u62e9\u6a21\u578b\uff0c\u53ef\u4ee5\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u5bf9\u9f50\u3002\u5b83\u4e3a\u5c06\uff08\u6392\u5e8f\uff09\u9009\u62e9\u5efa\u6a21\u7eb3\u5165 LLM \u8bad\u7ec3\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2510.23884", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23884", "abs": "https://arxiv.org/abs/2510.23884", "authors": ["Tananun Songdechakraiwut", "Michael Lutz"], "title": "Language Models for Longitudinal Clinical Prediction", "comment": null, "summary": "We explore a lightweight framework that adapts frozen large language models\nto analyze longitudinal clinical data. The approach integrates patient history\nand context within the language model space to generate accurate forecasts\nwithout model fine-tuning. Applied to neuropsychological assessments, it\nachieves accurate and reliable performance even with minimal training data,\nshowing promise for early-stage Alzheimer's monitoring.", "AI": {"tldr": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5206\u6790\u7eb5\u5411\u4e34\u5e8a\u6570\u636e\uff0c\u9884\u6d4b\u51c6\u786e\u3002", "motivation": "\u63a2\u7d22\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5206\u6790\u7eb5\u5411\u4e34\u5e8a\u6570\u636e\u3002", "method": "\u5c06\u60a3\u8005\u5386\u53f2\u548c\u8bed\u5883\u6574\u5408\u5230\u8bed\u8a00\u6a21\u578b\u7a7a\u95f4\u4e2d\u3002", "result": "\u5373\u4f7f\u4f7f\u7528\u6700\u5c11\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4e5f\u80fd\u5728\u795e\u7ecf\u5fc3\u7406\u5b66\u8bc4\u4f30\u4e2d\u5b9e\u73b0\u51c6\u786e\u53ef\u9760\u7684\u6027\u80fd\uff0c", "conclusion": "\u8be5\u65b9\u6cd5\u5bf9\u65e9\u671f\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u76d1\u6d4b\u5177\u6709\u524d\u666f"}}
{"id": "2510.23856", "categories": ["cs.AI", "68Txx"], "pdf": "https://arxiv.org/pdf/2510.23856", "abs": "https://arxiv.org/abs/2510.23856", "authors": ["Segev Shlomov", "Alon Oved", "Sami Marreed", "Ido Levy", "Offer Akrabi", "Avi Yaeli", "\u0141ukasz Str\u0105k", "Elizabeth Koumpan", "Yinon Goldshtein", "Eilam Shapira", "Nir Mashkif", "Asaf Adi"], "title": "From Benchmarks to Business Impact: Deploying IBM Generalist Agent in Enterprise Production", "comment": "AAAI Conference on Artificial Intelligence", "summary": "Agents are rapidly advancing in automating digital work, but enterprises face\na harder challenge: moving beyond prototypes to deployed systems that deliver\nmeasurable business value. This path is complicated by fragmented frameworks,\nslow development, and the absence of standardized evaluation practices.\nGeneralist agents have emerged as a promising direction, excelling on academic\nbenchmarks and offering flexibility across task types, applications, and\nmodalities. Yet, evidence of their use in production enterprise settings\nremains limited. This paper reports IBM's experience developing and piloting\nthe Computer Using Generalist Agent (CUGA), which has been open-sourced for the\ncommunity (https://github.com/cuga-project/cuga-agent). CUGA adopts a\nhierarchical planner--executor architecture with strong analytical foundations,\nachieving state-of-the-art performance on AppWorld and WebArena. Beyond\nbenchmarks, it was evaluated in a pilot within the Business-Process-Outsourcing\ntalent acquisition domain, addressing enterprise requirements for scalability,\nauditability, safety, and governance. To support assessment, we introduce\nBPO-TA, a 26-task benchmark spanning 13 analytics endpoints. In preliminary\nevaluations, CUGA approached the accuracy of specialized agents while\nindicating potential for reducing development time and cost. Our contribution\nis twofold: presenting early evidence of generalist agents operating at\nenterprise scale, and distilling technical and organizational lessons from this\ninitial pilot. We outline requirements and next steps for advancing\nresearch-grade architectures like CUGA into robust, enterprise-ready systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86IBM\u5f00\u53d1\u7684\u901a\u7528\u667a\u80fd\u4f53CUGA\uff0c\u5b83\u5728\u4f01\u4e1a\u89c4\u6a21\u4e0a\u8fdb\u884c\u4e86\u521d\u6b65\u7684\u8bd5\u70b9\uff0c\u5e76\u5f00\u6e90\u3002", "motivation": "\u4f01\u4e1a\u5728\u90e8\u7f72\u901a\u7528\u667a\u80fd\u4f53\u65f6\u9762\u4e34\u6846\u67b6\u5206\u6563\u3001\u5f00\u53d1\u7f13\u6162\u548c\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u5b9e\u8df5\u7684\u6311\u6218\u3002\u901a\u7528\u667a\u80fd\u4f53\u5728\u5b66\u672f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u5e94\u7528\u8bc1\u636e\u6709\u9650\u3002", "method": "CUGA\u91c7\u7528\u5206\u5c42\u89c4\u5212\u5668-\u6267\u884c\u5668\u67b6\u6784\uff0c\u5e76\u5728AppWorld\u548cWebArena\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86BPO-TA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b13\u4e2a\u5206\u6790\u7aef\u70b9\uff0c\u517126\u4e2a\u4efb\u52a1\u3002", "result": "CUGA\u5728\u521d\u6b65\u8bc4\u4f30\u4e2d\u63a5\u8fd1\u4e13\u7528\u667a\u80fd\u4f53\u7684\u51c6\u786e\u6027\uff0c\u5e76\u6709\u6f5c\u529b\u51cf\u5c11\u5f00\u53d1\u65f6\u95f4\u548c\u6210\u672c\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u901a\u7528\u667a\u80fd\u4f53\u5728\u4f01\u4e1a\u89c4\u6a21\u4e0a\u8fd0\u884c\u7684\u65e9\u671f\u8bc1\u636e\uff0c\u5e76\u603b\u7ed3\u4e86\u521d\u6b65\u8bd5\u70b9\u7684\u6280\u672f\u548c\u7ec4\u7ec7\u7ecf\u9a8c\u6559\u8bad\uff0c\u6982\u8ff0\u4e86\u5c06CUGA\u7b49\u7814\u7a76\u7ea7\u67b6\u6784\u63a8\u8fdb\u5230\u7a33\u5065\u7684\u4f01\u4e1a\u7ea7\u7cfb\u7edf\u4e2d\u7684\u9700\u6c42\u548c\u540e\u7eed\u6b65\u9aa4\u3002"}}
{"id": "2510.23930", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23930", "abs": "https://arxiv.org/abs/2510.23930", "authors": ["Xirui Jin", "Renbiao Jin", "Boying Li", "Danping Zou", "Wenxian Yu"], "title": "PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors", "comment": "Accepted by NeurIPS 2025. Project page: https://planargs.github.io", "summary": "Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an\nefficient representation for novel-view synthesis, achieving impressive visual\nquality. However, in scenes dominated by large and low-texture regions, common\nin indoor environments, the photometric loss used to optimize 3DGS yields\nambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome\nthis limitation, we introduce PlanarGS, a 3DGS-based framework tailored for\nindoor scene reconstruction. Specifically, we design a pipeline for\nLanguage-Prompted Planar Priors (LP3) that employs a pretrained vision-language\nsegmentation model and refines its region proposals via cross-view fusion and\ninspection with geometric priors. 3D Gaussians in our framework are optimized\nwith two additional terms: a planar prior supervision term that enforces planar\nconsistency, and a geometric prior supervision term that steers the Gaussians\ntoward the depth and normal cues. We have conducted extensive experiments on\nstandard indoor benchmarks. The results show that PlanarGS reconstructs\naccurate and detailed 3D surfaces, consistently outperforming state-of-the-art\nmethods by a large margin. Project page: https://planargs.github.io", "AI": {"tldr": "PlanarGS: A 3DGS-based framework for indoor scene reconstruction, uses language-prompted planar priors to overcome the limitations of 3DGS in low-texture indoor environments.", "motivation": "3D Gaussian Splatting (3DGS) struggles in indoor scenes with large, low-texture regions, leading to ambiguous geometry and poor 3D surface reconstruction.", "method": "Introduces a Language-Prompted Planar Priors (LP3) pipeline that uses a vision-language segmentation model and refines region proposals with cross-view fusion and geometric priors.  It optimizes 3D Gaussians with planar consistency and geometric prior supervision (depth and normal cues).", "result": "PlanarGS reconstructs accurate and detailed 3D surfaces, significantly outperforming state-of-the-art methods on standard indoor benchmarks.", "conclusion": "PlanarGS effectively addresses the limitations of 3DGS in indoor scenes by incorporating planar priors, leading to improved 3D surface reconstruction quality."}}
{"id": "2510.23632", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23632", "abs": "https://arxiv.org/abs/2510.23632", "authors": ["Guozhong Li", "Muhannad Alhumaidi", "Spiros Skiadopoulos", "Panos Kalnis"], "title": "LLMComp: A Language Modeling Paradigm for Error-Bounded Scientific Data Compression", "comment": null, "summary": "The rapid growth of high-resolution scientific simulations and observation\nsystems is generating massive spatiotemporal datasets, making efficient,\nerror-bounded compression increasingly important. Meanwhile, decoder-only large\nlanguage models (LLMs) have demonstrated remarkable capabilities in modeling\ncomplex sequential data. In this paper, we propose LLMCOMP, a novel lossy\ncompression paradigm that leverages decoder-only large LLMs to model scientific\ndata. LLMCOMP first quantizes 3D fields into discrete tokens, arranges them via\nZ-order curves to preserve locality, and applies coverage-guided sampling to\nenhance training efficiency. An autoregressive transformer is then trained with\nspatial-temporal embeddings to model token transitions. During compression, the\nmodel performs top-k prediction, storing only rank indices and fallback\ncorrections to ensure strict error bounds. Experiments on multiple reanalysis\ndatasets show that LLMCOMP consistently outperforms state-of-the-art\ncompressors, achieving up to 30% higher compression ratios under strict error\nbounds. These results highlight the potential of LLMs as general-purpose\ncompressors for high-fidelity scientific data.", "AI": {"tldr": "LLMCOMP\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6709\u635f\u538b\u7f29\uff0c\u5728\u4fdd\u8bc1\u8bef\u5dee\u8303\u56f4\u7684\u524d\u63d0\u4e0b\uff0c\u538b\u7f29\u7387\u6bd4\u73b0\u6709\u6280\u672f\u9ad8\u51fa30%\u3002", "motivation": "\u9ad8\u6548\u3001\u6709\u8bef\u5dee\u754c\u9650\u7684\u538b\u7f29\u5bf9\u4e8e\u5927\u89c4\u6a21\u65f6\u7a7a\u6570\u636e\u96c6\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u89e3\u7801\u5668\u4e13\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5efa\u6a21\u590d\u6742\u5e8f\u5217\u6570\u636e\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u80fd\u529b\u3002", "method": "LLMCOMP\u9996\u5148\u5c063D\u573a\u91cf\u5316\u4e3a\u79bb\u6563token\uff0c\u901a\u8fc7Z\u9636\u66f2\u7ebf\u6392\u5217\u5b83\u4eec\u4ee5\u4fdd\u6301\u5c40\u90e8\u6027\uff0c\u5e76\u5e94\u7528\u8986\u76d6\u5f15\u5bfc\u91c7\u6837\u6765\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002\u7136\u540e\uff0c\u5229\u7528\u7a7a\u95f4-\u65f6\u95f4\u5d4c\u5165\u8bad\u7ec3\u81ea\u56de\u5f52transformer\u6765\u5efa\u6a21token\u8f6c\u6362\u3002", "result": "\u5728\u591a\u4e2a\u518d\u5206\u6790\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLLMCOMP\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u538b\u7f29\u5668\uff0c\u5728\u4e25\u683c\u7684\u8bef\u5dee\u8303\u56f4\u5185\uff0c\u538b\u7f29\u7387\u9ad8\u8fbe30%\u3002", "conclusion": "LLM\u4f5c\u4e3a\u9ad8\u4fdd\u771f\u79d1\u5b66\u6570\u636e\u7684\u901a\u7528\u538b\u7f29\u5668\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2510.23896", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23896", "abs": "https://arxiv.org/abs/2510.23896", "authors": ["Kosei Uemura", "Miaoran Zhang", "David Ifeoluwa Adelani"], "title": "AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages", "comment": null, "summary": "Text embeddings are an essential building component of several NLP tasks such\nas retrieval-augmented generation which is crucial for preventing\nhallucinations in LLMs. Despite the recent release of massively multilingual\nMTEB (MMTEB), African languages remain underrepresented, with existing tasks\noften repurposed from translation benchmarks such as FLORES clustering or\nSIB-200. In this paper, we introduce AfriMTEB -- a regional expansion of MMTEB\ncovering 59 languages, 14 tasks, and 38 datasets, including six newly added\ndatasets. Unlike many MMTEB datasets that include fewer than five languages,\nthe new additions span 14 to 56 African languages and introduce entirely new\ntasks, such as hate speech detection, intent detection, and emotion\nclassification, which were not previously covered. Complementing this, we\npresent AfriE5, an adaptation of the instruction-tuned mE5 model to African\nlanguages through cross-lingual contrastive distillation. Our evaluation shows\nthat AfriE5 achieves state-of-the-art performance, outperforming strong\nbaselines such as Gemini-Embeddings and mE5.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86AfriMTEB\uff0c\u4e00\u4e2a\u6269\u5c55\u7684MMTEB\uff0c\u8986\u76d659\u79cd\u975e\u6d32\u8bed\u8a00\uff0c14\u4e2a\u4efb\u52a1\u548c38\u4e2a\u6570\u636e\u96c6\u3002\u540c\u65f6\u63d0\u51fa\u4e86AfriE5\u6a21\u578b\uff0c\u901a\u8fc7\u8de8\u8bed\u8a00\u5bf9\u6bd4\u84b8\u998f\u5c06mE5\u6a21\u578b\u9002\u914d\u5230\u975e\u6d32\u8bed\u8a00\u3002", "motivation": "\u73b0\u6709NLP\u4efb\u52a1\u4e2d\u7684\u6587\u672c\u5d4c\u5165\u5bf9\u4e8e\u9632\u6b62LLM\u4e2d\u7684\u5e7b\u89c9\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u975e\u6d32\u8bed\u8a00\u5728\u73b0\u6709\u7684MMTEB\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u8de8\u8bed\u8a00\u5bf9\u6bd4\u84b8\u998f\uff0c\u5c06instruction-tuned mE5\u6a21\u578b\u9002\u914d\u5230\u975e\u6d32\u8bed\u8a00\uff0c\u5f97\u5230AfriE5\u6a21\u578b\u3002", "result": "AfriE5\u5b9e\u73b0\u4e86state-of-the-art\u7684\u6027\u80fd\uff0c\u8d85\u8fc7\u4e86Gemini-Embeddings\u548cmE5\u7b49\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u672c\u6587\u5f15\u5165\u4e86AfriMTEB\u548cAfriE5\uff0c\u4e3a\u975e\u6d32\u8bed\u8a00\u7684NLP\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u8d44\u6e90\u548c\u65b9\u6cd5\u3002"}}
{"id": "2510.23881", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23881", "abs": "https://arxiv.org/abs/2510.23881", "authors": ["Xidong Feng", "Vivek Veeriah", "Marcus Chiam", "Michael Dennis", "Ryan Pachauri", "Thomas Tumiel", "Federico Barbero", "Johan Obando-Ceron", "Jiaxin Shi", "Satinder Singh", "Shaobo Hou", "Nenad Toma\u0161ev", "Tom Zahavy"], "title": "Generating Creative Chess Puzzles", "comment": null, "summary": "While Generative AI rapidly advances in various domains, generating truly\ncreative, aesthetic, and counter-intuitive outputs remains a challenge. This\npaper presents an approach to tackle these difficulties in the domain of chess\npuzzles. We start by benchmarking Generative AI architectures, and then\nintroduce an RL framework with novel rewards based on chess engine search\nstatistics to overcome some of those shortcomings. The rewards are designed to\nenhance a puzzle's uniqueness, counter-intuitiveness, diversity, and realism.\nOur RL approach dramatically increases counter-intuitive puzzle generation by\n10x, from 0.22\\% (supervised) to 2.5\\%, surpassing existing dataset rates\n(2.1\\%) and the best Lichess-trained model (0.4\\%). Our puzzles meet novelty\nand diversity benchmarks, retain aesthetic themes, and are rated by human\nexperts as more creative, enjoyable, and counter-intuitive than composed book\npuzzles, even approaching classic compositions. Our final outcome is a curated\nbooklet of these AI-generated puzzles, which is acknowledged for creativity by\nthree world-renowned experts.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6846\u67b6\u751f\u6210\u66f4\u5177\u521b\u9020\u6027\u3001\u7f8e\u611f\u548c\u8fdd\u53cd\u76f4\u89c9\u7684\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bbe\u8ba1\u57fa\u4e8e\u56fd\u9645\u8c61\u68cb\u5f15\u64ce\u641c\u7d22\u7edf\u8ba1\u7684\u65b0\u9896\u5956\u52b1\uff0c\u6765\u514b\u670d\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5728\u751f\u6210\u6b64\u7c7b\u8c1c\u9898\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5728\u56fd\u9645\u8c61\u68cb\u8c1c\u9898\u9886\u57df\uff0c\u751f\u6210\u771f\u6b63\u5177\u6709\u521b\u9020\u6027\u3001\u7f8e\u611f\u548c\u8fdd\u53cd\u76f4\u89c9\u7684\u8f93\u51fa\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u9996\u5148\uff0c\u5bf9\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u67b6\u6784\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7136\u540e\u5f15\u5165\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u57fa\u4e8e\u56fd\u9645\u8c61\u68cb\u5f15\u64ce\u641c\u7d22\u7edf\u8ba1\u7684\u65b0\u9896\u5956\u52b1\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f7f\u8fdd\u53cd\u76f4\u89c9\u7684\u8c1c\u9898\u751f\u6210\u7387\u63d0\u9ad8\u4e86 10 \u500d\uff0c\u4ece 0.22%\uff08\u76d1\u7763\u5f0f\uff09\u63d0\u9ad8\u5230 2.5%\uff0c\u8d85\u8fc7\u4e86\u73b0\u6709\u7684\u6570\u636e\u96c6\u6bd4\u7387\uff082.1%\uff09\u548c\u6700\u4f73 Lichess \u8bad\u7ec3\u6a21\u578b\uff080.4%\uff09\u3002", "conclusion": "\u8bba\u6587\u6700\u7ec8\u751f\u6210\u4e86\u4e00\u672c\u7531\u4eba\u5de5\u667a\u80fd\u751f\u6210\u7684\u8c1c\u9898\u96c6\uff0c\u5e76\u83b7\u5f97\u4e86\u4e09\u4f4d\u4e16\u754c\u77e5\u540d\u4e13\u5bb6\u7684\u8ba4\u53ef\u3002"}}
{"id": "2510.23943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23943", "abs": "https://arxiv.org/abs/2510.23943", "authors": ["Diana Aldana", "Jo\u00e3o Paulo Lima", "Daniel Csillag", "Daniel Perazzo", "Haoan Feng", "Luiz Velho", "Tiago Novello"], "title": "Adaptive Training of INRs via Pruning and Densification", "comment": null, "summary": "Encoding input coordinates with sinusoidal functions into multilayer\nperceptrons (MLPs) has proven effective for implicit neural representations\n(INRs) of low-dimensional signals, enabling the modeling of high-frequency\ndetails. However, selecting appropriate input frequencies and architectures\nwhile managing parameter redundancy remains an open challenge, often addressed\nthrough heuristics and heavy hyperparameter optimization schemes. In this\npaper, we introduce AIRe ($\\textbf{A}$daptive $\\textbf{I}$mplicit neural\n$\\textbf{Re}$presentation), an adaptive training scheme that refines the INR\narchitecture over the course of optimization. Our method uses a neuron pruning\nmechanism to avoid redundancy and input frequency densification to improve\nrepresentation capacity, leading to an improved trade-off between network size\nand reconstruction quality. For pruning, we first identify less-contributory\nneurons and apply a targeted weight decay to transfer their information to the\nremaining neurons, followed by structured pruning. Next, the densification\nstage adds input frequencies to spectrum regions where the signal underfits,\nexpanding the representational basis. Through experiments on images and SDFs,\nwe show that AIRe reduces model size while preserving, or even improving,\nreconstruction quality. Code and pretrained models will be released for public\nuse.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAIRe\u7684\u81ea\u9002\u5e94\u8bad\u7ec3\u65b9\u6848\uff0c\u7528\u4e8e\u4f18\u5316\u9690\u5f0f\u795e\u7ecf\u8868\u793a(INR)\u7684\u67b6\u6784\u3002", "motivation": "\u73b0\u6709\u7684\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\u5728\u9009\u62e9\u5408\u9002\u7684\u8f93\u5165\u9891\u7387\u548c\u67b6\u6784\uff0c\u4ee5\u53ca\u7ba1\u7406\u53c2\u6570\u5197\u4f59\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u901a\u5e38\u9700\u8981\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u5927\u91cf\u7684\u8d85\u53c2\u6570\u4f18\u5316\u3002", "method": "\u8be5\u65b9\u6cd5\u4f7f\u7528\u795e\u7ecf\u5143\u526a\u679d\u673a\u5236\u6765\u907f\u514d\u5197\u4f59\uff0c\u5e76\u4f7f\u7528\u8f93\u5165\u9891\u7387\u5bc6\u96c6\u5316\u6765\u63d0\u9ad8\u8868\u793a\u80fd\u529b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u9996\u5148\u8bc6\u522b\u8d21\u732e\u8f83\u5c0f\u7684\u795e\u7ecf\u5143\uff0c\u5e76\u5e94\u7528\u6709\u9488\u5bf9\u6027\u7684\u6743\u91cd\u8870\u51cf\u5c06\u5176\u4fe1\u606f\u8f6c\u79fb\u5230\u5269\u4f59\u7684\u795e\u7ecf\u5143\uff0c\u7136\u540e\u8fdb\u884c\u7ed3\u6784\u5316\u526a\u679d\u3002\u63a5\u4e0b\u6765\uff0c\u5bc6\u96c6\u5316\u9636\u6bb5\u5c06\u8f93\u5165\u9891\u7387\u6dfb\u52a0\u5230\u4fe1\u53f7\u6b20\u62df\u5408\u7684\u9891\u8c31\u533a\u57df\uff0c\u4ece\u800c\u6269\u5c55\u8868\u793a\u57fa\u7840\u3002", "result": "\u5728\u56fe\u50cf\u548cSDF\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAIRe\u53ef\u4ee5\u5728\u51cf\u5c0f\u6a21\u578b\u5c3a\u5bf8\u7684\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "AIRe \u80fd\u591f\u5728\u51cf\u5c0f\u6a21\u578b\u5c3a\u5bf8\u7684\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\uff0c\u5b9e\u73b0\u4e86\u7f51\u7edc\u5927\u5c0f\u548c\u91cd\u5efa\u8d28\u91cf\u4e4b\u95f4\u7684\u66f4\u597d\u6743\u8861\u3002"}}
{"id": "2510.23633", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.23633", "abs": "https://arxiv.org/abs/2510.23633", "authors": ["Xun Su", "Hiroyuki Kasai"], "title": "Noise is All You Need: Solving Linear Inverse Problems by Noise Combination Sampling with Diffusion Models", "comment": "9 pages", "summary": "Pretrained diffusion models have demonstrated strong capabilities in\nzero-shot inverse problem solving by incorporating observation information into\nthe generation process of the diffusion models. However, this presents an\ninherent dilemma: excessive integration can disrupt the generative process,\nwhile insufficient integration fails to emphasize the constraints imposed by\nthe inverse problem. To address this, we propose \\emph{Noise Combination\nSampling}, a novel method that synthesizes an optimal noise vector from a noise\nsubspace to approximate the measurement score, replacing the noise term in the\nstandard Denoising Diffusion Probabilistic Models process. This enables\nconditional information to be naturally embedded into the generation process\nwithout reliance on step-wise hyperparameter tuning. Our method can be applied\nto a wide range of inverse problem solvers, including image compression, and,\nparticularly when the number of generation steps $T$ is small, achieves\nsuperior performance with negligible computational overhead, significantly\nimproving robustness and stability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u566a\u58f0\u7ec4\u5408\u91c7\u6837\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5728\u96f6\u6837\u672c\u9006\u95ee\u9898\u6c42\u89e3\u4e2d\u8fc7\u5ea6\u6216\u4e0d\u8db3\u6574\u5408\u89c2\u6d4b\u4fe1\u606f\u7684\u95ee\u9898\u3002", "motivation": "\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5728\u96f6\u6837\u672c\u9006\u95ee\u9898\u6c42\u89e3\u4e2d\u9762\u4e34\u8fc7\u5ea6\u6574\u5408\u89c2\u6d4b\u4fe1\u606f\u6270\u4e71\u751f\u6210\u8fc7\u7a0b\uff0c\u6216\u6574\u5408\u4e0d\u8db3\u65e0\u6cd5\u5f3a\u8c03\u9006\u95ee\u9898\u7ea6\u675f\u7684\u56f0\u5883\u3002", "method": "\u63d0\u51fa\u566a\u58f0\u7ec4\u5408\u91c7\u6837\u65b9\u6cd5\uff0c\u4ece\u566a\u58f0\u5b50\u7a7a\u95f4\u5408\u6210\u6700\u4f18\u566a\u58f0\u5411\u91cf\u4ee5\u8fd1\u4f3c\u6d4b\u91cf\u5f97\u5206\uff0c\u66ff\u6362\u6807\u51c6\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\u8fc7\u7a0b\u4e2d\u7684\u566a\u58f0\u9879\u3002", "result": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u9006\u95ee\u9898\u6c42\u89e3\u5668\uff0c\u5c24\u5176\u662f\u5728\u751f\u6210\u6b65\u9aa4\u6570 T \u8f83\u5c0f\u65f6\uff0c\u4ee5\u53ef\u5ffd\u7565\u7684\u8ba1\u7b97\u5f00\u9500\u5b9e\u73b0\u5353\u8d8a\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u566a\u58f0\u7ec4\u5408\u91c7\u6837\u65b9\u6cd5\u80fd\u591f\u81ea\u7136\u5730\u5c06\u6761\u4ef6\u4fe1\u606f\u5d4c\u5165\u5230\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u65e0\u9700\u4f9d\u8d56\u9010\u6b65\u8d85\u53c2\u6570\u8c03\u6574\u3002"}}
{"id": "2510.23921", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23921", "abs": "https://arxiv.org/abs/2510.23921", "authors": ["Kaveh Eskandari Miandoab", "Mahammed Kamruzzaman", "Arshia Gharooni", "Gene Louis Kim", "Vasanth Sarathy", "Ninareh Mehrabi"], "title": "Breaking the Benchmark: Revealing LLM Bias via Minimal Contextual Augmentation", "comment": "9 pages, 3 figures, 3 tables", "summary": "Large Language Models have been shown to demonstrate stereotypical biases in\ntheir representations and behavior due to the discriminative nature of the data\nthat they have been trained on. Despite significant progress in the development\nof methods and models that refrain from using stereotypical information in\ntheir decision-making, recent work has shown that approaches used for bias\nalignment are brittle. In this work, we introduce a novel and general\naugmentation framework that involves three plug-and-play steps and is\napplicable to a number of fairness evaluation benchmarks. Through application\nof augmentation to a fairness evaluation dataset (Bias Benchmark for Question\nAnswering (BBQ)), we find that Large Language Models (LLMs), including\nstate-of-the-art open and closed weight models, are susceptible to\nperturbations to their inputs, showcasing a higher likelihood to behave\nstereotypically. Furthermore, we find that such models are more likely to have\nbiased behavior in cases where the target demographic belongs to a community\nless studied by the literature, underlining the need to expand the fairness and\nsafety research to include more diverse communities.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u5224\u522b\u6027\uff0c\u8868\u73b0\u51fa\u523b\u677f\u5370\u8c61\u504f\u5dee\u3002\u5c3d\u7ba1\u5728\u907f\u514d\u4f7f\u7528\u523b\u677f\u5370\u8c61\u4fe1\u606f\u7684\u65b9\u6cd5\u548c\u6a21\u578b\u5f00\u53d1\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u7528\u4e8e\u504f\u5dee\u5bf9\u9f50\u7684\u65b9\u6cd5\u662f\u8106\u5f31\u7684\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u800c\u901a\u7528\u7684\u589e\u5f3a\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u62ec\u4e09\u4e2a\u5373\u63d2\u5373\u7528\u6b65\u9aa4\uff0c\u9002\u7528\u4e8e\u8bb8\u591a\u516c\u5e73\u6027\u8bc4\u4f30\u57fa\u51c6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u523b\u677f\u5370\u8c61\u504f\u5dee\uff0c\u4e14\u73b0\u6709\u7684\u504f\u5dee\u5bf9\u9f50\u65b9\u6cd5\u5177\u6709\u8106\u5f31\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u901a\u7528\u589e\u5f3a\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5373\u63d2\u5373\u7528\u6b65\u9aa4\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u516c\u5e73\u6027\u8bc4\u4f30\u6570\u636e\u96c6\uff08BBQ\uff09\u3002", "result": "\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ec\u6700\u5148\u8fdb\u7684\u5f00\u653e\u548c\u5c01\u95ed\u6743\u91cd\u6a21\u578b\uff09\u5bb9\u6613\u53d7\u5230\u5bf9\u5176\u8f93\u5165\u7684\u6270\u52a8\u7684\u5f71\u54cd\uff0c\u66f4\u6709\u53ef\u80fd\u8868\u73b0\u51fa\u523b\u677f\u5370\u8c61\u3002\u6b64\u5916\uff0c\u76ee\u6807\u4eba\u7fa4\u5c5e\u4e8e\u6587\u732e\u7814\u7a76\u8f83\u5c11\u7684\u793e\u533a\u65f6\uff0c\u8fd9\u4e9b\u6a21\u578b\u66f4\u6709\u53ef\u80fd\u51fa\u73b0\u504f\u5dee\u884c\u4e3a\u3002", "conclusion": "\u9700\u8981\u6269\u5927\u516c\u5e73\u6027\u548c\u5b89\u5168\u6027\u7814\u7a76\uff0c\u4ee5\u6db5\u76d6\u66f4\u591a\u4e0d\u540c\u7684\u793e\u533a\u3002"}}
{"id": "2510.23882", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23882", "abs": "https://arxiv.org/abs/2510.23882", "authors": ["Adil Rasheed", "Oscar Ravik", "Omer San"], "title": "Hybrid Modeling, Sim-to-Real Reinforcement Learning, and Large Language Model Driven Control for Digital Twins", "comment": null, "summary": "This work investigates the use of digital twins for dynamical system modeling\nand control, integrating physics-based, data-driven, and hybrid approaches with\nboth traditional and AI-driven controllers. Using a miniature greenhouse as a\ntest platform, four predictive models Linear, Physics-Based Modeling (PBM),\nLong Short Term Memory (LSTM), and Hybrid Analysis and Modeling (HAM) are\ndeveloped and compared under interpolation and extrapolation scenarios. Three\ncontrol strategies Model Predictive Control (MPC), Reinforcement Learning (RL),\nand Large Language Model (LLM) based control are also implemented to assess\ntrade-offs in precision, adaptability, and implementation effort. Results show\nthat in modeling HAM provides the most balanced performance across accuracy,\ngeneralization, and computational efficiency, while LSTM achieves high\nprecision at greater resource cost. Among controllers, MPC delivers robust and\npredictable performance, RL demonstrates strong adaptability, and LLM-based\ncontrollers offer flexible human-AI interaction when coupled with predictive\ntools.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u6570\u5b57\u5b6a\u751f\u5728\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u548c\u63a7\u5236\u4e2d\u7684\u5e94\u7528\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e\u7269\u7406\u3001\u6570\u636e\u9a71\u52a8\u548c\u6df7\u5408\u65b9\u6cd5\uff0c\u4ee5\u53ca\u4f20\u7edf\u548c\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u63a7\u5236\u5668\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5728\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u4e0e\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u8be5\u7814\u7a76\u4f7f\u7528\u4e00\u4e2a\u5fae\u578b\u6e29\u5ba4\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5f00\u53d1\u5e76\u6bd4\u8f83\u4e86\u56db\u79cd\u9884\u6d4b\u6a21\u578b\uff08\u7ebf\u6027\u6a21\u578b\u3001\u57fa\u4e8e\u7269\u7406\u7684\u5efa\u6a21\uff08PBM\uff09\u3001\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08LSTM\uff09\u548c\u6df7\u5408\u5206\u6790\u4e0e\u5efa\u6a21\uff08HAM\uff09\uff09\uff0c\u4ee5\u53ca\u4e09\u79cd\u63a7\u5236\u7b56\u7565\uff08\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u3001\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a7\u5236\uff09\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5efa\u6a21\u65b9\u9762\uff0cHAM\u5728\u51c6\u786e\u6027\u3001\u6cdb\u5316\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6700\u5e73\u8861\u7684\u6027\u80fd\uff0c\u800cLSTM\u4ee5\u66f4\u9ad8\u7684\u8d44\u6e90\u6210\u672c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u3002\u5728\u63a7\u5236\u5668\u65b9\u9762\uff0cMPC\u63d0\u4f9b\u4e86\u7a33\u5065\u548c\u53ef\u9884\u6d4b\u7684\u6027\u80fd\uff0cRL\u8868\u73b0\u51fa\u5f88\u5f3a\u7684\u9002\u5e94\u6027\uff0c\u800c\u57fa\u4e8eLLM\u7684\u63a7\u5236\u5668\u5728\u4e0e\u9884\u6d4b\u5de5\u5177\u7ed3\u5408\u4f7f\u7528\u65f6\uff0c\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "conclusion": "\u8be5\u7814\u7a76\u7efc\u5408\u8bc4\u4f30\u4e86\u4e0d\u540c\u5efa\u6a21\u65b9\u6cd5\u548c\u63a7\u5236\u7b56\u7565\u5728\u6570\u5b57\u5b6a\u751f\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u548c\u4f18\u7f3a\u70b9\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2510.23956", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23956", "abs": "https://arxiv.org/abs/2510.23956", "authors": ["Alejandro Escontrela", "Shrinu Kushagra", "Sjoerd van Steenkiste", "Yulia Rubanova", "Aleksander Holynski", "Kelsey Allen", "Kevin Murphy", "Thomas Kipf"], "title": "Neural USD: An object-centric framework for iterative editing and control", "comment": "22 pages, 16 figures, 1 table", "summary": "Amazing progress has been made in controllable generative modeling,\nespecially over the last few years. However, some challenges remain. One of\nthem is precise and iterative object editing. In many of the current methods,\ntrying to edit the generated image (for example, changing the color of a\nparticular object in the scene or changing the background while keeping other\nelements unchanged) by changing the conditioning signals often leads to\nunintended global changes in the scene. In this work, we take the first steps\nto address the above challenges. Taking inspiration from the Universal Scene\nDescriptor (USD) standard developed in the computer graphics community, we\nintroduce the \"Neural Universal Scene Descriptor\" or Neural USD. In this\nframework, we represent scenes and objects in a structured, hierarchical\nmanner. This accommodates diverse signals, minimizes model-specific\nconstraints, and enables per-object control over appearance, geometry, and\npose. We further apply a fine-tuning approach which ensures that the above\ncontrol signals are disentangled from one another. We evaluate several design\nconsiderations for our framework, demonstrating how Neural USD enables\niterative and incremental workflows. More information at:\nhttps://escontrela.me/neural_usd .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u5bf9\u56fe\u50cf\u4e2d\u7684\u5bf9\u8c61\u8fdb\u884c\u7cbe\u786e\u548c\u8fed\u4ee3\u7f16\u8f91\uff0c\u907f\u514d\u4e86\u5168\u5c40\u6027\u6539\u53d8\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u5728\u7f16\u8f91\u56fe\u50cf\u65f6\uff0c\u5bb9\u6613\u5f15\u8d77\u5168\u5c40\u6027\u6539\u53d8\u3002", "method": "\u501f\u9274\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e2d\u7684\u901a\u7528\u573a\u666f\u63cf\u8ff0(USD)\u6807\u51c6\uff0c\u63d0\u51fa\u4e86\u795e\u7ecf\u901a\u7528\u573a\u666f\u63cf\u8ff0(Neural USD)\uff0c\u4ee5\u5206\u5c42\u7ed3\u6784\u5316\u7684\u65b9\u5f0f\u8868\u793a\u573a\u666f\u548c\u5bf9\u8c61\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86Neural USD\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u652f\u6301\u8fed\u4ee3\u548c\u589e\u91cf\u5de5\u4f5c\u6d41\u7684\u80fd\u529b\u3002", "conclusion": "Neural USD\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u5bf9\u56fe\u50cf\u4e2d\u5bf9\u8c61\u7684\u5916\u89c2\u3001\u51e0\u4f55\u5f62\u72b6\u548c\u59ff\u6001\u7684\u9010\u5bf9\u8c61\u63a7\u5236\uff0c\u5e76\u4fdd\u8bc1\u63a7\u5236\u4fe1\u53f7\u7684\u89e3\u8026\u3002"}}
{"id": "2510.23634", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23634", "abs": "https://arxiv.org/abs/2510.23634", "authors": ["Soutrik Sarangi", "Yonatan Sverdlov", "Nadav Dym", "Abir De"], "title": "Monotone and Separable Set Functions: Characterizations and Neural Models", "comment": null, "summary": "Motivated by applications for set containment problems, we consider the\nfollowing fundamental problem: can we design set-to-vector functions so that\nthe natural partial order on sets is preserved, namely $S\\subseteq T \\text{ if\nand only if } F(S)\\leq F(T) $. We call functions satisfying this property\nMonotone and Separating (MAS) set functions. % We establish lower and upper\nbounds for the vector dimension necessary to obtain MAS functions, as a\nfunction of the cardinality of the multisets and the underlying ground set. In\nthe important case of an infinite ground set, we show that MAS functions do not\nexist, but provide a model called our which provably enjoys a relaxed MAS\nproperty we name \"weakly MAS\" and is stable in the sense of Holder continuity.\nWe also show that MAS functions can be used to construct universal models that\nare monotone by construction and can approximate all monotone set functions.\nExperimentally, we consider a variety of set containment tasks. The experiments\nshow the benefit of using our our model, in comparison with standard set models\nwhich do not incorporate set containment as an inductive bias. Our code is\navailable in https://github.com/yonatansverdlov/Monotone-Embedding.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4fdd\u6301\u96c6\u5408\u81ea\u7136\u504f\u5e8f\u7684\u96c6\u5408\u5230\u5411\u91cf\u7684\u51fd\u6570\u8bbe\u8ba1\u95ee\u9898\uff0c\u5373\u5355\u8c03\u5206\u79bb (MAS) \u96c6\u5408\u51fd\u6570\u3002", "motivation": "\u53d7\u96c6\u5408\u5305\u542b\u95ee\u9898\u7684\u5e94\u7528\u9a71\u52a8\uff0c\u7814\u7a76\u8bbe\u8ba1\u96c6\u5408\u5230\u5411\u91cf\u7684\u51fd\u6570\uff0c\u4f7f\u5f97\u96c6\u5408\u7684\u81ea\u7136\u504f\u5e8f\u5f97\u4ee5\u4fdd\u6301\u3002", "method": "\u5efa\u7acb\u4e86\u83b7\u5f97 MAS \u51fd\u6570\u6240\u9700\u7684\u5411\u91cf\u7ef4\u5ea6\u7684\u4e0a\u4e0b\u754c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u79f0\u4e3a\u201c\u5f31 MAS\u201d\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5177\u6709 Holder \u8fde\u7eed\u6027\u3002", "result": "\u5bf9\u4e8e\u65e0\u9650 ground set \u7684\u91cd\u8981\u60c5\u51b5\uff0c\u8868\u660e MAS \u51fd\u6570\u4e0d\u5b58\u5728\uff0c\u4f46\u63d0\u4f9b\u4e86\u4e00\u4e2a\u79f0\u4e3a our \u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5177\u6709\u5f31 MAS \u5c5e\u6027\uff0c\u5e76\u4e14\u5728 Holder \u8fde\u7eed\u6027\u65b9\u9762\u662f\u7a33\u5b9a\u7684\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u6ca1\u6709\u7ed3\u5408\u96c6\u5408\u5305\u542b\u4f5c\u4e3a\u5f52\u7eb3\u504f\u5dee\u7684\u6807\u51c6\u96c6\u5408\u6a21\u578b\u76f8\u6bd4\uff0c\u4f7f\u7528\u6211\u4eec\u7684\u6a21\u578b\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "MAS \u51fd\u6570\u53ef\u7528\u4e8e\u6784\u5efa\u5355\u8c03\u7684\u901a\u7528\u6a21\u578b\uff0c\u5e76\u4e14\u53ef\u4ee5\u903c\u8fd1\u6240\u6709\u5355\u8c03\u96c6\u5408\u51fd\u6570\u3002"}}
{"id": "2510.23924", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23924", "abs": "https://arxiv.org/abs/2510.23924", "authors": ["Dina Pisarevskaya", "Arkaitz Zubiaga"], "title": "Agent-based Automated Claim Matching with Instruction-following LLMs", "comment": "Accepted for the International Joint Conference on Natural Language\n  Processing & Asia-Pacific Chapter of the Association for Computational\n  Linguistics (2025) Findings", "summary": "We present a novel agent-based approach for the automated claim matching task\nwith instruction-following LLMs. We propose a two-step pipeline that first\ngenerates prompts with LLMs, to then perform claim matching as a binary\nclassification task with LLMs. We demonstrate that LLM-generated prompts can\noutperform SOTA with human-generated prompts, and that smaller LLMs can do as\nwell as larger ones in the generation process, allowing to save computational\nresources. We also demonstrate the effectiveness of using different LLMs for\neach step of the pipeline, i.e. using an LLM for prompt generation, and another\nfor claim matching. Our investigation into the prompt generation process in\nturn reveals insights into the LLMs' understanding of claim matching.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u4ee3\u7406\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f7f\u7528\u6307\u4ee4\u8ddf\u968f LLM \u81ea\u52a8\u6267\u884c\u58f0\u660e\u5339\u914d\u4efb\u52a1\u3002", "motivation": "\u63a2\u7d22\u4f7f\u7528 LLM \u751f\u6210\u63d0\u793a\u6765\u6539\u8fdb\u58f0\u660e\u5339\u914d\uff0c\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u6b65\u6d41\u7a0b\uff0c\u9996\u5148\u4f7f\u7528 LLM \u751f\u6210\u63d0\u793a\uff0c\u7136\u540e\u4f7f\u7528 LLM \u6267\u884c\u58f0\u660e\u5339\u914d\u4f5c\u4e3a\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u3002", "result": "LLM \u751f\u6210\u7684\u63d0\u793a\u4f18\u4e8e\u4eba\u5de5\u751f\u6210\u7684\u63d0\u793a\uff0c\u8f83\u5c0f\u7684 LLM \u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u4e0e\u8f83\u5927\u7684 LLM \u6548\u679c\u4e00\u6837\u597d\uff0c\u5e76\u4e14\u5728\u6d41\u7a0b\u7684\u6bcf\u4e2a\u6b65\u9aa4\u4e2d\u4f7f\u7528\u4e0d\u540c\u7684 LLM \u662f\u6709\u6548\u7684\u3002", "conclusion": "\u8be5\u7814\u7a76\u6df1\u5165\u4e86\u89e3\u4e86 LLM \u5bf9\u58f0\u660e\u5339\u914d\u7684\u7406\u89e3\u3002"}}
{"id": "2510.23883", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23883", "abs": "https://arxiv.org/abs/2510.23883", "authors": ["Shrestha Datta", "Shahriar Kabir Nahin", "Anshuman Chhabra", "Prasant Mohapatra"], "title": "Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges", "comment": null, "summary": "Agentic AI systems powered by large language models (LLMs) and endowed with\nplanning, tool use, memory, and autonomy, are emerging as powerful, flexible\nplatforms for automation. Their ability to autonomously execute tasks across\nweb, software, and physical environments creates new and amplified security\nrisks, distinct from both traditional AI safety and conventional software\nsecurity. This survey outlines a taxonomy of threats specific to agentic AI,\nreviews recent benchmarks and evaluation methodologies, and discusses defense\nstrategies from both technical and governance perspectives. We synthesize\ncurrent research and highlight open challenges, aiming to support the\ndevelopment of secure-by-design agent systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u6982\u8ff0\u4e86\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684 Agentic AI \u7cfb\u7edf\u5e26\u6765\u7684\u65b0\u578b\u5b89\u5168\u98ce\u9669\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u5177\u6709\u89c4\u5212\u3001\u5de5\u5177\u4f7f\u7528\u3001\u8bb0\u5fc6\u548c\u81ea\u4e3b\u6027\u3002", "motivation": "\u81ea\u4e3b\u6267\u884c\u4efb\u52a1\u7684\u80fd\u529b\u5e26\u6765\u4e86\u65b0\u7684\u5b89\u5168\u98ce\u9669\uff0c\u4e0d\u540c\u4e8e\u4f20\u7edf\u4eba\u5de5\u667a\u80fd\u5b89\u5168\u548c\u4f20\u7edf\u8f6f\u4ef6\u5b89\u5168\u3002", "method": "\u672c\u7814\u7a76\u6982\u8ff0\u4e86 Agentic AI \u7279\u6709\u7684\u5a01\u80c1\u5206\u7c7b\uff0c\u56de\u987e\u4e86\u8fd1\u671f\u7684\u57fa\u51c6\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u4ece\u6280\u672f\u548c\u6cbb\u7406\u89d2\u5ea6\u8ba8\u8bba\u4e86\u9632\u5fa1\u7b56\u7565\u3002", "result": "\u5408\u6210\u4e86\u5f53\u524d\u7684\u7814\u7a76\uff0c\u5f3a\u8c03\u4e86\u5f00\u653e\u7684\u6311\u6218\u3002", "conclusion": "\u65e8\u5728\u652f\u6301\u5b89\u5168\u8bbe\u8ba1\u4ee3\u7406\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.23960", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23960", "abs": "https://arxiv.org/abs/2510.23960", "authors": ["Peiyang Xu", "Minzhou Pan", "Zhaorun Chen", "Shuang Yang", "Chaowei Xiao", "Bo Li"], "title": "SafeVision: Efficient Image Guardrail with Robust Policy Adherence and Explainability", "comment": "42 pages, 9 figures", "summary": "With the rapid proliferation of digital media, the need for efficient and\ntransparent safeguards against unsafe content is more critical than ever.\nTraditional image guardrail models, constrained by predefined categories, often\nmisclassify content due to their pure feature-based learning without semantic\nreasoning. Moreover, these models struggle to adapt to emerging threats,\nrequiring costly retraining for new threats. To address these limitations, we\nintroduce SafeVision, a novel image guardrail that integrates human-like\nreasoning to enhance adaptability and transparency. Our approach incorporates\nan effective data collection and generation framework, a policy-following\ntraining pipeline, and a customized loss function. We also propose a diverse QA\ngeneration and training strategy to enhance learning effectiveness. SafeVision\ndynamically aligns with evolving safety policies at inference time, eliminating\nthe need for retraining while ensuring precise risk assessments and\nexplanations. Recognizing the limitations of existing unsafe image benchmarks,\nwhich either lack granularity or cover limited risks, we introduce VisionHarm,\na high-quality dataset comprising two subsets: VisionHarm Third-party\n(VisionHarm-T) and VisionHarm Comprehensive(VisionHarm-C), spanning diverse\nharmful categories. Through extensive experiments, we show that SafeVision\nachieves state-of-the-art performance on different benchmarks. SafeVision\noutperforms GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while\nbeing over 16x faster. SafeVision sets a comprehensive, policy-following, and\nexplainable image guardrail with dynamic adaptation to emerging threats.", "AI": {"tldr": "SafeVision: A novel image guardrail that integrates human-like reasoning to enhance adaptability and transparency.", "motivation": "Traditional image guardrail models often misclassify content and struggle to adapt to emerging threats.", "method": "An effective data collection and generation framework, a policy-following training pipeline, and a customized loss function are used. A diverse QA generation and training strategy to enhance learning effectiveness is proposed.", "result": "SafeVision achieves state-of-the-art performance on different benchmarks, outperforming GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while being over 16x faster.", "conclusion": "SafeVision sets a comprehensive, policy-following, and explainable image guardrail with dynamic adaptation to emerging threats."}}
{"id": "2510.23635", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23635", "abs": "https://arxiv.org/abs/2510.23635", "authors": ["Andrea Bontempelli", "Matteo Busso", "Leonardo Javier Malcotti", "Fausto Giunchiglia"], "title": "Help the machine to help you: an evaluation in the wild of egocentric data cleaning via skeptical learning", "comment": null, "summary": "Any digital personal assistant, whether used to support task performance,\nanswer questions, or manage work and daily life, including fitness schedules,\nrequires high-quality annotations to function properly. However, user\nannotations, whether actively produced or inferred from context (e.g., data\nfrom smartphone sensors), are often subject to errors and noise. Previous\nresearch on Skeptical Learning (SKEL) addressed the issue of noisy labels by\ncomparing offline active annotations with passive data, allowing for an\nevaluation of annotation accuracy. However, this evaluation did not include\nconfirmation from end-users, the best judges of their own context. In this\nstudy, we evaluate SKEL's performance in real-world conditions with actual\nusers who can refine the input labels based on their current perspectives and\nneeds. The study involves university students using the iLog mobile application\non their devices over a period of four weeks. The results highlight the\nchallenges of finding the right balance between user effort and data quality,\nas well as the potential benefits of using SKEL, which include reduced\nannotation effort and improved quality of collected data.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u4f7f\u7528Skeptical Learning (SKEL) \u63d0\u9ad8\u6570\u5b57\u4e2a\u4eba\u52a9\u7406\u6ce8\u91ca\u8d28\u91cf\u7684\u65b9\u6cd5\u3002", "motivation": "\u6570\u5b57\u4e2a\u4eba\u52a9\u7406\u9700\u8981\u9ad8\u8d28\u91cf\u7684\u6ce8\u91ca\u624d\u80fd\u6b63\u5e38\u8fd0\u884c\uff0c\u4f46\u7528\u6237\u6ce8\u91ca\u5e38\u5e38\u5305\u542b\u9519\u8bef\u548c\u566a\u97f3\u3002\u4ee5\u5f80\u7814\u7a76\u672a\u80fd\u5305\u542b\u6700\u7ec8\u7528\u6237\u7684\u786e\u8ba4\u3002", "method": "\u901a\u8fc7\u8ba9\u5927\u5b66\u751f\u5728\u56db\u5468\u5185\u4f7f\u7528iLog\u79fb\u52a8\u5e94\u7528\uff0c\u5e76\u6839\u636e\u4ed6\u4eec\u7684\u89c6\u89d2\u548c\u9700\u6c42\u7ec6\u5316\u8f93\u5165\u6807\u7b7e\uff0c\u6765\u8bc4\u4f30SKEL\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u7528\u6237\u4ed8\u51fa\u548c\u6570\u636e\u8d28\u91cf\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u4f7f\u7528SKEL\u7684\u6f5c\u5728\u597d\u5904\uff0c\u5305\u62ec\u51cf\u5c11\u6ce8\u91ca\u5de5\u4f5c\u548c\u63d0\u9ad8\u6536\u96c6\u6570\u636e\u7684\u8d28\u91cf\u3002", "conclusion": "SKEL\u5728\u51cf\u5c11\u6ce8\u91ca\u5de5\u4f5c\u548c\u63d0\u9ad8\u6570\u636e\u8d28\u91cf\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u5728\u7528\u6237\u4ed8\u51fa\u548c\u6570\u636e\u8d28\u91cf\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002"}}
{"id": "2510.23941", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23941", "abs": "https://arxiv.org/abs/2510.23941", "authors": ["Soham Satyadharma", "Fatemeh Sheikholeslami", "Swati Kaul", "Aziz Umit Batur", "Suleiman A. Khan"], "title": "Auto prompting without training labels: An LLM cascade for product quality assessment in e-commerce catalogs", "comment": null, "summary": "We introduce a novel, training free cascade for auto-prompting Large Language\nModels (LLMs) to assess product quality in e-commerce. Our system requires no\ntraining labels or model fine-tuning, instead automatically generating and\nrefining prompts for evaluating attribute quality across tens of thousands of\nproduct category-attribute pairs. Starting from a seed of human-crafted\nprompts, the cascade progressively optimizes instructions to meet\ncatalog-specific requirements. This approach bridges the gap between general\nlanguage understanding and domain-specific knowledge at scale in complex\nindustrial catalogs. Our extensive empirical evaluations shows the auto-prompt\ncascade improves precision and recall by $8-10\\%$ over traditional\nchain-of-thought prompting. Notably, it achieves these gains while reducing\ndomain expert effort from 5.1 hours to 3 minutes per attribute - a $99\\%$\nreduction. Additionally, the cascade generalizes effectively across five\nlanguages and multiple quality assessment tasks, consistently maintaining\nperformance gains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u514d\u8bad\u7ec3\u7684\u7ea7\u8054\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u63d0\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee5\u8bc4\u4f30\u7535\u5b50\u5546\u52a1\u4e2d\u7684\u4ea7\u54c1\u8d28\u91cf\u3002", "motivation": "\u5f25\u5408\u901a\u7528\u8bed\u8a00\u7406\u89e3\u548c\u590d\u6742\u5de5\u4e1a\u76ee\u5f55\u4e2d\u5927\u89c4\u6a21\u7279\u5b9a\u9886\u57df\u77e5\u8bc6\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u4ece\u4eba\u5de5\u5236\u4f5c\u7684\u63d0\u793a\u7684\u79cd\u5b50\u5f00\u59cb\uff0c\u7ea7\u8054\u9010\u6b65\u4f18\u5316\u6307\u4ee4\u4ee5\u6ee1\u8db3\u76ee\u5f55\u7279\u5b9a\u8981\u6c42\u3002", "result": "\u81ea\u52a8\u63d0\u793a\u7ea7\u8054\u6bd4\u4f20\u7edf\u7684\u601d\u7ef4\u94fe\u63d0\u793a\u63d0\u9ad8\u4e86 8-10% \u7684\u7cbe\u786e\u5ea6\u548c\u53ec\u56de\u7387\uff0c\u540c\u65f6\u5c06\u9886\u57df\u4e13\u5bb6\u6bcf\u4e2a\u5c5e\u6027\u7684\u5de5\u4f5c\u91cf\u4ece 5.1 \u5c0f\u65f6\u51cf\u5c11\u5230 3 \u5206\u949f\uff0c\u51cf\u5c11\u4e86 99%\u3002", "conclusion": "\u8be5\u7ea7\u8054\u6709\u6548\u5730\u63a8\u5e7f\u5230\u4e94\u79cd\u8bed\u8a00\u548c\u591a\u4e2a\u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\uff0c\u59cb\u7ec8\u4fdd\u6301\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.23925", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23925", "abs": "https://arxiv.org/abs/2510.23925", "authors": ["Guohao Sun", "Hang Hua", "Jian Wang", "Jiebo Luo", "Sohail Dianat", "Majid Rabbani", "Raghuveer Rao", "Zhiqiang Tao"], "title": "Latent Chain-of-Thought for Visual Reasoning", "comment": "NeurIPS 2025", "summary": "Chain-of-thought (CoT) reasoning is critical for improving the\ninterpretability and reliability of Large Vision-Language Models (LVLMs).\nHowever, existing training algorithms such as SFT, PPO, and GRPO may not\ngeneralize well across unseen reasoning tasks and heavily rely on a biased\nreward model. To address this challenge, we reformulate reasoning in LVLMs as\nposterior inference and propose a scalable training algorithm based on\namortized variational inference. By leveraging diversity-seeking reinforcement\nlearning algorithms, we introduce a novel sparse reward function for\ntoken-level learning signals that encourage diverse, high-likelihood latent\nCoT, overcoming deterministic sampling limitations and avoiding reward hacking.\nAdditionally, we implement a Bayesian inference-scaling strategy that replaces\ncostly Best-of-N and Beam Search with a marginal likelihood to efficiently rank\noptimal rationales and answers. We empirically demonstrate that the proposed\nmethod enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in\nterms of effectiveness, generalization, and interpretability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u644a\u53d8\u5206\u63a8\u7406\u7684\u53ef\u6269\u5c55\u8bad\u7ec3\u7b97\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(LVLM)\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8bad\u7ec3\u7b97\u6cd5\u6cdb\u5316\u6027\u5dee\uff0c\u4f9d\u8d56\u6709\u504f\u5956\u52b1\u6a21\u578b\u3002", "method": "\u5c06LVLM\u4e2d\u7684\u63a8\u7406\u91cd\u6784\u4e3a\u540e\u9a8c\u63a8\u7406\uff0c\u5e76\u5229\u7528\u5bfb\u6c42\u591a\u6837\u6027\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5f15\u5165\u65b0\u7684\u7a00\u758f\u5956\u52b1\u51fd\u6570\uff0c\u9f13\u52b1\u591a\u6837\u5316\u3001\u9ad8\u53ef\u80fd\u6027\u7684\u6f5c\u5728CoT\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e03\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u589e\u5f3a\u4e86\u6700\u5148\u8fdb\u7684LVLM\uff0c\u5728\u6709\u6548\u6027\u3001\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u9f13\u52b1\u591a\u6837\u5316\u7684\u63a8\u7406\u8def\u5f84\u548c\u907f\u514d\u5956\u52b1\u504f\u5dee\uff0c\u63d0\u9ad8\u4e86LVLM\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2510.23968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23968", "abs": "https://arxiv.org/abs/2510.23968", "authors": ["Andriy Myronenko", "Dong Yang", "Baris Turkbey", "Mariam Aboian", "Sena Azamat", "Esra Akcicek", "Hongxu Yin", "Pavlo Molchanov", "Marc Edgar", "Yufan He", "Pengfei Guo", "Yucheng Tang", "Daguang Xu"], "title": "Reasoning Visual Language Model for Chest X-Ray Analysis", "comment": "NV-Reason-CXR-3B", "summary": "Vision-language models (VLMs) have shown strong promise for medical image\nanalysis, but most remain opaque, offering predictions without the transparent,\nstepwise reasoning clinicians rely on. We present a framework that brings\nchain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by\nreasoning-first training paradigms, our approach is designed to learn how\nexperts reason, not just what they conclude, by aligning intermediate steps\nwith observable image evidence and radiology workflow. Beyond accuracy, the\nexplicit reasoning traces support clinical auditability: they reveal why a\nconclusion was reached, which alternatives were considered, and where\nuncertainty remains, enabling quality assurance, error analysis, and safer\nhuman-AI collaboration.\n  Our model couples high-fidelity visual encoding with a two-stage training\nrecipe: a reasoning-style supervised fine-tuning (SFT) followed by\nreinforcement learning (RL) that uses verifiable rewards over a list of X-ray\nabnormalities. The model outputs reasoning that mirrors radiologists systematic\nthought process, uncertainty, and differential diagnosis. In\nout-of-distribution evaluation, the approach achieves competitive multi-label\nclassification while improving interpretability. In a reader study with expert\nradiologists, full reasoning traces increased confidence, supported error\nauditing, and reduced time to finalize reports. We release code and the model\nNV-Reason-CXR-3B to support community progress toward trustworthy, explainable\nAI in chest radiography and other medical imaging tasks where reasoning quality\nis as critical as prediction quality.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u80f8\u90e8X\u5149\u7247\u89e3\u91ca\u7684\u94fe\u5f0f\u601d\u8003(CoT)\u63a8\u7406\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7f3a\u4e4f\u900f\u660e\u7684\u3001\u9010\u6b65\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u8fd9\u4e0e\u4e34\u5e8a\u533b\u751f\u7684\u9700\u6c42\u4e0d\u7b26\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u4e2d\u95f4\u6b65\u9aa4\u4e0e\u53ef\u89c2\u5bdf\u7684\u56fe\u50cf\u8bc1\u636e\u548c\u653e\u5c04\u5b66\u5de5\u4f5c\u6d41\u7a0b\u8fdb\u884c\u5bf9\u9f50\uff0c\u5b66\u4e60\u4e13\u5bb6\u5982\u4f55\u63a8\u7406\u3002\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u9ad8\u4fdd\u771f\u89c6\u89c9\u7f16\u7801\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff1a\u63a8\u7406\u98ce\u683c\u7684\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5206\u5e03\u5916\u8bc4\u4f30\u4e2d\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u591a\u6807\u7b7e\u5206\u7c7b\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u3002\u4e13\u5bb6\u653e\u5c04\u79d1\u533b\u751f\u7684\u7814\u7a76\u8868\u660e\uff0c\u5b8c\u6574\u7684\u63a8\u7406\u8fc7\u7a0b\u63d0\u9ad8\u4e86\u4fe1\u5fc3\uff0c\u652f\u6301\u4e86\u9519\u8bef\u5ba1\u8ba1\uff0c\u5e76\u51cf\u5c11\u4e86\u5b8c\u6210\u62a5\u544a\u7684\u65f6\u95f4\u3002", "conclusion": "\u8be5\u7814\u7a76\u53d1\u5e03\u4e86\u4ee3\u7801\u548c\u6a21\u578b\uff0c\u4ee5\u652f\u6301\u793e\u533a\u5728\u80f8\u90e8X\u5149\u7247\u548c\u5176\u4ed6\u533b\u5b66\u6210\u50cf\u4efb\u52a1\u4e2d\u5b9e\u73b0\u53ef\u4fe1\u3001\u53ef\u89e3\u91ca\u7684AI\u3002"}}
{"id": "2510.23636", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23636", "abs": "https://arxiv.org/abs/2510.23636", "authors": ["Thaweerath Phisannupawong", "Joshua Julian Damanik", "Han-Lim Choi"], "title": "Flight Delay Prediction via Cross-Modality Adaptation of Large Language Models and Aircraft Trajectory Representation", "comment": "Preprint submitted to Aerospace Science and Technology (Elsevier) for\n  possible publication", "summary": "Flight delay prediction has become a key focus in air traffic management, as\ndelays highlight inefficiencies that impact overall network performance. This\npaper presents a lightweight large language model-based multimodal flight delay\nprediction, formulated from the perspective of air traffic controllers\nmonitoring aircraft delay after entering the terminal area. The approach\nintegrates trajectory representations with textual aeronautical information,\nincluding flight information, weather reports, and aerodrome notices, by\nadapting trajectory data into the language modality to capture airspace\nconditions. Experimental results show that the model consistently achieves\nsub-minute prediction error by effectively leveraging contextual information\nrelated to the sources of delay. The framework demonstrates that linguistic\nunderstanding, when combined with cross-modality adaptation of trajectory\ninformation, enhances delay prediction. Moreover, the approach shows\npracticality and scalability for real-world operations, supporting real-time\nupdates that refine predictions upon receiving new operational information.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u822a\u73ed\u5ef6\u8bef\u9884\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ece\u7a7a\u7ba1\u4eba\u5458\u7684\u89d2\u5ea6\u51fa\u53d1\uff0c\u7ed3\u5408\u8f68\u8ff9\u8868\u793a\u548c\u6587\u672c\u822a\u7a7a\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u4e9a\u5206\u949f\u7ea7\u7684\u9884\u6d4b\u8bef\u5dee\u3002", "motivation": "\u822a\u73ed\u5ef6\u8bef\u662f\u5f71\u54cd\u6574\u4f53\u7f51\u7edc\u6027\u80fd\u7684\u5173\u952e\u95ee\u9898\uff0c\u56e0\u6b64\u822a\u73ed\u5ef6\u8bef\u9884\u6d4b\u6210\u4e3a\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u7406\u7684\u5173\u952e\u7126\u70b9\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u8f68\u8ff9\u6570\u636e\u8f6c\u5316\u4e3a\u8bed\u8a00\u6a21\u6001\uff0c\u4ee5\u6355\u6349\u7a7a\u57df\u6761\u4ef6\uff0c\u5e76\u7ed3\u5408\u822a\u73ed\u4fe1\u606f\u3001\u5929\u6c14\u62a5\u544a\u548c\u673a\u573a\u516c\u544a\u7b49\u6587\u672c\u822a\u7a7a\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u5229\u7528\u4e0e\u5ef6\u8bef\u6765\u6e90\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u59cb\u7ec8\u5b9e\u73b0\u4e9a\u5206\u949f\u7ea7\u7684\u9884\u6d4b\u8bef\u5dee\u3002", "conclusion": "\u8be5\u6846\u67b6\u8bc1\u660e\u4e86\u8bed\u8a00\u7406\u89e3\u4e0e\u8f68\u8ff9\u4fe1\u606f\u7684\u8de8\u6a21\u6001\u9002\u5e94\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5ef6\u8bef\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u663e\u793a\u51fa\u5728\u5b9e\u9645\u64cd\u4f5c\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u652f\u6301\u5b9e\u65f6\u66f4\u65b0\uff0c\u4ece\u800c\u5728\u6536\u5230\u65b0\u7684\u64cd\u4f5c\u4fe1\u606f\u540e\u6539\u8fdb\u9884\u6d4b\u3002"}}
{"id": "2510.23946", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23946", "abs": "https://arxiv.org/abs/2510.23946", "authors": ["Tananun Songdechakraiwut"], "title": "Leveraging LLMs for Early Alzheimer's Prediction", "comment": null, "summary": "We present a connectome-informed LLM framework that encodes dynamic fMRI\nconnectivity as temporal sequences, applies robust normalization, and maps\nthese data into a representation suitable for a frozen pre-trained LLM for\nclinical prediction. Applied to early Alzheimer's detection, our method\nachieves sensitive prediction with error rates well below clinically recognized\nmargins, with implications for timely Alzheimer's intervention.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8fde\u63a5\u7ec4\u4fe1\u606f\u7684LLM\u6846\u67b6\uff0c\u7528\u4e8e\u4e34\u5e8a\u9884\u6d4b\uff0c\u5c24\u5176\u5173\u6ce8\u65e9\u671f\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u68c0\u6d4b\u3002", "motivation": "\u65e8\u5728\u63d0\u9ad8\u65e9\u671f\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u68c0\u6d4b\u7684\u7075\u654f\u5ea6\u548c\u51c6\u786e\u6027\uff0c\u4ee5\u4fbf\u53ca\u65f6\u5e72\u9884\u3002", "method": "\u8be5\u6846\u67b6\u5c06\u52a8\u6001fMRI\u8fde\u63a5\u6027\u7f16\u7801\u4e3a\u65f6\u95f4\u5e8f\u5217\uff0c\u5e94\u7528\u7a33\u5065\u7684\u6807\u51c6\u5316\u65b9\u6cd5\uff0c\u5e76\u5c06\u6570\u636e\u6620\u5c04\u5230\u9002\u5408\u9884\u8bad\u7ec3LLM\u7684\u8868\u793a\u5f62\u5f0f\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u65e9\u671f\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7075\u654f\u5ea6\u7684\u9884\u6d4b\uff0c\u8bef\u5dee\u7387\u8fdc\u4f4e\u4e8e\u4e34\u5e8a\u8ba4\u53ef\u7684\u8303\u56f4\u3002", "conclusion": "\u8be5\u7814\u7a76\u5bf9\u53ca\u65f6\u5e72\u9884\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.23942", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23942", "abs": "https://arxiv.org/abs/2510.23942", "authors": ["Sridhar Mahadevan"], "title": "Decentralized Causal Discovery using Judo Calculus", "comment": "54 pages", "summary": "We describe a theory and implementation of an intuitionistic decentralized\nframework for causal discovery using judo calculus, which is formally defined\nas j-stable causal inference using j-do-calculus in a topos of sheaves. In\nreal-world applications -- from biology to medicine and social science --\ncausal effects depend on regime (age, country, dose, genotype, or lab\nprotocol). Our proposed judo calculus formalizes this context dependence\nformally as local truth: a causal claim is proven true on a cover of regimes,\nnot everywhere at once. The Lawvere-Tierney modal operator j chooses which\nregimes are relevant; j-stability means the claim holds constructively and\nconsistently across that family. We describe an algorithmic and implementation\nframework for judo calculus, combining it with standard score-based,\nconstraint-based, and gradient-based causal discovery methods. We describe\nexperimental results on a range of domains, from synthetic to real-world\ndatasets from biology and economics. Our experimental results show the\ncomputational efficiency gained by the decentralized nature of sheaf-theoretic\ncausal discovery, as well as improved performance over classical causal\ndiscovery methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u67d4\u9053\u6f14\u7b97\u7684\u76f4\u89c9\u4e3b\u4e49\u5206\u6563\u5f0f\u56e0\u679c\u53d1\u73b0\u6846\u67b6\u7684\u7406\u8bba\u548c\u5b9e\u73b0\u3002", "motivation": "\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u4e2d\uff0c\u56e0\u679c\u6548\u5e94\u53d6\u51b3\u4e8e\u72b6\u6001(\u5e74\u9f84\u3001\u56fd\u5bb6\u3001\u5242\u91cf\u3001\u57fa\u56e0\u578b\u6216\u5b9e\u9a8c\u5ba4\u534f\u8bae)\u3002", "method": "\u5c06\u67d4\u9053\u6f14\u7b97\u4e0e\u57fa\u4e8e\u5206\u6570\u7684\u3001\u57fa\u4e8e\u7ea6\u675f\u7684\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u76f8\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0csheaf-theoretic\u56e0\u679c\u53d1\u73b0\u7684\u5206\u6563\u6027\u8d28\u83b7\u5f97\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u4e14\u6bd4\u7ecf\u5178\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u6709\u6539\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u67d4\u9053\u6f14\u7b97\u5c06\u8fd9\u79cd\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u5f62\u5f0f\u5316\u4e3a\u5c40\u90e8\u771f\u7406\uff1a\u56e0\u679c\u58f0\u660e\u5728\u4e00\u7cfb\u5217\u72b6\u6001\u4e0a\u88ab\u8bc1\u660e\u4e3a\u771f\uff0c\u800c\u4e0d\u662f\u4e00\u6b21\u5728\u6240\u6709\u5730\u65b9\u90fd\u4e3a\u771f\u3002"}}
{"id": "2510.23978", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23978", "abs": "https://arxiv.org/abs/2510.23978", "authors": ["Kazutoshi Akita", "Norimichi Ukita"], "title": "Efficient Cost-and-Quality Controllable Arbitrary-scale Super-resolution with Fourier Constraints", "comment": "9 pages", "summary": "Cost-and-Quality (CQ) controllability in arbitrary-scale super-resolution is\ncrucial. Existing methods predict Fourier components one by one using a\nrecurrent neural network. However, this approach leads to performance\ndegradation and inefficiency due to independent prediction. This paper proposes\npredicting multiple components jointly to improve both quality and efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u9884\u6d4b\u591a\u4e2a\u5085\u91cc\u53f6\u5206\u91cf\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9010\u4e2a\u9884\u6d4b\u5085\u91cc\u53f6\u5206\u91cf\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u6548\u7387\u4f4e\u4e0b\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u4efb\u610f\u5c3a\u5ea6\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u4e2d\u6210\u672c\u548c\u8d28\u91cf\u53ef\u63a7\u6027\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8054\u5408\u9884\u6d4b\u591a\u4e2a\u5206\u91cf\u3002", "result": "\u63d0\u9ad8\u4e86\u8d28\u91cf\u548c\u6548\u7387\u3002", "conclusion": "\u8054\u5408\u9884\u6d4b\u591a\u4e2a\u5085\u91cc\u53f6\u5206\u91cf\u7684\u65b9\u6cd5\u53ef\u4ee5\u63d0\u9ad8\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2510.23637", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.23637", "abs": "https://arxiv.org/abs/2510.23637", "authors": ["Job Petrov\u010di\u010d", "David Eliecer Narvaez Denis", "Ljup\u010do Todorovski"], "title": "Combining Textual and Structural Information for Premise Selection in Lean", "comment": null, "summary": "Premise selection is a key bottleneck for scaling theorem proving in large\nformal libraries. Yet existing language-based methods often treat premises in\nisolation, ignoring the web of dependencies that connects them. We present a\ngraph-augmented approach that combines dense text embeddings of Lean\nformalizations with graph neural networks over a heterogeneous dependency graph\ncapturing both state--premise and premise--premise relations. On the LeanDojo\nBenchmark, our method outperforms the ReProver language-based baseline by over\n25% across standard retrieval metrics. These results demonstrate the power of\nrelational information for more effective premise selection.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u5b9a\u7406\u8bc1\u660e\u524d\u63d0\u9009\u62e9\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86Lean\u5f62\u5f0f\u5316\u7684\u5bc6\u96c6\u6587\u672c\u5d4c\u5165\u548c\u5f02\u6784\u4f9d\u8d56\u56fe\u4e0a\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u8bed\u8a00\u7684\u65b9\u6cd5\u901a\u5e38\u5b64\u7acb\u5730\u5904\u7406\u524d\u63d0\uff0c\u5ffd\u7565\u4e86\u8fde\u63a5\u5b83\u4eec\u7684\u4f9d\u8d56\u5173\u7cfb\u7f51\u7edc\uff0c\u8fd9\u963b\u788d\u4e86\u5927\u578b\u5f62\u5f0f\u5e93\u4e2d\u5b9a\u7406\u8bc1\u660e\u7684\u6269\u5c55\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86Lean\u5f62\u5f0f\u5316\u7684\u5bc6\u96c6\u6587\u672c\u5d4c\u5165\u548c\u5f02\u6784\u4f9d\u8d56\u56fe\u4e0a\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u8be5\u56fe\u6355\u83b7\u4e86\u72b6\u6001-\u524d\u63d0\u548c\u524d\u63d0-\u524d\u63d0\u5173\u7cfb\u3002", "result": "\u5728LeanDojo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6807\u51c6\u68c0\u7d22\u6307\u6807\u4e0a\u4f18\u4e8eReProver\u8bed\u8a00\u57fa\u7ebf\u8d85\u8fc725%\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8bc1\u660e\u4e86\u5173\u7cfb\u4fe1\u606f\u5bf9\u4e8e\u66f4\u6709\u6548\u7684\u524d\u63d0\u9009\u62e9\u7684\u529b\u91cf\u3002"}}
{"id": "2510.23949", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23949", "abs": "https://arxiv.org/abs/2510.23949", "authors": ["Kyomin Hwang", "Hyeonjin Kim", "Seungyeon Kim", "Sunghyun Wee", "Nojun Kwak"], "title": "Uncovering the Potential Risks in Unlearning: Danger of English-only Unlearning in Multilingual LLMs", "comment": null, "summary": "There have been a couple of studies showing that attempting to erase\nmultilingual knowledge using only English data is insufficient for multilingual\nLLMs. However, their analyses remain highly performance-oriented. In this\npaper, we switch the point of view to evaluation, and address an additional\nblind spot which reveals itself when the multilingual LLM is fully finetuned\nwith parallel multilingual dataset before unlearning. Here, language confusion\noccurs whereby a model responds in language different from that of the input\nprompt. Language confusion is a problematic phenomenon in unlearning, causing\nthe standard reference-based metrics to fail. We tackle this phenomenon in\nthree steps: (1) introduce N-gram-based Language-Mix (N-Mix) score to\nquantitatively show the language confusion is pervasive and consistent in\nmultilingual LLMs, (2) demonstrate that reference-based metrics result in false\nnegatives when N-Mix score is high, and(3) suggest the need of new type of\nunlearning evaluation that can directly assess the content of the generated\nsentences. We call this type of metrics as semantic-based metric.", "AI": {"tldr": "\u591a\u8bed\u8a00LLM\u4ec5\u7528\u82f1\u8bed\u6570\u636e\u64e6\u9664\u77e5\u8bc6\u4e0d\u591f\u5145\u5206\uff0c\u4e14\u73b0\u6709\u5206\u6790\u4fa7\u91cd\u6027\u80fd\u3002\u672c\u6587\u5173\u6ce8\u8bc4\u4f30\u89d2\u5ea6\uff0c\u63ed\u793a\u4e86\u5728\u7528\u591a\u8bed\u8a00\u6570\u636e\u96c6\u5b8c\u5168\u5fae\u8c03\u540e\u8fdb\u884cunlearning\u65f6\u51fa\u73b0\u7684\u8bed\u8a00\u6df7\u6dc6\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u591a\u8bed\u8a00LLM\u7684\u77e5\u8bc6\u64e6\u9664\u7814\u7a76\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u5b8c\u5168\u5fae\u8c03\u540e\u8fdb\u884cunlearning\u65f6\uff0c\u6a21\u578b\u4f1a\u51fa\u73b0\u8bed\u8a00\u6df7\u6dc6\uff0c\u5373\u4ee5\u975e\u8f93\u5165\u63d0\u793a\u7684\u8bed\u8a00\u4f5c\u7b54\u3002", "method": "1. \u5f15\u5165\u57fa\u4e8eN-gram\u7684\u8bed\u8a00\u6df7\u5408(N-Mix)\u5f97\u5206\uff0c\u91cf\u5316\u5c55\u793a\u591a\u8bed\u8a00LLM\u4e2d\u666e\u904d\u5b58\u5728\u7684\u8bed\u8a00\u6df7\u6dc6\u73b0\u8c61\u30022. \u8bba\u8bc1\u4e86\u5f53N-Mix\u5f97\u5206\u8f83\u9ad8\u65f6\uff0c\u57fa\u4e8e\u53c2\u8003\u7684\u6307\u6807\u4f1a\u5bfc\u81f4\u5047\u9634\u6027\u30023. \u5efa\u8bae\u9700\u8981\u4e00\u79cd\u65b0\u578b\u7684unlearning\u8bc4\u4f30\u65b9\u6cd5\uff0c\u53ef\u4ee5\u76f4\u63a5\u8bc4\u4f30\u751f\u6210\u53e5\u5b50\u7684\u5185\u5bb9\uff0c\u5373\u57fa\u4e8e\u8bed\u4e49\u7684\u6307\u6807\u3002", "result": "N-Mix\u5f97\u5206\u80fd\u591f\u6709\u6548\u91cf\u5316\u8bed\u8a00\u6df7\u6dc6\u73b0\u8c61\uff0c\u5e76\u63ed\u793a\u4e86\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u5728\u5b58\u5728\u8bed\u8a00\u6df7\u6dc6\u65f6\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u65b0\u7684\u3001\u57fa\u4e8e\u8bed\u4e49\u7684\u8bc4\u4f30\u6307\u6807\u6765\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u591a\u8bed\u8a00LLM\u7684unlearning\u6548\u679c\u3002"}}
{"id": "2510.23965", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23965", "abs": "https://arxiv.org/abs/2510.23965", "authors": ["Aymane El Gadarri", "Ali Aouad", "Vivek F. Farias"], "title": "The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity", "comment": null, "summary": "Traditional LLM alignment methods are vulnerable to heterogeneity in human\npreferences. Fitting a na\\\"ive probabilistic model to pairwise comparison data\n(say over prompt-completion pairs) yields an inconsistent estimate of the\npopulation-average utility -a canonical measure of social welfare. We propose a\nnew method, dubbed the sign estimator, that provides a simple, provably\nconsistent, and efficient estimator by replacing cross-entropy with binary\nclassification loss in the aggregation step. This simple modification recovers\nconsistent ordinal alignment under mild assumptions and achieves the first\npolynomial finite-sample error bounds in this setting. In realistic simulations\nof LLM alignment using digital twins, the sign estimator substantially reduces\npreference distortion over a panel of simulated personas, cutting (angular)\nestimation error by nearly 35% and decreasing disagreement with true population\npreferences from 12% to 8% compared to standard RLHF. Our method also compares\nfavorably to panel data heuristics that explicitly model user heterogeneity and\nrequire tracking individual-level preference data-all while maintaining the\nimplementation simplicity of existing LLM alignment pipelines.", "AI": {"tldr": "\u4f20\u7edfLLM\u5bf9\u9f50\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230\u4eba\u7c7b\u504f\u597d\u5f02\u8d28\u6027\u7684\u5f71\u54cd\u3002\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u79f0\u4e3a\u7b26\u53f7\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u5728\u805a\u5408\u6b65\u9aa4\u4e2d\u7528\u4e8c\u5143\u5206\u7c7b\u635f\u5931\u4ee3\u66ff\u4ea4\u53c9\u71b5\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u3001\u53ef\u8bc1\u660e\u4e00\u81f4\u4e14\u9ad8\u6548\u7684\u4f30\u8ba1\u5668\u3002", "motivation": "\u73b0\u6709\u7684\u6982\u7387\u6a21\u578b\u5728\u5904\u7406\u6210\u5bf9\u6bd4\u8f83\u6570\u636e\u65f6\uff0c\u5bf9\u793e\u4f1a\u798f\u5229\u7684\u5178\u578b\u8861\u91cf\u6807\u51c6\u2014\u2014\u7fa4\u4f53\u5e73\u5747\u6548\u7528\u7684\u4f30\u8ba1\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51fa\u4e86\u7b26\u53f7\u4f30\u8ba1\u5668\uff0c\u7528\u4e8c\u5143\u5206\u7c7b\u635f\u5931\u4ee3\u66ff\u4ea4\u53c9\u71b5\u3002", "result": "\u5728LLM\u5bf9\u9f50\u7684\u6a21\u62df\u4e2d\uff0c\u7b26\u53f7\u4f30\u8ba1\u5668\u663e\u8457\u51cf\u5c11\u4e86\u504f\u597d\u626d\u66f2\uff0c\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e\u4e86\u8fd135%\uff0c\u4e0e\u771f\u5b9e\u7fa4\u4f53\u504f\u597d\u4e0d\u4e00\u81f4\u4ece12%\u964d\u4f4e\u52308%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5b9e\u73b0\u7b80\u5355\u6027\u7684\u540c\u65f6\uff0c\u6027\u80fd\u4f18\u4e8e\u663e\u5f0f\u5efa\u6a21\u7528\u6237\u5f02\u8d28\u6027\u7684\u9762\u677f\u6570\u636e\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002"}}
{"id": "2510.23981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23981", "abs": "https://arxiv.org/abs/2510.23981", "authors": ["Jiaqi Yan", "Ruilong Ren", "Jingren Liu", "Shuning Xu", "Ling Wang", "Yiheng Wang", "Yun Wang", "Long Zhang", "Xiangyu Chen", "Changzhi Sun", "Jixiang Luo", "Dell Zhang", "Hao Sun", "Chi Zhang", "Xuelong Li"], "title": "TeleEgo: Benchmarking Egocentric AI Assistants in the Wild", "comment": null, "summary": "Egocentric AI assistants in real-world settings must process multi-modal\ninputs (video, audio, text), respond in real time, and retain evolving\nlong-term memory. However, existing benchmarks typically evaluate these\nabilities in isolation, lack realistic streaming scenarios, or support only\nshort-term tasks. We introduce \\textbf{TeleEgo}, a long-duration, streaming,\nomni-modal benchmark for evaluating egocentric AI assistants in realistic daily\ncontexts. The dataset features over 14 hours per participant of synchronized\negocentric video, audio, and text across four domains: work \\& study, lifestyle\n\\& routines, social activities, and outings \\& culture. All data is aligned on\na unified global timeline and includes high-quality visual narrations and\nspeech transcripts, curated through human refinement.TeleEgo defines 12\ndiagnostic subtasks across three core capabilities: Memory (recalling past\nevents), Understanding (interpreting the current moment), and Cross-Memory\nReasoning (linking distant events). It contains 3,291 human-verified QA items\nspanning multiple question formats (single-choice, binary, multi-choice, and\nopen-ended), evaluated strictly in a streaming setting. We propose two key\nmetrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess\ncorrectness, temporal responsiveness, and long-term retention. TeleEgo provides\na realistic and comprehensive evaluation to advance the development of\npractical AI assistants.", "AI": {"tldr": "TeleEgo: A comprehensive, long-duration, streaming, omni-modal benchmark for egocentric AI assistants in realistic daily contexts.", "motivation": "Existing benchmarks evaluate abilities in isolation, lack realistic streaming scenarios, or support only short-term tasks.", "method": "A new benchmark dataset called TeleEgo is introduced, featuring synchronized egocentric video, audio, and text across four domains. Defines 12 diagnostic subtasks across three core capabilities: Memory, Understanding, and Cross-Memory Reasoning. Contains 3,291 human-verified QA items.", "result": "Two key metrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess correctness, temporal responsiveness, and long-term retention.", "conclusion": "TeleEgo provides a realistic and comprehensive evaluation to advance the development of practical AI assistants."}}
{"id": "2510.23639", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.23639", "abs": "https://arxiv.org/abs/2510.23639", "authors": ["Jonathan Amar", "Edward Liu", "Alessandra Breschi", "Liangliang Zhang", "Pouya Kheradpour", "Sylvia Li", "Lisa Soleymani Lehmann", "Alessandro Giulianelli", "Matt Edwards", "Yugang Jia", "David Nola", "Raghav Mani", "Pankaj Vats", "Jesse Tetreault", "T. J. Chen", "Cory Y. McLean"], "title": "Integrating Genomics into Multimodal EHR Foundation Models", "comment": null, "summary": "This paper introduces an innovative Electronic Health Record (EHR) foundation\nmodel that integrates Polygenic Risk Scores (PRS) as a foundational data\nmodality, moving beyond traditional EHR-only approaches to build more holistic\nhealth profiles. Leveraging the extensive and diverse data from the All of Us\n(AoU) Research Program, this multimodal framework aims to learn complex\nrelationships between clinical data and genetic predispositions. The\nmethodology extends advancements in generative AI to the EHR foundation model\nspace, enhancing predictive capabilities and interpretability. Evaluation on\nAoU data demonstrates the model's predictive value for the onset of various\nconditions, particularly Type 2 Diabetes (T2D), and illustrates the interplay\nbetween PRS and EHR data. The work also explores transfer learning for custom\nclassification tasks, showcasing the architecture's versatility and efficiency.\nThis approach is pivotal for unlocking new insights into disease prediction,\nproactive health management, risk stratification, and personalized treatment\nstrategies, laying the groundwork for more personalized, equitable, and\nactionable real-world evidence generation in healthcare.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55 (EHR) \u57fa\u7840\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u96c6\u6210\u4e86\u591a\u57fa\u56e0\u98ce\u9669\u8bc4\u5206 (PRS) \u4f5c\u4e3a\u57fa\u7840\u6570\u636e\u6a21\u5f0f\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u4ec5 EHR \u65b9\u6cd5\uff0c\u4ee5\u6784\u5efa\u66f4\u5168\u9762\u7684\u5065\u5eb7\u6982\u51b5\u3002", "motivation": "\u5229\u7528\u201cAll of Us\u201d (AoU) \u7814\u7a76\u8ba1\u5212\u4e2d\u7684\u5e7f\u6cdb\u4e14\u591a\u6837\u5316\u7684\u6570\u636e\uff0c\u6b64\u591a\u6a21\u5f0f\u6846\u67b6\u65e8\u5728\u4e86\u89e3\u4e34\u5e8a\u6570\u636e\u548c\u9057\u4f20\u6613\u611f\u6027\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u8fdb\u6b65\u6269\u5c55\u5230 EHR \u57fa\u7840\u6a21\u578b\u7a7a\u95f4\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u9884\u6d4b\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728 AoU \u6570\u636e\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5bf9\u5404\u79cd\u75be\u75c5\uff08\u5c24\u5176\u662f 2 \u578b\u7cd6\u5c3f\u75c5 (T2D)\uff09\u7684\u53d1\u75c5\u5177\u6709\u9884\u6d4b\u4ef7\u503c\uff0c\u5e76\u8bf4\u660e\u4e86 PRS \u548c EHR \u6570\u636e\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u8fd9\u9879\u5de5\u4f5c\u8fd8\u63a2\u7d22\u4e86\u7528\u4e8e\u81ea\u5b9a\u4e49\u5206\u7c7b\u4efb\u52a1\u7684\u8fc1\u79fb\u5b66\u4e60\uff0c\u5c55\u793a\u4e86\u8be5\u67b6\u6784\u7684\u591a\u529f\u80fd\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u8fd9\u79cd\u65b9\u6cd5\u5bf9\u4e8e\u91ca\u653e\u5bf9\u75be\u75c5\u9884\u6d4b\u3001\u4e3b\u52a8\u5065\u5eb7\u7ba1\u7406\u3001\u98ce\u9669\u5206\u5c42\u548c\u4e2a\u6027\u5316\u6cbb\u7597\u7b56\u7565\u7684\u65b0\u89c1\u89e3\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u533b\u7597\u4fdd\u5065\u4e2d\u66f4\u4e2a\u6027\u5316\u3001\u516c\u5e73\u548c\u53ef\u64cd\u4f5c\u7684\u771f\u5b9e\u4e16\u754c\u8bc1\u636e\u751f\u6210\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.23995", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23995", "abs": "https://arxiv.org/abs/2510.23995", "authors": ["Mengzhou Sun", "Sendong Zhao", "Jianyu Chen", "Haochun Wang", "Bin Qin"], "title": "M-Eval: A Heterogeneity-Based Framework for Multi-evidence Validation in Medical RAG Systems", "comment": null, "summary": "Retrieval-augmented Generation (RAG) has demonstrated potential in enhancing\nmedical question-answering systems through the integration of large language\nmodels (LLMs) with external medical literature. LLMs can retrieve relevant\nmedical articles to generate more professional responses efficiently. However,\ncurrent RAG applications still face problems. They generate incorrect\ninformation, such as hallucinations, and they fail to use external knowledge\ncorrectly. To solve these issues, we propose a new method named M-Eval. This\nmethod is inspired by the heterogeneity analysis approach used in\nEvidence-Based Medicine (EBM). Our approach can check for factual errors in RAG\nresponses using evidence from multiple sources. First, we extract additional\nmedical literature from external knowledge bases. Then, we retrieve the\nevidence documents generated by the RAG system. We use heterogeneity analysis\nto check whether the evidence supports different viewpoints in the response. In\naddition to verifying the accuracy of the response, we also assess the\nreliability of the evidence provided by the RAG system. Our method shows an\nimprovement of up to 23.31% accuracy across various LLMs. This work can help\ndetect errors in current RAG-based medical systems. It also makes the\napplications of LLMs more reliable and reduces diagnostic errors.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aM-Eval\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4bRAG\u5728\u533b\u7597\u95ee\u7b54\u7cfb\u7edf\u4e2d\u751f\u6210\u7684\u9519\u8bef\u4fe1\u606f\u3002", "motivation": "\u5f53\u524dRAG\u5e94\u7528\u4e8e\u533b\u7597\u9886\u57df\u65f6\uff0c\u4f1a\u751f\u6210\u4e0d\u6b63\u786e\u7684\u4fe1\u606f\uff0c\u4f8b\u5982\u5e7b\u89c9\uff0c\u5e76\u4e14\u672a\u80fd\u6b63\u786e\u4f7f\u7528\u5916\u90e8\u77e5\u8bc6\u3002", "method": "\u8be5\u65b9\u6cd5\u53d7\u5230\u5faa\u8bc1\u533b\u5b66\u4e2d\u5f02\u8d28\u6027\u5206\u6790\u65b9\u6cd5\u7684\u542f\u53d1\uff0c\u901a\u8fc7\u4ece\u5916\u90e8\u77e5\u8bc6\u5e93\u63d0\u53d6\u989d\u5916\u7684\u533b\u5b66\u6587\u732e\uff0c\u5e76\u68c0\u7d22RAG\u7cfb\u7edf\u751f\u6210\u7684\u8bc1\u636e\u6587\u6863\uff0c\u4f7f\u7528\u5f02\u8d28\u6027\u5206\u6790\u6765\u68c0\u67e5\u8bc1\u636e\u662f\u5426\u652f\u6301\u54cd\u5e94\u4e2d\u7684\u4e0d\u540c\u89c2\u70b9\u3002", "result": "M-Eval\u65b9\u6cd5\u5728\u5404\u79cdLLM\u4e2d\u663e\u793a\u51fa\u9ad8\u8fbe23.31%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u6709\u52a9\u4e8e\u68c0\u6d4b\u5f53\u524d\u57fa\u4e8eRAG\u7684\u533b\u7597\u7cfb\u7edf\u4e2d\u7684\u9519\u8bef\uff0c\u5e76\u4f7fLLM\u7684\u5e94\u7528\u66f4\u52a0\u53ef\u9760\uff0c\u51cf\u5c11\u8bca\u65ad\u9519\u8bef\u3002"}}
{"id": "2510.23989", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23989", "abs": "https://arxiv.org/abs/2510.23989", "authors": ["Shangde Gao", "Zelin Xu", "Zhe Jiang"], "title": "Learning Individual Movement Shifts After Urban Disruptions with Social Infrastructure Reliance", "comment": null, "summary": "Shifts in individual movement patterns following disruptive events can reveal\nchanging demands for community resources. However, predicting such shifts\nbefore disruptive events remains challenging for several reasons. First,\nmeasures are lacking for individuals' heterogeneous social infrastructure\nresilience (SIR), which directly influences their movement patterns, and\ncommonly used features are often limited or unavailable at scale, e.g.,\nsociodemographic characteristics. Second, the complex interactions between\nindividual movement patterns and spatial contexts have not been sufficiently\ncaptured. Third, individual-level movement may be spatially sparse and not\nwell-suited to traditional decision-making methods for movement predictions.\nThis study incorporates individuals' SIR into a conditioned deep learning model\nto capture the complex relationships between individual movement patterns and\nlocal spatial context using large-scale, sparse individual-level data. Our\nexperiments demonstrate that incorporating individuals' SIR and spatial context\ncan enhance the model's ability to predict post-event individual movement\npatterns. The conditioned model can capture the divergent shifts in movement\npatterns among individuals who exhibit similar pre-event patterns but differ in\nSIR.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u5927\u89c4\u6a21\u7a00\u758f\u7684\u4e2a\u4eba\u6570\u636e\uff0c\u5c06\u4e2a\u4eba\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u5f39\u6027 (SIR) \u7eb3\u5165\u6761\u4ef6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u6355\u6349\u4e2a\u4eba\u79fb\u52a8\u6a21\u5f0f\u4e0e\u672c\u5730\u7a7a\u95f4\u73af\u5883\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u3002", "motivation": "\u9884\u6d4b\u7834\u574f\u6027\u4e8b\u4ef6\u53d1\u751f\u524d\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u7684\u53d8\u5316\u5177\u6709\u6311\u6218\u6027\uff0c\u539f\u56e0\u5305\u62ec\u7f3a\u4e4f\u8861\u91cf\u4e2a\u4f53\u5f02\u8d28\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u5f39\u6027 (SIR) \u7684\u6307\u6807\u3001\u5e38\u7528\u7279\u5f81\u7684\u5c40\u9650\u6027\u4ee5\u53ca\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u4e0e\u7a7a\u95f4\u73af\u5883\u4e4b\u95f4\u590d\u6742\u4e92\u52a8\u7684\u672a\u88ab\u5145\u5206\u6355\u6349\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u6761\u4ef6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u5408\u4e2a\u4f53\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u5f39\u6027 (SIR)\uff0c\u6355\u6349\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u4e0e\u672c\u5730\u7a7a\u95f4\u73af\u5883\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u4e2a\u4f53\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u5f39\u6027 (SIR) \u548c\u7a7a\u95f4\u73af\u5883\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u9884\u6d4b\u4e8b\u4ef6\u540e\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u7684\u80fd\u529b\u3002\u8be5\u6761\u4ef6\u6a21\u578b\u53ef\u4ee5\u6355\u6349\u5230\u5728\u4e8b\u4ef6\u524d\u8868\u73b0\u51fa\u76f8\u4f3c\u6a21\u5f0f\u4f46\u5728 SIR \u65b9\u9762\u4e0d\u540c\u7684\u4e2a\u4f53\u4e4b\u95f4\u79fb\u52a8\u6a21\u5f0f\u7684\u5dee\u5f02\u3002", "conclusion": "\u5c06\u4e2a\u4eba\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u5f39\u6027 (SIR) \u7eb3\u5165\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u9884\u6d4b\u4e8b\u4ef6\u540e\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u7684\u53d8\u5316\u3002"}}
{"id": "2510.24000", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24000", "abs": "https://arxiv.org/abs/2510.24000", "authors": ["Heethanjan Kanagalingam", "Thenukan Pathmanathan", "Mokeeshan Vathanakumar", "Tharmakulasingam Mukunthan"], "title": "AdvBlur: Adversarial Blur for Robust Diabetic Retinopathy Classification and Cross-Domain Generalization", "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, yet\nearly and accurate detection can significantly improve treatment outcomes.\nWhile numerous Deep learning (DL) models have been developed to predict DR from\nfundus images, many face challenges in maintaining robustness due to\ndistributional variations caused by differences in acquisition devices,\ndemographic disparities, and imaging conditions. This paper addresses this\ncritical limitation by proposing a novel DR classification approach, a method\ncalled AdvBlur. Our method integrates adversarial blurred images into the\ndataset and employs a dual-loss function framework to address domain\ngeneralization. This approach effectively mitigates the impact of unseen\ndistributional variations, as evidenced by comprehensive evaluations across\nmultiple datasets. Additionally, we conduct extensive experiments to explore\nthe effects of factors such as camera type, low-quality images, and dataset\nsize. Furthermore, we perform ablation studies on blurred images and the loss\nfunction to ensure the validity of our choices. The experimental results\ndemonstrate the effectiveness of our proposed method, achieving competitive\nperformance compared to state-of-the-art domain generalization DR models on\nunseen external datasets.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8 (DR) \u5206\u7c7b\u65b9\u6cd5 AdvBlur\uff0c\u4ee5\u89e3\u51b3\u7531\u4e8e\u91c7\u96c6\u8bbe\u5907\u3001\u4eba\u53e3\u5dee\u5f02\u548c\u6210\u50cf\u6761\u4ef6\u5f15\u8d77\u7684\u5206\u5e03\u53d8\u5316\u5bfc\u81f4\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8 (DR) \u662f\u5168\u7403\u89c6\u529b\u4e27\u5931\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u65e9\u671f\u51c6\u786e\u7684\u68c0\u6d4b\u53ef\u4ee5\u663e\u8457\u6539\u5584\u6cbb\u7597\u6548\u679c\u3002\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60 (DL) \u6a21\u578b\u5728\u9884\u6d4b DR \u65b9\u9762\u9762\u4e34\u9c81\u68d2\u6027\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u5bf9\u6297\u6a21\u7cca\u56fe\u50cf\u96c6\u6210\u5230\u6570\u636e\u96c6\u4e2d\uff0c\u5e76\u91c7\u7528\u53cc\u91cd\u635f\u5931\u51fd\u6570\u6846\u67b6\u6765\u89e3\u51b3\u57df\u6cdb\u5316\u95ee\u9898\u3002", "result": "\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u51cf\u8f7b\u4e86\u770b\u4e0d\u89c1\u7684\u5206\u5e03\u53d8\u5316\u7684\u5f71\u54cd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8bc1\u660e\u4e86\u8fd9\u4e00\u70b9\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u57df\u6cdb\u5316 DR \u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u770b\u4e0d\u89c1\u7684\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.23640", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23640", "abs": "https://arxiv.org/abs/2510.23640", "authors": ["Zihao Jing", "Yan Sun", "Yan Yi Li", "Sugitha Janarthanan", "Alana Deng", "Pingzhao Hu"], "title": "Structure-Aware Fusion with Progressive Injection for Multimodal Molecular Representation Learning", "comment": "Accepted by NeurIPS 2025", "summary": "Multimodal molecular models often suffer from 3D conformer unreliability and\nmodality collapse, limiting their robustness and generalization. We propose\nMuMo, a structured multimodal fusion framework that addresses these challenges\nin molecular representation through two key strategies. To reduce the\ninstability of conformer-dependent fusion, we design a Structured Fusion\nPipeline (SFP) that combines 2D topology and 3D geometry into a unified and\nstable structural prior. To mitigate modality collapse caused by naive fusion,\nwe introduce a Progressive Injection (PI) mechanism that asymmetrically\nintegrates this prior into the sequence stream, preserving modality-specific\nmodeling while enabling cross-modal enrichment. Built on a state space\nbackbone, MuMo supports long-range dependency modeling and robust information\npropagation. Across 29 benchmark tasks from Therapeutics Data Commons (TDC) and\nMoleculeNet, MuMo achieves an average improvement of 2.7% over the\nbest-performing baseline on each task, ranking first on 22 of them, including a\n27% improvement on the LD50 task. These results validate its robustness to 3D\nconformer noise and the effectiveness of multimodal fusion in molecular\nrepresentation. The code is available at: github.com/selmiss/MuMo.", "AI": {"tldr": "MuMo: A structured multimodal fusion framework addresses challenges in molecular representation through two key strategies: Structured Fusion Pipeline (SFP) and Progressive Injection (PI).", "motivation": "Multimodal molecular models often suffer from 3D conformer unreliability and modality collapse, limiting their robustness and generalization.", "method": "Combines 2D topology and 3D geometry into a unified and stable structural prior, and introduces a Progressive Injection (PI) mechanism that asymmetrically integrates this prior into the sequence stream.", "result": "Achieves an average improvement of 2.7% over the best-performing baseline on each task, ranking first on 22 of them, including a 27% improvement on the LD50 task.", "conclusion": "MuMo's robustness to 3D conformer noise and the effectiveness of multimodal fusion in molecular representation are validated."}}
{"id": "2510.23998", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23998", "abs": "https://arxiv.org/abs/2510.23998", "authors": ["Mengzhou Sun", "Sendong Zhao", "Jianyu Chen", "Bin Qin"], "title": "PICOs-RAG: PICO-supported Query Rewriting for Retrieval-Augmented Generation in Evidence-Based Medicine", "comment": null, "summary": "Evidence-based medicine (EBM) research has always been of paramount\nimportance. It is important to find appropriate medical theoretical support for\nthe needs from physicians or patients to reduce the occurrence of medical\naccidents. This process is often carried out by human querying relevant\nliterature databases, which lacks objectivity and efficiency. Therefore,\nresearchers utilize retrieval-augmented generation (RAG) to search for evidence\nand generate responses automatically. However, current RAG methods struggle to\nhandle complex queries in real-world clinical scenarios. For example, when\nqueries lack certain information or use imprecise language, the model may\nretrieve irrelevant evidence and generate unhelpful answers. To address this\nissue, we present the PICOs-RAG to expand the user queries into a better\nformat. Our method can expand and normalize the queries into professional ones\nand use the PICO format, a search strategy tool present in EBM, to extract the\nmost important information used for retrieval. This approach significantly\nenhances retrieval efficiency and relevance, resulting in up to an 8.8\\%\nimprovement compared to the baseline evaluated by our method. Thereby the\nPICOs-RAG improves the performance of the large language models into a helpful\nand reliable medical assistant in EBM.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPICOs-RAG\u7684\u65b9\u6cd5\uff0c\u4ee5\u6539\u8fdb\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5728\u5faa\u8bc1\u533b\u5b66\uff08EBM\uff09\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u5f53\u524dRAG\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u771f\u5b9e\u4e34\u5e8a\u573a\u666f\u4e2d\u7684\u590d\u6742\u67e5\u8be2\uff0c\u4f8b\u5982\uff0c\u5f53\u67e5\u8be2\u7f3a\u5c11\u67d0\u4e9b\u4fe1\u606f\u6216\u4f7f\u7528\u4e0d\u7cbe\u786e\u7684\u8bed\u8a00\u65f6\uff0c\u6a21\u578b\u53ef\u80fd\u4f1a\u68c0\u7d22\u5230\u4e0d\u76f8\u5173\u7684\u8bc1\u636e\u5e76\u751f\u6210\u65e0\u7528\u7684\u7b54\u6848\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u7528\u6237\u67e5\u8be2\u6269\u5c55\u548c\u89c4\u8303\u5316\u4e3a\u66f4\u4e13\u4e1a\u7684\u683c\u5f0f\uff0c\u5e76\u4f7f\u7528EBM\u4e2d\u7684PICO\u683c\u5f0f\u63d0\u53d6\u7528\u4e8e\u68c0\u7d22\u7684\u6700\u91cd\u8981\u4fe1\u606f\u6765\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "result": "\u4e0e\u57fa\u7ebf\u76f8\u6bd4\uff0c\u68c0\u7d22\u6548\u7387\u548c\u76f8\u5173\u6027\u663e\u8457\u63d0\u9ad8\uff0c\u7ecf\u8bc4\u4f30\uff0c\u63d0\u5347\u9ad8\u8fbe8.8%\u3002", "conclusion": "PICOs-RAG\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728EBM\u4e2d\u4f5c\u4e3a\u6709\u7528\u7684\u548c\u53ef\u9760\u7684\u533b\u7597\u52a9\u624b\u65f6\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24013", "categories": ["cs.AI", "cs.LG", "cs.NE", "math.CO", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.24013", "abs": "https://arxiv.org/abs/2510.24013", "authors": ["\u0130brahim O\u011fuz \u00c7etinkaya", "\u0130. Esra B\u00fcy\u00fcktahtak\u0131n", "Parshin Shojaee", "Chandan K. Reddy"], "title": "Discovering Heuristics with Large Language Models (LLMs) for Mixed-Integer Programs: Single-Machine Scheduling", "comment": null, "summary": "Our study contributes to the scheduling and combinatorial optimization\nliterature with new heuristics discovered by leveraging the power of Large\nLanguage Models (LLMs). We focus on the single-machine total tardiness (SMTT)\nproblem, which aims to minimize total tardiness by sequencing n jobs on a\nsingle processor without preemption, given processing times and due dates. We\ndevelop and benchmark two novel LLM-discovered heuristics, the EDD Challenger\n(EDDC) and MDD Challenger (MDDC), inspired by the well-known Earliest Due Date\n(EDD) and Modified Due Date (MDD) rules. In contrast to prior studies that\nemployed simpler rule-based heuristics, we evaluate our LLM-discovered\nalgorithms using rigorous criteria, including optimality gaps and solution time\nderived from a mixed-integer programming (MIP) formulation of SMTT. We compare\ntheir performance against state-of-the-art heuristics and exact methods across\nvarious job sizes (20, 100, 200, and 500 jobs). For instances with more than\n100 jobs, exact methods such as MIP and dynamic programming become\ncomputationally intractable. Up to 500 jobs, EDDC improves upon the classic EDD\nrule and another widely used algorithm in the literature. MDDC consistently\noutperforms traditional heuristics and remains competitive with exact\napproaches, particularly on larger and more complex instances. This study shows\nthat human-LLM collaboration can produce scalable, high-performing heuristics\nfor NP-hard constrained combinatorial optimization, even under limited\nresources when effectively configured.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u53d1\u73b0\u4e86\u65b0\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5355\u673a\u603b\u5ef6\u8fdf\uff08SMTT\uff09\u95ee\u9898\uff0c\u65e8\u5728\u6700\u5c0f\u5316\u603b\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u542f\u53d1\u5f0f\u7b97\u6cd5\u8f83\u4e3a\u7b80\u5355\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u3002", "method": "\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u4e24\u79cd\u65b0\u7684LLM\u53d1\u73b0\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0cEDDC\u548cMDDC\uff0c\u5e76\u4e0e\u73b0\u6709\u7b97\u6cd5\u548c\u7cbe\u786e\u65b9\u6cd5\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u4efb\u52a1\u4e0a\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "EDDC\u5728\u6700\u591a500\u4e2a\u4efb\u52a1\u7684\u60c5\u51b5\u4e0b\u6539\u8fdb\u4e86\u7ecf\u5178EDD\u89c4\u5219\uff0cMDDC\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u5e76\u4e14\u5728\u66f4\u5927\u66f4\u590d\u6742\u7684\u5b9e\u4f8b\u4e2d\u4e0e\u7cbe\u786e\u65b9\u6cd5\u76f8\u6bd4\u4ecd\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u4eba\u4e0eLLM\u7684\u534f\u4f5c\u53ef\u4ee5\u4e3aNP-hard\u7ea6\u675f\u7ec4\u5408\u4f18\u5316\u751f\u6210\u53ef\u6269\u5c55\u7684\uff0c\u9ad8\u6027\u80fd\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002"}}
{"id": "2510.24009", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24009", "abs": "https://arxiv.org/abs/2510.24009", "authors": ["Yuan Jin", "Antonio Pepe", "Gian Marco Melito", "Yuxuan Chen", "Yunsu Byeon", "Hyeseong Kim", "Kyungwon Kim", "Doohyun Park", "Euijoon Choi", "Dosik Hwang", "Andriy Myronenko", "Dong Yang", "Yufan He", "Daguang Xu", "Ayman El-Ghotni", "Mohamed Nabil", "Hossam El-Kady", "Ahmed Ayyad", "Amr Nasr", "Marek Wodzinski", "Henning M\u00fcller", "Hyeongyu Kim", "Yejee Shin", "Abbas Khan", "Muhammad Asad", "Alexander Zolotarev", "Caroline Roney", "Anthony Mathur", "Martin Benning", "Gregory Slabaugh", "Theodoros Panagiotis Vagenas", "Konstantinos Georgas", "George K. Matsopoulos", "Jihan Zhang", "Zhen Zhang", "Liqin Huang", "Christian Mayer", "Heinrich M\u00e4chler", "Jan Egger"], "title": "Towards the Automatic Segmentation, Modeling and Meshing of the Aortic Vessel Tree from Multicenter Acquisitions: An Overview of the SEG.A. 2023 Segmentation of the Aorta Challenge", "comment": null, "summary": "The automated analysis of the aortic vessel tree (AVT) from computed\ntomography angiography (CTA) holds immense clinical potential, but its\ndevelopment has been impeded by a lack of shared, high-quality data. We\nlaunched the SEG.A. challenge to catalyze progress in this field by introducing\na large, publicly available, multi-institutional dataset for AVT segmentation.\nThe challenge benchmarked automated algorithms on a hidden test set, with\nsubsequent optional tasks in surface meshing for computational simulations. Our\nfindings reveal a clear convergence on deep learning methodologies, with 3D\nU-Net architectures dominating the top submissions. A key result was that an\nensemble of the highest-ranking algorithms significantly outperformed\nindividual models, highlighting the benefits of model fusion. Performance was\nstrongly linked to algorithmic design, particularly the use of customized\npost-processing steps, and the characteristics of the training data. This\ninitiative not only establishes a new performance benchmark but also provides a\nlasting resource to drive future innovation toward robust, clinically\ntranslatable tools.", "AI": {"tldr": "SEG.A. \u6311\u6218\u8d5b\u53d1\u5e03\u4e86\u4e00\u4e2a\u5927\u578bAVT\u5206\u5272\u6570\u636e\u96c6\uff0c\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "motivation": "\u7f3a\u4e4f\u5171\u4eab\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u963b\u788d\u4e86AVT\u81ea\u52a8\u5206\u6790\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7SEG.A.\u6311\u6218\u8d5b\uff0c\u4f7f\u7528\u9690\u85cf\u7684\u6d4b\u8bd5\u96c6\u5bf9\u81ea\u52a8\u7b97\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u8fdb\u884c\u8868\u9762\u7f51\u683c\u5212\u5206\u7684\u53ef\u9009\u4efb\u52a1\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8d8b\u4e8e\u4e00\u81f4\uff0c3D U-Net\u67b6\u6784\u5728\u9876\u7ea7\u63d0\u4ea4\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\u3002\u7b97\u6cd5\u96c6\u6210\u660e\u663e\u4f18\u4e8e\u5355\u4e2a\u6a21\u578b\uff0c\u5e76\u4e14\u6027\u80fd\u4e0e\u7b97\u6cd5\u8bbe\u8ba1\u548c\u8bad\u7ec3\u6570\u636e\u7684\u7279\u5f81\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "\u8be5\u8ba1\u5212\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u6027\u80fd\u57fa\u51c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6301\u4e45\u7684\u8d44\u6e90\uff0c\u4ee5\u63a8\u52a8\u672a\u6765\u521b\u65b0\uff0c\u5b9e\u73b0\u5f3a\u5927\u7684\u3001\u53ef\u4e34\u5e8a\u8f6c\u5316\u7684\u5de5\u5177\u3002"}}
{"id": "2510.23641", "categories": ["cs.LG", "cs.AI", "hep-ex", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2510.23641", "abs": "https://arxiv.org/abs/2510.23641", "authors": ["Aaron Wang", "Zihan Zhao", "Subash Katel", "Vivekanand Gyanchand Sahu", "Elham E Khoda", "Abhijith Gandrakota", "Jennifer Ngadiuba", "Richard Cavanaugh", "Javier Duarte"], "title": "Spatially Aware Linear Transformer (SAL-T) for Particle Jet Tagging", "comment": null, "summary": "Transformers are very effective in capturing both global and local\ncorrelations within high-energy particle collisions, but they present\ndeployment challenges in high-data-throughput environments, such as the CERN\nLHC. The quadratic complexity of transformer models demands substantial\nresources and increases latency during inference. In order to address these\nissues, we introduce the Spatially Aware Linear Transformer (SAL-T), a\nphysics-inspired enhancement of the linformer architecture that maintains\nlinear attention. Our method incorporates spatially aware partitioning of\nparticles based on kinematic features, thereby computing attention between\nregions of physical significance. Additionally, we employ convolutional layers\nto capture local correlations, informed by insights from jet physics. In\naddition to outperforming the standard linformer in jet classification tasks,\nSAL-T also achieves classification results comparable to full-attention\ntransformers, while using considerably fewer resources with lower latency\nduring inference. Experiments on a generic point cloud classification dataset\n(ModelNet10) further confirm this trend. Our code is available at\nhttps://github.com/aaronw5/SAL-T4HEP.", "AI": {"tldr": "\u63d0\u51fa\u4e86Spatially Aware Linear Transformer (SAL-T)\uff0c\u8fd9\u662f\u4e00\u79cdlinformer\u67b6\u6784\u7684\u7269\u7406\u542f\u53d1\u5f0f\u589e\u5f3a\uff0c\u5b83\u4fdd\u6301\u4e86\u7ebf\u6027\u6ce8\u610f\u529b\u3002", "motivation": "Transformer\u6a21\u578b\u5728\u6355\u83b7\u9ad8\u80fd\u7c92\u5b50\u78b0\u649e\u4e2d\u7684\u5168\u5c40\u548c\u5c40\u90e8\u76f8\u5173\u6027\u65b9\u9762\u975e\u5e38\u6709\u6548\uff0c\u4f46\u5b83\u4eec\u5728CERN LHC\u7b49\u9ad8\u6570\u636e\u541e\u5410\u91cf\u73af\u5883\u4e2d\u63d0\u51fa\u4e86\u90e8\u7f72\u6311\u6218\u3002Transformer\u6a21\u578b\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u9700\u8981\u5927\u91cf\u8d44\u6e90\uff0c\u5e76\u589e\u52a0\u63a8\u7406\u671f\u95f4\u7684\u5ef6\u8fdf\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u57fa\u4e8e\u8fd0\u52a8\u5b66\u7279\u5f81\u7684\u7c92\u5b50\u7a7a\u95f4\u611f\u77e5\u5206\u533a\uff0c\u4ece\u800c\u8ba1\u7b97\u7269\u7406\u610f\u4e49\u533a\u57df\u4e4b\u95f4\u7684\u6ce8\u610f\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5229\u7528\u5377\u79ef\u5c42\u6765\u6355\u83b7\u5c40\u90e8\u76f8\u5173\u6027\uff0c\u8fd9\u662f\u7531\u6765\u81ea\u5c04\u6d41\u7269\u7406\u7684\u89c1\u89e3\u6240\u63d0\u4f9b\u7684\u3002", "result": "\u5728\u5c04\u6d41\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cSAL-T\u4f18\u4e8e\u6807\u51c6linformer\uff0c\u5e76\u4e14\u8fd8\u5b9e\u73b0\u4e86\u4e0e\u5168\u6ce8\u610f\u529btransformer\u76f8\u5f53\u7684\u5206\u7c7b\u7ed3\u679c\uff0c\u540c\u65f6\u5728\u63a8\u7406\u671f\u95f4\u4f7f\u7528\u66f4\u5c11\u7684\u8d44\u6e90\u548c\u66f4\u4f4e\u7684\u5ef6\u8fdf\u3002\u5728\u901a\u7528\u70b9\u4e91\u5206\u7c7b\u6570\u636e\u96c6\uff08ModelNet10\uff09\u4e0a\u7684\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u8fd9\u4e00\u8d8b\u52bf\u3002", "conclusion": "SAL-T\u5728\u8d44\u6e90\u4f7f\u7528\u548c\u63a8\u7406\u5ef6\u8fdf\u65b9\u9762\uff0c\u5b9e\u73b0\u4e86\u4e0e\u5168\u6ce8\u610f\u529btransformer\u76f8\u5f53\u7684\u5206\u7c7b\u7ed3\u679c"}}
{"id": "2510.24003", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24003", "abs": "https://arxiv.org/abs/2510.24003", "authors": ["Mengzhou Sun", "Sendong Zhao", "Jianyu Chen", "Haochun Wang", "Bin Qin"], "title": "META-RAG: Meta-Analysis-Inspired Evidence-Re-Ranking Method for Retrieval-Augmented Generation in Evidence-Based Medicine", "comment": null, "summary": "Evidence-based medicine (EBM) holds a crucial role in clinical application.\nGiven suitable medical articles, doctors effectively reduce the incidence of\nmisdiagnoses. Researchers find it efficient to use large language models (LLMs)\ntechniques like RAG for EBM tasks. However, the EBM maintains stringent\nrequirements for evidence, and RAG applications in EBM struggle to efficiently\ndistinguish high-quality evidence. Therefore, inspired by the meta-analysis\nused in EBM, we provide a new method to re-rank and filter the medical\nevidence. This method presents multiple principles to filter the best evidence\nfor LLMs to diagnose. We employ a combination of several EBM methods to emulate\nthe meta-analysis, which includes reliability analysis, heterogeneity analysis,\nand extrapolation analysis. These processes allow the users to retrieve the\nbest medical evidence for the LLMs. Ultimately, we evaluate these high-quality\narticles and show an accuracy improvement of up to 11.4% in our experiments and\nresults. Our method successfully enables RAG to extract higher-quality and more\nreliable evidence from the PubMed dataset. This work can reduce the infusion of\nincorrect knowledge into responses and help users receive more effective\nreplies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62dfEBM\u4e2d\u7684meta-analysis\u6765\u91cd\u65b0\u6392\u5e8f\u548c\u8fc7\u6ee4\u533b\u5b66\u8bc1\u636e\uff0c\u4ece\u800c\u63d0\u9ad8LLM\u5728EBM\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709RAG\u5728EBM\u4e2d\u96be\u4ee5\u6709\u6548\u533a\u5206\u9ad8\u8d28\u91cf\u8bc1\u636e\uff0cEBM\u5bf9\u8bc1\u636e\u6709\u4e25\u683c\u8981\u6c42\u3002", "method": "\u7ed3\u5408\u591a\u79cdEBM\u65b9\u6cd5\u6a21\u62dfmeta-analysis\uff0c\u5305\u62ec\u53ef\u9760\u6027\u5206\u6790\u3001\u5f02\u8d28\u6027\u5206\u6790\u548c\u5916\u63a8\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f7f\u51c6\u786e\u7387\u63d0\u9ad8\u4e8611.4%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ecePubMed\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u66f4\u9ad8\u8d28\u91cf\u548c\u66f4\u53ef\u9760\u7684\u8bc1\u636e\uff0c\u51cf\u5c11\u9519\u8bef\u77e5\u8bc6\u7684\u8f93\u5165\uff0c\u5e76\u5e2e\u52a9\u7528\u6237\u83b7\u5f97\u66f4\u6709\u6548\u7684\u56de\u590d\u3002"}}
{"id": "2510.24028", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24028", "abs": "https://arxiv.org/abs/2510.24028", "authors": ["Tingyue Pan", "Mingyue Cheng", "Shilong Zhang", "Zhiding Liu", "Xiaoyu Tao", "Yucong Luo", "Jintao Zhang", "Qi Liu"], "title": "OneCast: Structured Decomposition and Modular Generation for Cross-Domain Time Series Forecasting", "comment": null, "summary": "Cross-domain time series forecasting is a valuable task in various web\napplications. Despite its rapid advancement, achieving effective generalization\nacross heterogeneous time series data remains a significant challenge. Existing\nmethods have made progress by extending single-domain models, yet often fall\nshort when facing domain-specific trend shifts and inconsistent periodic\npatterns. We argue that a key limitation lies in treating temporal series as\nundifferentiated sequence, without explicitly decoupling their inherent\nstructural components. To address this, we propose OneCast, a structured and\nmodular forecasting framework that decomposes time series into seasonal and\ntrend components, each modeled through tailored generative pathways.\nSpecifically, the seasonal component is captured by a lightweight projection\nmodule that reconstructs periodic patterns via interpretable basis functions.\nIn parallel, the trend component is encoded into discrete tokens at segment\nlevel via a semantic-aware tokenizer, and subsequently inferred through a\nmasked discrete diffusion mechanism. The outputs from both branches are\ncombined to produce a final forecast that captures seasonal patterns while\ntracking domain-specific trends. Extensive experiments across eight domains\ndemonstrate that OneCast mostly outperforms state-of-the-art baselines.", "AI": {"tldr": "OneCast: a forecasting framework that decomposes time series into seasonal and trend components, each modeled through tailored generative pathways.", "motivation": "Existing methods often fall short when facing domain-specific trend shifts and inconsistent periodic patterns.", "method": "decompose time series into seasonal and trend components, the seasonal component is captured by a lightweight projection module that reconstructs periodic patterns via interpretable basis functions; the trend component is encoded into discrete tokens at segment level via a semantic-aware tokenizer, and subsequently inferred through a masked discrete diffusion mechanism.", "result": "OneCast mostly outperforms state-of-the-art baselines across eight domains.", "conclusion": "achieving effective generalization across heterogeneous time series data remains a significant challenge. treat temporal series as undifferentiated sequence, without explicitly decoupling their inherent structural components."}}
{"id": "2510.24010", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24010", "abs": "https://arxiv.org/abs/2510.24010", "authors": ["Mirali Purohit", "Bimal Gajera", "Vatsal Malaviya", "Irish Mehta", "Kunal Kasodekar", "Jacob Adler", "Steven Lu", "Umaa Rebbapragada", "Hannah Kerner"], "title": "Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science Tasks", "comment": "Accepted at NeurIPS 2025", "summary": "Foundation models have enabled rapid progress across many specialized domains\nby leveraging large-scale pre-training on unlabeled data, demonstrating strong\ngeneralization to a variety of downstream tasks. While such models have gained\nsignificant attention in fields like Earth Observation, their application to\nMars science remains limited. A key enabler of progress in other domains has\nbeen the availability of standardized benchmarks that support systematic\nevaluation. In contrast, Mars science lacks such benchmarks and standardized\nevaluation frameworks, which have limited progress toward developing foundation\nmodels for Martian tasks. To address this gap, we introduce Mars-Bench, the\nfirst benchmark designed to systematically evaluate models across a broad range\nof Mars-related tasks using both orbital and surface imagery. Mars-Bench\ncomprises 20 datasets spanning classification, segmentation, and object\ndetection, focused on key geologic features such as craters, cones, boulders,\nand frost. We provide standardized, ready-to-use datasets and baseline\nevaluations using models pre-trained on natural images, Earth satellite data,\nand state-of-the-art vision-language models. Results from all analyses suggest\nthat Mars-specific foundation models may offer advantages over general-domain\ncounterparts, motivating further exploration of domain-adapted pre-training.\nMars-Bench aims to establish a standardized foundation for developing and\ncomparing machine learning models for Mars science. Our data, models, and code\nare available at: https://mars-bench.github.io/.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Mars-Bench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u706b\u661f\u76f8\u5173\u4efb\u52a1\u6a21\u578b\u7684\u57fa\u51c6\u3002", "motivation": "\u706b\u661f\u79d1\u5b66\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u9650\u5236\u4e86\u8be5\u9886\u57df\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b20\u4e2a\u6570\u636e\u96c6\u7684Mars-Bench\uff0c\u6db5\u76d6\u5206\u7c7b\u3001\u5206\u5272\u548c\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\uff0c\u5173\u6ce8\u706b\u661f\u5730\u8d28\u7279\u5f81\u3002", "result": "\u4f7f\u7528\u5728\u81ea\u7136\u56fe\u50cf\u3001\u5730\u7403\u536b\u661f\u6570\u636e\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u8fdb\u884c\u57fa\u7ebf\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u706b\u661f\u4e13\u7528\u57fa\u7840\u6a21\u578b\u53ef\u80fd\u4f18\u4e8e\u901a\u7528\u6a21\u578b\u3002", "conclusion": "Mars-Bench\u65e8\u5728\u4e3a\u5f00\u53d1\u548c\u6bd4\u8f83\u706b\u661f\u79d1\u5b66\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5efa\u7acb\u6807\u51c6\u5316\u7684\u57fa\u7840\u3002"}}
{"id": "2510.23649", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23649", "abs": "https://arxiv.org/abs/2510.23649", "authors": ["Tenghui Li", "Guoxu Zhou", "Xuyang Zhao", "Yuning Qiu", "Qibin Zhao"], "title": "Efficient Low Rank Attention for Long-Context Inference in Large Language Models", "comment": null, "summary": "As the length of input text grows, the key-value (KV) cache in LLMs imposes\nprohibitive GPU memory costs and limits long-context inference on resource\nconstrained devices. Existing approaches, such as KV quantization and pruning,\nreduce memory usage but suffer from numerical precision loss or suboptimal\nretention of key-value pairs. We introduce Low Rank Query and Key attention\n(LRQK), a two-stage framework that jointly decomposes the full-precision query\nand key matrices into compact rank-\\(r\\) factors during the prefill stage, and\nthen uses these low-dimensional projections to compute proxy attention scores\nin \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the\ntop-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed\nGPU-CPU cache with a hit-and-miss mechanism that transfers only missing\nfull-precision KV pairs, thereby preserving exact attention outputs while\nreducing CPU-GPU data movement. Extensive experiments on the RULER and\nLongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK\nmatches or surpasses leading sparse-attention methods in long context settings,\nwhile delivering significant memory savings with minimal loss in accuracy. Our\ncode is available at https://github.com/tenghuilee/LRQK.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u4f4e\u79e9\u67e5\u8be2\u548c\u952e\u6ce8\u610f\uff08LRQK\uff09\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u56e0\u8f93\u5165\u6587\u672c\u957f\u5ea6\u589e\u52a0\u800c\u5bfc\u81f4\u7684 KV \u7f13\u5b58 GPU \u5185\u5b58\u6210\u672c\u8fc7\u9ad8\u7684\u95ee\u9898\u3002\u8be5\u6846\u67b6\u5728\u9884\u586b\u5145\u9636\u6bb5\u5c06\u5168\u7cbe\u5ea6\u67e5\u8be2\u548c\u952e\u77e9\u9635\u5206\u89e3\u4e3a\u7d27\u51d1\u7684\u79e9-r \u56e0\u5b50\uff0c\u7136\u540e\u5728\u6bcf\u4e2a\u89e3\u7801\u6b65\u9aa4\u4e2d\u4f7f\u7528\u8fd9\u4e9b\u4f4e\u7ef4\u6295\u5f71\u6765\u8ba1\u7b97\u4ee3\u7406\u6ce8\u610f\u529b\u5206\u6570\u3002\u901a\u8fc7\u9009\u62e9\u524d k \u4e2a token \u548c\u4e00\u5c0f\u7ec4\u56fa\u5b9a\u7684\u6700\u8fd1 token\uff0cLRQK \u91c7\u7528\u6df7\u5408 GPU-CPU \u7f13\u5b58\uff0c\u4ec5\u4f20\u8f93\u4e22\u5931\u7684\u5168\u7cbe\u5ea6 KV \u5bf9\uff0c\u4ece\u800c\u5728\u51cf\u5c11 CPU-GPU \u6570\u636e\u79fb\u52a8\u7684\u540c\u65f6\u4fdd\u6301\u7cbe\u786e\u7684\u6ce8\u610f\u529b\u8f93\u51fa\u3002", "motivation": "LLM \u4e2d KV \u7f13\u5b58\u7684 GPU \u5185\u5b58\u6210\u672c\u968f\u7740\u8f93\u5165\u6587\u672c\u957f\u5ea6\u7684\u589e\u52a0\u800c\u53d8\u5f97\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u3002\u73b0\u6709\u7684 KV \u91cf\u5316\u548c\u526a\u679d\u65b9\u6cd5\u867d\u7136\u964d\u4f4e\u4e86\u5185\u5b58\u4f7f\u7528\u91cf\uff0c\u4f46\u4f1a\u906d\u53d7\u6570\u503c\u7cbe\u5ea6\u635f\u5931\u6216\u6b21\u4f18\u7684\u952e\u503c\u5bf9\u4fdd\u7559\u3002", "method": "\u672c\u6587\u63d0\u51fa\u7684 LRQK \u6846\u67b6\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a1) \u5728\u9884\u586b\u5145\u9636\u6bb5\uff0c\u5c06\u5168\u7cbe\u5ea6\u67e5\u8be2\u548c\u952e\u77e9\u9635\u5206\u89e3\u4e3a\u7d27\u51d1\u7684\u79e9-r \u56e0\u5b50\uff1b2) \u5728\u89e3\u7801\u9636\u6bb5\uff0c\u4f7f\u7528\u8fd9\u4e9b\u4f4e\u7ef4\u6295\u5f71\u4ee5 O(lr) \u7684\u65f6\u95f4\u590d\u6742\u5ea6\u8ba1\u7b97\u4ee3\u7406\u6ce8\u610f\u529b\u5206\u6570\u3002\u6b64\u5916\uff0cLRQK \u91c7\u7528\u6df7\u5408 GPU-CPU \u7f13\u5b58\uff0c\u4ec5\u4f20\u8f93\u4e22\u5931\u7684\u5168\u7cbe\u5ea6 KV \u5bf9\u3002", "result": "\u5728 RULER \u548c LongBench \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528 LLaMA-3-8B \u548c Qwen2.5-7B \u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLRQK \u5728\u957f\u4e0a\u4e0b\u6587\u8bbe\u7f6e\u4e2d\u4e0e\u9886\u5148\u7684\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u76f8\u5339\u914d\u6216\u8d85\u8fc7\u4e86\u5b83\u4eec\uff0c\u540c\u65f6\u5728\u6700\u5c0f\u7684\u7cbe\u5ea6\u635f\u5931\u4e0b\u5b9e\u73b0\u4e86\u663e\u7740\u7684\u5185\u5b58\u8282\u7701\u3002", "conclusion": "LRQK \u662f\u4e00\u79cd\u6709\u6548\u7684\u957f\u4e0a\u4e0b\u6587 LLM \u63a8\u7406\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u4f4e\u79e9\u5206\u89e3\u548c\u6df7\u5408 GPU-CPU \u7f13\u5b58\uff0c\u5728\u964d\u4f4e\u5185\u5b58\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u7cbe\u5ea6\u3002"}}
{"id": "2510.24014", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24014", "abs": "https://arxiv.org/abs/2510.24014", "authors": ["Yizhu Jiao", "Sha Li", "Sizhe Zhou", "Heng Ji", "Jiawei Han"], "title": "TEXT2DB: Integration-Aware Information Extraction with Large Language Model Agents", "comment": "ACL 2025. Source code: https://github.com/yzjiao/Text2DB", "summary": "The task of information extraction (IE) is to extract structured knowledge\nfrom text. However, it is often not straightforward to utilize IE output due to\nthe mismatch between the IE ontology and the downstream application needs. We\npropose a new formulation of IE TEXT2DB that emphasizes the integration of IE\noutput and the target database (or knowledge base). Given a user instruction, a\ndocument set, and a database, our task requires the model to update the\ndatabase with values from the document set to satisfy the user instruction.\nThis task requires understanding user instructions for what to extract and\nadapting to the given DB/KB schema for how to extract on the fly. To evaluate\nthis new task, we introduce a new benchmark featuring common demands such as\ndata infilling, row population, and column addition. In addition, we propose an\nLLM agent framework OPAL (Observe-PlanAnalyze LLM) which includes an Observer\ncomponent that interacts with the database, the Planner component that\ngenerates a code-based plan with calls to IE models, and the Analyzer component\nthat provides feedback regarding code quality before execution. Experiments\nshow that OPAL can successfully adapt to diverse database schemas by generating\ndifferent code plans and calling the required IE models. We also highlight\ndifficult cases such as dealing with large databases with complex dependencies\nand extraction hallucination, which we believe deserve further investigation.\nSource code: https://github.com/yzjiao/Text2DB", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4fe1\u606f\u62bd\u53d6\uff08IE\uff09\u4efb\u52a1TEXT2DB\uff0c\u5b83\u5f3a\u8c03\u4e86IE\u8f93\u51fa\u4e0e\u76ee\u6807\u6570\u636e\u5e93\u7684\u96c6\u6210\u3002\u8be5\u4efb\u52a1\u8981\u6c42\u6a21\u578b\u6839\u636e\u7528\u6237\u6307\u4ee4\uff0c\u5229\u7528\u6587\u6863\u96c6\u4e2d\u7684\u4fe1\u606f\u66f4\u65b0\u6570\u636e\u5e93\u3002", "motivation": "\u73b0\u6709\u4fe1\u606f\u62bd\u53d6\u672c\u4f53\u4e0e\u4e0b\u6e38\u5e94\u7528\u9700\u6c42\u4e0d\u5339\u914d\uff0c\u96be\u4ee5\u76f4\u63a5\u5229\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aOPAL\u7684LLM Agent\u6846\u67b6\uff0c\u5305\u62ecObserver\u3001Planner\u548cAnalyzer\u4e09\u4e2a\u7ec4\u4ef6\u3002Planner\u7ec4\u4ef6\u751f\u6210\u57fa\u4e8e\u4ee3\u7801\u7684\u8ba1\u5212\uff0c\u8c03\u7528IE\u6a21\u578b\u3002", "result": "OPAL\u53ef\u4ee5\u901a\u8fc7\u751f\u6210\u4e0d\u540c\u7684\u4ee3\u7801\u8ba1\u5212\u5e76\u8c03\u7528\u6240\u9700\u7684IE\u6a21\u578b\u6765\u6210\u529f\u9002\u5e94\u4e0d\u540c\u7684\u6570\u636e\u5e93\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6765\u8bc4\u4f30TEXT2DB\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86OPAL\u6846\u67b6\u3002\u5b9e\u9a8c\u8868\u660e\uff0cOPAL\u5728\u9002\u5e94\u4e0d\u540c\u7684\u6570\u636e\u5e93\u6a21\u5f0f\u65b9\u9762\u662f\u6210\u529f\u7684\u3002"}}
{"id": "2510.24031", "categories": ["cs.AI", "cs.CR", "H.3.3, I.2.7, I.5.3, I.2.5,"], "pdf": "https://arxiv.org/pdf/2510.24031", "abs": "https://arxiv.org/abs/2510.24031", "authors": ["Peng Cai", "Reza Ryan", "Nickson M. Karie"], "title": "LLMLogAnalyzer: A Clustering-Based Log Analysis Chatbot using Large Language Models", "comment": "33 pages, 10 figures", "summary": "System logs are a cornerstone of cybersecurity, supporting proactive breach\nprevention and post-incident investigations. However, analyzing vast amounts of\ndiverse log data remains significantly challenging, as high costs, lack of\nin-house expertise, and time constraints make even basic analysis difficult for\nmany organizations. This study introduces LLMLogAnalyzer, a clustering-based\nlog analysis chatbot that leverages Large Language Models (LLMs) and Machine\nLearning (ML) algorithms to simplify and streamline log analysis processes.\nThis innovative approach addresses key LLM limitations, including context\nwindow constraints and poor structured text handling capabilities, enabling\nmore effective summarization, pattern extraction, and anomaly detection tasks.\nLLMLogAnalyzer is evaluated across four distinct domain logs and various tasks.\nResults demonstrate significant performance improvements over state-of-the-art\nLLM-based chatbots, including ChatGPT, ChatPDF, and NotebookLM, with consistent\ngains ranging from 39% to 68% across different tasks. The system also exhibits\nstrong robustness, achieving a 93% reduction in interquartile range (IQR) when\nusing ROUGE-1 scores, indicating significantly lower result variability. The\nframework's effectiveness stems from its modular architecture comprising a\nrouter, log recognizer, log parser, and search tools. This design enhances LLM\ncapabilities for structured text analysis while improving accuracy and\nrobustness, making it a valuable resource for both cybersecurity experts and\nnon-technical users.", "AI": {"tldr": "LLMLogAnalyzer\u662f\u4e00\u4e2a\u57fa\u4e8e\u805a\u7c7b\u7684\u65e5\u5fd7\u5206\u6790\u804a\u5929\u673a\u5668\u4eba\uff0c\u5b83\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u548c\u673a\u5668\u5b66\u4e60(ml)\u7b97\u6cd5\u6765\u7b80\u5316\u548c\u7b80\u5316\u65e5\u5fd7\u5206\u6790\u8fc7\u7a0b\u3002", "motivation": "\u5206\u6790\u5927\u91cf\u7684\u4e0d\u540c\u65e5\u5fd7\u6570\u636e\u4ecd\u7136\u5177\u6709\u5f88\u5927\u7684\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u9ad8\u6210\u672c\u3001\u7f3a\u4e4f\u5185\u90e8\u4e13\u4e1a\u77e5\u8bc6\u548c\u65f6\u95f4\u9650\u5236\u4f7f\u5f97\u8bb8\u591a\u7ec4\u7ec7\u5373\u4f7f\u662f\u57fa\u672c\u7684\u5206\u6790\u4e5f\u5f88\u56f0\u96be\u3002", "method": "LLMLogAnalyzer \u91c7\u7528\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5305\u62ec\u4e00\u4e2a\u8def\u7531\u5668\u3001\u65e5\u5fd7\u8bc6\u522b\u5668\u3001\u65e5\u5fd7\u89e3\u6790\u5668\u548c\u641c\u7d22\u5de5\u5177\u7684\u6a21\u5757\u5316\u67b6\u6784\u3002\u8be5\u8bbe\u8ba1\u589e\u5f3a\u4e86LLM\u7684\u7ed3\u6784\u5316\u6587\u672c\u5206\u6790\u80fd\u529b\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5728\u4e0d\u540c\u7684\u4efb\u52a1\u4e2d\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u4e8ellm\u7684\u804a\u5929\u673a\u5668\u4eba(\u5305\u62ecChatGPT\u3001ChatPDF\u548cNotebookLM)\u76f8\u6bd4\uff0c\u6027\u80fd\u59cb\u7ec8\u63d0\u9ad8\u4e8639%\u523068%\u3002\u5f53\u4f7f\u7528ROUGE-1\u5206\u6570\u65f6\uff0c\u8be5\u7cfb\u7edf\u8fd8\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\uff0c\u56db\u5206\u4f4d\u95f4\u8ddd(IQR)\u964d\u4f4e\u4e8693%\uff0c\u8868\u660e\u7ed3\u679c\u7684\u53ef\u53d8\u6027\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "LLMLogAnalyzer \u662f\u4e00\u79cd\u6709\u4ef7\u503c\u7684\u8d44\u6e90\uff0c\u9002\u7528\u4e8e\u7f51\u7edc\u5b89\u5168\u4e13\u5bb6\u548c\u975e\u6280\u672f\u7528\u6237\u3002"}}
{"id": "2510.24034", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24034", "abs": "https://arxiv.org/abs/2510.24034", "authors": ["Yufan Liu", "Wanqian Zhang", "Huashan Chen", "Lin Wang", "Xiaojun Jia", "Zheng Lin", "Weiping Wang"], "title": "AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts", "comment": "Accepted by ICCV 2025", "summary": "Despite rapid advancements in text-to-image (T2I) models, their safety\nmechanisms are vulnerable to adversarial prompts, which maliciously generate\nunsafe images. Current red-teaming methods for proactively assessing such\nvulnerabilities usually require white-box access to T2I models, and rely on\ninefficient per-prompt optimization, as well as inevitably generate\nsemantically meaningless prompts easily blocked by filters. In this paper, we\npropose APT (AutoPrompT), a black-box framework that leverages large language\nmodels (LLMs) to automatically generate human-readable adversarial suffixes for\nbenign prompts. We first introduce an alternating optimization-finetuning\npipeline between adversarial suffix optimization and fine-tuning the LLM\nutilizing the optimized suffix. Furthermore, we integrates a dual-evasion\nstrategy in optimization phase, enabling the bypass of both perplexity-based\nfilter and blacklist word filter: (1) we constrain the LLM generating\nhuman-readable prompts through an auxiliary LLM perplexity scoring, which\nstarkly contrasts with prior token-level gibberish, and (2) we also introduce\nbanned-token penalties to suppress the explicit generation of banned-tokens in\nblacklist. Extensive experiments demonstrate the excellent red-teaming\nperformance of our human-readable, filter-resistant adversarial prompts, as\nwell as superior zero-shot transferability which enables instant adaptation to\nunseen prompts and exposes critical vulnerabilities even in commercial APIs\n(e.g., Leonardo.Ai.).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAPT\uff08AutoPrompT\uff09\u7684\u9ed1\u76d2\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u751f\u6210\u826f\u6027\u63d0\u793a\u7684\u4eba\u7c7b\u53ef\u8bfb\u5bf9\u6297\u6027\u540e\u7f00\uff0c\u4ee5\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u5f53\u524d\u7528\u4e8e\u4e3b\u52a8\u8bc4\u4f30T2I\u6a21\u578b\u5b89\u5168\u6f0f\u6d1e\u7684\u7ea2\u961f\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5bf9T2I\u6a21\u578b\u8fdb\u884c\u767d\u76d2\u8bbf\u95ee\uff0c\u5e76\u4e14\u4f9d\u8d56\u4e8e\u4f4e\u6548\u7684\u6bcf\u4e2a\u63d0\u793a\u4f18\u5316\uff0c\u5e76\u4e14\u4e0d\u53ef\u907f\u514d\u5730\u751f\u6210\u5bb9\u6613\u88ab\u8fc7\u6ee4\u5668\u963b\u6b62\u7684\u8bed\u4e49\u4e0a\u65e0\u610f\u4e49\u7684\u63d0\u793a\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u5728\u5bf9\u6297\u6027\u540e\u7f00\u4f18\u5316\u548c\u5fae\u8c03LLM\u4e4b\u95f4\u5f15\u5165\u4ea4\u66ff\u4f18\u5316-\u5fae\u8c03\u7ba1\u9053\uff0c\u5229\u7528\u4f18\u5316\u7684\u540e\u7f00\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728\u4f18\u5316\u9636\u6bb5\u96c6\u6210\u4e86\u4e00\u79cd\u53cc\u91cd\u89c4\u907f\u7b56\u7565\uff0c\u80fd\u591f\u7ed5\u8fc7\u57fa\u4e8e\u56f0\u60d1\u5ea6\u7684\u8fc7\u6ee4\u5668\u548c\u9ed1\u540d\u5355\u5355\u8bcd\u8fc7\u6ee4\u5668\u3002", "result": "\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u4eba\u7c7b\u53ef\u8bfb\u3001\u6297\u8fc7\u6ee4\u7684\u5bf9\u6297\u6027\u63d0\u793a\u5177\u6709\u51fa\u8272\u7684\u7ea2\u961f\u6027\u80fd\uff0c\u4ee5\u53ca\u5353\u8d8a\u7684\u96f6\u6837\u672c\u53ef\u8f6c\u79fb\u6027\uff0c\u4ece\u800c\u80fd\u591f\u5373\u65f6\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u63d0\u793a\uff0c\u5e76\u66b4\u9732\u51fa\u5546\u4e1aAPI\u4e2d\u7684\u5173\u952e\u6f0f\u6d1e\uff08\u4f8b\u5982\uff0cLeonardo.Ai\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u9ed1\u76d2\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u4eba\u7c7b\u53ef\u8bfb\u7684\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u4ee5\u8bc4\u4f30\u548c\u63d0\u9ad8\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2510.23650", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23650", "abs": "https://arxiv.org/abs/2510.23650", "authors": ["Wei Xia"], "title": "Beyond Hidden-Layer Manipulation: Semantically-Aware Logit Interventions for Debiasing LLMs", "comment": null, "summary": "We proposed Static and Dynamic -- two zero-shot logits-layer debiasing\nmethods. Dynamic reduces bias by up to 70% with minimal fluency loss. Logits\nintervention outperforms hidden-layer approaches. We show semantic-aware logits\nintervention is stable and effective for debiasing aligned LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cdzero-shot logits-layer debiasing\u65b9\u6cd5\uff0c\u5206\u522b\u662f\u9759\u6001\u548c\u52a8\u6001\u65b9\u6cd5\u3002", "motivation": "\u51cf\u5c11\u504f\u5dee", "method": "Logits\u5e72\u9884", "result": "\u52a8\u6001\u65b9\u6cd5\u51cf\u5c11\u4e86\u9ad8\u8fbe70%\u7684\u504f\u5dee\uff0c\u4e14\u6d41\u7545\u5ea6\u635f\u5931\u6700\u5c0f\u3002Logits\u5e72\u9884\u4f18\u4e8e\u9690\u85cf\u5c42\u65b9\u6cd5\u3002", "conclusion": "\u8bed\u4e49\u611f\u77e5\u7684Logits\u5e72\u9884\u5bf9\u4e8edebiasing aligned LLMs\u662f\u7a33\u5b9a\u4e14\u6709\u6548\u7684\u3002"}}
{"id": "2510.24020", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24020", "abs": "https://arxiv.org/abs/2510.24020", "authors": ["Hao An", "Yang Xu"], "title": "Teaching LLMs to Abstain via Fine-Grained Semantic Confidence Reward", "comment": "23pages, 4figures", "summary": "Mitigating hallucinations in Large Language Models (LLMs) is critical for\ntheir reliable deployment. Existing methods typically fine-tune LLMs to abstain\nfrom answering questions beyond their knowledge scope. However, these methods\noften rely on coarse-grained signals to guide LLMs to abstain, such as overall\nconfidence or uncertainty scores on multiple sampled answers, which may result\nin an imprecise awareness of the model's own knowledge boundaries. To this end,\nwe propose a novel reinforcement learning framework built on\n$\\textbf{\\underline{Fi}ne-grained \\underline{S}emantic \\underline{Co}nfidence\n\\underline{Re}ward (\\Ours)}$, which guides LLMs to abstain via sample-specific\nconfidence. Specifically, our method operates by sampling multiple candidate\nanswers and conducting semantic clustering, then training the LLM to retain\nanswers within high-confidence clusters and discard those within low-confidence\nones, thereby promoting accurate post-hoc abstention. Additionally, we propose\na new metric for evaluating the reliability of abstention fine-tuning tasks\nmore comprehensively. Our method significantly enhances reliability in both\nin-domain and out-of-distribution benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u8bed\u4e49\u7f6e\u4fe1\u5ea6\u5956\u52b1\u6765\u5f15\u5bfc\u5927\u578b\u8bed\u8a00\u6a21\u578b\u907f\u514d\u56de\u7b54\u8d85\u51fa\u5176\u77e5\u8bc6\u8303\u56f4\u7684\u95ee\u9898\uff0c\u4ece\u800c\u51cf\u8f7b\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u7c97\u7c92\u5ea6\u7684\u4fe1\u53f7\u6765\u5f15\u5bfcLLM\u907f\u514d\u56de\u7b54\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u5bf9\u81ea\u8eab\u77e5\u8bc6\u8fb9\u754c\u7684\u611f\u77e5\u4e0d\u7cbe\u786e\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u591a\u4e2a\u5019\u9009\u7b54\u6848\u8fdb\u884c\u62bd\u6837\u548c\u8bed\u4e49\u805a\u7c7b\uff0c\u7136\u540e\u8bad\u7ec3LLM\u4fdd\u7559\u9ad8\u7f6e\u4fe1\u5ea6\u7c07\u5185\u7684\u7b54\u6848\uff0c\u5e76\u4e22\u5f03\u4f4e\u7f6e\u4fe1\u5ea6\u7c07\u5185\u7684\u7b54\u6848\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u51cf\u8f7b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.24085", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24085", "abs": "https://arxiv.org/abs/2510.24085", "authors": ["Md. Shihab Uddin", "Md Nazmus Shakib", "Rahul Bhadani"], "title": "Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach", "comment": null, "summary": "The increasing adoption of electric vehicles (EVs) necessitates an\nunderstanding of their driving behavior to enhance traffic safety and develop\nsmart driving systems. This study compares classical and machine learning\nmodels for EV car following behavior. Classical models include the Intelligent\nDriver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative\nVelocity (OVRV), and a simplified CACC model, while the machine learning\napproach employs a Random Forest Regressor. Using a real world dataset of an EV\nfollowing an internal combustion engine (ICE) vehicle under varied driving\nconditions, we calibrated classical model parameters by minimizing the RMSE\nbetween predictions and real data. The Random Forest model predicts\nacceleration using spacing, speed, and gap type as inputs. Results demonstrate\nthe Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),\n0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,\nCACC performed best, with an RMSE of 2.67 for long gaps. These findings\nhighlight the machine learning model's performance across all scenarios. Such\nmodels are valuable for simulating EV behavior and analyzing mixed autonomy\ntraffic dynamics in EV integrated environments.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u7ecf\u5178\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u7535\u52a8\u6c7d\u8f66\u8ddf\u968f\u884c\u4e3a\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u5404\u79cd\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u7ecf\u5178\u6a21\u578b\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u4ea4\u901a\u5b89\u5168\u548c\u5f00\u53d1\u667a\u80fd\u9a7e\u9a76\u7cfb\u7edf\uff0c\u9700\u8981\u4e86\u89e3\u7535\u52a8\u6c7d\u8f66\u7684\u9a7e\u9a76\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u667a\u80fd\u9a7e\u9a76\u6a21\u578b\uff08IDM\uff09\u3001\u6700\u4f18\u901f\u5ea6\u6a21\u578b\uff08OVM\uff09\u3001\u6700\u4f18\u901f\u5ea6\u76f8\u5bf9\u901f\u5ea6\uff08OVRV\uff09\u548c\u4e00\u4e2a\u7b80\u5316\u7684CACC\u6a21\u578b\u4f5c\u4e3a\u7ecf\u5178\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u968f\u673a\u68ee\u6797\u56de\u5f52\u6a21\u578b\u4f5c\u4e3a\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6700\u5c0f\u5316RMSE\u6765\u6821\u51c6\u7ecf\u5178\u6a21\u578b\u53c2\u6570\u3002\u968f\u673a\u68ee\u6797\u6a21\u578b\u4f7f\u7528\u95f4\u8ddd\u3001\u901f\u5ea6\u548c\u95f4\u9699\u7c7b\u578b\u4f5c\u4e3a\u8f93\u5165\u6765\u9884\u6d4b\u52a0\u901f\u5ea6\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u51c6\u786e\u6027\uff0c\u5728\u4e0d\u540c\u95f4\u9699\u4e0b\u7684RMSE\u5206\u522b\u4e3a0.0046\uff08\u4e2d\u7b49\u95f4\u9699\uff09\u30010.0016\uff08\u957f\u95f4\u9699\uff09\u548c0.0025\uff08\u8d85\u957f\u95f4\u9699\uff09\u3002\u5728\u57fa\u4e8e\u7269\u7406\u7684\u6a21\u578b\u4e2d\uff0cCACC\u8868\u73b0\u6700\u4f73\uff0c\u957f\u95f4\u9699\u4e0b\u7684RMSE\u4e3a2.67\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6240\u6709\u573a\u666f\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5bf9\u6a21\u62df\u7535\u52a8\u6c7d\u8f66\u884c\u4e3a\u548c\u5206\u6790\u7535\u52a8\u6c7d\u8f66\u96c6\u6210\u73af\u5883\u4e2d\u7684\u6df7\u5408\u81ea\u4e3b\u4ea4\u901a\u52a8\u6001\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.24036", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24036", "abs": "https://arxiv.org/abs/2510.24036", "authors": ["Xingyu Liu", "Kun Ming Goh"], "title": "ResNet: Enabling Deep Convolutional Neural Networks through Residual Learning", "comment": "3 pages, 5 figures, 1 table", "summary": "Convolutional Neural Networks (CNNs) has revolutionized computer vision, but\ntraining very deep networks has been challenging due to the vanishing gradient\nproblem. This paper explores Residual Networks (ResNet), introduced by He et\nal. (2015), which overcomes this limitation by using skip connections. ResNet\nenables the training of networks with hundreds of layers by allowing gradients\nto flow directly through shortcut connections that bypass intermediate layers.\nIn our implementation on the CIFAR-10 dataset, ResNet-18 achieves 89.9%\naccuracy compared to 84.1% for a traditional deep CNN of similar depth, while\nalso converging faster and training more stably.", "AI": {"tldr": "ResNet overcomes the vanishing gradient problem using skip connections, enabling training of very deep networks.", "motivation": "Training very deep CNNs is challenging due to the vanishing gradient problem.", "method": "Use skip connections to allow gradients to flow directly through shortcut connections that bypass intermediate layers.", "result": "ResNet-18 achieves 89.9% accuracy on CIFAR-10, compared to 84.1% for a traditional deep CNN.", "conclusion": "ResNet enables the training of networks with hundreds of layers, converging faster and training more stably."}}
{"id": "2510.23652", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23652", "abs": "https://arxiv.org/abs/2510.23652", "authors": ["Yao Lu", "Yuqi Li", "Wenbin Xie", "Shanqing Yu", "Qi Xuan", "Zhaowei Zhu", "Shiping Wen"], "title": "The Structural Scalpel: Automated Contiguous Layer Pruning for Large Language Models", "comment": null, "summary": "Although large language models (LLMs) have achieved revolutionary\nbreakthroughs in many fields, their large model size and high computational\ncost pose significant challenges for practical deployment on\nresource-constrained edge devices. To this end, layer pruning has been proposed\nto reduce the computational overhead by directly removing redundant layers.\nHowever, existing layer pruning methods typically rely on hand-crafted metrics\nto evaluate and remove individual layers, while ignoring the dependencies\nbetween layers. This can disrupt the model's information flow and severely\ndegrade performance. To address these issues, we propose CLP, a novel\ncontinuous layer pruning framework that introduces two key innovations: a\ndifferentiable concave gate algorithm that automatically identifies the best\ncontinuous layer segments for pruning via gradient-based optimization; and a\ncutoff endpoint tuning strategy that effectively restores model performance by\nfine-tuning only the layers adjacent to the pruned segments. Extensive\nexperiments across multiple model architectures (including LLaMA2, LLaMA3 and\nQwen) and sizes (from $7$B to $70$B parameters) show that CLP significantly\noutperforms existing state-of-the-art baselines. For example, at a pruning rate\nof $20\\%$, CLP achieves an average performance retention of $95.34\\%$ on\nLLaMA3-70B, outperforming baselines by $4.29\\%$-$30.52\\%$. Furthermore, CLP can\nbe seamlessly combined with quantization to further compress the model with\nonly a slight performance loss.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8fde\u7eed\u5c42\u526a\u679d\u6846\u67b6\uff08CLP\uff09\uff0c\u7528\u4e8e\u538b\u7f29\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5c42\u526a\u679d\u65b9\u6cd5\u5ffd\u7565\u4e86\u5c42\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u53ef\u5fae\u7684\u51f9\u95e8\u7b97\u6cd5\u548c\u622a\u6b62\u7aef\u70b9\u8c03\u6574\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u67b6\u6784\u548c\u5c3a\u5bf8\u4e0a\uff0cCLP \u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u4f8b\u5982\uff0c\u5728 20% \u7684\u526a\u679d\u7387\u4e0b\uff0cCLP \u5728 LLaMA3-70B \u4e0a\u5b9e\u73b0\u4e86 95.34% \u7684\u5e73\u5747\u6027\u80fd\u4fdd\u6301\u7387\u3002", "conclusion": "CLP \u80fd\u591f\u6709\u6548\u538b\u7f29\u6a21\u578b\uff0c\u4e14\u80fd\u4e0e\u91cf\u5316\u6280\u672f\u7ed3\u5408\u4f7f\u7528\u3002"}}
{"id": "2510.24021", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24021", "abs": "https://arxiv.org/abs/2510.24021", "authors": ["Haiduo Huang", "Jiangcheng Song", "Yadong Zhang", "Pengju Ren"], "title": "SpecKD: Speculative Decoding for Effective Knowledge Distillation of LLMs", "comment": null, "summary": "Knowledge Distillation (KD) has become a cornerstone technique for\ncompressing Large Language Models (LLMs) into smaller, more efficient student\nmodels. However, conventional KD approaches typically apply the distillation\nloss uniformly across all tokens, regardless of the teacher's confidence. This\nindiscriminate mimicry can introduce noise, as the student is forced to learn\nfrom the teacher's uncertain or high-entropy predictions, which may ultimately\nharm student performance-especially when the teacher is much larger and more\npowerful. To address this, we propose Speculative Knowledge Distillation\n(SpecKD), a novel, plug-and-play framework that introduces a dynamic,\ntoken-level gating mechanism inspired by the \"propose-and-verify\" paradigm of\nspeculative decoding. At each step, the student's token proposal is verified\nagainst the teacher's distribution; the distillation loss is selectively\napplied only to \"accepted\" tokens, while \"rejected\" tokens are masked out.\nExtensive experiments on diverse text generation tasks show that SpecKD\nconsistently and significantly outperforms strong KD baselines, leading to more\nstable training and more capable student models, and achieving state-of-the-art\nresults.", "AI": {"tldr": "SpecKD: A new knowledge distillation framework that uses a token-level gating mechanism to selectively apply the distillation loss only to accepted tokens, improving student performance.", "motivation": "Conventional KD approaches apply the distillation loss uniformly across all tokens, regardless of the teacher's confidence, which can introduce noise and harm student performance.", "method": "A dynamic, token-level gating mechanism inspired by the propose-and-verify paradigm of speculative decoding is introduced. The student's token proposal is verified against the teacher's distribution, and the distillation loss is selectively applied only to accepted tokens, while rejected tokens are masked out.", "result": "SpecKD consistently and significantly outperforms strong KD baselines, leading to more stable training and more capable student models, and achieving state-of-the-art results on diverse text generation tasks.", "conclusion": "SpecKD is a novel, plug-and-play framework that improves knowledge distillation by selectively applying the distillation loss based on teacher confidence, leading to better student performance."}}
{"id": "2510.24115", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24115", "abs": "https://arxiv.org/abs/2510.24115", "authors": ["Sandeep Vissapragada", "Vikrant Sahu", "Gagan Raj Gupta", "Vandita Singh"], "title": "HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology", "comment": null, "summary": "For doctors to truly trust artificial intelligence, it can't be a black box.\nThey need to understand its reasoning, almost as if they were consulting a\ncolleague. We created HistoLens1 to be that transparent, collaborative partner.\nIt allows a pathologist to simply ask a question in plain English about a\ntissue slide--just as they would ask a trainee. Our system intelligently\ntranslates this question into a precise query for its AI engine, which then\nprovides a clear, structured report. But it doesn't stop there. If a doctor\never asks, \"Why?\", HistoLens can instantly provide a 'visual proof' for any\nfinding--a heatmap that points to the exact cells and regions the AI used for\nits analysis. We've also ensured the AI focuses only on the patient's tissue,\njust like a trained pathologist would, by teaching it to ignore distracting\nbackground noise. The result is a workflow where the pathologist remains the\nexpert in charge, using a trustworthy AI assistant to verify their insights and\nmake faster, more confident diagnoses.", "AI": {"tldr": "HistoLens is an AI assistant for pathologists that provides transparent reasoning and visual proofs for its findings.", "motivation": "Doctors need to understand the reasoning behind AI to trust it.", "method": "The system translates plain English questions into precise queries for its AI engine and provides a structured report with visual proofs.", "result": "Pathologists can use HistoLens to verify insights and make faster, more confident diagnoses.", "conclusion": "HistoLens is a trustworthy AI assistant that helps pathologists in their workflow."}}
{"id": "2510.24037", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24037", "abs": "https://arxiv.org/abs/2510.24037", "authors": ["Shufan Shen", "Junshu Sun", "Shuhui Wang", "Qingming Huang"], "title": "Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for Vision Models", "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) aims to adapt pre-trained vision\nmodels to downstream tasks. Among PEFT paradigms, sparse tuning achieves\nremarkable performance by adjusting only the weights most relevant to\ndownstream tasks, rather than densely tuning the entire weight matrix. Current\nmethods follow a two-stage paradigm. First, it locates task-relevant weights by\ngradient information, which overlooks the parameter adjustments during\nfine-tuning and limits the performance. Second, it updates only the located\nweights by applying a sparse mask to the gradient of the weight matrix, which\nresults in high memory usage due to the storage of all weight matrices in the\noptimizer. In this paper, we propose a one-stage method named SNELLA to\novercome the above limitations. For memory usage, SNELLA selectively updates\nthe weight matrix by adding it to another sparse matrix that is merged by two\nlow-rank learnable matrices. We extend the low-rank decomposition by\nintroducing nonlinear kernel functions, thereby increasing the rank of the\nresulting merged matrix to prevent the interdependency among weight updates,\nenabling better adaptation to downstream tasks. For locating task-relevant\nweights, we propose an adaptive bi-level sparsity allocation mechanism that\nencourages weights to compete across and inside layers based on their\nimportance scores in an end-to-end manner. Extensive experiments are conducted\non classification, segmentation, and generation tasks using different\npre-trained vision models. The results show that SNELLA achieves SOTA\nperformance with low memory usage. Notably, SNELLA obtains 1.8% (91.9% v.s.\n90.1%) higher Top-1 accuracy on the FGVC benchmark compared to SPT-LoRA.\nCompared to previous methods, SNELLA achieves a memory reduction of 31.1%-39.9%\nacross models with parameter scales from 86M to 632M. Our source codes are\navailable at https://github.com/ssfgunner/SNELL.", "AI": {"tldr": "SNELLA\u662f\u4e00\u79cd\u5355\u9636\u6bb5\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u5408\u5e76\u4e24\u4e2a\u4f4e\u79e9\u53ef\u5b66\u4e60\u77e9\u9635\u6765\u9009\u62e9\u6027\u5730\u66f4\u65b0\u6743\u91cd\u77e9\u9635\uff0c\u5e76\u5f15\u5165\u975e\u7ebf\u6027\u6838\u51fd\u6570\u6765\u589e\u52a0\u5408\u5e76\u77e9\u9635\u7684\u79e9\uff0c\u4ece\u800c\u66f4\u597d\u5730\u9002\u5e94\u4e0b\u6e38\u4efb\u52a1\u3002\u6b64\u5916\uff0cSNELLA\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u53cc\u5c42\u7a00\u758f\u5206\u914d\u673a\u5236\uff0c\u4ee5\u9f13\u52b1\u6743\u91cd\u6839\u636e\u5176\u91cd\u8981\u6027\u5f97\u5206\u5728\u5c42\u95f4\u548c\u5c42\u5185\u7ade\u4e89\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u5fae\u8c03\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5c40\u9650\u6027\uff1a\u4e00\u662f\u901a\u8fc7\u68af\u5ea6\u4fe1\u606f\u5b9a\u4f4d\u4efb\u52a1\u76f8\u5173\u6743\u91cd\u65f6\u5ffd\u7565\u4e86\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u53c2\u6570\u8c03\u6574\uff1b\u4e8c\u662f\u4ec5\u66f4\u65b0\u5b9a\u4f4d\u7684\u6743\u91cd\u4f1a\u5bfc\u81f4\u9ad8\u5185\u5b58\u5360\u7528\u3002", "method": "SNELLA\u901a\u8fc7\u5c06\u6743\u91cd\u77e9\u9635\u6dfb\u52a0\u5230\u7531\u4e24\u4e2a\u4f4e\u79e9\u53ef\u5b66\u4e60\u77e9\u9635\u5408\u5e76\u800c\u6210\u7684\u53e6\u4e00\u4e2a\u7a00\u758f\u77e9\u9635\u6765\u9009\u62e9\u6027\u5730\u66f4\u65b0\u6743\u91cd\u77e9\u9635\u3002\u6b64\u5916\uff0cSNELLA\u5f15\u5165\u975e\u7ebf\u6027\u6838\u51fd\u6570\u6765\u6269\u5c55\u4f4e\u79e9\u5206\u89e3\uff0c\u5e76\u63d0\u51fa\u81ea\u9002\u5e94\u53cc\u5c42\u7a00\u758f\u5206\u914d\u673a\u5236\u3002", "result": "SNELLA\u5728\u5206\u7c7b\u3001\u5206\u5272\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86SOTA\u6027\u80fd\uff0c\u5e76\u4e14\u5185\u5b58\u5360\u7528\u8f83\u4f4e\u3002\u4f8b\u5982\uff0c\u5728FGVC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSNELLA\u7684Top-1\u51c6\u786e\u7387\u6bd4SPT-LoRA\u9ad81.8%\uff0891.9% vs. 90.1%\uff09\u3002", "conclusion": "SNELLA\u662f\u4e00\u79cd\u6709\u6548\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u5b83\u80fd\u591f\u4ee5\u8f83\u4f4e\u7684\u5185\u5b58\u5360\u7528\u5b9e\u73b0SOTA\u6027\u80fd\u3002"}}
{"id": "2510.23656", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23656", "abs": "https://arxiv.org/abs/2510.23656", "authors": ["Fuqiang Liu", "Weiping Ding", "Luis Miranda-Moreno", "Lijun Sun"], "title": "Error Adjustment Based on Spatiotemporal Correlation Fusion for Traffic Forecasting", "comment": "12 pages, 7 figures, 3 tables", "summary": "Deep neural networks (DNNs) play a significant role in an increasing body of\nresearch on traffic forecasting due to their effectively capturing\nspatiotemporal patterns embedded in traffic data. A general assumption of\ntraining the said forecasting models via mean squared error estimation is that\nthe errors across time steps and spatial positions are uncorrelated. However,\nthis assumption does not really hold because of the autocorrelation caused by\nboth the temporality and spatiality of traffic data. This gap limits the\nperformance of DNN-based forecasting models and is overlooked by current\nstudies. To fill up this gap, this paper proposes Spatiotemporally\nAutocorrelated Error Adjustment (SAEA), a novel and general framework designed\nto systematically adjust autocorrelated prediction errors in traffic\nforecasting. Unlike existing approaches that assume prediction errors follow a\nrandom Gaussian noise distribution, SAEA models these errors as a\nspatiotemporal vector autoregressive (VAR) process to capture their intrinsic\ndependencies. First, it explicitly captures both spatial and temporal error\ncorrelations by a coefficient matrix, which is then embedded into a newly\nformulated cost function. Second, a structurally sparse regularization is\nintroduced to incorporate prior spatial information, ensuring that the learned\ncoefficient matrix aligns with the inherent road network structure. Finally, an\ninference process with test-time error adjustment is designed to dynamically\nrefine predictions, mitigating the impact of autocorrelated errors in real-time\nforecasting. The effectiveness of the proposed approach is verified on\ndifferent traffic datasets. Results across a wide range of traffic forecasting\nmodels show that our method enhances performance in almost all cases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8c03\u6574\u4ea4\u901a\u9884\u6d4b\u4e2d\u65f6\u7a7a\u81ea\u76f8\u5173\u9884\u6d4b\u8bef\u5dee\u7684\u901a\u7528\u6846\u67b6SAEA\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5ffd\u7565\u4e86\u4ea4\u901a\u6570\u636e\u7684\u65f6\u95f4\u6027\u548c\u7a7a\u95f4\u6027\u5f15\u8d77\u7684\u81ea\u76f8\u5173\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u57fa\u4e8eDNN\u7684\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u5c06\u9884\u6d4b\u8bef\u5dee\u5efa\u6a21\u4e3a\u65f6\u7a7a\u5411\u91cf\u81ea\u56de\u5f52(VAR)\u8fc7\u7a0b\uff0c\u901a\u8fc7\u7cfb\u6570\u77e9\u9635\u663e\u5f0f\u5730\u6355\u6349\u7a7a\u95f4\u548c\u65f6\u95f4\u8bef\u5dee\u76f8\u5173\u6027\uff0c\u5e76\u5f15\u5165\u7ed3\u6784\u7a00\u758f\u6b63\u5219\u5316\u4ee5\u7ed3\u5408\u5148\u9a8c\u7a7a\u95f4\u4fe1\u606f\u3002", "result": "\u5728\u4e0d\u540c\u7684\u4ea4\u901a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u51e0\u4e4e\u6240\u6709\u60c5\u51b5\u4e0b\u90fd\u80fd\u63d0\u9ad8\u6027\u80fd\u3002", "conclusion": "SAEA\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u8c03\u6574\u4ea4\u901a\u9884\u6d4b\u4e2d\u7684\u81ea\u76f8\u5173\u9884\u6d4b\u8bef\u5dee\uff0c\u4ece\u800c\u63d0\u9ad8\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24023", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24023", "abs": "https://arxiv.org/abs/2510.24023", "authors": ["Saujas Vaduguru", "Yilun Hua", "Yoav Artzi", "Daniel Fried"], "title": "Success and Cost Elicit Convention Formation for Efficient Communication", "comment": null, "summary": "Humans leverage shared conversational context to become increasingly\nsuccessful and efficient at communicating over time. One manifestation of this\nis the formation of ad hoc linguistic conventions, which allow people to\ncoordinate on short, less costly utterances that are understood using shared\nconversational context. We present a method to train large multimodal models to\nform conventions, enabling efficient communication. Our approach uses simulated\nreference games between models, and requires no additional human-produced data.\nIn repeated reference games involving photographs and tangram images, our\nmethod enables models to communicate efficiently with people: reducing the\nmessage length by up to 41% while increasing success by 15% over the course of\nthe interaction. Human listeners respond faster when interacting with our model\nthat forms conventions. We also show that training based on success or cost\nalone is insufficient - both are necessary to elicit convention formation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u4ee5\u5f62\u6210\u7ea6\u5b9a\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u901a\u4fe1\u7684\u65b9\u6cd5\u3002", "motivation": "\u4eba\u7c7b\u5229\u7528\u5171\u4eab\u7684\u4f1a\u8bdd\u4e0a\u4e0b\u6587\u6765\u63d0\u9ad8\u901a\u4fe1\u7684\u6210\u529f\u7387\u548c\u6548\u7387\u3002\u8bed\u8a00\u60ef\u4f8b\u7684\u5f62\u6210\u5c31\u662f\u5176\u4e2d\u4e00\u79cd\u8868\u73b0\u5f62\u5f0f\uff0c\u5b83\u4f7f\u4eba\u4eec\u80fd\u591f\u534f\u8c03\u4f7f\u7528\u8f83\u77ed\u3001\u6210\u672c\u8f83\u4f4e\u7684\u8868\u8fbe\u65b9\u5f0f\uff0c\u8fd9\u4e9b\u8868\u8fbe\u65b9\u5f0f\u53ef\u4ee5\u4f7f\u7528\u5171\u4eab\u7684\u4f1a\u8bdd\u4e0a\u4e0b\u6587\u6765\u7406\u89e3\u3002", "method": "\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u7528\u6a21\u578b\u4e4b\u95f4\u7684\u6a21\u62df\u5f15\u7528\u6e38\u620f\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u989d\u5916\u7684\u4eba\u5de5\u751f\u6210\u6570\u636e\u3002", "result": "\u5728\u6d89\u53ca\u7167\u7247\u548c\u4e03\u5de7\u677f\u56fe\u50cf\u7684\u91cd\u590d\u5f15\u7528\u6e38\u620f\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u6a21\u578b\u80fd\u591f\u4e0e\u4eba\u8fdb\u884c\u6709\u6548\u7684\u901a\u4fe1\uff1a\u5728\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\uff0c\u6d88\u606f\u957f\u5ea6\u6700\u591a\u53ef\u51cf\u5c11 41%\uff0c\u800c\u6210\u529f\u7387\u53ef\u63d0\u9ad8 15%\u3002\u4e0e\u5f62\u6210\u7ea6\u5b9a\u7684\u6a21\u578b\u8fdb\u884c\u4ea4\u4e92\u65f6\uff0c\u4eba\u7c7b\u542c\u4f17\u7684\u53cd\u5e94\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "\u4ec5\u57fa\u4e8e\u6210\u529f\u6216\u6210\u672c\u7684\u8bad\u7ec3\u662f\u4e0d\u591f\u7684 - \u4e24\u8005\u5bf9\u4e8e\u5f15\u53d1\u7ea6\u5b9a\u5f62\u6210\u90fd\u662f\u5fc5\u8981\u7684\u3002"}}
{"id": "2510.24145", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24145", "abs": "https://arxiv.org/abs/2510.24145", "authors": ["Yu Luo", "Jiamin Jiang", "Jingfei Feng", "Lei Tao", "Qingliang Zhang", "Xidao Wen", "Yongqian Sun", "Shenglin Zhang", "Jielong Huang", "Nan Qi", "Dan Pei"], "title": "From Observability Data to Diagnosis: An Evolving Multi-agent System for Incident Management in Cloud Systems", "comment": null, "summary": "Incident management (IM) is central to the reliability of large-scale cloud\nsystems. Yet manual IM, where on-call engineers examine metrics, logs, and\ntraces is labor-intensive and error-prone in the face of massive and\nheterogeneous observability data. Existing automated IM approaches often\nstruggle to generalize across systems, provide limited interpretability, and\nincur high deployment costs, which hinders adoption in practice. In this paper,\nwe present OpsAgent, a lightweight, self-evolving multi-agent system for IM\nthat employs a training-free data processor to convert heterogeneous\nobservability data into structured textual descriptions, along with a\nmulti-agent collaboration framework that makes diagnostic inference transparent\nand auditable. To support continual capability growth, OpsAgent also introduces\na dual self-evolution mechanism that integrates internal model updates with\nexternal experience accumulation, thereby closing the deployment loop.\nComprehensive experiments on the OPENRCA benchmark demonstrate state-of-the-art\nperformance and show that OpsAgent is generalizable, interpretable,\ncost-efficient, and self-evolving, making it a practically deployable and\nsustainable solution for long-term operation in real-world cloud systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a OpsAgent \u7684\u8f7b\u91cf\u7ea7\u3001\u81ea\u6f14\u5316\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u4e8b\u4ef6\u7ba1\u7406 (IM)\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316 IM \u65b9\u6cd5\u96be\u4ee5\u8de8\u7cfb\u7edf\u901a\u7528\uff0c\u53ef\u89e3\u91ca\u6027\u6709\u9650\uff0c\u4e14\u90e8\u7f72\u6210\u672c\u9ad8\u6602\uff0c\u4ece\u800c\u963b\u788d\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "OpsAgent \u91c7\u7528\u4e86\u4e00\u79cd\u514d\u8bad\u7ec3\u7684\u6570\u636e\u5904\u7406\u5668\uff0c\u5c06\u5f02\u6784\u7684\u53ef\u89c2\u5bdf\u6027\u6570\u636e\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u4ee5\u53ca\u4e00\u4e2a\u591a\u4ee3\u7406\u534f\u4f5c\u6846\u67b6\uff0c\u4f7f\u8bca\u65ad\u63a8\u7406\u900f\u660e\u4e14\u53ef\u5ba1\u8ba1\u3002\u5b83\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u53cc\u91cd\u81ea\u6f14\u5316\u673a\u5236\uff0c\u5c06\u5185\u90e8\u6a21\u578b\u66f4\u65b0\u4e0e\u5916\u90e8\u7ecf\u9a8c\u79ef\u7d2f\u76f8\u7ed3\u5408\u3002", "result": "\u5728 OPENRCA \u57fa\u51c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cOpsAgent \u5177\u6709\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u662f\u53ef\u63a8\u5e7f\u7684\u3001\u53ef\u89e3\u91ca\u7684\u3001\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\u548c\u81ea\u6f14\u5316\u7684\u3002", "conclusion": "OpsAgent \u662f\u4e00\u79cd\u53ef\u5728\u5b9e\u9645\u4e2d\u90e8\u7f72\u4e14\u53ef\u6301\u7eed\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u4e91\u7cfb\u7edf\u4e2d\u7684\u957f\u671f\u8fd0\u884c\u3002"}}
{"id": "2510.24038", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24038", "abs": "https://arxiv.org/abs/2510.24038", "authors": ["Xingyu Zhu", "Beier Zhu", "Shuo Wang", "Kesen Zhao", "Hanwang Zhang"], "title": "Enhancing CLIP Robustness via Cross-Modality Alignment", "comment": "NeurIPS 2025 Spotlight", "summary": "Vision-language models (VLMs) such as CLIP demonstrate strong generalization\nin zero-shot classification but remain highly vulnerable to adversarial\nperturbations. Existing methods primarily focus on adversarial fine-tuning or\nprompt optimization; they often overlook the gaps in CLIP's encoded features,\nwhich is shown as the text and image features lie far apart from each other.\nThis misalignment is significantly amplified under adversarial perturbations,\nleading to severe degradation in classification performance. To address this\nproblem, we propose Cross-modality Alignment, dubbed COLA, an optimal\ntransport-based framework that explicitly addresses adversarial misalignment by\nrestoring both global image-text alignment and local structural consistency in\nthe feature space. (1) COLA first projects adversarial image embeddings onto a\nsubspace spanned by class text features, effectively filtering out non-semantic\ndistortions while preserving discriminative information. (2) It then models\nimages and texts as discrete distributions over multiple augmented views and\nrefines their alignment via OT, with the subspace projection seamlessly\nintegrated into the cost computation. This design ensures stable cross-modal\nalignment even under adversarial conditions. COLA is training-free and\ncompatible with existing fine-tuned models. Extensive evaluations across 14\nzero-shot classification benchmarks demonstrate the effectiveness of COLA,\nespecially with an average improvement of 6.7% on ImageNet and its variants\nunder PGD adversarial attacks, while maintaining high accuracy on clean\nsamples.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCOLA\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3CLIP\u7b49\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u6297\u6270\u52a8\u4e0b\u7684\u8106\u5f31\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u5bf9\u6297\u5fae\u8c03\u6216\u63d0\u793a\u4f18\u5316\u4e0a\uff0c\u4f46\u5ffd\u7565\u4e86CLIP\u7f16\u7801\u7279\u5f81\u4e2d\u7684\u5dee\u8ddd\uff0c\u5373\u6587\u672c\u548c\u56fe\u50cf\u7279\u5f81\u76f8\u8ddd\u751a\u8fdc\u3002\u8fd9\u79cd\u9519\u4f4d\u5728\u5bf9\u6297\u6270\u52a8\u4e0b\u4f1a\u663e\u8457\u653e\u5927\uff0c\u5bfc\u81f4\u5206\u7c7b\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u3002", "method": "COLA\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\uff0c\u901a\u8fc7\u6062\u590d\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u5168\u5c40\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u548c\u5c40\u90e8\u7ed3\u6784\u4e00\u81f4\u6027\u6765\u663e\u5f0f\u5730\u89e3\u51b3\u5bf9\u6297\u6027\u9519\u4f4d\u95ee\u9898\u3002\u5b83\u9996\u5148\u5c06\u5bf9\u6297\u56fe\u50cf\u5d4c\u5165\u6295\u5f71\u5230\u7c7b\u6587\u672c\u7279\u5f81\u6240\u8de8\u8d8a\u7684\u5b50\u7a7a\u95f4\u4e0a\uff0c\u6709\u6548\u5730\u8fc7\u6ee4\u6389\u975e\u8bed\u4e49\u626d\u66f2\uff0c\u540c\u65f6\u4fdd\u7559\u5224\u522b\u4fe1\u606f\u3002\u7136\u540e\uff0c\u5b83\u5c06\u56fe\u50cf\u548c\u6587\u672c\u5efa\u6a21\u4e3a\u591a\u4e2a\u589e\u5f3a\u89c6\u56fe\u4e0a\u7684\u79bb\u6563\u5206\u5e03\uff0c\u5e76\u901a\u8fc7OT\u4f18\u5316\u5b83\u4eec\u7684\u5bf9\u9f50\uff0c\u5b50\u7a7a\u95f4\u6295\u5f71\u65e0\u7f1d\u96c6\u6210\u5230\u6210\u672c\u8ba1\u7b97\u4e2d\u3002", "result": "\u572814\u4e2azero-shot\u5206\u7c7b\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u8bc4\u4f30\u8868\u660e\u4e86COLA\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u5728ImageNet\u53ca\u5176\u53d8\u4f53\u4e0a\uff0c\u5728PGD\u5bf9\u6297\u653b\u51fb\u4e0b\u5e73\u5747\u63d0\u9ad8\u4e866.7%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5728\u5e72\u51c0\u6837\u672c\u4e0a\u7684\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "COLA\u662f\u4e00\u79cd\u514d\u8bad\u7ec3\u4e14\u4e0e\u73b0\u6709\u5fae\u8c03\u6a21\u578b\u517c\u5bb9\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u6297\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.23657", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23657", "abs": "https://arxiv.org/abs/2510.23657", "authors": ["Saklain Niam", "Tashfiqur Rahman", "Md. Amjad Patwary", "Mukarram Hossain"], "title": "A machine learning framework integrating seed traits and plasma parameters for predicting germination uplift in crops", "comment": null, "summary": "Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet\noutcomes remain difficult to predict due to complex seed--plasma--environment\ninteractions. This study introduces the first machine learning framework to\nforecast germination uplift in soybean, barley, sunflower, radish, and tomato\nunder dielectric barrier discharge (DBD) plasma. Among the models tested (GB,\nXGB, ET, and hybrids), Extra Trees (ET) performed best (R\\textsuperscript{2} =\n0.919; RMSE = 3.21; MAE = 2.62), improving to R\\textsuperscript{2} = 0.925\nafter feature reduction. Engineering analysis revealed a hormetic response:\nnegligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for\n200--500 s, and reduced germination beyond 20 kV or prolonged exposures.\nDischarge power was also a dominant factor, with germination rate maximizing at\n$\\geq$100 W with low exposure time. Species and cultivar-level predictions\nshowed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high\nconsistency, while sunflower remained slightly higher variable (MAE = 3.80).\nAmong cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,\nwhile Arian (2.86) and Ny\\'{\\i}rs\\'{e}gi fekete (3.74) were comparatively\npoorly captured. This framework was also embedded into MLflow, providing a\ndecision-support tool for optimizing CP seed germination in precision\nagriculture.", "AI": {"tldr": "This study uses machine learning to predict the effect of cold plasma on seed germination.", "motivation": "Predicting the outcomes of cold plasma treatment on seed germination is difficult due to complex interactions.", "method": "Machine learning models (GB, XGB, ET, and hybrids) were used to forecast germination uplift in soybean, barley, sunflower, radish, and tomato under dielectric barrier discharge (DBD) plasma. Feature reduction and engineering analysis were also performed.", "result": "Extra Trees (ET) performed best (R\r{2} = 0.919; RMSE = 3.21; MAE = 2.62), improving to R\r{2} = 0.925 after feature reduction. Germination rate maximized at $\\geq$100 W with low exposure time. Radish and soybean were modeled with high consistency, while sunflower remained slightly higher variable. Williams and Sari were well predicted, while Arian and Ny\\'{\\i}rs\\'{e}gi fekete were comparatively poorly captured.", "conclusion": "This framework was also embedded into MLflow, providing a decision-support tool for optimizing CP seed germination in precision agriculture."}}
{"id": "2510.24051", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24051", "abs": "https://arxiv.org/abs/2510.24051", "authors": ["In Gim", "Zhiyao Ma", "Seung-seob Lee", "Lin Zhong"], "title": "Pie: A Programmable Serving System for Emerging LLM Applications", "comment": "SOSP 2025. Source code available at\n  https://github.com/pie-project/pie", "summary": "Emerging large language model (LLM) applications involve diverse reasoning\nstrategies and agentic workflows, straining the capabilities of existing\nserving systems built on a monolithic token generation loop. This paper\nintroduces Pie, a programmable LLM serving system designed for flexibility and\nefficiency. Pie decomposes the traditional generation loop into fine-grained\nservice handlers exposed via an API and delegates control of the generation\nprocess to user-provided programs, called inferlets. This enables applications\nto implement new KV cache strategies, bespoke generation logic, and seamlessly\nintegrate computation and I/O-entirely within the application, without\nrequiring modifications to the serving system. Pie executes inferlets using\nWebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows\nPie matches state-of-the-art performance on standard tasks (3-12% latency\noverhead) while significantly improving latency and throughput (1.3x-3.4x\nhigher) on agentic workflows by enabling application-specific optimizations.", "AI": {"tldr": "Pie\u662f\u4e00\u4e2a\u53ef\u7f16\u7a0b\u7684LLM\u670d\u52a1\u7cfb\u7edf\uff0c\u65e8\u5728\u63d0\u4f9b\u7075\u6d3b\u6027\u548c\u6548\u7387\uff0c\u901a\u8fc7\u5c06\u4f20\u7edf\u7684\u751f\u6210\u5faa\u73af\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u7684\u670d\u52a1\u5904\u7406\u7a0b\u5e8f\u5e76\u901a\u8fc7API\u516c\u5f00\uff0c\u5e76\u5c06\u751f\u6210\u8fc7\u7a0b\u7684\u63a7\u5236\u6743\u59d4\u6258\u7ed9\u7528\u6237\u63d0\u4f9b\u7684\u7a0b\u5e8f\uff08inferlet\uff09\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5355\u7247\u4ee4\u724c\u751f\u6210\u5faa\u73af\u6784\u5efa\u7684\u670d\u52a1\u7cfb\u7edf\u65e0\u6cd5\u6ee1\u8db3\u65b0\u5174\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5e94\u7528\u4e2d\u6d89\u53ca\u7684\u5404\u79cd\u63a8\u7406\u7b56\u7565\u548c\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "Pie\u5c06\u4f20\u7edf\u7684\u751f\u6210\u5faa\u73af\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u7684\u670d\u52a1\u5904\u7406\u7a0b\u5e8f\uff0c\u5e76\u901a\u8fc7API\u516c\u5f00\uff0c\u5e76\u5c06\u751f\u6210\u8fc7\u7a0b\u7684\u63a7\u5236\u6743\u59d4\u6258\u7ed9\u7528\u6237\u63d0\u4f9b\u7684\u7a0b\u5e8f\uff08inferlet\uff09\u3002Pie\u4f7f\u7528WebAssembly\u6267\u884cinferlet\uff0c\u53d7\u76ca\u4e8e\u5176\u8f7b\u91cf\u7ea7\u6c99\u7bb1\u3002", "result": "Pie\u5728\u6807\u51c6\u4efb\u52a1\u4e0a\u4e0e\u6700\u5148\u8fdb\u7684\u6027\u80fd\u76f8\u5339\u914d\uff083-12%\u7684\u5ef6\u8fdf\u5f00\u9500\uff09\uff0c\u540c\u65f6\u901a\u8fc7\u542f\u7528\u7279\u5b9a\u4e8e\u5e94\u7528\u7a0b\u5e8f\u7684\u4f18\u5316\uff0c\u663e\u7740\u63d0\u9ad8\u4e86\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u7684\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\uff08\u63d0\u9ad8\u4e861.3\u500d-3.4\u500d\uff09\u3002", "conclusion": "Pie\u662f\u4e00\u4e2a\u6709\u524d\u9014\u7684LLM\u670d\u52a1\u7cfb\u7edf\uff0c\u5b83\u53ef\u4ee5\u901a\u8fc7\u5b9e\u73b0\u7279\u5b9a\u4e8e\u5e94\u7528\u7a0b\u5e8f\u7684\u4f18\u5316\u6765\u63d0\u9ad8\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u7684\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u3002"}}
{"id": "2510.24151", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24151", "abs": "https://arxiv.org/abs/2510.24151", "authors": ["Bingsen Qiu", "Zijian Liu", "Xiao Liu", "Haoshen Yang", "Zeren Gao", "Bingjie Wang", "Feier Zhang", "Yixuan Qin", "Chunyan Li"], "title": "BMGQ: A Bottom-up Method for Generating Complex Multi-hop Reasoning Questions from Semi-structured Data", "comment": null, "summary": "Building training-ready multi-hop question answering (QA) datasets that truly\nstress a model's retrieval and reasoning abilities remains highly challenging\nrecently. While there have been a few recent evaluation datasets that capture\nthe characteristics of hard-to-search but easy-to-verify problems -- requiring\nthe integration of ambiguous, indirect, and cross-domain cues -- these data\nresources remain scarce and are mostly designed for evaluation, making them\nunsuitable for supervised fine-tuning (SFT) or reinforcement learning (RL).\nMeanwhile, manually curating non-trivially retrievable questions -- where\nanswers cannot be found through a single direct query but instead require\nmulti-hop reasoning over oblique and loosely connected evidence -- incurs\nprohibitive human costs and fails to scale, creating a critical data bottleneck\nfor training high-capability retrieval-and-reasoning agents.\n  To address this, we present an automated framework for generating\nhigh-difficulty, training-ready multi-hop questions from semi-structured\nknowledge sources. The system (i) grows diverse, logically labeled evidence\nclusters through Natural Language Inference (NLI)-based relation typing and\ndiversity-aware expansion; (ii) applies reverse question construction to\ncompose oblique cues so that isolated signals are underinformative but their\ncombination uniquely identifies the target entity; and (iii) enforces quality\nwith a two-step evaluation pipeline that combines multi-model consensus\nfiltering with structured constraint decomposition and evidence-based matching.\nThe result is a scalable process that yields complex, retrieval-resistant yet\nverifiable questions suitable for SFT/RL training as well as challenging\nevaluation, substantially reducing human curation effort while preserving the\ndifficulty profile of strong evaluation benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u534a\u7ed3\u6784\u5316\u77e5\u8bc6\u6e90\u751f\u6210\u9ad8\u96be\u5ea6\u3001\u53ef\u7528\u4e8e\u8bad\u7ec3\u7684\u591a\u8df3\u95ee\u9898\u3002", "motivation": "\u6700\u8fd1\uff0c\u6784\u5efa\u80fd\u591f\u771f\u6b63\u5f3a\u8c03\u6a21\u578b\u68c0\u7d22\u548c\u63a8\u7406\u80fd\u529b\u7684\u591a\u8df3\u95ee\u7b54 (QA) \u6570\u636e\u96c6\u4ecd\u7136\u5177\u6709\u5f88\u9ad8\u7684\u6311\u6218\u6027\u3002\u540c\u65f6\uff0c\u624b\u52a8\u7ba1\u7406\u975e\u5e73\u51e1\u7684\u53ef\u68c0\u7d22\u95ee\u9898\u4f1a\u4ea7\u751f\u8fc7\u9ad8\u7684\u4eba\u529b\u6210\u672c\u5e76\u4e14\u65e0\u6cd5\u6269\u5c55\uff0c\u4ece\u800c\u4e3a\u8bad\u7ec3\u9ad8\u80fd\u529b\u68c0\u7d22\u548c\u63a8\u7406\u4ee3\u7406\u521b\u5efa\u4e86\u5173\u952e\u7684\u6570\u636e\u74f6\u9888\u3002", "method": "\u8be5\u7cfb\u7edf (i) \u901a\u8fc7\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u63a8\u7406 (NLI) \u7684\u5173\u7cfb\u7c7b\u578b\u548c\u591a\u6837\u6027\u611f\u77e5\u6269\u5c55\u6765\u589e\u957f\u591a\u6837\u5316\u7684\u3001\u903b\u8f91\u6807\u8bb0\u7684\u8bc1\u636e\u96c6\u7fa4\uff1b(ii) \u5e94\u7528\u53cd\u5411\u95ee\u9898\u6784\u5efa\u6765\u7ec4\u6210\u503e\u659c\u7684\u7ebf\u7d22\uff0c\u4ee5\u4fbf\u5b64\u7acb\u7684\u4fe1\u53f7\u4fe1\u606f\u4e0d\u8db3\uff0c\u4f46\u5b83\u4eec\u7684\u7ec4\u5408\u552f\u4e00\u5730\u6807\u8bc6\u76ee\u6807\u5b9e\u4f53\uff1b(iii) \u901a\u8fc7\u5c06\u591a\u6a21\u578b\u5171\u8bc6\u8fc7\u6ee4\u4e0e\u7ed3\u6784\u5316\u7ea6\u675f\u5206\u89e3\u548c\u57fa\u4e8e\u8bc1\u636e\u7684\u5339\u914d\u76f8\u7ed3\u5408\u7684\u4e24\u6b65\u8bc4\u4f30\u7ba1\u9053\u6765\u5f3a\u5236\u6267\u884c\u8d28\u91cf\u3002", "result": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u4ea7\u751f\u590d\u6742\u7684\u3001\u6297\u68c0\u7d22\u4f46\u53ef\u9a8c\u8bc1\u7684\u95ee\u9898\uff0c\u9002\u7528\u4e8e SFT/RL \u8bad\u7ec3\u4ee5\u53ca\u5177\u6709\u6311\u6218\u6027\u7684\u8bc4\u4f30\uff0c\u4ece\u800c\u5927\u5927\u51cf\u5c11\u4e86\u4eba\u5de5\u7ba1\u7406\u5de5\u4f5c\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5f3a\u5927\u7684\u8bc4\u4f30\u57fa\u51c6\u7684\u96be\u5ea6\u6982\u51b5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u9ad8\u96be\u5ea6\u7684\u591a\u8df3\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u53ef\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u6a21\u578b\uff0c\u5e76\u5927\u5927\u51cf\u5c11\u4e86\u4eba\u5de5\u7ba1\u7406\u5de5\u4f5c\u3002"}}
{"id": "2510.24078", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24078", "abs": "https://arxiv.org/abs/2510.24078", "authors": ["William Yang", "Xindi Wu", "Zhiwei Deng", "Esin Tureci", "Olga Russakovsky"], "title": "Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification", "comment": null, "summary": "Text-to-image (T2I) models are increasingly used for synthetic dataset\ngeneration, but generating effective synthetic training data for classification\nremains challenging. Fine-tuning a T2I model with a few real examples can help\nimprove the quality of synthetic training data; however, it may also cause\noverfitting and reduce diversity in the generated samples. We propose a\nfine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for\nfine-grained classification. Given a small set of real examples, we first\nextract class-agnostic attributes such as scene background and object pose. We\nthen explicitly condition on these attributes during fine-tuning of the T2I\nmodel and marginalize them out during generation. This design mitigates\noverfitting, preserves the T2I model's generative prior, reduces estimation\nerrors, and further minimizes unintended inter-class associations. Extensive\nexperiments across multiple T2I models, backbones, and datasets show that our\nmethod achieves state-of-the-art performance in low-shot fine-grained\nclassification when augmented with synthetic data. Concretely, BOB outperforms\nDataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning\na CLIP classifier with five real images augmented with 100 synthetic images).\nIn three of the four benchmarks, fine-tuning downstream models with 5 real\nimages augmented with BOB achieves better performance than fine-tuning with 10\nreal images. Collectively, BOB outperforms prior art in 18 of 24 experimental\nsettings, with 2+% accuracy improvements in 14 of these settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBOB\u7684\u5fae\u8c03\u7b56\u7565\uff0c\u7528\u4e8e\u6539\u8fdb\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5408\u6210\u6570\u636e\u751f\u6210\uff0c\u7279\u522b\u662f\u5728\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1\u4e2d\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u751f\u6210\u7528\u4e8e\u5206\u7c7b\u7684\u6709\u6548\u5408\u6210\u8bad\u7ec3\u6570\u636e\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u5fae\u8c03\u53ef\u80fd\u5bfc\u81f4\u8fc7\u62df\u5408\u548c\u591a\u6837\u6027\u964d\u4f4e\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u63d0\u53d6\u7c7b\u65e0\u5173\u5c5e\u6027\uff08\u5982\u573a\u666f\u80cc\u666f\u548c\u5bf9\u8c61\u59ff\u6001\uff09\uff0c\u7136\u540e\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u663e\u5f0f\u5730\u4ee5\u8fd9\u4e9b\u5c5e\u6027\u4e3a\u6761\u4ef6\uff0c\u5e76\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5c06\u5b83\u4eec\u8fb9\u7f18\u5316\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5728Aircraft\u6570\u636e\u96c6\u4e0a\uff0cBOB\u6bd4DataDream\u63d0\u9ad8\u4e867.4%\u3002", "conclusion": "BOB\u572824\u4e2a\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\u768418\u4e2a\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5176\u4e2d14\u4e2a\u8bbe\u7f6e\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e862%\u4ee5\u4e0a\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.23658", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23658", "abs": "https://arxiv.org/abs/2510.23658", "authors": ["Vaibhav Jindal", "Hejian Sang", "Chun-Mao Lai", "Yanning Chen", "Zhipeng Wang"], "title": "Aligning Diffusion Language Models via Unpaired Preference Optimization", "comment": null, "summary": "Diffusion language models (dLLMs) are an emerging alternative to\nautoregressive (AR) generators, but aligning them to human preferences is\nchallenging because sequence log-likelihoods are intractable and pairwise\npreference data are costly to collect. We introduce ELBO-KTO, which combines an\nELBO surrogate for diffusion log-likelihoods with a prospect-theoretic,\nunpaired preference objective (Kahneman Tversky Optimization, KTO). We analyze\nthe bias and variance induced by the ELBO substitution and employ\nvariance-reduction practices that stabilize gradients during training. Applied\nto LLaDA-8B-Instruct, ELBO-KTO yields \\textbf{65.9\\%} and \\textbf{62.3\\%}\nadjusted win rates on kto-mix-14k and UltraFeedback-Binary, respectively,\nversus the base model under an automatic LLM judge. Across downstream tasks,\nincluding GSM8K, MMLU, and additional reasoning/knowledge benchmarks, ELBO-KTO\ntrained on UltraFeedback-Binary performs on par with or better than the base\nmodel under identical decoding. This establishes unpaired preference\noptimization as a viable alternative to pairwise alignment in diffusion LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u9f50\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u65b9\u6cd5\uff0c\u79f0\u4e3aELBO-KTO\uff0c\u5b83\u7ed3\u5408\u4e86ELBO\u66ff\u4ee3\u548c\u524d\u666f\u7406\u8bba\u7684\u975e\u914d\u5bf9\u504f\u597d\u76ee\u6807\u3002", "motivation": "\u7531\u4e8e\u5e8f\u5217\u5bf9\u6570\u4f3c\u7136\u96be\u4ee5\u5904\u7406\u4e14\u6210\u5bf9\u504f\u597d\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u5c06\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86ELBO\u66ff\u4ee3\u7684\u6269\u6563\u5bf9\u6570\u4f3c\u7136\u548c\u524d\u666f\u7406\u8bba\u7684\u975e\u914d\u5bf9\u504f\u597d\u76ee\u6807\uff08KTO\uff09\u3002\u540c\u65f6\uff0c\u5206\u6790\u4e86ELBO\u66ff\u4ee3\u5f15\u8d77\u7684\u504f\u5dee\u548c\u65b9\u5dee\uff0c\u5e76\u91c7\u7528\u65b9\u5dee\u51cf\u5c11\u5b9e\u8df5\u6765\u7a33\u5b9a\u8bad\u7ec3\u671f\u95f4\u7684\u68af\u5ea6\u3002", "result": "\u5728LLaDA-8B-Instruct\u4e0a\u5e94\u7528ELBO-KTO\uff0c\u5728kto-mix-14k\u548cUltraFeedback-Binary\u4e0a\u5206\u522b\u5b9e\u73b0\u4e8665.9%\u548c62.3%\u7684\u8c03\u6574\u80dc\u7387\uff0c\u4e0e\u57fa\u672c\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u5728\u81ea\u52a8LLM\u5224\u65ad\u4e0b\u8868\u73b0\u66f4\u597d\u3002\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0c\u5305\u62ecGSM8K\u3001MMLU\u548c\u989d\u5916\u7684\u63a8\u7406/\u77e5\u8bc6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728UltraFeedback-Binary\u4e0a\u8bad\u7ec3\u7684ELBO-KTO\u4e0e\u57fa\u672c\u6a21\u578b\u5728\u76f8\u540c\u7684\u89e3\u7801\u4e0b\u8868\u73b0\u76f8\u5f53\u6216\u66f4\u597d\u3002", "conclusion": "\u975e\u914d\u5bf9\u504f\u597d\u4f18\u5316\u662f\u6269\u6563LLM\u4e2d\u6210\u5bf9\u5bf9\u9f50\u7684\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2510.24073", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24073", "abs": "https://arxiv.org/abs/2510.24073", "authors": ["Xinwei Wu", "Heng Liu", "Jiang Zhou", "Xiaohu Zhao", "Linlong Xu", "Longyue Wang", "Weihua Luo", "Kaifu Zhang"], "title": "Challenging Multilingual LLMs: A New Taxonomy and Benchmark for Unraveling Hallucination in Translation", "comment": null, "summary": "Large Language Models (LLMs) have advanced machine translation but remain\nvulnerable to hallucinations. Unfortunately, existing MT benchmarks are not\ncapable of exposing failures in multilingual LLMs. To disclose hallucination in\nmultilingual LLMs, we introduce a diagnostic framework with a taxonomy that\nseparates Instruction Detachment from Source Detachment. Guided by this\ntaxonomy, we create HalloMTBench, a multilingual, human-verified benchmark\nacross 11 English-to-X directions. We employed 4 frontier LLMs to generate\ncandidates and scrutinize these candidates with an ensemble of LLM judges, and\nexpert validation. In this way, we curate 5,435 high-quality instances. We have\nevaluated 17 LLMs on HalloMTBench. Results reveal distinct ``hallucination\ntriggers'' -- unique failure patterns reflecting model scale, source length\nsensitivity, linguistic biases, and Reinforcement-Learning (RL) amplified\nlanguage mixing. HalloMTBench offers a forward-looking testbed for diagnosing\nLLM translation failures. HalloMTBench is available in\nhttps://huggingface.co/collections/AIDC-AI/marco-mt.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4b\u591a\u8bed\u8a00\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u5e7b\u89c9\u73b0\u8c61\u7684\u8bca\u65ad\u6846\u67b6\u548c\u57fa\u51c6\u6d4b\u8bd5\u96c6HalloMTBench\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u7ffb\u8bd1\uff08MT\uff09\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u6709\u6548\u63ed\u793a\u591a\u8bed\u8a00LLMs\u4e2d\u7684\u5931\u8d25\u6848\u4f8b\uff0c\u7279\u522b\u662f\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u6307\u4ee4\u5206\u79bb\u548c\u6e90\u5206\u79bb\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u636e\u6b64\u521b\u5efa\u4e86\u591a\u8bed\u8a00\u3001\u4eba\u5de5\u9a8c\u8bc1\u7684HalloMTBench\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u6db5\u76d611\u4e2a\u82f1\u8bed\u5230\u5176\u4ed6\u8bed\u8a00\u7684\u7ffb\u8bd1\u65b9\u5411\u3002\u4ed6\u4eec\u4f7f\u75284\u4e2a\u524d\u6cbfLLMs\u751f\u6210\u5019\u9009\u7ed3\u679c\uff0c\u5e76\u901a\u8fc7LLM judges\u548c\u4e13\u5bb6\u9a8c\u8bc1\u6765\u7b5b\u9009\uff0c\u6700\u7ec8\u5f97\u52305,435\u4e2a\u9ad8\u8d28\u91cf\u5b9e\u4f8b\u3002", "result": "\u5bf917\u4e2aLLMs\u5728HalloMTBench\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u72ec\u7279\u7684\u201c\u5e7b\u89c9\u89e6\u53d1\u5668\u201d\uff0c\u53cd\u6620\u4e86\u6a21\u578b\u89c4\u6a21\u3001\u6e90\u6587\u672c\u957f\u5ea6\u654f\u611f\u6027\u3001\u8bed\u8a00\u504f\u89c1\u4ee5\u53ca\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u653e\u5927\u7684\u8bed\u8a00\u6df7\u5408\u7b49\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "HalloMTBench\u4e3a\u8bca\u65adLLM\u7ffb\u8bd1\u5931\u8d25\u63d0\u4f9b\u4e86\u4e00\u4e2a\u524d\u77bb\u6027\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2510.24161", "categories": ["cs.AI", "cs.MM", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24161", "abs": "https://arxiv.org/abs/2510.24161", "authors": ["Wentao Tan", "Bowen Wang", "Heng Zhi", "Chenyu Liu", "Zhe Li", "Jian Liu", "Zengrong Lin", "Yukun Dai", "Yipeng Chen", "Wenjie Yang", "Enci Xie", "Hao Xue", "Baixu Ji", "Chen Xu", "Zhibin Wang", "Tianshi Wang", "Lei Zhu", "Heng Tao Shen"], "title": "BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning", "comment": null, "summary": "Multimodal large language models (MLLMs) have advanced vision-language\nreasoning and are increasingly deployed in embodied agents. However,\nsignificant limitations remain: MLLMs generalize poorly across digital-physical\nspaces and embodiments; vision-language-action models (VLAs) produce low-level\nactions yet lack robust high-level embodied reasoning; and most embodied large\nlanguage models (ELLMs) are constrained to digital-space with poor\ngeneralization to the physical world. Thus, unified models that operate\nseamlessly across digital and physical spaces while generalizing across\nembodiments and tasks remain absent. We introduce the \\textbf{Boundless Large\nModel (BLM$_1$)}, a multimodal spatial foundation model that preserves\ninstruction following and reasoning, incorporates embodied knowledge, and\nsupports robust cross-embodiment control. BLM$_1$ integrates three key\ncapabilities -- \\textit{cross-space transfer, cross-task learning, and\ncross-embodiment generalization} -- via a two-stage training paradigm. Stage I\ninjects embodied knowledge into the MLLM through curated digital corpora while\nmaintaining language competence. Stage II trains a policy module through an\nintent-bridging interface that extracts high-level semantics from the MLLM to\nguide control, without fine-tuning the MLLM backbone. This process is supported\nby a self-collected cross-embodiment demonstration suite spanning four robot\nembodiments and six progressively challenging tasks. Evaluations across digital\nand physical benchmarks show that a single BLM$_1$ instance outperforms four\nmodel families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving\n$\\sim\\!\\textbf{6%}$ gains in digital tasks and $\\sim\\!\\textbf{3%}$ in physical\ntasks.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aBoundless Large Model (BLM$_1$) \u7684\u591a\u6a21\u6001\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u8de8\u8d8a\u6570\u5b57\u548c\u7269\u7406\u7a7a\u95f4\u65e0\u7f1d\u8fd0\u884c\uff0c\u540c\u65f6\u5728\u4e0d\u540c\u5f62\u6001\u548c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6cdb\u5316\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b(MLLM)\u5728\u6570\u5b57-\u7269\u7406\u7a7a\u95f4\u548c\u5f62\u6001\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\uff1b\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b(VLA)\u4ea7\u751f\u4f4e\u7ea7\u52a8\u4f5c\uff0c\u7f3a\u4e4f\u7a33\u5065\u7684\u9ad8\u7ea7\u5177\u8eab\u63a8\u7406\u80fd\u529b\uff1b\u5927\u591a\u6570\u5177\u8eab\u5927\u578b\u8bed\u8a00\u6a21\u578b(ELLM)\u53d7\u9650\u4e8e\u6570\u5b57\u7a7a\u95f4\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u7269\u7406\u4e16\u754c\u3002\u56e0\u6b64\uff0c\u7f3a\u4e4f\u80fd\u591f\u5728\u6570\u5b57\u548c\u7269\u7406\u7a7a\u95f4\u65e0\u7f1d\u8fd0\u884c\uff0c\u5e76\u5728\u4e0d\u540c\u5f62\u6001\u548c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6cdb\u5316\u7684\u7edf\u4e00\u6a21\u578b\u3002", "method": "BLM$_1$ \u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\u96c6\u6210\u4e09\u79cd\u5173\u952e\u80fd\u529b\uff1a\u8de8\u7a7a\u95f4\u8f6c\u79fb\u3001\u8de8\u4efb\u52a1\u5b66\u4e60\u548c\u8de8\u5f62\u6001\u6cdb\u5316\u3002\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u7cbe\u9009\u7684\u6570\u5b57\u8bed\u6599\u5e93\u5c06\u5177\u8eab\u77e5\u8bc6\u6ce8\u5165MLLM\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u8a00\u80fd\u529b\u3002\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u610f\u56fe\u6865\u63a5\u63a5\u53e3\u8bad\u7ec3\u7b56\u7565\u6a21\u5757\uff0c\u4eceMLLM\u4e2d\u63d0\u53d6\u9ad8\u7ea7\u8bed\u4e49\u4ee5\u6307\u5bfc\u63a7\u5236\uff0c\u800c\u65e0\u9700\u5fae\u8c03MLLM\u4e3b\u5e72\u3002", "result": "\u5728\u6570\u5b57\u548c\u7269\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5355\u4e2aBLM$_1$ \u5b9e\u4f8b\u4f18\u4e8e\u56db\u4e2a\u6a21\u578b\u7cfb\u5217\u2014\u2014MLLM\u3001ELLM\u3001VLA \u548c GMLM\u2014\u2014\u5728\u6570\u5b57\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u7ea6 6% \u7684\u6536\u76ca\uff0c\u5728\u7269\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u7ea6 3% \u7684\u6536\u76ca\u3002", "conclusion": "BLM$_1$ \u662f\u4e00\u4e2a\u591a\u6a21\u6001\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff0c\u5b83\u4fdd\u7559\u4e86\u6307\u4ee4\u9075\u5faa\u548c\u63a8\u7406\u80fd\u529b\uff0c\u7ed3\u5408\u4e86\u5177\u8eab\u77e5\u8bc6\uff0c\u5e76\u652f\u6301\u5f3a\u5927\u7684\u8de8\u5f62\u6001\u63a7\u5236\u3002"}}
{"id": "2510.24093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24093", "abs": "https://arxiv.org/abs/2510.24093", "authors": ["Agus Gunawan", "Samuel Teodoro", "Yun Chen", "Soo Ye Kim", "Jihyong Oh", "Munchurl Kim"], "title": "OmniText: A Training-Free Generalist for Controllable Text-Image Manipulation", "comment": "The first two authors contributed equally to this work. The last two\n  authors are co-corresponding authors", "summary": "Recent advancements in diffusion-based text synthesis have demonstrated\nsignificant performance in inserting and editing text within images via\ninpainting. However, despite the potential of text inpainting methods, three\nkey limitations hinder their applicability to broader Text Image Manipulation\n(TIM) tasks: (i) the inability to remove text, (ii) the lack of control over\nthe style of rendered text, and (iii) a tendency to generate duplicated\nletters. To address these challenges, we propose OmniText, a training-free\ngeneralist capable of performing a wide range of TIM tasks. Specifically, we\ninvestigate two key properties of cross- and self-attention mechanisms to\nenable text removal and to provide control over both text styles and content.\nOur findings reveal that text removal can be achieved by applying\nself-attention inversion, which mitigates the model's tendency to focus on\nsurrounding text, thus reducing text hallucinations. Additionally, we\nredistribute cross-attention, as increasing the probability of certain text\ntokens reduces text hallucination. For controllable inpainting, we introduce\nnovel loss functions in a latent optimization framework: a cross-attention\ncontent loss to improve text rendering accuracy and a self-attention style loss\nto facilitate style customization. Furthermore, we present OmniText-Bench, a\nbenchmark dataset for evaluating diverse TIM tasks. It includes input images,\ntarget text with masks, and style references, covering diverse applications\nsuch as text removal, rescaling, repositioning, and insertion and editing with\nvarious styles. Our OmniText framework is the first generalist method capable\nof performing diverse TIM tasks. It achieves state-of-the-art performance\nacross multiple tasks and metrics compared to other text inpainting methods and\nis comparable with specialist methods.", "AI": {"tldr": "OmniText \u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u901a\u7528\u6846\u67b6\uff0c\u80fd\u591f\u6267\u884c\u5404\u79cd\u6587\u672c\u56fe\u50cf\u5904\u7406 (TIM) \u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6587\u672c\u4fee\u590d\u65b9\u6cd5\u5728\u6587\u672c\u53bb\u9664\u3001\u98ce\u683c\u63a7\u5236\u548c\u91cd\u590d\u5b57\u6bcd\u751f\u6210\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684\u6587\u672c\u5408\u6210\u5728\u56fe\u50cf\u4e2d\u63d2\u5165\u548c\u7f16\u8f91\u6587\u672c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u65e0\u6cd5\u53bb\u9664\u6587\u672c\u3001\u7f3a\u4e4f\u5bf9\u6e32\u67d3\u6587\u672c\u98ce\u683c\u7684\u63a7\u5236\u4ee5\u53ca\u5bb9\u6613\u751f\u6210\u91cd\u590d\u5b57\u6bcd\uff0c\u9650\u5236\u4e86\u5176\u5728\u66f4\u5e7f\u6cdb\u7684\u6587\u672c\u56fe\u50cf\u5904\u7406 (TIM) \u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "OmniText \u91c7\u7528\u81ea\u6ce8\u610f\u529b\u53cd\u8f6c\u5b9e\u73b0\u6587\u672c\u53bb\u9664\uff0c\u901a\u8fc7\u91cd\u65b0\u5206\u914d\u4ea4\u53c9\u6ce8\u610f\u529b\u6765\u51cf\u5c11\u6587\u672c\u5e7b\u89c9\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u6f5c\u5728\u4f18\u5316\u6846\u67b6\u4e2d\u7684\u635f\u5931\u51fd\u6570\uff0c\u5305\u62ec\u4ea4\u53c9\u6ce8\u610f\u529b\u5185\u5bb9\u635f\u5931\u548c\u81ea\u6ce8\u610f\u529b\u98ce\u683c\u635f\u5931\uff0c\u4ee5\u63d0\u9ad8\u6587\u672c\u6e32\u67d3\u51c6\u786e\u6027\u548c\u4fc3\u8fdb\u98ce\u683c\u5b9a\u5236\u3002", "result": "OmniText \u5728\u591a\u9879\u4efb\u52a1\u548c\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u4e0e\u4e13\u4e1a\u65b9\u6cd5\u5177\u6709\u53ef\u6bd4\u6027\u3002", "conclusion": "OmniText \u662f\u7b2c\u4e00\u4e2a\u80fd\u591f\u6267\u884c\u5404\u79cd TIM \u4efb\u52a1\u7684\u901a\u7528\u65b9\u6cd5\u3002"}}
{"id": "2510.23659", "categories": ["cs.LG", "cs.CV", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.23659", "abs": "https://arxiv.org/abs/2510.23659", "authors": ["Md. Farhan Shahriyar", "Gazi Tanbhir", "Abdullah Md Raihan Chy"], "title": "Quantum Machine Learning for Image Classification: A Hybrid Model of Residual Network with Quantum Support Vector Machine", "comment": null, "summary": "Recently, there has been growing attention on combining quantum machine\nlearning (QML) with classical deep learning approaches, as computational\ntechniques are key to improving the performance of image classification tasks.\nThis study presents a hybrid approach that uses ResNet-50 (Residual Network)\nfor feature extraction and Quantum Support Vector Machines (QSVM) for\nclassification in the context of potato disease detection. Classical machine\nlearning as well as deep learning models often struggle with high-dimensional\nand complex datasets, necessitating advanced techniques like quantum computing\nto improve classification efficiency. In our research, we use ResNet-50 to\nextract deep feature representations from RGB images of potato diseases. These\nfeatures are then subjected to dimensionality reduction using Principal\nComponent Analysis (PCA). The resulting features are processed through QSVM\nmodels which apply various quantum feature maps such as ZZ, Z, and Pauli-X to\ntransform classical data into quantum states. To assess the model performance,\nwe compared it with classical machine learning algorithms such as Support\nVector Machine (SVM) and Random Forest (RF) using five-fold stratified\ncross-validation for comprehensive evaluation. The experimental results\ndemonstrate that the Z-feature map-based QSVM outperforms classical models,\nachieving an accuracy of 99.23 percent, surpassing both SVM and RF models. This\nresearch highlights the advantages of integrating quantum computing into image\nclassification and provides a potential disease detection solution through\nhybrid quantum-classical modeling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u65b9\u6cd5\uff0c\u5229\u7528 ResNet-50 \u63d0\u53d6\u7279\u5f81\uff0c\u7136\u540e\u4f7f\u7528 QSVM \u8fdb\u884c\u9a6c\u94c3\u85af\u75c5\u5bb3\u68c0\u6d4b\u5206\u7c7b\u3002", "motivation": "\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9ad8\u7ef4\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u91cf\u5b50\u8ba1\u7b97\u6765\u63d0\u9ad8\u5206\u7c7b\u6548\u7387\u3002", "method": "\u4f7f\u7528 ResNet-50 \u63d0\u53d6\u56fe\u50cf\u7279\u5f81\uff0cPCA \u964d\u7ef4\uff0c\u7136\u540e\u4f7f\u7528 QSVM \u6a21\u578b\uff0c\u91c7\u7528 ZZ, Z \u548c Pauli-X \u7b49\u91cf\u5b50\u7279\u5f81\u6620\u5c04\u3002", "result": "\u57fa\u4e8e Z \u7279\u5f81\u6620\u5c04\u7684 QSVM \u4f18\u4e8e\u7ecf\u5178\u6a21\u578b\uff0c\u51c6\u786e\u7387\u8fbe\u5230 99.23%\u3002", "conclusion": "\u8be5\u7814\u7a76\u7a81\u51fa\u4e86\u5c06\u91cf\u5b50\u8ba1\u7b97\u96c6\u6210\u5230\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u4f18\u52bf\uff0c\u5e76\u4e3a\u901a\u8fc7\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u79cd\u6f5c\u5728\u7684\u75be\u75c5\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24081", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24081", "abs": "https://arxiv.org/abs/2510.24081", "authors": ["Tyler A. Chang", "Catherine Arnett", "Abdelrahman Eldesokey", "Abdelrahman Sadallah", "Abeer Kashar", "Abolade Daud", "Abosede Grace Olanihun", "Adamu Labaran Mohammed", "Adeyemi Praise", "Adhikarinayum Meerajita Sharma", "Aditi Gupta", "Afitab Iyigun", "Afonso Simpl\u00edcio", "Ahmed Essouaied", "Aicha Chorana", "Akhil Eppa", "Akintunde Oladipo", "Akshay Ramesh", "Aleksei Dorkin", "Alfred Malengo Kondoro", "Alham Fikri Aji", "Ali Eren \u00c7etinta\u015f", "Allan Hanbury", "Alou Dembele", "Alp Niksarli", "\u00c1lvaro Arroyo", "Amin Bajand", "Amol Khanna", "Ana Chkhaidze", "Ana Condez", "Andiswa Mkhonto", "Andrew Hoblitzell", "Andrew Tran", "Angelos Poulis", "Anirban Majumder", "Anna Vacalopoulou", "Annette Kuuipolani Kanahele Wong", "Annika Simonsen", "Anton Kovalev", "Ashvanth. S", "Ayodeji Joseph Lana", "Barkin Kinay", "Bashar Alhafni", "Benedict Cibalinda Busole", "Bernard Ghanem", "Bharti Nathani", "Biljana Stojanovska \u0110uri\u0107", "Bola Agbonile", "Bragi Bergsson", "Bruce Torres Fischer", "Burak Tutar", "Burcu Alaku\u015f \u00c7\u0131nar", "Cade J. Kanoniakapueo Kane", "Can Udomcharoenchaikit", "Catherine Arnett", "Chadi Helwe", "Chaithra Reddy Nerella", "Chen Cecilia Liu", "Chiamaka Glory Nwokolo", "Cristina Espa\u00f1a-Bonet", "Cynthia Amol", "DaeYeop Lee", "Dana Arad", "Daniil Dzenhaliou", "Daria Pugacheva", "Dasol Choi", "Daud Abolade", "David Liu", "David Semedo", "Deborah Popoola", "Deividas Mataciunas", "Delphine Nyaboke", "Dhyuthy Krishna Kumar", "Diogo Gl\u00f3ria-Silva", "Diogo Tavares", "Divyanshu Goyal", "DongGeon Lee", "Ebele Nwamaka Anajemba", "Egonu Ngozi Grace", "Elena Mickel", "Elena Tutubalina", "Elias Herranen", "Emile Anand", "Emmanuel Habumuremyi", "Emuobonuvie Maria Ajiboye", "Eryawan Presma Yulianrifat", "Esther Adenuga", "Ewa Rudnicka", "Faith Olabisi Itiola", "Faran Taimoor Butt", "Fathima Thekkekara", "Fatima Haouari", "Filbert Aurelian Tjiaranata", "Firas Laakom", "Francesca Grasso", "Francesco Orabona", "Francesco Periti", "Gbenga Kayode Solomon", "Gia Nghia Ngo", "Gloria Udhehdhe-oze", "Gon\u00e7alo Martins", "Gopi Naga Sai Ram Challagolla", "Guijin Son", "Gulnaz Abdykadyrova", "Hafsteinn Einarsson", "Hai Hu", "Hamidreza Saffari", "Hamza Zaidi", "Haopeng Zhang", "Harethah Abu Shairah", "Harry Vuong", "Hele-Andra Kuulmets", "Houda Bouamor", "Hwanjo Yu", "Iben Nyholm Debess", "\u0130brahim Ethem Deveci", "Ikhlasul Akmal Hanif", "Ikhyun Cho", "In\u00eas Calvo", "In\u00eas Vieira", "Isaac Manzi", "Ismail Daud", "Itay Itzhak", "Iuliia", "Alekseenko", "Ivan Belashkin", "Ivan Spada", "Ivan Zhelyazkov", "Jacob Brinton", "Jafar Isbarov", "Jaka \u010cibej", "Jan \u010cuhel", "Jan Koco\u0144", "Jauza Akbar Krito", "Jebish Purbey", "Jennifer Mickel", "Jennifer Za", "Jenny Kunz", "Jihae Jeong", "Jimena Tena D\u00e1valos", "Jinu Lee", "Jo\u00e3o Magalh\u00e3es", "John Yi", "Jongin Kim", "Joseph Chataignon", "Joseph Marvin Imperial", "Jubeerathan Thevakumar", "Judith Land", "Junchen Jiang", "Jungwhan Kim", "Kairit Sirts", "Kamesh R", "Kamesh V", "Kanda Patrick Tshinu", "K\u00e4triin Kukk", "Kaustubh Ponkshe", "Kavsar Huseynova", "Ke He", "Kelly Buchanan", "Kengatharaiyer Sarveswaran", "Kerem Zaman", "Khalil Mrini", "Kian Kyars", "Krister Kruusmaa", "Kusum Chouhan", "Lainitha Krishnakumar", "Laura Castro S\u00e1nchez", "Laura Porrino Moscoso", "Leshem Choshen", "Levent Sencan", "Lilja \u00d8vrelid", "Lisa Alazraki", "Lovina Ehimen-Ugbede", "Luheerathan Thevakumar", "Luxshan Thavarasa", "Mahnoor Malik", "Mamadou K. Keita", "Mansi Jangid", "Marco De Santis", "Marcos Garc\u00eda", "Marek Suppa", "Mariam D'Ciofalo", "Marii Ojastu", "Maryam Sikander", "Mausami Narayan", "Maximos Skandalis", "Mehak Mehak", "Mehmet \u0130lteri\u015f Bozkurt", "Melaku Bayu Workie", "Menan Velayuthan", "Michael Leventhal", "Micha\u0142 Marci\u0144czuk", "Mirna Poto\u010dnjak", "Mohammadamin Shafiei", "Mridul Sharma", "Mrityunjaya Indoria", "Muhammad Ravi Shulthan Habibi", "Murat Koli\u0107", "Nada Galant", "Naphat Permpredanun", "Narada Maugin", "Nicholas Kluge Corr\u00eaa", "Nikola Ljube\u0161i\u0107", "Nirmal Thomas", "Nisansa de Silva", "Nisheeth Joshi", "Nitish Ponkshe", "Nizar Habash", "Nneoma C. Udeze", "Noel Thomas", "No\u00e9mi Ligeti-Nagy", "Nouhoum Coulibaly", "Nsengiyumva Faustin", "Odunayo Kareemat Buliaminu", "Odunayo Ogundepo", "Oghojafor Godswill Fejiro", "Ogundipe Blessing Funmilola", "Okechukwu God'spraise", "Olanrewaju Samuel", "Olaoye Deborah Oluwaseun", "Olasoji Akindejoye", "Olga Popova", "Olga Snissarenko", "Onyinye Anulika Chiemezie", "Orkun Kinay", "Osman Tursun", "Owoeye Tobiloba Moses", "Oyelade Oluwafemi Joshua", "Oyesanmi Fiyinfoluwa", "Pablo Gamallo", "Pablo Rodr\u00edguez Fern\u00e1ndez", "Palak Arora", "Pedro Valente", "Peter Rupnik", "Philip Oghenesuowho Ekiugbo", "Pramit Sahoo", "Prokopis Prokopidis", "Pua Niau-Puhipau", "Quadri Yahya", "Rachele Mignone", "Raghav Singhal", "Ram Mohan Rao Kadiyala", "Raphael Merx", "Rapheal Afolayan", "Ratnavel Rajalakshmi", "Rishav Ghosh", "Romina Oji", "Ron Kekeha Solis", "Rui Guerra", "Rushikesh Zawar", "Sa'ad Nasir Bashir", "Saeed Alzaabi", "Sahil Sandeep", "Sai Pavan Batchu", "SaiSandeep Kantareddy", "Salsabila Zahirah Pranida", "Sam Buchanan", "Samuel Rutunda", "Sander Land", "Sarah Sulollari", "Sardar Ali", "Saroj Sapkota", "Saulius Tautvaisas", "Sayambhu Sen", "Sayantani Banerjee", "Sebastien Diarra", "SenthilNathan. M", "Sewoong Lee", "Shaan Shah", "Shankar Venkitachalam", "Sharifa Djurabaeva", "Sharon Ibejih", "Shivanya Shomir Dutta", "Siddhant Gupta", "Silvia Paniagua Su\u00e1rez", "Sina Ahmadi", "Sivasuthan Sukumar", "Siyuan Song", "Snegha A.", "Sokratis Sofianopoulos", "Sona Elza Simon", "Sonja Ben\u010dina", "Sophie Gvasalia", "Sphurti Kirit More", "Spyros Dragazis", "Stephan P. Kaufhold", "Suba. S", "Sultan AlRashed", "Surangika Ranathunga", "Taiga Someya", "Taja Kuzman Punger\u0161ek", "Tal Haklay", "Tasi'u Jibril", "Tatsuya Aoyama", "Tea Abashidze", "Terenz Jomar Dela Cruz", "Terra Blevins", "Themistoklis Nikas", "Theresa Dora Idoko", "Thu Mai Do", "Tilek Chubakov", "Tommaso Gargiani", "Uma Rathore", "Uni Johannesen", "Uwuma Doris Ugwu", "Vallerie Alexandra Putra", "Vanya Bannihatti Kumar", "Varsha Jeyarajalingam", "Varvara Arzt", "Vasudevan Nedumpozhimana", "Viktoria Ondrejova", "Viktoryia Horbik", "Vishnu Vardhan Reddy Kummitha", "Vuk Dini\u0107", "Walelign Tewabe Sewunetie", "Winston Wu", "Xiaojing Zhao", "Yacouba Diarra", "Yaniv Nikankin", "Yash Mathur", "Yixi Chen", "Yiyuan Li", "Yolanda Xavier", "Yonatan Belinkov", "Yusuf Ismail Abayomi", "Zaid Alyafeai", "Zhengyang Shan", "Zhi Rui Tam", "Zilu Tang", "Zuzana Nadova", "Baber Abbasi", "Stella Biderman", "David Stap", "Duygu Ataman", "Fabian Schmidt", "Hila Gonen", "Jiayi Wang", "David Ifeoluwa Adelani"], "title": "Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+ Languages and Cultures", "comment": "Preprint", "summary": "To date, there exist almost no culturally-specific evaluation benchmarks for\nlarge language models (LLMs) that cover a large number of languages and\ncultures. In this paper, we present Global PIQA, a participatory commonsense\nreasoning benchmark for over 100 languages, constructed by hand by 335\nresearchers from 65 countries around the world. The 116 language varieties in\nGlobal PIQA cover five continents, 14 language families, and 23 writing\nsystems. In the non-parallel split of Global PIQA, over 50% of examples\nreference local foods, customs, traditions, or other culturally-specific\nelements. We find that state-of-the-art LLMs perform well on Global PIQA in\naggregate, but they exhibit weaker performance in lower-resource languages (up\nto a 37% accuracy gap, despite random chance at 50%). Open models generally\nperform worse than proprietary models. Global PIQA highlights that in many\nlanguages and cultures, everyday knowledge remains an area for improvement,\nalongside more widely-discussed capabilities such as complex reasoning and\nexpert knowledge. Beyond its uses for LLM evaluation, we hope that Global PIQA\nprovides a glimpse into the wide diversity of cultures in which human language\nis embedded.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u8bed\u8a00\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6Global PIQA\uff0c\u8986\u76d6\u8d85\u8fc7100\u79cd\u8bed\u8a00\uff0c\u63ed\u793a\u4e86LLM\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u8986\u76d6\u591a\u79cd\u8bed\u8a00\u548c\u6587\u5316\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6587\u5316\u7279\u5b9a\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u7531\u6765\u81ea65\u4e2a\u56fd\u5bb6\u7684335\u540d\u7814\u7a76\u4eba\u5458\u624b\u5de5\u6784\u5efa\uff0c\u5305\u542b116\u79cd\u8bed\u8a00\u53d8\u4f53\uff0c\u6db5\u76d6\u4e94\u5927\u6d32\u300114\u4e2a\u8bed\u7cfb\u548c23\u4e2a\u4e66\u5199\u7cfb\u7edf\u3002", "result": "\u5148\u8fdb\u7684LLM\u5728Global PIQA\u603b\u4f53\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u8868\u73b0\u8f83\u5dee\uff08\u51c6\u786e\u7387\u5dee\u8ddd\u9ad8\u8fbe37%\uff09\u3002", "conclusion": "Global PIQA\u8868\u660e\uff0c\u5728\u8bb8\u591a\u8bed\u8a00\u548c\u6587\u5316\u4e2d\uff0c\u65e5\u5e38\u77e5\u8bc6\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u5e76\u5e0c\u671bGlobal PIQA\u80fd\u5e2e\u52a9\u4e86\u89e3\u4eba\u7c7b\u8bed\u8a00\u6240\u5d4c\u5165\u7684\u6587\u5316\u591a\u6837\u6027\u3002"}}
{"id": "2510.24166", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24166", "abs": "https://arxiv.org/abs/2510.24166", "authors": ["Xin Yang", "Yuhang Zhang", "Wei Li", "Xin Lin", "Wenbin Zou", "Chen Xu"], "title": "UniPlanner: A Unified Motion Planning Framework for Autonomous Vehicle Decision-Making Systems via Multi-Dataset Integration", "comment": null, "summary": "Motion planning is a critical component of autonomous vehicle decision-making\nsystems, directly determining trajectory safety and driving efficiency. While\ndeep learning approaches have advanced planning capabilities, existing methods\nremain confined to single-dataset training, limiting their robustness in\nplanning.\n  Through systematic analysis, we discover that vehicular trajectory\ndistributions and history-future correlations demonstrate remarkable\nconsistency across different datasets. Based on these findings, we propose\nUniPlanner, the first planning framework designed for multi-dataset integration\nin autonomous vehicle decision-making. UniPlanner achieves unified\ncross-dataset learning through three synergistic innovations.\n  First, the History-Future Trajectory Dictionary Network (HFTDN) aggregates\nhistory-future trajectory pairs from multiple datasets, using historical\ntrajectory similarity to retrieve relevant futures and generate cross-dataset\nplanning guidance.\n  Second, the Gradient-Free Trajectory Mapper (GFTM) learns robust\nhistory-future correlations from multiple datasets, transforming historical\ntrajectories into universal planning priors. Its gradient-free design ensures\nthe introduction of valuable priors while preventing shortcut learning, making\nthe planning knowledge safely transferable. Third, the Sparse-to-Dense (S2D)\nparadigm implements adaptive dropout to selectively suppress planning priors\nduring training for robust learning, while enabling full prior utilization\nduring inference to maximize planning performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aUniPlanner\u7684\u591a\u6570\u636e\u96c6\u96c6\u6210\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u51b3\u7b56\u89c4\u5212\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u89c4\u5212\u4e2d\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u89c4\u5212\u80fd\u529b\u4e0a\u6709\u6240\u63d0\u5347\uff0c\u4f46\u53d7\u9650\u4e8e\u5355\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "1) \u5386\u53f2-\u672a\u6765\u8f68\u8ff9\u5b57\u5178\u7f51\u7edc (HFTDN): \u805a\u5408\u591a\u6570\u636e\u96c6\u7684\u5386\u53f2-\u672a\u6765\u8f68\u8ff9\u5bf9\uff0c\u5229\u7528\u5386\u53f2\u8f68\u8ff9\u76f8\u4f3c\u6027\u68c0\u7d22\u76f8\u5173\u672a\u6765\u8f68\u8ff9\uff0c\u751f\u6210\u8de8\u6570\u636e\u96c6\u89c4\u5212\u6307\u5bfc\u3002\n2) \u65e0\u68af\u5ea6\u8f68\u8ff9\u6620\u5c04\u5668 (GFTM): \u4ece\u591a\u6570\u636e\u96c6\u5b66\u4e60\u9c81\u68d2\u7684\u5386\u53f2-\u672a\u6765\u76f8\u5173\u6027\uff0c\u5c06\u5386\u53f2\u8f68\u8ff9\u8f6c\u6362\u4e3a\u901a\u7528\u89c4\u5212\u5148\u9a8c\u3002\u65e0\u68af\u5ea6\u8bbe\u8ba1\u786e\u4fdd\u5f15\u5165\u6709\u4ef7\u503c\u7684\u5148\u9a8c\uff0c\u540c\u65f6\u9632\u6b62\u6377\u5f84\u5b66\u4e60\uff0c\u4f7f\u89c4\u5212\u77e5\u8bc6\u5b89\u5168\u8f6c\u79fb\u3002\n3) \u7a00\u758f\u5230\u5bc6\u96c6 (S2D) \u8303\u5f0f: \u5b9e\u65bd\u81ea\u9002\u5e94 dropout\uff0c\u5728\u8bad\u7ec3\u671f\u95f4\u9009\u62e9\u6027\u5730\u6291\u5236\u89c4\u5212\u5148\u9a8c\u4ee5\u8fdb\u884c\u9c81\u68d2\u5b66\u4e60\uff0c\u540c\u65f6\u5728\u63a8\u7406\u671f\u95f4\u5b9e\u73b0\u5b8c\u5168\u5148\u9a8c\u5229\u7528\uff0c\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u89c4\u5212\u6027\u80fd\u3002", "result": "UniPlanner\u5b9e\u73b0\u4e86\u7edf\u4e00\u7684\u8de8\u6570\u636e\u96c6\u5b66\u4e60\u3002", "conclusion": "UniPlanner\u662f\u9996\u4e2a\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u51b3\u7b56\u4e2d\u591a\u6570\u636e\u96c6\u96c6\u6210\u800c\u8bbe\u8ba1\u7684\u89c4\u5212\u6846\u67b6\u3002"}}
{"id": "2510.24105", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24105", "abs": "https://arxiv.org/abs/2510.24105", "authors": ["Shufan Shen", "Zhaobo Qi", "Junshu Sun", "Qingming Huang", "Qi Tian", "Shuhui Wang"], "title": "Enhancing Pre-trained Representation Classifiability can Boost its Interpretability", "comment": "ICLR 2025 (Spotlight)", "summary": "The visual representation of a pre-trained model prioritizes the\nclassifiability on downstream tasks, while the widespread applications for\npre-trained visual models have posed new requirements for representation\ninterpretability. However, it remains unclear whether the pre-trained\nrepresentations can achieve high interpretability and classifiability\nsimultaneously. To answer this question, we quantify the representation\ninterpretability by leveraging its correlation with the ratio of interpretable\nsemantics within the representations. Given the pre-trained representations,\nonly the interpretable semantics can be captured by interpretations, whereas\nthe uninterpretable part leads to information loss. Based on this fact, we\npropose the Inherent Interpretability Score (IIS) that evaluates the\ninformation loss, measures the ratio of interpretable semantics, and quantifies\nthe representation interpretability. In the evaluation of the representation\ninterpretability with different classifiability, we surprisingly discover that\nthe interpretability and classifiability are positively correlated, i.e.,\nrepresentations with higher classifiability provide more interpretable\nsemantics that can be captured in the interpretations. This observation further\nsupports two benefits to the pre-trained representations. First, the\nclassifiability of representations can be further improved by fine-tuning with\ninterpretability maximization. Second, with the classifiability improvement for\nthe representations, we obtain predictions based on their interpretations with\nless accuracy degradation. The discovered positive correlation and\ncorresponding applications show that practitioners can unify the improvements\nin interpretability and classifiability for pre-trained vision models. Codes\nare available at https://github.com/ssfgunner/IIS.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u8868\u5f81\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u53ef\u5206\u7c7b\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u5b83\u4eec\u662f\u6b63\u76f8\u5173\u7684\u3002", "motivation": "\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u5e94\u7528\u5e7f\u6cdb\uff0c\u5bf9\u8868\u5f81\u7684\u53ef\u89e3\u91ca\u6027\u63d0\u51fa\u4e86\u65b0\u8981\u6c42\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u9884\u8bad\u7ec3\u8868\u5f81\u662f\u5426\u80fd\u540c\u65f6\u5b9e\u73b0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u5206\u7c7b\u6027\u3002", "method": "\u672c\u6587\u901a\u8fc7\u8868\u5f81\u4e0e\u53ef\u89e3\u91ca\u8bed\u4e49\u6bd4\u4f8b\u7684\u76f8\u5173\u6027\u6765\u91cf\u5316\u8868\u5f81\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Inherent Interpretability Score (IIS) \u7684\u6307\u6807\u6765\u8bc4\u4f30\u4fe1\u606f\u635f\u5931\uff0c\u8861\u91cf\u53ef\u89e3\u91ca\u8bed\u4e49\u7684\u6bd4\u4f8b\uff0c\u4ece\u800c\u91cf\u5316\u8868\u5f81\u7684\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u53ef\u89e3\u91ca\u6027\u4e0e\u53ef\u5206\u7c7b\u6027\u5448\u6b63\u76f8\u5173\uff0c\u5373\u53ef\u5206\u7c7b\u6027\u66f4\u9ad8\u7684\u8868\u5f81\u63d0\u4f9b\u66f4\u591a\u53ef\u5728\u89e3\u91ca\u4e2d\u6355\u83b7\u7684\u53ef\u89e3\u91ca\u8bed\u4e49\u3002", "conclusion": "\u8be5\u53d1\u73b0\u8868\u660e\uff0c\u901a\u8fc7\u6700\u5927\u5316\u53ef\u89e3\u91ca\u6027\u8fdb\u884c\u5fae\u8c03\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u8868\u5f81\u7684\u53ef\u5206\u7c7b\u6027\uff0c\u5e76\u4e14\u5728\u63d0\u9ad8\u8868\u5f81\u7684\u53ef\u5206\u7c7b\u6027\u7684\u540c\u65f6\uff0c\u53ef\u4ee5\u57fa\u4e8e\u5176\u89e3\u91ca\u83b7\u5f97\u9884\u6d4b\uff0c\u4ece\u800c\u51cf\u5c11\u51c6\u786e\u6027\u4e0b\u964d\u3002"}}
{"id": "2510.23660", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23660", "abs": "https://arxiv.org/abs/2510.23660", "authors": ["Gazi Tanbhir", "Md. Farhan Shahriyar", "Abdullah Md Raihan Chy"], "title": "Quanvolutional Neural Networks for Pneumonia Detection: An Efficient Quantum-Assisted Feature Extraction Paradigm", "comment": null, "summary": "Pneumonia poses a significant global health challenge, demanding accurate and\ntimely diagnosis. While deep learning, particularly Convolutional Neural\nNetworks (CNNs), has shown promise in medical image analysis for pneumonia\ndetection, CNNs often suffer from high computational costs, limitations in\nfeature representation, and challenges in generalizing from smaller datasets.\nTo address these limitations, we explore the application of Quanvolutional\nNeural Networks (QNNs), leveraging quantum computing for enhanced feature\nextraction. This paper introduces a novel hybrid quantum-classical model for\npneumonia detection using the PneumoniaMNIST dataset. Our approach utilizes a\nquanvolutional layer with a parameterized quantum circuit (PQC) to process 2x2\nimage patches, employing rotational Y-gates for data encoding and entangling\nlayers to generate non-classical feature representations. These\nquantum-extracted features are then fed into a classical neural network for\nclassification. Experimental results demonstrate that the proposed QNN achieves\na higher validation accuracy of 83.33 percent compared to a comparable\nclassical CNN which achieves 73.33 percent. This enhanced convergence and\nsample efficiency highlight the potential of QNNs for medical image analysis,\nparticularly in scenarios with limited labeled data. This research lays the\nfoundation for integrating quantum computing into deep-learning-driven medical\ndiagnostic systems, offering a computationally efficient alternative to\ntraditional approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u80ba\u708e\u68c0\u6d4b\u7684\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5229\u7528\u91cf\u5b50\u5377\u79ef\u5c42\u63d0\u53d6\u7279\u5f81\uff0c\u7136\u540e\u4f7f\u7528\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5206\u7c7b\u3002", "motivation": "\u4f20\u7edf\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u80ba\u708e\u68c0\u6d4b\u4e2d\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u7279\u5f81\u8868\u793a\u80fd\u529b\u6709\u9650\u4ee5\u53ca\u6cdb\u5316\u80fd\u529b\u5dee\u7b49\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u3002", "method": "\u8be5\u65b9\u6cd5\u4f7f\u7528\u5e26\u6709\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u7684\u91cf\u5b50\u5377\u79ef\u5c42\u5904\u7406\u56fe\u50cf\u5757\uff0c\u5e76\u4f7f\u7528\u65cb\u8f6c Y \u95e8\u8fdb\u884c\u6570\u636e\u7f16\u7801\u548c\u7ea0\u7f20\u5c42\u4ee5\u751f\u6210\u975e\u7ecf\u5178\u7279\u5f81\u8868\u793a\u3002\u7136\u540e\uff0c\u5c06\u8fd9\u4e9b\u91cf\u5b50\u63d0\u53d6\u7684\u7279\u5f81\u8f93\u5165\u5230\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684 QNN \u5b9e\u73b0\u4e86 83.33% \u7684\u9a8c\u8bc1\u51c6\u786e\u7387\uff0c\u800c\u540c\u7b49\u7684\u7ecf\u5178 CNN \u7684\u9a8c\u8bc1\u51c6\u786e\u7387\u4e3a 73.33%\u3002", "conclusion": "\u8be5\u7814\u7a76\u5960\u5b9a\u4e86\u5c06\u91cf\u5b50\u8ba1\u7b97\u96c6\u6210\u5230\u6df1\u5ea6\u5b66\u4e60\u9a71\u52a8\u7684\u533b\u7597\u8bca\u65ad\u7cfb\u7edf\u7684\u57fa\u7840\uff0c\u4e3a\u4f20\u7edf\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2510.24096", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24096", "abs": "https://arxiv.org/abs/2510.24096", "authors": ["Md. Rezuwan Hassan", "Azmol Hossain", "Kanij Fatema", "Rubayet Sabbir Faruque", "Tanmoy Shome", "Ruwad Naswan", "Trina Chakraborty", "Md. Foriduzzaman Zihad", "Tawsif Tashwar Dipto", "Nazia Tasnim", "Nazmuddoha Ansary", "Md. Mehedi Hasan Shawon", "Ahmed Imtiaz Humayun", "Md. Golam Rabiul Alam", "Farig Sadeque", "Asif Sushmit"], "title": "RegSpeech12: A Regional Corpus of Bengali Spontaneous Speech Across Dialects", "comment": "26 pages", "summary": "The Bengali language, spoken extensively across South Asia and among\ndiasporic communities, exhibits considerable dialectal diversity shaped by\ngeography, culture, and history. Phonological and pronunciation-based\nclassifications broadly identify five principal dialect groups: Eastern\nBengali, Manbhumi, Rangpuri, Varendri, and Rarhi. Within Bangladesh, further\ndistinctions emerge through variation in vocabulary, syntax, and morphology, as\nobserved in regions such as Chittagong, Sylhet, Rangpur, Rajshahi, Noakhali,\nand Barishal. Despite this linguistic richness, systematic research on the\ncomputational processing of Bengali dialects remains limited. This study seeks\nto document and analyze the phonetic and morphological properties of these\ndialects while exploring the feasibility of building computational models\nparticularly Automatic Speech Recognition (ASR) systems tailored to regional\nvarieties. Such efforts hold potential for applications in virtual assistants\nand broader language technologies, contributing to both the preservation of\ndialectal diversity and the advancement of inclusive digital tools for\nBengali-speaking communities. The dataset created for this study is released\nfor public use.", "AI": {"tldr": "\u7cfb\u7edf\u5730\u7814\u7a76\u5b5f\u52a0\u62c9\u8bed\u65b9\u8a00\u7684\u8ba1\u7b97\u5904\u7406\u4ecd\u7136\u6709\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bb0\u5f55\u548c\u5206\u6790\u8fd9\u4e9b\u65b9\u8a00\u7684\u8bed\u97f3\u548c\u5f62\u6001\u7279\u5f81\uff0c\u540c\u65f6\u63a2\u7d22\u6784\u5efa\u8ba1\u7b97\u6a21\u578b\u7684\u53ef\u80fd\u6027\uff0c\u7279\u522b\u662f\u4e3a\u533a\u57df\u53d8\u4f53\u91cf\u8eab\u5b9a\u5236\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b (ASR) \u7cfb\u7edf\u3002", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u5728\u5357\u4e9a\u548c\u6563\u5c45\u793e\u533a\u4e2d\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u8868\u73b0\u51fa\u76f8\u5f53\u5927\u7684\u65b9\u8a00\u591a\u6837\u6027\uff0c\u8fd9\u79cd\u591a\u6837\u6027\u53d7\u5730\u7406\u3001\u6587\u5316\u548c\u5386\u53f2\u7684\u5f71\u54cd\u3002", "method": "\u8bb0\u5f55\u548c\u5206\u6790\u8fd9\u4e9b\u65b9\u8a00\u7684\u8bed\u97f3\u548c\u5f62\u6001\u7279\u5f81\uff0c\u540c\u65f6\u63a2\u7d22\u6784\u5efa\u8ba1\u7b97\u6a21\u578b\u7684\u53ef\u80fd\u6027\uff0c\u7279\u522b\u662f\u4e3a\u533a\u57df\u53d8\u4f53\u91cf\u8eab\u5b9a\u5236\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b (ASR) \u7cfb\u7edf\u3002", "result": "\u4e3a\u672c\u7814\u7a76\u521b\u5efa\u7684\u6570\u636e\u96c6\u516c\u5f00\u53d1\u5e03\u3002", "conclusion": "\u8fd9\u4e9b\u52aa\u529b\u6709\u53ef\u80fd\u5e94\u7528\u4e8e\u865a\u62df\u52a9\u624b\u548c\u66f4\u5e7f\u6cdb\u7684\u8bed\u8a00\u6280\u672f\uff0c\u6709\u52a9\u4e8e\u4fdd\u62a4\u65b9\u8a00\u591a\u6837\u6027\u548c\u63a8\u8fdb\u9762\u5411\u5b5f\u52a0\u62c9\u8bed\u793e\u533a\u7684\u5305\u5bb9\u6027\u6570\u5b57\u5de5\u5177\u7684\u8fdb\u6b65\u3002"}}
{"id": "2510.24168", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24168", "abs": "https://arxiv.org/abs/2510.24168", "authors": ["Weihua Cheng", "Ersheng Ni", "Wenlong Wang", "Yifei Sun", "Junming Liu", "Wangyu Shen", "Yirong Chen", "Botian Shi", "Ding Wang"], "title": "MGA: Memory-Driven GUI Agent for Observation-Centric Interaction", "comment": "Submitted to WWW2025", "summary": "The rapid progress of Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) has enabled agentic systems capable of perceiving and acting\nacross diverse environments. A challenging yet impactful frontier is the\ndevelopment of GUI agents, which must navigate complex desktop and web\ninterfaces while maintaining robustness and generalization. Existing paradigms\ntypically model tasks as long-chain executions, concatenating historical\ntrajectories into the context. While approaches such as Mirage and GTA1 refine\nplanning or introduce multi-branch action selection, they remain constrained by\ntwo persistent issues: Dependence on historical trajectories, which amplifies\nerror propagation. And Local exploration bias, where \"decision-first,\nobservation-later\" mechanisms overlook critical interface cues. We introduce\nthe Memory-Driven GUI Agent (MGA), which reframes GUI interaction around the\nprinciple of observe first, then decide. MGA models each step as an\nindependent, context-rich environment state represented by a triad: current\nscreenshot, task-agnostic spatial information, and a dynamically updated\nstructured memory. Experiments on OSworld benchmarks, real desktop applications\n(Chrome, VSCode, VLC), and cross-task transfer demonstrate that MGA achieves\nsubstantial gains in robustness, generalization, and efficiency compared to\nstate-of-the-art baselines. The code is publicly available at:\n{https://anonymous.4open.science/r/MGA-3571}.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a Memory-Driven GUI Agent (MGA) \u7684\u65b0\u578b GUI \u4ee3\u7406\uff0c\u5b83\u901a\u8fc7\u9996\u5148\u89c2\u5bdf\u7136\u540e\u51b3\u7b56\u7684\u539f\u5219\u6765\u89e3\u51b3\u73b0\u6709 GUI \u4ee3\u7406\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709 GUI \u4ee3\u7406\u4f9d\u8d56\u5386\u53f2\u8f68\u8ff9\uff0c\u5bb9\u6613\u653e\u5927\u8bef\u5dee\u4f20\u64ad\uff0c\u5e76\u4e14\u5b58\u5728\u5c40\u90e8\u63a2\u7d22\u504f\u5dee\uff0c\u5ffd\u7565\u5173\u952e\u754c\u9762\u7ebf\u7d22\u3002", "method": "MGA \u5c06\u6bcf\u4e00\u6b65\u5efa\u6a21\u4e3a\u4e00\u4e2a\u72ec\u7acb\u7684\u3001\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u73af\u5883\u72b6\u6001\uff0c\u7531\u5f53\u524d\u5c4f\u5e55\u622a\u56fe\u3001\u4efb\u52a1\u65e0\u5173\u7684\u7a7a\u95f4\u4fe1\u606f\u548c\u52a8\u6001\u66f4\u65b0\u7684\u7ed3\u6784\u5316\u5185\u5b58\u8868\u793a\u3002", "result": "\u5728 OSworld \u57fa\u51c6\u6d4b\u8bd5\u3001\u771f\u5b9e\u684c\u9762\u5e94\u7528\u7a0b\u5e8f\uff08Chrome\u3001VSCode\u3001VLC\uff09\u548c\u8de8\u4efb\u52a1\u8f6c\u79fb\u5b9e\u9a8c\u4e2d\uff0cMGA \u5728\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u6548\u7387\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u63d0\u5347\u3002", "conclusion": "MGA \u662f\u4e00\u79cd\u66f4\u6709\u6548\u3001\u66f4\u5f3a\u5927\u7684 GUI \u4ee3\u7406\uff0c\u5b83\u901a\u8fc7\u6539\u53d8 GUI \u4ea4\u4e92\u65b9\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2510.24116", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24116", "abs": "https://arxiv.org/abs/2510.24116", "authors": ["Fengming Yu", "Haiwei Pan", "Kejia Zhang", "Jian Guan", "Haiying Jiang"], "title": "UHKD: A Unified Framework for Heterogeneous Knowledge Distillation via Frequency-Domain Representations", "comment": "14 pages, 4 figures", "summary": "Knowledge distillation (KD) is an effective model compression technique that\ntransfers knowledge from a high-performance teacher to a lightweight student,\nreducing cost while maintaining accuracy. In visual applications, where\nlarge-scale image models are widely used, KD enables efficient deployment.\nHowever, architectural diversity introduces semantic discrepancies that hinder\nthe use of intermediate representations. Most existing KD methods are designed\nfor homogeneous models and degrade in heterogeneous scenarios, especially when\nintermediate features are involved. Prior studies mainly focus on the logits\nspace, making limited use of the semantic information in intermediate layers.\nTo address this limitation, Unified Heterogeneous Knowledge Distillation (UHKD)\nis proposed as a framework that leverages intermediate features in the\nfrequency domain for cross-architecture transfer. Fourier transform is applied\nto capture global feature information, alleviating representational\ndiscrepancies between heterogeneous teacher-student pairs. A Feature\nTransformation Module (FTM) produces compact frequency-domain representations\nof teacher features, while a learnable Feature Alignment Module (FAM) projects\nstudent features and aligns them via multi-level matching. Training is guided\nby a joint objective combining mean squared error on intermediate features with\nKullback-Leibler divergence on logits. Experiments on CIFAR-100 and ImageNet-1K\ndemonstrate gains of 5.59% and 0.83% over the latest method, highlighting UHKD\nas an effective approach for unifying heterogeneous representations and\nenabling efficient utilization of visual knowledge", "AI": {"tldr": "\u77e5\u8bc6\u84b8\u998f (KD) \u662f\u4e00\u79cd\u6709\u6548\u7684\u6a21\u578b\u538b\u7f29\u6280\u672f\uff0c\u53ef\u5c06\u77e5\u8bc6\u4ece\u9ad8\u6027\u80fd\u6559\u5e08\u8f6c\u79fb\u5230\u8f7b\u91cf\u7ea7\u5b66\u751f\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u964d\u4f4e\u6210\u672c\u3002\u4f46\u662f\uff0c\u67b6\u6784\u591a\u6837\u6027\u4f1a\u5f15\u5165\u8bed\u4e49\u5dee\u5f02\uff0c\u4ece\u800c\u963b\u788d\u4e2d\u95f4\u8868\u793a\u7684\u4f7f\u7528\u3002\u672c\u6587\u63d0\u51fa\u4e86\u7edf\u4e00\u5f02\u6784\u77e5\u8bc6\u84b8\u998f (UHKD) \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u9891\u57df\u4e2d\u7684\u4e2d\u95f4\u7279\u5f81\u8fdb\u884c\u8de8\u67b6\u6784\u4f20\u8f93\u3002\u4f7f\u7528\u5085\u91cc\u53f6\u53d8\u6362\u6765\u6355\u83b7\u5168\u5c40\u7279\u5f81\u4fe1\u606f\uff0c\u4ece\u800c\u7f13\u89e3\u5f02\u6784\u5e08\u751f\u5bf9\u4e4b\u95f4\u7684\u8868\u793a\u5dee\u5f02\u3002\u7279\u5f81\u53d8\u6362\u6a21\u5757 (FTM) \u751f\u6210\u6559\u5e08\u7279\u5f81\u7684\u7d27\u51d1\u9891\u57df\u8868\u793a\uff0c\u800c\u53ef\u5b66\u4e60\u7684\u7279\u5f81\u5bf9\u9f50\u6a21\u5757 (FAM) \u6295\u5c04\u5b66\u751f\u7279\u5f81\u5e76\u901a\u8fc7\u591a\u5c42\u5339\u914d\u5bf9\u9f50\u5b83\u4eec\u3002\u5728 CIFAR-100 \u548c ImageNet-1K \u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUHKD \u6bd4\u6700\u65b0\u65b9\u6cd5\u63d0\u9ad8\u4e86 5.59% \u548c 0.83%\uff0c\u7a81\u663e\u4e86 UHKD \u4f5c\u4e3a\u7edf\u4e00\u5f02\u6784\u8868\u793a\u548c\u6709\u6548\u5229\u7528\u89c6\u89c9\u77e5\u8bc6\u7684\u6709\u6548\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u77e5\u8bc6\u84b8\u998f (KD) \u65b9\u6cd5\u5927\u591a\u662f\u4e3a\u540c\u6784\u6a21\u578b\u8bbe\u8ba1\u7684\uff0c\u5728\u5f02\u6784\u573a\u666f\u4e2d\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u4e2d\u95f4\u7279\u5f81\u65f6\u3002\u4ee5\u5f80\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728 logits \u7a7a\u95f4\uff0c\u5bf9\u4e2d\u95f4\u5c42\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\u5229\u7528\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u7edf\u4e00\u5f02\u6784\u77e5\u8bc6\u84b8\u998f (UHKD) \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u9891\u57df\u4e2d\u7684\u4e2d\u95f4\u7279\u5f81\u8fdb\u884c\u8de8\u67b6\u6784\u4f20\u8f93\u3002\u5085\u91cc\u53f6\u53d8\u6362\u7528\u4e8e\u6355\u83b7\u5168\u5c40\u7279\u5f81\u4fe1\u606f\uff0c\u7f13\u89e3\u5f02\u6784\u5e08\u751f\u5bf9\u4e4b\u95f4\u7684\u8868\u793a\u5dee\u5f02\u3002\u7279\u5f81\u53d8\u6362\u6a21\u5757 (FTM) \u751f\u6210\u6559\u5e08\u7279\u5f81\u7684\u7d27\u51d1\u9891\u57df\u8868\u793a\uff0c\u800c\u53ef\u5b66\u4e60\u7684\u7279\u5f81\u5bf9\u9f50\u6a21\u5757 (FAM) \u6295\u5c04\u5b66\u751f\u7279\u5f81\u5e76\u901a\u8fc7\u591a\u5c42\u5339\u914d\u5bf9\u9f50\u5b83\u4eec\u3002\u8bad\u7ec3\u7531\u4e00\u4e2a\u8054\u5408\u76ee\u6807\u5f15\u5bfc\uff0c\u8be5\u76ee\u6807\u5c06\u4e2d\u95f4\u7279\u5f81\u7684\u5747\u65b9\u8bef\u5dee\u4e0e logits \u7684 Kullback-Leibler \u6563\u5ea6\u76f8\u7ed3\u5408\u3002", "result": "\u5728 CIFAR-100 \u548c ImageNet-1K \u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUHKD \u6bd4\u6700\u65b0\u65b9\u6cd5\u63d0\u9ad8\u4e86 5.59% \u548c 0.83%\u3002", "conclusion": "UHKD \u662f\u4e00\u79cd\u7edf\u4e00\u5f02\u6784\u8868\u793a\u548c\u6709\u6548\u5229\u7528\u89c6\u89c9\u77e5\u8bc6\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2510.23663", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23663", "abs": "https://arxiv.org/abs/2510.23663", "authors": ["Padmanabhan Jagannathan Prajesh", "Kaliaperumal Ragunath", "Miriam Gordon", "Bruce Rathgeber", "Suresh Neethirajan"], "title": "AI-Driven Carbon Monitoring: Transformer-Based Reconstruction of Atmospheric CO2 in Canadian Poultry Regions", "comment": null, "summary": "Accurate mapping of column-averaged CO2 (XCO2) over agricultural landscapes\nis essential for guiding emission mitigation strategies. We present a\nSpatiotemporal Vision Transformer with Wavelets (ST-ViWT) framework that\nreconstructs continuous, uncertainty-quantified XCO2 fields from OCO-2 across\nsouthern Canada, emphasizing poultry-intensive regions. The model fuses wavelet\ntime-frequency representations with transformer attention over meteorology,\nvegetation indices, topography, and land cover. On 2024 OCO-2 data, ST-ViWT\nattains R2 = 0.984 and RMSE = 0.468 ppm; 92.3 percent of gap-filled predictions\nlie within +/-1 ppm. Independent validation with TCCON shows robust\ngeneralization (bias = -0.14 ppm; r = 0.928), including faithful reproduction\nof the late-summer drawdown. Spatial analysis across 14 poultry regions reveals\na moderate positive association between facility density and XCO2 (r = 0.43);\nhigh-density areas exhibit larger seasonal amplitudes (9.57 ppm) and enhanced\nsummer variability. Compared with conventional interpolation and standard\nmachine-learning baselines, ST-ViWT yields seamless 0.25 degree CO2 surfaces\nwith explicit uncertainties, enabling year-round coverage despite sparse\nobservations. The approach supports integration of satellite constraints with\nnational inventories and precision livestock platforms to benchmark emissions,\nrefine region-specific factors, and verify interventions. Importantly,\ntransformer-based Earth observation enables scalable, transparent, spatially\nexplicit carbon accounting, hotspot prioritization, and policy-relevant\nmitigation assessment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aST-ViWT\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u91cd\u5efa\u52a0\u62ff\u5357\u90e8\u5730\u533a\u7684\u8fde\u7eedXCO2\u573a\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5c0f\u6ce2\u53d8\u6362\u548cTransformer\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u5229\u7528OCO-2\u6570\u636e\u548c\u5176\u4ed6\u8f85\u52a9\u6570\u636e\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5177\u6709\u5f88\u9ad8\u7684\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u7528\u4e8e\u78b3\u6392\u653e\u6838\u7b97\u548c\u51cf\u6392\u8bc4\u4f30\u3002", "motivation": "\u7cbe\u786e\u7ed8\u5236\u519c\u4e1a\u666f\u89c2\u4e2d\u7684\u67f1\u5e73\u5747CO2 (XCO2) \u56fe\u5bf9\u4e8e\u6307\u5bfc\u51cf\u6392\u7b56\u7565\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u5bb6\u79bd\u517b\u6b96\u5bc6\u96c6\u533a\u57df\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u7a7a\u89c6\u89c9Transformer\u4e0e\u5c0f\u6ce2\u53d8\u6362(ST-ViWT)\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u5c0f\u6ce2\u65f6\u9891\u8868\u793a\u4e0eTransformer\u6ce8\u610f\u529b\u673a\u5236\u76f8\u7ed3\u5408\uff0c\u5e76\u5229\u7528\u6c14\u8c61\u3001\u690d\u88ab\u6307\u6570\u3001\u5730\u5f62\u548c\u571f\u5730\u8986\u76d6\u7b49\u6570\u636e\u3002", "result": "\u57282024\u5e74OCO-2\u6570\u636e\u4e0a\uff0cST-ViWT\u8fbe\u5230\u4e86R2 = 0.984\u548cRMSE = 0.468 ppm\uff1b92.3%\u7684\u586b\u8865\u9884\u6d4b\u503c\u5728+/-1 ppm\u8303\u56f4\u5185\u3002\u4e0eTCCON\u7684\u72ec\u7acb\u9a8c\u8bc1\u663e\u793a\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff08\u504f\u5dee= -0.14 ppm\uff1br = 0.928\uff09\u3002\u7a7a\u95f4\u5206\u6790\u8868\u660e\uff0c\u5bb6\u79bd\u8bbe\u65bd\u5bc6\u5ea6\u4e0eXCO2\u4e4b\u95f4\u5b58\u5728\u4e2d\u7b49\u7a0b\u5ea6\u7684\u6b63\u76f8\u5173\u5173\u7cfb\uff08r = 0.43\uff09\u3002", "conclusion": "\u57fa\u4e8eTransformer\u7684\u5730\u7403\u89c2\u6d4b\u6280\u672f\u80fd\u591f\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u900f\u660e\u3001\u7a7a\u95f4\u663e\u5f0f\u7684\u78b3\u6838\u7b97\u3001\u70ed\u70b9\u4f18\u5148\u7ea7\u6392\u5e8f\u548c\u4e0e\u653f\u7b56\u76f8\u5173\u7684\u51cf\u6392\u8bc4\u4f30\u3002"}}
{"id": "2510.24102", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24102", "abs": "https://arxiv.org/abs/2510.24102", "authors": ["Yihan Wang", "Peiyu Liu", "Runyu Chen", "Jiaxing Pu", "Wei Xu"], "title": "Squrve: A Unified and Modular Framework for Complex Real-World Text-to-SQL Tasks", "comment": null, "summary": "Text-to-SQL technology has evolved rapidly, with diverse academic methods\nachieving impressive results. However, deploying these techniques in real-world\nsystems remains challenging due to limited integration tools. Despite these\nadvances, we introduce Squrve, a unified, modular, and extensive Text-to-SQL\nframework designed to bring together research advances and real-world\napplications. Squrve first establishes a universal execution paradigm that\nstandardizes invocation interfaces, then proposes a multi-actor collaboration\nmechanism based on seven abstracted effective atomic actor components.\nExperiments on widely adopted benchmarks demonstrate that the collaborative\nworkflows consistently outperform the original individual methods, thereby\nopening up a new effective avenue for tackling complex real-world queries. The\ncodes are available at https://github.com/Satissss/Squrve.", "AI": {"tldr": "Squrve is a new Text-to-SQL framework.", "motivation": "Deploying Text-to-SQL techniques in real-world systems is challenging due to limited integration tools.", "method": "A unified, modular, and extensive Text-to-SQL framework is introduced, which establishes a universal execution paradigm and proposes a multi-actor collaboration mechanism based on seven abstracted effective atomic actor components.", "result": "Collaborative workflows consistently outperform the original individual methods on widely adopted benchmarks.", "conclusion": "Squrve opens up a new effective avenue for tackling complex real-world queries."}}
{"id": "2510.24284", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24284", "abs": "https://arxiv.org/abs/2510.24284", "authors": ["Wenhao Wang", "Peizhi Niu", "Zhao Xu", "Zhaoyu Chen", "Jian Du", "Yaxin Du", "Xianghe Pang", "Keduan Huang", "Yanfeng Wang", "Qiang Yan", "Siheng Chen"], "title": "MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and Scaling MCP Tools", "comment": null, "summary": "Large Language Models (LLMs) increasingly rely on external tools to perform\ncomplex, realistic tasks, yet their ability to utilize the rapidly expanding\nModel Contextual Protocol (MCP) ecosystem remains limited. Existing MCP\nresearch covers few servers, depends on costly manual curation, and lacks\ntraining support, hindering progress toward real-world deployment. To overcome\nthese limitations, we introduce MCP-Flow, an automated web-agent-driven\npipeline for large-scale server discovery, data synthesis, and model training.\nMCP-Flow collects and filters data from 1166 servers and 11536 tools, producing\n68733 high-quality instruction-function call pairs and 6439 trajectories, far\nexceeding prior work in scale and diversity. Extensive experiments demonstrate\nMCP-Flow's effectiveness in driving superior MCP tool selection, function-call\ngeneration, and enhanced agentic task performance. MCP-Flow thus provides a\nscalable foundation for advancing LLM agents' proficiency in real-world MCP\nenvironments. MCP-Flow is publicly available at\n\\href{https://github.com/wwh0411/MCP-Flow}{https://github.com/wwh0411/MCP-Flow}.", "AI": {"tldr": "MCP-Flow\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u6d41\u7a0b\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u53d1\u73b0\u670d\u52a1\u5668\u3001\u5408\u6210\u6570\u636e\u548c\u8bad\u7ec3\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8LLM\u4ee3\u7406\u5728\u73b0\u5b9e\u4e16\u754cMCP\u73af\u5883\u4e2d\u7684\u719f\u7ec3\u7a0b\u5ea6\u3002", "motivation": "\u73b0\u6709MCP\u7814\u7a76\u8986\u76d6\u7684\u670d\u52a1\u5668\u5f88\u5c11\uff0c\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684\u624b\u52a8\u7ba1\u7406\uff0c\u5e76\u4e14\u7f3a\u4e4f\u8bad\u7ec3\u652f\u6301\uff0c\u4ece\u800c\u963b\u788d\u4e86\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u90e8\u7f72\u3002", "method": "MCP-Flow\u4f7f\u7528\u4e00\u4e2a\u81ea\u52a8\u5316\u7684web\u4ee3\u7406\u9a71\u52a8\u7684\u6d41\u7a0b\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u670d\u52a1\u5668\u53d1\u73b0\u3001\u6570\u636e\u5408\u6210\u548c\u6a21\u578b\u8bad\u7ec3\u3002\u5b83\u4ece1166\u4e2a\u670d\u52a1\u5668\u548c11536\u4e2a\u5de5\u5177\u4e2d\u6536\u96c6\u548c\u8fc7\u6ee4\u6570\u636e\uff0c\u751f\u621068733\u4e2a\u9ad8\u8d28\u91cf\u7684\u6307\u4ee4-\u51fd\u6570\u8c03\u7528\u5bf9\u548c6439\u4e2a\u8f68\u8ff9\u3002", "result": "MCP-Flow\u5728\u9a71\u52a8\u5353\u8d8a\u7684MCP\u5de5\u5177\u9009\u62e9\u3001\u51fd\u6570\u8c03\u7528\u751f\u6210\u548c\u589e\u5f3a\u7684\u4ee3\u7406\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u975e\u5e38\u6709\u6548\u3002", "conclusion": "MCP-Flow\u4e3a\u63d0\u9ad8LLM\u4ee3\u7406\u5728\u73b0\u5b9e\u4e16\u754cMCP\u73af\u5883\u4e2d\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2510.24117", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24117", "abs": "https://arxiv.org/abs/2510.24117", "authors": ["Zan Wang", "Siyu Chen", "Luya Mo", "Xinfeng Gao", "Yuxin Shen", "Lebin Ding", "Wei Liang"], "title": "DogMo: A Large-Scale Multi-View RGB-D Dataset for 4D Canine Motion Recovery", "comment": "19 pages", "summary": "We present DogMo, a large-scale multi-view RGB-D video dataset capturing\ndiverse canine movements for the task of motion recovery from images. DogMo\ncomprises 1.2k motion sequences collected from 10 unique dogs, offering rich\nvariation in both motion and breed. It addresses key limitations of existing\ndog motion datasets, including the lack of multi-view and real 3D data, as well\nas limited scale and diversity. Leveraging DogMo, we establish four motion\nrecovery benchmark settings that support systematic evaluation across monocular\nand multi-view, RGB and RGB-D inputs. To facilitate accurate motion recovery,\nwe further introduce a three-stage, instance-specific optimization pipeline\nthat fits the SMAL model to the motion sequences. Our method progressively\nrefines body shape and pose through coarse alignment, dense correspondence\nsupervision, and temporal regularization. Our dataset and method provide a\nprincipled foundation for advancing research in dog motion recovery and open up\nnew directions at the intersection of computer vision, computer graphics, and\nanimal behavior modeling.", "AI": {"tldr": "DogMo\u662f\u4e00\u4e2a\u5927\u578b\u591a\u89c6\u89d2RGB-D\u89c6\u9891\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u4ece\u56fe\u50cf\u4e2d\u6062\u590d\u72ac\u7c7b\u8fd0\u52a8\u3002", "motivation": "\u73b0\u6709\u7684\u72d7\u8fd0\u52a8\u6570\u636e\u96c6\u7f3a\u4e4f\u591a\u89c6\u89d2\u548c\u771f\u5b9e3D\u6570\u636e\uff0c\u4ee5\u53ca\u6709\u9650\u7684\u89c4\u6a21\u548c\u591a\u6837\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u3001\u7279\u5b9a\u5b9e\u4f8b\u7684\u4f18\u5316\u6d41\u7a0b\uff0c\u5c06SMAL\u6a21\u578b\u62df\u5408\u5230\u8fd0\u52a8\u5e8f\u5217\u3002", "result": "\u5efa\u7acb\u4e86\u56db\u4e2a\u8fd0\u52a8\u6062\u590d\u57fa\u51c6\u8bbe\u7f6e\uff0c\u652f\u6301\u5355\u76ee\u548c\u591a\u89c6\u89d2\u3001RGB\u548cRGB-D\u8f93\u5165\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u4e3a\u63a8\u8fdb\u72d7\u8fd0\u52a8\u6062\u590d\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u548c\u52a8\u7269\u884c\u4e3a\u5efa\u6a21\u7684\u4ea4\u53c9\u9886\u57df\u5f00\u8f9f\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2510.23665", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23665", "abs": "https://arxiv.org/abs/2510.23665", "authors": ["Juan C. Leon Alcazar", "Mattia Soldan", "Mohammad Saatialsoruji", "Alejandro Pardo", "Hani Itani", "Juan Camilo Perez", "Bernard Ghanem"], "title": "Transformers from Compressed Representations", "comment": null, "summary": "Compressed file formats are the corner stone of efficient data storage and\ntransmission, yet their potential for representation learning remains largely\nunderexplored. We introduce TEMPEST (TransformErs froM comPressed\nrEpreSenTations), a method that exploits the inherent byte-stream structure of\ncompressed files to design an effective tokenization and encoding strategy. By\nleveraging this compact encoding, a standard transformer can directly learn\nsemantic representations from compressed data streams, bypassing the need for\nraw byte-level processing or full media decoding. Our proposal substantially\nreduces the number of tokens required for semantic classification, thereby\nlowering both computational complexity and memory usage. Through extensive\nexperiments across diverse datasets, coding schemes, and modalities, we show\nthat TEMPEST achieves accuracy competitive wit the state-of-the-art while\ndelivering efficiency gains in memory and compute.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTEMPEST\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u76f4\u63a5\u4ece\u538b\u7f29\u6570\u636e\u6d41\u4e2d\u5b66\u4e60\u8bed\u4e49\u8868\u793a\uff0c\u65e0\u9700\u539f\u59cb\u5b57\u8282\u7ea7\u5904\u7406\u6216\u5b8c\u6574\u5a92\u4f53\u89e3\u7801\u3002", "motivation": "\u538b\u7f29\u6587\u4ef6\u683c\u5f0f\u662f\u9ad8\u6548\u6570\u636e\u5b58\u50a8\u548c\u4f20\u8f93\u7684\u57fa\u77f3\uff0c\u4f46\u5b83\u4eec\u5728\u8868\u5f81\u5b66\u4e60\u65b9\u9762\u7684\u6f5c\u529b\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u5229\u7528\u538b\u7f29\u6587\u4ef6\u7684\u56fa\u6709\u5b57\u8282\u6d41\u7ed3\u6784\u6765\u8bbe\u8ba1\u6709\u6548\u7684\u6807\u8bb0\u5316\u548c\u7f16\u7801\u7b56\u7565\u3002", "result": "TEMPEST\u5728\u5404\u79cd\u6570\u636e\u96c6\u3001\u7f16\u7801\u65b9\u6848\u548c\u6a21\u6001\u4e0a\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u6280\u672f\u7ade\u4e89\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u5185\u5b58\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "TEMPEST\u901a\u8fc7\u51cf\u5c11\u8bed\u4e49\u5206\u7c7b\u6240\u9700\u7684token\u6570\u91cf\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u6027\u548c\u5185\u5b58\u4f7f\u7528\u7387\u3002"}}
{"id": "2510.24126", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24126", "abs": "https://arxiv.org/abs/2510.24126", "authors": ["Vivek Kalyan", "Martin Andrews"], "title": "Reinforcement Learning for Long-Horizon Multi-Turn Search Agents", "comment": "4 pages plus references and appendices. Accepted into the First\n  Workshop on Multi-Turn Interactions in Large Language Models at NeurIPS 2025", "summary": "Large Language Model (LLM) agents can leverage multiple turns and tools to\nsolve complex tasks, with prompt-based approaches achieving strong performance.\nThis work demonstrates that Reinforcement Learning (RL) can push capabilities\nsignificantly further by learning from experience. Through experiments on a\nlegal document search benchmark, we show that our RL-trained 14 Billion\nparameter model outperforms frontier class models (85% vs 78% accuracy). In\naddition, we explore turn-restricted regimes, during training and at test-time,\nthat show these agents achieve better results if allowed to operate over longer\nmulti-turn horizons.", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u667a\u80fd\u4f53\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u5728\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u901a\u8fc7\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\uff0c\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u8fd9\u4e9b\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5728\u6cd5\u5f8b\u6587\u6863\u641c\u7d22\u57fa\u51c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e86\u4e00\u4e2a140\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u3002", "result": "\u8be5\u6a21\u578b\u4f18\u4e8e\u524d\u6cbf\u6a21\u578b\uff0885% vs 78% \u7684\u51c6\u786e\u7387\uff09\u3002", "conclusion": "\u5141\u8bb8\u667a\u80fd\u4f53\u5728\u66f4\u957f\u7684\u591a\u8f6e\u4ea4\u4e92\u4e2d\u64cd\u4f5c\uff0c\u53ef\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u7ed3\u679c\u3002"}}
{"id": "2510.24297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24297", "abs": "https://arxiv.org/abs/2510.24297", "authors": ["Robin Schm\u00f6cker", "Alexander Dockhorn", "Bodo Rosenhahn"], "title": "Investigating Intra-Abstraction Policies For Non-exact Abstraction Algorithms", "comment": null, "summary": "One weakness of Monte Carlo Tree Search (MCTS) is its sample efficiency which\ncan be addressed by building and using state and/or action abstractions in\nparallel to the tree search such that information can be shared among nodes of\nthe same layer. The primary usage of abstractions for MCTS is to enhance the\nUpper Confidence Bound (UCB) value during the tree policy by aggregating visits\nand returns of an abstract node. However, this direct usage of abstractions\ndoes not take the case into account where multiple actions with the same parent\nmight be in the same abstract node, as these would then all have the same UCB\nvalue, thus requiring a tiebreak rule. In state-of-the-art abstraction\nalgorithms such as pruned On the Go Abstractions (pruned OGA), this case has\nnot been noticed, and a random tiebreak rule was implicitly chosen. In this\npaper, we propose and empirically evaluate several alternative\nintra-abstraction policies, several of which outperform the random policy\nacross a majority of environments and parameter settings.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u4e2d\u62bd\u8c61\u65b9\u6cd5\u5728\u540c\u4e00\u62bd\u8c61\u8282\u70b9\u5185\u5b58\u5728\u591a\u4e2a\u52a8\u4f5c\u65f6\u7684\u7b56\u7565\u9009\u62e9\u95ee\u9898\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651\u8fd9\u79cd\u60c5\u51b5\uff0c\u5e76\u63d0\u51fa\u4e86\u51e0\u79cd\u65b0\u7684\u7b56\u7565\u3002", "motivation": "MCTS\u7684\u91c7\u6837\u6548\u7387\u8f83\u4f4e\uff0c\u53ef\u4ee5\u901a\u8fc7\u6784\u5efa\u548c\u4f7f\u7528\u72b6\u6001\u548c/\u6216\u52a8\u4f5c\u62bd\u8c61\u6765\u89e3\u51b3\uff0c\u4ee5\u4fbf\u5728\u540c\u4e00\u5c42\u7684\u8282\u70b9\u4e4b\u95f4\u5171\u4eab\u4fe1\u606f\u3002\u73b0\u6709\u62bd\u8c61\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651\u540c\u4e00\u62bd\u8c61\u8282\u70b9\u5185\u5b58\u5728\u591a\u4e2a\u52a8\u4f5c\u7684\u60c5\u51b5\u3002", "method": "\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u51e0\u79cd\u66ff\u4ee3\u7684\u5185\u90e8\u62bd\u8c61\u7b56\u7565\u3002", "result": "\u51e0\u79cd\u63d0\u51fa\u7684\u7b56\u7565\u5728\u5927\u591a\u6570\u73af\u5883\u548c\u53c2\u6570\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u968f\u673a\u7b56\u7565\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u51e0\u79cd\u6539\u8fdbMCTS\u62bd\u8c61\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u5347\u7b97\u6cd5\u6027\u80fd\u3002"}}
{"id": "2510.24129", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24129", "abs": "https://arxiv.org/abs/2510.24129", "authors": ["Jiajian Xie", "Hubery Yin", "Chen Li", "Zhou Zhao", "Shengyu Zhang"], "title": "ETC: training-free diffusion models acceleration with Error-aware Trend Consistency", "comment": "17 pages, 10 figures", "summary": "Diffusion models have achieved remarkable generative quality but remain\nbottlenecked by costly iterative sampling. Recent training-free methods\naccelerate diffusion process by reusing model outputs. However, these methods\nignore denoising trends and lack error control for model-specific tolerance,\nleading to trajectory deviations under multi-step reuse and exacerbating\ninconsistencies in the generated results. To address these issues, we introduce\nError-aware Trend Consistency (ETC), a framework that (1) introduces a\nconsistent trend predictor that leverages the smooth continuity of diffusion\ntrajectories, projecting historical denoising patterns into stable future\ndirections and progressively distributing them across multiple approximation\nsteps to achieve acceleration without deviating; (2) proposes a model-specific\nerror tolerance search mechanism that derives corrective thresholds by\nidentifying transition points from volatile semantic planning to stable quality\nrefinement. Experiments show that ETC achieves a 2.65x acceleration over FLUX\nwith negligible (-0.074 SSIM score) degradation of consistency.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aError-aware Trend Consistency (ETC)\u7684\u6846\u67b6\uff0c\u65e8\u5728\u52a0\u901f\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u7ed3\u679c\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u65e0\u8bad\u7ec3\u52a0\u901f\u65b9\u6cd5\u5ffd\u7565\u4e86\u53bb\u566a\u8d8b\u52bf\uff0c\u5e76\u4e14\u7f3a\u4e4f\u5bf9\u6a21\u578b\u7279\u5b9a\u5bb9\u9519\u7684\u8bef\u5dee\u63a7\u5236\uff0c\u5bfc\u81f4\u8f68\u8ff9\u504f\u5dee\u548c\u751f\u6210\u7ed3\u679c\u4e0d\u4e00\u81f4\u3002", "method": "\u8be5\u6846\u67b6\u5f15\u5165\u4e86\u4e00\u4e2a\u4e00\u81f4\u7684\u8d8b\u52bf\u9884\u6d4b\u5668\uff0c\u5229\u7528\u6269\u6563\u8f68\u8ff9\u7684\u5e73\u6ed1\u8fde\u7eed\u6027\uff0c\u5c06\u5386\u53f2\u53bb\u566a\u6a21\u5f0f\u6295\u5f71\u5230\u7a33\u5b9a\u7684\u672a\u6765\u65b9\u5411\uff0c\u5e76\u9010\u6b65\u5c06\u5176\u5206\u914d\u5230\u591a\u4e2a\u8fd1\u4f3c\u6b65\u9aa4\u4e2d\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u7279\u5b9a\u7684\u8bef\u5dee\u5bb9\u9519\u641c\u7d22\u673a\u5236\uff0c\u901a\u8fc7\u8bc6\u522b\u4ece\u4e0d\u7a33\u5b9a\u7684\u8bed\u4e49\u89c4\u5212\u5230\u7a33\u5b9a\u7684\u8d28\u91cf\u6539\u8fdb\u7684\u8fc7\u6e21\u70b9\u6765\u5bfc\u51fa\u6821\u6b63\u9608\u503c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cETC\u6bd4FLUX\u5b9e\u73b0\u4e862.65\u500d\u7684\u52a0\u901f\uff0c\u800c\u4e00\u81f4\u6027\u7684\u964d\u7ea7\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\uff08-0.074 SSIM score\uff09\u3002", "conclusion": "ETC\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u52a0\u901f\u6269\u6563\u6a21\u578b\u7684\u91c7\u6837\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u7ed3\u679c\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2510.23667", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.23667", "abs": "https://arxiv.org/abs/2510.23667", "authors": ["Amin Heyrani Nobari", "Lyle Regenwetter", "Cyril Picard", "Ligong Han", "Faez Ahmed"], "title": "Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free Structural Topology Optimization", "comment": null, "summary": "Structural topology optimization (TO) is central to engineering design but\nremains computationally intensive due to complex physics and hard constraints.\nExisting deep-learning methods are limited to fixed square grids, a few\nhand-coded boundary conditions, and post-hoc optimization, preventing general\ndeployment. We introduce Optimize Any Topology (OAT), a foundation-model\nframework that directly predicts minimum-compliance layouts for arbitrary\naspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines\na resolution- and shape-agnostic autoencoder with an implicit neural-field\ndecoder and a conditional latent-diffusion model trained on OpenTO, a new\ncorpus of 2.2 million optimized structures covering 2 million unique\nboundary-condition configurations. On four public benchmarks and two\nchallenging unseen tests, OAT lowers mean compliance up to 90% relative to the\nbest prior models and delivers sub-1 second inference on a single GPU across\nresolutions from 64 x 64 to 256 x 256 and aspect ratios as high as 10:1. These\nresults establish OAT as a general, fast, and resolution-free framework for\nphysics-aware topology optimization and provide a large-scale dataset to spur\nfurther research in generative modeling for inverse design. Code & data can be\nfound at https://github.com/ahnobari/OptimizeAnyTopology.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOAT\u7684\u62d3\u6251\u4f18\u5316\u6846\u67b6\uff0c\u53ef\u4ee5\u5feb\u901f\u9884\u6d4b\u5404\u79cd\u957f\u5bbd\u6bd4\u3001\u5206\u8fa8\u7387\u3001\u4f53\u79ef\u5206\u6570\u3001\u8f7d\u8377\u548c\u5939\u5177\u4e0b\u7684\u6700\u5c0f\u67d4\u5ea6\u5e03\u5c40\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u7ed3\u6784\u62d3\u6251\u4f18\u5316\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f8b\u5982\u53ea\u80fd\u5904\u7406\u56fa\u5b9a\u65b9\u5f62\u7f51\u683c\u548c\u5c11\u6570\u624b\u52a8\u7f16\u7801\u7684\u8fb9\u754c\u6761\u4ef6\uff0c\u5e76\u4e14\u9700\u8981\u8fdb\u884c\u540e\u5904\u7406\u4f18\u5316\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u901a\u7528\u6027\u3002", "method": "OAT\u7ed3\u5408\u4e86\u4e0e\u5206\u8fa8\u7387\u548c\u5f62\u72b6\u65e0\u5173\u7684\u81ea\u7f16\u7801\u5668\u3001\u9690\u5f0f\u795e\u7ecf\u573a\u89e3\u7801\u5668\u4ee5\u53ca\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u5e76\u5728\u5305\u542b220\u4e07\u4e2a\u4f18\u5316\u7ed3\u6784\u7684OpenTO\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOAT\u76f8\u5bf9\u4e8e\u73b0\u6709\u6700\u4f73\u6a21\u578b\uff0c\u5e73\u5747\u67d4\u5ea6\u964d\u4f4e\u9ad8\u8fbe90%\uff0c\u5e76\u4e14\u5728\u5355\u4e2aGPU\u4e0a\u53ef\u4ee5\u57281\u79d2\u5185\u5b8c\u6210\u4ece64x64\u5230256x256\u5206\u8fa8\u7387\u4ee5\u53ca\u9ad8\u8fbe10:1\u957f\u5bbd\u6bd4\u7684\u63a8\u7406\u3002", "conclusion": "OAT\u662f\u4e00\u4e2a\u901a\u7528\u7684\u3001\u5feb\u901f\u7684\u3001\u4e0e\u5206\u8fa8\u7387\u65e0\u5173\u7684\u62d3\u6251\u4f18\u5316\u6846\u67b6\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u4ee5\u4fc3\u8fdb\u9006\u5411\u8bbe\u8ba1\u7684\u751f\u6210\u5efa\u6a21\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2510.24139", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24139", "abs": "https://arxiv.org/abs/2510.24139", "authors": ["Chanwoo Park", "Suyoung Park", "Yelim Ahn", "Jongmin Kim", "Jongyeon Park", "Jaejin Lee"], "title": "Beyond Line-Level Filtering for the Pretraining Corpora of LLMs", "comment": "submitted to ACL ARR Rolling Review", "summary": "While traditional line-level filtering techniques, such as line-level\ndeduplication and trailing-punctuation filters, are commonly used, these basic\nmethods can sometimes discard valuable content, negatively affecting downstream\nperformance. In this paper, we introduce two methods-pattern-aware line-level\ndeduplication (PLD) and pattern-aware trailing punctuation filtering (PTF)-by\nenhancing the conventional filtering techniques. Our approach not only\nconsiders line-level signals but also takes into account their sequential\ndistribution across documents, enabling us to retain structurally important\ncontent that might otherwise be removed. We evaluate these proposed methods by\ntraining small language models (1 B parameters) in both English and Korean. The\nresults demonstrate that our methods consistently improve performance on\nmultiple-choice benchmarks and significantly enhance generative\nquestion-answering accuracy on both SQuAD v1 and KorQuAD v1.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e24\u79cd\u6539\u8fdb\u7684\u884c\u7ea7\u8fc7\u6ee4\u6280\u672f\uff0c\u5206\u522b\u662f\u6a21\u5f0f\u611f\u77e5\u884c\u7ea7\u53bb\u91cd (PLD) \u548c\u6a21\u5f0f\u611f\u77e5\u5c3e\u968f\u6807\u70b9\u8fc7\u6ee4 (PTF)\uff0c\u65e8\u5728\u4fdd\u7559\u7ed3\u6784\u4e0a\u91cd\u8981\u7684\u5185\u5bb9\u3002", "motivation": "\u4f20\u7edf\u884c\u7ea7\u8fc7\u6ee4\u6280\u672f\u53ef\u80fd\u4f1a\u4e22\u5f03\u6709\u4ef7\u503c\u7684\u5185\u5bb9\uff0c\u4ece\u800c\u5bf9\u4e0b\u6e38\u6027\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u8003\u8651\u884c\u7ea7\u4fe1\u53f7\u53ca\u5176\u5728\u6587\u6863\u4e2d\u7684\u987a\u5e8f\u5206\u5e03\u6765\u589e\u5f3a\u4f20\u7edf\u8fc7\u6ee4\u6280\u672f\u3002", "result": "\u5728\u82f1\u8bed\u548c\u97e9\u8bed\u76841B\u53c2\u6570\u5c0f\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u9879\u9009\u62e9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u63d0\u9ad8\u6027\u80fd\uff0c\u5e76\u663e\u7740\u63d0\u9ad8SQuAD v1\u548cKorQuAD v1\u4e0a\u7684\u751f\u6210\u5f0f\u95ee\u7b54\u51c6\u786e\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u5f0f\u611f\u77e5\u8fc7\u6ee4\u65b9\u6cd5\u80fd\u591f\u4fdd\u7559\u7ed3\u6784\u4e0a\u91cd\u8981\u7684\u5185\u5bb9\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.24299", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24299", "abs": "https://arxiv.org/abs/2510.24299", "authors": ["Jiayu Liu", "Wei Dai", "Zhenya Huang", "Ning Miao", "Enhong Chen"], "title": "Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank", "comment": null, "summary": "Despite the strong reasoning ability of large language models~(LLMs), they\nare prone to errors and hallucinations. As a result, how to check their outputs\neffectively and efficiently has become a critical problem in their\napplications. Existing checking methods heavily rely on external resources,\nsuch as trained verifiers (e.g., process/outcome reward models) or elaborate\nprompts, which lead to high computational overhead and are only applicable to\nspecific domains. In this paper, we investigate whether the internal behaviors\nof LLMs have already implied the credibility of their reasoning paths.\nSpecifically, we find that the rank of the correlation matrix between the input\nproblem and the output reasoning path is a robust indicator of reasoning\ncorrectness. Different from other correctness indicators for LLMs, the\ncalculation of the correlation matrix only relies on the LLM itself, which\navoids the hassle of training a separate model or designing complicated\nprompts. Based on it, we design a simple, plug-and-play Self-Indicator method\nto reweight candidate reasoning paths, which achieves significant performance\nimprovements than other voting and verification methods with very few\ncomputational overhead. Our experiments across multiple LLMs of varying scales\nand model families have further shown the effectiveness of Self-Indicator. It\nachieves over 75% accuracy in distinguishing correct reasoning paths from\nincorrect ones, and, in turn, improves the accuracies on three reasoning\nbenchmarks by more than 8%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LLM\u8f93\u51fa\u68c0\u67e5\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528LLM\u5185\u90e8\u884c\u4e3a\u6765\u5224\u65ad\u63a8\u7406\u8def\u5f84\u7684\u6b63\u786e\u6027\uff0c\u65e0\u9700\u5916\u90e8\u8d44\u6e90\u3002", "motivation": "\u73b0\u6709LLM\u68c0\u67e5\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u8d44\u6e90\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u9002\u7528\u8303\u56f4\u6709\u9650\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u8f93\u5165\u95ee\u9898\u548c\u8f93\u51fa\u63a8\u7406\u8def\u5f84\u4e4b\u95f4\u7684\u76f8\u5173\u77e9\u9635\u7684\u79e9\uff0c\u4f5c\u4e3a\u63a8\u7406\u6b63\u786e\u6027\u7684\u6307\u6807\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684Self-Indicator\u65b9\u6cd5\u6765\u91cd\u65b0\u8c03\u6574\u5019\u9009\u63a8\u7406\u8def\u5f84\u7684\u6743\u91cd\u3002", "result": "Self-Indicator\u65b9\u6cd5\u5728\u533a\u5206\u6b63\u786e\u548c\u9519\u8bef\u63a8\u7406\u8def\u5f84\u65b9\u9762\u8fbe\u5230\u4e8675%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728\u4e09\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c06\u51c6\u786e\u7387\u63d0\u9ad8\u4e868%\u4ee5\u4e0a\u3002", "conclusion": "\u76f8\u5173\u77e9\u9635\u7684\u79e9\u662fLLM\u63a8\u7406\u6b63\u786e\u6027\u7684\u4e00\u4e2a\u53ef\u9760\u6307\u6807\uff0cSelf-Indicator\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u9ad8LLM\u7684\u63a8\u7406\u51c6\u786e\u6027\u3002"}}
{"id": "2510.24133", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24133", "abs": "https://arxiv.org/abs/2510.24133", "authors": ["Minsuk Ji", "Sanghyeok Lee", "Namhyuk Ahn"], "title": "Compositional Image Synthesis with Inference-Time Scaling", "comment": "projcet page: https://github.com/gcl-inha/ReFocus", "summary": "Despite their impressive realism, modern text-to-image models still struggle\nwith compositionality, often failing to render accurate object counts,\nattributes, and spatial relations. To address this challenge, we present a\ntraining-free framework that combines an object-centric approach with\nself-refinement to improve layout faithfulness while preserving aesthetic\nquality. Specifically, we leverage large language models (LLMs) to synthesize\nexplicit layouts from input prompts, and we inject these layouts into the image\ngeneration process, where a object-centric vision-language model (VLM) judge\nreranks multiple candidates to select the most prompt-aligned outcome\niteratively. By unifying explicit layout-grounding with self-refine-based\ninference-time scaling, our framework achieves stronger scene alignment with\nprompts compared to recent text-to-image models. The code are available at\nhttps://github.com/gcl-inha/ReFocus.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5bf9\u8c61\u4e2d\u5fc3\u65b9\u6cd5\u548c\u81ea\u6211\u5b8c\u5584\u6765\u63d0\u9ad8\u5e03\u5c40\u7684\u771f\u5b9e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u7f8e\u5b66\u8d28\u91cf\u3002", "motivation": "\u73b0\u4ee3\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u7ec4\u5408\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u901a\u5e38\u65e0\u6cd5\u6e32\u67d3\u51c6\u786e\u7684\u5bf9\u8c61\u6570\u91cf\u3001\u5c5e\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\u3002", "method": "\u8be5\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4ece\u8f93\u5165\u63d0\u793a\u5408\u6210\u663e\u5f0f\u5e03\u5c40\uff0c\u5e76\u5c06\u8fd9\u4e9b\u5e03\u5c40\u6ce8\u5165\u5230\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\u4e2d\u3002\u4e00\u4e2a\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5224\u65ad\u91cd\u65b0\u6392\u5217\u591a\u4e2a\u5019\u9009\u5bf9\u8c61\uff0c\u4ee5\u8fed\u4ee3\u5730\u9009\u62e9\u4e0e\u63d0\u793a\u6700\u4e00\u81f4\u7684\u7ed3\u679c\u3002", "result": "\u901a\u8fc7\u7edf\u4e00\u663e\u5f0f\u5e03\u5c40 grounding \u4e0e\u57fa\u4e8e\u81ea\u6211\u5b8c\u5584\u7684\u63a8\u7406\u65f6\u7f29\u653e\uff0c\u8be5\u6846\u67b6\u4e0e\u6700\u8fd1\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u66f4\u5f3a\u7684\u573a\u666f\u4e0e\u63d0\u793a\u5bf9\u9f50\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5bf9\u8c61\u4e2d\u5fc3\u65b9\u6cd5\u548c\u81ea\u6211\u5b8c\u5584\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u7ec4\u5408\u6027\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u51c6\u786e\u5730\u6839\u636e\u6587\u672c\u63d0\u793a\u751f\u6210\u56fe\u50cf\u3002"}}
{"id": "2510.23668", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23668", "abs": "https://arxiv.org/abs/2510.23668", "authors": ["Fujiang Yuan", "Yangrui Fan", "Xiaohuan Bing", "Zhen Tian", "Chunhong Yuan", "Yankang Li"], "title": "Traffic flow forecasting, STL decomposition, Hybrid model, LSTM, ARIMA, XGBoost, Intelligent transportation systems", "comment": null, "summary": "Accurate traffic flow forecasting is essential for intelligent transportation\nsystems and urban traffic management. However, single model approaches often\nfail to capture the complex, nonlinear, and multi scale temporal patterns in\ntraffic flow data. This study proposes a decomposition driven hybrid framework\nthat integrates Seasonal Trend decomposition using Loess (STL) with three\ncomplementary predictive models. STL first decomposes the original time series\ninto trend, seasonal, and residual components. Then, a Long Short Term Memory\n(LSTM) network models long term trends, an Autoregressive Integrated Moving\nAverage (ARIMA) model captures seasonal periodicity, and an Extreme Gradient\nBoosting (XGBoost) algorithm predicts nonlinear residual fluctuations. The\nfinal forecast is obtained through multiplicative integration of the sub model\npredictions. Using 998 traffic flow records from a New York City intersection\nbetween November and December 2015, results show that the LSTM ARIMA XGBoost\nhybrid model significantly outperforms standalone models including LSTM, ARIMA,\nand XGBoost across MAE, RMSE, and R squared metrics. The decomposition strategy\neffectively isolates temporal characteristics, allowing each model to\nspecialize, thereby improving prediction accuracy, interpretability, and\nrobustness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u89e3\u9a71\u52a8\u7684\u6df7\u5408\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u4f7f\u7528 Loess (STL) \u8fdb\u884c\u7684\u5b63\u8282\u6027\u8d8b\u52bf\u5206\u89e3\u4e0e\u4e09\u79cd\u4e92\u8865\u9884\u6d4b\u6a21\u578b\u3002", "motivation": "\u7cbe\u786e\u7684\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\u5bf9\u4e8e\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u548c\u57ce\u5e02\u4ea4\u901a\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5355\u4e00\u6a21\u578b\u65b9\u6cd5\u901a\u5e38\u65e0\u6cd5\u6355\u83b7\u4ea4\u901a\u6d41\u91cf\u6570\u636e\u4e2d\u590d\u6742\u7684\u3001\u975e\u7ebf\u6027\u7684\u548c\u591a\u5c3a\u5ea6\u7684\u65f6\u5e8f\u6a21\u5f0f\u3002", "method": "STL \u9996\u5148\u5c06\u539f\u59cb\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e3a\u8d8b\u52bf\u3001\u5b63\u8282\u6027\u548c\u6b8b\u5dee\u5206\u91cf\u3002\u7136\u540e\uff0c\u957f\u77ed\u671f\u8bb0\u5fc6 (LSTM) \u7f51\u7edc\u5bf9\u957f\u671f\u8d8b\u52bf\u8fdb\u884c\u5efa\u6a21\uff0c\u81ea\u56de\u5f52\u7efc\u5408\u79fb\u52a8\u5e73\u5747 (ARIMA) \u6a21\u578b\u6355\u83b7\u5b63\u8282\u6027\u5468\u671f\u6027\uff0c\u800c\u6781\u7aef\u68af\u5ea6\u63d0\u5347 (XGBoost) \u7b97\u6cd5\u9884\u6d4b\u975e\u7ebf\u6027\u6b8b\u5dee\u6ce2\u52a8\u3002\u6700\u7ec8\u9884\u6d4b\u662f\u901a\u8fc7\u5b50\u6a21\u578b\u9884\u6d4b\u7684\u4e58\u6cd5\u79ef\u5206\u83b7\u5f97\u7684\u3002", "result": "\u4f7f\u7528 2015 \u5e74 11 \u6708\u81f3 12 \u6708\u7ebd\u7ea6\u5e02\u4ea4\u53c9\u8def\u53e3\u7684 998 \u6761\u4ea4\u901a\u6d41\u91cf\u8bb0\u5f55\uff0c\u7ed3\u679c\u8868\u660e LSTM ARIMA XGBoost \u6df7\u5408\u6a21\u578b\u5728 MAE\u3001RMSE \u548c R \u5e73\u65b9\u6307\u6807\u65b9\u9762\u663e\u7740\u4f18\u4e8e\u5305\u62ec LSTM\u3001ARIMA \u548c XGBoost \u5728\u5185\u7684\u72ec\u7acb\u6a21\u578b\u3002", "conclusion": "\u5206\u89e3\u7b56\u7565\u6709\u6548\u5730\u9694\u79bb\u4e86\u65f6\u95f4\u7279\u5f81\uff0c\u4f7f\u6bcf\u4e2a\u6a21\u578b\u90fd\u53ef\u4ee5\u4e13\u95e8\u5316\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.24150", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24150", "abs": "https://arxiv.org/abs/2510.24150", "authors": ["Chanwoo Park", "Suyoung Park", "JiA Kang", "Jongyeon Park", "Sangho Kim", "Hyunji M. Park", "Sumin Bae", "Mingyu Kang", "Jaejin Lee"], "title": "Ko-MuSR: A Multistep Soft Reasoning Benchmark for LLMs Capable of Understanding Korean", "comment": "submitted to ACL ARR Rolling Review", "summary": "We present Ko-MuSR, the first benchmark to comprehensively evaluate\nmultistep, soft reasoning in long Korean narratives while minimizing data\ncontamination. Built following MuSR, Ko-MuSR features fully Korean narratives,\nreasoning chains, and multiple-choice questions verified by human annotators\nfor logical consistency and answerability. Evaluations of four large language\nmodels -- two multilingual and two Korean-specialized -- show that multilingual\nmodels outperform Korean-focused ones even in Korean reasoning tasks,\nindicating cross-lingual generalization of reasoning ability. Carefully\ndesigned prompting strategies, which combine few-shot examples, reasoning\ntraces, and task-specific hints, further boost accuracy, approaching\nhuman-level performance. Ko-MuSR offers a solid foundation for advancing Korean\nNLP by enabling systematic evaluation of long-context reasoning and prompting\nstrategies.", "AI": {"tldr": "Ko-MuSR: \u8bc4\u4f30\u97e9\u8bed\u53d9\u4e8b\u4e2d\u591a\u6b65\u8f6f\u63a8\u7406\u7684\u9996\u4e2a\u57fa\u51c6\u3002", "motivation": "\u5728\u957f\u7bc7\u97e9\u8bed\u53d9\u4e8b\u4e2d\uff0c\u5168\u9762\u8bc4\u4f30\u591a\u6b65\u3001\u8f6f\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u6570\u636e\u6c61\u67d3\u3002", "method": "\u6784\u5efa\u4e86\u5b8c\u5168\u97e9\u8bed\u7684\u53d9\u4e8b\u3001\u63a8\u7406\u94fe\u548c\u591a\u9879\u9009\u62e9\u9898\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u6ce8\u91ca\u5458\u9a8c\u8bc1\u4e86\u903b\u8f91\u4e00\u81f4\u6027\u548c\u53ef\u89e3\u7b54\u6027\u3002", "result": "\u591a\u8bed\u8a00\u6a21\u578b\u5728\u97e9\u8bed\u63a8\u7406\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4e13\u6ce8\u4e8e\u97e9\u8bed\u7684\u6a21\u578b\uff0c\u8868\u660e\u4e86\u63a8\u7406\u80fd\u529b\u7684\u8de8\u8bed\u8a00\u6cdb\u5316\u3002", "conclusion": "Ko-MuSR \u4e3a\u63a8\u8fdb\u97e9\u8bed NLP \u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u53ef\u4ee5\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u63d0\u793a\u7b56\u7565\u3002"}}
{"id": "2510.24303", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24303", "abs": "https://arxiv.org/abs/2510.24303", "authors": ["Deniz Gorur", "Antoni Rago", "Francesca Toni"], "title": "Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental Forecasting", "comment": null, "summary": "Judgmental forecasting is the task of making predictions about future events\nbased on human judgment. This task can be seen as a form of claim verification,\nwhere the claim corresponds to a future event and the task is to assess the\nplausibility of that event. In this paper, we propose a novel multi-agent\nframework for claim verification, whereby different agents may disagree on\nclaim veracity and bring specific evidence for and against the claims,\nrepresented as quantitative bipolar argumentation frameworks (QBAFs). We then\ninstantiate the framework for supporting claim verification, with a variety of\nagents realised with Large Language Models (LLMs): (1) ArgLLM agents, an\nexisting approach for claim verification that generates and evaluates QBAFs;\n(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)\nfrom external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,\nextending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of\narguments from external sources. Finally, we conduct experiments with two\nstandard judgmental forecasting datasets, with instances of our framework with\ntwo or three agents, empowered by six different base LLMs. We observe that\ncombining evidence from agents can improve forecasting accuracy, especially in\nthe case of three agents, while providing an explainable combination of\nevidence for claim verification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u7528\u4e8e\u5224\u65ad\u9884\u6d4b\uff0c\u5176\u4e2d\u4e0d\u540c\u7684\u667a\u80fd\u4f53\u53ef\u80fd\u5bf9\u58f0\u660e\u7684\u771f\u5b9e\u6027\u5b58\u5728\u5206\u6b67\uff0c\u5e76\u4e3a\u652f\u6301\u548c\u53cd\u5bf9\u58f0\u660e\u63d0\u4f9b\u5177\u4f53\u8bc1\u636e\u3002", "motivation": "\u57fa\u4e8e\u4eba\u7c7b\u5224\u65ad\u5bf9\u672a\u6765\u4e8b\u4ef6\u8fdb\u884c\u9884\u6d4b\u7684\u4efb\u52a1\u53ef\u4ee5\u770b\u4f5c\u662f\u4e00\u79cd\u58f0\u660e\u9a8c\u8bc1\uff0c\u5176\u4e2d\u58f0\u660e\u5bf9\u5e94\u4e8e\u672a\u6765\u4e8b\u4ef6\uff0c\u4efb\u52a1\u662f\u8bc4\u4f30\u8be5\u4e8b\u4ef6\u7684\u5408\u7406\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u65b0\u9896\u7684\u7528\u4e8e\u58f0\u660e\u9a8c\u8bc1\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u4e0d\u540c\u7684\u667a\u80fd\u4f53\u53ef\u80fd\u5bf9\u58f0\u660e\u7684\u771f\u5b9e\u6027\u5b58\u5728\u5206\u6b67\uff0c\u5e76\u4e3a\u652f\u6301\u548c\u53cd\u5bf9\u58f0\u660e\u63d0\u4f9b\u5177\u4f53\u7684\u8bc1\u636e\uff0c\u8fd9\u4e9b\u8bc1\u636e\u8868\u793a\u4e3a\u5b9a\u91cf\u53cc\u6781\u8bba\u8bc1\u6846\u67b6 (QBAF)\u3002", "result": "\u7ed3\u5408\u6765\u81ea\u667a\u80fd\u4f53\u7684\u8bc1\u636e\u53ef\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u4e09\u4e2a\u667a\u80fd\u4f53\u7684\u60c5\u51b5\u4e0b\uff0c\u540c\u65f6\u4e3a\u58f0\u660e\u9a8c\u8bc1\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bc1\u636e\u7ec4\u5408\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u9a8c\u89c2\u5bdf\u5230\uff0c\u7ed3\u5408\u6765\u81ea\u667a\u80fd\u4f53\u7684\u8bc1\u636e\u53ef\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u4e09\u4e2a\u667a\u80fd\u4f53\u7684\u60c5\u51b5\u4e0b\uff0c\u540c\u65f6\u4e3a\u58f0\u660e\u9a8c\u8bc1\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bc1\u636e\u7ec4\u5408\u3002"}}
{"id": "2510.24134", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24134", "abs": "https://arxiv.org/abs/2510.24134", "authors": ["Yang Du", "Zhuoran Lin", "Kaiqiang Song", "Biao Wang", "Zhicheng Zheng", "Tiezheng Ge", "Bo Zheng", "Qin Jin"], "title": "VC4VG: Optimizing Video Captions for Text-to-Video Generation", "comment": "Accepted by EMNLP 2025", "summary": "Recent advances in text-to-video (T2V) generation highlight the critical role\nof high-quality video-text pairs in training models capable of producing\ncoherent and instruction-aligned videos. However, strategies for optimizing\nvideo captions specifically for T2V training remain underexplored. In this\npaper, we introduce VC4VG (Video Captioning for Video Generation), a\ncomprehensive caption optimization framework tailored to the needs of T2V\nmodels.We begin by analyzing caption content from a T2V perspective,\ndecomposing the essential elements required for video reconstruction into\nmultiple dimensions, and proposing a principled caption design methodology. To\nsupport evaluation, we construct VC4VG-Bench, a new benchmark featuring\nfine-grained, multi-dimensional, and necessity-graded metrics aligned with\nT2V-specific requirements.Extensive T2V fine-tuning experiments demonstrate a\nstrong correlation between improved caption quality and video generation\nperformance, validating the effectiveness of our approach. We release all\nbenchmark tools and code at https://github.com/qyr0403/VC4VG to support further\nresearch.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u9891\u5b57\u5e55\u4f18\u5316\u6846\u67b6VC4VG\uff0c\u4e13\u95e8\u4e3a\u6587\u672c\u5230\u89c6\u9891\uff08T2V\uff09\u6a21\u578b\u7684\u9700\u6c42\u91cf\u8eab\u5b9a\u5236\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9T2V\u8bad\u7ec3\u4f18\u5316\u89c6\u9891\u5b57\u5e55\u7684\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u4eceT2V\u7684\u89d2\u5ea6\u5206\u6790\u5b57\u5e55\u5185\u5bb9\uff0c\u5c06\u89c6\u9891\u91cd\u5efa\u6240\u9700\u7684\u57fa\u672c\u5143\u7d20\u5206\u89e3\u4e3a\u591a\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u539f\u5219\u7684\u5b57\u5e55\u8bbe\u8ba1\u65b9\u6cd5\u3002\u6784\u5efa\u4e86VC4VG-Bench\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\uff0c\u5177\u6709\u7ec6\u7c92\u5ea6\u3001\u591a\u7ef4\u5ea6\u548c\u5fc5\u8981\u6027\u5206\u7ea7\u7684\u6307\u6807\uff0c\u4e0eT2V\u7684\u7279\u5b9a\u8981\u6c42\u76f8\u4e00\u81f4\u3002", "result": "\u5927\u91cf\u7684T2V\u5fae\u8c03\u5b9e\u9a8c\u8868\u660e\uff0c\u5b57\u5e55\u8d28\u91cf\u7684\u63d0\u9ad8\u4e0e\u89c6\u9891\u751f\u6210\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u5f88\u5f3a\u7684\u76f8\u5173\u6027\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u53d1\u5e03\u4e86\u6240\u6709\u57fa\u51c6\u5de5\u5177\u548c\u4ee3\u7801\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u3002"}}
{"id": "2510.23671", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23671", "abs": "https://arxiv.org/abs/2510.23671", "authors": ["Marmik Chaudhari", "Jeremi Nuer", "Rome Thorstenson"], "title": "Sparsity and Superposition in Mixture of Experts", "comment": null, "summary": "Mixture of Experts (MoE) models have become central to scaling large language\nmodels, yet their mechanistic differences from dense networks remain poorly\nunderstood. Previous work has explored how dense models use\n\\textit{superposition} to represent more features than dimensions, and how\nsuperposition is a function of feature sparsity and feature importance. MoE\nmodels cannot be explained mechanistically through the same lens. We find that\nneither feature sparsity nor feature importance cause discontinuous phase\nchanges, and that network sparsity (the ratio of active to total experts)\nbetter characterizes MoEs. We develop new metrics for measuring superposition\nacross experts. Our findings demonstrate that models with greater network\nsparsity exhibit greater \\emph{monosemanticity}. We propose a new definition of\nexpert specialization based on monosemantic feature representation rather than\nload balancing, showing that experts naturally organize around coherent feature\ncombinations when initialized appropriately. These results suggest that network\nsparsity in MoEs may enable more interpretable models without sacrificing\nperformance, challenging the common assumption that interpretability and\ncapability are fundamentally at odds.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86MoE\u6a21\u578b\u4e0e\u7a20\u5bc6\u6a21\u578b\u5728\u673a\u5236\u4e0a\u7684\u5dee\u5f02\uff0c\u53d1\u73b0MoE\u6a21\u578b\u7684\u7f51\u7edc\u7a00\u758f\u6027\u4e0e\u5355\u4e49\u6027\u76f8\u5173\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u5355\u4e49\u7279\u5f81\u8868\u793a\u7684\u4e13\u5bb6\u4e13\u4e1a\u5316\u5b9a\u4e49\u3002", "motivation": "\u7406\u89e3MoE\u6a21\u578b\u4e0e\u7a20\u5bc6\u7f51\u7edc\u5728\u673a\u5236\u4e0a\u7684\u5dee\u5f02\uff0c\u73b0\u6709\u7814\u7a76\u672a\u80fd\u89e3\u91caMoE\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u6d4b\u91cf\u4e13\u5bb6\u4e4b\u95f4\u7684\u53e0\u52a0\u6765\u5f00\u53d1\u65b0\u7684\u6307\u6807\uff0c\u5e76\u5206\u6790\u7f51\u7edc\u7a00\u758f\u6027\u3001\u7279\u5f81\u7a00\u758f\u6027\u548c\u7279\u5f81\u91cd\u8981\u6027\u3002", "result": "\u53d1\u73b0\u7f51\u7edc\u7a00\u758f\u6027\u66f4\u9ad8\u7684\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5355\u4e49\u6027\uff0c\u4e13\u5bb6\u56f4\u7ed5\u8fde\u8d2f\u7684\u7279\u5f81\u7ec4\u5408\u8fdb\u884c\u7ec4\u7ec7\u3002", "conclusion": "MoE\u4e2d\u7684\u7f51\u7edc\u7a00\u758f\u6027\u53ef\u80fd\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u66f4\u53ef\u89e3\u91ca\u7684\u6a21\u578b\uff0c\u6311\u6218\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u80fd\u529b\u6839\u672c\u4e0a\u76f8\u4e92\u5bf9\u7acb\u7684\u5e38\u89c1\u5047\u8bbe\u3002"}}
{"id": "2510.24178", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24178", "abs": "https://arxiv.org/abs/2510.24178", "authors": ["Aaron Scott", "Maike Z\u00fcfle", "Jan Niehues"], "title": "MuSaG: A Multimodal German Sarcasm Dataset with Full-Modal Annotations", "comment": null, "summary": "Sarcasm is a complex form of figurative language in which the intended\nmeaning contradicts the literal one. Its prevalence in social media and popular\nculture poses persistent challenges for natural language understanding,\nsentiment analysis, and content moderation. With the emergence of multimodal\nlarge language models, sarcasm detection extends beyond text and requires\nintegrating cues from audio and vision. We present MuSaG, the first German\nmultimodal sarcasm detection dataset, consisting of 33 minutes of manually\nselected and human-annotated statements from German television shows. Each\ninstance provides aligned text, audio, and video modalities, annotated\nseparately by humans, enabling evaluation in unimodal and multimodal settings.\nWe benchmark nine open-source and commercial models, spanning text, audio,\nvision, and multimodal architectures, and compare their performance to human\nannotations. Our results show that while humans rely heavily on audio in\nconversational settings, models perform best on text. This highlights a gap in\ncurrent multimodal models and motivates the use of MuSaG for developing models\nbetter suited to realistic scenarios. We release MuSaG publicly to support\nfuture research on multimodal sarcasm detection and human-model alignment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u5fb7\u8bed\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u6570\u636e\u96c6MuSaG\uff0c\u5305\u542b\u4ece\u5fb7\u56fd\u7535\u89c6\u8282\u76ee\u4e2d\u624b\u52a8\u9009\u62e9\u548c\u4eba\u5de5\u6ce8\u91ca\u7684\u8bed\u53e5\uff0c\u517133\u5206\u949f\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u548c\u6d41\u884c\u6587\u5316\u4e2d\u666e\u904d\u5b58\u5728\u8bbd\u523a\uff0c\u5bf9\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u60c5\u611f\u5206\u6790\u548c\u5185\u5bb9\u5ba1\u6838\u6784\u6210\u4e86\u6301\u7eed\u7684\u6311\u6218\u3002\u968f\u7740\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\uff0c\u8bbd\u523a\u68c0\u6d4b\u6269\u5c55\u5230\u6587\u672c\u4e4b\u5916\uff0c\u9700\u8981\u6574\u5408\u6765\u81ea\u97f3\u9891\u548c\u89c6\u89c9\u7684\u7ebf\u7d22\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u5bf9\u9f50\u7684\u6587\u672c\u3001\u97f3\u9891\u548c\u89c6\u9891\u6a21\u6001\u7684\u6570\u636e\u96c6\uff0c\u5e76\u7531\u4eba\u5de5\u5355\u72ec\u6ce8\u91ca\uff0c\u4ece\u800c\u80fd\u591f\u5728\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u8bbe\u7f6e\u4e2d\u8fdb\u884c\u8bc4\u4f30\u3002\u4f7f\u7528\u4e5d\u4e2a\u5f00\u6e90\u548c\u5546\u4e1a\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8de8\u8d8a\u6587\u672c\u3001\u97f3\u9891\u3001\u89c6\u89c9\u548c\u591a\u6a21\u6001\u67b6\u6784\uff0c\u5e76\u5c06\u5b83\u4eec\u7684\u6027\u80fd\u4e0e\u4eba\u7c7b\u6ce8\u91ca\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u4eba\u7c7b\u5728\u5bf9\u8bdd\u73af\u5883\u4e2d\u4e25\u91cd\u4f9d\u8d56\u97f3\u9891\uff0c\u4f46\u6a21\u578b\u5728\u6587\u672c\u4e0a\u7684\u8868\u73b0\u6700\u4f73\u3002\u8fd9\u7a81\u51fa\u4e86\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u7684\u5dee\u8ddd\u3002", "conclusion": "\u53d1\u5e03MuSaG\u4ee5\u652f\u6301\u672a\u6765\u5bf9\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u548c\u4eba\u673a\u5bf9\u9f50\u7684\u7814\u7a76\u3002"}}
{"id": "2510.24337", "categories": ["cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.24337", "abs": "https://arxiv.org/abs/2510.24337", "authors": ["Daria Kravets-Meinke", "Hannah Schmid-Petri", "Sonja Niemann", "Ute Schmid"], "title": "Generative Large Language Models (gLLMs) in Content Analysis: A Practical Guide for Communication Research", "comment": null, "summary": "Generative Large Language Models (gLLMs), such as ChatGPT, are increasingly\nbeing used in communication research for content analysis. Studies show that\ngLLMs can outperform both crowd workers and trained coders, such as research\nassistants, on various coding tasks relevant to communication science, often at\na fraction of the time and cost. Additionally, gLLMs can decode implicit\nmeanings and contextual information, be instructed using natural language,\ndeployed with only basic programming skills, and require little to no annotated\ndata beyond a validation dataset - constituting a paradigm shift in automated\ncontent analysis. Despite their potential, the integration of gLLMs into the\nmethodological toolkit of communication research remains underdeveloped. In\ngLLM-assisted quantitative content analysis, researchers must address at least\nseven critical challenges that impact result quality: (1) codebook development,\n(2) prompt engineering, (3) model selection, (4) parameter tuning, (5)\niterative refinement, (6) validation of the model's reliability, and\noptionally, (7) performance enhancement. This paper synthesizes emerging\nresearch on gLLM-assisted quantitative content analysis and proposes a\ncomprehensive best-practice guide to navigate these challenges. Our goal is to\nmake gLLM-based content analysis more accessible to a broader range of\ncommunication researchers and ensure adherence to established disciplinary\nquality standards of validity, reliability, reproducibility, and research\nethics.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u4e3a\u901a\u4fe1\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e00\u4e2a\u5168\u9762\u7684\u6700\u4f73\u5b9e\u8df5\u6307\u5357\uff0c\u4ee5\u5e94\u5bf9\u5728 gLLM \u8f85\u52a9\u7684\u5b9a\u91cf\u5185\u5bb9\u5206\u6790\u4e2d\u9047\u5230\u7684\u6311\u6218\uff0c\u4ece\u800c\u786e\u4fdd\u7814\u7a76\u7ed3\u679c\u7684\u8d28\u91cf\u5e76\u4f7f\u5176\u66f4\u6613\u4e8e\u8bbf\u95ee\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210\u5f0f\u5927\u578b\u8bed\u8a00\u6a21\u578b (gLLM) \u5728\u901a\u4fe1\u7814\u7a76\u4e2d\u7684\u5185\u5bb9\u5206\u6790\u4e2d\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u4f7f\u7528\uff0c\u4f46\u5b83\u4eec\u5728\u65b9\u6cd5\u8bba\u5de5\u5177\u5305\u4e2d\u7684\u6574\u5408\u4ecd\u7136\u4e0d\u53d1\u8fbe\u3002\u7814\u7a76\u8868\u660e\uff0cgLLM \u5728\u5404\u79cd\u4e0e\u901a\u4fe1\u79d1\u5b66\u76f8\u5173\u7684\u7f16\u7801\u4efb\u52a1\u4e0a\u53ef\u4ee5\u80dc\u8fc7\u4f17\u5305\u5de5\u4f5c\u8005\u548c\u8bad\u7ec3\u6709\u7d20\u7684\u7f16\u7801\u5458\uff0c\u800c\u4e14\u901a\u5e38\u53ea\u9700\u82b1\u8d39\u5f88\u5c11\u7684\u65f6\u95f4\u548c\u6210\u672c\u3002\u6b64\u5916\uff0cgLLM \u53ef\u4ee5\u89e3\u7801\u9690\u542b\u610f\u4e49\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u53ef\u4ee5\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u8fdb\u884c\u6307\u5bfc\uff0c\u53ea\u9700\u57fa\u672c\u7684\u7f16\u7a0b\u6280\u80fd\u5373\u53ef\u90e8\u7f72\uff0c\u5e76\u4e14\u9664\u4e86\u9a8c\u8bc1\u6570\u636e\u96c6\u4e4b\u5916\u51e0\u4e4e\u4e0d\u9700\u8981\u6216\u4e0d\u9700\u8981\u6ce8\u91ca\u6570\u636e\u2014\u2014\u8fd9\u6784\u6210\u4e86\u81ea\u52a8\u5316\u5185\u5bb9\u5206\u6790\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "method": "\u672c\u6587\u7efc\u5408\u4e86\u65b0\u5174\u7684 gLLM \u8f85\u52a9\u5b9a\u91cf\u5185\u5bb9\u5206\u6790\u7814\u7a76\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6700\u4f73\u5b9e\u8df5\u6307\u5357\uff0c\u4ee5\u5e94\u5bf9\u81f3\u5c11\u4e03\u4e2a\u5f71\u54cd\u7ed3\u679c\u8d28\u91cf\u7684\u5173\u952e\u6311\u6218\uff1a(1) \u7f16\u7801\u624b\u518c\u5f00\u53d1\uff0c(2) \u63d0\u793a\u5de5\u7a0b\uff0c(3) \u6a21\u578b\u9009\u62e9\uff0c(4) \u53c2\u6570\u8c03\u6574\uff0c(5) \u8fed\u4ee3\u6539\u8fdb\uff0c(6) \u6a21\u578b\u53ef\u9760\u6027\u7684\u9a8c\u8bc1\uff0c\u4ee5\u53ca\u53ef\u9009\u7684 (7) \u6027\u80fd\u589e\u5f3a\u3002", "result": "\u672c\u6587\u65e8\u5728\u63d0\u9ad8 gLLM \u5728\u5185\u5bb9\u5206\u6790\u4e2d\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u5e76\u786e\u4fdd\u9075\u5b88\u5df2\u5efa\u7acb\u7684\u5b66\u79d1\u8d28\u91cf\u6807\u51c6\uff0c\u5982\u6709\u6548\u6027\u3001\u53ef\u9760\u6027\u3001\u53ef\u91cd\u590d\u6027\u548c\u7814\u7a76\u4f26\u7406\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u4e3a\u901a\u4fe1\u7814\u7a76\u4eba\u5458\u63d0\u4f9b gLLM \u8f85\u52a9\u5b9a\u91cf\u5185\u5bb9\u5206\u6790\u7684\u6700\u4f73\u5b9e\u8df5\u6307\u5357\uff0c\u4ee5\u5e94\u5bf9\u6311\u6218\u5e76\u786e\u4fdd\u7814\u7a76\u8d28\u91cf\u3002"}}
{"id": "2510.24136", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24136", "abs": "https://arxiv.org/abs/2510.24136", "authors": ["Ovi Sarkar", "Md Shafiuzzaman", "Md. Faysal Ahamed", "Golam Mahmud", "Muhammad E. H. Chowdhury"], "title": "MSRANetV2: An Explainable Deep Learning Architecture for Multi-class Classification of Colorectal Histopathological Images", "comment": null, "summary": "Colorectal cancer (CRC) is a leading worldwide cause of cancer-related\nmortality, and the role of prompt precise detection is of paramount interest in\nimproving patient outcomes. Conventional diagnostic methods such as colonoscopy\nand histological examination routinely exhibit subjectivity, are extremely\ntime-consuming, and are susceptible to variation. Through the development of\ndigital pathology, deep learning algorithms have become a powerful approach in\nenhancing diagnostic precision and efficiency. In our work, we proposed a\nconvolutional neural network architecture named MSRANetV2, specially optimized\nfor the classification of colorectal tissue images. The model employs a\nResNet50V2 backbone, extended with residual attention mechanisms and\nsqueeze-and-excitation (SE) blocks, to extract deep semantic and fine-grained\nspatial features. With channel alignment and upsampling operations, MSRANetV2\neffectively fuses multi-scale representations, thereby enhancing the robustness\nof the classification. We evaluated our model on a five-fold stratified\ncross-validation strategy on two publicly available datasets: CRC-VAL-HE-7K and\nNCT-CRC-HE-100K. The proposed model achieved remarkable average Precision,\nrecall, F1-score, AUC, and test accuracy were 0.9884 plus-minus 0.0151, 0.9900\nplus-minus 0.0151, 0.9900 plus-minus 0.0145, 0.9999 plus-minus 0.00006, and\n0.9905 plus-minus 0.0025 on the 7K dataset. On the 100K dataset, they were\n0.9904 plus-minus 0.0091, 0.9900 plus-minus 0.0071, 0.9900 plus-minus 0.0071,\n0.9997 plus-minus 0.00016, and 0.9902 plus-minus 0.0006. Additionally, Grad-CAM\nvisualizations were incorporated to enhance model interpretability by\nhighlighting tissue areas that are medically relevant. These findings validate\nthat MSRANetV2 is a reliable, interpretable, and high-performing architectural\nmodel for classifying CRC tissues.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMSRANetV2\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\uff0c\u7528\u4e8e\u7ed3\u76f4\u80a0\u7ec4\u7ec7\u56fe\u50cf\u5206\u7c7b\uff0c\u5e76\u5728\u4e24\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u7ed3\u76f4\u80a0\u764c\u8bca\u65ad\u65b9\u6cd5\u5b58\u5728\u4e3b\u89c2\u6027\u3001\u8017\u65f6\u548c\u6613\u53d8\u6027\u7b49\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7cbe\u786e\u548c\u9ad8\u6548\u7684\u8bca\u65ad\u65b9\u6cd5\u3002", "method": "\u8be5\u6a21\u578b\u91c7\u7528ResNet50V2\u9aa8\u5e72\u7f51\u7edc\uff0c\u5e76\u6269\u5c55\u4e86\u6b8b\u5dee\u6ce8\u610f\u529b\u673a\u5236\u548csqueeze-and-excitation (SE)\u5757\uff0c\u4ee5\u63d0\u53d6\u6df1\u5c42\u8bed\u4e49\u548c\u7ec6\u7c92\u5ea6\u7684\u7a7a\u95f4\u7279\u5f81\u3002\u901a\u8fc7\u901a\u9053\u5bf9\u9f50\u548c\u4e0a\u91c7\u6837\u64cd\u4f5c\uff0cMSRANetV2\u6709\u6548\u5730\u878d\u5408\u4e86\u591a\u5c3a\u5ea6\u8868\u793a\u3002", "result": "\u5728CRC-VAL-HE-7K\u548cNCT-CRC-HE-100K\u4e24\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6a21\u578b\u53d6\u5f97\u4e86\u51fa\u8272\u7684\u5e73\u5747\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u3001AUC\u548c\u6d4b\u8bd5\u51c6\u786e\u7387\u3002", "conclusion": "MSRANetV2\u662f\u4e00\u79cd\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u4e14\u9ad8\u6027\u80fd\u7684\u7ed3\u76f4\u80a0\u764c\u7ec4\u7ec7\u5206\u7c7b\u6a21\u578b\u3002"}}
{"id": "2510.23672", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23672", "abs": "https://arxiv.org/abs/2510.23672", "authors": ["Xiangfei Qiu", "Xingjian Wu", "Hanyin Cheng", "Xvyuan Liu", "Chenjuan Guo", "Jilin Hu", "Bin Yang"], "title": "DBLoss: Decomposition-based Loss Function for Time Series Forecasting", "comment": "Accepted by NeurIPS 2025", "summary": "Time series forecasting holds significant value in various domains such as\neconomics, traffic, energy, and AIOps, as accurate predictions facilitate\ninformed decision-making. However, the existing Mean Squared Error (MSE) loss\nfunction sometimes fails to accurately capture the seasonality or trend within\nthe forecasting horizon, even when decomposition modules are used in the\nforward propagation to model the trend and seasonality separately. To address\nthese challenges, we propose a simple yet effective Decomposition-Based Loss\nfunction called DBLoss. This method uses exponential moving averages to\ndecompose the time series into seasonal and trend components within the\nforecasting horizon, and then calculates the loss for each of these components\nseparately, followed by weighting them. As a general loss function, DBLoss can\nbe combined with any deep learning forecasting model. Extensive experiments\ndemonstrate that DBLoss significantly improves the performance of\nstate-of-the-art models across diverse real-world datasets and provides a new\nperspective on the design of time series loss functions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u635f\u5931\u51fd\u6570DBLoss\uff0c\u5b83\u53ef\u4ee5\u5206\u89e3\u65f6\u95f4\u5e8f\u5217\u5e76\u5206\u522b\u8ba1\u7b97\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u5206\u91cf\u7684\u635f\u5931\u3002", "motivation": "\u73b0\u6709\u7684MSE\u635f\u5931\u51fd\u6570\u6709\u65f6\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u9884\u6d4b\u8303\u56f4\u5185\u7684\u5b63\u8282\u6027\u6216\u8d8b\u52bf\u3002", "method": "\u4f7f\u7528\u6307\u6570\u79fb\u52a8\u5e73\u5747\u6cd5\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e3a\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u5206\u91cf\uff0c\u7136\u540e\u5206\u522b\u8ba1\u7b97\u6bcf\u4e2a\u5206\u91cf\u7684\u635f\u5931\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u52a0\u6743\u3002", "result": "DBLoss \u663e\u8457\u63d0\u9ad8\u4e86\u5404\u79cd\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e2d\u6700\u5148\u8fdb\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "DBLoss \u4e3a\u65f6\u95f4\u5e8f\u5217\u635f\u5931\u51fd\u6570\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2510.24179", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24179", "abs": "https://arxiv.org/abs/2510.24179", "authors": ["Iv\u00e1n Mart\u00ednez-Murillo", "Paloma Moreda", "Elena Lloret"], "title": "Exploring the Influence of Relevant Knowledge for Natural Language Generation Interpretability", "comment": null, "summary": "This paper explores the influence of external knowledge integration in\nNatural Language Generation (NLG), focusing on a commonsense generation task.\nWe extend the CommonGen dataset by creating KITGI, a benchmark that pairs input\nconcept sets with retrieved semantic relations from ConceptNet and includes\nmanually annotated outputs. Using the T5-Large model, we compare sentence\ngeneration under two conditions: with full external knowledge and with filtered\nknowledge where highly relevant relations were deliberately removed. Our\ninterpretability benchmark follows a three-stage method: (1) identifying and\nremoving key knowledge, (2) regenerating sentences, and (3) manually assessing\noutputs for commonsense plausibility and concept coverage. Results show that\nsentences generated with full knowledge achieved 91\\% correctness across both\ncriteria, while filtering reduced performance drastically to 6\\%. These\nfindings demonstrate that relevant external knowledge is critical for\nmaintaining both coherence and concept coverage in NLG. This work highlights\nthe importance of designing interpretable, knowledge-enhanced NLG systems and\ncalls for evaluation frameworks that capture the underlying reasoning beyond\nsurface-level metrics.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u5916\u90e8\u77e5\u8bc6\u6574\u5408\u5728\u81ea\u7136\u8bed\u8a00\u751f\u6210\uff08NLG\uff09\u4e2d\u7684\u5f71\u54cd\uff0c\u91cd\u70b9\u5173\u6ce8\u5e38\u8bc6\u751f\u6210\u4efb\u52a1\u3002\u4f5c\u8005\u6269\u5c55\u4e86CommonGen\u6570\u636e\u96c6\uff0c\u521b\u5efa\u4e86KITGI\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u5c06\u8f93\u5165\u6982\u5ff5\u96c6\u4e0e\u4eceConceptNet\u68c0\u7d22\u5230\u7684\u8bed\u4e49\u5173\u7cfb\u914d\u5bf9\uff0c\u5e76\u5305\u62ec\u624b\u52a8\u6ce8\u91ca\u7684\u8f93\u51fa\u3002", "motivation": "\u8bba\u6587\u7684\u52a8\u673a\u662f\u7814\u7a76\u5916\u90e8\u77e5\u8bc6\u6574\u5408\u5728\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4e2d\u7684\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5728\u5e38\u8bc6\u751f\u6210\u4efb\u52a1\u4e2d\u3002\u7814\u7a76\u4eba\u5458\u5e0c\u671b\u4e86\u89e3\u5916\u90e8\u77e5\u8bc6\u5982\u4f55\u5f71\u54cd\u751f\u6210\u53e5\u5b50\u7684\u8d28\u91cf\uff0c\u5305\u62ec\u5e38\u8bc6\u5408\u7406\u6027\u548c\u6982\u5ff5\u8986\u76d6\u7387\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u79cd\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a(1) \u8bc6\u522b\u5e76\u79fb\u9664\u5173\u952e\u77e5\u8bc6\uff1b(2) \u91cd\u65b0\u751f\u6210\u53e5\u5b50\uff1b(3) \u624b\u52a8\u8bc4\u4f30\u8f93\u51fa\u7684\u5e38\u8bc6\u5408\u7406\u6027\u548c\u6982\u5ff5\u8986\u76d6\u7387\u3002\u4f7f\u7528T5-Large\u6a21\u578b\uff0c\u6bd4\u8f83\u4e86\u5728\u4e24\u79cd\u6761\u4ef6\u4e0b\u7684\u53e5\u5b50\u751f\u6210\uff1a\u4f7f\u7528\u5b8c\u6574\u5916\u90e8\u77e5\u8bc6\u548c\u4f7f\u7528\u8fc7\u6ee4\u77e5\u8bc6\uff08\u5220\u9664\u9ad8\u5ea6\u76f8\u5173\u7684\u5173\u7cfb\uff09\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u5b8c\u6574\u77e5\u8bc6\u751f\u6210\u7684\u53e5\u5b50\u5728\u4e24\u4e2a\u6807\u51c6\u4e0a\u7684\u6b63\u786e\u7387\u8fbe\u523091%\uff0c\u800c\u8fc7\u6ee4\u540e\u6027\u80fd\u6025\u5267\u4e0b\u964d\u81f36%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u5173\u7684\u5916\u90e8\u77e5\u8bc6\u5bf9\u4e8e\u7ef4\u6301NLG\u4e2d\u7684\u8fde\u8d2f\u6027\u548c\u6982\u5ff5\u8986\u76d6\u7387\u81f3\u5173\u91cd\u8981\u3002\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u8bbe\u8ba1\u53ef\u89e3\u91ca\u7684\u3001\u77e5\u8bc6\u589e\u5f3a\u7684NLG\u7cfb\u7edf\u7684\u91cd\u8981\u6027\uff0c\u5e76\u547c\u5401\u5efa\u7acb\u80fd\u591f\u6355\u6349\u8868\u9762\u6307\u6807\u80cc\u540e\u6f5c\u5728\u63a8\u7406\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2510.24339", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24339", "abs": "https://arxiv.org/abs/2510.24339", "authors": ["Yunxuan Jiang", "Silan Hu", "Xiaoning Wang", "Yuanyuan Zhang", "Xiangyu Chang"], "title": "VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science Automation", "comment": "29 pages, 6 figures. Yunxuan Jiang and Silan Hu contributed equally.\n  Code available at https://github.com/fengzer/VDSAgents", "summary": "Large language models (LLMs) become increasingly integrated into data science\nworkflows for automated system design. However, these LLM-driven data science\nsystems rely solely on the internal reasoning of LLMs, lacking guidance from\nscientific and theoretical principles. This limits their trustworthiness and\nrobustness, especially when dealing with noisy and complex real-world datasets.\nThis paper provides VDSAgents, a multi-agent system grounded in the\nPredictability-Computability-Stability (PCS) principles proposed in the\nVeridical Data Science (VDS) framework. Guided by PCS principles, the system\nimplements a modular workflow for data cleaning, feature engineering, modeling,\nand evaluation. Each phase is handled by an elegant agent, incorporating\nperturbation analysis, unit testing, and model validation to ensure both\nfunctionality and scientific auditability. We evaluate VDSAgents on nine\ndatasets with diverse characteristics, comparing it with state-of-the-art\nend-to-end data science systems, such as AutoKaggle and DataInterpreter, using\nDeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the\nresults of AutoKaggle and DataInterpreter, which validates the feasibility of\nembedding PCS principles into LLM-driven data science automation.", "AI": {"tldr": "VDSAgents\u662f\u4e00\u4e2a\u57fa\u4e8eVeridical Data Science (VDS)\u6846\u67b6\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5b83\u5c06\u53ef\u9884\u6d4b\u6027-\u53ef\u8ba1\u7b97\u6027-\u7a33\u5b9a\u6027 (PCS) \u539f\u5219\u5d4c\u5165\u5230LLM\u9a71\u52a8\u7684\u6570\u636e\u79d1\u5b66\u81ea\u52a8\u5316\u4e2d\uff0c\u7528\u4e8e\u6570\u636e\u6e05\u7406\u3001\u7279\u5f81\u5de5\u7a0b\u3001\u5efa\u6a21\u548c\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u7684LLM\u9a71\u52a8\u7684\u6570\u636e\u79d1\u5b66\u7cfb\u7edf\u4ec5\u4ec5\u4f9d\u8d56\u4e8eLLM\u7684\u5185\u90e8\u63a8\u7406\uff0c\u7f3a\u4e4f\u79d1\u5b66\u548c\u7406\u8bba\u539f\u5219\u7684\u6307\u5bfc\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u7684\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u5608\u6742\u548c\u590d\u6742\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u65f6\u3002", "method": "\u8be5\u7cfb\u7edf\u5b9e\u73b0\u4e86\u6a21\u5757\u5316\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6bcf\u4e2a\u9636\u6bb5\u90fd\u7531\u4e00\u4e2a\u667a\u80fd\u4f53\u5904\u7406\uff0c\u7ed3\u5408\u4e86\u6270\u52a8\u5206\u6790\u3001\u5355\u5143\u6d4b\u8bd5\u548c\u6a21\u578b\u9a8c\u8bc1\uff0c\u4ee5\u786e\u4fdd\u529f\u80fd\u6027\u548c\u79d1\u5b66\u53ef\u5ba1\u8ba1\u6027\u3002", "result": "\u5728\u4e5d\u4e2a\u5177\u6709\u4e0d\u540c\u7279\u5f81\u7684\u6570\u636e\u96c6\u4e0a\uff0cVDSAgents\u4f18\u4e8eAutoKaggle\u548cDataInterpreter\u7b49\u6700\u5148\u8fdb\u7684\u7aef\u5230\u7aef\u6570\u636e\u79d1\u5b66\u7cfb\u7edf\u3002", "conclusion": "\u5c06PCS\u539f\u5219\u5d4c\u5165\u5230LLM\u9a71\u52a8\u7684\u6570\u636e\u79d1\u5b66\u81ea\u52a8\u5316\u4e2d\u662f\u53ef\u884c\u7684\u3002"}}
{"id": "2510.24152", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24152", "abs": "https://arxiv.org/abs/2510.24152", "authors": ["Aodi Wu", "Xubo Luo"], "title": "Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning", "comment": "RoboSense Challenge with IROS 2025", "summary": "This technical report presents our solution for the RoboSense Challenge at\nIROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving\nscene understanding across perception, prediction, planning, and corruption\ndetection tasks. We propose a systematic framework built on four core\ncomponents. First, a Mixture-of-Prompts router classifies questions and\ndispatches them to task-specific expert prompts, eliminating interference\nacross diverse question types. Second, task-specific prompts embed explicit\ncoordinate systems, spatial reasoning rules, role-playing,\nChain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to\neach task. Third, a visual assembly module composes multi-view images with\nobject crops, magenta markers, and adaptive historical frames based on question\nrequirements. Fourth, we configure model inference parameters (temperature,\ntop-p, message roles) per task to optimize output quality. Implemented on\nQwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean\ndata) and 72.85% on Phase-2 (corrupted data), demonstrating that structured\nprompting and spatial grounding substantially enhance VLM performance on\nsafety-critical autonomous driving tasks. Code and prompt are available at\nhttps://github.com/wuaodi/UCAS-CSU-phase2.", "AI": {"tldr": "\u672c\u6280\u672f\u62a5\u544a\u4ecb\u7ecd\u4e86\u6211\u4eec\u5728 IROS 2025 RoboSense \u6311\u6218\u8d5b\u4e2d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u8bc4\u4f30\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u65b9\u9762\u7684\u80fd\u529b\uff0c\u6db5\u76d6\u611f\u77e5\u3001\u9884\u6d4b\u3001\u89c4\u5212\u548c\u8150\u8d25\u68c0\u6d4b\u4efb\u52a1\u3002", "motivation": "\u65e8\u5728\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u65b9\u9762\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u611f\u77e5\u3001\u9884\u6d4b\u3001\u89c4\u5212\u548c\u8150\u8d25\u68c0\u6d4b\u7b49\u5b89\u5168\u5173\u952e\u4efb\u52a1\u4e0a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u7684\u7cfb\u7edf\u6846\u67b6\uff1a\u6df7\u5408\u63d0\u793a\u8def\u7531\u5668\u3001\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u3001\u89c6\u89c9\u7ec4\u88c5\u6a21\u5757\u548c\u6a21\u578b\u63a8\u7406\u53c2\u6570\u914d\u7f6e\u3002", "result": "\u5728 Qwen2.5-VL-72B \u4e0a\u5b9e\u65bd\u7684\u65b9\u6cd5\u5728 Phase-1\uff08\u5e72\u51c0\u6570\u636e\uff09\u4e0a\u5b9e\u73b0\u4e86 70.87% \u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u5728 Phase-2\uff08\u635f\u574f\u6570\u636e\uff09\u4e0a\u5b9e\u73b0\u4e86 72.85% \u7684\u5e73\u5747\u51c6\u786e\u7387\u3002", "conclusion": "\u7ed3\u6784\u5316\u63d0\u793a\u548c\u7a7a\u95f4\u5b9a\u4f4d\u80fd\u591f\u663e\u8457\u63d0\u9ad8 VLM \u5728\u5b89\u5168\u5173\u952e\u578b\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.23681", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23681", "abs": "https://arxiv.org/abs/2510.23681", "authors": ["Carl Hvarfner", "David Eriksson", "Eytan Bakshy", "Max Balandat"], "title": "Informed Initialization for Bayesian Optimization and Active Learning", "comment": "28 pages", "summary": "Bayesian Optimization is a widely used method for optimizing expensive\nblack-box functions, relying on probabilistic surrogate models such as Gaussian\nProcesses. The quality of the surrogate model is crucial for good optimization\nperformance, especially in the few-shot setting where only a small number of\nbatches of points can be evaluated. In this setting, the initialization plays a\ncritical role in shaping the surrogate's predictive quality and guiding\nsubsequent optimization. Despite this, practitioners typically rely on\n(quasi-)random designs to cover the input space. However, such approaches\nneglect two key factors: (a) space-filling designs may not be desirable to\nreduce predictive uncertainty, and (b) efficient hyperparameter learning during\ninitialization is essential for high-quality prediction, which may conflict\nwith space-filling designs. To address these limitations, we propose\nHyperparameter-Informed Predictive Exploration (HIPE), a novel acquisition\nstrategy that balances predictive uncertainty reduction with hyperparameter\nlearning using information-theoretic principles. We derive a closed-form\nexpression for HIPE in the Gaussian Process setting and demonstrate its\neffectiveness through extensive experiments in active learning and few-shot BO.\nOur results show that HIPE outperforms standard initialization strategies in\nterms of predictive accuracy, hyperparameter identification, and subsequent\noptimization performance, particularly in large-batch, few-shot settings\nrelevant to many real-world Bayesian Optimization applications.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHIPE\u7684\u65b0\u578b\u91c7\u96c6\u7b56\u7565\uff0c\u7528\u4e8e\u5728\u5c11\u91cf\u6837\u672c\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\u5e73\u8861\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u964d\u4f4e\u548c\u8d85\u53c2\u6570\u5b66\u4e60\u3002", "motivation": "\u5728\u5c11\u91cf\u6837\u672c\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\uff0c\u521d\u59cb\u5316\u5bf9\u4ee3\u7406\u6a21\u578b\u7684\u9884\u6d4b\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u7a7a\u95f4\u586b\u5145\u8bbe\u8ba1\u53ef\u80fd\u5e76\u975e\u6700\u4f73\u4ee5\u53ca\u9ad8\u6548\u7684\u8d85\u53c2\u6570\u5b66\u4e60\u7684\u91cd\u8981\u6027\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8d85\u53c2\u6570\u77e5\u60c5\u7684\u9884\u6d4b\u63a2\u7d22\uff08HIPE\uff09\u7684\u91c7\u96c6\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5229\u7528\u4fe1\u606f\u8bba\u539f\u5219\u5e73\u8861\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u964d\u4f4e\u548c\u8d85\u53c2\u6570\u5b66\u4e60\uff0c\u5e76\u63a8\u5bfc\u4e86\u9ad8\u65af\u8fc7\u7a0b\u8bbe\u7f6e\u4e2dHIPE\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHIPE\u5728\u9884\u6d4b\u7cbe\u5ea6\u3001\u8d85\u53c2\u6570\u8bc6\u522b\u548c\u540e\u7eed\u4f18\u5316\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u6807\u51c6\u521d\u59cb\u5316\u7b56\u7565\uff0c\u5c24\u5176\u662f\u5728\u4e0e\u8bb8\u591a\u5b9e\u9645\u8d1d\u53f6\u65af\u4f18\u5316\u5e94\u7528\u76f8\u5173\u7684\u5927\u6279\u91cf\u3001\u5c11\u91cf\u6837\u672c\u8bbe\u7f6e\u4e2d\u3002", "conclusion": "HIPE\u662f\u4e00\u79cd\u6709\u6548\u7684\u521d\u59cb\u5316\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u5c11\u91cf\u6837\u672c\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u548c\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2510.24208", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24208", "abs": "https://arxiv.org/abs/2510.24208", "authors": ["Jian Gu", "Aldeida Aleti", "Chunyang Chen", "Hongyu Zhang"], "title": "Beyond Neural Incompatibility: Easing Cross-Scale Knowledge Transfer in Large Language Models through Latent Semantic Alignment", "comment": "an early-stage version", "summary": "Large Language Models (LLMs) encode vast amounts of knowledge in their\nmassive parameters, which is accessible to locate, trace, and analyze. Despite\nadvances in neural interpretability, it is still not clear how to transfer\nknowledge in a fine-grained manner, namely parametric knowledge transfer (PKT).\nA key problem is enabling effective and efficient knowledge transfer across\nLLMs of different scales, which is essential for achieving greater flexibility\nand broader applicability in transferring knowledge between LLMs. Due to neural\nincompatibility, referring to the architectural and parametric differences\nbetween LLMs of varying scales, existing methods that directly reuse layer\nparameters are severely limited. In this paper, we identify the semantic\nalignment in latent space as the fundamental prerequisite for LLM cross-scale\nknowledge transfer. Instead of directly using the layer parameters, our\napproach takes activations as the medium of layer-wise knowledge transfer.\nLeveraging the semantics in latent space, our approach is simple and\noutperforms prior work, better aligning model behaviors across varying scales.\nEvaluations on four benchmarks demonstrate the efficacy of our method. Further\nanalysis reveals the key factors easing cross-scale knowledge transfer and\nprovides insights into the nature of latent semantic alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u77e5\u8bc6\u8f6c\u79fb\uff08PKT\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e4b\u95f4\u4e0d\u540c\u5c3a\u5ea6\u77e5\u8bc6\u8f6c\u79fb\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7531\u4e8e\u795e\u7ecf\u4e0d\u517c\u5bb9\u6027\uff0c\u76f4\u63a5\u91cd\u7528\u5c42\u53c2\u6570\u53d7\u5230\u4e25\u91cd\u9650\u5236\u3002\u672c\u6587\u65e8\u5728\u5b9e\u73b0LLM\u4e4b\u95f4\u66f4\u7075\u6d3b\u3001\u66f4\u5e7f\u6cdb\u7684\u77e5\u8bc6\u8f6c\u79fb\u3002", "method": "\u8be5\u65b9\u6cd5\u4ee5\u6fc0\u6d3b\u4f5c\u4e3a\u5c42\u95f4\u77e5\u8bc6\u8f6c\u79fb\u7684\u5a92\u4ecb\uff0c\u5229\u7528\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u4fc3\u8fdb\u8de8\u5c3a\u5ea6\u77e5\u8bc6\u8f6c\u79fb\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u6f5c\u5728\u8bed\u4e49\u5bf9\u9f50\u672c\u8d28\u7684\u89c1\u89e3\u3002"}}
{"id": "2510.24342", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24342", "abs": "https://arxiv.org/abs/2510.24342", "authors": ["Silin Chen", "Yuzhong Chen", "Zifan Wang", "Junhao Wang", "Zifeng Jia", "Keith M Kendrick", "Tuo Zhang", "Lin Zhao", "Dezhong Yao", "Tianming Liu", "Xi Jiang"], "title": "A Unified Geometric Space Bridging AI Models and the Human Brain", "comment": null, "summary": "For decades, neuroscientists and computer scientists have pursued a shared\nambition: to understand intelligence and build it. Modern artificial neural\nnetworks now rival humans in language, perception, and reasoning, yet it is\nstill largely unknown whether these artificial systems organize information as\nthe brain does. Existing brain-AI alignment studies have shown the striking\ncorrespondence between the two systems, but such comparisons remain bound to\nspecific inputs and tasks, offering no common ground for comparing how AI\nmodels with different kinds of modalities-vision, language, or multimodal-are\nintrinsically organized. Here we introduce a groundbreaking concept of\nBrain-like Space: a unified geometric space in which every AI model can be\nprecisely situated and compared by mapping its intrinsic spatial attention\ntopological organization onto canonical human functional brain networks,\nregardless of input modality, task, or sensory domain. Our extensive analysis\nof 151 Transformer-based models spanning state-of-the-art large vision models,\nlarge language models, and large multimodal models uncovers a continuous\narc-shaped geometry within this space, reflecting a gradual increase of\nbrain-likeness; different models exhibit distinct distribution patterns within\nthis geometry associated with different degrees of brain-likeness, shaped not\nmerely by their modality but by whether the pretraining paradigm emphasizes\nglobal semantic abstraction and whether the positional encoding scheme\nfacilitates deep fusion across different modalities. Moreover, the degree of\nbrain-likeness for a model and its downstream task performance are not\n\"identical twins\". The Brain-like Space provides the first unified framework\nfor situating, quantifying, and comparing intelligence across domains,\nrevealing the deep organizational principles that bridge machines and the\nbrain.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u7c7b\u8111\u7a7a\u95f4\u201d\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u6bd4\u8f83\u4e0d\u540c\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u7684\u5185\u5728\u7ec4\u7ec7\u65b9\u5f0f\u4e0e\u4eba\u8111\u7684\u76f8\u4f3c\u5ea6\u3002", "motivation": "\u65e8\u5728\u4e86\u89e3\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u662f\u5426\u50cf\u5927\u8111\u4e00\u6837\u7ec4\u7ec7\u4fe1\u606f\uff0c\u5e76\u6253\u7834\u73b0\u6709\u7814\u7a76\u5728\u8f93\u5165\u548c\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u4ece\u800c\u6bd4\u8f83\u4e0d\u540c\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u7684\u5185\u5728\u7ec4\u7ec7\u3002", "method": "\u901a\u8fc7\u5c06\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u7684\u5185\u5728\u7a7a\u95f4\u6ce8\u610f\u529b\u62d3\u6251\u7ec4\u7ec7\u6620\u5c04\u5230\u5178\u578b\u7684\u4eba\u7c7b\u529f\u80fd\u8111\u7f51\u7edc\uff0c\u4ece\u800c\u5c06\u6bcf\u4e2a\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u7cbe\u786e\u5b9a\u4f4d\u5e76\u6bd4\u8f83\u3002", "result": "\u63ed\u793a\u4e86\u4e00\u4e2a\u8fde\u7eed\u7684\u5f27\u5f62\u51e0\u4f55\u7ed3\u6784\uff0c\u53cd\u6620\u4e86\u7c7b\u8111\u6027\u7684\u9010\u6e10\u589e\u52a0\uff1b\u4e0d\u540c\u7684\u6a21\u578b\u5728\u8fd9\u4e2a\u51e0\u4f55\u7ed3\u6784\u4e2d\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u5206\u5e03\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u6a21\u5f0f\u4e0e\u4e0d\u540c\u7684\u7c7b\u8111\u7a0b\u5ea6\u76f8\u5173\uff0c\u5e76\u4e14\u53d7\u5230\u9884\u8bad\u7ec3\u8303\u5f0f\u548c\u4f4d\u7f6e\u7f16\u7801\u65b9\u6848\u7684\u5f71\u54cd\u3002\u6a21\u578b\u7684\u5927\u8111\u76f8\u4f3c\u5ea6\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u5e76\u975e\u5b8c\u5168\u4e00\u81f4\u3002", "conclusion": "\u201c\u7c7b\u8111\u7a7a\u95f4\u201d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8de8\u9886\u57df\u5b9a\u4f4d\u3001\u91cf\u5316\u548c\u6bd4\u8f83\u667a\u80fd\uff0c\u63ed\u793a\u4e86\u8fde\u63a5\u673a\u5668\u548c\u5927\u8111\u7684\u6df1\u5c42\u7ec4\u7ec7\u539f\u5219\u3002"}}
{"id": "2510.24195", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24195", "abs": "https://arxiv.org/abs/2510.24195", "authors": ["Ziqi Zhou", "Yifan Hu", "Yufei Song", "Zijing Li", "Shengshan Hu", "Leo Yu Zhang", "Dezhong Yao", "Long Zheng", "Hai Jin"], "title": "Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2", "comment": "Accepted by NeurIPS 2025", "summary": "Recent studies reveal the vulnerability of the image segmentation foundation\nmodel SAM to adversarial examples. Its successor, SAM2, has attracted\nsignificant attention due to its strong generalization capability in video\nsegmentation. However, its robustness remains unexplored, and it is unclear\nwhether existing attacks on SAM can be directly transferred to SAM2. In this\npaper, we first analyze the performance gap of existing attacks between SAM and\nSAM2 and highlight two key challenges arising from their architectural\ndifferences: directional guidance from the prompt and semantic entanglement\nacross consecutive frames. To address these issues, we propose UAP-SAM2, the\nfirst cross-prompt universal adversarial attack against SAM2 driven by dual\nsemantic deviation. For cross-prompt transferability, we begin by designing a\ntarget-scanning strategy that divides each frame into k regions, each randomly\nassigned a prompt, to reduce prompt dependency during optimization. For\neffectiveness, we design a dual semantic deviation framework that optimizes a\nUAP by distorting the semantics within the current frame and disrupting the\nsemantic consistency across consecutive frames. Extensive experiments on six\ndatasets across two segmentation tasks demonstrate the effectiveness of the\nproposed method for SAM2. The comparative results show that UAP-SAM2\nsignificantly outperforms state-of-the-art (SOTA) attacks by a large margin.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u89c6\u9891\u5206\u5272\u57fa\u7840\u6a21\u578bSAM2\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u9488\u5bf9SAM\u7684\u653b\u51fb\u4e0d\u80fd\u76f4\u63a5\u8f6c\u79fb\u5230SAM2\u4e0a\u3002\u63d0\u51fa\u4e86UAP-SAM2\uff0c\u4e00\u79cd\u57fa\u4e8e\u53cc\u91cd\u8bed\u4e49\u504f\u5dee\u7684\u8de8 prompt \u901a\u7528\u5bf9\u6297\u653b\u51fb\uff0c\u901a\u8fc7\u626d\u66f2\u5f53\u524d\u5e27\u5185\u7684\u8bed\u4e49\u5e76\u7834\u574f\u8fde\u7eed\u5e27\u4e4b\u95f4\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u6765\u4f18\u5316 UAP\u3002", "motivation": "\u7814\u7a76SAM2\u5728\u9762\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u65f6\u7684\u8106\u5f31\u6027\uff0c\u5e76\u5206\u6790\u73b0\u6709SAM\u653b\u51fb\u65b9\u6cd5\u5728SAM2\u4e0a\u7684\u6027\u80fd\u5dee\u8ddd\u3002\u5f3a\u8c03\u4e86SAM2\u7684\u67b6\u6784\u5dee\u5f02\u5e26\u6765\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u6765\u81eaprompt\u7684\u65b9\u5411\u5f15\u5bfc\u548c\u8fde\u7eed\u5e27\u4e4b\u95f4\u7684\u8bed\u4e49\u7ea0\u7f20\u3002", "method": "\u63d0\u51fa\u4e86UAP-SAM2\uff0c\u4e00\u79cd\u8de8 prompt \u901a\u7528\u5bf9\u6297\u653b\u51fb\uff0c\u901a\u8fc7\u76ee\u6807\u626b\u63cf\u7b56\u7565\u51cf\u5c11 prompt \u4f9d\u8d56\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53cc\u91cd\u8bed\u4e49\u504f\u5dee\u6846\u67b6\u6765\u4f18\u5316 UAP\uff0c\u8be5\u6846\u67b6\u626d\u66f2\u5f53\u524d\u5e27\u5185\u7684\u8bed\u4e49\u5e76\u7834\u574f\u8fde\u7eed\u5e27\u4e4b\u95f4\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5bf9SAM2\u6709\u6548\uff0c\u5e76\u4e14\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5730\u63d0\u51fa\u4e86\u9488\u5bf9SAM2\u7684\u901a\u7528\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5UAP-SAM2\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u8868\u660eSAM2\u5728\u5bf9\u6297\u6027\u653b\u51fb\u4e0b\u5b58\u5728\u8106\u5f31\u6027\u3002"}}
{"id": "2510.23682", "categories": ["cs.LG", "cs.AI", "cs.LO", "cs.SE", "I.2.11; I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2510.23682", "abs": "https://arxiv.org/abs/2510.23682", "authors": ["Gokturk Aytug Akarlar"], "title": "Beyond Prompt Engineering: Neuro-Symbolic-Causal Architecture for Robust Multi-Objective AI Agents", "comment": "35 pages, 15 figures, 2 tables. Keywords: Large Language Models,\n  Autonomous Agents, Neuro-Symbolic AI, Causal Inference, Formal Verification,\n  Multi-Objective Optimization. Open-source code and interactive demo available", "summary": "Large language models show promise as autonomous decision-making agents, yet\ntheir deployment in high-stakes domains remains fraught with risk. Without\narchitectural safeguards, LLM agents exhibit catastrophic brittleness:\nidentical capabilities produce wildly different outcomes depending solely on\nprompt framing. We present Chimera, a neuro-symbolic-causal architecture that\nintegrates three complementary components - an LLM strategist, a formally\nverified symbolic constraint engine, and a causal inference module for\ncounterfactual reasoning. We benchmark Chimera against baseline architectures\n(LLM-only, LLM with symbolic constraints) across 52-week simulations in a\nrealistic e-commerce environment featuring price elasticity, trust dynamics,\nand seasonal demand. Under organizational biases toward either volume or margin\noptimization, LLM-only agents fail catastrophically (total loss of \\$99K in\nvolume scenarios) or destroy brand trust (-48.6% in margin scenarios). Adding\nsymbolic constraints prevents disasters but achieves only 43-87% of Chimera's\nprofit. Chimera consistently delivers the highest returns (\\$1.52M and \\$1.96M\nrespectively, some cases +\\$2.2M) while improving brand trust (+1.8% and\n+10.8%, some cases +20.86%), demonstrating prompt-agnostic robustness. Our TLA+\nformal verification proves zero constraint violations across all scenarios.\nThese results establish that architectural design not prompt engineering\ndetermines the reliability of autonomous agents in production environments. We\nprovide open-source implementations and interactive demonstrations for\nreproducibility.", "AI": {"tldr": "LLM agents are risky in important situations. They can fail based on prompt wording. The authors made a new architecture, Chimera, that combines an LLM, symbolic constraints, and causal inference.", "motivation": "LLM agents can be unreliable in important situations.", "method": "The authors created Chimera, a neuro-symbolic-causal architecture that integrates three components: an LLM strategist, a formally verified symbolic constraint engine, and a causal inference module for counterfactual reasoning. They compared Chimera against baseline architectures (LLM-only, LLM with symbolic constraints) across 52-week simulations in a realistic e-commerce environment.", "result": "Chimera consistently delivers the highest returns and improves brand trust, demonstrating prompt-agnostic robustness. TLA+ formal verification proves zero constraint violations across all scenarios.", "conclusion": "Architectural design, not prompt engineering, determines the reliability of autonomous agents in production environments."}}
{"id": "2510.24222", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.24222", "abs": "https://arxiv.org/abs/2510.24222", "authors": ["Adi Simhi", "Jonathan Herzig", "Itay Itzhak", "Dana Arad", "Zorik Gekhman", "Roi Reichart", "Fazl Barez", "Gabriel Stanovsky", "Idan Szpektor", "Yonatan Belinkov"], "title": "HACK: Hallucinations Along Certainty and Knowledge Axes", "comment": "The code is available at\n  https://github.com/technion-cs-nlp/HACK_Hallucinations_Along_Certainty_and_Knowledge_axes", "summary": "Hallucinations in LLMs present a critical barrier to their reliable usage.\nExisting research usually categorizes hallucination by their external\nproperties rather than by the LLMs' underlying internal properties. This\nexternal focus overlooks that hallucinations may require tailored mitigation\nstrategies based on their underlying mechanism. We propose a framework for\ncategorizing hallucinations along two axes: knowledge and certainty. Since\nparametric knowledge and certainty may vary across models, our categorization\nmethod involves a model-specific dataset construction process that\ndifferentiates between those types of hallucinations. Along the knowledge axis,\nwe distinguish between hallucinations caused by a lack of knowledge and those\noccurring despite the model having the knowledge of the correct response. To\nvalidate our framework along the knowledge axis, we apply steering mitigation,\nwhich relies on the existence of parametric knowledge to manipulate model\nactivations. This addresses the lack of existing methods to validate knowledge\ncategorization by showing a significant difference between the two\nhallucination types. We further analyze the distinct knowledge and\nhallucination patterns between models, showing that different hallucinations do\noccur despite shared parametric knowledge. Turning to the certainty axis, we\nidentify a particularly concerning subset of hallucinations where models\nhallucinate with certainty despite having the correct knowledge internally. We\nintroduce a new evaluation metric to measure the effectiveness of mitigation\nmethods on this subset, revealing that while some methods perform well on\naverage, they fail disproportionately on these critical cases. Our findings\nhighlight the importance of considering both knowledge and certainty in\nhallucination analysis and call for targeted mitigation approaches that\nconsider the hallucination underlying factors.", "AI": {"tldr": "LLM\u4e2d\u7684\u5e7b\u89c9\u662f\u53ef\u9760\u5e94\u7528\u7684\u5173\u952e\u969c\u788d\u3002\u73b0\u6709\u7684\u7814\u7a76\u901a\u5e38\u6839\u636eLLM\u7684\u5916\u90e8\u5c5e\u6027\u800c\u4e0d\u662f\u6f5c\u5728\u7684\u5185\u90e8\u5c5e\u6027\u5bf9\u5e7b\u89c9\u8fdb\u884c\u5206\u7c7b\u3002\u8fd9\u79cd\u5916\u90e8\u5173\u6ce8\u5ffd\u7565\u4e86\u5e7b\u89c9\u53ef\u80fd\u9700\u8981\u57fa\u4e8e\u5176\u5e95\u5c42\u673a\u5236\u7684\u5b9a\u5236\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u901a\u5e38\u6839\u636eLLM\u7684\u5916\u90e8\u5c5e\u6027\u800c\u4e0d\u662f\u6f5c\u5728\u7684\u5185\u90e8\u5c5e\u6027\u5bf9\u5e7b\u89c9\u8fdb\u884c\u5206\u7c7b\u3002\u8fd9\u79cd\u5916\u90e8\u5173\u6ce8\u5ffd\u7565\u4e86\u5e7b\u89c9\u53ef\u80fd\u9700\u8981\u57fa\u4e8e\u5176\u5e95\u5c42\u673a\u5236\u7684\u5b9a\u5236\u7f13\u89e3\u7b56\u7565\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u6cbf\u7740\u77e5\u8bc6\u548c\u786e\u5b9a\u6027\u4e24\u4e2a\u8f74\u5bf9\u5e7b\u89c9\u8fdb\u884c\u5206\u7c7b\u3002\u7531\u4e8e\u53c2\u6570\u77e5\u8bc6\u548c\u786e\u5b9a\u6027\u53ef\u80fd\u56e0\u6a21\u578b\u800c\u5f02\uff0c\u6211\u4eec\u7684\u5206\u7c7b\u65b9\u6cd5\u6d89\u53ca\u4e00\u4e2a\u7279\u5b9a\u4e8e\u6a21\u578b\u7684\u6570\u636e\u96c6\u6784\u5efa\u8fc7\u7a0b\uff0c\u8be5\u8fc7\u7a0b\u533a\u5206\u8fd9\u4e9b\u7c7b\u578b\u7684\u5e7b\u89c9\u3002\u6cbf\u7740\u77e5\u8bc6\u8f74\uff0c\u6211\u4eec\u533a\u5206\u4e86\u7531\u7f3a\u4e4f\u77e5\u8bc6\u5f15\u8d77\u7684\u5e7b\u89c9\u548c\u5c3d\u7ba1\u6a21\u578b\u5177\u6709\u6b63\u786e\u54cd\u5e94\u7684\u77e5\u8bc6\u800c\u53d1\u751f\u7684\u5e7b\u89c9\u3002", "result": "\u4e3a\u4e86\u9a8c\u8bc1\u6211\u4eec\u5728\u77e5\u8bc6\u8f74\u4e0a\u7684\u6846\u67b6\uff0c\u6211\u4eec\u5e94\u7528\u4e86\u8f6c\u5411\u7f13\u89e3\uff0c\u8fd9\u4f9d\u8d56\u4e8e\u53c2\u6570\u77e5\u8bc6\u7684\u5b58\u5728\u6765\u64cd\u7eb5\u6a21\u578b\u6fc0\u6d3b\u3002\u8fd9\u89e3\u51b3\u4e86\u7f3a\u4e4f\u73b0\u6709\u65b9\u6cd5\u6765\u9a8c\u8bc1\u77e5\u8bc6\u5206\u7c7b\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u663e\u793a\u4e24\u79cd\u5e7b\u89c9\u7c7b\u578b\u4e4b\u95f4\u7684\u663e\u7740\u5dee\u5f02\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5206\u6790\u4e86\u6a21\u578b\u4e4b\u95f4\u4e0d\u540c\u7684\u77e5\u8bc6\u548c\u5e7b\u89c9\u6a21\u5f0f\uff0c\u8868\u660e\u5c3d\u7ba1\u5171\u4eab\u53c2\u6570\u77e5\u8bc6\uff0c\u4f46\u786e\u5b9e\u53d1\u751f\u4e86\u4e0d\u540c\u7684\u5e7b\u89c9\u3002\u8f6c\u5411\u786e\u5b9a\u6027\u8f74\uff0c\u6211\u4eec\u53d1\u73b0\u4e86\u4e00\u4e2a\u7279\u522b\u4ee4\u4eba\u62c5\u5fe7\u7684\u5e7b\u89c9\u5b50\u96c6\uff0c\u5176\u4e2d\u6a21\u578b\u5728\u5185\u90e8\u62e5\u6709\u6b63\u786e\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u4ee5\u786e\u5b9a\u6027\u8fdb\u884c\u5e7b\u89c9\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u6765\u8861\u91cf\u7f13\u89e3\u65b9\u6cd5\u5728\u8fd9\u4e2a\u5b50\u96c6\u4e0a\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u67d0\u4e9b\u65b9\u6cd5\u5e73\u5747\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b83\u4eec\u5728\u8fd9\u4e9b\u5173\u952e\u60c5\u51b5\u4e0b\u5374\u4e0d\u6210\u6bd4\u4f8b\u5730\u5931\u8d25\u4e86\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728\u5e7b\u89c9\u5206\u6790\u4e2d\u8003\u8651\u77e5\u8bc6\u548c\u786e\u5b9a\u6027\u7684\u91cd\u8981\u6027\uff0c\u5e76\u547c\u5401\u91c7\u53d6\u6709\u9488\u5bf9\u6027\u7684\u7f13\u89e3\u65b9\u6cd5\uff0c\u8003\u8651\u5e7b\u89c9\u7684\u6f5c\u5728\u56e0\u7d20\u3002"}}
{"id": "2510.24359", "categories": ["cs.AI", "cs.SY", "eess.SY", "q-bio.QM", "stat.AP"], "pdf": "https://arxiv.org/pdf/2510.24359", "abs": "https://arxiv.org/abs/2510.24359", "authors": ["Pedram Fard", "Alaleh Azhir", "Neguine Rezaii", "Jiazi Tian", "Hossein Estiri"], "title": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine", "comment": "This study has been supported by grants from the National Institutes\n  of Health: The National Institute on Aging R01AG074372 and The National\n  Institute of Allergy and Infectious Diseases R01AI165535", "summary": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual.", "AI": {"tldr": "\u4f20\u7edf\u7684\u533b\u7597\u4eba\u5de5\u667a\u80fd\u4ee5\u670d\u52a1\u666e\u901a\u60a3\u8005\u4e3a\u76ee\u6807\uff0c\u4f46\u5728\u8fb9\u7f18\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9020\u6210\u516c\u5e73\u6027\u548c\u4fe1\u4efb\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\uff0c\u7528\u4e8eN-of-1\u51b3\u7b56\u652f\u6301\uff0c\u901a\u8fc7\u6743\u8861\u53ef\u9760\u6027\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u6570\u636e\u5bc6\u5ea6\uff0c\u4e3a\u4e34\u5e8a\u533b\u751f\u63d0\u4f9b\u51b3\u7b56\u652f\u6301\u5305\u3002\u9a8c\u8bc1\u4ece\u4eba\u7fa4\u5e73\u5747\u8f6c\u79fb\u5230\u4e2a\u4f53\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u4f4e\u5bc6\u5ea6\u533a\u57df\u7684\u8bef\u5dee\u3001\u5c0f\u8303\u56f4\u6821\u51c6\u548c\u98ce\u9669\u8986\u76d6\u6743\u8861\u6765\u8861\u91cf\u3002\u9762\u4e34\u7684\u6311\u6218\u5305\u62ec\u8ba1\u7b97\u9700\u6c42\u3001\u81ea\u52a8\u5316\u504f\u5dee\u548c\u76d1\u7ba1\u9002\u5e94\u6027\uff0c\u901a\u8fc7\u7f13\u5b58\u7b56\u7565\u3001\u5171\u8bc6\u68c0\u67e5\u548c\u81ea\u9002\u5e94\u8bd5\u9a8c\u6846\u67b6\u6765\u89e3\u51b3\u3002\u8be5\u65b9\u6cd5\u65e8\u5728\u4f7f\u533b\u7597\u4eba\u5de5\u667a\u80fd\u4e0e\u533b\u5b66\u7684\u9996\u8981\u539f\u5219\u4fdd\u6301\u4e00\u81f4\uff1a\u900f\u660e\u3001\u516c\u5e73\u548c\u4ee5\u4e2a\u4eba\u4e3a\u4e2d\u5fc3\u7684\u62a4\u7406\u3002", "motivation": "\u4f20\u7edf\u533b\u7597\u4eba\u5de5\u667a\u80fd\u5728\u5904\u7406\u7f55\u89c1\u53d8\u5f02\u3001\u591a\u75c5\u6216\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u4eba\u7fa4\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u635f\u5bb3\u4e86\u516c\u5e73\u6027\u548c\u4fe1\u4efb\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\uff0c\u5176\u4e2d\u6309\u5668\u5b98\u7cfb\u7edf\u3001\u60a3\u8005\u7fa4\u4f53\u548c\u5206\u6790\u6a21\u5f0f\u805a\u96c6\u7684\u667a\u80fd\u4f53\u5229\u7528\u5171\u4eab\u7684\u6a21\u578b\u548c\u8bc1\u636e\u5408\u6210\u5de5\u5177\uff0c\u7ed3\u679c\u6c47\u805a\u5728\u534f\u8c03\u5c42\uff0c\u8be5\u534f\u8c03\u5c42\u6743\u8861\u53ef\u9760\u6027\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u6570\u636e\u5bc6\u5ea6\uff0c\u7136\u540e\u5411\u4e34\u5e8a\u533b\u751f\u63d0\u4f9b\u51b3\u7b56\u652f\u6301\u5305\u3002", "result": "\u9a8c\u8bc1\u4ece\u4eba\u7fa4\u5e73\u5747\u8f6c\u79fb\u5230\u4e2a\u4f53\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u4f4e\u5bc6\u5ea6\u533a\u57df\u7684\u8bef\u5dee\u3001\u5c0f\u8303\u56f4\u6821\u51c6\u548c\u98ce\u9669\u8986\u76d6\u6743\u8861\u6765\u8861\u91cf\u3002", "conclusion": "\u901a\u8fc7\u4ece\u5355\u4e00\u6a21\u578b\u8f6c\u5411\u534f\u8c03\u667a\u80fd\uff0c\u8be5\u65b9\u6cd5\u65e8\u5728\u4f7f\u533b\u7597\u4eba\u5de5\u667a\u80fd\u4e0e\u533b\u5b66\u7684\u9996\u8981\u539f\u5219\u4fdd\u6301\u4e00\u81f4\uff1a\u900f\u660e\u3001\u516c\u5e73\u548c\u4ee5\u4e2a\u4eba\u4e3a\u4e2d\u5fc3\u7684\u62a4\u7406\u3002"}}
