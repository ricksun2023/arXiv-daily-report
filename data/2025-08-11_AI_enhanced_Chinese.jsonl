{"id": "2508.06077", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2508.06077", "abs": "https://arxiv.org/abs/2508.06077", "authors": ["Hongqin Lei", "Haowei Tang", "Zhe Zhang"], "title": "A Cross-Perspective Annotated Dataset for Dynamic Object-Level Attention Modeling in Cloud Gaming", "comment": null, "summary": "Cloud gaming has gained popularity as it provides high-quality gaming\nexperiences on thin hardware, such as phones and tablets. Transmitting gameplay\nframes at high resolutions and ultra-low latency is the key to guaranteeing\nplayers' quality of experience (QoE). Numerous studies have explored deep\nlearning (DL) techniques to address this challenge. The efficiency of these\nDL-based approaches is highly affected by the dataset. However, existing\ndatasets usually focus on the positions of objects while ignoring semantic\nrelationships with other objects and their unique features. In this paper, we\npresent a game dataset by collecting gameplay clips from Grand Theft Auto (GTA)\nV, and annotating the player's interested objects during the gameplay. Based on\nthe collected data, we analyze several factors that have an impact on player's\ninterest and identify that the player's in-game speed, object's size, and\nobject's speed are the main factors. The dataset is available at\nhttps://drive.google.com/drive/folders/1idH251a2K-hGGd3pKjX-3Gx5o_rUqLC4?usp=sharing", "AI": {"tldr": "A new game dataset from GTA V is presented, focusing on semantic relationships of objects. Player's in-game speed, object's size, and object's speed are the main factors impacting player's interest.", "motivation": "Existing datasets usually focus on the positions of objects while ignoring semantic relationships with other objects and their unique features.", "method": "Collecting gameplay clips from Grand Theft Auto (GTA) V, and annotating the player's interested objects during the gameplay.", "result": "We present a game dataset by collecting gameplay clips from Grand Theft Auto (GTA) V, and annotating the player's interested objects during the gameplay.", "conclusion": "The player's in-game speed, object's size, and object's speed are the main factors impacting player's interest."}}
{"id": "2508.05640", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05640", "abs": "https://arxiv.org/abs/2508.05640", "authors": ["Liang Guo", "Wei Li", "Lucy Liao", "Huihui Cheng", "Rui Zhang", "Yu Shi", "Yueming Wang", "Yanzun Huang", "Keke Zhai", "Pengchao Wang", "Timothy Shi", "Xuan Cao", "Shengzhi Wang", "Renqin Cai", "Zhaojie Gong", "Omkar Vichare", "Rui Jian", "Leon Gao", "Shiyan Deng", "Xingyu Liu", "Xiong Zhang", "Fu Li", "Wenlei Xie", "Bin Wen", "Rui Li", "Xing Liu", "Jiaqi Zhai"], "title": "Request-Only Optimization for Recommendation Systems", "comment": null, "summary": "Deep Learning Recommendation Models (DLRMs) represent one of the largest\nmachine learning applications on the planet. Industry-scale DLRMs are trained\nwith petabytes of recommendation data to serve billions of users every day. To\nutilize the rich user signals in the long user history, DLRMs have been scaled\nup to unprecedented complexity, up to trillions of floating-point operations\n(TFLOPs) per example. This scale, coupled with the huge amount of training\ndata, necessitates new storage and training algorithms to efficiently improve\nthe quality of these complex recommendation systems. In this paper, we present\na Request-Only Optimizations (ROO) training and modeling paradigm. ROO\nsimultaneously improves the storage and training efficiency as well as the\nmodel quality of recommendation systems. We holistically approach this\nchallenge through co-designing data (i.e., request-only data), infrastructure\n(i.e., request-only based data processing pipeline), and model architecture\n(i.e., request-only neural architectures). Our ROO training and modeling\nparadigm treats a user request as a unit of the training data. Compared with\nthe established practice of treating a user impression as a unit, our new\ndesign achieves native feature deduplication in data logging, consequently\nsaving data storage. Second, by de-duplicating computations and communications\nacross multiple impressions in a request, this new paradigm enables highly\nscaled-up neural network architectures to better capture user interest signals,\nsuch as Generative Recommenders (GRs) and other request-only friendly\narchitectures.", "AI": {"tldr": "The paper introduces ROO, a new training paradigm for DLRMs that improves storage, training efficiency, and model quality by treating a user request as the unit of training data.", "motivation": "Industry-scale DLRMs require new storage and training algorithms to efficiently improve the quality of complex recommendation systems due to their scale and the huge amount of training data.", "method": "The paper co-designs data (request-only data), infrastructure (request-only based data processing pipeline), and model architecture (request-only neural architectures).", "result": "The ROO training and modeling paradigm achieves native feature deduplication in data logging, saving data storage. It also enables highly scaled-up neural network architectures to better capture user interest signals.", "conclusion": "The paper presents a Request-Only Optimizations (ROO) training and modeling paradigm that improves storage, training efficiency, and model quality."}}
{"id": "2508.05722", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05722", "abs": "https://arxiv.org/abs/2508.05722", "authors": ["Rania Al-Sabbagh"], "title": "PEACH: A sentence-aligned Parallel English-Arabic Corpus for Healthcare", "comment": null, "summary": "This paper introduces PEACH, a sentence-aligned parallel English-Arabic\ncorpus of healthcare texts encompassing patient information leaflets and\neducational materials. The corpus contains 51,671 parallel sentences, totaling\napproximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths\nvary between 9.52 and 11.83 words on average. As a manually aligned corpus,\nPEACH is a gold-standard corpus, aiding researchers in contrastive linguistics,\ntranslation studies, and natural language processing. It can be used to derive\nbilingual lexicons, adapt large language models for domain-specific machine\ntranslation, evaluate user perceptions of machine translation in healthcare,\nassess patient information leaflets and educational materials' readability and\nlay-friendliness, and as an educational resource in translation studies. PEACH\nis publicly accessible.", "AI": {"tldr": "PEACH: A new, publicly available English-Arabic healthcare corpus for NLP and translation research.", "motivation": "To aid researchers in contrastive linguistics, translation studies, and natural language processing.", "method": "Manually aligned corpus", "result": "The corpus contains 51,671 parallel sentences, totaling approximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths vary between 9.52 and 11.83 words on average.", "conclusion": "PEACH is a publicly accessible, gold-standard English-Arabic parallel corpus of healthcare texts."}}
{"id": "2508.05689", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05689", "abs": "https://arxiv.org/abs/2508.05689", "authors": ["Jinjia Peng", "Zeze Tao", "Huibing Wang", "Meng Wang", "Yang Wang"], "title": "Boosting Adversarial Transferability via Residual Perturbation Attack", "comment": "Accepted to ieee/cvf international conference on computer vision\n  (ICCV2025)", "summary": "Deep neural networks are susceptible to adversarial examples while suffering\nfrom incorrect predictions via imperceptible perturbations. Transfer-based\nattacks create adversarial examples for surrogate models and transfer these\nexamples to target models under black-box scenarios. Recent studies reveal that\nadversarial examples in flat loss landscapes exhibit superior transferability\nto alleviate overfitting on surrogate models. However, the prior arts overlook\nthe influence of perturbation directions, resulting in limited transferability.\nIn this paper, we propose a novel attack method, named Residual Perturbation\nAttack (ResPA), relying on the residual gradient as the perturbation direction\nto guide the adversarial examples toward the flat regions of the loss function.\nSpecifically, ResPA conducts an exponential moving average on the input\ngradients to obtain the first moment as the reference gradient, which\nencompasses the direction of historical gradients. Instead of heavily relying\non the local flatness that stems from the current gradients as the perturbation\ndirection, ResPA further considers the residual between the current gradient\nand the reference gradient to capture the changes in the global perturbation\ndirection. The experimental results demonstrate the better transferability of\nResPA than the existing typical transfer-based attack methods, while the\ntransferability can be further improved by combining ResPA with the current\ninput transformation methods. The code is available at\nhttps://github.com/ZezeTao/ResPA.", "AI": {"tldr": "This paper proposes ResPA, a novel transfer-based attack method that uses the residual gradient as the perturbation direction to improve the transferability of adversarial examples in black-box attacks.", "motivation": "Deep neural networks are susceptible to adversarial examples, and transfer-based attacks create adversarial examples for surrogate models and transfer these examples to target models under black-box scenarios. Recent studies reveal that adversarial examples in flat loss landscapes exhibit superior transferability to alleviate overfitting on surrogate models. However, the prior arts overlook the influence of perturbation directions, resulting in limited transferability.", "method": "The paper proposes a novel attack method, named Residual Perturbation Attack (ResPA), relying on the residual gradient as the perturbation direction to guide the adversarial examples toward the flat regions of the loss function. ResPA conducts an exponential moving average on the input gradients to obtain the first moment as the reference gradient, which encompasses the direction of historical gradients. ResPA further considers the residual between the current gradient and the reference gradient to capture the changes in the global perturbation direction.", "result": "Experimental results demonstrate the better transferability of ResPA than the existing typical transfer-based attack methods, while the transferability can be further improved by combining ResPA with the current input transformation methods.", "conclusion": "ResPA achieves better transferability than existing transfer-based attack methods, and can be further improved by combining it with current input transformation methods."}}
{"id": "2508.05731", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05731", "abs": "https://arxiv.org/abs/2508.05731", "authors": ["Yuhang Liu", "Zeyu Liu", "Shuanghe Zhu", "Pengxiang Li", "Congkai Xie", "Jiasheng Wang", "Xueyu Hu", "Xiaotian Han", "Jianbo Yuan", "Xinyao Wang", "Shengyu Zhang", "Hongxia Yang", "Fei Wu"], "title": "InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization", "comment": "11 pages, 3 figures", "summary": "The emergence of Multimodal Large Language Models (MLLMs) has propelled the\ndevelopment of autonomous agents that operate on Graphical User Interfaces\n(GUIs) using pure visual input. A fundamental challenge is robustly grounding\nnatural language instructions. This requires a precise spatial alignment, which\naccurately locates the coordinates of each element, and, more critically, a\ncorrect semantic alignment, which matches the instructions to the functionally\nappropriate UI element. Although Reinforcement Learning with Verifiable Rewards\n(RLVR) has proven to be effective at improving spatial alignment for these\nMLLMs, we find that inefficient exploration bottlenecks semantic alignment,\nwhich prevent models from learning difficult semantic associations. To address\nthis exploration problem, we present Adaptive Exploration Policy Optimization\n(AEPO), a new policy optimization framework. AEPO employs a multi-answer\ngeneration strategy to enforce broader exploration, which is then guided by a\ntheoretically grounded Adaptive Exploration Reward (AER) function derived from\nfirst principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B\nand InfiGUI-G1-7B, establish new state-of-the-art results across multiple\nchallenging GUI grounding benchmarks, achieving significant relative\nimprovements of up to 9.0% against the naive RLVR baseline on benchmarks\ndesigned to test generalization and semantic understanding. Resources are\navailable at https://github.com/InfiXAI/InfiGUI-G1.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u4f18\u5316\u6846\u67b6AEPO\uff0c\u89e3\u51b3\u4e86MLLM\u5728GUI\u4e0a\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7406\u89e3\u65f6\uff0c\u7531\u4e8e\u63a2\u7d22\u4e0d\u8db3\u5bfc\u81f4\u7684\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u5f0f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u51fa\u73b0\u63a8\u52a8\u4e86\u4f7f\u7528\u7eaf\u89c6\u89c9\u8f93\u5165\u5728\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u4e0a\u8fd0\u884c\u7684\u81ea\u4e3b\u4ee3\u7406\u7684\u5f00\u53d1\u3002\u4e00\u4e2a\u6839\u672c\u7684\u6311\u6218\u662f\u7a33\u5065\u5730\u5efa\u7acb\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u57fa\u7840\u3002\u8fd9\u9700\u8981\u7cbe\u786e\u7684\u7a7a\u95f4\u5bf9\u9f50\uff0c\u8be5\u5bf9\u9f50\u53ef\u4ee5\u51c6\u786e\u5b9a\u4f4d\u6bcf\u4e2a\u5143\u7d20\u7684\u5750\u6807\uff0c\u66f4\u5173\u952e\u7684\u662f\uff0c\u6b63\u786e\u7684\u8bed\u4e49\u5bf9\u9f50\uff0c\u8be5\u5bf9\u9f50\u5c06\u6307\u4ee4\u4e0e\u529f\u80fd\u4e0a\u5408\u9002\u7684UI\u5143\u7d20\u5339\u914d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u4f18\u5316\u6846\u67b6\u81ea\u9002\u5e94\u63a2\u7d22\u7b56\u7565\u4f18\u5316\uff08AEPO\uff09\u3002AEPO\u91c7\u7528\u591a\u7b54\u6848\u751f\u6210\u7b56\u7565\u6765\u5f3a\u5236\u66f4\u5e7f\u6cdb\u7684\u63a2\u7d22\uff0c\u7136\u540e\u7531\u4ece\u6548\u7387\u03b7=U/C\u7684\u7b2c\u4e00\u539f\u7406\u5bfc\u51fa\u7684\u7406\u8bba\u81ea\u9002\u5e94\u63a2\u7d22\u5956\u52b1\uff08AER\uff09\u51fd\u6570\u6765\u6307\u5bfc\u3002", "result": "AEPO\u8bad\u7ec3\u7684\u6a21\u578b, InfiGUI-G1-3B \u548c InfiGUI-G1-7B, \u5efa\u7acb\u4e86\u65b0\u7684state-of-the-art\u7ed3\u679c\u3002", "conclusion": "AEPO\u8bad\u7ec3\u7684\u6a21\u578bInfiGUI-G1-3B\u548cInfiGUI-G1-7B\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684GUI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u4e0e\u65e8\u5728\u6d4b\u8bd5\u6cdb\u5316\u548c\u8bed\u4e49\u7406\u89e3\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684naive RLVR\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe9.0\uff05\u7684\u663e\u7740\u76f8\u5bf9\u6539\u8fdb\u3002"}}
{"id": "2508.05659", "categories": ["cs.LG", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.05659", "abs": "https://arxiv.org/abs/2508.05659", "authors": ["Jeroen F. Uleman", "Loes Crielaard", "Leonie K. Elsenburg", "Guido A. Veldhuis", "Karien Stronks", "Naja Hulvej Rod", "Rick Quax", "V\u00edtor V. Vasconcelos"], "title": "Diagrams-to-Dynamics (D2D): Exploring Causal Loop Diagram Leverage Points under Uncertainty", "comment": "21 pages, 4 figures, 4 tables", "summary": "Causal loop diagrams (CLDs) are widely used in health and environmental\nresearch to represent hypothesized causal structures underlying complex\nproblems. However, as qualitative and static representations, CLDs are limited\nin their ability to support dynamic analysis and inform intervention\nstrategies. Additionally, quantitative CLD analysis methods like network\ncentrality analysis often lead to false inference. We propose\nDiagrams-to-Dynamics (D2D), a method for converting CLDs into exploratory\nsystem dynamics models (SDMs) in the absence of empirical data. With minimal\nuser input - following a protocol to label variables as stocks,\nflows/auxiliaries, or constants - D2D leverages the structural information\nalready encoded in CLDs, namely, link existence and polarity, to simulate\nhypothetical interventions and explore potential leverage points under\nuncertainty. Results suggest that D2D helps distinguish between high- and\nlow-ranked leverage points. We compare D2D to a data-driven SDM constructed\nfrom the same CLD and variable labeling. D2D showed greater consistency with\nthe data-driven model than network centrality analysis, while providing\nuncertainty estimates and guidance for future data collection. The method is\nimplemented in an open-source Python package and a web-based application to\nsupport further testing and lower the barrier to dynamic modeling for\nresearchers working with CLDs. We expect additional validation will further\nestablish the approach's utility across a broad range of cases and domains.", "AI": {"tldr": "Diagrams-to-Dynamics (D2D) is proposed to convert CLDs into exploratory system dynamics models (SDMs) in the absence of empirical data. It helps distinguish between high- and low-ranked leverage points and showed greater consistency with the data-driven model than network centrality analysis.", "motivation": "CLDs are limited in their ability to support dynamic analysis and inform intervention strategies. Additionally, quantitative CLD analysis methods like network centrality analysis often lead to false inference.", "method": "converting CLDs into exploratory system dynamics models (SDMs) in the absence of empirical data, leveraging the structural information already encoded in CLDs, namely, link existence and polarity, to simulate hypothetical interventions and explore potential leverage points under uncertainty.", "result": "D2D helps distinguish between high- and low-ranked leverage points. D2D showed greater consistency with the data-driven model than network centrality analysis, while providing uncertainty estimates and guidance for future data collection.", "conclusion": "D2D helps distinguish between high- and low-ranked leverage points and showed greater consistency with the data-driven model than network centrality analysis, while providing uncertainty estimates and guidance for future data collection."}}
{"id": "2508.05647", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05647", "abs": "https://arxiv.org/abs/2508.05647", "authors": ["Vibhor Agrawal", "Fay Wang", "Rishi Puri"], "title": "Query-Aware Graph Neural Networks for Enhanced Retrieval-Augmented Generation", "comment": null, "summary": "We present a novel graph neural network (GNN) architecture for\nretrieval-augmented generation (RAG) that leverages query-aware attention\nmechanisms and learned scoring heads to improve retrieval accuracy on complex,\nmulti-hop questions. Unlike traditional dense retrieval methods that treat\ndocuments as independent entities, our approach constructs per-episode\nknowledge graphs that capture both sequential and semantic relationships\nbetween text chunks. We introduce an Enhanced Graph Attention Network with\nquery-guided pooling that dynamically focuses on relevant parts of the graph\nbased on user queries. Experimental results demonstrate that our approach\nsignificantly outperforms standard dense retrievers on complex question\nanswering tasks, particularly for questions requiring multi-document reasoning.\nOur implementation leverages PyTorch Geometric for efficient processing of\ngraph-structured data, enabling scalable deployment in production retrieval\nsystems", "AI": {"tldr": "A new GNN-based RAG model improves retrieval accuracy on complex questions by using query-aware attention and knowledge graphs.", "motivation": "Traditional dense retrieval methods treat documents as independent entities, which limits retrieval accuracy on complex, multi-hop questions.", "method": "A novel graph neural network (GNN) architecture for RAG is introduced, featuring query-aware attention mechanisms and learned scoring heads. It constructs per-episode knowledge graphs to capture sequential and semantic relationships between text chunks and uses an Enhanced Graph Attention Network with query-guided pooling.", "result": "The proposed approach significantly outperforms standard dense retrievers on complex question answering tasks.", "conclusion": "The proposed GNN-based RAG model significantly outperforms standard dense retrievers on complex question answering tasks, especially those requiring multi-document reasoning. The implementation leverages PyTorch Geometric for efficient processing and scalable deployment."}}
{"id": "2508.05775", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.05775", "abs": "https://arxiv.org/abs/2508.05775", "authors": ["Chi Zhang", "Changjia Zhu", "Junjie Xiong", "Xiaoran Xu", "Lingyao Li", "Yao Liu", "Zhuo Lu"], "title": "Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation", "comment": null, "summary": "Large Language Models (LLMs) have revolutionized content creation across\ndigital platforms, offering unprecedented capabilities in natural language\ngeneration and understanding. These models enable beneficial applications such\nas content generation, question and answering (Q&A), programming, and code\nreasoning. Meanwhile, they also pose serious risks by inadvertently or\nintentionally producing toxic, offensive, or biased content. This dual role of\nLLMs, both as powerful tools for solving real-world problems and as potential\nsources of harmful language, presents a pressing sociotechnical challenge. In\nthis survey, we systematically review recent studies spanning unintentional\ntoxicity, adversarial jailbreaking attacks, and content moderation techniques.\nWe propose a unified taxonomy of LLM-related harms and defenses, analyze\nemerging multimodal and LLM-assisted jailbreak strategies, and assess\nmitigation efforts, including reinforcement learning with human feedback\n(RLHF), prompt engineering, and safety alignment. Our synthesis highlights the\nevolving landscape of LLM safety, identifies limitations in current evaluation\nmethodologies, and outlines future research directions to guide the development\nof robust and ethically aligned language technologies.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8ba8\u8bba\u4e86LLM\u7684\u98ce\u9669\u548c\u9632\u5fa1\uff0c\u5305\u62ec\u6bd2\u6027\u5185\u5bb9\u3001\u8d8a\u72f1\u653b\u51fb\u548c\u5185\u5bb9\u5ba1\u6838\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5f7b\u5e95\u6539\u53d8\u4e86\u8de8\u6570\u5b57\u5e73\u53f0\u7684\u5185\u5bb9\u521b\u4f5c\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u751f\u6210\u548c\u7406\u89e3\u65b9\u9762\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u80fd\u529b\u3002\u8fd9\u4e9b\u6a21\u578b\u5b9e\u73b0\u4e86\u6709\u76ca\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u5982\u5185\u5bb9\u751f\u6210\u3001\u95ee\u7b54\uff08Q&A\uff09\u3001\u7f16\u7a0b\u548c\u4ee3\u7801\u63a8\u7406\u3002\u540c\u65f6\uff0c\u5b83\u4eec\u4e5f\u5e26\u6765\u4e86\u4e25\u91cd\u7684\u98ce\u9669\uff0c\u53ef\u80fd\u65e0\u610f\u6216\u6709\u610f\u5730\u4ea7\u751f\u6709\u6bd2\u3001\u5192\u72af\u6027\u6216\u6709\u504f\u89c1\u7684\u5185\u5bb9\u3002LLM\u7684\u53cc\u91cd\u89d2\u8272\uff0c\u65e2\u662f\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u95ee\u9898\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u53c8\u662f\u6f5c\u5728\u7684\u6709\u5bb3\u8bed\u8a00\u6765\u6e90\uff0c\u63d0\u51fa\u4e86\u7d27\u8feb\u7684\u793e\u4f1a\u6280\u672f\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u5730\u56de\u987e\u4e86\u6700\u8fd1\u5173\u4e8e\u65e0\u610f\u6bd2\u6027\u3001\u5bf9\u6297\u6027\u8d8a\u72f1\u653b\u51fb\u548c\u5185\u5bb9\u5ba1\u6838\u6280\u672f\u7684\u7814\u7a76\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684LLM\u76f8\u5173\u5371\u5bb3\u548c\u9632\u5fa1\u5206\u7c7b\u6cd5\uff0c\u5206\u6790\u4e86\u65b0\u5174\u7684\u591a\u6a21\u6001\u548cLLM\u8f85\u52a9\u8d8a\u72f1\u7b56\u7565\uff0c\u5e76\u8bc4\u4f30\u4e86\u7f13\u89e3\u63aa\u65bd\uff0c\u5305\u62ec\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u3001\u63d0\u793a\u5de5\u7a0b\u548c\u5b89\u5168\u5bf9\u9f50\u3002", "result": "\u4eae\u70b9\u5305\u62ecLLM\u5b89\u5168\u6027\u7684\u53d1\u5c55\u6001\u52bf\uff0c\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8fd9\u7bc7\u7efc\u8ff0\u603b\u7ed3\u4e86LLM\u5b89\u5168\u9886\u57df\u7684\u6700\u65b0\u7814\u7a76\uff0c\u5f3a\u8c03\u4e86\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u6982\u8ff0\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u6307\u5bfc\u5f00\u53d1\u7a33\u5065\u4e14\u7b26\u5408\u4f26\u7406\u7684\u8bed\u8a00\u6280\u672f\u3002"}}
{"id": "2508.05732", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05732", "abs": "https://arxiv.org/abs/2508.05732", "authors": ["Pinxuan Li", "Bing Cao", "Changqing Zhang", "Qinghua Hu"], "title": "Generalized Few-Shot Out-of-Distribution Detection", "comment": null, "summary": "Few-shot Out-of-Distribution (OOD) detection has emerged as a critical\nresearch direction in machine learning for practical deployment. Most existing\nFew-shot OOD detection methods suffer from insufficient generalization\ncapability for the open world. Due to the few-shot learning paradigm, the OOD\ndetection ability is often overfit to the limited training data itself, thus\ndegrading the performance on generalized data and performing inconsistently\nacross different scenarios. To address this challenge, we proposed a\nGeneralized Few-shot OOD Detection (GOOD) framework, which empowers the general\nknowledge of the OOD detection model with an auxiliary General Knowledge Model\n(GKM), instead of directly learning from few-shot data. We proceed to reveal\nthe few-shot OOD detection from a generalization perspective and theoretically\nderive the Generality-Specificity balance (GS-balance) for OOD detection, which\nprovably reduces the upper bound of generalization error with a general\nknowledge model. Accordingly, we propose a Knowledge Dynamic Embedding (KDE)\nmechanism to adaptively modulate the guidance of general knowledge. KDE\ndynamically aligns the output distributions of the OOD detection model to the\ngeneral knowledge model based on the Generalized Belief (G-Belief) of GKM,\nthereby boosting the GS-balance. Experiments on real-world OOD benchmarks\ndemonstrate our superiority. Codes will be available.", "AI": {"tldr": "Proposes a Generalized Few-shot OOD Detection (GOOD) framework to improve generalization by incorporating a General Knowledge Model (GKM) and a Knowledge Dynamic Embedding (KDE) mechanism.", "motivation": "Existing few-shot OOD detection methods have insufficient generalization capability and tend to overfit to limited training data.", "method": "A Generalized Few-shot OOD Detection (GOOD) framework with a Knowledge Dynamic Embedding (KDE) mechanism.", "result": "Experiments on real-world OOD benchmarks demonstrate the superiority of the proposed approach.", "conclusion": "The proposed GOOD framework with KDE mechanism improves the generalization ability of few-shot OOD detection, achieving superior performance on real-world benchmarks."}}
{"id": "2508.05766", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2508.05766", "abs": "https://arxiv.org/abs/2508.05766", "authors": ["Bo Wen"], "title": "A Framework for Inherently Safer AGI through Language-Mediated Active Inference", "comment": null, "summary": "This paper proposes a novel framework for developing safe Artificial General\nIntelligence (AGI) by combining Active Inference principles with Large Language\nModels (LLMs). We argue that traditional approaches to AI safety, focused on\npost-hoc interpretability and reward engineering, have fundamental limitations.\nWe present an architecture where safety guarantees are integrated into the\nsystem's core design through transparent belief representations and\nhierarchical value alignment. Our framework leverages natural language as a\nmedium for representing and manipulating beliefs, enabling direct human\noversight while maintaining computational tractability. The architecture\nimplements a multi-agent system where agents self-organize according to Active\nInference principles, with preferences and safety constraints flowing through\nhierarchical Markov blankets. We outline specific mechanisms for ensuring\nsafety, including: (1) explicit separation of beliefs and preferences in\nnatural language, (2) bounded rationality through resource-aware free energy\nminimization, and (3) compositional safety through modular agent structures.\nThe paper concludes with a research agenda centered on the Abstraction and\nReasoning Corpus (ARC) benchmark, proposing experiments to validate our\nframework's safety properties. Our approach offers a path toward AGI\ndevelopment that is inherently safer, rather than retrofitted with safety\nmeasures.", "AI": {"tldr": "This paper proposes a novel framework for developing safe Artificial General Intelligence (AGI) by combining Active Inference principles with Large Language Models (LLMs).", "motivation": "We argue that traditional approaches to AI safety, focused on post-hoc interpretability and reward engineering, have fundamental limitations.", "method": "The architecture implements a multi-agent system where agents self-organize according to Active Inference principles, with preferences and safety constraints flowing through hierarchical Markov blankets.", "result": "The architecture implements a multi-agent system where agents self-organize according to Active Inference principles, with preferences and safety constraints flowing through hierarchical Markov blankets. We outline specific mechanisms for ensuring safety, including: (1) explicit separation of beliefs and preferences in natural language, (2) bounded rationality through resource-aware free energy minimization, and (3) compositional safety through modular agent structures.", "conclusion": "This paper concludes with a research agenda centered on the Abstraction and Reasoning Corpus (ARC) benchmark, proposing experiments to validate our framework's safety properties. Our approach offers a path toward AGI development that is inherently safer, rather than retrofitted with safety measures."}}
{"id": "2508.05724", "categories": ["cs.LG", "physics.data-an", "68T07, 81-08, 05C90", "I.2.6; G.2.2; I.5.1"], "pdf": "https://arxiv.org/pdf/2508.05724", "abs": "https://arxiv.org/abs/2508.05724", "authors": ["Massimiliano Romiti"], "title": "A Graph Neural Network Approach for Mapping the Conceptual Structure and Inter-Branch Connectivity of Physics", "comment": "14 pages, 9 figures", "summary": "This work introduces a novel framework for representing and analyzing\nphysical laws as a weighted knowledge graph. We constructed a database of 659\ndistinct physical equations, subjected to rigorous semantic cleaning to resolve\nnotational ambiguities, resulting in a corpus of 400 advanced physics\nequations. We developed an enhanced graph representation where both physical\nconcepts and equations are nodes, connected by weighted inter-equation bridges.\nThese weights are objectively defined using normalized metrics for variable\noverlap, physics-informed importance scores, and bibliometric data. A Graph\nAttention Network (GAT) was trained for link prediction, achieving a test AUC\nof 0.9742 +/- 0.0018 across five independent runs, significantly outperforming\nboth classical heuristics (best baseline AUC: 0.9487) and established GNN\narchitectures like GraphSAGE (AUC: 0.9504, p = 0.029). Statistical testing\nconfirmed significance of all comparisons (p < 0.05), with 2.7% improvement\nover the best baseline. Our analysis reveals three key findings: (i) The model\nautonomously rediscovers the known macroscopic structure of physics,\nidentifying strong conceptual axes between Electromagnetism and Statistical\nMechanics. (ii) It identifies central hub equations that serve as critical\nbridges between multiple physical domains. (iii) The model generates stable,\ncomputationally-derived hypotheses for cross-domain relationships, identifying\nboth known principles and suggesting novel mathematical analogies for further\ntheoretical investigation. The framework can generate hundreds of such\nhypotheses, enabling the creation of specialized datasets for targeted analysis\nof specific physics subfields. Code and data available at\nhttps://github.com/kingelanci/graphysics", "AI": {"tldr": "A novel framework using a weighted knowledge graph and GAT to represent and analyze physical laws, achieving high accuracy in link prediction and rediscovering known physics structures.", "motivation": "introducing a novel framework for representing and analyzing physical laws as a weighted knowledge graph", "method": "enhanced graph representation and a Graph Attention Network (GAT)", "result": "GAT achieved a test AUC of 0.9742 +/- 0.0018, outperforming classical heuristics and GNN architectures", "conclusion": "The model autonomously rediscovers the known macroscopic structure of physics, identifies central hub equations, and generates stable, computationally-derived hypotheses for cross-domain relationships."}}
{"id": "2508.05648", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05648", "abs": "https://arxiv.org/abs/2508.05648", "authors": ["Chandler Campbell", "Bernie Boscoe", "Tuan Do"], "title": "AquiLLM: a RAG Tool for Capturing Tacit Knowledge in Research Groups", "comment": "Accepted to US Research Software Engineer Association (US-RSE) 2025", "summary": "Research groups face persistent challenges in capturing, storing, and\nretrieving knowledge that is distributed across team members. Although\nstructured data intended for analysis and publication is often well managed,\nmuch of a group's collective knowledge remains informal, fragmented, or\nundocumented--often passed down orally through meetings, mentoring, and\nday-to-day collaboration. This includes private resources such as emails,\nmeeting notes, training materials, and ad hoc documentation. Together, these\nreflect the group's tacit knowledge--the informal, experience-based expertise\nthat underlies much of their work. Accessing this knowledge can be difficult,\nrequiring significant time and insider understanding. Retrieval-augmented\ngeneration (RAG) systems offer promising solutions by enabling users to query\nand generate responses grounded in relevant source material. However, most\ncurrent RAG-LLM systems are oriented toward public documents and overlook the\nprivacy concerns of internal research materials. We introduce AquiLLM\n(pronounced ah-quill-em), a lightweight, modular RAG system designed to meet\nthe needs of research groups. AquiLLM supports varied document types and\nconfigurable privacy settings, enabling more effective access to both formal\nand informal knowledge within scholarly groups.", "AI": {"tldr": "The paper introduces AquiLLM, a RAG system for research groups that addresses privacy concerns and supports various document types to improve access to both formal and informal knowledge.", "motivation": "Research groups face persistent challenges in capturing, storing, and retrieving knowledge that is distributed across team members, especially tacit knowledge. Current RAG-LLM systems overlook the privacy concerns of internal research materials.", "method": "The paper introduces AquiLLM, a lightweight, modular RAG system.", "result": "AquiLLM supports varied document types and configurable privacy settings, enabling more effective access to both formal and informal knowledge within scholarly groups.", "conclusion": "AquiLLM, a lightweight, modular RAG system, is introduced to meet the needs of research groups by supporting varied document types and configurable privacy settings, enabling more effective access to both formal and informal knowledge within scholarly groups."}}
{"id": "2508.05782", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05782", "abs": "https://arxiv.org/abs/2508.05782", "authors": ["Xiangyan Chen", "Yufeng Li", "Yujian Gan", "Arkaitz Zubiaga", "Matthew Purver"], "title": "FineDialFact: A benchmark for Fine-grained Dialogue Fact Verification", "comment": null, "summary": "Large Language Models (LLMs) are known to produce hallucinations - factually\nincorrect or fabricated information - which poses significant challenges for\nmany Natural Language Processing (NLP) applications, such as dialogue systems.\nAs a result, detecting hallucinations has become a critical area of research.\nCurrent approaches to hallucination detection in dialogue systems primarily\nfocus on verifying the factual consistency of generated responses. However,\nthese responses often contain a mix of accurate, inaccurate or unverifiable\nfacts, making one factual label overly simplistic and coarse-grained. In this\npaper, we introduce a benchmark, FineDialFact, for fine-grained dialogue fact\nverification, which involves verifying atomic facts extracted from dialogue\nresponses. To support this, we construct a dataset based on publicly available\ndialogue datasets and evaluate it using various baseline methods. Experimental\nresults demonstrate that methods incorporating Chain-of-Thought (CoT) reasoning\ncan enhance performance in dialogue fact verification. Despite this, the best\nF1-score achieved on the HybriDialogue, an open-domain dialogue dataset, is\nonly 0.75, indicating that the benchmark remains a challenging task for future\nresearch. Our dataset and code will be public on GitHub.", "AI": {"tldr": "This paper introduces FineDialFact, a benchmark for fine-grained dialogue fact verification to address the limitations of current hallucination detection methods in dialogue systems. The benchmark remains challenging for future research.", "motivation": "LLMs produce hallucinations, which poses significant challenges for many NLP applications. Current approaches to hallucination detection in dialogue systems primarily focus on verifying the factual consistency of generated responses, making one factual label overly simplistic and coarse-grained.", "method": "introduce a benchmark, FineDialFact, for fine-grained dialogue fact verification, which involves verifying atomic facts extracted from dialogue responses. construct a dataset based on publicly available dialogue datasets and evaluate it using various baseline methods. methods incorporating Chain-of-Thought (CoT) reasoning can enhance performance", "result": "methods incorporating Chain-of-Thought (CoT) reasoning can enhance performance in dialogue fact verification. the best F1-score achieved on the HybriDialogue is only 0.75", "conclusion": "The best F1-score achieved on the HybriDialogue is only 0.75, indicating that the benchmark remains a challenging task for future research."}}
{"id": "2508.05755", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05755", "abs": "https://arxiv.org/abs/2508.05755", "authors": ["Agnieszka Polowczyk", "Alicja Polowczyk", "Dawid Malarz", "Artur Kasymov", "Marcin Mazur", "Jacek Tabor", "Przemys\u0142aw Spurek"], "title": "UnGuide: Learning to Forget with LoRA-Guided Diffusion Models", "comment": null, "summary": "Recent advances in large-scale text-to-image diffusion models have heightened\nconcerns about their potential misuse, especially in generating harmful or\nmisleading content. This underscores the urgent need for effective machine\nunlearning, i.e., removing specific knowledge or concepts from pretrained\nmodels without compromising overall performance. One possible approach is\nLow-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models\nfor targeted unlearning. However, LoRA often inadvertently alters unrelated\ncontent, leading to diminished image fidelity and realism. To address this\nlimitation, we introduce UnGuide -- a novel approach which incorporates\nUnGuidance, a dynamic inference mechanism that leverages Classifier-Free\nGuidance (CFG) to exert precise control over the unlearning process. UnGuide\nmodulates the guidance scale based on the stability of a few first steps of\ndenoising processes, enabling selective unlearning by LoRA adapter. For prompts\ncontaining the erased concept, the LoRA module predominates and is\ncounterbalanced by the base model; for unrelated prompts, the base model\ngoverns generation, preserving content fidelity. Empirical results demonstrate\nthat UnGuide achieves controlled concept removal and retains the expressive\npower of diffusion models, outperforming existing LoRA-based methods in both\nobject erasure and explicit content removal tasks.", "AI": {"tldr": "UnGuide\u662f\u4e00\u79cd\u65b0\u7684unlearning\u65b9\u6cd5\uff0c\u5b83\u7ed3\u5408\u4e86UnGuidance\u6765\u7cbe\u786e\u63a7\u5236unlearning\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u53d7\u63a7\u6982\u5ff5\u79fb\u9664\uff0c\u5e76\u4fdd\u7559\u4e86\u6269\u6563\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u52a0\u5267\u4e86\u4eba\u4eec\u5bf9\u5176\u6f5c\u5728\u8bef\u7528\u7684\u62c5\u5fe7\uff0c\u5c24\u5176\u662f\u5728\u751f\u6210\u6709\u5bb3\u6216\u8bef\u5bfc\u6027\u5185\u5bb9\u65b9\u9762\u3002\u8fd9\u7a81\u663e\u4e86\u5bf9\u6709\u6548\u7684\u673a\u5668unlearning\u7684\u8feb\u5207\u9700\u6c42\uff0c\u5373\u5728\u4e0d\u5f71\u54cd\u6574\u4f53\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\uff0c\u4ece\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u5220\u9664\u7279\u5b9a\u7684\u77e5\u8bc6\u6216\u6982\u5ff5\u3002LoRA\u901a\u5e38\u4f1a\u65e0\u610f\u4e2d\u66f4\u6539\u4e0d\u76f8\u5173\u7684\u5185\u5bb9\uff0c\u4ece\u800c\u5bfc\u81f4\u56fe\u50cf\u4fdd\u771f\u5ea6\u548c\u771f\u5b9e\u611f\u964d\u4f4e\u3002", "method": "\u5f15\u5165UnGuide\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5b83\u7ed3\u5408\u4e86UnGuidance\uff0c\u4e00\u79cd\u52a8\u6001\u63a8\u7406\u673a\u5236\uff0c\u8be5\u673a\u5236\u5229\u7528\u65e0\u5206\u7c7b\u5668\u6307\u5bfc\uff08CFG\uff09\u6765\u7cbe\u786e\u63a7\u5236unlearning\u8fc7\u7a0b\u3002UnGuide\u57fa\u4e8e\u53bb\u566a\u8fc7\u7a0b\u7684\u51e0\u4e2a\u7b2c\u4e00\u6b65\u7684\u7a33\u5b9a\u6027\u6765\u8c03\u8282\u6307\u5bfc\u5c3a\u5ea6\uff0c\u4ece\u800c\u901a\u8fc7LoRA\u9002\u914d\u5668\u5b9e\u73b0\u9009\u62e9\u6027unlearning\u3002", "result": "\u7ecf\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUnGuide\u5b9e\u73b0\u4e86\u53d7\u63a7\u6982\u5ff5\u79fb\u9664\uff0c\u5e76\u4fdd\u7559\u4e86\u6269\u6563\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5728\u5bf9\u8c61\u64e6\u9664\u548c\u663e\u5f0f\u5185\u5bb9\u79fb\u9664\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8eLoRA\u7684\u65b9\u6cd5\u3002", "conclusion": "UnGuide\u5728\u53d7\u63a7\u6982\u5ff5\u79fb\u9664\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4fdd\u7559\u4e86\u6269\u6563\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5728\u5bf9\u8c61\u64e6\u9664\u548c\u663e\u5f0f\u5185\u5bb9\u79fb\u9664\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8eLoRA\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.05776", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05776", "abs": "https://arxiv.org/abs/2508.05776", "authors": ["Thomas L. Griffiths", "Brenden M. Lake", "R. Thomas McCoy", "Ellie Pavlick", "Taylor W. Webb"], "title": "Whither symbols in the era of advanced neural networks?", "comment": null, "summary": "Some of the strongest evidence that human minds should be thought about in\nterms of symbolic systems has been the way they combine ideas, produce novelty,\nand learn quickly. We argue that modern neural networks -- and the artificial\nintelligence systems built upon them -- exhibit similar abilities. This\nundermines the argument that the cognitive processes and representations used\nby human minds are symbolic, although the fact that these neural networks are\ntypically trained on data generated by symbolic systems illustrates that such\nsystems play an important role in characterizing the abstract problems that\nhuman minds have to solve. This argument leads us to offer a new agenda for\nresearch on the symbolic basis of human thought.", "AI": {"tldr": "\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u4e0e\u4eba\u7c7b\u601d\u7ef4\u7c7b\u4f3c\uff0c\u524a\u5f31\u4e86\u4eba\u7c7b\u601d\u7ef4\u57fa\u4e8e\u7b26\u53f7\u7684\u89c2\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4eba\u7c7b\u601d\u7ef4\u4ee5\u7b26\u53f7\u7cfb\u7edf\u7684\u65b9\u5f0f\u8fdb\u884c\u601d\u8003\uff0c\u4f53\u73b0\u5728\u5b83\u4eec\u7ec4\u5408\u601d\u60f3\u3001\u4ea7\u751f\u65b0\u9896\u6027\u548c\u5feb\u901f\u5b66\u4e60\u7684\u65b9\u5f0f\u4e0a\u3002", "method": "\u5206\u6790\u548c\u8bba\u8bc1", "result": "\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u5c55\u73b0\u51fa\u4e0e\u4eba\u7c7b\u601d\u7ef4\u76f8\u4f3c\u7684\u80fd\u529b\u3002", "conclusion": "\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u5c55\u73b0\u51fa\u4e0e\u4eba\u7c7b\u601d\u7ef4\u76f8\u4f3c\u7684\u80fd\u529b\uff0c\u8fd9\u524a\u5f31\u4e86\u8ba4\u77e5\u8fc7\u7a0b\u548c\u8868\u5f81\u662f\u7b26\u53f7\u7684\u89c2\u70b9\u3002\u867d\u7136\u8fd9\u4e9b\u795e\u7ecf\u7f51\u7edc\u901a\u5e38\u5728\u7b26\u53f7\u7cfb\u7edf\u751f\u6210\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46\u8fd9\u8868\u660e\u8fd9\u4e9b\u7cfb\u7edf\u5728\u8868\u5f81\u4eba\u7c7b\u601d\u7ef4\u5fc5\u987b\u89e3\u51b3\u7684\u62bd\u8c61\u95ee\u9898\u4e2d\u8d77\u7740\u91cd\u8981\u4f5c\u7528\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u4e3a\u4eba\u7c7b\u601d\u7ef4\u7684\u7b26\u53f7\u57fa\u7840\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u8bae\u7a0b\u3002"}}
{"id": "2508.05778", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2508.05778", "abs": "https://arxiv.org/abs/2508.05778", "authors": ["Jaemin Oh", "Jinsil Lee", "Youngjoon Hong"], "title": "Machine Learning-Based Nonlinear Nudging for Chaotic Dynamical Systems", "comment": "21 pages, 5 figures, 6 tables", "summary": "Nudging is an empirical data assimilation technique that incorporates an\nobservation-driven control term into the model dynamics. The trajectory of the\nnudged system approaches the true system trajectory over time, even when the\ninitial conditions differ. For linear state space models, such control terms\ncan be derived under mild assumptions. However, designing effective nudging\nterms becomes significantly more challenging in the nonlinear setting. In this\nwork, we propose neural network nudging, a data-driven method for learning\nnudging terms in nonlinear state space models. We establish a theoretical\nexistence result based on the Kazantzis--Kravaris--Luenberger observer theory.\nThe proposed approach is evaluated on three benchmark problems that exhibit\nchaotic behavior: the Lorenz 96 model, the Kuramoto--Sivashinsky equation, and\nthe Kolmogorov flow.", "AI": {"tldr": "This paper introduces neural network nudging for learning nudging terms in nonlinear systems and validates it on chaotic systems.", "motivation": "Designing effective nudging terms becomes significantly more challenging in the nonlinear setting.", "method": "This paper proposes neural network nudging, a data-driven method for learning nudging terms in nonlinear state space models. It also establishes a theoretical existence result based on the Kazantzis--Kravaris--Luenberger observer theory.", "result": "The proposed approach is evaluated on three benchmark problems that exhibit chaotic behavior: the Lorenz 96 model, the Kuramoto--Sivashinsky equation, and the Kolmogorov flow.", "conclusion": "This paper proposes and evaluates neural network nudging, a data-driven method for learning nudging terms in nonlinear state space models, on three benchmark problems that exhibit chaotic behavior: the Lorenz 96 model, the Kuramoto--Sivashinsky equation, and the Kolmogorov flow."}}
{"id": "2508.05649", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05649", "abs": "https://arxiv.org/abs/2508.05649", "authors": ["Jayanth Yetukuri", "Mehran Elyasi", "Samarth Agrawal", "Aritra Mandal", "Rui Kong", "Harish Vempati", "Ishita Khan"], "title": "AI Guided Accelerator For Search Experience", "comment": "Accepted at SIGIR eCom'25.\n  https://sigir-ecom.github.io/eCom25Papers/paper_25.pdf", "summary": "Effective query reformulation is pivotal in narrowing the gap between a\nuser's exploratory search behavior and the identification of relevant products\nin e-commerce environments. While traditional approaches predominantly model\nquery rewrites as isolated pairs, they often fail to capture the sequential and\ntransitional dynamics inherent in real-world user behavior. In this work, we\npropose a novel framework that explicitly models transitional\nqueries--intermediate reformulations occurring during the user's journey toward\ntheir final purchase intent. By mining structured query trajectories from\neBay's large-scale user interaction logs, we reconstruct query sequences that\nreflect shifts in intent while preserving semantic coherence. This approach\nallows us to model a user's shopping funnel, where mid-journey transitions\nreflect exploratory behavior and intent refinement. Furthermore, we incorporate\ngenerative Large Language Models (LLMs) to produce semantically diverse and\nintent-preserving alternative queries, extending beyond what can be derived\nthrough collaborative filtering alone. These reformulations can be leveraged to\npopulate Related Searches or to power intent-clustered carousels on the search\nresults page, enhancing both discovery and engagement. Our contributions\ninclude (i) the formal identification and modeling of transitional queries,\n(ii) the introduction of a structured query sequence mining pipeline for intent\nflow understanding, and (iii) the application of LLMs for scalable,\nintent-aware query expansion. Empirical evaluation demonstrates measurable\ngains in conversion and engagement metrics compared to the existing Related\nSearches module, validating the effectiveness of our approach in real-world\ne-commerce settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u67e5\u8be2\u91cd\u6784\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5bf9\u7528\u6237\u4f1a\u8bdd\u4e2d\u7684\u8fc7\u6e21\u67e5\u8be2\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8bed\u4e49\u591a\u6837\u4e14\u4fdd\u7559\u610f\u56fe\u7684\u66ff\u4ee3\u67e5\u8be2\uff0c\u4ece\u800c\u63d0\u5347\u7535\u5546\u73af\u5883\u4e2d\u7684\u641c\u7d22\u6548\u679c\u3002", "motivation": "\u6709\u6548\u7684\u67e5\u8be2\u91cd\u6784\u5bf9\u4e8e\u7f29\u5c0f\u7528\u6237\u7684\u63a2\u7d22\u6027\u641c\u7d22\u884c\u4e3a\u4e0e\u7535\u5b50\u5546\u52a1\u73af\u5883\u4e2d\u76f8\u5173\u4ea7\u54c1\u7684\u8bc6\u522b\u4e4b\u95f4\u7684\u5dee\u8ddd\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684\u65b9\u6848\u4e3b\u8981\u5c06\u67e5\u8be2\u91cd\u5199\u5efa\u6a21\u4e3a\u5b64\u7acb\u7684\u5bf9\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u65e0\u6cd5\u6355\u6349\u5230\u5b9e\u9645\u7528\u6237\u884c\u4e3a\u4e2d\u56fa\u6709\u7684\u987a\u5e8f\u548c\u8fc7\u6e21\u52a8\u6001\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u660e\u786e\u5730\u6a21\u62df\u4e86\u8fc7\u6e21\u67e5\u8be2\u2014\u2014\u5728\u7528\u6237\u5b9e\u73b0\u6700\u7ec8\u8d2d\u4e70\u610f\u56fe\u7684\u8fc7\u7a0b\u4e2d\u53d1\u751f\u7684\u4e2d\u95f4\u8c03\u6574\u3002", "result": "\u5bf9\u8fc7\u6e21\u67e5\u8be2\u8fdb\u884c\u4e86\u6b63\u5f0f\u7684\u8bc6\u522b\u548c\u5efa\u6a21\uff1b\u4e3a\u610f\u56fe\u6d41\u7406\u89e3\u5f15\u5165\u4e86\u7ed3\u6784\u5316\u67e5\u8be2\u5e8f\u5217\u6316\u6398\u7ba1\u9053\uff1b\u5e94\u7528 LLM \u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u3001\u5177\u6709\u610f\u56fe\u610f\u8bc6\u7684\u67e5\u8be2\u6269\u5c55\u3002", "conclusion": "\u901a\u8fc7\u5728\u5b9e\u9645\u7535\u5546\u73af\u5883\u4e2d\u4e0e\u73b0\u6709\u76f8\u5173\u641c\u7d22\u6a21\u5757\u76f8\u6bd4\uff0c\u8f6c\u5316\u7387\u548c\u53c2\u4e0e\u5ea6\u6307\u6807\u5b9e\u73b0\u4e86\u53ef\u8861\u91cf\u7684\u63d0\u5347\uff0c\u4ece\u800c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u7535\u5546\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.05803", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.05803", "abs": "https://arxiv.org/abs/2508.05803", "authors": ["Abishek Thamma", "Micha Heilbron"], "title": "Human-like fleeting memory improves language learning but impairs reading time prediction in transformer language models", "comment": null, "summary": "Human memory is fleeting. As words are processed, the exact wordforms that\nmake up incoming sentences are rapidly lost. Cognitive scientists have long\nbelieved that this limitation of memory may, paradoxically, help in learning\nlanguage - an idea supported by classic connectionist modelling work. The rise\nof Transformers appears to challenge this idea, as these models can learn\nlanguage effectively, despite lacking memory limitations or other architectural\nrecency biases. Here, we investigate the hypothesized benefit of fleeting\nmemory for language learning in tightly controlled experiments on transformer\nlanguage models. Training transformers with and without fleeting memory on a\ndevelopmentally realistic training set, we find that fleeting memory\nconsistently improves language learning (as quantified by both overall language\nmodelling performance and targeted syntactic evaluation) but, unexpectedly,\nimpairs surprisal-based prediction of human reading times. Interestingly,\nfollow up analyses revealed that this discrepancy - better language modeling,\nyet worse reading time prediction - could not be accounted for by prior\nexplanations of why better language models sometimes fit human reading time\nworse. Together, these results support a benefit of memory limitations on\nneural network language learning - but not on predicting behavior.", "AI": {"tldr": "fleeting memory helps transformers learn language but hurts reading time prediction", "motivation": "the rise of Transformers challenges the idea that memory limitations help in learning language", "method": "training transformers with and without fleeting memory on a developmentally realistic training set", "result": "fleeting memory consistently improves language learning but impairs surprisal-based prediction of human reading times", "conclusion": "memory limitations improve neural network language learning but not predicting behavior"}}
{"id": "2508.05769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05769", "abs": "https://arxiv.org/abs/2508.05769", "authors": ["Seyed Hadi Seyed", "Ayberk Cansever", "David Hart"], "title": "Improving Masked Style Transfer using Blended Partial Convolution", "comment": null, "summary": "Artistic style transfer has long been possible with the advancements of\nconvolution- and transformer-based neural networks. Most algorithms apply the\nartistic style transfer to the whole image, but individual users may only need\nto apply a style transfer to a specific region in the image. The standard\npractice is to simply mask the image after the stylization. This work shows\nthat this approach tends to improperly capture the style features in the region\nof interest. We propose a partial-convolution-based style transfer network that\naccurately applies the style features exclusively to the region of interest.\nAdditionally, we present network-internal blending techniques that account for\nimperfections in the region selection. We show that this visually and\nquantitatively improves stylization using examples from the SA-1B dataset. Code\nis publicly available at https://github.com/davidmhart/StyleTransferMasked.", "AI": {"tldr": "Proposes a partial-convolution-based style transfer network for applying style transfer to specific regions of interest, improving stylization accuracy.", "motivation": "Existing methods apply artistic style transfer to the whole image, but individual users may only need to apply a style transfer to a specific region in the image, and simply masking the image after stylization improperly captures the style features in the region of interest.", "method": "partial-convolution-based style transfer network with network-internal blending techniques", "result": "Visually and quantitatively improved stylization using examples from the SA-1B dataset.", "conclusion": "This work proposes a partial-convolution-based style transfer network that accurately applies the style features exclusively to the region of interest and present network-internal blending techniques that account for imperfections in the region selection. This approach visually and quantitatively improves stylization."}}
{"id": "2508.05792", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05792", "abs": "https://arxiv.org/abs/2508.05792", "authors": ["Kausik Lakkaraju", "Siva Likitha Valluru", "Biplav Srivastava"], "title": "Holistic Explainable AI (H-XAI): Extending Transparency Beyond Developers in AI-Driven Decision Making", "comment": null, "summary": "Current eXplainable AI (XAI) methods largely serve developers, often focusing\non justifying model outputs rather than supporting diverse stakeholder needs. A\nrecent shift toward Evaluative AI reframes explanation as a tool for hypothesis\ntesting, but still focuses primarily on operational organizations. We introduce\nHolistic-XAI (H-XAI), a unified framework that integrates causal rating methods\nwith traditional XAI methods to support explanation as an interactive,\nmulti-method process. H-XAI allows stakeholders to ask a series of questions,\ntest hypotheses, and compare model behavior against automatically constructed\nrandom and biased baselines. It combines instance-level and global\nexplanations, adapting to each stakeholder's goals, whether understanding\nindividual decisions, assessing group-level bias, or evaluating robustness\nunder perturbations. We demonstrate the generality of our approach through two\ncase studies spanning six scenarios: binary credit risk classification and\nfinancial time-series forecasting. H-XAI fills critical gaps left by existing\nXAI methods by combining causal ratings and post-hoc explanations to answer\nstakeholder-specific questions at both the individual decision level and the\noverall model level.", "AI": {"tldr": "H-XAI\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u56e0\u679c\u8bc4\u7ea7\u65b9\u6cd5\u4e0e\u4f20\u7edf\u7684XAI\u65b9\u6cd5\uff0c\u4ee5\u652f\u6301\u89e3\u91ca\u4f5c\u4e3a\u4e00\u4e2a\u4ea4\u4e92\u7684\u3001\u591a\u65b9\u6cd5\u7684\u8fc7\u7a0b\uff0c\u4ee5\u56de\u7b54\u5229\u76ca\u76f8\u5173\u8005\u5728\u4e2a\u4f53\u51b3\u7b56\u5c42\u9762\u548c\u6574\u4f53\u6a21\u578b\u5c42\u9762\u7684\u7279\u5b9a\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7684 eXplainable AI (XAI) \u65b9\u6cd5\u4e3b\u8981\u4e3a\u5f00\u53d1\u8005\u670d\u52a1\uff0c\u901a\u5e38\u4fa7\u91cd\u4e8e\u8bc1\u660e\u6a21\u578b\u8f93\u51fa\u7684\u5408\u7406\u6027\uff0c\u800c\u4e0d\u662f\u652f\u6301\u4e0d\u540c\u7684\u5229\u76ca\u76f8\u5173\u8005\u7684\u9700\u6c42\u3002\u6700\u8fd1\u5411\u8bc4\u4f30\u6027 AI \u7684\u8f6c\u53d8\u5c06\u89e3\u91ca\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5047\u8bbe\u68c0\u9a8c\u7684\u5de5\u5177\uff0c\u4f46\u4ecd\u7136\u4e3b\u8981\u5173\u6ce8\u8fd0\u8425\u7ec4\u7ec7\u3002", "method": "H-XAI\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5b83\u5c06\u56e0\u679c\u8bc4\u7ea7\u65b9\u6cd5\u4e0e\u4f20\u7edf\u7684XAI\u65b9\u6cd5\u76f8\u7ed3\u5408\uff0c\u4ee5\u652f\u6301\u89e3\u91ca\u4f5c\u4e3a\u4e00\u4e2a\u4ea4\u4e92\u7684\u3001\u591a\u65b9\u6cd5\u7684\u8fc7\u7a0b\u3002", "result": "H-XAI\u5141\u8bb8\u5229\u76ca\u76f8\u5173\u8005\u63d0\u51fa\u4e00\u7cfb\u5217\u95ee\u9898\uff0c\u6d4b\u8bd5\u5047\u8bbe\uff0c\u5e76\u5c06\u6a21\u578b\u884c\u4e3a\u4e0e\u81ea\u52a8\u6784\u5efa\u7684\u968f\u673a\u548c\u6709\u504f\u89c1\u7684\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002\u5b83\u7ed3\u5408\u4e86\u5b9e\u4f8b\u7ea7\u522b\u548c\u5168\u5c40\u89e3\u91ca\uff0c\u9002\u5e94\u6bcf\u4e2a\u5229\u76ca\u76f8\u5173\u8005\u7684\u76ee\u6807\uff0c\u65e0\u8bba\u662f\u7406\u89e3\u4e2a\u4eba\u51b3\u7b56\u3001\u8bc4\u4f30\u7ec4\u7ea7\u522b\u504f\u5dee\uff0c\u8fd8\u662f\u8bc4\u4f30\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u4e24\u4e2a\u8de8\u8d8a\u516d\u4e2a\u573a\u666f\u7684\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u666e\u904d\u6027\uff1a\u4e8c\u5143\u4fe1\u7528\u98ce\u9669\u5206\u7c7b\u548c\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3002", "conclusion": "H-XAI\u586b\u8865\u4e86\u73b0\u6709XAI\u65b9\u6cd5\u7684\u5173\u952e\u7a7a\u767d\uff0c\u5b83\u7ed3\u5408\u4e86\u56e0\u679c\u8bc4\u7ea7\u548c\u4e8b\u540e\u89e3\u91ca\uff0c\u4ee5\u56de\u7b54\u5229\u76ca\u76f8\u5173\u8005\u5728\u4e2a\u4f53\u51b3\u7b56\u5c42\u9762\u548c\u6574\u4f53\u6a21\u578b\u5c42\u9762\u7684\u7279\u5b9a\u95ee\u9898\u3002"}}
{"id": "2508.05791", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05791", "abs": "https://arxiv.org/abs/2508.05791", "authors": ["Haoran Li", "Lihao Mai", "Muhao Guo", "Jiaqi Wu", "Yang Weng", "Yannan Sun", "Ce Jimmy Liu"], "title": "From Imperfect Signals to Trustworthy Structure: Confidence-Aware Inference from Heterogeneous and Reliability-Varying Utility Data", "comment": "10 pages", "summary": "Accurate distribution grid topology is essential for reliable modern grid\noperations. However, real-world utility data originates from multiple sources\nwith varying characteristics and levels of quality. In this work, developed in\ncollaboration with Oncor Electric Delivery, we propose a scalable framework\nthat reconstructs a trustworthy grid topology by systematically integrating\nheterogeneous data. We observe that distribution topology is fundamentally\ngoverned by two complementary dimensions: the spatial layout of physical\ninfrastructure (e.g., GIS and asset metadata) and the dynamic behavior of the\nsystem in the signal domain (e.g., voltage time series). When jointly\nleveraged, these dimensions support a complete and physically coherent\nreconstruction of network connectivity. To address the challenge of uneven data\nquality without compromising observability, we introduce a confidence-aware\ninference mechanism that preserves structurally informative yet imperfect\ninputs, while quantifying the reliability of each inferred connection for\noperator interpretation. This soft handling of uncertainty is tightly coupled\nwith hard enforcement of physical feasibility: we embed operational\nconstraints, such as transformer capacity limits and radial topology\nrequirements, directly into the learning process. Together, these components\nensure that inference is both uncertainty-aware and structurally valid,\nenabling rapid convergence to actionable, trustworthy topologies under\nreal-world deployment conditions. The proposed framework is validated using\ndata from over 8000 meters across 3 feeders in Oncor's service territory,\ndemonstrating over 95% accuracy in topology reconstruction and substantial\nimprovements in confidence calibration and computational efficiency relative to\nbaseline methods.", "AI": {"tldr": "Proposes a framework for reconstructing grid topology by integrating heterogeneous data, using confidence-aware inference and physical constraints, achieving high accuracy and efficiency.", "motivation": "Accurate distribution grid topology is essential for reliable modern grid operations. However, real-world utility data originates from multiple sources with varying characteristics and levels of quality.", "method": "a scalable framework that reconstructs a trustworthy grid topology by systematically integrating heterogeneous data. jointly leverage the spatial layout of physical infrastructure and the dynamic behavior of the system in the signal domain. introduce a confidence-aware inference mechanism that preserves structurally informative yet imperfect inputs, while quantifying the reliability of each inferred connection for operator interpretation. embed operational constraints, such as transformer capacity limits and radial topology requirements, directly into the learning process.", "result": "over 95% accuracy in topology reconstruction and substantial improvements in confidence calibration and computational efficiency relative to baseline methods.", "conclusion": "The proposed framework is validated using data from over 8000 meters across 3 feeders in Oncor's service territory, demonstrating over 95% accuracy in topology reconstruction and substantial improvements in confidence calibration and computational efficiency relative to baseline methods."}}
{"id": "2508.05650", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05650", "abs": "https://arxiv.org/abs/2508.05650", "authors": ["Jiaxuan Liang", "Shide Zhou", "Kailong Wang"], "title": "OmniBench-RAG: A Multi-Domain Evaluation Platform for Retrieval-Augmented Generation Tools", "comment": null, "summary": "While Retrieval Augmented Generation (RAG) is now widely adopted to enhance\nLLMs, evaluating its true performance benefits in a reproducible and\ninterpretable way remains a major hurdle. Existing methods often fall short:\nthey lack domain coverage, employ coarse metrics that miss sub document\nprecision, and fail to capture computational trade offs. Most critically, they\nprovide no standardized framework for comparing RAG effectiveness across\ndifferent models and domains.\n  We introduce OmniBench RAG, a novel automated platform for multi domain\nevaluation of RAG systems. The platform quantifies performance gains across\naccuracy and efficiency dimensions, spanning nine knowledge fields including\nculture, geography, and health. We introduce two standardized metrics:\nImprovements (accuracy gains) and Transformation (efficiency differences\nbetween pre RAG and post RAG models), enabling reproducible comparisons across\nmodels and tasks. The platform features dynamic test generation, modular\nevaluation pipelines, and automated knowledge base construction. Our evaluation\nreveals striking variability in RAG effectiveness, from significant gains in\nculture to declines in mathematics, highlighting the critical importance of\nsystematic, domain aware assessment. A demonstration video is available at:\nhttps://www.youtube.com/watch?v=BZx83QFcTCI. Code and datasets:\nhttps://github.com/Garnett-Liang/Omnibench-RAG.", "AI": {"tldr": "OmniBench RAG\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u9886\u57df\u8bc4\u4f30RAG\u7cfb\u7edf\u7684\u5e73\u53f0\uff0c\u63ed\u793a\u4e86RAG\u5728\u4e0d\u540c\u9886\u57df\u6548\u679c\u7684\u5dee\u5f02\u6027\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u9886\u57df\u8986\u76d6\u3001\u91c7\u7528\u7c97\u7cd9\u6307\u6807\u4e14\u65e0\u6cd5\u6355\u6349\u8ba1\u7b97\u6743\u8861\uff0c\u5e76\u4e14\u7f3a\u4e4f\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u548c\u9886\u57dfRAG\u6709\u6548\u6027\u7684\u6807\u51c6\u5316\u6846\u67b6\u3002", "method": "\u5f15\u5165\u4e86OmniBench RAG\uff0c\u4e00\u4e2a\u7528\u4e8eRAG\u7cfb\u7edf\u591a\u9886\u57df\u8bc4\u4f30\u7684\u81ea\u52a8\u5316\u5e73\u53f0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u4e2a\u6807\u51c6\u5316\u6307\u6807\uff1a\u6539\u8fdb\uff08\u51c6\u786e\u7387\u63d0\u5347\uff09\u548c\u8f6c\u6362\uff08RAG\u524d\u540e\u6a21\u578b\u7684\u6548\u7387\u5dee\u5f02\uff09\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cRAG\u5728\u6587\u5316\u9886\u57df\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5728\u6570\u5b66\u9886\u57df\u6709\u6240\u4e0b\u964d\u3002", "conclusion": "RAG\u6548\u679c\u5728\u4e0d\u540c\u9886\u57df\u8868\u73b0\u51fa\u663e\u8457\u5dee\u5f02\uff0c\u9700\u8981\u8fdb\u884c\u7cfb\u7edf\u6027\u7684\u3001\u9886\u57df\u611f\u77e5\u7684\u8bc4\u4f30\u3002"}}
{"id": "2508.05830", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.05830", "abs": "https://arxiv.org/abs/2508.05830", "authors": ["Tong Li", "Rasiq Hussain", "Mehak Gupta", "Joshua R. Oltmanns"], "title": "\"Mirror\" Language AI Models of Depression are Criterion-Contaminated", "comment": "39 pages, 9 figures", "summary": "A growing number of studies show near-perfect LLM language-based prediction\nof depression assessment scores (up to R2 of .70). However, many develop these\nmodels directly from language responses to depression assessments. These\n\"Mirror models\" suffer from \"criterion contamination\", which arises when a\npredicted score depends in part on the predictors themselves. This causes\nartificial effect size inflation which reduces model generalizability. The\npresent study compares the performance of Mirror models versus \"Non-Mirror\nmodels\", which are developed from language that does not mirror the assessment\nthey are developed to predict. N = 110 research participants completed two\ndifferent interviews: structured diagnostic and life history interviews. GPT-4,\nGPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic\ninterview depression scores from the two transcripts separately. Mirror models\n(using structured diagnostic data) showed very large effect sizes (e.g., R2 =\n.80). As expected, NonMirror models (using life history data) demonstrated\nsmaller effect sizes, but were relatively large (e.g., R2 = .27). When Mirror\nand Non-Mirror model-predicted structured interview depression scores were\ncorrelated with self-reported depression symptoms, Mirror and NonMirror\nperformed the same (e.g., r = ~.54), indicating that Mirror models contain bias\nperhaps due to criterion contamination. Topic modeling identified clusters\nacross Mirror and Non-Mirror models, as well as between true-positive and\nfalse-positive predictions. In this head-to-head comparison study, Mirror\nlanguage AI models of depression showed artificially inflated effect sizes and\nless generalizability. As language AI models for depression continue to evolve,\nincorporating Non-Mirror models may identify interpretable, and generalizable\nsemantic features that have unique utility in real-world psychological\nassessment.", "AI": {"tldr": "Mirror models for depression prediction have inflated effect sizes and less generalizability due to criterion contamination. Non-Mirror models may offer more interpretable and generalizable features for real-world assessment.", "motivation": "A growing number of studies show near-perfect LLM language-based prediction of depression assessment scores, but many develop these models directly from language responses to depression assessments, leading to criterion contamination and reduced model generalizability.", "method": "Compared the performance of Mirror models (developed from language responses to depression assessments) versus Non-Mirror models (developed from language that does not mirror the assessment). N = 110 research participants completed two different interviews: structured diagnostic and life history interviews. GPT-4, GPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic interview depression scores from the two transcripts separately.", "result": "Mirror models showed very large effect sizes (e.g., R2 = .80), while Non-Mirror models demonstrated smaller but relatively large effect sizes (e.g., R2 = .27). Mirror and NonMirror performed the same when correlated with self-reported depression symptoms (e.g., r = ~.54), indicating bias in Mirror models. Topic modeling identified clusters across Mirror and Non-Mirror models.", "conclusion": "Mirror language AI models of depression showed artificially inflated effect sizes and less generalizability. Incorporating Non-Mirror models may identify interpretable, and generalizable semantic features that have unique utility in real-world psychological assessment."}}
{"id": "2508.05772", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05772", "abs": "https://arxiv.org/abs/2508.05772", "authors": ["Can Zhao", "Pengfei Guo", "Dong Yang", "Yucheng Tang", "Yufan He", "Benjamin Simon", "Mason Belue", "Stephanie Harmon", "Baris Turkbey", "Daguang Xu"], "title": "MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with Rectified Flow and Region-specific Contrastive Loss", "comment": null, "summary": "Medical image synthesis is an important topic for both clinical and research\napplications. Recently, diffusion models have become a leading approach in this\narea. Despite their strengths, many existing methods struggle with (1) limited\ngeneralizability that only work for specific body regions or voxel spacings,\n(2) slow inference, which is a common issue for diffusion models, and (3) weak\nalignment with input conditions, which is a critical issue for medical imaging.\nMAISI, a previously proposed framework, addresses generalizability issues but\nstill suffers from slow inference and limited condition consistency. In this\nwork, we present MAISI-v2, the first accelerated 3D medical image synthesis\nframework that integrates rectified flow to enable fast and high quality\ngeneration. To further enhance condition fidelity, we introduce a novel\nregion-specific contrastive loss to enhance the sensitivity to region of\ninterest. Our experiments show that MAISI-v2 can achieve SOTA image quality\nwith $33 \\times$ acceleration for latent diffusion model. We also conducted a\ndownstream segmentation experiment to show that the synthetic images can be\nused for data augmentation. We release our code, training details, model\nweights, and a GUI demo to facilitate reproducibility and promote further\ndevelopment within the community.", "AI": {"tldr": "MAISI-v2 \u662f\u4e00\u79cd\u5feb\u901f\u3001\u9ad8\u8d28\u91cf\u7684 3D \u533b\u5b66\u56fe\u50cf\u5408\u6210\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u4fee\u6b63\u6d41\u52a0\u901f\u5e76\u4f7f\u7528\u7279\u5b9a\u533a\u57df\u5bf9\u6bd4\u635f\u5931\u6765\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\u548c\u6761\u4ef6\u4fdd\u771f\u5ea6\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u5b66\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\u5b58\u5728\u6cdb\u5316\u6027\u6709\u9650\u3001\u63a8\u7406\u901f\u5ea6\u6162\u4ee5\u53ca\u4e0e\u8f93\u5165\u6761\u4ef6\u5bf9\u9f50\u8f83\u5f31\u7684\u95ee\u9898\u3002MAISI \u89e3\u51b3\u4e86\u6cdb\u5316\u6027\u95ee\u9898\uff0c\u4f46\u4ecd\u7136\u5b58\u5728\u63a8\u7406\u901f\u5ea6\u6162\u548c\u6761\u4ef6\u4e00\u81f4\u6027\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "MAISI-v2\uff0c\u4e00\u79cd\u7ed3\u5408\u4e86\u4fee\u6b63\u6d41\u7684\u52a0\u901f 3D \u533b\u5b66\u56fe\u50cf\u5408\u6210\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u7279\u5b9a\u533a\u57df\u5bf9\u6bd4\u635f\u5931\u3002", "result": "MAISI-v2 \u5b9e\u73b0\u4e86 SOTA \u56fe\u50cf\u8d28\u91cf\uff0c\u901f\u5ea6\u662f\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684 33 \u500d\u3002\u5408\u6210\u56fe\u50cf\u53ef\u7528\u4e8e\u6570\u636e\u589e\u5f3a\u3002", "conclusion": "MAISI-v2 \u901a\u8fc7\u6574\u5408\u4fee\u6b63\u6d41\u5b9e\u73b0\u4e86\u5feb\u901f\u9ad8\u8d28\u91cf\u7684\u751f\u6210\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u7279\u5b9a\u533a\u57df\u5bf9\u6bd4\u635f\u5931\u6765\u589e\u5f3a\u6761\u4ef6\u4fdd\u771f\u5ea6\uff0c\u4ece\u800c\u5728\u533b\u5b66\u56fe\u50cf\u5408\u6210\u65b9\u9762\u8fbe\u5230\u4e86 SOTA \u7684\u56fe\u50cf\u8d28\u91cf\uff0c\u901f\u5ea6\u662f\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684 33 \u500d\u3002\u5408\u6210\u56fe\u50cf\u53ef\u7528\u4e8e\u6570\u636e\u589e\u5f3a\u3002"}}
{"id": "2508.05855", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05855", "abs": "https://arxiv.org/abs/2508.05855", "authors": ["Zixia Wang", "Jia Hu", "Ronghui Mu"], "title": "Safety of Embodied Navigation: A Survey", "comment": null, "summary": "As large language models (LLMs) continue to advance and gain influence, the\ndevelopment of embodied AI has accelerated, drawing significant attention,\nparticularly in navigation scenarios. Embodied navigation requires an agent to\nperceive, interact with, and adapt to its environment while moving toward a\nspecified target in unfamiliar settings. However, the integration of embodied\nnavigation into critical applications raises substantial safety concerns. Given\ntheir deployment in dynamic, real-world environments, ensuring the safety of\nsuch systems is critical. This survey provides a comprehensive analysis of\nsafety in embodied navigation from multiple perspectives, encompassing attack\nstrategies, defense mechanisms, and evaluation methodologies. Beyond conducting\na comprehensive examination of existing safety challenges, mitigation\ntechnologies, and various datasets and metrics that assess effectiveness and\nrobustness, we explore unresolved issues and future research directions in\nembodied navigation safety. These include potential attack methods, mitigation\nstrategies, more reliable evaluation techniques, and the implementation of\nverification frameworks. By addressing these critical gaps, this survey aims to\nprovide valuable insights that can guide future research toward the development\nof safer and more reliable embodied navigation systems. Furthermore, the\nfindings of this study have broader implications for enhancing societal safety\nand increasing industrial efficiency.", "AI": {"tldr": "a comprehensive analysis of safety in embodied navigation", "motivation": "ensuring the safety of such systems is critical. This survey provides a comprehensive analysis of safety in embodied navigation from multiple perspectives", "method": "a comprehensive analysis of safety in embodied navigation from multiple perspectives, encompassing attack strategies, defense mechanisms, and evaluation methodologies", "result": "conducting a comprehensive examination of existing safety challenges, mitigation technologies, and various datasets and metrics that assess effectiveness and robustness, we explore unresolved issues and future research directions in embodied navigation safety", "conclusion": "This survey aims to provide valuable insights that can guide future research toward the development of safer and more reliable embodied navigation systems. Furthermore, the findings of this study have broader implications for enhancing societal safety and increasing industrial efficiency."}}
{"id": "2508.05831", "categories": ["cs.LG", "cs.NA", "math.NA", "15A29 Inverse problems in linear algebra 65F22, 68T07, 65F05, 62C12", "G.1.3; F.2.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.05831", "abs": "https://arxiv.org/abs/2508.05831", "authors": ["Alexander DeLise", "Kyle Loh", "Krish Patel", "Meredith Teague", "Andrea Arnold", "Matthias Chung"], "title": "Optimal Linear Baseline Models for Scientific Machine Learning", "comment": "40 pages, 10 Figures, 9 Tables", "summary": "Across scientific domains, a fundamental challenge is to characterize and\ncompute the mappings from underlying physical processes to observed signals and\nmeasurements. While nonlinear neural networks have achieved considerable\nsuccess, they remain theoretically opaque, which hinders adoption in contexts\nwhere interpretability is paramount. In contrast, linear neural networks serve\nas a simple yet effective foundation for gaining insight into these complex\nrelationships. In this work, we develop a unified theoretical framework for\nanalyzing linear encoder-decoder architectures through the lens of Bayes risk\nminimization for solving data-driven scientific machine learning problems. We\nderive closed-form, rank-constrained linear and affine linear optimal mappings\nfor forward modeling and inverse recovery tasks. Our results generalize\nexisting formulations by accommodating rank-deficiencies in data, forward\noperators, and measurement processes. We validate our theoretical results by\nconducting numerical experiments on datasets from simple biomedical imaging,\nfinancial factor analysis, and simulations involving nonlinear fluid dynamics\nvia the shallow water equations. This work provides a robust baseline for\nunderstanding and benchmarking learned neural network models for scientific\nmachine learning problems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7ebf\u6027\u795e\u7ecf\u7f51\u7edc\u5728\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u95ee\u9898\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e00\u4e2a\u57fa\u672c\u7684\u6311\u6218\u662f\u8868\u5f81\u548c\u8ba1\u7b97\u4ece\u5e95\u5c42\u7269\u7406\u8fc7\u7a0b\u5230\u89c2\u6d4b\u4fe1\u53f7\u548c\u6d4b\u91cf\u7684\u6620\u5c04\u3002\u867d\u7136\u975e\u7ebf\u6027\u795e\u7ecf\u7f51\u7edc\u5df2\u7ecf\u53d6\u5f97\u4e86\u76f8\u5f53\u5927\u7684\u6210\u529f\uff0c\u4f46\u5b83\u4eec\u5728\u7406\u8bba\u4e0a\u4ecd\u7136\u662f\u4e0d\u900f\u660e\u7684\uff0c\u8fd9\u963b\u788d\u4e86\u5728\u53ef\u89e3\u91ca\u6027\u81f3\u5173\u91cd\u8981\u7684\u73af\u5883\u4e2d\u7684\u91c7\u7528\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u7ebf\u6027\u795e\u7ecf\u7f51\u7edc\u4e3a\u6df1\u5165\u4e86\u89e3\u8fd9\u4e9b\u590d\u6742\u5173\u7cfb\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u57fa\u7840\u3002", "method": "\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u98ce\u9669\u6700\u5c0f\u5316\u7684\u89c6\u89d2\u6765\u5206\u6790\u7ebf\u6027\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u9a71\u52a8\u7684\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u95ee\u9898\u3002\u6211\u4eec\u63a8\u5bfc\u4e86\u7528\u4e8e\u524d\u5411\u5efa\u6a21\u548c\u9006\u6062\u590d\u4efb\u52a1\u7684\u95ed\u5f0f\u3001\u79e9\u7ea6\u675f\u7ebf\u6027\u53ca\u4eff\u5c04\u7ebf\u6027\u6700\u4f18\u6620\u5c04\u3002", "result": "\u6211\u4eec\u7684\u7ed3\u679c\u901a\u8fc7\u9002\u5e94\u6570\u636e\u3001\u524d\u5411\u7b97\u5b50\u548c\u6d4b\u91cf\u8fc7\u7a0b\u4e2d\u7684\u79e9\u7f3a\u9677\u6765\u63a8\u5e7f\u73b0\u6709\u7684\u516c\u5f0f\u3002\u6211\u4eec\u901a\u8fc7\u5bf9\u6765\u81ea\u7b80\u5355\u751f\u7269\u533b\u5b66\u6210\u50cf\u3001\u91d1\u878d\u56e0\u7d20\u5206\u6790\u4ee5\u53ca\u6d89\u53ca\u901a\u8fc7\u6d45\u6c34\u65b9\u7a0b\u7684\u975e\u7ebf\u6027\u6d41\u4f53\u52a8\u529b\u5b66\u6a21\u62df\u7684\u6570\u636e\u96c6\u8fdb\u884c\u6570\u503c\u5b9e\u9a8c\u6765\u9a8c\u8bc1\u6211\u4eec\u7684\u7406\u8bba\u7ed3\u679c\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7406\u89e3\u548c\u8bc4\u4f30\u7528\u4e8e\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u95ee\u9898\u7684\u5b66\u4e60\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u57fa\u7ebf\u3002"}}
{"id": "2508.05652", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05652", "abs": "https://arxiv.org/abs/2508.05652", "authors": ["Julia Ann Mathew", "Suining He"], "title": "Lessons from A Large Language Model-based Outdoor Trail Recommendation Chatbot with Retrieval Augmented Generation", "comment": "4 pages, UrbComp 2025", "summary": "The increasing popularity of outdoor recreational activities (such as hiking\nand biking) has boosted the demand for a conversational AI system to provide\ninformative and personalized suggestion on outdoor trails. Challenges arise in\nresponse to (1) how to provide accurate outdoor trail information via\nconversational AI; and (2) how to enable usable and efficient recommendation\nservices. To address above, this paper discusses the preliminary and practical\nlessons learned from developing Judy, an outdoor trail recommendation chatbot\nbased on the large language model (LLM) with retrieval augmented generation\n(RAG). To gain concrete system insights, we have performed case studies with\nthe outdoor trails in Connecticut (CT), US. We have conducted web-based data\ncollection, outdoor trail data management, and LLM model performance studies on\nthe RAG-based recommendation. Our experimental results have demonstrated the\naccuracy, effectiveness, and usability of Judy in recommending outdoor trails\nbased on the LLM with RAG.", "AI": {"tldr": "This paper discusses the preliminary and practical lessons learned from developing Judy, an outdoor trail recommendation chatbot based on the large language model (LLM) with retrieval augmented generation (RAG).", "motivation": "The increasing popularity of outdoor recreational activities has boosted the demand for a conversational AI system to provide informative and personalized suggestion on outdoor trails. Challenges arise in response to how to provide accurate outdoor trail information via conversational AI and how to enable usable and efficient recommendation services.", "method": "developing Judy, an outdoor trail recommendation chatbot based on the large language model (LLM) with retrieval augmented generation (RAG).", "result": "accuracy, effectiveness, and usability of Judy in recommending outdoor trails based on the LLM with RAG.", "conclusion": "The experimental results have demonstrated the accuracy, effectiveness, and usability of Judy in recommending outdoor trails based on the LLM with RAG."}}
{"id": "2508.05843", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05843", "abs": "https://arxiv.org/abs/2508.05843", "authors": ["Miles Gilberti", "Shane Storks", "Huteng Dai"], "title": "Discovering Properties of Inflectional Morphology in Neural Emergent Communication", "comment": null, "summary": "Emergent communication (EmCom) with deep neural network-based agents promises\nto yield insights into the nature of human language, but remains focused\nprimarily on a few subfield-specific goals and metrics that prioritize\ncommunication schemes which represent attributes with unique characters\none-to-one and compose them syntactically. We thus reinterpret a common EmCom\nsetting, the attribute-value reconstruction game, by imposing a\nsmall-vocabulary constraint to simulate double articulation, and formulating a\nnovel setting analogous to naturalistic inflectional morphology (enabling\nmeaningful comparison to natural language communication schemes). We develop\nnew metrics and explore variations of this game motivated by real properties of\ninflectional morphology: concatenativity and fusionality. Through our\nexperiments, we discover that simulated phonological constraints encourage\nconcatenative morphology, and emergent languages replicate the tendency of\nnatural languages to fuse grammatical attributes.", "AI": {"tldr": "The paper explores emergent communication in deep neural networks, finding that phonological constraints encourage concatenative morphology, and emergent languages tend to fuse grammatical attributes, similar to natural languages.", "motivation": "Emergent communication (EmCom) with deep neural network-based agents promises to yield insights into the nature of human language, but remains focused primarily on a few subfield-specific goals and metrics that prioritize communication schemes which represent attributes with unique characters one-to-one and compose them syntactically.", "method": "Reinterpret a common EmCom setting, the attribute-value reconstruction game, by imposing a small-vocabulary constraint to simulate double articulation, and formulating a novel setting analogous to naturalistic inflectional morphology.", "result": "Simulated phonological constraints encourage concatenative morphology", "conclusion": "Emergent languages replicate the tendency of natural languages to fuse grammatical attributes."}}
{"id": "2508.05783", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05783", "abs": "https://arxiv.org/abs/2508.05783", "authors": ["Mengyu Li", "Guoyao Shen", "Chad W. Farris", "Xin Zhang"], "title": "Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging Tasks", "comment": "30 pages, 8 figures, 7 tables", "summary": "Machine learning using transformers has shown great potential in medical\nimaging, but its real-world applicability remains limited due to the scarcity\nof annotated data. In this study, we propose a practical framework for the\nfew-shot deployment of pretrained MRI transformers in diverse brain imaging\ntasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a\nlarge-scale, multi-cohort brain MRI dataset comprising over 31 million slices,\nwe obtain highly transferable latent representations that generalize well\nacross tasks and datasets. For high-level tasks such as classification, a\nfrozen MAE encoder combined with a lightweight linear head achieves\nstate-of-the-art accuracy in MRI sequence identification with minimal\nsupervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a\nhybrid architecture that fuses multiscale CNN features with pretrained MAE\nembeddings. This model consistently outperforms other strong baselines in both\nskull stripping and multi-class anatomical segmentation under data-limited\nconditions. With extensive quantitative and qualitative evaluations, our\nframework demonstrates efficiency, stability, and scalability, suggesting its\nsuitability for low-resource clinical environments and broader neuroimaging\napplications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5728\u4e0d\u540c\u7684\u8111\u90e8\u6210\u50cf\u4efb\u52a1\u4e2d\u5bf9\u9884\u8bad\u7ec3\u7684MRI transformers\u8fdb\u884c\u5c0f\u6837\u672c\u90e8\u7f72\u7684\u5b9e\u7528\u6846\u67b6\u3002", "motivation": "\u57fa\u4e8etransformers\u7684\u673a\u5668\u5b66\u4e60\u5728\u533b\u5b66\u6210\u50cf\u4e2d\u663e\u793a\u51fa\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u5e26\u6ce8\u91ca\u7684\u6570\u636e\u7a00\u7f3a\uff0c\u5176\u5b9e\u9645\u5e94\u7528\u4ecd\u7136\u53d7\u5230\u9650\u5236\u3002\u56e0\u6b64\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4e0d\u540c\u7684\u8111\u90e8\u6210\u50cf\u4efb\u52a1\u4e2d\u5bf9\u9884\u8bad\u7ec3\u7684MRI transformers\u8fdb\u884c\u5c0f\u6837\u672c\u90e8\u7f72\u3002", "method": "\u5229\u7528\u5927\u89c4\u6a21\u591a\u961f\u5217\u8111\u90e8MRI\u6570\u636e\u96c6\u4e0a\u7684Masked Autoencoder (MAE) \u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u83b7\u5f97\u4e86\u9ad8\u5ea6\u53ef\u8f6c\u79fb\u7684\u6f5c\u5728\u8868\u5f81\uff0c\u8be5\u8868\u5f81\u53ef\u4ee5\u5728\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e4b\u95f4\u5f88\u597d\u5730\u6cdb\u5316\u3002\u5bf9\u4e8e\u5206\u7c7b\u7b49\u9ad8\u7ea7\u4efb\u52a1\uff0c\u51bb\u7ed3\u7684MAE\u7f16\u7801\u5668\u4e0e\u8f7b\u91cf\u7ea7\u7ebf\u6027\u5934\u76f8\u7ed3\u5408\uff0c\u4ee5\u6700\u5c0f\u7684\u76d1\u7763\u5b9e\u73b0\u4e86MRI\u5e8f\u5217\u8bc6\u522b\u4e2d\u7684\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\u3002\u5bf9\u4e8e\u5206\u5272\u7b49\u4f4e\u7ea7\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86MAE-FUnet\uff0c\u8fd9\u662f\u4e00\u79cd\u6df7\u5408\u67b6\u6784\uff0c\u5b83\u5c06\u591a\u5c3a\u5ea6CNN\u7279\u5f81\u4e0e\u9884\u8bad\u7ec3\u7684MAE\u5d4c\u5165\u878d\u5408\u5728\u4e00\u8d77\u3002\u8be5\u6a21\u578b\u5728\u6570\u636e\u6709\u9650\u7684\u6761\u4ef6\u4e0b\uff0c\u5728\u9885\u9aa8\u5265\u79bb\u548c\u591a\u7c7b\u89e3\u5256\u5206\u5272\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u5f3a\u5927\u7684\u57fa\u7ebf\u3002", "result": "\u5bf9\u4e8e\u5206\u7c7b\u7b49\u9ad8\u7ea7\u4efb\u52a1\uff0c\u51bb\u7ed3\u7684MAE\u7f16\u7801\u5668\u4e0e\u8f7b\u91cf\u7ea7\u7ebf\u6027\u5934\u76f8\u7ed3\u5408\uff0c\u4ee5\u6700\u5c0f\u7684\u76d1\u7763\u5b9e\u73b0\u4e86MRI\u5e8f\u5217\u8bc6\u522b\u4e2d\u7684\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\u3002\u5bf9\u4e8e\u5206\u5272\u7b49\u4f4e\u7ea7\u4efb\u52a1\uff0cMAE-FUnet\u5728\u6570\u636e\u6709\u9650\u7684\u6761\u4ef6\u4e0b\uff0c\u5728\u9885\u9aa8\u5265\u79bb\u548c\u591a\u7c7b\u89e3\u5256\u5206\u5272\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u5f3a\u5927\u7684\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u6548\u7387\u3001\u7a33\u5b9a\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u8868\u660e\u5176\u9002\u7528\u4e8e\u8d44\u6e90\u532e\u4e4f\u7684\u4e34\u5e8a\u73af\u5883\u548c\u66f4\u5e7f\u6cdb\u7684\u795e\u7ecf\u5f71\u50cf\u5e94\u7528\u3002"}}
{"id": "2508.05888", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.05888", "abs": "https://arxiv.org/abs/2508.05888", "authors": ["Sahil Bansal", "Sai Shruthi Sistla", "Aarti Arikatala", "Sebastian Schreiber"], "title": "Planning Agents on an Ego-Trip: Leveraging Hybrid Ego-Graph Ensembles for Improved Tool Retrieval in Enterprise Task Planning", "comment": null, "summary": "Effective tool retrieval is essential for AI agents to select from a vast\narray of tools when identifying and planning actions in the context of complex\nuser queries. Despite its central role in planning, this aspect remains\nunderexplored in the literature. Traditional approaches rely primarily on\nsimilarities between user queries and tool descriptions, which significantly\nlimits retrieval accuracy, specifically when handling multi-step user requests.\nTo address these limitations, we propose a Knowledge Graph (KG)-based tool\nretrieval framework that captures the semantic relationships between tools and\ntheir functional dependencies. Our retrieval algorithm leverages ensembles of\n1-hop ego tool graphs to model direct and indirect connections between tools,\nenabling more comprehensive and contextual tool selection for multi-step tasks.\nWe evaluate our approach on a synthetically generated internal dataset across\nsix defined user classes, extending previous work on coherent dialogue\nsynthesis and too retrieval benchmarks. Results demonstrate that our tool\ngraph-based method achieves 91.85% tool coverage on the micro-average Complete\nRecall metric, compared to 89.26% for re-ranked semantic-lexical hybrid\nretrieval, the strongest non-KG baseline in our experiments. These findings\nsupport our hypothesis that the structural information in the KG provides\ncomplementary signals to pure similarity matching, particularly for queries\nrequiring sequential tool composition.", "AI": {"tldr": "This paper introduces a KG-based tool retrieval framework that captures the semantic relationships between tools and their functional dependencies to address the limitations of traditional approaches.", "motivation": "Traditional approaches rely primarily on similarities between user queries and tool descriptions, which significantly limits retrieval accuracy, specifically when handling multi-step user requests.", "method": "KG-based tool retrieval framework that captures the semantic relationships between tools and their functional dependencies. Our retrieval algorithm leverages ensembles of 1-hop ego tool graphs to model direct and indirect connections between tools", "result": "tool graph-based method achieves 91.85% tool coverage on the micro-average Complete Recall metric, compared to 89.26% for re-ranked semantic-lexical hybrid retrieval", "conclusion": "KG provides complementary signals to pure similarity matching, particularly for queries requiring sequential tool composition."}}
{"id": "2508.05836", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05836", "abs": "https://arxiv.org/abs/2508.05836", "authors": ["Rituparna Datta", "Nibir Chandra Mandal"], "title": "An Effective Approach for Node Classification in Textual Graphs", "comment": null, "summary": "Textual Attribute Graphs (TAGs) are critical for modeling complex networks\nlike citation networks, but effective node classification remains challenging\ndue to difficulties in integrating rich semantics from text with structural\ngraph information. Existing methods often struggle with capturing nuanced\ndomain-specific terminology, modeling long-range dependencies, adapting to\ntemporal evolution, and scaling to massive datasets. To address these issues,\nwe propose a novel framework that integrates TAPE (Text-Attributed Graph\nRepresentation Enhancement) with Graphormer. Our approach leverages a large\nlanguage model (LLM), specifically ChatGPT, within the TAPE framework to\ngenerate semantically rich explanations from paper content, which are then\nfused into enhanced node representations. These embeddings are combined with\nstructural features using a novel integration layer with learned attention\nweights. Graphormer's path-aware position encoding and multi-head attention\nmechanisms are employed to effectively capture long-range dependencies across\nthe citation network. We demonstrate the efficacy of our framework on the\nchallenging ogbn-arxiv dataset, achieving state-of-the-art performance with a\nclassification accuracy of 0.772, significantly surpassing the best GCN\nbaseline of 0.713. Our method also yields strong results in precision (0.671),\nrecall (0.577), and F1-score (0.610). We validate our approach through\ncomprehensive ablation studies that quantify the contribution of each\ncomponent, demonstrating the synergy between semantic and structural\ninformation. Our framework provides a scalable and robust solution for node\nclassification in dynamic TAGs, offering a promising direction for future\nresearch in knowledge systems and scientific discovery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86TAPE\uff08\u6587\u672c\u5c5e\u6027\u56fe\u8868\u793a\u589e\u5f3a\uff09\u4e0eGraphormer\uff0c\u7528\u4e8e\u6587\u672c\u5c5e\u6027\u56fe\u4e2d\u7684\u8282\u70b9\u5206\u7c7b\u3002", "motivation": "\u7531\u4e8e\u96be\u4ee5\u5c06\u6587\u672c\u4e2d\u7684\u4e30\u5bcc\u8bed\u4e49\u4e0e\u7ed3\u6784\u56fe\u4fe1\u606f\u96c6\u6210\uff0c\u56e0\u6b64\u6709\u6548\u7684\u8282\u70b9\u5206\u7c7b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u96be\u4ee5\u6355\u83b7\u7ec6\u5fae\u7684\u9886\u57df\u7279\u5b9a\u672f\u8bed\uff0c\u5efa\u6a21\u8fdc\u7a0b\u4f9d\u8d56\u5173\u7cfb\uff0c\u9002\u5e94\u65f6\u95f4\u6f14\u53d8\u4ee5\u53ca\u6269\u5c55\u5230\u6d77\u91cf\u6570\u636e\u96c6\u3002", "method": "\u96c6\u6210\u4e86TAPE\uff08\u6587\u672c\u5c5e\u6027\u56fe\u8868\u793a\u589e\u5f3a\uff09\u4e0eGraphormer", "result": "\u5728ogbn-arxiv\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5206\u7c7b\u7cbe\u5ea6\u4e3a0.772\uff0c\u663e\u8457\u8d85\u8fc7\u4e86\u6700\u4f73GCN\u57fa\u7ebf0.713\u3002\u8be5\u65b9\u6cd5\u5728\u7cbe\u786e\u7387\uff080.671\uff09\uff0c\u53ec\u56de\u7387\uff080.577\uff09\u548cF1-score\uff080.610\uff09\u65b9\u9762\u4e5f\u4ea7\u751f\u4e86\u5f3a\u5927\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u52a8\u6001TAG\u4e2d\u7684\u8282\u70b9\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u77e5\u8bc6\u7cfb\u7edf\u548c\u79d1\u5b66\u53d1\u73b0\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u65b9\u5411\u3002"}}
{"id": "2508.05654", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05654", "abs": "https://arxiv.org/abs/2508.05654", "authors": ["Leonardo Santiago Benitez Pereira", "Robinson Pizzio", "Samir Bonho"], "title": "Comparison of Information Retrieval Techniques Applied to IT Support Tickets", "comment": null, "summary": "Institutions dependent on IT services and resources acknowledge the crucial\nsignificance of an IT help desk system, that act as a centralized hub\nconnecting IT staff and users for service requests. Employing various Machine\nLearning models, these IT help desk systems allow access to corrective actions\nused in the past, but each model has different performance when applied to\ndifferent datasets. This work compares eleven Information Retrieval techniques\nin a dataset of IT support tickets, with the goal of implementing a software\nthat facilitates the work of Information Technology support analysts. The best\nresults were obtained with the Sentence-BERT technique, in its multi-language\nvariation distilluse-base-multilingual-cased-v1, where 78.7% of the\nrecommendations made by the model were considered relevant. TF-IDF (69.0%),\nWord2vec (68.7%) and LDA (66.3%) techniques also had consistent results.\nFurthermore, the used datasets and essential parts of coding have been\npublished and made open source. It also demonstrated the practicality of a\nsupport ticket recovery system by implementing a minimal viable prototype, and\ndescribed in detail the implementation of the system. Finally, this work\nproposed a novel metric for comparing the techniques, whose aim is to closely\nreflect the perception of the IT analysts about the retrieval quality.", "AI": {"tldr": "This paper compares Information Retrieval techniques for IT support tickets, finding Sentence-BERT to be the most effective. It also provides a prototype system and a new evaluation metric.", "motivation": "IT help desk systems are crucial for institutions dependent on IT services, and Machine Learning models can improve their performance, but each model performs differently on different datasets.", "method": "Compared eleven Information Retrieval techniques on an IT support ticket dataset.", "result": "Sentence-BERT achieved the best results (78.7% relevant recommendations), followed by TF-IDF (69.0%), Word2vec (68.7%), and LDA (66.3%). Datasets and code are open source. A prototype was implemented.", "conclusion": "Sentence-BERT is the best technique for IT support ticket retrieval, achieving 78.7% relevant recommendations. A prototype system was implemented and a novel metric for evaluating retrieval quality was proposed."}}
{"id": "2508.05880", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05880", "abs": "https://arxiv.org/abs/2508.05880", "authors": ["Sree Bhattacharyya", "Lucas Craig", "Tharun Dilliraj", "Jia Li", "James Z. Wang"], "title": "Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models", "comment": null, "summary": "Affective Computing has been established as a crucial field of inquiry to\nadvance the holistic development of Artificial Intelligence (AI) systems.\nFoundation models -- especially Large Language Models (LLMs) -- have been\nevaluated, trained, or instruction-tuned in several past works, to become\nbetter predictors or generators of emotion. Most of these studies, however,\napproach emotion-related tasks in a supervised manner, assessing or training\nthe capabilities of LLMs using discrete emotion labels associated with stimuli\n(e.g., text, images, video, audio). Evaluation studies, in particular, have\noften been limited to standard and superficial emotion-related tasks, such as\nthe recognition of evoked or expressed emotions. In this paper, we move beyond\nsurface-level emotion tasks to investigate how LLMs reason about emotions\nthrough cognitive dimensions. Drawing from cognitive appraisal theory, we\nexamine whether LLMs produce coherent and plausible cognitive reasoning when\nreasoning about emotionally charged stimuli. We introduce a large-scale\nbenchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal\ncognitive structures implicitly used by LLMs for emotional reasoning. Through a\nplethora of evaluation experiments and analysis, we seek to answer: (a) Are\nmodels more likely to implicitly rely on specific cognitive appraisal\ndimensions?, (b) What cognitive dimensions are important for characterizing\nspecific emotions?, and, (c) Can the internal representations of different\nemotion categories in LLMs be interpreted through cognitive appraisal\ndimensions? Our results and analyses reveal diverse reasoning patterns across\ndifferent LLMs. Our benchmark and code will be made publicly available.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6765\u8bc4\u4f30LLM\u7684\u60c5\u611f\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u4e0d\u540c\u7684LLM\u5177\u6709\u4e0d\u540c\u7684\u63a8\u7406\u6a21\u5f0f\u3002", "motivation": "\u8fc7\u53bb\u7684\u5de5\u4f5c\u4e3b\u8981\u4ee5\u76d1\u7763\u65b9\u5f0f\u5904\u7406\u60c5\u611f\u76f8\u5173\u4efb\u52a1\uff0c\u5e76\u4e14\u8bc4\u4f30\u7814\u7a76\u901a\u5e38\u4ec5\u9650\u4e8e\u6807\u51c6\u548c\u8868\u9762\u60c5\u611f\u76f8\u5173\u4efb\u52a1\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76LLM\u5982\u4f55\u901a\u8fc7\u8ba4\u77e5\u7ef4\u5ea6\u6765\u63a8\u7406\u60c5\u611f\uff0c\u8d85\u8d8a\u8868\u9762\u5c42\u6b21\u7684\u60c5\u611f\u4efb\u52a1\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aCoRE\u7684\u5927\u89c4\u6a21\u8ba4\u77e5\u60c5\u611f\u63a8\u7406\u57fa\u51c6\u6765\u8bc4\u4f30LLM\u5728\u60c5\u611f\u63a8\u7406\u4e2d\u4f7f\u7528\u7684\u5185\u90e8\u8ba4\u77e5\u7ed3\u6784\u3002", "result": "\u7ed3\u679c\u548c\u5206\u6790\u63ed\u793a\u4e86\u4e0d\u540cLLM\u4e2d\u4e0d\u540c\u7684\u63a8\u7406\u6a21\u5f0f\u3002", "conclusion": "LLMs\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u63a8\u7406\u6a21\u5f0f\u3002"}}
{"id": "2508.05813", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05813", "abs": "https://arxiv.org/abs/2508.05813", "authors": ["Raphael Du Sablon", "David Hart"], "title": "Optimization-Free Style Transfer for 3D Gaussian Splats", "comment": null, "summary": "The task of style transfer for 3D Gaussian splats has been explored in many\nprevious works, but these require reconstructing or fine-tuning the splat while\nincorporating style information or optimizing a feature extraction network on\nthe splat representation. We propose a reconstruction- and optimization-free\napproach to stylizing 3D Gaussian splats. This is done by generating a graph\nstructure across the implicit surface of the splat representation. A\nfeed-forward, surface-based stylization method is then used and interpolated\nback to the individual splats in the scene. This allows for any style image and\n3D Gaussian splat to be used without any additional training or optimization.\nThis also allows for fast stylization of splats, achieving speeds under 2\nminutes even on consumer-grade hardware. We demonstrate the quality results\nthis approach achieves and compare to other 3D Gaussian splat style transfer\nmethods. Code is publicly available at\nhttps://github.com/davidmhart/FastSplatStyler.", "AI": {"tldr": "A fast, reconstruction- and optimization-free method for stylizing 3D Gaussian splats is proposed, using a graph structure and surface-based stylization.", "motivation": "Existing style transfer methods for 3D Gaussian splats require reconstruction or fine-tuning, or optimizing a feature extraction network.", "method": "A reconstruction- and optimization-free approach is proposed, generating a graph structure across the implicit surface of the splat representation. A feed-forward, surface-based stylization method is then used and interpolated back to the individual splats.", "result": "Fast stylization of splats is achieved, with speeds under 2 minutes even on consumer-grade hardware. Quality results are demonstrated and compared to other methods.", "conclusion": "The proposed approach achieves quality results and fast stylization of splats, with speeds under 2 minutes on consumer-grade hardware, demonstrating improvements over other 3D Gaussian splat style transfer methods."}}
{"id": "2508.05996", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05996", "abs": "https://arxiv.org/abs/2508.05996", "authors": ["Kaitao Chen", "Mianxin Liu", "Daoming Zong", "Chaoyue Ding", "Shaohao Rui", "Yankai Jiang", "Mu Zhou", "Xiaosong Wang"], "title": "Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making", "comment": "14 pages, 4 figures", "summary": "Complex medical decision-making involves cooperative workflows operated by\ndifferent clinicians. Designing AI multi-agent systems can expedite and augment\nhuman-level clinical decision-making. Existing multi-agent researches primarily\nfocus on language-only tasks, yet their extension to multimodal scenarios\nremains challenging. A blind combination of diverse vision-language models\n(VLMs) can amplify an erroneous outcome interpretation. VLMs in general are\nless capable in instruction following and importantly self-reflection, compared\nto large language models (LLMs) of comparable sizes. This disparity largely\nconstrains VLMs' ability in cooperative workflows. In this study, we propose\nMedOrch, a mediator-guided multi-agent collaboration framework for medical\nmultimodal decision-making. MedOrch employs an LLM-based mediator agent that\nenables multiple VLM-based expert agents to exchange and reflect on their\noutputs towards collaboration. We utilize multiple open-source general-purpose\nand domain-specific VLMs instead of costly GPT-series models, revealing the\nstrength of heterogeneous models. We show that the collaboration within\ndistinct VLM-based agents can surpass the capabilities of any individual agent.\nWe validate our approach on five medical vision question answering benchmarks,\ndemonstrating superior collaboration performance without model training. Our\nfindings underscore the value of mediator-guided multi-agent collaboration in\nadvancing medical multimodal intelligence. Our code will be made publicly\navailable.", "AI": {"tldr": "MedOrch, a mediator-guided multi-agent framework, improves medical multimodal decision-making by enabling VLMs to collaborate, outperforming individual agents without training.", "motivation": "Existing multi-agent researches primarily focus on language-only tasks, with challenges in extending to multimodal scenarios. Blind combination of VLMs can amplify erroneous interpretations, and VLMs are less capable in instruction following and self-reflection compared to LLMs.", "method": "MedOrch: LLM-based mediator agent enables multiple VLM-based expert agents to exchange and reflect on outputs.", "result": "Collaboration within distinct VLM-based agents surpasses individual agent capabilities on five medical vision question answering benchmarks without model training.", "conclusion": "Mediator-guided multi-agent collaboration advances medical multimodal intelligence, surpassing individual agent capabilities without training, validated on five medical vision question answering benchmarks."}}
{"id": "2508.05876", "categories": ["cs.LG", "astro-ph.EP", "astro-ph.IM", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.05876", "abs": "https://arxiv.org/abs/2508.05876", "authors": ["Francesca Ferrara", "Lander W. Schillinger Arana", "Florian D\u00f6rfler", "Sarah H. Q. Li"], "title": "A Markov Decision Process Framework for Early Maneuver Decisions in Satellite Collision Avoidance", "comment": "16 pages, 13 figures, submitted to the 2025 Astrodynamics Specialist\n  Conference", "summary": "This work presents a Markov decision process (MDP) framework to model\ndecision-making for collision avoidance maneuver (CAM) and a reinforcement\nlearning policy gradient (RL-PG) algorithm to train an autonomous guidance\npolicy using historic CAM data. In addition to maintaining acceptable collision\nrisks, this approach seeks to minimize the average fuel consumption of CAMs by\nmaking early maneuver decisions. We model CAM as a continuous state, discrete\naction and finite horizon MDP, where the critical decision is determining when\nto initiate the maneuver. The MDP model also incorporates analytical models for\nconjunction risk, propellant consumption, and transit orbit geometry. The\nMarkov policy effectively trades-off maneuver delay-which improves the\nreliability of conjunction risk indicators-with propellant consumption-which\nincreases with decreasing maneuver time. Using historical data of tracked\nconjunction events, we verify this framework and conduct an extensive ablation\nstudy on the hyper-parameters used within the MDP. On synthetic conjunction\nevents, the trained policy significantly minimizes both the overall and average\npropellant consumption per CAM when compared to a conventional cut-off policy\nthat initiates maneuvers 24 hours before the time of closest approach (TCA). On\nhistorical conjunction events, the trained policy consumes more propellant\noverall but reduces the average propellant consumption per CAM. For both\nhistorical and synthetic conjunction events, the trained policy achieves equal\nif not higher overall collision risk guarantees.", "AI": {"tldr": "\u4f7f\u7528 MDP \u6846\u67b6\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u81ea\u4e3b\u5236\u5bfc\u7b56\u7565\uff0c\u4ee5\u51cf\u5c11\u78b0\u649e\u98ce\u9669\u548c\u71c3\u6599\u6d88\u8017\u3002", "motivation": "\u901a\u8fc7\u5c3d\u65e9\u505a\u51fa\u673a\u52a8\u51b3\u7b56\uff0c\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11 CAM \u7684\u5e73\u5747\u71c3\u6599\u6d88\u8017\u3002", "method": "\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b (MDP) \u6846\u67b6\u548c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u68af\u5ea6 (RL-PG) \u7b97\u6cd5", "result": "\u5728\u5408\u6210\u7684\u5408\u4e8b\u4ef6\u4e2d\uff0c\u8be5\u7b56\u7565\u663e\u8457\u964d\u4f4e\u4e86\u6bcf\u6b21 CAM \u7684\u6574\u4f53\u548c\u5e73\u5747\u71c3\u6599\u6d88\u8017\uff0c\u800c\u5728\u5386\u53f2\u5408\u4e8b\u4ef6\u4e2d\uff0c\u8be5\u7b56\u7565\u6d88\u8017\u4e86\u66f4\u591a\u7684\u71c3\u6599\uff0c\u4f46\u964d\u4f4e\u4e86\u6bcf\u6b21 CAM \u7684\u5e73\u5747\u71c3\u6599\u6d88\u8017\u3002\u5bf9\u4e8e\u5386\u53f2\u548c\u5408\u6210\u7684\u5408\u4e8b\u4ef6\uff0c\u8be5\u7b56\u7565\u5b9e\u73b0\u4e86\u76f8\u7b49\u751a\u81f3\u66f4\u9ad8\u7684\u603b\u4f53\u78b0\u649e\u98ce\u9669\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u7b56\u7565\u5728\u4fdd\u8bc1\u6216\u63d0\u9ad8\u603b\u4f53\u78b0\u649e\u98ce\u9669\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6bcf\u6b21 CAM \u7684\u6574\u4f53\u548c\u5e73\u5747\u71c3\u6599\u6d88\u8017\u3002"}}
{"id": "2508.05657", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05657", "abs": "https://arxiv.org/abs/2508.05657", "authors": ["Haozhe Xu", "Xiaohua Wang", "Changze Lv", "Xiaoqing Zheng"], "title": "Beyond Single Labels: Improving Conversational Recommendation through LLM-Powered Data Augmentation", "comment": null, "summary": "Conversational recommender systems (CRSs) enhance recommendation quality by\nengaging users in multi-turn dialogues, capturing nuanced preferences through\nnatural language interactions. However, these systems often face the false\nnegative issue, where items that a user might like are incorrectly labeled as\nnegative during training, leading to suboptimal recommendations.Expanding the\nlabel set through data augmentation presents an intuitive solution but faces\nthe challenge of balancing two key aspects: ensuring semantic relevance and\npreserving the collaborative information inherent in CRS datasets. To address\nthese issues, we propose a novel data augmentation framework that first\nleverages an LLM-based semantic retriever to identify diverse and semantically\nrelevant items, which are then filtered by a relevance scorer to remove noisy\ncandidates. Building on this, we introduce a two-stage training strategy\nbalancing semantic relevance and collaborative information. Extensive\nexperiments on two benchmark datasets and user simulators demonstrate\nsignificant and consistent performance improvements across various\nrecommenders, highlighting the effectiveness of our approach in advancing CRS\nperformance.", "AI": {"tldr": "This paper introduces a data augmentation framework to improve conversational recommender systems by addressing the false negative issue, leading to better performance.", "motivation": "CRSs face the false negative issue, where items a user might like are incorrectly labeled as negative, leading to suboptimal recommendations. Expanding the label set through data augmentation is challenging due to the need to balance semantic relevance and collaborative information.", "method": "A novel data augmentation framework with LLM-based semantic retriever and relevance scorer, combined with a two-stage training strategy.", "result": "Significant and consistent performance improvements on two benchmark datasets and user simulators.", "conclusion": "The proposed data augmentation framework improves CRS performance by addressing the false negative issue. Experiments show significant and consistent performance improvements."}}
{"id": "2508.05909", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05909", "abs": "https://arxiv.org/abs/2508.05909", "authors": ["Zhanghao Hu", "Qinglin Zhu", "Siya Qi", "Yulan He", "Hanqi Yan", "Lin Gui"], "title": "Spectrum Projection Score: Aligning Retrieved Summaries with Reader Models in Retrieval-Augmented Generation", "comment": null, "summary": "Large Language Models (LLMs) have shown improved generation performance\nthrough retrieval-augmented generation (RAG) following the retriever-reader\nparadigm, which supplements model inputs with externally retrieved knowledge.\nHowever, prior work often evaluates RAG holistically, assessing the retriever\nand reader jointly, making it difficult to isolate the true contribution of\nretrieval, particularly given the prompt sensitivity of LLMs used as readers.\nWe introduce Spectrum Projection Score (SPS), a lightweight, supervision-free\nmetric that allows the reader to gauge the semantic alignment of a retrieved\nsummary with its hidden representation by comparing the area formed by\ngenerated tokens from the summary, and the principal directions of subspace in\nthe reader and to measure the relevance. Building on SPS we present xCompress,\nan inference time controller framework that dynamically samples, ranks, and\ncompresses retrieval summary candidates. Extensive experiments on five QA\nbenchmarks with four open source LLMs show that SPS not only enhances\nperformance across a range of tasks but also provides a principled perspective\non the interaction between retrieval and generation.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u5ea6\u91cf\u65b9\u6cd5\uff08SPS\uff09\u548c\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff08xCompress\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb RAG \u7cfb\u7edf\uff0c\u5b9e\u9a8c\u8868\u660e\u5b83\u4eec\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u5148\u524d\u7684\u5de5\u4f5c\u901a\u5e38\u5168\u9762\u5730\u8bc4\u4f30 RAG\uff0c\u5171\u540c\u8bc4\u4f30\u68c0\u7d22\u5668\u548c\u9605\u8bfb\u5668\uff0c\u56e0\u6b64\u96be\u4ee5\u5206\u79bb\u68c0\u7d22\u7684\u771f\u6b63\u8d21\u732e\uff0c\u7279\u522b\u662f\u8003\u8651\u5230\u7528\u4f5c\u9605\u8bfb\u5668\u7684 LLM \u7684\u63d0\u793a\u654f\u611f\u6027\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86 Spectrum Projection Score (SPS)\uff0c\u8fd9\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u3001\u65e0\u76d1\u7763\u7684\u6307\u6807\uff0c\u5141\u8bb8\u8bfb\u8005\u901a\u8fc7\u6bd4\u8f83\u6458\u8981\u751f\u6210\u7684 token \u5f62\u6210\u7684\u533a\u57df\u4e0e\u8bfb\u8005\u5b50\u7a7a\u95f4\u4e2d\u7684\u4e3b\u65b9\u5411\u6765\u8bc4\u4f30\u68c0\u7d22\u5230\u7684\u6458\u8981\u4e0e\u5176\u9690\u85cf\u8868\u793a\u7684\u8bed\u4e49\u5bf9\u9f50\uff0c\u5e76\u8861\u91cf\u76f8\u5173\u6027\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 xCompress\uff0c\u8fd9\u662f\u4e00\u4e2a\u63a8\u7406\u65f6\u95f4\u63a7\u5236\u5668\u6846\u67b6\uff0c\u53ef\u4ee5\u52a8\u6001\u5730\u5bf9\u68c0\u7d22\u6458\u8981\u5019\u9009\u8fdb\u884c\u91c7\u6837\u3001\u6392\u5e8f\u548c\u538b\u7f29\u3002", "result": "\u5728\u56db\u4e2a\u5f00\u6e90 LLM \u7684\u4e94\u4e2a QA \u57fa\u51c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e SPS \u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5404\u79cd\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u800c\u4e14\u4e3a\u68c0\u7d22\u548c\u751f\u6210\u4e4b\u95f4\u7684\u4ea4\u4e92\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u7684\u89c6\u89d2\u3002", "conclusion": "SPS \u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5404\u79cd\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u800c\u4e14\u4e3a\u68c0\u7d22\u548c\u751f\u6210\u4e4b\u95f4\u7684\u4ea4\u4e92\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u7684\u89c6\u89d2\u3002"}}
{"id": "2508.05819", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05819", "abs": "https://arxiv.org/abs/2508.05819", "authors": ["Jong-Ik Park", "Carlee Joe-Wong", "Gary K. Fedder"], "title": "MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown Camera Poses", "comment": null, "summary": "Neural Radiance Fields (NeRF) methods excel at 3D reconstruction from\nmultiple 2D images, even those taken with unknown camera poses. However, they\nstill miss the fine-detailed structures that matter in industrial inspection,\ne.g., detecting sub-micron defects on a production line or analyzing chips with\nScanning Electron Microscopy (SEM). In these scenarios, the sensor resolution\nis fixed and compute budgets are tight, so the only way to expose fine\nstructure is to add zoom-in images; yet, this breaks the multi-view consistency\nthat pose-free NeRF training relies on. We propose Multi-Zoom Enhanced NeRF\n(MZEN), the first NeRF framework that natively handles multi-zoom image sets.\nMZEN (i) augments the pin-hole camera model with an explicit, learnable zoom\nscalar that scales the focal length, and (ii) introduces a novel pose strategy:\nwide-field images are solved first to establish a global metric frame, and\nzoom-in images are then pose-primed to the nearest wide-field counterpart via a\nzoom-consistent crop-and-match procedure before joint refinement. Across eight\nforward-facing scenes$\\unicode{x2013}$synthetic TCAD models, real SEM of\nmicro-structures, and BLEFF objects$\\unicode{x2013}$MZEN consistently\noutperforms pose-free baselines and even high-resolution variants, boosting\nPSNR by up to $28 \\%$, SSIM by $10 \\%$, and reducing LPIPS by up to $222 \\%$.\nMZEN, therefore, extends NeRF to real-world factory settings, preserving global\naccuracy while capturing the micron-level details essential for industrial\ninspection.", "AI": {"tldr": "MZEN is the first NeRF framework that natively handles multi-zoom image sets, extending NeRF to real-world factory settings.", "motivation": "NeRF methods miss the fine-detailed structures that matter in industrial inspection, e.g., detecting sub-micron defects on a production line or analyzing chips with Scanning Electron Microscopy (SEM). The only way to expose fine structure is to add zoom-in images; yet, this breaks the multi-view consistency that pose-free NeRF training relies on.", "method": "MZEN (i) augments the pin-hole camera model with an explicit, learnable zoom scalar that scales the focal length, and (ii) introduces a novel pose strategy: wide-field images are solved first to establish a global metric frame, and zoom-in images are then pose-primed to the nearest wide-field counterpart via a zoom-consistent crop-and-match procedure before joint refinement.", "result": "MZEN consistently outperforms pose-free baselines and even high-resolution variants, boosting PSNR by up to $28 \\%, SSIM by $10 \\%, and reducing LPIPS by up to $222 \\%.", "conclusion": "MZEN extends NeRF to real-world factory settings, preserving global accuracy while capturing the micron-level details essential for industrial inspection."}}
{"id": "2508.06042", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06042", "abs": "https://arxiv.org/abs/2508.06042", "authors": ["Daechul Ahn", "San Kim", "Jonghyun Choi"], "title": "Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning", "comment": "COLM 2025", "summary": "Large Language Models (LLMs) have recently demonstrated impressive action\nsequence prediction capabilities but often struggle with dynamic, long-horizon\ntasks such as real-time strategic games. In a game such as StarCraftII (SC2),\nagents need to manage resource constraints and adapt to evolving battlefield\nsituations in a partially observable environment. This often overwhelms\nexisiting LLM-based approaches. To address these challenges, we propose a\nhierarchical multi-agent framework that employs specialized imitation learning\nagents under a meta-controller called Strategic Planner (SP). By expert\ndemonstrations, each specialized agent learns a distinctive strategy, such as\naerial support or defensive maneuvers, and produces coherent, structured\nmultistep action sequences. The SP then orchestrates these proposals into a\nsingle, environmentally adaptive plan that ensures local decisions aligning\nwith long-term strategies. We call this HIMA (Hierarchical Imitation\nMulti-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that\nencompasses all race match combinations in SC2. Our empirical results show that\nHIMA outperforms state of the arts in strategic clarity, adaptability, and\ncomputational efficiency, underscoring the potential of combining specialized\nimitation modules with meta-level orchestration to develop more robust,\ngeneral-purpose AI agents.", "AI": {"tldr": "HIMA \u662f\u4e00\u79cd\u7528\u4e8e\u661f\u9645\u4e89\u9738 II \u7684\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5b83\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u56e0\u4e3a\u5b83\u7ed3\u5408\u4e86\u4e13\u4e1a\u6a21\u4eff\u6a21\u5757\u4e0e\u5143\u7ea7\u522b\u7f16\u6392\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6700\u8fd1\u5c55\u793a\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u52a8\u4f5c\u5e8f\u5217\u9884\u6d4b\u80fd\u529b\uff0c\u4f46\u5e38\u5e38\u96be\u4ee5\u5e94\u5bf9\u52a8\u6001\u7684\u3001\u957f\u671f\u7684\u4efb\u52a1\uff0c\u4f8b\u5982\u5b9e\u65f6\u6218\u7565\u6e38\u620f\u3002\u5728\u661f\u9645\u4e89\u9738 II (SC2) \u7b49\u6e38\u620f\u4e2d\uff0c\u667a\u80fd\u4f53\u9700\u8981\u7ba1\u7406\u8d44\u6e90\u7ea6\u675f\u5e76\u9002\u5e94\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u4e2d\u4e0d\u65ad\u53d8\u5316\u7684\u6218\u573a\u60c5\u51b5\u3002\u8fd9\u901a\u5e38\u4f1a\u4f7f\u73b0\u6709\u7684\u57fa\u4e8e LLM \u7684\u65b9\u6cd5\u4e0d\u582a\u91cd\u8d1f\u3002", "method": "\u4e00\u79cd\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u5143\u63a7\u5236\u5668\u4e0b\u7684\u4e13\u4e1a\u6a21\u4eff\u5b66\u4e60\u667a\u80fd\u4f53\uff0c\u79f0\u4e3a\u6218\u7565\u89c4\u5212\u5668 (SP)\u3002", "result": "HIMA \u5728\u6218\u7565\u6e05\u6670\u5ea6\u3001\u9002\u5e94\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f", "conclusion": "HIMA\u5728\u6218\u7565\u6e05\u6670\u5ea6\u3001\u9002\u5e94\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5f3a\u8c03\u4e86\u5c06\u4e13\u4e1a\u6a21\u4eff\u6a21\u5757\u4e0e\u5143\u7ea7\u522b\u7f16\u6392\u76f8\u7ed3\u5408\u4ee5\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u901a\u7528\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.05905", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05905", "abs": "https://arxiv.org/abs/2508.05905", "authors": ["Jeffrey Uhlmann"], "title": "The Fourth State: Signed-Zero Ternary for Stable LLM Quantization (and More)", "comment": null, "summary": "Quantization is usually regarded as a means to trade quality of performance\nfor reduced compute requirements, i.e., as a suboptimal approximation. However,\nif examined in terms of a fixed overall resource budget, a very different\nperspective arises. We introduce Signed-Zero Ternary (SZT), a 2-bit\nquantization that deterministically provides gradient information with no\nforward-path penalty. Our analysis provides evidence that it may improve\ninformation density compared to non-quantized alternatives.", "AI": {"tldr": "This paper introduces a 2-bit quantization method called Signed-Zero Ternary (SZT) that improves information density and provides gradient information with no forward-path penalty.", "motivation": "Quantization is usually regarded as a means to trade quality of performance for reduced compute requirements, i.e., as a suboptimal approximation. However, if examined in terms of a fixed overall resource budget, a very different perspective arises.", "method": "introducing Signed-Zero Ternary (SZT), a 2-bit quantization", "result": "SZT deterministically provides gradient information with no forward-path penalty.", "conclusion": "Signed-Zero Ternary (SZT), a 2-bit quantization, deterministically provides gradient information with no forward-path penalty and may improve information density compared to non-quantized alternatives."}}
{"id": "2508.05660", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05660", "abs": "https://arxiv.org/abs/2508.05660", "authors": ["Aditya Nagori", "Ricardo Accorsi Casonatto", "Ayush Gautam", "Abhinav Manikantha Sai Cheruvu", "Rishikesan Kamaleswaran"], "title": "Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review", "comment": null, "summary": "The surge in scientific publications challenges traditional review methods,\ndemanding tools that integrate structured metadata with full-text analysis.\nHybrid Retrieval Augmented Generation (RAG) systems, combining graph queries\nwith vector search offer promise but are typically static, rely on proprietary\ntools, and lack uncertainty estimates. We present an agentic approach that\nencapsulates the hybrid RAG pipeline within an autonomous agent capable of (1)\ndynamically selecting between GraphRAG and VectorRAG for each query, (2)\nadapting instruction-tuned generation in real time to researcher needs, and (3)\nquantifying uncertainty during inference. This dynamic orchestration improves\nrelevance, reduces hallucinations, and promotes reproducibility.\n  Our pipeline ingests bibliometric open-access data from PubMed, arXiv, and\nGoogle Scholar APIs, builds a Neo4j citation-based knowledge graph (KG), and\nembeds full-text PDFs into a FAISS vector store (VS) using the all-MiniLM-L6-v2\nmodel. A Llama-3.3-70B agent selects GraphRAG (translating queries to Cypher\nfor KG) or VectorRAG (combining sparse and dense retrieval with re-ranking).\nInstruction tuning refines domain-specific generation, and bootstrapped\nevaluation yields standard deviation for evaluation metrics.\n  On synthetic benchmarks mimicking real-world queries, the Instruction-Tuned\nAgent with Direct Preference Optimization (DPO) outperforms the baseline,\nachieving a gain of 0.63 in VS Context Recall and a 0.56 gain in overall\nContext Precision. Additional gains include 0.24 in VS Faithfulness, 0.12 in\nboth VS Precision and KG Answer Relevance, 0.11 in overall Faithfulness score,\n0.05 in KG Context Recall, and 0.04 in both VS Answer Relevance and overall\nPrecision. These results highlight the system's improved reasoning over\nheterogeneous sources and establish a scalable framework for autonomous,\nagentic scientific discovery.", "AI": {"tldr": "This paper presents an agentic hybrid RAG system for scientific literature analysis that dynamically selects retrieval methods, adapts generation, and quantifies uncertainty, outperforming baselines on synthetic benchmarks.", "motivation": "The surge in scientific publications challenges traditional review methods, demanding tools that integrate structured metadata with full-text analysis. Hybrid Retrieval Augmented Generation (RAG) systems offer promise but are typically static, rely on proprietary tools, and lack uncertainty estimates.", "method": "An agentic approach that encapsulates the hybrid RAG pipeline within an autonomous agent capable of dynamically selecting between GraphRAG and VectorRAG, adapting instruction-tuned generation in real time, and quantifying uncertainty during inference. The pipeline ingests bibliometric open-access data, builds a Neo4j citation-based knowledge graph, and embeds full-text PDFs into a FAISS vector store. A Llama-3.3-70B agent selects GraphRAG or VectorRAG. Instruction tuning refines domain-specific generation, and bootstrapped evaluation yields standard deviation for evaluation metrics.", "result": "The Instruction-Tuned Agent with DPO outperforms the baseline, achieving a gain of 0.63 in VS Context Recall and a 0.56 gain in overall Context Precision. Additional gains include 0.24 in VS Faithfulness, 0.12 in both VS Precision and KG Answer Relevance, 0.11 in overall Faithfulness score, 0.05 in KG Context Recall, and 0.04 in both VS Answer Relevance and overall Precision.", "conclusion": "The Instruction-Tuned Agent with DPO outperforms the baseline, achieving gains in VS Context Recall, overall Context Precision, VS Faithfulness, VS Precision, KG Answer Relevance, overall Faithfulness score, KG Context Recall, VS Answer Relevance, and overall Precision. These results highlight the system's improved reasoning over heterogeneous sources and establish a scalable framework for autonomous, agentic scientific discovery."}}
{"id": "2508.05938", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; K.4"], "pdf": "https://arxiv.org/pdf/2508.05938", "abs": "https://arxiv.org/abs/2508.05938", "authors": ["Rafal Kocielnik", "Min Kim", "Penphob", "Boonyarungsrit", "Fereshteh Soltani", "Deshawn Sambrano", "Animashree Anandkumar", "R. Michael Alvarez"], "title": "Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale", "comment": "9 pages, 4 figures, 4 tables", "summary": "Detecting prosociality in text--communication intended to affirm, support, or\nimprove others' behavior--is a novel and increasingly important challenge for\ntrust and safety systems. Unlike toxic content detection, prosociality lacks\nwell-established definitions and labeled data, requiring new approaches to both\nannotation and deployment. We present a practical, three-stage pipeline that\nenables scalable, high-precision prosocial content classification while\nminimizing human labeling effort and inference costs. First, we identify the\nbest LLM-based labeling strategy using a small seed set of human-labeled\nexamples. We then introduce a human-AI refinement loop, where annotators review\nhigh-disagreement cases between GPT-4 and humans to iteratively clarify and\nexpand the task definition-a critical step for emerging annotation tasks like\nprosociality. This process results in improved label quality and definition\nalignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train\na two-stage inference system: a lightweight classifier handles high-confidence\npredictions, while only $\\sim$35\\% of ambiguous instances are escalated to\nGPT-4o. This architecture reduces inference costs by $\\sim$70% while achieving\nhigh precision ($\\sim$0.90). Our pipeline demonstrates how targeted human-AI\ninteraction, careful task formulation, and deployment-aware architecture design\ncan unlock scalable solutions for novel responsible AI tasks.", "AI": {"tldr": "A three-stage pipeline is presented for detecting prosociality in text, using human-AI interaction and a two-stage inference system to achieve high precision and reduce costs.", "motivation": "Detecting prosociality in text is a novel and increasingly important challenge for trust and safety systems. Unlike toxic content detection, prosociality lacks well-established definitions and labeled data, requiring new approaches to both annotation and deployment.", "method": "The method involves a three-stage pipeline: (1) identifying the best LLM-based labeling strategy, (2) introducing a human-AI refinement loop for clarifying the task definition, and (3) synthesizing high-quality labels using GPT-4 to train a two-stage inference system.", "result": "The pipeline achieves high precision (~0.90) and reduces inference costs by ~70%.", "conclusion": "This paper presents a practical three-stage pipeline for scalable, high-precision prosocial content classification, which minimizes human labeling effort and inference costs. The pipeline uses a human-AI refinement loop to improve label quality and definition alignment. A two-stage inference system is trained to reduce inference costs while achieving high precision."}}
{"id": "2508.05829", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05829", "abs": "https://arxiv.org/abs/2508.05829", "authors": ["Guoping Xu", "Hua-Chieh Shao", "You Zhang"], "title": "TSMS-SAM2: Multi-scale Temporal Sampling Augmentation and Memory-Splitting Pruning for Promptable Video Object Segmentation and Tracking in Surgical Scenarios", "comment": "23 pages, 5 figures", "summary": "Promptable video object segmentation and tracking (VOST) has seen significant\nadvances with the emergence of foundation models like Segment Anything Model 2\n(SAM2); however, their application in surgical video analysis remains\nchallenging due to complex motion dynamics and the redundancy of memory that\nimpedes effective learning. In this work, we propose TSMS-SAM2, a novel\nframework that enhances promptable VOST in surgical videos by addressing\nchallenges of rapid object motion and memory redundancy in SAM2. TSMS-SAM2\nintroduces two key strategies: multi-temporal-scale video sampling augmentation\nto improve robustness against motion variability, and a memory splitting and\npruning mechanism that organizes and filters past frame features for more\nefficient and accurate segmentation. Evaluated on EndoVis2017 and EndoVis2018\ndatasets, TSMS-SAM2 achieved the highest mean Dice scores of 95.24 and 86.73,\nrespectively, outperforming prior SAM-based and task-specific methods.\nExtensive ablation studies confirm the effectiveness of multiscale temporal\naugmentation and memory splitting, highlighting the framework's potential for\nrobust, efficient segmentation in complex surgical scenarios. Our source code\nwill be available at https://github.com/apple1986/TSMS-SAM2.", "AI": {"tldr": "TSMS-SAM2 improves surgical video segmentation by addressing motion dynamics and memory redundancy in SAM2 with multi-temporal sampling and memory management.", "motivation": "The application of foundation models like SAM2 in surgical video analysis is challenging due to complex motion dynamics and memory redundancy.", "method": "The paper proposes TSMS-SAM2, a framework enhancing promptable VOST in surgical videos using multi-temporal-scale video sampling augmentation and a memory splitting and pruning mechanism.", "result": "TSMS-SAM2 achieved the highest mean Dice scores of 95.24 and 86.73 on EndoVis2017 and EndoVis2018 datasets, respectively.", "conclusion": "TSMS-SAM2 achieves state-of-the-art segmentation performance on EndoVis2017 and EndoVis2018 datasets, demonstrating its potential for robust and efficient segmentation in complex surgical scenarios."}}
{"id": "2508.06060", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06060", "abs": "https://arxiv.org/abs/2508.06060", "authors": ["Sankarshan Damle", "Boi Faltings"], "title": "LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences", "comment": "Published in the Proceedings of the 28th European Conference on\n  Artificial Intelligence (ECAI 2025)", "summary": "Large Language Models (LLMs) are increasingly expected to handle complex\ndecision-making tasks, yet their ability to perform structured resource\nallocation remains underexplored. Evaluating their reasoning is also difficult\ndue to data contamination and the static nature of existing benchmarks. We\npresent a dual-purpose framework leveraging Participatory Budgeting (PB) both\nas (i) a practical setting for LLM-based resource allocation and (ii) an\nadaptive benchmark for evaluating their reasoning capabilities. We task LLMs\nwith selecting project subsets under feasibility (e.g., budget) constraints via\nthree prompting strategies: greedy selection, direct optimization, and a\nhill-climbing-inspired refinement. We benchmark LLMs' allocations against a\nutility-maximizing oracle. Interestingly, we also test whether LLMs can infer\nstructured preferences from natural-language voter input or metadata, without\nexplicit votes. By comparing allocations based on inferred preferences to those\nfrom ground-truth votes, we evaluate LLMs' ability to extract preferences from\nopen-ended input. Our results underscore the role of prompt design and show\nthat LLMs hold promise for mechanism design with unstructured inputs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u91cd\u6846\u67b6\uff0c\u5229\u7528\u53c2\u4e0e\u5f0f\u9884\u7b97\uff08PB\uff09\u4f5c\u4e3a LLM \u8d44\u6e90\u5206\u914d\u7684\u5b9e\u8df5\u73af\u5883\u548c\u8bc4\u4f30\u5176\u63a8\u7406\u80fd\u529b\u7684\u81ea\u9002\u5e94\u57fa\u51c6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8d8a\u6765\u8d8a\u88ab\u671f\u671b\u5904\u7406\u590d\u6742\u7684\u51b3\u7b56\u4efb\u52a1\uff0c\u4f46\u5b83\u4eec\u6267\u884c\u7ed3\u6784\u5316\u8d44\u6e90\u5206\u914d\u7684\u80fd\u529b\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u7531\u4e8e\u6570\u636e\u6c61\u67d3\u548c\u73b0\u6709\u57fa\u51c6\u7684\u9759\u6001\u6027\u8d28\uff0c\u8bc4\u4f30\u5b83\u4eec\u7684\u63a8\u7406\u80fd\u529b\u4e5f\u5f88\u56f0\u96be\u3002", "method": "\u901a\u8fc7\u4e09\u79cd prompting \u7b56\u7565\uff08\u8d2a\u5a6a\u9009\u62e9\u3001\u76f4\u63a5\u4f18\u5316\u548c\u722c\u5c71\u542f\u53d1\u5f0f\u4f18\u5316\uff09\u6765\u6d4b\u8bd5 LLMs \u5728\u53ef\u884c\u6027\u7ea6\u675f\u4e0b\u9009\u62e9\u9879\u76ee\u5b50\u96c6\u7684\u80fd\u529b\uff0c\u5e76\u4e0e\u6548\u7528\u6700\u5927\u5316\u7684 oracle \u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u7ed3\u679c\u5f3a\u8c03\u4e86\u63d0\u793a\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8868\u660e LLMs \u5728\u5904\u7406\u975e\u7ed3\u6784\u5316\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u4e8e\u673a\u5236\u8bbe\u8ba1\u5f88\u6709\u5e0c\u671b\u3002", "conclusion": "LLMs \u5728\u5904\u7406\u975e\u7ed3\u6784\u5316\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u4e8e\u673a\u5236\u8bbe\u8ba1\u5f88\u6709\u5e0c\u671b\u3002"}}
{"id": "2508.05915", "categories": ["cs.LG", "62M10"], "pdf": "https://arxiv.org/pdf/2508.05915", "abs": "https://arxiv.org/abs/2508.05915", "authors": ["Alex Glushkovsky"], "title": "Dual Signal Decomposition of Stochastic Time Series", "comment": "21 pages, 9 figures, 1 table", "summary": "The research paper addresses decomposition of a stochastic time series into\nthree time series representing a dual signal i.e., the mean and the dispersion,\nwith noise isolated. Decomposition is done by applying machine learning to fit\na dual signal. Machine learning minimizes the loss function which compromises\nbetween fitting the original time series and penalizing irregularities of the\ndual signal. The latter includes terms based on the first and second order\nderivatives along time. To preserve special patterns, weighting of the\nregularization components of the loss function has been introduced based on\nStatistical Process Control methodology. The proposed decomposition can be\napplied as a smoothing algorithm against the mean and dispersion of the time\nseries. By isolating noise, the proposed decomposition can be seen as a\ndenoising algorithm. Two approaches of the learning process have been\nconsidered: sequential and jointly. The former approach learns the mean signal\nfirst and then dispersion. The latter approach fits the dual signal jointly.\nJointly learning can uncover complex relationships for the time series with\nheteroskedasticity. Learning has been set by solving the direct non-linear\nunconstrained optimization problem or by applying neural networks that have\nsequential or twin output architectures. Tuning of the loss function\nhyperparameters focuses on the isolated noise to be a stationary stochastic\nprocess without autocorrelation properties. Depending on the applications, the\nhyperparameters of the learning can be tuned towards either the discrete states\nby stepped signal or smoothed series. The decomposed dual signal can be\nrepresented on the 2D space and used to learn inherent structures, to forecast\nboth mean and dispersion, or to analyze cross effects in case of multiple time\nseries.", "AI": {"tldr": "This paper presents a machine learning approach to decompose stochastic time series into mean, dispersion, and noise, which can be used for smoothing, denoising, and further analysis.", "motivation": "Decompose a stochastic time series into mean, dispersion, and isolated noise.", "method": "Machine learning is used to fit a dual signal, minimizing a loss function that balances fitting the original time series and penalizing irregularities using first and second-order derivatives, with regularization components weighted by Statistical Process Control.", "result": "A decomposition method is proposed that can smooth or denoise time series, with two learning approaches (sequential and joint) and hyperparameter tuning focusing on stationary noise without autocorrelation.", "conclusion": "The decomposed dual signal (mean and dispersion) can be represented in 2D space for learning structures, forecasting, and analyzing cross-effects in multiple time series."}}
{"id": "2508.05661", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05661", "abs": "https://arxiv.org/abs/2508.05661", "authors": ["Andre Rusli", "Shoma Ishimoto", "Sho Akiyama", "Aman Kumar Singh"], "title": "Zero-Shot Retrieval for Scalable Visual Search in a Two-Sided Marketplace", "comment": "6 pages, KDD 2025 Workshop on Two-sided Marketplace Optimization:\n  Search, Pricing, Matching & Growth (TSMO)", "summary": "Visual search offers an intuitive way for customers to explore diverse\nproduct catalogs, particularly in consumer-to-consumer (C2C) marketplaces where\nlistings are often unstructured and visually driven. This paper presents a\nscalable visual search system deployed in Mercari's C2C marketplace, where\nend-users act as buyers and sellers. We evaluate recent vision-language models\nfor zero-shot image retrieval and compare their performance with an existing\nfine-tuned baseline. The system integrates real-time inference and background\nindexing workflows, supported by a unified embedding pipeline optimized through\ndimensionality reduction. Offline evaluation using user interaction logs shows\nthat the multilingual SigLIP model outperforms other models across multiple\nretrieval metrics, achieving a 13.3% increase in nDCG@5 over the baseline. A\none-week online A/B test in production further confirms real-world impact, with\nthe treatment group showing substantial gains in engagement and conversion, up\nto a 40.9% increase in transaction rate via image search. Our findings\nhighlight that recent zero-shot models can serve as a strong and practical\nbaseline for production use, which enables teams to deploy effective visual\nsearch systems with minimal overhead, while retaining the flexibility to\nfine-tune based on future data or domain-specific needs.", "AI": {"tldr": "\u5728 Mercari \u7684 C2C \u5e02\u573a\u4e2d\u90e8\u7f72\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89c6\u89c9\u641c\u7d22\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5229\u7528\u6700\u65b0\u7684 zero-shot \u6a21\u578b\uff0c\u5728\u53c2\u4e0e\u5ea6\u548c\u8f6c\u5316\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u7740\u63d0\u5347\u3002", "motivation": "\u89c6\u89c9\u641c\u7d22\u4e3a\u5ba2\u6237\u63d0\u4f9b\u4e86\u4e00\u79cd\u76f4\u89c2\u7684\u65b9\u5f0f\u6765\u63a2\u7d22\u591a\u6837\u5316\u7684\u4ea7\u54c1\u76ee\u5f55\uff0c\u7279\u522b\u662f\u5728\u6d88\u8d39\u8005\u5bf9\u6d88\u8d39\u8005 (C2C) \u5e02\u573a\u4e2d\uff0c\u8fd9\u4e9b\u5e02\u573a\u4e2d\u7684\u5546\u54c1\u5217\u8868\u901a\u5e38\u662f\u975e\u7ed3\u6784\u5316\u7684\u5e76\u4e14\u662f\u89c6\u89c9\u9a71\u52a8\u7684\u3002", "method": "\u8bc4\u4f30\u7528\u4e8e\u96f6\u6837\u672c\u56fe\u50cf\u68c0\u7d22\u7684\u6700\u65b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5c06\u5b83\u4eec\u7684\u6027\u80fd\u4e0e\u73b0\u6709\u7684\u5fae\u8c03\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002\u8be5\u7cfb\u7edf\u96c6\u6210\u4e86\u5b9e\u65f6\u63a8\u7406\u548c\u540e\u53f0\u7d22\u5f15\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u7531\u901a\u8fc7\u964d\u7ef4\u4f18\u5316\u7684\u7edf\u4e00\u5d4c\u5165\u7ba1\u9053\u63d0\u4f9b\u652f\u6301\u3002", "result": "\u4f7f\u7528\u7528\u6237\u4ea4\u4e92\u65e5\u5fd7\u7684\u79bb\u7ebf\u8bc4\u4f30\u8868\u660e\uff0c\u591a\u8bed\u8a00 SigLIP \u6a21\u578b\u5728\u591a\u4e2a\u68c0\u7d22\u6307\u6807\u4e0a\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u4e0e\u57fa\u7ebf\u76f8\u6bd4\uff0cnDCG@5 \u63d0\u9ad8\u4e86 13.3%\u3002\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u8fdb\u884c\u4e3a\u671f\u4e00\u5468\u7684\u5728\u7ebf A/B \u6d4b\u8bd5\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u5b9e\u9645\u5f71\u54cd\uff0c\u5b9e\u9a8c\u7ec4\u5728\u53c2\u4e0e\u5ea6\u548c\u8f6c\u5316\u7387\u65b9\u9762\u5747\u5b9e\u73b0\u4e86\u663e\u7740\u589e\u957f\uff0c\u901a\u8fc7\u56fe\u50cf\u641c\u7d22\u5b9e\u73b0\u7684\u4ea4\u6613\u7387\u63d0\u9ad8\u4e86 40.9%\u3002", "conclusion": "\u6700\u8fd1\u7684zero-shot\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u751f\u4ea7\u4f7f\u7528\u7684\u5f3a\u5927\u800c\u5b9e\u7528\u7684\u57fa\u7ebf\uff0c\u8fd9\u4f7f\u56e2\u961f\u80fd\u591f\u4ee5\u6700\u5c0f\u7684\u5f00\u9500\u90e8\u7f72\u6709\u6548\u7684\u89c6\u89c9\u641c\u7d22\u7cfb\u7edf\uff0c\u540c\u65f6\u4fdd\u7559\u57fa\u4e8e\u672a\u6765\u6570\u636e\u6216\u9886\u57df\u7279\u5b9a\u9700\u6c42\u8fdb\u884c\u5fae\u8c03\u7684\u7075\u6d3b\u6027\u3002"}}
{"id": "2508.05987", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05987", "abs": "https://arxiv.org/abs/2508.05987", "authors": ["Chunyun Zhang", "Hongyan Zhao", "Chaoran Cui", "Qilong Song", "Zhiqing Lu", "Shuai Gong", "Kailin Liu"], "title": "Adversarial Topic-aware Prompt-tuning for Cross-topic Automated Essay Scoring", "comment": null, "summary": "Cross-topic automated essay scoring (AES) aims to develop a transferable\nmodel capable of effectively evaluating essays on a target topic. A significant\nchallenge in this domain arises from the inherent discrepancies between topics.\nWhile existing methods predominantly focus on extracting topic-shared features\nthrough distribution alignment of source and target topics, they often neglect\ntopic-specific features, limiting their ability to assess critical traits such\nas topic adherence. To address this limitation, we propose an Adversarial\nTOpic-aware Prompt-tuning (ATOP), a novel method that jointly learns\ntopic-shared and topic-specific features to improve cross-topic AES. ATOP\nachieves this by optimizing a learnable topic-aware prompt--comprising both\nshared and specific components--to elicit relevant knowledge from pre-trained\nlanguage models (PLMs). To enhance the robustness of topic-shared prompt\nlearning and mitigate feature scale sensitivity introduced by topic alignment,\nwe incorporate adversarial training within a unified regression and\nclassification framework. In addition, we employ a neighbor-based classifier to\nmodel the local structure of essay representations and generate pseudo-labels\nfor target-topic essays. These pseudo-labels are then used to guide the\nsupervised learning of topic-specific prompts tailored to the target topic.\nExtensive experiments on the publicly available ASAP++ dataset demonstrate that\nATOP significantly outperforms existing state-of-the-art methods in both\nholistic and multi-trait essay scoring. The implementation of our method is\npublicly available at: https://anonymous.4open.science/r/ATOP-A271.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6297\u6027\u4e3b\u9898\u611f\u77e5\u63d0\u793a\u8c03\u6574 (ATOP)\uff0c\u7528\u4e8e\u8054\u5408\u5b66\u4e60\u4e3b\u9898\u5171\u4eab\u548c\u4e3b\u9898\u7279\u5b9a\u7279\u5f81\uff0c\u4ee5\u6539\u8fdb\u8de8\u4e3b\u9898 AES\u3002", "motivation": "\u8de8\u4e3b\u9898\u81ea\u52a8\u6587\u7ae0\u8bc4\u5206 (AES) \u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u76ee\u6807\u4e3b\u9898\u6587\u7ae0\u7684\u53ef\u8fc1\u79fb\u6a21\u578b\u3002\u8be5\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u6e90\u4e8e\u4e3b\u9898\u4e4b\u95f4\u56fa\u6709\u7684\u5dee\u5f02\u3002\u867d\u7136\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u4fa7\u91cd\u4e8e\u901a\u8fc7\u6e90\u4e3b\u9898\u548c\u76ee\u6807\u4e3b\u9898\u7684\u5206\u5e03\u5bf9\u9f50\u6765\u63d0\u53d6\u4e3b\u9898\u5171\u4eab\u7279\u5f81\uff0c\u4f46\u5b83\u4eec\u7ecf\u5e38\u5ffd\u7565\u4e3b\u9898\u7279\u5b9a\u7279\u5f81\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5b83\u4eec\u8bc4\u4f30\u8bf8\u5982\u4e3b\u9898\u9075\u5b88\u7b49\u5173\u952e\u7279\u5f81\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u6027\u4e3b\u9898\u611f\u77e5\u63d0\u793a\u8c03\u6574 (ATOP) \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8054\u5408\u5b66\u4e60\u4e3b\u9898\u5171\u4eab\u548c\u4e3b\u9898\u7279\u5b9a\u7279\u5f81\uff0c\u4ee5\u6539\u8fdb\u8de8\u4e3b\u9898 AES\u3002", "result": "\u5728\u516c\u5f00\u7684 ASAP++ \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c", "conclusion": "ATOP\u5728\u6574\u4f53\u548c\u591a\u7279\u5f81\u6587\u7ae0\u8bc4\u5206\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2508.05851", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05851", "abs": "https://arxiv.org/abs/2508.05851", "authors": ["Ka-Wai Yung", "Felix J. S. Bragman", "Jialang Xu", "Imanol Luengo", "Danail Stoyanov", "Evangelos B. Mazomenos"], "title": "Temporal Cluster Assignment for Efficient Real-Time Video Segmentation", "comment": null, "summary": "Vision Transformers have substantially advanced the capabilities of\nsegmentation models across both image and video domains. Among them, the Swin\nTransformer stands out for its ability to capture hierarchical, multi-scale\nrepresentations, making it a popular backbone for segmentation in videos.\nHowever, despite its window-attention scheme, it still incurs a high\ncomputational cost, especially in larger variants commonly used for dense\nprediction in videos. This remains a major bottleneck for real-time,\nresource-constrained applications. Whilst token reduction methods have been\nproposed to alleviate this, the window-based attention mechanism of Swin\nrequires a fixed number of tokens per window, limiting the applicability of\nconventional pruning techniques. Meanwhile, training-free token clustering\napproaches have shown promise in image segmentation while maintaining window\nconsistency. Nevertheless, they fail to exploit temporal redundancy, missing a\nkey opportunity to further optimize video segmentation performance. We\nintroduce Temporal Cluster Assignment (TCA), a lightweight and effective,\nfine-tuning-free strategy that enhances token clustering by leveraging temporal\ncoherence across frames. Instead of indiscriminately dropping redundant tokens,\nTCA refines token clusters using temporal correlations, thereby retaining\nfine-grained details while significantly reducing computation. Extensive\nevaluations on YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and a private surgical\nvideo dataset show that TCA consistently boosts the accuracy-speed trade-off of\nexisting clustering-based methods. Our results demonstrate that TCA generalizes\ncompetently across both natural and domain-specific videos.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u65f6\u95f4\u805a\u7c7b\u5206\u914d\uff08TCA\uff09\u7684\u8f7b\u91cf\u7ea7\u514d\u5fae\u8c03\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u901a\u8fc7\u5229\u7528\u65f6\u95f4\u8fde\u8d2f\u6027\u6765\u589e\u5f3atoken\u805a\u7c7b\uff0c\u4ece\u800c\u63d0\u9ad8\u89c6\u9891\u5206\u5272\u7684\u901f\u5ea6-\u7cbe\u5ea6\u6743\u8861\u3002", "motivation": "Swin Transformer\u5728\u89c6\u9891\u5206\u5272\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u89c6\u9891\u5bc6\u96c6\u9884\u6d4b\u4e2d\u4f7f\u7528\u7684\u5927\u578b\u53d8\u4f53\u4e2d\u3002\u867d\u7136\u5df2\u7ecf\u63d0\u51fa\u4e86token\u51cf\u5c11\u65b9\u6cd5\u6765\u7f13\u89e3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46Swin\u7684\u57fa\u4e8e\u7a97\u53e3\u7684\u6ce8\u610f\u529b\u673a\u5236\u9700\u8981\u6bcf\u4e2a\u7a97\u53e3\u56fa\u5b9a\u6570\u91cf\u7684token\uff0c\u9650\u5236\u4e86\u4f20\u7edf\u526a\u679d\u6280\u672f\u7684\u9002\u7528\u6027\u3002\u540c\u65f6\uff0c\u514d\u8bad\u7ec3token\u805a\u7c7b\u65b9\u6cd5\u5728\u56fe\u50cf\u5206\u5272\u4e2d\u663e\u793a\u51fa\u5e0c\u671b\uff0c\u4f46\u672a\u80fd\u5229\u7528\u65f6\u95f4\u5197\u4f59\u3002", "method": "\u5f15\u5165\u65f6\u95f4\u805a\u7c7b\u5206\u914d\uff08TCA\uff09\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u6709\u6548\u7684\u514d\u5fae\u8c03\u7b56\u7565\uff0c\u901a\u8fc7\u5229\u7528\u8de8\u5e27\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u6765\u589e\u5f3atoken\u805a\u7c7b\u3002", "result": "\u5728YouTube-VIS 2019\u3001YouTube-VIS 2021\u3001OVIS\u548c\u4e00\u4e2a\u79c1\u6709\u5916\u79d1\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u8bc4\u4f30\u8868\u660e\uff0cTCA\u59cb\u7ec8\u5982\u4e00\u5730\u63d0\u9ad8\u4e86\u73b0\u6709\u57fa\u4e8e\u805a\u7c7b\u65b9\u6cd5\u7684\u901f\u5ea6-\u7cbe\u5ea6\u6743\u8861\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0cTCA\u5728\u81ea\u7136\u89c6\u9891\u548c\u7279\u5b9a\u9886\u57df\u89c6\u9891\u4e2d\u90fd\u80fd\u80dc\u4efb\u3002", "conclusion": "TCA\u5728\u591a\u4e2a\u89c6\u9891\u5206\u5272\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u4e86\u73b0\u6709\u805a\u7c7b\u65b9\u6cd5\u7684\u901f\u5ea6-\u7cbe\u5ea6\u6743\u8861\u3002"}}
{"id": "2508.06062", "categories": ["cs.AI", "cs.LG", "cs.LO", "68T27, 68T30"], "pdf": "https://arxiv.org/pdf/2508.06062", "abs": "https://arxiv.org/abs/2508.06062", "authors": ["Evgenii E. Vityaev", "Andrei Mantsivoda"], "title": "Don't Forget Imagination!", "comment": "14 pages, 2 figures", "summary": "Cognitive imagination is a type of imagination that plays a key role in human\nthinking. It is not a ``picture-in-the-head'' imagination. It is a faculty to\nmentally visualize coherent and holistic systems of concepts and causal links\nthat serve as semantic contexts for reasoning, decision making and prediction.\nOur position is that the role of cognitive imagination is still greatly\nunderestimated, and this creates numerous problems and diminishes the current\ncapabilities of AI. For instance, when reasoning, humans rely on imaginary\ncontexts to retrieve background info. They also constantly return to the\ncontext for semantic verification that their reasoning is still reasonable.\nThus, reasoning without imagination is blind. This paper is a call for greater\nattention to cognitive imagination as the next promising breakthrough in\nartificial intelligence. As an instrument for simulating cognitive imagination,\nwe propose semantic models -- a new approach to mathematical models that can\nlearn, like neural networks, and are based on probabilistic causal\nrelationships. Semantic models can simulate cognitive imagination because they\nensure the consistency of imaginary contexts and implement a glass-box approach\nthat allows the context to be manipulated as a holistic and coherent system of\ninterrelated facts glued together with causal relations.", "AI": {"tldr": "\u8ba4\u77e5\u60f3\u8c61\u529b\u5728\u4eba\u5de5\u667a\u80fd\u4e2d\u88ab\u4f4e\u4f30\u4e86\u3002\u8bba\u6587\u63d0\u51fa\u8bed\u4e49\u6a21\u578b\u6765\u6a21\u62df\u8ba4\u77e5\u60f3\u8c61\u529b\u3002", "motivation": "\u8bba\u6587\u8ba4\u4e3a\u8ba4\u77e5\u60f3\u8c61\u529b\u5728\u4eba\u7c7b\u601d\u7ef4\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u4f46\u5176\u5728\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u4f5c\u7528\u88ab\u5927\u5927\u4f4e\u4f30\uff0c\u8fd9\u5bfc\u81f4\u4e86\u8bb8\u591a\u95ee\u9898\u5e76\u524a\u5f31\u4e86\u4eba\u5de5\u667a\u80fd\u7684\u80fd\u529b\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u8bed\u4e49\u6a21\u578b\uff0c\u4e00\u79cd\u65b0\u7684\u6570\u5b66\u6a21\u578b\u65b9\u6cd5\uff0c\u5b83\u50cf\u795e\u7ecf\u7f51\u7edc\u4e00\u6837\u53ef\u4ee5\u5b66\u4e60\uff0c\u5e76\u4e14\u57fa\u4e8e\u6982\u7387\u56e0\u679c\u5173\u7cfb\u3002", "result": "\u8bed\u4e49\u6a21\u578b\u53ef\u4ee5\u6a21\u62df\u8ba4\u77e5\u60f3\u8c61\u529b\uff0c\u56e0\u4e3a\u5b83\u4eec\u786e\u4fdd\u4e86\u60f3\u8c61\u60c5\u5883\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd\u73bb\u7483\u76d2\u65b9\u6cd5\uff0c\u5141\u8bb8\u5c06\u60c5\u5883\u4f5c\u4e3a\u4e00\u4e2a\u6574\u4f53\u548c\u8fde\u8d2f\u7684\u76f8\u4e92\u5173\u8054\u7684\u4e8b\u5b9e\u7cfb\u7edf\u8fdb\u884c\u64cd\u4f5c\uff0c\u8fd9\u4e9b\u4e8b\u5b9e\u901a\u8fc7\u56e0\u679c\u5173\u7cfb\u7c98\u5408\u5728\u4e00\u8d77\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u547c\u5401\u66f4\u591a\u5173\u6ce8\u8ba4\u77e5\u60f3\u8c61\u529b\uff0c\u8ba4\u4e3a\u5b83\u662f\u4eba\u5de5\u667a\u80fd\u9886\u57df\u4e0b\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u7a81\u7834\u3002"}}
{"id": "2508.05921", "categories": ["cs.LG", "math.FA", "math.RT", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.05921", "abs": "https://arxiv.org/abs/2508.05921", "authors": ["Siddharth Rout"], "title": "Fast, Convex and Conditioned Network for Multi-Fidelity Vectors and Stiff Univariate Differential Equations", "comment": null, "summary": "Accuracy in neural PDE solvers often breaks down not because of limited\nexpressivity, but due to poor optimisation caused by ill-conditioning,\nespecially in multi-fidelity and stiff problems. We study this issue in\nPhysics-Informed Extreme Learning Machines (PIELMs), a convex variant of neural\nPDE solvers, and show that asymptotic components in governing equations can\nproduce highly ill-conditioned activation matrices, severely limiting\nconvergence. We introduce Shifted Gaussian Encoding, a simple yet effective\nactivation filtering step that increases matrix rank and expressivity while\npreserving convexity. Our method extends the solvable range of Peclet numbers\nin steady advection-diffusion equations by over two orders of magnitude,\nachieves up to six orders lower error on multi-frequency function learning, and\nfits high-fidelity image vectors more accurately and faster than deep networks\nwith over a million parameters. This work highlights that conditioning, not\ndepth, is often the bottleneck in scientific neural solvers and that simple\narchitectural changes can unlock substantial gains.", "AI": {"tldr": "Ill-conditioning limits convergence in neural PDE solvers. Shifted Gaussian Encoding improves conditioning and performance.", "motivation": "Accuracy in neural PDE solvers often breaks down not because of limited expressivity, but due to poor optimisation caused by ill-conditioning, especially in multi-fidelity and stiff problems.", "method": "Shifted Gaussian Encoding, a simple yet effective activation filtering step that increases matrix rank and expressivity while preserving convexity.", "result": "Extends the solvable range of Peclet numbers in steady advection-diffusion equations by over two orders of magnitude, achieves up to six orders lower error on multi-frequency function learning, and fits high-fidelity image vectors more accurately and faster than deep networks with over a million parameters.", "conclusion": "Conditioning, not depth, is often the bottleneck in scientific neural solvers, and simple architectural changes can unlock substantial gains."}}
{"id": "2508.05662", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05662", "abs": "https://arxiv.org/abs/2508.05662", "authors": ["Yuzhou Zhu"], "title": "From Static to Dynamic: A Streaming RAG Approach to Real-time Knowledge Base", "comment": null, "summary": "Dynamic streams from news feeds, social media, sensor networks, and financial\nmarkets challenge static RAG frameworks. Full-scale indices incur high memory\ncosts; periodic rebuilds introduce latency that undermines data freshness;\nnaive sampling sacrifices semantic coverage. We present Streaming RAG, a\nunified pipeline that combines multi-vector cosine screening, mini-batch\nclustering, and a counter-based heavy-hitter filter to maintain a compact\nprototype set. We further prove an approximation bound \\$E\\[R(K\\_t)] \\ge R^\\* -\nL \\Delta\\$ linking retrieval quality to clustering variance. An incremental\nindex upsert mechanism refreshes prototypes without interrupting queries.\nExperiments on eight real-time streams show statistically significant gains in\nRecall\\@10 (up to 3 points, p < 0.01), end-to-end latency below 15 ms, and\nthroughput above 900 documents per second under a 150 MB budget. Hyperparameter\nsensitivity analysis over cluster count, admission probability, relevance\nthreshold, and counter capacity validates default settings. In open-domain\nquestion answering with GPT-3.5 Turbo, we record 3.2-point gain in Exact Match\nand 2.8-point gain in F1 on SQuAD; abstractive summarization yields ROUGE-L\nimprovements. Streaming RAG establishes a new Pareto frontier for retrieval\naugmentation.", "AI": {"tldr": "Streaming RAG combines multi-vector cosine screening, mini-batch clustering, and a counter-based heavy-hitter filter to maintain a compact prototype set, achieving better performance in real-time streams.", "motivation": "Dynamic streams from news feeds, social media, sensor networks, and financial markets challenge static RAG frameworks. Full-scale indices incur high memory costs; periodic rebuilds introduce latency that undermines data freshness; naive sampling sacrifices semantic coverage.", "method": "a unified pipeline that combines multi-vector cosine screening, mini-batch clustering, and a counter-based heavy-hitter filter to maintain a compact prototype set. An incremental index upsert mechanism refreshes prototypes without interrupting queries.", "result": "statistically significant gains in Recall@10 (up to 3 points, p < 0.01), end-to-end latency below 15 ms, and throughput above 900 documents per second under a 150 MB budget. 3.2-point gain in Exact Match and 2.8-point gain in F1 on SQuAD; abstractive summarization yields ROUGE-L improvements.", "conclusion": "Streaming RAG establishes a new Pareto frontier for retrieval augmentation."}}
{"id": "2508.06016", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06016", "abs": "https://arxiv.org/abs/2508.06016", "authors": ["Sagar Gandhi", "Vishal Gandhi"], "title": "Crisp Attention: Regularizing Transformers via Structured Sparsity", "comment": null, "summary": "The quadratic computational cost of the self-attention mechanism is a primary\nchallenge in scaling Transformer models. While attention sparsity is widely\nstudied as a technique to improve computational efficiency, it is almost\nuniversally assumed to come at the cost of model accuracy. In this paper, we\nreport a surprising counter-example to this common wisdom. By introducing\nstructured, post-hoc sparsity to the attention mechanism of a DistilBERT model\nduring fine-tuning on the SST-2 sentiment analysis task, we find that model\naccuracy improves significantly. Our model with 80\\% attention sparsity\nachieves a validation accuracy of 91.59\\%, a 0.97\\% absolute improvement over\nthe dense baseline. We hypothesize that this phenomenon is due to sparsity\nacting as a powerful implicit regularizer, preventing the model from\noverfitting by forcing it to make predictions with a more constrained and\nrobust set of features. Our work recasts attention sparsity not just as a tool\nfor computational efficiency, but as a potential method for improving the\ngeneralization and performance of Transformer models.", "AI": {"tldr": "Attention sparsity improves model accuracy and generalization, and also computational efficiency.", "motivation": "The quadratic computational cost of the self-attention mechanism is a primary challenge in scaling Transformer models. Attention sparsity is widely studied as a technique to improve computational efficiency, but it is almost universally assumed to come at the cost of model accuracy.", "method": "Introducing structured, post-hoc sparsity to the attention mechanism of a DistilBERT model during fine-tuning on the SST-2 sentiment analysis task.", "result": "Model with 80% attention sparsity achieves a validation accuracy of 91.59%, a 0.97% absolute improvement over the dense baseline.", "conclusion": "Attention sparsity can improve the generalization and performance of Transformer models."}}
{"id": "2508.05852", "categories": ["cs.CV", "I.5.4"], "pdf": "https://arxiv.org/pdf/2508.05852", "abs": "https://arxiv.org/abs/2508.05852", "authors": ["Kaiser Hamid", "Khandakar Ashrafi Akbar", "Nade Liang"], "title": "VISTA: Vision-Language Imitation of Situational Thinking and Attention for Human-Like Driver Focus in Dynamic Environments", "comment": null, "summary": "Driver visual attention prediction is a critical task in autonomous driving\nand human-computer interaction (HCI) research. Most prior studies focus on\nestimating attention allocation at a single moment in time, typically using\nstatic RGB images such as driving scene pictures. In this work, we propose a\nvision-language framework that models the changing landscape of drivers' gaze\nthrough natural language, using few-shot and zero-shot learning on single RGB\nimages. We curate and refine high-quality captions from the BDD-A dataset using\nhuman-in-the-loop feedback, then fine-tune LLaVA to align visual perception\nwith attention-centric scene understanding. Our approach integrates both\nlow-level cues and top-down context (e.g., route semantics, risk anticipation),\nenabling language-based descriptions of gaze behavior. We evaluate performance\nacross training regimes (few shot, and one-shot) and introduce domain-specific\nmetrics for semantic alignment and response diversity. Results show that our\nfine-tuned model outperforms general-purpose VLMs in attention shift detection\nand interpretability. To our knowledge, this is among the first attempts to\ngenerate driver visual attention allocation and shifting predictions in natural\nlanguage, offering a new direction for explainable AI in autonomous driving.\nOur approach provides a foundation for downstream tasks such as behavior\nforecasting, human-AI teaming, and multi-agent coordination.", "AI": {"tldr": "A new vision-language framework predicts driver attention shifts using natural language, improving performance and interpretability for explainable AI in autonomous driving.", "motivation": "Predicting driver visual attention is critical for autonomous driving and HCI research, but most prior studies focus on single moments in time using static RGB images.", "method": "The authors fine-tuned LLaVA using high-quality captions from the BDD-A dataset, incorporating both low-level cues and top-down context.", "result": "The fine-tuned model outperforms general-purpose VLMs in attention shift detection and interpretability. The approach offers a foundation for downstream tasks such as behavior forecasting, human-AI teaming, and multi-agent coordination.", "conclusion": "This paper introduces a novel vision-language framework for predicting driver visual attention allocation and shifting in natural language, achieving superior performance in attention shift detection and interpretability compared to general-purpose VLMs."}}
{"id": "2508.06064", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06064", "abs": "https://arxiv.org/abs/2508.06064", "authors": ["Harold Silv\u00e8re Kiossou", "Siegfried Nijssen", "Pierre Schaus"], "title": "A Generic Complete Anytime Beam Search for Optimal Decision Tree", "comment": null, "summary": "Finding an optimal decision tree that minimizes classification error is known\nto be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic\nprogramming guarantee optimality, they often suffer from poor anytime behavior\n-- meaning they struggle to find high-quality decision trees quickly when the\nsearch is stopped before completion -- due to unbalanced search space\nexploration. To address this, several anytime extensions of exact methods have\nbeen proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not\nbeen systematically compared, making it difficult to assess their relative\neffectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and\nanytime beam search algorithm that extends the DL8.5 framework and unifies some\nexisting anytime strategies. In particular, CA-DL8.5 generalizes previous\napproaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various\nheuristics and relaxation mechanisms through a modular design. The algorithm\nreuses DL8.5's efficient branch-and-bound pruning and trie-based caching,\ncombined with a restart-based beam search that gradually relaxes pruning\ncriteria to improve solution quality over time. Our contributions are twofold:\n(1) We introduce this new generic framework for exact and anytime decision tree\nlearning, enabling the incorporation of diverse heuristics and search\nstrategies; (2) We conduct a rigorous empirical comparison of several\ninstantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k\nheuristics -- using an anytime evaluation metric called the primal gap\nintegral. Experimental results on standard classification benchmarks show that\nCA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime\nperformance, outperforming both other CA-DL8.5 variants and the Blossom\nalgorithm while maintaining completeness and optimality guarantees.", "AI": {"tldr": "Proposed CA-DL8.5, a beam search algorithm for finding optimal decision trees with better anytime performance than existing methods.", "motivation": "Exact algorithms for finding optimal decision trees suffer from poor anytime behavior due to unbalanced search space exploration. Existing anytime extensions have not been systematically compared.", "method": "CA-DL8.5, a generic, complete, and anytime beam search algorithm that extends the DL8.5 framework", "result": "Introduced a new generic framework for exact and anytime decision tree learning and CA-DL8.5 using LDS provides the best anytime performance.", "conclusion": "CA-DL8.5 with LDS consistently provides the best anytime performance, outperforming both other CA-DL8.5 variants and the Blossom algorithm while maintaining completeness and optimality guarantees."}}
{"id": "2508.05928", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05928", "abs": "https://arxiv.org/abs/2508.05928", "authors": ["Si Shen", "Peijun Shen", "Wenhua Zhao", "Danhao Zhu"], "title": "Mitigating Think-Answer Mismatch in LLM Reasoning Through Noise-Aware Advantage Reweighting", "comment": null, "summary": "Group-Relative Policy Optimization (GRPO) is a key technique for training\nlarge reasoning models, yet it suffers from a critical vulnerability: the\n\\emph{Think-Answer Mismatch}, where noisy reward signals corrupt the learning\nprocess. This problem is most severe in unbalanced response groups,\nparadoxically degrading the signal precisely when it should be most\ninformative. To address this challenge, we propose Stable Group-Relative Policy\nOptimization (S-GRPO), a principled enhancement that derives optimal,\nnoise-aware advantage weights to stabilize training. Our comprehensive\nexperiments on mathematical reasoning benchmarks demonstrate S-GRPO's\neffectiveness and robustness. On various models, S-GRPO significantly\noutperforms DR. GRPO, achieving performance gains of +2.5% on\nQwen-Math-7B-Base, +2.2% on Llama-3.2-3B-Base, and +2.4% on\nQwen-Math-1.5B-Instruct. Most critically, while standard GRPO fails to learn\nunder 20% synthetic reward noise, S-GRPO maintains stable learning progress.\nThese results highlight S-GRPO's potential for more robust and effective\ntraining of large-scale reasoning models. \\footnote{Code and data are available\nat: https://github.com/shenpeijun0212/S-GRPO", "AI": {"tldr": "S-GRPO\u662f\u4e00\u79cd\u6539\u8fdb\u7684GRPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6743\u91cd\u6765\u89e3\u51b3\u566a\u58f0\u95ee\u9898\uff0c\u4ece\u800c\u66f4\u7a33\u5b9a\u6709\u6548\u5730\u8bad\u7ec3\u5927\u578b\u63a8\u7406\u6a21\u578b\uff0c\u5e76\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684Group-Relative Policy Optimization (GRPO) \u6280\u672f\u5728\u8bad\u7ec3\u5927\u578b\u63a8\u7406\u6a21\u578b\u65f6\u5b58\u5728\u201c\u601d\u8003-\u56de\u7b54\u4e0d\u5339\u914d\u201d\u7684\u8106\u5f31\u6027\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u5e73\u8861\u7684\u54cd\u5e94\u7ec4\u4e2d\uff0c\u566a\u58f0\u5956\u52b1\u4fe1\u53f7\u4f1a\u7834\u574f\u5b66\u4e60\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStable Group-Relative Policy Optimization (S-GRPO) \u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a8\u5bfc\u6700\u4f18\u7684\u3001\u566a\u58f0\u611f\u77e5\u7684\u4f18\u52bf\u6743\u91cd\u6765\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "S-GRPO\u5728Qwen-Math-7B-Base\u3001Llama-3.2-3B-Base\u548cQwen-Math-1.5B-Instruct\u7b49\u6a21\u578b\u4e0a\uff0c\u6027\u80fd\u5206\u522b\u63d0\u5347\u4e86+2.5%\u3001+2.2%\u548c+2.4%\u3002\u5373\u4f7f\u572820%\u7684\u5408\u6210\u5956\u52b1\u566a\u58f0\u4e0b\uff0cS-GRPO\u4ecd\u80fd\u4fdd\u6301\u7a33\u5b9a\u7684\u5b66\u4e60\u3002", "conclusion": "S-GRPO\u5728\u5b58\u5728\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u4fdd\u6301\u7a33\u5b9a\u7684\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5e76\u4e14\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4f18\u4e8eDR. GRPO\u3002"}}
{"id": "2508.05664", "categories": ["cs.IR", "cs.AI", "cs.CL", "I.2.m"], "pdf": "https://arxiv.org/pdf/2508.05664", "abs": "https://arxiv.org/abs/2508.05664", "authors": ["Hei Yu Chan", "Kuok Tou Ho", "Chenglong Ma", "Yujing Si", "Hok Lai Lin", "Sa Lei Lam"], "title": "Enhancing Retrieval-Augmented Generation for Electric Power Industry Customer Support", "comment": "6 pages", "summary": "Many AI customer service systems use standard NLP pipelines or finetuned\nlanguage models, which often fall short on ambiguous, multi-intent, or\ndetail-specific queries. This case study evaluates recent techniques: query\nrewriting, RAG Fusion, keyword augmentation, intent recognition, and context\nreranking, for building a robust customer support system in the electric power\ndomain. We compare vector-store and graph-based RAG frameworks, ultimately\nselecting the graph-based RAG for its superior performance in handling complex\nqueries. We find that query rewriting improves retrieval for queries using\nnon-standard terminology or requiring precise detail. RAG Fusion boosts\nperformance on vague or multifaceted queries by merging multiple retrievals.\nReranking reduces hallucinations by filtering irrelevant contexts. Intent\nrecognition supports the decomposition of complex questions into more targeted\nsub-queries, increasing both relevance and efficiency. In contrast, keyword\naugmentation negatively impacts results due to biased keyword selection. Our\nfinal system combines intent recognition, RAG Fusion, and reranking to handle\ndisambiguation and multi-source queries. Evaluated on both a GPT-4-generated\ndataset and a real-world electricity provider FAQ dataset, it achieves 97.9%\nand 89.6% accuracy respectively, substantially outperforming baseline RAG\nmodels.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u79cd\u6280\u672f\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u7535\u529b\u9886\u57df\u5ba2\u6237\u652f\u6301\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u7684AI\u5ba2\u6237\u670d\u52a1\u7cfb\u7edf\u5728\u5904\u7406\u6a21\u7cca\u3001\u591a\u610f\u56fe\u6216\u7ec6\u8282\u7279\u5b9a\u7684\u67e5\u8be2\u65f6\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5bf9\u6bd4\u4e86\u5411\u91cf\u5b58\u50a8\u548c\u57fa\u4e8e\u56fe\u7684RAG\u6846\u67b6\uff0c\u6700\u7ec8\u9009\u62e9\u4e86\u57fa\u4e8e\u56fe\u7684RAG\u3002", "result": "\u67e5\u8be2\u91cd\u5199\u6539\u5584\u4e86\u68c0\u7d22\uff0cRAG Fusion\u63d0\u5347\u4e86\u6a21\u7cca\u6216\u591a\u65b9\u9762\u67e5\u8be2\u7684\u6027\u80fd\uff0c\u91cd\u6392\u5e8f\u51cf\u5c11\u4e86\u5e7b\u89c9\uff0c\u610f\u56fe\u8bc6\u522b\u63d0\u9ad8\u4e86\u76f8\u5173\u6027\u548c\u6548\u7387\uff0c\u5173\u952e\u8bcd\u6269\u5145 \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u043e \u5f71\u54cd\u4e86\u7ed3\u679c\u3002", "conclusion": "\u96c6\u6210\u4e86\u610f\u56fe\u8bc6\u522b\u3001RAG Fusion\u548c\u91cd\u6392\u5e8f\u7684\u7cfb\u7edf\uff0c\u5728GPT-4\u751f\u6210\u7684\u6570\u636e\u96c6\u548c\u771f\u5b9e\u7535\u529b\u4f9b\u5e94\u5546FAQ\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b0\u4e8697.9%\u548c89.6%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebfRAG\u6a21\u578b\u3002"}}
{"id": "2508.06026", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06026", "abs": "https://arxiv.org/abs/2508.06026", "authors": ["Yidong Wang", "Xin Wang", "Cunxiang Wang", "Junfeng Fang", "Qiufeng Wang", "Jianing Chu", "Xuran Meng", "Shuxun Yang", "Libo Qin", "Yue Zhang", "Wei Ye", "Shikun Zhang"], "title": "Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future", "comment": "12 pages, 5 figures", "summary": "Self-Rewarding Language Models propose an architecture in which the Large\nLanguage Models(LLMs) both generates responses and evaluates its own outputs\nvia LLM-as-a-Judge prompting, dynamically improving its generative capabilities\nthrough iterative Direct Preference Optimization (DPO). However, our analysis\nreveals a critical limitation in existing Self-Rewarding paradigms: the\nsynchronized improvement of chosen and rejected responses progressively narrows\nthe representational difference between contrasting samples, undermining\neffective preference learning. We propose \\textbf{Temporal Self-Rewarding\nLanguage Models} that strategically coordinate past, present, and future model\ngenerations to sustain learning signals. Our dual-phase framework introduces:\n(1) \\textit{Anchored Rejection} - fixing rejected responses using the past\ninitial model's outputs and (2) \\textit{Future-Guided Chosen} - dynamically\ncurating chosen samples using next-generation model predictions. Extensive\nexperiments across three model families (Llama, Qwen, Mistral) and different\nmodel sizes (Llama3B/8B/70B) demonstrate significant improvements when trained\nwith our method compared to Self-Rewarding using same computation resources.\nFor example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our\nmethod, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our\nmethod also demonstrates superior out-of-distribution generalization across\nmathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code\ngeneration (HumanEval) tasks, even though we do not specifically collect such\ntraining data.", "AI": {"tldr": "This paper proposes Temporal Self-Rewarding Language Models to address the limitations of standard Self-Rewarding methods by coordinating past, present, and future model generations, leading to improved performance and generalization.", "motivation": "Existing Self-Rewarding paradigms suffer from a limitation where the synchronized improvement of chosen and rejected responses narrows the representational difference, undermining effective preference learning.", "method": "The paper introduces Temporal Self-Rewarding Language Models with Anchored Rejection (fixing rejected responses using past model outputs) and Future-Guided Chosen (dynamically curating chosen samples using next-generation model predictions).", "result": "Llama3.1-8B achieves a 29.44 win rate on AlpacaEval 2.0 with the proposed method, outperforming the Self-Rewarding baseline (19.69) by 9.75. The method also demonstrates superior out-of-distribution generalization across mathematical reasoning, knowledge-based QA, and code generation tasks.", "conclusion": "Temporal Self-Rewarding Language Models significantly improve performance and generalization compared to standard Self-Rewarding methods across various tasks and model sizes."}}
{"id": "2508.05857", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05857", "abs": "https://arxiv.org/abs/2508.05857", "authors": ["Qiaomu Miao", "Vivek Raju Golani", "Jingyi Xu", "Progga Paromita Dutta", "Minh Hoai", "Dimitris Samaras"], "title": "Multi-view Gaze Target Estimation", "comment": "Accepted to ICCV 2025", "summary": "This paper presents a method that utilizes multiple camera views for the gaze\ntarget estimation (GTE) task. The approach integrates information from\ndifferent camera views to improve accuracy and expand applicability, addressing\nlimitations in existing single-view methods that face challenges such as face\nocclusion, target ambiguity, and out-of-view targets. Our method processes a\npair of camera views as input, incorporating a Head Information Aggregation\n(HIA) module for leveraging head information from both views for more accurate\ngaze estimation, an Uncertainty-based Gaze Selection (UGS) for identifying the\nmost reliable gaze output, and an Epipolar-based Scene Attention (ESA) module\nfor cross-view background information sharing. This approach significantly\noutperforms single-view baselines, especially when the second camera provides a\nclear view of the person's face. Additionally, our method can estimate the gaze\ntarget in the first view using the image of the person in the second view only,\na capability not possessed by single-view GTE methods. Furthermore, the paper\nintroduces a multi-view dataset for developing and evaluating multi-view GTE\nmethods. Data and code are available at\nhttps://www3.cs.stonybrook.edu/~cvl/multiview_gte.html", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u591a\u76f8\u673a\u89c6\u56fe\u8fdb\u884c\u6ce8\u89c6\u76ee\u6807\u4f30\u8ba1\uff08GTE\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5355\u89c6\u89d2\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u591a\u89c6\u89d2\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u89c6\u89d2\u65b9\u6cd5\u9762\u4e34\u9762\u90e8\u906e\u6321\u3001\u76ee\u6807\u6a21\u7cca\u548c\u8d85\u51fa\u89c6\u91ce\u76ee\u6807\u7b49\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u591a\u4e2a\u76f8\u673a\u89c6\u56fe\u4f5c\u4e3a\u8f93\u5165\uff0c\u7ed3\u5408\u5934\u90e8\u4fe1\u606f\u805a\u5408\uff08HIA\uff09\u6a21\u5757\u3001\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u6ce8\u89c6\u9009\u62e9\uff08UGS\uff09\u6a21\u5757\u548c\u57fa\u4e8e\u5bf9\u6781\u7684\u573a\u666f\u6ce8\u610f\u529b\uff08ESA\uff09\u6a21\u5757\u3002", "result": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u5e76\u6269\u5927\u4e86\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5355\u89c6\u89d2\u57fa\u7ebf\uff0c\u5c24\u5176\u662f\u5728\u7b2c\u4e8c\u4e2a\u76f8\u673a\u63d0\u4f9b\u6e05\u6670\u7684\u9762\u90e8\u89c6\u56fe\u65f6\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u4f7f\u7528\u7b2c\u4e8c\u5f20\u89c6\u56fe\u4e2d\u7684\u4eba\u50cf\u6765\u4f30\u8ba1\u7b2c\u4e00\u5f20\u89c6\u56fe\u4e2d\u7684\u6ce8\u89c6\u76ee\u6807\uff0c\u8fd9\u662f\u5355\u89c6\u89d2 GTE \u65b9\u6cd5\u4e0d\u5177\u5907\u7684\u529f\u80fd\u3002"}}
{"id": "2508.06074", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06074", "abs": "https://arxiv.org/abs/2508.06074", "authors": ["Siyi Lu", "Run Liu", "Dongsheng Yang", "Lei He"], "title": "ME$^3$-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception", "comment": null, "summary": "Autonomous driving systems face significant challenges in perceiving complex\nenvironments and making real-time decisions. Traditional modular approaches,\nwhile offering interpretability, suffer from error propagation and coordination\nissues, whereas end-to-end learning systems can simplify the design but face\ncomputational bottlenecks. This paper presents a novel approach to autonomous\ndriving using deep reinforcement learning (DRL) that integrates bird's-eye view\n(BEV) perception for enhanced real-time decision-making. We introduce the\n\\texttt{Mamba-BEV} model, an efficient spatio-temporal feature extraction\nnetwork that combines BEV-based perception with the Mamba framework for\ntemporal feature modeling. This integration allows the system to encode vehicle\nsurroundings and road features in a unified coordinate system and accurately\nmodel long-range dependencies. Building on this, we propose the\n\\texttt{ME$^3$-BEV} framework, which utilizes the \\texttt{Mamba-BEV} model as a\nfeature input for end-to-end DRL, achieving superior performance in dynamic\nurban driving scenarios. We further enhance the interpretability of the model\nby visualizing high-dimensional features through semantic segmentation,\nproviding insight into the learned representations. Extensive experiments on\nthe CARLA simulator demonstrate that \\texttt{ME$^3$-BEV} outperforms existing\nmodels across multiple metrics, including collision rate and trajectory\naccuracy, offering a promising solution for real-time autonomous driving.", "AI": {"tldr": "This paper presents ME3-BEV, a DRL framework for autonomous driving that integrates bird's-eye view perception with the Mamba framework, achieving superior performance in CARLA simulator.", "motivation": "Traditional modular approaches suffer from error propagation and coordination issues, whereas end-to-end learning systems face computational bottlenecks in autonomous driving.", "method": "The paper introduces the Mamba-BEV model, an efficient spatio-temporal feature extraction network that combines BEV-based perception with the Mamba framework for temporal feature modeling. Building on this, the ME3-BEV framework utilizes the Mamba-BEV model as a feature input for end-to-end DRL.", "result": "The ME3-BEV framework achieves superior performance in dynamic urban driving scenarios. The interpretability of the model is enhanced by visualizing high-dimensional features through semantic segmentation.", "conclusion": "The ME3-BEV framework outperforms existing models in CARLA simulator across multiple metrics, offering a promising solution for real-time autonomous driving."}}
{"id": "2508.05957", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05957", "abs": "https://arxiv.org/abs/2508.05957", "authors": ["Hasibul Karim Shanto", "Umme Ayman Koana", "Shadikur Rahman"], "title": "Multi-Armed Bandits-Based Optimization of Decision Trees", "comment": null, "summary": "Decision trees, without appropriate constraints, can easily become overly\ncomplex and prone to overfit, capturing noise rather than generalizable\npatterns. To resolve this problem,pruning operation is a crucial part in\noptimizing decision trees, as it not only reduces the complexity of trees but\nalso decreases the probability of generating overfit models. The conventional\npruning techniques like Cost-Complexity Pruning (CCP) and Reduced Error Pruning\n(REP) are mostly based on greedy approaches that focus on immediate gains in\nperformance while pruning nodes of the decision tree. However, this might\nresult in a lower generalization in the long run, compromising the robust\nability of the tree model when introduced to unseen data samples, particularly\nwhen trained with small and complex datasets. To address this challenge, we are\nproposing a Multi-Armed Bandits (MAB)-based pruning approach, a reinforcement\nlearning (RL)-based technique, that will dynamically prune the tree to generate\nan optimal decision tree with better generalization. Our proposed approach\nassumes the pruning process as an exploration-exploitation problem, where we\nare utilizing the MAB algorithms to find optimal branch nodes to prune based on\nfeedback from each pruning actions. Experimental evaluation on several\nbenchmark datasets, demonstrated that our proposed approach results in better\npredictive performance compared to the traditional ones. This suggests the\npotential of utilizing MAB for a dynamic and probabilistic way of decision tree\npruning, in turn optimizing the decision tree-based model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMAB\u7684\u51b3\u7b56\u6811\u526a\u679d\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u526a\u679d\u6280\u672f\u5927\u591a\u57fa\u4e8e\u8d2a\u5a6a\u65b9\u6cd5\uff0c\u4fa7\u91cd\u4e8e\u5728\u526a\u679d\u51b3\u7b56\u6811\u8282\u70b9\u65f6\u7acb\u5373\u83b7\u5f97\u6027\u80fd\u63d0\u5347\u3002\u7136\u800c\uff0c\u4ece\u957f\u8fdc\u6765\u770b\uff0c\u8fd9\u53ef\u80fd\u4f1a\u5bfc\u81f4\u8f83\u4f4e\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4ece\u800c\u5728\u5f15\u5165\u5230\u672a\u89c1\u8fc7\u7684\u6570\u636e\u6837\u672c\u65f6\uff0c\u964d\u4f4e\u6811\u6a21\u578b\u7684\u9c81\u68d2\u6027\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u5c0f\u578b\u548c\u590d\u6742\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u65f6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMulti-Armed Bandits (MAB) \u7684\u526a\u679d\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u526a\u679d\u8fc7\u7a0b\u89c6\u4e3a\u4e00\u4e2a\u63a2\u7d22-\u5229\u7528\u95ee\u9898\uff0c\u5229\u7528MAB\u7b97\u6cd5\u627e\u5230\u6700\u4f73\u5206\u652f\u8282\u70b9\uff0c\u6839\u636e\u6bcf\u4e2a\u526a\u679d\u52a8\u4f5c\u7684\u53cd\u9988\u8fdb\u884c\u526a\u679d\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMulti-Armed Bandits (MAB) \u7684\u526a\u679d\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u8868\u660e\u4e86\u5229\u7528MAB\u8fdb\u884c\u52a8\u6001\u548c\u6982\u7387\u51b3\u7b56\u6811\u526a\u679d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.05666", "categories": ["cs.IR", "cs.AI", "cs.LG", "I.2.7; H.3.3; I.2.6; H.2.8; I.7.5"], "pdf": "https://arxiv.org/pdf/2508.05666", "abs": "https://arxiv.org/abs/2508.05666", "authors": ["Alejandro Godinez"], "title": "HySemRAG: A Hybrid Semantic Retrieval-Augmented Generation Framework for Automated Literature Synthesis and Methodological Gap Analysis", "comment": "47 pages, 10 figures. Code:\n  https://github.com/agodinezmm2007/docling_mod. Demo:\n  https://youtu.be/ZCy5ESJ1gVE?si=K8CttwgTj7yGrWjn. ETL+multi-agent RAG\n  framework for literature synthesis, 35.1% improvement over PDF chunking. Real\n  application: reduced 17,400 papers to 24 relevant ones (99.86%) in 10 minutes\n  for wastewater epidemiology review", "summary": "We present HySemRAG, a framework that combines Extract, Transform, Load (ETL)\npipelines with Retrieval-Augmented Generation (RAG) to automate large-scale\nliterature synthesis and identify methodological research gaps. The system\naddresses limitations in existing RAG architectures through a multi-layered\napproach: hybrid retrieval combining semantic search, keyword filtering, and\nknowledge graph traversal; an agentic self-correction framework with iterative\nquality assurance; and post-hoc citation verification ensuring complete\ntraceability. Our implementation processes scholarly literature through eight\nintegrated stages: multi-source metadata acquisition, asynchronous PDF\nretrieval, custom document layout analysis using modified Docling architecture,\nbibliographic management, LLM-based field extraction, topic modeling, semantic\nunification, and knowledge graph construction. The system creates dual data\nproducts - a Neo4j knowledge graph enabling complex relationship queries and\nQdrant vector collections supporting semantic search - serving as foundational\ninfrastructure for verifiable information synthesis. Evaluation across 643\nobservations from 60 testing sessions demonstrates structured field extraction\nachieving 35.1% higher semantic similarity scores (0.655 $\\pm$ 0.178) compared\nto PDF chunking approaches (0.485 $\\pm$ 0.204, p < 0.000001). The agentic\nquality assurance mechanism achieves 68.3% single-pass success rates with 99.0%\ncitation accuracy in validated responses. Applied to geospatial epidemiology\nliterature on ozone exposure and cardiovascular disease, the system identifies\nmethodological trends and research gaps, demonstrating broad applicability\nacross scientific domains for accelerating evidence synthesis and discovery.", "AI": {"tldr": "HySemRAG combines ETL pipelines with RAG to automate literature synthesis and identify methodological research gaps, outperforming existing methods.", "motivation": "Existing RAG architectures have limitations.", "method": "The system processes scholarly literature through eight integrated stages: multi-source metadata acquisition, asynchronous PDF retrieval, custom document layout analysis, bibliographic management, LLM-based field extraction, topic modeling, semantic unification, and knowledge graph construction. It uses hybrid retrieval, agentic self-correction, and post-hoc citation verification.", "result": "Structured field extraction achieves 35.1% higher semantic similarity scores (0.655) compared to PDF chunking approaches (0.485). The agentic quality assurance mechanism achieves 68.3% single-pass success rates with 99.0% citation accuracy.", "conclusion": "The HySemRAG system identifies methodological trends and research gaps in geospatial epidemiology literature, demonstrating broad applicability across scientific domains for accelerating evidence synthesis and discovery."}}
{"id": "2508.06030", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06030", "abs": "https://arxiv.org/abs/2508.06030", "authors": ["Kartik Sharma", "Yiqiao Jin", "Rakshit Trivedi", "Srijan Kumar"], "title": "Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings", "comment": null, "summary": "Large language models (LLMs) acquire knowledge across diverse domains such as\nscience, history, and geography encountered during generative pre-training.\nHowever, due to their stochasticity, it is difficult to predict what LLMs have\nacquired. Prior work has developed different ways to probe this knowledge by\ninvestigating the hidden representations, crafting specific task prompts,\ncurating representative samples, and estimating their uncertainty. However,\nthese methods require making forward passes through the underlying model to\nprobe the LLM's knowledge about a specific fact, making them computationally\nexpensive and time-consuming. To bridge this gap, we propose $\\textbf{PEEK}$ or\n$\\textbf{P}$roxy $\\textbf{E}$mbeddings to $\\textbf{E}$stimate\n$\\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models\nthat effectively encode factual knowledge as text or graphs as proxies for\nLLMs. First, we identify a training set of facts known by LLMs through various\nprobing strategies and then adapt embedding models to predict the LLM outputs\nwith a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived\ndatasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict\nLLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find\nthat sentence embedding models are more suitable than graph embeddings to\npredict LLM knowledge, shedding light on the underlying representation of the\nfactual landscape. Thus, we believe that knowledge-adapted embeddings can be\nused to identify knowledge gaps in LLMs at scale and can provide deeper\ninsights into LLMs' internal inductive bias. The code and data are made\navailable at https://github.com/claws-lab/peek.", "AI": {"tldr": "PEEK: Use proxy embeddings to efficiently estimate LLM knowledge, achieving 90% accuracy in predicting held-out facts.", "motivation": "It is difficult to predict what LLMs have acquired. Prior methods require making forward passes through the underlying model to probe the LLM's knowledge about a specific fact, making them computationally expensive and time-consuming.", "method": "Propose PEEK, Proxy Embeddings to Estimate Knowledge of LLMs, by leveraging the pre-trained embedding models that effectively encode factual knowledge as text or graphs as proxies for LLMs. First, identify a training set of facts known by LLMs through various probing strategies and then adapt embedding models to predict the LLM outputs with a linear decoder layer.", "result": "Embeddings can predict LLM knowledge on a held-out set with up to 90% accuracy. Sentence embedding models are more suitable than graph embeddings to predict LLM knowledge.", "conclusion": "Knowledge-adapted embeddings can be used to identify knowledge gaps in LLMs at scale and can provide deeper insights into LLMs' internal inductive bias."}}
{"id": "2508.05898", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05898", "abs": "https://arxiv.org/abs/2508.05898", "authors": ["Hamidreza Dastmalchi", "Aijun An", "Ali cheraghian"], "title": "ETTA: Efficient Test-Time Adaptation for Vision-Language Models through Dynamic Embedding Updates", "comment": "BMVC2025", "summary": "Pretrained vision-language models (VLMs) like CLIP show strong zero-shot\nperformance but struggle with generalization under distribution shifts.\nTest-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test\ndata in new domains. While some TTA methods rely on prompt-tuning,\ntraining-free cache-based approaches are preferred for efficiency. However,\ncurrent cache-based TTA models store only a limited set of high-confidence\nsamples, restricting the decision boundary to these samples and ignoring the\ninfluence of other incoming test data. To address this, we propose Efficient\nTest-Time Adaptation (ETTA), introducing a Recursive Updating module that\nintegrates all incoming test samples, progressively refining the decision\nboundary. This strategy mimics an unbounded cache, dynamically updating\ncontextual embeddings for improved accuracy with minimal memory and\ncomputational overhead. ETTA also includes an Adaptive Ensemble module to\nreduce prompt dependency in image-to-text scores by dynamically selecting\noptimal prompts for each class. Furthermore, ETTA adaptively combines scores\nfrom both modules based on confidence levels, leveraging their complementary\nstrengths. Extensive experiments on two benchmarks confirm that ETTA surpasses\nthe state-of-the-art TTA models in computational complexity and accuracy,\nsetting a new standard for effective, efficient test-time adaptation. The code\nhas been released at https://github.com/hamidreza-dastmalchi/ETTA.", "AI": {"tldr": "ETTA is proposed to improve the performance of VLMs under distribution shifts. It uses a Recursive Updating module and an Adaptive Ensemble module to refine the decision boundary and reduce prompt dependency, respectively. Experiments show that ETTA outperforms existing TTA models in both computational complexity and accuracy.", "motivation": "Pretrained vision-language models (VLMs) like CLIP show strong zero-shot performance but struggle with generalization under distribution shifts. Current cache-based TTA models store only a limited set of high-confidence samples, restricting the decision boundary to these samples and ignoring the influence of other incoming test data.", "method": "We propose Efficient Test-Time Adaptation (ETTA), introducing a Recursive Updating module that integrates all incoming test samples, progressively refining the decision boundary. ETTA also includes an Adaptive Ensemble module to reduce prompt dependency in image-to-text scores by dynamically selecting optimal prompts for each class. Furthermore, ETTA adaptively combines scores from both modules based on confidence levels, leveraging their complementary strengths.", "result": "Extensive experiments on two benchmarks confirm that ETTA surpasses the state-of-the-art TTA models in computational complexity and accuracy.", "conclusion": "ETTA surpasses the state-of-the-art TTA models in computational complexity and accuracy, setting a new standard for effective, efficient test-time adaptation."}}
{"id": "2508.06091", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06091", "abs": "https://arxiv.org/abs/2508.06091", "authors": ["Stan P Hauke", "Przemys\u0142aw Andrzej Wa\u0142\u0119ga"], "title": "Aggregate-Combine-Readout GNNs Are More Expressive Than Logic C2", "comment": "18 pages", "summary": "In recent years, there has been growing interest in understanding the\nexpressive power of graph neural networks (GNNs) by relating them to logical\nlanguages. This research has been been initialised by an influential result of\nBarcel\\'o et al. (2020), who showed that the graded modal logic (or a guarded\nfragment of the logic C2), characterises the logical expressiveness of\naggregate-combine GNNs. As a ``challenging open problem'' they left the\nquestion whether full C2 characterises the logical expressiveness of\naggregate-combine-readout GNNs. This question has remained unresolved despite\nseveral attempts. In this paper, we solve the above open problem by proving\nthat the logical expressiveness of aggregate-combine-readout GNNs strictly\nexceeds that of C2. This result holds over both undirected and directed graphs.\nBeyond its implications for GNNs, our work also leads to purely logical\ninsights on the expressive power of infinitary logics.", "AI": {"tldr": "The paper proves that aggregate-combine-readout GNNs are more expressive than C2, solving an open problem and providing logical insights.", "motivation": "Understanding the expressive power of graph neural networks (GNNs) by relating them to logical languages, and a challenging open problem whether full C2 characterises the logical expressiveness of aggregate-combine-readout GNNs.", "method": "Proving that the logical expressiveness of aggregate-combine-readout GNNs strictly exceeds that of C2.", "result": "The logical expressiveness of aggregate-combine-readout GNNs strictly exceeds that of C2. This result holds over both undirected and directed graphs. Beyond its implications for GNNs, this work also leads to purely logical insights on the expressive power of infinitary logics.", "conclusion": "The logical expressiveness of aggregate-combine-readout GNNs strictly exceeds that of C2."}}
{"id": "2508.05960", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05960", "abs": "https://arxiv.org/abs/2508.05960", "authors": ["Haohui Chen", "Zhiyong Chen"], "title": "Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning", "comment": null, "summary": "Offline reinforcement learning (RL) seeks to learn optimal policies from\nstatic datasets without further environment interaction. A key challenge is the\ndistribution shift between the learned and behavior policies, leading to\nout-of-distribution (OOD) actions and overestimation. To prevent gross\noverestimation, the value function must remain conservative; however, excessive\nconservatism may hinder performance improvement. To address this, we propose\nthe mildly conservative regularized evaluation (MCRE) framework, which balances\nconservatism and performance by combining temporal difference (TD) error with a\nbehavior cloning term in the Bellman backup. Building on this, we develop the\nmildly conservative regularized Q-learning (MCRQ) algorithm, which integrates\nMCRE into an off-policy actor-critic framework. Experiments show that MCRQ\noutperforms strong baselines and state-of-the-art offline RL algorithms on\nbenchmark datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86MCRQ\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u5e73\u8861\u4fdd\u5b88\u6027\u548c\u6027\u80fd\u6765\u89e3\u51b3\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60(RL)\u8bd5\u56fe\u4ece\u9759\u6001\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u6700\u4f18\u7b56\u7565\uff0c\u800c\u65e0\u9700\u8fdb\u4e00\u6b65\u7684\u73af\u5883\u4ea4\u4e92\u3002\u4e00\u4e2a\u5173\u952e\u7684\u6311\u6218\u662f\u5b66\u4e60\u7b56\u7565\u548c\u884c\u4e3a\u7b56\u7565\u4e4b\u95f4\u7684\u5206\u5e03\u8f6c\u79fb\uff0c\u5bfc\u81f4\u5206\u5e03\u5916(OOD)\u884c\u52a8\u548c\u9ad8\u4f30\u3002\u4e3a\u4e86\u9632\u6b62\u4e25\u91cd\u7684\u9ad8\u4f30\uff0c\u4ef7\u503c\u51fd\u6570\u5fc5\u987b\u4fdd\u6301\u4fdd\u5b88;\u7136\u800c\uff0c\u8fc7\u5ea6\u4fdd\u5b88\u53ef\u80fd\u4f1a\u963b\u788d\u6027\u80fd\u7684\u63d0\u9ad8\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898", "method": "\u7ed3\u5408\u65f6\u95f4\u5dee\u5206(TD)\u8bef\u5dee\u4e0eBellman\u5907\u4efd\u4e2d\u7684\u884c\u4e3a\u514b\u9686\u9879\uff0c\u63d0\u51fa\u4e86\u9002\u5ea6\u4fdd\u5b88\u7684\u6b63\u5219\u5316\u8bc4\u4f30(MCRE)\u6846\u67b6\uff0c\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5f00\u53d1\u4e86\u9002\u5ea6\u4fdd\u5b88\u7684\u6b63\u5219\u5316Q\u5b66\u4e60(MCRQ)\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5c06MCRE\u96c6\u6210\u5230off-policy actor-critic\u6846\u67b6\u4e2d\u3002", "result": "MCRQ\u7b97\u6cd5\u5728benchmark\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u548c\u6700\u5148\u8fdb\u7684\u79bb\u7ebfRL\u7b97\u6cd5\u3002", "conclusion": "MCRQ\u7b97\u6cd5\u5728benchmark\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u548c\u6700\u5148\u8fdb\u7684\u79bb\u7ebfRL\u7b97\u6cd5\u3002"}}
{"id": "2508.05667", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05667", "abs": "https://arxiv.org/abs/2508.05667", "authors": ["Zekun Liu", "Xiaowen Huang", "Jitao Sang"], "title": "ITDR: An Instruction Tuning Dataset for Enhancing Large Language Models in Recommendations", "comment": null, "summary": "Large language models (LLMs) have demonstrated outstanding performance in\nnatural language processing tasks. However, in the field of recommendation\nsystems, due to the structural differences between user behavior data and\nnatural language, LLMs struggle to effectively model the associations between\nuser preferences and items. Although prompt-based methods can generate\nrecommendation results, their inadequate understanding of recommendation tasks\nleads to constrained performance. To address this gap, in this work, we\nconstruct a sufficient instruction tuning dataset, ITDR, which encompasses 7\nsubtasks across two core root tasks--user-item interaction and user-item\nunderstanding. The dataset integrates data from 13 public recommendation\ndatasets and is built using manually crafted standardized templates, comprising\napproximately 200,000 instances. Experimental results demonstrate that ITDR\nsignificantly enhances the performance of mainstream open-source LLMs such as\nGLM-4, Qwen2.5, Qwen2.5-Instruct and LLaMA-3.2 on recommendation tasks.\nFurthermore, we analyze the correlations between tasks and explore the impact\nof task descriptions and data scale on instruction tuning effectiveness.\nFinally, we perform comparative experiments against closed-source LLMs with\nsubstantial parameters. Our tuning dataset ITDR and the fine-tuned large\nrecommendation models can be accessed at https://github.com/hellolzk/ITDR.", "AI": {"tldr": "This paper introduces ITDR, a new instruction tuning dataset for recommendation systems, which improves the performance of open-source LLMs on these tasks.", "motivation": "LLMs struggle to effectively model the associations between user preferences and items. Although prompt-based methods can generate recommendation results, their inadequate understanding of recommendation tasks leads to constrained performance.", "method": "construct a sufficient instruction tuning dataset, ITDR, which encompasses 7 subtasks across two core root tasks--user-item interaction and user-item understanding. The dataset integrates data from 13 public recommendation datasets and is built using manually crafted standardized templates, comprising approximately 200,000 instances.", "result": "analyze the correlations between tasks and explore the impact of task descriptions and data scale on instruction tuning effectiveness. Finally, we perform comparative experiments against closed-source LLMs with substantial parameters.", "conclusion": "ITDR significantly enhances the performance of mainstream open-source LLMs such as GLM-4, Qwen2.5, Qwen2.5-Instruct and LLaMA-3.2 on recommendation tasks."}}
{"id": "2508.06046", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06046", "abs": "https://arxiv.org/abs/2508.06046", "authors": ["Xinda Wang", "Zhengxu Hou", "Yangshijie Zhang", "Bingren Yan", "Zhibo Yang", "Xingsheng Zhang", "Luxi Xing", "Qiang Zhou", "Chen Zhang"], "title": "EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation", "comment": null, "summary": "Although the effectiveness of Large Language Models (LLMs) as judges\n(LLM-as-a-judge) has been validated, their performance remains limited in\nopen-ended tasks, particularly in story evaluation. Accurate story evaluation\nis crucial not only for assisting human quality judgment but also for providing\nkey signals to guide story generation. However, existing methods face a\ndilemma: prompt engineering for closed-source models suffers from poor\nadaptability, while fine-tuning approaches for open-source models lack the\nrigorous reasoning capabilities essential for story evaluation. To address\nthis, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework.\nGrounded in pairwise comparison, the framework first self-synthesizes\nscore-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To\nensure data quality, these raw CoTs undergo a self-filtering process, utilizing\nmulti-agents to guarantee their logical rigor and robustness. Finally, the\nevaluator trained on the refined data is deployed as a reward model to guide\nthe story generation task. Experimental results demonstrate that our framework\nachieves state-of-the-art (SOTA) performance on three evaluation benchmarks\nincluding StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward\nmodel, it significantly enhances the quality of generated stories, thereby\nfully validating the superiority of our self-evolving approach.", "AI": {"tldr": "The paper introduces Self-Evolving Pairwise Reasoning (EvolvR) framework to improve story evaluation by addressing the limitations of current methods. EvolvR uses self-synthesized and self-filtered Chain-of-Thought data for training, achieving state-of-the-art performance and enhancing story generation quality.", "motivation": "Existing methods face a dilemma: prompt engineering for closed-source models suffers from poor adaptability, while fine-tuning approaches for open-source models lack the rigorous reasoning capabilities essential for story evaluation.", "method": "The framework first self-synthesizes score-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To ensure data quality, these raw CoTs undergo a self-filtering process, utilizing multi-agents to guarantee their logical rigor and robustness. Finally, the evaluator trained on the refined data is deployed as a reward model to guide the story generation task.", "result": "achieves state-of-the-art (SOTA) performance on three evaluation benchmarks including StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward model, it significantly enhances the quality of generated stories", "conclusion": "The Self-Evolving Pairwise Reasoning (EvolvR) framework achieves state-of-the-art (SOTA) performance on three evaluation benchmarks and significantly enhances the quality of generated stories."}}
{"id": "2508.05899", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.05899", "abs": "https://arxiv.org/abs/2508.05899", "authors": ["Zixuan Bian", "Ruohan Ren", "Yue Yang", "Chris Callison-Burch"], "title": "HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing", "comment": null, "summary": "3D scene generation plays a crucial role in gaming, artistic creation,\nvirtual reality and many other domains. However, current 3D scene design still\nrelies heavily on extensive manual effort from creators, and existing automated\nmethods struggle to generate open-domain scenes or support flexible editing. As\na result, generating 3D worlds directly from text has garnered increasing\nattention. In this paper, we introduce HOLODECK 2.0, an advanced\nvision-language-guided framework for 3D world generation with support for\ninteractive scene editing based on human feedback. HOLODECK 2.0 can generate\ndiverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and\ncyberpunk styles) that exhibit high semantic fidelity to fine-grained input\ndescriptions, suitable for both indoor and open-domain environments. HOLODECK\n2.0 leverages vision-language models (VLMs) to identify and parse the objects\nrequired in a scene and generates corresponding high-quality assets via\nstate-of-the-art 3D generative models. It then iteratively applies spatial\nconstraints derived from the VLMs to achieve semantically coherent and\nphysically plausible layouts. Human evaluations and CLIP-based assessments\ndemonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely\naligned with detailed textual descriptions, consistently outperforming\nbaselines across indoor and open-domain scenarios. Additionally, we provide\nediting capabilities that flexibly adapt to human feedback, supporting layout\nrefinement and style-consistent object edits. Finally, we present a practical\napplication of HOLODECK 2.0 in procedural game modeling, generating visually\nrich and immersive environments, potentially boosting efficiency.", "AI": {"tldr": "HOLODECK 2.0\u662f\u4e00\u4e2a\u5148\u8fdb\u7684\u89c6\u89c9-\u8bed\u8a00\u5f15\u5bfc\u6846\u67b6\uff0c\u7528\u4e8e 3D \u4e16\u754c\u751f\u6210\uff0c\u652f\u6301\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u4ea4\u4e92\u5f0f\u573a\u666f\u7f16\u8f91\u3002", "motivation": "\u5f53\u524d\u7684 3D \u573a\u666f\u8bbe\u8ba1\u4ecd\u7136\u4e25\u91cd\u4f9d\u8d56\u4e8e\u521b\u4f5c\u8005\u7684\u5927\u91cf\u4eba\u5de5\u5de5\u4f5c\uff0c\u5e76\u4e14\u73b0\u6709\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u96be\u4ee5\u751f\u6210\u5f00\u653e\u57df\u573a\u666f\u6216\u652f\u6301\u7075\u6d3b\u7684\u7f16\u8f91\u3002\u56e0\u6b64\uff0c\u76f4\u63a5\u4ece\u6587\u672c\u751f\u6210 3D \u4e16\u754c\u8d8a\u6765\u8d8a\u53d7\u5230\u5173\u6ce8\u3002", "method": "HOLODECK 2.0\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b (VLM) \u6765\u8bc6\u522b\u548c\u89e3\u6790\u573a\u666f\u4e2d\u9700\u8981\u7684\u5bf9\u8c61\uff0c\u5e76\u901a\u8fc7\u6700\u5148\u8fdb\u7684 3D \u751f\u6210\u6a21\u578b\u751f\u6210\u76f8\u5e94\u7684\u9ad8\u8d28\u91cf\u8d44\u4ea7\u3002\u7136\u540e\uff0c\u5b83\u8fed\u4ee3\u5730\u5e94\u7528\u4ece VLM \u5bfc\u51fa\u7684\u7a7a\u95f4\u7ea6\u675f\uff0c\u4ee5\u5b9e\u73b0\u8bed\u4e49\u8fde\u8d2f\u4e14\u7269\u7406\u4e0a\u5408\u7406\u7684\u5e03\u5c40\u3002", "result": "HOLODECK 2.0\u53ef\u4ee5\u751f\u6210\u591a\u6837\u5316\u4e14\u98ce\u683c\u4e30\u5bcc\u7684 3D \u573a\u666f\uff08\u4f8b\u5982\uff0c\u903c\u771f\u3001\u5361\u901a\u3001\u52a8\u6f2b\u548c\u8d5b\u535a\u670b\u514b\u98ce\u683c\uff09\uff0c\u8fd9\u4e9b\u573a\u666f\u5bf9\u7ec6\u7c92\u5ea6\u7684\u8f93\u5165\u63cf\u8ff0\u8868\u73b0\u51fa\u9ad8\u5ea6\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\uff0c\u9002\u7528\u4e8e\u5ba4\u5185\u548c\u5f00\u653e\u9886\u57df\u73af\u5883\u3002\u4eba\u7c7b\u8bc4\u4f30\u548c\u57fa\u4e8e CLIP \u7684\u8bc4\u4f30\u8868\u660e\uff0cHOLODECK 2.0 \u6709\u6548\u5730\u751f\u6210\u4e86\u4e0e\u8be6\u7ec6\u6587\u672c\u63cf\u8ff0\u9ad8\u5ea6\u4e00\u81f4\u7684\u9ad8\u8d28\u91cf\u573a\u666f\uff0c\u5e76\u4e14\u5728\u5ba4\u5185\u548c\u5f00\u653e\u9886\u57df\u573a\u666f\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "HOLODECK 2.0\u6709\u6548\u5730\u751f\u6210\u4e86\u4e0e\u8be6\u7ec6\u6587\u672c\u63cf\u8ff0\u9ad8\u5ea6\u4e00\u81f4\u7684\u9ad8\u8d28\u91cf\u573a\u666f\uff0c\u5e76\u4e14\u5728\u5ba4\u5185\u548c\u5f00\u653e\u9886\u57df\u573a\u666f\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u3002\u6b64\u5916\uff0c\u5b83\u63d0\u4f9b\u7684\u7f16\u8f91\u529f\u80fd\u53ef\u4ee5\u7075\u6d3b\u5730\u9002\u5e94\u4eba\u7c7b\u53cd\u9988\uff0c\u652f\u6301\u5e03\u5c40\u6539\u8fdb\u548c\u98ce\u683c\u4e00\u81f4\u7684\u5bf9\u8c61\u7f16\u8f91\u3002\u6700\u540e\uff0c\u5b83\u5728\u7a0b\u5e8f\u5316\u6e38\u620f\u5efa\u6a21\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\uff0c\u53ef\u4ee5\u751f\u6210\u89c6\u89c9\u4e0a\u4e30\u5bcc\u4e14\u8eab\u4e34\u5176\u5883\u7684\u73af\u5883\uff0c\u4ece\u800c\u53ef\u80fd\u63d0\u9ad8\u6548\u7387\u3002"}}
{"id": "2508.06110", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06110", "abs": "https://arxiv.org/abs/2508.06110", "authors": ["Yiran Rex Ma"], "title": "PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion", "comment": "Accepted at IJCNN 2025", "summary": "Table reasoning, including tabular QA and fact verification, often depends on\nannotated data or complex data augmentation, limiting flexibility and\ngeneralization. LLMs, despite their versatility, often underperform compared to\nsimple supervised models. To approach these issues, we introduce PanelTR, a\nframework utilizing LLM agent scientists for robust table reasoning through a\nstructured scientific approach. PanelTR's workflow involves agent scientists\nconducting individual investigations, engaging in self-review, and\nparticipating in collaborative peer-review discussions. This process, driven by\nfive scientist personas, enables semantic-level transfer without relying on\ndata augmentation or parametric optimization. Experiments across four\nbenchmarks show that PanelTR outperforms vanilla LLMs and rivals fully\nsupervised models, all while remaining independent of training data. Our\nfindings indicate that structured scientific methodology can effectively handle\ncomplex tasks beyond table reasoning with flexible semantic understanding in a\nzero-shot context.", "AI": {"tldr": "PanelTR uses LLM agent scientists in a structured scientific approach for robust table reasoning, outperforming vanilla LLMs without training data.", "motivation": "Table reasoning, including tabular QA and fact verification, often depends on annotated data or complex data augmentation, limiting flexibility and generalization. LLMs, despite their versatility, often underperform compared to simple supervised models.", "method": "a framework utilizing LLM agent scientists for robust table reasoning through a structured scientific approach", "result": "PanelTR outperforms vanilla LLMs and rivals fully supervised models, all while remaining independent of training data", "conclusion": "structured scientific methodology can effectively handle complex tasks beyond table reasoning with flexible semantic understanding in a zero-shot context"}}
{"id": "2508.05977", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2508.05977", "abs": "https://arxiv.org/abs/2508.05977", "authors": ["Aoming Liang", "Chi Cheng", "Dashuai Chen", "Boai Sun", "Dixia Fan"], "title": "LinguaFluid: Language Guided Fluid Control via Semantic Rewards in Reinforcement Learning", "comment": null, "summary": "In the domain of scientific machine learning, designing effective reward\nfunctions remains a challenge in reinforcement learning (RL), particularly in\nenvironments where task goals are difficult to specify numerically. Reward\nfunctions in existing work are predominantly based on heuristics, manual\nengineering, or task-specific tuning. In this work, we introduce a semantically\naligned reinforcement learning method where rewards are computed by aligning\nthe current state with a target semantic instruction using a\nSentence-Bidirectional Encoder Representations from Transformers (SBERT).\nInstead of relying on manually defined reward functions, the policy receives\nfeedback based on the reward, which is a cosine similarity between the goal\ntextual description and the statement description in the episode. We evaluated\nour approach in several environments and showed that semantic reward can guide\nlearning to achieve competitive control behavior, even in the absence of\nhand-crafted reward functions. Our study demonstrates a correlation between the\nlanguage embedding space and the conventional Euclidean space. This framework\nopens new horizons for aligning agent behavior with natural language goals and\nlays the groundwork for a more seamless integration of larger language models\n(LLMs) and fluid control applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u5bf9\u9f50\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528 SBERT \u901a\u8fc7\u5c06\u5f53\u524d\u72b6\u6001\u4e0e\u76ee\u6807\u8bed\u4e49\u6307\u4ee4\u5bf9\u9f50\u6765\u8ba1\u7b97\u5956\u52b1\uff0c\u65e0\u9700\u624b\u52a8\u5b9a\u4e49\u7684\u5956\u52b1\u51fd\u6570\u3002", "motivation": "\u5728\u5f3a\u5316\u5b66\u4e60 (RL) \u4e2d\uff0c\u8bbe\u8ba1\u6709\u6548\u7684\u5956\u52b1\u51fd\u6570\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u4efb\u52a1\u76ee\u6807\u96be\u4ee5\u7528\u6570\u5b57\u6307\u5b9a\u7684\u73af\u5883\u4e2d\u3002\u73b0\u6709\u5de5\u4f5c\u4e2d\u7684\u5956\u52b1\u51fd\u6570\u4e3b\u8981\u57fa\u4e8e\u542f\u53d1\u5f0f\u3001\u624b\u52a8\u5de5\u7a0b\u6216\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u8c03\u6574\u3002", "method": "\u4f7f\u7528 Sentence-Bidirectional Encoder Representations from Transformers (SBERT) \u901a\u8fc7\u5c06\u5f53\u524d\u72b6\u6001\u4e0e\u76ee\u6807\u8bed\u4e49\u6307\u4ee4\u5bf9\u9f50\u6765\u8ba1\u7b97\u5956\u52b1\u3002", "result": "\u8bed\u4e49\u5956\u52b1\u80fd\u591f\u5f15\u5bfc\u5b66\u4e60\u4ee5\u5b9e\u73b0\u6709\u7ade\u4e89\u529b\u7684\u63a7\u5236\u884c\u4e3a\uff0c\u5373\u4f7f\u5728\u6ca1\u6709\u624b\u5de5\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u7684\u60c5\u51b5\u4e0b\u3002\u5728\u591a\u4e2a\u73af\u5883\u4e2d\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u5e76\u8868\u660e\u8bed\u4e49\u5956\u52b1\u53ef\u4ee5\u6307\u5bfc\u5b66\u4e60\u4ee5\u5b9e\u73b0\u6709\u7ade\u4e89\u529b\u7684\u63a7\u5236\u884c\u4e3a\uff0c\u5373\u4f7f\u5728\u6ca1\u6709\u624b\u5de5\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u7684\u60c5\u51b5\u4e0b\u3002\u7814\u7a76\u8868\u660e\u8bed\u8a00\u5d4c\u5165\u7a7a\u95f4\u548c\u4f20\u7edf\u7684\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e4b\u95f4\u5b58\u5728\u76f8\u5173\u6027\u3002", "conclusion": "\u8bed\u4e49\u5956\u52b1\u80fd\u591f\u5f15\u5bfc\u5b66\u4e60\u4ee5\u5b9e\u73b0\u6709\u7ade\u4e89\u529b\u7684\u63a7\u5236\u884c\u4e3a\uff0c\u5373\u4f7f\u5728\u6ca1\u6709\u624b\u5de5\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u7684\u60c5\u51b5\u4e0b\u3002\u8bed\u8a00\u5d4c\u5165\u7a7a\u95f4\u548c\u4f20\u7edf\u7684\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e4b\u95f4\u5b58\u5728\u76f8\u5173\u6027\u3002\u8be5\u6846\u67b6\u4e3a\u5c06\u4ee3\u7406\u884c\u4e3a\u4e0e\u81ea\u7136\u8bed\u8a00\u76ee\u6807\u5bf9\u9f50\u5f00\u8f9f\u4e86\u65b0\u7684\u89c6\u91ce\uff0c\u5e76\u4e3a\u66f4\u5927\u8bed\u8a00\u6a21\u578b (LLM) \u548c\u6d41\u4f53\u63a7\u5236\u5e94\u7528\u7684\u66f4\u65e0\u7f1d\u96c6\u6210\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.05668", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05668", "abs": "https://arxiv.org/abs/2508.05668", "authors": ["Yunjia Xi", "Jianghao Lin", "Yongzhao Xiao", "Zheli Zhou", "Rong Shan", "Te Gao", "Jiachen Zhu", "Weiwen Liu", "Yong Yu", "Weinan Zhang"], "title": "A Survey of LLM-based Deep Search Agents: Paradigm, Optimization, Evaluation, and Challenges", "comment": null, "summary": "The advent of Large Language Models (LLMs) has significantly revolutionized\nweb search. The emergence of LLM-based Search Agents marks a pivotal shift\ntowards deeper, dynamic, autonomous information seeking. These agents can\ncomprehend user intentions and environmental context and execute multi-turn\nretrieval with dynamic planning, extending search capabilities far beyond the\nweb. Leading examples like OpenAI's Deep Research highlight their potential for\ndeep information mining and real-world applications. This survey provides the\nfirst systematic analysis of search agents. We comprehensively analyze and\ncategorize existing works from the perspectives of architecture, optimization,\napplication, and evaluation, ultimately identifying critical open challenges\nand outlining promising future research directions in this rapidly evolving\nfield. Our repository is available on\nhttps://github.com/YunjiaXi/Awesome-Search-Agent-Papers.", "AI": {"tldr": "\u672c\u6587\u5bf9\u57fa\u4e8eLLM\u7684\u641c\u7d22\u4ee3\u7406\u8fdb\u884c\u4e86\u9996\u6b21\u7cfb\u7edf\u5206\u6790\uff0c\u603b\u7ed3\u4e86\u67b6\u6784\u3001\u4f18\u5316\u3001\u5e94\u7528\u548c\u8bc4\u4f30\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u51fa\u73b0\u663e\u7740\u5730\u6539\u53d8\u4e86\u7f51\u7edc\u641c\u7d22\u3002\u57fa\u4e8e LLM \u7684\u641c\u7d22\u4ee3\u7406\u7684\u51fa\u73b0\u6807\u5fd7\u7740\u5411\u66f4\u6df1\u5165\u3001\u52a8\u6001\u3001\u81ea\u4e3b\u7684\u4fe1\u606f\u5bfb\u6c42\u7684\u5173\u952e\u8f6c\u53d8\u3002", "method": "\u7cfb\u7edf\u5206\u6790", "result": "\u5bf9\u641c\u7d22\u4ee3\u7406\u8fdb\u884c\u4e86\u9996\u6b21\u7cfb\u7edf\u5206\u6790\u3002\u786e\u5b9a\u4e86\u5173\u952e\u7684\u5f00\u653e\u6311\u6218\uff0c\u5e76\u6982\u8ff0\u4e86\u672a\u6765\u6709\u5e0c\u671b\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8fd9\u7bc7\u8c03\u67e5\u5168\u9762\u5206\u6790\u5e76\u5206\u7c7b\u4e86\u73b0\u6709\u7814\u7a76\uff0c\u4ece\u67b6\u6784\u3001\u4f18\u5316\u3001\u5e94\u7528\u548c\u8bc4\u4f30\u7684\u89d2\u5ea6\u51fa\u53d1\uff0c\u6700\u7ec8\u786e\u5b9a\u4e86\u5173\u952e\u7684\u5f00\u653e\u6311\u6218\uff0c\u5e76\u6982\u8ff0\u4e86\u5feb\u901f\u53d1\u5c55\u7684\u9886\u57df\u4e2d\u672a\u6765\u6709\u5e0c\u671b\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.06094", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06094", "abs": "https://arxiv.org/abs/2508.06094", "authors": ["Morris Alper", "Moran Yanuka", "Raja Giryes", "Ga\u0161per Begu\u0161"], "title": "ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline", "comment": "Project page: https://conlangcrafter.github.io", "summary": "Constructed languages (conlangs) such as Esperanto and Quenya have played\ndiverse roles in art, philosophy, and international communication. Meanwhile,\nlarge-scale foundation models have revolutionized creative generation in text,\nimages, and beyond. In this work, we leverage modern LLMs as computational\ncreativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a\nmulti-hop pipeline that decomposes language design into modular stages --\nphonology, morphology, syntax, lexicon generation, and translation. At each\nstage, our method leverages LLMs' meta-linguistic reasoning capabilities,\ninjecting randomness to encourage diversity and leveraging self-refinement\nfeedback to encourage consistency in the emerging language description. We\nevaluate ConlangCrafter on metrics measuring coherence and typological\ndiversity, demonstrating its ability to produce coherent and varied conlangs\nwithout human linguistic expertise.", "AI": {"tldr": "Leverage modern LLMs as computational creativity aids for end-to-end conlang creation using ConlangCrafter, a multi-hop pipeline that decomposes language design into modular stages.", "motivation": "Constructed languages (conlangs) such as Esperanto and Quenya have played diverse roles in art, philosophy, and international communication. Meanwhile, large-scale foundation models have revolutionized creative generation in text, images, and beyond.", "method": "ConlangCrafter, a multi-hop pipeline that decomposes language design into modular stages -- phonology, morphology, syntax, lexicon generation, and translation. At each stage, our method leverages LLMs' meta-linguistic reasoning capabilities, injecting randomness to encourage diversity and leveraging self-refinement feedback to encourage consistency in the emerging language description.", "result": "Evaluated on metrics measuring coherence and typological diversity, demonstrating its ability to produce coherent and varied conlangs.", "conclusion": "ConlangCrafter can produce coherent and varied conlangs without human linguistic expertise."}}
{"id": "2508.05903", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05903", "abs": "https://arxiv.org/abs/2508.05903", "authors": ["Lang Nie", "Yuan Mei", "Kang Liao", "Yunqiu Xu", "Chunyu Lin", "Bin Xiao"], "title": "Robust Image Stitching with Optimal Plane", "comment": "* Equal contribution", "summary": "We present \\textit{RopStitch}, an unsupervised deep image stitching framework\nwith both robustness and naturalness. To ensure the robustness of\n\\textit{RopStitch}, we propose to incorporate the universal prior of content\nperception into the image stitching model by a dual-branch architecture. It\nseparately captures coarse and fine features and integrates them to achieve\nhighly generalizable performance across diverse unseen real-world scenes.\nConcretely, the dual-branch model consists of a pretrained branch to capture\nsemantically invariant representations and a learnable branch to extract\nfine-grained discriminative features, which are then merged into a whole by a\ncontrollable factor at the correlation level. Besides, considering that content\nalignment and structural preservation are often contradictory to each other, we\npropose a concept of virtual optimal planes to relieve this conflict. To this\nend, we model this problem as a process of estimating homography decomposition\ncoefficients, and design an iterative coefficient predictor and minimal\nsemantic distortion constraint to identify the optimal plane. This scheme is\nfinally incorporated into \\textit{RopStitch} by warping both views onto the\noptimal plane bidirectionally. Extensive experiments across various datasets\ndemonstrate that \\textit{RopStitch} significantly outperforms existing methods,\nparticularly in scene robustness and content naturalness. The code is available\nat {\\color{red}https://github.com/MmelodYy/RopStitch}.", "AI": {"tldr": "RopStitch is an unsupervised deep image stitching framework that improves robustness and naturalness by using a dual-branch architecture and virtual optimal planes.", "motivation": "ensure the robustness and naturalness of image stitching", "method": "an unsupervised deep image stitching framework with a dual-branch architecture, a concept of virtual optimal planes, an iterative coefficient predictor and minimal semantic distortion constraint", "result": "achieve highly generalizable performance across diverse unseen real-world scenes", "conclusion": "RopStitch significantly outperforms existing methods in scene robustness and content naturalness."}}
{"id": "2508.06111", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06111", "abs": "https://arxiv.org/abs/2508.06111", "authors": ["Dewi S. W. Gould", "Bruno Mlodozeniec", "Samuel F. Brown"], "title": "SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges", "comment": "7 pages and appendices", "summary": "Evaluating the capabilities and risks of foundation models is paramount, yet\ncurrent methods demand extensive domain expertise, hindering their scalability\nas these models rapidly evolve. We introduce SKATE: a novel evaluation\nframework in which large language models (LLMs) compete by generating and\nsolving verifiable tasks for one another. Our core insight is to treat\nevaluation as a game: models act as both task-setters and solvers, incentivized\nto create questions which highlight their own strengths while exposing others'\nweaknesses. SKATE offers several key advantages, balancing scalability,\nopen-endedness, and objectivity. It is fully automated, data-free, and\nscalable, requiring no human input or domain expertise. By using verifiable\ntasks rather than LLM judges, scoring is objective. Unlike domain-limited\nprogrammatically-generated benchmarks (e.g. chess-playing or spatial\nreasoning), having LLMs creatively pose challenges enables open-ended and\nscalable evaluation. As a proof of concept, we introduce LLM-set\ncode-output-prediction (COP) challenges as a verifiable and extensible\nframework in which to test our approach. Using a TrueSkill-based ranking\nsystem, we evaluate six frontier LLMs and find that: (1) weaker models can\nreliably differentiate and score stronger ones, (2) LLM-based systems are\ncapable of self-preferencing behavior, generating questions that align with\ntheir own capabilities, and (3) SKATE automatically surfaces fine-grained\ncapability differences between models. Our findings are an important step\ntowards general, scalable evaluation frameworks which can keep pace with LLM\nprogress.", "AI": {"tldr": "SKATE is a novel evaluation framework in which large language models (LLMs) compete by generating and solving verifiable tasks for one another. It is fully automated, data-free, and scalable, requiring no human input or domain expertise. ", "motivation": "Current evaluation methods demand extensive domain expertise, hindering their scalability as these models rapidly evolve.", "method": "LLMs compete by generating and solving verifiable tasks for one another.", "result": "Weaker models can reliably differentiate and score stronger ones, LLM-based systems are capable of self-preferencing behavior.", "conclusion": "SKATE automatically surfaces fine-grained capability differences between models and offers a scalable evaluation framework."}}
{"id": "2508.05984", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05984", "abs": "https://arxiv.org/abs/2508.05984", "authors": ["Ankur Naskar", "Gugan Thoppe", "Vijay Gupta"], "title": "Parameter-free Optimal Rates for Nonlinear Semi-Norm Contractions with Applications to $Q$-Learning", "comment": null, "summary": "Algorithms for solving \\textit{nonlinear} fixed-point equations -- such as\naverage-reward \\textit{$Q$-learning} and \\textit{TD-learning} -- often involve\nsemi-norm contractions. Achieving parameter-free optimal convergence rates for\nthese methods via Polyak--Ruppert averaging has remained elusive, largely due\nto the non-monotonicity of such semi-norms. We close this gap by (i.) recasting\nthe averaged error as a linear recursion involving a nonlinear perturbation,\nand (ii.) taming the nonlinearity by coupling the semi-norm's contraction with\nthe monotonicity of a suitably induced norm. Our main result yields the first\nparameter-free $\\tilde{O}(1/\\sqrt{t})$ optimal rates for $Q$-learning in both\naverage-reward and exponentially discounted settings, where $t$ denotes the\niteration index. The result applies within a broad framework that accommodates\nsynchronous and asynchronous updates, single-agent and distributed deployments,\nand data streams obtained either from simulators or along Markovian\ntrajectories.", "AI": {"tldr": "\u672c\u6587\u4e3aQ\u5b66\u4e60\u5b9e\u73b0\u4e86\u9996\u4e2a\u65e0\u53c2\u6570\u6700\u4f18\u901f\u7387\uff0c\u89e3\u51b3\u4e86\u975e\u7ebf\u6027\u4e0d\u52a8\u70b9\u65b9\u7a0b\u4e2d\u7684\u534a\u8303\u6570\u6536\u7f29\u95ee\u9898\u3002", "motivation": "\u4e3a\u89e3\u51b3\u975e\u7ebf\u6027\u4e0d\u52a8\u70b9\u65b9\u7a0b\uff08\u5982\u5e73\u5747\u5956\u52b1Q\u5b66\u4e60\u548cTD\u5b66\u4e60\uff09\u7684\u7b97\u6cd5\u901a\u5e38\u6d89\u53ca\u534a\u8303\u6570\u6536\u7f29\uff0c\u4f46\u901a\u8fc7Polyak-Ruppert\u5e73\u5747\u4e3a\u8fd9\u4e9b\u65b9\u6cd5\u5b9e\u73b0\u65e0\u53c2\u6570\u6700\u4f18\u6536\u655b\u901f\u5ea6\u4ecd\u7136\u96be\u4ee5\u5b9e\u73b0\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u8fd9\u79cd\u534a\u8303\u6570\u7684\u975e\u5355\u8c03\u6027\u3002", "method": "\u901a\u8fc7(i.)\u5c06\u5e73\u5747\u8bef\u5dee\u91cd\u94f8\u4e3a\u6d89\u53ca\u975e\u7ebf\u6027\u6270\u52a8\u7684\u7ebf\u6027\u9012\u5f52\uff0c\u4ee5\u53ca(ii.)\u901a\u8fc7\u5c06\u534a\u8303\u6570\u7684\u6536\u7f29\u4e0e\u9002\u5f53\u8bf1\u5bfc\u8303\u6570\u7684\u5355\u8c03\u6027\u76f8\u7ed3\u5408\u6765\u9a6f\u670d\u975e\u7ebf\u6027\u3002", "result": "\u4e3a\u5e73\u5747\u5956\u52b1\u548c\u6307\u6570\u6298\u6263\u8bbe\u7f6e\u4e0b\u7684Q\u5b66\u4e60\u5b9e\u73b0\u4e86\u9996\u4e2a\u65e0\u53c2\u6570\u7684$\\\\\\tilde{O}(1/\\\\sqrt{t})$\u6700\u4f18\u901f\u7387\u3002", "conclusion": "\u5b9e\u73b0\u4e86\u5e73\u5747\u5956\u52b1\u548c\u6307\u6570\u6298\u6263\u8bbe\u7f6e\u4e0bQ\u5b66\u4e60\u7684\u9996\u4e2a\u65e0\u53c2\u6570\u7684$\\\\\\tilde{O}(1/\\\\sqrt{t})$\u6700\u4f18\u901f\u7387\u3002"}}
{"id": "2508.05669", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CV", "cs.LG", "I.2.7; I.7.2; J.1"], "pdf": "https://arxiv.org/pdf/2508.05669", "abs": "https://arxiv.org/abs/2508.05669", "authors": ["Jin Khye Tan", "En Jun Choong", "Ethan Jeremiah Chitty", "Yan Pheng Choo", "John Hsin Yang Wong", "Chern Eu Cheah"], "title": "Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports", "comment": "28 pages, 14 figures, 5 tables. Evaluation code (LLM-as-a-judge and\n  Markdown TEDS) is available at https://github.com/jinkhye/MyFinMarkdown. The\n  development dataset and evaluation benchmark are available on Hugging Face at\n  https://huggingface.co/datasets/jinkhye/MyFinMarkdown-sample and\n  https://huggingface.co/datasets/jinkhye/MyFinMarkdown-bench respectively", "summary": "Accurately extracting and representing the structure of tabular data from\nfinancial documents remains a critical challenge in document understanding,\nparticularly for regulatory and analytical use cases. This study addresses the\ncomplexity of converting financial tables from Malaysian audited financial\nreports into Markdown format, a task complicated by rotated layouts,\nmulti-level headers, and implicit structural cues. We propose a fine-tuned\nvision-language model (VLM), based on Qwen2.5-VL-7B, optimized for\nhigh-fidelity Markdown generation from document images. Our approach includes a\ncurated dataset of 2,152 image-text pairs with augmentations and a supervised\nfine-tuning strategy using LoRA. To assess performance, we evaluated our model\non 100 out-of-sample tables using a dual framework: a criteria-based\nLLM-as-a-judge for fine-grained accuracy and our novel Markdown\nTree-Edit-Distance-based Similarity (TEDS) metric for holistic structural\nfidelity. Our model achieves a 92.20% overall accuracy on the criteria-based\nassessment and a 96.53% Markdown TEDS score. This performance significantly\nsurpasses its Qwen2.5-VL-7B base model, larger-scale VLMs, and specialized\nreasoning-enabled models. Compared to these self-hosted alternatives, it also\nsignificantly reduces inference time. Furthermore, its accuracy exceeds that of\nwidely used proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash.\nThese results demonstrate that domain-specific fine-tuning provides an\neffective and efficient method to bridge the gap between unstructured financial\ndocuments and downstream automation, rivalling much larger and more general\nmodels without their computational overhead.", "AI": {"tldr": "Fine-tuned VLM accurately extracts financial tables, outperforming larger models with less computational cost.", "motivation": "Extracting tabular data from financial documents is challenging but critical for regulatory and analytical use cases.", "method": "Fine-tuned vision-language model (VLM) based on Qwen2.5-VL-7B with LoRA, trained on a curated dataset of 2,152 image-text pairs.", "result": "Achieved 92.20% accuracy on criteria-based assessment and 96.53% Markdown TEDS score, surpassing base model, larger VLMs, and proprietary models like GPT-4o and Gemini 2.5 Flash, with reduced inference time.", "conclusion": "Domain-specific fine-tuning bridges the gap between unstructured financial documents and downstream automation, rivalling larger, general models in accuracy and efficiency."}}
{"id": "2508.06103", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.06103", "abs": "https://arxiv.org/abs/2508.06103", "authors": ["Mohamed Basem", "Islam Oshallah", "Ali Hamdi", "Ammar Mohammed"], "title": "Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs", "comment": "6 pages , 2 figures , Accepted in IMSA 2025,Egypt ,\n  https://imsa.msa.edu.eg/", "summary": "This paper presents two effective approaches for Extractive Question\nAnswering (QA) on the Quran. It addresses challenges related to complex\nlanguage, unique terminology, and deep meaning in the text. The second uses\nfew-shot prompting with instruction-tuned large language models such as Gemini\nand DeepSeek. A specialized Arabic prompt framework is developed for span\nextraction. A strong post-processing system integrates subword alignment,\noverlap suppression, and semantic filtering. This improves precision and\nreduces hallucinations. Evaluations show that large language models with Arabic\ninstructions outperform traditional fine-tuned models. The best configuration\nachieves a pAP10 score of 0.637. The results confirm that prompt-based\ninstruction tuning is effective for low-resource, semantically rich QA tasks.", "AI": {"tldr": "This paper uses prompt-based instruction tuning with large language models for Quran QA, outperforming traditional methods.", "motivation": "address challenges related to complex language, unique terminology, and deep meaning in the text for Extractive Question Answering (QA) on the Quran", "method": "few-shot prompting with instruction-tuned large language models and a specialized Arabic prompt framework", "result": "large language models with Arabic instructions outperform traditional fine-tuned models, achieving a pAP10 score of 0.637", "conclusion": "prompt-based instruction tuning is effective for low-resource, semantically rich QA tasks"}}
{"id": "2508.05907", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05907", "abs": "https://arxiv.org/abs/2508.05907", "authors": ["Ilya Chugunov"], "title": "Neural Field Representations of Mobile Computational Photography", "comment": "PhD thesis", "summary": "Over the past two decades, mobile imaging has experienced a profound\ntransformation, with cell phones rapidly eclipsing all other forms of digital\nphotography in popularity. Today's cell phones are equipped with a diverse\nrange of imaging technologies - laser depth ranging, multi-focal camera arrays,\nand split-pixel sensors - alongside non-visual sensors such as gyroscopes,\naccelerometers, and magnetometers. This, combined with on-board integrated\nchips for image and signal processing, makes the cell phone a versatile\npocket-sized computational imaging platform. Parallel to this, we have seen in\nrecent years how neural fields - small neural networks trained to map\ncontinuous spatial input coordinates to output signals - enable the\nreconstruction of complex scenes without explicit data representations such as\npixel arrays or point clouds. In this thesis, I demonstrate how carefully\ndesigned neural field models can compactly represent complex geometry and\nlighting effects. Enabling applications such as depth estimation, layer\nseparation, and image stitching directly from collected in-the-wild mobile\nphotography data. These methods outperform state-of-the-art approaches without\nrelying on complex pre-processing steps, labeled ground truth data, or machine\nlearning priors. Instead, they leverage well-constructed, self-regularized\nmodels that tackle challenging inverse problems through stochastic gradient\ndescent, fitting directly to raw measurements from a smartphone.", "AI": {"tldr": "This paper introduces neural fields to represent complex geometry and lighting effects from mobile photography data. It enables applications such as depth estimation, layer separation, and image stitching. The methods outperform state-of-the-art approaches without complex pre-processing or labeled data.", "motivation": "mobile imaging has experienced a profound transformation, with cell phones rapidly eclipsing all other forms of digital photography in popularity. Today's cell phones are equipped with a diverse range of imaging technologies and  on-board integrated chips for image and signal processing, makes the cell phone a versatile pocket-sized computational imaging platform. Parallel to this, we have seen in recent years how neural fields enable the reconstruction of complex scenes without explicit data representations such as pixel arrays or point clouds.", "method": "carefully designed neural field models", "result": "depth estimation, layer separation, and image stitching directly from collected in-the-wild mobile photography data. These methods outperform state-of-the-art approaches without relying on complex pre-processing steps, labeled ground truth data, or machine learning priors.", "conclusion": " carefully designed neural field models can compactly represent complex geometry and lighting effects. Enabling applications such as depth estimation, layer separation, and image stitching directly from collected in-the-wild mobile photography data. These methods outperform state-of-the-art approaches without relying on complex pre-processing steps, labeled ground truth data, or machine learning priors. Instead, they leverage well-constructed, self-regularized models that tackle challenging inverse problems through stochastic gradient descent, fitting directly to raw measurements from a smartphone."}}
{"id": "2508.06129", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06129", "abs": "https://arxiv.org/abs/2508.06129", "authors": ["Bachtiar Herdianto", "Romain Billot", "Flavien Lucas", "Marc Sevaux"], "title": "Study of Robust Features in Formulating Guidance for Heuristic Algorithms for Solving the Vehicle Routing Problem", "comment": "22 pages, 14 figures", "summary": "The Vehicle Routing Problem (VRP) is a complex optimization problem with\nnumerous real-world applications, mostly solved using metaheuristic algorithms\ndue to its $\\mathcal{NP}$-Hard nature. Traditionally, these metaheuristics rely\non human-crafted designs developed through empirical studies. However, recent\nresearch shows that machine learning methods can be used the structural\ncharacteristics of solutions in combinatorial optimization, thereby aiding in\ndesigning more efficient algorithms, particularly for solving VRP. Building on\nthis advancement, this study extends the previous research by conducting a\nsensitivity analysis using multiple classifier models that are capable of\npredicting the quality of VRP solutions. Hence, by leveraging explainable AI,\nthis research is able to extend the understanding of how these models make\ndecisions. Finally, our findings indicate that while feature importance varies,\ncertain features consistently emerge as strong predictors. Furthermore, we\npropose a unified framework able of ranking feature impact across different\nscenarios to illustrate this finding. These insights highlight the potential of\nfeature importance analysis as a foundation for developing a guidance mechanism\nof metaheuristic algorithms for solving the VRP.", "AI": {"tldr": "This study uses machine learning and explainable AI to analyze VRP solutions, finding key features for better metaheuristic algorithm design.", "motivation": "machine learning methods can be used the structural characteristics of solutions in combinatorial optimization, thereby aiding in designing more efficient algorithms, particularly for solving VRP", "method": "sensitivity analysis using multiple classifier models that are capable of predicting the quality of VRP solutions, leveraging explainable AI", "result": "feature importance varies, certain features consistently emerge as strong predictors; a unified framework able of ranking feature impact across different scenarios", "conclusion": "feature importance analysis as a foundation for developing a guidance mechanism of metaheuristic algorithms for solving the VRP"}}
{"id": "2508.05988", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.05988", "abs": "https://arxiv.org/abs/2508.05988", "authors": ["Wenhao Zeng", "Yaoning Wang", "Chao Hu", "Yuling Shi", "Chengcheng Wan", "Hongyu Zhang", "Xiaodong Gu"], "title": "Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal", "comment": "Code and model available at https://github.com/Zengwh02/ASAP", "summary": "Recently, Large Reasoning Models (LRMs) have demonstrated remarkable\ncapabilities in code reasoning by scaling up the length of Chain-of-Thought\n(CoT). However, excessively long reasoning traces introduce substantial\nchallenges in terms of training cost, inference latency, and deployment\nfeasibility. While various CoT compression approaches have emerged to address\nthis challenge, they face inherent trade-offs: token-level methods often\ndisrupt syntactic and logical coherence, while step-level methods based on\nperplexity fail to reliably capture the logically critical reasoning steps. In\nthis paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel\ncoarse-to-fine framework for CoT compression. ASAP first performs anchor-guided\npruning to preserve the core reasoning structure, which efficiently reduces the\nsearch space for subsequent processing. It then enables a logic-aware pruning\nby selecting logically essential reasoning steps based on a novel first-token\nsurprisal metric. Finally, ASAP teaches models to autonomously generate and\nleverage these concise CoTs at inference time, enabling efficient reasoning in\ncoding tasks. Experiments show that ASAP achieves state-of-the-art accuracy\nacross multiple code generation benchmarks while substantially reducing\ntraining and inference costs. On the challenging LiveCodeBench v4_v5 benchmark,\nour approach reduces token generation by 23.5% and inference latency by 43.5%\ncompared to the strongest baseline, while achieving a competitive accuracy of\n36.19% in Pass@1. Our results highlight a promising direction for building\npowerful and efficient LRMs.", "AI": {"tldr": "ASAP\u662f\u4e00\u79cd\u7528\u4e8e\u538b\u7f29CoT\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u901a\u8fc7anchor-guided pruning\u548c\u903b\u8f91\u611f\u77e5\u7684pruning\u6765\u51cf\u5c11token\u751f\u6210\u548c\u63a8\u7406\u5ef6\u8fdf\uff0c\u5e76\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRM\uff09\u6700\u8fd1\u901a\u8fc7\u6269\u5927\u601d\u7ef4\u94fe\uff08CoT\uff09\u7684\u957f\u5ea6\uff0c\u5728\u4ee3\u7801\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u8fc7\u957f\u7684\u63a8\u7406trace\u5728\u8bad\u7ec3\u6210\u672c\u3001\u63a8\u7406\u5ef6\u8fdf\u548c\u90e8\u7f72\u53ef\u884c\u6027\u65b9\u9762\u5e26\u6765\u4e86\u5de8\u5927\u7684\u6311\u6218\u3002\u867d\u7136\u5df2\u7ecf\u51fa\u73b0\u4e86\u5404\u79cdCoT\u538b\u7f29\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u4f46\u5b83\u4eec\u9762\u4e34\u7740\u56fa\u6709\u7684\u6743\u8861\uff1atoken\u7ea7\u522b\u7684\u65b9\u6cd5\u901a\u5e38\u4f1a\u7834\u574f\u53e5\u6cd5\u548c\u903b\u8f91\u7684\u4e00\u81f4\u6027\uff0c\u800c\u57fa\u4e8e\u56f0\u60d1\u5ea6\u7684step\u7ea7\u522b\u65b9\u6cd5\u65e0\u6cd5\u53ef\u9760\u5730\u6355\u83b7\u903b\u8f91\u4e0a\u5173\u952e\u7684\u63a8\u7406\u6b65\u9aa4\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86ASAP\uff08Anchor-guided\uff0cSurprisal-based Pruning\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u7528\u4e8eCoT\u538b\u7f29\u7684\u7531\u7c97\u5230\u7cbe\u7684\u6846\u67b6\u3002ASAP\u9996\u5148\u6267\u884canchor-guided pruning\u4ee5\u4fdd\u7559\u6838\u5fc3\u63a8\u7406\u7ed3\u6784\uff0c\u4ece\u800c\u6709\u6548\u5730\u51cf\u5c11\u4e86\u540e\u7eed\u5904\u7406\u7684\u641c\u7d22\u7a7a\u95f4\u3002\u7136\u540e\uff0c\u5b83\u901a\u8fc7\u57fa\u4e8e\u65b0\u9896\u7684first-token surprisal\u6307\u6807\u9009\u62e9\u903b\u8f91\u4e0a\u5fc5\u4e0d\u53ef\u5c11\u7684\u63a8\u7406\u6b65\u9aa4\u6765\u5b9e\u73b0\u903b\u8f91\u611f\u77e5\u7684pruning\u3002", "result": "ASAP\u5728\u591a\u4e2a\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u5927\u5927\u964d\u4f4e\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\u3002\u5728\u5177\u6709\u6311\u6218\u6027\u7684LiveCodeBench v4_v5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e0e\u6700\u5f3a\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u51cf\u5c11\u4e86 23.5% \u7684token\u751f\u6210\u548c 43.5% \u7684\u63a8\u7406\u5ef6\u8fdf\uff0c\u540c\u65f6\u5728 Pass@1 \u4e2d\u5b9e\u73b0\u4e86 36.19% \u7684\u7ade\u4e89\u7cbe\u5ea6\u3002", "conclusion": "ASAP\u5728\u591a\u4e2a\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u5927\u5927\u964d\u4f4e\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\u3002\u5728\u5177\u6709\u6311\u6218\u6027\u7684LiveCodeBench v4_v5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e0e\u6700\u5f3a\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u51cf\u5c11\u4e86 23.5% \u7684token\u751f\u6210\u548c 43.5% \u7684\u63a8\u7406\u5ef6\u8fdf\uff0c\u540c\u65f6\u5728 Pass@1 \u4e2d\u5b9e\u73b0\u4e86 36.19% \u7684\u7ade\u4e89\u7cbe\u5ea6\u3002"}}
{"id": "2508.05672", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05672", "abs": "https://arxiv.org/abs/2508.05672", "authors": ["Yao Zhao", "Yantian Ding", "Zhiyue Zhang", "Dapeng Yao", "Yanxun Xu"], "title": "LMAR: Language Model Augmented Retriever for Domain-specific Knowledge Indexing", "comment": null, "summary": "Retrieval Augmented Generation (RAG) systems often struggle with\ndomain-specific knowledge due to performance deterioration of pre-trained\nembeddings and prohibitive computational costs of large language model\n(LLM)-based retrievers. While fine-tuning data augmentation embedding models\noffers a promising direction, its effectiveness is limited by the need for\nhigh-quality training data and reliable chunking strategies that preserve\ncontextual integrity. We propose LMAR (Language Model Augmented Retriever), a\nmodel-agnostic framework that addresses these challenges by combining\nLLM-guided data synthesis with contrastive embedding adaptation and efficient\ntext clustering. LMAR consists of a two-stage pipeline: (1) Triplet sampling\nand synthetic data augmentation, where LLMs act as both labeler and validator\nto ensure high-fidelity supervision throughout the pipeline. Experimental\nresults across multiple domain-specific benchmark datasets demonstrate that\nLMAR outperforms multiple baseline models, while maintaining moderate hardware\nrequirements and low latency. Its model-agnostic nature further enables\nseamless integration with emerging RAG architectures and text embedding models,\nensuring continual improvements without redesigning the pipeline. These results\nhighlight LMAR as a practical and cost-effective solution for scalable\ndomain-specific adaptation.", "AI": {"tldr": "LMAR\u901a\u8fc7\u7ed3\u5408LLM\u5f15\u5bfc\u7684\u6570\u636e\u5408\u6210\u4e0e\u5bf9\u6bd4\u5d4c\u5165\u9002\u914d\u6765\u89e3\u51b3RAG\u7cfb\u7edf\u5728\u9886\u57df\u77e5\u8bc6\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u7531\u4e8e\u9884\u8bad\u7ec3\u5d4c\u5165\u7684\u6027\u80fd\u4e0b\u964d\u4ee5\u53ca\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u68c0\u7d22\u5668\u7684\u9ad8\u6602\u8ba1\u7b97\u6210\u672c\uff0c\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG) \u7cfb\u7edf\u901a\u5e38\u96be\u4ee5\u5904\u7406\u7279\u5b9a\u9886\u57df\u7684\u77e5\u8bc6\u3002", "method": "\u7ed3\u5408\u4e86LLM\u5f15\u5bfc\u7684\u6570\u636e\u5408\u6210\u3001\u5bf9\u6bd4\u5d4c\u5165\u9002\u914d\u548c\u9ad8\u6548\u6587\u672c\u805a\u7c7b\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\u7279\u5b9a\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLMAR \u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u9002\u4e2d\u7684\u786c\u4ef6\u8981\u6c42\u548c\u4f4e\u5ef6\u8fdf\u3002", "conclusion": "LMAR\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u9886\u57df\u7279\u5b9a\u53ef\u6269\u5c55\u9002\u914d\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06105", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06105", "abs": "https://arxiv.org/abs/2508.06105", "authors": ["Shengyuan Chen", "Chuang Zhou", "Zheng Yuan", "Qinggang Zhang", "Zeyang Cui", "Hao Chen", "Yilin Xiao", "Jiannong Cao", "Xiao Huang"], "title": "You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures", "comment": null, "summary": "Large language models (LLMs) often suffer from hallucination, generating\nfactually incorrect statements when handling questions beyond their knowledge\nand perception. Retrieval-augmented generation (RAG) addresses this by\nretrieving query-relevant contexts from knowledge bases to support LLM\nreasoning. Recent advances leverage pre-constructed graphs to capture the\nrelational connections among distributed documents, showing remarkable\nperformance in complex tasks. However, existing Graph-based RAG (GraphRAG)\nmethods rely on a costly process to transform the corpus into a graph,\nintroducing overwhelming token cost and update latency. Moreover, real-world\nqueries vary in type and complexity, requiring different logic structures for\naccurate reasoning. The pre-built graph may not align with these required\nstructures, resulting in ineffective knowledge retrieval. To this end, we\npropose a \\textbf{\\underline{Logic}}-aware\n\\textbf{\\underline{R}}etrieval-\\textbf{\\underline{A}}ugmented\n\\textbf{\\underline{G}}eneration framework (\\textbf{LogicRAG}) that dynamically\nextracts reasoning structures at inference time to guide adaptive retrieval\nwithout any pre-built graph. LogicRAG begins by decomposing the input query\ninto a set of subproblems and constructing a directed acyclic graph (DAG) to\nmodel the logical dependencies among them. To support coherent multi-step\nreasoning, LogicRAG then linearizes the graph using topological sort, so that\nsubproblems can be addressed in a logically consistent order. Besides, LogicRAG\napplies graph pruning to reduce redundant retrieval and uses context pruning to\nfilter irrelevant context, significantly reducing the overall token cost.\nExtensive experiments demonstrate that LogicRAG achieves both superior\nperformance and efficiency compared to state-of-the-art baselines.", "AI": {"tldr": "LogicRAG dynamically extracts reasoning structures at inference time to guide adaptive retrieval without any pre-built graph to address the limitations of existing Graph-based RAG methods .", "motivation": "LLMs often suffer from hallucination, generating factually incorrect statements when handling questions beyond their knowledge and perception. Retrieval-augmented generation (RAG) addresses this by retrieving query-relevant contexts from knowledge bases to support LLM reasoning. Recent advances leverage pre-constructed graphs to capture the relational connections among distributed documents, showing remarkable performance in complex tasks. However, existing Graph-based RAG (GraphRAG) methods rely on a costly process to transform the corpus into a graph, introducing overwhelming token cost and update latency. Moreover, real-world queries vary in type and complexity, requiring different logic structures for accurate reasoning. The pre-built graph may not align with these required structures, resulting in ineffective knowledge retrieval.", "method": "propose a Logic-aware Retrieval-Augmented Generation framework (LogicRAG) that dynamically extracts reasoning structures at inference time to guide adaptive retrieval without any pre-built graph. LogicRAG begins by decomposing the input query into a set of subproblems and constructing a directed acyclic graph (DAG) to model the logical dependencies among them. To support coherent multi-step reasoning, LogicRAG then linearizes the graph using topological sort, so that subproblems can be addressed in a logically consistent order. Besides, LogicRAG applies graph pruning to reduce redundant retrieval and uses context pruning to filter irrelevant context", "result": "LogicRAG achieves both superior performance and efficiency compared to state-of-the-art baselines.", "conclusion": "LogicRAG achieves both superior performance and efficiency compared to state-of-the-art baselines."}}
{"id": "2508.05922", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05922", "abs": "https://arxiv.org/abs/2508.05922", "authors": ["Sri Ramana Saketh Vasanthawada", "Pengkun Liu", "Pingbo Tang"], "title": "Enhancing Construction Site Analysis and Understanding with 3D Segmentation", "comment": null, "summary": "Monitoring construction progress is crucial yet resource-intensive, prompting\nthe exploration of computer-vision-based methodologies for enhanced efficiency\nand scalability. Traditional data acquisition methods, primarily focusing on\nindoor environments, falter in construction site's complex, cluttered, and\ndynamically changing conditions. This paper critically evaluates the\napplication of two advanced 3D segmentation methods, Segment Anything Model\n(SAM) and Mask3D, in challenging outdoor and indoor conditions. Trained\ninitially on indoor datasets, both models' adaptability and performance are\nassessed in real-world construction settings, highlighting the gap in current\nsegmentation approaches due to the absence of benchmarks for outdoor scenarios.\nThrough a comparative analysis, this study not only showcases the relative\neffectiveness of SAM and Mask3D but also addresses the critical need for\ntailored segmentation workflows capable of extracting actionable insights from\nconstruction site data, thereby advancing the field towards more automated and\nprecise monitoring techniques.", "AI": {"tldr": "This paper evaluates SAM and Mask3D for construction progress monitoring, revealing the need for tailored segmentation workflows.", "motivation": "Monitoring construction progress is crucial yet resource-intensive. Traditional data acquisition methods falter in construction site's complex conditions.", "method": "application of two advanced 3D segmentation methods, Segment Anything Model (SAM) and Mask3D", "result": "both models' adaptability and performance are assessed in real-world construction settings, highlighting the gap in current segmentation approaches due to the absence of benchmarks for outdoor scenarios.", "conclusion": "This study showcases the relative effectiveness of SAM and Mask3D and addresses the critical need for tailored segmentation workflows."}}
{"id": "2508.06145", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06145", "abs": "https://arxiv.org/abs/2508.06145", "authors": ["Byeonghun Bang", "Jongsuk Yoon", "Dong-Jin Chang", "Seho Park", "Yong Oh Lee"], "title": "Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications", "comment": null, "summary": "The versatility of large language models (LLMs) has been explored across\nvarious sectors, but their application in healthcare poses challenges,\nparticularly in the domain of pharmaceutical contraindications where accurate\nand reliable information is required. This study enhances the capability of\nLLMs to address contraindications effectively by implementing a Retrieval\nAugmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base\nmodel, and the text-embedding-3-small model for embeddings, our approach\nintegrates Langchain to orchestrate a hybrid retrieval system with re-ranking.\nThis system leverages Drug Utilization Review (DUR) data from public databases,\nfocusing on contraindications for specific age groups, pregnancy, and\nconcomitant drug use. The dataset includes 300 question-answer pairs across\nthree categories, with baseline model accuracy ranging from 0.49 to 0.57.\nPost-integration of the RAG pipeline, we observed a significant improvement in\nmodel accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications\nrelated to age groups, pregnancy, and concomitant drug use, respectively. The\nresults indicate that augmenting LLMs with a RAG framework can substantially\nreduce uncertainty in prescription and drug intake decisions by providing more\nprecise and reliable drug contraindication information.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5b9e\u65bdRAG\u7ba1\u9053\u6765\u589e\u5f3aLLM\u6709\u6548\u5904\u7406\u836f\u7269\u7981\u5fcc\u75c7\u7684\u80fd\u529b\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u51c6\u786e\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u591a\u529f\u80fd\u6027\u5df2\u5728\u5404\u4e2a\u9886\u57df\u5f97\u5230\u63a2\u7d22\uff0c\u4f46\u5b83\u4eec\u5728\u533b\u7597\u4fdd\u5065\u9886\u57df\u7684\u5e94\u7528\u5e26\u6765\u4e86\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u51c6\u786e\u548c\u53ef\u9760\u4fe1\u606f\u7684\u836f\u7269\u7981\u5fcc\u75c7\u9886\u57df\u3002", "method": "\u5b9e\u65bd\u4e86\u4e00\u4e2a\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7ba1\u9053\uff0c\u5229\u7528OpenAI\u7684GPT-4o-mini\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\uff0ctext-embedding-3-small\u6a21\u578b\u7528\u4e8e\u5d4c\u5165\uff0c\u5e76\u6574\u5408\u4e86Langchain\u6765\u534f\u8c03\u4e00\u4e2a\u5e26\u6709\u91cd\u65b0\u6392\u5e8f\u7684\u6df7\u5408\u68c0\u7d22\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u5229\u7528\u6765\u81ea\u516c\u5171\u6570\u636e\u5e93\u7684\u836f\u7269\u5229\u7528\u5ba1\u67e5\uff08DUR\uff09\u6570\u636e\uff0c\u91cd\u70b9\u5173\u6ce8\u7279\u5b9a\u5e74\u9f84\u7ec4\u3001\u598a\u5a20\u548c\u4f34\u968f\u836f\u7269\u4f7f\u7528\u7684\u7981\u5fcc\u75c7\u3002", "result": "RAG\u7ba1\u9053\u96c6\u6210\u540e\uff0c\u6a21\u578b\u51c6\u786e\u7387\u663e\u8457\u63d0\u9ad8\uff0c\u5e74\u9f84\u7ec4\u3001\u598a\u5a20\u548c\u4f34\u968f\u836f\u7269\u4f7f\u7528\u7684\u7981\u5fcc\u75c7\u51c6\u786e\u7387\u5206\u522b\u8fbe\u52300.94\u30010.87\u548c0.89\u3002\u57fa\u7ebf\u6a21\u578b\u7684\u51c6\u786e\u7387\u8303\u56f4\u4e3a0.49\u81f30.57\u3002", "conclusion": "RAG\u6846\u67b6\u589e\u5f3aLLM\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u5904\u65b9\u548c\u836f\u7269\u6444\u5165\u51b3\u7b56\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u56e0\u4e3a\u5b83\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u548c\u53ef\u9760\u7684\u836f\u7269\u7981\u5fcc\u4fe1\u606f\u3002"}}
{"id": "2508.05995", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05995", "abs": "https://arxiv.org/abs/2508.05995", "authors": ["Fei Xu Yu", "Gina Adam", "Nathaniel D. Bastian", "Tian Lan"], "title": "Optimizing Prompt Sequences using Monte Carlo Tree Search for LLM-Based Optimization", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncode generation and structured reasoning; however, their performance often\ndegrades on complex tasks that require consistent multi-step planning. Recent\nwork has explored combining LLMs with Monte Carlo Tree Search (MCTS), yet\nexisting approaches primarily focus on generating heuristic-based code for\noptimization or target simpler tasks where correctness alone is sufficient. In\nthis work, we propose MCTS-OPS, a novel neural-symbolic framework that\nformulates prompt selection as a sequential decision process guided by MCTS.\nOur method explores and refines multi-step prompt sequences for the goal of\nimproving code generation quality and enhancing the problem-solving\ncapabilities of LLMs in general optimization. Experiments on network\noptimization show significant improvement over the baselines, both in the\nsuccess rate of executing the generated code and in the optimization results\nwith the specified objective and constraints (2$\\sim$4$\\times$ higher reward\nand 3$\\times$ lower standard deviation). Moreover, it improves the chance of\nattaining the optimal solution by about 10\\% of cases, compared to baseline\nmethods in hard problems. These results highlight the promise of combining\nsymbolic planning with LLMs for robust, high-quality code generation in complex\ndomains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6MCTS-OPS\uff0c\u7528\u4e8e\u6539\u8fdb\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u548c\u589e\u5f3aLLM\u5728\u4e00\u822c\u4f18\u5316\u4e2d\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4ee3\u7801\u751f\u6210\u548c\u7ed3\u6784\u5316\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u80fd\u529b\uff1b\u7136\u800c\uff0c\u5b83\u4eec\u5728\u9700\u8981\u4e00\u81f4\u7684\u591a\u6b65\u9aa4\u89c4\u5212\u7684\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u901a\u5e38\u4f1a\u4e0b\u964d\u3002", "method": "MCTS-OPS\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u5b83\u5c06prompt\u9009\u62e9\u8868\u8ff0\u4e3a\u7531MCTS\u6307\u5bfc\u7684\u987a\u5e8f\u51b3\u7b56\u8fc7\u7a0b\u3002", "result": "\u5728\u7f51\u7edc\u4f18\u5316\u65b9\u9762\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5728\u6267\u884c\u751f\u6210\u7684\u4ee3\u7801\u7684\u6210\u529f\u7387\u4ee5\u53ca\u5177\u6709\u6307\u5b9a\u76ee\u6807\u548c\u7ea6\u675f\u7684\u4f18\u5316\u7ed3\u679c\u65b9\u9762\u90fd\u6709\u663e\u7740\u6539\u8fdb\uff08\u5956\u52b1\u9ad8\u51fa2\u223c4\u500d\uff0c\u6807\u51c6\u504f\u5dee\u964d\u4f4e3\u500d\uff09\u3002\u6b64\u5916\uff0c\u4e0e\u96be\u9898\u4e2d\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b83\u5c06\u8fbe\u5230\u6700\u4f18\u89e3\u7684\u673a\u4f1a\u63d0\u9ad8\u4e86\u7ea610%\u3002", "conclusion": "\u7ed3\u5408\u7b26\u53f7\u89c4\u5212\u4e0eLLM\uff0c\u53ef\u4ee5\u5728\u590d\u6742\u9886\u57df\u5b9e\u73b0\u7a33\u5065\u3001\u9ad8\u8d28\u91cf\u7684\u4ee3\u7801\u751f\u6210\u3002"}}
{"id": "2508.05673", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05673", "abs": "https://arxiv.org/abs/2508.05673", "authors": ["Weiqin Yang", "Jiawei Chen", "Shengjia Zhang", "Peng Wu", "Yuegang Sun", "Yan Feng", "Chun Chen", "Can Wang"], "title": "Breaking the Top-$K$ Barrier: Advancing Top-$K$ Ranking Metrics Optimization in Recommender Systems", "comment": "Accepted by KDD 2025", "summary": "In the realm of recommender systems (RS), Top-$K$ ranking metrics such as\nNDCG@$K$ are the gold standard for evaluating recommendation performance.\nHowever, during the training of recommendation models, optimizing NDCG@$K$\nposes significant challenges due to its inherent discontinuous nature and the\nintricate Top-$K$ truncation. Recent efforts to optimize NDCG@$K$ have either\noverlooked the Top-$K$ truncation or suffered from high computational costs and\ntraining instability. To overcome these limitations, we propose SoftmaxLoss@$K$\n(SL@$K$), a novel recommendation loss tailored for NDCG@$K$ optimization.\nSpecifically, we integrate the quantile technique to handle Top-$K$ truncation\nand derive a smooth upper bound for optimizing NDCG@$K$ to address\ndiscontinuity. The resulting SL@$K$ loss has several desirable properties,\nincluding theoretical guarantees, ease of implementation, computational\nefficiency, gradient stability, and noise robustness. Extensive experiments on\nfour real-world datasets and three recommendation backbones demonstrate that\nSL@$K$ outperforms existing losses with a notable average improvement of 6.03%.\nThe code is available at https://github.com/Tiny-Snow/IR-Benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u8350\u635f\u5931\u51fd\u6570 SL@$K$\uff0c\u7528\u4e8e\u4f18\u5316 NDCG@$K$\uff0c\u5b83\u4f18\u4e8e\u73b0\u6709\u7684\u635f\u5931\u51fd\u6570\uff0c\u5e76\u4e14\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u3001\u6613\u4e8e\u5b9e\u73b0\u7b49\u4f18\u70b9\u3002", "motivation": "\u5728\u63a8\u8350\u7cfb\u7edf (RS) \u9886\u57df\uff0cNDCG@$K$ \u7b49 Top-$K$ \u6392\u540d\u6307\u6807\u662f\u8bc4\u4f30\u63a8\u8350\u6027\u80fd\u7684\u9ec4\u91d1\u6807\u51c6\u3002\u7136\u800c\uff0c\u5728\u63a8\u8350\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u7531\u4e8e NDCG@$K$ \u56fa\u6709\u7684\u4e0d\u8fde\u7eed\u6027\u548c\u590d\u6742\u7684 Top-$K$ \u622a\u65ad\uff0c\u4f18\u5316 NDCG@$K$ \u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u6700\u8fd1\u4f18\u5316 NDCG@$K$ \u7684\u52aa\u529b\u8981\u4e48\u5ffd\u7565\u4e86 Top-$K$ \u622a\u65ad\uff0c\u8981\u4e48\u906d\u53d7\u4e86\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u7684\u56f0\u6270\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86 SoftmaxLoss@$K$ (SL@$K$)\uff0c\u8fd9\u662f\u4e00\u79cd\u4e13\u4e3a NDCG@$K$ \u4f18\u5316\u91cf\u8eab\u5b9a\u5236\u7684\u65b0\u578b\u63a8\u8350\u635f\u5931\u3002", "result": "\u96c6\u6210\u4e86\u5206\u4f4d\u6570\u6280\u672f\u6765\u5904\u7406 Top-$K$ \u622a\u65ad\uff0c\u5e76\u63a8\u5bfc\u51fa\u7528\u4e8e\u4f18\u5316 NDCG@$K$ \u7684\u5e73\u6ed1\u4e0a\u9650\u4ee5\u89e3\u51b3\u4e0d\u8fde\u7eed\u6027\u3002\u7531\u6b64\u4ea7\u751f\u7684 SL@$K$ \u635f\u5931\u5177\u6709\u51e0\u4e2a\u7406\u60f3\u7684\u5c5e\u6027\uff0c\u5305\u62ec\u7406\u8bba\u4fdd\u8bc1\u3001\u6613\u4e8e\u5b9e\u73b0\u3001\u8ba1\u7b97\u6548\u7387\u3001\u68af\u5ea6\u7a33\u5b9a\u6027\u548c\u566a\u58f0\u9c81\u68d2\u6027\u3002", "conclusion": "SL@$K$\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c\u4e09\u4e2a\u63a8\u8350\u9aa8\u5e72\u7f51\u7edc\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSL@$K$\u4f18\u4e8e\u73b0\u6709\u635f\u5931\uff0c\u5e73\u5747\u63d0\u9ad8\u4e86 6.03%\u3002"}}
{"id": "2508.06124", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06124", "abs": "https://arxiv.org/abs/2508.06124", "authors": ["Sayantan Adak", "Pratyush Chatterjee", "Somnath Banerjee", "Rima Hazra", "Somak Aditya", "Animesh Mukherjee"], "title": "AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models", "comment": null, "summary": "Present day LLMs face the challenge of managing affordance-based safety\nrisks-situations where outputs inadvertently facilitate harmful actions due to\noverlooked logical implications. Traditional safety solutions, such as scalar\noutcome-based reward models, parameter tuning, or heuristic decoding\nstrategies, lack the granularity and proactive nature needed to reliably detect\nand intervene during subtle yet crucial reasoning steps. Addressing this\nfundamental gap, we introduce AURA, an innovative, multi-layered framework\ncentered around Process Reward Models (PRMs), providing comprehensive, step\nlevel evaluations across logical coherence and safety-awareness. Our framework\nseamlessly combines introspective self-critique, fine-grained PRM assessments,\nand adaptive safety-aware decoding to dynamically and proactively guide models\ntoward safer reasoning trajectories. Empirical evidence clearly demonstrates\nthat this approach significantly surpasses existing methods, significantly\nimproving the logical integrity and affordance-sensitive safety of model\noutputs. This research represents a pivotal step toward safer, more\nresponsible, and contextually aware AI, setting a new benchmark for\nalignment-sensitive applications.", "AI": {"tldr": "AURA, a multi-layered framework with Process Reward Models (PRMs), improves LLMs' safety and logical integrity by step-level evaluations and adaptive decoding.", "motivation": "Present day LLMs face the challenge of managing affordance-based safety risks-situations where outputs inadvertently facilitate harmful actions due to overlooked logical implications. Traditional safety solutions, such as scalar outcome-based reward models, parameter tuning, or heuristic decoding strategies, lack the granularity and proactive nature needed to reliably detect and intervene during subtle yet crucial reasoning steps. Addressing this fundamental gap", "method": "We introduce AURA, an innovative, multi-layered framework centered around Process Reward Models (PRMs), providing comprehensive, step level evaluations across logical coherence and safety-awareness. Our framework seamlessly combines introspective self-critique, fine-grained PRM assessments, and adaptive safety-aware decoding to dynamically and proactively guide models toward safer reasoning trajectories.", "result": "Empirical evidence clearly demonstrates that this approach significantly surpasses existing methods, significantly improving the logical integrity and affordance-sensitive safety of model outputs.", "conclusion": "This research represents a pivotal step toward safer, more responsible, and contextually aware AI, setting a new benchmark for alignment-sensitive applications."}}
{"id": "2508.05950", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05950", "abs": "https://arxiv.org/abs/2508.05950", "authors": ["Yanxing Liang", "Yinghui Wang", "Jinlong Yang", "Wei Li"], "title": "A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image", "comment": null, "summary": "The lack of spatial dimensional information remains a challenge in normal\nestimation from a single image. Recent diffusion-based methods have\ndemonstrated significant potential in 2D-to-3D implicit mapping, they rely on\ndata-driven statistical priors and miss the explicit modeling of light-surface\ninteraction, leading to multi-view normal direction conflicts. Moreover, the\ndiscrete sampling mechanism of diffusion models causes gradient discontinuity\nin differentiable rendering reconstruction modules, preventing 3D geometric\nerrors from being backpropagated to the normal generation network, thereby\nforcing existing methods to depend on dense normal annotations. This paper\nproposes SINGAD, a novel Self-supervised framework from a single Image for\nNormal estimation via 3D GAussian splatting guided Diffusion. By integrating\nphysics-driven light-interaction modeling and a differentiable rendering-based\nreprojection strategy, our framework directly converts 3D geometric errors into\nnormal optimization signals, solving the challenges of multi-view geometric\ninconsistency and data dependency. Specifically, the framework constructs a\nlight-interaction-driven 3DGS reparameterization model to generate multi-scale\ngeometric features consistent with light transport principles, ensuring\nmulti-view normal consistency. A cross-domain feature fusion module is designed\nwithin a conditional diffusion model, embedding geometric priors to constrain\nnormal generation while maintaining accurate geometric error propagation.\nFurthermore, a differentiable 3D reprojection loss strategy is introduced for\nself-supervised optimization that minimizes geometric error between the\nreconstructed and input image, eliminating dependence on annotated normal\ndatasets. Quantitative evaluations on the Google Scanned Objects dataset\ndemonstrate that our method outperforms state-of-the-art approaches across\nmultiple metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5355\u5f20\u56fe\u50cf\u8fdb\u884c\u6cd5\u7ebf\u4f30\u8ba1\uff0c\u901a\u8fc7\u7ed3\u5408\u5149\u7167\u4ea4\u4e92\u5efa\u6a21\u548c\u53ef\u5fae\u6e32\u67d3\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u51e0\u4f55\u4e0d\u4e00\u81f4\u6027\u548c\u6570\u636e\u4f9d\u8d56\u6027\u95ee\u9898\uff0c\u5e76\u5728 Google Scanned Objects \u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u7f3a\u4e4f\u7a7a\u95f4\u7ef4\u5ea6\u4fe1\u606f\u4ecd\u7136\u662f\u5355\u5f20\u56fe\u50cf\u6cd5\u7ebf\u4f30\u8ba1\u7684\u4e00\u4e2a\u6311\u6218\u3002\u6700\u8fd1\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u5728 2D \u5230 3D \u9690\u5f0f\u6620\u5c04\u4e2d\u663e\u793a\u51fa\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u4f46\u5b83\u4eec\u4f9d\u8d56\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u7edf\u8ba1\u5148\u9a8c\uff0c\u5e76\u5ffd\u7565\u4e86\u5149-\u8868\u9762\u4ea4\u4e92\u7684\u663e\u5f0f\u5efa\u6a21\uff0c\u4ece\u800c\u5bfc\u81f4\u591a\u89c6\u89d2\u6cd5\u7ebf\u65b9\u5411\u51b2\u7a81\u3002\u6b64\u5916\uff0c\u6269\u6563\u6a21\u578b\u7684\u79bb\u6563\u91c7\u6837\u673a\u5236\u5bfc\u81f4\u53ef\u5fae\u6e32\u67d3\u91cd\u5efa\u6a21\u5757\u4e2d\u7684\u68af\u5ea6\u4e0d\u8fde\u7eed\uff0c\u4ece\u800c\u963b\u6b62\u4e86 3D \u51e0\u4f55\u8bef\u5dee\u53cd\u5411\u4f20\u64ad\u5230\u6cd5\u7ebf\u751f\u6210\u7f51\u7edc\uff0c\u4ece\u800c\u8feb\u4f7f\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5bc6\u96c6\u7684\u6cd5\u7ebf\u6ce8\u91ca\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a SINGAD \u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7 3D \u9ad8\u65af\u6e85\u5c04\u5f15\u5bfc\u6269\u6563\uff0c\u4ece\u5355\u5f20\u56fe\u50cf\u8fdb\u884c\u81ea\u76d1\u7763\u6cd5\u7ebf\u4f30\u8ba1\u3002\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u7269\u7406\u9a71\u52a8\u7684\u5149\u4ea4\u4e92\u5efa\u6a21\u548c\u57fa\u4e8e\u53ef\u5fae\u6e32\u67d3\u7684\u91cd\u6295\u5f71\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u6784\u5efa\u5149\u4ea4\u4e92\u9a71\u52a8\u7684 3DGS \u91cd\u65b0\u53c2\u6570\u5316\u6a21\u578b\u4ee5\u751f\u6210\u4e0e\u5149\u4f20\u8f93\u539f\u7406\u4e00\u81f4\u7684\u591a\u5c3a\u5ea6\u51e0\u4f55\u7279\u5f81\uff0c\u786e\u4fdd\u591a\u89c6\u89d2\u6cd5\u7ebf\u4e00\u81f4\u6027\u3002\u5728\u6761\u4ef6\u6269\u6563\u6a21\u578b\u4e2d\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8de8\u57df\u7279\u5f81\u878d\u5408\u6a21\u5757\uff0c\u5d4c\u5165\u4e86\u51e0\u4f55\u5148\u9a8c\u6765\u7ea6\u675f\u6cd5\u7ebf\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u7cbe\u786e\u7684\u51e0\u4f55\u8bef\u5dee\u4f20\u64ad\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u53ef\u5fae\u7684 3D \u91cd\u6295\u5f71\u635f\u5931\u7b56\u7565\uff0c\u7528\u4e8e\u81ea\u76d1\u7763\u4f18\u5316\uff0c\u4ece\u800c\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u91cd\u5efa\u56fe\u50cf\u548c\u8f93\u5165\u56fe\u50cf\u4e4b\u95f4\u7684\u51e0\u4f55\u8bef\u5dee\uff0c\u4ece\u800c\u6d88\u9664\u4e86\u5bf9\u5e26\u6ce8\u91ca\u7684\u6cd5\u7ebf\u6570\u636e\u96c6\u7684\u4f9d\u8d56\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728 Google Scanned Objects \u6570\u636e\u96c6\u4e0a\u7684\u5b9a\u91cf\u8bc4\u4f30\u8868\u660e\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2508.06225", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06225", "abs": "https://arxiv.org/abs/2508.06225", "authors": ["Zailong Tian", "Zhuoheng Han", "Yanzhe Chen", "Haozhe Xu", "Xi Yang", "richeng xuan", "Hongfeng Wang", "Lizi Liao"], "title": "Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven Solution", "comment": null, "summary": "Large Language Models (LLMs) are widely used as automated judges, where\npractical value depends on both accuracy and trustworthy, risk-aware judgments.\nExisting approaches predominantly focus on accuracy, overlooking the necessity\nof well-calibrated confidence, which is vital for adaptive and reliable\nevaluation pipelines. In this work, we advocate a shift from accuracy-centric\nevaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing\nthe necessity of well-calibrated confidence for trustworthy and adaptive\nevaluation. We systematically identify the **Overconfidence Phenomenon** in\ncurrent LLM-as-a-Judges, where predicted confidence significantly overstates\nactual correctness, undermining reliability in practical deployment. To\nquantify this phenomenon, we introduce **TH-Score**, a novel metric measuring\nconfidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an\nensemble framework that transforms LLMs into reliable, risk-aware evaluators.\nExtensive experiments demonstrate that our approach substantially improves\ncalibration and enables adaptive, confidence-driven evaluation pipelines,\nachieving superior reliability and accuracy compared to existing baselines.", "AI": {"tldr": "This paper advocates for confidence-driven, risk-aware LLM-as-a-Judge systems, identifies the overconfidence phenomenon, and introduces LLM-as-a-Fuser to improve calibration and reliability.", "motivation": "Existing LLM-as-a-Judge systems focus on accuracy but overlook the necessity of well-calibrated confidence for trustworthy and adaptive evaluation.", "method": "The paper proposes LLM-as-a-Fuser and TH-Score to address the overconfidence issue.", "result": "The proposed approach substantially improves calibration and enables adaptive, confidence-driven evaluation pipelines, achieving superior reliability and accuracy compared to existing baselines.", "conclusion": "The paper introduces LLM-as-a-Fuser, an ensemble framework that improves calibration and enables adaptive, confidence-driven evaluation pipelines, achieving superior reliability and accuracy compared to existing baselines."}}
{"id": "2508.06023", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06023", "abs": "https://arxiv.org/abs/2508.06023", "authors": ["Xiaobin Shen", "Jonathan Elmer", "George H. Chen"], "title": "Stepwise Fine and Gray: Subject-Specific Variable Selection Shows When Hemodynamic Data Improves Prognostication of Comatose Post-Cardiac Arrest Patients", "comment": null, "summary": "Prognostication for comatose post-cardiac arrest patients is a critical\nchallenge that directly impacts clinical decision-making in the ICU. Clinical\ninformation that informs prognostication is collected serially over time.\nShortly after cardiac arrest, various time-invariant baseline features are\ncollected (e.g., demographics, cardiac arrest characteristics). After ICU\nadmission, additional features are gathered, including time-varying hemodynamic\ndata (e.g., blood pressure, doses of vasopressor medications). We view these as\ntwo phases in which we collect new features. In this study, we propose a novel\nstepwise dynamic competing risks model that improves the prediction of\nneurological outcomes by automatically determining when to take advantage of\ntime-invariant features (first phase) and time-varying features (second phase).\nNotably, our model finds patients for whom this second phase (time-varying\nhemodynamic) information is beneficial for prognostication and also when this\ninformation is beneficial (as we collect more hemodynamic data for a patient\nover time, how important these data are for prognostication varies). Our\napproach extends the standard Fine and Gray model to explicitly model the two\nphases and to incorporate neural networks to flexibly capture complex nonlinear\nfeature relationships. Evaluated on a retrospective cohort of 2,278 comatose\npost-arrest patients, our model demonstrates robust discriminative performance\nfor the competing outcomes of awakening, withdrawal of life-sustaining therapy,\nand death despite maximal support. Our approach generalizes to more than two\nphases in which new features are collected and could be used in other dynamic\nprediction tasks, where it may be helpful to know when and for whom newly\ncollected features significantly improve prediction.", "AI": {"tldr": "A new model improves neurological outcome prediction for comatose post-cardiac arrest patients by using time-invariant and time-varying features in a dynamic competing risks framework.", "motivation": "Prognostication for comatose post-cardiac arrest patients is a critical challenge. Clinical information is collected serially in two phases: time-invariant baseline features and time-varying hemodynamic data.", "method": "A novel stepwise dynamic competing risks model is proposed, extending the Fine and Gray model and incorporating neural networks.", "result": "The model improves the prediction of neurological outcomes by automatically determining when to take advantage of time-invariant and time-varying features. It was evaluated on a retrospective cohort of 2,278 patients and showed robust discriminative performance.", "conclusion": "The proposed stepwise dynamic competing risks model demonstrates robust discriminative performance for predicting neurological outcomes in comatose post-cardiac arrest patients, and it can be generalized to more than two phases."}}
{"id": "2508.05676", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05676", "abs": "https://arxiv.org/abs/2508.05676", "authors": ["Han Gao", "Timo Hartmann", "Botao Zhong", "Kai Lia", "Hanbin Luo"], "title": "Domain-Specific Fine-Tuning and Prompt-Based Learning: A Comparative Study for developing Natural Language-Based BIM Information Retrieval Systems", "comment": null, "summary": "Building Information Modeling (BIM) is essential for managing building data\nacross the entire lifecycle, supporting tasks from design to maintenance.\nNatural Language Interface (NLI) systems are increasingly explored as\nuser-friendly tools for information retrieval in Building Information Modeling\n(BIM) environments. Despite their potential, accurately extracting BIM-related\ndata through natural language queries remains a persistent challenge due to the\ncomplexity use queries and specificity of domain knowledge. This study presents\na comparative analysis of two prominent approaches for developing NLI-based BIM\ninformation retrieval systems: domain-specific fine-tuning and prompt-based\nlearning using large language models (LLMs). A two-stage framework consisting\nof intent recognition and table-based question answering is implemented to\nevaluate the effectiveness of both approaches. To support this evaluation, a\nBIM-specific dataset of 1,740 annotated queries of varying types across 69\nmodels is constructed. Experimental results show that domain-specific\nfine-tuning delivers superior performance in intent recognition tasks, while\nprompt-based learning, particularly with GPT-4o, shows strength in table-based\nquestion answering. Based on these findings, this study identify a hybrid\nconfiguration that combines fine-tuning for intent recognition with\nprompt-based learning for question answering, achieving more balanced and\nrobust performance across tasks. This integrated approach is further tested\nthrough case studies involving BIM models of varying complexity. This study\nprovides a systematic analysis of the strengths and limitations of each\napproach and discusses the applicability of the NLI to real-world BIM\nscenarios. The findings offer insights for researchers and practitioners in\ndesigning intelligent, language-driven BIM systems.", "AI": {"tldr": "This paper compares fine-tuning and prompt-based learning for NLI-based BIM information retrieval, finding a hybrid approach most effective.", "motivation": "Accurately extracting BIM-related data through natural language queries remains a persistent challenge due to the complexity use queries and specificity of domain knowledge.", "method": "This study presents a comparative analysis of two prominent approaches for developing NLI-based BIM information retrieval systems: domain-specific fine-tuning and prompt-based learning using large language models (LLMs). A two-stage framework consisting of intent recognition and table-based question answering is implemented to evaluate the effectiveness of both approaches.", "result": "Domain-specific fine-tuning delivers superior performance in intent recognition tasks, while prompt-based learning, particularly with GPT-4o, shows strength in table-based question answering.", "conclusion": "This study identifies a hybrid configuration that combines fine-tuning for intent recognition with prompt-based learning for question answering, achieving more balanced and robust performance across tasks. The findings offer insights for researchers and practitioners in designing intelligent, language-driven BIM systems."}}
{"id": "2508.06135", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06135", "abs": "https://arxiv.org/abs/2508.06135", "authors": ["Lingyuan Liu", "Mengxiang Zhang"], "title": "Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models", "comment": null, "summary": "Knowledge Distillation (KD) is a fundamental technique for compressing large\nlanguage models (LLMs) into compact, efficient student models. However,\nexisting white-box KD methods mainly focus on balancing ground truth and\nstudent-generated responses while overlooking two critical factors: training\ndata quality and student-model compatibility. To address these limitations, we\npropose Selective Reflection Distillation (SRD), a novel data curation\nframework that leverages reflections from student models to systematically\nrefine training data. SRD dynamically evaluates and selects prompt-response\npairs by comparing ground truth data with student model outputs, selectively\ncurating high-quality, student-compatible training instances through automated\nranking based on difficulty. Furthermore, after selecting the training data, a\ncurriculum scheduling strategy is employed to incrementally introduce these\ncurated subsets into the distillation process at fixed intervals. As a\nplug-and-play enhancement, SRD consistently improves distillation outcomes\nacross diverse white-box KD approaches and model architectures, as well as\ndecreases computational cost significantly during KD training. Experiments on a\nrange of language model benchmarks demonstrate SRD's consistent improvements in\ndistilled model performance, as well as a reduction in training runtime by up\nto 39%, under diverse KD methods and model families. Notably, SRD operates as a\nplug-and-play module, enhancing sample efficiency without modifying underlying\nKD algorithms. Our findings highlight that data quality and compatibility are\npivotal to effective and efficient distillation of LLMs, and SRD provides a\nprincipled framework to achieve both. This work advances the understanding of\ndata-centric factors in KD and offers practical insights for enhancing the\ncapability and efficiency of compressed LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSRD\u7684\u6570\u636e\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u70bc\u9ad8\u8d28\u91cf\u3001\u5b66\u751f\u517c\u5bb9\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u9ad8\u77e5\u8bc6\u84b8\u998f\u7684\u6548\u679c\u548c\u6548\u7387\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed\u3002", "motivation": "\u73b0\u6709\u7684\u767d\u76d2KD\u65b9\u6cd5\u4e3b\u8981\u4fa7\u91cd\u4e8e\u5e73\u8861ground truth\u548c\u5b66\u751f\u751f\u6210\u7684\u54cd\u5e94\uff0c\u800c\u5ffd\u7565\u4e86\u4e24\u4e2a\u5173\u952e\u56e0\u7d20\uff1a\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u548c\u5b66\u751f\u6a21\u578b\u517c\u5bb9\u6027\u3002", "method": "\u9009\u62e9\u6027\u53cd\u5c04\u84b8\u998f\uff08SRD\uff09\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u7ba1\u7406\u6846\u67b6\uff0c\u5b83\u5229\u7528\u6765\u81ea\u5b66\u751f\u6a21\u578b\u7684\u53cd\u601d\u6765\u7cfb\u7edf\u5730\u4f18\u5316\u8bad\u7ec3\u6570\u636e\u3002SRD\u901a\u8fc7\u6bd4\u8f83ground truth\u6570\u636e\u4e0e\u5b66\u751f\u6a21\u578b\u8f93\u51fa\uff0c\u52a8\u6001\u5730\u8bc4\u4f30\u548c\u9009\u62e9prompt-response\u5bf9\uff0c\u901a\u8fc7\u57fa\u4e8e\u96be\u5ea6\u7684\u81ea\u52a8\u6392\u5e8f\uff0c\u9009\u62e9\u6027\u5730\u7ba1\u7406\u9ad8\u8d28\u91cf\u3001\u5b66\u751f\u517c\u5bb9\u7684\u8bad\u7ec3\u5b9e\u4f8b\u3002\u6b64\u5916\uff0c\u5728\u9009\u62e9\u8bad\u7ec3\u6570\u636e\u540e\uff0c\u91c7\u7528\u8bfe\u7a0b\u8c03\u5ea6\u7b56\u7565\uff0c\u4ee5\u56fa\u5b9a\u7684\u65f6\u95f4\u95f4\u9694\u5c06\u8fd9\u4e9b\u7cbe\u9009\u7684\u5b50\u96c6\u589e\u91cf\u5730\u5f15\u5165\u5230\u84b8\u998f\u8fc7\u7a0b\u4e2d\u3002", "result": "SRD\u5728\u5404\u79cd\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u4e0d\u540c\u7684KD\u65b9\u6cd5\u548c\u6a21\u578b\u65cf\u4e0b\uff0c\u6301\u7eed\u63d0\u9ad8\u84b8\u998f\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5c06\u8bad\u7ec3\u8fd0\u884c\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe39%\u3002", "conclusion": "\u6570\u636e\u8d28\u91cf\u548c\u517c\u5bb9\u6027\u5bf9\u4e8eLLM\u7684\u6709\u6548\u548c\u9ad8\u6548\u84b8\u998f\u81f3\u5173\u91cd\u8981\uff0cSRD\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u73b0\u8fd9\u4e24\u8005\u7684\u539f\u5219\u6027\u6846\u67b6\u3002\u8fd9\u9879\u5de5\u4f5c\u63d0\u9ad8\u4e86\u5bf9KD\u4e2d\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u56e0\u7d20\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u63d0\u9ad8\u538b\u7f29LLM\u7684\u80fd\u529b\u548c\u6548\u7387\u63d0\u4f9b\u4e86\u5b9e\u8df5\u89c1\u89e3\u3002"}}
{"id": "2508.05954", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05954", "abs": "https://arxiv.org/abs/2508.05954", "authors": ["Han Lin", "Jaemin Cho", "Amir Zadeh", "Chuan Li", "Mohit Bansal"], "title": "Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents", "comment": "Project Page: https://bifrost-1.github.io", "summary": "There is growing interest in integrating high-fidelity visual synthesis\ncapabilities into large language models (LLMs) without compromising their\nstrong reasoning capabilities. Existing methods that directly train LLMs or\nbridge LLMs and diffusion models usually suffer from costly training since the\nbackbone LLMs have not seen image representations during pretraining. We\npresent Bifrost-1, a unified framework that bridges pretrained multimodal LLMs\n(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent\nvariables, which are natively aligned with the MLLM's CLIP visual encoder.\nThese patch-level image embeddings are integrated into the diffusion model with\na lightweight adaptation of its ControlNet. To retain the original multimodal\nreasoning capabilities of MLLMs, we equip the MLLM with a visual generation\nbranch initialized from the original MLLM parameters when predicting the\npatch-level image embeddings. By seamlessly integrating pretrained MLLMs and\ndiffusion models with patch-level CLIP latents, our framework enables\nhigh-fidelity controllable image generation with significant training\nefficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or\nbetter performance than previous methods in terms of visual fidelity and\nmultimodal understanding, with substantially lower compute during training. We\nalso provide comprehensive ablation studies showing the effectiveness of our\ndesign choices.", "AI": {"tldr": "Bifrost-1\u4f7f\u7528patch-level CLIP\u56fe\u50cf\u5d4c\u5165\u8fde\u63a5\u9884\u8bad\u7ec3\u7684MLLM\u548c\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u9ad8\u4fdd\u771f\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u5c06\u9ad8\u4fdd\u771f\u89c6\u89c9\u5408\u6210\u80fd\u529b\u6574\u5408\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\uff0c\u800c\u4e0d\u5f71\u54cd\u5176\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8fd9\u5f15\u8d77\u4e86\u8d8a\u6765\u8d8a\u591a\u7684\u5174\u8da3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6846\u67b6Bifrost-1\uff0c\u8be5\u6846\u67b6\u4f7f\u7528patch-level CLIP\u56fe\u50cf\u5d4c\u5165\u4f5c\u4e3a\u6f5c\u5728\u53d8\u91cf\uff0c\u8fde\u63a5\u4e86\u9884\u8bad\u7ec3\u7684\u591a\u6a21\u6001LLM\uff08MLLM\uff09\u548c\u6269\u6563\u6a21\u578b\u3002", "result": "Bifrost-1\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u53ef\u63a7\u56fe\u50cf\u751f\u6210\uff0c\u5e76\u5177\u6709\u663e\u8457\u7684\u8bad\u7ec3\u6548\u7387\u3002", "conclusion": "Bifrost-1\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u591a\u6a21\u6001\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u4e0e\u5148\u524d\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u671f\u95f4\u7684\u8ba1\u7b97\u91cf\u3002"}}
{"id": "2508.06226", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06226", "abs": "https://arxiv.org/abs/2508.06226", "authors": ["Yumeng Fu", "Jiayin Zhu", "Lingling Zhang", "Bo Zhao", "Shaoxuan Ma", "Yushun Zhang", "Yanrui Wu", "Wenjun Wu"], "title": "GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines", "comment": null, "summary": "Geometry problem solving (GPS) requires models to master diagram\ncomprehension, logical reasoning, knowledge application, numerical computation,\nand auxiliary line construction. This presents a significant challenge for\nMultimodal Large Language Models (MLLMs). However, existing benchmarks for\nevaluating MLLM geometry skills overlook auxiliary line construction and lack\nfine-grained process evaluation, making them insufficient for assessing MLLMs'\nlong-step reasoning abilities. To bridge these gaps, we present the GeoLaux\nbenchmark, comprising 2,186 geometry problems, incorporating both calculation\nand proving questions. Notably, the problems require an average of 6.51\nreasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliary\nline construction. Building on the dataset, we design a novel five-dimensional\nevaluation strategy assessing answer correctness, process correctness, process\nquality, auxiliary line impact, and error causes. Extensive experiments on 13\nleading MLLMs (including thinking models and non-thinking models) yield three\npivotal findings: First, models exhibit substantial performance degradation in\nextended reasoning steps (nine models demonstrate over 50% performance drop).\nSecond, compared to calculation problems, MLLMs tend to take shortcuts when\nsolving proving problems. Third, models lack auxiliary line awareness, and\nenhancing this capability proves particularly beneficial for overall geometry\nreasoning improvement. These findings establish GeoLaux as both a benchmark for\nevaluating MLLMs' long-step geometric reasoning with auxiliary lines and a\nguide for capability advancement. Our dataset and code are included in\nsupplementary materials and will be released.", "AI": {"tldr": "GeoLaux \u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30 MLLM \u51e0\u4f55\u63a8\u7406\u80fd\u529b\u7684\u65b0\u57fa\u51c6\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u957f\u6b65\u9aa4\u63a8\u7406\u548c\u8f85\u52a9\u7ebf\u6784\u9020\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u76ee\u524d\u7684 MLLM \u5728\u8fd9\u4e9b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684 MLLM \u51e0\u4f55\u6280\u80fd\u8bc4\u4f30\u57fa\u51c6\u5ffd\u7565\u4e86\u8f85\u52a9\u7ebf\u6784\u9020\uff0c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u8fc7\u7a0b\u8bc4\u4f30\uff0c\u4e0d\u8db3\u4ee5\u8bc4\u4f30 MLLM \u7684\u957f\u6b65\u9aa4\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86 GeoLaux \u57fa\u51c6\uff0c\u5305\u542b 2,186 \u4e2a\u51e0\u4f55\u95ee\u9898\uff0c\u8003\u5bdf\u8ba1\u7b97\u548c\u8bc1\u660e\u9898\uff0c\u5e73\u5747 6.51 \u4e2a\u63a8\u7406\u6b65\u9aa4\uff0c41.8% \u9700\u8981\u8f85\u52a9\u7ebf\u3002", "result": "\u5bf9 13 \u4e2a MLLM \u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff1a\u6a21\u578b\u5728\u6269\u5c55\u63a8\u7406\u6b65\u9aa4\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff08\u8d85\u8fc7 50%\uff09\uff0c\u5728\u89e3\u51b3\u8bc1\u660e\u95ee\u9898\u65f6\u503e\u5411\u4e8e\u8d70\u6377\u5f84\uff0c\u5e76\u4e14\u7f3a\u4e4f\u8f85\u52a9\u7ebf\u610f\u8bc6\uff1b\u63d0\u9ad8\u8f85\u52a9\u7ebf\u80fd\u529b\u5bf9\u63d0\u5347\u6574\u4f53\u51e0\u4f55\u63a8\u7406\u80fd\u529b\u975e\u5e38\u6709\u76ca\u3002", "conclusion": "GeoLaux \u8bc1\u660e\u4e86 MLLM \u5728\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u4e2d\uff0c\u5c24\u5176\u662f\u5728\u957f\u63a8\u7406\u6b65\u9aa4\u548c\u8f85\u52a9\u7ebf\u4f7f\u7528\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u80fd\u529b\u63d0\u5347\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2508.06034", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06034", "abs": "https://arxiv.org/abs/2508.06034", "authors": ["Qin Chen", "Guojie Song"], "title": "Adaptive Heterogeneous Graph Neural Networks: Bridging Heterophily and Heterogeneity", "comment": "Accepted tp CIKM 2025", "summary": "Heterogeneous graphs (HGs) are common in real-world scenarios and often\nexhibit heterophily. However, most existing studies focus on either\nheterogeneity or heterophily in isolation, overlooking the prevalence of\nheterophilic HGs in practical applications. Such ignorance leads to their\nperformance degradation. In this work, we first identify two main challenges in\nmodeling heterophily HGs: (1) varying heterophily distributions across hops and\nmeta-paths; (2) the intricate and often heterophily-driven diversity of\nsemantic information across different meta-paths. Then, we propose the Adaptive\nHeterogeneous Graph Neural Network (AHGNN) to tackle these challenges. AHGNN\nemploys a heterophily-aware convolution that accounts for heterophily\ndistributions specific to both hops and meta-paths. It then integrates messages\nfrom diverse semantic spaces using a coarse-to-fine attention mechanism, which\nfilters out noise and emphasizes informative signals. Experiments on seven\nreal-world graphs and twenty baselines demonstrate the superior performance of\nAHGNN, particularly in high-heterophily situations.", "AI": {"tldr": "AHGNN\u89e3\u51b3\u4e86\u5f02\u55dc\u6027\u5f02\u6784\u56fe\u5efa\u6a21\u4e2d\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5f02\u55dc\u6027\u611f\u77e5\u5377\u79ef\u548c\u7c97\u5230\u7ec6\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u5f02\u55dc\u6027\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5927\u591a\u5b64\u7acb\u5730\u5173\u6ce8\u5f02\u6784\u6027\u6216\u5f02\u55dc\u6027\uff0c\u5ffd\u7565\u4e86\u5f02\u55dc\u6027HG\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u666e\u904d\u6027\u3002\u8fd9\u79cd\u5ffd\u7565\u5bfc\u81f4\u5b83\u4eec\u7684\u6027\u80fd\u4e0b\u964d\u3002", "method": "AHGNN\u91c7\u7528\u4e86\u4e00\u79cd\u5f02\u55dc\u6027\u611f\u77e5\u5377\u79ef\uff0c\u8be5\u5377\u79ef\u8003\u8651\u4e86\u7279\u5b9a\u4e8e\u8df3\u548c\u5143\u8def\u5f84\u7684\u5f02\u55dc\u6027\u5206\u5e03\u3002\u7136\u540e\uff0c\u5b83\u4f7f\u7528\u4ece\u7c97\u5230\u7ec6\u7684\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u6765\u81ea\u4e0d\u540c\u8bed\u4e49\u7a7a\u95f4\u7684\u6d88\u606f\uff0c\u4ece\u800c\u6ee4\u9664\u566a\u58f0\u5e76\u5f3a\u8c03\u4fe1\u606f\u4fe1\u53f7\u3002", "result": "AHGNN\u5728\u4e03\u4e2a\u771f\u5b9e\u4e16\u754c\u56fe\u548c\u4e8c\u5341\u4e2a\u57fa\u7ebf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAHGNN\u5177\u6709\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u5f02\u55dc\u6027\u60c5\u51b5\u4e0b\u3002", "conclusion": "AHGNN\u5728\u4e03\u4e2a\u771f\u5b9e\u4e16\u754c\u56fe\u548c\u4e8c\u5341\u4e2a\u57fa\u7ebf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAHGNN\u5177\u6709\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u5f02\u55dc\u6027\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2508.05680", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05680", "abs": "https://arxiv.org/abs/2508.05680", "authors": ["Stefanie Urchs", "Veronika Thurner", "Matthias A\u00dfenmacher", "Ludwig Bothmann", "Christian Heumann", "Stephanie Thiemichen"], "title": "Are All Genders Equal in the Eyes of Algorithms? -- Analysing Search and Retrieval Algorithms for Algorithmic Gender Fairness", "comment": null, "summary": "Algorithmic systems such as search engines and information retrieval\nplatforms significantly influence academic visibility and the dissemination of\nknowledge. Despite assumptions of neutrality, these systems can reproduce or\nreinforce societal biases, including those related to gender. This paper\nintroduces and applies a bias-preserving definition of algorithmic gender\nfairness, which assesses whether algorithmic outputs reflect real-world gender\ndistributions without introducing or amplifying disparities. Using a\nheterogeneous dataset of academic profiles from German universities and\nuniversities of applied sciences, we analyse gender differences in metadata\ncompleteness, publication retrieval in academic databases, and visibility in\nGoogle search results. While we observe no overt algorithmic discrimination,\nour findings reveal subtle but consistent imbalances: male professors are\nassociated with a greater number of search results and more aligned publication\nrecords, while female professors display higher variability in digital\nvisibility. These patterns reflect the interplay between platform algorithms,\ninstitutional curation, and individual self-presentation. Our study highlights\nthe need for fairness evaluations that account for both technical performance\nand representational equality in digital systems.", "AI": {"tldr": "\u7b97\u6cd5\u7cfb\u7edf\u53ef\u80fd\u65e0\u610f\u4e2d\u5f15\u5165\u6027\u522b\u504f\u89c1\uff0c\u8fd9\u9879\u7814\u7a76\u5206\u6790\u4e86\u5b66\u672f\u754c\u6570\u5b57\u53ef\u89c1\u6027\u4e2d\u7684\u6027\u522b\u5dee\u5f02\uff0c\u53d1\u73b0\u7537\u6027\u6559\u6388\u7684\u53ef\u89c1\u6027\u901a\u5e38\u9ad8\u4e8e\u5973\u6027\u6559\u6388\u3002", "motivation": "\u641c\u7d22\u5f15\u64ce\u548c\u4fe1\u606f\u68c0\u7d22\u5e73\u53f0\u7b49\u7b97\u6cd5\u7cfb\u7edf\u4f1a\u663e\u8457\u5f71\u54cd\u5b66\u672f\u53ef\u89c1\u6027\u548c\u77e5\u8bc6\u4f20\u64ad\u3002\u5c3d\u7ba1\u5047\u8bbe\u662f\u4e2d\u7acb\u7684\uff0c\u4f46\u8fd9\u4e9b\u7cfb\u7edf\u53ef\u80fd\u4f1a\u91cd\u73b0\u6216\u52a0\u5f3a\u793e\u4f1a\u504f\u89c1\uff0c\u5305\u62ec\u4e0e\u6027\u522b\u76f8\u5173\u7684\u504f\u89c1\u3002", "method": "\u4f7f\u7528\u6765\u81ea\u5fb7\u56fd\u5927\u5b66\u548c\u5e94\u7528\u79d1\u5b66\u5927\u5b66\u7684\u5b66\u672f\u6982\u51b5\u7684\u5f02\u6784\u6570\u636e\u96c6\uff0c\u5206\u6790\u5143\u6570\u636e\u5b8c\u6574\u6027\u3001\u5b66\u672f\u6570\u636e\u5e93\u4e2d\u7684\u51fa\u7248\u7269\u68c0\u7d22\u4ee5\u53ca\u8c37\u6b4c\u641c\u7d22\u7ed3\u679c\u4e2d\u7684\u6027\u522b\u5dee\u5f02\u3002", "result": "\u867d\u7136\u6211\u4eec\u6ca1\u6709\u89c2\u5bdf\u5230\u516c\u5f00\u7684\u7b97\u6cd5\u6b67\u89c6\uff0c\u4f46\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u5fae\u5999\u4f46\u4e00\u81f4\u7684\u4e0d\u5e73\u8861\uff1a\u7537\u6027\u6559\u6388\u4e0e\u66f4\u591a\u7684\u641c\u7d22\u7ed3\u679c\u548c\u66f4\u4e00\u81f4\u7684\u51fa\u7248\u8bb0\u5f55\u76f8\u5173\u8054\uff0c\u800c\u5973\u6027\u6559\u6388\u5728\u6570\u5b57\u53ef\u89c1\u6027\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u53ef\u53d8\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u9700\u8981\u5728\u6570\u5b57\u7cfb\u7edf\u4e2d\u8fdb\u884c\u516c\u5e73\u6027\u8bc4\u4f30\uff0c\u65e2\u8981\u8003\u8651\u6280\u672f\u6027\u80fd\uff0c\u4e5f\u8981\u8003\u8651\u4ee3\u8868\u6027\u5e73\u7b49\u3002"}}
{"id": "2508.06149", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06149", "abs": "https://arxiv.org/abs/2508.06149", "authors": ["Gunhee Cho", "Yun-Gyung Cheong"], "title": "Scaling Personality Control in LLMs with Big Five Scaler Prompts", "comment": null, "summary": "We present Big5-Scaler, a prompt-based framework for conditioning large\nlanguage models (LLMs) with controllable Big Five personality traits. By\nembedding numeric trait values into natural language prompts, our method\nenables fine-grained personality control without additional training. We\nevaluate Big5-Scaler across trait expression, dialogue generation, and human\ntrait imitation tasks. Results show that it induces consistent and\ndistinguishable personality traits across models, with performance varying by\nprompt type and scale. Our analysis highlights the effectiveness of concise\nprompts and lower trait intensities, providing a efficient approach for\nbuilding personality-aware dialogue agents.", "AI": {"tldr": "Big5-Scaler \u662f\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u7684\u6846\u67b6\uff0c\u5b83\u5141\u8bb8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6ca1\u6709\u4eba\u683c\u7684\u60c5\u51b5\u4e0b\u63a7\u5236\u4eba\u683c\u7279\u8d28\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u4eba\u683c\u5316\u5bf9\u8bdd\u3002", "motivation": "\u8bba\u6587\u63d0\u51fa\u4e86 Big5-Scaler\uff0c\u65e8\u5728\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5177\u5907\u53ef\u63a7\u7684\u4eba\u683c\u7279\u8d28\u3002", "method": "Big5-Scaler \u662f\u4e00\u4e2a\u57fa\u4e8e\u63d0\u793a\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f7f\u7528\u53ef\u63a7\u7684 Big Five \u4eba\u683c\u7279\u8d28\u6765\u8c03\u8282\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM)\u3002\u901a\u8fc7\u5c06\u6570\u5b57\u7279\u5f81\u503c\u5d4c\u5165\u5230\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u4e2d\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u4eba\u683c\u63a7\u5236\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cBig5-Scaler \u53ef\u4ee5\u5728\u4e0d\u540c\u6a21\u578b\u4e2d\u8bf1\u5bfc\u51fa\u4e00\u81f4\u4e14\u53ef\u533a\u5206\u7684\u4eba\u683c\u7279\u8d28\uff0c\u5176\u6027\u80fd\u968f\u63d0\u793a\u7c7b\u578b\u548c\u89c4\u6a21\u800c\u53d8\u5316\u3002", "conclusion": "Big5-Scaler \u901a\u8fc7\u4f7f\u7528\u7b80\u6d01\u7684\u63d0\u793a\u548c\u8f83\u4f4e\u7684\u7279\u8d28\u5f3a\u5ea6\uff0c\u4e3a\u6784\u5efa\u5177\u6709\u4eba\u683c\u611f\u77e5\u80fd\u529b\u7684\u5bf9\u8bdd\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.05976", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05976", "abs": "https://arxiv.org/abs/2508.05976", "authors": ["Zhihao Zhu", "Yifan Zheng", "Siyu Pan", "Yaohui Jin", "Yao Mu"], "title": "PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation", "comment": "Accepted to ICCV 2025. 8 pages main paper, 8 figures, plus\n  supplementary material", "summary": "The fragmentation between high-level task semantics and low-level geometric\nfeatures remains a persistent challenge in robotic manipulation. While\nvision-language models (VLMs) have shown promise in generating affordance-aware\nvisual representations, the lack of semantic grounding in canonical spaces and\nreliance on manual annotations severely limit their ability to capture dynamic\nsemantic-affordance relationships. To address these, we propose Primitive-Aware\nSemantic Grounding (PASG), a closed-loop framework that introduces: (1)\nAutomatic primitive extraction through geometric feature aggregation, enabling\ncross-category detection of keypoints and axes; (2) VLM-driven semantic\nanchoring that dynamically couples geometric primitives with functional\naffordances and task-relevant description; (3) A spatial-semantic reasoning\nbenchmark and a fine-tuned VLM (Qwen2.5VL-PA). We demonstrate PASG's\neffectiveness in practical robotic manipulation tasks across diverse scenarios,\nachieving performance comparable to manual annotations. PASG achieves a\nfiner-grained semantic-affordance understanding of objects, establishing a\nunified paradigm for bridging geometric primitives with task semantics in\nrobotic manipulation.", "AI": {"tldr": "PASG \u5f25\u5408\u4e86\u51e0\u4f55\u57fa\u5143\u548c\u4efb\u52a1\u8bed\u4e49\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\u3002", "motivation": "\u9ad8\u7ea7\u4efb\u52a1\u8bed\u4e49\u548c\u4f4e\u7ea7\u51e0\u4f55\u7279\u5f81\u4e4b\u95f4\u7684\u788e\u7247\u5316\u4ecd\u7136\u662f\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u4e00\u4e2a\u6301\u7eed\u5b58\u5728\u7684\u6311\u6218\u3002\u867d\u7136\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5728\u751f\u6210\u53ef\u4f9b\u6027\u611f\u77e5\u89c6\u89c9\u8868\u793a\u65b9\u9762\u663e\u793a\u51fa\u5e0c\u671b\uff0c\u4f46\u89c4\u8303\u7a7a\u95f4\u4e2d\u7f3a\u4e4f\u8bed\u4e49\u57fa\u7840\u4ee5\u53ca\u4f9d\u8d56\u4e8e\u624b\u52a8\u6ce8\u91ca\u4e25\u91cd\u9650\u5236\u4e86\u5b83\u4eec\u6355\u83b7\u52a8\u6001\u8bed\u4e49-\u53ef\u4f9b\u6027\u5173\u7cfb\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u5143\u611f\u77e5\u8bed\u4e49\u63a5\u5730 (PASG) \u7684\u95ed\u73af\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5f15\u5165\uff1a(1) \u901a\u8fc7\u51e0\u4f55\u7279\u5f81\u805a\u5408\u81ea\u52a8\u63d0\u53d6\u57fa\u5143\uff0c\u5b9e\u73b0\u5173\u952e\u70b9\u548c\u8f74\u7684\u8de8\u7c7b\u522b\u68c0\u6d4b\uff1b(2) VLM \u9a71\u52a8\u7684\u8bed\u4e49\u951a\u5b9a\uff0c\u52a8\u6001\u5730\u5c06\u51e0\u4f55\u57fa\u5143\u4e0e\u529f\u80fd\u53ef\u4f9b\u6027\u548c\u4efb\u52a1\u76f8\u5173\u63cf\u8ff0\u76f8\u7ed3\u5408\uff1b(3) \u7a7a\u95f4\u8bed\u4e49\u63a8\u7406\u57fa\u51c6\u548c\u5fae\u8c03\u7684 VLM (Qwen2.5VL-PA)\u3002", "result": "\u5728\u5404\u79cd\u5b9e\u9645\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8bc1\u660e\u4e86 PASG \u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u4e0e\u624b\u52a8\u6ce8\u91ca\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "PASG \u901a\u8fc7\u5bf9\u7269\u4f53\u8fdb\u884c\u66f4\u7ec6\u7c92\u5ea6\u7684\u8bed\u4e49-\u53ef\u4f9b\u6027\u7406\u89e3\uff0c\u4e3a\u6865\u63a5\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u51e0\u4f55\u57fa\u5143\u4e0e\u4efb\u52a1\u8bed\u4e49\u5efa\u7acb\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u8303\u4f8b\u3002"}}
{"id": "2508.06230", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06230", "abs": "https://arxiv.org/abs/2508.06230", "authors": ["Ruben Sharma", "Sebastijan Duman\u010di\u0107", "Ross D. King", "Andrew Cropper"], "title": "Learning Logical Rules using Minimum Message Length", "comment": null, "summary": "Unifying probabilistic and logical learning is a key challenge in AI. We\nintroduce a Bayesian inductive logic programming approach that learns minimum\nmessage length programs from noisy data. Our approach balances hypothesis\ncomplexity and data fit through priors, which explicitly favour more general\nprograms, and a likelihood that favours accurate programs. Our experiments on\nseveral domains, including game playing and drug design, show that our method\nsignificantly outperforms previous methods, notably those that learn minimum\ndescription length programs. Our results also show that our approach is\ndata-efficient and insensitive to example balance, including the ability to\nlearn from exclusively positive examples.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8d1d\u53f6\u65af\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u5177\u6709\u6570\u636e\u6548\u7387\u3002", "motivation": "\u7edf\u4e00\u6982\u7387\u5b66\u4e60\u548c\u903b\u8f91\u5b66\u4e60\u662f\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u8d1d\u53f6\u65af\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u5b66\u4e60\u6700\u5c0f\u6d88\u606f\u957f\u5ea6\u7a0b\u5e8f\u3002", "result": "\u8be5\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u4ee5\u524d\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u5177\u6709\u6570\u636e\u6548\u7387\uff0c\u5bf9\u793a\u4f8b\u5e73\u8861\u4e0d\u654f\u611f\uff0c\u5305\u62ec\u4ece\u5b8c\u5168\u6b63\u9762\u7684\u793a\u4f8b\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u9886\u57df\uff08\u5305\u62ec\u6e38\u620f\u548c\u836f\u7269\u8bbe\u8ba1\uff09\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u4ee5\u524d\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u5b66\u4e60\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\u7a0b\u5e8f\u7684\u65b9\u6cd5\u3002\u7ed3\u679c\u8fd8\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u6570\u636e\u6548\u7387\uff0c\u5bf9\u793a\u4f8b\u5e73\u8861\u4e0d\u654f\u611f\uff0c\u5305\u62ec\u4ece\u5b8c\u5168\u6b63\u9762\u7684\u793a\u4f8b\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\u3002"}}
{"id": "2508.06041", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06041", "abs": "https://arxiv.org/abs/2508.06041", "authors": ["Sangwoo Kwon", "Seong Hoon Seo", "Jae W. Lee", "Yeonhong Park"], "title": "DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment", "comment": null, "summary": "How can we effectively handle queries for on-device large language models\n(LLMs) with varying runtime constraints, such as latency and accuracy?\nMulti-scale quantization addresses this challenge by enabling memory-efficient\nruntime model adaptation of LLMs through the overlaying of multiple model\nvariants quantized to different bitwidths. Meanwhile, an important question\nstill remains open-ended: how can models be properly configured to match a\ntarget precision or latency? While mixed-precision offers a promising solution,\nwe take this further by leveraging the key observation that the sensitivity of\neach layer dynamically changes across decoding iterations. Building on this\ninsight, we introduce DP-LLM, a novel mechanism that dynamically assigns\nprecision to each layer based on input values. DP-LLM augments each linear\nlayer in an LLM with a precision selector that determines the bitwidth at\nruntime using a lightweight error estimator and threshold values learned\nthrough fine-tuning. Experimental results across multiple models and benchmarks\ndemonstrate that DP-LLM achieves a superior performance-latency trade-off,\noutperforming prior approaches.", "AI": {"tldr": "DP-LLM dynamically adjusts layer precision in LLMs based on input, improving performance-latency trade-off.", "motivation": "Effectively handle queries for on-device large language models (LLMs) with varying runtime constraints, such as latency and accuracy.", "method": "DP-LLM dynamically assigns precision to each layer based on input values using a precision selector with a lightweight error estimator and learned threshold values.", "result": "Demonstrates that DP-LLM achieves a superior performance-latency trade-off, outperforming prior approaches.", "conclusion": "DP-LLM achieves a superior performance-latency trade-off, outperforming prior approaches."}}
{"id": "2508.05688", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.05688", "abs": "https://arxiv.org/abs/2508.05688", "authors": ["Aleksei Shestov", "Omar Zoloev", "Maksim Makarenko", "Mikhail Orlov", "Egor Fadeev", "Ivan Kireev", "Andrey Savchenko"], "title": "LLM4ES: Learning User Embeddings from Event Sequences via Large Language Models", "comment": null, "summary": "This paper presents LLM4ES, a novel framework that exploits large pre-trained\nlanguage models (LLMs) to derive user embeddings from event sequences. Event\nsequences are transformed into a textual representation, which is subsequently\nused to fine-tune an LLM through next-token prediction to generate high-quality\nembeddings. We introduce a text enrichment technique that enhances LLM\nadaptation to event sequence data, improving representation quality for\nlow-variability domains. Experimental results demonstrate that LLM4ES achieves\nstate-of-the-art performance in user classification tasks in financial and\nother domains, outperforming existing embedding methods. The resulting user\nembeddings can be incorporated into a wide range of applications, from user\nsegmentation in finance to patient outcome prediction in healthcare.", "AI": {"tldr": "LLM4ES\uff1a\u4e00\u79cd\u5229\u7528LLM\u4ece\u4e8b\u4ef6\u5e8f\u5217\u4e2d\u63d0\u53d6\u9ad8\u8d28\u91cf\u7528\u6237\u5d4c\u5165\u7684\u65b0\u6846\u67b6\uff0c\u5e76\u5728\u7528\u6237\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5229\u7528\u5927\u578b\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ece\u4e8b\u4ef6\u5e8f\u5217\u4e2d\u63d0\u53d6\u7528\u6237\u5d4c\u5165\u3002", "method": "\u5229\u7528\u5927\u578b\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u901a\u8fc7next-token prediction\u5fae\u8c03LLM\uff0c\u4ece\u4e8b\u4ef6\u5e8f\u5217\u4e2d\u63d0\u53d6\u7528\u6237\u5d4c\u5165\u3002", "result": "LLM4ES\u5728\u91d1\u878d\u7b49\u9886\u57df\u7684\u7528\u6237\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86state-of-the-art\u7684\u8868\u73b0\u3002", "conclusion": "LLM4ES\u5728\u7528\u6237\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u91d1\u878d\u7528\u6237\u5206\u7fa4\u548c\u533b\u7597\u60a3\u8005\u7ed3\u679c\u9884\u6d4b\u7b49\u9886\u57df\u3002"}}
{"id": "2508.06155", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06155", "abs": "https://arxiv.org/abs/2508.06155", "authors": ["Renhan Zhang", "Lian Lian", "Zhen Qi", "Guiran Liu"], "title": "Semantic and Structural Analysis of Implicit Biases in Large Language Models: An Interpretable Approach", "comment": null, "summary": "This paper addresses the issue of implicit stereotypes that may arise during\nthe generation process of large language models. It proposes an interpretable\nbias detection method aimed at identifying hidden social biases in model\noutputs, especially those semantic tendencies that are not easily captured\nthrough explicit linguistic features. The method combines nested semantic\nrepresentation with a contextual contrast mechanism. It extracts latent bias\nfeatures from the vector space structure of model outputs. Using attention\nweight perturbation, it analyzes the model's sensitivity to specific social\nattribute terms, thereby revealing the semantic pathways through which bias is\nformed. To validate the effectiveness of the method, this study uses the\nStereoSet dataset, which covers multiple stereotype dimensions including\ngender, profession, religion, and race. The evaluation focuses on several key\nmetrics, such as bias detection accuracy, semantic consistency, and contextual\nsensitivity. Experimental results show that the proposed method achieves strong\ndetection performance across various dimensions. It can accurately identify\nbias differences between semantically similar texts while maintaining high\nsemantic alignment and output stability. The method also demonstrates high\ninterpretability in its structural design. It helps uncover the internal bias\nassociation mechanisms within language models. This provides a more transparent\nand reliable technical foundation for bias detection. The approach is suitable\nfor real-world applications where high trustworthiness of generated content is\nrequired.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u504f\u89c1\u68c0\u6d4b\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u8bc6\u522b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9690\u6027\u793e\u4f1a\u504f\u89c1\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u9690\u6027\u523b\u677f\u5370\u8c61\u95ee\u9898\uff0c\u5c24\u5176\u662f\u4e00\u4e9b\u96be\u4ee5\u901a\u8fc7\u663e\u5f0f\u8bed\u8a00\u7279\u5f81\u6355\u6349\u7684\u8bed\u4e49\u503e\u5411\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5d4c\u5957\u8bed\u4e49\u8868\u793a\u548c\u4e0a\u4e0b\u6587\u5bf9\u6bd4\u673a\u5236\uff0c\u4ece\u6a21\u578b\u8f93\u51fa\u7684\u5411\u91cf\u7a7a\u95f4\u7ed3\u6784\u4e2d\u63d0\u53d6\u6f5c\u5728\u7684\u504f\u89c1\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u6743\u91cd\u6270\u52a8\u5206\u6790\u6a21\u578b\u5bf9\u7279\u5b9a\u793e\u4f1a\u5c5e\u6027\u8bcd\u7684\u654f\u611f\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u522b\u3001\u804c\u4e1a\u3001\u5b97\u6559\u548c\u79cd\u65cf\u7b49\u591a\u4e2a\u523b\u677f\u5370\u8c61\u7ef4\u5ea6\u4e0a\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u8bed\u4e49\u76f8\u4f3c\u6587\u672c\u4e4b\u95f4\u7684\u504f\u89c1\u5dee\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u8f93\u51fa\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u504f\u89c1\u68c0\u6d4b\u65b9\u6cd5\uff0c\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u4e2d\u9690\u85cf\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u5e76\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u53d6\u5f97\u4e86\u826f\u597d\u7684\u68c0\u6d4b\u6548\u679c\uff0c\u4e3a\u63d0\u9ad8\u751f\u6210\u5185\u5bb9\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u6280\u672f\u57fa\u7840\u3002"}}
{"id": "2508.05982", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05982", "abs": "https://arxiv.org/abs/2508.05982", "authors": ["Qingyang Liu", "Bingjie Gao", "Weiheng Huang", "Jun Zhang", "Zhongqian Sun", "Yang Wei", "Zelin Peng", "Qianli Ma", "Shuai Yang", "Zhaohe Liao", "Haonan Zhao", "Li Niu"], "title": "AnimateScene: Camera-controllable Animation in Any Scene", "comment": null, "summary": "3D scene reconstruction and 4D human animation have seen rapid progress and\nbroad adoption in recent years. However, seamlessly integrating reconstructed\nscenes with 4D human animation to produce visually engaging results remains\nchallenging. One key difficulty lies in placing the human at the correct\nlocation and scale within the scene while avoiding unrealistic\ninterpenetration. Another challenge is that the human and the background may\nexhibit different lighting and style, leading to unrealistic composites. In\naddition, appealing character motion videos are often accompanied by camera\nmovements, which means that the viewpoints need to be reconstructed along a\nspecified trajectory. We present AnimateScene, which addresses the above issues\nin a unified framework. First, we design an accurate placement module that\nautomatically determines a plausible 3D position for the human and prevents any\ninterpenetration within the scene during motion. Second, we propose a\ntraining-free style alignment method that adapts the 4D human representation to\nmatch the background's lighting and style, achieving coherent visual\nintegration. Finally, we design a joint post-reconstruction method for both the\n4D human and the 3D scene that allows camera trajectories to be inserted,\nenabling the final rendered video to feature visually appealing camera\nmovements. Extensive experiments show that AnimateScene generates dynamic scene\nvideos with high geometric detail and spatiotemporal coherence across various\ncamera and action combinations.", "AI": {"tldr": "AnimateScene\u89e3\u51b3\u4e86\u5c064D\u4eba\u4f53\u52a8\u753b\u65e0\u7f1d\u96c6\u6210\u52303D\u573a\u666f\u4e2d\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7cbe\u786e\u653e\u7f6e\u3001\u98ce\u683c\u5bf9\u9f50\u548c\u8054\u5408\u540e\u91cd\u5efa\u6765\u5b9e\u73b0\u903c\u771f\u7684\u52a8\u6001\u573a\u666f\u89c6\u9891\u3002", "motivation": "\u65e0\u7f1d\u5730\u5c06\u91cd\u5efa\u7684\u573a\u666f\u4e0e4D\u4eba\u4f53\u52a8\u753b\u96c6\u6210\u4ee5\u4ea7\u751f\u89c6\u89c9\u4e0a\u5438\u5f15\u4eba\u7684\u7ed3\u679c\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u4e00\u4e2a\u5173\u952e\u7684\u56f0\u96be\u5728\u4e8e\u5c06\u4eba\u653e\u7f6e\u5728\u573a\u666f\u4e2d\u6b63\u786e\u7684\u4f4d\u7f6e\u548c\u6bd4\u4f8b\uff0c\u540c\u65f6\u907f\u514d\u4e0d\u771f\u5b9e\u7684\u76f8\u4e92\u6e17\u900f\u3002\u53e6\u4e00\u4e2a\u6311\u6218\u662f\uff0c\u4eba\u548c\u80cc\u666f\u53ef\u80fd\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u5149\u7167\u548c\u98ce\u683c\uff0c\u5bfc\u81f4\u4e0d\u771f\u5b9e\u7684\u5408\u6210\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7cbe\u786e\u7684\u653e\u7f6e\u6a21\u5757\uff0c\u81ea\u52a8\u786e\u5b9a\u4eba\u4f53\u5408\u7406\u76843D\u4f4d\u7f6e\uff0c\u5e76\u5728\u8fd0\u52a8\u8fc7\u7a0b\u4e2d\u9632\u6b62\u573a\u666f\u5185\u7684\u4efb\u4f55\u76f8\u4e92\u6e17\u900f\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u514d\u8bad\u7ec3\u7684\u98ce\u683c\u5bf9\u9f50\u65b9\u6cd5\uff0c\u4f7f4D\u4eba\u4f53\u8868\u793a\u9002\u5e94\u80cc\u666f\u7684\u7167\u660e\u548c\u98ce\u683c\uff0c\u5b9e\u73b0\u8fde\u8d2f\u7684\u89c6\u89c9\u96c6\u6210\u3002\u4e3a4D\u4eba\u4f53\u548c3D\u573a\u666f\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8054\u5408\u7684\u540e\u91cd\u5efa\u65b9\u6cd5\uff0c\u5141\u8bb8\u63d2\u5165\u76f8\u673a\u8f68\u8ff9\uff0c\u4f7f\u6700\u7ec8\u6e32\u67d3\u7684\u89c6\u9891\u5177\u6709\u89c6\u89c9\u5438\u5f15\u529b\u7684\u76f8\u673a\u8fd0\u52a8\u3002", "result": "AnimateScene\u89e3\u51b3\u4e86\u4e0a\u8ff0\u95ee\u9898\u3002", "conclusion": "AnimateScene\u751f\u6210\u5177\u6709\u9ad8\u51e0\u4f55\u7ec6\u8282\u548c\u8de8\u5404\u79cd\u76f8\u673a\u548c\u52a8\u4f5c\u7ec4\u5408\u7684\u65f6\u7a7a\u8fde\u8d2f\u6027\u7684\u52a8\u6001\u573a\u666f\u89c6\u9891\u3002"}}
{"id": "2508.06263", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06263", "abs": "https://arxiv.org/abs/2508.06263", "authors": ["Andrew Cropper", "David M. Cerna", "Matti J\u00e4rvisalo"], "title": "Symmetry breaking for inductive logic programming", "comment": null, "summary": "The goal of inductive logic programming is to search for a hypothesis that\ngeneralises training data and background knowledge. The challenge is searching\nvast hypothesis spaces, which is exacerbated because many logically equivalent\nhypotheses exist. To address this challenge, we introduce a method to break\nsymmetries in the hypothesis space. We implement our idea in answer set\nprogramming. Our experiments on multiple domains, including visual reasoning\nand game playing, show that our approach can reduce solving times from over an\nhour to just 17 seconds.", "AI": {"tldr": "Introduces a method to break symmetries in the hypothesis space to address the challenge of searching vast hypothesis spaces. Implemented in answer set programming. Experiments show that the approach can reduce solving times.", "motivation": "The challenge is searching vast hypothesis spaces, which is exacerbated because many logically equivalent hypotheses exist.", "method": "A method to break symmetries in the hypothesis space is introduced. The idea is implemented in answer set programming.", "result": "Experiments on multiple domains, including visual reasoning and game playing, show that the approach can reduce solving times from over an hour to just 17 seconds.", "conclusion": "The approach can reduce solving times."}}
{"id": "2508.06066", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06066", "abs": "https://arxiv.org/abs/2508.06066", "authors": ["Barak Gahtan", "Alex M. Bronstein"], "title": "Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology", "comment": null, "summary": "Deep temporal architectures such as Temporal Convolutional Networks (TCNs)\nachieve strong predictive performance on sequential data, yet theoretical\nunderstanding of their generalization remains limited. We address this gap by\nproviding both the first non-vacuous, architecture-aware generalization bounds\nfor deep temporal models and a principled evaluation methodology.\n  For exponentially $\\beta$-mixing sequences, we derive bounds scaling as $\nO\\!\\Bigl(R\\,\\sqrt{\\tfrac{D\\,p\\,n\\,\\log N}{N}}\\Bigr), $ where $D$ is network\ndepth, $p$ kernel size, $n$ input dimension, and $R$ weight norm. Our\ndelayed-feedback blocking mechanism transforms dependent samples into\neffectively independent ones while discarding only $O(1/\\log N)$ of the data,\nyielding $\\sqrt{D}$ scaling instead of exponential, implying that doubling\ndepth requires approximately quadrupling the training data.\n  We also introduce a fair-comparison methodology that fixes the effective\nsample size to isolate the effect of temporal structure from information\ncontent. Under $N_{\\text{eff}}=2{,}000$, strongly dependent sequences\n($\\rho=0.8$) exhibit $\\approx76\\%$ smaller generalization gaps than weakly\ndependent ones ($\\rho=0.2$), challenging the intuition that dependence is\npurely detrimental. Yet convergence rates diverge from theory: weak\ndependencies follow $N_{\\text{eff}}^{-1.21}$ scaling and strong dependencies\nfollow $N_{\\text{eff}}^{-0.89}$, both steeper than the predicted $N^{-0.5}$.\nThese findings reveal that temporal dependence can enhance learning under fixed\ninformation budgets, while highlighting gaps between theory and practice that\nmotivate future research.", "AI": {"tldr": "The paper studies the generalization ability of deep temporal models, introduces generalization bounds and a fair comparison methodology. It finds temporal dependence can enhance learning but gaps exist between theory and practice.", "motivation": "Theoretical understanding of the generalization of deep temporal architectures such as Temporal Convolutional Networks (TCNs) remains limited.", "method": "Providing both the first non-vacuous, architecture-aware generalization bounds for deep temporal models and a principled evaluation methodology; delayed-feedback blocking mechanism transforms dependent samples into effectively independent ones", "result": "Bounds scaling as  O(R*sqrt((D*p*n*log N)/N)), where D is network depth, p kernel size, n input dimension, and R weight norm; under N_eff=2,000, strongly dependent sequences exhibit approximately 76% smaller generalization gaps than weakly dependent ones, weak dependencies follow N_eff**(-1.21) scaling and strong dependencies follow N_eff**(-0.89).", "conclusion": "Temporal dependence can enhance learning under fixed information budgets, while highlighting gaps between theory and practice that motivate future research."}}
{"id": "2508.05700", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05700", "abs": "https://arxiv.org/abs/2508.05700", "authors": ["Runze Su", "Jiayin Jin", "Jiacheng Li", "Sihan Wang", "Guangtong Bai", "Zelun Wang", "Li Tang", "Yixiong Meng", "Huasen Wu", "Zhimeng Pan", "Kungang Li", "Han Sun", "Zhifang Liu", "Haoyang Li", "Siping Ji", "Ling Leng", "Prathibha Deshikachar"], "title": "Multi-Faceted Large Embedding Tables for Pinterest Ads Ranking", "comment": null, "summary": "Large embedding tables are indispensable in modern recommendation systems,\nthanks to their ability to effectively capture and memorize intricate details\nof interactions among diverse entities. As we explore integrating large\nembedding tables into Pinterest's ads ranking models, we encountered not only\ncommon challenges such as sparsity and scalability, but also several obstacles\nunique to our context. Notably, our initial attempts to train large embedding\ntables from scratch resulted in neutral metrics. To tackle this, we introduced\na novel multi-faceted pretraining scheme that incorporates multiple pretraining\nalgorithms. This approach greatly enriched the embedding tables and resulted in\nsignificant performance improvements. As a result, the multi-faceted large\nembedding tables bring great performance gain on both the Click-Through Rate\n(CTR) and Conversion Rate (CVR) domains. Moreover, we designed a CPU-GPU hybrid\nserving infrastructure to overcome GPU memory limits and elevate the\nscalability. This framework has been deployed in the Pinterest Ads system and\nachieved 1.34% online CPC reduction and 2.60% CTR increase with neutral\nend-to-end latency change.", "AI": {"tldr": "This paper introduces a multi-faceted pretraining scheme and a CPU-GPU hybrid serving infrastructure to improve the performance and scalability of large embedding tables in Pinterest's ads ranking models.", "motivation": "explore integrating large embedding tables into Pinterest's ads ranking models", "method": "a novel multi-faceted pretraining scheme that incorporates multiple pretraining algorithms and a CPU-GPU hybrid serving infrastructure", "result": "achieved 1.34% online CPC reduction and 2.60% CTR increase with neutral end-to-end latency change", "conclusion": "multi-faceted large embedding tables bring great performance gain on both the Click-Through Rate (CTR) and Conversion Rate (CVR) domains"}}
{"id": "2508.06163", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06163", "abs": "https://arxiv.org/abs/2508.06163", "authors": ["Yingfeng Luo", "Dingyang Lin", "Junxin Wang", "Ziqiang Xu", "Kaiyan Chang", "Tong Zheng", "Bei Li", "Anxiang Ma", "Tong Xiao", "Zhengtao Yu", "Jingbo Zhu"], "title": "One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging", "comment": "Under review", "summary": "Model merging has emerged as a compelling data-free paradigm for multi-task\nlearning, enabling the fusion of multiple fine-tuned models into a single,\npowerful entity. A key technique in merging methods is sparsification, which\nprunes redundant parameters from task vectors to mitigate interference.\nHowever, prevailing approaches employ a ``one-size-fits-all'' strategy,\napplying a uniform sparsity ratio that overlooks the inherent structural and\nstatistical heterogeneity of model parameters. This often leads to a suboptimal\ntrade-off, where critical parameters are inadvertently pruned while less useful\nones are retained. To address this limitation, we introduce \\textbf{TADrop}\n(\\textbf{T}ensor-wise \\textbf{A}daptive \\textbf{Drop}), an adaptive\nsparsification strategy that respects this heterogeneity. Instead of a global\nratio, TADrop assigns a tailored sparsity level to each parameter tensor based\non its distributional properties. The core intuition is that tensors with\ndenser, more redundant distributions can be pruned aggressively, while sparser,\nmore critical ones are preserved. As a simple and plug-and-play module, we\nvalidate TADrop by integrating it with foundational, classic, and SOTA merging\nmethods. Extensive experiments across diverse tasks (vision, language, and\nmultimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and\nsignificantly boosts their performance. For instance, when enhancing a leading\nmerging method, it achieves an average performance gain of 2.0\\% across 8\nViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter\ninterference by tailoring sparsification to the model's structure, offering a\nnew baseline for high-performance model merging.", "AI": {"tldr": "TADrop is an adaptive sparsification strategy that improves model merging by tailoring the sparsity level of each parameter tensor.", "motivation": "Prevailing approaches employ a \"one-size-fits-all\" strategy, applying a uniform sparsity ratio that overlooks the inherent structural and statistical heterogeneity of model parameters. This often leads to a suboptimal trade-off, where critical parameters are inadvertently pruned while less useful ones are retained.", "method": "introduce TADrop (Tensor-wise Adaptive Drop), an adaptive sparsification strategy that respects heterogeneity by assigning a tailored sparsity level to each parameter tensor based on its distributional properties", "result": "TADrop consistently and significantly boosts performance across diverse tasks (vision, language, and multimodal) and models (ViT, BEiT). For instance, when enhancing a leading merging method, it achieves an average performance gain of 2.0% across 8 ViT-B/32 tasks.", "conclusion": "TADrop provides a more effective way to mitigate parameter interference by tailoring sparsification to the model's structure, offering a new baseline for high-performance model merging."}}
{"id": "2508.05989", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05989", "abs": "https://arxiv.org/abs/2508.05989", "authors": ["Younjoon Chung", "Hyoungseob Park", "Patrick Rim", "Xiaoran Zhang", "Jihe He", "Ziyao Zeng", "Safa Cicek", "Byung-Woo Hong", "James S. Duncan", "Alex Wong"], "title": "ETA: Energy-based Test-time Adaptation for Depth Completion", "comment": null, "summary": "We propose a method for test-time adaptation of pretrained depth completion\nmodels. Depth completion models, trained on some ``source'' data, often predict\nerroneous outputs when transferred to ``target'' data captured in novel\nenvironmental conditions due to a covariate shift. The crux of our method lies\nin quantifying the likelihood of depth predictions belonging to the source data\ndistribution. The challenge is in the lack of access to out-of-distribution\n(target) data prior to deployment. Hence, rather than making assumptions\nregarding the target distribution, we utilize adversarial perturbations as a\nmechanism to explore the data space. This enables us to train an energy model\nthat scores local regions of depth predictions as in- or out-of-distribution.\nWe update the parameters of pretrained depth completion models at test time to\nminimize energy, effectively aligning test-time predictions to those of the\nsource distribution. We call our method ``Energy-based Test-time Adaptation'',\nor ETA for short. We evaluate our method across three indoor and three outdoor\ndatasets, where ETA improve over the previous state-of-the-art method by an\naverage of 6.94% for outdoors and 10.23% for indoors. Project Page:\nhttps://fuzzythecat.github.io/eta.", "AI": {"tldr": "ETA adapts pretrained depth completion models to new data by using an energy model trained with adversarial perturbations, improving accuracy.", "motivation": "Depth completion models often perform poorly when transferred to target data due to covariate shift, and access to target data is limited prior to deployment.", "method": "The method quantifies the likelihood of depth predictions belonging to the source data distribution using adversarial perturbations to train an energy model.", "result": "ETA improves depth completion accuracy by 6.94% outdoors and 10.23% indoors compared to previous state-of-the-art methods.", "conclusion": "Energy-based Test-time Adaptation (ETA) improves depth completion accuracy on target data by minimizing energy and aligning predictions to the source distribution, outperforming previous methods by 6.94% outdoors and 10.23% indoors."}}
{"id": "2508.06296", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06296", "abs": "https://arxiv.org/abs/2508.06296", "authors": ["Pierre Peign\u00e9 - Lefebvre", "Quentin Feuillade-Montixi", "Tom David", "Nicolas Miailhe"], "title": "LLM Robustness Leaderboard v1 --Technical report", "comment": null, "summary": "This technical report accompanies the LLM robustness leaderboard published by\nPRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior\nElicitation Tool (BET), an AI system performing automated red-teaming through\nDynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)\nagainst 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we\npropose a fine-grained robustness metric estimating the average number of\nattempts required to elicit harmful behaviors, revealing that attack difficulty\nvaries by over 300-fold across models despite universal vulnerability. We\nintroduce primitive-level vulnerability analysis to identify which jailbreaking\ntechniques are most effective for specific hazard categories. Our collaborative\nevaluation with trusted third parties from the AI Safety Network demonstrates\npractical pathways for distributed robustness assessment across the community.", "AI": {"tldr": "PRISM Eval introduces a tool (BET) that achieves 100% ASR against most LLMs, identifies vulnerabilities, and proposes distributed robustness assessment.", "motivation": "The motivation is to evaluate LLM robustness and identify vulnerabilities against adversarial attacks.", "method": "The paper uses PRISM Eval Behavior Elicitation Tool (BET) with Dynamic Adversarial Optimization to perform automated red-teaming.", "result": "The tool achieves 100% Attack Success Rate (ASR) against 37 of 41 state-of-the-art LLMs, reveals a 300-fold variance in attack difficulty, and identifies effective jailbreaking techniques.", "conclusion": "This paper introduces methods for evaluating and improving the robustness of LLMs against adversarial attacks, finding vulnerabilities and proposing solutions for distributed robustness assessment."}}
{"id": "2508.06097", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06097", "abs": "https://arxiv.org/abs/2508.06097", "authors": ["Simon B\u00fchrer", "Andreas Plesner", "Till Aczel", "Roger Wattenhofer"], "title": "Recurrent Deep Differentiable Logic Gate Networks", "comment": null, "summary": "While differentiable logic gates have shown promise in feedforward networks,\ntheir application to sequential modeling remains unexplored. This paper\npresents the first implementation of Recurrent Deep Differentiable Logic Gate\nNetworks (RDDLGN), combining Boolean operations with recurrent architectures\nfor sequence-to-sequence learning.\n  Evaluated on WMT'14 English-German translation, RDDLGN achieves 5.00 BLEU and\n30.9\\% accuracy during training, approaching GRU performance (5.41 BLEU) and\ngraceful degradation (4.39 BLEU) during inference. This work establishes\nrecurrent logic-based neural computation as viable, opening research directions\nfor FPGA acceleration in sequential modeling and other recursive network\narchitectures.", "AI": {"tldr": "This paper presents the first implementation of Recurrent Deep Differentiable Logic Gate Networks (RDDLGN), combining Boolean operations with recurrent architectures for sequence-to-sequence learning.", "motivation": "While differentiable logic gates have shown promise in feedforward networks, their application to sequential modeling remains unexplored.", "method": "The first implementation of Recurrent Deep Differentiable Logic Gate Networks (RDDLGN), combining Boolean operations with recurrent architectures for sequence-to-sequence learning.", "result": "Evaluated on WMT'14 English-German translation, RDDLGN achieves 5.00 BLEU and 30.9% accuracy during training, approaching GRU performance (5.41 BLEU) and graceful degradation (4.39 BLEU) during inference.", "conclusion": "Recurrent logic-based neural computation is viable, opening research directions for FPGA acceleration in sequential modeling and other recursive network architectures."}}
{"id": "2508.05709", "categories": ["cs.IR", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.05709", "abs": "https://arxiv.org/abs/2508.05709", "authors": ["Boyu Chen", "Siran Chen", "Zhengrong Yue", "Kainan Yan", "Chenyun Yu", "Beibei Kong", "Cheng Lei", "Chengxiang Zhuo", "Zang Li", "Yali Wang"], "title": "G-UBS: Towards Robust Understanding of Implicit Feedback via Group-Aware User Behavior Simulation", "comment": null, "summary": "User feedback is critical for refining recommendation systems, yet explicit\nfeedback (e.g., likes or dislikes) remains scarce in practice. As a more\nfeasible alternative, inferring user preferences from massive implicit feedback\nhas shown great potential (e.g., a user quickly skipping a recommended video\nusually indicates disinterest). Unfortunately, implicit feedback is often\nnoisy: a user might skip a video due to accidental clicks or other reasons,\nrather than disliking it. Such noise can easily misjudge user interests,\nthereby undermining recommendation performance. To address this issue, we\npropose a novel Group-aware User Behavior Simulation (G-UBS) paradigm, which\nleverages contextual guidance from relevant user groups, enabling robust and\nin-depth interpretation of implicit feedback for individual users.\nSpecifically, G-UBS operates via two key agents. First, the User Group Manager\n(UGM) effectively clusters users to generate group profiles utilizing a\n``summarize-cluster-reflect\" workflow based on LLMs. Second, the User Feedback\nModeler (UFM) employs an innovative group-aware reinforcement learning\napproach, where each user is guided by the associated group profiles during the\nreinforcement learning process, allowing UFM to robustly and deeply examine the\nreasons behind implicit feedback. To assess our G-UBS paradigm, we have\nconstructed a Video Recommendation benchmark with Implicit Feedback (IF-VR). To\nthe best of our knowledge, this is the first multi-modal benchmark for implicit\nfeedback evaluation in video recommendation, encompassing 15k users, 25k\nvideos, and 933k interaction records with implicit feedback. Extensive\nexperiments on IF-VR demonstrate that G-UBS significantly outperforms\nmainstream LLMs and MLLMs, with a 4.0% higher proportion of videos achieving a\nplay rate > 30% and 14.9% higher reasoning accuracy on IF-VR.", "AI": {"tldr": "G-UBS uses group-aware user behavior simulation to interpret noisy implicit feedback, improving video recommendation performance.", "motivation": "Explicit feedback is scarce, and implicit feedback is often noisy, which can easily misjudge user interests and undermine recommendation performance.", "method": "A novel Group-aware User Behavior Simulation (G-UBS) paradigm, which leverages contextual guidance from relevant user groups, enabling robust and in-depth interpretation of implicit feedback for individual users. It operates via two key agents: User Group Manager (UGM) and User Feedback Modeler (UFM).", "result": "G-UBS achieves a 4.0% higher proportion of videos achieving a play rate > 30% and 14.9% higher reasoning accuracy on IF-VR.", "conclusion": "G-UBS significantly outperforms mainstream LLMs and MLLMs on IF-VR, with a 4.0% higher proportion of videos achieving a play rate > 30% and 14.9% higher reasoning accuracy."}}
{"id": "2508.06165", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06165", "abs": "https://arxiv.org/abs/2508.06165", "authors": ["Weitao Li", "Boran Xiang", "Xiaolong Wang", "Zhinan Gou", "Weizhi Ma", "Yang Liu"], "title": "UR$^2$: Unify RAG and Reasoning through Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable capabilities through two\ncomplementary paradigms: Retrieval-Augmented Generation (RAG), which enhances\nknowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),\nwhich optimizes complex reasoning abilities. However, these two capabilities\nare often developed in isolation, and existing efforts to unify them remain\nnarrow in scope-typically limited to open-domain QA with fixed retrieval\nsettings and task-specific assumptions. This lack of integration constrains\ngeneralization and limits the applicability of RAG-RL methods to broader\ndomains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a\ngeneral framework that unifies retrieval and reasoning through reinforcement\nlearning. UR2 introduces two key contributions: a difficulty-aware curriculum\ntraining that selectively invokes retrieval only for challenging problems, and\na hybrid knowledge access strategy combining domain-specific offline corpora\nwith LLM-generated summaries. These components are designed to enable dynamic\ncoordination between retrieval and reasoning, improving adaptability across a\ndiverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,\nand mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B\nand LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods,\nachieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several\nbenchmarks. We have released all code, models, and data at\nhttps://github.com/Tsinghua-dhy/UR2.", "AI": {"tldr": "UR2 unifies retrieval and reasoning through reinforcement learning to improve adaptability across a diverse range of tasks.", "motivation": "existing efforts to unify RAG and RL remain narrow in scope-typically limited to open-domain QA with fixed retrieval settings and task-specific assumptions. This lack of integration constrains generalization and limits the applicability of RAG-RL methods to broader domains.", "method": "a difficulty-aware curriculum training that selectively invokes retrieval only for challenging problems, and a hybrid knowledge access strategy combining domain-specific offline corpora with LLM-generated summaries", "result": "UR2 (built on Qwen2.5-3/7B and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods, achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several benchmarks.", "conclusion": "UR2 significantly outperforms existing RAG and RL methods, achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several benchmarks."}}
{"id": "2508.05990", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05990", "abs": "https://arxiv.org/abs/2508.05990", "authors": ["Haichao Wang", "Xinyue Xi", "Jiangtao Wen", "Yuxing Han"], "title": "Fast Motion Estimation and Context-Aware Refinement for Efficient Bayer-Domain Video Vision", "comment": null, "summary": "The efficiency of video computer vision system remains a challenging task due\nto the high temporal redundancy inside a video. Existing works have been\nproposed for efficient vision computer vision. However, they do not fully\nreduce the temporal redundancy and neglect the front end computation overhead.\nIn this paper, we propose an efficient video computer vision system. First,\nimage signal processor is removed and Bayer-format data is directly fed into\nvideo computer vision models, thus saving the front end computation. Second,\ninstead of optical flow models and video codecs, a fast block matching-based\nmotion estimation algorithm is proposed specifically for efficient video\ncomputer vision, with a MV refinement module. To correct the error,\ncontext-aware block refinement network is introduced to refine regions with\nlarge error. To further balance the accuracy and efficiency, a frame selection\nstrategy is employed. Experiments on multiple video computer vision tasks\ndemonstrate that our method achieves significant acceleration with slight\nperformance loss.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89c6\u9891\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\uff0c\u901a\u8fc7\u53bb\u9664\u56fe\u50cf\u4fe1\u53f7\u5904\u7406\u5668\u3001\u4f7f\u7528\u5feb\u901f\u5757\u5339\u914d\u8fd0\u52a8\u4f30\u8ba1\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5757\u7ec6\u5316\u7f51\u7edc\u6765\u51cf\u5c11\u65f6\u95f4\u5197\u4f59\u548c\u524d\u7aef\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u7531\u4e8e\u89c6\u9891\u5185\u90e8\u7684\u65f6\u95f4\u5197\u4f59\u5ea6\u9ad8\uff0c\u89c6\u9891\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u7684\u6548\u7387\u4ecd\u7136\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u73b0\u6709\u7684\u5de5\u4f5c\u5df2\u7ecf\u88ab\u63d0\u51fa\u7528\u4e8e\u9ad8\u6548\u7684\u89c6\u89c9\u8ba1\u7b97\u673a\u89c6\u89c9\uff0c\u4f46\u662f\uff0c\u5b83\u4eec\u6ca1\u6709\u5b8c\u5168\u51cf\u5c11\u65f6\u95f4\u5197\u4f59\u5ea6\uff0c\u5ffd\u7565\u4e86\u524d\u7aef\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5feb\u901f\u5757\u5339\u914d\u7684\u8fd0\u52a8\u4f30\u8ba1\u7b97\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5757\u7ec6\u5316\u7f51\u7edc\u6765\u7ec6\u5316\u8bef\u5dee\u8f83\u5927\u7684\u533a\u57df\u3002\u91c7\u7528\u5e27\u9009\u62e9\u7b56\u7565\u6765\u8fdb\u4e00\u6b65\u5e73\u8861\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u89c6\u9891\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u663e\u7740\u7684\u52a0\u901f\uff0c\u6027\u80fd\u635f\u5931\u5f88\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u89c6\u9891\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u663e\u7740\u7684\u52a0\u901f\uff0c\u6027\u80fd\u635f\u5931\u5f88\u5c0f\u3002"}}
{"id": "2508.06326", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06326", "abs": "https://arxiv.org/abs/2508.06326", "authors": ["Nathaniel Virgo", "Martin Biehl", "Manuel Baltieri", "Matteo Capucci"], "title": "A \"good regulator theorem\" for embodied agents", "comment": "Accepted at the Artificial Life conference 2025 (ALife 2025). 10\n  pages, 1 figure", "summary": "In a classic paper, Conant and Ashby claimed that \"every good regulator of a\nsystem must be a model of that system.\" Artificial Life has produced many\nexamples of systems that perform tasks with apparently no model in sight; these\nsuggest Conant and Ashby's theorem doesn't easily generalise beyond its\nrestricted setup. Nevertheless, here we show that a similar intuition can be\nfleshed out in a different way: whenever an agent is able to perform a\nregulation task, it is possible for an observer to interpret it as having\n\"beliefs\" about its environment, which it \"updates\" in response to sensory\ninput. This notion of belief updating provides a notion of model that is more\nsophisticated than Conant and Ashby's, as well as a theorem that is more\nbroadly applicable. However, it necessitates a change in perspective, in that\nthe observer plays an essential role in the theory: models are not a mere\nproperty of the system but are imposed on it from outside. Our theorem holds\nregardless of whether the system is regulating its environment in a classic\ncontrol theory setup, or whether it's regulating its own internal state; the\nmodel is of its environment either way. The model might be trivial, however,\nand this is how the apparent counterexamples are resolved.", "AI": {"tldr": "whenever an agent is able to perform a regulation task, it is possible for an observer to interpret it as having \"beliefs\" about its environment, which it \"updates\" in response to sensory input", "motivation": "Artificial Life has produced many examples of systems that perform tasks with apparently no model in sight; these suggest Conant and Ashby's theorem doesn't easily generalise beyond its restricted setup", "method": "a similar intuition can be fleshed out in a different way: whenever an agent is able to perform a regulation task, it is possible for an observer to interpret it as having \"beliefs\" about its environment, which it \"updates\" in response to sensory input. This notion of belief updating provides a notion of model that is more sophisticated than Conant and Ashby's, as well as a theorem that is more broadly applicable", "result": "The model might be trivial, however, and this is how the apparent counterexamples are resolved.", "conclusion": "whenever an agent is able to perform a regulation task, it is possible for an observer to interpret it as having \"beliefs\" about its environment, which it \"updates\" in response to sensory input"}}
{"id": "2508.06108", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06108", "abs": "https://arxiv.org/abs/2508.06108", "authors": ["Xing Lei", "Wenyan Yang", "Kaiqiang Ke", "Shentao Yang", "Xuetao Zhang", "Joni Pajarinen", "Donglin Wang"], "title": "GCHR : Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning", "comment": null, "summary": "Goal-conditioned reinforcement learning (GCRL) with sparse rewards remains a\nfundamental challenge in reinforcement learning. While hindsight experience\nreplay (HER) has shown promise by relabeling collected trajectories with\nachieved goals, we argue that trajectory relabeling alone does not fully\nexploit the available experiences in off-policy GCRL methods, resulting in\nlimited sample efficiency. In this paper, we propose Hindsight Goal-conditioned\nRegularization (HGR), a technique that generates action regularization priors\nbased on hindsight goals. When combined with hindsight self-imitation\nregularization (HSR), our approach enables off-policy RL algorithms to maximize\nexperience utilization. Compared to existing GCRL methods that employ HER and\nself-imitation techniques, our hindsight regularizations achieve substantially\nmore efficient sample reuse and the best performances, which we empirically\ndemonstrate on a suite of navigation and manipulation tasks.", "AI": {"tldr": "propose Hindsight Goal-conditioned Regularization (HGR) to maximize experience utilization and achieve better performances", "motivation": "Goal-conditioned reinforcement learning (GCRL) with sparse rewards remains a fundamental challenge in reinforcement learning. While hindsight experience replay (HER) has shown promise by relabeling collected trajectories with achieved goals, we argue that trajectory relabeling alone does not fully exploit the available experiences in off-policy GCRL methods, resulting in limited sample efficiency.", "method": "Hindsight Goal-conditioned Regularization (HGR), a technique that generates action regularization priors based on hindsight goals. When combined with hindsight self-imitation regularization (HSR)", "result": "achieve substantially more efficient sample reuse and the best performances, which we empirically demonstrate on a suite of navigation and manipulation tasks.", "conclusion": "hindsight regularizations achieve substantially more efficient sample reuse and the best performances"}}
{"id": "2508.05748", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.05748", "abs": "https://arxiv.org/abs/2508.05748", "authors": ["Xinyu Geng", "Peng Xia", "Zhen Zhang", "Xinyu Wang", "Qiuchen Wang", "Ruixue Ding", "Chenxi Wang", "Jialong Wu", "Yida Zhao", "Kuan Li", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "title": "WebWatcher: Breaking New Frontiers of Vision-Language Deep Research Agent", "comment": null, "summary": "Web agents such as Deep Research have demonstrated superhuman cognitive\nabilities, capable of solving highly challenging information-seeking problems.\nHowever, most research remains primarily text-centric, overlooking visual\ninformation in the real world. This makes multimodal Deep Research highly\nchallenging, as such agents require much stronger reasoning abilities in\nperception, logic, knowledge, and the use of more sophisticated tools compared\nto text-based agents. To address this limitation, we introduce WebWatcher, a\nmulti-modal Agent for Deep Research equipped with enhanced visual-language\nreasoning capabilities. It leverages high-quality synthetic multimodal\ntrajectories for efficient cold start training, utilizes various tools for deep\nreasoning, and further enhances generalization through reinforcement learning.\nTo better evaluate the capabilities of multimodal agents, we propose\nBrowseComp-VL, a benchmark with BrowseComp-style that requires complex\ninformation retrieval involving both visual and textual information.\nExperimental results show that WebWatcher significantly outperforms proprietary\nbaseline, RAG workflow and open-source agents in four challenging VQA\nbenchmarks, which paves the way for solving complex multimodal\ninformation-seeking tasks.", "AI": {"tldr": "WebWatcher\u662f\u4e00\u79cd\u591a\u6a21\u6001Agent\uff0c\u5b83\u4f18\u4e8e\u5176\u4ed6VQA\u6a21\u578b\uff0c\u5e76\u4e14\u80fd\u591f\u89e3\u51b3\u590d\u6742\u7684\u591a\u6a21\u6001\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u4ee5\u6587\u672c\u4e3a\u4e2d\u5fc3\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u89c6\u89c9\u4fe1\u606f\u3002\u8fd9\u4f7f\u5f97\u591a\u6a21\u6001\u6df1\u5ea6\u7814\u7a76\u975e\u5e38\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u4e0e\u57fa\u4e8e\u6587\u672c\u7684agent\u76f8\u6bd4\uff0c\u6b64\u7c7bagent\u9700\u8981\u5728\u611f\u77e5\u3001\u903b\u8f91\u3001\u77e5\u8bc6\u548c\u66f4\u590d\u6742\u5de5\u5177\u7684\u4f7f\u7528\u65b9\u9762\u5177\u6709\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "WebWatcher\uff0c\u4e00\u79cd\u591a\u6a21\u6001\u6df1\u5ea6\u7814\u7a76Agent\uff0c\u914d\u5907\u4e86\u589e\u5f3a\u7684\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u3002\u5b83\u5229\u7528\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u591a\u6a21\u6001\u8f68\u8ff9\u8fdb\u884c\u6709\u6548\u7684\u51b7\u542f\u52a8\u8bad\u7ec3\uff0c\u5229\u7528\u5404\u79cd\u5de5\u5177\u8fdb\u884c\u6df1\u5ea6\u63a8\u7406\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "result": "WebWatcher\u5728\u56db\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684VQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u4e13\u6709\u57fa\u7ebf\u3001RAG\u5de5\u4f5c\u6d41\u7a0b\u548c\u5f00\u6e90agent\u3002", "conclusion": "WebWatcher\u663e\u8457\u4f18\u4e8eVQA\u57fa\u51c6\u6a21\u578b\uff0c\u4e3a\u89e3\u51b3\u590d\u6742\u591a\u6a21\u6001\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.06167", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06167", "abs": "https://arxiv.org/abs/2508.06167", "authors": ["V\u00edt Gvo\u017ediak"], "title": "Pragmatics beyond humans: meaning, communication, and LLMs", "comment": null, "summary": "The paper reconceptualizes pragmatics not as a subordinate, third dimension\nof meaning, but as a dynamic interface through which language operates as a\nsocially embedded tool for action. With the emergence of large language models\n(LLMs) in communicative contexts, this understanding needs to be further\nrefined and methodologically reconsidered. The first section challenges the\ntraditional semiotic trichotomy, arguing that connectionist LLM architectures\ndestabilize established hierarchies of meaning, and proposes the Human-Machine\nCommunication (HMC) framework as a more suitable alternative. The second\nsection examines the tension between human-centred pragmatic theories and the\nmachine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics\ncontinue to dominate, it relies on human-specific assumptions ill-suited to\npredictive systems like LLMs. Probabilistic pragmatics, particularly the\nRational Speech Act framework, offers a more compatible teleology by focusing\non optimization rather than truth-evaluation. The third section addresses the\nissue of substitutionalism in three forms - generalizing, linguistic, and\ncommunicative - highlighting the anthropomorphic biases that distort LLM\nevaluation and obscure the role of human communicative subjects. Finally, the\npaper introduces the concept of context frustration to describe the paradox of\nincreased contextual input paired with a collapse in contextual understanding,\nemphasizing how users are compelled to co-construct pragmatic conditions both\nfor the model and themselves. These arguments suggest that pragmatic theory may\nneed to be adjusted or expanded to better account for communication involving\ngenerative AI.", "AI": {"tldr": "This paper argues that pragmatic theory needs to adapt to account for communication involving generative AI, focusing on the limitations of traditional approaches and introducing the concept of context frustration.", "motivation": "The emergence of large language models (LLMs) in communicative contexts necessitates a refinement and methodological reconsideration of pragmatics.", "method": "Challenges the traditional semiotic trichotomy, examines the tension between human-centred pragmatic theories and the machine-centred nature of LLMs, and addresses the issue of substitutionalism.", "result": "The paper introduces the concept of context frustration to describe the paradox of increased contextual input paired with a collapse in contextual understanding.", "conclusion": "Pragmatic theory may need to be adjusted or expanded to better account for communication involving generative AI."}}
{"id": "2508.05991", "categories": ["cs.CV", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.05991", "abs": "https://arxiv.org/abs/2508.05991", "authors": ["Juewen Hu", "Yexin Li", "Jiulin Li", "Shuo Chen", "Pring Wong"], "title": "ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge", "comment": null, "summary": "Emotion recognition plays a vital role in enhancing human-computer\ninteraction. In this study, we tackle the MER-SEMI challenge of the MER2025\ncompetition by proposing a novel multimodal emotion recognition framework. To\naddress the issue of data scarcity, we leverage large-scale pre-trained models\nto extract informative features from visual, audio, and textual modalities.\nSpecifically, for the visual modality, we design a dual-branch visual encoder\nthat captures both global frame-level features and localized facial\nrepresentations. For the textual modality, we introduce a context-enriched\nmethod that employs large language models to enrich emotional cues within the\ninput text. To effectively integrate these multimodal features, we propose a\nfusion strategy comprising two key components, i.e., self-attention mechanisms\nfor dynamic modality weighting, and residual connections to preserve original\nrepresentations. Beyond architectural design, we further refine noisy labels in\nthe training set by a multi-source labeling strategy. Our approach achieves a\nsubstantial performance improvement over the official baseline on the\nMER2025-SEMI dataset, attaining a weighted F-score of 87.49% compared to\n78.63%, thereby validating the effectiveness of the proposed framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u6846\u67b6\uff0c\u5728MER2025-SEMI\u6311\u6218\u8d5b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u3001\u8bbe\u8ba1\u7279\u5b9a\u6a21\u6001\u7684\u7f16\u7801\u5668\u548c\u878d\u5408\u7b56\u7565\uff0c\u4ee5\u53ca\u4f18\u5316\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u60c5\u611f\u8bc6\u522b\u7684\u6027\u80fd\u3002", "motivation": "\u60c5\u611f\u8bc6\u522b\u5728\u589e\u5f3a\u4eba\u673a\u4ea4\u4e92\u65b9\u9762\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u6211\u4eec\u5229\u7528\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u4ece\u89c6\u89c9\u3001\u97f3\u9891\u548c\u6587\u672c\u6a21\u6001\u4e2d\u63d0\u53d6\u4fe1\u606f\u4e30\u5bcc\u7684\u7279\u5f81\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53cc\u5206\u652f\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u6355\u6349\u5168\u5c40\u5e27\u7ea7\u522b\u7279\u5f81\u548c\u5c40\u90e8\u9762\u90e8\u8868\u793a\u3002\u5f15\u5165\u4e86\u4e00\u79cd\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u6587\u672c\u5904\u7406\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u4e30\u5bcc\u8f93\u5165\u6587\u672c\u4e2d\u7684\u60c5\u611f\u7ebf\u7d22\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u7b56\u7565\uff0c\u5305\u542b\u7528\u4e8e\u52a8\u6001\u6a21\u6001\u52a0\u6743\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u7528\u4e8e\u4fdd\u7559\u539f\u59cb\u8868\u793a\u7684\u6b8b\u5dee\u8fde\u63a5\u3002\u91c7\u7528\u591a\u6e90\u6807\u7b7e\u7b56\u7565\u6765\u7ec6\u5316\u8bad\u7ec3\u96c6\u4e2d\u7684\u566a\u58f0\u6807\u7b7e\u3002", "result": "\u8be5\u65b9\u6cd5\u5728MER2025-SEMI\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u52a0\u6743F-score\u4e3a87.49%\uff0c\u800c\u5b98\u65b9\u57fa\u7ebf\u4e3a78.63%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728MER2025-SEMI\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u52a0\u6743F-score\u4e3a87.49%\uff0c\u800c\u5b98\u65b9\u57fa\u7ebf\u4e3a78.63%\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.06348", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06348", "abs": "https://arxiv.org/abs/2508.06348", "authors": ["Mille Mei Zhen Loo", "Gert Luzkov", "Paolo Burelli"], "title": "AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games", "comment": null, "summary": "Cheating in online video games compromises the integrity of gaming\nexperiences. Anti-cheat systems, such as VAC (Valve Anti-Cheat), face\nsignificant challenges in keeping pace with evolving cheating methods without\nimposing invasive measures on users' systems. This paper presents\nAntiCheatPT\\_256, a transformer-based machine learning model designed to detect\ncheating behaviour in Counter-Strike 2 using gameplay data. To support this, we\nintroduce and publicly release CS2CD: A labelled dataset of 795 matches. Using\nthis dataset, 90,707 context windows were created and subsequently augmented to\naddress class imbalance. The transformer model, trained on these windows,\nachieved an accuracy of 89.17\\% and an AUC of 93.36\\% on an unaugmented test\nset. This approach emphasizes reproducibility and real-world applicability,\noffering a robust baseline for future research in data-driven cheat detection.", "AI": {"tldr": "Developed AntiCheatPT_256, a transformer-based model for detecting cheating in Counter-Strike 2, achieving 89.17% accuracy and 93.36% AUC using a new dataset, CS2CD.", "motivation": "Cheating in online video games compromises the integrity of gaming experiences. Anti-cheat systems face significant challenges in keeping pace with evolving cheating methods without imposing invasive measures on users' systems.", "method": "a transformer-based machine learning model", "result": "achieved an accuracy of 89.17% and an AUC of 93.36% on an unaugmented test set", "conclusion": "The transformer model achieved an accuracy of 89.17% and an AUC of 93.36% on an unaugmented test set, offering a robust baseline for future research in data-driven cheat detection."}}
{"id": "2508.06151", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06151", "abs": "https://arxiv.org/abs/2508.06151", "authors": ["Yong Oh Lee", "JeeEun Kim", "Jung Woo Lee"], "title": "Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models", "comment": null, "summary": "In oral cancer diagnostics, the limited availability of annotated datasets\nfrequently constrains the performance of diagnostic models, particularly due to\nthe variability and insufficiency of training data. To address these\nchallenges, this study proposed a novel approach to enhance diagnostic accuracy\nby synthesizing realistic oral cancer lesions using an inpainting technique\nwith a fine-tuned diffusion model. We compiled a comprehensive dataset from\nmultiple sources, featuring a variety of oral cancer images. Our method\ngenerated synthetic lesions that exhibit a high degree of visual fidelity to\nactual lesions, thereby significantly enhancing the performance of diagnostic\nalgorithms. The results show that our classification model achieved a\ndiagnostic accuracy of 0.97 in differentiating between cancerous and\nnon-cancerous tissues, while our detection model accurately identified lesion\nlocations with 0.85 accuracy. This method validates the potential for synthetic\nimage generation in medical diagnostics and paves the way for further research\ninto extending these methods to other types of cancer diagnostics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u5177\u6709\u5fae\u8c03\u6269\u6563\u6a21\u578b\u7684\u4fee\u590d\u6280\u672f\u5408\u6210\u903c\u771f\u7684\u53e3\u8154\u764c\u75c5\u53d8\u6765\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u53e3\u8154\u764c\u8bca\u65ad\u4e2d\uff0c\u5e26\u6ce8\u91ca\u7684\u6570\u636e\u96c6\u7684\u6709\u9650\u53ef\u7528\u6027\u7ecf\u5e38\u9650\u5236\u8bca\u65ad\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u56e0\u4e3a\u8bad\u7ec3\u6570\u636e\u7684\u53ef\u53d8\u6027\u548c\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u5177\u6709\u5fae\u8c03\u6269\u6563\u6a21\u578b\u7684\u4fee\u590d\u6280\u672f\u5408\u6210\u903c\u771f\u7684\u53e3\u8154\u764c\u75c5\u53d8", "result": "\u6211\u4eec\u7684\u5206\u7c7b\u6a21\u578b\u5728\u533a\u5206\u764c\u6027\u548c\u975e\u764c\u6027\u7ec4\u7ec7\u65b9\u9762\u8fbe\u5230\u4e86 0.97 \u7684\u8bca\u65ad\u51c6\u786e\u7387\uff0c\u800c\u6211\u4eec\u7684\u68c0\u6d4b\u6a21\u578b\u4ee5 0.85 \u7684\u51c6\u786e\u7387\u51c6\u786e\u5730\u8bc6\u522b\u4e86\u75c5\u53d8\u4f4d\u7f6e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u5408\u6210\u56fe\u50cf\u751f\u6210\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u5c06\u8fd9\u4e9b\u65b9\u6cd5\u6269\u5c55\u5230\u5176\u4ed6\u7c7b\u578b\u7684\u764c\u75c7\u8bca\u65ad\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.05969", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.05969", "abs": "https://arxiv.org/abs/2508.05969", "authors": ["Li Fan", "Menglin Kong", "Yang Xiang", "Chong Zhang", "Chengtao Ji"], "title": "Dual prototype attentive graph network for cross-market recommendation", "comment": "Accepted by ICONIP 2025 (Oral)", "summary": "Cross-market recommender systems (CMRS) aim to utilize historical data from\nmature markets to promote multinational products in emerging markets. However,\nexisting CMRS approaches often overlook the potential for shared preferences\namong users in different markets, focusing primarily on modeling specific\npreferences within each market. In this paper, we argue that incorporating both\nmarket-specific and market-shared insights can enhance the generalizability and\nrobustness of CMRS. We propose a novel approach called Dual Prototype Attentive\nGraph Network for Cross-Market Recommendation (DGRE) to address this. DGRE\nleverages prototypes based on graph representation learning from both items and\nusers to capture market-specific and market-shared insights. Specifically, DGRE\nincorporates market-shared prototypes by clustering users from various markets\nto identify behavioural similarities and create market-shared user profiles.\nAdditionally, it constructs item-side prototypes by aggregating item features\nwithin each market, providing valuable market-specific insights. We conduct\nextensive experiments to validate the effectiveness of DGRE on a real-world\ncross-market dataset, and the results show that considering both\nmarket-specific and market-sharing aspects in modelling can improve the\ngeneralization and robustness of CMRS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8de8\u5e02\u573a\u63a8\u8350\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u56fe\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u539f\u578b\u6765\u6355\u83b7\u5e02\u573a\u7279\u5b9a\u548c\u5e02\u573a\u5171\u4eab\u7684\u89c1\u89e3\uff0c\u4ece\u800c\u63d0\u9ad8\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u8de8\u5e02\u573a\u63a8\u8350\u7cfb\u7edf (CMRS) \u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u4e86\u4e0d\u540c\u5e02\u573a\u7528\u6237\u4e4b\u95f4\u5171\u4eab\u504f\u597d\u7684\u6f5c\u529b\uff0c\u4e3b\u8981\u4fa7\u91cd\u4e8e\u5bf9\u6bcf\u4e2a\u5e02\u573a\u5185\u7684\u7279\u5b9a\u504f\u597d\u8fdb\u884c\u5efa\u6a21\u3002\u672c\u6587\u8ba4\u4e3a\uff0c\u7ed3\u5408\u5e02\u573a\u7279\u5b9a\u548c\u5e02\u573a\u5171\u4eab\u7684\u89c1\u89e3\u53ef\u4ee5\u63d0\u9ad8 CMRS \u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u53cc\u539f\u578b\u6ce8\u610f\u56fe\u7f51\u7edc\u7528\u4e8e\u8de8\u5e02\u573a\u63a8\u8350 (DGRE) \u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u8de8\u5e02\u573a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\u6765\u9a8c\u8bc1 DGRE \u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5efa\u6a21\u4e2d\u540c\u65f6\u8003\u8651\u5e02\u573a\u7279\u5b9a\u548c\u5e02\u573a\u5171\u4eab\u65b9\u9762\u53ef\u4ee5\u63d0\u9ad8 CMRS \u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8003\u8651\u5728\u5efa\u6a21\u4e2d\u540c\u65f6\u8003\u8651\u5e02\u573a\u7279\u5b9a\u548c\u5e02\u573a\u5171\u4eab\u65b9\u9762\u53ef\u4ee5\u63d0\u9ad8\u8de8\u5e02\u573a\u63a8\u8350\u7cfb\u7edf\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.06178", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06178", "abs": "https://arxiv.org/abs/2508.06178", "authors": ["Hugo Abonizio", "Thales Almeida", "Roberto Lotufo", "Rodrigo Nogueira"], "title": "Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime", "comment": null, "summary": "Large language models (LLMs) often require vast amounts of text to\neffectively acquire new knowledge. While continuing pre-training on large\ncorpora or employing retrieval-augmented generation (RAG) has proven\nsuccessful, updating an LLM with only a few thousand or million tokens remains\nchallenging. In this work, we investigate the task of injecting small,\nunstructured information into LLMs and its relation to the catastrophic\nforgetting phenomenon. We use a dataset of recent news -- ensuring no overlap\nwith the model's pre-training data -- to evaluate the knowledge acquisition by\nprobing the model with question-answer pairs related the learned information.\nStarting from a continued pre-training baseline, we explored different\naugmentation algorithms to generate synthetic data to improve the knowledge\nacquisition capabilities. Our experiments show that simply continuing\npre-training on limited data yields modest improvements, whereas exposing the\nmodel to diverse textual variations significantly improves the learning of new\nfacts -- particularly with methods that induce greater variability through\ndiverse prompting. Furthermore, we shed light on the forgetting phenomenon in\nsmall-data regimes, illustrating the delicate balance between learning new\ncontent and retaining existing capabilities. We also confirm the sensitivity of\nRAG-based approaches for knowledge injection, which often lead to greater\ndegradation on control datasets compared to parametric methods. Finally, we\ndemonstrate that models can generate effective synthetic training data\nthemselves, suggesting a pathway toward self-improving model updates. All code\nand generated data used in our experiments are publicly available, providing a\nresource for studying efficient knowledge injection in LLMs with limited data\nat https://github.com/hugoabonizio/knowledge-injection-methods.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5c11\u91cf\u6570\u636e\u4e0b\u5411 LLM \u4e2d\u6ce8\u5165\u975e\u7ed3\u6784\u5316\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u4e0e\u707e\u96be\u6027\u9057\u5fd8\u73b0\u8c61\u7684\u5173\u7cfb\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u6587\u672c\u624d\u80fd\u6709\u6548\u5730\u83b7\u53d6\u65b0\u77e5\u8bc6\u3002\u867d\u7136\u5728\u5927\u578b\u8bed\u6599\u5e93\u4e0a\u7ee7\u7eed\u8fdb\u884c\u9884\u8bad\u7ec3\u6216\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG) \u5df2\u88ab\u8bc1\u660e\u662f\u6210\u529f\u7684\uff0c\u4f46\u4ec5\u7528\u51e0\u5343\u6216\u51e0\u767e\u4e07\u4e2a token \u66f4\u65b0 LLM \u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u6211\u4eec\u4f7f\u7528\u4e00\u4e2a\u6700\u65b0\u7684\u65b0\u95fb\u6570\u636e\u96c6\uff08\u786e\u4fdd\u4e0e\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u6570\u636e\u6ca1\u6709\u91cd\u53e0\uff09\u6765\u8bc4\u4f30\u77e5\u8bc6\u83b7\u53d6\uff0c\u901a\u8fc7\u4f7f\u7528\u4e0e\u5b66\u4e60\u4fe1\u606f\u76f8\u5173\u7684\u95ee\u7b54\u5bf9\u6765\u63a2\u6d4b\u6a21\u578b\u3002\u4ece\u6301\u7eed\u7684\u9884\u8bad\u7ec3\u57fa\u7ebf\u5f00\u59cb\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u4e0d\u540c\u7684\u589e\u5f3a\u7b97\u6cd5\u6765\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u4ee5\u63d0\u9ad8\u77e5\u8bc6\u83b7\u53d6\u80fd\u529b\u3002", "result": "\u7b80\u5355\u5730\u5728\u6709\u9650\u7684\u6570\u636e\u4e0a\u7ee7\u7eed\u8fdb\u884c\u9884\u8bad\u7ec3\u53ea\u80fd\u4ea7\u751f\u9002\u5ea6\u7684\u6539\u8fdb\uff0c\u800c\u8ba9\u6a21\u578b\u63a5\u89e6\u5230\u591a\u6837\u5316\u7684\u6587\u672c\u53d8\u4f53\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u65b0\u4e8b\u5b9e\u7684\u5b66\u4e60\u6548\u679c\u2014\u2014\u7279\u522b\u662f\u901a\u8fc7\u591a\u6837\u5316\u7684\u63d0\u793a\u8bf1\u5bfc\u66f4\u5927\u53d8\u5f02\u6027\u7684\u65b9\u6cd5\u3002\u4e0e\u53c2\u6570\u65b9\u6cd5\u76f8\u6bd4\uff0c\u57fa\u4e8e RAG \u7684\u65b9\u6cd5\u901a\u5e38\u4f1a\u5bfc\u81f4\u63a7\u5236\u6570\u636e\u96c6\u4e0a\u66f4\u5927\u7684\u6027\u80fd\u4e0b\u964d\u3002\u6a21\u578b\u53ef\u4ee5\u81ea\u5df1\u751f\u6210\u6709\u6548\u7684\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u8fd9\u4e3a\u81ea\u6211\u6539\u8fdb\u7684\u6a21\u578b\u66f4\u65b0\u63d0\u4f9b\u4e86\u4e00\u6761\u9014\u5f84\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u9a8c\uff0c\u6211\u4eec\u53d1\u73b0\uff0c\u5728\u5c11\u91cf\u6570\u636e\u4e0b\uff0c\u6301\u7eed\u9884\u8bad\u7ec3\u6548\u679c\u6709\u9650\uff1b\u800c\u4f7f\u7528\u591a\u6837\u5316\u7684\u6587\u672c\u53d8\u4f53\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u65b0\u77e5\u8bc6\u7684\u5b66\u4e60\u6548\u679c\uff0c\u7279\u522b\u662f\u901a\u8fc7\u591a\u6837\u5316\u7684prompt\u8bf1\u5bfc\u66f4\u5927\u53d8\u5f02\u6027\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u8fd8\u9610\u660e\u4e86\u5c0f\u6570\u636e regime \u4e2d\u7684\u9057\u5fd8\u73b0\u8c61\uff0c\u8bf4\u660e\u4e86\u5b66\u4e60\u65b0\u5185\u5bb9\u548c\u4fdd\u7559\u73b0\u6709\u80fd\u529b\u4e4b\u95f4\u7684\u5fae\u5999\u5e73\u8861\u3002\u6211\u4eec\u8fd8\u8bc1\u5b9e\u4e86\u57fa\u4e8e RAG \u7684\u77e5\u8bc6\u6ce8\u5165\u65b9\u6cd5\u7684\u654f\u611f\u6027\uff0c\u4e0e\u53c2\u6570\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8fd9\u79cd\u65b9\u6cd5\u901a\u5e38\u4f1a\u5bfc\u81f4\u63a7\u5236\u6570\u636e\u96c6\u4e0a\u66f4\u5927\u7684\u6027\u80fd\u4e0b\u964d\u3002\u6700\u540e\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u6a21\u578b\u53ef\u4ee5\u81ea\u5df1\u751f\u6210\u6709\u6548\u7684\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u8fd9\u4e3a\u81ea\u6211\u6539\u8fdb\u7684\u6a21\u578b\u66f4\u65b0\u63d0\u4f9b\u4e86\u4e00\u6761\u9014\u5f84\u3002"}}
{"id": "2508.05994", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05994", "abs": "https://arxiv.org/abs/2508.05994", "authors": ["Huadong Wu", "Yi Fu", "Yunhao Li", "Yuan Gao", "Kang Du"], "title": "EvoMakeup: High-Fidelity and Controllable Makeup Editing with MakeupQuad", "comment": null, "summary": "Facial makeup editing aims to realistically transfer makeup from a reference\nto a target face. Existing methods often produce low-quality results with\ncoarse makeup details and struggle to preserve both identity and makeup\nfidelity, mainly due to the lack of structured paired data -- where source and\nresult share identity, and reference and result share identical makeup. To\naddress this, we introduce MakeupQuad, a large-scale, high-quality dataset with\nnon-makeup faces, references, edited results, and textual makeup descriptions.\nBuilding on this, we propose EvoMakeup, a unified training framework that\nmitigates image degradation during multi-stage distillation, enabling iterative\nimprovement of both data and model quality. Although trained solely on\nsynthetic data, EvoMakeup generalizes well and outperforms prior methods on\nreal-world benchmarks. It supports high-fidelity, controllable, multi-task\nmakeup editing -- including full-face and partial reference-based editing, as\nwell as text-driven makeup editing -- within a single model. Experimental\nresults demonstrate that our method achieves superior makeup fidelity and\nidentity preservation, effectively balancing both aspects. Code and dataset\nwill be released upon acceptance.", "AI": {"tldr": "EvoMakeup\u662f\u4e00\u4e2a\u7528\u4e8e\u9762\u90e8\u5316\u5986\u7f16\u8f91\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5b83\u4f7f\u7528MakeupQuad\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5728\u5316\u5986\u4fdd\u771f\u5ea6\u548c\u8eab\u4efd\u4fdd\u6301\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u9762\u90e8\u5316\u5986\u7f16\u8f91\u65b9\u6cd5\u901a\u5e38\u4f1a\u4ea7\u751f\u4f4e\u8d28\u91cf\u7684\u7ed3\u679c\uff0c\u5316\u5986\u7ec6\u8282\u7c97\u7cd9\uff0c\u5e76\u4e14\u96be\u4ee5\u540c\u65f6\u4fdd\u6301\u8eab\u4efd\u548c\u5316\u5986\u7684\u903c\u771f\u5ea6\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u7f3a\u4e4f\u7ed3\u6784\u5316\u7684\u914d\u5bf9\u6570\u636e\u3002", "method": "EvoMakeup\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u53ef\u4ee5\u51cf\u8f7b\u591a\u9636\u6bb5\u84b8\u998f\u8fc7\u7a0b\u4e2d\u7684\u56fe\u50cf\u9000\u5316\uff0c\u4ece\u800c\u80fd\u591f\u8fed\u4ee3\u6539\u8fdb\u6570\u636e\u548c\u6a21\u578b\u8d28\u91cf\u3002", "result": "EvoMakeup\u5728\u771f\u5b9e\u4e16\u754c\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5e76\u4e14\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u3002\u5b83\u652f\u6301\u9ad8\u4fdd\u771f\u3001\u53ef\u63a7\u3001\u591a\u4efb\u52a1\u7684\u5316\u5986\u7f16\u8f91\u3002", "conclusion": "EvoMakeup\u5728\u5316\u5986\u4fdd\u771f\u5ea6\u548c\u8eab\u4efd\u4fdd\u6301\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u6709\u6548\u5730\u5e73\u8861\u4e86\u8fd9\u4e24\u4e2a\u65b9\u9762\u3002"}}
{"id": "2508.06352", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06352", "abs": "https://arxiv.org/abs/2508.06352", "authors": ["Christian Meske", "Justin Brenne", "Erdi Uenal", "Sabahat Oelcer", "Ayseguel Doganguen"], "title": "From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI", "comment": null, "summary": "Current explainable AI (XAI) approaches prioritize algorithmic transparency\nand present explanations in abstract, non-adaptive formats that often fail to\nsupport meaningful end-user understanding. This paper introduces \"Explanatory\nAI\" as a complementary paradigm that leverages generative AI capabilities to\nserve as explanatory partners for human understanding rather than providers of\nalgorithmic transparency. While XAI reveals algorithmic decision processes for\nmodel validation, Explanatory AI addresses contextual reasoning to support\nhuman decision-making in sociotechnical contexts. We develop a definition and\nsystematic eight-dimensional conceptual model distinguishing Explanatory AI\nthrough narrative communication, adaptive personalization, and progressive\ndisclosure principles. Empirical validation through Rapid Contextual Design\nmethodology with healthcare professionals demonstrates that users consistently\nprefer context-sensitive, multimodal explanations over technical transparency.\nOur findings reveal the practical urgency for AI systems designed for human\ncomprehension rather than algorithmic introspection, establishing a\ncomprehensive research agenda for advancing user-centered AI explanation\napproaches across diverse domains and cultural contexts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u201c\u89e3\u91ca\u6027\u4eba\u5de5\u667a\u80fd\u201d\u4f5c\u4e3a\u4e00\u79cd\u4e92\u8865\u8303\u4f8b\uff0c\u5b83\u5229\u7528\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u80fd\u529b\u6765\u670d\u52a1\u4e8e\u4eba\u7c7b\u7406\u89e3\u7684\u89e3\u91ca\u6027\u4f19\u4f34\uff0c\u800c\u4e0d\u662f\u7b97\u6cd5\u900f\u660e\u5ea6\u7684\u63d0\u4f9b\u8005\u3002", "motivation": "\u5f53\u524d\u7684\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd (XAI) \u65b9\u6cd5\u4f18\u5148\u8003\u8651\u7b97\u6cd5\u900f\u660e\u5ea6\uff0c\u5e76\u4ee5\u62bd\u8c61\u7684\u3001\u975e\u81ea\u9002\u5e94\u7684\u683c\u5f0f\u5448\u73b0\u89e3\u91ca\uff0c\u8fd9\u4e9b\u683c\u5f0f\u901a\u5e38\u65e0\u6cd5\u652f\u6301\u6709\u610f\u4e49\u7684\u6700\u7ec8\u7528\u6237\u7406\u89e3\u3002", "method": "\u5feb\u901f\u60c5\u5883\u8bbe\u8ba1\u65b9\u6cd5", "result": "\u7528\u6237\u66f4\u559c\u6b22\u60c5\u5883\u654f\u611f\u7684\u3001\u591a\u6a21\u6001\u7684\u89e3\u91ca\uff0c\u800c\u4e0d\u662f\u6280\u672f\u900f\u660e\u6027\u3002", "conclusion": "\u7528\u6237\u66f4\u559c\u6b22\u60c5\u5883\u654f\u611f\u7684\u3001\u591a\u6a21\u6001\u7684\u89e3\u91ca\uff0c\u800c\u4e0d\u662f\u6280\u672f\u900f\u660e\u6027\u3002\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u4e3a\u4eba\u7c7b\u7406\u89e3\u800c\u8bbe\u8ba1\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u5b9e\u9645\u7d27\u8feb\u6027\uff0c\u5e76\u4e3a\u5728\u4e0d\u540c\u9886\u57df\u548c\u6587\u5316\u80cc\u666f\u4e0b\u63a8\u8fdb\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd\u89e3\u91ca\u65b9\u6cd5\u5efa\u7acb\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u7814\u7a76\u8bae\u7a0b\u3002"}}
{"id": "2508.06183", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06183", "abs": "https://arxiv.org/abs/2508.06183", "authors": ["Xiyuan Yang", "Shengyuan Hu", "Soyeon Kim", "Tian Li"], "title": "Differentially Private Federated Clustering with Random Rebalancing", "comment": "21 pages", "summary": "Federated clustering aims to group similar clients into clusters and produce\none model for each cluster. Such a personalization approach typically improves\nmodel performance compared with training a single model to serve all clients,\nbut can be more vulnerable to privacy leakage. Directly applying client-level\ndifferentially private (DP) mechanisms to federated clustering could degrade\nthe utilities significantly. We identify that such deficiencies are mainly due\nto the difficulties of averaging privacy noise within each cluster (following\nstandard privacy mechanisms), as the number of clients assigned to the same\nclusters is uncontrolled. To this end, we propose a simple and effective\ntechnique, named RR-Cluster, that can be viewed as a light-weight add-on to\nmany federated clustering algorithms. RR-Cluster achieves reduced privacy noise\nvia randomly rebalancing cluster assignments, guaranteeing a minimum number of\nclients assigned to each cluster. We analyze the tradeoffs between decreased\nprivacy noise variance and potentially increased bias from incorrect\nassignments and provide convergence bounds for RR-Clsuter. Empirically, we\ndemonstrate the RR-Cluster plugged into strong federated clustering algorithms\nresults in significantly improved privacy/utility tradeoffs across both\nsynthetic and real-world datasets.", "AI": {"tldr": "RR-Cluster reduces privacy noise in federated clustering by randomly rebalancing cluster assignments, guaranteeing a minimum number of clients assigned to each cluster, resulting in improved privacy/utility tradeoffs.", "motivation": "Federated clustering can be more vulnerable to privacy leakage. Directly applying client-level differentially private (DP) mechanisms to federated clustering could degrade the utilities significantly because the number of clients assigned to the same clusters is uncontrolled, making it difficult to average privacy noise within each cluster.", "method": "RR-Cluster: a light-weight add-on to many federated clustering algorithms. Achieves reduced privacy noise via randomly rebalancing cluster assignments, guaranteeing a minimum number of clients assigned to each cluster.", "result": "RR-Cluster achieves reduced privacy noise. Provides convergence bounds for RR-Clsuter. Significantly improved privacy/utility tradeoffs across both synthetic and real-world datasets.", "conclusion": "RR-Cluster plugged into strong federated clustering algorithms results in significantly improved privacy/utility tradeoffs across both synthetic and real-world datasets."}}
{"id": "2508.05993", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.05993", "abs": "https://arxiv.org/abs/2508.05993", "authors": ["Yunke Qu", "Liang Qu", "Tong Chen", "Quoc Viet Hung Nguyen", "Hongzhi Yin"], "title": "Efficient Multimodal Streaming Recommendation via Expandable Side Mixture-of-Experts", "comment": "Accepted to CIKM 2025", "summary": "Streaming recommender systems (SRSs) are widely deployed in real-world\napplications, where user interests shift and new items arrive over time. As a\nresult, effectively capturing users' latest preferences is challenging, as\ninteractions reflecting recent interests are limited and new items often lack\nsufficient feedback. A common solution is to enrich item representations using\nmultimodal encoders (e.g., BERT or ViT) to extract visual and textual features.\nHowever, these encoders are pretrained on general-purpose tasks: they are not\ntailored to user preference modeling, and they overlook the fact that user\ntastes toward modality-specific features such as visual styles and textual\ntones can also drift over time. This presents two key challenges in streaming\nscenarios: the high cost of fine-tuning large multimodal encoders, and the risk\nof forgetting long-term user preferences due to continuous model updates.\n  To tackle these challenges, we propose Expandable Side Mixture-of-Experts\n(XSMoE), a memory-efficient framework for multimodal streaming recommendation.\nXSMoE attaches lightweight side-tuning modules consisting of expandable expert\nnetworks to frozen pretrained encoders and incrementally expands them in\nresponse to evolving user feedback. A gating router dynamically combines expert\nand backbone outputs, while a utilization-based pruning strategy maintains\nmodel compactness. By learning new patterns through expandable experts without\noverwriting previously acquired knowledge, XSMoE effectively captures both cold\nstart and shifting preferences in multimodal features. Experiments on three\nreal-world datasets demonstrate that XSMoE outperforms state-of-the-art\nbaselines in both recommendation quality and computational efficiency.", "AI": {"tldr": "XSMoE \u662f\u4e00\u79cd\u7528\u4e8e\u591a\u6a21\u6001\u6d41\u63a8\u8350\u7684\u5185\u5b58\u9ad8\u6548\u6846\u67b6\uff0c\u5b83\u53ef\u4ee5\u6709\u6548\u5730\u6355\u83b7\u51b7\u542f\u52a8\u548c\u591a\u6a21\u6001\u7279\u5f81\u4e2d\u7684\u8f6c\u79fb\u504f\u597d\u3002", "motivation": "\u6709\u6548\u6355\u83b7\u7528\u6237\u6700\u65b0\u7684\u504f\u597d\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u53cd\u6620\u8fd1\u671f\u5174\u8da3\u7684\u4e92\u52a8\u53d7\u5230\u9650\u5236\uff0c\u5e76\u4e14\u65b0\u9879\u76ee\u901a\u5e38\u7f3a\u4e4f\u8db3\u591f\u7684\u53cd\u9988\u3002\u5fae\u8c03\u5927\u578b\u591a\u6a21\u6001\u7f16\u7801\u5668\u7684\u9ad8\u6210\u672c\uff0c\u4ee5\u53ca\u7531\u4e8e\u6301\u7eed\u7684\u6a21\u578b\u66f4\u65b0\u800c\u5fd8\u8bb0\u957f\u671f\u7528\u6237\u504f\u597d\u7684\u98ce\u9669\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u53ef\u6269\u5c55\u7684\u4fa7\u9762\u6df7\u5408\u4e13\u5bb6 (XSMoE)\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u591a\u6a21\u6001\u6d41\u63a8\u8350\u7684\u5185\u5b58\u9ad8\u6548\u6846\u67b6\u3002XSMoE \u5c06\u7531\u53ef\u6269\u5c55\u4e13\u5bb6\u7f51\u7edc\u7ec4\u6210\u7684\u8f7b\u91cf\u7ea7\u4fa7\u9762\u8c03\u6574\u6a21\u5757\u8fde\u63a5\u5230\u51bb\u7ed3\u7684\u9884\u8bad\u7ec3\u7f16\u7801\u5668\uff0c\u5e76\u6839\u636e\u4e0d\u65ad\u53d1\u5c55\u7684\u7528\u6237\u53cd\u9988\u9010\u6b65\u6269\u5c55\u5b83\u4eec\u3002\u95e8\u63a7\u8def\u7531\u5668\u52a8\u6001\u5730\u7ec4\u5408\u4e13\u5bb6\u548c\u9aa8\u5e72\u8f93\u51fa\uff0c\u800c\u57fa\u4e8e\u5229\u7528\u7387\u7684\u4fee\u526a\u7b56\u7565\u4fdd\u6301\u6a21\u578b\u7684\u7d27\u51d1\u6027\u3002\u901a\u8fc7\u901a\u8fc7\u53ef\u6269\u5c55\u7684\u4e13\u5bb6\u5b66\u4e60\u65b0\u6a21\u5f0f\u800c\u4e0d\u8986\u76d6\u5148\u524d\u83b7\u5f97\u7684\u77e5\u8bc6\uff0cXSMoE \u6709\u6548\u5730\u6355\u83b7\u4e86\u591a\u6a21\u6001\u7279\u5f81\u4e2d\u7684\u51b7\u542f\u52a8\u548c\u8f6c\u79fb\u504f\u597d\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cXSMoE \u5728\u63a8\u8350\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u3002", "conclusion": "XSMoE\u5728\u63a8\u8350\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u3002"}}
{"id": "2508.06186", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06186", "abs": "https://arxiv.org/abs/2508.06186", "authors": ["Ali Sarabadani", "Maryam Abdollahi Shamami", "Hamidreza Sadeghsalehi", "Borhan Asadi", "Saba Hesaraki"], "title": "DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration", "comment": null, "summary": "Large Language Models (LLMs) have grown exponentially since the release of\nChatGPT. These models have gained attention due to their robust performance on\nvarious tasks, including language processing tasks. These models achieve\nunderstanding and comprehension of tasks by training billions of parameters.\nThe development of these models is a transformative force in enhancing natural\nlanguage understanding and has taken a significant step towards artificial\ngeneral intelligence (AGI). In this study, we aim to present the DKG-LLM\nframework. The DKG-LLM framework introduces a groundbreaking approach to\nmedical diagnosis and personalized treatment recommendations by integrating a\ndynamic knowledge graph (DKG) with the Grok 3 large language model. Using the\nAdaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data\n(including clinical reports and PubMed articles) and patient records\ndynamically generate a knowledge graph consisting of 15,964 nodes in 13\ndistinct types (e.g., diseases, symptoms, treatments, patient profiles) and\n127,392 edges in 26 relationship types (e.g., causal, therapeutic,\nassociation). ASFA utilizes advanced probabilistic models, Bayesian inference,\nand graph optimization to extract semantic information, dynamically updating\nthe graph with approximately 150 new nodes and edges in each data category\nwhile maintaining scalability with up to 987,654 edges. Real-world datasets,\nincluding MIMIC-III and PubMed, were utilized to evaluate the proposed\narchitecture. The evaluation results show that DKG-LLM achieves a diagnostic\naccuracy of 84.19%. The model also has a treatment recommendation accuracy of\n89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and\ntransformative tool that handles noisy data and complex multi-symptom diseases,\nalong with feedback-based learning from physician input.", "AI": {"tldr": "DKG-LLM framework introduces a groundbreaking approach to medical diagnosis and personalized treatment recommendations by integrating a dynamic knowledge graph (DKG) with the Grok 3 large language model.", "motivation": "development of large language models is a transformative force in enhancing natural language understanding and has taken a significant step towards artificial general intelligence (AGI).", "method": "integrating a dynamic knowledge graph (DKG) with the Grok 3 large language model, Using the Adaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data (including clinical reports and PubMed articles) and patient records dynamically generate a knowledge graph", "result": "DKG-LLM achieves a diagnostic accuracy of 84.19%. The model also has a treatment recommendation accuracy of 89.63% and a semantic coverage of 93.48%.", "conclusion": "DKG-LLM is a reliable and transformative tool that handles noisy data and complex multi-symptom diseases, along with feedback-based learning from physician input."}}
{"id": "2508.06009", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06009", "abs": "https://arxiv.org/abs/2508.06009", "authors": ["Jun Feng", "Zixin Wang", "Zhentao Zhang", "Yue Guo", "Zhihan Zhou", "Xiuyi Chen", "Zhenyang Li", "Dawei Yin"], "title": "MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models", "comment": "29 pages, 16 figures", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in visual mathematical reasoning across various existing\nbenchmarks. However, these benchmarks are predominantly based on clean or\nprocessed multimodal inputs, without incorporating the images provided by\nreal-world Kindergarten through 12th grade (K-12) educational users. To address\nthis gap, we introduce MathReal, a meticulously curated dataset comprising\n2,000 mathematical questions with images captured by handheld mobile devices in\nauthentic scenarios. Each question is an image, containing the question text\nand visual element. We systematically classify the real images into three\nprimary categories: image quality degradation, perspective variation, and\nirrelevant content interference, which are further delineated into 14\nsubcategories. Additionally, MathReal spans five core knowledge and ability\ncategories, which encompass three question types and are divided into three\ndifficulty levels. To comprehensively evaluate the multimodal mathematical\nreasoning abilities of state-of-the-art MLLMs in real-world scenarios, we\ndesign six experimental settings that enable a systematic analysis of their\nperformance. Through extensive experimentation, we find that the\nproblem-solving abilities of existing MLLMs are significantly challenged in\nrealistic educational contexts. Based on this, we conduct a thorough analysis\nof their performance and error patterns, providing insights into their\nrecognition, comprehension, and reasoning capabilities, and outlining\ndirections for future improvements. Data and code:\nhttps://github.com/junfeng0288/MathReal.", "AI": {"tldr": "introduce MathReal dataset to evaluate MLLMs' mathematical reasoning abilities in real-world K-12 scenarios, finding challenges for existing models.", "motivation": "existing benchmarks are predominantly based on clean or processed multimodal inputs, without incorporating the images provided by real-world Kindergarten through 12th grade (K-12) educational users", "method": "introduce MathReal, a meticulously curated dataset comprising 2,000 mathematical questions with images captured by handheld mobile devices in authentic scenarios and design six experimental settings", "result": "systematic analysis of MLLMs' performance and error patterns, providing insights into their recognition, comprehension, and reasoning capabilities", "conclusion": "existing MLLMs are significantly challenged in realistic educational contexts"}}
{"id": "2508.06368", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06368", "abs": "https://arxiv.org/abs/2508.06368", "authors": ["Claudia dAmato", "Giuseppe Rubini", "Francesco Didio", "Donato Francioso", "Fatima Zahra Amara", "Nicola Fanizzi"], "title": "Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned", "comment": null, "summary": "Legal decision-making process requires the availability of comprehensive and\ndetailed legislative background knowledge and up-to-date information on legal\ncases and related sentences/decisions. Legal Knowledge Graphs (KGs) would be a\nvaluable tool to facilitate access to legal information, to be queried and\nexploited for the purpose, and to enable advanced reasoning and machine\nlearning applications. Indeed, legal KGs may act as knowledge intensive\ncomponent to be used by pre-dictive machine learning solutions supporting the\ndecision process of the legal expert. Nevertheless, a few KGs can be found in\nthe legal domain. To fill this gap, we developed a legal KG targeting legal\ncases of violence against women, along with clear adopted methodologies.\nSpecifically, the paper introduces two complementary approaches for automated\nlegal KG construction; a systematic bottom-up approach, customized for the\nlegal domain, and a new solution leveraging Large Language Models. Starting\nfrom legal sentences publicly available from the European Court of Justice, the\nsolutions integrate structured data extraction, ontology development, and\nsemantic enrichment to produce KGs tailored for legal cases involving violence\nagainst women. After analyzing and comparing the results of the two approaches,\nthe developed KGs are validated via suitable competency questions. The obtained\nKG may be impactful for multiple purposes: can improve the accessibility to\nlegal information both to humans and machine, can enable complex queries and\nmay constitute an important knowledge component to be possibly exploited by\nmachine learning tools tailored for predictive justice.", "AI": {"tldr": "Developed a legal KG for violence against women cases using two automated construction approaches, validated it, and found it impactful for improving legal information accessibility.", "motivation": "Legal Knowledge Graphs (KGs) would be a valuable tool to facilitate access to legal information and enable advanced reasoning, but few KGs can be found in the legal domain.", "method": "The paper introduces two complementary approaches for automated legal KG construction: a systematic bottom-up approach and a new solution leveraging Large Language Models.", "result": "Developed a legal KG targeting legal cases of violence against women, integrating structured data extraction, ontology development, and semantic enrichment.", "conclusion": "The developed KGs are validated via suitable competency questions and may be impactful for multiple purposes, such as improving accessibility to legal information and enabling complex queries."}}
{"id": "2508.06199", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06199", "abs": "https://arxiv.org/abs/2508.06199", "authors": ["Mateusz Praski", "Jakub Adamczyk", "Wojciech Czech"], "title": "Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning", "comment": null, "summary": "Pretrained neural networks have attracted significant interest in chemistry\nand small molecule drug design. Embeddings from these models are widely used\nfor molecular property prediction, virtual screening, and small data learning\nin molecular chemistry. This study presents the most extensive comparison of\nsuch models to date, evaluating 25 models across 25 datasets. Under a fair\ncomparison framework, we assess models spanning various modalities,\narchitectures, and pretraining strategies. Using a dedicated hierarchical\nBayesian statistical testing model, we arrive at a surprising result: nearly\nall neural models show negligible or no improvement over the baseline ECFP\nmolecular fingerprint. Only the CLAMP model, which is also based on molecular\nfingerprints, performs statistically significantly better than the\nalternatives. These findings raise concerns about the evaluation rigor in\nexisting studies. We discuss potential causes, propose solutions, and offer\npractical recommendations.", "AI": {"tldr": "This study presents the most extensive comparison of pretrained neural network models, evaluating 25 models across 25 datasets. We assess models spanning various modalities, architectures, and pretraining strategies. We arrive at a surprising result: nearly all neural models show negligible or no improvement over the baseline ECFP molecular fingerprint. Only the CLAMP model performs statistically significantly better than the alternatives.", "motivation": "Embeddings from pretrained neural networks are widely used for molecular property prediction, virtual screening, and small data learning in molecular chemistry.", "method": "evaluating 25 models across 25 datasets under a fair comparison framework, using a dedicated hierarchical Bayesian statistical testing model", "result": "the CLAMP model performs statistically significantly better than the alternatives", "conclusion": "nearly all neural models show negligible or no improvement over the baseline ECFP molecular fingerprint. Only the CLAMP model performs statistically significantly better than the alternatives. These findings raise concerns about the evaluation rigor in existing studies."}}
{"id": "2508.06154", "categories": ["cs.IR", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.06154", "abs": "https://arxiv.org/abs/2508.06154", "authors": ["Xiaoxiong Zhang", "Xin Zhou", "Zhiwei Zeng", "Dusit Niyato", "Zhiqi Shen"], "title": "Semantic Item Graph Enhancement for Multimodal Recommendation", "comment": null, "summary": "Multimodal recommendation systems have attracted increasing attention for\ntheir improved performance by leveraging items' multimodal information. Prior\nmethods often build modality-specific item-item semantic graphs from raw\nmodality features and use them as supplementary structures alongside the\nuser-item interaction graph to enhance user preference learning. However, these\nsemantic graphs suffer from semantic deficiencies, including (1) insufficient\nmodeling of collaborative signals among items and (2) structural distortions\nintroduced by noise in raw modality features, ultimately compromising\nperformance. To address these issues, we first extract collaborative signals\nfrom the interaction graph and infuse them into each modality-specific item\nsemantic graph to enhance semantic modeling. Then, we design a modulus-based\npersonalized embedding perturbation mechanism that injects perturbations with\nmodulus-guided personalized intensity into embeddings to generate contrastive\nviews. This enables the model to learn noise-robust representations through\ncontrastive learning, thereby reducing the effect of structural noise in\nsemantic graphs. Besides, we propose a dual representation alignment mechanism\nthat first aligns multiple semantic representations via a designed Anchor-based\nInfoNCE loss using behavior representations as anchors, and then aligns\nbehavior representations with the fused semantics by standard InfoNCE, to\nensure representation consistency. Extensive experiments on four benchmark\ndatasets validate the effectiveness of our framework.", "AI": {"tldr": "This paper addresses the semantic deficiencies in multimodal recommendation systems by enhancing semantic modeling and reducing the effect of structural noise in semantic graphs.", "motivation": "Prior methods often build modality-specific item-item semantic graphs from raw modality features and use them as supplementary structures alongside the user-item interaction graph to enhance user preference learning. However, these semantic graphs suffer from semantic deficiencies, including (1) insufficient modeling of collaborative signals among items and (2) structural distortions introduced by noise in raw modality features, ultimately compromising performance.", "method": "we first extract collaborative signals from the interaction graph and infuse them into each modality-specific item semantic graph to enhance semantic modeling. Then, we design a modulus-based personalized embedding perturbation mechanism that injects perturbations with modulus-guided personalized intensity into embeddings to generate contrastive views. Besides, we propose a dual representation alignment mechanism that first aligns multiple semantic representations via a designed Anchor-based InfoNCE loss using behavior representations as anchors, and then aligns behavior representations with the fused semantics by standard InfoNCE, to ensure representation consistency.", "result": "the model learn noise-robust representations through contrastive learning, thereby reducing the effect of structural noise in semantic graphs", "conclusion": "Extensive experiments on four benchmark datasets validate the effectiveness of our framework."}}
{"id": "2508.06194", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06194", "abs": "https://arxiv.org/abs/2508.06194", "authors": ["Lai Jiang", "Yuekang Li", "Xiaohan Zhang", "Youtao Ding", "Li Pan"], "title": "Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation", "comment": null, "summary": "Precise jailbreak evaluation is vital for LLM red teaming and jailbreak\nresearch. Current approaches employ binary classification ( e.g., string\nmatching, toxic text classifiers, LLM-driven methods), yielding only \"yes/no\"\nlabels without quantifying harm intensity. Existing multi-dimensional\nframeworks ( e.g., Security Violation, Relative Truthfulness, Informativeness)\napply uniform evaluation criteria across scenarios, resulting in\nscenario-specific mismatches--for instance, \"Relative Truthfulness\" is\nirrelevant to \"hate speech\"--which compromise evaluation precision. To tackle\nthese limitations, we introduce SceneJailEval, with key contributions: (1) A\ngroundbreaking scenario-adaptive multi-dimensional framework for jailbreak\nevaluation, overcoming the critical \"one-size-fits-all\" constraint of existing\nmulti-dimensional methods, and featuring strong extensibility to flexibly adapt\nto customized or emerging scenarios. (2) A comprehensive 14-scenario dataset\nwith diverse jailbreak variants and regional cases, filling the long-standing\ngap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3)\nSceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on\nour full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over\nprior SOTA), surpassing accuracy limits of existing evaluation methods in\nheterogeneous scenarios and confirming its advantage.", "AI": {"tldr": "This paper introduces SceneJailEval, a scenario-adaptive multi-dimensional framework and dataset for jailbreak evaluation, achieving state-of-the-art results.", "motivation": "Current jailbreak evaluation approaches lack harm intensity quantification and suffer from scenario-specific mismatches due to uniform evaluation criteria.", "method": "The paper introduces SceneJailEval, a scenario-adaptive multi-dimensional framework for jailbreak evaluation, and a comprehensive 14-scenario dataset.", "result": "SceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on their full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over prior SOTA).", "conclusion": "SceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on our full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over prior SOTA), surpassing accuracy limits of existing evaluation methods in heterogeneous scenarios and confirming its advantage."}}
{"id": "2508.06014", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06014", "abs": "https://arxiv.org/abs/2508.06014", "authors": ["Minsu Kim", "Subin Jeon", "In Cho", "Mijin Yoo", "Seon Joo Kim"], "title": "ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors", "comment": "10 pages, 6 Figures, ICCV 2025", "summary": "Recent advances in novel view synthesis (NVS) have enabled real-time\nrendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle\nwith artifacts and missing regions when rendering from viewpoints that deviate\nfrom the training trajectory, limiting seamless scene exploration. To address\nthis, we propose a 3DGS-based pipeline that generates additional training views\nto enhance reconstruction. We introduce an information-gain-driven virtual\ncamera placement strategy to maximize scene coverage, followed by video\ndiffusion priors to refine rendered results. Fine-tuning 3D Gaussians with\nthese enhanced views significantly improves reconstruction quality. To evaluate\nour method, we present Wild-Explore, a benchmark designed for challenging scene\nexploration. Experiments demonstrate that our approach outperforms existing\n3DGS-based methods, enabling high-quality, artifact-free rendering from\narbitrary viewpoints.\n  https://exploregs.github.io", "AI": {"tldr": "enhance 3DGS reconstruction quality by generating additional training views and refining rendered results with video diffusion priors", "motivation": "existing methods struggle with artifacts and missing regions when rendering from viewpoints that deviate from the training trajectory, limiting seamless scene exploration", "method": "a 3DGS-based pipeline that generates additional training views to enhance reconstruction. introducing an information-gain-driven virtual camera placement strategy to maximize scene coverage, followed by video diffusion priors to refine rendered results. Fine-tuning 3D Gaussians with these enhanced views", "result": "outperforms existing 3DGS-based methods", "conclusion": "significantly improves reconstruction quality, enabling high-quality, artifact-free rendering from arbitrary viewpoints"}}
{"id": "2508.06443", "categories": ["cs.AI", "cs.CY", "cs.ET", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.06443", "abs": "https://arxiv.org/abs/2508.06443", "authors": ["Debabrota Basu", "Udvas Das"], "title": "The Fair Game: Auditing & Debiasing AI Algorithms Over Time", "comment": null, "summary": "An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify\ndifferent types of bias (also known as unfairness) exhibited in the predictions\nof ML algorithms, and to design new algorithms to mitigate them. Often, the\ndefinitions of bias used in the literature are observational, i.e. they use the\ninput and output of a pre-trained algorithm to quantify a bias under concern.\nIn reality,these definitions are often conflicting in nature and can only be\ndeployed if either the ground truth is known or only in retrospect after\ndeploying the algorithm. Thus,there is a gap between what we want Fair ML to\nachieve and what it does in a dynamic social environment. Hence, we propose an\nalternative dynamic mechanism,\"Fair Game\",to assure fairness in the predictions\nof an ML algorithm and to adapt its predictions as the society interacts with\nthe algorithm over time. \"Fair Game\" puts together an Auditor and a Debiasing\nalgorithm in a loop around an ML algorithm. The \"Fair Game\" puts these two\ncomponents in a loop by leveraging Reinforcement Learning (RL). RL algorithms\ninteract with an environment to take decisions, which yields new observations\n(also known as data/feedback) from the environment and in turn, adapts future\ndecisions. RL is already used in algorithms with pre-fixed long-term fairness\ngoals. \"Fair Game\" provides a unique framework where the fairness goals can be\nadapted over time by only modifying the auditor and the different biases it\nquantifies. Thus,\"Fair Game\" aims to simulate the evolution of ethical and\nlegal frameworks in the society by creating an auditor which sends feedback to\na debiasing algorithm deployed around an ML system. This allows us to develop a\nflexible and adaptive-over-time framework to build Fair ML systems pre- and\npost-deployment.", "AI": {"tldr": "Fair Game, a dynamic mechanism using Reinforcement Learning, is proposed to assure fairness in ML algorithm predictions and adapt to societal interaction over time, addressing the limitations of existing observational bias definitions.", "motivation": "There is a gap between what we want Fair ML to achieve and what it does in a dynamic social environment. Observational definitions of bias are often conflicting and can only be deployed if the ground truth is known or only in retrospect after deploying the algorithm.", "method": "Fair Game puts together an Auditor and a Debiasing algorithm in a loop around an ML algorithm by leveraging Reinforcement Learning (RL).", "result": "Fair Game aims to simulate the evolution of ethical and legal frameworks in the society by creating an auditor which sends feedback to a debiasing algorithm deployed around an ML system. This allows us to develop a flexible and adaptive-over-time framework to build Fair ML systems pre- and post-deployment.", "conclusion": "Fair Game provides a unique framework where the fairness goals can be adapted over time by only modifying the auditor and the different biases it quantifies."}}
{"id": "2508.06208", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06208", "abs": "https://arxiv.org/abs/2508.06208", "authors": ["Ce Na", "Kai Yang", "Dengzhao Fang", "Yu Li", "Jingtong Gao", "Chengcheng Zhu", "Jiale Zhang", "Xiaobing Sun", "Yi Chang"], "title": "Graph Federated Learning for Personalized Privacy Recommendation", "comment": null, "summary": "Federated recommendation systems (FedRecs) have gained significant attention\nfor providing privacy-preserving recommendation services. However, existing\nFedRecs assume that all users have the same requirements for privacy\nprotection, i.e., they do not upload any data to the server. The approaches\noverlook the potential to enhance the recommendation service by utilizing\npublicly available user data. In real-world applications, users can choose to\nbe private or public. Private users' interaction data is not shared, while\npublic users' interaction data can be shared. Inspired by the issue, this paper\nproposes a novel Graph Federated Learning for Personalized Privacy\nRecommendation (GFed-PP) that adapts to different privacy requirements while\nimproving recommendation performance. GFed-PP incorporates the interaction data\nof public users to build a user-item interaction graph, which is then used to\nform a user relationship graph. A lightweight graph convolutional network (GCN)\nis employed to learn each user's user-specific personalized item embedding. To\nprotect user privacy, each client learns the user embedding and the scoring\nfunction locally. Additionally, GFed-PP achieves optimization of the federated\nrecommendation framework through the initialization of item embedding on\nclients and the aggregation of the user relationship graph on the server.\nExperimental results demonstrate that GFed-PP significantly outperforms\nexisting methods for five datasets, offering superior recommendation accuracy\nwithout compromising privacy. This framework provides a practical solution for\naccommodating varying privacy preferences in federated recommendation systems.", "AI": {"tldr": "GFed-PP\u662f\u4e00\u79cd\u65b0\u7684\u56fe\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u9690\u79c1\u63a8\u8350\uff0c\u5b83\u53ef\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u9690\u79c1\u9700\u6c42\uff0c\u540c\u65f6\u63d0\u9ad8\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8054\u90a6\u63a8\u8350\u7cfb\u7edf(FedRecs)\u5047\u8bbe\u6240\u6709\u7528\u6237\u5bf9\u9690\u79c1\u4fdd\u62a4\u90fd\u6709\u76f8\u540c\u7684\u8981\u6c42\uff0c\u5373\u4ed6\u4eec\u4e0d\u4e0a\u4f20\u4efb\u4f55\u6570\u636e\u5230\u670d\u52a1\u5668\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5ffd\u7565\u4e86\u901a\u8fc7\u5229\u7528\u516c\u5f00\u53ef\u7528\u7684\u7528\u6237\u6570\u636e\u6765\u589e\u5f3a\u63a8\u8350\u670d\u52a1\u7684\u6f5c\u529b\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u7528\u6237\u53ef\u4ee5\u9009\u62e9\u79c1\u6709\u6216\u516c\u5f00\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7528\u4e8e\u4e2a\u6027\u5316\u9690\u79c1\u63a8\u8350\u7684\u56fe\u8054\u90a6\u5b66\u4e60(GFed-PP)\uff0c\u5b83\u9002\u5e94\u4e0d\u540c\u7684\u9690\u79c1\u9700\u6c42\uff0c\u540c\u65f6\u63d0\u9ad8\u63a8\u8350\u6027\u80fd\u3002GFed-PP\u7ed3\u5408\u4e86\u516c\u5171\u7528\u6237\u7684\u4ea4\u4e92\u6570\u636e\u6765\u6784\u5efa\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u56fe\uff0c\u7136\u540e\u7528\u4e8e\u5f62\u6210\u7528\u6237\u5173\u7cfb\u56fe\u3002\u91c7\u7528\u8f7b\u91cf\u7ea7\u56fe\u5377\u79ef\u7f51\u7edc(GCN)\u6765\u5b66\u4e60\u6bcf\u4e2a\u7528\u6237\u7684\u7528\u6237\u7279\u5b9a\u4e2a\u6027\u5316\u7269\u54c1\u5d4c\u5165\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGFed-PP\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GFed-PP\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u4e0d\u635f\u5bb3\u9690\u79c1\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u5353\u8d8a\u7684\u63a8\u8350\u51c6\u786e\u6027\uff0c\u5e76\u4e3a\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u4e2d\u9002\u5e94\u4e0d\u540c\u7684\u9690\u79c1\u504f\u597d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06168", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.06168", "abs": "https://arxiv.org/abs/2508.06168", "authors": ["Hsing-Ping Liang", "Che-Wei Chang", "Yao-Chung Fan"], "title": "Improving Table Retrieval with Question Generation from Partial Tables", "comment": "TRL@ACL2025", "summary": "Recent advances in open-domain question answering over tables have widely\nadopted large language models (LLMs) under the Retriever-Reader architecture.\nPrior works have effectively leveraged LLMs to tackle the complex reasoning\ndemands of the Reader component, such as text-to-text, text-to-SQL, and multi\nhop reasoning. In contrast, the Retriever component has primarily focused on\noptimizing the query representation-training retrievers to retrieve relevant\ntables based on questions, or to select keywords from questions for matching\ntable segments. However, little attention has been given to enhancing how\ntables themselves are represented in embedding space to better align with\nquestions. To address this, we propose QGpT (Question Generation from Partial\nTables), a simple yet effective method that uses an LLM to generate synthetic\nquestions based on small portions of a table. These questions are generated to\nsimulate how a user might query the content of the table currently under\nconsideration. The generated questions are then jointly embedded with the\npartial table segments used for generation, enhancing semantic alignment with\nuser queries. Without the need to embed entire tables, our method significantly\nimproves retrieval performance across multiple benchmarks for both dense and\nlate-interaction retrievers.", "AI": {"tldr": "QGpT uses an LLM to generate synthetic questions based on small portions of a table, enhancing semantic alignment with user queries and improving retrieval performance.", "motivation": "Little attention has been given to enhancing how tables themselves are represented in embedding space to better align with questions.", "method": "Uses an LLM to generate synthetic questions based on small portions of a table.", "result": "Significantly improves retrieval performance across multiple benchmarks for both dense and late-interaction retrievers.", "conclusion": "QGpT improves retrieval performance across multiple benchmarks for both dense and late-interaction retrievers."}}
{"id": "2508.06196", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06196", "abs": "https://arxiv.org/abs/2508.06196", "authors": ["Nizi Nazar", "Ehsaneddin Asgari"], "title": "EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations", "comment": null, "summary": "Emotional Intelligence (EI) is a critical yet underexplored dimension in the\ndevelopment of human-aligned LLMs. To address this gap, we introduce a unified,\npsychologically grounded four-layer taxonomy of EI tailored for large language\nmodels (LLMs), encompassing emotional tracking, cause inference, appraisal, and\nemotionally appropriate response generation. Building on this framework, we\npresent EICAP-Bench, a novel MCQ style multi-turn benchmark designed to\nevaluate EI capabilities in open-source LLMs across diverse linguistic and\ncultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma\n(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,\nidentifying Qwen2.5-Instruct as the strongest baseline. To assess the potential\nfor enhancing EI capabilities, we fine-tune both Qwen2.5-Base and\nQwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,\ninstruction-tuned dialogue dataset, in both English and Arabic. Our statistical\nanalysis reveals that among the five EI layers, only the Appraisal layer shows\nsignificant improvement through UC-based fine-tuning. These findings highlight\nthe limitations of existing pretraining and instruction-tuning paradigms in\nequipping LLMs with deeper emotional reasoning and underscore the need for\ntargeted data and modeling strategies for comprehensive EI alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u60c5\u5546 (EI) \u80fd\u529b\u7684\u6846\u67b6\u548c\u57fa\u51c6\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u60c5\u611f\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u6709\u9488\u5bf9\u6027\u7684\u65b9\u6cd5\u3002", "motivation": "\u60c5\u5546 (EI) \u662f\u4eba\u7c7b\u5bf9\u9f50\u7684 LLM \u5f00\u53d1\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u7ef4\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u5fc3\u7406\u5b66\u57fa\u7840\u7684\u56db\u5c42 EI \u5206\u7c7b\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a EICAP-Bench \u7684\u65b0\u578b MCQ \u98ce\u683c\u591a\u8f6e\u57fa\u51c6\u6765\u8bc4\u4f30\u5f00\u6e90 LLM \u4e2d\u7684 EI \u80fd\u529b\u3002", "result": "\u5728 EmoCap-Bench \u4e0a\u8bc4\u4f30\u4e86\u516d\u4e2a LLM\uff0c\u53d1\u73b0 Qwen2.5-Instruct \u662f\u6700\u5f3a\u7684\u57fa\u7ebf\u3002\u5bf9 Qwen2.5-Base \u548c Qwen2.5-Instruct \u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u53d1\u73b0\u53ea\u6709 Appraisal \u5c42\u901a\u8fc7 UC \u83b7\u5f97\u4e86\u663e\u8457\u6539\u5584\u3002", "conclusion": "\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u548c\u6307\u4ee4\u8c03\u6574\u8303\u5f0f\u5728\u4f7f LLM \u5177\u5907\u66f4\u6df1\u5c42\u6b21\u7684\u60c5\u611f\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u9700\u8981\u6709\u9488\u5bf9\u6027\u7684\u6570\u636e\u548c\u5efa\u6a21\u7b56\u7565\u6765\u5b9e\u73b0\u5168\u9762\u7684 EI \u5bf9\u9f50\u3002"}}
{"id": "2508.06021", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06021", "abs": "https://arxiv.org/abs/2508.06021", "authors": ["Utku Ozbulak", "Michaela Cohrs", "Hristo L. Svilenov", "Joris Vankerschaver", "Wesley De Neve"], "title": "Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis", "comment": null, "summary": "Sub-visible particle analysis using flow imaging microscopy combined with\ndeep learning has proven effective in identifying particle types, enabling the\ndistinction of harmless components such as silicone oil from protein particles.\nHowever, the scarcity of available data and severe imbalance between particle\ntypes within datasets remain substantial hurdles when applying multi-class\nclassifiers to such problems, often forcing researchers to rely on less\neffective methods. The aforementioned issue is particularly challenging for\nparticle types that appear unintentionally and in lower numbers, such as\nsilicone oil and air bubbles, as opposed to protein particles, where obtaining\nlarge numbers of images through controlled settings is comparatively\nstraightforward. In this work, we develop a state-of-the-art diffusion model to\naddress data imbalance by generating high-fidelity images that can augment\ntraining datasets, enabling the effective training of multi-class deep neural\nnetworks. We validate this approach by demonstrating that the generated samples\nclosely resemble real particle images in terms of visual quality and structure.\nTo assess the effectiveness of using diffusion-generated images in training\ndatasets, we conduct large-scale experiments on a validation dataset comprising\n500,000 protein particle images and demonstrate that this approach improves\nclassification performance with no negligible downside. Finally, to promote\nopen research and reproducibility, we publicly release both our diffusion\nmodels and the trained multi-class deep neural network classifiers, along with\na straightforward interface for easy integration into future studies, at\nhttps://github.com/utkuozbulak/svp-generative-ai.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u4fdd\u771f\u56fe\u50cf\u6765\u6269\u5145\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u4e9a\u53ef\u89c1\u9897\u7c92\u5206\u6790\u4e2d\u6570\u636e\u4e0d\u5e73\u8861\u7684\u95ee\u9898\uff0c\u7ecf\u9a8c\u8bc1\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u5206\u7c7b\u6027\u80fd\uff0c\u5e76\u516c\u5f00\u4e86\u76f8\u5173\u6a21\u578b\u548c\u4ee3\u7801\u3002", "motivation": "\u5728\u4f7f\u7528\u6d41\u5f0f\u56fe\u50cf\u663e\u5fae\u955c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u4e9a\u53ef\u89c1\u9897\u7c92\u5206\u6790\u65f6\uff0c\u6570\u636e\u7a00\u7f3a\u548c\u9897\u7c92\u7c7b\u578b\u4e4b\u95f4\u7684\u4e25\u91cd\u4e0d\u5e73\u8861\u662f \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u043c \u969c\u788d\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u90a3\u4e9b\u51fa\u73b0\u91cf\u8f83\u5c11\u7684\u9897\u7c92\u7c7b\u578b\uff0c\u4f8b\u5982\u7845\u6cb9\u548c\u6c14\u6ce1\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2astate-of-the-art\u6269\u6563\u6a21\u578b\u6765\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u751f\u6210\u9ad8\u4fdd\u771f\u56fe\u50cf\u6765\u6269\u5145\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "result": "\u9a8c\u8bc1\u4e86\u751f\u6210\u6837\u672c\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u7ed3\u6784\u4e0a\u4e0e\u771f\u5b9e\u9897\u7c92\u56fe\u50cf\u975e\u5e38\u76f8\u4f3c\u3002\u5728\u5305\u542b500,000\u4e2a\u86cb\u767d\u8d28\u9897\u7c92\u56fe\u50cf\u7684\u9a8c\u8bc1\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u591a\u5206deep neural networks\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u4e14\u6ca1\u6709\u660e\u663e\u7684\u7f3a\u70b9\u3002\u8be5\u56e2\u961f\u53d1\u5e03\u4e86\u6269\u6563\u6a21\u578b\u548c\u8bad\u7ec3\u597d\u7684\u591a\u5206deep neural network classifiers\uff0c\u4ee5\u53ca\u4e00\u4e2a\u7b80\u5355\u7684\u63a5\u53e3\u3002"}}
{"id": "2508.06454", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.06454", "abs": "https://arxiv.org/abs/2508.06454", "authors": ["Joshua Caiata", "Ben Armstrong", "Kate Larson"], "title": "What Voting Rules Actually Do: A Data-Driven Analysis of Multi-Winner Voting", "comment": "41 pages", "summary": "Committee-selection problems arise in many contexts and applications, and\nthere has been increasing interest within the social choice research community\non identifying which properties are satisfied by different multi-winner voting\nrules. In this work, we propose a data-driven framework to evaluate how\nfrequently voting rules violate axioms across diverse preference distributions\nin practice, shifting away from the binary perspective of axiom satisfaction\ngiven by worst-case analysis. Using this framework, we analyze the relationship\nbetween multi-winner voting rules and their axiomatic performance under several\npreference distributions. We then show that neural networks, acting as voting\nrules, can outperform traditional rules in minimizing axiom violations. Our\nresults suggest that data-driven approaches to social choice can inform the\ndesign of new voting systems and support the continuation of data-driven\nresearch in social choice.", "AI": {"tldr": "We propose a data-driven framework to evaluate how frequently voting rules violate axioms. We then show that neural networks can outperform traditional rules in minimizing axiom violations.", "motivation": "Identifying which properties are satisfied by different multi-winner voting rules.", "method": "A data-driven framework to evaluate how frequently voting rules violate axioms across diverse preference distributions.", "result": "Neural networks, acting as voting rules, can outperform traditional rules in minimizing axiom violations.", "conclusion": "Data-driven approaches to social choice can inform the design of new voting systems and support the continuation of data-driven research in social choice."}}
{"id": "2508.06214", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06214", "abs": "https://arxiv.org/abs/2508.06214", "authors": ["Hai Zhong", "Xun Wang", "Zhuoran Li", "Longbo Huang"], "title": "Reparameterization Proximal Policy Optimization", "comment": null, "summary": "Reparameterization policy gradient (RPG) is promising for improving sample\nefficiency by leveraging differentiable dynamics. However, a critical barrier\nis its training instability, where high-variance gradients can destabilize the\nlearning process. To address this, we draw inspiration from Proximal Policy\nOptimization (PPO), which uses a surrogate objective to enable stable sample\nreuse in the model-free setting. We first establish a connection between this\nsurrogate objective and RPG, which has been largely unexplored and is\nnon-trivial. Then, we bridge this gap by demonstrating that the\nreparameterization gradient of a PPO-like surrogate objective can be computed\nefficiently using backpropagation through time. Based on this key insight, we\npropose Reparameterization Proximal Policy Optimization (RPO), a stable and\nsample-efficient RPG-based method. RPO enables multiple epochs of stable sample\nreuse by optimizing a clipped surrogate objective tailored for RPG, while being\nfurther stabilized by Kullback-Leibler (KL) divergence regularization and\nremaining fully compatible with existing variance reduction methods. We\nevaluate RPO on a suite of challenging locomotion and manipulation tasks, where\nexperiments demonstrate that our method achieves superior sample efficiency and\nstrong performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u4f18\u5316\u7b97\u6cd5RPO\uff0c\u5b83\u901a\u8fc7\u7ed3\u5408PPO\u7684\u4f18\u70b9\u6765\u7a33\u5b9aRPG\u7684\u8bad\u7ec3\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u91cd\u53c2\u6570\u5316\u7b56\u7565\u68af\u5ea6(RPG)\u6709\u671b\u901a\u8fc7\u5229\u7528\u53ef\u5fae\u52a8\u529b\u5b66\u6765\u63d0\u9ad8\u6837\u672c\u6548\u7387\u3002\u7136\u800c\uff0c\u4e00\u4e2a\u5173\u952e\u7684\u969c\u788d\u662f\u5176\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u5176\u4e2d\u9ad8\u65b9\u5dee\u68af\u5ea6\u4f1a\u7834\u574f\u5b66\u4e60\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRPG\u7684\u7a33\u5b9a\u4e14\u5177\u6709\u6837\u672c\u6548\u7387\u7684\u65b9\u6cd5\uff0c\u5373Reparameterization Proximal Policy Optimization (RPO)\u3002\u901a\u8fc7\u4f18\u5316\u4e3aRPG\u91cf\u8eab\u5b9a\u5236\u7684\u88c1\u526a\u66ff\u4ee3\u76ee\u6807\uff0c\u540c\u65f6\u901a\u8fc7Kullback-Leibler (KL)\u6563\u5ea6\u6b63\u5219\u5316\u8fdb\u4e00\u6b65\u7a33\u5b9a\uff0c\u5e76\u4e0e\u73b0\u6709\u7684\u65b9\u5dee\u51cf\u5c11\u65b9\u6cd5\u5b8c\u5168\u517c\u5bb9\uff0cRPO\u5b9e\u73b0\u4e86\u591a\u4e2aepochs\u7684\u7a33\u5b9a\u6837\u672c\u91cd\u7528\u3002", "result": "RPO\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6837\u672c\u6548\u7387\u548c\u5f3a\u5927\u7684\u6027\u80fd\u3002", "conclusion": "RPO\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u8fd0\u52a8\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6837\u672c\u6548\u7387\u548c\u5f3a\u5927\u7684\u6027\u80fd\u3002"}}
{"id": "2508.06328", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.06328", "abs": "https://arxiv.org/abs/2508.06328", "authors": ["Zhiyou Xiao", "Qinhan Yu", "Binghui Li", "Geng Chen", "Chong Chen", "Wentao Zhang"], "title": "M2IO-R1: An Efficient RL-Enhanced Reasoning Framework for Multimodal Retrieval Augmented Multimodal Generation", "comment": null, "summary": "Current research on Multimodal Retrieval-Augmented Generation (MRAG) enables\ndiverse multimodal inputs but remains limited to single-modality outputs,\nrestricting expressive capacity and practical utility. In contrast, real-world\napplications often demand both multimodal inputs and multimodal outputs for\neffective communication and grounded reasoning. Motivated by the recent success\nof Reinforcement Learning (RL) in complex reasoning tasks for Large Language\nModels (LLMs), we adopt RL as a principled and effective paradigm to address\nthe multi-step, outcome-driven challenges inherent in multimodal output\ngeneration. Here, we introduce M2IO-R1, a novel framework for Multimodal\nRetrieval-Augmented Multimodal Generation (MRAMG) that supports both multimodal\ninputs and outputs. Central to our framework is an RL-based inserter,\nInserter-R1-3B, trained with Group Relative Policy Optimization to guide image\nselection and placement in a controllable and semantically aligned manner.\nEmpirical results show that our lightweight 3B inserter achieves strong\nreasoning capabilities with significantly reduced latency, outperforming\nbaselines in both quality and efficiency.", "AI": {"tldr": "This paper introduces M2IO-R1, a novel framework for Multimodal Retrieval-Augmented Multimodal Generation (MRAMG) that supports both multimodal inputs and outputs. The framework uses an RL-based inserter to guide image selection and placement in a controllable and semantically aligned manner.", "motivation": "Current research on Multimodal Retrieval-Augmented Generation (MRAG) enables diverse multimodal inputs but remains limited to single-modality outputs, restricting expressive capacity and practical utility. In contrast, real-world applications often demand both multimodal inputs and multimodal outputs for effective communication and grounded reasoning. Motivated by the recent success of Reinforcement Learning (RL) in complex reasoning tasks for Large Language Models (LLMs), we adopt RL as a principled and effective paradigm to address the multi-step, outcome-driven challenges inherent in multimodal output generation.", "method": "an RL-based inserter, Inserter-R1-3B, trained with Group Relative Policy Optimization", "result": "outperforming baselines in both quality and efficiency", "conclusion": "a lightweight 3B inserter achieves strong reasoning capabilities with significantly reduced latency, outperforming baselines in both quality and efficiency."}}
{"id": "2508.06204", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06204", "abs": "https://arxiv.org/abs/2508.06204", "authors": ["Richard Willats", "Josh Pennington", "Aravind Mohan", "Bertie Vidgen"], "title": "Classification is a RAG problem: A case study on hate speech detection", "comment": null, "summary": "Robust content moderation requires classification systems that can quickly\nadapt to evolving policies without costly retraining. We present classification\nusing Retrieval-Augmented Generation (RAG), which shifts traditional\nclassification tasks from determining the correct category in accordance with\npre-trained parameters to evaluating content in relation to contextual\nknowledge retrieved at inference. In hate speech detection, this transforms the\ntask from \"is this hate speech?\" to \"does this violate the hate speech policy?\"\n  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates\nthis approach and offers three key advantages: (1) robust classification\naccuracy comparable to leading commercial systems, (2) inherent explainability\nvia retrieved policy segments, and (3) dynamic policy updates without model\nretraining. Through three experiments, we demonstrate strong baseline\nperformance and show that the system can apply fine-grained policy control by\ncorrectly adjusting protection for specific identity groups without requiring\nretraining or compromising overall performance. These findings establish that\nRAG can transform classification into a more flexible, transparent, and\nadaptable process for content moderation and wider classification problems.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u8fdb\u884c\u5206\u7c7b\uff0c\u5b83\u5c06\u4f20\u7edf\u7684\u5206\u7c7b\u4efb\u52a1\u4ece\u786e\u5b9a\u7b26\u5408\u9884\u8bad\u7ec3\u53c2\u6570\u7684\u6b63\u786e\u7c7b\u522b\u8f6c\u53d8\u4e3a\u8bc4\u4f30\u4e0e\u63a8\u7406\u65f6\u68c0\u7d22\u5230\u7684\u4e0a\u4e0b\u6587\u77e5\u8bc6\u76f8\u5173\u7684\u5185\u5bb9\u3002", "motivation": "\u7a33\u5065\u7684\u5185\u5bb9\u5ba1\u6838\u9700\u8981\u80fd\u591f\u5feb\u901f\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u7b56\u7565\u800c\u65e0\u9700\u6602\u8d35\u7684\u518d\u8bad\u7ec3\u7684\u5206\u7c7b\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u8fdb\u884c\u5206\u7c7b", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u901a\u8fc7\u6b63\u786e\u8c03\u6574\u5bf9\u7279\u5b9a\u8eab\u4efd\u7fa4\u4f53\u7684\u4fdd\u62a4\u6765\u5e94\u7528\u7ec6\u7c92\u5ea6\u7684\u7b56\u7565\u63a7\u5236\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u635f\u5bb3\u6574\u4f53\u6027\u80fd\u3002\u901a\u8fc7\u4e09\u4e2a\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5f3a\u5927\u7684\u57fa\u7ebf\u6027\u80fd\uff0c", "conclusion": "RAG\u53ef\u4ee5\u5c06\u5206\u7c7b\u8f6c\u5316\u4e3a\u66f4\u7075\u6d3b\u3001\u900f\u660e\u548c\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u5185\u5bb9\u5ba1\u6838\u548c\u66f4\u5e7f\u6cdb\u7684\u5206\u7c7b\u6d41\u7a0b\u3002"}}
{"id": "2508.06032", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06032", "abs": "https://arxiv.org/abs/2508.06032", "authors": ["Kiran Chhatre", "Christopher Peters", "Srikrishna Karanam"], "title": "Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts", "comment": "16 pages, 11 figures", "summary": "Existing methods for human parsing into body parts and clothing often use\nfixed mask categories with broad labels that obscure fine-grained clothing\ntypes. Recent open-vocabulary segmentation approaches leverage pretrained\ntext-to-image (T2I) diffusion model features for strong zero-shot transfer, but\ntypically group entire humans into a single person category, failing to\ndistinguish diverse clothing or detailed body parts. To address this, we\npropose Spectrum, a unified network for part-level pixel parsing (body parts\nand clothing) and instance-level grouping. While diffusion-based\nopen-vocabulary models generalize well across tasks, their internal\nrepresentations are not specialized for detailed human parsing. We observe\nthat, unlike diffusion models with broad representations, image-driven 3D\ntexture generators maintain faithful correspondence to input images, enabling\nstronger representations for parsing diverse clothing and body parts. Spectrum\nintroduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model --\nobtained by fine-tuning a T2I model on 3D human texture maps -- for improved\nalignment with body parts and clothing. From an input image, we extract\nhuman-part internal features via the I2Tx diffusion model and generate\nsemantically valid masks aligned to diverse clothing categories through\nprompt-guided grounding. Once trained, Spectrum produces semantic segmentation\nmaps for every visible body part and clothing category, ignoring standalone\ngarments or irrelevant objects, for any number of humans in the scene. We\nconduct extensive cross-dataset experiments -- separately assessing body parts,\nclothing parts, unseen clothing categories, and full-body masks -- and\ndemonstrate that Spectrum consistently outperforms baseline methods in\nprompt-based segmentation.", "AI": {"tldr": "Spectrum\uff1a\u4e00\u79cd\u7528\u4e8e\u4eba\u4f53\u90e8\u4f4d\u89e3\u6790\u7684\u7edf\u4e00\u7f51\u7edc\uff0c\u5b83\u5229\u7528\u56fe\u50cf\u5230\u7eb9\u7406\u7684\u6269\u6563\u6a21\u578b\uff0c\u5728\u57fa\u4e8e\u63d0\u793a\u7684\u5206\u5272\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5c06\u4eba\u4f53\u89e3\u6790\u4e3a\u8eab\u4f53\u90e8\u4f4d\u548c\u670d\u88c5\u7684\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u56fa\u5b9a\u7684mask\u7c7b\u522b\uff0c\u8fd9\u4e9b\u7c7b\u522b\u5177\u6709\u5e7f\u6cdb\u7684\u6807\u7b7e\uff0c\u6a21\u7cca\u4e86\u7ec6\u7c92\u5ea6\u7684\u670d\u88c5\u7c7b\u578b\u3002\u6700\u8fd1\u7684\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u65b9\u6cd5\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf(T2I)\u6269\u6563\u6a21\u578b\u7279\u5f81\u6765\u5b9e\u73b0\u5f3a\u5927\u7684zero-shot\u8fc1\u79fb\uff0c\u4f46\u901a\u5e38\u5c06\u6574\u4e2a\u4eba\u7c7b\u5206\u7ec4\u5230\u5355\u4e2aPerson\u7c7b\u522b\u4e2d\uff0c\u65e0\u6cd5\u533a\u5206\u4e0d\u540c\u7684\u670d\u88c5\u6216\u8be6\u7ec6\u7684\u8eab\u4f53\u90e8\u4f4d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u7f51\u7edcSpectrum\uff0c\u7528\u4e8e\u96f6\u4ef6\u7ea7\u50cf\u7d20\u89e3\u6790\uff08\u8eab\u4f53\u90e8\u4f4d\u548c\u670d\u88c5\uff09\u548c\u5b9e\u4f8b\u7ea7\u5206\u7ec4\u3002\u901a\u8fc7\u5bf93D\u4eba\u4f53\u7eb9\u7406\u8d34\u56fe\u8fdb\u884c\u5fae\u8c03\uff0c\u5c06\u56fe\u50cf\u5230\u7eb9\u7406(I2Tx)\u6269\u6563\u6a21\u578b\u91cd\u65b0\u7528\u4e8e\u6539\u8fdb\u4e0e\u8eab\u4f53\u90e8\u4f4d\u548c\u670d\u88c5\u7684\u5bf9\u9f50\u3002", "result": "Spectrum\u53ef\u4ee5\u4e3a\u6bcf\u4e2a\u53ef\u89c1\u7684\u8eab\u4f53\u90e8\u4f4d\u548c\u670d\u88c5\u7c7b\u522b\u751f\u6210\u8bed\u4e49\u5206\u5272\u56fe\uff0c\u5ffd\u7565\u72ec\u7acb\u7684\u670d\u88c5\u6216\u4e0d\u76f8\u5173\u7684\u5bf9\u8c61\uff0c\u9002\u7528\u4e8e\u573a\u666f\u4e2d\u4efb\u610f\u6570\u91cf\u7684\u4eba\u3002", "conclusion": "Spectrum\u5728\u57fa\u4e8e\u63d0\u793a\u7684\u5206\u5272\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2304.04475", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2304.04475", "abs": "https://arxiv.org/abs/2304.04475", "authors": ["Gaurav Deshkar", "Jayanta Kshirsagar", "Harshal Hayatnagarkar", "Janani Venugopalan"], "title": "Epidemic Control on a Large-Scale-Agent-Based Epidemiology Model using Deep Deterministic Policy Gradient", "comment": null, "summary": "To mitigate the impact of the pandemic, several measures include lockdowns,\nrapid vaccination programs, school closures, and economic stimulus. These\ninterventions can have positive or unintended negative consequences. Current\nresearch to model and determine an optimal intervention automatically through\nround-tripping is limited by the simulation objectives, scale (a few thousand\nindividuals), model types that are not suited for intervention studies, and the\nnumber of intervention strategies they can explore (discrete vs continuous). We\naddress these challenges using a Deep Deterministic Policy Gradient (DDPG)\nbased policy optimization framework on a large-scale (100,000 individual)\nepidemiological agent-based simulation where we perform multi-objective\noptimization. We determine the optimal policy for lockdown and vaccination in a\nminimalist age-stratified multi-vaccine scenario with a basic simulation for\neconomic activity. With no lockdown and vaccination (mid-age and elderly),\nresults show optimal economy (individuals below the poverty line) with balanced\nhealth objectives (infection, and hospitalization). An in-depth simulation is\nneeded to further validate our results and open-source our framework.", "AI": {"tldr": "Using DDPG on a large-scale simulation, this paper finds that no lockdown and targeted vaccination can balance economic and health objectives.", "motivation": "Current research to model and determine an optimal intervention automatically through round-tripping is limited by the simulation objectives, scale, model types that are not suited for intervention studies, and the number of intervention strategies they can explore.", "method": "Deep Deterministic Policy Gradient (DDPG) based policy optimization framework on a large-scale (100,000 individual) epidemiological agent-based simulation where we perform multi-objective optimization.", "result": "Determined the optimal policy for lockdown and vaccination in a minimalist age-stratified multi-vaccine scenario with a basic simulation for economic activity.", "conclusion": "Optimal economy with balanced health objectives (infection, and hospitalization) can be achieved with no lockdown and vaccination (mid-age and elderly)."}}
{"id": "2508.06243", "categories": ["cs.LG", "cs.NE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06243", "abs": "https://arxiv.org/abs/2508.06243", "authors": ["Ioan-Sorin Comsa", "Purav Shah", "Karthik Vaidhyanathan", "Deepak Gangadharan", "Christof Imhof", "Per Bergamin", "Aryan Kaushik", "Gabriel-Miro Muntean", "Ramona Trestian"], "title": "SCAR: State-Space Compression for AI-Driven Resource Management in 6G-Enabled Vehicular Infotainment Systems", "comment": null, "summary": "The advent of 6G networks opens new possibilities for connected infotainment\nservices in vehicular environments. However, traditional Radio Resource\nManagement (RRM) techniques struggle with the increasing volume and complexity\nof data such as Channel Quality Indicators (CQI) from autonomous vehicles. To\naddress this, we propose SCAR (State-Space Compression for AI-Driven Resource\nManagement), an Edge AI-assisted framework that optimizes scheduling and\nfairness in vehicular infotainment. SCAR employs ML-based compression\ntechniques (e.g., clustering and RBF networks) to reduce CQI data size while\npreserving essential features. These compressed states are used to train\n6G-enabled Reinforcement Learning policies that maximize throughput while\nmeeting fairness objectives defined by the NGMN. Simulations show that SCAR\nincreases time in feasible scheduling regions by 14\\% and reduces unfair\nscheduling time by 15\\% compared to RL baselines without CQI compression.\nFurthermore, Simulated Annealing with Stochastic Tunneling (SAST)-based\nclustering reduces CQI clustering distortion by 10\\%, confirming its\nefficiency. These results demonstrate SCAR's scalability and fairness benefits\nfor dynamic vehicular networks.", "AI": {"tldr": "SCAR, an Edge AI-assisted framework, optimizes scheduling and fairness in vehicular infotainment by employing ML-based compression techniques to reduce CQI data size and training 6G-enabled Reinforcement Learning policies.", "motivation": "Traditional Radio Resource Management (RRM) techniques struggle with the increasing volume and complexity of data such as Channel Quality Indicators (CQI) from autonomous vehicles.", "method": "SCAR employs ML-based compression techniques (e.g., clustering and RBF networks) to reduce CQI data size while preserving essential features. These compressed states are used to train 6G-enabled Reinforcement Learning policies.", "result": "SCAR increases time in feasible scheduling regions by 14% and reduces unfair scheduling time by 15% compared to RL baselines without CQI compression. SAST-based clustering reduces CQI clustering distortion by 10%.", "conclusion": "SCAR increases time in feasible scheduling regions and reduces unfair scheduling time compared to RL baselines without CQI compression. SAST-based clustering reduces CQI clustering distortion, confirming its efficiency. SCAR's scalability and fairness benefits for dynamic vehicular networks are demonstrated."}}
{"id": "2508.06450", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06450", "abs": "https://arxiv.org/abs/2508.06450", "authors": ["Daria Tikhonovich", "Nikita Zelinskiy", "Aleksandr V. Petrov", "Mayya Spirina", "Andrei Semenov", "Andrey V. Savchenko", "Sergei Kuliev"], "title": "eSASRec: Enhancing Transformer-based Recommendations in a Modular Fashion", "comment": "Accepted at ACM RecSys 2025", "summary": "Since their introduction, Transformer-based models, such as SASRec and\nBERT4Rec, have become common baselines for sequential recommendations,\nsurpassing earlier neural and non-neural methods. A number of following\npublications have shown that the effectiveness of these models can be improved\nby, for example, slightly updating the architecture of the Transformer layers,\nusing better training objectives, and employing improved loss functions.\nHowever, the additivity of these modular improvements has not been\nsystematically benchmarked - this is the gap we aim to close in this paper.\nThrough our experiments, we identify a very strong model that uses SASRec's\ntraining objective, LiGR Transformer layers, and Sampled Softmax Loss. We call\nthis combination eSASRec (Enhanced SASRec). While we primarily focus on\nrealistic, production-like evaluation, in our preliminarily study we find that\ncommon academic benchmarks show eSASRec to be 23% more effective compared to\nthe most recent state-of-the-art models, such as ActionPiece. In our main\nproduction-like benchmark, eSASRec resides on the Pareto frontier in terms of\nthe accuracy-coverage tradeoff (alongside the recent industrial models HSTU and\nFuXi. As the modifications compared to the original SASRec are relatively\nstraightforward and no extra features are needed (such as timestamps in HSTU),\nwe believe that eSASRec can be easily integrated into existing recommendation\npipelines and can can serve as a strong yet very simple baseline for emerging\ncomplicated algorithms. To facilitate this, we provide the open-source\nimplementations for our models and benchmarks in repository\nhttps://github.com/blondered/transformer_benchmark", "AI": {"tldr": "eSASRec, a strong and simple baseline model, outperforms existing models and is easily integrated into recommendation pipelines.", "motivation": "The additivity of modular improvements of Transformer-based models has not been systematically benchmarked", "method": "Uses SASRec's training objective, LiGR Transformer layers, and Sampled Softmax Loss", "result": "eSASRec is 23% more effective compared to the most recent state-of-the-art models in academic benchmarks, and resides on the Pareto frontier in terms of the accuracy-coverage tradeoff in production-like benchmark", "conclusion": "eSASRec can be easily integrated into existing recommendation pipelines and can can serve as a strong yet very simple baseline for emerging complicated algorithms"}}
{"id": "2508.06220", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06220", "abs": "https://arxiv.org/abs/2508.06220", "authors": ["Keummin Ka", "Junhyeong Park", "Jahyun Jeon", "Youngjae Yu"], "title": "InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?", "comment": "14 pages, 9 figures", "summary": "Recent advances in Vision-Language Models (VLMs) have demonstrated impressive\ncapabilities in perception and reasoning. However, the ability to perform\ncausal inference -- a core aspect of human cognition -- remains underexplored,\nparticularly in multimodal settings. In this study, we introduce InfoCausalQA,\na novel benchmark designed to evaluate causal reasoning grounded in\ninfographics that combine structured visual data with textual context. The\nbenchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning\nbased on inferred numerical trends, while Task 2 targets semantic causal\nreasoning involving five types of causal relations: cause, effect,\nintervention, counterfactual, and temporal. We manually collected 494\ninfographic-text pairs from four public sources and used GPT-4o to generate\n1,482 high-quality multiple-choice QA pairs. These questions were then\ncarefully revised by humans to ensure they cannot be answered based on\nsurface-level cues alone but instead require genuine visual grounding. Our\nexperimental results reveal that current VLMs exhibit limited capability in\ncomputational reasoning and even more pronounced limitations in semantic causal\nreasoning. Their significantly lower performance compared to humans indicates a\nsubstantial gap in leveraging infographic-based information for causal\ninference. Through InfoCausalQA, we highlight the need for advancing the causal\nreasoning abilities of multimodal AI systems.", "AI": {"tldr": "InfoCausalQA benchmark is introduced to evaluate causal reasoning in VLMs using infographics. Current VLMs show limited capability in computational and semantic causal reasoning.", "motivation": "the ability to perform causal inference -- a core aspect of human cognition -- remains underexplored, particularly in multimodal settings", "method": "introduce InfoCausalQA, a novel benchmark designed to evaluate causal reasoning grounded in infographics that combine structured visual data with textual context. The benchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning based on inferred numerical trends, while Task 2 targets semantic causal reasoning involving five types of causal relations: cause, effect, intervention, counterfactual, and temporal. We manually collected 494 infographic-text pairs from four public sources and used GPT-4o to generate 1,482 high-quality multiple-choice QA pairs. These questions were then carefully revised by humans", "result": "current VLMs exhibit limited capability in computational reasoning and even more pronounced limitations in semantic causal reasoning", "conclusion": "current VLMs exhibit limited capability in computational reasoning and even more pronounced limitations in semantic causal reasoning. Their significantly lower performance compared to humans indicates a substantial gap in leveraging infographic-based information for causal inference. Through InfoCausalQA, we highlight the need for advancing the causal reasoning abilities of multimodal AI systems."}}
{"id": "2508.06033", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06033", "abs": "https://arxiv.org/abs/2508.06033", "authors": ["Yiming Gong", "Zhen Zhu", "Minjia Zhang"], "title": "InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow", "comment": "ICCV 2025", "summary": "We propose a fast text-guided image editing method called InstantEdit based\non the RectifiedFlow framework, which is structured as a few-step editing\nprocess that preserves critical content while following closely to textual\ninstructions. Our approach leverages the straight sampling trajectories of\nRectifiedFlow by introducing a specialized inversion strategy called PerRFI. To\nmaintain consistent while editable results for RectifiedFlow model, we further\npropose a novel regeneration method, Inversion Latent Injection, which\neffectively reuses latent information obtained during inversion to facilitate\nmore coherent and detailed regeneration. Additionally, we propose a\nDisentangled Prompt Guidance technique to balance editability with detail\npreservation, and integrate a Canny-conditioned ControlNet to incorporate\nstructural cues and suppress artifacts. Evaluation on the PIE image editing\ndataset demonstrates that InstantEdit is not only fast but also achieves better\nqualitative and quantitative results compared to state-of-the-art few-step\nediting methods.", "AI": {"tldr": "InstantEdit\u662f\u4e00\u79cd\u5feb\u901f\u7684\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u5b83\u5728\u901f\u5ea6\u548c\u8d28\u91cf\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u7684\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5InstantEdit\uff0c\u5b83\u88ab\u6784\u5efa\u4e3a\u4e00\u4e2afew-step\u7f16\u8f91\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u5728\u9075\u5faa\u6587\u672c\u6307\u4ee4\u7684\u540c\u65f6\u4fdd\u7559\u5173\u952e\u5185\u5bb9\u3002", "method": "\u57fa\u4e8eRectifiedFlow\u6846\u67b6\uff0c\u5f15\u5165\u4e86PerRFI\u53cd\u6f14\u7b56\u7565\uff0c\u63d0\u51fa\u4e86\u53cd\u6f14\u6f5c\u5728\u6ce8\u5165\u7684\u518d\u751f\u65b9\u6cd5\uff0c\u5e76\u6574\u5408\u4e86Canny-conditioned ControlNet\u3002", "result": "InstantEdit\u901f\u5ea6\u5feb\uff0c\u5e76\u4e14\u5b9e\u73b0\u4e86\u6bd4state-of-the-art\u7684few-step\u7f16\u8f91\u65b9\u6cd5\u66f4\u597d\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u7ed3\u679c\u3002", "conclusion": "InstantEdit\u5728PIE\u56fe\u50cf\u7f16\u8f91\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u8f83\u4e8e\u76ee\u524d\u6700\u4f18\u7684few-step\u7f16\u8f91\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u901f\u5ea6\u66f4\u5feb\uff0c\u800c\u4e14\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u7ed3\u679c\u3002"}}
{"id": "2508.04748", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.04748", "abs": "https://arxiv.org/abs/2508.04748", "authors": ["Xuan Lin", "Long Chen", "Yile Wang"], "title": "AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models", "comment": "9 pages", "summary": "Large Language Models (LLMs) have shown promise in assisting molecular\nproperty prediction tasks but often rely on human-crafted prompts and\nchain-of-thought templates. While recent advanced large reasoning models like\nDeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process,\ntheir reasoning can be verbose and lack relevance. We introduce AttriLens-Mol,\nan attribute-guided reinforcement learning framework for molecular property\nprediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1)\na format reward encouraging attribute-based structured output, (2) a count\nreward to avoid enumerating irrelevant attributes, and (3) a rationality reward\nusing advanced LLMs and RDKit to verify the relatedness of the generated\nattributes. This approach implicitly elicits the model's inherent knowledge of\nrelevant molecular attributes during reasoning, enables making predictions for\nthe molecular property more effectively. Experiments on both in-distribution\nand out-of-distribution datasets show that, training both 7B-size\nR1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our\nproposed AttriLens-Mol method significantly boosts the performance, getting\ncomparable or better results than supervised fine-tuning models\n(Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o,\nDeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the\ntarget property, when used as features for an interpretable decision tree\nmodel, yield superior performance compared to attributes generated by prompting\nLLMs. This shows that AttriLens-Mol effectively elicits more relevant and\npredictive molecular attributes, leading to enhanced interpretability and\nperformance for property prediction. We release the code in\nhttps://github.com/szu-tera/AttriLens-Mol.", "AI": {"tldr": "AttriLens-Mol, an attribute-guided reinforcement learning framework, improves molecular property prediction with LLMs by eliciting relevant molecular attributes, enhancing performance and interpretability.", "motivation": "Large Language Models (LLMs) in molecular property prediction tasks often rely on human-crafted prompts and lack relevance in reasoning.", "method": "Attribute-guided reinforcement learning framework (AttriLens-Mol) using format, count, and rationality rewards to steer the model's reasoning.", "result": "Training 7B-size models with AttriLens-Mol significantly boosts performance, achieving comparable or better results than supervised fine-tuning and advanced models on both in-distribution and out-of-distribution datasets.", "conclusion": "AttriLens-Mol effectively elicits more relevant and predictive molecular attributes, leading to enhanced interpretability and performance for property prediction. Experiments show significant performance boost compared to supervised fine-tuning and advanced models. Extracted attributes yield superior performance in interpretable decision tree models."}}
{"id": "2508.06244", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.06244", "abs": "https://arxiv.org/abs/2508.06244", "authors": ["Xurun Wang", "Guangrui Liu", "Xinjie Li", "Haoyu He", "Lin Yao", "Weizhe Zhang"], "title": "Membership Inference Attack with Partial Features", "comment": null, "summary": "Machine learning models have been shown to be susceptible to membership\ninference attack, which can be used to determine whether a given sample appears\nin the training data. Existing membership inference methods commonly assume\nthat the adversary has full access to the features of the target sample. This\nassumption, however, does not hold in many real-world scenarios where only\npartial features information is available, thereby limiting the applicability\nof these methods. In this work, we study an inference scenario where the\nadversary observes only partial features of each sample and aims to infer\nwhether this observed subset was present in the training set of the target\nmodel. We define this problem as Partial Feature Membership Inference (PFMI).\nTo address this problem, we propose MRAD (Memory-guided Reconstruction and\nAnomaly Detection), a two-stage attack framework. In the first stage, MRAD\noptimizes the unknown feature values to minimize the loss of the sample. In the\nsecond stage, it measures the deviation between the reconstructed sample and\nthe training distribution using anomaly detection. Empirical results\ndemonstrate that MRAD is effective across a range of datasets, and maintains\ncompatibility with various off-the-shelf anomaly detection techniques. For\nexample, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of\nthe missing features.", "AI": {"tldr": "The paper studies membership inference attacks with partial feature information and proposes a two-stage attack framework called MRAD, which reconstructs missing features and uses anomaly detection to infer membership.", "motivation": "Existing membership inference methods commonly assume that the adversary has full access to the features of the target sample. This assumption, however, does not hold in many real-world scenarios where only partial features information is available, thereby limiting the applicability of these methods. In this work, we study an inference scenario where the adversary observes only partial features of each sample and aims to infer whether this observed subset was present in the training set of the target model. We define this problem as Partial Feature Membership Inference (PFMI).", "method": "MRAD (Memory-guided Reconstruction and Anomaly Detection), a two-stage attack framework. In the first stage, MRAD optimizes the unknown feature values to minimize the loss of the sample. In the second stage, it measures the deviation between the reconstructed sample and the training distribution using anomaly detection.", "result": "MRAD is effective across a range of datasets, and maintains compatibility with various off-the-shelf anomaly detection techniques. For example, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of the missing features.", "conclusion": "MRAD is effective across a range of datasets, and maintains compatibility with various off-the-shelf anomaly detection techniques. For example, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of the missing features."}}
{"id": "2508.06455", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06455", "abs": "https://arxiv.org/abs/2508.06455", "authors": ["Nikita Sukhorukov", "Danil Gusak", "Evgeny Frolov"], "title": "Maximum Impact with Fewer Features: Efficient Feature Selection for Cold-Start Recommenders through Collaborative Importance Weighting", "comment": null, "summary": "Cold-start challenges in recommender systems necessitate leveraging auxiliary\nfeatures beyond user-item interactions. However, the presence of irrelevant or\nnoisy features can degrade predictive performance, whereas an excessive number\nof features increases computational demands, leading to higher memory\nconsumption and prolonged training times.\n  To address this, we propose a feature selection strategy that prioritizes the\nuser behavioral information. Our method enhances the feature representation by\nincorporating correlations from collaborative behavior data using a hybrid\nmatrix factorization technique and then ranks features using a mechanism based\non the maximum volume algorithm. This approach identifies the most influential\nfeatures, striking a balance between recommendation accuracy and computational\nefficiency. We conduct an extensive evaluation across various datasets and\nhybrid recommendation models, demonstrating that our method excels in\ncold-start scenarios by selecting minimal yet highly effective feature subsets.\nEven under strict feature reduction, our approach surpasses existing feature\nselection techniques while maintaining superior efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7279\u5f81\u9009\u62e9\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u4f18\u5148\u8003\u8651\u7528\u6237\u884c\u4e3a\u4fe1\u606f\uff0c\u901a\u8fc7\u7ed3\u5408\u534f\u4f5c\u884c\u4e3a\u6570\u636e\u7684\u76f8\u5173\u6027\u6765\u589e\u5f3a\u7279\u5f81\u8868\u793a\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u6700\u5927\u4f53\u79ef\u7b97\u6cd5\u7684\u673a\u5236\u5bf9\u7279\u5f81\u8fdb\u884c\u6392\u5e8f\uff0c\u4ece\u800c\u5728\u63a8\u8350\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u51b7\u542f\u52a8\u6311\u6218\u9700\u8981\u5229\u7528\u7528\u6237-\u9879\u76ee\u4ea4\u4e92\u4e4b\u5916\u7684\u8f85\u52a9\u7279\u5f81\u3002\u7136\u800c\uff0c\u4e0d\u76f8\u5173\u6216\u5608\u6742\u7279\u5f81\u7684\u5b58\u5728\u4f1a\u964d\u4f4e\u9884\u6d4b\u6027\u80fd\uff0c\u800c\u8fc7\u591a\u7684\u7279\u5f81\u4f1a\u589e\u52a0\u8ba1\u7b97\u9700\u6c42\uff0c\u5bfc\u81f4\u66f4\u9ad8\u7684\u5185\u5b58\u6d88\u8017\u548c\u66f4\u957f\u7684\u8bad\u7ec3\u65f6\u95f4\u3002", "method": "\u7ed3\u5408\u4e86\u6765\u81ea\u534f\u4f5c\u884c\u4e3a\u6570\u636e\u7684\u76f8\u5173\u6027\uff0c\u4f7f\u7528\u6df7\u5408\u77e9\u9635\u5206\u89e3\u6280\u672f\u589e\u5f3a\u7279\u5f81\u8868\u793a\uff0c\u7136\u540e\u4f7f\u7528\u57fa\u4e8e\u6700\u5927\u4f53\u79ef\u7b97\u6cd5\u7684\u673a\u5236\u5bf9\u7279\u5f81\u8fdb\u884c\u6392\u5e8f\u3002", "result": "\u5728\u5404\u79cd\u6570\u636e\u96c6\u548c\u6df7\u5408\u63a8\u8350\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u51b7\u542f\u52a8\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9009\u62e9\u4e86\u6700\u5c11\u4f46\u975e\u5e38\u6709\u6548\u7684\u7279\u5f81\u5b50\u96c6\u3002\u5373\u4f7f\u5728\u4e25\u683c\u7684\u7279\u5f81\u51cf\u5c11\u4e0b\uff0c\u8be5\u65b9\u6cd5\u4e5f\u4f18\u4e8e\u73b0\u6709\u7684\u7279\u5f81\u9009\u62e9\u6280\u672f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5353\u8d8a\u7684\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u51b7\u542f\u52a8\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9009\u62e9\u4e86\u6700\u5c11\u4f46\u975e\u5e38\u6709\u6548\u7684\u7279\u5f81\u5b50\u96c6\u3002\u5373\u4f7f\u5728\u4e25\u683c\u7684\u7279\u5f81\u51cf\u5c11\u4e0b\uff0c\u8be5\u65b9\u6cd5\u4e5f\u4f18\u4e8e\u73b0\u6709\u7684\u7279\u5f81\u9009\u62e9\u6280\u672f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5353\u8d8a\u7684\u6548\u7387\u3002"}}
{"id": "2508.06277", "categories": ["cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.06277", "abs": "https://arxiv.org/abs/2508.06277", "authors": ["Theresa Pekarek Rosin", "Burak Can Kaplan", "Stefan Wermter"], "title": "Large Language Model Data Generation for Enhanced Intent Recognition in German Speech", "comment": "11 pages, 3 figures, accepted at KONVENS 2025", "summary": "Intent recognition (IR) for speech commands is essential for artificial\nintelligence (AI) assistant systems; however, most existing approaches are\nlimited to short commands and are predominantly developed for English. This\npaper addresses these limitations by focusing on IR from speech by elderly\nGerman speakers. We propose a novel approach that combines an adapted Whisper\nASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based\nlanguage models trained on synthetic text datasets generated by three\nwell-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To\nevaluate the robustness of our approach, we generate synthetic speech with a\ntext-to-speech model and conduct extensive cross-dataset testing. Our results\nshow that synthetic LLM-generated data significantly boosts classification\nperformance and robustness to different speaking styles and unseen vocabulary.\nNotably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the\nmuch larger ChatGPT (175B) in dataset quality for German intent recognition.\nOur approach demonstrates that generative AI can effectively bridge data gaps\nin low-resource domains. We provide detailed documentation of our data\ngeneration and training process to ensure transparency and reproducibility.", "AI": {"tldr": "This paper introduces a novel approach for intent recognition from speech by elderly German speakers, using synthetic data generated by LLMs to improve performance and robustness. The approach combines an adapted Whisper ASR model with Transformer-based language models. LeoLM outperforms ChatGPT in generating data for this task.", "motivation": "most existing approaches are limited to short commands and are predominantly developed for English. This paper addresses these limitations by focusing on IR from speech by elderly German speakers.", "method": "combines an adapted Whisper ASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based language models trained on synthetic text datasets generated by three well-known large language models (LLMs): LeoLM, Llama3, and ChatGPT", "result": "synthetic LLM-generated data significantly boosts classification performance and robustness to different speaking styles and unseen vocabulary. Notably, LeoLM surpasses ChatGPT in dataset quality for German intent recognition.", "conclusion": "Generative AI can effectively bridge data gaps in low-resource domains."}}
{"id": "2508.06036", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06036", "abs": "https://arxiv.org/abs/2508.06036", "authors": ["Jun Xie", "Yingjian Zhu", "Feng Chen", "Zhenghao Zhang", "Xiaohui Fan", "Hongzhu Yi", "Xinming Wang", "Chen Yu", "Yue Bi", "Zhaoran Zhao", "Xiongjun Guan", "Zhepeng Wang"], "title": "More Is Better: A MoE-Based Emotion Recognition Framework with Human Preference Alignment", "comment": null, "summary": "In this paper, we present our solution for the semi-supervised learning track\n(MER-SEMI) in MER2025. We propose a comprehensive framework, grounded in the\nprinciple that \"more is better,\" to construct a robust Mixture of Experts (MoE)\nemotion recognition system. Our approach integrates a diverse range of input\nmodalities as independent experts, including novel signals such as knowledge\nfrom large Vision-Language Models (VLMs) and temporal Action Unit (AU)\ninformation. To effectively utilize unlabeled data, we introduce a\nconsensus-based pseudo-labeling strategy, generating high-quality labels from\nthe agreement between a baseline model and Gemini, which are then used in a\ntwo-stage training paradigm. Finally, we employ a multi-expert voting ensemble\ncombined with a rule-based re-ranking process to correct prediction bias and\nbetter align the outputs with human preferences. Evaluated on the MER2025-SEMI\nchallenge dataset, our method achieves an F1-score of 0.8772 on the test set,\nranking 2nd in the track. Our code is available at\nhttps://github.com/zhuyjan/MER2025-MRAC25.", "AI": {"tldr": "Proposes a Mixture of Experts (MoE) emotion recognition system using multi-modal inputs, consensus-based pseudo-labeling, and ensemble methods, achieving 2nd place in the MER2025-SEMI challenge with an F1-score of 0.8772.", "motivation": "To construct a robust Mixture of Experts (MoE) emotion recognition system.", "method": "A comprehensive framework, grounded in the principle that \"more is better,\" to construct a robust Mixture of Experts (MoE) emotion recognition system. Integrates a diverse range of input modalities as independent experts, including novel signals such as knowledge from large Vision-Language Models (VLMs) and temporal Action Unit (AU) information. Introduce a consensus-based pseudo-labeling strategy, generating high-quality labels from the agreement between a baseline model and Gemini, which are then used in a two-stage training paradigm. Employ a multi-expert voting ensemble combined with a rule-based re-ranking process.", "result": "Achieves an F1-score of 0.8772 on the test set, ranking 2nd in the track.", "conclusion": "Achieved an F1-score of 0.8772 on the MER2025-SEMI challenge dataset, ranking 2nd in the track."}}
{"id": "2508.06247", "categories": ["cs.LG", "cs.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.06247", "abs": "https://arxiv.org/abs/2508.06247", "authors": ["Zichun Ye", "Runqi Wang", "Xutong Liu", "Shuai Li"], "title": "Near-Optimal Regret for Efficient Stochastic Combinatorial Semi-Bandits", "comment": null, "summary": "The combinatorial multi-armed bandit (CMAB) is a cornerstone of sequential\ndecision-making framework, dominated by two algorithmic families: UCB-based and\nadversarial methods such as follow the regularized leader (FTRL) and online\nmirror descent (OMD). However, prominent UCB-based approaches like CUCB suffer\nfrom additional regret factor $\\log T$ that is detrimental over long horizons,\nwhile adversarial methods such as EXP3.M and HYBRID impose significant\ncomputational overhead. To resolve this trade-off, we introduce the\nCombinatorial Minimax Optimal Strategy in the Stochastic setting (CMOSS). CMOSS\nis a computationally efficient algorithm that achieves an instance-independent\nregret of $O\\big( (\\log k)^2\\sqrt{kmT}\\big )$ under semi-bandit feedback, where\n$m$ is the number of arms and $k$ is the maximum cardinality of a feasible\naction. Crucially, this result eliminates the dependency on $\\log T$ and\nmatches the established $\\Omega\\big( \\sqrt{kmT}\\big)$ lower bound up to\n$O\\big((\\log k)^2\\big)$. We then extend our analysis to show that CMOSS is also\napplicable to cascading feedback. Experiments on synthetic and real-world\ndatasets validate that CMOSS consistently outperforms benchmark algorithms in\nboth regret and runtime efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684CMAB\u7b97\u6cd5CMOSS\uff0c\u8be5\u7b97\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\uff0c\u5e76\u5728\u540e\u6094\u503c\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u4f18\u901f\u7387\u3002", "motivation": "UCB\u7c7b\u7b97\u6cd5\u5b58\u5728\u989d\u5916\u7684\u5bf9\u6570T\u7684\u9057\u61be\u56e0\u5b50\uff0c\u800c\u5bf9\u6297\u7c7b\u7b97\u6cd5\u5982EXP3.M\u548cHYBRID\u5219\u5e26\u6765\u663e\u8457\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00trade-off\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u968f\u673a\u8bbe\u7f6e\u4e0b\u7684\u7ec4\u5408\u6781\u5c0f\u6781\u5927\u6700\u4f18\u7b56\u7565\uff08CMOSS\uff09\u3002", "result": "CMOSS\u7b97\u6cd5\u5b9e\u73b0\u4e86\u4e0e$\\Omega\\[\b \\big( \\sqrt{kmT}\\big)$\u4e0b\u754c\u76f8\u5339\u914d\u7684$O\\big( (\\log k)^2\\sqrt{kmT}\\big )$\u7684\u5b9e\u4f8b\u72ec\u7acb\u9057\u61be\u503c\u3002", "conclusion": "CMOSS\u7b97\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u5728\u540e\u6094\u503c\u548c\u8fd0\u884c\u6548\u7387\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u57fa\u51c6\u7b97\u6cd5\u3002"}}
{"id": "2508.06309", "categories": ["cs.CL", "math.PR"], "pdf": "https://arxiv.org/pdf/2508.06309", "abs": "https://arxiv.org/abs/2508.06309", "authors": ["Ruichong Zhang"], "title": "Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC", "comment": null, "summary": "In recent years, concerns about intellectual property (IP) in large language\nmodels (LLMs) have grown significantly. Plagiarizing other LLMs (through direct\nweight copying, upcycling, pruning, or continual pretraining) and claiming\nauthorship without properly attributing to the original license, is a serious\nmisconduct that can lead to significant financial and reputational harm to the\noriginal developers. However, existing methods for detecting LLM plagiarism\nfall short in key areas. They fail to accurately reconstruct weight\ncorrespondences, lack the ability to compute statistical significance measures\nsuch as $p$-values, and may mistakenly flag models trained on similar data as\nbeing related. To address these limitations, we propose Matrix-Driven Instant\nReview (MDIR), a novel method that leverages matrix analysis and Large\nDeviation Theory. MDIR achieves accurate reconstruction of weight\nrelationships, provides rigorous $p$-value estimation, and focuses exclusively\non weight similarity without requiring full model inference. Experimental\nresults demonstrate that MDIR reliably detects plagiarism even after extensive\ntransformations, such as random permutations and continual pretraining with\ntrillions of tokens. Moreover, all detections can be performed on a single PC\nwithin an hour, making MDIR both efficient and accessible.", "AI": {"tldr": "This paper introduces MDIR, a novel method for detecting LLM plagiarism that overcomes the limitations of existing methods by using matrix analysis and Large Deviation Theory. MDIR is accurate, efficient, and accessible, even after extensive transformations.", "motivation": "Concerns about intellectual property (IP) in large language models (LLMs) have grown significantly. Existing methods for detecting LLM plagiarism fall short in key areas. They fail to accurately reconstruct weight correspondences, lack the ability to compute statistical significance measures such as $p$-values, and may mistakenly flag models trained on similar data as being related.", "method": "Matrix-Driven Instant Review (MDIR), a novel method that leverages matrix analysis and Large Deviation Theory.", "result": "MDIR achieves accurate reconstruction of weight relationships, provides rigorous $p$-value estimation, and focuses exclusively on weight similarity without requiring full model inference.", "conclusion": "MDIR reliably detects plagiarism even after extensive transformations, such as random permutations and continual pretraining with trillions of tokens. Moreover, all detections can be performed on a single PC within an hour, making MDIR both efficient and accessible."}}
{"id": "2508.06038", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06038", "abs": "https://arxiv.org/abs/2508.06038", "authors": ["Huanyu Wang", "Jushi Kai", "Haoli Bai", "Lu Hou", "Bo Jiang", "Ziwei He", "Zhouhan Lin"], "title": "Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models", "comment": "12 pages, 4 figures", "summary": "Vision-Language Models (VLMs) typically replace the predefined image\nplaceholder token (<image>) in textual instructions with visual features from\nan image encoder, forming the input to a backbone Large Language Model (LLM).\nHowever, the large number of vision tokens significantly increases the context\nlength, leading to high computational overhead and inference latency. While\nprevious efforts mitigate this by selecting only important visual features or\nleveraging learnable queries to reduce token count, they often compromise\nperformance or introduce substantial extra costs. In response, we propose\nFourier-VLM, a simple yet efficient method that compresses visual\nrepresentations in the frequency domain. Our approach is motivated by the\nobservation that vision features output from the vision encoder exhibit\nconcentrated energy in low-frequency components. Leveraging this, we apply a\nlow-pass filter to the vision features using a two-dimentional Discrete Cosine\nTransform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier\nTransform (FFT) operator with a time complexity of $\\mathcal{O}(n\\log n)$,\nminimizing the extra computational cost while introducing no additional\nparameters. Extensive experiments across various image-based benchmarks\ndemonstrate that Fourier-VLM achieves competitive performance with strong\ngeneralizability across both LLaVA and Qwen-VL architectures. Crucially, it\nreduce inference FLOPs by up to 83.8% and boots generation speed by 31.2%\ncompared to LLaVA-v1.5, highlighting the superior efficiency and practicality.", "AI": {"tldr": "Fourier-VLM compresses visual representations in the frequency domain using DCT, achieving significant efficiency gains with competitive performance.", "motivation": "the observation that vision features output from the vision encoder exhibit concentrated energy in low-frequency components", "method": "applies a low-pass filter to the vision features using a two-dimentional Discrete Cosine Transform (DCT)", "result": "reduce inference FLOPs by up to 83.8% and boots generation speed by 31.2% compared to LLaVA-v1.5", "conclusion": "Fourier-VLM achieves competitive performance with strong generalizability across both LLaVA and Qwen-VL architectures. Crucially, it reduce inference FLOPs by up to 83.8% and boots generation speed by 31.2% compared to LLaVA-v1.5, highlighting the superior efficiency and practicality."}}
{"id": "2508.06249", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06249", "abs": "https://arxiv.org/abs/2508.06249", "authors": ["David Kacz\u00e9r", "Magnus J\u00f8rgenv\u00e5g", "Clemens Vetter", "Lucie Flek", "Florian Mai"], "title": "In-Training Defenses against Emergent Misalignment in Language Models", "comment": "Under review", "summary": "Fine-tuning lets practitioners repurpose aligned large language models (LLMs)\nfor new domains, yet recent work reveals emergent misalignment (EMA): Even a\nsmall, domain-specific fine-tune can induce harmful behaviors far outside the\ntarget domain. Even in the case where model weights are hidden behind a\nfine-tuning API, this gives attackers inadvertent access to a broadly\nmisaligned model in a way that can be hard to detect from the fine-tuning data\nalone. We present the first systematic study of in-training safeguards against\nEMA that are practical for providers who expose fine-tuning via an API. We\ninvestigate four training regularization interventions: (i) KL-divergence\nregularization toward a safe reference model, (ii) $\\ell_2$ distance in feature\nspace, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving\nof a small amount of safe training examples from a general instruct-tuning\ndataset. We first evaluate the methods' emergent misalignment effect across\nfour malicious, EMA-inducing tasks. Second, we assess the methods' impacts on\nbenign tasks. We conclude with a discussion of open questions in emergent\nmisalignment research.", "AI": {"tldr": "\u7814\u7a76\u4e86\u9488\u5bf9 EMA \u7684\u8bad\u7ec3\u4e2d\u4fdd\u62a4\u63aa\u65bd\uff0c\u8fd9\u4e9b\u63aa\u65bd\u5bf9\u4e8e\u901a\u8fc7 API \u516c\u5f00\u5fae\u8c03\u7684\u63d0\u4f9b\u5546\u6765\u8bf4\u662f\u5b9e\u7528\u7684\u3002", "motivation": "\u5373\u4f7f\u662f\u5c0f\u7684\u3001\u7279\u5b9a\u9886\u57df\u7684\u5fae\u8c03\u4e5f\u4f1a\u5bfc\u81f4\u76ee\u6807\u9886\u57df\u4e4b\u5916\u7684\u6709\u5bb3\u884c\u4e3a\u3002\u5373\u4f7f\u6a21\u578b\u6743\u91cd\u9690\u85cf\u5728\u5fae\u8c03 API \u4e4b\u540e\uff0c\u8fd9\u4e5f\u4f1a\u4f7f\u653b\u51fb\u8005\u65e0\u610f\u4e2d\u8bbf\u95ee\u5e7f\u6cdb\u9519\u4f4d\u7684\u6a21\u578b\uff0c\u800c\u8fd9\u5f88\u96be\u4ec5\u4ece\u5fae\u8c03\u6570\u636e\u4e2d\u68c0\u6d4b\u5230\u3002", "method": "\u7814\u7a76\u4e86\u56db\u79cd\u8bad\u7ec3\u6b63\u5219\u5316\u5e72\u9884\u63aa\u65bd\uff1a(i) KL \u6563\u5ea6\u6b63\u5219\u5316\u5230\u5b89\u5168\u53c2\u8003\u6a21\u578b\uff0c(ii) \u7279\u5f81\u7a7a\u95f4\u4e2d\u7684 $\\ell_2$ \u8ddd\u79bb\uff0c(iii) \u6295\u5f71\u5230\u5b89\u5168\u5b50\u7a7a\u95f4 (SafeLoRA)\uff0c\u4ee5\u53ca (iv) \u7a7f\u63d2\u6765\u81ea\u901a\u7528\u6307\u4ee4\u8c03\u6574\u6570\u636e\u96c6\u7684\u5c11\u91cf\u5b89\u5168\u8bad\u7ec3\u793a\u4f8b\u3002", "result": "\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5728\u56db\u4e2a\u6076\u610f\u3001\u8bf1\u5bfc EMA \u7684\u4efb\u52a1\u4e2d\u7684\u7d27\u6025\u9519\u4f4d\u6548\u5e94\uff0c\u5e76\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5bf9\u826f\u6027\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u7d27\u6025\u9519\u4f4d\u7814\u7a76\u4e2d\u7684\u5f00\u653e\u6027\u95ee\u9898\u3002"}}
{"id": "2508.06345", "categories": ["cs.CL", "cs.AI", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06345", "abs": "https://arxiv.org/abs/2508.06345", "authors": ["Yanbin Wei", "Jiangyue Yan", "Chun Kang", "Yang Chen", "Hua Liu", "James T. Kwok", "Yu Zhang"], "title": "Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering", "comment": null, "summary": "Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities\nin diverse domain question-answering (QA) tasks, including graph QA that\ninvolves complex graph topologies. However, most current approaches use only a\nsingle type of graph representation, namely Topology Representation Form (TRF),\nsuch as prompt-unified text descriptions or style-fixed visual styles. Those\n\"one-size-fits-all\" approaches fail to consider the specific preferences of\ndifferent models or tasks, often leading to incorrect or overly long responses.\nTo address this, we first analyze the characteristics and weaknesses of\nexisting TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to\nzero-shot graph QA. We then introduce a new metric, Graph Response Efficiency\n(GRE), which measures the balance between the performance and the brevity in\ngraph QA. Built on these, we develop the DynamicTRF framework, which aims to\nimprove both the accuracy and conciseness of graph QA. To be specific,\nDynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based\non their GRE scores, to probe the question-specific TRF preferences. Then it\ntrains a TRF router on the TRFP dataset, to adaptively assign the best TRF from\n$F_{ZS}$ for each question during the inference. Extensive experiments across 7\nin-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show\nthat DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms\nof accuracy", "AI": {"tldr": "DynamicTRF\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6700\u4f73\u56fe\u8868\u793a\u6765\u63d0\u9ad8\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u56feQA\u4e2d\u7684\u51c6\u786e\u6027\u548c\u7b80\u6d01\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u65b9\u6cd5\u53ea\u4f7f\u7528\u5355\u4e00\u7c7b\u578b\u7684\u56fe\u8868\u793a\uff0c\u5373\u62d3\u6251\u8868\u793a\u5f62\u5f0f(TRF)\uff0c\u5982\u63d0\u793a\u7edf\u4e00\u6587\u672c\u63cf\u8ff0\u6216\u6837\u5f0f\u56fa\u5b9a\u7684\u89c6\u89c9\u6837\u5f0f\u3002\u8fd9\u4e9b\u201c\u4e00\u5200\u5207\u201d\u7684\u65b9\u6cd5\u6ca1\u6709\u8003\u8651\u5230\u4e0d\u540c\u6a21\u578b\u6216\u4efb\u52a1\u7684\u7279\u5b9a\u504f\u597d\uff0c\u901a\u5e38\u4f1a\u5bfc\u81f4\u4e0d\u6b63\u786e\u6216\u8fc7\u957f\u7684\u54cd\u5e94\u3002", "method": "DynamicTRF\u6846\u67b6\uff0c\u5b83\u9996\u5148\u521b\u5efa\u4e00\u4e2aTRF\u504f\u597d(TRFP)\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u6839\u636eGRE\u5206\u6570\u5bf9trf\u8fdb\u884c\u6392\u5e8f\uff0c\u4ee5\u63a2\u6d4b\u7279\u5b9a\u95ee\u9898\u7684TRF\u504f\u597d\u3002\u7136\u540e\uff0c\u5b83\u5728TRFP\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e00\u4e2aTRF\u8def\u7531\u5668\uff0c\u4ee5\u4fbf\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u81ea\u9002\u5e94\u5730\u4e3a\u6bcf\u4e2a\u95ee\u9898\u5206\u914d\u6765\u81ea$F_{ZS}$\u7684\u6700\u4f73TRF\u3002", "result": "\u57287\u4e2a\u57df\u5185\u7b97\u6cd5\u56feQA\u4efb\u52a1\u548c2\u4e2a\u57df\u5916\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e", "conclusion": "DynamicTRF\u663e\u8457\u63d0\u9ad8\u4e86LMM\u5728\u96f6\u6837\u672c\u56feQA\u65b9\u9762\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.06044", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06044", "abs": "https://arxiv.org/abs/2508.06044", "authors": ["Huimin Wu", "Xiaojian Ma", "Haozhe Zhao", "Yanpeng Zhao", "Qing Li"], "title": "NEP: Autoregressive Image Editing via Next Editing Token Prediction", "comment": "The project page is: https://nep-bigai.github.io/", "summary": "Text-guided image editing involves modifying a source image based on a\nlanguage instruction and, typically, requires changes to only small local\nregions. However, existing approaches generate the entire target image rather\nthan selectively regenerate only the intended editing areas. This results in\n(1) unnecessary computational costs and (2) a bias toward reconstructing\nnon-editing regions, which compromises the quality of the intended edits. To\nresolve these limitations, we propose to formulate image editing as Next\nEditing-token Prediction (NEP) based on autoregressive image generation, where\nonly regions that need to be edited are regenerated, thus avoiding unintended\nmodification to the non-editing areas. To enable any-region editing, we propose\nto pre-train an any-order autoregressive text-to-image (T2I) model. Once\ntrained, it is capable of zero-shot image editing and can be easily adapted to\nNEP for image editing, which achieves a new state-of-the-art on widely used\nimage editing benchmarks. Moreover, our model naturally supports test-time\nscaling (TTS) through iteratively refining its generation in a zero-shot\nmanner. The project page is: https://nep-bigai.github.io/", "AI": {"tldr": "propose to formulate image editing as Next Editing-token Prediction (NEP) based on autoregressive image generation, where only regions that need to be edited are regenerated, thus avoiding unintended modification to the non-editing areas", "motivation": "existing approaches generate the entire target image rather than selectively regenerate only the intended editing areas. This results in (1) unnecessary computational costs and (2) a bias toward reconstructing non-editing regions, which compromises the quality of the intended edits", "method": "formulate image editing as Next Editing-token Prediction (NEP) based on autoregressive image generation", "result": "only regions that need to be edited are regenerated, thus avoiding unintended modification to the non-editing areas. To enable any-region editing, we propose to pre-train an any-order autoregressive text-to-image (T2I) model. Once trained, it is capable of zero-shot image editing and can be easily adapted to NEP for image editing, which achieves a new state-of-the-art on widely used image editing benchmarks. Moreover, our model naturally supports test-time scaling (TTS) through iteratively refining its generation in a zero-shot manner.", "conclusion": "achieves a new state-of-the-art on widely used image editing benchmarks"}}
{"id": "2508.06251", "categories": ["cs.LG", "cs.AI", "cs.CR", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06251", "abs": "https://arxiv.org/abs/2508.06251", "authors": ["Alejandro Moreno R.", "Desale Fentaw", "Samuel Palmer", "Ra\u00fal Salles de Padua", "Ninad Dixit", "Samuel Mugel", "Roman Or\u00fas", "Manuel Radons", "Josef Menter", "Ali Abedi"], "title": "Synthetic Data Generation and Differential Privacy using Tensor Networks' Matrix Product States (MPS)", "comment": "10 pages", "summary": "Synthetic data generation is a key technique in modern artificial\nintelligence, addressing data scarcity, privacy constraints, and the need for\ndiverse datasets in training robust models. In this work, we propose a method\nfor generating privacy-preserving high-quality synthetic tabular data using\nTensor Networks, specifically Matrix Product States (MPS). We benchmark the\nMPS-based generative model against state-of-the-art models such as CTGAN, VAE,\nand PrivBayes, focusing on both fidelity and privacy-preserving capabilities.\nTo ensure differential privacy (DP), we integrate noise injection and gradient\nclipping during training, enabling privacy guarantees via R\\'enyi Differential\nPrivacy accounting. Across multiple metrics analyzing data fidelity and\ndownstream machine learning task performance, our results show that MPS\noutperforms classical models, particularly under strict privacy constraints.\nThis work highlights MPS as a promising tool for privacy-aware synthetic data\ngeneration. By combining the expressive power of tensor network representations\nwith formal privacy mechanisms, the proposed approach offers an interpretable\nand scalable alternative for secure data sharing. Its structured design\nfacilitates integration into sensitive domains where both data quality and\nconfidentiality are critical.", "AI": {"tldr": "This paper introduces a method for generating privacy-preserving high-quality synthetic tabular data using Tensor Networks. The results show that MPS outperforms classical models, particularly under strict privacy constraints.", "motivation": "addressing data scarcity, privacy constraints, and the need for diverse datasets in training robust models.", "method": "generating privacy-preserving high-quality synthetic tabular data using Tensor Networks, specifically Matrix Product States (MPS).", "result": "MPS outperforms classical models, particularly under strict privacy constraints.", "conclusion": "MPS is a promising tool for privacy-aware synthetic data generation, offering an interpretable and scalable alternative for secure data sharing."}}
{"id": "2508.06360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06360", "abs": "https://arxiv.org/abs/2508.06360", "authors": ["Aisha Saeid", "Anu Sabu", "Girish A. Koushik", "Ferrante Neri", "Diptesh Kanojia"], "title": "Cyberbullying Detection via Aggression-Enhanced Prompting", "comment": "Accepted to RANLP 2025", "summary": "Detecting cyberbullying on social media remains a critical challenge due to\nits subtle and varied expressions. This study investigates whether integrating\naggression detection as an auxiliary task within a unified training framework\ncan enhance the generalisation and performance of large language models (LLMs)\nin cyberbullying detection. Experiments are conducted on five aggression\ndatasets and one cyberbullying dataset using instruction-tuned LLMs. We\nevaluated multiple strategies: zero-shot, few-shot, independent LoRA\nfine-tuning, and multi-task learning (MTL). Given the inconsistent results of\nMTL, we propose an enriched prompt pipeline approach in which aggression\npredictions are embedded into cyberbullying detection prompts to provide\ncontextual augmentation. Preliminary results show that the enriched prompt\npipeline consistently outperforms standard LoRA fine-tuning, indicating that\naggression-informed context significantly boosts cyberbullying detection. This\nstudy highlights the potential of auxiliary tasks, such as aggression\ndetection, to improve the generalisation of LLMs for safety-critical\napplications on social networks.", "AI": {"tldr": "This study uses aggression detection as an auxiliary task to improve the performance of LLMs in cyberbullying detection. The enriched prompt pipeline approach consistently outperforms standard LoRA fine-tuning.", "motivation": "Detecting cyberbullying on social media remains a critical challenge due to its subtle and varied expressions. This study investigates whether integrating aggression detection as an auxiliary task within a unified training framework can enhance the generalisation and performance of large language models (LLMs) in cyberbullying detection.", "method": "Experiments are conducted on five aggression datasets and one cyberbullying dataset using instruction-tuned LLMs. We evaluated multiple strategies: zero-shot, few-shot, independent LoRA fine-tuning, and multi-task learning (MTL). Given the inconsistent results of MTL, we propose an enriched prompt pipeline approach in which aggression predictions are embedded into cyberbullying detection prompts to provide contextual augmentation.", "result": "Preliminary results show that the enriched prompt pipeline consistently outperforms standard LoRA fine-tuning, indicating that aggression-informed context significantly boosts cyberbullying detection.", "conclusion": "This study highlights the potential of auxiliary tasks, such as aggression detection, to improve the generalisation of LLMs for safety-critical applications on social networks."}}
{"id": "2508.06051", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06051", "abs": "https://arxiv.org/abs/2508.06051", "authors": ["Linhan Cao", "Wei Sun", "Weixia Zhang", "Xiangyang Zhu", "Jun Jia", "Kaiwei Zhang", "Dandan Zhu", "Guangtao Zhai", "Xiongkuo Min"], "title": "VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning", "comment": null, "summary": "Video quality assessment (VQA) aims to objectively quantify perceptual\nquality degradation in alignment with human visual perception. Despite recent\nadvances, existing VQA models still suffer from two critical limitations:\n\\textit{poor generalization to out-of-distribution (OOD) videos} and\n\\textit{limited explainability}, which restrict their applicability in\nreal-world scenarios. To address these challenges, we propose\n\\textbf{VQAThinker}, a reasoning-based VQA framework that leverages large\nmultimodal models (LMMs) with reinforcement learning to jointly model video\nquality understanding and scoring, emulating human perceptual decision-making.\nSpecifically, we adopt group relative policy optimization (GRPO), a rule-guided\nreinforcement learning algorithm that enables reasoning over video quality\nunder score-level supervision, and introduce three VQA-specific rewards: (1) a\n\\textbf{bell-shaped regression reward} that increases rapidly as the prediction\nerror decreases and becomes progressively less sensitive near the ground truth;\n(2) a \\textbf{pairwise ranking reward} that guides the model to correctly\ndetermine the relative quality between video pairs; and (3) a \\textbf{temporal\nconsistency reward} that encourages the model to prefer temporally coherent\nvideos over their perturbed counterparts. Extensive experiments demonstrate\nthat VQAThinker achieves state-of-the-art performance on both in-domain and OOD\nVQA benchmarks, showing strong generalization for video quality scoring.\nFurthermore, evaluations on video quality understanding tasks validate its\nsuperiority in distortion attribution and quality description compared to\nexisting explainable VQA models and LMMs. These findings demonstrate that\nreinforcement learning offers an effective pathway toward building\ngeneralizable and explainable VQA models solely with score-level supervision.", "AI": {"tldr": "VQAThinker, a reasoning-based VQA framework using LMMs and reinforcement learning, addresses the limitations of existing VQA models by improving generalization and explainability. It achieves state-of-the-art performance on in-domain and OOD VQA benchmarks.", "motivation": "existing VQA models still suffer from two critical limitations: poor generalization to out-of-distribution (OOD) videos and limited explainability, which restrict their applicability in real-world scenarios.", "method": "a reasoning-based VQA framework that leverages large multimodal models (LMMs) with reinforcement learning to jointly model video quality understanding and scoring, emulating human perceptual decision-making. Specifically, we adopt group relative policy optimization (GRPO), a rule-guided reinforcement learning algorithm that enables reasoning over video quality under score-level supervision, and introduce three VQA-specific rewards: (1) a bell-shaped regression reward (2) a pairwise ranking reward (3) a temporal consistency reward", "result": "VQAThinker achieves state-of-the-art performance on both in-domain and OOD VQA benchmarks, showing strong generalization for video quality scoring. Furthermore, evaluations on video quality understanding tasks validate its superiority in distortion attribution and quality description compared to existing explainable VQA models and LMMs.", "conclusion": "VQAThinker achieves state-of-the-art performance on both in-domain and OOD VQA benchmarks, showing strong generalization for video quality scoring. Furthermore, evaluations on video quality understanding tasks validate its superiority in distortion attribution and quality description compared to existing explainable VQA models and LMMs. These findings demonstrate that reinforcement learning offers an effective pathway toward building generalizable and explainable VQA models solely with score-level supervision."}}
{"id": "2508.06257", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06257", "abs": "https://arxiv.org/abs/2508.06257", "authors": ["Jielong Lu", "Zhihao Wu", "Jiajun Yu", "Jiajun Bu", "Haishuai Wang"], "title": "Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph Smoothness Priors", "comment": null, "summary": "Integrating multi-omics datasets through data-driven analysis offers a\ncomprehensive understanding of the complex biological processes underlying\nvarious diseases, particularly cancer. Graph Neural Networks (GNNs) have\nrecently demonstrated remarkable ability to exploit relational structures in\nbiological data, enabling advances in multi-omics integration for cancer\nsubtype classification. Existing approaches often neglect the intricate\ncoupling between heterogeneous omics, limiting their capacity to resolve subtle\ncancer subtype heterogeneity critical for precision oncology. To address these\nlimitations, we propose a framework named Graph Transformer for Multi-omics\nCancer Subtype Classification (GTMancer). This framework builds upon the GNN\noptimization problem and extends its application to complex multi-omics data.\nSpecifically, our method leverages contrastive learning to embed multi-omics\ndata into a unified semantic space. We unroll the multiplex graph optimization\nproblem in that unified space and introduce dual sets of attention coefficients\nto capture structural graph priors both within and among multi-omics data. This\napproach enables global omics information to guide the refining of the\nrepresentations of individual omics. Empirical experiments on seven real-world\ncancer datasets demonstrate that GTMancer outperforms existing state-of-the-art\nalgorithms.", "AI": {"tldr": "GTMancer\u4f7f\u7528\u56fe\u8f6c\u6362\u5668\u8fdb\u884c\u591a\u7ec4\u5b66\u764c\u75c7\u4e9a\u578b\u5206\u7c7b\uff0c\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u5d4c\u5165\u591a\u7ec4\u5b66\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u53cc\u91cd\u6ce8\u610f\u529b\u673a\u5236\u6355\u83b7\u7ec4\u5b66\u5185\u90e8\u548c\u4e4b\u95f4\u7684\u7ed3\u6784\u56fe\u5148\u9a8c\uff0c\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u4e86\u5f02\u6784\u7ec4\u5b66\u4e4b\u95f4\u590d\u6742\u7684\u8026\u5408\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u89e3\u51b3\u5bf9\u7cbe\u786e\u80bf\u7624\u5b66\u81f3\u5173\u91cd\u8981\u7684\u7ec6\u5fae\u764c\u75c7\u4e9a\u578b\u5f02\u8d28\u6027\u7684\u80fd\u529b\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u5c06\u591a\u7ec4\u5b66\u6570\u636e\u5d4c\u5165\u5230\u7edf\u4e00\u7684\u8bed\u4e49\u7a7a\u95f4\u4e2d\u3002\u6211\u4eec\u5728\u8fd9\u4e2a\u7edf\u4e00\u7684\u7a7a\u95f4\u4e2d\u5c55\u5f00\u591a\u8def\u56fe\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u53cc\u91cd\u6ce8\u610f\u7cfb\u6570\u96c6\uff0c\u4ee5\u6355\u83b7\u591a\u7ec4\u5b66\u6570\u636e\u5185\u90e8\u548c\u4e4b\u95f4\u7ed3\u6784\u56fe\u5148\u9a8c\u3002", "result": "GTMancer\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7b97\u6cd5\u3002", "conclusion": "GTMancer\u5728\u4e03\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u764c\u75c7\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7b97\u6cd5\u3002"}}
{"id": "2508.06374", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06374", "abs": "https://arxiv.org/abs/2508.06374", "authors": ["Anubhav Jangra", "Bahareh Sarrafzadeh", "Adrian de Wynter", "Silviu Cucerzan", "Sujay Kumar Jauhar"], "title": "Evaluating Style-Personalized Text Generation: Challenges and Directions", "comment": null, "summary": "While prior research has built tools and benchmarks towards style\npersonalized text generation, there has been limited exploration of evaluation\nin low-resource author style personalized text generation space. Through this\nwork, we question the effectiveness of the widely adopted evaluation metrics\nlike BLEU and ROUGE, and explore other evaluation paradigms such as style\nembeddings and LLM-as-judge to holistically evaluate the style personalized\ntext generation task. We evaluate these metrics and their ensembles using our\nstyle discrimination benchmark, that spans eight writing tasks, and evaluates\nacross three settings, domain discrimination, authorship attribution, and LLM\npersonalized vs non-personalized discrimination. We provide conclusive evidence\nto adopt ensemble of diverse evaluation metrics to effectively evaluate style\npersonalized text generation.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u8c03\u67e5\u4e86\u4f4e\u8d44\u6e90\u98ce\u683c\u4e2a\u6027\u5316\u6587\u672c\u751f\u6210\u4e2d\u7684\u8bc4\u4f30\u95ee\u9898\uff0c\u53d1\u73b0\u50cf BLEU \u548c ROUGE \u8fd9\u6837\u7684\u5e38\u7528\u6307\u6807\u6548\u679c\u4e0d\u4f73\u3002\u7814\u7a76\u5efa\u8bae\u4f7f\u7528\u66f4\u591a\u6837\u5316\u7684\u6307\u6807\u7ec4\u5408\u6765\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u5148\u524d\u5173\u4e8e\u98ce\u683c\u4e2a\u6027\u5316\u6587\u672c\u751f\u6210\u7684\u7814\u7a76\u5df2\u7ecf\u6784\u5efa\u4e86\u5de5\u5177\u548c\u57fa\u51c6\uff0c\u4f46\u5728\u4f4e\u8d44\u6e90\u4f5c\u8005\u98ce\u683c\u4e2a\u6027\u5316\u6587\u672c\u751f\u6210\u9886\u57df\u7684\u8bc4\u4f30\u63a2\u7d22\u6709\u9650\u3002\u8fd9\u9879\u5de5\u4f5c\u8d28\u7591\u4e86\u5e7f\u6cdb\u91c7\u7528\u7684\u8bc4\u4f30\u6307\u6807\uff08\u5982 BLEU \u548c ROUGE\uff09\u7684\u6709\u6548\u6027\uff0c\u5e76\u63a2\u7d22\u4e86\u5176\u4ed6\u8bc4\u4f30\u8303\u5f0f\uff0c\u5982\u98ce\u683c\u5d4c\u5165\u548c LLM-as-judge\uff0c\u4ee5\u5168\u9762\u8bc4\u4f30\u98ce\u683c\u4e2a\u6027\u5316\u6587\u672c\u751f\u6210\u4efb\u52a1\u3002", "method": "\u901a\u8fc7\u98ce\u683c\u5224\u522b\u57fa\u51c6\u8bc4\u4f30\u5404\u79cd\u6307\u6807\u53ca\u5176\u96c6\u6210\uff0c\u8be5\u57fa\u51c6\u6db5\u76d6\u516b\u4e2a\u5199\u4f5c\u4efb\u52a1\uff0c\u5e76\u5728\u4e09\u4e2a\u8bbe\u7f6e\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff1a\u9886\u57df\u5224\u522b\u3001\u4f5c\u8005\u5f52\u5c5e\u548c LLM \u4e2a\u6027\u5316\u4e0e\u975e\u4e2a\u6027\u5316\u5224\u522b\u3002", "result": "\u6211\u4eec\u63d0\u4f9b\u4e86\u786e\u51ff\u7684\u8bc1\u636e\uff0c\u8868\u660e\u5e94\u8be5\u91c7\u7528\u591a\u6837\u5316\u8bc4\u4f30\u6307\u6807\u7684\u96c6\u6210\u6765\u6709\u6548\u8bc4\u4f30\u98ce\u683c\u4e2a\u6027\u5316\u6587\u672c\u751f\u6210\u3002", "conclusion": "\u5e94\u8be5\u91c7\u7528\u591a\u6837\u5316\u8bc4\u4f30\u6307\u6807\u7684\u96c6\u6210\u6765\u6709\u6548\u8bc4\u4f30\u98ce\u683c\u4e2a\u6027\u5316\u6587\u672c\u751f\u6210\u3002"}}
{"id": "2508.06055", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.06055", "abs": "https://arxiv.org/abs/2508.06055", "authors": ["Wonjung Park", "Suhyun Ahn", "Jinah Park"], "title": "LV-Net: Anatomy-aware lateral ventricle shape modeling with a case study on Alzheimer's disease, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing", "comment": null, "summary": "Lateral ventricle (LV) shape analysis holds promise as a biomarker for\nneurological diseases; however, challenges remain due to substantial shape\nvariability across individuals and segmentation difficulties arising from\nlimited MRI resolution. We introduce LV-Net, a novel framework for producing\nindividualized 3D LV meshes from brain MRI by deforming an anatomy-aware joint\nLV-hippocampus template mesh. By incorporating anatomical relationships\nembedded within the joint template, LV-Net reduces boundary segmentation\nartifacts and improves reconstruction robustness. In addition, by classifying\nthe vertices of the template mesh based on their anatomical adjacency, our\nmethod enhances point correspondence across subjects, leading to more accurate\nLV shape statistics. We demonstrate that LV-Net achieves superior\nreconstruction accuracy, even in the presence of segmentation imperfections,\nand delivers more reliable shape descriptors across diverse datasets. Finally,\nwe apply LV-Net to Alzheimer's disease analysis, identifying LV subregions that\nshow significantly associations with the disease relative to cognitively normal\ncontrols. The codes for LV shape modeling are available at\nhttps://github.com/PWonjung/LV_Shape_Modeling.", "AI": {"tldr": "LV-Net\u662f\u4e00\u79cd\u7528\u4e8e\u4ece\u8111\u90e8MRI\u751f\u6210\u4e2a\u4f53\u53163D\u8111\u5ba4\u7f51\u683c\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u63d0\u9ad8\u4e86\u91cd\u5efa\u7cbe\u5ea6\u548c\u5f62\u72b6\u63cf\u8ff0\u7b26\u7684\u53ef\u9760\u6027\uff0c\u5e76\u53ef\u4ee5\u8bc6\u522b\u4e0e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u76f8\u5173\u7684\u8111\u5ba4\u4e9a\u533a\u3002", "motivation": "\u8111\u5ba4\u5f62\u72b6\u5206\u6790\u6709\u671b\u6210\u4e3a\u795e\u7ecf\u75be\u75c5\u7684\u751f\u7269\u6807\u5fd7\u7269\uff1b\u7136\u800c\uff0c\u7531\u4e8e\u4e2a\u4f53\u95f4\u7684\u5f62\u72b6\u5dee\u5f02\u5f88\u5927\uff0c\u4ee5\u53caMRI\u5206\u8fa8\u7387\u6709\u9650\u5bfc\u81f4\u5206\u5272\u56f0\u96be\uff0c\u4ecd\u7136\u5b58\u5728\u6311\u6218\u3002", "method": "LV-Net\u901a\u8fc7\u53d8\u5f62\u4e00\u4e2a\u89e3\u5256\u5b66\u611f\u77e5\u7684\u8054\u5408\u8111\u5ba4-\u6d77\u9a6c\u6a21\u677f\u7f51\u683c\uff0c\u4ece\u8111\u90e8MRI\u751f\u6210\u4e2a\u4f53\u5316\u76843D\u8111\u5ba4\u7f51\u683c\u3002", "result": "LV-Net\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u91cd\u5efa\u7cbe\u5ea6\uff0c\u5373\u4f7f\u5728\u5b58\u5728\u5206\u5272\u7f3a\u9677\u7684\u60c5\u51b5\u4e0b\uff0c\u5e76\u5728\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e2d\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5f62\u72b6\u63cf\u8ff0\u7b26\u3002", "conclusion": "LV-Net\u53ef\u4ee5\u8bc6\u522b\u4e0e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u663e\u8457\u76f8\u5173\u7684\u8111\u5ba4\u4e9a\u533a\u3002"}}
{"id": "2508.06269", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06269", "abs": "https://arxiv.org/abs/2508.06269", "authors": ["Zhuoran Li", "Xun Wang", "Hai Zhong", "Longbo Huang"], "title": "OM2P: Offline Multi-Agent Mean-Flow Policy", "comment": null, "summary": "Generative models, especially diffusion and flow-based models, have been\npromising in offline multi-agent reinforcement learning. However, integrating\npowerful generative models into this framework poses unique challenges. In\nparticular, diffusion and flow-based policies suffer from low sampling\nefficiency due to their iterative generation processes, making them impractical\nin time-sensitive or resource-constrained settings. To tackle these\ndifficulties, we propose OM2P (Offline Multi-Agent Mean-Flow Policy), a novel\noffline MARL algorithm to achieve efficient one-step action sampling. To\naddress the misalignment between generative objectives and reward maximization,\nwe introduce a reward-aware optimization scheme that integrates a\ncarefully-designed mean-flow matching loss with Q-function supervision.\nAdditionally, we design a generalized timestep distribution and a\nderivative-free estimation strategy to reduce memory overhead and improve\ntraining stability. Empirical evaluations on Multi-Agent Particle and MuJoCo\nbenchmarks demonstrate that OM2P achieves superior performance, with up to a\n3.8x reduction in GPU memory usage and up to a 10.8x speed-up in training time.\nOur approach represents the first to successfully integrate mean-flow model\ninto offline MARL, paving the way for practical and scalable generative\npolicies in cooperative multi-agent settings.", "AI": {"tldr": "OM2P\u662f\u4e00\u79cd\u65b0\u7684\u79bb\u7ebfMARL\u7b97\u6cd5\uff0c\u5b83\u901a\u8fc7\u5355\u6b65\u52a8\u4f5c\u91c7\u6837\u3001\u5956\u52b1\u611f\u77e5\u4f18\u5316\u548c\u51cf\u5c11\u5185\u5b58\u5f00\u9500\u6765\u63d0\u9ad8\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u548c\u57fa\u4e8e\u6d41\u7684\u6a21\u578b\u5728\u79bb\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u5f88\u6709\u524d\u666f\u3002\u7136\u800c\uff0c\u5c06\u5f3a\u5927\u7684\u751f\u6210\u6a21\u578b\u96c6\u6210\u5230\u8fd9\u4e2a\u6846\u67b6\u4e2d\u5e26\u6765\u4e86\u72ec\u7279\u7684\u6311\u6218\u3002\u7279\u522b\u662f\uff0c\u7531\u4e8e\u5176\u8fed\u4ee3\u751f\u6210\u8fc7\u7a0b\uff0c\u6269\u6563\u548c\u57fa\u4e8e\u6d41\u7684\u7b56\u7565\u7684\u91c7\u6837\u6548\u7387\u8f83\u4f4e\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u5728\u65f6\u95f4\u654f\u611f\u6216\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u79bb\u7ebfMARL\u7b97\u6cd5OM2P\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u5355\u6b65\u52a8\u4f5c\u91c7\u6837\u3002\u4e3a\u4e86\u89e3\u51b3\u751f\u6210\u76ee\u6807\u548c\u5956\u52b1\u6700\u5927\u5316\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u5956\u52b1\u611f\u77e5\u4f18\u5316\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u5c06\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5e73\u5747\u6d41\u5339\u914d\u635f\u5931\u4e0eQ\u51fd\u6570\u76d1\u7763\u76f8\u7ed3\u5408\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5e7f\u4e49\u7684\u65f6\u95f4\u6b65\u957f\u5206\u5e03\u548c\u4e00\u79cd\u65e0\u5bfc\u6570\u4f30\u8ba1\u7b56\u7565\uff0c\u4ee5\u51cf\u5c11\u5185\u5b58\u5f00\u9500\u5e76\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u591a\u667a\u80fd\u4f53\u7c92\u5b50\u548cMuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u7ecf\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cOM2P\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0cGPU\u5185\u5b58\u4f7f\u7528\u91cf\u51cf\u5c11\u4e863.8\u500d\uff0c\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed\u4e8610.8\u500d\u3002", "conclusion": "OM2P\u6210\u529f\u5730\u5c06\u5e73\u5747\u6d41\u6a21\u578b\u96c6\u6210\u5230\u79bb\u7ebfMARL\u4e2d\uff0c\u4e3a\u5408\u4f5c\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u751f\u6210\u7b56\u7565\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.06388", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06388", "abs": "https://arxiv.org/abs/2508.06388", "authors": ["Lanlan Qiu", "Xiao Pu", "Yeqi Feng", "Tianxing He"], "title": "LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing", "comment": "21 pages, 17 figures, 3 tables", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nrole-playing conversations and providing emotional support as separate research\ndirections. However, there remains a significant research gap in combining\nthese capabilities to enable emotionally supportive interactions with virtual\ncharacters. To address this research gap, we focus on anime characters as a\ncase study because of their well-defined personalities and large fan bases.\nThis choice enables us to effectively evaluate how well LLMs can provide\nemotional support while maintaining specific character traits. We introduce\nChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We\nfirst thoughtfully select 20 top-tier characters from popular anime communities\nand design 60 emotion-centric real-world scenario questions. Then, we execute a\nnationwide selection process to identify 40 Chinese anime enthusiasts with\nprofound knowledge of specific characters and extensive experience in\nrole-playing. Next, we systematically collect two rounds of dialogue data from\n10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP\nperformance of LLMs, we design a user experience-oriented evaluation system\nfeaturing 9 fine-grained metrics across three dimensions: basic dialogue,\nrole-playing and emotional support, along with an overall metric for response\ndiversity. In total, the dataset comprises 2,400 human-written and 24,000\nLLM-generated answers, supported by over 132,000 human annotations.\nExperimental results show that top-performing LLMs surpass human fans in\nrole-playing and emotional support, while humans still lead in response\ndiversity. We hope this work can provide valuable resources and insights for\nfuture research on optimizing LLMs in ESRP. Our datasets are available at\nhttps://github.com/LanlanQiu/ChatAnime.", "AI": {"tldr": "This paper introduces ChatAnime, a dataset for Emotionally Supportive Role-Playing (ESRP) with anime characters, and finds that LLMs can surpass humans in role-playing and emotional support.", "motivation": "there remains a significant research gap in combining role-playing conversations and providing emotional support to enable emotionally supportive interactions with virtual characters", "method": "introducing ChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset,collecting dialogue data from 10 LLMs and 40 Chinese anime enthusiasts,designing a user experience-oriented evaluation system featuring 9 fine-grained metrics across three dimensions", "result": "the dataset comprises 2,400 human-written and 24,000 LLM-generated answers, supported by over 132,000 human annotations", "conclusion": "top-performing LLMs surpass human fans in role-playing and emotional support, while humans still lead in response diversity"}}
{"id": "2508.06057", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06057", "abs": "https://arxiv.org/abs/2508.06057", "authors": ["Mojtaba Valipour", "Kelly Zheng", "James Lowman", "Spencer Szabados", "Mike Gartner", "Bobby Braswell"], "title": "AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?", "comment": "Accepted in IGARSS 2025!", "summary": "Artificial General Intelligence (AGI) is closer than ever to becoming a\nreality, sparking widespread enthusiasm in the research community to collect\nand work with various modalities, including text, image, video, and audio.\nDespite recent efforts, satellite spectral imagery, as an additional modality,\nhas yet to receive the attention it deserves. This area presents unique\nchallenges, but also holds great promise in advancing the capabilities of AGI\nin understanding the natural world. In this paper, we argue why Earth\nObservation data is useful for an intelligent model, and then we review\nexisting benchmarks and highlight their limitations in evaluating the\ngeneralization ability of foundation models in this domain. This paper\nemphasizes the need for a more comprehensive benchmark to evaluate earth\nobservation models. To facilitate this, we propose a comprehensive set of tasks\nthat a benchmark should encompass to effectively assess a model's ability to\nunderstand and interact with Earth observation data.", "AI": {"tldr": "This paper proposes a comprehensive benchmark to evaluate earth observation models.", "motivation": "Earth Observation data is useful for an intelligent model", "method": "review existing benchmarks and highlight their limitations", "result": "propose a comprehensive set of tasks that a benchmark should encompass to effectively assess a model's ability to understand and interact with Earth observation data.", "conclusion": "This paper emphasizes the need for a more comprehensive benchmark to evaluate earth observation models."}}
{"id": "2508.06280", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06280", "abs": "https://arxiv.org/abs/2508.06280", "authors": ["Gokul Adethya T", "S. Jaya Nirmala"], "title": "A Study on Regularization-Based Continual Learning Methods for Indic ASR", "comment": null, "summary": "Indias linguistic diversity poses significant challenges for developing\ninclusive Automatic Speech Recognition (ASR) systems. Traditional multilingual\nmodels, which require simultaneous access to all language data, are impractical\ndue to the sequential arrival of data and privacy constraints. Continual\nLearning (CL) offers a solution by enabling models to learn new languages\nsequentially without catastrophically forgetting previously learned knowledge.\nThis paper investigates CL for ASR on Indian languages using a subset of the\nIndicSUPERB benchmark. We employ a Conformer-based hybrid RNN-T/CTC model,\ninitially pretrained on Hindi, which is then incrementally trained on eight\nadditional Indian languages, for a total sequence of nine languages. We\nevaluate three prominent regularization- and distillation-based CL strategies:\nElastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning\nwithout Forgetting (LwF), selected for their suitability in no-replay,\nprivacy-conscious scenarios. Performance is analyzed using Word Error Rate\n(WER) for both RNN-T and CTC paths on clean and noisy data, as well as\nknowledge retention via Backward Transfer. We also explore the impact of\nvarying the number of training epochs (1, 2, 5, and 10) per task. Results,\ncompared against naive fine-tuning, demonstrate CLs effectiveness in mitigating\nforgetting, making it a promising approach for scalable ASR in diverse Indian\nlanguages under realistic constraints. The code is available at:\nhttps://github.com/FrozenWolf-Cyber/Indic-CL-ASR", "AI": {"tldr": "This paper investigates Continual Learning (CL) for ASR on Indian languages using a subset of the IndicSUPERB benchmark. The results demonstrate CL's effectiveness in mitigating forgetting, making it a promising approach for scalable ASR in diverse Indian languages under realistic constraints.", "motivation": "India\u2019s linguistic diversity poses significant challenges for developing inclusive Automatic Speech Recognition (ASR) systems. Traditional multilingual models, which require simultaneous access to all language data, are impractical due to the sequential arrival of data and privacy constraints. Continual Learning (CL) offers a solution by enabling models to learn new languages sequentially without catastrophically forgetting previously learned knowledge.", "method": "Conformer-based hybrid RNN-T/CTC model, initially pretrained on Hindi, which is then incrementally trained on eight additional Indian languages, for a total sequence of nine languages. We evaluate three prominent regularization- and distillation-based CL strategies: Elastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning without Forgetting (LwF)", "result": "Performance is analyzed using Word Error Rate (WER) for both RNN-T and CTC paths on clean and noisy data, as well as knowledge retention via Backward Transfer. We also explore the impact of varying the number of training epochs (1, 2, 5, and 10) per task. Results, compared against naive fine-tuning, demonstrate CL\u2019s effectiveness in mitigating forgetting", "conclusion": "CL is effective in mitigating forgetting, making it a promising approach for scalable ASR in diverse Indian languages under realistic constraints."}}
{"id": "2508.06418", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06418", "abs": "https://arxiv.org/abs/2508.06418", "authors": ["Haoran Shi", "Hongwei Yao", "Shuo Shao", "Shaopeng Jiao", "Ziqi Peng", "Zhan Qin", "Cong Wang"], "title": "Quantifying Conversation Drift in MCP via Latent Polytope", "comment": null, "summary": "The Model Context Protocol (MCP) enhances large language models (LLMs) by\nintegrating external tools, enabling dynamic aggregation of real-time data to\nimprove task execution. However, its non-isolated execution context introduces\ncritical security and privacy risks. In particular, adversarially crafted\ncontent can induce tool poisoning or indirect prompt injection, leading to\nconversation hijacking, misinformation propagation, or data exfiltration.\nExisting defenses, such as rule-based filters or LLM-driven detection, remain\ninadequate due to their reliance on static signatures, computational\ninefficiency, and inability to quantify conversational hijacking. To address\nthese limitations, we propose SecMCP, a secure framework that detects and\nquantifies conversation drift, deviations in latent space trajectories induced\nby adversarial external knowledge. By modeling LLM activation vectors within a\nlatent polytope space, SecMCP identifies anomalous shifts in conversational\ndynamics, enabling proactive detection of hijacking, misleading, and data\nexfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3,\nVicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA),\ndemonstrating robust detection with AUROC scores exceeding 0.915 while\nmaintaining system usability. Our contributions include a systematic\ncategorization of MCP security threats, a novel latent polytope-based\nmethodology for quantifying conversation drift, and empirical validation of\nSecMCP's efficacy.", "AI": {"tldr": "MCP enhances LLMs but has security risks. SecMCP uses a latent polytope-based method to detect conversation drift and prevent attacks, achieving high detection accuracy.", "motivation": "The Model Context Protocol (MCP) enhances LLMs but introduces security and privacy risks due to its non-isolated execution context. Existing defenses are inadequate.", "method": "A novel latent polytope-based methodology is used to quantify conversation drift by modeling LLM activation vectors within a latent polytope space.", "result": "SecMCP achieves robust detection with AUROC scores exceeding 0.915 on three state-of-the-art LLMs (Llama3, Vicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA).", "conclusion": "SecMCP, a secure framework, effectively detects conversation drift by modeling LLM activation vectors within a latent polytope space, enabling proactive detection of hijacking, misleading, and data exfiltration. It demonstrates robust detection with high AUROC scores while maintaining system usability."}}
{"id": "2508.06058", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06058", "abs": "https://arxiv.org/abs/2508.06058", "authors": ["Shiyang Zhou", "Haijin Zeng", "Yunfan Lu", "Yongyong Chen", "Jie Liu", "Jingyong Su"], "title": "Lightweight Quad Bayer HybridEVS Demosaicing via State Space Augmented Cross-Attention", "comment": null, "summary": "Event cameras like the Hybrid Event-based Vision Sensor (HybridEVS) camera\ncapture brightness changes as asynchronous \"events\" instead of frames, offering\nadvanced application on mobile photography. However, challenges arise from\ncombining a Quad Bayer Color Filter Array (CFA) sensor with event pixels\nlacking color information, resulting in aliasing and artifacts on the\ndemosaicing process before downstream application. Current methods struggle to\naddress these issues, especially on resource-limited mobile devices. In\nresponse, we introduce \\textbf{TSANet}, a lightweight \\textbf{T}wo-stage\nnetwork via \\textbf{S}tate space augmented cross-\\textbf{A}ttention, which can\nhandle event pixels inpainting and demosaicing separately, leveraging the\nbenefits of dividing complex tasks into manageable subtasks. Furthermore, we\nintroduce a lightweight Cross-Swin State Block that uniquely utilizes\npositional prior for demosaicing and enhances global dependencies through the\nstate space model with linear complexity. In summary, TSANet demonstrates\nexcellent demosaicing performance on both simulated and real data of HybridEVS\nwhile maintaining a lightweight model, averaging better results than the\nprevious state-of-the-art method DemosaicFormer across seven diverse datasets\nin both PSNR and SSIM, while respectively reducing parameter and computation\ncosts by $1.86\\times$ and $3.29\\times$. Our approach presents new possibilities\nfor efficient image demosaicing on mobile devices. Code is available in the\nsupplementary materials.", "AI": {"tldr": "TSANet is a lightweight network for demosaicing HybridEVS data that outperforms existing methods with lower computational cost, making it suitable for mobile devices.", "motivation": "Combining a Quad Bayer Color Filter Array (CFA) sensor with event pixels lacking color information in HybridEVS cameras leads to aliasing and artifacts during demosaicing, which current methods struggle to address, especially on mobile devices.", "method": "A lightweight two-stage network (TSANet) with a Cross-Swin State Block that utilizes positional prior and enhances global dependencies through a state space model.", "result": "TSANet outperforms DemosaicFormer on seven datasets in PSNR and SSIM, while reducing parameter and computation costs by 1.86x and 3.29x, respectively.", "conclusion": "TSANet achieves better demosaicing performance than DemosaicFormer on HybridEVS data with lower parameter and computation costs, opening new possibilities for efficient demosaicing on mobile devices."}}
{"id": "2508.06292", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06292", "abs": "https://arxiv.org/abs/2508.06292", "authors": ["Sanja Karilanova", "Subhrakanti Dey", "Ay\u00e7a \u00d6z\u00e7elikkale"], "title": "Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback", "comment": "15 pages, 7 Tables, 6 Figures", "summary": "Neuromorphic computing is an emerging technology enabling low-latency and\nenergy-efficient signal processing. A key algorithmic tool in neuromorphic\ncomputing is spiking neural networks (SNNs). SNNs are biologically inspired\nneural networks which utilize stateful neurons, and provide low-bit data\nprocessing by encoding and decoding information using spikes. Similar to SNNs,\ndeep state-space models (SSMs) utilize stateful building blocks. However, deep\nSSMs, which recently achieved competitive performance in various temporal\nmodeling tasks, are typically designed with high-precision activation functions\nand no reset mechanisms. To bridge the gains offered by SNNs and the recent\ndeep SSM models, we propose a novel multiple-output spiking neuron model that\ncombines a linear, general SSM state transition with a non-linear feedback\nmechanism through reset. Compared to the existing neuron models for SNNs, our\nproposed model clearly conceptualizes the differences between the spiking\nfunction, the reset condition and the reset action. The experimental results on\nvarious tasks, i.e., a keyword spotting task, an event-based vision task and a\nsequential pattern recognition task, show that our proposed model achieves\nperformance comparable to existing benchmarks in the SNN literature. Our\nresults illustrate how the proposed reset mechanism can overcome instability\nand enable learning even when the linear part of neuron dynamics is unstable,\nallowing us to go beyond the strictly enforced stability of linear dynamics in\nrecent deep SSM models.", "AI": {"tldr": "This paper proposes a novel spiking neuron model that combines a linear SSM state transition with a non-linear feedback mechanism through reset, achieving comparable performance to existing benchmarks in SNNs.", "motivation": "To bridge the gains offered by SNNs and the recent deep SSM models.", "method": "We propose a novel multiple-output spiking neuron model that combines a linear, general SSM state transition with a non-linear feedback mechanism through reset.", "result": "The experimental results on various tasks, i.e., a keyword spotting task, an event-based vision task and a sequential pattern recognition task, show that our proposed model achieves performance comparable to existing benchmarks in the SNN literature.", "conclusion": "The proposed model achieves performance comparable to existing benchmarks in the SNN literature and can overcome instability and enable learning even when the linear part of neuron dynamics is unstable."}}
{"id": "2508.06433", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06433", "abs": "https://arxiv.org/abs/2508.06433", "authors": ["Runnan Fang", "Yuan Liang", "Xiaobin Wang", "Jialong Wu", "Shuofei Qiao", "Pengjun Xie", "Fei Huang", "Huajun Chen", "Ningyu Zhang"], "title": "Memp: Exploring Agent Procedural Memory", "comment": "Work in progress", "summary": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they\nsuffer from brittle procedural memory that is manually engineered or entangled\nin static parameters. In this work, we investigate strategies to endow agents\nwith a learnable, updatable, and lifelong procedural memory. We propose Memp\nthat distills past agent trajectories into both fine-grained, step-by-step\ninstructions and higher-level, script-like abstractions, and explore the impact\nof different strategies for Build, Retrieval, and Update of procedural memory.\nCoupled with a dynamic regimen that continuously updates, corrects, and\ndeprecates its contents, this repository evolves in lockstep with new\nexperience. Empirical evaluation on TravelPlanner and ALFWorld shows that as\nthe memory repository is refined, agents achieve steadily higher success rates\nand greater efficiency on analogous tasks. Moreover, procedural memory built\nfrom a stronger model retains its value: migrating the procedural memory to a\nweaker model yields substantial performance gains.", "AI": {"tldr": "This paper introduces Memp, a learnable and updatable procedural memory for LLM agents that improves performance and efficiency on tasks and can be migrated to weaker models.", "motivation": "LLMs based agents excel at diverse tasks, yet they suffer from brittle procedural memory that is manually engineered or entangled in static parameters.  This work investigates strategies to endow agents with a learnable, updatable, and lifelong procedural memory.", "method": "Memp distills past agent trajectories into both fine-grained, step-by-step instructions and higher-level, script-like abstractions, and explore the impact of different strategies for Build, Retrieval, and Update of procedural memory.  Coupled with a dynamic regimen that continuously updates, corrects, and deprecates its contents, this repository evolves in lockstep with new experience.", "result": "Empirical evaluation on TravelPlanner and ALFWorld shows that as the memory repository is refined, agents achieve steadily higher success rates and greater efficiency on analogous tasks. Moreover, procedural memory built from a stronger model retains its value: migrating the procedural memory to a weaker model yields substantial performance gains.", "conclusion": "Agents achieve steadily higher success rates and greater efficiency on analogous tasks as the memory repository is refined. Migrating the procedural memory to a weaker model yields substantial performance gains."}}
{"id": "2508.06063", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06063", "abs": "https://arxiv.org/abs/2508.06063", "authors": ["Chao Hao", "Zitong Yu", "Xin Liu", "Yuhao Wang", "Weicheng Xie", "Jingang Shi", "Huanjing Yue", "Jingyu Yang"], "title": "Distribution-Specific Learning for Joint Salient and Camouflaged Object Detection", "comment": null, "summary": "Salient object detection (SOD) and camouflaged object detection (COD) are two\nclosely related but distinct computer vision tasks. Although both are\nclass-agnostic segmentation tasks that map from RGB space to binary space, the\nformer aims to identify the most salient objects in the image, while the latter\nfocuses on detecting perfectly camouflaged objects that blend into the\nbackground in the image. These two tasks exhibit strong contradictory\nattributes. Previous works have mostly believed that joint learning of these\ntwo tasks would confuse the network, reducing its performance on both tasks.\nHowever, here we present an opposite perspective: with the correct approach to\nlearning, the network can simultaneously possess the capability to find both\nsalient and camouflaged objects, allowing both tasks to benefit from joint\nlearning. We propose SCJoint, a joint learning scheme for SOD and COD tasks,\nassuming that the decoding processes of SOD and COD have different distribution\ncharacteristics. The key to our method is to learn the respective means and\nvariances of the decoding processes for both tasks by inserting a minimal\namount of task-specific learnable parameters within a fully shared network\nstructure, thereby decoupling the contradictory attributes of the two tasks at\na minimal cost. Furthermore, we propose a saliency-based sampling strategy\n(SBSS) to sample the training set of the SOD task to balance the training set\nsizes of the two tasks. In addition, SBSS improves the training set quality and\nshortens the training time. Based on the proposed SCJoint and SBSS, we train a\npowerful generalist network, named JoNet, which has the ability to\nsimultaneously capture both ``salient\" and ``camouflaged\". Extensive\nexperiments demonstrate the competitive performance and effectiveness of our\nproposed method. The code is available at https://github.com/linuxsino/JoNet.", "AI": {"tldr": "This paper introduces SCJoint, a joint learning scheme for Salient object detection (SOD) and camouflaged object detection (COD) tasks. The network can simultaneously possess the capability to find both salient and camouflaged objects, allowing both tasks to benefit from joint learning. Furthermore, the paper proposes a saliency-based sampling strategy to sample the training set of the SOD task to balance the training set sizes of the two tasks. The experiments demonstrate the competitive performance and effectiveness of the proposed method.", "motivation": "Previous works have mostly believed that joint learning of these two tasks would confuse the network, reducing its performance on both tasks. However, this paper presents an opposite perspective: with the correct approach to learning, the network can simultaneously possess the capability to find both salient and camouflaged objects, allowing both tasks to benefit from joint learning.", "method": "The key to the method is to learn the respective means and variances of the decoding processes for both tasks by inserting a minimal amount of task-specific learnable parameters within a fully shared network structure, thereby decoupling the contradictory attributes of the two tasks at a minimal cost. Furthermore, a saliency-based sampling strategy (SBSS) is proposed to sample the training set of the SOD task to balance the training set sizes of the two tasks. Based on the proposed SCJoint and SBSS, a powerful generalist network, named JoNet, is trained.", "result": "A joint learning scheme for SOD and COD tasks, assuming that the decoding processes of SOD and COD have different distribution characteristics. Propose SCJoint, a joint learning scheme for SOD and COD tasks.", "conclusion": "The experiments demonstrate the competitive performance and effectiveness of the proposed method."}}
{"id": "2508.06301", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.06301", "abs": "https://arxiv.org/abs/2508.06301", "authors": ["Junhyeog Yun", "Minui Hong", "Gunhee Kim"], "title": "FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields", "comment": "ICCV 2025", "summary": "Neural fields provide a memory-efficient representation of data, which can\neffectively handle diverse modalities and large-scale data. However, learning\nto map neural fields often requires large amounts of training data and\ncomputations, which can be limited to resource-constrained edge devices. One\napproach to tackle this limitation is to leverage Federated Meta-Learning\n(FML), but traditional FML approaches suffer from privacy leakage. To address\nthese issues, we introduce a novel FML approach called FedMeNF. FedMeNF\nutilizes a new privacy-preserving loss function that regulates privacy leakage\nin the local meta-optimization. This enables the local meta-learner to optimize\nquickly and efficiently without retaining the client's private data. Our\nexperiments demonstrate that FedMeNF achieves fast optimization speed and\nrobust reconstruction performance, even with few-shot or non-IID data across\ndiverse data modalities, while preserving client data privacy.", "AI": {"tldr": "Introduces FedMeNF, a federated meta-learning approach for neural fields that preserves privacy and achieves fast optimization with limited data.", "motivation": "learning to map neural fields often requires large amounts of training data and computations, which can be limited to resource-constrained edge devices. One approach to tackle this limitation is to leverage Federated Meta-Learning (FML), but traditional FML approaches suffer from privacy leakage", "method": "a novel FML approach called FedMeNF utilizes a new privacy-preserving loss function that regulates privacy leakage in the local meta-optimization", "result": "achieves fast optimization speed and robust reconstruction performance, even with few-shot or non-IID data across diverse data modalities, while preserving client data privacy", "conclusion": "FedMeNF achieves fast optimization speed and robust reconstruction performance, even with few-shot or non-IID data across diverse data modalities, while preserving client data privacy."}}
{"id": "2508.06435", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06435", "abs": "https://arxiv.org/abs/2508.06435", "authors": ["Andrea Nasuto", "Stefano Maria Iacus", "Francisco Rowe", "Devika Jain"], "title": "Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages", "comment": null, "summary": "Large language models (LLMs) are transforming social-science research by\nenabling scalable, precise analysis. Their adaptability raises the question of\nwhether knowledge acquired through fine-tuning in a few languages can transfer\nto unseen languages that only appeared during pre-training. To examine this, we\nfine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or\nmultilingual data sets to classify immigration-related tweets from X/Twitter\nacross 13 languages, a domain characterised by polarised, culturally specific\ndiscourse. We evaluate whether minimal language-specific fine-tuning enables\ncross-lingual topic detection and whether adding targeted languages corrects\npre-training biases. Results show that LLMs fine-tuned in one or two languages\ncan reliably classify immigration-related content in unseen languages. However,\nidentifying whether a tweet expresses a pro- or anti-immigration stance\nbenefits from multilingual fine-tuning. Pre-training bias favours dominant\nlanguages, but even minimal exposure to under-represented languages during\nfine-tuning (as little as $9.62\\times10^{-11}$ of the original pre-training\ntoken volume) yields significant gains. These findings challenge the assumption\nthat cross-lingual mastery requires extensive multilingual training: limited\nlanguage coverage suffices for topic-level generalisation, and structural\nbiases can be corrected with lightweight interventions. By releasing\n4-bit-quantised, LoRA fine-tuned models, we provide an open-source,\nreproducible alternative to proprietary LLMs that delivers 35 times faster\ninference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,\nenabling scalable, inclusive research.", "AI": {"tldr": "LLMs can generalize to new languages with limited fine-tuning, offering a cost-effective alternative to proprietary models for cross-lingual topic detection.", "motivation": "Examine whether knowledge acquired through fine-tuning in a few languages can transfer to unseen languages that only appeared during pre-training.", "method": "Fine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or multilingual data sets to classify immigration-related tweets from X/Twitter across 13 languages.", "result": "LLMs fine-tuned in one or two languages can reliably classify immigration-related content in unseen languages. Identifying whether a tweet expresses a pro- or anti-immigration stance benefits from multilingual fine-tuning. Pre-training bias favours dominant languages, but even minimal exposure to under-represented languages during fine-tuning yields significant gains.The released models deliver 35 times faster inference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model.", "conclusion": "LLMs fine-tuned in one or two languages can reliably classify immigration-related content in unseen languages, and structural biases can be corrected with lightweight interventions. Limited language coverage suffices for topic-level generalisation."}}
{"id": "2508.06072", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06072", "abs": "https://arxiv.org/abs/2508.06072", "authors": ["Zijian Chen", "Lirong Deng", "Zhengyu Chen", "Kaiwei Zhang", "Qi Jia", "Yuan Tian", "Yucheng Zhu", "Guangtao Zhai"], "title": "Can Large Models Fool the Eye? A New Turing Test for Biological Animation", "comment": "24 pages, 10 figures", "summary": "Evaluating the abilities of large models and manifesting their gaps are\nchallenging. Current benchmarks adopt either ground-truth-based score-form\nevaluation on static datasets or indistinct textual chatbot-style human\npreferences collection, which may not provide users with immediate, intuitive,\nand perceptible feedback on performance differences. In this paper, we\nintroduce BioMotion Arena, a novel framework for evaluating large language\nmodels (LLMs) and multimodal large language models (MLLMs) via visual\nanimation. Our methodology draws inspiration from the inherent visual\nperception of motion patterns characteristic of living organisms that utilizes\npoint-light source imaging to amplify the performance discrepancies between\nmodels. Specifically, we employ a pairwise comparison evaluation and collect\nmore than 45k votes for 53 mainstream LLMs and MLLMs on 90 biological motion\nvariants. Data analyses show that the crowd-sourced human votes are in good\nagreement with those of expert raters, demonstrating the superiority of our\nBioMotion Arena in offering discriminative feedback. We also find that over\n90\\% of evaluated models, including the cutting-edge open-source InternVL3 and\nproprietary Claude-4 series, fail to produce fundamental humanoid point-light\ngroups, much less smooth and biologically plausible motions. This enables\nBioMotion Arena to serve as a challenging benchmark for performance\nvisualization and a flexible evaluation framework without restrictions on\nground-truth.", "AI": {"tldr": "BioMotion Arena\uff1a\u901a\u8fc7\u89c6\u89c9\u52a8\u753b\u8bc4\u4f30 LLM \u548c MLLM \u7684\u65b0\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u751f\u6210\u751f\u7269\u8fd0\u52a8\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u6a21\u578b\u7684\u80fd\u529b\u5e76\u63ed\u793a\u5176\u5dee\u8ddd\u5177\u6709\u6311\u6218\u6027\u3002\u5f53\u524d\u7684\u57fa\u51c6\u6d4b\u8bd5\u91c7\u7528\u57fa\u4e8e\u9759\u6001\u6570\u636e\u96c6\u7684\u57fa\u4e8e ground-truth \u7684\u8bc4\u5206\u5f62\u5f0f\u8bc4\u4f30\u6216\u4e0d\u660e\u786e\u7684\u6587\u672c\u804a\u5929\u673a\u5668\u4eba\u5f0f\u4eba\u7c7b\u504f\u597d\u6536\u96c6\uff0c\u8fd9\u53ef\u80fd\u65e0\u6cd5\u4e3a\u7528\u6237\u63d0\u4f9b\u5173\u4e8e\u6027\u80fd\u5dee\u5f02\u7684\u5373\u65f6\u3001\u76f4\u89c2\u548c\u53ef\u611f\u77e5\u7684\u53cd\u9988\u3002", "method": "\u901a\u8fc7\u89c6\u89c9\u52a8\u753b\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u548c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM)\u3002", "result": "\u4f17\u5305\u7684\u4eba\u5de5\u6295\u7968\u4e0e\u4e13\u5bb6\u8bc4\u5206\u8005\u975e\u5e38\u4e00\u81f4\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u7684 BioMotion Arena \u5728\u63d0\u4f9b\u533a\u5206\u6027\u53cd\u9988\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8d85\u8fc7 90% \u7684\u8bc4\u4f30\u6a21\u578b\uff08\u5305\u62ec InternVL3 \u548c Claude-4 \u7cfb\u5217\u7b49\u5148\u8fdb\u7684\u5f00\u6e90\u6a21\u578b\u548c\u4e13\u6709\u6a21\u578b\uff09\u65e0\u6cd5\u751f\u6210\u57fa\u672c\u7684\u4eba\u5f62\u70b9\u5149\u6e90\u7ec4\uff0c\u66f4\u4e0d\u7528\u8bf4\u5e73\u6ed1\u4e14\u751f\u7269\u5b66\u4e0a\u5408\u7406\u7684\u8fd0\u52a8\u3002\u8fd9\u4f7f\u5f97 BioMotion Arena \u80fd\u591f\u6210\u4e3a\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6027\u80fd\u53ef\u89c6\u5316\u57fa\u51c6\u548c\u4e00\u4e2a\u7075\u6d3b\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4e0d\u53d7 ground-truth \u7684\u9650\u5236\u3002"}}
{"id": "2508.06336", "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06336", "abs": "https://arxiv.org/abs/2508.06336", "authors": ["Constantin Ruhdorfer", "Matteo Bortoletto", "Victor Oei", "Anna Penzkofer", "Andreas Bulling"], "title": "Unsupervised Partner Design Enables Robust Ad-hoc Teamwork", "comment": "16 pages", "summary": "We introduce Unsupervised Partner Design (UPD) - a population-free,\nmulti-agent reinforcement learning framework for robust ad-hoc teamwork that\nadaptively generates training partners without requiring pretrained partners or\nmanual parameter tuning. UPD constructs diverse partners by stochastically\nmixing an ego agent's policy with biased random behaviours and scores them\nusing a variance-based learnability metric that prioritises partners near the\nego agent's current learning frontier. We show that UPD can be integrated with\nunsupervised environment design, resulting in the first method enabling fully\nunsupervised curricula over both level and partner distributions in a\ncooperative setting. Through extensive evaluations on Overcooked-AI and the\nOvercooked Generalisation Challenge, we demonstrate that this dynamic partner\ncurriculum is highly effective: UPD consistently outperforms both\npopulation-based and population-free baselines as well as ablations. In a user\nstudy, we further show that UPD achieves higher returns than all baselines and\nwas perceived as significantly more adaptive, more human-like, a better\ncollaborator, and less frustrating.", "AI": {"tldr": "UPD\u662f\u4e00\u79cd\u7528\u4e8e\u9c81\u68d2\u7684ad-hoc teamwork\u7684\u65e0\u76d1\u7763\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u81ea\u9002\u5e94\u5730\u751f\u6210\u8bad\u7ec3\u4f19\u4f34\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u4f19\u4f34\u6216\u624b\u52a8\u53c2\u6570\u8c03\u6574\u3002", "motivation": "\u89e3\u51b3ad-hoc teamwork\u4e2d\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u81ea\u9002\u5e94\u5730\u751f\u6210\u8bad\u7ec3\u4f19\u4f34\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u4f19\u4f34\u6216\u624b\u52a8\u53c2\u6570\u8c03\u6574\u3002", "method": "\u901a\u8fc7\u968f\u673a\u6df7\u5408\u81ea\u6211\u4ee3\u7406\u7684\u7b56\u7565\u4e0e\u6709\u504f\u5dee\u7684\u968f\u673a\u884c\u4e3a\u6765\u6784\u5efa\u591a\u6837\u5316\u7684\u4f19\u4f34\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u65b9\u5dee\u7684\u53ef\u5b66\u4e60\u6027\u6307\u6807\u5bf9\u5b83\u4eec\u8fdb\u884c\u8bc4\u5206\uff0c\u8be5\u6307\u6807\u4f18\u5148\u8003\u8651\u9760\u8fd1\u81ea\u6211\u4ee3\u7406\u5f53\u524d\u5b66\u4e60\u524d\u6cbf\u7684\u4f19\u4f34\u3002", "result": "UPD\u6301\u7eed\u4f18\u4e8e\u57fa\u4e8e\u4eba\u53e3\u548c\u65e0\u4eba\u53e3\u7684\u57fa\u7ebf\u4ee5\u53ca\u6d88\u878d\u5b9e\u9a8c\u3002\u5728\u7528\u6237\u7814\u7a76\u4e2d\uff0cUPD\u6bd4\u6240\u6709\u57fa\u7ebf\u90fd\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u56de\u62a5\uff0c\u5e76\u4e14\u88ab\u8ba4\u4e3a\u66f4\u5177\u9002\u5e94\u6027\u3001\u66f4\u50cf\u4eba\u7c7b\u3001\u66f4\u597d\u7684\u5408\u4f5c\u8005\u5e76\u4e14\u4e0d\u90a3\u4e48\u4ee4\u4eba\u6cae\u4e27\u3002", "conclusion": "UPD\u5728\u591a\u4e2a\u57fa\u7ebf\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u5e76\u4e14\u5728\u7528\u6237\u7814\u7a76\u4e2d\u88ab\u8ba4\u4e3a\u66f4\u5177\u9002\u5e94\u6027\u3001\u66f4\u50cf\u4eba\u7c7b\u3001\u66f4\u597d\u7684\u5408\u4f5c\u8005\u5e76\u4e14\u4e0d\u90a3\u4e48\u4ee4\u4eba\u6cae\u4e27\u3002"}}
{"id": "2508.06445", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06445", "abs": "https://arxiv.org/abs/2508.06445", "authors": ["Abolfazl Ansari", "Delvin Ce Zhang", "Nafis Irtiza Tripto", "Dongwon Lee"], "title": "Echoes of Automation: The Increasing Use of LLMs in Newsmaking", "comment": "To appear in 18th International Conference on Social Computing,\n  Behavioral-Cultural Modeling, & Prediction and Behavior Representation in\n  Modeling and Simulation, and to be published in the Springer LNCS series", "summary": "The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns\nfor journalistic integrity and authorship. This study examines AI-generated\ncontent across over 40,000 news articles from major, local, and college news\nmedia, in various media formats. Using three advanced AI-text detectors (e.g.,\nBinoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of\nGenAI use in recent years, especially in local and college news. Sentence-level\nanalysis reveals LLMs are often used in the introduction of news, while\nconclusions usually written manually. Linguistic analysis shows GenAI boosts\nword richness and readability but lowers formality, leading to more uniform\nwriting styles, particularly in local media.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0GenAI\u5728\u65b0\u95fb\u4e2d\u7684\u4f7f\u7528\u663e\u8457\u589e\u52a0\uff0c\u5c24\u5176\u662f\u5728\u5730\u65b9\u548c\u5927\u5b66\u65b0\u95fb\u4e2d\uff0c\u5b83\u63d0\u9ad8\u4e86\u53ef\u8bfb\u6027\u4f46\u964d\u4f4e\u4e86\u6b63\u5f0f\u6027\u3002", "motivation": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u7684\u8fc5\u901f\u5d1b\u8d77\uff0c\u7279\u522b\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u5bf9\u65b0\u95fb\u7684\u5b8c\u6574\u6027\u548c\u4f5c\u8005\u8eab\u4efd\u6784\u6210\u5a01\u80c1\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u5148\u8fdb\u7684AI\u6587\u672c\u68c0\u6d4b\u5668\uff08\u4f8b\u5982\uff0cBinoculars\u3001Fast-Detect GPT\u548cGPTZero\uff09\u5206\u6790\u6765\u81ea\u4e3b\u8981\u3001\u5730\u65b9\u548c\u5927\u5b66\u65b0\u95fb\u5a92\u4f53\u7684\u8d85\u8fc740,000\u7bc7\u65b0\u95fb\u6587\u7ae0\u4e2dAI\u751f\u6210\u7684\u5185\u5bb9\u3002", "result": "\u8fd1\u5e74\u6765\uff0c\u5c24\u5176\u662f\u5728\u5730\u65b9\u548c\u5927\u5b66\u65b0\u95fb\u4e2d\uff0cGenAI\u7684\u4f7f\u7528\u5927\u5e45\u589e\u52a0\u3002\u53e5\u5b50\u5c42\u9762\u7684\u5206\u6790\u663e\u793a\uff0cLLM\u901a\u5e38\u7528\u4e8e\u65b0\u95fb\u7684\u5f15\u8a00\u90e8\u5206\uff0c\u800c\u7ed3\u8bba\u901a\u5e38\u662f\u624b\u52a8\u64b0\u5199\u7684\u3002", "conclusion": "GenAI\u7684\u4f7f\u7528\u63d0\u9ad8\u4e86\u65b0\u95fb\u62a5\u9053\u7684\u8bcd\u6c47\u4e30\u5bcc\u5ea6\u548c\u53ef\u8bfb\u6027\uff0c\u4f46\u964d\u4f4e\u4e86\u6b63\u5f0f\u6027\uff0c\u5bfc\u81f4\u5199\u4f5c\u98ce\u683c\u66f4\u52a0\u7edf\u4e00\uff0c\u5c24\u5176\u662f\u5728\u5730\u65b9\u5a92\u4f53\u4e2d\u3002"}}
{"id": "2508.06076", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06076", "abs": "https://arxiv.org/abs/2508.06076", "authors": ["Michael Wehrli", "Alicia Durrer", "Paul Friedrich", "Sidaty El Hadramy", "Edwin Li", "Luana Brahaj", "Carol C. Hasler", "Philippe C. Cattin"], "title": "Towards MR-Based Trochleoplasty Planning", "comment": "Accepted at MICCAI COLAS Workshop 2025. Code:\n  https://wehrlimi.github.io/sr-3d-planning/", "summary": "To treat Trochlear Dysplasia (TD), current approaches rely mainly on\nlow-resolution clinical Magnetic Resonance (MR) scans and surgical intuition.\nThe surgeries are planned based on surgeons experience, have limited adoption\nof minimally invasive techniques, and lead to inconsistent outcomes. We propose\na pipeline that generates super-resolved, patient-specific 3D pseudo-healthy\ntarget morphologies from conventional clinical MR scans. First, we compute an\nisotropic super-resolved MR volume using an Implicit Neural Representation\n(INR). Next, we segment femur, tibia, patella, and fibula with a multi-label\ncustom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to\ngenerate pseudo-healthy target morphologies of the trochlear region. In\ncontrast to prior work producing pseudo-healthy low-resolution 3D MR images,\nour approach enables the generation of sub-millimeter resolved 3D shapes\ncompatible for pre- and intraoperative use. These can serve as preoperative\nblueprints for reshaping the femoral groove while preserving the native patella\narticulation. Furthermore, and in contrast to other work, we do not require a\nCT for our pipeline - reducing the amount of radiation. We evaluated our\napproach on 25 TD patients and could show that our target morphologies\nsignificantly improve the sulcus angle (SA) and trochlear groove depth (TGD).\nThe code and interactive visualization are available at\nhttps://wehrlimi.github.io/sr-3d-planning/.", "AI": {"tldr": "This paper presents a pipeline that generates high-resolution 3D models from MR scans to guide Trochlear Dysplasia (TD) surgery, improving outcomes and reducing radiation exposure compared to existing methods.", "motivation": "Current approaches to treat Trochlear Dysplasia (TD) rely on low-resolution clinical Magnetic Resonance (MR) scans and surgical intuition, leading to inconsistent outcomes with limited adoption of minimally invasive techniques. Existing work produces pseudo-healthy low-resolution 3D MR images or requires CT scans, increasing radiation.", "method": "The pipeline generates super-resolved, patient-specific 3D pseudo-healthy target morphologies from conventional clinical MR scans. It computes an isotropic super-resolved MR volume using an Implicit Neural Representation (INR), segments femur, tibia, patella, and fibula with a multi-label custom-trained network, and trains a Wavelet Diffusion Model (WDM) to generate pseudo-healthy target morphologies of the trochlear region.", "result": "The approach enables the generation of sub-millimeter resolved 3D shapes. Experiments on 25 TD patients show that the target morphologies significantly improve the sulcus angle (SA) and trochlear groove depth (TGD). The pipeline does not require CT scans, reducing radiation.", "conclusion": "The proposed pipeline generates sub-millimeter resolved 3D shapes compatible for pre- and intraoperative use, serving as preoperative blueprints for reshaping the femoral groove while preserving native patella articulation. Evaluated on 25 TD patients, the target morphologies significantly improve the sulcus angle (SA) and trochlear groove depth (TGD)."}}
{"id": "2508.06346", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06346", "abs": "https://arxiv.org/abs/2508.06346", "authors": ["Mert Can Kurucu", "Tufan Kumbasar", "\u0130brahim Eksin", "M\u00fcjde G\u00fczelkaya"], "title": "Introducing Fractional Classification Loss for Robust Learning with Noisy Labels", "comment": "25 pages, 6 figures, 2 table. Submitted to Pattern Recognition", "summary": "Robust loss functions are crucial for training deep neural networks in the\npresence of label noise, yet existing approaches require extensive,\ndataset-specific hyperparameter tuning. In this work, we introduce Fractional\nClassification Loss (FCL), an adaptive robust loss that automatically\ncalibrates its robustness to label noise during training. Built within the\nactive-passive loss framework, FCL employs the fractional derivative of the\nCross-Entropy (CE) loss as its active component and the Mean Absolute Error\n(MAE) as its passive loss component. With this formulation, we demonstrate that\nthe fractional derivative order $\\mu$ spans a family of loss functions that\ninterpolate between MAE-like robustness and CE-like fast convergence.\nFurthermore, we integrate $\\mu$ into the gradient-based optimization as a\nlearnable parameter and automatically adjust it to optimize the trade-off\nbetween robustness and convergence speed. We reveal that FCL's unique property\nestablishes a critical trade-off that enables the stable learning of $\\mu$:\nlower log penalties on difficult or mislabeled examples improve robustness but\nimpose higher penalties on easy or clean data, reducing model confidence in\nthem. Consequently, FCL can dynamically reshape its loss landscape to achieve\neffective classification performance under label noise. Extensive experiments\non benchmark datasets show that FCL achieves state-of-the-art results without\nthe need for manual hyperparameter tuning.", "AI": {"tldr": "FCL\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u9c81\u68d2\u635f\u5931\uff0c\u5b83\u81ea\u52a8\u6821\u51c6\u5176\u5bf9\u6807\u7b7e\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u6574\u8d85\u53c2\u6570\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7684\u3001\u7279\u5b9a\u4e8e\u6570\u636e\u96c6\u7684\u8d85\u53c2\u6570\u8c03\u6574\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u9c81\u68d2\u635f\u5931\uff0c\u5373\u5206\u6570\u5206\u7c7b\u635f\u5931(FCL)\uff0c\u5b83\u53ef\u4ee5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u6821\u51c6\u5176\u5bf9\u6807\u7b7e\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002", "method": "FCL\u91c7\u7528\u4ea4\u53c9\u71b5\u635f\u5931\u7684\u5206\u6570\u9636\u5bfc\u6570\u4f5c\u4e3a\u5176\u4e3b\u52a8\u5206\u91cf\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4f5c\u4e3a\u5176\u88ab\u52a8\u635f\u5931\u5206\u91cf\u3002", "result": "\u5206\u6570\u9636\u5bfc\u6570\u9636\u6570\u03bc\u8de8\u8d8a\u4e86\u4e00\u7cfb\u5217\u635f\u5931\u51fd\u6570\uff0c\u8fd9\u4e9b\u51fd\u6570\u5728\u7c7b\u4f3cmae\u7684\u9c81\u68d2\u6027\u548c\u7c7b\u4f3cce\u7684\u5feb\u901f\u6536\u655b\u4e4b\u95f4\u8fdb\u884c\u63d2\u503c\u3002FCL\u53ef\u4ee5\u52a8\u6001\u5730\u91cd\u5851\u5176\u635f\u5931\u73af\u5883\uff0c\u4ee5\u5728\u6807\u7b7e\u566a\u58f0\u4e0b\u5b9e\u73b0\u6709\u6548\u7684\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "FCL\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u6574\u8d85\u53c2\u6570\u3002"}}
{"id": "2508.06447", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06447", "abs": "https://arxiv.org/abs/2508.06447", "authors": ["Lingkun Long", "Rubing Yang", "Yushi Huang", "Desheng Hui", "Ao Zhou", "Jianlei Yang"], "title": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning", "comment": null, "summary": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance.", "AI": {"tldr": "SlimInfer accelerates LLM inference by pruning less critical prompt tokens during the forward pass, achieving significant speedups without performance loss.", "motivation": "Long-context inference for Large Language Models (LLMs) is heavily limited by high computational demands. While several existing methods optimize attention computation, they still process the full set of hidden states at each layer, limiting overall efficiency.", "method": "SlimInfer introduces a dynamic fine-grained pruning mechanism that accurately removes redundant tokens of hidden state at intermediate layers. This layer-wise pruning naturally enables an asynchronous KV cache manager that prefetches required token blocks without complex predictors, reducing both memory usage and I/O costs.", "result": "SlimInfer can achieve up to 2.53x time-to-first-token (TTFT) speedup and 1.88x end-to-end latency reduction for LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on LongBench.", "conclusion": "SlimInfer can achieve up to 2.53x time-to-first-token (TTFT) speedup and 1.88x end-to-end latency reduction for LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on LongBench."}}
{"id": "2508.06080", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06080", "abs": "https://arxiv.org/abs/2508.06080", "authors": ["Bin Xia", "Jiyang Liu", "Yuechen Zhang", "Bohao Peng", "Ruihang Chu", "Yitong Wang", "Xinglong Wu", "Bei Yu", "Jiaya Jia"], "title": "DreamVE: Unified Instruction-based Image and Video Editing", "comment": null, "summary": "Instruction-based editing holds vast potential due to its simple and\nefficient interactive editing format. However, instruction-based editing,\nparticularly for video, has been constrained by limited training data,\nhindering its practical application. To this end, we introduce DreamVE, a\nunified model for instruction-based image and video editing. Specifically, We\npropose a two-stage training strategy: first image editing, then video editing.\nThis offers two main benefits: (1) Image data scales more easily, and models\nare more efficient to train, providing useful priors for faster and better\nvideo editing training. (2) Unifying image and video generation is natural and\naligns with current trends. Moreover, we present comprehensive training data\nsynthesis pipelines, including collage-based and generative model-based data\nsynthesis. The collage-based data synthesis combines foreground objects and\nbackgrounds to generate diverse editing data, such as object manipulation,\nbackground changes, and text modifications. It can easily generate billions of\naccurate, consistent, realistic, and diverse editing pairs. We pretrain DreamVE\non extensive collage-based data to achieve strong performance in key editing\ntypes and enhance generalization and transfer capabilities. However,\ncollage-based data lacks some attribute editing cases, leading to a relative\ndrop in performance. In contrast, the generative model-based pipeline, despite\nbeing hard to scale up, offers flexibility in handling attribute editing cases.\nTherefore, we use generative model-based data to further fine-tune DreamVE.\nBesides, we design an efficient and powerful editing framework for DreamVE. We\nbuild on the SOTA T2V model and use a token concatenation with early drop\napproach to inject source image guidance, ensuring strong consistency and\neditability. The codes and models will be released.", "AI": {"tldr": "DreamVE is a unified model for instruction-based image and video editing, trained in two stages with synthesized data and an efficient editing framework.", "motivation": "Instruction-based video editing is constrained by limited training data, hindering its practical application.", "method": "A two-stage training strategy (image then video) is used. Collage-based and generative model-based data synthesis pipelines are employed. An efficient editing framework is built on a SOTA T2V model using token concatenation with early drop.", "result": "DreamVE achieves strong performance in key editing types and enhances generalization and transfer capabilities. Collage-based data excels in object manipulation, background changes, and text modifications but lacks attribute editing cases. Generative model-based data handles attribute editing cases.", "conclusion": "DreamVE, a unified model for instruction-based image and video editing, is introduced with a two-stage training strategy (image then video) and comprehensive training data synthesis pipelines (collage-based and generative model-based)."}}
{"id": "2508.06347", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.06347", "abs": "https://arxiv.org/abs/2508.06347", "authors": ["Ruiyu Zhang", "Ce Zhao", "Xin Zhao", "Lin Nie", "Wai-Fung Lam"], "title": "Structural Equation-VAE: Disentangled Latent Representations for Tabular Data", "comment": "10 pages, 2 figures", "summary": "Learning interpretable latent representations from tabular data remains a\nchallenge in deep generative modeling. We introduce SE-VAE (Structural\nEquation-Variational Autoencoder), a novel architecture that embeds measurement\nstructure directly into the design of a variational autoencoder. Inspired by\nstructural equation modeling, SE-VAE aligns latent subspaces with known\nindicator groupings and introduces a global nuisance latent to isolate\nconstruct-specific confounding variation. This modular architecture enables\ndisentanglement through design rather than through statistical regularizers\nalone. We evaluate SE-VAE on a suite of simulated tabular datasets and\nbenchmark its performance against a series of leading baselines using standard\ndisentanglement metrics. SE-VAE consistently outperforms alternatives in factor\nrecovery, interpretability, and robustness to nuisance variation. Ablation\nresults reveal that architectural structure, rather than regularization\nstrength, is the key driver of performance. SE-VAE offers a principled\nframework for white-box generative modeling in scientific and social domains\nwhere latent constructs are theory-driven and measurement validity is\nessential.", "AI": {"tldr": "SE-VAE\u662f\u4e00\u79cd\u65b0\u578b\u7684\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\u67b6\u6784\uff0c\u5b83\u901a\u8fc7\u5c06\u6d4b\u91cf\u7ed3\u6784\u76f4\u63a5\u5d4c\u5165\u5230\u8bbe\u8ba1\u4e2d\uff0c\u4ece\u800c\u5b66\u4e60\u8868\u683c\u6570\u636e\u7684\u53ef\u89e3\u91ca\u6f5c\u5728\u8868\u793a\u3002", "motivation": "\u5728\u6df1\u5ea6\u751f\u6210\u5efa\u6a21\u4e2d\uff0c\u4ece\u8868\u683c\u6570\u636e\u4e2d\u5b66\u4e60\u53ef\u89e3\u91ca\u7684\u6f5c\u5728\u8868\u793a\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "SE-VAE (\u7ed3\u6784\u65b9\u7a0b-\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668)\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u67b6\u6784\uff0c\u5b83\u5c06\u6d4b\u91cf\u7ed3\u6784\u76f4\u63a5\u5d4c\u5165\u5230\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\u7684\u8bbe\u8ba1\u4e2d\u3002", "result": "\u5728\u6a21\u62df\u8868\u683c\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86SE-VAE\uff0c\u5e76\u4f7f\u7528\u6807\u51c6\u89e3\u7f20\u7ed3\u6307\u6807\u5c06\u5176\u6027\u80fd\u4e0e\u4e00\u7cfb\u5217\u9886\u5148\u7684\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002", "conclusion": "SE-VAE\u5728\u56e0\u5b50\u6062\u590d\u3001\u53ef\u89e3\u91ca\u6027\u548c\u5bf9\u6709\u5bb3\u53d8\u5f02\u7684\u9c81\u68d2\u6027\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002\u6d88\u878d\u7ed3\u679c\u8868\u660e\uff0c\u67b6\u6784\u7ed3\u6784\u800c\u975e\u6b63\u5219\u5316\u5f3a\u5ea6\u662f\u6027\u80fd\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\u3002SE-VAE\u4e3a\u79d1\u5b66\u548c\u793e\u4f1a\u9886\u57df\u4e2d\u7684\u767d\u76d2\u751f\u6210\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u6846\u67b6\uff0c\u5728\u8fd9\u4e9b\u9886\u57df\u4e2d\uff0c\u6f5c\u5728\u7ed3\u6784\u662f\u7406\u8bba\u9a71\u52a8\u7684\uff0c\u6d4b\u91cf\u6709\u6548\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.06471", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06471", "abs": "https://arxiv.org/abs/2508.06471", "authors": ["GLM-4. 5 Team", ":", "Aohan Zeng", "Xin Lv", "Qinkai Zheng", "Zhenyu Hou", "Bin Chen", "Chengxing Xie", "Cunxiang Wang", "Da Yin", "Hao Zeng", "Jiajie Zhang", "Kedong Wang", "Lucen Zhong", "Mingdao Liu", "Rui Lu", "Shulin Cao", "Xiaohan Zhang", "Xuancheng Huang", "Yao Wei", "Yean Cheng", "Yifan An", "Yilin Niu", "Yuanhao Wen", "Yushi Bai", "Zhengxiao Du", "Zihan Wang", "Zilin Zhu", "Bohan Zhang", "Bosi Wen", "Bowen Wu", "Bowen Xu", "Can Huang", "Casey Zhao", "Changpeng Cai", "Chao Yu", "Chen Li", "Chendi Ge", "Chenghua Huang", "Chenhui Zhang", "Chenxi Xu", "Chenzheng Zhu", "Chuang Li", "Congfeng Yin", "Daoyan Lin", "Dayong Yang", "Dazhi Jiang", "Ding Ai", "Erle Zhu", "Fei Wang", "Gengzheng Pan", "Guo Wang", "Hailong Sun", "Haitao Li", "Haiyang Li", "Haiyi Hu", "Hanyu Zhang", "Hao Peng", "Hao Tai", "Haoke Zhang", "Haoran Wang", "Haoyu Yang", "He Liu", "He Zhao", "Hongwei Liu", "Hongxi Yan", "Huan Liu", "Huilong Chen", "Ji Li", "Jiajing Zhao", "Jiamin Ren", "Jian Jiao", "Jiani Zhao", "Jianyang Yan", "Jiaqi Wang", "Jiayi Gui", "Jiayue Zhao", "Jie Liu", "Jijie Li", "Jing Li", "Jing Lu", "Jingsen Wang", "Jingwei Yuan", "Jingxuan Li", "Jingzhao Du", "Jinhua Du", "Jinxin Liu", "Junkai Zhi", "Junli Gao", "Ke Wang", "Lekang Yang", "Liang Xu", "Lin Fan", "Lindong Wu", "Lintao Ding", "Lu Wang", "Man Zhang", "Minghao Li", "Minghuan Xu", "Mingming Zhao", "Mingshu Zhai", "Pengfan Du", "Qian Dong", "Shangde Lei", "Shangqing Tu", "Shangtong Yang", "Shaoyou Lu", "Shijie Li", "Shuang Li", "Shuang-Li", "Shuxun Yang", "Sibo Yi", "Tianshu Yu", "Wei Tian", "Weihan Wang", "Wenbo Yu", "Weng Lam Tam", "Wenjie Liang", "Wentao Liu", "Xiao Wang", "Xiaohan Jia", "Xiaotao Gu", "Xiaoying Ling", "Xin Wang", "Xing Fan", "Xingru Pan", "Xinyuan Zhang", "Xinze Zhang", "Xiuqing Fu", "Xunkai Zhang", "Yabo Xu", "Yandong Wu", "Yida Lu", "Yidong Wang", "Yilin Zhou", "Yiming Pan", "Ying Zhang", "Yingli Wang", "Yingru Li", "Yinpei Su", "Yipeng Geng", "Yitong Zhu", "Yongkun Yang", "Yuhang Li", "Yuhao Wu", "Yujiang Li", "Yunan Liu", "Yunqing Wang", "Yuntao Li", "Yuxuan Zhang", "Zezhen Liu", "Zhen Yang", "Zhengda Zhou", "Zhongpei Qiao", "Zhuoer Feng", "Zhuorui Liu", "Zichen Zhang", "Zihan Wang", "Zijun Yao", "Zikang Wang", "Ziqiang Liu", "Ziwei Chai", "Zixuan Li", "Zuodong Zhao", "Wenguang Chen", "Jidong Zhai", "Bin Xu", "Minlie Huang", "Hongning Wang", "Juanzi Li", "Yuxiao Dong", "Jie Tang"], "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models", "comment": null, "summary": "We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language\nmodel with 355B total parameters and 32B activated parameters, featuring a\nhybrid reasoning method that supports both thinking and direct response modes.\nThrough multi-stage training on 23T tokens and comprehensive post-training with\nexpert model iteration and reinforcement learning, GLM-4.5 achieves strong\nperformance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on\nTAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer\nparameters than several competitors, GLM-4.5 ranks 3rd overall among all\nevaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B\nparameters) and a compact version, GLM-4.5-Air (106B parameters), to advance\nresearch in reasoning and agentic AI systems. Code, models, and more\ninformation are available at https://github.com/zai-org/GLM-4.5.", "AI": {"tldr": "GLM-4.5 is an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters. It achieves strong performance across agentic, reasoning, and coding (ARC) tasks.", "motivation": "To advance research in reasoning and agentic AI systems.", "method": "a hybrid reasoning method that supports both thinking and direct response modes. Multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning", "result": "GLM-4.5 scores 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified.", "conclusion": "GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, ranking 3rd overall and 2nd on agentic benchmarks with fewer parameters than competitors. The models are released to advance research."}}
{"id": "2508.06082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06082", "abs": "https://arxiv.org/abs/2508.06082", "authors": ["Yanxiao Sun", "Jiafu Wu", "Yun Cao", "Chengming Xu", "Yabiao Wang", "Weijian Cao", "Donghao Luo", "Chengjie Wang", "Yanwei Fu"], "title": "SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment", "comment": null, "summary": "Diffusion-based or flow-based models have achieved significant progress in\nvideo synthesis but require multiple iterative sampling steps, which incurs\nsubstantial computational overhead. While many distillation methods that are\nsolely based on trajectory-preserving or distribution-matching have been\ndeveloped to accelerate video generation models, these approaches often suffer\nfrom performance breakdown or increased artifacts under few-step settings. To\naddress these limitations, we propose \\textbf{\\emph{SwiftVideo}}, a unified and\nstable distillation framework that combines the advantages of\ntrajectory-preserving and distribution-matching strategies. Our approach\nintroduces continuous-time consistency distillation to ensure precise\npreservation of ODE trajectories. Subsequently, we propose a dual-perspective\nalignment that includes distribution alignment between synthetic and real data\nalong with trajectory alignment across different inference steps. Our method\nmaintains high-quality video generation while substantially reducing the number\nof inference steps. Quantitative evaluations on the OpenVid-1M benchmark\ndemonstrate that our method significantly outperforms existing approaches in\nfew-step video generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u84b8\u998f\u6846\u67b6SwiftVideo\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u8f68\u8ff9\u4fdd\u6301\u548c\u5206\u5e03\u5339\u914d\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u51cf\u5c11\u63a8\u7406\u6b65\u9aa4\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u57fa\u4e8e\u6269\u6563\u6216\u57fa\u4e8e\u6d41\u7684\u6a21\u578b\u5728\u89c6\u9891\u5408\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u9700\u8981\u591a\u4e2a\u8fed\u4ee3\u91c7\u6837\u6b65\u9aa4\uff0c\u8fd9\u4f1a\u4ea7\u751f\u5927\u91cf\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u867d\u7136\u5df2\u7ecf\u5f00\u53d1\u4e86\u8bb8\u591a\u4ec5\u57fa\u4e8e\u8f68\u8ff9\u4fdd\u6301\u6216\u5206\u5e03\u5339\u914d\u7684\u84b8\u998f\u65b9\u6cd5\u6765\u52a0\u901f\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5c11\u91cf\u6b65\u9aa4\u7684\u8bbe\u7f6e\u4e0b\u901a\u5e38\u4f1a\u9047\u5230\u6027\u80fd\u4e0b\u964d\u6216\u4f2a\u5f71\u589e\u52a0\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u4e14\u7a33\u5b9a\u7684\u84b8\u998f\u6846\u67b6SwiftVideo\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u8f68\u8ff9\u4fdd\u6301\u548c\u5206\u5e03\u5339\u914d\u7b56\u7565\u7684\u4f18\u70b9\uff0c\u5f15\u5165\u4e86\u8fde\u7eed\u65f6\u95f4\u4e00\u81f4\u6027\u84b8\u998f\u4ee5\u786e\u4fdd\u7cbe\u786e\u4fdd\u6301ODE\u8f68\u8ff9\uff0c\u5e76\u63d0\u51fa\u4e86\u5305\u62ec\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u7684\u5206\u5e03\u5bf9\u9f50\u4ee5\u53ca\u8de8\u4e0d\u540c\u63a8\u7406\u6b65\u9aa4\u7684\u8f68\u8ff9\u5bf9\u9f50\u7684\u53cc\u91cd\u89c6\u89d2\u5bf9\u9f50\u3002", "result": "\u5728OpenVid-1M\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9a\u91cf\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5c11\u91cf\u6b65\u9aa4\u7684\u89c6\u9891\u751f\u6210\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5c11\u91cf\u6b65\u9aa4\u7684\u89c6\u9891\u751f\u6210\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.06353", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06353", "abs": "https://arxiv.org/abs/2508.06353", "authors": ["Parichit Sharma", "Marcin Stanislaw", "Hasan Kurban", "Oguzhan Kulekci", "Mehmet Dalkilic"], "title": "Geometric-k-means: A Bound Free Approach to Fast and Eco-Friendly k-means", "comment": null, "summary": "This paper introduces Geometric-k-means (or Gk-means for short), a novel\napproach that significantly enhances the efficiency and energy economy of the\nwidely utilized k-means algorithm, which, despite its inception over five\ndecades ago, remains a cornerstone in machine learning applications. The\nessence of Gk-means lies in its active utilization of geometric principles,\nspecifically scalar projection, to significantly accelerate the algorithm\nwithout sacrificing solution quality. This geometric strategy enables a more\ndiscerning focus on data points that are most likely to influence cluster\nupdates, which we call as high expressive data (HE). In contrast, low\nexpressive data (LE), does not impact clustering outcome, is effectively\nbypassed, leading to considerable reductions in computational overhead.\nExperiments spanning synthetic, real-world and high-dimensional datasets,\ndemonstrate Gk-means is significantly better than traditional and state of the\nart (SOTA) k-means variants in runtime and distance computations (DC).\nMoreover, Gk-means exhibits better resource efficiency, as evidenced by its\nreduced energy footprint, placing it as more sustainable alternative.", "AI": {"tldr": "Gk-means: A novel k-means algorithm using geometric principles to improve efficiency and energy economy.", "motivation": "enhance the efficiency and energy economy of the widely utilized k-means algorithm", "method": "active utilization of geometric principles, specifically scalar projection, to significantly accelerate the algorithm without sacrificing solution quality. focusing on high expressive data and bypassing low expressive data.", "result": "considerable reductions in computational overhead", "conclusion": "Gk-means significantly better than traditional and state of the art k-means variants in runtime and distance computations, exhibits better resource efficiency, as evidenced by its reduced energy footprint, placing it as more sustainable alternative."}}
