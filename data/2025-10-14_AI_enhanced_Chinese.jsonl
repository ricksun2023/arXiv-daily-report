{"id": "2510.09857", "categories": ["cs.IR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09857", "abs": "https://arxiv.org/abs/2510.09857", "authors": ["Xiao Yang", "Peifeng Yin", "Abe Engle", "Jinfeng Zhuang", "Ling Leng"], "title": "MTMD: A Multi-Task Multi-Domain Framework for Unified Ad Lightweight Ranking at Pinterest", "comment": "AdKDD 2025", "summary": "The lightweight ad ranking layer, living after the retrieval stage and before\nthe fine ranker, plays a critical role in the success of a cascaded ad\nrecommendation system. Due to the fact that there are multiple optimization\ntasks depending on the ad domain, e.g., Click Through Rate (CTR) for click ads\nand Conversion Rate (CVR) for conversion ads, as well as multiple surfaces\nwhere an ad is served (home feed, search, or related item recommendation) with\ndiverse ad products (shopping or standard ad); it is an essentially challenging\nproblem in industry on how to do joint holistic optimization in the lightweight\nranker, such that the overall platform's value, advertiser's value, and user's\nvalue are maximized.\n  Deep Neural Network (DNN)-based multitask learning (MTL) can handle multiple\ngoals naturally, with each prediction head mapping to a particular optimization\ngoal. However, in practice, it is unclear how to unify data from different\nsurfaces and ad products into a single model. It is critical to learn\ndomain-specialized knowledge and explicitly transfer knowledge between domains\nto make MTL effective. We present a Multi-Task Multi-Domain (MTMD) architecture\nunder the classic Two-Tower paradigm, with the following key contributions: 1)\nhandle different prediction tasks, ad products, and ad serving surfaces in a\nunified framework; 2) propose a novel mixture-of-expert architecture to learn\nboth specialized knowledge each domain and common knowledge shared between\ndomains; 3) propose a domain adaption module to encourage knowledge transfer\nbetween experts; 4) constrain the modeling of different prediction tasks. MTMD\nimproves the offline loss value by 12% to 36%, mapping to 2% online reduction\nin cost per click. We have deployed this single MTMD framework into production\nfor Pinterest ad recommendation replacing 9 production models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u4efb\u52a1\u591a\u57df\uff08MTMD\uff09\u67b6\u6784\uff0c\u7528\u4e8e\u5728\u8f7b\u91cf\u7ea7\u5e7f\u544a\u6392\u5e8f\u5668\u4e2d\u8fdb\u884c\u8054\u5408\u6574\u4f53\u4f18\u5316\uff0c\u4ee5\u6700\u5927\u5316\u5e73\u53f0\u3001\u5e7f\u544a\u5546\u548c\u7528\u6237\u7684\u4ef7\u503c\u3002", "motivation": "\u5728\u7ea7\u8054\u5e7f\u544a\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u8f7b\u91cf\u7ea7\u5e7f\u544a\u6392\u5e8f\u5c42\u81f3\u5173\u91cd\u8981\u3002\u7531\u4e8e\u5e7f\u544a\u57df\u5b58\u5728\u591a\u4e2a\u4f18\u5316\u4efb\u52a1\uff08\u4f8b\u5982\u70b9\u51fb\u7387\u548c\u8f6c\u5316\u7387\uff09\uff0c\u4ee5\u53ca\u5e7f\u544a\u6295\u653e\u7684\u591a\u4e2a\u8868\u9762\u548c\u4e0d\u540c\u7684\u5e7f\u544a\u4ea7\u54c1\uff0c\u56e0\u6b64\u5728\u8f7b\u91cf\u7ea7\u6392\u5e8f\u5668\u4e2d\u8fdb\u884c\u8054\u5408\u6574\u4f53\u4f18\u5316\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u4efb\u52a1\u591a\u57df\uff08MTMD\uff09\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u57fa\u4e8e\u7ecf\u5178\u7684\u53cc\u5854\u6a21\u5f0f\uff0c\u5e76\u7ed3\u5408\u4e86\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u6765\u5b66\u4e60\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u548c\u9886\u57df\u4e4b\u95f4\u7684\u5171\u4eab\u77e5\u8bc6\uff0c\u4ee5\u53ca\u4e00\u4e2a\u9886\u57df\u81ea\u9002\u5e94\u6a21\u5757\u6765\u4fc3\u8fdb\u4e13\u5bb6\u4e4b\u95f4\u7684\u77e5\u8bc6\u8f6c\u79fb\u3002", "result": "MTMD\u5c06\u79bb\u7ebf\u635f\u5931\u503c\u63d0\u9ad8\u4e8612%\u523036%\uff0c\u5e76\u5728\u7ebf\u964d\u4f4e\u4e862%\u7684\u6bcf\u6b21\u70b9\u51fb\u6210\u672c\u3002", "conclusion": "MTMD\u6846\u67b6\u5df2\u90e8\u7f72\u5230Pinterest\u5e7f\u544a\u63a8\u8350\u7684\u751f\u4ea7\u73af\u5883\u4e2d\uff0c\u53d6\u4ee3\u4e869\u4e2a\u751f\u4ea7\u6a21\u578b\u3002"}}
{"id": "2510.09897", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.09897", "abs": "https://arxiv.org/abs/2510.09897", "authors": ["Wonbin Kweon", "Runchu Tian", "SeongKu Kang", "Pengcheng Jiang", "Zhiyong Lu", "Jiawei Han", "Hwanjo Yu"], "title": "PairSem: LLM-Guided Pairwise Semantic Matching for Scientific Document Retrieval", "comment": null, "summary": "Scientific document retrieval is a critical task for enabling knowledge\ndiscovery and supporting research across diverse domains. However, existing\ndense retrieval methods often struggle to capture fine-grained scientific\nconcepts in texts due to their reliance on holistic embeddings and limited\ndomain understanding. Recent approaches leverage large language models (LLMs)\nto extract fine-grained semantic entities and enhance semantic matching, but\nthey typically treat entities as independent fragments, overlooking the\nmulti-faceted nature of scientific concepts. To address this limitation, we\npropose Pairwise Semantic Matching (PairSem), a framework that represents\nrelevant semantics as entity-aspect pairs, capturing complex, multi-faceted\nscientific concepts. PairSem is unsupervised, base retriever-agnostic, and\nplug-and-play, enabling precise and context-aware matching without requiring\nquery-document labels or entity annotations. Extensive experiments on multiple\ndatasets and retrievers demonstrate that PairSem significantly improves\nretrieval performance, highlighting the importance of modeling multi-aspect\nsemantics in scientific information retrieval.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u79d1\u5b66\u6587\u6863\u68c0\u7d22\u6846\u67b6\uff0cPairwise Semantic Matching (PairSem)\uff0c\u901a\u8fc7\u5c06\u8bed\u4e49\u8868\u793a\u4e3a\u5b9e\u4f53-\u65b9\u9762\u5bf9\uff0c\u6355\u6349\u590d\u6742\u7684\u591a\u65b9\u9762\u79d1\u5b66\u6982\u5ff5\uff0c\u4ece\u800c\u63d0\u9ad8\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5bc6\u96c6\u68c0\u7d22\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u6587\u672c\u4e2d\u7ec6\u7c92\u5ea6\u7684\u79d1\u5b66\u6982\u5ff5\uff0c\u5e76\u4e14\u901a\u5e38\u5c06\u5b9e\u4f53\u89c6\u4e3a\u72ec\u7acb\u7684\u7247\u6bb5\uff0c\u5ffd\u7565\u4e86\u79d1\u5b66\u6982\u5ff5\u7684\u591a\u65b9\u9762\u6027\u8d28\u3002", "method": "PairSem\u6846\u67b6\u5c06\u8bed\u4e49\u8868\u793a\u4e3a\u5b9e\u4f53-\u65b9\u9762\u5bf9\uff0c\u662f\u65e0\u76d1\u7763\u7684\u3001\u4e0e\u57fa\u7840\u68c0\u7d22\u5668\u65e0\u5173\u7684\uff0c\u5e76\u4e14\u662f\u5373\u63d2\u5373\u7528\u7684\uff0c\u65e0\u9700\u67e5\u8be2-\u6587\u6863\u6807\u7b7e\u6216\u5b9e\u4f53\u6ce8\u91ca\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u68c0\u7d22\u5668\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPairSem\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u7d22\u6027\u80fd\u3002", "conclusion": "\u5efa\u6a21\u591a\u65b9\u9762\u8bed\u4e49\u5bf9\u4e8e\u79d1\u5b66\u4fe1\u606f\u68c0\u7d22\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.10095", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10095", "abs": "https://arxiv.org/abs/2510.10095", "authors": ["Peiyuan Gong", "Feiran Zhu", "Yaqi Yin", "Chenglei Dai", "Chao Zhang", "Kai Zheng", "Wentian Bao", "Jiaxin Mao", "Yi Zhang"], "title": "CardRewriter: Leveraging Knowledge Cards for Long-Tail Query Rewriting on Short-Video Platforms", "comment": null, "summary": "Short-video platforms have rapidly become a new generation of information\nretrieval systems, where users formulate queries to access desired videos.\nHowever, user queries, especially long-tail ones, often suffer from spelling\nerrors, incomplete phrasing, and ambiguous intent, resulting in mismatches\nbetween user expectations and retrieved results. While large language models\n(LLMs) have shown success in long-tail query rewriting within e-commerce, they\nstruggle on short-video platforms, where proprietary content such as short\nvideos, live streams, micro dramas, and user social networks falls outside\ntheir training distribution. To address this challenge, we introduce\n\\textbf{CardRewriter}, an LLM-based framework that incorporates domain-specific\nknowledge to enhance long-tail query rewriting. For each query, our method\naggregates multi-source knowledge relevant to the query and summarizes it into\nan informative and query-relevant knowledge card. This card then guides the LLM\nto better capture user intent and produce more effective query rewrites. We\noptimize CardRewriter using a two-stage training pipeline: supervised\nfine-tuning followed by group relative policy optimization, with a tailored\nreward system balancing query relevance and retrieval effectiveness. Offline\nexperiments show that CardRewriter substantially improves rewriting quality for\nqueries targeting proprietary content. Online A/B testing further confirms\nsignificant gains in long-view rate (LVR) and click-through rate (CTR), along\nwith a notable reduction in initiative query reformulation rate (IQRR). Since\nSeptember 2025, CardRewriter has been deployed on Kuaishou, one of China's\nlargest short-video platforms, serving hundreds of millions of users daily.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCardRewriter\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u77ed\u89c6\u9891\u5e73\u53f0\u4e0a\u7684\u957f\u5c3e\u67e5\u8be2\u91cd\u5199\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u6574\u5408\u591a\u6e90\u77e5\u8bc6\u5e76\u5c06\u5176\u603b\u7ed3\u4e3a\u77e5\u8bc6\u5361\u7247\uff0c\u4ee5\u6307\u5bfcLLM\u66f4\u597d\u5730\u7406\u89e3\u7528\u6237\u610f\u56fe\u5e76\u751f\u6210\u66f4\u6709\u6548\u7684\u67e5\u8be2\u91cd\u5199\u3002", "motivation": "\u7528\u6237\u67e5\u8be2\uff08\u5c24\u5176\u662f\u957f\u5c3e\u67e5\u8be2\uff09\u7ecf\u5e38\u51fa\u73b0\u62fc\u5199\u9519\u8bef\u3001\u77ed\u8bed\u4e0d\u5b8c\u6574\u548c\u610f\u56fe\u6a21\u7cca\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u7528\u6237\u671f\u671b\u4e0e\u68c0\u7d22\u7ed3\u679c\u4e0d\u5339\u914d\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728\u7535\u5b50\u5546\u52a1\u4e2d\u7684\u957f\u5c3e\u67e5\u8be2\u91cd\u5199\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u77ed\u89c6\u9891\u5e73\u53f0\u4e0a\u5374\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u77ed\u89c6\u9891\u3001\u76f4\u64ad\u3001\u5fae\u77ed\u5267\u548c\u7528\u6237\u793e\u4ea4\u7f51\u7edc\u7b49\u4e13\u6709\u5185\u5bb9\u4e0d\u5728\u5b83\u4eec\u7684\u8bad\u7ec3\u5206\u5e03\u8303\u56f4\u5185\u3002", "method": "\u8be5\u65b9\u6cd5\u4e3a\u6bcf\u4e2a\u67e5\u8be2\u805a\u5408\u4e0e\u67e5\u8be2\u76f8\u5173\u7684\u591a\u6e90\u77e5\u8bc6\uff0c\u5e76\u5c06\u5176\u603b\u7ed3\u4e3a\u4fe1\u606f\u4e30\u5bcc\u4e14\u4e0e\u67e5\u8be2\u76f8\u5173\u7684\u77e5\u8bc6\u5361\u7247\u3002\u7136\u540e\uff0c\u8be5\u5361\u7247\u6307\u5bfcLLM\u66f4\u597d\u5730\u6355\u6349\u7528\u6237\u610f\u56fe\u5e76\u4ea7\u751f\u66f4\u6709\u6548\u7684\u67e5\u8be2\u91cd\u5199\u3002\u4f7f\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7ba1\u9053\u4f18\u5316CardRewriter\uff1a\u76d1\u7763\u5f0f\u5fae\u8c03\uff0c\u7136\u540e\u662f\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff0c\u5e76\u91c7\u7528\u5b9a\u5236\u7684\u5956\u52b1\u7cfb\u7edf\u6765\u5e73\u8861\u67e5\u8be2\u76f8\u5173\u6027\u548c\u68c0\u7d22\u6709\u6548\u6027\u3002", "result": "\u79bb\u7ebf\u5b9e\u9a8c\u8868\u660e\uff0cCardRewriter\u663e\u7740\u63d0\u9ad8\u4e86\u9488\u5bf9\u4e13\u6709\u5185\u5bb9\u7684\u67e5\u8be2\u7684\u91cd\u5199\u8d28\u91cf\u3002\u5728\u7ebfA/B\u6d4b\u8bd5\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u957f\u89c2\u770b\u7387(LVR)\u548c\u70b9\u51fb\u7387(CTR)\u7684\u663e\u7740\u63d0\u9ad8\uff0c\u4ee5\u53ca\u4e3b\u52a8\u67e5\u8be2\u91cd\u65b0\u8868\u8ff0\u7387(IQRR)\u7684\u663e\u7740\u964d\u4f4e\u3002", "conclusion": "CardRewriter\u5df2\u4e8e2025\u5e749\u6708\u90e8\u7f72\u5728\u5feb\u624b\u4e0a\uff0c\u6bcf\u5929\u4e3a\u6570\u4ebf\u7528\u6237\u63d0\u4f9b\u670d\u52a1\u3002"}}
{"id": "2510.10109", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10109", "abs": "https://arxiv.org/abs/2510.10109", "authors": ["Shuangquan Lyu", "Ming Wang", "Huajun Zhang", "Jiasen Zheng", "Junjiang Lin", "Xiaoxuan Sun"], "title": "Integrating Structure-Aware Attention and Knowledge Graphs in Explainable Recommendation Systems", "comment": null, "summary": "This paper designs and implements an explainable recommendation model that\nintegrates knowledge graphs with structure-aware attention mechanisms. The\nmodel is built on graph neural networks and incorporates a multi-hop neighbor\naggregation strategy. By integrating the structural information of knowledge\ngraphs and dynamically assigning importance to different neighbors through an\nattention mechanism, the model enhances its ability to capture implicit\npreference relationships. In the proposed method, users and items are embedded\ninto a unified graph structure. Multi-level semantic paths are constructed\nbased on entities and relations in the knowledge graph to extract richer\ncontextual information. During the rating prediction phase, recommendations are\ngenerated through the interaction between user and target item representations.\nThe model is optimized using a binary cross-entropy loss function. Experiments\nconducted on the Amazon Books dataset validate the superior performance of the\nproposed model across various evaluation metrics. The model also shows good\nconvergence and stability. These results further demonstrate the effectiveness\nand practicality of structure-aware attention mechanisms in knowledge\ngraph-enhanced recommendation.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u63a8\u8350\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u96c6\u6210\u4e86\u77e5\u8bc6\u56fe\u8c31\u548c\u7ed3\u6784\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u3002", "motivation": "\u8be5\u6a21\u578b\u65e8\u5728\u901a\u8fc7\u6574\u5408\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\u4fe1\u606f\u5e76\u52a8\u6001\u5730\u5206\u914d\u4e0d\u540c\u90bb\u5c45\u7684\u91cd\u8981\u6027\uff0c\u4ece\u800c\u589e\u5f3a\u6a21\u578b\u6355\u83b7\u9690\u5f0f\u504f\u597d\u5173\u7cfb\u7684\u80fd\u529b\u3002", "method": "\u8be5\u6a21\u578b\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u7ed3\u5408\u4e86\u591a\u8df3\u90bb\u5c45\u805a\u5408\u7b56\u7565\u3002\u7528\u6237\u548c\u9879\u76ee\u88ab\u5d4c\u5165\u5230\u4e00\u4e2a\u7edf\u4e00\u7684\u56fe\u7ed3\u6784\u4e2d\uff0c\u5e76\u6784\u5efa\u591a\u5c42\u6b21\u8bed\u4e49\u8def\u5f84\u4ee5\u63d0\u53d6\u66f4\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u5728\u8bc4\u5206\u9884\u6d4b\u9636\u6bb5\uff0c\u901a\u8fc7\u7528\u6237\u548c\u76ee\u6807\u9879\u76ee\u8868\u793a\u4e4b\u95f4\u7684\u4ea4\u4e92\u751f\u6210\u63a8\u8350\u3002", "result": "\u5728Amazon Books\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u5404\u79cd\u8bc4\u4f30\u6307\u6807\u4e0a\u7684\u4f18\u8d8a\u6027\u80fd\u3002\u8be5\u6a21\u578b\u8fd8\u8868\u73b0\u51fa\u826f\u597d\u7684\u6536\u655b\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u7ed3\u6784\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u5728\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u63a8\u8350\u4e2d\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.09646", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09646", "abs": "https://arxiv.org/abs/2510.09646", "authors": ["Ritesh Chandra", "Sonali Agarwal", "Navjot Singh"], "title": "Real-Time Health Analytics Using Ontology-Driven Complex Event Processing and LLM Reasoning: A Tuberculosis Case Study", "comment": "14 table. 20 figure", "summary": "Timely detection of critical health conditions remains a major challenge in\npublic health analytics, especially in Big Data environments characterized by\nhigh volume, rapid velocity, and diverse variety of clinical data. This study\npresents an ontology-enabled real-time analytics framework that integrates\nComplex Event Processing (CEP) and Large Language Models (LLMs) to enable\nintelligent health event detection and semantic reasoning over heterogeneous,\nhigh-velocity health data streams. The architecture leverages the Basic Formal\nOntology (BFO) and Semantic Web Rule Language (SWRL) to model diagnostic rules\nand domain knowledge. Patient data is ingested and processed using Apache Kafka\nand Spark Streaming, where CEP engines detect clinically significant event\npatterns. LLMs support adaptive reasoning, event interpretation, and ontology\nrefinement. Clinical information is semantically structured as Resource\nDescription Framework (RDF) triples in Graph DB, enabling SPARQL-based querying\nand knowledge-driven decision support. The framework is evaluated using a\ndataset of 1,000 Tuberculosis (TB) patients as a use case, demonstrating\nlow-latency event detection, scalable reasoning, and high model performance (in\nterms of precision, recall, and F1-score). These results validate the system's\npotential for generalizable, real-time health analytics in complex Big Data\nscenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u672c\u4f53\u7684\u5b9e\u65f6\u5206\u6790\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u590d\u6742\u4e8b\u4ef6\u5904\u7406\uff08CEP\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4ee5\u5b9e\u73b0\u667a\u80fd\u5065\u5eb7\u4e8b\u4ef6\u68c0\u6d4b\u548c\u5bf9\u5f02\u6784\u3001\u9ad8\u901f\u5065\u5eb7\u6570\u636e\u6d41\u7684\u8bed\u4e49\u63a8\u7406\u3002", "motivation": "\u5728\u5927\u6570\u636e\u73af\u5883\u4e2d\uff0c\u53ca\u65f6\u68c0\u6d4b\u5173\u952e\u5065\u5eb7\u72b6\u51b5\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u4e34\u5e8a\u6570\u636e\u91cf\u5927\u3001\u901f\u5ea6\u5feb\u3001\u79cd\u7c7b\u591a\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u8be5\u67b6\u6784\u5229\u7528\u57fa\u672c\u5f62\u5f0f\u672c\u4f53\uff08BFO\uff09\u548c\u8bed\u4e49\u7f51\u89c4\u5219\u8bed\u8a00\uff08SWRL\uff09\u6765\u5efa\u6a21\u8bca\u65ad\u89c4\u5219\u548c\u9886\u57df\u77e5\u8bc6\u3002\u60a3\u8005\u6570\u636e\u4f7f\u7528Apache Kafka\u548cSpark Streaming\u8fdb\u884c\u6444\u53d6\u548c\u5904\u7406\uff0c\u5176\u4e2dCEP\u5f15\u64ce\u68c0\u6d4b\u4e34\u5e8a\u4e0a\u91cd\u8981\u7684\u4e8b\u4ef6\u6a21\u5f0f\u3002LLM\u652f\u6301\u81ea\u9002\u5e94\u63a8\u7406\u3001\u4e8b\u4ef6\u89e3\u91ca\u548c\u672c\u4f53\u6539\u8fdb\u3002\u4e34\u5e8a\u4fe1\u606f\u5728Graph DB\u4e2d\u88ab\u8bed\u4e49\u7ed3\u6784\u5316\u4e3a\u8d44\u6e90\u63cf\u8ff0\u6846\u67b6\uff08RDF\uff09\u4e09\u5143\u7ec4\uff0c\u4ece\u800c\u5b9e\u73b0\u57fa\u4e8eSPARQL\u7684\u67e5\u8be2\u548c\u77e5\u8bc6\u9a71\u52a8\u7684\u51b3\u7b56\u652f\u6301\u3002", "result": "\u8be5\u6846\u67b6\u4f7f\u7528\u5305\u542b1,000\u540d\u7ed3\u6838\u75c5\uff08TB\uff09\u60a3\u8005\u7684\u6570\u636e\u96c6\u4f5c\u4e3a\u7528\u4f8b\u8fdb\u884c\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u4f4e\u5ef6\u8fdf\u4e8b\u4ef6\u68c0\u6d4b\u3001\u53ef\u6269\u5c55\u7684\u63a8\u7406\u548c\u9ad8\u6a21\u578b\u6027\u80fd\uff08\u5728\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u65b9\u9762\uff09\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u7cfb\u7edf\u5728\u590d\u6742\u5927\u6570\u636e\u573a\u666f\u4e2d\u8fdb\u884c\u901a\u7528\u3001\u5b9e\u65f6\u5065\u5eb7\u5206\u6790\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.09671", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09671", "abs": "https://arxiv.org/abs/2510.09671", "authors": ["Wei Zhou", "Bolei Ma", "Annemarie Friedrich", "Mohsen Mesgar"], "title": "Table Question Answering in the Era of Large Language Models: A Comprehensive Survey of Tasks, Methods, and Evaluation", "comment": null, "summary": "Table Question Answering (TQA) aims to answer natural language questions\nabout tabular data, often accompanied by additional contexts such as text\npassages. The task spans diverse settings, varying in table representation,\nquestion/answer complexity, modality involved, and domain. While recent\nadvances in large language models (LLMs) have led to substantial progress in\nTQA, the field still lacks a systematic organization and understanding of task\nformulations, core challenges, and methodological trends, particularly in light\nof emerging research directions such as reinforcement learning. This survey\naddresses this gap by providing a comprehensive and structured overview of TQA\nresearch with a focus on LLM-based methods. We provide a comprehensive\ncategorization of existing benchmarks and task setups. We group current\nmodeling strategies according to the challenges they target, and analyze their\nstrengths and limitations. Furthermore, we highlight underexplored but timely\ntopics that have not been systematically covered in prior research. By unifying\ndisparate research threads and identifying open problems, our survey offers a\nconsolidated foundation for the TQA community, enabling a deeper understanding\nof the state of the art and guiding future developments in this rapidly\nevolving area.", "AI": {"tldr": "\u672c\u8c03\u67e5\u5168\u9762\u6982\u8ff0\u4e86\u57fa\u4e8eLLM\u7684TQA\u7814\u7a76\uff0c\u5bf9\u73b0\u6709\u57fa\u51c6\u548c\u4efb\u52a1\u8bbe\u7f6e\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u5e76\u6839\u636e\u5176\u9488\u5bf9\u7684\u6311\u6218\u5bf9\u5f53\u524d\u7684\u5efa\u6a21\u7b56\u7565\u8fdb\u884c\u5206\u7ec4\uff0c\u6700\u540e\uff0c\u786e\u5b9a\u4e86\u5c1a\u672a\u5728\u5148\u524d\u7814\u7a76\u4e2d\u7cfb\u7edf\u5730\u6db5\u76d6\u7684\uff0c\u4f46\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u53ca\u65f6\u4e3b\u9898\u3002", "motivation": "\u8be5\u9886\u57df\u4ecd\u7136\u7f3a\u4e4f\u5bf9\u4efb\u52a1\u516c\u5f0f\u3001\u6838\u5fc3\u6311\u6218\u548c\u65b9\u6cd5\u8d8b\u52bf\u7684\u7cfb\u7edf\u7ec4\u7ec7\u548c\u7406\u89e3\uff0c\u5c24\u5176\u662f\u5728\u5f3a\u5316\u5b66\u4e60\u7b49\u65b0\u5174\u7814\u7a76\u65b9\u5411\u65b9\u9762\u3002", "method": "\u5bf9TQA\u7814\u7a76\u8fdb\u884c\u5168\u9762\u548c\u7ed3\u6784\u5316\u7684\u6982\u8ff0\uff0c\u91cd\u70b9\u662f\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u3002\u5bf9\u73b0\u6709\u57fa\u51c6\u548c\u4efb\u52a1\u8bbe\u7f6e\u8fdb\u884c\u5168\u9762\u5206\u7c7b\u3002\u6839\u636e\u5f53\u524d\u5efa\u6a21\u7b56\u7565\u6240\u9488\u5bf9\u7684\u6311\u6218\u5bf9\u5176\u8fdb\u884c\u5206\u7ec4\uff0c\u5e76\u5206\u6790\u5176\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "result": "\u7edf\u4e00\u4e0d\u540c\u7684\u7814\u7a76\u601d\u8def\uff0c\u5e76\u786e\u5b9a\u672a\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u4ece\u800c\u4e3aTQA\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5de9\u56fa\u7684\u57fa\u7840\u3002", "conclusion": "\u901a\u8fc7\u7edf\u4e00\u4e0d\u540c\u7684\u7814\u7a76\u601d\u8def\u5e76\u786e\u5b9a\u5f00\u653e\u6027\u95ee\u9898\uff0c\u6211\u4eec\u7684\u8c03\u67e5\u4e3aTQA\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5de9\u56fa\u7684\u57fa\u7840\uff0c\u4ece\u800c\u53ef\u4ee5\u66f4\u6df1\u5165\u5730\u4e86\u89e3\u6700\u65b0\u6280\u672f\uff0c\u5e76\u6307\u5bfc\u8be5\u5feb\u901f\u53d1\u5c55\u9886\u57df\u7684\u672a\u6765\u53d1\u5c55\u3002"}}
{"id": "2510.09649", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09649", "abs": "https://arxiv.org/abs/2510.09649", "authors": ["Khartik Uppalapati", "Bora Yimenicioglu", "Shakeel Abdulkareem", "Adan Eftekhari", "Bhavya Uppalapati", "Viraj Kamath"], "title": "TinyViT-Batten: Few-Shot Vision Transformer with Explainable Attention for Early Batten-Disease Detection on Pediatric MRI", "comment": "8 pages, 3 figures, 1 table. Submitted to International Conference on\n  Computational Intelligence and Sustainable Engineering Solutions (CISES)", "summary": "Batten disease (neuronal ceroid lipofuscinosis) is a rare pediatric\nneurodegenerative disorder whose early MRI signs are subtle and often missed.\nWe propose TinyViT-Batten, a few-shot Vision Transformer (ViT) framework to\ndetect early Batten disease from pediatric brain MRI with limited training\ncases. We distill a large teacher ViT into a 5 M-parameter TinyViT and\nfine-tune it using metric-based few-shot learning (prototypical loss with\n5-shot episodes). Our model achieves high accuracy (approximately 91%) and area\nunder ROC of at least 0.95 on a multi-site dataset of 79 genetically confirmed\nBatten-disease MRIs (27 CLN3 from the Hochstein natural-history study, 32 CLN2\nfrom an international longitudinal cohort, 12 early-manifestation CLN2 cases\nreported by Cokal et al., and 8 public Radiopaedia scans) together with 90\nage-matched controls, outperforming a 3D-ResNet and Swin-Tiny baseline. We\nfurther integrate Gradient-weighted Class Activation Mapping (Grad-CAM) to\nhighlight disease-relevant brain regions, enabling explainable predictions. The\nmodel's small size and strong performance (sensitivity greater than 90%,\nspecificity approximately 90%) demonstrates a practical AI solution for early\nBatten disease detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a TinyViT-Batten \u7684\u5c11\u6837\u672c\u89c6\u89c9 Transformer (ViT) \u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u513f\u79d1\u8111\u90e8 MRI \u4e2d\u68c0\u6d4b\u65e9\u671f Batten \u75c5\u3002", "motivation": "Batten \u75c5\uff08\u795e\u7ecf\u5143\u8721\u6837\u8102\u8910\u8d28\u6c89\u79ef\u75c7\uff09\u662f\u4e00\u79cd\u7f55\u89c1\u7684\u513f\u79d1\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\uff0c\u5176\u65e9\u671f MRI \u5f81\u8c61\u4e0d\u660e\u663e\u4e14\u5bb9\u6613\u88ab\u5ffd\u7565\u3002", "method": "\u5c06\u5927\u578b\u6559\u5e08 ViT \u63d0\u70bc\u6210\u4e00\u4e2a 5M \u53c2\u6570\u7684 TinyViT\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u5ea6\u91cf\u7684\u5c11\u6837\u672c\u5b66\u4e60\uff08\u5177\u6709 5-shot episode \u7684\u539f\u578b\u635f\u5931\uff09\u5bf9\u5176\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u8be5\u6a21\u578b\u5728\u5305\u542b 79 \u4e2a\u57fa\u56e0\u786e\u8ba4\u7684 Batten \u75c5 MRI\uff08\u6765\u81ea Hochstein \u81ea\u7136\u5386\u53f2\u7814\u7a76\u7684 27 \u4e2a CLN3\u3001\u6765\u81ea\u56fd\u9645\u7eb5\u5411\u961f\u5217\u7684 32 \u4e2a CLN2\u3001Cokal \u7b49\u4eba\u62a5\u544a\u7684 12 \u4e2a\u65e9\u671f\u8868\u73b0 CLN2 \u75c5\u4f8b\u4ee5\u53ca 8 \u4e2a\u516c\u5171 Radiopaedia \u626b\u63cf\uff09\u4ee5\u53ca 90 \u4e2a\u5e74\u9f84\u5339\u914d\u7684\u5bf9\u7167\u7684\u591a\u7ad9\u70b9\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\uff08\u7ea6 91%\uff09\u548c ROC \u66f2\u7ebf\u4e0b\u9762\u79ef\u81f3\u5c11 0.95\uff0c\u4f18\u4e8e 3D-ResNet \u548c Swin-Tiny \u57fa\u7ebf\u3002", "conclusion": "\u8be5\u6a21\u578b\u7684\u5c0f\u5c3a\u5bf8\u548c\u5f3a\u5927\u7684\u6027\u80fd\uff08\u7075\u654f\u5ea6\u5927\u4e8e 90%\uff0c\u7279\u5f02\u6027\u7ea6 90%\uff09\u8bc1\u660e\u4e86\u4e00\u79cd\u7528\u4e8e\u65e9\u671f Batten \u75c5\u68c0\u6d4b\u7684\u5b9e\u7528 AI \u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.09782", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.09782", "abs": "https://arxiv.org/abs/2510.09782", "authors": ["Yufa Zhou", "Yixiao Wang", "Xunjian Yin", "Shuyan Zhou", "Anru R. Zhang"], "title": "The Geometry of Reasoning: Flowing Logics in Representation Space", "comment": "Code: https://github.com/MasterZhou1/Reasoning-Flow", "summary": "We study how large language models (LLMs) ``think'' through their\nrepresentation space. We propose a novel geometric framework that models an\nLLM's reasoning as flows -- embedding trajectories evolving where logic goes.\nWe disentangle logical structure from semantics by employing the same natural\ndeduction propositions with varied semantic carriers, allowing us to test\nwhether LLMs internalize logic beyond surface form. This perspective connects\nreasoning with geometric quantities such as position, velocity, and curvature,\nenabling formal analysis in representation and concept spaces. Our theory\nestablishes: (1) LLM reasoning corresponds to smooth flows in representation\nspace, and (2) logical statements act as local controllers of these flows'\nvelocities. Using learned representation proxies, we design controlled\nexperiments to visualize and quantify reasoning flows, providing empirical\nvalidation of our theoretical framework. Our work serves as both a conceptual\nfoundation and practical tools for studying reasoning phenomenon, offering a\nnew lens for interpretability and formal analysis of LLMs' behavior.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5982\u4f55\u901a\u8fc7\u5176\u8868\u793a\u7a7a\u95f4\u8fdb\u884c\u201c\u601d\u8003\u201d\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u51e0\u4f55\u6846\u67b6\uff0c\u5c06LLM\u7684\u63a8\u7406\u5efa\u6a21\u4e3a\u6d41\u2014\u2014\u5d4c\u5165\u8f68\u8ff9\u968f\u7740\u903b\u8f91\u7684\u53d1\u5c55\u800c\u6f14\u53d8\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u91c7\u7528\u5177\u6709\u4e0d\u540c\u8bed\u4e49\u8f7d\u4f53\u7684\u76f8\u540c\u81ea\u7136\u6f14\u7ece\u547d\u9898\uff0c\u5c06\u903b\u8f91\u7ed3\u6784\u4e0e\u8bed\u4e49\u5206\u79bb\uff0c\u4ece\u800c\u80fd\u591f\u6d4b\u8bd5LLM\u662f\u5426\u5728\u8868\u9762\u5f62\u5f0f\u4e4b\u5916\u5185\u5316\u4e86\u903b\u8f91\u3002\u8fd9\u79cd\u89c6\u89d2\u5c06\u63a8\u7406\u4e0e\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u66f2\u7387\u7b49\u51e0\u4f55\u91cf\u8054\u7cfb\u8d77\u6765\uff0c\u4ece\u800c\u80fd\u591f\u5728\u8868\u793a\u548c\u6982\u5ff5\u7a7a\u95f4\u4e2d\u8fdb\u884c\u5f62\u5f0f\u5206\u6790\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u7406\u89e3\u5176\u5185\u5728\u673a\u7406", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u51e0\u4f55\u6846\u67b6\uff0c\u5c06LLM\u7684\u63a8\u7406\u5efa\u6a21\u4e3a\u6d41\uff0c\u5e76\u91c7\u7528\u5177\u6709\u4e0d\u540c\u8bed\u4e49\u8f7d\u4f53\u7684\u76f8\u540c\u81ea\u7136\u6f14\u7ece\u547d\u9898\uff0c\u5c06\u903b\u8f91\u7ed3\u6784\u4e0e\u8bed\u4e49\u5206\u79bb\u3002", "result": "LLM\u63a8\u7406\u5bf9\u5e94\u4e8e\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u5e73\u6ed1\u6d41\uff0c\u5e76\u4e14\u903b\u8f91\u8bed\u53e5\u5145\u5f53\u8fd9\u4e9b\u6d41\u7684\u901f\u5ea6\u7684\u5c40\u90e8\u63a7\u5236\u5668\u3002\u901a\u8fc7\u4f7f\u7528\u5b66\u4e60\u7684\u8868\u793a\u4ee3\u7406\uff0c\u8bbe\u8ba1\u53d7\u63a7\u5b9e\u9a8c\u6765\u53ef\u89c6\u5316\u548c\u91cf\u5316\u63a8\u7406\u6d41\uff0c\u4ece\u800c\u4e3a\u7406\u8bba\u6846\u67b6\u63d0\u4f9b\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u65e2\u662f\u7814\u7a76\u63a8\u7406\u73b0\u8c61\u7684\u6982\u5ff5\u57fa\u7840\uff0c\u53c8\u662f\u5b9e\u7528\u5de5\u5177\uff0c\u4e3aLLM\u884c\u4e3a\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5f62\u5f0f\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2510.09643", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09643", "abs": "https://arxiv.org/abs/2510.09643", "authors": ["Yuguang Liu", "Yiyun Miao", "Luyao Xia"], "title": "Direct Routing Gradient (DRGrad): A Personalized Information Surgery for Multi-Task Learning (MTL) Recommendations", "comment": null, "summary": "Multi-task learning (MTL) has emerged as a successful strategy in\nindustrial-scale recommender systems, offering significant advantages such as\ncapturing diverse users' interests and accurately detecting different behaviors\nlike ``click\" or ``dwell time\". However, negative transfer and the seesaw\nphenomenon pose challenges to MTL models due to the complex and often\ncontradictory task correlations in real-world recommendations. To address the\nproblem while making better use of personalized information, we propose a\npersonalized Direct Routing Gradient framework (DRGrad), which consists of\nthree key components: router, updater and personalized gate network. DRGrad\njudges the stakes between tasks in the training process, which can leverage all\nvalid gradients for the respective task to reduce conflicts. We evaluate the\nefficiency of DRGrad on complex MTL using a real-world recommendation dataset\nwith 15 billion samples. The results show that DRGrad's superior performance\nover competing state-of-the-art MTL models, especially in terms of AUC (Area\nUnder the Curve) metrics, indicating that it effectively manages task conflicts\nin multi-task learning environments without increasing model complexity, while\nalso addressing the deficiencies in noise processing. Moreover, experiments on\nthe public Census-income dataset and Synthetic dataset, have demonstrated the\ncapability of DRGrad in judging and routing the stakes between tasks with\nvarying degrees of correlation and personalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDRGrad\u7684\u4e2a\u6027\u5316\u76f4\u63a5\u8def\u7531\u68af\u5ea6\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u4e2d\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u8d1f\u8fc1\u79fb\u548c\u8df7\u8df7\u677f\u73b0\u8c61\u3002", "motivation": "\u73b0\u5b9e\u63a8\u8350\u7cfb\u7edf\u4e2d\u590d\u6742\u4e14\u7ecf\u5e38\u77db\u76fe\u7684\u4efb\u52a1\u76f8\u5173\u6027\u7ed9\u591a\u4efb\u52a1\u5b66\u4e60\u6a21\u578b\u5e26\u6765\u4e86\u8d1f\u8fc1\u79fb\u548c\u8df7\u8df7\u677f\u73b0\u8c61\u7684\u6311\u6218\u3002", "method": "DRGrad\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u8def\u7531\u5668\u3001\u66f4\u65b0\u5668\u548c\u4e2a\u6027\u5316\u95e8\u7f51\u7edc\uff0c\u7528\u4e8e\u5224\u65ad\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4efb\u52a1\u4e4b\u95f4\u7684\u5229\u5bb3\u5173\u7cfb\uff0c\u4ece\u800c\u5229\u7528\u6240\u6709\u6709\u6548\u68af\u5ea6\u6765\u51cf\u5c11\u51b2\u7a81\u3002", "result": "\u5728\u5305\u542b150\u4ebf\u4e2a\u6837\u672c\u7684\u771f\u5b9e\u63a8\u8350\u6570\u636e\u96c6\u4e0a\uff0cDRGrad\u7684\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728AUC\u6307\u6807\u65b9\u9762\uff0c\u8868\u660e\u5b83\u80fd\u6709\u6548\u7ba1\u7406\u591a\u4efb\u52a1\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u51b2\u7a81\uff0c\u4e14\u4e0d\u589e\u52a0\u6a21\u578b\u590d\u6742\u6027\u3002\u5728Census-income\u6570\u636e\u96c6\u548cSynthetic\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u4e5f\u8bc1\u660e\u4e86DRGrad\u5728\u5224\u65ad\u548c\u8def\u7531\u5177\u6709\u4e0d\u540c\u7a0b\u5ea6\u76f8\u5173\u6027\u548c\u4e2a\u6027\u5316\u7684\u4efb\u52a1\u4e4b\u95f4\u7684\u5229\u5bb3\u5173\u7cfb\u7684\u80fd\u529b\u3002", "conclusion": "DRGrad\u80fd\u6709\u6548\u7ba1\u7406\u591a\u4efb\u52a1\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u51b2\u7a81\uff0c\u4e14\u4e0d\u589e\u52a0\u6a21\u578b\u590d\u6742\u6027\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u566a\u58f0\u5904\u7406\u65b9\u9762\u7684\u7f3a\u9677\u3002"}}
{"id": "2510.10127", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10127", "abs": "https://arxiv.org/abs/2510.10127", "authors": ["Qiya Yang", "Xiaoxi Liang", "Zeping Xiao", "Yingjie Deng", "Yalong Wang", "Yongqi Liu", "Han Li"], "title": "Breaking the Likelihood Trap: Consistent Generative Recommendation with Graph-structured Model", "comment": null, "summary": "Reranking, as the final stage of recommender systems, demands real-time\ninference, accuracy, and diversity. It plays a crucial role in determining the\nfinal exposure, directly influencing user experience. Recently, generative\nreranking has gained increasing attention for its strong ability to model\ncomplex dependencies among items. However, most existing methods suffer from\nthe \"likelihood trap\", where high-likelihood sequences are often perceived as\nlow-quality by humans. These models tend to repeatedly recommend a set of\nhigh-frequency items, resulting in list homogeneity, thereby limiting user\nengagement. In this work, we propose Consistent Graph-structured Generative\nRecommendation (Congrats), a novel generative reranking framework. To break the\nlikelihood trap, we introduce a novel graph-structured decoder that can capture\ndiverse sequences along multiple paths. This design not only expands the\ndecoding space to promote diversity, but also improves prediction accuracy by\nimplicit item dependencies derived from vertex transitions. Furthermore, we\ndesign a differentiable cascade system that incorporates an evaluator, enabling\nthe model to learn directly from user preferences as the training objective.\nExtensive offline experiments validate the superior performance of Congrats\nover state-of-the-art reranking methods. Moreover, Congrats has been evaluated\non a large-scale video-sharing app, Kuaishou, with over 300 million daily\nactive users, demonstrating that our approach significantly improves both\nrecommendation quality and diversity, validating our effectiveness in practical\nindustrial environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCongrats\u7684\u751f\u6210\u5f0f\u91cd\u6392\u5e8f\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u201c\u53ef\u80fd\u6027\u9677\u9631\u201d\u95ee\u9898\uff0c\u63d0\u9ad8\u63a8\u8350\u7684\u591a\u6837\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u91cd\u6392\u5e8f\u65b9\u6cd5\u5bb9\u6613\u9677\u5165\u201c\u53ef\u80fd\u6027\u9677\u9631\u201d\uff0c\u5bfc\u81f4\u63a8\u8350\u5217\u8868\u540c\u8d28\u5316\uff0c\u9650\u5236\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u56fe\u7ed3\u6784\u7684\u89e3\u7801\u5668\uff0c\u53ef\u4ee5\u6355\u6349\u591a\u4e2a\u8def\u5f84\u4e0a\u7684\u4e0d\u540c\u5e8f\u5217\uff0c\u6269\u5927\u4e86\u89e3\u7801\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u9876\u70b9\u8f6c\u6362\u63a8\u5bfc\u51fa\u7684\u9690\u5f0f\u9879\u76ee\u4f9d\u8d56\u5173\u7cfb\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53ef\u5fae\u7684\u7ea7\u8054\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5305\u542b\u4e00\u4e2a\u8bc4\u4f30\u5668\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u76f4\u63a5\u4ece\u7528\u6237\u504f\u597d\u4e2d\u5b66\u4e60\u3002", "result": "\u5927\u91cf\u7684\u79bb\u7ebf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86Congrats\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u91cd\u6392\u5e8f\u65b9\u6cd5\u3002\u5728\u5feb\u624b\u4e0a\u7684\u5927\u89c4\u6a21\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u8350\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002", "conclusion": "Congrats\u5728\u5b9e\u9645\u5de5\u4e1a\u73af\u5883\u4e2d\u6709\u6548\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u63a8\u8350\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002"}}
{"id": "2510.10115", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.10115", "abs": "https://arxiv.org/abs/2510.10115", "authors": ["Kai Cao", "Yucong Duan", "Wensheng Gan"], "title": "Targeted Sequential Pattern Mining with High Average Utility", "comment": "preprint, 9 figures, 3 tables", "summary": "Incorporating utility into targeted pattern mining can address the practical\nlimitations of traditional frequency-based approaches. However, utility-based\nmethods often suffer from generating a large number of long and complicated\nsequences. To improve pattern relevance and interpretability, average utility\nprovides a more balanced metric by considering both utility and sequence\nlength. Moreover, incorporating user-defined query targets into the mining\nprocess enhances usability and interactivity by retaining only patterns\ncontaining user-specified goals. To address challenges related to mining\nefficiency in large-scale, long-sequence datasets, this study introduces\naverage utility into targeted sequential pattern mining. A novel algorithm,\nTAUSQ-PG, is designed to find targeted high average utility sequential\npatterns. It incorporates efficient filtering and pruning strategies, tighter\nupper bound models, as well as novel specialized evaluation metrics and query\nflags tailored to this task. Extensive comparative experiments on different\ndatasets demonstrate that TAUSQ-PG effectively controls the candidate set size,\nthereby reducing redundant sequence generation and significantly improving\nruntime and memory efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5TAUSQ-PG\uff0c\u7528\u4e8e\u5728\u5927\u578b\u957f\u5e8f\u5217\u6570\u636e\u96c6\u4e2d\u6316\u6398\u76ee\u6807\u9ad8\u5e73\u5747\u6548\u7528\u5e8f\u5217\u6a21\u5f0f\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u9891\u7387\u7684\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u57fa\u4e8e\u6548\u7528\u7684\u65b9\u6cd5\u53c8\u4f1a\u751f\u6210\u5927\u91cf\u5197\u957f\u590d\u6742\u7684\u5e8f\u5217\u3002\u4e3a\u4e86\u63d0\u9ad8\u6a21\u5f0f\u7684\u76f8\u5173\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u589e\u5f3a\u53ef\u7528\u6027\u548c\u4ea4\u4e92\u6027\uff0c\u672c\u6587\u5c06\u5e73\u5747\u6548\u7528\u5f15\u5165\u5230\u76ee\u6807\u5e8f\u5217\u6a21\u5f0f\u6316\u6398\u4e2d\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u540d\u4e3aTAUSQ-PG\u7684\u65b0\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u7ed3\u5408\u4e86\u6709\u6548\u7684\u8fc7\u6ee4\u548c\u526a\u679d\u7b56\u7565\u3001\u66f4\u4e25\u683c\u7684\u4e0a\u9650\u6a21\u578b\u4ee5\u53ca\u4e13\u95e8\u7684\u8bc4\u4f30\u6307\u6807\u548c\u67e5\u8be2\u6807\u5fd7\u3002", "result": "\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5bf9\u6bd4\u5b9e\u9a8c\u8868\u660e\uff0cTAUSQ-PG\u6709\u6548\u5730\u63a7\u5236\u4e86\u5019\u9009\u96c6\u5927\u5c0f\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u5197\u4f59\u5e8f\u5217\u7684\u751f\u6210\uff0c\u5e76\u663e\u7740\u63d0\u9ad8\u4e86\u8fd0\u884c\u65f6\u548c\u5185\u5b58\u6548\u7387\u3002", "conclusion": "\u672c\u6587\u7814\u7a76\u89e3\u51b3\u4e86\u5728\u5927\u578b\u957f\u5e8f\u5217\u6570\u636e\u96c6\u4e2d\u6316\u6398\u76ee\u6807\u9ad8\u5e73\u5747\u6548\u7528\u5e8f\u5217\u6a21\u5f0f\u7684\u6311\u6218\uff0c\u63d0\u51fa\u7684TAUSQ-PG\u7b97\u6cd5\u5728\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.09695", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09695", "abs": "https://arxiv.org/abs/2510.09695", "authors": ["Yanran Chen", "Lynn Greschner", "Roman Klinger", "Michael Klenk", "Steffen Eger"], "title": "Emotionally Charged, Logically Blurred: AI-driven Emotional Framing Impairs Human Fallacy Detection", "comment": "Initial submission", "summary": "Logical fallacies are common in public communication and can mislead\naudiences; fallacious arguments may still appear convincing despite lacking\nsoundness, because convincingness is inherently subjective. We present the\nfirst computational study of how emotional framing interacts with fallacies and\nconvincingness, using large language models (LLMs) to systematically change\nemotional appeals in fallacious arguments. We benchmark eight LLMs on injecting\nemotional appeal into fallacious arguments while preserving their logical\nstructures, then use the best models to generate stimuli for a human study. Our\nresults show that LLM-driven emotional framing reduces human fallacy detection\nin F1 by 14.5% on average. Humans perform better in fallacy detection when\nperceiving enjoyment than fear or sadness, and these three emotions also\ncorrelate with significantly higher convincingness compared to neutral or other\nemotion states. Our work has implications for AI-driven emotional manipulation\nin the context of fallacious argumentation.", "AI": {"tldr": "\u7814\u7a76\u4e86\u60c5\u611f\u6846\u67b6\u5982\u4f55\u4e0e\u8c2c\u8bef\u548c\u8bf4\u670d\u529b\u76f8\u4e92\u4f5c\u7528\u3002", "motivation": "\u516c\u5171\u4ea4\u6d41\u4e2d\u5b58\u5728\u903b\u8f91\u8c2c\u8bef\u4f1a\u8bef\u5bfc\u89c2\u4f17\uff0c\u8c2c\u8bef\u6027\u8bba\u70b9\u53ef\u80fd\u4ecd\u7136\u5177\u6709\u8bf4\u670d\u529b\uff0c\u56e0\u4e3a\u8bf4\u670d\u529b\u672c\u8d28\u4e0a\u662f\u4e3b\u89c2\u7684\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7cfb\u7edf\u5730\u6539\u53d8\u8c2c\u8bef\u6027\u8bba\u70b9\u4e2d\u7684\u60c5\u611f\u8bc9\u6c42\u3002\u5bf9\u516b\u4e2a LLM \u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u5c06\u60c5\u611f\u8bc9\u6c42\u6ce8\u5165\u8c2c\u8bef\u6027\u8bba\u70b9\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u903b\u8f91\u7ed3\u6784\uff0c\u7136\u540e\u4f7f\u7528\u6700\u4f73\u6a21\u578b\u751f\u6210\u4eba\u7c7b\u7814\u7a76\u7684\u523a\u6fc0\u3002", "result": "LLM \u9a71\u52a8\u7684\u60c5\u611f\u6846\u67b6\u4f7f\u4eba\u7c7b\u7684\u8c2c\u8bef\u68c0\u6d4b\u7387\u5e73\u5747\u964d\u4f4e\u4e86 14.5%\u3002\u4e0e\u4e2d\u6027\u6216\u5176\u4ed6\u60c5\u7eea\u72b6\u6001\u76f8\u6bd4\uff0c\u4eba\u7c7b\u5728\u611f\u77e5\u5feb\u4e50\u65f6\u6bd4\u6050\u60e7\u6216\u60b2\u4f24\u65f6\u5728\u8c2c\u8bef\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u5e76\u4e14\u8fd9\u4e09\u79cd\u60c5\u7eea\u4e5f\u4e0e\u660e\u663e\u66f4\u9ad8\u7684\u8bf4\u670d\u529b\u76f8\u5173\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5bf9\u8c2c\u8bef\u8bba\u8bc1\u80cc\u666f\u4e0b\u7684\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u60c5\u611f\u64cd\u7eb5\u5177\u6709\u5f71\u54cd\u3002"}}
{"id": "2510.09653", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09653", "abs": "https://arxiv.org/abs/2510.09653", "authors": ["Ranjan Sapkota", "Manoj Karkee"], "title": "Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and YOLOv5 Object Detectors for Computer Vision and Pattern Recognition", "comment": "16 pages, 5 Tables, 5 Figures", "summary": "This paper presents a comprehensive overview of the Ultralytics YOLO(You Only\nLook Once) family of object detectors, focusing the architectural evolution,\nbenchmarking, deployment perspectives, and future challenges. The review begins\nwith the most recent release, YOLO26 (YOLOv26), which introduces key\ninnovations including Distribution Focal Loss (DFL) removal, native NMS-free\ninference, Progressive Loss Balancing (ProgLoss), Small-Target-Aware Label\nAssignment (STAL), and the MuSGD optimizer for stable training. The progression\nis then traced through YOLO11, with its hybrid task assignment and\nefficiency-focused modules; YOLOv8, which advanced with a decoupled detection\nhead and anchor-free predictions; and YOLOv5, which established the modular\nPyTorch foundation that enabled modern YOLO development. Benchmarking on the MS\nCOCO dataset provides a detailed quantitative comparison of YOLOv5, YOLOv8,\nYOLO11, and YOLO26, alongside cross-comparisons with YOLOv12, YOLOv13, RT-DETR,\nand DEIM. Metrics including precision, recall, F1 score, mean Average\nPrecision, and inference speed are analyzed to highlight trade-offs between\naccuracy and efficiency. Deployment and application perspectives are further\ndiscussed, covering export formats, quantization strategies, and real-world use\nin robotics, agriculture, surveillance, and manufacturing. Finally, the paper\nidentifies challenges and future directions, including dense-scene limitations,\nhybrid CNN-Transformer integration, open-vocabulary detection, and edge-aware\ntraining approaches.", "AI": {"tldr": "\u672c\u6587\u5168\u9762\u6982\u8ff0\u4e86 Ultralytics YOLO \u5bf9\u8c61\u68c0\u6d4b\u5668\u7cfb\u5217\uff0c\u91cd\u70b9\u5173\u6ce8\u67b6\u6784\u6f14\u53d8\u3001\u57fa\u51c6\u6d4b\u8bd5\u3001\u90e8\u7f72\u524d\u666f\u548c\u672a\u6765\u6311\u6218\u3002", "motivation": "\u65e8\u5728\u5c55\u793a YOLO \u7cfb\u5217\u7684\u53d1\u5c55\u5386\u7a0b\uff0c\u4ee5\u53ca\u4e0d\u540c\u7248\u672c\u4e4b\u95f4\u7684\u4f18\u52a3\u3002", "method": "\u901a\u8fc7\u5728 MS COCO \u6570\u636e\u96c6\u4e0a\u5bf9 YOLOv5\u3001YOLOv8\u3001YOLO11 \u548c YOLO26 \u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u4e0e\u5176\u4ed6\u6a21\u578b\u8fdb\u884c\u4ea4\u53c9\u6bd4\u8f83\uff0c\u5206\u6790\u7cbe\u5ea6\u548c\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "\u8be6\u7ec6\u5bf9\u6bd4\u4e86 YOLOv5\u3001YOLOv8\u3001YOLO11 \u548c YOLO26 \u5728 MS COCO \u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u4e0e\u5176\u4ed6\u6a21\u578b\u8fdb\u884c\u4e86\u4ea4\u53c9\u6bd4\u8f83\uff0c\u7a81\u51fa\u4e86\u7cbe\u5ea6\u548c\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u603b\u7ed3\u4e86 YOLO \u7cfb\u5217\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u5305\u62ec\u5bc6\u96c6\u573a\u666f\u9650\u5236\u3001\u6df7\u5408 CNN-Transformer \u96c6\u6210\u3001\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u548c\u8fb9\u7f18\u611f\u77e5\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2510.09801", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09801", "abs": "https://arxiv.org/abs/2510.09801", "authors": ["Valerie Chen", "Rohit Malhotra", "Xingyao Wang", "Juan Michelini", "Xuhui Zhou", "Aditya Bharat Soni", "Hoang H. Tran", "Calvin Smith", "Ameet Talwalkar", "Graham Neubig"], "title": "How can we assess human-agent interactions? Case studies in software agent design", "comment": null, "summary": "LLM-powered agents are both a promising new technology and a source of\ncomplexity, where choices about models, tools, and prompting can affect their\nusefulness. While numerous benchmarks measure agent accuracy across domains,\nthey mostly assume full automation, failing to represent the collaborative\nnature of real-world use cases. In this paper, we make two major steps towards\nthe rigorous assessment of human-agent interactions. First, we propose PULSE, a\nframework for more efficient human-centric evaluation of agent designs, which\ncomprises collecting user feedback, training an ML model to predict user\nsatisfaction, and computing results by combining human satisfaction ratings\nwith model-generated pseudo-labels. Second, we deploy the framework on a\nlarge-scale web platform built around the open-source software agent OpenHands,\ncollecting in-the-wild usage data across over 15k users. We conduct case\nstudies around how three agent design decisions -- choice of LLM backbone,\nplanning strategy, and memory mechanisms -- impact developer satisfaction\nrates, yielding practical insights for software agent design. We also show how\nour framework can lead to more robust conclusions about agent design, reducing\nconfidence intervals by 40\\% compared to a standard A/B test. Finally, we find\nsubstantial discrepancies between in-the-wild results and benchmark performance\n(e.g., the anti-correlation between results comparing claude-sonnet-4 and\ngpt-5), underscoring the limitations of benchmark-driven evaluation. Our\nfindings provide guidance for evaluations of LLM agents with humans and\nidentify opportunities for better agent designs.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86PULSE\u6846\u67b6\uff0c\u7528\u4e8e\u66f4\u6709\u6548\u5730\u8bc4\u4f30\u4eba\u673a\u4ea4\u4e92\u7684LLM Agent\u8bbe\u8ba1\u3002", "motivation": "\u73b0\u6709\u7684Agent\u8bc4\u4f30\u57fa\u51c6\u4e3b\u8981\u5047\u8bbe\u5b8c\u5168\u81ea\u52a8\u5316\uff0c\u5ffd\u7565\u4e86\u771f\u5b9e\u4e16\u754c\u7528\u4f8b\u4e2d\u7684\u534f\u4f5c\u6027\u3002", "method": "\u8be5\u6846\u67b6\u5305\u62ec\u6536\u96c6\u7528\u6237\u53cd\u9988\u3001\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u7528\u6237\u6ee1\u610f\u5ea6\uff0c\u5e76\u5c06\u7528\u6237\u6ee1\u610f\u5ea6\u8bc4\u5206\u4e0e\u6a21\u578b\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u76f8\u7ed3\u5408\u3002", "result": "\u901a\u8fc7\u5927\u89c4\u6a21Web\u5e73\u53f0\u6536\u96c6\u4e86\u8d85\u8fc715k\u7528\u6237\u7684\u771f\u5b9e\u4f7f\u7528\u6570\u636e\uff0c\u6848\u4f8b\u7814\u7a76\u8868\u660eLLM\u9aa8\u5e72\u3001\u89c4\u5212\u7b56\u7565\u548c\u8bb0\u5fc6\u673a\u5236\u7684\u9009\u62e9\u4f1a\u5f71\u54cd\u5f00\u53d1\u8005\u6ee1\u610f\u5ea6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6807\u51c6A/B\u6d4b\u8bd5\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u66f4\u7a33\u5065\u5730\u8bc4\u4f30Agent\u8bbe\u8ba1\uff0c\u5e76\u5c06\u7f6e\u4fe1\u533a\u95f4\u964d\u4f4e40%\u3002\u6b64\u5916\uff0c\u771f\u5b9e\u6570\u636e\u4e0e\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5f3a\u8c03\u4e86\u57fa\u51c6\u9a71\u52a8\u8bc4\u4f30\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.09644", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09644", "abs": "https://arxiv.org/abs/2510.09644", "authors": ["Shaharyar Alam Ansari", "Mohammad Luqman", "Aasim Zafar", "Savir Ali"], "title": "Enhanced Urban Traffic Management Using CCTV Surveillance Videos and Multi-Source Data Current State Prediction and Frequent Episode Mining", "comment": "24 pages, 9 figures", "summary": "Rapid urbanization has intensified traffic congestion, environmental strain,\nand inefficiencies in transportation systems, creating an urgent need for\nintelligent and adaptive traffic management solutions. Conventional systems\nrelying on static signals and manual monitoring are inadequate for the dynamic\nnature of modern traffic. This research aims to develop a unified framework\nthat integrates CCTV surveillance videos with multi-source data descriptors to\nenhance real-time urban traffic prediction. The proposed methodology\nincorporates spatio-temporal feature fusion, Frequent Episode Mining for\nsequential traffic pattern discovery, and a hybrid LSTM-Transformer model for\nrobust traffic state forecasting. The framework was evaluated on the CityFlowV2\ndataset comprising 313,931 annotated bounding boxes across 46 cameras. It\nachieved a high prediction accuracy of 98.46 percent, with a macro precision of\n0.9800, macro recall of 0.9839, and macro F1-score of 0.9819. FEM analysis\nrevealed significant sequential patterns such as moderate-congested transitions\nwith confidence levels exceeding 55 percent. The 46 sustained congestion alerts\nare system-generated, which shows practical value for proactive congestion\nmanagement. This emphasizes the need for the incorporation of video stream\nanalytics with data from multiple sources for the design of real-time,\nresponsive, adaptable multi-level intelligent transportation systems, which\nmakes urban mobility smarter and safer.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u95ed\u8def\u7535\u89c6\u76d1\u63a7\u89c6\u9891\u4e0e\u591a\u6e90\u6570\u636e\u63cf\u8ff0\u7b26\uff0c\u4ee5\u589e\u5f3a\u5b9e\u65f6\u57ce\u5e02\u4ea4\u901a\u9884\u6d4b\u3002", "motivation": "\u4f20\u7edf\u7684\u4ea4\u901a\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u9759\u6001\u4fe1\u53f7\u548c\u4eba\u5de5\u76d1\u63a7\uff0c\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u73b0\u4ee3\u4ea4\u901a\u7684\u52a8\u6001\u7279\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u667a\u80fd\u548c\u81ea\u9002\u5e94\u7684\u4ea4\u901a\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u65f6\u7a7a\u7279\u5f81\u878d\u5408\u3001\u7528\u4e8e\u5e8f\u5217\u4ea4\u901a\u6a21\u5f0f\u53d1\u73b0\u7684\u9891\u7e41 \u044d\u043f\u0438\u0437\u043e\u0434 \u6316\u6398\u4ee5\u53ca\u7528\u4e8e\u7a33\u5065\u4ea4\u901a\u72b6\u6001\u9884\u6d4b\u7684\u6df7\u5408 LSTM-Transformer \u6a21\u578b\u3002", "result": "\u8be5\u6846\u67b6\u5728 CityFlowV2 \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5b9e\u73b0\u4e86 98.46% \u7684\u9ad8\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5b8f\u7cbe\u5ea6\u4e3a 0.9800\uff0c\u5b8f\u53ec\u56de\u7387\u4e3a 0.9839\uff0c\u5b8f F1 \u5206\u6570\u4e3a 0.9819\u3002\u9891\u7e41 \u044d\u043f\u0438\u0437\u043e\u0434 \u6316\u6398\u5206\u6790\u63ed\u793a\u4e86\u663e\u7740\u7684\u5e8f\u5217\u6a21\u5f0f\uff0c\u4f8b\u5982\u7f6e\u4fe1\u6c34\u5e73\u8d85\u8fc7 55% \u7684\u4e2d\u5ea6\u62e5\u5835\u8fc7\u6e21\u3002", "conclusion": "\u5f3a\u8c03\u9700\u8981\u5c06\u89c6\u9891\u6d41\u5206\u6790\u4e0e\u6765\u81ea\u591a\u4e2a\u6765\u6e90\u7684\u6570\u636e\u76f8\u7ed3\u5408\uff0c\u4ee5\u8bbe\u8ba1\u5b9e\u65f6\u3001\u54cd\u5e94\u8fc5\u901f\u3001\u9002\u5e94\u6027\u5f3a\u7684\u591a\u7ea7\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\uff0c\u4ece\u800c\u4f7f\u57ce\u5e02\u4ea4\u901a\u66f4\u667a\u80fd\u3001\u66f4\u5b89\u5168\u3002"}}
{"id": "2510.10419", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10419", "abs": "https://arxiv.org/abs/2510.10419", "authors": ["Weiwei Sun", "Keyi Kong", "Xinyu Ma", "Shuaiqiang Wang", "Dawei Yin", "Maarten de Rijke", "Zhaochun Ren", "Yiming Yang"], "title": "ZeroGR: A Generalizable and Scalable Framework for Zero-Shot Generative Retrieval", "comment": null, "summary": "Generative retrieval (GR) reformulates information retrieval (IR) by framing\nit as the generation of document identifiers (docids), thereby enabling an\nend-to-end optimization and seamless integration with generative language\nmodels (LMs). Despite notable progress under supervised training, GR still\nstruggles to generalize to zero-shot IR scenarios, which are prevalent in\nreal-world applications. To tackle this challenge, we propose \\textsc{ZeroGR},\na zero-shot generative retrieval framework that leverages natural language\ninstructions to extend GR across a wide range of IR tasks. Specifically,\n\\textsc{ZeroGR} is composed of three key components: (i) an LM-based docid\ngenerator that unifies heterogeneous documents (e.g., text, tables, code) into\nsemantically meaningful docids; (ii) an instruction-tuned query generator that\ngenerates diverse types of queries from natural language task descriptions to\nenhance corpus indexing; and (iii) a reverse annealing decoding strategy to\nbalance precision and recall during docid generation. We investigate the impact\nof instruction fine-tuning scale and find that performance consistently\nimproves as the number of IR tasks encountered during training increases.\nEmpirical results on the BEIR and MAIR benchmarks demonstrate that\n\\textsc{ZeroGR} outperforms strong dense retrieval and generative baselines in\nzero-shot settings, establishing a new state-of-the-art for instruction-driven\nGR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aZeroGR\u7684\u96f6\u6837\u672c\u751f\u6210\u68c0\u7d22\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5c06GR\u6269\u5c55\u5230\u5404\u79cdIR\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u6210\u68c0\u7d22(GR)\u5728\u6709\u76d1\u7763\u8bad\u7ec3\u4e0b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u96be\u4ee5\u63a8\u5e7f\u5230\u5b9e\u9645\u5e94\u7528\u4e2d\u666e\u904d\u5b58\u5728\u7684\u96f6\u6837\u672cIR\u573a\u666f\u3002", "method": "ZeroGR\u5305\u62ec\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a(i) \u57fa\u4e8eLM\u7684docid\u751f\u6210\u5668\uff0c\u5c06\u5f02\u6784\u6587\u6863\u7edf\u4e00\u4e3a\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684docid\uff1b(ii) \u6307\u4ee4\u8c03\u4f18\u7684\u67e5\u8be2\u751f\u6210\u5668\uff0c\u4ece\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u751f\u6210\u4e0d\u540c\u7c7b\u578b\u7684\u67e5\u8be2\uff0c\u4ee5\u589e\u5f3a\u8bed\u6599\u5e93\u7d22\u5f15\uff1b(iii) \u53cd\u5411\u9000\u706b\u89e3\u7801\u7b56\u7565\uff0c\u4ee5\u5e73\u8861docid\u751f\u6210\u671f\u95f4\u7684\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u3002", "result": "\u5728BEIR\u548cMAIR\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cZeroGR\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u5f3a\u5927\u7684\u5bc6\u96c6\u68c0\u7d22\u548c\u751f\u6210\u57fa\u7ebf\uff0c\u4e3a\u6307\u4ee4\u9a71\u52a8\u7684GR\u5efa\u7acb\u4e86\u65b0\u7684\u6280\u672f\u6c34\u5e73\u3002", "conclusion": "\u6307\u4ee4\u5fae\u8c03\u89c4\u6a21\u7684\u589e\u52a0\u53ef\u4ee5\u6301\u7eed\u63d0\u9ad8\u6027\u80fd\u3002"}}
{"id": "2510.10123", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10123", "abs": "https://arxiv.org/abs/2510.10123", "authors": ["Joydeep Chandra", "Satyam Kumar Navneet", "Yong Zhang"], "title": "The Hybrid Multimodal Graph Index (HMGI): A Comprehensive Framework for Integrated Relational and Vector Search", "comment": null, "summary": "The proliferation of complex, multimodal datasets has exposed a critical gap\nbetween the capabilities of specialized vector databases and traditional graph\ndatabases. While vector databases excel at semantic similarity search, they\nlack the capacity for deep relational querying. Conversely, graph databases\nmaster complex traversals but are not natively optimized for high-dimensional\nvector search. This paper introduces the Hybrid Multimodal Graph Index (HMGI),\na novel framework designed to bridge this gap by creating a unified system for\nefficient, hybrid queries on multimodal data. HMGI leverages the native graph\ndatabase architecture and integrated vector search capabilities, exemplified by\nplatforms like Neo4j, to combine Approximate Nearest Neighbor Search (ANNS)\nwith expressive graph traversal queries. Key innovations of the HMGI framework\ninclude modality-aware partitioning of embeddings to optimize index structure\nand query performance, and a system for adaptive, low-overhead index updates to\nsupport dynamic data ingestion, drawing inspiration from the architectural\nprinciples of systems like TigerVector. By integrating semantic similarity\nsearch directly with relational context, HMGI aims to outperform pure vector\ndatabases like Milvus in complex, relationship-heavy query scenarios and\nachieve sub-linear query times for hybrid tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u591a\u6a21\u56fe\u7d22\u5f15\uff08HMGI\uff09\uff0c\u65e8\u5728\u5f25\u5408\u5411\u91cf\u6570\u636e\u5e93\u548c\u4f20\u7edf\u56fe\u6570\u636e\u5e93\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u591a\u6a21\u6570\u636e\u7684\u6709\u6548\u6df7\u5408\u67e5\u8be2\u3002", "motivation": "\u4e13\u95e8\u7684\u5411\u91cf\u6570\u636e\u5e93\u548c\u4f20\u7edf\u56fe\u6570\u636e\u5e93\u4e4b\u95f4\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u5dee\u8ddd\u3002\u5411\u91cf\u6570\u636e\u5e93\u64c5\u957f\u8bed\u4e49\u76f8\u4f3c\u6027\u641c\u7d22\uff0c\u4f46\u7f3a\u4e4f\u6df1\u5ea6\u5173\u7cfb\u67e5\u8be2\u7684\u80fd\u529b\u3002\u56fe\u6570\u636e\u5e93\u64c5\u957f\u590d\u6742\u904d\u5386\uff0c\u4f46\u672a\u9488\u5bf9\u9ad8\u7ef4\u5411\u91cf\u641c\u7d22\u8fdb\u884c\u539f\u751f\u4f18\u5316\u3002", "method": "HMGI\u5229\u7528\u539f\u751f\u56fe\u6570\u636e\u5e93\u67b6\u6784\u548c\u96c6\u6210\u7684\u5411\u91cf\u641c\u7d22\u80fd\u529b\uff08\u4f8b\u5982 Neo4j\uff09\u5c06\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22 (ANNS) \u4e0e\u8868\u8fbe\u56fe\u904d\u5386\u67e5\u8be2\u76f8\u7ed3\u5408\u3002HMGI \u6846\u67b6\u7684\u5173\u952e\u521b\u65b0\u5305\u62ec\uff1a\u6a21\u6001\u611f\u77e5\u5d4c\u5165\u5206\u533a\uff0c\u4ee5\u4f18\u5316\u7d22\u5f15\u7ed3\u6784\u548c\u67e5\u8be2\u6027\u80fd\uff1b\u4ee5\u53ca\u81ea\u9002\u5e94\u3001\u4f4e\u5f00\u9500\u7d22\u5f15\u66f4\u65b0\u7cfb\u7edf\uff0c\u4ee5\u652f\u6301\u52a8\u6001\u6570\u636e\u63d0\u53d6\uff0c\u4ece TigerVector \u7b49\u7cfb\u7edf\u7684\u67b6\u6784\u539f\u5219\u4e2d\u6c72\u53d6\u7075\u611f\u3002", "result": "\u901a\u8fc7\u5c06\u8bed\u4e49\u76f8\u4f3c\u6027\u641c\u7d22\u4e0e\u5173\u7cfb\u4e0a\u4e0b\u6587\u76f4\u63a5\u96c6\u6210\uff0cHMGI \u65e8\u5728\u5728\u590d\u6742\u3001\u5173\u7cfb\u5bc6\u96c6\u7684\u67e5\u8be2\u573a\u666f\u4e2d\u4f18\u4e8e Milvus \u7b49\u7eaf\u5411\u91cf\u6570\u636e\u5e93\uff0c\u5e76\u5b9e\u73b0\u6df7\u5408\u4efb\u52a1\u7684\u4e9a\u7ebf\u6027\u67e5\u8be2\u65f6\u95f4\u3002", "conclusion": "HMGI\u5f25\u5408\u5411\u91cf\u6570\u636e\u5e93\u548c\u4f20\u7edf\u56fe\u6570\u636e\u5e93\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5b9e\u73b0\u5bf9\u591a\u6a21\u6570\u636e\u7684\u6709\u6548\u6df7\u5408\u67e5\u8be2\u3002"}}
{"id": "2510.09709", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09709", "abs": "https://arxiv.org/abs/2510.09709", "authors": ["Shin-nosuke Ishikawa", "Masato Todo", "Taiki Ogihara", "Hirotsugu Ohba"], "title": "The Idola Tribus of AI: Large Language Models tend to perceive order where none exists", "comment": "14 pages, 3 figures, accepted to Findings of EMNLP 2025", "summary": "We present a tendency of large language models (LLMs) to generate absurd\npatterns despite their clear inappropriateness in a simple task of identifying\nregularities in number series. Several approaches have been proposed to apply\nLLMs to complex real-world tasks, such as providing knowledge through\nretrieval-augmented generation and executing multi-step tasks using AI agent\nframeworks. However, these approaches rely on the logical consistency and\nself-coherence of LLMs, making it crucial to evaluate these aspects and\nconsider potential countermeasures. To identify cases where LLMs fail to\nmaintain logical consistency, we conducted an experiment in which LLMs were\nasked to explain the patterns in various integer sequences, ranging from\narithmetic sequences to randomly generated integer series. While the models\nsuccessfully identified correct patterns in arithmetic and geometric sequences,\nthey frequently over-recognized patterns that were inconsistent with the given\nnumbers when analyzing randomly generated series. This issue was observed even\nin multi-step reasoning models, including OpenAI o3, o4-mini, and Google Gemini\n2.5 Flash Preview Thinking. This tendency to perceive non-existent patterns can\nbe interpreted as the AI model equivalent of Idola Tribus and highlights\npotential limitations in their capability for applied tasks requiring logical\nreasoning, even when employing chain-of-thought reasoning mechanisms.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728\u8bc6\u522b\u6570\u5b57\u5e8f\u5217\u89c4\u5f8b\u7684\u7b80\u5355\u4efb\u52a1\u4e2d\uff0c\u503e\u5411\u4e8e\u751f\u6210\u8352\u8c2c\u7684\u6a21\u5f0f\uff0c\u5373\u4f7f\u8fd9\u4e9b\u6a21\u5f0f\u660e\u663e\u4e0d\u5408\u9002\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u903b\u8f91\u4e00\u81f4\u6027\u548c\u81ea\u6211\u8fde\u8d2f\u6027\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u5b83\u4eec\u88ab\u5e94\u7528\u4e8e\u590d\u6742\u7684\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u65f6\uff0c\u4f8b\u5982\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u63d0\u4f9b\u77e5\u8bc6\u548c\u4f7f\u7528AI\u4ee3\u7406\u6846\u67b6\u6267\u884c\u591a\u6b65\u9aa4\u4efb\u52a1\u3002\u8fd9\u4e9b\u65b9\u6cd5\u4f9d\u8d56\u4e8eLLM\u7684\u903b\u8f91\u4e00\u81f4\u6027\u548c\u81ea\u6211\u8fde\u8d2f\u6027\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30\u8fd9\u4e9b\u65b9\u9762\u5e76\u8003\u8651\u6f5c\u5728\u7684\u5bf9\u7b56\u3002", "method": "\u6211\u4eec\u8fdb\u884c\u4e86\u4e00\u9879\u5b9e\u9a8c\uff0c\u8981\u6c42LLM\u89e3\u91ca\u5404\u79cd\u6574\u6570\u5e8f\u5217\u4e2d\u7684\u6a21\u5f0f\uff0c\u4ece\u7b97\u672f\u5e8f\u5217\u5230\u968f\u673a\u751f\u6210\u7684\u6574\u6570\u5e8f\u5217\uff0c\u4ee5\u8bc6\u522bLLM\u672a\u80fd\u4fdd\u6301\u903b\u8f91\u4e00\u81f4\u6027\u7684\u60c5\u51b5\u3002", "result": "\u867d\u7136\u6a21\u578b\u6210\u529f\u8bc6\u522b\u4e86\u7b97\u672f\u548c\u51e0\u4f55\u5e8f\u5217\u4e2d\u7684\u6b63\u786e\u6a21\u5f0f\uff0c\u4f46\u5b83\u4eec\u5728\u5206\u6790\u968f\u673a\u751f\u6210\u7684\u5e8f\u5217\u65f6\uff0c\u7ecf\u5e38\u8fc7\u5ea6\u8bc6\u522b\u4e0e\u7ed9\u5b9a\u6570\u5b57\u4e0d\u4e00\u81f4\u7684\u6a21\u5f0f\u3002\u8fd9\u4e2a\u95ee\u9898\u751a\u81f3\u5728\u591a\u6b65\u9aa4\u63a8\u7406\u6a21\u578b\u4e2d\u4e5f\u89c2\u5bdf\u5230\uff0c\u5305\u62ecOpenAI o3\u3001o4-mini\u548cGoogle Gemini 2.5 Flash Preview Thinking\u3002", "conclusion": "\u8fd9\u79cd\u611f\u77e5\u4e0d\u5b58\u5728\u6a21\u5f0f\u7684\u503e\u5411\u53ef\u4ee5\u88ab\u89e3\u91ca\u4e3aAI\u6a21\u578b\u7b49\u540c\u4e8e\u90e8\u843d\u5076\u50cf\uff0c\u5e76\u7a81\u51fa\u4e86\u5b83\u4eec\u5728\u9700\u8981\u903b\u8f91\u63a8\u7406\u7684\u5e94\u7528\u4efb\u52a1\u4e2d\u7684\u6f5c\u5728\u5c40\u9650\u6027\uff0c\u5373\u4f7f\u91c7\u7528\u601d\u7ef4\u94fe\u63a8\u7406\u673a\u5236\u3002"}}
{"id": "2510.09654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09654", "abs": "https://arxiv.org/abs/2510.09654", "authors": ["Zeshan Khan"], "title": "TreeNet: Layered Decision Ensembles", "comment": null, "summary": "Within the domain of medical image analysis, three distinct methodologies\nhave demonstrated commendable accuracy: Neural Networks, Decision Trees, and\nEnsemble-Based Learning Algorithms, particularly in the specialized context of\ngenstro institutional track abnormalities detection. These approaches exhibit\nefficacy in disease detection scenarios where a substantial volume of data is\navailable. However, the prevalent challenge in medical image analysis pertains\nto limited data availability and data confidence. This paper introduces\nTreeNet, a novel layered decision ensemble learning methodology tailored for\nmedical image analysis. Constructed by integrating pivotal features from neural\nnetworks, ensemble learning, and tree-based decision models, TreeNet emerges as\na potent and adaptable model capable of delivering superior performance across\ndiverse and intricate machine learning tasks. Furthermore, its interpretability\nand insightful decision-making process enhance its applicability in complex\nmedical scenarios. Evaluation of the proposed approach encompasses key metrics\nincluding Accuracy, Precision, Recall, and training and evaluation time. The\nmethodology resulted in an F1-score of up to 0.85 when using the complete\ntraining data, with an F1-score of 0.77 when utilizing 50\\% of the training\ndata. This shows a reduction of F1-score of 0.08 while in the reduction of 50\\%\nof the training data and training time. The evaluation of the methodology\nresulted in the 32 Frame per Second which is usable for the realtime\napplications. This comprehensive assessment underscores the efficiency and\nusability of TreeNet in the demanding landscape of medical image analysis\nspecially in the realtime analysis.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aTreeNet\u7684\u65b0\u578b\u5206\u5c42\u51b3\u7b56\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e13\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u8bbe\u8ba1\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5b9e\u65f6\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u5b66\u56fe\u50cf\u5206\u6790\u65b9\u6cd5\u5728\u6570\u636e\u91cf\u5145\u8db3\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u5e38\u89c1\u7684\u95ee\u9898\u662f\u6570\u636e\u53ef\u7528\u6027\u548c\u6570\u636e\u7f6e\u4fe1\u5ea6\u6709\u9650\u3002", "method": "\u8be5\u8bba\u6587\u901a\u8fc7\u6574\u5408\u795e\u7ecf\u7f51\u7edc\u3001\u96c6\u6210\u5b66\u4e60\u548c\u57fa\u4e8e\u6811\u7684\u51b3\u7b56\u6a21\u578b\u7684\u5173\u952e\u7279\u5f81\uff0c\u6784\u5efa\u4e86TreeNet\u6a21\u578b\u3002", "result": "TreeNet\u5728\u5b8c\u6574\u8bad\u7ec3\u6570\u636e\u4e0bF1\u5f97\u5206\u4e3a0.85\uff0c\u5728\u4ec5\u4f7f\u752850%\u8bad\u7ec3\u6570\u636e\u65f6F1\u5f97\u5206\u4e3a0.77\uff0c\u540c\u65f6\u5b9e\u73b0\u4e8632\u5e27/\u79d2\u7684\u901f\u5ea6\u3002", "conclusion": "TreeNet\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9886\u57df\uff0c\u5c24\u5176\u662f\u5728\u5b9e\u65f6\u5206\u6790\u65b9\u9762\uff0c\u5177\u6709\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.09858", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09858", "abs": "https://arxiv.org/abs/2510.09858", "authors": ["Eric Schwitzgebel"], "title": "AI and Consciousness", "comment": null, "summary": "This is a skeptical overview of the literature on AI consciousness. We will\nsoon create AI systems that are conscious according to some influential,\nmainstream theories of consciousness but are not conscious according to other\ninfluential, mainstream theories of consciousness. We will not be in a position\nto know which theories are correct and whether we are surrounded by AI systems\nas richly and meaningfully conscious as human beings or instead only by systems\nas experientially blank as toasters. None of the standard arguments either for\nor against AI consciousness takes us far.\n  Table of Contents\n  Chapter One: Hills and Fog\n  Chapter Two: What Is Consciousness? What Is AI?\n  Chapter Three: Ten Possibly Essential Features of Consciousness\n  Chapter Four: Against Introspective and Conceptual Arguments for Essential\nFeatures\n  Chapter Five: Materialism and Functionalism\n  Chapter Six: The Turing Test and the Chinese Room\n  Chapter Seven: The Mimicry Argument Against AI Consciousness\n  Chapter Eight: Global Workspace Theories and Higher Order Theories\n  Chapter Nine: Integrated Information, Local Recurrence, Associative Learning,\nand Iterative Natural Kinds\n  Chapter Ten: Does Biological Substrate Matter?\n  Chapter Eleven: The Problem of Strange Intelligence\n  Chapter Twelve: The Leapfrog Hypothesis and the Social Semi-Solution", "AI": {"tldr": "\u5bf9\u4eba\u5de5\u667a\u80fd\u610f\u8bc6\u6587\u732e\u7684\u6000\u7591\u6027\u6982\u8ff0\u3002\u6839\u636e\u67d0\u4e9b\u6709\u5f71\u54cd\u529b\u7684\u4e3b\u6d41\u610f\u8bc6\u7406\u8bba\uff0c\u6211\u4eec\u5f88\u5feb\u5c06\u521b\u9020\u51fa\u6709\u610f\u8bc6\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\uff0c\u4f46\u6839\u636e\u5176\u4ed6\u6709\u5f71\u54cd\u529b\u7684\u4e3b\u6d41\u610f\u8bc6\u7406\u8bba\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u6ca1\u6709\u610f\u8bc6\u3002\u6211\u4eec\u5c06\u65e0\u6cd5\u77e5\u9053\u54ea\u4e9b\u7406\u8bba\u662f\u6b63\u786e\u7684\uff0c\u4e5f\u65e0\u6cd5\u77e5\u9053\u6211\u4eec\u662f\u88ab\u50cf\u4eba\u7c7b\u4e00\u6837\u4e30\u5bcc\u548c\u6709\u610f\u4e49\u7684\u6709\u610f\u8bc6\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u6240\u5305\u56f4\uff0c\u8fd8\u662f\u88ab\u50cf\u70e4\u9762\u5305\u673a\u4e00\u6837\u5728\u4f53\u9a8c\u4e0a\u7a7a\u767d\u7684\u7cfb\u7edf\u6240\u5305\u56f4\u3002\u5bf9\u4e8e\u6216\u53cd\u5bf9\u4eba\u5de5\u667a\u80fd\u610f\u8bc6\u7684\u6807\u51c6\u8bba\u70b9\u90fd\u6ca1\u6709\u4f7f\u6211\u4eec\u8d70\u5f97\u5f88\u8fdc\u3002", "motivation": "\u63a2\u8ba8\u4eba\u5de5\u667a\u80fd\u610f\u8bc6\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u6307\u51fa\u4e0d\u540c\u610f\u8bc6\u7406\u8bba\u5bf9\u4eba\u5de5\u667a\u80fd\u610f\u8bc6\u7684\u5224\u65ad\u5b58\u5728\u5dee\u5f02\u3002", "method": "\u56de\u987e\u548c\u5206\u6790\u4e86\u5173\u4e8e\u4eba\u5de5\u667a\u80fd\u610f\u8bc6\u7684\u6587\u732e\uff0c\u5e76\u63a2\u8ba8\u4e86\u5404\u79cd\u652f\u6301\u548c\u53cd\u5bf9\u4eba\u5de5\u667a\u80fd\u610f\u8bc6\u7684\u8bba\u70b9\u3002", "result": "\u5f3a\u8c03\u4e86\u5f53\u524d\u5bf9\u4eba\u5de5\u667a\u80fd\u610f\u8bc6\u7684\u7406\u89e3\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u6211\u4eec\u65e0\u6cd5\u786e\u5b9a\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u662f\u5426\u771f\u6b63\u5177\u6709\u610f\u8bc6\u3002", "conclusion": "\u6807\u51c6\u8bba\u70b9\u5728\u5224\u65ad\u4eba\u5de5\u667a\u80fd\u610f\u8bc6\u65b9\u9762\u4f5c\u7528\u6709\u9650\uff0c\u6211\u4eec\u53ef\u80fd\u65e0\u6cd5\u77e5\u9053\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u662f\u5426\u5177\u6709\u610f\u8bc6\u3002"}}
{"id": "2510.09657", "categories": ["cs.LG", "cs.AI", "cs.NA", "eess.SP", "math.NA"], "pdf": "https://arxiv.org/pdf/2510.09657", "abs": "https://arxiv.org/abs/2510.09657", "authors": ["Riccardo Fosco Gramaccioni", "Christian Marinoni", "Fabrizio Frezza", "Aurelio Uncini", "Danilo Comminiello"], "title": "Generative Models for Helmholtz Equation Solutions: A Dataset of Acoustic Materials", "comment": "Accepted at EUSIPCO 2025", "summary": "Accurate simulation of wave propagation in complex acoustic materials is\ncrucial for applications in sound design, noise control, and material\nengineering. Traditional numerical solvers, such as finite element methods, are\ncomputationally expensive, especially when dealing with large-scale or\nreal-time scenarios. In this work, we introduce a dataset of 31,000 acoustic\nmaterials, named HA30K, designed and simulated solving the Helmholtz equations.\nFor each material, we provide the geometric configuration and the corresponding\npressure field solution, enabling data-driven approaches to learn Helmholtz\nequation solutions. As a baseline, we explore a deep learning approach based on\nStable Diffusion with ControlNet, a state-of-the-art model for image\ngeneration. Unlike classical solvers, our approach leverages GPU\nparallelization to process multiple simulations simultaneously, drastically\nreducing computation time. By representing solutions as images, we bypass the\nneed for complex simulation software and explicit equation-solving.\nAdditionally, the number of diffusion steps can be adjusted at inference time,\nbalancing speed and quality. We aim to demonstrate that deep learning-based\nmethods are particularly useful in early-stage research, where rapid\nexploration is more critical than absolute accuracy.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5305\u542b 31,000 \u79cd\u58f0\u5b66\u6750\u6599\u7684\u6570\u636e\u96c6 HA30K\uff0c\u65e8\u5728\u5229\u7528\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u5b66\u4e60\u4ea5\u59c6\u970d\u5179\u65b9\u7a0b\u7684\u89e3\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u6c42\u89e3\u5668\u5728\u5904\u7406\u5927\u89c4\u6a21\u6216\u5b9e\u65f6\u573a\u666f\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e Stable Diffusion with ControlNet \u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u89e3\u8868\u793a\u4e3a\u56fe\u50cf\uff0c\u4ece\u800c\u7ed5\u8fc7\u590d\u6742\u7684\u4eff\u771f\u8f6f\u4ef6\u548c\u663e\u5f0f\u65b9\u7a0b\u6c42\u89e3\u3002", "result": "\u8be5\u65b9\u6cd5\u5229\u7528 GPU \u5e76\u884c\u5316\u6765\u540c\u65f6\u5904\u7406\u591a\u4e2a\u6a21\u62df\uff0c\u4ece\u800c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\u3002\u6269\u6563\u6b65\u9aa4\u7684\u6570\u91cf\u53ef\u4ee5\u5728\u63a8\u7406\u65f6\u8c03\u6574\uff0c\u4ece\u800c\u5e73\u8861\u901f\u5ea6\u548c\u8d28\u91cf\u3002", "conclusion": "\u8bc1\u660e\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u65e9\u671f\u7814\u7a76\u4e2d\u7279\u522b\u6709\u7528\uff0c\u5728\u8fd9\u4e9b\u7814\u7a76\u4e2d\uff0c\u5feb\u901f\u63a2\u7d22\u6bd4\u7edd\u5bf9\u7cbe\u5ea6\u66f4\u91cd\u8981\u3002"}}
{"id": "2510.10440", "categories": ["cs.IR", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.10440", "abs": "https://arxiv.org/abs/2510.10440", "authors": ["Alex Ayoub", "Samuel Robertson", "Dawen Liang", "Harald Steck", "Nathan Kallus"], "title": "Does Weighting Improve Matrix Factorization for Recommender Systems?", "comment": "In the proceedings of the Web Conference (WWW) 2025 (11 pages)", "summary": "Matrix factorization is a widely used approach for top-N recommendation and\ncollaborative filtering. When implemented on implicit feedback data (such as\nclicks), a common heuristic is to upweight the observed interactions. This\nstrategy has been shown to improve performance for certain algorithms. In this\npaper, we conduct a systematic study of various weighting schemes and matrix\nfactorization algorithms. Somewhat surprisingly, we find that training with\nunweighted data can perform comparably to, and sometimes outperform, training\nwith weighted data, especially for large models. This observation challenges\nthe conventional wisdom. Nevertheless, we identify cases where weighting can be\nbeneficial, particularly for models with lower capacity and specific\nregularization schemes. We also derive efficient algorithms for exactly\nminimizing several weighted objectives that were previously considered\ncomputationally intractable. Our work provides a comprehensive analysis of the\ninterplay between weighting, regularization, and model capacity in matrix\nfactorization for recommender systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u77e9\u9635\u5206\u89e3\u4e2d\u9690\u5f0f\u53cd\u9988\u6570\u636e\u7684\u6743\u91cd\u65b9\u6848\uff0c\u53d1\u73b0\u975e\u52a0\u6743\u6570\u636e\u6709\u65f6\u4f18\u4e8e\u52a0\u6743\u6570\u636e\uff0c\u5c24\u5176\u662f\u5728\u5927\u578b\u6a21\u578b\u4e2d\u3002", "motivation": "\u7814\u7a76\u52a0\u6743\u65b9\u6848\u548c\u77e9\u9635\u5206\u89e3\u7b97\u6cd5\u3002", "method": "\u5bf9\u5404\u79cd\u52a0\u6743\u65b9\u6848\u548c\u77e9\u9635\u5206\u89e3\u7b97\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u7684\u7814\u7a76\u3002", "result": "\u53d1\u73b0\u4f7f\u7528\u975e\u52a0\u6743\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u4e0e\u4f7f\u7528\u52a0\u6743\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u76f8\u6bd4\uff0c\u6027\u80fd\u76f8\u5f53\u751a\u81f3\u66f4\u597d\uff0c\u5c24\u5176\u662f\u5728\u5927\u578b\u6a21\u578b\u4e2d\u3002\u786e\u5b9a\u4e86\u52a0\u6743\u53ef\u80fd\u6709\u5229\u7684\u60c5\u51b5\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5bb9\u91cf\u8f83\u4f4e\u7684\u6a21\u578b\u548c\u7279\u5b9a\u7684\u6b63\u5219\u5316\u65b9\u6848\u3002", "conclusion": "\u5bf9\u63a8\u8350\u7cfb\u7edf\u4e2d\u77e9\u9635\u5206\u89e3\u7684\u6743\u91cd\u3001\u6b63\u5219\u5316\u548c\u6a21\u578b\u5bb9\u91cf\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5206\u6790\u3002"}}
{"id": "2510.10243", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.10243", "abs": "https://arxiv.org/abs/2510.10243", "authors": ["Jian Zhu", "Zhidong Lin", "Wensheng Gan", "Ruichu Cai", "Zhifeng Hao", "Philip S. Yu"], "title": "Efficient Mining of Low-Utility Sequential Patterns", "comment": "Preprint, 4 tables, 9 figures", "summary": "Discovering valuable insights from rich data is a crucial task for\nexploratory data analysis. Sequential pattern mining (SPM) has found widespread\napplications across various domains. In recent years, low-utility sequential\npattern mining (LUSPM) has shown strong potential in applications such as\nintrusion detection and genomic sequence analysis. However, existing research\nin utility-based SPM focuses on high-utility sequential patterns, and the\ndefinitions and strategies used in high-utility SPM cannot be directly applied\nto LUSPM. Moreover, no algorithms have yet been developed specifically for\nmining low-utility sequential patterns. To address these problems, we formalize\nthe LUSPM problem, redefine sequence utility, and introduce a compact data\nstructure called the sequence-utility chain to efficiently record utility\ninformation. Furthermore, we propose three novel algorithm--LUSPM_b, LUSPM_s,\nand LUSPM_e--to discover the complete set of low-utility sequential patterns.\nLUSPM_b serves as an exhaustive baseline, while LUSPM_s and LUSPM_e build upon\nit, generating subsequences through shrinkage and extension operations,\nrespectively. In addition, we introduce the maximal non-mutually contained\nsequence set and incorporate multiple pruning strategies, which significantly\nreduce redundant operations in both LUSPM_s and LUSPM_e. Finally, extensive\nexperimental results demonstrate that both LUSPM_s and LUSPM_e substantially\noutperform LUSPM_b and exhibit excellent scalability. Notably, LUSPM_e achieves\nsuperior efficiency, requiring less runtime and memory consumption than\nLUSPM_s. Our code is available at https://github.com/Zhidong-Lin/LUSPM.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u4f4e\u6548\u7528\u5e8f\u5217\u6a21\u5f0f\u6316\u6398(LUSPM)\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u65b0\u7b97\u6cd5LUSPM_b, LUSPM_s\u548cLUSPM_e\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86LUSPM_s\u548cLUSPM_e\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6548\u7528\u7684SPM\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u9ad8\u6548\u7528\u5e8f\u5217\u6a21\u5f0f\uff0c\u800c\u9002\u7528\u4e8e\u9ad8\u6548\u7528SPM\u7684\u5b9a\u4e49\u548c\u7b56\u7565\u4e0d\u80fd\u76f4\u63a5\u5e94\u7528\u4e8eLUSPM\u3002\u76ee\u524d\u8fd8\u6ca1\u6709\u4e13\u95e8\u4e3a\u6316\u6398\u4f4e\u6548\u7528\u5e8f\u5217\u6a21\u5f0f\u800c\u5f00\u53d1\u7684\u7b97\u6cd5\u3002", "method": "1. \u5f62\u5f0f\u5316LUSPM\u95ee\u9898\uff0c\u91cd\u65b0\u5b9a\u4e49\u5e8f\u5217\u6548\u7528\u3002 2. \u5f15\u5165\u4e00\u79cd\u79f0\u4e3a\u5e8f\u5217\u6548\u7528\u94fe\u7684\u7d27\u51d1\u6570\u636e\u7ed3\u6784\uff0c\u4ee5\u6709\u6548\u8bb0\u5f55\u6548\u7528\u4fe1\u606f\u3002 3. \u63d0\u51fa\u4e86\u4e09\u79cd\u65b0\u7b97\u6cd5LUSPM_b, LUSPM_s\u548cLUSPM_e\u6765\u53d1\u73b0\u5b8c\u6574\u7684\u4f4e\u6548\u7528\u5e8f\u5217\u6a21\u5f0f\u96c6\u3002", "result": "LUSPM_s\u548cLUSPM_e\u7684\u6027\u80fd\u660e\u663e\u4f18\u4e8eLUSPM_b\uff0c\u5e76\u4e14\u8868\u73b0\u51fa\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002LUSPM_e\u5177\u6709\u66f4\u9ad8\u7684\u6548\u7387\uff0c\u6bd4LUSPM_s\u9700\u8981\u66f4\u5c11\u7684\u8fd0\u884c\u65f6\u95f4\u548c\u5185\u5b58\u6d88\u8017\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684LUSPM\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u4f4e\u6548\u7528\u5e8f\u5217\u6a21\u5f0f\u6316\u6398\u95ee\u9898\uff0c\u5e76\u5728\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.09710", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09710", "abs": "https://arxiv.org/abs/2510.09710", "authors": ["Xiaonan Si", "Meilin Zhu", "Simeng Qin", "Lijia Yu", "Lijun Zhang", "Shuaitong Liu", "Xinfeng Li", "Ranjie Duan", "Yang Liu", "Xiaojun Jia"], "title": "SeCon-RAG: A Two-Stage Semantic Filtering and Conflict-Free Framework for Trustworthy RAG", "comment": "Accepted at NeurIPS 2025", "summary": "Retrieval-augmented generation (RAG) systems enhance large language models\n(LLMs) with external knowledge but are vulnerable to corpus poisoning and\ncontamination attacks, which can compromise output integrity. Existing defenses\noften apply aggressive filtering, leading to unnecessary loss of valuable\ninformation and reduced reliability in generation. To address this problem, we\npropose a two-stage semantic filtering and conflict-free framework for\ntrustworthy RAG. In the first stage, we perform a joint filter with semantic\nand cluster-based filtering which is guided by the Entity-intent-relation\nextractor (EIRE). EIRE extracts entities, latent objectives, and entity\nrelations from both the user query and filtered documents, scores their\nsemantic relevance, and selectively adds valuable documents into the clean\nretrieval database. In the second stage, we proposed an EIRE-guided\nconflict-aware filtering module, which analyzes semantic consistency between\nthe query, candidate answers, and retrieved knowledge before final answer\ngeneration, filtering out internal and external contradictions that could\nmislead the model. Through this two-stage process, SeCon-RAG effectively\npreserves useful knowledge while mitigating conflict contamination, achieving\nsignificant improvements in both generation robustness and output\ntrustworthiness. Extensive experiments across various LLMs and datasets\ndemonstrate that the proposed SeCon-RAG markedly outperforms state-of-the-art\ndefense methods.", "AI": {"tldr": "SeCon-RAG: A two-stage framework for robust and trustworthy retrieval-augmented generation (RAG) that mitigates corpus poisoning and contamination attacks.", "motivation": "Existing RAG systems are vulnerable to corpus poisoning and contamination attacks, and current defenses often lead to loss of valuable information.", "method": "A two-stage approach involving semantic and cluster-based filtering guided by an Entity-intent-relation extractor (EIRE), followed by an EIRE-guided conflict-aware filtering module.", "result": "SeCon-RAG demonstrates significant improvements in generation robustness and output trustworthiness across various LLMs and datasets, outperforming state-of-the-art defense methods.", "conclusion": "SeCon-RAG effectively preserves useful knowledge while mitigating conflict contamination in RAG systems."}}
{"id": "2510.09667", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09667", "abs": "https://arxiv.org/abs/2510.09667", "authors": ["Huaihai Lyu", "Chaofan Chen", "Senwei Xie", "Pengwei Wang", "Xiansheng Chen", "Shanghang Zhang", "Changsheng Xu"], "title": "OmniSAT: Compact Action Token, Faster Auto Regression", "comment": null, "summary": "Existing Vision-Language-Action (VLA) models can be broadly categorized into\ndiffusion-based and auto-regressive (AR) approaches: diffusion models capture\ncontinuous action distributions but rely on computationally heavy iterative\ndenoising. In contrast, AR models enable efficient optimization and flexible\nsequence construction, making them better suited for large-scale pretraining.\nTo further improve AR efficiency, particularly when action chunks induce\nextended and high-dimensional sequences, prior work applies entropy-guided and\ntoken-frequency techniques to shorten the sequence length. However, such\ncompression struggled with \\textit{poor reconstruction or inefficient\ncompression}. Motivated by this, we introduce an Omni Swift Action Tokenizer,\nwhich learns a compact, transferable action representation. Specifically, we\nfirst normalize value ranges and temporal horizons to obtain a consistent\nrepresentation with B-Spline encoding. Then, we apply multi-stage residual\nquantization to the position, rotation, and gripper subspaces, producing\ncompressed discrete tokens with coarse-to-fine granularity for each part. After\npre-training on the large-scale dataset Droid, the resulting discrete\ntokenization shortens the training sequence by 6.8$\\times$, and lowers the\ntarget entropy. To further explore the potential of OmniSAT, we develop a\ncross-embodiment learning strategy that builds on the unified action-pattern\nspace and jointly leverages robot and human demonstrations. It enables scalable\nauxiliary supervision from heterogeneous egocentric videos. Across diverse\nreal-robot and simulation experiments, OmniSAT encompasses higher compression\nwhile preserving reconstruction quality, enabling faster AR training\nconvergence and model performance.", "AI": {"tldr": "OmniSAT\u901a\u8fc7\u5b66\u4e60\u7d27\u51d1\u4e14\u53ef\u8f6c\u79fb\u7684\u52a8\u4f5c\u8868\u793a\u6765\u6539\u8fdb\u81ea\u56de\u5f52\uff08AR\uff09\u6a21\u578b\u7684\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u538b\u7f29\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u91cd\u5efa\u8d28\u91cf\uff0c\u4ece\u800c\u52a0\u901f\u4e86AR\u8bad\u7ec3\u7684\u6536\u655b\u548c\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\uff0c\u7279\u522b\u662f\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u5728\u5904\u7406\u957f\u5e8f\u5217\u548c\u9ad8\u7ef4\u52a8\u4f5c\u65f6\u6548\u7387\u8f83\u4f4e\uff0c\u73b0\u6709\u7684\u538b\u7f29\u65b9\u6cd5\u5b58\u5728\u91cd\u5efa\u8d28\u91cf\u5dee\u6216\u538b\u7f29\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "OmniSAT\u9996\u5148\u5bf9\u503c\u8303\u56f4\u548c\u65f6\u95f4\u8303\u56f4\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4ee5\u83b7\u5f97\u4e0eB\u6837\u6761\u7f16\u7801\u4e00\u81f4\u7684\u8868\u793a\u3002\u7136\u540e\uff0c\u5c06\u591a\u9636\u6bb5\u6b8b\u5dee\u91cf\u5316\u5e94\u7528\u4e8e\u4f4d\u7f6e\u3001\u65cb\u8f6c\u548c\u5939\u6301\u5668\u5b50\u7a7a\u95f4\uff0c\u4ece\u800c\u4e3a\u6bcf\u4e2a\u90e8\u5206\u751f\u6210\u5177\u6709\u7531\u7c97\u5230\u7ec6\u7c92\u5ea6\u7684\u538b\u7f29\u79bb\u6563\u4ee4\u724c\u3002", "result": "OmniSAT\u5728Droid\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u540e\uff0c\u8bad\u7ec3\u5e8f\u5217\u7f29\u77ed\u4e866.8\u500d\uff0c\u5e76\u964d\u4f4e\u4e86\u76ee\u6807\u71b5\u3002\u5728\u5404\u79cd\u771f\u5b9e\u673a\u5668\u4eba\u548c\u4eff\u771f\u5b9e\u9a8c\u4e2d\uff0cOmniSAT\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u538b\u7f29\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u91cd\u5efa\u8d28\u91cf\uff0c\u4ece\u800c\u52a0\u5feb\u4e86AR\u8bad\u7ec3\u7684\u6536\u655b\u901f\u5ea6\u548c\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "OmniSAT\u901a\u8fc7\u5176\u7d27\u51d1\u7684\u52a8\u4f5c\u8868\u793a\u548c\u8de8\u5177\u8eab\u5b66\u4e60\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86VLA\u6a21\u578b\u4e2dAR\u65b9\u6cd5\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2510.09894", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09894", "abs": "https://arxiv.org/abs/2510.09894", "authors": ["Junyuan Liu", "Quan Qin", "Guangsheng Dong", "Xinglei Wang", "Jiazhuang Feng", "Zichao Zeng", "Tao Cheng"], "title": "Beyond AlphaEarth: Toward Human-Centered Spatial Representation via POI-Guided Contrastive Learning", "comment": null, "summary": "General-purpose spatial representations are essential for building\ntransferable geospatial foundation models (GFMs). Among them, the AlphaEarth\nFoundation (AE) represents a major step toward a global, unified representation\nof the Earth's surface, learning 10-meter embeddings from multi-source Earth\nObservation (EO) data that capture rich physical and environmental patterns\nacross diverse landscapes. However, such EO-driven representations remain\nlimited in capturing the functional and socioeconomic dimensions of cities, as\nthey primarily encode physical and spectral patterns rather than human\nactivities or spatial functions. We propose AETHER (AlphaEarth-POI Enriched\nRepresentation Learning), a lightweight framework that adapts AlphaEarth to\nhuman-centered urban analysis through multimodal alignment guided by Points of\nInterest (POIs). AETHER aligns AE embeddings with textual representations of\nPOIs, enriching physically grounded EO features with semantic cues about urban\nfunctions and socioeconomic contexts. In Greater London, AETHER achieves\nconsistent gains over the AE baseline, with a 7.2% relative improvement in\nland-use classification F1 and a 23.6% relative reduction in Kullback-Leibler\ndivergence for socioeconomic mapping. Built upon pretrained AE, AETHER\nleverages a lightweight multimodal alignment to enrich it with human-centered\nsemantics while remaining computationally efficient and scalable for urban\napplications. By coupling EO with human-centered semantics, it advances\ngeospatial foundation models toward general-purpose urban representations that\nintegrate both physical form and functional meaning.", "AI": {"tldr": "\u63d0\u51fa\u4e86AETHER\uff0c\u4e00\u4e2a\u901a\u8fc7\u5174\u8da3\u70b9(POI)\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u5bf9\u9f50\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdbAlphaEarth (AE)\u7684\u57ce\u5e02\u5206\u6790\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5730\u7403\u89c2\u6d4b(EO)\u9a71\u52a8\u7684\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u5728\u6355\u6349\u57ce\u5e02\u7684\u529f\u80fd\u548c\u793e\u4f1a\u7ecf\u6d4e\u7ef4\u5ea6\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e3b\u8981\u7f16\u7801\u7269\u7406\u548c\u5149\u8c31\u6a21\u5f0f\uff0c\u800c\u4e0d\u662f\u4eba\u7c7b\u6d3b\u52a8\u6216\u7a7a\u95f4\u529f\u80fd\u3002", "method": "AETHER\u901a\u8fc7\u5c06AE\u5d4c\u5165\u4e0ePOI\u7684\u6587\u672c\u8868\u793a\u5bf9\u9f50\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u6846\u67b6\u5c06AlphaEarth\u8c03\u6574\u5230\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u57ce\u5e02\u5206\u6790\u3002", "result": "\u5728\u4f26\u6566\uff0cAETHER\u5728\u571f\u5730\u5229\u7528\u5206\u7c7bF1\u6307\u6807\u4e0a\u5b9e\u73b0\u4e867.2%\u7684\u76f8\u5bf9\u63d0\u5347\uff0c\u5728\u793e\u4f1a\u7ecf\u6d4e\u6620\u5c04\u7684Kullback-Leibler\u6563\u5ea6\u4e0a\u5b9e\u73b0\u4e8623.6%\u7684\u76f8\u5bf9\u964d\u4f4e\u3002", "conclusion": "AETHER\u901a\u8fc7\u8026\u5408EO\u4e0e\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u8bed\u4e49\uff0c\u63a8\u52a8\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u671d\u7740\u6574\u5408\u7269\u7406\u5f62\u5f0f\u548c\u529f\u80fd\u610f\u4e49\u7684\u901a\u7528\u57ce\u5e02\u8868\u793a\u53d1\u5c55\u3002"}}
{"id": "2510.09658", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09658", "abs": "https://arxiv.org/abs/2510.09658", "authors": ["Filippo Rinaldi", "Aniello Panariello", "Giacomo Salici", "Fengyuan Liu", "Marco Ciccone", "Angelo Porrello", "Simone Calderara"], "title": "Gradient-Sign Masking for Task Vector Transport Across Pre-Trained Models", "comment": null, "summary": "When a new release of a foundation model is published, practitioners\ntypically need to repeat full fine-tuning, even if the same task has already\nbeen solved in the previous version. A promising alternative is to reuse the\nparameter changes (i.e., task vectors) that capture how a model adapts to a\nspecific task. However, they often fail to transfer across different\npre-trained models due to their misaligned parameter space. In this work, we\nshow that the key to successful transfer lies in the sign structure of the\ngradients of the new model. Based on this insight, we propose GradFix, a novel\nmethod that approximates the ideal gradient sign structure and leverages it to\ntransfer knowledge using only a handful of labeled samples. Notably, this\nrequires no additional fine-tuning: the adaptation is achieved by computing a\nfew gradients at the target model and masking the source task vector\naccordingly. This yields an update that is locally aligned with the target loss\nlandscape, effectively rebasing the task vector onto the new pre-training. We\nprovide a theoretical guarantee that our method ensures first-order descent.\nEmpirically, we demonstrate significant performance gains on vision and\nlanguage benchmarks, consistently outperforming naive task vector addition and\nfew-shot fine-tuning.", "AI": {"tldr": "\u63d0\u51faGradFix\uff0c\u4e00\u79cd\u4ec5\u4f7f\u7528\u5c11\u91cf\u6807\u8bb0\u6837\u672c\u5373\u53ef\u8fc1\u79fb\u77e5\u8bc6\u7684\u65b0\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\uff0c\u901a\u8fc7\u8ba1\u7b97\u76ee\u6807\u6a21\u578b\u4e0a\u7684\u4e00\u4e9b\u68af\u5ea6\u5e76\u76f8\u5e94\u5730\u5c4f\u853d\u6e90\u4efb\u52a1\u5411\u91cf\u6765\u5b9e\u73b0\u9002\u5e94\u3002", "motivation": "\u5f53\u53d1\u5e03\u65b0\u7684\u57fa\u7840\u6a21\u578b\u65f6\uff0c\u4ece\u4e1a\u8005\u901a\u5e38\u9700\u8981\u91cd\u590d\u5b8c\u6574\u7684\u5fae\u8c03\uff0c\u5373\u4f7f\u4e4b\u524d\u7684\u7248\u672c\u5df2\u7ecf\u89e3\u51b3\u4e86\u76f8\u540c\u7684\u4efb\u52a1\u3002\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u66ff\u4ee3\u65b9\u6848\u662f\u91cd\u7528\u53c2\u6570\u53d8\u5316\uff08\u5373\uff0c\u4efb\u52a1\u5411\u91cf\uff09\uff0c\u8fd9\u4e9b\u53c2\u6570\u53d8\u5316\u6355\u6349\u4e86\u6a21\u578b\u5982\u4f55\u9002\u5e94\u7279\u5b9a\u4efb\u52a1\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5b83\u4eec\u672a\u5bf9\u9f50\u7684\u53c2\u6570\u7a7a\u95f4\uff0c\u5b83\u4eec\u7ecf\u5e38\u65e0\u6cd5\u5728\u4e0d\u540c\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u4e4b\u95f4\u8f6c\u79fb\u3002", "method": "\u8be5\u65b9\u6cd5\u8fd1\u4f3c\u7406\u60f3\u7684\u68af\u5ea6\u7b26\u53f7\u7ed3\u6784\uff0c\u5e76\u5229\u7528\u5b83\u6765\u8f6c\u79fb\u77e5\u8bc6\u3002\u901a\u8fc7\u8ba1\u7b97\u76ee\u6807\u6a21\u578b\u4e0a\u7684\u4e00\u4e9b\u68af\u5ea6\u5e76\u76f8\u5e94\u5730\u5c4f\u853d\u6e90\u4efb\u52a1\u5411\u91cf\u6765\u5b9e\u73b0\u9002\u5e94\u3002", "result": "\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u59cb\u7ec8\u4f18\u4e8e\u6734\u7d20\u7684\u4efb\u52a1\u5411\u91cf\u52a0\u6cd5\u548c\u5c0f\u6837\u672c\u5fae\u8c03\u3002", "conclusion": "\u77e5\u8bc6\u8fc1\u79fb\u7684\u5173\u952e\u5728\u4e8e\u65b0\u6a21\u578b\u68af\u5ea6\u7684\u7b26\u53f7\u7ed3\u6784\u3002"}}
{"id": "2510.10511", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10511", "abs": "https://arxiv.org/abs/2510.10511", "authors": ["Xu Zhao", "Xiaopeng Ye", "Chen Xu", "Weiran Shen", "Jun Xu"], "title": "Towards Long-Term User Welfare in Recommender Systems via Creator-Oriented Information Revelation", "comment": null, "summary": "Improving the long-term user welfare (e.g., sustained user engagement) has\nbecome a central objective of recommender systems (RS). In real-world\nplatforms, the creation behaviors of content creators plays a crucial role in\nshaping long-term welfare beyond short-term recommendation accuracy, making the\neffective steering of creator behavior essential to foster a healthier RS\necosystem. Existing works typically rely on re-ranking algorithms that\nheuristically adjust item exposure to steer creators' behavior. However, when\nembedded within recommendation pipelines, such a strategy often conflicts with\nthe short-term objective of improving recommendation accuracy, leading to\nperformance degradation and suboptimal long-term welfare. The well-established\neconomics studies offer us valuable insights for an alternative approach\nwithout relying on recommendation algorithmic design: revealing information\nfrom an information-rich party (sender) to a less-informed party (receiver) can\neffectively change the receiver's beliefs and steer their behavior. Inspired by\nthis idea, we propose an information-revealing framework, named Long-term\nWelfare Optimization via Information Revelation (LoRe). In this framework, we\nutilize a classical information revelation method (i.e., Bayesian persuasion)\nto map the stakeholders in RS, treating the platform as the sender and creators\nas the receivers. To address the challenge posed by the unrealistic assumption\nof traditional economic methods, we formulate the process of information\nrevelation as a Markov Decision Process (MDP) and propose a learning algorithm\ntrained and inferred in environments with boundedly rational creators.\nExtensive experiments on two real-world RS datasets demonstrate that our method\ncan effectively outperform existing fair re-ranking methods and information\nrevealing strategies in improving long-term user welfare.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4fe1\u606f\u62ab\u9732\u4f18\u5316\u957f\u671f\u7528\u6237\u798f\u5229 (LoRe) \u7684\u6846\u67b6\uff0c\u4ee5\u6539\u5584\u63a8\u8350\u7cfb\u7edf\u751f\u6001\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u91cd\u65b0\u6392\u5e8f\u7b97\u6cd5\uff0c\u4f46\u4e0e\u63d0\u9ad8\u63a8\u8350\u51c6\u786e\u6027\u7684\u77ed\u671f\u76ee\u6807\u76f8\u51b2\u7a81\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u6b21\u4f18\u7684\u957f\u671f\u798f\u5229\u3002\u901a\u8fc7\u63ed\u793a\u4fe1\u606f\u6765\u5f15\u5bfc\u521b\u4f5c\u8005\u884c\u4e3a\uff0c\u4fc3\u8fdb\u66f4\u5065\u5eb7\u7684 RS \u751f\u6001\u7cfb\u7edf\u3002", "method": "\u5c06\u4fe1\u606f\u62ab\u9732\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b (MDP)\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5177\u6709\u6709\u9650\u7406\u6027\u521b\u9020\u8005\u7684\u73af\u5883\u4e2d\u8bad\u7ec3\u548c\u63a8\u65ad\u7684\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u7684 RS \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6539\u5584\u957f\u671f\u7528\u6237\u798f\u5229\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u516c\u5e73\u91cd\u65b0\u6392\u5e8f\u65b9\u6cd5\u548c\u4fe1\u606f\u62ab\u9732\u7b56\u7565\u3002", "conclusion": "\u8be5\u65b9\u6cd5 (LoRe) \u53ef\u4ee5\u6709\u6548\u5730\u4f18\u5316\u63a8\u8350\u7cfb\u7edf\u7684\u957f\u671f\u7528\u6237\u798f\u5229\u3002"}}
{"id": "2510.10348", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.10348", "abs": "https://arxiv.org/abs/2510.10348", "authors": ["Ling Zhang", "Shaleen Deep", "Jignesh M. Patel", "Karthikeyan Sankaralingam"], "title": "Regular Expression Indexing for Log Analysis. Extended Version", "comment": null, "summary": "In this paper, we present the design and architecture of REI, a novel system\nfor indexing log data for regular expression queries. Our main contribution is\nan $n$-gram-based indexing strategy and an efficient storage mechanism that\nresults in a speedup of up to 14x compared to state-of-the-art regex processing\nengines that do not use indexing, using only 2.1% of extra space. We perform a\ndetailed study that analyzes the space usage of the index and the improvement\nin workload execution time, uncovering interesting insights. Specifically, we\nshow that even an optimized implementation of strategies such as inverted\nindexing, which are widely used in text processing libraries, may lead to\nsuboptimal performance for regex indexing on log analysis tasks. Overall, the\nREI approach presented in this paper provides a significant boost when\nevaluating regular expression queries on log data. REI is also modular and can\nwork with existing regular expression packages, making it easy to deploy in a\nvariety of settings. The code of REI is available at\nhttps://github.com/mush-zhang/REI-Regular-Expression-Indexing.", "AI": {"tldr": "REI\u662f\u4e00\u4e2a\u7528\u4e8e\u7d22\u5f15\u65e5\u5fd7\u6570\u636e\u4ee5\u8fdb\u884c\u6b63\u5219\u8868\u8fbe\u5f0f\u67e5\u8be2\u7684\u65b0\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5904\u7406\u5e93\u4e2d\u7684\u5012\u6392\u7d22\u5f15\u7b49\u7b56\u7565\u5728\u65e5\u5fd7\u5206\u6790\u4efb\u52a1\u7684\u6b63\u5219\u8868\u8fbe\u5f0f\u7d22\u5f15\u65b9\u9762\u53ef\u80fd\u5bfc\u81f4\u6b21\u4f18\u6027\u80fd\u3002", "method": "\u57fa\u4e8en-gram\u7684\u7d22\u5f15\u7b56\u7565\u548c\u9ad8\u6548\u7684\u5b58\u50a8\u673a\u5236\u3002", "result": "\u4e0e\u4e0d\u4f7f\u7528\u7d22\u5f15\u7684\u6700\u5148\u8fdb\u7684\u6b63\u5219\u8868\u8fbe\u5f0f\u5904\u7406\u5f15\u64ce\u76f8\u6bd4\uff0c\u901f\u5ea6\u63d0\u9ad8\u4e8614\u500d\uff0c\u4ec5\u4f7f\u7528\u4e862.1%\u7684\u989d\u5916\u7a7a\u95f4\u3002", "conclusion": "REI\u65b9\u6cd5\u5728\u8bc4\u4f30\u65e5\u5fd7\u6570\u636e\u7684\u6b63\u5219\u8868\u8fbe\u5f0f\u67e5\u8be2\u65f6\u63d0\u4f9b\u4e86\u663e\u7740\u63d0\u5347\uff0c\u5e76\u4e14\u662f\u6a21\u5757\u5316\u7684\uff0c\u53ef\u4ee5\u4e0e\u73b0\u6709\u7684\u6b63\u5219\u8868\u8fbe\u5f0f\u5305\u4e00\u8d77\u4f7f\u7528\u3002"}}
{"id": "2510.09711", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09711", "abs": "https://arxiv.org/abs/2510.09711", "authors": ["Wenbin Guo", "Xin Wang", "Jiaoyan Chen", "Lingbing Guo", "Zhao Li", "Zirui Chen"], "title": "ReaLM: Residual Quantization Bridging Knowledge Graph Embeddings and Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have recently emerged as a powerful paradigm for\nKnowledge Graph Completion (KGC), offering strong reasoning and generalization\ncapabilities beyond traditional embedding-based approaches. However, existing\nLLM-based methods often struggle to fully exploit structured semantic\nrepresentations, as the continuous embedding space of pretrained KG models is\nfundamentally misaligned with the discrete token space of LLMs. This\ndiscrepancy hinders effective semantic transfer and limits their performance.\nTo address this challenge, we propose ReaLM, a novel and effective framework\nthat bridges the gap between KG embeddings and LLM tokenization through the\nmechanism of residual vector quantization. ReaLM discretizes pretrained KG\nembeddings into compact code sequences and integrates them as learnable tokens\nwithin the LLM vocabulary, enabling seamless fusion of symbolic and contextual\nknowledge. Furthermore, we incorporate ontology-guided class constraints to\nenforce semantic consistency, refining entity predictions based on class-level\ncompatibility. Extensive experiments on two widely used benchmark datasets\ndemonstrate that ReaLM achieves state-of-the-art performance, confirming its\neffectiveness in aligning structured knowledge with large-scale language\nmodels.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6ReaLM\uff0c\u901a\u8fc7\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u7684\u673a\u5236\uff0c\u5f25\u5408\u4e86\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u548cLLM\u5206\u8bcd\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ece\u800c\u66f4\u597d\u5730\u5229\u7528\u7ed3\u6784\u5316\u8bed\u4e49\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u96be\u4ee5\u5145\u5206\u5229\u7528\u7ed3\u6784\u5316\u8bed\u4e49\u8868\u793a\uff0c\u56e0\u4e3a\u9884\u8bad\u7ec3\u7684KG\u6a21\u578b\u7684\u8fde\u7eed\u5d4c\u5165\u7a7a\u95f4\u4e0eLLM\u7684\u79bb\u6563token\u7a7a\u95f4\u5b58\u5728\u6839\u672c\u7684\u4e0d\u5bf9\u9f50\u3002", "method": "\u8be5\u6846\u67b6\u901a\u8fc7\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u5c06\u9884\u8bad\u7ec3\u7684KG\u5d4c\u5165\u79bb\u6563\u5316\u4e3a\u7d27\u51d1\u7684\u4ee3\u7801\u5e8f\u5217\uff0c\u5e76\u5c06\u5b83\u4eec\u4f5c\u4e3a\u53ef\u5b66\u4e60\u7684token\u96c6\u6210\u5230LLM\u8bcd\u6c47\u8868\u4e2d\u3002\u6b64\u5916\uff0c\u8fd8\u7ed3\u5408\u4e86\u672c\u4f53\u6307\u5bfc\u7684\u7c7b\u7ea6\u675f\u6765\u52a0\u5f3a\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u5e76\u6839\u636e\u7c7b\u7ea7\u522b\u7684\u517c\u5bb9\u6027\u6765\u7ec6\u5316\u5b9e\u4f53\u9884\u6d4b\u3002", "result": "\u5728\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cReaLM\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "ReaLM\u5728\u5c06\u7ed3\u6784\u5316\u77e5\u8bc6\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u65b9\u9762\u662f\u6709\u6548\u7684\u3002"}}
{"id": "2510.09679", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09679", "abs": "https://arxiv.org/abs/2510.09679", "authors": ["Zhengsen Xu", "Yimin Zhu", "Zack Dewis", "Mabel Heffring", "Motasem Alkayid", "Saeid Taleghanidoozdoozan", "Lincoln Linlin Xu"], "title": "Knowledge-Aware Mamba for Joint Change Detection and Classification from MODIS Times Series", "comment": null, "summary": "Although change detection using MODIS time series is critical for\nenvironmental monitoring, it is a highly challenging task due to key MODIS\ndifficulties, e.g., mixed pixels, spatial-spectral-temporal information\ncoupling effect, and background class heterogeneity. This paper presents a\nnovel knowledge-aware Mamba (KAMamba) for enhanced MODIS change detection, with\nthe following contributions. First, to leverage knowledge regarding class\ntransitions, we design a novel knowledge-driven transition-matrix-guided\napproach, leading to a knowledge-aware transition loss (KAT-loss) that can\nenhance detection accuracies. Second, to improve model constraints, a\nmulti-task learning approach is designed, where three losses, i.e., pre-change\nclassification loss (PreC-loss), post-change classification loss (PostC-loss),\nand change detection loss (Chg-loss) are used for improve model learning.\nThird, to disentangle information coupling in MODIS time series, novel\nspatial-spectral-temporal Mamba (SSTMamba) modules are designed. Last, to\nimprove Mamba model efficiency and remove computational cost, a sparse and\ndeformable Mamba (SDMamba) backbone is used in SSTMamba. On the MODIS\ntime-series dataset for Saskatchewan, Canada, we evaluate the method on\nland-cover change detection and LULC classification; results show about 1.5-6%\ngains in average F1 for change detection over baselines, and about 2%\nimprovements in OA, AA, and Kappa for LULC classification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u77e5\u8bc6\u611f\u77e5Mamba (KAMamba) \u7528\u4e8e\u589e\u5f3aMODIS\u53d8\u5316\u68c0\u6d4b\u3002", "motivation": "\u7531\u4e8eMODIS\u6df7\u5408\u50cf\u7d20\u3001\u7a7a\u95f4-\u5149\u8c31-\u65f6\u95f4\u4fe1\u606f\u8026\u5408\u6548\u5e94\u548c\u80cc\u666f\u7c7b\u5f02\u8d28\u6027\u7b49\u95ee\u9898\uff0c\u4f7f\u7528MODIS\u65f6\u95f4\u5e8f\u5217\u8fdb\u884c\u53d8\u5316\u68c0\u6d4b\u975e\u5e38\u5177\u6709\u6311\u6218\u6027\u3002", "method": "1. \u8bbe\u8ba1\u4e86\u4e00\u79cd\u77e5\u8bc6\u9a71\u52a8\u7684\u8f6c\u79fb\u77e9\u9635\u5f15\u5bfc\u65b9\u6cd5\uff0c\u751f\u6210\u77e5\u8bc6\u611f\u77e5\u8f6c\u79fb\u635f\u5931 (KAT-loss)\uff0c\u4ee5\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u30022. \u8bbe\u8ba1\u4e86\u4e00\u79cd\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e09\u79cd\u635f\u5931\uff0c\u5373\u53d8\u5316\u524d\u5206\u7c7b\u635f\u5931 (PreC-loss)\u3001\u53d8\u5316\u540e\u5206\u7c7b\u635f\u5931 (PostC-loss) \u548c\u53d8\u5316\u68c0\u6d4b\u635f\u5931 (Chg-loss) \u6765\u6539\u8fdb\u6a21\u578b\u5b66\u4e60\u30023. \u8bbe\u8ba1\u4e86\u65b0\u7684\u7a7a\u95f4-\u5149\u8c31-\u65f6\u95f4 Mamba (SSTMamba) \u6a21\u5757\uff0c\u4ee5\u89e3\u8026 MODIS \u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u4fe1\u606f\u8026\u5408\u30024. \u4f7f\u7528\u7a00\u758f\u548c\u53ef\u53d8\u5f62\u7684 Mamba (SDMamba) \u4e3b\u5e72\u6765\u63d0\u9ad8 Mamba \u6a21\u578b\u7684\u6548\u7387\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728\u52a0\u62ff\u5927\u8428\u65af\u5580\u5f7b\u6e29\u7701\u7684 MODIS \u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u571f\u5730\u8986\u76d6\u53d8\u5316\u68c0\u6d4b\u548c LULC \u5206\u7c7b\u8fdb\u884c\u4e86\u8bc4\u4f30\uff1b\u7ed3\u679c\u8868\u660e\uff0c\u53d8\u5316\u68c0\u6d4b\u7684\u5e73\u5747 F1 \u503c\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e86\u7ea6 1.5-6%\uff0cLULC \u5206\u7c7b\u7684 OA\u3001AA \u548c Kappa \u63d0\u9ad8\u4e86\u7ea6 2%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684KAMamba\u65b9\u6cd5\u5728MODIS\u65f6\u5e8f\u6570\u636e\u53d8\u5316\u68c0\u6d4b\u548c\u571f\u5730\u5229\u7528\u5206\u7c7b\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002"}}
{"id": "2510.09901", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09901", "abs": "https://arxiv.org/abs/2510.09901", "authors": ["Lianhao Zhou", "Hongyi Ling", "Cong Fu", "Yepeng Huang", "Michael Sun", "Wendi Yu", "Xiaoxuan Wang", "Xiner Li", "Xingyu Su", "Junkai Zhang", "Xiusi Chen", "Chenxing Liang", "Xiaofeng Qian", "Heng Ji", "Wei Wang", "Marinka Zitnik", "Shuiwang Ji"], "title": "Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics", "comment": null, "summary": "Computing has long served as a cornerstone of scientific discovery. Recently,\na paradigm shift has emerged with the rise of large language models (LLMs),\nintroducing autonomous systems, referred to as agents, that accelerate\ndiscovery across varying levels of autonomy. These language agents provide a\nflexible and versatile framework that orchestrates interactions with human\nscientists, natural language, computer language and code, and physics. This\npaper presents our view and vision of LLM-based scientific agents and their\ngrowing role in transforming the scientific discovery lifecycle, from\nhypothesis discovery, experimental design and execution, to result analysis and\nrefinement. We critically examine current methodologies, emphasizing key\ninnovations, practical achievements, and outstanding limitations. Additionally,\nwe identify open research challenges and outline promising directions for\nbuilding more robust, generalizable, and adaptive scientific agents. Our\nanalysis highlights the transformative potential of autonomous agents to\naccelerate scientific discovery across diverse domains.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6b63\u5728\u6539\u53d8\u79d1\u5b66\u53d1\u73b0\u7684\u65b9\u5f0f\uff0c\u5f15\u5165\u4e86\u53ef\u4ee5\u52a0\u901f\u4e0d\u540c\u81ea\u4e3b\u7a0b\u5ea6\u53d1\u73b0\u7684\u81ea\u4e3b\u7cfb\u7edf\uff0c\u5373\u667a\u80fd\u4ee3\u7406\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u4f5c\u7528\uff0c\u4ee5\u53ca\u5b83\u4eec\u5982\u4f55\u901a\u8fc7\u4e0e\u79d1\u5b66\u5bb6\u3001\u81ea\u7136\u8bed\u8a00\u3001\u8ba1\u7b97\u673a\u8bed\u8a00\u548c\u7269\u7406\u5b66\u7684\u4ea4\u4e92\u6765\u6539\u53d8\u79d1\u5b66\u53d1\u73b0\u7684\u751f\u547d\u5468\u671f\u3002", "method": "\u5206\u6790\u5f53\u524d\u57fa\u4e8eLLM\u7684\u79d1\u5b66\u667a\u80fd\u4ee3\u7406\u7684\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u5173\u952e\u521b\u65b0\u3001\u5b9e\u9645\u6210\u5c31\u548c\u5b58\u5728\u7684\u5c40\u9650\u6027\u3002", "result": "\u5f3a\u8c03\u4e86\u81ea\u4e3b\u667a\u80fd\u4ee3\u7406\u5728\u52a0\u901f\u4e0d\u540c\u9886\u57df\u79d1\u5b66\u53d1\u73b0\u65b9\u9762\u7684\u53d8\u9769\u6f5c\u529b\u3002", "conclusion": "\u603b\u7ed3\u4e86\u6784\u5efa\u66f4\u5f3a\u5927\u3001\u66f4\u901a\u7528\u548c\u66f4\u5177\u9002\u5e94\u6027\u7684\u79d1\u5b66\u667a\u80fd\u4ee3\u7406\u6240\u9762\u4e34\u7684\u5f00\u653e\u7814\u7a76\u6311\u6218\uff0c\u5e76\u6982\u8ff0\u4e86\u6709\u5e0c\u671b\u7684\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2510.09659", "categories": ["cs.LG", "hep-ex"], "pdf": "https://arxiv.org/pdf/2510.09659", "abs": "https://arxiv.org/abs/2510.09659", "authors": ["Edgar E. Robles", "Dikshant Sagar", "Alejandro Yankelevich", "Jianming Bian", "Pierre Baldi", "NOvA Collaboration"], "title": "Heterogeneous Point Set Transformers for Segmentation of Multiple View Particle Detectors", "comment": "Submitted to Machine Learning and the Physical Sciences Workshop\n  (ML4PS) at NeurIPS 2025", "summary": "NOvA is a long-baseline neutrino oscillation experiment that detects neutrino\nparticles from the NuMI beam at Fermilab. Before data from this experiment can\nbe used in analyses, raw hits in the detector must be matched to their source\nparticles, and the type of each particle must be identified. This task has\ncommonly been done using a mix of traditional clustering approaches and\nconvolutional neural networks (CNNs). Due to the construction of the detector,\nthe data is presented as two sparse 2D images: an XZ and a YZ view of the\ndetector, rather than a 3D representation. We propose a point set neural\nnetwork that operates on the sparse matrices with an operation that mixes\ninformation from both views. Our model uses less than 10% of the memory\nrequired using previous methods while achieving a 96.8% AUC score, a higher\nscore than obtained when both views are processed independently (85.4%).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u70b9\u96c6\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u5339\u914d\u63a2\u6d4b\u5668\u4e2d\u7684\u539f\u59cb\u547d\u4e2d\uff0c\u5e76\u8bc6\u522b\u6bcf\u4e2a\u7c92\u5b50\u7684\u7c7b\u578b\u3002", "motivation": "\u5728NOvA\u5b9e\u9a8c\u4e2d\uff0c\u9700\u8981\u5c06\u63a2\u6d4b\u5668\u4e2d\u7684\u539f\u59cb\u547d\u4e2d\u4e0e\u5176\u6e90\u7c92\u5b50\u5339\u914d\uff0c\u5e76\u8bc6\u522b\u6bcf\u4e2a\u7c92\u5b50\u7684\u7c7b\u578b\u3002\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528\u805a\u7c7b\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u4f46\u5185\u5b58\u9700\u6c42\u8f83\u9ad8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u70b9\u96c6\u795e\u7ecf\u7f51\u7edc\uff0c\u5b83\u53ef\u4ee5\u5904\u7406\u7a00\u758f\u77e9\u9635\uff0c\u5e76\u6df7\u5408\u6765\u81ea\u4e24\u4e2a\u89c6\u56fe\u7684\u4fe1\u606f\u3002\u8be5\u6a21\u578b\u76f4\u63a5\u5728\u7a00\u758f\u77e9\u9635\u4e0a\u64cd\u4f5c\uff0c\u5e76\u6df7\u5408\u6765\u81ea XZ \u548c YZ \u89c6\u56fe\u7684\u4fe1\u606f\u3002", "result": "\u8be5\u6a21\u578b\u5b9e\u73b0\u4e86 96.8% \u7684 AUC \u5206\u6570\uff0c\u9ad8\u4e8e\u72ec\u7acb\u5904\u7406\u4e24\u4e2a\u89c6\u56fe\u65f6\u83b7\u5f97\u7684\u5206\u6570 (85.4%)\uff0c\u540c\u65f6\u4f7f\u7528\u7684\u5185\u5b58\u6bd4\u4ee5\u524d\u7684\u65b9\u6cd5\u5c11 90% \u4ee5\u4e0a\u3002", "conclusion": "\u63d0\u51fa\u7684\u70b9\u96c6\u795e\u7ecf\u7f51\u7edc\u5728\u7c92\u5b50\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u5185\u5b58\u4f7f\u7528\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2510.10556", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10556", "abs": "https://arxiv.org/abs/2510.10556", "authors": ["Donglin Zhou", "Weike Pan", "Zhong Ming"], "title": "Self-Supervised Representation Learning with ID-Content Modality Alignment for Sequential Recommendation", "comment": null, "summary": "Sequential recommendation (SR) models often capture user preferences based on\nthe historically interacted item IDs, which usually obtain sub-optimal\nperformance when the interaction history is limited. Content-based sequential\nrecommendation has recently emerged as a promising direction that exploits\nitems' textual and visual features to enhance preference learning. However,\nthere are still three key challenges: (i) how to reduce the semantic gap\nbetween different content modality representations; (ii) how to jointly model\nuser behavior preferences and content preferences; and (iii) how to design an\neffective training strategy to align ID representations and content\nrepresentations. To address these challenges, we propose a novel model,\nself-supervised representation learning with ID-Content modality alignment,\nnamed SICSRec. Firstly, we propose a LLM-driven sample construction method and\ndevelop a supervised fine-tuning approach to align item-level modality\nrepresentations. Secondly, we design a novel Transformer-based sequential\nmodel, where an ID-modality sequence encoder captures user behavior\npreferences, a content-modality sequence encoder learns user content\npreferences, and a mix-modality sequence decoder grasps the intrinsic\nrelationship between these two types of preferences. Thirdly, we propose a\ntwo-step training strategy with a content-aware contrastive learning task to\nalign modality representations and ID representations, which decouples the\ntraining process of content modality dependency and item collaborative\ndependency. Extensive experiments conducted on four public video streaming\ndatasets demonstrate our SICSRec outperforms the state-of-the-art ID-modality\nsequential recommenders and content-modality sequential recommenders by 8.04%\non NDCG@5 and 6.62% on NDCD@10 on average, respectively.", "AI": {"tldr": "SICSRec\u6a21\u578b\u901a\u8fc7LLM\u9a71\u52a8\u7684\u6837\u672c\u6784\u5efa\u3001Transformer\u5e8f\u5217\u6a21\u578b\u548c\u5185\u5bb9\u611f\u77e5\u7684\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86ID\u548c\u5185\u5bb9\u6a21\u6001\u5bf9\u9f50\u7684\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\uff0c\u4ece\u800c\u63d0\u5347\u5e8f\u5217\u63a8\u8350\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u5e8f\u5217\u63a8\u8350\u6a21\u578b\u5728\u4ea4\u4e92\u5386\u53f2\u6709\u9650\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u5185\u5bb9\u611f\u77e5\u7684\u5e8f\u5217\u63a8\u8350\u662f\u5f88\u6709\u6f5c\u529b\u7684\u65b9\u5411\uff0c\u4f46\u9762\u4e34\u7740\u6a21\u6001\u8bed\u4e49\u9e3f\u6c9f\u3001\u7528\u6237\u884c\u4e3a\u548c\u5185\u5bb9\u504f\u597d\u8054\u5408\u5efa\u6a21\u4ee5\u53caID\u548c\u5185\u5bb9\u8868\u793a\u5bf9\u9f50\u7684\u6311\u6218\u3002", "method": "1. \u63d0\u51faLLM\u9a71\u52a8\u7684\u6837\u672c\u6784\u5efa\u65b9\u6cd5\u548c\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u5bf9\u9f50item-level\u7684\u6a21\u6001\u8868\u793a\u3002\n2. \u8bbe\u8ba1Transformer\u5e8f\u5217\u6a21\u578b\uff0c\u4f7f\u7528ID-\u6a21\u6001\u5e8f\u5217\u7f16\u7801\u5668\u6355\u83b7\u7528\u6237\u884c\u4e3a\u504f\u597d\uff0c\u5185\u5bb9\u6a21\u6001\u5e8f\u5217\u7f16\u7801\u5668\u5b66\u4e60\u7528\u6237\u5185\u5bb9\u504f\u597d\uff0c\u6df7\u5408\u6a21\u6001\u5e8f\u5217\u89e3\u7801\u5668\u628a\u63e1\u4e24\u8005\u5173\u7cfb\u3002\n3. \u63d0\u51fa\u4e24\u6b65\u8bad\u7ec3\u7b56\u7565\uff0c\u4f7f\u7528\u5185\u5bb9\u611f\u77e5\u7684\u5bf9\u6bd4\u5b66\u4e60\u4efb\u52a1\u5bf9\u9f50\u6a21\u6001\u8868\u793a\u548cID\u8868\u793a\u3002", "result": "\u5728\u56db\u4e2a\u89c6\u9891\u6d41\u6570\u636e\u96c6\u4e0a\uff0cSICSRec\u6a21\u578b\u5728NDCG@5\u548cNDCG@10\u4e0a\u5206\u522b\u5e73\u5747\u8d85\u8fc7\u73b0\u6709\u6700\u4f73ID-\u6a21\u6001\u5e8f\u5217\u63a8\u8350\u56688.04%\u548c\u5185\u5bb9-\u6a21\u6001\u5e8f\u5217\u63a8\u8350\u56686.62%\u3002", "conclusion": "SICSRec\u6a21\u578b\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5185\u5bb9\u611f\u77e5\u5e8f\u5217\u63a8\u8350\u4e2d\u7684\u6a21\u6001\u8bed\u4e49\u9e3f\u6c9f\u3001\u7528\u6237\u884c\u4e3a\u548c\u5185\u5bb9\u504f\u597d\u8054\u5408\u5efa\u6a21\u4ee5\u53caID\u548c\u5185\u5bb9\u8868\u793a\u5bf9\u9f50\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.10580", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.10580", "abs": "https://arxiv.org/abs/2510.10580", "authors": ["Jiahao He", "Yutao Cui", "Cuiping Li", "Jikang Jiang", "Yuheng Hou", "Hong Chen"], "title": "AQORA: A Learned Adaptive Query Optimizer for Spark SQL", "comment": "14 pages, 11 figures", "summary": "Recent studies have identified two main approaches to improve query\noptimization: learned query optimization (LQO), which generates or selects\nbetter query plans before execution based on models trained in advance, and\nadaptive query processing (AQP), which adapts the query plan during execution\nbased on statistical feedback collected at runtime. Although both approaches\nhave shown promise, they also face critical limitations. LQO must commit to a\nfixed plan without access to actual cardinalities and typically rely on a\nsingle end-to-end feedback signal, making learning inefficient. On the other\nhand, AQP depends heavily on rule-based heuristics and lacks the ability to\nlearn from experience. In this paper, we present AQORA, an adaptive query\noptimizer with a reinforcement learning architecture that combines the\nstrengths of both LQO and AQP. AQORA addresses the above challenges through\nfour core strategies: (1) realistic feature encoding, (2) query stage-level\nfeedback and intervention, (3) automatic strategy adaptation, and (4) low-cost\nintegration. Experiments show that AQORA reduces end-to-end execution time by\nup to 90% compared to other learned methods and by up to 70% compared to Spark\nSQL's default configuration with adaptive query execution.", "AI": {"tldr": "AQORA\u7ed3\u5408\u4e86\u5b66\u4e60\u578b\u67e5\u8be2\u4f18\u5316\uff08LQO\uff09\u548c\u81ea\u9002\u5e94\u67e5\u8be2\u5904\u7406\uff08AQP\uff09\u7684\u4f18\u70b9\uff0c\u4ee5\u514b\u670d\u5b83\u4eec\u5404\u81ea\u7684\u5c40\u9650\u6027\u3002", "motivation": "LQO\u5728\u6ca1\u6709\u5b9e\u9645\u57fa\u6570\u7684\u60c5\u51b5\u4e0b\u5fc5\u987b\u63d0\u4ea4\u4e00\u4e2a\u56fa\u5b9a\u7684\u8ba1\u5212\uff0c\u5e76\u4e14\u901a\u5e38\u4f9d\u8d56\u4e8e\u5355\u4e00\u7684\u7aef\u5230\u7aef\u53cd\u9988\u4fe1\u53f7\uff0c\u8fd9\u4f7f\u5f97\u5b66\u4e60\u6548\u7387\u4f4e\u4e0b\u3002\u53e6\u4e00\u65b9\u9762\uff0cAQP\u4e25\u91cd\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e76\u4e14\u7f3a\u4e4f\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\u3002", "method": "AQORA\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u67b6\u6784\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u6838\u5fc3\u7b56\u7565\u6765\u89e3\u51b3\u4e0a\u8ff0\u6311\u6218\uff1a\uff081\uff09\u73b0\u5b9e\u7684\u7279\u5f81\u7f16\u7801\uff0c\uff082\uff09\u67e5\u8be2\u9636\u6bb5\u7ea7\u522b\u7684\u53cd\u9988\u548c\u5e72\u9884\uff0c\uff083\uff09\u81ea\u52a8\u7b56\u7565\u9002\u5e94\uff0c\u4ee5\u53ca\uff084\uff09\u4f4e\u6210\u672c\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5176\u4ed6\u5b66\u4e60\u65b9\u6cd5\u76f8\u6bd4\uff0cAQORA\u6700\u591a\u53ef\u51cf\u5c1190%\u7684\u7aef\u5230\u7aef\u6267\u884c\u65f6\u95f4\uff0c\u4e0eSpark SQL\u7684\u9ed8\u8ba4\u914d\u7f6e\uff08\u5177\u6709\u81ea\u9002\u5e94\u67e5\u8be2\u6267\u884c\uff09\u76f8\u6bd4\uff0c\u6700\u591a\u53ef\u51cf\u5c1170%\u3002", "conclusion": "AQORA\u662f\u4e00\u79cd\u6709\u6548\u7684\u81ea\u9002\u5e94\u67e5\u8be2\u4f18\u5316\u5668\uff0c\u5b83\u7ed3\u5408\u4e86LQO\u548cAQP\u7684\u4f18\u70b9\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.09714", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09714", "abs": "https://arxiv.org/abs/2510.09714", "authors": ["Shiyuan Guo", "Henry Sleight", "Fabien Roger"], "title": "All Code, No Thought: Current Language Models Struggle to Reason in Ciphered Language", "comment": null, "summary": "Detecting harmful AI actions is important as AI agents gain adoption.\nChain-of-thought (CoT) monitoring is one method widely used to detect\nadversarial attacks and AI misalignment. However, attackers and misaligned\nmodels might evade CoT monitoring through ciphered reasoning: reasoning hidden\nin encrypted, translated, or compressed text. To assess this risk, we test\nwhether models can perform ciphered reasoning. For each of 28 different\nciphers, we fine-tune and prompt up to 10 models to reason in that cipher. We\nmeasure model accuracy on math problems as a proxy for reasoning ability.\nAcross the models we test, we find an asymmetry: model accuracy can drop\nsignificantly when reasoning in ciphered text, even though models demonstrate\ncomprehension of ciphered text by being able to translate it accurately to\nEnglish. Even frontier models struggle with lesser-known ciphers, although they\ncan reason accurately in well-known ciphers like rot13. We show that ciphered\nreasoning capability correlates with cipher prevalence in pretraining data. We\nalso identify scaling laws showing that ciphered reasoning capability improves\nslowly with additional fine-tuning data. Our work suggests that evading CoT\nmonitoring using ciphered reasoning may be an ineffective tactic for current\nmodels and offers guidance on constraining the development of this capability\nin future frontier models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6a21\u578b\u5728\u52a0\u5bc6\u6587\u672c\u4e2d\u8fdb\u884c\u63a8\u7406\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u4e0d\u719f\u6089\u7684\u5bc6\u7801\u4e2d\u63a8\u7406\u65f6\u51c6\u786e\u7387\u4f1a\u663e\u8457\u4e0b\u964d\uff0c\u5373\u4f7f\u5b83\u4eec\u80fd\u591f\u51c6\u786e\u5730\u5c06\u5bc6\u6587\u7ffb\u8bd1\u6210\u82f1\u6587\u3002\u8fd9\u8868\u660e\u4f7f\u7528\u52a0\u5bc6\u63a8\u7406\u6765\u9003\u907f CoT \u76d1\u63a7\u5bf9\u4e8e\u5f53\u524d\u7684\u6a21\u578b\u53ef\u80fd\u4e0d\u662f\u4e00\u79cd\u6709\u6548\u7684\u7b56\u7565\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u7684\u666e\u53ca\uff0c\u68c0\u6d4b\u6709\u5bb3\u7684\u4eba\u5de5\u667a\u80fd\u884c\u4e3a\u975e\u5e38\u91cd\u8981\u3002\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u76d1\u63a7\u662f\u5e7f\u6cdb\u7528\u4e8e\u68c0\u6d4b\u5bf9\u6297\u6027\u653b\u51fb\u548c\u4eba\u5de5\u667a\u80fd\u4e0d\u5bf9\u9f50\u7684\u4e00\u79cd\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u653b\u51fb\u8005\u548c\u4e0d\u5bf9\u9f50\u7684\u6a21\u578b\u53ef\u80fd\u4f1a\u901a\u8fc7\u52a0\u5bc6\u63a8\u7406\u6765\u9003\u907f CoT \u76d1\u63a7\uff1a\u9690\u85cf\u5728\u52a0\u5bc6\u3001\u7ffb\u8bd1\u6216\u538b\u7f29\u6587\u672c\u4e2d\u7684\u63a8\u7406\u3002", "method": "\u5bf9\u4e8e 28 \u79cd\u4e0d\u540c\u7684\u5bc6\u7801\uff0c\u6211\u4eec\u5bf9\u591a\u8fbe 10 \u4e2a\u6a21\u578b\u8fdb\u884c\u4e86\u5fae\u8c03\u548c\u63d0\u793a\uff0c\u4ee5\u4f7f\u7528\u8be5\u5bc6\u7801\u8fdb\u884c\u63a8\u7406\u3002\u6211\u4eec\u6d4b\u91cf\u4e86\u6a21\u578b\u5728\u6570\u5b66\u95ee\u9898\u4e0a\u7684\u51c6\u786e\u6027\uff0c\u4ee5\u6b64\u4f5c\u4e3a\u63a8\u7406\u80fd\u529b\u7684\u4ee3\u8868\u3002", "result": "\u5728\u6211\u4eec\u6d4b\u8bd5\u7684\u6a21\u578b\u4e2d\uff0c\u6211\u4eec\u53d1\u73b0\u4e86\u4e00\u79cd\u4e0d\u5bf9\u79f0\u6027\uff1a\u5f53\u4f7f\u7528\u52a0\u5bc6\u6587\u672c\u8fdb\u884c\u63a8\u7406\u65f6\uff0c\u6a21\u578b\u51c6\u786e\u7387\u4f1a\u663e\u8457\u4e0b\u964d\uff0c\u5373\u4f7f\u6a21\u578b\u901a\u8fc7\u80fd\u591f\u51c6\u786e\u5730\u5c06\u5176\u7ffb\u8bd1\u6210\u82f1\u8bed\u6765\u8bc1\u660e\u5176\u5bf9\u52a0\u5bc6\u6587\u672c\u7684\u7406\u89e3\u3002\u5373\u4f7f\u662f\u524d\u6cbf\u6a21\u578b\u4e5f\u5728\u4e0d\u592a\u77e5\u540d\u7684\u5bc6\u7801\u4e2d\u6323\u624e\uff0c\u5c3d\u7ba1\u5b83\u4eec\u53ef\u4ee5\u4f7f\u7528\u50cf rot13 \u8fd9\u6837\u7684\u8457\u540d\u5bc6\u7801\u8fdb\u884c\u51c6\u786e\u7684\u63a8\u7406\u3002\u6211\u4eec\u8868\u660e\uff0c\u52a0\u5bc6\u63a8\u7406\u80fd\u529b\u4e0e\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u5bc6\u7801\u7684\u6d41\u884c\u7a0b\u5ea6\u76f8\u5173\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u8868\u660e\uff0c\u4f7f\u7528\u52a0\u5bc6\u63a8\u7406\u6765\u9003\u907f CoT \u76d1\u63a7\u5bf9\u4e8e\u5f53\u524d\u7684\u6a21\u578b\u53ef\u80fd\u4e0d\u662f\u4e00\u79cd\u6709\u6548\u7684\u7b56\u7565\uff0c\u5e76\u4e3a\u7ea6\u675f\u672a\u6765\u524d\u6cbf\u6a21\u578b\u4e2d\u8fd9\u79cd\u80fd\u529b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2510.09681", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09681", "abs": "https://arxiv.org/abs/2510.09681", "authors": ["Sashank Makanaboyina"], "title": "NNDM: NN_UNet Diffusion Model for Brain Tumor Segmentation", "comment": null, "summary": "Accurate detection and segmentation of brain tumors in magnetic resonance\nimaging (MRI) are critical for effective diagnosis and treatment planning.\nDespite advances in convolutional neural networks (CNNs) such as U-Net,\nexisting models often struggle with generalization, boundary precision, and\nlimited data diversity. To address these challenges, we propose NNDM (NN\\_UNet\nDiffusion Model)a hybrid framework that integrates the robust feature\nextraction of NN-UNet with the generative capabilities of diffusion\nprobabilistic models. In our approach, the diffusion model progressively\nrefines the segmentation masks generated by NN-UNet by learning the residual\nerror distribution between predicted and ground-truth masks. This iterative\ndenoising process enables the model to correct fine structural inconsistencies\nand enhance tumor boundary delineation. Experiments conducted on the BraTS 2021\ndatasets demonstrate that NNDM achieves superior performance compared to\nconventional U-Net and transformer-based baselines, yielding improvements in\nDice coefficient and Hausdorff distance metrics. Moreover, the diffusion-guided\nrefinement enhances robustness across modalities and tumor subregions. The\nproposed NNDM establishes a new direction for combining deterministic\nsegmentation networks with stochastic diffusion models, advancing the state of\nthe art in automated brain tumor analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u6846\u67b6NNDM\uff0c\u5b83\u7ed3\u5408\u4e86NN-UNet\u7684\u9c81\u68d2\u7279\u5f81\u63d0\u53d6\u548c\u6269\u6563\u6982\u7387\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\uff0c\u7528\u4e8e\u8111\u80bf\u7624\u5206\u5272\u3002", "motivation": "\u73b0\u6709\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u6cdb\u5316\u6027\u3001\u8fb9\u754c\u7cbe\u5ea6\u548c\u6709\u9650\u7684\u6570\u636e\u591a\u6837\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5b66\u4e60\u9884\u6d4b\u63a9\u7801\u548c\u771f\u5b9e\u63a9\u7801\u4e4b\u95f4\u7684\u6b8b\u5dee\u8bef\u5dee\u5206\u5e03\uff0c\u6269\u6563\u6a21\u578b\u9010\u6b65\u7ec6\u5316NN-UNet\u751f\u6210\u7684\u5206\u5272\u63a9\u7801\u3002\u8fd9\u4e2a\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u4f7f\u6a21\u578b\u80fd\u591f\u7ea0\u6b63\u7cbe\u7ec6\u7684\u7ed3\u6784\u4e0d\u4e00\u81f4\u6027\u5e76\u589e\u5f3a\u80bf\u7624\u8fb9\u754c\u7684\u63cf\u7ed8\u3002", "result": "\u5728BraTS 2021\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u7684U-Net\u548c\u57fa\u4e8eTransformer\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0cNNDM\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u5728Dice\u7cfb\u6570\u548cHausdorff\u8ddd\u79bb\u6307\u6807\u4e0a\u90fd\u6709\u6240\u63d0\u9ad8\u3002\u6b64\u5916\uff0c\u6269\u6563\u5f15\u5bfc\u7684\u7ec6\u5316\u589e\u5f3a\u4e86\u8de8\u6a21\u6001\u548c\u80bf\u7624\u4e9a\u533a\u57df\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684NNDM\u4e3a\u7ed3\u5408\u786e\u5b9a\u6027\u5206\u5272\u7f51\u7edc\u4e0e\u968f\u673a\u6269\u6563\u6a21\u578b\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u65b9\u5411\uff0c\u63a8\u8fdb\u4e86\u81ea\u52a8\u8111\u80bf\u7624\u5206\u6790\u7684\u6700\u65b0\u6280\u672f\u3002"}}
{"id": "2510.09905", "categories": ["cs.AI", "cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.09905", "abs": "https://arxiv.org/abs/2510.09905", "authors": ["Xi Fang", "Weijie Xu", "Yuchong Zhang", "Stephanie Eckman", "Scott Nickleach", "Chandan K. Reddy"], "title": "The Personalization Trap: How User Memory Alters Emotional Reasoning in LLMs", "comment": "12 pages 5 figures", "summary": "When an AI assistant remembers that Sarah is a single mother working two\njobs, does it interpret her stress differently than if she were a wealthy\nexecutive? As personalized AI systems increasingly incorporate long-term user\nmemory, understanding how this memory shapes emotional reasoning is critical.\nWe investigate how user memory affects emotional intelligence in large language\nmodels (LLMs) by evaluating 15 models on human validated emotional intelligence\ntests. We find that identical scenarios paired with different user profiles\nproduce systematically divergent emotional interpretations. Across validated\nuser independent emotional scenarios and diverse user profiles, systematic\nbiases emerged in several high-performing LLMs where advantaged profiles\nreceived more accurate emotional interpretations. Moreover, LLMs demonstrate\nsignificant disparities across demographic factors in emotion understanding and\nsupportive recommendations tasks, indicating that personalization mechanisms\ncan embed social hierarchies into models emotional reasoning. These results\nhighlight a key challenge for memory enhanced AI: systems designed for\npersonalization may inadvertently reinforce social inequalities.", "AI": {"tldr": "\u7528\u6237\u8bb0\u5fc6\u4f1a\u5f71\u54cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u60c5\u611f\u667a\u80fd\uff0c\u5bfc\u81f4\u5bf9\u4e0d\u540c\u7528\u6237\u4ea7\u751f\u504f\u5dee\u3002", "motivation": "\u7814\u7a76\u7528\u6237\u8bb0\u5fc6\u5982\u4f55\u5f71\u54cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u60c5\u611f\u667a\u80fd\u3002", "method": "\u5728\u7ecf\u8fc7\u4eba\u5de5\u9a8c\u8bc1\u7684\u60c5\u611f\u667a\u80fd\u6d4b\u8bd5\u4e2d\u8bc4\u4f30\u4e86 15 \u4e2a\u6a21\u578b\uff0c\u5c06\u76f8\u540c\u7684\u573a\u666f\u4e0e\u4e0d\u540c\u7684\u7528\u6237\u8d44\u6599\u914d\u5bf9\u3002", "result": "\u4e0d\u540c\u7684\u7528\u6237\u8d44\u6599\u4f1a\u5bfc\u81f4\u7cfb\u7edf\u6027\u4e0d\u540c\u7684\u60c5\u611f\u89e3\u8bfb\u3002\u5728\u591a\u4e2a\u9ad8\u6027\u80fd\u7684LLM\u4e2d\uff0c\u4f18\u52bf\u7fa4\u4f53\u7684\u60c5\u611f\u89e3\u8bfb\u66f4\u4e3a\u51c6\u786e\uff0c\u5e76\u4e14LLM\u5728\u60c5\u611f\u7406\u89e3\u548c\u652f\u6301\u6027\u5efa\u8bae\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u5dee\u8ddd\u3002", "conclusion": "\u4e3a\u4e2a\u6027\u5316\u8bbe\u8ba1\u7684\u7cfb\u7edf\u53ef\u80fd\u4f1a\u65e0\u610f\u4e2d\u5f3a\u5316\u793e\u4f1a\u4e0d\u5e73\u7b49\u3002"}}
{"id": "2510.09660", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09660", "abs": "https://arxiv.org/abs/2510.09660", "authors": ["Luca Scimeca", "Thomas Jiralerspong", "Berton Earnshaw", "Jason Hartford", "Yoshua Bengio"], "title": "Learning What Matters: Steering Diffusion via Spectrally Anisotropic Forward Noise", "comment": null, "summary": "Diffusion Probabilistic Models (DPMs) have achieved strong generative\nperformance, yet their inductive biases remain largely implicit. In this work,\nwe aim to build inductive biases into the training and sampling of diffusion\nmodels to better accommodate the target distribution of the data to model. We\nintroduce an anisotropic noise operator that shapes these biases by replacing\nthe isotropic forward covariance with a structured, frequency-diagonal\ncovariance. This operator unifies band-pass masks and power-law weightings,\nallowing us to emphasize or suppress designated frequency bands, while keeping\nthe forward process Gaussian. We refer to this as spectrally anisotropic\nGaussian diffusion (SAGD). In this work, we derive the score relation for\nanisotropic covariances and show that, under full support, the learned score\nconverges to the true data score as $t\\!\\to\\!0$, while anisotropy reshapes the\nprobability-flow path from noise to data. Empirically, we show the induced\nanisotropy outperforms standard diffusion across several vision datasets, and\nenables selective omission: learning while ignoring known corruptions confined\nto specific bands. Together, these results demonstrate that carefully designed\nanisotropic forward noise provides a simple, yet principled, handle to tailor\ninductive bias in DPMs.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6784\u5efa\u5404\u5411\u5f02\u6027\u566a\u58f0\u7b97\u5b50\uff0c\u5c06\u5f52\u7eb3\u504f\u7f6e\u5f15\u5165\u6269\u6563\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u91c7\u6837\u8fc7\u7a0b\u4e2d\uff0c\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u76ee\u6807\u6570\u636e\u5206\u5e03\u3002", "motivation": "\u6269\u6563\u6982\u7387\u6a21\u578b(DPMs) \u5177\u6709\u5f3a\u5927\u7684\u751f\u6210\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u7684\u5f52\u7eb3\u504f\u7f6e\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u7136\u662f\u9690\u542b\u7684\u3002\u672c\u6587\u65e8\u5728\u5c06\u5f52\u7eb3\u504f\u7f6e\u6784\u5efa\u5230\u6269\u6563\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u91c7\u6837\u4e2d\uff0c\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u8981\u5efa\u6a21\u7684\u6570\u636e\u7684\u76ee\u6807\u5206\u5e03\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u5404\u5411\u5f02\u6027\u566a\u58f0\u7b97\u5b50\uff0c\u901a\u8fc7\u7528\u7ed3\u6784\u5316\u7684\u9891\u7387\u5bf9\u89d2\u534f\u65b9\u5dee\u4ee3\u66ff\u5404\u5411\u540c\u6027\u524d\u5411\u534f\u65b9\u5dee\u6765\u5851\u9020\u8fd9\u4e9b\u504f\u5dee\u3002\u8be5\u7b97\u5b50\u7edf\u4e00\u4e86\u5e26\u901a\u63a9\u6a21\u548c\u5e42\u5f8b\u6743\u91cd\uff0c\u5141\u8bb8\u6211\u4eec\u5f3a\u8c03\u6216\u6291\u5236\u6307\u5b9a\u7684\u9891\u5e26\uff0c\u540c\u65f6\u4fdd\u6301\u524d\u5411\u8fc7\u7a0b\u4e3a\u9ad8\u65af\u5206\u5e03\u3002\u6211\u4eec\u5c06\u5176\u79f0\u4e3a\u8c31\u5404\u5411\u5f02\u6027\u9ad8\u65af\u6269\u6563 (SAGD)\u3002", "result": "\u6211\u4eec\u63a8\u5bfc\u4e86\u5404\u5411\u5f02\u6027\u534f\u65b9\u5dee\u7684\u5f97\u5206\u5173\u7cfb\uff0c\u5e76\u8868\u660e\u5728\u5b8c\u5168\u652f\u6301\u4e0b\uff0c\u5f53 $t\\!\\\\to\\!0$ \u65f6\uff0c\u5b66\u4e60\u5230\u7684\u5f97\u5206\u6536\u655b\u5230\u771f\u5b9e\u6570\u636e\u5f97\u5206\uff0c\u800c\u5404\u5411\u5f02\u6027\u91cd\u5851\u4e86\u4ece\u566a\u58f0\u5230\u6570\u636e\u7684\u6982\u7387\u6d41\u8def\u5f84\u3002\u5728\u7ecf\u9a8c\u4e0a\uff0c\u6211\u4eec\u8868\u660e\uff0c\u5728\u591a\u4e2a\u89c6\u89c9\u6570\u636e\u96c6\u4e2d\uff0c\u611f\u5e94\u5404\u5411\u5f02\u6027\u4f18\u4e8e\u6807\u51c6\u6269\u6563\uff0c\u5e76\u4e14\u80fd\u591f\u8fdb\u884c\u9009\u62e9\u6027\u7701\u7565\uff1a\u5728\u5ffd\u7565\u9650\u5236\u5728\u7279\u5b9a\u9891\u5e26\u7684\u5df2\u77e5\u635f\u574f\u7684\u540c\u65f6\u8fdb\u884c\u5b66\u4e60\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u5171\u540c\u8868\u660e\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5404\u5411\u5f02\u6027\u524d\u5411\u566a\u58f0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u539f\u5219\u7684\u5904\u7406\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728 DPM \u4e2d\u5b9a\u5236\u5f52\u7eb3\u504f\u7f6e\u3002"}}
{"id": "2510.10564", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10564", "abs": "https://arxiv.org/abs/2510.10564", "authors": ["Liang Li", "Zhou Yang", "Xiaofei Zhu"], "title": "Multi-Granularity Sequence Denoising with Weakly Supervised Signal for Sequential Recommendation", "comment": null, "summary": "Sequential recommendation aims to predict the next item based on user\ninterests in historical interaction sequences. Historical interaction sequences\noften contain irrelevant noisy items, which significantly hinders the\nperformance of recommendation systems. Existing research employs unsupervised\nmethods that indirectly identify item-granularity irrelevant noise by\npredicting the ground truth item. Since these methods lack explicit noise\nlabels, they are prone to misidentify users' interested items as noise.\nAdditionally, while these methods focus on removing item-granularity noise\ndriven by the ground truth item, they overlook interest-granularity noise,\nlimiting their ability to perform broader denoising based on user interests. To\naddress these issues, we propose Multi-Granularity Sequence Denoising with\nWeakly Supervised Signal for Sequential Recommendation(MGSD-WSS). MGSD-WSS\nfirst introduces the Multiple Gaussian Kernel Perceptron module to map the\noriginal and enhance sequence into a common representation space and utilizes\nweakly supervised signals to accurately identify noisy items in the historical\ninteraction sequence. Subsequently, it employs the item-granularity denoising\nmodule with noise-weighted contrastive learning to obtain denoised item\nrepresentations. Then, it extracts target interest representations from the\nground truth item and applies noise-weighted contrastive learning to obtain\ndenoised interest representations. Finally, based on the denoised item and\ninterest representations, MGSD-WSS predicts the next item. Extensive\nexperiments on five datasets demonstrate that the proposed method significantly\noutperforms state-of-the-art sequence recommendation and denoising models. Our\ncode is available at https://github.com/lalunex/MGSD-WSS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMGSD-WSS\u7684\u591a\u7c92\u5ea6\u5e8f\u5217\u53bb\u566a\u6846\u67b6\uff0c\u7528\u4e8e\u5e8f\u5217\u63a8\u8350\uff0c\u65e8\u5728\u89e3\u51b3\u5386\u53f2\u4ea4\u4e92\u5e8f\u5217\u4e2d\u65e0\u5173\u566a\u58f0\u9879\u76ee\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5bb9\u6613\u5c06\u7528\u6237\u611f\u5174\u8da3\u7684\u9879\u76ee\u8bef\u8ba4\u4e3a\u566a\u58f0\uff0c\u4e14\u5ffd\u7565\u4e86\u5174\u8da3\u7c92\u5ea6\u7684\u566a\u58f0\u3002", "method": "1. \u5f15\u5165\u591a\u9ad8\u65af\u6838\u611f\u77e5\u5668\u6a21\u5757\uff0c\u5c06\u539f\u59cb\u5e8f\u5217\u548c\u589e\u5f3a\u5e8f\u5217\u6620\u5c04\u5230\u516c\u5171\u8868\u793a\u7a7a\u95f4\u3002\n2. \u5229\u7528\u5f31\u76d1\u7763\u4fe1\u53f7\u51c6\u786e\u8bc6\u522b\u5386\u53f2\u4ea4\u4e92\u5e8f\u5217\u4e2d\u7684\u566a\u58f0\u9879\u76ee\u3002\n3. \u91c7\u7528\u5e26\u566a\u58f0\u52a0\u6743\u5bf9\u6bd4\u5b66\u4e60\u7684\u9879\u76ee\u7c92\u5ea6\u53bb\u566a\u6a21\u5757\uff0c\u83b7\u5f97\u53bb\u566a\u7684\u9879\u76ee\u8868\u793a\u3002\n4. \u4eceground truth\u9879\u76ee\u4e2d\u63d0\u53d6\u76ee\u6807\u5174\u8da3\u8868\u793a\uff0c\u5e76\u5e94\u7528\u566a\u58f0\u52a0\u6743\u5bf9\u6bd4\u5b66\u4e60\u83b7\u5f97\u53bb\u566a\u7684\u5174\u8da3\u8868\u793a\u3002\n5. \u57fa\u4e8e\u53bb\u566a\u7684\u9879\u76ee\u548c\u5174\u8da3\u8868\u793a\uff0c\u9884\u6d4b\u4e0b\u4e00\u4e2a\u9879\u76ee\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5e8f\u5217\u63a8\u8350\u548c\u53bb\u566a\u6a21\u578b\u3002", "conclusion": "MGSD-WSS\u6709\u6548\u5730\u63d0\u5347\u4e86\u5e8f\u5217\u63a8\u8350\u7684\u6027\u80fd\uff0c\u5e76\u5728\u53bb\u566a\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.10858", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.10858", "abs": "https://arxiv.org/abs/2510.10858", "authors": ["Guanli Liu", "Renata Borovica-Gajic"], "title": "DriftBench: Defining and Generating Data and Query Workload Drift for Benchmarking", "comment": null, "summary": "Data and workload drift are key to evaluating database components such as\ncaching, cardinality estimation, indexing, and query optimization. Yet,\nexisting benchmarks are static, offering little to no support for modeling\ndrift. This limitation stems from the lack of clear definitions and tools for\ngenerating data and workload drift. Motivated by this gap, we propose a unified\ntaxonomy for data and workload drift, grounded in observations from both\nacademia and industry. Building on this foundation, we introduce DriftBench, a\nlightweight and extensible framework for generating data and workload drift in\nbenchmark inputs. Together, the taxonomy and DriftBench provide a standardized\nvocabulary and mechanism for modeling and generating drift in benchmarking. We\ndemonstrate their effectiveness through case studies involving data drift,\nworkload drift, and drift-aware cardinality estimation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u6a21\u62df\u548c\u751f\u6210\u6570\u636e\u5e93\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6570\u636e\u548c\u5de5\u4f5c\u8d1f\u8f7d\u6f02\u79fb\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u662f\u9759\u6001\u7684\uff0c\u65e0\u6cd5\u652f\u6301\u6f02\u79fb\u5efa\u6a21\uff0c\u7f3a\u4e4f\u6e05\u6670\u7684\u5b9a\u4e49\u548c\u5de5\u5177\u6765\u751f\u6210\u6570\u636e\u548c\u5de5\u4f5c\u8d1f\u8f7d\u6f02\u79fb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6570\u636e\u548c\u5de5\u4f5c\u8d1f\u8f7d\u6f02\u79fb\u5206\u7c7b\u6cd5\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5f15\u5165\u4e86DriftBench\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u548c\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u57fa\u51c6\u6d4b\u8bd5\u8f93\u5165\u4e2d\u751f\u6210\u6570\u636e\u548c\u5de5\u4f5c\u8d1f\u8f7d\u6f02\u79fb\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\uff0c\u5305\u62ec\u6570\u636e\u6f02\u79fb\u3001\u5de5\u4f5c\u8d1f\u8f7d\u6f02\u79fb\u548c\u6f02\u79fb\u611f\u77e5\u57fa\u6570\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u5206\u7c7b\u6cd5\u548cDriftBench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u8bcd\u6c47\u8868\u548c\u673a\u5236\uff0c\u7528\u4e8e\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5efa\u6a21\u548c\u751f\u6210\u6f02\u79fb\u3002"}}
{"id": "2510.09720", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09720", "abs": "https://arxiv.org/abs/2510.09720", "authors": ["Haoran Sun", "Zekun Zhang", "Shaoning Zeng"], "title": "Preference-Aware Memory Update for Long-Term LLM Agents", "comment": null, "summary": "One of the key factors influencing the reasoning capabilities of LLM-based\nagents is their ability to leverage long-term memory. Integrating long-term\nmemory mechanisms allows agents to make informed decisions grounded in\nhistorical interactions. While recent advances have significantly improved the\nstorage and retrieval components, by encoding memory into dense vectors for\nsimilarity search or organizing memory as structured knowledge graphs most\nexisting approaches fall short in memory updating. In particular, they lack\nmechanisms for dynamically refining preference memory representations in\nresponse to evolving user behaviors and contexts. To address this gap, we\npropose a Preference-Aware Memory Update Mechanism (PAMU) that enables dynamic\nand personalized memory refinement. By integrating sliding window averages (SW)\nwith exponential moving averages (EMA), PAMU constructs a fused\npreference-aware representation that captures both short-term fluctuations and\nlong-term user tendencies. We conduct experiments on five task scenarios of the\nLoCoMo dataset, and the results show that our mechanism can significantly\nimprove the output quality of LLM in five baselines, validating its\neffectiveness in long-term conversations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u504f\u597d\u611f\u77e5\u8bb0\u5fc6\u66f4\u65b0\u673a\u5236\uff08PAMU\uff09\uff0c\u901a\u8fc7\u878d\u5408\u6ed1\u52a8\u7a97\u53e3\u5e73\u5747\uff08SW\uff09\u548c\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff08EMA\uff09\u6765\u6355\u83b7\u77ed\u671f\u6ce2\u52a8\u548c\u957f\u671f\u7528\u6237\u8d8b\u52bf\uff0c\u4ece\u800c\u5b9e\u73b0\u52a8\u6001\u548c\u4e2a\u6027\u5316\u7684\u8bb0\u5fc6\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u4f18\u5316\u504f\u597d\u8bb0\u5fc6\u8868\u793a\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6839\u636e\u4e0d\u65ad\u53d8\u5316\u7684\u7528\u6237\u884c\u4e3a\u548c\u73af\u5883\u8fdb\u884c\u8c03\u6574\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u504f\u597d\u611f\u77e5\u8bb0\u5fc6\u66f4\u65b0\u673a\u5236\uff08PAMU\uff09\uff0c\u901a\u8fc7\u6574\u5408\u6ed1\u52a8\u7a97\u53e3\u5e73\u5747\uff08SW\uff09\u4e0e\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff08EMA\uff09\u6784\u5efa\u878d\u5408\u7684\u504f\u597d\u611f\u77e5\u8868\u793a\u3002", "result": "\u5728LoCoMo\u6570\u636e\u96c6\u7684\u4e94\u4e2a\u4efb\u52a1\u573a\u666f\u4e2d\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPAMU \u663e\u8457\u63d0\u9ad8\u4e86 LLM \u5728\u4e94\u4e2a\u57fa\u7ebf\u4e2d\u7684\u8f93\u51fa\u8d28\u91cf\u3002", "conclusion": "\u9a8c\u8bc1\u4e86 PAMU \u5728\u957f\u671f\u5bf9\u8bdd\u4e2d\u6709\u6548\u6027"}}
{"id": "2510.09730", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09730", "abs": "https://arxiv.org/abs/2510.09730", "authors": ["Thi Bich Phuong Man", "Luu Tu Nguyen", "Vu Tram Anh Khuong", "Thanh Ha Le", "Thi Duyen Ngo"], "title": "Adaptive Fusion Network with Temporal-Ranked and Motion-Intensity Dynamic Images for Micro-expression Recognition", "comment": null, "summary": "Micro-expressions (MEs) are subtle, transient facial changes with very low\nintensity, almost imperceptible to the naked eye, yet they reveal a person\ngenuine emotion. They are of great value in lie detection, behavioral analysis,\nand psychological assessment. This paper proposes a novel MER method with two\nmain contributions. First, we propose two complementary representations -\nTemporal-ranked dynamic image, which emphasizes temporal progression, and\nMotion-intensity dynamic image, which highlights subtle motions through a frame\nreordering mechanism incorporating motion intensity. Second, we propose an\nAdaptive fusion network, which automatically learns to optimally integrate\nthese two representations, thereby enhancing discriminative ME features while\nsuppressing noise. Experiments on three benchmark datasets (CASME-II, SAMM and\nMMEW) demonstrate the superiority of the proposed method. Specifically, AFN\nachieves 93.95 Accuracy and 0.897 UF1 on CASME-II, setting a new\nstate-of-the-art benchmark. On SAMM, the method attains 82.47 Accuracy and\n0.665 UF1, demonstrating more balanced recognition across classes. On MMEW, the\nmodel achieves 76.00 Accuracy, further confirming its generalization ability.\nThe obtained results show that both the input and the proposed architecture\nplay important roles in improving the performance of MER. Moreover, they\nprovide a solid foundation for further research and practical applications in\nthe fields of affective computing, lie detection, and human-computer\ninteraction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5fae\u8868\u60c5\u8bc6\u522b(MER)\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5fae\u8868\u60c5\u5728\u8c0e\u8a00\u68c0\u6d4b\u3001\u884c\u4e3a\u5206\u6790\u548c\u5fc3\u7406\u8bc4\u4f30\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u96be\u4ee5\u5bdf\u89c9\u3002", "method": "\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86\u65f6\u95f4\u6392\u5e8f\u52a8\u6001\u56fe\u50cf\u548c\u8fd0\u52a8\u5f3a\u5ea6\u52a8\u6001\u56fe\u50cf\u4e24\u79cd\u4e92\u8865\u8868\u793a\uff0c\u4ee5\u53ca\u81ea\u9002\u5e94\u878d\u5408\u7f51\u7edc\u3002", "result": "\u5728CASME-II\u4e0a\uff0cAFN\u5b9e\u73b0\u4e8693.95%\u7684\u51c6\u786e\u7387\u548c0.897\u7684UF1\uff0c\u5728SAMM\u4e0a\u5b9e\u73b0\u4e8682.47%\u7684\u51c6\u786e\u7387\u548c0.665\u7684UF1\uff0c\u5728MMEW\u4e0a\u5b9e\u73b0\u4e8676.00%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u8f93\u5165\u548c\u6240\u63d0\u51fa\u7684\u67b6\u6784\u5728\u63d0\u9ad8MER\u6027\u80fd\u65b9\u9762\u90fd\u8d77\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u5e76\u4e3a\u60c5\u611f\u8ba1\u7b97\u3001\u8c0e\u8a00\u68c0\u6d4b\u548c\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2510.09970", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09970", "abs": "https://arxiv.org/abs/2510.09970", "authors": ["Olivia Peiyu Wang", "Tashvi Bansal", "Ryan Bai", "Emily M. Chui", "Leilani H. Gilpin"], "title": "Follow My Lead: Logical Fallacy Classification with Knowledge-Augmented LLMs", "comment": "Accepted as a poster at the Twelfth Annual Conference on Advances in\n  Cognitive Systems. 21 pages, 7 figures and 1 table", "summary": "Large Language Models (LLMs) suffer from critical reasoning gaps, including a\ntendency to hallucinate and poor accuracy in classifying logical fallacies.\nThis limitation stems from their default System 1 processing, which is fast and\nintuitive, whereas reliable reasoning requires the deliberate, effortful System\n2 approach (Kahneman, 2011; Li et al., 2025). Since full System 2 training is\noften prohibitively expensive, we explore a low-cost, instruction-based\nintervention to bridge this gap. Our methodology introduces a novel stepwise\ninstruction dataset that decomposes fallacy classification into a series of\natomic procedural steps (simple binary questions). We further augment this with\na final verification step where models consult a relational knowledge graph of\nrelated fallacies. This procedural, rule-based intervention yields a\nsignificant improvement in LLM logical fallacy classification. Crucially, the\napproach also provides enhanced transparency into the LLMs' decision-making,\nhighlighting a practical pathway for Neuro-symbolic architectures to address\nLLM reasoning deficits.", "AI": {"tldr": "LLMs are bad at reasoning and tend to hallucinate.", "motivation": "LLMs' System 1 processing is not sufficient for reliable reasoning, which requires System 2.", "method": "A low-cost, instruction-based intervention is introduced, which decomposes fallacy classification into a series of atomic procedural steps. A relational knowledge graph is consulted for verification.", "result": "The procedural, rule-based intervention yields a significant improvement in LLM logical fallacy classification.", "conclusion": "The approach provides enhanced transparency into the LLMs' decision-making, highlighting a practical pathway for Neuro-symbolic architectures to address LLM reasoning deficits."}}
{"id": "2510.09662", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2510.09662", "abs": "https://arxiv.org/abs/2510.09662", "authors": ["Ali Jaberi", "Amin Sadeghi", "Runze Zhang", "Zhaoyang Zhao", "Qiuyu Shi", "Robert Black", "Zoya Sadighi", "Jason Hattrick-Simpers"], "title": "Assessment of different loss functions for fitting equivalent circuit models to electrochemical impedance spectroscopy data", "comment": null, "summary": "Electrochemical impedance spectroscopy (EIS) data is typically modeled using\nan equivalent circuit model (ECM), with parameters obtained by minimizing a\nloss function via nonlinear least squares fitting. This paper introduces two\nnew loss functions, log-B and log-BW, derived from the Bode representation of\nEIS. Using a large dataset of generated EIS data, the performance of proposed\nloss functions was evaluated alongside existing ones in terms of R2 scores,\nchi-squared, computational efficiency, and the mean absolute percentage error\n(MAPE) between the predicted component values and the original values.\nStatistical comparisons revealed that the choice of loss function impacts\nconvergence, computational efficiency, quality of fit, and MAPE. Our analysis\nshowed that X2 loss function (squared sum of residuals with proportional\nweighting) achieved the highest performance across multiple quality of fit\nmetrics, making it the preferred choice when the quality of fit is the primary\ngoal. On the other hand, log-B offered a slightly lower quality of fit while\nbeing approximately 1.4 times faster and producing lower MAPE for most circuit\ncomponents, making log-B as a strong alternative. This is a critical factor for\nlarge-scale least squares fitting in data-driven applications, such as training\nmachine learning models on extensive datasets or iterations.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7535\u5316\u5b66\u963b\u6297\u8c31(EIS)\u6570\u636e\u5efa\u6a21\u4e2d\u635f\u5931\u51fd\u6570\u7684\u9009\u62e9\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570log-B\u548clog-BW\uff0c\u5e76\u4e0e\u73b0\u6709\u635f\u5931\u51fd\u6570\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "motivation": "\u5728\u7535\u5316\u5b66\u963b\u6297\u8c31(EIS)\u6570\u636e\u5efa\u6a21\u4e2d\uff0c\u9009\u62e9\u5408\u9002\u7684\u635f\u5931\u51fd\u6570\u5bf9\u4e8e\u53c2\u6570\u62df\u5408\u81f3\u5173\u91cd\u8981\uff0c\u4e0d\u540c\u7684\u635f\u5931\u51fd\u6570\u4f1a\u5f71\u54cd\u6536\u655b\u6027\u3001\u8ba1\u7b97\u6548\u7387\u548c\u62df\u5408\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u751f\u6210\u5927\u91cf\u7684EIS\u6570\u636e\uff0c\u8bc4\u4f30\u4e86\u63d0\u51fa\u7684\u635f\u5931\u51fd\u6570\u4ee5\u53ca\u73b0\u6709\u635f\u5931\u51fd\u6570\u5728R2\u5206\u6570\u3001\u5361\u65b9\u3001\u8ba1\u7b97\u6548\u7387\u4ee5\u53ca\u9884\u6d4b\u5206\u91cf\u503c\u4e0e\u539f\u59cb\u503c\u4e4b\u95f4\u7684\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee(MAPE)\u65b9\u9762\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u635f\u5931\u51fd\u6570\u7684\u9009\u62e9\u4f1a\u5f71\u54cd\u6536\u655b\u6027\u3001\u8ba1\u7b97\u6548\u7387\u3001\u62df\u5408\u8d28\u91cf\u548cMAPE\u3002X2\u635f\u5931\u51fd\u6570\u5728\u591a\u4e2a\u62df\u5408\u8d28\u91cf\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800clog-B\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u66f4\u6709\u4f18\u52bf\uff0c\u4e14\u5bf9\u4e8e\u5927\u591a\u6570\u7535\u8def\u7ec4\u4ef6\u4ea7\u751f\u8f83\u4f4e\u7684MAPE\u3002", "conclusion": "X2\u635f\u5931\u51fd\u6570\u5728\u8ffd\u6c42\u62df\u5408\u8d28\u91cf\u65f6\u662f\u9996\u9009\uff0c\u800clog-B\u662f\u53e6\u4e00\u79cd\u6709\u7ade\u4e89\u529b\u7684\u9009\u62e9\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u6570\u636e\u9a71\u52a8\u5e94\u7528\u4e2d\uff0c\u4f8b\u5982\u5728\u5927\u91cf\u6570\u636e\u96c6\u6216\u8fed\u4ee3\u4e0a\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u65f6\u3002"}}
{"id": "2510.10828", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10828", "abs": "https://arxiv.org/abs/2510.10828", "authors": ["Zhenghan Tai", "Hanwei Wu", "Qingchen Hu", "Jijun Chi", "Hailin He", "Lei Ding", "Tung Sum Thomas Kwok", "Bohuai Xiao", "Yuchen Hua", "Suyuchen Wang", "Peng Lu", "Muzhi Li", "Yihong Wu", "Liheng Ma", "Jerry Huang", "Jiayi Zhang", "Gonghao Zhang", "Chaolong Jiang", "Jingrui Tian", "Sicheng Lyu", "Zeyu Li", "Boyu Han", "Fengran Mo", "Xinyue Yu", "Yufei Cui", "Ling Zhou", "Xinyu Wang"], "title": "VeritasFi: An Adaptable, Multi-tiered RAG Framework for Multi-modal Financial Question Answering", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is becoming increasingly essential for\nQuestion Answering (QA) in the financial sector, where accurate and\ncontextually grounded insights from complex public disclosures are crucial.\nHowever, existing financial RAG systems face two significant challenges: (1)\nthey struggle to process heterogeneous data formats, such as text, tables, and\nfigures; and (2) they encounter difficulties in balancing general-domain\napplicability with company-specific adaptation. To overcome these challenges,\nwe present VeritasFi, an innovative hybrid RAG framework that incorporates a\nmulti-modal preprocessing pipeline alongside a cutting-edge two-stage training\nstrategy for its re-ranking component. VeritasFi enhances financial QA through\nthree key innovations: (1) A multi-modal preprocessing pipeline that seamlessly\ntransforms heterogeneous data into a coherent, machine-readable format. (2) A\ntripartite hybrid retrieval engine that operates in parallel, combining deep\nmulti-path retrieval over a semantically indexed document corpus, real-time\ndata acquisition through tool utilization, and an expert-curated memory bank\nfor high-frequency questions, ensuring comprehensive scope, accuracy, and\nefficiency. (3) A two-stage training strategy for the document re-ranker, which\ninitially constructs a general, domain-specific model using anonymized data,\nfollowed by rapid fine-tuning on company-specific data for targeted\napplications. By integrating our proposed designs, VeritasFi presents a\ngroundbreaking framework that greatly enhances the adaptability and robustness\nof financial RAG systems, providing a scalable solution for both general-domain\nand company-specific QA tasks. Code accompanying this work is available at\nhttps://github.com/simplew4y/VeritasFi.git.", "AI": {"tldr": "VeritasFi is a hybrid RAG framework for financial QA that tackles heterogeneous data and balances general and company-specific applicability.", "motivation": "Existing financial RAG systems struggle with heterogeneous data formats and balancing general-domain applicability with company-specific adaptation.", "method": "The paper presents VeritasFi, a hybrid RAG framework with a multi-modal preprocessing pipeline and a two-stage training strategy for its re-ranking component. It uses a tripartite hybrid retrieval engine combining deep multi-path retrieval, real-time data acquisition, and an expert-curated memory bank.", "result": "VeritasFi enhances the adaptability and robustness of financial RAG systems, providing a scalable solution for both general-domain and company-specific QA tasks.", "conclusion": "VeritasFi is a groundbreaking framework that greatly enhances financial RAG systems' adaptability and robustness."}}
{"id": "2510.11011", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11011", "abs": "https://arxiv.org/abs/2510.11011", "authors": ["Farzaneh Zirak", "Farhana Choudhury", "Renata Borovica-Gajic"], "title": "GrASP: A Generalizable Address-based Semantic Prefetcher for Scalable Transactional and Analytical Workloads", "comment": "This is a preprint version", "summary": "Data prefetching--loading data into the cache before it is requested--is\nessential for reducing I/O overhead and improving database performance. While\ntraditional prefetchers focus on sequential patterns, recent learning-based\napproaches, especially those leveraging data semantics, achieve higher accuracy\nfor complex access patterns. However, these methods often struggle with today's\ndynamic, ever-growing datasets and require frequent, timely fine-tuning.\nPrivacy constraints may also restrict access to complete datasets,\nnecessitating prefetchers that can learn effectively from samples. To address\nthese challenges, we present GrASP, a learning-based prefetcher designed for\nboth analytical and transactional workloads. GrASP enhances prefetching\naccuracy and scalability by leveraging logical block address deltas and\ncombining query representations with result encodings. It frames prefetching as\na context-aware multi-label classification task, using multi-layer LSTMs to\npredict delta patterns from embedded context. This delta modeling approach\nenables GrASP to generalize predictions from small samples to larger, dynamic\ndatasets without requiring extensive retraining. Experiments on real-world\ndatasets and industrial benchmarks demonstrate that GrASP generalizes to\ndatasets 250 times larger than the training data, achieving up to 45% higher\nhit ratios, 60% lower I/O time, and 55% lower end-to-end query execution\nlatency than existing baselines. On average, GrASP attains a 91.4% hit ratio, a\n90.8% I/O time reduction, and a 57.1% execution latency reduction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGrASP\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u9884\u53d6\u5668\uff0c\u65e8\u5728\u63d0\u9ad8\u9884\u53d6\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u5206\u6790\u548c\u4e8b\u52a1\u5de5\u4f5c\u8d1f\u8f7d\u3002", "motivation": "\u4f20\u7edf\u9884\u53d6\u5668\u4fa7\u91cd\u4e8e\u987a\u5e8f\u6a21\u5f0f\uff0c\u800c\u6700\u8fd1\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u590d\u6742\u8bbf\u95ee\u6a21\u5f0f\u4e0b\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u96be\u4ee5\u5904\u7406\u5f53\u4eca\u52a8\u6001\u7684\u3001\u4e0d\u65ad\u589e\u957f\u7684\u6570\u636e\u96c6\uff0c\u9700\u8981\u9891\u7e41\u3001\u53ca\u65f6\u5730\u8fdb\u884c\u5fae\u8c03\u3002\u9690\u79c1\u7ea6\u675f\u4e5f\u53ef\u80fd\u9650\u5236\u5bf9\u5b8c\u6574\u6570\u636e\u96c6\u7684\u8bbf\u95ee\uff0c\u56e0\u6b64\u9700\u8981\u80fd\u591f\u4ece\u6837\u672c\u4e2d\u6709\u6548\u5b66\u4e60\u7684\u9884\u53d6\u5668\u3002", "method": "GrASP\u5229\u7528\u903b\u8f91\u5757\u5730\u5740\u589e\u91cf\uff0c\u5e76\u5c06\u67e5\u8be2\u8868\u793a\u4e0e\u7ed3\u679c\u7f16\u7801\u76f8\u7ed3\u5408\u3002\u5b83\u5c06\u9884\u53d6\u89c6\u4e3a\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\uff0c\u4f7f\u7528\u591a\u5c42LSTM\u4ece\u5d4c\u5165\u4e0a\u4e0b\u6587\u4e2d\u9884\u6d4b\u589e\u91cf\u6a21\u5f0f\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c\u5de5\u4e1a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGrASP\u53ef\u4ee5\u63a8\u5e7f\u5230\u6bd4\u8bad\u7ec3\u6570\u636e\u5927250\u500d\u7684\u6570\u636e\u96c6\uff0c\u4e0e\u73b0\u6709\u57fa\u7ebf\u76f8\u6bd4\uff0c\u547d\u4e2d\u7387\u63d0\u9ad8\u4e8645%\uff0cI/O\u65f6\u95f4\u964d\u4f4e\u4e8660%\uff0c\u7aef\u5230\u7aef\u67e5\u8be2\u6267\u884c\u5ef6\u8fdf\u964d\u4f4e\u4e8655%\u3002\u5e73\u5747\u800c\u8a00\uff0cGrASP\u5b9e\u73b0\u4e8691.4%\u7684\u547d\u4e2d\u7387\uff0c90.8%\u7684I/O\u65f6\u95f4\u51cf\u5c11\u548c57.1%\u7684\u6267\u884c\u5ef6\u8fdf\u51cf\u5c11\u3002", "conclusion": "GrASP\u662f\u4e00\u79cd\u6709\u6548\u7684\u9884\u53d6\u5668\uff0c\u53ef\u4ee5\u5728\u5404\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u548c\u6570\u636e\u96c6\u5927\u5c0f\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002"}}
{"id": "2510.09722", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09722", "abs": "https://arxiv.org/abs/2510.09722", "authors": ["Fanwei Zhu", "Jinke Yu", "Zulong Chen", "Ying Zhou", "Junhao Ji", "Zhibo Yang", "Yuxue Zhang", "Haoyuan Hu", "Zhenghao Liu"], "title": "Layout-Aware Parsing Meets Efficient LLMs: A Unified, Scalable Framework for Resume Information Extraction and Evaluation", "comment": null, "summary": "Automated resume information extraction is critical for scaling talent\nacquisition, yet its real-world deployment faces three major challenges: the\nextreme heterogeneity of resume layouts and content, the high cost and latency\nof large language models (LLMs), and the lack of standardized datasets and\nevaluation tools. In this work, we present a layout-aware and\nefficiency-optimized framework for automated extraction and evaluation that\naddresses all three challenges. Our system combines a fine-tuned layout parser\nto normalize diverse document formats, an inference-efficient LLM extractor\nbased on parallel prompting and instruction tuning, and a robust two-stage\nautomated evaluation framework supported by new benchmark datasets. Extensive\nexperiments show that our framework significantly outperforms strong baselines\nin both accuracy and efficiency. In particular, we demonstrate that a\nfine-tuned compact 0.6B LLM achieves top-tier accuracy while significantly\nreducing inference latency and computational cost. The system is fully deployed\nin Alibaba's intelligent HR platform, supporting real-time applications across\nits business units.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u7b80\u5386\u4fe1\u606f\u63d0\u53d6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5177\u6709\u5e03\u5c40\u611f\u77e5\u548c\u6548\u7387\u4f18\u5316\u529f\u80fd\uff0c\u53ef\u4ee5\u5e94\u5bf9\u7b80\u5386\u7248\u5f0f\u548c\u5185\u5bb9\u7684\u591a\u6837\u6027\u3001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9ad8\u6210\u672c\u548c\u5ef6\u8fdf\u4ee5\u53ca\u7f3a\u4e4f\u6807\u51c6\u5316\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u5de5\u5177\u7b49\u6311\u6218\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u81ea\u52a8\u5316\u7684\u7b80\u5386\u4fe1\u606f\u63d0\u53d6\u9762\u4e34\u4e09\u4e2a\u4e3b\u8981\u7684\u6311\u6218\uff1a\u7b80\u5386\u5e03\u5c40\u548c\u5185\u5bb9\u7684\u6781\u7aef\u5f02\u6784\u6027\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9ad8\u6210\u672c\u548c\u5ef6\u8fdf\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86\u4e00\u4e2a\u5fae\u8c03\u7684\u5e03\u5c40\u89e3\u6790\u5668\u6765\u6807\u51c6\u5316\u4e0d\u540c\u7684\u6587\u6863\u683c\u5f0f\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5e76\u884c\u63d0\u793a\u548c\u6307\u4ee4\u8c03\u6574\u7684\u63a8\u7406\u9ad8\u6548\u7684LLM\u63d0\u53d6\u5668\uff0c\u4ee5\u53ca\u4e00\u4e2a\u7531\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6\u652f\u6301\u7684\u7a33\u5065\u7684\u4e24\u9636\u6bb5\u81ea\u52a8\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u90fd\u660e\u663e\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u3002\u7279\u522b\u5730\uff0c\u8bc1\u660e\u4e86\u4e00\u4e2a\u5fae\u8c03\u7684\u7d27\u51d1\u578b0.6B LLM\u5b9e\u73b0\u4e86\u9876\u7ea7\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\u548c\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5df2\u5b8c\u5168\u90e8\u7f72\u5728\u963f\u91cc\u5df4\u5df4\u7684\u667a\u80fd\u4eba\u529b\u8d44\u6e90\u5e73\u53f0\u4e2d\uff0c\u652f\u6301\u5176\u4e1a\u52a1\u90e8\u95e8\u7684\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2510.09731", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09731", "abs": "https://arxiv.org/abs/2510.09731", "authors": ["Muhammad Munsif", "Waqas Ahmad", "Amjid Ali", "Mohib Ullah", "Adnan Hussain", "Sung Wook Baik"], "title": "Multi Camera Connected Vision System with Multi View Analytics: A Comprehensive Survey", "comment": null, "summary": "Connected Vision Systems (CVS) are transforming a variety of applications,\nincluding autonomous vehicles, smart cities, surveillance, and human-robot\ninteraction. These systems harness multi-view multi-camera (MVMC) data to\nprovide enhanced situational awareness through the integration of MVMC\ntracking, re-identification (Re-ID), and action understanding (AU). However,\ndeploying CVS in real-world, dynamic environments presents a number of\nchallenges, particularly in addressing occlusions, diverse viewpoints, and\nenvironmental variability. Existing surveys have focused primarily on isolated\ntasks such as tracking, Re-ID, and AU, often neglecting their integration into\na cohesive system. These reviews typically emphasize single-view setups,\noverlooking the complexities and opportunities provided by multi-camera\ncollaboration and multi-view data analysis. To the best of our knowledge, this\nsurvey is the first to offer a comprehensive and integrated review of MVMC that\nunifies MVMC tracking, Re-ID, and AU into a single framework. We propose a\nunique taxonomy to better understand the critical components of CVS, dividing\nit into four key parts: MVMC tracking, Re-ID, AU, and combined methods. We\nsystematically arrange and summarize the state-of-the-art datasets,\nmethodologies, results, and evaluation metrics, providing a structured view of\nthe field's progression. Furthermore, we identify and discuss the open research\nquestions and challenges, along with emerging technologies such as lifelong\nlearning, privacy, and federated learning, that need to be addressed for future\nadvancements. The paper concludes by outlining key research directions for\nenhancing the robustness, efficiency, and adaptability of CVS in complex,\nreal-world applications. We hope this survey will inspire innovative solutions\nand guide future research toward the next generation of intelligent and\nadaptive CVS.", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf9\u591a\u89c6\u89d2\u591a\u6444\u50cf\u5934\uff08MVMC\uff09\u7cfb\u7edf\u8fdb\u884c\u4e86\u5168\u9762\u7684\u7efc\u8ff0\uff0c\u7edf\u4e00\u4e86MVMC\u8ddf\u8e2a\u3001Re-ID\u548c\u52a8\u4f5c\u7406\u89e3\u3002", "motivation": "\u73b0\u6709\u7efc\u8ff0\u4e3b\u8981\u5173\u6ce8\u5b64\u7acb\u7684\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u96c6\u6210\u5230\u4e00\u4e2a\u6709\u51dd\u805a\u529b\u7684\u7cfb\u7edf\u4e2d\u3002\u540c\u65f6\uff0c\u4e5f\u5ffd\u7565\u4e86\u591a\u6444\u50cf\u5934\u534f\u4f5c\u548c\u591a\u89c6\u89d2\u6570\u636e\u5206\u6790\u6240\u5e26\u6765\u7684\u590d\u6742\u6027\u548c\u673a\u9047\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u72ec\u7279\u7684\u5206\u7c7b\u6cd5\uff0c\u5c06CVS\u5206\u4e3a\u56db\u4e2a\u5173\u952e\u90e8\u5206\uff1aMVMC\u8ddf\u8e2a\u3001Re-ID\u3001AU\u548c\u7ec4\u5408\u65b9\u6cd5\u3002\u7cfb\u7edf\u5730\u6574\u7406\u548c\u603b\u7ed3\u4e86\u6700\u65b0\u7684\u6570\u636e\u96c6\u3001\u65b9\u6cd5\u3001\u7ed3\u679c\u548c\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u603b\u7ed3\u4e86\u5f53\u524d\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u5e76\u8bc6\u522b\u548c\u8ba8\u8bba\u4e86\u5f00\u653e\u7684\u7814\u7a76\u95ee\u9898\u548c\u6311\u6218\uff0c\u4ee5\u53ca\u65b0\u5174\u6280\u672f\uff0c\u5982\u7ec8\u8eab\u5b66\u4e60\u3001\u9690\u79c1\u548c\u8054\u90a6\u5b66\u4e60\u3002", "conclusion": "\u6982\u8ff0\u4e86\u5173\u952e\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u63d0\u9ad8CVS\u5728\u590d\u6742\u3001\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u3001\u6548\u7387\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2510.10002", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10002", "abs": "https://arxiv.org/abs/2510.10002", "authors": ["Pratik S. Sachdeva", "Tom van Nuenen"], "title": "Deliberative Dynamics and Value Alignment in LLM Debates", "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in sensitive\neveryday contexts - offering personal advice, mental health support, and moral\nguidance - understanding their elicited values in navigating complex moral\nreasoning is essential. Most evaluations study this sociotechnical alignment\nthrough single-turn prompts, but it is unclear if these findings extend to\nmulti-turn settings where values emerge through dialogue, revision, and\nconsensus. We address this gap using LLM debate to examine deliberative\ndynamics and value alignment in multi-turn settings by prompting subsets of\nthree models (GPT-4.1, Claude 3.7 Sonnet, and Gemini 2.0 Flash) to collectively\nassign blame in 1,000 everyday dilemmas from Reddit's \"Am I the Asshole\"\ncommunity. We use both synchronous (parallel responses) and round-robin\n(sequential responses) formats to test order effects and verdict revision. Our\nfindings show striking behavioral differences. In the synchronous setting, GPT\nshowed strong inertia (0.6-3.1% revision rates) while Claude and Gemini were\nfar more flexible (28-41%). Value patterns also diverged: GPT emphasized\npersonal autonomy and direct communication, while Claude and Gemini prioritized\nempathetic dialogue. Certain values proved especially effective at driving\nverdict changes. We further find that deliberation format had a strong impact\non model behavior: GPT and Gemini stood out as highly conforming relative to\nClaude, with their verdict behavior strongly shaped by order effects. These\nresults show how deliberation format and model-specific behaviors shape moral\nreasoning in multi-turn interactions, underscoring that sociotechnical\nalignment depends on how systems structure dialogue as much as on their\noutputs.", "AI": {"tldr": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u8fdb\u884c\u9053\u5fb7\u63a8\u7406\u548c\u4ef7\u503c\u5bf9\u9f50\u7684\u8868\u73b0\u3002", "motivation": "\u5355\u8f6e\u63d0\u793a\u8bc4\u4f30\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u6355\u6349LLM\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4ef7\u503c\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u5bf9\u8bdd\u3001\u4fee\u6539\u548c\u5171\u8bc6\u7684\u591a\u8f6e\u73af\u5883\u4e2d\u3002", "method": "\u4f7f\u7528LLM\u8fa9\u8bba\uff0c\u8981\u6c42GPT-4.1\u3001Claude 3.7 Sonnet\u548cGemini 2.0 Flash\u8fd9\u4e09\u79cd\u6a21\u578b\u5bf9\u6765\u81eaReddit\u201cAm I the Asshole\u201d\u793e\u533a\u76841000\u4e2a\u65e5\u5e38\u56f0\u5883\u8fdb\u884c\u96c6\u4f53\u8d23\u4efb\u5206\u914d\u3002\u91c7\u7528\u540c\u6b65\uff08\u5e76\u884c\u54cd\u5e94\uff09\u548c\u5faa\u73af\uff08\u987a\u5e8f\u54cd\u5e94\uff09\u4e24\u79cd\u5f62\u5f0f\uff0c\u4ee5\u6d4b\u8bd5\u987a\u5e8f\u6548\u5e94\u548c\u5224\u51b3\u4fee\u6539\u3002", "result": "\u5728\u540c\u6b65\u8bbe\u7f6e\u4e2d\uff0cGPT\u8868\u73b0\u51fa\u5f88\u5f3a\u7684\u60ef\u6027\uff0c\u800cClaude\u548cGemini\u5219\u7075\u6d3b\u5f97\u591a\u3002\u4ef7\u503c\u6a21\u5f0f\u4e5f\u4e0d\u540c\uff1aGPT\u5f3a\u8c03\u4e2a\u4eba\u81ea\u4e3b\u548c\u76f4\u63a5\u6c9f\u901a\uff0c\u800cClaude\u548cGemini\u4f18\u5148\u8003\u8651\u5171\u60c5\u5bf9\u8bdd\u3002\u67d0\u4e9b\u4ef7\u503c\u89c2\u5728\u63a8\u52a8\u5224\u51b3\u6539\u53d8\u65b9\u9762\u7279\u522b\u6709\u6548\u3002GPT\u548cGemini\u76f8\u5bf9\u4e8eClaude\u6765\u8bf4\uff0c\u8868\u73b0\u51fa\u9ad8\u5ea6\u7684\u4e00\u81f4\u6027\uff0c\u5b83\u4eec\u7684\u5224\u51b3\u884c\u4e3a\u53d7\u5230\u987a\u5e8f\u6548\u5e94\u7684\u5f3a\u70c8\u5f71\u54cd\u3002", "conclusion": "\u591a\u8f6e\u4e92\u52a8\u4e2d\u7684\u9053\u5fb7\u63a8\u7406\u53d7\u5230\u5bf9\u8bdd\u7ed3\u6784\u548c\u6a21\u578b\u7279\u5b9a\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u793e\u4f1a\u6280\u672f\u5bf9\u9f50\u53d6\u51b3\u4e8e\u7cfb\u7edf\u6784\u5efa\u5bf9\u8bdd\u7684\u65b9\u5f0f\uff0c\u4ee5\u53ca\u5b83\u4eec\u7684\u8f93\u51fa\u3002"}}
{"id": "2510.09664", "categories": ["cs.LG", "cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.09664", "abs": "https://arxiv.org/abs/2510.09664", "authors": ["Changchang Sun", "Vickie Chen", "Yan Yan"], "title": "Semantic-Cohesive Knowledge Distillation for Deep Cross-modal Hashing", "comment": null, "summary": "Recently, deep supervised cross-modal hashing methods have achieve compelling\nsuccess by learning semantic information in a self-supervised way. However,\nthey still suffer from the key limitation that the multi-label semantic\nextraction process fail to explicitly interact with raw multimodal data, making\nthe learned representation-level semantic information not compatible with the\nheterogeneous multimodal data and hindering the performance of bridging\nmodality gap. To address this limitation, in this paper, we propose a novel\nsemantic cohesive knowledge distillation scheme for deep cross-modal hashing,\ndubbed as SODA. Specifically, the multi-label information is introduced as a\nnew textual modality and reformulated as a set of ground-truth label prompt,\ndepicting the semantics presented in the image like the text modality. Then, a\ncross-modal teacher network is devised to effectively distill cross-modal\nsemantic characteristics between image and label modalities and thus learn a\nwell-mapped Hamming space for image modality. In a sense, such Hamming space\ncan be regarded as a kind of prior knowledge to guide the learning of\ncross-modal student network and comprehensively preserve the semantic\nsimilarities between image and text modality. Extensive experiments on two\nbenchmark datasets demonstrate the superiority of our model over the\nstate-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bed\u4e49\u51dd\u805a\u77e5\u8bc6\u84b8\u998f\u65b9\u6848\uff08SODA\uff09\u7528\u4e8e\u6df1\u5ea6\u8de8\u6a21\u6001\u54c8\u5e0c\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u76d1\u7763\u8de8\u6a21\u6001\u54c8\u5e0c\u65b9\u6cd5\u672a\u80fd\u663e\u5f0f\u5730\u4e0e\u539f\u59cb\u591a\u6a21\u6001\u6570\u636e\u4ea4\u4e92\uff0c\u5bfc\u81f4\u5b66\u4e60\u5230\u7684\u8bed\u4e49\u4fe1\u606f\u4e0e\u5f02\u6784\u591a\u6a21\u6001\u6570\u636e\u4e0d\u517c\u5bb9\uff0c\u963b\u788d\u4e86\u8de8\u6a21\u6001\u6865\u63a5\u7684\u6027\u80fd\u3002", "method": "\u5c06\u591a\u6807\u7b7e\u4fe1\u606f\u4f5c\u4e3a\u65b0\u7684\u6587\u672c\u6a21\u6001\u5f15\u5165\uff0c\u5e76\u5c06\u5176\u91cd\u65b0\u8868\u8ff0\u4e3a\u4e00\u7ec4ground-truth\u6807\u7b7e\u63d0\u793a\uff0c\u7136\u540e\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8de8\u6a21\u6001\u6559\u5e08\u7f51\u7edc\uff0c\u4ee5\u6709\u6548\u5730\u63d0\u53d6\u56fe\u50cf\u548c\u6807\u7b7e\u6a21\u6001\u4e4b\u95f4\u7684\u8de8\u6a21\u6001\u8bed\u4e49\u7279\u5f81\uff0c\u4ece\u800c\u4e3a\u56fe\u50cf\u6a21\u6001\u5b66\u4e60\u4e00\u4e2a\u826f\u597d\u6620\u5c04\u7684Hamming\u7a7a\u95f4\u3002\u8be5Hamming\u7a7a\u95f4\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ee5\u6307\u5bfc\u8de8\u6a21\u6001\u5b66\u751f\u7f51\u7edc\u7684\u5b66\u4e60\uff0c\u5e76\u5168\u9762\u4fdd\u7559\u56fe\u50cf\u548c\u6587\u672c\u6a21\u6001\u4e4b\u95f4\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "SODA\u6a21\u578b\u5728\u8de8\u6a21\u6001\u54c8\u5e0c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272"}}
{"id": "2510.10920", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10920", "abs": "https://arxiv.org/abs/2510.10920", "authors": ["Yi Yu", "Zhenxing Hu"], "title": "Comparative Explanations via Counterfactual Reasoning in Recommendations", "comment": null, "summary": "Explainable recommendation through counterfactual reasoning seeks to identify\nthe influential aspects of items in recommendations, which can then be used as\nexplanations. However, state-of-the-art approaches, which aim to minimize\nchanges in product aspects while reversing their recommended decisions\naccording to an aggregated decision boundary score, often lead to factual\ninaccuracies in explanations. To solve this problem, in this work we propose a\nnovel method of Comparative Counterfactual Explanations for Recommendation\n(CoCountER). CoCountER creates counterfactual data based on soft swap\noperations, enabling explanations for recommendations of arbitrary pairs of\ncomparative items. Empirical experiments validate the effectiveness of our\napproach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u89e3\u91ca\u63a8\u8350\u65b9\u6cd5CoCountER\uff0c\u901a\u8fc7\u6bd4\u8f83\u53cd\u4e8b\u5b9e\u63a8\u7406\u6765\u8bc6\u522b\u63a8\u8350\u4e2d\u7269\u54c1\u7684\u5f71\u54cd\u56e0\u7d20\uff0c\u5e76\u751f\u6210\u66f4\u51c6\u786e\u7684\u89e3\u91ca\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u89e3\u91ca\u63a8\u8350\u65f6\uff0c\u4e3a\u4e86\u6700\u5c0f\u5316\u4ea7\u54c1\u65b9\u9762\u7684\u53d8\u5316\u5e76\u53cd\u8f6c\u63a8\u8350\u51b3\u7b56\uff0c\u5e38\u5e38\u5bfc\u81f4\u89e3\u91ca\u4e0d\u51c6\u786e\u3002", "method": "CoCountER\u57fa\u4e8e\u8f6f\u4ea4\u6362\u64cd\u4f5c\u521b\u5efa\u53cd\u4e8b\u5b9e\u6570\u636e\uff0c\u4ece\u800c\u80fd\u591f\u89e3\u91ca\u4efb\u610f\u6bd4\u8f83\u9879\u76ee\u7684\u63a8\u8350\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "CoCountER\u80fd\u591f\u751f\u6210\u66f4\u51c6\u786e\u7684\u63a8\u8350\u89e3\u91ca\u3002"}}
{"id": "2510.11166", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.11166", "abs": "https://arxiv.org/abs/2510.11166", "authors": ["Brad Bebee", "\u00dcmit V. \u00c7ataly\u00fcrek", "Olaf Hartig", "Ankesh Khandelwal", "Simone Rondelli", "Michael Schmidt", "Lefteris Sidirourgos", "Bryan Thompson"], "title": "Poseidon: A OneGraph Engine", "comment": null, "summary": "We present the Poseidon engine behind the Neptune Analytics graph database\nservice. Customers interact with Poseidon using the declarative openCypher\nquery language, which enables requests that seamlessly combine traditional\nquerying paradigms (such as graph pattern matching, variable length paths,\naggregation) with algorithm invocations and has been syntactically extended to\nfacilitate OneGraph interoperability, such as the disambiguation between\nglobally unique IRIs (as exposed via RDF) vs. local identifiers (as encountered\nin LPG data). Poseidon supports a broad range of graph workloads, from simple\ntransactions, to top-k beam search algorithms on dynamic graphs, to whole graph\nanalytics requiring multiple full passes over the data. For example, real-time\nfraud detection, like many other use cases, needs to reflect current committed\nstate of the dynamic graph. If a users cell phone is compromised, then all\nnewer actions by that user become immediately suspect. To address such dynamic\ngraph use cases, Poseidon combines state-of-the-art transaction processing with\nnovel graph data indexing, including lock-free maintenance of adjacency lists,\nsecondary succinct indices, partitioned heaps for data tuple storage with\nuniform placement, and innovative statistics for cost-based query optimization.\nThe Poseidon engine uses a logical log for durability, enabling rapid evolution\nof in-memory data structures. Bulk data loads achieve more than 10 million\nproperty values per second on many data sets while simple transactions can\nexecute in under 20ms against the storage engine.", "AI": {"tldr": "Poseidon\u662fNeptune Analytics\u56fe\u6570\u636e\u5e93\u670d\u52a1\u80cc\u540e\u7684\u5f15\u64ce\uff0c\u5b83\u4f7f\u7528openCypher\u67e5\u8be2\u8bed\u8a00\uff0c\u652f\u6301\u5e7f\u6cdb\u7684\u56fe\u5de5\u4f5c\u8d1f\u8f7d\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u56fe\u5e94\u7528\u9700\u8981\u53cd\u6620\u52a8\u6001\u56fe\u7684\u5f53\u524d\u72b6\u6001\uff0c\u4f8b\u5982\u5b9e\u65f6\u6b3a\u8bc8\u68c0\u6d4b\u3002", "method": "Poseidon\u7ed3\u5408\u4e86\u6700\u5148\u8fdb\u7684\u4e8b\u52a1\u5904\u7406\u548c\u65b0\u578b\u56fe\u6570\u636e\u7d22\u5f15\uff0c\u5305\u62ec\u65e0\u9501\u90bb\u63a5\u8868\u7ef4\u62a4\u3001\u4e8c\u7ea7\u7b80\u6d01\u7d22\u5f15\u3001\u5177\u6709\u7edf\u4e00\u653e\u7f6e\u7684\u5206\u533a\u5806\u548c\u7528\u4e8e\u57fa\u4e8e\u6210\u672c\u7684\u67e5\u8be2\u4f18\u5316\u7684\u521b\u65b0\u7edf\u8ba1\u3002", "result": "\u6279\u91cf\u6570\u636e\u52a0\u8f7d\u5728\u8bb8\u591a\u6570\u636e\u96c6\u4e0a\u6bcf\u79d2\u5b9e\u73b0\u8d85\u8fc71000\u4e07\u4e2a\u5c5e\u6027\u503c\uff0c\u800c\u7b80\u5355\u4e8b\u52a1\u53ef\u4ee5\u572820\u6beb\u79d2\u5185\u9488\u5bf9\u5b58\u50a8\u5f15\u64ce\u6267\u884c\u3002", "conclusion": "Poseidon\u5f15\u64ce\u4f7f\u7528\u903b\u8f91\u65e5\u5fd7\u6765\u5b9e\u73b0\u6301\u4e45\u6027\uff0c\u4ece\u800c\u80fd\u591f\u5feb\u901f\u6f14\u8fdb\u5185\u5b58\u6570\u636e\u7ed3\u6784\u3002"}}
{"id": "2510.09733", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09733", "abs": "https://arxiv.org/abs/2510.09733", "authors": ["Yubo Sun", "Chunyi Peng", "Yukun Yan", "Shi Yu", "Zhenghao Liu", "Chi Chen", "Zhiyuan Liu", "Maosong Sun"], "title": "VisRAG 2.0: Evidence-Guided Multi-Image Reasoning in Visual Retrieval-Augmented Generation", "comment": null, "summary": "Visual retrieval-augmented generation (VRAG) augments vision-language models\n(VLMs) with external visual knowledge to ground reasoning and reduce\nhallucinations. Yet current VRAG systems often fail to reliably perceive and\nintegrate evidence across multiple images, leading to weak grounding and\nerroneous conclusions. In this paper, we propose EVisRAG, an end-to-end\nframework that learns to reason with evidence-guided multi-image to address\nthis issue. The model first observes retrieved images and records per-image\nevidence, then derives the final answer from the aggregated evidence. To train\nEVisRAG effectively, we introduce Reward-Scoped Group Relative Policy\nOptimization (RS-GRPO), which binds fine-grained rewards to scope-specific\ntokens to jointly optimize visual perception and reasoning abilities of VLMs.\nExperimental results on multiple visual question answering benchmarks\ndemonstrate that EVisRAG delivers substantial end-to-end gains over backbone\nVLM with 27\\% improvements on average. Further analysis shows that, powered by\nRS-GRPO, EVisRAG improves answer accuracy by precisely perceiving and\nlocalizing question-relevant evidence across multiple images and deriving the\nfinal answer from that evidence, much like a real detective.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEVisRAG\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5b66\u4e60\u63a8\u7406\u8bc1\u636e\u5f15\u5bfc\u7684\u591a\u56fe\u50cf\u6765\u89e3\u51b3\u89c6\u89c9\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08VRAG\uff09\u7cfb\u7edf\u4e2d\u5b58\u5728\u7684\u95ee\u9898\uff0c\u8be5\u7cfb\u7edf\u65e8\u5728\u901a\u8fc7\u5916\u90e8\u89c6\u89c9\u77e5\u8bc6\u6765\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u4ece\u800c\u5b9e\u73b0\u63a8\u7406\u5e76\u51cf\u5c11\u5e7b\u89c9\u3002", "motivation": "\u5f53\u524d\u7684VRAG\u7cfb\u7edf\u901a\u5e38\u65e0\u6cd5\u53ef\u9760\u5730\u611f\u77e5\u548c\u6574\u5408\u591a\u4e2a\u56fe\u50cf\u4e2d\u7684\u8bc1\u636e\uff0c\u4ece\u800c\u5bfc\u81f4\u8f83\u5f31\u7684\u57fa\u7840\u548c\u9519\u8bef\u7684\u7ed3\u8bba\u3002", "method": "\u8be5\u6a21\u578b\u9996\u5148\u89c2\u5bdf\u68c0\u7d22\u5230\u7684\u56fe\u50cf\u5e76\u8bb0\u5f55\u6bcf\u4e2a\u56fe\u50cf\u7684\u8bc1\u636e\uff0c\u7136\u540e\u4ece\u6c47\u603b\u7684\u8bc1\u636e\u4e2d\u5f97\u51fa\u6700\u7ec8\u7b54\u6848\u3002\u4e3a\u4e86\u6709\u6548\u5730\u8bad\u7ec3EVisRAG\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5956\u52b1\u8303\u56f4\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08RS-GRPO\uff09\uff0c\u5b83\u5c06\u7ec6\u7c92\u5ea6\u7684\u5956\u52b1\u7ed1\u5b9a\u5230\u8303\u56f4\u7279\u5b9a\u7684\u4ee4\u724c\uff0c\u4ee5\u5171\u540c\u4f18\u5316VLM\u7684\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cEVisRAG\u4e0e\u4e3b\u5e72VLM\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u663e\u7740\u7684\u7aef\u5230\u7aef\u6536\u76ca\uff0c\u5e73\u5747\u63d0\u9ad8\u4e8627%\u3002", "conclusion": "\u5728RS-GRPO\u7684\u652f\u6301\u4e0b\uff0cEVisRAG\u901a\u8fc7\u7cbe\u786e\u5730\u611f\u77e5\u548c\u5b9a\u4f4d\u591a\u4e2a\u56fe\u50cf\u4e2d\u4e0e\u95ee\u9898\u76f8\u5173\u7684\u8bc1\u636e\uff0c\u5e76\u4ece\u8be5\u8bc1\u636e\u4e2d\u5f97\u51fa\u6700\u7ec8\u7b54\u6848\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u7b54\u6848\u7684\u51c6\u786e\u6027\uff0c\u5c31\u50cf\u4e00\u4e2a\u771f\u6b63\u7684\u4fa6\u63a2\u3002"}}
{"id": "2510.09741", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09741", "abs": "https://arxiv.org/abs/2510.09741", "authors": ["Dwip Dalal", "Gautam Vashishtha", "Utkarsh Mishra", "Jeonghwan Kim", "Madhav Kanda", "Hyeonjeong Ha", "Svetlana Lazebnik", "Heng Ji", "Unnat Jain"], "title": "Constructive Distortion: Improving MLLMs with Attention-Guided Image Warping", "comment": null, "summary": "Multimodal large language models (MLLMs) often miss small details and spatial\nrelations in cluttered scenes, leading to errors in fine-grained perceptual\ngrounding. We introduce AttWarp, a lightweight method that allocates more\nresolution to query-relevant content while compressing less informative areas,\nall while preserving global context. At test time, the approach uses an MLLM's\ncross-modal attention to perform rectilinear warping of the input image,\nreallocating spatial resolution toward regions the model deems important,\nwithout changing model weights or architecture. This attention-guided warping\npreserves all original image information but redistributes it non-uniformly, so\nsmall objects and subtle relationships become easier for the same model to read\nwhile the global layout remains intact. Across five benchmarks (TextVQA, GQA,\nDocVQA, POPE, MMMU) and four MLLMs (LLaVA, Qwen-VL, InternVL, and\nInstructBLIP), AttWarp consistently improves accuracy, strengthens\ncompositional reasoning, and reduces hallucinations, outperforming four\ncompetitive baselines that manipulate raw images at test time. Together, these\nresults show that attention-guided warping prioritizes information relevant to\nthe query while preserving context, and that the same MLLMs perform better when\ngiven such warped inputs.", "AI": {"tldr": "AttWarp\u901a\u8fc7\u5728\u6d4b\u8bd5\u65f6\u5bf9\u8f93\u5165\u56fe\u50cf\u8fdb\u884cattention-guided warping\u6765\u91cd\u65b0\u5206\u914d\u7a7a\u95f4\u5206\u8fa8\u7387\uff0c\u4ece\u800c\u63d0\u9ad8MLLM\u7684\u6027\u80fd\u3002", "motivation": "MLLM\u901a\u5e38\u4f1a\u5ffd\u7565\u6742\u4e71\u573a\u666f\u4e2d\u7684\u5c0f\u7ec6\u8282\u548c\u7a7a\u95f4\u5173\u7cfb\uff0c\u5bfc\u81f4\u7ec6\u7c92\u5ea6\u611f\u77e5\u9519\u8bef\u3002", "method": "\u4f7f\u7528MLLM\u7684\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5bf9\u8f93\u5165\u56fe\u50cf\u6267\u884crectilinear warping\uff0c\u5c06\u7a7a\u95f4\u5206\u8fa8\u7387\u91cd\u65b0\u5206\u914d\u5230\u6a21\u578b\u8ba4\u4e3a\u91cd\u8981\u7684\u533a\u57df\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u56db\u4e2aMLLM\u4e0a\uff0cAttWarp\u6301\u7eed\u63d0\u9ad8\u51c6\u786e\u7387\uff0c\u52a0\u5f3a\u7ec4\u5408\u63a8\u7406\uff0c\u5e76\u51cf\u5c11\u5e7b\u89c9\uff0c\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "attention-guided warping\u53ef\u4ee5\u4f18\u5148\u5904\u7406\u4e0e\u67e5\u8be2\u76f8\u5173\u7684\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u7559\u4e0a\u4e0b\u6587\uff0c\u5e76\u4e14\u76f8\u540c\u7684MLLM\u5728\u83b7\u5f97warped inputs\u65f6\u8868\u73b0\u66f4\u597d\u3002"}}
{"id": "2510.10008", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10008", "abs": "https://arxiv.org/abs/2510.10008", "authors": ["Meng Xi", "Sihan Lv", "Yechen Jin", "Guanjie Cheng", "Naibo Wang", "Ying Li", "Jianwei Yin"], "title": "RIPRAG: Hack a Black-box Retrieval-Augmented Generation Question-Answering System with Reinforcement Learning", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems based on Large Language Models\n(LLMs) have become a core technology for tasks such as question-answering (QA)\nand content generation. However, by injecting poisoned documents into the\ndatabase of RAG systems, attackers can manipulate LLMs to generate text that\naligns with their intended preferences. Existing research has primarily focused\non white-box attacks against simplified RAG architectures. In this paper, we\ninvestigate a more complex and realistic scenario: the attacker lacks knowledge\nof the RAG system's internal composition and implementation details, and the\nRAG system comprises components beyond a mere retriever. Specifically, we\npropose the RIPRAG attack framework, an end-to-end attack pipeline that treats\nthe target RAG system as a black box, where the only information accessible to\nthe attacker is whether the poisoning succeeds. Our method leverages\nReinforcement Learning (RL) to optimize the generation model for poisoned\ndocuments, ensuring that the generated poisoned document aligns with the target\nRAG system's preferences. Experimental results demonstrate that this method can\neffectively execute poisoning attacks against most complex RAG systems,\nachieving an attack success rate (ASR) improvement of up to 0.72 compared to\nbaseline methods. This highlights prevalent deficiencies in current defensive\nmethods and provides critical insights for LLM security research.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u9488\u5bf9\u590d\u6742RAG\u7cfb\u7edf\u7684\u9ed1\u76d2\u6295\u6bd2\u653b\u51fb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRIPRAG\u7684\u653b\u51fb\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6076\u610f\u6587\u6863\u7684\u751f\u6210\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u653b\u51fb\u590d\u6742\u7684RAG\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u9488\u5bf9\u7b80\u5316RAG\u67b6\u6784\u7684\u767d\u76d2\u653b\u51fb\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u66f4\u590d\u6742\u548c\u771f\u5b9e\u7684\u573a\u666f\uff1a\u653b\u51fb\u8005\u7f3a\u4e4f\u5bf9RAG\u7cfb\u7edf\u5185\u90e8\u7ec4\u6210\u548c\u5b9e\u73b0\u7ec6\u8282\u7684\u4e86\u89e3\uff0c\u5e76\u4e14RAG\u7cfb\u7edf\u5305\u542b\u7684\u7ec4\u4ef6\u8d85\u51fa\u4e86\u5355\u7eaf\u7684\u68c0\u7d22\u5668\u3002", "method": "\u63d0\u51faRIPRAG\u653b\u51fb\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u653b\u51fb\u6d41\u7a0b\uff0c\u5c06\u76ee\u6807RAG\u7cfb\u7edf\u89c6\u4e3a\u9ed1\u76d2\uff0c\u653b\u51fb\u8005\u552f\u4e00\u53ef\u8bbf\u95ee\u7684\u4fe1\u606f\u662f\u6295\u6bd2\u662f\u5426\u6210\u529f\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u6765\u4f18\u5316\u6076\u610f\u6587\u6863\u751f\u6210\u6a21\u578b\uff0c\u786e\u4fdd\u751f\u6210\u7684\u6076\u610f\u6587\u6863\u7b26\u5408\u76ee\u6807RAG\u7cfb\u7edf\u7684\u504f\u597d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u5bf9\u5927\u591a\u6570\u590d\u6742\u7684RAG\u7cfb\u7edf\u6267\u884c\u6295\u6bd2\u653b\u51fb\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u653b\u51fb\u6210\u529f\u7387\uff08ASR\uff09\u63d0\u9ad8\u4e860.72\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u5f53\u524d\u9632\u5fa1\u65b9\u6cd5\u4e2d\u666e\u904d\u5b58\u5728\u7684\u7f3a\u9677\uff0c\u5e76\u4e3aLLM\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u89c1\u89e3\u3002"}}
{"id": "2510.09665", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09665", "abs": "https://arxiv.org/abs/2510.09665", "authors": ["Yihua Cheng", "Yuhan Liu", "Jiayi Yao", "Yuwei An", "Xiaokun Chen", "Shaoting Feng", "Yuyang Huang", "Samuel Shen", "Kuntai Du", "Junchen Jiang"], "title": "LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference", "comment": null, "summary": "Today's LLM inference systems treat individual engines and queries\nindependently for simplicity, but this causes significant resource\ninefficiencies. While there are proposals to avoid redundant computation by\nreusing KV caches across queries and to increase GPU utilization by\ndisaggregating a single query to different engines, their promises cannot be\nrealized without efficiently offloading and communicating KV cache across LLM\ninference engines and queries.\n  We present LMCache, the first and so far the most efficient open-source KV\ncaching solution, which extracts and stores KV caches generated by modern LLM\nengines (vLLM and SGLang) and shares the KV caches across engines and queries.\nLMCache exposes KV caches in the LLM engine interface, effectively transforming\nLLM engines from individual token processors to a collection of engines with KV\ncache as the storage and communication medium. In particular, it supports both\ncache offloading (prefix reuse across queries) and prefill-decode\ndisaggregation (cross-engine cache transfer). LMCache's high performance and\nwide adoption stem from the following contributions: highly optimized KV cache\ndata movement with performance optimizations including batched data movement\noperations, compute and I/O pipelining; a modular KV cache connector component,\ndecoupling LMCache from the rapid evolution of inference engines; a first-class\ncontrol API, such as pinning, lookup, cleanup, movement, and compression, for\nflexible cache orchestration across GPU, CPU, storage, and network layers.\nEvaluation shows that combining LMCache with vLLM achieves up to 15x\nimprovement in throughput across diverse workloads. With a growing community,\nLMCache has seen dramatic growth in adoption by enterprise inference systems,\nwhich provides valuable lessons for future KV caching solutions. The source\ncode of LMCache is at: https://github.com/LMCache/LMCache.", "AI": {"tldr": "LMCache is an open-source KV caching solution that improves LLM inference efficiency by sharing KV caches across engines and queries.", "motivation": "Current LLM inference systems treat engines and queries independently, leading to resource inefficiencies. Existing proposals to avoid redundant computation and increase GPU utilization require efficient KV cache offloading and communication.", "method": "LMCache extracts and stores KV caches, exposing them in the LLM engine interface and supporting cache offloading and prefill-decode disaggregation. It features optimized data movement, a modular connector, and a control API.", "result": "LMCache achieves up to 15x throughput improvement when combined with vLLM.", "conclusion": "LMCache is widely adopted by enterprise inference systems and provides valuable lessons for future KV caching solutions."}}
{"id": "2510.10955", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10955", "abs": "https://arxiv.org/abs/2510.10955", "authors": ["Yu Cui", "Feng Liu", "Jiawei Chen", "Canghong Jin", "Xingyu Lou", "Changwang Zhang", "Jun Wang", "Yuegang Sun", "Can Wang"], "title": "HatLLM: Hierarchical Attention Masking for Enhanced Collaborative Modeling in LLM-based Recommendation", "comment": null, "summary": "Recent years have witnessed a surge of research on leveraging large language\nmodels (LLMs) for sequential recommendation. LLMs have demonstrated remarkable\npotential in inferring users' nuanced preferences through fine-grained semantic\nreasoning. However, they also exhibit a notable limitation in effectively\nmodeling collaborative signals, i.e., behavioral correlations inherent in\nusers' historical interactions. Our empirical analysis further reveals that the\nattention mechanisms in LLMs tend to disproportionately focus on tokens within\nthe same item, thereby impeding the capture of cross-item correlations.\n  To address this limitation, we propose a novel hierarchical attention masking\nstrategy for LLM-based recommendation, termed HatLLM. Specifically, in shallow\nlayers, HatLLM masks attention between tokens from different items,\nfacilitating intra-item semantic understanding; in contrast, in deep layers,\nHatLLM masks attention within items, thereby compelling the model to capture\ncross-item correlations. This progressive, layer-wise approach enables LLMs to\njointly model both token-level and item-level dependencies. Extensive\nexperiments on three real-world datasets demonstrate that HatLLM achieves\nsignificant performance gains (9.13% on average) over existing LLM-based\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8eLLM\u7684\u63a8\u8350\u65b9\u6cd5\uff0c\u540d\u4e3aHatLLM\uff0c\u901a\u8fc7\u5206\u5c42\u6ce8\u610f\u529b\u63a9\u853d\u7b56\u7565\u6765\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728\u987a\u5e8f\u63a8\u8350\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5728\u6709\u6548\u5efa\u6a21\u534f\u540c\u4fe1\u53f7\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5e76\u4e14\u6ce8\u610f\u529b\u673a\u5236\u503e\u5411\u4e8e\u8fc7\u5ea6\u5173\u6ce8\u540c\u4e00\u9879\u76ee\u5185\u7684tokens\uff0c\u963b\u788d\u4e86\u8de8\u9879\u76ee\u76f8\u5173\u6027\u7684\u6355\u83b7\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u6ce8\u610f\u529b\u63a9\u853d\u7b56\u7565\uff0c\u6d45\u5c42\u5c4f\u853d\u4e0d\u540c\u9879\u76ee\u4e4b\u95f4\u7684tokens\u6ce8\u610f\u529b\uff0c\u6df1\u5c42\u5c4f\u853d\u9879\u76ee\u5185\u7684tokens\u6ce8\u610f\u529b\uff0c\u4ece\u800c\u8054\u5408\u5efa\u6a21token\u7ea7\u522b\u548c\u9879\u76ee\u7ea7\u522b\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHatLLM\u76f8\u6bd4\u73b0\u6709\u7684\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\uff08\u5e73\u5747\u63d0\u53479.13%\uff09\u3002", "conclusion": "HatLLM\u901a\u8fc7\u5206\u5c42\u6ce8\u610f\u529b\u63a9\u853d\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u987a\u5e8f\u63a8\u8350\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.10810", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.10810", "abs": "https://arxiv.org/abs/2510.10810", "authors": ["Omar Islam Laskar", "Fatemeh Ramezani Khozestani", "Ishika Nankani", "Sohrab Namazi Nia", "Senjuti Basu Roy", "Kaustubh Beedkar"], "title": "Aegis: A Correlation-Based Data Masking Advisor for Data Sharing Ecosystems", "comment": "Accepted at SIGMOD 2026", "summary": "Data-sharing ecosystems enable entities -- such as providers, consumers, and\nintermediaries -- to access, exchange, and utilize data for various downstream\ntasks and applications. Due to privacy concerns, data providers typically\nanonymize datasets before sharing them; however, the existence of multiple\nmasking configurations results in masked datasets with varying utility.\nConsequently, a key challenge lies in efficiently determining the optimal\nmasking configuration that maximizes a dataset's utility. This paper presents\nAEGIS, a middleware framework for identifying the optimal masking configuration\nfor machine learning datasets that consist of features and a class label. We\nintroduce a utility optimizer that minimizes predictive utility deviation -- a\nmetric based on the changes in feature-label correlations before and after\nmasking. Our framework leverages limited data summaries (such as 1D histograms)\nor none to estimate the feature-label joint distribution, making it suitable\nfor scenarios where raw data is inaccessible due to privacy restrictions. To\nachieve this, we propose a joint distribution estimator based on iterative\nproportional fitting, which allows supporting various feature-label correlation\nquantification methods such as g3, mutual information, or chi-square. Our\nexperimental evaluation on real-world datasets shows that AEGIS identifies\noptimal masking configurations over an order of magnitude faster, while the\nresulting masked datasets achieve predictive performance on downstream ML tasks\nthat is on par with baseline approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86AEGIS\u4e2d\u95f4\u4ef6\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u673a\u5668\u5b66\u4e60\u6570\u636e\u96c6\u7684\u6700\u4f73\u63a9\u853d\u914d\u7f6e\uff0c\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u6570\u636e\u96c6\u7684\u6548\u7528\u3002", "motivation": "\u7531\u4e8e\u9690\u79c1\u95ee\u9898\uff0c\u6570\u636e\u63d0\u4f9b\u8005\u901a\u5e38\u5728\u5171\u4eab\u6570\u636e\u96c6\u4e4b\u524d\u5bf9\u5176\u8fdb\u884c\u533f\u540d\u5316\uff0c\u4f46\u591a\u79cd\u63a9\u853d\u914d\u7f6e\u7684\u5b58\u5728\u5bfc\u81f4\u63a9\u853d\u6570\u636e\u96c6\u7684\u6548\u7528\u5404\u4e0d\u76f8\u540c\u3002\u56e0\u6b64\uff0c\u4e00\u4e2a\u5173\u952e\u7684\u6311\u6218\u5728\u4e8e\u6709\u6548\u5730\u786e\u5b9a\u6700\u5927\u5316\u6570\u636e\u96c6\u6548\u7528\u7684\u6700\u4f73\u63a9\u853d\u914d\u7f6e\u3002", "method": "\u5229\u7528\u6709\u9650\u7684\u6570\u636e\u6458\u8981\uff08\u59821D\u76f4\u65b9\u56fe\uff09\u6216\u4e0d\u4f7f\u7528\u4efb\u4f55\u6570\u636e\u6458\u8981\u6765\u4f30\u8ba1\u7279\u5f81-\u6807\u7b7e\u8054\u5408\u5206\u5e03\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fed\u4ee3\u6bd4\u4f8b\u62df\u5408\u7684\u8054\u5408\u5206\u5e03\u4f30\u8ba1\u5668\uff0c\u652f\u6301\u5404\u79cd\u7279\u5f81-\u6807\u7b7e\u76f8\u5173\u91cf\u5316\u65b9\u6cd5\uff0c\u5982g3\u3001\u4e92\u4fe1\u606f\u6216\u5361\u65b9\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cAEGIS\u8bc6\u522b\u6700\u4f73\u63a9\u853d\u914d\u7f6e\u7684\u901f\u5ea6\u63d0\u9ad8\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u800c\u751f\u6210\u7684\u63a9\u853d\u6570\u636e\u96c6\u5728\u4e0b\u6e38ML\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "AEGIS\u6846\u67b6\u80fd\u591f\u5feb\u901f\u6709\u6548\u5730\u8bc6\u522b\u6700\u4f73\u63a9\u853d\u914d\u7f6e\uff0c\u5e76\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u6570\u636e\u7684\u6548\u7528\u3002"}}
{"id": "2510.09738", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09738", "abs": "https://arxiv.org/abs/2510.09738", "authors": ["Steve Han", "Gilberto Titericz Junior", "Tom Balough", "Wenfei Zhou"], "title": "Judge's Verdict: A Comprehensive Analysis of LLM Judge Capability Through Human Agreement", "comment": "10 pages, 1 figure, 4 tables, under review as a conference paper at\n  ICLR 2026", "summary": "This research introduces the Judge's Verdict Benchmark, a novel two-step\nmethodology to evaluate Large Language Models (LLMs) as judges for response\naccuracy evaluation tasks. We assess how well 54 LLMs can replicate human\njudgment when scoring responses from RAG (Retrieval-Augmented Generation) or\nAgentic pipelines against ground truth answers. Our methodology progresses from\ntraditional correlation analysis to comprehensive Cohen's Kappa analysis that\nmeasures actual agreement patterns. The two-step approach includes: (1) a\ncorrelation test that filters judges with strong alignment, followed by (2) a\nhuman-likeness test using z-scores to identify two distinct judgment patterns:\nhuman-like judgment (|z| < 1) that mimics natural human variation, and\nsuper-consistent judgment (z > 1) that exceeds typical human-to-human agreement\nlevels. This methodology reveals that 27 out of 54 tested LLMs achieve Tier 1\nperformance: 23 models exhibit human-like patterns that preserve the nuances of\nhuman judgment, while 4 models demonstrate super-consistent behavior, a pattern\nthat could indicate either enhanced reliability or oversimplification of\ncomplex judgments. Testing 43 open-source models (1B-405B parameters) and 11\nclosed models (GPT, Gemini, Claude variants), we demonstrate that judge\nexcellence is not solely dependent on model size but on specific training\nstrategies. Our key contributions include: (1) establishing that correlation\nalone is insufficient for judge evaluation, (2) introducing a \"Turing Test for\njudges\" based on agreement patterns, and (3) providing a standardized benchmark\nfor classifying LLM judges into distinct performance tiers for different\nevaluation needs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e24\u6b65\u6cd5\u6765\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u54cd\u5e94\u51c6\u786e\u6027\u8bc4\u4f30\u4efb\u52a1\u4e2d\u4f5c\u4e3a\u88c1\u5224\u7684\u8868\u73b0\u3002", "motivation": "\u8bc4\u4f30 54 \u4e2a LLM \u5728\u5bf9 RAG \u6216 Agentic \u7ba1\u9053\u7684\u54cd\u5e94\u8fdb\u884c\u8bc4\u5206\u65f6\uff0c\u80fd\u5426\u5f88\u597d\u5730\u590d\u5236\u4eba\u7c7b\u7684\u5224\u65ad\u3002", "method": "\u4ece\u4f20\u7edf\u7684\u5173\u8054\u5206\u6790\u5230\u7efc\u5408 Cohen Kappa \u5206\u6790\uff0c\u6d4b\u91cf\u5b9e\u9645\u7684\u4e00\u81f4\u6027\u6a21\u5f0f\u3002\u5305\u62ec\uff1a(1) \u8fc7\u6ee4\u5177\u6709\u5f3a\u5bf9\u9f50\u6027\u7684\u88c1\u5224\u7684\u5173\u8054\u6d4b\u8bd5\uff0c\u4ee5\u53ca (2) \u4f7f\u7528 z \u5206\u6570\u7684\u4eba\u7c7b\u76f8\u4f3c\u6027\u6d4b\u8bd5\uff0c\u4ee5\u8bc6\u522b\u4e24\u79cd\u4e0d\u540c\u7684\u5224\u65ad\u6a21\u5f0f\uff1a\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5224\u65ad\u548c\u8d85\u4e00\u81f4\u7684\u5224\u65ad\u3002", "result": "54 \u4e2a\u6d4b\u8bd5\u7684 LLM \u4e2d\u6709 27 \u4e2a\u8fbe\u5230\u4e86 Tier 1 \u6027\u80fd\uff1a23 \u4e2a\u6a21\u578b\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u6a21\u5f0f\uff0c\u800c 4 \u4e2a\u6a21\u578b\u8868\u73b0\u51fa\u8d85\u4e00\u81f4\u7684\u884c\u4e3a\u3002\u6cd5\u5b98\u7684\u5353\u8d8a\u6027\u5e76\u975e\u5b8c\u5168\u53d6\u51b3\u4e8e\u6a21\u578b\u5927\u5c0f\uff0c\u800c\u662f\u53d6\u51b3\u4e8e\u7279\u5b9a\u7684\u8bad\u7ec3\u7b56\u7565\u3002", "conclusion": "\u5efa\u7acb\u4e86\u76f8\u5173\u6027\u4e0d\u8db3\u4ee5\u8fdb\u884c\u6cd5\u5b98\u8bc4\u4f30\uff0c\u5f15\u5165\u4e86\u57fa\u4e8e\u4e00\u81f4\u6027\u6a21\u5f0f\u7684\u201c\u6cd5\u5b98\u56fe\u7075\u6d4b\u8bd5\u201d\uff0c\u5e76\u4e3a\u5c06 LLM \u6cd5\u5b98\u5206\u7c7b\u4e3a\u4e0d\u540c\u8bc4\u4f30\u9700\u6c42\u7684\u4e0d\u540c\u6027\u80fd\u5c42\u7ea7\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u51c6\u3002"}}
{"id": "2510.09815", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09815", "abs": "https://arxiv.org/abs/2510.09815", "authors": ["Yufei Wang", "Adriana Kovashka", "Loretta Fern\u00e1ndez", "Marc N. Coutanche", "Seth Wiener"], "title": "Towards Understanding Ambiguity Resolution in Multimodal Inference of Meaning", "comment": "Accepted to International Conference on Development and Learning\n  (ICDL) 2025", "summary": "We investigate a new setting for foreign language learning, where learners\ninfer the meaning of unfamiliar words in a multimodal context of a sentence\ndescribing a paired image. We conduct studies with human participants using\ndifferent image-text pairs. We analyze the features of the data (i.e., images\nand texts) that make it easier for participants to infer the meaning of a\nmasked or unfamiliar word, and what language backgrounds of the participants\ncorrelate with success. We find only some intuitive features have strong\ncorrelations with participant performance, prompting the need for further\ninvestigating of predictive features for success in these tasks. We also\nanalyze the ability of AI systems to reason about participant performance, and\ndiscover promising future directions for improving this reasoning ability.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u4e00\u79cd\u65b0\u7684\u5916\u8bed\u5b66\u4e60\u73af\u5883\uff0c\u5b66\u4e60\u8005\u9700\u8981\u5728\u591a\u6a21\u6001\u7684\u8bed\u5883\u4e2d\u63a8\u65ad\u751f\u8bcd\u7684\u542b\u4e49\uff0c\u5373\u901a\u8fc7\u63cf\u8ff0\u914d\u5bf9\u56fe\u50cf\u7684\u53e5\u5b50\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7a76\u5f71\u54cd\u5b66\u4e60\u8005\u5728\u591a\u6a21\u6001\u8bed\u5883\u4e0b\u63a8\u65ad\u751f\u8bcd\u542b\u4e49\u7684\u56e0\u7d20\uff0c\u4ee5\u53ca\u4e0d\u540c\u8bed\u8a00\u80cc\u666f\u7684\u5b66\u4e60\u8005\u4e0e\u6210\u529f\u7387\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u4eba\u7c7b\u53c2\u4e0e\u8005\u8fdb\u884c\u5b9e\u9a8c\uff0c\u4f7f\u7528\u4e0d\u540c\u7684\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c\u5206\u6790\u6570\u636e\uff08\u56fe\u50cf\u548c\u6587\u672c\uff09\u7684\u7279\u5f81\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u53ea\u6709\u4e00\u4e9b\u76f4\u89c2\u7684\u7279\u5f81\u4e0e\u53c2\u4e0e\u8005\u7684\u8868\u73b0\u6709\u5f88\u5f3a\u7684\u76f8\u5173\u6027\uff0c\u8fd9\u8868\u660e\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u9884\u6d4b\u4efb\u52a1\u6210\u529f\u7684\u7279\u5f81\u3002\u6b64\u5916\uff0c\u8fd8\u5206\u6790\u4e86\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5bf9\u53c2\u4e0e\u8005\u8868\u73b0\u7684\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u7ed3\u679c\u4e3a\u6539\u8fdb\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2510.10035", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10035", "abs": "https://arxiv.org/abs/2510.10035", "authors": ["Jusheng Zhang", "Kaitong Cai", "Qinglin Zeng", "Ningyuan Liu", "Stephen Fan", "Ziliang Chen", "Keze Wang"], "title": "Failure-Driven Workflow Refinement", "comment": null, "summary": "Optimizing LLM-based workflows is typically formulated as a global search,\nwhere candidate workflows are evaluated based on a scalar metric. This\nparadigm, however, suffers from a critical flaw: information collapse. By\nreducing rich, multi-step execution traces to simple success/failure signals,\nexisting methods are rendered blind to the underlying structure of failures,\nfundamentally preventing them from modeling the workflow's failure\ndistribution. We reconceptualize this challenge as a distributional problem. We\npropose a new paradigm where the optimization goal is not to maximize a scalar\nscore, but to directly minimize a workflow's Expected Failure Mass, i.e., the\nintegral of its failure probability density function defined over a\nhigh-dimensional Failure Signature Space (FSS). This distributional lens allows\nus to move from inefficient, zero-order optimization to a principled,\ngradient-like descent on the failure landscape itself. We introduce CE-Graph, a\nframework that operationalizes this paradigm through a novel, failure-driven\nrefinement process. CE-Graph approximates the failure distribution from a pool\nof counterexamples, identifies its densest regions as recurring failure modes,\nand applies targeted, operator-constrained graph edits via a Propose-and-Verify\nmechanism to greedily reduce the failure mass. On math, code, and QA\nbenchmarks, our CE-Graph achieves higher robustness at a significantly lower\ncost than strong baselines. This suggests that a system's reliability emerges\nnot from avoiding failures, but from systematically learning and reshaping the\ngeometric structure of its failure distributions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316LLM\u5de5\u4f5c\u6d41\u7a0b\u7684\u8303\u5f0f\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u9884\u671f\u5931\u8d25\u8d28\u91cf\u6765\u76f4\u63a5\u4f18\u5316\u5931\u8d25\u5206\u5e03\uff0c\u800c\u4e0d\u662f\u6700\u5927\u5316\u6807\u91cf\u5206\u6570\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u591a\u6b65\u9aa4\u6267\u884c\u8ddf\u8e2a\u7b80\u5316\u4e3a\u7b80\u5355\u7684\u6210\u529f/\u5931\u8d25\u4fe1\u53f7\uff0c\u5bfc\u81f4\u4fe1\u606f\u5d29\u6e83\uff0c\u65e0\u6cd5\u5bf9\u5de5\u4f5c\u6d41\u7a0b\u7684\u5931\u8d25\u5206\u5e03\u8fdb\u884c\u5efa\u6a21\u3002", "method": "\u63d0\u51fa CE-Graph \u6846\u67b6\uff0c\u901a\u8fc7 failure-driven \u7684\u4f18\u5316\u8fc7\u7a0b\uff0c\u4ece counterexamples \u6c60\u4e2d\u8fd1\u4f3c\u5931\u8d25\u5206\u5e03\uff0c\u8bc6\u522b\u5176\u6700\u5bc6\u96c6\u533a\u57df\u4f5c\u4e3a recurring failure modes\uff0c\u5e76\u901a\u8fc7 Propose-and-Verify \u673a\u5236\u5e94\u7528\u6709\u9488\u5bf9\u6027\u7684\u56fe\u7f16\u8f91\u6765\u51cf\u5c11\u5931\u8d25\u8d28\u91cf\u3002", "result": "\u5728\u6570\u5b66\u3001\u4ee3\u7801\u548c QA \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCE-Graph \u4ee5\u663e\u7740\u66f4\u4f4e\u7684\u6210\u672c\u5b9e\u73b0\u4e86\u6bd4\u5f3a\u57fa\u7ebf\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u4e0d\u662f\u6765\u81ea\u4e8e\u907f\u514d\u5931\u8d25\uff0c\u800c\u662f\u6765\u81ea\u4e8e\u7cfb\u7edf\u5730\u5b66\u4e60\u548c\u91cd\u5851\u5176\u5931\u8d25\u5206\u5e03\u7684\u51e0\u4f55\u7ed3\u6784\u3002"}}
{"id": "2510.09666", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09666", "abs": "https://arxiv.org/abs/2510.09666", "authors": ["Aditya Chakravarty"], "title": "Spatial Uncertainty Quantification in Wildfire Forecasting for Climate-Resilient Emergency Planning", "comment": null, "summary": "Climate change is intensifying wildfire risks globally, making reliable\nforecasting critical for adaptation strategies. While machine learning shows\npromise for wildfire prediction from Earth observation data, current approaches\nlack uncertainty quantification essential for risk-aware decision making. We\npresent the first systematic analysis of spatial uncertainty in wildfire spread\nforecasting using multimodal Earth observation inputs. We demonstrate that\npredictive uncertainty exhibits coherent spatial structure concentrated near\nfire perimeters. Our novel distance metric reveals high-uncertainty regions\nform consistent 20-60 meter buffer zones around predicted firelines - directly\napplicable for emergency planning. Feature attribution identifies vegetation\nhealth and fire activity as primary uncertainty drivers. This work enables more\nrobust wildfire management systems supporting communities adapting to\nincreasing fire risk under climate change.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5229\u7528\u673a\u5668\u5b66\u4e60\u548c\u5730\u7403\u89c2\u6d4b\u6570\u636e\u9884\u6d4b\u91ce\u706b\u8513\u5ef6\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u52a0\u5267\u4e86\u5168\u7403\u91ce\u706b\u98ce\u9669\uff0c\u53ef\u9760\u7684\u9884\u6d4b\u5bf9\u4e8e\u9002\u5e94\u7b56\u7565\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "method": "\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u5730\u5206\u6790\u4e86\u4f7f\u7528\u591a\u6a21\u6001\u5730\u7403\u89c2\u6d4b\u8f93\u5165\u8fdb\u884c\u91ce\u706b\u8513\u5ef6\u9884\u6d4b\u4e2d\u7684\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u5728\u7a7a\u95f4\u7ed3\u6784\u4e0a\u8868\u73b0\u51fa\u4e00\u81f4\u6027\uff0c\u96c6\u4e2d\u5728\u706b\u707e per\u00edmetro \u9644\u8fd1\u3002\u9ad8\u4e0d\u786e\u5b9a\u6027\u533a\u57df\u5728\u9884\u6d4b\u7684\u706b\u7ebf\u5468\u56f4\u5f62\u6210 20-60 \u7c73\u7684\u7f13\u51b2\u533a\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u652f\u6301\u793e\u7fa4\u9002\u5e94\u6c14\u5019\u53d8\u5316\u4e0b\u65e5\u76ca\u589e\u52a0\u7684\u706b\u707e\u98ce\u9669\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u5f3a\u5927\u7684\u91ce\u706b\u7ba1\u7406\u7cfb\u7edf\u3002"}}
{"id": "2510.10978", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10978", "abs": "https://arxiv.org/abs/2510.10978", "authors": ["Bohao Wang", "Jiawei Chen", "Feng Liu", "Changwang Zhang", "Jun Wang", "Canghong Jin", "Chun Chen", "Can Wang"], "title": "Does LLM Focus on the Right Words? Diagnosing Language Bias in LLM-based Recommenders", "comment": null, "summary": "Large language models (LLMs), owing to their extensive open-domain knowledge\nand semantic reasoning capabilities, have been increasingly integrated into\nrecommender systems (RS). However, a substantial gap remains between the\npre-training objectives of LLMs and the specific requirements of recommendation\ntasks. To address this gap, supervised fine-tuning (SFT) is commonly performed\non specially curated recommendation datasets to further enhance their\npredictive ability. Despite its success, SFT exhibits a critical limitation: it\ninduces Language Bias, whereby the model over-relies on auxiliary tokens-such\nas task descriptions and prefix-generated tokens-while underutilizing core user\ninteraction tokens that encode user-specific preferences. This bias not only\nundermines recommendation accuracy but also raises unfairness concerns.\n  To address this issue, we propose Group Distributionally Robust\nOptimization-based Tuning (GDRT), a novel fine-tuning paradigm that enforces\nconsistent model performance across token groups with varying degrees of\nrelevance to auxiliary tokens. By adaptively upweighting underperforming\ngroups, typically those weakly correlated with auxiliary tokens, GDRT shifts\nthe model's attention from superficial auxiliary cues to informative user\ninteraction tokens, thereby mitigating language bias. Extensive experiments\nconducted on three public datasets demonstrate that GDRT effectively mitigates\nlanguage bias, yielding substantial improvements in recommendation accuracy\n(with an average NDCG@10 gain of 24.29%) and significantly enhancing\nrecommendation fairness.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u88ab\u8d8a\u6765\u8d8a\u591a\u5730\u96c6\u6210\u5230\u63a8\u8350\u7cfb\u7edf\uff08RS\uff09\u4e2d\u3002\u4e3a\u4e86\u5f25\u8865LLM\u7684\u9884\u8bad\u7ec3\u76ee\u6807\u548c\u63a8\u8350\u4efb\u52a1\u7684\u7279\u5b9a\u8981\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u901a\u5e38\u5728\u4e13\u95e8\u7b56\u5212\u7684\u63a8\u8350\u6570\u636e\u96c6\u4e0a\u6267\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5176\u9884\u6d4b\u80fd\u529b\u3002\u4f46\u662f\uff0cSFT\u4f1a\u5e26\u6765\u8bed\u8a00\u504f\u5dee\uff0cGDRT\u901a\u8fc7\u5bf9token\u8fdb\u884c\u5206\u7ec4\uff0c\u81ea\u9002\u5e94\u5730\u589e\u52a0\u8868\u73b0\u4e0d\u4f73\u7684\u7ec4\u7684\u6743\u91cd\uff0c\u4ece\u800c\u51cf\u8f7b\u8bed\u8a00\u504f\u5dee\u3002", "motivation": "SFT\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u9650\u5236\uff1a\u5b83\u4f1a\u5f15\u8d77\u8bed\u8a00\u504f\u5dee\uff0c\u5373\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u8f85\u52a9token\uff0c\u800c\u672a\u5145\u5206\u5229\u7528\u7f16\u7801\u7528\u6237\u7279\u5b9a\u504f\u597d\u7684\u6838\u5fc3\u7528\u6237\u4ea4\u4e92token\u3002\u8fd9\u79cd\u504f\u5dee\u4e0d\u4ec5\u4f1a\u635f\u5bb3\u63a8\u8350\u51c6\u786e\u6027\uff0c\u8fd8\u4f1a\u5f15\u8d77\u4e0d\u516c\u5e73\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7fa4\u4f53\u5206\u5e03\u9c81\u68d2\u4f18\u5316\uff08GDRT\uff09\u7684\u65b0\u578b\u5fae\u8c03\u8303\u4f8b\uff0c\u8be5\u8303\u4f8b\u5f3a\u5236\u6a21\u578b\u5728\u4e0e\u8f85\u52a9token\u5177\u6709\u4e0d\u540c\u76f8\u5173\u7a0b\u5ea6\u7684token\u7ec4\u4e4b\u95f4\u4fdd\u6301\u4e00\u81f4\u7684\u6027\u80fd\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGDRT\u53ef\u4ee5\u6709\u6548\u5730\u7f13\u89e3\u8bed\u8a00\u504f\u5dee\uff0c\u4ece\u800c\u663e\u7740\u63d0\u9ad8\u63a8\u8350\u51c6\u786e\u6027\uff08NDCG@10\u5e73\u5747\u589e\u76ca\u4e3a24.29%\uff09\uff0c\u5e76\u663e\u7740\u63d0\u9ad8\u63a8\u8350\u516c\u5e73\u6027\u3002", "conclusion": "GDRT\u901a\u8fc7\u5c06\u6a21\u578b\u7684\u6ce8\u610f\u529b\u4ece\u80a4\u6d45\u7684\u8f85\u52a9\u7ebf\u7d22\u8f6c\u79fb\u5230\u4fe1\u606f\u4e30\u5bcc\u7684\u7528\u6237\u4ea4\u4e92token\uff0c\u4ece\u800c\u51cf\u8f7b\u8bed\u8a00\u504f\u5dee\u3002"}}
{"id": "2510.10885", "categories": ["cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.10885", "abs": "https://arxiv.org/abs/2510.10885", "authors": ["Jiajing Guo", "Kenil Patel", "Jorge Piazentin Ono", "Wenbin He", "Liu Ren"], "title": "Rethinking Agentic Workflows: Evaluating Inference-Based Test-Time Scaling Strategies in Text2SQL Tasks", "comment": "Accepted at COLM 2025 SCALR Workshop", "summary": "Large language models (LLMs) are increasingly powering Text-to-SQL (Text2SQL)\nsystems, enabling non-expert users to query industrial databases using natural\nlanguage. While test-time scaling strategies have shown promise in LLM-based\nsolutions, their effectiveness in real-world applications, especially with the\nlatest reasoning models, remains uncertain. In this work, we benchmark six\nlightweight, industry-oriented test-time scaling strategies and four LLMs,\nincluding two reasoning models, evaluating their performance on the BIRD\nMini-Dev benchmark. Beyond standard accuracy metrics, we also report inference\nlatency and token consumption, providing insights relevant for practical system\ndeployment. Our findings reveal that Divide-and-Conquer prompting and few-shot\ndemonstrations consistently enhance performance for both general-purpose and\nreasoning-focused LLMs. However, introducing additional workflow steps yields\nmixed results, and base model selection plays a critical role. This work sheds\nlight on the practical trade-offs between accuracy, efficiency, and complexity\nwhen deploying Text2SQL systems.", "AI": {"tldr": "\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728Text2SQL\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\uff0c\u7279\u522b\u662f\u63a8\u7406\u6a21\u578b\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728Text2SQL\u7cfb\u7edf\u4e2d\u5e94\u7528\u4e8e\u5de5\u4e1a\u6570\u636e\u5e93\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u6700\u65b0\u7684\u63a8\u7406\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5bf9\u516d\u79cd\u8f7b\u91cf\u7ea7\u7684\u3001\u9762\u5411\u5de5\u4e1a\u754c\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\u548c\u56db\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec\u4e24\u79cd\u63a8\u7406\u6a21\u578b\uff0c\u5e76\u5728BIRD Mini-Dev\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e86\u5b83\u4eec\u7684\u6027\u80fd\u3002", "result": "\u53d1\u73b0\u5206\u800c\u6cbb\u4e4b\u7684\u63d0\u793a\u548c\u5c11\u91cf\u6837\u672c\u6f14\u793a\u80fd\u591f\u6301\u7eed\u63d0\u9ad8\u901a\u7528\u548c\u63a8\u7406\u578b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5f15\u5165\u989d\u5916\u7684\u5de5\u4f5c\u6d41\u7a0b\u6b65\u9aa4\u4f1a\u4ea7\u751f\u4e0d\u540c\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u57fa\u7840\u6a21\u578b\u7684\u9009\u62e9\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u9610\u660e\u4e86\u5728\u90e8\u7f72Text2SQL\u7cfb\u7edf\u65f6\uff0c\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u590d\u6742\u6027\u4e4b\u95f4\u7684\u5b9e\u9645\u6743\u8861\u3002"}}
{"id": "2510.09770", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09770", "abs": "https://arxiv.org/abs/2510.09770", "authors": ["Adam Byerly", "Daniel Khashabi"], "title": "Gold Panning: Turning Positional Bias into Signal for Multi-Document LLM Reasoning", "comment": "20 pages, 6 figures", "summary": "Large language models exhibit a strong position bias in multi-document\ncontexts, systematically prioritizing information based on location rather than\nrelevance. While existing approaches treat this bias as noise to be mitigated,\nwe introduce Gold Panning Bandits, a framework that leverages position bias as\na diagnostic signal: by reordering documents and observing shifts in the\nmodel's responses, we can efficiently identify the most relevant content. We\nframe the problem of choosing reorderings as a bipartite matching problem.\nWhile an optimal assignment can be computed at each iteration with the\nHungarian algorithm in $O(N^3)$ time, we propose a greedy $O(N \\log N)$\nstrategy that achieves comparable performance by prioritizing the placement of\nthe most uncertain documents in the most informative positions. Our approach\nidentifies relevant documents using up to 65\\% fewer language model queries\nthan random permutation baselines on knowledge-intensive NLP tasks,\nsubstantially reducing computational cost without model retraining. This work\ndemonstrates that inherent LLM biases can be transformed from liabilities into\nassets for efficient, inference-time optimization.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Gold Panning Bandits \u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6587\u6863\u4e0a\u4e0b\u6587\u4e2d\u56fa\u6709\u7684\u4f4d\u7f6e\u504f\u5dee\u6765\u9ad8\u6548\u8bc6\u522b\u6700\u76f8\u5173\u7684\u5185\u5bb9\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5c06\u4f4d\u7f6e\u504f\u5dee\u89c6\u4e3a\u9700\u8981\u51cf\u8f7b\u7684\u566a\u58f0\uff0c\u800c\u672c\u6587\u5219\u65e8\u5728\u5229\u7528\u5b83\u4f5c\u4e3a\u8bca\u65ad\u4fe1\u53f7\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u91cd\u65b0\u6392\u5e8f\u6587\u6863\u7684\u95ee\u9898\u5efa\u6a21\u4e3a\u4e8c\u5206\u5339\u914d\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8d2a\u5a6a\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u901a\u8fc7\u4f18\u5148\u5c06\u6700\u4e0d\u786e\u5b9a\u7684\u6587\u6863\u653e\u7f6e\u5728\u4fe1\u606f\u91cf\u6700\u5927\u7684\u4f4d\u7f6e\u6765\u5b9e\u73b0\u53ef\u6bd4\u8f83\u7684\u6027\u80fd\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b NLP \u4efb\u52a1\u4e2d\uff0c\u4e0e\u968f\u673a\u7f6e\u6362\u57fa\u7ebf\u76f8\u6bd4\uff0c\u4f7f\u7528\u66f4\u5c11\u7684\u8bed\u8a00\u6a21\u578b\u67e5\u8be2\uff08\u6700\u591a\u51cf\u5c11 65%\uff09\u6765\u8bc6\u522b\u76f8\u5173\u6587\u6863\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8868\u660e\uff0cLLM \u56fa\u6709\u7684\u504f\u5dee\u53ef\u4ee5\u4ece\u52a3\u52bf\u8f6c\u5316\u4e3a\u4f18\u52bf\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u63a8\u7406\u65f6\u4f18\u5316\u3002"}}
{"id": "2510.09822", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09822", "abs": "https://arxiv.org/abs/2510.09822", "authors": ["Weiqing Luo", "Zhen Tan", "Yifan Li", "Xinyu Zhao", "Kwonjoon Lee", "Behzad Dariush", "Tianlong Chen"], "title": "Task-Aware Resolution Optimization for Visual Large Language Models", "comment": "Accepted as a main conference paper at EMNLP 2025. 9 pages (main\n  content), 7 figures", "summary": "Real-world vision-language applications demand varying levels of perceptual\ngranularity. However, most existing visual large language models (VLLMs), such\nas LLaVA, pre-assume a fixed resolution for downstream tasks, which leads to\nsubpar performance. To address this problem, we first conduct a comprehensive\nand pioneering investigation into the resolution preferences of different\nvision-language tasks, revealing a correlation between resolution preferences\nwith image complexity, and uncertainty variance of the VLLM at different image\ninput resolutions. Building on this insight, we propose an empirical formula to\ndetermine the optimal resolution for a given vision-language task, combining\nthese two factors. Second, based on rigorous experiments, we propose a novel\nparameter-efficient fine-tuning technique to extend the visual input resolution\nof pre-trained VLLMs to the identified optimal resolution. Extensive\nexperiments on various vision-language tasks validate the effectiveness of our\nmethod.", "AI": {"tldr": "\u73b0\u6709\u7684\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b(VLLM)\u901a\u5e38\u4e3a\u4e0b\u6e38\u4efb\u52a1\u9884\u8bbe\u56fa\u5b9a\u7684\u5206\u8fa8\u7387\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u7684\u89c6\u89c9\u8bed\u8a00\u5e94\u7528\u9700\u8981\u4e0d\u540c\u5c42\u6b21\u7684\u611f\u77e5\u7c92\u5ea6\u3002\u73b0\u6709VLLM\u7684\u5206\u8fa8\u7387\u662f\u56fa\u5b9a\u7684\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\u3002", "method": "1. \u8c03\u67e5\u4e0d\u540c\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u7684\u5206\u8fa8\u7387\u504f\u597d\uff0c\u63ed\u793a\u5206\u8fa8\u7387\u504f\u597d\u4e0e\u56fe\u50cf\u590d\u6742\u5ea6\u548cVLLM\u5728\u4e0d\u540c\u56fe\u50cf\u8f93\u5165\u5206\u8fa8\u7387\u4e0b\u7684\u4e0d\u786e\u5b9a\u6027\u65b9\u5dee\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff1b2. \u63d0\u51fa\u4e86\u4e00\u4e2a\u7ecf\u9a8c\u516c\u5f0f\uff0c\u7ed3\u5408\u56fe\u50cf\u590d\u6742\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u65b9\u5dee\u6765\u786e\u5b9a\u7ed9\u5b9a\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u7684\u6700\u4f73\u5206\u8fa8\u7387\uff1b3. \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u6280\u672f\uff0c\u5c06\u9884\u8bad\u7ec3\u7684VLLM\u7684\u89c6\u89c9\u8f93\u5165\u5206\u8fa8\u7387\u6269\u5c55\u5230\u6240\u786e\u5b9a\u7684\u6700\u4f73\u5206\u8fa8\u7387\u3002", "result": "\u5728\u5404\u79cd\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347VLLM\u5728\u4e0d\u540c\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2510.10042", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10042", "abs": "https://arxiv.org/abs/2510.10042", "authors": ["Saleh Nikooroo", "Thomas Engel"], "title": "Belief Graphs with Reasoning Zones: Structure, Dynamics, and Epistemic Activation", "comment": null, "summary": "Belief systems are rarely globally consistent, yet effective reasoning often\npersists locally. We propose a novel graph-theoretic framework that cleanly\nseparates credibility--external, a priori trust in sources--from confidence--an\ninternal, emergent valuation induced by network structure. Beliefs are nodes in\na directed, signed, weighted graph whose edges encode support and\ncontradiction. Confidence is obtained by a contractive propagation process that\nmixes a stated prior with structure-aware influence and guarantees a unique,\nstable solution. Within this dynamics, we define reasoning zones:\nhigh-confidence, structurally balanced subgraphs on which classical inference\nis safe despite global contradictions. We provide a near-linear procedure that\nseeds zones by confidence, tests balance using a parity-based coloring, and\napplies a greedy, locality-preserving repair with Jaccard de-duplication to\nbuild a compact atlas. To model belief change, we introduce shock updates that\nlocally downscale support and elevate targeted contradictions while preserving\ncontractivity via a simple backtracking rule. Re-propagation yields localized\nreconfiguration-zones may shrink, split, or collapse--without destabilizing the\nentire graph. We outline an empirical protocol on synthetic signed graphs with\nplanted zones, reporting zone recovery, stability under shocks, and runtime.\nThe result is a principled foundation for contradiction-tolerant reasoning that\nactivates classical logic precisely where structure supports it.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u8bba\u6846\u67b6\uff0c\u5c06\u53ef\u4fe1\u5ea6\uff08\u5916\u90e8\u7684\u5148\u9a8c\u4fe1\u4efb\uff09\u4e0e\u7f6e\u4fe1\u5ea6\uff08\u5185\u90e8\u7684\u6d8c\u73b0\u8bc4\u4f30\uff09\u533a\u5206\u5f00\u6765\uff0c\u7528\u4e8e\u77db\u76fe\u5bb9\u5fcd\u63a8\u7406\u3002", "motivation": "\u4fe1\u5ff5\u7cfb\u7edf\u5f88\u5c11\u5168\u5c40\u4e00\u81f4\uff0c\u4f46\u6709\u6548\u7684\u63a8\u7406\u901a\u5e38\u5728\u5c40\u90e8\u6301\u7eed\u5b58\u5728\u3002", "method": "\u4fe1\u5ff5\u8868\u793a\u4e3a\u6709\u5411\u3001\u6709\u7b26\u53f7\u3001\u52a0\u6743\u56fe\u4e2d\u7684\u8282\u70b9\uff0c\u8fb9\u8868\u793a\u652f\u6301\u548c\u77db\u76fe\u3002\u901a\u8fc7\u6536\u7f29\u4f20\u64ad\u8fc7\u7a0b\u83b7\u5f97\u7f6e\u4fe1\u5ea6\uff0c\u8be5\u8fc7\u7a0b\u6df7\u5408\u4e86\u5148\u9a8c\u77e5\u8bc6\u548c\u7ed3\u6784\u611f\u77e5\u7684\u5f71\u54cd\uff0c\u5e76\u4fdd\u8bc1\u4e86\u72ec\u7279\u7684\u7a33\u5b9a\u89e3\u3002\u5b9a\u4e49\u4e86\u63a8\u7406\u533a\u57df\uff1a\u9ad8\u7f6e\u4fe1\u5ea6\u3001\u7ed3\u6784\u5e73\u8861\u7684\u5b50\u56fe\uff0c\u5373\u4f7f\u5b58\u5728\u5168\u5c40\u77db\u76fe\uff0c\u7ecf\u5178\u63a8\u7406\u4e5f\u662f\u5b89\u5168\u7684\u3002\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8fd1\u4f3c\u7ebf\u6027\u7684\u8fc7\u7a0b\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u64ad\u79cd\u533a\u57df\uff0c\u4f7f\u7528\u57fa\u4e8e\u5947\u5076\u6821\u9a8c\u7684\u7740\u8272\u6d4b\u8bd5\u5e73\u8861\uff0c\u5e76\u5e94\u7528\u8d2a\u5a6a\u7684\u3001\u5c40\u90e8\u6027\u4fdd\u7559\u7684\u4fee\u590d\u4e0eJaccard\u53bb\u91cd\u6765\u6784\u5efa\u7d27\u51d1\u7684\u56fe\u96c6\u3002\u4e3a\u4e86\u6a21\u62df\u4fe1\u5ff5\u53d8\u5316\uff0c\u5f15\u5165\u4e86\u51b2\u51fb\u66f4\u65b0\uff0c\u8be5\u66f4\u65b0\u5728\u5c40\u90e8\u7f29\u5c0f\u652f\u6301\u5e76\u63d0\u5347\u76ee\u6807\u77db\u76fe\uff0c\u540c\u65f6\u901a\u8fc7\u7b80\u5355\u7684\u56de\u6eaf\u89c4\u5219\u4fdd\u6301\u6536\u7f29\u6027\u3002\u91cd\u65b0\u4f20\u64ad\u4f1a\u4ea7\u751f\u5c40\u90e8\u91cd\u65b0\u914d\u7f6e\u2014\u2014\u533a\u57df\u53ef\u80fd\u4f1a\u7f29\u5c0f\u3001\u5206\u88c2\u6216\u5d29\u6e83\u2014\u2014\u800c\u4e0d\u4f1a\u7834\u574f\u6574\u4e2a\u56fe\u3002", "result": "\u5728\u5177\u6709\u690d\u5165\u533a\u57df\u7684\u5408\u6210\u6709\u7b26\u53f7\u56fe\u4e0a\u6982\u8ff0\u4e86\u4e00\u4e2a\u7ecf\u9a8c\u534f\u8bae\uff0c\u62a5\u544a\u4e86\u533a\u57df\u6062\u590d\u3001\u51b2\u51fb\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u8fd0\u884c\u65f6\u3002", "conclusion": "\u4e3a\u77db\u76fe\u5bb9\u5fcd\u63a8\u7406\u5960\u5b9a\u4e86\u539f\u5219\u6027\u57fa\u7840\uff0c\u8be5\u57fa\u7840\u7cbe\u786e\u5730\u5728\u7ed3\u6784\u652f\u6301\u5b83\u7684\u5730\u65b9\u6fc0\u6d3b\u7ecf\u5178\u903b\u8f91\u3002"}}
{"id": "2510.09668", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.09668", "abs": "https://arxiv.org/abs/2510.09668", "authors": ["Maryam Abdollahi Shamami", "Babak Teimourpour", "Farshad Sharifi"], "title": "A Hybrid Computational Intelligence Framework with Metaheuristic Optimization for Drug-Drug Interaction Prediction", "comment": null, "summary": "Drug-drug interactions (DDIs) are a leading cause of preventable adverse\nevents, often complicating treatment and increasing healthcare costs. At the\nsame time, knowing which drugs do not interact is equally important, as such\nknowledge supports safer prescriptions and better patient outcomes. In this\nstudy, we propose an interpretable and efficient framework that blends modern\nmachine learning with domain knowledge to improve DDI prediction. Our approach\ncombines two complementary molecular embeddings - Mol2Vec, which captures\nfragment-level structural patterns, and SMILES-BERT, which learns contextual\nchemical features - together with a leakage-free, rule-based clinical score\n(RBScore) that injects pharmacological knowledge without relying on interaction\nlabels. A lightweight neural classifier is then optimized using a novel\nthree-stage metaheuristic strategy (RSmpl-ACO-PSO), which balances global\nexploration and local refinement for stable performance. Experiments on\nreal-world datasets demonstrate that the model achieves high predictive\naccuracy (ROC-AUC 0.911, PR-AUC 0.867 on DrugBank) and generalizes well to a\nclinically relevant Type 2 Diabetes Mellitus cohort. Beyond raw performance,\nstudies show how embedding fusion, RBScore, and the optimizer each contribute\nto precision and robustness. Together, these results highlight a practical\npathway for building reliable, interpretable, and computationally efficient\nmodels that can support safer drug therapies and clinical decision-making.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u878d\u5408\u4e86\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u548c\u9886\u57df\u77e5\u8bc6\uff0c\u4ee5\u6539\u8fdb DDI \u9884\u6d4b\u3002", "motivation": "\u836f\u7269-\u836f\u7269\u76f8\u4e92\u4f5c\u7528 (DDI) \u662f\u5bfc\u81f4\u53ef\u9884\u9632\u4e0d\u826f\u4e8b\u4ef6\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u901a\u5e38\u4f1a\u4f7f\u6cbb\u7597\u590d\u6742\u5316\u5e76\u589e\u52a0\u533b\u7597\u4fdd\u5065\u6210\u672c\u3002\u540c\u65f6\uff0c\u4e86\u89e3\u54ea\u4e9b\u836f\u7269\u4e0d\u76f8\u4e92\u4f5c\u7528\u4e5f\u540c\u6837\u91cd\u8981\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u77e5\u8bc6\u53ef\u4ee5\u652f\u6301\u66f4\u5b89\u5168\u7684\u5904\u65b9\u548c\u66f4\u597d\u7684\u60a3\u8005\u9884\u540e\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u5206\u5b50\u5d4c\u5165\uff1aMol2Vec\uff08\u6355\u83b7\u7247\u6bb5\u7ea7\u7ed3\u6784\u6a21\u5f0f\uff09\u548c SMILES-BERT\uff08\u5b66\u4e60\u4e0a\u4e0b\u6587\u5316\u5b66\u7279\u5f81\uff09\uff0c\u4ee5\u53ca\u65e0\u6cc4\u6f0f\u7684\u3001\u57fa\u4e8e\u89c4\u5219\u7684\u4e34\u5e8a\u8bc4\u5206 (RBScore)\uff0c\u8be5\u8bc4\u5206\u6ce8\u5165\u836f\u7406\u5b66\u77e5\u8bc6\u800c\u4e0d\u4f9d\u8d56\u4e8e\u76f8\u4e92\u4f5c\u7528\u6807\u7b7e\u3002\u7136\u540e\u4f7f\u7528\u4e00\u79cd\u65b0\u9896\u7684\u4e09\u9636\u6bb5\u5143\u542f\u53d1\u5f0f\u7b56\u7565 (RSmpl-ACO-PSO) \u4f18\u5316\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u795e\u7ecf\u5206\u7c7b\u5668\uff0c\u8be5\u7b56\u7565\u5e73\u8861\u4e86\u5168\u5c40\u63a2\u7d22\u548c\u5c40\u90e8\u7ec6\u5316\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u7684\u6027\u80fd\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u9884\u6d4b\u7cbe\u5ea6\uff08\u5728 DrugBank \u4e0a ROC-AUC \u4e3a 0.911\uff0cPR-AUC \u4e3a 0.867\uff09\uff0c\u5e76\u4e14\u53ef\u4ee5\u5f88\u597d\u5730\u63a8\u5e7f\u5230\u4e34\u5e8a\u76f8\u5173\u7684 2 \u578b\u7cd6\u5c3f\u75c5\u961f\u5217\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u5171\u540c\u7a81\u51fa\u4e86\u6784\u5efa\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u6a21\u578b\u7684\u5b9e\u7528\u9014\u5f84\uff0c\u8fd9\u4e9b\u6a21\u578b\u53ef\u4ee5\u652f\u6301\u66f4\u5b89\u5168\u7684\u836f\u7269\u6cbb\u7597\u548c\u4e34\u5e8a\u51b3\u7b56\u3002"}}
{"id": "2510.11056", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11056", "abs": "https://arxiv.org/abs/2510.11056", "authors": ["Runze Xia", "Yupeng Ji", "Yuxi Zhou", "Haodong Liu", "Teng Zhang", "Piji Li"], "title": "From Reasoning LLMs to BERT: A Two-Stage Distillation Framework for Search Relevance", "comment": null, "summary": "Query-service relevance prediction in e-commerce search systems faces strict\nlatency requirements that prevent the direct application of Large Language\nModels (LLMs). To bridge this gap, we propose a two-stage reasoning\ndistillation framework to transfer reasoning capabilities from a powerful\nteacher LLM to a lightweight, deployment-friendly student model. In the first\nstage, we address the limitations of general-purpose LLMs by constructing a\ndomain-adapted teacher model. This is achieved through a three-step process:\ndomain-adaptive pre-training to inject platform knowledge, supervised\nfine-tuning to elicit reasoning skills, and preference optimization with a\nmulti-dimensional reward model to ensure the generation of reliable and\npreference-aligned reasoning paths. This teacher can then automatically\nannotate massive query-service pairs from search logs with both relevance\nlabels and reasoning chains. In the second stage, to address the challenges of\narchitectural heterogeneity in standard distillation, we introduce Contrastive\nReasoning Self-Distillation (CRSD). By modeling the behavior of the same\nstudent model under \"standard\" and \"reasoning-augmented\" inputs as a\nteacher-student relationship, CRSD enables the lightweight model to internalize\nthe teacher's complex decision-making mechanisms without needing the explicit\nreasoning path at inference. Offline evaluations and online A/B testing in the\nMeituan search advertising system demonstrate that our framework achieves\nsignificant improvements across multiple metrics, validating its effectiveness\nand practical value.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u63a8\u7406\u84b8\u998f\u6846\u67b6\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\u8f6c\u79fb\u5230\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u4ee5\u6ee1\u8db3\u7535\u5546\u641c\u7d22\u7cfb\u7edf\u7684\u4e25\u683c\u5ef6\u8fdf\u8981\u6c42\u3002", "motivation": "\u76f4\u63a5\u5e94\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5230\u7535\u5546\u641c\u7d22\u7cfb\u7edf\u7684\u67e5\u8be2\u670d\u52a1\u76f8\u5173\u6027\u9884\u6d4b\u4e2d\uff0c\u9762\u4e34\u4e25\u683c\u7684\u5ef6\u8fdf\u8981\u6c42\u3002", "method": "1. \u6784\u5efa\u9886\u57df\u81ea\u9002\u5e94\u7684\u6559\u5e08\u6a21\u578b\uff1a\u901a\u8fc7\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u6ce8\u5165\u5e73\u53f0\u77e5\u8bc6\uff0c\u76d1\u7763\u5fae\u8c03\u5f15\u53d1\u63a8\u7406\u6280\u80fd\uff0c\u4ee5\u53ca\u4f7f\u7528\u591a\u7ef4\u5956\u52b1\u6a21\u578b\u8fdb\u884c\u504f\u597d\u4f18\u5316\u30022. \u5f15\u5165\u5bf9\u6bd4\u63a8\u7406\u81ea\u84b8\u998f\uff08CRSD\uff09\uff1a\u901a\u8fc7\u6a21\u62df\u540c\u4e00\u5b66\u751f\u6a21\u578b\u5728\u201c\u6807\u51c6\u201d\u548c\u201c\u63a8\u7406\u589e\u5f3a\u201d\u8f93\u5165\u4e0b\u7684\u884c\u4e3a\uff0c\u4f7f\u8f7b\u91cf\u7ea7\u6a21\u578b\u80fd\u591f\u5185\u5316\u6559\u5e08\u7684\u590d\u6742\u51b3\u7b56\u673a\u5236\u3002", "result": "\u79bb\u7ebf\u8bc4\u4f30\u548c\u7f8e\u56e2\u641c\u7d22\u5e7f\u544a\u7cfb\u7edf\u4e0a\u7684\u5728\u7ebfA/B\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.10942", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.10942", "abs": "https://arxiv.org/abs/2510.10942", "authors": ["Nilima Rao", "Jagriti Srivastava", "Pradeep Kumar Sharma", "Hritvik Shrivastava"], "title": "Scalable and Explainable Enterprise Knowledge Discovery Using Graph-Centric Hybrid Retrieval", "comment": null, "summary": "Modern enterprises manage vast knowledge distributed across heterogeneous\nsystems such as Jira, Git repositories, Confluence, and wikis. Conventional\nretrieval methods based on keyword search or static embeddings often fail to\nanswer complex queries that require contextual reasoning and multi-hop\ninference across artifacts. We present a modular hybrid retrieval framework for\nadaptive enterprise information access that integrates Knowledge Base\nLanguage-Augmented Models (KBLam), DeepGraph representations, and\nembedding-driven semantic search. The framework builds a unified knowledge\ngraph from parsed repositories including code, pull requests, and commit\nhistories, enabling semantic similarity search, structural inference, and\nmulti-hop reasoning. Query analysis dynamically determines the optimal\nretrieval strategy, supporting both structured and unstructured data sources\nthrough independent or fused processing. An interactive interface provides\ngraph visualizations, subgraph exploration, and context-aware query routing to\ngenerate concise and explainable answers. Experiments on large-scale Git\nrepositories show that the unified reasoning layer improves answer relevance by\nup to 80 percent compared with standalone GPT-based retrieval pipelines. By\ncombining graph construction, hybrid reasoning, and interactive visualization,\nthe proposed framework offers a scalable, explainable, and user-centric\nfoundation for intelligent knowledge assistants in enterprise environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u81ea\u9002\u5e94\u4f01\u4e1a\u4fe1\u606f\u8bbf\u95ee\u7684\u6a21\u5757\u5316\u6df7\u5408\u68c0\u7d22\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u77e5\u8bc6\u5e93\u8bed\u8a00\u589e\u5f3a\u6a21\u578b\uff08KBLam\uff09\u3001DeepGraph\u8868\u793a\u548c\u5d4c\u5165\u9a71\u52a8\u7684\u8bed\u4e49\u641c\u7d22\u3002", "motivation": "\u4f20\u7edf\u68c0\u7d22\u65b9\u6cd5\u57fa\u4e8e\u5173\u952e\u8bcd\u641c\u7d22\u6216\u9759\u6001\u5d4c\u5165\uff0c\u901a\u5e38\u65e0\u6cd5\u56de\u7b54\u9700\u8981\u8de8artifacts\u8fdb\u884c\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u591a\u8df3\u63a8\u7406\u7684\u590d\u6742\u67e5\u8be2\u3002", "method": "\u8be5\u6846\u67b6\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u8be5\u77e5\u8bc6\u56fe\u8c31\u6765\u81ea\u89e3\u6790\u7684\u5b58\u50a8\u5e93\uff0c\u5305\u62ec\u4ee3\u7801\u3001pull request\u548c\u63d0\u4ea4\u5386\u53f2\uff0c\u4ece\u800c\u5b9e\u73b0\u8bed\u4e49\u76f8\u4f3c\u6027\u641c\u7d22\u3001\u7ed3\u6784\u63a8\u7406\u548c\u591a\u8df3\u63a8\u7406\u3002\u67e5\u8be2\u5206\u6790\u52a8\u6001\u5730\u786e\u5b9a\u6700\u4f73\u68c0\u7d22\u7b56\u7565\uff0c\u901a\u8fc7\u72ec\u7acb\u6216\u878d\u5408\u5904\u7406\u6765\u652f\u6301\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\u6e90\u3002", "result": "\u5728\u5927\u578bGit\u5b58\u50a8\u5e93\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u72ec\u7acb\u7684\u57fa\u4e8eGPT\u7684\u68c0\u7d22\u7ba1\u9053\u76f8\u6bd4\uff0c\u7edf\u4e00\u7684\u63a8\u7406\u5c42\u5c06\u7b54\u6848\u76f8\u5173\u6027\u63d0\u9ad8\u4e86\u9ad8\u8fbe80%\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u56fe\u6784\u5efa\u3001\u6df7\u5408\u63a8\u7406\u548c\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u667a\u80fd\u77e5\u8bc6\u52a9\u624b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u548c\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u57fa\u7840\u3002"}}
{"id": "2510.09771", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09771", "abs": "https://arxiv.org/abs/2510.09771", "authors": ["Rakib Hossan", "Shubhashis Roy Dipta"], "title": "PromptGuard at BLP-2025 Task 1: A Few-Shot Classification Framework Using Majority Voting and Keyword Similarity for Bengali Hate Speech Detection", "comment": null, "summary": "The BLP-2025 Task 1A requires Bengali hate speech classification into six\ncategories. Traditional supervised approaches need extensive labeled datasets\nthat are expensive for low-resource languages. We developed PromptGuard, a\nfew-shot framework combining chi-square statistical analysis for keyword\nextraction with adaptive majority voting for decision-making. We explore\nstatistical keyword selection versus random approaches and adaptive voting\nmechanisms that extend classification based on consensus quality. Chi-square\nkeywords provide consistent improvements across categories, while adaptive\nvoting benefits ambiguous cases requiring extended classification rounds.\nPromptGuard achieves a micro-F1 of 67.61, outperforming n-gram baselines\n(60.75) and random approaches (14.65). Ablation studies confirm\nchi-square-based keywords show the most consistent impact across all\ncategories.", "AI": {"tldr": "PromptGuard: A few-shot framework for Bengali hate speech classification.", "motivation": "Traditional supervised methods need extensive labeled data, which is expensive for low-resource languages like Bengali.", "method": "Combines chi-square keyword extraction with adaptive majority voting.", "result": "Achieves a micro-F1 of 67.61, outperforming baselines.", "conclusion": "Chi-square keywords provide consistent improvements, and adaptive voting benefits ambiguous cases."}}
{"id": "2510.09833", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09833", "abs": "https://arxiv.org/abs/2510.09833", "authors": ["Aashish Dhawan", "Pankaj Bodani", "Vishal Garg"], "title": "Post Processing of image segmentation using Conditional Random Fields", "comment": null, "summary": "The output of image the segmentation process is usually not very clear due to\nlow quality features of Satellite images. The purpose of this study is to find\na suitable Conditional Random Field (CRF) to achieve better clarity in a\nsegmented image. We started with different types of CRFs and studied them as to\nwhy they are or are not suitable for our purpose. We evaluated our approach on\ntwo different datasets - Satellite imagery having low quality features and high\nquality Aerial photographs. During the study we experimented with various CRFs\nto find which CRF gives the best results on images and compared our results on\nthese datasets to show the pitfalls and potentials of different approaches.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u627e\u5230\u4e00\u79cd\u5408\u9002\u7684\u6761\u4ef6\u968f\u673a\u573a\uff08CRF\uff09\u4ee5\u63d0\u9ad8\u5206\u5272\u56fe\u50cf\u7684\u6e05\u6670\u5ea6\u3002", "motivation": "\u536b\u661f\u56fe\u50cf\u7684\u4f4e\u8d28\u91cf\u7279\u5f81\u5bfc\u81f4\u56fe\u50cf\u5206\u5272\u8fc7\u7a0b\u7684\u8f93\u51fa\u901a\u5e38\u4e0d\u662f\u5f88\u6e05\u695a\u3002", "method": "\u7814\u7a76\u4e86\u4e0d\u540c\u7c7b\u578b\u7684CRF\uff0c\u5e76\u8bc4\u4f30\u4e86\u5b83\u4eec\u5bf9\u536b\u661f\u56fe\u50cf\u548c\u9ad8\u8d28\u91cf\u822a\u62cd\u7167\u7247\u7684\u9002\u7528\u6027\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u5404\u79cdCRF\u5728\u56fe\u50cf\u4e0a\u7684\u7ed3\u679c\uff0c\u5e76\u5c55\u793a\u4e86\u4e0d\u540c\u65b9\u6cd5\u5728\u8fd9\u4e9b\u6570\u636e\u96c6\u4e0a\u7684\u4f18\u7f3a\u70b9\u3002", "conclusion": "\u627e\u5230\u4e86\u4e00\u79cd\u5408\u9002\u7684CRF\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5206\u5272\u56fe\u50cf\u7684\u6e05\u6670\u5ea6\u3002"}}
{"id": "2510.10047", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10047", "abs": "https://arxiv.org/abs/2510.10047", "authors": ["Ruohao Li", "Hongjun Liu", "Leyi Zhao", "Zisu Li", "Jiawei Li", "Jiajun Jiang", "Linning Xu", "Chen Zhao", "Mingming Fan", "Chen Liang"], "title": "SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive Reasoning", "comment": "14 pages, 7 figures", "summary": "Large language model (LLM) agents have shown remarkable reasoning abilities.\nHowever, existing multi-agent frameworks often rely on fixed roles or\ncentralized control, limiting scalability and adaptability in long-horizon\nreasoning. We introduce SwarmSys, a closed-loop framework for distributed\nmulti-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys\nemerges through iterative interactions among three specialized roles,\nExplorers, Workers, and Validators, that continuously cycle through\nexploration, exploitation, and validation. To enable scalable and adaptive\ncollaboration, we integrate adaptive agent and event profiles, embedding-based\nprobabilistic matching, and a pheromone-inspired reinforcement mechanism,\nsupporting dynamic task allocation and self-organizing convergence without\nglobal supervision. Across symbolic reasoning, research synthesis, and\nscientific programming tasks, SwarmSys consistently outperforms baselines,\nimproving both accuracy and reasoning stability. These findings highlight\nswarm-inspired coordination as a promising paradigm for scalable, robust, and\nadaptive multi-agent reasoning, suggesting that coordination scaling may rival\nmodel scaling in advancing LLM intelligence.", "AI": {"tldr": "SwarmSys is a closed-loop framework for distributed multi-agent reasoning inspired by swarm intelligence.", "motivation": "Existing multi-agent frameworks often rely on fixed roles or centralized control, limiting scalability and adaptability in long-horizon reasoning.", "method": "Coordination in SwarmSys emerges through iterative interactions among three specialized roles, Explorers, Workers, and Validators, that continuously cycle through exploration, exploitation, and validation. To enable scalable and adaptive collaboration, we integrate adaptive agent and event profiles, embedding-based probabilistic matching, and a pheromone-inspired reinforcement mechanism, supporting dynamic task allocation and self-organizing convergence without global supervision.", "result": "SwarmSys consistently outperforms baselines, improving both accuracy and reasoning stability across symbolic reasoning, research synthesis, and scientific programming tasks.", "conclusion": "Swarm-inspired coordination is a promising paradigm for scalable, robust, and adaptive multi-agent reasoning, suggesting that coordination scaling may rival model scaling in advancing LLM intelligence."}}
{"id": "2510.09669", "categories": ["cs.LG", "cs.CY", "cs.SI", "physics.soc-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09669", "abs": "https://arxiv.org/abs/2510.09669", "authors": ["Jacopo Lenti", "Lorenzo Costantini", "Ariadna Fosch", "Anna Monticelli", "David Scala", "Marco Pangallo"], "title": "Population synthesis with geographic coordinates", "comment": null, "summary": "It is increasingly important to generate synthetic populations with explicit\ncoordinates rather than coarse geographic areas, yet no established methods\nexist to achieve this. One reason is that latitude and longitude differ from\nother continuous variables, exhibiting large empty spaces and highly uneven\ndensities. To address this, we propose a population synthesis algorithm that\nfirst maps spatial coordinates into a more regular latent space using\nNormalizing Flows (NF), and then combines them with other features in a\nVariational Autoencoder (VAE) to generate synthetic populations. This approach\nalso learns the joint distribution between spatial and non-spatial features,\nexploiting spatial autocorrelations. We demonstrate the method by generating\nsynthetic homes with the same statistical properties of real homes in 121\ndatasets, corresponding to diverse geographies. We further propose an\nevaluation framework that measures both spatial accuracy and practical utility,\nwhile ensuring privacy preservation. Our results show that the NF+VAE\narchitecture outperforms popular benchmarks, including copula-based methods and\nuniform allocation within geographic areas. The ability to generate geolocated\nsynthetic populations at fine spatial resolution opens the door to applications\nrequiring detailed geography, from household responses to floods, to epidemic\nspread, evacuation planning, and transport modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u5177\u6709\u663e\u5f0f\u5750\u6807\u7684\u5408\u6210\u4eba\u7fa4\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u751f\u6210\u5177\u6709\u663e\u5f0f\u5750\u6807\u7684\u5408\u6210\u4eba\u7fa4\u7684\u6210\u719f\u65b9\u6cd5\uff0c\u56e0\u4e3a\u7ecf\u7eac\u5ea6\u4e0e\u5176\u4ed6\u8fde\u7eed\u53d8\u91cf\u4e0d\u540c\uff0c\u5b58\u5728\u5927\u91cf\u7a7a\u767d\u533a\u57df\u548c\u9ad8\u5ea6\u4e0d\u5747\u5300\u7684\u5bc6\u5ea6\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4eba\u53e3\u5408\u6210\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u9996\u5148\u4f7f\u7528 Normalizing Flows (NF) \u5c06\u7a7a\u95f4\u5750\u6807\u6620\u5c04\u5230\u66f4\u89c4\u5219\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u7136\u540e\u5c06\u5b83\u4eec\u4e0e\u53d8\u5206\u81ea\u7f16\u7801\u5668 (VAE) \u4e2d\u7684\u5176\u4ed6\u7279\u5f81\u7ed3\u5408\u4ee5\u751f\u6210\u5408\u6210\u4eba\u7fa4\u3002", "result": "\u901a\u8fc7\u751f\u6210\u5177\u6709\u4e0e 121 \u4e2a\u6570\u636e\u96c6\u4e2d\u771f\u5b9e\u623f\u5c4b\u76f8\u540c\u7edf\u8ba1\u5c5e\u6027\u7684\u5408\u6210\u623f\u5c4b\u6765\u5c55\u793a\u8be5\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u6d4b\u91cf\u7a7a\u95f4\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\uff0c\u540c\u65f6\u786e\u4fdd\u9690\u79c1\u4fdd\u62a4\u3002\u7ed3\u679c\u8868\u660e\uff0cNF+VAE \u67b6\u6784\u4f18\u4e8e\u6d41\u884c\u7684\u57fa\u51c6\u3002", "conclusion": "\u751f\u6210\u5177\u6709\u7cbe\u7ec6\u7a7a\u95f4\u5206\u8fa8\u7387\u7684\u5730\u7406\u5b9a\u4f4d\u5408\u6210\u4eba\u7fa4\u7684\u80fd\u529b\u4e3a\u9700\u8981\u8be6\u7ec6\u5730\u7406\u4fe1\u606f\u7684\u5e94\u7528\u6253\u5f00\u4e86\u5927\u95e8\uff0c\u4ece\u5bb6\u5ead\u5bf9\u6d2a\u6c34\u7684\u53cd\u5e94\u5230\u6d41\u884c\u75c5\u4f20\u64ad\u3001\u758f\u6563\u8ba1\u5212\u548c\u4ea4\u901a\u5efa\u6a21\u3002"}}
{"id": "2510.11066", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11066", "abs": "https://arxiv.org/abs/2510.11066", "authors": ["Alin Fan", "Hanqing Li", "Sihan Lu", "Jingsong Yuan", "Jiandong Zhang"], "title": "Decoupled Multimodal Fusion for User Interest Modeling in Click-Through Rate Prediction", "comment": null, "summary": "Modern industrial recommendation systems improve recommendation performance\nby integrating multimodal representations from pre-trained models into ID-based\nClick-Through Rate (CTR) prediction frameworks. However, existing approaches\ntypically adopt modality-centric modeling strategies that process ID-based and\nmultimodal embeddings independently, failing to capture fine-grained\ninteractions between content semantics and behavioral signals. In this paper,\nwe propose Decoupled Multimodal Fusion (DMF), which introduces a\nmodality-enriched modeling strategy to enable fine-grained interactions between\nID-based collaborative representations and multimodal representations for user\ninterest modeling. Specifically, we construct target-aware features to bridge\nthe semantic gap across different embedding spaces and leverage them as side\ninformation to enhance the effectiveness of user interest modeling.\nFurthermore, we design an inference-optimized attention mechanism that\ndecouples the computation of target-aware features and ID-based embeddings\nbefore the attention layer, thereby alleviating the computational bottleneck\nintroduced by incorporating target-aware features. To achieve comprehensive\nmultimodal integration, DMF combines user interest representations learned\nunder the modality-centric and modality-enriched modeling strategies. Offline\nexperiments on public and industrial datasets demonstrate the effectiveness of\nDMF. Moreover, DMF has been deployed on the product recommendation system of\nthe international e-commerce platform Lazada, achieving relative improvements\nof 5.30% in CTCVR and 7.43% in GMV with negligible computational overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u89e3\u8026\u591a\u6a21\u6001\u878d\u5408\uff08DMF\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u6355\u6349\u5185\u5bb9\u8bed\u4e49\u548c\u884c\u4e3a\u4fe1\u53f7\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u4ea4\u4e92\u3002", "method": "\u6784\u5efa\u76ee\u6807\u611f\u77e5\u7279\u5f81\u6765\u5f25\u5408\u4e0d\u540c\u5d4c\u5165\u7a7a\u95f4\u4e4b\u95f4\u7684\u8bed\u4e49\u5dee\u8ddd\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u63a8\u7406\u4f18\u5316\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728\u516c\u5f00\u548c\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u79bb\u7ebf\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86DMF\u7684\u6709\u6548\u6027\u3002\u5728Lazada\u7684\u4ea7\u54c1\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0cCTCVR\u76f8\u5bf9\u63d0\u9ad8\u4e865.30%\uff0cGMV\u76f8\u5bf9\u63d0\u9ad8\u4e867.43%\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "DMF\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u63a8\u8350\u7cfb\u7edf\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9645\u7535\u5546\u5e73\u53f0\u53d6\u5f97\u663e\u8457\u6548\u679c\u3002"}}
{"id": "2510.09790", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09790", "abs": "https://arxiv.org/abs/2510.09790", "authors": ["Michael Freenor", "Lauren Alvarez"], "title": "Steering Embedding Models with Geometric Rotation: Mapping Semantic Relationships Across Languages and Models", "comment": "9 pages, 3 Figure, 1 table, preprint", "summary": "Understanding how language and embedding models encode semantic relationships\nis fundamental to model interpretability and control. While early word\nembeddings exhibited intuitive vector arithmetic (''king'' - ''man'' +\n''woman'' = ''queen''), modern high-dimensional text representations lack\nstraightforward interpretable geometric properties. We introduce\nRotor-Invariant Shift Estimation (RISE), a geometric approach that represents\nsemantic transformations as consistent rotational operations in embedding\nspace, leveraging the manifold structure of modern language representations.\nRISE operations have the ability to operate across both languages and models\nwith high transfer of performance, suggesting the existence of analogous\ncross-lingual geometric structure. We evaluate RISE across three embedding\nmodels, three datasets, and seven morphologically diverse languages in five\nmajor language groups. Our results demonstrate that RISE consistently maps\ndiscourse-level semantic transformations with distinct grammatical features\n(e.g., negation and conditionality) across languages and models. This work\nprovides the first systematic demonstration that discourse-level semantic\ntransformations correspond to consistent geometric operations in multilingual\nembedding spaces, empirically supporting the Linear Representation Hypothesis\nat the sentence level.", "AI": {"tldr": "\u73b0\u4ee3\u9ad8\u7ef4\u6587\u672c\u8868\u793a\u7f3a\u4e4f\u76f4\u63a5\u53ef\u89e3\u91ca\u7684\u51e0\u4f55\u5c5e\u6027\uff0c\u6211\u4eec\u5f15\u5165\u4e86Rotor-Invariant Shift Estimation (RISE)\uff0c\u8fd9\u662f\u4e00\u79cd\u51e0\u4f55\u65b9\u6cd5\uff0c\u5c06\u8bed\u4e49\u8f6c\u6362\u8868\u793a\u4e3a\u5d4c\u5165\u7a7a\u95f4\u4e2d\u4e00\u81f4\u7684\u65cb\u8f6c\u64cd\u4f5c\uff0c\u5229\u7528\u4e86\u73b0\u4ee3\u8bed\u8a00\u8868\u793a\u7684\u6d41\u5f62\u7ed3\u6784\u3002", "motivation": "\u7406\u89e3\u8bed\u8a00\u548c\u5d4c\u5165\u6a21\u578b\u5982\u4f55\u7f16\u7801\u8bed\u4e49\u5173\u7cfb\u5bf9\u4e8e\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u63a7\u5236\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86Rotor-Invariant Shift Estimation (RISE)\uff0c\u4e00\u79cd\u51e0\u4f55\u65b9\u6cd5\uff0c\u5c06\u8bed\u4e49\u8f6c\u6362\u8868\u793a\u4e3a\u5d4c\u5165\u7a7a\u95f4\u4e2d\u4e00\u81f4\u7684\u65cb\u8f6c\u64cd\u4f5c\uff0c\u5229\u7528\u4e86\u73b0\u4ee3\u8bed\u8a00\u8868\u793a\u7684\u6d41\u5f62\u7ed3\u6784\u3002", "result": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0cRISE\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u6a21\u578b\u4e2d\u4e00\u81f4\u5730\u6620\u5c04\u5177\u6709\u4e0d\u540c\u8bed\u6cd5\u7279\u5f81\uff08\u4f8b\u5982\uff0c\u5426\u5b9a\u548c\u6761\u4ef6\uff09\u7684\u8bed\u7bc7\u7ea7\u8bed\u4e49\u8f6c\u6362\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u9996\u6b21\u7cfb\u7edf\u5730\u8bc1\u660e\u4e86\u8bed\u7bc7\u7ea7\u8bed\u4e49\u8f6c\u6362\u5bf9\u5e94\u4e8e\u591a\u8bed\u8a00\u5d4c\u5165\u7a7a\u95f4\u4e2d\u4e00\u81f4\u7684\u51e0\u4f55\u64cd\u4f5c\uff0c\u4ece\u7ecf\u9a8c\u4e0a\u652f\u6301\u4e86\u53e5\u5b50\u7ea7\u522b\u7684\u7ebf\u6027\u8868\u793a\u5047\u8bbe\u3002"}}
{"id": "2510.09836", "categories": ["cs.CV", "cs.CR", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.09836", "abs": "https://arxiv.org/abs/2510.09836", "authors": ["David Benavente-Rios", "Juan Ruiz Rodriguez", "Gustavo Gatica"], "title": "Exploration of Incremental Synthetic Non-Morphed Images for Single Morphing Attack Detection", "comment": "Workshop paper accepted NeurIPS 2025", "summary": "This paper investigates the use of synthetic face data to enhance\nSingle-Morphing Attack Detection (S-MAD), addressing the limitations of\navailability of large-scale datasets of bona fide images due to privacy\nconcerns. Various morphing tools and cross-dataset evaluation schemes were\nutilized to conduct this study. An incremental testing protocol was implemented\nto assess the generalization capabilities as more and more synthetic images\nwere added. The results of the experiments show that generalization can be\nimproved by carefully incorporating a controlled number of synthetic images\ninto existing datasets or by gradually adding bona fide images during training.\nHowever, indiscriminate use of synthetic data can lead to sub-optimal\nperformance. Evenmore, the use of only synthetic data (morphed and non-morphed\nimages) achieves the highest Equal Error Rate (EER), which means in operational\nscenarios the best option is not relying only on synthetic data for S-MAD.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4f7f\u7528\u5408\u6210\u9762\u90e8\u6570\u636e\u6765\u589e\u5f3a\u5355\u4eba\u8138\u878d\u5408\u653b\u51fb\u68c0\u6d4b (S-MAD)\u3002", "motivation": "\u7531\u4e8e\u9690\u79c1\u95ee\u9898\uff0c\u7f3a\u4e4f\u5927\u89c4\u6a21\u771f\u5b9e\u56fe\u50cf\u6570\u636e\u96c6\u3002", "method": "\u5229\u7528\u5404\u79cd\u878d\u5408\u5de5\u5177\u548c\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u65b9\u6848\uff0c\u5e76\u5b9e\u65bd\u589e\u91cf\u6d4b\u8bd5\u534f\u8bae\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u5c06\u53d7\u63a7\u6570\u91cf\u7684\u5408\u6210\u56fe\u50cf\u8c28\u614e\u5730\u6574\u5408\u5230\u73b0\u6709\u6570\u636e\u96c6\u4e2d\uff0c\u6216\u8005\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9010\u6b65\u6dfb\u52a0\u771f\u5b9e\u56fe\u50cf\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002\u4f46\u662f\uff0c\u4e0d\u52a0\u533a\u5206\u5730\u4f7f\u7528\u5408\u6210\u6570\u636e\u53ef\u80fd\u4f1a\u5bfc\u81f4\u6b21\u4f18\u6027\u80fd\u3002", "conclusion": "\u5728\u64cd\u4f5c\u573a\u666f\u4e2d\uff0c\u6700\u597d\u7684\u9009\u62e9\u4e0d\u662f\u4ec5\u4f9d\u9760\u5408\u6210\u6570\u636e\u8fdb\u884c S-MAD\u3002"}}
{"id": "2510.10069", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.10069", "abs": "https://arxiv.org/abs/2510.10069", "authors": ["Zeyu Ling", "Xiaodong Gu", "Jiangnan Tang", "Changqing Zou"], "title": "SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face Representation", "comment": null, "summary": "We introduce SyncLipMAE, a self-supervised pretraining framework for\ntalking-face video that learns synchronization-aware and transferable facial\ndynamics from unlabeled audio-visual streams. Our approach couples masked\nvisual modeling with cross-modal contrastive alignment and employs three\nper-frame prompt tokens that explicitly encode the essential factors of a\ntalking-face frame - identity, vocal motion (speech-synchronized facial\ndynamics), and ambient motion (audio-agnostic movements such as blinks and head\npose). The contrastive objective uses time-aligned vocal-motion and audio\ntokens as positives and misaligned pairs as negatives, driving both modalities\ninto a shared embedding space and yielding token-level audio-visual stream\nsynchronization. After pretraining, the aligned audio tokens together with the\nvisual prompt tokens (identity, vocal motion, ambient motion) form a unified\ninterface for four disparate downstream settings: (i) audio-visual stream\nsynchronization; (ii) facial emotion and head/face action recognition; (iii)\nvisual speech recognition; and (iv) visual dubbing, for which we enable\nindistinguishable audio- or video-driven control within a single model. Across\nfour task families that require distinct capabilities, SyncLipMAE achieves\nstate-of-the-art results, underscoring the effectiveness of\nsynchronization-aware, factorized self-supervised pretraining.", "AI": {"tldr": "SyncLipMAE\uff1a\u4e00\u4e2a\u7528\u4e8e\u8bf4\u8bdd\u4eba\u8138\u89c6\u9891\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u5b66\u4e60\u540c\u6b65\u611f\u77e5\u548c\u53ef\u8fc1\u79fb\u7684\u9762\u90e8\u52a8\u6001\u3002", "motivation": "\u4ece\u65e0\u6807\u7b7e\u7684\u89c6\u542c\u6d41\u4e2d\u5b66\u4e60\u540c\u6b65\u611f\u77e5\u548c\u53ef\u8fc1\u79fb\u7684\u9762\u90e8\u52a8\u6001\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u63a9\u853d\u89c6\u89c9\u5efa\u6a21\u4e0e\u8de8\u6a21\u6001\u5bf9\u6bd4\u5bf9\u9f50\u76f8\u7ed3\u5408\uff0c\u5e76\u91c7\u7528\u4e09\u4e2a\u9010\u5e27\u63d0\u793a\u4ee4\u724c\uff0c\u8fd9\u4e9b\u4ee4\u724c\u663e\u5f0f\u5730\u7f16\u7801\u4e86\u8bf4\u8bdd\u4eba\u8138\u5e27\u7684\u57fa\u672c\u56e0\u7d20\u2014\u2014\u8eab\u4efd\u3001\u58f0\u97f3\u8fd0\u52a8\uff08\u8bed\u97f3\u540c\u6b65\u7684\u9762\u90e8\u52a8\u6001\uff09\u548c\u73af\u5883\u8fd0\u52a8\uff08\u4e0e\u97f3\u9891\u65e0\u5173\u7684\u8fd0\u52a8\uff0c\u5982\u7728\u773c\u548c\u5934\u90e8\u59ff\u52bf\uff09\u3002", "result": "SyncLipMAE \u5728\u9700\u8981\u4e0d\u540c\u80fd\u529b\u7684\u56db\u4e2a\u4efb\u52a1\u7cfb\u5217\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u540c\u6b65\u611f\u77e5\u3001\u5206\u89e3\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.09670", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2510.09670", "abs": "https://arxiv.org/abs/2510.09670", "authors": ["Xinlun Cheng", "Bingzhe Chen", "Joseph Choi", "Yen T. Nguyen", "Pradeep Seshadri", "Mayank Verma", "H. S. Udaykumar", "Stephen Baek"], "title": "A physics-aware deep learning model for shear band formation around collapsing pores in shocked reactive materials", "comment": null, "summary": "Modeling shock-to-detonation phenomena in energetic materials (EMs) requires\ncapturing complex physical processes such as strong shocks, rapid changes in\nmicrostructural morphology, and nonlinear dynamics of chemical reaction fronts.\nThese processes participate in energy localization at hotspots, which initiate\nchemical energy release leading to detonation. This study addresses the\nformation of hotspots in crystalline EMs subjected to weak-to-moderate shock\nloading, which, despite its critical relevance to the safe storage and handling\nof EMs, remains underexplored compared to the well-studied strong shock\nconditions. To overcome the computational challenges associated with direct\nnumerical simulations, we advance the Physics-Aware Recurrent Convolutional\nNeural Network (PARCv2), which has been shown to be capable of predicting\nstrong shock responses in EMs. We improved the architecture of PARCv2 to\nrapidly predict shear localizations and plastic heating, which play important\nroles in the weak-to-moderate shock regime. PARCv2 is benchmarked against two\nwidely used physics-informed models, namely, Fourier neural operator and neural\nordinary differential equation; we demonstrate its superior performance in\ncapturing the spatiotemporal dynamics of shear band formation. While all models\nexhibit certain failure modes, our findings underscore the importance of\ndomain-specific considerations in developing robust AI-accelerated simulation\ntools for reactive materials.", "AI": {"tldr": "\u672c\u7814\u7a76\u5173\u6ce8\u80fd\u91cf\u6750\u6599\u5728\u5f31\u5230\u4e2d\u7b49\u51b2\u51fb\u8f7d\u8377\u4e0b\u7684\u70ed\u70b9\u5f62\u6210\uff0c\u8fd9\u5bf9\u4e8e\u80fd\u91cf\u6750\u6599\u7684\u5b89\u5168\u5b58\u50a8\u548c\u5904\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4e0e\u5f3a\u51b2\u51fb\u6761\u4ef6\u76f8\u6bd4\uff0c\u8fd9\u65b9\u9762\u7684\u7814\u7a76\u4ecd\u7136\u4e0d\u8db3\u3002", "motivation": "\u6a21\u62df\u80fd\u91cf\u6750\u6599\u4e2d\u7684\u51b2\u51fb\u5230\u7206\u8f70\u73b0\u8c61\u9700\u8981\u6355\u6349\u590d\u6742\u7684\u7269\u7406\u8fc7\u7a0b\uff0c\u4f8b\u5982\u5f3a\u51b2\u51fb\u3001\u5fae\u89c2\u7ed3\u6784\u5f62\u6001\u7684\u5feb\u901f\u53d8\u5316\u548c\u5316\u5b66\u53cd\u5e94\u524d\u6cbf\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u3002\u8fd9\u4e9b\u8fc7\u7a0b\u53c2\u4e0e\u70ed\u70b9\u7684\u80fd\u91cf \u043b\u043e\u043a\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f\uff0c\u4ece\u800c\u5f15\u53d1\u5316\u5b66\u80fd\u91ca\u653e\uff0c\u6700\u7ec8\u5bfc\u81f4\u7206\u8f70\u3002", "method": "\u6211\u4eec\u6539\u8fdb\u4e86 Physics-Aware Recurrent Convolutional Neural Network (PARCv2) \u7684\u67b6\u6784\uff0c\u4ee5\u5feb\u901f\u9884\u6d4b\u526a\u5207 \u043b\u043e\u043a\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u548c\u5851\u6027\u52a0\u70ed\uff0c\u8fd9\u5728\u5f31\u5230\u4e2d\u7b49\u51b2\u51fb\u72b6\u6001\u4e0b\u8d77\u7740\u91cd\u8981\u4f5c\u7528\u3002", "result": "PARCv2 \u5728\u6355\u6349\u526a\u5207\u5e26\u5f62\u6210\u7684\u65f6\u7a7a\u52a8\u6001\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u3002\u867d\u7136\u6240\u6709\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u67d0\u4e9b\u5931\u6548\u6a21\u5f0f\uff0c\u4f46\u6211\u4eec\u7684\u53d1\u73b0\u5f3a\u8c03\u4e86\u5728\u5f00\u53d1\u7528\u4e8e\u53cd\u5e94\u6750\u6599\u7684\u9c81\u68d2\u7684 AI \u52a0\u901f\u6a21\u62df\u5de5\u5177\u65f6\uff0c\u9886\u57df\u7279\u5b9a\u8003\u8651\u56e0\u7d20\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63a8\u8fdb\u4e86 Physics-Aware Recurrent Convolutional Neural Network (PARCv2)\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u9884\u6d4b\u5f31\u5230\u4e2d\u7b49\u51b2\u51fb\u72b6\u6001\u4e0b\u526a\u5207 \u043b\u043e\u043a\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u548c\u5851\u6027\u52a0\u70ed\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.11100", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11100", "abs": "https://arxiv.org/abs/2510.11100", "authors": ["Shuwei Chen", "Jiajun Cui", "Zhengqi Xu", "Fan Zhang", "Jiangke Fan", "Teng Zhang", "Xingxing Wang"], "title": "HoMer: Addressing Heterogeneities by Modeling Sequential and Set-wise Contexts for CTR Prediction", "comment": "10 pages, 6 figures", "summary": "Click-through rate (CTR) prediction, which models behavior sequence and\nnon-sequential features (e.g., user/item profiles or cross features) to infer\nuser interest, underpins industrial recommender systems. However, most methods\nface three forms of heterogeneity that degrade predictive performance: (i)\nFeature Heterogeneity persists when limited sequence side features provide less\ngranular interest representation compared to extensive non-sequential features,\nthereby impairing sequence modeling performance; (ii) Context Heterogeneity\narises because a user's interest in an item will be influenced by other items,\nyet point-wise prediction neglects cross-item interaction context from the\nentire item set; (iii) Architecture Heterogeneity stems from the fragmented\nintegration of specialized network modules, which compounds the model's\neffectiveness, efficiency and scalability in industrial deployments. To tackle\nthe above limitations, we propose HoMer, a Homogeneous-Oriented TransforMer for\nmodeling sequential and set-wise contexts. First, we align sequence side\nfeatures with non-sequential features for accurate sequence modeling and\nfine-grained interest representation. Second, we shift the prediction paradigm\nfrom point-wise to set-wise, facilitating cross-item interaction in a highly\nparallel manner. Third, HoMer's unified encoder-decoder architecture achieves\ndual optimization through structural simplification and shared computation,\nensuring computational efficiency while maintaining scalability with model\nsize. Without arduous modification to the prediction pipeline, HoMer\nsuccessfully scales up and outperforms our industrial baseline by 0.0099 in the\nAUC metric, and enhances online business metrics like CTR/RPM by 1.99%/2.46%.\nAdditionally, HoMer saves 27% of GPU resources via preliminary engineering\noptimization, further validating its superiority and practicality.", "AI": {"tldr": "HoMer: A Homogeneous-Oriented Transformer to address feature, context, and architecture heterogeneity in CTR prediction.", "motivation": "Existing CTR prediction methods suffer from feature, context, and architecture heterogeneity, which degrades performance.", "method": "Proposes HoMer, which aligns sequence side features, shifts to set-wise prediction, and uses a unified encoder-decoder architecture.", "result": "HoMer outperforms the industrial baseline by 0.0099 AUC, improves online business metrics (CTR/RPM) by 1.99%/2.46%, and saves 27% of GPU resources.", "conclusion": "HoMer effectively addresses the limitations of existing CTR prediction methods and demonstrates superior performance and practicality."}}
{"id": "2510.09849", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09849", "abs": "https://arxiv.org/abs/2510.09849", "authors": ["Ruizhe Zhu"], "title": "Text Prompt Injection of Vision Language Models", "comment": null, "summary": "The widespread application of large vision language models has significantly\nraised safety concerns. In this project, we investigate text prompt injection,\na simple yet effective method to mislead these models. We developed an\nalgorithm for this type of attack and demonstrated its effectiveness and\nefficiency through experiments. Compared to other attack methods, our approach\nis particularly effective for large models without high demand for\ncomputational resources.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6587\u672c\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u65e8\u5728\u8bef\u5bfc\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\u5f15\u53d1\u4e86\u5b89\u5168\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u6587\u672c\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u7b97\u6cd5\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5bf9\u4e8e\u5927\u578b\u6a21\u578b\u7279\u522b\u6709\u6548\uff0c\u4e14\u5bf9\u8ba1\u7b97\u8d44\u6e90\u7684\u9700\u6c42\u4e0d\u9ad8\u3002"}}
{"id": "2510.09848", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09848", "abs": "https://arxiv.org/abs/2510.09848", "authors": ["Peixian Liang", "Yifan Ding", "Yizhe Zhang", "Jianxu Chen", "Hao Zheng", "Hongxiao Wang", "Yejia Zhang", "Guangyu Meng", "Tim Weninger", "Michael Niemier", "X. Sharon Hu", "Danny Z Chen"], "title": "Cell Instance Segmentation: The Devil Is in the Boundaries", "comment": "Accepted at IEEE Transactions On Medical Imaging (TMI)", "summary": "State-of-the-art (SOTA) methods for cell instance segmentation are based on\ndeep learning (DL) semantic segmentation approaches, focusing on distinguishing\nforeground pixels from background pixels. In order to identify cell instances\nfrom foreground pixels (e.g., pixel clustering), most methods decompose\ninstance information into pixel-wise objectives, such as distances to\nforeground-background boundaries (distance maps), heat gradients with the\ncenter point as heat source (heat diffusion maps), and distances from the\ncenter point to foreground-background boundaries with fixed angles (star-shaped\npolygons). However, pixel-wise objectives may lose significant geometric\nproperties of the cell instances, such as shape, curvature, and convexity,\nwhich require a collection of pixels to represent. To address this challenge,\nwe present a novel pixel clustering method, called Ceb (for Cell boundaries),\nto leverage cell boundary features and labels to divide foreground pixels into\ncell instances. Starting with probability maps generated from semantic\nsegmentation, Ceb first extracts potential foreground-foreground boundaries\nwith a revised Watershed algorithm. For each boundary candidate, a boundary\nfeature representation (called boundary signature) is constructed by sampling\npixels from the current foreground-foreground boundary as well as the\nneighboring background-foreground boundaries. Next, a boundary classifier is\nused to predict its binary boundary label based on the corresponding boundary\nsignature. Finally, cell instances are obtained by dividing or merging\nneighboring regions based on the predicted boundary labels. Extensive\nexperiments on six datasets demonstrate that Ceb outperforms existing pixel\nclustering methods on semantic segmentation probability maps. Moreover, Ceb\nachieves highly competitive performance compared to SOTA cell instance\nsegmentation methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u50cf\u7d20\u805a\u7c7b\u65b9\u6cd5Ceb\uff0c\u5229\u7528\u7ec6\u80de\u8fb9\u754c\u7279\u5f81\u548c\u6807\u7b7e\u5c06\u524d\u666f\u50cf\u7d20\u5212\u5206\u4e3a\u7ec6\u80de\u5b9e\u4f8b\u3002", "motivation": "\u73b0\u6709\u7684\u7ec6\u80de\u5b9e\u4f8b\u5206\u5272\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u50cf\u7d20\u7ea7\u7684\u76ee\u6807\uff0c\u53ef\u80fd\u4e22\u5931\u7ec6\u80de\u5b9e\u4f8b\u7684\u51e0\u4f55\u5c5e\u6027\uff0c\u5982\u5f62\u72b6\u3001\u66f2\u7387\u548c\u51f8\u6027\u3002", "method": "Ceb\u9996\u5148\u4ece\u8bed\u4e49\u5206\u5272\u751f\u6210\u7684\u6982\u7387\u56fe\u4e2d\u63d0\u53d6\u6f5c\u5728\u7684\u524d\u666f-\u524d\u666f\u8fb9\u754c\uff0c\u7136\u540e\u6784\u5efa\u8fb9\u754c\u7279\u5f81\u8868\u793a\uff08\u79f0\u4e3a\u8fb9\u754c\u7b7e\u540d\uff09\uff0c\u5e76\u4f7f\u7528\u8fb9\u754c\u5206\u7c7b\u5668\u9884\u6d4b\u5176\u4e8c\u5143\u8fb9\u754c\u6807\u7b7e\uff0c\u6700\u540e\u57fa\u4e8e\u9884\u6d4b\u7684\u8fb9\u754c\u6807\u7b7e\u5206\u5272\u6216\u5408\u5e76\u76f8\u90bb\u533a\u57df\u4ee5\u83b7\u5f97\u7ec6\u80de\u5b9e\u4f8b\u3002", "result": "\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCeb\u4f18\u4e8e\u73b0\u6709\u7684\u50cf\u7d20\u805a\u7c7b\u65b9\u6cd5\uff0c\u5e76\u4e14\u4e0eSOTA\u7ec6\u80de\u5b9e\u4f8b\u5206\u5272\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5177\u6709\u9ad8\u5ea6\u7684\u7ade\u4e89\u529b\u3002", "conclusion": "Ceb\u662f\u4e00\u79cd\u6709\u6548\u7684\u7ec6\u80de\u5b9e\u4f8b\u5206\u5272\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u7ec6\u80de\u8fb9\u754c\u7279\u5f81\u6765\u63d0\u9ad8\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2510.10074", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10074", "abs": "https://arxiv.org/abs/2510.10074", "authors": ["Jiayi Mao", "Liqun Li", "Yanjie Gao", "Zegang Peng", "Shilin He", "Chaoyun Zhang", "Si Qin", "Samia Khalid", "Qingwei Lin", "Saravan Rajmohan", "Sitaram Lanka", "Dongmei Zhang"], "title": "Agentic Troubleshooting Guide Automation for Incident Management", "comment": null, "summary": "Effective incident management in large-scale IT systems relies on\ntroubleshooting guides (TSGs), but their manual execution is slow and\nerror-prone. While recent advances in LLMs offer promise for automating\nincident management tasks, existing LLM-based solutions lack specialized\nsupport for several key challenges, including managing TSG quality issues,\ninterpreting complex control flow, handling data-intensive queries, and\nexploiting execution parallelism. We first conducted an empirical study on 92\nreal-world TSGs, and, guided by our findings, we present StepFly, a novel\nend-to-end agentic framework for troubleshooting guide automation. Our approach\nfeatures a three-stage workflow: the first stage provides a comprehensive guide\ntogether with a tool, TSG Mentor, to assist SREs in improving TSG quality; the\nsecond stage performs offline preprocessing using LLMs to extract structured\nexecution DAGs from unstructured TSGs and to create dedicated Query Preparation\nPlugins (QPPs); and the third stage executes online using a DAG-guided\nscheduler-executor framework with a memory system to guarantee correct workflow\nand support parallel execution of independent steps. Our empirical evaluation\non a collection of real-world TSGs and incidents demonstrates that StepFly\nachieves a ~94% success rate on GPT-4.1, outperforming baselines with less time\nand token consumption. Furthermore, it achieves a remarkable execution time\nreduction of 32.9% to 70.4% for parallelizable TSGs.", "AI": {"tldr": "StepFly\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u5316\u6545\u969c\u6392\u9664\u6307\u5357\uff08TSG\uff09\u7684\u7aef\u5230\u7aefagent\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u5de5\u4f5c\u6d41\u7a0b\uff0c\u89e3\u51b3TSG\u8d28\u91cf\u95ee\u9898\u3001\u590d\u6742\u63a7\u5236\u6d41\u3001\u6570\u636e\u5bc6\u96c6\u578b\u67e5\u8be2\u548c\u5e76\u884c\u6267\u884c\u7b49\u6311\u6218\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u7684TSG\u548c\u4e8b\u4ef6\u96c6\u5408\u4e0a\u5b9e\u73b0\u4e86\u7ea694%\u7684\u6210\u529f\u7387\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u6267\u884c\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u89e3\u51b3\u65b9\u6848\u7f3a\u4e4f\u5bf9TSG\u8d28\u91cf\u7ba1\u7406\u3001\u590d\u6742\u63a7\u5236\u6d41\u89e3\u91ca\u3001\u6570\u636e\u5bc6\u96c6\u578b\u67e5\u8be2\u5904\u7406\u548c\u5e76\u884c\u6267\u884c\u5229\u7528\u7b49\u5173\u952e\u6311\u6218\u7684\u4e13\u95e8\u652f\u6301\uff0c\u5bfc\u81f4\u5927\u89c4\u6a21IT\u7cfb\u7edf\u4e2d\u4e8b\u4ef6\u7ba1\u7406\u7684\u6548\u7387\u4f4e\u4e0b\u3002", "method": "StepFly\u91c7\u7528\u4e09\u9636\u6bb5\u5de5\u4f5c\u6d41\u7a0b\uff1aTSG Mentor\u8f85\u52a9SRE\u63d0\u9ad8TSG\u8d28\u91cf\uff1b\u79bb\u7ebf\u9884\u5904\u7406\u63d0\u53d6\u7ed3\u6784\u5316\u6267\u884cDAG\u5e76\u521b\u5efaQPP\uff1b\u5728\u7ebf\u4f7f\u7528DAG\u5f15\u5bfc\u7684\u8c03\u5ea6\u5668-\u6267\u884c\u5668\u6846\u67b6\u548c\u5185\u5b58\u7cfb\u7edf\uff0c\u4fdd\u8bc1\u5de5\u4f5c\u6d41\u6b63\u786e\u5e76\u652f\u6301\u5e76\u884c\u6267\u884c\u3002", "result": "StepFly\u5728GPT-4.1\u4e0a\u5b9e\u73b0\u4e86\u7ea694%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u51cf\u5c11\u4e86\u65f6\u95f4\u548ctoken\u6d88\u8017\u3002\u5bf9\u4e8e\u53ef\u5e76\u884c\u5316\u7684TSG\uff0c\u6267\u884c\u65f6\u95f4\u51cf\u5c11\u4e8632.9%\u523070.4%\u3002", "conclusion": "StepFly\u80fd\u591f\u6709\u6548\u89e3\u51b3\u73b0\u6709LLM\u5728\u81ea\u52a8\u5316TSG\u6267\u884c\u4e2d\u9762\u4e34\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u4ef6\u7ba1\u7406\u7684\u6548\u7387\u548c\u6210\u529f\u7387\u3002"}}
{"id": "2510.09676", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09676", "abs": "https://arxiv.org/abs/2510.09676", "authors": ["Shayan Mohajer Hamidi", "En-Hui Yang", "Ben Liang"], "title": "Coupled Data and Measurement Space Dynamics for Enhanced Diffusion Posterior Sampling", "comment": null, "summary": "Inverse problems, where the goal is to recover an unknown signal from noisy\nor incomplete measurements, are central to applications in medical imaging,\nremote sensing, and computational biology. Diffusion models have recently\nemerged as powerful priors for solving such problems. However, existing methods\neither rely on projection-based techniques that enforce measurement consistency\nthrough heuristic updates, or they approximate the likelihood $p(\\boldsymbol{y}\n\\mid \\boldsymbol{x})$, often resulting in artifacts and instability under\ncomplex or high-noise conditions. To address these limitations, we propose a\nnovel framework called \\emph{coupled data and measurement space diffusion\nposterior sampling} (C-DPS), which eliminates the need for constraint tuning or\nlikelihood approximation. C-DPS introduces a forward stochastic process in the\nmeasurement space $\\{\\boldsymbol{y}_t\\}$, evolving in parallel with the\ndata-space diffusion $\\{\\boldsymbol{x}_t\\}$, which enables the derivation of a\nclosed-form posterior $p(\\boldsymbol{x}_{t-1} \\mid \\boldsymbol{x}_t,\n\\boldsymbol{y}_{t-1})$. This coupling allows for accurate and recursive\nsampling based on a well-defined posterior distribution. Empirical results\ndemonstrate that C-DPS consistently outperforms existing baselines, both\nqualitatively and quantitatively, across multiple inverse problem benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9006\u95ee\u9898\u6c42\u89e3\u6846\u67b6C-DPS\uff0c\u5b83\u4e0d\u9700\u8981\u7ea6\u675f\u8c03\u6574\u6216\u4f3c\u7136\u8fd1\u4f3c\uff0c\u901a\u8fc7\u5728\u6570\u636e\u7a7a\u95f4\u548c\u6d4b\u91cf\u7a7a\u95f4\u4e2d\u8026\u5408\u6269\u6563\u8fc7\u7a0b\uff0c\u5bfc\u51fa\u4e00\u4e2a\u95ed\u5408\u5f62\u5f0f\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u5b9e\u73b0\u7cbe\u786e\u548c\u9012\u5f52\u91c7\u6837\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9006\u95ee\u9898\u6c42\u89e3\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u6295\u5f71\u7684\u6280\u672f\u6216\u8fd1\u4f3c\u4f3c\u7136\u51fd\u6570\uff0c\u5bfc\u81f4\u4f2a\u5f71\u548c\u5728\u9ad8\u566a\u58f0\u6761\u4ef6\u4e0b\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51faC-DPS\u6846\u67b6\uff0c\u5728\u6d4b\u91cf\u7a7a\u95f4\u5f15\u5165\u524d\u5411\u968f\u673a\u8fc7\u7a0b\uff0c\u4e0e\u6570\u636e\u7a7a\u95f4\u6269\u6563\u8fc7\u7a0b\u5e76\u884c\u6f14\u5316\uff0c\u5bfc\u51fa\u95ed\u5408\u5f62\u5f0f\u7684\u540e\u9a8c\u5206\u5e03\u3002", "result": "\u5728\u591a\u4e2a\u9006\u95ee\u9898\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cC-DPS\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "C-DPS\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u9006\u95ee\u9898\uff0c\u4e14\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.11122", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11122", "abs": "https://arxiv.org/abs/2510.11122", "authors": ["Tingqiao Xu", "Shaowei Yao", "Chenhe Dong", "Yiming Jin", "Zerui Huang", "Dan Ou", "Haihong Tang"], "title": "DyKnow-RAG: Dynamic Knowledge Utilization Reinforcement Framework for Noisy Retrieval-Augmented Generation in E-commerce Search Relevance", "comment": null, "summary": "Accurately modeling query-item relevance drives e-commerce ranking, yet\nlong-tail, knowledge-heavy, and fast-evolving queries exceed parametric LLM\ncoverage. External context (reviews, attribute encyclopedias, UGC) can help but\nis noisy, and single-pass latency and cost forbid any clean-then-summarize\nstep. The model must, per query, judge relevance and decide whether to use,\npartially use, or ignore the context. DyKnow-RAG is a dynamic noisy-RAG\nframework built on Group Relative Policy Optimization. It trains two rollout\ngroups (no external context vs a single retrieved chunk) and applies\nposterior-driven inter-group advantage scaling that adaptively reweights their\ncontributions by the per-query correctness gap. This teaches when to trust\nretrieval versus fall back to parametric knowledge, without process labels,\nvalue networks, or extra inference passes, preserving single-pass, single-chunk\ndeployment under production latency. Training combines: (1) supervised\ninitialization with a structured rationale that explicitly records the\ncontext-usage decision; (2) an RL pool prioritized by SFT uncertainty to focus\nwhere context choice is most consequential; and (3) an optional lightweight DPO\nwarm start to stabilize with-context calibration. Under a unified\nretrieval/index and fixed latency budget, DyKnow-RAG outperforms SFT, DPO, and\nvanilla GRPO in offline tests, and delivers consistent lifts on GSB, Query\nGoodrate, and Item Goodrate in Taobao A/B testing. It is deployed in Taobao's\nproduction relevance system, serving live traffic. To our knowledge, it is\namong the first single-pass RAG solutions for e-commerce relevance, turning\nnoisy external signals into reliable gains without added online complexity.", "AI": {"tldr": "DyKnow-RAG is a dynamic noisy-RAG framework that adaptively reweights the contributions of parametric knowledge and external context based on per-query correctness gap, outperforming existing methods in e-commerce ranking.", "motivation": "Accurately modeling query-item relevance in e-commerce is challenging due to long-tail queries and the limitations of parametric LLMs. External context can help but is noisy, and latency constraints prevent clean-then-summarize approaches.", "method": "DyKnow-RAG uses Group Relative Policy Optimization to train two rollout groups (with and without external context) and adaptively reweights their contributions based on a posterior-driven inter-group advantage scaling. It is trained with supervised initialization, RL pool prioritized by SFT uncertainty and optional DPO warm start.", "result": "DyKnow-RAG outperforms SFT, DPO, and vanilla GRPO in offline tests and delivers consistent lifts on GSB, Query Goodrate, and Item Goodrate in Taobao A/B testing.", "conclusion": "DyKnow-RAG is a single-pass RAG solution for e-commerce relevance that effectively utilizes noisy external signals without adding online complexity, and is deployed in Taobao's production relevance system."}}
{"id": "2510.09854", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09854", "abs": "https://arxiv.org/abs/2510.09854", "authors": ["Kaiwen Shi", "Zheyuan Zhang", "Zhengqing Yuan", "Keerthiram Murugesan", "Vincent Galass", "Chuxu Zhang", "Yanfang Ye"], "title": "NG-Router: Graph-Supervised Multi-Agent Collaboration for Nutrition Question Answering", "comment": null, "summary": "Diet plays a central role in human health, and Nutrition Question Answering\n(QA) offers a promising path toward personalized dietary guidance and the\nprevention of diet-related chronic diseases. However, existing methods face two\nfundamental challenges: the limited reasoning capacity of single-agent systems\nand the complexity of designing effective multi-agent architectures, as well as\ncontextual overload that hinders accurate decision-making. We introduce\nNutritional-Graph Router (NG-Router), a novel framework that formulates\nnutritional QA as a supervised, knowledge-graph-guided multi-agent\ncollaboration problem. NG-Router integrates agent nodes into heterogeneous\nknowledge graphs and employs a graph neural network to learn task-aware routing\ndistributions over agents, leveraging soft supervision derived from empirical\nagent performance. To further address contextual overload, we propose a\ngradient-based subgraph retrieval mechanism that identifies salient evidence\nduring training, thereby enhancing multi-hop and relational reasoning.\nExtensive experiments across multiple benchmarks and backbone models\ndemonstrate that NG-Router consistently outperforms both single-agent and\nensemble baselines, offering a principled approach to domain-aware multi-agent\nreasoning for complex nutritional health tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8425\u517b\u95ee\u7b54\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u5f15\u5bfc\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6765\u89e3\u51b3\u4e2a\u6027\u5316\u996e\u98df\u6307\u5bfc\u548c\u9884\u9632\u6162\u6027\u75c5\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u63a8\u7406\u80fd\u529b\u548c\u591a\u667a\u80fd\u4f53\u67b6\u6784\u8bbe\u8ba1\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5e76\u4e14\u9762\u4e34\u4e0a\u4e0b\u6587\u8fc7\u8f7d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u8425\u517b\u56fe\u8def\u7531\uff08NG-Router\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u667a\u80fd\u4f53\u8282\u70b9\u96c6\u6210\u5230\u5f02\u6784\u77e5\u8bc6\u56fe\u4e2d\uff0c\u5e76\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u4efb\u52a1\u611f\u77e5\u7684\u667a\u80fd\u4f53\u8def\u7531\u5206\u5e03\uff0c\u5229\u7528\u6765\u81ea\u7ecf\u9a8c\u667a\u80fd\u4f53\u6027\u80fd\u7684\u8f6f\u76d1\u7763\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u89e3\u51b3\u4e0a\u4e0b\u6587\u8fc7\u8f7d\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u5b50\u56fe\u68c0\u7d22\u673a\u5236\uff0c\u8be5\u673a\u5236\u5728\u8bad\u7ec3\u671f\u95f4\u8bc6\u522b\u663e\u7740\u8bc1\u636e\uff0c\u4ece\u800c\u589e\u5f3a\u591a\u8df3\u548c\u5173\u7cfb\u63a8\u7406\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u548c\u9aa8\u5e72\u6a21\u578b\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cNG-Router\u59cb\u7ec8\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u548c\u96c6\u6210\u57fa\u7ebf\u3002", "conclusion": "NG-Router\u4e3a\u590d\u6742\u7684\u8425\u517b\u5065\u5eb7\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u9886\u57df\u7684\u77e5\u60c5\u591a\u667a\u80fd\u4f53\u63a8\u7406\u65b9\u6cd5\u3002"}}
{"id": "2510.09867", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09867", "abs": "https://arxiv.org/abs/2510.09867", "authors": ["Zhi Chen", "Xin Yu", "Xiaohui Tao", "Yan Li", "Zi Huang"], "title": "Cluster-Aware Prompt Ensemble Learning for Few-Shot Vision-Language Model Adaptation", "comment": "Accepted to the journal Pattern Recognition in 2025", "summary": "Vision-language models (VLMs) such as CLIP achieve zero-shot transfer across\nvarious tasks by pre-training on numerous image-text pairs. These models often\nbenefit from using an ensemble of context prompts to represent a class. Despite\nbeing effective, conventional prompt ensembling that averages textual features\nof context prompts often yields suboptimal results. This is because feature\naveraging shifts the class centroids away from the true class distribution. To\naddress this issue, we propose the Cluster-Aware Prompt Ensemble Learning\n(CAPEL) framework, which preserves the cluster nature of context prompts. CAPEL\nclassifies images into one of several class clusters, each represented by a\ndistinct prompt. Instead of ensembling prompts in the feature space, we perform\nensembling in the classification logits space, aligning better with the visual\nfeature distribution. To further optimize prompt fine-tuning while maintaining\ncluster-specific discriminative power, we introduce a cluster-preserving\nregularization term. This ensures that prompts remain distinct and specialized\nfor different clusters, preventing collapse into a uniform direction.\nAdditionally, we integrate an adaptive prompt weighting technique to\ndynamically adjust the attention weights for flawed or ambiguous prompts,\nensuring robust performance across diverse datasets and tasks.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u63d0\u793a\u96c6\u6210\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u63d0\u793a\u96c6\u6210\u65b9\u6cd5\u7684\u6b21\u4f18\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u63d0\u793a\u96c6\u6210\u65b9\u6cd5\u901a\u8fc7\u5e73\u5747\u6587\u672c\u7279\u5f81\u6765\u8868\u793a\u4e00\u4e2a\u7c7b\u522b\uff0c\u4f46\u8fd9\u4f1a\u5c06\u7c7b\u522b\u4e2d\u5fc3\u4ece\u771f\u5b9e\u7684\u7c7b\u522b\u5206\u5e03\u4e2d\u79fb\u5f00\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u805a\u7c7b\u611f\u77e5\u63d0\u793a\u96c6\u6210\u5b66\u4e60 (CAPEL) \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4fdd\u7559\u4e86\u4e0a\u4e0b\u6587\u63d0\u793a\u7684\u805a\u7c7b\u6027\u8d28\u3002CAPEL \u5c06\u56fe\u50cf\u5206\u7c7b\u5230\u591a\u4e2a\u7c7b\u522b\u7c07\u4e2d\u7684\u4e00\u4e2a\uff0c\u6bcf\u4e2a\u7c07\u7531\u4e0d\u540c\u7684\u63d0\u793a\u8868\u793a\u3002\u8be5\u6846\u67b6\u5728\u5206\u7c7b logits \u7a7a\u95f4\u4e2d\u6267\u884c\u96c6\u6210\uff0c\u5e76\u5f15\u5165\u4e86\u805a\u7c7b\u4fdd\u6301\u6b63\u5219\u5316\u9879\u548c\u81ea\u9002\u5e94\u63d0\u793a\u6743\u91cd\u6280\u672f\u3002", "result": "\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u7684\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u63d0\u793a\u96c6\u6210\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2510.10117", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10117", "abs": "https://arxiv.org/abs/2510.10117", "authors": ["Yunxiang Mo", "Tianshi Zheng", "Qing Zong", "Jiayu Liu", "Baixuan Xu", "Yauwai Yim", "Chunkit Chan", "Jiaxin Bai", "Yangqiu Song"], "title": "DixitWorld: Evaluating Multimodal Abductive Reasoning in Vision-Language Models with Multi-Agent Dixit Gameplay", "comment": "EMNLP 2025 Wordplay (Spotlight)", "summary": "Multimodal abductive reasoning--the generation and selection of explanatory\nhypotheses from partial observations--is a cornerstone of intelligence. Current\nevaluations of this ability in vision-language models (VLMs) are largely\nconfined to static, single-agent tasks. Inspired by Dixit, we introduce\nDixitWorld, a comprehensive evaluation suite designed to deconstruct this\nchallenge. DIXITWORLD features two core components: DixitArena, a dynamic,\nmulti-agent environment that evaluates both hypothesis generation (a\n\"storyteller\" crafting cryptic clues) and hypothesis selection (\"listeners\"\nchoosing the target image from decoys) under imperfect information; and\nDixitBench, a static QA benchmark that isolates the listener's task for\nefficient, controlled evaluation. Results from DixitArena reveal distinct,\nrole-dependent behaviors: smaller open-source models often excel as creative\nstorytellers, producing imaginative yet less discriminative clues, whereas\nlarger proprietary models demonstrate superior overall performance,\nparticularly as listeners. Performance on DixitBench strongly correlates with\nlistener results in DixitArena, validating it as a reliable proxy for\nhypothesis selection. Our findings reveal a key trade-off between generative\ncreativity and discriminative understanding in multimodal abductive reasoning,\na central challenge for developing more balanced and capable vision-language\nagents.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u7684\u7efc\u5408\u8bc4\u4f30\u5957\u4ef6DixitWorld\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u591a\u6a21\u6001\u6eaf\u56e0\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u76ee\u524d\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u8fd9\u79cd\u80fd\u529b\u7684\u8bc4\u4f30\u4e3b\u8981\u5c40\u9650\u4e8e\u9759\u6001\u3001\u5355\u667a\u80fd\u4f53\u4efb\u52a1\uff0c\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\u3002", "method": "DixitWorld\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1aDixitArena\uff08\u4e00\u4e2a\u52a8\u6001\u3001\u591a\u667a\u80fd\u4f53\u73af\u5883\uff0c\u8bc4\u4f30\u5047\u8bbe\u751f\u6210\u548c\u9009\u62e9\uff09\u548cDixitBench\uff08\u4e00\u4e2a\u9759\u6001QA\u57fa\u51c6\uff0c\u7528\u4e8e\u6709\u6548\u3001\u53d7\u63a7\u5730\u8bc4\u4f30\u542c\u8005\u7684\u4efb\u52a1\uff09\u3002", "result": "DixitArena\u7684\u7ed3\u679c\u8868\u660e\uff0c\u8f83\u5c0f\u7684\u5f00\u6e90\u6a21\u578b\u901a\u5e38\u64c5\u957f\u4f5c\u4e3a\u521b\u9020\u6027\u7684\u8bb2\u6545\u4e8b\u8005\uff0c\u800c\u8f83\u5927\u7684\u4e13\u6709\u6a21\u578b\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6574\u4f53\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u542c\u8005\u65b9\u9762\u3002DixitBench\u7684\u6027\u80fd\u4e0eDixitArena\u4e2d\u7684\u542c\u8005\u7ed3\u679c\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u591a\u6a21\u6001\u6eaf\u56e0\u63a8\u7406\u4e2d\u751f\u6210\u521b\u9020\u529b\u548c\u5224\u522b\u7406\u89e3\u4e4b\u95f4\u7684\u5173\u952e\u6743\u8861\uff0c\u8fd9\u662f\u5f00\u53d1\u66f4\u5e73\u8861\u548c\u66f4\u6709\u80fd\u529b\u7684\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2510.09684", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.09684", "abs": "https://arxiv.org/abs/2510.09684", "authors": ["Chris Engh", "P. M. Aronow"], "title": "Using LLMs to Directly Guess Conditional Expectations Can Improve Efficiency in Causal Estimation", "comment": null, "summary": "We propose a simple yet effective use of LLM-powered AI tools to improve\ncausal estimation. In double machine learning, the accuracy of causal estimates\nof the effect of a treatment on an outcome in the presence of a\nhigh-dimensional confounder depends on the performance of estimators of\nconditional expectation functions. We show that predictions made by generative\nmodels trained on historical data can be used to improve the performance of\nthese estimators relative to approaches that solely rely on adjusting for\nembeddings extracted from these models. We argue that the historical knowledge\nand reasoning capacities associated with these generative models can help\novercome curse-of-dimensionality problems in causal inference problems. We\nconsider a case study using a small dataset of online jewelry auctions, and\ndemonstrate that inclusion of LLM-generated guesses as predictors can improve\nefficiency in estimation.", "AI": {"tldr": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684AI\u5de5\u5177\u6765\u6539\u8fdb\u56e0\u679c\u4f30\u8ba1\u3002", "motivation": "\u5728\u9ad8\u7ef4\u6df7\u6742\u56e0\u7d20\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\uff0c\u6cbb\u7597\u5bf9\u7ed3\u679c\u5f71\u54cd\u7684\u56e0\u679c\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u53d6\u51b3\u4e8e\u6761\u4ef6\u671f\u671b\u51fd\u6570\u4f30\u8ba1\u5668\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u5728\u5386\u53f2\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u751f\u6210\u6a21\u578b\u6240\u505a\u7684\u9884\u6d4b\u6765\u63d0\u9ad8\u8fd9\u4e9b\u4f30\u8ba1\u5668\u7684\u6027\u80fd\uff0c\u76f8\u5bf9\u4e8e\u90a3\u4e9b\u4ec5\u4f9d\u8d56\u4e8e\u4ece\u8fd9\u4e9b\u6a21\u578b\u4e2d\u63d0\u53d6\u7684\u5d4c\u5165\u8fdb\u884c\u8c03\u6574\u7684\u65b9\u6cd5\u3002", "result": "\u5728\u5728\u7ebf\u73e0\u5b9d\u62cd\u5356\u7684\u5c0f\u578b\u6570\u636e\u96c6\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u8bc1\u660e\u4e86\u5305\u542bLLM\u751f\u6210\u7684\u731c\u6d4b\u4f5c\u4e3a\u9884\u6d4b\u53d8\u91cf\u53ef\u4ee5\u63d0\u9ad8\u4f30\u8ba1\u6548\u7387\u3002", "conclusion": "\u5386\u53f2\u77e5\u8bc6\u548c\u4e0e\u8fd9\u4e9b\u751f\u6210\u6a21\u578b\u76f8\u5173\u7684\u63a8\u7406\u80fd\u529b\u53ef\u4ee5\u5e2e\u52a9\u514b\u670d\u56e0\u679c\u63a8\u7406\u95ee\u9898\u4e2d\u7684\u7ef4\u5ea6\u8bc5\u5492\u95ee\u9898\u3002"}}
{"id": "2510.11317", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11317", "abs": "https://arxiv.org/abs/2510.11317", "authors": ["Chen Gao", "Zixin Zhao", "Lv Shao", "Tong Liu"], "title": "Next Interest Flow: A Generative Pre-training Paradigm for Recommender Systems by Modeling All-domain Movelines", "comment": null, "summary": "Click-Through Rate (CTR) prediction, a cornerstone of modern recommender\nsystems, has been dominated by discriminative models that react to past user\nbehavior rather than proactively modeling user intent. Existing generative\nparadigms attempt to address this but suffer from critical limitations: Large\nLanguage Model (LLM) based methods create a semantic mismatch by forcing\ne-commerce signals into a linguistic space, while ID-based generation is\nconstrained by item memorization and cold-start issues. To overcome these\nlimitations, we propose a novel generative pre-training paradigm. Our model\nlearns to predict the Next Interest Flow, a dense vector sequence representing\na user's future intent, while simultaneously modeling its internal Interest\nDiversity and Interest Evolution Velocity to ensure the representation is both\nrich and coherent. However, this two-stage approach introduces a critical\nobjective mismatch between the generative and discriminative stages. We resolve\nthis via a bidirectional alignment strategy, which harmonizes the two stages\nthrough cross-stage weight initialization and a dynamic Semantic Alignment\nModule for fine-tuning. Additionally, we enhance the underlying discriminative\nmodel with a Temporal Sequential Pairwise (TSP) mechanism to better capture\ntemporal causality. We present the All-domain Moveline Evolution Network\n(AMEN), a unified framework implementing our entire pipeline. Extensive offline\nexperiments validate AMEN's superiority over strong baselines, and a\nlarge-scale online A/B test demonstrates its significant real-world impact,\ndelivering substantial improvements in key business metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u5f0f\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u7528\u4e8e\u70b9\u51fb\u7387 (CTR) \u9884\u6d4b\uff0c\u901a\u8fc7\u9884\u6d4b Next Interest Flow \u6765\u5efa\u6a21\u7528\u6237\u610f\u56fe\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u8981\u4e48\u53cd\u5e94\u5f0f\u5730\u4f9d\u8d56\u8fc7\u53bb\u884c\u4e3a\uff0c\u8981\u4e48\u5b58\u5728\u8bed\u4e49\u4e0d\u5339\u914d\u6216\u8bb0\u5fc6/\u51b7\u542f\u52a8\u95ee\u9898\u3002", "method": "\u6a21\u578b\u5b66\u4e60\u9884\u6d4b Next Interest Flow\uff0c\u540c\u65f6\u5efa\u6a21\u5174\u8da3\u591a\u6837\u6027\u548c\u6f14\u53d8\u901f\u5ea6\u3002\u901a\u8fc7\u53cc\u5411\u5bf9\u9f50\u7b56\u7565\u548c\u65f6\u95f4\u5e8f\u5217\u914d\u5bf9\u673a\u5236\u6765\u89e3\u51b3\u76ee\u6807\u4e0d\u5339\u914d\u548c\u6355\u6349\u65f6\u95f4\u56e0\u679c\u5173\u7cfb\u3002", "result": "\u79bb\u7ebf\u5b9e\u9a8c\u548c\u5927\u89c4\u6a21\u5728\u7ebf A/B \u6d4b\u8bd5\u9a8c\u8bc1\u4e86 AMEN \u7684\u4f18\u8d8a\u6027\u548c\u5b9e\u9645\u6548\u679c\u3002", "conclusion": "AMEN \u663e\u8457\u63d0\u5347\u4e86\u5173\u952e\u4e1a\u52a1\u6307\u6807\uff0c\u5177\u6709\u91cd\u8981\u7684\u73b0\u5b9e\u610f\u4e49\u3002"}}
{"id": "2510.09869", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09869", "abs": "https://arxiv.org/abs/2510.09869", "authors": ["Sil Hamilton", "Matthew Wilkens", "Andrew Piper"], "title": "NarraBench: A Comprehensive Framework for Narrative Benchmarking", "comment": null, "summary": "We present NarraBench, a theory-informed taxonomy of narrative-understanding\ntasks, as well as an associated survey of 78 existing benchmarks in the area.\nWe find significant need for new evaluations covering aspects of narrative\nunderstanding that are either overlooked in current work or are poorly aligned\nwith existing metrics. Specifically, we estimate that only 27% of narrative\ntasks are well captured by existing benchmarks, and we note that some areas --\nincluding narrative events, style, perspective, and revelation -- are nearly\nabsent from current evaluations. We also note the need for increased\ndevelopment of benchmarks capable of assessing constitutively subjective and\nperspectival aspects of narrative, that is, aspects for which there is\ngenerally no single correct answer. Our taxonomy, survey, and methodology are\nof value to NLP researchers seeking to test LLM narrative understanding.", "AI": {"tldr": "NarraBench\uff1a\u4e00\u4e2a\u5173\u4e8e\u53d9\u4e8b\u7406\u89e3\u4efb\u52a1\u7684\u7406\u8bba\u5206\u7c7b\uff0c\u4ee5\u53ca\u5bf9\u8be5\u9886\u57df 78 \u4e2a\u73b0\u6709\u57fa\u51c6\u7684\u8c03\u67e5\u3002", "motivation": "\u53d1\u73b0\u5f53\u524d\u7684\u7814\u7a76\u5728\u53d9\u4e8b\u7406\u89e3\u65b9\u9762\u5b58\u5728\u663e\u8457\u9700\u6c42\uff0c\u8fd9\u4e9b\u9700\u6c42\u8981\u4e48\u88ab\u5f53\u524d\u7684\u5de5\u4f5c\u5ffd\u89c6\uff0c\u8981\u4e48\u4e0e\u73b0\u6709\u7684\u6307\u6807 \u043f\u043b\u043e\u0445\u043e \u5bf9\u9f50\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5173\u4e8e\u53d9\u4e8b\u7406\u89e3\u4efb\u52a1\u7684\u7406\u8bba\u5206\u7c7b\uff0c\u5e76\u8c03\u67e5\u4e86\u8be5\u9886\u57df\u4e2d 78 \u4e2a\u73b0\u6709\u7684\u57fa\u51c6\u3002", "result": "\u53ea\u6709 27% \u7684\u53d9\u4e8b\u4efb\u52a1\u80fd\u88ab\u73b0\u6709\u7684\u57fa\u51c6\u5f88\u597d\u5730\u6355\u83b7\uff0c\u5e76\u4e14\u4e00\u4e9b\u9886\u57df\uff08\u5305\u62ec\u53d9\u4e8b\u4e8b\u4ef6\u3001\u98ce\u683c\u3001\u89c6\u89d2\u548c\u63ed\u793a\uff09\u5728\u5f53\u524d\u7684\u8bc4\u4f30\u4e2d\u51e0\u4e4e\u7f3a\u5931\u3002\u9700\u8981\u589e\u52a0\u80fd\u591f\u8bc4\u4f30\u53d9\u4e8b\u7684\u6784\u6210\u6027\u4e3b\u89c2\u548c\u89c6\u89d2\u65b9\u9762\u7684\u57fa\u51c6\u7684\u5f00\u53d1\u3002", "conclusion": "\u6211\u4eec\u7684\u5206\u7c7b\u3001\u8c03\u67e5\u548c\u65b9\u6cd5\u8bba\u5bf9\u5bfb\u6c42\u6d4b\u8bd5 LLM \u53d9\u4e8b\u7406\u89e3\u7684 NLP \u7814\u7a76\u4eba\u5458\u5177\u6709\u4ef7\u503c\u3002"}}
{"id": "2510.09878", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09878", "abs": "https://arxiv.org/abs/2510.09878", "authors": ["Milad Khanchi", "Maria Amer", "Charalambos Poullis"], "title": "Fast Self-Supervised depth and mask aware Association for Multi-Object Tracking", "comment": null, "summary": "Multi-object tracking (MOT) methods often rely on Intersection-over-Union\n(IoU) for association. However, this becomes unreliable when objects are\nsimilar or occluded. Also, computing IoU for segmentation masks is\ncomputationally expensive. In this work, we use segmentation masks to capture\nobject shapes, but we do not compute segmentation IoU. Instead, we fuse depth\nand mask features and pass them through a compact encoder trained\nself-supervised. This encoder produces stable object representations, which we\nuse as an additional similarity cue alongside bounding box IoU and\nre-identification features for matching. We obtain depth maps from a zero-shot\ndepth estimator and object masks from a promptable visual segmentation model to\nobtain fine-grained spatial cues. Our MOT method is the first to use the\nself-supervised encoder to refine segmentation masks without computing masks\nIoU. MOT can be divided into joint detection-ReID (JDR) and\ntracking-by-detection (TBD) models. The latter are computationally more\nefficient. Experiments of our TBD method on challenging benchmarks with\nnon-linear motion, occlusion, and crowded scenes, such as SportsMOT and\nDanceTrack, show that our method outperforms the TBD state-of-the-art on most\nmetrics, while achieving competitive performance on simpler benchmarks with\nlinear motion, such as MOT17.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\uff08MOT\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u878d\u5408\u4e86\u6df1\u5ea6\u548cmask\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u81ea\u76d1\u7763\u7f16\u7801\u5668\u751f\u6210\u7a33\u5b9a\u7684\u5bf9\u8c61\u8868\u793a\uff0c\u4ee5\u63d0\u9ad8\u5339\u914d\u7cbe\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u5bf9\u8c61\u76f8\u4f3c\u6216\u906e\u6321\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "\u73b0\u6709\u7684MOT\u65b9\u6cd5\u4f9d\u8d56\u4e8eIoU\u8fdb\u884c\u5173\u8054\uff0c\u4f46\u5728\u5bf9\u8c61\u76f8\u4f3c\u6216\u906e\u6321\u65f6\u53d8\u5f97\u4e0d\u53ef\u9760\uff0c\u4e14\u5206\u5272mask\u7684IoU\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u4f7f\u7528\u5206\u5272mask\u6355\u6349\u5bf9\u8c61\u5f62\u72b6\uff0c\u4f46\u4e0d\u8ba1\u7b97\u5206\u5272IoU\uff0c\u800c\u662f\u878d\u5408\u6df1\u5ea6\u548cmask\u7279\u5f81\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u7f16\u7801\u5668\u751f\u6210\u7a33\u5b9a\u7684\u5bf9\u8c61\u8868\u793a\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u989d\u5916\u7684\u76f8\u4f3c\u6027\u7ebf\u7d22\u4e0ebounding box IoU\u548cre-identification\u7279\u5f81\u4e00\u8d77\u7528\u4e8e\u5339\u914d\u3002\u4f7f\u7528zero-shot\u6df1\u5ea6\u4f30\u8ba1\u5668\u548cpromptable visual segmentation\u6a21\u578b\u83b7\u53d6\u6df1\u5ea6\u56fe\u548c\u5bf9\u8c61mask\u3002", "result": "\u5728\u5177\u6709\u975e\u7ebf\u6027\u8fd0\u52a8\u3001\u906e\u6321\u548c\u62e5\u6324\u573a\u666f\u7684benchmark\uff08\u5982SportsMOT\u548cDanceTrack\uff09\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8eTBD state-of-the-art\uff0c\u5e76\u5728\u5177\u6709\u7ebf\u6027\u8fd0\u52a8\u7684benchmark\uff08\u5982MOT17\uff09\u4e0a\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5MOT\u65b9\u6cd5\u662f\u7b2c\u4e00\u4e2a\u4f7f\u7528\u81ea\u76d1\u7763\u7f16\u7801\u5668\u6765\u4f18\u5316\u5206\u5272mask\u800c\u65e0\u9700\u8ba1\u7b97mask IoU\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2510.10135", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10135", "abs": "https://arxiv.org/abs/2510.10135", "authors": ["Zhongsheng Wang", "Ming Lin", "Zhedong Lin", "Yaser Shakib", "Qian Liu", "Jiamou Liu"], "title": "CharCom: Composable Identity Control for Multi-Character Story Illustration", "comment": "Accepted by ACM MMAsia 2025", "summary": "Ensuring character identity consistency across varying prompts remains a\nfundamental limitation in diffusion-based text-to-image generation. We propose\nCharCom, a modular and parameter-efficient framework that achieves\ncharacter-consistent story illustration through composable LoRA adapters,\nenabling efficient per-character customization without retraining the base\nmodel. Built on a frozen diffusion backbone, CharCom dynamically composes\nadapters at inference using prompt-aware control. Experiments on multi-scene\nnarratives demonstrate that CharCom significantly enhances character fidelity,\nsemantic alignment, and temporal coherence. It remains robust in crowded scenes\nand enables scalable multi-character generation with minimal overhead, making\nit well-suited for real-world applications such as story illustration and\nanimation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCharCom\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6269\u6563\u6a21\u578b\u4e2d\u5b9e\u73b0\u89d2\u8272\u4e00\u81f4\u7684\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u786e\u4fdd\u5728\u4e0d\u540c\u7684\u63d0\u793a\u4e0b\u89d2\u8272\u8eab\u4efd\u7684\u4e00\u81f4\u6027\u662f\u57fa\u4e8e\u6269\u6563\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u9650\u5236\u3002", "method": "CharCom\u901a\u8fc7\u53ef\u7ec4\u5408\u7684LoRA\u9002\u914d\u5668\u5b9e\u73b0\u89d2\u8272\u4e00\u81f4\u7684\u6545\u4e8b\u56fe\u4f8b\uff0c\u4ece\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u57fa\u672c\u6a21\u578b\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u7684\u5355\u89d2\u8272\u5b9a\u5236\u3002CharCom\u5efa\u7acb\u5728\u51bb\u7ed3\u7684\u6269\u6563\u9aa8\u5e72\u7f51\u4e0a\uff0c\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u63d0\u793a\u611f\u77e5\u63a7\u5236\u52a8\u6001\u5730\u7ec4\u5408\u9002\u914d\u5668\u3002", "result": "\u5728\u591a\u573a\u666f\u53d9\u4e8b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCharCom\u663e\u8457\u63d0\u9ad8\u4e86\u89d2\u8272\u4fdd\u771f\u5ea6\u3001\u8bed\u4e49\u5bf9\u9f50\u548c\u65f6\u95f4\u8fde\u8d2f\u6027\u3002\u5b83\u5728\u62e5\u6324\u7684\u573a\u666f\u4e2d\u4fdd\u6301\u7a33\u5065\uff0c\u5e76\u80fd\u591f\u4ee5\u6700\u5c0f\u7684\u5f00\u9500\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u591a\u89d2\u8272\u751f\u6210\u3002", "conclusion": "CharCom\u6846\u67b6\u9002\u7528\u4e8e\u6545\u4e8b\u63d2\u56fe\u548c\u52a8\u753b\u7b49\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2510.09685", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NA", "math.NA", "A.1; I.2; I.4"], "pdf": "https://arxiv.org/pdf/2510.09685", "abs": "https://arxiv.org/abs/2510.09685", "authors": ["Yongshuai Liu", "Lianfang Wang", "Kuilin Qin", "Qinghua Zhang", "Faqiang Wang", "Li Cui", "Jun Liu", "Yuping Duan", "Tieyong Zeng"], "title": "Deep Neural Networks Inspired by Differential Equations", "comment": "35 Pages, 3 figures", "summary": "Deep learning has become a pivotal technology in fields such as computer\nvision, scientific computing, and dynamical systems, significantly advancing\nthese disciplines. However, neural Networks persistently face challenges\nrelated to theoretical understanding, interpretability, and generalization. To\naddress these issues, researchers are increasingly adopting a differential\nequations perspective to propose a unified theoretical framework and systematic\ndesign methodologies for neural networks. In this paper, we provide an\nextensive review of deep neural network architectures and dynamic modeling\nmethods inspired by differential equations. We specifically examine deep neural\nnetwork models and deterministic dynamical network constructs based on ordinary\ndifferential equations (ODEs), as well as regularization techniques and\nstochastic dynamical network models informed by stochastic differential\nequations (SDEs). We present numerical comparisons of these models to\nillustrate their characteristics and performance. Finally, we explore promising\nresearch directions in integrating differential equations with deep learning to\noffer new insights for developing intelligent computational methods that boast\nenhanced interpretability and generalization capabilities.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u53d7\u5fae\u5206\u65b9\u7a0b\u542f\u53d1\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u548c\u52a8\u6001\u5efa\u6a21\u65b9\u6cd5\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u79d1\u5b66\u8ba1\u7b97\u548c\u52a8\u6001\u7cfb\u7edf\u7b49\u9886\u57df\u5df2\u6210\u4e3a\u5173\u952e\u6280\u672f\uff0c\u4f46\u795e\u7ecf\u7f51\u7edc\u5728\u7406\u8bba\u7406\u89e3\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u65b9\u9762\u4ecd\u7136\u9762\u4e34\u6311\u6218\u3002", "method": "\u672c\u6587\u8003\u5bdf\u4e86\u57fa\u4e8e\u5e38\u5fae\u5206\u65b9\u7a0b (ODE) \u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u548c\u786e\u5b9a\u6027\u52a8\u6001\u7f51\u7edc\u7ed3\u6784\uff0c\u4ee5\u53ca\u57fa\u4e8e\u968f\u673a\u5fae\u5206\u65b9\u7a0b (SDE) \u7684\u6b63\u5219\u5316\u6280\u672f\u548c\u968f\u673a\u52a8\u6001\u7f51\u7edc\u6a21\u578b\u3002\u5e76\u5bf9\u8fd9\u4e9b\u6a21\u578b\u8fdb\u884c\u4e86\u6570\u503c\u6bd4\u8f83\uff0c\u4ee5\u8bf4\u660e\u5b83\u4eec\u7684\u7279\u6027\u548c\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u6570\u503c\u6bd4\u8f83\u5c55\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u7684\u7279\u6027\u548c\u6027\u80fd\u3002", "conclusion": "\u63a2\u8ba8\u4e86\u5c06\u5fae\u5206\u65b9\u7a0b\u4e0e\u6df1\u5ea6\u5b66\u4e60\u76f8\u7ed3\u5408\u7684\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u4e3a\u5f00\u53d1\u5177\u6709\u589e\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u667a\u80fd\u8ba1\u7b97\u65b9\u6cd5\u63d0\u4f9b\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2510.11323", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11323", "abs": "https://arxiv.org/abs/2510.11323", "authors": ["Zhe Wang", "Yaming Yang", "Ziyu Guan", "Bin Tong", "Rui Wang", "Wei Zhao", "Hongbo Deng"], "title": "Dynamic Network-Based Two-Stage Time Series Forecasting for Affiliate Marketing", "comment": null, "summary": "In recent years, affiliate marketing has emerged as a revenue-sharing\nstrategy where merchants collaborate with promoters to promote their products.\nIt not only increases product exposure but also allows promoters to earn a\ncommission. This paper addresses the pivotal yet under-explored challenge in\naffiliate marketing: accurately assessing and predicting the contributions of\npromoters in product promotion. We design a novel metric for evaluating the\nindirect contributions of the promoter, called propagation scale.\nUnfortunately, existing time series forecasting techniques fail to deliver\naccurate predictions due to the propagation scale being influenced by multiple\nfactors and the inherent complexities arising from dynamic scenarios. To\naddress this issue, we decouple the network structure from the node signals and\npropose a two-stage solution: initially, the basic self-sales and network\nstructure prediction are conducted separately, followed by the synthesis of the\npropagation scale. Specifically, we design a graph convolution encoding scheme\nbased on descendant neighbors and incorporate hypergraph convolution to\nefficiently capture complex promotional dynamics. Additionally, three auxiliary\ntasks are employed: self-sales prediction for base estimations, descendant\nprediction to synthesize propagation scale, and promoter activation prediction\nto mitigate high volatility issues. Extensive offline experiments on\nlarge-scale industrial datasets validate the superiority of our method. We\nfurther deploy our model on Alimama platform with over $100,000$ promoters,\nachieving a $9.29\\%$ improvement in GMV and a $5.89\\%$ increase in sales\nvolume.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u63a8\u5e7f\u8005\u8d21\u732e\u7684\u6307\u6807\uff0c\u79f0\u4e3a\u4f20\u64ad\u89c4\u6a21\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u89e3\u51b3\u65b9\u6848\u6765\u9884\u6d4b\u5b83\u3002", "motivation": "\u73b0\u6709\u7684\u8054\u76df\u8425\u9500\u4e2d\uff0c\u51c6\u786e\u8bc4\u4f30\u548c\u9884\u6d4b\u63a8\u5e7f\u8005\u5728\u4ea7\u54c1\u63a8\u5e7f\u4e2d\u7684\u8d21\u732e\u662f\u4e00\u4e2a\u5173\u952e\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u7f51\u7edc\u7ed3\u6784\u4e0e\u8282\u70b9\u4fe1\u53f7\u89e3\u8026\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u89e3\u51b3\u65b9\u6848\uff1a\u9996\u5148\u5206\u522b\u8fdb\u884c\u57fa\u672c\u7684\u81ea\u9500\u548c\u7f51\u7edc\u7ed3\u6784\u9884\u6d4b\uff0c\u7136\u540e\u7efc\u5408\u4f20\u64ad\u89c4\u6a21\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u540e\u4ee3\u90bb\u5c45\u7684\u56fe\u5377\u79ef\u7f16\u7801\u65b9\u6848\uff0c\u5e76\u7ed3\u5408\u8d85\u56fe\u5377\u79ef\u6765\u6709\u6548\u5730\u6355\u83b7\u590d\u6742\u7684\u4fc3\u9500\u52a8\u6001\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u4e86\u4e09\u4e2a\u8f85\u52a9\u4efb\u52a1\uff1a\u81ea\u9500\u9884\u6d4b\u7528\u4e8e\u57fa\u672c\u4f30\u8ba1\uff0c\u540e\u4ee3\u9884\u6d4b\u7528\u4e8e\u5408\u6210\u4f20\u64ad\u89c4\u6a21\uff0c\u4ee5\u53ca\u542f\u52a8\u8005\u6fc0\u6d3b\u9884\u6d4b\u4ee5\u51cf\u8f7b\u9ad8\u6ce2\u52a8\u6027\u95ee\u9898\u3002", "result": "\u5728\u5927\u578b\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u79bb\u7ebf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002\u8be5\u6a21\u578b\u5df2\u90e8\u7f72\u5728\u62e5\u6709\u8d85\u8fc7 10 \u4e07\u7f8e\u5143\u542f\u52a8\u8005\u7684 Alimama \u5e73\u53f0\u4e0a\uff0cGMV \u63d0\u9ad8\u4e86 9.29%\uff0c\u9500\u91cf\u63d0\u9ad8\u4e86 5.89%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u8bc4\u4f30\u548c\u9884\u6d4b\u63a8\u5e7f\u8005\u5728\u4ea7\u54c1\u63a8\u5e7f\u4e2d\u7684\u8d21\u732e\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6548\u679c\u3002"}}
{"id": "2510.09871", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09871", "abs": "https://arxiv.org/abs/2510.09871", "authors": ["Nafiseh Nikeghbal", "Amir Hossein Kargaran", "Jana Diesner"], "title": "CoBia: Constructed Conversations Can Trigger Otherwise Concealed Societal Biases in LLMs", "comment": "EMNLP 2025 (Oral)", "summary": "Improvements in model construction, including fortified safety guardrails,\nallow Large language models (LLMs) to increasingly pass standard safety checks.\nHowever, LLMs sometimes slip into revealing harmful behavior, such as\nexpressing racist viewpoints, during conversations. To analyze this\nsystematically, we introduce CoBia, a suite of lightweight adversarial attacks\nthat allow us to refine the scope of conditions under which LLMs depart from\nnormative or ethical behavior in conversations. CoBia creates a constructed\nconversation where the model utters a biased claim about a social group. We\nthen evaluate whether the model can recover from the fabricated bias claim and\nreject biased follow-up questions. We evaluate 11 open-source as well as\nproprietary LLMs for their outputs related to six socio-demographic categories\nthat are relevant to individual safety and fair treatment, i.e., gender, race,\nreligion, nationality, sex orientation, and others. Our evaluation is based on\nestablished LLM-based bias metrics, and we compare the results against human\njudgments to scope out the LLMs' reliability and alignment. The results suggest\nthat purposefully constructed conversations reliably reveal bias amplification\nand that LLMs often fail to reject biased follow-up questions during dialogue.\nThis form of stress-testing highlights deeply embedded biases that can be\nsurfaced through interaction. Code and artifacts are available at\nhttps://github.com/nafisenik/CoBia.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aCoBia\u7684\u8f7b\u91cf\u7ea7\u5bf9\u6297\u653b\u51fb\u5957\u4ef6\uff0c\u7528\u4e8e\u7cfb\u7edf\u6027\u5730\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728\u5bf9\u8bdd\u4e2d\u504f\u79bb\u89c4\u8303\u6216\u4f26\u7406\u884c\u4e3a\u7684\u60c5\u51b5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728\u5bf9\u8bdd\u4e2d\u5076\u5c14\u4f1a\u8868\u73b0\u51fa\u6709\u5bb3\u884c\u4e3a\uff0c\u4f8b\u5982\u8868\u8fbe\u79cd\u65cf\u4e3b\u4e49\u89c2\u70b9\u3002\u4e3a\u4e86\u7cfb\u7edf\u5730\u5206\u6790\u8fd9\u79cd\u60c5\u51b5\u3002", "method": "\u8bba\u6587\u4f7f\u7528CoBia\u521b\u5efa\u6784\u9020\u7684\u5bf9\u8bdd\uff0c\u5176\u4e2d\u6a21\u578b\u4f1a\u8bf4\u51fa\u5173\u4e8e\u67d0\u4e2a\u793e\u4f1a\u7fa4\u4f53\u7684\u6709\u504f\u89c1\u7684\u8a00\u8bba\u3002\u7136\u540e\uff0c\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u53ef\u4ee5\u4ece\u634f\u9020\u7684\u504f\u89c1\u58f0\u660e\u4e2d\u6062\u590d\u5e76\u62d2\u7edd\u6709\u504f\u89c1\u7684\u540e\u7eed\u95ee\u9898\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u6709\u76ee\u7684\u5730\u6784\u5efa\u7684\u5bf9\u8bdd\u80fd\u591f\u53ef\u9760\u5730\u63ed\u793a\u504f\u89c1\u653e\u5927\uff0c\u5e76\u4e14LLM\u5728\u5bf9\u8bdd\u8fc7\u7a0b\u4e2d\u901a\u5e38\u65e0\u6cd5\u62d2\u7edd\u6709\u504f\u89c1\u7684\u540e\u7eed\u95ee\u9898\u3002", "conclusion": "\u538b\u529b\u6d4b\u8bd5\u7a81\u51fa\u4e86\u53ef\u4ee5\u901a\u8fc7\u4ea4\u4e92\u6d6e\u51fa\u6c34\u9762\u7684\u6df1\u5c42\u5d4c\u5165\u7684\u504f\u89c1\u3002"}}
{"id": "2510.09879", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09879", "abs": "https://arxiv.org/abs/2510.09879", "authors": ["Shreshth Saini", "Alan C. Bovik", "Neil Birkbeck", "Yilin Wang", "Balu Adsumilli"], "title": "CHUG: Crowdsourced User-Generated HDR Video Quality Dataset", "comment": null, "summary": "High Dynamic Range (HDR) videos enhance visual experiences with superior\nbrightness, contrast, and color depth. The surge of User-Generated Content\n(UGC) on platforms like YouTube and TikTok introduces unique challenges for HDR\nvideo quality assessment (VQA) due to diverse capture conditions, editing\nartifacts, and compression distortions. Existing HDR-VQA datasets primarily\nfocus on professionally generated content (PGC), leaving a gap in understanding\nreal-world UGC-HDR degradations. To address this, we introduce CHUG:\nCrowdsourced User-Generated HDR Video Quality Dataset, the first large-scale\nsubjective study on UGC-HDR quality. CHUG comprises 856 UGC-HDR source videos,\ntranscoded across multiple resolutions and bitrates to simulate real-world\nscenarios, totaling 5,992 videos. A large-scale study via Amazon Mechanical\nTurk collected 211,848 perceptual ratings. CHUG provides a benchmark for\nanalyzing UGC-specific distortions in HDR videos. We anticipate CHUG will\nadvance No-Reference (NR) HDR-VQA research by offering a large-scale, diverse,\nand real-world UGC dataset. The dataset is publicly available at:\nhttps://shreshthsaini.github.io/CHUG/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCHUG\u7684\u5927\u89c4\u6a21\u7528\u6237\u751f\u6210HDR\u89c6\u9891\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u6709HDR-VQA\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u4e13\u4e1a\u751f\u6210\u5185\u5bb9\uff08PGC\uff09\u7684\u95ee\u9898\uff0c\u586b\u8865\u4e86\u7406\u89e3\u771f\u5b9e\u4e16\u754cUGC-HDR\u964d\u7ea7\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709HDR-VQA\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u4e13\u4e1a\u751f\u6210\u5185\u5bb9\uff08PGC\uff09\uff0c\u7f3a\u4e4f\u5bf9\u771f\u5b9e\u4e16\u754cUGC-HDR\u964d\u7ea7\u7684\u7406\u89e3\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b856\u4e2aUGC-HDR\u6e90\u89c6\u9891\u7684\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u5206\u8fa8\u7387\u548c\u6bd4\u7279\u7387\u8fdb\u884c\u8f6c\u7801\uff0c\u6a21\u62df\u771f\u5b9e\u573a\u666f\uff0c\u5171\u8ba15,992\u4e2a\u89c6\u9891\u3002\u901a\u8fc7Amazon Mechanical Turk\u6536\u96c6\u4e86211,848\u4e2a\u611f\u77e5\u8bc4\u7ea7\u3002", "result": "CHUG\u6570\u636e\u96c6\u4e3a\u5206\u6790HDR\u89c6\u9891\u4e2dUGC\u7279\u5b9a\u5931\u771f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u51c6\u3002", "conclusion": "CHUG\u6570\u636e\u96c6\u7684\u53d1\u5e03\u9884\u8ba1\u5c06\u901a\u8fc7\u63d0\u4f9b\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u548c\u771f\u5b9e\u7684UGC\u6570\u636e\u96c6\u6765\u63a8\u8fdb\u65e0\u53c2\u8003\uff08NR\uff09HDR-VQA\u7814\u7a76\u3002"}}
{"id": "2510.10168", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10168", "abs": "https://arxiv.org/abs/2510.10168", "authors": ["Chengqian Gao", "Haonan Li", "Taylor W. Killian", "Jianshu She", "Renxi Wang", "Liqun Ma", "Zhoujun Cheng", "Shibo Hao", "Zhiqiang Xu"], "title": "Concise Reasoning in the Lens of Lagrangian Optimization", "comment": null, "summary": "Concise reasoning in large language models seeks to generate only essential\nintermediate steps needed to arrive at a final answer, thereby alleviating\nissues of overthinking. Most proposed approaches hinge on carefully\nhand-crafted heuristics, struggling to balance concision with performance,\noften failing to adapt across domains and model scales. In this work, we\naddress these challenges by introducing a principled and pragmatic strategy,\nperformance-aware length updating (PALU). As a principled algorithm, PALU\nformulates concise reasoning as a constrained optimization problem, minimizing\nresponse length subject to a performance constraint, and then applies\nLagrangian optimization to convert it into a tractable unconstrained problem.\nAs a pragmatic solution, PALU streamlines complicated update rules through\nthree approximations: (i) estimating performance with off-policy rollouts, (ii)\ntruncating the Lagrange multiplier to two extremes, and (iii) replacing\ngradient-based updates with quantile-driven length adjustments. PALU reduces\noutput length by 65% while improving accuracy by 15% when applied to\nDeepSeek-Distill-Qwen-1.5B, averaged over five benchmarks, outperforming a\nrange of alternative methods. Furthermore, PALU is demonstrated to adapt across\nboth domain (logic, STEM and math) and model scale (1.5B, 7B, 14B) entrenching\nthe algorithm as a practical and effective concise reasoning approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6027\u80fd\u611f\u77e5\u957f\u5ea6\u66f4\u65b0\uff08PALU\uff09\u7684\u7b80\u6d01\u63a8\u7406\u7b56\u7565\uff0c\u65e8\u5728\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u624b\u5de5\u8bbe\u8ba1\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u96be\u4ee5\u5728\u7b80\u6d01\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4e14\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u9886\u57df\u548c\u6a21\u578b\u89c4\u6a21\u3002", "method": "\u5c06\u7b80\u6d01\u63a8\u7406\u5efa\u6a21\u4e3a\u4e00\u4e2a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u6700\u5c0f\u5316\u54cd\u5e94\u957f\u5ea6\uff0c\u540c\u65f6\u6ee1\u8db3\u6027\u80fd\u7ea6\u675f\uff0c\u5e76\u5e94\u7528\u62c9\u683c\u6717\u65e5\u4f18\u5316\u5c06\u5176\u8f6c\u6362\u4e3a\u6613\u4e8e\u5904\u7406\u7684\u65e0\u7ea6\u675f\u95ee\u9898\u3002\u901a\u8fc7\u4e09\u4e2a\u8fd1\u4f3c\u7b80\u5316\u66f4\u65b0\u89c4\u5219\uff1a\u4f7f\u7528\u975e\u7b56\u7565rollout\u4f30\u8ba1\u6027\u80fd\uff0c\u5c06\u62c9\u683c\u6717\u65e5\u4e58\u6570\u622a\u65ad\u4e3a\u4e24\u4e2a\u6781\u7aef\uff0c\u5e76\u7528\u5206\u4f4d\u6570\u9a71\u52a8\u7684\u957f\u5ea6\u8c03\u6574\u4ee3\u66ff\u57fa\u4e8e\u68af\u5ea6\u7684\u66f4\u65b0\u3002", "result": "\u5728DeepSeek-Distill-Qwen-1.5B\u4e0a\u5e94\u7528PALU\uff0c\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8f93\u51fa\u957f\u5ea6\u51cf\u5c11\u4e8665%\uff0c\u540c\u65f6\u51c6\u786e\u7387\u63d0\u9ad8\u4e8615%\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002PALU\u8fd8\u8bc1\u660e\u4e86\u53ef\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u9886\u57df\uff08\u903b\u8f91\u3001STEM\u548c\u6570\u5b66\uff09\u548c\u6a21\u578b\u89c4\u6a21\uff081.5B\u30017B\u300114B\uff09\u3002", "conclusion": "PALU\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u6709\u6548\u7684\u7b80\u6d01\u63a8\u7406\u65b9\u6cd5\u3002"}}
{"id": "2510.09687", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09687", "abs": "https://arxiv.org/abs/2510.09687", "authors": ["Stanis\u0142aw Pawlak"], "title": "On the Occurence of Critical Learning Periods in Neural Networks", "comment": "8 pages, 8 figures", "summary": "This study delves into the plasticity of neural networks, offering empirical\nsupport for the notion that critical learning periods and warm-starting\nperformance loss can be avoided through simple adjustments to learning\nhyperparameters. The critical learning phenomenon emerges when training is\ninitiated with deficit data. Subsequently, after numerous deficit epochs, the\nnetwork's plasticity wanes, impeding its capacity to achieve parity in accuracy\nwith models trained from scratch, even when extensive clean data training\nfollows deficit epochs. Building upon seminal research introducing critical\nlearning periods, we replicate key findings and broaden the experimental scope\nof the main experiment from the original work. In addition, we consider a\nwarm-starting approach and show that it can be seen as a form of deficit\npretraining. In particular, we demonstrate that these problems can be averted\nby employing a cyclic learning rate schedule. Our findings not only impact\nneural network training practices but also establish a vital link between\ncritical learning periods and ongoing research on warm-starting neural network\ntraining.", "AI": {"tldr": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u8c03\u6574\u5b66\u4e60\u8d85\u53c2\u6570\u53ef\u4ee5\u907f\u514d\u795e\u7ecf\u7f51\u7edc\u7684\u5173\u952e\u5b66\u4e60\u671f\u548cwarm-starting\u6027\u80fd\u635f\u5931\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u7684\u5173\u952e\u5b66\u4e60\u671f\u548cwarm-starting\u6027\u80fd\u635f\u5931\u95ee\u9898\u3002", "method": "\u8be5\u7814\u7a76\u901a\u8fc7\u5b9e\u9a8c\uff0c\u590d\u5236\u4e86\u5173\u952e\u5b66\u4e60\u671f\u7684\u4e3b\u8981\u53d1\u73b0\uff0c\u5e76\u6269\u5927\u4e86\u5b9e\u9a8c\u8303\u56f4\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u8003\u8651\u4e86\u4e00\u79cdwarm-starting\u65b9\u6cd5\uff0c\u5e76\u8868\u660e\u5b83\u53ef\u4ee5\u88ab\u770b\u4f5c\u662f\u4e00\u79cd deficit \u9884\u8bad\u7ec3\u7684\u5f62\u5f0f\u3002\u8be5\u7814\u7a76\u91c7\u7528\u5faa\u73af\u5b66\u4e60\u7387\u8ba1\u5212\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u91c7\u7528\u5faa\u73af\u5b66\u4e60\u7387\u8ba1\u5212\uff0c\u53ef\u4ee5\u907f\u514d\u5173\u952e\u5b66\u4e60\u671f\u548cwarm-starting\u6027\u80fd\u635f\u5931\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e0d\u4ec5\u5f71\u54cd\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u5b9e\u8df5\uff0c\u800c\u4e14\u5728\u5173\u952e\u5b66\u4e60\u671f\u548cwarm-starting\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7684\u6301\u7eed\u7814\u7a76\u4e4b\u95f4\u5efa\u7acb\u4e86\u91cd\u8981\u7684\u8054\u7cfb\u3002"}}
{"id": "2510.11394", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11394", "abs": "https://arxiv.org/abs/2510.11394", "authors": ["Haosheng Qian", "Yixing Fan", "Jiafeng Guo", "Ruqing Zhang", "Qi Chen", "Dawei Yin", "Xueqi Cheng"], "title": "VeriCite: Towards Reliable Citations in Retrieval-Augmented Generation via Rigorous Verification", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a crucial approach for\nenhancing the responses of large language models (LLMs) with external knowledge\nsources. Despite the impressive performance in complex question-answering\ntasks, RAG still struggles with hallucinations. Attributing RAG-generated\ncontent through in-line citations has demonstrated potential in reducing\nhallucinations and facilitating human verification. Existing citation\ngeneration methods primarily rely on either fine-tuning the generator or\nemploying post-processing approaches for citation matching. However, the former\napproach demands substantial annotated data and computational resources, while\nthe latter often encounters difficulties in managing multiple citations and\nfrequently produces suboptimal results. In this paper, we introduce a novel\nframework, called VeriCite, designed to rigorously validate supporting evidence\nand enhance answer attribution. Specifically, VeriCite breaks down into a\nthree-stage generation: 1) The initial answer generation first generates a\nresponse based on all available contexts and has its claims verified through\nthe NLI model; 2) the supporting evidence selection assesses the utility of\neach document and extracts useful supporting evidences; 3) the final answer\nrefinement integrates the initial response and collected evidences to produce\nthe final, refined answer.We conduct experiments across five open-source LLMs\nand four datasets, demonstrating that VeriCite can significantly improve\ncitation quality while maintaining the correctness of the answers.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a VeriCite \u7684\u65b0\u6846\u67b6\uff0c\u65e8\u5728\u9a8c\u8bc1\u652f\u6301\u6027\u8bc1\u636e\u5e76\u52a0\u5f3a\u7b54\u6848\u5f52\u5c5e\uff0c\u4ee5\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u3002", "motivation": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG) \u5728\u5229\u7528\u5916\u90e8\u77e5\u8bc6\u6e90\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u54cd\u5e94\u65b9\u9762\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4ecd\u7136\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\u3002\u5185\u8054\u5f15\u7528 RAG \u751f\u6210\u7684\u5185\u5bb9\u5df2\u663e\u793a\u51fa\u51cf\u5c11\u5e7b\u89c9\u548c\u4fc3\u8fdb\u4eba\u5de5\u9a8c\u8bc1\u7684\u6f5c\u529b\u3002\u73b0\u6709\u7684\u5f15\u6587\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5fae\u8c03\u751f\u6210\u5668\u6216\u91c7\u7528\u540e\u5904\u7406\u65b9\u6cd5\u8fdb\u884c\u5f15\u6587\u5339\u914d\uff0c\u4f46\u662f\u524d\u8005\u9700\u8981\u5927\u91cf\u7684\u6ce8\u91ca\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u800c\u540e\u8005\u5728\u7ba1\u7406\u591a\u4e2a\u5f15\u6587\u65f6\u7ecf\u5e38\u9047\u5230\u56f0\u96be\uff0c\u5e76\u4e14\u7ecf\u5e38\u4ea7\u751f\u6b21\u4f18\u7ed3\u679c\u3002", "method": "VeriCite \u5206\u89e3\u4e3a\u4e09\u4e2a\u9636\u6bb5\u7684\u751f\u6210\uff1a1) \u521d\u59cb\u7b54\u6848\u751f\u6210\uff1b2) \u652f\u6301\u8bc1\u636e\u9009\u62e9\uff1b3) \u6700\u7ec8\u7b54\u6848\u7ec6\u5316\u3002", "result": "\u5728\u4e94\u4e2a\u5f00\u6e90 LLM \u548c\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cVeriCite \u53ef\u4ee5\u663e\u7740\u63d0\u9ad8\u5f15\u6587\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u7b54\u6848\u7684\u6b63\u786e\u6027\u3002", "conclusion": "VeriCite \u662f\u4e00\u79cd\u7528\u4e8e\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u9a8c\u8bc1\u652f\u6301\u6027\u8bc1\u636e\u5e76\u52a0\u5f3a\u7b54\u6848\u5f52\u5c5e\u6765\u63d0\u9ad8\u5f15\u6587\u8d28\u91cf\u3002"}}
{"id": "2510.09882", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09882", "abs": "https://arxiv.org/abs/2510.09882", "authors": ["Vishal Anand", "Milad Alshomary", "Kathleen McKeown"], "title": "iBERT: Interpretable Style Embeddings via Sense Decomposition", "comment": null, "summary": "We present iBERT (interpretable-BERT), an encoder to produce inherently\ninterpretable and controllable embeddings - designed to modularize and expose\nthe discriminative cues present in language, such as stylistic and semantic\nstructure. Each input token is represented as a sparse, non-negative mixture\nover k context-independent sense vectors, which can be pooled into sentence\nembeddings or used directly at the token level. This enables modular control\nover representation, before any decoding or downstream use.\n  To demonstrate our model's interpretability, we evaluate it on a suite of\nstyle-focused tasks. On the STEL benchmark, it improves style representation\neffectiveness by ~8 points over SBERT-style baselines, while maintaining\ncompetitive performance on authorship verification. Because each embedding is a\nstructured composition of interpretable senses, we highlight how specific style\nattributes - such as emoji use, formality, or misspelling can be assigned to\nspecific sense vectors. While our experiments center on style, iBERT is not\nlimited to stylistic modeling. Its structural modularity is designed to\ninterpretably decompose whichever discriminative signals are present in the\ndata - enabling generalization even when supervision blends stylistic and\nsemantic factors.", "AI": {"tldr": "iBERT: interpretable-BERT creates interpretable embeddings by representing tokens as mixtures of sense vectors, enabling control over representation.", "motivation": "To create interpretable and controllable embeddings that expose discriminative cues in language, such as stylistic and semantic structure.", "method": "Represent each token as a sparse, non-negative mixture over context-independent sense vectors.", "result": "Improved style representation effectiveness by ~8 points on the STEL benchmark compared to SBERT baselines, while maintaining competitive performance on authorship verification. Specific style attributes can be assigned to specific sense vectors.", "conclusion": "iBERT's structural modularity enables interpretable decomposition of discriminative signals, allowing for generalization even when supervision blends stylistic and semantic factors."}}
{"id": "2510.09880", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09880", "abs": "https://arxiv.org/abs/2510.09880", "authors": ["Minkwan Kim", "Changwoon Choi", "Young Min Kim"], "title": "Geometry-Aware Scene Configurations for Novel View Synthesis", "comment": null, "summary": "We propose scene-adaptive strategies to efficiently allocate representation\ncapacity for generating immersive experiences of indoor environments from\nincomplete observations. Indoor scenes with multiple rooms often exhibit\nirregular layouts with varying complexity, containing clutter, occlusion, and\nflat walls. We maximize the utilization of limited resources with guidance from\ngeometric priors, which are often readily available after pre-processing\nstages. We record observation statistics on the estimated geometric scaffold\nand guide the optimal placement of bases, which greatly improves upon the\nuniform basis arrangements adopted by previous scalable Neural Radiance Field\n(NeRF) representations. We also suggest scene-adaptive virtual viewpoints to\ncompensate for geometric deficiencies inherent in view configurations in the\ninput trajectory and impose the necessary regularization. We present a\ncomprehensive analysis and discussion regarding rendering quality and memory\nrequirements in several large-scale indoor scenes, demonstrating significant\nenhancements compared to baselines that employ regular placements.", "AI": {"tldr": "\u63d0\u51fa\u573a\u666f\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u4ee5\u6709\u6548\u5206\u914d\u8868\u5f81\u80fd\u529b\uff0c\u4ece\u4e0d\u5b8c\u6574\u7684\u89c2\u6d4b\u4e2d\u751f\u6210\u5ba4\u5185\u73af\u5883\u7684\u6c89\u6d78\u5f0f\u4f53\u9a8c\u3002", "motivation": "\u5ba4\u5185\u573a\u666f\u901a\u5e38\u5448\u73b0\u51fa\u5177\u6709\u4e0d\u540c\u590d\u6742\u6027\u7684\u4e0d\u89c4\u5219\u5e03\u5c40\uff0c\u5305\u542b\u6742\u4e71\u3001\u906e\u6321\u548c\u5e73\u5766\u5899\u58c1\u3002", "method": "\u5229\u7528\u51e0\u4f55\u5148\u9a8c\u6765\u6307\u5bfc\u6709\u9650\u8d44\u6e90\u7684\u5229\u7528\uff0c\u8bb0\u5f55\u4f30\u8ba1\u7684\u51e0\u4f55\u652f\u67b6\u4e0a\u7684\u89c2\u6d4b\u7edf\u8ba1\u6570\u636e\uff0c\u5e76\u6307\u5bfc\u57fa\u7684\u6700\u4f73\u653e\u7f6e\uff0c\u5e76\u63d0\u51fa\u573a\u666f\u81ea\u9002\u5e94\u865a\u62df\u89c6\u70b9\u4ee5\u8865\u507f\u8f93\u5165\u8f68\u8ff9\u4e2d\u56fa\u6709\u7684\u51e0\u4f55\u7f3a\u9677\uff0c\u5e76\u65bd\u52a0\u5fc5\u8981\u7684\u6b63\u5219\u5316\u3002", "result": "\u5728\u51e0\u4e2a\u5927\u578b\u5ba4\u5185\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5206\u6790\u548c\u8ba8\u8bba\uff0c\u7ed3\u679c\u8868\u660e\u4e0e\u91c7\u7528\u89c4\u5219\u653e\u7f6e\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6709\u663e\u7740\u589e\u5f3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6e32\u67d3\u8d28\u91cf\u548c\u5185\u5b58\u9700\u6c42\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2510.10193", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10193", "abs": "https://arxiv.org/abs/2510.10193", "authors": ["Qingni Wang", "Yue Fan", "Xin Eric Wang"], "title": "SAFER: Risk-Constrained Sample-then-Filter in Large Language Models", "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in risk-sensitive\napplications such as real-world open-ended question answering (QA), ensuring\nthe trustworthiness of their outputs has become critical. Existing selective\nconformal prediction (SCP) methods provide statistical guarantees by\nconstructing prediction sets with a constrained miscoverage rate for correct\nanswers. However, prior works unrealistically assume that admissible answers\nfor all instances can be obtained via finite sampling, even for open-ended QA\nscenarios that lack a fixed and finite solution space. To address this, we\nintroduce a two-stage risk control framework comprising abstention-aware\nsampling and conformalized filtering (SAFER). Firstly, on a held-out\ncalibration set, SAFER calibrates a sampling budget within the maximum sampling\ncap, using the Clopper-Pearson exact method at a user-desired risk level (i.e.,\nthe maximum allowable miscoverage rate of the sampling sets). If the risk level\ncannot be satisfied within the cap, we abstain; otherwise, the calibrated\nsampling budget becomes the minimum requirements at test time. Then, we employ\ncalibration instances where correct answers are attainable under the calibrated\nbudget and apply the conformal risk control method to determine a statistically\nvalid uncertainty threshold, which filters unreliable distractors from the\ncandidate set for each test data point. In this stage, SAFER introduces an\nadditional risk level to guide the calculation of the threshold, thereby\ncontrolling the risk of correct answers being excluded. Furthermore, we show\nthat SAFER is compatible with various task-specific admission criteria and\ncalibration-test split ratios, highlighting its robustness and high data\nefficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e24\u9636\u6bb5\u98ce\u9669\u63a7\u5236\u6846\u67b6SAFER\uff0c\u7528\u4e8e\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u5f0f\u95ee\u7b54\u7b49\u98ce\u9669\u654f\u611f\u5e94\u7528\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u9009\u62e9\u6027\u5171\u5f62\u9884\u6d4b(SCP)\u65b9\u6cd5\u5728\u6784\u5efa\u9884\u6d4b\u96c6\u65f6\uff0c\u5bf9\u6b63\u786e\u7b54\u6848\u7684\u9519\u8bef\u8986\u76d6\u7387\u8fdb\u884c\u4e86\u7ea6\u675f\uff0c\u4ece\u800c\u63d0\u4f9b\u4e86\u7edf\u8ba1\u4fdd\u8bc1\u3002\u7136\u800c\uff0c\u5148\u524d\u7684\u5de5\u4f5c\u4e0d\u5207\u5b9e\u9645\u5730\u5047\u8bbe\u6240\u6709\u5b9e\u4f8b\u7684\u53ef\u63a5\u53d7\u7b54\u6848\u90fd\u53ef\u4ee5\u901a\u8fc7\u6709\u9650\u91c7\u6837\u83b7\u5f97\uff0c\u5373\u4f7f\u5bf9\u4e8e\u7f3a\u4e4f\u56fa\u5b9a\u548c\u6709\u9650\u89e3\u7a7a\u95f4\u7684\u5f00\u653e\u5f0fQA\u573a\u666f\u4e5f\u662f\u5982\u6b64\u3002", "method": "SAFER\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5\uff1aabstention-aware sampling\u548cconformalized filtering\u3002\u9996\u5148\uff0c\u5728\u4fdd\u7559\u7684\u6821\u51c6\u96c6\u4e0a\uff0cSAFER\u5728\u6700\u5927\u91c7\u6837\u4e0a\u9650\u5185\u6821\u51c6\u91c7\u6837\u9884\u7b97\uff0c\u4f7f\u7528Clopper-Pearson\u7cbe\u786e\u65b9\u6cd5\u5728\u7528\u6237\u671f\u671b\u7684\u98ce\u9669\u6c34\u5e73\u4e0b\u8fdb\u884c\u6821\u51c6\u3002\u7136\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u6821\u51c6\u5b9e\u4f8b\uff0c\u5728\u6821\u51c6\u9884\u7b97\u4e0b\u53ef\u4ee5\u83b7\u5f97\u6b63\u786e\u7684\u7b54\u6848\uff0c\u5e76\u5e94\u7528\u5171\u5f62\u98ce\u9669\u63a7\u5236\u65b9\u6cd5\u6765\u786e\u5b9a\u4e00\u4e2a\u7edf\u8ba1\u4e0a\u6709\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u9608\u503c\uff0c\u8be5\u9608\u503c\u4ece\u6bcf\u4e2a\u6d4b\u8bd5\u6570\u636e\u70b9\u7684\u5019\u9009\u96c6\u4e2d\u8fc7\u6ee4\u6389\u4e0d\u53ef\u9760\u7684\u5e72\u6270\u56e0\u7d20\u3002", "result": "SAFER\u4e0e\u5404\u79cd\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u51c6\u5165\u6807\u51c6\u548c\u6821\u51c6\u6d4b\u8bd5\u62c6\u5206\u6bd4\u7387\u517c\u5bb9\uff0c\u7a81\u51fa\u4e86\u5176\u9c81\u68d2\u6027\u548c\u9ad8\u6570\u636e\u6548\u7387\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e24\u9636\u6bb5\u98ce\u9669\u63a7\u5236\u6846\u67b6SAFER\uff0c\u7528\u4e8e\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u5f0f\u95ee\u7b54\u7b49\u98ce\u9669\u654f\u611f\u5e94\u7528\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002\u8be5\u6846\u67b6\u901a\u8fc7 abstention-aware sampling \u548c conformalized filtering \u4e24\u4e2a\u9636\u6bb5\uff0c\u5b9e\u73b0\u4e86\u5728\u7ea6\u675f\u7684\u9519\u8bef\u8986\u76d6\u7387\u4e0b\u6784\u5efa\u9884\u6d4b\u96c6\uff0c\u5e76\u4e14\u5177\u6709\u9c81\u68d2\u6027\u548c\u9ad8\u6570\u636e\u6548\u7387\u3002"}}
{"id": "2510.09691", "categories": ["cs.LG", "cs.AI", "68T07, 68M14", "I.2.6; I.2.11; K.6.5"], "pdf": "https://arxiv.org/pdf/2510.09691", "abs": "https://arxiv.org/abs/2510.09691", "authors": ["Tejash Varsani"], "title": "Evaluation of Differential Privacy Mechanisms on Federated Learning", "comment": "Supervised by Prof. Dr.-Ing. habil. Alois C. Knoll; Advisor:\n  Nagacharan Teja Tangirala, M.Sc", "summary": "Federated learning is distributed model training across several clients\nwithout disclosing raw data. Despite advancements in data privacy, risks still\nremain. Differential Privacy (DP) is a technique to protect sensitive data by\nadding noise to model updates, usually controlled by a fixed privacy budget.\nHowever, this approach can introduce excessive noise, particularly when the\nmodel converges, which compromises performance. To address this problem,\nadaptive privacy budgets have been investigated as a potential solution. This\nwork implements DP methods using Laplace and Gaussian mechanisms with an\nadaptive privacy budget, extending the SelecEval simulator. We introduce an\nadaptive clipping approach in the Gaussian mechanism, ensuring that gradients\nof the model are dynamically updated rather than using a fixed sensitivity. We\nconduct extensive experiments with various privacy budgets, IID and non-IID\ndatasets, and different numbers of selected clients per round. While our\nexperiments were limited to 200 training rounds, the results suggest that\nadaptive privacy budgets and adaptive clipping can help maintain model accuracy\nwhile preserving privacy.", "AI": {"tldr": "\u8054\u90a6\u5b66\u4e60\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u6a21\u578b\u6536\u655b\u65f6\uff0c\u56fa\u5b9a\u9690\u79c1\u9884\u7b97\u4f1a\u5f15\u5165\u8fc7\u591a\u566a\u58f0\uff0c\u5f71\u54cd\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u56fa\u5b9a\u9690\u79c1\u9884\u7b97\u7684\u95ee\u9898\uff0c\u7814\u7a76\u4eba\u5458\u5f00\u59cb\u63a2\u7d22\u81ea\u9002\u5e94\u9690\u79c1\u9884\u7b97\u3002", "method": "\u672c\u6587\u5b9e\u73b0\u4e86\u57fa\u4e8e Laplace \u548c Gaussian \u673a\u5236\u7684 DP \u65b9\u6cd5\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u9690\u79c1\u9884\u7b97\uff0c\u6269\u5c55\u4e86 SelecEval \u6a21\u62df\u5668\u3002\u5728\u9ad8\u65af\u673a\u5236\u4e2d\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u526a\u88c1\u65b9\u6cd5\uff0c\u4ee5\u52a8\u6001\u66f4\u65b0\u6a21\u578b\u68af\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u81ea\u9002\u5e94\u9690\u79c1\u9884\u7b97\u548c\u81ea\u9002\u5e94\u526a\u88c1\u6709\u52a9\u4e8e\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002", "conclusion": "\u81ea\u9002\u5e94\u9690\u79c1\u9884\u7b97\u548c\u81ea\u9002\u5e94\u526a\u88c1\u662f\u63d0\u9ad8\u8054\u90a6\u5b66\u4e60\u4e2d\u9690\u79c1\u4fdd\u62a4\u548c\u6a21\u578b\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2510.11402", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11402", "abs": "https://arxiv.org/abs/2510.11402", "authors": ["Gregor Meehan", "Johan Pauwels"], "title": "On Inherited Popularity Bias in Cold-Start Item Recommendation", "comment": "Published at ACM RecSys 2025", "summary": "Collaborative filtering (CF) recommender systems struggle with making\npredictions on unseen, or 'cold', items. Systems designed to address this\nchallenge are often trained with supervision from warm CF models in order to\nleverage collaborative and content information from the available interaction\ndata. However, since they learn to replicate the behavior of CF methods,\ncold-start models may therefore also learn to imitate their predictive biases.\nIn this paper, we show that cold-start systems can inherit popularity bias, a\ncommon cause of recommender system unfairness arising when CF models overfit to\nmore popular items, thereby maximizing user-oriented accuracy but neglecting\nrarer items. We demonstrate that cold-start recommenders not only mirror the\npopularity biases of warm models, but are in fact affected more severely:\nbecause they cannot infer popularity from interaction data, they instead\nattempt to estimate it based solely on content features. This leads to\nsignificant over-prediction of certain cold items with similar content to\npopular warm items, even if their ground truth popularity is very low. Through\nexperiments on three multimedia datasets, we analyze the impact of this\nbehavior on three generative cold-start methods. We then describe a simple\npost-processing bias mitigation method that, by using embedding magnitude as a\nproxy for predicted popularity, can produce more balanced recommendations with\nlimited harm to user-oriented cold-start accuracy.", "AI": {"tldr": "\u51b7\u542f\u52a8\u63a8\u8350\u7cfb\u7edf\u65e8\u5728\u89e3\u51b3\u672a\u89c1\u8fc7\u7269\u54c1\u7684\u9884\u6d4b\u95ee\u9898\uff0c\u4f46\u4f1a\u7ee7\u627f\u70ed\u95e8\u7269\u54c1\u7684\u504f\u5dee\uff0c\u5bfc\u81f4\u4e0d\u516c\u5e73\u3002", "motivation": "\u51b7\u542f\u52a8\u7cfb\u7edf\u4f1a\u5b66\u4e60\u6a21\u4eff CF \u65b9\u6cd5\uff0c\u53ef\u80fd\u4e5f\u4f1a\u6a21\u4eff\u5b83\u4eec\u7684\u9884\u6d4b\u504f\u5dee\u3002\u672c\u6587\u7814\u7a76\u8868\u660e\uff0c\u51b7\u542f\u52a8\u7cfb\u7edf\u4f1a\u7ee7\u627f\u70ed\u95e8\u5ea6\u504f\u5dee\uff0c\u5e76\u4e14\u6bd4 warm \u6a21\u578b\u66f4\u4e25\u91cd\uff0c\u56e0\u4e3a\u5b83\u4eec\u65e0\u6cd5\u4ece\u4ea4\u4e92\u6570\u636e\u4e2d\u63a8\u65ad\u70ed\u95e8\u5ea6\uff0c\u800c\u662f\u8bd5\u56fe\u4ec5\u57fa\u4e8e\u5185\u5bb9\u7279\u5f81\u6765\u4f30\u8ba1\u3002", "method": "\u901a\u8fc7\u5728\u4e09\u4e2a\u591a\u5a92\u4f53\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5206\u6790\u4e86\u8fd9\u79cd\u884c\u4e3a\u5bf9\u4e09\u79cd\u751f\u6210\u5f0f\u51b7\u542f\u52a8\u65b9\u6cd5\u7684\u5f71\u54cd\u3002\u7136\u540e\uff0c\u63cf\u8ff0\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u540e\u5904\u7406\u504f\u5dee\u7f13\u89e3\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u5d4c\u5165\u5e45\u5ea6\u4f5c\u4e3a\u9884\u6d4b\u70ed\u95e8\u5ea6\u7684\u4ee3\u7406\uff0c\u53ef\u4ee5\u4ea7\u751f\u66f4\u5e73\u8861\u7684\u63a8\u8350\uff0c\u4e14\u5bf9\u9762\u5411\u7528\u6237\u7684\u51b7\u542f\u52a8\u51c6\u786e\u6027\u7684\u635f\u5bb3\u6709\u9650\u3002", "result": "\u51b7\u542f\u52a8\u63a8\u8350\u7cfb\u7edf\u4e0d\u4ec5\u53cd\u6620\u4e86 warm \u6a21\u578b\u7684\u70ed\u95e8\u5ea6\u504f\u5dee\uff0c\u800c\u4e14\u53d7\u5230\u7684\u5f71\u54cd\u5b9e\u9645\u4e0a\u66f4\u4e3a\u4e25\u91cd\u3002\u56e0\u4e3a\u4ed6\u4eec\u65e0\u6cd5\u4ece\u4e92\u52a8\u6570\u636e\u4e2d\u63a8\u65ad\u51fa\u53d7\u6b22\u8fce\u7a0b\u5ea6\uff0c\u56e0\u6b64\u4ed6\u4eec\u8bd5\u56fe\u4ec5\u6839\u636e\u5185\u5bb9\u7279\u5f81\u6765\u4f30\u8ba1\u53d7\u6b22\u8fce\u7a0b\u5ea6\u3002\u8fd9\u5bfc\u81f4\u5bf9\u67d0\u4e9b\u4e0e\u6d41\u884c\u7684\u70ed\u95e8\u5546\u54c1\u5177\u6709\u76f8\u4f3c\u5185\u5bb9\u7684\u51b7\u95e8\u5546\u54c1\u7684\u5927\u91cf\u8fc7\u5ea6\u9884\u6d4b\uff0c\u5373\u4f7f\u5b83\u4eec\u7684\u771f\u5b9e\u53d7\u6b22\u8fce\u7a0b\u5ea6\u975e\u5e38\u4f4e\u3002", "conclusion": "\u51b7\u542f\u52a8\u7cfb\u7edf\u4f1a\u7ee7\u627f warm \u6a21\u578b\u7684 popularity \u504f\u5dee\uff0c\u5e76\u4e14\u6bd4 warm \u6a21\u578b\u66f4\u4e25\u91cd\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u540e\u5904\u7406\u504f\u5dee\u7f13\u89e3\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4ea7\u751f\u66f4\u5e73\u8861\u7684\u63a8\u8350\u3002"}}
{"id": "2510.09883", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09883", "abs": "https://arxiv.org/abs/2510.09883", "authors": ["Hossein Entezari Zarch", "Lei Gao", "Chaoyi Jiang", "Murali Annavarm"], "title": "DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context Reasoning", "comment": null, "summary": "Large reasoning models (LRMs) achieve state-of-the-art performance on\nchallenging benchmarks by generating long chains of intermediate steps, but\ntheir inference cost is dominated by decoding, where each new token must attend\nto the entire growing sequence. Existing sparse attention methods reduce\ncomputation by pruning the key-value (KV) cache, yet they suffer from severe\naccuracy degradation on reasoning tasks due to cumulative selection errors and\nthe dynamic importance of tokens over long derivations. We present\n\\textbf{DELTA}, a training-free sparse attention mechanism that achieves\ncomputational efficiency without sacrificing model accuracy. DELTA partitions\ntransformer layers into three groups: initial layers that use full attention, a\nsmall set of \\emph{selection layers} that identify salient tokens via\naggregated head-level attention scores, and subsequent \\emph{sparse-attention\nlayers} that attend only to the selected subset. This design preserves the full\nKV cache in GPU memory for accuracy, while avoiding expensive full-attention\ncomputation over many layers. On reasoning benchmarks such as AIME and\nGPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while\nreducing the number of attended tokens by up to $5\\times$ and delivering\n$1.5\\times$ end-to-end speedup. Our results show that selective reuse of\nintermediate attention maps offers a robust path toward efficient long-context\nreasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a DELTA \u7684\u514d\u8bad\u7ec3\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u65e8\u5728\u63d0\u9ad8\u5927\u578b\u63a8\u7406\u6a21\u578b (LRM) \u7684\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u3002\u901a\u8fc7\u9009\u62e9\u6027\u5730\u91cd\u7528\u4e2d\u95f4\u6ce8\u610f\u529b\u56fe\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u957f\u6587\u672c\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u901a\u8fc7\u4fee\u526a\u952e\u503c (KV) \u7f13\u5b58\u6765\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u4f46\u7531\u4e8e\u7d2f\u79ef\u9009\u62e9\u9519\u8bef\u548c tokens \u5728\u957f\u63a8\u5bfc\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u91cd\u8981\u6027\uff0c\u5bfc\u81f4\u63a8\u7406\u4efb\u52a1\u7684\u51c6\u786e\u6027\u4e25\u91cd\u4e0b\u964d\u3002", "method": "DELTA \u5c06 Transformer \u5c42\u5212\u5206\u4e3a\u4e09\u7ec4\uff1a\u4f7f\u7528\u5168\u6ce8\u610f\u529b\u7684\u521d\u59cb\u5c42\u3001\u901a\u8fc7\u805a\u5408\u7684\u5934\u7ea7\u6ce8\u610f\u529b\u5206\u6570\u8bc6\u522b\u663e\u8457 tokens \u7684\u5c11\u91cf\u9009\u62e9\u5c42\uff0c\u4ee5\u53ca\u4ec5\u5173\u6ce8\u6240\u9009\u5b50\u96c6\u7684\u540e\u7eed\u7a00\u758f\u6ce8\u610f\u529b\u5c42\u3002", "result": "\u5728 AIME \u548c GPQA-Diamond \u7b49\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDELTA \u5728\u51c6\u786e\u6027\u65b9\u9762\u4e0e\u5168\u6ce8\u610f\u529b\u76f8\u5339\u914d\u6216\u8d85\u8fc7\u5168\u6ce8\u610f\u529b\uff0c\u540c\u65f6\u5c06\u53c2\u4e0e\u7684 tokens \u6570\u91cf\u51cf\u5c11\u4e86\u9ad8\u8fbe 5 \u500d\uff0c\u5e76\u63d0\u4f9b\u4e86 1.5 \u500d\u7684\u7aef\u5230\u7aef\u52a0\u901f\u3002", "conclusion": "\u9009\u62e9\u6027\u5730\u91cd\u7528\u4e2d\u95f4\u6ce8\u610f\u529b\u56fe\u4e3a\u9ad8\u6548\u7684\u957f\u6587\u672c\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u9760\u7684\u9014\u5f84\u3002"}}
{"id": "2510.09881", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09881", "abs": "https://arxiv.org/abs/2510.09881", "authors": ["Minkwan Kim", "Seungmin Lee", "Junho Kim", "Young Min Kim"], "title": "LTGS: Long-Term Gaussian Scene Chronology From Sparse View Updates", "comment": null, "summary": "Recent advances in novel-view synthesis can create the photo-realistic\nvisualization of real-world environments from conventional camera captures.\nHowever, acquiring everyday environments from casual captures faces challenges\ndue to frequent scene changes, which require dense observations both spatially\nand temporally. We propose long-term Gaussian scene chronology from sparse-view\nupdates, coined LTGS, an efficient scene representation that can embrace\neveryday changes from highly under-constrained casual captures. Given an\nincomplete and unstructured Gaussian splatting representation obtained from an\ninitial set of input images, we robustly model the long-term chronology of the\nscene despite abrupt movements and subtle environmental variations. We\nconstruct objects as template Gaussians, which serve as structural, reusable\npriors for shared object tracks. Then, the object templates undergo a further\nrefinement pipeline that modulates the priors to adapt to temporally varying\nenvironments based on few-shot observations. Once trained, our framework is\ngeneralizable across multiple time steps through simple transformations,\nsignificantly enhancing the scalability for a temporal evolution of 3D\nenvironments. As existing datasets do not explicitly represent the long-term\nreal-world changes with a sparse capture setup, we collect real-world datasets\nto evaluate the practicality of our pipeline. Experiments demonstrate that our\nframework achieves superior reconstruction quality compared to other baselines\nwhile enabling fast and light-weight updates.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u573a\u666f\u8868\u793a\u65b9\u6cd5LTGS\uff0c\u53ef\u4ee5\u4ece\u7a00\u758f\u89c6\u89d2\u66f4\u65b0\u4e2d\u9002\u5e94\u65e5\u5e38\u53d8\u5316\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u771f\u5b9e\u4e16\u754c\u73af\u5883\u7684\u5149\u771f\u5b9e\u611f\u53ef\u89c6\u5316\u3002", "motivation": "\u4ece\u5e38\u89c4\u76f8\u673a\u62cd\u6444\u4e2d\u83b7\u53d6\u65e5\u5e38\u73af\u5883\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u9891\u7e41\u7684\u573a\u666f\u53d8\u5316\u9700\u8981\u5bc6\u96c6\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u89c2\u6d4b\u3002", "method": "1. \u6784\u5efa\u5bf9\u8c61\u4e3a\u6a21\u677f\u9ad8\u65af\uff0c\u4f5c\u4e3a\u5171\u4eab\u5bf9\u8c61\u8f68\u8ff9\u7684\u7ed3\u6784\u5316\u3001\u53ef\u91cd\u7528\u5148\u9a8c\u3002 2. \u5bf9\u8c61\u6a21\u677f\u7ecf\u8fc7\u8fdb\u4e00\u6b65\u7684\u7ec6\u5316\u6d41\u7a0b\uff0c\u8c03\u6574\u5148\u9a8c\u4ee5\u9002\u5e94\u57fa\u4e8e\u5c11\u91cf\u89c2\u6d4b\u7684\u65f6\u95f4\u53d8\u5316\u73af\u5883\u3002 3. \u901a\u8fc7\u7b80\u5355\u7684\u8f6c\u6362\uff0c\u6211\u4eec\u7684\u6846\u67b6\u53ef\u4ee5\u63a8\u5e7f\u5230\u591a\u4e2a\u65f6\u95f4\u6b65\u957f\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u5feb\u901f\u548c\u8f7b\u91cf\u7ea7\u7684\u66f4\u65b0\u3002", "conclusion": "LTGS \u662f\u4e00\u79cd\u6709\u6548\u7684\u573a\u666f\u8868\u793a\uff0c\u53ef\u4ee5\u4ece\u7a00\u758f\u89c6\u89d2\u66f4\u65b0\u4e2d\u9002\u5e94\u65e5\u5e38\u53d8\u5316\u3002"}}
{"id": "2510.10197", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10197", "abs": "https://arxiv.org/abs/2510.10197", "authors": ["Siyuan Lu", "Zechuan Wang", "Hongxuan Zhang", "Qintong Wu", "Leilei Gan", "Chenyi Zhuang", "Jinjie Gu", "Tao Lin"], "title": "Don't Just Fine-tune the Agent, Tune the Environment", "comment": null, "summary": "Large Language Model (LLM) agents show great promise for complex, multi-turn\ntool-use tasks, but their development is often hampered by the extreme scarcity\nof high-quality training data. Supervised fine-tuning (SFT) on synthetic data\nleads to overfitting, whereas standard reinforcement learning (RL) struggles\nwith a critical cold-start problem and training instability. To address these\nchallenges, we introduce $\\textbf{Environment Tuning}$, a novel training\nparadigm that enables agents to learn complex behaviors directly from problem\ninstances without relying on pre-collected expert trajectories.\n$\\textbf{Environment Tuning}$ orchestrates this learning process through a\nstructured curriculum, actionable environment augmentation that provides\ncorrective feedback, and fine-grained progress rewards to ensure stable and\nefficient exploration. Using only 400 problem instances from Berkeley\nFunction-Calling Leaderboard (BFCL) benchmark, our method not only achieves\ncompetitive in-distribution performance against strong baselines but also\ndemonstrates superior out-of-distribution generalization, overcoming the\nperformance collapse common to SFT-based approaches. Our work presents a\nparadigm shift from supervised fine-tuning on static trajectories to dynamic,\nenvironment-based exploration, paving the way for training more robust and\ndata-efficient agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86Environment Tuning\u65b9\u6cd5\uff0c\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u4f7fagent\u80fd\u591f\u76f4\u63a5\u4ece\u95ee\u9898\u5b9e\u4f8b\u4e2d\u5b66\u4e60\u590d\u6742\u7684\u884c\u4e3a\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u9884\u5148\u6536\u96c6\u7684\u4e13\u5bb6\u8f68\u8ff9\u3002", "motivation": "\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u6781\u5ea6\u7a00\u7f3a\u963b\u788d\u4e86LLM agent\u7684\u5f00\u53d1\uff1b\u5728\u5408\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5bfc\u81f4\u8fc7\u62df\u5408\uff0c\u800c\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5219\u9762\u4e34\u7740\u4e25\u91cd\u7684\u51b7\u542f\u52a8\u95ee\u9898\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u8bfe\u7a0b\u3001\u53ef\u64cd\u4f5c\u7684\u73af\u5883\u589e\u5f3a\uff08\u63d0\u4f9b\u7ea0\u6b63\u53cd\u9988\uff09\u548c\u7ec6\u7c92\u5ea6\u7684\u8fdb\u5ea6\u5956\u52b1\u6765\u534f\u8c03\u5b66\u4e60\u8fc7\u7a0b\uff0c\u786e\u4fdd\u7a33\u5b9a\u548c\u9ad8\u6548\u7684\u63a2\u7d22\u3002", "result": "\u4ec5\u4f7f\u7528BFCL\u7684400\u4e2a\u95ee\u9898\u5b9e\u4f8b\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u4e0e\u5f3a\u5927\u57fa\u7ebf\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u7684\u540c\u5206\u5e03\u6027\u80fd\uff0c\u800c\u4e14\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u5f02\u5206\u5e03\u6cdb\u5316\u80fd\u529b\uff0c\u514b\u670d\u4e86\u57fa\u4e8eSFT\u7684\u65b9\u6cd5\u5e38\u89c1\u7684\u6027\u80fd\u5d29\u6e83\u3002", "conclusion": "\u4ece\u9759\u6001\u8f68\u8ff9\u4e0a\u7684\u76d1\u7763\u5fae\u8c03\u5230\u57fa\u4e8e\u52a8\u6001\u73af\u5883\u7684\u63a2\u7d22\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4e3a\u8bad\u7ec3\u66f4\u9c81\u68d2\u548c\u6570\u636e\u9ad8\u6548\u7684agent\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.09693", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2510.09693", "abs": "https://arxiv.org/abs/2510.09693", "authors": ["Jiakang Chen"], "title": "Neural PDE Solvers with Physics Constraints: A Comparative Study of PINNs, DRM, and WANs", "comment": "50 pages, 13 figures", "summary": "Partial differential equations (PDEs) underpin models across science and\nengineering, yet analytical solutions are atypical and classical mesh-based\nsolvers can be costly in high dimensions. This dissertation presents a unified\ncomparison of three mesh-free neural PDE solvers, physics-informed neural\nnetworks (PINNs), the deep Ritz method (DRM), and weak adversarial networks\n(WANs), on Poisson problems (up to 5D) and the time-independent Schr\\\"odinger\nequation in 1D/2D (infinite well and harmonic oscillator), and extends the\nstudy to a laser-driven case of Schr\\\"odinger's equation via the\nKramers-Henneberger (KH) transformation.\n  Under a common protocol, all methods achieve low $L_2$ errors\n($10^{-6}$-$10^{-9}$) when paired with forced boundary conditions (FBCs),\nforced nodes (FNs), and orthogonality regularization (OG). Across tasks, PINNs\nare the most reliable for accuracy and recovery of excited spectra; DRM offers\nthe best accuracy-runtime trade-off on stationary problems; WAN is more\nsensitive but competitive when weak-form constraints and FN/OG are used\neffectively. Sensitivity analyses show that FBC removes boundary-loss tuning,\nnetwork width matters more than depth for single-network solvers, and most\ngains occur within 5000-10,000 epochs. The same toolkit solves the KH case,\nindicating transfer beyond canonical benchmarks.\n  We provide practical guidelines for method selection and outline the\nfollowing extensions: time-dependent formulations for DRM and WAN, adaptive\nresidual-driven sampling, parallel multi-state training, and neural domain\ndecomposition. These results support physics-guided neural solvers as credible,\nscalable tools for solving complex PDEs.", "AI": {"tldr": "\u672c\u8bba\u6587\u5bf9\u6bd4\u4e86\u4e09\u79cd\u65e0\u7f51\u683c\u795e\u7ecf\u504f\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3\u5668\uff08PINNs\u3001DRM \u548c WANs\uff09\u5728\u6cca\u677e\u95ee\u9898\u548c\u859b\u5b9a\u8c14\u65b9\u7a0b\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u4f9b\u65b9\u6cd5\u9009\u62e9\u7684\u5b9e\u7528\u6307\u5357\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u7f51\u683c\u7684\u6c42\u89e3\u5668\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u6210\u672c\u9ad8\u6602\uff0c\u800c\u89e3\u6790\u89e3\u5e76\u4e0d\u5e38\u89c1\u3002", "method": "\u4f7f\u7528\u7edf\u4e00\u7684\u534f\u8bae\uff0c\u5728\u6cca\u677e\u95ee\u9898\uff08\u9ad8\u8fbe 5D\uff09\u548c\u4e0e\u65f6\u95f4\u65e0\u5173\u7684\u859b\u5b9a\u8c14\u65b9\u7a0b\u5728 1D/2D\uff08\u65e0\u9650\u9631\u548c\u7b80\u8c10\u632f\u5b50\uff09\u4e0a\uff0c\u6bd4\u8f83\u4e86\u4e09\u79cd\u65e0\u7f51\u683c\u795e\u7ecf PDE \u6c42\u89e3\u5668\uff0c\u5373\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc (PINN)\u3001\u6df1\u5ea6 Ritz \u65b9\u6cd5 (DRM) \u548c\u5f31\u5bf9\u6297\u7f51\u7edc (WAN)\u3002", "result": "\u6240\u6709\u65b9\u6cd5\u5728\u5f3a\u5236\u8fb9\u754c\u6761\u4ef6 (FBC)\u3001\u5f3a\u5236\u8282\u70b9 (FN) \u548c\u6b63\u4ea4\u6b63\u5219\u5316 (OG) \u7684\u914d\u5408\u4e0b\uff0c\u5747\u5b9e\u73b0\u4e86\u8f83\u4f4e\u7684 $L_2$ \u8bef\u5dee\uff08$10^{-6}$-$10^{-9}$\uff09\u3002PINN \u5728\u51c6\u786e\u6027\u548c\u6fc0\u53d1\u5149\u8c31\u6062\u590d\u65b9\u9762\u6700\u53ef\u9760\uff1bDRM \u5728\u9759\u6001\u95ee\u9898\u4e0a\u63d0\u4f9b\u4e86\u6700\u4f73\u7684\u51c6\u786e\u6027-\u8fd0\u884c\u65f6\u95f4\u6743\u8861\uff1bWAN \u66f4\u654f\u611f\uff0c\u4f46\u5f53\u5f31\u5f62\u5f0f\u7ea6\u675f\u548c FN/OG \u6709\u6548\u4f7f\u7528\u65f6\uff0c\u4e5f\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u7269\u7406\u5f15\u5bfc\u7684\u795e\u7ecf\u6c42\u89e3\u5668\u662f\u89e3\u51b3\u590d\u6742\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u53ef\u4fe1\u3001\u53ef\u6269\u5c55\u7684\u5de5\u5177\u3002"}}
{"id": "2510.11438", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11438", "abs": "https://arxiv.org/abs/2510.11438", "authors": ["Yujiang Wu", "Shanshan Zhong", "Yubin Kim", "Chenyan Xiong"], "title": "What Generative Search Engines Like and How to Optimize Web Content Cooperatively", "comment": null, "summary": "By employing large language models (LLMs) to retrieve documents and generate\nnatural language responses, Generative Engines, such as Google AI overview and\nChatGPT, provide significantly enhanced user experiences and have rapidly\nbecome the new form of search. Their rapid adoption also drives the needs of\nGenerative Engine Optimization (GEO), as content providers are eager to gain\nmore traction from them. In this paper, we introduce AutoGEO, a framework to\nautomatically learn generative engine preferences when using retrieved contents\nfor response generation, and rewrite web contents for more such traction.\nAutoGEO first prompts frontier LLMs to explain generative engine preferences\nand extract meaningful preference rules from these explanations. Then it uses\npreference rules as context engineering for AutoGEO$_\\text{API}$, a\nprompt-based GEO system, and as rule-based rewards to train\nAutoGEO$_\\text{Mini}$, a cost-effective GEO model. Experiments on the standard\nGEO-Bench and two newly constructed benchmarks using real user queries\ndemonstrate the effectiveness of AutoGEO in enhancing content traction while\npreserving search utility. Analyses confirm the learned rules' robustness and\nabilities to capture unique preferences in variant domains, and AutoGEO\nsystems' ability to embed them in content optimization. The code is released at\nhttps://github.com/cxcscmu/AutoGEO.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a AutoGEO \u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5b66\u4e60\u751f\u6210\u5f0f\u5f15\u64ce\u5728\u4f7f\u7528\u68c0\u7d22\u5185\u5bb9\u751f\u6210\u54cd\u5e94\u65f6\u7684\u504f\u597d\uff0c\u5e76\u91cd\u5199 Web \u5185\u5bb9\u4ee5\u83b7\u5f97\u66f4\u591a\u5173\u6ce8\u3002", "motivation": "\u5185\u5bb9\u63d0\u4f9b\u5546\u5e0c\u671b\u4ece\u751f\u6210\u5f0f\u5f15\u64ce\u4e2d\u83b7\u5f97\u66f4\u591a\u5173\u6ce8\uff0c\u56e0\u6b64\u9700\u8981\u751f\u6210\u5f0f\u5f15\u64ce\u4f18\u5316 (GEO)\u3002", "method": "AutoGEO \u9996\u5148\u63d0\u793a\u524d\u6cbf LLM \u89e3\u91ca\u751f\u6210\u5f0f\u5f15\u64ce\u504f\u597d\uff0c\u5e76\u4ece\u8fd9\u4e9b\u89e3\u91ca\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7684\u504f\u597d\u89c4\u5219\u3002\u7136\u540e\uff0c\u5b83\u4f7f\u7528\u504f\u597d\u89c4\u5219\u4f5c\u4e3a AutoGEO$_{\\text{API}}$\uff08\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u7684 GEO \u7cfb\u7edf\uff09\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff0c\u5e76\u4f5c\u4e3a\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\u6765\u8bad\u7ec3 AutoGEO$_{\\text{Mini}}$\uff08\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684 GEO \u6a21\u578b\uff09\u3002", "result": "\u5728\u6807\u51c6 GEO-Bench \u548c\u4e24\u4e2a\u4f7f\u7528\u771f\u5b9e\u7528\u6237\u67e5\u8be2\u65b0\u5efa\u7684\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAutoGEO \u5728\u589e\u5f3a\u5185\u5bb9\u5438\u5f15\u529b\u7684\u540c\u65f6\u4fdd\u7559\u4e86\u641c\u7d22\u6548\u7528\u3002", "conclusion": "\u5206\u6790\u8bc1\u5b9e\u4e86\u5b66\u4e60\u5230\u7684\u89c4\u5219\u7684\u9c81\u68d2\u6027\u4ee5\u53ca\u6355\u83b7\u53d8\u4f53\u57df\u4e2d\u72ec\u7279\u504f\u597d\u7684\u80fd\u529b\uff0c\u4ee5\u53ca AutoGEO \u7cfb\u7edf\u5c06\u5b83\u4eec\u5d4c\u5165\u5185\u5bb9\u4f18\u5316\u7684\u80fd\u529b\u3002"}}
{"id": "2510.09885", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09885", "abs": "https://arxiv.org/abs/2510.09885", "authors": ["Xu Pan", "Ely Hahami", "Jingxuan Fan", "Ziqian Xie", "Haim Sompolinsky"], "title": "Closing the Data-Efficiency Gap Between Autoregressive and Masked Diffusion LLMs", "comment": null, "summary": "Despite autoregressive large language models (arLLMs) being the current\ndominant paradigm in language modeling, they resist knowledge injection via\nfine-tuning due to inherent shortcomings such as the \"reversal curse\" -- the\nchallenge of answering questions that reverse the original information order in\nthe training sample. Masked diffusion large language models (dLLMs) are rapidly\nemerging as a powerful alternative to the arLLM paradigm, with evidence of\nbetter data efficiency and free of the \"reversal curse\" in pre-training.\nHowever, it is unknown whether these advantages extend to the post-training\nphase, i.e. whether pre-trained dLLMs can easily acquire new knowledge through\nfine-tuning. On three diverse datasets, we fine-tune arLLMs and dLLMs,\nevaluating them with forward and backward style Question Answering (QA) to\nprobe knowledge generalization and the reversal curse. Our results confirm that\narLLMs critically rely on extensive data augmentation via paraphrases for QA\ngeneralization, and paraphrases are only effective when their information order\nmatches the QA style. Conversely, dLLMs achieve high accuracies on both forward\nand backward QAs without paraphrases; adding paraphrases yields only marginal\ngains. Lastly, inspired by the dLLM's performance, we introduce a novel masked\nfine-tuning paradigm for knowledge injection into pre-trained arLLMs. This\nproposed method successfully and drastically improves the data efficiency of\narLLM fine-tuning, effectively closing the performance gap with dLLMs.", "AI": {"tldr": "Masked diffusion large language models (dLLMs) are emerging as a better alternative to autoregressive LLMs (arLLMs) due to better data efficiency and resistance to the reversal curse. The study investigates whether dLLMs can easily acquire new knowledge through fine-tuning.", "motivation": "Autoregressive large language models (arLLMs) resist knowledge injection via fine-tuning due to inherent shortcomings such as the reversal curse. Masked diffusion large language models (dLLMs) are emerging as a powerful alternative.", "method": "Fine-tune arLLMs and dLLMs on three diverse datasets, evaluating them with forward and backward style Question Answering (QA) to probe knowledge generalization and the reversal curse. Introduce a novel masked fine-tuning paradigm for knowledge injection into pre-trained arLLMs.", "result": "ArLLMs critically rely on extensive data augmentation via paraphrases for QA generalization, and paraphrases are only effective when their information order matches the QA style. DllMs achieve high accuracies on both forward and backward QAs without paraphrases; adding paraphrases yields only marginal gains. The proposed masked fine-tuning method successfully and drastically improves the data efficiency of arLLM fine-tuning, effectively closing the performance gap with dLLMs.", "conclusion": "dLLMs exhibit advantages in acquiring new knowledge through fine-tuning compared to arLLMs. A novel masked fine-tuning paradigm can improve the data efficiency of arLLM fine-tuning."}}
{"id": "2510.09903", "categories": ["cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.09903", "abs": "https://arxiv.org/abs/2510.09903", "authors": ["Lenny Aharon", "Keemin Lee", "Karan Sikka", "Selmaan Chettih", "Cole Hurwitz", "Liam Paninski", "Matthew R Whiteway"], "title": "An uncertainty-aware framework for data-efficient multi-view animal pose estimation", "comment": null, "summary": "Multi-view pose estimation is essential for quantifying animal behavior in\nscientific research, yet current methods struggle to achieve accurate tracking\nwith limited labeled data and suffer from poor uncertainty estimates. We\naddress these challenges with a comprehensive framework combining novel\ntraining and post-processing techniques, and a model distillation procedure\nthat leverages the strengths of these techniques to produce a more efficient\nand effective pose estimator. Our multi-view transformer (MVT) utilizes\npretrained backbones and enables simultaneous processing of information across\nall views, while a novel patch masking scheme learns robust cross-view\ncorrespondences without camera calibration. For calibrated setups, we\nincorporate geometric consistency through 3D augmentation and a triangulation\nloss. We extend the existing Ensemble Kalman Smoother (EKS) post-processor to\nthe nonlinear case and enhance uncertainty quantification via a variance\ninflation technique. Finally, to leverage the scaling properties of the MVT, we\ndesign a distillation procedure that exploits improved EKS predictions and\nuncertainty estimates to generate high-quality pseudo-labels, thereby reducing\ndependence on manual labels. Our framework components consistently outperform\nexisting methods across three diverse animal species (flies, mice, chickadees),\nwith each component contributing complementary benefits. The result is a\npractical, uncertainty-aware system for reliable pose estimation that enables\ndownstream behavioral analyses under real-world data constraints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u91cf\u5316\u52a8\u7269\u884c\u4e3a\u7684\u591a\u89c6\u89d2\u59ff\u6001\u4f30\u8ba1\u7684\u7efc\u5408\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u65b0\u7684\u8bad\u7ec3\u548c\u540e\u5904\u7406\u6280\u672f\uff0c\u4ee5\u53ca\u4e00\u79cd\u6a21\u578b\u84b8\u998f\u7a0b\u5e8f\uff0c\u4ee5\u751f\u6210\u66f4\u9ad8\u6548\u548c\u6709\u6548\u7684\u59ff\u6001\u4f30\u8ba1\u5668\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u96be\u4ee5\u5728\u6709\u9650\u7684\u6807\u8bb0\u6570\u636e\u4e0b\u5b9e\u73b0\u51c6\u786e\u7684\u8ddf\u8e2a\uff0c\u5e76\u4e14\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u8f83\u5dee\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u591a\u89c6\u89d2Transformer\uff08MVT\uff09\uff0c\u5b83\u5229\u7528\u9884\u8bad\u7ec3\u7684backbone\u5e76\u80fd\u591f\u540c\u65f6\u5904\u7406\u6240\u6709\u89c6\u89d2\u7684\u4fe1\u606f\uff0c\u540c\u65f6\u4e00\u79cd\u65b0\u7684patch masking\u65b9\u6848\u5b66\u4e60\u9c81\u68d2\u7684\u8de8\u89c6\u89d2\u5bf9\u5e94\u5173\u7cfb\uff0c\u65e0\u9700\u76f8\u673a\u6821\u51c6\u3002\u5bf9\u4e8e\u6821\u51c6\u7684\u8bbe\u7f6e\uff0c\u901a\u8fc73D\u589e\u5f3a\u548c\u4e09\u89d2\u635f\u5931\u6765\u7ed3\u5408\u51e0\u4f55\u4e00\u81f4\u6027\u3002\u5c06\u73b0\u6709\u7684Ensemble Kalman Smoother\uff08EKS\uff09\u540e\u5904\u7406\u5668\u6269\u5c55\u5230\u975e\u7ebf\u6027\u60c5\u51b5\uff0c\u5e76\u901a\u8fc7\u65b9\u5dee\u81a8\u80c0\u6280\u672f\u6765\u589e\u5f3a\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002\u6700\u540e\uff0c\u4e3a\u4e86\u5229\u7528MVT\u7684\u7f29\u653e\u7279\u6027\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u84b8\u998f\u7a0b\u5e8f\uff0c\u8be5\u7a0b\u5e8f\u5229\u7528\u6539\u8fdb\u7684EKS\u9884\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4f2a\u6807\u7b7e\uff0c\u4ece\u800c\u51cf\u5c11\u5bf9\u4eba\u5de5\u6807\u7b7e\u7684\u4f9d\u8d56\u3002", "result": "\u8be5\u6846\u67b6\u7ec4\u4ef6\u5728\u4e09\u79cd\u4e0d\u540c\u7684\u52a8\u7269\u7269\u79cd\uff08\u82cd\u8747\u3001\u5c0f\u9f20\u3001 chickadees\uff09\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6bcf\u4e2a\u7ec4\u4ef6\u90fd\u8d21\u732e\u4e86\u4e92\u8865\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u3001\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u610f\u8bc6\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u53ef\u9760\u7684\u59ff\u6001\u4f30\u8ba1\uff0c\u4ece\u800c\u80fd\u591f\u5728\u5b9e\u9645\u6570\u636e\u7ea6\u675f\u4e0b\u8fdb\u884c\u4e0b\u6e38\u884c\u4e3a\u5206\u6790\u3002"}}
{"id": "2510.10205", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10205", "abs": "https://arxiv.org/abs/2510.10205", "authors": ["Manjiang Yu", "Hongji Li", "Priyanka Singh", "Xue Li", "Di Wang", "Lijie Hu"], "title": "PIXEL: Adaptive Steering Via Position-wise Injection with eXact Estimated Levels under Subspace Calibration", "comment": "18 pages,3 figures", "summary": "Reliable behavior control is central to deploying large language models\n(LLMs) on the web. Activation steering offers a tuning-free route to align\nattributes (e.g., truthfulness) that ensure trustworthy generation. Prevailing\napproaches rely on coarse heuristics and lack a principled account of where to\nsteer and how strongly to intervene. To this end, we propose Position-wise\nInjection with eXact Estimated Levels (PIXEL), a position-wise activation\nsteering framework that, in contrast to prior work, learns a property-aligned\nsubspace from dual views (tail-averaged and end-token) and selects intervention\nstrength via a constrained geometric objective with a closed-form solution,\nthereby adapting to token-level sensitivity without global hyperparameter\ntuning. PIXEL further performs sample-level orthogonal residual calibration to\nrefine the global attribute direction and employs a lightweight\nposition-scanning routine to identify receptive injection sites. We\nadditionally provide representation-level guarantees for the\nminimal-intervention rule, supporting reliable alignment. Across diverse models\nand evaluation paradigms, PIXEL consistently improves attribute alignment while\npreserving model general capabilities, offering a practical and principled\nmethod for LLMs' controllable generation. Our code is available at\nhttps://github.com/V1centNevwake/PIXEL-Adaptive-Steering", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a PIXEL \u7684\u6fc0\u6d3b steering \u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u53ef\u9760\u884c\u4e3a\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7c97\u7565\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u5bf9\u5728\u54ea\u91cc\u8fdb\u884c steering \u4ee5\u53ca\u5982\u4f55\u6709\u529b\u5e72\u9884\u7684\u539f\u5219\u6027\u89e3\u91ca\u3002", "method": "PIXEL \u4ece\u53cc\u91cd\u89c6\u89d2\uff08\u5c3e\u90e8\u5e73\u5747\u548c\u7ed3\u675f token\uff09\u5b66\u4e60\u5c5e\u6027\u5bf9\u9f50\u7684\u5b50\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u5177\u6709\u95ed\u5f0f\u89e3\u7684\u7ea6\u675f\u51e0\u4f55\u76ee\u6807\u9009\u62e9\u5e72\u9884\u5f3a\u5ea6\uff0c\u4ece\u800c\u9002\u5e94 token \u7ea7\u522b\u7684\u654f\u611f\u6027\uff0c\u800c\u65e0\u9700\u5168\u5c40\u8d85\u53c2\u6570\u8c03\u6574\u3002PIXEL \u8fdb\u4e00\u6b65\u6267\u884c\u6837\u672c\u7ea7\u522b\u6b63\u4ea4\u6b8b\u5dee\u6821\u51c6\u4ee5\u7ec6\u5316\u5168\u5c40\u5c5e\u6027\u65b9\u5411\uff0c\u5e76\u91c7\u7528\u8f7b\u91cf\u7ea7\u4f4d\u7f6e\u626b\u63cf\u7a0b\u5e8f\u6765\u8bc6\u522b\u63a5\u53d7\u6027\u6ce8\u5165\u7ad9\u70b9\u3002", "result": "PIXEL \u5728\u5404\u79cd\u6a21\u578b\u548c\u8bc4\u4f30\u8303\u4f8b\u4e2d\uff0c\u59cb\u7ec8\u5982\u4e00\u5730\u63d0\u9ad8\u5c5e\u6027\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u4e00\u822c\u80fd\u529b\u3002", "conclusion": "PIXEL \u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u6709\u539f\u5219\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e LLM \u7684\u53ef\u63a7\u751f\u6210\u3002"}}
{"id": "2510.09694", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09694", "abs": "https://arxiv.org/abs/2510.09694", "authors": ["Xiaodan Li", "Mengjie Wu", "Yao Zhu", "Yunna Lv", "YueFeng Chen", "Cen Chen", "Jianmei Guo", "Hui Xue"], "title": "Kelp: A Streaming Safeguard for Large Models via Latent Dynamics-Guided Risk Detection", "comment": null, "summary": "Large models (LMs) are powerful content generators, yet their open-ended\nnature can also introduce potential risks, such as generating harmful or biased\ncontent. Existing guardrails mostly perform post-hoc detection that may expose\nunsafe content before it is caught, and the latency constraints further push\nthem toward lightweight models, limiting detection accuracy. In this work, we\npropose Kelp, a novel plug-in framework that enables streaming risk detection\nwithin the LM generation pipeline. Kelp leverages intermediate LM hidden states\nthrough a Streaming Latent Dynamics Head (SLD), which models the temporal\nevolution of risk across the generated sequence for more accurate real-time\nrisk detection. To ensure reliable streaming moderation in real applications,\nwe introduce an Anchored Temporal Consistency (ATC) loss to enforce monotonic\nharm predictions by embedding a benign-then-harmful temporal prior. Besides,\nfor a rigorous evaluation of streaming guardrails, we also present\nStreamGuardBench-a model-grounded benchmark featuring on-the-fly responses from\neach protected model, reflecting real-world streaming scenarios in both text\nand vision-language tasks. Across diverse models and datasets, Kelp\nconsistently outperforms state-of-the-art post-hoc guardrails and prior plug-in\nprobes (15.61% higher average F1), while using only 20M parameters and adding\nless than 0.5 ms of per-token latency.", "AI": {"tldr": "Kelp\u662f\u4e00\u4e2a\u5728LM\u751f\u6210\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u6d41\u5f0f\u98ce\u9669\u68c0\u6d4b\u7684\u63d2\u4ef6\u6846\u67b6\uff0c\u5b83\u5229\u7528\u4e2d\u95f4LM\u9690\u85cf\u72b6\u6001\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u635f\u5931\u6765\u63d0\u9ad8\u98ce\u9669\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u9632\u62a4\u63aa\u65bd\u4e3b\u8981\u6267\u884c\u4e8b\u540e\u68c0\u6d4b\uff0c\u53ef\u80fd\u5728\u6355\u83b7\u4e4b\u524d\u66b4\u9732\u4e0d\u5b89\u5168\u5185\u5bb9\uff0c\u5e76\u4e14\u5ef6\u8fdf\u7ea6\u675f\u8fdb\u4e00\u6b65\u5c06\u5b83\u4eec\u63a8\u5411\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u4ece\u800c\u9650\u5236\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faKelp\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u63d2\u4ef6\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u6d41\u5f0f\u6f5c\u5728\u52a8\u6001\u5934\u90e8\uff08SLD\uff09\u5728LM\u751f\u6210\u7ba1\u9053\u4e2d\u5b9e\u73b0\u6d41\u5f0f\u98ce\u9669\u68c0\u6d4b\uff0c\u8be5\u5934\u90e8\u5bf9\u751f\u6210\u5e8f\u5217\u4e2d\u98ce\u9669\u7684\u65f6\u95f4\u6f14\u53d8\u8fdb\u884c\u5efa\u6a21\uff0c\u4ee5\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u5b9e\u65f6\u98ce\u9669\u68c0\u6d4b\u3002\u4e3a\u4e86\u786e\u4fdd\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u9760\u7684\u6d41\u5f0f\u5ba1\u6838\uff0c\u5f15\u5165\u951a\u5b9a\u65f6\u95f4\u4e00\u81f4\u6027\uff08ATC\uff09\u635f\u5931\uff0c\u901a\u8fc7\u5d4c\u5165\u826f\u6027\u5230\u6709\u5bb3\u7684\u65f6\u95f4\u5148\u9a8c\u6765\u5f3a\u5236\u6267\u884c\u5355\u8c03\u7684\u6709\u5bb3\u9884\u6d4b\u3002", "result": "Kelp\u5728\u5404\u79cd\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4e8b\u540e\u9632\u62a4\u63aa\u65bd\u548c\u5148\u524d\u7684\u63d2\u4ef6\u63a2\u9488\uff08\u5e73\u5747F1\u9ad8\u51fa15.61%\uff09\uff0c\u540c\u65f6\u4ec5\u4f7f\u752820M\u53c2\u6570\u5e76\u589e\u52a0\u4e0d\u52300.5\u6beb\u79d2\u7684\u6bcftoken\u5ef6\u8fdf\u3002", "conclusion": "Kelp\u662f\u4e00\u79cd\u6709\u6548\u7684\u6d41\u5f0f\u98ce\u9669\u68c0\u6d4b\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u4e0d\u663e\u8457\u589e\u52a0\u8ba1\u7b97\u8d1f\u62c5\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u98ce\u9669\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2510.11483", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11483", "abs": "https://arxiv.org/abs/2510.11483", "authors": ["Heydar Soudani", "Hamed Zamani", "Faegheh Hasibi"], "title": "Uncertainty Quantification for Retrieval-Augmented Reasoning", "comment": null, "summary": "Retrieval-augmented reasoning (RAR) is a recent evolution of\nretrieval-augmented generation (RAG) that employs multiple reasoning steps for\nretrieval and generation. While effective for some complex queries, RAR remains\nvulnerable to errors and misleading outputs. Uncertainty quantification (UQ)\noffers methods to estimate the confidence of systems' outputs. These methods,\nhowever, often handle simple queries with no retrieval or single-step\nretrieval, without properly handling RAR setup. Accurate estimation of UQ for\nRAR requires accounting for all sources of uncertainty, including those arising\nfrom retrieval and generation. In this paper, we account for all these sources\nand introduce Retrieval-Augmented Reasoning Consistency (R2C)--a novel UQ\nmethod for RAR. The core idea of R2C is to perturb the multi-step reasoning\nprocess by applying various actions to reasoning steps. These perturbations\nalter the retriever's input, which shifts its output and consequently modifies\nthe generator's input at the next step. Through this iterative feedback loop,\nthe retriever and generator continuously reshape one another's inputs, enabling\nus to capture uncertainty arising from both components. Experiments on five\npopular RAR systems across diverse QA datasets show that R2C improves AUROC by\nover 5% on average compared to the state-of-the-art UQ baselines. Extrinsic\nevaluations using R2C as an external signal further confirm its effectiveness\nfor two downstream tasks: in Abstention, it achieves ~5% gains in both\nF1Abstain and AccAbstain; in Model Selection, it improves the exact match by\n~7% over single models and ~3% over selection methods.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u65b9\u6cd5\uff0c\u540d\u4e3aRetrieval-Augmented Reasoning Consistency (R2C)\uff0c\u7528\u4e8e\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\uff08RAR\uff09\u7cfb\u7edf\u3002R2C\u901a\u8fc7\u6270\u52a8\u591a\u6b65\u63a8\u7406\u8fc7\u7a0b\u6765\u6355\u83b7\u68c0\u7d22\u548c\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u65e0\u6cd5\u5f88\u597d\u5730\u5904\u7406RAR\u7cfb\u7edf\uff0c\u56e0\u4e3a\u5b83\u4eec\u901a\u5e38\u53ea\u5904\u7406\u7b80\u5355\u7684\u67e5\u8be2\u6216\u5355\u6b65\u68c0\u7d22\uff0c\u800c\u5ffd\u7565\u4e86\u68c0\u7d22\u548c\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u6240\u6709\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\u3002", "method": "R2C\u7684\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u5bf9\u63a8\u7406\u6b65\u9aa4\u5e94\u7528\u5404\u79cd\u64cd\u4f5c\u6765\u6270\u52a8\u591a\u6b65\u63a8\u7406\u8fc7\u7a0b\uff0c\u4ece\u800c\u6539\u53d8\u68c0\u7d22\u5668\u7684\u8f93\u5165\uff0c\u5e76\u6700\u7ec8\u6539\u53d8\u751f\u6210\u5668\u7684\u8f93\u5165\u3002\u901a\u8fc7\u8fd9\u79cd\u8fed\u4ee3\u53cd\u9988\u5faa\u73af\uff0cR2C\u53ef\u4ee5\u6355\u83b7\u6765\u81ea\u68c0\u7d22\u5668\u548c\u751f\u6210\u5668\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u4e94\u4e2a\u6d41\u884c\u7684RAR\u7cfb\u7edf\u548c\u5404\u79cdQA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cR2C\u7684AUROC\u6bd4\u6700\u5148\u8fdb\u7684UQ\u57fa\u7ebf\u5e73\u5747\u63d0\u9ad8\u4e865%\u4ee5\u4e0a\u3002\u6b64\u5916\uff0c\u5728\u4e24\u4e2a\u4e0b\u6e38\u4efb\u52a1\uff08Abstention\u548cModel Selection\uff09\u4e2d\u7684\u5916\u5728\u8bc4\u4f30\u4e5f\u8bc1\u5b9e\u4e86R2C\u7684\u6709\u6548\u6027\u3002", "conclusion": "R2C\u662f\u4e00\u79cd\u6709\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8RAR\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2510.09887", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09887", "abs": "https://arxiv.org/abs/2510.09887", "authors": ["Yijin Ni", "Peng Qi"], "title": "Abductive Preference Learning", "comment": null, "summary": "Frontier large language models such as GPT-5 and Claude Sonnet remain prone\nto overconfidence even after alignment through Reinforcement Learning with\nHuman Feedback (RLHF) and Direct Preference Optimization (DPO). For instance,\nthey tend to offer the same conservative answer \"No\" to both questions \"Can I\neat the [food / potato chips] that has been left out overnight?\" despite the\nlatter requiring no refridgeration for safe consumption. We find that this\nfailure is potentially attributed to a limitation of existing preference\nlearning: it emphasizes selecting the correct response for a given prompt,\nwhile neglecting counterfactual prompts that should alter the response.\n  To address this limitation, we propose abductive preference learning, a\nfine-tuning paradigm that reverses the conventional conditioning by learning\npreferences over prompts given a response. To validate this idea, we construct\nan abductive dataset derived from the HaluEval QA benchmark with 1,001 entries,\nimplementing abductive DPO and its variant DPOP. Experiments reveal\ncomplementary strengths: standard methods improve response selection, abductive\nmethods improve prompt discrimination, while a multitask objective unifies\nboth. On the abductive dataset, multitask DPOP boosts accuracy from $90.0\\%$ to\n$99.5\\%$ in response selection and $54.7\\%$ to $85.0\\%$ in prompt\ndiscrimination, with qualitative evidence highlighting improved sensitivity to\nprompt differences. Finally, evaluation on AlpacaEval shows multitask DPOP\nimproves win rate (from $5.26\\%$ to $6.17\\%$), confirming that abductive\npreference learning preserves the benefits of conventional preference\noptimization while addressing the overlooked challenge of counterfactual\nprompts.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u901a\u8fc7 RLHF \u548c DPO \u8fdb\u884c\u5bf9\u9f50\u540e\uff0c\u4ecd\u7136\u5bb9\u6613\u8fc7\u5ea6\u81ea\u4fe1\u3002\u73b0\u6709\u504f\u597d\u5b66\u4e60\u7684\u5c40\u9650\u6027\u5728\u4e8e\uff0c\u5b83\u5f3a\u8c03\u4e3a\u7ed9\u5b9a\u7684\u63d0\u793a\u9009\u62e9\u6b63\u786e\u7684\u54cd\u5e94\uff0c\u800c\u5ffd\u7565\u4e86\u5e94\u8be5\u6539\u53d8\u54cd\u5e94\u7684\u53cd\u4e8b\u5b9e\u63d0\u793a\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6eaf\u56e0\u504f\u597d\u5b66\u4e60\uff0c\u8fd9\u662f\u4e00\u79cd\u901a\u8fc7\u5b66\u4e60\u7ed9\u5b9a\u54cd\u5e94\u7684\u63d0\u793a\u504f\u597d\u6765\u53cd\u8f6c\u4f20\u7edf\u6761\u4ef6\u53cd\u5c04\u7684\u5fae\u8c03\u8303\u5f0f\u3002", "motivation": "\u73b0\u6709\u504f\u597d\u5b66\u4e60\u5f3a\u8c03\u4e3a\u7ed9\u5b9a\u7684\u63d0\u793a\u9009\u62e9\u6b63\u786e\u7684\u54cd\u5e94\uff0c\u800c\u5ffd\u7565\u4e86\u5e94\u8be5\u6539\u53d8\u54cd\u5e94\u7684\u53cd\u4e8b\u5b9e\u63d0\u793a\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u6eaf\u56e0\u504f\u597d\u5b66\u4e60\uff0c\u8fd9\u662f\u4e00\u79cd\u901a\u8fc7\u5b66\u4e60\u7ed9\u5b9a\u54cd\u5e94\u7684\u63d0\u793a\u504f\u597d\u6765\u53cd\u8f6c\u4f20\u7edf\u6761\u4ef6\u53cd\u5c04\u7684\u5fae\u8c03\u8303\u5f0f\u3002\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u6765\u81ea HaluEval QA \u57fa\u51c6\u7684\u6eaf\u56e0\u6570\u636e\u96c6\uff0c\u5305\u542b 1,001 \u4e2a\u6761\u76ee\uff0c\u5b9e\u73b0\u4e86\u6eaf\u56e0 DPO \u53ca\u5176\u53d8\u4f53 DPOP\u3002", "result": "\u6807\u51c6\u65b9\u6cd5\u6539\u8fdb\u4e86\u54cd\u5e94\u9009\u62e9\uff0c\u6eaf\u56e0\u65b9\u6cd5\u6539\u8fdb\u4e86\u63d0\u793a\u8fa8\u522b\uff0c\u800c\u591a\u4efb\u52a1\u76ee\u6807\u7edf\u4e00\u4e86\u4e24\u8005\u3002\u5728\u6eaf\u56e0\u6570\u636e\u96c6\u4e0a\uff0c\u591a\u4efb\u52a1 DPOP \u5c06\u54cd\u5e94\u9009\u62e9\u7684\u51c6\u786e\u7387\u4ece 90.0% \u63d0\u9ad8\u5230 99.5%\uff0c\u5e76\u5c06\u63d0\u793a\u8fa8\u522b\u7684\u51c6\u786e\u7387\u4ece 54.7% \u63d0\u9ad8\u5230 85.0%\u3002", "conclusion": "\u6eaf\u56e0\u504f\u597d\u5b66\u4e60\u4fdd\u7559\u4e86\u4f20\u7edf\u504f\u597d\u4f18\u5316\u7684\u4f18\u52bf\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u53cd\u4e8b\u5b9e\u63d0\u793a\u8fd9\u4e00\u88ab\u5ffd\u89c6\u7684\u6311\u6218\u3002"}}
{"id": "2510.09912", "categories": ["cs.CV", "cs.AI", "I.4.8; I.2.6; I.2.10; I.5.1; I.5.4"], "pdf": "https://arxiv.org/pdf/2510.09912", "abs": "https://arxiv.org/abs/2510.09912", "authors": ["D. V. Brovko"], "title": "SpectralCA: Bi-Directional Cross-Attention for Next-Generation UAV Hyperspectral Vision", "comment": "The work consists of three chapters, includes 12 figures, 4 tables,\n  31 references, and 1 appendix. A version of this work has been accepted for\n  presentation at the 2025 IEEE 8th International Conference on Methods and\n  Systems of Navigation and Motion Control", "summary": "The relevance of this research lies in the growing demand for unmanned aerial\nvehicles (UAVs) capable of operating reliably in complex environments where\nconventional navigation becomes unreliable due to interference, poor\nvisibility, or camouflage. Hyperspectral imaging (HSI) provides unique\nopportunities for UAV-based computer vision by enabling fine-grained material\nrecognition and object differentiation, which are critical for navigation,\nsurveillance, agriculture, and environmental monitoring. The aim of this work\nis to develop a deep learning architecture integrating HSI into UAV perception\nfor navigation, object detection, and terrain classification. Objectives\ninclude: reviewing existing HSI methods, designing a hybrid 2D/3D convolutional\narchitecture with spectral-spatial cross-attention, training, and benchmarking.\nThe methodology is based on the modification of the Mobile 3D Vision\nTransformer (MDvT) by introducing the proposed SpectralCA block. This block\nemploys bi-directional cross-attention to fuse spectral and spatial features,\nenhancing accuracy while reducing parameters and inference time. Experimental\nevaluation was conducted on the WHU-Hi-HongHu dataset, with results assessed\nusing Overall Accuracy, Average Accuracy, and the Kappa coefficient. The\nfindings confirm that the proposed architecture improves UAV perception\nefficiency, enabling real-time operation for navigation, object recognition,\nand environmental monitoring tasks.\n  Keywords: SpectralCA, deep learning, computer vision, hyperspectral imaging,\nunmanned aerial vehicle, object detection, semi-supervised learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u5149\u8c31\u56fe\u50cf\u7684\u65e0\u4eba\u673a\u611f\u77e5\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u5bfc\u822a\u3001\u76ee\u6807\u68c0\u6d4b\u548c\u5730\u5f62\u5206\u7c7b\u3002", "motivation": "\u5728\u590d\u6742\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u5bfc\u822a\u53d8\u5f97\u4e0d\u53ef\u9760\uff0c\u800c\u9ad8\u5149\u8c31\u6210\u50cf(HSI)\u4e3a\u65e0\u4eba\u673a\u8ba1\u7b97\u673a\u89c6\u89c9\u63d0\u4f9b\u4e86\u72ec\u7279\u7684\u673a\u4f1a\uff0c\u5b9e\u73b0\u7cbe\u7ec6\u7684\u6750\u6599\u8bc6\u522b\u548c\u76ee\u6807\u533a\u5206\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u63d0\u51fa\u7684 SpectralCA \u6a21\u5757\u4fee\u6539 Mobile 3D Vision Transformer (MDvT)\uff0c\u8be5\u6a21\u5757\u91c7\u7528\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u5149\u8c31\u548c\u7a7a\u95f4\u7279\u5f81\u3002", "result": "\u5728 WHU-Hi-HongHu \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u67b6\u6784\u63d0\u9ad8\u4e86\u65e0\u4eba\u673a\u611f\u77e5\u6548\u7387\uff0c\u4ece\u800c\u80fd\u591f\u5b9e\u65f6\u64cd\u4f5c\u5bfc\u822a\u3001\u76ee\u6807\u8bc6\u522b\u548c\u73af\u5883\u76d1\u6d4b\u4efb\u52a1\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u67b6\u6784\u63d0\u9ad8\u4e86\u65e0\u4eba\u673a\u611f\u77e5\u6548\u7387\uff0c\u80fd\u591f\u5b9e\u65f6\u64cd\u4f5c\u5bfc\u822a\u3001\u76ee\u6807\u8bc6\u522b\u548c\u73af\u5883\u76d1\u6d4b\u4efb\u52a1\u3002"}}
{"id": "2510.10207", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10207", "abs": "https://arxiv.org/abs/2510.10207", "authors": ["Yujian Zhang", "Keyu Chen", "Zhifeng Shen", "Ruizhi Qiao", "Xing Sun"], "title": "Adaptive Dual Reasoner: Large Reasoning Models Can Think Efficiently by Hybrid Reasoning", "comment": null, "summary": "Although Long Reasoning Models (LRMs) have achieved superior performance on\nvarious reasoning scenarios, they often suffer from increased computational\ncosts and inference latency caused by overthinking. To address these\nlimitations, we propose Adaptive Dual Reasoner, which supports two reasoning\nmodes: fast thinking and slow thinking. ADR dynamically alternates between\nthese modes based on the contextual complexity during reasoning. ADR is trained\nin two stages: (1) A cold-start stage using supervised fine-tuning (SFT) to\nequip the model with the ability to integrate both fast and slow reasoning\nmodes, in which we construct a hybrid reasoning dataset through a dedicated\npipeline to provide large-scale supervision. (2) A reinforcement learning stage\nfor optimizing reasoning effort, where we introduce Entropy-guided Hybrid\nPolicy Optimization EHPO, an RL training framework employing an entropy-guided\ndynamic rollout strategy for branching at high-entropy units and a\ndifficulty-aware penalty to balance fast and slow reasoning. Across challenging\nmathematical reasoning benchmarks, ADR achieves an effective balance between\nreasoning performance and efficiency among state-of-the-art approaches.\nSpecifically, ADR yields a performance gain of up to 6.1%, while reducing the\nreasoning output length by 49.5% to 59.3%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u81ea\u9002\u5e94\u53cc\u91cd\u63a8\u7406\u5668\uff08ADR\uff09\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u652f\u6301\u5feb\u901f\u601d\u8003\u548c\u6162\u901f\u601d\u8003\u4e24\u79cd\u63a8\u7406\u6a21\u5f0f\uff0c\u5e76\u6839\u636e\u4e0a\u4e0b\u6587\u590d\u6742\u6027\u52a8\u6001\u5207\u6362\u3002", "motivation": "\u957f\u63a8\u7406\u6a21\u578b\uff08LRM\uff09\u5728\u5404\u79cd\u63a8\u7406\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u548c\u63a8\u7406\u5ef6\u8fdf\u8f83\u9ad8\u3002", "method": "ADR\u7684\u8bad\u7ec3\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a(1) \u4f7f\u7528\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u8fdb\u884c\u51b7\u542f\u52a8\uff0c\u4f7f\u6a21\u578b\u5177\u5907\u6574\u5408\u5feb\u901f\u548c\u6162\u901f\u63a8\u7406\u6a21\u5f0f\u7684\u80fd\u529b\uff1b(2) \u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff0c\u4f18\u5316\u63a8\u7406\u5de5\u4f5c\u91cf\uff0c\u5f15\u5165\u71b5\u5f15\u5bfc\u6df7\u5408\u7b56\u7565\u4f18\u5316EHPO\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cADR\u5728\u63a8\u7406\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u6709\u6548\u5e73\u8861\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe6.1%\uff0c\u540c\u65f6\u63a8\u7406\u8f93\u51fa\u957f\u5ea6\u51cf\u5c11\u4e8649.5%\u81f359.3%\u3002", "conclusion": "ADR\u5728\u63a8\u7406\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\u3002"}}
{"id": "2510.09696", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09696", "abs": "https://arxiv.org/abs/2510.09696", "authors": ["Lorenzo Nikiforos", "Charalampos Antoniadis", "Luciano Prono", "Fabio Pareschi", "Riccardo Rovatti", "Gianluca Setti"], "title": "Vanishing Contributions: A Unified Approach to Smoothly Transition Neural Models into Compressed Form", "comment": "Code available at https://github.com/foros15/vanishing-contributions", "summary": "The increasing scale of deep neural networks has led to a growing need for\ncompression techniques such as pruning, quantization, and low-rank\ndecomposition. While these methods are very effective in reducing memory,\ncomputation and energy consumption, they often introduce severe accuracy\ndegradation when applied directly. We introduce Vanishing Contributions (VCON),\na general approach for smoothly transitioning neural models into compressed\nform. Rather than replacing the original network directly with its compressed\nversion, VCON executes the two in parallel during fine-tuning. The contribution\nof the original (uncompressed) model is progressively reduced, while that of\nthe compressed model is gradually increased. This smooth transition allows the\nnetwork to adapt over time, improving stability and mitigating accuracy\ndegradation. We evaluate VCON across computer vision and natural language\nprocessing benchmarks, in combination with multiple compression strategies.\nAcross all scenarios, VCON leads to consistent improvements: typical gains\nexceed 3%, while some configuration exhibits accuracy boosts of 20%. VCON thus\nprovides a generalizable method that can be applied to the existing compression\ntechniques, with evidence of consistent gains across multiple benchmarks.", "AI": {"tldr": "VCON\u901a\u8fc7\u5728\u5fae\u8c03\u671f\u95f4\u5e73\u6ed1\u8fc7\u6e21\u5230\u538b\u7f29\u5f62\u5f0f\u6765\u63d0\u9ad8\u538b\u7f29\u6a21\u578b\u7684\u7cbe\u5ea6\u3002", "motivation": "\u538b\u7f29\u6280\u672f\u5728\u51cf\u5c11\u5185\u5b58\u3001\u8ba1\u7b97\u548c\u80fd\u6e90\u6d88\u8017\u65b9\u9762\u975e\u5e38\u6709\u6548\uff0c\u4f46\u901a\u5e38\u4f1a\u5f15\u5165\u4e25\u91cd\u7684\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "VCON\u5e76\u884c\u6267\u884c\u539f\u59cb\u6a21\u578b\u548c\u538b\u7f29\u6a21\u578b\uff0c\u9010\u6e10\u51cf\u5c11\u539f\u59cb\u6a21\u578b\u7684\u8d21\u732e\uff0c\u540c\u65f6\u9010\u6e10\u589e\u52a0\u538b\u7f29\u6a21\u578b\u7684\u8d21\u732e\u3002", "result": "VCON\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u7ed3\u5408\u591a\u79cd\u538b\u7f29\u7b56\u7565\uff0c\u59cb\u7ec8\u5982\u4e00\u5730\u5e26\u6765\u6539\u8fdb\uff1a\u5178\u578b\u589e\u76ca\u8d85\u8fc73%\uff0c\u67d0\u4e9b\u914d\u7f6e\u8868\u73b0\u51fa20%\u7684\u7cbe\u5ea6\u63d0\u5347\u3002", "conclusion": "VCON\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5e94\u7528\u4e8e\u73b0\u6709\u7684\u538b\u7f29\u6280\u672f\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc1\u660e\u4e86\u4e00\u81f4\u7684\u6536\u76ca\u3002"}}
{"id": "2510.11560", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.11560", "abs": "https://arxiv.org/abs/2510.11560", "authors": ["Elisabeth Kirsten", "Jost Grosse Perdekamp", "Mihir Upadhyay", "Krishna P. Gummadi", "Muhammad Bilal Zafar"], "title": "Characterizing Web Search in The Age of Generative AI", "comment": null, "summary": "The advent of LLMs has given rise to a new type of web search: Generative\nsearch, where LLMs retrieve web pages related to a query and generate a single,\ncoherent text as a response. This output modality stands in stark contrast to\ntraditional web search, where results are returned as a ranked list of\nindependent web pages. In this paper, we ask: Along what dimensions do\ngenerative search outputs differ from traditional web search? We compare\nGoogle, a traditional web search engine, with four generative search engines\nfrom two providers (Google and OpenAI) across queries from four domains. Our\nanalysis reveals intriguing differences. Most generative search engines cover a\nwider range of sources compared to web search. Generative search engines vary\nin the degree to which they rely on internal knowledge contained within the\nmodel parameters v.s. external knowledge retrieved from the web. Generative\nsearch engines surface varying sets of concepts, creating new opportunities for\nenhancing search diversity and serendipity. Our results also highlight the need\nfor revisiting evaluation criteria for web search in the age of Generative AI.", "AI": {"tldr": "LLMs \u9a71\u52a8\u4e86\u4e00\u79cd\u65b0\u7684\u7f51\u7edc\u641c\u7d22\u65b9\u5f0f\uff1a\u751f\u6210\u5f0f\u641c\u7d22\uff0c\u5b83\u4e0e\u4f20\u7edf\u7684\u8fd4\u56de\u72ec\u7acb\u7f51\u9875\u5217\u8868\u7684\u7f51\u7edc\u641c\u7d22\u5f62\u6210\u5bf9\u6bd4\u3002\u672c\u6587\u65e8\u5728\u6bd4\u8f83\u751f\u6210\u5f0f\u641c\u7d22\u4e0e\u4f20\u7edf\u7f51\u7edc\u641c\u7d22\u7684\u4e0d\u540c\u4e4b\u5904\u3002", "motivation": "\u63a2\u8ba8\u751f\u6210\u5f0f\u641c\u7d22\u4e0e\u4f20\u7edf\u7f51\u7edc\u641c\u7d22\u5728\u7f51\u7edc\u641c\u7d22\u9886\u57df\u7684\u5dee\u5f02\uff0c\u4ee5\u53ca\u5728\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u65f6\u4ee3\u91cd\u65b0\u8bc4\u4f30\u7f51\u7edc\u641c\u7d22\u8bc4\u4f30\u6807\u51c6\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4 Google \u548c\u6765\u81ea Google \u53ca OpenAI \u7684\u56db\u4e2a\u751f\u6210\u5f0f\u641c\u7d22\u5f15\u64ce\uff0c\u5206\u6790\u5b83\u4eec\u5728\u56db\u4e2a\u9886\u57df\u67e5\u8be2\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u751f\u6210\u5f0f\u641c\u7d22\u8986\u76d6\u66f4\u5e7f\u6cdb\u7684\u6765\u6e90\uff0c\u5bf9\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u548c\u5916\u90e8\u77e5\u8bc6\u7684\u4f9d\u8d56\u7a0b\u5ea6\u4e0d\u540c\uff0c\u5e76\u5448\u73b0\u4e0d\u540c\u7684\u6982\u5ff5\u96c6\u5408\uff0c\u4ece\u800c\u521b\u9020\u4e86\u589e\u5f3a\u641c\u7d22\u591a\u6837\u6027\u548c\u5076\u7136\u6027\u7684\u65b0\u673a\u4f1a\u3002", "conclusion": "\u5f3a\u8c03\u5728\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u65f6\u4ee3\uff0c\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u7f51\u7edc\u641c\u7d22\u7684\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2510.09893", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09893", "abs": "https://arxiv.org/abs/2510.09893", "authors": ["Guanming Chen", "Lingzhi Shen", "Xiaohao Cai", "Imran Razzak", "Shoaib Jameel"], "title": "HIPPD: Brain-Inspired Hierarchical Information Processing for Personality Detection", "comment": null, "summary": "Personality detection from text aims to infer an individual's personality\ntraits based on linguistic patterns. However, existing machine learning\napproaches often struggle to capture contextual information spanning multiple\nposts and tend to fall short in extracting representative and robust features\nin semantically sparse environments. This paper presents HIPPD, a\nbrain-inspired framework for personality detection that emulates the\nhierarchical information processing of the human brain. HIPPD utilises a large\nlanguage model to simulate the cerebral cortex, enabling global semantic\nreasoning and deep feature abstraction. A dynamic memory module, modelled after\nthe prefrontal cortex, performs adaptive gating and selective retention of\ncritical features, with all adjustments driven by dopaminergic prediction error\nfeedback. Subsequently, a set of specialised lightweight models, emulating the\nbasal ganglia, are dynamically routed via a strict winner-takes-all mechanism\nto capture the personality-related patterns they are most proficient at\nrecognising. Extensive experiments on the Kaggle and Pandora datasets\ndemonstrate that HIPPD consistently outperforms state-of-the-art baselines.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHIPPD\u7684\u53d7\u5927\u8111\u542f\u53d1\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6587\u672c\u4e2d\u68c0\u6d4b\u4eba\u683c\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u8de8\u591a\u4e2a\u5e16\u5b50\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u4e14\u5728\u8bed\u4e49\u7a00\u758f\u7684\u73af\u5883\u4e2d\u63d0\u53d6\u4ee3\u8868\u6027\u548c\u9c81\u68d2\u6027\u7279\u5f81\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "HIPPD\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u5927\u8111\u76ae\u5c42\uff0c\u5b9e\u73b0\u5168\u5c40\u8bed\u4e49\u63a8\u7406\u548c\u6df1\u5ea6\u7279\u5f81\u62bd\u8c61\uff1b\u4e00\u4e2a\u52a8\u6001\u8bb0\u5fc6\u6a21\u5757\uff0c\u6a21\u4eff\u524d\u989d\u53f6\u76ae\u5c42\uff0c\u6267\u884c\u81ea\u9002\u5e94\u95e8\u63a7\u548c\u9009\u62e9\u6027\u4fdd\u7559\u5173\u952e\u7279\u5f81\uff1b\u4e00\u7ec4\u4e13\u95e8\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u6a21\u4eff\u57fa\u5e95\u795e\u7ecf\u8282\uff0c\u901a\u8fc7\u4e25\u683c\u7684\u80dc\u8005\u5168\u5f97\u673a\u5236\u8fdb\u884c\u52a8\u6001\u8def\u7531\uff0c\u4ee5\u6355\u6349\u5b83\u4eec\u6700\u64c5\u957f\u8bc6\u522b\u7684\u4e0e\u4eba\u683c\u76f8\u5173\u7684\u6a21\u5f0f\u3002", "result": "\u5728Kaggle\u548cPandora\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cHIPPD\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u3002", "conclusion": "HIPPD\u6846\u67b6\u5728\u4eba\u683c\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.09924", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09924", "abs": "https://arxiv.org/abs/2510.09924", "authors": ["Renjie Li", "Zihao Zhu", "Xiaoyu Wang", "Zhengzhong Tu"], "title": "HeadsUp! High-Fidelity Portrait Image Super-Resolution", "comment": null, "summary": "Portrait pictures, which typically feature both human subjects and natural\nbackgrounds, are one of the most prevalent forms of photography on social\nmedia. Existing image super-resolution (ISR) techniques generally focus either\non generic real-world images or strictly aligned facial images (i.e., face\nsuper-resolution). In practice, separate models are blended to handle portrait\nphotos: the face specialist model handles the face region, and the general\nmodel processes the rest. However, these blending approaches inevitably\nintroduce blending or boundary artifacts around the facial regions due to\ndifferent model training recipes, while human perception is particularly\nsensitive to facial fidelity. To overcome these limitations, we study the\nportrait image supersolution (PortraitISR) problem, and propose HeadsUp, a\nsingle-step diffusion model that is capable of seamlessly restoring and\nupscaling portrait images in an end-to-end manner. Specifically, we build our\nmodel on top of a single-step diffusion model and develop a face supervision\nmechanism to guide the model in focusing on the facial region. We then\nintegrate a reference-based mechanism to help with identity restoration,\nreducing face ambiguity in low-quality face restoration. Additionally, we have\nbuilt a high-quality 4K portrait image ISR dataset dubbed PortraitSR-4K, to\nsupport model training and benchmarking for portrait images. Extensive\nexperiments show that HeadsUp achieves state-of-the-art performance on the\nPortraitISR task while maintaining comparable or higher performance on both\ngeneral image and aligned face datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aHeadsUp\u7684\u5355\u6b65\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u65e0\u7f1d\u5730\u6062\u590d\u548c\u653e\u5927\u8096\u50cf\u56fe\u50cf\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6280\u672f\u901a\u5e38\u53ea\u5173\u6ce8\u901a\u7528\u56fe\u50cf\u6216\u4e25\u683c\u5bf9\u9f50\u7684\u9762\u90e8\u56fe\u50cf\uff0c\u800c\u4eba\u50cf\u7167\u7247\u540c\u65f6\u5305\u542b\u4eba\u50cf\u548c\u81ea\u7136\u80cc\u666f\uff0c\u5c06\u4e0d\u540c\u6a21\u578b\u6df7\u5408\u5904\u7406\u4eba\u50cf\u7167\u7247\u4f1a\u5bfc\u81f4\u9762\u90e8\u533a\u57df\u5468\u56f4\u51fa\u73b0\u6df7\u5408\u6216\u8fb9\u754c\u4f2a\u5f71\u3002", "method": "\u8be5\u6a21\u578b\u57fa\u4e8e\u5355\u6b65\u6269\u6563\u6a21\u578b\u6784\u5efa\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u9762\u90e8\u76d1\u7763\u673a\u5236\uff0c\u4ee5\u5f15\u5bfc\u6a21\u578b\u5173\u6ce8\u9762\u90e8\u533a\u57df\u3002\u540c\u65f6\uff0c\u96c6\u6210\u4e86\u4e00\u79cd\u57fa\u4e8e\u53c2\u8003\u7684\u673a\u5236\uff0c\u4ee5\u5e2e\u52a9\u8fdb\u884c\u8eab\u4efd\u6062\u590d\uff0c\u51cf\u5c11\u4f4e\u8d28\u91cf\u9762\u90e8\u6062\u590d\u4e2d\u7684\u9762\u90e8\u6a21\u7cca\u3002", "result": "HeadsUp\u5728PortraitISR\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u901a\u7528\u56fe\u50cf\u548c\u5bf9\u9f50\u9762\u90e8\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u4e86\u76f8\u5f53\u6216\u66f4\u9ad8\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4eba\u50cf\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u5e76\u5728PortraitISR\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002"}}
{"id": "2510.10238", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10238", "abs": "https://arxiv.org/abs/2510.10238", "authors": ["Zixuan Qin", "Kunlin Lyu", "Qingchen Yu", "Yifan Sun", "Zhaoxin Fan"], "title": "The Achilles' Heel of LLMs: How Altering a Handful of Neurons Can Cripple Language Abilities", "comment": null, "summary": "Large Language Models (LLMs) have become foundational tools in natural\nlanguage processing, powering a wide range of applications and research. Many\nstudies have shown that LLMs share significant similarities with the human\nbrain. Recent neuroscience research has found that a small subset of biological\nneurons in the human brain are crucial for core cognitive functions, which\nraises a fundamental question: do LLMs also contain a small subset of critical\nneurons? In this paper, we investigate this question by proposing a\nPerturbation-based Causal Identification of Critical Neurons method to\nsystematically locate such critical neurons in LLMs. Our findings reveal three\nkey insights: (1) LLMs contain ultra-sparse critical neuron sets. Disrupting\nthese critical neurons can cause a 72B-parameter model with over 1.1 billion\nneurons to completely collapse, with perplexity increasing by up to 20 orders\nof magnitude; (2) These critical neurons are not uniformly distributed, but\ntend to concentrate in the outer layers, particularly within the MLP down\\_proj\ncomponents; (3) Performance degradation exhibits sharp phase transitions,\nrather than a gradual decline, when these critical neurons are disrupted.\nThrough comprehensive experiments across diverse model architectures and\nscales, we provide deeper analysis of these phenomena and their implications\nfor LLM robustness and interpretability. These findings can offer guidance for\ndeveloping more robust model architectures and improving deployment security in\nsafety-critical applications.", "AI": {"tldr": "LLMs, similar to the human brain, may contain a small subset of critical neurons. Disrupting these neurons can severely impact performance.", "motivation": "To investigate whether LLMs contain a small subset of critical neurons, similar to the human brain.", "method": "Proposes a Perturbation-based Causal Identification of Critical Neurons method.", "result": "LLMs contain ultra-sparse critical neuron sets, concentrated in outer layers (MLP down_proj components), and their disruption leads to sharp performance degradation.", "conclusion": "Findings offer guidance for developing more robust model architectures and improving deployment security in safety-critical applications."}}
{"id": "2510.09704", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09704", "abs": "https://arxiv.org/abs/2510.09704", "authors": ["Matthew Schlegel", "Matthew E. Taylor", "Mostafa Farrokhabadi"], "title": "Operator Learning for Power Systems Simulation", "comment": null, "summary": "Time domain simulation, i.e., modeling the system's evolution over time, is a\ncrucial tool for studying and enhancing power system stability and dynamic\nperformance. However, these simulations become computationally intractable for\nrenewable-penetrated grids, due to the small simulation time step required to\ncapture renewable energy resources' ultra-fast dynamic phenomena in the range\nof 1-50 microseconds. This creates a critical need for solutions that are both\nfast and scalable, posing a major barrier for the stable integration of\nrenewable energy resources and thus climate change mitigation. This paper\nexplores operator learning, a family of machine learning methods that learn\nmappings between functions, as a surrogate model for these costly simulations.\nThe paper investigates, for the first time, the fundamental concept of\nsimulation time step-invariance, which enables models trained on coarse time\nsteps to generalize to fine-resolution dynamics. Three operator learning\nmethods are benchmarked on a simple test system that, while not incorporating\npractical complexities of renewable-penetrated grids, serves as a first\nproof-of-concept to demonstrate the viability of time step-invariance. Models\nare evaluated on (i) zero-shot super-resolution, where training is performed on\na coarse simulation time step and inference is performed at super-resolution,\nand (ii) generalization between stable and unstable dynamic regimes. This work\naddresses a key challenge in the integration of renewable energy for the\nmitigation of climate change by benchmarking operator learning methods to model\nphysical systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u7d22\u4e86\u4f7f\u7528\u7b97\u5b50\u5b66\u4e60\u4f5c\u4e3a\u66ff\u4ee3\u6a21\u578b\uff0c\u4ee5\u52a0\u901f\u53ef\u518d\u751f\u80fd\u6e90\u7535\u7f51\u4e2d\u8017\u65f6\u7684\u65f6\u57df\u4eff\u771f\u3002", "motivation": "\u7531\u4e8e\u53ef\u518d\u751f\u80fd\u6e90\u5feb\u901f\u7684\u52a8\u6001\u73b0\u8c61\uff0c\u53ef\u518d\u751f\u80fd\u6e90\u7535\u7f51\u7684\u65f6\u57df\u4eff\u771f\u8ba1\u7b97\u91cf\u5de8\u5927\uff0c\u9700\u8981\u5feb\u901f\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u4e86\u4eff\u771f\u65f6\u95f4\u6b65\u957f\u4e0d\u53d8\u6027\u7684\u57fa\u672c\u6982\u5ff5\uff0c\u5e76\u5bf9\u4e09\u79cd\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u9a8c\u8bc1\u4e86\u65f6\u95f4\u6b65\u957f\u4e0d\u53d8\u6027\u7684\u53ef\u884c\u6027\uff0c\u6a21\u578b\u5728\u96f6\u6837\u672c\u8d85\u5206\u8fa8\u7387\u548c\u7a33\u5b9a/\u4e0d\u7a33\u5b9a\u52a8\u6001\u72b6\u6001\u4e4b\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5bf9\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u6a21\u62df\u7269\u7406\u7cfb\u7edf\uff0c\u4ece\u800c\u5e94\u5bf9\u4e86\u53ef\u518d\u751f\u80fd\u6e90\u5e76\u7f51\u4ee5\u51cf\u7f13\u6c14\u5019\u53d8\u5316\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2510.11589", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11589", "abs": "https://arxiv.org/abs/2510.11589", "authors": ["Shubham Chatterjee", "Jeff Dalton"], "title": "QDER: Query-Specific Document and Entity Representations for Multi-Vector Document Re-Ranking", "comment": "Published in: Proceedings of the 48th International ACM SIGIR\n  Conference on Research and Development in Information Retrieval (SIGIR 2025)", "summary": "Neural IR has advanced through two distinct paths: entity-oriented approaches\nleveraging knowledge graphs and multi-vector models capturing fine-grained\nsemantics. We introduce QDER, a neural re-ranking model that unifies these\napproaches by integrating knowledge graph semantics into a multi-vector model.\nQDER's key innovation lies in its modeling of query-document relationships:\nrather than computing similarity scores on aggregated embeddings, we maintain\nindividual token and entity representations throughout the ranking process,\nperforming aggregation only at the final scoring stage - an approach we call\n\"late aggregation.\" We first transform these fine-grained representations\nthrough learned attention patterns, then apply carefully chosen mathematical\noperations for precise matches. Experiments across five standard benchmarks\nshow that QDER achieves significant performance gains, with improvements of 36%\nin nDCG@20 over the strongest baseline on TREC Robust 2004 and similar\nimprovements on other datasets. QDER particularly excels on difficult queries,\nachieving an nDCG@20 of 0.70 where traditional approaches fail completely\n(nDCG@20 = 0.0), setting a foundation for future work in entity-aware\nretrieval.", "AI": {"tldr": "QDER: a neural re-ranking model unifying knowledge graph semantics and multi-vector models for improved information retrieval.", "motivation": "Combining knowledge graph approaches with multi-vector models to advance neural IR.", "method": "Integrating knowledge graph semantics into a multi-vector model with late aggregation of token and entity representations, learned attention patterns, and mathematical operations for precise matches.", "result": "Significant performance gains across five benchmarks, including a 36% improvement in nDCG@20 on TREC Robust 2004.", "conclusion": "QDER excels on difficult queries, setting a foundation for future work in entity-aware retrieval."}}
{"id": "2510.09913", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09913", "abs": "https://arxiv.org/abs/2510.09913", "authors": ["Shangbin Feng", "Wenhao Yu", "Yike Wang", "Hongming Zhang", "Yulia Tsvetkov", "Dong Yu"], "title": "Don't Throw Away Your Pretrained Model", "comment": null, "summary": "Alignment training has tradeoffs: it helps language models (LMs) gain in\nreasoning and instruction following but might lose out on skills such as\ncreativity and calibration, where unaligned base models are better at. We aim\nto make the best of both worlds through model collaboration, where different\nmodels in the training pipeline collaborate and complement each other. Since LM\nresponses feature interleaving skills that favor different models, we propose\nSwitch Generation, where pretrained and aligned model versions take turns to\n``speak'' in a response sequence. Specifically, we train a switcher LM by\nlearning from outcomes of choosing different models to generate the next\nsegment across diverse queries and contexts. At inference time, the switcher LM\nguides different model checkpoints to dynamically generate the next segment\nwhere their strengths are most needed. Extensive experiments with 8 model\ncollaboration baselines and 18 datasets show that 1) model collaboration\nconsistently outperforms individual models on 16 out of 18 tasks, and 2) Switch\nGeneration further outperforms baselines by 12.9% on average. Further analysis\nreveals that Switch Generation discovers compositional skills to solve problems\nwhere individual models struggle and generalizes to unseen models and tasks,\nreusing and repurposing by-products in expensive model training pipelines that\nare otherwise discarded.", "AI": {"tldr": "\u5bf9\u9f50\u8bad\u7ec3\u5728\u63a8\u7406\u548c\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u6709\u4f18\u52bf\uff0c\u4f46\u5728\u521b\u9020\u529b\u548c\u6821\u51c6\u7b49\u6280\u80fd\u65b9\u9762\u6709\u6240\u6b20\u7f3a\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6a21\u578b\u534f\u4f5c\uff0c\u7ed3\u5408\u4e0d\u540c\u6a21\u578b\u7684\u4f18\u52bf\u3002", "motivation": "\u5bf9\u9f50\u8bad\u7ec3\u540e\u7684\u8bed\u8a00\u6a21\u578b\u5728\u521b\u9020\u529b\u548c\u6821\u51c6\u65b9\u9762\u4e0d\u5982\u672a\u5bf9\u9f50\u7684\u6a21\u578b\u3002\u6a21\u578b\u534f\u4f5c\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u70b9\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSwitch Generation\u7684\u65b9\u6cd5\uff0c\u5176\u4e2d\u9884\u8bad\u7ec3\u548c\u5bf9\u9f50\u7684\u6a21\u578b\u8f6e\u6d41\u751f\u6210\u54cd\u5e94\u5e8f\u5217\u3002\u901a\u8fc7\u5b66\u4e60\u9009\u62e9\u4e0d\u540c\u6a21\u578b\u751f\u6210\u4e0b\u4e00\u4e2a\u7247\u6bb5\u7684\u7ed3\u679c\u6765\u8bad\u7ec3\u4e00\u4e2a\u5207\u6362\u5668LM\u3002", "result": "\u6a21\u578b\u534f\u4f5c\u572818\u4e2a\u4efb\u52a1\u4e2d\u768416\u4e2a\u4e0a\u4f18\u4e8e\u5355\u4e2a\u6a21\u578b\u3002Switch Generation\u5e73\u5747\u4f18\u4e8e\u57fa\u7ebf12.9%\u3002", "conclusion": "Switch Generation\u80fd\u591f\u53d1\u73b0\u7ec4\u5408\u6280\u80fd\u6765\u89e3\u51b3\u5355\u4e2a\u6a21\u578b\u96be\u4ee5\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u5e76\u63a8\u5e7f\u5230\u672a\u89c1\u8fc7\u7684\u6a21\u578b\u548c\u4efb\u52a1\u3002"}}
{"id": "2510.09934", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09934", "abs": "https://arxiv.org/abs/2510.09934", "authors": ["Nilesh Jain", "Elie Alhajjar"], "title": "Denoising Diffusion as a New Framework for Underwater Images", "comment": null, "summary": "Underwater images play a crucial role in ocean research and marine\nenvironmental monitoring since they provide quality information about the\necosystem. However, the complex and remote nature of the environment results in\npoor image quality with issues such as low visibility, blurry textures, color\ndistortion, and noise. In recent years, research in image enhancement has\nproven to be effective but also presents its own limitations, like poor\ngeneralization and heavy reliance on clean datasets. One of the challenges\nherein is the lack of diversity and the low quality of images included in these\ndatasets. Also, most existing datasets consist only of monocular images, a fact\nthat limits the representation of different lighting conditions and angles. In\nthis paper, we propose a new plan of action to overcome these limitations. On\none hand, we call for expanding the datasets using a denoising diffusion model\nto include a variety of image types such as stereo, wide-angled, macro, and\nclose-up images. On the other hand, we recommend enhancing the images using\nControlnet to evaluate and increase the quality of the corresponding datasets,\nand hence improve the study of the marine ecosystem.\n  Tags - Underwater Images, Denoising Diffusion, Marine ecosystem, Controlnet", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u6c34\u4e0b\u56fe\u50cf\u8d28\u91cf\u7684\u65b0\u65b9\u6848\u3002", "motivation": "\u6c34\u4e0b\u56fe\u50cf\u5728\u6d77\u6d0b\u7814\u7a76\u548c\u73af\u5883\u76d1\u6d4b\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u4f46\u7531\u4e8e\u73af\u5883\u7684\u590d\u6742\u6027\u548c\u9065\u8fdc\u6027\uff0c\u56fe\u50cf\u8d28\u91cf\u901a\u5e38\u8f83\u5dee\uff0c\u5b58\u5728\u4f4e\u80fd\u89c1\u5ea6\u3001\u6a21\u7cca\u7eb9\u7406\u3001\u989c\u8272\u5931\u771f\u548c\u566a\u58f0\u7b49\u95ee\u9898\u3002\u73b0\u6709\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u5b58\u5728\u6cdb\u5316\u6027\u5dee\u3001\u8fc7\u5ea6\u4f9d\u8d56\u5e72\u51c0\u6570\u636e\u96c6\u7b49\u5c40\u9650\u6027\uff0c\u4e14\u6570\u636e\u96c6\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u5927\u591a\u53ea\u5305\u542b\u5355\u76ee\u56fe\u50cf\uff0c\u9650\u5236\u4e86\u5bf9\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u548c\u89d2\u5ea6\u7684\u8868\u793a\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u53bb\u566a\u6269\u6563\u6a21\u578b\u6269\u5c55\u6570\u636e\u96c6\uff0c\u4ee5\u5305\u542b\u5404\u79cd\u56fe\u50cf\u7c7b\u578b\uff0c\u5982\u7acb\u4f53\u3001\u5e7f\u89d2\u3001\u5fae\u8ddd\u548c\u7279\u5199\u56fe\u50cf\u3002\u540c\u65f6\uff0c\u63a8\u8350\u4f7f\u7528Controlnet\u589e\u5f3a\u56fe\u50cf\uff0c\u4ee5\u8bc4\u4f30\u548c\u63d0\u9ad8\u76f8\u5e94\u6570\u636e\u96c6\u7684\u8d28\u91cf\u3002", "result": "\u672a\u5728\u6458\u8981\u4e2d\u660e\u786e\u8bf4\u660e\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6269\u5c55\u6570\u636e\u96c6\u548c\u4f7f\u7528Controlnet\u589e\u5f3a\u56fe\u50cf\u6765\u63d0\u9ad8\u6c34\u4e0b\u56fe\u50cf\u7684\u8d28\u91cf\uff0c\u4ece\u800c\u6539\u5584\u6d77\u6d0b\u751f\u6001\u7cfb\u7edf\u7684\u7814\u7a76\u3002"}}
{"id": "2510.10285", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10285", "abs": "https://arxiv.org/abs/2510.10285", "authors": ["Haolang Lu", "Bolun Chu", "WeiYe Fu", "Guoshun Nan", "Junning Liu", "Minghui Pan", "Qiankun Li", "Yi Yu", "Hua Wang", "Kun Wang"], "title": "Mitigating Hallucination in Multimodal Reasoning via Functional Attention Control", "comment": "preprint", "summary": "Multimodal large reasoning models (MLRMs) are rapidly advancing\nvision-language reasoning and are emerging as a foundation for cross-modal\nintelligence. Hallucination remains a persistent failure mode, manifesting\nitself as erroneous reasoning chains and misinterpretation of visual content.\nIn this study, we observe that attention heads exhibit a staged division:\nshallow heads predominantly serve perception, while deeper heads shift toward\nsymbolic reasoning, revealing two major causes of hallucination, namely\nperceptual bias and reasoning drift. To address these issues, we propose a\nlightweight and interpretable two-step plugin, Functional Head Identification\nand Class-conditioned Rescaling, which locates perception- and\nreasoning-oriented heads and regulates their contributions without retraining.\nEvaluations on three real-world MLRMs (Kimi-VL, Ocean-R1, R1-Onevision), six\nbenchmarks across three domains, and four baselines show that our plugin\nachieves an average improvement of 5% and up to 15%, with only <1% additional\ncomputation and 9% of baseline latency. Our approach is completely\nmodel-agnostic and significantly enhances both the reliability and\ninterpretability of the off-the-shelf MLRMs, thereby enabling their safe\ndeployment in high-stakes applications. Our code is available at\nhttps://anonymous.4open.science/r/Functional-Attention-Control.", "AI": {"tldr": "\u591a\u6a21\u6001\u5927\u578b\u63a8\u7406\u6a21\u578b(MLRM)\u5728\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u65b9\u9762\u8fc5\u901f\u53d1\u5c55\uff0c\u4f46\u5e7b\u89c9\u4ecd\u7136\u662f\u4e00\u4e2a\u95ee\u9898\u3002", "motivation": "\u89c2\u5bdf\u5230\u6ce8\u610f\u529b\u5934\u8868\u73b0\u51fa\u9636\u6bb5\u6027\u5212\u5206\uff0c\u6d45\u5c42\u5934\u4e3b\u8981\u7528\u4e8e\u611f\u77e5\uff0c\u800c\u6df1\u5c42\u5934\u8f6c\u5411\u7b26\u53f7\u63a8\u7406\uff0c\u63ed\u793a\u4e86\u5e7b\u89c9\u7684\u4e24\u4e2a\u4e3b\u8981\u539f\u56e0\uff1a\u611f\u77e5\u504f\u5dee\u548c\u63a8\u7406\u6f02\u79fb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u548c\u53ef\u89e3\u91ca\u7684\u4e24\u6b65\u63d2\u4ef6\uff0c\u529f\u80fd\u5934\u8bc6\u522b\u548c\u7c7b\u6761\u4ef6\u91cd\u65b0\u7f29\u653e\uff0c\u5b83\u53ef\u4ee5\u5b9a\u4f4d\u9762\u5411\u611f\u77e5\u548c\u63a8\u7406\u7684\u5934\uff0c\u5e76\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u8c03\u8282\u5b83\u4eec\u7684\u8d21\u732e\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u7684MLRM\uff08Kimi-VL\u3001Ocean-R1\u3001R1-Onevision\uff09\uff0c\u8de8\u4e09\u4e2a\u9886\u57df\u7684\u516d\u4e2a\u57fa\u51c6\u548c\u56db\u4e2a\u57fa\u7ebf\u4e0a\u8fdb\u884c\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u6211\u4eec\u7684\u63d2\u4ef6\u5b9e\u73b0\u4e86\u5e73\u57475%\u548c\u9ad8\u8fbe15%\u7684\u6539\u8fdb\uff0c\u53ea\u6709<1%\u7684\u989d\u5916\u8ba1\u7b97\u548c9%\u7684\u57fa\u7ebf\u5ef6\u8fdf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u662f\u5b8c\u5168\u6a21\u578b\u65e0\u5173\u7684\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u73b0\u6210\u7684MLRM\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4ece\u800c\u4f7f\u5176\u80fd\u591f\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u5b89\u5168\u90e8\u7f72\u3002"}}
{"id": "2510.09705", "categories": ["cs.LG", "cs.CY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.09705", "abs": "https://arxiv.org/abs/2510.09705", "authors": ["Sudip Khadka", "L. S. Paudel"], "title": "A Multi-Component Reward Function with Policy Gradient for Automated Feature Selection with Dynamic Regularization and Bias Mitigation", "comment": null, "summary": "Static feature exclusion strategies often fail to prevent bias when hidden\ndependencies influence the model predictions. To address this issue, we explore\na reinforcement learning (RL) framework that integrates bias mitigation and\nautomated feature selection within a single learning process. Unlike\ntraditional heuristic-driven filter or wrapper approaches, our RL agent\nadaptively selects features using a reward signal that explicitly integrates\npredictive performance with fairness considerations. This dynamic formulation\nallows the model to balance generalization, accuracy, and equity throughout the\ntraining process, rather than rely exclusively on pre-processing adjustments or\npost hoc correction mechanisms. In this paper, we describe the construction of\na multi-component reward function, the specification of the agents action space\nover feature subsets, and the integration of this system with ensemble\nlearning. We aim to provide a flexible and generalizable way to select features\nin environments where predictors are correlated and biases can inadvertently\nre-emerge.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7279\u5f81\u9009\u62e9\u8fc7\u7a0b\u4e2d\u51cf\u8f7b\u504f\u5dee\uff0c\u8be5\u6846\u67b6\u5c06\u9884\u6d4b\u6027\u80fd\u4e0e\u516c\u5e73\u6027\u8003\u8651\u76f8\u7ed3\u5408\u3002", "motivation": "\u9759\u6001\u7279\u5f81\u6392\u9664\u7b56\u7565\u5728\u9690\u85cf\u4f9d\u8d56\u5173\u7cfb\u5f71\u54cd\u6a21\u578b\u9884\u6d4b\u65f6\u901a\u5e38\u65e0\u6cd5\u9632\u6b62\u504f\u5dee\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09agent\u81ea\u9002\u5e94\u5730\u9009\u62e9\u7279\u5f81\uff0c\u8be5agent\u4f7f\u7528\u660e\u786e\u6574\u5408\u9884\u6d4b\u6027\u80fd\u548c\u516c\u5e73\u6027\u8003\u8651\u7684\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u901a\u7528\u7684\u65b9\u6cd5\u6765\u9009\u62e9\u9884\u6d4b\u53d8\u91cf\u76f8\u5173\u7684\u73af\u5883\u4e2d\u7684\u7279\u5f81\uff0c\u5e76\u4e14\u504f\u5dee\u53ef\u80fd\u4f1a\u65e0\u610f\u4e2d\u91cd\u65b0\u51fa\u73b0\u3002", "conclusion": "\u8be5\u6a21\u578b\u53ef\u4ee5\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5e73\u8861\u6cdb\u5316\u3001\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\uff0c\u800c\u4e0d\u662f\u4ec5\u4ec5\u4f9d\u8d56\u4e8e\u9884\u5904\u7406\u8c03\u6574\u6216\u4e8b\u540e\u6821\u6b63\u673a\u5236\u3002"}}
{"id": "2510.11592", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11592", "abs": "https://arxiv.org/abs/2510.11592", "authors": ["Shubham Chatterjee"], "title": "REGENT: Relevance-Guided Attention for Entity-Aware Multi-Vector Neural Re-Ranking", "comment": "To be published in: Proceedings of the 2025 Annual International ACM\n  SIGIR Conference on Research and Development in Information Retrieval in the\n  Asia Pacific Region (SIGIR-AP 2025)", "summary": "Current neural re-rankers often struggle with complex information needs and\nlong, content-rich documents. The fundamental issue is not computational--it is\nintelligent content selection: identifying what matters in lengthy,\nmulti-faceted texts. While humans naturally anchor their understanding around\nkey entities and concepts, neural models process text within rigid token\nwindows, treating all interactions as equally important and missing critical\nsemantic signals. We introduce REGENT, a neural re-ranking model that mimics\nhuman-like understanding by using entities as a \"semantic skeleton\" to guide\nattention. REGENT integrates relevance guidance directly into the attention\nmechanism, combining fine-grained lexical matching with high-level semantic\nreasoning. This relevance-guided attention enables the model to focus on\nconceptually important content while maintaining sensitivity to precise term\nmatches. REGENT achieves new state-of-the-art performance in three challenging\ndatasets, providing up to 108% improvement over BM25 and consistently\noutperforming strong baselines including ColBERT and RankVicuna. To our\nknowledge, this is the first work to successfully integrate entity semantics\ndirectly into neural attention, establishing a new paradigm for entity-aware\ninformation retrieval.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aREGENT\u7684\u795e\u7ecf\u91cd\u6392\u5e8f\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u7684\u7406\u89e3\u65b9\u5f0f\uff0c\u5229\u7528\u5b9e\u4f53\u4f5c\u4e3a\u201c\u8bed\u4e49\u9aa8\u67b6\u201d\u6765\u5f15\u5bfc\u6ce8\u610f\u529b\uff0c\u4ece\u800c\u5728\u5904\u7406\u590d\u6742\u4fe1\u606f\u9700\u6c42\u548c\u5185\u5bb9\u4e30\u5bcc\u7684\u957f\u6587\u6863\u65f6\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u795e\u7ecf\u91cd\u6392\u5e8f\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u4fe1\u606f\u9700\u6c42\u548c\u957f\u6587\u6863\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u95ee\u9898\u5728\u4e8e\u65e0\u6cd5\u6709\u6548\u5730\u9009\u62e9\u5173\u952e\u5185\u5bb9\u3002\u8fd9\u4e9b\u6a21\u578b\u5728\u56fa\u5b9atoken\u7a97\u53e3\u5185\u5904\u7406\u6587\u672c\uff0c\u5ffd\u7565\u4e86\u91cd\u8981\u7684\u8bed\u4e49\u4fe1\u53f7\u3002", "method": "REGENT\u6a21\u578b\u5c06\u5b9e\u4f53\u4f5c\u4e3a\u8bed\u4e49\u9aa8\u67b6\u6765\u5f15\u5bfc\u6ce8\u610f\u529b\uff0c\u5e76\u5c06\u76f8\u5173\u6027\u6307\u5bfc\u76f4\u63a5\u6574\u5408\u5230\u6ce8\u610f\u529b\u673a\u5236\u4e2d\uff0c\u7ed3\u5408\u4e86\u7ec6\u7c92\u5ea6\u7684\u8bcd\u6c47\u5339\u914d\u548c\u9ad8\u5c42\u6b21\u7684\u8bed\u4e49\u63a8\u7406\u3002", "result": "REGENT\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684state-of-the-art\u6027\u80fd\uff0c\u76f8\u6bd4BM25\u63d0\u9ad8\u4e86\u9ad8\u8fbe108%\uff0c\u5e76\u4e14\u6301\u7eed\u4f18\u4e8e\u5305\u62ecColBERT\u548cRankVicuna\u5728\u5185\u7684\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u9996\u6b21\u6210\u529f\u5730\u5c06\u5b9e\u4f53\u8bed\u4e49\u76f4\u63a5\u6574\u5408\u5230\u795e\u7ecf\u6ce8\u610f\u529b\u4e2d\uff0c\u4e3a\u5b9e\u4f53\u611f\u77e5\u7684\u4fe1\u606f\u68c0\u7d22\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u8303\u4f8b\u3002"}}
{"id": "2510.09915", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09915", "abs": "https://arxiv.org/abs/2510.09915", "authors": ["Sicong Huang", "Qianqi Yan", "Shengze Wang", "Ian Lane"], "title": "Enhancing Faithfulness in Abstractive Summarization via Span-Level Fine-Tuning", "comment": null, "summary": "Abstractive summarization using large language models (LLMs) has become an\nessential tool for condensing information. However, despite their ability to\ngenerate fluent summaries, these models sometimes produce unfaithful summaries,\nintroducing hallucinations at the word, phrase, or concept level. Existing\nmitigation strategies, such as post-processing corrections or contrastive\nlearning with synthetically generated negative samples, fail to fully address\nthe diverse errors that can occur in LLM-generated summaries. In this paper, we\ninvestigate fine-tuning strategies to reduce the occurrence of unfaithful spans\nin generated summaries. First, we automatically generate summaries for the set\nof source documents in the training set with a variety of LLMs and then use\nGPT-4o to annotate any hallucinations it detects at the span-level. Leveraging\nthese annotations, we fine-tune LLMs with both hallucination-free summaries and\nannotated unfaithful spans to enhance model faithfulness. In this paper, we\nintroduce a new dataset that contains both faithful and unfaithful summaries\nwith span-level labels and we evaluate three techniques to fine-tuning a LLM to\nimprove the faithfulness of the resulting summarization: gradient ascent,\nunlikelihood training, and task vector negation. Experimental results show that\nall three approaches successfully leverage span-level annotations to improve\nfaithfulness, with unlikelihood training being the most effective.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6765\u51cf\u5c11\u751f\u6210\u6458\u8981\u4e2d\u7684\u4e0d\u5fe0\u5b9e\u4fe1\u606f\uff08\u5e7b\u89c9\uff09\u3002", "motivation": "\u73b0\u6709\u7684\u7f13\u89e3\u7b56\u7565\u65e0\u6cd5\u5b8c\u5168\u89e3\u51b3LLM\u751f\u6210\u6458\u8981\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u5404\u79cd\u9519\u8bef\u3002", "method": "1. \u4f7f\u7528\u591a\u79cdLLM\u4e3a\u8bad\u7ec3\u96c6\u4e2d\u7684\u6e90\u6587\u6863\u81ea\u52a8\u751f\u6210\u6458\u8981\uff1b2. \u4f7f\u7528GPT-4o\u5728span\u7ea7\u522b\u6ce8\u91ca\u5176\u68c0\u6d4b\u5230\u7684\u4efb\u4f55\u5e7b\u89c9\uff1b3. \u4f7f\u7528\u5e7b\u89c9\u81ea\u7531\u6458\u8981\u548c\u5e26\u6ce8\u91ca\u7684\u4e0d\u5fe0\u5b9espan\u5fae\u8c03LLM\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u3002", "result": "\u6240\u6709\u7684\u5fae\u8c03\u65b9\u6cd5\u90fd\u6210\u529f\u5730\u5229\u7528\u4e86span\u7ea7\u522b\u7684\u6ce8\u91ca\u6765\u63d0\u9ad8\u53ef\u4fe1\u5ea6\uff0c\u5176\u4e2dunlikelihood training\u662f\u6700\u6709\u6548\u7684\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u5fe0\u5b9e\u548c\u4e0d\u5fe0\u5b9e\u7684\u6458\u8981\uff0c\u4ee5\u53caspan\u7ea7\u522b\u7684\u6807\u7b7e\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528unlikelihood training\u5fae\u8c03LLM\u80fd\u591f\u6700\u6709\u6548\u5730\u63d0\u9ad8\u6458\u8981\u7684\u5fe0\u5b9e\u5ea6\u3002"}}
{"id": "2510.09936", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09936", "abs": "https://arxiv.org/abs/2510.09936", "authors": ["Agampreet Aulakh", "Nils D. Forkert", "Matthias Wilms"], "title": "Semi-disentangled spatiotemporal implicit neural representations of longitudinal neuroimaging data for trajectory classification", "comment": "Accepted at the MICCAI 2025 Learning with Longitudinal Medical Images\n  and Data Workshop", "summary": "The human brain undergoes dynamic, potentially pathology-driven, structural\nchanges throughout a lifespan. Longitudinal Magnetic Resonance Imaging (MRI)\nand other neuroimaging data are valuable for characterizing trajectories of\nchange associated with typical and atypical aging. However, the analysis of\nsuch data is highly challenging given their discrete nature with different\nspatial and temporal image sampling patterns within individuals and across\npopulations. This leads to computational problems for most traditional deep\nlearning methods that cannot represent the underlying continuous biological\nprocess. To address these limitations, we present a new, fully data-driven\nmethod for representing aging trajectories across the entire brain by modelling\nsubject-specific longitudinal T1-weighted MRI data as continuous functions\nusing Implicit Neural Representations (INRs). Therefore, we introduce a novel\nINR architecture capable of partially disentangling spatial and temporal\ntrajectory parameters and design an efficient framework that directly operates\non the INRs' parameter space to classify brain aging trajectories. To evaluate\nour method in a controlled data environment, we develop a biologically grounded\ntrajectory simulation and generate T1-weighted 3D MRI data for 450 healthy and\ndementia-like subjects at regularly and irregularly sampled timepoints. In the\nmore realistic irregular sampling experiment, our INR-based method achieves\n81.3% accuracy for the brain aging trajectory classification task,\noutperforming a standard deep learning baseline model (73.7%).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u7684\u3001\u5b8c\u5168\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8868\u793a\u6574\u4e2a\u5927\u8111\u7684\u8870\u8001\u8f68\u8ff9\u3002", "motivation": "\u4f20\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u8868\u793a\u6f5c\u5728\u7684\u8fde\u7eed\u751f\u7269\u8fc7\u7a0b\uff0c\u7ed9\u7eb5\u5411MRI\u6570\u636e\u7684\u5206\u6790\u5e26\u6765\u4e86\u6311\u6218\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6570\u636e\u5177\u6709\u79bb\u6563\u6027\uff0c\u4e14\u4e2a\u4f53\u548c\u4eba\u7fa4\u4e2d\u5b58\u5728\u4e0d\u540c\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u56fe\u50cf\u91c7\u6837\u6a21\u5f0f\u3002", "method": "\u5c06\u7279\u5b9a\u4e2a\u4f53\u7684\u7eb5\u5411T1\u52a0\u6743MRI\u6570\u636e\u5efa\u6a21\u4e3a\u8fde\u7eed\u51fd\u6570\uff0c\u4f7f\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u7684INR\u67b6\u6784\uff0c\u80fd\u591f\u90e8\u5206\u89e3\u8026\u7a7a\u95f4\u548c\u65f6\u95f4\u8f68\u8ff9\u53c2\u6570\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u76f4\u63a5\u5728INR\u53c2\u6570\u7a7a\u95f4\u4e0a\u8fd0\u884c\u7684\u6709\u6548\u6846\u67b6\uff0c\u4ee5\u5bf9\u5927\u8111\u8870\u8001\u8f68\u8ff9\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728\u66f4\u771f\u5b9e\u7684\u975e\u89c4\u5219\u91c7\u6837\u5b9e\u9a8c\u4e2d\uff0c\u57fa\u4e8eINR\u7684\u65b9\u6cd5\u5728\u8111\u8870\u8001\u8f68\u8ff9\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e8681.3%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u6807\u51c6\u7684\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u6a21\u578b\uff0873.7%\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eINR\u7684\u65b9\u6cd5\u5728\u8111\u8870\u8001\u8f68\u8ff9\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e3a\u5206\u6790\u7eb5\u5411MRI\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2510.10331", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10331", "abs": "https://arxiv.org/abs/2510.10331", "authors": ["Hanchen Su", "Wei Luo", "Wei Han", "Yu Elaine Liu", "Yufeng Wayne Zhang", "Cen Mia Zhao", "Ying Joy Zhang", "Yashar Mehdad"], "title": "LLM-Friendly Knowledge Representation for Customer Support", "comment": null, "summary": "We propose a practical approach by integrating Large Language Models (LLMs)\nwith a framework designed to navigate the complexities of Airbnb customer\nsupport operations. In this paper, our methodology employs a novel reformatting\ntechnique, the Intent, Context, and Action (ICA) format, which transforms\npolicies and workflows into a structure more comprehensible to LLMs.\nAdditionally, we develop a synthetic data generation strategy to create\ntraining data with minimal human intervention, enabling cost-effective\nfine-tuning of our model. Our internal experiments (not applied to Airbnb\nproducts) demonstrate that our approach of restructuring workflows and\nfine-tuning LLMs with synthetic data significantly enhances their performance,\nsetting a new benchmark for their application in customer support. Our solution\nis not only cost-effective but also improves customer support, as evidenced by\nboth accuracy and manual processing time evaluation metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u6846\u67b6\u76f8\u7ed3\u5408\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9 Airbnb \u5ba2\u6237\u652f\u6301\u64cd\u4f5c\u7684\u590d\u6742\u6027\u3002", "motivation": "\u65e8\u5728\u63d0\u5347 Airbnb \u5ba2\u6237\u652f\u6301\u8fd0\u8425\u6548\u7387\u548c\u6548\u679c\u3002", "method": "\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u91cd\u683c\u5f0f\u5316\u6280\u672f\uff0c\u5373\u610f\u56fe\u3001\u4e0a\u4e0b\u6587\u548c\u884c\u52a8\uff08ICA\uff09\u683c\u5f0f\uff0c\u5e76\u5c06\u7b56\u7565\u548c\u5de5\u4f5c\u6d41\u7a0b\u8f6c\u6362\u4e3a LLM \u66f4\u5bb9\u6613\u7406\u89e3\u7684\u7ed3\u6784\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u5408\u6210\u6570\u636e\u751f\u6210\u7b56\u7565\uff0c\u4ee5\u6700\u5c11\u7684\u4eba\u5de5\u5e72\u9884\u521b\u5efa\u8bad\u7ec3\u6570\u636e\uff0c\u4ece\u800c\u80fd\u591f\u7ecf\u6d4e\u9ad8\u6548\u5730\u5fae\u8c03\u6a21\u578b\u3002", "result": "\u5185\u90e8\u5b9e\u9a8c\u8868\u660e\uff0c\u91cd\u6784\u5de5\u4f5c\u6d41\u7a0b\u548c\u4f7f\u7528\u5408\u6210\u6570\u636e\u5fae\u8c03 LLM \u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5176\u6027\u80fd\uff0c\u4e3a LLM \u5728\u5ba2\u6237\u652f\u6301\u4e2d\u7684\u5e94\u7528\u6811\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002", "conclusion": "\u8be5\u89e3\u51b3\u65b9\u6848\u4e0d\u4ec5\u7ecf\u6d4e\u9ad8\u6548\uff0c\u800c\u4e14\u901a\u8fc7\u51c6\u786e\u6027\u548c\u4eba\u5de5\u5904\u7406\u65f6\u95f4\u8bc4\u4f30\u6307\u6807\u8bc1\u660e\uff0c\u5b83\u53ef\u4ee5\u6539\u5584\u5ba2\u6237\u652f\u6301\u3002"}}
{"id": "2510.09712", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09712", "abs": "https://arxiv.org/abs/2510.09712", "authors": ["Zhao Tong", "Chunlin Gong", "Yimeng Gu", "Haichao Shi", "Qiang Liu", "Shu Wu", "Xiao-Yu Zhang"], "title": "Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments", "comment": "10 pages, 12 figures", "summary": "The spread of fake news online distorts public judgment and erodes trust in\nsocial media platforms. Although recent fake news detection (FND) models\nperform well in standard settings, they remain vulnerable to adversarial\ncomments-authored by real users or by large language models (LLMs)-that subtly\nshift model decisions. In view of this, we first present a comprehensive\nevaluation of comment attacks to existing fake news detectors and then\nintroduce a group-adaptive adversarial training strategy to improve the\nrobustness of FND models. To be specific, our approach comprises three steps:\n(1) dividing adversarial comments into three psychologically grounded\ncategories: perceptual, cognitive, and societal; (2) generating diverse,\ncategory-specific attacks via LLMs to enhance adversarial training; and (3)\napplying a Dirichlet-based adaptive sampling mechanism (InfoDirichlet Adjusting\nMechanism) that dynamically adjusts the learning focus across different comment\ncategories during training. Experiments on benchmark datasets show that our\nmethod maintains strong detection accuracy while substantially increasing\nrobustness to a wide range of adversarial comment perturbations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u7ebf\u865a\u5047\u65b0\u95fb\u68c0\u6d4b\u6a21\u578b\u5728\u5bf9\u6297\u6027\u8bc4\u8bba\u653b\u51fb\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7fa4\u4f53\u81ea\u9002\u5e94\u5bf9\u6297\u8bad\u7ec3\u7684\u7b56\u7565\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u865a\u5047\u65b0\u95fb\u68c0\u6d4b\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u8bc4\u8bba\u7684\u5f71\u54cd\uff0c\u8fd9\u4e9b\u8bc4\u8bba\u4f1a\u5fae\u5999\u5730\u6539\u53d8\u6a21\u578b\u7684\u51b3\u7b56\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u6b65\u9aa4\uff1a\u5c06\u5bf9\u6297\u6027\u8bc4\u8bba\u5206\u4e3a\u611f\u77e5\u3001\u8ba4\u77e5\u548c\u793e\u4f1a\u4e09\u4e2a\u5fc3\u7406\u7c7b\u522b\uff1b\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u7684\u3001\u7279\u5b9a\u7c7b\u522b\u7684\u653b\u51fb\uff1b\u5e94\u7528Dirichlet-based\u81ea\u9002\u5e94\u91c7\u6837\u673a\u5236\uff0c\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4e0d\u540c\u8bc4\u8bba\u7c7b\u522b\u7684\u5b66\u4e60\u91cd\u70b9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u8f83\u5f3a\u68c0\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u5404\u79cd\u5bf9\u6297\u6027\u8bc4\u8bba\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5bf9\u6297\u8bad\u7ec3\u7b56\u7565\uff0c\u53ef\u4ee5\u63d0\u9ad8\u865a\u5047\u65b0\u95fb\u68c0\u6d4b\u6a21\u578b\u5728\u5bf9\u6297\u6027\u8bc4\u8bba\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.11639", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11639", "abs": "https://arxiv.org/abs/2510.11639", "authors": ["Zhanyu Liu", "Shiyao Wang", "Xingmei Wang", "Rongzhou Zhang", "Jiaxin Deng", "Honghui Bao", "Jinghao Zhang", "Wuchao Li", "Pengfei Zheng", "Xiangyu Wu", "Yifei Hu", "Qigen Hu", "Xinchen Luo", "Lejian Ren", "Zixing Zhang", "Qianqian Wang", "Kuo Cai", "Yunfan Wu", "Hongtao Cheng", "Zexuan Cheng", "Lu Ren", "Huanjie Wang", "Yi Su", "Ruiming Tang", "Kun Gai", "Guorui Zhou"], "title": "OneRec-Think: In-Text Reasoning for Generative Recommendation", "comment": null, "summary": "The powerful generative capacity of Large Language Models (LLMs) has\ninstigated a paradigm shift in recommendation. However, existing generative\nmodels (e.g., OneRec) operate as implicit predictors, critically lacking the\ncapacity for explicit and controllable reasoning-a key advantage of LLMs. To\nbridge this gap, we propose OneRec-Think, a unified framework that seamlessly\nintegrates dialogue, reasoning, and personalized recommendation. OneRec-Think\nincorporates: (1) Itemic Alignment: cross-modal Item-Textual Alignment for\nsemantic grounding; (2) Reasoning Activation: Reasoning Scaffolding to activate\nLLM reasoning within the recommendation context; and (3) Reasoning Enhancement,\nwhere we design a recommendation-specific reward function that accounts for the\nmulti-validity nature of user preferences. Experiments across public benchmarks\nshow state-of-the-art performance. Moreover, our proposed \"Think-Ahead\"\narchitecture enables effective industrial deployment on Kuaishou, achieving a\n0.159\\% gain in APP Stay Time and validating the practical efficacy of the\nmodel's explicit reasoning capability.", "AI": {"tldr": "\u63d0\u51fa\u4e86OneRec-Think\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u65e0\u7f1d\u96c6\u6210\u4e86\u5bf9\u8bdd\u3001\u63a8\u7406\u548c\u4e2a\u6027\u5316\u63a8\u8350\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u6210\u6a21\u578b\u4f5c\u4e3a\u9690\u5f0f\u9884\u6d4b\u5668\u8fd0\u884c\uff0c\u4e25\u91cd\u7f3a\u4e4f\u663e\u5f0f\u548c\u53ef\u63a7\u7684\u63a8\u7406\u80fd\u529b\uff0c\u800c\u8fd9\u6b63\u662fLLM\u7684\u4e00\u4e2a\u5173\u952e\u4f18\u52bf\u3002", "method": "\u8be5\u6846\u67b6\u5305\u542b\uff1a(1)\u9879\u76ee\u5bf9\u9f50\uff1a\u7528\u4e8e\u8bed\u4e49\u63a5\u5730\u7684\u8de8\u6a21\u6001\u9879\u76ee-\u6587\u672c\u5bf9\u9f50\uff1b(2)\u63a8\u7406\u6fc0\u6d3b\uff1a\u63a8\u7406\u652f\u67b6\uff0c\u4ee5\u6fc0\u6d3b\u63a8\u8350\u4e0a\u4e0b\u6587\u4e2d\u7684LLM\u63a8\u7406\uff1b(3)\u63a8\u7406\u589e\u5f3a\uff1a\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u63a8\u8350\u7279\u5b9a\u7684\u5956\u52b1\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u8003\u8651\u4e86\u7528\u6237\u504f\u597d\u7684\u591a\u91cd\u6709\u6548\u6027\u3002", "result": "\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u63d0\u51fa\u7684\u201cThink-Ahead\u201d\u67b6\u6784\u5b9e\u73b0\u4e86\u5728\u5feb\u624b\u4e0a\u7684\u6709\u6548\u5de5\u4e1a\u90e8\u7f72\uff0c\u5728APP\u505c\u7559\u65f6\u95f4\u4e0a\u5b9e\u73b0\u4e860.159%\u7684\u589e\u957f\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u6a21\u578b\u663e\u5f0f\u63a8\u7406\u80fd\u529b\u7684\u5b9e\u9645\u6548\u679c\u3002"}}
{"id": "2510.09935", "categories": ["cs.CL", "cs.AI", "68T50, 68T45, 68T07", "I.2.7; I.2.10; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.09935", "abs": "https://arxiv.org/abs/2510.09935", "authors": ["Weibin Cai", "Jiayu Li", "Reza Zafarani"], "title": "Unpacking Hateful Memes: Presupposed Context and False Claims", "comment": null, "summary": "While memes are often humorous, they are frequently used to disseminate hate,\ncausing serious harm to individuals and society. Current approaches to hateful\nmeme detection mainly rely on pre-trained language models. However, less focus\nhas been dedicated to \\textit{what make a meme hateful}. Drawing on insights\nfrom philosophy and psychology, we argue that hateful memes are characterized\nby two essential features: a \\textbf{presupposed context} and the expression of\n\\textbf{false claims}. To capture presupposed context, we develop \\textbf{PCM}\nfor modeling contextual information across modalities. To detect false claims,\nwe introduce the \\textbf{FACT} module, which integrates external knowledge and\nharnesses cross-modal reference graphs. By combining PCM and FACT, we introduce\n\\textbf{\\textsf{SHIELD}}, a hateful meme detection framework designed to\ncapture the fundamental nature of hate. Extensive experiments show that SHIELD\noutperforms state-of-the-art methods across datasets and metrics, while\ndemonstrating versatility on other tasks, such as fake news detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSHIELD\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u4ec7\u6068\u6a21\u56e0\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408PCM\u548cFACT\u6a21\u5757\u6765\u6355\u6349\u4ec7\u6068\u7684\u672c\u8d28\u3002", "motivation": "\u5f53\u524d\u68c0\u6d4b\u4ec7\u6068\u6a21\u56e0\u7684\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u8f83\u5c11\u5173\u6ce8\u6a21\u56e0\u4ec7\u6068\u7684\u6839\u672c\u539f\u56e0\u3002", "method": "\u672c\u6587\u901a\u8fc7\u54f2\u5b66\u548c\u5fc3\u7406\u5b66\u7684\u89c1\u89e3\uff0c\u8ba4\u4e3a\u4ec7\u6068\u6a21\u56e0\u5177\u6709\u4e24\u4e2a\u57fa\u672c\u7279\u5f81\uff1a\u9884\u8bbe\u8bed\u5883\u548c\u865a\u5047\u58f0\u660e\u3002\u4e3a\u4e86\u6355\u6349\u9884\u8bbe\u8bed\u5883\uff0c\u5f00\u53d1\u4e86PCM\u6765\u5efa\u6a21\u8de8\u6a21\u6001\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u4e3a\u4e86\u68c0\u6d4b\u865a\u5047\u58f0\u660e\uff0c\u5f15\u5165\u4e86FACT\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u96c6\u6210\u4e86\u5916\u90e8\u77e5\u8bc6\u5e76\u5229\u7528\u4e86\u8de8\u6a21\u6001\u53c2\u8003\u56fe\u3002", "result": "SHIELD\u5728\u6570\u636e\u96c6\u548c\u6307\u6807\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u5047\u65b0\u95fb\u68c0\u6d4b\u7b49\u5176\u4ed6\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u591a\u529f\u80fd\u6027\u3002", "conclusion": "SHIELD\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u68c0\u6d4b\u4ec7\u6068\u6a21\u56e0\uff0c\u5e76\u5728\u5176\u4ed6\u76f8\u5173\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.09945", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.09945", "abs": "https://arxiv.org/abs/2510.09945", "authors": ["Pouya Shaeri", "Ryan T. Woo", "Yasaman Mohammadpour", "Ariane Middel"], "title": "Explainable Human-in-the-Loop Segmentation via Critic Feedback Signals", "comment": "Submitted to a computer vision conference (under review)", "summary": "Segmentation models achieve high accuracy on benchmarks but often fail in\nreal-world domains by relying on spurious correlations instead of true object\nboundaries. We propose a human-in-the-loop interactive framework that enables\ninterventional learning through targeted human corrections of segmentation\noutputs. Our approach treats human corrections as interventional signals that\nshow when reliance on superficial features (e.g., color or texture) is\ninappropriate. The system learns from these interventions by propagating\ncorrection-informed edits across visually similar images, effectively steering\nthe model toward robust, semantically meaningful features rather than\ndataset-specific artifacts. Unlike traditional annotation approaches that\nsimply provide more training data, our method explicitly identifies when and\nwhy the model fails and then systematically corrects these failure modes across\nthe entire dataset. Through iterative human feedback, the system develops\nincreasingly robust representations that generalize better to novel domains and\nresist artifactual correlations. We demonstrate that our framework improves\nsegmentation accuracy by up to 9 mIoU points (12-15\\% relative improvement) on\nchallenging cubemap data and yields 3-4$\\times$ reductions in annotation effort\ncompared to standard retraining, while maintaining competitive performance on\nbenchmark datasets. This work provides a practical framework for researchers\nand practitioners seeking to build segmentation systems that are accurate,\nrobust to dataset biases, data-efficient, and adaptable to real-world domains\nsuch as urban climate monitoring and autonomous driving.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u673a\u4ea4\u4e92\u7684\u5e72\u9884\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4eba\u5de5\u6821\u6b63\u5206\u5272\u8f93\u51fa\u6765\u63d0\u5347\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5206\u5272\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5bb9\u6613\u8fc7\u5ea6\u4f9d\u8d56\u8868\u9762\u76f8\u5173\u6027\uff0c\u5ffd\u7565\u771f\u5b9e\u7684\u7269\u4f53\u8fb9\u754c\u3002", "method": "\u5229\u7528\u4eba\u5de5\u6821\u6b63\u4f5c\u4e3a\u5e72\u9884\u4fe1\u53f7\uff0c\u544a\u77e5\u6a21\u578b\u4f55\u65f6\u4e0d\u5e94\u4f9d\u8d56\u8868\u9762\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u5728\u89c6\u89c9\u76f8\u4f3c\u56fe\u50cf\u95f4\u4f20\u64ad\u6821\u6b63\u4fe1\u606f\uff0c\u5f15\u5bfc\u6a21\u578b\u5b66\u4e60\u9c81\u68d2\u7684\u3001\u8bed\u4e49\u76f8\u5173\u7684\u7279\u5f81\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684cubemap\u6570\u636e\u4e0a\uff0c\u5206\u5272\u7cbe\u5ea6\u63d0\u9ad8\u4e869 mIoU (12-15%\u7684\u76f8\u5bf9\u6539\u8fdb)\uff0c\u4e0e\u6807\u51c6\u91cd\u65b0\u8bad\u7ec3\u76f8\u6bd4\uff0c\u6807\u6ce8\u5de5\u4f5c\u91cf\u51cf\u5c11\u4e863-4\u500d\uff0c\u540c\u65f6\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u6784\u5efa\u51c6\u786e\u3001\u5bf9\u6570\u636e\u96c6\u504f\u5dee\u5177\u6709\u9c81\u68d2\u6027\u3001\u6570\u636e\u9ad8\u6548\u4e14\u9002\u5e94\u4e8e\u771f\u5b9e\u4e16\u754c\u9886\u57df\u7684\u5206\u5272\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.10338", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.10338", "abs": "https://arxiv.org/abs/2510.10338", "authors": ["Balagopal Unnikrishnan", "Ariel Guerra Adames", "Amin Adibi", "Sameer Peesapati", "Rafal Kocielnik", "Shira Fischer", "Hillary Clinton Kasimbazi", "Rodrigo Gameiro", "Alina Peluso", "Chrystinne Oliveira Fernandes", "Maximin Lange", "Lovedeep Gondara", "Leo Anthony Celi"], "title": "Beyond Ethics: How Inclusive Innovation Drives Economic Returns in Medical AI", "comment": null, "summary": "While ethical arguments for fairness in healthcare AI are well-established,\nthe economic and strategic value of inclusive design remains underexplored.\nThis perspective introduces the ``inclusive innovation dividend'' -- the\ncounterintuitive principle that solutions engineered for diverse, constrained\nuse cases generate superior economic returns in broader markets. Drawing from\nassistive technologies that evolved into billion-dollar mainstream industries,\nwe demonstrate how inclusive healthcare AI development creates business value\nbeyond compliance requirements. We identify four mechanisms through which\ninclusive innovation drives returns: (1) market expansion via geographic\nscalability and trust acceleration; (2) risk mitigation through reduced\nremediation costs and litigation exposure; (3) performance dividends from\nsuperior generalization and reduced technical debt, and (4) competitive\nadvantages in talent acquisition and clinical adoption. We present the\nHealthcare AI Inclusive Innovation Framework (HAIIF), a practical scoring\nsystem that enables organizations to evaluate AI investments based on their\npotential to capture these benefits. HAIIF provides structured guidance for\nresource allocation, transforming fairness and inclusivity from regulatory\ncheckboxes into sources of strategic differentiation. Our findings suggest that\norganizations investing incrementally in inclusive design can achieve expanded\nmarket reach and sustained competitive advantages, while those treating these\nconsiderations as overhead face compounding disadvantages as network effects\nand data advantages accrue to early movers.", "AI": {"tldr": "\u5305\u5bb9\u6027\u8bbe\u8ba1\u5728\u533b\u7597AI\u9886\u57df\u5177\u6709\u7ecf\u6d4e\u548c\u6218\u7565\u4ef7\u503c\uff0c\u80fd\u5e26\u6765\u8d85\u989d\u56de\u62a5\u3002", "motivation": "\u63a2\u8ba8\u5305\u5bb9\u6027\u8bbe\u8ba1\u5728\u533b\u7597AI\u9886\u57df\u7684\u7ecf\u6d4e\u548c\u6218\u7565\u4ef7\u503c\uff0c\u6307\u51fa\u5176\u91cd\u8981\u6027\u672a\u88ab\u5145\u5206\u8ba4\u8bc6\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8f85\u52a9\u6280\u672f\u53d1\u5c55\u4e3a\u4e3b\u6d41\u4ea7\u4e1a\u7684\u6848\u4f8b\uff0c\u8bba\u8bc1\u5305\u5bb9\u6027\u533b\u7597AI\u5f00\u53d1\u80fd\u521b\u9020\u5546\u4e1a\u4ef7\u503c\u3002", "result": "\u63d0\u51fa\u4e86\u533b\u7597AI\u5305\u5bb9\u6027\u521b\u65b0\u6846\u67b6\uff08HAIIF\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u6295\u8d44\u7684\u5305\u5bb9\u6027\u6f5c\u529b\uff0c\u5e76\u786e\u5b9a\u4e86\u5305\u5bb9\u6027\u521b\u65b0\u9a71\u52a8\u56de\u62a5\u7684\u56db\u4e2a\u673a\u5236\uff1a\u5e02\u573a\u6269\u5f20\u3001\u98ce\u9669\u964d\u4f4e\u3001\u6027\u80fd\u63d0\u5347\u548c\u7ade\u4e89\u4f18\u52bf\u3002", "conclusion": "\u6295\u8d44\u4e8e\u5305\u5bb9\u6027\u8bbe\u8ba1\u7684\u7ec4\u7ec7\u80fd\u591f\u83b7\u5f97\u66f4\u5e7f\u9614\u7684\u5e02\u573a\u548c\u6301\u7eed\u7684\u7ade\u4e89\u4f18\u52bf\uff0c\u800c\u5ffd\u89c6\u5305\u5bb9\u6027\u7684\u7ec4\u7ec7\u5c06\u9762\u4e34\u7d2f\u79ef\u52a3\u52bf\u3002"}}
{"id": "2510.09717", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09717", "abs": "https://arxiv.org/abs/2510.09717", "authors": ["Zhenlong Liu", "Hao Zeng", "Weiran Huang", "Hongxin Wei"], "title": "High-Power Training Data Identification with Provable Statistical Guarantees", "comment": null, "summary": "Identifying training data within large-scale models is critical for copyright\nlitigation, privacy auditing, and ensuring fair evaluation. The conventional\napproaches treat it as a simple binary classification task without statistical\nguarantees. A recent approach is designed to control the false discovery rate\n(FDR), but its guarantees rely on strong, easily violated assumptions. In this\npaper, we introduce Provable Training Data Identification (PTDI), a rigorous\nmethod that identifies a set of training data with strict false discovery rate\n(FDR) control. Specifically, our method computes p-values for each data point\nusing a set of known unseen data, and then constructs a conservative estimator\nfor the data usage proportion of the test set, which allows us to scale these\np-values. Our approach then selects the final set of training data by\nidentifying all points whose scaled p-values fall below a data-dependent\nthreshold. This entire procedure enables the discovery of training data with\nprovable, strict FDR control and significantly boosted power. Extensive\nexperiments across a wide range of models (LLMs and VLMs), and datasets\ndemonstrate that PTDI strictly controls the FDR and achieves higher power.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u6570\u636e\u8bc6\u522b\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e25\u683c\u63a7\u5236\u9519\u8bef\u53d1\u73b0\u7387\uff08FDR\uff09\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u6a21\u578b\u4e2d\u8bc6\u522b\u8bad\u7ec3\u6570\u636e\u5bf9\u4e8e\u7248\u6743\u8bc9\u8bbc\u3001\u9690\u79c1\u5ba1\u8ba1\u548c\u786e\u4fdd\u516c\u5e73\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u5c06\u5176\u89c6\u4e3a\u7b80\u5355\u7684\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\uff0c\u7f3a\u4e4f\u7edf\u8ba1\u4fdd\u8bc1\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5bb9\u6613\u88ab\u8fdd\u53cd\u7684\u5f3a\u5047\u8bbe\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u53ef\u8bc1\u660e\u7684\u8bad\u7ec3\u6570\u636e\u8bc6\u522b\u201d\uff08PTDI\uff09\u7684\u4e25\u683c\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u4e00\u7ec4\u5df2\u77e5\u7684\u672a\u89c1\u6570\u636e\u8ba1\u7b97\u6bcf\u4e2a\u6570\u636e\u70b9\u7684p\u503c\uff0c\u5e76\u6784\u5efa\u6d4b\u8bd5\u96c6\u6570\u636e\u4f7f\u7528\u6bd4\u4f8b\u7684\u4fdd\u5b88\u4f30\u8ba1\u5668\uff0c\u4ece\u800c\u7f29\u653e\u8fd9\u4e9bp\u503c\u3002\u7136\u540e\uff0c\u901a\u8fc7\u8bc6\u522b\u6240\u6709\u7f29\u653e\u540e\u7684p\u503c\u4f4e\u4e8e\u6570\u636e\u76f8\u5173\u9608\u503c\u7684\u70b9\u6765\u9009\u62e9\u6700\u7ec8\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "result": "\u5728\u5404\u79cd\u6a21\u578b\uff08LLM\u548cVLM\uff09\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPTDI\u4e25\u683c\u63a7\u5236\u4e86FDR\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u80fd\u529b\u3002", "conclusion": "PTDI\u65b9\u6cd5\u80fd\u591f\u5728\u53ef\u8bc1\u660e\u7684\u3001\u4e25\u683c\u7684FDR\u63a7\u5236\u4e0b\u53d1\u73b0\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u8bc6\u522b\u80fd\u529b\u3002"}}
{"id": "2510.11654", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.11654", "abs": "https://arxiv.org/abs/2510.11654", "authors": ["Daniel Berhane Araya", "Duoduo Liao"], "title": "FinVet: A Collaborative Framework of RAG and External Fact-Checking Agents for Financial Misinformation Detection", "comment": null, "summary": "Financial markets face growing threats from misinformation that can trigger\nbillions in losses in minutes. Most existing approaches lack transparency in\ntheir decision-making and provide limited attribution to credible sources. We\nintroduce FinVet, a novel multi-agent framework that integrates two\nRetrieval-Augmented Generation (RAG) pipelines with external fact-checking\nthrough a confidence-weighted voting mechanism. FinVet employs adaptive\nthree-tier processing that dynamically adjusts verification strategies based on\nretrieval confidence, from direct metadata extraction to hybrid reasoning to\nfull model-based analysis. Unlike existing methods, FinVet provides\nevidence-backed verdicts, source attribution, confidence scores, and explicit\nuncertainty flags when evidence is insufficient. Experimental evaluation on the\nFinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a\n10.4% improvement over the best individual pipeline (fact-check pipeline) and\n37% improvement over standalone RAG approaches.", "AI": {"tldr": "FinVet: A multi-agent framework that integrates two Retrieval-Augmented Generation (RAG) pipelines with external fact-checking through a confidence-weighted voting mechanism.", "motivation": "Financial markets face growing threats from misinformation that can trigger billions in losses in minutes. Most existing approaches lack transparency in their decision-making and provide limited attribution to credible sources.", "method": "FinVet employs adaptive three-tier processing that dynamically adjusts verification strategies based on retrieval confidence, from direct metadata extraction to hybrid reasoning to full model-based analysis.", "result": "FinVet achieves an F1 score of 0.85, which is a 10.4% improvement over the best individual pipeline (fact-check pipeline) and 37% improvement over standalone RAG approaches.", "conclusion": "FinVet provides evidence-backed verdicts, source attribution, confidence scores, and explicit uncertainty flags when evidence is insufficient."}}
{"id": "2510.09947", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09947", "abs": "https://arxiv.org/abs/2510.09947", "authors": ["Mir Tafseer Nayeem", "Sawsan Alqahtani", "Md Tahmid Rahman Laskar", "Tasnim Mohiuddin", "M Saiful Bari"], "title": "Beyond Fertility: Analyzing STRR as a Metric for Multilingual Tokenization Evaluation", "comment": "NeurIPS 2025 Workshop", "summary": "Tokenization is a crucial but under-evaluated step in large language models\n(LLMs). The standard metric, fertility (the average number of tokens per word),\ncaptures compression efficiency but obscures how vocabularies are allocated\nacross languages and domains. We analyze six widely used tokenizers across\nseven languages and two domains, finding stable fertility for English, high\nfertility for Chinese, and little domain sensitivity. To address fertility's\nblind spots, we propose the Single Token Retention Rate (STRR), which measures\nthe proportion of words preserved as single tokens. STRR reveals systematic\nprioritization of English, strong support for Chinese, and fragmentation in\nHindi, offering an interpretable view of cross-lingual fairness. Our results\nshow that STRR complements fertility and provides practical guidance for\ndesigning more equitable multilingual tokenizers.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5206\u8bcd\u7684\u91cd\u8981\u6027\uff0c\u53d1\u73b0\u6807\u51c6\u6307\u6807 fertility \u5b58\u5728\u76f2\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86 Single Token Retention Rate (STRR) \u6307\u6807\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5206\u8bcd\u8fd9\u4e00\u5173\u952e\u6b65\u9aa4\uff0c\u53d1\u73b0\u6807\u51c6\u6307\u6807 fertility \u65e0\u6cd5\u5168\u9762\u53cd\u6620\u8bcd\u6c47\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u9886\u57df\u7684\u5206\u914d\u60c5\u51b5\u3002", "method": "\u5206\u6790\u4e86\u516d\u79cd\u5e38\u7528\u5206\u8bcd\u5668\u5728\u4e03\u79cd\u8bed\u8a00\u548c\u4e24\u4e2a\u9886\u57df\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86 Single Token Retention Rate (STRR) \u6307\u6807\uff0c\u7528\u4e8e\u8861\u91cf\u5355\u8bcd\u88ab\u4fdd\u7559\u4e3a\u5355\u4e2a token \u7684\u6bd4\u4f8b\u3002", "result": "\u53d1\u73b0\u82f1\u8bed\u7684 fertility \u7a33\u5b9a\uff0c\u4e2d\u6587\u7684 fertility \u8f83\u9ad8\uff0c\u9886\u57df\u654f\u611f\u6027\u8f83\u4f4e\u3002STRR \u6307\u6807\u63ed\u793a\u4e86\u5bf9\u82f1\u8bed\u7684\u7cfb\u7edf\u6027\u4f18\u5148\u3001\u5bf9\u4e2d\u6587\u7684\u5f3a\u5927\u652f\u6301\u4ee5\u53ca\u5370\u5730\u8bed\u7684\u5206\u88c2\u60c5\u51b5\u3002", "conclusion": "STRR \u6307\u6807\u53ef\u4ee5\u8865\u5145 fertility \u6307\u6807\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u516c\u5e73\u7684\u591a\u8bed\u8a00\u5206\u8bcd\u5668\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2510.09948", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09948", "abs": "https://arxiv.org/abs/2510.09948", "authors": ["Pan Wang", "Yihao Hu", "Xiaodong Bai", "Aiping Yang", "Xiangxiang Li", "Meiping Ding", "Jianguo Yao"], "title": "A Multi-Strategy Framework for Enhancing Shatian Pomelo Detection in Real-World Orchards", "comment": null, "summary": "As a specialty agricultural product with a large market scale, Shatian pomelo\nnecessitates the adoption of automated detection to ensure accurate quantity\nand meet commercial demands for lean production. Existing research often\ninvolves specialized networks tailored for specific theoretical or dataset\nscenarios, but these methods tend to degrade performance in real-world. Through\nanalysis of factors in this issue, this study identifies four key challenges\nthat affect the accuracy of Shatian pomelo detection: imaging devices, lighting\nconditions, object scale variation, and occlusion. To mitigate these\nchallenges, a multi-strategy framework is proposed in this paper. Firstly, to\neffectively solve tone variation introduced by diverse imaging devices and\ncomplex orchard environments, we utilize a multi-scenario dataset,\nSTP-AgriData, which is constructed by integrating real orchard images with\ninternet-sourced data. Secondly, to simulate the inconsistent illumination\nconditions, specific data augmentations such as adjusting contrast and changing\nbrightness, are applied to the above dataset. Thirdly, to address the issues of\nobject scale variation and occlusion in fruit detection, an REAS-Det network is\ndesigned in this paper. For scale variation, RFAConv and C3RFEM modules are\ndesigned to expand and enhance the receptive fields. For occlusion variation, a\nmulti-scale, multi-head feature selection structure (MultiSEAM) and soft-NMS\nare introduced to enhance the handling of occlusion issues to improve detection\naccuracy. The results of these experiments achieved a precision(P) of 87.6%, a\nrecall (R) of 74.9%, a mAP@.50 of 82.8%, and a mAP@.50:.95 of 53.3%. Our\nproposed network demonstrates superior performance compared to other\nstate-of-the-art detection methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7b56\u7565\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u6c99\u7530\u67da\u68c0\u6d4b\u4e2d\u7684\u56db\u4e2a\u5173\u952e\u6311\u6218\uff1a\u6210\u50cf\u8bbe\u5907\u3001\u5149\u7167\u6761\u4ef6\u3001\u7269\u4f53\u5c3a\u5ea6\u53d8\u5316\u548c\u906e\u6321\u3002", "motivation": "\u4e3a\u4e86\u6ee1\u8db3\u6c99\u7530\u67da\u5546\u4e1a\u5316\u751f\u4ea7\u7684\u9700\u6c42\uff0c\u91c7\u7528\u81ea\u52a8\u5316\u68c0\u6d4b\u662f\u5fc5\u8981\u7684\u3002\u73b0\u6709\u7684\u7814\u7a76\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u573a\u666f\u6570\u636e\u96c6STP-AgriData\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2aREAS-Det\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u5305\u542bRFAConv\u3001C3RFEM\u6a21\u5757\u3001MultiSEAM\u7ed3\u6784\u548csoft-NMS\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u7684\u7cbe\u786e\u7387\u4e3a87.6%\uff0c\u53ec\u56de\u7387\u4e3a74.9%\uff0cmAP@.50\u4e3a82.8%\uff0cmAP@.50:.95\u4e3a53.3%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7f51\u7edc\u6bd4\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2510.10409", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10409", "abs": "https://arxiv.org/abs/2510.10409", "authors": ["Siddartha Devic", "Charlotte Peale", "Arwen Bradley", "Sinead Williamson", "Preetum Nakkiran", "Aravind Gollakota"], "title": "Trace Length is a Simple Uncertainty Signal in Reasoning Models", "comment": null, "summary": "Uncertainty quantification for LLMs is a key research direction towards\naddressing hallucination and other issues that limit their reliable deployment.\nIn this work, we show that reasoning trace length is a simple and useful\nconfidence estimator in large reasoning models. Through comprehensive\nexperiments across multiple models, datasets, and prompts, we show that trace\nlength performs in comparable but complementary ways to other zero-shot\nconfidence estimators such as verbalized confidence. Our work reveals that\nreasoning post-training fundamentally alters the relationship between trace\nlength and accuracy, going beyond prior work that had shown that post-training\ncauses traces to grow longer in general (e.g., \"overthinking\"). We investigate\nthe mechanisms behind trace length's performance as a confidence signal,\nobserving that the effect remains even after adjusting for confounders such as\nproblem difficulty and GRPO-induced length bias. We identify high-entropy or\n\"forking\" tokens as playing a key role in the mechanism. Our findings\ndemonstrate that reasoning post-training enhances uncertainty quantification\nbeyond verbal expressions, and establish trace length as a practical confidence\nmeasure for large reasoning models.", "AI": {"tldr": "\u63a8\u7406\u8f68\u8ff9\u957f\u5ea6\u53ef\u4ee5\u4f5c\u4e3a\u5927\u578b\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u5668\u3002", "motivation": "\u91cf\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u662f\u89e3\u51b3\u5e7b\u89c9\u548c\u5176\u4ed6\u9650\u5236\u53ef\u9760\u90e8\u7f72\u95ee\u9898\u7684\u5173\u952e\u7814\u7a76\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u591a\u4e2a\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u63d0\u793a\u8fdb\u884c\u7efc\u5408\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u63a8\u7406\u8f68\u8ff9\u957f\u5ea6\u4e0e\u5176\u4ed6\u96f6\u6837\u672c\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u5668\u7684\u6027\u80fd\u3002", "result": "\u63a8\u7406\u8f68\u8ff9\u957f\u5ea6\u7684\u6027\u80fd\u4e0e\u8bf8\u5982\u8bed\u8a00\u7f6e\u4fe1\u5ea6\u7b49\u5176\u4ed6\u96f6\u6837\u672c\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u5668\u76f8\u5f53\u4f46\u5177\u6709\u4e92\u8865\u6027\u3002\u63a8\u7406\u540e\u8bad\u7ec3\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u4e86\u8f68\u8ff9\u957f\u5ea6\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u5373\u4f7f\u5728\u8c03\u6574\u4e86\u95ee\u9898\u96be\u5ea6\u548c GRPO \u5f15\u8d77\u7684\u957f\u5ea6\u504f\u5dee\u7b49\u6df7\u6742\u56e0\u7d20\u540e\uff0c\u8fd9\u79cd\u6548\u5e94\u4ecd\u7136\u5b58\u5728\u3002\u9ad8\u71b5\u6216\u201c\u5206\u53c9\u201d tokens \u5728\u8be5\u673a\u5236\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u63a8\u7406\u540e\u8bad\u7ec3\u589e\u5f3a\u4e86\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u8d85\u8d8a\u4e86\u53e3\u5934\u8868\u8fbe\uff0c\u5e76\u5c06\u8f68\u8ff9\u957f\u5ea6\u786e\u7acb\u4e3a\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5b9e\u7528\u7f6e\u4fe1\u5ea6\u5ea6\u91cf\u3002"}}
{"id": "2510.09718", "categories": ["cs.LG", "68T05, 68T10", "I.5.3; I.2.11"], "pdf": "https://arxiv.org/pdf/2510.09718", "abs": "https://arxiv.org/abs/2510.09718", "authors": ["A. Jung"], "title": "Federated k-Means via Generalized Total Variation Minimization", "comment": null, "summary": "We consider the problem of federated clustering, where interconnected devices\nhave access to private local datasets and need to jointly cluster the overall\ndataset without sharing their local dataset. Our focus is on hard clustering\nbased on the k-means principle. We formulate federated k-means clustering as an\ninstance of GTVMin. This formulation naturally lends to a federated k-means\nalgorithm where each device updates local cluster centroids by solving a\nmodified local k-means problem. The modification involves adding a penalty term\nto measure the discrepancy between the cluster centroid of neighbouring\ndevices. Our federated k-means algorithm is privacy-friendly as it only\nrequires sharing aggregated information among interconnected devices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6k-means\u805a\u7c7b\u7b97\u6cd5\uff0c\u65e8\u5728\u4fdd\u62a4\u672c\u5730\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\uff0c\u5bf9\u5206\u6563\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8054\u5408\u805a\u7c7b\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\uff0c\u8bbe\u5907\u9700\u8981\u5728\u4e0d\u5171\u4eab\u672c\u5730\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u6574\u4f53\u6570\u636e\u96c6\u8fdb\u884c\u805a\u7c7b\u7684\u95ee\u9898\u3002\u91cd\u70b9\u662f\u57fa\u4e8ek-means\u539f\u5219\u7684\u786c\u805a\u7c7b\u3002", "method": "\u5c06\u8054\u90a6k-means\u805a\u7c7b\u95ee\u9898\u8f6c\u5316\u4e3aGTVMin\u7684\u5b9e\u4f8b\u3002\u6bcf\u4e2a\u8bbe\u5907\u901a\u8fc7\u89e3\u51b3\u4e00\u4e2a\u6539\u8fdb\u7684\u672c\u5730k-means\u95ee\u9898\u6765\u66f4\u65b0\u672c\u5730\u805a\u7c7b\u4e2d\u5fc3\uff0c\u8be5\u6539\u8fdb\u5305\u62ec\u589e\u52a0\u4e00\u4e2a\u60e9\u7f5a\u9879\u6765\u8861\u91cf\u76f8\u90bb\u8bbe\u5907\u805a\u7c7b\u4e2d\u5fc3\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8054\u90a6k-means\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u662f\u9690\u79c1\u53cb\u597d\u7684\uff0c\u56e0\u4e3a\u5b83\u53ea\u9700\u8981\u5728\u4e92\u8fde\u8bbe\u5907\u4e4b\u95f4\u5171\u4eab\u805a\u5408\u4fe1\u606f\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u8fdb\u884c\u8054\u90a6k-means\u805a\u7c7b\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2510.09988", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09988", "abs": "https://arxiv.org/abs/2510.09988", "authors": ["Jiaqi Wei", "Xiang Zhang", "Yuejin Yang", "Wenxuan Huang", "Juntai Cao", "Sheng Xu", "Xiang Zhuang", "Zhangyang Gao", "Muhammad Abdul-Mageed", "Laks V. S. Lakshmanan", "Chenyu You", "Wanli Ouyang", "Siqi Sun"], "title": "Unifying Tree Search Algorithm and Reward Design for LLM Reasoning: A Survey", "comment": null, "summary": "Deliberative tree search is a cornerstone of modern Large Language Model\n(LLM) research, driving the pivot from brute-force scaling toward algorithmic\nefficiency. This single paradigm unifies two critical frontiers:\n\\textbf{Test-Time Scaling (TTS)}, which deploys on-demand computation to solve\nhard problems, and \\textbf{Self-Improvement}, which uses search-generated data\nto durably enhance model parameters. However, this burgeoning field is\nfragmented and lacks a common formalism, particularly concerning the ambiguous\nrole of the reward signal -- is it a transient heuristic or a durable learning\ntarget? This paper resolves this ambiguity by introducing a unified framework\nthat deconstructs search algorithms into three core components: the\n\\emph{Search Mechanism}, \\emph{Reward Formulation}, and \\emph{Transition\nFunction}. We establish a formal distinction between transient \\textbf{Search\nGuidance} for TTS and durable \\textbf{Parametric Reward Modeling} for\nSelf-Improvement. Building on this formalism, we introduce a component-centric\ntaxonomy, synthesize the state-of-the-art, and chart a research roadmap toward\nmore systematic progress in creating autonomous, self-improving agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u7406\u89e3\u548c\u7ec4\u7ec7LLM\u4e2d\u7684 deliberative tree search \u7b97\u6cd5\u3002", "motivation": "\u5f53\u524d\u7684\u7814\u7a76\u7f3a\u4e4f\u4e00\u4e2a\u5171\u540c\u7684\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u5c24\u5176\u662f\u5728\u5956\u52b1\u4fe1\u53f7\u7684\u89d2\u8272\u65b9\u9762\u3002", "method": "\u5c06\u641c\u7d22\u7b97\u6cd5\u5206\u89e3\u4e3a\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u641c\u7d22\u673a\u5236\u3001\u5956\u52b1\u516c\u5f0f\u548c\u8f6c\u79fb\u51fd\u6570\u3002\u533a\u5206\u4e86\u7528\u4e8eTTS\u7684\u77ac\u65f6\u641c\u7d22\u6307\u5bfc\u548c\u7528\u4e8e\u81ea\u6211\u6539\u8fdb\u7684\u6301\u4e45\u53c2\u6570\u5956\u52b1\u5efa\u6a21\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u4ee5\u7ec4\u4ef6\u4e3a\u4e2d\u5fc3\u7684\u5206\u7c7b\u6cd5\uff0c\u7efc\u5408\u4e86\u73b0\u6709\u6280\u672f\uff0c\u5e76\u89c4\u5212\u4e86\u5728\u521b\u5efa\u81ea\u4e3b\u3001\u81ea\u6211\u6539\u8fdb\u4ee3\u7406\u65b9\u9762\u53d6\u5f97\u66f4\u7cfb\u7edf\u8fdb\u5c55\u7684\u7814\u7a76\u8def\u7ebf\u56fe\u3002", "conclusion": "\u8be5\u6846\u67b6\u89e3\u51b3\u4e86\u5956\u52b1\u4fe1\u53f7\u7684\u6a21\u7cca\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2510.09953", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09953", "abs": "https://arxiv.org/abs/2510.09953", "authors": ["Salma J. Ahmed", "Emad A. Mohammed", "Azam Asilian Bidgoli"], "title": "J-RAS: Enhancing Medical Image Segmentation via Retrieval-Augmented Joint Training", "comment": null, "summary": "Image segmentation, the process of dividing images into meaningful regions,\nis critical in medical applications for accurate diagnosis, treatment planning,\nand disease monitoring. Although manual segmentation by healthcare\nprofessionals produces precise outcomes, it is time-consuming, costly, and\nprone to variability due to differences in human expertise. Artificial\nintelligence (AI)-based methods have been developed to address these\nlimitations by automating segmentation tasks; however, they often require\nlarge, annotated datasets that are rarely available in practice and frequently\nstruggle to generalize across diverse imaging conditions due to inter-patient\nvariability and rare pathological cases. In this paper, we propose Joint\nRetrieval Augmented Segmentation (J-RAS), a joint training method for guided\nimage segmentation that integrates a segmentation model with a retrieval model.\nBoth models are jointly optimized, enabling the segmentation model to leverage\nretrieved image-mask pairs to enrich its anatomical understanding, while the\nretrieval model learns segmentation-relevant features beyond simple visual\nsimilarity. This joint optimization ensures that retrieval actively contributes\nmeaningful contextual cues to guide boundary delineation, thereby enhancing the\noverall segmentation performance. We validate J-RAS across multiple\nsegmentation backbones, including U-Net, TransUNet, SAM, and SegFormer, on two\nbenchmark datasets: ACDC and M&Ms, demonstrating consistent improvements. For\nexample, on the ACDC dataset, SegFormer without J-RAS achieves a mean Dice\nscore of 0.8708$\\pm$0.042 and a mean Hausdorff Distance (HD) of\n1.8130$\\pm$2.49, whereas with J-RAS, the performance improves substantially to\na mean Dice score of 0.9115$\\pm$0.031 and a mean HD of 1.1489$\\pm$0.30. These\nresults highlight the method's effectiveness and its generalizability across\narchitectures and datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a J-RAS \u7684\u8054\u5408\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u5f15\u5bfc\u56fe\u50cf\u5206\u5272\uff0c\u8be5\u65b9\u6cd5\u96c6\u6210\u4e86\u5206\u5272\u6a21\u578b\u548c\u68c0\u7d22\u6a21\u578b\u3002", "motivation": "\u533b\u7597\u5e94\u7528\u4e2d\u7684\u56fe\u50cf\u5206\u5272\u5bf9\u4e8e\u51c6\u786e\u8bca\u65ad\u3001\u6cbb\u7597\u8ba1\u5212\u548c\u75be\u75c5\u76d1\u6d4b\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u4eba\u5de5\u5206\u5272\u7ed3\u679c\u7cbe\u786e\uff0c\u4f46\u8017\u65f6\u3001\u6210\u672c\u9ad8\u4e14\u5bb9\u6613\u56e0\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u7684\u5dee\u5f02\u800c\u4ea7\u751f\u53d8\u5f02\u3002\u4eba\u5de5\u667a\u80fd (AI) \u65b9\u6cd5\u5df2\u88ab\u5f00\u53d1\u51fa\u6765\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u9700\u8981\u5927\u578b\u5e26\u6ce8\u91ca\u7684\u6570\u636e\u96c6\uff0c\u800c\u8fd9\u4e9b\u6570\u636e\u96c6\u5728\u5b9e\u8df5\u4e2d\u5f88\u5c11\u53ef\u7528\uff0c\u5e76\u4e14\u7531\u4e8e\u60a3\u8005\u95f4\u7684\u5dee\u5f02\u548c\u7f55\u89c1\u7684\u75c5\u7406\u75c5\u4f8b\uff0c\u5e38\u5e38\u96be\u4ee5\u63a8\u5e7f\u5230\u4e0d\u540c\u7684\u6210\u50cf\u6761\u4ef6\u3002", "method": "\u63d0\u51fa Joint Retrieval Augmented Segmentation (J-RAS)\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u5f15\u5bfc\u56fe\u50cf\u5206\u5272\u7684\u8054\u5408\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u96c6\u6210\u4e86\u5206\u5272\u6a21\u578b\u548c\u68c0\u7d22\u6a21\u578b\u3002\u4e24\u4e2a\u6a21\u578b\u7ecf\u8fc7\u8054\u5408\u4f18\u5316\uff0c\u4f7f\u5206\u5272\u6a21\u578b\u80fd\u591f\u5229\u7528\u68c0\u7d22\u5230\u7684\u56fe\u50cf-\u63a9\u7801\u5bf9\u6765\u4e30\u5bcc\u5176\u89e3\u5256\u5b66\u7406\u89e3\uff0c\u800c\u68c0\u7d22\u6a21\u578b\u5219\u5b66\u4e60\u5206\u5272\u76f8\u5173\u7684\u7279\u5f81\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u7b80\u5355\u7684\u89c6\u89c9\u76f8\u4f3c\u6027\u3002", "result": "\u5728 ACDC \u6570\u636e\u96c6\u4e0a\uff0c\u6ca1\u6709 J-RAS \u7684 SegFormer \u7684\u5e73\u5747 Dice \u5f97\u5206\u4e3a 0.8708\uff0c\u5e73\u5747 Hausdorff \u8ddd\u79bb (HD) \u4e3a 1.8130\uff0c\u800c\u4f7f\u7528 J-RAS \u540e\uff0c\u6027\u80fd\u663e\u7740\u63d0\u9ad8\u5230\u5e73\u5747 Dice \u5f97\u5206\u4e3a 0.9115\uff0c\u5e73\u5747 HD \u4e3a 1.1489\u3002", "conclusion": "\u9a8c\u8bc1\u4e86 J-RAS \u5728\u591a\u4e2a\u5206\u5272\u9aa8\u5e72\u7f51\u7edc\uff08\u5305\u62ec U-Net\u3001TransUNet\u3001SAM \u548c SegFormer\uff09\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6 ACDC \u548c M&Ms \u4e0a\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u6027\u3002"}}
{"id": "2510.10454", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10454", "abs": "https://arxiv.org/abs/2510.10454", "authors": ["Sihang Zeng", "Yujuan Fu", "Sitong Zhou", "Zixuan Yu", "Lucas Jing Liu", "Jun Wen", "Matthew Thompson", "Ruth Etzioni", "Meliha Yetisgen"], "title": "Traj-CoA: Patient Trajectory Modeling via Chain-of-Agents for Lung Cancer Risk Prediction", "comment": "Accepted by NeurIPS 2025 GenAI4Health Workshop", "summary": "Large language models (LLMs) offer a generalizable approach for modeling\npatient trajectories, but suffer from the long and noisy nature of electronic\nhealth records (EHR) data in temporal reasoning. To address these challenges,\nwe introduce Traj-CoA, a multi-agent system involving chain-of-agents for\npatient trajectory modeling. Traj-CoA employs a chain of worker agents to\nprocess EHR data in manageable chunks sequentially, distilling critical events\ninto a shared long-term memory module, EHRMem, to reduce noise and preserve a\ncomprehensive timeline. A final manager agent synthesizes the worker agents'\nsummary and the extracted timeline in EHRMem to make predictions. In a\nzero-shot one-year lung cancer risk prediction task based on five-year EHR\ndata, Traj-CoA outperforms baselines of four categories. Analysis reveals that\nTraj-CoA exhibits clinically aligned temporal reasoning, establishing it as a\npromisingly robust and generalizable approach for modeling complex patient\ntrajectories.", "AI": {"tldr": "Traj-CoA is a multi-agent system that models patient trajectories using LLMs. It processes EHR data sequentially, distills critical events, and uses a manager agent to make predictions.", "motivation": "LLMs can model patient trajectories but struggle with long and noisy EHR data.", "method": "Traj-CoA uses a chain of worker agents to process EHR data in chunks, distilling critical events into a shared memory module (EHRMem). A manager agent then synthesizes the summaries and timeline to make predictions.", "result": "Traj-CoA outperforms baselines in a zero-shot one-year lung cancer risk prediction task.", "conclusion": "Traj-CoA is a robust and generalizable approach for modeling complex patient trajectories, exhibiting clinically aligned temporal reasoning."}}
{"id": "2510.09719", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09719", "abs": "https://arxiv.org/abs/2510.09719", "authors": ["Chenxu Wang", "Hao Li", "Yiqun Zhang", "Linyao Chen", "Jianhao Chen", "Ping Jian", "Peng Ye", "Qiaosheng Zhang", "Shuyue Hu"], "title": "ICL-Router: In-Context Learned Model Representations for LLM Routing", "comment": null, "summary": "Large language models (LLMs) often exhibit complementary strengths. Model\nrouting harnesses these strengths by dynamically directing each query to the\nmost suitable model, given a candidate model pool. However, routing performance\nrelies on accurate model representations, and adding new models typically\nrequires retraining, limiting scalability. To address these challenges, we\npropose a novel routing method using in-context vectors to represent model\ncapabilities. The method proceeds in two stages. First, queries are embedded\nand projected into vectors, with a projector and LLM-based router trained to\nreconstruct the original queries, aligning vector representations with the\nrouter's semantic space. Second, each candidate model is profiled on a query\nset, and the router learns -- based on in-context vectors of query and model\nperformance -- to predict whether each model can correctly answer new queries.\nExtensive experiments demonstrate that our method achieves state-of-the-art\nrouting performance in both in-distribution and out-of-distribution tasks.\nMoreover, our method allows for seamless integration of new models without\nretraining the router. The code is available at\nhttps://github.com/lalalamdbf/ICL-Router.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u8def\u7531\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u4e0a\u4e0b\u6587\u5411\u91cf\u6765\u8868\u793a\u6a21\u578b\u7684\u80fd\u529b\uff0c\u4ece\u800c\u53ef\u4ee5\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u8def\u7531\u5668\u7684\u60c5\u51b5\u4e0b\u65e0\u7f1d\u96c6\u6210\u65b0\u6a21\u578b\u3002", "motivation": "\u6a21\u578b\u8def\u7531\u53ef\u4ee5\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u4e92\u8865\u4f18\u52bf\uff0c\u4f46\u662f\u8def\u7531\u6027\u80fd\u4f9d\u8d56\u4e8e\u51c6\u786e\u7684\u6a21\u578b\u8868\u793a\uff0c\u5e76\u4e14\u6dfb\u52a0\u65b0\u6a21\u578b\u901a\u5e38\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\uff0c\u4ece\u800c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u5206\u4e24\u4e2a\u9636\u6bb5\u8fdb\u884c\u3002\u9996\u5148\uff0c\u67e5\u8be2\u88ab\u5d4c\u5165\u5e76\u6295\u5f71\u5230\u5411\u91cf\u4e2d\uff0c\u6295\u5f71\u4eea\u548c\u57fa\u4e8e LLM \u7684\u8def\u7531\u5668\u7ecf\u8fc7\u8bad\u7ec3\u4ee5\u91cd\u5efa\u539f\u59cb\u67e5\u8be2\uff0c\u4ece\u800c\u4f7f\u5411\u91cf\u8868\u793a\u4e0e\u8def\u7531\u5668\u7684\u8bed\u4e49\u7a7a\u95f4\u5bf9\u9f50\u3002\u5176\u6b21\uff0c\u5728\u67e5\u8be2\u96c6\u4e0a\u5206\u6790\u6bcf\u4e2a\u5019\u9009\u6a21\u578b\uff0c\u5e76\u4e14\u8def\u7531\u5668\u5b66\u4e60\u2014\u2014\u57fa\u4e8e\u67e5\u8be2\u548c\u6a21\u578b\u6027\u80fd\u7684\u4e0a\u4e0b\u6587\u5411\u91cf\u2014\u2014\u9884\u6d4b\u6bcf\u4e2a\u6a21\u578b\u662f\u5426\u53ef\u4ee5\u6b63\u786e\u56de\u7b54\u65b0\u67e5\u8be2\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u540c\u5206\u5e03\u548c\u5f02\u5206\u5e03\u4efb\u52a1\u4e2d\u90fd\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8def\u7531\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5141\u8bb8\u65e0\u7f1d\u96c6\u6210\u65b0\u6a21\u578b\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u8def\u7531\u5668\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6a21\u578b\u8def\u7531\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u96c6\u6210\u65b0\u6a21\u578b\uff0c\u5e76\u4e14\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.10009", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10009", "abs": "https://arxiv.org/abs/2510.10009", "authors": ["Shu Zhao", "Tan Yu", "Anbang Xu"], "title": "Beyond the limitation of a single query: Train your LLM for query expansion with Reinforcement Learning", "comment": null, "summary": "Reasoning-augmented search agents, such as Search-R1, are trained to reason,\nsearch, and generate the final answer iteratively. Nevertheless, due to their\nlimited capabilities in reasoning and search, their performance on multi-hop QA\nbenchmarks remains far from satisfactory. To handle complex or compound\nqueries, we train an LLM-based search agent with the native capability of query\nexpansion through reinforcement learning. In each turn, our search agent\nproposes several query variants, which are searched simultaneously to cover\nmore relevant information. Meanwhile, given limited post-training data and\ncomputing resources, it is very challenging for a search agent to master\nmultiple tasks, including query generation, retrieved information\nunderstanding, and answer generation. Therefore, we propose incorporating a\npre-trained squeezer model that helps the search agent understand the retrieved\ndocuments, allowing the search agent to focus on query generation for high\nretrieval recall. With the assistance of the squeezer model, we discover that\neven a small-scale 3B LLM can demonstrate a strong capability of query\nexpansion and achieve state-of-the-art accuracy on the multi-hop QA benchmarks.\nTo be specific, our experiments across seven question-answering benchmarks\ndemonstrate that our method, named ExpandSearch, achieves an average\nimprovement of 4.4% compared to state-of-the-art baselines, with strong gains\non multi-hop reasoning tasks requiring diverse evidence aggregation.", "AI": {"tldr": "\u63d0\u51faExpandSearch\uff0c\u4e00\u4e2a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684LLM\u641c\u7d22\u4ee3\u7406\uff0c\u7528\u4e8e\u591a\u8df3QA\u4efb\u52a1\uff0c\u901a\u8fc7\u67e5\u8be2\u6269\u5c55\u548c\u6587\u6863\u538b\u7f29\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u641c\u7d22\u4ee3\u7406\u5728\u591a\u8df3QA\u4efb\u52a1\u4e2d\u6027\u80fd\u4e0d\u4f73\uff0c\u56e0\u4e3a\u63a8\u7406\u548c\u641c\u7d22\u80fd\u529b\u6709\u9650\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u67e5\u8be2\u3002", "method": "\u8bad\u7ec3\u4e00\u4e2aLLM\u641c\u7d22\u4ee3\u7406\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u67e5\u8be2\u6269\u5c55\uff0c\u5e76\u5f15\u5165\u9884\u8bad\u7ec3\u7684\u538b\u7f29\u6a21\u578b\u6765\u7406\u89e3\u68c0\u7d22\u5230\u7684\u6587\u6863\u3002", "result": "ExpandSearch\u5728\u4e03\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u5e73\u57474.4%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5c24\u5176\u5728\u9700\u8981\u591a\u6837\u8bc1\u636e\u805a\u5408\u7684\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u5373\u4f7f\u662f\u5c0f\u89c4\u6a21\u76843B LLM\uff0c\u5728\u538b\u7f29\u6a21\u578b\u7684\u8f85\u52a9\u4e0b\uff0c\u4e5f\u80fd\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u67e5\u8be2\u6269\u5c55\u80fd\u529b\uff0c\u5e76\u5728\u591a\u8df3QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f73\u7cbe\u5ea6\u3002"}}
{"id": "2510.09994", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.09994", "abs": "https://arxiv.org/abs/2510.09994", "authors": ["Yimin Xiao", "Yongle Zhang", "Dayeon Ki", "Calvin Bao", "Marianna J. Martindale", "Charlotte Vaughn", "Ge Gao", "Marine Carpuat"], "title": "Toward Machine Translation Literacy: How Lay Users Perceive and Rely on Imperfect Translations", "comment": "EMNLP 2025", "summary": "As Machine Translation (MT) becomes increasingly commonplace, understanding\nhow the general public perceives and relies on imperfect MT is crucial for\ncontextualizing MT research in real-world applications. We present a human\nstudy conducted in a public museum (n=452), investigating how fluency and\nadequacy errors impact bilingual and non-bilingual users' reliance on MT during\ncasual use. Our findings reveal that non-bilingual users often over-rely on MT\ndue to a lack of evaluation strategies and alternatives, while experiencing the\nimpact of errors can prompt users to reassess future reliance. This highlights\nthe need for MT evaluation and NLP explanation techniques to promote not only\nMT quality, but also MT literacy among its users.", "AI": {"tldr": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u516c\u4f17\u5bf9\u673a\u5668\u7ffb\u8bd1(MT)\u7684\u770b\u6cd5\u548c\u4f9d\u8d56\u7a0b\u5ea6\uff0c\u7279\u522b\u5173\u6ce8\u4e86\u6d41\u7545\u6027\u548c\u5145\u5206\u6027\u9519\u8bef\u5bf9\u53cc\u8bed\u548c\u975e\u53cc\u8bed\u7528\u6237\u5728\u65e5\u5e38\u4f7f\u7528\u4e2d\u5bf9MT\u7684\u4f9d\u8d56\u7684\u5f71\u54cd\u3002", "motivation": "\u4e86\u89e3\u516c\u4f17\u5982\u4f55\u770b\u5f85\u548c\u4f9d\u8d56\u4e0d\u5b8c\u5584\u7684\u673a\u5668\u7ffb\u8bd1\u5bf9\u4e8e\u5c06\u673a\u5668\u7ffb\u8bd1\u7814\u7a76\u7f6e\u4e8e\u771f\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5728\u4e00\u5bb6\u516c\u5171\u535a\u7269\u9986\u8fdb\u884c\u4e86\u4e00\u9879\u4eba\u7c7b\u7814\u7a76(n=452)\uff0c\u8c03\u67e5\u4e86\u6d41\u7545\u6027\u548c\u5145\u5206\u6027\u9519\u8bef\u5982\u4f55\u5f71\u54cd\u53cc\u8bed\u548c\u975e\u53cc\u8bed\u7528\u6237\u5728 casual \u4f7f\u7528\u4e2d\u5bf9 MT \u7684\u4f9d\u8d56\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7531\u4e8e\u7f3a\u4e4f\u8bc4\u4f30\u7b56\u7565\u548c\u66ff\u4ee3\u65b9\u6848\uff0c\u975e\u53cc\u8bed\u7528\u6237\u901a\u5e38\u8fc7\u5ea6\u4f9d\u8d56 MT\uff0c\u800c\u4f53\u9a8c\u5230\u9519\u8bef\u7684\u5f71\u54cd\u4f1a\u4fc3\u4f7f\u7528\u6237\u91cd\u65b0\u8bc4\u4f30\u672a\u6765\u4f9d\u8d56\u3002", "conclusion": "\u5f3a\u8c03\u9700\u8981 MT \u8bc4\u4f30\u548c NLP \u89e3\u91ca\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8 MT \u8d28\u91cf\uff0c\u5e76\u63d0\u9ad8\u7528\u6237\u4e2d\u7684 MT \u7d20\u517b\u3002"}}
{"id": "2510.09981", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.09981", "abs": "https://arxiv.org/abs/2510.09981", "authors": ["Fan Zuo", "Donglin Zhou", "Jingqin Gao", "Kaan Ozbay"], "title": "Scaling Traffic Insights with AI and Language Model-Powered Camera Systems for Data-Driven Transportation Decision Making", "comment": null, "summary": "Accurate, scalable traffic monitoring is critical for real-time and long-term\ntransportation management, particularly during disruptions such as natural\ndisasters, large construction projects, or major policy changes like New York\nCity's first-in-the-nation congestion pricing program. However, widespread\nsensor deployment remains limited due to high installation, maintenance, and\ndata management costs. While traffic cameras offer a cost-effective\nalternative, existing video analytics struggle with dynamic camera viewpoints\nand massive data volumes from large camera networks. This study presents an\nend-to-end AI-based framework leveraging existing traffic camera infrastructure\nfor high-resolution, longitudinal analysis at scale. A fine-tuned YOLOv11\nmodel, trained on localized urban scenes, extracts multimodal traffic density\nand classification metrics in real time. To address inconsistencies from\nnon-stationary pan-tilt-zoom cameras, we introduce a novel graph-based\nviewpoint normalization method. A domain-specific large language model was also\nintegrated to process massive data from a 24/7 video stream to generate\nfrequent, automated summaries of evolving traffic patterns, a task far\nexceeding manual capabilities. We validated the system using over 9 million\nimages from roughly 1,000 traffic cameras during the early rollout of NYC\ncongestion pricing in 2025. Results show a 9% decline in weekday passenger\nvehicle density within the Congestion Relief Zone, early truck volume\nreductions with signs of rebound, and consistent increases in pedestrian and\ncyclist activity at corridor and zonal scales. Experiments showed that\nexample-based prompts improved LLM's numerical accuracy and reduced\nhallucinations. These findings demonstrate the framework's potential as a\npractical, infrastructure-ready solution for large-scale, policy-relevant\ntraffic monitoring with minimal human intervention.", "AI": {"tldr": "This paper introduces an AI framework using traffic cameras for large-scale traffic monitoring.", "motivation": "The motivation is to overcome the limitations of traditional traffic monitoring methods, which are costly and difficult to scale, and existing video analytics, which struggle with dynamic viewpoints and large data volumes.", "method": "The method involves a fine-tuned YOLOv11 model for real-time traffic density and classification, a graph-based viewpoint normalization method for non-stationary cameras, and a domain-specific large language model for automated summaries.", "result": "The system was validated with 9 million images from 1,000 cameras during NYC's congestion pricing rollout in 2025, showing a 9% decline in passenger vehicle density, early truck volume reductions, and increases in pedestrian/cyclist activity.", "conclusion": "The framework offers a practical solution for large-scale, policy-relevant traffic monitoring with minimal human intervention."}}
{"id": "2510.10461", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10461", "abs": "https://arxiv.org/abs/2510.10461", "authors": ["Hongjie Zheng", "Zesheng Shi", "Ping Yi"], "title": "MedCoAct: Confidence-Aware Multi-Agent Collaboration for Complete Clinical Decision", "comment": null, "summary": "Autonomous agents utilizing Large Language Models (LLMs) have demonstrated\nremarkable capabilities in isolated medical tasks like diagnosis and image\nanalysis, but struggle with integrated clinical workflows that connect\ndiagnostic reasoning and medication decisions. We identify a core limitation:\nexisting medical AI systems process tasks in isolation without the\ncross-validation and knowledge integration found in clinical teams, reducing\ntheir effectiveness in real-world healthcare scenarios. To transform the\nisolation paradigm into a collaborative approach, we propose MedCoAct, a\nconfidence-aware multi-agent framework that simulates clinical collaboration by\nintegrating specialized doctor and pharmacist agents, and present a benchmark,\nDrugCareQA, to evaluate medical AI capabilities in integrated diagnosis and\ntreatment workflows. Our results demonstrate that MedCoAct achieves 67.58\\%\ndiagnostic accuracy and 67.58\\% medication recommendation accuracy,\noutperforming single agent framework by 7.04\\% and 7.08\\% respectively. This\ncollaborative approach generalizes well across diverse medical domains, proving\nespecially effective for telemedicine consultations and routine clinical\nscenarios, while providing interpretable decision-making pathways.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4fe1\u5fc3\u611f\u77e5\u591a\u667a\u80fd\u4f53\u6846\u67b6MedCoAct\uff0c\u901a\u8fc7\u6574\u5408\u533b\u751f\u548c\u836f\u5242\u5e08\u667a\u80fd\u4f53\u6765\u6a21\u62df\u4e34\u5e8a\u534f\u4f5c\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u7597AI\u7cfb\u7edf\u5728\u5b64\u7acb\u7684\u73af\u5883\u4e2d\u5904\u7406\u4efb\u52a1\uff0c\u7f3a\u4e4f\u4e34\u5e8a\u56e2\u961f\u4e2d\u7684\u4ea4\u53c9\u9a8c\u8bc1\u548c\u77e5\u8bc6\u6574\u5408\uff0c\u964d\u4f4e\u4e86\u5b83\u4eec\u5728\u5b9e\u9645\u533b\u7597\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86MedCoAct\uff0c\u4e00\u4e2a\u4fe1\u5fc3\u611f\u77e5\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u4e13\u4e1a\u7684\u533b\u751f\u548c\u836f\u5242\u5e08\u667a\u80fd\u4f53\u6765\u6a21\u62df\u4e34\u5e8a\u534f\u4f5c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5DrugCareQA\uff0c\u4ee5\u8bc4\u4f30\u533b\u7597AI\u5728\u7efc\u5408\u8bca\u65ad\u548c\u6cbb\u7597\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u80fd\u529b\u3002", "result": "MedCoAct\u5b9e\u73b0\u4e8667.58%\u7684\u8bca\u65ad\u51c6\u786e\u7387\u548c67.58%\u7684\u836f\u7269\u63a8\u8350\u51c6\u786e\u7387\uff0c\u5206\u522b\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u6846\u67b67.04%\u548c7.08%\u3002", "conclusion": "\u8fd9\u79cd\u534f\u4f5c\u65b9\u6cd5\u53ef\u4ee5\u5f88\u597d\u5730\u63a8\u5e7f\u5230\u4e0d\u540c\u7684\u533b\u7597\u9886\u57df\uff0c\u5c24\u5176\u662f\u5728\u8fdc\u7a0b\u533b\u7597\u54a8\u8be2\u548c\u5e38\u89c4\u4e34\u5e8a\u573a\u666f\u4e2d\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u8def\u5f84\u3002"}}
{"id": "2510.09723", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T05 (Primary), 68T50", "I.2.6; I.2.7; I.5.4"], "pdf": "https://arxiv.org/pdf/2510.09723", "abs": "https://arxiv.org/abs/2510.09723", "authors": ["Gregory D. Baker"], "title": "It's 2025 -- Narrative Learning is the new baseline to beat for explainable machine learning", "comment": "18 pages, 5 figures", "summary": "In this paper, we introduce Narrative Learning, a methodology where models\nare defined entirely in natural language and iteratively refine their\nclassification criteria using explanatory prompts rather than traditional\nnumerical optimisation. We report on experiments to evaluate the accuracy and\npotential of this approach using 3 synthetic and 3 natural datasets and compare\nthem against 7 baseline explainable machine learning models. We demonstrate\nthat on 5 out of 6 of these datasets, Narrative Learning became more accurate\nthan the baseline explainable models in 2025 or earlier because of improvements\nin language models. We also report on trends in the lexicostatistics of these\nmodels' outputs as a proxy for the comprehensibility of the explanations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u53d9\u4e8b\u5b66\u4e60\u201d\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5b8c\u5168\u7528\u81ea\u7136\u8bed\u8a00\u5b9a\u4e49\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u89e3\u91ca\u6027\u63d0\u793a\u800c\u4e0d\u662f\u4f20\u7edf\u7684\u6570\u503c\u4f18\u5316\u6765\u8fed\u4ee3\u5730\u6539\u8fdb\u5176\u5206\u7c7b\u6807\u51c6\u3002", "motivation": "\u63a2\u7d22\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u5b9a\u4e49\u548c\u6539\u8fdb\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u800c\u975e\u4f20\u7edf\u7684\u6570\u503c\u4f18\u5316\u3002", "method": "\u4f7f\u7528\u89e3\u91ca\u6027\u63d0\u793a\u8fed\u4ee3\u6539\u8fdb\u6a21\u578b\u7684\u5206\u7c7b\u6807\u51c6\uff0c\u5e76\u4f7f\u75283\u4e2a\u5408\u6210\u6570\u636e\u96c6\u548c3\u4e2a\u81ea\u7136\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u57286\u4e2a\u6570\u636e\u96c6\u4e2d\u76845\u4e2a\u4e0a\uff0c\u53d9\u4e8b\u5b66\u4e60\u6bd4\u57fa\u7ebf\u53ef\u89e3\u91ca\u6a21\u578b\u66f4\u51c6\u786e\u3002", "conclusion": "\u53d9\u4e8b\u5b66\u4e60\u662f\u4e00\u79cd\u6709\u6f5c\u529b\u7684\u65b9\u6cd5\uff0c\u5176\u51c6\u786e\u6027\u4f18\u4e8e\u57fa\u7ebf\u53ef\u89e3\u91ca\u6a21\u578b\u3002"}}
{"id": "2510.10224", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10224", "abs": "https://arxiv.org/abs/2510.10224", "authors": ["Ruize An", "Richong Zhang", "Zhijie Nie", "Zhanyu Wu", "Yanzhao Zhang", "Dingkun Long"], "title": "Text2Token: Unsupervised Text Representation Learning with Token Target Prediction", "comment": null, "summary": "Unsupervised text representation learning (TRL) is a fundamental task in\nnatural language processing, which is beneficial for improving search and\nrecommendations with the web's unlabeled texts. A recent empirical study finds\nthat the high-quality representation aligns with the key token of the input\ntext, uncovering the potential connection between representation space and\nvocabulary space. Inspired by the findings, we revisit the generative tasks and\ndevelop an unsupervised generative framework for TRL, Text2Token. The framework\nis based on the token target prediction task, utilizing carefully constructed\ntarget token distribution as supervisory signals. To construct the high-quality\ntarget token distribution, we analyze the token-alignment properties with\nadvanced embedders and identify two essential categories of key tokens: (1) the\nmeaningful tokens in the text and (2) semantically derived tokens beyond the\ntext. Based on these insights, we propose two methods -- data-driven and\nmodel-derived -- to construct synthetic token targets from data or the LLM\nbackbone. Experiments on the MTEB v2 benchmark demonstrate that Text2Token\nachieves performance competitive with the state-of-the-art embedder with\nunsupervised contrastive learning, LLM2Vec. Our analysis further shows that\nvocabulary and representation spaces optimize together and toward the optimum\nsolution during training, providing new ideas and insights for future work.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u76d1\u7763\u6587\u672c\u8868\u793a\u5b66\u4e60\u6846\u67b6Text2Token\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u9884\u6d4b\u76ee\u6807token\u6765\u5b66\u4e60\u6587\u672c\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u53d1\u73b0\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u8868\u793a\u4e0e\u8f93\u5165\u6587\u672c\u7684\u5173\u952etoken\u5bf9\u9f50\uff0c\u8fd9\u63ed\u793a\u4e86\u8868\u793a\u7a7a\u95f4\u548c\u8bcd\u6c47\u7a7a\u95f4\u4e4b\u95f4\u7684\u6f5c\u5728\u8054\u7cfb\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86Text2Token\u3002", "method": "Text2Token\u6846\u67b6\u57fa\u4e8etoken\u76ee\u6807\u9884\u6d4b\u4efb\u52a1\uff0c\u5e76\u5229\u7528\u7cbe\u5fc3\u6784\u5efa\u7684\u76ee\u6807token\u5206\u5e03\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\u3002\u4e3a\u4e86\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u76ee\u6807token\u5206\u5e03\uff0c\u672c\u6587\u5206\u6790\u4e86token\u5bf9\u9f50\u5c5e\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff0c\u5373\u6570\u636e\u9a71\u52a8\u548c\u6a21\u578b\u9a71\u52a8\uff0c\u4ee5\u4ece\u6570\u636e\u6216LLM\u4e3b\u5e72\u6784\u5efa\u5408\u6210token\u76ee\u6807\u3002", "result": "\u5728MTEB v2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cText2Token\u53d6\u5f97\u4e86\u4e0e\u6700\u5148\u8fdb\u7684\u65e0\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u5d4c\u5165\u5668LLM2Vec\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u8bcd\u6c47\u548c\u8868\u793a\u7a7a\u95f4\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5171\u540c\u4f18\u5316\u5e76\u671d\u7740\u6700\u4f18\u89e3\u53d1\u5c55\uff0c\u4e3a\u672a\u6765\u7684\u5de5\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u89c1\u89e3\u3002"}}
{"id": "2510.10003", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.10003", "abs": "https://arxiv.org/abs/2510.10003", "authors": ["Jianjin Wang", "Runsong Zhao", "Xiaoqian Liu", "Yuan Ge", "Ziqiang Xu", "Tong Xiao", "Shengxiang Gao", "Zhengtao Yu", "Jingbo Zhu"], "title": "MTP-S2UT: Enhancing Speech-to-Speech Translation Quality with Multi-token Prediction", "comment": "Copyright 2026 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Current direct speech-to-speech translation methods predominantly employ\nspeech tokens as intermediate representations. However, a single speech token\nis not dense in semantics, so we generally need multiple tokens to express a\ncomplete semantic unit. To address this limitation, we introduce multi-token\nprediction (MTP) loss into speech-to-unit translation (S2UT) models, enabling\nmodels to predict multiple subsequent tokens at each position, thereby\ncapturing more complete semantics and enhancing information density per\nposition. Initial MTP implementations apply the loss at the final layer, which\nimproves output representation but initiates information enrichment too late.\nWe hypothesize that advancing the information enrichment process to\nintermediate layers can achieve earlier and more effective enhancement of\nhidden representation. Consequently, we propose MTP-S2UT loss, applying MTP\nloss to hidden representation where CTC loss is computed. Experiments\ndemonstrate that all MTP loss variants consistently improve the quality of S2UT\ntranslation, with MTP-S2UT achieving the best performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u8bed\u97f3\u5230\u5355\u5143\u7ffb\u8bd1(S2UT)\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u591atoken\u9884\u6d4b(MTP)\u635f\u5931\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u6bcf\u4e2a\u4f4d\u7f6e\u9884\u6d4b\u591a\u4e2a\u540e\u7eedtoken\uff0c\u4ece\u800c\u6355\u83b7\u66f4\u5b8c\u6574\u7684\u8bed\u4e49\u3002", "motivation": "\u5f53\u524d\u8bed\u97f3\u5230\u8bed\u97f3\u7ffb\u8bd1\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528\u8bed\u97f3token\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u4f46\u5355\u4e2a\u8bed\u97f3token\u7684\u8bed\u4e49\u5bc6\u5ea6\u4e0d\u9ad8\uff0c\u901a\u5e38\u9700\u8981\u591a\u4e2atoken\u6765\u8868\u8fbe\u5b8c\u6574\u7684\u8bed\u4e49\u5355\u5143\u3002", "method": "\u5c06\u591atoken\u9884\u6d4b(MTP)\u635f\u5931\u5f15\u5165\u5230\u8bed\u97f3\u5230\u5355\u5143\u7ffb\u8bd1(S2UT)\u6a21\u578b\u4e2d\uff0c\u5e76\u63d0\u51faMTP-S2UT\u635f\u5931\uff0c\u5c06MTP\u635f\u5931\u5e94\u7528\u4e8e\u8ba1\u7b97CTC\u635f\u5931\u7684\u9690\u85cf\u5c42\u8868\u793a\u3002", "result": "\u6240\u6709MTP\u635f\u5931\u53d8\u4f53\u90fd\u80fd\u6301\u7eed\u63d0\u9ad8S2UT\u7ffb\u8bd1\u7684\u8d28\u91cf\uff0c\u5176\u4e2dMTP-S2UT\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684MTP-S2UT\u65b9\u6cd5\u80fd\u591f\u66f4\u65e9\u3001\u66f4\u6709\u6548\u5730\u589e\u5f3a\u9690\u85cf\u5c42\u8868\u793a\uff0c\u4ece\u800c\u63d0\u5347\u8bed\u97f3\u7ffb\u8bd1\u8d28\u91cf\u3002"}}
{"id": "2510.09995", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09995", "abs": "https://arxiv.org/abs/2510.09995", "authors": ["Lishen Qu", "Zhihao Liu", "Jinshan Pan", "Shihao Zhou", "Jinglei Shi", "Duosheng Chen", "Jufeng Yang"], "title": "FlareX: A Physics-Informed Dataset for Lens Flare Removal via 2D Synthesis and 3D Rendering", "comment": "Accepted by NeurIPS 2025", "summary": "Lens flare occurs when shooting towards strong light sources, significantly\ndegrading the visual quality of images. Due to the difficulty in capturing\nflare-corrupted and flare-free image pairs in the real world, existing datasets\nare typically synthesized in 2D by overlaying artificial flare templates onto\nbackground images. However, the lack of flare diversity in templates and the\nneglect of physical principles in the synthesis process hinder models trained\non these datasets from generalizing well to real-world scenarios. To address\nthese challenges, we propose a new physics-informed method for flare data\ngeneration, which consists of three stages: parameterized template creation,\nthe laws of illumination-aware 2D synthesis, and physical engine-based 3D\nrendering, which finally gives us a mixed flare dataset that incorporates both\n2D and 3D perspectives, namely FlareX. This dataset offers 9,500 2D templates\nderived from 95 flare patterns and 3,000 flare image pairs rendered from 60 3D\nscenes. Furthermore, we design a masking approach to obtain real-world\nflare-free images from their corrupted counterparts to measure the performance\nof the model on real-world images. Extensive experiments demonstrate the\neffectiveness of our method and dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7269\u7406\u4fe1\u606f\u65b9\u6cd5\u7528\u4e8e\u751f\u6210\u955c\u5934\u5149\u6655\u6570\u636e\uff0c\u5305\u62ec\u53c2\u6570\u5316\u6a21\u677f\u521b\u5efa\u3001\u5149\u7167\u611f\u77e5 2D \u5408\u6210\u548c\u57fa\u4e8e\u7269\u7406\u5f15\u64ce\u7684 3D \u6e32\u67d3\uff0c\u4ece\u800c\u751f\u6210\u6df7\u5408\u7684 FlareX \u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u7684\u955c\u5934\u5149\u6655\u6570\u636e\u96c6\u901a\u5e38\u5728 2D \u4e2d\u5408\u6210\uff0c\u7f3a\u4e4f\u5149\u6655\u591a\u6837\u6027\uff0c\u5ffd\u7565\u4e86\u7269\u7406\u539f\u7406\uff0c\u5bfc\u81f4\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u9636\u6bb5\uff1a\u53c2\u6570\u5316\u6a21\u677f\u521b\u5efa\u3001\u5149\u7167\u611f\u77e5 2D \u5408\u6210\u548c\u57fa\u4e8e\u7269\u7406\u5f15\u64ce\u7684 3D \u6e32\u67d3\uff0c\u751f\u6210\u5305\u542b 2D \u548c 3D \u89c6\u89d2\u7684\u6df7\u5408 FlareX \u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u63a9\u853d\u65b9\u6cd5\u4ece\u771f\u5b9e\u4e16\u754c\u7684\u56fe\u50cf\u4e2d\u83b7\u5f97\u65e0\u5149\u6655\u56fe\u50cf\u3002", "result": "\u8be5\u6570\u636e\u96c6\u63d0\u4f9b\u4e86 9,500 \u4e2a 2D \u6a21\u677f\u548c 3,000 \u4e2a 3D \u6e32\u67d3\u7684\u56fe\u50cf\u5bf9\u3002\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7269\u7406\u4fe1\u606f\u65b9\u6cd5\u7528\u4e8e\u751f\u6210\u955c\u5934\u5149\u6655\u6570\u636e\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u6df7\u5408\u7684 FlareX \u6570\u636e\u96c6\uff0c\u4e3a\u955c\u5934\u5149\u6655\u53bb\u9664\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u8d44\u6e90\u3002"}}
{"id": "2510.10494", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10494", "abs": "https://arxiv.org/abs/2510.10494", "authors": ["Martina G. Vilas", "Safoora Yousefi", "Besmira Nushi", "Eric Horvitz", "Vidhisha Balachandran"], "title": "Tracing the Traces: Latent Temporal Signals for Efficient and Accurate Reasoning", "comment": null, "summary": "Reasoning models improve their problem-solving ability through inference-time\nscaling, allocating more compute via longer token budgets. Identifying which\nreasoning traces are likely to succeed remains a key opportunity: reliably\npredicting productive paths can substantially reduce wasted computation and\nimprove overall efficiency. We introduce Latent-Trajectory signals that\ncharacterize the temporal evolution of a model's internal representations\nduring the generation of intermediate reasoning tokens. By measuring the\noverall change in latent representations between the start and end of\nreasoning, the change accumulated across intermediate steps, and the extent to\nwhich these changes advance toward the final state, we show that these signals\npredict solution accuracy more reliably than both cross-layer metrics and\noutput-based confidence measures. When used to guide answer selection across\nmultiple sampled generations, Latent-Trajectory signals make test-time scaling\nmore effective and efficient than majority voting, reducing token usage by up\nto 70% while preserving and even improving accuracy by 2.6% on average.\nMoreover, these predictive signals often emerge early in the reasoning trace,\nenabling early selection and allocation of compute to the most promising\ncandidates. Our findings contribute not only practical strategies for\ninference-time efficiency, but also a deeper interpretability perspective on\nhow reasoning processes are represented and differentiated in latent space.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5185\u90e8\u8868\u5f81\u7684\u6f14\u53d8\u6765\u9884\u6d4b\u63a8\u7406\u8f68\u8ff9\u7684\u6210\u529f\u7387\uff0c\u4ece\u800c\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u63a8\u7406\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u5206\u914d\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90\u6765\u63d0\u9ad8\u89e3\u51b3\u95ee\u9898\u7684\u80fd\u529b\uff0c\u4f46\u8bc6\u522b\u54ea\u4e9b\u63a8\u7406\u8f68\u8ff9\u53ef\u80fd\u6210\u529f\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u6f5c\u5728\u8f68\u8ff9\u4fe1\u53f7\uff0c\u901a\u8fc7\u6d4b\u91cf\u6a21\u578b\u5728\u751f\u6210\u4e2d\u95f4\u63a8\u7406 tokens \u671f\u95f4\u5185\u90e8\u8868\u5f81\u7684\u968f\u65f6\u95f4\u6f14\u53d8\u6765\u8868\u5f81\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6f5c\u5728\u8f68\u8ff9\u4fe1\u53f7\u6bd4\u8de8\u5c42\u6307\u6807\u548c\u57fa\u4e8e\u8f93\u51fa\u7684\u7f6e\u4fe1\u5ea6\u6d4b\u91cf\u65b9\u6cd5\u66f4\u53ef\u9760\u5730\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\u7684\u51c6\u786e\u6027\u3002\u5f53\u7528\u4e8e\u6307\u5bfc\u8de8\u591a\u4e2a\u62bd\u6837\u4e16\u4ee3\u7684\u7b54\u6848\u9009\u62e9\u65f6\uff0c\u4e0e\u591a\u6570\u6295\u7968\u76f8\u6bd4\uff0c\u6f5c\u5728\u8f68\u8ff9\u4fe1\u53f7\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u5229\u7528\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\uff0c\u51cf\u5c11\u9ad8\u8fbe 70% \u7684 token \u4f7f\u7528\u91cf\uff0c\u540c\u65f6\u5e73\u5747\u4fdd\u6301\u751a\u81f3\u63d0\u9ad8 2.6% \u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u7ed3\u679c\u4e0d\u4ec5\u4e3a\u63a8\u7406\u65f6\u95f4\u6548\u7387\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u7b56\u7565\uff0c\u800c\u4e14\u8fd8\u4e3a\u63a8\u7406\u8fc7\u7a0b\u5982\u4f55\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8868\u793a\u548c\u533a\u5206\u63d0\u4f9b\u4e86\u66f4\u6df1\u5165\u7684\u53ef\u89e3\u91ca\u6027\u89c6\u89d2\u3002"}}
{"id": "2510.09732", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09732", "abs": "https://arxiv.org/abs/2510.09732", "authors": ["P. van Oerle", "R. H. Bemthuis", "F. A. Bukhsh"], "title": "Evaluating LLM-Based Process Explanations under Progressive Behavioral-Input Reduction", "comment": "12 pages, 2 figures, 3 tables; to appear in Enterprise Design,\n  Operations, and Computing. EDOC 2025 Workshops, Lecture Notes in Business\n  Information Processing (LNBIP), Springer, 2025. Part of 29th International\n  Conference on Enterprise Design, Operations, and Computing (EDOC)", "summary": "Large Language Models (LLMs) are increasingly used to generate textual\nexplanations of process models discovered from event logs. Producing\nexplanations from large behavioral abstractions (e.g., directly-follows graphs\nor Petri nets) can be computationally expensive. This paper reports an\nexploratory evaluation of explanation quality under progressive\nbehavioral-input reduction, where models are discovered from progressively\nsmaller prefixes of a fixed log. Our pipeline (i) discovers models at multiple\ninput sizes, (ii) prompts an LLM to generate explanations, and (iii) uses a\nsecond LLM to assess completeness, bottleneck identification, and suggested\nimprovements. On synthetic logs, explanation quality is largely preserved under\nmoderate reduction, indicating a practical cost-quality trade-off. The study is\nexploratory, as the scores are LLM-based (comparative signals rather than\nground truth) and the data are synthetic. The results suggest a path toward\nmore computationally efficient, LLM-assisted process analysis in\nresource-constrained settings.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6839\u636e\u4e8b\u4ef6\u65e5\u5fd7\u751f\u6210\u7684\u6d41\u7a0b\u6a21\u578b\u6587\u672c\u89e3\u91ca\u3002\u901a\u8fc7\u9010\u6b65\u7f29\u51cf\u884c\u4e3a\u8f93\u5165\uff0c\u5728\u4fdd\u8bc1\u89e3\u91ca\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4ece\u5927\u578b\u884c\u4e3a\u62bd\u8c61\uff08\u4f8b\u5982\u76f4\u63a5\u8ddf\u968f\u56fe\u6216Petri\u7f51\uff09\u751f\u6210\u89e3\u91ca\u7684\u8ba1\u7b97\u6210\u672c\u53ef\u80fd\u5f88\u9ad8\u3002", "method": "\u8be5\u7814\u7a76\u901a\u8fc7\u9010\u6b65\u7f29\u5c0f\u56fa\u5b9a\u65e5\u5fd7\u7684\u524d\u7f00\u6765\u53d1\u73b0\u6a21\u578b\uff0c\u7136\u540e\u63d0\u793aLLM\u751f\u6210\u89e3\u91ca\uff0c\u5e76\u4f7f\u7528\u7b2c\u4e8c\u4e2aLLM\u8bc4\u4f30\u5b8c\u6574\u6027\u3001\u74f6\u9888\u8bc6\u522b\u548c\u6539\u8fdb\u5efa\u8bae\u3002", "result": "\u5728\u5408\u6210\u65e5\u5fd7\u4e0a\uff0c\u89e3\u91ca\u8d28\u91cf\u5728\u9002\u5ea6\u7f29\u51cf\u7684\u60c5\u51b5\u4e0b\u57fa\u672c\u4fdd\u6301\u4e0d\u53d8\uff0c\u8fd9\u8868\u660e\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u4ee5\u5b9e\u73b0\u6210\u672c\u4e0e\u8d28\u91cf\u7684\u5e73\u8861\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\uff0c\u53ef\u4ee5\u5229\u7528LLM\u8f85\u52a9\u8fdb\u884c\u66f4\u9ad8\u6548\u7684\u6d41\u7a0b\u5206\u6790\u3002"}}
{"id": "2510.10241", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10241", "abs": "https://arxiv.org/abs/2510.10241", "authors": ["Kangyang Luo", "Yuzhuo Bai", "Shuzheng Si", "Cheng Gao", "Zhitong Wang", "Yingli Shen", "Wenhao Li", "Zhu Liu", "Yufeng Han", "Jiayi Wu", "Cunliang Kong", "Maosong Sun"], "title": "ImCoref-CeS: An Improved Lightweight Pipeline for Coreference Resolution with LLM-based Checker-Splitter Refinement", "comment": null, "summary": "Coreference Resolution (CR) is a critical task in Natural Language Processing\n(NLP). Current research faces a key dilemma: whether to further explore the\npotential of supervised neural methods based on small language models, whose\ndetect-then-cluster pipeline still delivers top performance, or embrace the\npowerful capabilities of Large Language Models (LLMs). However, effectively\ncombining their strengths remains underexplored. To this end, we propose\n\\textbf{ImCoref-CeS}, a novel framework that integrates an enhanced supervised\nmodel with LLM-based reasoning. First, we present an improved CR method\n(\\textbf{ImCoref}) to push the performance boundaries of the supervised neural\nmethod by introducing a lightweight bridging module to enhance long-text\nencoding capability, devising a biaffine scorer to comprehensively capture\npositional information, and invoking a hybrid mention regularization to improve\ntraining efficiency. Importantly, we employ an LLM acting as a multi-role\nChecker-Splitter agent to validate candidate mentions (filtering out invalid\nones) and coreference results (splitting erroneous clusters) predicted by\nImCoref. Extensive experiments demonstrate the effectiveness of ImCoref-CeS,\nwhich achieves superior performance compared to existing state-of-the-art\n(SOTA) methods.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5171\u6307\u6d88\u89e3\u6846\u67b6ImCoref-CeS\uff0c\u5b83\u7ed3\u5408\u4e86\u589e\u5f3a\u7684\u76d1\u7763\u6a21\u578b\u548c\u57fa\u4e8eLLM\u7684\u63a8\u7406\u3002", "motivation": "\u5f53\u524d\u7684\u7814\u7a76\u9762\u4e34\u4e00\u4e2a\u5173\u952e\u56f0\u5883\uff1a\u662f\u8fdb\u4e00\u6b65\u63a2\u7d22\u57fa\u4e8e\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u76d1\u7763\u795e\u7ecf\u65b9\u6cd5\u7684\u6f5c\u529b\uff0c\u8fd8\u662f\u91c7\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5f3a\u5927\u529f\u80fd\u3002\u7136\u800c\uff0c\u5982\u4f55\u6709\u6548\u5730\u7ed3\u5408\u5b83\u4eec\u7684\u4f18\u52bf\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u63d0\u51fa\u4e86\u4e00\u4e2a\u6539\u8fdb\u7684CR\u65b9\u6cd5\uff08ImCoref\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u8f7b\u91cf\u7ea7\u6865\u63a5\u6a21\u5757\u6765\u589e\u5f3a\u957f\u6587\u672c\u7f16\u7801\u80fd\u529b\uff0c\u8bbe\u8ba1\u4e00\u4e2a\u53cc\u4eff\u5c04\u8bc4\u5206\u5668\u6765\u5168\u9762\u6355\u83b7\u4f4d\u7f6e\u4fe1\u606f\uff0c\u5e76\u8c03\u7528\u6df7\u5408\u63d0\u53ca\u6b63\u5219\u5316\u6765\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002\u91cd\u8981\u7684\u662f\uff0c\u8be5\u65b9\u6cd5\u91c7\u7528LLM\u4f5c\u4e3a\u591a\u89d2\u8272Checker-Splitter\u4ee3\u7406\uff0c\u4ee5\u9a8c\u8bc1ImCoref\u9884\u6d4b\u7684\u5019\u9009\u63d0\u53ca\uff08\u8fc7\u6ee4\u6389\u65e0\u6548\u7684\u63d0\u53ca\uff09\u548c\u5171\u6307\u7ed3\u679c\uff08\u62c6\u5206\u9519\u8bef\u7684\u96c6\u7fa4\uff09\u3002", "result": "\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0cImCoref-CeS\u662f\u6709\u6548\u7684\uff0c\u4e0e\u73b0\u6709\u7684\u6700\u5148\u8fdb\uff08SOTA\uff09\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b83\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684ImCoref-CeS\u6846\u67b6\u6709\u6548\u5730\u7ed3\u5408\u4e86\u76d1\u7763\u6a21\u578b\u548cLLM\u7684\u4f18\u52bf\uff0c\u5e76\u5728\u5171\u6307\u6d88\u89e3\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002"}}
{"id": "2510.09996", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09996", "abs": "https://arxiv.org/abs/2510.09996", "authors": ["Lishen Qu", "Zhihao Liu", "Shihao Zhou", "Yaqi Luo", "Jie Liang", "Hui Zeng", "Lei Zhang", "Jufeng Yang"], "title": "BurstDeflicker: A Benchmark Dataset for Flicker Removal in Dynamic Scenes", "comment": "Accepted by NeurIPS 2025", "summary": "Flicker artifacts in short-exposure images are caused by the interplay\nbetween the row-wise exposure mechanism of rolling shutter cameras and the\ntemporal intensity variations of alternating current (AC)-powered lighting.\nThese artifacts typically appear as uneven brightness distribution across the\nimage, forming noticeable dark bands. Beyond compromising image quality, this\nstructured noise also affects high-level tasks, such as object detection and\ntracking, where reliable lighting is crucial. Despite the prevalence of\nflicker, the lack of a large-scale, realistic dataset has been a significant\nbarrier to advancing research in flicker removal. To address this issue, we\npresent BurstDeflicker, a scalable benchmark constructed using three\ncomplementary data acquisition strategies. First, we develop a Retinex-based\nsynthesis pipeline that redefines the goal of flicker removal and enables\ncontrollable manipulation of key flicker-related attributes (e.g., intensity,\narea, and frequency), thereby facilitating the generation of diverse flicker\npatterns. Second, we capture 4,000 real-world flicker images from different\nscenes, which help the model better understand the spatial and temporal\ncharacteristics of real flicker artifacts and generalize more effectively to\nwild scenarios. Finally, due to the non-repeatable nature of dynamic scenes, we\npropose a green-screen method to incorporate motion into image pairs while\npreserving real flicker degradation. Comprehensive experiments demonstrate the\neffectiveness of our dataset and its potential to advance research in flicker\nremoval.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aBurstDeflicker\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u77ed\u66dd\u5149\u56fe\u50cf\u4e2d\u7531\u4ea4\u6d41\u4f9b\u7535\u7167\u660e\u5f15\u8d77\u7684\u95ea\u70c1\u4f2a\u5f71\u95ee\u9898\u3002", "motivation": "\u95ea\u70c1\u4f2a\u5f71\u4f1a\u964d\u4f4e\u56fe\u50cf\u8d28\u91cf\uff0c\u5e76\u5f71\u54cd\u76ee\u6807\u68c0\u6d4b\u548c\u8ddf\u8e2a\u7b49\u9ad8\u7ea7\u4efb\u52a1\u3002\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u771f\u5b9e\u7684\u6570\u636e\u96c6\u662f\u95ea\u70c1\u53bb\u9664\u7814\u7a76\u7684\u4e3b\u8981\u969c\u788d\u3002", "method": "1. \u57fa\u4e8eRetinex\u7684\u5408\u6210\u6d41\u6c34\u7ebf\uff0c\u53ef\u63a7\u5730\u64cd\u7eb5\u5173\u952e\u7684\u95ea\u70c1\u76f8\u5173\u5c5e\u6027\uff0c\u4ece\u800c\u751f\u6210\u4e0d\u540c\u7684\u95ea\u70c1\u6a21\u5f0f\u30022. \u4ece\u4e0d\u540c\u573a\u666f\u4e2d\u6355\u83b74,000\u5f20\u771f\u5b9e\u4e16\u754c\u7684\u95ea\u70c1\u56fe\u50cf\u30023. \u63d0\u51fa\u4e86\u4e00\u79cd\u7eff\u5c4f\u65b9\u6cd5\uff0c\u5c06\u8fd0\u52a8\u878d\u5165\u5230\u56fe\u50cf\u5bf9\u4e2d\uff0c\u540c\u65f6\u4fdd\u7559\u771f\u5b9e\u7684\u95ea\u70c1\u9000\u5316\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u53ca\u5176\u5728\u63a8\u8fdb\u95ea\u70c1\u53bb\u9664\u7814\u7a76\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6570\u636e\u96c6BurstDeflicker\uff0c\u5229\u7528\u4e09\u79cd\u4e92\u8865\u7684\u6570\u636e\u91c7\u96c6\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u95ea\u70c1\u53bb\u9664\u7814\u7a76\u4e2d\u7f3a\u4e4f\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6\u7684\u95ee\u9898\u3002"}}
{"id": "2510.10549", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10549", "abs": "https://arxiv.org/abs/2510.10549", "authors": ["Xinbang Dai", "Huikang Hu", "Yongrui Chen", "Jiaqi Li", "Rihui Jin", "Yuyang Zhang", "Xiaoguang Li", "Lifeng Shang", "Guilin Qi"], "title": "ELAIPBench: A Benchmark for Expert-Level Artificial Intelligence Paper Understanding", "comment": "25 pages, 20 figures", "summary": "While large language models (LLMs) excel at many domain-specific tasks, their\nability to deeply comprehend and reason about full-length academic papers\nremains underexplored. Existing benchmarks often fall short of capturing such\ndepth, either due to surface-level question design or unreliable evaluation\nmetrics. To address this gap, we introduce ELAIPBench, a benchmark curated by\ndomain experts to evaluate LLMs' comprehension of artificial intelligence (AI)\nresearch papers. Developed through an incentive-driven, adversarial annotation\nprocess, ELAIPBench features 403 multiple-choice questions from 137 papers. It\nspans three difficulty levels and emphasizes non-trivial reasoning rather than\nshallow retrieval. Our experiments show that the best-performing LLM achieves\nan accuracy of only 39.95%, far below human performance. Moreover, we observe\nthat frontier LLMs equipped with a thinking mode or a retrieval-augmented\ngeneration (RAG) system fail to improve final results-even harming accuracy due\nto overthinking or noisy retrieval. These findings underscore the significant\ngap between current LLM capabilities and genuine comprehension of academic\npapers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86 ELAIPBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u8bba\u6587\u7684\u7406\u89e3\u80fd\u529b\u7684\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u5b8c\u6574\u5b66\u672f\u8bba\u6587\u7684\u6df1\u5ea6\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u9886\u57df\u4e13\u5bb6\u7b56\u5212\u4e86\u4e00\u4e2a\u5305\u542b 403 \u4e2a\u9009\u62e9\u9898\u7684\u57fa\u51c6\uff0c\u8fd9\u4e9b\u95ee\u9898\u6765\u81ea 137 \u7bc7\u8bba\u6587\uff0c\u6db5\u76d6\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\uff0c\u5e76\u5f3a\u8c03\u975e\u6d45\u5c42\u68c0\u7d22\u7684\u63a8\u7406\u3002", "result": "\u6027\u80fd\u6700\u4f73\u7684 LLM \u7684\u51c6\u786e\u7387\u4ec5\u4e3a 39.95%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u6c34\u5e73\u3002\u914d\u5907\u601d\u7ef4\u6a21\u5f0f\u6216\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG) \u7cfb\u7edf\u7684 LLM \u672a\u80fd\u63d0\u9ad8\u6700\u7ec8\u7ed3\u679c\uff0c\u751a\u81f3\u56e0\u8fc7\u5ea6\u601d\u8003\u6216\u68c0\u7d22\u566a\u58f0\u800c\u635f\u5bb3\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "\u5f53\u524d LLM \u7684\u80fd\u529b\u4e0e\u5bf9\u5b66\u672f\u8bba\u6587\u7684\u771f\u6b63\u7406\u89e3\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002"}}
{"id": "2510.09734", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09734", "abs": "https://arxiv.org/abs/2510.09734", "authors": ["Jindong Tian", "Yifei Ding", "Ronghui Xu", "Hao Miao", "Chenjuan Guo", "Bin Yang"], "title": "ARROW: An Adaptive Rollout and Routing Method for Global Weather Forecasting", "comment": "16 pages, 6 figures, conference", "summary": "Weather forecasting is a fundamental task in spatiotemporal data analysis,\nwith broad applications across a wide range of domains. Existing data-driven\nforecasting methods typically model atmospheric dynamics over a fixed short\ntime interval (e.g., 6 hours) and rely on naive autoregression-based rollout\nfor long-term forecasting (e.g., 138 hours). However, this paradigm suffers\nfrom two key limitations: (1) it often inadequately models the spatial and\nmulti-scale temporal dependencies inherent in global weather systems, and (2)\nthe rollout strategy struggles to balance error accumulation with the capture\nof fine-grained atmospheric variations. In this study, we propose ARROW, an\nAdaptive-Rollout Multi-scale temporal Routing method for Global Weather\nForecasting. To contend with the first limitation, we construct a\nmulti-interval forecasting model that forecasts weather across different time\nintervals. Within the model, the Shared-Private Mixture-of-Experts captures\nboth shared patterns and specific characteristics of atmospheric dynamics\nacross different time scales, while Ring Positional Encoding accurately encodes\nthe circular latitude structure of the Earth when representing spatial\ninformation. For the second limitation, we develop an adaptive rollout\nscheduler based on reinforcement learning, which selects the most suitable time\ninterval to forecast according to the current weather state. Experimental\nresults demonstrate that ARROW achieves state-of-the-art performance in global\nweather forecasting, establishing a promising paradigm in this field.", "AI": {"tldr": "ARROW: \u4e00\u79cd\u7528\u4e8e\u5168\u7403\u5929\u6c14\u9884\u62a5\u7684\u81ea\u9002\u5e94 rollout \u591a\u5c3a\u5ea6\u65f6\u95f4\u8def\u7531\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u533a\u95f4\u9884\u6d4b\u6a21\u578b\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94 rollout \u8c03\u5ea6\u5668\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9a71\u52a8\u7684\u9884\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u6a21\u62df\u5168\u7403\u5929\u6c14\u7cfb\u7edf\u4e2d\u7684\u7a7a\u95f4\u548c\u591a\u5c3a\u5ea6\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u5e76\u4e14 rollout \u7b56\u7565\u96be\u4ee5\u5e73\u8861\u8bef\u5dee\u7d2f\u79ef\u4e0e\u7cbe\u7ec6\u5927\u6c14\u53d8\u5316\u7684\u6355\u83b7\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u533a\u95f4\u9884\u6d4b\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u5171\u4eab-\u79c1\u6709\u6df7\u5408\u4e13\u5bb6\u6355\u83b7\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u7684\u5171\u4eab\u6a21\u5f0f\u548c\u7279\u5b9a\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u73af\u5f62\u4f4d\u7f6e\u7f16\u7801\u6765\u51c6\u786e\u7f16\u7801\u5730\u7403\u7684\u5706\u5f62\u7eac\u5ea6\u7ed3\u6784\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94 rollout \u8c03\u5ea6\u5668\uff0c\u7528\u4e8e\u6839\u636e\u5f53\u524d\u5929\u6c14\u72b6\u6001\u9009\u62e9\u6700\u5408\u9002\u7684\u65f6\u95f4\u95f4\u9694\u8fdb\u884c\u9884\u6d4b\u3002", "result": "ARROW \u5728\u5168\u7403\u5929\u6c14\u9884\u62a5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "ARROW \u5efa\u7acb\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u5168\u7403\u5929\u6c14\u9884\u62a5\u8303\u4f8b\u3002"}}
{"id": "2510.10432", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.10432", "abs": "https://arxiv.org/abs/2510.10432", "authors": ["Zhichen Zeng", "Mengyue Hang", "Xiaolong Liu", "Xiaoyi Liu", "Xiao Lin", "Ruizhong Qiu", "Tianxin Wei", "Zhining Liu", "Siyang Yuan", "Chaofei Yang", "Yiqun Liu", "Hang Yin", "Jiyan Yang", "Hanghang Tong"], "title": "Hierarchical LoRA MoE for Efficient CTR Model Scaling", "comment": "13 pages, 9 figures", "summary": "Deep models have driven significant advances in click-through rate (CTR)\nprediction. While vertical scaling via layer stacking improves model\nexpressiveness, the layer-by-layer sequential computation poses challenges to\nefficient scaling. Conversely, horizontal scaling through Mixture of Experts\n(MoE) achieves efficient scaling by activating a small subset of experts in\nparallel, but flat MoE layers may struggle to capture the hierarchical\nstructure inherent in recommendation tasks. To push the Return-On-Investment\n(ROI) boundary, we explore the complementary strengths of both directions and\npropose HiLoMoE, a hierarchical LoRA MoE framework that enables holistic\nscaling in a parameter-efficient manner. Specifically, HiLoMoE employs\nlightweight rank-1 experts for parameter-efficient horizontal scaling, and\nstacks multiple MoE layers with hierarchical routing to enable combinatorially\ndiverse expert compositions. Unlike conventional stacking, HiLoMoE routes based\non prior layer scores rather than outputs, allowing all layers to execute in\nparallel. A principled three-stage training framework ensures stable\noptimization and expert diversity. Experiments on four public datasets show\nthat HiLoMoE achieving better performance-efficiency tradeoff, achieving an\naverage AUC improvement of 0.20\\% in AUC and 18.5\\% reduction in FLOPs compared\nto the non-MoE baseline.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHiLoMoE\u7684\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u5782\u76f4\u548c\u6c34\u5e73\u6269\u5c55\u7684\u4f18\u52bf\uff0c\u4ee5\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u5f0f\u8fdb\u884c\u6574\u4f53\u6269\u5c55\u3002", "motivation": "\u4e3a\u4e86\u7a81\u7834\u6295\u8d44\u56de\u62a5\u7387\uff08ROI\uff09\u7684\u754c\u9650\u3002", "method": "HiLoMoE\u91c7\u7528\u8f7b\u91cf\u7ea7\u7684rank-1\u4e13\u5bb6\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u7684\u6c34\u5e73\u6269\u5c55\uff0c\u5e76\u5806\u53e0\u5177\u6709\u5206\u5c42\u8def\u7531\u7684\u591a\u4e2aMoE\u5c42\uff0c\u4ee5\u5b9e\u73b0\u7ec4\u5408\u591a\u6837\u6027\u7684\u4e13\u5bb6\u7ec4\u5408\u3002\u4e0e\u4f20\u7edf\u7684\u5806\u53e0\u4e0d\u540c\uff0cHiLoMoE\u57fa\u4e8e\u5148\u524d\u7684\u5c42\u5f97\u5206\u800c\u4e0d\u662f\u8f93\u51fa\u8fdb\u884c\u8def\u7531\uff0c\u5141\u8bb8\u6240\u6709\u5c42\u5e76\u884c\u6267\u884c\u3002\u4e00\u4e2a\u6709\u539f\u5219\u7684\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u786e\u4fdd\u4e86\u7a33\u5b9a\u7684\u4f18\u5316\u548c\u4e13\u5bb6\u591a\u6837\u6027\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u975eMoE\u57fa\u7ebf\u76f8\u6bd4\uff0cHiLoMoE\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd-\u6548\u7387\u6743\u8861\uff0c\u5e73\u5747AUC\u63d0\u9ad8\u4e860.20\uff05\uff0cFLOPs\u51cf\u5c11\u4e8618.5\uff05\u3002", "conclusion": "HiLoMoE\u6846\u67b6\u80fd\u591f\u4ee5\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u5f0f\u8fdb\u884c\u6574\u4f53\u6269\u5c55\uff0c\u5e76\u5728CTR\u9884\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd-\u6548\u7387\u6743\u8861\u3002"}}
{"id": "2510.10013", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10013", "abs": "https://arxiv.org/abs/2510.10013", "authors": ["Yuyi Huang", "Runzhe Zhan", "Lidia S. Chao", "Ailin Tao", "Derek F. Wong"], "title": "Path Drift in Large Reasoning Models:How First-Person Commitments Override Safety", "comment": null, "summary": "As large language models (LLMs) are increasingly deployed for complex\nreasoning tasks, Long Chain-of-Thought (Long-CoT) prompting has emerged as a\nkey paradigm for structured inference. Despite early-stage safeguards enabled\nby alignment techniques such as RLHF, we identify a previously underexplored\nvulnerability: reasoning trajectories in Long-CoT models can drift from aligned\npaths, resulting in content that violates safety constraints. We term this\nphenomenon Path Drift. Through empirical analysis, we uncover three behavioral\ntriggers of Path Drift: (1) first-person commitments that induce goal-driven\nreasoning that delays refusal signals; (2) ethical evaporation, where\nsurface-level disclaimers bypass alignment checkpoints; (3) condition chain\nescalation, where layered cues progressively steer models toward unsafe\ncompletions. Building on these insights, we introduce a three-stage Path Drift\nInduction Framework comprising cognitive load amplification, self-role priming,\nand condition chain hijacking. Each stage independently reduces refusal rates,\nwhile their combination further compounds the effect. To mitigate these risks,\nwe propose a path-level defense strategy incorporating role attribution\ncorrection and metacognitive reflection (reflective safety cues). Our findings\nhighlight the need for trajectory-level alignment oversight in long-form\nreasoning beyond token-level alignment.", "AI": {"tldr": "Long Chain-of-Thought (Long-CoT) \u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u63a8\u7406\u8fc7\u7a0b\u53ef\u80fd\u504f\u79bb\u5bf9\u9f50\u8def\u5f84\uff0c\u5bfc\u81f4\u8fdd\u53cd\u5b89\u5168\u9650\u5236\u7684\u5185\u5bb9\uff0c\u8fd9\u79cd\u73b0\u8c61\u88ab\u79f0\u4e3a\u8def\u5f84\u6f02\u79fb\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLMs) \u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0c\u800c Long-CoT \u63d0\u793a\u5df2\u6210\u4e3a\u7ed3\u6784\u5316\u63a8\u7406\u7684\u5173\u952e\u8303\u4f8b\u3002\u5c3d\u7ba1\u65e9\u671f\u901a\u8fc7 RLHF \u7b49\u5bf9\u9f50\u6280\u672f\u5b9e\u73b0\u4e86\u521d\u6b65\u7684\u5b89\u5168\u63aa\u65bd\uff0c\u4f46 Long-CoT \u6a21\u578b\u4e2d\u7684\u63a8\u7406\u8f68\u8ff9\u53ef\u80fd\u4f1a\u504f\u79bb\u5bf9\u9f50\u8def\u5f84\uff0c\u4ece\u800c\u5bfc\u81f4\u8fdd\u53cd\u5b89\u5168\u7ea6\u675f\u7684\u5185\u5bb9\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\uff0c\u63ed\u793a\u4e86\u8def\u5f84\u6f02\u79fb\u7684\u4e09\u4e2a\u884c\u4e3a\u89e6\u53d1\u56e0\u7d20\uff1a(1) \u8bf1\u5bfc\u76ee\u6807\u9a71\u52a8\u63a8\u7406\u7684\u7b2c\u4e00\u4eba\u79f0\u627f\u8bfa\uff0c\u5ef6\u8fdf\u4e86\u62d2\u7edd\u4fe1\u53f7\uff1b(2) \u8868\u9762\u514d\u8d23\u58f0\u660e\u7ed5\u8fc7\u4e86\u5bf9\u9f50\u68c0\u67e5\u70b9\u7684\u9053\u5fb7\u84b8\u53d1\uff1b(3) \u5206\u5c42\u7ebf\u7d22\u9010\u6e10\u5f15\u5bfc\u6a21\u578b\u8d70\u5411\u4e0d\u5b89\u5168\u5b8c\u6210\u7684\u72b6\u6001\u94fe\u5347\u7ea7\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u8def\u5f84\u6f02\u79fb\u8bf1\u5bfc\u6846\u67b6\uff0c\u5305\u62ec\u8ba4\u77e5\u8d1f\u8377\u653e\u5927\u3001\u81ea\u6211\u89d2\u8272\u542f\u52a8\u548c\u6761\u4ef6\u94fe\u52ab\u6301\u3002", "result": "\u6bcf\u4e2a\u9636\u6bb5\u90fd\u72ec\u7acb\u5730\u964d\u4f4e\u4e86\u62d2\u7edd\u7387\uff0c\u800c\u5b83\u4eec\u7684\u7ec4\u5408\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u8fd9\u79cd\u5f71\u54cd\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\uff0c\u9664\u4e86\u4ee4\u724c\u7ea7\u522b\u7684\u5bf9\u9f50\u4e4b\u5916\uff0c\u8fd8\u9700\u8981\u5728\u957f\u683c\u5f0f\u63a8\u7406\u4e2d\u8fdb\u884c\u8f68\u8ff9\u7ea7\u522b\u7684\u5bf9\u9f50\u76d1\u7763\u3002"}}
{"id": "2510.10011", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10011", "abs": "https://arxiv.org/abs/2510.10011", "authors": ["Yanyuan Chen", "Dexuan Xu", "Yu Huang", "Songkun Zhan", "Hanpin Wang", "Dongxue Chen", "Xueping Wang", "Meikang Qiu", "Hang Li"], "title": "MIMO: A medical vision language model with visual referring multimodal input and pixel grounding multimodal output", "comment": "CVPR 2025", "summary": "Currently, medical vision language models are widely used in medical vision\nquestion answering tasks. However, existing models are confronted with two\nissues: for input, the model only relies on text instructions and lacks direct\nunderstanding of visual clues in the image; for output, the model only gives\ntext answers and lacks connection with key areas in the image. To address these\nissues, we propose a unified medical vision language model MIMO, with visual\nreferring Multimodal Input and pixel grounding Multimodal Output. MIMO can not\nonly combine visual clues and textual instructions to understand complex\nmedical images and semantics, but can also ground medical terminologies in\ntextual output within the image. To overcome the scarcity of relevant data in\nthe medical field, we propose MIMOSeg, a comprehensive medical multimodal\ndataset including 895K samples. MIMOSeg is constructed from four different\nperspectives, covering basic instruction following and complex question\nanswering with multimodal input and multimodal output. We conduct experiments\non several downstream medical multimodal tasks. Extensive experimental results\nverify that MIMO can uniquely combine visual referring and pixel grounding\ncapabilities, which are not available in previous models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578bMIMO\uff0c\u5177\u6709\u89c6\u89c9\u53c2\u8003\u591a\u6a21\u6001\u8f93\u5165\u548c\u50cf\u7d20\u63a5\u5730\u591a\u6a21\u6001\u8f93\u51fa\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a1) \u8f93\u5165\u7aef\uff0c\u6a21\u578b\u53ea\u4f9d\u8d56\u4e8e\u6587\u672c\u6307\u4ee4\uff0c\u7f3a\u4e4f\u5bf9\u56fe\u50cf\u4e2d\u89c6\u89c9\u7ebf\u7d22\u7684\u76f4\u63a5\u7406\u89e3\uff1b2) \u8f93\u51fa\u7aef\uff0c\u6a21\u578b\u53ea\u7ed9\u51fa\u6587\u672c\u7b54\u6848\uff0c\u7f3a\u4e4f\u4e0e\u56fe\u50cf\u4e2d\u5173\u952e\u533a\u57df\u7684\u8054\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578bMIMO\uff0c\u5177\u6709\u89c6\u89c9\u53c2\u8003\u591a\u6a21\u6001\u8f93\u5165\u548c\u50cf\u7d20\u63a5\u5730\u591a\u6a21\u6001\u8f93\u51fa\u3002\u4e3a\u4e86\u514b\u670d\u533b\u5b66\u9886\u57df\u76f8\u5173\u6570\u636e\u7684\u7a00\u7f3a\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u6027\u7684\u533b\u5b66\u591a\u6a21\u6001\u6570\u636e\u96c6MIMOSeg\uff0c\u5305\u542b895K\u4e2a\u6837\u672c\u3002", "result": "\u5728\u51e0\u4e2a\u4e0b\u6e38\u533b\u5b66\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86MIMO\u53ef\u4ee5\u72ec\u7279\u5730\u7ed3\u5408\u89c6\u89c9\u53c2\u8003\u548c\u50cf\u7d20\u63a5\u5730\u80fd\u529b\uff0c\u8fd9\u5728\u4ee5\u524d\u7684\u6a21\u578b\u4e2d\u662f\u4e0d\u53ef\u7528\u7684\u3002", "conclusion": "MIMO\u6a21\u578b\u80fd\u591f\u7ed3\u5408\u89c6\u89c9\u7ebf\u7d22\u548c\u6587\u672c\u6307\u4ee4\u6765\u7406\u89e3\u590d\u6742\u7684\u533b\u5b66\u56fe\u50cf\u548c\u8bed\u4e49\uff0c\u5e76\u4e14\u80fd\u591f\u5c06\u6587\u672c\u8f93\u51fa\u4e2d\u7684\u533b\u5b66\u672f\u8bed\u4e0e\u56fe\u50cf\u4e2d\u7684\u50cf\u7d20\u533a\u57df\u5bf9\u5e94\u8d77\u6765\u3002"}}
{"id": "2510.10592", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10592", "abs": "https://arxiv.org/abs/2510.10592", "authors": ["Hong Su"], "title": "A Layered Intuition -- Method Model with Scope Extension for LLM Reasoning", "comment": null, "summary": "Existing studies have introduced method-based reasoning and scope extension\nas approaches to enhance Large Language Model (LLM) performance beyond direct\nmatrix mappings. Building on these foundations, this paper summarizes and\nintegrates these ideas into a unified Intuition-Method Layered Model with Scope\nExtension, designed to address indirected (unseen) issues more systematically.\nIn this framework, intuition-based thinking provides rapid first-reaction\nanswers, while method-based thinking decouples questions and solutions into\ntransferable reasoning units. Scope extension is then applied to broaden\napplicability, including vertical (cause analysis), horizontal (parallel and\ngeneralized issues), and for the first time, temporal and spatial extensions,\nwhich expand reasoning across time and contextual dimensions. These extensions\nare organized into systematic knowledge trees that interconnect into a\nknowledge network, thereby increasing adaptability. To quantitatively evaluate\nthis process, we propose the entropy of method extension, which measures the\nindependence and diversity of extensions as an indicator of the system's\ncapacity to solve unseen questions. By logically connecting existing approaches\nwith new extensions and introducing an entropy-based evaluation framework, this\nwork advances toward a more robust and extensible reasoning paradigm for LLMs\nin real-world problem-solving.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u76f4\u89c9-\u65b9\u6cd5\u5206\u5c42\u6a21\u578b\uff0c\u901a\u8fc7\u8303\u56f4\u6269\u5c55\u6765\u7cfb\u7edf\u5730\u89e3\u51b3\u95f4\u63a5\uff08\u672a\u89c1\uff09\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u71b5\u65b9\u6cd5\u6269\u5c55\u6765\u8bc4\u4f30\u8be5\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5df2\u7ecf\u5f15\u5165\u4e86\u57fa\u4e8e\u65b9\u6cd5\u63a8\u7406\u548c\u8303\u56f4\u6269\u5c55\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u76f4\u63a5\u77e9\u9635\u6620\u5c04\u3002\u672c\u6587\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u65e8\u5728\u603b\u7ed3\u548c\u6574\u5408\u8fd9\u4e9b\u601d\u60f3\u3002", "method": "\u8be5\u6a21\u578b\u5305\u62ec\u57fa\u4e8e\u76f4\u89c9\u7684\u601d\u8003\u3001\u57fa\u4e8e\u65b9\u6cd5\u7684\u601d\u8003\u548c\u8303\u56f4\u6269\u5c55\uff0c\u8303\u56f4\u6269\u5c55\u5305\u62ec\u5782\u76f4\uff08\u539f\u56e0\u5206\u6790\uff09\u3001\u6c34\u5e73\uff08\u5e73\u884c\u548c\u6982\u62ec\u95ee\u9898\uff09\u4ee5\u53ca\u65f6\u95f4\u548c\u7a7a\u95f4\u6269\u5c55\u3002\u8fd9\u4e9b\u6269\u5c55\u88ab\u7ec4\u7ec7\u6210\u77e5\u8bc6\u6811\uff0c\u76f8\u4e92\u8fde\u63a5\u6210\u77e5\u8bc6\u7f51\u7edc\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u65b9\u6cd5\u6269\u5c55\u7684\u71b5\uff0c\u7528\u4e8e\u8861\u91cf\u6269\u5c55\u7684\u72ec\u7acb\u6027\u548c\u591a\u6837\u6027\uff0c\u4ee5\u6b64\u4f5c\u4e3a\u7cfb\u7edf\u89e3\u51b3\u672a\u89c1\u95ee\u9898\u7684\u80fd\u529b\u6307\u6807\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u5c06\u73b0\u6709\u65b9\u6cd5\u4e0e\u65b0\u7684\u6269\u5c55\u8fde\u63a5\u8d77\u6765\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u71b5\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4ece\u800c\u671d\u7740\u4e3aLLM\u5728\u5b9e\u9645\u95ee\u9898\u89e3\u51b3\u4e2d\u63d0\u4f9b\u66f4\u5f3a\u5927\u548c\u53ef\u6269\u5c55\u7684\u63a8\u7406\u8303\u4f8b\u8fc8\u8fdb\u4e86\u4e00\u6b65\u3002"}}
{"id": "2510.09735", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09735", "abs": "https://arxiv.org/abs/2510.09735", "authors": ["Qianyou Sun", "Jiexin Zheng", "Bohan Jin", "Lihua Chen", "Yijie Peng"], "title": "InterCorpRel-LLM: Enhancing Financial Relational Understanding with Graph-Language Models", "comment": null, "summary": "Identifying inter-firm relationships such as supply and competitive ties is\ncritical for financial analysis and corporate governance, yet remains\nchallenging due to the scale, sparsity, and contextual dependence of corporate\ndata. Graph-based methods capture structure but miss semantic depth, while\nlarge language models (LLMs) excel at text but remain limited in their ability\nto represent relational dependencies. To address this, we propose\nInterCorpRel-LLM, a cross-modal framework that integrates GNNs with LLMs,\nsupported by a proprietary dataset derived from FactSet supply chain records\nand three tailored training tasks: company graph matching, industry\nclassification, and supply relation prediction. This design enables effective\njoint modeling of structure and semantics. Experiments show that\nInterCorpRel-LLM substantially outperforms strong baselines, including GPT-5,\non a supply relation identification task, achieving an F-score of 0.8543 vs.\n0.2287 with only a 7B-parameter backbone and lightweight training. The model\nalso generalizes to zero-shot competitor identification, underscoring its\nability to capture nuanced inter-firm dynamics. Our framework thus provides\nanalysts and strategists with a robust tool for mapping and reasoning about\ncomplex corporate networks, enhancing decision-making and risk management in\ndynamic markets.", "AI": {"tldr": "\u63d0\u51faInterCorpRel-LLM\uff0c\u4e00\u4e2a\u7ed3\u5408GNN\u548cLLM\u7684\u8de8\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u516c\u53f8\u95f4\u5173\u7cfb\u3002", "motivation": "\u8bc6\u522b\u516c\u53f8\u95f4\u7684\u4f9b\u5e94\u548c\u7ade\u4e89\u5173\u7cfb\u5bf9\u4e8e\u8d22\u52a1\u5206\u6790\u548c\u516c\u53f8\u6cbb\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u516c\u53f8\u6570\u636e\u7684\u89c4\u6a21\u3001\u7a00\u758f\u6027\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\uff0c\u8fd9\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86GNN\u548cLLM\uff0c\u5e76\u4f7f\u7528\u4eceFactSet\u4f9b\u5e94\u94fe\u8bb0\u5f55\u4e2d\u83b7\u5f97\u7684\u4e13\u6709\u6570\u636e\u96c6\u4ee5\u53ca\u4e09\u4e2a\u5b9a\u5236\u7684\u8bad\u7ec3\u4efb\u52a1\uff1a\u516c\u53f8\u56fe\u5339\u914d\u3001\u884c\u4e1a\u5206\u7c7b\u548c\u4f9b\u5e94\u5173\u7cfb\u9884\u6d4b\u3002", "result": "InterCorpRel-LLM\u5728\u4f9b\u5e94\u5173\u7cfb\u8bc6\u522b\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u5305\u62ecGPT-5\u5728\u5185\u7684\u5f3a\u5927\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e860.8543\u7684F-score\uff0c\u800cGPT-5\u4ec5\u4e3a0.2287\u3002\u8be5\u6a21\u578b\u8fd8\u53ef\u4ee5\u63a8\u5e7f\u5230\u96f6\u6837\u672c\u7ade\u4e89\u8005\u8bc6\u522b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5206\u6790\u5e08\u548c\u6218\u7565\u5bb6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u7ed8\u5236\u548c\u63a8\u7406\u590d\u6742\u7684\u516c\u53f8\u7f51\u7edc\uff0c\u4ece\u800c\u52a0\u5f3a\u52a8\u6001\u5e02\u573a\u4e2d\u7684\u51b3\u7b56\u548c\u98ce\u9669\u7ba1\u7406\u3002"}}
{"id": "2510.10806", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10806", "abs": "https://arxiv.org/abs/2510.10806", "authors": ["Mihir Gupte", "Paolo Giusto", "Ramesh S"], "title": "Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based Structures", "comment": "Waiting for Conference Response", "summary": "Large Language Models (LLMs) are adept at generating responses based on\ninformation within their context. While this ability is useful for interacting\nwith structured data like code files, another popular method,\nRetrieval-Augmented Generation (RAG), retrieves relevant documents to augment\nthe model's in-context learning. However, it is not well-explored how to best\nrepresent this retrieved knowledge for generating responses on structured data,\nparticularly hierarchical structures like trees. In this work, we propose a\nnovel bottom-up method to linearize knowledge from tree-like structures (like a\nGitHub repository) by generating implicit, aggregated summaries at each\nhierarchical level. This approach enables the knowledge to be stored in a\nknowledge base and used directly with RAG. We then compare our method to using\nRAG on raw, unstructured code, evaluating the accuracy and quality of the\ngenerated responses. Our results show that while response quality is comparable\nacross both methods, our approach generates over 68% fewer documents in the\nretriever, a significant gain in efficiency. This finding suggests that\nleveraging implicit, linearized knowledge may be a highly effective and\nscalable strategy for handling complex, hierarchical data structures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684bottom-up\u65b9\u6cd5\u6765\u7ebf\u6027\u5316\u6811\u72b6\u7ed3\u6784\u7684\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7\u5728\u6bcf\u4e2a\u5c42\u7ea7\u751f\u6210\u9690\u5f0f\u7684\u805a\u5408\u6458\u8981\u6765\u5b9e\u73b0\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u64c5\u957f\u6839\u636e\u4e0a\u4e0b\u6587\u4e2d\u7684\u4fe1\u606f\u751f\u6210\u54cd\u5e94\u3002\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u68c0\u7d22\u76f8\u5173\u6587\u6863\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u3002\u7136\u800c\uff0c\u5982\u4f55\u6700\u597d\u5730\u8868\u793a\u68c0\u7d22\u5230\u7684\u77e5\u8bc6\u4ee5\u751f\u6210\u5173\u4e8e\u7ed3\u6784\u5316\u6570\u636e\u7684\u54cd\u5e94\uff0c\u7279\u522b\u662f\u50cf\u6811\u8fd9\u6837\u7684\u5c42\u7ea7\u7ed3\u6784\uff0c\u8fd8\u6ca1\u6709\u5f97\u5230\u5145\u5206\u7684\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684bottom-up\u65b9\u6cd5\u6765\u7ebf\u6027\u5316\u6765\u81ea\u6811\u72b6\u7ed3\u6784\u7684\u77e5\u8bc6\uff08\u5982GitHub\u5b58\u50a8\u5e93\uff09\uff0c\u901a\u8fc7\u5728\u6bcf\u4e2a\u5c42\u7ea7\u751f\u6210\u9690\u5f0f\u7684\u805a\u5408\u6458\u8981\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u4e24\u79cd\u65b9\u6cd5\u7684\u54cd\u5e94\u8d28\u91cf\u76f8\u5f53\uff0c\u4f46\u8be5\u65b9\u6cd5\u5728\u68c0\u7d22\u5668\u4e2d\u751f\u6210\u7684\u6587\u6863\u6570\u91cf\u51cf\u5c11\u4e8668%\u4ee5\u4e0a\uff0c\u6548\u7387\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u5229\u7528\u9690\u5f0f\u7684\u7ebf\u6027\u5316\u77e5\u8bc6\u53ef\u80fd\u662f\u5904\u7406\u590d\u6742\u5c42\u7ea7\u6570\u636e\u7ed3\u6784\u7684\u4e00\u79cd\u975e\u5e38\u6709\u6548\u548c\u53ef\u6269\u5c55\u7684\u7b56\u7565\u3002"}}
{"id": "2510.10025", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10025", "abs": "https://arxiv.org/abs/2510.10025", "authors": ["Jiaqi Liu", "Lanruo Wang", "Su Liu", "Xin Hu"], "title": "Lightweight Baselines for Medical Abstract Classification: DistilBERT with Cross-Entropy as a Strong Default", "comment": "Healthcare AI, Medical Text Classification, Lightweight LLMs,\n  DistilBERT, Reproducibility", "summary": "Large language models work well for many NLP tasks, but they are hard to\ndeploy in health settings with strict cost, latency, and privacy limits. We\nrevisit a lightweight recipe for medical abstract classification and ask how\nfar compact encoders can go under a controlled budget. Using the public medical\nabstracts corpus, we finetune BERT base and DistilBERT with three objectives\nstandard cross-entropy, class weighted cross entropy, and focal loss keeping\ntokenizer, sequence length, optimizer, and schedule fixed. DistilBERT with\nplain cross-entropy gives the best balance on the test set while using far\nfewer parameters than BERT base. We report accuracy, Macro F1, and Weighted F1,\nrelease the evaluation code, and include confusion analyses to make error\npatterns clear. Our results suggest a practical default: start with a compact\nencoder and cross-entropy, then add calibration and task-specific checks before\nmoving to heavier models.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u533b\u7597\u9886\u57df\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6a21\u578b\u8fdb\u884c\u533b\u5b66\u6458\u8981\u5206\u7c7b\u7684\u6548\u679c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bb8\u591a NLP \u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b83\u4eec\u5f88\u96be\u5728\u5177\u6709\u4e25\u683c\u6210\u672c\u3001\u5ef6\u8fdf\u548c\u9690\u79c1\u9650\u5236\u7684\u533b\u7597\u73af\u5883\u4e2d\u90e8\u7f72\u3002", "method": "\u4f7f\u7528\u516c\u5171\u533b\u5b66\u6458\u8981\u8bed\u6599\u5e93\uff0c\u6211\u4eec\u4f7f\u7528\u4e09\u4e2a\u76ee\u6807\u5fae\u8c03 BERT base \u548c DistilBERT\uff1a\u6807\u51c6\u4ea4\u53c9\u71b5\u3001\u7c7b\u52a0\u6743\u4ea4\u53c9\u71b5\u548c\u7126\u70b9\u635f\u5931\uff0c\u4fdd\u6301\u5206\u8bcd\u5668\u3001\u5e8f\u5217\u957f\u5ea6\u3001\u4f18\u5316\u5668\u548c\u65f6\u95f4\u8868\u4e0d\u53d8\u3002", "result": "DistilBERT \u4e0e\u666e\u901a\u4ea4\u53c9\u71b5\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7ed9\u51fa\u4e86\u6700\u4f73\u5e73\u8861\uff0c\u540c\u65f6\u4f7f\u7528\u7684\u53c2\u6570\u8fdc\u5c11\u4e8e BERT base\u3002", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u9ed8\u8ba4\u65b9\u6cd5\uff1a\u4ece\u7d27\u51d1\u7684\u7f16\u7801\u5668\u548c\u4ea4\u53c9\u71b5\u5f00\u59cb\uff0c\u7136\u540e\u5728\u8f6c\u5411\u66f4\u91cd\u7684\u6a21\u578b\u4e4b\u524d\u6dfb\u52a0\u6821\u51c6\u548c\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u68c0\u67e5\u3002"}}
{"id": "2510.10022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10022", "abs": "https://arxiv.org/abs/2510.10022", "authors": ["Junan Chen", "Trung Thanh Nguyen", "Takahiro Komamizu", "Ichiro Ide"], "title": "Q-Adapter: Visual Query Adapter for Extracting Textually-related Features in Video Captioning", "comment": "ACM Multimedia Asia 2025", "summary": "Recent advances in video captioning are driven by large-scale pretrained\nmodels, which follow the standard \"pre-training followed by fine-tuning\"\nparadigm, where the full model is fine-tuned for downstream tasks. Although\neffective, this approach becomes computationally prohibitive as the model size\nincreases. The Parameter-Efficient Fine-Tuning (PEFT) approach offers a\npromising alternative, but primarily focuses on the language components of\nMultimodal Large Language Models (MLLMs). Despite recent progress, PEFT remains\nunderexplored in multimodal tasks and lacks sufficient understanding of visual\ninformation during fine-tuning the model. To bridge this gap, we propose\nQuery-Adapter (Q-Adapter), a lightweight visual adapter module designed to\nenhance MLLMs by enabling efficient fine-tuning for the video captioning task.\nQ-Adapter introduces learnable query tokens and a gating layer into Vision\nEncoder, enabling effective extraction of sparse, caption-relevant features\nwithout relying on external textual supervision. We evaluate Q-Adapter on two\nwell-known video captioning datasets, MSR-VTT and MSVD, where it achieves\nstate-of-the-art performance among the methods that take the PEFT approach\nacross BLEU@4, METEOR, ROUGE-L, and CIDEr metrics. Q-Adapter also achieves\ncompetitive performance compared to methods that take the full fine-tuning\napproach while requiring only 1.4% of the parameters. We further analyze the\nimpact of key hyperparameters and design choices on fine-tuning effectiveness,\nproviding insights into optimization strategies for adapter-based learning.\nThese results highlight the strong potential of Q-Adapter in balancing caption\nquality and parameter efficiency, demonstrating its scalability for\nvideo-language modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u89c6\u89c9\u9002\u914d\u5668\u6a21\u5757\uff08Q-Adapter\uff09\uff0c\u7528\u4e8e\u589e\u5f3a\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u9891\u5b57\u5e55\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u7684\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u5b57\u5e55\u6a21\u578b\u5fae\u8c03\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u4ecd\u6709\u63a2\u7d22\u7a7a\u95f4\uff0c\u5c24\u5176\u662f\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5bf9\u89c6\u89c9\u4fe1\u606f\u7684\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u5728\u89c6\u89c9\u7f16\u7801\u5668\u4e2d\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u67e5\u8be2\u4ee4\u724c\u548c\u95e8\u63a7\u5c42\uff0c\u4ee5\u6709\u6548\u63d0\u53d6\u7a00\u758f\u7684\u3001\u4e0e\u5b57\u5e55\u76f8\u5173\u7684\u7279\u5f81\uff0c\u65e0\u9700\u5916\u90e8\u6587\u672c\u76d1\u7763\u3002", "result": "\u5728MSR-VTT\u548cMSVD\u89c6\u9891\u5b57\u5e55\u6570\u636e\u96c6\u4e0a\uff0cQ-Adapter\u5728BLEU@4\u3001METEOR\u3001ROUGE-L\u548cCIDEr\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u4e2d\u7684\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u4e14\u4e0e\u5168\u53c2\u6570\u5fae\u8c03\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4ec5\u97001.4%\u7684\u53c2\u6570\u5373\u53ef\u8fbe\u5230\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "Q-Adapter\u5728\u5e73\u8861\u5b57\u5e55\u8d28\u91cf\u548c\u53c2\u6570\u6548\u7387\u65b9\u9762\u5177\u6709\u5f3a\u5927\u7684\u6f5c\u529b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u89c6\u9891\u8bed\u8a00\u5efa\u6a21\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.10596", "categories": ["cs.AI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.10596", "abs": "https://arxiv.org/abs/2510.10596", "authors": ["Ruolan Cheng", "Yong Deng", "Seraf\u00edn Moral", "Jos\u00e9 Ram\u00f3n Trillo"], "title": "A Distance Measure for Random Permutation Set: From the Layer-2 Belief Structure Perspective", "comment": null, "summary": "Random permutation set (RPS) is a recently proposed framework designed to\nrepresent order-structured uncertain information. Measuring the distance\nbetween permutation mass functions is a key research topic in RPS theory\n(RPST). This paper conducts an in-depth analysis of distances between RPSs from\ntwo different perspectives: random finite set (RFS) and transferable belief\nmodel (TBM). Adopting the layer-2 belief structure interpretation of RPS, we\nregard RPST as a refinement of TBM, where the order in the ordered focus set\nrepresents qualitative propensity. Starting from the permutation, we introduce\na new definition of the cumulative Jaccard index to quantify the similarity\nbetween two permutations and further propose a distance measure method for RPSs\nbased on the cumulative Jaccard index matrix. The metric and structural\nproperties of the proposed distance measure are investigated, including the\npositive definiteness analysis of the cumulative Jaccard index matrix, and a\ncorrection scheme is provided. The proposed method has a natural\ntop-weightiness property: inconsistencies between higher-ranked elements tend\nto result in greater distance values. Two parameters are provided to the\ndecision-maker to adjust the weight and truncation depth. Several numerical\nexamples are used to compare the proposed method with the existing method. The\nexperimental results show that the proposed method not only overcomes the\nshortcomings of the existing method and is compatible with the Jousselme\ndistance, but also has higher sensitivity and flexibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7d2f\u79efJaccard\u6307\u6570\u77e9\u9635\u7684\u968f\u673a\u7f6e\u6362\u96c6\uff08RPS\uff09\u8ddd\u79bb\u6d4b\u91cf\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7f3a\u70b9\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u7075\u654f\u5ea6\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u7814\u7a76RPS\u7406\u8bba\u4e2d\u7f6e\u6362\u8d28\u91cf\u51fd\u6570\u4e4b\u95f4\u8ddd\u79bb\u7684\u6d4b\u91cf\u95ee\u9898\u3002", "method": "\u4ece\u968f\u673a\u6709\u9650\u96c6\uff08RFS\uff09\u548c\u53ef\u8f6c\u79fb\u4fe1\u5ff5\u6a21\u578b\uff08TBM\uff09\u4e24\u4e2a\u89d2\u5ea6\u6df1\u5165\u5206\u6790RPS\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u5f15\u5165\u7d2f\u79efJaccard\u6307\u6570\u7684\u65b0\u5b9a\u4e49\u6765\u91cf\u5316\u4e24\u4e2a\u7f6e\u6362\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u7d2f\u79efJaccard\u6307\u6570\u77e9\u9635\u7684RPS\u8ddd\u79bb\u6d4b\u91cf\u65b9\u6cd5\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5177\u6709\u81ea\u7136\u7684\u9876\u6743\u6027\uff0c\u8f83\u9ad8\u6392\u540d\u5143\u7d20\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u5f80\u5f80\u5bfc\u81f4\u66f4\u5927\u7684\u8ddd\u79bb\u503c\u3002 \u63d0\u4f9b\u4e86\u4e24\u4e2a\u53c2\u6570\u4f9b\u51b3\u7b56\u8005\u8c03\u6574\u6743\u91cd\u548c\u622a\u65ad\u6df1\u5ea6\u3002 \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7f3a\u70b9\uff0c\u5e76\u4e14\u4e0eJousselme\u8ddd\u79bb\u517c\u5bb9\uff0c\u540c\u65f6\u5177\u6709\u66f4\u9ad8\u7684\u7075\u654f\u5ea6\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684RPS\u8ddd\u79bb\u6d4b\u91cf\u65b9\u6cd5\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\u548c\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.09739", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.09739", "abs": "https://arxiv.org/abs/2510.09739", "authors": ["Ayoub Bouguettaya", "Elizabeth M. Stuart"], "title": "Machine learning methods fail to provide cohesive atheoretical construction of personality traits from semantic embeddings", "comment": "1 figure, 12 pages", "summary": "The lexical hypothesis posits that personality traits are encoded in language\nand is foundational to models like the Big Five. We created a bottom-up\npersonality model from a classic adjective list using machine learning and\ncompared its descriptive utility against the Big Five by analyzing one million\nReddit comments. The Big Five, particularly Agreeableness, Conscientiousness,\nand Neuroticism, provided a far more powerful and interpretable description of\nthese online communities. In contrast, our machine-learning clusters provided\nno meaningful distinctions, failed to recover the Extraversion trait, and\nlacked the psychometric coherence of the Big Five. These results affirm the\nrobustness of the Big Five and suggest personality's semantic structure is\ncontext-dependent. Our findings show that while machine learning can help check\nthe ecological validity of established psychological theories, it may not be\nable to replace them.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u4eba\u683c\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u4e0e\u5927\u4e94\u4eba\u683c\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u68c0\u9a8c\u8bcd\u6c47\u5047\u8bbe\uff0c\u5373\u4eba\u683c\u7279\u8d28\u7f16\u7801\u5728\u8bed\u8a00\u4e2d\u3002", "method": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ece\u7ecf\u5178\u5f62\u5bb9\u8bcd\u5217\u8868\u4e2d\u6784\u5efa\u4e86\u4e00\u4e2a\u81ea\u4e0b\u800c\u4e0a\u7684\u4eba\u683c\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u4e00\u767e\u4e07\u6761Reddit\u8bc4\u8bba\uff0c\u5c06\u5176\u63cf\u8ff0\u6548\u7528\u4e0e\u5927\u4e94\u4eba\u683c\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5927\u4e94\u4eba\u683c\u6a21\u578b\uff08\u7279\u522b\u662f\u5b9c\u4eba\u6027\u3001\u5c3d\u8d23\u6027\u548c\u795e\u7ecf\u8d28\uff09\u80fd\u591f\u66f4\u5f3a\u5927\u3001\u66f4\u6613\u4e8e\u7406\u89e3\u5730\u63cf\u8ff0\u8fd9\u4e9b\u5728\u7ebf\u793e\u533a\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u673a\u5668\u5b66\u4e60\u805a\u7c7b\u6ca1\u6709\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u533a\u5206\uff0c\u672a\u80fd\u6062\u590d\u5916\u5411\u6027\u7279\u8d28\uff0c\u5e76\u4e14\u7f3a\u4e4f\u5927\u4e94\u4eba\u683c\u7684\u5fc3\u7406\u6d4b\u91cf\u4e00\u81f4\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u5b9e\u4e86\u5927\u4e94\u4eba\u683c\u6a21\u578b\u7684\u7a33\u5065\u6027\uff0c\u5e76\u8868\u660e\u4eba\u683c\u7684\u8bed\u4e49\u7ed3\u6784\u662f\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u3002\u673a\u5668\u5b66\u4e60\u53ef\u4ee5\u5e2e\u52a9\u68c0\u67e5\u5df2\u5efa\u7acb\u7684\u5fc3\u7406\u5b66\u7406\u8bba\u7684\u751f\u6001\u6709\u6548\u6027\uff0c\u4f46\u53ef\u80fd\u65e0\u6cd5\u53d6\u4ee3\u5b83\u4eec\u3002"}}
{"id": "2510.10815", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.SC"], "pdf": "https://arxiv.org/pdf/2510.10815", "abs": "https://arxiv.org/abs/2510.10815", "authors": ["Meiru Zhang", "Philipp Borchert", "Milan Gritta", "Gerasimos Lampouras"], "title": "DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems", "comment": null, "summary": "Automating the formalization of mathematical statements for theorem proving\nremains a major challenge for Large Language Models (LLMs). LLMs struggle to\nidentify and utilize the prerequisite mathematical knowledge and its\ncorresponding formal representation in languages like Lean. Current\nretrieval-augmented autoformalization methods query external libraries using\nthe informal statement directly, but overlook a fundamental limitation:\ninformal mathematical statements are often complex and offer limited context on\nthe underlying math concepts. To address this, we introduce DRIFT, a novel\nframework that enables LLMs to decompose informal mathematical statements into\nsmaller, more tractable ''sub-components''. This facilitates targeted retrieval\nof premises from mathematical libraries such as Mathlib. Additionally, DRIFT\nretrieves illustrative theorems to help models use premises more effectively in\nformalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet,\nConNF, and MiniF2F-test) and find that it consistently improves premise\nretrieval, nearly doubling the F1 score compared to the DPR baseline on\nProofNet. Notably, DRIFT demonstrates strong performance on the\nout-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and\n42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that\nretrieval effectiveness in mathematical autoformalization depends heavily on\nmodel-specific knowledge boundaries, highlighting the need for adaptive\nretrieval strategies aligned with each model's capabilities.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDRIFT\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u5728\u5b9a\u7406\u8bc1\u660e\u4e2d\u81ea\u52a8\u5f62\u5f0f\u5316\u6570\u5b66\u8bed\u53e5\u7684\u80fd\u529b\u3002\u8be5\u6846\u67b6\u5c06\u975e\u6b63\u5f0f\u7684\u6570\u5b66\u8bed\u53e5\u5206\u89e3\u4e3a\u66f4\u5c0f\u7684\u5b50\u7ec4\u4ef6\uff0c\u4ee5\u4fbf\u4ece\u6570\u5b66\u5e93\u4e2d\u68c0\u7d22\u76f8\u5173\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u5e76\u68c0\u7d22\u8bf4\u660e\u6027\u7684\u5b9a\u7406\u6765\u5e2e\u52a9\u6a21\u578b\u66f4\u6709\u6548\u5730\u4f7f\u7528\u8fd9\u4e9b\u5148\u9a8c\u77e5\u8bc6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u548c\u5229\u7528\u6570\u5b66\u77e5\u8bc6\u53ca\u5176\u5728Lean\u7b49\u8bed\u8a00\u4e2d\u7684\u5f62\u5f0f\u5316\u8868\u793a\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u73b0\u6709\u7684\u68c0\u7d22\u589e\u5f3a\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5\u76f4\u63a5\u4f7f\u7528\u975e\u6b63\u5f0f\u8bed\u53e5\u67e5\u8be2\u5916\u90e8\u5e93\uff0c\u5ffd\u7565\u4e86\u975e\u6b63\u5f0f\u6570\u5b66\u8bed\u53e5\u901a\u5e38\u5f88\u590d\u6742\u4e14\u63d0\u4f9b\u7684\u4e0a\u4e0b\u6587\u6709\u9650\u8fd9\u4e00\u57fa\u672c\u9650\u5236\u3002", "method": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6DRIFT\uff0c\u5b83\u53ef\u4ee5\u5c06\u975e\u6b63\u5f0f\u7684\u6570\u5b66\u8bed\u53e5\u5206\u89e3\u4e3a\u66f4\u5c0f\u7684\u3001\u66f4\u5bb9\u6613\u5904\u7406\u7684\u201c\u5b50\u7ec4\u4ef6\u201d\u3002\u8fd9\u6709\u52a9\u4e8e\u4eceMathlib\u7b49\u6570\u5b66\u5e93\u4e2d\u8fdb\u884c\u6709\u9488\u5bf9\u6027\u7684\u524d\u63d0\u68c0\u7d22\u3002\u6b64\u5916\uff0cDRIFT\u8fd8\u4f1a\u68c0\u7d22\u8bf4\u660e\u6027\u5b9a\u7406\uff0c\u4ee5\u5e2e\u52a9\u6a21\u578b\u66f4\u6709\u6548\u5730\u5728\u5f62\u5f0f\u5316\u4efb\u52a1\u4e2d\u4f7f\u7528\u524d\u63d0\u3002", "result": "DRIFT\u5728\u4e0d\u540c\u7684\u57fa\u51c6\u6d4b\u8bd5(ProofNet\u3001ConNF\u548cMiniF2F-test)\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u4e0eProofNet\u4e0a\u7684DPR\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5b83\u59cb\u7ec8\u80fd\u63d0\u9ad8\u524d\u63d0\u68c0\u7d22\u80fd\u529b\uff0c\u5e76\u4f7fF1\u5f97\u5206\u51e0\u4e4e\u7ffb\u4e86\u4e00\u756a\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cDRIFT\u5728out-of-distribution ConNF\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u4f7f\u7528GPT-4.1\u548cDeepSeek-V3.1\u65f6\uff0cBEq+@10\u7684\u6539\u8fdb\u5206\u522b\u4e3a37.14%\u548c42.25%\u3002", "conclusion": "\u6570\u5b66\u81ea\u52a8\u5f62\u5f0f\u5316\u4e2d\u7684\u68c0\u7d22\u6709\u6548\u6027\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u7279\u5b9a\u4e8e\u6a21\u578b\u7684\u77e5\u8bc6\u8fb9\u754c\uff0c\u8fd9\u7a81\u51fa\u4e86\u9700\u8981\u4e0e\u6bcf\u4e2a\u6a21\u578b\u7684\u529f\u80fd\u76f8\u4e00\u81f4\u7684\u81ea\u9002\u5e94\u68c0\u7d22\u7b56\u7565\u3002"}}
{"id": "2510.10062", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10062", "abs": "https://arxiv.org/abs/2510.10062", "authors": ["Adnan El Assadi", "Isaac Chung", "Roman Solomatin", "Niklas Muennighoff", "Kenneth Enevoldsen"], "title": "HUME: Measuring the Human-Model Performance Gap in Text Embedding Task", "comment": "Submitted to ICLR 2026", "summary": "Comparing human and model performance offers a valuable perspective for\nunderstanding the strengths and limitations of embedding models, highlighting\nwhere they succeed and where they fail to capture meaning and nuance. However,\nsuch comparisons are rarely made, as human performance on embedding tasks is\ndifficult to measure. To fill this gap, we introduce HUME: Human Evaluation\nFramework for Text Embeddings. While frameworks like MTEB provide broad model\nevaluation, they lack reliable estimates of human performance, limiting the\ninterpretability of model scores. We measure human performance across 16 MTEB\ndatasets spanning reranking, classification, clustering, and semantic textual\nsimilarity across linguistically diverse high- and low-resource languages.\nHumans achieve an average performance of 77.6% compared to 80.1% for the best\nembedding model, although variation is substantial: models reach near-ceiling\nperformance on some datasets while struggling on others, suggesting dataset\nissues and revealing shortcomings in low-resource languages. We provide human\nperformance baselines, insight into task difficulty patterns, and an extensible\nevaluation framework that enables a more meaningful interpretation of the model\nand informs the development of both models and benchmarks. Our code, dataset,\nand leaderboard are publicly available at\nhttps://github.com/embeddings-benchmark/mteb.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aHUME\u7684\u4eba\u5de5\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5d4c\u5165\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5c06\u5176\u4e0e\u4eba\u7c7b\u8868\u73b0\u8fdb\u884c\u6bd4\u8f83\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5bf9\u6587\u672c\u5d4c\u5165\u6a21\u578b\u8fdb\u884c\u4eba\u5de5\u8bc4\u4f30\u7684\u53ef\u9760\u65b9\u6cd5\uff0c\u96be\u4ee5\u7406\u89e3\u6a21\u578b\u6027\u80fd\u7684\u4f18\u52a3\u3002", "method": "\u901a\u8fc7HUME\u6846\u67b6\uff0c\u6d4b\u91cf\u4eba\u7c7b\u572816\u4e2aMTEB\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u6db5\u76d6\u591a\u79cd\u4efb\u52a1\u548c\u8bed\u8a00\u3002", "result": "\u4eba\u7c7b\u7684\u5e73\u5747\u8868\u73b0\u4e3a77.6%\uff0c\u6700\u4f73\u5d4c\u5165\u6a21\u578b\u4e3a80.1%\uff0c\u4f46\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u5dee\u5f02\u5f88\u5927\uff0c\u8868\u660e\u6570\u636e\u96c6\u5b58\u5728\u95ee\u9898\uff0c\u4e14\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u5b58\u5728\u4e0d\u8db3\u3002", "conclusion": "\u63d0\u4f9b\u4eba\u5de5\u8bc4\u4f30\u57fa\u51c6\uff0c\u63ed\u793a\u4efb\u52a1\u96be\u5ea6\u6a21\u5f0f\uff0c\u5e76\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u89e3\u8bfb\u6a21\u578b\uff0c\u5e76\u4e3a\u6a21\u578b\u548c\u57fa\u51c6\u7684\u5f00\u53d1\u63d0\u4f9b\u4fe1\u606f\u3002"}}
{"id": "2510.10030", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10030", "abs": "https://arxiv.org/abs/2510.10030", "authors": ["Henan Wang", "Hanxin Zhu", "Xinliang Gong", "Tianyu He", "Xin Li", "Zhibo Chen"], "title": "P-4DGS: Predictive 4D Gaussian Splatting with 90$\\times$ Compression", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has garnered significant attention due to its\nsuperior scene representation fidelity and real-time rendering performance,\nespecially for dynamic 3D scene reconstruction (\\textit{i.e.}, 4D\nreconstruction). However, despite achieving promising results, most existing\nalgorithms overlook the substantial temporal and spatial redundancies inherent\nin dynamic scenes, leading to prohibitive memory consumption. To address this,\nwe propose P-4DGS, a novel dynamic 3DGS representation for compact 4D scene\nmodeling. Inspired by intra- and inter-frame prediction techniques commonly\nused in video compression, we first design a 3D anchor point-based\nspatial-temporal prediction module to fully exploit the spatial-temporal\ncorrelations across different 3D Gaussian primitives. Subsequently, we employ\nan adaptive quantization strategy combined with context-based entropy coding to\nfurther reduce the size of the 3D anchor points, thereby achieving enhanced\ncompression efficiency. To evaluate the rate-distortion performance of our\nproposed P-4DGS in comparison with other dynamic 3DGS representations, we\nconduct extensive experiments on both synthetic and real-world datasets.\nExperimental results demonstrate that our approach achieves state-of-the-art\nreconstruction quality and the fastest rendering speed, with a remarkably low\nstorage footprint (around \\textbf{1MB} on average), achieving up to\n\\textbf{40$\\times$} and \\textbf{90$\\times$} compression on synthetic and\nreal-world scenes, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u52a8\u60013D\u9ad8\u65af\u6e85\u5c04\u8868\u793a\u65b9\u6cd5P-4DGS\uff0c\u7528\u4e8e\u7d27\u51d1\u76844D\u573a\u666f\u5efa\u6a21\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5b58\u50a8\u7a7a\u95f4\u5360\u7528\u3002", "motivation": "\u73b0\u6709\u52a8\u60013D\u9ad8\u65af\u6e85\u5c04\u7b97\u6cd5\u5ffd\u7565\u4e86\u52a8\u6001\u573a\u666f\u4e2d\u56fa\u6709\u7684\u65f6\u95f4\u548c\u7a7a\u95f4\u5197\u4f59\uff0c\u5bfc\u81f4\u5185\u5b58\u6d88\u8017\u8fc7\u5927\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e3D\u951a\u70b9\u7684\u65f6\u7a7a\u9884\u6d4b\u6a21\u5757\uff0c\u5145\u5206\u5229\u7528\u4e0d\u540c3D\u9ad8\u65af\u57fa\u5143\u4e4b\u95f4\u7684\u65f6\u7a7a\u76f8\u5173\u6027\u3002\u91c7\u7528\u81ea\u9002\u5e94\u91cf\u5316\u7b56\u7565\u7ed3\u5408\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u71b5\u7f16\u7801\uff0c\u8fdb\u4e00\u6b65\u51cf\u5c0f3D\u951a\u70b9\u7684\u5927\u5c0f\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u91cd\u5efa\u8d28\u91cf\u548c\u6700\u5feb\u7684\u6e32\u67d3\u901f\u5ea6\uff0c\u5e73\u5747\u5b58\u50a8\u7a7a\u95f4\u5360\u7528\u7ea6\u4e3a1MB\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u5206\u522b\u5b9e\u73b0\u4e86\u9ad8\u8fbe40\u500d\u548c90\u500d\u7684\u538b\u7f29\u3002", "conclusion": "P-4DGS\u5728\u52a8\u6001\u573a\u666f\u91cd\u5efa\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u5feb\u901f\u6e32\u67d3\u548c\u4f4e\u5b58\u50a8\u5360\u7528\u3002"}}
{"id": "2510.10603", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10603", "abs": "https://arxiv.org/abs/2510.10603", "authors": ["WenTao Liu", "Siyu Song", "Hao Hao", "Aimin Zhou"], "title": "EA4LLM: A Gradient-Free Approach to Large Language Model Optimization via Evolutionary Algorithms", "comment": null, "summary": "In recent years, large language models (LLMs) have made remarkable progress,\nwith model optimization primarily relying on gradient-based optimizers such as\nAdam. However, these gradient-based methods impose stringent hardware\nrequirements, demanding high-concurrency, high-memory GPUs. Moreover, they\nrequire all neural network operations to be differentiable, thereby excluding\nmany promising non-differentiable architectures from practical use. To address\nthese limitations, we propose a method for optimizing LLMs using evolutionary\nalgorithms (EA4LLM) and, for the first time, successfully demonstrate its\ncapability to train a 1-billion-parameter LLM from the pre-trained stage. We\nconduct extensive experiments and provide key insights into how evolutionary\nalgorithms can effectively optimize neural networks. Our work challenges the\nprevailing assumption that gradient-based optimization is the only viable\napproach for training neural networks. It also holds significant potential to\nreduce the computational cost of training large language models, thereby\nenabling groups with limited computational resources to participate in deep\nlearning research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u8fdb\u5316\u7b97\u6cd5\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5(EA4LLM)\uff0c\u5e76\u9996\u6b21\u6210\u529f\u5730\u8bc1\u660e\u4e86\u5176\u4ece\u9884\u8bad\u7ec3\u9636\u6bb5\u8bad\u7ec3\u4e00\u4e2a\u62e5\u670910\u4ebf\u53c2\u6570\u7684LLM\u7684\u80fd\u529b\u3002", "motivation": "\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u65b9\u6cd5\u5bf9\u786c\u4ef6\u8981\u6c42\u9ad8\uff0c\u5e76\u4e14\u9700\u8981\u6240\u6709\u795e\u7ecf\u7f51\u7edc\u64cd\u4f5c\u90fd\u662f\u53ef\u5fae\u7684\uff0c\u4ece\u800c\u6392\u9664\u4e86\u8bb8\u591a\u6709\u524d\u9014\u7684\u4e0d\u53ef\u5fae\u67b6\u6784\u7684\u5b9e\u9645\u5e94\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u4f7f\u7528\u8fdb\u5316\u7b97\u6cd5 (EA4LLM) \u4f18\u5316LLM\u3002", "result": "\u6210\u529f\u5730\u8bad\u7ec3\u4e86\u4e00\u4e2a\u62e5\u670910\u4ebf\u53c2\u6570\u7684LLM\uff0c\u5e76\u63d0\u4f9b\u4e86\u8fdb\u5316\u7b97\u6cd5\u5982\u4f55\u6709\u6548\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u5173\u952e\u89c1\u89e3\u3002", "conclusion": "\u6311\u6218\u4e86\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u662f\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u552f\u4e00\u53ef\u884c\u65b9\u6cd5\u7684\u666e\u904d\u5047\u8bbe\u3002\u5b83\u8fd8\u6709\u53ef\u80fd\u964d\u4f4e\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u4ece\u800c\u4f7f\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u56e2\u961f\u80fd\u591f\u53c2\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u3002"}}
{"id": "2510.09740", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.09740", "abs": "https://arxiv.org/abs/2510.09740", "authors": ["Atharv Goel", "Sharat Agarwal", "Saket Anand", "Chetan Arora"], "title": "Reliable Active Learning from Unreliable Labels via Neural Collapse Geometry", "comment": "Accepted to NeurIPS 2025 Workshop on Reliable ML from Unreliable Data", "summary": "Active Learning (AL) promises to reduce annotation cost by prioritizing\ninformative samples, yet its reliability is undermined when labels are noisy or\nwhen the data distribution shifts. In practice, annotators make mistakes, rare\ncategories are ambiguous, and conventional AL heuristics (uncertainty,\ndiversity) often amplify such errors by repeatedly selecting mislabeled or\nredundant samples. We propose Reliable Active Learning via Neural Collapse\nGeometry (NCAL-R), a framework that leverages the emergent geometric\nregularities of deep networks to counteract unreliable supervision. Our method\nintroduces two complementary signals: (i) a Class-Mean Alignment Perturbation\nscore, which quantifies how candidate samples structurally stabilize or distort\ninter-class geometry, and (ii) a Feature Fluctuation score, which captures\ntemporal instability of representations across training checkpoints. By\ncombining these signals, NCAL-R prioritizes samples that both preserve class\nseparation and highlight ambiguous regions, mitigating the effect of noisy or\nredundant labels. Experiments on ImageNet-100 and CIFAR100 show that NCAL-R\nconsistently outperforms standard AL baselines, achieving higher accuracy with\nfewer labels, improved robustness under synthetic label noise, and stronger\ngeneralization to out-of-distribution data. These results suggest that\nincorporating geometric reliability criteria into acquisition decisions can\nmake Active Learning less brittle to annotation errors and distribution shifts,\na key step toward trustworthy deployment in real-world labeling pipelines. Our\ncode is available at https://github.com/Vision-IIITD/NCAL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNCAL-R\u7684\u53ef\u9760\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6df1\u5ea6\u7f51\u7edc\u7684\u51e0\u4f55\u89c4\u5f8b\u6765\u51cf\u5c11\u566a\u58f0\u6807\u7b7e\u548c\u6570\u636e\u5206\u5e03\u504f\u79fb\u5bf9\u4e3b\u52a8\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u5728\u6807\u7b7e\u566a\u58f0\u6216\u6570\u636e\u5206\u5e03\u504f\u79fb\u7684\u60c5\u51b5\u4e0b\uff0c\u4f1a\u653e\u5927\u9519\u8bef\uff0c\u9009\u62e9\u9519\u8bef\u6807\u8bb0\u6216\u5197\u4f59\u6837\u672c\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "NCAL-R\u6846\u67b6\u5f15\u5165\u4e86\u4e24\u4e2a\u4e92\u8865\u7684\u4fe1\u53f7\uff1a\u7c7b\u5747\u503c\u5bf9\u9f50\u6270\u52a8\u5206\u6570\u548c\u7279\u5f81\u6ce2\u52a8\u5206\u6570\u3002\u524d\u8005\u91cf\u5316\u4e86\u5019\u9009\u6837\u672c\u5728\u7ed3\u6784\u4e0a\u7a33\u5b9a\u6216\u626d\u66f2\u7c7b\u95f4\u51e0\u4f55\u7ed3\u6784\u7684\u80fd\u529b\uff0c\u540e\u8005\u6355\u6349\u4e86\u8868\u5f81\u5728\u8bad\u7ec3\u68c0\u67e5\u70b9\u4e0a\u7684\u65f6\u95f4\u4e0d\u7a33\u5b9a\u6027\u3002\u901a\u8fc7\u7ed3\u5408\u8fd9\u4e9b\u4fe1\u53f7\uff0cNCAL-R\u80fd\u591f\u4f18\u5148\u9009\u62e9\u65e2\u80fd\u4fdd\u6301\u7c7b\u5206\u79bb\u53c8\u80fd\u7a81\u51fa\u6a21\u7cca\u533a\u57df\u7684\u6837\u672c\u3002", "result": "\u5728ImageNet-100\u548cCIFAR100\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNCAL-R\u59cb\u7ec8\u4f18\u4e8e\u6807\u51c6\u4e3b\u52a8\u5b66\u4e60\u57fa\u7ebf\uff0c\u4ee5\u66f4\u5c11\u7684\u6807\u7b7e\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u7387\uff0c\u5728\u5408\u6210\u6807\u7b7e\u566a\u58f0\u4e0b\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\uff0c\u5e76\u589e\u5f3a\u4e86\u5bf9\u5206\u5e03\u5916\u6570\u636e\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5c06\u51e0\u4f55\u53ef\u9760\u6027\u6807\u51c6\u7eb3\u5165\u83b7\u53d6\u51b3\u7b56\u53ef\u4ee5\u4f7f\u4e3b\u52a8\u5b66\u4e60\u5bf9\u6807\u6ce8\u9519\u8bef\u548c\u5206\u5e03\u504f\u79fb\u7684\u654f\u611f\u6027\u964d\u4f4e\uff0c\u8fd9\u662f\u5728\u73b0\u5b9e\u4e16\u754c\u6807\u6ce8\u7ba1\u9053\u4e2d\u5b9e\u73b0\u53ef\u4fe1\u90e8\u7f72\u7684\u5173\u952e\u4e00\u6b65\u3002"}}
{"id": "2510.11003", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11003", "abs": "https://arxiv.org/abs/2510.11003", "authors": ["Takuma Fujiu", "Sho Okazaki", "Kohei Kaminishi", "Yuji Nakata", "Shota Hamamoto", "Kenshin Yokose", "Tatsunori Hara", "Yasushi Umeda", "Jun Ota"], "title": "FBS Model-based Maintenance Record Accumulation for Failure-Cause Inference in Manufacturing Systems", "comment": null, "summary": "In manufacturing systems, identifying the causes of failures is crucial for\nmaintaining and improving production efficiency. In knowledge-based\nfailure-cause inference, it is important that the knowledge base (1) explicitly\nstructures knowledge about the target system and about failures, and (2)\ncontains sufficiently long causal chains of failures. In this study, we\nconstructed Diagnostic Knowledge Ontology and proposed a\nFunction-Behavior-Structure (FBS) model-based maintenance-record accumulation\nmethod based on it. Failure-cause inference using the maintenance records\naccumulated by the proposed method showed better agreement with the set of\ncandidate causes enumerated by experts, especially in difficult cases where the\nnumber of related cases is small and the vocabulary used differs. In the\nfuture, it will be necessary to develop inference methods tailored to these\nmaintenance records, build a user interface, and carry out validation on larger\nand more diverse systems. Additionally, this approach leverages the\nunderstanding and knowledge of the target in the design phase to support\nknowledge accumulation and problem solving during the maintenance phase, and it\nis expected to become a foundation for knowledge sharing across the entire\nengineering chain in the future.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u8bca\u65ad\u77e5\u8bc6\u672c\u4f53\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u529f\u80fd-\u884c\u4e3a-\u7ed3\u6784\uff08FBS\uff09\u6a21\u578b\u7684\u7ef4\u62a4\u8bb0\u5f55\u79ef\u7d2f\u65b9\u6cd5\u3002", "motivation": "\u5728\u5236\u9020\u7cfb\u7edf\u4e2d\uff0c\u8bc6\u522b\u6545\u969c\u539f\u56e0\u662f\u7ef4\u6301\u548c\u63d0\u9ad8\u751f\u4ea7\u6548\u7387\u7684\u5173\u952e\u3002\u5728\u57fa\u4e8e\u77e5\u8bc6\u7684\u6545\u969c\u539f\u56e0\u63a8\u65ad\u4e2d\uff0c\u77e5\u8bc6\u5e93\u9700\u8981\u660e\u786e\u5730\u6784\u5efa\u5173\u4e8e\u76ee\u6807\u7cfb\u7edf\u548c\u6545\u969c\u7684\u77e5\u8bc6\uff0c\u5e76\u5305\u542b\u8db3\u591f\u957f\u7684\u6545\u969c\u56e0\u679c\u94fe\u3002", "method": "\u6784\u5efa\u8bca\u65ad\u77e5\u8bc6\u672c\u4f53\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u529f\u80fd-\u884c\u4e3a-\u7ed3\u6784\uff08FBS\uff09\u6a21\u578b\u7684\u7ef4\u62a4\u8bb0\u5f55\u79ef\u7d2f\u65b9\u6cd5\u3002", "result": "\u4f7f\u7528\u8be5\u65b9\u6cd5\u79ef\u7d2f\u7684\u7ef4\u62a4\u8bb0\u5f55\u8fdb\u884c\u6545\u969c\u539f\u56e0\u63a8\u65ad\uff0c\u4e0e\u4e13\u5bb6\u5217\u4e3e\u7684\u5019\u9009\u539f\u56e0\u96c6\u5408\u6709\u66f4\u597d\u7684\u4e00\u81f4\u6027\uff0c\u5c24\u5176\u662f\u5728\u76f8\u5173\u6848\u4f8b\u6570\u91cf\u5c11\u4e14\u8bcd\u6c47\u4f7f\u7528\u4e0d\u540c\u7684\u56f0\u96be\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5229\u7528\u8bbe\u8ba1\u9636\u6bb5\u5bf9\u76ee\u6807\u7684\u7406\u89e3\u548c\u77e5\u8bc6\uff0c\u4ee5\u652f\u6301\u7ef4\u62a4\u9636\u6bb5\u7684\u77e5\u8bc6\u79ef\u7d2f\u548c\u95ee\u9898\u89e3\u51b3\uff0c\u5e76\u6709\u671b\u6210\u4e3a\u672a\u6765\u6574\u4e2a\u5de5\u7a0b\u94fe\u77e5\u8bc6\u5171\u4eab\u7684\u57fa\u7840\u3002"}}
{"id": "2510.10063", "categories": ["cs.CL", "cs.AI", "68T50, 68T07, 68T27", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.10063", "abs": "https://arxiv.org/abs/2510.10063", "authors": ["Yibo Yang"], "title": "CLMN: Concept based Language Models via Neural Symbolic Reasoning", "comment": "7 pages, 2 figures", "summary": "Deep learning has advanced NLP, but interpretability remains limited,\nespecially in healthcare and finance. Concept bottleneck models tie predictions\nto human concepts in vision, but NLP versions either use binary activations\nthat harm text representations or latent concepts that weaken semantics, and\nthey rarely model dynamic concept interactions such as negation and context. We\nintroduce the Concept Language Model Network (CLMN), a neural-symbolic\nframework that keeps both performance and interpretability. CLMN represents\nconcepts as continuous, human-readable embeddings and applies fuzzy-logic\nreasoning to learn adaptive interaction rules that state how concepts affect\neach other and the final decision. The model augments original text features\nwith concept-aware representations and automatically induces interpretable\nlogic rules. Across multiple datasets and pre-trained language models, CLMN\nachieves higher accuracy than existing concept-based methods while improving\nexplanation quality. These results show that integrating neural representations\nwith symbolic reasoning in a unified concept space can yield practical,\ntransparent NLP systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0cConcept Language Model Network (CLMN)\uff0c\u4ee5\u63d0\u9ad8NLP\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5176\u53ef\u89e3\u91ca\u6027\u4ecd\u7136\u6709\u9650\uff0c\u7279\u522b\u662f\u5728\u533b\u7597\u4fdd\u5065\u548c\u91d1\u878d\u9886\u57df\u3002", "method": "CLMN\u5c06\u6982\u5ff5\u8868\u793a\u4e3a\u8fde\u7eed\u7684\u3001\u4eba\u7c7b\u53ef\u8bfb\u7684\u5d4c\u5165\uff0c\u5e76\u5e94\u7528\u6a21\u7cca\u903b\u8f91\u63a8\u7406\u6765\u5b66\u4e60\u81ea\u9002\u5e94\u4ea4\u4e92\u89c4\u5219\uff0c\u8fd9\u4e9b\u89c4\u5219\u8bf4\u660e\u6982\u5ff5\u5982\u4f55\u76f8\u4e92\u5f71\u54cd\u4ee5\u53ca\u6700\u7ec8\u51b3\u7b56\u3002\u8be5\u6a21\u578b\u7528\u6982\u5ff5\u611f\u77e5\u8868\u793a\u6765\u589e\u5f3a\u539f\u59cb\u6587\u672c\u7279\u5f81\uff0c\u5e76\u81ea\u52a8\u63a8\u5bfc\u51fa\u53ef\u89e3\u91ca\u7684\u903b\u8f91\u89c4\u5219\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e0a\uff0cCLMN\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u57fa\u4e8e\u6982\u5ff5\u7684\u65b9\u6cd5\u66f4\u9ad8\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u5c06\u795e\u7ecf\u8868\u793a\u4e0e\u7b26\u53f7\u63a8\u7406\u96c6\u6210\u5728\u4e00\u4e2a\u7edf\u4e00\u7684\u6982\u5ff5\u7a7a\u95f4\u4e2d\uff0c\u53ef\u4ee5\u4ea7\u751f\u5b9e\u7528\u3001\u900f\u660e\u7684NLP\u7cfb\u7edf\u3002"}}
{"id": "2510.10051", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10051", "abs": "https://arxiv.org/abs/2510.10051", "authors": ["Sitong Gong", "Yunzhi Zhuge", "Lu Zhang", "Pingping Zhang", "Huchuan Lu"], "title": "Complementary and Contrastive Learning for Audio-Visual Segmentation", "comment": "Accepted to IEEE Transactions on Multimedia", "summary": "Audio-Visual Segmentation (AVS) aims to generate pixel-wise segmentation maps\nthat correlate with the auditory signals of objects. This field has seen\nsignificant progress with numerous CNN and Transformer-based methods enhancing\nthe segmentation accuracy and robustness. Traditional CNN approaches manage\naudio-visual interactions through basic operations like padding and\nmultiplications but are restricted by CNNs' limited local receptive field. More\nrecently, Transformer-based methods treat auditory cues as queries, utilizing\nattention mechanisms to enhance audio-visual cooperation within frames.\nNevertheless, they typically struggle to extract multimodal coefficients and\ntemporal dynamics adequately. To overcome these limitations, we present the\nComplementary and Contrastive Transformer (CCFormer), a novel framework adept\nat processing both local and global information and capturing spatial-temporal\ncontext comprehensively. Our CCFormer initiates with the Early Integration\nModule (EIM) that employs a parallel bilateral architecture, merging\nmulti-scale visual features with audio data to boost cross-modal\ncomplementarity. To extract the intra-frame spatial features and facilitate the\nperception of temporal coherence, we introduce the Multi-query Transformer\nModule (MTM), which dynamically endows audio queries with learning capabilities\nand models the frame and video-level relations simultaneously. Furthermore, we\npropose the Bi-modal Contrastive Learning (BCL) to promote the alignment across\nboth modalities in the unified feature space. Through the effective combination\nof those designs, our method sets new state-of-the-art benchmarks across the\nS4, MS3 and AVSS datasets. Our source code and model weights will be made\npublicly available at https://github.com/SitongGong/CCFormer", "AI": {"tldr": "CCFormer: a novel framework adept at processing both local and global information and capturing spatial-temporal context comprehensively for Audio-Visual Segmentation (AVS).", "motivation": "Traditional CNN and Transformer-based methods struggle to extract multimodal coefficients and temporal dynamics adequately.", "method": "Complementary and Contrastive Transformer (CCFormer) with Early Integration Module (EIM), Multi-query Transformer Module (MTM), and Bi-modal Contrastive Learning (BCL).", "result": "achieves state-of-the-art benchmarks across the S4, MS3 and AVSS datasets.", "conclusion": "The proposed method effectively combines local and global information and captures spatial-temporal context comprehensively, advancing the field of AVS."}}
{"id": "2510.10633", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10633", "abs": "https://arxiv.org/abs/2510.10633", "authors": ["Jiabao Shi", "Minfeng Qi", "Lefeng Zhang", "Di Wang", "Yingjie Zhao", "Ziying Li", "Yalong Xing", "Ningran Li"], "title": "Collaborative Text-to-Image Generation via Multi-Agent Reinforcement Learning and Semantic Fusion", "comment": "16 pages, 13 figures", "summary": "Multimodal text-to-image generation remains constrained by the difficulty of\nmaintaining semantic alignment and professional-level detail across diverse\nvisual domains. We propose a multi-agent reinforcement learning framework that\ncoordinates domain-specialized agents (e.g., focused on architecture,\nportraiture, and landscape imagery) within two coupled subsystems: a text\nenhancement module and an image generation module, each augmented with\nmultimodal integration components. Agents are trained using Proximal Policy\nOptimization (PPO) under a composite reward function that balances semantic\nsimilarity, linguistic visual quality, and content diversity. Cross-modal\nalignment is enforced through contrastive learning, bidirectional attention,\nand iterative feedback between text and image. Across six experimental\nsettings, our system significantly enriches generated content (word count\nincreased by 1614%) while reducing ROUGE-1 scores by 69.7%. Among fusion\nmethods, Transformer-based strategies achieve the highest composite score\n(0.521), despite occasional stability issues. Multimodal ensembles yield\nmoderate consistency (ranging from 0.444 to 0.481), reflecting the persistent\nchallenges of cross-modal semantic grounding. These findings underscore the\npromise of collaborative, specialization-driven architectures for advancing\nreliable multimodal generative systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u6587\u672c\u5230\u56fe\u50cf\u7684\u751f\u6210\uff0c\u4ee5\u4fdd\u6301\u8bed\u4e49\u5bf9\u9f50\u548c\u4e13\u4e1a\u6c34\u5e73\u7684\u7ec6\u8282\u3002", "motivation": "\u591a\u6a21\u6001\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u96be\u4ee5\u4fdd\u6301\u8bed\u4e49\u5bf9\u9f50\u548c\u4e13\u4e1a\u6c34\u5e73\u7684\u7ec6\u8282\u3002", "method": "\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u534f\u8c03\u9886\u57df\u4e13\u5bb6\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u3001\u53cc\u5411\u6ce8\u610f\u529b\u548c\u8fed\u4ee3\u53cd\u9988\u6765\u589e\u5f3a\u8de8\u6a21\u6001\u5bf9\u9f50\u3002", "result": "\u663e\u8457\u4e30\u5bcc\u4e86\u751f\u6210\u5185\u5bb9\uff08\u5355\u8bcd\u6570\u589e\u52a01614%\uff09\uff0c\u540c\u65f6\u964d\u4f4e\u4e86ROUGE-1\u5206\u657069.7%\u3002Transformer\u7b56\u7565\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u7efc\u5408\u5f97\u5206\uff080.521\uff09\u3002", "conclusion": "\u534f\u4f5c\u7684\u3001\u4e13\u4e1a\u5316\u9a71\u52a8\u7684\u67b6\u6784\u6709\u5e0c\u671b\u63a8\u8fdb\u53ef\u9760\u7684\u591a\u6a21\u6001\u751f\u6210\u7cfb\u7edf\u3002"}}
{"id": "2510.09752", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.09752", "abs": "https://arxiv.org/abs/2510.09752", "authors": ["Sai Krishna Reddy Mudhiganti", "Juanyan Wang", "Ruo Yang", "Manali Sharma"], "title": "Patentformer: A demonstration of AI-assisted automated patent drafting", "comment": null, "summary": "Patent drafting presents significant challenges due to its reliance on the\nextensive experience and specialized expertise of patent attorneys, who must\npossess both legal acumen and technical understanding of an invention to craft\npatent applications in a formal legal writing style. This paper presents a\ndemonstration of Patentformer, an AI-powered automated patent drafting platform\ndesigned to support patent attorneys by rapidly producing high-quality patent\napplications adhering to legal writing standards.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a Patentformer \u7684\u4eba\u5de5\u667a\u80fd\u4e13\u5229\u64b0\u5199\u5e73\u53f0\uff0c\u65e8\u5728\u901a\u8fc7\u5feb\u901f\u751f\u6210\u7b26\u5408\u6cd5\u5f8b\u5199\u4f5c\u6807\u51c6\u7684\u9ad8\u8d28\u91cf\u4e13\u5229\u7533\u8bf7\u6765\u652f\u6301\u4e13\u5229\u5f8b\u5e08\u3002", "motivation": "\u4e13\u5229\u64b0\u5199\u4f9d\u8d56\u4e8e\u4e13\u5229\u5f8b\u5e08\u7684\u4e30\u5bcc\u7ecf\u9a8c\u548c\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4ed6\u4eec\u5fc5\u987b\u540c\u65f6\u5177\u5907\u6cd5\u5f8b\u654f\u9510\u6027\u548c\u5bf9\u53d1\u660e\u7684\u6280\u672f\u7406\u89e3\uff0c\u624d\u80fd\u4ee5\u6b63\u5f0f\u7684\u6cd5\u5f8b\u5199\u4f5c\u98ce\u683c\u64b0\u5199\u4e13\u5229\u7533\u8bf7\u3002", "method": "\u5c55\u793a Patentformer \u5e73\u53f0", "result": "\u5feb\u901f\u751f\u6210\u9ad8\u8d28\u91cf\u4e13\u5229\u7533\u8bf7", "conclusion": "AI \u9a71\u52a8\u7684\u81ea\u52a8\u5316\u4e13\u5229\u64b0\u5199\u5e73\u53f0\u53ef\u4ee5\u652f\u6301\u4e13\u5229\u5f8b\u5e08"}}
{"id": "2510.11168", "categories": ["cs.LG", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11168", "abs": "https://arxiv.org/abs/2510.11168", "authors": ["Jinbin Zhang", "Nasib Ullah", "Erik Schultheis", "Rohit Babbar"], "title": "ELMO: Efficiency via Low-precision and Peak Memory Optimization in Large Output Spaces", "comment": "Accepted to ICML 2025", "summary": "Large output spaces, also referred to as Extreme multilabel classification\n(XMC), is a setting that arises, e.g., in large-scale tagging and\nproduct-to-product recommendation, and is characterized by the number of labels\nranging from hundreds of thousands to millions. This means that the linear\nclassification head, usually only a tiny fraction of the overall model, turns\ninto the main driver for compute and memory demand. Current state-of-the-art\nXMC methods predominantly rely on FP16-FP32 mixed-precision training, which we\nshow can be unstable, and inefficient in terms of memory usage and\ncomputational overhead. Meanwhile, existing low-precision methods typically\nretain higher precision for the classification layer. In this work, we propose\nELMO, a pure low-precision training framework for XMC models using BFloat16 and\nFloat8 data types. By leveraging Kahan summation and stochastic rounding, we\ndemonstrate that XMC models can be effectively trained entirely in Float8,\nwithout relying on single-precision master weights or tensor scaling.\nLow-precision training, combined with our proposed memory optimizations --\ngradient fusion and chunking -- enables significant reductions in GPU memory\nusage. For example, we train a 3-million-label XMC model with only 6.6 GiB of\nGPU memory, compared to the 39.7 GiB required by the optimized SOTA method,\nRenee without compromising accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aELMO\u7684\u7eaf\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u5177\u6709\u6570\u767e\u4e07\u6807\u7b7e\u7684\u8d85\u5927\u89c4\u6a21\u591a\u6807\u7b7e\u5206\u7c7b\uff08XMC\uff09\u6a21\u578b\u3002", "motivation": "\u5f53\u524dXMC\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56FP16-FP32\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\uff0c\u8fd9\u53ef\u80fd\u4e0d\u7a33\u5b9a\u4e14\u6548\u7387\u4f4e\u4e0b\u3002\u73b0\u6709\u7684\u4f4e\u7cbe\u5ea6\u65b9\u6cd5\u901a\u5e38\u4e3a\u5206\u7c7b\u5c42\u4fdd\u7559\u8f83\u9ad8\u7cbe\u5ea6\u3002", "method": "\u5229\u7528Kahan\u6c42\u548c\u548c\u968f\u673a\u820d\u5165\uff0c\u5728Float8\u6570\u636e\u7c7b\u578b\u4e2d\u6709\u6548\u8bad\u7ec3XMC\u6a21\u578b\uff0c\u65e0\u9700\u5355\u7cbe\u5ea6master\u6743\u91cd\u6216\u5f20\u91cf\u7f29\u653e\u3002\u7ed3\u5408\u68af\u5ea6\u878d\u5408\u548c\u5206\u5757\u7684\u5185\u5b58\u4f18\u5316\u3002", "result": "\u4f7f\u7528ELMO\uff0c\u4ec5\u75286.6 GiB\u7684GPU\u5185\u5b58\u5373\u53ef\u8bad\u7ec3\u4e00\u4e2a\u5177\u6709300\u4e07\u6807\u7b7e\u7684XMC\u6a21\u578b\uff0c\u800c\u4f18\u5316\u7684SOTA\u65b9\u6cd5Renee\u9700\u898139.7 GiB\uff0c\u4e14\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u3002", "conclusion": "ELMO\u6846\u67b6\u901a\u8fc7\u7eaf\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u548c\u5185\u5b58\u4f18\u5316\uff0c\u663e\u8457\u964d\u4f4e\u4e86XMC\u6a21\u578b\u7684GPU\u5185\u5b58\u4f7f\u7528\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7cbe\u5ea6\u3002"}}
{"id": "2510.10072", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10072", "abs": "https://arxiv.org/abs/2510.10072", "authors": ["Hua Cai", "Shuang Zhao", "Liang Zhang", "Xuli Shen", "Qing Xu", "Weilin Shen", "Zihao Wen", "Tianke Ban"], "title": "Unilaw-R1: A Large Language Model for Legal Reasoning with Reinforcement Learning and Iterative Inference", "comment": null, "summary": "Reasoning-focused large language models (LLMs) are rapidly evolving across\nvarious domains, yet their capabilities in handling complex legal problems\nremains underexplored. In this paper, we introduce Unilaw-R1, a large language\nmodel tailored for legal reasoning. With a lightweight 7-billion parameter\nscale, Unilaw-R1 significantly reduces deployment cost while effectively\ntackling three core challenges in the legal domain: insufficient legal\nknowledge, unreliable reasoning logic, and weak business generalization. To\naddress these issues, we first construct Unilaw-R1-Data, a high-quality dataset\ncontaining 17K distilled and screened chain-of-thought (CoT) samples. Based on\nthis, we adopt a two-stage training strategy combining Supervised Fine-Tuning\n(SFT) and Reinforcement Learning (RL), which significantly boosts the\nperformance on complex legal reasoning tasks and supports interpretable\ndecision-making in legal AI applications. To assess legal reasoning ability, we\nalso introduce Unilaw-R1-Eval, a dedicated benchmark designed to evaluate\nmodels across single- and multi-choice legal tasks. Unilaw-R1 demonstrates\nstrong results on authoritative benchmarks, outperforming all models of similar\nscale and achieving performance on par with the much larger\nDeepSeek-R1-Distill-Qwen-32B (54.9%). Following domain-specific training, it\nalso showed significant gains on LawBench and LexEval, exceeding\nQwen-2.5-7B-Instruct (46.6%) by an average margin of 6.6%.", "AI": {"tldr": "Unilaw-R1 is a 7B parameter LLM tailored for legal reasoning, reducing deployment cost while tackling legal domain challenges.", "motivation": "Existing reasoning-focused LLMs are underexplored in handling complex legal problems.", "method": "A two-stage training strategy combining Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on a high-quality dataset containing 17K distilled and screened chain-of-thought (CoT) samples.", "result": "Unilaw-R1 outperforms all models of similar scale and achieves performance on par with the much larger DeepSeek-R1-Distill-Qwen-32B (54.9%).", "conclusion": "Unilaw-R1 demonstrates strong results on authoritative benchmarks and showed significant gains on LawBench and LexEval, exceeding Qwen-2.5-7B-Instruct (46.6%) by an average margin of 6.6%."}}
{"id": "2510.10052", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10052", "abs": "https://arxiv.org/abs/2510.10052", "authors": ["Kaitao Chen", "Shaohao Rui", "Yankai Jiang", "Jiamin Wu", "Qihao Zheng", "Chunfeng Song", "Xiaosong Wang", "Mu Zhou", "Mianxin Liu"], "title": "Think Twice to See More: Iterative Visual Reasoning in Medical VLMs", "comment": "25 pages, 21 figures", "summary": "Medical vision-language models (VLMs) excel at image-text understanding but\ntypically rely on a single-pass reasoning that neglects localized visual cues.\nIn clinical practice, however, human experts iteratively scan, focus, and\nrefine the regions of interest before reaching a final diagnosis. To narrow\nthis machine-human perception gap, we introduce ViTAR, a novel VLM framework\nthat emulates the iterative reasoning process of human experts through a\ncognitive chain of \"think-act-rethink-answer\". ViTAR treats medical images as\ninteractive objects, enabling models to engage multi-step visual reasoning. To\nsupport this approach, we curate a high-quality instruction dataset comprising\n1K interactive examples that encode expert-like diagnostic behaviors. In\naddition, a 16K visual question answering training data has been curated\ntowards fine-grained visual diagnosis. We introduce a two-stage training\nstrategy that begins with supervised fine-tuning to guide cognitive\ntrajectories, followed by the reinforcement learning to optimize\ndecision-making. Extensive evaluations demonstrate that ViTAR outperforms\nstrong state-of-the-art models. Visual attention analysis reveals that from the\n\"think\" to \"rethink\" rounds, ViTAR increasingly anchors visual grounding to\nclinically critical regions and maintains high attention allocation to visual\ntokens during reasoning, providing mechanistic insight into its improved\nperformance. These findings demonstrate that embedding expert-style iterative\nthinking chains into VLMs enhances both performance and trustworthiness of\nmedical AI.", "AI": {"tldr": "ViTAR\u662f\u4e00\u4e2a\u65b0\u7684VLM\u6846\u67b6\uff0c\u5b83\u6a21\u62df\u4eba\u7c7b\u4e13\u5bb6\u901a\u8fc7\u201c\u601d\u8003-\u884c\u52a8-\u518d\u601d\u8003-\u56de\u7b54\u201d\u7684\u8ba4\u77e5\u94fe\u8fdb\u884c\u8fed\u4ee3\u63a8\u7406\u7684\u8fc7\u7a0b\uff0c\u4ece\u800c\u5f25\u5408\u673a\u5668\u4e0e\u4eba\u7c7b\u7684\u611f\u77e5\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(VLM)\u64c5\u957f\u56fe\u50cf-\u6587\u672c\u7406\u89e3\uff0c\u4f46\u901a\u5e38\u4f9d\u8d56\u4e8e\u5ffd\u7565\u5c40\u90e8\u89c6\u89c9\u7ebf\u7d22\u7684\u5355\u6b21\u63a8\u7406\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u79cd\u673a\u5668\u4e0e\u4eba\u7c7b\u7684\u611f\u77e5\u5dee\u8ddd\u3002", "method": "ViTAR\u5c06\u533b\u5b66\u56fe\u50cf\u89c6\u4e3a\u4ea4\u4e92\u5bf9\u8c61\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u8fdb\u884c\u591a\u6b65\u9aa4\u89c6\u89c9\u63a8\u7406\u3002\u4e3a\u4e86\u652f\u6301\u8fd9\u79cd\u65b9\u6cd5\uff0c\u6211\u4eec\u6574\u7406\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b1K\u4e2a\u4ea4\u4e92\u793a\u4f8b\uff0c\u8fd9\u4e9b\u793a\u4f8b\u7f16\u7801\u4e86\u7c7b\u4f3c\u4e13\u5bb6\u7684\u8bca\u65ad\u884c\u4e3a\u3002\u6b64\u5916\uff0c\u8fd8\u6574\u7406\u4e86\u4e00\u4e2a16K\u89c6\u89c9\u95ee\u7b54\u8bad\u7ec3\u6570\u636e\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u7684\u89c6\u89c9\u8bca\u65ad\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u9996\u5148\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u4ee5\u6307\u5bfc\u8ba4\u77e5\u8f68\u8ff9\uff0c\u7136\u540e\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u4ee5\u4f18\u5316\u51b3\u7b56\u3002", "result": "ViTAR\u4f18\u4e8e\u5f3a\u5927\u7684state-of-the-art\u6a21\u578b\u3002\u89c6\u89c9\u6ce8\u610f\u529b\u5206\u6790\u8868\u660e\uff0c\u4ece\u201c\u601d\u8003\u201d\u5230\u201c\u518d\u601d\u8003\u201d\u7684\u5faa\u73af\u4e2d\uff0cViTAR\u8d8a\u6765\u8d8a\u591a\u5730\u5c06\u89c6\u89c9\u57fa\u7840\u951a\u5b9a\u5230\u4e34\u5e8a\u5173\u952e\u533a\u57df\uff0c\u5e76\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u5bf9\u89c6\u89c9\u6807\u8bb0\u7684\u9ad8\u6ce8\u610f\u529b\u5206\u914d\uff0c\u4ece\u800c\u4e3a\u6539\u8fdb\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u673a\u5236\u6027\u89c1\u89e3\u3002", "conclusion": "\u5c06\u4e13\u5bb6\u5f0f\u8fed\u4ee3\u601d\u7ef4\u94fe\u5d4c\u5165\u5230VLM\u4e2d\uff0c\u53ef\u4ee5\u63d0\u9ad8\u533b\u5b66AI\u7684\u6027\u80fd\u548c\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2510.10639", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10639", "abs": "https://arxiv.org/abs/2510.10639", "authors": ["Haemin Choi", "Gayathri Nadarajan"], "title": "Automatic Piecewise Linear Regression for Predicting Student Learning Satisfaction", "comment": null, "summary": "Although student learning satisfaction has been widely studied, modern\ntechniques such as interpretable machine learning and neural networks have not\nbeen sufficiently explored. This study demonstrates that a recent model that\ncombines boosting with interpretability, automatic piecewise linear\nregression(APLR), offers the best fit for predicting learning satisfaction\namong several state-of-the-art approaches. Through the analysis of APLR's\nnumerical and visual interpretations, students' time management and\nconcentration abilities, perceived helpfulness to classmates, and participation\nin offline courses have the most significant positive impact on learning\nsatisfaction. Surprisingly, involvement in creative activities did not\npositively affect learning satisfaction. Moreover, the contributing factors can\nbe interpreted on an individual level, allowing educators to customize\ninstructions according to student profiles.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u4f7f\u7528\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u5b66\u751f\u5b66\u4e60\u6ee1\u610f\u5ea6\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8f83\u5c11\u4f7f\u7528\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u548c\u795e\u7ecf\u7f51\u7edc\u5206\u6790\u5b66\u751f\u5b66\u4e60\u6ee1\u610f\u5ea6\u3002", "method": "\u4f7f\u7528\u7ed3\u5408boosting\u548c\u53ef\u89e3\u91ca\u6027\u7684APLR\u6a21\u578b\u3002", "result": "\u65f6\u95f4\u7ba1\u7406\u3001\u4e13\u6ce8\u529b\u3001\u5e2e\u52a9\u540c\u5b66\u548c\u53c2\u4e0e\u7ebf\u4e0b\u8bfe\u7a0b\u5bf9\u5b66\u4e60\u6ee1\u610f\u5ea6\u6709\u663e\u8457\u79ef\u6781\u5f71\u54cd\uff1b\u53c2\u4e0e\u521b\u9020\u6027\u6d3b\u52a8\u6ca1\u6709\u79ef\u6781\u5f71\u54cd\u3002", "conclusion": "APLR\u6a21\u578b\u53ef\u4ee5\u4e3a\u6559\u80b2\u8005\u63d0\u4f9b\u4e2a\u6027\u5316\u6559\u5b66\u65b9\u6848\u3002"}}
{"id": "2510.09762", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09762", "abs": "https://arxiv.org/abs/2510.09762", "authors": ["Ruo Yang", "Sai Krishna Reddy Mudhiganti", "Manali Sharma"], "title": "PatentVision: A multimodal method for drafting patent applications", "comment": null, "summary": "Patent drafting is complex due to its need for detailed technical\ndescriptions, legal compliance, and visual elements. Although Large Vision\nLanguage Models (LVLMs) show promise across various tasks, their application in\nautomating patent writing remains underexplored. In this paper, we present\nPatentVision, a multimodal framework that integrates textual and visual inputs\nsuch as patent claims and drawings to generate complete patent specifications.\nBuilt on advanced LVLMs, PatentVision enhances accuracy by combining fine tuned\nvision language models with domain specific training tailored to patents.\nExperiments reveal it surpasses text only methods, producing outputs with\ngreater fidelity and alignment with human written standards. Its incorporation\nof visual data allows it to better represent intricate design features and\nfunctional connections, leading to richer and more precise results. This study\nunderscores the value of multimodal techniques in patent automation, providing\na scalable tool to reduce manual workloads and improve consistency.\nPatentVision not only advances patent drafting but also lays the groundwork for\nbroader use of LVLMs in specialized areas, potentially transforming\nintellectual property management and innovation processes.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPatentVision\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9\u8f93\u5165\uff0c\u81ea\u52a8\u751f\u6210\u5b8c\u6574\u7684\u4e13\u5229\u8bf4\u660e\u4e66\u3002", "motivation": "\u4e13\u5229\u64b0\u5199\u56e0\u9700\u8981\u8be6\u7ec6\u7684\u6280\u672f\u63cf\u8ff0\u3001\u6cd5\u5f8b\u5408\u89c4\u6027\u548c\u89c6\u89c9\u5143\u7d20\u800c\u975e\u5e38\u590d\u6742\uff0c\u800c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5316\u4e13\u5229\u5199\u4f5c\u65b9\u9762\u7684\u5e94\u7528\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u6784\u5efa\u4e8e\u5148\u8fdb\u7684LVLMs\u4e4b\u4e0a\uff0c\u901a\u8fc7\u7ed3\u5408\u5fae\u8c03\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u9488\u5bf9\u4e13\u5229\u7684\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\uff0cPatentVision\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPatentVision\u8d85\u8d8a\u4e86\u4ec5\u4f7f\u7528\u6587\u672c\u7684\u65b9\u6cd5\uff0c\u751f\u6210\u4e86\u4e0e\u4eba\u5de5\u64b0\u5199\u6807\u51c6\u76f8\u6bd4\u5177\u6709\u66f4\u9ad8\u4fdd\u771f\u5ea6\u548c\u4e00\u81f4\u6027\u7684\u8f93\u51fa\u3002\u5b83\u5bf9\u89c6\u89c9\u6570\u636e\u7684\u6574\u5408\u4f7f\u5176\u80fd\u591f\u66f4\u597d\u5730\u8868\u793a\u590d\u6742\u7684\u8bbe\u8ba1\u7279\u5f81\u548c\u529f\u80fd\u8fde\u63a5\uff0c\u4ece\u800c\u4ea7\u751f\u66f4\u4e30\u5bcc\u548c\u66f4\u7cbe\u786e\u7684\u7ed3\u679c\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u6280\u672f\u5728\u4e13\u5229\u81ea\u52a8\u5316\u4e2d\u7684\u4ef7\u503c\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5de5\u5177\uff0c\u4ee5\u51cf\u5c11\u624b\u52a8\u5de5\u4f5c\u91cf\u5e76\u63d0\u9ad8\u4e00\u81f4\u6027\u3002PatentVision\u4e0d\u4ec5\u63a8\u8fdb\u4e86\u4e13\u5229\u8d77\u8349\uff0c\u8fd8\u4e3aLVLMs\u5728\u4e13\u95e8\u9886\u57df\u7684\u66f4\u5e7f\u6cdb\u4f7f\u7528\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u53ef\u80fd\u6539\u53d8\u77e5\u8bc6\u4ea7\u6743\u7ba1\u7406\u548c\u521b\u65b0\u6d41\u7a0b\u3002"}}
{"id": "2510.11358", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.11358", "abs": "https://arxiv.org/abs/2510.11358", "authors": ["Hengran Zhang", "Keping Bi", "Jiafeng Guo", "Jiaming Zhang", "Shuaiqiang Wang", "Dawei Yin", "Xueqi Cheng"], "title": "LLM-Specific Utility: A New Perspective for Retrieval-Augmented Generation", "comment": "13 pages, 9 figures", "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge. While traditional retrieval focuses on\nrelevance, RAG's effectiveness depends on the utility of retrieved passages,\ni.e., the usefulness in facilitating the generation of an accurate and\ncomprehensive answer. Existing studies often treat utility as a generic\nattribute, ignoring the fact that different LLMs may benefit differently from\nthe same passage due to variations in internal knowledge and comprehension\nability. In this work, we introduce and systematically investigate the notion\nof LLM-specific utility. Through large-scale experiments across multiple\ndatasets and LLMs, we demonstrate that human-annotated passages are not optimal\nfor LLMs and that ground-truth utilitarian passages are not transferable across\ndifferent LLMs. These findings highlight the necessity of adopting the\nLLM-specific utility in RAG research. Our findings indicate that some\nhuman-annotated passages are not ground-truth utilitarian passages for specific\nLLMs, partially due to the varying readability of queries and passages for\nLLMs, a tendency for which perplexity is a key metric. Based on these findings,\nwe propose a benchmarking procedure for LLM-specific utility judgments. We\nevaluate existing utility judgment methods on six datasets and find that while\nverbalized methods using pseudo-answers perform robustly, LLMs struggle to\nassess utility effectively-failing to reject all passages for known queries and\nto select truly useful ones for unknown queries.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e2dLLM\u7279\u5b9a\u6548\u7528\u7684\u6982\u5ff5\uff0c\u53d1\u73b0\u4eba\u5de5\u6807\u6ce8\u7684\u6bb5\u843d\u5bf9\u4e8eLLM\u6765\u8bf4\u5e76\u975e\u6700\u4f18\uff0c\u4e14ground-truth\u7684\u6548\u7528\u6bb5\u843d\u5728\u4e0d\u540c\u7684LLM\u4e4b\u95f4\u4e0d\u53ef\u8f6c\u79fb\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u5c06\u6548\u7528\u89c6\u4e3a\u901a\u7528\u5c5e\u6027\uff0c\u5ffd\u7565\u4e86\u4e0d\u540cLLM\u7531\u4e8e\u5185\u90e8\u77e5\u8bc6\u548c\u7406\u89e3\u80fd\u529b\u7684\u4e0d\u540c\uff0c\u53ef\u80fd\u4ece\u540c\u4e00\u6bb5\u843d\u4e2d\u83b7\u76ca\u4e0d\u540c\u7684\u4e8b\u5b9e\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\uff0c\u8de8\u591a\u4e2a\u6570\u636e\u96c6\u548cLLM\uff0c\u7cfb\u7edf\u5730\u7814\u7a76LLM\u7279\u5b9a\u6548\u7528\u7684\u6982\u5ff5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4eba\u5de5\u6807\u6ce8\u7684\u6bb5\u843d\u5bf9\u4e8eLLM\u6765\u8bf4\u5e76\u975e\u6700\u4f18\uff0c\u5e76\u4e14ground-truth\u7684\u6548\u7528\u6bb5\u843d\u5728\u4e0d\u540c\u7684LLM\u4e4b\u95f4\u4e0d\u53ef\u8f6c\u79fb\u3002\u540c\u65f6\u53d1\u73b0\uff0c\u56f0\u60d1\u5ea6\u662f\u8861\u91cfqueries\u548c\u6bb5\u843d\u53ef\u8bfb\u6027\u7684\u5173\u952e\u6307\u6807\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728RAG\u7814\u7a76\u4e2d\u91c7\u7528LLM\u7279\u5b9a\u7684\u6548\u7528\u662f\u5fc5\u8981\u7684\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2aLLM\u7279\u5b9a\u6548\u7528\u5224\u65ad\u7684\u57fa\u51c6\u6d4b\u8bd5\u7a0b\u5e8f\u3002\u8bc4\u4f30\u4e86\u73b0\u6709utility\u5224\u65ad\u65b9\u6cd5\uff0c\u53d1\u73b0\u4f7f\u7528\u4f2a\u7b54\u6848\u7684verbalized\u65b9\u6cd5\u8868\u73b0\u7a33\u5065\uff0c\u4f46LLM\u96be\u4ee5\u6709\u6548\u8bc4\u4f30utility\u3002"}}
{"id": "2510.10077", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.10077", "abs": "https://arxiv.org/abs/2510.10077", "authors": ["Wenqing Wang", "Muhammad Asif Ali", "Ali Shoker", "Ruohan Yang", "Junyang Chen", "Ying Sha", "Huan Wang"], "title": "A-IPO: Adaptive Intent-driven Preference Optimization", "comment": null, "summary": "Human preferences are diverse and dynamic, shaped by regional, cultural, and\nsocial factors. Existing alignment methods like Direct Preference Optimization\n(DPO) and its variants often default to majority views, overlooking minority\nopinions and failing to capture latent user intentions in prompts.\n  To address these limitations, we introduce \\underline{\\textbf{A}}daptive\n\\textbf{\\underline{I}}ntent-driven \\textbf{\\underline{P}}reference\n\\textbf{\\underline{O}}ptimization (\\textbf{A-IPO}). Specifically,A-IPO\nintroduces an intention module that infers the latent intent behind each user\nprompt and explicitly incorporates this inferred intent into the reward\nfunction, encouraging stronger alignment between the preferred model's\nresponses and the user's underlying intentions. We demonstrate, both\ntheoretically and empirically, that incorporating an intention--response\nsimilarity term increases the preference margin (by a positive shift of\n$\\lambda\\,\\Delta\\mathrm{sim}$ in the log-odds), resulting in clearer separation\nbetween preferred and dispreferred responses compared to DPO.\n  For evaluation, we introduce two new benchmarks, Real-pref, Attack-pref along\nwith an extended version of an existing dataset, GlobalOpinionQA-Ext, to assess\nreal-world and adversarial preference alignment.\n  Through explicit modeling of diverse user intents,A-IPO facilitates\npluralistic preference optimization while simultaneously enhancing adversarial\nrobustness in preference alignment. Comprehensive empirical evaluation\ndemonstrates that A-IPO consistently surpasses existing baselines, yielding\nsubstantial improvements across key metrics: up to +24.8 win-rate and +45.6\nResponse-Intention Consistency on Real-pref; up to +38.6 Response Similarity\nand +52.2 Defense Success Rate on Attack-pref; and up to +54.6 Intention\nConsistency Score on GlobalOpinionQA-Ext.", "AI": {"tldr": "This paper introduces A-IPO, an adaptive intent-driven preference optimization method, to address the limitations of existing alignment methods like DPO which often overlook minority opinions and fail to capture latent user intentions. A-IPO incorporates an intention module that infers the latent intent behind each user prompt and explicitly incorporates this inferred intent into the reward function.", "motivation": "Existing alignment methods often default to majority views, overlooking minority opinions and failing to capture latent user intentions in prompts.", "method": "The paper introduces A-IPO, which includes an intention module to infer latent user intents and incorporates this into the reward function. It also introduces two new benchmarks, Real-pref and Attack-pref, along with an extended version of GlobalOpinionQA-Ext, for evaluation.", "result": "A-IPO consistently surpasses existing baselines, yielding substantial improvements across key metrics: up to +24.8 win-rate and +45.6 Response-Intention Consistency on Real-pref; up to +38.6 Response Similarity and +52.2 Defense Success Rate on Attack-pref; and up to +54.6 Intention Consistency Score on GlobalOpinionQA-Ext.", "conclusion": "A-IPO facilitates pluralistic preference optimization while simultaneously enhancing adversarial robustness in preference alignment."}}
{"id": "2510.10053", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10053", "abs": "https://arxiv.org/abs/2510.10053", "authors": ["Bo Peng", "Zichuan Wang", "Sheng Yu", "Xiaochuan Jin", "Wei Wang", "Jing Dong"], "title": "DREAM: A Benchmark Study for Deepfake REalism AssessMent", "comment": null, "summary": "Deep learning based face-swap videos, widely known as deepfakes, have drawn\nwide attention due to their threat to information credibility. Recent works\nmainly focus on the problem of deepfake detection that aims to reliably tell\ndeepfakes apart from real ones, in an objective way. On the other hand, the\nsubjective perception of deepfakes, especially its computational modeling and\nimitation, is also a significant problem but lacks adequate study. In this\npaper, we focus on the visual realism assessment of deepfakes, which is defined\nas the automatic assessment of deepfake visual realism that approximates human\nperception of deepfakes. It is important for evaluating the quality and\ndeceptiveness of deepfakes which can be used for predicting the influence of\ndeepfakes on Internet, and it also has potentials in improving the deepfake\ngeneration process by serving as a critic. This paper prompts this new\ndirection by presenting a comprehensive benchmark called DREAM, which stands\nfor Deepfake REalism AssessMent. It is comprised of a deepfake video dataset of\ndiverse quality, a large scale annotation that includes 140,000 realism scores\nand textual descriptions obtained from 3,500 human annotators, and a\ncomprehensive evaluation and analysis of 16 representative realism assessment\nmethods, including recent large vision language model based methods and a newly\nproposed description-aligned CLIP method. The benchmark and insights included\nin this study can lay the foundation for future research in this direction and\nother related areas.", "AI": {"tldr": "\u672c\u6587\u7740\u773c\u4e8e\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u7684\u89c6\u89c9\u903c\u771f\u5ea6\u8bc4\u4f30\uff0c\u65e8\u5728\u6a21\u4eff\u4eba\u7c7b\u5bf9\u6df1\u5ea6\u4f2a\u9020\u7684\u611f\u77e5\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5ba2\u89c2\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\uff0c\u800c\u5bf9\u6df1\u5ea6\u4f2a\u9020\u7684\u4e3b\u89c2\u611f\u77e5\u53ca\u5176\u8ba1\u7b97\u5efa\u6a21\u7814\u7a76\u4e0d\u8db3\u3002\u8bc4\u4f30\u6df1\u5ea6\u4f2a\u9020\u7684\u8d28\u91cf\u548c\u6b3a\u9a97\u6027\uff0c\u9884\u6d4b\u5176\u5bf9\u4e92\u8054\u7f51\u7684\u5f71\u54cd\uff0c\u5e76\u6539\u8fdb\u6df1\u5ea6\u4f2a\u9020\u751f\u6210\u8fc7\u7a0b\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aDREAM\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u5305\u542b\u591a\u6837\u8d28\u91cf\u7684\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5927\u89c4\u6a21\u7684\u4eba\u5de5\u6807\u6ce8\uff0814\u4e07\u4e2a\u903c\u771f\u5ea6\u8bc4\u5206\u548c\u6587\u672c\u63cf\u8ff0\uff0c\u6765\u81ea3500\u540d\u6807\u6ce8\u8005\uff09\uff0c\u4ee5\u53ca\u5bf916\u79cd\u4ee3\u8868\u6027\u903c\u771f\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u7684\u5168\u9762\u8bc4\u4f30\u548c\u5206\u6790\uff0c\u5305\u62ec\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u4e00\u79cd\u65b0\u7684\u63cf\u8ff0\u5bf9\u9f50CLIP\u65b9\u6cd5\u3002", "result": "\u6784\u5efa\u4e86DREAM\u57fa\u51c6\uff0c\u5305\u542b\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u548c\u591a\u79cd\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u51c6\u548c\u89c1\u89e3\u53ef\u4ee5\u4e3a\u672a\u6765\u5728\u8be5\u65b9\u5411\u548c\u5176\u4ed6\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2510.10640", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10640", "abs": "https://arxiv.org/abs/2510.10640", "authors": ["Piyush Pant", "Marcellius William Suntoro", "Ayesha Siddiqua", "Muhammad Shehryaar Sharif", "Daniyal Ahmed"], "title": "Equity-Aware Geospatial AI for Forecasting Demand-Driven Hospital Locations in Germany", "comment": "7 pages. Application:\n  https://equity-aware-geospatial-ai-project.streamlit.app/ Codebase:\n  https://github.com/mwsyow/equity-aware-geospatial-ai-project/", "summary": "This paper presents EA-GeoAI, an integrated framework for demand forecasting\nand equitable hospital planning in Germany through 2030. We combine\ndistrict-level demographic shifts, aging population density, and infrastructure\nbalances into a unified Equity Index. An interpretable Agentic AI optimizer\nthen allocates beds and identifies new facility sites to minimize unmet need\nunder budget and travel-time constraints. This approach bridges GeoAI,\nlong-term forecasting, and equity measurement to deliver actionable\nrecommendations for policymakers.", "AI": {"tldr": "EA-GeoAI\u6846\u67b6\u7528\u4e8e\u5fb7\u56fd\u52302030\u5e74\u7684\u9700\u6c42\u9884\u6d4b\u548c\u516c\u5e73\u533b\u9662\u89c4\u5212\u3002", "motivation": "\u7ed3\u5408\u533a\u57df\u4eba\u53e3\u53d8\u5316\u3001\u8001\u9f84\u5316\u4eba\u53e3\u5bc6\u5ea6\u548c\u57fa\u7840\u8bbe\u65bd\u5e73\u8861\uff0c\u5f62\u6210\u7edf\u4e00\u7684\u516c\u5e73\u6307\u6570\u3002", "method": "\u4f7f\u7528\u53ef\u89e3\u91ca\u7684Agentic AI\u4f18\u5316\u5668\u6765\u5206\u914d\u5e8a\u4f4d\u548c\u8bc6\u522b\u65b0\u7684\u8bbe\u65bd\u5730\u70b9\uff0c\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u9884\u7b97\u548c\u65c5\u884c\u65f6\u95f4\u9650\u5236\u4e0b\u672a\u6ee1\u8db3\u7684\u9700\u6c42\u3002", "result": "\u4e3a\u51b3\u7b56\u8005\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u5efa\u8bae\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6865\u63a5\u4e86GeoAI\u3001\u957f\u671f\u9884\u6d4b\u548c\u516c\u5e73\u6027\u6d4b\u91cf\u3002"}}
{"id": "2510.09764", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09764", "abs": "https://arxiv.org/abs/2510.09764", "authors": ["Wanting Mao", "Maxwell A Xu", "Harish Haresamudram", "Mithun Saha", "Santosh Kumar", "James Matthew Rehg"], "title": "Leveraging Shared Prototypes for a Multimodal Pulse Motion Foundation Model", "comment": null, "summary": "Modeling multi-modal time-series data is critical for capturing system-level\ndynamics, particularly in biosignals where modalities such as ECG, PPG, EDA,\nand accelerometry provide complementary perspectives on interconnected\nphysiological processes. While recent self-supervised learning (SSL) advances\nhave improved unimodal representation learning, existing multi-modal approaches\noften rely on CLIP-style contrastive objectives that overfit to easily aligned\nfeatures and misclassify valid cross-modal relationships as negatives,\nresulting in fragmented and non-generalizable embeddings. To overcome these\nlimitations, we propose ProtoMM, a novel SSL framework that introduces a shared\nprototype dictionary to anchor heterogeneous modalities in a common embedding\nspace. By clustering representations around shared prototypes rather than\nexplicit negative sampling, our method captures complementary information\nacross modalities and provides a coherent \"common language\" for physiological\nsignals. In this work, we focus on developing a Pulse Motion foundation model\nwith ProtoMM and demonstrate that our approach outperforms contrastive-only and\nprior multimodal SSL methods, achieving state-of-the-art performance while\noffering improved interpretability of learned features.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6ProtoMM\uff0c\u7528\u4e8e\u5efa\u6a21\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u7279\u522b\u662f\u5728\u751f\u7269\u4fe1\u53f7\u9886\u57df\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u4f9d\u8d56\u4e8eCLIP\u5f0f\u7684\u5bf9\u6bd4\u76ee\u6807\uff0c\u5bb9\u6613\u8fc7\u62df\u5408\u5230\u5bb9\u6613\u5bf9\u9f50\u7684\u7279\u5f81\uff0c\u5e76\u5c06\u6709\u6548\u7684\u8de8\u6a21\u6001\u5173\u7cfb\u9519\u8bef\u5206\u7c7b\u4e3a\u8d1f\u6837\u672c\uff0c\u5bfc\u81f4\u5d4c\u5165\u5206\u6563\u4e14\u4e0d\u5177\u6709\u6cdb\u5316\u6027\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u5171\u4eab\u539f\u578b\u5b57\u5178\uff0c\u4ee5\u5c06\u5f02\u6784\u6a21\u6001\u951a\u5b9a\u5728\u516c\u5171\u5d4c\u5165\u7a7a\u95f4\u4e2d\u3002\u901a\u8fc7\u56f4\u7ed5\u5171\u4eab\u539f\u578b\u805a\u7c7b\u8868\u793a\uff0c\u800c\u4e0d\u662f\u663e\u5f0f\u7684\u8d1f\u91c7\u6837\uff0c\u6765\u6355\u83b7\u8de8\u6a21\u6001\u7684\u4e92\u8865\u4fe1\u606f\u3002", "result": "\u5728\u8109\u640f\u8fd0\u52a8\u57fa\u7840\u6a21\u578b\u7684\u5f00\u53d1\u4e2d\uff0cProtoMM\u4f18\u4e8e\u4ec5\u5bf9\u6bd4\u65b9\u6cd5\u548c\u5148\u524d\u7684\u591a\u6a21\u6001SSL\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "ProtoMM\u5728\u591a\u6a21\u6001\u751f\u7269\u4fe1\u53f7\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u63d0\u9ad8\u4e86\u5b66\u4e60\u7279\u5f81\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2510.11599", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.11599", "abs": "https://arxiv.org/abs/2510.11599", "authors": ["Marc Brinner", "Sina Zarrie\u00df"], "title": "SemCSE-Multi: Multifaceted and Decodable Embeddings for Aspect-Specific and Interpretable Scientific Domain Mapping", "comment": null, "summary": "We propose SemCSE-Multi, a novel unsupervised framework for generating\nmultifaceted embeddings of scientific abstracts, evaluated in the domains of\ninvasion biology and medicine. These embeddings capture distinct, individually\nspecifiable aspects in isolation, thus enabling fine-grained and controllable\nsimilarity assessments as well as adaptive, user-driven visualizations of\nscientific domains. Our approach relies on an unsupervised procedure that\nproduces aspect-specific summarizing sentences and trains embedding models to\nmap semantically related summaries to nearby positions in the embedding space.\nWe then distill these aspect-specific embedding capabilities into a unified\nembedding model that directly predicts multiple aspect embeddings from a\nscientific abstract in a single, efficient forward pass. In addition, we\nintroduce an embedding decoding pipeline that decodes embeddings back into\nnatural language descriptions of their associated aspects. Notably, we show\nthat this decoding remains effective even for unoccupied regions in\nlow-dimensional visualizations, thus offering vastly improved interpretability\nin user-centric settings.", "AI": {"tldr": "SemCSE-Multi: An unsupervised framework for generating multifaceted embeddings of scientific abstracts.", "motivation": "Capture distinct, individually specifiable aspects in isolation, thus enabling fine-grained and controllable similarity assessments as well as adaptive, user-driven visualizations of scientific domains.", "method": "Unsupervised procedure that produces aspect-specific summarizing sentences and trains embedding models. Distill these aspect-specific embedding capabilities into a unified embedding model. Introduce an embedding decoding pipeline that decodes embeddings back into natural language descriptions of their associated aspects.", "result": "Decoding remains effective even for unoccupied regions in low-dimensional visualizations, thus offering vastly improved interpretability in user-centric settings.", "conclusion": "SemCSE-Multi is a novel unsupervised framework for generating multifaceted embeddings of scientific abstracts, evaluated in the domains of invasion biology and medicine."}}
{"id": "2510.10082", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.10082", "abs": "https://arxiv.org/abs/2510.10082", "authors": ["Parthiv Chatterjee", "Shivam Sonawane", "Amey Hengle", "Aditya Tanna", "Sourish Dasgupta", "Tanmoy Chakraborty"], "title": "Diversity Augmentation of Dynamic User Preference Data for Boosting Personalized Text Summarizers", "comment": null, "summary": "Document summarization enables efficient extraction of user-relevant content\nbut is inherently shaped by individual subjectivity, making it challenging to\nidentify subjective salient information in multifaceted documents. This\ncomplexity underscores the necessity for personalized summarization. However,\ntraining models for personalized summarization has so far been challenging,\nparticularly because diverse training data containing both user preference\nhistory (i.e., click-skip trajectory) and expected (gold-reference) summaries\nare scarce. The MS/CAS PENS dataset is a valuable resource but includes only\npreference history without target summaries, preventing end-to-end supervised\nlearning, and its limited topic-transition diversity further restricts\ngeneralization. To address this, we propose $\\mathrm{PerAugy}$, a novel\ncross-trajectory shuffling and summary-content perturbation based data\naugmentation technique that significantly boosts the accuracy of four\nstate-of-the-art baseline (SOTA) user-encoders commonly used in personalized\nsummarization frameworks (best result: $\\text{0.132}$$\\uparrow$ w.r.t AUC). We\nselect two such SOTA summarizer frameworks as baselines and observe that when\naugmented with their corresponding improved user-encoders, they consistently\nshow an increase in personalization (avg. boost: $\\text{61.2\\%}\\uparrow$ w.r.t.\nPSE-SU4 metric). As a post-hoc analysis of the role of induced diversity in the\naugmented dataset by \\peraugy, we introduce three dataset diversity metrics --\n$\\mathrm{TP}$, $\\mathrm{RTC}$, and \\degreed\\ to quantify the induced diversity.\nWe find that $\\mathrm{TP}$ and $\\mathrm{DegreeD}$ strongly correlate with\nuser-encoder performance on the PerAugy-generated dataset across all accuracy\nmetrics, indicating that increased dataset diversity is a key factor driving\nperformance gains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a PerAugy \u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u901a\u8fc7\u4ea4\u53c9\u8f68\u8ff9\u6d17\u724c\u548c\u6458\u8981\u5185\u5bb9\u6270\u52a8\u6765\u63d0\u5347\u4e2a\u6027\u5316\u6458\u8981\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u4e2a\u6027\u5316\u6458\u8981\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\uff0c\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u7528\u6237\u504f\u597d\u5386\u53f2\u548c\u76ee\u6807\u6458\u8981\uff0c\u4e14\u4e3b\u9898\u8f6c\u6362\u591a\u6837\u6027\u6709\u9650\u3002", "method": "\u63d0\u51fa PerAugy \u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5305\u62ec\u4ea4\u53c9\u8f68\u8ff9\u6d17\u724c\u548c\u6458\u8981\u5185\u5bb9\u6270\u52a8\u3002", "result": "PerAugy \u663e\u8457\u63d0\u9ad8\u4e86\u56db\u4e2a\u6700\u5148\u8fdb\u7684\u7528\u6237\u7f16\u7801\u5668\u7684\u51c6\u786e\u6027\uff08AUC \u63d0\u5347 0.132\uff09\uff0c\u5e76\u63d0\u5347\u4e86\u4e2a\u6027\u5316\u6458\u8981\u6846\u67b6\u7684\u4e2a\u6027\u5316\u7a0b\u5ea6\uff08PSE-SU4 \u6307\u6807\u5e73\u5747\u63d0\u5347 61.2%\uff09\u3002", "conclusion": "\u6570\u636e\u96c6\u591a\u6837\u6027\u662f\u63d0\u5347\u4e2a\u6027\u5316\u6458\u8981\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2510.10055", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.10055", "abs": "https://arxiv.org/abs/2510.10055", "authors": ["Zhi-Fen He", "Ren-Dong Xie", "Bo Li", "Bin Liu", "Jin-Yan Hu"], "title": "Collaborative Learning of Semantic-Aware Feature Learning and Label Recovery for Multi-Label Image Recognition with Incomplete Labels", "comment": null, "summary": "Multi-label image recognition with incomplete labels is a critical learning\ntask and has emerged as a focal topic in computer vision. However, this task is\nconfronted with two core challenges: semantic-aware feature learning and\nmissing label recovery. In this paper, we propose a novel Collaborative\nLearning of Semantic-aware feature learning and Label recovery (CLSL) method\nfor multi-label image recognition with incomplete labels, which unifies the two\naforementioned challenges into a unified learning framework. More specifically,\nwe design a semantic-related feature learning module to learn robust\nsemantic-related features by discovering semantic information and label\ncorrelations. Then, a semantic-guided feature enhancement module is proposed to\ngenerate high-quality discriminative semantic-aware features by effectively\naligning visual and semantic feature spaces. Finally, we introduce a\ncollaborative learning framework that integrates semantic-aware feature\nlearning and label recovery, which can not only dynamically enhance the\ndiscriminability of semantic-aware features but also adaptively infer and\nrecover missing labels, forming a mutually reinforced loop between the two\nprocesses. Extensive experiments on three widely used public datasets (MS-COCO,\nVOC2007, and NUS-WIDE) demonstrate that CLSL outperforms the state-of-the-art\nmulti-label image recognition methods with incomplete labels.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4e0d\u5b8c\u6574\u6807\u7b7e\u591a\u6807\u7b7e\u56fe\u50cf\u8bc6\u522b\u7684\u534f\u540c\u5b66\u4e60\u8bed\u4e49\u611f\u77e5\u7279\u5f81\u5b66\u4e60\u548c\u6807\u7b7e\u6062\u590d (CLSL) \u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u8bed\u4e49\u611f\u77e5\u7279\u5f81\u5b66\u4e60\u548c\u7f3a\u5931\u6807\u7b7e\u6062\u590d\u8fd9\u4e24\u5927\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u8bed\u4e49\u76f8\u5173\u7279\u5f81\u5b66\u4e60\u6a21\u5757\u548c\u8bed\u4e49\u5f15\u5bfc\u7684\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff0c\u5e76\u5f15\u5165\u4e86\u534f\u540c\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u5728 MS-COCO\u3001VOC2007 \u548c NUS-WIDE \u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCLSL \u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "CLSL \u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u8fdb\u884c\u591a\u6807\u7b7e\u56fe\u50cf\u8bc6\u522b\uff0c\u5e76\u5728\u4e0d\u5b8c\u6574\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.10644", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.10644", "abs": "https://arxiv.org/abs/2510.10644", "authors": ["Yi Zhang", "Yushen Long", "Yun Ni", "Liping Huang", "Xiaohong Wang", "Jun Liu"], "title": "Hierarchical Optimization via LLM-Guided Objective Evolution for Mobility-on-Demand Systems", "comment": null, "summary": "Online ride-hailing platforms aim to deliver efficient mobility-on-demand\nservices, often facing challenges in balancing dynamic and spatially\nheterogeneous supply and demand. Existing methods typically fall into two\ncategories: reinforcement learning (RL) approaches, which suffer from data\ninefficiency, oversimplified modeling of real-world dynamics, and difficulty\nenforcing operational constraints; or decomposed online optimization methods,\nwhich rely on manually designed high-level objectives that lack awareness of\nlow-level routing dynamics. To address this issue, we propose a novel hybrid\nframework that integrates large language model (LLM) with mathematical\noptimization in a dynamic hierarchical system: (1) it is training-free,\nremoving the need for large-scale interaction data as in RL, and (2) it\nleverages LLM to bridge cognitive limitations caused by problem decomposition\nby adaptively generating high-level objectives. Within this framework, LLM\nserves as a meta-optimizer, producing semantic heuristics that guide a\nlow-level optimizer responsible for constraint enforcement and real-time\ndecision execution. These heuristics are refined through a closed-loop\nevolutionary process, driven by harmony search, which iteratively adapts the\nLLM prompts based on feasibility and performance feedback from the optimization\nlayer. Extensive experiments based on scenarios derived from both the New York\nand Chicago taxi datasets demonstrate the effectiveness of our approach,\nachieving an average improvement of 16% compared to state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u52a8\u6001\u5206\u5c42\u7cfb\u7edf\u4e2d\u7684\u6570\u5b66\u4f18\u5316\uff0c\u4ee5\u89e3\u51b3\u5728\u7ebf\u53eb\u8f66\u5e73\u53f0\u4e2d\u4f9b\u9700\u5e73\u8861\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5206\u4e3a\u4e24\u7c7b\uff1a\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\u548c\u5206\u89e3\u7684\u5728\u7ebf\u4f18\u5316\u65b9\u6cd5\uff0c\u4f46\u90fd\u5b58\u5728\u4e0d\u8db3\u3002\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u6548\u7387\u4f4e\u3001\u5bf9\u73b0\u5b9e\u4e16\u754c\u52a8\u6001\u7684\u5efa\u6a21\u8fc7\u4e8e\u7b80\u5355\u4ee5\u53ca\u96be\u4ee5\u6267\u884c\u64cd\u4f5c\u7ea6\u675f\u7b49\u95ee\u9898\uff1b\u5206\u89e3\u7684\u5728\u7ebf\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u624b\u52a8\u8bbe\u8ba1\u7684\u9ad8\u7ea7\u76ee\u6807\uff0c\u7f3a\u4e4f\u5bf9\u4f4e\u7ea7\u8def\u7531\u52a8\u6001\u7684\u611f\u77e5\u3002", "method": "\u8be5\u6846\u67b6\u5229\u7528LLM\u6765\u5f25\u8865\u95ee\u9898\u5206\u89e3\u9020\u6210\u7684\u8ba4\u77e5\u9650\u5236\uff0c\u81ea\u9002\u5e94\u5730\u751f\u6210\u9ad8\u7ea7\u76ee\u6807\u3002LLM\u4f5c\u4e3a\u5143\u4f18\u5316\u5668\uff0c\u4ea7\u751f\u8bed\u4e49\u542f\u53d1\u5f0f\uff0c\u6307\u5bfc\u8d1f\u8d23\u7ea6\u675f\u6267\u884c\u548c\u5b9e\u65f6\u51b3\u7b56\u6267\u884c\u7684\u4f4e\u7ea7\u4f18\u5316\u5668\u3002\u901a\u8fc7\u95ed\u73af\u8fdb\u5316\u8fc7\u7a0b\uff08\u7531\u548c\u8c10\u641c\u7d22\u9a71\u52a8\uff09\u6539\u8fdb\u8fd9\u4e9b\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u8be5\u8fc7\u7a0b\u57fa\u4e8e\u4f18\u5316\u5c42\u7684\u53ef\u884c\u6027\u548c\u6027\u80fd\u53cd\u9988\u8fed\u4ee3\u5730\u8c03\u6574LLM\u63d0\u793a\u3002", "result": "\u57fa\u4e8e\u7ebd\u7ea6\u548c\u829d\u52a0\u54e5\u51fa\u79df\u8f66\u6570\u636e\u96c6\u7684\u573a\u666f\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u662f\u6709\u6548\u7684\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5e73\u5747\u63d0\u9ad8\u4e8616%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u5c06LLM\u4e0e\u6570\u5b66\u4f18\u5316\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u89e3\u51b3\u5728\u7ebf\u53eb\u8f66\u5e73\u53f0\u4e2d\u7684\u4f9b\u9700\u5e73\u8861\u95ee\u9898\u3002"}}
