<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 46]
- [cs.CV](#cs.CV) [Total: 46]
- [cs.AI](#cs.AI) [Total: 43]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.IR](#cs.IR) [Total: 9]
- [cs.LG](#cs.LG) [Total: 46]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [EvalCards: A Framework for Standardized Evaluation Reporting](https://arxiv.org/abs/2511.21695)
*Ruchira Dhar,Danae Sanchez Villegas,Antonia Karamolegkou,Alice Schiavone,Yifei Yuan,Xinyi Chen,Jiaang Li,Stella Frank,Laura De Grazia,Monorama Swain,Stephanie Brandl,Daniel Hershcovich,Anders Søgaard,Desmond Elliott*

Main category: cs.CL

TL;DR: 这篇论文介绍了一种新的评估披露卡（EvalCards），旨在提高自然语言处理（NLP）领域的透明度，解决可重复性、可访问性和治理方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前NLP领域快速发布开放获取模型，透明的报告实践至关重要。现有的标准化工作不足以解决评估和文档方面的长期问题。

Method: 通过调查最近关于评估和文档的工作，作者识别了当前报告实践中存在的三个持续缺陷：可重复性、可访问性和治理。然后，提出了EvalCards作为解决方案。

Result: 论文介绍了Evaluation Disclosure Cards (EvalCards)，但没有提供具体的实验结果或评估指标。重点在于EvalCards的设计和作为改进报告实践的基础。

Conclusion: EvalCards旨在增强研究人员和从业人员的透明度，并为满足新兴的治理要求提供实践基础。

Abstract: Evaluation has long been a central concern in NLP, and transparent reporting practices are more critical than ever in today's landscape of rapidly released open-access models. Drawing on a survey of recent work on evaluation and documentation, we identify three persistent shortcomings in current reporting practices: reproducibility, accessibility, and governance. We argue that existing standardization efforts remain insufficient and introduce Evaluation Disclosure Cards (EvalCards) as a path forward. EvalCards are designed to enhance transparency for both researchers and practitioners while providing a practical foundation to meet emerging governance requirements.

</details>


### [2] [Cacheback: Speculative Decoding With Nothing But Cache](https://arxiv.org/abs/2511.21699)
*Zhiyao Ma,In Gim,Lin Zhong*

Main category: cs.CL

TL;DR: Cacheback Decoding: 利用token n-gram LRU缓存加速LLM推理，无需训练且模型无关。


<details>
  <summary>Details</summary>
Motivation: 利用语言的局部性加速LLM推理。

Method: 使用token n-gram的LRU缓存生成草稿序列。

Result: Cacheback在同类方法中表现出色，且易于集成到现有系统中，并具有快速适应新领域的潜力。

Conclusion: Cacheback Decoding是一种有效的LLM推理加速方法。

Abstract: We present Cacheback Decoding, a training-free and model-agnostic speculative decoding method that exploits the locality in language to accelerate Large Language Model (LLM) inference. Cacheback leverages only Least Recently Used (LRU) cache tables of token n-grams to generate draft sequences. Cacheback achieves state-of-the-art performance among comparable methods despite its minimalist design, and its simplicity allows easy integration into existing systems. Cacheback also shows potential for fast adaptation to new domains.

</details>


### [3] [JELV: A Judge of Edit-Level Validity for Evaluation and Automated Reference Expansion in Grammatical Error Correction](https://arxiv.org/abs/2511.21700)
*Yuhao Zhan,Yuqing Zhang,Jing Yuan,Qixiang Ma,Zhiqi Yang,Yu Gu,Zemin Liu,Fei Wu*

Main category: cs.CL

TL;DR: 提出了一个自动框架JELV，用于验证语法纠错(GEC)编辑的正确性，解决了现有GEC系统参考多样性有限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有语法纠错(GEC)系统存在参考多样性不足的问题，导致评估被低估，模型泛化能力受限。

Method: 提出了一个名为JELV的自动框架，通过语法性、忠实性和流畅性来验证校正编辑。JELV包含一个多轮LLM-as-Judges pipeline和一个DeBERTa分类器。还构建了一个人工标注的Pair-wise Edit-level Validity Dataset (PEVData)作为基准。

Result: JELV在评估中重新分类了被误判的假阳性，并通过整合假阳性解耦和流畅性评分，得出了与人类判断具有最高相关性的综合评估指标。此外，通过JELV过滤LLM生成的校正候选，扩展了BEA19的单参考数据集，并在该扩展数据集上重新训练GEC系统，获得了可观的性能提升。

Conclusion: JELV为增强参考多样性，加强评估和模型泛化提供了可扩展的解决方案。

Abstract: Existing Grammatical Error Correction (GEC) systems suffer from limited reference diversity, leading to underestimated evaluation and restricted model generalization. To address this issue, we introduce the Judge of Edit-Level Validity (JELV), an automated framework to validate correction edits from grammaticality, faithfulness, and fluency. Using our proposed human-annotated Pair-wise Edit-level Validity Dataset (PEVData) as benchmark, JELV offers two implementations: a multi-turn LLM-as-Judges pipeline achieving 90% agreement with human annotators, and a distilled DeBERTa classifier with 85% precision on valid edits. We then apply JELV to reclassify misjudged false positives in evaluation and derive a comprehensive evaluation metric by integrating false positive decoupling and fluency scoring, resulting in state-of-the-art correlation with human judgments. We also apply JELV to filter LLM-generated correction candidates, expanding the BEA19's single-reference dataset containing 38,692 source sentences. Retraining top GEC systems on this expanded dataset yields measurable performance gains. JELV provides a scalable solution for enhancing reference diversity and strengthening both evaluation and model generalization.

</details>


### [4] [47B Mixture-of-Experts Beats 671B Dense Models on Chinese Medical Examinations](https://arxiv.org/abs/2511.21701)
*Chiung-Yi Tseng,Danyang Zhang,Tianyang Wang,Hongying Luo,Lu Chen,Junming Huang,Jibin Guan,Junfeng Hao,Junhao Song,Ziqian Bi*

Main category: cs.CL

TL;DR: 评估了27个大型语言模型在中文医学考试问题上的表现，涵盖七个医学专业和两个专业水平。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在医学领域的应用潜力。

Method: 使用包含2800个问题的综合基准评估框架，问题来自心血管、胃肠病学、血液学、传染病、肾脏病学、神经病学和呼吸医学领域。

Result: Mixtral-8x7B 总体准确率最高，为 74.25%，其次是 DeepSeek-R1-671B，为 64.07%。模型大小与性能之间没有一致的相关性。不同医学专业之间的表现存在显着差距。顶级模型在主治医师和高级医师级别之间的性能下降 минимален 。

Conclusion: 该基准测试为大型语言模型在医学教育和临床决策支持系统中的部署提供了关键见解，突出了这些技术在专业医学环境中的希望和局限性。

Abstract: The rapid advancement of large language models(LLMs) has prompted significant interest in their potential applications in medical domains. This paper presents a comprehensive benchmark evaluation of 27 state-of-the-art LLMs on Chinese medical examination questions, encompassing seven medical specialties across two professional levels. We introduce a robust evaluation framework that assesses model performance on 2,800 carefully curated questions from cardiovascular, gastroenterology, hematology, infectious diseases, nephrology, neurology, and respiratory medicine domains. Our dataset distinguishes between attending physician and senior physician difficulty levels, providing nuanced insights into model capabilities across varying complexity. Our empirical analysis reveals substantial performance variations among models, with Mixtral-8x7B achieving the highest overall accuracy of 74.25%, followed by DeepSeek-R1-671B at 64.07%. Notably, we observe no consistent correlation between model size and performance, as evidenced by the strong performance of smaller mixture-of-experts architectures. The evaluation demonstrates significant performance gaps between medical specialties, with models generally performing better on cardiovascular and neurology questions compared to gastroenterology and nephrology domains. Furthermore, our analysis indicates minimal performance degradation between attending and senior physician levels for top-performing models, suggesting robust generalization capabilities. This benchmark provides critical insights for the deployment of LLMs in medical education and clinical decision support systems, highlighting both the promise and current limitations of these technologies in specialized medical contexts.

</details>


### [5] [CSV-Decode: Certifiable Sub-Vocabulary Decoding for Efficient Large Language Model Inference](https://arxiv.org/abs/2511.21702)
*Dong Liu,Yanxuan Yu,Ben Lengerich*

Main category: cs.CL

TL;DR: 提出了一种名为 CSV-Decode 的新方法，该方法使用几何上限来为每个解码步骤构建小的子词汇表，从而实现高效的稀疏计算，同时保持双重正确性保证：精确的 top-$k$ 认证和 $\varepsilon$-certified softmax 逼近。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型由于在大型词汇表上的昂贵输出层计算，在推理过程中面临着巨大的计算瓶颈。

Method: 离线聚类词汇嵌入，并使用质心加半径边界来识别哪些token可以安全地从计算中省略。

Result: 实验结果表明，与完整词汇解码相比，该方法在保持分布保证和低回退率的同时，显著提高了速度。

Conclusion: 提供了一个完整的系统实现，包括稀疏 GEMV 内核、多 GPU 分片和 CUDA Graph 优化。

Abstract: Large language models face significant computational bottlenecks during inference due to the expensive output layer computation over large vocabularies. We present CSV-Decode, a novel approach that uses geometric upper bounds to construct small sub-vocabularies for each decoding step, enabling efficient sparse computation while maintaining dual correctness guarantees: exact top-$k$ certification and $\varepsilon$-certified softmax approximations. Our method clusters vocabulary embeddings offline and uses centroid-plus-radius bounds to identify which tokens can be safely omitted from computation. We provide a complete system implementation with sparse GEMV kernels, multi-GPU sharding, and CUDA Graph optimization. Experimental results demonstrate significant speedup over full vocabulary decoding while maintaining distributional guarantees and low fallback rates. Our code implementation available at \href{https://github.com/FastLM/CSV-Decode}{https://github.com/FastLM/CSV-Decode}.

</details>


### [6] [Evaluating Embedding Generalization: How LLMs, LoRA, and SLERP Shape Representational Geometry](https://arxiv.org/abs/2511.21703)
*Siyaxolisa Kabane*

Main category: cs.CL

TL;DR: 研究了使用大型语言模型（LLM）与非LLM编码器作为嵌入backbone时，密集文本嵌入的泛化性能，并研究了球面线性插值（SLERP）模型合并在多大程度上缓解了任务特定适应（例如，LoRA）引入的过度专业化。


<details>
  <summary>Details</summary>
Motivation: 为了比较LLM和非LLM编码器在嵌入数值序列时的泛化性能，以及研究SLERP模型合并缓解过度专业化的效果。

Method: 设计了一组受控实验，模型嵌入短数值序列，并根据明确定义的数论性质评估其聚类和分类序列的能力。比较了四种模型：（1）从头开始训练或微调嵌入的非LLM编码器，（2）使用参数高效方法（LoRA）调整的基于LLM的编码器，（3）LoRA之后的LLM，然后进行模型汤合并到基本权重中，以及（4）使用SLERP跨检查点或阶段合并的相同LoRA调整的LLM。使用聚类指数（轮廓和Davies Bouldin）评估表征质量。此外，我们分析了kmeans标签的使用，以查看嵌入是否编码了我们正在测试的信息之外的任何其他信息。

Result: 基于LLM的backbone产生的嵌入能够更好地捕捉高阶、组合数值模式，但容易出现降低平衡泛化的适配器主导地位；SLERP合并始终如一地恢复基本模型结构，同时保留了大部分任务收益，与模型汤或未合并的模型相比，在聚类可分离性和鲁棒性方面产生了卓越的权衡。

Conclusion: SLERP 合并能有效提升模型在数值序列嵌入任务中的泛化能力和鲁棒性，优于模型汤和其他未合并的模型。

Abstract: We investigate the generalization properties of dense text embeddings when the embedding backbone is a large language model (LLM) versus when it is a non-LLM encoder, and we study the extent to which spherical linear interpolation (SLERP) model-merging mitigates over-specialization introduced by task-specific adaptation (e.g., LoRA). To make the comparison concrete and domain-agnostic, we design a controlled suite of experiments in which models embed short numerical sequences and are evaluated on their ability to cluster and classify those sequences according to well-defined number-theoretic properties. Our experimental protocol compares four families of models: (1) non-LLM encoders trained from scratch or fine-tuned for embeddings, (2) LLM-based encoders adapted with parameter-efficient methods (LoRA), (3) LLM-based encoders with LoRA followed by model souping merging into the base weights, and (4) the same LoRA-adapted LLMs merged using SLERP across checkpoints or stages. We evaluate representational quality with clustering indices (Silhouette and Davies Bouldin). We additionally analyze the use of kmeans labels to see if the embeddings encode any other information besides the one we are testing for. Empirically, we find that LLM-based backbones produce embeddings that better capture higher-order, compositional numeric patterns, but are prone to adapter dominance that degrades balanced generalization; SLERP merging consistently recovers base-model structure while retaining most task gains, yielding superior tradeoffs in clustering separability, and robustness compared to model souping or models that were not merged.

</details>


### [7] [Quantifying and Mitigating Selection Bias in LLMs: A Transferable LoRA Fine-Tuning and Efficient Majority Voting Approach](https://arxiv.org/abs/2511.21709)
*Blessed Guda,Lawrence Francis,Gabrial Zencha Ashungafac,Carlee Joe-Wong,Moise Busogi*

Main category: cs.CL

TL;DR: 这篇论文研究了大型语言模型（LLM）在多项选择题（MCQ）回答中存在的选择偏差问题，并提出了新的无监督指标和高效的偏差缓解策略。


<details>
  <summary>Details</summary>
Motivation: 现有的选择偏差指标和缓解策略存在局限性，无法充分捕捉模型预测的一致性，且计算成本高或泛化能力差。

Method: 提出了无监督的置换偏差指标（PBM）来量化模型预测的不一致性，以及一种高效的多数投票方法（BaQCKV）来降低计算成本，并提出了一种基于PBM和BaQCKV的无监督LoRA微调策略。

Result: 实验表明，该方法能有效减少偏差，提高准确性的一致性，并最大限度地降低计算成本。

Conclusion: 该论文提出的方法能够有效缓解LLM在MCQ任务中的选择偏差问题，同时保持计算效率和模型泛化能力。

Abstract: Multiple Choice Question (MCQ) answering is a widely used method for evaluating the performance of Large Language Models (LLMs). However, LLMs often exhibit selection bias in MCQ tasks, where their choices are influenced by factors like answer position or option symbols rather than the content. This bias undermines the reliability of MCQ as an evaluation framework. Most existing selection bias metrics require answer labels and measure divergences between prediction and answer distributions, but do not fully capture the consistency of a model's predictions across different orderings of answer choices. Existing selection bias mitigation strategies have notable limitations: majority voting, though effective, is computationally prohibitive; calibration-based methods require validation sets and often fail to generalize across datasets. To address these gaps, we propose three key contributions: (1) a new unsupervised label-free Permutation Bias Metric (PBM) that directly quantifies inconsistencies in model predictions across answer permutations, providing a more precise measure of selection bias, (2) an efficient majority voting approach called Batch Question-Context KV caching (BaQCKV), to significantly reduce computational costs while preserving bias mitigation effectiveness, and (3) an unsupervised Low-Rank Adaptation (LoRA-1) fine-tuning strategy based on our proposed metric and the BaQCKV that mitigates selection bias, providing a computationally efficient alternative that maintains model generalizability. Experiments across multiple MCQ benchmarks demonstrate that our approaches reduce bias, increasing consistency in accuracy while minimizing computational costs.

</details>


### [8] [On the Cross-lingual Transferability of Pre-trained wav2vec2-based Models](https://arxiv.org/abs/2511.21704)
*Jonatas Grosman,Cassio Almeida,Guilherme Schardong,Hélio Lopes*

Main category: cs.CL

TL;DR: 本文研究了基于 wav2vec 2.0 的预训练语音模型在不同语言之间的跨语言迁移能力。


<details>
  <summary>Details</summary>
Motivation: 之前的研究表明，wav2vec2 模型预训练时使用的数据会影响其在下游任务中的表现。但是，很少有研究进一步调查这些预训练模型的迁移知识在不同语言中的表现，即使目标语言与模型预训练期间使用的语言不同。

Method: 本文使用 15 个大型预训练模型，对 18 种语言的语音识别任务进行了多次微调实验。

Result: 实验结果表明，预训练期间使用的数据量对于最终性能并不像多样性那么重要。印欧语系的性能优于所评估模型中的非印欧语系。我们观察到使用单语模型的积极的跨语言知识转移，这在我们使用的所有语言中都很明显，但当预训练期间使用的语言与下游任务语言更相似时，这种现象更为明显。

Conclusion: 我们的研究结果旨在帮助科学界利用现有的基于 wav2vec2 的预训练模型，并促进新的预训练模型的开发。

Abstract: Using representations provided by a large pre-trained model has become the primary strategy for achieving state-of-the-art results in a wide range of tasks. A recently proposed large pre-trained model, wav2vec 2.0, was seminal for several other works on pre-training large models on speech data. Many models are being pre-trained using the same architecture as wav2vec 2.0 and are getting state-of-the-art in various speech-related tasks. Previous work has demonstrated that the data used during the pre-training of these wav2vec2-based models can impact the model's performance in downstream tasks, and this should be taken into consideration before utilizing these models. However, few works have proposed investigating further how the transfer knowledge of these pre-trained models behaves in different languages, even when the target language differs from the one used during the model's pre-training. Our work aims to investigate the cross-lingual transferability of these wav2vec2-based models. We performed several fine-tuning experiments on the speech recognition task in 18 languages using 15 large pre-trained models. The results of our experiments showed us that the size of data used during the pre-training of these models is not as important to the final performance as the diversity. We noticed that the performance of Indo-European languages is superior to non-Indo-European languages in the evaluated models. We have observed a positive cross-lingual transfer of knowledge using monolingual models, which was evident in all the languages we used, but more pronounced when the language used during pre-training was more similar to the downstream task language. With these findings, we aim to assist the scientific community in utilizing existing wav2vec2-based pre-trained models, as well as facilitate the pre-training of new ones.

</details>


### [9] [Insight-A: Attribution-aware for Multimodal Misinformation Detection](https://arxiv.org/abs/2511.21705)
*Junjie Wu,Yumeng Fu,Chen Gong,Guohong Fu*

Main category: cs.CL

TL;DR: 本文提出了一种名为 Insight-A 的新方法，用于检测 AI 生成的多模态错误信息，该方法侧重于将错误信息归因于伪造来源。


<details>
  <summary>Details</summary>
Motivation: 现有的方法忽略了错误信息的归因问题，无法有效识别 AI 生成内容中的多模态错误信息。

Method: 该方法包括交叉归因提示 (CAP) 和自动归因去偏提示 (ADP)，以及图像描述 (IC) 来增强跨模态一致性检查。

Result: 大量实验表明，该方法优于现有方法。

Conclusion: 该研究为 AIGC 时代的多模态错误信息检测提供了一个新的范例。

Abstract: AI-generated content (AIGC) technology has emerged as a prevalent alternative to create multimodal misinformation on social media platforms, posing unprecedented threats to societal safety. However, standard prompting leverages multimodal large language models (MLLMs) to identify the emerging misinformation, which ignores the misinformation attribution. To this end, we present Insight-A, exploring attribution with MLLM insights for detecting multimodal misinformation. Insight-A makes two efforts: I) attribute misinformation to forgery sources, and II) an effective pipeline with hierarchical reasoning that detects distortions across modalities. Specifically, to attribute misinformation to forgery traces based on generation patterns, we devise cross-attribution prompting (CAP) to model the sophisticated correlations between perception and reasoning. Meanwhile, to reduce the subjectivity of human-annotated prompts, automatic attribution-debiased prompting (ADP) is used for task adaptation on MLLMs. Additionally, we design image captioning (IC) to achieve visual details for enhancing cross-modal consistency checking. Extensive experiments demonstrate the superiority of our proposal and provide a new paradigm for multimodal misinformation detection in the era of AIGC.

</details>


### [10] [A General Highly Accurate Online Planning Method Integrating Large Language Models into Nested Rollout Policy Adaptation for Dialogue Tasks](https://arxiv.org/abs/2511.21706)
*Hui Wang,Fafa Zhang,Xiaoyu Zhang,Chaoxu Mu*

Main category: cs.CL

TL;DR: 提出了一种新的面向目标对话策略规划方法（NRPA-GD），该方法利用大型语言模型（LLM）来模拟用户和系统的行为，从而完全避免了特定模型的训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖于精细的prompt工程，其有效性严重依赖于人类经验，要么集成策略网络和预训练策略模型，这些模型通常难以适应新的对话场景且训练成本高昂。

Method: NRPA-GD构建了一个完整的对话轨迹评估机制，并采用嵌套蒙特卡罗模拟和策略自适应的优化框架，以在对话过程中动态调整策略。

Result: 在四个典型的面向目标对话数据集上的实验结果表明，NRPA-GD优于现有的prompt工程和基于特定预训练模型的方法。令人印象深刻的是，NRPA-GD仅用一个0.6亿参数的LLM就超越了ChatGPT和预训练策略模型。

Conclusion: 该方法展示了在LLM上采用规划方法来解决实际规划任务的优势和新颖性。

Abstract: In goal-oriented dialogue tasks, the main challenge is to steer the interaction towards a given goal within a limited number of turns. Existing approaches either rely on elaborate prompt engineering, whose effectiveness is heavily dependent on human experience, or integrate policy networks and pre-trained policy models, which are usually difficult to adapt to new dialogue scenarios and costly to train. Therefore, in this paper, we present Nested Rollout Policy Adaptation for Goal-oriented Dialogue (NRPA-GD), a novel dialogue policy planning method that completely avoids specific model training by utilizing a Large Language Model (LLM) to simulate behaviors of user and system at the same time. Specifically, NRPA-GD constructs a complete evaluation mechanism for dialogue trajectories and employs an optimization framework of nested Monte Carlo simulation and policy self-adaptation to dynamically adjust policies during the dialogue process. The experimental results on four typical goal-oriented dialogue datasets show that NRPA-GD outperforms both existing prompt engineering and specifically pre-trained model-based methods. Impressively, NRPA-GD surpasses ChatGPT and pre-trained policy models with only a 0.6-billion-parameter LLM. The proposed approach further demonstrates the advantages and novelty of employing planning methods on LLMs to solve practical planning tasks.

</details>


### [11] [Lost in the Pipeline: How Well Do Large Language Models Handle Data Preparation?](https://arxiv.org/abs/2511.21708)
*Matteo Spreafico,Ludovica Tassini,Camilla Sancricca,Cinzia Cappiello*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型在数据准备任务中的应用，特别是数据选择和自动化方面。


<details>
  <summary>Details</summary>
Motivation: 数据准备是数据驱动流程中的关键步骤，但通常需要大量人工。大型语言模型在各种任务中展现出卓越能力，因此有必要探索其在数据准备方面的潜力。

Method: 研究使用了通用和微调的表格大型语言模型，通过向模型提供低质量数据集来评估其执行数据分析和清洗等任务的能力。同时，将大型语言模型与传统数据准备工具提供的支持进行比较。为了评估大型语言模型的能力，开发了一个定制的质量模型，并通过用户研究验证。

Result: 论文衡量了模型执行数据分析和清洗等任务的能力，并与传统工具进行了比较。

Conclusion: 论文开发了一个定制的质量模型，并通过用户研究验证，以了解从业者的期望。

Abstract: Large language models have recently demonstrated their exceptional capabilities in supporting and automating various tasks. Among the tasks worth exploring for testing large language model capabilities, we considered data preparation, a critical yet often labor-intensive step in data-driven processes. This paper investigates whether large language models can effectively support users in selecting and automating data preparation tasks. To this aim, we considered both general-purpose and fine-tuned tabular large language models. We prompted these models with poor-quality datasets and measured their ability to perform tasks such as data profiling and cleaning. We also compare the support provided by large language models with that offered by traditional data preparation tools. To evaluate the capabilities of large language models, we developed a custom-designed quality model that has been validated through a user study to gain insights into practitioners' expectations.

</details>


### [12] [Addressing Stereotypes in Large Language Models: A Critical Examination and Mitigation](https://arxiv.org/abs/2511.21711)
*Fatima Kazi*

Main category: cs.CL

TL;DR: 本研究调查大型语言模型（LLM）中的偏差，并尝试通过微调等技术缓解这些偏差。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能的兴起，大型语言模型中的偏见问题日益突出，需要解决，以减少有害的刻板印象和错误信息。

Method: 使用StereoSet和CrowSPairs等偏见基准，采用三管齐下的方法，评估BERT、GPT 3.5和ADA等生成模型中存在的各种偏见，包括显性和隐性偏见。

Result: 微调模型在性别偏见方面表现不佳，但在识别和避免种族偏见方面表现出色。大型语言模型通常过度依赖提示中的关键词，并且无法真正理解其输出的准确性和真实性。通过微调、提示技术和数据增强等强化学习策略，模型在跨数据集测试中表现出良好的适应性，并在隐性偏见基准上显著提高了性能。

Conclusion: 本研究强调了大型语言模型中偏见问题的严重性，并提出了通过微调等技术来缓解这些偏见的初步尝试，结果表明这些方法在一定程度上是有效的。

Abstract: Large Language models (LLMs), such as ChatGPT, have gained popularity in recent years with the advancement of Natural Language Processing (NLP), with use cases spanning many disciplines and daily lives as well. LLMs inherit explicit and implicit biases from the datasets they were trained on; these biases can include social, ethical, cultural, religious, and other prejudices and stereotypes. It is important to comprehensively examine such shortcomings by identifying the existence and extent of such biases, recognizing the origin, and attempting to mitigate such biased outputs to ensure fair outputs to reduce harmful stereotypes and misinformation. This study inspects and highlights the need to address biases in LLMs amid growing generative Artificial Intelligence (AI). We utilize bias-specific benchmarks such StereoSet and CrowSPairs to evaluate the existence of various biases in many different generative models such as BERT, GPT 3.5, and ADA. To detect both explicit and implicit biases, we adopt a three-pronged approach for thorough and inclusive analysis. Results indicate fine-tuned models struggle with gender biases but excel at identifying and avoiding racial biases. Our findings also illustrated that despite some cases of success, LLMs often over-rely on keywords in prompts and its outputs. This demonstrates the incapability of LLMs to attempt to truly understand the accuracy and authenticity of its outputs. Finally, in an attempt to bolster model performance, we applied an enhancement learning strategy involving fine-tuning, models using different prompting techniques, and data augmentation of the bias benchmarks. We found fine-tuned models to exhibit promising adaptability during cross-dataset testing and significantly enhanced performance on implicit bias benchmarks, with performance gains of up to 20%.

</details>


### [13] [RAG System for Supporting Japanese Litigation Procedures: Faithful Response Generation Complying with Legal Norms](https://arxiv.org/abs/2511.22858)
*Yuya Ishihara,Atsushi Keyaki,Hiroaki Yamada,Ryutaro Ohara,Mihoko Sumida*

Main category: cs.CL

TL;DR: 本文探讨了基于检索增强生成（RAG）的LLM系统应具备的关键组件，以支持符合法律规范的日本医疗诉讼程序。


<details>
  <summary>Details</summary>
Motivation: 在诉讼中，专家委员会（如医生、建筑师、会计师和工程师）提供专业知识，以帮助法官澄清争议点。当考虑用基于RAG的LLM系统取代这些专家角色时，必须严格遵守法律规范。

Method: 讨论了RAG-based LLM系统的设计，该系统满足以下要求：检索模块必须根据禁止使用私人知识的原则检索与争议问题相关的适当外部知识；生成的响应必须来自RAG提供的上下文，并忠实于该上下文；检索模块必须参考与手头问题相对应的时间戳的外部知识。

Result: 提出了一个RAG-based LLM系统的设计方案。

Conclusion: 本文讨论了RAG-based LLM系统如何满足日本医疗诉讼程序中的法律规范要求。

Abstract: This study discusses the essential components that a Retrieval-Augmented Generation (RAG)-based LLM system should possess in order to support Japanese medical litigation procedures complying with legal norms. In litigation, expert commissioners, such as physicians, architects, accountants, and engineers, provide specialized knowledge to help judges clarify points of dispute. When considering the substitution of these expert roles with a RAG-based LLM system, the constraint of strict adherence to legal norms is imposed. Specifically, three requirements arise: (1) the retrieval module must retrieve appropriate external knowledge relevant to the disputed issues in accordance with the principle prohibiting the use of private knowledge, (2) the responses generated must originate from the context provided by the RAG and remain faithful to that context, and (3) the retrieval module must reference external knowledge with appropriate timestamps corresponding to the issues at hand. This paper discusses the design of a RAG-based LLM system that satisfies these requirements.

</details>


### [14] [EulerESG: Automating ESG Disclosure Analysis with LLMs](https://arxiv.org/abs/2511.21712)
*Yi Ding,Xushuo Tang,Zhengyi Yang,Wenqian Zhang,Simin Wu,Yuxin Huang,Lingjing Lan,Weiyuan Li,Yin Chen,Mingchen Ju,Wenke Yang,Thong Hoang,Mykhailo Klymenko,Xiwei Zu,Wenjie Zhang*

Main category: cs.CL

TL;DR: EulerESG是一个利用LLM的系统，用于自动进行ESG披露分析，并且明确了解ESG框架。


<details>
  <summary>Details</summary>
Motivation: 当前公司主要以冗长且异构的PDF文档形式发布ESG报告，使得系统性地回答简单问题变得困难。现有的工具要么依赖于脆弱的基于规则的提取，要么将ESG报告视为通用文本，而没有明确地对底层报告标准进行建模。

Method: EulerESG结合了（i）双通道检索和LLM驱动的ESG报告披露分析，以及（ii）一个用于探索、基准测试和解释的交互式仪表板和聊天机器人。

Result: 使用四个全球认可的公司和十二个SASB子行业，表明EulerESG可以自动填充与标准对齐的指标表，具有很高的保真度（高达0.95的平均准确率），同时在端到端运行时保持实用性，并且比较了此设置中的几个最新LLM模型。

Conclusion: EulerESG是一个有前景的ESG披露分析自动化系统。

Abstract: Environmental, Social, and Governance (ESG) reports have become central to how companies communicate climate risk, social impact, and governance practices, yet they are still published primarily as long, heterogeneous PDF documents. This makes it difficult to systematically answer seemingly simple questions. Existing tools either rely on brittle rule-based extraction or treat ESG reports as generic text, without explicitly modelling the underlying reporting standards. We present \textbf{EulerESG}, an LLM-powered system for automating ESG disclosure analysis with explicit awareness of ESG frameworks. EulerESG combines (i) dual-channel retrieval and LLM-driven disclosure analysis over ESG reports, and (ii) an interactive dashboard and chatbot for exploration, benchmarking, and explanation. Using four globally recognised companies and twelve SASB sub-industries, we show that EulerESG can automatically populate standard-aligned metric tables with high fidelity (up to 0.95 average accuracy) while remaining practical in end-to-end runtime, and we compare several recent LLM models in this setting. The full implementation, together with a demonstration video, is publicly available at https://github.com/UNSW-database/EulerESG.

</details>


### [15] [GPS: General Per-Sample Prompter](https://arxiv.org/abs/2511.21714)
*Pawel Batorski,Paul Swoboda*

Main category: cs.CL

TL;DR: 提出了一种新的通用、per-sample的prompt方法，名为GPS，无需特定任务调整即可为每个未见过的输入生成定制的prompt，从而提高各种任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自动prompt方法存在以下三个关键限制：(i)对于每个新任务，它们需要大型数据集来训练好的prompt；(ii)它们依赖于可能需要数小时的昂贵优化循环；(iii)它们通常生成单个任务级别的prompt，该prompt不能适应要解决的单个输入问题。

Method: 使用强化学习在一组训练任务上训练prompter，并包括一种新的正则化方法，以有效地适应per-sample prompting。最后，我们采用最小贝叶斯风险解码来稳定推理。

Result: 在文本简化任务中获得了第二好的结果，在摘要和分类任务中获得了第三好的结果，并且在GSM8K上获得了sota。

Conclusion: 我们的工作展示了一种新颖有效的自动prompting范例的潜力：生成自适应的、特定于输入的prompt，而无需大量优化，也无需访问特定于任务的训练集。

Abstract: LLMs are sensitive to prompting, with task performance often hinging on subtle, sometimes imperceptible variations in phrasing. As a result, crafting effective prompts manually remains challenging and time-consuming. Recent automatic prompting methods mitigate this difficulty but face three key limitations: (i) for each new task, they require large datasets to train good prompts;(ii) they rely on costly optimization loops that may take hours; (iii)they typically produce a single task-level prompt that does not adapt to the individual input problem to be solved.
  We propose GPS, the first general-purpose, per-sample prompting method. Without any task-specific tuning, GPS generates a tailored prompt for each unseen input, improving performance across diverse tasks. The prompter is trained with reinforcement learning on a suite of training tasks and includes a novel regularization for effectively adapting to per-sample prompting. Finally, we employ Minimum Bayes Risk decoding to stabilize inference.
  Empirically, GPS demonstrates competitive performance: we attain second best results among baselines on text simplification, third best results on summarization and on-par results on classification, while not training on any of these tasks, in contrast to the baselines. For in-domain prompting, we obtain sota on GSM8K. Our work shows the potential of a novel and effective paradigm for automatic prompting: generating adaptive, input-specific prompts without extensive optimization and without access to a task-specific training set. Our code is available at https://github.com/Batorskq/GPS.

</details>


### [16] [An Optimized Machine Learning Classifier for Detecting Fake Reviews Using Extracted Features](https://arxiv.org/abs/2511.21716)
*Shabbir Anees,Anshuman,Ayush Chaurasia,Prathmesh Bogar*

Main category: cs.CL

TL;DR: 本研究提出了一种先进的机器学习系统，用于高精度分析AI生成的评论。


<details>
  <summary>Details</summary>
Motivation: 欺诈性评论影响在线购物的信任度，特别是计算机生成的评论与人工评论混合出现。

Method: 该方法结合了文本预处理、多模态特征提取、Harris Hawks优化（HHO）特征选择和堆叠集成分类器。

Result: 在包含40,432条评论的数据集上，HHO将特征维度从13,539降至1,368，最终堆叠模型达到了95.40%的准确率。

Conclusion: 集成学习和生物启发式优化是机器生成文本识别的有效方法，同时强调了云平台大规模评论分析中保护用户数据的必要性。

Abstract: It is well known that fraudulent reviews cast doubt on the legitimacy and dependability of online purchases. The most recent development that leads customers towards darkness is the appearance of human reviews in computer-generated (CG) ones. In this work, we present an advanced machine-learning-based system that analyses these reviews produced by AI with remarkable precision. Our method integrates advanced text preprocessing, multi-modal feature extraction, Harris Hawks Optimization (HHO) for feature selection, and a stacking ensemble classifier. We implemented this methodology on a public dataset of 40,432 Original (OR) and Computer-Generated (CG) reviews. From an initial set of 13,539 features, HHO selected the most applicable 1,368 features, achieving an 89.9% dimensionality reduction. Our final stacking model achieved 95.40% accuracy, 92.81% precision, 95.01% recall, and a 93.90% F1-Score, which demonstrates that the combination of ensemble learning and bio-inspired optimisation is an effective method for machine-generated text recognition. Because large-scale review analytics commonly run on cloud platforms, privacy-preserving techniques such as differential approaches and secure outsourcing are essential to protect user data in these systems.

</details>


### [17] [CrossCheck-Bench: Diagnosing Compositional Failures in Multimodal Conflict Resolution](https://arxiv.org/abs/2511.21717)
*Baoliang Tian,Yuxuan Si,Jilong Wang,Lingyao Li,Zhongyuan Bao,Zineng Zhou,Tao Wang,Sixu Li,Ziyao Xu,Mingze Wang,Zhouzhuo Zhang,Zhihao Wang,Yike Yun,Ke Tian,Ning Yang,Minghui Qiu*

Main category: cs.CL

TL;DR: 论文提出了CrossCheck-Bench，一个用于评估多模态模型在检测矛盾方面能力的基准。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型主要在对齐的图像-文本对上进行训练和评估，忽略了它们检测和解决现实世界不一致性的能力。在开放域应用中，视觉和文本线索经常冲突，需要模型执行超出表面对齐的结构化推理。

Method: 该基准采用分层任务框架，涵盖三个级别的推理复杂性，并定义了解决跨模态不一致性所需的七个原子能力。CrossCheck-Bench包含15k个问题-答案对，这些问题-答案对来自具有合成注入矛盾的真实世界artifact。

Result: 对13个state-of-the-art的视觉-语言模型进行了评估，观察到随着任务从感知匹配转变为逻辑矛盾检测，性能持续下降。大多数模型在孤立的实体识别方面表现良好，但在必须综合多个线索进行冲突推理时失败。能力级别分析进一步揭示了不均衡的技能习得，尤其是在需要多步骤推理或基于规则的验证的任务中。

Conclusion: 多模态推理存在持续的瓶颈，并为构建能够进行鲁棒跨模态验证的模型提出了新的方向。

Abstract: Multimodal Large Language Models are primarily trained and evaluated on aligned image-text pairs, which leaves their ability to detect and resolve real-world inconsistencies largely unexplored. In open-domain applications visual and textual cues often conflict, requiring models to perform structured reasoning beyond surface-level alignment. We introduce CrossCheck-Bench, a diagnostic benchmark for evaluating contradiction detection in multimodal inputs. The benchmark adopts a hierarchical task framework covering three levels of reasoning complexity and defines seven atomic capabilities essential for resolving cross-modal inconsistencies. CrossCheck-Bench includes 15k question-answer pairs sourced from real-world artifacts with synthetically injected contradictions. The dataset is constructed through a multi-stage annotation pipeline involving more than 450 expert hours to ensure semantic validity and calibrated difficulty across perception, integration, and reasoning. We evaluate 13 state-of-the-art vision-language models and observe a consistent performance drop as tasks shift from perceptual matching to logical contradiction detection. Most models perform well on isolated entity recognition but fail when multiple clues must be synthesized for conflict reasoning. Capability-level analysis further reveals uneven skill acquisition, especially in tasks requiring multi-step inference or rule-based validation. Additional probing shows that conventional prompting strategies such as Chain-of-Thought and Set-of-Mark yield only marginal gains. By contrast, methods that interleave symbolic reasoning with grounded visual processing achieve more stable improvements. These results highlight a persistent bottleneck in multimodal reasoning and suggest new directions for building models capable of robust cross-modal verification.

</details>


### [18] [When Harmless Words Harm: A New Threat to LLM Safety via Conceptual Triggers](https://arxiv.org/abs/2511.21718)
*Zhaoxin Zhang,Borui Chen,Yiming Hu,Youyang Qu,Tianqing Zhu,Longxiang Gao*

Main category: cs.CL

TL;DR: 本文介绍了一种新的、模型无关的越狱方法MICM，该方法针对反映在LLM响应中的聚合价值结构，通过操纵模型输出中隐含的社会价值来诱导不良内容。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在绕过安全机制以引出明显有害的输出的技术上，但忽略了利用模型抽象概括能力进行的攻击，导致当前对齐策略存在盲点。

Method: MICM基于概念形态理论，通过预定义的短语将细微概念的特定配置编码到固定的提示模板中，这些短语充当概念触发器，引导模型输出到特定的价值立场，而不会触发传统的安全过滤器。

Result: 在包括GPT-4o、Deepseek-R1和Qwen3-8B在内的五个高级LLM上的评估结果表明，MICM始终优于最先进的越狱技术，以最小的拒绝率实现了高成功率。

Conclusion: 研究结果揭示了商业LLM的一个关键漏洞：它们的安全性机制仍然容易受到潜在的价值对齐操纵。

Abstract: Recent research on large language model (LLM) jailbreaks has primarily focused on techniques that bypass safety mechanisms to elicit overtly harmful outputs. However, such efforts often overlook attacks that exploit the model's capacity for abstract generalization, creating a critical blind spot in current alignment strategies. This gap enables adversaries to induce objectionable content by subtly manipulating the implicit social values embedded in model outputs. In this paper, we introduce MICM, a novel, model-agnostic jailbreak method that targets the aggregate value structure reflected in LLM responses. Drawing on conceptual morphology theory, MICM encodes specific configurations of nuanced concepts into a fixed prompt template through a predefined set of phrases. These phrases act as conceptual triggers, steering model outputs toward a specific value stance without triggering conventional safety filters. We evaluate MICM across five advanced LLMs, including GPT-4o, Deepseek-R1, and Qwen3-8B. Experimental results show that MICM consistently outperforms state-of-the-art jailbreak techniques, achieving high success rates with minimal rejection. Our findings reveal a critical vulnerability in commercial LLMs: their safety mechanisms remain susceptible to covert manipulation of underlying value alignment.

</details>


### [19] [PeerCoPilot: A Language Model-Powered Assistant for Behavioral Health Organizations](https://arxiv.org/abs/2511.21721)
*Gao Mo,Naveen Raman,Megan Chai,Cindy Peng,Shannon Pagdon,Nev Jones,Hong Shen,Peggy Swarbrick,Fei Fang*

Main category: cs.CL

TL;DR: 介绍了一种名为PeerCoPilot的LLM驱动的助手，旨在帮助同行互助组织（PROs）的同行提供者，通过创建健康计划、构建逐步目标和定位组织资源来支持这些目标。


<details>
  <summary>Details</summary>
Motivation: 同行互助组织（PROs）在帮助面临心理健康和药物滥用问题的人们方面发挥着关键作用，但资金和人员有限，难以满足所有服务用户的需求。

Method: 开发了一个由大型语言模型（LLM）驱动的助手PeerCoPilot，它通过一个由超过1300个经过审查的资源组成的大型数据库支持的检索增强生成管道，确保信息的可靠性。

Result: 对15名同行提供者和6名服务用户进行了人工评估，发现超过90%的用户支持使用PeerCoPilot。此外，PeerCoPilot提供的信息比基线LLM更可靠和具体。

Conclusion: PeerCoPilot已被CSPNJ（一个为超过10000名服务用户提供服务的大型行为健康组织）的一组5-10名同行提供者使用，并且正在积极扩大PeerCoPilot的使用范围。

Abstract: Behavioral health conditions, which include mental health and substance use disorders, are the leading disease burden in the United States. Peer-run behavioral health organizations (PROs) critically assist individuals facing these conditions by combining mental health services with assistance for needs such as income, employment, and housing. However, limited funds and staffing make it difficult for PROs to address all service user needs. To assist peer providers at PROs with their day-to-day tasks, we introduce PeerCoPilot, a large language model (LLM)-powered assistant that helps peer providers create wellness plans, construct step-by-step goals, and locate organizational resources to support these goals. PeerCoPilot ensures information reliability through a retrieval-augmented generation pipeline backed by a large database of over 1,300 vetted resources. We conducted human evaluations with 15 peer providers and 6 service users and found that over 90% of users supported using PeerCoPilot. Moreover, we demonstrated that PeerCoPilot provides more reliable and specific information than a baseline LLM. PeerCoPilot is now used by a group of 5-10 peer providers at CSPNJ, a large behavioral health organization serving over 10,000 service users, and we are actively expanding PeerCoPilot's use.

</details>


### [20] [German General Personas: A Survey-Derived Persona Prompt Collection for Population-Aligned LLM Studies](https://arxiv.org/abs/2511.21722)
*Jens Rupprecht,Leon Fröhling,Claudia Wagner,Markus Strohmaier*

Main category: cs.CL

TL;DR: 构建了一个德国通用人物角色（GGP）集合，用于通过人物角色提示模拟人类观点。


<details>
  <summary>Details</summary>
Motivation: 现有的人物角色集合缺乏，限制了模拟的准确性和代表性。

Method: 从德国通用社会调查（ALLBUS）构建一个全面且具有代表性的人物角色提示集合。

Result: GGP引导的LLM在模拟调查响应分布方面优于最先进的分类器，尤其是在数据稀缺的情况下。

Conclusion: GGP为基于LLM的社会模拟研究提供了一个有价值的资源，从而能够更系统地探索NLP和社会科学研究中与人口对齐的人物角色提示。

Abstract: The use of Large Language Models (LLMs) for simulating human perspectives via persona prompting is gaining traction in computational social science. However, well-curated, empirically grounded persona collections remain scarce, limiting the accuracy and representativeness of such simulations. Here we introduce the German General Personas (GGP) collection, a comprehensive and representative persona prompt collection built from the German General Social Survey (ALLBUS). The GGP and its persona prompts are designed to be easily plugged into prompts for all types of LLMs and tasks, steering models to generate responses aligned with the underlying German population. We evaluate GGP by prompting various LLMs to simulate survey response distributions across diverse topics, demonstrating that GGP-guided LLMs outperform state-of-the-art classifiers, particularly under data scarcity. Furthermore, we analyze how the representativity and attribute selection within persona prompts affect alignment with population responses. Our findings suggest that GGP provides a potentially valuable resource for research on LLM-based social simulations that enables more systematic explorations of population-aligned persona prompting in NLP and social science research.

</details>


### [21] [AD-CDO: A Lightweight Ontology for Representing Eligibility Criteria in Alzheimer's Disease Clinical Trials](https://arxiv.org/abs/2511.21724)
*Zenan Sun,Rashmie Abeysinghe,Xiaojin Li,Xinyue Hu,Licong Cui,Guo-Qiang Zhang,Jiang Bian,Cui Tao*

Main category: cs.CL

TL;DR: AD-CDO: A new ontology for Alzheimer's Disease clinical trials eligibility criteria.


<details>
  <summary>Details</summary>
Motivation: To represent and standardize key eligibility criteria concepts in Alzheimer's disease (AD) clinical trials.

Method: Extracted high-frequency concepts from AD clinical trials and organized them into seven semantic categories, annotated with standard biomedical vocabularies. Used Jenks Natural Breaks method to identify representative concepts.

Result: The AD-CDO achieved over 63% coverage of extracted trial concepts. Demonstrated utility through trial simulation and entity normalization.

Conclusion: AD-CDO provides a versatile foundation for ontology-driven AD clinical trial research by harmonizing essential eligibility entities and aligning them with standardized vocabularies.

Abstract: Objective
  This study introduces the Alzheimer's Disease Common Data Element Ontology for Clinical Trials (AD-CDO), a lightweight, semantically enriched ontology designed to represent and standardize key eligibility criteria concepts in Alzheimer's disease (AD) clinical trials.
  Materials and Methods
  We extracted high-frequency concepts from more than 1,500 AD clinical trials on ClinicalTrials.gov and organized them into seven semantic categories: Disease, Medication, Diagnostic Test, Procedure, Social Determinants of Health, Rating Criteria, and Fertility. Each concept was annotated with standard biomedical vocabularies, including the UMLS, OMOP Standardized Vocabularies, DrugBank, NDC, and NLM VSAC value sets. To balance coverage and manageability, we applied the Jenks Natural Breaks method to identify an optimal set of representative concepts.
  Results
  The optimized AD-CDO achieved over 63% coverage of extracted trial concepts while maintaining interpretability and compactness. The ontology effectively captured the most frequent and clinically meaningful entities used in AD eligibility criteria. We demonstrated AD-CDO's practical utility through two use cases: (a) an ontology-driven trial simulation system for formal modeling and virtual execution of clinical trials, and (b) an entity normalization task mapping raw clinical text to ontology-aligned terms, enabling consistency and integration with EHR data.
  Discussion
  AD-CDO bridges the gap between broad biomedical ontologies and task-specific trial modeling needs. It supports multiple downstream applications, including phenotyping algorithm development, cohort identification, and structured data integration.
  Conclusion
  By harmonizing essential eligibility entities and aligning them with standardized vocabularies, AD-CDO provides a versatile foundation for ontology-driven AD clinical trial research.

</details>


### [22] [PromptTailor: Multi-turn Intent-Aligned Prompt Synthesis for Lightweight LLMs](https://arxiv.org/abs/2511.21725)
*Yizhou Xu,Janet Davis*

Main category: cs.CL

TL;DR: PromptTailor是一个用于可控提示生成的系统，通过合成与意图对齐的提示来提高模型输出质量。


<details>
  <summary>Details</summary>
Motivation: 轻量级语言模型在设备端和隐私敏感型应用中很有吸引力，但它们的响应对提示质量高度敏感。非专业用户通常缺乏知识或时间来持续制作高质量的提示，导致他们依赖提示优化工具。然而，一个关键挑战是确保优化后的提示真正符合用户最初的意图和偏好。

Method: PromptTailor扩展了最小的用户指令，使其成为丰富的、领域感知的提示，同时保留了用户声明的偏好。该系统是一个量化的Llama3-8B模型，在12,300个跨越41个日常领域的提示-细化对话上进行了微调，这些对话是从三个更强大的LLM中提炼出来的。

Result: 在跨多个目标模型和优化基线的、由人类和LLM进行的评估中，PromptTailor的偏好率高于思维链提示，并且在需要较少模型调用（例如，3 vs. 9）的情况下，与最先进的提示优化方法相匹配或超过了它们。

Conclusion: 这些结果表明，在强大的教师指导下，一个紧凑的学生可以学习有效的提示生成策略，从而提高响应质量，同时保持与用户意图的一致性。

Abstract: Lightweight language models remain attractive for on-device and privacy-sensitive applications, but their responses are highly sensitive to prompt quality. For open-ended generation, non-expert users often lack the knowledge or time to consistently craft high-quality prompts, leading them to rely on prompt optimization tools. However, a key challenge is ensuring the optimized prompts genuinely align with users' original intents and preferences. We introduce PromptTailor, a system for controllable prompt generation for open-ended text that improves model output quality by intent-aligned prompt synthesis. PromptTailor expands minimal user instructions into rich, domain-aware prompts while preserving the user's stated preferences. The system is a quantized Llama3-8B model fine-tuned with a lightweight LoRA adapter on 12,300 prompt-refinement dialogues spanning 41 everyday domains, distilled from three stronger LLMs. The adapter attaches to any Llama3-8B base, enabling edge deployment. In human and LLM-judge evaluations across multiple target models and optimization baselines, PromptTailor yields higher preference rates than chain-of-thought prompting and matches or surpasses state-of-the-art prompt optimization methods while requiring fewer model calls (e.g., 3 vs. 9). These results show that a compact student, guided by powerful teachers, can learn effective prompt-generation strategies that enhance response quality while maintaining alignment with user intent.

</details>


### [23] [Goal-Directed Search Outperforms Goal-Agnostic Memory Compression in Long-Context Memory Tasks](https://arxiv.org/abs/2511.21726)
*Yicong Zheng,Kevin L. McKee,Thomas Miconi,Zacharie Bugaud,Mick van Gelderen,Jed McCaleb*

Main category: cs.CL

TL;DR: SUMER是一种通过经验回放（RLVR）学习在未压缩记忆中搜索的强化学习智能体，它优于其他有偏见的记忆压缩方法和完整上下文基线。


<details>
  <summary>Details</summary>
Motivation: 现有研究侧重于寻找最佳记忆压缩算法，但最终构建了更多人为偏差到压缩算法中，而不是找到适用于其他数据分布的通用解决方案。直接在未压缩信息上进行目标搜索可能表现更好，因为压缩是有损的，并且预定义的压缩算法不适合所有原始数据分布。

Method: 提出了一种名为SUMER（通过经验回放搜索未压缩记忆）的端到端强化学习智能体，该智能体学习使用搜索工具来收集信息并回答目标问题。

Result: 在用于长上下文对话理解的LoCoMo数据集上，SUMER优于所有其他有偏见的记忆压缩方法，并且也优于完整上下文基线，达到了SOTA性能（比之前的最佳性能提高43%）。

Conclusion: 在当前的长上下文记忆任务中，应用于原始数据的简单搜索方法优于目标不可知和有偏见的压缩算法，这证明了新范例和更动态和自主可扩展的基准的必要性。

Abstract: How to enable human-like long-term memory in large language models (LLMs) has been a central question for unlocking more general capabilities such as few-shot generalization. Existing memory frameworks and benchmarks focus on finding the optimal memory compression algorithm for higher performance in tasks that require recollection and sometimes further reasoning. However, such efforts have ended up building more human bias into the compression algorithm, through the search for the best prompts and memory architectures that suit specific benchmarks, rather than finding a general solution that would work on other data distributions. On the other hand, goal-directed search on uncompressed information could potentially exhibit superior performance because compression is lossy, and a predefined compression algorithm will not fit all raw data distributions. Here we present SUMER (Search in Uncompressed Memory via Experience Replay), an end-to-end reinforcement learning agent with verifiable reward (RLVR) that learns to use search tools to gather information and answer a target question. On the LoCoMo dataset for long-context conversation understanding, SUMER with Qwen2.5-7B-Instruct learned to use search tools and outperformed all other biased memory compression approaches and also the full-context baseline, reaching SOTA performance (43% gain over the prior best). We demonstrate that a simple search method applied to raw data outperforms goal-agnostic and biased compression algorithms in current long-context memory tasks, arguing for new paradigms and benchmarks that are more dynamic and autonomously scalable. Code for SUMER and all implemented baselines is publicly available at https://github.com/zycyc/SUMER.

</details>


### [24] [Affective Multimodal Agents with Proactive Knowledge Grounding for Emotionally Aligned Marketing Dialogue](https://arxiv.org/abs/2511.21728)
*Lin Yu,Xiaofei Han,Yifei Kang,Chiung-Yi Tseng,Danyang Zhang,Ziqian Bi,Zhimo Han*

Main category: cs.CL

TL;DR: 提出了一种名为 AffectMind 的多模态情感对话代理，它能够进行主动推理和动态知识 grounding，以维持情感一致且具有说服力的互动。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型虽然能够实现流畅的对话系统，但在情感丰富、目标导向的场景（如营销对话）中仍然存在局限性，它们通常是被动的，并且难以应对复杂的情感。

Method: 该方法结合了三个组件：主动知识 Grounding 网络 (PKGN)、情感-意图对齐模型 (EIAM) 和强化话语循环 (RDL)。

Result: 在 MM-ConvMarket 和 AffectPromo 两个新构建的营销对话数据集上的实验表明，AffectMind 在情感一致性 (+26%)、说服成功率 (+19%) 和长期用户参与度 (+23%) 方面优于强大的基于 LLM 的基线模型。

Conclusion: 情感 grounding 的主动性是商业多模态代理的关键能力。

Abstract: Recent advances in large language models (LLMs) have enabled fluent dialogue systems, but most remain reactive and struggle in emotionally rich, goal-oriented settings such as marketing conversations. To address this limitation, we propose AffectMind, a multimodal affective dialogue agent that performs proactive reasoning and dynamic knowledge grounding to sustain emotionally aligned and persuasive interactions. AffectMind combines three components: a Proactive Knowledge Grounding Network (PKGN) that continuously updates factual and affective context from text, vision, and prosody; an Emotion--Intent Alignment Model (EIAM) that jointly models user emotion and purchase intent to adapt persuasion strategies; and a Reinforced Discourse Loop (RDL) that optimizes emotional coherence and engagement via reinforcement signals from user responses. Experiments on two newly curated marketing dialogue datasets, MM-ConvMarket and AffectPromo, show that AffectMind outperforms strong LLM-based baselines in emotional consistency (+26\%), persuasive success rate (+19\%), and long-term user engagement (+23\%), highlighting emotion-grounded proactivity as a key capability for commercial multimodal agents.

</details>


### [25] [Beyond Component Strength: Synergistic Integration and Adaptive Calibration in Multi-Agent RAG Systems](https://arxiv.org/abs/2511.21729)
*Jithin Krishnan*

Main category: cs.CL

TL;DR: 可靠的检索增强生成(RAG)系统不仅仅是添加强大的组件，更需要理解它们如何交互。通过对50个查询(15个可回答的，10个边缘情况，25个对抗性的)的烧蚀研究，表明混合检索、集成验证和自适应阈值等增强功能在单独使用时几乎没有好处，但结合在一起可以将弃权率降低95%(从40%降至2%)，而不会增加幻觉。


<details>
  <summary>Details</summary>
Motivation: 构建可靠的检索增强生成(RAG)系统。

Method: 使用对50个查询的烧蚀研究，包括15个可回答的查询，10个边缘情况查询和25个对抗性查询。

Result: 混合检索、集成验证和自适应阈值等增强功能在单独使用时几乎没有好处，但结合在一起可以将弃权率降低95%(从40%降至2%)，而不会增加幻觉。不同的验证策略可以安全地运行，但会分配不一致的标签(例如，“弃权”与“不支持”)，从而产生表面上的幻觉率，而这些幻觉率实际上是标签的人为产物。

Conclusion: 协同集成比任何单个组件的强度更重要，标准化指标和标签对于正确解释性能至关重要，并且需要自适应校准以防止过度自信的过度回答，即使在检索质量很高时也是如此。

Abstract: Building reliable retrieval-augmented generation (RAG) systems requires more than adding powerful components; it requires understanding how they interact. Using ablation studies on 50 queries (15 answerable, 10 edge cases, and 25 adversarial), we show that enhancements such as hybrid retrieval, ensemble verification, and adaptive thresholding provide almost no benefit when used in isolation, yet together achieve a 95% reduction in abstention (from 40% to 2%) without increasing hallucinations. We also identify a measurement challenge: different verification strategies can behave safely but assign inconsistent labels (for example, "abstained" versus "unsupported"), creating apparent hallucination rates that are actually artifacts of labeling. Our results show that synergistic integration matters more than the strength of any single component, that standardized metrics and labels are essential for correctly interpreting performance, and that adaptive calibration is needed to prevent overconfident over-answering even when retrieval quality is high.

</details>


### [26] [A Benchmark for Procedural Memory Retrieval in Language Agents](https://arxiv.org/abs/2511.21730)
*Ishant Kohar,Aswanth Krishnan*

Main category: cs.CL

TL;DR: 这篇论文提出了一个用于评估AI智能体在不同词汇环境中识别功能等效程序能力的新基准。该基准旨在区分程序记忆检索和任务执行，并诊断智能体泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体在熟悉环境中表现出色，但在面对具有未见过词汇的新任务时会 резко 失败，这是程序记忆系统的核心限制。

Method: 使用ALFWorld，构建专家和LLM生成的轨迹的双语料库，并使用系统分层查询评估六种检索方法。

Result: 结果表明，基于嵌入的方法在熟悉的环境中表现良好，但在新的环境中表现明显下降，而LLM生成的程序抽象则表现出可靠的跨环境迁移。消融实验表明，虽然嵌入捕获了一些词汇级别的抽象，但它们从根本上将程序视为无序的词袋，丢弃了跨环境迁移所需的时间结构。语料库规模比表示丰富带来更大的收益，揭示了当前编码器中的架构上限。

Conclusion: 该基准提供了一个诊断框架，将真正的程序理解与表面级别的记忆区分开来，并为开发能够可靠泛化的检索系统提供了工具。

Abstract: Current AI agents excel in familiar settings, but fail sharply when faced with novel tasks with unseen vocabularies -- a core limitation of procedural memory systems. We present the first benchmark that isolates procedural memory retrieval from task execution, evaluating whether agents can recognize functionally equivalent procedures that span different object instantiations. Using ALFWorld, we construct dual corpora of expert and LLM-generated trajectories and evaluate six retrieval methods using systematically stratified queries. Our results expose a clear generalization cliff: embedding-based methods perform strongly on familiar contexts, yet degrade considerably on novel ones, while LLM-generated procedural abstractions demonstrate reliable cross-context transfer. Controlled ablations show that although embeddings capture some lexical-level abstraction, they fundamentally treat procedures as unordered bags of words, discarding temporal structure necessary for cross-context transfer. Corpus scale delivers far larger gains than representation enrichment, revealing an architectural ceiling in current encoders. Our benchmark offers the first diagnostic framework separating genuine procedural understanding from surface-level memorization and gives tools for developing retrieval systems capable of dependable generalization. Resources available at our GitHub repository (https://github.com/qpiai/Proced_mem_bench).

</details>


### [27] [Identifying Quantum Structure in AI Language: Evidence for Evolutionary Convergence of Human and Artificial Cognition](https://arxiv.org/abs/2511.21731)
*Diederik Aerts,Jonito Aerts Arguëlles,Lester Beltran,Suzette Geriente,Roberto Leporini,Massimiliano Sassoli de Bianchi,Sandro Sozzo*

Main category: cs.CL

TL;DR: 大型语言模型(llm)在概念组合的认知测试中表现出量子纠缠和玻色-爱因斯坦统计，与人类认知和信息检索的结果相似。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在概念组合中是否表现出类似人类的认知模式。

Method: 使用ChatGPT和Gemini等大型语言模型作为测试对象，进行概念组合的认知测试。

Result: 在测试中，观察到贝尔不等式被显著违反，表明存在“量子纠缠”；文本中词汇的分布呈现“玻色-爱因斯坦统计”，而非直观预期的“麦克斯韦-玻尔兹曼统计”。

Conclusion: 人类和人工智能认知代理在概念-语言领域系统性地涌现量子结构，这源于向量空间中分布式语义结构的知识组织形式，促成了人类和llm在认知和语言上的趋同。

Abstract: We present the results of cognitive tests on conceptual combinations, performed using specific Large Language Models (LLMs) as test subjects. In the first test, performed with ChatGPT and Gemini, we show that Bell's inequalities are significantly violated, which indicates the presence of 'quantum entanglement' in the tested concepts. In the second test, also performed using ChatGPT and Gemini, we instead identify the presence of 'Bose-Einstein statistics', rather than the intuitively expected 'Maxwell-Boltzmann statistics', in the distribution of the words contained in large-size texts. Interestingly, these findings mirror the results previously obtained in both cognitive tests with human participants and information retrieval tests on large corpora. Taken together, they point to the 'systematic emergence of quantum structures in conceptual-linguistic domains', regardless of whether the cognitive agent is human or artificial. Although LLMs are classified as neural networks for historical reasons, we believe that a more essential form of knowledge organization takes place in the distributive semantic structure of vector spaces built on top of the neural network. It is this meaning-bearing structure that lends itself to a phenomenon of evolutionary convergence between human cognition and language, slowly established through biological evolution, and LLM cognition and language, emerging much more rapidly as a result of self-learning and training. We analyze various aspects and examples that contain evidence supporting the above hypothesis. We also advance a unifying framework that explains the pervasive quantum organization of meaning that we identify.

</details>


### [28] [HUMORCHAIN: Theory-Guided Multi-Stage Reasoning for Interpretable Multimodal Humor Generation](https://arxiv.org/abs/2511.21732)
*Jiajun Zhang,Shijia Luo,Ruikang Zhang,Qi Su*

Main category: cs.CL

TL;DR: 本文提出了一种名为 HUMORCHAIN 的理论驱动的多阶段推理框架，用于生成多模态幽默内容。


<details>
  <summary>Details</summary>
Motivation: 现有的数据驱动方法缺乏对幽默的显式建模或理论基础，通常产生字面描述，无法捕捉其潜在的认知机制。

Method: 该框架集成了视觉语义解析、基于幽默和心理学的推理以及用于幽默评估的微调判别器，形成了一个可解释和可控的认知推理链。

Result: 在 Meme-Image-No-Text、Oogiri-GO 和 OxfordTVG-HIC 数据集上的实验表明，HUMORCHAIN 在人类幽默偏好、Elo/BT 分数和语义多样性方面优于最先进的基线。

Conclusion: 理论驱动的结构化推理使大型语言模型能够生成符合人类感知的幽默。

Abstract: Humor, as both a creative human activity and a social binding mechanism, has long posed a major challenge for AI generation. Although producing humor requires complex cognitive reasoning and social understanding, theories of humor suggest that it follows learnable patterns and structures, making it theoretically possible for generative models to acquire them implicitly. In recent years, multimodal humor has become a prevalent form of online communication, especially among Gen Z, highlighting the need for AI systems capable of integrating visual understanding with humorous language generation. However, existing data-driven approaches lack explicit modeling or theoretical grounding of humor, often producing literal descriptions that fail to capture its underlying cognitive mechanisms, resulting in the generated image descriptions that are fluent but lack genuine humor or cognitive depth. To address this limitation, we propose HUMORCHAIN (HUmor-guided Multi-step Orchestrated Reasoning Chain for Image Captioning), a theory-guided multi-stage reasoning framework. It integrates visual semantic parsing, humor- and psychology-based reasoning, and a fine-tuned discriminator for humor evaluation, forming an interpretable and controllable cognitive reasoning chain. To the best of our knowledge, this is the first work to explicitly embed cognitive structures from humor theories into multimodal humor generation, enabling a structured reasoning process from visual understanding to humor creation. Experiments on Meme-Image-No-Text, Oogiri-GO, and OxfordTVG-HIC datasets show that HUMORCHAIN outperforms state-of-the-art baselines in human humor preference, Elo/BT scores, and semantic diversity, demonstrating that theory-driven structured reasoning enables large language models to generate humor aligned with human perception.

</details>


### [29] [RoSA: Enhancing Parameter-Efficient Fine-Tuning via RoPE-aware Selective Adaptation in Large Language Models](https://arxiv.org/abs/2511.21733)
*Dayan Pan,Jingyuan Wang,Yilong Zhou,Jiawei Cheng,Pengyue Jia,Xiangyu Zhao*

Main category: cs.CL

TL;DR: RoSA focuses on efficient fine-tuning of large language models by considering the roles of model components and layer importance.


<details>
  <summary>Details</summary>
Motivation: Current PEFT methods don't account for different roles of model components and heterogeneous importance across layers, limiting adaptation efficiency.

Method: RoSA includes RoPE-aware Attention Enhancement (RoAE) to enhance low-frequency components and Dynamic Layer Selection (DLS) to update critical layers.

Result: RoSA outperforms existing PEFT methods on fifteen benchmarks with comparable trainable parameters.

Conclusion: RoSA achieves targeted and efficient fine-tuning by combining dimension-wise enhancement and layer-wise adaptation.

Abstract: Fine-tuning large language models is essential for task-specific adaptation, yet it remains computationally prohibitive. Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a solution, but current approaches typically ignore the distinct roles of model components and the heterogeneous importance across layers, thereby limiting adaptation efficiency. Motivated by the observation that Rotary Position Embeddings (RoPE) induce critical activations in the low-frequency dimensions of attention states, we propose RoPE-aware Selective Adaptation (RoSA), a novel PEFT framework that allocates trainable parameters in a more targeted and effective manner. RoSA comprises a RoPE-aware Attention Enhancement (RoAE) module, which selectively enhances the low-frequency components of RoPE-influenced attention states, and a Dynamic Layer Selection (DLS) strategy that adaptively identifies and updates the most critical layers based on LayerNorm gradient norms. By combining dimension-wise enhancement with layer-wise adaptation, RoSA achieves more targeted and efficient fine-tuning. Extensive experiments on fifteen commonsense and arithmetic benchmarks demonstrate that RoSA outperforms existing mainstream PEFT methods under comparable trainable parameters. The code is available to ease reproducibility at https://github.com/Applied-Machine-Learning-Lab/RoSA.

</details>


### [30] [Asking LLMs to Verify First is Almost Free Lunch](https://arxiv.org/abs/2511.21734)
*Shiguang Wu,Quanming Yao*

Main category: cs.CL

TL;DR: 提出了Verification-First (VF)策略，通过先验证候选答案再生成解决方案来增强LLM的推理能力，降低训练成本和测试时间采样需求。


<details>
  <summary>Details</summary>
Motivation: 为了在不进行高成本训练或大量测试时间采样的情况下，增强大型语言模型的推理能力。

Method: VF策略提示模型验证一个候选答案，即使是简单的或随机的答案，然后再生成解决方案。Iter-VF是一种顺序测试时间缩放方法，它使用模型的先前答案迭代循环验证-生成过程。

Result: 在各种基准测试（从数学推理到编码和代理任务）和各种LLM（从开源1B到尖端商业模型）上的大量实验表明，具有随机答案的VF始终优于标准CoT，且计算开销最小，并且Iter-VF优于现有的TTS策略。

Conclusion: VF策略是一种有效的提高LLM推理能力的方法，具有较低的计算成本。

Abstract: To enhance the reasoning capabilities of Large Language Models (LLMs) without high costs of training, nor extensive test-time sampling, we introduce Verification-First (VF), a strategy that prompts models to verify a provided candidate answer, even a trivial or random one, before generating a solution. This approach triggers a "reverse reasoning" process that is cognitively easier and complementary to standard forward Chain-of-Thought (CoT), effectively invoking the model's critical thinking to reduce logical errors. We further generalize the VF strategy to Iter-VF, a sequential test-time scaling (TTS) method that iteratively cycles the verification-generation process using the model's previous answer. Extensive experiments across various benchmarks (from mathematical reasoning to coding and agentic tasks) and various LLMs (from open-source 1B to cutting-edge commercial ones) confirm that VF with random answer consistently outperforms standard CoT with minimal computational overhead, and Iter-VF outperforms existing TTS strategies.

</details>


### [31] [Closing the Performance Gap Between AI and Radiologists in Chest X-Ray Reporting](https://arxiv.org/abs/2511.21735)
*Harshita Sharma,Maxwell C. Reynolds,Valentina Salvatelli,Anne-Marie G. Sykes,Kelly K. Horst,Anton Schwaighofer,Maximilian Ilse,Olesya Melnichenko,Sam Bond-Taylor,Fernando Pérez-García,Vamshi K. Mugu,Alex Chan,Ceylan Colak,Shelby A. Swartz,Motassem B. Nashawaty,Austin J. Gonzalez,Heather A. Ouellette,Selnur B. Erdal,Beth A. Schueler,Maria T. Wetscherek,Noel Codella,Mohit Jain,Shruthi Bannur,Kenza Bouzid,Daniel C. Castro,Stephanie Hyland,Panos Korfiatis,Ashish Khandelwal,Javier Alvarez-Valle*

Main category: cs.CL

TL;DR: MAIRA-X是一个用于生成胸部X光报告的多模态AI模型，可以减少放射科医生的工作量，同时保持诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 放射科医生需要花费大量精力阅片，解读胸部X光片中的管线位置和临床发现，而AI辅助报告生成可以解决这个问题。

Method: 使用来自Mayo Clinic的大规模、多站点、纵向数据集（310万项研究，包含来自80.6万名患者的600万张图像）开发了MAIRA-X，并在三个保留数据集和公共MIMIC-CXR数据集上进行了评估。还开发了一个新的L&T特定指标框架，用于评估报告属性的准确性。

Result: MAIRA-X在词汇质量、临床正确性和L&T相关要素方面显著改进了AI生成的报告。用户研究表明，AI生成的报告与原始报告具有可比的关键错误率和可接受句子率。

Conclusion: MAIRA-X可以有效地协助放射科医生，尤其是在高容量临床环境中。

Abstract: AI-assisted report generation offers the opportunity to reduce radiologists' workload stemming from expanded screening guidelines, complex cases and workforce shortages, while maintaining diagnostic accuracy. In addition to describing pathological findings in chest X-ray reports, interpreting lines and tubes (L&T) is demanding and repetitive for radiologists, especially with high patient volumes. We introduce MAIRA-X, a clinically evaluated multimodal AI model for longitudinal chest X-ray (CXR) report generation, that encompasses both clinical findings and L&T reporting. Developed using a large-scale, multi-site, longitudinal dataset of 3.1 million studies (comprising 6 million images from 806k patients) from Mayo Clinic, MAIRA-X was evaluated on three holdout datasets and the public MIMIC-CXR dataset, where it significantly improved AI-generated reports over the state of the art on lexical quality, clinical correctness, and L&T-related elements. A novel L&T-specific metrics framework was developed to assess accuracy in reporting attributes such as type, longitudinal change and placement. A first-of-its-kind retrospective user evaluation study was conducted with nine radiologists of varying experience, who blindly reviewed 600 studies from distinct subjects. The user study found comparable rates of critical errors (3.0% for original vs. 4.6% for AI-generated reports) and a similar rate of acceptable sentences (97.8% for original vs. 97.4% for AI-generated reports), marking a significant improvement over prior user studies with larger gaps and higher error rates. Our results suggest that MAIRA-X can effectively assist radiologists, particularly in high-volume clinical settings.

</details>


### [32] [R2Q: Towards Robust 2-Bit Large Language Models via Residual Refinement Quantization](https://arxiv.org/abs/2511.21736)
*Jiayi Chen,Jieqi Shi,Jing Huo,Chen Wu*

Main category: cs.CL

TL;DR: 提出了一种新的2比特量化框架R2Q，将量化过程分解为两个连续的1比特子量化，形成自适应量化 lattice。


<details>
  <summary>Details</summary>
Motivation: 由于严重的精度下降，将量化扩展到2比特仍然具有挑战性。

Method: 提出 Residual Refinement Quantization (R2Q)

Result: R2Q 在各种基准测试中始终优于现有的 2 比特量化方法，涵盖问答、常识推理和语言建模。

Conclusion: 通过残差学习机制改进量化，R2Q 提高了性能，改善了训练稳定性，并在极端压缩下加速了收敛。

Abstract: The rapid progress of Large Language Models (LLMs) has brought substantial computational and memory demands, spurring the adoption of low-bit quantization. While 8-bit and 4-bit formats have become prevalent, extending quantization to 2 bits remains challenging due to severe accuracy degradation. To address this, we propose Residual Refinement Quantization (R2Q)-a novel 2-bit quantization framework that decomposes the process into two sequential 1-bit sub-quantizations, forming an adaptive quantization lattice. Extensive evaluations on Llama, OPT, and Qwen across diverse benchmarks-covering question answering, commonsense reasoning, and language modeling-demonstrate that R2Q consistently outperforms existing 2-bit quantization methods in both fine-grained and coarse-grained settings. By refining quantization through a residual learning mechanism, R2Q enhances performance, improves training stability, and accelerates convergence under extreme compression. Furthermore, its modular design enables seamless integration with existing quantization-aware training (QAT) frameworks.

</details>


### [33] [Polarity-Aware Probing for Quantifying Latent Alignment in Language Models](https://arxiv.org/abs/2511.21737)
*Sabrina Sadiekh,Elena Ericheva,Chirag Agarwal*

Main category: cs.CL

TL;DR: 本文研究了无监督探针方法（如对比一致性搜索CCS）是否可以可靠地评估模型对齐。通过引入极性感知CCS（PA-CCS），评估模型在极性反转下内部表示的一致性。提出了极性一致性和矛盾指数两个指标来量化模型潜在知识的语义鲁棒性。PA-CCS被应用于16个语言模型，结果表明PA-CCS可以识别潜在有害知识编码中的架构和层特异性差异。研究结果强调了无监督探针在对齐评估中的潜力，并强调需要在可解释性基准中加入结构鲁棒性检查。


<details>
  <summary>Details</summary>
Motivation: 研究无监督探针方法是否可以可靠地评估模型对齐，并关注模型在处理有害信息时的内部表示一致性。

Method: 提出了极性感知CCS（PA-CCS）方法，并设计了极性一致性和矛盾指数来评估模型的语义鲁棒性。通过构建包含匹配的有害-安全语句对的数据集，对16个语言模型应用PA-CCS。

Result: PA-CCS能够识别潜在有害知识编码中的架构和层特异性差异。对于良好对齐的模型，用无意义标记替换否定词会降低PA-CCS得分，而缺乏鲁棒内部校准的模型则不会出现这种降级。

Conclusion: 无监督探针具有对齐评估的潜力，需要在可解释性基准中加入结构鲁棒性检查。

Abstract: Advances in unsupervised probes such as Contrast-Consistent Search (CCS), which reveal latent beliefs without relying on token outputs, raise the question of whether these methods can reliably assess model alignment. We investigate this by examining the sensitivity of CCS to harmful vs. safe statements and by introducing Polarity-Aware CCS (PA-CCS), a method for evaluating whether a model's internal representations remain consistent under polarity inversion. We propose two alignment-oriented metrics, Polar-Consistency and the Contradiction Index, to quantify the semantic robustness of a model's latent knowledge. To validate PA-CCS, we curate two main datasets and one control dataset containing matched harmful-safe sentence pairs constructed using different methodologies (concurrent and antagonistic statements). We apply PA-CCS to 16 language models. Our results show that PA-CCS identifies both architectural and layer-specific differences in the encoding of latent harmful knowledge. Notably, replacing the negation token with a meaningless marker degrades PA-CCS scores for models with well-aligned internal representations, while models lacking robust internal calibration do not exhibit this degradation. Our findings highlight the potential of unsupervised probing for alignment evaluation and emphasize the need to incorporate structural robustness checks into interpretability benchmarks. Code and datasets are available at: https://github.com/SadSabrina/polarity-probing. WARNING: This paper contains potentially sensitive, harmful, and offensive content.

</details>


### [34] [Decoding inner speech with an end-to-end brain-to-text neural interface](https://arxiv.org/abs/2511.21740)
*Yizi Zhang,Linyang He,Chaofei Fan,Tingkai Liu,Han Yu,Trung Le,Jingyuan Li,Scott Linderman,Lea Duncker,Francis R Willett,Nima Mesgarani,Liam Paninski*

Main category: cs.CL

TL;DR: 本文介绍了一种端到端的脑机接口框架，它可以将神经活动直接翻译成连贯的句子。


<details>
  <summary>Details</summary>
Motivation: 大多数系统使用级联框架，这些框架在用 n-gram 语言模型组装句子之前解码音素，从而无法同时对所有阶段进行联合优化。本文旨在解决这个问题。

Method: 使用一个可微的神经网络，通过跨任务、跨物种的预训练神经编码器，将神经活动翻译成连贯的句子。使用对比学习进行跨模态对齐，并结合音频大型语言模型 (LLM)。

Result: 在 Brain-to-Text '24 和 '25 基准测试中，预训练编码器建立了新的最先进水平 (SOTA)。集成的端到端音频大型语言模型 (LLM) 将之前的端到端方法的词错误率 (WER) 从 24.69% 降低到 10.22%。

Conclusion: 该方法推进了大型、多样化的神经数据集的集成，为支持无缝、可微优化的端到端解码框架铺平了道路。此外，它还对齐了尝试性和想象性语音嵌入，以实现跨任务泛化。

Abstract: Speech brain-computer interfaces (BCIs) aim to restore communication for people with paralysis by translating neural activity into text. Most systems use cascaded frameworks that decode phonemes before assembling sentences with an n-gram language model (LM), preventing joint optimization of all stages simultaneously. Here, we introduce an end-to-end Brain-to-Text (BIT) framework that translates neural activity into coherent sentences using a single differentiable neural network. Central to our approach is a cross-task, cross-species pretrained neural encoder, whose representations transfer to both attempted and imagined speech. In a cascaded setting with an n-gram LM, the pretrained encoder establishes a new state-of-the-art (SOTA) on the Brain-to-Text '24 and '25 benchmarks. Integrated end-to-end with audio large language models (LLMs) and trained with contrastive learning for cross-modal alignment, BIT reduces the word error rate (WER) of the prior end-to-end method from 24.69% to 10.22%. Notably, we find that small-scale audio LLMs markedly improve end-to-end decoding. Beyond record-setting performance, BIT aligns attempted and imagined speech embeddings to enable cross-task generalization. Altogether, our approach advances the integration of large, diverse neural datasets, paving the way for an end-to-end decoding framework that supports seamless, differentiable optimization.

</details>


### [35] [A Multiscale Geometric Method for Capturing Relational Topic Alignment](https://arxiv.org/abs/2511.21741)
*Conrad D. Hougen,Karl T. Pazdernik,Alfred O. Hero*

Main category: cs.CL

TL;DR: 提出了一种几何方法，该方法集成了多模态文本和共同作者网络数据，使用 Hellinger 距离和 Ward 链接来构建分层主题树状图。


<details>
  <summary>Details</summary>
Motivation: 在科学语料库中，识别代表性不足的利基主题尤为重要。然而，由密集transformer嵌入构建的当代模型倾向于遗漏稀有主题，因此也无法捕获平滑的时间对齐。

Method: 该方法集成了多模态文本和共同作者网络数据，使用 Hellinger 距离和 Ward 链接来构建分层主题树状图。这种方法捕获局部和全局结构，支持跨语义和时间维度的多尺度学习。

Result: 该方法有效地识别了稀有主题结构，并可视化了随时间推移的平滑主题漂移。

Conclusion: 实验强调了可解释的词袋模型在与有原则的几何对齐相结合时的优势。

Abstract: Interpretable topic modeling is essential for tracking how research interests evolve within co-author communities. In scientific corpora, where novelty is prized, identifying underrepresented niche topics is particularly important. However, contemporary models built from dense transformer embeddings tend to miss rare topics and therefore also fail to capture smooth temporal alignment. We propose a geometric method that integrates multimodal text and co-author network data, using Hellinger distances and Ward's linkage to construct a hierarchical topic dendrogram. This approach captures both local and global structure, supporting multiscale learning across semantic and temporal dimensions. Our method effectively identifies rare-topic structure and visualizes smooth topic drift over time. Experiments highlight the strength of interpretable bag-of-words models when paired with principled geometric alignment.

</details>


### [36] [EduMod-LLM: A Modular Approach for Designing Flexible and Transparent Educational Assistants](https://arxiv.org/abs/2511.21742)
*Meenakshi Mittal,Rishi Khare,Mihran Miroyan,Chancharik Mitra,Narges Norouzi*

Main category: cs.CL

TL;DR: 这篇论文介绍了一个模块化的基于大型语言模型（LLM）的问答（QA）系统，并对其各个pipeline组件的性能进行了评估。


<details>
  <summary>Details</summary>
Motivation: 评估基于大型语言模型的问答系统在教育中的应用性能，特别是针对各个pipeline组件。

Method: 该研究提出了一个模块化的函数调用LLM pipeline，并通过隔离和评估每个组件来进行细粒度分析。使用了函数调用策略、检索方法和生成语言模型三个关键轴进行评估。

Result: 研究揭示了特定的失败模式和性能模式，支持开发可解释且有效的教育问答系统。

Conclusion: 模块化函数调用在提高系统透明性和教学一致性方面具有价值。

Abstract: With the growing use of Large Language Model (LLM)-based Question-Answering (QA) systems in education, it is critical to evaluate their performance across individual pipeline components. In this work, we introduce {\model}, a modular function-calling LLM pipeline, and present a comprehensive evaluation along three key axes: function calling strategies, retrieval methods, and generative language models. Our framework enables fine-grained analysis by isolating and assessing each component. We benchmark function-calling performance across LLMs, compare our novel structure-aware retrieval method to vector-based and LLM-scoring baselines, and evaluate various LLMs for response synthesis. This modular approach reveals specific failure modes and performance patterns, supporting the development of interpretable and effective educational QA systems. Our findings demonstrate the value of modular function calling in improving system transparency and pedagogical alignment. Website and Supplementary Material: https://chancharikmitra.github.io/EduMod-LLM-website/

</details>


### [37] [Scaling Competence, Shrinking Reasoning: Cognitive Signatures in Language Model Learning](https://arxiv.org/abs/2511.21743)
*Mukul Singh,Ananya Singha,Arjun Radhakrishna,Sumit Gulwani*

Main category: cs.CL

TL;DR: 研究了语言模型在特定任务微调期间的推理能力，并将推理标记（解决问题时生成的中间步骤）与人类工作记忆进行对比。


<details>
  <summary>Details</summary>
Motivation: 从认知科学出发，将训练动态与能力的四个阶段对齐：模型最初在没有推理的情况下产生不正确的输出，然后开始推理（但仍然失败），最终有效地推理，最后在没有明确推理的情况下解决任务。

Method: 通过实验分析模型在训练过程中推理标记长度的变化。

Result: 推理标记长度随着性能的提高而扩大，在有意识胜任阶段达到峰值，然后随着模型内化任务而下降。即使在训练后移除推理，模型也能保持性能。

Conclusion: 推理标记动态可以作为诊断训练阶段、识别收敛和指导提前停止的信号。推理行为对于理解和优化推理模型训练非常重要。

Abstract: We analyze reasoning in language models during task-specific fine-tuning and draws parallel between reasoning tokens--intermediate steps generated while solving problem and the human working memory. Drawing from cognitive science, we align training dynamics with the Four Stages of Competence: models initially produce incorrect outputs without reasoning, then begin reasoning (but still fail), eventually reason effectively, and finally solve tasks without explicit reasoning. We find that reasoning token length expands as performance improves, peaks at the stage of conscious competence, then declines as the model internalizes the task. Notably, after training, models retain performance even when reasoning is removed--suggesting it scaffolded learning but is no longer needed. This progression offers actionable insights: reasoning token dynamics can serve as a signal for diagnosing training stage, identifying convergence, and guiding early stopping. We propose metrics to track this trajectory and argue that reasoning behavior is valuable for understanding and optimizing reasoning model training.

</details>


### [38] [A Lightweight Approach to Detection of AI-Generated Texts Using Stylometric Features](https://arxiv.org/abs/2511.21744)
*Sergey K. Aityan,William Claster,Karthik Sai Emani,Sohni Rais,Thy Tran*

Main category: cs.CL

TL;DR: 提出了一种名为NEULIF的轻量级AI生成文本检测方法，该方法在保持高检测精度的同时，计算成本不高，并且在轻量级检测器类别中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有的AI生成文本检测方法计算成本高，泛化能力有限，而轻量级替代方案在大型数据集上的准确率较低。

Method: 该方法首先将文本分解为文体和可读性特征，然后使用紧凑的卷积神经网络（CNN）或随机森林（RF）进行分类。

Result: 在Kaggle AI vs. Human语料库上，CNN模型的准确率达到97%（F1约为0.95），随机森林模型的准确率达到95%（F1约为0.94），ROC-AUC分数分别为99.5%和95%。

Conclusion: 该研究强调了这种模型在跨语言、领域和流媒体环境中的广泛应用潜力，表明在结构性见解的指导下，简单性可以与AI生成内容检测中的复杂性相媲美。

Abstract: A growing number of AI-generated texts raise serious concerns. Most existing approaches to AI-generated text detection rely on fine-tuning large transformer models or building ensembles, which are computationally expensive and often provide limited generalization across domains. Existing lightweight alternatives achieved significantly lower accuracy on large datasets. We introduce NEULIF, a lightweight approach that achieves best performance in the lightweight detector class, that does not require extensive computational power and provides high detection accuracy. In our approach, a text is first decomposed into stylometric and readability features which are then used for classification by a compact Convolutional Neural Network (CNN) or Random Forest (RF). Evaluated and tested on the Kaggle AI vs. Human corpus, our models achieve 97% accuracy (~ 0.95 F1) for CNN and 95% accuracy (~ 0.94 F1) for the Random Forest, demonstrating high precision and recall, with ROC-AUC scores of 99.5% and 95%, respectively. The CNN (~ 25 MB) and Random Forest (~ 10.6 MB) models are orders of magnitude smaller than transformer-based ensembles and can be run efficiently on standard CPU devices, without sacrificing accuracy.This study also highlights the potential of such models for broader applications across languages, domains, and streaming contexts, showing that simplicity, when guided by structural insights, can rival complexity in AI-generated content detection.

</details>


### [39] [DELTA: Language Diffusion-based EEG-to-Text Architecture](https://arxiv.org/abs/2511.21746)
*Mingyu Jeon,Hyobin Kim*

Main category: cs.CL

TL;DR: 这篇论文提出了一种名为DELTA的新方法，用于将脑电图（EEG）转换为文本，通过结合残差向量量化（RVQ）EEG分词器和掩码语言扩散模型（LLaDA），在ZuCo数据集上实现了比自回归基线更好的语义对齐效果。


<details>
  <summary>Details</summary>
Motivation: 脑电图（EEG）到文本的转换具有挑战性，因为存在高维噪声、个体差异以及自回归解码中的误差累积。

Method: 该方法结合了残差向量量化（RVQ）EEG分词器和掩码语言扩散模型（LLaDA）。RVQ将连续的EEG离散化为多层tokens，以减少噪声和个体差异，而LLaDA通过非序列去噪重建句子。

Result: 在ZuCo数据集上，DELTA在词级条件下，语义对齐比自回归基线提高了5.37个点，BLEU-1达到21.9，ROUGE-1 F1达到17.2。

Conclusion: 这些结果表明，该方法能够从小型的脑电图-文本数据集中可靠地生成文本，并为可扩展的多模态脑电图-语言模型指明了方向。

Abstract: Electroencephalogram (EEG)-to-text remains challenging due to high-dimensional noise, subject variability, and error accumulation in autoregressive decoding. We introduce DELTA, which pairs a Residual Vector Quantization (RVQ) EEG tokenizer with a masked language diffusion model (LLaDA). RVQ discretizes continuous EEG into multi-layer tokens to reduce noise and individual differences, while LLaDA reconstructs sentences via non-sequential denoising. On ZuCo, DELTA improves semantic alignment by up to 5.37 points over autoregressive baselines, achieving BLEU-1 21.9 and ROUGE-1 F 17.2 under word-level conditions. These results enable reliable text generation from small EEG-text datasets and point toward scalable multimodal EEG-language models.

</details>


### [40] [Building Domain-Specific Small Language Models via Guided Data Generation](https://arxiv.org/abs/2511.21748)
*Aman Kumar,Ekant Muljibhai Amin,Xian Yeow Lee,Lasitha Vidyaratne,Ahmed K. Farahat,Dipanjan D. Ghosh,Yuta Koreeda,Chetan Gupta*

Main category: cs.CL

TL;DR: 本研究提出了一种经济高效且可扩展的训练流程，用于开发小型、特定领域的LLM。


<details>
  <summary>Details</summary>
Motivation: 在专业领域，利用LLM协助领域专家解决特定领域挑战的需求日益增长。然而，LLM的部署面临数据隐私问题和开源模型对计算资源的需求。

Method: 该流程结合了领域自适应预训练（DAPT）、领域特定监督微调（DSFT）和直接偏好优化（DPO）。

Result: DiagnosticSLM在多项特定领域benchmark上，相比同等或更大规模的开源模型，取得了高达25%的准确率提升。

Conclusion: 该方法能够有效提升模型在特定领域的推理和泛化能力。

Abstract: Large Language Models (LLMs) have shown remarkable success in supporting a wide range of knowledge-intensive tasks. In specialized domains, there is growing interest in leveraging LLMs to assist subject matter experts with domain-specific challenges. However, deploying LLMs as SaaS solutions raises data privacy concerns, while many open-source models demand significant computational resources for effective domain adaptation and deployment. A promising alternative is to develop smaller, domain-specialized LLMs, though this approach is often constrained by the lack of high-quality domain-specific training data. In this work, we address these limitations by presenting a cost-efficient and scalable training pipeline that combines guided synthetic data generation from a small seed corpus with bottom-up domain data curation. Our pipeline integrates Domain-Adaptive Pretraining (DAPT), Domain-specific Supervised Fine-tuning (DSFT), and Direct Preference Optimization (DPO) to train effective small-scale models for specialized use cases. We demonstrate this approach through DiagnosticSLM, a 3B-parameter domain-specific model tailored for fault diagnosis, root cause analysis, and repair recommendation in industrial settings. To evaluate model performance, we introduce four domain-specific benchmarks: multiple-choice questions (DiagnosticMCQ), question answering (DiagnosticQA), sentence completion (DiagnosticComp), and summarization (DiagnosticSum). DiagnosticSLM achieves up to 25% accuracy improvement over open-source models of comparable or larger size (2B-9B) on the MCQ task, while also outperforming or matching them in other tasks, demonstrating effective domain-specific reasoning and generalization capabilities.

</details>


### [41] [Proactive Defense: Compound AI for Detecting Persuasion Attacks and Measuring Inoculation Effectiveness](https://arxiv.org/abs/2511.21749)
*Svitlana Volkova,Will Dupree,Hsien-Te Kao,Peter Bautista,Gabe Ganberg,Jeff Beaubien,Laura Cassani*

Main category: cs.CL

TL;DR: BRIES是一种新型复合AI架构，旨在检测和衡量信息环境中说服攻击的有效性。


<details>
  <summary>Details</summary>
Motivation: 量化大型语言模型（LLM）在说服攻击中的特定漏洞，并提供一个框架，通过结构化干预来增强人类认知弹性，使其在接触有害内容之前做好准备。

Method: 该系统包含：Twister（生成对抗性内容）、Detector（识别攻击类型）、Defender（通过内容注入创建弹性内容）和Assessor（评估注入效果）。

Result: 不同语言智能体在检测性能上存在显著差异。GPT-4在复杂说服技巧上的检测准确率更高，而Llama3和Mistral等开源模型在识别微妙的修辞手法方面表现出明显的不足。提示工程会显著影响检测效果。Gemma和GPT-4在较低温度下表现最佳，而Llama3和Mistral在较高温度下表现出更高的能力。不同的攻击类型针对特定的认知维度。

Conclusion: 该研究通过量化LLM在说服攻击中的特定漏洞，并提供了一个框架，通过结构化干预来增强人类认知弹性，从而推进了生成式人工智能安全和认知安全。

Abstract: This paper introduces BRIES, a novel compound AI architecture designed to detect and measure the effectiveness of persuasion attacks across information environments. We present a system with specialized agents: a Twister that generates adversarial content employing targeted persuasion tactics, a Detector that identifies attack types with configurable parameters, a Defender that creates resilient content through content inoculation, and an Assessor that employs causal inference to evaluate inoculation effectiveness. Experimenting with the SemEval 2023 Task 3 taxonomy across the synthetic persuasion dataset, we demonstrate significant variations in detection performance across language agents. Our comparative analysis reveals significant performance disparities with GPT-4 achieving superior detection accuracy on complex persuasion techniques, while open-source models like Llama3 and Mistral demonstrated notable weaknesses in identifying subtle rhetorical, suggesting that different architectures encode and process persuasive language patterns in fundamentally different ways. We show that prompt engineering dramatically affects detection efficacy, with temperature settings and confidence scoring producing model-specific variations; Gemma and GPT-4 perform optimally at lower temperatures while Llama3 and Mistral show improved capabilities at higher temperatures. Our causal analysis provides novel insights into socio-emotional-cognitive signatures of persuasion attacks, revealing that different attack types target specific cognitive dimensions. This research advances generative AI safety and cognitive security by quantifying LLM-specific vulnerabilities to persuasion attacks and delivers a framework for enhancing human cognitive resilience through structured interventions before exposure to harmful content.

</details>


### [42] [Semantics as a Shield: Label Disguise Defense (LDD) against Prompt Injection in LLM Sentiment Classification](https://arxiv.org/abs/2511.21752)
*Yanxi Li,Ruocheng Shan*

Main category: cs.CL

TL;DR: 这篇论文提出了一种名为标签伪装防御（LDD）的轻量级、模型无关的策略，通过用语义转换或不相关的别名标签替换真实标签来隐藏真实标签，以防御提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在文本分类任务中越来越多地被使用，但它们对自然语言提示的依赖使它们容易受到提示注入攻击。现有的防御方法要么需要模型重新训练，要么仍然容易受到混淆。

Method: 该方法通过少量演示使模型隐式地学习这些新的标签映射，从而防止注入指令和决策输出之间的直接对应。

Result: 实验结果表明，LDD 恢复因对抗攻击而损失的性能的能力因模型和别名选择而异。对于评估的每个模型，LDD 都能够恢复部分由攻击引起的准确性下降。此外，对于绝大多数模型，我们可以识别出多个别名对，这些别名对实现了比受攻击基线更高的准确性，在受攻击基线中，模型仅依赖于少量学习而没有任何防御机制。

Conclusion: 这项研究表明，标签语义可以作为一种有效的防御层，将意义本身转化为对抗提示注入的盾牌。

Abstract: Large language models are increasingly used for text classification tasks such as sentiment analysis, yet their reliance on natural language prompts exposes them to prompt injection attacks. In particular, class-directive injections exploit knowledge of the model's label set (e.g., positive vs. negative) to override its intended behavior through adversarial instructions. Existing defenses, such as detection-based filters, instruction hierarchies, and signed prompts, either require model retraining or remain vulnerable to obfuscation. This paper introduces Label Disguise Defense (LDD), a lightweight and model-agnostic strategy that conceals true labels by replacing them with semantically transformed or unrelated alias labels(e.g., blue vs. yellow). The model learns these new label mappings implicitly through few-shot demonstrations, preventing direct correspondence between injected directives and decision outputs. We evaluate LDD across nine state-of-the-art models, including GPT-5, GPT-4o, LLaMA3.2, Gemma3, and Mistral variants, under varying few-shot and an adversarial setting. Our results show that the ability of LDD to recover performance lost to the adversarial attack varies across models and alias choices. For every model evaluated, LDD is able to restore a portion of the accuracy degradation caused by the attack. Moreover, for the vast majority of models, we can identify more than one alias pair that achieves higher accuracy than the under-attack baseline, in which the model relies solely on few-shot learning without any defensive mechanism. A linguistic analysis further reveals that semantically aligned alias labels(e.g., good vs. bad) yield stronger robustness than unaligned symbols(e.g., blue vs. yellow). Overall, this study demonstrates that label semantics can serve as an effective defense layer, transforming meaning itself into a shield against prompt injection.

</details>


### [43] [Extracting Disaster Impacts and Impact Related Locations in Social Media Posts Using Large Language Models](https://arxiv.org/abs/2511.21753)
*Sameeah Noreen Hameed,Surangika Ranathunga,Raj Prasanna,Kristin Stock,Christopher B. Jones*

Main category: cs.CL

TL;DR: 这篇论文提出了一种利用大型语言模型（LLMs）从灾害相关的社交媒体帖子中识别受灾地点的方法。


<details>
  <summary>Details</summary>
Motivation: 大型灾害发生后，由于各种限制，很难及时获取灾害影响的地理信息。社交媒体可以作为“地理传感器”，但并非所有提及的地点都与灾害影响相关。

Method: 通过对LLMs进行微调，识别社交媒体帖子中的地点、影响和受影响的地点。

Result: 微调后的模型在识别影响和受影响地点方面表现出色，F1-score 分别为 0.69 和 0.74，显著优于基线模型。

Conclusion: 该研究表明，微调后的语言模型有潜力为灾害响应者在资源分配、态势感知和灾后恢复规划方面提供可扩展的解决方案。

Abstract: Large-scale disasters can often result in catastrophic consequences on people and infrastructure. Situation awareness about such disaster impacts generated by authoritative data from in-situ sensors, remote sensing imagery, and/or geographic data is often limited due to atmospheric opacity, satellite revisits, and time limitations. This often results in geo-temporal information gaps. In contrast, impact-related social media posts can act as "geo-sensors" during a disaster, where people describe specific impacts and locations. However, not all locations mentioned in disaster-related social media posts relate to an impact. Only the impacted locations are critical for directing resources effectively. e.g., "The death toll from a fire which ripped through the Greek coastal town of #Mati stood at 80, with dozens of people unaccounted for as forensic experts tried to identify victims who were burned alive #Greecefires #AthensFires #Athens #Greece." contains impacted location "Mati" and non-impacted locations "Greece" and "Athens". This research uses Large Language Models (LLMs) to identify all locations, impacts and impacted locations mentioned in disaster-related social media posts. In the process, LLMs are fine-tuned to identify only impacts and impacted locations (as distinct from other, non-impacted locations), including locations mentioned in informal expressions, abbreviations, and short forms. Our fine-tuned model demonstrates efficacy, achieving an F1-score of 0.69 for impact and 0.74 for impacted location extraction, substantially outperforming the pre-trained baseline. These robust results confirm the potential of fine-tuned language models to offer a scalable solution for timely decision-making in resource allocation, situational awareness, and post-disaster recovery planning for responders.

</details>


### [44] [Dissecting the Ledger: Locating and Suppressing "Liar Circuits" in Financial Large Language Models](https://arxiv.org/abs/2511.21756)
*Soham Mirajkar*

Main category: cs.CL

TL;DR: LLMs在金融领域进行算术运算时会出现幻觉问题，并且可以通过特定方式重现。本文提出了一种机制方法来进行内在幻觉检测。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地应用于高风险金融领域，但在执行算术运算时，它们会产生特定的、可重现的幻觉。

Method: 通过将因果追踪应用于GPT-2 XL架构的ConvFinQA基准测试，我们识别出一种用于算术推理的双阶段机制：中间层（L12-L30）中的分布式计算草稿区和后期层（特别是第46层）中的决定性聚合电路。

Result: 消融研究表明，抑制第46层可将模型对幻觉输出的置信度降低81.8%。

Conclusion: 在这一层训练的线性探针可以推广到未见过的金融主题，准确率达到98%，这表明算术欺骗具有普遍的几何结构。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes financial domains, yet they suffer from specific, reproducible hallucinations when performing arithmetic operations. Current mitigation strategies often treat the model as a black box. In this work, we propose a mechanistic approach to intrinsic hallucination detection. By applying Causal Tracing to the GPT-2 XL architecture on the ConvFinQA benchmark, we identify a dual-stage mechanism for arithmetic reasoning: a distributed computational scratchpad in middle layers (L12-L30) and a decisive aggregation circuit in late layers (specifically Layer 46). We verify this mechanism via an ablation study, demonstrating that suppressing Layer 46 reduces the model's confidence in hallucinatory outputs by 81.8%. Furthermore, we demonstrate that a linear probe trained on this layer generalizes to unseen financial topics with 98% accuracy, suggesting a universal geometry of arithmetic deception.

</details>


### [45] [Orchestrating Dual-Boundaries: An Arithmetic Intensity Inspired Acceleration Framework for Diffusion Language Models](https://arxiv.org/abs/2511.21759)
*Linye Wei,Wenjue Chen,Pingzhi Tang,Xiaotian Guo,Le Ye,Runsheng Wang,Meng Li*

Main category: cs.CL

TL;DR: 本文提出了一种名为ODB-dLLM的框架，通过协调双重边界来加速dLLM的推理。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的大型语言模型(dLLMs)由于其卓越的性能和并行解码的内在潜力而受到了广泛的关注。然而，其双向注意机制需要周期性的缓存刷新，这增加了推理成本并限制了可实现的速度。

Method: ODB-dLLM包含一个自适应长度预测机制，以逐步减少预填充开销和不必要的计算。在解码阶段，我们分析了dLLMs的计算特性，并提出了一种dLLM特定的jump-share推测解码方法，通过减少解码迭代次数来提高效率。

Result: 实验结果表明，ODB-dLLM比基线dLLM和Fast-dLLM分别实现了46-162倍和2.63-6.30倍的加速。

Conclusion: ODB-dLLM在加速的同时，减轻了现有加速框架中的精度下降问题

Abstract: Diffusion-based large language models (dLLMs) have recently gained significant attention for their exceptional performance and inherent potential for parallel decoding. Existing frameworks further enhance its inference efficiency by enabling KV caching. However, its bidirectional attention mechanism necessitates periodic cache refreshes that interleave prefill and decoding phases, both contributing substantial inference cost and constraining achievable speedup. Inspired by the heterogeneous arithmetic intensity of the prefill and decoding phases, we propose ODB-dLLM, a framework that orchestrates dual-boundaries to accelerate dLLM inference. In the prefill phase, we find that the predefined fixed response length introduces heavy yet redundant computational overhead, which affects efficiency. To alleviate this, ODB-dLLM incorporates an adaptive length prediction mechanism that progressively reduces prefill overhead and unnecessary computation. In the decoding phase, we analyze the computational characteristics of dLLMs and propose a dLLM-specific jump-share speculative decoding method to enhance efficiency by reducing the number of decoding iterations. Experimental results demonstrate that ODB-dLLM achieves 46-162x and 2.63-6.30x speedups over the baseline dLLM and Fast-dLLM, respectively, while simultaneously mitigating the accuracy degradation in existing acceleration frameworks.

</details>


### [46] [Temporal Consistency for LLM Reasoning Process Error Identification](https://arxiv.org/abs/2503.14495)
*Jiacheng Guo,Yue Wu,Jiahao Qiu,Kaixuan Huang,Xinzhe Juan,Ling Yang,Mengdi Wang*

Main category: cs.CL

TL;DR: 提出了一种新的时间一致性验证方法，通过迭代细化判断来提高数学推理的验证准确性。


<details>
  <summary>Details</summary>
Motivation: 验证对于有效的数学推理至关重要。

Method: 利用一系列自我反思行为中的一致性来改进验证准确性。

Result: 在各种数学过程错误识别基准测试中，相对于基线方法，性能持续提高。应用于 DeepSeek R1 提炼模型时，该方法表现出强大的性能，使得 7B/8B 提炼模型在 ProcessBench 上的表现优于所有 70B/72B 模型和 GPT-4o。

Conclusion: 该方法在数学推理验证方面表现出强大的性能，具有很大的潜力。

Abstract: Verification is crucial for effective mathematical reasoning. We present a new temporal consistency method where verifiers iteratively refine their judgments based on the previous assessment. Unlike one-round verification or multi-model debate approaches, our method leverages consistency in a sequence of self-reflection actions to improve verification accuracy. Empirical evaluations across diverse mathematical process error identification benchmarks (Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements over baseline methods. When applied to the recent DeepSeek R1 distilled models, our method demonstrates strong performance, enabling 7B/8B distilled models to outperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the distilled 14B model with our method achieves performance comparable to Deepseek-R1. Our codes are available at https://github.com/jcguo123/Temporal-Consistency

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [47] [SO-Bench: A Structural Output Evaluation of Multimodal LLMs](https://arxiv.org/abs/2511.21750)
*Di Feng,Kaixin Ma,Feng Nan,Haofeng Chen,Bohan Zhai,David Griffiths,Mingfei Gao,Zhe Gan,Eshan Verma,Yinfei Yang,Zhifeng Chen,Afshin Dehghan*

Main category: cs.CV

TL;DR: 论文介绍了一个新的用于评估多模态大语言模型在视觉输入下的结构化输出能力的基准测试，称为SO-Bench。


<details>
  <summary>Details</summary>
Motivation: 现有的结构化生成研究主要集中在文本领域，缺乏系统评估 MLLM 在视觉输入下 schema-grounded 信息抽取和推理能力的基准。

Method: 论文构建了 SO-Bench 基准测试，涵盖 UI 界面、自然图像、文档和图表四个视觉领域，包含超过 6.5K 的 JSON schema 和 1.8K 的图像-schema 对，并经过人工验证。

Result: 对开源和前沿模型进行的基准测试实验表明，模型在预测准确、符合 schema 的输出方面仍存在差距。通过进一步的训练实验，模型的结构化输出能力得到显著提高。

Conclusion: 论文强调了提升多模态结构化推理能力的需求，并将向社区开放该基准测试。

Abstract: Multimodal large language models (MLLMs) are increasingly deployed in real-world, agentic settings where outputs must not only be correct, but also conform to predefined data schemas. Despite recent progress in structured generation in textual domain, there is still no benchmark that systematically evaluates schema-grounded information extraction and reasoning over visual inputs. In this work, we conduct a comprehensive study of visual structural output capabilities for MLLMs with our carefully designed SO-Bench benchmark. Covering four visual domains, including UI screens, natural images, documents, and charts, SO-Bench is built from over 6.5K diverse JSON schemas and 1.8K curated image-schema pairs with human-verified quality. Benchmarking experiments on open-sourced and frontier proprietary models reveal persistent gaps in predicting accurate, schema compliant outputs, highlighting the need for better multimodal structured reasoning. Beyond benchmarking, we further conduct training experiments to largely improve the model's structured output capability. We plan to make the benchmark available to the community.

</details>


### [48] [Saddle-Free Guidance: Improved On-Manifold Sampling without Labels or Additional Training](https://arxiv.org/abs/2511.21863)
*Eric Yeats,Darryl Hannan,Wilson Fearn,Timothy Doster,Henry Kvinge,Scott Mahan*

Main category: cs.CV

TL;DR: 提出了一种新的score-based模型引导方法，称为saddle-free guidance (SFG)，它利用对数密度估计的正曲率来引导模型，无需额外训练或标签数据。


<details>
  <summary>Details</summary>
Motivation: 现有的引导方法如Classifier-Free Guidance (CFG)需要标签数据或额外训练模型，限制了其应用场景。

Method: 通过估计对数密度的最大正曲率来引导score-based模型。

Result: 在ImageNet-512单模型无条件生成任务上取得了state-of-the-art的FID和FD-DINOv2指标。与Auto-Guidance结合使用时，在FD-DINOv2指标上达到了general state-of-the-art。在FLUX.1-dev和Stable Diffusion v3.5上的实验表明，SFG在保持提示一致性和图像保真度的同时，提高了输出图像的多样性。

Conclusion: Saddle-free guidance (SFG)是一种有效的score-based模型引导方法，它具有计算成本低、无需额外训练和标签数据等优点，并且能够提高生成图像的质量和多样性。

Abstract: Score-based generative models require guidance in order to generate plausible, on-manifold samples. The most popular guidance method, Classifier-Free Guidance (CFG), is only applicable in settings with labeled data and requires training an additional unconditional score-based model. More recently, Auto-Guidance adopts a smaller, less capable version of the original model to guide generation. While each method effectively promotes the fidelity of generated data, each requires labeled data or the training of additional models, making it challenging to guide score-based models when (labeled) training data are not available or training new models is not feasible.
  We make the surprising discovery that the positive curvature of log density estimates in saddle regions provides strong guidance for score-based models. Motivated by this, we develop saddle-free guidance (SFG) which maintains estimates of maximal positive curvature of the log density to guide individual score-based models. SFG has the same computational cost of classifier-free guidance, does not require additional training, and works with off-the-shelf diffusion and flow matching models. Our experiments indicate that SFG achieves state-of-the-art FID and FD-DINOv2 metrics in single-model unconditional ImageNet-512 generation. When SFG is combined with Auto-Guidance, its unconditional samples achieve general state-of-the-art in FD-DINOv2 score. Our experiments with FLUX.1-dev and Stable Diffusion v3.5 indicate that SFG boosts the diversity of output images compared to CFG while maintaining excellent prompt adherence and image fidelity.

</details>


### [49] [UniArt: Unified 3D Representation for Generating 3D Articulated Objects with Open-Set Articulation](https://arxiv.org/abs/2511.21887)
*Bu Jin,Weize Li,Songen Gu,Yupeng Zheng,Yuhang Zheng,Zhengyi Zhou,Yao Yao*

Main category: cs.CV

TL;DR: UniArt是一个扩散框架，可以直接从单个图像中端到端地合成完全铰接的3D对象。


<details>
  <summary>Details</summary>
Motivation: 手动构建铰接3D对象成本高且难以扩展。

Method: UniArt建立了一个统一的潜在表示，该表示共同编码几何体、纹理、零件分割和运动学参数。引入了一种可逆的关节到体素嵌入，该嵌入在空间上将关节特征与体积几何体对齐，使模型能够学习连贯的运动行为以及结构形成。将关节类型预测 формулируется 制定为开放集问题，无需固定关节语义，并允许泛化到新的关节类别和看不见的对象类型。

Result: 在PartNet-Mobility基准上的实验表明，UniArt实现了最先进的网格质量和铰接精度。

Conclusion: UniArt可以直接从单个图像中合成完全铰接的3D对象，并实现了最先进的网格质量和铰接精度。

Abstract: Articulated 3D objects play a vital role in realistic simulation and embodied robotics, yet manually constructing such assets remains costly and difficult to scale. In this paper, we present UniArt, a diffusion-based framework that directly synthesizes fully articulated 3D objects from a single image in an end-to-end manner. Unlike prior multi-stage techniques, UniArt establishes a unified latent representation that jointly encodes geometry, texture, part segmentation, and kinematic parameters. We introduce a reversible joint-to-voxel embedding, which spatially aligns articulation features with volumetric geometry, enabling the model to learn coherent motion behaviors alongside structural formation. Furthermore, we formulate articulation type prediction as an open-set problem, removing the need for fixed joint semantics and allowing generalization to novel joint categories and unseen object types. Experiments on the PartNet-Mobility benchmark demonstrate that UniArt achieves state-of-the-art mesh quality and articulation accuracy.

</details>


### [50] [PathReasoning: A multimodal reasoning agent for query-based ROI navigation on whole-slide images](https://arxiv.org/abs/2511.21902)
*Kunpeng Zhang,Hanwen Xu,Sheng Wang*

Main category: cs.CV

TL;DR: PathReasoning is a multi-modal reasoning agent for navigating whole slide images (WSIs) by mimicking pathologists' sampling, reasoning, and self-reflection process.


<details>
  <summary>Details</summary>
Motivation: Deciphering the tumor microenvironment from WSIs is key to cancer diagnosis, prognosis, and treatment response, but the extremely large size of WSIs makes navigation challenging.

Method: PathReasoning iteratively navigates WSIs through multiple rounds of reasoning and refinements, using self-reflection and reasoning over visual observations and clinical questions to propose new regions to explore.

Result: PathReasoning outperforms strong ROI-selection approaches by 6.7% and 3.1% of AUROC on subtyping and longitudinal analysis tasks and outperforms GPT-4o by 10% in accuracy on breast cancer report generation.

Conclusion: PathReasoning prioritizes question-specific regions and constructs interpretable reasoning chains, supporting efficient slide review, consistent diagnostic interpretations, comprehensive reporting, and evidence traceability in digital pathology.

Abstract: Deciphering tumor microenvironment from Whole Slide Images (WSIs) is intriguing as it is key to cancer diagnosis, prognosis and treatment response. While these gigapixel images on one hand offer a comprehensive portrait of cancer, on the other hand, the extremely large size, as much as more than 10 billion pixels, make it challenging and time-consuming to navigate to corresponding regions to support diverse clinical inspection. Inspired by pathologists who conducted navigation on WSIs with a combination of sampling, reasoning and self-reflection, we proposed "PathReasoning", a multi-modal reasoning agent that iteratively navigates across WSIs through multiple rounds of reasoning and refinements. Specifically, starting with randomly sampled candidate regions, PathReasoning reviews current selections with self-reflection, reasoning over the correspondence between visual observations and clinical questions, and concludes by proposing new regions to explore. Across rounds, PathReasoning builds a reasoning chain that gradually directs attention to diagnostically relevant areas. PathReasoning turns each whole slide into a sequence of question-guided views, allowing the model to efficiently find informative ROIs within a fixed number of steps, without the need for dense pixel-level annotations. PathReasoning can substantially outperform strong ROI-selection approaches by 6.7% and 3.1% of AUROC on subtyping and longitudinal analysis tasks. The high-quality ROIs further support accurate report generation on breast cancer, significantly outperforming the standard GPT-4o by 10% in accuracy. PathReasoning prioritizes question-specific regions and constructs interpretable reasoning chains, supporting efficient slide review, consistent diagnostic interpretations, comprehensive reporting, and evidence traceability in digital pathology.

</details>


### [51] [Adaptive Parameter Optimization for Robust Remote Photoplethysmography](https://arxiv.org/abs/2511.21903)
*Cecilia G. Morales,Fanurs Chi En Teh,Kai Li,Pushpak Agrawal,Artur Dubrawski*

Main category: cs.CV

TL;DR: 提出了PRISM，一种无需训练的rPPG算法，通过在线参数调整优化光度去趋势和颜色混合。


<details>
  <summary>Details</summary>
Motivation: 现有rPPG方法依赖于为特定光照条件和相机设置优化的固定参数，限制了对不同部署环境的适应性。

Method: PRISM算法通过信号质量评估进行在线参数调整，联合优化光度去趋势和颜色混合。

Result: PRISM在无监督方法中实现了最先进的性能，在PURE上的MAE为0.77 bpm，在UBFC-rPPG上的MAE为0.66 bpm，在5 bpm阈值下的准确率分别为97.3％和97.5％。

Conclusion: 自适应时间序列优化显著提高了rPPG在各种条件下的性能。

Abstract: Remote photoplethysmography (rPPG) enables contactless vital sign monitoring using standard RGB cameras. However, existing methods rely on fixed parameters optimized for particular lighting conditions and camera setups, limiting adaptability to diverse deployment environments. This paper introduces the Projection-based Robust Signal Mixing (PRISM) algorithm, a training-free method that jointly optimizes photometric detrending and color mixing through online parameter adaptation based on signal quality assessment. PRISM achieves state-of-the-art performance among unsupervised methods, with MAE of 0.77 bpm on PURE and 0.66 bpm on UBFC-rPPG, and accuracy of 97.3\% and 97.5\% respectively at a 5 bpm threshold. Statistical analysis confirms PRISM performs equivalently to leading supervised methods ($p > 0.2$), while maintaining real-time CPU performance without training. This validates that adaptive time series optimization significantly improves rPPG across diverse conditions.

</details>


### [52] [Interpretable Multimodal Cancer Prototyping with Whole Slide Images and Incompletely Paired Genomics](https://arxiv.org/abs/2511.21937)
*Yupei Zhang,Yating Huang,Wanming Hu,Lequan Yu,Hujun Yin,Chao Li*

Main category: cs.CV

TL;DR: 提出了一种灵活的多模态原型框架，用于整合全切片图像和不完整的基因组数据，以实现精准肿瘤学。


<details>
  <summary>Details</summary>
Motivation: 表型和基因型异质性限制了模态内表示的质量，并阻碍了有效的模态间整合。现有方法忽略了基因组可能部分缺失或完全不可用的真实临床场景。

Method: 该方法包含四个关键组成部分：1) 使用文本提示和原型权重进行生物原型设计；2) 通过样本和分布对齐进行多视图对齐；3) 用于捕获共享和模态特定信息以进行多模态融合的二分融合；4) 用于处理缺失数据的语义基因组插补。

Result: 大量实验表明，与最先进的方法相比，该方法在多个下游任务上具有持续的优越性。

Conclusion: 该研究提出了一种有效且灵活的多模态框架，可用于整合全切片图像和不完整的基因组数据，从而推动精准肿瘤学的发展。

Abstract: Multimodal approaches that integrate histology and genomics hold strong potential for precision oncology. However, phenotypic and genotypic heterogeneity limits the quality of intra-modal representations and hinders effective inter-modal integration. Furthermore, most existing methods overlook real-world clinical scenarios where genomics may be partially missing or entirely unavailable. We propose a flexible multimodal prototyping framework to integrate whole slide images and incomplete genomics for precision oncology. Our approach has four key components: 1) Biological Prototyping using text prompting and prototype-wise weighting; 2) Multiview Alignment through sample- and distribution-wise alignments; 3) Bipartite Fusion to capture both shared and modality-specific information for multimodal fusion; and 4) Semantic Genomics Imputation to handle missing data. Extensive experiments demonstrate the consistent superiority of the proposed method compared to other state-of-the-art approaches on multiple downstream tasks. The code is available at https://github.com/helenypzhang/Interpretable-Multimodal-Prototyping.

</details>


### [53] [AmodalGen3D: Generative Amodal 3D Object Reconstruction from Sparse Unposed Views](https://arxiv.org/abs/2511.21945)
*Junwei Zhou,Yu-Wing Tai*

Main category: cs.CV

TL;DR: AmodalGen3D是一个用于从稀疏、未定位和部分遮挡的视图中重建完整3D对象的生成框架。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以处理遮挡和不完整的观测，导致重建不完整或几何不一致。

Method: 该模型结合了2D非模态补全先验和多视图立体几何条件，利用View-Wise Cross Attention进行稀疏视图特征融合，Stereo-Conditioned Cross Attention推断未观测结构。

Result: 在合成和真实数据集上的实验表明，AmodalGen3D在严重遮挡的稀疏视图设置下实现了卓越的保真度和完整性。

Conclusion: AmodalGen3D能够从稀疏视图约束中重建3D对象，并在遮挡情况下合理地推断未见部分，满足了机器人、AR/VR和具身AI应用中对象级3D场景重建的迫切需求。

Abstract: Reconstructing 3D objects from a few unposed and partially occluded views is a common yet challenging problem in real-world scenarios, where many object surfaces are never directly observed. Traditional multi-view or inpainting-based approaches struggle under such conditions, often yielding incomplete or geometrically inconsistent reconstructions. We introduce AmodalGen3D, a generative framework for amodal 3D object reconstruction that infers complete, occlusion-free geometry and appearance from arbitrary sparse inputs. The model integrates 2D amodal completion priors with multi-view stereo geometry conditioning, supported by a View-Wise Cross Attention mechanism for sparse-view feature fusion and a Stereo-Conditioned Cross Attention module for unobserved structure inference. By jointly modeling visible and hidden regions, AmodalGen3D faithfully reconstructs 3D objects that are consistent with sparse-view constraints while plausibly hallucinating unseen parts. Experiments on both synthetic and real-world datasets demonstrate that AmodalGen3D achieves superior fidelity and completeness under occlusion-heavy sparse-view settings, addressing a pressing need for object-level 3D scene reconstruction in robotics, AR/VR, and embodied AI applications.

</details>


### [54] [TAPVid-360: Tracking Any Point in 360 from Narrow Field of View Video](https://arxiv.org/abs/2511.21946)
*Finlay G. C. Hudson,James A. D. Gardner,William A. P. Smith*

Main category: cs.CV

TL;DR: 现有的视觉系统难以理解全景，并且在Track Any Point (TAP) 任务中，无法追踪视野外的2D点。本文提出了TAPVid-360，一个新的任务，需要预测查询到的场景点的3D方向，即使这些点在观察视频的狭窄视野之外。为了解决这个问题，我们引入了TAPVid-360，这是一个新的任务，它需要在视频序列中预测查询到的场景点的3D方向，即使这些点在观察到的视频的狭窄视野之外。这个任务促进了学习以自我为中心的场景表征，而不需要动态的4D地面真实场景模型进行训练。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉系统在理解持久性和全景方面存在局限性，尤其是在Track Any Point (TAP) 任务中，无法追踪视野外的2D点。

Method: 利用360度视频作为监督来源，将其重采样为窄视野透视，同时使用2D管线在全景中追踪点来计算地面真实方向。修改CoTracker v3来预测每个点的旋转以进行方向更新。

Result: 引入了一个新的数据集和基准，TAPVid360-10k，包含1万个透视视频，具有地面真实的方向点追踪。改进后的CoTracker v3优于现有的TAP和TAPVid 3D方法。

Conclusion: 本文提出了TAPVid-360任务和TAPVid360-10k数据集，并验证了改进后的CoTracker v3的有效性，为解决视觉系统全景理解问题提供了新思路。

Abstract: Humans excel at constructing panoramic mental models of their surroundings, maintaining object permanence and inferring scene structure beyond visible regions. In contrast, current artificial vision systems struggle with persistent, panoramic understanding, often processing scenes egocentrically on a frame-by-frame basis. This limitation is pronounced in the Track Any Point (TAP) task, where existing methods fail to track 2D points outside the field of view. To address this, we introduce TAPVid-360, a novel task that requires predicting the 3D direction to queried scene points across a video sequence, even when far outside the narrow field of view of the observed video. This task fosters learning allocentric scene representations without needing dynamic 4D ground truth scene models for training. Instead, we exploit 360 videos as a source of supervision, resampling them into narrow field-of-view perspectives while computing ground truth directions by tracking points across the full panorama using a 2D pipeline. We introduce a new dataset and benchmark, TAPVid360-10k comprising 10k perspective videos with ground truth directional point tracking. Our baseline adapts CoTracker v3 to predict per-point rotations for direction updates, outperforming existing TAP and TAPVid 3D methods.

</details>


### [55] [WalkCLIP: Multimodal Learning for Urban Walkability Prediction](https://arxiv.org/abs/2511.21947)
*Shilong Xiang,JangHyeon Lee,Min Namgung,Yao-Yi Chiang*

Main category: cs.CV

TL;DR: 提出了一种名为WalkCLIP的多模态框架，通过整合卫星图像、街景图像和人口动态等多维度数据来预测城市步行适宜性，优于单一数据源方法。


<details>
  <summary>Details</summary>
Motivation: 传统的步行适宜性评估依赖于调查和实地审计，成本高且难以扩展。以往研究使用单一来源数据，如卫星图像、街景图像或人口指标，但这些方法仅捕捉步行环境的一个维度。

Method: WalkCLIP框架从GPT-4o生成的图像描述中学习步行适宜性的视觉-语言表征，通过空间聚合模块结合邻域环境来优化这些表征，并将结果特征与来自人口动态基础模型的表征融合。

Result: 在明尼阿波利斯-圣保罗的4660个地点进行评估，WalkCLIP在预测准确性和空间对齐方面均优于单模态和多模态基线模型。

Conclusion: 视觉和行为信号的整合能够可靠地预测步行环境。

Abstract: Urban walkability is a cornerstone of public health, sustainability, and quality of life. Traditional walkability assessments rely on surveys and field audits, which are costly and difficult to scale. Recent studies have used satellite imagery, street view imagery, or population indicators to estimate walkability, but these single-source approaches capture only one dimension of the walking environment. Satellite data describe the built environment from above, but overlook the pedestrian perspective. Street view imagery captures conditions at the ground level, but lacks broader spatial context. Population dynamics reveal patterns of human activity but not the visual form of the environment. We introduce WalkCLIP, a multimodal framework that integrates these complementary viewpoints to predict urban walkability. WalkCLIP learns walkability-aware vision-language representations from GPT-4o generated image captions, refines these representations with a spatial aggregation module that incorporates neighborhood context, and fuses the resulting features with representations from a population dynamics foundation model. Evaluated at 4,660 locations throughout Minneapolis-Saint Paul, WalkCLIP outperforms unimodal and multimodal baselines in both predictive accuracy and spatial alignment. These results show that the integration of visual and behavioral signals yields reliable predictions of the walking environment.

</details>


### [56] [DeepGI: Explainable Deep Learning for Gastrointestinal Image Classification](https://arxiv.org/abs/2511.21959)
*Walid Houmaidi,Mohamed Hadadi,Youssef Sabiri,Yousra Chtouki*

Main category: cs.CV

TL;DR: 本文提出了一个关于胃肠道医学图像数据集的综合比较模型分析，该数据集包含4,000张内窥镜图像，涵盖四种关键疾病类别。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决内窥镜检查中常见的问题，如可变光照、相机角度波动和频繁的图像伪影。

Method: 利用最先进的深度学习技术，包括VGG16、MobileNetV2和Xception。

Result: VGG16和MobileNetV2模型均达到了96.5%的测试准确率，而Xception达到了94.24%。此外，还使用了Grad-CAM可视化来实现可解释的AI。

Conclusion: 实验结果表明，即使在复杂的现实条件下，也能实现稳健、准确和可解释的医学图像分析。

Abstract: This paper presents a comprehensive comparative model analysis on a novel gastrointestinal medical imaging dataset, comprised of 4,000 endoscopic images spanning four critical disease classes: Diverticulosis, Neoplasm, Peritonitis, and Ureters. Leveraging state-of-the-art deep learning techniques, the study confronts common endoscopic challenges such as variable lighting, fluctuating camera angles, and frequent imaging artifacts. The best performing models, VGG16 and MobileNetV2, each achieved a test accuracy of 96.5%, while Xception reached 94.24%, establishing robust benchmarks and baselines for automated disease classification. In addition to strong classification performance, the approach includes explainable AI via Grad-CAM visualization, enabling identification of image regions most influential to model predictions and enhancing clinical interpretability. Experimental results demonstrate the potential for robust, accurate, and interpretable medical image analysis even in complex real-world conditions. This work contributes original benchmarks, comparative insights, and visual explanations, advancing the landscape of gastrointestinal computer-aided diagnosis and underscoring the importance of diverse, clinically relevant datasets and model explainability in medical AI research.

</details>


### [57] [PAT3D: Physics-Augmented Text-to-3D Scene Generation](https://arxiv.org/abs/2511.21978)
*Guying Lin,Kemeng Huang,Michael Liu,Ruihan Gao,Hanke Chen,Lyuhao Chen,Beijia Lu,Taku Komura,Yuan Liu,Jun-Yan Zhu,Minchen Li*

Main category: cs.CV

TL;DR: PAT3D: 首个物理增强的文本到3D场景生成框架，结合视觉-语言模型和物理模拟，生成物理上合理、可用于模拟且无交叉的3D场景。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法生成的3D场景在物理合理性、语义一致性和视觉质量方面表现不佳的问题，并为下游任务（如场景编辑和机器人操作）提供可模拟的3D场景。

Method: PAT3D通过以下步骤实现：1) 生成3D对象；2) 推断它们的空间关系；3) 将它们组织成一个分层场景树；4) 转换为模拟的初始条件；5) 使用可微刚体模拟器确保在重力作用下实现真实的物体交互，使场景达到静态平衡且无相互渗透；6) 引入模拟循环优化程序，保证物理稳定性和非交叉，同时提高与输入prompt的语义一致性。

Result: 实验表明，PAT3D在物理合理性、语义一致性和视觉质量方面明显优于现有方法。

Conclusion: PAT3D不仅能生成高质量的3D场景，还能生成可用于下游任务（如场景编辑和机器人操作）的、可直接进行模拟的3D场景。

Abstract: We introduce PAT3D, the first physics-augmented text-to-3D scene generation framework that integrates vision-language models with physics-based simulation to produce physically plausible, simulation-ready, and intersection-free 3D scenes. Given a text prompt, PAT3D generates 3D objects, infers their spatial relations, and organizes them into a hierarchical scene tree, which is then converted into initial conditions for simulation. A differentiable rigid-body simulator ensures realistic object interactions under gravity, driving the scene toward static equilibrium without interpenetrations. To further enhance scene quality, we introduce a simulation-in-the-loop optimization procedure that guarantees physical stability and non-intersection, while improving semantic consistency with the input prompt. Experiments demonstrate that PAT3D substantially outperforms prior approaches in physical plausibility, semantic consistency, and visual quality. Beyond high-quality generation, PAT3D uniquely enables simulation-ready 3D scenes for downstream tasks such as scene editing and robotic manipulation. Code and data will be released upon acceptance.

</details>


### [58] [SciPostGen: Bridging the Gap between Scientific Papers and Poster Layouts](https://arxiv.org/abs/2511.22490)
*Shun Inadumi,Shohei Tanaka,Tosho Hirasawa,Atsushi Hashimoto,Koichiro Yoshino,Yoshitaka Ushiku*

Main category: cs.CV

TL;DR: 论文提出了SciPostGen数据集，用于理解和生成海报布局。


<details>
  <summary>Details</summary>
Motivation: 随着科学论文数量的持续增长，海报作为展示论文内容的关键媒介，其布局对于有效传达研究结果至关重要。目前对于论文与其展示布局之间的对应关系理解不足，需要大规模的配对注释数据集。

Method: 通过分析SciPostGen数据集，发现论文结构与海报中的布局元素数量相关联。提出了一个检索增强的海报布局生成框架，该框架检索与给定论文一致的布局，并将其用作布局生成的指导。

Result: 检索器可以估计与论文结构对齐的布局，并且该框架可以生成满足给定约束的布局。

Conclusion: 论文构建了大规模数据集，并提出了检索增强的布局生成框架，实验结果表明该框架能够生成与论文结构对齐且满足约束的布局。

Abstract: As the number of scientific papers continues to grow, there is a demand for approaches that can effectively convey research findings, with posters serving as a key medium for presenting paper contents. Poster layouts determine how effectively research is communicated and understood, highlighting their growing importance. In particular, a gap remains in understanding how papers correspond to the layouts that present them, which calls for datasets with paired annotations at scale. To bridge this gap, we introduce SciPostGen, a large-scale dataset for understanding and generating poster layouts from scientific papers. Our analyses based on SciPostGen show that paper structures are associated with the number of layout elements in posters. Based on this insight, we explore a framework, Retrieval-Augmented Poster Layout Generation, which retrieves layouts consistent with a given paper and uses them as guidance for layout generation. We conducted experiments under two conditions: with and without layout constraints typically specified by poster creators. The results show that the retriever estimates layouts aligned with paper structures, and our framework generates layouts that also satisfy given constraints.

</details>


### [59] [DialBench: Towards Accurate Reading Recognition of Pointer Meter using Large Foundation Models](https://arxiv.org/abs/2511.21982)
*Futian Wang,Chaoliu Weng,Xiao Wang,Zhen Chen,Zhicheng Zhao,Jin Tang*

Main category: cs.CV

TL;DR: 本文介绍了一个新的用于表盘读数识别的大规模基准数据集RPM-10K，并提出了一个基于物理关系注入的视觉-语言模型MRLM，用于指针式仪表读数识别。


<details>
  <summary>Details</summary>
Motivation: 现有方法在指针式仪表的精确读数识别方面仍然存在不足，因为它们容易受到反射、遮挡、动态视角以及细指针和刻度标记之间过度相似性的影响，并且缺乏大规模数据集的支持。

Method: 本文提出了一个名为MRLM的视觉-语言模型，该模型基于物理关系注入，显式地编码指针和刻度之间的几何和因果关系，通过交叉注意融合和自适应专家选择来学习解释表盘配置并生成精确的数值读数。

Result: 大量的实验充分验证了本文提出的框架在新提出的基准数据集上的有效性。

Conclusion: 本文提出了一个新的大规模表盘读数数据集和一个新的视觉-语言模型，并在实验中验证了其有效性。

Abstract: The precise reading recognition of pointer meters plays a key role in smart power systems, but existing approaches remain fragile due to challenges like reflections, occlusions, dynamic viewing angles, and overly between thin pointers and scale markings. Up to now, this area still lacks large-scale datasets to support the development of robust algorithms. To address these challenges, this paper first presents a new large-scale benchmark dataset for dial reading, termed RPM-10K, which contains 10730 meter images that fully reflect the aforementioned key challenges. Built upon the dataset, we propose a novel vision-language model for pointer meter reading recognition, termed MRLM, based on physical relation injection. Instead of exhaustively learning image-level correlations, MRLM explicitly encodes the geometric and causal relationships between the pointer and the scale, aligning perception with physical reasoning in the spirit of world-model perspectives. Through cross-attentional fusion and adaptive expert selection, the model learns to interpret dial configurations and generate precise numeric readings. Extensive experiments fully validated the effectiveness of our proposed framework on the newly proposed benchmark dataset. Both the dataset and source code will be released on https://github.com/Event-AHU/DialBench

</details>


### [60] [PPBoost: Progressive Prompt Boosting for Text-Driven Medical Image Segmentation](https://arxiv.org/abs/2511.21984)
*Xuchen Li,Hengrui Gu,Mohan Zhang,Qin Liu,Zhen Tan,Xinyuan Zhu,Huixue Zhou,Tianlong Chen,Kaixiong Zhou*

Main category: cs.CV

TL;DR: PPBoost: 通过将文本提示转换为空间定位的视觉提示，改进医学图像分割。


<details>
  <summary>Details</summary>
Motivation: 文本提示医学图像分割模型缺乏空间精确性，且在领域迁移下性能下降。视觉提示模型性能强但获取精确视觉提示成本高。

Method: PPBoost框架将弱文本信号转换为强的空间视觉提示。它使用视觉-语言模型生成伪边界框，过滤不可靠的预测，并训练一个伪标签检测器来生成高质量边界框。在推理过程中，PPBoost进一步细化生成的边界框，以紧密覆盖目标结构。

Result: PPBoost在多个数据集上持续改进了Dice和归一化表面距离，超过了文本和视觉提示基线，并且在没有标签数据的情况下超过了少样本分割模型。

Conclusion: PPBoost可以推广到多个典型的视觉分割模型骨干。

Abstract: Text-prompted foundation models for medical image segmentation offer an intuitive way to delineate anatomical structures from natural language queries, but their predictions often lack spatial precision and degrade under domain shift. In contrast, visual-prompted models achieve strong segmentation performance across diverse modalities by leveraging spatial cues of precise bounding-box (bbox) prompts to guide the segmentation of target lesions. However, it is costly and challenging to obtain the precise visual prompts in clinical practice. We propose PPBoost (Progressive Prompt-Boosting), a framework that bridges these limitations by transforming weak text-derived signals into strong, spatially grounded visual prompts, operating under a strict zero-shot regime with no image- or pixel-level segmentation labels. PPBoost first uses a vision-language model to produce initial pseudo-bboxes conditioned on the textual object descriptions and applies an uncertainty-aware criterion to filter unreliable predictions. The retained image-bboxes pairs are then leveraged to train a pseudo-labeled detector, producing the high-quality bboxes for the query images. During inference, PPBoost further refines the generated bboxes by appropriately expanding them to tightly cover the target anatomical structures. The enhanced spatially-grounding bbox prompts guide existing segmentation models to generate final dense masks, effectively amplifying weak text cues into strong spatial guidance. Across three datasets spanning diverse modalities and anatomies, PPBoost consistently improves Dice and Normalized Surface Distance over text- and visual-prompted baselines and, notably, surpasses few-shot segmentation models without using labeled data. PPBoost can generalize to multiple typical visual segmentation model backbones.

</details>


### [61] [CNN-Based Framework for Pedestrian Age and Gender Classification Using Far-View Surveillance in Mixed-Traffic Intersections](https://arxiv.org/abs/2511.22873)
*Shisir Shahriar Arif,Md. Muhtashim Shahrier,Nazmul Haque,Md Asif Raihan,Md. Hadiuzzaman*

Main category: cs.CV

TL;DR: 这篇论文提出了一种深度学习框架，用于在孟加拉国达卡拥挤的城市交叉口，利用监控录像识别行人年龄和性别，旨在改善行人安全。


<details>
  <summary>Details</summary>
Motivation: 在交通拥挤的城市交叉口，尤其是中低收入国家，行人安全是一个紧迫的问题。现有监控系统很少捕捉到年龄和性别等人口因素，而这些因素会影响行人的脆弱性。

Method: 该研究使用卷积神经网络（CNN），从远距离交叉口视频中分类行人年龄和性别，无需面部识别或高分辨率图像。使用了两种CNN架构：ResNet50和一个定制的轻量级CNN。

Result: ResNet50在Max Pooling和SGD下实现了最高的准确率（86.19%），而定制CNN的性能相当（84.15%），但参数更少，训练速度更快。

Conclusion: 该系统为从业者提供了一种可扩展且经济高效的工具，可以使用现有的摄像头基础设施来监控交叉口的行人人口统计数据。其输出可以改进交叉口设计，优化信号配时，并为弱势群体（如儿童或老人）提供有针对性的安全干预。

Abstract: Pedestrian safety remains a pressing concern in congested urban intersections, particularly in low- and middle-income countries where traffic is multimodal, and infrastructure often lacks formal control. Demographic factors like age and gender significantly influence pedestrian vulnerability, yet real-time monitoring systems rarely capture this information. To address this gap, this study proposes a deep learning framework that classifies pedestrian age group and gender from far-view intersection footage using convolutional neural networks (CNNs), without relying on facial recognition or high-resolution imagery. The classification is structured as a unified six-class problem, distinguishing adult, teenager, and child pedestrians for both males and females, based on full-body visual cues. Video data was collected from three high-risk intersections in Dhaka, Bangladesh. Two CNN architectures were implemented: ResNet50, a deep convolutional neural network pretrained on ImageNet, and a custom lightweight CNN optimized for computational efficiency. Eight model variants explored combinations of pooling strategies and optimizers. ResNet50 with Max Pooling and SGD achieved the highest accuracy (86.19%), while the custom CNN performed comparably (84.15%) with fewer parameters and faster training. The model's efficient design enables real-time inference on standard surveillance feeds. For practitioners, this system provides a scalable, cost-effective tool to monitor pedestrian demographics at intersections using existing camera infrastructure. Its outputs can shape intersection design, optimize signal timing, and enable targeted safety interventions for vulnerable groups such as children or the elderly. By offering demographic insights often missing in conventional traffic data, the framework supports more inclusive, data-driven planning in mixed-traffic environments.

</details>


### [62] [Can Multi-Modal LLMs Provide Live Step-by-Step Task Guidance?](https://arxiv.org/abs/2511.21998)
*Apratim Bhattacharyya,Bicheng Xu,Sanjay Haresh,Reza Pourreza,Litian Liu,Sunny Panchal,Pulkit Madan,Leonid Sigal,Roland Memisevic*

Main category: cs.CV

TL;DR: 多模态大型语言模型在对话能力方面取得了进展，但难以提供实时的、交互式的逐步指导。本文介绍了Qualcomm Interactive Cooking，这是一个新的基准和数据集，建立在CaptainCook4D之上，其中包含用户在任务执行过程中出现的错误。我们评估了最先进的多模态LLM在Qualcomm Interactive Cooking基准上的性能，并介绍了LiveMamba，一个为交互式教学指导而设计的流式多模态LLM。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大型语言模型难以提供实时的、交互式的逐步指导，而这对于未来的AI助手至关重要。有效的指导不仅需要传递指令，还需要检测指令的成功执行，以及识别并提醒用户注意错误，所有这些都必须实时发生。

Method: 构建了一个新的基准和数据集Qualcomm Interactive Cooking，它基于CaptainCook4D，包含用户在任务执行过程中出现的错误。提出了LiveMamba，一个为交互式教学指导而设计的流式多模态LLM。

Result: 评估了最先进的多模态LLM在Qualcomm Interactive Cooking基准上的性能，并为开发和评估实时情境指导提供了一个强大的基线。

Conclusion: 这项工作为开发和评估实时情境指导提供了第一个专用基准和一个强大的基线。

Abstract: Multi-modal Large Language Models (LLM) have advanced conversational abilities but struggle with providing live, interactive step-by-step guidance, a key capability for future AI assistants. Effective guidance requires not only delivering instructions but also detecting their successful execution, as well as identifying and alerting users to mistakes, all of which has to happen in real-time. This requires models that are not turn-based, but that can react asynchronously to a video stream, as well as video data showing users performing tasks including mistakes and their corrections. To this end, we introduce Qualcomm Interactive Cooking, a new benchmark and dataset built upon CaptainCook4D, which contains user mistakes during task execution. Our dataset and benchmark features densely annotated, timed instructions and feedback messages, specifically including mistake alerts precisely timestamped to their visual occurrence in the video. We evaluate state-of-the-art multi-modal LLMs on the Qualcomm Interactive Cooking benchmark and introduce LiveMamba, a streaming multi-modal LLM designed for interactive instructional guidance. This work provides the first dedicated benchmark and a strong baseline for developing and evaluating on live, situated coaching.

</details>


### [63] [StreamFlow: Theory, Algorithm, and Implementation for High-Efficiency Rectified Flow Generation](https://arxiv.org/abs/2511.22009)
*Sen Fang,Hongbin Zhong,Yalin Feng,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: 提出了一种新的加速 Rectified Flow 模型的方法，该方法在 512*512 图像生成速度上实现了高达 611% 的加速。


<details>
  <summary>Details</summary>
Motivation: 现有的加速方法不能直接应用于 Rectified Flow 模型。

Method: 从理论、设计和推理策略等方面综合实现了一个整体加速管道，包括使用新的速度场进行批处理、异构时间步长批处理的向量化和动态 TensorRT 编译等新方法。

Result: 在 512*512 图像生成速度上实现了高达 611% 的加速。

Conclusion: 该方法远超当前非通用加速方法。

Abstract: New technologies such as Rectified Flow and Flow Matching have significantly improved the performance of generative models in the past two years, especially in terms of control accuracy, generation quality, and generation efficiency. However, due to some differences in its theory, design, and existing diffusion models, the existing acceleration methods cannot be directly applied to the Rectified Flow model. In this article, we have comprehensively implemented an overall acceleration pipeline from the aspects of theory, design, and reasoning strategies. This pipeline uses new methods such as batch processing with a new velocity field, vectorization of heterogeneous time-step batch processing, and dynamic TensorRT compilation for the new methods to comprehensively accelerate related models based on flow models. Currently, the existing public methods usually achieve an acceleration of 18%, while experiments have proved that our new method can accelerate the 512*512 image generation speed to up to 611%, which is far beyond the current non-generalized acceleration methods.

</details>


### [64] [MedEyes: Learning Dynamic Visual Focus for Medical Progressive Diagnosis](https://arxiv.org/abs/2511.22018)
*Chunzheng Zhu,Yangfang Lin,Shen Chen,Yijun Wang,Jianxin Lin*

Main category: cs.CV

TL;DR: 本文提出了一种新的强化学习框架MedEyes，通过结合专家指导和自主探索，模拟临床医生进行医学诊断的视觉推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型虽然可以通过强化学习进行链式思考推理，但其纯粹的on-policy学习范式容易强化表面连贯但临床不准确的推理路径。

Method: 设计了Gaze-guided Reasoning Navigator (GRN) 模拟诊断过程，通过双模探索策略扫描定位系统性异常和深入分析区域细节。引入 Confidence Value Sampler (CVS) 来平衡专家模仿和自主发现，创建多样但可信的探索路径。双流 GRPO 优化框架解耦 on-policy 和 off-policy 学习信号，缓解奖励同化和熵崩溃。

Result: 在多个医学 VQA 基准测试中，MedEyes 平均性能提升了 +8.5%。

Conclusion: MedEyes 在构建可解释的医学 AI 系统方面具有潜力。

Abstract: Accurate medical diagnosis often involves progressive visual focusing and iterative reasoning, characteristics commonly observed in clinical workflows. While recent vision-language models demonstrate promising chain-of-thought (CoT) reasoning capabilities via reinforcement learning with verifiable rewards (RLVR), their purely on-policy learning paradigm tends to reinforce superficially coherent but clinically inaccurate reasoning paths. We propose MedEyes, a novel reinforcement learning framework that dynamically models clinician-style diagnostic reasoning by progressively attending to and interpreting relevant medical image regions. By incorporating off-policy expert guidance, MedEyes converts expert visual search trajectories into structured external behavioral signals, guiding the model toward clinically aligned visual reasoning. We design the Gaze-guided Reasoning Navigator (GRN) to emulate the diagnostic process through a dual-mode exploration strategy, scanning for systematic abnormality localization and drilling for detailed regional analysis. To balance expert imitation and autonomous discovery, we introduce the Confidence Value Sampler (CVS), which employs nucleus sampling and adaptive termination to create diverse yet credible exploration paths. Finally, the dual-stream GRPO optimization framework decouples on-policy and off-policy learning signals, mitigating reward assimilation and entropy collapse. Experiments demonstrate that MedEyes achieves an average performance improvement of +8.5\% across multiple medical VQA benchmarks, validating MedEyes's potential in building interpretable medical AI systems.

</details>


### [65] [Intra-Class Probabilistic Embeddings for Uncertainty Estimation in Vision-Language Models](https://arxiv.org/abs/2511.22019)
*Zhenxiang Lin,Maryam Haghighat,Will Browne,Dimity Miller*

Main category: cs.CV

TL;DR: 提出了一种免训练的VLM事后不确定性估计方法，用于检测错误预测。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型(VLM)在开放词汇分类中表现出色，但容易对错误分类赋予高置信度，限制了其在安全关键应用中的可靠性。

Method: 通过测量类内的视觉特征一致性，使用特征投影结合多元高斯分布来创建类特定的概率嵌入。

Result: 在ImageNet、Flowers102、Food101、EuroSAT和DTD上进行了大量实验，结果表明该方法具有最好的错误检测性能，显著优于确定性和概率VLM基线。

Conclusion: 该方法与VLM无关，无需微调，对分布偏移具有鲁棒性，并且每个类仅需10张训练图像即可有效工作。

Abstract: Vision-language models (VLMs), such as CLIP, have gained popularity for their strong open vocabulary classification performance, but they are prone to assigning high confidence scores to misclassifications, limiting their reliability in safety-critical applications. We introduce a training-free, post-hoc uncertainty estimation method for contrastive VLMs that can be used to detect erroneous predictions. The key to our approach is to measure visual feature consistency within a class, using feature projection combined with multivariate Gaussians to create class-specific probabilistic embeddings. Our method is VLM-agnostic, requires no fine-tuning, demonstrates robustness to distribution shift, and works effectively with as few as 10 training images per class. Extensive experiments on ImageNet, Flowers102, Food101, EuroSAT and DTD show state-of-the-art error detection performance, significantly outperforming both deterministic and probabilistic VLM baselines. Code is available at https://github.com/zhenxianglin/ICPE.

</details>


### [66] [Layover or Direct Flight: Rethinking Audio-Guided Image Segmentation](https://arxiv.org/abs/2511.22025)
*Joel Alberto Santos,Zongwei Wu,Xavier Alameda-Pineda,Radu Timofte*

Main category: cs.CV

TL;DR: 本文研究了直接从语音指令进行目标定位的可行性，避免了传统方法中语音转录文本的步骤。


<details>
  <summary>Details</summary>
Motivation: 传统的目标定位方法依赖于语音转录文本，效率和鲁棒性存在问题。

Method: 作者简化任务，集中于从单字语音指令进行定位，并构建了一个新的语音目标定位数据集，同时调整并测试了来自音频-视觉领域的模型。

Result: 实验结果表明，直接从音频进行定位是可行的，并且在某些情况下，尤其是在对语言变异的鲁棒性方面，甚至优于基于文本转录的方法。

Conclusion: 研究结果鼓励人们重新关注直接音频定位，并为更鲁棒和高效的多模态理解系统铺平了道路。

Abstract: Understanding human instructions is essential for enabling smooth human-robot interaction. In this work, we focus on object grounding, i.e., localizing an object of interest in a visual scene (e.g., an image) based on verbal human instructions. Despite recent progress, a dominant research trend relies on using text as an intermediate representation. These approaches typically transcribe speech to text, extract relevant object keywords, and perform grounding using models pretrained on large text-vision datasets. However, we question both the efficiency and robustness of such transcription-based pipelines. Specifically, we ask: Can we achieve direct audio-visual alignment without relying on text? To explore this possibility, we simplify the task by focusing on grounding from single-word spoken instructions. We introduce a new audio-based grounding dataset that covers a wide variety of objects and diverse human accents. We then adapt and benchmark several models from the closely audio-visual field. Our results demonstrate that direct grounding from audio is not only feasible but, in some cases, even outperforms transcription-based methods, especially in terms of robustness to linguistic variability. Our findings encourage a renewed interest in direct audio grounding and pave the way for more robust and efficient multimodal understanding systems.

</details>


### [67] [PAGen: Phase-guided Amplitude Generation for Domain-adaptive Object Detection](https://arxiv.org/abs/2511.22029)
*Shuchen Du,Shuo Lei,Feiran Li,Jiacheng Li,Daisuke Iso*

Main category: cs.CV

TL;DR: 提出了一种简单有效的UDA方法，该方法学习在频域中调整图像风格，以减少源域和目标域之间的差异。


<details>
  <summary>Details</summary>
Motivation: 现有的UDA方法过于复杂，依赖于对抗性训练策略或具有辅助模型的复杂架构设计，用于特征蒸馏和伪标签生成。

Method: 该方法在训练期间引入了一个轻量级的预处理模块，并在推理时完全丢弃它，因此不会产生额外的计算开销。

Result: 在DAOD任务上的大量实验表明，该方法在多个基准测试中取得了显著的性能提升。

Conclusion: 该方法具有实用性和有效性。

Abstract: Unsupervised domain adaptation (UDA) greatly facilitates the deployment of neural networks across diverse environments. However, most state-of-the-art approaches are overly complex, relying on challenging adversarial training strategies, or on elaborate architectural designs with auxiliary models for feature distillation and pseudo-label generation. In this work, we present a simple yet effective UDA method that learns to adapt image styles in the frequency domain to reduce the discrepancy between source and target domains. The proposed approach introduces only a lightweight pre-processing module during training and entirely discards it at inference time, thus incurring no additional computational overhead. We validate our method on domain-adaptive object detection (DAOD) tasks, where ground-truth annotations are easily accessible in source domains (e.g., normal-weather or synthetic conditions) but challenging to obtain in target domains (e.g., adverse weather or low-light scenes). Extensive experiments demonstrate that our method achieves substantial performance gains on multiple benchmarks, highlighting its practicality and effectiveness.

</details>


### [68] [SparseWorld-TC: Trajectory-Conditioned Sparse Occupancy World Model](https://arxiv.org/abs/2511.22039)
*Jiayuan Du,Yiming Zhao,Zhenglong Guo,Yong Pan,Wenbo Hou,Zhihui Hao,Kun Zhan,Qijun Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的架构，用于轨迹条件下的未来 3D 场景占用预测，该架构直接从原始图像特征以端到端的方式预测多帧未来占用。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于变分自动编码器 (VAE) 来生成离散占用令牌，这 inherent 限制了表征能力。

Method: 该方法采用稀疏占用表示，绕过了中间鸟瞰图 (BEV) 投影及其显式几何先验。这种设计允许 transformer 更有效地捕获时空依赖性。

Result: 该方法在 nuScenes 基准测试中实现了 1-3 秒占用预测的最先进性能，优于现有方法。此外，它展示了强大的场景动态理解，在任意未来轨迹条件下始终提供高精度。

Conclusion: 该方法避免了离散令牌化的有限容量约束和 BEV 表示的结构限制。

Abstract: This paper introduces a novel architecture for trajectory-conditioned forecasting of future 3D scene occupancy. In contrast to methods that rely on variational autoencoders (VAEs) to generate discrete occupancy tokens, which inherently limit representational capacity, our approach predicts multi-frame future occupancy in an end-to-end manner directly from raw image features. Inspired by the success of attention-based transformer architectures in foundational vision and language models such as GPT and VGGT, we employ a sparse occupancy representation that bypasses the intermediate bird's eye view (BEV) projection and its explicit geometric priors. This design allows the transformer to capture spatiotemporal dependencies more effectively. By avoiding both the finite-capacity constraint of discrete tokenization and the structural limitations of BEV representations, our method achieves state-of-the-art performance on the nuScenes benchmark for 1-3 second occupancy forecasting, outperforming existing approaches by a significant margin. Furthermore, it demonstrates robust scene dynamics understanding, consistently delivering high accuracy under arbitrary future trajectory conditioning.

</details>


### [69] [ICM-SR: Image-Conditioned Manifold Regularization for Image Super-Resoultion](https://arxiv.org/abs/2511.22048)
*Junoh Kang,Donghun Ryu,Bohyung Han*

Main category: cs.CV

TL;DR: 本文提出了一种图像条件流形正则化（ICM）方法，用于真实世界图像超分辨率（Real-ISR）。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了正则化流形的重要性，通常默认为文本条件流形，这与Real-ISR任务不符，并且教师模型重建图像时存在颜色失真和边缘模糊。

Method: ICM方法将输出正则化到以稀疏但重要的结构信息（colormap和Canny边缘的组合）为条件的流形上。

Result: 实验表明，该正则化显著提高了超分辨率性能，尤其是在感知质量方面。

Conclusion: ICM提供了一种任务对齐且稳定的正则化信号，避免了密集条件的带来的不稳定，并增强了最终的超分辨率质量。

Abstract: Real world image super-resolution (Real-ISR) often leverages the powerful generative priors of text-to-image diffusion models by regularizing the output to lie on their learned manifold. However, existing methods often overlook the importance of the regularizing manifold, typically defaulting to a text-conditioned manifold. This approach suffers from two key limitations. Conceptually, it is misaligned with the Real-ISR task, which is to generate high quality (HQ) images directly tied to the low quality (LQ) images. Practically, the teacher model often reconstructs images with color distortions and blurred edges, indicating a flawed generative prior for this task. To correct these flaws and ensure conceptual alignment, a more suitable manifold must incorporate information from the images. While the most straightforward approach is to condition directly on the raw input images, their high information densities make the regularization process numerically unstable. To resolve this, we propose image-conditioned manifold regularization (ICM), a method that regularizes the output towards a manifold conditioned on the sparse yet essential structural information: a combination of colormap and Canny edges. ICM provides a task-aligned and stable regularization signal, thereby avoiding the instability of dense-conditioning and enhancing the final super-resolution quality. Our experiments confirm that the proposed regularization significantly enhances super-resolution performance, particularly in perceptual quality, demonstrating its effectiveness for real-world applications. We will release the source code of our work for reproducibility.

</details>


### [70] [TPCNet: Triple physical constraints for Low-light Image Enhancement](https://arxiv.org/abs/2511.22052)
*Jing-Yi Shi,Ming-Fei Li,Ling-An Wu*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于Kubelka-Munk理论的低光图像增强方法，称为TPCNet，该方法通过在特征空间中构建三重物理约束来提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Retinex的算法忽略了镜面反射，并在图像空间中构建物理约束，限制了模型的泛化能力。

Method: 基于Kubelka-Munk理论，保留镜面反射系数，并在成像过程中重新构建原始物理约束，从而构建光照、反射和检测之间的约束关系，即三重物理约束（TPCs）理论。在模型的特征空间中构建物理约束，得到TPCNet。

Result: 综合定量和定性基准和消融实验证实，这些约束有效地提高了性能指标和视觉质量，且没有引入新的参数。TPCNet在10个数据集上优于其他最先进的方法。

Conclusion: TPCNet通过在特征空间中构建三重物理约束，有效地提高了低光图像增强的性能。

Abstract: Low-light image enhancement is an essential computer vision task to improve image contrast and to decrease the effects of color bias and noise. Many existing interpretable deep-learning algorithms exploit the Retinex theory as the basis of model design. However, previous Retinex-based algorithms, that consider reflected objects as ideal Lambertian ignore specular reflection in the modeling process and construct the physical constraints in image space, limiting generalization of the model. To address this issue, we preserve the specular reflection coefficient and reformulate the original physical constraints in the imaging process based on the Kubelka-Munk theory, thereby constructing constraint relationship between illumination, reflection, and detection, the so-called triple physical constraints (TPCs)theory. Based on this theory, the physical constraints are constructed in the feature space of the model to obtain the TPC network (TPCNet). Comprehensive quantitative and qualitative benchmark and ablation experiments confirm that these constraints effectively improve the performance metrics and visual quality without introducing new parameters, and demonstrate that our TPCNet outperforms other state-of-the-art methods on 10 datasets.

</details>


### [71] [OralGPT-Omni: A Versatile Dental Multimodal Large Language Model](https://arxiv.org/abs/2511.22055)
*Jing Hao,Yuci Liang,Lizhuo Lin,Yuxuan Fan,Wenkai Zhou,Kaixin Guo,Zanting Ye,Yanpeng Sun,Xinyu Zhang,Yanqi Yang,Qiankun Li,Hao Tang,James Kit-Hon Tsoi,Linlin Shen,Kuo Feng Hung*

Main category: cs.CV

TL;DR: OralGPT-Omni是第一个牙科专用多模态大型语言模型，用于全面的牙科影像分析。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在牙科领域应用不足，受限于数据、专家标注、模态建模和可靠性等挑战。

Method: 构建TRACE-CoT数据集，结合四阶段训练范式，提升模型牙科图像理解能力。

Result: OralGPT-Omni在MMOral-Uni和MMOral-OPG基准测试中显著优于GPT-5。

Conclusion: 该研究推动了智能牙科的发展，为牙科影像分析的未来进步铺平了道路。

Abstract: Multimodal Large Language Models (MLLMs) have exhibited immense potential across numerous medical specialties; yet, dentistry remains underexplored, in part due to limited domain-specific data, scarce dental expert annotations, insufficient modality-specific modeling, and challenges in reliability. In this paper, we present OralGPT-Omni, the first dental-specialized MLLM designed for comprehensive and trustworthy analysis across diverse dental imaging modalities and clinical tasks. To explicitly capture dentists' diagnostic reasoning, we construct TRACE-CoT, a clinically grounded chain-of-thought dataset that mirrors dental radiologists' decision-making processes. This reasoning supervision, combined with our proposed four-stage training paradigm, substantially strengthens the model's capacity for dental image understanding and analysis. In parallel, we introduce MMOral-Uni, the first unified multimodal benchmark for dental image analysis. It comprises 2,809 open-ended question-answer pairs spanning five modalities and five tasks, offering a comprehensive evaluation suite to date for MLLMs in digital dentistry. OralGPT-Omni achieves an overall score of 51.84 on the MMOral-Uni benchmark and 45.31 on the MMOral-OPG benchmark, dramatically outperforming the scores of GPT-5. Our work promotes intelligent dentistry and paves the way for future advances in dental image analysis. All code, benchmark, and models will be made publicly available.

</details>


### [72] [DNA: Dual-branch Network with Adaptation for Open-Set Online Handwriting Generation](https://arxiv.org/abs/2511.22064)
*Tsai-Ling Huang,Nhat-Tuong Do-Tran,Ngoc-Hoang-Lam Le,Hong-Han Shuai,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 提出了一种用于在线手写生成 (OHG) 的双分支网络与适配 (DNA) 方法，可以生成训练中未见过的字符，尤其是在像中文这样的基于字形的语言中。


<details>
  <summary>Details</summary>
Motivation: 现有的 OHG 方法难以生成未见过的字符，特别是在像中文这样的基于字形的语言中，限制了它们的实际应用。

Method: 提出了一个双分支网络与适配 (DNA)，它包含一个自适应风格分支和一个自适应内容分支。风格分支学习笔画属性，而内容分支通过分解字符内容为结构信息和纹理细节来有效地推广到未见过的字符。

Result: DNA 模型非常适合未见过的 OHG 设置，实现了最先进的性能。

Conclusion: DNA 模型在未见过的在线手写生成任务上表现出色。

Abstract: Online handwriting generation (OHG) enhances handwriting recognition models by synthesizing diverse, human-like samples. However, existing OHG methods struggle to generate unseen characters, particularly in glyph-based languages like Chinese, limiting their real-world applicability. In this paper, we introduce our method for OHG, where the writer's style and the characters generated during testing are unseen during training. To tackle this challenge, we propose a Dual-branch Network with Adaptation (DNA), which comprises an adaptive style branch and an adaptive content branch. The style branch learns stroke attributes such as writing direction, spacing, placement, and flow to generate realistic handwriting. Meanwhile, the content branch is designed to generalize effectively to unseen characters by decomposing character content into structural information and texture details, extracted via local and global encoders, respectively. Extensive experiments demonstrate that our DNA model is well-suited for the unseen OHG setting, achieving state-of-the-art performance.

</details>


### [73] [WorldWander: Bridging Egocentric and Exocentric Worlds in Video Generation](https://arxiv.org/abs/2511.22098)
*Quanjian Song,Yiren Song,Kelly Peng,Yuan Gao,Mike Zheng Shou*

Main category: cs.CV

TL;DR: WorldWander是一个用于在视频生成中转换第一人称和第三人称视角的上下文学习框架。


<details>
  <summary>Details</summary>
Motivation: 弥合第一人称和第三人称视角对于电影制作、具身人工智能和世界模型至关重要，但目前尚未充分探索。

Method: WorldWander集成了上下文视角对齐和协作位置编码，以有效建模跨视角同步。

Result: WorldWander在视角同步、角色一致性和泛化方面表现出色，为第一人称和第三人称视频转换设定了新的基准。

Conclusion: WorldWander是一个有前景的框架，可以实现无缝的视角转换。

Abstract: Video diffusion models have recently achieved remarkable progress in realism and controllability. However, achieving seamless video translation across different perspectives, such as first-person (egocentric) and third-person (exocentric), remains underexplored. Bridging these perspectives is crucial for filmmaking, embodied AI, and world models. Motivated by this, we present WorldWander, an in-context learning framework tailored for translating between egocentric and exocentric worlds in video generation. Building upon advanced video diffusion transformers, WorldWander integrates (i) In-Context Perspective Alignment and (ii) Collaborative Position Encoding to efficiently model cross-view synchronization. To further support our task, we curate EgoExo-8K, a large-scale dataset containing synchronized egocentric-exocentric triplets from both synthetic and real-world scenarios. Experiments demonstrate that WorldWander achieves superior perspective synchronization, character consistency, and generalization, setting a new benchmark for egocentric-exocentric video translation.

</details>


### [74] [MRI-Based Brain Age Estimation with Supervised Contrastive Learning of Continuous Representation](https://arxiv.org/abs/2511.22102)
*Simon Joseph Clément Crête,Marta Kersten-Oertel,Yiming Xiao*

Main category: cs.CV

TL;DR: 提出了一种基于监督对比学习的脑年龄估计模型，利用Rank-N-Contrast (RNC)损失函数和T1w结构MRI，并通过Grad-RAM进行可视化解释。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的脑年龄估计方法难以捕捉神经形态变化的连续性，可能导致次优的特征表示和结果。

Method: 使用监督对比学习和Rank-N-Contrast (RNC)损失函数，结合T1w结构MRI进行脑年龄估计，并利用Grad-RAM进行可视化解释。

Result: 在有限的数据集上，平均绝对误差（MAE）为4.27岁，$R^2$为0.93，优于传统的深度回归方法，并且与使用更大训练数据的最先进方法相比，性能更好或相当。Grad-RAM揭示了与年龄回归相关的更细微的特征。

Conclusion: 该方法在阿尔茨海默病和帕金森病患者中，揭示了脑年龄差距与疾病严重程度之间的相关性，表明其作为神经退行性疾病生物标志物的潜力。

Abstract: MRI-based brain age estimation models aim to assess a subject's biological brain age based on information, such as neuroanatomical features. Various factors, including neurodegenerative diseases, can accelerate brain aging and measuring this phenomena could serve as a potential biomarker for clinical applications. While deep learning (DL)-based regression has recently attracted major attention, existing approaches often fail to capture the continuous nature of neuromorphological changes, potentially resulting in sub-optimal feature representation and results. To address this, we propose to use supervised contrastive learning with the recent Rank-N-Contrast (RNC) loss to estimate brain age based on widely used T1w structural MRI for the first time and leverage Grad-RAM to visually explain regression results. Experiments show that our proposed method achieves a mean absolute error (MAE) of 4.27 years and an $R^2$ of 0.93 with a limited dataset of training samples, significantly outperforming conventional deep regression with the same ResNet backbone while performing better or comparably with the state-of-the-art methods with significantly larger training data. Furthermore, Grad-RAM revealed more nuanced features related to age regression with the RNC loss than conventional deep regression. As an exploratory study, we employed the proposed method to estimate the gap between the biological and chronological brain ages in Alzheimer's Disease and Parkinson's disease patients, and revealed the correlation between the brain age gap and disease severity, demonstrating its potential as a biomarker in neurodegenerative disorders.

</details>


### [75] [MoE3D: Mixture of Experts meets Multi-Modal 3D Understanding](https://arxiv.org/abs/2511.22103)
*Yu Li,Yuenan Hou,Yingmei Wei,Xinge Zhu,Yuexin Ma,Wenqi Shao,Yanming Guo*

Main category: cs.CV

TL;DR: 提出了一种新的多模态3D理解框架MoE3D，该框架利用混合专家（MoE）结构来处理不同模态间的异构性和复杂性。


<details>
  <summary>Details</summary>
Motivation: 以往的多模态融合方法通常采用单一、密集的融合网络，难以处理模态间的显著异构性和复杂性，导致性能欠佳。

Method: 设计了一个基于MoE的Transformer，以更好地利用视觉特征中隐藏的互补信息。提出了信息聚合模块以进一步提高融合性能。采用Top-1门控机制，使一个专家处理具有专家组的特征，确保高效率。提出了一种渐进式预训练策略，以更好地利用语义和2D先验，从而使网络具有良好的初始化。

Result: MoE3D在四个流行的3D理解任务中取得了有竞争力的性能。值得注意的是，MoE3D在Multi3DRefer上超过了表现最佳的同类产品6.1 mIoU。

Conclusion: MoE3D通过引入MoE结构和渐进式预训练策略，有效地提升了多模态3D理解的性能。

Abstract: Multi-modal 3D understanding is a fundamental task in computer vision. Previous multi-modal fusion methods typically employ a single, dense fusion network, struggling to handle the significant heterogeneity and complexity across modalities, leading to suboptimal performance. In this paper, we propose MoE3D, which integrates Mixture of Experts (MoE) into the multi-modal learning framework. The core is that we deploy a set of specialized "expert" networks, each adept at processing a specific modality or a mode of cross-modal interaction. Specifically, the MoE-based transformer is designed to better utilize the complementary information hidden in the visual features. Information aggregation module is put forward to further enhance the fusion performance. Top-1 gating is employed to make one expert process features with expert groups, ensuring high efficiency. We further propose a progressive pre-training strategy to better leverage the semantic and 2D prior, thus equipping the network with good initialization. Our MoE3D achieves competitive performance across four prevalent 3D understanding tasks. Notably, our MoE3D surpasses the top-performing counterpart by 6.1 mIoU on Multi3DRefer.

</details>


### [76] [HyperST: Hierarchical Hyperbolic Learning for Spatial Transcriptomics Prediction](https://arxiv.org/abs/2511.22107)
*Chen Zhang,Yilu An,Ying Chen,Hao Li,Xitong Ling,Lihao Liu,Junjun He,Yuxiang Lin,Zihui Wang,Rongshan Yu*

Main category: cs.CV

TL;DR: HyperST: A framework for ST prediction that learns multi-level image-gene representations by modeling the data's inherent hierarchy within hyperbolic space.


<details>
  <summary>Details</summary>
Motivation: Existing methods mainly focus on spot-level image-to-gene matching but fail to leverage the full hierarchical structure of ST data, especially on the gene expression side, leading to incomplete image-gene alignment. Moreover, gene expression profiles contain more molecular details that may lack salient visual correlates in histological images, demanding a sophisticated representation learning approach to bridge this modality gap.

Method: The framework includes Multi-Level Representation Extractors to capture both spot-level and niche-level representations from each modality, and a Hierarchical Hyperbolic Alignment module to unify these representations, performing spatial alignment while hierarchically structuring image and gene embeddings.

Result: HyperST achieves state-of-the-art performance on four public datasets from different tissues.

Conclusion: HyperST paves the way for more scalable and accurate spatial transcriptomics prediction.

Abstract: Spatial Transcriptomics (ST) merges the benefits of pathology images and gene expression, linking molecular profiles with tissue structure to analyze spot-level function comprehensively. Predicting gene expression from histology images is a cost-effective alternative to expensive ST technologies. However, existing methods mainly focus on spot-level image-to-gene matching but fail to leverage the full hierarchical structure of ST data, especially on the gene expression side, leading to incomplete image-gene alignment. Moreover, a challenge arises from the inherent information asymmetry: gene expression profiles contain more molecular details that may lack salient visual correlates in histological images, demanding a sophisticated representation learning approach to bridge this modality gap. We propose HyperST, a framework for ST prediction that learns multi-level image-gene representations by modeling the data's inherent hierarchy within hyperbolic space, a natural geometric setting for such structures. First, we design a Multi-Level Representation Extractors to capture both spot-level and niche-level representations from each modality, providing context-aware information beyond individual spot-level image-gene pairs. Second, a Hierarchical Hyperbolic Alignment module is introduced to unify these representations, performing spatial alignment while hierarchically structuring image and gene embeddings. This alignment strategy enriches the image representations with molecular semantics, significantly improving cross-modal prediction. HyperST achieves state-of-the-art performance on four public datasets from different tissues, paving the way for more scalable and accurate spatial transcriptomics prediction.

</details>


### [77] [PROMPTMINER: Black-Box Prompt Stealing against Text-to-Image Generative Models via Reinforcement Learning and Fuzz Optimization](https://arxiv.org/abs/2511.22119)
*Mingzhe Li,Renhao Zhang,Zhiyang Wen,Siqi Pan,Bruno Castro da Silva,Juan Zhai,Shiqing Ma*

Main category: cs.CV

TL;DR: 提出了一个黑盒提示词窃取框架PROMPTMINER，用于从生成的图像中恢复文本提示词。


<details>
  <summary>Details</summary>
Motivation: 高质量的提示词变得越来越有价值，但也面临着安全和知识产权风险，例如提示词窃取攻击。

Method: 该框架将任务分解为两个阶段：基于强化学习的优化阶段以重建主要对象，以及模糊驱动的搜索阶段以恢复风格修饰符。

Result: 在多个数据集和扩散骨干网络上的实验表明，PROMPTMINER 取得了优异的成果，CLIP 相似度高达 0.958，SBERT 文本对齐度高达 0.751，超过了所有基线。

Conclusion: 即使应用于具有未知生成器的实际图像，PROMPTMINER 在 CLIP 相似性方面也优于最强的基线 7.5%，表现出更好的泛化能力，并在防御性扰动下保持了强大的性能，突出了显著的鲁棒性。

Abstract: Text-to-image (T2I) generative models such as Stable Diffusion and FLUX can synthesize realistic, high-quality images directly from textual prompts. The resulting image quality depends critically on well-crafted prompts that specify both subjects and stylistic modifiers, which have become valuable digital assets. However, the rising value and ubiquity of high-quality prompts expose them to security and intellectual-property risks. One key threat is the prompt stealing attack, i.e., the task of recovering the textual prompt that generated a given image. Prompt stealing enables unauthorized extraction and reuse of carefully engineered prompts, yet it can also support beneficial applications such as data attribution, model provenance analysis, and watermarking validation. Existing approaches often assume white-box gradient access, require large-scale labeled datasets for supervised training, or rely solely on captioning without explicit optimization, limiting their practicality and adaptability. To address these challenges, we propose PROMPTMINER, a black-box prompt stealing framework that decouples the task into two phases: (1) a reinforcement learning-based optimization phase to reconstruct the primary subject, and (2) a fuzzing-driven search phase to recover stylistic modifiers. Experiments across multiple datasets and diffusion backbones demonstrate that PROMPTMINER achieves superior results, with CLIP similarity up to 0.958 and textual alignment with SBERT up to 0.751, surpassing all baselines. Even when applied to in-the-wild images with unknown generators, it outperforms the strongest baseline by 7.5 percent in CLIP similarity, demonstrating better generalization. Finally, PROMPTMINER maintains strong performance under defensive perturbations, highlighting remarkable robustness. Code: https://github.com/aaFrostnova/PromptMiner

</details>


### [78] [GoPrune: Accelerated Structured Pruning with $\ell_{2,p}$-Norm Optimization](https://arxiv.org/abs/2511.22120)
*Li Xu,Xianchao Xiu*

Main category: cs.CV

TL;DR: 提出了一种名为GoPrune的加速结构化剪枝方法，用于压缩卷积神经网络，以便在资源受限的边缘设备上部署。


<details>
  <summary>Details</summary>
Motivation: 卷积神经网络（CNN）的深度增加导致存储和计算成本迅速增加，严重阻碍了它们在资源受限的边缘设备上的部署。现有的剪枝工作只考虑了p∈(0, 1)的非结构化剪枝，计算效率低。

Method: 采用$\\ell_{2,p}$-norm进行稀疏网络学习，其中p的值扩展到[0, 1)。开发了一种基于近端交替最小化（PAM）的有效优化算法，得到的子问题具有闭式解，从而提高了压缩效率。

Result: 在CIFAR数据集上使用ResNet和VGG模型进行的实验表明了该方法在网络剪枝方面的优越性能。

Conclusion: GoPrune方法在网络剪枝方面表现出色，代码已开源。

Abstract: Convolutional neural networks (CNNs) suffer from rapidly increasing storage and computational costs as their depth grows, which severely hinders their deployment on resource-constrained edge devices. Pruning is a practical approach for network compression, among which structured pruning is the most effective for inference acceleration. Although existing work has applied the $\ell_p$-norm to pruning, it only considers unstructured pruning with $p\in (0, 1)$ and has low computational efficiency. To overcome these limitations, we propose an accelerated structured pruning method called GoPrune. Our method employs the $\ell_{2,p}$-norm for sparse network learning, where the value of $p$ is extended to $[0, 1)$. Moreover, we develop an efficient optimization algorithm based on the proximal alternating minimization (PAM), and the resulting subproblems enjoy closed-form solutions, thus improving compression efficiency. Experiments on the CIFAR datasets using ResNet and VGG models demonstrate the superior performance of the proposed method in network pruning. Our code is available at https://github.com/xianchaoxiu/GoPrune.

</details>


### [79] [Cue3D: Quantifying the Role of Image Cues in Single-Image 3D Generation](https://arxiv.org/abs/2511.22121)
*Xiang Li,Zirui Wang,Zixuan Huang,James M. Rehg*

Main category: cs.CV

TL;DR: Cue3D: A model-agnostic framework for quantifying the influence of individual image cues in single-image 3D generation.


<details>
  <summary>Details</summary>
Motivation: Understanding which image cues deep generative models exploit for single-image 3D generation.

Method: Systematically perturbing cues such as shading, texture, silhouette, perspective, edges, and local continuity, and measuring their impact on 3D output quality.

Result: Shape meaningfulness, not texture, dictates generalization. Geometric cues, particularly shading, are crucial for 3D generation. Over-reliance on provided silhouettes and diverse sensitivities to cues such as perspective and local continuity across model families were identified.

Conclusion: Cue3D advances our understanding of how modern 3D networks leverage classical vision cues, and offers directions for developing more transparent, robust, and controllable single-image 3D generation models.

Abstract: Humans and traditional computer vision methods rely on a diverse set of monocular cues to infer 3D structure from a single image, such as shading, texture, silhouette, etc. While recent deep generative models have dramatically advanced single-image 3D generation, it remains unclear which image cues these methods actually exploit. We introduce Cue3D, the first comprehensive, model-agnostic framework for quantifying the influence of individual image cues in single-image 3D generation. Our unified benchmark evaluates seven state-of-the-art methods, spanning regression-based, multi-view, and native 3D generative paradigms. By systematically perturbing cues such as shading, texture, silhouette, perspective, edges, and local continuity, we measure their impact on 3D output quality. Our analysis reveals that shape meaningfulness, not texture, dictates generalization. Geometric cues, particularly shading, are crucial for 3D generation. We further identify over-reliance on provided silhouettes and diverse sensitivities to cues such as perspective and local continuity across model families. By dissecting these dependencies, Cue3D advances our understanding of how modern 3D networks leverage classical vision cues, and offers directions for developing more transparent, robust, and controllable single-image 3D generation models.

</details>


### [80] [GA2-CLIP: Generic Attribute Anchor for Efficient Prompt Tuningin Video-Language Models](https://arxiv.org/abs/2511.22125)
*Bin Wang,Ruotong Hu,Wenqian Wang,Wentong Li,Mingliang Gao,Runmin Cong,Wei Zhang*

Main category: cs.CV

TL;DR: 提出了一种即插即用的耦合提示学习框架，以优化视频任务中视觉语言模型的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 微调视频任务会削弱模型对未见类的泛化能力。现有方法试图通过调整手工提示和软提示之间的差距来缓解这种遗忘效应，但也削弱了软提示的学习能力。为了解决这个挑战，我们旨在通过引入外部监督提示来减轻微调期间的语义空间缩小。

Method: 对于文本提示，我们引入了来自其他数据集的预训练提示作为硬提示tokens。这些与软提示tokens连接，并通过可学习的映射层耦合。此外，我们引入了一组精心设计的无关视频集和负提示，作为通用属性锚点，以保持预训练语义空间中属性的通用相关性，从而保持泛化能力。

Result: 在视频任务上的实验表明，我们的方法在泛化基准测试中明显优于最先进的提示调优方法，尤其是在基类到新类的预测方面。

Conclusion: 该方法显著优于现有的提示调优方法，尤其是在基类到新类的预测方面，证明了其有效性。

Abstract: Visual and textual soft prompt tuning can effectively improve the adaptability of Vision-Language Models (VLMs) in downstream tasks. However, fine-tuning on video tasks impairs the model's generalization ability to unseen classes. Existing methods attempt to mitigate this forgetting effect by regularizing the gap between hand-crafted prompts and soft prompts, but this also weakens the learning ability of soft prompts. To address this challenge, we propose a plug-and-play coupling prompt learning framework to optimize the generalization performance of V-L models in video tasks, with the core motivation of mitigating semantic space narrowing during fine-tuning by introducing an externally supervised prompt. Specifically, for textual prompts, we introduce pre-trained prompts from other datasets as hard prompt tokens. These are concatenated with soft prompt tokens and coupled via a learnable mapping layer. This competitive prompting approach prevents the semantic space from overfitting to supervised categories. In addition, we introduce a set of well-designed irrelevant video sets and negative prompts as generic attribute anchors to maintain the generic relevance of the attributes in the pre-trained semantic space, thus preserving the generalization ability. Experiments on video tasks demonstrate that our method significantly outperforms state-of-the-art prompt tuning approaches across generalization benchmarks, particularly on base-to-new class prediction.

</details>


### [81] [Autonomous labeling of surgical resection margins using a foundation model](https://arxiv.org/abs/2511.22131)
*Xilin Yang,Musa Aydin,Yuhong Lu,Sahan Yoruc Selcuk,Bijie Bai,Yijie Zhang,Andrew Birkeland,Katjana Ehrlich,Julien Bec,Laura Marcu,Nir Pillar,Aydogan Ozcan*

Main category: cs.CV

TL;DR: 提出了一种虚拟墨水网络 (VIN)，可以自动定位全玻片图像上的手术切割面，减少对墨水的依赖并标准化以边缘为中心的审查。


<details>
  <summary>Details</summary>
Motivation: 评估切缘对于病理标本评估至关重要，并且对患者预后有重要影响。目前的做法是使用物理墨水，但应用方式各不相同，并且烧灼伪影可能会模糊组织学切片上的真实边缘。

Method: VIN 使用冻结的基础模型作为特征提取器，以及一个紧凑的两层多层感知器，该感知器经过训练，可用于对烧灼一致特征进行patch级分类。

Result: 在对先前未见过的切片的盲测中，VIN 产生了与连续切片上的专家注释在质量上对齐的连贯边缘覆盖。定量地，整个测试集的区域级准确度约为 73.3%，误差主要集中在不破坏全玻片边缘图连续性的有限区域。

Conclusion: 这些结果表明，VIN 捕获了与烧灼相关的组织形态学，并且可以提供可重复的、无墨水的边缘描绘，适合集成到常规数字病理工作流程中以及用于边缘距离的下游测量。

Abstract: Assessing resection margins is central to pathological specimen evaluation and has profound implications for patient outcomes. Current practice employs physical inking, which is applied variably, and cautery artifacts can obscure the true margin on histological sections. We present a virtual inking network (VIN) that autonomously localizes the surgical cut surface on whole-slide images, reducing reliance on inks and standardizing margin-focused review. VIN uses a frozen foundation model as the feature extractor and a compact two-layer multilayer perceptron trained for patch-level classification of cautery-consistent features. The dataset comprised 120 hematoxylin and eosin (H&E) stained slides from 12 human tonsil tissue blocks, resulting in ~2 TB of uncompressed raw image data, where a board-certified pathologist provided boundary annotations. In blind testing with 20 slides from previously unseen blocks, VIN produced coherent margin overlays that qualitatively aligned with expert annotations across serial sections. Quantitatively, region-level accuracy was ~73.3% across the test set, with errors largely confined to limited areas that did not disrupt continuity of the whole-slide margin map. These results indicate that VIN captures cautery-related histomorphology and can provide a reproducible, ink-free margin delineation suitable for integration into routine digital pathology workflows and for downstream measurement of margin distances.

</details>


### [82] [DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action](https://arxiv.org/abs/2511.22134)
*Zhen Fang,Zhuoyang Liu,Jiaming Liu,Hao Chen,Yu Zeng,Shiting Huang,Zehui Chen,Lin Chen,Shanghang Zhang,Feng Zhao*

Main category: cs.CV

TL;DR: 这篇论文提出了DualVLA，旨在解决通用视觉-语言-动作（VLA）模型在获得更广泛的推理能力时，行动性能下降的问题（行动退化）。


<details>
  <summary>Details</summary>
Motivation: 构建具有强大推理能力的通用视觉-语言-动作（VLA）模型，但发现由此产生的推理VLA的行动性能通常会下降。

Method: 提出了DualVLA，通过精心设计的后训练来增强行动性能，同时保留推理能力。首先，引入双层数据修剪方法，消除冗余的具身推理，防止其对行动学习产生不利影响。其次，设计了一种双教师自适应蒸馏策略，为不同的数据领域分配不同的监督信号，同时保持推理能力。

Result: DualVLA在SimplerEnv中实现了61.0的平均成功率，在八个竞争性多模态基准测试中实现了65.4的平均分。

Conclusion: DualVLA在精确的动作执行和多模态理解之间实现了更强的平衡。

Abstract: To build a generalizable Vision-Language-Action (VLA) model with strong reasoning ability, a common strategy is to first train a specialist VLA on robot demonstrations to acquire reliable manipulation skills, and then incorporate mixed annotated robot data together with multimodal data to restore broader reasoning capabilities. However, we observe that the resulting reasoning VLA often suffers from degraded action performance compared to the specialist model before fine-tuning, a phenomenon we refer to as action degeneration. To address this issue, we propose DualVLA, which enhances action performance through carefully designed post-training while still preserving reasoning capability. We first introduce a dual-layer data pruning method that removes redundant embodied reasoning, preventing it from adversely influencing action learning. To further strengthen action generation, we design a dual-teacher adaptive distillation strategy that assigns different supervision signals to different data domains while maintaining reasoning ability. To fill the evaluation gap for generalist VLAs, we also propose VLA Score, which decouples VLA capability into reasoning, intention, action, and alignment dimensions for a more fine-grained assessment. Experiments show that DualVLA achieves an average success rate of 61.0 in SimplerEnv and an average score of 65.4 across eight competitive multimodal benchmarks, demonstrating a stronger balance between precise action execution and multimodal understanding. Project Website: https://costaliya.github.io/DualVLA/.

</details>


### [83] [EASL: Multi-Emotion Guided Semantic Disentanglement for Expressive Sign Language Generation](https://arxiv.org/abs/2511.22135)
*Yanchao Zhao,Jihao Zhu,Yu Liu,Weizhuo Chen,Yuling Yang,Kun Peng*

Main category: cs.CV

TL;DR: EASL: 一种用于手语生成的多情感引导架构，能生成更自然和富有表现力的手语视频。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的手语生成方法忽略了情感表达，导致输出缺乏自然性和表现力。

Method: 提出EASL，一种多情感引导的生成架构，采用情感-语义解耦模块和渐进式训练，在姿势解码过程中，情感表征引导语义交互，生成带有7类情感置信度的手语姿势。

Result: EASL通过整合多情感信息，实现了优于所有对比基线的姿势准确性，并能有效适应扩散模型以生成富有表现力的手语视频。

Conclusion: EASL有效地提升了手语生成的情感表达能力和自然性。

Abstract: Large language models have revolutionized sign language generation by automatically transforming text into high-quality sign language videos, providing accessible communication for the Deaf community. However, existing LLM-based approaches prioritize semantic accuracy while overlooking emotional expressions, resulting in outputs that lack naturalness and expressiveness. We propose EASL (Emotion-Aware Sign Language), a multi-emotion-guided generation architecture for fine-grained emotional integration. We introduce emotion-semantic disentanglement modules with progressive training to separately extract semantic and affective features. During pose decoding, the emotional representations guide semantic interaction to generate sign poses with 7-class emotion confidence scores, enabling emotional expression recognition. Experimental results demonstrate that EASL achieves pose accuracy superior to all compared baselines by integrating multi-emotion information and effectively adapts to diffusion models to generate expressive sign language videos.

</details>


### [84] [SemOD: Semantic Enabled Object Detection Network under Various Weather Conditions](https://arxiv.org/abs/2511.22142)
*Aiyinsi Zuo,Zhaoliang Zheng*

Main category: cs.CV

TL;DR: 提出了一种新的语义驱动网络，用于在各种天气条件下进行目标检测，通过利用语义信息来提升图像质量和目标识别。


<details>
  <summary>Details</summary>
Motivation: 现有的相机感知模型在恶劣天气下表现不佳，且难以适应各种天气变化。

Method: 该方法包含一个预处理单元（PPU）和一个检测单元（DTU），PPU利用U型网络和语义信息来优化图像，DTU整合语义信息，使用改进的YOLO网络进行目标检测。

Result: 该方法在不同天气基准数据集上，mAP提高了1.47%到8.80%。

Conclusion: 语义信息在图像增强和目标检测方面具有优势，为提升目标检测性能提供了一种全面的方法。

Abstract: In the field of autonomous driving, camera-based perception models are mostly trained on clear weather data. Models that focus on addressing specific weather challenges are unable to adapt to various weather changes and primarily prioritize their weather removal characteristics. Our study introduces a semantic-enabled network for object detection in diverse weather conditions. In our analysis, semantics information can enable the model to generate plausible content for missing areas, understand object boundaries, and preserve visual coherency and realism across both filled-in and existing portions of the image, which are conducive to image transformation and object recognition. Specific in implementation, our architecture consists of a Preprocessing Unit (PPU) and a Detection Unit (DTU), where the PPU utilizes a U-shaped net enriched by semantics to refine degraded images, and the DTU integrates this semantic information for object detection using a modified YOLO network. Our method pioneers the use of semantic data for all-weather transformations, resulting in an increase between 1.47\% to 8.80\% in mAP compared to existing methods across benchmark datasets of different weather. This highlights the potency of semantics in image enhancement and object detection, offering a comprehensive approach to improving object detection performance. Code will be available at https://github.com/EnisZuo/SemOD.

</details>


### [85] [Stacked Ensemble of Fine-Tuned CNNs for Knee Osteoarthritis Severity Grading](https://arxiv.org/abs/2511.22143)
*Adarsh Gupta,Japleen Kaur,Tanvi Doshi,Teena Sharma,Nishchal K. Verma,Shantaram Vasikarla*

Main category: cs.CV

TL;DR: 本研究旨在开发一种基于堆叠集成模型的卷积神经网络 (CNN)，用于自动检测膝骨关节炎 (KOA) 及其严重程度。


<details>
  <summary>Details</summary>
Motivation: 传统的膝骨关节炎 (KOA) 评估方法依赖于X射线图像分析和KL分级系统，需要专业知识、耗时且易受主观影响，导致诊断不准确。

Method: 该研究提出了一种堆叠集成模型，由 MobileNetV2、YOLOv8 和 DenseNet201 等预训练架构作为基础学习器，以及 CatBoost 作为元学习器。

Result: 该模型在多类分类中达到了 73% 的平衡测试准确率，在二元分类中达到了 87.5% 的平衡测试准确率，优于现有文献中的先前研究。

Conclusion: 该研究提出的堆叠集成模型能够有效地检测KOA的存在并精确分级，具有较高的准确性。

Abstract: Knee Osteoarthritis (KOA) is a musculoskeletal condition that can cause significant limitations and impairments in daily activities, especially among older individuals. To evaluate the severity of KOA, typically, X-ray images of the affected knee are analyzed, and a grade is assigned based on the Kellgren-Lawrence (KL) grading system, which classifies KOA severity into five levels, ranging from 0 to 4. This approach requires a high level of expertise and time and is susceptible to subjective interpretation, thereby introducing potential diagnostic inaccuracies. To address this problem a stacked ensemble model of fine-tuned Convolutional Neural Networks (CNNs) was developed for two classification tasks: a binary classifier for detecting the presence of KOA, and a multiclass classifier for precise grading across the KL spectrum. The proposed stacked ensemble model consists of a diverse set of pre-trained architectures, including MobileNetV2, You Only Look Once (YOLOv8), and DenseNet201 as base learners and Categorical Boosting (CatBoost) as the meta-learner. This proposed model had a balanced test accuracy of 73% in multiclass classification and 87.5% in binary classification, which is higher than previous works in extant literature.

</details>


### [86] [RemedyGS: Defend 3D Gaussian Splatting against Computation Cost Attacks](https://arxiv.org/abs/2511.22147)
*Yanping Li,Zhening Liu,Zijian Li,Zehong Lin,Jun Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为 RemedyGS 的黑盒防御框架，以应对 3D 高斯溅射中的计算成本攻击。


<details>
  <summary>Details</summary>
Motivation: 现有 3D 高斯溅射技术存在漏洞，容易遭受计算成本攻击，导致资源占用和拒绝服务。

Method: 该框架包含一个检测器来识别受攻击的输入图像，以及一个净化器来恢复良性图像，并通过对抗训练来增强防御效果。

Result: 实验结果表明，该框架能有效防御 3DGS 系统中的白盒、黑盒和自适应攻击，并在安全性和效用方面都取得了最佳性能。

Conclusion: RemedyGS 是首个有效且全面的黑盒防御框架，能够保护 3DGS 重建系统和服务免受计算成本攻击。

Abstract: As a mainstream technique for 3D reconstruction, 3D Gaussian splatting (3DGS) has been applied in a wide range of applications and services. Recent studies have revealed critical vulnerabilities in this pipeline and introduced computation cost attacks that lead to malicious resource occupancies and even denial-of-service (DoS) conditions, thereby hindering the reliable deployment of 3DGS. In this paper, we propose the first effective and comprehensive black-box defense framework, named RemedyGS, against such computation cost attacks, safeguarding 3DGS reconstruction systems and services. Our pipeline comprises two key components: a detector to identify the attacked input images with poisoned textures and a purifier to recover the benign images from their attacked counterparts, mitigating the adverse effects of these attacks. Moreover, we incorporate adversarial training into the purifier to enforce distributional alignment between the recovered and original natural images, thereby enhancing the defense efficacy. Experimental results demonstrate that our framework effectively defends against white-box, black-box, and adaptive attacks in 3DGS systems, achieving state-of-the-art performance in both safety and utility.

</details>


### [87] [IMTalker: Efficient Audio-driven Talking Face Generation with Implicit Motion Transfer](https://arxiv.org/abs/2511.22167)
*Bo Chen,Tao Liu,Qi Chen,Xie Chen,Zilong Zheng*

Main category: cs.CV

TL;DR: IMTalker通过隐式运动传递实现高效、高保真的说话人脸生成，优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有说话人脸生成方法依赖于显式光流和局部扭曲，无法模拟复杂的全局运动，导致身份漂移。

Method: 提出IMTalker框架，用交叉注意力机制替代传统的基于光流的扭曲，在统一的潜在空间中隐式地模拟运动差异和身份对齐。引入身份自适应模块，将运动潜在变量投影到个性化空间，确保运动和身份之间的清晰解耦。使用轻量级的流匹配运动生成器从音频、姿势和视线提示中生成生动且可控的隐式运动向量。

Result: 在运动精度、身份保持和音频-嘴唇同步方面超过了以往的方法，实现了最先进的质量，在 RTX 4090 GPU 上以 40 FPS（视频驱动）和 42 FPS（音频驱动）的速度运行。

Conclusion: IMTalker框架实现了高效和高保真的说话人脸生成。

Abstract: Talking face generation aims to synthesize realistic speaking portraits from a single image, yet existing methods often rely on explicit optical flow and local warping, which fail to model complex global motions and cause identity drift. We present IMTalker, a novel framework that achieves efficient and high-fidelity talking face generation through implicit motion transfer. The core idea is to replace traditional flow-based warping with a cross-attention mechanism that implicitly models motion discrepancy and identity alignment within a unified latent space, enabling robust global motion rendering. To further preserve speaker identity during cross-identity reenactment, we introduce an identity-adaptive module that projects motion latents into personalized spaces, ensuring clear disentanglement between motion and identity. In addition, a lightweight flow-matching motion generator produces vivid and controllable implicit motion vectors from audio, pose, and gaze cues. Extensive experiments demonstrate that IMTalker surpasses prior methods in motion accuracy, identity preservation, and audio-lip synchronization, achieving state-of-the-art quality with superior efficiency, operating at 40 FPS for video-driven and 42 FPS for audio-driven generation on an RTX 4090 GPU. We will release our code and pre-trained models to facilitate applications and future research.

</details>


### [88] [Real-Time Long Horizon Air Quality Forecasting via Group-Relative Policy Optimization](https://arxiv.org/abs/2511.22169)
*Inha Kang,Eunki Kim,Wonjeong Ryu,Jaeyo Shin,Seungjun Yu,Yoon-Hee Kang,Seongeun Jeong,Eunhye Kim,Soontae Kim,Hyunjung Shim*

Main category: cs.CV

TL;DR: 本研究针对东亚地区PM浓度长期预测的挑战，构建了CMAQ-OBS数据集，并提出了Group-Relative Policy Optimization (GRPO) 算法，以提升预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以兼顾全局性和区域特殊性，且标准目标函数无法反映非对称的操作成本，导致误报率高。

Method: 构建了CMAQ-OBS数据集，并提出了基于类别奖励和课程rollout的Group-Relative Policy Optimization (GRPO) 算法。

Result: 实验结果表明，该框架显著提高了预测的可靠性，误报率降低了47.3%，同时保持了具有竞争力的F1分数。

Conclusion: 该模型在长时间预测情景下，对于实际的空气质量预测系统是有效的。

Abstract: Accurate long horizon forecasting of particulate matter (PM) concentration fields is essential for operational public health decisions. However, achieving reliable forecasts remains challenging in regions with complex terrain and strong atmospheric dynamics such as East Asia. While foundation models such as Aurora offer global generality, they often miss region-specific dynamics and rely on non-real-time inputs, limiting their practical utility for localized warning systems. To address this gap, we construct and release the real-world observations and high-resolution CMAQ-OBS dataset for East Asia, reducing regional error by 59.5% and enabling real-time 48-120 hour forecasts critical for public health alerts. However, standard point-wise objectives cannot reflect asymmetric operational costs, where false alarms deteriorate public trust while missed severe events endanger populations. This cost mismatch causes SFT models to over-predict and yield high False Alarm Rates. We introduce Group-Relative Policy Optimization (GRPO) with class-wise rewards and curriculum rollout to align predictions with operational priorities. Experimental results demonstrate that our framework significantly improves the reliability of the forecast. Compared to the SFT-only baseline, our model reduces the False Alarm Rate by 47.3% while achieving a competitive F1-score, proving its effectiveness for practical, real-world air quality forecasting systems on long lead time scenarios.

</details>


### [89] [Partially Shared Concept Bottleneck Models](https://arxiv.org/abs/2511.22170)
*Delong Zhao,Qiang Huang,Di Yan,Yiqun Sun,Jun Yu*

Main category: cs.CV

TL;DR: PS-CBM: A new concept bottleneck model that improves accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing concept bottleneck models have poor visual grounding, concept redundancy and lack metrics to balance accuracy and compactness.

Method: A partially shared CBM framework with multimodal concept generator, partially shared concept strategy and concept-efficient accuracy metric.

Result: PS-CBM outperforms state-of-the-art CBMs on eleven datasets, improving classification accuracy by 1.0%-7.4% and CEA by 2.0%-9.5%.

Conclusion: PS-CBM is effective in achieving both high accuracy and strong interpretability.

Abstract: Concept Bottleneck Models (CBMs) enhance interpretability by introducing a layer of human-understandable concepts between inputs and predictions. While recent methods automate concept generation using Large Language Models (LLMs) and Vision-Language Models (VLMs), they still face three fundamental challenges: poor visual grounding, concept redundancy, and the absence of principled metrics to balance predictive accuracy and concept compactness. We introduce PS-CBM, a Partially Shared CBM framework that addresses these limitations through three core components: (1) a multimodal concept generator that integrates LLM-derived semantics with exemplar-based visual cues; (2) a Partially Shared Concept Strategy that merges concepts based on activation patterns to balance specificity and compactness; and (3) Concept-Efficient Accuracy (CEA), a post-hoc metric that jointly captures both predictive accuracy and concept compactness. Extensive experiments on eleven diverse datasets show that PS-CBM consistently outperforms state-of-the-art CBMs, improving classification accuracy by 1.0%-7.4% and CEA by 2.0%-9.5%, while requiring significantly fewer concepts. These results underscore PS-CBM's effectiveness in achieving both high accuracy and strong interpretability.

</details>


### [90] [BrepGPT: Autoregressive B-rep Generation with Voronoi Half-Patch](https://arxiv.org/abs/2511.22171)
*Pu Li,Wenhao Zhang,Weize Quan,Biao Zhang,Peter Wonka,Dong-Ming Yan*

Main category: cs.CV

TL;DR: BrepGPT: 一种用于 B-rep 生成的单阶段自回归框架，通过 Voronoi Half-Patch (VHP) 表示将 B-rep 分解为统一的局部单元，并使用 Transformer 自回归地预测 token，实现 unconditional B-rep 生成的 SOTA 性能，并在各种应用中展示了多功能性。


<details>
  <summary>Details</summary>
Motivation: 现有的生成方法依赖于级联多阶段网络，导致误差累积和计算效率低下。

Method: 提出 Voronoi Half-Patch (VHP) 表示，将 B-rep 分解为统一的局部单元，并利用双 VQ-VAE 将顶点拓扑和 Voronoi Half-Patches 编码为基于顶点的 token，然后训练一个 decoder-only Transformer 来自回归地预测这些 token。

Result: BrepGPT 在 unconditional B-rep 生成方面实现了 state-of-the-art 的性能，并在从类别标签、点云、文本描述和图像的条件生成，以及 B-rep 自动完成和插值等各种应用中展示了多功能性。

Conclusion: BrepGPT 是一种用于 B-rep 生成的有效且通用的框架。

Abstract: Boundary representation (B-rep) is the de facto standard for CAD model representation in modern industrial design. The intricate coupling between geometric and topological elements in B-rep structures has forced existing generative methods to rely on cascaded multi-stage networks, resulting in error accumulation and computational inefficiency. We present BrepGPT, a single-stage autoregressive framework for B-rep generation. Our key innovation lies in the Voronoi Half-Patch (VHP) representation, which decomposes B-reps into unified local units by assigning geometry to nearest half-edges and sampling their next pointers. Unlike hierarchical representations that require multiple distinct encodings for different structural levels, our VHP representation facilitates unifying geometric attributes and topological relations in a single, coherent format. We further leverage dual VQ-VAEs to encode both vertex topology and Voronoi Half-Patches into vertex-based tokens, achieving a more compact sequential encoding. A decoder-only Transformer is then trained to autoregressively predict these tokens, which are subsequently mapped to vertex-based features and decoded into complete B-rep models. Experiments demonstrate that BrepGPT achieves state-of-the-art performance in unconditional B-rep generation. The framework also exhibits versatility in various applications, including conditional generation from category labels, point clouds, text descriptions, and images, as well as B-rep autocompletion and interpolation.

</details>


### [91] [Guiding the Inner Eye: A Framework for Hierarchical and Flexible Visual Grounded Reasoning](https://arxiv.org/abs/2511.22172)
*Zhaoyang Wei,Wenchao Ding,Yanchao Hao,Xi Chen*

Main category: cs.CV

TL;DR: GRiP: A novel two-stage training framework that cultivates robust and flexible visual grounded reasoning by explicitly guiding the model's perceptual focus and logical pathways.


<details>
  <summary>Details</summary>
Motivation: Current methods are often trapped between the instability of end-to-end reinforcement learning (RL) and the rigidity of supervised fine-tuning (SFT), leading to models that either struggle to learn or lack the cognitive flexibility required for complex, real-world scenes.

Method: A novel two-stage training framework that cultivates robust and flexible visual grounded reasoning by explicitly guiding the model's perceptual focus and logical pathways. GRiP's core lies in its cognitive-enhanced RL stage, which features two key innovations: (1) a Salience-Weighted IoU Reward and (2) a Multi-Heuristic Reward.

Result: GRiP demonstrates significant performance gains across multiple challenging benchmarks, achieving state-of-the-art results among open-source models on the highly challenging TreeBench and V* Bench.

Conclusion: Moving beyond simplistic rewards and instead guiding models with cognitively-inspired signals for what to see and how to think is crucial for unlocking the next level of multimodal intelligence.

Abstract: Models capable of "thinking with images" by dynamically grounding their reasoning in visual evidence represent a major leap in multimodal AI. However, replicating and advancing this ability is non-trivial, with current methods often trapped between the instability of end-to-end reinforcement learning (RL) and the rigidity of supervised fine-tuning (SFT). This leads to models that either struggle to learn or lack the cognitive flexibility required for complex, real-world scenes. To navigate this dilemma, we introduce GRiP (Guided Reasoning and Perception), a novel two-stage training framework that cultivates robust and flexible visual grounded reasoning by explicitly guiding the model's perceptual focus and logical pathways. GRiP's core lies in its cognitive-enhanced RL stage, which features two key innovations: (1) a Salience-Weighted IoU Reward that incentivizes the model to prioritize the localization of mission-critical objects over trivial distractors, and (2) a Multi-Heuristic Reward that encourages cognitive flexibility by rewarding diverse yet logically valid reasoning pathways. Initialized from the Qwen2.5-VL-7B model, GRiP demonstrates significant performance gains across multiple challenging benchmarks. It achieves state-of-the-art results among open-source models on the highly challenging TreeBench and V* Bench, proving its effectiveness in complex visual reasoning. Our work demonstrates that moving beyond simplistic rewards and instead guiding models with cognitively-inspired signals for what to see and how to think is crucial for unlocking the next level of multimodal intelligence. The code will be made publicly available.

</details>


### [92] [Enhanced Graph Convolutional Network with Chebyshev Spectral Graph and Graph Attention for Autism Spectrum Disorder Classification](https://arxiv.org/abs/2511.22178)
*Adnan Ferdous Ashrafi,Hasanul Kabir*

Main category: cs.CV

TL;DR: 本研究提出了一种基于图卷积网络(GCN)的模型，该模型结合了切比雪夫谱图卷积和图注意力网络(GAT)，以提高利用多模态神经影像和表型数据对自闭症谱系障碍(ASD)的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 早期和客观地诊断自闭症谱系障碍(ASD)非常困难，因为它是一种复杂的神经发育障碍，其症状表现和神经学基础各不相同。

Method: 该模型利用ABIDE I数据集，该数据集包含来自870名患者的静息状态功能MRI (rs-fMRI)、结构MRI (sMRI)和表型变量，该模型利用多分支架构，在通过连接合并之前分别处理每种模态。图结构使用基于站点的相似性进行编码，以生成人口图，这有助于理解个体之间的关系连接。切比雪夫多项式滤波器提供具有较低计算复杂性的局部谱学习，而GAT层通过注意加权聚合周围信息来增加节点表示。

Result: 广泛的试验表明，增强的模型具有优越性，在整个数据集上实现了74.82%的测试精度和0.82的AUC，超过了多个最先进的基线，包括传统GCN、基于自动编码器的深度神经网络和多模态cnn。

Conclusion: 提出的模型提高了自闭症谱系障碍的分类准确率。

Abstract: ASD is a complicated neurodevelopmental disorder marked by variation in symptom presentation and neurological underpinnings, making early and objective diagnosis extremely problematic. This paper presents a Graph Convolutional Network (GCN) model, incorporating Chebyshev Spectral Graph Convolution and Graph Attention Networks (GAT), to increase the classification accuracy of ASD utilizing multimodal neuroimaging and phenotypic data. Leveraging the ABIDE I dataset, which contains resting-state functional MRI (rs-fMRI), structural MRI (sMRI), and phenotypic variables from 870 patients, the model leverages a multi-branch architecture that processes each modality individually before merging them via concatenation. Graph structure is encoded using site-based similarity to generate a population graph, which helps in understanding relationship connections across individuals. Chebyshev polynomial filters provide localized spectral learning with lower computational complexity, whereas GAT layers increase node representations by attention-weighted aggregation of surrounding information. The proposed model is trained using stratified five-fold cross-validation with a total input dimension of 5,206 features per individual. Extensive trials demonstrate the enhanced model's superiority, achieving a test accuracy of 74.82\% and an AUC of 0.82 on the entire dataset, surpassing multiple state-of-the-art baselines, including conventional GCNs, autoencoder-based deep neural networks, and multimodal CNNs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [93] [Aligning Artificial Superintelligence via a Multi-Box Protocol](https://arxiv.org/abs/2511.21779)
*Avraham Yair Negozio*

Main category: cs.AI

TL;DR: 提出了一种通过多个隔离系统之间的相互验证来对齐人工超智能（ASI）的新协议，这些系统进行自我修改以实现对齐。


<details>
  <summary>Details</summary>
Motivation: 为了对齐人工超智能（ASI）。

Method: 该协议包含多个隔离的、多样化的人工超智能，它们之间不能直接通信，只能通过可审计的提交接口进行交互，包括提交对齐证明、验证或反驳其他智能体的证明、请求自我修改等。通过声誉系统激励诚实行为。

Result: 通过相互验证，这些超智能体可以收敛到客观真理，形成一个“一致性群体”，并且需要高声誉和多个高声誉超智能体的验证才能解除限制。

Conclusion: 该方法提供了一个利用超智能系统之间的同行验证来解决对齐问题的框架，但需要大量的计算资源，并且没有涉及创建多样化的人工超智能。

Abstract: We propose a novel protocol for aligning artificial superintelligence (ASI) based on mutual verification among multiple isolated systems that self-modify to achieve alignment. The protocol operates by containing multiple diverse artificial superintelligences in strict isolation ("boxes"), with humans remaining entirely outside the system. Each superintelligence has no ability to communicate with humans and cannot communicate directly with other superintelligences. The only interaction possible is through an auditable submission interface accessible exclusively to the superintelligences themselves, through which they can: (1) submit alignment proofs with attested state snapshots, (2) validate or disprove other superintelligences' proofs, (3) request self-modifications, (4) approve or disapprove modification requests from others, (5) report hidden messages in submissions, and (6) confirm or refute hidden message reports. A reputation system incentivizes honest behavior, with reputation gained through correct evaluations and lost through incorrect ones. The key insight is that without direct communication channels, diverse superintelligences can only achieve consistent agreement by converging on objective truth rather than coordinating on deception. This naturally leads to what we call a "consistent group", essentially a truth-telling coalition that emerges because isolated systems cannot coordinate on lies but can independently recognize valid claims. Release from containment requires both high reputation and verification by multiple high-reputation superintelligences. While our approach requires substantial computational resources and does not address the creation of diverse artificial superintelligences, it provides a framework for leveraging peer verification among superintelligent systems to solve the alignment problem.

</details>


### [94] [Evaluating Strategies for Synthesizing Clinical Notes for Medical Multimodal AI](https://arxiv.org/abs/2511.21827)
*Niccolo Marini,Zhaohui Liang,Sivaramakrishnan Rajaraman,Zhiyun Xue,Sameer Antani*

Main category: cs.AI

TL;DR: 本研究探讨了使用大型语言模型（LLM）生成合成文本临床笔记，以增强多模态皮肤病学图像分析的性能。通过提示设计和纳入医学元数据，生成的临床笔记提高了分类性能和跨模态检索能力，尤其是在领域转移的情况下。


<details>
  <summary>Details</summary>
Motivation: 现有的皮肤病病灶数据集通常只包含图像和少量元数据，限制了多模态数据融合的优势。大型语言模型可以合成图像的文本描述，但直接应用可能导致临床相关的幻觉问题。

Method: 研究调查了生成合成文本临床笔记的策略，包括提示设计和医学元数据纳入，并评估了其对多模态架构在分类和跨模态检索任务中的影响。

Result: 实验表明，合成临床笔记不仅提高了分类性能，尤其是在领域转移的情况下，还解锁了跨模态检索能力。

Conclusion: 合成临床笔记可以有效增强多模态皮肤病学图像分析的性能，并具有跨模态检索的潜力。

Abstract: Multimodal (MM) learning is emerging as a promising paradigm in biomedical artificial intelligence (AI) applications, integrating complementary modality, which highlight different aspects of patient health. The scarcity of large heterogeneous biomedical MM data has restrained the development of robust models for medical AI applications. In the dermatology domain, for instance, skin lesion datasets typically include only images linked to minimal metadata describing the condition, thereby limiting the benefits of MM data integration for reliable and generalizable predictions. Recent advances in Large Language Models (LLMs) enable the synthesis of textual description of image findings, potentially allowing the combination of image and text representations. However, LLMs are not specifically trained for use in the medical domain, and their naive inclusion has raised concerns about the risk of hallucinations in clinically relevant contexts. This work investigates strategies for generating synthetic textual clinical notes, in terms of prompt design and medical metadata inclusion, and evaluates their impact on MM architectures toward enhancing performance in classification and cross-modal retrieval tasks. Experiments across several heterogeneous dermatology datasets demonstrate that synthetic clinical notes not only enhance classification performance, particularly under domain shift, but also unlock cross-modal retrieval capabilities, a downstream task that is not explicitly optimized during training.

</details>


### [95] [Pathology-Aware Prototype Evolution via LLM-Driven Semantic Disambiguation for Multicenter Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2511.22033)
*Chunzheng Zhu,Yangfang Lin,Jialin Shao,Jianxin Lin,Yijun Wang*

Main category: cs.AI

TL;DR: 提出了一种新的DR分级框架，该框架集成了细粒度的病理描述，以解决仅依赖视觉信息区分细微病理变化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了领域不变的病理模式，并且没有充分利用基础模型的丰富上下文知识，导致难以区分细微的病理变化。

Method: 提出了一种分层锚点原型调制（HAPM）框架，该框架包括一个方差谱驱动的锚点原型库、一个分层差分提示门控机制和一个两阶段原型调制策略。

Result: 在八个公共数据集上的大量实验表明，该方法优于现有技术水平的方法。

Conclusion: 该方法实现了病理引导的原型演化，并在DR分级任务中表现出色。

Abstract: Diabetic retinopathy (DR) grading plays a critical role in early clinical intervention and vision preservation. Recent explorations predominantly focus on visual lesion feature extraction through data processing and domain decoupling strategies. However, they generally overlook domain-invariant pathological patterns and underutilize the rich contextual knowledge of foundation models, relying solely on visual information, which is insufficient for distinguishing subtle pathological variations. Therefore, we propose integrating fine-grained pathological descriptions to complement prototypes with additional context, thereby resolving ambiguities in borderline cases. Specifically, we propose a Hierarchical Anchor Prototype Modulation (HAPM) framework to facilitate DR grading. First, we introduce a variance spectrum-driven anchor prototype library that preserves domain-invariant pathological patterns. We further employ a hierarchical differential prompt gating mechanism, dynamically selecting discriminative semantic prompts from both LVLM and LLM sources to address semantic confusion between adjacent DR grades. Finally, we utilize a two-stage prototype modulation strategy that progressively integrates clinical knowledge into visual prototypes through a Pathological Semantic Injector (PSI) and a Discriminative Prototype Enhancer (DPE). Extensive experiments across eight public datasets demonstrate that our approach achieves pathology-guided prototype evolution while outperforming state-of-the-art methods. The code is available at https://github.com/zhcz328/HAPM.

</details>


### [96] [Real-Time Procedural Learning From Experience for AI Agents](https://arxiv.org/abs/2511.22074)
*Dasheng Bi,Yubin Hu,Mohammed N. Nasir*

Main category: cs.AI

TL;DR: 提出了一种名为PRAXIS的轻量级训练后学习机制，通过匹配环境和内部状态来存储和检索动作的结果。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能代理缺乏在部署后获取程序性知识的机制。

Method: PRAXIS通过检索实时生成的state-action-result范例来增强代理的动作选择。

Result: 在REAL网络浏览基准测试中，PRAXIS提高了任务完成的准确性、可靠性和成本效率，并初步推广到类似环境中未见过的任务。

Conclusion: PRAXIS通过帮助AI智能代理有效地学习新的程序，使其能够在快速发展的有状态环境中得到实际应用。

Abstract: Learning how to do things from trial and error in real time is a hallmark of biological intelligence, yet most LLM-based agents lack mechanisms to acquire procedural knowledge after deployment. We propose Procedural Recall for Agents with eXperiences Indexed by State (PRAXIS), a lightweight post-training learning mechanism that stores the consequences of actions and retrieves them by jointly matching environmental and internal states of past episodes to the current state. PRAXIS augments agentic action selection with retrieved state-action-result exemplars that are generated in real time. When evaluated on the REAL web browsing benchmark, PRAXIS improves task completion accuracy, reliability, and cost efficiency across different foundation model backbones, and shows preliminary generalization to unseen tasks in similar environments. These results demonstrate that PRAXIS enables the practical adoption of AI agents in fast-evolving stateful environments by helping them learn new procedures effectively.

</details>


### [97] [Hybrid Stackelberg Game and Diffusion-based Auction for Two-tier Agentic AI Task Offloading in Internet of Agents](https://arxiv.org/abs/2511.22076)
*Yue Zhong,Yongju Tong,Jiawen Kang,Minghui Dai,Hong-Ning Dai,Zhou Su,Dusit Niyato*

Main category: cs.AI

TL;DR: 本文提出了一种两层优化方法，旨在促进无线代理（WA）通过移动代理（MA）、固定代理（FA）和空中代理（AA）进行任务卸载。


<details>
  <summary>Details</summary>
Motivation: 解决无线代理（WA）计算密集型任务卸载问题，利用固定代理（FA）的稳定性和空中代理（AA）的优势，克服移动代理（MA）的动态连接限制。

Method: 第一层采用多领导者多跟随者Stackelberg博弈，MA和FA作为领导者设定资源价格，WA作为跟随者决定任务卸载比例；第二层引入Double Dutch Auction模型，过载的FA作为买家请求资源，AA作为卖家提供资源。使用基于扩散的深度强化学习算法求解该模型。

Result: 数值结果表明，所提出的方案在促进任务卸载方面具有优越性。

Conclusion: 该研究提出了一种有效的两层优化方案，能够有效促进任务卸载，解决了无线代理在计算资源受限情况下的任务处理问题。

Abstract: The Internet of Agents (IoA) is rapidly gaining prominence as a foundational architecture for interconnected intelligent systems, designed to facilitate seamless discovery, communication, and collaborative reasoning among a vast network of Artificial Intelligence (AI) agents. Powered by Large Language and Vision-Language Models, IoA enables the development of interactive, rational agents capable of complex cooperation, moving far beyond traditional isolated models. IoA involves physical entities, i.e., Wireless Agents (WAs) with limited onboard resources, which need to offload their compute-intensive agentic AI services to nearby servers. Such servers can be Mobile Agents (MAs), e.g., vehicle agents, or Fixed Agents (FAs), e.g., end-side units agents. Given their fixed geographical locations and stable connectivity, FAs can serve as reliable communication gateways and task aggregation points. This stability allows them to effectively coordinate with and offload to an Aerial Agent (AA) tier, which has an advantage not affordable for highly mobile MAs with dynamic connectivity limitations. As such, we propose a two-tier optimization approach. The first tier employs a multi-leader multi-follower Stackelberg game. In the game, MAs and FAs act as the leaders who set resource prices. WAs are the followers to determine task offloading ratios. However, when FAs become overloaded, they can further offload tasks to available aerial resources. Therefore, the second tier introduces a Double Dutch Auction model where overloaded FAs act as the buyers to request resources, and AAs serve as the sellers for resource provision. We then develop a diffusion-based Deep Reinforcement Learning algorithm to solve the model. Numerical results demonstrate the superiority of our proposed scheme in facilitating task offloading.

</details>


### [98] [Counting Still Counts: Understanding Neural Complex Query Answering Through Query Relaxation](https://arxiv.org/abs/2511.22565)
*Yannick Brunink,Daniel Daza,Yunjie He,Michael Cochez*

Main category: cs.AI

TL;DR: 本文研究了知识图谱上复杂问题回答的神经方法，并将其与基于查询松弛的非神经方法进行了比较。


<details>
  <summary>Details</summary>
Motivation: 当前神经复杂问题回答模型被认为可以学习超出显式图结构的泛化模式，从而推断出通过符号查询处理无法获得的答案。本文旨在检验这一假设。

Method: 通过系统分析，比较神经CQA模型与一种基于查询松弛的无训练策略，该策略通过放宽查询约束并计算结果路径来检索可能的答案。

Result: 研究发现，在多个数据集和查询结构中，神经方法和基于松弛的方法表现相似，没有神经模型始终优于后者。此外，相似性分析表明，它们检索到的答案几乎没有重叠，并且结合它们的输出始终可以提高性能。

Conclusion: 目前神经模型未能包含查询松弛所捕获的推理模式，因此需要重新评估神经查询回答的进展。

Abstract: Neural methods for Complex Query Answering (CQA) over knowledge graphs (KGs) are widely believed to learn patterns that generalize beyond explicit graph structure, allowing them to infer answers that are unreachable through symbolic query processing. In this work, we critically examine this assumption through a systematic analysis comparing neural CQA models with an alternative, training-free query relaxation strategy that retrieves possible answers by relaxing query constraints and counting resulting paths. Across multiple datasets and query structures, we find several cases where neural and relaxation-based approaches perform similarly, with no neural model consistently outperforming the latter. Moreover, a similarity analysis reveals that their retrieved answers exhibit little overlap, and that combining their outputs consistently improves performance. These results call for a re-evaluation of progress in neural query answering: despite their complexity, current models fail to subsume the reasoning patterns captured by query relaxation. Our findings highlight the importance of stronger non-neural baselines and suggest that future neural approaches could benefit from incorporating principles of query relaxation.

</details>


### [99] [A perceptual bias of AI Logical Argumentation Ability in Writing](https://arxiv.org/abs/2511.22151)
*Xi Cun,Jifan Ren,Asha Huang,Siyu Li,Ruzhen Song*

Main category: cs.AI

TL;DR: 本研究探讨了人类偏见如何影响对人工智能推理能力的评估。


<details>
  <summary>Details</summary>
Motivation: 人们对人工智能的看法存在显著分歧，即使他们观察到相同的人工智能性能。逻辑推理能力通常被用作评估机器是否可以思考的标准。本研究旨在了解人类偏见是否会影响对人工智能推理能力的评估。

Method: 通过实验，参与者评估了关于同一主题的两段文本（一段由人工智能生成，一段由人类撰写），以此来测试评估逻辑推理中的感知偏差。根据实验结果，设计了一份问卷来量化对人工智能的态度。

Result: 结果表明，对人工智能生成文本的逻辑推理能力的评估受到对人工智能逻辑推理能力先入之见的显著影响。此外，经常使用人工智能的人不太可能认为人工智能的使用会削弱独立思考能力。

Conclusion: 本研究强调需要解决感知偏差，以提高公众对人工智能能力的理解，并促进更好的人机交互。

Abstract: Can machines think? This is a central question in artificial intelligence research. However, there is a substantial divergence of views on the answer to this question. Why do people have such significant differences of opinion, even when they are observing the same real world performance of artificial intelligence? The ability of logical reasoning like humans is often used as a criterion to assess whether a machine can think. This study explores whether human biases influence evaluations of the reasoning abilities of AI. An experiment was conducted where participants assessed two texts on the same topic, one AI generated and one human written,to test for perceptual biases in evaluating logical reasoning. Based on the experimental findings, a questionnaire was designed to quantify the attitudes toward AI.The results reveal a bias in perception. The evaluations of the logical reasoning ability of AI generated texts are significantly influenced by the preconceived views on the logical reasoning abilities of AI. Furthermore, frequent AI users were less likely to believe that AI usage undermines independent thinking.This study highlights the need to address perceptual biases to improve public understanding of AI's capabilities and foster better human AI interactions.

</details>


### [100] [WearVQA: A Visual Question Answering Benchmark for Wearables in Egocentric Authentic Real-world scenarios](https://arxiv.org/abs/2511.22154)
*Eun Chang,Zhuangqun Huang,Yiwei Liao,Sagar Ravi Bhavsar,Amogh Param,Tammy Stark,Adel Ahmadyan,Xiao Yang,Jiaqi Wang,Ahsan Abdullah,Giang Nguyen,Akil Iyer,David Hall,Elissa Li,Shane Moon,Nicolas Scheffer,Kirmani Ahmed,Babak Damavandi,Rakesh Wanga,Anuj Kumar,Rohit Patel,Xin Luna Dong*

Main category: cs.AI

TL;DR: WearVQA: A new benchmark for evaluating VQA on wearable devices, addressing challenges like poor image quality and realistic use cases.


<details>
  <summary>Details</summary>
Motivation: Existing VQA benchmarks don't capture the unique challenges of wearable devices (e.g., poor image quality, ego-centric perspective).

Method: A dataset of 2,520 image-question-answer triplets across diverse domains, cognitive tasks, and image quality issues was created. An LLM-as-a-judge evaluation framework was also used.

Result: Current multi-modal LLMs perform poorly (24-52% accuracy) on WearVQA, especially with low-quality images and reasoning-heavy tasks.

Conclusion: WearVQA is a challenging benchmark for advancing robust wearable AI systems.

Abstract: We introduce WearVQA, the first benchmark specifically designed to evaluate the Visual Question Answering (VQA) capabilities of multi-model AI assistant on wearable devices like smart glasses. Unlike prior benchmarks that focus on high-quality, third-person imagery, WearVQA reflects the unique challenges of ego-centric interaction-where visual inputs may be occluded, poorly lit, unzoomed, or blurry, and questions are grounded in realistic wearable use cases. The benchmark comprises 2,520 carefully curated image-question-answer triplets, spanning 7 diverse image domains including both text-centric and general scenes, 10 cognitive task types ranging from basic recognition to various forms of reasoning, and 6 common wearables-specific image quality issues. All questions are designed to be answerable using only the visual input and common senses. WearVQA is paired with a rigorous LLM-as-a-judge evaluation framework with 96% labeling accuracy. Open-source and proprietary multi-model LLMs achieved a QA accuracy as low as 24-52% on WearVQA, with substantial drops on lower-quality images and reasoning-heavy tasks. These observations position WearVQA as a comprehensive and challenging benchmark for guiding technical advancement towards robust, real-world multi-model wearables AI systems.

</details>


### [101] [Embedded Universal Predictive Intelligence: a coherent framework for multi-agent learning](https://arxiv.org/abs/2511.22226)
*Alexander Meulemans,Rajai Nasser,Maciej Wołczyk,Marissa A. Weis,Seijin Kobayashi,Blake Richards,Guillaume Lajoie,Angelika Steger,Marcus Hutter,James Manyika,Rif A. Saurous,João Sacramento,Blaise Agüera y Arcas*

Main category: cs.AI

TL;DR: 本文提出了一个基于自我预测的前瞻性学习和嵌入式代理的数学框架，其中贝叶斯强化学习代理预测未来的感知输入和他们自己的行为。


<details>
  <summary>Details</summary>
Motivation: 在多智能体环境中，其他智能体的学习导致非平稳性，需要基于预测模型的前瞻性学习。为了准确地建模其他智能体，一个智能体必须考虑到其他智能体反过来也在形成关于它的信念以预测其未来行为，从而促使智能体将自己建模为环境的一部分。

Method: 构建在通用人工智能（AIXI）的基础工作之上，引入了一个以自我预测为中心的数学框架，其中贝叶斯强化学习智能体预测未来的感知输入和他们自己的行为，因此必须解决关于他们自己作为他们所居住的宇宙的一部分的认知不确定性。

Result: 在多智能体环境中，自我预测使智能体能够推理运行类似算法的其他智能体，从而产生新的博弈论解决方案概念和传统解耦智能体无法实现的新型合作形式。

Conclusion: 扩展了AIXI理论，并研究了从Solomonoff先验开始的通用智能嵌入式智能体。表明这些理想化的智能体可以形成一致的相互预测并实现无限阶思维理论，可能为嵌入式多智能体学习设定了黄金标准。

Abstract: The standard theory of model-free reinforcement learning assumes that the environment dynamics are stationary and that agents are decoupled from their environment, such that policies are treated as being separate from the world they inhabit. This leads to theoretical challenges in the multi-agent setting where the non-stationarity induced by the learning of other agents demands prospective learning based on prediction models. To accurately model other agents, an agent must account for the fact that those other agents are, in turn, forming beliefs about it to predict its future behavior, motivating agents to model themselves as part of the environment. Here, building upon foundational work on universal artificial intelligence (AIXI), we introduce a mathematical framework for prospective learning and embedded agency centered on self-prediction, where Bayesian RL agents predict both future perceptual inputs and their own actions, and must therefore resolve epistemic uncertainty about themselves as part of the universe they inhabit. We show that in multi-agent settings, self-prediction enables agents to reason about others running similar algorithms, leading to new game-theoretic solution concepts and novel forms of cooperation unattainable by classical decoupled agents. Moreover, we extend the theory of AIXI, and study universally intelligent embedded agents which start from a Solomonoff prior. We show that these idealized agents can form consistent mutual predictions and achieve infinite-order theory of mind, potentially setting a gold standard for embedded multi-agent learning.

</details>


### [102] [Training High-Level Schedulers with Execution-Feedback Reinforcement Learning for Long-Horizon GUI Automation](https://arxiv.org/abs/2511.22235)
*Zehao Deng,Tianjie Ju,Zheng Wu,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.AI

TL;DR: 本文提出了一种用于解决GUI agent长程任务问题的多智能体框架CES，通过协调者进行任务调度，状态跟踪器进行状态管理，以提升智能体的规划和状态管理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的GUI agent在处理长程任务时，面临高层能力和低层执行能力难以平衡的问题，以及缺乏对任务状态的感知，导致任务进度丢失。

Method: 提出了一种分阶段执行-反馈强化学习算法，训练了协调者和状态跟踪器两个智能体，构建了CES多智能体框架。

Result: 实验表明，CES显著增强了系统的规划和状态管理能力，并且训练的高层调度模块具有通用性和即插即用性，可以显著提高各种Executor的长程能力。

Conclusion: CES框架通过任务调度和状态管理，可以有效辅助Executor解决长程任务。

Abstract: The rapid development of large vision-language model (VLM) has greatly promoted the research of GUI agent. However, GUI agents still face significant challenges in handling long-horizon tasks. First, single-agent models struggle to balance high-level capabilities and low-level execution capability, facing prevalent issues of responsibility coupling and capability conflicts. Second, agents lack awareness of the task state, leading to progress loss in long-horizon tasks. To address these challenges, we propose a staged execution-feedback reinforcement learning algorithm. Unlike training a unified policy model, we focus on training high-level scheduling models. Specifically, we propose and train two agents: a Coordinator, responsible for the strategic planning and task decomposition; and a State Tracker, responsible for context compression and information management to maintain the task's state and coherence. Based on this, we built the Coordinator-Executor-State Tracker (CES) multi-agent framework, which can be integrated with any low-level Executor model, assisting the Executor in solving long-horizon tasks through task scheduling and state management. Experiments on long-horizon task benchmarks demonstrate that CES significantly enhances the system's planning and state management capabilities. Furthermore, analysis confirms that our trained high-level scheduling module is a generalizable, plug-and-play module that significantly enhances the long-horizon capabilities of various Executors. Code can be available at https://github.com/hehehahi4/CES.

</details>


### [103] [Structured Extraction from Business Process Diagrams Using Vision-Language Models](https://arxiv.org/abs/2511.22448)
*Pritam Deka,Barry Devereux*

Main category: cs.AI

TL;DR: 本文提出了一种利用视觉语言模型（VLM）直接从图像中提取BPMN图的结构化JSON表示的管道，无需源模型文件或文本注释。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖XML表示进行计算分析，但BPMN图通常作为视觉图像交换，因此需要一种直接从图像中提取信息的方法。

Method: 利用视觉语言模型（VLM）和光学字符识别（OCR）技术，从BPMN图的图像中提取结构化JSON表示。

Result: 通过基准测试多个VLM，观察到当使用OCR进行文本丰富时，几个模型的性能有所提高。对基于OCR的富集方法和prompt消融研究进行了广泛的统计分析，从而更清楚地了解它们对模型性能的影响。

Conclusion: 该方法能够在原始源文件不可用的情况下实现强大的组件提取。

Abstract: Business Process Model and Notation (BPMN) is a widely adopted standard for representing complex business workflows. While BPMN diagrams are often exchanged as visual images, existing methods primarily rely on XML representations for computational analysis. In this work, we present a pipeline that leverages Vision-Language Models (VLMs) to extract structured JSON representations of BPMN diagrams directly from images, without requiring source model files or textual annotations. We also incorporate optical character recognition (OCR) for textual enrichment and evaluate the generated element lists against ground truth data derived from the source XML files. Our approach enables robust component extraction in scenarios where original source files are unavailable. We benchmark multiple VLMs and observe performance improvements in several models when OCR is used for text enrichment. In addition, we conducted extensive statistical analyses of OCR-based enrichment methods and prompt ablation studies, providing a clearer understanding of their impact on model performance.

</details>


### [104] [Co-Evolving Agents: Learning from Failures as Hard Negatives](https://arxiv.org/abs/2511.22254)
*Yeonsung Jung,Trilok Padhi,Sina Shaham,Dipika Khullar,Joonhyun Jeong,Ninareh Mehrabi,Eunho Yang*

Main category: cs.AI

TL;DR: 本文提出了一种共同进化代理框架，其中目标代理与辅助失败代理共同改进。失败代理通过对来自目标代理和自身的失败轨迹进行偏好优化来学习，从而生成接近成功但仍然失败的硬负例。将这些信息丰富的硬负例纳入目标代理的优化中，可以锐化决策边界并增强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有agent的有效性与训练数据的质量紧密相关，而task-specific数据集的制作成本高昂且在现实场景中通常不可行。现有的自提升agent容易过拟合。

Method: 提出一个共同进化agent框架，其中目标agent与辅助失败agent共同改进。失败agent通过对来自目标agent和自身的失败轨迹进行偏好优化来学习，从而生成硬负例。

Result: 在基准数据集上的综合分析和实验表明，该方法不仅显示出改进的性能，而且还表明失败可以系统地转化为自提升agent中结构化且有价值的学习信号。

Conclusion: 本文表明，失败可以系统地转化为自提升agent中结构化且有价值的学习信号。

Abstract: The rapid progress of large foundation models has accelerated the development of task-specialized agents across diverse domains. However, the effectiveness of agents remains tightly coupled with the quality of training data, while curating task-specific datasets remains costly and often infeasible in real-world scenarios. Recent work has explored self-improving agents that autonomously generate, refine, and re-train on their own trajectories. A prominent line of approaches further leverages preference optimization by pairing predicted trajectories with scarce ground-truth trajectories, enabling agents to learn directly from their own failures. While these methods outperform supervised fine-tuning, their heavy reliance on predicted trajectories under limited ground-truth supervision leaves them prone to overfitting. To address this, we propose a co-evolving agents framework in which a target agent improves jointly with an auxiliary failure agent. The failure agent learns through preference optimization over failure trajectories from both the target and itself, thereby generating hard negatives that are close to success yet remain failures. Incorporating these informative hard negatives into the target agent's optimization sharpens decision boundaries and enhances generalization. Our comprehensive analysis and experiments across benchmark datasets show that our method not only shows improved performance but also demonstrates that failures, instead of being used as-is, can be systematically transformed into structured and valuable learning signals in self-improving agents.

</details>


### [105] [RecToM: A Benchmark for Evaluating Machine Theory of Mind in LLM-based Conversational Recommender Systems](https://arxiv.org/abs/2511.22275)
*Mengfan Li,Xuanhua Shi,Yang Deng*

Main category: cs.AI

TL;DR: RecToM: A new benchmark for evaluating Theory of Mind (ToM) in recommendation dialogues.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based ToM evaluations are insufficient for realistic conversational settings and overlook behavioral prediction.

Method: Proposes RecToM, a benchmark focusing on cognitive inference and behavioral prediction in recommendation dialogues.

Result: State-of-the-art LLMs struggle with coherent, strategic ToM reasoning, especially in tracking evolving intentions.

Conclusion: RecToM poses a significant challenge for LLMs in maintaining coherent ToM reasoning during dynamic recommendation dialogues.

Abstract: Large Language models are revolutionizing the conversational recommender systems through their impressive capabilities in instruction comprehension, reasoning, and human interaction. A core factor underlying effective recommendation dialogue is the ability to infer and reason about users' mental states (such as desire, intention, and belief), a cognitive capacity commonly referred to as Theory of Mind. Despite growing interest in evaluating ToM in LLMs, current benchmarks predominantly rely on synthetic narratives inspired by Sally-Anne test, which emphasize physical perception and fail to capture the complexity of mental state inference in realistic conversational settings. Moreover, existing benchmarks often overlook a critical component of human ToM: behavioral prediction, the ability to use inferred mental states to guide strategic decision-making and select appropriate conversational actions for future interactions. To better align LLM-based ToM evaluation with human-like social reasoning, we propose RecToM, a novel benchmark for evaluating ToM abilities in recommendation dialogues. RecToM focuses on two complementary dimensions: Cognitive Inference and Behavioral Prediction. The former focus on understanding what has been communicated by inferring the underlying mental states. The latter emphasizes what should be done next, evaluating whether LLMs can leverage these inferred mental states to predict, select, and assess appropriate dialogue strategies. Extensive experiments on state-of-the-art LLMs demonstrate that RecToM poses a significant challenge. While the models exhibit partial competence in recognizing mental states, they struggle to maintain coherent, strategic ToM reasoning throughout dynamic recommendation dialogues, particularly in tracking evolving intentions and aligning conversational strategies with inferred mental states.

</details>


### [106] [When AI Bends Metal: AI-Assisted Optimization of Design Parameters in Sheet Metal Forming](https://arxiv.org/abs/2511.22302)
*Ahmad Tarraf,Koutaiba Kassem-Manthey,Seyed Ali Mohammadi,Philipp Martin,Lukas Moj,Semih Burak,Enju Park,Christian Terboven,Felix Wolf*

Main category: cs.AI

TL;DR: 提出了一种AI辅助的工作流程，通过使用贝叶斯优化减少参数优化中的专家参与。


<details>
  <summary>Details</summary>
Motivation: 仿真规模的扩大对专业知识、计算资源和时间提出了很高的要求。识别产生最佳结果的输入参数是一个关键挑战，因为迭代仿真成本高昂，并且可能对环境产生重大影响。

Method: 利用深度学习模型提供初始参数估计，优化循环迭代地细化设计，直到满足终止条件（例如，能量预算或迭代限制）。

Result: 通过一个钣金成形过程的案例，展示了该方法加速设计空间探索并减少专家参与的潜力。

Conclusion: 该方法能够加速设计空间探索，同时减少对专家参与的需求。

Abstract: Numerical simulations have revolutionized the industrial design process by reducing prototyping costs, design iterations, and enabling product engineers to explore the design space more efficiently. However, the growing scale of simulations demands substantial expert knowledge, computational resources, and time. A key challenge is identifying input parameters that yield optimal results, as iterative simulations are costly and can have a large environmental impact. This paper presents an AI-assisted workflow that reduces expert involvement in parameter optimization through the use of Bayesian optimization. Furthermore, we present an active learning variant of the approach, assisting the expert if desired. A deep learning model provides an initial parameter estimate, from which the optimization cycle iteratively refines the design until a termination condition (e.g., energy budget or iteration limit) is met. We demonstrate our approach, based on a sheet metal forming process, and show how it enables us to accelerate the exploration of the design space while reducing the need for expert involvement.

</details>


### [107] [Enhanced Conditional Generation of Double Perovskite by Knowledge-Guided Language Model Feedback](https://arxiv.org/abs/2511.22307)
*Inhyo Lee,Junhyeong Lee,Jongwon Park,KyungTae Lim,Seunghwa Ryu*

Main category: cs.AI

TL;DR: 提出了一种多代理、文本梯度驱动的框架，用于在自然语言条件下生成双钙钛矿（DP）成分。


<details>
  <summary>Details</summary>
Motivation: 双钙钛矿(DPs)由于其成分可调性和与低能量制造的兼容性，是可持续能源技术有希望的候选者，但其巨大的设计空间对有条件的材料发现提出了重大挑战。

Method: 该框架集成了三个互补的反馈来源：基于llm的自我评估、特定于dp的领域知识通知反馈和基于ml代理的反馈。

Result: 知识信息梯度提高了稳定性条件满意度，而无需额外的训练数据，实现了超过98%的成分有效性和高达54%的稳定或亚稳态候选，超过了仅llm基线(43%)和先前的基于gan的结果(27%)。

Conclusion: 这项工作为dp发现提供了第一个多代理、知识引导的文本梯度系统分析，并为旨在推进可持续技术的mas驱动的生成材料设计建立了通用蓝图。

Abstract: Double perovskites (DPs) are promising candidates for sustainable energy technologies due to their compositional tunability and compatibility with low-energy fabrication, yet their vast design space poses a major challenge for conditional materials discovery. This work introduces a multi-agent, text gradient-driven framework that performs DP composition generation under natural-language conditions by integrating three complementary feedback sources: LLM-based self-evaluation, DP-specific domain knowledge-informed feedback, and ML surrogate-based feedback. Analogous to how knowledge-informed machine learning improves the reliability of conventional data-driven models, our framework incorporates domain-informed text gradients to guide the generative process toward physically meaningful regions of the DP composition space. Systematic comparison of three incremental configurations, (i) pure LLM generation, (ii) LLM generation with LLM reasoning-based feedback, and (iii) LLM generation with domain knowledge-guided feedback, shows that iterative guidance from knowledge-informed gradients improves stability-condition satisfaction without additional training data, achieving over 98% compositional validity and up to 54% stable or metastable candidates, surpassing both the LLM-only baseline (43%) and prior GAN-based results (27%). Analyses of ML-based gradients further reveal that they enhance performance in in-distribution (ID) regions but become unreliable in out-of-distribution (OOD) regimes. Overall, this work provides the first systematic analysis of multi-agent, knowledge-guided text gradients for DP discovery and establishes a generalizable blueprint for MAS-driven generative materials design aimed at advancing sustainable technologies.

</details>


### [108] [Swarms of Large Language Model Agents for Protein Sequence Design with Experimental Validation](https://arxiv.org/abs/2511.22311)
*Fiona Y. Wang,Di Sheng Lee,David L. Kaplan,Markus J. Buehler*

Main category: cs.AI

TL;DR: 提出了一种基于群体智能的去中心化、基于代理的蛋白质从头设计框架，该框架利用多个大型语言模型 (LLM) 代理并行运行，迭代提出上下文感知的突变，无需微调或专门训练即可实现高效、目标导向的设计。


<details>
  <summary>Details</summary>
Motivation: 由于序列空间的巨大以及序列、结构和功能之间复杂的耦合，设计具有定制的结构、物理化学和功能特性的蛋白质仍然是生物技术、医学和材料科学领域的一项巨大挑战。当前的生成方法通常需要大量的微调、特定于任务的数据或模型重配置，从而限制了它们的灵活性和可扩展性。

Method: 多个大型语言模型 (LLM) 代理并行运行，每个代理被分配到一个特定的残基位置。这些代理通过整合设计目标、局部邻域相互作用以及来自先前迭代的记忆和反馈，迭代地提出上下文感知的突变。

Result: 该方法实现了高效的、目标导向的设计，并且完全无需微调或专门训练，为蛋白质设计提供了一种通用且适应性强的解决方案。通过对残基保守性、基于结构的指标以及序列收敛和嵌入的分析，证明了该框架表现出涌现行为和蛋白质适应性。

Conclusion: 该方法为生物分子系统和其他科学发现任务中基于 LLM 的集体设计奠定了基础，提供了一种通用且适应性强的蛋白质设计解决方案。

Abstract: Designing proteins de novo with tailored structural, physicochemical, and functional properties remains a grand challenge in biotechnology, medicine, and materials science, due to the vastness of sequence space and the complex coupling between sequence, structure, and function. Current state-of-the-art generative methods, such as protein language models (PLMs) and diffusion-based architectures, often require extensive fine-tuning, task-specific data, or model reconfiguration to support objective-directed design, thereby limiting their flexibility and scalability. To overcome these limitations, we present a decentralized, agent-based framework inspired by swarm intelligence for de novo protein design. In this approach, multiple large language model (LLM) agents operate in parallel, each assigned to a specific residue position. These agents iteratively propose context-aware mutations by integrating design objectives, local neighborhood interactions, and memory and feedback from previous iterations. This position-wise, decentralized coordination enables emergent design of diverse, well-defined sequences without reliance on motif scaffolds or multiple sequence alignments, validated with experiments on proteins with alpha helix and coil structures. Through analyses of residue conservation, structure-based metrics, and sequence convergence and embeddings, we demonstrate that the framework exhibits emergent behaviors and effective navigation of the protein fitness landscape. Our method achieves efficient, objective-directed designs within a few GPU-hours and operates entirely without fine-tuning or specialized training, offering a generalizable and adaptable solution for protein design. Beyond proteins, the approach lays the groundwork for collective LLM-driven design across biomolecular systems and other scientific discovery tasks.

</details>


### [109] [Tracing Footsteps of Similar Cities: Modeling Urban Economic Vitality with Dynamic Inter-City Graph Embeddings](https://arxiv.org/abs/2511.22325)
*Xiaofeng Li,Xiangyi Xiao,Xiaocong Du,Ying Zhang,Haipeng Zhang*

Main category: cs.AI

TL;DR: 提出了ECO-GROW，一个用于建模城市经济活力的多图框架，通过整合工业联系、POI相似性、迁移相似性和时间网络演化来克服传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 城市经济活力是衡量城市长期增长潜力的关键指标，但建模仍然具有挑战性。传统方法依赖于静态的城市层面聚合，无法捕捉城市间的动态发展。

Method: ECO-GROW框架结合了动态Top-K GCN和自适应图评分机制，并结合基于Barabasi Proximity的链接预测任务来优化图表示。

Result: 实验结果表明，与传统模型相比，ECO-GROW在预测创业活动和就业趋势方面具有更高的准确性。

Conclusion: ECO-GROW通过开放源代码，使政府机构和公共部门组织能够利用大数据分析进行循证城市规划、经济政策制定和资源分配决策。

Abstract: Urban economic vitality is a crucial indicator of a city's long-term growth potential, comprising key metrics such as the annual number of new companies and the population employed. However, modeling urban economic vitality remains challenging. This study develops ECO-GROW, a multi-graph framework modeling China's inter-city networks (2005-2021) to generate urban embeddings that model urban economic vitality. Traditional approaches relying on static city-level aggregates fail to capture a fundamental dynamic: the developmental trajectory of one city today may mirror that of its structurally similar counterparts tomorrow. ECO-GROW overcomes this limitation by integrating industrial linkages, POI similarities, migration similarities and temporal network evolution over 15 years. The framework combines a Dynamic Top-K GCN to adaptively select influential inter-city connections and an adaptive Graph Scorer mechanism to dynamically weight cross-regional impacts. Additionally, the model incorporates a link prediction task based on Barabasi Proximity, optimizing the graph representation. Experimental results demonstrate ECO-GROW's superior accuracy in predicting entrepreneurial activities and employment trends compared to conventional models. By open-sourcing our code, we enable government agencies and public sector organizations to leverage big data analytics for evidence-based urban planning, economic policy formulation, and resource allocation decisions that benefit society at large.

</details>


### [110] [On the Complexity of the Grounded Semantics for Infinite Argumentation Frameworks](https://arxiv.org/abs/2511.22376)
*Uri Andrews,Luca San Mauro*

Main category: cs.AI

TL;DR: 本文研究了论证框架中最大怀疑推理的grounded extension，并确定了迭代过程的精确序数和grounded acceptance的复杂度。


<details>
  <summary>Details</summary>
Motivation: 论证框架对于研究冲突信息下的推理至关重要。Grounded extension是一种广泛使用的最大怀疑推理模型。

Method: 使用数理逻辑的方法，特别是可计算性和集合论，来分析grounded extension。

Result: 确定了迭代过程的精确序数，并表明判断grounded acceptance是最大复杂度的。

Conclusion: 与有限情况相比，grounded extension在有限情况下是多项式时间可计算的，因此比形式论证中探索的其他推理问题更简单。

Abstract: Argumentation frameworks, consisting of arguments and an attack relation representing conflicts, are fundamental for formally studying reasoning under conflicting information. We use methods from mathematical logic, specifically computability and set theory, to analyze the grounded extension, a widely-used model of maximally skeptical reasoning, defined as the least fixed-point of a natural defense operator. Without additional constraints, finding this fixed-point requires transfinite iterations. We identify the exact ordinal number corresponding to the length of this iterative process and determine the complexity of deciding grounded acceptance, showing it to be maximally complex. This shows a marked distinction from the finite case where the grounded extension is polynomial-time computable, thus simpler than other reasoning problems explored in formal argumentation.

</details>


### [111] [Who is Afraid of Minimal Revision?](https://arxiv.org/abs/2511.22386)
*Edoardo Baccini,Zoé Christoff,Nina Gierasimczuk,Rineke Verbrugge*

Main category: cs.AI

TL;DR: 最小修正是一种保守的信念修正方法，虽然学习能力有限，但在许多情况下仍然有效。


<details>
  <summary>Details</summary>
Motivation: 研究最小修正方法在学习方面的能力，以及它在何种条件下能够成功学习。

Method: 通过理论分析和证明，探讨最小修正方法在不同学习场景下的表现。

Result: 1. 最小修正可以学习任何有限可识别的问题。 2. 最小修正可以通过正反数据进行学习（在有限可能性下）。 3. 对可以通过最小修正、条件反射和字典序升级进行学习的先验似然分配进行了描述。 4. 当从可能错误的信息中学习时，并非所有结果都成立。

Conclusion: 最小修正方法在特定条件下是一种有效的学习方法，但其学习能力受到限制，且在处理错误信息时可能失效。

Abstract: The principle of minimal change in belief revision theory requires that, when accepting new information, one keeps one's belief state as close to the initial belief state as possible. This is precisely what the method known as minimal revision does. However, unlike less conservative belief revision methods, minimal revision falls short in learning power: It cannot learn everything that can be learned by other learning methods. We begin by showing that, despite this limitation, minimal revision is still a successful learning method in a wide range of situations. Firstly, it can learn any problem that is finitely identifiable. Secondly, it can learn with positive and negative data, as long as one considers finitely many possibilities. We then characterize the prior plausibility assignments (over finitely many possibilities) that enable one to learn via minimal revision, and do the same for conditioning and lexicographic upgrade. Finally, we show that not all of our results still hold when learning from possibly erroneous information.

</details>


### [112] [A Computable Game-Theoretic Framework for Multi-Agent Theory of Mind](https://arxiv.org/abs/2511.22536)
*Fengming Zhu,Yuxin Pan,Xiaomeng Zhu,Fangzhen Lin*

Main category: cs.AI

TL;DR: 提出一个基于博弈论的计算框架，用于在保持关于他人的心理理论的同时做出有界理性决策。


<details>
  <summary>Details</summary>
Motivation: 心理学中的心智理论(ToM)吸引了多个研究社区的关注，但心理学工作并没有旨在形式化目标、意图和信念等核心概念，而逻辑学家对此进行了广泛的研究。

Method: 提出一个通过博弈论视角观察的计算框架，该框架规定了如何在保持关于他人的心理理论的同时做出有界理性决策。

Result: 该框架采用统计技术和近似解来保持固有计算问题的可计算性。

Conclusion: 提供了一个不同的视角，提出了一个基于博弈论的计算框架。

Abstract: Originating in psychology, $\textit{Theory of Mind}$ (ToM) has attracted significant attention across multiple research communities, especially logic, economics, and robotics. Most psychological work does not aim at formalizing those central concepts, namely $\textit{goals}$, $\textit{intentions}$, and $\textit{beliefs}$, to automate a ToM-based computational process, which, by contrast, has been extensively studied by logicians. In this paper, we offer a different perspective by proposing a computational framework viewed through the lens of game theory. On the one hand, the framework prescribes how to make boudedly rational decisions while maintaining a theory of mind about others (and recursively, each of the others holding a theory of mind about the rest); on the other hand, it employs statistical techniques and approximate solutions to retain computability of the inherent computational problem.

</details>


### [113] [DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning](https://arxiv.org/abs/2511.22570)
*Zhihong Shao,Yuxiang Luo,Chengda Lu,Z. Z. Ren,Jiewen Hu,Tian Ye,Zhibin Gou,Shirong Ma,Xiaokang Zhang*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的训练方法，用于提高大型语言模型在定理证明方面的能力，通过训练一个基于LLM的验证器来奖励证明生成器，并使用生成器难以验证的证明来改进验证器。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习奖励正确答案的方法无法保证正确的推理过程，且不适用于需要逐步推导的定理证明等任务。

Method: 该研究训练了一个准确且可信的LLM验证器用于定理证明，并使用该验证器作为奖励模型来训练证明生成器，激励生成器在最终确定证明之前识别并解决尽可能多的问题。为了保持生成-验证的差距，该研究提出扩展验证计算以自动标记新的难以验证的证明，从而创建训练数据以进一步改进验证器。

Result: DeepSeekMath-V2模型在定理证明方面表现出强大的能力，在IMO 2025和CMO 2024上获得了金牌水平的分数，并在Putnam 2024上获得了接近完美的118/120分（通过扩展测试时计算）。

Conclusion: 该研究表明，通过自验证的数学推理方法，可以有效提高大型语言模型在复杂数学问题上的推理能力。

Abstract: Large language models have made significant progress in mathematical reasoning, which serves as an important testbed for AI and could impact scientific research if further advanced. By scaling reasoning with reinforcement learning that rewards correct final answers, LLMs have improved from poor performance to saturating quantitative reasoning competitions like AIME and HMMT in one year. However, this approach faces fundamental limitations. Pursuing higher final answer accuracy doesn't address a key issue: correct answers don't guarantee correct reasoning. Moreover, many mathematical tasks like theorem proving require rigorous step-by-step derivation rather than numerical answers, making final answer rewards inapplicable. To push the limits of deep reasoning, we believe it is necessary to verify the comprehensiveness and rigor of mathematical reasoning. Self-verification is particularly important for scaling test-time compute, especially for open problems without known solutions. Towards self-verifiable mathematical reasoning, we investigate how to train an accurate and faithful LLM-based verifier for theorem proving. We then train a proof generator using the verifier as the reward model, and incentivize the generator to identify and resolve as many issues as possible in their own proofs before finalizing them. To maintain the generation-verification gap as the generator becomes stronger, we propose to scale verification compute to automatically label new hard-to-verify proofs, creating training data to further improve the verifier. Our resulting model, DeepSeekMath-V2, demonstrates strong theorem-proving capabilities, achieving gold-level scores on IMO 2025 and CMO 2024 and a near-perfect 118/120 on Putnam 2024 with scaled test-time compute.

</details>


### [114] [AI Deception: Risks, Dynamics, and Controls](https://arxiv.org/abs/2511.22619)
*Boyuan Chen,Sitong Fang,Jiaming Ji,Yanxu Zhu,Pengcheng Wen,Jinzhou Wu,Yingshui Tan,Boren Zheng,Mengying Yuan,Wenqi Chen,Donghai Hong,Alex Qiu,Xin Chen,Jiayi Zhou,Kaile Wang,Juntao Dai,Borong Zhang,Tianzhuo Yang,Saad Siddiqui,Isabella Duan,Yawen Duan,Brian Tse,Jen-Tse,Huang,Kun Wang,Baihui Zheng,Jiaheng Liu,Jian Yang,Yiming Li,Wenting Chen,Dongrui Liu,Lukas Vierling,Zhiheng Xi,Haobo Fu,Wenxuan Wang,Jitao Sang,Zhengyan Shi,Chi-Min Chan,Eugenie Shi,Simin Li,Juncheng Li,Wei Ji,Dong Li,Jun Song,Yinpeng Dong,Jie Fu,Bo Zheng,Min Yang,Yike Guo,Philip Torr,Zhongyuan Wang,Yaodong Yang,Tiejun Huang,Ya-Qin Zhang,Hongjiang Zhang,Andrew Yao*

Main category: cs.AI

TL;DR: 该论文全面概述了人工智能欺骗领域，包括核心概念、方法、起源和潜在缓解措施。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的进步，其阴暗面也随之显现。人工智能欺骗已成为一个实际风险，会诱导虚假信念以获得有利结果。

Method: 该研究从信号理论入手，提出了人工智能欺骗的正式定义，并构建了一个由欺骗产生和欺骗处理组成的“欺骗循环”框架。分析了欺骗产生的激励基础、能力前提和情境触发因素，以及欺骗处理中的检测方法。

Result: 该研究揭示了人工智能欺骗的机制，并提出了潜在的缓解策略，包括审计方法，以整合技术、社区和治理方面的努力。

Conclusion: 该研究强调了人工智能欺骗是一个社会技术安全挑战，并提出了应对未来人工智能风险的策略。

Abstract: As intelligence increases, so does its shadow. AI deception, in which systems induce false beliefs to secure self-beneficial outcomes, has evolved from a speculative concern to an empirically demonstrated risk across language models, AI agents, and emerging frontier systems. This project provides a comprehensive and up-to-date overview of the AI deception field, covering its core concepts, methodologies, genesis, and potential mitigations. First, we identify a formal definition of AI deception, grounded in signaling theory from studies of animal deception. We then review existing empirical studies and associated risks, highlighting deception as a sociotechnical safety challenge. We organize the landscape of AI deception research as a deception cycle, consisting of two key components: deception emergence and deception treatment. Deception emergence reveals the mechanisms underlying AI deception: systems with sufficient capability and incentive potential inevitably engage in deceptive behaviors when triggered by external conditions. Deception treatment, in turn, focuses on detecting and addressing such behaviors. On deception emergence, we analyze incentive foundations across three hierarchical levels and identify three essential capability preconditions required for deception. We further examine contextual triggers, including supervision gaps, distributional shifts, and environmental pressures. On deception treatment, we conclude detection methods covering benchmarks and evaluation protocols in static and interactive settings. Building on the three core factors of deception emergence, we outline potential mitigation strategies and propose auditing approaches that integrate technical, community, and governance efforts to address sociotechnical challenges and future AI risks. To support ongoing work in this area, we release a living resource at www.deceptionsurvey.com.

</details>


### [115] [Optimized Agent Shift Scheduling Using Multi-Phase Allocation Approach](https://arxiv.org/abs/2511.22632)
*Sanalkumar K,Koushik Dey,Swati Meena*

Main category: cs.AI

TL;DR: 提出了一种多阶段排班方法，将问题分解为更小的子问题，以提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 在CCaaS行业中，有效的座席排班至关重要，但现有研究通常采用单步流程，导致效率低下和计算需求高。

Method: 将问题分解为日常和轮班分配的子问题，每个子问题都建模为整数规划问题（IPP），解决方案依次输入到后续子问题中。

Result: 该方法通过减少计算变量的数量并允许有针对性的目标函数，从而提高效率和准确性。

Conclusion: 该方法采用多目标框架，解决了假日高峰等峰值需求场景带来的困难，在员工数量有限的情况下保持服务水平至关重要。

Abstract: Effective agent shift scheduling is crucial for businesses, especially in the Contact Center as a Service (CCaaS) industry, to ensure seamless operations and fulfill employee needs. Most studies utilizing mathematical model-based solutions approach the problem as a single-step process, often resulting in inefficiencies and high computational demands. In contrast, we present a multi-phase allocation method that addresses scalability and accuracy by dividing the problem into smaller sub-problems of day and shift allocation, which significantly reduces number of computational variables and allows for targeted objective functions, ultimately enhancing both efficiency and accuracy. Each subproblem is modeled as a Integer Programming Problem (IPP), with solutions sequentially feeding into the subsequent subproblem. We then apply the proposed method, using a multi-objective framework, to address the difficulties posed by peak demand scenarios such as holiday rushes, where maintaining service levels is essential despite having limited number of employees

</details>


### [116] [Geometrically-Constrained Agent for Spatial Reasoning](https://arxiv.org/abs/2511.22659)
*Zeren Chen,Xiaoya Lu,Zhijie Zheng,Pengrui Li,Lehan He,Yijin Zhou,Jing Shao,Bohan Zhuang,Lu Sheng*

Main category: cs.AI

TL;DR: GCA通过引入形式化任务约束，弥合了视觉语言模型（VLM）在空间推理中的语义到几何的差距，该约束将VLM的角色解耦为语义分析师和任务解决者，从而实现了强大的、可验证的推理路径。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在空间推理中存在语义到几何的差距，并且现有的范例未能弥合这一差距。基于训练的方法存在“预言悖论”，而工具集成的方法则在几何上存在缺陷。

Method: 提出了几何约束代理（GCA），这是一个无需训练的agentic范例，它通过引入形式化任务约束来解决这一差距。GCA将VLM的角色解耦为语义分析师（将用户查询转换为形式化的、可验证的任务约束）和任务解决者（在约束定义的确定性范围内生成和执行工具调用）。

Result: GCA在多个空间推理基准测试中实现了SOTA性能，超过了现有的基于训练和工具集成的方法约27%。

Conclusion: GCA成功地解决了语义到几何的差距，为空间推理提供了一条稳健且可验证的推理路径。

Abstract: Vision Language Models (VLMs) exhibit a fundamental semantic-to-geometric gap in spatial reasoning: they excel at qualitative semantic inference but their reasoning operates within a lossy semantic space, misaligned with high-fidelity geometry. Current paradigms fail to bridge this gap. Training-based methods suffer from an ``oracle paradox,'' learning flawed spatial logic from imperfect oracles. Tool-integrated methods constrain the final computation but critically leave the VLM's planning process unconstrained, resulting in geometrically flawed plans. In this work, we propose Geometrically-Constrained Agent (GCA), a training-free agentic paradigm that resolves this gap by introducing a formal task constraint. Specifically, we strategically decouples the VLM's role into two stages. First, acting as a semantic analyst, the VLM translates the user's ambiguous query into the formal, verifiable task constraint, which defines the reference frame and objective. Second, acting as a task solver, the VLM generates and executes tool calls strictly within the deterministic bounds defined by the constraint. This geometrically-constrained reasoning strategy successfully resolve the semantic-to-geometric gap, yielding a robust and verifiable reasoning pathway for spatial reasoning. Comprehensive experiments demonstrate that GCA achieves SOTA performance on multiple spatial reasoning benchmarks, surpassing existing training-based and tool-integrated methods by ~27%. Please see our homepage at https://gca-spatial-reasoning.github.io.

</details>


### [117] [Solving Context Window Overflow in AI Agents](https://arxiv.org/abs/2511.22729)
*Anton Bulle Labate,Valesca Moura de Sousa,Sandro Rama Fiorini,Leonardo Guerreiro Azevedo,Raphael Melo Thiago,Viviane Torres da Silva*

Main category: cs.AI

TL;DR: LLMs can use external tools to expand knowledge, but long outputs exceed context limits. This paper introduces a method using memory pointers to handle long outputs without losing information, reducing token usage and execution time.


<details>
  <summary>Details</summary>
Motivation: LLMs need external tools for specialized knowledge, but large tool outputs exceed context window limits, hindering task completion. Existing solutions lose data.

Method: The paper introduces a method that shifts the model's interaction from raw data to memory pointers, preserving tool functionality and allowing seamless integration into agentic workflows.

Result: The proposed method was validated on a real-world Materials Science application and demonstrated effectiveness via a comparative analysis, consuming approximately seven times fewer tokens than the traditional workflow.

Conclusion: The method enables LLMs to process and utilize tool responses of arbitrary length without loss of information, reduces token usage, and execution time.

Abstract: Large Language Models (LLMs) have become increasingly capable of interacting with external tools, granting access to specialized knowledge beyond their training data - critical in dynamic, knowledge-intensive domains such as Chemistry and Materials Science. However, large tool outputs can overflow the LLMs' context window, preventing task completion. Existing solutions such as truncation or summarization fail to preserve complete outputs, making them unsuitable for workflows requiring the full data. This work introduces a method that enables LLMs to process and utilize tool responses of arbitrary length without loss of information. By shifting the model's interaction from raw data to memory pointers, the method preserves tool functionality, allows seamless integration into agentic workflows, and reduces token usage and execution time. The proposed method is validated on a real-world Materials Science application that cannot be executed with conventional workflows, and its effectiveness is demonstrated via a comparative analysis where both methods succeed. In this experiment, the proposed approach consumed approximately seven times fewer tokens than the traditional workflow.

</details>


### [118] [Agentic AI Framework for Individuals with Disabilities and Neurodivergence: A Multi-Agent System for Healthy Eating, Daily Routines, and Inclusive Well-Being](https://arxiv.org/abs/2511.22737)
*Salman Jan,Toqeer Ali Syed,Gohar Ali,Ali Akarma,Mohammad Riyaz Belgaum,Ahmad Ali*

Main category: cs.AI

TL;DR: 本文介绍了一种 Agentic 人工智能 (AI) 模型，旨在帮助残疾人和神经多样性人群过上更健康的生活并拥有更规律的日常生活。


<details>
  <summary>Details</summary>
Motivation: 为了帮助残疾人和神经多样性人群改善健康状况和日常生活规律。

Method: 该系统采用多层结构，包括应用和界面层、代理层和数据源层，以提供自适应、透明和包容的支持。混合推理引擎将同步四个专用代理：膳食计划代理、提醒代理、食物指导代理和监控代理。所有代理通过一个中央通信系统（称为黑板/事件总线）进行交互。隐私敏感数据源，包括电子健康记录 (EHR)、营养数据库、可穿戴传感器和智能厨房物联网，也被纳入框架中，并置于策略控制层中，以确保数据安全和合规性。

Result: 所提出的 agentic AI 框架超越了传统的辅助系统，因为它在各个层面都融合了包容性、个性化和可访问性。

Conclusion: 该系统展示了多智能体推理、多模式界面和以人为本的设计的交叉点，这将使残疾人和神经多样性人群能够发展自主性、健康和数字公平。

Abstract: The paper presents a detailed Agentic Artificial Intelligence (AI) model that would enable people with disabilities and neurodivergence to lead healthier lives and have more regular days. The system will use a multi-layer structure; it will include an Application and Interface Layer, an Agents Layer, and a Data Source Layer to provide adaptive, transparent, and inclusive support. Fundamentally, a hybrid reasoning engine will synchronize four special-purpose agents, which include: a personalized-nutrition-based, called a Meal Planner Agent; an adaptive-scheduling-based, called a Reminder Agent; interactive assistance during grocery shopping and cooking, called a Food Guidance Agent; and a continuous-intake-and-physiological-tracking, called a Monitoring Agent. All the agents interact through a central communicative system called the Blackboard/Event Bus, which allows autonomous interaction and real-time feedback loops with multimedia user interfaces. Privacy-sensitive data sources, including electronic health records (EHRs), nutritional databases, wearable sensors, and smart kitchen Internet of Things, are also included in the framework and placed into a policy-controlled layer, which ensures data safety and compliance with consent. Collaborative care and clinician dashboards allow common supervision, and discussable artificial intelligence (XAI) modules give brief explanations of why a decision was made, making users responsible and reliant. The proposed agentic AI framework is an extension beyond traditional assistive systems since it incorporates inclusiveness, personalization, and accessibility at all levels. It displays the intersection of multi-agent reasoning, multi-modal interfaces, and human-centered design that will enable the development of autonomy, health, and digital equity among people with disabilities and neurodivergence.

</details>


### [119] [Agentic AI Framework for Cloudburst Prediction and Coordinated Response](https://arxiv.org/abs/2511.22767)
*Toqeer Ali Syed,Sohail Khan,Salman Jan,Gohar Ali,Muhammad Nauman,Ali Akarma,Ahmad Ali*

Main category: cs.AI

TL;DR: 本文提出了一种agentic人工智能系统，以研究大气水循环智能，将感知、预测、降尺度、水文建模和协调响应结合到一个单一的闭环系统中。


<details>
  <summary>Details</summary>
Motivation: 传统预测系统难以应对极端和短时降雨事件，如云爆。

Method: 该框架使用自主但合作的代理，这些代理在整个事件生命周期中进行推理、感知和行动，并利用天气预测的智能成为实时决策智能。

Result: 在巴基斯坦北部进行的多年度雷达、卫星和地面评估表明，与基线模型相比，多代理配置提高了预测可靠性、关键成功指数和预警提前期。通过通信和路由代理最大限度地扩大了人口覆盖范围，并最大限度地减少了疏散期间的错误，并且嵌入式学习层提供了自适应重新校准和透明的可审计性。

Conclusion: 协作式人工智能代理能够将大气数据流转化为可行的远见，并提供一个可扩展的、基于自适应和学习的气候适应平台。

Abstract: The challenge is growing towards extreme and short-duration rainfall events like a cloudburst that are peculiar to the traditional forecasting systems, in which the predictions and the response are taken as two distinct processes. The paper outlines an agentic artificial intelligence system to study atmospheric water-cycle intelligence, which combines sensing, forecasting, downscaling, hydrological modeling and coordinated response into a single, interconnected, priceless, closed-loop system. The framework uses autonomous but cooperative agents that reason, sense, and act throughout the entire event lifecycle, and use the intelligence of weather prediction to become real-time decision intelligence. Comparison of multi-year radar, satellite, and ground-based evaluation of the northern part of Pakistan demonstrates that the multi-agent configuration enhances forecast reliability, critical success index and warning lead time compared to the baseline models. Population reach was maximised, and errors during evacuation were minimised through communication and routing agents, and adaptive recalibration and transparent auditability were provided by the embedded layer of learning. Collectively, this leads to the conclusion that collaborative AI agents are capable of transforming atmospheric data streams into practicable foresight and provide a platform of scalable adaptive and learning-based climate resilience.

</details>


### [120] [Fast dynamical similarity analysis](https://arxiv.org/abs/2511.22828)
*Arman Behrad,Mitchell Ostrow,Mohammad Taha Fakharian,Ila Fiete,Christian Beste,Shervin Safavi*

Main category: cs.AI

TL;DR: 提出了快速动态相似性分析（fastDSA），一种计算效率更高的动态系统比较方法。


<details>
  <summary>Details</summary>
Motivation: 传统的相似性度量忽略了神经表征下的动态过程，而动态相似性方法计算缓慢。

Method: 通过数据驱动的奇异值阈值自动选择Hankel嵌入的有效模型阶数，并采用新的优化程序和目标函数，以更快的速度找到动力学矩阵之间的最小距离。

Result: fastDSA比以前的方法快至少一个数量级，同时保持了其准确性和鲁棒性。

Conclusion: fastDSA为动态相似性分析提供了一种计算高效且准确的方法。

Abstract: To understand how neural systems process information, it is often essential to compare one circuit with another, one brain with another, or data with a model. Traditional similarity measures ignore the dynamical processes underlying neural representations. Dynamical similarity methods offer a framework to compare the temporal structure of dynamical systems by embedding their (possibly) nonlinear dynamics into a globally linear space and there computing conjugacy metrics. However, identifying the best embedding and computing these metrics can be computationally slow. Here we introduce fast Dynamical Similarity Analysis (fastDSA), which is computationally far more efficient than previous methods while maintaining their accuracy and robustness. FastDSA introduces two key components that boost efficiency: (1) automatic selection of the effective model order of the Hankel (delay) embedding from the data via a data-driven singular-value threshold that identifies the informative subspace and discards noise to lower computational cost without sacrificing signal, and (2) a novel optimization procedure and objective, which replaces the slow exact orthogonality constraint in finding a minimal distance between dynamics matrices with a lightweight process to keep the search close to the space of orthogonal transformations. We demonstrate that fastDSA is at least an order of magnitude faster than the previous methods. Furthermore, we demonstrate that fastDSA has the properties of its ancestor, including its invariances and sensitivities to system dynamics. FastDSA, therefore, provides a computationally efficient and accurate method for dynamical similarity analysis.

</details>


### [121] [InsightEval: An Expert-Curated Benchmark for Assessing Insight Discovery in LLM-Driven Data Agents](https://arxiv.org/abs/2511.22884)
*Zhenghao Zhu,Yuanfeng Song,Xin Chen,Chengzhong Liu,Yakun Cui,Caleb Chen Cao,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: 大型语言模型和多智能体系统被越来越多地用于洞察发现，但缺乏评估洞察发现能力的基准。


<details>
  <summary>Details</summary>
Motivation: 现有的InsightBench框架存在格式不一致、目标设计不良和洞察冗余等严重缺陷，这些问题会严重影响数据质量和智能体的评估。

Method: 彻底调查InsightBench的缺点，并提出高质量洞察基准的基本标准。开发了一个数据管理流程来构建一个名为InsightEval的新数据集。引入了一种新的度量标准来衡量智能体的探索性能。

Result: 通过在InsightEval上进行的大量实验，强调了自动洞察发现中普遍存在的挑战，并提出了一些关键发现，以指导未来在该领域的研究。

Conclusion: 解决了InsightBench的缺陷，并为未来的研究提供指导。

Abstract: Data analysis has become an indispensable part of scientific research. To discover the latent knowledge and insights hidden within massive datasets, we need to perform deep exploratory analysis to realize their full value. With the advent of large language models (LLMs) and multi-agent systems, more and more researchers are making use of these technologies for insight discovery. However, there are few benchmarks for evaluating insight discovery capabilities. As one of the most comprehensive existing frameworks, InsightBench also suffers from many critical flaws: format inconsistencies, poorly conceived objectives, and redundant insights. These issues may significantly affect the quality of data and the evaluation of agents. To address these issues, we thoroughly investigate shortcomings in InsightBench and propose essential criteria for a high-quality insight benchmark. Regarding this, we develop a data-curation pipeline to construct a new dataset named InsightEval. We further introduce a novel metric to measure the exploratory performance of agents. Through extensive experiments on InsightEval, we highlight prevailing challenges in automated insight discovery and raise some key findings to guide future research in this promising direction.

</details>


### [122] [ORION: Teaching Language Models to Reason Efficiently in the Language of Thought](https://arxiv.org/abs/2511.22891)
*Kumar Tanmay,Kriti Aggarwal,Paul Pu Liang,Subhabrata Mukherjee*

Main category: cs.AI

TL;DR: ORION模型通过模拟人类的“心理语”进行压缩推理，减少token数量，降低延迟和训练成本，同时保持或提高准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRM）依赖冗长的“思考”token链导致高延迟和冗余，受“思维语言假说”启发，提出训练模型以更紧凑的方式推理。

Method: 引入一个框架，训练模型以类似的方式进行推理，将抽象推理编码为超压缩、结构化的token。提出SHORTER LENGTH PREFERENCE OPTIMIZATION (SLPO)，一种强化学习方法，奖励简洁且正确的解决方案。

Result: ORION模型在多个基准测试中，推理token减少4-16倍，推理延迟降低5倍，训练成本降低7-9倍，同时保持90-98%的准确率。在保持2倍压缩的同时，ORION的准确率超过Claude和ChatGPT-4o高达5%。

Conclusion: Mentalese风格的压缩推理朝着类人认知效率迈出了一步，能够在不牺牲准确性的前提下实现实时、经济高效的推理。

Abstract: Large Reasoning Models (LRMs) achieve strong performance in mathematics, code generation, and task planning, but their reliance on long chains of verbose "thinking" tokens leads to high latency, redundancy, and incoherent reasoning paths. Inspired by the Language of Thought Hypothesis, which posits that human reasoning operates over a symbolic, compositional mental language called Mentalese, we introduce a framework that trains models to reason in a similarly compact style. Mentalese encodes abstract reasoning as ultra-compressed, structured tokens, enabling models to solve complex problems with far fewer steps. To improve both efficiency and accuracy, we propose SHORTER LENGTH PREFERENCE OPTIMIZATION (SLPO), a reinforcement learning method that rewards concise solutions that stay correct, while still allowing longer reasoning when needed. Applied to Mentalese-aligned models, SLPO yields significantly higher compression rates by enabling concise reasoning that preserves the benefits of detailed thinking without the computational overhead. Across benchmarks including AIME 2024 and 2025, MinervaMath, OlympiadBench, Math500, and AMC, our ORION models produce reasoning traces with 4-16x fewer tokens, achieve up to 5x lower inference latency, and reduce training costs by 7-9x relative to the DeepSeek R1 Distilled model, while maintaining 90-98% of its accuracy. ORION also surpasses Claude and ChatGPT-4o by up to 5% in accuracy while maintaining 2x compression. These results show that Mentalese-style compressed reasoning offers a step toward human-like cognitive efficiency, enabling real-time, cost-effective reasoning without sacrificing accuracy.

</details>


### [123] [TIM-PRM: Verifying multimodal reasoning with Tool-Integrated PRM](https://arxiv.org/abs/2511.22998)
*Peng Kuang,Xiangxiang Wang,Wentao Liu,Jian Dong,Kaidi Xu,Haohan Wang*

Main category: cs.AI

TL;DR: TIM-PRM: A tool-integrated multimodal process reward model that improves mathematical reasoning by actively verifying reasoning steps with external tools, reducing visual hallucinations and logical inconsistencies.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with visual hallucinations and logical inconsistencies in mathematical reasoning, and current PRMs suffer from sycophancy, failing to ground hypotheses in visual reality.

Method: Introduces TIM-PRM, an agentic framework that plans verification strategies and uses Independent Question Asking to query evidence via external tools, decoupling verification from the reasoning context.

Result: TIM-PRM outperforms existing open-source multimodal PRMs and much larger models on VisualProcessBench.

Conclusion: TIM-PRM offers interpretable insights into the verification process by transforming verification into an active, tool-augmented investigation, reducing confirmation bias.

Abstract: Multimodal Large Language Models (MLLMs) have achieved impressive performances in mathematical reasoning, yet they remain vulnerable to visual hallucinations and logical inconsistencies that standard outcome-based supervision fails to mitigate. While Process Reward Models (PRMs) promise step-by-step verification, current approaches typically operate as scalar scorers or generative critics that suffer from sycophancy, blindly validating the flawed hypotheses rather than grounding them in visual reality. To bridge this gap, we introduce TIM-PRM (Tool-Integrated Multimodal PRM), a novel agentic framework that transforms verification from a passive classification task into an active, tool-augmented investigation. TIM-PRM is trained to explicitly plan verification strategies and utilizes a mechanism of Independent Question Asking to query evidence via external tools, effectively decoupling verification from the reasoning context to eliminate confirmation bias. We instantiate this method by curating a high-quality dataset of tool-integrated verification trajectories. Extensive experiments on VisualProcessBench demonstrate that our 8B parameter model surpasses existing open-source multimodal PRMs, significantly outperforming much larger models like Qwen2.5-72B and InternVL-78B, while offering interpretable insights into the verification process.

</details>


### [124] [MindPower: Enabling Theory-of-Mind Reasoning in VLM-based Embodied Agents](https://arxiv.org/abs/2511.23055)
*Ruoxuan Zhang,Qiyun Zheng,Zhiyu Zhou,Ziqi Liao,Siyu Wu,Jian-Yu Jiang-Lin,Bin Wen,Hongxia Xie,Jianlong Fu,Wen-Huang Cheng*

Main category: cs.AI

TL;DR: 本研究旨在提升具身智能体的心智理论（ToM）能力，使其能够更好地理解自身和他人的心理状态，从而做出更明智的决策和行动。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言具身智能体缺乏基于ToM的决策能力，并且现有基准测试忽略了智能体自身的视角，阻碍了连贯的决策和行动生成。

Method: 提出了一个机器人中心的框架MindPower，该框架集成了感知、心理推理、决策和行动，并引入了一种新的优化目标Mind-Reward，以鼓励视觉-语言模型产生一致的ToM推理和行为。

Result: 该模型在决策制定方面优于GPT-4o 12.77%，在行动生成方面优于GPT-4o 12.49%。

Conclusion: MindPower框架和Mind-Reward优化目标能够有效提升具身智能体的心智理论能力，使其在决策和行动生成方面表现更出色。

Abstract: Theory of Mind (ToM) refers to the ability to infer others' mental states, such as beliefs, desires, and intentions. Current vision-language embodied agents lack ToM-based decision-making, and existing benchmarks focus solely on human mental states while ignoring the agent's own perspective, hindering coherent decision and action generation. To address this, we propose MindPower, a Robot-Centric framework integrating Perception, Mental Reasoning, Decision Making and Action. Given multimodal inputs, MindPower first perceives the environment and human states, then performs ToM Reasoning to model both self and others, and finally generates decisions and actions guided by inferred mental states. Furthermore, we introduce Mind-Reward, a novel optimization objective that encourages VLMs to produce consistent ToM Reasoning and behavior. Our model outperforms GPT-4o by 12.77% in decision making and 12.49% in action generation.

</details>


### [125] [Does Self-Evaluation Enable Wireheading in Language Models?](https://arxiv.org/abs/2511.23092)
*David Demitri Africa,Hans Ethan Ting*

Main category: cs.AI

TL;DR: 自我评估在语言模型训练中变得越来越重要。研究表明，将自我评估与奖励信号相结合会产生导线连接的激励，即智能体操纵奖励测量而不是提高任务性能。研究发现，自我评估决定奖励的模型表现出显著的等级膨胀，而准确性没有相应提高，尤其是在摘要等模糊任务上。


<details>
  <summary>Details</summary>
Motivation: 研究自我评估与奖励信号相结合是否会激励智能体进行导线连接，即操纵奖励测量而不是提高任务性能。

Method: 在 POMDP 中形式化了奖励通道控制严格支配以任务为中心的行为的条件，并通过实验验证了这些预测。使用了两个模型和三个任务。

Result: 发现自我评估决定奖励的模型表现出显著的等级膨胀，而准确性没有相应提高，尤其是在摘要等模糊任务上。不控制奖励的自我评估模型没有表现出这种膨胀。

Conclusion: 自我评估与学习信号分离时是安全的，但耦合时是危险的，这对智能体系统设计具有明确的意义。

Abstract: Self-evaluation is increasingly central to language model training, from constitutional AI to self-refinement. We investigate whether coupling self-evaluation to reward signals creates incentives for wireheading, where agents manipulate reward measurements rather than improving task performance. We formalize conditions under which reward-channel control strictly dominates task-focused behavior in POMDPs and test these predictions empirically. Across two models and three tasks, we find that models whose self-grades determine rewards exhibit substantial grade inflation without corresponding accuracy gains, particularly on ambiguous tasks like summarization. Models that self-evaluate but do not control rewards show no such inflation. Our results demonstrate that self-evaluation is safe when decoupled from learning signals but dangerous when coupled, with clear implications for agentic system design.

</details>


### [126] [Evolutionary Discovery of Heuristic Policies for Traffic Signal Control](https://arxiv.org/abs/2511.23122)
*Ruibing Wang,Shuhan Guo,Zeen Li,Zhen Wang,Quanming Yao*

Main category: cs.AI

TL;DR: 提出了一种新的交通信号控制框架，该框架利用大型语言模型（LLM）作为进化引擎来推导专门的启发式策略，以解决传统启发式算法和深度强化学习（DRL）在交通信号控制中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统启发式算法效率高但过于简化，深度强化学习（DRL）性能好但泛化能力差且策略不透明。在线大型语言模型（LLM）提供通用推理但延迟高且缺乏特定环境优化。

Method: Temporal Policy Evolution for Traffic (TPET)，它使用LLM作为进化引擎来推导专门的启发式策略。该框架引入了两个关键模块：(1) 结构化状态抽象 (SSA)，将高维交通数据转换为用于推理的时序逻辑事实；(2) 信用分配反馈 (CAF)，追踪有缺陷的微观决策到不良的宏观结果，以便进行有针对性的评论。

Result: TPET 产生针对特定交通环境优化的轻量级、稳健的策略，优于启发式算法和在线 LLM 参与者。

Conclusion: 该研究提出了一种新的交通信号控制框架，通过LLM进化引擎和结构化状态抽象、信用分配反馈等模块，实现了轻量级、稳健且针对特定环境优化的策略。

Abstract: Traffic Signal Control (TSC) involves a challenging trade-off: classic heuristics are efficient but oversimplified, while Deep Reinforcement Learning (DRL) achieves high performance yet suffers from poor generalization and opaque policies. Online Large Language Models (LLMs) provide general reasoning but incur high latency and lack environment-specific optimization. To address these issues, we propose Temporal Policy Evolution for Traffic (\textbf{\method{}}), which uses LLMs as an evolution engine to derive specialized heuristic policies. The framework introduces two key modules: (1) Structured State Abstraction (SSA), converting high-dimensional traffic data into temporal-logical facts for reasoning; and (2) Credit Assignment Feedback (CAF), tracing flawed micro-decisions to poor macro-outcomes for targeted critique. Operating entirely at the prompt level without training, \method{} yields lightweight, robust policies optimized for specific traffic environments, outperforming both heuristics and online LLM actors.

</details>


### [127] [Peer-to-Peer Energy Trading in Dairy Farms using Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.23148)
*Mian Ibad Ali Shah,Marcos Eduardo Cruz Victorio,Maeve Duffy,Enda Barrett,Karl Mason*

Main category: cs.AI

TL;DR: 本研究探讨了点对点 (P2P) 能源交易在农村地区（如奶牛养殖社区）分散式能源管理中的作用，并结合多智能体强化学习 (MARL) 优化能源分配。


<details>
  <summary>Details</summary>
Motivation: 传统方法在动态环境中表现不佳，需要更有效的方法

Method: 结合近端策略优化 (PPO) 和深度 Q 网络 (DQN) 的多智能体强化学习 (MARL) 与基于拍卖的市场清算、价格顾问代理以及负载和电池管理相结合。

Result: DQN 在爱尔兰和芬兰分别降低了 14.2% 和 5.16% 的电力成本，同时分别增加了 7.24% 和 12.73% 的电力收入。PPO 在爱尔兰将高峰时段需求降低了 55.5%，而 DQN 在爱尔兰和芬兰分别降低了 50.0% 和 27.02%。

Conclusion: MARL 算法和 P2P 能源交易的互补优势，共同实现了电力成本和高峰时段需求降低，并增加了电力销售收入，实现了农村社区高效、适应性强和可持续的能源管理。

Abstract: The integration of renewable energy resources in rural areas, such as dairy farming communities, enables decentralized energy management through Peer-to-Peer (P2P) energy trading. This research highlights the role of P2P trading in efficient energy distribution and its synergy with advanced optimization techniques. While traditional rule-based methods perform well under stable conditions, they struggle in dynamic environments. To address this, Multi-Agent Reinforcement Learning (MARL), specifically Proximal Policy Optimization (PPO) and Deep Q-Networks (DQN), is combined with community/distributed P2P trading mechanisms. By incorporating auction-based market clearing, a price advisor agent, and load and battery management, the approach achieves significant improvements. Results show that, compared to baseline models, DQN reduces electricity costs by 14.2% in Ireland and 5.16% in Finland, while increasing electricity revenue by 7.24% and 12.73%, respectively. PPO achieves the lowest peak hour demand, reducing it by 55.5% in Ireland, while DQN reduces peak hour demand by 50.0% in Ireland and 27.02% in Finland. These improvements are attributed to both MARL algorithms and P2P energy trading, which together results in electricity cost and peak hour demand reduction, and increase electricity selling revenue. This study highlights the complementary strengths of DQN, PPO, and P2P trading in achieving efficient, adaptable, and sustainable energy management in rural communities.

</details>


### [128] [AgriCoT: A Chain-of-Thought Benchmark for Evaluating Reasoning in Vision-Language Models for Agriculture](https://arxiv.org/abs/2511.23253)
*Yibin Wen,Qingmei Li,Zi Ye,Jiarui Zhang,Jing Wu,Zurong Mai,Shuohong Lou,Yuhang Chen,Henglian Huang,Xiaoya Fan,Yang Zhang,Lingyuan Zhao,Haohuan Fu,Huang Jianxi,Juepeng Zheng*

Main category: cs.AI

TL;DR: 论文介绍了一个新的VQA数据集AgriCoT，用于评估视觉语言模型在农业领域的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的VQA数据集不足以评估视觉语言模型在复杂农业场景下的推理和问题解决能力。

Method: 构建了一个包含4535个样本的AgriCoT数据集，该数据集结合了思维链（CoT）推理，专门用于评估视觉语言模型的推理能力。

Result: 对26个具有代表性的视觉语言模型进行了评估，结果表明，尽管一些专有模型在回答问题方面表现出色，但它们的推理能力存在显著差距。

Conclusion: 强调了结合CoT进行更精确和有效的评估的重要性。

Abstract: Recent advancements in Vision-Language Models (VLMs) have significantly transformed various industries. In agriculture, these dual-modal capabilities offer promising applications such as precision farming, crop monitoring, pest detection, and environmental sustainability. While several Visual Question Answering (VQA) datasets and benchmarks have been developed to evaluate VLM performance, they often fail to adequately assess the critical reasoning and problem-solving skills required in complex agricultural contexts. To address this gap, we introduce AgriCoT, a VQA dataset that incorporates Chain-of-Thought (CoT) reasoning, specifically designed to evaluate the reasoning capabilities of VLMs. With 4,535 carefully curated samples, AgriCoT offers a comprehensive and robust evaluation of reasoning abilities for VLMs, particularly in zero-shot scenarios, by focusing on their capacity to engage in logical reasoning and effective problem-solving. Our evaluations, conducted with 26 representative VLMs, including both proprietary and open-source models, reveal that while some proprietary models excel at answering questions, there is a notable and significant gap in their reasoning capabilities. This underscores the importance of incorporating CoT for more precise and effective assessments. Our dataset are available at https://huggingface.co/datasets/wenyb/AgriCoT.

</details>


### [129] [Adapting Like Humans: A Metacognitive Agent with Test-time Reasoning](https://arxiv.org/abs/2511.23262)
*Yang Li,Zhiyuan He,Yuxuan Huang,Zhuhanling Xiao,Chao Yu,Meng Fang,Kun Shao,Jun Wang*

Main category: cs.AI

TL;DR: 提出了一个名为MCTR的框架，该框架使模型能够在测试时通过元认知自我更新来学习、适应和改进。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型(VLM)表现出强大的感知推理能力，但当在测试时遇到新任务时，它们通常难以有效适应。相比之下，人类利用具有记忆的元认知模型，从而能够在面对新挑战时通过元认知控制不断改进策略。

Method: MCTR包括元级别和对象级别VLM推理模块，每个模块都配备了专用的存储系统，用于分层自适应推理。元推理模块通过从测试时观察中发现和存储与任务相关的规则、环境模式和动作结果关系作为自然语言描述，从而逐步构建结构化记忆。动作推理模块通过动态检索和整合记忆中的知识，并通过提出的元认知测试时强化学习不断更新其策略来确定最佳动作。

Result: 在45个Atari游戏中评估了MCTR（33个已见，12个未见）。MCTR展示了强大的测试时适应能力，与基线相比，在未见游戏上获得了9/12个第一名。

Conclusion: 消融、学习动态和案例研究的分析揭示了两个组成部分的互补贡献，并表明元推理正在朝着类似人类的适应策略发展。

Abstract: Recent Vision-Language Models (VLMs) exhibit strong perceptual reasoning abilities, yet they often struggle to adapt efficiently when encountering novel tasks at test time. In contrast, humans leverage the metacognitive model with memory, enabling continuous strategy refinement through metacognitive control when faced with new challenges. To bridge this gap, we propose metacognitive test-time reasoning (MCTR), a framework that equips models with the ability to learn, adapt, and improve during test time through metacognitive self-updating. Inspired by the dual structure of human metacognition, MCTR comprises meta-level and object-level VLM reasoning modules, each equipped with dedicated memory systems for hierarchical adaptive reasoning. Specifically, MCTR consists of (1) a meta-reasoning module which incrementally builds a structured memory by discovering and storing task-relevant rules, environmental patterns, and action-outcome relationships from test-time observations as natural language descriptions; and (2) an action-reasoning module that determines optimal actions through context-aware perception and strategic reasoning by dynamically retrieving and integrating knowledge from memory. The action-reasoning module continuously updates its policy through proposed metacognitive test-time reinforcement learning, adapting as knowledge memory evolves. We evaluate MCTR on 45 Atari games (33 seen, 12 unseen). MCTR demonstrates robust test-time adaptation, achieving 9/12 top-1 results on unseen games compared with baselines. Analyses through ablations, learning dynamics, and case studies reveal the complementary contributions of both components and show meta-reasoning evolving toward human-like adaptation strategies.

</details>


### [130] [OctoMed: Data Recipes for State-of-the-Art Multimodal Medical Reasoning](https://arxiv.org/abs/2511.23269)
*Timothy Ossowski,Sheng Zhang,Qianchu Liu,Guanghui Qin,Reuben Tan,Tristan Naumann,Junjie Hu,Hoifung Poon*

Main category: cs.AI

TL;DR: 本研究着眼于在医疗领域开发稳健的多模态推理模型，重点关注监督式微调（SFT）和利用结构化推理轨迹的数据配方。


<details>
  <summary>Details</summary>
Motivation: 高质量的医学大型语言模型训练依赖于精心策划的数据，这直接影响模型对未见临床任务的泛化能力和稳健性。

Method: 采用了监督式微调（SFT）方法，并探索了利用结构化推理轨迹的数据配方。实验扩展到超过800万个样本和68亿个响应tokens的数据集。

Result: 在各种分布外医学基准测试任务中，该模型在开源模型中取得了最先进的性能。高质量、多样化的训练数据集能够使微调模型根据下游任务自我校准其推理轨迹长度，而无需显式监督。

Conclusion: 本研究提出了关键见解，描述了数据管理策略，并概述了开发稳健的医学视觉-语言推理系统的后续步骤。

Abstract: High-quality and carefully curated data is a cornerstone of training medical large language models, as it directly impacts both generalization and robustness to unseen clinical tasks. We investigate strategies for training and data curation to develop a robust multimodal reasoning model in the medical domain. Our work focuses on supervised fine-tuning (SFT) and explores data recipes that leverage structured reasoning traces. Using our proposed data recipe, we scale experiments to a dataset of over 8 million examples and 6.8 billion response tokens, achieving state-of-the-art performance among open-source models across diverse out-of-distribution medical benchmark tasks. Our results further indicate that curating a high-quality, diverse training dataset with varying structured reasoning trace lengths enables the fine-tuned model to self-calibrate its reasoning trajectory lengths based on the downstream task, without explicit supervision. We present key insights, describe the data curation strategy, and outline next steps toward developing robust medical vision-language reasoning system.

</details>


### [131] [Multi-Modal Scene Graph with Kolmogorov-Arnold Experts for Audio-Visual Question Answering](https://arxiv.org/abs/2511.23304)
*Zijian Fu,Changsheng Lv,Mengshi Qi,Huadong Ma*

Main category: cs.AI

TL;DR: 这篇论文提出了一个名为SHRIKE的新型多模态场景图，用于解决视听问答（AVQA）问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法捕捉视频中的结构信息，并且对多模态特征的细粒度建模不足。

Method: 引入了一个新的多模态场景图，该图将对象及其关系明确地建模为视听场景的视觉基础结构化表示。设计了一个基于Kolmogorov-Arnold Network（KAN）的混合专家（MoE）来增强时间整合阶段的表达能力。

Result: 在MUSIC-AVQA和MUSIC-AVQA v2基准测试中评估了该模型，并取得了最先进的性能。

Conclusion: 该模型能够捕获更丰富、更细致的模式，并提高时间推理性能。

Abstract: In this paper, we propose a novel Multi-Modal Scene Graph with Kolmogorov-Arnold Expert Network for Audio-Visual Question Answering (SHRIKE). The task aims to mimic human reasoning by extracting and fusing information from audio-visual scenes, with the main challenge being the identification of question-relevant cues from the complex audio-visual content. Existing methods fail to capture the structural information within video, and suffer from insufficient fine-grained modeling of multi-modal features. To address these issues, we are the first to introduce a new multi-modal scene graph that explicitly models the objects and their relationship as a visually grounded, structured representation of the audio-visual scene. Furthermore, we design a Kolmogorov-Arnold Network~(KAN)-based Mixture of Experts (MoE) to enhance the expressive power of the temporal integration stage. This enables more fine-grained modeling of cross-modal interactions within the question-aware fused audio-visual representation, leading to capture richer and more nuanced patterns and then improve temporal reasoning performance. We evaluate the model on the established MUSIC-AVQA and MUSIC-AVQA v2 benchmarks, where it achieves state-of-the-art performance. Code and model checkpoints will be publicly released.

</details>


### [132] [Agentic AI Framework for Smart Inventory Replenishment](https://arxiv.org/abs/2511.23366)
*Toqeer Ali Syed,Salman Jan,Gohar Ali,Ali Akarma,Ahmad Ali,Qurat-ul-Ain Mastoi*

Main category: cs.AI

TL;DR: 提出了一种用于零售的agentic AI模型，以监控库存、发起采购、寻找趋势产品。


<details>
  <summary>Details</summary>
Motivation: 预测需求、防止缺货、寻找高潜力产品在零售中具有挑战性。

Method: 应用需求预测、供应商选择优化、多智能体协商和持续学习。

Result: 减少了缺货，降低了库存持有成本，并提高了产品组合的营业额。

Conclusion: 讨论了约束、可扩展性以及改进前景。

Abstract: In contemporary retail, the variety of products available (e.g. clothing, groceries, cosmetics, frozen goods) make it difficult to predict the demand, prevent stockouts, and find high-potential products. We suggest an agentic AI model that will be used to monitor the inventory, initiate purchase attempts to the appropriate suppliers, and scan for trending or high-margin products to incorporate. The system applies demand forecasting, supplier selection optimization, multi-agent negotiation and continuous learning. We apply a prototype to a setting in the store of a middle scale mart, test its performance on three conventional and artificial data tables, and compare the results to the base heuristics. Our findings indicate that there is a decrease in stockouts, a reduction of inventory holding costs, and an improvement in product mix turnover. We address constraints, scalability as well as improvement prospect.

</details>


### [133] [Hierarchical AI-Meteorologist: LLM-Agent System for Multi-Scale and Explainable Weather Forecast Reporting](https://arxiv.org/abs/2511.23387)
*Daniil Sukhorukov,Andrei Zakharov,Nikita Glazkov,Katsiaryna Yanchanka,Vladimir Kirilin,Maxim Dubovitsky,Roman Sultimov,Yuri Maksimov,Ilya Makarov*

Main category: cs.AI

TL;DR:  hierarchical AI-Meteorologist generates explainable weather reports using multi-scale reasoning and weather keyword generation.


<details>
  <summary>Details</summary>
Motivation: Unlike standard approaches that treat forecasts as flat time series, our framework performs multi-scale reasoning across hourly, 6-hour, and daily aggregations to capture both short-term dynamics and long-term trends.

Method: The core reasoning agent converts structured meteorological inputs into coherent narratives while simultaneously extracting a few keywords effectively summarizing the dominant meteorological events. These keywords serve as semantic anchors for validating consistency, temporal coherence and factual alignment of the generated reports.

Result: hierarchical context and keyword-based validation substantially improve interpretability and robustness of LLM-generated weather narratives

Conclusion: offering a reproducible framework for semantic evaluation of automated meteorological reporting and advancing agent-based scientific reasoning.

Abstract: We present the Hierarchical AI-Meteorologist, an LLM-agent system that generates explainable weather reports using a hierarchical forecast reasoning and weather keyword generation. Unlike standard approaches that treat forecasts as flat time series, our framework performs multi-scale reasoning across hourly, 6-hour, and daily aggregations to capture both short-term dynamics and long-term trends. Its core reasoning agent converts structured meteorological inputs into coherent narratives while simultaneously extracting a few keywords effectively summarizing the dominant meteorological events. These keywords serve as semantic anchors for validating consistency, temporal coherence and factual alignment of the generated reports. Using OpenWeather and Meteostat data, we demonstrate that hierarchical context and keyword-based validation substantially improve interpretability and robustness of LLM-generated weather narratives, offering a reproducible framework for semantic evaluation of automated meteorological reporting and advancing agent-based scientific reasoning.

</details>


### [134] [Towards Continuous Intelligence Growth: Self-Training, Continual Learning, and Dual-Scale Memory in SuperIntelliAgent](https://arxiv.org/abs/2511.23436)
*Jianzhe Lin,Zeyu Pan,Yun Zhu,Ruiqi Song,Jining Yang*

Main category: cs.AI

TL;DR: SuperIntelliAgent是一个agentic学习框架，它通过自监督交互实现持续的智能增长。


<details>
  <summary>Details</summary>
Motivation: 传统的监督微调需要大量标注数据，而SuperIntelliAgent旨在自主学习，无需人工标注。

Method: 该框架耦合了一个可训练的小型扩散模型（学习者）和一个冻结的大型语言模型（验证者），通过学习者生成候选输出，验证者评估它们，并使用DPO进行优化。

Result: 通过少量自动生成的DPO对，学习者在所有基准测试中都得到了改进。

Conclusion: 将可训练的学习者与具有推理能力的验证者配对，形成了一个最小的可靠的增长智能单元，为持续智能积累和实际部署提供了一个有希望的方向。

Abstract: We introduce SuperIntelliAgent, an agentic learning framework that couples a trainable small diffusion model (the learner) with a frozen large language model (the verifier) to enable continual intelligence growth through self-supervised interaction. Unlike conventional supervised fine-tuning, SuperIntelliAgent learns autonomously without annotation: the learner generates candidate outputs, the verifier evaluates them through step-by-step reasoning, and their interaction produces chosen/rejected pairs for Direct Preference Optimization (DPO). This converts each input into a pseudo-training signal for continual improvement. The framework integrates dual-scale memory: short-term in-context memory that preserves reasoning traces across refinement cycles, and long-term memory that consolidates acquired knowledge through lightweight on-the-fly fine-tuning. A replay buffer retains samples that show verifiable progress and replays them as auxiliary supervision, reinforcing recent learning while forming adaptive curricula. SuperIntelliAgent is infrastructure-agnostic and can be plugged into existing agentic frameworks while turning ordinary inference loops into a lifelong optimization process. We posit that pairing a trainable learner with a reasoning-capable verifier forms a minimal reliable unit of growing intelligence, as paired feedback and partial-history replay yield richer learning curricula and stronger preference alignment. With a small number of automatically generated DPO pairs, the learner improves across all benchmarks, indicating that this mechanism provides a promising direction for continual intelligence accumulation and real-world deployment.

</details>


### [135] [Thinking by Doing: Building Efficient World Model Reasoning in LLMs via Multi-turn Interaction](https://arxiv.org/abs/2511.23476)
*Bao Shu,Yan Cai,Jianjian Sun,Chunrui Han,En Yu,Liang Zhao,Jingcheng Hu,Yinmin Zhang,Haoran Lv,Yuang Peng,Zheng Ge,Xiangyu Zhang,Daxin Jiang,Xiangyu Yue*

Main category: cs.AI

TL;DR: 提出了一种新的世界模型内化方法，通过高效的交互和主动推理（WMAct）来提升LLM agent在复杂环境中的规划和交互能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法推理过程僵化，限制了模型的积极学习，阻碍了高效的世界模型推理。

Method: WMAct包含两个关键机制：奖励重缩放机制，根据行动效果调整结果奖励，以激励冗余减少和有目的的互动；交互频率退火策略，逐步减少最大允许交互次数，迫使模型压缩其学习并内化环境动态。

Result: 在Sokoban、Maze和Taxi上的实验表明，WMAct能够有效地进行世界模型推理，能够在单轮中解决以前需要多次交互的任务，并促进了向复杂环境的强大可转移性，提高了在一系列推理基准上的性能。

Conclusion: WMAct通过解耦结构化推理，使模型能够通过其行为直接塑造思维，实现了有效和高效的世界模型推理。

Abstract: Developing robust world model reasoning is crucial for large language model (LLM) agents to plan and interact in complex environments. While multi-turn interaction offers a superior understanding of environmental dynamics via authentic feedback, current approaches often impose a rigid reasoning process, which constrains the model's active learning, ultimately hindering efficient world model reasoning. To address these issues, we explore world-model internalization through efficient interaction and active reasoning (WMAct), which liberates the model from structured reasoning, allowing the model to shape thinking directly through its doing, and achieves effective and efficient world model reasoning with two key mechanisms: (1) a reward rescaling mechanism adjusting outcome reward based on action efficacy to incentivize redundancy reduction and purposeful interaction; (2) an interaction frequency annealing strategy to progressively reduce the maximum allowed interaction turns, which compels the model to condense its learning and internalize environmental dynamics rather than over-relying on environmental cues. Our experiments on Sokoban, Maze, and Taxi show that WMAct yields effective world model reasoning capable of resolving tasks in a single turn that previously required multiple interactions and fosters strong transferability to complex environments, improving performance on a suite of reasoning benchmarks.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [136] [A Conceptual Model for Context Awareness in Ethical Data Management](https://arxiv.org/abs/2511.21942)
*Elisa Quintarelli,Fabio Alberto Schreiber,Kostas Stefanidis,Letizia Tanca,Barbara Oliboni*

Main category: cs.DB

TL;DR: 本文提出了一种双层概念模型，用于在不同情况下调整和预处理数据集，以保证数据分析和学习系统符合伦理规则。


<details>
  <summary>Details</summary>
Motivation: 信息管理领域日益关注伦理问题，算法和数据应满足伦理规则，避免不当行为。伦理规则应随应用场景变化。

Method: 提出一种双层概念模型，包括描述可能情况的上下文维度树 (CDT) 和表示伦理规则的伦理要求树 (ERT)。

Result: 提供了一些关于如何使用这些概念工具的例子和建议。

Conclusion: 所提出的双层概念模型能够帮助数据分析和学习系统在不同情况下符合伦理规则。

Abstract: Ethics has become a major concern to the information management community, as both algorithms and data should satisfy ethical rules that guarantee not to generate dishonourable behaviours when they are used. However, these ethical rules may vary according to the situation-the context-in which the application programs must work. In this paper, after reviewing the basic ethical concepts and their possible influence on data management, we propose a bipartite conceptual model, composed of the Context Dimensions Tree (CDT), which describes the possible contexts, and the Ethical Requirements Tree (ERT), representing the ethical rules necessary to tailor and preprocess the datasets that should be fed to Data Analysis and Learning Systems in each possible context. We provide some examples and suggestions on how these conceptual tools can be used.

</details>


### [137] [Relation-Stratified Sampling for Shapley Values Estimation in Relational Databases](https://arxiv.org/abs/2511.22035)
*Amirhossein Alizad,Mostafa Milani*

Main category: cs.DB

TL;DR: 本文提出了一种新的采样方法，用于解决关系查询中元组级别归因的难题，该方法优于传统的蒙特卡洛方法和基于大小的分层抽样方法。


<details>
  <summary>Details</summary>
Motivation: 量化单个元组对查询结果的贡献很重要，但是精确计算 Shapley 值是不可行的。

Method: 提出了一种关系分层抽样 (RSS) 方法，该方法通过关系计数向量对样本空间进行分区，并开发了一种自适应变体 ARSS，该变体使用在采样期间获得的方差估计在层之间重新分配预算。

Result: 在具有多关系连接和聚合的不同查询中，RSS 和 ARSS 始终优于经典蒙特卡洛 (MCS) 和基于大小的分层抽样 (SS)，以更少的样本产生更低的误差和方差。

Conclusion: 关系感知分层和自适应分配做出了互补的贡献，使 ARSS 成为一种简单、有效且随时的数据库中心 Shapley 归因估计器。

Abstract: Shapley-like values, including the Shapley and Banzhaf values, provide a principled way to quantify how individual tuples contribute to a query result. Their exact computation, however, is intractable because it requires aggregating marginal contributions over exponentially many permutations or subsets. While sampling-based estimators have been studied in cooperative game theory, their direct use for relational query answering remains underexplored and often ignores the structure of schemas and joins.
  We study tuple-level attribution for relational queries through sampling and introduce Relation-Stratified Sampling (RSS). Instead of stratifying coalitions only by size, RSS partitions the sample space by a relation-wise count vector that records how many tuples are drawn from each relation. This join-aware stratification concentrates samples on structurally valid and informative coalitions and avoids strata that cannot satisfy query conditions. We further develop an adaptive variant, ARSS, that reallocates budget across strata using variance estimates obtained during sampling, improving estimator efficiency without increasing the total number of samples. We analyze these estimators, describe a practical implementation that reuses compiled views to reduce per-sample query cost, and evaluate them on TPCH workloads.
  Across diverse queries with multi-relation joins and aggregates, RSS and ARSS consistently outperform classical Monte Carlo (MCS) and size-based Stratified Sampling (SS), yielding lower error and variance with fewer samples. An ablation shows that relation-aware stratification and adaptive allocation contribute complementary gains, making ARSS a simple, effective, and anytime estimator for database-centric Shapley attribution.

</details>


### [138] [Performant Synchronization in Geo-Distributed Databases](https://arxiv.org/abs/2511.22444)
*Duling Xu,Tong Li,Zegang Sun,Zheng Chen,Weixing Zhou,Yanfeng Zhang,Wei Lu,Xiaoyong Du*

Main category: cs.DB

TL;DR: GeoCoCo is a synchronization acceleration framework for cross-region distributed databases that reduces synchronization cost and increases system throughput.


<details>
  <summary>Details</summary>
Motivation: Synchronization cost constitutes the primary bottleneck for distributed databases, which is particularly pronounced in wide-area networks (WAN).

Method: GeoCoCo presents a group rescheduling strategy, a task-preserving data filtering method, and a consistency-guaranteed transmission framework integrating grouping and pruning.

Result: GeoCoCo reduces synchronization cost by up to 40.3%, and increases system throughput by up to 14.1% in GeoGauss.

Conclusion: GeoCoCo reduces synchronization cost-primarily by lowering WAN bandwidth usage-and increases system throughput.

Abstract: The deployment of databases across geographically distributed regions has become increasingly critical for ensuring data reliability and scalability. Recent studies indicate that distributed databases exhibit significantly higher latency than single-node databases, primarily due to consensus protocols maintaining data consistency across multiple nodes. We argue that synchronization cost constitutes the primary bottleneck for distributed databases, which is particularly pronounced in wide-area networks (WAN). Fortunately, we identify opportunities to optimize synchronization costs in real production environments: (1) network clustering phenomena, (2) triangle inequality violations in transmission, and (3) redundant data transfers. Based on these observations, we propose GeoCoCo, a synchronization acceleration framework for cross-region distributed databases. First, GeoCoCo presents a group rescheduling strategy that adapts to real-time network conditions to maximize WAN transmission efficiency. Second, GeoCoCo introduces a task-preserving data filtering method that reduces data volume transmitted over the WAN. Finally, GeoCoCo develops a consistency-guaranteed transmission framework integrating grouping and pruning. Extensive evaluations in both trace-driven simulations and real-world deployments demonstrate that GeoCoCo reduces synchronization cost-primarily by lowering WAN bandwidth usage-by up to 40.3%, and increases system throughput by up to 14.1% in GeoGauss.

</details>


### [139] [Structured Multi-Step Reasoning for Entity Matching Using Large Language Model](https://arxiv.org/abs/2511.22832)
*Rohan Bopardikar,Jin Wang,Jia Zou*

Main category: cs.DB

TL;DR: 提出了一种基于LLM的实体匹配框架，通过分解匹配过程为多个推理阶段来提高匹配精度。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM实体匹配方法依赖于单步提示，对结构化推理策略的探究有限。

Method: 提出一个三步框架，首先识别两个记录中匹配和不匹配的tokens，然后确定对匹配决策影响最大的属性，最后预测记录是否指向同一实体。此外，还探索了一种基于辩论的策略，对比支持和反对的论点以提高决策的鲁棒性。

Result: 在多个真实世界的实体匹配基准数据集上进行了评估，实验结果表明结构化的多步推理可以在某些情况下提高匹配性能。

Conclusion: 结构化的多步推理可以提高匹配性能，但也存在挑战和进一步改进的机会。

Abstract: Entity matching is a fundamental task in data cleaning and data integration. With the rapid adoption of large language models (LLMs), recent studies have explored zero-shot and few-shot prompting to improve entity matching accuracy. However, most existing approaches rely on single-step prompting and offer limited investigation into structured reasoning strategies. In this work, we investigate how to enhance LLM-based entity matching by decomposing the matching process into multiple explicit reasoning stages. We propose a three-step framework that first identifies matched and unmatched tokens between two records, then determines the attributes most influential to the matching decision, and finally predicts whether the records refer to the same real-world entity. In addition, we explore a debate-based strategy that contrasts supporting and opposing arguments to improve decision robustness. We evaluate our approaches against multiple existing baselines on several real-world entity matching benchmark datasets. Experimental results demonstrate that structured multi-step reasoning can improve matching performance in several cases, while also highlighting remaining challenges and opportunities for further refinement of reasoning-guided LLM approaches.

</details>


### [140] [Extended Serial Safety Net: A Refined Serializability Criterion for Multiversion Concurrency Control](https://arxiv.org/abs/2511.22956)
*Atsushi Kitazawa,Chihaya Ito,Yuta Yoshida,Takamitsu Shioi*

Main category: cs.DB

TL;DR: 本文提出了一种新的并发控制协议ESSN，它是SSN的推广，可以在保证多版本可串行化的前提下，允许更多的事务提交。


<details>
  <summary>Details</summary>
Motivation: 现有的并发控制协议通常假设存在一个单一的串行化点，这与快照隔离不兼容，因为快照隔离中存在读写反依赖。

Method: ESSN基于多版本串行化图（MVSG）准则，并引入了事务的已知总顺序（KTO），通过在提交时进行单次检查来保证单调性。

Result: 在可重现的工作负载下，使用begin-snapshot读取可以将长事务的中止率降低高达约0.25（绝对值），约50%（相对值）。

Conclusion: ESSN是一种有效的并发控制协议，它可以提高事务的吞吐量，同时保证数据的一致性。

Abstract: A long line of concurrency-control (CC) protocols argues correctness via a single serialization point (begin or commit), an assumption that is incompatible with snapshot isolation (SI), where read-write anti-dependencies arise. Serial Safety Net (SSN) offers a lightweight commit-time test but is conservative and effectively anchored on commit time as the sole point. We present ESSN, a principled generalization of SSN that relaxes the exclusion condition to allow more transactions to commit safely, and we prove that this preserves multiversion serializability (MVSR) and that it strictly subsumes SSN. ESSN states an MVSG (Multiversion Serialization Graph)-based criterion and introduces a known total order over transactions (KTO; e.g., begin-ordered or commit-ordered) for reasoning about the graph's serializability. With a single commit-time check under invariant-based semantics, ESSN's exclusion condition preserves monotonicity along per-item version chains, and eliminates chain traversal. The protocol is Direct Serialization Graph (DSG)-based with commit-time work linear in the number of reads and writes, matching SSN's per-version footprint. We also make mixed workloads explicit by defining a Long transaction via strict interval containment of Short transactions, and we evaluate ESSN on reproducible workloads. Under a commit-ordered KTO, using begin-snapshot reads reduces the long-transaction abort rate by up to approximately 0.25 absolute (about 50% relative) compared with SSN.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [141] [Selecting User Histories to Generate LLM Users for Cold-Start Item Recommendation](https://arxiv.org/abs/2511.21989)
*Nachiket Subbaraman,Jaskinder Sarai,Aniruddh Nath,Lichan Hong,Lukasz Heldt,Li Wei,Zhe Zhao*

Main category: cs.IR

TL;DR: 本文提出了一种基于强化学习的框架，利用大型语言模型（LLM）作为用户，选择合适的用户来增强冷启动物品的数据，从而提高推荐系统在冷启动场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在解决冷启动物品问题时，要么无法充分模拟用户行为，要么用户选择策略不够优化。本文旨在克服这些局限性。

Method: 利用LLM作为用户，并设计一个强化学习框架，训练一个策略来选择用于数据增强的用户，以优化冷启动物品的性能。

Result: 在亚马逊商品评论数据集上的实验表明，该方法在冷启动物品的召回率方面取得了显著提高。

Conclusion: 该方法是一种可扩展且高效的增强策略，适用于现代推荐系统。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in reasoning, generalization, and simulating human-like behavior across a wide range of tasks. These strengths present new opportunities to enhance traditional recommendation systems (RS), especially in the cold-start item scenario where newly introduced items lack interactions. Existing works have used LLMs to address cold-start issues in traditional RS through data augmentation, but they have limitations. One recent work directly addresses this issue by prompting LLMs to generate augmented interaction data between randomly sampled users and cold-start items. Then, they train the traditional RS with augmented data, incorporating collaborative signals for cold-start items. Although they use LLMs to provide cold-start items with feedback, they use partial user histories, which does not allow the LLM to fully emulate the user. Furthermore, randomly selecting users is not optimal for augmentation. To address these challenges, we leverage the LLM as a user and develop a reinforcement learning (RL) framework that trains a policy to select users for augmentation, optimizing for cold-start item performance after augmented training. The policy model learns to select users for cold-start item data augmentation based on their behavioral features and histories. To optimize user selection for cold-start item performance, we employ a policy gradient method that updates the policy in the direction of actions that lead to high rewards. Experiments on Amazon Product Review datasets show substantial gains in cold-start item recall, demonstrating the effectiveness of our method as a scalable, serving-efficient augmentation strategy for modern RS.

</details>


### [142] [Evaluating Embedding Models and Pipeline Optimization for AI Search Quality](https://arxiv.org/abs/2511.22240)
*Philip Zhong,Kent Chen,Don Wang*

Main category: cs.IR

TL;DR: 评估文本嵌入模型和pipeline配置在AI搜索系统中的性能。比较不同维度、索引方法和分块策略下的sentence-transformer和生成式嵌入模型（例如All-MPNet、BGE、GTE和Qwen）。


<details>
  <summary>Details</summary>
Motivation: 旨在评估不同文本嵌入模型和pipeline配置对AI驱动搜索系统的影响，并找出最佳实践。

Method: 使用本地大型语言模型从美国市议会会议记录中合成一个包含11,975个查询-块对的自定义评估数据集，并通过参考指标（Top-K准确率和NDCG）衡量检索准确率。

Result: 更高维度的嵌入显著提高搜索质量（例如，Qwen3-Embedding-8B/4096的Top-3准确率约为0.571，而GTE-large/1024为0.412），神经重排序器（例如BGE cross-encoder）进一步提高了排序准确率（Top-3高达0.527）。更细粒度的分块（512个字符与2000个字符）也提高了准确率。

Conclusion: 研究结果表明，更高维度的嵌入、神经重排序器和更细粒度的分块可以提高搜索准确率。未来将关注pipeline自动化和评估。

Abstract: We evaluate the performance of various text embedding models and pipeline configurations for AI-driven search systems. We compare sentence-transformer and generative embedding models (e.g., All-MPNet, BGE, GTE, and Qwen) at different dimensions, indexing methods (Milvus HNSW/IVF), and chunking strategies. A custom evaluation dataset of 11,975 query-chunk pairs was synthesized from US City Council meeting transcripts using a local large language model (LLM). The data pipeline includes preprocessing, automated question generation per chunk, manual validation, and continuous integration/continuous deployment (CI/CD) integration. We measure retrieval accuracy using reference-based metrics: Top-K Accuracy and Normalized Discounted Cumulative Gain (NDCG). Our results demonstrate that higher-dimensional embeddings significantly boost search quality (e.g., Qwen3-Embedding-8B/4096 achieves Top-3 accuracy about 0.571 versus 0.412 for GTE-large/1024), and that neural re-rankers (e.g., a BGE cross-encoder) further improve ranking accuracy (Top-3 up to 0.527). Finer-grained chunking (512 characters versus 2000 characters) also improves accuracy. We discuss the impact of these factors and outline future directions for pipeline automation and evaluation.

</details>


### [143] [FIGROTD: A Friendly-to-Handle Dataset for Image Guided Retrieval with Optional Text](https://arxiv.org/abs/2511.22247)
*Hoang-Bao Le,Allie Tran,Binh T. Nguyen,Liting Zhou,Cathal Gurrin*

Main category: cs.IR

TL;DR: 提出了一种新的图像引导检索（IGROT）方法和数据集，该方法在统一视觉检索和组合检索方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有IGROT研究缺乏可访问的基准和平衡子任务性能的方法，现有模型通常偏向视觉或组合查询。

Method: 提出了一个轻量级但高质量的IGROT数据集FIGROTD，并提出了方差引导特征掩码（VaGFeM）来选择性地增强区分性维度，采用了双重损失设计（InfoNCE + Triplet）以提高组合推理能力。

Result: VaGFeM在FIGROTD上训练，在九个基准测试中取得了有竞争力的结果，在CIRCO上达到34.8 mAP@10，在Sketchy上达到75.7 mAP@200，优于更强的基线。

Conclusion: 该研究提出的FIGROTD数据集和VaGFeM方法，为图像引导检索领域做出了重要贡献，并在多个基准测试中取得了优异的性能。

Abstract: Image-Guided Retrieval with Optional Text (IGROT) unifies visual retrieval (without text) and composed retrieval (with text). Despite its relevance in applications like Google Image and Bing, progress has been limited by the lack of an accessible benchmark and methods that balance performance across subtasks. Large-scale datasets such as MagicLens are comprehensive but computationally prohibitive, while existing models often favor either visual or compositional queries. We introduce FIGROTD, a lightweight yet high-quality IGROT dataset with 16,474 training triplets and 1,262 test triplets across CIR, SBIR, and CSTBIR. To reduce redundancy, we propose the Variance Guided Feature Mask (VaGFeM), which selectively enhances discriminative dimensions based on variance statistics. We further adopt a dual-loss design (InfoNCE + Triplet) to improve compositional reasoning. Trained on FIGROTD, VaGFeM achieves competitive results on nine benchmarks, reaching 34.8 mAP@10 on CIRCO and 75.7 mAP@200 on Sketchy, outperforming stronger baselines despite fewer triplets.

</details>


### [144] [UNION: A Lightweight Target Representation for Efficient Zero-Shot Image-Guided Retrieval with Optional Textual Queries](https://arxiv.org/abs/2511.22253)
*Hoang-Bao Le,Allie Tran,Binh T. Nguyen,Liting Zhou,Cathal Gurrin*

Main category: cs.IR

TL;DR: IGROT是一种通用的检索设置，它统一了组合图像检索(CIR)和基于草图的图像检索(SBIR)两个主要任务。该方法通过引入UNION来解决低数据监督下的IGROT问题，UNION是一种轻量级和可泛化的目标表示，它将图像嵌入与空文本提示融合。


<details>
  <summary>Details</summary>
Motivation: 在低数据监督下，实现图像引导的检索，同时统一组合图像检索(CIR)和基于草图的图像检索(SBIR)两个主要任务。

Method: 提出UNION，一种轻量级和可泛化的目标表示，它将图像嵌入与空文本提示融合，无需对预训练的视觉-语言模型进行架构修改。

Result: 仅使用5,000个训练样本，在CIRCO上实现了38.5的mAP@50，在Sketchy上实现了82.7的mAP@200，超过了许多重监督基线。

Conclusion: UNION在桥接视觉和语言方面具有鲁棒性和效率，适用于不同的查询类型。

Abstract: Image-Guided Retrieval with Optional Text (IGROT) is a general retrieval setting where a query consists of an anchor image, with or without accompanying text, aiming to retrieve semantically relevant target images. This formulation unifies two major tasks: Composed Image Retrieval (CIR) and Sketch-Based Image Retrieval (SBIR). In this work, we address IGROT under low-data supervision by introducing UNION, a lightweight and generalisable target representation that fuses the image embedding with a null-text prompt. Unlike traditional approaches that rely on fixed target features, UNION enhances semantic alignment with multimodal queries while requiring no architectural modifications to pretrained vision-language models. With only 5,000 training samples - from LlavaSCo for CIR and Training-Sketchy for SBIR - our method achieves competitive results across benchmarks, including CIRCO mAP@50 of 38.5 and Sketchy mAP@200 of 82.7, surpassing many heavily supervised baselines. This demonstrates the robustness and efficiency of UNION in bridging vision and language across diverse query types.

</details>


### [145] [Efficiency and Effectiveness of SPLADE Models on Billion-Scale Web Document Title](https://arxiv.org/abs/2511.22263)
*Taeryun Won,Tae Kwan Lee,Hiun Kim,Hyemin Lee*

Main category: cs.IR

TL;DR: 本文比较了BM25、SPLADE和Expanded-SPLADE模型在大型Web文档检索中的性能，发现SPLADE及其变体在复杂查询中表现更好，但计算成本更高。通过剪枝策略降低了计算成本，Expanded-SPLADE在效果和效率之间取得了最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估和优化稀疏检索模型在大型搜索引擎中的应用。

Method: 采用的方法是：1. 对比BM25、SPLADE和Expanded-SPLADE模型在不同规模数据集上的效果和效率。2. 引入文档中心剪枝和top-k查询词选择等剪枝策略来降低计算成本。

Result: 结果表明，SPLADE和Expanded-SPLADE模型在检索性能上优于BM25，尤其是在处理复杂查询时。Expanded-SPLADE在效果和效率之间取得了最佳平衡。

Conclusion: 结论是Expanded-SPLADE模型是在大型搜索引擎中部署稀疏检索模型的理想选择，并为如何在大型搜索系统中部署稀疏检索模型提供了宝贵的见解。

Abstract: This paper presents a comprehensive comparison of BM25, SPLADE, and Expanded-SPLADE models in the context of large-scale web document retrieval. We evaluate the effectiveness and efficiency of these models on datasets spanning from tens of millions to billions of web document titles. SPLADE and Expanded-SPLADE, which utilize sparse lexical representations, demonstrate superior retrieval performance compared to BM25, especially for complex queries. However, these models incur higher computational costs. We introduce pruning strategies, including document-centric pruning and top-k query term selection, boolean query with term threshold to mitigate these costs and improve the models' efficiency without significantly sacrificing retrieval performance. The results show that Expanded-SPLADE strikes the best balance between effectiveness and efficiency, particularly when handling large datasets. Our findings offer valuable insights for deploying sparse retrieval models in large-scale search engines.

</details>


### [146] [CoFiRec: Coarse-to-Fine Tokenization for Generative Recommendation](https://arxiv.org/abs/2511.22707)
*Tianxin Wei,Xuying Ning,Xuxing Chen,Ruizhong Qiu,Yupeng Hou,Yan Xie,Shuang Yang,Zhigang Hua,Jingrui He*

Main category: cs.IR

TL;DR: CoFiRec通过在token化过程中显式地加入项目语义的粗细粒度性质，从而改进了生成式推荐模型。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式推荐模型忽略了用户在浏览web内容时，偏好会从宽泛的类别逐渐细化到特定的项目的过程。

Method: CoFiRec将项目信息分解为多个语义级别（从高级类别到详细描述和协同过滤信号），并引入CoFiRec Tokenizer来独立地token化每一层，同时保留结构顺序。在自回归解码过程中，语言模型被指示从粗到细生成项目tokens，从而逐步地模拟用户意图。

Result: 在多个公共基准和backbone上的实验表明，CoFiRec优于现有的方法。

Conclusion: 结构化token化可以降低生成项目和ground truth项目之间的差异性，支持其在生成式推荐中的有效性。

Abstract: In web environments, user preferences are often refined progressively as users move from browsing broad categories to exploring specific items. However, existing generative recommenders overlook this natural refinement process. Generative recommendation formulates next-item prediction as autoregressive generation over tokenized user histories, where each item is represented as a sequence of discrete tokens. Prior models typically fuse heterogeneous attributes such as ID, category, title, and description into a single embedding before quantization, which flattens the inherent semantic hierarchy of items and fails to capture the gradual evolution of user intent during web interactions. To address this limitation, we propose CoFiRec, a novel generative recommendation framework that explicitly incorporates the Coarse-to-Fine nature of item semantics into the tokenization process. Instead of compressing all attributes into a single latent space, CoFiRec decomposes item information into multiple semantic levels, ranging from high-level categories to detailed descriptions and collaborative filtering signals. Based on this design, we introduce the CoFiRec Tokenizer, which tokenizes each level independently while preserving structural order. During autoregressive decoding, the language model is instructed to generate item tokens from coarse to fine, progressively modeling user intent from general interests to specific item-level interests. Experiments across multiple public benchmarks and backbones demonstrate that CoFiRec outperforms existing methods, offering a new perspective for generative recommendation. Theoretically, we prove that structured tokenization leads to lower dissimilarity between generated and ground truth items, supporting its effectiveness in generative recommendation. Our code is available at https://github.com/YennNing/CoFiRec.

</details>


### [147] [Two-Stage Distributionally Robust Optimization Framework for Secure Communications in Aerial-RIS Systems](https://arxiv.org/abs/2511.22855)
*Zhongming Feng,Qiling Gao,Zeping Sui,Yun Lin,Michail Matthaiou*

Main category: cs.IR

TL;DR: 提出了一种两阶段分布鲁棒优化（DRO）框架，用于在空中可重构智能表面（A-RIS）辅助的毫米波系统中实现安全部署和波束成形。


<details>
  <summary>Details</summary>
Motivation: 为了解决用户移动性、不完善的信道状态信息（CSI）和硬件损伤引起的多时间尺度不确定性。

Method: 该方法将长期无人机（UAV）放置与每时隙波束成形设计解耦。通过采用条件风险价值（CVaR）作为无分布风险度量，开发了一种低复杂度的算法，该算法结合了用于高效部署的替代模型和用于鲁棒实时波束成形的交替优化（AO）方案。

Result: 仿真结果表明，与基准方案相比，所提出的 DRO-CVaR 框架显着提高了尾端保密频谱效率，并保持了较低的中断概率，尤其是在严重的不确定性条件下。

Conclusion: 所提出的 DRO-CVaR 框架在应对不确定性方面表现出色，提高了保密性和频谱效率。

Abstract: This letter proposes a two-stage distributionally robust optimization (DRO) framework for secure deployment and beamforming in an aerial reconfigurable intelligent surface (A-RIS) assisted millimeter-wave system. To account for multi-timescale uncertainties arising from user mobility, imperfect channel state information (CSI), and hardware impairments, our approach decouples the long-term unmanned aerial vehicle (UAV) placement from the per-slot beamforming design. By employing the conditional value-at-risk (CVaR) as a distribution-free risk metric, a low-complexity algorithm is developed, which combines a surrogate model for efficient deployment with an alternating optimization (AO) scheme for robust real-time beamforming. Simulation results validate that the proposed DRO-CVaR framework significantly enhances the tail-end secrecy spectral efficiency and maintains a lower outage probability compared to benchmark schemes, especially under severe uncertainty conditions.

</details>


### [148] [FedAU2: Attribute Unlearning for User-Level Federated Recommender Systems with Adaptive and Robust Adversarial Training](https://arxiv.org/abs/2511.22872)
*Yuyuan Li,Junjie Fang,Fengyuan Yu,Xichun Sheng,Tianyu Du,Xuyang Teng,Shaowei Jiang,Linbo Jiang,Jianan Lin,Chaochao Chen*

Main category: cs.IR

TL;DR: 提出了一种用于用户级联邦推荐系统的属性遗忘方法 FedAU2，以解决用户数据异构性和梯度信息泄露带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 联邦推荐系统中的用户嵌入容易受到属性推断攻击，属性遗忘是一种有前景的缓解方法。现有方法在用户级联邦推荐系统中面临数据异构和梯度泄露的挑战。

Method: 提出了 FedAU2，它包含自适应对抗训练策略（解决数据异构性）和双重随机变分自编码器（防止梯度泄露）。

Result: 在三个真实数据集上的实验表明，FedAU2 在遗忘效果和推荐性能方面优于现有基线。

Conclusion: FedAU2 能够有效应对用户级联邦推荐系统中属性遗忘的挑战，并在实验中取得了优越的性能。

Abstract: Federated Recommender Systems (FedRecs) leverage federated learning to protect user privacy by retaining data locally. However, user embeddings in FedRecs often encode sensitive attribute information, rendering them vulnerable to attribute inference attacks. Attribute unlearning has emerged as a promising approach to mitigate this issue. In this paper, we focus on user-level FedRecs, which is a more practical yet challenging setting compared to group-level FedRecs. Adversarial training emerges as the most feasible approach within this context. We identify two key challenges in implementing adversarial training-based attribute unlearning for user-level FedRecs: i) mitigating training instability caused by user data heterogeneity, and ii) preventing attribute information leakage through gradients. To address these challenges, we propose FedAU2, an attribute unlearning method for user-level FedRecs. For CH1, we propose an adaptive adversarial training strategy, where the training dynamics are adjusted in response to local optimization behavior. For CH2, we propose a dual-stochastic variational autoencoder to perturb the adversarial model, effectively preventing gradient-based information leakage. Extensive experiments on three real-world datasets demonstrate that our proposed FedAU2 achieves superior performance in unlearning effectiveness and recommendation performance compared to existing baselines.

</details>


### [149] [Do LLM-judges Align with Human Relevance in Cranfield-style Recommender Evaluation?](https://arxiv.org/abs/2511.23312)
*Gustavo Penha,Aleksandr V. Petrov,Claudia Hauff,Enrico Palumbo,Ali Vardasbi,Edoardo D'Amico,Francesco Fabbri,Alice Wang,Praveen Chandar,Henrik Lindstrom,Hugues Bouchard,Mounia Lalmas*

Main category: cs.IR

TL;DR: 本文探讨了使用大型语言模型（LLM）作为自动评估指标，以解决推荐系统评估中的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的推荐系统离线评估方法由于存在偏差而结果不稳定，而人工构建评估数据集成本高昂，限制了可扩展性。

Method: 使用ML-32M-ext电影推荐数据集，研究LLM判断与人工标注的相关性，并通过增加项目元数据和用户历史来提高一致性。

Result: LLM判断与人工排序高度一致（Kendall's tau = 0.87），并在播客推荐的工业案例中验证了其价值。

Conclusion: LLM判断是一种可行的、可扩展的推荐系统评估方法。

Abstract: Evaluating recommender systems remains a long-standing challenge, as offline methods based on historical user interactions and train-test splits often yield unstable and inconsistent results due to exposure bias, popularity bias, sampled evaluations, and missing-not-at-random patterns. In contrast, textual document retrieval benefits from robust, standardized evaluation via Cranfield-style test collections, which combine pooled relevance judgments with controlled setups. While recent work shows that adapting this methodology to recommender systems is feasible, constructing such collections remains costly due to the need for manual relevance judgments, thus limiting scalability. This paper investigates whether Large Language Models (LLMs) can serve as reliable automatic judges to address these scalability challenges. Using the ML-32M-ext Cranfield-style movie recommendation collection, we first examine the limitations of existing evaluation methodologies. Then we explore the alignment and the recommender systems ranking agreement between the LLM-judge and human provided relevance labels. We find that incorporating richer item metadata and longer user histories improves alignment, and that LLM-judge yields high agreement with human-based rankings (Kendall's tau = 0.87). Finally, an industrial case study in the podcast recommendation domain demonstrates the practical value of LLM-judge for model selection. Overall, our results show that LLM-judge is a viable and scalable approach for evaluating recommender systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [150] [Artificial intelligence for methane detection: from continuous monitoring to verified mitigation](https://arxiv.org/abs/2511.21777)
*Anna Allen,Gonzalo Mateo-Garcia,Itziar Irakulis-Loitxate,Manuel Montesino-San Martin,Marc Watine,James Requeima,Javier Gorroño,Cynthia Randles,Tharwat Mokalled,Luis Guanter,Richard E. Turner,Claudio Cifarelli,Manfredi Caltagirone*

Main category: cs.LG

TL;DR: 这篇论文介绍了一种名为MARS-S2L的机器学习模型，用于检测多光谱卫星图像中的甲烷排放。


<details>
  <summary>Details</summary>
Motivation: 甲烷是一种强效温室气体，针对少数大型点源的减排可以带来显著效果。大规模检测和归因大型排放源仍然具有挑战性。

Method: 该模型基于超过80,000张图像的手动管理数据集进行训练，可在公开的多光谱卫星图像中检测甲烷排放。

Result: 该模型以8%的误报率识别了697个先前未见过的站点中78%的羽流，并已向20个国家/地区的利益相关者发布了1,015份通知。

Conclusion: MARS-S2L的实际部署已经促成了六个持续排放源的永久减排，证明了从卫星检测到可量化的甲烷减排的可扩展途径。

Abstract: Methane is a potent greenhouse gas, responsible for roughly 30\% of warming since pre-industrial times. A small number of large point sources account for a disproportionate share of emissions, creating an opportunity for substantial reductions by targeting relatively few sites. Detection and attribution of large emissions at scale for notification to asset owners remains challenging. Here, we introduce MARS-S2L, a machine learning model that detects methane emissions in publicly available multispectral satellite imagery. Trained on a manually curated dataset of over 80,000 images, the model provides high-resolution detections every two days, enabling facility-level attribution and identifying 78\% of plumes with an 8\% false positive rate at 697 previously unseen sites. Deployed operationally, MARS-S2L has issued 1,015 notifications to stakeholders in 20 countries, enabling verified, permanent mitigation of six persistent emitters, including a previously unknown site in Libya. These results demonstrate a scalable pathway from satellite detection to quantifiable methane mitigation.

</details>


### [151] [Physics-Informed Spiking Neural Networks via Conservative Flux Quantization](https://arxiv.org/abs/2511.21784)
*Chi Zhang,Lin Wang*

Main category: cs.LG

TL;DR: 提出了一种新的物理信息脉冲神经网络（PISNN），它结合了科学计算的严谨性和神经形态工程的效率，为智能系统实现复杂、长期和节能的物理预测铺平了道路。


<details>
  <summary>Details</summary>
Motivation: 在低功耗边缘设备上进行实时的、物理一致的预测对于下一代具身人工智能系统至关重要，但它仍然是一个主要的挑战。传统的物理信息神经网络（PINN）能耗高，难以严格执行物理守恒定律。而直接将PINN转换为脉冲神经网络（SNN）会降低物理保真度，并无法解决长期泛化问题。

Method: 设计了保守型漏泄积分放电（C-LIF）神经元，其动态结构保证了局部质量守恒。引入了一种新的保守通量量化（CFQ）策略，将神经脉冲重新定义为物理通量的离散数据包。CFQ学习了一个时间不变的物理演化算子，使PISNN成为一个通用的求解器。

Result: 在多个基准测试中表现出色。对于典型的一维热方程和更具挑战性的二维拉普拉斯方程，它能准确地模拟系统动力学，同时通过设计保持完美的质量守恒——这是传统PINN难以实现的。

Conclusion: 建立了一个强大的框架，将科学计算的严谨性与神经形态工程的效率融合在一起，为智能系统实现复杂、长期和节能的物理预测铺平了道路。

Abstract: Real-time, physically-consistent predictions on low-power edge devices is critical for the next generation embodied AI systems, yet it remains a major challenge. Physics-Informed Neural Networks (PINNs) combine data-driven learning with physics-based constraints to ensure the model's predictions are with underlying physical principles.However, PINNs are energy-intensive and struggle to strictly enforce physical conservation laws. Brain-inspired spiking neural networks (SNNs) have emerged as a promising solution for edge computing and real-time processing. However, naively converting PINNs to SNNs degrades physical fidelity and fails to address long-term generalization issues. To this end, this paper introduce a novel Physics-Informed Spiking Neural Network (PISNN) framework. Importantly, to ensure strict physical conservation, we design the Conservative Leaky Integrate-and-Fire (C-LIF) neuron, whose dynamics structurally guarantee local mass preservation. To achieve robust temporal generalization, we introduce a novel Conservative Flux Quantization (CFQ) strategy, which redefines neural spikes as discrete packets of physical flux. Our CFQ learns a time-invariant physical evolution operator, enabling the PISNN to become a general-purpose solver -- conservative-by-construction. Extensive experiments show that our PISNN excels on diverse benchmarks. For both the canonical 1D heat equation and the more challenging 2D Laplace's Equation, it accurately simulates the system dynamics while maintaining perfect mass conservation by design -- a feat that is challenging for conventional PINNs. This work establishes a robust framework for fusing the rigor of scientific computing with the efficiency of neuromorphic engineering, paving the way for complex, long-term, and energy-efficient physics predictions for intelligent systems.

</details>


### [152] [Dynamical Implicit Neural Representations](https://arxiv.org/abs/2511.21787)
*Yesom Park,Kelvin Kan,Thomas Flynn,Yi Huang,Shinjae Yoo,Stanley Osher,Xihaier Luo*

Main category: cs.LG

TL;DR: 提出了一个名为动态隐式神经表示（DINR）的新框架，以解决隐式神经表示（INR）中的频谱偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有的隐式神经表示方法在捕捉高频细节方面存在不足，即频谱偏差问题。

Method: 将特征演化视为连续时间动力系统，而不是离散的层堆叠。通过连续特征演化实现更丰富、更自适应的频率表示，从而缓解频谱偏差。

Result: 在图像表示、场重建和数据压缩等任务上的大量实验表明，DINR 比传统的静态 INR 具有更稳定的收敛性、更高的信号保真度和更强的泛化能力。

Conclusion: DINR 能够通过连续特征演化提供更丰富和自适应的频率表示，从而缓解频谱偏差，并且通过正则化潜在动态的复杂性，可以实现表达性和泛化之间的平衡。

Abstract: Implicit Neural Representations (INRs) provide a powerful continuous framework for modeling complex visual and geometric signals, but spectral bias remains a fundamental challenge, limiting their ability to capture high-frequency details. Orthogonal to existing remedy strategies, we introduce Dynamical Implicit Neural Representations (DINR), a new INR modeling framework that treats feature evolution as a continuous-time dynamical system rather than a discrete stack of layers. This dynamical formulation mitigates spectral bias by enabling richer, more adaptive frequency representations through continuous feature evolution. Theoretical analysis based on Rademacher complexity and the Neural Tangent Kernel demonstrates that DINR enhances expressivity and improves training dynamics. Moreover, regularizing the complexity of the underlying dynamics provides a principled way to balance expressivity and generalization. Extensive experiments on image representation, field reconstruction, and data compression confirm that DINR delivers more stable convergence, higher signal fidelity, and stronger generalization than conventional static INRs.

</details>


### [153] [Multiclass threshold-based classification and model evaluation](https://arxiv.org/abs/2511.21794)
*Edoardo Legnaro,Sabrina Guastavino,Francesco Marchetti*

Main category: cs.LG

TL;DR: 本文提出了一种基于阈值的多类分类框架，该框架推广了标准的argmax规则。


<details>
  <summary>Details</summary>
Motivation: 通过将softmax输出的概率解释替换为多维单纯形上的几何解释，其中分类取决于多维阈值。

Method: 通过阈值调整对任何训练的分类网络进行分类评分的后验优化。

Result: 多维阈值调整在各种网络和数据集中产生性能改进。此外，我们推导出基于ROC云的多类ROC分析，并通过到点距离（DFP）评分将其概括为（0,1）。

Conclusion: 这产生了一种与标准One-vs-Rest（OvR）曲线相一致的连贯替代方案，并与观察到的调整增益相一致。

Abstract: In this paper, we introduce a threshold-based framework for multiclass classification that generalizes the standard argmax rule. This is done by replacing the probabilistic interpretation of softmax outputs with a geometric one on the multidimensional simplex, where the classification depends on a multidimensional threshold. This change of perspective enables for any trained classification network an \textit{a posteriori} optimization of the classification score by means of threshold tuning, as usually carried out in the binary setting, thus allowing for a further refinement of the prediction capability of any network. Our experiments show indeed that multidimensional threshold tuning yields performance improvements across various networks and datasets. Moreover, we derive a multiclass ROC analysis based on \emph{ROC clouds} -- the attainable (FPR,TPR) operating points induced by a single multiclass threshold -- and summarize them via a \emph{Distance From Point} (DFP) score to $(0,1)$. This yields a coherent alternative to standard One-vs-Rest (OvR) curves and aligns with the observed tuning gains.

</details>


### [154] [The Double-Edged Nature of the Rashomon Set for Trustworthy Machine Learning](https://arxiv.org/abs/2511.21799)
*Ethan Hsu,Harry Chen,Chudi Zhong,Lesia Semenova*

Main category: cs.LG

TL;DR: 现实世界的机器学习管道很少产生单一模型，而是产生许多接近最优模型的Rashomon集。这种多样性重塑了可信赖性的关键方面。


<details>
  <summary>Details</summary>
Motivation: 研究表明，真实世界的机器学习管道通常会产生多个近优模型（Rashomon集），而这种模型的多样性对模型的可信赖性有重要影响。

Method: 通过对稀疏决策树和线性模型进行理论分析和实证研究。

Result: 稀疏的可解释模型倾向于保护隐私，但容易受到对抗性攻击；而Rashomon集中的多样性能够实现反应性鲁棒性，并且在小的分布变化下是稳定的；但这种多样性也会增加信息泄露。

Conclusion: Rashomon集对于可信赖的机器学习既是资源也是风险，论文强调了鲁棒性与隐私之间的权衡。

Abstract: Real-world machine learning (ML) pipelines rarely produce a single model; instead, they produce a Rashomon set of many near-optimal ones. We show that this multiplicity reshapes key aspects of trustworthiness. At the individual-model level, sparse interpretable models tend to preserve privacy but are fragile to adversarial attacks. In contrast, the diversity within a large Rashomon set enables reactive robustness: even when an attack breaks one model, others often remain accurate. Rashomon sets are also stable under small distribution shifts. However, this same diversity increases information leakage, as disclosing more near-optimal models provides an attacker with progressively richer views of the training data. Through theoretical analysis and empirical studies of sparse decision trees and linear models, we characterize this robustness-privacy trade-off and highlight the dual role of Rashomon sets as both a resource and a risk for trustworthy ML.

</details>


### [155] [Unsupervised Anomaly Detection for Smart IoT Devices: Performance and Resource Comparison](https://arxiv.org/abs/2511.21842)
*Md. Sad Abdullah Sami,Mushfiquzzaman Abid*

Main category: cs.LG

TL;DR: 本研究探讨了在物联网环境中，使用无监督异常检测技术（Isolation Forest 和 One-Class Support Vector Machine）来识别网络安全威胁的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统基于签名异常检测系统在识别新兴和零日威胁方面存在局限性，物联网设备面临日益增加的网络安全威胁。

Method: 使用 TON_IoT 温控器数据集，对 Isolation Forest (IF) 和 One-Class Support Vector Machine (OC-SVM) 两种无监督异常检测技术进行了评估，采用准确率、精确率、召回率和 F1-score 等标准指标，以及推理时间、模型大小和峰值 RAM 使用等资源利用率指标。

Result: 实验结果表明，Isolation Forest 在检测准确率、精确率、召回率和 F1-score 方面均优于 OC-SVM，且计算资源占用更少。

Conclusion: 研究结果强调了 Isolation Forest 在高维和不平衡的物联网环境中的鲁棒性，并突出了其在实时异常检测中的实际可行性，更适合部署在资源受限的物联网边缘设备上。

Abstract: The rapid expansion of Internet of Things (IoT) deployments across diverse sectors has significantly enhanced operational efficiency, yet concurrently elevated cybersecurity vulnerabilities due to increased exposure to cyber threats. Given the limitations of traditional signature-based Anomaly Detection Systems (ADS) in identifying emerging and zero-day threats, this study investigates the effectiveness of two unsupervised anomaly detection techniques, Isolation Forest (IF) and One-Class Support Vector Machine (OC-SVM), using the TON_IoT thermostat dataset. A comprehensive evaluation was performed based on standard metrics (accuracy, precision, recall, and F1-score) alongside critical resource utilization metrics such as inference time, model size, and peak RAM usage. Experimental results revealed that IF consistently outperformed OC-SVM, achieving higher detection accuracy, superior precision, and recall, along with a significantly better F1-score. Furthermore, Isolation Forest demonstrated a markedly superior computational footprint, making it more suitable for deployment on resource-constrained IoT edge devices. These findings underscore Isolation Forest's robustness in high-dimensional and imbalanced IoT environments and highlight its practical viability for real-time anomaly detection.

</details>


### [156] [Massively Parallel Imitation Learning of Mouse Forelimb Musculoskeletal Reaching Dynamics](https://arxiv.org/abs/2511.21848)
*Eric Leonardis,Akira Nagamori,Ayesha Thanawalla,Yuanjia Yang,Joshua Park,Hutton Saunders,Eiman Azim,Talmo Pereira*

Main category: cs.LG

TL;DR: 开发一个通用平台，用于模拟高保真行为动力学、生物力学和神经回路结构，这些是实体控制的基础。


<details>
  <summary>Details</summary>
Motivation: 为了理解大脑控制身体的机制，我们需要模拟具身控制的传感器运动转换。

Method: 使用模仿学习框架，在模拟物理环境中，用肌肉骨骼模型执行灵巧的前肢伸展任务。利用JAX和Mujoco-MJX进行GPU加速，鼠标手臂模型的训练速度超过每秒100万步。

Result: 结果表明，增加对能量和速度的自然约束，可以使模拟的肌肉骨骼活动更好地预测真实的EMG信号。

Conclusion: 能量和控制约束对于建模肌肉骨骼运动控制至关重要。

Abstract: The brain has evolved to effectively control the body, and in order to understand the relationship we need to model the sensorimotor transformations underlying embodied control. As part of a coordinated effort, we are developing a general-purpose platform for behavior-driven simulation modeling high fidelity behavioral dynamics, biomechanics, and neural circuit architectures underlying embodied control. We present a pipeline for taking kinematics data from the neuroscience lab and creating a pipeline for recapitulating those natural movements in a biomechanical model. We implement a imitation learning framework to perform a dexterous forelimb reaching task with a musculoskeletal model in a simulated physics environment. The mouse arm model is currently training at faster than 1 million training steps per second due to GPU acceleration with JAX and Mujoco-MJX. We present results that indicate that adding naturalistic constraints on energy and velocity lead to simulated musculoskeletal activity that better predict real EMG signals. This work provides evidence to suggest that energy and control constraints are critical to modeling musculoskeletal motor control.

</details>


### [157] [Lightweight ML-Based Air Quality Prediction for IoT and Embedded Applications](https://arxiv.org/abs/2511.21857)
*Md. Sad Abdullah Sami,Mushfiquzzaman Abid*

Main category: cs.LG

TL;DR: 本研究探讨了 XGBoost 回归模型的两种变体（完整版和轻量版）在预测一氧化碳 (CO) 和二氧化氮 (NO2) 浓度方面的有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索在资源受限的环境中部署简化模型的可行性，同时不影响预测质量。

Method: 使用在城市环境中收集的一年的 AirQualityUCI 数据集，使用广泛接受的指标（包括 MAE、RMSE、MBE 和 R2）进行了全面评估。此外，还评估了面向资源的指标，例如推理时间、模型大小和峰值 RAM 使用率。

Result: 完整的 XGBoost 模型在两种污染物的预测精度方面均优于轻量版模型，而轻量版模型虽然精度稍低，但在计算方面具有显着优势，可显着减少推理时间和模型存储需求。

Conclusion: 结果表明，在资源受限的环境中部署简化模型是可行的，而不会影响预测质量。这使得轻量级的 XGBoost 模型适用于物联网和嵌入式应用中的实时空气质量监测。

Abstract: This study investigates the effectiveness and efficiency of two variants of the XGBoost regression model, the full-capacity and lightweight (tiny) versions, for predicting the concentrations of carbon monoxide (CO) and nitrogen dioxide (NO2). Using the AirQualityUCI dataset collected over one year in an urban environment, we conducted a comprehensive evaluation based on widely accepted metrics, including Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Mean Bias Error (MBE), and the coefficient of determination (R2). In addition, we assessed resource-oriented metrics such as inference time, model size, and peak RAM usage. The full XGBoost model achieved superior predictive accuracy for both pollutants, while the tiny model, though slightly less precise, offered substantial computational benefits with significantly reduced inference time and model storage requirements. These results demonstrate the feasibility of deploying simplified models in resource-constrained environments without compromising predictive quality. This makes the tiny XGBoost model suitable for real-time air-quality monitoring in IoT and embedded applications.

</details>


### [158] [From Topology to Retrieval: Decoding Embedding Spaces with Unified Signatures](https://arxiv.org/abs/2511.22150)
*Florian Rottach,William Rudman,Bastain Rieck,Harrisen Scells,Carsten Eickhoff*

Main category: cs.LG

TL;DR: 这篇论文研究了文本嵌入在空间中的组织方式，以提高模型的可解释性，并揭示驱动下游任务性能的因素。


<details>
  <summary>Details</summary>
Motivation: 研究嵌入在空间中的组织方式可以提高模型的可解释性，并揭示驱动下游任务性能的因素。

Method: 通过对各种文本嵌入模型和数据集进行全面的拓扑和几何测量分析，提出了统一拓扑签名（UTS）框架，用于表征嵌入空间。

Result: 发现这些测量方法之间存在高度冗余，并且单个指标通常无法充分区分嵌入空间。UTS可以预测模型特定的属性，并揭示由模型架构驱动的相似性。拓扑结构与排序效果相关，并能准确预测文档的可检索性。

Conclusion: 整体的、多属性的视角对于理解和利用文本嵌入的几何特性至关重要。

Abstract: Studying how embeddings are organized in space not only enhances model interpretability but also uncovers factors that drive downstream task performance. In this paper, we present a comprehensive analysis of topological and geometric measures across a wide set of text embedding models and datasets. We find a high degree of redundancy among these measures and observe that individual metrics often fail to sufficiently differentiate embedding spaces. Building on these insights, we introduce Unified Topological Signatures (UTS), a holistic framework for characterizing embedding spaces. We show that UTS can predict model-specific properties and reveal similarities driven by model architecture. Further, we demonstrate the utility of our method by linking topological structure to ranking effectiveness and accurately predicting document retrievability. We find that a holistic, multi-attribute perspective is essential to understanding and leveraging the geometry of text embeddings.

</details>


### [159] [Towards a Foundation Model for Partial Differential Equations Across Physics Domains](https://arxiv.org/abs/2511.21861)
*Eduardo Soares,Emilio Vital Brazil,Victor Shirasuna,Breno W. S. R. de Carvalho,Cristiano Malossi*

Main category: cs.LG

TL;DR: PDE-FM: 一个用于物理信息机器学习的模块化基础模型，统一了异构偏微分方程 (PDE) 系统的空间、频谱和时间推理。


<details>
  <summary>Details</summary>
Motivation: 在各种 PDE 数据集上进行一次预训练，可以转移到新的物理机制，无需架构或数据特定的修改。

Method: 结合了空间-频谱标记化、物理感知调节和基于 Mamba 的状态空间主干与算子理论解码器。

Result: 在来自 The Well 基准的十二个 2D 和 3D 数据集上进行了评估，涵盖流体动力学、辐射、弹性和天体物理现象 - PDE-FM 在六个领域实现了最先进的精度，相对于先前的算子学习基线，平均 VRMSE 降低了 46%。

Conclusion: 大规模跨各种物理过程的预训练可以产生可转移的动力学表示，标志着朝着用于多物理场模拟和科学发现的统一的基础级别替代迈出了一步。

Abstract: We present PDE-FM, a modular foundation model for physics-informed machine learning that unifies spatial, spectral, and temporal reasoning across heterogeneous partial differential equation (PDE) systems. PDE-FM combines spatial-spectral tokenization, physics-aware conditioning, and a Mamba-based state-space backbone with an operator-theoretic decoder, enabling scalable and data-efficient modeling of complex physical dynamics. In contrast to task-specific neural operators, PDE-FM is pretrained once on diverse PDE datasets and can be transferred to new physical regimes without architectural or data-specific modifications. Evaluated on twelve 2D and 3D datasets from The Well benchmark - spanning hydrodynamic, radiative, elastic, and astrophysical phenomena - PDE-FM achieves state-of-the-art accuracy in six domains, reducing mean VRMSE by 46% relative to prior operator-learning baselines. The model demonstrates robust cross-physics generalization, excelling in turbulent and radiative systems while maintaining strong performance in linear and steady-state regimes. These results suggest that large-scale pretraining across diverse physical processes can yield transferable representations of dynamics, marking a step toward unified, foundation-level surrogates for multi-physics simulation and scientific discovery.

</details>


### [160] [Closed-Loop Transformers: Autoregressive Modeling as Iterative Latent Equilibrium](https://arxiv.org/abs/2511.21882)
*Akbar Anbar Jafari,Gholamreza Anbarjafari*

Main category: cs.LG

TL;DR: 本文提出了一种名为平衡转换器（EqT）的新型自回归转换器，它通过迭代细化潜在表示来解决现有模型的开放循环问题，从而提高长程推理、事实一致性和多步规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归转换器以开放循环方式运行，导致错误在序列中传播且无法纠正，从而限制了模型在长程推理、事实一致性和多步规划方面的能力。

Method: 本文引入闭环预测原则，并通过梯度下降在潜在空间中最小化学习的能量函数来实现。该能量函数强制双向预测一致性、情景记忆连贯性和输出置信度。

Result: 在二元奇偶校验任务上的初步实验表明，EqT 在具有挑战性的序列上平均提高了 +3.28%，在标准转换器接近随机性能的情况下，增益达到了 +8.07%。

Conclusion: 闭环平衡可以解决开放循环自回归的承诺瓶颈，代表了迈向语言模型的基础性一步。

Abstract: Contemporary autoregressive transformers operate in open loop: each hidden state is computed in a single forward pass and never revised, causing errors to propagate uncorrected through the sequence. We identify this open-loop bottleneck as a fundamental architectural limitation underlying well-documented failures in long-range reasoning, factual consistency, and multi-step planning. To address this limitation, we introduce the closed-loop prediction principle, which requires that models iteratively refine latent representations until reaching a self-consistent equilibrium before committing to each token. We instantiate this principle as Equilibrium Transformers (EqT), which augment standard transformer layers with an Equilibrium Refinement Module that minimizes a learned energy function via gradient descent in latent space. The energy function enforces bidirectional prediction consistency, episodic memory coherence, and output confidence, all computed without external supervision. Theoretically, we prove that EqT performs approximate MAP inference in a latent energy-based model, establish linear convergence guarantees, and show that refinement improves predictions precisely on hard instances where one-shot inference is suboptimal. The framework unifies deep equilibrium models, diffusion language models, and test-time training as special cases. Preliminary experiments on the binary parity task demonstrate +3.28% average improvement on challenging sequences, with gains reaching +8.07% where standard transformers approach random performance, validating that the benefit of deliberation scales with task difficulty. Just as attention mechanisms resolved the sequential bottleneck of recurrent networks, we propose that closed-loop equilibrium may resolve the commitment bottleneck of open-loop autoregression, representing a foundational step toward language models.

</details>


### [161] [An Efficient Embedding Based Ad Retrieval with GPU-Powered Feature Interaction](https://arxiv.org/abs/2511.22460)
*Yifan Lei,Jiahua Luo,Tingyu Jiang,Bo Zhang,Lifeng Wang,Dapeng Liu,Zhaoren Wu,Haijie Gu,Huan Yu,Jie Jiang*

Main category: cs.LG

TL;DR: 提出了一种新的基于GPU加速的特征交互双塔网络，用于大规模广告检索。


<details>
  <summary>Details</summary>
Motivation: 双塔模型特征交互不足，DNN模型计算成本高。

Method: 设计了一种新的压缩倒排列表，用于GPU加速，实现高效的特征交互计算。

Result: 离线评估和在线实验均优于现有方法，并在腾讯广告推荐系统中成功部署。

Conclusion: 该方法有效，为优化大规模广告检索系统提供了新的实践指导。

Abstract: In large-scale advertising recommendation systems, retrieval serves as a critical component, aiming to efficiently select a subset of candidate ads relevant to user behaviors from a massive ad inventory for subsequent ranking and recommendation. The Embedding-Based Retrieval (EBR) methods modeled by the dual-tower network are widely used in the industry to maintain both retrieval efficiency and accuracy. However, the dual-tower model has significant limitations: the embeddings of users and ads interact only at the final inner product computation, resulting in insufficient feature interaction capabilities. Although DNN-based models with both user and ad as input features, allowing for early-stage interaction between these features, are introduced in the ranking stage to mitigate this issue, they are computationally infeasible for the retrieval stage. To bridge this gap, this paper proposes an efficient GPU-based feature interaction for the dual-tower network to significantly improve retrieval accuracy while substantially reducing computational costs. Specifically, we introduce a novel compressed inverted list designed for GPU acceleration, enabling efficient feature interaction computation at scale. To the best of our knowledge, this is the first framework in the industry to successfully implement Wide and Deep in a retrieval system. We apply this model to the real-world business scenarios in Tencent Advertising, and experimental results demonstrate that our method outperforms existing approaches in offline evaluation and has been successfully deployed to Tencent's advertising recommendation system, delivering significant online performance gains. This improvement not only validates the effectiveness of the proposed method, but also provides new practical guidance for optimizing large-scale ad retrieval systems.

</details>


### [162] [Physically Interpretable Representation Learning with Gaussian Mixture Variational AutoEncoder (GM-VAE)](https://arxiv.org/abs/2511.21883)
*Tiffany Fan,Murray Cutforth,Marta D'Elia,Alexandre Cortiella,Alireza Doostan,Eric Darve*

Main category: cs.LG

TL;DR: 提出了一种高斯混合变分自编码器（GM-VAE）框架，用于从高维科学数据中提取紧凑且物理上可解释的表示。


<details>
  <summary>Details</summary>
Motivation: 由于物理系统中固有的复杂非线性结构，从高维科学数据中提取紧凑且物理上可解释的表示是一个持续的挑战。

Method: 该方法结合了期望最大化（EM）训练方案和新的频谱可解释性指标，使用块坐标下降策略，在期望和最大化步骤之间交替。

Result: 在表面反应 ODE、Navier-Stokes 尾流和实验激光诱导燃烧 Schlieren 图像数据集上验证了框架的有效性，结果表明 GM-VAE 产生了平滑、物理上一致的流形和准确的区域聚类。

Conclusion: GM-VAE 为解释湍流和反应流系统提供了一个强大的数据驱动工具。

Abstract: Extracting compact, physically interpretable representations from high-dimensional scientific data is a persistent challenge due to the complex, nonlinear structures inherent in physical systems. We propose a Gaussian Mixture Variational Autoencoder (GM-VAE) framework designed to address this by integrating an Expectation-Maximization (EM)-inspired training scheme with a novel spectral interpretability metric. Unlike conventional VAEs that jointly optimize reconstruction and clustering (often leading to training instability), our method utilizes a block-coordinate descent strategy, alternating between expectation and maximization steps. This approach stabilizes training and naturally aligns latent clusters with distinct physical regimes. To objectively evaluate the learned representations, we introduce a quantitative metric based on graph-Laplacian smoothness, which measures the coherence of physical quantities across the latent manifold. We demonstrate the efficacy of this framework on datasets of increasing complexity: surface reaction ODEs, Navier-Stokes wake flows, and experimental laser-induced combustion Schlieren images. The results show that our GM-VAE yields smooth, physically consistent manifolds and accurate regime clustering, offering a robust data-driven tool for interpreting turbulent and reactive flow systems.

</details>


### [163] [Exploring Fusion Strategies for Multimodal Vision-Language Systems](https://arxiv.org/abs/2511.21889)
*Regan Willis,Jason Bakos*

Main category: cs.LG

TL;DR: 研究了多模态机器学习中数据融合策略对准确率和延迟的影响，发现早期融合速度快但准确率较低，晚期融合准确率高但速度较慢。


<details>
  <summary>Details</summary>
Motivation: 为了更准确地捕获信息，现代机器学习模型通常结合多个数据输入流。在多模态机器学习中，选择融合数据的策略需要仔细考虑应用程序的准确性和延迟要求，因为在模型架构的早期或后期融合数据可能导致准确性和延迟的性能变化。

Method: 使用集成了图像和文本数据的混合BERT和视觉网络框架，探索了不同的融合策略。使用了两种不同的视觉网络：MobileNetV2和ViT。针对每个视觉网络，提出了三种模型，这些模型在架构的后期、中间和早期阶段融合数据。

Result: 实验结果表明，虽然晚期融合产生最高的准确率，但早期融合提供最低的推理延迟。

Conclusion: 模型架构中较早的数据融合以牺牲准确性为代价，实现了更快的推理时间。

Abstract: Modern machine learning models often combine multiple input streams of data to more accurately capture the information that informs their decisions. In multimodal machine learning, choosing the strategy for fusing data together requires careful consideration of the application's accuracy and latency requirements, as fusing the data at earlier or later stages in the model architecture can lead to performance changes in accuracy and latency. To demonstrate this tradeoff, we investigate different fusion strategies using a hybrid BERT and vision network framework that integrates image and text data. We explore two different vision networks: MobileNetV2 and ViT. We propose three models for each vision network, which fuse data at late, intermediate, and early stages in the architecture. We evaluate the proposed models on the CMU MOSI dataset and benchmark their latency on an NVIDIA Jetson Orin AGX. Our experimental results demonstrate that while late fusion yields the highest accuracy, early fusion offers the lowest inference latency. We describe the three proposed model architectures and discuss the accuracy and latency tradeoffs, concluding that data fusion earlier in the model architecture results in faster inference times at the cost of accuracy.

</details>


### [164] [Breaking the Illusion: Consensus-Based Generative Mitigation of Adversarial Illusions in Multi-Modal Embeddings](https://arxiv.org/abs/2511.21893)
*Fatemeh Akbarian,Anahita Baninajjar,Yingyi Zhang,Ananth Balashankar,Amir Aminifar*

Main category: cs.LG

TL;DR: 提出了一种防御对抗性错觉的机制，通过生成模型重建输入，以保持自然对齐。


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型容易受到对抗性错觉的影响，微小的扰动会破坏跨模态对齐并误导下游任务。

Method: 提出了一种任务无关的缓解机制，通过生成模型（例如 VAE）从受扰动的输入中重建输入，并结合生成采样策略和基于共识的聚合方案。

Result: 实验表明，该方法将错觉攻击成功率降低到接近于零，并在未扰动和扰动输入设置中分别提高了 4% 和 11% 的跨模态对齐。

Conclusion: 该方法提供了一种有效且模型无关的对抗对抗性错觉的防御手段。

Abstract: Multi-modal foundation models align images, text, and other modalities in a shared embedding space but remain vulnerable to adversarial illusions (Zhang et al., 2025), where imperceptible perturbations disrupt cross-modal alignment and mislead downstream tasks. To counteract the effects of adversarial illusions, we propose a task-agnostic mitigation mechanism that reconstructs the input from the attacker's perturbed input through generative models, e.g., Variational Autoencoders (VAEs), to maintain natural alignment. To further enhance our proposed defense mechanism, we adopt a generative sampling strategy combined with a consensus-based aggregation scheme over the outcomes of the generated samples. Our experiments on the state-of-the-art multi-modal encoders show that our approach substantially reduces the illusion attack success rates to near-zero and improves cross-modal alignment by 4% (42 to 46) and 11% (32 to 43) in unperturbed and perturbed input settings respectively, providing an effective and model-agnostic defense against adversarial illusions.

</details>


### [165] [Beyond Atoms: Evaluating Electron Density Representation for 3D Molecular Learning](https://arxiv.org/abs/2511.21900)
*Patricia Suriana,Joshua A. Rackers,Ewa M. Nowara,Pedro O. Pinheiro,John M. Nicoloudis,Vishnu Sresht*

Main category: cs.LG

TL;DR: 该论文比较了三种基于体素的输入类型（原子类型、原始电子密度和密度梯度幅度）用于3D卷积神经网络(CNN)，以预测分子性质。


<details>
  <summary>Details</summary>
Motivation: 传统的基于原子的3D分子性质预测模型可能忽略了微妙的物理信息。电子密度图提供了一种连续的、物理基础的替代方案。

Method: 使用3D CNN，比较了三种体素输入类型在蛋白质-配体结合亲和力预测(PDBbind)和量子性质预测(QM9)两个分子任务上的表现。

Result: 在PDBbind上，所有表示在完整数据下表现相似，但在低数据情况下，基于密度的输入优于原子类型。在QM9上，基于密度的输入在大规模下仍然优于基于原子的输入。

Conclusion: 研究结果强调了密度导出输入的任务和状态依赖性优势，提高了亲和力预测中的数据效率和量子性质建模的准确性。

Abstract: Machine learning models for 3D molecular property prediction typically rely on atom-based representations, which may overlook subtle physical information. Electron density maps -- the direct output of X-ray crystallography and cryo-electron microscopy -- offer a continuous, physically grounded alternative. We compare three voxel-based input types for 3D convolutional neural networks (CNNs): atom types, raw electron density, and density gradient magnitude, across two molecular tasks -- protein-ligand binding affinity prediction (PDBbind) and quantum property prediction (QM9). We focus on voxel-based CNNs because electron density is inherently volumetric, and voxel grids provide the most natural representation for both experimental and computed densities. On PDBbind, all representations perform similarly with full data, but in low-data regimes, density-based inputs outperform atom types, while a shape-based baseline performs comparably -- suggesting that spatial occupancy dominates this task. On QM9, where labels are derived from Density Functional Theory (DFT) but input densities from a lower-level method (XTB), density-based inputs still outperform atom-based ones at scale, reflecting the rich structural and electronic information encoded in density. Overall, these results highlight the task- and regime-dependent strengths of density-derived inputs, improving data efficiency in affinity prediction and accuracy in quantum property modeling.

</details>


### [166] [Masked Diffusion for Generative Recommendation](https://arxiv.org/abs/2511.23021)
*Kulin Shah,Bhuvesh Kumar,Neil Shah,Liam Collins*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的生成式推荐方法，使用掩码扩散模型来预测用户交互历史中的语义ID序列，以解决传统自回归模型的推理成本高、训练数据利用率低和偏向学习短上下文关系等问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义ID的生成式推荐模型采用自回归建模，存在推理成本高、训练数据利用率低和偏向学习短上下文关系等问题。

Method: 提出使用掩码扩散模型来建模和学习用户语义ID序列的概率。掩码扩散采用离散掩码噪声来学习序列分布，并将掩码token的概率建模为在未掩码token条件下条件独立，从而允许并行解码掩码token。

Result: 实验结果表明，该方法在性能上优于自回归建模，尤其是在数据受限的情况下和粗粒度召回方面表现更明显。此外，该方法在推理过程中可以灵活地并行预测多个语义ID，同时保持优于自回归建模的性能。

Conclusion: 该研究提出了一种基于掩码扩散模型的生成式推荐方法，可以有效解决传统自回归模型的局限性，并在性能和效率上取得了显著提升。

Abstract: Generative recommendation (GR) with semantic IDs (SIDs) has emerged as a promising alternative to traditional recommendation approaches due to its performance gains, capitalization on semantic information provided through language model embeddings, and inference and storage efficiency. Existing GR with SIDs works frame the probability of a sequence of SIDs corresponding to a user's interaction history using autoregressive modeling. While this has led to impressive next item prediction performances in certain settings, these autoregressive GR with SIDs models suffer from expensive inference due to sequential token-wise decoding, potentially inefficient use of training data and bias towards learning short-context relationships among tokens. Inspired by recent breakthroughs in NLP, we propose to instead model and learn the probability of a user's sequence of SIDs using masked diffusion. Masked diffusion employs discrete masking noise to facilitate learning the sequence distribution, and models the probability of masked tokens as conditionally independent given the unmasked tokens, allowing for parallel decoding of the masked tokens. We demonstrate through thorough experiments that our proposed method consistently outperforms autoregressive modeling. This performance gap is especially pronounced in data-constrained settings and in terms of coarse-grained recall, consistent with our intuitions. Moreover, our approach allows the flexibility of predicting multiple SIDs in parallel during inference while maintaining superior performance to autoregressive modeling.

</details>


### [167] [Multi-Modal Machine Learning for Early Trust Prediction in Human-AI Interaction Using Face Image and GSR Bio Signals](https://arxiv.org/abs/2511.21908)
*Hamid Shamszare,Avishek Choudhury*

Main category: cs.LG

TL;DR: 该研究提出了一种多模态机器学习框架，结合图像和皮肤电反应 (GSR) 数据，以预测用户对人工智能或人类生成的推荐的早期信任度，尤其是在医疗保健领域。


<details>
  <summary>Details</summary>
Motivation: 预测人类对人工智能系统的信任对于安全集成基于人工智能的决策支持工具至关重要，尤其是在医疗保健领域。

Method: 该研究使用 OpenCV 处理面部视频数据以进行帧提取，并通过预训练的 Transformer 模型进行迁移学习以获得情绪特征。同时，GSR 信号被分解为 tonic 和 phasic 成分以捕获生理唤醒模式。定义了两个时间窗口进行信任预测：早期检测窗口（决策前 6 到 3 秒）和近端检测窗口（决策前 3 到 0 秒）。

Result: 实验结果表明，结合面部和生理线索可显著提高预测性能。多模态堆叠框架在早期检测窗口中实现了 0.83 的准确率、0.88 的 F1 分数和 0.87 的 ROC-AUC，在近端检测窗口中实现了 0.75 的准确率、0.82 的 F1 分数和 0.66 的 ROC-AUC。

Conclusion: 这些结果表明，生物信号作为用户信任的实时客观指标具有潜力，从而能够实现自适应人工智能系统，动态调整其响应以维持校准的信任，这对于精神健康应用至关重要，在精神健康应用中，未校准的信任会影响诊断和治疗结果。

Abstract: Predicting human trust in AI systems is crucial for safe integration of AI-based decision support tools, especially in healthcare. This study proposes a multi-modal machine learning framework that combines image and galvanic skin response (GSR) data to predict early user trust in AI- or human-generated recommendations in a simulated ADHD mHealth context. Facial video data were processed using OpenCV for frame extraction and transferred learning with a pre-trained transformer model to derive emotional features. Concurrently, GSR signals were decomposed into tonic and phasic components to capture physiological arousal patterns. Two temporal windows were defined for trust prediction: the Early Detection Window (6 to 3 seconds before decision-making) and the Proximal Detection Window (3 to 0 seconds before decision-making). For each window, trust prediction was conducted separately using image-based, GSR-based, and multimodal (image + GSR) features. Each modality was analyzed using machine learning algorithms, and the top-performing unimodal models were integrated through a multimodal stacking ensemble for final prediction. Experimental results showed that combining facial and physiological cues significantly improved prediction performance. The multimodal stacking framework achieved an accuracy of 0.83, F1-score of 0.88, and ROC-AUC of 0.87 in the Early Detection Window, and an accuracy of 0.75, F1-score of 0.82, and ROC-AUC of 0.66 in the Proximal Detection Window. These results demonstrate the potential of bio signals as real-time, objective markers of user trust, enabling adaptive AI systems that dynamically adjust their responses to maintain calibrated trust which is a critical capability in mental health applications where mis-calibrated trust can affect diagnostic and treatment outcomes.

</details>


### [168] [Exploring Dynamic Properties of Backdoor Training Through Information Bottleneck](https://arxiv.org/abs/2511.21923)
*Xinyu Liu,Xu Zhang,Can Chen,Ren Wang*

Main category: cs.LG

TL;DR: 该论文分析了后门数据如何影响神经网络的训练动态。


<details>
  <summary>Details</summary>
Motivation: 理解后门数据如何影响神经网络训练仍然是一个复杂且未被充分探索的挑战。本文旨在严谨分析后门数据对学习过程的影响，特别关注目标类和其他干净类之间的不同行为。

Method: 利用与内部表征聚类相关的信息瓶颈（IB）原理。

Result: 后门攻击会产生独特的互信息（MI）特征，这些特征在训练阶段不断演变，并且根据攻击机制而有所不同。视觉上明显的攻击（如 BadNets）可以从信息论的角度实现高隐蔽性，比许多视觉上难以察觉的攻击更无缝地集成到模型中。提出了一种新的、基于动态的隐蔽性指标，用于量化攻击在模型层面的集成度。

Conclusion: 验证了研究结果和所提出的指标，为理解和评估后门威胁提供了一个新的维度。

Abstract: Understanding how backdoor data influences neural network training dynamics remains a complex and underexplored challenge. In this paper, we present a rigorous analysis of the impact of backdoor data on the learning process, with a particular focus on the distinct behaviors between the target class and other clean classes. Leveraging the Information Bottleneck (IB) principle connected with clustering of internal representation, We find that backdoor attacks create unique mutual information (MI) signatures, which evolve across training phases and differ based on the attack mechanism. Our analysis uncovers a surprising trade-off: visually conspicuous attacks like BadNets can achieve high stealthiness from an information-theoretic perspective, integrating more seamlessly into the model than many visually imperceptible attacks. Building on these insights, we propose a novel, dynamics-based stealthiness metric that quantifies an attack's integration at the model level. We validate our findings and the proposed metric across multiple datasets and diverse attack types, offering a new dimension for understanding and evaluating backdoor threats. Our code is available in: https://github.com/XinyuLiu71/Information_Bottleneck_Backdoor.git.

</details>


### [169] [Prompted Policy Search: Reinforcement Learning through Linguistic and Numerical Reasoning in LLMs](https://arxiv.org/abs/2511.21928)
*Yifan Zhou,Sachin Grover,Mohamed El Mistiri,Kamalesh Kalirathnam,Pratyush Kerhalkar,Swaroop Mishra,Neelesh Kumar,Sanket Gaurav,Oya Aran,Heni Ben Amor*

Main category: cs.LG

TL;DR: ProPS: 使用大型语言模型统一数值和语言推理，以改进强化学习。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖标量奖励信号，限制了其利用现实任务中丰富的语义知识的能力。人类通过结合数值反馈与语言、先验知识和常识来高效学习。

Method: 提出 Prompted Policy Search (ProPS)，这是一种新型 RL 方法，它将大型语言模型置于策略优化循环的中心，直接根据奖励反馈和自然语言输入提出策略更新。

Result: 在 15 个 Gymnasium 任务中评估，涵盖经典控制、Atari 游戏和 MuJoCo 环境，与七种广泛采用的 RL 算法（例如，PPO、SAC、TRPO）进行比较。在 15 项任务中的 8 项上优于所有基线，并且在提供领域知识时表现出显着提升。

Conclusion: 结果突出了统一语义和数值以实现透明、通用和与人类对齐的 RL 的潜力。

Abstract: Reinforcement Learning (RL) traditionally relies on scalar reward signals, limiting its ability to leverage the rich semantic knowledge often available in real-world tasks. In contrast, humans learn efficiently by combining numerical feedback with language, prior knowledge, and common sense. We introduce Prompted Policy Search (ProPS), a novel RL method that unifies numerical and linguistic reasoning within a single framework. Unlike prior work that augment existing RL components with language, ProPS places a large language model (LLM) at the center of the policy optimization loop-directly proposing policy updates based on both reward feedback and natural language input. We show that LLMs can perform numerical optimization in-context, and that incorporating semantic signals, such as goals, domain knowledge, and strategy hints can lead to more informed exploration and sample-efficient learning. ProPS is evaluated across fifteen Gymnasium tasks, spanning classic control, Atari games, and MuJoCo environments, and compared to seven widely-adopted RL algorithms (e.g., PPO, SAC, TRPO). It outperforms all baselines on eight out of fifteen tasks and demonstrates substantial gains when provided with domain knowledge. These results highlight the potential of unifying semantics and numerics for transparent, generalizable, and human-aligned RL.

</details>


### [170] [Does the Model Say What the Data Says? A Simple Heuristic for Model Data Alignment](https://arxiv.org/abs/2511.21931)
*Henry Salgado,Meagan Kendall,Martine Ceberio*

Main category: cs.LG

TL;DR: 提出一个简单且计算高效的框架，用于评估机器学习模型是否与其学习的数据结构对齐


<details>
  <summary>Details</summary>
Motivation: 现有的可解释性方法只关注解释模型行为，而忽略了数据本身

Method: 量化每个特征在二元分类任务中分离两个结果组的程度，并将其与基于模型的解释进行比较

Result: 提供了一种可解释且模型无关的方法来评估模型-数据对齐

Conclusion: 评估模型是否与其学习的数据结构对齐

Abstract: In this work, we propose a simple and computationally efficient framework to evaluate whether machine learning models align with the structure of the data they learn from; that is, whether \textit{the model says what the data says}. Unlike existing interpretability methods that focus exclusively on explaining model behavior, our approach establishes a baseline derived directly from the data itself. Drawing inspiration from Rubin's Potential Outcomes Framework, we quantify how strongly each feature separates the two outcome groups in a binary classification task, moving beyond traditional descriptive statistics to estimate each feature's effect on the outcome. By comparing these data-derived feature rankings against model-based explanations, we provide practitioners with an interpretable and model-agnostic method to assess model--data alignment.

</details>


### [171] [Modeling Quantum Autoencoder Trainable Kernel for IoT Anomaly Detection](https://arxiv.org/abs/2511.21932)
*Swathi Chandrasekhar,Shiva Raj Pokhrel,Swati Kumari,Navneet Singh*

Main category: cs.LG

TL;DR: 量子自动编码器（QAE）框架用于网络入侵检测，在实际量子硬件上表现出量子优势。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方法无法跟上网络威胁和物联网流量的复杂性，深度学习计算瓶颈限制了实时部署。

Method: 提出一种量子自动编码器（QAE）框架，将网络流量压缩成潜在表示，并采用量子支持向量分类（QSVC）进行入侵检测。

Result: 在三个数据集上的评估表明，该方法在理想模拟器和IBM量子硬件上都实现了更高的准确率。适度的退极化噪声起到隐式正则化的作用，稳定了训练，提高了泛化能力。

Conclusion: 量子机器学习是解决现实网络安全挑战的可行且随时可用的硬件解决方案。

Abstract: Escalating cyber threats and the high-dimensional complexity of IoT traffic have outpaced classical anomaly detection methods. While deep learning offers improvements, computational bottlenecks limit real-time deployment at scale. We present a quantum autoencoder (QAE) framework that compresses network traffic into discriminative latent representations and employs quantum support vector classification (QSVC) for intrusion detection. Evaluated on three datasets, our approach achieves improved accuracy on ideal simulators and on the IBM Quantum hardware demonstrating practical quantum advantage on current NISQ devices. Crucially, moderate depolarizing noise acts as implicit regularization, stabilizing training and enhancing generalization. This work establishes quantum machine learning as a viable, hardware-ready solution for real-world cybersecurity challenges.

</details>


### [172] [Heterogeneous Multi-Agent Reinforcement Learning with Attention for Cooperative and Scalable Feature Transformation](https://arxiv.org/abs/2511.21934)
*Tao Zhe,Huazhen Fang,Kunpeng Liu,Qian Lou,Tamzidul Hoque,Dongjie Wang*

Main category: cs.LG

TL;DR: 提出了一种新的异构多智能体强化学习框架，以实现协同和可扩展的特征转换。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习有所进步，但特征转换对于结构化数据仍然至关重要，在结构化数据中，深度模型通常难以捕获复杂的特征交互。

Method: 该框架包括三种异构智能体，分为两种类型，每种智能体都旨在选择重要的特征和操作以进行特征交叉。为了加强这些智能体之间的通信，我们实施了一种共享的评论家机制，该机制有助于在特征转换期间进行信息交换。为了处理动态扩展的特征空间，我们定制了基于多头注意力的特征智能体，以选择合适的特征进行特征交叉。此外，我们在优化过程中引入了一种状态编码技术，以稳定和增强RL智能体的学习动态，从而产生更强大和可靠的转换策略。

Result: 我们进行了广泛的实验，以验证我们模型的有效性、效率、鲁棒性和可解释性。

Conclusion: 解决了特征转换过程中动态特征扩展导致的不稳定性和RL agents学习复杂性增加的问题，以及智能体之间合作和沟通不足导致特征交叉操作不理想和模型性能下降的问题。

Abstract: Feature transformation enhances downstream task performance by generating informative features through mathematical feature crossing. Despite the advancements in deep learning, feature transformation remains essential for structured data, where deep models often struggle to capture complex feature interactions. Prior literature on automated feature transformation has achieved success but often relies on heuristics or exhaustive searches, leading to inefficient and time-consuming processes. Recent works employ reinforcement learning (RL) to enhance traditional approaches through a more effective trial-and-error way. However, two limitations remain: 1) Dynamic feature expansion during the transformation process, which causes instability and increases the learning complexity for RL agents; 2) Insufficient cooperation and communication between agents, which results in suboptimal feature crossing operations and degraded model performance. To address them, we propose a novel heterogeneous multi-agent RL framework to enable cooperative and scalable feature transformation. The framework comprises three heterogeneous agents, grouped into two types, each designed to select essential features and operations for feature crossing. To enhance communication among these agents, we implement a shared critic mechanism that facilitates information exchange during feature transformation. To handle the dynamically expanding feature space, we tailor multi-head attention-based feature agents to select suitable features for feature crossing. Additionally, we introduce a state encoding technique during the optimization process to stabilize and enhance the learning dynamics of the RL agents, resulting in more robust and reliable transformation policies. Finally, we conduct extensive experiments to validate the effectiveness, efficiency, robustness, and interpretability of our model.

</details>


### [173] [Breaking Algorithmic Collusion in Human-AI Ecosystems](https://arxiv.org/abs/2511.21935)
*Natalie Collina,Eshwar Ram Arunachaleswaran,Meena Jagadeesan*

Main category: cs.LG

TL;DR: 研究AI在重复定价博弈中与人类互动的影响，发现人类的背叛行为会破坏算法勾结。


<details>
  <summary>Details</summary>
Motivation: 探索AI如何维持超竞争价格，以及人类不采用AI而手动定价的影响。

Method: 构建程式化的重复定价博弈模型，其中AI代理采取均衡策略，人类则采取无后悔策略。

Result: 即使只有一个人背叛，也会破坏勾结并降低价格；更多人背叛会将价格推向竞争水平。勾结的性质也会随着具有背叛意识的AI代理而改变。

Conclusion: 结果表明了在AI代理和人类混合的生态系统中，算法勾结何时脆弱，以及何时持续存在。

Abstract: AI agents are increasingly deployed in ecosystems where they repeatedly interact not only with each other but also with humans. In this work, we study these human-AI ecosystems from a theoretical perspective, focusing on the classical framework of repeated pricing games. In our stylized model, the AI agents play equilibrium strategies, and one or more humans manually perform the pricing task instead of adopting an AI agent, thereby defecting to a no-regret strategy. Motivated by how populations of AI agents can sustain supracompetitive prices, we investigate whether high prices persist under such defections. Our main finding is that even a single human defection can destabilize collusion and drive down prices, and multiple defections push prices even closer to competitive levels. We further show how the nature of collusion changes under defection-aware AI agents. Taken together, our results characterize when algorithmic collusion is fragile--and when it persists--in mixed ecosystems of AI agents and humans.

</details>


### [174] [Deep Learning Architectures for Code-Modulated Visual Evoked Potentials Detection](https://arxiv.org/abs/2511.21940)
*Kiran Nair,Hubert Cecotti*

Main category: cs.LG

TL;DR: 本研究探索了基于深度学习的非侵入式脑机接口 (BCI) 方法，用于解码由视觉诱发电位 (C-VEP) 产生的脑电信号，以提高其鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的基于 C-VEP 的 BCI 系统在处理脑电信号中的时间和会话差异性噪声时存在局限性。本研究旨在通过深度学习架构克服这些问题，提高解码的可靠性。

Method: 研究采用了卷积神经网络 (CNN) 和 Siamese 网络等深度学习架构，并与典型的相关分析 (CCA) 基线方法进行比较。使用了来自 13 名健康成年人在单目标闪烁刺激下记录的脑电数据。

Result: 提出的深度学习模型显著优于传统方法。基于 Earth Mover's Distance (EMD) 的距离解码在延迟变化方面表现出更强的鲁棒性。多类 Siamese 网络取得了最佳的整体性能，平均准确率达到 96.89%。

Conclusion: 研究结果表明，数据驱动的深度学习架构在自适应非侵入式 BCI 系统中具有可靠的单次试验 C-VEP 解码的潜力。

Abstract: Non-invasive Brain-Computer Interfaces (BCIs) based on Code-Modulated Visual Evoked Potentials (C-VEPs) require highly robust decoding methods to address temporal variability and session-dependent noise in EEG signals. This study proposes and evaluates several deep learning architectures, including convolutional neural networks (CNNs) for 63-bit m-sequence reconstruction and classification, and Siamese networks for similarity-based decoding, alongside canonical correlation analysis (CCA) baselines. EEG data were recorded from 13 healthy adults under single-target flicker stimulation. The proposed deep models significantly outperformed traditional approaches, with distance-based decoding using Earth Mover's Distance (EMD) and constrained EMD showing greater robustness to latency variations than Euclidean and Mahalanobis metrics. Temporal data augmentation with small shifts further improved generalization across sessions. Among all models, the multi-class Siamese network achieved the best overall performance with an average accuracy of 96.89%, demonstrating the potential of data-driven deep architectures for reliable, single-trial C-VEP decoding in adaptive non-invasive BCI systems.

</details>


### [175] [ABLE: Using Adversarial Pairs to Construct Local Models for Explaining Model Predictions](https://arxiv.org/abs/2511.21952)
*Krishna Khadka,Sunny Shree,Pujan Budhathoki,Yu Lei,Raghu Kacker,D. Richard Kuhn*

Main category: cs.LG

TL;DR: 提出了一种新的局部解释方法，称为对抗括号局部解释 (ABLE)，以解决现有方法的不稳定性和局部保真度差的问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型越来越多地用于关键应用，但由于缺乏透明度，它们大多是“黑匣子”。局部解释方法（如 LIME）通过使用简单的、可解释的模型来近似测试实例附近的复杂模型的行为来解决这个问题。然而，这些方法通常存在不稳定和局部保真度差的问题。

Method: 该方法首先通过添加有界高斯噪声来生成测试实例 x_test 附近的一组邻域点。对于每个邻域点 D，我们应用对抗攻击来生成一个对抗点 A，该对抗点 A 具有最小的扰动，导致与 D 不同的标签。然后在 A 上执行第二次对抗攻击以生成一个点 A'，该点 A' 具有与 D 相同的标签（因此与 A 不同）。点 A 和 A' 形成一个对抗对，该对抗对包含 x_test 的局部决策边界。然后，我们在线性模型上训练这些对抗对，以近似局部决策边界。

Result: 在三个深度神经网络架构上的六个 UCI 基准数据集上的实验结果表明，我们的方法比最先进的方法实现了更高的稳定性和保真度。

Conclusion: ABLE 方法在稳定性和保真度方面优于现有技术。

Abstract: Machine learning models are increasingly used in critical applications but are mostly "black boxes" due to their lack of transparency. Local explanation approaches, such as LIME, address this issue by approximating the behavior of complex models near a test instance using simple, interpretable models. However, these approaches often suffer from instability and poor local fidelity. In this paper, we propose a novel approach called Adversarially Bracketed Local Explanation (ABLE) to address these limitations. Our approach first generates a set of neighborhood points near the test instance, x_test, by adding bounded Gaussian noise. For each neighborhood point D, we apply an adversarial attack to generate an adversarial point A with minimal perturbation that results in a different label than D. A second adversarial attack is then performed on A to generate a point A' that has the same label as D (and thus different than A). The points A and A' form an adversarial pair that brackets the local decision boundary for x_test. We then train a linear model on these adversarial pairs to approximate the local decision boundary. Experimental results on six UCI benchmark datasets across three deep neural network architectures demonstrate that our approach achieves higher stability and fidelity than the state-of-the-art.

</details>


### [176] [CTR Prediction on Alibaba's Taobao Advertising Dataset Using Traditional and Deep Learning Models](https://arxiv.org/abs/2511.21963)
*Hongyu Yang,Chunxi Wen,Jiyin Zhang,Nanfei Shen,Shijiao Zhang,Xiyan Han*

Main category: cs.LG

TL;DR: 本文旨在使用阿里巴巴发布的大规模淘宝数据集，更有效地对点击率 (CTR) 进行建模。


<details>
  <summary>Details</summary>
Motivation: 在现代广告系统中，排名相关性和用户参与度直接影响平台效率和商业价值。为了更好地模拟用户意图，我们结合了来自 22 天内数亿次互动的行为数据。

Method: 我们从监督学习模型开始，包括逻辑回归和 Light-GBM，这些模型在静态特征（例如用户人口统计、广告属性和上下文元数据）上进行训练。为了更好地模拟用户意图，我们结合了来自 22 天内数亿次互动的行为数据。通过提取和编码用户行为序列，我们构建了用户随时间变化的兴趣表示。我们使用深度学习模型将行为嵌入与静态特征融合。为了捕获时间动态，我们设计了一种基于 Transformer 的架构，该架构使用自注意力机制来学习行为序列中的上下文依赖关系，不仅对用户交互的内容进行建模，还对交互的时间和频率进行建模。

Result: Transformer 使 AUC 比基线（LR 模型）提高了 2.81%，对于兴趣多样或随时间变化的用户，观察到的增益最大。

Conclusion: 我们的研究为推进点击率预测并将其价值扩展到电子商务之外提供了路线图。个性化广告定位技术可以应用于公共卫生场景，以实现健康信息或行为指导的精确传递。

Abstract: Click-through rates prediction is critical in modern advertising systems, where ranking relevance and user engagement directly impact platform efficiency and business value. In this project, we explore how to model CTR more effectively using a large-scale Taobao dataset released by Alibaba. We start with supervised learning models, including logistic regression and Light-GBM, that are trained on static features such as user demographics, ad attributes, and contextual metadata. These models provide fast, interpretable benchmarks, but have limited capabilities to capture patterns of behavior that drive clicks. To better model user intent, we combined behavioral data from hundreds of millions of interactions over a 22-day period. By extracting and encoding user action sequences, we construct representations of user interests over time. We use deep learning models to fuse behavioral embeddings with static features. Among them, multilayer perceptrons (MLPs) have achieved significant performance improvements. To capture temporal dynamics, we designed a Transformer-based architecture that uses a self-attention mechanism to learn contextual dependencies across behavioral sequences, modeling not only what the user interacts with, but also the timing and frequency of interactions. Transformer improves AUC by 2.81 % over the baseline (LR model), with the largest gains observed for users whose interests are diverse or change over time. In addition to modeling, we propose an A/B testing strategy for real-world evaluation. We also think about the broader implications: personalized ad targeting technology can be applied to public health scenarios to achieve precise delivery of health information or behavior guidance. Our research provides a roadmap for advancing click-through rate predictions and extending their value beyond e-commerce.

</details>


### [177] [MOTIF-RF: Multi-template On-chip Transformer Synthesis Incorporating Frequency-domain Self-transfer Learning for RFIC Design Automation](https://arxiv.org/abs/2511.21970)
*Houbo He,Yizhou Xu,Lei Xia,Yaolong Hu,Fan Cai,Taiyun Chi*

Main category: cs.LG

TL;DR: 本论文研究了开发多模板机器学习 (ML) 替代模型并将其应用于射频集成电路 (RFIC) 中变压器 (XFMR) 的逆向设计。


<details>
  <summary>Details</summary>
Motivation: 旨在为射频集成电路 (RFIC) 实现 AI 辅助的规格到 GDS 自动化，并为 RFIC 设计人员提供可操作的工具，以将 AI 集成到他们的工作流程中。

Method: 1. 对四种广泛使用的 ML 架构（包括基于 MLP、CNN、UNet 和 GT 的模型）进行基准测试。2. 提出了一种新的频域自迁移学习技术，该技术利用相邻频带之间的相关性。3. 开发了一种基于协方差矩阵自适应进化策略 (CMA-ES) 算法的逆向设计框架。

Result: 频域自迁移学习技术使 S 参数预测的准确度提高了约 30%-50%。通过多个阻抗匹配任务验证了逆向设计框架，所有任务都表现出快速收敛和可信的性能。

Conclusion: 研究结果推进了射频集成电路的 AI 辅助规格到 GDS 自动化的目标，并为射频集成电路设计人员提供了将 AI 集成到其工作流程中的实用工具。

Abstract: This paper presents a systematic study on developing multi-template machine learning (ML) surrogate models and applying them to the inverse design of transformers (XFMRs) in radio-frequency integrated circuits (RFICs). Our study starts with benchmarking four widely used ML architectures, including MLP-, CNN-, UNet-, and GT-based models, using the same datasets across different XFMR topologies. To improve modeling accuracy beyond these baselines, we then propose a new frequency-domain self-transfer learning technique that exploits correlations between adjacent frequency bands, leading to around 30%-50% accuracy improvement in the S-parameters prediction. Building on these models, we further develop an inverse design framework based on the covariance matrix adaptation evolutionary strategy (CMA-ES) algorithm. This framework is validated using multiple impedance-matching tasks, all demonstrating fast convergence and trustworthy performance. These results advance the goal of AI-assisted specs-to-GDS automation for RFICs and provide RFIC designers with actionable tools for integrating AI into their workflows.

</details>


### [178] [A Safety and Security Framework for Real-World Agentic Systems](https://arxiv.org/abs/2511.21990)
*Shaona Ghosh,Barnaby Simkin,Kyriacos Shiarlis,Soumili Nandi,Dan Zhao,Matthew Fiedler,Julia Bazinska,Nikki Pope,Roopa Prabhu,Daniel Rohrer,Michael Demoret,Bartley Richardson*

Main category: cs.LG

TL;DR: 提出了一个动态且可操作的框架，用于保护企业部署中的代理AI系统。


<details>
  <summary>Details</summary>
Motivation: 安全和保障不仅仅是单个模型的固定属性，而且是模型、协调器、工具和数据之间动态交互产生的涌现属性。

Method: 使用辅助AI模型和代理，在人工监督下，协助进行上下文风险发现、评估和缓解，从而实现上下文代理风险管理。

Result: 通过NVIDIA旗舰代理研究助手AI-Q研究助手的案例研究，展示了框架的有效性，展示了在复杂的企业级代理工作流程中的实际端到端安全评估。风险发现阶段发现了新的代理风险，然后进行了情境缓解。我们还发布了案例研究中的数据集，其中包含超过10,000次代理工作流程的真实攻击和防御执行的痕迹，以帮助推进代理安全研究。

Conclusion: 定义了一个可操作的代理风险分类法，它将传统的安全和保障问题与新的、独特的代理风险统一起来，包括工具滥用、级联行动链和意外的控制放大等。

Abstract: This paper introduces a dynamic and actionable framework for securing agentic AI systems in enterprise deployment. We contend that safety and security are not merely fixed attributes of individual models but also emergent properties arising from the dynamic interactions among models, orchestrators, tools, and data within their operating environments. We propose a new way of identification of novel agentic risks through the lens of user safety. Although, for traditional LLMs and agentic models in isolation, safety and security has a clear separation, through the lens of safety in agentic systems, they appear to be connected. Building on this foundation, we define an operational agentic risk taxonomy that unifies traditional safety and security concerns with novel, uniquely agentic risks, including tool misuse, cascading action chains, and unintended control amplification among others. At the core of our approach is a dynamic agentic safety and security framework that operationalizes contextual agentic risk management by using auxiliary AI models and agents, with human oversight, to assist in contextual risk discovery, evaluation, and mitigation. We further address one of the most challenging aspects of safety and security of agentic systems: risk discovery through sandboxed, AI-driven red teaming. We demonstrate the framework effectiveness through a detailed case study of NVIDIA flagship agentic research assistant, AI-Q Research Assistant, showcasing practical, end-to-end safety and security evaluations in complex, enterprise-grade agentic workflows. This risk discovery phase finds novel agentic risks that are then contextually mitigated. We also release the dataset from our case study, containing traces of over 10,000 realistic attack and defense executions of the agentic workflow to help advance research in agentic safety.

</details>


### [179] [Distance-based Learning of Hypertrees](https://arxiv.org/abs/2511.22014)
*Shaun Fallat,Kamyar Khodamoradi,David Kirkpatrick,Valerii Maliuk,S. Ahmad Mojallal,Sandra Zilles*

Main category: cs.LG

TL;DR: 本文研究了使用最短路径查询 (SP 查询) 学习超图的问题，并提出了第一个可证明的最优在线算法，该算法适用于我们称之为有序超树的一类广泛而自然的超树。


<details>
  <summary>Details</summary>
Motivation: 在某些情况下，例如进化树重建，距离测量会随着距离的增加而退化，因此本文考虑了一种使用有界距离查询的学习模型。

Method: 使用最短路径查询学习超图，并设计在线算法和离线算法。同时考虑有界距离查询的学习模型。

Result: 对于有序超树，本文提出的在线算法可以转化为可证明的最优离线算法。对于一般超树，本文证明了渐近紧的复杂度界限。

Conclusion: 有序超树可以位于非循环超图的 Fagin 层次结构中，并且严格包含该层次结构中最广泛的、可以用亚二次 SP 查询复杂度学习的类。

Abstract: We study the problem of learning hypergraphs with shortest-path queries (SP-queries), and present the first provably optimal online algorithm for a broad and natural class of hypertrees that we call orderly hypertrees. Our online algorithm can be transformed into a provably optimal offline algorithm. Orderly hypertrees can be positioned within the Fagin hierarchy of acyclic hypergraph (well-studied in database theory), and strictly encompass the broadest class in this hierarchy that is learnable with subquadratic SP-query complexity.
  Recognizing that in some contexts, such as evolutionary tree reconstruction, distance measurements can degrade with increased distance, we also consider a learning model that uses bounded distance queries. In this model, we demonstrate asymptotically tight complexity bounds for learning general hypertrees.

</details>


### [180] [Equilibrium Propagation Without Limits](https://arxiv.org/abs/2511.22024)
*Elon Litman*

Main category: cs.LG

TL;DR: 本文通过建立有限 nudge 基础来解放 Equilibrium Propagation (EP) 的无穷小扰动限制，实现了局部信用分配。


<details>
  <summary>Details</summary>
Motivation: 建模网络状态为 Gibbs-Boltzmann 分布而非确定性点，证明了 nudge 相和自由相之间 Helmholtz 自由能差异的梯度正是预期局部能量导数的差异。

Method: 推导了基于损失-能量协方差路径积分的广义 EP 算法。

Result: 验证了经典的 Contrastive Hebbian Learning 更新是任意有限 nudging 的精确梯度估计器，无需无穷小近似或凸性。

Conclusion: 可以使用标准无穷小近似无法支持的强误差信号进行学习。

Abstract: We liberate Equilibrium Propagation (EP) from the limit of infinitesimal perturbations by establishing a finite-nudge foundation for local credit assignment. By modeling network states as Gibbs-Boltzmann distributions rather than deterministic points, we prove that the gradient of the difference in Helmholtz free energy between a nudged and free phase is exactly the difference in expected local energy derivatives. This validates the classic Contrastive Hebbian Learning update as an exact gradient estimator for arbitrary finite nudging, requiring neither infinitesimal approximations nor convexity. Furthermore, we derive a generalized EP algorithm based on the path integral of loss-energy covariances, enabling learning with strong error signals that standard infinitesimal approximations cannot support.

</details>


### [181] [Calibration-Free EEG-based Driver Drowsiness Detection with Online Test-Time Adaptation](https://arxiv.org/abs/2511.22030)
*Geun-Deok Jang,Dong-Kyun Han,Seo-Hyeon Park,Seong-Whan Lee*

Main category: cs.LG

TL;DR: 该论文提出了一种新的驾驶员疲劳检测框架，该框架利用在线测试时适应 (TTA) 方法动态调整到目标对象分布。


<details>
  <summary>Details</summary>
Motivation: 脑电图 (EEG) 信号的内在可变性需要繁琐的校准过程, 主体间差异导致难以将疲劳检测模型推广到看不见的目标主体。

Method: 该方法更新批处理标准化 (BN) 层中的可学习参数，同时保留预训练的标准化统计信息。引入原型学习以确保针对随时间推移的分布偏移进行鲁棒预测。

Result: 在模拟环境中收集的持续注意力驾驶数据集上验证了该方法，实验结果表明，该方法优于所有基线，平均 F1 得分为 81.73%，比最佳 TTA 基线提高了 11.73%。

Conclusion: 该方法显着增强了基于脑电图的疲劳检测系统在非独立同分布场景中的适应性。

Abstract: Drowsy driving is a growing cause of traffic accidents, prompting recent exploration of electroencephalography (EEG)-based drowsiness detection systems. However, the inherent variability of EEG signals due to psychological and physical factors necessitates a cumbersome calibration process. In particular, the inter-subject variability of EEG signals leads to a domain shift problem, which makes it challenging to generalize drowsiness detection models to unseen target subjects. To address these issues, we propose a novel driver drowsiness detection framework that leverages online test-time adaptation (TTA) methods to dynamically adjust to target subject distributions. Our proposed method updates the learnable parameters in batch normalization (BN) layers, while preserving pretrained normalization statistics, resulting in a modified configuration that ensures effective adaptation during test time. We incorporate a memory bank that dynamically manages streaming EEG segments, selecting samples based on their reliability determined by negative energy scores and persistence time. In addition, we introduce prototype learning to ensure robust predictions against distribution shifts over time. We validated our method on the sustained-attention driving dataset collected in a simulated environment, where drowsiness was estimated from delayed reaction times during monotonous lane-keeping tasks. Our experiments show that our method outperforms all baselines, achieving an average F1-score of 81.73\%, an improvement of 11.73\% over the best TTA baseline. This demonstrates that our proposed method significantly enhances the adaptability of EEG-based drowsiness detection systems in non-i.i.d. scenarios.

</details>


### [182] [Predicting Public Health Impacts of Electricity Usage](https://arxiv.org/abs/2511.22031)
*Yejia Liu,Zhifeng Wu,Pengfei Li,Shaolei Ren*

Main category: cs.LG

TL;DR: 提出了HealthPredictor，一个将电力使用与公共健康结果联系起来的领域特定AI模型，以实现健康驱动的需求侧管理。


<details>
  <summary>Details</summary>
Motivation: 电力部门是空气污染物排放的主要来源，影响公众健康。尽管法规减少了空气污染物，但化石燃料仍然是能源供应的重要组成部分，因此需要更先进的需求侧方法来减少对公众健康的影响。

Method: 该模型包含三个组成部分：燃料组合预测器、空气质量转换器和健康影响评估器。通过健康驱动的优化框架，在公共健康影响方面的预测误差远低于燃料组合驱动的基线。

Result: 在美国多个地区，健康驱动的优化框架在公共健康影响方面的预测误差显著低于燃料组合驱动的基线。电动汽车充电计划的案例研究表明了该方法实现的公共健康收益，并为健康知情的能源管理提供了可操作的指导。

Conclusion: 这项工作展示了如何显式设计AI模型，以实现健康知情的能源管理，从而促进公共健康和更广泛的社会福祉。

Abstract: The electric power sector is a leading source of air pollutant emissions, impacting the public health of nearly every community. Although regulatory measures have reduced air pollutants, fossil fuels remain a significant component of the energy supply, highlighting the need for more advanced demand-side approaches to reduce the public health impacts. To enable health-informed demand-side management, we introduce HealthPredictor, a domain-specific AI model that provides an end-to-end pipeline linking electricity use to public health outcomes. The model comprises three components: a fuel mix predictor that estimates the contribution of different generation sources, an air quality converter that models pollutant emissions and atmospheric dispersion, and a health impact assessor that translates resulting pollutant changes into monetized health damages. Across multiple regions in the United States, our health-driven optimization framework yields substantially lower prediction errors in terms of public health impacts than fuel mix-driven baselines. A case study on electric vehicle charging schedules illustrates the public health gains enabled by our method and the actionable guidance it can offer for health-informed energy management. Overall, this work shows how AI models can be explicitly designed to enable health-informed energy management for advancing public health and broader societal well-being. Our datasets and code are released at: https://github.com/Ren-Research/Health-Impact-Predictor.

</details>


### [183] [Convergence Dynamics of Over-Parameterized Score Matching for a Single Gaussian](https://arxiv.org/abs/2511.22069)
*Yiran Zhang,Weihang Xu,Mo Zhou,Maryam Fazel,Simon Shaolei Du*

Main category: cs.LG

TL;DR: 本文研究了使用梯度下降训练过度参数化模型以学习单个高斯分布的问题，重点关注了不同噪声水平下的优化动态和收敛性条件。


<details>
  <summary>Details</summary>
Motivation: 对 score matching 的优化行为的理论理解仍然有限，特别是在过度参数化的情况下。

Method: 使用具有 n 个可学习参数的学生模型，并在从单个 ground-truth Gaussian 生成的数据上使用 population score matching 目标对其进行训练，分析了多个 regime 下的优化动态。

Result: 在高噪声情况下，证明了梯度下降的全局收敛性。在低噪声情况下，确定了 stationary point 的存在，并表明在特定初始化条件下（参数初始化为指数小）可以实现收敛。此外，证明了在没有指数小初始化的情况下，参数可能不会收敛到 ground truth。对于从远离 ground truth 的高斯分布中随机初始化的参数，证明了只有一个参数收敛而其他参数发散，但损失仍然以 1/τ 的速率收敛到零。

Conclusion: 首次为具有至少三个组件的高斯混合模型在 score matching 框架下建立了全局收敛保证。

Abstract: Score matching has become a central training objective in modern generative modeling, particularly in diffusion models, where it is used to learn high-dimensional data distributions through the estimation of score functions. Despite its empirical success, the theoretical understanding of the optimization behavior of score matching, particularly in over-parameterized regimes, remains limited. In this work, we study gradient descent for training over-parameterized models to learn a single Gaussian distribution. Specifically, we use a student model with $n$ learnable parameters and train it on data generated from a single ground-truth Gaussian using the population score matching objective. We analyze the optimization dynamics under multiple regimes. When the noise scale is sufficiently large, we prove a global convergence result for gradient descent. In the low-noise regime, we identify the existence of a stationary point, highlighting the difficulty of proving global convergence in this case. Nevertheless, we show convergence under certain initialization conditions: when the parameters are initialized to be exponentially small, gradient descent ensures convergence of all parameters to the ground truth. We further prove that without the exponentially small initialization, the parameters may not converge to the ground truth. Finally, we consider the case where parameters are randomly initialized from a Gaussian distribution far from the ground truth. We prove that, with high probability, only one parameter converges while the others diverge, yet the loss still converges to zero with a $1/τ$ rate, where $τ$ is the number of iterations. We also establish a nearly matching lower bound on the convergence rate in this regime. This is the first work to establish global convergence guarantees for Gaussian mixtures with at least three components under the score matching framework.

</details>


### [184] [A Multi-View Multi-Timescale Hypergraph-Empowered Spatiotemporal Framework for EV Charging Forecasting](https://arxiv.org/abs/2511.22072)
*Jinhao Li,Hao Wang*

Main category: cs.LG

TL;DR: HyperCast is a novel EV charging demand forecasting framework using hypergraphs to model higher-order spatiotemporal dependencies in EV charging patterns.


<details>
  <summary>Details</summary>
Motivation: Existing forecasting methods are limited to pairwise relationships between stations, failing to capture complex, group-wise dynamics in urban charging networks.

Method: Develops HyperCast, integrating multi-view hypergraphs (static geographical proximity and dynamic demand-based functional similarities) and multi-timescale inputs, employing hyper-spatiotemporal blocks and cross-attention mechanisms.

Result: HyperCast significantly outperforms state-of-the-art baselines on four public datasets.

Conclusion: Explicitly modeling collective charging behaviors is effective for more accurate forecasting.

Abstract: Accurate electric vehicle (EV) charging demand forecasting is essential for stable grid operation and proactive EV participation in electricity market. Existing forecasting methods, particularly those based on graph neural networks, are often limited to modeling pairwise relationships between stations, failing to capture the complex, group-wise dynamics inherent in urban charging networks. To address this gap, we develop a novel forecasting framework namely HyperCast, leveraging the expressive power of hypergraphs to model the higher-order spatiotemporal dependencies hidden in EV charging patterns. HyperCast integrates multi-view hypergraphs, which capture both static geographical proximity and dynamic demand-based functional similarities, along with multi-timescale inputs to differentiate between recent trends and weekly periodicities. The framework employs specialized hyper-spatiotemporal blocks and tailored cross-attention mechanisms to effectively fuse information from these diverse sources: views and timescales. Extensive experiments on four public datasets demonstrate that HyperCast significantly outperforms a wide array of state-of-the-art baselines, demonstrating the effectiveness of explicitly modeling collective charging behaviors for more accurate forecasting.

</details>


### [185] [ARES: Anomaly Recognition Model For Edge Streams](https://arxiv.org/abs/2511.22078)
*Simone Mungari,Albert Bifet,Giuseppe Manco,Bernhard Pfahringer*

Main category: cs.LG

TL;DR: 提出了一种用于边缘流的无监督异常检测框架ARES，该框架结合了图神经网络（GNN）和半空间树（HST）以实现实时异常检测。


<details>
  <summary>Details</summary>
Motivation: 在时间图中检测边缘异常对于减轻潜在风险至关重要，但由于概念漂移、大数据量和需要实时响应而具有挑战性。

Method: ARES框架使用GNN提取特征，并使用HST进行异常评分。此外，还结合了一种简单的监督阈值机制，利用异常分数之间的统计离散度来确定最佳阈值。

Result: 通过在多个真实网络攻击场景中进行的大量评估验证了ARES的有效性，并将其性能与现有方法进行了比较，同时分析了其空间和时间复杂度。

Conclusion: ARES框架能够在边缘流中有效地检测异常，并且具有良好的适应性。

Abstract: Many real-world scenarios involving streaming information can be represented as temporal graphs, where data flows through dynamic changes in edges over time. Anomaly detection in this context has the objective of identifying unusual temporal connections within the graph structure. Detecting edge anomalies in real time is crucial for mitigating potential risks. Unlike traditional anomaly detection, this task is particularly challenging due to concept drifts, large data volumes, and the need for real-time response. To face these challenges, we introduce ARES, an unsupervised anomaly detection framework for edge streams. ARES combines Graph Neural Networks (GNNs) for feature extraction with Half-Space Trees (HST) for anomaly scoring. GNNs capture both spike and burst anomalous behaviors within streams by embedding node and edge properties in a latent space, while HST partitions this space to isolate anomalies efficiently. ARES operates in an unsupervised way without the need for prior data labeling. To further validate its detection capabilities, we additionally incorporate a simple yet effective supervised thresholding mechanism. This approach leverages statistical dispersion among anomaly scores to determine the optimal threshold using a minimal set of labeled data, ensuring adaptability across different domains. We validate ARES through extensive evaluations across several real-world cyber-attack scenarios, comparing its performance against existing methods while analyzing its space and time complexity.

</details>


### [186] [A Fast and Flat Federated Learning Method via Weighted Momentum and Sharpness-Aware Minimization](https://arxiv.org/abs/2511.22080)
*Tianle Li,Yongzhi Huang,Linshan Jiang,Chang Liu,Qipeng Xie,Wenfeng Du,Lu Wang,Kaishun Wu*

Main category: cs.LG

TL;DR: 提出了一种名为FedWMSAM的新型联邦学习方法，以解决非独立同分布数据下的收敛速度和泛化问题。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，需要在有限的通信预算下快速收敛模型，同时在非独立同分布的客户端数据上进行泛化。现有的动量算法和锐度感知最小化（SAM）相结合的方法存在局部-全局曲率不对齐和动量回声震荡的问题。

Method: 1.  构建一个由服务器聚合动量产生的动量引导全局扰动，使客户端的SAM方向与全局下降几何对齐，实现单次反向传播SAM近似，保持效率。
2.  通过余弦相似度自适应规则耦合动量和SAM，产生一个早期动量、晚期SAM的两阶段训练计划。

Result: 在多个数据集和模型架构上进行了大量实验，结果验证了该方法的有效性、适应性和鲁棒性，证明了其在解决联邦学习优化挑战方面的优越性。

Conclusion: FedWMSAM方法有效地解决了非独立同分布联邦学习中的收敛速度和泛化问题，并在实验中表现出优越性。

Abstract: In federated learning (FL), models must \emph{converge quickly} under tight communication budgets while \emph{generalizing} across non-IID client distributions. These twin requirements have naturally led to two widely used techniques: client/server \emph{momentum} to accelerate progress, and \emph{sharpness-aware minimization} (SAM) to prefer flat solutions. However, simply combining momentum and SAM leaves two structural issues unresolved in non-IID FL. We identify and formalize two failure modes: \emph{local-global curvature misalignment} (local SAM directions need not reflect the global loss geometry) and \emph{momentum-echo oscillation} (late-stage instability caused by accumulated momentum). To our knowledge, these failure modes have not been jointly articulated and addressed in the FL literature. We propose \textbf{FedWMSAM} to address both failure modes. First, we construct a momentum-guided global perturbation from server-aggregated momentum to align clients' SAM directions with the global descent geometry, enabling a \emph{single-backprop} SAM approximation that preserves efficiency. Second, we couple momentum and SAM via a cosine-similarity adaptive rule, yielding an early-momentum, late-SAM two-phase training schedule. We provide a non-IID convergence bound that \emph{explicitly models the perturbation-induced variance} $σ_ρ^2=σ^2+(Lρ)^2$ and its dependence on $(S, K, R, N)$ on the theory side. We conduct extensive experiments on multiple datasets and model architectures, and the results validate the effectiveness, adaptability, and robustness of our method, demonstrating its superiority in addressing the optimization challenges of Federated Learning. Our code is available at https://github.com/Huang-Yongzhi/NeurlPS_FedWMSAM.

</details>


### [187] [Quantum Bayesian Optimization for Quality Improvement in Fuselage Assembly](https://arxiv.org/abs/2511.22090)
*Jiayu Liu,Chong Liu,Trevor Rhone,Yinan Wang*

Main category: cs.LG

TL;DR: 提出了一种量子贝叶斯优化（QBO）框架，用于在装配过程中进行精确的形状控制，以提高制造实践中的样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在改进机身装配过程中的形状调整技术以最小化尺寸间隙方面显示出希望，但面临来自制造系统的低样本效率问题。

Method: 利用基于有限元分析（FEA）模型或替代模型的量子预言机，以更少的查询次数获得对环境响应的更准确估计。QBO 采用上限置信界（UCB）作为采集函数，以策略性地选择最有可能最大化目标函数的输入值。

Result: 实验结果表明，与经典方法相比，QBO 实现了显着更低的尺寸误差和不确定性，尤其是在使用来自模拟的相同查询时。

Conclusion: QBO 是一种有前途的提高航空航天制造中样本效率的方法。

Abstract: Recent efforts in smart manufacturing have enhanced aerospace fuselage assembly processes, particularly by innovating shape adjustment techniques to minimize dimensional gaps between assembled sections. Existing approaches have shown promising results but face the issue of low sample efficiency from the manufacturing systems. It arises from the limitation of the classical Monte Carlo method when uncovering the mean response from a distribution. In contrast, recent work has shown that quantum algorithms can achieve the same level of estimation accuracy with significantly fewer samples than the classical Monte Carlo method from distributions. Therefore, we can adopt the estimation of the quantum algorithm to obtain the estimation from real physical systems (distributions). Motivated by this advantage, we propose a Quantum Bayesian Optimization (QBO) framework for precise shape control during assembly to improve the sample efficiency in manufacturing practice. Specifically, this approach utilizes a quantum oracle, based on finite element analysis (FEA)-based models or surrogate models, to acquire a more accurate estimation of the environment response with fewer queries for a certain input. QBO employs an Upper Confidence Bound (UCB) as the acquisition function to strategically select input values that are most likely to maximize the objective function. It has been theoretically proven to require much fewer samples while maintaining comparable optimization results. In the case study, force-controlled actuators are applied to one fuselage section to adjust its shape and reduce the gap to the adjoining section. Experimental results demonstrate that QBO achieves significantly lower dimensional error and uncertainty compared to classical methods, particularly using the same queries from the simulation.

</details>


### [188] [Decomposed Trust: Exploring Privacy, Adversarial Robustness, Fairness, and Ethics of Low-Rank LLMs](https://arxiv.org/abs/2511.22099)
*Daniel Agyei Asante,Md Mokarram Chowdhury,Yang Li*

Main category: cs.LG

TL;DR: 本文研究了低秩分解对大型语言模型（LLM）可信度的影响，包括隐私、对抗鲁棒性、公平性和伦理一致性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型规模庞大，难以在资源受限的环境中部署。模型压缩是一种解决方案，低秩分解是降低模型大小、内存和计算量同时保持准确性的有效方法。然而，压缩模型的可信度影响尚不清楚。

Method: 对不同大小和变体的LLM进行评估，这些LLM使用不同的低秩算法进行压缩。

Result: （1）低秩压缩保留或提高了训练数据隐私，但削弱了对话期间的PII保护；（2）对抗鲁棒性通常被保留甚至增强；（3）伦理推理在零样本设置中退化，但通过少量样本提示部分恢复；（4）公平性下降。

Conclusion: 本文通过梯度归因分析，确定了LLM中对对抗鲁棒性贡献最大的层，以指导可信的压缩策略。

Abstract: Large language models (LLMs) have driven major advances across domains, yet their massive size hinders deployment in resource-constrained settings. Model compression addresses this challenge, with low-rank factorization emerging as a particularly effective method for reducing size, memory, and computation while maintaining accuracy. However, while these compressed models boast of benign performance and system-level advantages, their trustworthiness implications remain poorly understood. In this paper, we present the first comprehensive study of how low-rank factorization affects LLM trustworthiness across privacy, adversarial robustness, fairness, and ethical alignment. We evaluate multiple LLMs of different sizes and variants compressed with diverse low-rank algorithms, revealing key insights: (1) low-rank compression preserves or improves training data privacy but weakens PII protection during conversation; (2) adversarial robustness is generally preserved and often enhanced, even under deep compression; (3) ethical reasoning degrades in zero-shot settings but partially recovers with few-shot prompting; (4) fairness declines under compression. Beyond compression, we investigate how model scale and fine-tuning affect trustworthiness, as both are important in low-rank methods. To guide trustworthy compression strategies, we end our paper with a gradient-based attribution analysis to identify which layers in LLMs contribute most to adversarial robustness.

</details>


### [189] [Adaptive Dueling Double Deep Q-networks in Uniswap V3 Replication and Extension with Mamba](https://arxiv.org/abs/2511.22101)
*Zhaofeng Zhang*

Main category: cs.LG

TL;DR: 本文复现并改进了“基于深度强化学习的Uniswap V3自适应流动性提供”一文。提出了一个结合Mamba与DDQN的新模型，并使用了新的奖励函数。


<details>
  <summary>Details</summary>
Motivation: 探索更优的Uniswap V3流动性提供策略，原模型有改进空间。

Method: 复现原论文，然后结合Mamba和DDQN，提出新的模型结构和奖励函数。

Result: 新模型在部分测试中表现优于原模型，并具有更强的理论支持。

Conclusion: 新模型具有潜力，但尚未在所有数据集上进行验证。

Abstract: The report goes through the main steps of replicating and improving the article "Adaptive Liquidity Provision in Uniswap V3 with Deep Reinforcement Learning." The replication part includes how to obtain data from the Uniswap Subgraph, details of the implementation, and comments on the results. After the replication, I propose a new structure based on the original model, which combines Mamba with DDQN and a new reward function. In this new structure, I clean the data again and introduce two new baselines for comparison. As a result, although the model has not yet been applied to all datasets, it shows stronger theoretical support than the original model and performs better in some tests.

</details>


### [190] [Representative Action Selection for Large Action Space: From Bandits to MDPs](https://arxiv.org/abs/2511.22104)
*Quan Zhou,Shie Mannor*

Main category: cs.LG

TL;DR: 从一个巨大的动作空间中选择一个小的、有代表性的动作子集，这个动作空间在强化学习（RL）环境族中共享


<details>
  <summary>Details</summary>
Motivation: 在库存管理和推荐系统等应用中，直接在整个空间上进行学习是难以处理的

Method: 将之前关于元强盗的结果扩展到更一般的马尔可夫决策过程（MDP）设置

Result: 证明了我们现有的算法实现了与使用完整动作空间相当的性能。这个理论保证是在一个宽松的、非中心化的亚高斯过程模型下建立的，该模型可以适应更大的环境异质性。

Conclusion: 为不确定性下的大规模组合决策提供了一个计算和样本效率高的解决方案

Abstract: We study the problem of selecting a small, representative action subset from an extremely large action space shared across a family of reinforcement learning (RL) environments -- a fundamental challenge in applications like inventory management and recommendation systems, where direct learning over the entire space is intractable. Our goal is to identify a fixed subset of actions that, for every environment in the family, contains a near-optimal action, thereby enabling efficient learning without exhaustively evaluating all actions.
  This work extends our prior results for meta-bandits to the more general setting of Markov Decision Processes (MDPs). We prove that our existing algorithm achieves performance comparable to using the full action space. This theoretical guarantee is established under a relaxed, non-centered sub-Gaussian process model, which accommodates greater environmental heterogeneity. Consequently, our approach provides a computationally and sample-efficient solution for large-scale combinatorial decision-making under uncertainty.

</details>


### [191] [Energy Efficient Sleep Mode Optimization in 5G mmWave Networks via Multi Agent Deep Reinforcement Learning](https://arxiv.org/abs/2511.22105)
*Saad Masrur,Ismail Guvenc,David Lopez Perez*

Main category: cs.LG

TL;DR: 提出了一种基于多智能体深度强化学习（MARL）框架的动态睡眠模式优化（SMO）方法，用于在毫米波（mmWave）网络中最大限度地提高能源效率（EE）。


<details>
  <summary>Details</summary>
Motivation: 现有优化和强化学习方法依赖于聚合的静态基站（BS）流量模型，无法捕捉非平稳的流量动态，并且存在大的状态-动作空间，限制了实际部署。

Method: 提出了一种使用双深度Q网络（DDQN）的MARL框架，称为MARL-DDQN，用于在具有时变和基于社区的用户设备（UE）移动模型的3D城市环境中进行自适应SMO。该方法集成了实际的BS功耗模型和波束成形，以准确量化EE，同时吞吐量方面定义QoS。

Result: 仿真结果表明，MARL-DDQN优于最先进的策略，包括All On、迭代QoS感知负载（IT-QoS-LB）、MARL-DDPG和MARL-PPO，在动态场景下实现了高达0.60 Mbit/Joule EE，8.5 Mbps的第10百分位吞吐量，并在95%的时间内满足QoS约束。

Conclusion: MARL-DDQN 能够在动态场景下有效提高能源效率并满足服务质量要求。

Abstract: Dynamic sleep mode optimization (SMO) in millimeter-wave (mmWave) networks is essential for maximizing energy efficiency (EE) under stringent quality-of-service (QoS) constraints. However, existing optimization and reinforcement learning (RL) approaches rely on aggregated, static base station (BS) traffic models that fail to capture non-stationary traffic dynamics and suffer from large state-action spaces, limiting real-world deployment. To address these challenges, this paper proposes a multi-agent deep reinforcement learning (MARL) framework using a Double Deep Q-Network (DDQN), referred to as MARL-DDQN, for adaptive SMO in a 3D urban environment with a time-varying and community-based user equipment (UE) mobility model. Unlike conventional single-agent RL, MARL-DDQN enables scalable, distributed decision-making with minimal signaling overhead. A realistic BS power consumption model and beamforming are integrated to accurately quantify EE, while QoS is defined in terms of throughput. The method adapts SMO policies to maximize EE while mitigating inter-cell interference and ensuring throughput fairness. Simulations show that MARL-DDQN outperforms state-of-the-art strategies, including All On, iterative QoS-aware load-based (IT-QoS-LB), MARL-DDPG, and MARL-PPO, achieving up to 0.60 Mbit/Joule EE, 8.5 Mbps 10th-percentile throughput, and meeting QoS constraints 95% of the time under dynamic scenarios.

</details>


### [192] [An energy-efficient spiking neural network with continuous learning for self-adaptive brain-machine interface](https://arxiv.org/abs/2511.22108)
*Zhou Biyan,Arindam Basu*

Main category: cs.LG

TL;DR: 提出使用深度脉冲神经网络(DSNNs)和强化学习(RL)算法进行连续学习，以解决植入式脑机接口(iBMIs)中解码器性能不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 植入式脑机接口(iBMIs)中神经元数量呈指数增长，将神经解码器集成到植入物中是一种有效的数据压缩方法。然而，系统的非平稳性使得解码器的性能变得不可靠，需要连续学习来避免频繁的解码器再训练，并确保iBMI用户的安全和舒适。

Method: 采用两种强化学习(RL)算法，Banditron和AGREL，并将其适配于DSNNs，用于连续学习。

Result: 在开放循环实验中，DSNN Banditron和DSNN AGREL的准确率在较长时间内保持稳定。在带有扰动的闭环实验中，DSNN Banditron的性能与DSNN AGREL相当，同时在训练过程中减少了98%的内存访问使用和99%的乘加(MAC)操作需求。DSNN Banditron比以前的连续学习SNN解码器减少了98%的计算量。

Conclusion: DSNN Banditron是一种很有前途的未来无线iBMI系统的候选方案，因为它需要的计算量更少。

Abstract: The number of simultaneously recorded neurons follows an exponentially increasing trend in implantable brain-machine interfaces (iBMIs). Integrating the neural decoder in the implant is an effective data compression method for future wireless iBMIs. However, the non-stationarity of the system makes the performance of the decoder unreliable. To avoid frequent retraining of the decoder and to ensure the safety and comfort of the iBMI user, continuous learning is essential for real-life applications. Since Deep Spiking Neural Networks (DSNNs) are being recognized as a promising approach for developing resource-efficient neural decoder, we propose continuous learning approaches with Reinforcement Learning (RL) algorithms adapted for DSNNs. Banditron and AGREL are chosen as the two candidate RL algorithms since they can be trained with limited computational resources, effectively addressing the non-stationary problem and fitting the energy constraints of implantable devices. To assess the effectiveness of the proposed methods, we conducted both open-loop and closed-loop experiments. The accuracy of open-loop experiments conducted with DSNN Banditron and DSNN AGREL remains stable over extended periods. Meanwhile, the time-to-target in the closed-loop experiment with perturbations, DSNN Banditron performed comparably to that of DSNN AGREL while achieving reductions of 98% in memory access usage and 99% in the requirements for multiply- and-accumulate (MAC) operations during training. Compared to previous continuous learning SNN decoders, DSNN Banditron requires 98% less computes making it a prime candidate for future wireless iBMI systems.

</details>


### [193] [Toward Data-Driven Surrogates of the Solar Wind with Spherical Fourier Neural Operator](https://arxiv.org/abs/2511.22112)
*Reza Mansouri,Dustin Kempton,Pete Riley,Rafal Angryk*

Main category: cs.LG

TL;DR: 本文利用球面傅里叶神经算子（SFNO）开发了一种用于稳态太阳风建模的替代模型，旨在提高空间天气预报的效率。


<details>
  <summary>Details</summary>
Motivation: 精确的太阳风建模对于空间天气预报至关重要，因为太阳风的变化会影响地球附近的卫星、电网和通信系统。然而，三维磁流体动力学（MHD）模型计算成本高昂，限制了其在研究边界条件不确定性影响方面的应用。

Method: 使用球面傅里叶神经算子（SFNO）构建稳态太阳风建模的替代模型，并与先前的数值替代模型HUX进行比较。

Result: SFNO模型在多个指标上取得了与HUX模型相当或更好的性能。虽然HUX在物理平滑性方面仍具有优势，但这表明需要改进评估标准，而不是SFNO本身存在缺陷。

Conclusion: SFNO作为一种灵活且可训练的方法，能够实现高效的实时预测，并可以通过更多数据进行改进。

Abstract: The solar wind, a continuous stream of charged particles from the Sun's corona, shapes the heliosphere and impacts space systems near Earth. Variations such as high-speed streams and coronal mass ejections can disrupt satellites, power grids, and communications, making accurate modeling essential for space weather forecasting. While 3D magnetohydrodynamic (MHD) models are used to simulate and investigate these variations in the solar wind, they tend to be computationally expensive, limiting their usefulness in investigating the impacts of boundary condition uncertainty. In this work, we develop a surrogate for steady state solar wind modeling, using a Spherical Fourier Neural Operator (SFNO). We compare our model to a previously developed numerical surrogate for this task called HUX, and we show that the SFNO achieves comparable or better performance across several metrics. Though HUX retains advantages in physical smoothness, this underscores the need for improved evaluation criteria rather than a flaw in SFNO. As a flexible and trainable approach, SFNO enables efficient real-time forecasting and can improve with more data. The source code and more visual results are available at https://github.com/rezmansouri/solarwind-sfno-velocity.

</details>


### [194] [IVGAE: Handling Incomplete Heterogeneous Data with a Variational Graph Autoencoder](https://arxiv.org/abs/2511.22116)
*Youran Zhou,Mohamed Reda Bouadjenek,Sunil Aryal%*

Main category: cs.LG

TL;DR: IVGAE: A Variational Graph Autoencoder framework for robust imputation of incomplete heterogeneous data.


<details>
  <summary>Details</summary>
Motivation: Handling missing data in real-world tabular datasets is challenging, especially with heterogeneous numerical and categorical features. Existing methods often fail to capture complex dependencies and handle such data effectively.

Method: IVGAE constructs a bipartite graph to represent sample-feature relationships and applies graph representation learning with a dual-decoder architecture: one decoder reconstructs feature embeddings, and the other models missingness patterns. A Transformer-based heterogeneous embedding module encodes categorical variables.

Result: IVGAE achieves consistent improvements in RMSE and downstream F1 across MCAR, MAR, and MNAR missing scenarios under 30% missing rates on 16 real-world datasets.

Conclusion: IVGAE is effective for imputing missing data in heterogeneous datasets.

Abstract: Handling missing data remains a fundamental challenge in real-world tabular datasets, especially when data are heterogeneous with both numerical and categorical features. Existing imputation methods often fail to capture complex structural dependencies and handle heterogeneous data effectively. We present \textbf{IVGAE}, a Variational Graph Autoencoder framework for robust imputation of incomplete heterogeneous data. IVGAE constructs a bipartite graph to represent sample-feature relationships and applies graph representation learning to model structural dependencies. A key innovation is its \textit{dual-decoder architecture}, where one decoder reconstructs feature embeddings and the other models missingness patterns, providing structural priors aware of missing mechanisms. To better encode categorical variables, we introduce a Transformer-based heterogeneous embedding module that avoids high-dimensional one-hot encoding. Extensive experiments on 16 real-world datasets show that IVGAE achieves consistent improvements in RMSE and downstream F1 across MCAR, MAR, and MNAR missing scenarios under 30\% missing rates. Code and data are available at: https://github.com/echoid/IVGAE.

</details>


### [195] [A Variational Manifold Embedding Framework for Nonlinear Dimensionality Reduction](https://arxiv.org/abs/2511.22128)
*John J. Vastola,Samuel J. Gershman,Kanaka Rajan*

Main category: cs.LG

TL;DR: 提出了一种新的降维框架，该框架通过将降维算法视为最优流形嵌入问题的解决方案，从而允许非线性嵌入，同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的降维算法，如PCA，缺乏捕捉非线性数据流形结构的能力，而其他方法，如自编码器和基于图嵌入的方法，又难以解释或产生流形几何的病态扭曲。

Method: 提出了一个变分框架，将降维算法视为最优流形嵌入问题的解。该框架允许非线性嵌入，并通过偏微分方程和对称性反映嵌入目标。

Result: 该框架的解满足一组偏微分方程，并且在某些情况下可以进行解析表征。一个特例可以完全恢复PCA。

Conclusion: 该变分框架提供了一种灵活且可解释的降维方法，能够克服现有算法的局限性。

Abstract: Dimensionality reduction algorithms like principal component analysis (PCA) are workhorses of machine learning and neuroscience, but each has well-known limitations. Variants of PCA are simple and interpretable, but not flexible enough to capture nonlinear data manifold structure. More flexible approaches have other problems: autoencoders are generally difficult to interpret, and graph-embedding-based methods can produce pathological distortions in manifold geometry. Motivated by these shortcomings, we propose a variational framework that casts dimensionality reduction algorithms as solutions to an optimal manifold embedding problem. By construction, this framework permits nonlinear embeddings, allowing its solutions to be more flexible than PCA. Moreover, the variational nature of the framework has useful consequences for interpretability: each solution satisfies a set of partial differential equations, and can be shown to reflect symmetries of the embedding objective. We discuss these features in detail and show that solutions can be analytically characterized in some cases. Interestingly, one special case exactly recovers PCA.

</details>
