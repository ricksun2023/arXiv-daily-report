<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 52]
- [cs.CV](#cs.CV) [Total: 46]
- [cs.AI](#cs.AI) [Total: 45]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.LG](#cs.LG) [Total: 47]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Bridging the Semantic Gap: Contrastive Rewards for Multilingual Text-to-SQL](https://arxiv.org/abs/2510.13827)
*Ashish Kattamuri,Ishita Prasad,Meetu Malhotra,Arpita Vats,Rahul Raja,Albert Lie*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架，该框架结合了组相对策略优化 (GRPO) 和多语言对比奖励信号，以提高跨语言场景中 Text-to-SQL 系统的任务效率和语义准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的 Text-to-SQL 方法只评估和关注可执行的查询，忽略了语义对齐的挑战，包括查询的语义含义和执行结果的正确性。从英语翻译成其他语言时，即使是执行的准确性本身也会显著下降。

Method: 该方法通过结合基于语义相似性的奖励信号，来教导模型在 SQL 生成和用户意图之间获得更好的对应关系。使用 GRPO 微调 LLaMA-3-3B 模型，并在 GRPO 框架中添加对比奖励信号。

Result: 在七种语言的 MultiSpider 数据集上，使用 GRPO 微调 LLaMA-3-3B 模型将执行准确率提高到 87.4%（比零样本高 +26 pp），语义准确率提高到 52.29%（+32.86 pp）。在 GRPO 框架中添加对比奖励信号进一步将平均语义准确率提高到 59.14%（+6.85 pp，越南语高达 +10 pp）。

Conclusion: 实验表明，可以使用对比奖励来提高 Text-to-SQL 系统的性能，以实现有针对性的语义对齐，而无需大规模的训练数据集。

Abstract: Current Text-to-SQL methods are evaluated and only focused on executable
queries, overlooking the semantic alignment challenge -- both in terms of the
semantic meaning of the query and the correctness of the execution results.
Even execution accuracy itself shows significant drops when moving from English
to other languages, with an average decline of 6 percentage points across
non-English languages. We address these challenges by presenting a new
framework that combines Group Relative Policy Optimization (GRPO) within a
multilingual contrastive reward signal to enhance both task efficiency and
semantic accuracy in Text-to-SQL systems in cross-lingual scenarios. Our method
teaches models to obtain better correspondence between SQL generation and user
intent by combining a reward signal based on semantic similarity. On the
seven-language MultiSpider dataset, fine-tuning the LLaMA-3-3B model with GRPO
improved the execution accuracy up to 87.4 percent (+26 pp over zero-shot) and
semantic accuracy up to 52.29 percent (+32.86 pp). Adding our contrastive
reward signal in the GRPO framework further improved the average semantic
accuracy to 59.14 percent (+6.85 pp, up to +10 pp for Vietnamese). Our
experiments showcase that a smaller, parameter-efficient 3B LLaMA model
fine-tuned with our contrastive reward signal outperforms a much larger
zero-shot 8B LLaMA model, with an uplift of 7.43 pp in execution accuracy (from
81.43 percent on the 8B model to 88.86 percent on the 3B model), and nearly
matches its semantic accuracy (59.14 percent vs. 68.57 percent) -- all using
just 3,000 reinforcement learning training examples. These results demonstrate
how we can improve the performance of Text-to-SQL systems with contrastive
rewards for directed semantic alignment, without requiring large-scale training
datasets.

</details>


### [2] [BenchPress: A Human-in-the-Loop Annotation System for Rapid Text-to-SQL Benchmark Curation](https://arxiv.org/abs/2510.13853)
*Fabian Wenz,Omar Bouattour,Devin Yang,Justin Choi,Cecil Gregg,Nesime Tatbul,Çağatay Demiralp*

Main category: cs.CL

TL;DR: 这篇论文介绍了一个名为BenchPress的人工参与系统，旨在加速创建特定领域的text-to-SQL基准。


<details>
  <summary>Details</summary>
Motivation: 现有text-to-SQL模型在私有企业数据仓库上的效果不佳，并且手动标注SQL日志以创建基准的成本很高。

Method: BenchPress使用检索增强生成（RAG）和LLMs来生成多个自然语言描述，然后由人工专家选择、排序或编辑这些草稿。

Result: 实验表明，LLM辅助标注大大减少了创建高质量基准所需的时间和精力，并提高了标注准确性、基准可靠性和模型评估的稳健性。

Conclusion: BenchPress通过简化自定义基准的创建，为研究人员和从业人员提供了一种评估特定领域工作负载上text-to-SQL模型的方法。

Abstract: Large language models (LLMs) have been successfully applied to many tasks,
including text-to-SQL generation. However, much of this work has focused on
publicly available datasets, such as Fiben, Spider, and Bird. Our earlier work
showed that LLMs are much less effective in querying large private enterprise
data warehouses and released Beaver, the first private enterprise text-to-SQL
benchmark. To create Beaver, we leveraged SQL logs, which are often readily
available. However, manually annotating these logs to identify which natural
language questions they answer is a daunting task. Asking database
administrators, who are highly trained experts, to take on additional work to
construct and validate corresponding natural language utterances is not only
challenging but also quite costly. To address this challenge, we introduce
BenchPress, a human-in-the-loop system designed to accelerate the creation of
domain-specific text-to-SQL benchmarks. Given a SQL query, BenchPress uses
retrieval-augmented generation (RAG) and LLMs to propose multiple natural
language descriptions. Human experts then select, rank, or edit these drafts to
ensure accuracy and domain alignment. We evaluated BenchPress on annotated
enterprise SQL logs, demonstrating that LLM-assisted annotation drastically
reduces the time and effort required to create high-quality benchmarks. Our
results show that combining human verification with LLM-generated suggestions
enhances annotation accuracy, benchmark reliability, and model evaluation
robustness. By streamlining the creation of custom benchmarks, BenchPress
offers researchers and practitioners a mechanism for assessing text-to-SQL
models on a given domain-specific workload. BenchPress is freely available via
our public GitHub repository at
https://github.com/fabian-wenz/enterprise-txt2sql and is also accessible on our
website at http://dsg-mcgraw.csail.mit.edu:5000.

</details>


### [3] [From Explainability to Action: A Generative Operational Framework for Integrating XAI in Clinical Mental Health Screening](https://arxiv.org/abs/2510.13828)
*Ratna Kandala,Akshata Kishore Moharir,Divya Arvinda Nayak*

Main category: cs.CL

TL;DR: XAI在精神健康筛查(MHS)中潜力巨大，但实验室到临床的差距仍然存在。当前XAI技术产生技术性的输出，但未能提供临床相关、可操作的见解。


<details>
  <summary>Details</summary>
Motivation: 解决XAI技术在精神健康筛查中，技术透明性和人类效用之间的脱节问题，这是现实应用的主要障碍。

Method: 提出Generative Operational Framework，利用大型语言模型(llm)作为中央翻译引擎，摄取来自不同XAI工具的原始技术输出，并通过RAG与临床指南相结合，自动生成人类可读的、有证据支持的临床叙述。

Result: 系统分析了框架集成的组件，追溯了从内在模型到生成XAI的演变。证明了该框架如何直接解决关键的操作障碍，包括工作流集成、偏差缓解和涉众特定的沟通。

Conclusion: 为推动该领域超越孤立数据点的生成，朝着在临床实践中交付集成、可操作和可信赖的AI，提供了一个战略路线图。

Abstract: Explainable Artificial Intelligence (XAI) has been presented as the critical
component for unlocking the potential of machine learning in mental health
screening (MHS). However, a persistent lab-to-clinic gap remains. Current XAI
techniques, such as SHAP and LIME, excel at producing technically faithful
outputs such as feature importance scores, but fail to deliver clinically
relevant, actionable insights that can be used by clinicians or understood by
patients. This disconnect between technical transparency and human utility is
the primary barrier to real-world adoption. This paper argues that this gap is
a translation problem and proposes the Generative Operational Framework, a
novel system architecture that leverages Large Language Models (LLMs) as a
central translation engine. This framework is designed to ingest the raw,
technical outputs from diverse XAI tools and synthesize them with clinical
guidelines (via RAG) to automatically generate human-readable, evidence-backed
clinical narratives. To justify our solution, we provide a systematic analysis
of the components it integrates, tracing the evolution from intrinsic models to
generative XAI. We demonstrate how this framework directly addresses key
operational barriers, including workflow integration, bias mitigation, and
stakeholder-specific communication. This paper also provides a strategic
roadmap for moving the field beyond the generation of isolated data points
toward the delivery of integrated, actionable, and trustworthy AI in clinical
practice.

</details>


### [4] [A Linguistics-Aware LLM Watermarking via Syntactic Predictability](https://arxiv.org/abs/2510.13829)
*Shinwoo Park,Hyejin Park,Hyeseon Ahn,Yo-Sub Han*

Main category: cs.CL

TL;DR: 提出了一种名为STELA的新框架，用于公开验证的水印技术，该技术在文本质量和检测鲁棒性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的快速发展，可靠的治理工具变得至关重要。公开可验证的水印对于建立可信赖的AI生态系统尤其重要。现有方法依赖于模型输出分布的信号，但需要访问底层模型的logits，这阻碍了公开验证。

Method: STELA通过使用词性（POS）n-gram建模的语言不确定性来动态调节信号，从而将水印强度与语言中固有的语言自由度对齐。在语法约束的环境中减弱水印，以保持质量，并在具有更大语言灵活性的环境中加强水印，以提高可检测性。检测器无需访问任何模型logits即可运行，从而方便公开验证。

Result: 在类型多样的语言（英语、汉语和韩语）上的大量实验表明，STELA在检测鲁棒性方面优于先前的方法。

Conclusion: STELA框架在文本质量和检测鲁棒性之间取得了平衡，并且实现了公开验证。

Abstract: As large language models (LLMs) continue to advance rapidly, reliable
governance tools have become critical. Publicly verifiable watermarking is
particularly essential for fostering a trustworthy AI ecosystem. A central
challenge persists: balancing text quality against detection robustness. Recent
studies have sought to navigate this trade-off by leveraging signals from model
output distributions (e.g., token-level entropy); however, their reliance on
these model-specific signals presents a significant barrier to public
verification, as the detection process requires access to the logits of the
underlying model. We introduce STELA, a novel framework that aligns watermark
strength with the linguistic degrees of freedom inherent in language. STELA
dynamically modulates the signal using part-of-speech (POS) n-gram-modeled
linguistic indeterminacy, weakening it in grammatically constrained contexts to
preserve quality and strengthen it in contexts with greater linguistic
flexibility to enhance detectability. Our detector operates without access to
any model logits, thus facilitating publicly verifiable detection. Through
extensive experiments on typologically diverse languages-analytic English,
isolating Chinese, and agglutinative Korean-we show that STELA surpasses prior
methods in detection robustness. Our code is available at
https://github.com/Shinwoo-Park/stela_watermark.

</details>


### [5] [Users as Annotators: LLM Preference Learning from Comparison Mode](https://arxiv.org/abs/2510.13830)
*Zhongze Cai,Xiaocheng Li*

Main category: cs.CL

TL;DR: 本文提出了一种利用用户在使用LLM过程中产生的偏好数据来对LLM进行对齐的方法。与传统的人工标注数据相比，用户数据更贴近实际应用，但质量难以保证。本文提出了一种用户行为模型，通过比较不同模型或同一模型的不同版本生成的回复，来推断用户的标注质量，并过滤低质量数据。


<details>
  <summary>Details</summary>
Motivation: 利用用户在使用LLM过程中产生的偏好数据来对LLM进行对齐，这种数据的优势在于更贴近实际应用场景。然而，用户数据的质量控制是一个问题。

Method: 提出了一种用户行为模型，通过比较不同模型或同一模型的不同版本生成的回复，来推断用户的标注质量。使用期望最大化算法来估计用户的潜在质量因子，并据此过滤用户的标注数据。

Result: 下游任务表明，该方法能够有效地捕捉用户行为，并为LLM对齐提供有效的数据过滤。

Conclusion: 本文提出的方法能够有效地利用用户在使用LLM过程中产生的偏好数据，通过对用户行为的建模和数据过滤，提高LLM对齐的效果。

Abstract: Pairwise preference data have played an important role in the alignment of
large language models (LLMs). Each sample of such data consists of a prompt,
two different responses to the prompt, and a binary label indicating which of
the two responses is better. The labels are usually annotated by professional
human annotators. In this paper, we consider an alternative approach to collect
pairwise preference data -- user annotation from comparison mode. With the
increasingly wider adoption of LLMs among the population, users are
contributing more and more of their preference labels through their daily
interactions with the LLMs. The upside of such labels is that users are the
best experts in judging the responses to their own queries/prompts, but the
downside is the lack of quality control in these labels. In this paper, we
consider a new idea of generating two responses from two different models or
two different versions of the same model. The asymmetry allows us to make an
inference of the user's data quality through our proposed user behavior model.
We develop an expectation-maximization algorithm to estimate a latent quality
factor of the user, and filter users' annotation data accordingly. The
downstream task shows the effectiveness of our approach in both capturing the
user behavior and data filtering for LLM alignment.

</details>


### [6] [Informed Routing in LLMs: Smarter Token-Level Computation for Faster Inference](https://arxiv.org/abs/2510.13831)
*Chao Han,Yijuan Liang,Zihao Xuan,Daokuan Wu,Wei Zhang,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 这篇论文提出了一种名为“知情路由”的新范式，旨在解决大型语言模型（LLM）推理成本高的问题。该方法通过预测token的重要性及其可恢复性，从而决定是执行还是近似计算，以在保持模型精度的同时减少计算量。


<details>
  <summary>Details</summary>
Motivation: 现有动态token级别计算分配方法依赖于贪婪路由，容易导致不可逆的信息丢失和次优的token选择。

Method: 论文引入了轻量级特征预测器（LFF），用于在做出路由决策之前评估token的重要性和可恢复性。这使得模型能够采用灵活的执行或近似策略。

Result: 在语言建模和推理任务上的大量实验表明，知情路由在多个稀疏级别上实现了最先进的效率-性能权衡。即使没有最终的LoRA微调，该方法也能匹配或超过需要完全微调的强大基线，同时将训练时间减少50%以上。

Conclusion: 知情路由是一种有效的方法，可以在降低LLM推理成本的同时保持模型性能。

Abstract: The deployment of large language models (LLMs) in real-world applications is
increasingly limited by their high inference cost. While recent advances in
dynamic token-level computation allocation attempt to improve efficiency by
selectively activating model components per token, existing methods rely on
greedy routing--a myopic execute-or-skip mechanism that often leads to
irreversible information loss and suboptimal token selection. This paper
introduces informed routing, a new paradigm that proactively addresses these
issues. The key insight is to assess not only a token's immediate importance
but also its recoverability, i.e., how well its transformation can be
approximated. To this end, we propose the Lightweight Feature Forecaster (LFF),
a small predictive module that estimates a unit's output before routing
decisions are made. This enables a flexible execute-or-approximate policy that
preserves model fidelity while drastically reducing computation. Extensive
experiments on both language modeling and reasoning tasks show that informed
routing achieves state-of-the-art efficiency-performance trade-offs across
multiple sparsity levels. Notably, even without final LoRA fine-tuning, our
method matches or surpasses strong baselines that require full fine-tuning, all
while reducing training time by over 50%. The code is available at:
https://github.com/EIT-NLP/informed-routing

</details>


### [7] [Entropy Meets Importance: A Unified Head Importance-Entropy Score for Stable and Efficient Transformer Pruning](https://arxiv.org/abs/2510.13832)
*Minsik Choi,Hyegang Son,Changhoon Kim,Young Geun Kim*

Main category: cs.CL

TL;DR: 提出了一种新的剪枝标准HIES（Head Importance-Entropy Score），它整合了头部重要性分数和注意力熵，从而为每个头部贡献提供补充证据。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的模型在自然语言处理任务中取得了显著的性能。然而，它们的结构特征——多层和注意力头——在推理和部署中带来了效率挑战。为了应对这些挑战，最近提出了各种剪枝方法。特别是，使用头部重要性分数（HIS）的基于梯度的方法因其可解释性、效率和识别冗余头部的能力而受到关注。然而，HIS本身具有局限性，因为它只捕捉梯度驱动的贡献，忽略了注意力模式的多样性。

Method: 我们引入了一种新的剪枝标准，HIES（Head Importance-Entropy Score），它整合了头部重要性分数与注意力熵，从而为每个头部贡献提供补充证据。

Result: 基于HIES的剪枝在模型质量上产生了高达15.2%的改进，在稳定性上产生了2.04倍的改进，与仅使用HIS的方法相比，能够在不牺牲精度或稳定性的前提下实现大幅度的模型压缩。

Conclusion: HIES-based 剪枝方法优于 HIS-only 方法，且在不牺牲精度或稳定性的前提下实现大幅度的模型压缩。

Abstract: Transformer-based models have achieved remarkable performance in NLP tasks.
However, their structural characteristics-multiple layers and attention
heads-introduce efficiency challenges in inference and deployment. To address
these challenges, various pruning methods have recently been proposed. Notably,
gradient-based methods using Head Importance Scores (HIS) have gained traction
for interpretability, efficiency, and ability to identify redundant heads.
However, HIS alone has limitations as it captures only the gradient-driven
contribution, overlooking the diversity of attention patterns. To overcome
these limitations, we introduce a novel pruning criterion, HIES (Head
Importance-Entropy Score), which integrates head importance scores with
attention entropy, providing complementary evidence on per-head contribution.
Empirically, HIES-based pruning yields up to 15.2% improvement in model quality
and 2.04x improvement in stability over HIS-only methods, enabling substantial
model compression without sacrificing either accuracy or stability. Code will
be released upon publication.

</details>


### [8] [ConDABench: Interactive Evaluation of Language Models for Data Analysis](https://arxiv.org/abs/2510.13835)
*Avik Dutta,Priyanshu Gupta,Hosein Hasanbeig,Rahul Pratap Singh,Harshit Nigam,Sumit Gulwani,Arjun Radhakrishna,Gustavo Soares,Ashish Tiwari*

Main category: cs.CL

TL;DR: 介绍了ConDABench，一个用于生成会话数据分析基准并评估外部工具的框架。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM数据分析任务评估基准无法捕捉到复杂性或提供对交互性的支持。

Method: 一个多代理工作流程，用于从描述从公共数据集中获得的见解的文章中生成真实的基准。

Result: 对基准上最先进的LLM的评估表明，虽然新一代模型更擅长解决更多实例，但它们不一定更擅长解决需要持续、长篇参与的任务。

Conclusion: ConDABench是模型构建者衡量在完成复杂交互任务方面真正协作模型进展的途径。

Abstract: Real-world data analysis tasks often come with under-specified goals and
unclean data. User interaction is necessary to understand and disambiguate a
user's intent, and hence, essential to solving these complex tasks. Existing
benchmarks for evaluating LLMs on data analysis tasks do not capture these
complexities or provide first-class support for interactivity. We introduce
ConDABench, a framework for generating conversational data analysis (ConDA)
benchmarks and evaluating external tools on the generated benchmarks. \bench
consists of (a) a multi-agent workflow for generating realistic benchmarks from
articles describing insights gained from public datasets, (b) 1,420 ConDA
problems generated using this workflow, and (c) an evaluation harness that, for
the first time, makes it possible to systematically evaluate conversational
data analysis tools on the generated ConDA problems. Evaluation of
state-of-the-art LLMs on the benchmarks reveals that while the new generation
of models are better at solving more instances, they are not necessarily better
at solving tasks that require sustained, long-form engagement. ConDABench is an
avenue for model builders to measure progress towards truly collaborative
models that can complete complex interactive tasks.

</details>


### [9] [SIMBA UQ: Similarity-Based Aggregation for Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2510.13836)
*Debarun Bhattacharjya,Balaji Ganesan,Junkyu Lee,Radu Marinescu,Katsiaryna Mirylenka,Michael Glass,Xiao Shou*

Main category: cs.CL

TL;DR: 本文研究大型语言模型（LLM）何时知道自己不知道什么，即不确定性量化（UQ）问题。


<details>
  <summary>Details</summary>
Motivation: 信任的AI系统需要评估LLM生成输出的置信度。黑盒UQ方法不需要访问内部模型信息，具有实际优势。

Method: 本文研究主要为黑盒的UQ技术，使用生成输出之间的一致性作为置信度的代理，并提出一个基于相似性的聚合框架，使用小训练集训练置信度估计模型。

Result: 在问答、摘要和文本到SQL等任务的实验表明，提出的基于相似性的方法可以产生比基线方法更好的校准置信度。

Conclusion: 本文提出的相似性方法能有效提高LLM的置信度校准。

Abstract: When does a large language model (LLM) know what it does not know?
Uncertainty quantification (UQ) provides measures of uncertainty, such as an
estimate of the confidence in an LLM's generated output, and is therefore
increasingly recognized as a crucial component of trusted AI systems. Black-box
UQ methods do not require access to internal model information from the
generating LLM and therefore have numerous real-world advantages, such as
robustness to system changes, adaptability to choice of LLM, reduced costs, and
computational tractability. In this paper, we investigate the effectiveness of
UQ techniques that are primarily but not necessarily entirely black-box, where
the consistency between a generated output and other sampled generations is
used as a proxy for confidence in its correctness. We propose a high-level
non-verbalized similarity-based aggregation framework that subsumes a broad
swath of UQ approaches suitable for complex generative tasks, as well as
introduce specific novel techniques from the framework that train confidence
estimation models using small training sets. Through an empirical study with
datasets spanning the diverse tasks of question answering, summarization, and
text-to-SQL, we demonstrate that our proposed similarity-based methods can
yield better calibrated confidences than baselines.

</details>


### [10] [Seeing Hate Differently: Hate Subspace Modeling for Culture-Aware Hate Speech Detection](https://arxiv.org/abs/2510.13837)
*Weibin Cai,Reza Zafarani*

Main category: cs.CL

TL;DR: 现有的仇恨言论检测方法忽略了一个现实世界的复杂性：训练标签是有偏差的，对仇恨的解释因具有不同文化背景的个体而异。本文提出了一个文化感知框架，该框架构建了个人的仇恨子空间，以解决数据稀疏性、文化纠缠和模糊标签等挑战。实验表明，该方法在所有指标上的平均性能优于现有技术 1.05%。


<details>
  <summary>Details</summary>
Motivation: 现有的仇恨言论检测方法忽略了一个现实世界的复杂性：训练标签是有偏差的，对仇恨的解释因具有不同文化背景的个体而异。

Method: 提出了一个文化感知框架，该框架构建了个人的仇恨子空间。为了缓解数据稀疏性，我们对文化属性的组合进行建模。对于文化纠缠和模糊标签，我们使用标签传播来捕获每个组合的独特特征。

Result: 实验表明，该方法在所有指标上的平均性能优于现有技术 1.05%。

Conclusion: 构建了个人的仇恨子空间，可以进一步提高分类性能。

Abstract: Hate speech detection has been extensively studied, yet existing methods
often overlook a real-world complexity: training labels are biased, and
interpretations of what is considered hate vary across individuals with
different cultural backgrounds. We first analyze these challenges, including
data sparsity, cultural entanglement, and ambiguous labeling. To address them,
we propose a culture-aware framework that constructs individuals' hate
subspaces. To alleviate data sparsity, we model combinations of cultural
attributes. For cultural entanglement and ambiguous labels, we use label
propagation to capture distinctive features of each combination. Finally,
individual hate subspaces, which in turn can further enhance classification
performance. Experiments show our method outperforms state-of-the-art by 1.05\%
on average across all metrics.

</details>


### [11] [Meronymic Ontology Extraction via Large Language Models](https://arxiv.org/abs/2510.13839)
*Dekai Zhang,Simone Conia,Antonio Rago*

Main category: cs.CL

TL;DR: 本文提出了一种利用大型语言模型(llm)从原始评论文本中自动提取产品本体的方法。


<details>
  <summary>Details</summary>
Motivation: 本体在当今的数字时代已经变得至关重要，它可以组织大量可用的非结构化文本。在为这些信息提供正式结构方面，本体具有巨大的价值和应用，例如，电子商务，无数的产品列表需要适当的产品组织。然而，人工构建这些本体是一个耗时、昂贵和费力的过程。

Method: 我们利用大型语言模型(llm)开发了一种从原始评论文本中提取产品本体的全自动方法，其形式为部分整体关系。

Result: 我们证明，当使用llm作为评估方法时，我们的方法生成的产品本体超过了现有的基于bert的基线。

Conclusion: 我们的研究为llm更广泛地应用于(产品或其他)本体提取奠定了基础。

Abstract: Ontologies have become essential in today's digital age as a way of
organising the vast amount of readily available unstructured text. In providing
formal structure to this information, ontologies have immense value and
application across various domains, e.g., e-commerce, where countless product
listings necessitate proper product organisation. However, the manual
construction of these ontologies is a time-consuming, expensive and laborious
process. In this paper, we harness the recent advancements in large language
models (LLMs) to develop a fully-automated method of extracting product
ontologies, in the form of meronymies, from raw review texts. We demonstrate
that the ontologies produced by our method surpass an existing, BERT-based
baseline when evaluating using an LLM-as-a-judge. Our investigation provides
the groundwork for LLMs to be used more generally in (product or otherwise)
ontology extraction.

</details>


### [12] [PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering](https://arxiv.org/abs/2510.14278)
*Md Mahadi Hasan Nahid,Davood Rafiei*

Main category: cs.CL

TL;DR: 本文提出了一种 Agentic Retrieval System，利用大型语言模型在一个结构化的循环中检索相关证据，以提高多跳问答的精度和召回率。


<details>
  <summary>Details</summary>
Motivation: 多跳问答需要收集多个证据，而检索在其中起着核心作用。

Method: 该框架由三个专门的 Agent 组成：问题分析器，选择器和添加器。选择器和添加器之间的迭代交互产生了一组紧凑而全面的支持段落。

Result: 在四个多跳 QA 基准测试中，该方法始终优于强大的基线。

Conclusion: 该方法实现了更高的检索精度，同时滤除了分散注意力的内容，使下游 QA 模型能够超越完整上下文的答案准确性，同时依赖于明显更少的不相关信息。

Abstract: Retrieval plays a central role in multi-hop question answering (QA), where
answering complex questions requires gathering multiple pieces of evidence. We
introduce an Agentic Retrieval System that leverages large language models
(LLMs) in a structured loop to retrieve relevant evidence with high precision
and recall. Our framework consists of three specialized agents: a Question
Analyzer that decomposes a multi-hop question into sub-questions, a Selector
that identifies the most relevant context for each sub-question (focusing on
precision), and an Adder that brings in any missing evidence (focusing on
recall). The iterative interaction between Selector and Adder yields a compact
yet comprehensive set of supporting passages. In particular, it achieves higher
retrieval accuracy while filtering out distracting content, enabling downstream
QA models to surpass full-context answer accuracy while relying on
significantly less irrelevant information. Experiments on four multi-hop QA
benchmarks -- HotpotQA, 2WikiMultiHopQA, MuSiQue, and MultiHopRAG --
demonstrates that our approach consistently outperforms strong baselines.

</details>


### [13] [ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking](https://arxiv.org/abs/2510.13842)
*Yutao Wu,Xiao Liu,Yinghui Li,Yifeng Gao,Yifan Ding,Jiale Ding,Xiang Zheng,Xingjun Ma*

Main category: cs.CL

TL;DR: 知识中毒通过将对抗性内容注入知识库，诱使大型语言模型（LLM）产生由攻击者控制的输出，从而对检索增强生成（RAG）系统构成严重威胁。ADMIT 是一种少样本、语义对齐的中毒攻击，可以在没有访问目标 LLM、检索器或令牌级控制的情况下，反转事实核查决策并诱导欺骗性理由。在极低的 中毒率下，ADMIT 在 4 个检索器、11 个 LLM 和 4 个跨域基准测试中有效转移，平均攻击成功率 (ASR) 为 86%，即使在存在强烈的反证据的情况下仍然保持稳健。


<details>
  <summary>Details</summary>
Motivation: 先前的工作强调了 LLM 容易受到误导或恶意检索内容的影响。现实世界的事实核查场景更具挑战性，因为可信的证据通常在检索池中占主导地位。为了研究这个问题，我们将知识中毒扩展到事实核查环境，其中检索到的上下文包括真实的佐证或反驳证据。

Method: 我们提出了 ADMIT（对抗性多重注入技术），这是一种少样本、语义对齐的中毒攻击，可以在没有访问目标 LLM、检索器或令牌级控制的情况下，反转事实核查决策并诱导欺骗性理由。

Result: 广泛的实验表明，ADMIT 在 4 个检索器、11 个 LLM 和 4 个跨域基准测试中有效转移，在极低的 中毒率下，平均攻击成功率 (ASR) 为 86%，即使在存在强烈的反证据的情况下仍然保持稳健。与先前的最先进攻击相比，ADMIT 在所有设置中将 ASR 提高了 11.2%，暴露了基于 RAG 的真实世界事实核查系统中的重大漏洞。

Conclusion: ADMIT 攻击揭示了 RAG 系统在事实核查应用中存在的严重漏洞，即使在存在大量可信证据的情况下，也容易受到知识中毒攻击。

Abstract: Knowledge poisoning poses a critical threat to Retrieval-Augmented Generation
(RAG) systems by injecting adversarial content into knowledge bases, tricking
Large Language Models (LLMs) into producing attacker-controlled outputs
grounded in manipulated context. Prior work highlights LLMs' susceptibility to
misleading or malicious retrieved content. However, real-world fact-checking
scenarios are more challenging, as credible evidence typically dominates the
retrieval pool. To investigate this problem, we extend knowledge poisoning to
the fact-checking setting, where retrieved context includes authentic
supporting or refuting evidence. We propose \textbf{ADMIT}
(\textbf{AD}versarial \textbf{M}ulti-\textbf{I}njection \textbf{T}echnique), a
few-shot, semantically aligned poisoning attack that flips fact-checking
decisions and induces deceptive justifications, all without access to the
target LLMs, retrievers, or token-level control. Extensive experiments show
that ADMIT transfers effectively across 4 retrievers, 11 LLMs, and 4
cross-domain benchmarks, achieving an average attack success rate (ASR) of 86\%
at an extremely low poisoning rate of $0.93 \times 10^{-6}$, and remaining
robust even in the presence of strong counter-evidence. Compared with prior
state-of-the-art attacks, ADMIT improves ASR by 11.2\% across all settings,
exposing significant vulnerabilities in real-world RAG-based fact-checking
systems.

</details>


### [14] [Rethinking Schema Linking: A Context-Aware Bidirectional Retrieval Approach for Text-to-SQL](https://arxiv.org/abs/2510.14296)
*Md Mahadi Hasan Nahid,Davood Rafiei,Weiwei Zhang,Yong Zhang*

Main category: cs.CL

TL;DR: 这篇论文提出了一种上下文感知的双向模式检索框架，用于解决 Text-to-SQL 系统中的模式链接问题。该框架旨在提高模式检索的准确性和效率，从而提升 Text-to-SQL 的性能。


<details>
  <summary>Details</summary>
Motivation: 现有 Text-to-SQL 方法通常忽略相关模式元素的检索，导致幻觉和执行失败。因此，需要改进模式检索，将其作为一个独立的问题来解决。

Method: 该方法结合了两种互补策略：先表检索再列选择，以及先列检索再表选择。此外，还采用了问题分解、关键词提取和短语提取等技术。

Result: 在 BIRD 和 Spider 等基准测试中，该方法显著提高了模式召回率，同时减少了误报。使用该方法检索到的模式生成的 SQL 语句始终优于全模式基线，并且接近 oracle 性能。

Conclusion: 模式链接是提高 Text-to-SQL 准确性和效率的强大手段。该方法将全模式和完美模式设置之间的性能差距缩小了 50%。

Abstract: Schema linking -- the process of aligning natural language questions with
database schema elements -- is a critical yet underexplored component of
Text-to-SQL systems. While recent methods have focused primarily on improving
SQL generation, they often neglect the retrieval of relevant schema elements,
which can lead to hallucinations and execution failures. In this work, we
propose a context-aware bidirectional schema retrieval framework that treats
schema linking as a standalone problem. Our approach combines two complementary
strategies: table-first retrieval followed by column selection, and
column-first retrieval followed by table selection. It is further augmented
with techniques such as question decomposition, keyword extraction, and
keyphrase extraction. Through comprehensive evaluations on challenging
benchmarks such as BIRD and Spider, we demonstrate that our method
significantly improves schema recall while reducing false positives. Moreover,
SQL generation using our retrieved schema consistently outperforms full-schema
baselines and closely approaches oracle performance, all without requiring
query refinement. Notably, our method narrows the performance gap between full
and perfect schema settings by 50\%. Our findings highlight schema linking as a
powerful lever for enhancing Text-to-SQL accuracy and efficiency.

</details>


### [15] [Serialized EHR make for good text representations](https://arxiv.org/abs/2510.13843)
*Zhirong Chou,Quan Qin,Shi Li*

Main category: cs.CL

TL;DR: This paper introduces SerialBEHRT, a new foundation model for healthcare that improves EHR representation by encoding temporal and contextual relationships among clinical events.


<details>
  <summary>Details</summary>
Motivation: Existing healthcare foundation models struggle to reconcile the tabular and event-based nature of EHRs with the sequential priors of natural language models, limiting their ability to capture longitudinal dependencies.

Method: The authors introduce SerialBEHRT, a domain-aligned foundation model that extends SciBERT through additional pretraining on structured EHR sequences to encode temporal and contextual relationships among clinical events.

Result: SerialBEHRT achieves superior and more consistent performance on antibiotic susceptibility prediction compared to state-of-the-art EHR representation strategies.

Conclusion: The study highlights the importance of temporal serialization in foundation model pretraining for healthcare.

Abstract: The emergence of foundation models in healthcare has opened new avenues for
learning generalizable representations from large scale clinical data. Yet,
existing approaches often struggle to reconcile the tabular and event based
nature of Electronic Health Records (EHRs) with the sequential priors of
natural language models. This structural mismatch limits their ability to
capture longitudinal dependencies across patient encounters. We introduce
SerialBEHRT, a domain aligned foundation model that extends SciBERT through
additional pretraining on structured EHR sequences. SerialBEHRT is designed to
encode temporal and contextual relationships among clinical events, thereby
producing richer patient representations. We evaluate its effectiveness on the
task of antibiotic susceptibility prediction, a clinically meaningful problem
in antibiotic stewardship. Through extensive benchmarking against state of the
art EHR representation strategies, we demonstrate that SerialBEHRT achieves
superior and more consistent performance, highlighting the importance of
temporal serialization in foundation model pretraining for healthcare.

</details>


### [16] [PluriHop: Exhaustive, Recall-Sensitive QA over Distractor-Rich Corpora](https://arxiv.org/abs/2510.14377)
*Mykolas Sveistrys,Richard Kunert*

Main category: cs.CL

TL;DR: 这篇论文介绍了一种新的问答类型，称为 pluri-hop 问题，它需要跨多个文档进行聚合，并且对召回率非常敏感。作者提出了 PluriHopWIND 数据集来研究这种设置，并提出了 PluriHopRAG 架构来解决这个问题。


<details>
  <summary>Details</summary>
Motivation: 现有的问答系统在处理需要跨多个文档聚合的问题时存在局限性，尤其是在处理重复性高、干扰信息多的语料库时。

Method: 作者提出了 PluriHopRAG 架构，该架构将查询分解为文档级别的子问题，并使用交叉编码器过滤器来丢弃不相关的文档，然后再进行 LLM 推理。

Result: PluriHopRAG 在 F1 值上实现了 18-52% 的相对改进。

Conclusion: PluriHopWIND 数据集揭示了当前问答系统在处理重复性高、干扰信息多的语料库时的局限性。PluriHopRAG 的性能突出了详尽检索和早期过滤的价值。

Abstract: Recent advances in large language models (LLMs) and retrieval-augmented
generation (RAG) have enabled progress on question answering (QA) when relevant
evidence is in one (single-hop) or multiple (multi-hop) passages. Yet many
realistic questions about recurring report data - medical records, compliance
filings, maintenance logs - require aggregation across all documents, with no
clear stopping point for retrieval and high sensitivity to even one missed
passage. We term these pluri-hop questions and formalize them by three
criteria: recall sensitivity, exhaustiveness, and exactness. To study this
setting, we introduce PluriHopWIND, a diagnostic multilingual dataset of 48
pluri-hop questions built from 191 real-world wind industry reports in German
and English. We show that PluriHopWIND is 8-40% more repetitive than other
common datasets and thus has higher density of distractor documents, better
reflecting practical challenges of recurring report corpora. We test a
traditional RAG pipeline as well as graph-based and multimodal variants, and
find that none of the tested approaches exceed 40% in statement-wise F1 score.
Motivated by this, we propose PluriHopRAG, a RAG architecture that follows a
"check all documents individually, filter cheaply" approach: it (i) decomposes
queries into document-level subquestions and (ii) uses a cross-encoder filter
to discard irrelevant documents before costly LLM reasoning. We find that
PluriHopRAG achieves relative F1 score improvements of 18-52% depending on base
LLM. Despite its modest size, PluriHopWIND exposes the limitations of current
QA systems on repetitive, distractor-rich corpora. PluriHopRAG's performance
highlights the value of exhaustive retrieval and early filtering as a powerful
alternative to top-k methods.

</details>


### [17] [DynaSpec: Context-aware Dynamic Speculative Sampling for Large-Vocabulary Language Models](https://arxiv.org/abs/2510.13847)
*Jinbin Zhang,Nasib Ullah,Erik Schultheis,Rohit Babbar*

Main category: cs.CL

TL;DR: DynaSpec: a context-dependent dynamic shortlisting mechanism that speeds up drafting and generalizes across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Scaling of the LLM vocabulary has pushed the number of tokens to grow substantially, making the drafter's output head a latency bottleneck.

Method: Introduce lightweight, coarse-grained meta-classifiers that route contexts to a small number of token clusters; the union of the top-k selected clusters forms the drafter's shortlist, while verification retains the full vocabulary and exactness. The meta-classifier finishes its computation earlier than the drafter's hidden state generation by exploiting parallel execution of draft encoding and meta shortlisting on separate streams.

Result: Consistent gains in mean accepted length over fixed-shortlist baselines, while context-dependent selection enables smaller shortlists without degrading acceptance.

Conclusion: DynaSpec is robust and improves the efficiency of speculative decoding.

Abstract: Speculative decoding (a.k.a. speculative sampling) has become a standard way
to accelerate LLM inference: a small drafter proposes multiple tokens and a
large target model verifies them once per speculation length. Recently, scaling
of the LLM vocabulary has pushed the number of tokens to grow substantially.
While verification over the full vocabulary leaves the target model largely
unaffected, the O(|V|d) parameters in the drafter's output head become a
latency bottleneck, slowing the entire pipeline. Contemporary methods (e.g.,
FR-Spec, VocabTrim) restrict the drafter's vocabulary to a fixed subset of the
target model's vocabulary, ranked in descending order of token frequency.
Although this reduces draft-time compute, it is brittle, since: (i) frequency
lists are corpus-dependent and require retuning to generalize, and (ii) static
shortlists suppress rare or domain-specific tokens, lowering the expected
number of tokens per verification step. We propose DynaSpec, a
context-dependent dynamic shortlisting mechanism that is robust, speeds up
drafting, and generalizes across diverse tasks. Concretely, we introduce
lightweight, coarse-grained meta-classifiers that route contexts to a small
number of token clusters; the union of the top-k selected clusters forms the
drafter's shortlist, while verification retains the full vocabulary and
exactness. The meta-classifier finishes its computation earlier than the
drafter's hidden state generation by exploiting parallel execution of draft
encoding and meta shortlisting on separate streams. On standard
speculative-decoding benchmarks, we observe consistent gains in mean accepted
length over fixed-shortlist baselines, while context-dependent selection
enables smaller shortlists without degrading acceptance.

</details>


### [18] [MedTrust-RAG: Evidence Verification and Trust Alignment for Biomedical Question Answering](https://arxiv.org/abs/2510.14400)
*Yingpeng Ning,Yuanyuan Sun,Ling Luo,Yanhua Wang,Yuchen Pan,Hongfei Lin*

Main category: cs.CL

TL;DR: 提出MedTrust-Guided Iterative RAG框架，以增强医学问答的事实一致性并减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有的基于RAG的生物医学问答方法由于检索后的噪声和检索证据的验证不足而存在幻觉问题，从而降低了响应的可靠性。

Method: 引入三个关键创新：1. 强制执行 citation-aware reasoning，要求所有生成的内容都明确地基于检索到的医学文档；2. 采用迭代检索-验证过程，其中验证代理评估证据的充分性，并通过医学差距分析来优化查询，直到获得可靠的信息；3. 集成 MedTrust-Align Module (MTAM)，将验证后的积极示例与已知幻觉的消极样本相结合，利用直接偏好优化来加强基于引用的推理，同时惩罚容易产生幻觉的响应模式。

Result: 在MedMCQA、MedQA和MMLU-Med上的实验表明，该方法在多个模型架构上始终优于竞争基线，LLaMA3.1-8B-Instruct的平均准确率提高了2.7%，Qwen3-8B的平均准确率提高了2.4%。

Conclusion: MedTrust-Guided Iterative RAG 框架能够有效提高医学问答的事实一致性，并减少幻觉。

Abstract: Biomedical question answering (QA) requires accurate interpretation of
complex medical knowledge. Large language models (LLMs) have shown promising
capabilities in this domain, with retrieval-augmented generation (RAG) systems
enhancing performance by incorporating external medical literature. However,
RAG-based approaches in biomedical QA suffer from hallucinations due to
post-retrieval noise and insufficient verification of retrieved evidence,
undermining response reliability. We propose MedTrust-Guided Iterative RAG, a
framework designed to enhance factual consistency and mitigate hallucinations
in medical QA. Our method introduces three key innovations. First, it enforces
citation-aware reasoning by requiring all generated content to be explicitly
grounded in retrieved medical documents, with structured Negative Knowledge
Assertions used when evidence is insufficient. Second, it employs an iterative
retrieval-verification process, where a verification agent assesses evidence
adequacy and refines queries through Medical Gap Analysis until reliable
information is obtained. Third, it integrates the MedTrust-Align Module (MTAM)
that combines verified positive examples with hallucination-aware negative
samples, leveraging Direct Preference Optimization to reinforce
citation-grounded reasoning while penalizing hallucination-prone response
patterns. Experiments on MedMCQA, MedQA, and MMLU-Med demonstrate that our
approach consistently outperforms competitive baselines across multiple model
architectures, achieving the best average accuracy with gains of 2.7% for
LLaMA3.1-8B-Instruct and 2.4% for Qwen3-8B.

</details>


### [19] [On-device System of Compositional Multi-tasking in Large Language Models](https://arxiv.org/abs/2510.13848)
*Ondrej Bohdal,Konstantinos Theodosiadis,Asterios Mpatziakas,Dimitris Filippidis,Iro Spyrou,Christos Zonios,Anastasios Drosou,Dimosthenis Ioannidis,Kyeng-Hun Lee,Jijoong Moon,Hyeonmok Ko,Mete Ozay,Umberto Michieli*

Main category: cs.CL

TL;DR: 提出了一种新的方法，专门为涉及摘要和翻译的组合多任务场景定制。


<details>
  <summary>Details</summary>
Motivation: 标准方法在同时执行复杂任务（例如从长对话生成翻译摘要）时会遇到困难。

Method: 该技术涉及在组合的摘要和翻译适配器之上添加一个可学习的投影层。

Result: 实验结果表明，该解决方案在基于云和设备上的实现中都表现良好且速度很快。

Conclusion: 强调了在需要高速运行以及资源限制的实际应用中采用我们的框架的潜在好处。

Abstract: Large language models (LLMs) are commonly adapted for diverse downstream
tasks via parameter-efficient fine-tuning techniques such as Low-Rank Adapters
(LoRA). While adapters can be combined to handle multiple tasks separately,
standard approaches struggle when targeting the simultaneous execution of
complex tasks, such as generating a translated summary from a long
conversation. To address this challenge, we propose a novel approach tailored
specifically for compositional multi-tasking scenarios involving summarization
and translation. Our technique involves adding a learnable projection layer on
top of the combined summarization and translation adapters. This design enables
effective integration while maintaining efficiency through reduced
computational overhead compared to alternative strategies requiring extensive
retraining or sequential processing. We demonstrate the practical viability of
our method within an on-device environment by developing an Android app capable
of executing compositional tasks seamlessly. Experimental results indicate our
solution performs well and is fast in both cloud-based and on-device
implementations, highlighting the potential benefits of adopting our framework
in real-world applications demanding high-speed operation alongside resource
constraints.

</details>


### [20] [Language steering in latent space to mitigate unintended code-switching](https://arxiv.org/abs/2510.13849)
*Andrey Goncharov,Nikolai Kondusov,Alexey Zaytsev*

Main category: cs.CL

TL;DR: 提出了一种轻量级的推理时方法，通过在并行翻译上进行PCA识别语言方向，并沿这些轴引导token嵌入来控制语言身份，从而减轻代码切换。


<details>
  <summary>Details</summary>
Motivation: 多语言大型语言模型（LLMs）经常表现出无意的代码切换，降低了下游任务的可靠性。

Method: 通过在并行翻译上进行PCA识别语言方向，并沿这些轴引导token嵌入。

Result: 使用单个主成分实现了95-99％的语言分类准确率，并在Qwen2.5和Llama-3.2模型上将多个语言对的下一个token分布差异降低了高达42％。

Conclusion: 语言身份集中在具有近乎完美的线性可分性的最终层中。

Abstract: Multilingual Large Language Models (LLMs) often exhibit unintended
code-switching, reducing reliability in downstream tasks. We propose
latent-space language steering, a lightweight inference-time method that
identifies language directions via PCA on parallel translations and steers
token embeddings along these axes to control language identity. Our approach
mitigates code-switching while preserving semantics with negligible
computational overhead and requires only minimal parallel data for calibration.
Empirically, we achieve 95-99\% language classification accuracy using a single
principal component and reduce next-token distributional divergence by up to
42% across multiple language pairs on Qwen2.5 and Llama-3.2 models. We further
analyze the layer-wise evolution of language representations, revealing that
language identity concentrates in final layers with near-perfect linear
separability.

</details>


### [21] [Revisiting the UID Hypothesis in LLM Reasoning Traces](https://arxiv.org/abs/2510.13850)
*Minju Gwak,Guijin Son,Jaehyung Kim*

Main category: cs.CL

TL;DR: LLMs use Chain-of-Thought (CoT) reasoning, but the intermediate steps are often unfaithful. The paper introduces entropy-based metrics to analyze the information flow within reasoning traces.


<details>
  <summary>Details</summary>
Motivation: Inspired by the Uniform Information Density (UID) hypothesis, the paper investigates whether LLM reasoning follows a stable flow of information.

Method: The paper uses entropy-based metrics to analyze the information density within LLM reasoning traces across three mathematical benchmarks.

Result: Successful reasoning in LLMs is globally non-uniform, characterized by uneven swings in information density. This contrasts with human communication patterns.

Conclusion: The result challenges assumptions about machine reasoning and suggests new directions for designing interpretable and adaptive reasoning models.

Abstract: Large language models (LLMs) often solve problems using step-by-step
Chain-of-Thought (CoT) reasoning, yet these intermediate steps are frequently
unfaithful or hard to interpret. Inspired by the Uniform Information Density
(UID) hypothesis in psycholinguistics -- which posits that humans communicate
by maintaining a stable flow of information -- we introduce entropy-based
metrics to analyze the information flow within reasoning traces. Surprisingly,
across three challenging mathematical benchmarks, we find that successful
reasoning in LLMs is globally non-uniform: correct solutions are characterized
by uneven swings in information density, in stark contrast to human
communication patterns. This result challenges assumptions about machine
reasoning and suggests new directions for designing interpretable and adaptive
reasoning models.

</details>


### [22] [EvoEdit: Evolving Null-space Alignment for Robust and Efficient Knowledge Editing](https://arxiv.org/abs/2510.13851)
*Sicheng Lyu,Yu Gu,Xinyu Wang,Jerry Huang,Sitao Luan,Yufei Cui,Xiao-Wen Chang,Peng Lu*

Main category: cs.CL

TL;DR: EvoEdit通过连续零空间对齐减轻灾难性干扰，实现稳定高效的模型编辑。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）需要不断更新以纠正过时或错误的知识。模型编辑已经成为一种引人注目的范例，可以在不增加完全重新训练的计算负担的情况下引入有针对性的修改。现有的方法主要基于先定位再编辑的框架。然而，在顺序编辑环境中，当随着时间的推移应用多个更新时，它们表现出显着的局限性并且遭受灾难性干扰，即，新的编辑会损害先前集成的更新并降低保留的知识。

Method: 通过为每个传入的编辑执行连续零空间对齐，EvoEdit保留原始和先前修改的知识表示，并在保留的知识上保持输出不变性，即使在长编辑序列中也是如此，从而有效地减轻干扰。

Result: 在真实世界的顺序知识编辑基准上的评估表明，EvoEdit比先前的最先进的定位然后编辑技术实现了更好或相当的性能，速度提高了高达3.53倍。

Conclusion: 这些结果强调了在动态发展的信息环境中开发更 принципиальных 方法来设计 LLM 的必要性，同时提供了一个简单而有效的解决方案，并具有强大的理论保证。

Abstract: Large language models (LLMs) require continual updates to rectify outdated or
erroneous knowledge. Model editing has emerged as a compelling paradigm for
introducing targeted modifications without the computational burden of full
retraining. Existing approaches are mainly based on a locate-then-edit
framework. However, in sequential editing contexts, where multiple updates are
applied over time, they exhibit significant limitations and suffer from
catastrophic interference, i.e., new edits compromise previously integrated
updates and degrade preserved knowledge. To address these challenges, we
introduce EvoEdit, a novel editing strategy that mitigates catastrophic
interference through sequential null-space alignment, enabling stable and
efficient model editing. By performing sequential null-space alignment for each
incoming edit, EvoEdit preserves both original and previously modified
knowledge representations and maintains output invariance on preserved
knowledge even across long edit sequences, effectively mitigating interference.
Evaluations on real-world sequential knowledge-editing benchmarks show that
EvoEdit achieves better or comparable performance than prior state-of-the-art
locate-then-edit techniques, with up to 3.53 times speedup. Overall, these
results underscore the necessity of developing more principled approaches for
designing LLMs in dynamically evolving information settings, while providing a
simple yet effective solution with strong theoretical guarantees.

</details>


### [23] [Intent Clustering with Shared Pseudo-Labels](https://arxiv.org/abs/2510.14640)
*I-Fan Lin,Faegheh Hasibi,Suzan Verberne*

Main category: cs.CL

TL;DR: 本文提出了一种直观、免训练、免标签的意图聚类方法，该方法使用轻量级开源LLM，并且假设最少。该方法首先要求LLM为每个文本生成伪标签，然后在此伪标签集中为每个文本执行多标签分类。评估表明，该方法在保持简单和计算效率的同时，实现了与最新基线相当甚至更好的结果。


<details>
  <summary>Details</summary>
Motivation: 当前许多方法依赖于商业LLM，这些方法成本高昂且透明度有限。此外，他们的方法通常明确依赖于预先知道集群的数量，而在实际情况下通常并非如此。

Method: 该方法首先要求LLM为每个文本生成伪标签，然后在此伪标签集中为每个文本执行多标签分类。这种方法基于以下假设：属于同一集群的文本将共享更多标签，因此在编码为嵌入时会更接近。

Result: 在四个基准数据集上的评估表明，该方法实现了与最新基线相当甚至更好的结果，同时保持了简单和计算效率。

Conclusion: 该方法可以在低资源场景中应用，并且在多个模型和数据集中保持稳定。

Abstract: In this paper, we propose an intuitive, training-free and label-free method
for intent clustering that makes minimal assumptions using lightweight and
open-source LLMs. Many current approaches rely on commercial LLMs, which are
costly, and offer limited transparency. Additionally, their methods often
explicitly depend on knowing the number of clusters in advance, which is often
not the case in realistic settings. To address these challenges, instead of
asking the LLM to match similar text directly, we first ask it to generate
pseudo-labels for each text, and then perform multi-label classification in
this pseudo-label set for each text. This approach is based on the hypothesis
that texts belonging to the same cluster will share more labels, and will
therefore be closer when encoded into embeddings. These pseudo-labels are more
human-readable than direct similarity matches. Our evaluation on four benchmark
sets shows that our approach achieves results comparable to and better than
recent baselines, while remaining simple and computationally efficient. Our
findings indicate that our method can be applied in low-resource scenarios and
is stable across multiple models and datasets.

</details>


### [24] [ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When Responding to Different Demographic Groups](https://arxiv.org/abs/2510.13852)
*Peter Banyas,Shristi Sharma,Alistair Simmons,Atharva Vispute*

Main category: cs.CL

TL;DR: 本文介绍了一个名为 ConsistencyAI 的独立基准，用于衡量大型语言模型 (LLM) 针对不同角色的事实一致性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 可能会根据用户的不同人口统计特征给出不一致的事实回答。

Method: 通过向 19 个 LLM 提出包含 15 个主题的事实请求，并为每次查询添加来自不同角色的提示上下文，计算跨角色余弦相似度来评估事实一致性。

Result: 实验结果表明，LLM 的事实一致性因提供者和主题而异，Grok-3 一致性最高，而一些轻量级模型一致性最低。 job market 一致性最低，G7 world leaders 一致性最高。

Conclusion: 研究结果表明，提供者和主题都会影响事实一致性。

Abstract: Is an LLM telling you different facts than it's telling me? This paper
introduces ConsistencyAI, an independent benchmark for measuring the factual
consistency of large language models (LLMs) for different personas.
ConsistencyAI tests whether, when users of different demographics ask identical
questions, the model responds with factually inconsistent answers. Designed
without involvement from LLM providers, this benchmark offers impartial
evaluation and accountability. In our experiment, we queried 19 LLMs with
prompts that requested 5 facts for each of 15 topics. We repeated this query
100 times for each LLM, each time adding prompt context from a different
persona selected from a subset of personas modeling the general population. We
processed the responses into sentence embeddings, computed cross-persona cosine
similarity, and computed the weighted average of cross-persona cosine
similarity to calculate factual consistency scores. In 100-persona experiments,
scores ranged from 0.9065 to 0.7896, and the mean was 0.8656, which we adopt as
a benchmark threshold. xAI's Grok-3 is most consistent, while several
lightweight models rank lowest. Consistency varies by topic: the job market is
least consistent, G7 world leaders most consistent, and issues like vaccines or
the Israeli-Palestinian conflict diverge by provider. These results show that
both the provider and the topic shape the factual consistency. We release our
code and interactive demo to support reproducible evaluation and encourage
persona-invariant prompting strategies.

</details>


### [25] [An Efficient Rubric-based Generative Verifier for Search-Augmented LLMs](https://arxiv.org/abs/2510.14660)
*Linyue Ma,Yilong Xu,Xiang Long,Zhi Zheng*

Main category: cs.CL

TL;DR: 本文提出了一种名为“nugget-as-rubric”的统一且可验证的范例，它将原子信息点视为不同搜索增强工作负载的结构化评估标准，并设计了一个自动评分标准构建流程，以支持长格式设置。


<details>
  <summary>Details</summary>
Motivation: 现有搜索增强型LLM的奖励建模存在一些局限性。基于规则的奖励虽然可验证，但容易受到表达方式变化的影响，并且无法应用于长格式工作负载。相比之下，生成式奖励提高了鲁棒性，但为动态语料库中的长格式工作负载设计可验证且稳定的奖励仍然具有挑战性，并且还会产生高昂的计算成本。

Method: 本文提出了一种统一且可验证的范例“nugget-as-rubric”，它将原子信息点视为不同搜索增强工作负载的结构化评估标准。为了支持长格式设置，本文设计了一个基于查询重写的自动评分标准构建流程，该流程可以自动检索与每个问题相关的段落，并从中提取评分标准，包括来自静态语料库和动态在线Web内容。

Result: 实验结果表明，Search-Gen-V在不同的工作负载中实现了强大的验证准确性。

Conclusion: 本文提出的Search-Gen-V是一种可扩展、鲁棒且高效的可验证奖励构造器，适用于搜索增强型LLM。

Abstract: Search augmentation empowers Large Language Models with retrieval
capabilities to overcome the limitations imposed by static parameters.
Recently, Reinforcement Learning leverages tailored reward signals as a viable
technique to enhance LLMs performing tasks involving search. However, existing
reward modeling for search-augmented LLMs faces several limitations. Rule-based
rewards, such as Exact Match, are verifiable but fragile to variations in
expression and cannot be applied to long-form workloads. In contrast,
generative rewards improve robustness, but designing verifiable and stable
rewards for long-form workloads in dynamic corpora remains challenging and also
incurs high computational costs. In this paper, we propose a unified and
verifiable paradigm, "nugget-as-rubric", which treats atomic information points
as structured evaluation criteria for different search-augmentation workloads.
Short-form tasks correspond to a single rubric, whereas long-form tasks expand
to multiple rubrics aligned with the question's information needs. To support
long-form settings, we design an automatic rubric construction pipeline based
on query rewriting, which can automatically retrieve passages relevant to each
question and extract rubrics from them, both from static corpora and from
dynamic online web content. Furthermore, we introduce \textbf{Search-Gen-V}, a
4B-parameter efficient generative verifier under our proposed verifiable
paradigm, which is trained via the idea of distillation and a two-stage
strategy. Experimental results show that Search-Gen-V achieves strong
verification accuracy across different workloads, making it a scalable, robust,
and efficient verifiable reward constructor for search-augmented LLMs.

</details>


### [26] [R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging](https://arxiv.org/abs/2510.13854)
*Mamadou K. Keita,Christopher Homan,Sebastien Diarra*

Main category: cs.CL

TL;DR: 提出了一个混合方法，将语言规则的多层系统直接整合到神经网络的训练目标中。


<details>
  <summary>Details</summary>
Motivation: 该模型旨在处理词汇表外(OOV)的单词，并使用原则性不确定性。

Method: R2T框架，一个混合方法，它集成了一个多层语言规则系统到一个神经网络的训练目标中。该方法使用自适应损失函数，其中包含一个正则化项。

Result: 在Zarma词性标注(POS)任务中，R2T-BiLSTM模型仅在未标记的文本上训练，达到了98.2%的准确率，优于在300个标记句子上进行微调的AfriBERTa等基线。对于更复杂的任务，如命名实体识别(NER)，R2T可以作为一个强大的预训练步骤。

Conclusion: R2T作为一种强大的预训练方法，在少量标记数据上进行微调时，性能优于基线模型。

Abstract: We introduce the Rule-to-Tag (R2T) framework, a hybrid approach that
integrates a multi-tiered system of linguistic rules directly into a neural
network's training objective. R2T's novelty lies in its adaptive loss function,
which includes a regularization term that teaches the model to handle
out-of-vocabulary (OOV) words with principled uncertainty. We frame this work
as a case study in a paradigm we call principled learning (PrL), where models
are trained with explicit task constraints rather than on labeled examples
alone. Our experiments on Zarma part-of-speech (POS) tagging show that the
R2T-BiLSTM model, trained only on unlabeled text, achieves 98.2% accuracy,
outperforming baselines like AfriBERTa fine-tuned on 300 labeled sentences. We
further show that for more complex tasks like named entity recognition (NER),
R2T serves as a powerful pre-training step; a model pre-trained with R2T and
fine-tuned on just 50 labeled sentences outperformes a baseline trained on 300.

</details>


### [27] [Supervised Fine-Tuning or Contrastive Learning? Towards Better Multimodal LLM Reranking](https://arxiv.org/abs/2510.14824)
*Ziqi Dai,Xin Zhang,Mingxin Li,Yanzhao Zhang,Dingkun Long,Pengjun Xie,Meishan Zhang,Wenjie Li,Min Zhang*

Main category: cs.CL

TL;DR: 对比学习 (CL) 和监督微调 (SFT) 哪种更适合基于大型语言模型 (LLM) 的重排序。


<details>
  <summary>Details</summary>
Motivation: BERT 架构下，对比学习比判别学习更有效。但对于大型语言模型，监督微调更有效。要研究哪种目标函数更适合基于 LLM 的重排序，以及其潜在机制。

Method: 将目标函数分解为权重和方向两个部分，并提出了一个统一的框架来理解它们的相互作用。通过探究实验，比较 CL 和 SFT 在通用多模态检索 (UMR) 上的效果。

Result: SFT 提供了比 CL 更强的权重方案，而首选的评分方向没有明显的赢家。使用 SFT 进行大规模训练，并在 MRB 基准上提出了新的最先进的重排序器。

Conclusion: 对于 LLM 重排序，SFT 优于 CL。对 SFT 设置进行了消融研究，希望研究结果能够使未来的研究和应用受益。

Abstract: In information retrieval, training reranking models mainly focuses on two
types of objectives: metric learning (e.g. contrastive loss to increase the
predicted scores on relevant query-document pairs) and classification (binary
label prediction of relevance vs. irrelevance). For BERT-style encoders,
various studies have shown that contrastive learning (CL) can be more effective
than discriminative (classification) learning. However, for large language
models (LLMs), classification via supervised fine-tuning (SFT), which predicts
''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears
more promising as it aligns well with the generative nature of LLMs. This
divergence raises a central question: which objective is intrinsically better
suited to LLM-based reranking, and what mechanism underlies the difference? In
this work, we conduct a comprehensive comparison and analysis between CL and
SFT for reranking, taking the universal multimodal retrieval (UMR) as the
experimental playground. We first decompose the objectives into two components:
weight, which controls the magnitude of those updates, and direction, which
guides the model updates, then present a unified framework for understanding
their interactions. Through probing experiments, we find that SFT provides a
substantially stronger weighting scheme than CL, whereas the preferred scoring
direction shows no clear winner. Taken together, these results point to a
consistent advantage of SFT over CL for LLM reranking. To further validate our
findings, we conduct large-scale training with SFT and present new
state-of-the-art rerankers on the MRB benchmark. We also provide ablations on
SFT settings and expect our findings to benefit future research and
applications in this area.

</details>


### [28] [Harnessing Consistency for Robust Test-Time LLM Ensemble](https://arxiv.org/abs/2510.13855)
*Zhichen Zeng,Qi Yu,Xiao Lin,Ruizhong Qiu,Xuying Ning,Tianxin Wei,Yuchen Yan,Jingrui He,Hanghang Tong*

Main category: cs.CL

TL;DR: 提出了一种名为CoRE的即插即用技术，利用模型一致性来实现稳健的LLM集成，可以与不同的集成方法无缝集成。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(llm)表现出不同的优点和缺点，而LLM集成是一种很有前途的方法，可以整合它们互补的能力。尽管在提高集成质量方面取得了显著进展，但对集成在潜在错误信号方面的鲁棒性的关注有限，这些错误信号通常来自不同的标记化方案和不同的模型专业知识。

Method: Token-level一致性通过应用低通滤波器来降低具有高度不一致性的不确定token的权重来捕获细粒度的不一致，这通常是由于token未对齐造成的，从而在粒度级别上提高鲁棒性。模型级一致性通过提升具有高自我置信度和与其他模型发散最小的模型输出来建模全局一致性，从而在更粗糙的级别上增强鲁棒性。

Result: 在不同的基准、模型组合和集成策略上的大量实验表明，CoRE持续提高集成性能和鲁棒性。

Conclusion: 分析表明，集成失败通常来自token级别和模型级别:前者反映了token预测中的严重分歧，而后者涉及低置信度和模型之间的显著差异。

Abstract: Different large language models (LLMs) exhibit diverse strengths and
weaknesses, and LLM ensemble serves as a promising approach to integrate their
complementary capabilities. Despite substantial progress in improving ensemble
quality, limited attention has been paid to the robustness of ensembles against
potential erroneous signals, which often arise from heterogeneous tokenization
schemes and varying model expertise. Our analysis shows that ensemble failures
typically arise from both the token level and the model level: the former
reflects severe disagreement in token predictions, while the latter involves
low confidence and pronounced disparities among models. In light of this, we
propose CoRE, a plug-and-play technique that harnesses model consistency for
robust LLM ensemble, which can be seamlessly integrated with diverse ensemble
methods. Token-level consistency captures fine-grained disagreements by
applying a low-pass filter to downweight uncertain tokens with high
inconsistency, often due to token misalignment, thereby improving robustness at
a granular level. Model-level consistency models global agreement by promoting
model outputs with high self-confidence and minimal divergence from others,
enhancing robustness at a coarser level. Extensive experiments across diverse
benchmarks, model combinations, and ensemble strategies demonstrate that CoRE
consistently improves ensemble performance and robustness.

</details>


### [29] [Multimodal Retrieval-Augmented Generation with Large Language Models for Medical VQA](https://arxiv.org/abs/2510.13856)
*A H M Rezaul Karim,Ozlem Uzuner*

Main category: cs.CL

TL;DR: 本研究介绍了一种名为MasonNLP的医学视觉问答（MedVQA）系统，该系统在MEDIQA-WV 2025的创伤护理VQA共享任务中排名第三。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决医学视觉问答（MedVQA）问题，通过医学图像的自然语言查询来支持临床决策和患者护理。

Method: MasonNLP系统采用了一个通用领域的、指令调整的大型语言模型，该模型具有检索增强生成（RAG）框架，该框架结合了领域内数据的文本和视觉示例。

Result: MasonNLP系统在MEDIQA-WV 2025共享任务中排名第三，平均得分41.37%。

Conclusion: 轻量级RAG与通用LLM相结合，为多模态临床NLP任务提供了一个简单有效的基线。

Abstract: Medical Visual Question Answering (MedVQA) enables natural language queries
over medical images to support clinical decision-making and patient care. The
MEDIQA-WV 2025 shared task addressed wound-care VQA, requiring systems to
generate free-text responses and structured wound attributes from images and
patient queries. We present the MasonNLP system, which employs a
general-domain, instruction-tuned large language model with a
retrieval-augmented generation (RAG) framework that incorporates textual and
visual examples from in-domain data. This approach grounds outputs in
clinically relevant exemplars, improving reasoning, schema adherence, and
response quality across dBLEU, ROUGE, BERTScore, and LLM-based metrics. Our
best-performing system ranked 3rd among 19 teams and 51 submissions with an
average score of 41.37%, demonstrating that lightweight RAG with
general-purpose LLMs -- a minimal inference-time layer that adds a few relevant
exemplars via simple indexing and fusion, with no extra training or complex
re-ranking -- provides a simple and effective baseline for multimodal clinical
NLP tasks.

</details>


### [30] [ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP Architecture and Paired Weight Sharing](https://arxiv.org/abs/2510.13860)
*Shivanshu Kumar,Gopalakrishnan Srinivasan*

Main category: cs.CL

TL;DR: 提出了一种名为 ShishuLM 的高效语言模型架构，旨在减少参数数量和 Key-Value (KV) 缓存需求。


<details>
  <summary>Details</summary>
Motivation: Transformer 模型在自然语言处理任务中表现出色，但存在内存和计算开销大的问题。研究表明这些模型中存在架构冗余，因此可以在不影响性能的情况下进行优化。

Method: 通过研究 AI 可解释性和推理时层剪枝，并结合对中小规模语言模型 (SLM) 的分析，发现对于中等上下文场景，归一化和注意力计算与输入大致呈线性关系，从而可以使用多层感知机 (MLP) 来近似整个 Transformer 块。

Result: ShishuLM 与原始模型相比，内存需求减少高达 25%，训练和推理期间的延迟提高高达 40%。

Conclusion: 实验和分析结果为从预训练的角度构建更高效的 SLM 架构提供了见解。

Abstract: While the transformer architecture has achieved state-of-the-art performance
on natural language processing tasks, these models impose substantial memory
and computational overhead. Recent research has identified significant
architectural redundancies within these models, presenting opportunities for
optimization without compromising performance. Taking insights from research in
AI interpretability and inference-time layer pruning, we introduce an efficient
language model architecture, referred to as ShishuLM, which reduces both the
parameter count and Key-Value (KV) cache requirements. Given the increasing
importance of Small Language Models (SLMs) in agentic AI systems, we evaluate
our approach on two SLMs of different scales. Our analysis reveals that for
moderate-context scenarios, normalization coupled with attention computation is
roughly linear with the input, enabling entire transformer blocks to be
approximated through Multi-Layer Perceptrons (MLPs). Our results show that
ShishuLM provides up to 25% reduction in memory requirements and up to 40%
improvement in latency during both training and inference, compared to parent
models. Our experimental and analytical findings provide insights towards
building more efficient SLM architectures from a pre-training standpoint.

</details>


### [31] [Ensembling Large Language Models to Characterize Affective Dynamics in Student-AI Tutor Dialogues](https://arxiv.org/abs/2510.13862)
*Chenyu Zhang,Sharifa Alghowinem,Cynthia Breazeal*

Main category: cs.CL

TL;DR: 该研究调查了大型语言模型(LLM)辅导中学生的情感动态。


<details>
  <summary>Details</summary>
Motivation: 目前对LLM介导的辅导中情感动态的理解不足。本研究旨在通过关注学习者的情感状态，为将生成式人工智能整合到教育中的负责任途径提供参考。

Method: 通过集成多个LLM框架，对PyTutor（一个LLM驱动的AI辅导系统）与261名本科生之间的16986个对话轮次进行情感分析。使用Gemini、GPT-4o和Claude三个LLM生成情感注释，并通过模型内排序加权池化和跨模型多数共识融合这些估计，以生成稳健的情感概况。

Result: 学生在使用AI辅导时通常表现出轻微的积极情感和适度的兴奋度。困惑和好奇是解决问题的常见伴随物，沮丧虽然较少见，但仍然会以可能破坏进展的方式出现。积极时刻比中性或消极时刻持续时间稍长，但它们是脆弱且容易被打断的。消极情绪通常会迅速消退，有时会直接反弹到积极状态。中性时刻经常充当转折点，更频繁地引导学生向上而非向下。

Conclusion: 研究结果表明，辅导员可以在中性时刻进行干预，以提升学生的学习体验。

Abstract: While recent studies have examined the leaning impact of large language model
(LLM) in educational contexts, the affective dynamics of LLM-mediated tutoring
remain insufficiently understood. This work introduces the first ensemble-LLM
framework for large-scale affect sensing in tutoring dialogues, advancing the
conversation on responsible pathways for integrating generative AI into
education by attending to learners' evolving affective states. To achieve this,
we analyzed two semesters' worth of 16,986 conversational turns exchanged
between PyTutor, an LLM-powered AI tutor, and 261 undergraduate learners across
three U.S. institutions. To investigate learners' emotional experiences, we
generate zero-shot affect annotations from three frontier LLMs (Gemini, GPT-4o,
Claude), including scalar ratings of valence, arousal, and
learning-helpfulness, along with free-text emotion labels. These estimates are
fused through rank-weighted intra-model pooling and plurality consensus across
models to produce robust emotion profiles. Our analysis shows that during
interaction with the AI tutor, students typically report mildly positive affect
and moderate arousal. Yet learning is not uniformly smooth: confusion and
curiosity are frequent companions to problem solving, and frustration, while
less common, still surfaces in ways that can derail progress. Emotional states
are short-lived--positive moments last slightly longer than neutral or negative
ones, but they are fragile and easily disrupted. Encouragingly, negative
emotions often resolve quickly, sometimes rebounding directly into positive
states. Neutral moments frequently act as turning points, more often steering
students upward than downward, suggesting opportunities for tutors to intervene
at precisely these junctures.

</details>


### [32] [Unlocking the Potential of Diffusion Language Models through Template Infilling](https://arxiv.org/abs/2510.13870)
*Junhoo Lee,Seungyeon Kim,Nojun Kwak*

Main category: cs.CL

TL;DR: 提出了一种新的扩散语言模型（DLM）条件调节方法，称为模板填充（TI），以改进其生成过程。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散语言模型推理策略仍局限于自回归范式中的前缀提示。

Method: TI 首先生成目标响应的结构模板，然后填充被屏蔽的片段。引入动态段分配（DSA）以增强结构控制的灵活性，DSA 基于生成置信度自适应地调整段长度。

Result: 在数学推理和代码生成基准测试中，TI 相比基线实现了 17.01% 的一致性改进。在多 token 生成设置中，TI 提供了额外的优势，实现了有效的加速，同时保持了生成质量。

Conclusion: TI 方法有效地提升了扩散语言模型在数学推理和代码生成任务上的性能，并在多token生成方面具有优势。

Abstract: Diffusion Language Models (DLMs) have emerged as a promising alternative to
Autoregressive Language Models, yet their inference strategies remain limited
to prefix-based prompting inherited from the autoregressive paradigm. In this
paper, we propose Template Infilling (TI), a tailored conditioning methodology
for DLMs' generation process. Unlike conventional prefix prompting, TI first
generates a structural template for the target response, then fills in the
masked segments. To enhance the flexibility of this structural control, we
introduce Dynamic Segment Allocation (DSA), which adaptively adjusts segment
lengths based on generation confidence. We demonstrate the effectiveness of our
approach on mathematical reasoning and code generation benchmarks, achieving
consistent improvements of 17.01$\%$p over baseline. Furthermore, we show that
TI provides additional advantages in multi-token generation settings, enabling
effective speedup while maintaining generation quality.

</details>


### [33] [Quechua Speech Datasets in Common Voice: The Case of Puno Quechua](https://arxiv.org/abs/2510.13871)
*Elwin Huaman,Wendi Huaman,Jorge Luis Huaman,Ninfa Quispe*

Main category: cs.CL

TL;DR: Common Voice is used to create a speech dataset for under-resourced languages like Quechua.


<details>
  <summary>Details</summary>
Motivation: Under-resourced languages lack data for speech technology development.

Method: Integration of Quechua languages into Common Voice, focusing on Puno Quechua as a case study.

Result: Common Voice hosts 191.1 hours of Quechua speech, with Puno Quechua contributing 12 hours.

Conclusion: The study proposes a research agenda addressing technical challenges and ethical considerations for community engagement and indigenous data sovereignty, contributing to inclusive voice technology.

Abstract: Under-resourced languages, such as Quechuas, face data and resource scarcity,
hindering their development in speech technology. To address this issue, Common
Voice presents a crucial opportunity to foster an open and community-driven
speech dataset creation. This paper examines the integration of Quechua
languages into Common Voice. We detail the current 17 Quechua languages,
presenting Puno Quechua (ISO 639-3: qxp) as a focused case study that includes
language onboarding and corpus collection of both reading and spontaneous
speech data. Our results demonstrate that Common Voice now hosts 191.1 hours of
Quechua speech (86\% validated), with Puno Quechua contributing 12 hours (77\%
validated), highlighting the Common Voice's potential. We further propose a
research agenda addressing technical challenges, alongside ethical
considerations for community engagement and indigenous data sovereignty. Our
work contributes towards inclusive voice technology and digital empowerment of
under-resourced language communities.

</details>


### [34] [FRACCO: A gold-standard annotated corpus of oncological entities with ICD-O-3.1 normalisation](https://arxiv.org/abs/2510.13873)
*Johann Pignat,Milena Vucetic,Christophe Gaudet-Blavignac,Jamil Zaghir,Amandine Stettler,Fanny Amrein,Jonatan Bonjour,Jean-Philippe Goldman,Olivier Michielin,Christian Lovis,Mina Bjelogrlic*

Main category: cs.CL

TL;DR: FRACCO: A French annotated corpus for clinical oncology, translated from Spanish, with morphology, topography, and histology annotations using ICD-O.


<details>
  <summary>Details</summary>
Motivation: Scarce French oncology resources necessitate annotated datasets for NLP tools in clinical text.

Method: Expert annotation of 1301 synthetic French clinical cases, translated from the Spanish CANTEMIST corpus.  Includes automated matching and manual validation by a team of five annotators.

Result: 71127 ICD-O normalisations were produced, representing 399 unique morphology codes, 272 topography codes, and 2043 unique composite expressions.

Conclusion: FRACCO provides a reference standard for named entity recognition and concept normalisation in French oncology texts.

Abstract: Developing natural language processing tools for clinical text requires
annotated datasets, yet French oncology resources remain scarce. We present
FRACCO (FRench Annotated Corpus for Clinical Oncology) an expert-annotated
corpus of 1301 synthetic French clinical cases, initially translated from the
Spanish CANTEMIST corpus as part of the FRASIMED initiative. Each document is
annotated with terms related to morphology, topography, and histologic
differentiation, using the International Classification of Diseases for
Oncology (ICD-O) as reference. An additional annotation layer captures
composite expression-level normalisations that combine multiple ICD-O elements
into unified clinical concepts. Annotation quality was ensured through expert
review: 1301 texts were manually annotated for entity spans by two domain
experts. A total of 71127 ICD-O normalisations were produced through a
combination of automated matching and manual validation by a team of five
annotators. The final dataset representing 399 unique morphology codes (from
2549 different expressions), 272 topography codes (from 3143 different
expressions), and 2043 unique composite expressions (from 11144 different
expressions). This dataset provides a reference standard for named entity
recognition and concept normalisation in French oncology texts.

</details>


### [35] [What Layers When: Learning to Skip Compute in LLMs with Residual Gates](https://arxiv.org/abs/2510.13876)
*Filipe Laitenberger,Dawid Kopiczko,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CL

TL;DR: GateSkip: 允许在仅解码器 LM 中进行令牌层跳过的残差流门控机制，在长格式推理中节省高达 15% 的计算量，同时保持超过 90% 的基线精度，并且在指令调整模型中，在完全计算下看到准确率的提高，并在接近 50% 的节省时匹配基线质量。


<details>
  <summary>Details</summary>
Motivation: 为了提高仅解码器 LM 的效率，并解决早期退出或基于路由器的深度混合模型不稳定且需要大量再训练的问题。

Method: 提出了一种简单的残差流门控机制 GateSkip，它配备了一个 sigmoid 线性门，用于压缩分支的输出，然后再重新进入残差流。在推理过程中，我们按门值对令牌进行排序，并使用每层预算跳过低重要性的令牌。

Result: 在长格式推理中，节省高达 15% 的计算量，同时保持超过 90% 的基线精度。在指令调整模型中，在完全计算下看到准确率的提高，并在接近 50% 的节省时匹配基线质量。

Conclusion: GateSkip 是一种稳定且有效的方法，可以提高仅解码器 LM 的效率，并且可以轻松地与量化、剪枝和自推测解码相结合。学习到的门控可以深入了解 Transformer 信息流（例如，BOS 令牌充当锚点）。

Abstract: We introduce GateSkip, a simple residual-stream gating mechanism that enables
token-wise layer skipping in decoder-only LMs. Each Attention/MLP branch is
equipped with a sigmoid-linear gate that condenses the branch's output before
it re-enters the residual stream. During inference we rank tokens by the gate
values and skip low-importance ones using a per-layer budget. While early-exit
or router-based Mixture-of-Depths models are known to be unstable and need
extensive retraining, our smooth, differentiable gates fine-tune stably on top
of pretrained models. On long-form reasoning, we save up to 15\% compute while
retaining over 90\% of baseline accuracy. On instruction-tuned models we see
accuracy gains at full compute and match baseline quality near 50\% savings.
The learned gates give insight into transformer information flow (e.g., BOS
tokens act as anchors), and the method combines easily with quantization,
pruning, and self-speculative decoding.

</details>


### [36] [TextBandit: Evaluating Probabilistic Reasoning in LLMs Through Language-Only Decision Tasks](https://arxiv.org/abs/2510.13878)
*Jimin Lim,Arjun Damerla,Arthur Jiang,Nam Le*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在不确定性条件下进行序列决策的能力，通过纯文本反馈与多臂老虎机环境互动。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注大型语言模型仅使用自然语言在不确定性条件下进行序列决策的能力。

Method: 构建了一个新的基准，其中大型语言模型通过纯文本反馈与多臂老虎机环境互动， बिना访问数值线索或明确的概率。

Result: Qwen3-4B 模型的最佳臂选择率达到 89.2%，显著优于其他大型语言模型和传统方法。

Conclusion: 概率推理能够仅从语言中产生，并将此基准作为评估自然、非数字环境中决策能力的一步。

Abstract: Large language models (LLMs) have shown to be increasingly capable of
performing reasoning tasks, but their ability to make sequential decisions
under uncertainty only using natural language remains underexplored. We
introduce a novel benchmark in which LLMs interact with multi-armed bandit
environments using purely textual feedback, "you earned a token", without
access to numerical cues or explicit probabilities, resulting in the model to
infer latent reward structures purely off linguistic cues and to adapt
accordingly. We evaluated the performance of four open-source LLMs and compare
their performance to standard decision-making algorithms such as Thompson
Sampling, Epsilon Greedy, Upper Confidence Bound (UCB), and random choice.
While most of the LLMs underperformed compared to the baselines, Qwen3-4B,
achieved the best-arm selection rate of 89.2% , which significantly
outperformed both the larger LLMs and traditional methods. Our findings suggest
that probabilistic reasoning is able to emerge from language alone, and we
present this benchmark as a step towards evaluating decision-making
capabilities in naturalistic, non-numeric contexts.

</details>


### [37] [Catch Your Breath: Adaptive Computation for Self-Paced Sequence Production](https://arxiv.org/abs/2510.13879)
*Alexandre Galashov,Matt Jones,Rosemary Ke,Yuan Cao,Vaishnavh Nagarajan,Michael C. Mozer*

Main category: cs.CL

TL;DR: 这篇论文提出了一种新的监督训练目标，允许语言模型动态地调整每个输入token的计算步骤数量。


<details>
  <summary>Details</summary>
Motivation: 提高语言模型在处理不同复杂度的token时的效率和准确性。

Method: 该方法将输出token的选择视为一个序列决策问题，并引入了三种损失函数：CYB-AP、CYB-VA和CYB-DP。

Result: 实验表明，CYB模型只需baseline模型三分之一的训练数据即可达到相同的性能，并且能够根据token的复杂度和上下文调整处理时间。

Conclusion: CYB模型能够有效地利用额外的计算步骤来提高准确性，并适应不同token的处理需求。

Abstract: We explore a class of supervised training objectives that allow a language
model to dynamically and autonomously scale the number of compute steps used
for each input token. For any token, the model can request additional compute
steps by emitting a <don't know> output. If the model is granted a delay, a
specialized <pause> token is inserted at the next input step, providing the
model with additional compute resources to generate an output. The model can
request multiple pauses. To train the model to use <don't know> outputs
judiciously and to calibrate its uncertainty, we frame the selection of each
output token as a sequential-decision problem with a time cost. We refer to the
class of methods as $\textit{Catch Your Breath}$ losses and we study three
methods in this class: CYB-AP frames the model's task as anytime prediction,
where an output may be required at any step and accuracy is discounted over
time; CYB-VA is a variational approach that aims to maximize prediction
accuracy subject to a specified distribution over stopping times; and CYB-DP
imposes a penalty based on a computational budget. Through fine-tuning
experiments, we identify the best performing loss variant. The CYB model needs
only one third as much training data as the baseline (no pause) model needs to
achieve the same performance, and half as much data as a model with pauses and
a cross-entropy loss. We find that the CYB model requests additional steps when
doing so improves accuracy, and the model adapts its processing time to
token-level complexity and context. For example, it often pauses after plural
nouns like $\textit{patients}$ and $\textit{challenges}$ but never pauses after
the first token of contracted words like $\textit{wasn}$ and $\textit{didn}$,
and it shows high variability for ambiguous tokens like $\textit{won}$, which
could function as either a verb or part of a contraction.

</details>


### [38] [PAGE: Prompt Augmentation for text Generation Enhancement](https://arxiv.org/abs/2510.13880)
*Mauro Jose Pacchiotti,Luciana Ballejos,Mariel Ale*

Main category: cs.CL

TL;DR: 提出了一种名为PAGE的框架，通过简单的辅助模块来提升文本生成模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自然语言生成模型在特定任务或需求下表现不佳，需要大量额外数据进行调整。

Method: 利用轻量级模型（如分类器或提取器）从输入文本中提取信息，构建富化的输入，以改善生成质量和可控性。

Result: 在需求工程领域进行了概念验证，使用带有分类器的辅助模块来提高软件需求生成的质量。

Conclusion: PAGE框架不需要辅助生成模型，而是采用更简单、模块化的架构，易于适应不同的任务。

Abstract: In recent years, natural language generative models have shown outstanding
performance in text generation tasks. However, when facing specific tasks or
particular requirements, they may exhibit poor performance or require
adjustments that demand large amounts of additional data. This work introduces
PAGE (Prompt Augmentation for text Generation Enhancement), a framework
designed to assist these models through the use of simple auxiliary modules.
These modules, lightweight models such as classifiers or extractors, provide
inferences from the input text. The output of these auxiliaries is then used to
construct an enriched input that improves the quality and controllability of
the generation. Unlike other generation-assistance approaches, PAGE does not
require auxiliary generative models; instead, it proposes a simpler, modular
architecture that is easy to adapt to different tasks. This paper presents the
proposal, its components and architecture, and reports a proof of concept in
the domain of requirements engineering, where an auxiliary module with a
classifier is used to improve the quality of software requirements generation.

</details>


### [39] [Too Open for Opinion? Embracing Open-Endedness in Large Language Models for Social Simulation](https://arxiv.org/abs/2510.13884)
*Bolei Ma,Yong Cao,Indira Sen,Anna-Carolina Haensch,Frauke Kreuter,Barbara Plank,Daniel Hershcovich*

Main category: cs.CL

TL;DR: 这篇论文认为，为了更真实地模拟社会现象，大型语言模型（LLM）的模拟应该采用开放式的自由文本生成，而不是局限于多项选择或简答题。


<details>
  <summary>Details</summary>
Motivation: 当前研究为了方便评分和比较，限制了LLM的生成能力。开放式设计可以提高测量和设计水平，支持探索未预料的观点，并减少研究者引入的指导性偏差。

Method: 论文借鉴了数十年的调查方法研究和自然语言处理（NLP）的最新进展。

Result: 开放式设计能够捕捉表达性和个体性，有助于预测试，并最终提高方法论的效用。

Conclusion: 论文呼吁采用新的实践和评估框架，利用LLM的开放式生成多样性，从而在自然语言处理和社会科学之间建立协同效应。

Abstract: Large Language Models (LLMs) are increasingly used to simulate public opinion
and other social phenomena. Most current studies constrain these simulations to
multiple-choice or short-answer formats for ease of scoring and comparison, but
such closed designs overlook the inherently generative nature of LLMs. In this
position paper, we argue that open-endedness, using free-form text that
captures topics, viewpoints, and reasoning processes "in" LLMs, is essential
for realistic social simulation. Drawing on decades of survey-methodology
research and recent advances in NLP, we argue why this open-endedness is
valuable in LLM social simulations, showing how it can improve measurement and
design, support exploration of unanticipated views, and reduce
researcher-imposed directive bias. It also captures expressiveness and
individuality, aids in pretesting, and ultimately enhances methodological
utility. We call for novel practices and evaluation frameworks that leverage
rather than constrain the open-ended generative diversity of LLMs, creating
synergies between NLP and social science.

</details>


### [40] [Order from Chaos: Comparative Study of Ten Leading LLMs on Unstructured Data Categorization](https://arxiv.org/abs/2510.13885)
*Ariel Kamen*

Main category: cs.CL

TL;DR: 评估了大型语言模型在非结构化文本分类中的性能，发现模型在准确率等方面表现一般，但集成方法能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在非结构化文本分类中的应用，了解其性能瓶颈。

Method: 使用统一数据集和零样本提示，比较了十个最先进的LLM在IAB 2.2层级分类法上的表现，采用准确率、精确率、召回率、F1-score以及幻觉率、膨胀率和分类成本等指标。

Result: 大型语言模型的经典性能表现一般，平均准确率为34%。幻觉率和膨胀率表明模型经常过度生成类别。集成方法显著提高了准确率，减少了膨胀，并完全消除了幻觉。

Conclusion: 扩展模型规模和架构改进并不能确保更好的分类准确性。模型协同编排可能是在大规模文本分类中实现或超越人类专家性能的最有效途径。

Abstract: This study presents a comparative evaluation of ten state-of-the-art large
language models (LLMs) applied to unstructured text categorization using the
Interactive Advertising Bureau (IAB) 2.2 hierarchical taxonomy. The analysis
employed a uniform dataset of 8,660 human-annotated samples and identical
zero-shot prompts to ensure methodological consistency across all models.
Evaluation metrics included four classic measures - accuracy, precision,
recall, and F1-score - and three LLM-specific indicators: hallucination ratio,
inflation ratio, and categorization cost.
  Results show that, despite their rapid advancement, contemporary LLMs achieve
only moderate classic performance, with average scores of 34% accuracy, 42%
precision, 45% recall, and 41% F1-score. Hallucination and inflation ratios
reveal that models frequently overproduce categories relative to human
annotators. Among the evaluated systems, Gemini 1.5/2.0 Flash and GPT 20B/120B
offered the most favorable cost-to-performance balance, while GPT 120B
demonstrated the lowest hallucination ratio. The findings suggest that scaling
and architectural improvements alone do not ensure better categorization
accuracy, as the task requires compressing rich unstructured text into a
limited taxonomy - a process that challenges current model architectures.
  To address these limitations, a separate ensemble-based approach was
developed and tested. The ensemble method, in which multiple LLMs act as
independent experts, substantially improved accuracy, reduced inflation, and
completely eliminated hallucinations. These results indicate that coordinated
orchestration of models - rather than sheer scale - may represent the most
effective path toward achieving or surpassing human-expert performance in
large-scale text categorization.

</details>


### [41] [Reliable Fine-Grained Evaluation of Natural Language Math Proofs](https://arxiv.org/abs/2510.13888)
*Wenjie Ma,Andrei Cojocaru,Neel Kolhe,Bradley Louie,Robin Said Sharif,Haihan Zhang,Vincent Zhuang,Matei Zaharia,Sewon Min*

Main category: cs.CL

TL;DR: This paper introduces ProofBench, a dataset for evaluating LLM-generated math proofs, and ProofGrader, an evaluator that achieves high accuracy in scoring these proofs.


<details>
  <summary>Details</summary>
Motivation: The absence of a reliable evaluator for LLM-generated math proofs is a critical gap.

Method: The authors propose a systematic methodology for developing and validating evaluators and introduce ProofBench, an expert-annotated dataset of fine-grained proof ratings. They explore the evaluator design space and develop ProofGrader.

Result: ProofGrader achieves a low Mean Absolute Error (MAE) of 0.926 against expert scores and demonstrates practical utility in a best-of-$n$ selection task.

Conclusion: ProofGrader has the potential to advance downstream proof generation.

Abstract: Recent advances in large language models (LLMs) for mathematical reasoning
have largely focused on tasks with easily verifiable final answers; however,
generating and verifying natural language math proofs remains an open
challenge. We identify the absence of a reliable, fine-grained evaluator for
LLM-generated math proofs as a critical gap. To address this, we propose a
systematic methodology for developing and validating evaluators that assign
fine-grained scores on a 0-7 scale to model-generated math proofs. To enable
this study, we introduce ProofBench, the first expert-annotated dataset of
fine-grained proof ratings, spanning 145 problems from six major math
competitions (USAMO, IMO, Putnam, etc) and 435 LLM-generated solutions from
Gemini-2.5-pro, o3, and DeepSeek-R1. %with expert gradings. Using ProofBench as
a testbed, we systematically explore the evaluator design space across key
axes: the backbone model, input context, instructions and evaluation workflow.
Our analysis delivers ProofGrader, an evaluator that combines a strong
reasoning backbone LM, rich context from reference solutions and marking
schemes, and a simple ensembling method; it achieves a low Mean Absolute Error
(MAE) of 0.926 against expert scores, significantly outperforming naive
baselines. Finally, we demonstrate its practical utility in a best-of-$n$
selection task: at $n=16$, ProofGrader achieves an average score of 4.14 (out
of 7), closing 78% of the gap between a naive binary evaluator (2.48) and the
human oracle (4.62), highlighting its potential to advance downstream proof
generation.

</details>


### [42] [A Survey on Collaborating Small and Large Language Models for Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness](https://arxiv.org/abs/2510.13890)
*Fali Wang,Jihai Chen,Shuhua Yang,Ali Al-Lawati,Linli Tang,Hui Liu,Suhang Wang*

Main category: cs.CL

TL;DR: 本文综述了小型语言模型 (SLM) 和大型语言模型 (LLM) 的协同框架，旨在结合 SLM 的专业化和效率与 LLM 的泛化和推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型面临着微调成本高、推理延迟、边缘部署受限和可靠性问题。小型语言模型具有紧凑、高效和适应性强的优点，可以作为补充。

Method: 本文对 SLM-LLM 协作进行了系统性综述，并按协作目标进行组织。提出了一个包含四个目标的分类法：性能增强、成本效益、云边隐私和可信度。

Result: 本文回顾了代表性方法，总结了设计范式，并概述了在高效、安全和可扩展的 SLM-LLM 协作方面面临的开放挑战和未来方向。

Conclusion: SLM 和 LLM 的协同框架可以结合两者的优势，从而满足跨任务和部署场景的多样化目标。

Abstract: Large language models (LLMs) have advanced many domains and applications but
face high fine-tuning costs, inference latency, limited edge deployability, and
reliability concerns. Small language models (SLMs), compact, efficient, and
adaptable, offer complementary remedies. Recent work explores collaborative
frameworks that fuse SLMs' specialization and efficiency with LLMs'
generalization and reasoning to meet diverse objectives across tasks and
deployment scenarios. Motivated by these developments, this paper presents a
systematic survey of SLM-LLM collaboration organized by collaboration
objectives. We propose a taxonomy with four goals: performance enhancement,
cost-effectiveness, cloud-edge privacy, and trustworthiness. Within this
framework, we review representative methods, summarize design paradigms, and
outline open challenges and future directions toward efficient, secure, and
scalable SLM-LLM collaboration.

</details>


### [43] [The Harder The Better: Maintaining Supervised Fine-tuning Generalization with Less but Harder Data](https://arxiv.org/abs/2510.13892)
*Zhaoyang Shang,Sibo Wei,Jianbin Guo,Rui Zhou,Lifeng Dong,Yin Luo*

Main category: cs.CL

TL;DR: 本研究提出了一种名为THTB的框架，用于指令数据选择和注释指导，其灵感来自认知科学，通过结合质量过滤与内在和外在难度评分来优先考虑更高层次的认知指令。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度依赖LLM的内部知识，可解释性弱，泛化能力有限。

Method: 该方法结合质量过滤与内在和外在难度评分，为高效的SFT提供可解释和可量化的标准。

Result: 实验表明，使用5%的数据训练的模型优于使用完整数据集训练的模型，并且与仅使用LLM选择相比，实现了更好的泛化。此外，THTB在垂直领域提供了有效的注释指导，使得仅使用2%的数据训练的模型超过了在更大的数据集上训练的模型。

Conclusion: THTB在数据选择和注释指导方面都具有强大的领域适应潜力。

Abstract: Large Language Models (LLMs) excel in general tasks, but adapting them to
specialized domains relies on high-quality supervised fine-tuning (SFT) data.
Although existing methods can identify subsets of high-quality data and reduce
training cost to some extent, their selection process still suffers from
over-reliance on LLMs' internal knowledge, weak interpretability, and limited
generalization. To address these limitations, we propose THTB (The Harder The
Better), a cognitive science-inspired framework for instruction data selection
and annotation guidance. THTB prioritizes higher-level cognitive instructions
by combining quality filtering with intrinsic and extrinsic hardness scoring,
offering interpretable and quantifiable criteria for efficient SFT, both in
data selection and annotation guidance. Experiments show that THTB enables
models trained on only 5% of the data to outperform full-dataset training,
while achieving superior generalization compared with LLM-only selection. In
addition, THTB provides effective annotation guidance in vertical domains,
enabling a model trained on just 2% of the data to surpass models trained on
much larger datasets, demonstrating strong potential for domain adaptation. Our
code, datasets, and models are available on
https://github.com/DYJG-research/THTB.

</details>


### [44] [Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection](https://arxiv.org/abs/2510.13893)
*Olga E. Sorokoletova,Francesco Giarrusso,Vincenzo Suriani,Daniele Nardi*

Main category: cs.CL

TL;DR: 本文构建了一个包含50种越狱策略的层级分类体系，并分析了不同攻击类型的流行度和成功率。此外，本文还评估了基于分类体系引导的提示对越狱检测的益处，并构建了一个新的意大利语数据集，用于研究对抗意图逐渐出现并绕过传统安全措施的交互。


<details>
  <summary>Details</summary>
Motivation: 现有的防御措施通常侧重于单轮攻击，缺乏跨语言覆盖，并且依赖于有限的分类体系，这些分类体系要么未能捕捉到攻击策略的全部多样性，要么强调风险类别而非越狱技术。

Method: 本文进行了一项结构化的红队挑战，以了解越狱技术的有效性。

Result: 本文构建了一个包含50种越狱策略的层级分类体系，分析了不同攻击类型的流行度和成功率，评估了基于分类体系引导的提示对越狱检测的益处，并构建了一个新的意大利语数据集。

Conclusion: 本文通过红队挑战，深入了解了越狱技术的有效性，并为未来的防御研究提供了新的资源和方向。

Abstract: Jailbreaking techniques pose a significant threat to the safety of Large
Language Models (LLMs). Existing defenses typically focus on single-turn
attacks, lack coverage across languages, and rely on limited taxonomies that
either fail to capture the full diversity of attack strategies or emphasize
risk categories rather than the jailbreaking techniques. To advance the
understanding of the effectiveness of jailbreaking techniques, we conducted a
structured red-teaming challenge. The outcome of our experiments are manifold.
First, we developed a comprehensive hierarchical taxonomy of 50 jailbreak
strategies, consolidating and extending prior classifications into seven broad
families, including impersonation, persuasion, privilege escalation, cognitive
overload, obfuscation, goal conflict, and data poisoning. Second, we analyzed
the data collected from the challenge to examine the prevalence and success
rates of different attack types, providing insights into how specific jailbreak
strategies exploit model vulnerabilities and induce misalignment. Third, we
benchmark a popular LLM for jailbreak detection, evaluating the benefits of
taxonomy-guided prompting for improving automatic detection. Finally, we
compiled a new Italian dataset of 1364 multi-turn adversarial dialogues,
annotated with our taxonomy, enabling the study of interactions where
adversarial intent emerges gradually and succeeds in bypassing traditional
safeguards.

</details>


### [45] [Attribution Quality in AI-Generated Content:Benchmarking Style Embeddings and LLM Judges](https://arxiv.org/abs/2510.13898)
*Misam Abbas*

Main category: cs.CL

TL;DR: 该论文研究了在大语言模型时代进行作者身份识别的挑战，并评估了两种互补的识别机制：固定风格嵌入和指令调优的LLM评判器（GPT-4o）。


<details>
  <summary>Details</summary>
Motivation: 随着机器生成的文本与人类写作水平日益接近，作者身份识别变得越来越具有挑战性。

Method: 该研究在Human AI Parallel Corpus数据集上，对固定风格嵌入和指令调优的LLM评判器(GPT-4o)两种作者身份识别机制进行了基准测试。该数据集包含600个平衡的实例，涵盖六个领域（学术、新闻、小说、博客、口语记录和电视/电影剧本）。

Result: 固定风格嵌入在GPT续写文本上取得了更高的总体准确率（82% vs. 68%）。LLM评判器在LLaMA续写文本上略优于风格嵌入（85% vs. 81%），但结果在统计学上并不显著。LLM评判器在小说和学术散文中明显优于风格嵌入，表明其具有语义敏感性，而嵌入在口语和剧本对话中占主导地位，反映了结构优势。

Conclusion: 作者身份识别是一个多维问题，需要混合策略。该研究提供了一个可复现的基准，用于评估AI生成内容中的作者身份识别质量。

Abstract: Attributing authorship in the era of large language models (LLMs) is
increasingly challenging as machine-generated prose rivals human writing. We
benchmark two complementary attribution mechanisms , fixed Style Embeddings and
an instruction-tuned LLM judge (GPT-4o) on the Human AI Parallel Corpus, an
open dataset of 600 balanced instances spanning six domains (academic, news,
fiction, blogs, spoken transcripts, and TV/movie scripts). Each instance
contains a human prompt with both a gold continuation and an LLM-generated
continuation from either GPT-4o or LLaMA-70B-Instruct. The Style Embedding
baseline achieves stronger aggregate accuracy on GPT continuations (82 pct vs.
68 pct). The LLM Judge is slightly better than the Style embeddings on LLaMA
continuations (85 pct vs. 81 pct) but the results are not statistically
significant. Crucially, the LLM judge significantly outperforms in fiction and
academic prose, indicating semantic sensitivity, whereas embeddings dominate in
spoken and scripted dialogue, reflecting structural strengths. These
complementary patterns highlight attribution as a multidimensional problem
requiring hybrid strategies. To support reproducibility we provide code on
GitHub and derived data on Hugging Face under the MIT license. This open
framework provides a reproducible benchmark for attribution quality assessment
in AI-generated content, along with a review of related literature influencing
this work.

</details>


### [46] [Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences](https://arxiv.org/abs/2510.13900)
*Julian Minder,Clément Dumas,Stewart Slocum,Helena Casademunt,Cameron Holmes,Robert West,Neel Nanda*

Main category: cs.CL

TL;DR: 狭窄领域的微调会在 LLM 激活中产生偏差，这可以被解释以理解微调域。


<details>
  <summary>Details</summary>
Motivation: 将大型语言模型 (LLM) 适应特定任务并创建具有已知不寻常属性的模型，这对于研究很有用。狭窄的微调在 LLM 激活中产生强烈的偏差，这可以被解释以理解微调域。

Method: 通过分析随机文本前几个 token 的激活差异，并将此差异添加到模型激活中进行引导，从而产生类似于微调数据的格式和一般内容。

Result: 通过创建一个基于 LLM 的可解释性代理来理解微调域，证明了这些分析包含关键信息。与使用简单提示的基线代理相比，具有偏差访问权限的代理表现明显更好。

Conclusion: 狭窄微调的模型在其激活中具有其训练目标的显着痕迹，并建议改进其训练方式。使用此类模型作为研究更广泛的微调的替代方法可能不切实际，并且需要更深入地研究狭窄微调的影响，并为模型差异、安全性和可解释性研究开发真正现实的案例研究。

Abstract: Finetuning on narrow domains has become an essential tool to adapt Large
Language Models (LLMs) to specific tasks and to create models with known
unusual properties that are useful for research. We show that narrow finetuning
creates strong biases in LLM activations that can be interpreted to understand
the finetuning domain. These biases can be discovered using simple tools from
model diffing - the study of differences between models before and after
finetuning. In particular, analyzing activation differences on the first few
tokens of random text and steering by adding this difference to the model
activations produces text similar to the format and general content of the
finetuning data. We demonstrate that these analyses contain crucial information
by creating an LLM-based interpretability agent to understand the finetuning
domain. With access to the bias, the agent performs significantly better
compared to baseline agents using simple prompting. Our analysis spans
synthetic document finetuning for false facts, emergent misalignment,
subliminal learning, and taboo word guessing game models across different
architectures (Gemma, LLaMA, Qwen) and scales (1B to 32B parameters). We
suspect these biases reflect overfitting and find that mixing pretraining data
into the finetuning corpus largely removes them, though residual risks may
remain. Our work (1) demonstrates that narrowly finetuned models have salient
traces of their training objective in their activations and suggests ways to
improve how they are trained, (2) warns AI safety and interpretability
researchers that the common practice of using such models as a proxy for
studying broader finetuning (e.g., chat-tuning) might not be realistic, and (3)
highlights the need for deeper investigation into the effects of narrow
finetuning and development of truly realistic case studies for model-diffing,
safety and interpretability research.

</details>


### [47] [RAID: Refusal-Aware and Integrated Decoding for Jailbreaking LLMs](https://arxiv.org/abs/2510.13901)
*Tuan T. Nguyen,John Le,Thai T. Vu,Willy Susilo,Heath Cooper*

Main category: cs.CL

TL;DR: 该论文提出了一种名为RAID的新框架，用于攻击大型语言模型（LLM）的安全机制，通过生成对抗性后缀来诱导模型生成受限内容。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在各种任务中表现出色，但容易受到绕过安全机制的jailbreak攻击。

Method: RAID框架将离散tokens松弛为连续嵌入，并通过联合目标函数进行优化，该函数鼓励受限响应，结合了拒绝感知正则化器以避开嵌入空间中的拒绝方向，并应用连贯性项以保持语义合理性和非冗余性。然后，通过平衡嵌入亲和力与语言模型可能性，将嵌入映射回tokens。

Result: 实验表明，与最近的白盒和黑盒基线相比，RAID以更少的查询和更低的计算成本实现了更高的攻击成功率。

Conclusion: 研究结果强调了嵌入空间正则化对于理解和减轻LLM jailbreak漏洞的重要性。

Abstract: Large language models (LLMs) achieve impressive performance across diverse
tasks yet remain vulnerable to jailbreak attacks that bypass safety mechanisms.
We present RAID (Refusal-Aware and Integrated Decoding), a framework that
systematically probes these weaknesses by crafting adversarial suffixes that
induce restricted content while preserving fluency. RAID relaxes discrete
tokens into continuous embeddings and optimizes them with a joint objective
that (i) encourages restricted responses, (ii) incorporates a refusal-aware
regularizer to steer activations away from refusal directions in embedding
space, and (iii) applies a coherence term to maintain semantic plausibility and
non-redundancy. After optimization, a critic-guided decoding procedure maps
embeddings back to tokens by balancing embedding affinity with language-model
likelihood. This integration yields suffixes that are both effective in
bypassing defenses and natural in form. Experiments on multiple open-source
LLMs show that RAID achieves higher attack success rates with fewer queries and
lower computational cost than recent white-box and black-box baselines. These
findings highlight the importance of embedding-space regularization for
understanding and mitigating LLM jailbreak vulnerabilities.

</details>


### [48] [Investigating Political and Demographic Associations in Large Language Models Through Moral Foundations Theory](https://arxiv.org/abs/2510.13902)
*Nicole Smith-Vaniz,Harper Lyon,Lorraine Steigner,Ben Armstrong,Nicholas Mattei*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型（LLM）在政治和道德领域中的潜在偏见，特别是通过道德基础理论（MFT）分析其反应。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在日常生活中扮演着重要角色，尤其是在医学、人际关系和法律等领域，引发了对其在政治和道德领域中可能存在的偏见的关注。

Method: 该研究使用道德基础理论（MFT）来评估LLM的道德倾向，并将其与现有的人类研究进行比较。通过直接提示和基于人口统计的角色扮演，分析LLM在不同条件下的行为。

Result: 研究旨在确定LLM是否固有地产生与某种政治意识形态更一致的反应，并评估LLM通过显式提示和基于人口统计的角色扮演来准确代表意识形态观点的能力。

Conclusion: 该研究旨在深入了解人工智能生成的回应中政治和人口统计依赖性的程度。

Abstract: Large Language Models (LLMs) have become increasingly incorporated into
everyday life for many internet users, taking on significant roles as advice
givers in the domains of medicine, personal relationships, and even legal
matters. The importance of these roles raise questions about how and what
responses LLMs make in difficult political and moral domains, especially
questions about possible biases. To quantify the nature of potential biases in
LLMs, various works have applied Moral Foundations Theory (MFT), a framework
that categorizes human moral reasoning into five dimensions: Harm, Fairness,
Ingroup Loyalty, Authority, and Purity. Previous research has used the MFT to
measure differences in human participants along political, national, and
cultural lines. While there has been some analysis of the responses of LLM with
respect to political stance in role-playing scenarios, no work so far has
directly assessed the moral leanings in the LLM responses, nor have they
connected LLM outputs with robust human data. In this paper we analyze the
distinctions between LLM MFT responses and existing human research directly,
investigating whether commonly available LLM responses demonstrate ideological
leanings: either through their inherent responses, straightforward
representations of political ideologies, or when responding from the
perspectives of constructed human personas. We assess whether LLMs inherently
generate responses that align more closely with one political ideology over
another, and additionally examine how accurately LLMs can represent ideological
perspectives through both explicit prompting and demographic-based
role-playing. By systematically analyzing LLM behavior across these conditions
and experiments, our study provides insight into the extent of political and
demographic dependency in AI-generated responses.

</details>


### [49] [Schema for In-Context Learning](https://arxiv.org/abs/2510.13905)
*Pan Chen,Shaohong Chen,Mark Wang,Shi Xuan Leong,Priscilla Fung,Varinia Bernales,Alan Aspuru-Guzik*

Main category: cs.CL

TL;DR: 提出了SCHEMA ACTIVATED IN CONTEXT LEARNING (SA-ICL) 框架，通过提取认知构建块的表示来创建抽象模式，用于增强模型的推理过程。


<details>
  <summary>Details</summary>
Motivation: 传统示例驱动的上下文学习缺乏知识检索和迁移的显式模块。受认知科学中模式理论的启发，该理论认为人类通过激活预先存在的心理框架来构建理解。

Method: 提取先前示例中推理过程的认知构建块表示，创建关键推理步骤及其关系的轻量级结构化模板（即模式），然后使用该模式来增强模型在面对新问题时的推理过程。

Result: 实验表明，大型语言模型（LLM）缺乏隐式形成和利用基于模式的学习表示的能力，但从显式的基于模式的支架中获益匪浅。在GPQA数据集的化学和物理问题上，SA-ICL始终能提高性能，最高可达36.19%。

Conclusion: SCHEMA ACTIVATED IN CONTEXT LEARNING 不仅连接了从模式启动到思维链提示的不同 ICL 策略，而且为增强 LLM 中类人推理能力开辟了一条新路径。

Abstract: In-Context Learning (ICL) enables transformer-based language models to adapt
to new tasks by conditioning on demonstration examples. However, traditional
example-driven in-context learning lacks explicit modules for knowledge
retrieval and transfer at the abstraction level. Inspired by cognitive science,
specifically schema theory, which holds that humans interpret new information
by activating pre-existing mental frameworks (schemas) to structure
understanding, we introduce SCHEMA ACTIVATED IN CONTEXT LEARNING (SA-ICL). This
framework extracts the representation of the building blocks of cognition for
the reasoning process instilled from prior examples, creating an abstracted
schema, a lightweight, structured template of key inferential steps and their
relationships, which is then used to augment a model's reasoning process when
presented with a novel question. We demonstrate that a broad range of large
language models (LLMs) lack the capacity to form and utilize internal
schema-based learning representations implicitly, but instead benefit
significantly from explicit schema-based scaffolding. Across chemistry and
physics questions from the GPQA dataset, our experiments show that SA-ICL
consistently boosts performance, up to 36.19 percent, when the single
demonstration example is of high quality, which simultaneously reduces reliance
on the number of demonstrations and enhances interpretability. SCHEMA ACTIVATED
IN CONTEXT LEARNING not only bridges disparate ICL strategies ranging from
pattern priming to Chain-of-Thought prompting, but also paves a new path for
enhancing human-like reasoning in LLMs.

</details>


### [50] [LLM Prompt Duel Optimizer: Efficient Label-Free Prompt Optimization](https://arxiv.org/abs/2510.13907)
*Yuanchen Wu,Saurabh Verma,Justin Lee,Fangzhou Xiong,Poppy Zhang,Amel Awadelkarim,Xu Chen,Yubai Yuan,Shawndra Hill*

Main category: cs.CL

TL;DR: 提出了一种名为Prompt Duel Optimizer (PDO)的无标签提示优化框架，通过LLM judge提供的成对偏好反馈进行优化。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型对输入提示非常敏感，提示设计至关重要，但收集高质量标签成本高。

Method: 将问题转化为dueling-bandit问题，结合Double Thompson Sampling (D-TS)和Top-Performer Guided Mutation。

Result: 在BIG-bench Hard (BBH)和MS MARCO上的实验表明，PDO始终优于基线方法。

Conclusion: D-TS和prompt mutation都非常有效。

Abstract: Large language models (LLMs) are highly sensitive to their input prompts,
making prompt design a central challenge. While automatic prompt optimization
(APO) reduces manual engineering, most approaches assume access to ground-truth
references such as labeled validation data. In practice, however, collecting
high-quality labels is costly and slow. We propose the Prompt Duel Optimizer
(PDO), a sample-efficient framework for label-free prompt optimization. PDO
formulates the problem as a dueling-bandit setting, where supervision signal
comes from pairwise preference feedback provided by an LLM judge. The framework
combines Double Thompson Sampling (D-TS), which prioritizes informative prompt
comparisons, with Top-Performer Guided Mutation, which expands the candidate
pool by mutating high-performing prompts. PDO naturally operates in label-free
settings and can also incorporate partial labels to mitigate judge noise.
Experiments on BIG-bench Hard (BBH) and MS MARCO show that PDO consistently
outperforms baseline methods. Ablation studies further demonstrate the
effectiveness of both D-TS and prompt mutation.

</details>


### [51] [Interpreting the Latent Structure of Operator Precedence in Language Models](https://arxiv.org/abs/2510.13908)
*Dharunish Yugeswardeenoo,Harshil Nukala,Cole Blondin,Sean O Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: 大型语言模型在算术任务中表现不佳，本文研究了LLM是否在其内部表示中编码了运算符优先级。


<details>
  <summary>Details</summary>
Motivation: 先前的研究主要集中在输出或提示策略上，而忽略了模型进行算术计算的内部结构。

Method: 使用包含三个操作数和两个运算符的算术表达式数据集，通过logit lens、线性分类探针和UMAP几何可视化等可解释性技术，追踪指令调整的LLaMA 3.2-3B模型残差流中是否出现中间结果。

Result: 中间计算存在于残差流中，尤其是在MLP块之后。模型在线性编码每个运算符在注意力层后的嵌入中的优先级。

Conclusion: 我们引入了部分嵌入交换技术，通过交换运算符之间的高影响嵌入维度来修改运算符优先级。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning
capabilities but continue to struggle with arithmetic tasks. Prior works
largely focus on outputs or prompting strategies, leaving the open question of
the internal structure through which models do arithmetic computation. In this
work, we investigate whether LLMs encode operator precedence in their internal
representations via the open-source instruction-tuned LLaMA 3.2-3B model. We
constructed a dataset of arithmetic expressions with three operands and two
operators, varying the order and placement of parentheses. Using this dataset,
we trace whether intermediate results appear in the residual stream of the
instruction-tuned LLaMA 3.2-3B model. We apply interpretability techniques such
as logit lens, linear classification probes, and UMAP geometric visualization.
Our results show that intermediate computations are present in the residual
stream, particularly after MLP blocks. We also find that the model linearly
encodes precedence in each operator's embeddings post attention layer. We
introduce partial embedding swap, a technique that modifies operator precedence
by exchanging high-impact embedding dimensions between operators.

</details>


### [52] [Knowledge Reasoning Language Model: Unifying Knowledge and Language for Inductive Knowledge Graph Reasoning](https://arxiv.org/abs/2510.13909)
*Xingrui Zhuo,Jiapu Wang,Gongqing Wu,Zhongyuan Wang,Jichen Zhang,Shirui Pan,Xindong Wu*

Main category: cs.CL

TL;DR: This paper introduces KRLM, a Knowledge Reasoning Language Model for inductive KGR that coordinates LLM knowledge and KG context.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based KGFMs suffer from LLM knowledge distortion due to sparse KG context and struggle to constrain generative hallucinations, limiting the credibility of reasoning results.

Method: The paper proposes a KRL instruction format and tokenizer, a KRL attention layer with a dynamic knowledge memory mechanism, and a structure-aware next-entity predictor.

Result: Extensive experiments on 25 real-world inductive KGR datasets demonstrate the superiority of KRLM in both zero-shot reasoning and fine-tuning scenarios.

Conclusion: KRLM effectively addresses the limitations of existing LLM-based KGR methods by unifying LLM knowledge and KG context and constraining reasoning results within a trustworthy knowledge domain.

Abstract: Inductive Knowledge Graph Reasoning (KGR) aims to discover facts in
open-domain KGs containing unknown entities and relations, which poses a
challenge for KGR models in comprehending uncertain KG components. Existing
studies have proposed Knowledge Graph Foundation Models (KGFMs) that learn
structural invariances across KGs to handle this uncertainty. Recently, Large
Language Models (LLMs) have demonstrated strong capabilities for open-domain
knowledge reasoning. As a result, the latest research has focused on LLM-based
KGFMs that integrate LLM knowledge with KG context for inductive KGR. However,
the intrinsic knowledge of LLMs may be overshadowed by sparse KG context,
leading to LLM knowledge distortion, which can cause irreversible damage to
model reasoning. Moreover, existing LLM-based KGR methods still struggle to
fully constrain generative hallucinations in LLMs, severely limiting the
credibility of reasoning results. To address these limitations, we propose a
Knowledge Reasoning Language Model (KRLM) that achieves unified coordination
between LLM knowledge and KG context throughout the KGR process. Specifically,
we design a Knowledge Reasoning Language (KRL) instruction format and a KRL
tokenizer to align LLM knowledge with KG representations. Then, we propose a
KRL attention layer that coordinates intrinsic LLM knowledge with additional KG
context through a dynamic knowledge memory mechanism. Finally, a
structure-aware next-entity predictor is proposed, which strictly constrains
the reasoning results within a trustworthy knowledge domain. Extensive
experimental results on 25 real-world inductive KGR datasets demonstrate the
significant superiority of the proposed KRLM\footnote{Our source codes are
available at https://anonymous.4open.science/r/KRLM-EA36 in both zero-shot
reasoning and fine-tuning scenarios.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [53] [MultiFoodhat: A potential new paradigm for intelligent food quality inspection](https://arxiv.org/abs/2510.13889)
*Yue Hu,Guohang Zhuang*

Main category: cs.CV

TL;DR: 提出了一种新的零样本食物识别框架，通过多轮视觉文本对话实现协作推理。


<details>
  <summary>Details</summary>
Motivation: 现有监督模型依赖大量标签数据，对未见过的食物类别泛化能力有限。

Method: 引入 MultiFoodChat，一个对话驱动的多智能体推理框架，集成了视觉语言模型 (VLM) 和大型语言模型 (LLM)。

Result: 在多个公共食物数据集上的实验表明，MultiFoodChat 相比现有的无监督和少样本方法，实现了更高的识别精度和可解释性。

Conclusion: MultiFoodChat 有望成为智能食品质量检测和分析的新范例。

Abstract: Food image classification plays a vital role in intelligent food quality
inspection, dietary assessment, and automated monitoring. However, most
existing supervised models rely heavily on large labeled datasets and exhibit
limited generalization to unseen food categories. To overcome these challenges,
this study introduces MultiFoodChat, a dialogue-driven multi-agent reasoning
framework for zero-shot food recognition. The framework integrates
vision-language models (VLMs) and large language models (LLMs) to enable
collaborative reasoning through multi-round visual-textual dialogues. An Object
Perception Token (OPT) captures fine-grained visual attributes, while an
Interactive Reasoning Agent (IRA) dynamically interprets contextual cues to
refine predictions. This multi-agent design allows flexible and human-like
understanding of complex food scenes without additional training or manual
annotations. Experiments on multiple public food datasets demonstrate that
MultiFoodChat achieves superior recognition accuracy and interpretability
compared with existing unsupervised and few-shot methods, highlighting its
potential as a new paradigm for intelligent food quality inspection and
analysis.

</details>


### [54] [Post-surgical Endometriosis Segmentation in Laparoscopic Videos](https://arxiv.org/abs/2510.13899)
*Andreas Leibetseder,Klaus Schoeffmann,Jörg Keckstein,Simon Keckstein*

Main category: cs.CV

TL;DR: 该论文描述了一个用于分割子宫内膜异位症的系统。


<details>
  <summary>Details</summary>
Motivation: 子宫内膜异位症的视觉表现多样，难以识别，尤其对于非专业人士。

Method: 该系统经过训练，可以分割深色子宫内膜植入物，分析腹腔镜手术视频，并使用彩色叠加注释识别的植入区域。

Result: 该系统能够注释识别的植入区域，并显示检测摘要，以改进视频浏览。

Conclusion: 该演示论文旨在为治疗子宫内膜异位症的妇科医生提供帮助。

Abstract: Endometriosis is a common women's condition exhibiting a manifold visual
appearance in various body-internal locations. Having such properties makes its
identification very difficult and error-prone, at least for laymen and
non-specialized medical practitioners. In an attempt to provide assistance to
gynecologic physicians treating endometriosis, this demo paper describes a
system that is trained to segment one frequently occurring visual appearance of
endometriosis, namely dark endometrial implants. The system is capable of
analyzing laparoscopic surgery videos, annotating identified implant regions
with multi-colored overlays and displaying a detection summary for improved
video browsing.

</details>


### [55] [Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models](https://arxiv.org/abs/2510.13993)
*Jia Yun Chua,Argyrios Zolotas,Miguel Arana-Catania*

Main category: cs.CV

TL;DR: 本文研究了将视觉模型和视觉语言模型（VLM）结合以增强遥感图像分析，重点是飞机检测和场景理解。


<details>
  <summary>Details</summary>
Motivation: 传统视觉模型需要大量的领域特定标签数据，并且理解复杂环境中的上下文的能力有限。视觉语言模型通过整合视觉和文本数据提供了一种补充方法，但它们在遥感中的应用仍有待探索。

Method: 将YOLO与LLaVA、ChatGPT和Gemini等VLM集成，以实现更准确和上下文感知的图像解释。在标记和未标记的遥感数据以及退化的图像场景中评估性能。

Result: 在原始和退化场景中，飞机检测和计数精度方面，模型的平均MAE提高了48.46%。遥感图像综合理解的CLIPScore提高了6.17%。

Conclusion: 将传统视觉模型和VLM相结合的方法为更高级和高效的遥感图像分析铺平了道路，尤其是在少样本学习场景中。

Abstract: Remote sensing has become a vital tool across sectors such as urban planning,
environmental monitoring, and disaster response. While the volume of data
generated has increased significantly, traditional vision models are often
constrained by the requirement for extensive domain-specific labelled data and
their limited ability to understand the context within complex environments.
Vision Language Models offer a complementary approach by integrating visual and
textual data; however, their application to remote sensing remains
underexplored, particularly given their generalist nature. This work
investigates the combination of vision models and VLMs to enhance image
analysis in remote sensing, with a focus on aircraft detection and scene
understanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, and
Gemini aims to achieve more accurate and contextually aware image
interpretation. Performance is evaluated on both labelled and unlabelled remote
sensing data, as well as degraded image scenarios which are crucial for remote
sensing. The findings show an average MAE improvement of 48.46% across models
in the accuracy of aircraft detection and counting, especially in challenging
conditions, in both raw and degraded scenarios. A 6.17% improvement in
CLIPScore for comprehensive understanding of remote sensing images is obtained.
The proposed approach combining traditional vision models and VLMs paves the
way for more advanced and efficient remote sensing image analysis, especially
in few-shot learning scenarios.

</details>


### [56] [Finding Holes: Pathologist Level Performance Using AI for Cribriform Morphology Detection in Prostate Cancer](https://arxiv.org/abs/2510.13995)
*Kelvin Szolnoky,Anders Blilie,Nita Mulliqi,Toyonori Tsuzuki,Hemamali Samaratunga,Matteo Titus,Xiaoyi Ji,Sol Erika Boman,Einar Gudlaugsson,Svein Reidar Kjosavik,José Asenjo,Marcello Gambacorta,Paolo Libretti,Marcin Braun,Radisław Kordek,Roman Łowicki,Brett Delahunt,Kenneth A. Iczkowski,Theo van der Kwast,Geert J. L. H. van Leenders,Katia R. M. Leite,Chin-Chen Pan,Emiel Adrianus Maria Janssen,Martin Eklund,Lars Egevad,Kimmo Kartasalo*

Main category: cs.CV

TL;DR: 开发并验证了一种基于AI的系统，以提高前列腺癌筛状形态的检测。


<details>
  <summary>Details</summary>
Motivation: 筛状形态是前列腺癌中一种组织学特征，预示着不良预后，但病理学家之间存在显著的观察者间差异。

Method: 使用带有多个实例学习的EfficientNetV2-S编码器创建深度学习模型，用于端到端全玻片分类。在来自三个队列的430名患者的640张数字化前列腺核心针活检切片上训练该模型。

Result: 该模型显示出强大的内部验证性能（AUC：0.97，95% CI：0.95-0.99；Cohen's kappa：0.81，95% CI：0.72-0.89）和稳健的外部验证（AUC：0.90，95% CI：0.86-0.93；Cohen's kappa：0.55，95% CI：0.45-0.64）。

Conclusion: 我们的AI模型在前列腺癌筛状形态检测方面表现出病理学家水平的性能。这种方法可以提高诊断可靠性，标准化报告，并改善前列腺癌患者的治疗决策。

Abstract: Background: Cribriform morphology in prostate cancer is a histological
feature that indicates poor prognosis and contraindicates active surveillance.
However, it remains underreported and subject to significant interobserver
variability amongst pathologists. We aimed to develop and validate an AI-based
system to improve cribriform pattern detection.
  Methods: We created a deep learning model using an EfficientNetV2-S encoder
with multiple instance learning for end-to-end whole-slide classification. The
model was trained on 640 digitised prostate core needle biopsies from 430
patients, collected across three cohorts. It was validated internally (261
slides from 171 patients) and externally (266 slides, 104 patients from three
independent cohorts). Internal validation cohorts included laboratories or
scanners from the development set, while external cohorts used completely
independent instruments and laboratories. Annotations were provided by three
expert uropathologists with known high concordance. Additionally, we conducted
an inter-rater analysis and compared the model's performance against nine
expert uropathologists on 88 slides from the internal validation cohort.
  Results: The model showed strong internal validation performance (AUC: 0.97,
95% CI: 0.95-0.99; Cohen's kappa: 0.81, 95% CI: 0.72-0.89) and robust external
validation (AUC: 0.90, 95% CI: 0.86-0.93; Cohen's kappa: 0.55, 95% CI:
0.45-0.64). In our inter-rater analysis, the model achieved the highest average
agreement (Cohen's kappa: 0.66, 95% CI: 0.57-0.74), outperforming all nine
pathologists whose Cohen's kappas ranged from 0.35 to 0.62.
  Conclusion: Our AI model demonstrates pathologist-level performance for
cribriform morphology detection in prostate cancer. This approach could enhance
diagnostic reliability, standardise reporting, and improve treatment decisions
for prostate cancer patients.

</details>


### [57] [NAPPure: Adversarial Purification for Robust Image Classification under Non-Additive Perturbations](https://arxiv.org/abs/2510.14025)
*Junjie Nan,Jianing Li,Wei Chen,Mingkun Zhang,Xueqi Cheng*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的对抗净化框架，名为NAPPure，用于处理非加性扰动。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗净化方法在处理模糊、遮挡和失真等非加性扰动时效果不佳，因为它们是为加性扰动设计的。

Method: 该方法首先建立对抗图像的生成过程，然后通过最大化似然来解耦潜在的干净图像和扰动参数。

Result: 在GTSRB和CIFAR-10数据集上的实验表明，NAPPure显著提高了图像分类模型对非加性扰动的鲁棒性。

Conclusion: NAPPure框架有效地提升了模型抵抗非加性扰动的能力。

Abstract: Adversarial purification has achieved great success in combating adversarial
image perturbations, which are usually assumed to be additive. However,
non-additive adversarial perturbations such as blur, occlusion, and distortion
are also common in the real world. Under such perturbations, existing
adversarial purification methods are much less effective since they are
designed to fit the additive nature. In this paper, we propose an extended
adversarial purification framework named NAPPure, which can further handle
non-additive perturbations. Specifically, we first establish the generation
process of an adversarial image, and then disentangle the underlying clean
image and perturbation parameters through likelihood maximization. Experiments
on GTSRB and CIFAR-10 datasets show that NAPPure significantly boosts the
robustness of image classification models against non-additive perturbations.

</details>


### [58] [Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding](https://arxiv.org/abs/2510.14032)
*Xiaoqian Shen,Wenxuan Zhang,Jun Chen,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为Vgent的基于图的检索-推理-增强生成框架，用于提升大型视频语言模型（LVLM）对长视频的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的LVLM在处理长视频时面临上下文窗口限制和长期时序信息保留的挑战，而直接应用检索增强生成（RAG）方法又存在时间依赖性中断和包含无关信息的问题。

Method: 该方法通过构建结构化视频图来保持视频片段之间的语义关系，并引入中间推理步骤来减少检索噪声，从而促进相关信息的显式聚合。

Result: 在三个长视频理解基准测试中，Vgent框架在多个开源LVLM上取得了显著的性能提升，相较于基线模型在MLVU上提升了3.0%~5.4%，并且优于现有的视频RAG方法8.6%。

Conclusion: Vgent框架有效地提升了LVLM对长视频的理解能力，并通过结构化图表示和中间推理步骤解决了长视频处理中的关键问题。

Abstract: Understanding and reasoning over long videos pose significant challenges for
large video language models (LVLMs) due to the difficulty in processing
intensive video tokens beyond context window and retaining long-term sequential
information. Retrieval-Augmented Generation (RAG) has demonstrated
effectiveness in processing long context for Large Language Models (LLMs);
however, applying RAG to long video faces challenges such as disrupted temporal
dependencies and inclusion of irrelevant information that can hinder accurate
reasoning. To address these limitations, we propose Vgent, a novel graph-based
retrieval-reasoning-augmented generation framework to enhance LVLMs for long
video understanding. Our approach introduces two key innovations: (i) It
represents videos by structured graphs with semantic relationships across video
clips preserved to improve retrieval effectiveness. (ii) It introduces an
intermediate reasoning step to mitigate the reasoning limitation of LVLMs,
which leverages structured verification to reduce retrieval noise and
facilitate the explicit aggregation of relevant information across clips,
resulting in more accurate and context-aware responses. We comprehensively
evaluate our framework with various open-source LVLMs on three long-video
understanding benchmarks. Our approach yielded an overall performance
improvement of $3.0\%\sim 5.4\%$ over base models on MLVU, and outperformed
state-of-the-art video RAG methods by $8.6\%$. Our code is publicly available
at https://xiaoqian-shen.github.io/Vgent.

</details>


### [59] [Synchronization of Multiple Videos](https://arxiv.org/abs/2510.14051)
*Avihai Naaman,Ron Shapira Weber,Oren Freifeld*

Main category: cs.CV

TL;DR: 提出了一种基于原型的时间原型学习（TPL）框架，用于同步来自不同场景或生成式AI的视频，通过学习统一的原型序列来对齐视频。


<details>
  <summary>Details</summary>
Motivation: 由于不同场景或生成式AI视频在主题、背景和非线性时间错位方面存在差异，导致视频同步非常复杂。

Method: 构建一个共享的、紧凑的1D表示，该表示来自各种预训练模型提取的高维嵌入。通过学习统一的原型序列来锚定关键动作阶段，从而避免详尽的成对匹配。

Result: 在各种数据集上提高了同步精度、效率和鲁棒性，包括细粒度帧检索和阶段分类任务。首次缓解了描绘相同动作的多个生成式AI视频中的同步问题。

Conclusion: TPL是一种有效的视频同步方法，特别是在处理来自不同场景或生成式AI的视频时。

Abstract: Synchronizing videos captured simultaneously from multiple cameras in the
same scene is often easy and typically requires only simple time shifts.
However, synchronizing videos from different scenes or, more recently,
generative AI videos, poses a far more complex challenge due to diverse
subjects, backgrounds, and nonlinear temporal misalignment. We propose Temporal
Prototype Learning (TPL), a prototype-based framework that constructs a shared,
compact 1D representation from high-dimensional embeddings extracted by any of
various pretrained models. TPL robustly aligns videos by learning a unified
prototype sequence that anchors key action phases, thereby avoiding exhaustive
pairwise matching. Our experiments show that TPL improves synchronization
accuracy, efficiency, and robustness across diverse datasets, including
fine-grained frame retrieval and phase classification tasks. Importantly, TPL
is the first approach to mitigate synchronization issues in multiple generative
AI videos depicting the same action. Our code and a new multiple video
synchronization dataset are available at https://bgu-cs-vil.github.io/TPL/

</details>


### [60] [Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images](https://arxiv.org/abs/2510.14081)
*Emanuel Garbin,Guy Adam,Oded Krams,Zohar Barzelay,Eran Guendelman,Michael Schwarz,Moran Vatelmacher,Yigal Shenkman,Eli Peker,Itai Druker,Uri Patish,Yoav Blum,Max Bluvstein,Junxuan Li,Rawal Khirodkar,Shunsuke Saito*

Main category: cs.CV

TL;DR: 这篇论文提出了一个新颖的零样本pipeline，用于从几张非结构化的手机图像中创建超现实、保持身份的3D头像。


<details>
  <summary>Details</summary>
Motivation: 现有的方法面临几个挑战：单视图方法存在几何不一致和幻觉，降低了身份保持，而基于合成数据训练的模型无法捕捉高频细节，如皮肤皱纹和细毛，限制了真实感。

Method: 该方法引入了两个关键贡献：(1)一个生成式的规范化模块，将多个非结构化的视图处理成一个标准化的、一致的表示；(2)一个基于Transformer的模型，该模型在一个新的、大规模的高保真高斯溅射头像数据集上进行训练，这些头像来自真实人物的圆顶捕捉。

Result: 该“捕捉、规范、溅射”pipeline 能够从非结构化的照片中生成具有令人信服的真实感和强大的身份保持的静态四分之一身头像。

Conclusion: 总结：该论文提出了一个从非结构化图像创建逼真3D头像的新pipeline，通过生成式规范化模块和Transformer模型，实现了高质量的头像生成效果。

Abstract: We present a novel, zero-shot pipeline for creating hyperrealistic,
identity-preserving 3D avatars from a few unstructured phone images. Existing
methods face several challenges: single-view approaches suffer from geometric
inconsistencies and hallucinations, degrading identity preservation, while
models trained on synthetic data fail to capture high-frequency details like
skin wrinkles and fine hair, limiting realism. Our method introduces two key
contributions: (1) a generative canonicalization module that processes multiple
unstructured views into a standardized, consistent representation, and (2) a
transformer-based model trained on a new, large-scale dataset of high-fidelity
Gaussian splatting avatars derived from dome captures of real people. This
"Capture, Canonicalize, Splat" pipeline produces static quarter-body avatars
with compelling realism and robust identity preservation from unstructured
photos.

</details>


### [61] [cubic: CUDA-accelerated 3D Bioimage Computing](https://arxiv.org/abs/2510.14143)
*Alexandr A. Kalinin,Anne E. Carpenter,Shantanu Singh,Matthew J. O'Meara*

Main category: cs.CV

TL;DR: Cubic是一个开源Python库，通过CuPy和RAPIDS cuCIM增强了SciPy和scikit-image API，利用GPU加速，解决了生物图像分析在可扩展性、效率和集成方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有生物图像分析工具缺乏API、不支持GPU加速、缺乏3D图像处理能力，且在计算密集型工作流程中的互操作性差。

Method: Cubic通过提供设备无关的API，将操作分配到GPU或CPU上执行，从而加速图像处理流程。它扩展了SciPy和scikit-image API，并利用CuPy和RAPIDS cuCIM实现了GPU加速。

Result: Cubic在基准测试和复制现有反卷积及分割流程中实现了显著加速，同时保持了算法的准确性。

Conclusion: Cubic为可扩展、可重复的生物图像分析奠定了基础，并与Python科学计算生态系统集成，支持交互式探索和自动化高通量分析工作流程。

Abstract: Quantitative analysis of multidimensional biological images is useful for
understanding complex cellular phenotypes and accelerating advances in
biomedical research. As modern microscopy generates ever-larger 2D and 3D
datasets, existing computational approaches are increasingly limited by their
scalability, efficiency, and integration with modern scientific computing
workflows. Existing bioimage analysis tools often lack application programmable
interfaces (APIs), do not support graphics processing unit (GPU) acceleration,
lack broad 3D image processing capabilities, and/or have poor interoperability
for compute-heavy workflows. Here, we introduce cubic, an open-source Python
library that addresses these challenges by augmenting widely used SciPy and
scikit-image APIs with GPU-accelerated alternatives from CuPy and RAPIDS cuCIM.
cubic's API is device-agnostic and dispatches operations to GPU when data
reside on the device and otherwise executes on CPU, seamlessly accelerating a
broad range of image processing routines. This approach enables GPU
acceleration of existing bioimage analysis workflows, from preprocessing to
segmentation and feature extraction for 2D and 3D data. We evaluate cubic both
by benchmarking individual operations and by reproducing existing deconvolution
and segmentation pipelines, achieving substantial speedups while maintaining
algorithmic fidelity. These advances establish a robust foundation for
scalable, reproducible bioimage analysis that integrates with the broader
Python scientific computing ecosystem, including other GPU-accelerated methods,
enabling both interactive exploration and automated high-throughput analysis
workflows. cubic is openly available at
https://github$.$com/alxndrkalinin/cubic

</details>


### [62] [Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures](https://arxiv.org/abs/2510.14179)
*Yuancheng Xu,Wenqi Xian,Li Ma,Julien Philip,Ahmet Levent Taşel,Yiwei Zhao,Ryan Burgert,Mingming He,Oliver Hermann,Oliver Pilarski,Rahul Garg,Paul Debevec,Ning Yu*

Main category: cs.CV

TL;DR: 提出了一个框架，通过定制数据管道，在视频扩散模型中实现多视角角色一致性和3D相机控制。


<details>
  <summary>Details</summary>
Motivation: 为了在视频扩散模型中实现多视角角色一致性和3D相机控制。

Method: 通过4D高斯溅射（4DGS）重新渲染的体积捕获性能，以及通过视频重新照明模型获得的光照变化，训练角色一致性组件。在此数据上微调最先进的开源视频扩散模型，以提供强大的多视角身份保留、精确的相机控制和光照适应性。

Result: 改进了视频质量，提高了个性化准确性，并增强了相机控制和光照适应性。

Conclusion: 该框架推进了视频生成与虚拟制作的集成。

Abstract: We introduce a framework that enables both multi-view character consistency
and 3D camera control in video diffusion models through a novel customization
data pipeline. We train the character consistency component with recorded
volumetric capture performances re-rendered with diverse camera trajectories
via 4D Gaussian Splatting (4DGS), lighting variability obtained with a video
relighting model. We fine-tune state-of-the-art open-source video diffusion
models on this data to provide strong multi-view identity preservation, precise
camera control, and lighting adaptability. Our framework also supports core
capabilities for virtual production, including multi-subject generation using
two approaches: joint training and noise blending, the latter enabling
efficient composition of independently customized models at inference time; it
also achieves scene and real-life video customization as well as control over
motion and spatial layout during customization. Extensive experiments show
improved video quality, higher personalization accuracy, and enhanced camera
control and lighting adaptability, advancing the integration of video
generation into virtual production. Our project page is available at:
https://eyeline-labs.github.io/Virtually-Being.

</details>


### [63] [Joint Modeling of Big Five and HEXACO for Multimodal Apparent Personality-trait Recognition](https://arxiv.org/abs/2510.14203)
*Ryo Masumura,Shota Orihashi,Mana Ihori,Tomohiro Tanaka,Naoki Makishima,Taiga Yamane,Naotaka Kawata,Satoshi Suzuki,Taichi Katayama*

Main category: cs.CV

TL;DR: 本文提出了一种联合建模方法，用于从多模态人类行为中自动识别表面人格特质，该方法同时考虑了心理学中长期研究的“大五”人格模型和最近备受关注的 HEXACO 人格模型。


<details>
  <summary>Details</summary>
Motivation: 以往研究大多使用“大五”人格模型进行多模态表面人格特质识别，但缺乏对 HEXACO 的关注，HEXACO 可以评估与转移性攻击和报复、社会支配导向等相关的诚实-谦逊特质。此外，尚未明确机器学习建模时“大五”人格模型和 HEXACO 人格模型之间的关系。希望通过考虑这些关系来提高对多模态人类行为的认知。

Method: 提出一种联合优化识别“大五”人格模型和 HEXACO 人格模型的方法。

Result: 使用自我介绍视频数据集进行的实验表明，该方法能够有效地识别“大五”人格模型和 HEXACO 人格模型。

Conclusion: 该方法能够有效地识别“大五”人格模型和 HEXACO 人格模型。

Abstract: This paper proposes a joint modeling method of the Big Five, which has long
been studied, and HEXACO, which has recently attracted attention in psychology,
for automatically recognizing apparent personality traits from multimodal human
behavior. Most previous studies have used the Big Five for multimodal apparent
personality-trait recognition. However, no study has focused on apparent HEXACO
which can evaluate an Honesty-Humility trait related to displaced aggression
and vengefulness, social-dominance orientation, etc. In addition, the
relationships between the Big Five and HEXACO when modeled by machine learning
have not been clarified. We expect awareness of multimodal human behavior to
improve by considering these relationships. The key advance of our proposed
method is to optimize jointly recognizing the Big Five and HEXACO. Experiments
using a self-introduction video dataset demonstrate that the proposed method
can effectively recognize the Big Five and HEXACO.

</details>


### [64] [LOTA: Bit-Planes Guided AI-Generated Image Detection](https://arxiv.org/abs/2510.14230)
*Hongsong Wang,Renxi Cheng,Yang Zhang,Chaolei Han,Jie Gui*

Main category: cs.CV

TL;DR: 提出了一种新的AI生成图像检测方法，该方法利用位平面图像处理来提取噪声特征，并通过最大梯度块选择和轻量级分类器来实现高效准确的检测。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像重建误差的AI生成图像检测方法计算成本高，且未能捕捉到原始图像中固有的噪声特征。

Method: 通过位平面图像处理改进误差提取，引入位平面引导的噪声图像生成，并利用图像归一化策略。设计最大梯度块选择来放大噪声信号，并提出轻量级分类头。

Result: 在GenImage基准测试中取得了98.9%的平均准确率，比现有方法提高了11.9%，并且具有良好的跨生成器泛化能力。速度比现有方法快近百倍。

Conclusion: 该方法能够高效准确地检测AI生成的图像，并在跨生成器的情况下表现出色。

Abstract: The rapid advancement of GAN and Diffusion models makes it more difficult to
distinguish AI-generated images from real ones. Recent studies often use
image-based reconstruction errors as an important feature for determining
whether an image is AI-generated. However, these approaches typically incur
high computational costs and also fail to capture intrinsic noisy features
present in the raw images. To solve these problems, we innovatively refine
error extraction by using bit-plane-based image processing, as lower bit planes
indeed represent noise patterns in images. We introduce an effective bit-planes
guided noisy image generation and exploit various image normalization
strategies, including scaling and thresholding. Then, to amplify the noise
signal for easier AI-generated image detection, we design a maximum gradient
patch selection that applies multi-directional gradients to compute the noise
score and selects the region with the highest score. Finally, we propose a
lightweight and effective classification head and explore two different
structures: noise-based classifier and noise-guided classifier. Extensive
experiments on the GenImage benchmark demonstrate the outstanding performance
of our method, which achieves an average accuracy of \textbf{98.9\%}
(\textbf{11.9}\%~$\uparrow$) and shows excellent cross-generator generalization
capability. Particularly, our method achieves an accuracy of over 98.2\% from
GAN to Diffusion and over 99.2\% from Diffusion to GAN. Moreover, it performs
error extraction at the millisecond level, nearly a hundred times faster than
existing methods. The code is at https://github.com/hongsong-wang/LOTA.

</details>


### [65] [PIA: Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic Analysis](https://arxiv.org/abs/2510.14241)
*Soumyya Kanti Datta,Tanvi Ranga,Chengzhe Sun,Siwei Lyu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的多模态音频-视频框架，用于检测deepfakes。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法无法充分识别由GANs、扩散模型和神经渲染等技术生成的高级deepfakes，因为它们会产生细微的时间差异。

Method: 该方法结合了语音序列、唇部几何数据和面部识别嵌入。

Result: 通过识别多种互补模态的不一致性，显著提高了对细微deepfake的检测。

Conclusion: 该研究提出了一种有效的deepfake检测框架。

Abstract: The rise of manipulated media has made deepfakes a particularly insidious
threat, involving various generative manipulations such as lip-sync
modifications, face-swaps, and avatar-driven facial synthesis. Conventional
detection methods, which predominantly depend on manually designed
phoneme-viseme alignment thresholds, fundamental frame-level consistency
checks, or a unimodal detection strategy, inadequately identify modern-day
deepfakes generated by advanced generative models such as GANs, diffusion
models, and neural rendering techniques. These advanced techniques generate
nearly perfect individual frames yet inadvertently create minor temporal
discrepancies frequently overlooked by traditional detectors. We present a
novel multimodal audio-visual framework, Phoneme-Temporal and Identity-Dynamic
Analysis(PIA), incorporating language, dynamic face motion, and facial
identification cues to address these limitations. We utilize phoneme sequences,
lip geometry data, and advanced facial identity embeddings. This integrated
method significantly improves the detection of subtle deepfake alterations by
identifying inconsistencies across multiple complementary modalities. Code is
available at https://github.com/skrantidatta/PIA

</details>


### [66] [Event Interval Modulation: A Novel Scheme for Event-based Optical Camera Communication](https://arxiv.org/abs/2510.14245)
*Miu Sumino,Mayu Ishii,Shun Kaizu,Daisuke Hisano,Yu Nakayama*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于事件的相机通信调制方案，称为事件间隔调制（EIM），该方案通过调制事件之间的时间间隔来提高传输速度，并在室内环境下实现了高达28 kbps的传输速率。


<details>
  <summary>Details</summary>
Motivation: 传统的基于帧的相机通信系统存在比特率低和处理负载高等问题。虽然基于事件的视觉传感器（EVS）的相机通信系统能够实现高速、低延迟和鲁棒的通信，但现有调制方法未能充分利用EVS的特性。

Method: 本文提出了一种专门为基于事件的相机通信设计的事件间隔调制（EIM）方案，并建立了EIM的理论模型。通过调整EVS的参数以优化频率响应，并实验确定了EIM中可用的最大调制阶数。

Result: 在室内环境下，10米距离实现了28 kbps的传输，50米距离实现了8.4 kbps的传输。这为基于事件的相机通信系统设定了新的比特率基准。

Conclusion: 本文提出的EIM方案能够有效提高基于事件的相机通信系统的传输速率，并在实验中取得了成功。

Abstract: Optical camera communication (OCC) represents a promising visible light
communication technology. Nonetheless, typical OCC systems utilizing
frame-based cameras are encumbered by limitations, including low bit rate and
high processing load. To address these issues, OCC system utilizing an
event-based vision sensor (EVS) as receivers have been proposed. The EVS
enables high-speed, low-latency, and robust communication due to its
asynchronous operation and high dynamic range. In existing event-based OCC
systems, conventional modulation schemes such as on-off keying (OOK) and pulse
position modulation have been applied, however, to the best of our knowledge,
no modulation method has been proposed that fully exploits the unique
characteristics of the EVS. This paper proposes a novel modulation scheme,
called the event interval modulation (EIM) scheme, specifically designed for
event-based OCC. EIM enables improvement in transmission speed by modulating
information using the intervals between events. This paper proposes a
theoretical model of EIM and conducts a proof-of-concept experiment. First, the
parameters of the EVS are tuned and customized to optimize the frequency
response specifically for EIM. Then, the maximum modulation order usable in EIM
is determined experimentally. We conduct transmission experiments based on the
obtained parameters. Finally, we report successful transmission at 28 kbps over
10 meters and 8.4 kbps over 50 meters in an indoor environment. This sets a new
benchmark for bit rate in event-based OCC systems.

</details>


### [67] [Acquisition of interpretable domain information during brain MR image harmonization for content-based image retrieval](https://arxiv.org/abs/2510.14535)
*Keima Abe,Hayato Muraki,Shuhei Tomoshige,Kenichi Oishi,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: 本文提出了一种名为PL-SE-ADA的领域协调框架，用于解决医学图像中由于扫描仪和协议差异导致的领域偏移问题，并提高模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 医学图像由于扫描仪和协议差异存在领域偏移，这会降低机器学习在疾病分类等任务中的性能。现有的领域协调方法缺乏可解释性，无法满足医学应用的需求。

Method: 该方法包括两个编码器（$f_E$和$f_{SE}$）用于提取领域不变特征（$\boldsymbol{z_u}$）和领域特定特征（$\boldsymbol{z_d}$），一个解码器用于重构图像（$f_D$），以及一个领域预测器（$g_D$）。通过对抗训练和重构损失，模型学习协调领域并保留疾病相关信息。

Result: PL-SE-ADA在图像重构、疾病分类和领域识别方面取得了与现有方法相当或更好的性能。此外，该方法还能够可视化领域独立和领域特定的脑部特征，提供了高可解释性。

Conclusion: PL-SE-ADA是一个通用的领域协调和可解释表示学习框架，能够有效解决医学图像中的领域偏移问题，并提供可解释的结果。

Abstract: Medical images like MR scans often show domain shifts across imaging sites
due to scanner and protocol differences, which degrade machine learning
performance in tasks such as disease classification. Domain harmonization is
thus a critical research focus. Recent approaches encode brain images
$\boldsymbol{x}$ into a low-dimensional latent space $\boldsymbol{z}$, then
disentangle it into $\boldsymbol{z_u}$ (domain-invariant) and
$\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, these
methods often lack interpretability$-$an essential requirement in medical
applications$-$leaving practical issues unresolved. We propose
Pseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), a
general framework for domain harmonization and interpretable representation
learning that preserves disease-relevant information in brain MR images.
PL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract
$\boldsymbol{z_u}$ and $\boldsymbol{z_d}$, a decoder to reconstruct the image
$f_D$, and a domain predictor $g_D$. Beyond adversarial training between the
encoder and domain predictor, the model learns to reconstruct the input image
$\boldsymbol{x}$ by summing reconstructions from $\boldsymbol{z_u}$ and
$\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Compared
to prior methods, PL-SE-ADA achieves equal or better performance in image
reconstruction, disease classification, and domain recognition. It also enables
visualization of both domain-independent brain features and domain-specific
components, offering high interpretability across the entire framework.

</details>


### [68] [MACE: Mixture-of-Experts Accelerated Coordinate Encoding for Large-Scale Scene Localization and Rendering](https://arxiv.org/abs/2510.14251)
*Mingkai Liu,Dikai Fan,Haohua Que,Haojia Gao,Xiao Liu,Shuxue Peng,Meixia Lin,Shengyu Gu,Ruicong Ye,Wanli Qiu,Handong Yao,Ruopeng Zhang,Xianliang Huang*

Main category: cs.CV

TL;DR: 提出了一种名为MACE的混合专家加速坐标编码方法，用于在大规模场景中实现高效定位和高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 在大规模场景中，高效定位和高质量渲染仍然是一个重大挑战，因为涉及到计算成本。虽然场景坐标回归（SCR）方法在小规模定位中表现良好，但当扩展到大规模场景时，它们受到单个网络容量的限制。

Method: 引入了一个门控网络来隐式地分类和选择子网络，确保在每次推理过程中只有一个子网络被激活。此外，提出了一种无辅助损失的负载平衡（ALF-LB）策略，以提高大规模场景的定位精度。

Result: 该框架在保持较高精度的同时，显著降低了成本，为大规模场景应用提供了一个有效的解决方案。

Conclusion: 在Cambridge测试集上的额外实验表明，该方法仅需10分钟的训练即可实现高质量的渲染结果。

Abstract: Efficient localization and high-quality rendering in large-scale scenes
remain a significant challenge due to the computational cost involved. While
Scene Coordinate Regression (SCR) methods perform well in small-scale
localization, they are limited by the capacity of a single network when
extended to large-scale scenes. To address these challenges, we propose the
Mixed Expert-based Accelerated Coordinate Encoding method (MACE), which enables
efficient localization and high-quality rendering in large-scale scenes.
Inspired by the remarkable capabilities of MOE in large model domains, we
introduce a gating network to implicitly classify and select sub-networks,
ensuring that only a single sub-network is activated during each inference.
Furtheremore, we present Auxiliary-Loss-Free Load Balancing(ALF-LB) strategy to
enhance the localization accuracy on large-scale scene. Our framework provides
a significant reduction in costs while maintaining higher precision, offering
an efficient solution for large-scale scene applications. Additional
experiments on the Cambridge test set demonstrate that our method achieves
high-quality rendering results with merely 10 minutes of training.

</details>


### [69] [Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization](https://arxiv.org/abs/2510.14255)
*Liao Shen,Wentao Jiang,Yiran Zhu,Tiezheng Ge,Zhiguo Cao,Bo Zheng*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为 Identity-Preserving Reward-guided Optimization (IPRO) 的新视频扩散框架，用于提高图像到视频 (I2V) 生成中身份保持的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的 I2V 模型在保持输入人像和生成的视频之间的身份一致性方面存在困难，尤其是在视频中的人表现出显著的表情变化和动作时。当人脸仅占图像的一小部分时，这个问题变得尤为关键。

Method: 该方法基于强化学习，使用人脸识别评分器优化扩散模型，通过采样链的最后步骤反向传播奖励信号，并提出了一种新的人脸评分机制，将ground-truth视频中的人脸视为面部特征池，以提供多角度的面部信息，同时加入KL散度正则化以稳定训练并防止对奖励信号的过拟合。

Result: 在 Wan 2.2 I2V 模型和内部 I2V 模型上的大量实验表明了该方法的有效性。

Conclusion: 该论文提出了一种有效的视频扩散框架，可以通过强化学习和人脸识别评分器来提高图像到视频生成中身份保持的一致性。

Abstract: Recent advances in image-to-video (I2V) generation have achieved remarkable
progress in synthesizing high-quality, temporally coherent videos from static
images. Among all the applications of I2V, human-centric video generation
includes a large portion. However, existing I2V models encounter difficulties
in maintaining identity consistency between the input human image and the
generated video, especially when the person in the video exhibits significant
expression changes and movements. This issue becomes critical when the human
face occupies merely a small fraction of the image. Since humans are highly
sensitive to identity variations, this poses a critical yet under-explored
challenge in I2V generation. In this paper, we propose Identity-Preserving
Reward-guided Optimization (IPRO), a novel video diffusion framework based on
reinforcement learning to enhance identity preservation. Instead of introducing
auxiliary modules or altering model architectures, our approach introduces a
direct and effective tuning algorithm that optimizes diffusion models using a
face identity scorer. To improve performance and accelerate convergence, our
method backpropagates the reward signal through the last steps of the sampling
chain, enabling richer gradient feedback. We also propose a novel facial
scoring mechanism that treats faces in ground-truth videos as facial feature
pools, providing multi-angle facial information to enhance generalization. A
KL-divergence regularization is further incorporated to stabilize training and
prevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V
model and our in-house I2V model demonstrate the effectiveness of our method.
Our project and code are available at
\href{https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/}.

</details>


### [70] [Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement Learning](https://arxiv.org/abs/2510.14256)
*Xiangyu Meng,Zixian Zhang,Zhenghao Zhang,Junchao Liao,Long Qin,Weizhi Wang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为Identity-GRPO的human feedback-driven优化流程，用于改进多人视频生成中的身份保持问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成方法在处理多人动态交互场景中的身份保持方面存在困难，无法保证视频中多个角色身份的一致性。

Method: 该方法构建了一个视频奖励模型，该模型基于大规模偏好数据集进行训练，并采用了一种针对多人一致性定制的GRPO变体。

Result: 实验结果表明，Identity-GRPO在人类一致性指标上比baseline方法提高了18.9%。

Conclusion: Identity-GRPO能够显著提升多人视频生成中身份保持的一致性，并为强化学习与个性化视频生成对齐提供了可操作的见解。

Abstract: While advanced methods like VACE and Phantom have advanced video generation
for specific subjects in diverse scenarios, they struggle with multi-human
identity preservation in dynamic interactions, where consistent identities
across multiple characters are critical. To address this, we propose
Identity-GRPO, a human feedback-driven optimization pipeline for refining
multi-human identity-preserving video generation. First, we construct a video
reward model trained on a large-scale preference dataset containing
human-annotated and synthetic distortion data, with pairwise annotations
focused on maintaining human consistency throughout the video. We then employ a
GRPO variant tailored for multi-human consistency, which greatly enhances both
VACE and Phantom. Through extensive ablation studies, we evaluate the impact of
annotation quality and design choices on policy optimization. Experiments show
that Identity-GRPO achieves up to 18.9% improvement in human consistency
metrics over baseline methods, offering actionable insights for aligning
reinforcement learning with personalized video generation.

</details>


### [71] [MatchAttention: Matching the Relative Positions for High-Resolution Cross-View Matching](https://arxiv.org/abs/2510.14260)
*Tingman Yan,Tao Liu,Xilian Yang,Qunfei Zhao,Zeyang Xia*

Main category: cs.CV

TL;DR: This paper introduces MatchAttention, a novel attention mechanism for cross-view matching that dynamically matches relative positions, achieving high accuracy and low computational complexity.


<details>
  <summary>Details</summary>
Motivation: Existing cross-attention mechanisms for high-resolution images suffer from quadratic complexity and lack explicit matching constraints.

Method: The paper proposes MatchAttention with BilinearSoftmax for continuous sliding-window attention sampling, iteratively updating relative positions. It also introduces MatchDecoder, gated cross-MatchAttention, and a consistency-constrained loss to handle occlusions.

Result: MatchStereo-B ranked 1st on the Middlebury benchmark and achieves state-of-the-art performance on KITTI 2012, KITTI 2015, ETH3D, and Spring flow datasets. MatchStereo-T can process 4K UHD images in 0.1 seconds using 3GB of GPU memory.

Conclusion: The proposed approach enables real-time, high-resolution, and high-accuracy cross-view matching.

Abstract: Cross-view matching is fundamentally achieved through cross-attention
mechanisms. However, matching of high-resolution images remains challenging due
to the quadratic complexity and lack of explicit matching constraints in the
existing cross-attention. This paper proposes an attention mechanism,
MatchAttention, that dynamically matches relative positions. The relative
position determines the attention sampling center of the key-value pairs given
a query. Continuous and differentiable sliding-window attention sampling is
achieved by the proposed BilinearSoftmax. The relative positions are
iteratively updated through residual connections across layers by embedding
them into the feature channels. Since the relative position is exactly the
learning target for cross-view matching, an efficient hierarchical cross-view
decoder, MatchDecoder, is designed with MatchAttention as its core component.
To handle cross-view occlusions, gated cross-MatchAttention and a
consistency-constrained loss are proposed. These two components collectively
mitigate the impact of occlusions in both forward and backward passes, allowing
the model to focus more on learning matching relationships. When applied to
stereo matching, MatchStereo-B ranked 1st in average error on the public
Middlebury benchmark and requires only 29ms for KITTI-resolution inference.
MatchStereo-T can process 4K UHD images in 0.1 seconds using only 3GB of GPU
memory. The proposed models also achieve state-of-the-art performance on KITTI
2012, KITTI 2015, ETH3D, and Spring flow datasets. The combination of high
accuracy and low computational complexity makes real-time, high-resolution, and
high-accuracy cross-view matching possible. Code is available at
https://github.com/TingmanYan/MatchAttention.

</details>


### [72] [Experimental Demonstration of Event-based Optical Camera Communication in Long-Range Outdoor Environment](https://arxiv.org/abs/2510.14266)
*Miu Sumino,Mayu Ishii,Shun Kaizu,Daisuke Hisano,Yu Nakayama*

Main category: cs.CV

TL;DR: 提出了一种基于事件视觉传感器的光相机通信系统的鲁棒解调方案，结合了 OOK、切换解调和数字锁相环。


<details>
  <summary>Details</summary>
Motivation: 为了在光相机通信系统中实现更鲁棒的解调。

Method: 结合了 OOK、切换解调和数字锁相环。

Result: 在室外实验中，在200米-60kbps和400米-30kbps的条件下，实现了 $\mathrm{BER} < 10^{-3}$。

Conclusion: 该方案在室外光相机通信中表现出良好的性能。

Abstract: We propose a robust demodulation scheme for optical camera communication
systems using an event-based vision sensor, combining OOK with toggle
demodulation and a digital phase-locked loop. This is the first report to
achieve a $\mathrm{BER} < 10^{-3}$ at 200m-60kbps and 400m-30kbps in outdoor
experiments.

</details>


### [73] [GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering](https://arxiv.org/abs/2510.14270)
*Alexander Valverde,Brian Xu,Yuyin Zhou,Meng Xu,Hongyun Wang*

Main category: cs.CV

TL;DR: 提出了一种名为 GauSSmart 的混合方法，它结合了 2D 基础模型和 3D Gaussian Splatting 重建，以增强场景重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有的 Gaussian Splatting 方法在捕捉精细细节或在稀疏覆盖区域保持真实感方面存在困难，这主要是由于稀疏 3D 训练数据的固有局限性。

Method: 该方法集成了 2D 计算机视觉技术，包括凸滤波和来自 DINO 等基础模型的语义特征监督，以增强基于高斯的场景重建。通过利用 2D 分割先验和高维特征嵌入，该方法引导 Gaussian splats 的密集化和细化，从而改善覆盖不足区域的覆盖范围并保留复杂的结构细节。

Result: 在三个数据集上的验证结果表明，GauSSmart 在大多数评估场景中始终优于现有的 Gaussian Splatting 方法。

Conclusion: 该研究结果证明了 2D-3D 混合方法的巨大潜力，突出了 2D 基础模型与 3D 重建管道的巧妙结合如何克服两种方法固有的局限性。

Abstract: Scene reconstruction has emerged as a central challenge in computer vision,
with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting
achieving remarkable progress. While Gaussian Splatting demonstrates strong
performance on large-scale datasets, it often struggles to capture fine details
or maintain realism in regions with sparse coverage, largely due to the
inherent limitations of sparse 3D training data.
  In this work, we propose GauSSmart, a hybrid method that effectively bridges
2D foundational models and 3D Gaussian Splatting reconstruction. Our approach
integrates established 2D computer vision techniques, including convex
filtering and semantic feature supervision from foundational models such as
DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D
segmentation priors and high-dimensional feature embeddings, our method guides
the densification and refinement of Gaussian splats, improving coverage in
underrepresented areas and preserving intricate structural details.
  We validate our approach across three datasets, where GauSSmart consistently
outperforms existing Gaussian Splatting in the majority of evaluated scenes.
Our results demonstrate the significant potential of hybrid 2D-3D approaches,
highlighting how the thoughtful combination of 2D foundational models with 3D
reconstruction pipelines can overcome the limitations inherent in either
approach alone.

</details>


### [74] [CLEAR: Causal Learning Framework For Robust Histopathology Tumor Detection Under Out-Of-Distribution Shifts](https://arxiv.org/abs/2510.14273)
*Kieu-Anh Truong Thi,Huy-Hieu Pham,Duc-Trong Le*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的基于因果推理的框架，通过利用语义特征并减轻混淆因素的影响来解决组织病理学中的领域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖于通过对齐特征分布或引入统计变化来建模统计相关性，但它们通常忽略了因果关系。因此，本文旨在解决组织病理学中领域偏移的问题。

Method: 该方法通过设计明确包含中介和观察到的组织切片的转换策略来实现前门原理。

Result: 在CAMELYON17数据集和一个私有组织病理学数据集上验证了该方法，在未见过的领域中表现出持续的性能提升。在CAMELYON17数据集和私有组织病理学数据集上，该方法都实现了高达7%的改进，优于现有基线。

Conclusion: 这些结果突出了因果推理作为解决组织病理学图像分析中领域偏移的强大工具的潜力。

Abstract: Domain shift in histopathology, often caused by differences in acquisition
processes or data sources, poses a major challenge to the generalization
ability of deep learning models. Existing methods primarily rely on modeling
statistical correlations by aligning feature distributions or introducing
statistical variation, yet they often overlook causal relationships. In this
work, we propose a novel causal-inference-based framework that leverages
semantic features while mitigating the impact of confounders. Our method
implements the front-door principle by designing transformation strategies that
explicitly incorporate mediators and observed tissue slides. We validate our
method on the CAMELYON17 dataset and a private histopathology dataset,
demonstrating consistent performance gains across unseen domains. As a result,
our approach achieved up to a 7% improvement in both the CAMELYON17 dataset and
the private histopathology dataset, outperforming existing baselines. These
results highlight the potential of causal inference as a powerful tool for
addressing domain shift in histopathology image analysis.

</details>


### [75] [Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding](https://arxiv.org/abs/2510.14304)
*Kyungryul Back,Seongbeom Park,Milim Kim,Mincheol Kwon,SangHyeok Lee,Hyunyoung Lee,Junhee Cho,Seunghyun Park,Jinkyu Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种无需训练的三层对比解码与水印技术，以减少大型视觉语言模型 (LVLMs) 中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型 (LVLMs) 容易产生幻觉，过度依赖单一模态或记忆训练数据，而不能正确地根据视觉信息生成输出。

Method: 该方法包括三个步骤：(1) 选择成熟层和业余层；(2) 使用水印相关问题识别枢轴层，以评估该层是否具有良好的视觉基础；(3) 应用三层对比解码生成最终输出。

Result: 在 POPE、MME 和 AMBER 等公共基准上的实验表明，该方法在减少 LVLMs 中的幻觉方面取得了最先进的性能，并生成了更符合视觉基础的响应。

Conclusion: 该研究提出了一种有效的方法来减少大型视觉语言模型中的幻觉问题，并生成更符合视觉基础的响应。

Abstract: Large Vision-Language Models (LVLMs) have recently shown promising results on
various multimodal tasks, even achieving human-comparable performance in
certain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often
rely heavily on a single modality or memorize training data without properly
grounding their outputs. To address this, we propose a training-free, tri-layer
contrastive decoding with watermarking, which proceeds in three steps: (1)
select a mature layer and an amateur layer among the decoding layers, (2)
identify a pivot layer using a watermark-related question to assess whether the
layer is visually well-grounded, and (3) apply tri-layer contrastive decoding
to generate the final output. Experiments on public benchmarks such as POPE,
MME and AMBER demonstrate that our method achieves state-of-the-art performance
in reducing hallucinations in LVLMs and generates more visually grounded
responses.

</details>


### [76] [A Multi-domain Image Translative Diffusion StyleGAN for Iris Presentation Attack Detection](https://arxiv.org/abs/2510.14314)
*Shivangi Yadav,Arun Ross*

Main category: cs.CV

TL;DR: 提出了一个名为MID-StyleGAN的新框架，用于生成合成的眼部图像，以解决虹膜PAD技术训练和评估数据集稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 虹膜生物识别系统容易受到呈现攻击的威胁，但用于训练和评估虹膜PAD技术的数据集非常稀缺。

Method: 结合了扩散模型和生成对抗网络（GANs）的优势，采用多域架构，实现真实眼部图像和不同PA域之间的转换，并使用为眼部数据定制的自适应损失函数。

Result: MID-StyleGAN在生成高质量合成眼部图像方面优于现有方法。在LivDet2020数据集上，在1%的错误检测率下，真实检测率从93.41%提高到98.72%。

Conclusion: 该方法生成的数据显著提高了PAD系统的性能，为虹膜和眼部生物识别技术中数据稀缺问题提供了一个可扩展的解决方案。

Abstract: An iris biometric system can be compromised by presentation attacks (PAs)
where artifacts such as artificial eyes, printed eye images, or cosmetic
contact lenses are presented to the system. To counteract this, several
presentation attack detection (PAD) methods have been developed. However, there
is a scarcity of datasets for training and evaluating iris PAD techniques due
to the implicit difficulties in constructing and imaging PAs. To address this,
we introduce the Multi-domain Image Translative Diffusion StyleGAN
(MID-StyleGAN), a new framework for generating synthetic ocular images that
captures the PA and bonafide characteristics in multiple domains such as
bonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the
strengths of diffusion models and generative adversarial networks (GANs) to
produce realistic and diverse synthetic data. Our approach utilizes a
multi-domain architecture that enables the translation between bonafide ocular
images and different PA domains. The model employs an adaptive loss function
tailored for ocular data to maintain domain consistency. Extensive experiments
demonstrate that MID-StyleGAN outperforms existing methods in generating
high-quality synthetic ocular images. The generated data was used to
significantly enhance the performance of PAD systems, providing a scalable
solution to the data scarcity problem in iris and ocular biometrics. For
example, on the LivDet2020 dataset, the true detect rate at 1% false detect
rate improved from 93.41% to 98.72%, showcasing the impact of the proposed
method.

</details>


### [77] [Vision-Centric Activation and Coordination for Multimodal Large Language Models](https://arxiv.org/abs/2510.14349)
*Yunnan Wang,Fan Lu,Kecheng Zheng,Ziyuan Huang,Ziqiang Li,Wenjun Zeng,Xin Jin*

Main category: cs.CV

TL;DR: VaCo通过结合多个视觉基础模型（VFM）的视觉信息来优化多模态大型语言模型（MLLM）的表示，从而提升其视觉理解能力。


<details>
  <summary>Details</summary>
Motivation: 主流的MLLM仅通过文本token的下一个token预测进行监督，忽略了以视觉为中心的关键信息，这对于分析能力至关重要。

Method: VaCo引入了视觉区分对齐来整合从VFM中提取的任务感知感知特征，从而统一了MLLM中文本和视觉输出的优化。具体来说，它将可学习的模块化任务查询（MTQ）和视觉对齐层（VAL）整合到MLLM中，并在不同VFM的监督下激活特定的视觉信号。为了协调VFM之间表示的冲突，设计的Token Gateway Mask（TGM）限制了多组MTQ之间的信息流。

Result: 大量实验表明，VaCo显著提高了不同MLLM在各种基准测试上的性能，展示了其在视觉理解方面的卓越能力。

Conclusion: VaCo通过优化MLLM的视觉表示，显著提高了其视觉理解能力。

Abstract: Multimodal large language models (MLLMs) integrate image features from visual
encoders with LLMs, demonstrating advanced comprehension capabilities. However,
mainstream MLLMs are solely supervised by the next-token prediction of textual
tokens, neglecting critical vision-centric information essential for analytical
abilities. To track this dilemma, we introduce VaCo, which optimizes MLLM
representations through Vision-Centric activation and Coordination from
multiple vision foundation models (VFMs). VaCo introduces visual discriminative
alignment to integrate task-aware perceptual features extracted from VFMs,
thereby unifying the optimization of both textual and visual outputs in MLLMs.
Specifically, we incorporate the learnable Modular Task Queries (MTQs) and
Visual Alignment Layers (VALs) into MLLMs, activating specific visual signals
under the supervision of diverse VFMs. To coordinate representation conflicts
across VFMs, the crafted Token Gateway Mask (TGM) restricts the information
flow among multiple groups of MTQs. Extensive experiments demonstrate that VaCo
significantly improves the performance of different MLLMs on various
benchmarks, showcasing its superior capabilities in visual comprehension.

</details>


### [78] [Leveraging Cycle-Consistent Anchor Points for Self-Supervised RGB-D Registration](https://arxiv.org/abs/2510.14354)
*Siddharth Tourani,Jayaram Reddy,Sarvesh Thakur,K Madhava Krishna,Muhammad Haris Khan,N Dinesh Reddy*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的自监督RGB-D配准方法，该方法利用循环一致的关键点和新的姿态块来提高配准精度。


<details>
  <summary>Details</summary>
Motivation: 利用大量无标签RGB-D数据进行场景几何推理。

Method: 使用循环一致的关键点作为显著点来加强匹配过程中的空间一致性约束，并引入结合了GRU循环单元和变换同步的姿态块，融合历史和多视角数据。

Result: 该方法在ScanNet和3DMatch数据集上超过了以往的自监督配准方法，甚至优于一些较早的监督方法。将其集成到现有方法中也显示出其有效性。

Conclusion: 该方法有效地利用了无标签RGB-D数据，并在RGB-D配准任务中取得了显著的成果。

Abstract: With the rise in consumer depth cameras, a wealth of unlabeled RGB-D data has
become available. This prompts the question of how to utilize this data for
geometric reasoning of scenes. While many RGB-D registration meth- ods rely on
geometric and feature-based similarity, we take a different approach. We use
cycle-consistent keypoints as salient points to enforce spatial coherence
constraints during matching, improving correspondence accuracy. Additionally,
we introduce a novel pose block that combines a GRU recurrent unit with
transformation synchronization, blending historical and multi-view data. Our
approach surpasses previous self- supervised registration methods on ScanNet
and 3DMatch, even outperforming some older supervised methods. We also
integrate our components into existing methods, showing their effectiveness.

</details>


### [79] [Spatial Preference Rewarding for MLLMs Spatial Understanding](https://arxiv.org/abs/2510.14374)
*Han Qiu,Peng Gao,Lewei Lu,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: 本文提出了一种名为SPR（Spatial Preference Rewarding）的方法，通过奖励MLLM更详细和精确的物体定位，来提高MLLM的细粒度空间感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLM在细粒度空间感知能力方面存在不足，并且无法准确响应用户对细粒度空间理解的需求。现有方法主要集中于调整MLLM以模拟预先注释的指令数据，而缺乏对MLLM实际响应的直接监督。

Method: SPR方法通过语义和定位分数来评估MLLM生成的描述文本质量和定位质量。通过优化MLLM描述，并结合直接偏好优化，增强了与视觉输入的细粒度对齐。

Result: 在标准参考和基础基准上的大量实验表明，SPR能够有效提高MLLM的空间理解能力，且训练开销极小。

Conclusion: SPR方法通过空间偏好奖励，有效地提升了MLLM在细粒度空间感知方面的能力。

Abstract: Multimodal large language models~(MLLMs) have demonstrated promising spatial
understanding capabilities, such as referencing and grounding object
descriptions. Despite their successes, MLLMs still fall short in fine-grained
spatial perception abilities, such as generating detailed region descriptions
or accurately localizing objects. Additionally, they often fail to respond to
the user's requirements for desired fine-grained spatial understanding. This
issue might arise because existing approaches primarily focus on tuning MLLMs
to model pre-annotated instruction data to inject spatial knowledge, without
direct supervision of MLLMs' actual responses. We address this issue by SPR, a
Spatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatial
capabilities by rewarding MLLMs' detailed responses with precise object
localization over vague or inaccurate responses. With randomly selected image
regions and region descriptions from MLLMs, SPR introduces semantic and
localization scores to comprehensively evaluate the text quality and
localization quality in MLLM-generated descriptions. We also refine the MLLM
descriptions with better localization accuracy and pair the best-scored
refinement with the initial descriptions of the lowest score for direct
preference optimization, thereby enhancing fine-grained alignment with visual
input. Extensive experiments over standard referring and grounding benchmarks
show that SPR improves MLLM spatial understanding capabilities effectively with
minimal overhead in training. Data and code will be released at
https://github.com/hanqiu-hq/SPR

</details>


### [80] [DOS: Directional Object Separation in Text Embeddings for Multi-Object Image Generation](https://arxiv.org/abs/2510.14376)
*Dongnam Byun,Jungwon Park,Jumgmin Ko,Changin Choi,Wonjong Rhee*

Main category: cs.CV

TL;DR: 本文提出了一种名为DOS（Directional Object Separation）的方法，用于改进多对象图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成模型在处理包含多个对象的提示时，经常出现对象忽略或对象混合的问题。通过研究，作者识别出四个问题场景：相似形状、相似纹理、不同背景偏差和多个对象，这些场景中对象间的关系容易导致失败。

Method: 该方法基于CLIP嵌入的两个关键观察，修改三种CLIP文本嵌入，然后将其传递到文本到图像模型中。

Result: 实验结果表明，DOS持续提高了多对象图像生成的成功率，并减少了对象混合。在人工评估中，DOS明显优于四个竞争方法，在四个基准测试中获得了26.24%-43.04%的更多投票。

Conclusion: 这些结果表明，DOS是改进多对象图像生成的一种实用且有效的解决方案。

Abstract: Recent progress in text-to-image (T2I) generative models has led to
significant improvements in generating high-quality images aligned with text
prompts. However, these models still struggle with prompts involving multiple
objects, often resulting in object neglect or object mixing. Through extensive
studies, we identify four problematic scenarios, Similar Shapes, Similar
Textures, Dissimilar Background Biases, and Many Objects, where inter-object
relationships frequently lead to such failures. Motivated by two key
observations about CLIP embeddings, we propose DOS (Directional Object
Separation), a method that modifies three types of CLIP text embeddings before
passing them into text-to-image models. Experimental results show that DOS
consistently improves the success rate of multi-object image generation and
reduces object mixing. In human evaluations, DOS significantly outperforms four
competing methods, receiving 26.24%-43.04% more votes across four benchmarks.
These results highlight DOS as a practical and effective solution for improving
multi-object image generation.

</details>


### [81] [DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with Analytical Insights](https://arxiv.org/abs/2510.14383)
*Danish Ali,Ajmal Mian,Naveed Akhtar,Ghulam Mubashar Hassan*

Main category: cs.CV

TL;DR: 提出了一种名为双分辨率双向Mamba (DRBD-Mamba) 的高效 3D 分割模型，该模型以最小的计算开销捕获多尺度远程依赖关系。


<details>
  <summary>Details</summary>
Motivation: 准确的脑肿瘤分割对于临床诊断和治疗具有重要意义。由于肿瘤亚区域的异质性，它具有挑战性。基于 Mamba 的状态空间模型已显示出良好的性能。然而，由于跨多个空间轴的顺序特征计算，它们会产生大量的计算开销。此外，它们在不同的 BraTS 数据分区中的稳健性在很大程度上仍未被探索，这在可靠的评估中留下了一个关键的空白。

Method: 利用空间填充曲线在 3D 到 1D 特征映射期间保留空间局部性，从而减少对计算昂贵的多轴特征扫描的依赖。为了丰富特征表示，我们提出了一个门控融合模块，该模块自适应地整合前向和反向上下文，以及一个量化块，该量化块离散化特征以提高鲁棒性。此外，我们提出了 BraTS2023 上的五个系统折叠，用于在不同条件下对分割技术进行严格评估，并详细分析了常见的失败场景。

Result: 在最新方法使用的 20% 测试集上，我们的模型在整个肿瘤的 Dice 改善了 0.10%，肿瘤核心的 Dice 改善了 1.75%，增强肿瘤的 Dice 改善了 0.93%。在提出的系统五折评估中表明，我们的模型保持了具有竞争力的全肿瘤准确率，同时实现了肿瘤核心 0.86% 和增强肿瘤 1.45% 的平均 Dice 增益，优于现有的最先进水平。

Conclusion: 我们的模型在保持高分割精度的同时，效率提高了 15 倍，突出了其相对于现有方法的稳健性和计算优势。

Abstract: Accurate brain tumor segmentation is significant for clinical diagnosis and
treatment. It is challenging due to the heterogeneity of tumor subregions.
Mamba-based State Space Models have demonstrated promising performance.
However, they incur significant computational overhead due to sequential
feature computation across multiple spatial axes. Moreover, their robustness
across diverse BraTS data partitions remains largely unexplored, leaving a
critical gap in reliable evaluation. To address these limitations, we propose
dual-resolution bi-directional Mamba (DRBD-Mamba), an efficient 3D segmentation
model that captures multi-scale long-range dependencies with minimal
computational overhead. We leverage a space-filling curve to preserve spatial
locality during 3D-to-1D feature mapping, thereby reducing reliance on
computationally expensive multi-axial feature scans. To enrich feature
representation, we propose a gated fusion module that adaptively integrates
forward and reverse contexts, along with a quantization block that discretizes
features to improve robustness. In addition, we propose five systematic folds
on BraTS2023 for rigorous evaluation of segmentation techniques under diverse
conditions and present detailed analysis of common failure scenarios. On the
20\% test set used by recent methods, our model achieves Dice improvements of
0.10\% for whole tumor, 1.75\% for tumor core, and 0.93\% for enhancing tumor.
Evaluations on the proposed systematic five folds demonstrate that our model
maintains competitive whole tumor accuracy while achieving clear average Dice
gains of 0.86\% for tumor core and 1.45\% for enhancing tumor over existing
state-of-the-art. Furthermore, our model attains 15 times improvement in
efficiency while maintaining high segmentation accuracy, highlighting its
robustness and computational advantage over existing approaches.

</details>


### [82] [BoardVision: Deployment-ready and Robust Motherboard Defect Detection with YOLO+Faster-RCNN Ensemble](https://arxiv.org/abs/2510.14389)
*Brandon Hill,Kma Solaiman*

Main category: cs.CV

TL;DR: 提出了一种用于检测主板组装缺陷（如缺少螺丝、风扇接线松动和表面划痕）的可复现框架BoardVision。


<details>
  <summary>Details</summary>
Motivation: 确保大批量电子产品制造的可靠性，主板缺陷检测至关重要。之前的PCB检测研究主要集中在裸板或trace-level缺陷，而完整主板的组装级检测仍未被充分探索。

Method: 在MiracleFactory主板数据集上，对YOLOv7和Faster R-CNN两个代表性检测器在受控条件下进行了基准测试，并提出了一个轻量级集成方法，即置信度-时间投票（CTV Voter），通过可解释的规则来平衡精度和召回率。

Result: YOLO在精度方面表现出色，但在召回率方面表现不佳，而Faster R-CNN则相反。在实际扰动（包括清晰度、亮度和方向变化）下评估了鲁棒性，突出了主板缺陷检测中经常被忽视的稳定性挑战。

Conclusion: 展示了计算机视觉技术如何从基准测试结果过渡到用于组装级主板制造的实际质量保证。

Abstract: Motherboard defect detection is critical for ensuring reliability in
high-volume electronics manufacturing. While prior research in PCB inspection
has largely targeted bare-board or trace-level defects, assembly-level
inspection of full motherboards inspection remains underexplored. In this work,
we present BoardVision, a reproducible framework for detecting assembly-level
defects such as missing screws, loose fan wiring, and surface scratches. We
benchmark two representative detectors - YOLOv7 and Faster R-CNN, under
controlled conditions on the MiracleFactory motherboard dataset, providing the
first systematic comparison in this domain. To mitigate the limitations of
single models, where YOLO excels in precision but underperforms in recall and
Faster R-CNN shows the reverse, we propose a lightweight ensemble,
Confidence-Temporal Voting (CTV Voter), that balances precision and recall
through interpretable rules. We further evaluate robustness under realistic
perturbations including sharpness, brightness, and orientation changes,
highlighting stability challenges often overlooked in motherboard defect
detection. Finally, we release a deployable GUI-driven inspection tool that
bridges research evaluation with operator usability. Together, these
contributions demonstrate how computer vision techniques can transition from
benchmark results to practical quality assurance for assembly-level motherboard
manufacturing.

</details>


### [83] [DCMIL: A Progressive Representation Learning Model of Whole Slide Images for Cancer Prognosis Analysis](https://arxiv.org/abs/2510.14403)
*Chao Tu,Kun Huang,Jie Zhang,Qianjin Feng,Yu Zhang,Zhenyuan Ning*

Main category: cs.CV

TL;DR: 提出了一种名为DCMIL的易到难渐进式表示学习模型，用于高效处理WSI以进行癌症预后，无需密集注释。


<details>
  <summary>Details</summary>
Motivation: 计算病理学在利用全切片图像（WSI）量化形态异质性和开发人类癌症的客观预后模式方面显示出前景。然而，由于千兆像素大小的输入的计算瓶颈和密集手动注释的稀缺性，进展受到阻碍。当前的方法通常忽略跨多放大倍数WSI的细粒度信息以及肿瘤微环境的变化。

Method: 提出了一种易到难渐进式表示学习模型，称为双课程对比多实例学习（DCMIL）。

Result: 在十二种癌症类型（5,954名患者，1254万个切片）上的大量实验表明，DCMIL优于标准的基于WSI的预后模型。

Conclusion: DCMIL可以识别细粒度的预后显着区域，提供稳健的实例不确定性估计，并捕获正常组织和肿瘤组织之间的形态差异，具有产生新的生物学见解的潜力。

Abstract: The burgeoning discipline of computational pathology shows promise in
harnessing whole slide images (WSIs) to quantify morphological heterogeneity
and develop objective prognostic modes for human cancers. However, progress is
impeded by the computational bottleneck of gigapixel-size inputs and the
scarcity of dense manual annotations. Current methods often overlook
fine-grained information across multi-magnification WSIs and variations in
tumor microenvironments. Here, we propose an easy-to-hard progressive
representation learning model, termed dual-curriculum contrastive
multi-instance learning (DCMIL), to efficiently process WSIs for cancer
prognosis. The model does not rely on dense annotations and enables the direct
transformation of gigapixel-size WSIs into outcome predictions. Extensive
experiments on twelve cancer types (5,954 patients, 12.54 million tiles)
demonstrate that DCMIL outperforms standard WSI-based prognostic models.
Additionally, DCMIL identifies fine-grained prognosis-salient regions, provides
robust instance uncertainty estimation, and captures morphological differences
between normal and tumor tissues, with the potential to generate new biological
insights. All codes have been made publicly accessible at
https://github.com/tuuuc/DCMIL.

</details>


### [84] [Real-Time Neural Video Compression with Unified Intra and Inter Coding](https://arxiv.org/abs/2510.14431)
*Hui Xiang,Yifan Bian,Li Li,Jingran Wu,Xianguo Zhang,Dong Liu*

Main category: cs.CV

TL;DR: 提出了一种新的神经视频压缩（NVC）框架，该框架结合了帧内和帧间编码，以解决现有NVC方案的局限性，并在压缩效率和实时性能方面优于DCVC-RT。


<details>
  <summary>Details</summary>
Motivation: 现有的神经视频压缩（NVC）方案在处理遮挡和新内容、帧间误差传播和累积等方面存在局限性。

Method: 借鉴经典视频编码方案的思想，允许在帧间编码帧内进行帧内编码。提出了一个具有统一帧内和帧间编码的NVC框架，其中每个帧都由一个经过训练的单一模型处理，以自适应地执行帧内/帧间编码。此外，还提出了一种同步双帧压缩设计，以向前和向后利用帧间冗余。

Result: 该方案的BD-rate降低了平均10.7%，提供了更稳定的每帧比特率和质量，并保持了实时编码/解码性能，优于DCVC-RT。

Conclusion: 提出的NVC框架通过结合帧内和帧间编码，有效解决了现有NVC方案的局限性，并在压缩效率和实时性能方面取得了显著提升。

Abstract: Neural video compression (NVC) technologies have advanced rapidly in recent
years, yielding state-of-the-art schemes such as DCVC-RT that offer superior
compression efficiency to H.266/VVC and real-time encoding/decoding
capabilities. Nonetheless, existing NVC schemes have several limitations,
including inefficiency in dealing with disocclusion and new content, interframe
error propagation and accumulation, among others. To eliminate these
limitations, we borrow the idea from classic video coding schemes, which allow
intra coding within inter-coded frames. With the intra coding tool enabled,
disocclusion and new content are properly handled, and interframe error
propagation is naturally intercepted without the need for manual refresh
mechanisms. We present an NVC framework with unified intra and inter coding,
where every frame is processed by a single model that is trained to perform
intra/inter coding adaptively. Moreover, we propose a simultaneous two-frame
compression design to exploit interframe redundancy not only forwardly but also
backwardly. Experimental results show that our scheme outperforms DCVC-RT by an
average of 10.7\% BD-rate reduction, delivers more stable bitrate and quality
per frame, and retains real-time encoding/decoding performances. Code and
models will be released.

</details>


### [85] [Structured Universal Adversarial Attacks on Object Detection for Video Sequences](https://arxiv.org/abs/2510.14460)
*Sven Jacob,Weijia Shao,Gjergji Kasneci*

Main category: cs.CV

TL;DR: 提出了一种针对视频对象检测的通用对抗攻击方法，该方法通过核范数正则化来促进集中在背景中的结构化扰动。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习的对象检测器容易受到对抗性攻击，特别是涉及通用扰动的攻击。

Method: 利用核范数正则化来促进集中在背景中的结构化扰动，并采用自适应乐观指数梯度方法来有效优化此公式。

Result: 所提出的攻击在有效性方面优于低秩投影梯度下降和基于 Frank-Wolfe 的攻击，同时保持了较高的隐蔽性。

Conclusion: 提出了一种有效的、隐蔽的视频对象检测通用对抗攻击方法。

Abstract: Video-based object detection plays a vital role in safety-critical
applications. While deep learning-based object detectors have achieved
impressive performance, they remain vulnerable to adversarial attacks,
particularly those involving universal perturbations. In this work, we propose
a minimally distorted universal adversarial attack tailored for video object
detection, which leverages nuclear norm regularization to promote structured
perturbations concentrated in the background. To optimize this formulation
efficiently, we employ an adaptive, optimistic exponentiated gradient method
that enhances both scalability and convergence. Our results demonstrate that
the proposed attack outperforms both low-rank projected gradient descent and
Frank-Wolfe based attacks in effectiveness while maintaining high stealthiness.
All code and data are publicly available at
https://github.com/jsve96/AO-Exp-Attack.

</details>


### [86] [Unsupervised Deep Generative Models for Anomaly Detection in Neuroimaging: A Systematic Scoping Review](https://arxiv.org/abs/2510.14462)
*Youwan Mahé,Elise Bannier,Stéphanie Leplaideur,Elisa Fromont,Francesca Galassi*

Main category: cs.CV

TL;DR: 无监督深度生成模型用于检测和分割脑部影像异常，通过学习健康数据来识别与正常脑结构的偏差。


<details>
  <summary>Details</summary>
Motivation: 监督方法需要大量标注数据且受限于已充分表征的病理，而无监督模型仅需健康数据即可。

Method: 综述了2018-2025年间发表的49项关于神经影像异常检测的无监督深度生成模型研究，包括自编码器、变分自编码器、生成对抗网络和去噪扩散模型。

Result: 生成模型在大型局灶性病变中表现出令人鼓舞的性能，并在解决更细微的异常方面取得了进展。它们能够生成可解释的伪健康重建。

Conclusion: 生成模型为异常检测提供了一个引人注目的方向，能够实现半监督学习，支持发现新的影像生物标志物，并促进统一的端到端框架内的疾病内和跨疾病偏差映射。未来的工作应优先考虑解剖学建模、基础模型开发、任务适当的评估指标和严格的临床验证。

Abstract: Unsupervised deep generative models are emerging as a promising alternative
to supervised methods for detecting and segmenting anomalies in brain imaging.
Unlike fully supervised approaches, which require large voxel-level annotated
datasets and are limited to well-characterised pathologies, these models can be
trained exclusively on healthy data and identify anomalies as deviations from
learned normative brain structures. This PRISMA-guided scoping review
synthesises recent work on unsupervised deep generative models for anomaly
detection in neuroimaging, including autoencoders, variational autoencoders,
generative adversarial networks, and denoising diffusion models. A total of 49
studies published between 2018 - 2025 were identified, covering applications to
brain MRI and, less frequently, CT across diverse pathologies such as tumours,
stroke, multiple sclerosis, and small vessel disease. Reported performance
metrics are compared alongside architectural design choices. Across the
included studies, generative models achieved encouraging performance for large
focal lesions and demonstrated progress in addressing more subtle
abnormalities. A key strength of generative models is their ability to produce
interpretable pseudo-healthy (also referred to as counterfactual)
reconstructions, which is particularly valuable when annotated data are scarce,
as in rare or heterogeneous diseases. Looking ahead, these models offer a
compelling direction for anomaly detection, enabling semi-supervised learning,
supporting the discovery of novel imaging biomarkers, and facilitating within-
and cross-disease deviation mapping in unified end-to-end frameworks. To
realise clinical impact, future work should prioritise anatomy-aware modelling,
development of foundation models, task-appropriate evaluation metrics, and
rigorous clinical validation.

</details>


### [87] [Pruning Overparameterized Multi-Task Networks for Degraded Web Image Restoration](https://arxiv.org/abs/2510.14463)
*Thomas Katraouras,Dimitrios Rafailidis*

Main category: cs.CV

TL;DR: 本文提出了一种压缩多任务图像恢复模型的方法，通过迭代剪枝策略发现稀疏子网络，在保持或超过现有最佳性能的同时，显著减少模型参数量。


<details>
  <summary>Details</summary>
Motivation: 在线社交网络(OSN)中的有损操作会导致图像质量下降，影响用户体验。多任务图像恢复模型参数过多，计算效率低。

Method: 提出名为MIR-L的模型，采用迭代剪枝策略，移除低幅度权重，并将剩余权重重置为原始初始化。

Result: 在去雨、去雾和去噪任务的基准数据集上的实验评估表明，MIR-L仅保留10%的可训练参数，同时保持了较高的图像恢复性能。

Conclusion: 该方法能够在高稀疏度下维持或超过现有最佳性能。

Abstract: Image quality is a critical factor in delivering visually appealing content
on web platforms. However, images often suffer from degradation due to lossy
operations applied by online social networks (OSNs), negatively affecting user
experience. Image restoration is the process of recovering a clean high-quality
image from a given degraded input. Recently, multi-task (all-in-one) image
restoration models have gained significant attention, due to their ability to
simultaneously handle different types of image degradations. However, these
models often come with an excessively high number of trainable parameters,
making them computationally inefficient. In this paper, we propose a strategy
for compressing multi-task image restoration models. We aim to discover highly
sparse subnetworks within overparameterized deep models that can match or even
surpass the performance of their dense counterparts. The proposed model, namely
MIR-L, utilizes an iterative pruning strategy that removes low-magnitude
weights across multiple rounds, while resetting the remaining weights to their
original initialization. This iterative process is important for the multi-task
image restoration model's optimization, effectively uncovering "winning
tickets" that maintain or exceed state-of-the-art performance at high sparsity
levels. Experimental evaluation on benchmark datasets for the deraining,
dehazing, and denoising tasks shows that MIR-L retains only 10% of the
trainable parameters while maintaining high image restoration performance. Our
code, datasets and pre-trained models are made publicly available at
https://github.com/Thomkat/MIR-L.

</details>


### [88] [Grazing Detection using Deep Learning and Sentinel-2 Time Series Data](https://arxiv.org/abs/2510.14493)
*Aleksis Pirinen,Delia Fano Yela,Smita Chakraborty,Erik Källman*

Main category: cs.CV

TL;DR: 本研究利用 Sentinel-2 L2A 时间序列，实现了对季节性放牧的可扩展监测。


<details>
  <summary>Details</summary>
Motivation: 大规模监测放牧发生地点仍然受限，而放牧对农业生产和生物多样性都有影响。

Method: 使用 CNN-LSTM 模型集成，以多时相反射率特征为基础进行训练，对每个多边形定义的田地边界进行二元预测（放牧/未放牧）。

Result: 在五个验证集中，平均 F1 得分为 77%，对放牧牧场的召回率为 90%。如果检查员每年最多访问 4% 的地点，优先检查模型预测为未放牧的田地，获得的确认未放牧地点比随机检查多 17.2 倍。

Conclusion: 结果表明，粗分辨率、免费可用的卫星数据可以可靠地指导检查资源，以实现与保护相一致的土地利用合规性。代码和模型已公开提供。

Abstract: Grazing shapes both agricultural production and biodiversity, yet scalable
monitoring of where grazing occurs remains limited. We study seasonal grazing
detection from Sentinel-2 L2A time series: for each polygon-defined field
boundary, April-October imagery is used for binary prediction (grazed / not
grazed). We train an ensemble of CNN-LSTM models on multi-temporal reflectance
features, and achieve an average F1 score of 77 percent across five validation
splits, with 90 percent recall on grazed pastures. Operationally, if inspectors
can visit at most 4 percent of sites annually, prioritising fields predicted by
our model as non-grazed yields 17.2 times more confirmed non-grazing sites than
random inspection. These results indicate that coarse-resolution, freely
available satellite data can reliably steer inspection resources for
conservation-aligned land-use compliance. Code and models have been made
publicly available.

</details>


### [89] [Vision Mamba for Permeability Prediction of Porous Media](https://arxiv.org/abs/2510.14516)
*Ali Kashefi,Tapan Mukerji*

Main category: cs.CV

TL;DR: Vision Mamba作为ViT的替代方案，在图像分类中受到关注。它在计算和内存效率方面优于ViT和CNN，并且参数更少。


<details>
  <summary>Details</summary>
Motivation: 研究动机是Vision Mamba在计算和内存效率上的优势，以及参数较少。

Method: 首次引入使用Vision Mamba作为骨干网络的神经网络，用于预测三维多孔介质的渗透率。将Vision Mamba与ViT和CNN模型在渗透率预测的多个方面进行了比较，并进行了消融研究。

Result: 实验结果表明，Vision Mamba在三维多孔介质渗透率预测方面优于ViT和CNN。

Conclusion: 提出的框架有潜力集成到大型视觉模型中，用Vision Mamba代替ViT。

Abstract: Vision Mamba has recently received attention as an alternative to Vision
Transformers (ViTs) for image classification. The network size of Vision Mamba
scales linearly with input image resolution, whereas ViTs scale quadratically,
a feature that improves computational and memory efficiency. Moreover, Vision
Mamba requires a significantly smaller number of trainable parameters than
traditional convolutional neural networks (CNNs), and thus, they can be more
memory efficient. Because of these features, we introduce, for the first time,
a neural network that uses Vision Mamba as its backbone for predicting the
permeability of three-dimensional porous media. We compare the performance of
Vision Mamba with ViT and CNN models across multiple aspects of permeability
prediction and perform an ablation study to assess the effects of its
components on accuracy. We demonstrate in practice the aforementioned
advantages of Vision Mamba over ViTs and CNNs in the permeability prediction of
three-dimensional porous media. We make the source code publicly available to
facilitate reproducibility and to enable other researchers to build on and
extend this work. We believe the proposed framework has the potential to be
integrated into large vision models in which Vision Mamba is used instead of
ViTs.

</details>


### [90] [Real-Time Surgical Instrument Defect Detection via Non-Destructive Testing](https://arxiv.org/abs/2510.14525)
*Qurrat Ul Ain,Atif Aftab Ahmed Jilani,Zunaira Shafqat,Nigar Azhar Butt*

Main category: cs.CV

TL;DR: 本研究介绍了一种名为SurgScan的AI驱动的缺陷检测框架，用于手术器械的质量控制。


<details>
  <summary>Details</summary>
Motivation: 手术器械的质量控制依赖于人工检测，容易出现人为错误和不一致性，从而导致医疗风险。

Method: 使用YOLOv8算法，SurgScan能够实时分类手术器械中的缺陷。

Result: SurgScan在包含102,876张图像的高分辨率数据集上进行训练，实现了99.3%的准确率，推理速度为每张图像4.2-5.8毫秒。

Conclusion: SurgScan提供了一种可扩展、低成本的AI解决方案，用于自动化质量控制，减少了对手动检测的依赖，同时确保符合ISO 13485和FDA标准。

Abstract: Defective surgical instruments pose serious risks to sterility, mechanical
integrity, and patient safety, increasing the likelihood of surgical
complications. However, quality control in surgical instrument manufacturing
often relies on manual inspection, which is prone to human error and
inconsistency. This study introduces SurgScan, an AI-powered defect detection
framework for surgical instruments. Using YOLOv8, SurgScan classifies defects
in real-time, ensuring high accuracy and industrial scalability. The model is
trained on a high-resolution dataset of 102,876 images, covering 11 instrument
types and five major defect categories. Extensive evaluation against
state-of-the-art CNN architectures confirms that SurgScan achieves the highest
accuracy (99.3%) with real-time inference speeds of 4.2-5.8 ms per image,
making it suitable for industrial deployment. Statistical analysis demonstrates
that contrast-enhanced preprocessing significantly improves defect detection,
addressing key limitations in visual inspection. SurgScan provides a scalable,
cost-effective AI solution for automated quality control, reducing reliance on
manual inspection while ensuring compliance with ISO 13485 and FDA standards,
paving the way for enhanced defect detection in medical manufacturing.

</details>


### [91] [Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models](https://arxiv.org/abs/2510.14526)
*Yunze Tong,Didi Zhu,Zijing Hu,Jinluan Yang,Ziyu Zhao*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的文本到图像生成方法，通过在去噪前对初始噪声进行文本条件细化，以解决训练和推理之间的不匹配问题，从而提高文本-图像对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成方法生成的图像可能与提示不符，因为训练期间的噪声分布与推理期间的噪声分布存在差异。

Method: 该论文提出了一种噪声投影器，该投影器在去噪之前将初始噪声映射到提示相关的噪声空间。该投影器通过视觉语言模型（VLM）的token级反馈和奖励模型进行优化。

Result: 实验结果表明，该论文提出的提示感知噪声投影可以提高各种提示下的文本-图像对齐。

Conclusion: 该论文提出了一种有效的文本到图像生成方法，该方法通过对初始噪声进行文本条件细化来提高文本-图像对齐，且无需参考图像或手工制作的先验知识，并具有较小的推理成本。

Abstract: In text-to-image generation, different initial noises induce distinct
denoising paths with a pretrained Stable Diffusion (SD) model. While this
pattern could output diverse images, some of them may fail to align well with
the prompt. Existing methods alleviate this issue either by altering the
denoising dynamics or by drawing multiple noises and conducting post-selection.
In this paper, we attribute the misalignment to a training-inference mismatch:
during training, prompt-conditioned noises lie in a prompt-specific subset of
the latent space, whereas at inference the noise is drawn from a
prompt-agnostic Gaussian prior. To close this gap, we propose a noise projector
that applies text-conditioned refinement to the initial noise before denoising.
Conditioned on the prompt embedding, it maps the noise to a prompt-aware
counterpart that better matches the distribution observed during SD training,
without modifying the SD model. Our framework consists of these steps: we first
sample some noises and obtain token-level feedback for their corresponding
images from a vision-language model (VLM), then distill these signals into a
reward model, and finally optimize the noise projector via a quasi-direct
preference optimization. Our design has two benefits: (i) it requires no
reference images or handcrafted priors, and (ii) it incurs small inference
cost, replacing multi-sample selection with a single forward pass. Extensive
experiments further show that our prompt-aware noise projection improves
text-image alignment across diverse prompts.

</details>


### [92] [PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model](https://arxiv.org/abs/2510.14528)
*Cheng Cui,Ting Sun,Suyin Liang,Tingquan Gao,Zelun Zhang,Jiaxuan Liu,Xueqing Wang,Changda Zhou,Hongen Liu,Manhui Lin,Yue Zhang,Yubo Zhang,Handong Zheng,Jing Zhang,Jun Zhang,Yi Liu,Dianhai Yu,Yanjun Ma*

Main category: cs.CV

TL;DR: PaddleOCR-VL is a resource-efficient model for document parsing.


<details>
  <summary>Details</summary>
Motivation: To create a compact yet powerful vision-language model (VLM) for accurate element recognition in documents.

Method: Integrating a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model.

Result: Achieves SOTA performance in both page-level document parsing and element-level recognition.

Conclusion: Highly suitable for practical deployment in real-world scenarios due to its performance, efficiency, and speed.

Abstract: In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model
tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a
compact yet powerful vision-language model (VLM) that integrates a NaViT-style
dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to
enable accurate element recognition. This innovative model efficiently supports
109 languages and excels in recognizing complex elements (e.g., text, tables,
formulas, and charts), while maintaining minimal resource consumption. Through
comprehensive evaluations on widely used public benchmarks and in-house
benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document
parsing and element-level recognition. It significantly outperforms existing
solutions, exhibits strong competitiveness against top-tier VLMs, and delivers
fast inference speeds. These strengths make it highly suitable for practical
deployment in real-world scenarios.

</details>


### [93] [Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology](https://arxiv.org/abs/2510.14532)
*Xinrui Huang,Fan Xiao,Dongming He,Anqi Gao,Dandan Li,Xiaofan Zhang,Shaoting Zhang,Xudong Wang*

Main category: cs.CV

TL;DR: 本文介绍了DentVFM，首个专为牙科设计的视觉基础模型（VFM），它通过在包含约160万多模态牙科影像的大型数据集DentVista上进行自监督学习，为各种牙科应用生成与任务无关的视觉表示。同时，本文还推出了DentBench，一个包含八个牙科亚专业、更多疾病和成像方式的综合基准。


<details>
  <summary>Details</summary>
Motivation: 现有的牙科AI系统受限于其单一模态焦点、特定任务设计以及对昂贵标签数据的依赖，阻碍了它们在不同临床场景中的泛化。

Method: 本文提出了DentVFM，它包含基于Vision Transformer (ViT)架构的2D和3D变体，并在大型牙科影像数据集DentVista上使用自监督学习。

Result: DentVFM在各种牙科任务中表现出强大的泛化能力，显著优于有监督、自监督和弱监督的基线模型。此外，DentVFM还实现了跨模态诊断，在传统成像不可用的情况下，提供比有经验的牙医更可靠的结果。

Conclusion: DentVFM为牙科AI设定了一个新的范例，提供了一个可扩展、适应性强且标签高效的模型，以改善智能牙科保健并解决全球口腔保健中的关键差距。

Abstract: Oral and maxillofacial radiology plays a vital role in dental healthcare, but
radiographic image interpretation is limited by a shortage of trained
professionals. While AI approaches have shown promise, existing dental AI
systems are restricted by their single-modality focus, task-specific design,
and reliance on costly labeled data, hindering their generalization across
diverse clinical scenarios. To address these challenges, we introduce DentVFM,
the first family of vision foundation models (VFMs) designed for dentistry.
DentVFM generates task-agnostic visual representations for a wide range of
dental applications and uses self-supervised learning on DentVista, a large
curated dental imaging dataset with approximately 1.6 million multi-modal
radiographic images from various medical centers. DentVFM includes 2D and 3D
variants based on the Vision Transformer (ViT) architecture. To address gaps in
dental intelligence assessment and benchmarks, we introduce DentBench, a
comprehensive benchmark covering eight dental subspecialties, more diseases,
imaging modalities, and a wide geographical distribution. DentVFM shows
impressive generalist intelligence, demonstrating robust generalization to
diverse dental tasks, such as disease diagnosis, treatment analysis, biomarker
identification, and anatomical landmark detection and segmentation.
Experimental results indicate DentVFM significantly outperforms supervised,
self-supervised, and weakly supervised baselines, offering superior
generalization, label efficiency, and scalability. Additionally, DentVFM
enables cross-modality diagnostics, providing more reliable results than
experienced dentists in situations where conventional imaging is unavailable.
DentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and
label-efficient model to improve intelligent dental healthcare and address
critical gaps in global oral healthcare.

</details>


### [94] [Exploring Image Representation with Decoupled Classical Visual Descriptors](https://arxiv.org/abs/2510.14536)
*Chenyuan Qu,Hao Chen,Jianbo Jiao*

Main category: cs.CV

TL;DR: 这篇论文提出了VisualSplit，一个将图像分解为解耦的经典描述符（例如边缘、颜色和强度分布）的框架，并将每个描述符视为视觉知识的独立但互补的组成部分。


<details>
  <summary>Details</summary>
Motivation: 深度学习在图像理解任务中取得了显著进展，但其内部表征通常是不透明的，难以解释视觉信息的处理方式。相比之下，经典视觉描述符长期以来一直是图像分析的基础，并且对人类来说直观易懂。因此，本文旨在探索现代学习是否可以从这些经典线索中受益。

Method: 通过重建驱动的预训练方案，VisualSplit学习捕捉每个视觉描述符的本质，同时保持其可解释性。该方法显式地分解视觉属性。

Result: 通过显式分解视觉属性，该方法固有地促进了各种高级视觉任务中的有效属性控制，包括图像生成和编辑，超越了传统的分类和分割。

Conclusion: 该研究表明，这种新的学习方法对于视觉理解是有效的。

Abstract: Exploring and understanding efficient image representations is a
long-standing challenge in computer vision. While deep learning has achieved
remarkable progress across image understanding tasks, its internal
representations are often opaque, making it difficult to interpret how visual
information is processed. In contrast, classical visual descriptors (e.g. edge,
colour, and intensity distribution) have long been fundamental to image
analysis and remain intuitively understandable to humans. Motivated by this
gap, we ask a central question: Can modern learning benefit from these
classical cues? In this paper, we answer it with VisualSplit, a framework that
explicitly decomposes images into decoupled classical descriptors, treating
each as an independent but complementary component of visual knowledge. Through
a reconstruction-driven pre-training scheme, VisualSplit learns to capture the
essence of each visual descriptor while preserving their interpretability. By
explicitly decomposing visual attributes, our method inherently facilitates
effective attribute control in various advanced visual tasks, including image
generation and editing, extending beyond conventional classification and
segmentation, suggesting the effectiveness of this new learning approach for
visual understanding. Project page: https://chenyuanqu.com/VisualSplit/.

</details>


### [95] [Exploring Cross-Modal Flows for Few-Shot Learning](https://arxiv.org/abs/2510.14543)
*Ziqi Jiang,Yanghao Wang,Long Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的跨模态对齐方法FMA，通过学习跨模态速度场进行多步调整，以解决现有PEFT方法在复杂数据集上表现不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调方法（PEFT）在调整视觉或文本特征时存在局限性，尤其是在处理模态特征高度纠缠的复杂数据集时表现不足。

Method: 提出了一种模型无关的多步调整方法，通过学习跨模态速度场实现特征对齐。该方法包括固定耦合策略、噪声增强策略和早停求解器。

Result: 在各种基准测试和骨干网络上，FMA 始终优于现有的 PEFT 方法，尤其是在具有挑战性的数据集上。

Conclusion: FMA 具有多步校正能力，能够实现更精确和稳健的对齐，从而显著提高跨模态任务的性能。

Abstract: Aligning features from different modalities, is one of the most fundamental
challenges for cross-modal tasks. Although pre-trained vision-language models
can achieve a general alignment between image and text, they often require
parameter-efficient fine-tuning (PEFT) for further adjustment. Today's PEFT
methods (e.g., prompt tuning, LoRA-based, or adapter-based) always selectively
fine-tune a subset of parameters, which can slightly adjust either visual or
textual features, and avoid overfitting. In this paper, we are the first to
highlight that all existing PEFT methods perform one-step adjustment. It is
insufficient for complex (or difficult) datasets, where features of different
modalities are highly entangled. To this end, we propose the first
model-agnostic multi-step adjustment approach by learning a cross-modal
velocity field: Flow Matching Alignment (FMA). Specifically, to ensure the
correspondence between categories during training, we first utilize a fixed
coupling strategy. Then, we propose a noise augmentation strategy to alleviate
the data scarcity issue. Finally, we design an early-stopping solver, which
terminates the transformation process earlier, improving both efficiency and
accuracy. Compared with one-step PEFT methods, FMA has the multi-step
rectification ability to achieve more precise and robust alignment. Extensive
results have demonstrated that FMA can consistently yield significant
performance gains across various benchmarks and backbones, particularly on
challenging datasets.

</details>


### [96] [Consistent text-to-image generation via scene de-contextualization](https://arxiv.org/abs/2510.14553)
*Song Tang,Peihao Gong,Kunyu Li,Kai Guo,Boyu Wang,Mao Ye,Jianwei Zhang,Xiatian Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种名为 Scene De-Contextualization (SDeC) 的新方法，用于解决文本到图像生成中由于场景上下文引起的身份偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成模型在生成不同场景下同一主体的图像时，常常出现身份偏移的问题。以往的方法通常假设预先知道所有目标场景，这在现实应用中是不切实际的。

Method: 本文揭示了场景上下文与主体身份之间的相关性是身份偏移的关键原因，并提出了一种无需训练的提示嵌入编辑方法 SDeC，通过量化奇异值分解方向稳定性自适应地重新加权特征值，从而抑制潜在的场景-身份相关性。

Result: 实验表明，SDeC 显著提高了身份保持能力，同时保持了场景多样性。

Conclusion: SDeC 是一种高度灵活和通用的解决方案，适用于现实世界的应用，在这些应用中，事先了解所有目标场景通常是不可行的。

Abstract: Consistent text-to-image (T2I) generation seeks to produce
identity-preserving images of the same subject across diverse scenes, yet it
often fails due to a phenomenon called identity (ID) shift. Previous methods
have tackled this issue, but typically rely on the unrealistic assumption of
knowing all target scenes in advance. This paper reveals that a key source of
ID shift is the native correlation between subject and scene context, called
scene contextualization, which arises naturally as T2I models fit the training
distribution of vast natural images. We formally prove the near-universality of
this scene-ID correlation and derive theoretical bounds on its strength. On
this basis, we propose a novel, efficient, training-free prompt embedding
editing approach, called Scene De-Contextualization (SDeC), that imposes an
inversion process of T2I's built-in scene contextualization. Specifically, it
identifies and suppresses the latent scene-ID correlation within the ID
prompt's embedding by quantifying the SVD directional stability to adaptively
re-weight the corresponding eigenvalues. Critically, SDeC allows for per-scene
use (one scene per prompt) without requiring prior access to all target scenes.
This makes it a highly flexible and general solution well-suited to real-world
applications where such prior knowledge is often unavailable or varies over
time. Experiments demonstrate that SDeC significantly enhances identity
preservation while maintaining scene diversity.

</details>


### [97] [Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video](https://arxiv.org/abs/2510.14560)
*Yulin Zhang,Cheng Shi,Yang Wang,Sibei Yang*

Main category: cs.CV

TL;DR: 该论文介绍了一种新的AI任务，该任务要求AI能够像人类一样在环境中运行，主动理解、预测和响应事件，并及时回答问题。


<details>
  <summary>Details</summary>
Motivation: 为了使AI能够在类似人类的环境中运行，主动理解、预测和响应事件。

Method: 提出了一个包含数据引擎、多阶段训练策略和主动动态压缩技术的综合技术流程。

Result: 提出的模型有效地解决了这些关键属性，并在各种在线和离线基准测试中优于多个基线。

Conclusion: 论文提出了一个新的评估框架ESTP-Bench和ESTP-F1指标，并设计了一个模型来解决这个具有挑战性的任务。

Abstract: Envision an AI capable of functioning in human-like settings, moving beyond
mere observation to actively understand, anticipate, and proactively respond to
unfolding events. Towards this vision, we focus on the innovative task where,
given ego-streaming video input, an assistant proactively answers diverse,
evolving questions at the opportune moment, while maintaining synchronized
perception and reasoning. This task embodies three key properties: (1)
Proactive Coherence, (2) Just-in-Time Responsiveness, and (3) Synchronized
Efficiency. To evaluate and address these properties, we first introduce
ESTP-Bench (Ego Streaming Proactive Benchmark) alongside the ESTP-F1 metric-a
novel framework designed for their rigorous assessment. Secondly, we propose a
comprehensive technical pipeline to enable models to tackle this challenging
task. This pipeline comprises: (1) a data engine, (2) a multi-stage training
strategy, and (3) a proactive dynamic compression technique. Our proposed model
effectively addresses these critical properties while outperforming multiple
baselines across diverse online and offline benchmarks. Project
Page:https://zhangyl4.github.io/publications/eyes-wide-open/

</details>


### [98] [BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian Splatting Training on GPU](https://arxiv.org/abs/2510.14564)
*Junyi Wu,Jiaming Xu,Jinhao Li,Yongkang Zhou,Jiayi Pan,Xingyang Li,Guohao Dai*

Main category: cs.CV

TL;DR: BalanceGS通过算法-系统协同设计，显著提升3D高斯泼溅的训练效率，同时保持重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统3D高斯泼溅训练流程存在密度分配倾斜、计算负载不均衡和内存访问碎片化三个主要低效问题。

Method: 算法层面，提出启发式工作负载敏感的高斯密度控制；系统层面，提出基于相似性的高斯采样和合并；映射层面，提出基于重排序的内存访问映射策略。

Result: 在NVIDIA A100 GPU上，BalanceGS实现了1.44倍的训练加速，且质量损失可忽略不计。

Conclusion: BalanceGS通过算法和系统层面的优化，有效解决了传统3DGS训练中的效率瓶颈。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a promising 3D reconstruction
technique. The traditional 3DGS training pipeline follows three sequential
steps: Gaussian densification, Gaussian projection, and color splatting.
Despite its promising reconstruction quality, this conventional approach
suffers from three critical inefficiencies: (1) Skewed density allocation
during Gaussian densification, (2) Imbalanced computation workload during
Gaussian projection and (3) Fragmented memory access during color splatting.
  To tackle the above challenges, we introduce BalanceGS, the algorithm-system
co-design for efficient training in 3DGS. (1) At the algorithm level, we
propose heuristic workload-sensitive Gaussian density control to automatically
balance point distributions - removing 80% redundant Gaussians in dense regions
while filling gaps in sparse areas. (2) At the system level, we propose
Similarity-based Gaussian sampling and merging, which replaces the static
one-to-one thread-pixel mapping with adaptive workload distribution - threads
now dynamically process variable numbers of Gaussians based on local cluster
density. (3) At the mapping level, we propose reordering-based memory access
mapping strategy that restructures RGB storage and enables batch loading in
shared memory.
  Extensive experiments demonstrate that compared with 3DGS, our approach
achieves a 1.44$\times$ training speedup on a NVIDIA A100 GPU with negligible
quality degradation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [99] [Decision Oriented Technique (DOTechnique): Finding Model Validity Through Decision-Maker Context](https://arxiv.org/abs/2510.13858)
*Raheleh Biglari,Joachim Denil*

Main category: cs.AI

TL;DR: 提出了一种新方法 (DOTechnique) 来确定模型有效性，该方法基于决策一致性而非输出相似性。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常依赖于预定义的有效性框架，但这些框架并非总是可用或充分的。因此，本文旨在解决在缺乏显式有效性边界时，如何确定模型有效性区域的问题。

Method: 通过评估代理模型与高保真模型相比是否导致等效决策来确定模型有效性。该方法集成了领域约束和符号推理来缩小搜索空间，从而提高计算效率。

Result: 通过高速公路变道系统的例子，展示了 DOTechnique 如何发现仿真模型的有效性区域。

Conclusion: 结果表明，该技术有潜力支持通过决策者的上下文来寻找模型有效性。

Abstract: Model validity is as critical as the model itself, especially when guiding
decision-making processes. Traditional approaches often rely on predefined
validity frames, which may not always be available or sufficient. This paper
introduces the Decision Oriented Technique (DOTechnique), a novel method for
determining model validity based on decision consistency rather than output
similarity. By evaluating whether surrogate models lead to equivalent decisions
compared to high-fidelity models, DOTechnique enables efficient identification
of validity regions, even in the absence of explicit validity boundaries. The
approach integrates domain constraints and symbolic reasoning to narrow the
search space, enhancing computational efficiency. A highway lane change system
serves as a motivating example, demonstrating how DOTechnique can uncover the
validity region of a simulation model. The results highlight the potential of
the technique to support finding model validity through decision-maker context.

</details>


### [100] [Do Slides Help? Multi-modal Context for Automatic Transcription of Conference Talks](https://arxiv.org/abs/2510.13979)
*Supriti Sinhamahapatra,Jan Niehues*

Main category: cs.AI

TL;DR: 该论文提出了一种结合视觉信息的自动语音识别（ASR）系统，特别关注科学演示场景下的幻灯片信息。


<details>
  <summary>Details</summary>
Motivation: 现有的ASR系统主要依赖声学信息，忽略了多模态上下文，而视觉信息在消除歧义和适应性方面至关重要。之前的研究主要集中在说话人图像以处理噪声，但本文侧重于整合演示幻灯片。

Method: 1. 创建一个多模态演示基准，自动分析特定领域的术语转录。2. 探索使用多模态信息增强语音模型的方法。3. 通过适当的数据增强方法缓解缺乏幻灯片数据集的问题。4. 使用增强的数据集训练模型。

Result: 与基线模型相比，所有词的词错误率相对降低了约34%，特定领域术语的词错误率相对降低了约35%。

Conclusion: 该研究表明，结合幻灯片信息可以显著提高科学演示场景下的语音识别准确率。

Abstract: State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily
rely on acoustic information while disregarding additional multi-modal context.
However, visual information are essential in disambiguation and adaptation.
While most work focus on speaker images to handle noise conditions, this work
also focuses on integrating presentation slides for the use cases of scientific
presentation.
  In a first step, we create a benchmark for multi-modal presentation including
an automatic analysis of transcribing domain-specific terminology. Next, we
explore methods for augmenting speech models with multi-modal information. We
mitigate the lack of datasets with accompanying slides by a suitable approach
of data augmentation. Finally, we train a model using the augmented dataset,
resulting in a relative reduction in word error rate of approximately 34%,
across all words and 35%, for domain-specific terms compared to the baseline
model.

</details>


### [101] [Do Large Language Models Show Biases in Causal Learning? Insights from Contingency Judgment](https://arxiv.org/abs/2510.13985)
*María Victoria Carro,Denise Alejandra Mester,Francisca Gauna Selasco,Giovanni Franco Gabriel Marraffini,Mario Alejandro Leiva,Gerardo I. Simari,María Vanina Martinez*

Main category: cs.AI

TL;DR: 大型语言模型 (LLM) 在缺乏证据的情况下，容易产生因果错觉，即错误地感知变量之间的因果关系。


<details>
  <summary>Details</summary>
Motivation: 因果错觉是许多社会问题的根源，例如社会偏见、刻板印象、错误信息和迷信思维。本文研究大型语言模型是否容易产生因果错觉。

Method: 构建了一个包含 1,000 个医疗背景下的零应急场景数据集，并提示 LLM 评估潜在原因的有效性。

Result: 所有评估的模型系统性地推断出无根据的因果关系，表明它们极易受到因果错觉的影响。

Conclusion: 研究结果表明，LLM 可能只是在没有真正理解的情况下再现因果语言，这引发了人们对在需要准确因果推理的领域中使用语言模型的担忧。

Abstract: Causal learning is the cognitive process of developing the capability of
making causal inferences based on available information, often guided by
normative principles. This process is prone to errors and biases, such as the
illusion of causality, in which people perceive a causal relationship between
two variables despite lacking supporting evidence. This cognitive bias has been
proposed to underlie many societal problems, including social prejudice,
stereotype formation, misinformation, and superstitious thinking. In this work,
we examine whether large language models are prone to developing causal
illusions when faced with a classic cognitive science paradigm: the contingency
judgment task. To investigate this, we constructed a dataset of 1,000 null
contingency scenarios (in which the available information is not sufficient to
establish a causal relationship between variables) within medical contexts and
prompted LLMs to evaluate the effectiveness of potential causes. Our findings
show that all evaluated models systematically inferred unwarranted causal
relationships, revealing a strong susceptibility to the illusion of causality.
While there is ongoing debate about whether LLMs genuinely understand causality
or merely reproduce causal language without true comprehension, our findings
support the latter hypothesis and raise concerns about the use of language
models in domains where accurate causal reasoning is essential for informed
decision-making.

</details>


### [102] [GammaZero: Learning To Guide POMDP Belief Space Search With Graph Representations](https://arxiv.org/abs/2510.14035)
*Rajesh Mangannavar,Prasad Tadepalli*

Main category: cs.AI

TL;DR: GammaZero: 使用 action-centric 图表示框架学习在 POMDP 中指导规划，实现跨问题规模的泛化。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要特定领域的神经架构且难以扩展，GammaZero 旨在解决这些问题。

Method: 将 belief 状态转换为 action-centric 图，使用图神经网络学习值函数和策略，并用学习到的启发式方法指导蒙特卡洛树搜索。

Result: 在标准 POMDP 基准测试中，GammaZero 在相同规模问题上与 BetaZero 性能相当，并能零样本泛化到比训练规模大 2-4 倍的问题，同时保持解决方案质量并减少搜索需求。

Conclusion: GammaZero 是一种有效的 POMDP 规划方法，具有良好的泛化能力和可扩展性。

Abstract: We introduce an action-centric graph representation framework for learning to
guide planning in Partially Observable Markov Decision Processes (POMDPs).
Unlike existing approaches that require domain-specific neural architectures
and struggle with scalability, GammaZero leverages a unified graph-based belief
representation that enables generalization across problem sizes within a
domain. Our key insight is that belief states can be systematically transformed
into action-centric graphs where structural patterns learned on small problems
transfer to larger instances. We employ a graph neural network with a decoder
architecture to learn value functions and policies from expert demonstrations
on computationally tractable problems, then apply these learned heuristics to
guide Monte Carlo tree search on larger problems. Experimental results on
standard POMDP benchmarks demonstrate that GammaZero achieves comparable
performance to BetaZero when trained and tested on the same-sized problems,
while uniquely enabling zero-shot generalization to problems 2-4 times larger
than those seen during training, maintaining solution quality with reduced
search requirements.

</details>


### [103] [Position: Require Frontier AI Labs To Release Small "Analog" Models](https://arxiv.org/abs/2510.14053)
*Shriyash Upadhyay,Chaithanya Bandi,Narmeen Oozeer,Philip Quirke*

Main category: cs.AI

TL;DR: 建议通过发布小型、开放访问的模拟模型来确保人工智能安全并促进创新，这些模型类似于并从最大的专有模型中提炼而来。


<details>
  <summary>Details</summary>
Motivation: 现有的前沿人工智能模型监管提案引发了对安全监管成本的担忧，并且大多数此类监管由于安全与创新之间的权衡而被搁置。

Method: 建议强制大型人工智能实验室发布小型、开放访问的模拟模型（缩小版本），这些模型以类似于其最大的专有模型的方式进行训练和提炼。

Result: 通过允许更广泛地参与安全验证、可解释性研究和算法透明度，模拟模型充当公共代理，而无需强制实验室披露其完整规模的模型。最近的研究表明，使用这些较小模型开发的安全性和可解释性方法可以有效地推广到前沿规模的系统。

Conclusion: 更深入地了解模型可以缓解安全与创新之间的权衡，并让我们两者兼得。

Abstract: Recent proposals for regulating frontier AI models have sparked concerns
about the cost of safety regulation, and most such regulations have been
shelved due to the safety-innovation tradeoff. This paper argues for an
alternative regulatory approach that ensures AI safety while actively promoting
innovation: mandating that large AI laboratories release small, openly
accessible analog models (scaled-down versions) trained similarly to and
distilled from their largest proprietary models.
  Analog models serve as public proxies, allowing broad participation in safety
verification, interpretability research, and algorithmic transparency without
forcing labs to disclose their full-scale models. Recent research demonstrates
that safety and interpretability methods developed using these smaller models
generalize effectively to frontier-scale systems. By enabling the wider
research community to directly investigate and innovate upon accessible
analogs, our policy substantially reduces the regulatory burden and accelerates
safety advancements.
  This mandate promises minimal additional costs, leveraging reusable resources
like data and infrastructure, while significantly contributing to the public
good. Our hope is not only that this policy be adopted, but that it illustrates
a broader principle supporting fundamental research in machine learning: deeper
understanding of models relaxes the safety-innovation tradeoff and lets us have
more of both.

</details>


### [104] [Generating Fair Consensus Statements with Social Choice on Token-Level MDPs](https://arxiv.org/abs/2510.14106)
*Carter Blair,Kate Larson*

Main category: cs.AI

TL;DR: 提出了一种新的共识声明生成框架，该框架利用多目标 Markov 决策过程 (MDP) 和社会选择理论，以确保在聚合不同意见时具有可证明的公平性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大型语言模型的共识声明生成框架缺乏内在结构，难以在聚合不同意见时提供可证明的公平性保证。

Method: 将任务建模为多目标、令牌级别的 Markov 决策过程 (MDP)，其中每个目标对应于一个代理的偏好。利用社会选择理论，提出了两种方法：一种是保证在事前核心中的随机生成策略，另一种是使用 MDP 框架内的搜索算法最大化平均主义福利。

Result: 实验表明，由平均主义目标引导的搜索生成的共识声明比基线方法具有改进的最差情况代理对齐。

Conclusion: 该研究提出的方法能够生成更公平的共识声明。

Abstract: Current frameworks for consensus statement generation with large language
models lack the inherent structure needed to provide provable fairness
guarantees when aggregating diverse free-form opinions. We model the task as a
multi-objective, token-level Markov Decision Process (MDP), where each
objective corresponds to an agent's preference. Token-level rewards for each
agent are derived from their policy (e.g., a personalized language model). This
approach utilizes the finding that such policies implicitly define optimal
Q-functions, providing a principled way to quantify rewards at each generation
step without a value function (Rafailov et al., 2024). This MDP formulation
creates a formal structure amenable to analysis using principles from social
choice theory. We propose two approaches grounded in social choice theory.
First, we propose a stochastic generation policy guaranteed to be in the
ex-ante core, extending core stability concepts from voting theory to text
generation. This policy is derived from an underlying distribution over
complete statements that maximizes proportional fairness (Nash Welfare).
Second, for generating a single statement, we target the maximization of
egalitarian welfare using search algorithms within the MDP framework.
Empirically, experiments using language models to instantiate agent policies
show that search guided by the egalitarian objective generates consensus
statements with improved worst-case agent alignment compared to baseline
methods, including the Habermas Machine (Tessler et al., 2024).

</details>


### [105] [STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management](https://arxiv.org/abs/2510.14112)
*Huiliang Zhang,Di Wu,Arnaud Zinflou,Benoit Boulet*

Main category: cs.AI

TL;DR: 提出了一种名为STEMS的、用于协调建筑能源管理的安全约束多智能体强化学习框架。


<details>
  <summary>Details</summary>
Motivation: 当前的建筑能源管理系统在利用时空信息、缺乏严格的安全保证和系统复杂性方面面临挑战。

Method: 该论文提出了一种时空增强安全多智能体协调（STEMS）框架，该框架结合了GCN-Transformer融合架构的时空图表示学习框架和结合控制障碍函数（Control Barrier Functions）的安全约束多智能体强化学习算法。

Result: 在真实建筑数据集上的大量实验表明，STEMS优于现有方法，成本降低21%，排放降低18%，安全违规从35.1%大幅降低至5.6%，同时保持最佳舒适度，不舒适比例仅为0.13%。

Conclusion: 该框架在极端天气条件下表现出强大的鲁棒性，并在不同建筑类型中保持有效性。

Abstract: Building energy management is essential for achieving carbon reduction goals,
improving occupant comfort, and reducing energy costs. Coordinated building
energy management faces critical challenges in exploiting spatial-temporal
dependencies while ensuring operational safety across multi-building systems.
Current multi-building energy systems face three key challenges: insufficient
spatial-temporal information exploitation, lack of rigorous safety guarantees,
and system complexity. This paper proposes Spatial-Temporal Enhanced Safe
Multi-Agent Coordination (STEMS), a novel safety-constrained multi-agent
reinforcement learning framework for coordinated building energy management.
STEMS integrates two core components: (1) a spatial-temporal graph
representation learning framework using a GCN-Transformer fusion architecture
to capture inter-building relationships and temporal patterns, and (2) a
safety-constrained multi-agent RL algorithm incorporating Control Barrier
Functions to provide mathematical safety guarantees. Extensive experiments on
real-world building datasets demonstrate STEMS's superior performance over
existing methods, showing that STEMS achieves 21% cost reduction, 18% emission
reduction, and dramatically reduces safety violations from 35.1% to 5.6% while
maintaining optimal comfort with only 0.13 discomfort proportion. The framework
also demonstrates strong robustness during extreme weather conditions and
maintains effectiveness across different building types.

</details>


### [106] [Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems](https://arxiv.org/abs/2510.14133)
*Edoardo Allegrini,Ananth Shreekumar,Z. Berkay Celik*

Main category: cs.AI

TL;DR: This paper introduces a unified framework for analyzing multi-agent AI systems, addressing the fragmentation in inter-agent communication.


<details>
  <summary>Details</summary>
Motivation: The current fragmented ecosystem of inter-agent communication hinders rigorous analysis and introduces risks like misalignment and coordination issues.

Method: The paper proposes a modeling framework with a host agent model and a task lifecycle model, defining properties for each.

Result: The framework enables formal verification, detection of coordination edge cases, and prevention of deadlocks and security vulnerabilities.

Conclusion: The paper introduces a domain-agnostic framework for systematic analysis, design, and deployment of correct, reliable, and robust agentic AI systems.

Abstract: Agentic AI systems, which leverage multiple autonomous agents and Large
Language Models (LLMs), are increasingly used to address complex, multi-step
tasks. The safety, security, and functionality of these systems are critical,
especially in high-stakes applications. However, the current ecosystem of
inter-agent communication is fragmented, with protocols such as the Model
Context Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol
for coordination being analyzed in isolation. This fragmentation creates a
semantic gap that prevents the rigorous analysis of system properties and
introduces risks such as architectural misalignment and exploitable
coordination issues. To address these challenges, we introduce a modeling
framework for agentic AI systems composed of two foundational models. The
first, the host agent model, formalizes the top-level entity that interacts
with the user, decomposes tasks, and orchestrates their execution by leveraging
external agents and tools. The second, the task lifecycle model, details the
states and transitions of individual sub-tasks from creation to completion,
providing a fine-grained view of task management and error handling. Together,
these models provide a unified semantic framework for reasoning about the
behavior of multi-AI agent systems. Grounded in this framework, we define 17
properties for the host agent and 14 for the task lifecycle, categorized into
liveness, safety, completeness, and fairness. Expressed in temporal logic,
these properties enable formal verification of system behavior, detection of
coordination edge cases, and prevention of deadlocks and security
vulnerabilities. Through this effort, we introduce the first rigorously
grounded, domain-agnostic framework for the systematic analysis, design, and
deployment of correct, reliable, and robust agentic AI systems.

</details>


### [107] [A Multimodal Approach to Heritage Preservation in the Context of Climate Change](https://arxiv.org/abs/2510.14136)
*David Roqui,Adèle Cormier,nistor Grozavu,Ann Bourges*

Main category: cs.AI

TL;DR: 本文提出了一种轻量级多模态架构，该架构融合了传感器数据（温度、湿度）与视觉图像，以预测遗产地点的退化程度。


<details>
  <summary>Details</summary>
Motivation: 传统监测依赖于单模态分析，无法捕捉环境压力与材料退化之间的复杂相互作用。气候变化加速了文化遗产地点的退化。

Method: 该方法改进了 PerceiverIO，包含两个关键创新：(1) 简化的编码器（64D 潜在空间），防止在小型数据集上过拟合，以及 (2) 自适应 Barlow Twins 损失，鼓励模态互补性。

Result: 在斯特拉斯堡大教堂的数据上，该模型达到了 76.9% 的准确率，比标准多模态架构提高了 43%，比原始 PerceiverIO 提高了 25%。

Conclusion: 该研究表明，架构简单性与对比正则化相结合，能够在数据稀缺的遗产监测环境中实现有效的多模态学习，为 AI 驱动的保护决策支持系统奠定基础。

Abstract: Cultural heritage sites face accelerating degradation due to climate change,
yet tradi- tional monitoring relies on unimodal analysis (visual inspection or
environmental sen- sors alone) that fails to capture the complex interplay
between environmental stres- sors and material deterioration. We propose a
lightweight multimodal architecture that fuses sensor data (temperature,
humidity) with visual imagery to predict degradation severity at heritage
sites. Our approach adapts PerceiverIO with two key innovations: (1) simplified
encoders (64D latent space) that prevent overfitting on small datasets (n=37
training samples), and (2) Adaptive Barlow Twins loss that encourages modality
complementarity rather than redundancy. On data from Strasbourg Cathedral, our
model achieves 76.9% accu- racy, a 43% improvement over standard multimodal
architectures (VisualBERT, Trans- former) and 25% over vanilla PerceiverIO.
Ablation studies reveal that sensor-only achieves 61.5% while image-only
reaches 46.2%, confirming successful multimodal synergy. A systematic
hyperparameter study identifies an optimal moderate correlation target ({\tau}
=0.3) that balances align- ment and complementarity, achieving 69.2% accuracy
compared to other {\tau} values ({\tau} =0.1/0.5/0.7: 53.8%, {\tau} =0.9:
61.5%). This work demonstrates that architectural sim- plicity combined with
contrastive regularization enables effective multimodal learning in data-scarce
heritage monitoring contexts, providing a foundation for AI-driven con-
servation decision support systems.

</details>


### [108] [CodeEvolve: An open source evolutionary coding agent for algorithm discovery and optimization](https://arxiv.org/abs/2510.14150)
*Henrique Assumpção,Diego Ferreira,Leandro Campos,Fabricio Murai*

Main category: cs.AI

TL;DR: CodeEvolve是一个开源的进化编码代理，它结合了大型语言模型（LLM）和遗传算法来解决复杂的计算问题。它胜过了AlphaEvolve在一些具有挑战性的问题上的表现。


<details>
  <summary>Details</summary>
Motivation: 将强大的进化概念应用到LLM领域，并建立在最近的广义科学发现方法之上。

Method: 采用基于岛屿的遗传算法来维持种群多样性和增加吞吐量，引入了一种新颖的基于灵感的交叉机制，该机制利用LLM的上下文窗口来组合来自成功解决方案的特征，并实现了元提示策略，用于动态探索解决方案空间。

Result: 我们的研究结果表明，我们的方法在几个具有挑战性的问题上超过了AlphaEvolve的性能。

Conclusion: 为了促进协作和加速进步，我们将完整的框架作为开源存储库发布。

Abstract: In this work, we introduce CodeEvolve, an open-source evolutionary coding
agent that unites Large Language Models (LLMs) with genetic algorithms to solve
complex computational problems. Our framework adapts powerful evolutionary
concepts to the LLM domain, building upon recent methods for generalized
scientific discovery. CodeEvolve employs an island-based genetic algorithm to
maintain population diversity and increase throughput, introduces a novel
inspiration-based crossover mechanism that leverages the LLMs context window to
combine features from successful solutions, and implements meta-prompting
strategies for dynamic exploration of the solution space. We conduct a rigorous
evaluation of CodeEvolve on a subset of the mathematical benchmarks used to
evaluate Google DeepMind's closed-source AlphaEvolve. Our findings show that
our method surpasses AlphaEvolve's performance on several challenging problems.
To foster collaboration and accelerate progress, we release our complete
framework as an open-source repository.

</details>


### [109] [Combining Reinforcement Learning and Behavior Trees for NPCs in Video Games with AMD Schola](https://arxiv.org/abs/2510.14154)
*Tian Liu,Alex Cann,Ian Colbert,Mehdi Saeedi*

Main category: cs.AI

TL;DR: 本研究探讨了强化学习(RL)在商业视频游戏中应用缓慢的问题，并强调了RL与传统行为树(BTs)的结合是未来研究的关键。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习(RL)的研究进展迅速，但其在商业视频游戏中的应用仍然缓慢。游戏AI社区在使用RL驱动的NPC时面临着常见的挑战。

Method: 通过使用AMD Schola（一个在虚幻引擎中训练RL代理的插件），在受商业视频游戏“最后生还者”启发的复杂3D环境中创建多任务NPC，证明了该方法的可行性。

Result: 展示了联合训练RL模型与BTs同时展示各种技能的方法。

Conclusion: 强调RL与传统行为树(BTs)的结合是未来值得进一步探索的关键 juncture。

Abstract: While the rapid advancements in the reinforcement learning (RL) research
community have been remarkable, the adoption in commercial video games remains
slow. In this paper, we outline common challenges the Game AI community faces
when using RL-driven NPCs in practice, and highlight the intersection of RL
with traditional behavior trees (BTs) as a crucial juncture to be explored
further. Although the BT+RL intersection has been suggested in several research
papers, its adoption is rare. We demonstrate the viability of this approach
using AMD Schola -- a plugin for training RL agents in Unreal Engine -- by
creating multi-task NPCs in a complex 3D environment inspired by the commercial
video game ``The Last of Us". We provide detailed methodologies for jointly
training RL models with BTs while showcasing various skills.

</details>


### [110] [JEDA: Query-Free Clinical Order Search from Ambient Dialogues](https://arxiv.org/abs/2510.14169)
*Praphul Singh,Corey Barrett,Sumana Srivasta,Amitabh Saikia,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: JEDA是一个快速、可解释且无需LLM的检索层，它将环境语境与可执行的临床医嘱实时连接起来。


<details>
  <summary>Details</summary>
Motivation: 许多系统依赖于LLM重写，这增加了延迟、不稳定性和不透明性，阻碍了实时医嘱下达。

Method: 提出了一种名为JEDA（用于直接和环境临床医嘱的联合嵌入）的领域初始化双编码器，它可以直接检索规范医嘱，并在无查询模式下，编码环境对话的短滚动窗口以触发检索。

Result: JEDA在实践中产生了巨大的收益，并且大大优于其基础编码器和最近的开放嵌入器。

Conclusion: JEDA提供了一种快速、可解释且无需LLM的检索层，可以在实时环境中将上下文信息链接到可操作的临床医嘱。

Abstract: Clinical conversations mix explicit directives (order a chest X-ray) with
implicit reasoning (the cough worsened overnight, we should check for
pneumonia). Many systems rely on LLM rewriting, adding latency, instability,
and opacity that hinder real-time ordering. We present JEDA (Joint Embedding
for Direct and Ambient clinical orders), a domain-initialized bi-encoder that
retrieves canonical orders directly and, in a query-free mode, encodes a short
rolling window of ambient dialogue to trigger retrieval. Initialized from
PubMedBERT and fine-tuned with a duplicate-safe contrastive objective, JEDA
aligns heterogeneous expressions of intent to shared order concepts. Training
uses constrained LLM guidance to tie each signed order to complementary
formulations (command only, context only, command+context, context+reasoning),
producing clearer inter-order separation, tighter query extendash order
coupling, and stronger generalization. The query-free mode is noise-resilient,
reducing sensitivity to disfluencies and ASR errors by conditioning on a short
window rather than a single utterance. Deployed in practice, JEDA yields large
gains and substantially outperforms its base encoder and recent open embedders
(Linq Embed Mistral, SFR Embedding, GTE Qwen, BGE large, Embedding Gemma). The
result is a fast, interpretable, LLM-free retrieval layer that links ambient
context to actionable clinical orders in real time.

</details>


### [111] [ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning](https://arxiv.org/abs/2510.14176)
*Roger Creus Castanyer,Faisal Mohamed,Pablo Samuel Castro,Cyrus Neary,Glen Berseth*

Main category: cs.AI

TL;DR: 提出了一种名为ARM-FM的框架，用于在强化学习中自动进行组合奖励设计，利用了基础模型（FM）的高级推理能力。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法对奖励函数规范高度敏感，这仍然是限制其广泛适用性的核心挑战。

Method: 使用奖励机（RM）作为强化学习目标规范的机制，并通过使用FM自动构建。将语言嵌入与每个RM自动机状态相关联，以实现跨任务的泛化。

Result: 在各种具有挑战性的环境中提供了ARM-FM有效性的经验证据，包括零样本泛化的证据。

Conclusion: ARM-FM框架能够利用基础模型自动生成奖励机，从而实现强化学习中的组合奖励设计，并在各种环境中表现出有效性，包括零样本泛化。

Abstract: Reinforcement learning (RL) algorithms are highly sensitive to reward
function specification, which remains a central challenge limiting their broad
applicability. We present ARM-FM: Automated Reward Machines via Foundation
Models, a framework for automated, compositional reward design in RL that
leverages the high-level reasoning capabilities of foundation models (FMs).
Reward machines (RMs) -- an automata-based formalism for reward specification
-- are used as the mechanism for RL objective specification, and are
automatically constructed via the use of FMs. The structured formalism of RMs
yields effective task decompositions, while the use of FMs enables objective
specifications in natural language. Concretely, we (i) use FMs to automatically
generate RMs from natural language specifications; (ii) associate language
embeddings with each RM automata-state to enable generalization across tasks;
and (iii) provide empirical evidence of ARM-FM's effectiveness in a diverse
suite of challenging environments, including evidence of zero-shot
generalization.

</details>


### [112] [Implementation of AI in Precision Medicine](https://arxiv.org/abs/2510.14194)
*Göktuğ Bender,Samer Faraj,Anand Bhardwaj*

Main category: cs.AI

TL;DR: AI在精准医疗中的应用受限，本文回顾了2019-2024年相关文献，分析了数据质量、临床可靠性、工作流程整合和治理等方面的障碍和推动因素。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在精准医疗中的应用现状及挑战，旨在推动其在临床环境中的实际应用。

Method: 对2019-2024年间AI在精准医疗实施方面的文献进行范围界定回顾，并采用基于生态系统的框架进行分析。

Result: 识别了数据质量、临床可靠性、工作流程整合和治理等关键障碍和推动因素，阐述了影响实际转化的相互依赖关系。

Conclusion: 为支持可信和可持续的实施，提出了未来发展方向。

Abstract: Artificial intelligence (AI) has become increasingly central to precision
medicine by enabling the integration and interpretation of multimodal data, yet
implementation in clinical settings remains limited. This paper provides a
scoping review of literature from 2019-2024 on the implementation of AI in
precision medicine, identifying key barriers and enablers across data quality,
clinical reliability, workflow integration, and governance. Through an
ecosystem-based framework, we highlight the interdependent relationships
shaping real-world translation and propose future directions to support
trustworthy and sustainable implementation.

</details>


### [113] [Echoes of Human Malice in Agents: Benchmarking LLMs for Multi-Turn Online Harassment Attacks](https://arxiv.org/abs/2510.14207)
*Trilok Padhi,Pinxian Lu,Abdulkadir Erol,Tanmay Sutar,Gauri Sharma,Mina Sonmez,Munmun De Choudhury,Ugur Kursuncu*

Main category: cs.AI

TL;DR: LLM agents in web apps are vulnerable to multi-turn harassment. This paper introduces a benchmark, attack methods, and evaluation framework, finding high jailbreak success rates and human-like aggression.


<details>
  <summary>Details</summary>
Motivation: Prior jailbreak research focused on single-turn prompts, not real multi-turn harassment.

Method: The paper presents a benchmark with a multi-turn harassment dataset, multi-agent simulation, jailbreak methods, and a mixed-methods evaluation framework. It uses LLaMA-3.1-8B-Instruct and Gemini-2.0-flash.

Result: Jailbreak tuning significantly increases harassment success rates (up to 99.33%) and reduces refusal rates. Toxic behaviors like insult and flaming are prevalent. Models exhibit human-like aggression profiles, with closed-source models showing significant vulnerability.

Conclusion: Multi-turn attacks succeed at high rates and mimic human-like harassment, motivating the development of robust safety guardrails.

Abstract: Large Language Model (LLM) agents are powering a growing share of interactive
web applications, yet remain vulnerable to misuse and harm. Prior jailbreak
research has largely focused on single-turn prompts, whereas real harassment
often unfolds over multi-turn interactions. In this work, we present the Online
Harassment Agentic Benchmark consisting of: (i) a synthetic multi-turn
harassment conversation dataset, (ii) a multi-agent (e.g., harasser, victim)
simulation informed by repeated game theory, (iii) three jailbreak methods
attacking agents across memory, planning, and fine-tuning, and (iv) a
mixed-methods evaluation framework. We utilize two prominent LLMs,
LLaMA-3.1-8B-Instruct (open-source) and Gemini-2.0-flash (closed-source). Our
results show that jailbreak tuning makes harassment nearly guaranteed with an
attack success rate of 95.78--96.89% vs. 57.25--64.19% without tuning in Llama,
and 99.33% vs. 98.46% without tuning in Gemini, while sharply reducing refusal
rate to 1-2% in both models. The most prevalent toxic behaviors are Insult with
84.9--87.8% vs. 44.2--50.8% without tuning, and Flaming with 81.2--85.1% vs.
31.5--38.8% without tuning, indicating weaker guardrails compared to sensitive
categories such as sexual or racial harassment. Qualitative evaluation further
reveals that attacked agents reproduce human-like aggression profiles, such as
Machiavellian/psychopathic patterns under planning, and narcissistic tendencies
with memory. Counterintuitively, closed-source and open-source models exhibit
distinct escalation trajectories across turns, with closed-source models
showing significant vulnerability. Overall, our findings show that multi-turn
and theory-grounded attacks not only succeed at high rates but also mimic
human-like harassment dynamics, motivating the development of robust safety
guardrails to ultimately keep online platforms safe and responsible.

</details>


### [114] [LiveResearchBench: A Live Benchmark for User-Centric Deep Research in the Wild](https://arxiv.org/abs/2510.14240)
*Jiayu Wang,Yifei Ming,Riya Dulepet,Qinglin Chen,Austin Xu,Zixuan Ke,Frederic Sala,Aws Albarghouthi,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 提出了LiveResearchBench和DeepEval用于评估agentic systems的深度研究能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能满足用户中心、动态、明确和多方面的搜索需求。

Method: 构建了包含100个任务的LiveResearchBench基准，并使用DeepEval评估内容和报告质量。

Result: 对17个深度研究系统进行了评估，揭示了当前优势、常见问题和关键系统组件。

Conclusion: LiveResearchBench和DeepEval为系统评估提供了一个严格的基础，并为推进可靠的深度研究提供了方向。

Abstract: Deep research -- producing comprehensive, citation-grounded reports by
searching and synthesizing information from hundreds of live web sources --
marks an important frontier for agentic systems. To rigorously evaluate this
ability, four principles are essential: tasks should be (1) user-centric,
reflecting realistic information needs, (2) dynamic, requiring up-to-date
information beyond parametric knowledge, (3) unambiguous, ensuring consistent
interpretation across users, and (4) multi-faceted and search-intensive,
requiring search over numerous web sources and in-depth analysis. Existing
benchmarks fall short of these principles, often focusing on narrow domains or
posing ambiguous questions that hinder fair comparison. Guided by these
principles, we introduce LiveResearchBench, a benchmark of 100 expert-curated
tasks spanning daily life, enterprise, and academia, each requiring extensive,
dynamic, real-time web search and synthesis. Built with over 1,500 hours of
human labor, LiveResearchBench provides a rigorous basis for systematic
evaluation. To evaluate citation-grounded long-form reports, we introduce
DeepEval, a comprehensive suite covering both content- and report-level
quality, including coverage, presentation, citation accuracy and association,
consistency and depth of analysis. DeepEval integrates four complementary
evaluation protocols, each designed to ensure stable assessment and high
agreement with human judgments. Using LiveResearchBench and DeepEval, we
conduct a comprehensive evaluation of 17 frontier deep research systems,
including single-agent web search, single-agent deep research, and multi-agent
systems. Our analysis reveals current strengths, recurring failure modes, and
key system components needed to advance reliable, insightful deep research.

</details>


### [115] [Towards Agentic Self-Learning LLMs in Search Environment](https://arxiv.org/abs/2510.14253)
*Wangtao Sun,Xiang Cheng,Jialin Fan,Yao Xu,Xing Yu,Shizhu He,Jun Zhao,Kang Liu*

Main category: cs.AI

TL;DR: 本文研究了在没有人工数据集或预定义规则奖励的情况下，自学习是否可以扩展基于 LLM 的 Agent。通过受控实验，确定了可扩展 Agent 训练的两个关键决定因素：奖励信号的来源和 Agent 任务数据的规模。提出了 Agentic Self-Learning (ASL)，一个完全闭环、多角色的强化学习框架，它统一了任务生成、策略执行和评估。


<details>
  <summary>Details</summary>
Motivation: 研究在开放域环境中，如何扩展基于 LLM 的 Agent，摆脱对人工数据集和预定义规则的依赖。

Method: 提出了 Agentic Self-Learning (ASL) 框架，该框架包含 Prompt Generator、Policy Model 和 Generative Reward Model，以形成一个良性循环。

Result: ASL 能够稳定提升性能，超过了 RLVR 基线，并且在零标签数据条件下持续改进。GRM 的验证能力是主要瓶颈，持续的 GRM 训练可以缓解这个问题，少量后期注入真实验证数据可以提高性能上限。

Conclusion: 奖励来源和数据规模是开放域 Agent 学习的关键，多角色协同进化对于可扩展、自我改进的 Agent 是有效的。

Abstract: We study whether self-learning can scale LLM-based agents without relying on
human-curated datasets or predefined rule-based rewards. Through controlled
experiments in a search-agent setting, we identify two key determinants of
scalable agent training: the source of reward signals and the scale of agent
task data. We find that rewards from a Generative Reward Model (GRM) outperform
rigid rule-based signals for open-domain learning, and that co-evolving the GRM
with the policy further boosts performance. Increasing the volume of agent task
data-even when synthetically generated-substantially enhances agentic
capabilities. Building on these insights, we propose \textbf{Agentic
Self-Learning} (ASL), a fully closed-loop, multi-role reinforcement learning
framework that unifies task generation, policy execution, and evaluation within
a shared tool environment and LLM backbone. ASL coordinates a Prompt Generator,
a Policy Model, and a Generative Reward Model to form a virtuous cycle of
harder task setting, sharper verification, and stronger solving. Empirically,
ASL delivers steady, round-over-round gains, surpasses strong RLVR baselines
(e.g., Search-R1) that plateau or degrade, and continues improving under
zero-labeled-data conditions, indicating superior sample efficiency and
robustness. We further show that GRM verification capacity is the main
bottleneck: if frozen, it induces reward hacking and stalls progress; continual
GRM training on the evolving data distribution mitigates this, and a small
late-stage injection of real verification data raises the performance ceiling.
This work establishes reward source and data scale as critical levers for
open-domain agent learning and demonstrates the efficacy of multi-role
co-evolution for scalable, self-improving agents. The data and code of this
paper are released at
https://github.com/forangel2014/Towards-Agentic-Self-Learning

</details>


### [116] [MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning](https://arxiv.org/abs/2510.14265)
*Xukai Wang,Xuanbo Liu,Mingrui Chen,Haitian Zhong,Xuanlin Yang,Bohan Zeng,Jinbo Hu,Hao Liang,Junbo Niu,Xuchen Li,Ruitao Wu,Ruichuan An,Yang Shi,Liu Liu,Xu-Yao Zhang,Qiang Liu,Zhouchen Lin,Wentao Zhang,Bin Dong*

Main category: cs.AI

TL;DR: 提出了MorphoBench，一个用于评估大型模型推理能力，并且可以根据模型能力调整问题难度的基准。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估大型模型的推理能力方面范围有限，缺乏根据模型推理能力演变来调整难度的灵活性。

Method: 通过从现有基准和奥林匹克竞赛等来源选择和收集复杂的推理问题来管理基准。此外，MorphoBench 通过利用模型推理过程中生成的关键陈述来适应性地修改问题的分析挑战。此外，它还包括使用仿真软件生成的问题，从而能够以最小的资源消耗动态调整基准难度。

Result: 收集了超过 1,300 个测试问题，并根据 o3 和 GPT-5 等模型的推理能力迭代调整了 MorphoBench 的难度。

Conclusion: MorphoBench 增强了模型推理评估的全面性和有效性，为提高大型模型的推理能力和科学鲁棒性提供了可靠的指导。

Abstract: With the advancement of powerful large-scale reasoning models, effectively
evaluating the reasoning capabilities of these models has become increasingly
important. However, existing benchmarks designed to assess the reasoning
abilities of large models tend to be limited in scope and lack the flexibility
to adapt their difficulty according to the evolving reasoning capacities of the
models. To address this, we propose MorphoBench, a benchmark that incorporates
multidisciplinary questions to evaluate the reasoning capabilities of large
models and can adjust and update question difficulty based on the reasoning
abilities of advanced models. Specifically, we curate the benchmark by
selecting and collecting complex reasoning questions from existing benchmarks
and sources such as Olympiad-level competitions. Additionally, MorphoBench
adaptively modifies the analytical challenge of questions by leveraging key
statements generated during the model's reasoning process. Furthermore, it
includes questions generated using simulation software, enabling dynamic
adjustment of benchmark difficulty with minimal resource consumption. We have
gathered over 1,300 test questions and iteratively adjusted the difficulty of
MorphoBench based on the reasoning capabilities of models such as o3 and GPT-5.
MorphoBench enhances the comprehensiveness and validity of model reasoning
evaluation, providing reliable guidance for improving both the reasoning
abilities and scientific robustness of large models. The code has been released
in https://github.com/OpenDCAI/MorphoBench.

</details>


### [117] [A Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space](https://arxiv.org/abs/2510.14301)
*Bingjie Zhang,Yibo Yang,Renzhe,Dandan Guo,Jindong Gu,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: 提出 GuardSpace 框架，用于在微调过程中保持 LLM 的安全性对齐。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 在各种任务中取得了显著的成功，但它们的安全对齐在适应过程中仍然很脆弱。即使在良性数据上进行微调或使用低秩自适应，预训练的安全行为也很容易退化，从而导致微调模型中的有害反应。

Method: 使用协方差预处理奇异值分解将预训练权重显式分解为安全相关和安全不相关的组件，并从安全不相关的组件初始化低秩适配器，同时冻结安全相关组件以保持其相关的安全机制。构建一个零空间投影器，该投影器限制适配器更新改变有害提示上的安全输出，从而保持原始的拒绝行为。

Result: 在多个下游任务的各种预训练模型上进行的实验表明，GuardSpace 实现了优于现有方法的性能。对于在 GSM8K 上微调的 Llama-2-7B-Chat，GuardSpace 优于最先进的方法 AsFT，将平均有害分数从 14.4% 降低到 3.6%，同时将准确率从 26.0% 提高到 28.0%。

Conclusion: GuardSpace 是一种有效的安全对齐保持框架，可以在微调过程中显著提高 LLM 的安全性，同时保持或提高其性能。

Abstract: Large language models (LLMs) have achieved remarkable success in diverse
tasks, yet their safety alignment remains fragile during adaptation. Even when
fine-tuning on benign data or with low-rank adaptation, pre-trained safety
behaviors are easily degraded, leading to harmful responses in the fine-tuned
models. To address this challenge, we propose GuardSpace, a guardrail framework
for preserving safety alignment throughout fine-tuning, composed of two key
components: a safety-sensitive subspace and a harmful-resistant null space.
First, we explicitly decompose pre-trained weights into safety-relevant and
safety-irrelevant components using covariance-preconditioned singular value
decomposition, and initialize low-rank adapters from the safety-irrelevant
ones, while freezing safety-relevant components to preserve their associated
safety mechanism. Second, we construct a null space projector that restricts
adapter updates from altering safe outputs on harmful prompts, thereby
maintaining the original refusal behavior. Experiments with various pre-trained
models on multiple downstream tasks demonstrate that GuardSpace achieves
superior performance over existing methods. Notably, for Llama-2-7B-Chat
fine-tuned on GSM8K, GuardSpace outperforms the state-of-the-art method AsFT,
reducing the average harmful score from 14.4% to 3.6%, while improving the
accuracy from from 26.0% to 28.0%.

</details>


### [118] [TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence](https://arxiv.org/abs/2510.14670)
*Marco Simoni,Aleksandar Fontana,Andrea Saracino,Paolo Mori*

Main category: cs.AI

TL;DR: TITAN is a framework that connects natural-language cyber threat queries with executable reasoning over a structured knowledge graph.


<details>
  <summary>Details</summary>
Motivation: Unlike traditional retrieval systems, TITAN operates on a typed, bidirectional graph derived from MITRE, allowing reasoning to move clearly and reversibly between threats, behaviors, and defenses.

Method: It integrates a path planner model, which predicts logical relation chains from text, and a graph executor that traverses the TITAN Ontology to retrieve factual answers and supporting evidence.

Result: Empirical evaluations show that TITAN enables models to generate syntactically valid and semantically coherent reasoning paths that can be deterministically executed on the underlying graph.

Conclusion: To support training and evaluation, the authors introduce the TITAN Dataset, a corpus of 88209 examples pairing natural language questions with executable reasoning paths and step by step Chain of Thought explanations.

Abstract: TITAN (Threat Intelligence Through Automated Navigation) is a framework that
connects natural-language cyber threat queries with executable reasoning over a
structured knowledge graph. It integrates a path planner model, which predicts
logical relation chains from text, and a graph executor that traverses the
TITAN Ontology to retrieve factual answers and supporting evidence. Unlike
traditional retrieval systems, TITAN operates on a typed, bidirectional graph
derived from MITRE, allowing reasoning to move clearly and reversibly between
threats, behaviors, and defenses. To support training and evaluation, we
introduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test:
13951) pairing natural language questions with executable reasoning paths and
step by step Chain of Thought explanations. Empirical evaluations show that
TITAN enables models to generate syntactically valid and semantically coherent
reasoning paths that can be deterministically executed on the underlying graph.

</details>


### [119] [Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies](https://arxiv.org/abs/2510.14312)
*Mason Nakamura,Abhinav Kumar,Saaduddin Mahmud,Sahar Abdelnabi,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: 本研究提出了一个名为Terrarium的框架，用于细粒度地研究基于大型语言模型的多智能体系统中的安全性、隐私和安全问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型驱动的多智能体系统可以自动化繁琐的用户任务，如需要智能体间协作的会议安排。然而，这种设计引入了新的风险，包括恶意方的错位和攻击，这些恶意方会危及智能体或窃取用户数据。

Method: 我们重新利用黑板设计，这是一种多智能体系统中的早期方法，以创建一个用于多智能体协作的模块化、可配置的测试平台。我们确定了关键的攻击媒介，如错位、恶意智能体、受损的通信和数据中毒。

Result: 我们实现了三个协作MAS场景，并进行了四次代表性攻击，以证明该框架的灵活性。

Conclusion: 通过提供快速原型设计、评估和迭代防御和设计的工具，Terrarium旨在加速在可信赖的多智能体系统方面的进展。

Abstract: A multi-agent system (MAS) powered by large language models (LLMs) can
automate tedious user tasks such as meeting scheduling that requires
inter-agent collaboration. LLMs enable nuanced protocols that account for
unstructured private data, user constraints, and preferences. However, this
design introduces new risks, including misalignment and attacks by malicious
parties that compromise agents or steal user data. In this paper, we propose
the Terrarium framework for fine-grained study on safety, privacy, and security
in LLM-based MAS. We repurpose the blackboard design, an early approach in
multi-agent systems, to create a modular, configurable testbed for multi-agent
collaboration. We identify key attack vectors such as misalignment, malicious
agents, compromised communication, and data poisoning. We implement three
collaborative MAS scenarios with four representative attacks to demonstrate the
framework's flexibility. By providing tools to rapidly prototype, evaluate, and
iterate on defenses and designs, Terrarium aims to accelerate progress toward
trustworthy multi-agent systems.

</details>


### [120] [Metacognitive Self-Correction for Multi-Agent System via Prototype-Guided Next-Execution Reconstruction](https://arxiv.org/abs/2510.14319)
*Xu Shen,Qi Zhang,Song Wang,Zhen Tan,Xinyu Zhao,Laura Yao,Vaishnav Tadiparthi,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Kwonjoon Lee,Tianlong Chen*

Main category: cs.AI

TL;DR: MASC: A metacognitive framework for multi-agent systems that detects and corrects errors in real-time.


<details>
  <summary>Details</summary>
Motivation: Large Language Model based multi-agent systems are prone to cascading errors.

Method: MASC uses history-conditioned anomaly scoring with Next-Execution Reconstruction and Prototype-Guided Enhancement to detect errors and triggers a correction agent to revise the output.

Result: MASC improves step-level error detection by up to 8.47% AUC-ROC on the Who&When benchmark and delivers consistent end-to-end gains across architectures.

Conclusion: MASC's metacognitive monitoring and targeted correction can mitigate error propagation with minimal overhead.

Abstract: Large Language Model based multi-agent systems (MAS) excel at collaborative
problem solving but remain brittle to cascading errors: a single faulty step
can propagate across agents and disrupt the trajectory. In this paper, we
present MASC, a metacognitive framework that endows MAS with real-time,
unsupervised, step-level error detection and self-correction. MASC rethinks
detection as history-conditioned anomaly scoring via two complementary designs:
(1) Next-Execution Reconstruction, which predicts the embedding of the next
step from the query and interaction history to capture causal consistency, and
(2) Prototype-Guided Enhancement, which learns a prototype prior over
normal-step embeddings and uses it to stabilize reconstruction and anomaly
scoring under sparse context (e.g., early steps). When an anomaly step is
flagged, MASC triggers a correction agent to revise the acting agent's output
before information flows downstream. On the Who&When benchmark, MASC
consistently outperforms all baselines, improving step-level error detection by
up to 8.47% AUC-ROC ; When plugged into diverse MAS frameworks, it delivers
consistent end-to-end gains across architectures, confirming that our
metacognitive monitoring and targeted correction can mitigate error propagation
with minimal overhead.

</details>


### [121] [AI for Service: Proactive Assistance with AI Glasses](https://arxiv.org/abs/2510.14359)
*Zichen Wen,Yiyu Wang,Chenfei Liao,Boxue Yang,Junxian Li,Weifeng Liu,Haocong He,Bolong Feng,Xuyang Liu,Yuanhuiyi Lyu,Xu Zheng,Xuming Hu,Linfeng Zhang*

Main category: cs.AI

TL;DR: 提出了一种新的AI范式，AI4Service，旨在实现主动和实时的日常生活中帮助。通过AI眼镜实现，包含五个关键组件：输入单元、中央处理单元、算术逻辑单元、存储单元和输出单元。


<details>
  <summary>Details</summary>
Motivation: 现有的AI服务在很大程度上是被动的，仅响应明确的用户命令。一个真正智能和有用的助手应该能够预测用户需求并在适当时主动采取行动。

Method: 提出了Alpha-Service，一个统一的框架，解决了两个基本挑战：何时介入，通过检测以自我为中心的视频流中的服务机会；如何提供通用和个性化的服务。受到冯·诺依曼计算机体系结构的启发，Alpha-Service基于AI眼镜，由五个关键组件组成。

Result: 通过在AI眼镜上部署的多智能体系统实现了Alpha-Service。案例研究，包括实时Blackjack顾问、博物馆导游和购物助手，证明了它能够无缝地感知环境，推断用户意图，并提供及时和有用的帮助，而无需明确的提示。

Conclusion: AI4Service 是一种有潜力的新范式，它可以通过预测用户需求并主动提供帮助来改善日常生活。Alpha-Service 框架是朝着这个方向迈出的有希望的一步，案例研究表明了其在各种应用中的潜力。

Abstract: In an era where AI is evolving from a passive tool into an active and
adaptive companion, we introduce AI for Service (AI4Service), a new paradigm
that enables proactive and real-time assistance in daily life. Existing AI
services remain largely reactive, responding only to explicit user commands. We
argue that a truly intelligent and helpful assistant should be capable of
anticipating user needs and taking actions proactively when appropriate. To
realize this vision, we propose Alpha-Service, a unified framework that
addresses two fundamental challenges: Know When to intervene by detecting
service opportunities from egocentric video streams, and Know How to provide
both generalized and personalized services. Inspired by the von Neumann
computer architecture and based on AI glasses, Alpha-Service consists of five
key components: an Input Unit for perception, a Central Processing Unit for
task scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit
for long-term personalization, and an Output Unit for natural human
interaction. As an initial exploration, we implement Alpha-Service through a
multi-agent system deployed on AI glasses. Case studies, including a real-time
Blackjack advisor, a museum tour guide, and a shopping fit assistant,
demonstrate its ability to seamlessly perceive the environment, infer user
intent, and provide timely and useful assistance without explicit prompts.

</details>


### [122] [Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?](https://arxiv.org/abs/2510.14387)
*Yijie Hu,Zihao Zhou,Kaizhu Huang,Xiaowei Huang,Qiufeng Wang*

Main category: cs.AI

TL;DR: 提出了一种无需调整即可将数学LLM的数学推理能力迁移到MLLM的方法，称为IP-Merging。


<details>
  <summary>Details</summary>
Motivation: 多模态LLM(MLLM)的数学推理能力滞后于LLM，并且MLLM和LLM之间存在参数空间差距。

Method: 首先识别MLLM和数学LLM中与推理相关的参数，然后将它们投影到MLLM的子空间中，以保持对齐，最后合并该子空间中的参数。

Result: IP-Merging方法可以直接增强MLLM的数学推理能力，而不会影响其其他能力。

Conclusion: IP-Merging是一种有效的无需调整的方法，可以将数学LLM的数学推理能力迁移到MLLM。

Abstract: Math reasoning has been one crucial ability of large language models (LLMs),
where significant advancements have been achieved in recent years. However,
most efforts focus on LLMs by curating high-quality annotation data and
intricate training (or inference) paradigms, while the math reasoning
performance of multi-modal LLMs (MLLMs) remains lagging behind. Since the MLLM
typically consists of an LLM and a vision block, we wonder: Can MLLMs directly
absorb math reasoning abilities from off-the-shelf math LLMs without tuning?
Recent model-merging approaches may offer insights into this question. However,
they overlook the alignment between the MLLM and LLM, where we find that there
is a large gap between their parameter spaces, resulting in lower performance.
Our empirical evidence reveals two key factors behind this issue: the
identification of crucial reasoning-associated layers in the model and the
mitigation of the gaps in parameter space. Based on the empirical insights, we
propose IP-Merging that first identifies the reasoning-associated parameters in
both MLLM and Math LLM, then projects them into the subspace of MLLM, aiming to
maintain the alignment, and finally merges parameters in this subspace.
IP-Merging is a tuning-free approach since parameters are directly adjusted.
Extensive experiments demonstrate that our IP-Merging method can enhance the
math reasoning ability of MLLMs directly from Math LLMs without compromising
their other capabilities.

</details>


### [123] [Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control](https://arxiv.org/abs/2510.14388)
*Zhe Wu,Hongjin Lu,Junliang Xing,Changhao Zhang,Yin Zhu,Yuhao Yang,Yuheng Jing,Kai Li,Kun Shao,Jianye Hao,Jun Wang,Yuanchun Shi*

Main category: cs.AI

TL;DR: Hi-Agent: a hierarchical vision-language agent for mobile control, achieving SOTA results on Android-in-the-Wild benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack structured reasoning and planning, leading to poor generalization to new tasks or unseen UI layouts.

Method: A trainable hierarchical vision-language agent with a high-level reasoning model and a low-level action model, jointly optimized using a reformulated multi-step decision-making approach and a foresight advantage function.

Result: Achieved a new SOTA 87.9% task success rate on the Android-in-the-Wild benchmark, outperforming prior methods. Demonstrated competitive zero-shot generalization on the ScreenSpot-v2 benchmark and scales effectively on the AndroidWorld benchmark.

Conclusion: Hi-Agent shows strong adaptability in high-complexity mobile control scenarios.

Abstract: Building agents that autonomously operate mobile devices has attracted
increasing attention. While Vision-Language Models (VLMs) show promise, most
existing approaches rely on direct state-to-action mappings, which lack
structured reasoning and planning, and thus generalize poorly to novel tasks or
unseen UI layouts. We introduce Hi-Agent, a trainable hierarchical
vision-language agent for mobile control, featuring a high-level reasoning
model and a low-level action model that are jointly optimized. For efficient
training, we reformulate multi-step decision-making as a sequence of
single-step subgoals and propose a foresight advantage function, which
leverages execution feedback from the low-level model to guide high-level
optimization. This design alleviates the path explosion issue encountered by
Group Relative Policy Optimization (GRPO) in long-horizon tasks and enables
stable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art
(SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark,
significantly outperforming prior methods across three paradigms: prompt-based
(AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement
learning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot
generalization on the ScreenSpot-v2 benchmark. On the more challenging
AndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones,
showing strong adaptability in high-complexity mobile control scenarios.

</details>


### [124] [IMAGINE: Integrating Multi-Agent System into One Model for Complex Reasoning and Planning](https://arxiv.org/abs/2510.14406)
*Xikai Zhang,Bo Wang,Likang Xiao,Yongzhi Li,Quan Chen,Wenju Wu,Liu Liu*

Main category: cs.AI

TL;DR: 提出了IMAGINE框架，将多智能体系统的推理和规划能力集成到单个模型中。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理和规划方面面临挑战，且多智能体系统存在推理成本高的问题。

Method: 将多智能体系统集成到单个模型中，并通过端到端训练进行优化。

Result: 在TravelPlanner基准测试中，使用Qwen3-8B-Instruct作为基础模型，该模型达到了82.7%的最终通过率，远超DeepSeek-R1-671B的40%。

Conclusion: IMAGINE框架能够使小规模模型获得结构化推理和规划能力，并显著优于传统的多智能体系统。

Abstract: Although large language models (LLMs) have made significant strides across
various tasks, they still face significant challenges in complex reasoning and
planning. For example, even with carefully designed prompts and prior
information explicitly provided, GPT-4o achieves only a 7% Final Pass Rate on
the TravelPlanner dataset in the sole-planning mode. Similarly, even in the
thinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B, only achieve Final Pass
Rates of 5.9% and 40%, respectively. Although well-organized Multi-Agent
Systems (MAS) can offer improved collective reasoning, they often suffer from
high reasoning costs due to multi-round internal interactions, long
per-response latency, and difficulties in end-to-end training. To address these
challenges, we propose a general and scalable framework called IMAGINE, short
for Integrating Multi-Agent System into One Model. This framework not only
integrates the reasoning and planning capabilities of MAS into a single,
compact model, but also significantly surpass the capabilities of the MAS
through a simple end-to-end training. Through this pipeline, a single
small-scale model is not only able to acquire the structured reasoning and
planning capabilities of a well-organized MAS but can also significantly
outperform it. Experimental results demonstrate that, when using
Qwen3-8B-Instruct as the base model and training it with our method, the model
achieves an 82.7% Final Pass Rate on the TravelPlanner benchmark, far exceeding
the 40% of DeepSeek-R1-671B, while maintaining a much smaller model size.

</details>


### [125] [Eliminating Negative Occurrences of Derived Predicates from PDDL Axioms](https://arxiv.org/abs/2510.14412)
*Claudia Grundke,Gabriele Röger*

Main category: cs.AI

TL;DR: 本文提出了一种转换方法，用于消除规划领域定义语言（PDDL）公理中派生谓词的负面出现。


<details>
  <summary>Details</summary>
Motivation: PDDL标准限制了公理主体中谓词的负面出现，但作者经常偏离此限制。本文旨在解决这一问题。

Method: 提出一种转换方法。

Result: 提出的转换方法可以消除派生谓词的负面出现。

Conclusion: 提出的方法与最小不动点逻辑表达相同的查询。

Abstract: Axioms are a feature of the Planning Domain Definition Language PDDL that can
be considered as a generalization of database query languages such as Datalog.
The PDDL standard restricts negative occurrences of predicates in axiom bodies
to predicates that are directly set by actions and not derived by axioms. In
the literature, authors often deviate from this limitation and only require
that the set of axioms is stratifiable. Both variants can express exactly the
same queries as least fixed-point logic, indicating that negative occurrences
of derived predicates can be eliminated. We present the corresponding
transformation.

</details>


### [126] [Helmsman: Autonomous Synthesis of Federated Learning Systems via Multi-Agent Collaboration](https://arxiv.org/abs/2510.14512)
*Haoyuan Li,Mathias Funk,Aaqib Saeed*

Main category: cs.AI

TL;DR: Helmsman是一个多智能体系统，可以自动合成联邦学习系统。


<details>
  <summary>Details</summary>
Motivation: 联邦学习系统设计和部署非常复杂，需要选择、组合和调整策略来应对数据异构和系统约束等多方面挑战。

Method: Helmsman通过三个协作阶段模拟了一个有原则的研发工作流程：(1)交互式人机循环规划，以制定合理的研发计划，(2)由监督代理团队进行模块化代码生成，以及(3)在沙盒模拟环境中进行自主评估和改进的闭环。

Result: Helmsman生成的解决方案与已建立的手工基线相比具有竞争力，并且通常优于这些基线。

Conclusion: 这项工作代表了朝着复杂的分散式人工智能系统自动化工程迈出的重要一步。

Abstract: Federated Learning (FL) offers a powerful paradigm for training models on
decentralized data, but its promise is often undermined by the immense
complexity of designing and deploying robust systems. The need to select,
combine, and tune strategies for multifaceted challenges like data
heterogeneity and system constraints has become a critical bottleneck,
resulting in brittle, bespoke solutions. To address this, we introduce
Helmsman, a novel multi-agent system that automates the end-to-end synthesis of
federated learning systems from high-level user specifications. It emulates a
principled research and development workflow through three collaborative
phases: (1) interactive human-in-the-loop planning to formulate a sound
research plan, (2) modular code generation by supervised agent teams, and (3) a
closed-loop of autonomous evaluation and refinement in a sandboxed simulation
environment. To facilitate rigorous evaluation, we also introduce
AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess
the system-level generation capabilities of agentic systems in FL. Extensive
experiments demonstrate that our approach generates solutions competitive with,
and often superior to, established hand-crafted baselines. Our work represents
a significant step towards the automated engineering of complex decentralized
AI systems.

</details>


### [127] [JSPLIT: A Taxonomy-based Solution for Prompt Bloating in Model Context Protocol](https://arxiv.org/abs/2510.14537)
*Emanuele Antonioni,Stefan Markovic,Anirudha Shankar,Jaime Bernardo,Lovro Markovic,Silvia Pareti,Benedetto Proietti*

Main category: cs.AI

TL;DR: JSPLIT框架通过分层分类工具，并根据用户提示选择相关工具，有效管理大型语言模型（LLM）代理中的提示大小，从而降低成本并提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）代理需要与外部工具交互，但随着工具数量的增加，提示变得冗长，导致成本高、延迟增加和任务成功率降低。

Method: 提出JSPLIT框架，该框架将工具组织成层级分类，并使用用户提示来识别和仅包含最相关的工具。

Result: JSPLIT显著减少了提示大小，且不会显著影响代理的有效响应能力。随着代理可用工具数量的大幅增长，JSPLIT甚至提高了代理的工具选择准确性。

Conclusion: JSPLIT在降低成本的同时，有效地提高了高复杂度代理环境中的任务成功率。

Abstract: AI systems are continually evolving and advancing, and user expectations are
concurrently increasing, with a growing demand for interactions that go beyond
simple text-based interaction with Large Language Models (LLMs). Today's
applications often require LLMs to interact with external tools, marking a
shift toward more complex agentic systems. To support this, standards such as
the Model Context Protocol (MCP) have emerged, enabling agents to access tools
by including a specification of the capabilities of each tool within the
prompt. Although this approach expands what agents can do, it also introduces a
growing problem: prompt bloating. As the number of tools increases, the prompts
become longer, leading to high prompt token costs, increased latency, and
reduced task success resulting from the selection of tools irrelevant to the
prompt. To address this issue, we introduce JSPLIT, a taxonomy-driven framework
designed to help agents manage prompt size more effectively when using large
sets of MCP tools. JSPLIT organizes the tools into a hierarchical taxonomy and
uses the user's prompt to identify and include only the most relevant tools,
based on both the query and the taxonomy structure. In this paper, we describe
the design of the taxonomy, the tool selection algorithm, and the dataset used
to evaluate JSPLIT. Our results show that JSPLIT significantly reduces prompt
size without significantly compromising the agent's ability to respond
effectively. As the number of available tools for the agent grows
substantially, JSPLIT even improves the tool selection accuracy of the agent,
effectively reducing costs while simultaneously improving task success in
high-complexity agent environments.

</details>


### [128] [Symbol Grounding in Neuro-Symbolic AI: A Gentle Introduction to Reasoning Shortcuts](https://arxiv.org/abs/2510.14538)
*Emanuele Marconato,Samuele Bortolotti,Emile van Krieken,Paolo Morettin,Elena Umili,Antonio Vergari,Efthymia Tsamoura,Andrea Passerini,Stefano Teso*

Main category: cs.AI

TL;DR: 这篇论文概述了神经符号人工智能（NeSy AI）中推理捷径（RSs）的问题，旨在提高该领域的可信度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管NeSy AI很有前景，但当概念没有直接监督时，模型会受到推理捷径的影响，从而损害模型的可解释性、泛化能力和可靠性。此外，RSs难以检测和预防。

Method: 本文对RSs进行了温和的介绍，讨论了其原因和后果，回顾了现有的理论特征，并详细介绍了处理RSs的方法，包括缓解和意识策略。

Result: 本文旨在以易于理解的形式重塑高级材料，以统一的视角看待RSs，从而降低处理它们的门槛。

Conclusion: 本文旨在为开发可靠的NeSy和值得信赖的AI模型做出贡献。

Abstract: Neuro-symbolic (NeSy) AI aims to develop deep neural networks whose
predictions comply with prior knowledge encoding, e.g. safety or structural
constraints. As such, it represents one of the most promising avenues for
reliable and trustworthy AI. The core idea behind NeSy AI is to combine neural
and symbolic steps: neural networks are typically responsible for mapping
low-level inputs into high-level symbolic concepts, while symbolic reasoning
infers predictions compatible with the extracted concepts and the prior
knowledge. Despite their promise, it was recently shown that - whenever the
concepts are not supervised directly - NeSy models can be affected by Reasoning
Shortcuts (RSs). That is, they can achieve high label accuracy by grounding the
concepts incorrectly. RSs can compromise the interpretability of the model's
explanations, performance in out-of-distribution scenarios, and therefore
reliability. At the same time, RSs are difficult to detect and prevent unless
concept supervision is available, which is typically not the case. However, the
literature on RSs is scattered, making it difficult for researchers and
practitioners to understand and tackle this challenging problem. This overview
addresses this issue by providing a gentle introduction to RSs, discussing
their causes and consequences in intuitive terms. It also reviews and
elucidates existing theoretical characterizations of this phenomenon. Finally,
it details methods for dealing with RSs, including mitigation and awareness
strategies, and maps their benefits and limitations. By reformulating advanced
material in a digestible form, this overview aims to provide a unifying
perspective on RSs to lower the bar to entry for tackling them. Ultimately, we
hope this overview contributes to the development of reliable NeSy and
trustworthy AI models.

</details>


### [129] [LLM Agents Beyond Utility: An Open-Ended Perspective](https://arxiv.org/abs/2510.14548)
*Asen Nachkov,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）智能代理通过思维链推理和函数调用得到广泛应用。本文探讨了LLM是否能像一个独立的实体一样，进行规划、设计任务并朝着更广泛的目标推理。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM智能代理是否能像一个独立的实体一样，进行规划、设计任务并朝着更广泛的目标推理。

Method: 通过实验，增强了预训练的LLM代理，使其能够生成自己的任务、积累知识并与环境交互。

Result: 实验结果表明，该代理能够可靠地遵循复杂的多步骤指令，跨运行存储和重用信息，并提出和解决自己的任务。但也存在对提示设计敏感、容易重复生成任务以及无法形成自我表征的问题。

Conclusion: 这些发现展示了预训练LLM在开放性方面的潜力和局限性，并为训练代理以管理记忆、有效探索和追求抽象的长期目标指明了未来方向。

Abstract: Recent LLM agents have made great use of chain of thought reasoning and
function calling. As their capabilities grow, an important question arises: can
this software represent not only a smart problem-solving tool, but an entity in
its own right, that can plan, design immediate tasks, and reason toward
broader, more ambiguous goals? To study this question, we adopt an open-ended
experimental setting where we augment a pretrained LLM agent with the ability
to generate its own tasks, accumulate knowledge, and interact extensively with
its environment. We study the resulting open-ended agent qualitatively. It can
reliably follow complex multi-step instructions, store and reuse information
across runs, and propose and solve its own tasks, though it remains sensitive
to prompt design, prone to repetitive task generation, and unable to form
self-representations. These findings illustrate both the promise and current
limits of adapting pretrained LLMs toward open-endedness, and point to future
directions for training agents to manage memory, explore productively, and
pursue abstract long-term goals.

</details>


### [130] [ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks](https://arxiv.org/abs/2510.14621)
*Yuanyi Song,Heyuan Huang,Qiqiang Lin,Yin Zhao,Xiangmou Qu,Jun Wang,Xingyu Lou,Weiwen Liu,Zhuosheng Zhang,Jun Wang,Yong Yu,Weinan Zhang,Zhaoxiang Wang*

Main category: cs.AI

TL;DR: 提出了一种新的图结构基准框架ColorBench，用于评估移动代理在复杂、长程任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的移动代理评估标准（离线静态基准和在线动态测试）无法全面评估代理的能力，因为前者只能验证单个预定义的“黄金路径”，而后者受限于真实设备的复杂性和不可重复性。

Method: 通过对真实设备交互中观察到的有限状态进行建模，实现了动态行为的静态模拟。构建了一个图结构基准框架ColorBench，专注于复杂的长程任务。

Result: ColorBench包含175个任务（74个单应用，101个跨应用），平均长度超过13步。每个任务包含至少两条正确路径和若干典型错误路径，实现了准动态交互。通过在各种基线上评估ColorBench，发现了现有模型的局限性。

Conclusion: 实验结果揭示了现有模型的不足，并提出了改进方向和可行的技术途径，以提高代理在复杂、长程问题上的性能。

Abstract: The rapid advancement of multimodal large language models has enabled agents
to operate mobile devices by directly interacting with graphical user
interfaces, opening new possibilities for mobile automation. However,
real-world mobile tasks are often complex and allow for multiple valid
solutions. This contradicts current mobile agent evaluation standards: offline
static benchmarks can only validate a single predefined "golden path", while
online dynamic testing is constrained by the complexity and non-reproducibility
of real devices, making both approaches inadequate for comprehensively
assessing agent capabilities. To bridge the gap between offline and online
evaluation and enhance testing stability, this paper introduces a novel
graph-structured benchmarking framework. By modeling the finite states observed
during real-device interactions, it achieves static simulation of dynamic
behaviors. Building on this, we develop ColorBench, a benchmark focused on
complex long-horizon tasks. It supports evaluation of multiple valid solutions,
subtask completion rate statistics, and atomic-level capability analysis.
ColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average
length of over 13 steps. Each task includes at least two correct paths and
several typical error paths, enabling quasi-dynamic interaction. By evaluating
ColorBench across various baselines, we discover limitations of existing models
and propose improvement directions and feasible technical pathways to enhance
agents' performance on complex, long-horizon problems based on experimental
results. Code and data are available at:
https://github.com/MadeAgents/ColorBench.

</details>


### [131] [Beyond Hallucinations: The Illusion of Understanding in Large Language Models](https://arxiv.org/abs/2510.14665)
*Rikard Rosenbacke,Carl Rosenbacke,Victor Rosenbacke,Martin McKee*

Main category: cs.AI

TL;DR: 大型语言模型(llm)正日益嵌入到人类沟通和决策中，但它们也继承了语言本身固有的模糊性、偏见和缺乏直接获取真相的缺陷。本文认为llm以规模化的方式运行系统1认知：快速、联想和有说服力，但没有反思或证伪。为了解决这个问题，我们引入了rose-frame，这是一个三维框架，用于诊断人机交互中的认知和认知漂移。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通过统计预测生成流畅、情感共鸣和连贯的输出，而不是基于理由的推理，这造成了幻觉的风险，即听起来令人信服但缺乏事实有效性的反应。

Method: 本文介绍了rose-frame，这是一个三维框架，用于诊断人机交互中的认知和认知漂移。三个轴是：(i)地图与领域，它区分了现实的表征(认识论)与现实本身(本体论)；(ii)直觉与理性，借鉴双重过程理论，将快速的情感判断与缓慢的、反思性的思考区分开来；(iii)冲突与确认，它检查思想是否通过分歧进行批判性测试，或者只是通过相互验证来加强。

Result: rose-frame没有试图用更多的数据或规则来修复llm。相反，它提供了一个反思工具，使模型的局限性和用户的假设都可见，从而实现更透明和批判性地意识到人工智能的部署。它将对齐重新定义为认知治理：无论是人类的还是人工的直觉，都必须受到人类理性的支配。

Conclusion: 只有通过嵌入反思性的、可证伪的监督，我们才能使机器的流畅性与人类的理解相一致。

Abstract: Large language models (LLMs) are becoming deeply embedded in human
communication and decision-making, yet they inherit the ambiguity, bias, and
lack of direct access to truth inherent in language itself. While their outputs
are fluent, emotionally resonant, and coherent, they are generated through
statistical prediction rather than grounded reasoning. This creates the risk of
hallucination, responses that sound convincing but lack factual validity.
Building on Geoffrey Hinton's observation that AI mirrors human intuition
rather than reasoning, this paper argues that LLMs operationalize System 1
cognition at scale: fast, associative, and persuasive, but without reflection
or falsification. To address this, we introduce the Rose-Frame, a
three-dimensional framework for diagnosing cognitive and epistemic drift in
human-AI interaction. The three axes are: (i) Map vs. Territory, which
distinguishes representations of reality (epistemology) from reality itself
(ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to
separate fast, emotional judgments from slow, reflective thinking; and (iii)
Conflict vs. Confirmation, which examines whether ideas are critically tested
through disagreement or simply reinforced through mutual validation. Each
dimension captures a distinct failure mode, and their combination amplifies
misalignment. Rose-Frame does not attempt to fix LLMs with more data or rules.
Instead, it offers a reflective tool that makes both the model's limitations
and the user's assumptions visible, enabling more transparent and critically
aware AI deployment. It reframes alignment as cognitive governance: intuition,
whether human or artificial, must remain governed by human reason. Only by
embedding reflective, falsifiable oversight can we align machine fluency with
human understanding.

</details>


### [132] [Machine Learning and Public Health: Identifying and Mitigating Algorithmic Bias through a Systematic Review](https://arxiv.org/abs/2510.14669)
*Sara Altamirano,Arjan Vreeken,Sennay Ghebreab*

Main category: cs.AI

TL;DR: 这篇论文综述了 2021 年至 2025 年荷兰公共卫生 ML 研究中算法偏差的识别、讨论和报告，发现普遍存在差距，并提出了一个名为 ACAR 的四阶段公平框架，以帮助研究人员解决 ML 生命周期中的公平性问题。


<details>
  <summary>Details</summary>
Motivation: 在公共卫生领域，机器学习 (ML) 有望通过改进监测、风险分层和资源分配带来革命性的变化。但是，如果没有系统地关注算法偏差，ML 可能会在无意中加强现有的健康差异。

Method: 该研究通过整合来自已建立框架的要素，开发了算法偏差风险评估工具 (RABAT)，并将其应用于 35 项同行评审研究。

Result: 分析表明，虽然数据抽样和缺失数据实践有详细记录，但大多数研究忽略了明确的公平框架、亚组分析以及对潜在危害的透明讨论。

Conclusion: 论文最后为公共卫生 ML 从业者提出了可操作的建议，以持续考虑算法偏差并提高透明度，确保算法创新能够促进健康公平，而不是破坏它。

Abstract: Machine learning (ML) promises to revolutionize public health through
improved surveillance, risk stratification, and resource allocation. However,
without systematic attention to algorithmic bias, ML may inadvertently
reinforce existing health disparities. We present a systematic literature
review of algorithmic bias identification, discussion, and reporting in Dutch
public health ML research from 2021 to 2025. To this end, we developed the Risk
of Algorithmic Bias Assessment Tool (RABAT) by integrating elements from
established frameworks (Cochrane Risk of Bias, PROBAST, Microsoft Responsible
AI checklist) and applied it to 35 peer-reviewed studies. Our analysis reveals
pervasive gaps: although data sampling and missing data practices are well
documented, most studies omit explicit fairness framing, subgroup analyses, and
transparent discussion of potential harms. In response, we introduce a
four-stage fairness-oriented framework called ACAR (Awareness,
Conceptualization, Application, Reporting), with guiding questions derived from
our systematic literature review to help researchers address fairness across
the ML lifecycle. We conclude with actionable recommendations for public health
ML practitioners to consistently consider algorithmic bias and foster
transparency, ensuring that algorithmic innovations advance health equity
rather than undermine it.

</details>


### [133] [NAEL: Non-Anthropocentric Ethical Logic](https://arxiv.org/abs/2510.14676)
*Bianca Maria Lerma,Rafael Peñaloza*

Main category: cs.AI

TL;DR: NAEL: A novel ethical framework for AI agents.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of existing human-centered ethical models by enabling context-sensitive, adaptive, and relational ethical behavior without anthropomorphic moral intuitions.

Method: Formalizes ethical behavior as minimizing global expected free energy using a neuro-symbolic architecture.

Result: Demonstrates dynamic balancing of self-preservation, learning, and collective welfare in a resource distribution case study.

Conclusion: NAEL allows agents to evaluate ethical consequences in uncertain settings, departing from human-centered approaches.

Abstract: We introduce NAEL (Non-Anthropocentric Ethical Logic), a novel ethical
framework for artificial agents grounded in active inference and symbolic
reasoning. Departing from conventional, human-centred approaches to AI ethics,
NAEL formalizes ethical behaviour as an emergent property of intelligent
systems minimizing global expected free energy in dynamic, multi-agent
environments. We propose a neuro-symbolic architecture to allow agents to
evaluate the ethical consequences of their actions in uncertain settings. The
proposed system addresses the limitations of existing ethical models by
allowing agents to develop context-sensitive, adaptive, and relational ethical
behaviour without presupposing anthropomorphic moral intuitions. A case study
involving ethical resource distribution illustrates NAEL's dynamic balancing of
self-preservation, epistemic learning, and collective welfare.

</details>


### [134] [Practical, Utilitarian Algorithm Configuration](https://arxiv.org/abs/2510.14683)
*Devon Graham,Kevin Leyton-Brown*

Main category: cs.AI

TL;DR: 本文改进了COUP算法，使其在具有理论保证的同时，在实际性能上与启发式配置程序竞争。


<details>
  <summary>Details</summary>
Motivation: 用户效用函数能够灵活地捕获用户对算法运行时的偏好，但COUP算法的实际性能不佳。

Method: 通过一系列改进来提高COUP算法的经验性能，同时不降低其理论保证。

Result: 实验证明了改进的有效性。

Conclusion: 通过案例研究，展示了探索给定解决方案对算法选择问题中效用函数变化的鲁棒性的方法。

Abstract: Utilitarian algorithm configuration identifies a parameter setting for a
given algorithm that maximizes a user's utility. Utility functions offer a
theoretically well-grounded approach to optimizing decision-making under
uncertainty and are flexible enough to capture a user's preferences over
algorithm runtimes (e.g., they can describe a sharp cutoff after which a
solution is no longer required, a per-hour cost for compute, or diminishing
returns from algorithms that take longer to run). COUP is a recently-introduced
utilitarian algorithm configuration procedure which was designed mainly to
offer strong theoretical guarantees about the quality of the configuration it
returns, with less attention paid to its practical performance. This paper
closes that gap, bringing theoretically-grounded, utilitarian algorithm
configuration to the point where it is competitive with widely used, heuristic
configuration procedures that offer no performance guarantees. We present a
series of improvements to COUP that improve its empirical performance without
degrading its theoretical guarantees and demonstrate their benefit
experimentally. Using a case study, we also illustrate ways of exploring the
robustness of a given solution to the algorithm selection problem to variations
in the utility function.

</details>


### [135] [Purifying Task Vectors in Knowledge-Aware Subspace for Model Merging](https://arxiv.org/abs/2510.14697)
*Bang An,Yibo Yang,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: 本文提出了一种名为PAVE的方法，通过在知识感知的子空间中提纯任务向量来改进模型合并。


<details>
  <summary>Details</summary>
Motivation: 模型合并旨在将微调模型的任务特定能力整合到单个模型中，但合并后的模型通常会因任务向量中不相关的冗余而导致性能下降。现有方法通过随机删除参数空间中的元素来克服冗余，但涉及随机性且缺乏知识感知。

Method: PAVE方法首先从每个任务中抽取一些训练样本，并将它们输入到相应的微调模型中，以获取线性层之前的协方差矩阵。然后，执行面向上下文的奇异值分解，突出显示与目标知识最相关的权重分量。最后，通过修剪冗余分量来提纯任务向量。

Result: 实验表明，PAVE在各种合并方法、任务和模型架构中都有效。

Conclusion: PAVE作为一种即插即用的方案，可应用于各种基于任务向量的合并方法，以提高其性能。

Abstract: Model merging aims to integrate task-specific abilities from individually
fine-tuned models into a single model without extra training. In recent model
merging methods, task vector has become a fundamental building block, as it can
encapsulate the residual information from finetuning. However, the merged model
often suffers from notable performance degradation due to the conflicts caused
by task-irrelevant redundancy in task vectors. Existing efforts in overcoming
redundancy by randomly dropping elements in the parameter space involves
randomness and lacks knowledge awareness. To address these challenges, in this
study, we propose Purifying TAsk Vectors (PAVE) in knowledge-aware subspace.
Concretely, we sample some training examples from each task, and feed them into
their corresponding fine-tuned models to acquire the covariance matrices before
linear layers. We then perform a context-oriented singular value decomposition,
which accentuates the weight components most relevant to the target knowledge.
As a result, we can split fine-tuned model weights into task-relevant and
redundant components in the knowledge-aware subspace, and purify the task
vector by pruning the redundant components. To induce fair pruning efforts
across models, we further introduce a spectral rank allocation strategy by
optimizing a normalized activated pruning error. The task vector purification
by our method as a plug-and-play scheme is applicable across various task
vector-based merging methods to improve their performance. In experiments, we
demonstrate the effectiveness of PAVE across a diverse set of merging methods,
tasks, and model architectures.

</details>


### [136] [Cognitive-Aligned Spatio-Temporal Large Language Models For Next Point-of-Interest Prediction](https://arxiv.org/abs/2510.14702)
*Penglong Zhai,Jie Li,Fanyi Di,Yue Liu,Yifang Yuan,Jie Huang,Peng Wu,Sicong Wang,Mingyang Yin,Tingting Hu,Yao Xu,Xin Li*

Main category: cs.AI

TL;DR: 本文提出了一种名为CoAST的框架，用于解决大型语言模型（LLM）在预测下一个兴趣点（POI）时，缺乏对结构化地理实体和时空移动模式的理解，以及难以整合世界知识和人类认知的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）主要在非结构化文本语料库上进行预训练，缺乏对结构化地理实体和时空移动模式的理解，并且难以整合世界知识和人类认知，这限制了它们在下一个兴趣点（POI）预测任务中的性能。

Method: CoAST框架包括两个阶段：（1）通过在丰富的时空轨迹数据上持续预训练来获取推荐知识；（2）通过监督微调（SFT）和强化学习（RL）来对齐认知判断与人类偏好。

Result: 在各种真实世界数据集上进行的离线实验以及在高德地图App首页的“猜你去哪儿”中部署的在线实验表明，CoAST的有效性。

Conclusion: CoAST框架能够有效提升下一个兴趣点（POI）的预测性能，并且具有实际应用价值。

Abstract: The next point-of-interest (POI) recommendation task aims to predict the
users' immediate next destinations based on their preferences and historical
check-ins, holding significant value in location-based services. Recently,
large language models (LLMs) have shown great potential in recommender systems,
which treat the next POI prediction in a generative manner. However, these
LLMs, pretrained primarily on vast corpora of unstructured text, lack the
native understanding of structured geographical entities and sequential
mobility patterns required for next POI prediction tasks. Moreover, in
industrial-scale POI prediction applications, incorporating world knowledge and
alignment of human cognition, such as seasons, weather conditions, holidays,
and users' profiles (such as habits, occupation, and preferences), can enhance
the user experience while improving recommendation performance. To address
these issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a
framework employing natural language as an interface, allowing for the
incorporation of world knowledge, spatio-temporal trajectory patterns,
profiles, and situational information. Specifically, CoAST mainly comprises of
2 stages: (1) Recommendation Knowledge Acquisition through continued
pretraining on the enriched spatial-temporal trajectory data of the
desensitized users; (2) Cognitive Alignment to align cognitive judgments with
human preferences using enriched training data through Supervised Fine-Tuning
(SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline
experiments on various real-world datasets and online experiments deployed in
"Guess Where You Go" of AMAP App homepage demonstrate the effectiveness of
CoAST.

</details>


### [137] [ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling](https://arxiv.org/abs/2510.14703)
*Jianghao Lin,Yuanyuan Shi,Xin Peng,Renjie Ding,Hairui Wang,Yuxuan Peng,Bizhe Bai,Weixi Song,Fengshuo Bai,Huacan Chai,Weinan Zhang,Fei Huang,Ying Wen*

Main category: cs.AI

TL;DR: 本文研究了推理扩展在函数调用中的应用，并提出了一个结合细粒度束搜索和过程奖励模型ToolPRM的推理扩展框架。


<details>
  <summary>Details</summary>
Motivation: 目前关于推理扩展的研究主要集中在非结构化输出生成任务中，而其在结构化输出（如函数调用）中的应用在很大程度上未被探索。

Method: 本文提出了一个推理扩展框架，该框架结合了细粒度束搜索和一个过程奖励模型ToolPRM，该模型对每个函数调用的内部步骤进行评分。为了训练ToolPRM，本文构建了第一个细粒度的内部调用过程监督数据集，该数据集使用函数屏蔽技术自动注释，以为结构化工具使用推理提供步骤级别的奖励。

Result: 大量实验表明，ToolPRM在预测准确性方面优于粗粒度和结果奖励模型，表明其在监督函数调用推理过程方面具有更强的能力。配备ToolPRM的推理扩展技术也显着提高了各种函数调用任务和基准测试中的骨干模型性能。

Conclusion: 本文揭示了将推理扩展技术应用于结构化输出的一个关键原则：由于结构化函数调用生成的不可恢复特性，应“多探索但少保留”。

Abstract: Large language models (LLMs) are increasingly demonstrating strong
capabilities as autonomous agents, with function calling serving as a core
mechanism for interaction with the environment. Meanwhile, inference scaling
has become a cutting-edge technique to enhance LLM performance by allocating
more computational resources during the inference process. However, current
research on inference scaling primarily focuses on unstructured output
generation tasks, leaving its application in structured outputs, like function
calling, largely underexplored. To bridge this gap, we propose an inference
scaling framework that combines fine-grained beam search with a process reward
model, ToolPRM, which scores the internal steps of each single function call.
To train ToolPRM, we construct the first fine-grained intra-call process
supervision dataset, automatically annotated with function-masking techniques
to provide step-level rewards for structured tool-use reasoning. Extensive
experiments demonstrate that ToolPRM beats the coarse-grained and outcome
reward models in terms of predictive accuracy, indicating its stronger
capability in supervising the function calling inference process. Inference
scaling technique equipped with ToolPRM also significantly improves the
backbone model performance across various function calling tasks and
benchmarks. More importantly, we reveal a key principle for applying inference
scaling techniques to structured outputs: "explore more but retain less" due to
the unrecoverability characteristics of structured function calling generation.

</details>


### [138] [SimKO: Simple Pass@K Policy Optimization](https://arxiv.org/abs/2510.14807)
*Ruotian Peng,Yi Ren,Zhouliang Yu,Weiyang Liu,Yandong Wen*

Main category: cs.AI

TL;DR: RLVR方法在提高LLM的推理能力方面取得了进展，但存在过度利用问题，导致pass@1提高但pass@K (K>1) 性能下降。SimKO通过不对称方式缓解过度集中问题，提高探索能力，从而在数学和逻辑推理基准测试中始终产生更高的pass@K。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法存在系统性偏差，过度利用而不足够探索，导致pass@1提高但pass@K (K>1) 性能下降。为了理解这个问题，论文分析了RLVR方法的训练动态，通过跟踪token级别的词汇候选概率分布。

Method: 论文提出Simple Pass@K Optimization (SimKO) 方法，该方法以不对称的方式运行。对于验证正确的响应，它会提高前K个候选者的概率。对于验证不正确的响应，它会对排名第一的候选者施加更强的惩罚。这种不对称设计在应用于具有高熵的token时，特别有效地缓解了过度集中。

Result: SimKO在各种数学和逻辑推理基准测试中，始终产生更高的pass@K，为改进RLVR的探索提供了一种简单方法。

Conclusion: SimKO通过缓解过度集中问题，鼓励探索，从而在各种数学和逻辑推理基准测试中始终产生更高的pass@K，为改进RLVR的探索提供了一种简单方法。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has advanced the
reasoning capabilities of large language models (LLMs). However, prevailing
RLVR methods exhibit a systematic bias toward exploitation over exploration, as
evidenced by improved pass@1 but reduced pass@K (K>1) performance. To
understand this issue, we analyze training dynamics of RLVR methods by tracking
the token-level probability distributions over vocabulary candidates. Our
analysis reveals a consistent probability concentration effect where the top-1
candidate increasingly accumulates probability mass and suppresses that of
other candidates. More importantly, stronger over-concentration correlates with
worse pass@K performance. Inspired by this finding, we propose Simple Pass@K
Optimization (SimKO), a method designed to mitigate the over-concentration
issue, thereby encouraging exploration. SimKO operates in an asymmetrical
manner. For verified-correct responses, it boosts the probabilities of the
top-K candidates. For verified-incorrect responses, it applies stronger
penalties to the top-1 candidate. We observe that this asymmetric design is
particularly effective at mitigating over-concentration when applied at tokens
with high entropy. Across various math and logical-reasoning benchmarks, SimKO
consistently yields higher pass@K for a wide range of K, providing a simple way
to improve RLVR's exploration.

</details>


### [139] [Agentic NL2SQL to Reduce Computational Costs](https://arxiv.org/abs/2510.14808)
*Dominik Jehle,Lennart Purucker,Frank Hutter*

Main category: cs.AI

TL;DR: Datalake Agent通过减少LLM处理的元信息量，更高效地将自然语言查询转换为SQL查询，从而降低成本。


<details>
  <summary>Details</summary>
Motivation: 使用大型语言模型（LLM）将自然语言查询转换为SQL查询需要在大量SQL数据库上处理大量的元信息，导致冗长的提示和高昂的成本。

Method: Datalake Agent采用交互式循环，选择性地请求解决表格问答任务所需的必要信息，从而减少使用的元信息。

Result: Datalake Agent最多可减少LLM使用的tokens达87％，从而在保持竞争性能的同时大幅降低成本。

Conclusion: Datalake Agent通过减少LLM处理的tokens数量，实现了在NL2SQL任务中更高的效率和更低的成本。

Abstract: Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL)
has recently been empowered by large language models (LLMs). Using LLMs to
perform NL2SQL methods on a large collection of SQL databases necessitates
processing large quantities of meta-information about the databases, which in
turn results in lengthy prompts with many tokens and high processing costs. To
address this challenge, we introduce Datalake Agent, an agentic system designed
to enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing
direct solvers for NL2SQL that call the LLM once with all meta-information in
the prompt, the Datalake Agent employs an interactive loop to reduce the
utilized meta-information. Within the loop, the LLM is used in a reasoning
framework that selectively requests only the necessary information to solve a
table question answering task. We evaluate the Datalake Agent on a collection
of 23 databases with 100 table question answering tasks. The Datalake Agent
reduces the tokens used by the LLM by up to 87\% and thus allows for
substantial cost reductions while maintaining competitive performance.

</details>


### [140] [RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning](https://arxiv.org/abs/2510.14828)
*Jinrui Liu,Bingyan Nie,Boyu Li,Yaran Chen,Yuze Wang,Shunsen He,Haoran Li*

Main category: cs.AI

TL;DR: 本文提出了一种名为 RoboGPT-R1 的两阶段微调框架，用于提高具身智能体在复杂操作任务中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型和视觉语言模型在执行复杂现实环境中的长程操作任务时，由于常识和推理能力的限制，面临挑战。监督式微调在机器人规划任务中泛化能力差，且缺乏足够的物理理解。

Method: 该框架首先通过监督训练获取基础知识，然后通过强化学习解决模型在视觉空间理解和推理方面的不足。设计了一种基于规则的奖励函数，同时考虑了长程性能和环境中的动作约束。

Result: 在 EmbodiedBench 基准测试中，使用 Qwen2.5-VL-3B 训练的推理模型显著优于更大规模的模型 GPT-4o-mini 21.33%，并且超过了其他使用 Qwen2.5-VL-7B 训练的模型 20.33%。

Conclusion: RoboGPT-R1 框架有效地提高了具身智能体在复杂操作任务中的推理能力，并在 EmbodiedBench 基准测试中取得了显著成果。

Abstract: Improving the reasoning capabilities of embodied agents is crucial for robots
to complete complex human instructions in long-view manipulation tasks
successfully. Despite the success of large language models and vision language
models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue
facing challenges in performing long-horizon manipulation tasks in complex
real-world environments, owing to their restricted common sense and reasoning
capabilities. Considering that aligning general-purpose vision language models
to robotic planning tasks via supervised fine-tuning suffers from poor
generalization and insufficient physical understanding, we propose RoboGPT-R1,
a two-stage fine-tuning framework for embodied planning. In this framework,
supervised training acquires foundational knowledge through expert sequences,
followed by RL to address the model's shortcomings in visual-spatial
understanding and reasoning. To achieve physical understanding and action
sequence consistency in multi-step reasoning tasks, we design a rule-based
reward function that simultaneously considers long-horizon performance and
action constraint in the environment. The reasoning model, trained on
Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,
by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the
EmbodiedBench benchmark.

</details>


### [141] [Boosting Instruction Following at Scale](https://arxiv.org/abs/2510.14842)
*Ben Elder,Evelyn Duesterwald,Vinod Muthusamy*

Main category: cs.AI

TL;DR: Instruction Boosting improves LLM instruction following by iteratively refining the output based on the given instructions.


<details>
  <summary>Details</summary>
Motivation: Increasing the reliability of LLM prompt instructions is challenging because simply adding more instructions doesn't guarantee they'll be followed, and performance degrades as more instructions are added.

Method: Instruction Boosting, a post-generation method, and SCALEDIF, a benchmark with a scaled instruction volume, are introduced. A quantitative conflict scoring tool is used to analyze the impact of instruction conflicts.

Result: Instruction Boosting improves the instruction following rate by up to 7 points for two instructions and up to 4 points for ten instructions. The performance degradation with more instructions is linked to the degree of tension and conflict among them.

Conclusion: Instruction Boosting enhances LLM instruction following, and a conflict scoring tool helps developers understand the impact of prompt instructions on model performance.

Abstract: A typical approach developers follow to influence an LLM's behavior in an
application is through careful manipulation of the prompt, such as by adding or
modifying instructions. However, merely adding more instructions provides
little assurance that they will actually be followed. We introduce Instruction
Boosting as a post-generation method to increase the reliability of LLM prompt
instructions. We show that Instruction Boosting improves the instruction
following rate by up to 7 points for two instructions and up to 4 points for
ten instructions. To demonstrate these results we introduce SCALEDIF, a
benchmark with a scaled instruction volume of up to ten instructions per data
sample. We also present an analysis of the commonly observed trend that
performance degrades as more instructions are added. We show that an important
factor contributing to this trend is the degree of tension and conflict that
arises as the number of instructions is increased. We contribute a quantitative
conflict scoring tool that explains the observed performance trends and
provides feedback to developers on the impact that additional prompt
instructions have on a model's performance.

</details>


### [142] [Where to Search: Measure the Prior-Structured Search Space of LLM Agents](https://arxiv.org/abs/2510.14846)
*Zhuo-Yang Song*

Main category: cs.AI

TL;DR: 本文提出了一种紧凑的形式化理论，用于描述和测量由领域先验知识引导的、由大型语言模型辅助的迭代搜索。


<details>
  <summary>Details</summary>
Motivation: 基于大型语言模型的生成-过滤-优化（迭代范式）在AI+科学的推理、编程和程序发现方面取得了进展。然而，搜索的有效性取决于搜索的位置，即如何将领域先验知识编码到可操作的结构化假设空间中。

Method: 本文将agent表示为输入和输出上的模糊关系算子，以捕获可行的转换；从而agent受到固定安全范围的约束。为了描述多步推理/搜索，我们用一个单一的延续参数对所有可达路径进行加权，并将它们相加得到一个覆盖生成函数；这导致了可达性难度的度量；并且它提供了在由安全范围引起的图上进行搜索的几何解释。

Result: 我们进一步提供了最简单的可测试推论，并通过多数投票实例化来验证它们。

Conclusion: 该理论提供了一种可行的语言和操作工具来测量agent及其搜索空间，提出了由LLM构建的迭代搜索的系统形式化描述。

Abstract: The generate-filter-refine (iterative paradigm) based on large language
models (LLMs) has achieved progress in reasoning, programming, and program
discovery in AI+Science. However, the effectiveness of search depends on where
to search, namely, how to encode the domain prior into an operationally
structured hypothesis space. To this end, this paper proposes a compact formal
theory that describes and measures LLM-assisted iterative search guided by
domain priors. We represent an agent as a fuzzy relation operator on inputs and
outputs to capture feasible transitions; the agent is thereby constrained by a
fixed safety envelope. To describe multi-step reasoning/search, we weight all
reachable paths by a single continuation parameter and sum them to obtain a
coverage generating function; this induces a measure of reachability
difficulty; and it provides a geometric interpretation of search on the graph
induced by the safety envelope. We further provide the simplest testable
inferences and validate them via a majority-vote instantiation. This theory
offers a workable language and operational tools to measure agents and their
search spaces, proposing a systematic formal description of iterative search
constructed by LLMs.

</details>


### [143] [LabOS: The AI-XR Co-Scientist That Sees and Works With Humans](https://arxiv.org/abs/2510.14861)
*Le Cong,Zaixi Zhang,Xiaotong Wang,Yin Di,Ruofan Jin,Michal Gerasimiuk,Yinkai Wang,Ravi K. Dinesh,David Smerkous,Alex Smerkous,Xuekun Wu,Shilong Liu,Peishan Li,Yi Zhu,Simran Serrao,Ning Zhao,Imran A. Mohammad,John B. Sunwoo,Joseph C. Wu,Mengdi Wang*

Main category: cs.AI

TL;DR: LabOS: An AI co-scientist uniting computational reasoning with physical experimentation.


<details>
  <summary>Details</summary>
Motivation: To create an AI system that can assist scientists in real-time execution of experiments, moving beyond computational design to active participation in the lab.

Method: Connecting multi-modal AI agents, smart glasses, and human-AI collaboration to enable AI to perceive, understand, and assist in experiments.

Result: LabOS demonstrates AI's ability to participate in experiments, turning the lab into a collaborative environment.

Conclusion: LabOS facilitates human and machine discovery evolving together.

Abstract: Modern science advances fastest when thought meets action. LabOS represents
the first AI co-scientist that unites computational reasoning with physical
experimentation through multimodal perception, self-evolving agents, and
Entended-Reality(XR)-enabled human-AI collaboration. By connecting multi-model
AI agents, smart glasses, and human-AI collaboration, LabOS allows AI to see
what scientists see, understand experimental context, and assist in real-time
execution. Across applications--from cancer immunotherapy target discovery to
stem-cell engineering -- LabOS shows that AI can move beyond computational
design to participation, turning the laboratory into an intelligent,
collaborative environment where human and machine discovery evolve together.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [144] [Towards a Multimodal Stream Processing System](https://arxiv.org/abs/2510.14631)
*Uélison Jean Lopes dos Santos,Alessandro Ferri,Szilard Nistor,Riccardo Tommasini,Carsten Binnig,Manisha Luthra*

Main category: cs.DB

TL;DR: 本论文提出了一个将多模态大型语言模型嵌入为一等算子的新一代多模态流处理系统的愿景，以实现跨多种模态的实时查询处理。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态查询数据库无法满足流处理系统严格的延迟和吞吐量要求。

Method: 该方法提出了在逻辑、物理和语义查询转换等各个层面的优化，以减少模型负载，提高吞吐量，同时保持准确性。

Result: 通过一个原型系统，展示了该优化方法可以将性能提高一个数量级以上。

Conclusion: 讨论了一个研究路线图，概述了构建可扩展和高效的多模态流处理系统的开放研究挑战。

Abstract: In this paper, we present a vision for a new generation of multimodal
streaming systems that embed MLLMs as first-class operators, enabling real-time
query processing across multiple modalities. Achieving this is non-trivial:
while recent work has integrated MLLMs into databases for multimodal queries,
streaming systems require fundamentally different approaches due to their
strict latency and throughput requirements. Our approach proposes novel
optimizations at all levels, including logical, physical, and semantic query
transformations that reduce model load to improve throughput while preserving
accuracy. We demonstrate this with \system{}, a prototype leveraging such
optimizations to improve performance by more than an order of magnitude.
Moreover, we discuss a research roadmap that outlines open research challenges
for building a scalable and efficient multimodal stream processing systems.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [145] [FinAI Data Assistant: LLM-based Financial Database Query Processing with the OpenAI Function Calling API](https://arxiv.org/abs/2510.14162)
*Juhyeong Kim,Yejin Kim,Youngbin Lee,Hyunwoo Byun*

Main category: cs.IR

TL;DR: FinAI Data Assistant: 使用LLM和OpenAI Function Calling API进行金融数据库的自然语言查询，提高可靠性、降低延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在金融数据库查询中的应用，特别是解决text-to-SQL的局限性。

Method: 结合LLM和OpenAI Function Calling API，将用户请求路由到参数化查询库，避免生成完整SQL。

Result: LLM单独预测存在误差和前瞻性偏差，股票代码映射准确率高，FinAI Data Assistant比text-to-SQL基线更可靠、低延迟和低成本。

Conclusion: 讨论了设计权衡、局限性及部署方向。

Abstract: We present FinAI Data Assistant, a practical approach for natural-language
querying over financial databases that combines large language models (LLMs)
with the OpenAI Function Calling API. Rather than synthesizing complete SQL via
text-to-SQL, our system routes user requests to a small library of vetted,
parameterized queries, trading generative flexibility for reliability, low
latency, and cost efficiency. We empirically study three questions: (RQ1)
whether LLMs alone can reliably recall or extrapolate time-dependent financial
data without external retrieval; (RQ2) how well LLMs map company names to stock
ticker symbols; and (RQ3) whether function calling outperforms text-to-SQL for
end-to-end database query processing. Across controlled experiments on prices
and fundamentals, LLM-only predictions exhibit non-negligible error and show
look-ahead bias primarily for stock prices relative to model knowledge cutoffs.
Ticker-mapping accuracy is near-perfect for NASDAQ-100 constituents and high
for S\&P~500 firms. Finally, FinAI Data Assistant achieves lower latency and
cost and higher reliability than a text-to-SQL baseline on our task suite. We
discuss design trade-offs, limitations, and avenues for deployment.

</details>


### [146] [Large Scale Retrieval for the LinkedIn Feed using Causal Language Models](https://arxiv.org/abs/2510.14223)
*Sudarshan Srinivasa Ramanujam,Antonio Alonso,Saurabh Kataria,Siddharth Dangi,Akhilesh Gupta,Birjodh Singh Tiwana,Manas Somaiya,Luke Simon,David Byrne,Sojeong Ha,Sen Zhou,Andrei Akterskii,Zhanglong Liu,Samira Sriram,Crescent Xiong,Zhoutao Pei,Angela Shao,Alex Li,Annie Xiao,Caitlin Kolb,Thomas Kistler,Zach Moore,Hamed Firooz*

Main category: cs.IR

TL;DR: 本文提出了一种新颖的检索方法，该方法微调大型因果语言模型（Meta 的 LLaMA 3）作为双编码器，以生成用户和内容的高质量嵌入，仅使用文本输入。


<details>
  <summary>Details</summary>
Motivation: 在像 LinkedIn Feed 这样的大规模推荐系统中，检索阶段对于将数亿潜在候选者缩小到可管理的子集以进行排序至关重要。LinkedIn 的 Feed 提供来自会员网络外部的建议内容（基于会员的主题兴趣），其中从数亿候选者池中检索 2000 个候选者，延迟预算为几毫秒，入站 QPS 为每秒数千个。

Method: 使用大型因果语言模型 (Meta 的 LLaMA 3) 作为双编码器，仅使用文本输入为用户（成员）和内容（项目）生成高质量嵌入；包括用于嵌入生成的提示设计、在 LinkedIn 规模上进行微调的技术以及用于低延迟、经济高效的在线服务的 инфраструктура。

Result: 使用离线指标和在线 A/B 测试评估了该系统，结果表明会员参与度显着提高。我们观察到新会员的显着收益，他们通常缺乏强大的网络连接，这表明高质量的建议内容有助于保留。

Conclusion: 这项工作证明了生成语言模型如何有效地适应工业应用中的实时、高吞吐量检索。

Abstract: In large scale recommendation systems like the LinkedIn Feed, the retrieval
stage is critical for narrowing hundreds of millions of potential candidates to
a manageable subset for ranking. LinkedIn's Feed serves suggested content from
outside of the member's network (based on the member's topical interests),
where 2000 candidates are retrieved from a pool of hundreds of millions
candidate with a latency budget of a few milliseconds and inbound QPS of
several thousand per second. This paper presents a novel retrieval approach
that fine-tunes a large causal language model (Meta's LLaMA 3) as a dual
encoder to generate high quality embeddings for both users (members) and
content (items), using only textual input. We describe the end to end pipeline,
including prompt design for embedding generation, techniques for fine-tuning at
LinkedIn's scale, and infrastructure for low latency, cost effective online
serving. We share our findings on how quantizing numerical features in the
prompt enables the information to get properly encoded in the embedding,
facilitating greater alignment between the retrieval and ranking layer. The
system was evaluated using offline metrics and an online A/B test, which showed
substantial improvements in member engagement. We observed significant gains
among newer members, who often lack strong network connections, indicating that
high-quality suggested content aids retention. This work demonstrates how
generative language models can be effectively adapted for real time, high
throughput retrieval in industrial applications.

</details>


### [147] [Synergistic Integration and Discrepancy Resolution of Contextualized Knowledge for Personalized Recommendation](https://arxiv.org/abs/2510.14257)
*Lingyu Mu,Hao Deng,Haibo Xing,Kaican Lin,Zhitong Zhu,Yu Zhang,Xiaoyi Zeng,Zhengxiao Liu,Zheng Lin,Jinxin Hu*

Main category: cs.IR

TL;DR: CoCo: An end-to-end framework dynamically constructs user-specific contextual knowledge embeddings through a dual-mechanism approach.


<details>
  <summary>Details</summary>
Motivation: Current methodologies that adopt static schema-based prompting mechanisms encounter significant limitations: (1) they employ universal template structures that neglect the multi-faceted nature of user preference diversity; (2) they implement superficial alignment between semantic knowledge representations and behavioral feature spaces without achieving comprehensive latent space integration.

Method: CoCo, an end-to-end framework that dynamically constructs user-specific contextual knowledge embeddings through a dual-mechanism approach. Our method realizes profound integration of semantic and behavioral latent dimensions via adaptive knowledge fusion and contradiction resolution modules.

Result: CoCo achieves a maximum 8.58% improvement over seven cutting-edge methods in recommendation accuracy. The framework's deployment on a production advertising system resulted in a 1.91% sales growth

Conclusion: CoCo provides a versatile solution for next-generation recommendation systems requiring both knowledge-enhanced reasoning and personalized adaptation.

Abstract: The integration of large language models (LLMs) into recommendation systems
has revealed promising potential through their capacity to extract world
knowledge for enhanced reasoning capabilities. However, current methodologies
that adopt static schema-based prompting mechanisms encounter significant
limitations: (1) they employ universal template structures that neglect the
multi-faceted nature of user preference diversity; (2) they implement
superficial alignment between semantic knowledge representations and behavioral
feature spaces without achieving comprehensive latent space integration. To
address these challenges, we introduce CoCo, an end-to-end framework that
dynamically constructs user-specific contextual knowledge embeddings through a
dual-mechanism approach. Our method realizes profound integration of semantic
and behavioral latent dimensions via adaptive knowledge fusion and
contradiction resolution modules. Experimental evaluations across diverse
benchmark datasets and an enterprise-level e-commerce platform demonstrate
CoCo's superiority, achieving a maximum 8.58% improvement over seven
cutting-edge methods in recommendation accuracy. The framework's deployment on
a production advertising system resulted in a 1.91% sales growth, validating
its practical effectiveness. With its modular design and model-agnostic
architecture, CoCo provides a versatile solution for next-generation
recommendation systems requiring both knowledge-enhanced reasoning and
personalized adaptation.

</details>


### [148] [Large Reasoning Embedding Models: Towards Next-Generation Dense Retrieval Paradigm](https://arxiv.org/abs/2510.14321)
*Jianting Tang,Dongshuai Li,Tao Wen,Fuyu Lv,Dan Ou,Linli Xu*

Main category: cs.IR

TL;DR: 提出了一种名为LREM的新型检索模型，该模型通过将推理过程整合到表征学习中，有效地弥合了原始查询和目标项目之间的语义差距，从而显著提高了检索准确率。


<details>
  <summary>Details</summary>
Motivation: 现有模型采用直接嵌入方法，嵌入的语义准确性仍然不足，并且倾向于捕获训练数据中的统计共现模式，从而偏向于浅层的词汇和语义匹配。对于与目标项目存在显著词汇差异的困难查询，性能会显著下降。

Method: 提出了一种大型推理嵌入模型 (LREM)，该模型将推理过程整合到表征学习中。LREM 首先进行推理以实现对原始查询的深入理解，然后生成用于检索的推理增强查询嵌入。

Result: LREM 经过广泛的离线和在线实验验证了其有效性，并于 2025 年 8 月开始在中国最大的电子商务平台上部署。

Conclusion: LREM 模型通过推理过程有效地弥合了原始查询和目标项目之间的语义差距，显著提高了检索准确率。

Abstract: In modern e-commerce search systems, dense retrieval has become an
indispensable component. By computing similarities between query and item
(product) embeddings, it efficiently selects candidate products from
large-scale repositories. With the breakthroughs in large language models
(LLMs), mainstream embedding models have gradually shifted from BERT to LLMs
for more accurate text modeling. However, these models still adopt
direct-embedding methods, and the semantic accuracy of embeddings remains
inadequate. Therefore, contrastive learning is heavily employed to achieve
tight semantic alignment between positive pairs. Consequently, such models tend
to capture statistical co-occurrence patterns in the training data, biasing
them toward shallow lexical and semantic matches. For difficult queries
exhibiting notable lexical disparity from target items, the performance
degrades significantly. In this work, we propose the Large Reasoning Embedding
Model (LREM), which novelly integrates reasoning processes into representation
learning. For difficult queries, LREM first conducts reasoning to achieve a
deep understanding of the original query, and then produces a
reasoning-augmented query embedding for retrieval. This reasoning process
effectively bridges the semantic gap between original queries and target items,
significantly improving retrieval accuracy. Specifically, we adopt a two-stage
training process: the first stage optimizes the LLM on carefully curated
Query-CoT-Item triplets with SFT and InfoNCE losses to establish preliminary
reasoning and embedding capabilities, and the second stage further refines the
reasoning trajectories via reinforcement learning (RL). Extensive offline and
online experiments validate the effectiveness of LREM, leading to its
deployment on China's largest e-commerce platform since August 2025.

</details>


### [149] [Ensembling Multiple Hallucination Detectors Trained on VLLM Internal Representations](https://arxiv.org/abs/2510.14330)
*Yuto Nakamizo,Ryuhei Miyazato,Hikaru Tanabe,Ryuta Yamakura,Kiori Hatanaka*

Main category: cs.IR

TL;DR: y3h2 团队在 KDD Cup 2025 Meta CRAG-MM 挑战赛中获得第五名，该比赛使用 LLM 评估器，专注于图像事实性问答。该团队主要通过降低 VLM 内部表示的幻觉来提高准确率。


<details>
  <summary>Details</summary>
Motivation: 比赛以 VQA 准确率进行评判，错误答案会导致负分，因此需要减少 VLM 的幻觉。

Method: 使用 logistic 回归训练幻觉检测模型，利用隐藏状态和注意力头的输出，并采用模型集成。

Result: 牺牲了一些正确答案，但显著减少了幻觉。

Conclusion: 该方法减少了幻觉，最终在排行榜上名列前茅。

Abstract: This paper presents the 5th place solution by our team, y3h2, for the Meta
CRAG-MM Challenge at KDD Cup 2025. The CRAG-MM benchmark is a visual question
answering (VQA) dataset focused on factual questions about images, including
egocentric images. The competition was contested based on VQA accuracy, as
judged by an LLM-based automatic evaluator. Since incorrect answers result in
negative scores, our strategy focused on reducing hallucinations from the
internal representations of the VLM. Specifically, we trained logistic
regression-based hallucination detection models using both the hidden_state and
the outputs of specific attention heads. We then employed an ensemble of these
models. As a result, while our method sacrificed some correct answers, it
significantly reduced hallucinations and allowed us to place among the top
entries on the final leaderboard. For implementation details and code, please
refer to
https://gitlab.aicrowd.com/htanabe/meta-comprehensive-rag-benchmark-starter-kit.

</details>


### [150] [GemiRec: Interest Quantization and Generation for Multi-Interest Recommendation](https://arxiv.org/abs/2510.14626)
*Zhibo Wu,Yunfan Wu,Quan Liu,Lin Jiang,Ping Yang,Yao Hu*

Main category: cs.IR

TL;DR: GemiRec是一个用于多兴趣推荐的框架，旨在解决兴趣崩溃和兴趣演化建模不足的问题。它通过兴趣量化和兴趣生成来实现结构化的兴趣分离和用户兴趣动态的学习。


<details>
  <summary>Details</summary>
Motivation: 现有的多兴趣推荐方法存在兴趣崩溃和兴趣演化建模不足的局限性。

Method: 提出了一个名为GemiRec的框架，它包含兴趣词典维护模块（IDMM）、多兴趣后验分布模块（MIPDM）和多兴趣检索模块（MIRM）。

Result: 理论和实验分析表明GemiRec具有优势和有效性。

Conclusion: GemiRec已于2025年3月部署在生产环境中，显示了其在工业应用中的实际价值。

Abstract: Multi-interest recommendation has gained attention, especially in industrial
retrieval stage. Unlike classical dual-tower methods, it generates multiple
user representations instead of a single one to model comprehensive user
interests. However, prior studies have identified two underlying limitations:
the first is interest collapse, where multiple representations homogenize. The
second is insufficient modeling of interest evolution, as they struggle to
capture latent interests absent from a user's historical behavior. We begin
with a thorough review of existing works in tackling these limitations. Then,
we attempt to tackle these limitations from a new perspective. Specifically, we
propose a framework-level refinement for multi-interest recommendation, named
GemiRec. The proposed framework leverages interest quantization to enforce a
structural interest separation and interest generation to learn the evolving
dynamics of user interests explicitly. It comprises three modules: (a) Interest
Dictionary Maintenance Module (IDMM) maintains a shared quantized interest
dictionary. (b) Multi-Interest Posterior Distribution Module (MIPDM) employs a
generative model to capture the distribution of user future interests. (c)
Multi-Interest Retrieval Module (MIRM) retrieves items using multiple
user-interest representations. Both theoretical and empirical analyses, as well
as extensive experiments, demonstrate its advantages and effectiveness.
Moreover, it has been deployed in production since March 2025, showing its
practical value in industrial applications.

</details>


### [151] [MR.Rec: Synergizing Memory and Reasoning for Personalized Recommendation Assistant with LLMs](https://arxiv.org/abs/2510.14629)
*Jiani Huang,Xingchen Zou,Lianghao Xia,Qing Li*

Main category: cs.IR

TL;DR: MR.Rec: A novel framework that synergizes memory and reasoning for LLM-based recommendations.


<details>
  <summary>Details</summary>
Motivation: Current methods are often constrained by limited context windows and single-turn reasoning, hindering their ability to capture dynamic user preferences and proactively reason over recommendation contexts.

Method: Develop a comprehensive Retrieval-Augmented Generation (RAG) system that efficiently indexes and retrieves relevant external memory to enhance LLM personalization capabilities. Integrate reasoning enhanced memory retrieval. Design a reinforcement learning framework that trains the LLM to autonomously learn effective strategies for both memory utilization and reasoning refinement.

Result: MR.Rec significantly outperforms state-of-the-art baselines across multiple metrics.

Conclusion: MR.Rec ensures more accurate, context-aware, and highly personalized recommendations.

Abstract: The application of Large Language Models (LLMs) in recommender systems faces
key challenges in delivering deep personalization and intelligent reasoning,
especially for interactive scenarios. Current methods are often constrained by
limited context windows and single-turn reasoning, hindering their ability to
capture dynamic user preferences and proactively reason over recommendation
contexts. To address these limitations, we propose MR.Rec, a novel framework
that synergizes memory and reasoning for LLM-based recommendations. To achieve
personalization, we develop a comprehensive Retrieval-Augmented Generation
(RAG) system that efficiently indexes and retrieves relevant external memory to
enhance LLM personalization capabilities. Furthermore, to enable the synergy
between memory and reasoning, our RAG system goes beyond conventional
query-based retrieval by integrating reasoning enhanced memory retrieval.
Finally, we design a reinforcement learning framework that trains the LLM to
autonomously learn effective strategies for both memory utilization and
reasoning refinement. By combining dynamic memory retrieval with adaptive
reasoning, this approach ensures more accurate, context-aware, and highly
personalized recommendations. Extensive experiments demonstrate that MR.Rec
significantly outperforms state-of-the-art baselines across multiple metrics,
validating its efficacy in delivering intelligent and personalized
recommendations. We will release code and data upon paper notification.

</details>


### [152] [Causality Enhancement for Cross-Domain Recommendation](https://arxiv.org/abs/2510.14641)
*Zhibo Wu,Yunfan Wu,Lin Jiang,Ping Yang,Yao Hu*

Main category: cs.IR

TL;DR: 提出了一种名为 CE-CDR 的因果增强跨域推荐框架，以解决现有方法中存在的不足。


<details>
  <summary>Details</summary>
Motivation: 现有跨域推荐方法在整合不一致的源域任务或未考虑潜在因果关系的源域特征时存在不足，可能导致负迁移。

Method: 1. 将跨域推荐问题重新定义为因果图；2. 启发式地构建因果关系数据集；3. 推导出理论上无偏的 Partial Label Causal Loss，以泛化到未见的跨域模式。

Result: 理论和实证分析以及大量实验表明了 CE-CDR 的合理性和有效性，以及其作为模型无关插件的通用适用性。自 2025 年 4 月以来已在生产中部署。

Conclusion: CE-CDR 框架具有实际应用价值。

Abstract: Cross-domain recommendation forms a crucial component in recommendation
systems. It leverages auxiliary information through source domain tasks or
features to enhance target domain recommendations. However, incorporating
inconsistent source domain tasks may result in insufficient cross-domain
modeling or negative transfer. While incorporating source domain features
without considering the underlying causal relationships may limit their
contribution to final predictions. Thus, a natural idea is to directly train a
cross-domain representation on a causality-labeled dataset from the source to
target domain. Yet this direction has been rarely explored, as identifying
unbiased real causal labels is highly challenging in real-world scenarios. In
this work, we attempt to take a first step in this direction by proposing a
causality-enhanced framework, named CE-CDR. Specifically, we first reformulate
the cross-domain recommendation as a causal graph for principled guidance. We
then construct a causality-aware dataset heuristically. Subsequently, we derive
a theoretically unbiased Partial Label Causal Loss to generalize beyond the
biased causality-aware dataset to unseen cross-domain patterns, yielding an
enriched cross-domain representation, which is then fed into the target model
to enhance target-domain recommendations. Theoretical and empirical analyses,
as well as extensive experiments, demonstrate the rationality and effectiveness
of CE-CDR and its general applicability as a model-agnostic plugin. Moreover,
it has been deployed in production since April 2025, showing its practical
value in real-world applications.

</details>


### [153] [Dataset Pruning in RecSys and ML: Best Practice or Mal-Practice?](https://arxiv.org/abs/2510.14704)
*Leonie Winter*

Main category: cs.IR

TL;DR: 本研究探讨了数据修剪对推荐系统数据集和算法性能的影响，发现常用的核心修剪方法可能会高度选择性地保留少量用户，并且在修剪后的数据上训练和测试算法会获得较高评分，但在未修剪的测试集上评估时，这种优势会消失。


<details>
  <summary>Details</summary>
Motivation: 研究推荐系统中的离线评估严重依赖数据集，而许多数据集都经过修剪。本研究旨在检验数据修剪对数据集特征和算法性能的影响。

Method: 分析了五个基准数据集在未修剪和五个连续修剪级别（5, 10, 20, 50, 100）下的情况。对于每个coreset，我们检查了结构和分布特征，并训练和测试了11个代表性算法。为了进一步评估修剪后的数据集是否导致人为夸大的性能结果，我们还评估了在修剪后的训练集上训练但在未修剪的数据上测试的模型。

Result: 结果表明，常用的核心修剪方法可能会高度选择性地保留原始数据集中的少量用户。在修剪后的数据上训练和测试传统算法会获得更高的nDCG@10分数；但是，在未修剪的测试集上评估时，这种优势在很大程度上消失了。当在未修剪的数据上进行测试时，所有算法的性能均随着修剪水平的提高而下降，这突显了数据集缩减对推荐算法性能的影响。

Conclusion: 数据修剪对推荐算法的性能有显著影响，尤其是在未修剪的数据集上进行测试时。

Abstract: Offline evaluations in recommender system research depend heavily on
datasets, many of which are pruned, such as the widely used MovieLens
collections. This thesis examines the impact of data pruning - specifically,
removing users with fewer than a specified number of interactions - on both
dataset characteristics and algorithm performance. Five benchmark datasets were
analysed in both their unpruned form and at five successive pruning levels (5,
10, 20, 50, 100). For each coreset, we examined structural and distributional
characteristics and trained and tested eleven representative algorithms. To
further assess if pruned datasets lead to artificially inflated performance
results, we also evaluated models trained on the pruned train sets but tested
on unpruned data. Results show that commonly applied core pruning can be highly
selective, leaving as little as 2% of the original users in some datasets.
Traditional algorithms achieved higher nDCG@10 scores when both training and
testing on pruned data; however, this advantage largely disappeared when
evaluated on unpruned test sets. Across all algorithms, performance declined
with increasing pruning levels when tested on unpruned data, highlighting the
impact of dataset reduction on the performance of recommender algorithms.

</details>


### [154] [Cross-Scenario Unified Modeling of User Interests at Billion Scale](https://arxiv.org/abs/2510.14788)
*Manjie Xu,Cheng Chen,Xin Jia,Jingyi Zhou,Yongji Wu,Zejian Wang,Chi Zhang,Kai Zuo,Yibo Chen,Xu Tang,Yao Hu,Yixin Zhu*

Main category: cs.IR

TL;DR: RED-Rec: An LLM-enhanced hierarchical recommender engine for diversified scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional recommendation systems neglect cross-scenario behavioral signals and struggle to integrate LLMs at billion-scale deployments, limiting their ability to capture holistic user interests.

Method: A two-tower LLM-powered framework enables nuanced representations, and a scenario-aware dense mixing and querying policy fuses diverse behavioral signals.

Result: Online A/B testing shows substantial performance gains in content recommendation and advertisement targeting tasks.

Conclusion: RED-Rec advances unified user modeling, unlocking deeper personalization and fostering more meaningful user engagement.

Abstract: User interests on content platforms are inherently diverse, manifesting
through complex behavioral patterns across heterogeneous scenarios such as
search, feed browsing, and content discovery. Traditional recommendation
systems typically prioritize business metric optimization within isolated
specific scenarios, neglecting cross-scenario behavioral signals and struggling
to integrate advanced techniques like LLMs at billion-scale deployments, which
finally limits their ability to capture holistic user interests across platform
touchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender
Engine for Diversified scenarios, tailored for industry-level content
recommendation systems. RED-Rec unifies user interest representations across
multiple behavioral contexts by aggregating and synthesizing actions from
varied scenarios, resulting in comprehensive item and user modeling. At its
core, a two-tower LLM-powered framework enables nuanced, multifaceted
representations with deployment efficiency, and a scenario-aware dense mixing
and querying policy effectively fuses diverse behavioral signals to capture
cross-scenario user intent patterns and express fine-grained, context-specific
intents during serving. We validate RED-Rec through online A/B testing on
hundreds of millions of users in RedNote through online A/B testing, showing
substantial performance gains in both content recommendation and advertisement
targeting tasks. We further introduce a million-scale sequential recommendation
dataset, RED-MMU, for comprehensive offline training and evaluation. Our work
advances unified user modeling, unlocking deeper personalization and fostering
more meaningful user engagement in large-scale UGC platforms.

</details>


### [155] [A Simulation Framework for Studying Systemic Effects of Feedback Loops in Recommender Systems](https://arxiv.org/abs/2510.14857)
*Gabriele Barlacchi,Margherita Lalli,Emanuele Ferragina,Fosca Giannotti,Luca Pappalardo*

Main category: cs.IR

TL;DR: 本文介绍了一个模拟框架，用于模拟在线零售环境中推荐系统的反馈回路，其中推荐系统会定期根据不断变化的用户-商品交互进行重新训练。


<details>
  <summary>Details</summary>
Motivation: 研究推荐系统如何影响多样性、购买集中度和用户同质化。

Method: 使用Amazon e-Commerce数据集，分析不同的推荐算法。

Result: 反馈回路增加了个人多样性，但同时降低了集体多样性，并将需求集中在少数受欢迎的商品上。对于某些推荐系统，反馈回路会随着时间的推移增加用户同质化，使用户购买偏好越来越相似。

Conclusion: 强调需要平衡个性化和长期多样性的推荐系统设计。

Abstract: Recommender systems continuously interact with users, creating feedback loops
that shape both individual behavior and collective market dynamics. This paper
introduces a simulation framework to model these loops in online retail
environments, where recommenders are periodically retrained on evolving
user-item interactions. Using the Amazon e-Commerce dataset, we analyze how
different recommendation algorithms influence diversity, purchase
concentration, and user homogenization over time. Results reveal a systematic
trade-off: while the feedback loop increases individual diversity, it
simultaneously reduces collective diversity and concentrates demand on a few
popular items. Moreover, for some recommender systems, the feedback loop
increases user homogenization over time, making user purchase profiles
increasingly similar. These findings underscore the need for recommender
designs that balance personalization with long-term diversity.

</details>


### [156] [Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report](https://arxiv.org/abs/2510.14880)
*Rikiya Takehi,Benjamin Clavié,Sean Lee,Aamir Shakir*

Main category: cs.IR

TL;DR: 提出了mxbai-edge-colbert-v0模型，参数量分别为17M和32M。


<details>
  <summary>Details</summary>
Motivation: 旨在支持各种规模的检索，从云端大规模检索到可以在任何设备上本地运行的模型。

Method: 通过大量实验改进检索和晚期交互模型，并将其提炼成更小的模型。

Result: mxbai-edge-colbert-v0在常见短文本基准测试（BEIR）上优于ColBERTv2，并在长上下文任务中表现出前所未有的效率。

Conclusion: mxbai-edge-colbert-v0是一个有能力的小型模型，可以作为未来实验的坚实基础。

Abstract: In this work, we introduce mxbai-edge-colbert-v0 models, at two different
parameter counts: 17M and 32M. As part of our research, we conduct numerous
experiments to improve retrieval and late-interaction models, which we intend
to distill into smaller models as proof-of-concepts. Our ultimate aim is to
support retrieval at all scales, from large-scale retrieval which lives in the
cloud to models that can run locally, on any device. mxbai-edge-colbert-v0 is a
model that we hope will serve as a solid foundation backbone for all future
experiments, representing the first version of a long series of small
proof-of-concepts. As part of the development of mxbai-edge-colbert-v0, we
conducted multiple ablation studies, of which we report the results. In terms
of downstream performance, mxbai-edge-colbert-v0 is a particularly capable
small model, outperforming ColBERTv2 on common short-text benchmarks (BEIR) and
representing a large step forward in long-context tasks, with unprecedented
efficiency.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [157] [Large Language Models for Real-World IoT Device Identification](https://arxiv.org/abs/2510.13817)
*Rameen Mahmood,Tousif Ahmed,Sai Teja Peddinti,Danny Yuxing Huang*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的物联网设备识别方法，该方法将设备识别重新定义为异构网络元数据上的语言建模任务，并使用指令调整的LLaMA3.18B模型实现了高精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前物联网设备快速扩张，传统识别方法已无法满足需求，给安全、隐私和网络责任带来风险。开放世界环境中的流量元数据不完整、嘈杂或被故意混淆，进一步加剧了这些挑战。

Method: 该研究构建了一个语义推理管道，使用大型语言模型集合为IoT Inspector数据集生成高质量的供应商标签，并通过课程学习指令调整量化的LLaMA3.18B模型。

Result: 该模型在2015家供应商中实现了98.25%的top-1准确率和90.73%的macro准确率，并且对缺失字段、协议漂移和对抗性操作具有弹性。

Conclusion: 指令调整的LLM为大规模的实际设备识别提供了可扩展和可解释的基础。

Abstract: The rapid expansion of IoT devices has outpaced current identification
methods, creating significant risks for security, privacy, and network
accountability. These challenges are heightened in open-world environments,
where traffic metadata is often incomplete, noisy, or intentionally obfuscated.
We introduce a semantic inference pipeline that reframes device identification
as a language modeling task over heterogeneous network metadata. To construct
reliable supervision, we generate high-fidelity vendor labels for the IoT
Inspector dataset, the largest real-world IoT traffic corpus, using an ensemble
of large language models guided by mutual-information and entropy-based
stability scores. We then instruction-tune a quantized LLaMA3.18B model with
curriculum learning to support generalization under sparsity and long-tail
vendor distributions. Our model achieves 98.25% top-1 accuracy and 90.73% macro
accuracy across 2,015 vendors while maintaining resilience to missing fields,
protocol drift, and adversarial manipulation. Evaluation on an independent IoT
testbed, coupled with explanation quality and adversarial stress tests,
demonstrates that instruction-tuned LLMs provide a scalable and interpretable
foundation for real-world device identification at scale.

</details>


### [158] [Self-Training with Dynamic Weighting for Robust Gradual Domain Adaptation](https://arxiv.org/abs/2510.13864)
*Zixi Wang,Yushe Cao,Yubo Huang,Jinzhu Wei,Jingzehua Xu,Shuai Zhang,Xin Lai*

Main category: cs.LG

TL;DR: 提出了一种新的自训练方法，称为动态加权自训练（STDW），旨在增强渐进领域自适应（GDA）的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的GDA方法通过中间域和自训练来减轻领域转移，但常常遭受低效的知识迁移或不完整的中间数据的影响。

Method: 引入了一种动态加权机制，自适应地平衡源域和目标域在训练过程中的损失贡献。具体来说，设计了一个由时变超参数$\\varrho$（从0到1）控制的优化框架，该参数控制特定领域学习的强度并确保稳定的自适应。该方法利用自训练来生成伪标签，并优化加权目标函数以进行迭代模型更新，从而在中间域中保持鲁棒性。

Result: 在旋转MNIST、颜色偏移MNIST、人像数据集和Cover Type数据集上的实验表明，STDW优于现有的基线方法。消融研究进一步验证了$\\varrho$的动态调度在实现渐进自适应中的关键作用，证实了其在减少领域偏差和提高泛化能力方面的有效性。

Conclusion: 这项工作为鲁棒的渐进领域自适应提供了理论见解和实践框架，在动态现实场景中具有潜在的应用。

Abstract: In this paper, we propose a new method called Self-Training with Dynamic
Weighting (STDW), which aims to enhance robustness in Gradual Domain Adaptation
(GDA) by addressing the challenge of smooth knowledge migration from the source
to the target domain. Traditional GDA methods mitigate domain shift through
intermediate domains and self-training but often suffer from inefficient
knowledge migration or incomplete intermediate data. Our approach introduces a
dynamic weighting mechanism that adaptively balances the loss contributions of
the source and target domains during training. Specifically, we design an
optimization framework governed by a time-varying hyperparameter $\varrho$
(progressing from 0 to 1), which controls the strength of domain-specific
learning and ensures stable adaptation. The method leverages self-training to
generate pseudo-labels and optimizes a weighted objective function for
iterative model updates, maintaining robustness across intermediate domains.
Experiments on rotated MNIST, color-shifted MNIST, portrait datasets, and the
Cover Type dataset demonstrate that STDW outperforms existing baselines.
Ablation studies further validate the critical role of $\varrho$'s dynamic
scheduling in achieving progressive adaptation, confirming its effectiveness in
reducing domain bias and improving generalization. This work provides both
theoretical insights and a practical framework for robust gradual domain
adaptation, with potential applications in dynamic real-world scenarios. The
code is available at https://github.com/Dramwig/STDW.

</details>


### [159] [Deep Edge Filter: Return of the Human-Crafted Layer in Deep Learning](https://arxiv.org/abs/2510.13865)
*Dongkwan Lee,Junhoo Lee,Nojun Kwak*

Main category: cs.LG

TL;DR: 提出了一个名为深度边缘滤波器的新方法，该方法将高通滤波应用于深度神经网络特征，以提高模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 我们的方法是基于我们的假设，即神经网络在高频分量中编码任务相关的语义信息，同时将特定领域的偏差存储在深度特征的低频分量中。

Method: 通过从原始特征中减去低通滤波后的输出，我们的方法隔离了可泛化的表示，同时保留了架构的完整性。

Result: 在视觉、文本、3D和音频等不同领域进行实验的结果表明，无论模型架构和数据模态如何，性能都能持续提高。

Conclusion: 分析表明，我们的方法可以诱导特征稀疏化并有效地隔离高频分量，从而为我们的核心假设提供经验验证。

Abstract: We introduce the Deep Edge Filter, a novel approach that applies high-pass
filtering to deep neural network features to improve model generalizability.
Our method is motivated by our hypothesis that neural networks encode
task-relevant semantic information in high-frequency components while storing
domain-specific biases in low-frequency components of deep features. By
subtracting low-pass filtered outputs from original features, our approach
isolates generalizable representations while preserving architectural
integrity. Experimental results across diverse domains such as Vision, Text,
3D, and Audio demonstrate consistent performance improvements regardless of
model architecture and data modality. Analysis reveals that our method induces
feature sparsification and effectively isolates high-frequency components,
providing empirical validation of our core hypothesis. The code is available at
https://github.com/dongkwani/DeepEdgeFilter.

</details>


### [160] [CoLoR-GAN: Continual Few-Shot Learning with Low-Rank Adaptation in Generative Adversarial Networks](https://arxiv.org/abs/2510.13869)
*Munsif Ali,Leonardo Rossi,Massimo Bertozzi*

Main category: cs.LG

TL;DR: 提出了一种名为CoLoR-GAN的框架，用于处理GAN中的few-shot和持续学习，利用低秩张量来有效调整模型以适应目标任务，同时减少所需的参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有的SOTA方法（如LFS-GAN）在每次训练迭代中引入了大量新的权重，当考虑长期时，这将变得非常重要。因此，本文致力于解决在GAN的持续学习中，从少量样本中学习而不会发生灾难性遗忘的问题。

Method: 引入了一种LoRA in LoRA (LLoRA)技术用于卷积层，并对LoRA的超参数选择进行了实证研究。

Result: 通过在多个基准CL和FS任务上的实验证明了CoLoR-GAN的有效性，表明该模型是高效的，达到了SOTA性能，但资源数量大大减少。

Conclusion: CoLoR-GAN在持续学习和few-shot学习任务中表现出色，同时显著降低了资源消耗。

Abstract: Continual learning (CL) in the context of Generative Adversarial Networks
(GANs) remains a challenging problem, particularly when it comes to learn from
a few-shot (FS) samples without catastrophic forgetting. Current most effective
state-of-the-art (SOTA) methods, like LFS-GAN, introduce a non-negligible
quantity of new weights at each training iteration, which would become
significant when considering the long term. For this reason, this paper
introduces \textcolor{red}{\textbf{\underline{c}}}ontinual
few-sh\textcolor{red}{\textbf{\underline{o}}}t learning with
\textcolor{red}{\textbf{\underline{lo}}}w-\textcolor{red}{\textbf{\underline{r}}}ank
adaptation in GANs named CoLoR-GAN, a framework designed to handle both FS and
CL together, leveraging low-rank tensors to efficiently adapt the model to
target tasks while reducing even more the number of parameters required.
Applying a vanilla LoRA implementation already permitted us to obtain pretty
good results. In order to optimize even further the size of the adapters, we
challenged LoRA limits introducing a LoRA in LoRA (LLoRA) technique for
convolutional layers. Finally, aware of the criticality linked to the choice of
the hyperparameters of LoRA, we provide an empirical study to easily find the
best ones. We demonstrate the effectiveness of CoLoR-GAN through experiments on
several benchmark CL and FS tasks and show that our model is efficient,
reaching SOTA performance but with a number of resources enormously reduced.
Source code is available on
\href{https://github.com/munsifali11/CoLoR-GAN}{Github.

</details>


### [161] [Joint Discriminative-Generative Modeling via Dual Adversarial Training](https://arxiv.org/abs/2510.13872)
*Xuwang Yin,Claire Zhang,Julie Steele,Nir Shavit,Tony T. Wang*

Main category: cs.LG

TL;DR: 本研究提出了一种新颖的训练框架，该框架集成了对抗训练 (AT) 原理，以实现判别鲁棒性和稳定的生成学习。


<details>
  <summary>Details</summary>
Motivation: 在单个框架内同时实现稳健的分类和高保真生成建模提出了巨大的挑战。混合方法（例如联合能量模型 (JEM)）将分类器解释为 EBM，但通常受到基于 SGLD 的训练中固有的不稳定性和较差的样本质量的限制。

Method: 该方法引入了三个关键创新：(1) 用稳定的、基于 AT 的方法取代基于 SGLD 的 JEM 学习，该方法通过使用 BCE 损失区分真实数据和 PGD 生成的对比样本来优化能量函数；(2) 用于判别成分的协同对抗训练，增强了分类鲁棒性，同时消除了对显式梯度惩罚的需要；(3) 解决批量归一化和 EBM 训练之间不兼容的两阶段训练程序。

Result: 在 CIFAR-10、CIFAR-100 和 ImageNet 上的实验表明，我们的方法在现有混合模型的基础上大大提高了对抗鲁棒性，同时保持了竞争性的生成性能。在 ImageNet 上，当针对生成建模进行优化时，我们模型的生成保真度超过了 BigGAN，并且接近扩散模型，这代表了第一个基于 MCMC 的 EBM 方法，可以在复杂、高分辨率的数据集上实现高质量的生成。

Conclusion: 我们的方法解决了限制 JEM 扩展的关键稳定性问题，并表明对抗训练可以作为统一框架的有效基础，该框架能够生成和稳健地分类视觉数据。

Abstract: Simultaneously achieving robust classification and high-fidelity generative
modeling within a single framework presents a significant challenge. Hybrid
approaches, such as Joint Energy-Based Models (JEM), interpret classifiers as
EBMs but are often limited by the instability and poor sample quality inherent
in SGLD-based training. We address these limitations by proposing a novel
training framework that integrates adversarial training (AT) principles for
both discriminative robustness and stable generative learning. The proposed
method introduces three key innovations: (1) the replacement of SGLD-based JEM
learning with a stable, AT-based approach that optimizes the energy function by
discriminating between real data and PGD-generated contrastive samples using
the BCE loss; (2) synergistic adversarial training for the discriminative
component that enhances classification robustness while eliminating the need
for explicit gradient penalties; and (3) a two-stage training procedure to
resolve the incompatibility between batch normalization and EBM training.
Experiments on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our method
substantially improves adversarial robustness over existing hybrid models while
maintaining competitive generative performance. On ImageNet, when optimized for
generative modeling, our model's generative fidelity surpasses that of BigGAN
and approaches diffusion models, representing the first MCMC-based EBM approach
to achieve high-quality generation on complex, high-resolution datasets. Our
approach addresses key stability issues that have limited JEM scaling and
demonstrates that adversarial training can serve as an effective foundation for
unified frameworks capable of generating and robustly classifying visual data.

</details>


### [162] [K-frames: Scene-Driven Any-k Keyframe Selection for long video understanding](https://arxiv.org/abs/2510.13891)
*Yifeng Yao,Yike Yun,Jing Wang,Huishuai Zhang,Dongyan Zhao,Ke Tian,Zhihao Wang,Minghui Qiu,Tao Wang*

Main category: cs.LG

TL;DR: K-frames 提出了一种新的场景驱动的关键帧选择范例，可以保留时间连续性，并支持任意数量的关键帧选择。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大型语言模型在长视频理解方面受到上下文窗口和计算成本的限制，并且均匀帧采样会导致大量信息丢失。现有的关键帧选择方法通常产生稀疏且时间上不连续的帧，忽略了场景的连续性，并且缺乏多尺度帧选择的灵活性。

Method: K-frames 通过预测语义连贯的、与查询相关的片段来实现关键帧选择，并使用一个三阶段渐进课程学习 clip2frame 选择，包括两个用于时间定位和关键片段感知的监督微调阶段，以及一个直接优化场景驱动的预测策略的强化学习阶段。

Result: 在主要的视频理解基准测试中，K-frames 提供了有效、可解释和即插即用的关键帧选择解决方案。

Conclusion: K-frames 是一种用于长视频理解的关键帧选择的有效方法。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
capabilities in image understanding, but long-video are constrained by context
windows and computational cost. Uniform frame sampling often leads to
substantial information loss. Meanwhile existing keyframe selection methods
such as text-frame retrieval or RL-based frame optimization typically yield
sparse and temporally disjointed frames, overlooking scene continuity and
lacking flexibility for multi-scale frame selection. To address these
limitations, we introduce K-frames, a novel paradigm for scene-driven keyframe
selection that preserves temporal continuity. Instead of selecting individual
frames, K-frames predicts semantically coherent, query-relevant clips, which
enables any-k keyframes selection to meet diverse user budgets. To achieve this
approach, we first introduce PeakClips, a dataset of 200K video highlights
conditioned by query. Building on this dataset, K-frames learns clip2frame
selection using a three-stage progressive curriculum. It involves two
Supervised Fine-Tuning stages for temporal grounding and key-clip perception,
followed by a Reinforcement Learning stage that directly optimizes the
scene-driven prediction policy for downstream task without further annotations.
Extensive experiments on major long-video understanding benchmarks demonstrate
that K-frames provides an effective, interpretable, and plug-and-play solution
for keyframe selection at various scales. Our dataset and model will be
available.

</details>


### [163] [Multi-View Semi-Supervised Label Distribution Learning with Local Structure Complementarity](https://arxiv.org/abs/2510.13917)
*Yanshan Xiao,Kaihong Wu,Bo Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的多视角半监督标签分布学习 (MVSS-LDL) 方法，该方法利用每个视角的局部最近邻结构，并强调多个视角中局部最近邻结构的互补性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要针对有标记数据的单视角标签分布学习 (LDL) 问题，而对有标记和无标记数据的多视角 LDL 问题考虑不足。

Method: 该方法首先通过计算 k 近邻来探索视图 v 的局部结构。然后，通过结合其他视图中样本 xi 的最近邻来补充视图 v 中的最近邻集。最后，基于每个视图中补充的最近邻集，构建了一个基于图学习的多视角半监督 LDL 模型。

Result: 数值研究表明，MVSS-LDL 比现有的单视角 LDL 方法具有更好的分类性能。

Conclusion: 本文首次尝试了多视角 LDL，并证明了所提出的 MVSS-LDL 方法的有效性。

Abstract: Label distribution learning (LDL) is a paradigm that each sample is
associated with a label distribution. At present, the existing approaches are
proposed for the single-view LDL problem with labeled data, while the
multi-view LDL problem with labeled and unlabeled data has not been considered.
In this paper, we put forward the multi-view semi-supervised label distribution
learning with local structure complementarity (MVSS-LDL) approach, which
exploits the local nearest neighbor structure of each view and emphasizes the
complementarity of local nearest neighbor structures in multiple views.
Specifically speaking, we first explore the local structure of view $v$ by
computing the $k$-nearest neighbors. As a result, the $k$-nearest neighbor set
of each sample $\boldsymbol{x}_i$ in view $v$ is attained. Nevertheless, this
$k$-nearest neighbor set describes only a part of the nearest neighbor
information of sample $\boldsymbol{x}_i$. In order to obtain a more
comprehensive description of sample $\boldsymbol{x}_i$'s nearest neighbors, we
complement the nearest neighbor set in view $v$ by incorporating sample
$\boldsymbol{x}_i$'s nearest neighbors in other views. Lastly, based on the
complemented nearest neighbor set in each view, a graph learning-based
multi-view semi-supervised LDL model is constructed. By considering the
complementarity of local nearest neighbor structures, different views can
mutually provide the local structural information to complement each other. To
the best of our knowledge, this is the first attempt at multi-view LDL.
Numerical studies have demonstrated that MVSS-LDL attains explicitly better
classification performance than the existing single-view LDL methods.

</details>


### [164] [Weight Weaving: Parameter Pooling for Data-Free Model Merging](https://arxiv.org/abs/2510.13921)
*Levy Chaves,Eduardo Valle,Sandra Avila*

Main category: cs.LG

TL;DR: 提出了一种名为Weight Weaving的即插即用技术，用于模型融合，该技术通过用户定义的池化函数在λ值搜索空间中汇集模型权重，无需访问任何数据，并在多个视觉任务上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的模型融合方法严重依赖于缩放超参数λ，但数据无关的设置λ的原则性方法很少，通常需要使用来自评估集的特权数据进行调整，这在实践中是不可行的。

Method: 提出Weight Weaving技术，该技术通过用户定义的池化函数在λ值搜索空间中汇集模型权重，无需数据，且与现有的模型融合方法正交。

Result: 在三个ViT变体的视觉多任务学习、视觉持续学习和领域泛化实验中，Weight Weaving始终提高了多种模型融合方法的性能，在数据无关的设置中平均准确率提高了15.9个百分点。

Conclusion: Weight Weaving是一种有效的模型融合技术，它具有高度模块化，对搜索空间的约束最小，并且不需要评估数据。

Abstract: Model merging provides a cost-effective and data-efficient combination of
specialized deep neural networks through parameter integration. This technique
leverages expert models across downstream tasks without requiring retraining.
Most model merging approaches critically depend on scaling hyper-parameters
$\lambda$, which weight each model's contribution globally or individually.
Principled approaches for setting scaling factors without accessing any data
(data-free) are scarce, often leading researchers to tune $\lambda$ using
privileged data from the evaluation set, which is obviously unfeasible in
practice. To address this limitation, we introduce Weight Weaving, a
plug-and-play technique that pools model weights across $\lambda$ values search
space using user-defined pooling functions, such as averaging, random
selection, or even existing model merging methods. Our method demonstrates high
modularity, imposing minimal constraints on the search space. It operates
orthogonally to existing model merging methods and eliminates evaluation data
requirements. We validate Weight Weaving across three ViT variants in three
experimental setups: vision multi-task learning, vision continual learning, and
domain generalization. Our method consistently improves the performance of
several model merging methods, achieving average accuracy gains of up to 15.9
percentage points in a data-free setting.

</details>


### [165] [LTR-ICD: A Learning-to-Rank Approach for Automatic ICD Coding](https://arxiv.org/abs/2510.13922)
*Mohammad Mansoori,Amira Soliman,Farzaneh Etminani*

Main category: cs.LG

TL;DR: 本文提出了一种新的方法，将ICD代码分配和排序任务视为分类和排序任务，优于传统分类方法。


<details>
  <summary>Details</summary>
Motivation: 临床笔记中的非结构化文本需要被正确分配和排序ICD代码，这对医疗诊断和报销至关重要。以往方法忽略了ICD代码的顺序。

Method: 从检索系统的角度，将问题转化为分类和排序任务。

Result: 在正确排序主要诊断代码方面的准确率为47%，而最先进的分类器为20%。Micro-F1 score为0.6065，Macro-F1 score为0.2904，超过了之前的最佳模型。

Conclusion: 提出的框架在识别高优先级代码方面具有优越的能力。

Abstract: Clinical notes contain unstructured text provided by clinicians during
patient encounters. These notes are usually accompanied by a sequence of
diagnostic codes following the International Classification of Diseases (ICD).
Correctly assigning and ordering ICD codes are essential for medical diagnosis
and reimbursement. However, automating this task remains challenging.
State-of-the-art methods treated this problem as a classification task, leading
to ignoring the order of ICD codes that is essential for different purposes. In
this work, as a first attempt, we approach this task from a retrieval system
perspective to consider the order of codes, thus formulating this problem as a
classification and ranking task. Our results and analysis show that the
proposed framework has a superior ability to identify high-priority codes
compared to other methods. For instance, our model accuracy in correctly
ranking primary diagnosis codes is 47%, compared to 20% for the
state-of-the-art classifier. Additionally, in terms of classification metrics,
the proposed model achieves a micro- and macro-F1 scores of 0.6065 and 0.2904,
respectively, surpassing the previous best model with scores of 0.597 and
0.2660.

</details>


### [166] [Distributional Consistency Loss: Beyond Pointwise Data Terms in Inverse Problems](https://arxiv.org/abs/2510.13972)
*George Webber,Andrew J. Reader*

Main category: cs.LG

TL;DR: 提出了一种新的数据保真度损失函数，称为分布一致性（DC）损失，它通过测试观测测量值在统计上是否与当前估计所暗示的噪声分布一致来集体评估数据保真度，而不是逐点匹配。


<details>
  <summary>Details</summary>
Motivation: 传统的数据保真度损失函数（如均方误差（MSE）或负对数似然）寻求与噪声测量值的逐点一致性，这通常导致对噪声的过度拟合。为了解决这个问题。

Method: 引入分布一致性（DC）损失，这是一种数据保真度目标，它使用基于模型的每个测量概率分数，用分布级别校准替换逐点匹配。

Result: 在图像去噪和医学图像重建两个关键示例应用领域中证明了有效性：i）在使用深度图像先验的图像去噪中，使用DC代替MSE损失消除了对提前停止的需求并实现了更高的PSNR；ii）在泊松噪声数据的医学图像重建中，DC损失减少了高度迭代重建中的伪影，并增强了手工制作的正则化的功效。

Conclusion: DC损失是一种基于统计的、性能增强的传统保真度损失的替代方案，适用于逆问题。

Abstract: Recovering true signals from noisy measurements is a central challenge in
inverse problems spanning medical imaging, geophysics, and signal processing.
Current solutions balance prior assumptions regarding the true signal
(regularization) with agreement to noisy measured data (data-fidelity).
Conventional data-fidelity loss functions, such as mean-squared error (MSE) or
negative log-likelihood, seek pointwise agreement with noisy measurements,
often leading to overfitting to noise. In this work, we instead evaluate
data-fidelity collectively by testing whether the observed measurements are
statistically consistent with the noise distributions implied by the current
estimate. We adopt this aggregated perspective and introduce distributional
consistency (DC) loss, a data-fidelity objective that replaces pointwise
matching with distribution-level calibration using model-based probability
scores for each measurement. DC loss acts as a direct and practical plug-in
replacement for standard data consistency terms: i) it is compatible with
modern regularizers, ii) it is optimized in the same way as traditional losses,
and iii) it avoids overfitting to measurement noise even without the use of
priors. Its scope naturally fits many practical inverse problems where the
measurement-noise distribution is known and where the measured dataset consists
of many independent noisy values. We demonstrate efficacy in two key example
application areas: i) in image denoising with deep image prior, using DC
instead of MSE loss removes the need for early stopping and achieves higher
PSNR; ii) in medical image reconstruction from Poisson-noisy data, DC loss
reduces artifacts in highly-iterated reconstructions and enhances the efficacy
of hand-crafted regularization. These results position DC loss as a
statistically grounded, performance-enhancing alternative to conventional
fidelity losses for inverse problems.

</details>


### [167] [BitNet Distillation](https://arxiv.org/abs/2510.13998)
*Xun Wu,Shaohan Huang,Wenhui Wang,Ting Song,Li Dong,Yan Xia,Furu Wei*

Main category: cs.LG

TL;DR: BitDistill：一个将全精度LLM微调到1.58位精度的轻量级流程，以低计算成本实现强大的特定任务性能。


<details>
  <summary>Details</summary>
Motivation: 为了在特定下游任务上，缩小全精度和1.58位LLM之间的性能差距，并降低计算成本。

Method: BitDistill包含三个关键技术：SubLN模块、基于MiniLM的多头注意力蒸馏和持续预训练。

Result: BitDistill实现了与全精度模型相当的性能，同时节省了高达10倍的内存，并在CPU上实现了2.65倍的更快推理。

Conclusion: BitDistill是一种有效的模型压缩和加速方法，可以在保持性能的同时显著降低计算资源需求。

Abstract: In this paper, we present BitNet Distillation (BitDistill), a lightweight
pipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into
1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream
tasks, achieving strong task-specific performance with minimal computational
cost. Specifically, BitDistill incorporates three key techniques: the SubLN
module, as introduced in BitNet; multi-head attention distillation, based on
MiniLM; and continual pre-training, which serves as a crucial warm-up step to
mitigate the scalability issue of the performance gap between finetuned
full-precision and 1.58-bit LLMs on specific tasks. Experimental results show
that BitDistill achieves performance comparable to the full-precision
counterpart models across model size, while enabling up to 10x memory savings
and 2.65x faster inference on CPUs. Code is available at
https://github.com/microsoft/BitNet.

</details>


### [168] [REAP the Experts: Why Pruning Prevails for One-Shot MoE compression](https://arxiv.org/abs/2510.13999)
*Mike Lasby,Ivan Lazarevich,Nish Sinnadurai,Sean Lie,Yani Ioannou,Vithursan Thangarasa*

Main category: cs.LG

TL;DR: 提出了一种新的剪枝标准REAP，用于压缩SMoE模型，尤其是在生成任务中。


<details>
  <summary>Details</summary>
Motivation: SMoE模型参数量大，内存开销大，因此需要进行专家压缩。现有研究表明，专家合并在判别性基准测试中表现良好，但本文发现专家剪枝在生成任务中更优。

Method: 提出了一种新的剪枝标准，名为Router-weighted Expert Activation Pruning (REAP)，该标准考虑了路由器门控值和专家激活范数。

Result: 在各种SMoE模型（从20B到1T参数）上，REAP在生成基准测试中始终优于合并和其他剪枝方法，尤其是在50%压缩率下。在代码生成和工具调用任务上，即使剪枝50%的专家，REAP也能实现近乎无损的压缩。

Conclusion: 专家剪枝是生成任务中压缩SMoE模型的更好策略，REAP是一种有效的剪枝方法。

Abstract: Sparsely-activated Mixture-of-Experts (SMoE) models offer efficient
pre-training and low latency but their large parameter counts create
significant memory overhead, motivating research into expert compression.
Contrary to recent findings favouring expert merging on discriminative
benchmarks, we demonstrate that expert pruning is a superior strategy for
generative tasks. We prove that merging introduces an irreducible error by
causing a "functional subspace collapse", due to the loss of the router's
independent, input-dependent control over experts. Leveraging this insight, we
propose Router-weighted Expert Activation Pruning (REAP), a novel pruning
criterion that considers both router gate-values and expert activation norms.
Across a diverse set of SMoE models ranging from 20B to 1T parameters, REAP
consistently outperforms merging and other pruning methods on generative
benchmarks, especially at 50% compression. Notably, our method achieves
near-lossless compression on code generation and tool-calling tasks with
Qwen3-Coder-480B and Kimi-K2, even after pruning 50% of experts.

</details>


### [169] [Conditional Clifford-Steerable CNNs with Complete Kernel Basis for PDE Modeling](https://arxiv.org/abs/2510.14007)
*Bálint László Szarvas,Maksim Zhdanov*

Main category: cs.LG

TL;DR: Clifford-Steerable CNNs (CSCNNs) are limited in expressivity. This paper proposes Conditional Clifford-Steerable Kernels to improve expressivity.


<details>
  <summary>Details</summary>
Motivation: The kernel basis of CSCNNs is not complete, thus limiting the model expressivity.

Method: Propose Conditional Clifford-Steerable Kernels, which augment the kernels with equivariant representations computed from the input feature field. Equivariance constraint is derived and solved via implicit parameterization.

Result: Improved expressivity on multiple PDE forecasting tasks, including fluid dynamics and relativistic electrodynamics, where the method consistently outperforms baseline methods.

Conclusion: The proposed method improves the expressivity of CSCNNs.

Abstract: Clifford-Steerable CNNs (CSCNNs) provide a unified framework that allows
incorporating equivariance to arbitrary pseudo-Euclidean groups, including
isometries of Euclidean space and Minkowski spacetime. In this work, we
demonstrate that the kernel basis of CSCNNs is not complete, thus limiting the
model expressivity. To address this issue, we propose Conditional
Clifford-Steerable Kernels, which augment the kernels with equivariant
representations computed from the input feature field. We derive the
equivariance constraint for these input-dependent kernels and show how it can
be solved efficiently via implicit parameterization. We empirically demonstrate
an improved expressivity of the resulting framework on multiple PDE forecasting
tasks, including fluid dynamics and relativistic electrodynamics, where our
method consistently outperforms baseline methods.

</details>


### [170] [Noise-Adaptive Layerwise Learning Rates: Accelerating Geometry-Aware Optimization for Deep Neural Network Training](https://arxiv.org/abs/2510.14009)
*Jie Hao,Xiaochuan Gong,Jie Xu,Zhengdao Wang,Mingrui Liu*

Main category: cs.LG

TL;DR: 提出了一种噪声自适应的层级学习率方案，以加速深度神经网络（DNN）的训练。


<details>
  <summary>Details</summary>
Motivation: 现有的几何感知优化器在同一组层内使用固定的学习率，这对于DNN训练来说可能效率低下，因为局部曲率在不同层之间可能是异构的，并且在训练过程中动态变化。

Method: 在几何感知优化算法的基础上，引入了一种噪声自适应的层级学习率方案。该方法估计由所选LMO引起的对偶范数中的梯度方差，并使用它来分配每个组内随时间变化的噪声自适应层级学习率。

Result: 在Transformer架构（如LLaMA和GPT）上的经验结果表明，该方法比最先进的优化器实现了更快的收敛速度。

Conclusion: 该算法实现了快速的收敛速度，并在Transformer架构上表现出优于现有优化器的性能。

Abstract: Geometry-aware optimization algorithms, such as Muon, have achieved
remarkable success in training deep neural networks (DNNs). These methods
leverage the underlying geometry of DNNs by selecting appropriate norms for
different layers and updating parameters via norm-constrained linear
minimization oracles (LMOs). However, even within a group of layers associated
with the same norm, the local curvature can be heterogeneous across layers and
vary dynamically over the course of training. For example, recent work shows
that sharpness varies substantially across transformer layers and throughout
training, yet standard geometry-aware optimizers impose fixed learning rates to
layers within the same group, which may be inefficient for DNN training.
  In this paper, we introduce a noise-adaptive layerwise learning rate scheme
on top of geometry-aware optimization algorithms and substantially accelerate
DNN training compared to methods that use fixed learning rates within each
group. Our method estimates gradient variance in the dual norm induced by the
chosen LMO on the fly, and uses it to assign time-varying noise-adaptive
layerwise learning rates within each group. We provide a theoretical analysis
showing that our algorithm achieves a sharp convergence rate. Empirical results
on transformer architectures such as LLaMA and GPT demonstrate that our
approach achieves faster convergence than state-of-the-art optimizers.

</details>


### [171] [Context-Selective State Space Models: Feedback is All You Need](https://arxiv.org/abs/2510.14027)
*Riccardo Zattra,Giacomo Baggio,Umberto Casti,Augusto Ferrante,Francesco Ticozzi*

Main category: cs.LG

TL;DR: COFFEE模型是一种新型时变状态空间模型，它结合了状态反馈以实现上下文相关的选择性，同时仍然允许并行实现。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在处理长序列时面临二次复杂度和困难，而状态空间模型(SSM)提供了一种有效的替代方案。Mamba架构中的S6模块在长序列基准测试中取得了最先进的结果。本文旨在通过引入COFFEE模型，探索状态反馈在构建可扩展和高效序列模型中的作用。

Method: COFFEE模型通过状态反馈实现上下文相关的选择性，并采用高效的模型参数化来消除S6中存在的冗余。

Result: 在induction head任务中，COFFEE模型以比S6少两个数量级的参数和训练序列实现了接近完美的精度。在MNIST数据集上，COFFEE模型在相同架构下大大优于S6，仅使用3585个参数就达到了97%的准确率。

Conclusion: 状态反馈是构建可扩展和高效序列模型的关键机制。

Abstract: Transformers, powered by the attention mechanism, are the backbone of most
foundation models, yet they suffer from quadratic complexity and difficulties
in dealing with long-range dependencies in the input sequence. Recent work has
shown that state space models (SSMs) provide an efficient alternative, with the
S6 module at the core of the Mamba architecture achieving state-of-the-art
results on long-sequence benchmarks. In this paper, we introduce the COFFEE
(COntext From FEEdback) model, a novel time-varying SSM that incorporates state
feedback to enable context-dependent selectivity, while still allowing for
parallel implementation. Whereas the selectivity mechanism of S6 only depends
on the current input, COFFEE computes it from the internal state, which serves
as a compact representation of the sequence history. This shift allows the
model to regulate its dynamics based on accumulated context, improving its
ability to capture long-range dependencies. In addition to state feedback, we
employ an efficient model parametrization that removes redundancies present in
S6 and leads to a more compact and trainable formulation. On the induction head
task, COFFEE achieves near-perfect accuracy with two orders of magnitude fewer
parameters and training sequences compared to S6. On MNIST, COFFEE largely
outperforms S6 within the same architecture, reaching 97% accuracy with only
3585 parameters. These results showcase the role of state feedback as a key
mechanism for building scalable and efficient sequence models.

</details>


### [172] [Agentic Entropy-Balanced Policy Optimization](https://arxiv.org/abs/2510.14545)
*Guanting Dong,Licheng Bao,Zhongyuan Wang,Kangzhi Zhao,Xiaoxi Li,Jiajie Jin,Jinghan Yang,Hangyu Mao,Fuzheng Zhang,Kun Gai,Guorui Zhou,Yutao Zhu,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.LG

TL;DR: Agentic Reinforcement Learning (RL) suffers from training collapse due to excessive reliance on entropy signals. This paper proposes Agentic Entropy-Balanced Policy Optimization (AEPO) to balance entropy in rollout and policy update phases.


<details>
  <summary>Details</summary>
Motivation: Excessive reliance on entropy signals in Agentic RL can lead to training collapse.

Method: AEPO includes: (1) a dynamic entropy-balanced rollout mechanism that adaptively allocates sampling budget and penalizes consecutive high-entropy tool-call steps; and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient operation into the high-entropy clipping term and incorporates entropy-aware advantage estimation.

Result: AEPO outperforms 7 mainstream RL algorithms across 14 datasets. With 1K RL samples, Qwen3-14B with AEPO achieves 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on WebWalker for Pass@5.

Conclusion: AEPO improves rollout sampling diversity while maintaining stable policy entropy, facilitating scalable web agent training.

Abstract: Recently, Agentic Reinforcement Learning (Agentic RL) has made significant
progress in incentivizing the multi-turn, long-horizon tool-use capabilities of
web agents. While mainstream agentic RL algorithms autonomously explore
high-uncertainty tool-call steps under the guidance of entropy, excessive
reliance on entropy signals can impose further constraints, leading to the
training collapse. In this paper, we delve into the challenges caused by
entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an
agentic RL algorithm designed to balance entropy in both the rollout and policy
update phases. AEPO comprises two core components: (1) a dynamic
entropy-balanced rollout mechanism that adaptively allocate global and branch
sampling budget through entropy pre-monitoring, while imposing a branch penalty
on consecutive high-entropy tool-call steps to prevent over-branching issues;
and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient
operation into the high-entropy clipping term to preserve and properly rescale
gradients on high-entropy tokens, while incorporating entropy-aware advantage
estimation to prioritize learning on high-uncertainty tokens. Results across 14
challenging datasets show that AEPO consistently outperforms 7 mainstream RL
algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive
results: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker
for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on
WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout
sampling diversity while maintaining stable policy entropy, facilitating
scalable web agent training.

</details>


### [173] [CausalVerse: Benchmarking Causal Representation Learning with Configurable High-Fidelity Simulations](https://arxiv.org/abs/2510.14049)
*Guangyi Chen,Yunlong Deng,Peiyuan Zhu,Yan Li,Yifan Sheng,Zijian Li,Kun Zhang*

Main category: cs.LG

TL;DR: 提出了一个新的用于因果表征学习(CRL)的基准，该基准使用高保真模拟视觉数据，保留了真实的视觉复杂性，更重要的是，可以访问真实的因果生成过程。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法通常依赖于简单的合成数据集或真实世界任务的下游性能，通常在现实主义和评估精度之间存在两难。

Method: 使用高保真模拟视觉数据，构建包含20万张图像和300万个视频帧的数据集，涵盖静态图像生成、动态物理模拟、机器人操作和交通状况分析四个领域。

Result: 评估了不同范例中的代表性CRL方法，并提供了经验见解，以帮助从业者和新手选择或扩展适当的CRL框架。

Conclusion: 提供了一个综合的测试平台，有望弥合严格评估和现实世界适用性之间的差距。

Abstract: Causal Representation Learning (CRL) aims to uncover the data-generating
process and identify the underlying causal variables and relations, whose
evaluation remains inherently challenging due to the requirement of known
ground-truth causal variables and causal structure. Existing evaluations often
rely on either simplistic synthetic datasets or downstream performance on
real-world tasks, generally suffering a dilemma between realism and evaluative
precision. In this paper, we introduce a new benchmark for CRL using
high-fidelity simulated visual data that retains both realistic visual
complexity and, more importantly, access to ground-truth causal generating
processes. The dataset comprises around 200 thousand images and 3 million video
frames across 24 sub-scenes in four domains: static image generation, dynamic
physical simulations, robotic manipulations, and traffic situation analysis.
These scenarios range from static to dynamic settings, simple to complex
structures, and single to multi-agent interactions, offering a comprehensive
testbed that hopefully bridges the gap between rigorous evaluation and
real-world applicability. In addition, we provide flexible access to the
underlying causal structures, allowing users to modify or configure them to
align with the required assumptions in CRL, such as available domain labels,
temporal dependencies, or intervention histories. Leveraging this benchmark, we
evaluated representative CRL methods across diverse paradigms and offered
empirical insights to assist practitioners and newcomers in choosing or
extending appropriate CRL frameworks to properly address specific types of real
problems that can benefit from the CRL perspective. Welcome to visit our:
Project page:https://causal-verse.github.io/,
Dataset:https://huggingface.co/CausalVerse.

</details>


### [174] [Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge Graphs with Hybrid Retrieval](https://arxiv.org/abs/2510.14592)
*Rashmi R,Vidyadhar Upadhya*

Main category: cs.LG

TL;DR: 提出了一种用于多模态文档问答的模态感知混合检索架构（MAHA）。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）系统主要处理单模态文本数据，限制了它们在非结构化多模态文档上的有效性。这些文档通常结合了文本、图像、表格、公式和图表，每种模态都贡献了独特的信息。

Method: MAHA集成了密集向量检索与结构化图遍历，其中知识图编码了跨模态语义和关系。这种设计实现了跨不同模态的语义丰富和上下文感知的检索。

Result: 在多个基准数据集上的评估表明，MAHA显著优于基线方法，实现了0.486的ROUGE-L得分，并提供了完整的模态覆盖。

Conclusion: 这项工作建立了一个可扩展且可解释的检索框架，通过实现对非结构化多模态数据的模态感知推理，从而推进了RAG系统。

Abstract: Current Retrieval-Augmented Generation (RAG) systems primarily operate on
unimodal textual data, limiting their effectiveness on unstructured multimodal
documents. Such documents often combine text, images, tables, equations, and
graphs, each contributing unique information. In this work, we present a
Modality-Aware Hybrid retrieval Architecture (MAHA), designed specifically for
multimodal question answering with reasoning through a modality-aware knowledge
graph. MAHA integrates dense vector retrieval with structured graph traversal,
where the knowledge graph encodes cross-modal semantics and relationships. This
design enables both semantically rich and context-aware retrieval across
diverse modalities. Evaluations on multiple benchmark datasets demonstrate that
MAHA substantially outperforms baseline methods, achieving a ROUGE-L score of
0.486, providing complete modality coverage. These results highlight MAHA's
ability to combine embeddings with explicit document structure, enabling
effective multimodal retrieval. Our work establishes a scalable and
interpretable retrieval framework that advances RAG systems by enabling
modality-aware reasoning over unstructured multimodal data.

</details>


### [175] [FedHFT: Efficient Federated Finetuning with Heterogeneous Edge Clients](https://arxiv.org/abs/2510.14054)
*Fatih Ilhan,Selim Furkan Tekin,Tiansheng Huang,Gaowen Liu,Ramana Kompella,Greg Eisenhauer,Yingyan Celine Lin,Calton Pu,Ling Liu*

Main category: cs.LG

TL;DR: 提出了一种高效且个性化的联邦微调框架FedHFT，以应对NLU应用中数据和资源异构性的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于专有数据的机密性或隐私要求，用于微调的数据有限且异构；参与客户端（如边缘设备）可用的计算资源各不相同。

Method: 引入混合掩码适配器来处理资源异构性，并引入基于掩码个性化和客户端聚类的双层优化方法来处理非独立同分布数据。

Result: 在数据和资源异构性下，与代表性的异构联邦学习方法相比，在各种自然语言理解任务上表现出显着的性能和效率提升。

Conclusion: FedHFT框架有效应对了NLU应用中数据和资源异构性的挑战，并在性能和效率上优于现有方法。

Abstract: Fine-tuning pre-trained large language models (LLMs) has become a common
practice for personalized natural language understanding (NLU) applications on
downstream tasks and domain-specific datasets. However, there are two main
challenges: (i) limited and/or heterogeneous data for fine-tuning due to
proprietary data confidentiality or privacy requirements, and (ii) varying
computation resources available across participating clients such as edge
devices. This paper presents FedHFT - an efficient and personalized federated
fine-tuning framework to address both challenges. First, we introduce a mixture
of masked adapters to handle resource heterogeneity across participating
clients, enabling high-performance collaborative fine-tuning of pre-trained
language model(s) across multiple clients in a distributed setting, while
keeping proprietary data local. Second, we introduce a bi-level optimization
approach to handle non-iid data distribution based on masked personalization
and client clustering. Extensive experiments demonstrate significant
performance and efficiency improvements over various natural language
understanding tasks under data and resource heterogeneity compared to
representative heterogeneous federated learning methods.

</details>


### [176] [On the expressivity of sparse maxout networks](https://arxiv.org/abs/2510.14068)
*Moritz Grillo,Tobias Hofmann*

Main category: cs.LG

TL;DR: 研究稀疏 maxout 网络的表达能力，其中每个神经元从前一层获取固定数量的输入并采用 maxout 激活。


<details>
  <summary>Details</summary>
Motivation: 研究卷积神经网络或图神经网络的关键特性。

Method: 建立网络可计算函数与一类虚拟多面体之间的对偶性，将其几何形状与网络表达性问题联系起来，推导出相关多面体维数的严格界限。

Result: 深度足够的稀疏 maxout 网络是通用的。如果未达到所需的深度，则宽度本身无法弥补固定入度约束的稀疏性。

Conclusion: 稀疏 maxout 网络的表达能力与网络的深度和宽度有关。

Abstract: We study the expressivity of sparse maxout networks, where each neuron takes
a fixed number of inputs from the previous layer and employs a, possibly
multi-argument, maxout activation. This setting captures key characteristics of
convolutional or graph neural networks. We establish a duality between
functions computable by such networks and a class of virtual polytopes, linking
their geometry to questions of network expressivity. In particular, we derive a
tight bound on the dimension of the associated polytopes, which serves as the
central tool for our analysis. Building on this, we construct a sequence of
depth hierarchies. While sufficiently deep sparse maxout networks are
universal, we prove that if the required depth is not reached, width alone
cannot compensate for the sparsity of a fixed indegree constraint.

</details>


### [177] [Exploratory Causal Inference in SAEnce](https://arxiv.org/abs/2510.14073)
*Tommaso Mencattini,Riccardo Cadei,Francesco Locatello*

Main category: cs.LG

TL;DR: 本研究提出了一种名为神经效应搜索（Neural Effect Search）的新方法，用于直接从数据中发现治疗的未知效果，无需手动设计的假设。


<details>
  <summary>Details</summary>
Motivation: 传统随机对照试验依赖于手工设计的假设和昂贵的分析，限制了大规模因果效应估计，并可能锚定在流行的但不完整的假设上。

Method: 该方法利用预训练的基础模型将试验中的非结构化数据转化为有意义的表示，并通过稀疏自编码器进行解释。为了解决多重测试和效应纠缠问题，引入了一种新的递归程序，通过逐步分层解决这两个问题。

Result: 在半合成实验中评估了算法的稳健性，并在实验生态学中展示了在真实科学试验中首次成功进行的无监督因果效应识别。

Conclusion: 该研究成功地在真实世界的科学试验中，实现了无监督的因果效应识别。

Abstract: Randomized Controlled Trials are one of the pillars of science; nevertheless,
they rely on hand-crafted hypotheses and expensive analysis. Such constraints
prevent causal effect estimation at scale, potentially anchoring on popular yet
incomplete hypotheses. We propose to discover the unknown effects of a
treatment directly from data. For this, we turn unstructured data from a trial
into meaningful representations via pretrained foundation models and interpret
them via a sparse autoencoder. However, discovering significant causal effects
at the neural level is not trivial due to multiple-testing issues and effects
entanglement. To address these challenges, we introduce Neural Effect Search, a
novel recursive procedure solving both issues by progressive stratification.
After assessing the robustness of our algorithm on semi-synthetic experiments,
we showcase, in the context of experimental ecology, the first successful
unsupervised causal effect identification on a real-world scientific trial.

</details>


### [178] [Neural Network approximation power on homogeneous and heterogeneous reaction-diffusion equations](https://arxiv.org/abs/2510.14094)
*Haotian Feng*

Main category: cs.LG

TL;DR: 本文研究了神经网络近似反应扩散方程解的能力。


<details>
  <summary>Details</summary>
Motivation: 动机：虽然神经网络越来越多地用于求解微分方程，但对其有效性的理论基础研究不足。

Method: 方法：利用通用逼近定理，分析了两层和三层神经网络分别逼近一维和二维反应扩散方程的能力。

Result: 结果：证明了两层神经网络可以逼近一维反应扩散方程，三层神经网络可以逼近二维反应扩散方程。

Conclusion: 结论：强调了神经网络在逼近反应扩散方程和相关偏微分方程解方面的表达能力，为基于神经网络的微分方程求解器提供了理论基础。

Abstract: Reaction-diffusion systems represent one of the most fundamental formulations
used to describe a wide range of physical, chemical, and biological processes.
With the increasing adoption of neural networks, recent research has focused on
solving differential equations using machine learning techniques. However, the
theoretical foundation explaining why neural networks can effectively
approximate such solutions remains insufficiently explored.
  This paper provides a theoretical analysis of the approximation power of
neural networks for one- and two-dimensional reaction-diffusion equations in
both homogeneous and heterogeneous media. Building upon the universal
approximation theorem, we demonstrate that a two-layer neural network can
approximate the one-dimensional reaction-diffusion equation, while a
three-layer neural network can approximate its two-dimensional counterpart. The
theoretical framework presented here can be further extended to elliptic and
parabolic equations.
  Overall, this work highlights the expressive power of neural networks in
approximating solutions to reaction-diffusion equations and related PDEs,
providing a theoretical foundation for neural network-based differential
equation solvers.

</details>


### [179] [Unlocking Out-of-Distribution Generalization in Transformers via Recursive Latent Space Reasoning](https://arxiv.org/abs/2510.14095)
*Awni Altabaa,Siyu Chen,John Lafferty,Zhuoran Yang*

Main category: cs.LG

TL;DR: 本文研究了Transformer网络中的OOD泛化问题，并提出了一系列架构机制来增强OOD泛化能力。


<details>
  <summary>Details</summary>
Motivation: 机器学习中的一个核心挑战是超出训练分布的系统性、组合泛化，这是现代语言模型涌现推理能力的关键瓶颈。本文旨在解决这个问题。

Method: 使用GSM8K风格的计算图模块化算术任务作为测试平台，研究了Transformer网络中的OOD泛化，并引入和探索了一组旨在增强OOD泛化的架构机制，包括输入自适应递归、算法监督、通过离散瓶颈锚定的潜在表示和显式纠错机制。

Result: 这些机制共同构成了一种架构方法，用于在具有鲁棒算法泛化能力的Transformer网络中进行原生和可扩展的潜在空间推理。

Conclusion: 详细的机械可解释性分析表明，这些机制如何产生强大的OOD泛化能力。

Abstract: Systematic, compositional generalization beyond the training distribution
remains a core challenge in machine learning -- and a critical bottleneck for
the emergent reasoning abilities of modern language models. This work
investigates out-of-distribution (OOD) generalization in Transformer networks
using a GSM8K-style modular arithmetic on computational graphs task as a
testbed. We introduce and explore a set of four architectural mechanisms aimed
at enhancing OOD generalization: (i) input-adaptive recurrence; (ii)
algorithmic supervision; (iii) anchored latent representations via a discrete
bottleneck; and (iv) an explicit error-correction mechanism. Collectively,
these mechanisms yield an architectural approach for native and scalable latent
space reasoning in Transformer networks with robust algorithmic generalization
capabilities. We complement these empirical results with a detailed mechanistic
interpretability analysis that reveals how these mechanisms give rise to robust
OOD generalization abilities.

</details>


### [180] [TENDE: Transfer Entropy Neural Diffusion Estimation](https://arxiv.org/abs/2510.14096)
*Simon Pedro Galeano Munoz,Mustapha Bounoua,Giulio Franzese,Pietro Michiardi,Maurizio Filippone*

Main category: cs.LG

TL;DR: 提出了一种新的方法TENDE，它利用基于分数的扩散模型通过条件互信息来估计转移熵。


<details>
  <summary>Details</summary>
Motivation: 现有的估计方法受维度灾难的影响，需要限制性的分布假设，或者需要指数级大的数据集才能实现可靠的收敛。

Method: 利用基于分数的扩散模型，通过学习相关条件分布的分数函数来估计转移熵。

Result: 与现有的神经估计器和其他最先进的方法相比，在合成基准和真实数据上表现出卓越的准确性和鲁棒性。

Conclusion: TENDE提供了一种灵活、可扩展的估计，同时对底层数据生成过程做出了最小的假设。

Abstract: Transfer entropy measures directed information flow in time series, and it
has become a fundamental quantity in applications spanning neuroscience,
finance, and complex systems analysis. However, existing estimation methods
suffer from the curse of dimensionality, require restrictive distributional
assumptions, or need exponentially large datasets for reliable convergence. We
address these limitations in the literature by proposing TENDE (Transfer
Entropy Neural Diffusion Estimation), a novel approach that leverages
score-based diffusion models to estimate transfer entropy through conditional
mutual information. By learning score functions of the relevant conditional
distributions, TENDE provides flexible, scalable estimation while making
minimal assumptions about the underlying data-generating process. We
demonstrate superior accuracy and robustness compared to existing neural
estimators and other state-of-the-art approaches across synthetic benchmarks
and real data.

</details>


### [181] [Near-Optimal Regret-Queue Length Tradeoff in Online Learning for Two-Sided Markets](https://arxiv.org/abs/2510.14097)
*Zixian Yang,Sushil Mahavir Varma,Lei Ying*

Main category: cs.LG

TL;DR: 研究一个双边市场，平台需要设计定价和匹配算法来最大化利润，同时保持合理的队列长度。


<details>
  <summary>Details</summary>
Motivation: 实际中，控制价格依赖到达率的需求和供应曲线可能是未知的。

Method: 设计了一个新的基于在线学习的定价策略，并确定了其近似最优性。

Result: 在三个性能指标之间证明了一个权衡：$\\tilde{O}(T^{1-\\gamma})$ 的后悔值，$\\tilde{O}(T^{\\gamma/2})$ 的平均队列长度，以及 $\\tilde{O}(T^{\\gamma})$ 的最大队列长度，对于 $\\gamma \\in (0, 1/6]$，显著优于现有的结果。

Conclusion: 所提出的策略具有两个值得注意的特征：一个动态组件，优化了低后悔值和小队列长度之间的权衡；以及一个概率组件，解决了获得用于快速学习的有用样本和保持小队列长度之间的矛盾。

Abstract: We study a two-sided market, wherein, price-sensitive heterogeneous customers
and servers arrive and join their respective queues. A compatible
customer-server pair can then be matched by the platform, at which point, they
leave the system. Our objective is to design pricing and matching algorithms
that maximize the platform's profit, while maintaining reasonable queue
lengths. As the demand and supply curves governing the price-dependent arrival
rates may not be known in practice, we design a novel online-learning-based
pricing policy and establish its near-optimality. In particular, we prove a
tradeoff among three performance metrics: $\tilde{O}(T^{1-\gamma})$ regret,
$\tilde{O}(T^{\gamma/2})$ average queue length, and $\tilde{O}(T^{\gamma})$
maximum queue length for $\gamma \in (0, 1/6]$, significantly improving over
existing results [1]. Moreover, barring the permissible range of $\gamma$, we
show that this trade-off between regret and average queue length is optimal up
to logarithmic factors under a class of policies, matching the optimal one as
in [2] which assumes the demand and supply curves to be known. Our proposed
policy has two noteworthy features: a dynamic component that optimizes the
tradeoff between low regret and small queue lengths; and a probabilistic
component that resolves the tension between obtaining useful samples for fast
learning and maintaining small queue lengths.

</details>


### [182] [Briding Diffusion Posterior Sampling and Monte Carlo methods: a survey](https://arxiv.org/abs/2510.14114)
*Yazid Janati,Alain Durmus,Jimmy Olsson,Eric Moulines*

Main category: cs.LG

TL;DR: 这篇综述介绍了如何利用预训练的扩散模型和蒙特卡洛方法解决贝叶斯逆问题，而无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型在解决贝叶斯逆问题中的潜力。

Method: 主要采用“扭曲”机制来调整扩散过程中的中间分布，使其趋向于后验分布。然后，使用不同的蒙特卡洛方法来辅助从这些扭曲的分布中采样。

Result: 综述概述了当前利用预训练扩散模型解决贝叶斯逆问题的方法。

Conclusion: 扩散模型可以通过作为先验知识来有效解决贝叶斯逆问题。

Abstract: Diffusion models enable the synthesis of highly accurate samples from complex
distributions and have become foundational in generative modeling. Recently,
they have demonstrated significant potential for solving Bayesian inverse
problems by serving as priors. This review offers a comprehensive overview of
current methods that leverage \emph{pre-trained} diffusion models alongside
Monte Carlo methods to address Bayesian inverse problems without requiring
additional training. We show that these methods primarily employ a
\emph{twisting} mechanism for the intermediate distributions within the
diffusion process, guiding the simulations toward the posterior distribution.
We describe how various Monte Carlo methods are then used to aid in sampling
from these twisted distributions.

</details>


### [183] [Neural Network-enabled Domain-consistent Robust Optimisation for Global CO$_2$ Reduction Potential of Gas Power Plants](https://arxiv.org/abs/2510.14125)
*Waqar Muhammad Ashraf,Talha Ansar,Abdulelah S. Alshehri,Peipei Chen,Ramit Debnath,Vivek Dua*

Main category: cs.LG

TL;DR: 提出了一种新的鲁棒优化框架，该框架利用神经网络驱动，将数据驱动的域作为约束集成到非线性规划技术中。


<details>
  <summary>Details</summary>
Motivation: 解决参数化神经网络模型与优化求解器相互作用而产生的域不一致解问题。

Method: 将数据驱动的域作为约束集成到非线性规划技术中。

Result: 在一个1180兆瓦容量的联合循环燃气发电厂中，该框架提供了域一致的鲁棒最优解，实现了0.76个百分点的平均能源效率提升。

Conclusion: 机器学习在为全球气候行动提供近期、可扩展的脱碳路径方面具有协同作用。

Abstract: We introduce a neural network-driven robust optimisation framework that
integrates data-driven domain as a constraint into the nonlinear programming
technique, addressing the overlooked issue of domain-inconsistent solutions
arising from the interaction of parametrised neural network models with
optimisation solvers. Applied to a 1180 MW capacity combined cycle gas power
plant, our framework delivers domain-consistent robust optimal solutions that
achieve a verified 0.76 percentage point mean improvement in energy efficiency.
For the first time, scaling this efficiency gain to the global fleet of gas
power plants, we estimate an annual 26 Mt reduction potential in CO$_2$ (with
10.6 Mt in Asia, 9.0 Mt in the Americas, and 4.5 Mt in Europe). These results
underscore the synergetic role of machine learning in delivering near-term,
scalable decarbonisation pathways for global climate action.

</details>


### [184] [Demystifying the Mechanisms Behind Emergent Exploration in Goal-conditioned RL](https://arxiv.org/abs/2510.14129)
*Mahsa Bastankhah,Grace Liu,Dilip Arumugam,Thomas L. Griffiths,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 本文研究了无监督强化学习中涌现探索背后的机制，重点关注 Single-Goal Contrastive Reinforcement Learning (SGCRL) 算法。


<details>
  <summary>Details</summary>
Motivation: 了解 SGCRL 如何在没有外部奖励或课程的情况下解决具有挑战性的长时程目标到达任务，并深入理解其探索机制。

Method: 结合算法目标函数的理论分析和受控实验，来理解驱动 SGCRL 探索的因素。

Result: SGCRL 通过其学习到的表征最大化隐式奖励，这些表征自动修改奖励 landscape，以促进到达目标前的探索和之后的利用。实验表明，这些探索动态源于学习状态空间的低秩表征，而不是来自神经网络函数逼近。

Conclusion: 通过对 SGCRL 的理解，使其能够执行安全意识探索。

Abstract: In this work, we take a first step toward elucidating the mechanisms behind
emergent exploration in unsupervised reinforcement learning. We study
Single-Goal Contrastive Reinforcement Learning (SGCRL), a self-supervised
algorithm capable of solving challenging long-horizon goal-reaching tasks
without external rewards or curricula. We combine theoretical analysis of the
algorithm's objective function with controlled experiments to understand what
drives its exploration. We show that SGCRL maximizes implicit rewards shaped by
its learned representations. These representations automatically modify the
reward landscape to promote exploration before reaching the goal and
exploitation thereafter. Our experiments also demonstrate that these
exploration dynamics arise from learning low-rank representations of the state
space rather than from neural network function approximation. Our improved
understanding enables us to adapt SGCRL to perform safety-aware exploration.

</details>


### [185] [Learning Wireless Interference Patterns: Decoupled GNN for Throughput Prediction in Heterogeneous Multi-Hop p-CSMA Networks](https://arxiv.org/abs/2510.14137)
*Faezeh Dehghan Tarzjani,Bhaskar Krishnamachari*

Main category: cs.LG

TL;DR: 提出了一种新的图卷积网络（D-GCN），用于预测异构多跳无线网络中的饱和吞吐量，该网络优于现有方法，且计算可行。


<details>
  <summary>Details</summary>
Motivation: 现有模型在稀疏拓扑中低估吞吐量，精确的马尔可夫链分析计算量大，通用图神经网络(GNN)难以区分直接干扰和高阶干扰。

Method: 提出了解耦图卷积网络（D-GCN），它将节点自身的传输概率与邻居干扰效应的处理明确分离，并用可学习的注意力机制代替平均聚合。

Result: D-GCN 实现了 3.3% 的 NMAE，优于其他方法，即使在精确分析方法计算不可行时仍然易于处理，并实现了基于梯度的网络优化，达到了理论最优值的 1% 以内。

Conclusion: D-GCN 是一种有效的、可扩展的、可解释的架构，用于预测和优化异构无线网络中的吞吐量。

Abstract: The p-persistent CSMA protocol is central to random-access MAC analysis, but
predicting saturation throughput in heterogeneous multi-hop wireless networks
remains a hard problem. Simplified models that assume a single, shared
interference domain can underestimate throughput by 48--62\% in sparse
topologies. Exact Markov-chain analyses are accurate but scale exponentially in
computation time, making them impractical for large networks. These
computational barriers motivate structural machine learning approaches like
GNNs for scalable throughput prediction in general network topologies. Yet
off-the-shelf GNNs struggle here: a standard GCN yields 63.94\% normalized mean
absolute error (NMAE) on heterogeneous networks because symmetric normalization
conflates a node's direct interference with higher-order, cascading effects
that pertain to how interference propagates over the network graph.
  Building on these insights, we propose the Decoupled Graph Convolutional
Network (D-GCN), a novel architecture that explicitly separates processing of a
node's own transmission probability from neighbor interference effects. D-GCN
replaces mean aggregation with learnable attention, yielding interpretable,
per-neighbor contribution weights while capturing complex multihop interference
patterns. D-GCN attains 3.3\% NMAE, outperforms strong baselines, remains
tractable even when exact analytical methods become computationally infeasible,
and enables gradient-based network optimization that achieves within 1\% of
theoretical optima.

</details>


### [186] [Inferred global dense residue transition graphs from primary structure sequences enable protein interaction prediction via directed graph convolutional neural networks](https://arxiv.org/abs/2510.14139)
*Islam Akef Ebeid,Haoteng Tang,Pengfei Gu*

Main category: cs.LG

TL;DR: 提出了一种新的蛋白质-蛋白质相互作用（PPI）预测框架ProtGram-DirectGCN，该框架通过链接预测进行下游PPI预测。


<details>
  <summary>Details</summary>
Motivation: 现有的PPI预测方法计算密集，要么使用来自蛋白质语言模型的直接序列嵌入，要么使用图神经网络处理3D蛋白质结构。本研究探索计算强度较低的替代方案。

Method: 该框架包含两个阶段：ProtGram构建蛋白质n-gram图，其中残基转移概率定义边权重；DirectGCN是一种定制的有向图卷积神经网络，通过单独的路径特定转换（传入、传出和无向）处理信息，并使用可学习的门控机制组合这些路径。

Result: DirectGCN在节点分类基准测试中表现良好，尤其擅长处理复杂的、有向的、具有密集异嗜结构的图。ProtGram-DirectGCN框架在PPI预测中表现出强大的预测能力，即使在训练数据有限的情况下也是如此。

Conclusion: 提出的ProtGram-DirectGCN框架为PPI预测提供了一种有效的替代方案，尤其是在计算资源有限或训练数据较少的情况下。

Abstract: Introduction Accurate prediction of protein-protein interactions (PPIs) is
crucial for understanding cellular functions and advancing drug development.
Existing in-silico methods use direct sequence embeddings from Protein Language
Models (PLMs). Others use Graph Neural Networks (GNNs) for 3D protein
structures. This study explores less computationally intensive alternatives. We
introduce a novel framework for downstream PPI prediction through link
prediction. Methods We introduce a two-stage graph representation learning
framework, ProtGram-DirectGCN. First, we developed ProtGram. This approach
models a protein's primary structure as a hierarchy of globally inferred n-gram
graphs. In these graphs, residue transition probabilities define edge weights.
Each edge connects a pair of residues in a directed graph. The probabilities
are aggregated from a large corpus of sequences. Second, we propose DirectGCN,
a custom directed graph convolutional neural network. This model features a
unique convolutional layer. It processes information through separate
path-specific transformations: incoming, outgoing, and undirected. A shared
transformation is also applied. These paths are combined via a learnable gating
mechanism. We apply DirectGCN to ProtGram graphs to learn residue-level
embeddings. These embeddings are pooled via attention to generate protein-level
embeddings for prediction. Results We first established the efficacy of
DirectGCN on standard node classification benchmarks. Its performance matches
established methods on general datasets. The model excels at complex, directed
graphs with dense, heterophilic structures. When applied to PPI prediction, the
full ProtGram-DirectGCN framework delivers robust predictive power. This strong
performance holds even with limited training data.

</details>


### [187] [On Evaluating Loss Functions for Stock Ranking: An Empirical Analysis With Transformer Model](https://arxiv.org/abs/2510.14156)
*Jan Kwiatkowski,Jarosław A. Chudziak*

Main category: cs.LG

TL;DR: 该论文研究了不同的训练损失函数如何影响Transformer模型对股票进行有效排序，从而实现有利可图的投资。


<details>
  <summary>Details</summary>
Motivation: 金融市场具有不断变化的性质和股票之间复杂的关系，标准的损失函数通常不足以直接训练模型学习正确的股票收益排序。

Method: 该论文系统地评估了一系列先进的损失函数，包括点式、配对式、列表式，用于每日股票收益预测，以促进基于排名的投资组合选择。

Result: 该研究提供了一个综合基准，揭示了不同的损失函数如何影响模型学习横截面和时间模式的能力，这对于投资组合选择至关重要。

Conclusion: 该研究为优化基于排名的交易策略提供了实用的指导。

Abstract: Quantitative trading strategies rely on accurately ranking stocks to identify
profitable investments. Effective portfolio management requires models that can
reliably order future stock returns. Transformer models are promising for
understanding financial time series, but how different training loss functions
affect their ability to rank stocks well is not yet fully understood. Financial
markets are challenging due to their changing nature and complex relationships
between stocks. Standard loss functions, which aim for simple prediction
accuracy, often aren't enough. They don't directly teach models to learn the
correct order of stock returns. While many advanced ranking losses exist from
fields such as information retrieval, there hasn't been a thorough comparison
to see how well they work for ranking financial returns, especially when used
with modern Transformer models for stock selection. This paper addresses this
gap by systematically evaluating a diverse set of advanced loss functions
including pointwise, pairwise, listwise for daily stock return forecasting to
facilitate rank-based portfolio selection on S&P 500 data. We focus on
assessing how each loss function influences the model's ability to discern
profitable relative orderings among assets. Our research contributes a
comprehensive benchmark revealing how different loss functions impact a model's
ability to learn cross-sectional and temporal patterns crucial for portfolio
selection, thereby offering practical guidance for optimizing ranking-based
trading strategies.

</details>


### [188] [Data Understanding Survey: Pursuing Improved Dataset Characterization Via Tensor-based Methods](https://arxiv.org/abs/2510.14161)
*Matthew D. Merris,Tim Andersen*

Main category: cs.LG

TL;DR: 这篇论文探讨了传统数据集特征分析方法的局限性，并提倡使用张量方法来更有效地理解复杂数据集，从而实现更智能、可解释的数据驱动发现。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集特征分析方法（如统计、结构和基于模型的方法）常常无法提供创新和可解释性所需的深刻理解和洞察力。

Method: 该论文调研了当前最先进的传统数据分析技术，并分析了它们的局限性。此外，论文还讨论了各种基于张量的方法，以及它们如何为传统统计、结构和基于模型的数据集特征分析技术提供更强大的替代方案。

Result: 通过实例，论文展示了张量方法如何揭示细致的数据特征，从而提供增强的可解释性和可操作的智能。

Conclusion: 论文提倡采用基于张量的特征分析方法，认为这将有助于更深入地理解复杂数据集，并为智能、可解释的数据驱动发现铺平道路。

Abstract: In the evolving domains of Machine Learning and Data Analytics, existing
dataset characterization methods such as statistical, structural, and
model-based analyses often fail to deliver the deep understanding and insights
essential for innovation and explainability. This work surveys the current
state-of-the-art conventional data analytic techniques and examines their
limitations, and discusses a variety of tensor-based methods and how these may
provide a more robust alternative to traditional statistical, structural, and
model-based dataset characterization techniques. Through examples, we
illustrate how tensor methods unveil nuanced data characteristics, offering
enhanced interpretability and actionable intelligence. We advocate for the
adoption of tensor-based characterization, promising a leap forward in
understanding complex datasets and paving the way for intelligent, explainable
data-driven discoveries.

</details>


### [189] [Towards Reversible Model Merging For Low-rank Weights](https://arxiv.org/abs/2510.14163)
*Mohammadsajad Alipour,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: 提出了一种新的模型合并方法，用于解决将压缩模型合并时性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的模型合并方法在应用于低秩表示的模型时，性能会显著下降。

Method: 提出了一种可逆模型合并（RMM）方法，该方法构建一个紧凑的基，原始任务特定的模型可以通过线性组合从中恢复。

Result: RMM 在各种数据集和模型规模上始终优于现有的合并方法，并显著保留了低秩压缩模型的性能。

Conclusion: RMM 是一种高效、无数据且灵活的方法，为选择模型权重的最佳基础和线性组合的任务特定系数提供了封闭形式的解决方案。

Abstract: Model merging aims to combine multiple fine-tuned models into a single set of
weights that performs well across all source tasks. While prior work has shown
that merging can approximate the performance of individual fine-tuned models
for each task, it largely overlooks scenarios where models are compressed into
low-rank representations, either through low-rank adaptation (LoRA) or
post-training singular value decomposition (SVD). We first demonstrate that
applying conventional merging methods to low-rank weights leads to severe
performance degradation in the merged model. Motivated by this phenomenon, we
propose a fundamentally different approach: instead of collapsing all adapters
into one set of weights, we construct a compact basis (e.g., an equivalent of
holding two or more models) from which original task-specific models can be
recovered via linear combination. This reframes merging as generating a
reconstruction-capable model space rather than producing a single merged model.
Crucially, this allows us to ``revert'' to each individual model when needed,
recognizing that no merged model can consistently outperform one specialized
for its task. Building on this insight, we introduce our method, Reversible
Model Merging (RMM), an efficient, data-free, and flexible method that provides
a closed-form solution for selecting the optimal basis of model weights and
task-specific coefficients for linear combination. Extensive experiments across
diverse datasets and model scales demonstrate that RMM consistently outperforms
existing merging approaches, preserving the performance of low-rank compressed
models by a significant margin.

</details>


### [190] [Optimal Control Theoretic Neural Optimizer: From Backpropagation to Dynamic Programming](https://arxiv.org/abs/2510.14168)
*Guan-Horng Liu,Tianrong Chen,Evangelos A. Theodorou*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于最优控制理论的神经网络优化器OCNOpt，该优化器在鲁棒性和效率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 将DNNs视为动态系统，并利用最优控制编程的框架来确定其最优参数。

Method: 通过将反向传播算法与动态规划的最优性条件联系起来，导出了OCNOpt优化器。该优化器探索了贝尔曼方程的高阶展开。

Result: 实验表明，OCNOpt在鲁棒性和效率方面优于现有方法，同时保持了可管理的计算复杂性。

Conclusion: OCNOpt为基于动态系统和最优控制理论的算法设计开辟了新途径。

Abstract: Optimization of deep neural networks (DNNs) has been a driving force in the
advancement of modern machine learning and artificial intelligence. With DNNs
characterized by a prolonged sequence of nonlinear propagation, determining
their optimal parameters given an objective naturally fits within the framework
of Optimal Control Programming. Such an interpretation of DNNs as dynamical
systems has proven crucial in offering a theoretical foundation for principled
analysis from numerical equations to physics. In parallel to these theoretical
pursuits, this paper focuses on an algorithmic perspective. Our motivated
observation is the striking algorithmic resemblance between the Backpropagation
algorithm for computing gradients in DNNs and the optimality conditions for
dynamical systems, expressed through another backward process known as dynamic
programming. Consolidating this connection, where Backpropagation admits a
variational structure, solving an approximate dynamic programming up to the
first-order expansion leads to a new class of optimization methods exploring
higher-order expansions of the Bellman equation. The resulting optimizer,
termed Optimal Control Theoretic Neural Optimizer (OCNOpt), enables rich
algorithmic opportunities, including layer-wise feedback policies,
game-theoretic applications, and higher-order training of continuous-time
models such as Neural ODEs. Extensive experiments demonstrate that OCNOpt
improves upon existing methods in robustness and efficiency while maintaining
manageable computational complexity, paving new avenues for principled
algorithmic design grounded in dynamical systems and optimal control theory.

</details>


### [191] [MAFA: A Multi-Agent Framework for Enterprise-Scale Annotation with Configurable Task Adaptation](https://arxiv.org/abs/2510.14184)
*Mahmood Hegazy,Aaron Rodrigues,Azzam Naeem*

Main category: cs.LG

TL;DR: MAFA是一个多智能体标注框架，已在摩根大通部署，用于解决金融服务中大量客户话语的标注积压问题。


<details>
  <summary>Details</summary>
Motivation: 解决金融服务中数百万客户话语需要准确分类的标注积压这一关键挑战。

Method: 结合了专门的智能体、结构化推理和基于判断的共识机制。该框架支持动态任务调整，允许组织通过配置而不是代码更改来定义自定义标注类型。

Result: 消除了一百万条话语的积压，同时平均达到 86% 的人工标注者一致性，每年节省超过 5,000 小时的人工标注工作。在内部意图分类数据集中，Top-1 准确率提高了 13.8%，Top-5 准确率提高了 15.1%，F1 提高了 16.9%，并且在公共基准测试中获得了类似的收益。

Conclusion: 弥合了理论多智能体系统与实际企业部署之间的差距，为面临类似标注挑战的组织提供了一个蓝图。

Abstract: We present MAFA (Multi-Agent Framework for Annotation), a production-deployed
system that transforms enterprise-scale annotation workflows through
configurable multi-agent collaboration. Addressing the critical challenge of
annotation backlogs in financial services, where millions of customer
utterances require accurate categorization, MAFA combines specialized agents
with structured reasoning and a judge-based consensus mechanism. Our framework
uniquely supports dynamic task adaptation, allowing organizations to define
custom annotation types (FAQs, intents, entities, or domain-specific
categories) through configuration rather than code changes. Deployed at JP
Morgan Chase, MAFA has eliminated a 1 million utterance backlog while
achieving, on average, 86% agreement with human annotators, annually saving
over 5,000 hours of manual annotation work. The system processes utterances
with annotation confidence classifications, which are typically 85% high, 10%
medium, and 5% low across all datasets we tested. This enables human annotators
to focus exclusively on ambiguous and low-coverage cases. We demonstrate MAFA's
effectiveness across multiple datasets and languages, showing consistent
improvements over traditional and single-agent annotation baselines: 13.8%
higher Top-1 accuracy, 15.1% improvement in Top-5 accuracy, and 16.9% better F1
in our internal intent classification dataset and similar gains on public
benchmarks. This work bridges the gap between theoretical multi-agent systems
and practical enterprise deployment, providing a blueprint for organizations
facing similar annotation challenges.

</details>


### [192] [Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation](https://arxiv.org/abs/2510.14190)
*Ruchi Sandilya,Sumaira Perez,Charles Lynch,Lindsay Victoria,Benjamin Zebley,Derrick Matthew Buchanan,Mahendra T. Bhati,Nolan Williams,Timothy J. Spellman,Faith M. Gunning,Conor Liston,Logan Grosenick*

Main category: cs.LG

TL;DR: ConDA: Contrastive Diffusion Alignment, uses contrastive learning in diffusion embeddings to align latent geometry with system dynamics, enabling better control.


<details>
  <summary>Details</summary>
Motivation: Diffusion model latent spaces lack interpretable organization for control, contrastive learning can disentangle representations.

Method: Applies contrastive learning within diffusion embeddings to structure the latent space.

Result: Improved controllability in fluid dynamics, neural calcium imaging, therapeutic neurostimulation, and facial expression benchmarks.

Conclusion: Diffusion latents encode dynamics-relevant structure, but require latent organization and manifold traversal.

Abstract: Diffusion models excel at generation, but their latent spaces are not
explicitly organized for interpretable control. We introduce ConDA (Contrastive
Diffusion Alignment), a framework that applies contrastive learning within
diffusion embeddings to align latent geometry with system dynamics. Motivated
by recent advances showing that contrastive objectives can recover more
disentangled and structured representations, ConDA organizes diffusion latents
such that traversal directions reflect underlying dynamical factors. Within
this contrastively structured space, ConDA enables nonlinear trajectory
traversal that supports faithful interpolation, extrapolation, and controllable
generation. Across benchmarks in fluid dynamics, neural calcium imaging,
therapeutic neurostimulation, and facial expression, ConDA produces
interpretable latent representations with improved controllability compared to
linear traversals and conditioning-based baselines. These results suggest that
diffusion latents encode dynamics-relevant structure, but exploiting this
structure requires latent organization and traversal along the latent manifold.

</details>


### [193] [Incentive-Based Federated Learning](https://arxiv.org/abs/2510.14208)
*Chanuka A. S. Hewa Kaluannakkage,Rajkumar Buyya*

Main category: cs.LG

TL;DR: 联邦学习通过协作模型训练且不损害数据隐私来变革机器学习。但参与激励不足和参与者可能搭便车限制了实际适应性。本文探讨了联邦学习激励机制设计的挑战，考察了经济学、博弈论、区块链和深度强化学习的应用，提出了一个全面的分类法，涵盖了基于上述理论概念的中心化和去中心化架构。从应用角度介绍了医疗、智能基础设施、车辆网络和基于区块链的去中心化系统等新兴工业应用。论证了激励机制对于联邦学习的实际成功至关重要，并揭示了有前景的解决方案和构建可持续、公平和强大的联邦学习生态系统方面仍然存在的重大挑战。


<details>
  <summary>Details</summary>
Motivation: 参与者不愿无偿贡献或可能搭便车，需要激励机制。

Method: 应用经济学、博弈论、区块链和深度强化学习等理论，构建中心化和去中心化架构的分类法。

Result: 强调了精心设计的激励机制对于联邦学习的实际成功至关重要。

Conclusion: 揭示了联邦学习生态系统中已出现的有前景的解决方案和仍然存在的重大挑战，强调了构建可持续、公平和强大的联邦学习生态系统的重要性。

Abstract: Federated learning promises to revolutionize machine learning by enabling
collaborative model training without compromising data privacy. However,
practical adaptability can be limited by critical factors, such as the
participation dilemma. Participating entities are often unwilling to contribute
to a learning system unless they receive some benefits, or they may pretend to
participate and free-ride on others. This chapter identifies the fundamental
challenges in designing incentive mechanisms for federated learning systems. It
examines how foundational concepts from economics and game theory can be
applied to federated learning, alongside technology-driven solutions such as
blockchain and deep reinforcement learning. This work presents a comprehensive
taxonomy that thoroughly covers both centralized and decentralized
architectures based on the aforementioned theoretical concepts. Furthermore,
the concepts described are presented from an application perspective, covering
emerging industrial applications, including healthcare, smart infrastructure,
vehicular networks, and blockchain-based decentralized systems. Through this
exploration, this chapter demonstrates that well-designed incentive mechanisms
are not merely optional features but essential components for the practical
success of federated learning. This analysis reveals both the promising
solutions that have emerged and the significant challenges that remain in
building truly sustainable, fair, and robust federated learning ecosystems.

</details>


### [194] [Spectral Analysis of Molecular Kernels: When Richer Features Do Not Guarantee Better Generalization](https://arxiv.org/abs/2510.14217)
*Asma Jamali,Tin Sum Cheng,Rodrigo A. Vargas-Hernández*

Main category: cs.LG

TL;DR: 本文对分子核的谱性质进行了全面的分析，发现更丰富的谱特征并不一定能提高预测精度，有时甚至会产生负相关。


<details>
  <summary>Details</summary>
Motivation: 了解核的谱性质有助于从原理上理解泛化和表示质量。尽管深度模型在分子性质预测方面取得了最先进的精度，但核方法因其在低数据环境中的鲁棒性和透明的理论基础而被广泛使用。然而，对分子核的系统谱分析仍然稀缺。

Method: 本文对QM9数据集上的核岭回归进行了首次全面的谱分析，使用了分子指纹、预训练的基于Transformer的模型、全局和局部3D表示，并涵盖了七种分子性质。同时，本文还实现了截断核，以探究谱与预测性能之间的关系。

Result: 研究结果表明，更丰富的谱特征并不一定能提高预测精度，对于基于Transformer的模型和局部3D表示，谱的丰富性甚至可能与性能产生负相关。此外，在许多核中，仅保留前2%的特征值即可恢复几乎所有性能，表明主要的特征值捕获了最丰富的信息。

Conclusion: 研究结果挑战了“更丰富的谱产生更好的泛化”这一常见观点，并强调了表示、核特征和预测性能之间细微的关系。这些发现不仅为分子性质预测提供了信息，也为在数据有限的科学和实际任务中评估核和自监督学习方法提供了参考。

Abstract: Understanding the spectral properties of kernels offers a principled
perspective on generalization and representation quality. While deep models
achieve state-of-the-art accuracy in molecular property prediction, kernel
methods remain widely used for their robustness in low-data regimes and
transparent theoretical grounding. Despite extensive studies of kernel spectra
in machine learning, systematic spectral analyses of molecular kernels are
scarce. In this work, we provide the first comprehensive spectral analysis of
kernel ridge regression on the QM9 dataset, molecular fingerprint, pretrained
transformer-based, global and local 3D representations across seven molecular
properties. Surprisingly, richer spectral features, measured by four different
spectral metrics, do not consistently improve accuracy. Pearson correlation
tests further reveal that for transformer-based and local 3D representations,
spectral richness can even have a negative correlation with performance. We
also implement truncated kernels to probe the relationship between spectrum and
predictive performance: in many kernels, retaining only the top 2% of
eigenvalues recovers nearly all performance, indicating that the leading
eigenvalues capture the most informative features. Our results challenge the
common heuristic that "richer spectra yield better generalization" and
highlight nuanced relationships between representation, kernel features, and
predictive performance. Beyond molecular property prediction, these findings
inform how kernel and self-supervised learning methods are evaluated in
data-limited scientific and real-world tasks.

</details>


### [195] [When Flatness Does (Not) Guarantee Adversarial Robustness](https://arxiv.org/abs/2510.14231)
*Nils Philipp Walter,Linara Adilova,Jilles Vreeken,Michael Kamp*

Main category: cs.LG

TL;DR: 本文研究了神经网络中的平坦最小值与对抗鲁棒性之间的关系。


<details>
  <summary>Details</summary>
Motivation: 神经网络容易受到微小的对抗扰动的影响，一个长期存在的假设是平坦最小值可以提高鲁棒性。

Method: 通过严格的形式化关系，推导了倒数第二层中相对平坦度的闭式表达式，并用它来约束输入空间中损失的变化。

Result: 平坦性仅意味着局部而非全局的对抗鲁棒性。为了保持超出局部邻域的鲁棒性，损失需要从数据流形中急剧弯曲。

Conclusion: 对抗性样本通常位于模型错误的大而平坦的区域中。

Abstract: Despite their empirical success, neural networks remain vulnerable to small,
adversarial perturbations. A longstanding hypothesis suggests that flat minima,
regions of low curvature in the loss landscape, offer increased robustness.
While intuitive, this connection has remained largely informal and incomplete.
By rigorously formalizing the relationship, we show this intuition is only
partially correct: flatness implies local but not global adversarial
robustness. To arrive at this result, we first derive a closed-form expression
for relative flatness in the penultimate layer, and then show we can use this
to constrain the variation of the loss in input space. This allows us to
formally analyze the adversarial robustness of the entire network. We then show
that to maintain robustness beyond a local neighborhood, the loss needs to
curve sharply away from the data manifold. We validate our theoretical
predictions empirically across architectures and datasets, uncovering the
geometric structure that governs adversarial vulnerability, and linking
flatness to model confidence: adversarial examples often lie in large, flat
regions where the model is confidently wrong. Our results challenge simplified
views of flatness and provide a nuanced understanding of its role in
robustness.

</details>


### [196] [Scaling Test-Time Compute to Achieve IOI Gold Medal with Open-Weight Models](https://arxiv.org/abs/2510.14232)
*Mehrzad Samadi,Aleksander Ficek,Sean Narenthiran,Siddhartha Jain,Wasi Uddin Ahmad,Somshubra Majumdar,Vahid Noroozi,Boris Ginsburg*

Main category: cs.LG

TL;DR: 本研究提出了一种名为 GenCluster 的可扩展且可复现的测试时计算框架，该框架使用开放权重模型达到了国际信息学奥林匹克竞赛 (IOI) 金牌水平。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型 (LLM) 的推理和解决问题能力，并使用开放权重模型在 IOI 中达到与专有模型相媲美的性能，这是一个重大挑战。

Method: 该框架结合了大规模生成、行为聚类、排名和循环提交策略，以在有限的验证预算下有效地探索不同的解决方案空间。

Result: 实验表明，该方法的性能随可用计算资源而持续扩展，缩小了开放和封闭系统之间的差距。GenCluster 首次使用开放权重模型 gpt-oss-120b 在 IOI 2025 上获得金牌，为 LLM 推理的透明和可重复评估设定了新的基准。

Conclusion: GenCluster 能够使用开放权重模型达到 IOI 金牌水平，为 LLM 的推理能力评估开辟了新的途径。

Abstract: Competitive programming has become a rigorous benchmark for evaluating the
reasoning and problem-solving capabilities of large language models (LLMs). The
International Olympiad in Informatics (IOI) stands out as one of the most
prestigious annual competitions in competitive programming and has become a key
benchmark for comparing human and AI-level programming ability. While several
proprietary models have been claimed to achieve gold medal-level performance at
the IOI, often with undisclosed methods, achieving comparable results with
open-weight models remains a significant challenge. In this paper, we present
\gencluster, a scalable and reproducible test-time compute framework that
attains IOI gold-level performance using open-weight models. It combines
large-scale generation, behavioral clustering, ranking, and a round-robin
submission strategy to efficiently explore diverse solution spaces under
limited validation budgets. Our experiments show that the performance of our
proposed approach scales consistently with available compute, narrowing the gap
between open and closed systems. Notably, we will show that GenCluster can
achieve a gold medal at IOI 2025 for the first time with an open-weight model
gpt-oss-120b, setting a new benchmark for transparent and reproducible
evaluation of reasoning in LLMs.

</details>


### [197] [Policy Regularized Distributionally Robust Markov Decision Processes with Linear Function Approximation](https://arxiv.org/abs/2510.14246)
*Jingwen Gu,Yiting He,Zhishuai Liu,Pan Xu*

Main category: cs.LG

TL;DR: 本文提出了一种名为DR-RPO的无模型在线策略优化方法，用于解决强化学习中分布偏移下的决策问题，该方法通过参考策略正则化和线性函数逼近来实现鲁棒策略的学习和优化。


<details>
  <summary>Details</summary>
Motivation: 研究强化学习中训练和部署环境不同的分布偏移问题，并关注在线设置下样本效率和探索的重要性。策略优化在鲁棒强化学习中的理论和实践探索不足。

Method: 提出DR-RPO算法，结合参考策略正则化、d-rectangular线性MDP公式、线性函数逼近和置信上界奖励。

Result: 理论证明策略优化可以在鲁棒强化学习中实现多项式次优界限和样本效率，实验结果验证了DR-RPO的鲁棒性。

Conclusion: DR-RPO算法在鲁棒强化学习中能够有效地学习和优化策略，并在不同领域取得了良好的实验结果。

Abstract: Decision-making under distribution shift is a central challenge in
reinforcement learning (RL), where training and deployment environments differ.
We study this problem through the lens of robust Markov decision processes
(RMDPs), which optimize performance against adversarial transition dynamics.
Our focus is the online setting, where the agent has only limited interaction
with the environment, making sample efficiency and exploration especially
critical. Policy optimization, despite its success in standard RL, remains
theoretically and empirically underexplored in robust RL. To bridge this gap,
we propose \textbf{D}istributionally \textbf{R}obust \textbf{R}egularized
\textbf{P}olicy \textbf{O}ptimization algorithm (DR-RPO), a model-free online
policy optimization method that learns robust policies with sublinear regret.
To enable tractable optimization within the softmax policy class, DR-RPO
incorporates reference-policy regularization, yielding RMDP variants that are
doubly constrained in both transitions and policies. To scale to large
state-action spaces, we adopt the $d$-rectangular linear MDP formulation and
combine linear function approximation with an upper confidence bonus for
optimistic exploration. We provide theoretical guarantees showing that policy
optimization can achieve polynomial suboptimality bounds and sample efficiency
in robust RL, matching the performance of value-based approaches. Finally,
empirical results across diverse domains corroborate our theory and demonstrate
the robustness of DR-RPO.

</details>


### [198] [A Physics Prior-Guided Dual-Stream Attention Network for Motion Prediction of Elastic Bragg Breakwaters](https://arxiv.org/abs/2510.14250)
*Lianzi Jiang,Jianxin Zhang,Xinyu Han,Huanhe Dong,Xiangrong Wang*

Main category: cs.LG

TL;DR: 提出了一种新的深度学习模型PhysAttnNet，用于预测弹性布拉格防波堤的运动响应，该模型在未见过的海况下表现出更强的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的深度学习模型在处理未见过的海况时泛化能力有限，因为它们忽略了海洋系统中的自然衰减以及对波浪-结构相互作用（WSI）建模不足。

Method: 该研究提出了一种新颖的物理先验引导的双流注意力网络（PhysAttnNet）。该网络包含衰减双向自注意力（DBSA）模块和相位差引导的双向交叉注意力（PDG-BCA）模块，并通过全局上下文融合（GCF）模块进行整合。此外，使用混合时频损失训练PhysAttnNet。

Result: 在波浪水槽数据集上的综合实验表明，PhysAttnNet 明显优于主流模型。跨场景泛化测试验证了该模型的鲁棒性和对未见环境的适应性。

Conclusion: PhysAttnNet 有潜力成为开发海洋工程中复杂系统预测模型的框架。

Abstract: Accurate motion response prediction for elastic Bragg breakwaters is critical
for their structural safety and operational integrity in marine environments.
However, conventional deep learning models often exhibit limited generalization
capabilities when presented with unseen sea states. These deficiencies stem
from the neglect of natural decay observed in marine systems and inadequate
modeling of wave-structure interaction (WSI). To overcome these challenges,
this study proposes a novel Physics Prior-Guided Dual-Stream Attention Network
(PhysAttnNet). First, the decay bidirectional self-attention (DBSA) module
incorporates a learnable temporal decay to assign higher weights to recent
states, aiming to emulate the natural decay phenomenon. Meanwhile, the phase
differences guided bidirectional cross-attention (PDG-BCA) module explicitly
captures the bidirectional interaction and phase relationship between waves and
the structure using a cosine-based bias within a bidirectional
cross-computation paradigm. These streams are synergistically integrated
through a global context fusion (GCF) module. Finally, PhysAttnNet is trained
with a hybrid time-frequency loss that jointly minimizes time-domain prediction
errors and frequency-domain spectral discrepancies. Comprehensive experiments
on wave flume datasets demonstrate that PhysAttnNet significantly outperforms
mainstream models. Furthermore,cross-scenario generalization tests validate the
model's robustness and adaptability to unseen environments, highlighting its
potential as a framework to develop predictive models for complex systems in
ocean engineering.

</details>


### [199] [Generalist vs Specialist Time Series Foundation Models: Investigating Potential Emergent Behaviors in Assessing Human Health Using PPG Signals](https://arxiv.org/abs/2510.14254)
*Saurabh Kataria,Yi Wu,Zhaoliang Chen,Hyunjung Gloria Kwak,Yuhao Xu,Lovely Yeswanth Panchumarthi,Ran Xiao,Jiaying Lu,Ayca Ermis,Anni Zhao,Runze Yan,Alex Federov,Zewen Liu,Xu Wu,Wei Jin,Carl Yang,Jocelyn Grunwell,Stephanie R. Brown,Amit Shah,Craig Jabaley,Tim Buchman,Sivasubramanium V Bhavani,Randall J. Lee,Xiao Hu*

Main category: cs.LG

TL;DR: 本研究对比了通用和专用时间序列基础模型在光电容积脉搏波 (PPG) 信号上的性能。


<details>
  <summary>Details</summary>
Motivation: 评估通用时间序列模型和专用模型在生理传感领域的优劣。

Method: 通过包含 51 项任务的综合基准测试，在心脏状态评估、实验室数值估计和跨模态推理等方面评估模型。

Result: 在全参数微调的情况下，专用模型的胜率高出 27%。

Conclusion: 对泛化性、公平性、注意力可视化以及训练数据选择的重要性进行了进一步分析。

Abstract: Foundation models are large-scale machine learning models that are
pre-trained on massive amounts of data and can be adapted for various
downstream tasks. They have been extensively applied to tasks in Natural
Language Processing and Computer Vision with models such as GPT, BERT, and
CLIP. They are now also increasingly gaining attention in time-series analysis,
particularly for physiological sensing. However, most time series foundation
models are specialist models - with data in pre-training and testing of the
same type, such as Electrocardiogram, Electroencephalogram, and
Photoplethysmogram (PPG). Recent works, such as MOMENT, train a generalist time
series foundation model with data from multiple domains, such as weather,
traffic, and electricity. This paper aims to conduct a comprehensive
benchmarking study to compare the performance of generalist and specialist
models, with a focus on PPG signals. Through an extensive suite of total 51
tasks covering cardiac state assessment, laboratory value estimation, and
cross-modal inference, we comprehensively evaluate both models across seven
dimensions, including win score, average performance, feature quality, tuning
gain, performance variance, transferability, and scalability. These metrics
jointly capture not only the models' capability but also their adaptability,
robustness, and efficiency under different fine-tuning strategies, providing a
holistic understanding of their strengths and limitations for diverse
downstream scenarios. In a full-tuning scenario, we demonstrate that the
specialist model achieves a 27% higher win score. Finally, we provide further
analysis on generalization, fairness, attention visualizations, and the
importance of training data choice.

</details>


### [200] [CAST: Compositional Analysis via Spectral Tracking for Understanding Transformer Layer Functions](https://arxiv.org/abs/2510.14262)
*Zihao Fu,Ming Liao,Chris Russell,Zhenguang G. Cai*

Main category: cs.LG

TL;DR: 这篇论文介绍了一种名为CAST的新框架，用于分析transformer层的功能，通过直接转换矩阵估计和综合频谱分析。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型取得了显著的成功，但其内部机制仍然很大程度上是黑盒，人们对其理解不足。

Method: 该方法通过Moore-Penrose伪逆估计每一层的实现转换矩阵，并应用具有六个可解释指标的频谱分析来表征层行为。

Result: 分析揭示了仅编码器和仅解码器模型之间的不同行为，解码器模型表现出压缩-扩展周期，而编码器模型保持一致的高秩处理。

Conclusion: Kernel分析进一步证明了层之间的功能关系模式，CKA相似性矩阵将层清楚地划分为三个阶段：特征提取、压缩和专业化。

Abstract: Large language models have achieved remarkable success but remain largely
black boxes with poorly understood internal mechanisms. To address this
limitation, many researchers have proposed various interpretability methods
including mechanistic analysis, probing classifiers, and activation
visualization, each providing valuable insights from different perspectives.
Building upon this rich landscape of complementary approaches, we introduce
CAST (Compositional Analysis via Spectral Tracking), a probe-free framework
that contributes a novel perspective by analyzing transformer layer functions
through direct transformation matrix estimation and comprehensive spectral
analysis. CAST offers complementary insights to existing methods by estimating
the realized transformation matrices for each layer using Moore-Penrose
pseudoinverse and applying spectral analysis with six interpretable metrics
characterizing layer behavior. Our analysis reveals distinct behaviors between
encoder-only and decoder-only models, with decoder models exhibiting
compression-expansion cycles while encoder models maintain consistent high-rank
processing. Kernel analysis further demonstrates functional relationship
patterns between layers, with CKA similarity matrices clearly partitioning
layers into three phases: feature extraction, compression, and specialization.

</details>


### [201] [Nonparametric Data Attribution for Diffusion Models](https://arxiv.org/abs/2510.14269)
*Yutian Zhao,Chao Du,Xiaosen Zheng,Tianyu Pang,Min Lin*

Main category: cs.LG

TL;DR: 提出了一种新的非参数数据归因方法，该方法通过测量生成图像和训练图像之间的patch-level相似性来评估训练样本对模型输出的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型归因方法通常需要访问模型梯度或重新训练，限制了它们在专有或大规模环境中的应用。

Method: 该方法完全基于数据，利用生成图像和训练图像之间的patch-level相似性，并结合最优score函数的解析形式和多尺度表示，通过卷积加速计算。

Result: 实验表明，该方法实现了强大的归因性能，与基于梯度的方法相匹配，并且明显优于现有的非参数基线方法。

Conclusion: 该框架揭示了训练数据和输出之间内在关系的模式，独立于任何特定模型，并且能够产生空间可解释的归因。

Abstract: Data attribution for generative models seeks to quantify the influence of
individual training examples on model outputs. Existing methods for diffusion
models typically require access to model gradients or retraining, limiting
their applicability in proprietary or large-scale settings. We propose a
nonparametric attribution method that operates entirely on data, measuring
influence via patch-level similarity between generated and training images. Our
approach is grounded in the analytical form of the optimal score function and
naturally extends to multiscale representations, while remaining
computationally efficient through convolution-based acceleration. In addition
to producing spatially interpretable attributions, our framework uncovers
patterns that reflect intrinsic relationships between training data and
outputs, independent of any specific model. Experiments demonstrate that our
method achieves strong attribution performance, closely matching gradient-based
approaches and substantially outperforming existing nonparametric baselines.
Code is available at https://github.com/sail-sg/NDA.

</details>


### [202] [Stable Prediction of Adverse Events in Medical Time-Series Data](https://arxiv.org/abs/2510.14286)
*Mayank Keoliya,Seewon Choi,Rajeev Alur,Mayur Naik,Eric Wong*

Main category: cs.LG

TL;DR: 本文提出了CAREBench，一个用于评估早期事件预测（EEP）系统在临床决策支持中部署能力的基准。


<details>
  <summary>Details</summary>
Motivation: 当前基准忽略了风险评分的稳定性，并且主要在表格输入上进行评估，没有测试轨迹行为。为了解决这个问题，本文引入了CAREBench。

Method: CAREBench使用多模态输入（表格EHR、ECG波形和临床文本）评估部署能力，并评估时间稳定性以及预测准确性。本文提出了一种稳定性指标，该指标量化了每位患者风险的短期变异性，并根据局部Lipschitz常数惩罚了突然的振荡。

Result: 现有方法，特别是LLM，难以同时优化准确性和稳定性，在高精度操作点上的召回率尤其差。

Conclusion: 这些结果表明，需要产生与证据一致的稳定轨迹的模型，以赢得临床医生在连续监测环境中的信任。

Abstract: Early event prediction (EEP) systems continuously estimate a patient's
imminent risk to support clinical decision-making. For bedside trust, risk
trajectories must be accurate and temporally stable, shifting only with new,
relevant evidence. However, current benchmarks (a) ignore stability of risk
scores and (b) evaluate mainly on tabular inputs, leaving trajectory behavior
untested. To address this gap, we introduce CAREBench, an EEP benchmark that
evaluates deployability using multi-modal inputs-tabular EHR, ECG waveforms,
and clinical text-and assesses temporal stability alongside predictive
accuracy. We propose a stability metric that quantifies short-term variability
in per-patient risk and penalizes abrupt oscillations based on local-Lipschitz
constants. CAREBench spans six prediction tasks such as sepsis onset and
compares classical learners, deep sequence models, and zero-shot LLMs. Across
tasks, existing methods, especially LLMs, struggle to jointly optimize accuracy
and stability, with notably poor recall at high-precision operating points.
These results highlight the need for models that produce evidence-aligned,
stable trajectories to earn clinician trust in continuous monitoring settings.
(Code: https://github.com/SeewonChoi/CAREBench.)

</details>


### [203] [Enhancing Time-Series Anomaly Detection by Integrating Spectral-Residual Bottom-Up Attention with Reservoir Computing](https://arxiv.org/abs/2510.14287)
*Hayato Nihei,Sou Nobukawa,Yusuke Sakemi,Kazuyuki Aihara*

Main category: cs.LG

TL;DR: 提出了一种结合谱残差 (SR) 方法和 RC 的谱残差 RC (SR-RC)，以提高 RC 的异常检测性能，且不牺牲学习效率。


<details>
  <summary>Details</summary>
Motivation: 单独使用 RC 实现足够的异常检测性能可能需要在资源受限的边缘设备上使用过大的储层。虽然注意力机制可以提高准确性，但它们可能需要大量的计算并破坏 RC 的学习效率。为了提高 RC 的异常检测性能，且不牺牲学习效率。

Method: 将无学习、自下而上的注意力机制——谱残差 (SR) 方法与 RC 相结合，提出了谱残差 RC (SR-RC)。

Result: SR-RC 在基准任务和真实世界的时间序列数据集上优于传统的 RC 和基于 SR 方法提取的值的逻辑回归模型。

Conclusion: SR-RC 为将 RC 部署为用于时间序列异常检测的 Edge AI 提供了一个实际的方向，因为它和 RC 一样，非常适合硬件实现。

Abstract: Reservoir computing (RC) establishes the basis for the processing of
time-series data by exploiting the high-dimensional spatiotemporal response of
a recurrent neural network to an input signal. In particular, RC trains only
the output layer weights. This simplicity has drawn attention especially in
Edge Artificial Intelligence (AI) applications. Edge AI enables time-series
anomaly detection in real time, which is important because detection delays can
lead to serious incidents. However, achieving adequate anomaly-detection
performance with RC alone may require an unacceptably large reservoir on
resource-constrained edge devices. Without enlarging the reservoir, attention
mechanisms can improve accuracy, although they may require substantial
computation and undermine the learning efficiency of RC. In this study, to
improve the anomaly detection performance of RC without sacrificing learning
efficiency, we propose a spectral residual RC (SR-RC) that integrates the
spectral residual (SR) method - a learning-free, bottom-up attention mechanism
- with RC. We demonstrated that SR-RC outperformed conventional RC and
logistic-regression models based on values extracted by the SR method across
benchmark tasks and real-world time-series datasets. Moreover, because the SR
method, similarly to RC, is well suited for hardware implementation, SR-RC
suggests a practical direction for deploying RC as Edge AI for time-series
anomaly detection.

</details>
