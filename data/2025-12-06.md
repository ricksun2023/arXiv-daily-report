<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 32]
- [cs.CV](#cs.CV) [Total: 38]
- [cs.AI](#cs.AI) [Total: 37]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.LG](#cs.LG) [Total: 38]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral](https://arxiv.org/abs/2512.04220)
*Wenlong Deng,Yushu Li,Boying Gong,Yi Ren,Christos Thrampoulidis,Xiaoxiao Li*

Main category: cs.CL

TL;DR: GRPO在工具集成强化学习中训练崩溃，原因是Lazy Likelihood Displacement (LLD) 导致的likelihood下降。提出了LLDS正则化方法，缓解LLD，稳定训练，提高性能。


<details>
  <summary>Details</summary>
Motivation: GRPO具有快速收敛和无价值公式的优点，但在工具集成强化学习中经常发生训练崩溃。本文旨在研究其崩溃的原因并提出解决方案。

Method: 通过实验分析，揭示了LLD是导致GRPO崩溃的核心机制。提出了轻量级的LLDS正则化方法，该方法仅在轨迹的可能性降低时激活，并仅对负责的token进行正则化。

Result: 在七个开放域和多跳QA基准测试中，LLDS稳定了训练，防止了梯度爆炸，并带来了显着的性能提升，例如在Qwen2.5-3B上获得了+37.8%的收益，在Qwen2.5-7B上获得了+32.0%的收益。

Conclusion: LLD是基于GRPO的TIRL中的一个根本瓶颈，LLDS为工具集成LLM的稳定、可扩展训练提供了一条可行的途径。

Abstract: Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.

</details>


### [2] [Computational Linguistics Meets Libyan Dialect: A Study on Dialect Identification](https://arxiv.org/abs/2512.04257)
*Mansour Essgaer,Khamis Massud,Rabia Al Mamlook,Najah Ghmaid*

Main category: cs.CL

TL;DR: 本研究利用logistic回归、线性支持向量机、多项式朴素贝叶斯和伯努利朴素贝叶斯对来自Twitter的利比亚方言话语进行分类，使用QADI语料库，包含18种阿拉伯方言的54万个句子。结果表明，多项式朴素贝叶斯在使用(1,2)词n-gram和(1,5)字符n-gram表示时，准确率最高，为85.89%。


<details>
  <summary>Details</summary>
Motivation: 研究利比亚方言文本分类，解决阿拉伯方言NLP应用中的问题。

Method: 使用logistic回归、线性支持向量机、多项式朴素贝叶斯和伯努利朴素贝叶斯等方法，结合卡方分析进行特征选择，并评估不同词和字符n-gram表示下的分类器性能。

Result: 多项式朴素贝叶斯(MNB)在使用(1,2)词n-gram和(1,5)字符n-gram表示时，达到了85.89%的最高准确率和0.85741的F1分数。

Conclusion: 精心选择的n-gram表示和分类模型在提高利比亚方言识别的准确性方面起着关键作用。

Abstract: This study investigates logistic regression, linear support vector machine, multinomial Naive Bayes, and Bernoulli Naive Bayes for classifying Libyan dialect utterances gathered from Twitter. The dataset used is the QADI corpus, which consists of 540,000 sentences across 18 Arabic dialects. Preprocessing challenges include handling inconsistent orthographic variations and non-standard spellings typical of the Libyan dialect. The chi-square analysis revealed that certain features, such as email mentions and emotion indicators, were not significantly associated with dialect classification and were thus excluded from further analysis. Two main experiments were conducted: (1) evaluating the significance of meta-features extracted from the corpus using the chi-square test and (2) assessing classifier performance using different word and character n-gram representations. The classification experiments showed that Multinomial Naive Bayes (MNB) achieved the highest accuracy of 85.89% and an F1-score of 0.85741 when using a (1,2) word n-gram and (1,5) character n-gram representation. In contrast, Logistic Regression and Linear SVM exhibited slightly lower performance, with maximum accuracies of 84.41% and 84.73%, respectively. Additional evaluation metrics, including log loss, Cohen kappa, and Matthew correlation coefficient, further supported the effectiveness of MNB in this task. The results indicate that carefully selected n-gram representations and classification models play a crucial role in improving the accuracy of Libyan dialect identification. This study provides empirical benchmarks and insights for future research in Arabic dialect NLP applications.

</details>


### [3] [SQuARE: Structured Query & Adaptive Retrieval Engine For Tabular Formats](https://arxiv.org/abs/2512.04292)
*Chinmay Gondhalekar,Urjitkumar Patel,Fang-Chun Yeh*

Main category: cs.CL

TL;DR: SQuARE是一个混合检索框架，它可以处理具有复杂表头的电子表格的问答。


<details>
  <summary>Details</summary>
Motivation: 由于多行表头、合并单元格和单位注释会破坏简单的分块，而刚性的SQL视图在缺少一致模式的文件上会失败，因此在真实的电子表格上进行准确的问答仍然很困难。

Method: SQuARE计算一个基于表头深度和合并密度的连续分数，然后通过保留结构的块检索或基于自动构建的关系表示的SQL来路由查询。当置信度较低时，轻量级代理会监督跨两条路径的检索、细化或结果组合。

Result: 在多表头公司资产负债表、大量合并的世界银行工作簿和各种公共数据集上进行评估，SQuARE在检索精度和端到端答案准确性方面始终超过单策略基线和ChatGPT-4o，同时保持延迟可预测。

Conclusion: 通过将检索与模型选择分离，该系统与新兴的表格基础模型兼容，并为更强大的表格理解提供了实用的桥梁。

Abstract: Accurate question answering over real spreadsheets remains difficult due to multirow headers, merged cells, and unit annotations that disrupt naive chunking, while rigid SQL views fail on files lacking consistent schemas. We present SQuARE, a hybrid retrieval framework with sheet-level, complexity-aware routing. It computes a continuous score based on header depth and merge density, then routes queries either through structure-preserving chunk retrieval or SQL over an automatically constructed relational representation. A lightweight agent supervises retrieval, refinement, or combination of results across both paths when confidence is low. This design maintains header hierarchies, time labels, and units, ensuring that returned values are faithful to the original cells and straightforward to verify. Evaluated on multi-header corporate balance sheets, a heavily merged World Bank workbook, and diverse public datasets, SQuARE consistently surpasses single-strategy baselines and ChatGPT-4o on both retrieval precision and end-to-end answer accuracy while keeping latency predictable. By decoupling retrieval from model choice, the system is compatible with emerging tabular foundation models and offers a practical bridge toward a more robust table understanding.

</details>


### [4] [DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle](https://arxiv.org/abs/2512.04324)
*Fangyu Lei,Jinxiang Meng,Yiming Huang,Junjie Zhao,Yitong Zhang,Jianwen Luo,Xin Zou,Ruiyi Yang,Wenbo Shi,Yan Gao,Shizhu He,Zuo Wang,Qian Liu,Yang Wang,Ke Wang,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: DAComp是一个包含210个任务的基准，旨在模仿真实的企业数据智能工作流程，包括数据工程和数据分析。


<details>
  <summary>Details</summary>
Motivation: 现有模型在复杂的数据工程和数据分析任务中表现不佳，无法满足企业需求。

Method: DAComp包含数据工程和数据分析任务，使用多指标评估工程任务，使用LLM-judge评估开放式任务。

Result: 现有技术水平的agent在DAComp上的表现不佳，数据工程任务的成功率低于20%，数据分析任务的平均得分低于40%。

Conclusion: DAComp揭示了现有agent在数据工程和数据分析方面的局限性，并提供了一个严格且真实的测试平台，以推动企业环境中真正有能力的自主数据代理的开发。

Abstract: Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io

</details>


### [5] [ClusterFusion: Hybrid Clustering with Embedding Guidance and LLM Adaptation](https://arxiv.org/abs/2512.04350)
*Yiming Xu,Yuan Yuan,Vijay Viswanathan,Graham Neubig*

Main category: cs.CL

TL;DR: ClusterFusion是一个混合框架，它将大型语言模型（LLM）作为聚类核心，并以轻量级嵌入方法为指导，从而在领域特定的文本聚类任务中实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的聚类算法在没有昂贵的微调的情况下，通常在领域特定的上下文中表现不佳。大型语言模型（LLM）提供了强大的上下文推理能力。

Method: ClusterFusion包含三个阶段：嵌入引导的子集划分、LLM驱动的主题总结和基于LLM的主题分配。

Result: 在三个公共基准数据集和两个新的领域特定数据集上的实验表明，ClusterFusion不仅在标准任务上实现了最先进的性能，而且在专门领域也取得了显著的收益。

Conclusion: ClusterFusion框架通过将LLM作为聚类核心，能够直接整合领域知识和用户偏好，充分利用LLM的上下文适应性，从而在文本聚类任务中取得优异表现。

Abstract: Text clustering is a fundamental task in natural language processing, yet traditional clustering algorithms with pre-trained embeddings often struggle in domain-specific contexts without costly fine-tuning. Large language models (LLMs) provide strong contextual reasoning, yet prior work mainly uses them as auxiliary modules to refine embeddings or adjust cluster boundaries. We propose ClusterFusion, a hybrid framework that instead treats the LLM as the clustering core, guided by lightweight embedding methods. The framework proceeds in three stages: embedding-guided subset partition, LLM-driven topic summarization, and LLM-based topic assignment. This design enables direct incorporation of domain knowledge and user preferences, fully leveraging the contextual adaptability of LLMs. Experiments on three public benchmarks and two new domain-specific datasets demonstrate that ClusterFusion not only achieves state-of-the-art performance on standard tasks but also delivers substantial gains in specialized domains. To support future work, we release our newly constructed dataset and results on all benchmarks.

</details>


### [6] [LangSAT: A Novel Framework Combining NLP and Reinforcement Learning for SAT Solving](https://arxiv.org/abs/2512.04374)
*Muyu Pan,Matthew Walter,Dheeraj Kodakandla,Mahfuza Farooque*

Main category: cs.CL

TL;DR: 提出了一种新的基于强化学习 (RL) 的框架，以优化冲突驱动子句学习 (CDCL) 过程中的启发式选择，从而提高布尔可满足性 (SAT) 求解的效率。


<details>
  <summary>Details</summary>
Motivation: 弥合自然语言输入和命题逻辑之间的差距，通过将英文描述转换为合取范式 (CNF) 表达式，并使用 RL 增强的 CDCL SAT 求解器求解它们，从而使 SAT 求解更易于访问。

Method: Lang2Logic 将英文句子翻译成 CNF 表达式, SmartSAT 编码子句-变量关系作为结构化图表示，并提取特定于 SAT 问题的全局特征，为 RL 代理提供更深层次的上下文信息，从而更有效地解决 SAT 问题。

Result: LangSAT 框架的 SmartSAT 在求解时间方面表现出与传统 CDCL 启发式算法相当的性能。

Conclusion: LangSAT 框架为跨推理、形式验证和调试的 SAT 求解任务提供了一种更易于访问和可扩展的解决方案。

Abstract: Our work presents a novel reinforcement learning (RL) based framework to optimize heuristic selection within the conflict-driven clause learning (CDCL) process, improving the efficiency of Boolean satisfiability (SAT) solving. The proposed system, LangSAT, bridges the gap between natural language inputs and propositional logic by converting English descriptions into Conjunctive Normal Form (CNF) expressions and solving them using an RL-enhanced CDCL SAT solver. Unlike existing SAT-solving platforms that require CNF as input, LangSAT enables users to input standard English descriptions, making SAT-solving more accessible. The framework comprises two key components: Lang2Logic, which translates English sentences into CNF expressions, and SmartSAT, an RL-based SAT solver. SmartSAT encodes clause-variable relationships as structured graph representations and extracts global features specific to the SAT problem. This implementation provides the RL agent with deeper contextual information, enabling SAT problems to be solved more efficiently. Lang2Logic was evaluated on diverse natural language inputs, processing descriptions up to 450 words. The generated CNFs were solved by SmartSAT, which demonstrated comparable performance to traditional CDCL heuristics with respect to solving time. The combined LangSAT framework offers a more accessible and scalable solution for SAT-solving tasks across reasoning, formal verification, and debugging.

</details>


### [7] [MASE: Interpretable NLP Models via Model-Agnostic Saliency Estimation](https://arxiv.org/abs/2512.04386)
*Zhou Yang,Shunyan Luo,Jiazhen Zhu,Fang Jin*

Main category: cs.CL

TL;DR: 提出了一种名为 MASE 的新框架，用于解释 NLP 中基于文本的预测模型，无需了解模型内部结构。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在 NLP 领域取得了显著进展，但其可解释性仍然难以捉摸，尤其是在评估其复杂的决策过程时。

Method: 该方法利用嵌入层上的归一化线性高斯扰动 (NLGP) 来有效估计输入显着性。

Result: 结果表明，MASE 优于其他与模型无关的解释方法，尤其是在 Delta 准确性方面。

Conclusion: MASE 是一种很有前途的工具，可用于阐明 NLP 中基于文本的模型的运作方式。

Abstract: Deep neural networks (DNNs) have made significant strides in Natural Language Processing (NLP), yet their interpretability remains elusive, particularly when evaluating their intricate decision-making processes. Traditional methods often rely on post-hoc interpretations, such as saliency maps or feature visualization, which might not be directly applicable to the discrete nature of word data in NLP. Addressing this, we introduce the Model-agnostic Saliency Estimation (MASE) framework. MASE offers local explanations for text-based predictive models without necessitating in-depth knowledge of a model's internal architecture. By leveraging Normalized Linear Gaussian Perturbations (NLGP) on the embedding layer instead of raw word inputs, MASE efficiently estimates input saliency. Our results indicate MASE's superiority over other model-agnostic interpretation methods, especially in terms of Delta Accuracy, positioning it as a promising tool for elucidating the operations of text-based models in NLP.

</details>


### [8] [Sarcasm Detection on Reddit Using Classical Machine Learning and Feature Engineering](https://arxiv.org/abs/2512.04396)
*Subrata Karmaker*

Main category: cs.CL

TL;DR: 本文研究了在没有神经网络或父评论上下文的情况下，仅使用经典机器学习方法和显式特征工程进行讽刺检测。


<details>
  <summary>Details</summary>
Motivation: 由于讽刺的真正含义通常与字面意思相悖，因此机器很难识别。这项工作旨在研究讽刺检测。

Method: 使用来自 Self-Annotated Reddit Corpus (SARC 2.0) 的 100,000 条评论子样本，将词级和字符级 TF-IDF 特征与简单的文体指标相结合。评估了四种模型：逻辑回归、线性 SVM、多项式朴素贝叶斯和随机森林。

Result: 朴素贝叶斯和逻辑回归表现最强，讽刺评论的 F1 分数约为 0.57。

Conclusion: 虽然缺乏对话上下文限制了性能，但结果为使用轻量级和可解释的方法进行讽刺检测提供了清晰且可重现的基线。

Abstract: Sarcasm is common in online discussions, yet difficult for machines to identify because the intended meaning often contradicts the literal wording. In this work, I study sarcasm detection using only classical machine learning methods and explicit feature engineering, without relying on neural networks or context from parent comments. Using a 100,000-comment subsample of the Self-Annotated Reddit Corpus (SARC 2.0), I combine word-level and character-level TF-IDF features with simple stylistic indicators. Four models are evaluated: logistic regression, a linear SVM, multinomial Naive Bayes, and a random forest. Naive Bayes and logistic regression perform the strongest, achieving F1-scores around 0.57 for sarcastic comments. Although the lack of conversational context limits performance, the results offer a clear and reproducible baseline for sarcasm detection using lightweight and interpretable methods.

</details>


### [9] [OsmT: Bridging OpenStreetMap Queries and Natural Language with Open-source Tag-aware Language Models](https://arxiv.org/abs/2512.04738)
*Zhuoyue Wan,Wentao Hu,Chen Jason Zhang,Yuanfeng Song,Shuaimin Li,Ruiqiang Xiao,Xiao-Yong Wei,Raymond Chi-Wing Wong*

Main category: cs.CL

TL;DR: OsmT：一个开源的标签感知语言模型，旨在桥接自然语言和OverpassQL，用于访问大规模OpenStreetMap数据。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案依赖大型闭源模型，存在推理成本高、透明度有限和轻量级部署适应性不足的问题。

Method: 引入标签检索增强（TRA）机制，将上下文相关的标签知识融入生成过程，并定义反向任务OverpassQL-to-Text，将结构化查询转换为自然语言解释。

Result: 在公共基准上评估OsmT，观察到查询生成和解释方面持续改进，且模型参数显著减少的情况下实现了具有竞争力的准确性。

Conclusion: 开源预训练语言模型在桥接自然语言和结构化查询语言方面是有效的，尤其是在富模式地理空间环境中。

Abstract: Bridging natural language and structured query languages is a long-standing challenge in the database community. While recent advances in language models have shown promise in this direction, existing solutions often rely on large-scale closed-source models that suffer from high inference costs, limited transparency, and lack of adaptability for lightweight deployment. In this paper, we present OsmT, an open-source tag-aware language model specifically designed to bridge natural language and Overpass Query Language (OverpassQL), a structured query language for accessing large-scale OpenStreetMap (OSM) data. To enhance the accuracy and structural validity of generated queries, we introduce a Tag Retrieval Augmentation (TRA) mechanism that incorporates contextually relevant tag knowledge into the generation process. This mechanism is designed to capture the hierarchical and relational dependencies present in the OSM database, addressing the topological complexity inherent in geospatial query formulation. In addition, we define a reverse task, OverpassQL-to-Text, which translates structured queries into natural language explanations to support query interpretation and improve user accessibility. We evaluate OsmT on a public benchmark against strong baselines and observe consistent improvements in both query generation and interpretation. Despite using significantly fewer parameters, our model achieves competitive accuracy, demonstrating the effectiveness of open-source pre-trained language models in bridging natural language and structured query languages within schema-rich geospatial environments.

</details>


### [10] [RapidUn: Influence-Driven Parameter Reweighting for Efficient Large Language Model Unlearning](https://arxiv.org/abs/2512.04457)
*Guoshenghui Zhao,Huawei Lin,Weijie Zhao*

Main category: cs.CL

TL;DR: 提出了一种名为 RapidUn 的高效 LLM 反学习框架，通过影响驱动的参数重加权实现选择性参数更新，从而在保留通用知识的同时消除有害行为。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型（LLM）中移除特定数据的影响仍然具有挑战性，因为重新训练成本高昂，现有的近似反学习方法通常不稳定，并且当忘记集很小或不平衡时，问题会更加严重。

Method: RapidUn 框架首先通过快速估计模块估计每个样本的影响，然后将这些分数映射到自适应更新权重，以指导选择性参数更新。

Result: 在 Dolly-15k 和 Alpaca-57k 上，RapidUn 在 Mistral-7B 和 Llama-3-8B 上的效率比完全重新训练高出 100 倍，并且在同分布和异分布遗忘方面均优于 Fisher、GA 和 LoReUn。

Conclusion: 影响引导的参数重加权是 LLM 反学习的可扩展且可解释的范例。

Abstract: Removing specific data influence from large language models (LLMs) remains challenging, as retraining is costly and existing approximate unlearning methods are often unstable. The challenge is exacerbated when the forget set is small or imbalanced. We introduce RapidUn, an influence-driven and parameter-efficient unlearning framework. It first estimates per-sample influence through a fast estimation module, then maps these scores into adaptive update weights that guide selective parameter updates -- forgetting harmful behavior while retaining general knowledge. On Mistral-7B and Llama-3-8B across Dolly-15k and Alpaca-57k, RapidUn achieves up to 100 times higher efficiency than full retraining and consistently outperforms Fisher, GA, and LoReUn on both in-distribution and out-of-distribution forgetting. These results establish influence-guided parameter reweighting as a scalable and interpretable paradigm for LLM unlearning.

</details>


### [11] [MSME: A Multi-Stage Multi-Expert Framework for Zero-Shot Stance Detection](https://arxiv.org/abs/2512.04492)
*Yuanshuo Zhang,Aohua Li,Bo Chen,Jingbo Sun,Xiaobing Zhao*

Main category: cs.CL

TL;DR: MSME是一个多阶段、多专家的零样本立场检测框架，它通过知识准备、专家推理和决策聚合三个阶段来解决复杂场景下的立场检测问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的方法在需要动态背景知识、复杂目标定义和反讽等修辞手法的实际场景中表现不佳。

Method: 提出MSME框架，包含知识准备、专家推理（知识专家、标签专家、语用专家）和决策聚合三个阶段。

Result: 在三个公共数据集上的实验表明，MSME取得了最先进的性能。

Conclusion: MSME框架有效地提升了零样本立场检测在复杂场景下的性能。

Abstract: LLM-based approaches have recently achieved impressive results in zero-shot stance detection. However, they still struggle in complex real-world scenarios, where stance understanding requires dynamic background knowledge, target definitions involve compound entities or events that must be explicitly linked to stance labels, and rhetorical devices such as irony often obscure the author's actual intent. To address these challenges, we propose MSME, a Multi-Stage, Multi-Expert framework for zero-shot stance detection. MSME consists of three stages: (1) Knowledge Preparation, where relevant background knowledge is retrieved and stance labels are clarified; (2) Expert Reasoning, involving three specialized modules-Knowledge Expert distills salient facts and reasons from a knowledge perspective, Label Expert refines stance labels and reasons accordingly, and Pragmatic Expert detects rhetorical cues such as irony to infer intent from a pragmatic angle; (3) Decision Aggregation, where a Meta-Judge integrates all expert analyses to produce the final stance prediction. Experiments on three public datasets show that MSME achieves state-of-the-art performance across the board.

</details>


### [12] [UW-BioNLP at ChemoTimelines 2025: Thinking, Fine-Tuning, and Dictionary-Enhanced LLM Systems for Chemotherapy Timeline Extraction](https://arxiv.org/abs/2512.04518)
*Tianmai M. Zhang,Zhaoyi Sun,Sihang Zeng,Chenxi Li,Neil F. Abernethy,Barbara D. Lam,Fei Xia,Meliha Yetisgen*

Main category: cs.CL

TL;DR: 本文介绍了一种从癌症患者的电子健康记录中构建全身性抗癌治疗时间线的方法，并描述了我们在ChemoTimelines共享任务子任务2中的方法、结果和发现。


<details>
  <summary>Details</summary>
Motivation: 旨在改进时间线提取。

Method: 我们评估了涉及思维链、监督微调、直接偏好优化和基于字典的查找等策略。

Result: 微调后的Qwen3-14B取得了0.678的最佳官方得分。

Conclusion: 我们的结果和分析可以为未来尝试此任务以及类似任务的设计提供有用的见解。

Abstract: The ChemoTimelines shared task benchmarks methods for constructing timelines of systemic anticancer treatment from electronic health records of cancer patients. This paper describes our methods, results, and findings for subtask 2 -- generating patient chemotherapy timelines from raw clinical notes. We evaluated strategies involving chain-of-thought thinking, supervised fine-tuning, direct preference optimization, and dictionary-based lookup to improve timeline extraction. All of our approaches followed a two-step workflow, wherein an LLM first extracted chemotherapy events from individual clinical notes, and then an algorithm normalized and aggregated events into patient-level timelines. Each specific method differed in how the associated LLM was utilized and trained. Multiple approaches yielded competitive performances on the test set leaderboard, with fine-tuned Qwen3-14B achieving the best official score of 0.678. Our results and analyses could provide useful insights for future attempts on this task as well as the design of similar tasks.

</details>


### [13] [EvoEdit: Lifelong Free-Text Knowledge Editing through Latent Perturbation Augmentation and Knowledge-driven Parameter Fusion](https://arxiv.org/abs/2512.04545)
*Pengfei Cao,Zeao Ji,Daojian Zeng,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 提出了一种新的终身自由文本知识编辑（LF-Edit）任务，以解决大型语言模型（LLM）部署后知识更新的问题。构建了一个大规模基准测试（MRLF-Bench），并提出了EvoEdit方法，该方法通过潜在扰动增强知识注入，并通过知识驱动的参数融合保留先前的信息。


<details>
  <summary>Details</summary>
Motivation: 现有的知识编辑方法依赖于结构化的三元组，与LLM预训练的自由文本性质不符，并且通常只支持一次性的知识更新，对顺序或终身编辑的研究相对有限。

Method: 提出了EvoEdit方法，该方法通过潜在扰动增强知识注入，并通过知识驱动的参数融合保留先前的信息。

Result: EvoEdit在提出的LF-Edit任务上明显优于现有的知识编辑方法。

Conclusion: EvoEdit在终身自由文本知识编辑任务上表现出色，为解决LLM知识更新问题提供了一个有希望的方案。

Abstract: Adjusting the outdated knowledge of large language models (LLMs) after deployment remains a major challenge. This difficulty has spurred the development of knowledge editing, which seeks to accurately and efficiently modify a model's internal (parametric) knowledge without retraining it from scratch. However, existing methods suffer from two limitations. First, they depend on structured triplets that are misaligned with the free-text nature of LLM pretraining and fail to capture the nuanced relationships among facts. Second, they typically support one-time knowledge updates, with relatively limited research on the problem of sequential or lifelong editing. To address these gaps, we propose a new task, Lifelong Free-text Knowledge Editing (LF-Edit), which enables models to incorporate updates expressed in natural language and supports continual editing over time. Despite its promise, LF-Edit faces the dual challenge of integrating new knowledge while mitigating the forgetting of prior information. To foster research on this new task, we construct a large-scale benchmark, Multi-Rank Lifelong Free-text Editing Benchmark (MRLF-Bench), containing 16,835 free-text edit requests. We further design a cognitively inspired multi-rank evaluation framework encompassing four levels: memorization, understanding, constrained comprehension, and reasoning. To tackle the challenges inherent in LF-Edit, we introduce a novel approach named EvoEdit that enhances knowledge injection through Latent Perturbation Augmentation and preserves prior information via Knowledge-driven Parameter Fusion. Experimental results demonstrate that EvoEdit substantially outperforms existing knowledge editing methods on the proposed LF-Edit task.

</details>


### [14] [AdmTree: Compressing Lengthy Context with Adaptive Semantic Trees](https://arxiv.org/abs/2512.04550)
*Yangning Li,Shaoshen Chen,Yinghui Li,Yankai Chen,Hai-Tao Zheng,Hui Wang,Wenhao Jiang,Philip S. Yu*

Main category: cs.CL

TL;DR: AdmTree: Adaptive hierarchical context compression for LLMs, preserving semantic fidelity and efficiency.


<details>
  <summary>Details</summary>
Motivation: Quadratic complexity of self-attention limits LLMs in processing long contexts. Existing methods compromise detail, suffer from biases, degrade information, or fail to capture long-range dependencies.

Method: Adaptive, hierarchical context compression using AdmTree, which dynamically segments input and uses gist tokens in a semantic binary tree. It employs a lightweight aggregation mechanism and a frozen backbone LLM.

Result: Robust retention of semantic information in long contexts by preserving fine-grained details, global coherence, mitigating positional bias, and dynamically adapting to content.

Conclusion: AdmTree effectively addresses the limitations of existing context compression methods by adaptively and hierarchically compressing context while maintaining high semantic fidelity and efficiency.

Abstract: The quadratic complexity of self-attention constrains Large Language Models (LLMs) in processing long contexts, a capability essential for many advanced applications. Context compression aims to alleviate this computational bottleneck while retaining critical semantic information. However, existing approaches often fall short: explicit methods may compromise local detail, whereas implicit methods can suffer from positional biases, information degradation, or an inability to capture long-range semantic dependencies. We propose AdmTree, a novel framework for adaptive, hierarchical context compression with a central focus on preserving high semantic fidelity while maintaining efficiency. AdmTree dynamically segments input based on information density, utilizing gist tokens to summarize variable-length segments as the leaves of a semantic binary tree. This structure, together with a lightweight aggregation mechanism and a frozen backbone LLM (thereby minimizing new trainable parameters), enables efficient hierarchical abstraction of the context. By preserving fine-grained details alongside global semantic coherence, mitigating positional bias, and dynamically adapting to content, AdmTree robustly retains the semantic information of long contexts.

</details>


### [15] [ADAPT: Learning Task Mixtures for Budget-Constrained Instruction Tuning](https://arxiv.org/abs/2512.04555)
*Pritam Kadasi,Abhishek Upperwal,Mayank SIngh*

Main category: cs.CL

TL;DR: ADAPT is a meta-learning algorithm that learns task sampling proportions under an explicit token budget for multi-task instruction tuning.


<details>
  <summary>Details</summary>
Motivation: Instead of fixing task weights by hand, ADAPT maintains a continuous distribution over tasks and updates it via meta-gradients of a smooth worst-case validation objective, inducing an adaptive curriculum that allocates more tokens to useful tasks while avoiding collapse.

Method: ADAPT is instantiated on three ~1B-parameter open-weight LLMs (Gemma-3-1B, LLaMA-3.2-1B, Qwen-0.6B), training on 20 Natural Instructions task types under budgets of 1%, 5%, and 10% of the available supervised tokens, and compare against strong supervised fine-tuning baselines with uniform and size-proportional mixing.

Result: ADAPT matches or slightly improves average downstream performance relative to the best static mixture, while using fewer effective training tokens and reallocating budget toward harder, benchmark-aligned tasks.

Conclusion: ADAPT is effective in learning task sampling proportions and improving performance with fewer tokens.

Abstract: We propose ADAPT, a meta-learning algorithm that \emph{learns} task sampling proportions under an explicit token budget for multi-task instruction tuning. Instead of fixing task weights by hand, \adapt{} maintains a continuous distribution over tasks and updates it via meta-gradients of a smooth worst-case validation objective, inducing an adaptive curriculum that allocates more tokens to useful tasks while avoiding collapse. We instantiate ADAPT on three $\sim$1B-parameter open-weight LLMs (Gemma-3-1B, LLaMA-3.2-1B, Qwen-0.6B), training on 20 Natural Instructions task types under budgets of $1\%$, $5\%$, and $10\%$ of the available supervised tokens, and compare against strong supervised fine-tuning baselines with uniform and size-proportional mixing. We conduct evaluations on 11 out-of-domain benchmarks spanning reasoning, reading comprehension, code generation, and instruction following, we find that ADAPT matches or slightly improves average downstream performance relative to the best static mixture, while using fewer effective training tokens and reallocating budget toward harder, benchmark-aligned tasks.

</details>


### [16] [LexGenius: An Expert-Level Benchmark for Large Language Models in Legal General Intelligence](https://arxiv.org/abs/2512.04578)
*Wenjin Liu,Haoran Luo,Xin Feng,Xiang Ji,Lijuan Zhou,Rui Mao,Jiapu Wang,Shirui Pan,Erik Cambria*

Main category: cs.CL

TL;DR: 提出了LexGenius，一个专家级的中文法律基准，用于评估大型语言模型（LLMs）的法律通用智能（GI）。


<details>
  <summary>Details</summary>
Motivation: 现有的基准是结果导向的，未能系统地评估大型语言模型的法律智能，阻碍了法律通用智能的发展。

Method: 构建了一个维度-任务-能力框架，涵盖七个维度、十一个任务和二十个能力。使用了最新的法律案例和考题来创建多项选择题，并结合人工和LLM审查，以降低数据泄露风险。

Result: 评估了12个最先进的LLM，发现LLM在法律智能能力方面存在显著差异，即使是最好的LLM也落后于人类法律专业人士。

Conclusion: LexGenius可以评估LLM的法律智能能力，并促进法律通用智能的发展。

Abstract: Legal general intelligence (GI) refers to artificial intelligence (AI) that encompasses legal understanding, reasoning, and decision-making, simulating the expertise of legal experts across domains. However, existing benchmarks are result-oriented and fail to systematically evaluate the legal intelligence of large language models (LLMs), hindering the development of legal GI. To address this, we propose LexGenius, an expert-level Chinese legal benchmark for evaluating legal GI in LLMs. It follows a Dimension-Task-Ability framework, covering seven dimensions, eleven tasks, and twenty abilities. We use the recent legal cases and exam questions to create multiple-choice questions with a combination of manual and LLM reviews to reduce data leakage risks, ensuring accuracy and reliability through multiple rounds of checks. We evaluate 12 state-of-the-art LLMs using LexGenius and conduct an in-depth analysis. We find significant disparities across legal intelligence abilities for LLMs, with even the best LLMs lagging behind human legal professionals. We believe LexGenius can assess the legal intelligence abilities of LLMs and enhance legal GI development. Our project is available at https://github.com/QwenQKing/LexGenius.

</details>


### [17] [Geschlechtsübergreifende Maskulina im Sprachgebrauch Eine korpusbasierte Untersuchung zu lexemspezifischen Unterschieden](https://arxiv.org/abs/2512.04683)
*Carolin Mueller-Spitzer,Samira Ochs,Jan Oliver Ruediger,Sascha Wolfer*

Main category: cs.CL

TL;DR: 本研究调查了当代德语媒体文本中通用男性 (GM) 的分布和语言特征。


<details>
  <summary>Details</summary>
Motivation: 学术界和公众对使用男性人称名词来指代混合性别群体或未指明身份的个人的做法存在广泛争议，对其性别中立性存在相互冲突的观点。心理语言学研究表明，GM 更容易与男性指称对象相关联，但对其真实使用的基于语料库的分析仍然很少。

Method: 我们调查了大型媒体文本语料库中的 GM，重点关注不同类型人称名词之间的词素特定差异。我们对 21 个人称名词的整个屈折范式进行了手动注释，产生了 6,195 个带注释的标记。

Result: 我们的研究结果揭示了词汇项目之间的显着差异，尤其是在被动角色名词和与声望相关的人称名词之间。在语法层面上，我们发现 GM 主要出现在复数和不定名词短语中。此外，我们的数据表明，GM 并非主要用于表示整个人群类别，正如之前所声称的那样。

Conclusion: 通过提供对 GM 在真实书面语言中的使用的经验性见解，我们有助于更细致地理解其形式和表现。这些发现为使心理语言学研究中的语言刺激与现实世界的语言使用更加紧密地结合奠定了坚实的基础。

Abstract: This study examines the distribution and linguistic characteristics of generic masculines (GM) in contemporary German press texts. The use of masculine personal nouns to refer to mixed-gender groups or unspecified individuals has been widely debated in academia and the public, with con-flicting perspectives on its gender-neutrality. While psycholinguistic studies suggest that GM is more readily associated with male referents, corpus-based analyses of its actual use remain scarce. We investigate GM in a large corpus of press texts, focusing on lexeme-specific differences across dif-ferent types of personal nouns. We conducted manual annotations of the whole inflectional para-digm of 21 personal nouns, resulting in 6,195 annotated tokens. Our findings reveal considerable differences between lexical items, especially between passive role nouns and prestige-related per-sonal nouns. On a grammatical level, we find that GM occurs predominantly in the plural and in indefinite noun phrases. Furthermore, our data shows that GM is not primarily used to denote entire classes of people, as has been previously claimed. By providing an empirical insight into the use of GM in authentic written language, we contribute to a more nuanced understanding of its forms and manifestations. These findings provide a solid basis for aligning linguistic stimuli in psy-cholinguistic studies more closely with real-world language use.

</details>


### [18] [SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs](https://arxiv.org/abs/2512.04746)
*Wenhua Cheng,Weiwei Zhang,Heng Guo,Haihao Shen*

Main category: cs.CL

TL;DR: SignRoundV2是一种后训练量化框架，即使在没有混合精度的情况下也非常有效，它通过结合梯度信息和量化引起的偏差来指导层间的比特分配，并进行轻量级的预调优搜索量化尺度，从而缩小与全精度模型的差距。


<details>
  <summary>Details</summary>
Motivation: 极低比特量化对于有效部署大型语言模型（LLM）至关重要，但它通常会导致在2比特甚至4比特时性能严重下降。

Method: SignRoundV2 引入了一种快速灵敏度指标，该指标结合了梯度信息与量化引起的偏差，以指导层级的比特分配；以及一种轻量级预调优搜索，用于量化尺度，以改善极低比特量化。

Result: SignRoundV2 能够在 LLM 上保持具有竞争力的准确性，在 4-5 比特时实现约 1% 方差的生产级性能，即使在 2 比特时也能获得强大的结果。

Conclusion: SignRoundV2 是一种有效的后训练量化框架，即使在极低比特量化下也能保持 LLM 的性能，并缩小与全精度模型的差距。

Abstract: Extreme low-bit quantization is critical for efficiently deploying Large Language Models (LLMs), yet it often leads to severe performance degradation at 2-bits and even 4-bits (e.g., MXFP4). We present SignRoundV2, a post-training quantization framework that is highly effective even without mixed-precision. SignRoundV2 introduces (1) a fast sensitivity metric that combines gradient information with quantization-induced deviations to guide layer-wise bit allocation, and (2) a lightweight pre-tuning search for quantization scales to improve extremely low-bit quantization. These components allow SignRoundV2 to close the gap with full-precision models. Extensive experiments indicate that our method sustains competitive accuracy for LLMs, achieving production-grade performance with about 1 percent variance at 4-5 bits and strong results even at 2 bits. The implementation is available at https://github.com/intel/auto-round.

</details>


### [19] [Model Whisper: Steering Vectors Unlock Large Language Models' Potential in Test-time](https://arxiv.org/abs/2512.04748)
*Xinyue Kang,Diwei Shi,Li Chen*

Main category: cs.CL

TL;DR: 提出了一种名为测试时操纵向量（TTSV）的轻量级组件，用于在不调整模型参数的情况下，提高大型语言模型（LLM）在特定任务上的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有测试时自适应方法需要调整模型参数，计算成本高且可能降低模型原有能力。

Method: 通过优化测试数据上的TTSV以最小化模型输出熵，引导模型达到更高置信度的内部状态。

Result: 在MATH500任务上，TTSV在Qwen2.5-Math-7B模型上实现了45.88%的相对性能提升，在Qwen3-4B模型上实现了16.22%的相对性能提升，并且具有强大的泛化能力。

Conclusion: TTSV是一种轻量级、高效且具有即插即用增强特性的方法，可以有效提高LLM在特定任务上的推理能力。

Abstract: It is a critical challenge to efficiently unlock the powerful reasoning potential of Large Language Models (LLMs) for specific tasks or new distributions. Existing test-time adaptation methods often require tuning model parameters, which is not only computationally expensive but also risks degrading the model's pre-existing abilities.To address this, we introduce a lightweight component, Test-Time Steering Vectors (TTSV), which is prepended to the input while keeping the LLM's parameters entirely frozen. By optimizing the TTSV on test data to minimize the model's output entropy, we steer the model towards an internal state of higher confidence, activating its inherent abilities most relevant to the current task. TTSV is both lightweight and highly efficient to optimize, making it a true plug-and-play enhancement. Extensive experiments validate our approach's effectiveness on both base models and reasoning-enhanced models. For instance, on the MATH500 task, TTSV achieves a 45.88% relative performance gain on the Qwen2.5-Math-7B model and a 16.22% relative gain on the Qwen3-4B model. Furthermore, our approach exhibits robust generalization, with its steering vectors proving highly transferable across diverse tasks.

</details>


### [20] [EtCon: Edit-then-Consolidate for Reliable Knowledge Editing](https://arxiv.org/abs/2512.04753)
*Ruilin Li,Yibin Wang,Wenhong Zhu,Chenglin Li,Jinghao Zhang,Chenliang Li,Junchi Yan,Jiaqi Wang*

Main category: cs.CL

TL;DR: 提出了一种新的知识编辑范式 Edit-then-Consolidate，旨在弥合理论知识编辑方法与其实际应用之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法在受控环境有效，但在终身学习等实际场景中效果不佳，存在过拟合和知识整合不足的问题。

Method: 首先，使用 Targeted Proximal Supervised Fine-Tuning (TPSFT) 减轻过拟合；然后，使用 Group Relative Policy Optimization (GRPO) 通过优化轨迹级行为，将编辑后的知识与 CoT 推理策略对齐。

Result: 实验表明，该框架在实际评估中持续提高编辑可靠性和泛化性，同时更好地保留了局部性和预训练能力。

Conclusion: 该研究提出了一种更实用的知识编辑方法，可以有效解决现有方法在实际应用中的问题。

Abstract: Knowledge editing aims to update specific facts in large language models (LLMs) without full retraining. Prior efforts sought to tune the knowledge layers of LLMs, proving effective for making selective edits. However, a significant gap exists between their performance in controlled, teacher-forcing evaluations and their real-world effectiveness in lifelong learning scenarios, which greatly limits their practical applicability. This work's empirical analysis reveals two recurring issues associated with this gap: (1) Most traditional methods lead the edited model to overfit to the new fact, thereby degrading pre-trained capabilities; (2) There is a critical absence of a knowledge consolidation stage, leaving new facts insufficiently integrated into LLMs' inference-time behavior under autoregressive generation, thereby leading to a mismatch between parametric knowledge and actual generation behavior. To this end, we propose Edit-then-Consolidate, a novel knowledge editing paradigm that aims to bridge the gap between theoretical knowledge editing methods and their real-world applicability. Specifically, (1) our framework mitigates overfitting via Targeted Proximal Supervised Fine-Tuning (TPSFT) that localizes the edit via a trust-region objective to limit policy drift; (2) Then, a consolidation stage using Group Relative Policy Optimization (GRPO) aligns the edited knowledge with CoT-based inference policy by optimizing trajectory-level behavior under comprehensive reward signals. Extensive experiments demonstrate our framework consistently improves editing reliability and generalization under real-world evaluations, while better preserving locality and pre-trained capabilities.

</details>


### [21] [Challenging the Abilities of Large Language Models in Italian: a Community Initiative](https://arxiv.org/abs/2512.04759)
*Malvina Nissim,Danilo Croce,Viviana Patti,Pierpaolo Basile,Giuseppe Attanasio,Elio Musacchio,Matteo Rinaldi,Federico Borazio,Maria Francis,Jacopo Gili,Daniel Scalena,Begoña Altuna,Ekhi Azurmendi,Valerio Basile,Luisa Bentivogli,Arianna Bisazza,Marianna Bolognesi,Dominique Brunato,Tommaso Caselli,Silvia Casola,Maria Cassese,Mauro Cettolo,Claudia Collacciani,Leonardo De Cosmo,Maria Pia Di Buono,Andrea Esuli,Julen Etxaniz,Chiara Ferrando,Alessia Fidelangeli,Simona Frenda,Achille Fusco,Marco Gaido,Andrea Galassi,Federico Galli,Luca Giordano,Mattia Goffetti,Itziar Gonzalez-Dios,Lorenzo Gregori,Giulia Grundler,Sandro Iannaccone,Chunyang Jiang,Moreno La Quatra,Francesca Lagioia,Soda Marem Lo,Marco Madeddu,Bernardo Magnini,Raffaele Manna,Fabio Mercorio,Paola Merlo,Arianna Muti,Vivi Nastase,Matteo Negri,Dario Onorati,Elena Palmieri,Sara Papi,Lucia Passaro,Giulia Pensa,Andrea Piergentili,Daniele Potertì,Giovanni Puccetti,Federico Ranaldi,Leonardo Ranaldi,Andrea Amelio Ravelli,Martina Rosola,Elena Sofia Ruzzetti,Giuseppe Samo,Andrea Santilli,Piera Santin,Gabriele Sarti,Giovanni Sartor,Beatrice Savoldi,Antonio Serino,Andrea Seveso,Lucia Siciliani,Paolo Torroni,Rossella Varvara,Andrea Zaninello,Asya Zanollo,Fabio Massimo Zanzotto,Kamyar Zeinalipour,Andrea Zugarini*

Main category: cs.CL

TL;DR: CALAMITA是一个意大利语的大规模LLM基准测试项目，旨在通过社区合作，设计、评估各种任务，并建立统一的评估流程。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估主要集中在英语，对其他语言的系统评估有限。

Method: 该项目联合学术界、工业界和公共部门的80多位贡献者，设计并评估涵盖语言能力、常识推理、事实一致性、公平性、摘要、翻译和代码生成的任务。

Result: 构建了一个包含20多个任务和近100个子任务的基准，并建立了支持异构数据集和指标的集中评估流程。评估了四个开放权重LLM，揭示了它们的优势和劣势。

Conclusion: CALAMITA不仅是一个资源，也是一个可持续的、社区驱动的评估框架，为其他语言和社区提供了包容和严格的LLM评估实践蓝图。

Abstract: The rapid progress of Large Language Models (LLMs) has transformed natural language processing and broadened its impact across research and society. Yet, systematic evaluation of these models, especially for languages beyond English, remains limited. "Challenging the Abilities of LAnguage Models in ITAlian" (CALAMITA) is a large-scale collaborative benchmarking initiative for Italian, coordinated under the Italian Association for Computational Linguistics. Unlike existing efforts that focus on leaderboards, CALAMITA foregrounds methodology: it federates more than 80 contributors from academia, industry, and the public sector to design, document, and evaluate a diverse collection of tasks, covering linguistic competence, commonsense reasoning, factual consistency, fairness, summarization, translation, and code generation. Through this process, we not only assembled a benchmark of over 20 tasks and almost 100 subtasks, but also established a centralized evaluation pipeline that supports heterogeneous datasets and metrics. We report results for four open-weight LLMs, highlighting systematic strengths and weaknesses across abilities, as well as challenges in task-specific evaluation. Beyond quantitative results, CALAMITA exposes methodological lessons: the necessity of fine-grained, task-representative metrics, the importance of harmonized pipelines, and the benefits and limitations of broad community engagement. CALAMITA is conceived as a rolling benchmark, enabling continuous integration of new tasks and models. This makes it both a resource -- the most comprehensive and diverse benchmark for Italian to date -- and a framework for sustainable, community-driven evaluation. We argue that this combination offers a blueprint for other languages and communities seeking inclusive and rigorous LLM evaluation practices.

</details>


### [22] [AdiBhashaa: A Community-Curated Benchmark for Machine Translation into Indian Tribal Languages](https://arxiv.org/abs/2512.04765)
*Pooja Singh,Sandeep Kumar*

Main category: cs.CL

TL;DR: AdiBhashaa是一个社区驱动的倡议，旨在为四种主要的印度部落语言构建首个开放的平行语料库和基线机器翻译系统。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和多语言机器翻译系统越来越多地驱动着信息的获取，但许多部落社区的语言在这些技术中仍然实际上是不可见的。 这种不可见性加剧了教育、治理和数字参与方面现有的结构性不平等。

Method: 结合了参与式数据创建与母语人士、人机协同验证以及对编码器-解码器机器翻译模型和大型语言模型的系统评估。

Result: 构建了四种印度部落语言Bhili、Mundari、Gondi和Santali的首个开放平行语料库和基线机器翻译系统。

Conclusion: AdiBhashaa展示了一种更公平的AI研究的可能模式：它以当地专业知识为中心，在来自边缘化社区的早期研究人员中培养能力，并在语言技术的开发中突出人类验证。

Abstract: Large language models and multilingual machine translation (MT) systems increasingly drive access to information, yet many languages of the tribal communities remain effectively invisible in these technologies. This invisibility exacerbates existing structural inequities in education, governance, and digital participation. We present AdiBhashaa, a community-driven initiative that constructs the first open parallel corpora and baseline MT systems for four major Indian tribal languages-Bhili, Mundari, Gondi, and Santali. This work combines participatory data creation with native speakers, human-in-the-loop validation, and systematic evaluation of both encoder-decoder MT models and large language models. In addition to reporting technical findings, we articulate how AdiBhashaa illustrates a possible model for more equitable AI research: it centers local expertise, builds capacity among early-career researchers from marginalized communities, and foregrounds human validation in the development of language technologies.

</details>


### [23] [DaLA: Danish Linguistic Acceptability Evaluation Guided by Real World Errors](https://arxiv.org/abs/2512.04799)
*Gianluca Barmina,Nathalie Carmen Hau Norman,Peter Schneider-Kamp,Lukas Galke*

Main category: cs.CL

TL;DR: 本文提出了一个增强的丹麦语语言可接受性基准，用于评估大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前丹麦语语言可接受性基准覆盖面窄、全面性不足，无法有效评估大型语言模型。

Method: 通过分析常见的丹麦语书写错误，设计了十四种错误生成函数，系统性地向正确的丹麦语句子中引入错误。采用人工和自动方法评估这些错误生成的有效性。

Result: 该基准比现有技术更广泛、更全面。大型语言模型在新基准上的表现低于现有基准，表明任务难度增加。该基准具有更高的区分度，可以更好地区分表现良好和表现不佳的模型。

Conclusion: 新基准通过包含更多种类的错误类型，对语言可接受性进行了更严格的评估。

Abstract: We present an enhanced benchmark for evaluating linguistic acceptability in Danish. We first analyze the most common errors found in written Danish. Based on this analysis, we introduce a set of fourteen corruption functions that generate incorrect sentences by systematically introducing errors into existing correct Danish sentences. To ensure the accuracy of these corruptions, we assess their validity using both manual and automatic methods. The results are then used as a benchmark for evaluating Large Language Models on a linguistic acceptability judgement task. Our findings demonstrate that this extension is both broader and more comprehensive than the current state of the art. By incorporating a greater variety of corruption types, our benchmark provides a more rigorous assessment of linguistic acceptability, increasing task difficulty, as evidenced by the lower performance of LLMs on our benchmark compared to existing ones. Our results also suggest that our benchmark has a higher discriminatory power which allows to better distinguish well-performing models from low-performing ones.

</details>


### [24] [DAMASHA: Detecting AI in Mixed Adversarial Texts via Segmentation with Human-interpretable Attribution](https://arxiv.org/abs/2512.04838)
*L. D. M. S. Sai Teja,N. Siva Gopala Krishna,Ufaq Khan,Muhammad Haris Khan,Partha Pakray,Atul Mishra*

Main category: cs.CL

TL;DR: 这篇论文提出了一种名为 Info-Mask 的新框架，用于检测混合作者文本中的作者身份转变点，并构建了一个对抗性基准数据集 MAS 来评估系统的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型 (LLM) 时代，人类和 AI 生成文本之间的界限变得越来越模糊，因此需要识别文本中作者身份的转变点，这对真实性、信任和人工监督具有重要意义。

Method: 该方法集成了文体线索、困惑度驱动的信号和结构化边界建模来准确分割协作的人工-AI 内容。

Result: Info-Mask 在对抗条件下显著提高了跨度级别的鲁棒性，建立了新的基线，同时揭示了剩余的挑战。

Conclusion: 研究结果强调了对抗鲁棒、可解释的混合作者身份检测的前景和局限性，对人机协同创作中的信任和监督具有重要意义。

Abstract: In the age of advanced large language models (LLMs), the boundaries between human and AI-generated text are becoming increasingly blurred. We address the challenge of segmenting mixed-authorship text, that is identifying transition points in text where authorship shifts from human to AI or vice-versa, a problem with critical implications for authenticity, trust, and human oversight. We introduce a novel framework, called Info-Mask for mixed authorship detection that integrates stylometric cues, perplexity-driven signals, and structured boundary modeling to accurately segment collaborative human-AI content. To evaluate the robustness of our system against adversarial perturbations, we construct and release an adversarial benchmark dataset Mixed-text Adversarial setting for Segmentation (MAS), designed to probe the limits of existing detectors. Beyond segmentation accuracy, we introduce Human-Interpretable Attribution (HIA overlays that highlight how stylometric features inform boundary predictions, and we conduct a small-scale human study assessing their usefulness. Across multiple architectures, Info-Mask significantly improves span-level robustness under adversarial conditions, establishing new baselines while revealing remaining challenges. Our findings highlight both the promise and limitations of adversarially robust, interpretable mixed-authorship detection, with implications for trust and oversight in human-AI co-authorship.

</details>


### [25] [Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates](https://arxiv.org/abs/2512.04844)
*Atsuki Yamaguchi,Terufumi Morishita,Aline Villavicencio,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 本文提出了一种在低资源条件下，仅使用无标签目标语言数据来调整指令大型语言模型的方法，以扩展其语言多样性。


<details>
  <summary>Details</summary>
Motivation: 扩展指令大型语言模型的语言多样性对于全球可访问性至关重要，但通常受到对昂贵的专业目标语言标注数据的依赖和适应过程中灾难性遗忘的阻碍。

Method: 提出了一种名为源屏蔽更新（SSU）的选择性参数更新策略，该策略主动保留源知识。SSU使用少量源数据和参数重要性评分方法，识别对维持源能力至关重要的参数。然后，它应用列式冻结策略来保护这些参数，然后再进行适配。

Result: 实验结果表明，SSU成功缓解了灾难性遗忘。在单语源任务上的性能下降平均仅为3.4%（7B模型）和2.8%（13B模型），这与完全微调的20.3%和22.3%形成了鲜明对比。SSU还实现了与完全微调高度竞争的目标语言性能，在7B模型的所有基准测试中以及13B模型的大多数基准测试中均优于完全微调。

Conclusion: SSU是一种有效的策略，可在低资源条件下调整指令大型语言模型，同时保持源语言能力并获得具有竞争力的目标语言性能。

Abstract: Expanding the linguistic diversity of instruct large language models (LLMs) is crucial for global accessibility but is often hindered by the reliance on costly specialized target language labeled data and catastrophic forgetting during adaptation. We tackle this challenge under a realistic, low-resource constraint: adapting instruct LLMs using only unlabeled target language data. We introduce Source-Shielded Updates (SSU), a selective parameter update strategy that proactively preserves source knowledge. Using a small set of source data and a parameter importance scoring method, SSU identifies parameters critical to maintaining source abilities. It then applies a column-wise freezing strategy to protect these parameters before adaptation. Experiments across five typologically diverse languages and 7B and 13B models demonstrate that SSU successfully mitigates catastrophic forgetting. It reduces performance degradation on monolingual source tasks to just 3.4% (7B) and 2.8% (13B) on average, a stark contrast to the 20.3% and 22.3% from full fine-tuning. SSU also achieves target-language performance highly competitive with full fine-tuning, outperforming it on all benchmarks for 7B models and the majority for 13B models.

</details>


### [26] [SEAL: Self-Evolving Agentic Learning for Conversational Question Answering over Knowledge Graphs](https://arxiv.org/abs/2512.04868)
*Hao Wang,Jialun Zhong,Changcheng Wang,Zhujun Nie,Zheng Li,Shunyu Yao,Yanzeng Li,Xinchi Li*

Main category: cs.CL

TL;DR: SEAL: A new two-stage semantic parsing framework using self-evolving agentic learning to improve accuracy and efficiency in knowledge-based conversational question answering (KBCQA).


<details>
  <summary>Details</summary>
Motivation: Existing KBCQA approaches struggle with coreference resolution, contextual dependency modeling, and complex logical reasoning, leading to structural inaccuracies and high computational costs.

Method: A two-stage semantic parsing framework (SEAL) is introduced, featuring LLM-based S-expression core extraction and agentic calibration, followed by template-based completion. It incorporates a self-evolving mechanism for continuous adaptation.

Result: SEAL achieves state-of-the-art performance on the SPICE benchmark, especially in multi-hop reasoning, comparison, and aggregation tasks, with gains in structural accuracy and computational efficiency.

Conclusion: SEAL demonstrates robust and scalable conversational reasoning capabilities by simplifying logical form generation and enhancing structural fidelity and linking efficiency.

Abstract: Knowledge-based conversational question answering (KBCQA) confronts persistent challenges in resolving coreference, modeling contextual dependencies, and executing complex logical reasoning. Existing approaches, whether end-to-end semantic parsing or stepwise agent-based reasoning, often suffer from structural inaccuracies and prohibitive computational costs, particularly when processing intricate queries over large knowledge graphs. To address these limitations, we introduce SEAL, a novel two-stage semantic parsing framework grounded in self-evolving agentic learning. In the first stage, a large language model (LLM) extracts a minimal S-expression core that captures the essential semantics of the input query. This core is then refined by an agentic calibration module, which corrects syntactic inconsistencies and aligns entities and relations precisely with the underlying knowledge graph. The second stage employs template-based completion, guided by question-type prediction and placeholder instantiation, to construct a fully executable S-expression. This decomposition not only simplifies logical form generation but also significantly enhances structural fidelity and linking efficiency. Crucially, SEAL incorporates a self-evolving mechanism that integrates local and global memory with a reflection module, enabling continuous adaptation from dialog history and execution feedback without explicit retraining. Extensive experiments on the SPICE benchmark demonstrate that SEAL achieves state-of-the-art performance, especially in multi-hop reasoning, comparison, and aggregation tasks. The results validate notable gains in both structural accuracy and computational efficiency, underscoring the framework's capacity for robust and scalable conversational reasoning.

</details>


### [27] [LLMs Know More Than Words: A Genre Study with Syntax, Metaphor & Phonetics](https://arxiv.org/abs/2512.04957)
*Weiye Shi,Zhaowei Zhang,Shaoheng Yan,Yaodong Yang*

Main category: cs.CL

TL;DR: 大型语言模型在各种语言相关任务中表现出卓越的潜力，但它们是否能从原始文本中捕捉到更深层的语言特性，如句法结构、语音线索和韵律模式，仍不清楚。


<details>
  <summary>Details</summary>
Motivation: 分析大型语言模型是否能有效地学习这些特征，并将它们应用于重要的自然语言相关任务。

Method: 我们引入了一个新的多语种体裁分类数据集，该数据集来源于古腾堡计划，这是一个大型数字图书馆，提供免费访问数千个公共领域的文学作品，每个二元任务（诗歌 vs. 小说；戏剧 vs. 诗歌；戏剧 vs. 小说）包含数千个句子，使用六种语言（英语、法语、德语、意大利语、西班牙语和葡萄牙语）。我们用三个显式的语言特征集（句法树结构、隐喻计数和语音度量）来增强每个数据集，以评估它们对分类性能的影响。

Result: 实验表明，虽然大型语言模型分类器可以从原始文本或显式提供的特征中学习潜在的语言结构，但不同的特征对任务的贡献不均衡。

Conclusion: 这 подчеркивает 在模型训练过程中加入更复杂的语言信号的重要性。

Abstract: Large language models (LLMs) demonstrate remarkable potential across diverse language related tasks, yet whether they capture deeper linguistic properties, such as syntactic structure, phonetic cues, and metrical patterns from raw text remains unclear. To analysis whether LLMs can learn these features effectively and apply them to important nature language related tasks, we introduce a novel multilingual genre classification dataset derived from Project Gutenberg, a large-scale digital library offering free access to thousands of public domain literary works, comprising thousands of sentences per binary task (poetry vs. novel;drama vs. poetry;drama vs. novel) in six languages (English, French, German, Italian, Spanish, and Portuguese). We augment each with three explicit linguistic feature sets (syntactic tree structures, metaphor counts, and phonetic metrics) to evaluate their impact on classification performance. Experiments demonstrate that although LLM classifiers can learn latent linguistic structures either from raw text or from explicitly provided features, different features contribute unevenly across tasks, which underscores the importance of incorporating more complex linguistic signals during model training.

</details>


### [28] [Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction](https://arxiv.org/abs/2512.04987)
*Nex-AGI Team,:,Yuxuan Cai,Lu Chen,Qiaoling Chen,Yuyang Ding,Liwen Fan,Wenjie Fu,Yufei Gao,Honglin Guo,Pinxue Guo,Zhenhua Han,Zhengfu He,Hanglei Hu,Kai Hu,Shengjia Hua,Tianyu Huai,Baodai Huang,Li Ji,Zhen Jiang,Zhikai Lei,Bufan Li,Jiahang Lin,Lizhi Lin,Jinxiu Liu,Shichun Liu,Ziming Liu,Yuchen Ni,Pengfang Qian,Yujiong Shen,Qingyun Shi,Wentao Shu,Peng Sun,Yiran Suo,Tian Tang,Boyu Tian,Guoteng Wang,Junzhe Wang,Peixin Wang,Zhiheng Xi,Hang Yan,Jie Yang,Zhixiong Yang,Tianchu Yao,Guangze Ye,Qianxi Yu,Shuo Zhang,Xinyue Zhang,Yiqi Zhang,Jiarong Zhao,Miao Zheng,Rui Zheng,Enyu Zhou,Jiazheng Zhou,Maosen Zhou,Yuhao Zhou,Tao Gui,Yining Zheng,Xinchi Chen,Jie Zhou,Siyuan Feng,Qin Chen,Liang He,Qi Zhang,Xuanjing Huang,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文介绍了一种用于扩展交互式环境多样性和复杂性的综合方法，以促进大型语言模型从被动响应者到自主代理的转变。


<details>
  <summary>Details</summary>
Motivation: 缺乏可扩展的基础设施来构建高质量的交互信号，阻碍了大型语言模型向自主代理的转变。

Method: 该方法通过解决三个正交维度来实现扩展：复杂度（NexAU，一个灵活的代理框架，支持通过简单的配置构建复杂的代理层次结构）；多样性（NexA4A，自动从自然语言生成不同的代理层次结构以覆盖无限域）；保真度（NexGAP，通过集成动态真实世界环境来弥合模拟-现实差距，以进行基础轨迹合成）。

Result: 在SWE-bench和tau2等基准测试中，Nex-N1始终优于SOTA开源模型，并在复杂的代理任务上实现了与前沿专有模型竞争的性能。

Conclusion: 我们开源了Nex生态系统和模型权重，以促进进一步的研究。

Abstract: The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.

</details>


### [29] [Factuality and Transparency Are All RAG Needs! Self-Explaining Contrastive Evidence Re-ranking](https://arxiv.org/abs/2512.05012)
*Francielle Vargas,Daniel Pedronette*

Main category: cs.CL

TL;DR: 本文提出了一种新的检索方法，通过对比学习微调嵌入，并为每个检索到的段落生成token级别的归因理由，从而围绕事实证据重构检索。


<details>
  <summary>Details</summary>
Motivation: 提高检索准确性，减轻RAG系统中的幻觉可能性，并提供透明的、基于证据的检索，从而增强可靠性，尤其是在安全关键领域。

Method: 使用对比学习微调嵌入，并为每个检索到的段落生成token级别的归因理由。使用基于主观性的标准自动选择困难负样本，迫使模型将事实理由拉得更近，同时将主观或误导性解释推开。

Result: 在临床试验报告上的评估表明，CER提高了检索准确性。

Conclusion: CER通过对比学习和token级别归因理由，能够有效提高检索准确性，并增强RAG系统的可靠性。

Abstract: This extended abstract introduces Self-Explaining Contrastive Evidence Re-Ranking (CER), a novel method that restructures retrieval around factual evidence by fine-tuning embeddings with contrastive learning and generating token-level attribution rationales for each retrieved passage. Hard negatives are automatically selected using a subjectivity-based criterion, forcing the model to pull factual rationales closer while pushing subjective or misleading explanations apart. As a result, the method creates an embedding space explicitly aligned with evidential reasoning. We evaluated our method on clinical trial reports, and initial experimental results show that CER improves retrieval accuracy, mitigates the potential for hallucinations in RAG systems, and provides transparent, evidence-based retrieval that enhances reliability, especially in safety-critical domains.

</details>


### [30] [Arbitrage: Efficient Reasoning via Advantage-Aware Speculation](https://arxiv.org/abs/2512.05033)
*Monishwaran Maheswaran,Rishabh Tiwari,Yuezhou Hu,Kerem Dilmen,Coleman Hooper,Haocheng Xi,Nicholas Lee,Mehrdad Farajtabar,Michael W. Mahoney,Kurt Keutzer,Amir Gholami*

Main category: cs.CL

TL;DR: 提出了一种新的步级推测解码框架Arbitrage，通过动态路由生成步骤来优化推理效率和准确率。


<details>
  <summary>Details</summary>
Motivation: 现有token级推测解码在推理任务中表现不佳，步级方法又浪费计算资源。

Method: 训练一个轻量级路由器，预测目标模型何时可能产生更有意义的步骤，动态路由生成步骤。

Result: 在多个数学推理基准测试中，Arbitrage超越了现有的步级推测解码基线，在匹配的精度下，推理延迟最多降低了约2倍。

Conclusion: Arbitrage 框架通过动态路由实现了接近最优的效率-精度权衡。

Abstract: Modern Large Language Models achieve impressive reasoning capabilities with long Chain of Thoughts, but they incur substantial computational cost during inference, and this motivates techniques to improve the performance-cost ratio. Among these techniques, Speculative Decoding accelerates inference by employing a fast but inaccurate draft model to autoregressively propose tokens, which are then verified in parallel by a more capable target model. However, due to unnecessary rejections caused by token mismatches in semantically equivalent steps, traditional token-level Speculative Decoding struggles in reasoning tasks. Although recent works have shifted to step-level semantic verification, which improve efficiency by accepting or rejecting entire reasoning steps, existing step-level methods still regenerate many rejected steps with little improvement, wasting valuable target compute. To address this challenge, we propose Arbitrage, a novel step-level speculative generation framework that routes generation dynamically based on the relative advantage between draft and target models. Instead of applying a fixed acceptance threshold, Arbitrage uses a lightweight router trained to predict when the target model is likely to produce a meaningfully better step. This routing approximates an ideal Arbitrage Oracle that always chooses the higher-quality step, achieving near-optimal efficiency-accuracy trade-offs. Across multiple mathematical reasoning benchmarks, Arbitrage consistently surpasses prior step-level Speculative Decoding baselines, reducing inference latency by up to $\sim2\times$ at matched accuracy.

</details>


### [31] [Structured Document Translation via Format Reinforcement Learning](https://arxiv.org/abs/2512.05100)
*Haiyue Song,Johannes Eschbach-Dymanus,Hour Kaing,Sumire Honda,Hideki Tanaka,Bianka Buschbeck,Masao Utiyama*

Main category: cs.CL

TL;DR: 本文提出了一种名为格式强化学习（FormatRL）的方法，以解决结构化文本翻译中难以有效处理文档级XML或HTML结构的问题。


<details>
  <summary>Details</summary>
Motivation: 现有结构化文本翻译研究主要集中在句子层面，难以有效处理复杂的文档级XML或HTML结构。

Method: 该方法在监督微调模型的基础上，采用组相对策略优化，直接优化结构感知奖励：1) TreeSim，衡量预测和参考XML树之间的结构相似性；2) Node-chrF，衡量XML节点层面的翻译质量。此外，还应用StrucAUC，一种区分小错误和主要结构性错误的细粒度指标。

Result: 在SAP软件文档基准上的实验表明，该方法在六个指标上都有所改进。

Conclusion: 不同的奖励函数对结构和翻译质量的提高都有贡献。

Abstract: Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose \textbf{Format Reinforcement Learning (FormatRL)}, which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.

</details>


### [32] [Semantic Soft Bootstrapping: Long Context Reasoning in LLMs without Reinforcement Learning](https://arxiv.org/abs/2512.05105)
*Purbesh Mitra,Sennur Ulukus*

Main category: cs.CL

TL;DR: 提出了一种名为语义软引导 (SSB) 的自蒸馏技术，用于提高大型语言模型在长上下文推理中的能力，尤其是在解决数学问题方面。该方法通过让模型学习自身生成的正确和错误答案，自动创建训练数据集，无需人工干预，并在 GSM8K 数据集上进行了实验，结果表明，在 MATH500 和 AIME2024 基准测试中，相对于常用的 RLVR 算法 GRPO，准确率分别提高了 10.6% 和 10%。


<details>
  <summary>Details</summary>
Motivation: 现有基于可验证奖励的强化学习 (RLVR) 方法在训练大型语言模型进行推理时存在奖励稀疏和样本效率低下的问题，导致需要大量的计算资源。

Method: 提出了一种名为语义软引导 (SSB) 的自蒸馏技术，该技术利用同一基础语言模型作为教师和学生，但在训练时接收关于结果正确性的不同语义上下文。该模型首先被提示一个数学问题并生成多个 rollouts，然后过滤出正确和最常见的错误响应，并在上下文中提供给模型，以产生更稳健的逐步解释和经过验证的最终答案。该流程自动从原始问题-答案数据中创建一个配对的师生训练集，无需任何人工干预。

Result: 在 GSM8K 数据集上对 Qwen2.5-3B-Instruct 进行了参数高效的微调实验，并在 MATH500 和 AIME2024 基准测试中测试了其准确性。实验结果表明，相对于常用的 RLVR 算法 GRPO，准确率分别提高了 10.6% 和 10%。

Conclusion: 该研究提出了一种有效的自蒸馏技术 (SSB)，用于提高大型语言模型在解决数学问题方面的长上下文推理能力，实验结果表明，该方法优于现有的 RLVR 算法。

Abstract: Long context reasoning in large language models (LLMs) has demonstrated enhancement of their cognitive capabilities via chain-of-thought (CoT) inference. Training such models is usually done via reinforcement learning with verifiable rewards (RLVR) in reasoning based problems, like math and programming. However, RLVR is limited by several bottlenecks, such as, lack of dense reward, and inadequate sample efficiency. As a result, it requires significant compute resources in post-training phase. To overcome these limitations, in this work, we propose \textbf{Semantic Soft Bootstrapping (SSB)}, a self-distillation technique, in which the same base language model plays the role of both teacher and student, but receives different semantic contexts about the correctness of its outcome at training time. The model is first prompted with a math problem and several rollouts are generated. From them, the correct and most common incorrect response are filtered, and then provided to the model in context to produce a more robust, step-by-step explanation with a verified final answer. This pipeline automatically curates a paired teacher-student training set from raw problem-answer data, without any human intervention. This generation process also produces a sequence of logits, which is what the student model tries to match in the training phase just from the bare question alone. In our experiment, Qwen2.5-3B-Instruct on GSM8K dataset via parameter-efficient fine-tuning. We then tested its accuracy on MATH500, and AIME2024 benchmarks. Our experiments show a jump of 10.6%, and 10% improvements in accuracy, respectively, over group relative policy optimization (GRPO), which is a commonly used RLVR algorithm. Our code is available at https://github.com/purbeshmitra/semantic-soft-bootstrapping, and the model, curated dataset is available at https://huggingface.co/purbeshmitra/semantic-soft-bootstrapping.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [33] [Beyond Flicker: Detecting Kinematic Inconsistencies for Generalizable Deepfake Video Detection](https://arxiv.org/abs/2512.04175)
*Alejandro Cobo,Roberto Valle,José Miguel Buenaposada,Luis Baumela*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的deepfake视频检测方法，通过在训练数据中引入微妙的运动学不一致性来提高模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的deepfake检测方法难以泛化到未知的manipulation，并且忽略了面部不同区域之间自然运动依赖关系的违反。

Method: 该论文提出了一种合成视频生成方法，通过操纵面部landmark配置的运动基来选择性地破坏面部运动中的自然相关性，并将这些artifacts通过人脸变形引入到原始视频中。

Result: 在几个流行的benchmark上取得了state-of-the-art的泛化结果。

Conclusion: 该方法通过学习识别复杂的生物力学缺陷来检测deepfake视频。

Abstract: Generalizing deepfake detection to unseen manipulations remains a key challenge. A recent approach to tackle this issue is to train a network with pristine face images that have been manipulated with hand-crafted artifacts to extract more generalizable clues. While effective for static images, extending this to the video domain is an open issue. Existing methods model temporal artifacts as frame-to-frame instabilities, overlooking a key vulnerability: the violation of natural motion dependencies between different facial regions. In this paper, we propose a synthetic video generation method that creates training data with subtle kinematic inconsistencies. We train an autoencoder to decompose facial landmark configurations into motion bases. By manipulating these bases, we selectively break the natural correlations in facial movements and introduce these artifacts into pristine videos via face morphing. A network trained on our data learns to spot these sophisticated biomechanical flaws, achieving state-of-the-art generalization results on several popular benchmarks.

</details>


### [34] [OnSight Pathology: A real-time platform-agnostic computational pathology companion for histopathology](https://arxiv.org/abs/2512.04187)
*Jinzhen Hu,Kevin Faust,Parsa Babaei Zadeh,Adrienn Bourkas,Shane Eaton,Andrew Young,Anzar Alvi,Dimitrios George Oreopoulos,Ameesha Paliwal,Assem Saleh Alrumeh,Evelyn Rose Kamski-Hennekam,Phedias Diamandis*

Main category: cs.CV

TL;DR: OnSight Pathology是一个平台无关的计算机视觉软件，它使用连续的自定义屏幕截图，在用户查看数字切片图像时向用户提供实时AI推断。


<details>
  <summary>Details</summary>
Motivation: 当前病理组织分析依赖主观解读和专家资源，新兴AI技术有潜力实现自动化组织学分析，但专有的数字病理解决方案阻碍了实际应用。

Method: 开发了一个名为OnSight Pathology的平台无关软件，该软件通过连续自定义屏幕截图，为用户提供实时AI推断。

Result: OnSight Pathology在2500多张公开的数字切片图像上进行了测试，包括脑肿瘤分类、有丝分裂检测和免疫组化染色定量等任务，并验证了其在常规组织病理学任务中的有效性。该软件还兼容显微镜相机实时拍摄，包括智能手机。

Conclusion: OnSight Pathology可以跨多种病理学流程提供实时AI推断，消除了AI工具在组织病理学中应用的关键障碍。

Abstract: The microscopic examination of surgical tissue remains a cornerstone of disease classification but relies on subjective interpretations and access to highly specialized experts, which can compromise accuracy and clinical care. While emerging breakthroughs in artificial intelligence (AI) offer promise for automated histological analysis, the growing number of proprietary digital pathology solutions has created barriers to real-world deployment. To address these challenges, we introduce OnSight Pathology, a platform-agnostic computer vision software that uses continuous custom screen captures to provide real-time AI inferences to users as they review digital slide images. Accessible as a single, self-contained executable file (https://onsightpathology.github.io/ ), OnSight Pathology operates locally on consumer-grade personal computers without complex software integration, enabling cost-effective and secure deployment in research and clinical workflows. Here we demonstrate the utility of OnSight Pathology using over 2,500 publicly available whole slide images across different slide viewers, as well as cases from our clinical digital pathology setup. The software's robustness is highlighted across routine histopathological tasks, including the classification of common brain tumor types, mitosis detection, and the quantification of immunohistochemical stains. A built-in multi-modal chat assistant provides verifiable descriptions of images, free of rigid class labels, for added quality control. Lastly, we show compatibility with live microscope camera feeds, including from personal smartphones, offering potential for deployment in more analog, inter-operative, and telepathology settings. Together, we highlight how OnSight Pathology can deliver real-time AI inferences across a broad range of pathology pipelines, removing key barriers to the adoption of AI tools in histopathology.

</details>


### [35] [Look Around and Pay Attention: Multi-camera Point Tracking Reimagined with Transformers](https://arxiv.org/abs/2512.04213)
*Bishoy Galoaa,Xiangyu Bai,Shayda Moezzi,Utsav Nandi,Sai Siddhartha Vivek Dhir Rangoju,Somaieh Amraee,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: LAPA: 一种新的基于 Transformer 的多摄像头点跟踪架构，它集成了基于外观的匹配和几何约束。


<details>
  <summary>Details</summary>
Motivation: 传统方法分离检测、关联和跟踪，导致误差传播和时间不一致。LAPA 通过联合推理跨视图和时间来解决这些限制。

Method: LAPA 利用注意力机制，通过增强了几何先验的跨视图注意力机制建立软对应关系；通过注意力加权聚合构建 3D 点表示，并使用 Transformer 解码器保持时间一致性。

Result: 在 TAPVid-3D-MC 上实现了 37.5% 的 APD，在 PointOdyssey-MC 上实现了 90.3% 的 APD，显著优于现有方法。

Conclusion: LAPA 统一的方法在复杂运动和遮挡场景中表现出色

Abstract: This paper presents LAPA (Look Around and Pay Attention), a novel end-to-end transformer-based architecture for multi-camera point tracking that integrates appearance-based matching with geometric constraints. Traditional pipelines decouple detection, association, and tracking, leading to error propagation and temporal inconsistency in challenging scenarios. LAPA addresses these limitations by leveraging attention mechanisms to jointly reason across views and time, establishing soft correspondences through a cross-view attention mechanism enhanced with geometric priors. Instead of relying on classical triangulation, we construct 3D point representations via attention-weighted aggregation, inherently accommodating uncertainty and partial observations. Temporal consistency is further maintained through a transformer decoder that models long-range dependencies, preserving identities through extended occlusions. Extensive experiments on challenging datasets, including our newly created multi-camera (MC) versions of TAPVid-3D panoptic and PointOdyssey, demonstrate that our unified approach significantly outperforms existing methods, achieving 37.5% APD on TAPVid-3D-MC and 90.3% APD on PointOdyssey-MC, particularly excelling in scenarios with complex motions and occlusions. Code is available at https://github.com/ostadabbas/Look-Around-and-Pay-Attention-LAPA-

</details>


### [36] [Generalized Event Partonomy Inference with Structured Hierarchical Predictive Learning](https://arxiv.org/abs/2512.04219)
*Zhou Chen,Joe Lin,Sathyanarayanan N. Aakur\\*

Main category: cs.CV

TL;DR: PARSE是一个统一的框架，无需监督即可直接从流视频中学习多尺度事件结构。


<details>
  <summary>Details</summary>
Motivation: 在计算机视觉中复制这种结构需要能够对视频进行分割的模型，不仅可以回顾性地分割，还可以预测性和分层地分割。

Method: PARSE将感知组织成一个循环预测器的层次结构，每个预测器都以自己的时间粒度运行：较低的层对短期动态进行建模，而较高的层通过基于注意力的反馈整合较长期的上下文。

Result: 在三个基准测试中评估，PARSE在流方法中实现了最先进的性能，并且在时间对齐和结构一致性方面可以与离线基线相媲美。

Conclusion: 结果表明，不确定性下的预测学习为实现类人时间抽象和组合事件理解提供了一条可扩展的路径。

Abstract: Humans naturally perceive continuous experience as a hierarchy of temporally nested events, fine-grained actions embedded within coarser routines. Replicating this structure in computer vision requires models that can segment video not just retrospectively, but predictively and hierarchically. We introduce PARSE, a unified framework that learns multiscale event structure directly from streaming video without supervision. PARSE organizes perception into a hierarchy of recurrent predictors, each operating at its own temporal granularity: lower layers model short-term dynamics while higher layers integrate longer-term context through attention-based feedback. Event boundaries emerge naturally as transient peaks in prediction error, yielding temporally coherent, nested partonomies that mirror the containment relations observed in human event perception. Evaluated across three benchmarks, Breakfast Actions, 50 Salads, and Assembly 101, PARSE achieves state-of-the-art performance among streaming methods and rivals offline baselines in both temporal alignment (H-GEBD) and structural consistency (TED, hF1). The results demonstrate that predictive learning under uncertainty provides a scalable path toward human-like temporal abstraction and compositional event understanding.

</details>


### [37] [MoReGen: Multi-Agent Motion-Reasoning Engine for Code-based Text-to-Video Synthesis](https://arxiv.org/abs/2512.04221)
*Xiangyu Bai,He Liang,Bishoy Galoaa,Utsav Nandi,Shayda Moezzi,Yuhang He,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: 本文研究了符合牛顿运动定律的文本生成视频，强调物理精确性和运动连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成视频技术在逼真度方面取得了显著进展，但在生成符合物理原理的视频方面仍面临核心挑战。

Method: 本文提出了一个名为MoReGen的运动感知、物理基础的T2V框架，该框架集成了多智能体LLM、物理模拟器和渲染器，以从代码域中的文本提示生成可重现的、物理上准确的视频。

Result: 本文提出了对象轨迹对应作为直接评估指标，并提出了一个包含1275个人工注释视频的基准测试MoReSet，涵盖了牛顿现象的九个类别，包括场景描述、时空关系和地面实况轨迹。实验结果表明，现有模型难以保持物理有效性，而MoReGen为物理连贯的视频合成建立了一个原则性方向。

Conclusion: 本文研究了符合牛顿运动定律的文本生成视频，提出了MoReGen框架和MoReSet基准测试，实验表明现有模型在物理有效性方面存在不足，MoReGen为物理连贯的视频合成提供了一个有希望的方向。

Abstract: While text-to-video (T2V) generation has achieved remarkable progress in photorealism, generating intent-aligned videos that faithfully obey physics principles remains a core challenge. In this work, we systematically study Newtonian motion-controlled text-to-video generation and evaluation, emphasizing physical precision and motion coherence. We introduce MoReGen, a motion-aware, physics-grounded T2V framework that integrates multi-agent LLMs, physics simulators, and renderers to generate reproducible, physically accurate videos from text prompts in the code domain. To quantitatively assess physical validity, we propose object-trajectory correspondence as a direct evaluation metric and present MoReSet, a benchmark of 1,275 human-annotated videos spanning nine classes of Newtonian phenomena with scene descriptions, spatiotemporal relations, and ground-truth trajectories. Using MoReSet, we conduct experiments on existing T2V models, evaluating their physical validity through both our MoRe metrics and existing physics-based evaluators. Our results reveal that state-of-the-art models struggle to maintain physical validity, while MoReGen establishes a principled direction toward physically coherent video synthesis.

</details>


### [38] [ReasonX: MLLM-Guided Intrinsic Image Decomposition](https://arxiv.org/abs/2512.04222)
*Alara Dirik,Tuanfeng Wang,Duygu Ceylan,Stefanos Zafeiriou,Anna Frühstück*

Main category: cs.CV

TL;DR: 提出了ReasonX，利用多模态大型语言模型(MLLM)作为感知评判器，提供相对内在比较，并使用这些比较作为GRPO奖励，用于在未标记的真实图像上微调内在分解模型。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散和transformer的模型受益于合成数据集的配对监督，但它们在各种真实场景中的泛化仍然具有挑战性。

Method: 利用多模态大型语言模型(MLLM)作为感知评判器，提供相对内在比较，并使用这些比较作为GRPO奖励，用于微调内在分解模型。

Result: 在多个基础架构和模态中，ReasonX 产生了显著的改进，包括在 IIW 反照率上减少 9-25% 的 WHDR，以及在 ETH3D 上高达 46% 的深度精度提升。

Conclusion: MLLM引导的比较监督有希望弥合低级和高级视觉推理。

Abstract: Intrinsic image decomposition aims to separate images into physical components such as albedo, depth, normals, and illumination. While recent diffusion- and transformer-based models benefit from paired supervision from synthetic datasets, their generalization to diverse, real-world scenarios remains challenging. We propose ReasonX, a novel framework that leverages a multimodal large language model (MLLM) as a perceptual judge providing relative intrinsic comparisons, and uses these comparisons as GRPO rewards for fine-tuning intrinsic decomposition models on unlabeled, in-the-wild images. Unlike RL methods for generative models, our framework aligns conditional intrinsic predictors by rewarding agreement between the judge's relational assessments and analytically derived relations from the model's outputs. ReasonX is model-agnostic and can be applied to different intrinsic predictors. Across multiple base architectures and modalities, ReasonX yields significant improvements, including 9-25% WHDR reduction on IIW albedo and up to 46% depth accuracy gains on ETH3D, highlighting the promise of MLLM-guided comparative supervision to bridge low- and high-level vision reasoning.

</details>


### [39] [6 Fingers, 1 Kidney: Natural Adversarial Medical Images Reveal Critical Weaknesses of Vision-Language Models](https://arxiv.org/abs/2512.04238)
*Leon Mayer,Piotr Kalinowski,Caroline Ebersbach,Marcel Knopp,Tim Rädsch,Evangelia Christodoulou,Annika Reinke,Fiona R. Kolbinger,Lena Maier-Hein*

Main category: cs.CV

TL;DR: 现有的视觉语言模型在处理罕见解剖变异时表现不佳，这是一个重要的未被量化的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的基准主要评估常见解剖结构的表现，未能捕捉到罕见变异带来的挑战。

Method: 引入AdversarialAnatomyBench，这是一个包含自然发生的罕见解剖变异的基准，涵盖不同的成像方式和解剖区域。使用AdversarialAnatomyBench对22个先进的VLM进行基准测试。

Result: 当查询基本的医学感知任务时，平均准确率从典型解剖结构的74%下降到非典型解剖结构的29%。即使是表现最好的模型，GPT-5、Gemini 2.5 Pro和Llama 4 Maverick，也出现了41-51%的性能下降。模型错误与预期的解剖偏差密切相关。模型缩放和干预措施（包括偏见感知提示和测试时推理）都未能解决这些问题。

Conclusion: 目前VLM对罕见解剖结构表现出较差的泛化能力。AdversarialAnatomyBench为系统地测量和缓解多模态医学AI系统中的解剖偏见奠定了基础。

Abstract: Vision-language models are increasingly integrated into clinical workflows. However, existing benchmarks primarily assess performance on common anatomical presentations and fail to capture the challenges posed by rare variants. To address this gap, we introduce AdversarialAnatomyBench, the first benchmark comprising naturally occurring rare anatomical variants across diverse imaging modalities and anatomical regions. We call such variants that violate learned priors about "typical" human anatomy natural adversarial anatomy. Benchmarking 22 state-of-the-art VLMs with AdversarialAnatomyBench yielded three key insights. First, when queried with basic medical perception tasks, mean accuracy dropped from 74% on typical to 29% on atypical anatomy. Even the best-performing models, GPT-5, Gemini 2.5 Pro, and Llama 4 Maverick, showed performance drops of 41-51%. Second, model errors closely mirrored expected anatomical biases. Third, neither model scaling nor interventions, including bias-aware prompting and test-time reasoning, resolved these issues. These findings highlight a critical and previously unquantified limitation in current VLM: their poor generalization to rare anatomical presentations. AdversarialAnatomyBench provides a foundation for systematically measuring and mitigating anatomical bias in multimodal medical AI systems.

</details>


### [40] [MVRoom: Controllable 3D Indoor Scene Generation with Multi-View Diffusion Models](https://arxiv.org/abs/2512.04248)
*Shaoheng Fang,Chaohui Yu,Fan Wang,Qixing Huang*

Main category: cs.CV

TL;DR: MVRoom是一个可控的室内场景新视角合成(NVS)流程，它使用以粗糙3D布局为条件的多视角扩散。


<details>
  <summary>Details</summary>
Motivation: 为了解决3D室内场景的新视角合成问题，并实现可控的场景生成。

Method: MVRoom采用两阶段设计，利用3D布局来保证多视角一致性。第一阶段使用新颖的表示来连接3D布局和图像条件信号，第二阶段进行图像条件多视角生成，并加入布局感知的极线注意力机制以增强多视角一致性。此外，还引入了一个迭代框架，通过递归地执行多视角生成来创建具有不同数量对象和复杂度的3D场景，支持文本到场景的生成。

Result: 实验结果表明，该方法在NVS任务上实现了高保真和可控的3D场景生成，在定量和定性方面均优于现有方法。

Conclusion: MVRoom有效地解决了3D室内场景的新视角合成问题，实现了高质量和可控的场景生成，并通过实验验证了关键组件的有效性。

Abstract: We introduce MVRoom, a controllable novel view synthesis (NVS) pipeline for 3D indoor scenes that uses multi-view diffusion conditioned on a coarse 3D layout. MVRoom employs a two-stage design in which the 3D layout is used throughout to enforce multi-view consistency. The first stage employs novel representations to effectively bridge the 3D layout and consistent image-based condition signals for multi-view generation. The second stage performs image-conditioned multi-view generation, incorporating a layout-aware epipolar attention mechanism to enhance multi-view consistency during the diffusion process. Additionally, we introduce an iterative framework that generates 3D scenes with varying numbers of objects and scene complexities by recursively performing multi-view generation (MVRoom), supporting text-to-scene generation. Experimental results demonstrate that our approach achieves high-fidelity and controllable 3D scene generation for NVS, outperforming state-of-the-art baseline methods both quantitatively and qualitatively. Ablation studies further validate the effectiveness of key components within our generation pipeline.

</details>


### [41] [UniLight: A Unified Representation for Lighting](https://arxiv.org/abs/2512.04267)
*Zitian Zhang,Iliyan Georgiev,Michael Fischer,Yannick Hold-Geoffroy,Jean-François Lalonde,Valentin Deschaintre*

Main category: cs.CV

TL;DR: UniLight提出了一种联合潜在空间作为光照表示，它统一了共享嵌入中的多个模态。


<details>
  <summary>Details</summary>
Motivation: 现有的光照表示（如环境图、辐照度、球谐函数或文本）不兼容，限制了跨模态转移。

Method: 使用对比学习训练文本、图像、辐照度和环境图的模态特定编码器以对齐它们的表示，并使用辅助球谐函数预测任务来加强方向理解。

Result: 实验表明，UniLight表示捕获了一致且可转移的光照特征，从而可以在模态之间进行灵活操作。

Conclusion: UniLight实现了基于光照的检索、环境图生成和扩散图像合成中的光照控制等任务。

Abstract: Lighting has a strong influence on visual appearance, yet understanding and representing lighting in images remains notoriously difficult. Various lighting representations exist, such as environment maps, irradiance, spherical harmonics, or text, but they are incompatible, which limits cross-modal transfer. We thus propose UniLight, a joint latent space as lighting representation, that unifies multiple modalities within a shared embedding. Modality-specific encoders for text, images, irradiance, and environment maps are trained contrastively to align their representations, with an auxiliary spherical-harmonics prediction task reinforcing directional understanding. Our multi-modal data pipeline enables large-scale training and evaluation across three tasks: lighting-based retrieval, environment-map generation, and lighting control in diffusion-based image synthesis. Experiments show that our representation captures consistent and transferable lighting features, enabling flexible manipulation across modalities.

</details>


### [42] [Inference-time Stochastic Refinement of GRU-Normalizing Flow for Real-time Video Motion Transfer](https://arxiv.org/abs/2512.04282)
*Tasmiah Haque,Srinjoy Das*

Main category: cs.CV

TL;DR: 提出了一种新的推理时细化技术，将门控循环单元-标准化流 (GRU-NF) 与随机抽样方法相结合，以提高序列预测的多样性。


<details>
  <summary>Details</summary>
Motivation: 沉浸式游戏和基于视觉的异常检测等实时视频动作迁移应用需要准确而多样化的未来预测，以支持真实的合成和不确定性下稳健的下游决策。

Method: 在 GRU-NF 推理过程中引入马尔可夫链蒙特卡罗 (MCMC) 步骤，使模型能够探索更丰富的输出空间，更好地逼近真实数据分布，而无需重新训练。

Result: Gated Recurrent Unit- Stochastic Normalizing Flows (GRU-SNF) 在生成多样化输出方面优于 GRU-NF，且不牺牲准确性，即使在更长的预测范围内也是如此。

Conclusion: 通过在推理过程中注入随机性，我们的方法可以更有效地捕获多模态行为。这些结果突出了将随机动态与基于流的序列模型集成以进行生成时间序列预测的潜力。

Abstract: Real-time video motion transfer applications such as immersive gaming and vision-based anomaly detection require accurate yet diverse future predictions to support realistic synthesis and robust downstream decision making under uncertainty. To improve the diversity of such sequential forecasts we propose a novel inference-time refinement technique that combines Gated Recurrent Unit-Normalizing Flows (GRU-NF) with stochastic sampling methods. While GRU-NF can capture multimodal distributions through its integration of normalizing flows within a temporal forecasting framework, its deterministic transformation structure can limit expressivity. To address this, inspired by Stochastic Normalizing Flows (SNF), we introduce Markov Chain Monte Carlo (MCMC) steps during GRU-NF inference, enabling the model to explore a richer output space and better approximate the true data distribution without retraining. We validate our approach in a keypoint-based video motion transfer pipeline, where capturing temporally coherent and perceptually diverse future trajectories is essential for realistic samples and low bandwidth communication. Experiments show that our inference framework, Gated Recurrent Unit- Stochastic Normalizing Flows (GRU-SNF) outperforms GRU-NF in generating diverse outputs without sacrificing accuracy, even under longer prediction horizons. By injecting stochasticity during inference, our approach captures multimodal behavior more effectively. These results highlight the potential of integrating stochastic dynamics with flow-based sequence models for generative time series forecasting.

</details>


### [43] [Plug-and-Play Image Restoration with Flow Matching: A Continuous Viewpoint](https://arxiv.org/abs/2512.04283)
*Fan Jia,Yuhao Huang,Shih-Hsin Wang,Cristina Garcia-Cardona,Andrea L. Bertozzi,Bao Wang*

Main category: cs.CV

TL;DR: 本文对即插即用流匹配(PnP-Flow)模型进行了理论分析，并提出了改进方法。


<details>
  <summary>Details</summary>
Motivation: PnP-Flow在图像修复方面取得了显著的经验性成功，但缺乏理论理解。

Method: 推导了PnP-Flow的连续极限，得到了一个随机微分方程(SDE)替代模型。

Result: SDE模型提供了改进PnP-Flow的两个见解：量化图像修复误差以改进步长调度和正则化神经网络参数化向量场的Lipschitz常数以减少误差；通过外推加速现成的PnP-Flow模型。

Conclusion: 通过在图像去噪、去模糊、超分辨率和图像修复等多个基准任务上验证了SDE改进的PnP-Flow的有效性。数值结果表明，该方法显著优于基线PnP-Flow和其他最先进的方法，在评估指标上取得了优异的性能。

Abstract: Flow matching-based generative models have been integrated into the plug-and-play image restoration framework, and the resulting plug-and-play flow matching (PnP-Flow) model has achieved some remarkable empirical success for image restoration. However, the theoretical understanding of PnP-Flow lags its empirical success. In this paper, we derive a continuous limit for PnP-Flow, resulting in a stochastic differential equation (SDE) surrogate model of PnP-Flow. The SDE model provides two particular insights to improve PnP-Flow for image restoration: (1) It enables us to quantify the error for image restoration, informing us to improve step scheduling and regularize the Lipschitz constant of the neural network-parameterized vector field for error reduction. (2) It informs us to accelerate off-the-shelf PnP-Flow models via extrapolation, resulting in a rescaled version of the proposed SDE model. We validate the efficacy of the SDE-informed improved PnP-Flow using several benchmark tasks, including image denoising, deblurring, super-resolution, and inpainting. Numerical results show that our method significantly outperforms the baseline PnP-Flow and other state-of-the-art approaches, achieving superior performance across evaluation metrics.

</details>


### [44] [Learning Single-Image Super-Resolution in the JPEG Compressed Domain](https://arxiv.org/abs/2512.04284)
*Sruthi Srinivasan,Elham Shakibapour,Rajy Rawther,Mehdi Saeedi*

Main category: cs.CV

TL;DR: 提出了一种直接在编码的JPEG特征上训练模型的方法，以减少数据加载的瓶颈，提高训练和推理速度。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型越来越复杂，输入数据大小也在相应地增加。尽管专用深度学习硬件取得了实质性进展，但数据加载仍然是一个主要的瓶颈，限制了训练和推理速度。

Method: 提出了一种轻量级的超分辨率pipeline，该pipeline在频域中的JPEG离散余弦变换（DCT）系数上运行。

Result: 数据加载速度提高了2.6倍，训练速度提高了2.5倍，同时保持了与标准SISR方法相当的视觉质量。

Conclusion: 该方法在单图像超分辨率（SISR）的恢复任务中有效，且显著提高了数据加载效率。

Abstract: Deep learning models have grown increasingly complex, with input data sizes scaling accordingly. Despite substantial advances in specialized deep learning hardware, data loading continues to be a major bottleneck that limits training and inference speed. To address this challenge, we propose training models directly on encoded JPEG features, reducing the computational overhead associated with full JPEG decoding and significantly improving data loading efficiency. While prior works have focused on recognition tasks, we investigate the effectiveness of this approach for the restoration task of single-image super-resolution (SISR). We present a lightweight super-resolution pipeline that operates on JPEG discrete cosine transform (DCT) coefficients in the frequency domain. Our pipeline achieves a 2.6x speedup in data loading and a 2.5x speedup in training, while preserving visual quality comparable to standard SISR approaches.

</details>


### [45] [Gamma-from-Mono: Road-Relative, Metric, Self-Supervised Monocular Geometry for Vehicular Applications](https://arxiv.org/abs/2512.04303)
*Gasser Elazab,Maximilian Jansen,Michael Unterreiner,Olaf Hellwich*

Main category: cs.CV

TL;DR: 提出了一种轻量级的单目几何估计方法 Gamma-from-Mono (GfM)，通过解耦全局和局部结构来解决单摄像头重建中的投影模糊性。


<details>
  <summary>Details</summary>
Motivation: 传统的单目深度估计通常会过度平滑道路几何细节（如颠簸、斜坡和表面不规则），丢失了运动规划和稳定性的关键信息。

Method: GfM 预测一个主要路面平面以及残差变化，残差变化由 gamma 表示，gamma 是一种无量纲的垂直偏差度量，定义为点的高度与其到相机深度的比率。利用相机离地高度，通过闭合形式确定性地恢复度量深度。

Result: 在 KITTI 和 Road Surface Reconstruction Dataset (RSRD) 上评估，GfM 在深度和 gamma 估计方面都实现了最先进的近场精度，同时保持了有竞争力的全局深度性能。该模型参数量为 8.88M，能够适应不同的相机设置。

Conclusion: GfM 是一种轻量级的单目几何估计方法，在近场精度上表现出色，并且可以自监督学习，无需大型注释数据集。

Abstract: Accurate perception of the vehicle's 3D surroundings, including fine-scale road geometry, such as bumps, slopes, and surface irregularities, is essential for safe and comfortable vehicle control. However, conventional monocular depth estimation often oversmooths these features, losing critical information for motion planning and stability. To address this, we introduce Gamma-from-Mono (GfM), a lightweight monocular geometry estimation method that resolves the projective ambiguity in single-camera reconstruction by decoupling global and local structure. GfM predicts a dominant road surface plane together with residual variations expressed by gamma, a dimensionless measure of vertical deviation from the plane, defined as the ratio of a point's height above it to its depth from the camera, and grounded in established planar parallax geometry. With only the camera's height above ground, this representation deterministically recovers metric depth via a closed form, avoiding full extrinsic calibration and naturally prioritizing near-road detail. Its physically interpretable formulation makes it well suited for self-supervised learning, eliminating the need for large annotated datasets. Evaluated on KITTI and the Road Surface Reconstruction Dataset (RSRD), GfM achieves state-of-the-art near-field accuracy in both depth and gamma estimation while maintaining competitive global depth performance. Our lightweight 8.88M-parameter model adapts robustly across diverse camera setups and, to our knowledge, is the first self-supervised monocular approach evaluated on RSRD.

</details>


### [46] [How (Mis)calibrated is Your Federated CLIP and What To Do About It?](https://arxiv.org/abs/2512.04305)
*Mainak Singha,Masih Aminbeidokhti,Paolo Casari,Elisa Ricci,Subhankar Roy*

Main category: cs.CV

TL;DR: 这篇论文研究了联邦学习（FL）环境下CLIP模型的校准问题，发现微调会降低校准效果，并提出了FL^2oRA方法来改善校准。


<details>
  <summary>Details</summary>
Motivation: 研究CLIP模型在联邦学习环境下的校准问题，现有研究较少关注此问题。

Method: 分析了Textual Prompt Tuning方法和现有的训练中校准技术在联邦学习中的表现，提出了基于LoRA的FL^2oRA方法。

Result: 实验表明，FL^2oRA方法能够产生良好校准的模型，减少了对显式校准过程的需求。

Conclusion: 论文表明，在联邦学习中，选择合适的微调组件是提高CLIP模型校准的关键，FL^2oRA方法有效。

Abstract: While vision-language models like CLIP have been extensively studied, their calibration, crucial for reliable predictions, has received limited attention. Although a few prior works have examined CLIP calibration in offline settings, the impact of fine-tuning CLIP in a federated learning (FL) setup remains unexplored. In this work, we investigate how FL affects CLIP calibration and propose strategies to improve reliability in this distributed setting. We first analyze Textual Prompt Tuning approaches and show that they degrade calibration metrics when operating under FL. We also evaluate existing in-training calibration techniques across four global aggregation methods, finding that they provide limited improvements. Our results suggest that the key challenge lies not only in how we aggregate or calibrate, but in which components we choose to fine-tune. Motivated by this insight, we propose $\text{FL}^2\text{oRA}$, a straightforward LoRA-based approach that naturally improves calibration in FL, and we analyze the factors behind its effectiveness. Experiments on multiple benchmarks demonstrate that $\text{FL}^2\text{oRA}$ consistently produces well-calibrated models, reducing the need for explicit calibration procedures. Codes are available at https://github.com/mainaksingha01/FL2oRA.

</details>


### [47] [Text-Only Training for Image Captioning with Retrieval Augmentation and Modality Gap Correction](https://arxiv.org/abs/2512.04309)
*Rui Fonseca,Bruno Martins,Gil Rocha*

Main category: cs.CV

TL;DR: 提出了一种名为TOMCap的图像描述方法，该方法无需任何人工标注的图像-文本对进行训练，并且优于其他无训练和纯文本方法。


<details>
  <summary>Details</summary>
Motivation: 旨在减少对人工标注数据的依赖，探索无需任何人工标注的图像-文本对进行训练的图像描述。

Method: 该方法基于提示预训练语言模型解码器，该解码器具有从CLIP表示形式获得的信息，经过减少模态差距的过程。我们专门测试了检索到的字幕示例和潜在向量表示的组合使用，以指导生成过程。

Result: 通过大量实验，表明TOMCap优于其他无训练和纯文本方法。

Conclusion: 分析了有关检索增强和模态差距减少组件配置的不同选择的影响。

Abstract: Image captioning has drawn considerable attention from the natural language processing and computer vision fields. Aiming to reduce the reliance on curated data, several studies have explored image captioning without any humanly-annotated image-text pairs for training, although existing methods are still outperformed by fully supervised approaches. This paper proposes TOMCap, i.e., an improved text-only training method that performs captioning without the need for aligned image-caption pairs. The method is based on prompting a pre-trained language model decoder with information derived from a CLIP representation, after undergoing a process to reduce the modality gap. We specifically tested the combined use of retrieved examples of captions, and latent vector representations, to guide the generation process. Through extensive experiments, we show that TOMCap outperforms other training-free and text-only methods. We also analyze the impact of different choices regarding the configuration of the retrieval-augmentation and modality gap reduction components.

</details>


### [48] [Real-time Cricket Sorting By Sex](https://arxiv.org/abs/2512.04311)
*Juan Manuel Cantarero Angulo,Matthew Smith*

Main category: cs.CV

TL;DR: 本文提出了一种低成本、实时的蟋蟀自动性别分类系统，该系统结合了计算机视觉和物理驱动。


<details>
  <summary>Details</summary>
Motivation: 目前养殖蟋蟀的做法通常是在不进行自动性别分类的混合性别种群中饲养，但选择性育种、优化繁殖率和营养区分等可能带来益处。

Method: 该设备集成了 Raspberry Pi 5、 Raspberry AI 相机和定制的 YOLOv8 nano 目标检测模型，以及伺服驱动的分类臂。

Result: 该模型在测试中达到了 0.977 的 mAP@0.5，实际蟋蟀分组实验的总体分类准确率为 86.8%。

Conclusion: 这些结果证明了在资源受限的设备上部署轻量级深度学习模型用于昆虫养殖应用的可行性，为提高蟋蟀生产的效率和可持续性提供了一个实用的解决方案。

Abstract: The global demand for sustainable protein sources is driving increasing interest in edible insects, with Acheta domesticus (house cricket) identified as one of the most suitable species for industrial production. Current farming practices typically rear crickets in mixed-sex populations without automated sex sorting, despite potential benefits such as selective breeding, optimized reproduction ratios, and nutritional differentiation. This work presents a low-cost, real-time system for automated sex-based sorting of Acheta domesticus, combining computer vision and physical actuation. The device integrates a Raspberry Pi 5 with the official Raspberry AI Camera and a custom YOLOv8 nano object detection model, together with a servo-actuated sorting arm. The model reached a mean Average Precision at IoU 0.5 (mAP@0.5) of 0.977 during testing, and real-world experiments with groups of crickets achieved an overall sorting accuracy of 86.8%. These results demonstrate the feasibility of deploying lightweight deep learning models on resource-constrained devices for insect farming applications, offering a practical solution to improve efficiency and sustainability in cricket production.

</details>


### [49] [Mind-to-Face: Neural-Driven Photorealistic Avatar Synthesis via EEG Decoding](https://arxiv.org/abs/2512.04313)
*Haolin Xiong,Tianwen Fu,Pratusha Bhuvana Prasad,Yunxuan Cai,Haiwei Chen,Wenbin Teng,Hanyuan Xiao,Yajie Zhao*

Main category: cs.CV

TL;DR: Mind-to-Face: A new framework that decodes EEG signals into high-fidelity facial expressions.


<details>
  <summary>Details</summary>
Motivation: Current avatar systems rely heavily on visual cues, failing when faces are occluded or when emotions remain internal.

Method: A CNN-Transformer encoder maps EEG signals into dense 3D position maps, rendered through a modified 3D Gaussian Splatting pipeline.

Result: EEG alone can reliably predict dynamic, subject-specific facial expressions, including subtle emotional responses.

Conclusion: Mind-to-Face establishes a new paradigm for neural-driven avatars, enabling personalized, emotion-aware telepresence and cognitive interaction.

Abstract: Current expressive avatar systems rely heavily on visual cues, failing when faces are occluded or when emotions remain internal. We present Mind-to-Face, the first framework that decodes non-invasive electroencephalogram (EEG) signals directly into high-fidelity facial expressions. We build a dual-modality recording setup to obtain synchronized EEG and multi-view facial video during emotion-eliciting stimuli, enabling precise supervision for neural-to-visual learning. Our model uses a CNN-Transformer encoder to map EEG signals into dense 3D position maps, capable of sampling over 65k vertices, capturing fine-scale geometry and subtle emotional dynamics, and renders them through a modified 3D Gaussian Splatting pipeline for photorealistic, view-consistent results. Through extensive evaluation, we show that EEG alone can reliably predict dynamic, subject-specific facial expressions, including subtle emotional responses, demonstrating that neural signals contain far richer affective and geometric information than previously assumed. Mind-to-Face establishes a new paradigm for neural-driven avatars, enabling personalized, emotion-aware telepresence and cognitive interaction in immersive environments.

</details>


### [50] [DisentangleFormer: Spatial-Channel Decoupling for Multi-Channel Vision](https://arxiv.org/abs/2512.04314)
*Jiashu Liao,Pietro Liò,Marc de Kamps,Duygu Sarikaya*

Main category: cs.CV

TL;DR: Vision Transformers have limitations in processing spatial and channel dimensions jointly, leading to entangled representations. DisentangleFormer architecture addresses this by decoupling spatial and channel processing for better multi-channel vision representation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of Vision Transformers in hyperspectral imaging where channels capture distinct cues, and standard self-attention leads to entangled representations.

Method: The method involves a parallel design with independent spatial-token and channel-token streams, a Squeezed Token Enhancer for dynamic fusion, and a Multi-Scale FFN to capture fine-grained dependencies.

Result: DisentangleFormer achieves state-of-the-art performance on hyperspectral benchmarks (Indian Pine, Pavia University, Houston, BigEarthNet) and an infrared pathology dataset. It also maintains competitive accuracy on ImageNet with reduced computational cost.

Conclusion: DisentangleFormer effectively decouples spatial and channel processing, leading to improved performance in hyperspectral imaging and competitive accuracy on ImageNet with reduced computational cost.

Abstract: Vision Transformers face a fundamental limitation: standard self-attention jointly processes spatial and channel dimensions, leading to entangled representations that prevent independent modeling of structural and semantic dependencies. This problem is especially pronounced in hyperspectral imaging, from satellite hyperspectral remote sensing to infrared pathology imaging, where channels capture distinct biophysical or biochemical cues. We propose DisentangleFormer, an architecture that achieves robust multi-channel vision representation through principled spatial-channel decoupling. Motivated by information-theoretic principles of decorrelated representation learning, our parallel design enables independent modeling of structural and semantic cues while minimizing redundancy between spatial and channel streams. Our design integrates three core components: (1) Parallel Disentanglement: Independently processes spatial-token and channel-token streams, enabling decorrelated feature learning across spatial and spectral dimensions, (2) Squeezed Token Enhancer: An adaptive calibration module that dynamically fuses spatial and channel streams, and (3) Multi-Scale FFN: complementing global attention with multi-scale local context to capture fine-grained structural and semantic dependencies. Extensive experiments on hyperspectral benchmarks demonstrate that DisentangleFormer achieves state-of-the-art performance, consistently outperforming existing models on Indian Pine, Pavia University, and Houston, the large-scale BigEarthNet remote sensing dataset, as well as an infrared pathology dataset. Moreover, it retains competitive accuracy on ImageNet while reducing computational cost by 17.8% in FLOPs. The code will be made publicly available upon acceptance.

</details>


### [51] [SyncTrack4D: Cross-Video Motion Alignment and Video Synchronization for Multi-Video 4D Gaussian Splatting](https://arxiv.org/abs/2512.04315)
*Yonghan Lee,Tsung-Wei Huang,Shiv Gehlot,Jaehoon Choi,Guan-Ming Su,Dinesh Manocha*

Main category: cs.CV

TL;DR: 提出了一个名为 SyncTrack4D 的新方法，用于处理真实世界中未同步的多视频，实现动态 3D 场景的建模。


<details>
  <summary>Details</summary>
Motivation: 由于动态 3D 场景的高维度特性，需要聚合来自多个视角的信息来重建随时间变化的 3D 几何体和运动。

Method: 该方法利用动态场景部分的密集 4D 轨迹表示作为线索，同步进行跨视频同步和 4DGS 重建。首先，通过 Fused Gromov-Wasserstein 最优传输方法计算密集的每个视频 4D 特征轨迹和跨视频轨迹对应关系。接下来，执行全局帧级时间对齐，以最大化匹配的 4D 轨迹的重叠运动。最后，通过构建在运动样条支架表示上的多视频 4D 高斯溅射实现亚帧同步。

Result: 在 Panoptic Studio 和 SyncNeRF Blender 数据集上进行了评估，证明了亚帧同步的准确性，平均时间误差低于 0.26 帧，并在 Panoptic Studio 数据集上实现了 26.3 PSNR 分数的高保真 4D 重建。

Conclusion: 该论文是第一个通用的 4D 高斯溅射方法，用于未同步的视频集，且不假设存在预定义的场景对象或先验模型。

Abstract: Modeling dynamic 3D scenes is challenging due to their high-dimensional nature, which requires aggregating information from multiple views to reconstruct time-evolving 3D geometry and motion. We present a novel multi-video 4D Gaussian Splatting (4DGS) approach designed to handle real-world, unsynchronized video sets. Our approach, SyncTrack4D, directly leverages dense 4D track representation of dynamic scene parts as cues for simultaneous cross-video synchronization and 4DGS reconstruction. We first compute dense per-video 4D feature tracks and cross-video track correspondences by Fused Gromov-Wasserstein optimal transport approach. Next, we perform global frame-level temporal alignment to maximize overlapping motion of matched 4D tracks. Finally, we achieve sub-frame synchronization through our multi-video 4D Gaussian splatting built upon a motion-spline scaffold representation. The final output is a synchronized 4DGS representation with dense, explicit 3D trajectories, and temporal offsets for each video. We evaluate our approach on the Panoptic Studio and SyncNeRF Blender, demonstrating sub-frame synchronization accuracy with an average temporal error below 0.26 frames, and high-fidelity 4D reconstruction reaching 26.3 PSNR scores on the Panoptic Studio dataset. To the best of our knowledge, our work is the first general 4D Gaussian Splatting approach for unsynchronized video sets, without assuming the existence of predefined scene objects or prior models.

</details>


### [52] [Bayes-DIC Net: Estimating Digital Image Correlation Uncertainty with Bayesian Neural Networks](https://arxiv.org/abs/2512.04323)
*Biao Chen,Zhenhua Lei,Yahui Zhang,Tongzhi Niu*

Main category: cs.CV

TL;DR: 本文提出了一种基于非均匀B样条曲面生成高质量数字图像相关(DIC)数据集的新方法，并提出了一种新的网络结构Bayes-DIC Net。


<details>
  <summary>Details</summary>
Motivation: 增强基于深度学习的DIC算法的训练和泛化能力。

Method: 通过随机生成控制点坐标，构建包含各种真实位移场景的位移场，然后用于生成散斑图案数据集。Bayes-DIC Net在下采样阶段提取多层次信息，并通过单个跳跃连接促进各种层次信息的聚合。通过将dropout模块集成到Bayes-DIC Net中并在网络推理阶段激活它们，Bayes-DIC Net被转换为贝叶斯神经网络。

Result: 该网络不仅可以提供预测结果，还可以提供在处理真实未标记数据集时对这些预测的置信度。

Conclusion: 本文为DIC领域的数据集生成和算法性能增强提供了新的视角和方法。

Abstract: This paper introduces a novel method for generating high-quality Digital Image Correlation (DIC) dataset based on non-uniform B-spline surfaces. By randomly generating control point coordinates, we construct displacement fields that encompass a variety of realistic displacement scenarios, which are subsequently used to generate speckle pattern datasets. This approach enables the generation of a large-scale dataset that capture real-world displacement field situations, thereby enhancing the training and generalization capabilities of deep learning-based DIC algorithms. Additionally, we propose a novel network architecture, termed Bayes-DIC Net, which extracts information at multiple levels during the down-sampling phase and facilitates the aggregation of information across various levels through a single skip connection during the up-sampling phase. Bayes-DIC Net incorporates a series of lightweight convolutional blocks designed to expand the receptive field and capture rich contextual information while minimizing computational costs. Furthermore, by integrating appropriate dropout modules into Bayes-DIC Net and activating them during the network inference stage, Bayes-DIC Net is transformed into a Bayesian neural network. This transformation allows the network to provide not only predictive results but also confidence levels in these predictions when processing real unlabeled datasets. This feature significantly enhances the practicality and reliability of our network in real-world displacement field prediction tasks. Through these innovations, this paper offers new perspectives and methods for dataset generation and algorithm performance enhancement in the field of DIC.

</details>


### [53] [A Retrieval-Augmented Generation Approach to Extracting Algorithmic Logic from Neural Networks](https://arxiv.org/abs/2512.04329)
*Waleed Khalid,Dmitry Ignatov,Radu Timofte*

Main category: cs.CV

TL;DR: NN-RAG：一个检索增强生成系统，将PyTorch代码库转换为可搜索和执行的神经模块库。


<details>
  <summary>Details</summary>
Motivation: 重用现有的神经网络组件对研究效率至关重要，但在数千个开源存储库中发现、提取和验证这些模块仍然很困难。

Method: NN-RAG执行范围感知的依赖关系解析、保留导入的重构和验证器门控提升，以确保每个检索到的块都是范围封闭的、可编译的和可运行的。

Result: 从19个主要存储库中提取了1,289个候选块，验证了941个(73.0%)，并证明超过80%的结构是唯一的。NN-RAG为LEMUR数据集贡献了绝大多数独特的架构，提供了大约72%的所有新的网络结构。

Conclusion: NN-RAG将分散的视觉代码转换为可重复的、具有出处跟踪的算法发现基质，提供了一个第一个开源解决方案，可以量化和扩展跨存储库的可执行神经架构的多样性。

Abstract: Reusing existing neural-network components is central to research efficiency, yet discovering, extracting, and validating such modules across thousands of open-source repositories remains difficult. We introduce NN-RAG, a retrieval-augmented generation system that converts large, heterogeneous PyTorch codebases into a searchable and executable library of validated neural modules. Unlike conventional code search or clone-detection tools, NN-RAG performs scope-aware dependency resolution, import-preserving reconstruction, and validator-gated promotion -- ensuring that every retrieved block is scope-closed, compilable, and runnable. Applied to 19 major repositories, the pipeline extracted 1,289 candidate blocks, validated 941 (73.0%), and demonstrated that over 80% are structurally unique. Through multi-level de-duplication (exact, lexical, structural), we find that NN-RAG contributes the overwhelming majority of unique architectures to the LEMUR dataset, supplying approximately 72% of all novel network structures. Beyond quantity, NN-RAG uniquely enables cross-repository migration of architectural patterns, automatically identifying reusable modules in one project and regenerating them, dependency-complete, in another context. To our knowledge, no other open-source system provides this capability at scale. The framework's neutral specifications further allow optional integration with language models for synthesis or dataset registration without redistributing third-party code. Overall, NN-RAG transforms fragmented vision code into a reproducible, provenance-tracked substrate for algorithmic discovery, offering a first open-source solution that both quantifies and expands the diversity of executable neural architectures across repositories.

</details>


### [54] [Open Set Face Forgery Detection via Dual-Level Evidence Collection](https://arxiv.org/abs/2512.04331)
*Zhongyi Cai,Bryce Gernon,Wentao Bao,Yifan Li,Matthew Wright,Yu Kong*

Main category: cs.CV

TL;DR: 本文研究了开放集人脸伪造检测 (OSFFD) 问题，旨在识别新型伪造类别。


<details>
  <summary>Details</summary>
Motivation: 现有人脸伪造检测方法通常局限于二元真假分类或识别已知伪造类别，无法检测新型伪造类别。

Method: 本文提出了双层证据人脸伪造检测 (DLED) 方法，该方法在空间和频率层面上收集和融合类别特定证据，以估计预测不确定性。

Result: 在各种实验环境中进行的大量评估表明，所提出的 DLED 方法实现了最先进的性能，在检测来自新型伪造类别的伪造品方面，比各种基线模型平均高出 20%。

Conclusion: DLED 方法在传统的真假人脸伪造检测任务中也表现出竞争性性能。

Abstract: The proliferation of face forgeries has increasingly undermined confidence in the authenticity of online content. Given the rapid development of face forgery generation algorithms, new fake categories are likely to keep appearing, posing a major challenge to existing face forgery detection methods. Despite recent advances in face forgery detection, existing methods are typically limited to binary Real-vs-Fake classification or the identification of known fake categories, and are incapable of detecting the emergence of novel types of forgeries. In this work, we study the Open Set Face Forgery Detection (OSFFD) problem, which demands that the detection model recognize novel fake categories. We reformulate the OSFFD problem and address it through uncertainty estimation, enhancing its applicability to real-world scenarios. Specifically, we propose the Dual-Level Evidential face forgery Detection (DLED) approach, which collects and fuses category-specific evidence on the spatial and frequency levels to estimate prediction uncertainty. Extensive evaluations conducted across diverse experimental settings demonstrate that the proposed DLED method achieves state-of-the-art performance, outperforming various baseline models by an average of 20% in detecting forgeries from novel fake categories. Moreover, on the traditional Real-versus-Fake face forgery detection task, our DLED method concurrently exhibits competitive performance.

</details>


### [55] [Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment](https://arxiv.org/abs/2512.04356)
*Kai-Po Chang,Wei-Yuan Cheng,Chi-Pin Huang,Fu-En Yang,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为SANTA的框架，旨在减少多模态LLM在视频描述中出现的幻觉问题，特别是针对视觉对象和时间动作方面的幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态LLM在生成视频描述时存在事实不准确的问题，导致严重的幻觉。之前的研究主要集中在静态图像上，而动态视频中的视觉对象和时间动作幻觉的联合缓解仍然是一个未解决的难题。

Method: 论文提出了一个自增强对比对齐（SANTA）框架，通过排除虚假相关性和强调视觉事实来实现对象和动作的保真度。SANTA采用幻觉自增强方案来识别MLLM中潜在的幻觉，并将原始字幕转换为对比的负样本。此外，还开发了一种轨迹-短语对比对齐方法，以将区域对象和关系引导的动作与其相应的视觉和时间短语进行匹配。

Result: 大量的实验表明，SANTA在缓解对象和动作幻觉方面优于现有方法，并在幻觉检查基准测试中产生了卓越的性能。

Conclusion: SANTA框架有效地减轻了多模态LLM在视频描述中产生的对象和动作幻觉，并在相关基准测试中取得了优异的成果。

Abstract: Recent advancement in multimodal LLMs (MLLMs) has demonstrated their remarkable capability to generate descriptive captions for input videos. However, these models suffer from factual inaccuracies in the generated descriptions, causing severe hallucination issues. While prior works have explored alleviating hallucinations for static images, jointly mitigating visual object and temporal action hallucinations for dynamic videos remains a challenging and unsolved task. To tackle this challenge, we propose a Self-Augmented Contrastive Alignment (SANTA) framework for enabling object and action faithfulness by exempting the spurious correlations and enforcing the emphasis on visual facts. SANTA employs a hallucinative self-augmentation scheme to identify the potential hallucinations that lie in the MLLM and transform the original captions to the contrasted negatives. Furthermore, we develop a tracklet-phrase contrastive alignment to match the regional objects and relation-guided actions with their corresponding visual and temporal phrases. Extensive experiments demonstrate that SANTA outperforms existing methods in alleviating object and action hallucinations, yielding superior performance on the hallucination examination benchmarks.

</details>


### [56] [MAFNet:Multi-frequency Adaptive Fusion Network for Real-time Stereo Matching](https://arxiv.org/abs/2512.04358)
*Ao Xu,Rujin Zhao,Xiong Xu,Boceng Huang,Yujia Jia,Hongfeng Long,Fuxuan Chen,Zilong Cao,Fangyuan Chen*

Main category: cs.CV

TL;DR: 提出了一种新的立体匹配网络MAFNet，它仅使用有效的2D卷积即可生成高质量的视差图，从而在精度和实时性能之间取得了良好的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的立体匹配网络通常依赖于基于3D卷积的代价体构建或基于迭代优化的变形方法。前者在代价聚合过程中产生显著的计算开销，而后者通常缺乏建模非局部上下文信息的能力。这些方法在资源受限的移动设备上表现出较差的兼容性，限制了它们在实时应用中的部署。

Method: 设计了一个自适应频域滤波注意力模块，将完整代价体分解为高频和低频体，分别执行频率感知特征聚合。随后，引入了一种基于Linformer的低秩注意力机制，以自适应地融合高频和低频信息，从而产生更鲁棒的视差估计。

Result: 在Scene Flow和KITTI 2015等公共数据集上，所提出的MAFNet显著优于现有的实时方法。

Conclusion: MAFNet在精度和实时性能之间取得了良好的平衡。

Abstract: Existing stereo matching networks typically rely on either cost-volume construction based on 3D convolutions or deformation methods based on iterative optimization. The former incurs significant computational overhead during cost aggregation, whereas the latter often lacks the ability to model non-local contextual information. These methods exhibit poor compatibility on resource-constrained mobile devices, limiting their deployment in real-time applications. To address this, we propose a Multi-frequency Adaptive Fusion Network (MAFNet), which can produce high-quality disparity maps using only efficient 2D convolutions. Specifically, we design an adaptive frequency-domain filtering attention module that decomposes the full cost volume into high-frequency and low-frequency volumes, performing frequency-aware feature aggregation separately. Subsequently, we introduce a Linformer-based low-rank attention mechanism to adaptively fuse high- and low-frequency information, yielding more robust disparity estimation. Extensive experiments demonstrate that the proposed MAFNet significantly outperforms existing real-time methods on public datasets such as Scene Flow and KITTI 2015, showing a favorable balance between accuracy and real-time performance.

</details>


### [57] [FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring](https://arxiv.org/abs/2512.04390)
*Geunhyuk Youk,Jihyong Oh,Munchurl Kim*

Main category: cs.CV

TL;DR: FMA-Net++是一个用于联合视频超分辨率和去模糊的框架，它显式地模拟了运动和动态变化的曝光的耦合效应。


<details>
  <summary>Details</summary>
Motivation: 现实世界的视频修复受到运动和动态变化的曝光的复杂退化的困扰，这是一个关键的挑战，但之前的研究大多忽略了自动曝光或低光捕获的常见伪影。

Method: FMA-Net++采用了一种序列级架构，该架构建立在具有双向传播块的分层细化的基础上，从而实现了并行、远程的时间建模。在每个块中，一个曝光时间感知调制层根据每帧曝光来调节特征，这反过来又驱动一个曝光感知流引导动态滤波模块来推断运动和曝光感知的退化核。FMA-Net++将退化学习与修复分离：前者预测曝光和运动感知的先验知识来指导后者，从而提高准确性和效率。

Result: 在我们的新基准和GoPro上，仅在合成数据上训练的FMA-Net++实现了最先进的精度和时间一致性，在恢复质量和推理速度方面都优于最近的方法，并且可以很好地推广到具有挑战性的真实世界视频。

Conclusion: FMA-Net++在现实场景视频修复上表现出色，并在REDS-ME (多重曝光) 和 REDS-RE (随机曝光) 基准测试中表现出最先进的性能。

Abstract: Real-world video restoration is plagued by complex degradations from motion coupled with dynamically varying exposure - a key challenge largely overlooked by prior works and a common artifact of auto-exposure or low-light capture. We present FMA-Net++, a framework for joint video super-resolution and deblurring that explicitly models this coupled effect of motion and dynamically varying exposure. FMA-Net++ adopts a sequence-level architecture built from Hierarchical Refinement with Bidirectional Propagation blocks, enabling parallel, long-range temporal modeling. Within each block, an Exposure Time-aware Modulation layer conditions features on per-frame exposure, which in turn drives an exposure-aware Flow-Guided Dynamic Filtering module to infer motion- and exposure-aware degradation kernels. FMA-Net++ decouples degradation learning from restoration: the former predicts exposure- and motion-aware priors to guide the latter, improving both accuracy and efficiency. To evaluate under realistic capture conditions, we introduce REDS-ME (multi-exposure) and REDS-RE (random-exposure) benchmarks. Trained solely on synthetic data, FMA-Net++ achieves state-of-the-art accuracy and temporal consistency on our new benchmarks and GoPro, outperforming recent methods in both restoration quality and inference speed, and generalizes well to challenging real-world videos.

</details>


### [58] [Fourier-Attentive Representation Learning: A Fourier-Guided Framework for Few-Shot Generalization in Vision-Language Models](https://arxiv.org/abs/2512.04395)
*Hieu Dinh Trung Pham,Huy Minh Nhat Nguyen,Cuong Tuan Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种新的框架，通过傅里叶分析显式地解开视觉表征，从而增强视觉-语言模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模预训练视觉-语言模型通常学习整体表征，其中图像的领域不变结构与其领域特定的风格隐式地纠缠在一起。这为通过解开这些视觉线索来进一步增强泛化能力提供了机会。

Method: 本文提出了一种傅里叶注意力表征学习（FARL）框架，该框架通过傅里叶分析显式地解开视觉表征。该方法的核心是一种双重交叉注意机制，其中可学习的表征tokens分别查询图像的结构特征（来自相位谱）和风格特征（来自幅度谱）。

Result: 在15个数据集上的大量实验表明了该方法的有效性。

Conclusion: 本文提出的方法能够有效地解开视觉表征，从而提高视觉-语言模型的泛化能力。

Abstract: Large-scale pre-trained Vision-Language Models (VLMs) have demonstrated strong few-shot learning capabilities. However, these methods typically learn holistic representations where an image's domain-invariant structure is implicitly entangled with its domain-specific style. This presents an opportunity to further enhance generalization by disentangling these visual cues. In this paper, we propose Fourier-Attentive Representation Learning (FARL), a novel framework that addresses this by explicitly disentangling visual representations using Fourier analysis. The core of our method is a dual cross-attention mechanism, where learnable representation tokens separately query an image's structural features (from the phase spectrum) and stylistic features (from the amplitude spectrum). This process yields enriched, disentangled tokens that are then injected deep into the VLM encoders to guide adaptation. Our design, which includes an asymmetric injection strategy, forces the model to learn a more robust vision-language alignment. Extensive experiments on 15 datasets demonstrate the effectiveness of our approach.

</details>


### [59] [Performance Evaluation of Transfer Learning Based Medical Image Classification Techniques for Disease Detection](https://arxiv.org/abs/2512.04397)
*Zeeshan Ahmad,Shudi Bao,Meng Chen*

Main category: cs.CV

TL;DR: 本文研究了迁移学习技术在医学图像分类中的应用，通过在胸部X射线数据集上评估六个预训练模型，发现InceptionV3模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 在医学图像分类中，由于难以从头开始训练大型深度学习模型，因此迁移学习成为一种解决方案。

Method: 在自定义的胸部X射线数据集上评估六个预训练模型（AlexNet, VGG16, ResNet18, ResNet34, ResNet50, and InceptionV3）。

Result: 实验结果表明，InceptionV3在所有标准指标上均优于其他模型。ResNet系列随着深度的增加表现越来越好，而VGG16和AlexNet表现良好但准确率较低。

Conclusion: 迁移学习在大多数情况下是有益的，尤其是在数据有限的情况下，但改进的程度取决于模型架构、数据集大小以及源任务和目标任务之间的域相似性等因素。

Abstract: Medical image classification plays an increasingly vital role in identifying various diseases by classifying medical images, such as X-rays, MRIs and CT scans, into different categories based on their features. In recent years, deep learning techniques have attracted significant attention in medical image classification. However, it is usually infeasible to train an entire large deep learning model from scratch. To address this issue, one of the solutions is the transfer learning (TL) technique, where a pre-trained model is reused for a new task. In this paper, we present a comprehensive analysis of TL techniques for medical image classification using deep convolutional neural networks. We evaluate six pre-trained models (AlexNet, VGG16, ResNet18, ResNet34, ResNet50, and InceptionV3) on a custom chest X-ray dataset for disease detection. The experimental results demonstrate that InceptionV3 consistently outperforms other models across all the standard metrics. The ResNet family shows progressively better performance with increasing depth, whereas VGG16 and AlexNet perform reasonably well but with lower accuracy. In addition, we also conduct uncertainty analysis and runtime comparison to assess the robustness and computational efficiency of these models. Our findings reveal that TL is beneficial in most cases, especially with limited data, but the extent of improvement depends on several factors such as model architecture, dataset size, and domain similarity between source and target tasks. Moreover, we demonstrate that with a well-trained feature extractor, only a lightweight feedforward model is enough to provide efficient prediction. As such, this study contributes to the understanding of TL in medical image classification, and provides insights for selecting appropriate models based on specific requirements.

</details>


### [60] [Dual-Stream Spectral Decoupling Distillation for Remote Sensing Object Detection](https://arxiv.org/abs/2512.04413)
*Xiangyi Gao,Danpei Zhao,Bo Yuan,Wentao Li*

Main category: cs.CV

TL;DR: 提出了一种名为双流光谱解耦蒸馏 (DS2D2) 的架构无关的蒸馏方法，用于通用遥感目标检测任务。


<details>
  <summary>Details</summary>
Motivation: 现有的蒸馏方法经常遇到遥感图像 (RSI) 中混合特征的问题，并忽略了由细微特征变化引起的差异，导致纠缠的知识混淆。

Method: DS2D2 结合了基于光谱分解的显式和隐式蒸馏。首先，应用一阶小波变换进行光谱分解，以保留 RSI 的关键空间特征。利用这种空间保留，设计了一种密度无关尺度权重 (DISW)，以解决 RSI 中常见的密集和小目标检测的挑战。其次，展示了隐藏在细微的学生-教师特征差异中的隐式知识，当被检测头激活时，这些知识会显着影响预测。这种隐式知识通过全频和高频放大器提取，这些放大器将特征差异映射到预测偏差。

Result: 在 DIOR 数据集上，DS2D2 在 RetinaNet 的 AP50 上实现了 4.2% 的改进，在 Faster R-CNN 的 AP50 上实现了 3.8% 的改进，优于现有的蒸馏方法。

Conclusion: DS2D2 是一种有效的遥感目标检测知识蒸馏方法。

Abstract: Knowledge distillation is an effective and hardware-friendly method, which plays a key role in lightweighting remote sensing object detection. However, existing distillation methods often encounter the issue of mixed features in remote sensing images (RSIs), and neglect the discrepancies caused by subtle feature variations, leading to entangled knowledge confusion. To address these challenges, we propose an architecture-agnostic distillation method named Dual-Stream Spectral Decoupling Distillation (DS2D2) for universal remote sensing object detection tasks. Specifically, DS2D2 integrates explicit and implicit distillation grounded in spectral decomposition. Firstly, the first-order wavelet transform is applied for spectral decomposition to preserve the critical spatial characteristics of RSIs. Leveraging this spatial preservation, a Density-Independent Scale Weight (DISW) is designed to address the challenges of dense and small object detection common in RSIs. Secondly, we show implicit knowledge hidden in subtle student-teacher feature discrepancies, which significantly influence predictions when activated by detection heads. This implicit knowledge is extracted via full-frequency and high-frequency amplifiers, which map feature differences to prediction deviations. Extensive experiments on DIOR and DOTA datasets validate the effectiveness of the proposed method. Specifically, on DIOR dataset, DS2D2 achieves improvements of 4.2% in AP50 for RetinaNet and 3.8% in AP50 for Faster R-CNN, outperforming existing distillation approaches. The source code will be available at https://github.com/PolarAid/DS2D2.

</details>


### [61] [UTrice: Unifying Primitives in Differentiable Ray Tracing and Rasterization via Triangles for Particle-Based 3D Scenes](https://arxiv.org/abs/2512.04421)
*Changhe Liu,Ehsan Javanmardi,Naren Bao,Alex Orsholits,Manabu Tsukada*

Main category: cs.CV

TL;DR: 提出了一种可微的基于三角形的光线追踪渲染管线，可以直接将三角形作为渲染图元，而无需依赖任何代理几何体。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通过代理几何体追踪高斯粒子，这需要构建复杂的中间网格并执行昂贵的相交测试。基于高斯的粒子不太适合作为光线追踪和栅格化的统一图元。

Method: 提出了一种可微的基于三角形的光线追踪管线，该管线直接将三角形作为渲染图元。

Result: 该方法比现有的光线追踪方法实现了更高的渲染质量，同时保持了实时的渲染性能。该管线可以直接渲染通过基于栅格化的方法Triangle Splatting优化的三角形。

Conclusion: 统一了新视角合成中使用的图元。

Abstract: Ray tracing 3D Gaussian particles enables realistic effects such as depth of field, refractions, and flexible camera modeling for novel-view synthesis. However, existing methods trace Gaussians through proxy geometry, which requires constructing complex intermediate meshes and performing costly intersection tests. This limitation arises because Gaussian-based particles are not well suited as unified primitives for both ray tracing and rasterization. In this work, we propose a differentiable triangle-based ray tracing pipeline that directly treats triangles as rendering primitives without relying on any proxy geometry. Our results show that the proposed method achieves significantly higher rendering quality than existing ray tracing approaches while maintaining real-time rendering performance. Moreover, our pipeline can directly render triangles optimized by the rasterization-based method Triangle Splatting, thus unifying the primitives used in novel-view synthesis.

</details>


### [62] [Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models](https://arxiv.org/abs/2512.04425)
*Manar Alnaasan,Md Selim Sarowar,Sungho Kim*

Main category: cs.CV

TL;DR: 提出了一种可解释的多模态框架，该框架集成了RGB和深度（RGB-D）数据，以识别真实条件下的帕金森步态模式。


<details>
  <summary>Details</summary>
Motivation: 现有的帕金森病（PD）早期检测步态分析方法通常受限于单模态输入、鲁棒性低和缺乏临床透明度。

Method: 采用基于双YOLOv11的编码器进行特定模态的特征提取，然后使用多尺度局部-全局提取（MLGE）模块和跨空间颈融合机制来增强空间-时间表示。

Result: 在多模态步态数据集上的实验评估表明，所提出的RGB-D融合框架实现了更高的识别精度，提高了对环境变化的鲁棒性，并提供了清晰的视觉-语言推理。

Conclusion: 通过结合多模态特征学习和基于语言的可解释性，该研究弥合了视觉识别和临床理解之间的差距，为可靠且可解释的帕金森病步态分析提供了一种新颖的视觉-语言范例。

Abstract: Accurate and interpretable gait analysis plays a crucial role in the early detection of Parkinsons disease (PD),yet most existing approaches remain limited by single-modality inputs, low robustness, and a lack of clinical transparency. This paper presents an explainable multimodal framework that integrates RGB and Depth (RGB-D) data to recognize Parkinsonian gait patterns under realistic conditions. The proposed system employs dual YOLOv11-based encoders for modality-specific feature extraction, followed by a Multi-Scale Local-Global Extraction (MLGE) module and a Cross-Spatial Neck Fusion mechanism to enhance spatial-temporal representation. This design captures both fine-grained limb motion (e.g., reduced arm swing) and overall gait dynamics (e.g., short stride or turning difficulty), even in challenging scenarios such as low lighting or occlusion caused by clothing. To ensure interpretability, a frozen Large Language Model (LLM) is incorporated to translate fused visual embeddings and structured metadata into clinically meaningful textual explanations. Experimental evaluations on multimodal gait datasets demonstrate that the proposed RGB-D fusion framework achieves higher recognition accuracy, improved robustness to environmental variations, and clear visual-linguistic reasoning compared with single-input baselines. By combining multimodal feature learning with language-based interpretability, this study bridges the gap between visual recognition and clinical understanding, offering a novel vision-language paradigm for reliable and explainable Parkinsons disease gait analysis. Code:https://github.com/manaralnaasan/RGB-D_parkinson-LLM

</details>


### [63] [Self-Paced and Self-Corrective Masked Prediction for Movie Trailer Generation](https://arxiv.org/abs/2512.04426)
*Sidan Zhu,Hongteng Xu,Dixin Luo*

Main category: cs.CV

TL;DR: 提出了一种新的自步和自校正掩码预测方法SSMP，用于自动电影预告片生成，通过双向上下文建模和渐进式自校正，实现了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法采用“选择然后排序”的范例，存在误差传播，限制了生成预告片的质量。

Method: 训练一个Transformer编码器，将电影镜头序列作为提示，并相应地生成相应的预告片镜头序列。该模型通过掩码预测进行训练，从其随机掩码的对应物重建每个预告片镜头序列。掩码比率是自步的。

Result: 定量结果和用户研究表明，与现有的自动电影预告片生成方法相比，SSMP具有优越性。

Conclusion: SSMP通过双向上下文建模和渐进式自校正，在自动电影预告片生成方面取得了最先进的结果。

Abstract: As a challenging video editing task, movie trailer generation involves selecting and reorganizing movie shots to create engaging trailers. Currently, most existing automatic trailer generation methods employ a "selection-then-ranking" paradigm (i.e., first selecting key shots and then ranking them), which suffers from inevitable error propagation and limits the quality of the generated trailers. Beyond this paradigm, we propose a new self-paced and self-corrective masked prediction method called SSMP, which achieves state-of-the-art results in automatic trailer generation via bi-directional contextual modeling and progressive self-correction. In particular, SSMP trains a Transformer encoder that takes the movie shot sequences as prompts and generates corresponding trailer shot sequences accordingly. The model is trained via masked prediction, reconstructing each trailer shot sequence from its randomly masked counterpart. The mask ratio is self-paced, allowing the task difficulty to adapt to the model and thereby improving model performance. When generating a movie trailer, the model fills the shot positions with high confidence at each step and re-masks the remaining positions for the next prediction, forming a progressive self-correction mechanism that is analogous to how human editors work. Both quantitative results and user studies demonstrate the superiority of SSMP in comparison to existing automatic movie trailer generation methods. Demo is available at: https://github.com/Dixin-Lab/SSMP.

</details>


### [64] [MindDrive: An All-in-One Framework Bridging World Models and Vision-Language Model for End-to-End Autonomous Driving](https://arxiv.org/abs/2512.04441)
*Bin Suna,Yaoguang Caob,Yan Wanga,Rui Wanga,Jiachen Shanga,Xiejie Fenga,Jiayi Lu,Jia Shi,Shichun Yang,Xiaoyu Yane,Ziying Song*

Main category: cs.CV

TL;DR: MindDrive: A framework integrating trajectory generation with decision-making for autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Existing autonomous driving methods lack either trajectory generation quality or decision-making capability.

Method: A harmonized framework with Future-aware Trajectory Generator (FaTG) and VLM-oriented Evaluator (VLoE).

Result: Achieves state-of-the-art performance on NAVSIM benchmarks, improving safety and generalization.

Conclusion: Provides a path toward interpretable and cognitively guided autonomous driving.

Abstract: End-to-End autonomous driving (E2E-AD) has emerged as a new paradigm, where trajectory planning plays a crucial role. Existing studies mainly follow two directions: trajectory generation oriented, which focuses on producing high-quality trajectories with simple decision mechanisms, and trajectory selection oriented, which performs multi-dimensional evaluation to select the best trajectory yet lacks sufficient generative capability. In this work, we propose MindDrive, a harmonized framework that integrates high-quality trajectory generation with comprehensive decision reasoning. It establishes a structured reasoning paradigm of "context simulation - candidate generation - multi-objective trade-off". In particular, the proposed Future-aware Trajectory Generator (FaTG), based on a World Action Model (WaM), performs ego-conditioned "what-if" simulations to predict potential future scenes and generate foresighted trajectory candidates. Building upon this, the VLM-oriented Evaluator (VLoE) leverages the reasoning capability of a large vision-language model to conduct multi-objective evaluations across safety, comfort, and efficiency dimensions, leading to reasoned and human-aligned decision making. Extensive experiments on the NAVSIM-v1 and NAVSIM-v2 benchmarks demonstrate that MindDrive achieves state-of-the-art performance across multi-dimensional driving metrics, significantly enhancing safety, compliance, and generalization. This work provides a promising path toward interpretable and cognitively guided autonomous driving.

</details>


### [65] [StreamEQA: Towards Streaming Video Understanding for Embodied Scenarios](https://arxiv.org/abs/2512.04451)
*Yifei Wang,Zhenkai Li,Tianwen Qian,Huanran Zheng,Zheng Wang,Yuqian Fu,Xiaoling Wang*

Main category: cs.CV

TL;DR: StreamEQA：首个针对具身场景下流视频问答的基准，旨在评估模型在连续视觉输入中的感知和推理能力。


<details>
  <summary>Details</summary>
Motivation: 具身智能需要在真实世界部署中持续感知和推理流式视觉输入，而现有模型难以胜任。

Method: 提出 StreamEQA 基准，包含 156 个长视频，定义了 42 个任务，生成约 2.1 万个问题-答案对，通过混合流程结合自动生成和人工改进。

Result: 对 13 个最先进的视频-LLM 的评估表明，尽管在传统基准上表现出色，但这些模型在具身场景下的流视频理解方面仍然存在困难。

Conclusion: StreamEQA 的提出旨在促进具身应用中流视频理解的研究。

Abstract: As embodied intelligence advances toward real-world deployment, the ability to continuously perceive and reason over streaming visual inputs becomes essential. In such settings, an agent must maintain situational awareness of its environment, comprehend the interactions with surrounding entities, and dynamically plan actions informed by past observations, current contexts, and anticipated future events. To facilitate progress in this direction, we introduce StreamEQA, the first benchmark designed for streaming video question answering in embodied scenarios. StreamEQA evaluates existing MLLMs along two orthogonal dimensions: Embodied and Streaming. Along the embodied dimension, we categorize the questions into three levels: perception, interaction, and planning, which progressively assess a model's ability to recognize fine-grained visual details, reason about agent-object interactions, and perform high-level goal-directed reasoning. For the streaming dimension, questions are divided into backward, real-time, and forward reasoning, with each mode relying on a distinct temporal context. Built upon 156 independent long videos, StreamEQA defines 42 tasks and generates approximately 21K question-answer pairs with precise timestamps through a hybrid pipeline combining automated generation and human refinement. Evaluations of 13 state-of-the-art video-LLMs reveal that, despite strong performance on conventional benchmarks, these models still struggle with streaming video understanding in embodied scenarios. We hope StreamEQA will catalyze research on streaming video understanding for embodied applications.

</details>


### [66] [GuidNoise: Single-Pair Guided Diffusion for Generalized Noise Synthesis](https://arxiv.org/abs/2512.04456)
*Changjin Kim,HyeokJun Lee,YoungJoon Yoo*

Main category: cs.CV

TL;DR: 提出了一种名为 GuidNoise 的单对引导扩散方法，用于生成广义噪声，使用单个噪声/干净图像对作为指导，克服了对大量噪声数据的需求。


<details>
  <summary>Details</summary>
Motivation: 现有图像去噪方法依赖生成模型合成真实噪声，但通常需要相机元数据和大量特定目标的噪声-干净图像对，泛化能力有限。

Method: 提出了 GuidNoise，它利用 guidance-aware affine feature modification (GAFM) 和 noise-aware refine loss 来训练扩散模型，从而合成噪声图像。

Result: GuidNoise 能够合成高质量的噪声图像，无需额外的元数据，并且可以高效地生成噪声-干净图像对，从而显著提高去噪性能。

Conclusion: 通过 self-augmentation，GuidNoise 在轻量级模型和有限训练数据的情况下，尤其是在实际场景中，能够有效提升去噪性能。

Abstract: Recent image denoising methods have leveraged generative modeling for real noise synthesis to address the costly acquisition of real-world noisy data. However, these generative models typically require camera metadata and extensive target-specific noisy-clean image pairs, often showing limited generalization between settings. In this paper, to mitigate the prerequisites, we propose a Single-Pair Guided Diffusion for generalized noise synthesis GuidNoise, which uses a single noisy/clean pair as the guidance, often easily obtained by itself within a training set. To train GuidNoise, which generates synthetic noisy images from the guidance, we introduce a guidance-aware affine feature modification (GAFM) and a noise-aware refine loss to leverage the inherent potential of diffusion models. This loss function refines the diffusion model's backward process, making the model more adept at generating realistic noise distributions. The GuidNoise synthesizes high-quality noisy images under diverse noise environments without additional metadata during both training and inference. Additionally, GuidNoise enables the efficient generation of noisy-clean image pairs at inference time, making synthetic noise readily applicable for augmenting training data. This self-augmentation significantly improves denoising performance, especially in practical scenarios with lightweight models and limited training data. The code is available at https://github.com/chjinny/GuidNoise.

</details>


### [67] [SEASON: Mitigating Temporal Hallucination in Video Large Language Models via Self-Diagnostic Contrastive Decoding](https://arxiv.org/abs/2512.04643)
*Chang-Hsun Wu,Kai-Po Chang,Yu-Yang Sheng,Hung-Kai Chung,Kuei-Chun Wang,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: 提出了一种名为自诊断对比解码 (SEASON) 的免训练方法，用于增强视频大语言模型的时间和空间忠实度，从而解决幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视频大语言模型在理解视频中的时间信息方面存在困难，导致产生时间不一致或因果关系不合理的事件描述，从而引发严重的幻觉问题。以往的研究主要集中在空间幻觉上，而对视频理解中的时间推理研究不足。

Method: 提出了一种名为 SEASON 的免训练方法，该方法通过动态诊断每个 token 的幻觉倾向，并针对其相应的时间和空间负例应用自适应对比解码来实现。

Result: 在三个幻觉检测基准测试中，SEASON 优于所有现有的免训练幻觉缓解方法，并在四个通用视频理解基准测试中进一步改进了 VideoLLM。

Conclusion: SEASON 方法有效地提高了视频大语言模型的时间和空间忠实度，缓解了幻觉问题，并在多个基准测试中取得了优异的表现。

Abstract: Video Large Language Models (VideoLLMs) have shown remarkable progress in video understanding. However, these models still struggle to effectively perceive and exploit rich temporal information in videos when responding to user queries. Therefore, they often generate descriptions of events that are temporal inconsistent or causally implausible, causing severe hallucination issues. While most prior studies have focused on spatial hallucinations (e.g. object mismatches), temporal reasoning in video understanding remains relatively underexplored. To address this issue, we propose Self-Diagnostic Contrastive Decoding (SEASON), a training-free method that adaptively enhances temporal and spatial faithfulness for each output token. It achieves this by dynamically diagnosing each token's hallucination tendency and applying adaptive contrastive decoding against its corresponding temporal and spatial negatives. Extensive experiments demonstrate that SEASON outperforms all existing training-free hallucination mitigation approaches on three hallucination examination benchmarks, while further improves VideoLLMs across four general video understanding benchmarks. The code will be released upon acceptance.

</details>


### [68] [dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning](https://arxiv.org/abs/2512.04459)
*Yingzi Ma,Yulong Cao,Wenhao Ding,Shuibai Zhang,Yan Wang,Boris Ivanovic,Ming Jiang,Marco Pavone,Chaowei Xiao*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散的视觉语言模型（dVLM-AD），用于端到端自动驾驶，该模型在保持高层推理和低层规划之间的一致性和可控性方面优于现有的基于自回归（AR）的模型。


<details>
  <summary>Details</summary>
Motivation: 现有的基于自回归的视觉语言模型在端到端自动驾驶中，由于因果注意力和序列token生成，难以保持高层推理和低层规划之间的一致性和可控性。

Method: 提出一种基于扩散的视觉语言模型（dVLM-AD），它统一了感知、结构化推理和低层规划。

Result: 在nuScenes和WOD-E2E数据集上评估，dVLM-AD产生了更一致的推理-行动对，并且在规划性能上与现有的驾驶VLM/VLA系统相当，同时在行为-轨迹一致性方面提高了9%，在长尾WOD-E2E场景中的RFS提高了6%。

Conclusion: 结果表明，对于可扩展的端到端驾驶，基于扩散的视觉语言模型是一条可控且可靠的途径。

Abstract: The autonomous driving community is increasingly focused on addressing the challenges posed by out-of-distribution (OOD) driving scenarios. A dominant research trend seeks to enhance end-to-end (E2E) driving systems by integrating vision-language models (VLMs), leveraging their rich world knowledge and reasoning abilities to improve generalization across diverse environments. However, most existing VLMs or vision-language agents (VLAs) are built upon autoregressive (AR) models. In this paper, we observe that existing AR-based VLMs -- limited by causal attention and sequential token generation -- often fail to maintain consistency and controllability between high-level reasoning and low-level planning. In contrast, recent discrete diffusion VLMs equipped with bidirectional attention exhibit superior controllability and reliability through iterative denoising. Building on these observations, we introduce dVLM-AD, a diffusion-based vision-language model that unifies perception, structured reasoning, and low-level planning for end-to-end driving. Evaluated on nuScenes and WOD-E2E, dVLM-AD yields more consistent reasoning-action pairs and achieves planning performance comparable to existing driving VLM/VLA systems despite a modest backbone, outperforming AR-based baselines with a 9 percent improvement in behavior-trajectory consistency and a 6 percent increase in RFS on long-tail WOD-E2E scenarios. These results suggest a controllable and reliable pathway for scalable end-to-end driving.

</details>


### [69] [UniTS: Unified Time Series Generative Model for Remote Sensing](https://arxiv.org/abs/2512.04461)
*Yuxiang Zhang,Shunlin Liang,Wenyuan Li,Han Ma,Jianglei Xu,Yichuan Ma,Jiangwei Xie,Wei Li,Mengmeng Zhang,Ran Tao,Xiang-Gen Xia*

Main category: cs.CV

TL;DR: 提出了一种统一的时间序列生成模型（UniTS），适用于时间序列重建、云去除、语义变化检测和预测等多种任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要针对不同任务定制专门的模型，缺乏对多个时间序列任务的时空特征的统一建模。

Method: 基于流动匹配生成范式，构建从噪声到目标在特定任务条件下的确定性演化路径；设计自适应条件注入器（ACor）以增强模型对多模态输入的条件感知；设计时空感知调制器（STM）以提高时空块捕获复杂时空依赖性的能力。

Result: UniTS在低级和高级时间序列任务中表现出卓越的生成和认知能力，显著优于现有方法，尤其是在面对严重的云污染、模态缺失和预测物候变化等挑战时。

Conclusion: UniTS是一个通用的框架，适用于各种时间序列任务，包括时间序列重建，时间序列云去除，时间序列语义变化检测，和时间序列预测。

Abstract: One of the primary objectives of satellite remote sensing is to capture the complex dynamics of the Earth environment, which encompasses tasks such as reconstructing continuous cloud-free time series images, detecting land cover changes, and forecasting future surface evolution. However, existing methods typically require specialized models tailored to different tasks, lacking unified modeling of spatiotemporal features across multiple time series tasks. In this paper, we propose a Unified Time Series Generative Model (UniTS), a general framework applicable to various time series tasks, including time series reconstruction, time series cloud removal, time series semantic change detection, and time series forecasting. Based on the flow matching generative paradigm, UniTS constructs a deterministic evolution path from noise to targets under the guidance of task-specific conditions, achieving unified modeling of spatiotemporal representations for multiple tasks. The UniTS architecture consists of a diffusion transformer with spatio-temporal blocks, where we design an Adaptive Condition Injector (ACor) to enhance the model's conditional perception of multimodal inputs, enabling high-quality controllable generation. Additionally, we design a Spatiotemporal-aware Modulator (STM) to improve the ability of spatio-temporal blocks to capture complex spatiotemporal dependencies. Furthermore, we construct two high-quality multimodal time series datasets, TS-S12 and TS-S12CR, filling the gap of benchmark datasets for time series cloud removal and forecasting tasks. Extensive experiments demonstrate that UniTS exhibits exceptional generative and cognitive capabilities in both low-level and high-level time series tasks. It significantly outperforms existing methods, particularly when facing challenges such as severe cloud contamination, modality absence, and forecasting phenological variations.

</details>


### [70] [DeRA: Decoupled Representation Alignment for Video Tokenization](https://arxiv.org/abs/2512.04483)
*Pengbo Guo,Junke Wang,Zhen Xing,Chengxu Liu,Daoguo Dong,Xueming Qian,Zuxuan Wu*

Main category: cs.CV

TL;DR: DeRA: A novel 1D video tokenizer that decouples spatial-temporal representation learning for improved efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To improve training efficiency and performance of video tokenization.

Method: Factorizes video encoding into appearance and motion streams, using Symmetric Alignment-Conflict Projection (SACP) to address gradient conflicts.

Result: Outperforms previous state-of-the-art video tokenizer LARP by 25% on UCF-101 in terms of rFVD. Achieves new state-of-the-art results on UCF-101 class-conditional generation and K600 frame prediction.

Conclusion: DeRA is effective for autoregressive video generation.

Abstract: This paper presents DeRA, a novel 1D video tokenizer that decouples the spatial-temporal representation learning in video tokenization to achieve better training efficiency and performance. Specifically, DeRA maintains a compact 1D latent space while factorizing video encoding into appearance and motion streams, which are aligned with pretrained vision foundation models to capture the spatial semantics and temporal dynamics in videos separately. To address the gradient conflicts introduced by the heterogeneous supervision, we further propose the Symmetric Alignment-Conflict Projection (SACP) module that proactively reformulates gradients by suppressing the components along conflicting directions. Extensive experiments demonstrate that DeRA outperforms LARP, the previous state-of-the-art video tokenizer by 25% on UCF-101 in terms of rFVD. Moreover, using DeRA for autoregressive video generation, we also achieve new state-of-the-art results on both UCF-101 class-conditional generation and K600 frame prediction.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [71] [Solving N-Queen Problem using Las Vegas Algorithm with State Pruning](https://arxiv.org/abs/2512.04139)
*Susmita Sharma,Aayush Shrestha,Sitasma Thapa,Prashant Timalsina,Prakash Poudyal*

Main category: cs.AI

TL;DR: 提出了一种混合算法，用于解决N皇后问题，该算法优于传统回溯法和纯拉斯维加斯算法。


<details>
  <summary>Details</summary>
Motivation: 传统回溯法在解决大规模N皇后问题时效率低下，而纯拉斯维加斯算法性能不稳定。

Method: 该方法在拉斯维加斯算法的基础上，通过迭代剪枝动态消除无效位置来减少搜索空间。

Result: 实验结果表明，该方法比传统回溯法更快地生成有效解。

Conclusion: 该混合算法在计算成本和解的保真度之间取得了平衡，适用于资源受限的计算环境。

Abstract: The N-Queens problem, placing all N queens in a N x N chessboard where none attack the other, is a classic problem for constraint satisfaction algorithms. While complete methods like backtracking guarantee a solution, their exponential time complexity makes them impractical for large-scale instances thus, stochastic approaches, such as Las Vegas algorithm, are preferred. While it offers faster approximate solutions, it suffers from significant performance variance due to random placement of queens on the board. This research introduces a hybrid algorithm built on top of the standard Las Vegas framework through iterative pruning, dynamically eliminating invalid placements during the random assignment phase, thus this method effectively reduces the search space. The analysis results that traditional backtracking scales poorly with increasing N. In contrast, the proposed technique consistently generates valid solutions more rapidly, establishing it as a superior alternative to use where a single, timely solution is preferred over completeness. Although large N causes some performance variability, the algorithm demonstrates a highly effective trade-off between computational cost and solution fidelity, making it particularly suited for resource-constrained computing environments.

</details>


### [72] [RippleBench: Capturing Ripple Effects Using Existing Knowledge Repositories](https://arxiv.org/abs/2512.04144)
*Roy Rinberg,Usha Bhalla,Igor Shilov,Flavio P. Calmon,Rohit Gandikota*

Main category: cs.AI

TL;DR: 这篇论文介绍了一个名为RippleBench-Maker的自动工具，用于生成问答数据集，以评估模型编辑任务中的涟漪效应。该工具基于维基百科的RAG流水线，可以生成与目标概念具有不同语义距离的多项选择题。


<details>
  <summary>Details</summary>
Motivation: 现有的模型干预方法（如非学习、去偏见或模型编辑）旨在修改模型中的特定信息，但其影响通常会传播到相关但非预期的领域，即涟漪效应。为了更好地评估和理解这种效应，需要一个自动化的评估工具。

Method: 该论文提出了RippleBench-Maker，它利用WikiRAG生成多项选择题，并通过控制问题与目标概念的语义距离来评估涟漪效应。论文还构建了一个基于WMDP数据集的基准RippleBench-Bio。

Result: 通过对八种最先进的非学习方法进行评估，发现所有方法在与非学习知识越来越远的主题上都表现出明显的准确性下降，并且每种方法都具有不同的传播特性。

Conclusion: 该论文发布了用于即时涟漪评估的代码库以及基准RippleBench-Bio，以支持未来的研究。

Abstract: Targeted interventions on language models, such as unlearning, debiasing, or model editing, are a central method for refining model behavior and keeping knowledge up to date. While these interventions aim to modify specific information within models (e.g., removing virology content), their effects often propagate to related but unintended areas (e.g., allergies); these side-effects are commonly referred to as the ripple effect. In this work, we present RippleBench-Maker, an automatic tool for generating Q&A datasets that allow for the measurement of ripple effects in any model-editing task. RippleBench-Maker builds on a Wikipedia-based RAG pipeline (WikiRAG) to generate multiple-choice questions at varying semantic distances from the target concept (e.g., the knowledge being unlearned). Using this framework, we construct RippleBench-Bio, a benchmark derived from the WMDP (Weapons of Mass Destruction Paper) dataset, a common unlearning benchmark. We evaluate eight state-of-the-art unlearning methods and find that all exhibit non-trivial accuracy drops on topics increasingly distant from the unlearned knowledge, each with distinct propagation profiles. To support ongoing research, we release our codebase for on-the-fly ripple evaluation, along with the benchmark, RippleBench-Bio.

</details>


### [73] [Are LLMs Truly Multilingual? Exploring Zero-Shot Multilingual Capability of LLMs for Information Retrieval: An Italian Healthcare Use Case](https://arxiv.org/abs/2512.04834)
*Vignesh Kumar Kembu,Pierandrea Morandini,Marta Bianca Maria Ranzini,Antonino Nocera*

Main category: cs.AI

TL;DR: 本研究探讨了开源多语言LLM在理解意大利语EHR和从中提取信息方面的能力，特别关注合并症提取任务。


<details>
  <summary>Details</summary>
Motivation: 传统NLP技术在临床记录信息提取方面存在局限性，而大型语言模型(LLM)在理解和生成类人文本方面表现出色，因此被认为是该领域的一种有效工具。

Method: 通过对EHR进行合并症提取的实验，评估开源多语言LLM的性能。

Result: 研究发现，一些LLM在zero-shot、on-premises设置中表现不佳，并且在泛化到不同疾病时表现出显著的性能变化。

Conclusion: 开源多语言LLM在处理意大利语EHR合并症提取任务时，性能表现不一，部分模型存在泛化能力不足的问题。

Abstract: Large Language Models (LLMs) have become a key topic in AI and NLP, transforming sectors like healthcare, finance, education, and marketing by improving customer service, automating tasks, providing insights, improving diagnostics, and personalizing learning experiences. Information extraction from clinical records is a crucial task in digital healthcare. Although traditional NLP techniques have been used for this in the past, they often fall short due to the complexity, variability of clinical language, and high inner semantics in the free clinical text. Recently, Large Language Models (LLMs) have become a powerful tool for better understanding and generating human-like text, making them highly effective in this area. In this paper, we explore the ability of open-source multilingual LLMs to understand EHRs (Electronic Health Records) in Italian and help extract information from them in real-time. Our detailed experimental campaign on comorbidity extraction from EHR reveals that some LLMs struggle in zero-shot, on-premises settings, and others show significant variation in performance, struggling to generalize across various diseases when compared to native pattern matching and manual annotations.

</details>


### [74] [Orchestrator Multi-Agent Clinical Decision Support System for Secondary Headache Diagnosis in Primary Care](https://arxiv.org/abs/2512.04207)
*Xizhi Wu,Nelly Estefanie Garduno-Rapp,Justin F Rousseau,Mounika Thakkallapally,Hang Zhang,Yuelyu Ji,Shyam Visweswaran,Yifan Peng,Yanshan Wang*

Main category: cs.AI

TL;DR: 本研究提出了一种基于大型语言模型（LLM）的多智能体临床决策支持系统，用于从自由文本临床案例中诊断继发性头痛。


<details>
  <summary>Details</summary>
Motivation: 在初级保健环境中，识别需要紧急评估的头痛患者仍然具有挑战性，因为临床医生工作时间有限、信息不完整且症状表现多样。

Method: 该系统将诊断分解为七个领域专业智能体，每个智能体生成结构化且基于证据的理由，而中央协调器执行任务分解并协调智能体路由。使用90个专家验证的继发性头痛病例评估了多智能体系统，并将其性能与单个LLM基线进行了比较。

Result: 多智能体系统与基于临床实践指南的提示（GPrompt）一致地实现了最高的F1分数，较小的模型获得了更大的收益。

Conclusion: 结构化多智能体推理提高了准确性，并为继发性头痛诊断中的可解释决策支持提供了一种透明的、临床对齐的方法。

Abstract: Unlike most primary headaches, secondary headaches need specialized care and can have devastating consequences if not treated promptly. Clinical guidelines highlight several 'red flag' features, such as thunderclap onset, meningismus, papilledema, focal neurologic deficits, signs of temporal arteritis, systemic illness, and the 'worst headache of their life' presentation. Despite these guidelines, determining which patients require urgent evaluation remains challenging in primary care settings. Clinicians often work with limited time, incomplete information, and diverse symptom presentations, which can lead to under-recognition and inappropriate care. We present a large language model (LLM)-based multi-agent clinical decision support system built on an orchestrator-specialist architecture, designed to perform explicit and interpretable secondary headache diagnosis from free-text clinical vignettes. The multi-agent system decomposes diagnosis into seven domain-specialized agents, each producing a structured and evidence-grounded rationale, while a central orchestrator performs task decomposition and coordinates agent routing. We evaluated the multi-agent system using 90 expert-validated secondary headache cases and compared its performance with a single-LLM baseline across two prompting strategies: question-based prompting (QPrompt) and clinical practice guideline-based prompting (GPrompt). We tested five open-source LLMs (Qwen-30B, GPT-OSS-20B, Qwen-14B, Qwen-8B, and Llama-3.1-8B), and found that the orchestrated multi-agent system with GPrompt consistently achieved the highest F1 scores, with larger gains in smaller models. These findings demonstrate that structured multi-agent reasoning improves accuracy beyond prompt engineering alone and offers a transparent, clinically aligned approach for explainable decision support in secondary headache diagnosis.

</details>


### [75] [Balancing Safety and Helpfulness in Healthcare AI Assistants through Iterative Preference Alignment](https://arxiv.org/abs/2512.04210)
*Huy Nghiem,Swetasudha Panda,Devashish Khatwani,Huy V. Nguyen,Krishnaram Kenthapadi,Hal Daumé*

Main category: cs.AI

TL;DR: 本研究提出了一种迭代的部署后对齐框架，利用KTO和DPO来提高LLM在医疗领域的安全性。


<details>
  <summary>Details</summary>
Motivation: 确保大型语言模型（LLM）在医疗保健中的安全性和可信度仍然是部署的障碍。对话式医疗助手必须避免不安全合规，同时避免过度拒绝良性查询。

Method: 使用Kahneman-Tversky Optimization (KTO)和Direct Preference Optimization (DPO)对模型进行迭代优化，以应对特定领域的安全信号。

Result: 在CARES-18K基准测试中，有害查询检测的安全相关指标提高了42%，同时也发现了架构相关的校准偏差。

Conclusion: 在设计对话式医疗助手时，平衡患者安全、用户信任和临床效用至关重要。

Abstract: Large Language Models (LLMs) are increasingly used in healthcare, yet ensuring their safety and trustworthiness remains a barrier to deployment. Conversational medical assistants must avoid unsafe compliance without over-refusing benign queries. We present an iterative post-deployment alignment framework that applies Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) to refine models against domain-specific safety signals. Using the CARES-18K benchmark for adversarial robustness, we evaluate four LLMs (Llama-3B/8B, Meditron-8B, Mistral-7B) across multiple cycles. Our results show up to 42% improvement in safety-related metrics for harmful query detection, alongside interesting trade-offs against erroneous refusals, thereby exposing architecture-dependent calibration biases. We also perform ablation studies to identify when self-evaluation is reliable and when external or finetuned judges are necessary to maximize performance gains. Our findings underscore the importance of adopting best practices that balance patient safety, user trust, and clinical utility in the design of conversational medical assistants.

</details>


### [76] [Educational Cone Model in Embedding Vector Spaces](https://arxiv.org/abs/2512.04227)
*Yo Ehara*

Main category: cs.AI

TL;DR: 本研究提出了一种基于几何框架的教育锥模型，用于评估文本嵌入方法在分析文本难度方面的适用性，通过优化问题和特定的损失函数，该模型能够快速有效地识别与难度标注的教育文本对齐的最佳嵌入空间。


<details>
  <summary>Details</summary>
Motivation: 现有的文本难度分析依赖于人工标注的数据集和嵌入向量空间，但嵌入方法的选择是一个挑战。

Method: 提出了教育锥模型，该模型基于更容易的文本更不具多样性，而更难的文本更多样性的假设，构建了一个几何框架，并通过优化问题和特定的损失函数来评估嵌入。

Result: 在真实数据集上的实验验证了该模型在识别与难度标注的教育文本对齐的最佳嵌入空间方面的有效性和速度。

Conclusion: 该研究提出的教育锥模型能够有效且快速地识别适合分析文本难度的嵌入空间。

Abstract: Human-annotated datasets with explicit difficulty ratings are essential in intelligent educational systems. Although embedding vector spaces are widely used to represent semantic closeness and are promising for analyzing text difficulty, the abundance of embedding methods creates a challenge in selecting the most suitable method. This study proposes the Educational Cone Model, which is a geometric framework based on the assumption that easier texts are less diverse (focusing on fundamental concepts), whereas harder texts are more diverse. This assumption leads to a cone-shaped distribution in the embedding space regardless of the embedding method used. The model frames the evaluation of embeddings as an optimization problem with the aim of detecting structured difficulty-based patterns. By designing specific loss functions, efficient closed-form solutions are derived that avoid costly computation. Empirical tests on real-world datasets validated the model's effectiveness and speed in identifying the embedding spaces that are best aligned with difficulty-annotated educational texts.

</details>


### [77] [Addressing Logical Fallacies In Scientific Reasoning From Large Language Models: Towards a Dual-Inference Training Framework](https://arxiv.org/abs/2512.04228)
*Peter B. Walker,Hannah Davidson,Aiden Foster,Matthew Lienert,Thomas Pardue,Dale Russell*

Main category: cs.AI

TL;DR: 大型语言模型在自然语言处理领域取得了显著进展，但在科学推理方面存在缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型主要基于肯定式推理，容易出现逻辑谬误和对抗性操纵，在因果推理方面存在不足。

Method: 提出了一种双重推理训练框架，将肯定式生成与结构化反事实否定相结合。

Result: 该框架能够提高模型在否定、反例和错误前提下的推理能力。

Conclusion: 通过结合生成式合成和显式的否定感知目标，该框架可以产生更具弹性、可解释性且与人类推理对齐的系统。

Abstract: Large Language Models (LLMs) have transformed natural language processing and hold growing promise for advancing science, healthcare, and decision-making. Yet their training paradigms remain dominated by affirmation-based inference, akin to \textit{modus ponens}, where accepted premises yield predicted consequents. While effective for generative fluency, this one-directional approach leaves models vulnerable to logical fallacies, adversarial manipulation, and failures in causal reasoning. This paper makes two contributions. First, it demonstrates how existing LLMs from major platforms exhibit systematic weaknesses when reasoning in scientific domains with negation, counterexamples, or faulty premises \footnote{Code to recreate these experiments are at https://github.com/hannahdavidsoncollege-maker/ScientificReasoningForEnvironment-MedicineWithLLMs. Second, it introduces a dual-reasoning training framework that integrates affirmative generation with structured counterfactual denial. Grounded in formal logic, cognitive science, and adversarial training, this training paradigm formalizes a computational analogue of ``denying the antecedent'' as a mechanism for disconfirmation and robustness. By coupling generative synthesis with explicit negation-aware objectives, the framework enables models that not only affirm valid inferences but also reject invalid ones, yielding systems that are more resilient, interpretable, and aligned with human reasoning.

</details>


### [78] [Toward Virtuous Reinforcement Learning](https://arxiv.org/abs/2512.04246)
*Majid Ghasemi,Mark Crowley*

Main category: cs.AI

TL;DR: 本文批评了强化学习（RL）中常见的机器伦理模式，并提出了一种以 virtue 为中心的替代方案。


<details>
  <summary>Details</summary>
Motivation: 目前文献中的两个局限性：(i) 基于规则的方法在模糊性和非平稳性下存在困难，并且不能培养持久的习惯；(ii) 许多基于奖励的方法将不同的道德考虑压缩成一个单一的标量信号，这会模糊权衡并导致代理博弈。

Method: 将伦理视为策略层面的性格，即在激励、伙伴或环境发生变化时保持相对稳定的习惯。结合四个组成部分：(1) 多智能体强化学习中的社会学习；(2) 多目标和约束公式；(3) 基于亲和力的正则化；(4) 将不同的伦理传统作为实际控制信号进行操作。

Result: 将评估从规则检查或标量回报转变为特征总结、干预下的持久性以及道德权衡的显式报告。

Conclusion: 本文为道德强化学习提供了一个新的方向，强调了道德习惯的培养和道德价值的显式表达。

Abstract: This paper critiques common patterns in machine ethics for Reinforcement Learning (RL) and argues for a virtue focused alternative. We highlight two recurring limitations in much of the current literature: (i) rule based (deontological) methods that encode duties as constraints or shields often struggle under ambiguity and nonstationarity and do not cultivate lasting habits, and (ii) many reward based approaches, especially single objective RL, implicitly compress diverse moral considerations into a single scalar signal, which can obscure trade offs and invite proxy gaming in practice. We instead treat ethics as policy level dispositions, that is, relatively stable habits that hold up when incentives, partners, or contexts change. This shifts evaluation beyond rule checks or scalar returns toward trait summaries, durability under interventions, and explicit reporting of moral trade offs. Our roadmap combines four components: (1) social learning in multi agent RL to acquire virtue like patterns from imperfect but normatively informed exemplars; (2) multi objective and constrained formulations that preserve value conflicts and incorporate risk aware criteria to guard against harm; (3) affinity based regularization toward updateable virtue priors that support trait like stability under distribution shift while allowing norms to evolve; and (4) operationalizing diverse ethical traditions as practical control signals, making explicit the value and cultural assumptions that shape ethical RL benchmarks.

</details>


### [79] [The Geometry of Benchmarks: A New Path Toward AGI](https://arxiv.org/abs/2512.04276)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 论文提出了一个用于评估人工智能进展的几何框架，将所有心理测量电池视为结构化模空间中的点，并用能力泛函描述智能体的性能。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能评估实践仅在孤立的测试集上评估模型，并且对于通用性或自主自我改进的推理几乎没有指导。

Method: 1. 定义了一个自主AI（AAI）等级，这是一个基于跨任务系列的可衡量性能的Kardashev式自主等级。
2. 构建了一个电池的模空间，识别了在智能体排序和能力推断层面上无法区分的基准的等价类。
3. 引入了一个通用的生成器-验证器-更新器（GVU）算子，它包含了强化学习、自我对弈、辩论和基于验证器的微调作为特例，并定义了一个自我改进系数κ。

Result: 导出了确定性结果：密集的电池族足以证明在整个任务空间区域的性能。推导了生成和验证的组合噪声的方差不等式，为κ> 0提供了充分条件。

Conclusion: 迈向通用人工智能（AGI）的进展最好被理解为基准模空间上的流动，由GVU动态驱动，而不是由单个排行榜上的分数驱动。

Abstract: Benchmarks are the primary tool for assessing progress in artificial intelligence (AI), yet current practice evaluates models on isolated test suites and provides little guidance for reasoning about generality or autonomous self-improvement. Here we introduce a geometric framework in which all psychometric batteries for AI agents are treated as points in a structured moduli space, and agent performance is described by capability functionals over this space. First, we define an Autonomous AI (AAI) Scale, a Kardashev-style hierarchy of autonomy grounded in measurable performance on batteries spanning families of tasks (for example reasoning, planning, tool use and long-horizon control). Second, we construct a moduli space of batteries, identifying equivalence classes of benchmarks that are indistinguishable at the level of agent orderings and capability inferences. This geometry yields determinacy results: dense families of batteries suffice to certify performance on entire regions of task space. Third, we introduce a general Generator-Verifier-Updater (GVU) operator that subsumes reinforcement learning, self-play, debate and verifier-based fine-tuning as special cases, and we define a self-improvement coefficient $κ$ as the Lie derivative of a capability functional along the induced flow. A variance inequality on the combined noise of generation and verification provides sufficient conditions for $κ> 0$. Our results suggest that progress toward artificial general intelligence (AGI) is best understood as a flow on moduli of benchmarks, driven by GVU dynamics rather than by scores on individual leaderboards.

</details>


### [80] [Artificial Intelligence Applications in Horizon Scanning for Infectious Diseases](https://arxiv.org/abs/2512.04287)
*Ian Miles,Mayumi Wakimoto,Wagner Meira,Daniela Paula,Daylene Ticiane,Bruno Rosa,Jane Biddulph,Stelios Georgiou,Valdir Ermida*

Main category: cs.AI

TL;DR: 本综述探讨了人工智能在远景扫描中的应用，重点关注与传染病相关的新兴威胁和机遇的识别和响应。


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能工具如何加强信号检测、数据监测、情景分析和决策支持。

Method: 通过文献回顾和分析。

Result: 展示了人工智能在公共卫生准备方面的潜力和局限性。

Conclusion: 强调了有效实施和治理人工智能的策略，并为远见文献做出了贡献。

Abstract: This review explores the integration of Artificial Intelligence into Horizon Scanning, focusing on identifying and responding to emerging threats and opportunities linked to Infectious Diseases. We examine how AI tools can enhance signal detection, data monitoring, scenario analysis, and decision support. We also address the risks associated with AI adoption and propose strategies for effective implementation and governance. The findings contribute to the growing body of Foresight literature by demonstrating the potential and limitations of AI in Public Health preparedness.

</details>


### [81] [Towards better dense rewards in Reinforcement Learning Applications](https://arxiv.org/abs/2512.04302)
*Shuyuan Zhang*

Main category: cs.AI

TL;DR: 构建有意义和准确的密集奖励是强化学习中的一项基本任务，它可以使智能体更有效地探索环境。


<details>
  <summary>Details</summary>
Motivation: 当奖励信号稀疏、延迟或与预期任务目标不一致时，智能体通常难以有效地学习。密集奖励函数通过塑造智能体的行为和加速学习，提供了一个潜在的解决方案。然而，设计不当的奖励函数可能导致意想不到的行为、奖励入侵或低效的探索。

Method: 探索了几种处理这些尚未解决的问题的方法，并增强了不同RL应用中密集奖励构建的有效性和可靠性。

Result: 最近的研究探索了各种方法，包括逆强化学习、基于人类偏好的奖励建模以及内在奖励的自我监督学习。

Conclusion: 虽然这些方法提供了有希望的方向，但它们通常需要在通用性、可扩展性和与人类意图对齐之间进行权衡。

Abstract: Finding meaningful and accurate dense rewards is a fundamental task in the field of reinforcement learning (RL) that enables agents to explore environments more efficiently. In traditional RL settings, agents learn optimal policies through interactions with an environment guided by reward signals. However, when these signals are sparse, delayed, or poorly aligned with the intended task objectives, agents often struggle to learn effectively. Dense reward functions, which provide informative feedback at every step or state transition, offer a potential solution by shaping agent behavior and accelerating learning. Despite their benefits, poorly crafted reward functions can lead to unintended behaviors, reward hacking, or inefficient exploration. This problem is particularly acute in complex or high-dimensional environments where handcrafted rewards are difficult to specify and validate. To address this, recent research has explored a variety of approaches, including inverse reinforcement learning, reward modeling from human preferences, and self-supervised learning of intrinsic rewards. While these methods offer promising directions, they often involve trade-offs between generality, scalability, and alignment with human intent. This proposal explores several approaches to dealing with these unsolved problems and enhancing the effectiveness and reliability of dense reward construction in different RL applications.

</details>


### [82] [A Conceptual Model for AI Adoption in Financial Decision-Making: Addressing the Unique Challenges of Small and Medium-Sized Enterprises](https://arxiv.org/abs/2512.04339)
*Manh Chien Vu,Thang Le Dinh,Manh Chien Vu,Tran Duc Le,Thi Lien Huong Nguyen*

Main category: cs.AI

TL;DR: AI有潜力改变中小企业(SME)，特别是在加强财务决策过程方面。但中小企业在实施人工智能技术时，往往面临资源、技术和数据管理能力等重大障碍。本文提出了一个在中小企业财务决策中采用人工智能的概念模型。


<details>
  <summary>Details</summary>
Motivation: 中小企业在实施人工智能技术时，往往面临资源、技术和数据管理能力等重大障碍。

Method: 该模型分为几个层级：数据源、数据处理和集成、AI模型部署、决策支持和自动化、验证和风险管理。

Result: 通过逐步实施人工智能，中小企业可以优化财务预测、预算、投资策略和风险管理。

Conclusion: 本研究总结了中小企业采用人工智能驱动的财务流程的影响，并提出了未来中小企业金融人工智能应用的研究方向。

Abstract: The adoption of artificial intelligence (AI) offers transformative potential for small and medium-sized enterprises (SMEs), particularly in enhancing financial decision-making processes. However, SMEs often face significant barriers to implementing AI technologies, including limited resources, technical expertise, and data management capabilities. This paper presents a conceptual model for the adoption of AI in financial decision-making for SMEs. The proposed model addresses key challenges faced by SMEs, including limited resources, technical expertise, and data management capabilities. The model is structured into layers: data sources, data processing and integration, AI model deployment, decision support and automation, and validation and risk management. By implementing AI incrementally, SMEs can optimize financial forecasting, budgeting, investment strategies, and risk management. This paper highlights the importance of data quality and continuous model validation, providing a practical roadmap for SMEs to integrate AI into their financial operations. The study concludes with implications for SMEs adopting AI-driven financial processes and suggests areas for future research in AI applications for SME finance.

</details>


### [83] [Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning](https://arxiv.org/abs/2512.04359)
*Hongye Cao,Zhixin Bai,Ziyue Peng,Boyan Wang,Tianpei Yang,Jing Huo,Yuyao Zhang,Yang Gao*

Main category: cs.AI

TL;DR: 该论文提出了一种新的强化学习框架，旨在解决现有方法中存在的熵崩溃问题，从而提升大型语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于可验证奖励的强化学习方法在提高大型语言模型的推理能力方面表现出色，但容易出现熵崩溃，限制了策略探索和推理能力。

Method: 该方法从数据和算法两个层面入手。在数据层面，引入语义熵引导的课程学习，由易到难地组织训练数据。在算法层面，采用非均匀token处理，对低熵token施加KL正则化，并对这些token内的高协方差部分施加更强的约束。

Result: 在6个基准测试和3个不同参数规模的基础模型上的实验结果表明，该方法在提高推理能力方面优于其他基于熵的方法。

Conclusion: 通过联合优化数据组织和算法设计，该方法有效地缓解了熵崩溃，并增强了大型语言模型的推理能力。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has demonstrated superior performance in enhancing the reasoning capability of large language models (LLMs). However, this accuracy-oriented learning paradigm often suffers from entropy collapse, which reduces policy exploration and limits reasoning capabilities. To address this challenge, we propose an efficient reinforcement learning framework that leverages entropy signals at both the semantic and token levels to improve reasoning. From the data perspective, we introduce semantic entropy-guided curriculum learning, organizing training data from low to high semantic entropy to guide progressive optimization from easier to more challenging tasks. For the algorithmic design, we adopt non-uniform token treatment by imposing KL regularization on low-entropy tokens that critically impact policy exploration and applying stronger constraints on high-covariance portions within these tokens. By jointly optimizing data organization and algorithmic design, our method effectively mitigates entropy collapse and enhances LLM reasoning. Experimental results across 6 benchmarks with 3 different parameter-scale base models demonstrate that our method outperforms other entropy-based approaches in improving reasoning.

</details>


### [84] [AgentBay: A Hybrid Interaction Sandbox for Seamless Human-AI Intervention in Agentic Systems](https://arxiv.org/abs/2512.04367)
*Yun Piao,Hongbo Min,Hang Su,Leilei Zhang,Lei Wang,Yue Yin,Xiao Wu,Zhejing Xu,Liwei Qu,Hang Li,Xinxin Zeng,Wei Tian,Fei Yu,Xiaowei Li,Jiayi Jiang,Tongxu Liu,Hao Tian,Yufei Que,Xiaobing Tu,Bing Suo,Yuebing Li,Xiangting Chen,Zeen Zhao,Jiaming Tang,Wei Huang,Xuguang Li,Jing Zhao,Jin Li,Jie Shen,Jinkui Ren,Xiantao Zhang*

Main category: cs.AI

TL;DR: AgentBay是一个为混合人机交互设计的沙盒服务，旨在提高自主AI Agent在现实世界异常情况下的可靠性。


<details>
  <summary>Details</summary>
Motivation: 自主AI Agent在面对现实世界的异常情况时仍然脆弱，需要人机回路（HITL）监督。

Method: 提出AgentBay，一个新颖的沙盒服务，它提供跨多个平台的安全、隔离的执行环境，并使用自适应流协议（ASP）实现无缝的人工干预。

Result: AgentBay（Agent + Human）模型在复杂任务的基准测试中成功率提高了48%以上。与标准RDP相比，ASP协议降低了高达50%的带宽消耗，并在端到端延迟方面降低了约5%，尤其是在较差的网络条件下。

Conclusion: AgentBay为构建下一代可靠的、人工监督的自主系统提供了一个基础原型。

Abstract: The rapid advancement of Large Language Models (LLMs) is catalyzing a shift towards autonomous AI Agents capable of executing complex, multi-step tasks. However, these agents remain brittle when faced with real-world exceptions, making Human-in-the-Loop (HITL) supervision essential for mission-critical applications. In this paper, we present AgentBay, a novel sandbox service designed from the ground up for hybrid interaction. AgentBay provides secure, isolated execution environments spanning Windows, Linux, Android, Web Browsers, and Code interpreters. Its core contribution is a unified session accessible via a hybrid control interface: An AI agent can interact programmatically via mainstream interfaces (MCP, Open Source SDK), while a human operator can, at any moment, seamlessly take over full manual control. This seamless intervention is enabled by Adaptive Streaming Protocol (ASP). Unlike traditional VNC/RDP, ASP is specifically engineered for this hybrid use case, delivering an ultra-low-latency, smoother user experience that remains resilient even in weak network environments. It achieves this by dynamically blending command-based and video-based streaming, adapting its encoding strategy based on network conditions and the current controller (AI or human). Our evaluation demonstrates strong results in security, performance, and task completion rates. In a benchmark of complex tasks, the AgentBay (Agent + Human) model achieved more than 48% success rate improvement. Furthermore, our ASP protocol reduces bandwidth consumption by up to 50% compared to standard RDP, and in end-to-end latency with around 5% reduction, especially under poor network conditions. We posit that AgentBay provides a foundational primitive for building the next generation of reliable, human-supervised autonomous systems.

</details>


### [85] [Executable Governance for AI: Translating Policies into Rules Using LLMs](https://arxiv.org/abs/2512.04408)
*Gautam Varma Datla,Anudeep Vurity,Tejaswani Dash,Tazeem Ahmad,Mohd Adnan,Saima Rafi*

Main category: cs.AI

TL;DR: 提出了一个名为 Policy-to-Tests (P2T) 的框架，可以将自然语言策略文档转换为机器可读的规则。


<details>
  <summary>Details</summary>
Motivation: 现有AI策略指南主要以散文形式撰写，需要从业者手动转换为可执行规则，这个过程缓慢、容易出错且难以扩展。

Method: 该框架包含一个管道和一个紧凑的领域特定语言 (DSL)，用于编码危害、范围、条件、例外和所需证据，从而产生提取规则的规范表示。

Result: AI 生成的规则在跨度级别和规则级别指标上与强大的人工基线非常匹配，并且在黄金数据集上具有强大的注释者间协议。将 HIPAA 衍生的保障措施添加到生成代理，并将其与没有防护栏的相同代理进行比较，验证了下游行为和安全影响。

Conclusion: 发布代码库、DSL、提示和规则集作为开源资源，以实现可重复的评估。

Abstract: AI policy guidance is predominantly written as prose, which practitioners must first convert into executable rules before frameworks can evaluate or enforce them. This manual step is slow, error-prone, difficult to scale, and often delays the use of safeguards in real-world deployments. To address this gap, we present Policy-to-Tests (P2T), a framework that converts natural-language policy documents into normalized, machine-readable rules. The framework comprises a pipeline and a compact domain-specific language (DSL) that encodes hazards, scope, conditions, exceptions, and required evidence, yielding a canonical representation of extracted rules. To test the framework beyond a single policy, we apply it across general frameworks, sector guidance, and enterprise standards, extracting obligation-bearing clauses and converting them into executable rules. These AI-generated rules closely match strong human baselines on span-level and rule-level metrics, with robust inter-annotator agreement on the gold set. To evaluate downstream behavioral and safety impact, we add HIPAA-derived safeguards to a generative agent and compare it with an otherwise identical agent without guardrails. An LLM-based judge, aligned with gold-standard criteria, measures violation rates and robustness to obfuscated and compositional prompts. Detailed results are provided in the appendix. We release the codebase, DSL, prompts, and rule sets as open-source resources to enable reproducible evaluation.

</details>


### [86] [GovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows](https://arxiv.org/abs/2512.04416)
*Zhou Liu,Zhaoyang Han,Guochen Yan,Hao Liang,Bohan Zeng,Xing Chen,Yuanfeng Song,Wentao Zhang*

Main category: cs.AI

TL;DR: 提出了GovBench，一个用于评估数据治理自动化任务的基准，并提出了DataGovAgent框架以解决现有模型在复杂数据治理任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有自动数据科学的基准不能捕捉数据治理的独特挑战，即确保数据本身的正确性和质量。

Method: 构建了一个包含150个真实场景任务的GovBench基准，使用“反向目标”方法合成噪声，并采用严格的指标评估端到端流程的可靠性。提出了DataGovAgent框架，该框架采用Planner-Executor-Evaluator架构，集成了基于约束的规划、检索增强生成和沙盒反馈驱动的调试。

Result: DataGovAgent在复杂任务上的平均任务得分（ATS）从39.7提升到54.9，并减少了超过77.9%的调试迭代次数。

Conclusion: 目前的大模型在复杂的多步骤工作流程中表现不佳，缺乏强大的纠错机制，而DataGovAgent框架能显著提升数据治理任务的性能。

Abstract: Data governance ensures data quality, security, and compliance through policies and standards, a critical foundation for scaling modern AI development. Recently, large language models (LLMs) have emerged as a promising solution for automating data governance by translating user intent into executable transformation code. However, existing benchmarks for automated data science often emphasize snippet-level coding or high-level analytics, failing to capture the unique challenge of data governance: ensuring the correctness and quality of the data itself. To bridge this gap, we introduce GovBench, a benchmark featuring 150 diverse tasks grounded in real-world scenarios, built on data from actual cases. GovBench employs a novel "reversed-objective" methodology to synthesize realistic noise and utilizes rigorous metrics to assess end-to-end pipeline reliability. Our analysis on GovBench reveals that current models struggle with complex, multi-step workflows and lack robust error-correction mechanisms. Consequently, we propose DataGovAgent, a framework utilizing a Planner-Executor-Evaluator architecture that integrates constraint-based planning, retrieval-augmented generation, and sandboxed feedback-driven debugging. Experimental results show that DataGovAgent significantly boosts the Average Task Score (ATS) on complex tasks from 39.7 to 54.9 and reduces debugging iterations by over 77.9 percent compared to general-purpose baselines.

</details>


### [87] [Solving LLM Repetition Problem in Production: A Comprehensive Study of Multiple Solutions](https://arxiv.org/abs/2512.04419)
*Weiwei Wang,Weijie Zou,Jiyong Min*

Main category: cs.AI

TL;DR: 大型语言模型(llms)中的重复问题会导致严重的性能下降和系统停滞。本文对实际批量代码解释任务中遇到的重复问题进行了全面的调查，并提出了多个实用的解决方案。


<details>
  <summary>Details</summary>
Motivation: 确定重复问题的根本原因是贪婪解码无法逃脱重复循环，以及自我强化效应。

Method: 通过基于马尔可夫模型的严格理论分析，结合全面的实验评估，验证了三种可行的解决方案。

Result: 结果表明，使用early_stopping=True的束搜索解码可以有效地解决所有三种重复模式。presence_penalty超参数为特定BadCase 1提供了一个有效的解决方案。直接偏好优化(dpo)微调为所有三个BadCase提供了一个通用的模型级解决方案。

Conclusion: 这项工作的主要价值在于将第一手的生产经验与广泛的实验验证相结合。

Abstract: The repetition problem, where Large Language Models (LLMs) continuously generate repetitive content without proper termination, poses a critical challenge in production deployments, causing severe performance degradation and system stalling. This paper presents a comprehensive investigation and multiple practical solutions for the repetition problem encountered in real-world batch code interpretation tasks.
  We identify three distinct repetition patterns: (1) business rule generation repetition, (2) method call relationship analysis repetition, and (3) PlantUML diagram syntax generation repetition. Through rigorous theoretical analysis based on Markov models, we establish that the root cause lies in greedy decoding's inability to escape repetitive loops, exacerbated by self-reinforcement effects.
  Our comprehensive experimental evaluation demonstrates three viable solutions: (1) Beam Search decoding with early_stopping=True serves as a universal post-hoc mechanism that effectively resolves all three repetition patterns; (2) presence_penalty hyperparameter provides an effective solution specifically for BadCase 1; and (3) Direct Preference Optimization (DPO) fine-tuning offers a universal model-level solution for all three BadCases.
  The primary value of this work lies in combining first-hand production experience with extensive experimental validation. Our main contributions include systematic theoretical analysis of repetition mechanisms, comprehensive evaluation of multiple solutions with task-specific applicability mapping, identification of early_stopping as the critical parameter for Beam Search effectiveness, and practical production-ready solutions validated in real deployment environments.

</details>


### [88] [TaskEval: Synthesised Evaluation for Foundation-Model Tasks](https://arxiv.org/abs/2512.04442)
*Dilani Widanapathiranage,Scott Barnett,Stefanus Kurniawan,Wannita Takerngsaksiri*

Main category: cs.AI

TL;DR: 这篇论文提出了一种合成FM任务特定评估器程序的方法，该程序提供自动化和用于捕获反馈的自定义UI。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法或基准数据集不能帮助软件团队解决特定任务的FM应用，当没有指标或数据集时。

Method: 该方法包括：(1) 捕获任何FM任务属性的任务无关元模型，(2) 有效利用人工反馈的交互协议，(3) 选择或生成一组适当评估的评估合成器。

Result: 在图表数据提取和文档问答两个不同的FM任务上的初步评估表明，所选评估的准确率分别为93%和90%。

Conclusion: 该研究解决了工程团队面临的一个日益严重的问题，即如何评估和审查FM任务的输出。

Abstract: Hallucinations are a key concern when creating applications that rely on Foundation models (FMs). Understanding where and how these subtle failures occur in an application relies on evaluation methods known as \textit{evals}. Prior work focuses on defining new eval methods or benchmark datasets for specific tasks. However, neither helps a software team with a task-specific FM application when there is no metric or dataset. The demand for both automated approaches and deep integration of human insight makes this a challenging problem. We address this gap by proposing an approach to synthesise a FM task-specific evaluator program that provides automation and a custom UI for capturing feedback. The core novelty of our approach lies in: (1) a task-agnostic meta-model that captures properties of any FM task, (2) an interaction protocol for efficient use of human feedback, and (3) an eval synthesiser that selects or generates an appropriate set of evals. We implement our approach in \toolname and demonstrate the concept on two diverse FM tasks: chart data extraction and document question answering. A preliminary evaluation on the quality of our selected evals shows 93\% and 90\% accuracy respectively. Our research tackles a growing problem facing engineering teams, how to evaluate and review outputs from FM tasks.

</details>


### [89] [MARL Warehouse Robots](https://arxiv.org/abs/2512.04463)
*Price Allman,Lian Thang,Dre Simmons,Salmon Riaz*

Main category: cs.AI

TL;DR: 这篇论文比较了用于协同仓库机器人的多智能体强化学习 (MARL) 算法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索 MARL 在仓库机器人中的应用。

Method: 作者在 Robotic Warehouse (RWARE) 环境和一个定制的 Unity 3D 模拟中评估了 QMIX 和 IPPO 算法。

Result: 实验结果表明，QMIX 的价值分解明显优于独立学习方法，但需要大量的超参数调整。在 Unity ML-Agents 中成功部署，经过 100 万次训练步骤后实现了持续的包裹交付。

Conclusion: 虽然 MARL 在小型部署中显示出希望，但仍然存在重大的扩展挑战。

Abstract: We present a comparative study of multi-agent reinforcement learning (MARL) algorithms for cooperative warehouse robotics. We evaluate QMIX and IPPO on the Robotic Warehouse (RWARE) environment and a custom Unity 3D simulation. Our experiments reveal that QMIX's value decomposition significantly outperforms independent learning approaches (achieving 3.25 mean return vs. 0.38 for advanced IPPO), but requires extensive hyperparameter tuning -- particularly extended epsilon annealing (5M+ steps) for sparse reward discovery. We demonstrate successful deployment in Unity ML-Agents, achieving consistent package delivery after 1M training steps. While MARL shows promise for small-scale deployments (2-4 robots), significant scaling challenges remain. Code and analyses: https://pallman14.github.io/MARL-QMIX-Warehouse-Robots/

</details>


### [90] [Mathematical Framing for Different Agent Strategies](https://arxiv.org/abs/2512.04469)
*Philip Stephens,Emmanuel Salawu*

Main category: cs.AI

TL;DR: 提出了一个统一的数学和概率框架，用于理解和比较不同的AI agent策略。


<details>
  <summary>Details</summary>
Motivation: 弥合了高级agent设计概念（如ReAct、多agent系统和控制流）与严格的数学公式之间的差距。

Method: 将agent过程构建为概率链，从而能够详细分析不同的策略如何操纵这些概率以实现期望的结果。

Result: 引入了“自由度”的概念，直观地区分了每种方法可用的可优化杠杆，从而指导选择适合特定任务的策略。

Conclusion: 旨在提高设计和评估AI agent的清晰度和精确度，从而深入了解如何在复杂的agent系统中最大化成功行动的概率。

Abstract: We introduce a unified mathematical and probabilistic framework for understanding and comparing diverse AI agent strategies. We bridge the gap between high-level agent design concepts, such as ReAct, multi-agent systems, and control flows, and a rigorous mathematical formulation. Our approach frames agentic processes as a chain of probabilities, enabling a detailed analysis of how different strategies manipulate these probabilities to achieve desired outcomes. Our framework provides a common language for discussing the trade-offs inherent in various agent architectures. One of our many key contributions is the introduction of the "Degrees of Freedom" concept, which intuitively differentiates the optimizable levers available for each approach, thereby guiding the selection of appropriate strategies for specific tasks. This work aims to enhance the clarity and precision in designing and evaluating AI agents, offering insights into maximizing the probability of successful actions within complex agentic systems.

</details>


### [91] [AI-Assisted Game Management Decisions: A Fuzzy Logic Approach to Real-Time Substituitions](https://arxiv.org/abs/2512.04480)
*Pedro Passos*

Main category: cs.AI

TL;DR: 本文介绍了一种基于模糊逻辑的决策支持系统（DSS），用于实时、规范的比赛管理，旨在优化足球比赛中的换人决策。


<details>
  <summary>Details</summary>
Motivation: 精英足球比赛中的换人决策具有重要的经济和体育意义，但目前仍严重依赖直觉或仅模仿历史偏差的预测模型。传统机器学习方法试图复制人类行为，遇到了预测上限。

Method: 该系统通过客观的、基于规则的推理引擎来审计性能。重新制定了PlayeRank指标，改用累积平均值和角色感知归一化，消除了累积总和模型中固有的上场时间偏差，以实现准确的赛中比较。将改进后的指标与生理指标（疲劳）和情境变量（受战术角色调节的纪律风险）相结合，以计算动态换人优先级（P final）。

Result: 在2018年FIFA世界杯巴西对比利时比赛的案例研究验证表明，该系统不仅与专家对已执行换人（例如加布里埃尔·热苏斯）的共识相一致，而且关键是，它还识别出了人类决策者忽略的高风险情景。例如，该模型在关键黄牌出现前几分钟标记了“FAGNER悖论”（最高优先级的防守风险），并检测到“卢卡库悖论”，即孤立的助攻掩盖了参与度的严重下降。

Conclusion: 模糊逻辑为优化实时战术决策提供了一种透明、可解释且优于黑盒模型的替代方案。

Abstract: In elite soccer, substitution decisions entail significant financial and sporting consequences yet remain heavily reliant on intuition or predictive models that merely mimic historical biases. This paper introduces a Fuzzy Logic based Decision Support System (DSS) designed for real time, prescriptive game management. Unlike traditional Machine Learning approaches that encounter a predictive ceiling by attempting to replicate human behavior, our system audits performance through an objective, rule based inference engine. We propose a methodological advancement by reformulating the PlayeRank metric into a Cumulative Mean with Role Aware Normalization, eliminating the play time exposure bias inherent in cumulative sum models to enable accurate intra match comparison. The system integrates this refined metric with physiological proxies (fatigue) and contextual variables (disciplinary risk modulated by tactical role) to calculate a dynamic Substitution Priority (P final). Validation via a case study of the 2018 FIFA World Cup match between Brazil and Belgium demonstrates the system's ecological validity: it not only aligned with expert consensus on executed substitutions (for example Gabriel Jesus) but, crucially, identified high risk scenarios ignored by human decision makers. Specifically, the model flagged the "FAGNER Paradox" - a maximum priority defensive risk - minutes before a critical yellow card, and detected the "Lukaku Paradox", where an isolated assist masked a severe drop in participation. These results confirm that Fuzzy Logic offers a transparent, explainable, and superior alternative to black box models for optimizing real time tactical decisions.

</details>


### [92] [Persona-based Multi-Agent Collaboration for Brainstorming](https://arxiv.org/abs/2512.04488)
*Nate Straub,Saara Khan,Katharina Jay,Brian Cabral,Oskar Linde*

Main category: cs.AI

TL;DR: Persona-based multi-agent brainstorming improves idea generation.


<details>
  <summary>Details</summary>
Motivation: Generalized multi-agent collaboration is better than single agent, but persona selection can further improve brainstorming.

Method: A framework for persona-based agent selection with different persona pairings and collaboration modes.

Result: Persona choice shapes idea domains, collaboration mode shifts diversity, and multi-agent persona-driven brainstorming produces idea depth and cross-domain coverage.

Conclusion: Persona-based multi-agent brainstorming is important for diverse topics and subject matter ideation.

Abstract: We demonstrate the importance of persona-based multi-agents brainstorming for both diverse topics and subject matter ideation. Prior work has shown that generalized multi-agent collaboration often provides better reasoning than a single agent alone. In this paper, we propose and develop a framework for persona-based agent selection, showing how persona domain curation can improve brainstorming outcomes. Using multiple experimental setups, we evaluate brainstorming outputs across different persona pairings (e.g., Doctor vs VR Engineer) and A2A (agent-to-agent) dynamics (separate, together, separate-then-together). Our results show that (1) persona choice shapes idea domains, (2) collaboration mode shifts diversity of idea generation, and (3) multi-agent persona-driven brainstorming produces idea depth and cross-domain coverage.

</details>


### [93] [A Modular Cognitive Architecture for Assisted Reasoning: The Nemosine Framework](https://arxiv.org/abs/2512.04500)
*Edervaldo Melo*

Main category: cs.AI

TL;DR: Nemosine框架是一个模块化认知架构，旨在支持辅助推理、结构化思维和系统分析。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在为辅助问题解决和决策支持提供一个可操作的结构。

Method: 该模型通过功能性认知模块（“角色”）来组织任务，例如计划、评估、交叉检查和叙述综合。该框架结合了来自元认知、分布式认知和模块化认知系统的原理。

Result: 该架构通过正式规范、内部一致性标准和可复制的结构组件进行记录。

Conclusion: 该论文旨在为未来的计算实现提供清晰的概念基础，并为推理的符号模块化架构研究做出贡献。

Abstract: This paper presents the Nemosine Framework, a modular cognitive architecture designed to support assisted reasoning, structured thinking, and systematic analysis. The model operates through functional cognitive modules ("personas") that organize tasks such as planning, evaluation, cross-checking, and narrative synthesis. The framework combines principles from metacognition, distributed cognition, and modular cognitive systems to offer an operational structure for assisted problem-solving and decision support. The architecture is documented through formal specification, internal consistency criteria, and reproducible structural components. The goal is to provide a clear conceptual basis for future computational implementations and to contribute to the study of symbolic-modular architectures for reasoning.

</details>


### [94] [BiTAgent: A Task-Aware Modular Framework for Bidirectional Coupling between Multimodal Large Language Models and World Models](https://arxiv.org/abs/2512.04513)
*Yu-Wei Zhan,Xin Wang,Pengzhe Mao,Tongtong Feng,Ren Wang,Wenwu Zhu*

Main category: cs.AI

TL;DR: BiTAgent: A task-aware dynamic joint framework for embodied agents, enabling bidirectional coupling between MLLMs and WMs.


<details>
  <summary>Details</summary>
Motivation: Combining MLLMs and WMs for embodied intelligence faces challenges in coupling semantic intent and dynamic state representations, and achieving task-aware adaptability.

Method: Proposes BiTAgent with a forward path injecting MLLM representations into the WM's latent space and a backward path refining the MLLM's semantic space via WM-generated feedback. It uses Task-Aware Dynamic Joint Learning, Task-Aware Behavior Learning, and MLLM-WM Joint Optimization.

Result: Demonstrates superior stability and generalization over state-of-the-art baselines in multi-task and cross-environment settings.

Conclusion: BiTAgent marks a step toward open-ended embodied learning by harmonizing semantic reasoning and dynamic prediction.

Abstract: Building generalist embodied agents requires a unified system that can interpret multimodal goals, model environment dynamics, and execute reliable actions across diverse real-world tasks. Multimodal large language models (MLLMs) offer strong semantic priors and cross-modal generalization, while world models (WMs) provide actionable latent dynamics for prediction and control. Their combination holds promise for open-ended embodied intelligence, yet introduces two key challenges: (1) establishing a tight coupling between the semantic intent from MLLMs and the dynamic state representations within the WM's latent space, and (2) achieving task-aware adaptability that supports multi-task learning and cross-environment generalization. To address these limitations, we propose BiTAgent, a task-aware dynamic joint framework that enables bidirectional coupling between MLLMs and WMs. BiTAgent establishes two complementary pathways: a forward path that injects MLLM representations into the WM's latent space for semantically guided imagination, and a backward path where WM-generated feedback refines the MLLM's semantic space via dense text-conditioned rewards. This bidirectional interaction is realized through three synergistic components: Task-Aware Dynamic Joint Learning, Task-Aware Behavior Learning, and MLLM-WM Joint Optimization, which together harmonize semantic reasoning and dynamic prediction. Extensive experiments across multi-task and cross-environment settings demonstrate superior stability and generalization over state-of-the-art baselines, marking a step toward open-ended embodied learning.

</details>


### [95] [SlideGen: Collaborative Multimodal Agents for Scientific Slide Generation](https://arxiv.org/abs/2512.04529)
*Xin Liang,Xiang Zhang,Yiwei Xu,Siqi Sun,Chenyu You*

Main category: cs.AI

TL;DR: SlideGen是一个用于从科学论文生成学术幻灯片的框架，它通过整合协同规划、映射、安排、注释合成和迭代优化，生成高质量的幻灯片。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要将幻灯片生成简化为文本摘要，忽略了视觉成分和设计密集型特性。本文旨在解决这个问题，提出一个agentic, modular, and visual in the loop的框架。

Method: SlideGen Orchestrates a group of vision language agents that reason collaboratively over the document structure and semantics, producing editable PPTX slides with logical flow and compelling visual presentation. 通过整合协同规划、映射、安排、注释合成和迭代优化

Result: SlideGen在视觉质量、内容忠实度和可读性方面优于现有方法。

Conclusion: SlideGen为设计感知的多模态幻灯片生成奠定了基础，展示了代理协作如何在复杂的多模态推理任务中桥接理解和演示。

Abstract: Generating academic slides from scientific papers is a challenging multimodal reasoning task that requires both long context understanding and deliberate visual planning. Existing approaches largely reduce it to text only summarization, overlooking the visual component and design intensive nature of slide creation. In this paper we introduce SlideGen, an agentic, modular, and visual in the loop framework for scientific paper to slide generation. SlideGen orchestrates a group of vision language agents that reason collaboratively over the document structure and semantics, producing editable PPTX slides with logical flow and compelling visual presentation. By integrating coordinated outlining, mapping, arrangement, note synthesis, and iterative refinement, our system consistently delivers slides of expert level quality. Across diverse benchmarks and strong baselines, SlideGen outperforms existing methods in visual quality, content faithfulness, and readability, positioning it as the new state of the art in automated slide generation. Our work establishes a foundation for design aware multimodal slide generation, demonstrating how agentic collaboration can bridge understanding and presentation in complex multimodal reasoning tasks.

</details>


### [96] [GTM: Simulating the World of Tools for AI Agents](https://arxiv.org/abs/2512.04535)
*Zhenzhen Ren,Xinpeng Zhang,Zhenxing Qian,Yan Gao,Yu Shi,Shuxin Zheng,Jiyan He*

Main category: cs.AI

TL;DR: 提出了通用工具模型（GTM），一个15亿参数的模型，学习充当通用工具模拟器。


<details>
  <summary>Details</summary>
Motivation: 直接、持续地与各种工具交互来训练LLM智能体成本高、速度慢，并增加额外的开发和维护开销。

Method: 提出了上下文感知响应生成（CARG）流程，合成了全面的训练数据，涵盖超过20,000个工具，跨越300个领域，包括物理、医学、机器人和金融。

Result: GTM产生高质量的输出，具有很强的一致性和可靠性。在用于智能体训练的实际强化学习场景中，GTM表现出比实际工具更快的模拟速度，同时保持相当的输出质量，以及显著的泛化和领域适应性。

Conclusion: GTM是开发未来AI智能体的基础组件，能够高效且可扩展地训练工具增强系统。

Abstract: The integration of external tools is pivotal for empowering Large Language Model (LLM) agents with real-world capabilities. However, training these agents through direct, continuous interaction with diverse tools is often prohibitively expensive, slow, and introduces additional development and maintenance overhead. To address this challenge, we introduce the Generalist Tool Model (GTM), a 1.5-billion-parameter model that learns to act as a universal tool simulator. With only prompt-level configuration, GTM accesses tool functionalities along with input arguments and generates outputs that faithfully mimic real tool execution, providing a fast and cost-effective solution that eliminates development overhead. To build GTM, we propose the Context-Aware Response Generation (CARG) pipeline, which synthesizes comprehensive training data covering over 20,000 tools across 300 domains including physics, medicine, robotics, and finance. Through this pipeline, GTM learns to produce not only syntactically correct outputs but also logically coherent and contextually appropriate responses. Experiments demonstrate that GTM produces high-quality outputs with strong consistency and reliability. Besides when used in real reinforcement learning scenarios for agent training, GTM exhibits significantly faster simulation speed compared to real tools while maintaining comparable output quality, along with remarkable generalization and domain adaptability. Our results establish GTM as a foundational component for developing future AI agents, enabling efficient and scalable training of tool-augmented systems.

</details>


### [97] [The Ethics of Generative AI](https://arxiv.org/abs/2512.04598)
*Michael Klenk*

Main category: cs.AI

TL;DR: 本章探讨了生成式AI的伦理问题。


<details>
  <summary>Details</summary>
Motivation: 展示了生成式AI如何使人们体验技术，就像它是人类一样，这种能力为生成式AI的哲学伦理学提供了一个有意义的焦点。

Method: 首先提供了一个技术入门，然后展示了生成式AI如何加剧和缓解人工智能伦理中常见的伦理问题，包括责任、隐私、偏见和公平，以及异化和剥削的形式。

Result: 生成式AI的模仿生成性引发了具体的伦理问题。

Conclusion: 考察了生成式AI的模仿生成性所产生的伦理问题，例如关于作者身份和署名的辩论，与机器之间出现的类似社会关系，以及新的影响、说服和操纵形式。

Abstract: This chapter discusses the ethics of generative AI. It provides a technical primer to show how generative AI affords experiencing technology as if it were human, and this affordance provides a fruitful focus for the philosophical ethics of generative AI. It then shows how generative AI can both aggravate and alleviate familiar ethical concerns in AI ethics, including responsibility, privacy, bias and fairness, and forms of alienation and exploitation. Finally, the chapter examines ethical questions that arise specifically from generative AI's mimetic generativity, such as debates about authorship and credit, the emergence of as-if social relationships with machines, and new forms of influence, persuasion, and manipulation.

</details>


### [98] [Neural Decoding of Overt Speech from ECoG Using Vision Transformers and Contrastive Representation Learning](https://arxiv.org/abs/2512.04618)
*Mohamed Baha Ben Ticha,Xingchen Ran,Guillaume Saldanha,Gaël Le Godais,Philémon Roussel,Marc Aubert,Amina Fontanell,Thomas Costecalde,Lucas Struber,Serpil Karakas,Shaomin Zhang,Philippe Kahane,Guillaume Charvet,Stéphan Chabardès,Blaise Yvert*

Main category: cs.AI

TL;DR: 本文提出了一种基于编码器-解码器深度神经架构的离线语音解码流水线，该架构集成了视觉Transformer和对比学习，以增强从ECoG信号直接回归语音的效果。


<details>
  <summary>Details</summary>
Motivation: 旨在解决表面ECoG记录重建语音的难题，并探索使用完全可植入和无线硬膜外记录系统解码语音的可能性。

Method: 使用集成了视觉Transformer和对比学习的编码器-解码器深度神经架构，直接从ECoG信号回归语音。

Result: 该方法在两个数据集上进行了评估，一个来自癫痫患者的临床硬膜下电极，另一个来自运动BCI试验参与者的完全可植入WIMAGINE硬膜外系统。

Conclusion: 该研究首次尝试从完全可植入和无线硬膜外记录系统解码语音，为长期使用提供了前景。

Abstract: Speech Brain Computer Interfaces (BCIs) offer promising solutions to people with severe paralysis unable to communicate. A number of recent studies have demonstrated convincing reconstruction of intelligible speech from surface electrocorticographic (ECoG) or intracortical recordings by predicting a series of phonemes or words and using downstream language models to obtain meaningful sentences. A current challenge is to reconstruct speech in a streaming mode by directly regressing cortical signals into acoustic speech. While this has been achieved recently using intracortical data, further work is needed to obtain comparable results with surface ECoG recordings. In particular, optimizing neural decoders becomes critical in this case. Here we present an offline speech decoding pipeline based on an encoder-decoder deep neural architecture, integrating Vision Transformers and contrastive learning to enhance the direct regression of speech from ECoG signals. The approach is evaluated on two datasets, one obtained with clinical subdural electrodes in an epileptic patient, and another obtained with the fully implantable WIMAGINE epidural system in a participant of a motor BCI trial. To our knowledge this presents a first attempt to decode speech from a fully implantable and wireless epidural recording system offering perspectives for long-term use.

</details>


### [99] [BioMedGPT-Mol: Multi-task Learning for Molecular Understanding and Generation](https://arxiv.org/abs/2512.04629)
*Chenyang Zuo,Siqi Fan,Zaiqing Nie*

Main category: cs.AI

TL;DR: BioMedGPT-Mol is a molecular language model for molecular understanding and generation tasks.


<details>
  <summary>Details</summary>
Motivation: Explore how general-purpose language models can be adapted for molecular science, especially small molecule drug development.

Method: Fine-tuned a general-purpose language model using a curated, large-scale dataset and a multi-task learning framework.

Result: Achieved remarkable performance on a consolidated benchmark and demonstrated competitive capability as an end-to-end retrosynthetic planner.

Conclusion: A general-purpose reasoning model can be effectively post-trained into a professional molecular language model and the approach can be extended to other biomedical domains.

Abstract: Molecules play a crucial role in biomedical research and discovery, particularly in the field of small molecule drug development. Given the rapid advancements in large language models, especially the recent emergence of reasoning models, it is natural to explore how a general-purpose language model can be efficiently adapted for molecular science applications. In this work, we introduce BioMedGPT-Mol, a molecular language model designed to support molecular understanding and generation tasks. By curating and unifying existing public instruction datasets, we have assembled a large-scale, comprehensive, and high-quality training dataset. The model is then fine-tuned through a meticulously designed multi-task learning framework. On a consolidated benchmark derived from LlaSMol, TOMG-Bench, and MuMOInstruct, BioMedGPT-Mol achieves remarkable performance. Our experimental results demonstrate that a general-purpose reasoning model can be effectively and efficiently post-trained into a professional molecular language model through a well-structured multi-task curriculum. Leveraging the power of it, we further explore retrosynthetic planning task, and the performance on RetroBench demonstrates its competitive capability of acting as an end-to-end retrosynthetic planner. We anticipate that our approach can be extended to other biomedical scientific domains.

</details>


### [100] [Turbo-Muon: Accelerating Orthogonality-Based Optimization with Pre-Conditioning](https://arxiv.org/abs/2512.04632)
*Thibaut Boissin,Thomas Massena,Franck Mamalet,Mathieu Serrurier*

Main category: cs.AI

TL;DR: 提出了一种预处理程序，以加速 Newton-Schulz 收敛并降低其计算成本。


<details>
  <summary>Details</summary>
Motivation: 基于正交性的优化器（如 Muon）最近在大型训练和社区驱动的效率挑战中表现出强大的性能。然而，这些方法依赖于昂贵的梯度正交化步骤。即使是有效的迭代近似（如 Newton-Schulz）仍然很昂贵，通常需要数十次矩阵乘法才能收敛。

Method: 引入了一种预处理程序，可以加速 Newton-Schulz 收敛并降低其计算成本。通过加快收敛速度，可以减少 Newton-Schulz 近似中的迭代次数，而不会降低近似质量。

Result: 在 Newton-Schulz 近似中实现了高达 2.8 倍的加速。在实际的训练场景中，端到端训练运行时间提高了 5-10%。

Conclusion: 该方法无需超参数调整，可以作为简单的直接替代品采用。

Abstract: Orthogonality-based optimizers, such as Muon, have recently shown strong performance across large-scale training and community-driven efficiency challenges. However, these methods rely on a costly gradient orthogonalization step. Even efficient iterative approximations such as Newton-Schulz remain expensive, typically requiring dozens of matrix multiplications to converge. We introduce a preconditioning procedure that accelerates Newton-Schulz convergence and reduces its computational cost. We evaluate its impact and show that the overhead of our preconditioning can be made negligible. Furthermore, the faster convergence it enables allows us to remove one iteration out of the usual five without degrading approximation quality. Our publicly available implementation achieves up to a 2.8x speedup in the Newton-Schulz approximation. We also show that this has a direct impact on end-to-end training runtime with 5-10% improvement in realistic training scenarios across two efficiency-focused tasks. On challenging language or vision tasks, we validate that our method maintains equal or superior model performance while improving runtime. Crucially, these improvements require no hyperparameter tuning and can be adopted as a simple drop-in replacement. Our code is publicly available on github.

</details>


### [101] [Towards Ethical Multi-Agent Systems of Large Language Models: A Mechanistic Interpretability Perspective](https://arxiv.org/abs/2512.04691)
*Jae Hee Lee,Anne Lauscher,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: 本文旨在探讨大型语言模型多智能体系统(MALMs)的伦理问题，并从可解释性的角度提出了研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型被广泛应用于多智能体系统，但同时也带来了伦理挑战。

Method: 从机制可解释性的角度，确定了三个关键研究挑战：开发评估框架、阐明内部机制、实施参数高效的对齐技术。

Result: 提出了确保MALMs伦理行为的研究议程。

Conclusion: 通过可解释性方法，引导MALMs实现伦理行为，同时不影响其性能。

Abstract: Large language models (LLMs) have been widely deployed in various applications, often functioning as autonomous agents that interact with each other in multi-agent systems. While these systems have shown promise in enhancing capabilities and enabling complex tasks, they also pose significant ethical challenges. This position paper outlines a research agenda aimed at ensuring the ethical behavior of multi-agent systems of LLMs (MALMs) from the perspective of mechanistic interpretability. We identify three key research challenges: (i) developing comprehensive evaluation frameworks to assess ethical behavior at individual, interactional, and systemic levels; (ii) elucidating the internal mechanisms that give rise to emergent behaviors through mechanistic interpretability; and (iii) implementing targeted parameter-efficient alignment techniques to steer MALMs towards ethical behaviors without compromising their performance.

</details>


### [102] [Playing the Player: A Heuristic Framework for Adaptive Poker AI](https://arxiv.org/abs/2512.04714)
*Andrew Paterson,Carl Sanders*

Main category: cs.AI

TL;DR: 这篇论文介绍了一种名为Patrick的扑克AI，它通过最大限度地利用人类玩家的弱点来获得胜利，而不是追求完美和无法利用的策略。


<details>
  <summary>Details</summary>
Motivation: 挑战了传统扑克AI追求完美策略的观念，认为理解和利用人类玩家的不完美性才是更重要的。

Method: Patrick的架构是专门为理解和攻击人类对手的缺陷而设计的，采用了一种新颖的预测锚定学习方法。

Result: Patrick在一个64,267手牌的试验中表现出盈利能力。

Conclusion: 论文认为，创造能够掌握人类不完美艺术的AI，比解决“已解决”的神话更有意义，也更有趣。

Abstract: For years, the discourse around poker AI has been dominated by the concept of solvers and the pursuit of unexploitable, machine-perfect play. This paper challenges that orthodoxy. It presents Patrick, an AI built on the contrary philosophy: that the path to victory lies not in being unexploitable, but in being maximally exploitative. Patrick's architecture is a purpose-built engine for understanding and attacking the flawed, psychological, and often irrational nature of human opponents. Through detailed analysis of its design, its novel prediction-anchored learning method, and its profitable performance in a 64,267-hand trial, this paper makes the case that the solved myth is a distraction from the real, far more interesting challenge: creating AI that can master the art of human imperfection.

</details>


### [103] [Sequential Enumeration in Large Language Models](https://arxiv.org/abs/2512.04727)
*Kuinan Hou,Marco Zorzi,Alberto Testolin*

Main category: cs.AI

TL;DR: 大型语言模型(llms)在可靠地计数和生成项目序列方面仍然面临重大挑战。尽管基于规则的符号系统可以轻松处理这种能力，但神经模型很难学习系统地部署计数程序。本文旨在研究五个最先进的llm的顺序枚举能力。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明，循环架构只能近似地跟踪和枚举事件序列，并且包括llm在内的现代深度学习系统是否可以对离散符号序列部署系统计数程序仍不清楚。这项研究旨在填补这一空白。

Method: 我们通过采用各种提示指令来探索思维链在计数策略的自发出现中的作用，从而在涉及字母和单词列表的顺序命名和生成任务中探测llm。我们还评估了具有相同架构但尺寸不断增加的开源模型，以了解计数原理的掌握是否遵循缩放比例定律，并且我们分析了顺序枚举过程中的嵌入动力学，以研究数量的新兴编码。

Result: 我们发现，当明确提示时，某些llm确实能够部署计数程序，但是当仅要求枚举序列中项目的数量时，它们都不会自发地进行计数。

Conclusion: 我们的结果表明，尽管llm具有令人印象深刻的新兴能力，但它们仍无法可靠且系统地部署计数程序，这突显了神经方法和符号方法之间在组合泛化方面仍然存在差距。

Abstract: Reliably counting and generating sequences of items remain a significant challenge for neural networks, including Large Language Models (LLMs). Indeed, although this capability is readily handled by rule-based symbolic systems based on serial computation, learning to systematically deploy counting procedures is difficult for neural models, which should acquire these skills through learning. Previous research has demonstrated that recurrent architectures can only approximately track and enumerate sequences of events, and it remains unclear whether modern deep learning systems, including LLMs, can deploy systematic counting procedures over sequences of discrete symbols. This paper aims to fill this gap by investigating the sequential enumeration abilities of five state-of-the-art LLMs, including proprietary, open-source, and reasoning models. We probe LLMs in sequential naming and production tasks involving lists of letters and words, adopting a variety of prompting instructions to explore the role of chain-of-thought in the spontaneous emerging of counting strategies. We also evaluate open-source models with the same architecture but increasing size to see whether the mastering of counting principles follows scaling laws, and we analyze the embedding dynamics during sequential enumeration to investigate the emergent encoding of numerosity. We find that some LLMs are indeed capable of deploying counting procedures when explicitly prompted to do so, but none of them spontaneously engage in counting when simply asked to enumerate the number of items in a sequence. Our results suggest that, despite their impressive emergent abilities, LLMs cannot yet robustly and systematically deploy counting procedures, highlighting a persistent gap between neural and symbolic approaches to compositional generalization.

</details>


### [104] [Human Cognitive Biases in Explanation-Based Interaction: The Case of Within and Between Session Order Effect](https://arxiv.org/abs/2512.04764)
*Dario Pesenti,Alessandro Bogani,Katya Tentori,Stefano Teso*

Main category: cs.AI

TL;DR: 本文研究了解释性交互学习（XIL）中顺序效应的影响，发现其对用户反馈质量影响不大。


<details>
  <summary>Details</summary>
Motivation: 解释性交互学习(XIL) 是一种通过与 AI 模型的解释进行交互来定制和纠正 AI 模型的框架。但最近的研究表明，解释性交互可能会触发顺序效应，影响用户信任和反馈质量。本文旨在澄清顺序效应和解释性交互之间的相互作用。

Method: 通过两项大规模用户研究（n = 713），模拟常见的 XIL 任务，操纵呈现给参与者的正确和错误解释的顺序，评估调试会话内部和之间的顺序效应。

Result: 顺序效应在调试会话内部对用户与模型的一致性（信任的行为度量）有有限但显著的影响，但在会话之间没有影响。用户反馈的质量总体上令人满意，顺序效应在两个实验中仅产生微小且不一致的影响。

Conclusion: 顺序效应对于成功应用 XIL 方法来说不是一个重要问题。这项工作有助于理解 AI 中的人为因素。

Abstract: Explanatory Interactive Learning (XIL) is a powerful interactive learning framework designed to enable users to customize and correct AI models by interacting with their explanations. In a nutshell, XIL algorithms select a number of items on which an AI model made a decision (e.g. images and their tags) and present them to users, together with corresponding explanations (e.g. image regions that drive the model's decision). Then, users supply corrective feedback for the explanations, which the algorithm uses to improve the model. Despite showing promise in debugging tasks, recent studies have raised concerns that explanatory interaction may trigger order effects, a well-known cognitive bias in which the sequence of presented items influences users' trust and, critically, the quality of their feedback. We argue that these studies are not entirely conclusive, as the experimental designs and tasks employed differ substantially from common XIL use cases, complicating interpretation. To clarify the interplay between order effects and explanatory interaction, we ran two larger-scale user studies (n = 713 total) designed to mimic common XIL tasks. Specifically, we assessed order effects both within and between debugging sessions by manipulating the order in which correct and wrong explanations are presented to participants. Order effects had a limited, through significant impact on users' agreement with the model (i.e., a behavioral measure of their trust), and only when examined withing debugging sessions, not between them. The quality of users' feedback was generally satisfactory, with order effects exerting only a small and inconsistent influence in both experiments. Overall, our findings suggest that order effects do not pose a significant issue for the successful employment of XIL approaches. More broadly, our work contributes to the ongoing efforts for understanding human factors in AI.

</details>


### [105] [ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications](https://arxiv.org/abs/2512.04785)
*Eranga Bandara,Amin Hass,Ross Gore,Sachin Shetty,Ravi Mukkamala,Safdar H. Bouk,Xueping Liang,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.AI

TL;DR: ASTRIDE: automated threat modeling platform for AI agent-based systems, extends STRIDE with AI-specific threats, integrates VLMs and LLM for diagram-driven analysis.


<details>
  <summary>Details</summary>
Motivation: AI agent systems have novel security challenges (prompt injection, context poisoning, etc.) not captured by traditional threat modeling.

Method: Automated threat modeling using fine-tuned VLMs and OpenAI-gpt-oss, extending STRIDE with 'A' (AI Agent-Specific Attacks).

Result: ASTRIDE provides accurate, scalable, and explainable threat modeling.

Conclusion: ASTRIDE is the first framework to extend STRIDE with AI-specific threats and automate diagram-driven threat modeling in AI agent-based applications.

Abstract: AI agent-based systems are becoming increasingly integral to modern software architectures, enabling autonomous decision-making, dynamic task execution, and multimodal interactions through large language models (LLMs). However, these systems introduce novel and evolving security challenges, including prompt injection attacks, context poisoning, model manipulation, and opaque agent-to-agent communication, that are not effectively captured by traditional threat modeling frameworks. In this paper, we introduce ASTRIDE, an automated threat modeling platform purpose-built for AI agent-based systems. ASTRIDE extends the classical STRIDE framework by introducing a new threat category, A for AI Agent-Specific Attacks, which encompasses emerging vulnerabilities such as prompt injection, unsafe tool invocation, and reasoning subversion, unique to agent-based applications. To automate threat modeling, ASTRIDE combines a consortium of fine-tuned vision-language models (VLMs) with the OpenAI-gpt-oss reasoning LLM to perform end-to-end analysis directly from visual agent architecture diagrams, such as data flow diagrams(DFDs). LLM agents orchestrate the end-to-end threat modeling automation process by coordinating interactions between the VLM consortium and the reasoning LLM. Our evaluations demonstrate that ASTRIDE provides accurate, scalable, and explainable threat modeling for next-generation intelligent systems. To the best of our knowledge, ASTRIDE is the first framework to both extend STRIDE with AI-specific threats and integrate fine-tuned VLMs with a reasoning LLM to fully automate diagram-driven threat modeling in AI agent-based applications.

</details>


### [106] [SIMA 2: A Generalist Embodied Agent for Virtual Worlds](https://arxiv.org/abs/2512.04797)
*SIMA team,Adrian Bolton,Alexander Lerchner,Alexandra Cordell,Alexandre Moufarek,Andrew Bolt,Andrew Lampinen,Anna Mitenkova,Arne Olav Hallingstad,Bojan Vujatovic,Bonnie Li,Cong Lu,Daan Wierstra,Daniel P. Sawyer,Daniel Slater,David Reichert,Davide Vercelli,Demis Hassabis,Drew A. Hudson,Duncan Williams,Ed Hirst,Fabio Pardo,Felix Hill,Frederic Besse,Hannah Openshaw,Harris Chan,Hubert Soyer,Jane X. Wang,Jeff Clune,John Agapiou,John Reid,Joseph Marino,Junkyung Kim,Karol Gregor,Kaustubh Sridhar,Kay McKinney,Laura Kampis,Lei M. Zhang,Loic Matthey,Luyu Wang,Maria Abi Raad,Maria Loks-Thompson,Martin Engelcke,Matija Kecman,Matthew Jackson,Maxime Gazeau,Ollie Purkiss,Oscar Knagg,Peter Stys,Piermaria Mendolicchio,Raia Hadsell,Rosemary Ke,Ryan Faulkner,Sarah Chakera,Satinder Singh Baveja,Shane Legg,Sheleem Kashem,Tayfun Terzi,Thomas Keck,Tim Harley,Tim Scholtes,Tyson Roberts,Volodymyr Mnih,Yulan Liu,Zhengdong Wang,Zoubin Ghahramani*

Main category: cs.AI

TL;DR: SIMA 2 is a generalist embodied agent that can understand and act in various 3D virtual worlds, built upon a Gemini foundation model.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create an agent capable of active, goal-directed interaction within an embodied environment, addressing limitations of prior work like SIMA 1 which was limited to simple language commands.

Method: SIMA 2 uses a Gemini foundation model to act as an interactive partner, reasoning about high-level goals, conversing with the user, and handling complex instructions through language and images. It also leverages Gemini for task generation and reward provision for self-improvement.

Result: SIMA 2 substantially closes the gap with human performance across a diverse portfolio of games and demonstrates robust generalization to unseen environments. It can also autonomously learn new skills in a new environment.

Conclusion: The work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.

Abstract: We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.

</details>


### [107] [Enabling Ethical AI: A case study in using Ontological Context for Justified Agentic AI Decisions](https://arxiv.org/abs/2512.04822)
*Liam McGee,James Harvey,Lucy Cull,Andreas Vermeulen,Bart-Floris Visscher,Malvika Sharan*

Main category: cs.AI

TL;DR: Collaborative human-AI approach for building an inspectable semantic layer for Agentic AI.


<details>
  <summary>Details</summary>
Motivation: Capture tacit institutional knowledge, improve response quality/efficiency, and mitigate institutional amnesia.

Method: AI agents propose knowledge structures from data; experts validate/correct/extend them, improving models with feedback.

Result: Improved response quality and efficiency.

Conclusion: Shift to justifiable Agentic AI with decisions grounded in explicit, inspectable evidence and reasoning.

Abstract: In this preprint, we present A collaborative human-AI approach to building an inspectable semantic layer for Agentic AI. AI agents first propose candidate knowledge structures from diverse data sources; domain experts then validate, correct, and extend these structures, with their feedback used to improve subsequent models. Authors show how this process captures tacit institutional knowledge, improves response quality and efficiency, and mitigates institutional amnesia. We argue for a shift from post-hoc explanation to justifiable Agentic AI, where decisions are grounded in explicit, inspectable evidence and reasoning accessible to both experts and non-specialists.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [108] [Energy Profiling of Data-Sharing Pipelines: Modeling, Estimation, and Reuse Strategies](https://arxiv.org/abs/2512.04086)
*Sepideh Masoudi,Sebastian Werner,Pierluigi Plebani,Stefan Tai*

Main category: cs.DB

TL;DR: 这篇论文提出了一种新的方法来建模和评估数据共享管道中不同执行配置的能耗，并识别了跨管道共享阶段的重用潜力，以减少大型数据共享联盟中的能源消耗。


<details>
  <summary>Details</summary>
Motivation: 当前的数据共享管道在能源效率方面关注不足。

Method: 该论文提出了一种新的方法来建模和评估数据共享管道中不同执行配置的能耗，并识别跨管道共享阶段的重用潜力。

Result: 通过模拟实验验证了该方法，揭示了跨组织管道优化的潜力。

Conclusion: 该研究为节能执行策略奠定了基础。

Abstract: Data-sharing pipelines involve a series of stages that apply policy-based data transformations to enable secure and effective data exchange among organizations. Although numerous tools and platforms exist to manage governance and enforcement in these pipelines, energy efficiency in data exchange has received limited attention. This paper introduces a novel method to model and estimate the energy consumption of different execution configurations in data-sharing pipelines. Additionally, this method identifies reuse potential in shared stages across pipelines that hold the key to reducing energy in large data-sharing federations. We validate this method through simulation experiments, revealing promising potential for cross-organizational pipeline optimization and laying a foundation for energy-conscious execution strategies.

</details>


### [109] [A Fast Ethereum-Compatible Forkless Database](https://arxiv.org/abs/2512.04735)
*Herbert Jordan,Kamil Jezek,Pavle Subotic,Bernhard Scholz*

Main category: cs.DB

TL;DR: 以太坊的StateDB是为分叉链设计的，这使得它在新的非分叉链上效率很低。这篇论文介绍了一种新的state database，它兼容以太坊，同时专门为非分叉链设计。


<details>
  <summary>Details</summary>
Motivation: 当前的区块链为了提高吞吐量和最终性，使用快速共识协议来避免分叉。但以太坊的StateDB是为维护多个状态版本的分叉链设计的,这使得它在新的非分叉链上效率很低。此外，现有的StateDB实现是建立在键-值存储（例如LevelDB）之上的，这使得它们的效率更低。

Method: 设计一个新的state database，它是一个原生的数据库实现，并保持以太坊的兼容性，同时专门用于非分叉区块链。

Result: 对于验证器，数据库实现了10倍的速度提升和99%的空间减少，对于存档节点，存储需求减少了三倍。

Conclusion: 这篇论文提出了一个专门为非分叉区块链设计的新的state database，它比现有的state database更有效率。

Abstract: The State Database of a blockchain stores account data and enables authentication. Modern blockchains use fast consensus protocols to avoid forking, improving throughput and finality. However, Ethereum's StateDB was designed for a forking chain that maintains multiple state versions. While newer blockchains adopt Ethereum's standard for DApp compatibility, they do not require multiple state versions, making legacy Ethereum databases inefficient for fast, non-forking blockchains. Moreover, existing StateDB implementations have been built on key-value stores (e.g., LevelDB), which make them less efficient.
  This paper introduces a novel state database that is a native database implementation and maintains Ethereum compatibility while being specialized for non-forking blockchains. Our database delivers ten times speedups and 99% space reductions for validators, and a threefold decrease in storage requirements for archive nodes.

</details>


### [110] [High-Performance DBMSs with io_uring: When and How to use it](https://arxiv.org/abs/2512.04859)
*Matthias Jasny,Muhammad El-Hindi,Tobias Ziegler,Viktor Leis,Carsten Binnig*

Main category: cs.DB

TL;DR: 本文研究了现代数据库系统如何利用 Linux io_uring 接口实现高效、低开销的 I/O。


<details>
  <summary>Details</summary>
Motivation: 传统 I/O 接口存在局限性，而简单地用 io_uring 替代传统 I/O 接口不一定能带来性能提升。

Method: 通过将 io_uring 集成到存储受限的缓冲管理器中，并将其用于网络受限的分析工作负载中的高吞吐量数据混洗，来评估 io_uring 的性能。

Result: 研究表明，底层优化何时转化为切实的系统级收益，以及架构选择如何影响这些收益。在 PostgreSQL 的最新 io_uring 集成案例研究中，应用本文的指南产生了 14% 的性能提升。

Conclusion: 本文为使用 io_uring 设计 I/O 密集型系统提供了实用的指导，并验证了其有效性。

Abstract: We study how modern database systems can leverage the Linux io_uring interface for efficient, low-overhead I/O. io_uring is an asynchronous system call batching interface that unifies storage and network operations, addressing limitations of existing Linux I/O interfaces. However, naively replacing traditional I/O interfaces with io_uring does not necessarily yield performance benefits. To demonstrate when io_uring delivers the greatest benefits and how to use it effectively in modern database systems, we evaluate it in two use cases: Integrating io_uring into a storage-bound buffer manager and using it for high-throughput data shuffling in network-bound analytical workloads. We further analyze how advanced io_uring features, such as registered buffers and passthrough I/O, affect end-to-end performance. Our study shows when low-level optimizations translate into tangible system-wide gains and how architectural choices influence these benefits. Building on these insights, we derive practical guidelines for designing I/O-intensive systems using io_uring and validate their effectiveness in a case study of PostgreSQL's recent io_uring integration, where applying our guidelines yields a performance improvement of 14%.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [111] [The Personalization Paradox: Semantic Loss vs. Reasoning Gains in Agentic AI Q&A](https://arxiv.org/abs/2512.04343)
*Satyajit Movidi,Stephen Russell*

Main category: cs.IR

TL;DR: AIVisor，一个用于学生咨询的 agentic 检索增强 LLM，用于检查个性化如何影响跨多个评估维度的系统性能。


<details>
  <summary>Details</summary>
Motivation: 旨在研究个性化对学生咨询系统性能的影响，使用精心设计的、强调词汇精确性的真实咨询问题。

Method: 比较了十种个性化和非个性化系统配置，并使用线性混合效应模型分析了词汇（BLEU、ROUGE-L）、语义（METEOR、BERTScore）和 grounding (RAGAS) 指标的结果。

Result: 个性化提高了推理质量和 grounding，但对语义相似性产生了显著的负面影响，这不是因为答案质量差，而是因为现有指标的局限性，这些指标会惩罚有意义的、个性化的、偏离通用参考文本的偏差。完全集成的个性化配置产生了最高的整体收益。

Conclusion: 个性化会产生依赖于度量的变化，而不是统一的改进，并为 agentic AI 中更透明和稳健的个性化提供了方法论基础。

Abstract: AIVisor, an agentic retrieval-augmented LLM for student advising, was used to examine how personalization affects system performance across multiple evaluation dimensions. Using twelve authentic advising questions intentionally designed to stress lexical precision, we compared ten personalized and non-personalized system configurations and analyzed outcomes with a Linear Mixed-Effects Model across lexical (BLEU, ROUGE-L), semantic (METEOR, BERTScore), and grounding (RAGAS) metrics. Results showed a consistent trade-off: personalization reliably improved reasoning quality and grounding, yet introduced a significant negative interaction on semantic similarity, driven not by poorer answers but by the limits of current metrics, which penalize meaningful personalized deviations from generic reference texts. This reveals a structural flaw in prevailing LLM evaluation methods, which are ill-suited for assessing user-specific responses. The fully integrated personalized configuration produced the highest overall gains, suggesting that personalization can enhance system effectiveness when evaluated with appropriate multidimensional metrics. Overall, the study demonstrates that personalization produces metric-dependent shifts rather than uniform improvements and provides a methodological foundation for more transparent and robust personalization in agentic AI.

</details>


### [112] [UserSimCRS v2: Simulation-Based Evaluation for Conversational Recommender Systems](https://arxiv.org/abs/2512.04588)
*Nolwenn Bernard,Krisztian Balog*

Main category: cs.IR

TL;DR: UserSimCRS v2 is an upgraded toolkit for evaluating conversational recommender systems (CRSs).


<details>
  <summary>Details</summary>
Motivation: Lack of resources for simulation-based evaluation of CRSs.

Method: Enhanced agenda-based user simulator, large language model-based simulators, integration for a wider range of CRSs and datasets, and new LLM-as-a-judge evaluation utilities.

Result: Demonstrated extensions in a case study.

Conclusion: UserSimCRS v2 aligns the toolkit with state-of-the-art research.

Abstract: Resources for simulation-based evaluation of conversational recommender systems (CRSs) are scarce. The UserSimCRS toolkit was introduced to address this gap. In this work, we present UserSimCRS v2, a significant upgrade aligning the toolkit with state-of-the-art research. Key extensions include an enhanced agenda-based user simulator, introduction of large language model-based simulators, integration for a wider range of CRSs and datasets, and new LLM-as-a-judge evaluation utilities. We demonstrate these extensions in a case study.

</details>


### [113] [Spatially-Enhanced Retrieval-Augmented Generation for Walkability and Urban Discovery](https://arxiv.org/abs/2512.04790)
*Maddalena Amendola,Chiara Pugliese,Raffaele Perego,Chiara Renso*

Main category: cs.IR

TL;DR: 本文介绍了一种名为 WalkRAG 的空间 RAG 框架，该框架具有对话界面，用于推荐可步行的城市行程。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 在空间检索和推理方面存在局限性，容易产生幻觉，因此需要新的解决方案。检索增强生成 (RAG) 已成为一种很有前途的增强 LLM 的方法，而空间 RAG 将这种方法扩展到涉及地理理解的任务。

Method: 本文介绍了一种名为 WalkRAG 的基于空间 RAG 的框架，该框架具有对话界面，用于推荐可步行的城市行程。用户可以请求满足特定空间约束和偏好的路线，同时以交互方式检索有关路径和沿途兴趣点 (POI) 的信息。

Result: 初步结果表明，将信息检索、空间推理和 LLM 相结合以支持城市探索是有效的。

Conclusion: 本文结合信息检索、空间推理和 LLM 来支持城市探索。

Abstract: Large Language Models (LLMs) have become foundational tools in artificial intelligence, supporting a wide range of applications beyond traditional natural language processing, including urban systems and tourist recommendations. However, their tendency to hallucinate and their limitations in spatial retrieval and reasoning are well known, pointing to the need for novel solutions. Retrieval-augmented generation (RAG) has recently emerged as a promising way to enhance LLMs with accurate, domain-specific, and timely information. Spatial RAG extends this approach to tasks involving geographic understanding. In this work, we introduce WalkRAG, a spatial RAG-based framework with a conversational interface for recommending walkable urban itineraries. Users can request routes that meet specific spatial constraints and preferences while interactively retrieving information about the path and points of interest (POIs) along the way. Preliminary results show the effectiveness of combining information retrieval, spatial reasoning, and LLMs to support urban discovery.

</details>


### [114] [Ask Safely: Privacy-Aware LLM Query Generation for Knowledge Graphs](https://arxiv.org/abs/2512.04852)
*Mauro Dalle Lucca Tosi,Jordi Cabot*

Main category: cs.IR

TL;DR: 提出了一种保护隐私的知识图谱查询生成方法，该方法可以在不泄露敏感数据的情况下，利用大型语言模型将自然语言问题转换为 Cypher 查询。


<details>
  <summary>Details</summary>
Motivation: 当知识图谱包含敏感数据且用户缺乏部署本地生成式LLM的资源时，传统的知识图谱查询方法无法应用。

Method: 该方法基于图的结构识别敏感信息，并在请求LLM将自然语言问题转换为Cypher查询之前省略这些值。

Result: 实验结果表明，该方法在保留生成查询质量的同时，可以防止敏感数据传输到第三方服务。

Conclusion: 该方法能够在保护隐私的前提下，利用大型语言模型查询知识图谱。

Abstract: Large Language Models (LLMs) are increasingly used to query knowledge graphs (KGs) due to their strong semantic understanding and extrapolation capabilities compared to traditional approaches. However, these methods cannot be applied when the KG contains sensitive data and the user lacks the resources to deploy a local generative LLM. To address this issue, we propose a privacy-aware query generation approach for KGs. Our method identifies sensitive information in the graph based on its structure and omits such values before requesting the LLM to translate natural language questions into Cypher queries. Experimental results show that our approach preserves the quality of the generated queries while preventing sensitive data from being transmitted to third-party services.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [115] [ASCIIBench: Evaluating Language-Model-Based Understanding of Visually-Oriented Text](https://arxiv.org/abs/2512.04125)
*Kerry Luo,Michael Fu,Joshua Peguero,Husnain Malik,Anvay Patil,Joyce Lin,Megan Van Overborg,Ryan Sarmiento,Kevin Zhu*

Main category: cs.LG

TL;DR: 论文介绍了一个新的用于评估大型语言模型在生成和分类ASCII艺术图像方面能力的基准测试，名为ASCIIBench。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在需要精确空间和位置推理的任务中表现不佳。ASCII艺术提供了一种独特的探究这种局限性的方法。

Method: 论文提出了ASCIIBench，一个包含5315个带有类别标签的ASCII图像的数据集，并发布了一个经过微调的CLIP模型，用于捕获ASCII结构，以评估大型语言模型生成的ASCII艺术。

Result: 分析表明，CLIP嵌入上的余弦相似度无法区分大多数ASCII类别，即使对于低方差类别，也只能产生机会水平的性能。内部平均相似度高的类别表现出明显的区分性，表明瓶颈在于表示而不是生成方差。

Conclusion: ASCII艺术可以作为多模态表示的压力测试，并推动开发新的嵌入方法或专门为符号视觉模式定制的评估指标。所有的资源都可以在https://github.com/ASCIIBench/ASCIIBench找到。

Abstract: Large language models (LLMs) have demonstrated several emergent behaviors with scale, including reasoning and fluency in long-form text generation. However, they continue to struggle with tasks requiring precise spatial and positional reasoning. ASCII art, a symbolic medium where characters encode structure and form, provides a unique probe of this limitation. We introduce ASCIIBench, a novel benchmark for evaluating both the generation and classification of ASCII-text images. ASCIIBench consists of a filtered dataset of 5,315 class-labeled ASCII images and is, to our knowledge, the first publicly available benchmark of its kind. Alongside the dataset, we release weights for a fine-tuned CLIP model adapted to capture ASCII structure, enabling the evaluation of LLM-generated ASCII art. Our analysis shows that cosine similarity over CLIP embeddings fails to separate most ASCII categories, yielding chance-level performance even for low-variance classes. In contrast, classes with high internal mean similarity exhibit clear discriminability, revealing that the bottleneck lies in representation rather than generational variance. These findings position ASCII art as a stress test for multimodal representations and motivate the development of new embedding methods or evaluation metrics tailored to symbolic visual modalities. All resources are available at https://github.com/ASCIIBench/ASCIIBench.

</details>


### [116] [MechDetect: Detecting Data-Dependent Errors](https://arxiv.org/abs/2512.04138)
*Philipp Jung,Nicholas Chandler,Sebastian Jäger,Felix Biessmann*

Main category: cs.LG

TL;DR: 本研究提出了一种名为MechDetect的算法，用于研究数据集中错误生成的机制，通过判断错误是否依赖于数据本身来帮助追踪和修复错误。


<details>
  <summary>Details</summary>
Motivation: 了解错误是如何产生的，对于追踪和修复错误至关重要。现有方法很少关注错误生成机制。

Method: 基于统计学中缺失值处理的理论，利用机器学习模型来估计错误是否依赖于数据。

Result: 在基准数据集上的实验证明了MechDetect算法的有效性。

Conclusion: MechDetect算法可以检测缺失值机制，并应用于其他错误类型，前提是存在错误掩码。

Abstract: Data quality monitoring is a core challenge in modern information processing systems. While many approaches to detect data errors or shifts have been proposed, few studies investigate the mechanisms governing error generation. We argue that knowing how errors were generated can be key to tracing and fixing them. In this study, we build on existing work in the statistics literature on missing values and propose MechDetect, a simple algorithm to investigate error generation mechanisms. Given a tabular data set and a corresponding error mask, the algorithm estimates whether or not the errors depend on the data using machine learning models. Our work extends established approaches to detect mechanisms underlying missing values and can be readily applied to other error types, provided that an error mask is available. We demonstrate the effectiveness of MechDetect in experiments on established benchmark datasets.

</details>


### [117] [Decoding Large Language Diffusion Models with Foreseeing Movement](https://arxiv.org/abs/2512.04135)
*Yichuan Mo,Quan Chen,Mingjie Li,Zeming Wei,Yisen Wang*

Main category: cs.LG

TL;DR: 提出了一种新的解码方法，旨在提高大型语言扩散模型（LLDMs）的推理性能，通过综合考虑局部和全局效应来优化解码顺序。


<details>
  <summary>Details</summary>
Motivation: 现有的启发式方法主要关注局部效应，忽略了长期影响，导致推理性能对token的解码顺序非常敏感。

Method: 提出Foreseeing Decoding Method (FDM)，它整合了局部和全局考虑，采用基于搜索的策略在离散空间中实现有效优化。还提出FDM with Acceleration (FDM-A)，通过分析完整解码过程中所选token的一致性，将深度探索限制在被识别为探索和平衡环境的关键步骤中。

Result: 在不同的基准测试和模型架构上的大量实验验证了FDM的可扩展性，并证明了FDM-A实现了卓越的效率-性能权衡。

Conclusion: 这项工作可能为LLDMs更强大的解码方法提供有原则的一步。

Abstract: Large Language Diffusion Models (LLDMs) benefit from a flexible decoding mechanism that enables parallelized inference and controllable generations over autoregressive models. Yet such flexibility introduces a critical challenge: inference performance becomes highly sensitive to the decoding order of tokens. Existing heuristic methods, however, focus mainly on local effects while overlooking long-term impacts. To address this limitation, we propose the Foreseeing Decoding Method (FDM), a novel approach that integrates both local and global considerations to unlock the full potential, employing a search-based strategy to enable effective optimization in discrete spaces. Furthermore, by analyzing the consistency of chosen tokens in the full decoding process, we develop a variant, FDM with Acceleration (FDM-A), which restricts deep exploration to critical steps identified as the exploration and balance circumantences. Extensive experiments across diverse benchmarks and model architectures validate the scalability of FDM and demonstrate the superior efficiency-performance trade-off achieved by FDM-A. Our work might potentially provide a principled step toward more powerful decoding methods for LLDMs.

</details>


### [118] [Mitigating the Curse of Detail: Scaling Arguments for Feature Learning and Sample Complexity](https://arxiv.org/abs/2512.04165)
*Noa Rubin,Orit Davidovich,Zohar Ringel*

Main category: cs.LG

TL;DR: 本文提出了一种预测深度学习中特征学习出现的尺度的方法，该方法比现有理论更简单，并且可以推广到更复杂的模型。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习理论在解释特征学习机制和确定网络隐式偏差方面存在挑战，现有理论主要集中在单层或双层网络或深度线性网络，且预测结果通常为高维非线性方程，计算复杂。

Method: 提出了一种强大的启发式尺度分析方法。

Result: 该方法重现了各种已知结果的尺度指数，并对复杂玩具架构（如三层非线性网络和注意力头）做出了新的预测。

Conclusion: 该方法扩展了深度学习的第一性原理理论的范围。

Abstract: Two pressing topics in the theory of deep learning are the interpretation of feature learning mechanisms and the determination of implicit bias of networks in the rich regime. Current theories of rich feature learning effects revolve around networks with one or two trainable layers or deep linear networks. Furthermore, even under such limiting settings, predictions often appear in the form of high-dimensional non-linear equations, which require computationally intensive numerical solutions. Given the many details that go into defining a deep learning problem, this analytical complexity is a significant and often unavoidable challenge. Here, we propose a powerful heuristic route for predicting the data and width scales at which various patterns of feature learning emerge. This form of scale analysis is considerably simpler than such exact theories and reproduces the scaling exponents of various known results. In addition, we make novel predictions on complex toy architectures, such as three-layer non-linear networks and attention heads, thus extending the scope of first-principle theories of deep learning.

</details>


### [119] [BEP: A Binary Error Propagation Algorithm for Binary Neural Networks Training](https://arxiv.org/abs/2512.04189)
*Luca Colombo,Fabrizio Pittorino,Daniele Zambon,Carlo Baldassi,Manuel Roveri,Cesare Alippi*

Main category: cs.LG

TL;DR: 这篇论文介绍了一种名为二元误差传播（BEP）的新算法，用于训练二元神经网络（BNNs）。该算法能够在完全二元变量上运行，并使用位运算执行所有前向和后向计算。


<details>
  <summary>Details</summary>
Motivation: 二元神经网络（BNNs）在计算复杂性、内存占用和能源消耗方面具有显著优势，使其非常适合在资源受限的设备上部署。然而，由于其变量的离散性，通过基于梯度的优化来训练BNNs仍然具有挑战性。现有的量化感知训练方法虽然可以解决这个问题，但需要在训练期间保持潜在的全精度参数，并使用浮点运算执行反向传播，从而丧失了二元运算的效率。基于局部学习规则的替代方法不适合全局信用分配和多层架构中的误差反向传播。

Method: 论文介绍了二元误差传播（BEP），这是一种建立在反向传播链式法则基础上的离散模拟算法。该机制能够将表示为二元向量的误差信号反向传播通过神经网络的多个层。

Result: 在多层感知器和循环神经网络上验证了BEP的有效性，在测试精度方面分别提高了+6.89%和+10.57%。

Conclusion: BEP是第一个为循环神经网络架构实现端到端二元训练的解决方案。

Abstract: Binary Neural Networks (BNNs), which constrain both weights and activations to binary values, offer substantial reductions in computational complexity, memory footprint, and energy consumption. These advantages make them particularly well suited for deployment on resource-constrained devices. However, training BNNs via gradient-based optimization remains challenging due to the discrete nature of their variables. The dominant approach, quantization-aware training, circumvents this issue by employing surrogate gradients. Yet, this method requires maintaining latent full-precision parameters and performing the backward pass with floating-point arithmetic, thereby forfeiting the efficiency of binary operations during training. While alternative approaches based on local learning rules exist, they are unsuitable for global credit assignment and for back-propagating errors in multi-layer architectures. This paper introduces Binary Error Propagation (BEP), the first learning algorithm to establish a principled, discrete analog of the backpropagation chain rule. This mechanism enables error signals, represented as binary vectors, to be propagated backward through multiple layers of a neural network. BEP operates entirely on binary variables, with all forward and backward computations performed using only bitwise operations. Crucially, this makes BEP the first solution to enable end-to-end binary training for recurrent neural network architectures. We validate the effectiveness of BEP on both multi-layer perceptrons and recurrent neural networks, demonstrating gains of up to +6.89% and +10.57% in test accuracy, respectively. The proposed algorithm is released as an open-source repository.

</details>


### [120] [Network of Theseus (like the ship)](https://arxiv.org/abs/2512.04198)
*Vighnesh Subramaniam,Colin Conwell,Boris Katz,Andrei Barbu,Brian Cheung*

Main category: cs.LG

TL;DR: 提出了Network of Theseus (NoT)方法，可以在保持guide网络性能的同时，逐步将guide网络架构转换为完全不同的target网络架构。


<details>
  <summary>Details</summary>
Motivation: 神经网络架构的归纳偏置必须从训练到推理持续存在，这限制了选择具有理想效率或设计属性的架构。

Method: 逐步将guide网络架构中的组件替换为target架构模块，并通过表征相似性度量进行对齐。即使在架构发生重大变化的情况下，此过程也能在很大程度上保持guide网络的功能。

Result: 通过将优化与部署分离，NoT 扩大了可行的推理时架构空间，为更好的精度-效率权衡开辟了机会，并能够更有针对性地探索架构设计空间。

Conclusion: NoT方法能够将一个训练好的网络逐步转换为一个完全不同的目标网络，同时保持原网络的性能。

Abstract: A standard assumption in deep learning is that the inductive bias introduced by a neural network architecture must persist from training through inference. The architecture you train with is the architecture you deploy. This assumption constrains the community from selecting architectures that may have desirable efficiency or design properties due to difficulties with optimization. We challenge this assumption with Network of Theseus (NoT), a method for progressively converting a trained, or even untrained, guide network architecture part-by-part into an entirely different target network architecture while preserving the performance of the guide network. At each stage, components in the guide network architecture are incrementally replaced with target architecture modules and aligned via representational similarity metrics. This procedure largely preserves the functionality of the guide network even under substantial architectural changes-for example, converting a convolutional network into a multilayer perceptron, or GPT-2 into a recurrent neural network. By decoupling optimization from deployment, NoT expands the space of viable inference-time architectures, opening opportunities for better accuracy-efficiency tradeoffs and enabling more directed exploration of the architectural design space.

</details>


### [121] [ActVAE: Modelling human activity schedules with a deep conditional generative approach](https://arxiv.org/abs/2512.04223)
*Fred Shone,Tim Hillel*

Main category: cs.LG

TL;DR: 提出了一种用于建模人类活动安排行为的深度条件生成机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 建模人类活动安排行为的复杂性和多样性具有内在挑战性。

Method: 结合了结构化潜在生成方法和条件方法，通过一种新的条件VAE架构。

Result: 该模型能够快速生成针对不同输入标签的精确和真实的计划表，并在联合密度估计框架和多个案例研究中进行了广泛的评估。

Conclusion: 通过将组合方法与纯生成模型和纯条件模型进行比较，突出了使用深度生成方法显式建模复杂和多样的人类行为随机性的有效性。

Abstract: Modelling the complexity and diversity of human activity scheduling behaviour is inherently challenging. We demonstrate a deep conditional-generative machine learning approach for the modelling of realistic activity schedules depending on input labels such as an individual's age, employment status, or other information relevant to their scheduling. We combine (i) a structured latent generative approach, with (ii) a conditional approach, through a novel Conditional VAE architecture. This allows for the rapid generation of precise and realistic schedules for different input labels. We extensively evaluate model capabilities using a joint density estimation framework and several case studies. We additionally show that our approach has practical data and computational requirements, and can be deployed within new and existing demand modelling frameworks. We evaluate the importance of generative capability more generally, by comparing our combined approach to (i) a purely generative model without conditionality, and (ii) a purely conditional model which outputs the most likely schedule given the input labels. This comparison highlights the usefulness of explicitly modelling the randomness of complex and diverse human behaviours using deep generative approaches.

</details>


### [122] [Fine-Tuning ChemBERTa for Predicting Inhibitory Activity Against TDP1 Using Deep Learning](https://arxiv.org/abs/2512.04252)
*Baichuan Zeng*

Main category: cs.LG

TL;DR: 本研究提出了一种基于ChemBERTa的深度学习框架，用于预测小分子对Tyrosyl-DNA Phosphodiesterase 1 (TDP1)的抑制活性。


<details>
  <summary>Details</summary>
Motivation: 克服癌症化学抗性是早期药物发现中的一个关键挑战，而预测小分子对TDP1的抑制能力是其中的关键。

Method: 使用经过微调的ChemBERTa变体，通过分子SMILES字符串定量回归pIC50值。利用大规模数据集，系统地评估了两种预训练策略：Masked Language Modeling (MLM)和Masked Token Regression (MTR)。

Result: 该方法在回归准确性和虚拟筛选效用方面优于经典基线Random Predictor，并且与Random Forest相比具有竞争优势，实现了高富集因子和精确度。

Conclusion: 该模型为实验测试中优先考虑TDP1抑制剂提供了一个稳健且易于部署的工具，展示了化学Transformer在加速靶向药物发现方面的变革潜力。

Abstract: Predicting the inhibitory potency of small molecules against Tyrosyl-DNA Phosphodiesterase 1 (TDP1)-a key target in overcoming cancer chemoresistance-remains a critical challenge in early drug discovery. We present a deep learning framework for the quantitative regression of pIC50 values from molecular Simplified Molecular Input Line Entry System (SMILES) strings using fine-tuned variants of ChemBERTa, a pre-trained chemical language model. Leveraging a large-scale consensus dataset of 177,092 compounds, we systematically evaluate two pre-training strategies-Masked Language Modeling (MLM) and Masked Token Regression (MTR)-under stratified data splits and sample weighting to address severe activity imbalance which only 2.1% are active. Our approach outperforms classical baselines Random Predictor in both regression accuracy and virtual screening utility, and has competitive performance compared to Random Forest, achieving high enrichment factor EF@1% 17.4 and precision Precision@1% 37.4 among top-ranked predictions. The resulting model, validated through rigorous ablation and hyperparameter studies, provides a robust, ready-to-deploy tool for prioritizing TDP1 inhibitors for experimental testing. By enabling accurate, 3D-structure-free pIC50 prediction directly from SMILES, this work demonstrates the transformative potential of chemical transformers in accelerating target-specific drug discovery.

</details>


### [123] [Studying Various Activation Functions and Non-IID Data for Machine Learning Model Robustness](https://arxiv.org/abs/2512.04264)
*Long Dang,Thushari Hapuarachchi,Kaiqi Xiong,Jing Lin*

Main category: cs.LG

TL;DR: 研究了在集中式和联邦学习环境中，使用不同激活函数进行对抗训练时，机器学习模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 提高机器学习模型的鲁棒性，现有研究多集中于ReLU激活函数和集中式训练环境。

Method: 1. 提出一种先进的对抗训练方法，结合模型架构改变、软标签、简化数据增强和可变学习率。 2. 在集中式和联邦学习环境中，对包括ReLU在内的十种激活函数进行实验。 3. 在联邦学习环境中，引入数据共享以解决非独立同分布（non-IID）数据下的性能下降问题。

Result: 1. 在集中式环境中，提出的对抗训练方法在CIFAR-10数据集上取得了较高的自然准确率和鲁棒准确率。 2. ReLU激活函数通常表现最佳。 3. 在联邦学习环境中，鲁棒准确率显著下降，但在数据共享后性能得到提升。

Conclusion: 适当比例的数据共享可以显著提高机器学习模型的鲁棒性，这在实际应用中很有用。

Abstract: Adversarial training is an effective method to improve the machine learning (ML) model robustness. Most existing studies typically consider the Rectified linear unit (ReLU) activation function and centralized training environments. In this paper, we study the ML model robustness using ten different activation functions through adversarial training in centralized environments and explore the ML model robustness in federal learning environments. In the centralized environment, we first propose an advanced adversarial training approach to improving the ML model robustness by incorporating model architecture change, soft labeling, simplified data augmentation, and varying learning rates. Then, we conduct extensive experiments on ten well-known activation functions in addition to ReLU to better understand how they impact the ML model robustness. Furthermore, we extend the proposed adversarial training approach to the federal learning environment, where both independent and identically distributed (IID) and non-IID data settings are considered. Our proposed centralized adversarial training approach achieves a natural and robust accuracy of 77.08% and 67.96%, respectively on CIFAR-10 against the fast gradient sign attacks. Experiments on ten activation functions reveal ReLU usually performs best. In the federated learning environment, however, the robust accuracy decreases significantly, especially on non-IID data. To address the significant performance drop in the non-IID data case, we introduce data sharing and achieve the natural and robust accuracy of 70.09% and 54.79%, respectively, surpassing the CalFAT algorithm, when 40% data sharing is used. That is, a proper percentage of data sharing can significantly improve the ML model robustness, which is useful to some real-world applications.

</details>


### [124] [The Initialization Determines Whether In-Context Learning Is Gradient Descent](https://arxiv.org/abs/2512.04268)
*Shifeng Xie,Rui Yuan,Simone Rossi,Thomas Hannagan*

Main category: cs.LG

TL;DR: 本研究致力于理解大型语言模型中上下文学习(ICL)的机制，特别是线性自注意力(LSA)与梯度下降(GD)之间的关系。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明LSA与GD有关，但其简化的假设条件受到了质疑。这项研究旨在更真实的条件下，研究多头LSA如何逼近GD，具体来说，是在ICL的线性回归公式中加入非零高斯先验均值。

Method: 1. 扩展多头LSA嵌入矩阵，引入查询的初始估计，称为初始猜测。2. 证明了ICL线性回归设置所需的head数量的上限。3. 引入yq-LSA，这是一种单头LSA的简单概括，具有可训练的初始猜测yq。4. 在线性回归任务上进行了实验验证，并考虑了具有初始猜测能力的LLM，并在语义相似性任务上验证了其性能。

Result: 1. 实验证实了head数量上限的结果。2. 观察到一步GD和多头LSA之间仍然存在性能差距。3. 理论上建立了yq-LSA的能力。4. 具有初始猜测能力的LLM在语义相似性任务上的性能得到了提高。

Conclusion: 这项研究扩展了连接ICL和GD的理论，并通过引入初始猜测的概念，提高了LSA在模拟梯度下降方面的能力，并最终提升了LLM在特定任务上的性能。

Abstract: In-context learning (ICL) in large language models (LLMs) is a striking phenomenon, yet its underlying mechanisms remain only partially understood. Previous work connects linear self-attention (LSA) to gradient descent (GD), this connection has primarily been established under simplified conditions with zero-mean Gaussian priors and zero initialization for GD. However, subsequent studies have challenged this simplified view by highlighting its overly restrictive assumptions, demonstrating instead that under conditions such as multi-layer or nonlinear attention, self-attention performs optimization-like inference, akin to but distinct from GD. We investigate how multi-head LSA approximates GD under more realistic conditions specifically when incorporating non-zero Gaussian prior means in linear regression formulations of ICL. We first extend multi-head LSA embedding matrix by introducing an initial estimation of the query, referred to as the initial guess. We prove an upper bound on the number of heads needed for ICL linear regression setup. Our experiments confirm this result and further observe that a performance gap between one-step GD and multi-head LSA persists. To address this gap, we introduce yq-LSA, a simple generalization of single-head LSA with a trainable initial guess yq. We theoretically establish the capabilities of yq-LSA and provide experimental validation on linear regression tasks, thereby extending the theory that bridges ICL and GD. Finally, inspired by our findings in the case of linear regression, we consider widespread LLMs augmented with initial guess capabilities, and show that their performance is improved on a semantic similarity task.

</details>


### [125] [Bootstrapped Mixed Rewards for RL Post-Training: Injecting Canonical Action Order](https://arxiv.org/abs/2512.04277)
*Prakhar Gupta,Vaibhav Gupta*

Main category: cs.LG

TL;DR: 使用强化学习进行后训练通常优化单一标量目标，忽略解决方案的产生方式的结构。我们研究了在强化学习后训练期间使用的，针对规范解算器排序的标量提示，即使在随机解序列上进行微调时，是否可以提高性能。在 Sudoku 上，我们使用标准微调在随机求解顺序上训练 Transformer，然后使用组相对策略优化 (GRPO) 对其进行后训练，并提供两个奖励：单元格准确率和排序奖励，当模型的发射顺序与求解器顺序一致时，排序奖励会增加。为了干净地比较信号，我们将它们通过固定混合进行组合，并使用简单的自举缩放来均衡初始化时的分量幅度。混合奖励通常优于仅单元格优化——最佳混合产生的测试准确率远高于在随机顺序上训练的仅微调模型，并且在准确率上接近在求解器顺序序列上训练的仅微调模型。这些结果表明，粗略的排序信号可以引导强化学习后训练朝着求解器顺序轨迹发展，而无需修改监督数据或架构。


<details>
  <summary>Details</summary>
Motivation: 研究标量提示对强化学习后训练的影响，以提高模型性能。

Method: 使用组相对策略优化 (GRPO) 对 Transformer 模型进行后训练，并结合单元格准确率和排序奖励。

Result: 混合奖励优于仅单元格优化，最佳混合产生的测试准确率远高于在随机顺序上训练的仅微调模型，并且在准确率上接近在求解器顺序序列上训练的仅微调模型。

Conclusion: 粗略的排序信号可以引导强化学习后训练朝着求解器顺序轨迹发展，而无需修改监督数据或架构。

Abstract: Post-training with reinforcement learning (RL) typically optimizes a single scalar objective and ignores structure in how solutions are produced. We ask whether a scalar hint toward a canonical solver ordering, used only during RL post-training, improves performance even when fine-tuned on randomized solution sequences. On Sudoku, we train a Transformer with standard fine-tuning on randomized solving orders, then post-train it with Group Relative Policy Optimization (GRPO) with two rewards: cell accuracy and an ordering reward that increases when the model's emission order aligns with the solver order. To compare signals cleanly, we combine them via fixed mixtures and use a simple bootstrapped scaling to equalize component magnitudes at initialization. Mixed rewards generally outperform cell-only optimization--the best mixture yields substantially higher test accuracy than the fine-tuned-only model trained on random-order and approaches the fine-tuned-only model trained on solver-order sequences in accuracy. These results suggest that coarse ordering signals can steer RL post-training toward solver-order trajectories without modifying supervised data or architecture.

</details>


### [126] [GRASP: GRouped Activation Shared Parameterization for Parameter-Efficient Fine-Tuning and Robust Inference of Transformers](https://arxiv.org/abs/2512.04296)
*Malyaban Bal,Abhronil Sengupta*

Main category: cs.LG

TL;DR: GRASP is a parameter-efficient fine-tuning (PEFT) framework that reduces the number of trainable parameters by partitioning token representations into groups and learning a shared scaling and shifting vector for each group. StochGRASP extends this by learning Gaussian distributions as perturbations and improves robustness under noisy conditions.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods have limitations in terms of parameter efficiency and robustness to hardware-level variability, especially for edge deployment.

Method: The paper introduces GRASP, which uses grouped activation shared parameterization to reduce trainable parameters. It also proposes StochGRASP, a probabilistic extension that learns Gaussian distributions to model hardware variability.

Result: GRASP matches or exceeds the performance of existing PEFT methods with an order of magnitude reduction in trainable parameters. StochGRASP outperforms deterministic variants under noisy conditions.

Conclusion: GRASP and StochGRASP are suitable for energy-efficient and noise-prone hardware platforms, making them promising for edge-based AI hardware deployment.

Abstract: Parameter-efficient fine-tuning (PEFT) provides a scalable alternative to full-model adaptation by updating only a small subset of parameters in large pre-trained models. We introduce GRASP - GRouped Activation Shared Parameterization - a lightweight PEFT framework that partitions the D-dimensional token representations of selected layers into K << D groups and learns a shared scaling and shifting vector for each group. This grouped modulation reduces the number of trainable parameters significantly while preserving the ability of the model to learn task-specific features. Building on this formulation, we further propose StochGRASP, which learns Gaussian distributions as perturbations to the pre-trained weights rather than deterministic values. This probabilistic parameterization along with a noise-aware loss function formulation enables modelling hardware-level variability in programmed weights and significantly improves robustness under non-ideal inference conditions-an important requirement for deployment on edge-based emerging AI hardware. Across GLUE (RoBERTa-base & RoBERTa-large) and E2E NLG (GPT-2 Medium), GRASP matches or exceeds the performance of established PEFT methods while achieving an order of magnitude reduction in trainable parameters compared to LoRA and BitFit. Under varying levels of noise, StochGRASP consistently outperforms deterministic variants, demonstrating its suitability for energy-efficient and noise-prone hardware platforms.

</details>


### [127] [When do spectral gradient updates help in deep learning?](https://arxiv.org/abs/2512.04299)
*Damek Davis,Dmitriy Drusvyatskiy*

Main category: cs.LG

TL;DR: 光谱梯度方法是训练深度神经网络和transformers的一种有前景的替代方案。本文提出了一个简单的分层条件，可以预测何时光谱更新比欧几里德梯度步长产生更大的损失减少。


<details>
  <summary>Details</summary>
Motivation: 了解光谱梯度方法在哪些情况下表现更好

Method: 比较每个参数块的梯度的平方核-弗罗贝尼乌斯比与传入激活的稳定秩。证明了后激活矩阵在高斯初始化时具有低稳定秩。在 spiked 随机特征模型中表明，在短时间预热后，欧几里德梯度的核-弗罗贝尼乌斯比随数据维度增长，而激活的稳定秩保持有界，因此光谱更新的预测优势随维度缩放。

Result: 中间激活在整个训练过程中具有低稳定秩，并且相应的梯度保持较大的核-弗罗贝尼乌斯比。验证了在合成回归实验和 NanoGPT 规模的语言模型训练中的预测。

Conclusion: 这些结果确定了光谱梯度方法（如 Muon）在训练深度网络和 transformers 中有效的条件。

Abstract: Spectral gradient methods, such as the recently popularized Muon optimizer, are a promising alternative to standard Euclidean gradient descent for training deep neural networks and transformers, but it is still unclear in which regimes they are expected to perform better. We propose a simple layerwise condition that predicts when a spectral update yields a larger decrease in the loss than a Euclidean gradient step. This condition compares, for each parameter block, the squared nuclear-to-Frobenius ratio of the gradient to the stable rank of the incoming activations. To understand when this condition may be satisfied, we first prove that post-activation matrices have low stable rank at Gaussian initialization in random feature regression, feedforward networks, and transformer blocks. In spiked random feature models we then show that, after a short burn-in, the Euclidean gradient's nuclear-to-Frobenius ratio grows with the data dimension while the stable rank of the activations remains bounded, so the predicted advantage of spectral updates scales with dimension. We validate these predictions in synthetic regression experiments and in NanoGPT-scale language model training, where we find that intermediate activations have low-stable-rank throughout training and the corresponding gradients maintain large nuclear-to-Frobenius ratios. Together, these results identify conditions for spectral gradient methods, such as Muon, to be effective in training deep networks and transformers.

</details>


### [128] [Evaluating Long-Context Reasoning in LLM-Based WebAgents](https://arxiv.org/abs/2512.04307)
*Andy Chung,Yichi Zhang,Kaixiang Lin,Aditya Rawal,Qiaozi Gao,Joyce Chai*

Main category: cs.LG

TL;DR: 大型语言模型（LLM）驱动的代理在日常数字交互中日益普及，它们在长交互历史中进行推理的能力对于提供个性化和情境感知的帮助至关重要。本文针对WebAgent在真实Web环境中操作的长上下文推理能力进行了基准评估，发现随着上下文长度的增加，模型性能显著下降。提出了一个隐式的RAG方法，通过生成任务相关的摘要来提供适度的改进。


<details>
  <summary>Details</summary>
Motivation: 评估WebAgent在长上下文场景中的推理能力，特别是在实际Web环境中，这是一个尚未充分探索的领域。

Method: 通过构建一个基准，该基准通过顺序相关的子任务评估WebAgent的长上下文推理能力，这些子任务需要从扩展的交互历史中检索和应用信息。该框架模拟多会话用户交互，通过在相关的子任务之间注入不相关的任务轨迹，创建25,000到150,000个token的上下文。

Result: 对四种流行的模型（Claude-3.7、GPT-4.1、Llama 4和o4-mini）进行了广泛的评估，观察到随着上下文长度的增加，性能显著下降，成功率从基线条件的40-50%降至长上下文场景中的10%以下。

Conclusion: 研究结果强调了在实际的长期用户交互场景中部署WebAgent的关键挑战，并为开发更强大的代理架构提供了见解，这些架构能够在扩展的上下文中保持连贯的任务执行。

Abstract: As large language model (LLM)-based agents become increasingly integrated into daily digital interactions, their ability to reason across long interaction histories becomes crucial for providing personalized and contextually aware assistance. However, the performance of these agents in long context scenarios, particularly for action-taking WebAgents operating in realistic web environments, remains largely unexplored. This paper introduces a benchmark for evaluating long context reasoning capabilities of WebAgents through sequentially dependent subtasks that require retrieval and application of information from extended interaction histories. We develop a novel evaluation framework that simulates multi-session user interactions by injecting irrelevant task trajectories between dependent subtasks, creating contexts ranging from 25,000 to 150,000 tokens. Through extensive evaluation of four popular models, Claude-3.7, GPT-4.1, Llama 4, and o4-mini, we observe a dramatic performance degradation as context length increases, with success rates dropping from 40-50\% in baseline conditions to less than 10\% in long context scenarios. Our detailed error analysis reveals that agents primarily fail due to getting stuck in loops and losing track of original task objectives. We further propose an implicit RAG approach that provides modest improvements by generating task-relevant summaries, though fundamental limitations in long context reasoning persist. These findings highlight critical challenges for deploying WebAgents in realistic, long-term user interaction scenarios and provide insights for developing more robust agent architectures capable of maintaining coherent task execution across extended contexts.

</details>


### [129] [RNNs perform task computations by dynamically warping neural representations](https://arxiv.org/abs/2512.04310)
*Arthur Pellegrino,Angus Chadwick*

Main category: cs.LG

TL;DR: 本研究探讨了循环神经网络(RNNs)如何通过动态弯曲其任务变量的表示来进行计算。


<details>
  <summary>Details</summary>
Motivation: 理解计算动力学和表征几何之间的联系。

Method: 开发黎曼几何框架，从输入流形中推导出动力系统的流形拓扑和几何。

Result: 表明动态弯曲是RNNs计算的一个基本特征。

Conclusion: 动态弯曲是RNNs计算的一个基本特征。

Abstract: Analysing how neural networks represent data features in their activations can help interpret how they perform tasks. Hence, a long line of work has focused on mathematically characterising the geometry of such "neural representations." In parallel, machine learning has seen a surge of interest in understanding how dynamical systems perform computations on time-varying input data. Yet, the link between computation-through-dynamics and representational geometry remains poorly understood. Here, we hypothesise that recurrent neural networks (RNNs) perform computations by dynamically warping their representations of task variables. To test this hypothesis, we develop a Riemannian geometric framework that enables the derivation of the manifold topology and geometry of a dynamical system from the manifold of its inputs. By characterising the time-varying geometry of RNNs, we show that dynamic warping is a fundamental feature of their computations.

</details>


### [130] [Data-regularized Reinforcement Learning for Diffusion Models at Scale](https://arxiv.org/abs/2512.04332)
*Haotian Ye,Kaiwen Zheng,Jiashu Xu,Puheng Li,Huayu Chen,Jiaqi Han,Sheng Liu,Qinsheng Zhang,Hanzi Mao,Zekun Hao,Prithvijit Chattopadhyay,Dinghao Yang,Liang Feng,Maosheng Liao,Junjie Bai,Ming-Yu Liu,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: 提出了一种名为数据正则化扩散强化学习 (DDRL) 的新框架，该框架使用前向 KL 散度将策略锚定到离线数据分布，从而实现 RL 与标准扩散训练的稳健、无偏集成。


<details>
  <summary>Details</summary>
Motivation: 将生成扩散模型与人类偏好对齐至关重要但具有挑战性。现有算法容易出现奖励攻击，例如质量下降、过度风格化或降低多样性，这是由于其正则化的固有局限性造成的，该局限性提供了不可靠的惩罚。

Method: DDRL 结合了奖励最大化和扩散损失最小化。

Result: DDRL 显着提高了奖励，同时减轻了基线中看到的奖励攻击，在人类偏好方面取得了最高水平，并为扩散后训练建立了稳健且可扩展的范例。在超过一百万个 GPU 小时的实验和一万个双盲人工评估中，在高分辨率视频生成任务上证明了这一点。

Conclusion: DDRL 是一种很有前途的扩散模型对齐方法，它通过数据正则化来提高稳健性和避免奖励攻击。

Abstract: Aligning generative diffusion models with human preferences via reinforcement learning (RL) is critical yet challenging. Most existing algorithms are often vulnerable to reward hacking, such as quality degradation, over-stylization, or reduced diversity. Our analysis demonstrates that this can be attributed to the inherent limitations of their regularization, which provides unreliable penalties. We introduce Data-regularized Diffusion Reinforcement Learning (DDRL), a novel framework that uses the forward KL divergence to anchor the policy to an off-policy data distribution. Theoretically, DDRL enables robust, unbiased integration of RL with standard diffusion training. Empirically, this translates into a simple yet effective algorithm that combines reward maximization with diffusion loss minimization. With over a million GPU hours of experiments and ten thousand double-blind human evaluations, we demonstrate on high-resolution video generation tasks that DDRL significantly improves rewards while alleviating the reward hacking seen in baselines, achieving the highest human preference and establishing a robust and scalable paradigm for diffusion post-training.

</details>


### [131] [RGE-GCN: Recursive Gene Elimination with Graph Convolutional Networks for RNA-seq based Early Cancer Detection](https://arxiv.org/abs/2512.04333)
*Shreyas Shende,Varsha Narayanan,Vishal Fenn,Yiran Huang,Dincer Goksuluk,Gaurav Choudhary,Melih Agraz,Mengjia Xu*

Main category: cs.LG

TL;DR: 提出了一种名为RGE-GCN的新框架，用于从RNA-seq数据中识别癌症生物标志物。


<details>
  <summary>Details</summary>
Motivation: 早期癌症检测在提高生存率方面起着关键作用，但从RNA-seq数据中识别可靠的生物标志物仍然是一个主要挑战。数据是高维的，传统的统计方法通常无法捕捉基因之间复杂的关系。

Method: 该方法构建基因表达谱图，使用图卷积网络进行分类，并应用集成梯度来突出信息量最大的基因。通过递归地删除不太相关的基因，该模型收敛到一组紧凑的生物标志物。

Result: 在所有数据集上，该方法比DESeq2、edgeR和limma-voom等标准工具实现了更高的准确率和F1分数。重要的是，所选择的基因与包括PI3K-AKT、MAPK、SUMOylation和免疫调节在内的著名癌症通路相一致。

Conclusion: RGE-GCN有希望成为一种基于RNA-seq的早期癌症检测和生物标志物发现的通用方法。

Abstract: Early detection of cancer plays a key role in improving survival rates, but identifying reliable biomarkers from RNA-seq data is still a major challenge. The data are high-dimensional, and conventional statistical methods often fail to capture the complex relationships between genes. In this study, we introduce RGE-GCN (Recursive Gene Elimination with Graph Convolutional Networks), a framework that combines feature selection and classification in a single pipeline. Our approach builds a graph from gene expression profiles, uses a Graph Convolutional Network to classify cancer versus normal samples, and applies Integrated Gradients to highlight the most informative genes. By recursively removing less relevant genes, the model converges to a compact set of biomarkers that are both interpretable and predictive. We evaluated RGE-GCN on synthetic data as well as real-world RNA-seq cohorts of lung, kidney, and cervical cancers. Across all datasets, the method consistently achieved higher accuracy and F1-scores than standard tools such as DESeq2, edgeR, and limma-voom. Importantly, the selected genes aligned with well-known cancer pathways including PI3K-AKT, MAPK, SUMOylation, and immune regulation. These results suggest that RGE-GCN shows promise as a generalizable approach for RNA-seq based early cancer detection and biomarker discovery (https://rce-gcn.streamlit.app/ ).

</details>


### [132] [Long-Horizon Model-Based Offline Reinforcement Learning Without Conservatism](https://arxiv.org/abs/2512.04341)
*Tianwei Ni,Esther Derman,Vineet Jain,Vincent Taboga,Siamak Ravanbakhsh,Pierre-Luc Bacon*

Main category: cs.LG

TL;DR: 本文提出了一种基于贝叶斯思想的离线强化学习算法Neubay，该算法通过对世界模型建立后验分布来处理离线数据中的认知不确定性，并训练一个与历史相关的agent来最大化预期收益，从而实现测试时的泛化。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习方法依赖于保守主义，通过惩罚超出数据集的动作或限制规划范围来实现。本文质疑了这一原则的普遍性，并重新审视了一种互补的原则：贝叶斯视角。

Method: 该方法通过对世界模型中的层归一化和自适应长程规划等关键设计选择来减轻复合误差和价值高估问题。提出的Neubay算法基于中性贝叶斯原则。

Result: 在D4RL和NeoRL基准测试中，Neubay通常与领先的保守算法相匹配或超越，并在7个数据集上实现了新的state-of-the-art。值得注意的是，它在数百步的规划范围内取得了成功，挑战了普遍的看法。

Conclusion: 本文表明，Neubay在低质量数据集上优于保守主义，并为离线和基于模型的强化学习开辟了一个新的方向。

Abstract: Popular offline reinforcement learning (RL) methods rely on conservatism, either by penalizing out-of-dataset actions or by restricting planning horizons. In this work, we question the universality of this principle and instead revisit a complementary one: a Bayesian perspective. Rather than enforcing conservatism, the Bayesian approach tackles epistemic uncertainty in offline data by modeling a posterior distribution over plausible world models and training a history-dependent agent to maximize expected rewards, enabling test-time generalization. We first illustrate, in a bandit setting, that Bayesianism excels on low-quality datasets where conservatism fails. We then scale the principle to realistic tasks, identifying key design choices, such as layer normalization in the world model and adaptive long-horizon planning, that mitigate compounding error and value overestimation. These yield our practical algorithm, Neubay, grounded in the neutral Bayesian principle. On D4RL and NeoRL benchmarks, Neubay generally matches or surpasses leading conservative algorithms, achieving new state-of-the-art on 7 datasets. Notably, it succeeds with planning horizons of several hundred steps, challenging common belief. Finally, we characterize when Neubay is preferable to conservatism, laying the foundation for a new direction in offline and model-based RL.

</details>


### [133] [Distance Is All You Need: Radial Dispersion for Uncertainty Estimation in Large Language Models](https://arxiv.org/abs/2512.04351)
*Manh Nguyen,Sunil Gupta,Hung Le*

Main category: cs.LG

TL;DR: 提出了一种新的、简单且与模型无关的不确定性度量方法，称为径向离散度分数（RDS），用于检测大型语言模型（LLM）的不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于复杂，依赖于脆弱的语义聚类或内部状态，因此需要更简单有效的方法。

Method: 通过测量嵌入空间中采样生成结果的径向离散度来评估不确定性。还提出了一种轻量级的概率加权变体，该变体结合了模型自身的token概率。

Result: 在四个具有挑战性的自由形式问答数据集和多个LLM上，该指标在幻觉检测和答案选择方面取得了最先进的性能，同时保持了对样本大小和嵌入选择的鲁棒性和可扩展性。

Conclusion: RDS 是一种有效且通用的不确定性度量方法，可用于提高LLM的可靠性。

Abstract: Detecting when large language models (LLMs) are uncertain is critical for building reliable systems, yet existing methods are overly complicated, relying on brittle semantic clustering or internal states. We introduce \textbf{Radial Dispersion Score (RDS)}, a simple, parameter-free, fully model-agnostic uncertainty metric that measures the radial dispersion of sampled generations in embedding space. A lightweight probability-weighted variant further incorporates the model's own token probabilities when available, outperforming different nine strong baselines. Moroever, RDS naturally extends to per-sample scoring, enabling applications such as best-of-$N$ selection and confidence-based filtering. Across four challenging free-form QA datasets and multiple LLMs, our metrics achieve state-of-the-art hallucination detection and answer selection performance, while remaining robust and scalable with respect to sample size and embedding choice.

</details>


### [134] [SmartAlert: Implementing Machine Learning-Driven Clinical Decision Support for Inpatient Lab Utilization Reduction](https://arxiv.org/abs/2512.04354)
*April S. Liang,Fatemeh Amrollahi,Yixing Jiang,Conor K. Corbin,Grace Y. E. Kim,David Mui,Trevor Crowell,Aakash Acharya,Sreedevi Mony,Soumya Punnathanam,Jack McKeown,Margaret Smith,Steven Lin,Arnold Milstein,Kevin Schulman,Jason Hom,Michael A. Pfeffer,Tho D. Pham,David Svec,Weihan Chu,Lisa Shieh,Christopher Sharp,Stephen P. Ma,Jonathan H. Chen*

Main category: cs.LG

TL;DR: 本研究介绍了一种基于机器学习的临床决策支持系统SmartAlert，旨在减少不必要的重复实验室测试。


<details>
  <summary>Details</summary>
Motivation: 重复的实验室测试给患者带来负担并增加医疗保健成本，而现有的干预措施效果有限。

Method: 通过在电子健康记录中集成SmartAlert系统，预测稳定的实验室结果，以减少不必要的重复测试。在两家医院的八个急症护理病房中，对9270名入院患者进行了随机对照试验，目标是全血细胞计数（CBC）的利用率。

Result: 结果显示，在SmartAlert显示后的52小时内，CBC结果的数量显著减少（1.54 vs 1.82，p <0.01），且未对次要安全结果产生不利影响，重复测试相对减少了15%。

Conclusion: 基于机器学习的CDS系统，在周密的实施和治理流程的支持下，可以为住院患者的实验室测试提供精确指导，从而安全地减少不必要的重复测试。

Abstract: Repetitive laboratory testing unlikely to yield clinically useful information is a common practice that burdens patients and increases healthcare costs. Education and feedback interventions have limited success, while general test ordering restrictions and electronic alerts impede appropriate clinical care. We introduce and evaluate SmartAlert, a machine learning (ML)-driven clinical decision support (CDS) system integrated into the electronic health record that predicts stable laboratory results to reduce unnecessary repeat testing. This case study describes the implementation process, challenges, and lessons learned from deploying SmartAlert targeting complete blood count (CBC) utilization in a randomized controlled pilot across 9270 admissions in eight acute care units across two hospitals between August 15, 2024, and March 15, 2025. Results show significant decrease in number of CBC results within 52 hours of SmartAlert display (1.54 vs 1.82, p <0.01) without adverse effect on secondary safety outcomes, representing a 15% relative reduction in repetitive testing. Implementation lessons learned include interpretation of probabilistic model predictions in clinical contexts, stakeholder engagement to define acceptable model behavior, governance processes for deploying a complex model in a clinical environment, user interface design considerations, alignment with clinical operational priorities, and the value of qualitative feedback from end users. In conclusion, a machine learning-driven CDS system backed by a deliberate implementation and governance process can provide precision guidance on inpatient laboratory testing to safely reduce unnecessary repetitive testing.

</details>


### [135] [STeP-Diff: Spatio-Temporal Physics-Informed Diffusion Models for Mobile Fine-Grained Pollution Forecasting](https://arxiv.org/abs/2512.04385)
*Nan Zhou,Weijie Hong,Huandong Wang,Jianfeng Zheng,Qiuhua Wang,Yali Song,Xiao-Ping Zhang,Yong Li,Xinlei Chen*

Main category: cs.LG

TL;DR: 提出了一种名为STeP-Diff的时空物理信息扩散模型，用于细粒度空气污染预测，该模型利用移动平台上的便携式传感器收集的数据，并通过深度学习和物理信息相结合的方法，提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 城市管理和健康建筑发展需要细粒度空气污染预测，利用移动平台部署便携式传感器是一种低成本、易维护、广覆盖的数据收集方案，但数据不完整且时间不一致。

Method: 提出了STeP-Diff模型，利用DeepONet模拟空间序列测量，并结合PDE信息扩散模型，从不完整和时变数据中预测时空场。通过PDE约束的正则化框架，确保预测结果与真实测量和污染扩散的基本物理规律对齐。

Result: 在两个城市部署了59个便携式传感设备，运行14天收集数据。实验结果表明，与第二好的算法相比，STeP-Diff在MAE、RMSE和MAPE指标上分别提高了89.12%、82.30%和25.00%。

Conclusion: STeP-Diff模型能有效捕捉空气污染场的时空依赖性，从而实现更精确的预测。

Abstract: Fine-grained air pollution forecasting is crucial for urban management and the development of healthy buildings. Deploying portable sensors on mobile platforms such as cars and buses offers a low-cost, easy-to-maintain, and wide-coverage data collection solution. However, due to the random and uncontrollable movement patterns of these non-dedicated mobile platforms, the resulting sensor data are often incomplete and temporally inconsistent. By exploring potential training patterns in the reverse process of diffusion models, we propose Spatio-Temporal Physics-Informed Diffusion Models (STeP-Diff). STeP-Diff leverages DeepONet to model the spatial sequence of measurements along with a PDE-informed diffusion model to forecast the spatio-temporal field from incomplete and time-varying data. Through a PDE-constrained regularization framework, the denoising process asymptotically converges to the convection-diffusion dynamics, ensuring that predictions are both grounded in real-world measurements and aligned with the fundamental physics governing pollution dispersion. To assess the performance of the system, we deployed 59 self-designed portable sensing devices in two cities, operating for 14 days to collect air pollution data. Compared to the second-best performing algorithm, our model achieved improvements of up to 89.12% in MAE, 82.30% in RMSE, and 25.00% in MAPE, with extensive evaluations demonstrating that STeP-Diff effectively captures the spatio-temporal dependencies in air pollution fields.

</details>


### [136] [Learning to Orchestrate Agents in Natural Language with the Conductor](https://arxiv.org/abs/2512.04388)
*Stefan Nielsen,Edoardo Cetin,Peter Schwendeman,Qi Sun,Jinglue Xu,Yujin Tang*

Main category: cs.LG

TL;DR: 提出了一种新的Conductor模型，通过强化学习自动发现LLM之间有效的协调策略。


<details>
  <summary>Details</summary>
Motivation: 不同提供商的LLM经过大量训练和微调，专注于不同的领域，为了实现LLM之间的有效协作。

Method: 使用强化学习训练Conductor模型，学习设计有针对性的通信拓扑和提示工程指令。

Result: 7B Conductor模型在具有挑战性的推理基准测试中获得了超越任何单个worker的显著性能提升，达到了最先进的结果。

Conclusion: 通过强化学习可以解锁语言模型的协调能力，强大的协调策略可以通过纯粹的端到端奖励最大化在LLM中自然产生。

Abstract: Powerful large language models (LLMs) from different providers have been expensively trained and finetuned to specialize across varying domains. In this work, we introduce a new kind of Conductor model trained with reinforcement learning to automatically discover powerful coordination strategies among LLMs. Our Conductor learns not only to design targeted communication topologies for effective agent-to-agent collaboration, but also to prompt engineer focused instructions to the LLMs to maximally leverage their individual capabilities. We show that, by learning optimal coordination strategies over pools of powerful worker LLMs, a 7B Conductor achieves significant performance gains beyond any individual worker, attaining state-of-the-art results in challenging reasoning benchmarks, such as LiveCodeBench and GPQA. By training with randomized agent pools, our conductor effectively adapts to arbitrary sets of open- and closed-source agents, meeting any user requirements. Furthermore, allowing the Conductor to select itself as a worker gives rise to recursive topologies, elevating performance with a new form of dynamic test-time scaling through online iterative adaptation. More broadly, ours is among the early work demonstrating language model coordination can be unlocked through RL, where powerful coordination strategies emerge naturally in LLMs through pure end-to-end reward maximization.

</details>


### [137] [Feature Engineering vs. Deep Learning for Automated Coin Grading: A Comparative Study on Saint-Gaudens Double Eagles](https://arxiv.org/abs/2512.04464)
*Tanmay Dogra,Eric Ngo,Mohammad Alam,Jean-Paul Talavera,Asim Dahal*

Main category: cs.LG

TL;DR: 传统方法在小数据集和不平衡类别的情况下，优于深度学习。


<details>
  <summary>Details</summary>
Motivation: 质疑深度学习总是优于传统技术的常见观点，以自动分级圣高登斯双鹰金币为例。

Method: 构建了一个基于192个自定义特征的特征人工神经网络（ANN），这些特征来自Sobel边缘检测和HSV颜色分析，并与混合卷积神经网络（CNN）（结合了EfficientNetV2）以及一个简单的支持向量机（SVM）进行比较。

Result: ANN实现了86%的精确匹配率，并在允许3级误差的情况下达到了98%。CNN和SVM的精确匹配率分别为31%和30%。

Conclusion: 在数据量少且类别不平衡的情况下，通过特征设计融入领域专家知识胜过不可解释的端到端深度学习设置。

Abstract: We challenge the common belief that deep learning always trumps older techniques, using the example of grading Saint-Gaudens Double Eagle gold coins automatically. In our work, we put a feature-based Artificial Neural Network built around 192 custom features pulled from Sobel edge detection and HSV color analysis up against a hybrid Convolutional Neural Network that blends in EfficientNetV2, plus a straightforward Support Vector Machine as the control. Testing 1,785 coins graded by experts, the ANN nailed 86% exact matches and hit 98% when allowing a 3-grade leeway. On the flip side, CNN and SVM mostly just guessed the most common grade, scraping by with 31% and 30% exact hits. Sure, the CNN looked good on broader tolerance metrics, but that is because of some averaging trick in regression that hides how it totally flops at picking out specific grades. All told, when you are stuck with under 2,000 examples and lopsided classes, baking in real coin-expert knowledge through feature design beats out those inscrutable, all-in-one deep learning setups. This rings true for other niche quality checks where data's thin and know-how matters more than raw compute.

</details>


### [138] [GraphBench: Next-generation graph learning benchmarking](https://arxiv.org/abs/2512.04475)
*Timo Stoll,Chendi Qian,Ben Finkelshtein,Ali Parviz,Darius Weber,Fabrizio Frasca,Hadar Shavit,Antoine Siraudin,Arman Mielke,Marie Anastacio,Erik Müller,Maya Bechler-Speicher,Michael Bronstein,Mikhail Galkin,Holger Hoos,Mathias Niepert,Bryan Perozzi,Jan Tönshoff,Christopher Morris*

Main category: cs.LG

TL;DR: 图机器学习在各个领域取得了显著进展，但基准测试实践仍然分散，依赖于狭窄的、特定于任务的数据集和不一致的评估协议，这阻碍了可重复性和更广泛的进展。为了解决这个问题，我们引入了 GraphBench，这是一个综合的基准测试套件，涵盖不同的领域和预测任务，包括节点级、边级、图级和生成设置。GraphBench 提供了标准化的评估协议——具有一致的数据集分割和性能指标，这些指标考虑了分布外泛化——以及统一的超参数调整框架。此外，我们使用消息传递神经网络和图Transformer模型对 GraphBench 进行了基准测试，提供了有原则的基线并建立了参考性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图机器学习基准测试实践分散，依赖于狭窄的数据集和不一致的评估协议，阻碍了可重复性和更广泛的进展。

Method: 引入 GraphBench，一个综合的基准测试套件，涵盖不同的领域和预测任务，包括节点级、边级、图级和生成设置。GraphBench 提供了标准化的评估协议，具有一致的数据集分割和性能指标，以及统一的超参数调整框架。使用消息传递神经网络和图 Transformer 模型对 GraphBench 进行了基准测试。

Result: GraphBench 提供了标准化的评估协议，具有一致的数据集分割和性能指标，以及统一的超参数调整框架。使用消息传递神经网络和图 Transformer 模型对 GraphBench 进行了基准测试，提供了有原则的基线并建立了参考性能。

Conclusion: GraphBench 是一个综合的基准测试套件，可以促进图机器学习领域的可重复性和更广泛的进展。

Abstract: Machine learning on graphs has recently achieved impressive progress in various domains, including molecular property prediction and chip design. However, benchmarking practices remain fragmented, often relying on narrow, task-specific datasets and inconsistent evaluation protocols, which hampers reproducibility and broader progress. To address this, we introduce GraphBench, a comprehensive benchmarking suite that spans diverse domains and prediction tasks, including node-level, edge-level, graph-level, and generative settings. GraphBench provides standardized evaluation protocols -- with consistent dataset splits and performance metrics that account for out-of-distribution generalization -- as well as a unified hyperparameter tuning framework. Additionally, we benchmark GraphBench using message-passing neural networks and graph transformer models, providing principled baselines and establishing a reference performance. See www.graphbench.io for further details.

</details>


### [139] [Context-Aware Mixture-of-Experts Inference on CXL-Enabled GPU-NDP Systems](https://arxiv.org/abs/2512.04476)
*Zehao Fan,Zhenyu Liu,Yunzhen Liu,Yayue Hou,Hadjer Benmeziane,Kaoutar El Maghraoui,Liu Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于CXL-NDP的MoE模型推理系统，通过将冷专家卸载到CXL内存并在那里执行，减少了参数移动的开销。


<details>
  <summary>Details</summary>
Motivation: MoE模型推理时，当专家权重超过GPU内存容量时，会产生大量的内存访问开销。

Method: 1. 使用预填充阶段的激活统计信息来指导解码阶段的专家放置；2. 动态地将热专家固定在GPU端的HBM中，其余的映射到CXL-NDP；3. 引入上下文感知的混合精度量化，基于预填充阶段为每个专家分配bitwidth。

Result: 在GPU-NDP系统上的评估表明，该方法比state-of-the-art方法实现了高达8.7倍的解码吞吐量提升，而平均精度下降仅为0.13%。

Conclusion: 该研究表明，使用CXL-NDP作为卸载层，并结合上下文感知优化，可以有效提高MoE模型的推理效率。

Abstract: Mixture-of-Experts (MoE) models scale large language models through conditional computation, but inference becomes memory-bound once expert weights exceed the capacity of GPU memory. In this case, weights must be offloaded to external memory, and fetching them incurs costly and repeated transfers. We address this by adopting CXL-attached near-data processing (CXL-NDP) as the offloading tier to execute cold experts in place, converting expensive parameter movement into cheaper activation movement. Unlike prior GPU-NDP systems that are largely context-agnostic and reactive, we develop a context-aware MoE system that uses prefill-stage activation statistics to guide decoding-stage expert placement, dynamically pins hot experts in GPU-side HBM, and maps the remainder to CXL-NDP. To meet NDP's limited compute throughput, we introduce context-aware mixed-precision quantization that allocates per-expert bitwidths (1-4 bit) based on prefill stage. The resulting MoE inference system overlaps GPU and NDP execution while minimizing cross-device movement. The evaluation on the GPU-NDP system shows that our approach achieves up to an 8.7-fold decoding throughput improvement over the state-of-the-art method, while incurring only a 0.13% average accuracy drop.

</details>


### [140] [Prototype-Based Semantic Consistency Alignment for Domain Adaptive Retrieval](https://arxiv.org/abs/2512.04524)
*Tianle Hu,Weijun Lv,Na Han,Xiaozhao Fang,Jie Wen,Jiaxing Li,Guoxu Zhou*

Main category: cs.LG

TL;DR: 提出了一种新的领域自适应检索框架，通过原型对齐和特征重构来解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有领域自适应检索方法忽略了类级别语义对齐、缺乏伪标签可靠性考虑以及直接量化受域偏移影响的原始特征。

Method: 提出原型语义一致性对齐（PSCA）框架，包括原型学习和域特定量化两个阶段。第一阶段通过正交原型建立类级别语义连接，利用几何邻近性评估伪标签置信度，并进行特征重构。第二阶段在重构特征上进行域特定量化，生成统一的二值哈希码。

Result: 大量实验验证了PSCA在多个数据集上的优越性能。

Conclusion: PSCA框架有效地解决了领域自适应检索中的问题，并在实验中表现出色。

Abstract: Domain adaptive retrieval aims to transfer knowledge from a labeled source domain to an unlabeled target domain, enabling effective retrieval while mitigating domain discrepancies. However, existing methods encounter several fundamental limitations: 1) neglecting class-level semantic alignment and excessively pursuing pair-wise sample alignment; 2) lacking either pseudo-label reliability consideration or geometric guidance for assessing label correctness; 3) directly quantizing original features affected by domain shift, undermining the quality of learned hash codes. In view of these limitations, we propose Prototype-Based Semantic Consistency Alignment (PSCA), a two-stage framework for effective domain adaptive retrieval. In the first stage, a set of orthogonal prototypes directly establishes class-level semantic connections, maximizing inter-class separability while gathering intra-class samples. During the prototype learning, geometric proximity provides a reliability indicator for semantic consistency alignment through adaptive weighting of pseudo-label confidences. The resulting membership matrix and prototypes facilitate feature reconstruction, ensuring quantization on reconstructed rather than original features, thereby improving subsequent hash coding quality and seamlessly connecting both stages. In the second stage, domain-specific quantization functions process the reconstructed features under mutual approximation constraints, generating unified binary hash codes across domains. Extensive experiments validate PSCA's superior performance across multiple datasets.

</details>


### [141] [Explainable Graph Representation Learning via Graph Pattern Analysis](https://arxiv.org/abs/2512.04530)
*Xudong Wang,Ziheng Sun,Chris Ding,Jicong Fan*

Main category: cs.LG

TL;DR: 本文着重于表征层面的可解释图学习，并探讨了图表征中捕获了哪些关于图的特定信息。


<details>
  <summary>Details</summary>
Motivation: 先前的工作主要集中在模型层面和实例层面的可解释图学习，而对可解释图表征学习的研究有限。

Method: 本文受到图核的启发，通过计算特定图模式中的子结构来评估图的相似性。引入了一个框架 (PXGL-GNN)，用于通过图模式分析来学习和解释图表征。

Result: 通过在真实世界数据上学习和解释图表征，并在监督和非监督学习任务中与多个基线方法进行比较，验证了该方法的有效性。

Conclusion: 本文提出了一种新的图学习框架，通过图模式分析，实现了对图表征的学习和解释，并在实验中验证了其有效性。

Abstract: Explainable artificial intelligence (XAI) is an important area in the AI community, and interpretability is crucial for building robust and trustworthy AI models. While previous work has explored model-level and instance-level explainable graph learning, there has been limited investigation into explainable graph representation learning. In this paper, we focus on representation-level explainable graph learning and ask a fundamental question: What specific information about a graph is captured in graph representations? Our approach is inspired by graph kernels, which evaluate graph similarities by counting substructures within specific graph patterns. Although the pattern counting vector can serve as an explainable representation, it has limitations such as ignoring node features and being high-dimensional. To address these limitations, we introduce a framework (PXGL-GNN) for learning and explaining graph representations through graph pattern analysis. We start by sampling graph substructures of various patterns. Then, we learn the representations of these patterns and combine them using a weighted sum, where the weights indicate the importance of each graph pattern's contribution. We also provide theoretical analyses of our methods, including robustness and generalization. In our experiments, we show how to learn and explain graph representations for real-world data using pattern analysis. Additionally, we compare our method against multiple baselines in both supervised and unsupervised learning tasks to demonstrate its effectiveness.

</details>


### [142] [On the Limits of Test-Time Compute: Sequential Reward Filtering for Better Inference](https://arxiv.org/abs/2512.04558)
*Yue Yu,Qiwei Di,Quanquan Gu,Dongruo Zhou*

Main category: cs.LG

TL;DR: 这篇论文研究了如何提升大型语言模型（LLM）的测试时计算（TTC）效率。他们发现现有的最佳n选1（BoN）方法并非最优，并提出了一种奖励过滤的顺序推理方法，该方法选择性地将高质量的生成结果纳入上下文，从而更有效地利用计算资源。实验结果表明，该方法在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时计算方法，如BoN，存在效率限制，其根本原因尚不明确。

Method: 该论文分析了一种混合参考策略模型，并提出了一种奖励过滤的顺序推理方法，该方法根据奖励选择性地整合高质量生成结果。

Result: 理论分析表明，奖励过滤的顺序推理方法比标准的TTC方法具有更强的保证。实验结果表明，该方法在多个基准测试中表现更好。

Conclusion: 奖励过滤的顺序推理是一种有效的测试时计算方法，可以提高大型语言模型的性能。

Abstract: Test-time compute (TTC) has become an increasingly prominent paradigm for enhancing large language models (LLMs). Despite the empirical success of methods such as best-of-$n$ (BoN) sampling and sequential revision, their fundamental limits remain unclear. We address this gap by analyzing a mixture-of-reference policy model and proving that standard BoN is inherently suboptimal. To move closer to the optimal frontier, we study reward-filtered sequential inference, a simple procedure that selectively incorporates only high-reward generations into the context. This mechanism concentrates computation on superior policy candidates and suppresses inferior ones. On the theoretical side, we show that reward-filtered sequential inference yields strictly stronger guarantees than standard TTC paradigms. On the empirical side, we evaluate such an inference strategy across diverse benchmarks and observe consistent improvements over widely used approaches, demonstrating the practical effectiveness of our framework.

</details>


### [143] [Diffusion Fine-Tuning via Reparameterized Policy Gradient of the Soft Q-Function](https://arxiv.org/abs/2512.04559)
*Hyeongyu Kang,Jaewoo Lee,Woocheol Shin,Kiyoung Om,Jinkyoo Park*

Main category: cs.LG

TL;DR: 提出了一种名为软Q扩散微调(SQDF)的KL正则化RL方法，用于扩散对齐，该方法应用了无训练、可微的软Q函数估计的重参数化策略梯度。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型微调方法存在奖励过度优化的问题，导致生成高奖励但不自然的样本，并降低了多样性。为了缓解过度优化。

Method: SQDF，一种KL正则化RL方法，它应用了无训练、可微的软Q函数估计的重参数化策略梯度。此外，还采用了折扣因子、一致性模型和off-policy重放缓冲区等创新。

Result: SQDF在文本到图像对齐中实现了卓越的目标奖励，同时保持了多样性。在在线黑盒优化中，SQDF在保持自然性和多样性的同时，获得了高的样本效率。

Conclusion: SQDF是一种有效的扩散模型对齐方法，可以在实现高奖励的同时保持生成样本的自然性和多样性。

Abstract: Diffusion models excel at generating high-likelihood samples but often require alignment with downstream objectives. Existing fine-tuning methods for diffusion models significantly suffer from reward over-optimization, resulting in high-reward but unnatural samples and degraded diversity. To mitigate over-optimization, we propose \textbf{Soft Q-based Diffusion Finetuning (SQDF)}, a novel KL-regularized RL method for diffusion alignment that applies a reparameterized policy gradient of a training-free, differentiable estimation of the soft Q-function. SQDF is further enhanced with three innovations: a discount factor for proper credit assignment in the denoising process, the integration of consistency models to refine Q-function estimates, and the use of an off-policy replay buffer to improve mode coverage and manage the reward-diversity trade-off. Our experiments demonstrate that SQDF achieves superior target rewards while preserving diversity in text-to-image alignment. Furthermore, in online black-box optimization, SQDF attains high sample efficiency while maintaining naturalness and diversity.

</details>


### [144] [LeMat-GenBench: A Unified Evaluation Framework for Crystal Generative Models](https://arxiv.org/abs/2512.04562)
*Siddharth Betala,Samuel P. Gleason,Ali Ramlaoui,Andy Xu,Georgia Channing,Daniel Levy,Clémentine Fourrier,Nikita Kazeev,Chaitanya K. Joshi,Sékou-Oumar Kaba,Félix Therrien,Alex Hernandez-Garcia,Rocío Mercado,N. M. Anoop Krishnan,Alexandre Duval*

Main category: cs.LG

TL;DR: 提出了LeMat-GenBench，一个用于晶体材料生成模型的统一基准。


<details>
  <summary>Details</summary>
Motivation: 缺乏标准化的评估框架，使得评估和比较生成模型具有挑战性。

Method: 构建了一个包含评估指标的基准，并评估了12个最新的生成模型。

Result: 稳定性增加通常会导致新颖性和多样性降低，没有模型在所有维度上都表现出色。

Conclusion: LeMat-GenBench 为公平的模型比较奠定了基础，旨在指导开发更可靠的、面向发现的晶体材料生成模型。

Abstract: Generative machine learning (ML) models hold great promise for accelerating materials discovery through the inverse design of inorganic crystals, enabling an unprecedented exploration of chemical space. Yet, the lack of standardized evaluation frameworks makes it challenging to evaluate, compare, and further develop these ML models meaningfully. In this work, we introduce LeMat-GenBench, a unified benchmark for generative models of crystalline materials, supported by a set of evaluation metrics designed to better inform model development and downstream applications. We release both an open-source evaluation suite and a public leaderboard on Hugging Face, and benchmark 12 recent generative models. Results reveal that an increase in stability leads to a decrease in novelty and diversity on average, with no model excelling across all dimensions. Altogether, LeMat-GenBench establishes a reproducible and extensible foundation for fair model comparison and aims to guide the development of more reliable, discovery-oriented generative models for crystalline materials.

</details>


### [145] [Reliable Statistical Guarantees for Conformal Predictors with Small Datasets](https://arxiv.org/abs/2512.04566)
*Miguel Sánchez-Domínguez,Lucas Lacasa,Javier de Vicente,Gonzalo Rubio,Eusebio Valero*

Main category: cs.LG

TL;DR: 针对代理模型在安全关键应用中不确定性量化的问题，提出了新的统计保证方法。


<details>
  <summary>Details</summary>
Motivation: 传统的conformal prediction (CP)方法在校准集较小时，覆盖率分布分散，导致不确定性模型可靠性下降。

Method: 提出了一种新的统计保证，为单个conformal predictor的覆盖率提供概率信息。该方法在校准集较大时收敛到CP的标准解，在数据量较小时仍能提供可靠的覆盖率信息。

Result: 通过一系列例子验证了该方法的有效性，并实现了一个开放访问的软件解决方案。

Conclusion: 该研究提出了一种新的不确定性量化方法，可以有效提高代理模型在安全关键应用中的可靠性。

Abstract: Surrogate models (including deep neural networks and other machine learning algorithms in supervised learning) are capable of approximating arbitrarily complex, high-dimensional input-output problems in science and engineering, but require a thorough data-agnostic uncertainty quantification analysis before these can be deployed for any safety-critical application. The standard approach for data-agnostic uncertainty quantification is to use conformal prediction (CP), a well-established framework to build uncertainty models with proven statistical guarantees that do not assume any shape for the error distribution of the surrogate model. However, since the classic statistical guarantee offered by CP is given in terms of bounds for the marginal coverage, for small calibration set sizes (which are frequent in realistic surrogate modelling that aims to quantify error at different regions), the potentially strong dispersion of the coverage distribution around its average negatively impacts the reliability of the uncertainty model, often obtaining coverages below the expected value, resulting in a less applicable framework. After providing a gentle presentation of uncertainty quantification for surrogate models for machine learning practitioners, in this paper we bridge the gap by proposing a new statistical guarantee that offers probabilistic information for the coverage of a single conformal predictor. We show that the proposed framework converges to the standard solution offered by CP for large calibration set sizes and, unlike the classic guarantee, still offers reliable information about the coverage of a conformal predictor for small data sizes. We illustrate and validate the methodology in a suite of examples, and implement an open access software solution that can be used alongside common conformal prediction libraries to obtain uncertainty models that fulfil the new guarantee.

</details>


### [146] [Temp-SCONE: A Novel Out-of-Distribution Detection and Domain Generalization Framework for Wild Data with Temporal Shift](https://arxiv.org/abs/2512.04571)
*Aditi Naiknaware,Sanchit Singh,Hajar Homayouni,Salimeh Sekeh*

Main category: cs.LG

TL;DR: Temp-SCONE: A temporally consistent extension of SCONE for handling temporal shifts in dynamic environments, improving robustness and OOD detection.


<details>
  <summary>Details</summary>
Motivation: Existing OWL models degrade in dynamic domains due to temporal shifts.

Method: Introduces a confidence-driven regularization loss based on Average Thresholded Confidence (ATC) to penalize instability in predictions across time steps.

Result: Significantly improves robustness under temporal drift, yielding higher corrupted-data accuracy and more reliable OOD detection compared to SCONE. Maintains comparable performance on datasets without temporal continuity.

Conclusion: Temp-SCONE is a step toward reliable OWL in evolving dynamic environments, with theoretical insights on temporal stability and generalization error.

Abstract: Open-world learning (OWL) requires models that can adapt to evolving environments while reliably detecting out-of-distribution (OOD) inputs. Existing approaches, such as SCONE, achieve robustness to covariate and semantic shifts but assume static environments, leading to degraded performance in dynamic domains. In this paper, we propose Temp-SCONE, a temporally consistent extension of SCONE designed to handle temporal shifts in dynamic environments. Temp-SCONE introduces a confidence-driven regularization loss based on Average Thresholded Confidence (ATC), penalizing instability in predictions across time steps while preserving SCONE's energy-margin separation. Experiments on dynamic datasets demonstrate that Temp-SCONE significantly improves robustness under temporal drift, yielding higher corrupted-data accuracy and more reliable OOD detection compared to SCONE. On distinct datasets without temporal continuity, Temp-SCONE maintains comparable performance, highlighting the importance and limitations of temporal regularization. Our theoretical insights on temporal stability and generalization error further establish Temp-SCONE as a step toward reliable OWL in evolving dynamic environments.

</details>


### [147] [Exploiting \texttt{ftrace}'s \texttt{function\_graph} Tracer Features for Machine Learning: A Case Study on Encryption Detection](https://arxiv.org/abs/2512.04590)
*Kenan Begovic,Abdulaziz Al-Ali,Qutaibah Malluhi*

Main category: cs.LG

TL;DR: 本研究提出使用Linux内核ftrace框架为机器学习应用生成系统级数据，并在加密检测任务中验证了所提特征的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决机器学习在系统行为分析、程序识别和异常检测中的应用问题，特别是加密活动检测。

Method: 利用Linux内核ftrace框架，特别是函数图追踪器，生成信息丰富的系统级数据，并提取基于图的特征。

Result: 在实际加密检测任务中，准确率达到99.28%，并在多标签分类问题中得到进一步验证。

Conclusion: 该研究为预处理原始追踪数据和提取基于图的特征提供了全面的方法，为性能监控和安全分析中的创新解决方案铺平了道路。

Abstract: This paper proposes using the Linux kernel ftrace framework, particularly the function graph tracer, to generate informative system level data for machine learning (ML) applications. Experiments on a real world encryption detection task demonstrate the efficacy of the proposed features across several learning algorithms. The learner faces the problem of detecting encryption activities across a large dataset of files, using function call traces and graph based features. Empirical results highlight an outstanding accuracy of 99.28 on the task at hand, underscoring the efficacy of features derived from the function graph tracer. The results were further validated in an additional experiment targeting a multilabel classification problem, in which running programs were identified from trace data. This work provides comprehensive methodologies for preprocessing raw trace data and extracting graph based features, offering significant advancements in applying ML to system behavior analysis, program identification, and anomaly detection. By bridging the gap between system tracing and ML, this paper paves the way for innovative solutions in performance monitoring and security analytics.

</details>


### [148] [Natural Language Actor-Critic: Scalable Off-Policy Learning in Language Space](https://arxiv.org/abs/2512.04601)
*Joey Hong,Kang Liu,Zhan Ling,Jiecao Chen,Sergey Levine*

Main category: cs.LG

TL;DR: 提出了一种新的actor-critic算法，使用生成式LLM评论家生成自然语言而非标量值，以训练LLM策略，从而为LLM策略提供更丰富和可操作的训练信号。


<details>
  <summary>Details</summary>
Motivation: 在缺乏专家演示的情况下，训练LLM agent依赖于策略梯度方法，但对于具有稀疏奖励的长期任务，从轨迹层面奖励中学习可能存在噪声，导致训练不稳定且样本复杂度高。此外，策略改进依赖于通过探索发现更好的行动，但当行动位于自然语言空间中时，这可能很困难。

Method: 提出了一种名为自然语言Actor-Critic (NLAC) 的新颖actor-critic算法，该算法使用生成式LLM评论家来训练LLM策略，该评论家产生自然语言而非标量值。

Result: 在推理、网页浏览以及工具使用与对话任务的混合中，NLAC在性能上优于现有的训练方法，并为LLM agent提供了更具可扩展性和稳定性的训练范例。

Conclusion: NLAC 算法为 LLM agent 提供了一种更具可扩展性和稳定性的训练范例，并且在性能上优于现有的训练方法。

Abstract: Large language model (LLM) agents -- LLMs that dynamically interact with an environment over long horizons -- have become an increasingly important area of research, enabling automation in complex tasks involving tool-use, web browsing, and dialogue with people. In the absence of expert demonstrations, training LLM agents has relied on policy gradient methods that optimize LLM policies with respect to an (often sparse) reward function. However, in long-horizon tasks with sparse rewards, learning from trajectory-level rewards can be noisy, leading to training that is unstable and has high sample complexity. Furthermore, policy improvement hinges on discovering better actions through exploration, which can be difficult when actions lie in natural language space. In this paper, we propose Natural Language Actor-Critic (NLAC), a novel actor-critic algorithm that trains LLM policies using a generative LLM critic that produces natural language rather than scalar values. This approach leverages the inherent strengths of LLMs to provide a richer and more actionable training signal; particularly, in tasks with large, open-ended action spaces, natural language explanations for why an action is suboptimal can be immensely useful for LLM policies to reason how to improve their actions, without relying on random exploration. Furthermore, our approach can be trained off-policy without policy gradients, offering a more data-efficient and stable alternative to existing on-policy methods. We present results on a mixture of reasoning, web browsing, and tool-use with dialogue tasks, demonstrating that NLAC shows promise in outperforming existing training approaches and offers a more scalable and stable training paradigm for LLM agents.

</details>


### [149] [QoSDiff: An Implicit Topological Embedding Learning Framework Leveraging Denoising Diffusion and Adversarial Attention for Robust QoS Prediction](https://arxiv.org/abs/2512.04596)
*Guanchen Du,Jianlong Xu,Wei Wei*

Main category: cs.LG

TL;DR: 提出了一种名为 QoSDiff 的新框架，用于服务质量 (QoS) 预测，无需构建显式用户-服务交互图。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于构建显式用户-服务交互图，存在可扩展性瓶颈，且在连接稀疏或受噪声干扰时性能受限。

Method: 利用去噪扩散概率模型从噪声初始化中恢复内在潜在结构，并提出一个对抗交互模块，该模块集成了双向混合注意力机制，以捕捉高阶交互。

Result: 在两个大规模真实世界数据集上的大量实验表明，QoSDiff 显著优于最先进的基线。

Conclusion: QoSDiff 框架具有卓越的跨数据集泛化能力，并且对数据稀疏性和观测噪声具有出色的鲁棒性。

Abstract: Accurate Quality of Service (QoS) prediction is fundamental to service computing, providing essential data-driven guidance for service selection and ensuring superior user experiences. However, prevalent approaches, particularly Graph Neural Networks (GNNs), heavily rely on constructing explicit user--service interaction graphs. This dependency introduces severe scalability bottlenecks and limits performance when explicit connections are sparse or corrupted by noise. To address these challenges, this paper introduces \emph{QoSDiff}, a novel embedding learning framework that bypasses the prerequisite of explicit graph construction. Specifically, it leverages a denoising diffusion probabilistic model to recover intrinsic latent structures from noisy initializations. To further capture high-order interactions, we propose an adversarial interaction module that integrates a bidirectional hybrid attention mechanism. This adversarial paradigm dynamically distinguishes informative patterns from noise, enabling a dual-perspective modeling of intricate user--service associations. Extensive experiments on two large-scale real-world datasets demonstrate that QoSDiff significantly outperforms state-of-the-art baselines. Notably, the results highlight the framework's superior cross-dataset generalization capability and exceptional robustness against data sparsity and observational noise.

</details>


### [150] [Score Matching for Estimating Finite Point Processes](https://arxiv.org/abs/2512.04617)
*Haoqun Cao,Yixuan Zhang,Feng Zhou*

Main category: cs.LG

TL;DR: 本文研究了有限点过程的得分匹配估计，解决了传统方法在处理此类问题时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的点过程得分匹配估计方法缺乏对有限点过程的数学严谨分析，导致其存在局限性。

Method: 1.  通过Janossy测度为有限点过程开发了一个正式的得分匹配框架。2. 引入了一个自回归加权得分匹配估计器，并分析了其在经典参数设置中的统计特性。3. 提出了一个简单的生存分类增强方法，为任何基于强度的时空点过程模型产生了一个完整的、无积分的训练目标。

Result: 在合成和真实世界的时空数据集上的实验表明，该方法能够准确地恢复强度，并达到与MLE相当的性能，且效率更高。

Conclusion: 该文提出的方法能够有效解决有限点过程的得分匹配估计问题，并在实验中表现出良好的性能。

Abstract: Score matching estimators have garnered significant attention in recent years because they eliminate the need to compute normalizing constants, thereby mitigating the computational challenges associated with maximum likelihood estimation (MLE).While several studies have proposed score matching estimators for point processes, this work highlights the limitations of these existing methods, which stem primarily from the lack of a mathematically rigorous analysis of how score matching behaves on finite point processes -- special random configurations on bounded spaces where many of the usual assumptions and properties of score matching no longer hold. To this end, we develop a formal framework for score matching on finite point processes via Janossy measures and, within this framework, introduce an (autoregressive) weighted score-matching estimator, whose statistical properties we analyze in classical parametric settings. For general nonparametric (e.g., deep) point process models, we show that score matching alone does not uniquely identify the ground-truth distribution due to subtle normalization issues, and we propose a simple survival-classification augmentation that yields a complete, integration-free training objective for any intensity-based point process model for spatio-temporal case. Experiments on synthetic and real-world temporal and spatio-temporal datasets, demonstrate that our method accurately recovers intensities and achieves performance comparable to MLE with better efficiency.

</details>


### [151] [MemLoRA: Distilling Expert Adapters for On-Device Memory Systems](https://arxiv.org/abs/2512.04763)
*Massimo Bini,Ondrej Bohdal,Umberto Michieli,Zeynep Akata,Mete Ozay,Taha Ceritli*

Main category: cs.LG

TL;DR: MemLoRA introduces memory adapters for Small Language Models (SLMs) to enable on-device memory operations, outperforming larger models. MemLoRA-V extends this to vision, integrating small Vision-Language Models (SVLMs) for visual understanding.


<details>
  <summary>Details</summary>
Motivation: Memory-augmented LLMs are effective but too costly for on-device deployment. SLMs are suitable for on-device use but lack performance and visual capabilities.

Method: MemLoRA uses specialized memory adapters trained via knowledge distillation for knowledge extraction, memory update, and memory-augmented generation. MemLoRA-V integrates SVLMs into the memory system.

Result: MemLoRA outperforms 10x larger models and matches 60x larger models on text tasks. MemLoRA-V significantly improves visual question answering over caption-based approaches.

Conclusion: MemLoRA and MemLoRA-V enable accurate on-device memory operations and visual understanding without cloud dependency, demonstrating efficacy in multimodal contexts.

Abstract: Memory-augmented Large Language Models (LLMs) have demonstrated remarkable consistency during prolonged dialogues by storing relevant memories and incorporating them as context. Such memory-based personalization is also key in on-device settings that allow users to keep their conversations and data private. However, memory-augmented systems typically rely on LLMs that are too costly for local on-device deployment. Even though Small Language Models (SLMs) are more suitable for on-device inference than LLMs, they cannot achieve sufficient performance. Additionally, these LLM-based systems lack native visual capabilities, limiting their applicability in multimodal contexts. In this paper, we introduce (i) MemLoRA, a novel memory system that enables local deployment by equipping SLMs with specialized memory adapters, and (ii) its vision extension MemLoRA-V, which integrates small Vision-Language Models (SVLMs) to memory systems, enabling native visual understanding. Following knowledge distillation principles, each adapter is trained separately for specific memory operations$\unicode{x2013}$knowledge extraction, memory update, and memory-augmented generation. Equipped with memory adapters, small models enable accurate on-device memory operations without cloud dependency. On text-only operations, MemLoRA outperforms 10$\times$ larger baseline models (e.g., Gemma2-27B) and achieves performance comparable to 60$\times$ larger models (e.g., GPT-OSS-120B) on the LoCoMo benchmark. To evaluate visual understanding operations instead, we extend LoCoMo with challenging Visual Question Answering tasks that require direct visual reasoning. On this, our VLM-integrated MemLoRA-V shows massive improvements over caption-based approaches (81.3 vs. 23.7 accuracy) while keeping strong performance in text-based tasks, demonstrating the efficacy of our method in multimodal contexts.

</details>


### [152] [Rethinking Decoupled Knowledge Distillation: A Predictive Distribution Perspective](https://arxiv.org/abs/2512.04625)
*Bowen Zheng,Ran Cheng*

Main category: cs.LG

TL;DR: 本文从预测分布的角度重新审视了解耦知识蒸馏（DKD），发现top logit的划分可以改善非top logits的相互关系，并提出了一种改进的GDKD算法，该算法具有高效的划分策略来处理教师模型预测分布的多模态问题。


<details>
  <summary>Details</summary>
Motivation: DKD重新强调了logit知识的重要性，但其潜在机制值得更深入的探索。

Method: 1. 提出了广义解耦知识蒸馏（GDKD）损失，它为解耦logits提供了一种更通用的方法。2. 关注教师模型的预测分布及其对GDKD损失梯度的影响，揭示了两个关键见解。(1) top logit的划分可以显著改善非top logits的相互关系。(2) 放大对非top logits的蒸馏损失的关注，可以加强它们之间的知识提取。3. 提出了一种简化的GDKD算法，该算法具有高效的划分策略来处理教师模型预测分布的多模态问题。

Result: 在CIFAR-100、ImageNet、Tiny-ImageNet、CUB-200-2011和Cityscapes等各种基准上进行的综合实验表明，GDKD的性能优于原始DKD和其他领先的知识蒸馏方法。

Conclusion: GDKD算法在知识蒸馏方面表现出色，并通过实验验证了其有效性。

Abstract: In the history of knowledge distillation, the focus has once shifted over time from logit-based to feature-based approaches. However, this transition has been revisited with the advent of Decoupled Knowledge Distillation (DKD), which re-emphasizes the importance of logit knowledge through advanced decoupling and weighting strategies. While DKD marks a significant advancement, its underlying mechanisms merit deeper exploration. As a response, we rethink DKD from a predictive distribution perspective. First, we introduce an enhanced version, the Generalized Decoupled Knowledge Distillation (GDKD) loss, which offers a more versatile method for decoupling logits. Then we pay particular attention to the teacher model's predictive distribution and its impact on the gradients of GDKD loss, uncovering two critical insights often overlooked: (1) the partitioning by the top logit considerably improves the interrelationship of non-top logits, and (2) amplifying the focus on the distillation loss of non-top logits enhances the knowledge extraction among them. Utilizing these insights, we further propose a streamlined GDKD algorithm with an efficient partition strategy to handle the multimodality of teacher models' predictive distribution. Our comprehensive experiments conducted on a variety of benchmarks, including CIFAR-100, ImageNet, Tiny-ImageNet, CUB-200-2011, and Cityscapes, demonstrate GDKD's superior performance over both the original DKD and other leading knowledge distillation methods. The code is available at https://github.com/ZaberKo/GDKD.

</details>
