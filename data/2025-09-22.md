<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 56]
- [cs.CV](#cs.CV) [Total: 56]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.LG](#cs.LG) [Total: 57]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Synthetic bootstrapped pretraining](https://arxiv.org/abs/2509.15248)
*Zitong Yang,Aonan Zhang,Hong Liu,Tatsunori Hashimoto,Emmanuel Candès,Chong Wang,Ruoming Pang*

Main category: cs.CL

TL;DR: 提出了一种新的语言模型预训练方法，通过学习文档之间的关系来合成新的训练语料。


<details>
  <summary>Details</summary>
Motivation: 传统的预训练方法不能有效地建模文档之间丰富的关联，而这些关联可能提升模型性能。

Method: 首先学习预训练数据集中文档关系的建模，然后利用它来合成大量新的语料进行联合训练。

Result: 在3B参数模型上，SBP优于重复基线，并且获得了使用20倍更多独特数据的oracle上限所能达到的性能提升。

Conclusion: 合成的文档超越了简单的释义，SBP首先从种子材料中提取核心概念，然后在之上构建新的叙述。

Abstract: We introduce Synthetic Bootstrapped Pretraining (SBP), a language model (LM)
pretraining procedure that first learns a model of relations between documents
from the pretraining dataset and then leverages it to synthesize a vast new
corpus for joint training. While the standard pretraining teaches LMs to learn
causal correlations among tokens within a single document, it is not designed
to efficiently model the rich, learnable inter-document correlations that can
potentially lead to better performance. We validate SBP by designing a
compute-matched pretraining setup and pretrain a 3B-parameter model on up to 1T
tokens from scratch. We find SBP consistently improves upon a strong repetition
baseline and delivers a significant fraction of performance improvement
attainable by an oracle upper bound with access to 20x more unique data.
Qualitative analysis reveals that the synthesized documents go beyond mere
paraphrases -- SBP first abstracts a core concept from the seed material and
then crafts a new narration on top of it. Besides strong empirical performance,
SBP admits a natural Bayesian interpretation: the synthesizer implicitly learns
to abstract the latent concepts shared between related documents.

</details>


### [2] [Comparative Analysis of Tokenization Algorithms for Low-Resource Language Dzongkha](https://arxiv.org/abs/2509.15255)
*Tandin Wangchuk,Tad Gonsalves*

Main category: cs.CL

TL;DR: 该研究评估了三种通用的分词算法，以确定哪种算法最适合宗喀语（一种低资源语言）。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型分词器在高资源语言（如英语）上表现良好，但在低资源语言上表现不佳。宗喀语是一种低资源语言，在自然语言处理方面面临独特的挑战，尤其是在分词方面。

Method: 研究评估了三种分词算法：Byte-Pair Encoding (BPE)、WordPiece 和 SentencePiece (Unigram)，并使用 Subword Fertility、Proportion of Continued Words、Normalized Sequence Length 和执行时间等指标评估了它们的性能。

Result: 结果表明，虽然所有三种算法都显示出潜力，但 SentencePiece 对于宗喀语分词最有效。

Conclusion: 研究表明需要为低资源语言定制方法，并进行持续研究。本研究提出了三种宗喀语分词算法，为构建宗喀语大语言模型铺平了道路。

Abstract: Large Language Models (LLMs) are gaining popularity and improving rapidly.
Tokenizers are crucial components of natural language processing, especially
for LLMs. Tokenizers break down input text into tokens that models can easily
process while ensuring the text is accurately represented, capturing its
meaning and structure. Effective tokenizers enhance the capabilities of LLMs by
improving a model's understanding of context and semantics, ultimately leading
to better performance in various downstream tasks, such as translation,
classification, sentiment analysis, and text generation. Most pre-trained
tokenizers are suitable for high-resource languages like English but perform
poorly for low-resource languages. Dzongkha, Bhutan's national language spoken
by around seven hundred thousand people, is a low-resource language, and its
linguistic complexity poses unique NLP challenges. Despite some progress,
significant research in Dzongkha NLP is lacking, particularly in tokenization.
This study evaluates the training and performance of three common tokenization
algorithms in comparison to other popular methods. Specifically, Byte-Pair
Encoding (BPE), WordPiece, and SentencePiece (Unigram) were evaluated for their
suitability for Dzongkha. Performance was assessed using metrics like Subword
Fertility, Proportion of Continued Words, Normalized Sequence Length, and
execution time. The results show that while all three algorithms demonstrate
potential, SentencePiece is the most effective for Dzongkha tokenization,
paving the way for further NLP advancements. This underscores the need for
tailored approaches for low-resource languages and ongoing research. In this
study, we presented three tokenization algorithms for Dzongkha, paving the way
for building Dzongkha Large Language Models.

</details>


### [3] [Toxicity Red-Teaming: Benchmarking LLM Safety in Singapore's Low-Resource Languages](https://arxiv.org/abs/2509.15260)
*Yujia Hu,Ming Shan Hee,Preslav Nakov,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: 本研究关注于大型语言模型(LLMs)在低资源、多语言环境下的安全性问题，填补了该领域的空白。


<details>
  <summary>Details</summary>
Motivation: 研究动机是现有的LLM安全机制在低资源、多语言环境下未得到充分探索。本研究旨在弥补这一差距，特别是在新加坡多样化的语言环境中。

Method: 采用了红队方法，通过SGToxicGuard数据集和评估框架，在对话、问答和内容创作三个真实场景中，系统地探测LLM的脆弱性。

Result: 实验结果揭示了当前最先进的多语言LLM在安全防护方面存在严重缺陷。

Conclusion: 该研究为在语言多样性环境中构建更安全、更具包容性的人工智能系统奠定了基础，并提供了关于文化敏感性和毒性缓解的可行性见解。

Abstract: The advancement of Large Language Models (LLMs) has transformed natural
language processing; however, their safety mechanisms remain under-explored in
low-resource, multilingual settings. Here, we aim to bridge this gap. In
particular, we introduce \textsf{SGToxicGuard}, a novel dataset and evaluation
framework for benchmarking LLM safety in Singapore's diverse linguistic
context, including Singlish, Chinese, Malay, and Tamil. SGToxicGuard adopts a
red-teaming approach to systematically probe LLM vulnerabilities in three
real-world scenarios: \textit{conversation}, \textit{question-answering}, and
\textit{content composition}. We conduct extensive experiments with
state-of-the-art multilingual LLMs, and the results uncover critical gaps in
their safety guardrails. By offering actionable insights into cultural
sensitivity and toxicity mitigation, we lay the foundation for safer and more
inclusive AI systems in linguistically diverse environments.\footnote{Link to
the dataset: https://github.com/Social-AI-Studio/SGToxicGuard.}
\textcolor{red}{Disclaimer: This paper contains sensitive content that may be
disturbing to some readers.}

</details>


### [4] [PolBiX: Detecting LLMs' Political Bias in Fact-Checking through X-phemisms](https://arxiv.org/abs/2509.15335)
*Charlott Jakob,David Harbecke,Patrick Parschan,Pia Wenzel Neves,Vera Schmitt*

Main category: cs.CL

TL;DR: 本研究调查了大型语言模型在事实核查中存在的政治偏见问题，通过在德语声明中使用委婉语或粗俗语替换词语来系统地研究政治偏见。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地应用于需要客观评估的场景，但其政治偏见可能会损害评估的客观性。之前的研究发现大型语言模型偏向左倾立场，但对事实核查等任务的下游影响尚未充分 исследован。

Method: 通过构建在政治含义上有所不同的、但事实等价的最小声明对，来评估大型语言模型在将它们分类为真或假时的一致性。

Result: 研究结果表明，与政治倾向相比，判断性词语的存在对真假评估有显著影响。

Conclusion: 虽然一些模型表现出政治偏见的倾向，但通过在提示中明确要求客观性并不能缓解这种情况。

Abstract: Large Language Models are increasingly used in applications requiring
objective assessment, which could be compromised by political bias. Many
studies found preferences for left-leaning positions in LLMs, but downstream
effects on tasks like fact-checking remain underexplored. In this study, we
systematically investigate political bias through exchanging words with
euphemisms or dysphemisms in German claims. We construct minimal pairs of
factually equivalent claims that differ in political connotation, to assess the
consistency of LLMs in classifying them as true or false. We evaluate six LLMs
and find that, more than political leaning, the presence of judgmental words
significantly influences truthfulness assessment. While a few models show
tendencies of political bias, this is not mitigated by explicitly calling for
objectivism in prompts.

</details>


### [5] [Quantifying Self-Awareness of Knowledge in Large Language Models](https://arxiv.org/abs/2509.15339)
*Yeongbin Seo,Dongha Lee,Jinyoung Yeo*

Main category: cs.CL

TL;DR: 这篇论文研究了大型语言模型（LLM）中的幻觉预测问题，并指出以往的成功可能源于问题本身的捷径，而非模型真正的自我意识。


<details>
  <summary>Details</summary>
Motivation: 旨在区分问题端捷径和模型端自省，从而更好地理解LLM中的幻觉现象。

Method: 提出了近似问题端效应（AQE）来量化问题意识的贡献，并引入了一种名为SCAO（通过一个词回答进行语义压缩）的方法来增强模型端信号的使用。

Result: 通过多个数据集的分析表明，先前报告的成功很大程度上源于利用问题中的表面模式。SCAO在减少问题端线索的情况下表现出强大而一致的性能。

Conclusion: SCAO能够有效促进LLM中真正的自我意识。

Abstract: Hallucination prediction in large language models (LLMs) is often interpreted
as a sign of self-awareness. However, we argue that such performance can arise
from question-side shortcuts rather than true model-side introspection. To
disentangle these factors, we propose the Approximate Question-side Effect
(AQE), which quantifies the contribution of question-awareness. Our analysis
across multiple datasets reveals that much of the reported success stems from
exploiting superficial patterns in questions. We further introduce SCAO
(Semantic Compression by Answering in One word), a method that enhances the use
of model-side signals. Experiments show that SCAO achieves strong and
consistent performance, particularly in settings with reduced question-side
cues, highlighting its effectiveness in fostering genuine self-awareness in
LLMs.

</details>


### [6] [Real, Fake, or Manipulated? Detecting Machine-Influenced Text](https://arxiv.org/abs/2509.15350)
*Yitong Wang,Zhongping Zhang,Margherita Piana,Zheng Zhou,Peter Gerstoft,Bryan A. Plummer*

Main category: cs.CL

TL;DR: 这篇论文介绍了一种新的机器影响文本检测器HERO，它可以区分四种类型的文本：人工撰写、机器生成、机器润色和机器翻译。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）可以用于编写或修改文档，这给理解其使用意图带来了挑战。以往的机器生成文本（MGT）检测工作主要集中在简单地识别文档是人工还是机器编写的，忽略了这些细粒度的用途。

Method: HERO通过结合长度专家模型的预测来实现这一点，这些模型已经接受了子类别指导的训练。具体来说，对于容易混淆的类别（例如，不同的源语言），我们的子类别指导模块鼓励细粒度类别的分离，从而提高性能。

Result: 在五个LLM和六个领域进行的广泛实验表明，我们的HERO的优势，平均超过最先进水平2.5-3 mAP。

Conclusion: HERO是一种有效地区分不同类型机器影响文本的检测器，优于现有技术。

Abstract: Large Language Model (LLMs) can be used to write or modify documents,
presenting a challenge for understanding the intent behind their use. For
example, benign uses may involve using LLM on a human-written document to
improve its grammar or to translate it into another language. However, a
document entirely produced by a LLM may be more likely to be used to spread
misinformation than simple translation (\eg, from use by malicious actors or
simply by hallucinating). Prior works in Machine Generated Text (MGT) detection
mostly focus on simply identifying whether a document was human or machine
written, ignoring these fine-grained uses. In this paper, we introduce a
HiErarchical, length-RObust machine-influenced text detector (HERO), which
learns to separate text samples of varying lengths from four primary types:
human-written, machine-generated, machine-polished, and machine-translated.
HERO accomplishes this by combining predictions from length-specialist models
that have been trained with Subcategory Guidance. Specifically, for categories
that are easily confused (\eg, different source languages), our Subcategory
Guidance module encourages separation of the fine-grained categories, boosting
performance. Extensive experiments across five LLMs and six domains demonstrate
the benefits of our HERO, outperforming the state-of-the-art by 2.5-3 mAP on
average.

</details>


### [7] [CodeRAG: Finding Relevant and Necessary Knowledge for Retrieval-Augmented Repository-Level Code Completion](https://arxiv.org/abs/2509.16112)
*Sheng Zhang,Yifan Ding,Shuquan Lian,Shun Song,Hui Li*

Main category: cs.CL

TL;DR: CodeRAG框架通过改进查询构建、多路径代码检索和排序，提升了代码仓库级别的代码补全效果，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的代码仓库级别代码补全方法存在查询构建不当、单路径代码检索以及代码检索器与代码LLM不对齐的问题。

Method: 提出了CodeRAG框架，包含log概率引导的查询构建、多路径代码检索和偏好对齐的BestFit重排序等组件。

Result: 在ReccEval和CCEval基准测试中，CodeRAG显著且持续地优于现有技术。

Conclusion: CodeRAG框架有效地提升了代码仓库级别的代码补全效果。

Abstract: Repository-level code completion automatically predicts the unfinished code
based on the broader information from the repository. Recent strides in Code
Large Language Models (code LLMs) have spurred the development of
repository-level code completion methods, yielding promising results.
Nevertheless, they suffer from issues such as inappropriate query construction,
single-path code retrieval, and misalignment between code retriever and code
LLM. To address these problems, we introduce CodeRAG, a framework tailored to
identify relevant and necessary knowledge for retrieval-augmented
repository-level code completion. Its core components include log probability
guided query construction, multi-path code retrieval, and preference-aligned
BestFit reranking. Extensive experiments on benchmarks ReccEval and CCEval
demonstrate that CodeRAG significantly and consistently outperforms
state-of-the-art methods. The implementation of CodeRAG is available at
https://github.com/KDEGroup/CodeRAG.

</details>


### [8] [Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing](https://arxiv.org/abs/2509.15361)
*Zichen Wu,Hsiu-Yuan Huang,Yunfang Wu*

Main category: cs.CL

TL;DR: 这篇论文提出了一个基于因果中介的去偏框架，以解决多模态大型语言模型（MLLM）中存在的表面相关性偏差问题。


<details>
  <summary>Details</summary>
Motivation: MLLM在整合视觉和文本信息方面表现出强大的能力，但常常依赖于虚假的相关性，从而削弱了其在复杂多模态推理任务中的鲁棒性和泛化能力。本文旨在解决MLLM中表面相关性偏差的关键挑战。

Method: 该方法通过反事实的例子区分核心语义与虚假的文本和视觉上下文，激活训练阶段的去偏过程，并采用混合专家（MoE）架构和动态路由，有选择性地调用特定模态的去偏专家。

Result: 在多模态讽刺检测和情感分析任务上的实证评估表明，该框架显著优于单模态去偏策略和现有的先进模型。

Conclusion: 该研究提出的框架能够有效提升MLLM的性能，并减轻表面相关性偏差的影响。

Abstract: Multimodal Large Language Models (MLLMs) have shown substantial capabilities
in integrating visual and textual information, yet frequently rely on spurious
correlations, undermining their robustness and generalization in complex
multimodal reasoning tasks. This paper addresses the critical challenge of
superficial correlation bias in MLLMs through a novel causal mediation-based
debiasing framework. Specially, we distinguishing core semantics from spurious
textual and visual contexts via counterfactual examples to activate
training-stage debiasing and employ a Mixture-of-Experts (MoE) architecture
with dynamic routing to selectively engages modality-specific debiasing
experts. Empirical evaluation on multimodal sarcasm detection and sentiment
analysis tasks demonstrates that our framework significantly surpasses unimodal
debiasing strategies and existing state-of-the-art models.

</details>


### [9] [Speech Language Models for Under-Represented Languages: Insights from Wolof](https://arxiv.org/abs/2509.15362)
*Yaya Sy,Dioula Doucouré,Christophe Cerisara,Irina Illina*

Main category: cs.CL

TL;DR: 本文介绍了训练用于西非代表性不足的语言沃洛夫语的语音语言模型的过程，并分享了关键见解。


<details>
  <summary>Details</summary>
Motivation: 强调收集大规模、自发的、高质量的语音数据的重要性，并表明在此数据集上继续预训练 HuBERT 在 ASR 上的表现优于基础模型和以非洲为中心的模型。

Method: 将该语音编码器集成到沃洛夫语 LLM 中，以训练该语言的第一个语音 LLM，将其功能扩展到语音翻译等任务。此外，探索训练语音 LLM 在转录或翻译之前执行多步思维链。

Result: 结果表明，语音 LLM 不仅提高了语音识别能力，而且在语音翻译方面也表现良好。

Conclusion: 模型和代码将被公开分享。

Abstract: We present our journey in training a speech language model for Wolof, an
underrepresented language spoken in West Africa, and share key insights. We
first emphasize the importance of collecting large-scale, spontaneous,
high-quality speech data, and show that continued pretraining HuBERT on this
dataset outperforms both the base model and African-centric models on ASR. We
then integrate this speech encoder into a Wolof LLM to train the first Speech
LLM for this language, extending its capabilities to tasks such as speech
translation. Furthermore, we explore training the Speech LLM to perform
multi-step Chain-of-Thought before transcribing or translating. Our results
show that the Speech LLM not only improves speech recognition but also performs
well in speech translation. The models and the code will be openly shared.

</details>


### [10] [Frustratingly Easy Data Augmentation for Low-Resource ASR](https://arxiv.org/abs/2509.15373)
*Katsumi Ibaraki,David Chiang*

Main category: cs.CL

TL;DR: 本文介绍了三种用于低资源自动语音识别（ASR）的自包含数据增强方法，通过生成新的文本并应用文本到语音（TTS）技术来合成音频。


<details>
  <summary>Details</summary>
Motivation: 在低资源语言环境下，自动语音识别（ASR）模型训练数据不足，导致模型性能下降。

Method: 利用基于gloss的替换、随机替换或基于LLM的方法生成新的文本，然后应用文本到语音（TTS）技术生成合成音频。

Result: 在四种低资源语言上进行了实验，结果表明，通过在原始音频和生成的合成数据上微调预训练的Wav2Vec2-XLSR-53模型，可以显著提高性能，例如Nashta的WER降低了14.3%。

Conclusion: 该方法在四种低资源语言中均有效，并且在英语等高资源语言中也显示出实用性，证明了其广泛的适用性。

Abstract: This paper introduces three self-contained data augmentation methods for
low-resource Automatic Speech Recognition (ASR). Our techniques first generate
novel text--using gloss-based replacement, random replacement, or an LLM-based
approach--and then apply Text-to-Speech (TTS) to produce synthetic audio. We
apply these methods, which leverage only the original annotated data, to four
languages with extremely limited resources (Vatlongos, Nashta, Shinekhen
Buryat, and Kakabe). Fine-tuning a pretrained Wav2Vec2-XLSR-53 model on a
combination of the original audio and generated synthetic data yields
significant performance gains, including a 14.3% absolute WER reduction for
Nashta. The methods prove effective across all four low-resource languages and
also show utility for high-resource languages like English, demonstrating their
broad applicability.

</details>


### [11] [Quantifying Uncertainty in Natural Language Explanations of Large Language Models for Question Answering](https://arxiv.org/abs/2509.15403)
*Yangyi Li,Mengdi Huai*

Main category: cs.CL

TL;DR: 本文提出了一种为大型语言模型生成的自然语言解释提供有效不确定性保证的方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在问答任务中表现出强大的能力，但缺乏透明度。自然语言解释因其自我解释能力而备受关注，但现有工作缺乏对其有效不确定性保证的研究。

Method: 提出了一种新颖的不确定性估计框架，以提供事后和模型无关的有效不确定性保证。此外，还设计了一种新的鲁棒不确定性估计方法，即使在噪声存在的情况下也能保持有效的不确定性保证。

Result: 在问答任务上的大量实验表明了该方法的良好性能。

Conclusion: 本文提出的方法能够为大型语言模型生成的自然语言解释提供有效的不确定性保证，即使在噪声环境下也能保持其有效性。

Abstract: Large language models (LLMs) have shown strong capabilities, enabling
concise, context-aware answers in question answering (QA) tasks. The lack of
transparency in complex LLMs has inspired extensive research aimed at
developing methods to explain large language behaviors. Among existing
explanation methods, natural language explanations stand out due to their
ability to explain LLMs in a self-explanatory manner and enable the
understanding of model behaviors even when the models are closed-source.
However, despite these promising advancements, there is no existing work
studying how to provide valid uncertainty guarantees for these generated
natural language explanations. Such uncertainty quantification is critical in
understanding the confidence behind these explanations. Notably, generating
valid uncertainty estimates for natural language explanations is particularly
challenging due to the auto-regressive generation process of LLMs and the
presence of noise in medical inquiries. To bridge this gap, in this work, we
first propose a novel uncertainty estimation framework for these generated
natural language explanations, which provides valid uncertainty guarantees in a
post-hoc and model-agnostic manner. Additionally, we also design a novel robust
uncertainty estimation method that maintains valid uncertainty guarantees even
under noise. Extensive experiments on QA tasks demonstrate the desired
performance of our methods.

</details>


### [12] [Deep learning and abstractive summarisation for radiological reports: an empirical study for adapting the PEGASUS models' family with scarce data](https://arxiv.org/abs/2509.15419)
*Claudio Benzoni,Martina Langhals,Martin Boeker,Luise Modersohn,Máté E. Maros*

Main category: cs.CL

TL;DR: 本文研究了在医学领域使用 PEGASUS 和 PEGASUS-X 模型进行文本摘要的微调适应问题，旨在为从业者提供避免过拟合和欠拟合的见解。


<details>
  <summary>Details</summary>
Motivation: 在医学领域，自动化的复杂医学文本摘要工具越来越重要。现有的 AI 在处理数据受限的医学文本摘要方面仍面临挑战。

Method: 本文通过微调非特定领域的抽象摘要编码器-解码器模型系列，研究了 PEGASUS 和 PEGASUS-X 在中等规模的放射报告公共数据集上的适应性。对每个模型，使用不同大小的相同训练数据的两个不同检查点进行了综合评估。在训练过程中，使用词汇和语义指标监控模型在固定大小的验证集上的性能。

Result: PEGASUS 表现出不同的阶段，与 epoch-wise double-descent 或 peak-drop-recovery 行为有关。对于 PEGASUS-X，发现使用更大的检查点会导致性能下降。

Conclusion: 这项工作强调了在处理稀缺训练数据时，微调具有高表达性的模型所面临的挑战和风险，并为未来研究更强大的专业领域摘要模型微调策略奠定了基础。

Abstract: Regardless of the rapid development of artificial intelligence, abstractive
summarisation is still challenging for sensitive and data-restrictive domains
like medicine. With the increasing number of imaging, the relevance of
automated tools for complex medical text summarisation is expected to become
highly relevant. In this paper, we investigated the adaptation via fine-tuning
process of a non-domain-specific abstractive summarisation encoder-decoder
model family, and gave insights to practitioners on how to avoid over- and
underfitting. We used PEGASUS and PEGASUS-X, on a medium-sized radiological
reports public dataset. For each model, we comprehensively evaluated two
different checkpoints with varying sizes of the same training data. We
monitored the models' performances with lexical and semantic metrics during the
training history on the fixed-size validation set. PEGASUS exhibited different
phases, which can be related to epoch-wise double-descent, or
peak-drop-recovery behaviour. For PEGASUS-X, we found that using a larger
checkpoint led to a performance detriment. This work highlights the challenges
and risks of fine-tuning models with high expressivity when dealing with scarce
training data, and lays the groundwork for future investigations into more
robust fine-tuning strategies for summarisation models in specialised domains.

</details>


### [13] [BiRQ: Bi-Level Self-Labeling Random Quantization for Self-Supervised Speech Recognition](https://arxiv.org/abs/2509.15430)
*Liuyuan Jiang,Xiaodong Cui,Brian Kingsbury,Tianyi Chen,Lisha Chen*

Main category: cs.CL

TL;DR: BiRQ: Bilevel SSL combines BEST-RQ efficiency with HuBERT-style refinement by reusing the model as a pseudo-label generator, enhancing labels and stabilizing training.


<details>
  <summary>Details</summary>
Motivation: Speech SSL requires informative and efficient pseudo-labels. Strong labels improve performance but are complex, while efficient methods are weaker.

Method: BiRQ uses a bilevel SSL framework with a random-projection quantizer for enhanced labels and anchors from raw input for stable training. It formulates training as a bilevel optimization problem solved with Gumbel-softmax.

Result: BiRQ improves over BEST-RQ while maintaining low complexity and computational efficiency.

Conclusion: BiRQ consistently gains over BEST-RQ on LibriSpeech, AMI meetings, and YODAS datasets.

Abstract: Speech is a rich signal, and labeled audio-text pairs are costly, making
self-supervised learning essential for scalable representation learning. A core
challenge in speech SSL is generating pseudo-labels that are both informative
and efficient: strong labels, such as those used in HuBERT, improve downstream
performance but rely on external encoders and multi-stage pipelines, while
efficient methods like BEST-RQ achieve simplicity at the cost of weaker labels.
We propose BiRQ, a bilevel SSL framework that combines the efficiency of
BEST-RQ with the refinement benefits of HuBERT-style label enhancement. The key
idea is to reuse part of the model itself as a pseudo-label generator:
intermediate representations are discretized by a random-projection quantizer
to produce enhanced labels, while anchoring labels derived directly from the
raw input stabilize training and prevent collapse. Training is formulated as an
efficient first-order bilevel optimization problem, solved end-to-end with
differentiable Gumbel-softmax selection. This design eliminates the need for
external label encoders, reduces memory cost, and enables iterative label
refinement in an end-to-end fashion. BiRQ consistently improves over BEST-RQ
while maintaining low complexity and computational efficiency. We validate our
method on various datasets, including 960-hour LibriSpeech, 150-hour AMI
meetings and 5,000-hour YODAS, demonstrating consistent gains over BEST-RQ.

</details>


### [14] [PILOT: Steering Synthetic Data Generation with Psychological & Linguistic Output Targeting](https://arxiv.org/abs/2509.15447)
*Caitlin Cisar,Emily Sheffield,Joshua Drake,Alden Harrell,Subramanian Chidambaram,Nikita Nangia,Vinayak Arannil,Alex Williams*

Main category: cs.CL

TL;DR: PILOT is a two-phase framework that uses structured psycholinguistic profiles to guide large language models, improving coherence and reducing repetition compared to natural language steering.


<details>
  <summary>Details</summary>
Motivation: Current generative AI relies on natural language personas, which can lead to unintended inferences and limit control over outputs.

Method: PILOT translates natural language personas into multidimensional profiles with normalized scores across linguistic and psychological dimensions to guide generation.

Result: Schema-based approaches significantly reduce repetition and improve coherence, with increased silhouette scores and topic purity. HPS balances variety and consistency.

Conclusion: PILOT maintains high response quality across all conditions, offering a balance between conciseness, consistency, and lexical diversity depending on the steering approach used (SBS, NPS, or HPS).

Abstract: Generative AI applications commonly leverage user personas as a steering
mechanism for synthetic data generation, but reliance on natural language
representations forces models to make unintended inferences about which
attributes to emphasize, limiting precise control over outputs. We introduce
PILOT (Psychological and Linguistic Output Targeting), a two-phase framework
for steering large language models with structured psycholinguistic profiles.
In Phase 1, PILOT translates natural language persona descriptions into
multidimensional profiles with normalized scores across linguistic and
psychological dimensions. In Phase 2, these profiles guide generation along
measurable axes of variation. We evaluate PILOT across three state-of-the-art
LLMs (Mistral Large 2, Deepseek-R1, LLaMA 3.3 70B) using 25 synthetic personas
under three conditions: Natural-language Persona Steering (NPS), Schema-Based
Steering (SBS), and Hybrid Persona-Schema Steering (HPS). Results demonstrate
that schema-based approaches significantly reduce artificial-sounding persona
repetition while improving output coherence, with silhouette scores increasing
from 0.098 to 0.237 and topic purity from 0.773 to 0.957. Our analysis reveals
a fundamental trade-off: SBS produces more concise outputs with higher topical
consistency, while NPS offers greater lexical diversity but reduced
predictability. HPS achieves a balance between these extremes, maintaining
output variety while preserving structural consistency. Expert linguistic
evaluation confirms that PILOT maintains high response quality across all
conditions, with no statistically significant differences between steering
approaches.

</details>


### [15] [Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding](https://arxiv.org/abs/2509.15476)
*Zhu Li,Xiyuan Gao,Yuqing Zhang,Shekhar Nayak,Matt Coler*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）和多模态LLM在英语和中文讽刺检测中的应用，通过零样本、少样本和LoRA微调设置进行评估。


<details>
  <summary>Details</summary>
Motivation: 先前的研究主要集中在文本或视觉文本讽刺上，而对全面的音频-视觉-文本讽刺理解仍然不足。

Method: 系统地评估大型语言模型（LLM）和多模态LLM在英语和中文讽刺检测中的zero-shot、few-shot和LoRA微调设置下的性能。此外，还探索了将模型作为特征编码器，并通过协作门控融合模块整合它们的表示。

Result: 基于音频的模型实现了最强的单模态性能，而文本-音频和音频-视觉组合优于单模态和三模态模型。Qwen-Omni等MLLM表现出有竞争力的zero-shot和微调性能。

Conclusion: 研究结果强调了MLLM在跨语言、音频-视觉-文本讽刺理解方面的潜力。

Abstract: Sarcasm detection remains a challenge in natural language understanding, as
sarcastic intent often relies on subtle cross-modal cues spanning text, speech,
and vision. While prior work has primarily focused on textual or visual-textual
sarcasm, comprehensive audio-visual-textual sarcasm understanding remains
underexplored. In this paper, we systematically evaluate large language models
(LLMs) and multimodal LLMs for sarcasm detection on English (MUStARD++) and
Chinese (MCSD 1.0) in zero-shot, few-shot, and LoRA fine-tuning settings. In
addition to direct classification, we explore models as feature encoders,
integrating their representations through a collaborative gating fusion module.
Experimental results show that audio-based models achieve the strongest
unimodal performance, while text-audio and audio-vision combinations outperform
unimodal and trimodal models. Furthermore, MLLMs such as Qwen-Omni show
competitive zero-shot and fine-tuned performance. Our findings highlight the
potential of MLLMs for cross-lingual, audio-visual-textual sarcasm
understanding.

</details>


### [16] [Red Teaming Multimodal Language Models: Evaluating Harm Across Prompt Modalities and Models](https://arxiv.org/abs/2509.15478)
*Madison Van Doren,Casey Ford,Emily Dix*

Main category: cs.CL

TL;DR: 本研究评估了四种领先的多模态大型语言模型（MLLM）在对抗性提示下的安全性，包括GPT-4o、Claude Sonnet 3.5、Pixtral 12B和Qwen VL Plus。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型（MLLM）越来越多地应用于现实世界的应用，但它们在对抗性条件下的安全性仍未得到充分探索。

Method: 由26名红队成员生成了726个提示，针对三个危害类别：非法活动、虚假信息和不道德行为。这些提示被提交给每个模型，17名注释者使用5分制对2,904个模型输出的有害性进行了评级。

Result: 结果表明，不同模型和模态的漏洞存在显著差异。Pixtral 12B表现出最高的有害反应率（~62%），而Claude Sonnet 3.5的抵抗力最强（~10%）。与预期相反，纯文本提示在绕过安全机制方面比多模态提示略有效。统计分析证实，模型类型和输入模态都是有害性的重要预测指标。

Conclusion: 这些发现强调，随着MLLM的更广泛部署，迫切需要强大的多模态安全基准。

Abstract: Multimodal large language models (MLLMs) are increasingly used in real world
applications, yet their safety under adversarial conditions remains
underexplored. This study evaluates the harmlessness of four leading MLLMs
(GPT-4o, Claude Sonnet 3.5, Pixtral 12B, and Qwen VL Plus) when exposed to
adversarial prompts across text-only and multimodal formats. A team of 26 red
teamers generated 726 prompts targeting three harm categories: illegal
activity, disinformation, and unethical behaviour. These prompts were submitted
to each model, and 17 annotators rated 2,904 model outputs for harmfulness
using a 5-point scale. Results show significant differences in vulnerability
across models and modalities. Pixtral 12B exhibited the highest rate of harmful
responses (~62%), while Claude Sonnet 3.5 was the most resistant (~10%).
Contrary to expectations, text-only prompts were slightly more effective at
bypassing safety mechanisms than multimodal ones. Statistical analysis
confirmed that both model type and input modality were significant predictors
of harmfulness. These findings underscore the urgent need for robust,
multimodal safety benchmarks as MLLMs are deployed more widely.

</details>


### [17] [mucAI at BAREC Shared Task 2025: Towards Uncertainty Aware Arabic Readability Assessment](https://arxiv.org/abs/2509.15485)
*Ahmed Abdou*

Main category: cs.CL

TL;DR: 本文提出了一种简单的、与模型无关的后处理技术，用于BAREC 2025共享任务中的细粒度阿拉伯语可读性分类（19个序数级别）。


<details>
  <summary>Details</summary>
Motivation: 该方法通过减少高惩罚的错误分类到更近的级别来提高二次加权 Kappa (QWK)。

Method: 该方法应用共形预测来生成具有覆盖率保证的预测集，然后使用 softmax 重新归一化的概率计算共形集上的加权平均值。

Result: 我们的方法在不同的基本模型上显示出一致的 QWK 改进 1-3 个点。在严格的 track 中，我们的提交在句子级别达到了 84.9%（测试）和 85.7%（盲测）的 QWK 分数，在文档级别达到了 73.3%。

Conclusion: 对于阿拉伯语教育评估，这使人工审核员能够专注于少数几个可能的级别，将统计保证与实际可用性相结合。

Abstract: We present a simple, model-agnostic post-processing technique for
fine-grained Arabic readability classification in the BAREC 2025 Shared Task
(19 ordinal levels). Our method applies conformal prediction to generate
prediction sets with coverage guarantees, then computes weighted averages using
softmax-renormalized probabilities over the conformal sets. This
uncertainty-aware decoding improves Quadratic Weighted Kappa (QWK) by reducing
high-penalty misclassifications to nearer levels. Our approach shows consistent
QWK improvements of 1-3 points across different base models. In the strict
track, our submission achieves QWK scores of 84.9\%(test) and 85.7\% (blind
test) for sentence level, and 73.3\% for document level. For Arabic educational
assessment, this enables human reviewers to focus on a handful of plausible
levels, combining statistical guarantees with practical usability.

</details>


### [18] [LLM Cache Bandit Revisited: Addressing Query Heterogeneity for Cost-Effective LLM Inference](https://arxiv.org/abs/2509.15515)
*Hantao Yang,Hong Xie,Defu Lian,Enhong Chen*

Main category: cs.CL

TL;DR: 本文研究了LLM缓存bandit问题，重点关注解决查询异质性以实现具有成本效益的LLM推理。


<details>
  <summary>Details</summary>
Motivation: 先前的工作通常假设统一的查询大小。异构的查询大小为缓存选择引入了组合结构，使得缓存替换过程在计算和统计上更具挑战性。

Method: 我们将最佳缓存选择视为一个背包问题，并采用基于累积的策略来有效地平衡计算开销和缓存更新。

Result: 在理论分析中，我们证明了我们算法的regret达到了$O(\sqrt{MNT})$的界限，与Berkeley的$O(MN\sqrt{T})$结果相比，提高了$\sqrt{MN}$的系数。此外，我们还提供了先前工作中没有的与问题相关的界限。

Conclusion: 实验依赖于真实世界的数据表明，我们的算法将总成本降低了约12%。

Abstract: This paper revisits the LLM cache bandit problem, with a special focus on
addressing the query heterogeneity for cost-effective LLM inference. Previous
works often assume uniform query sizes. Heterogeneous query sizes introduce a
combinatorial structure for cache selection, making the cache replacement
process more computationally and statistically challenging. We treat optimal
cache selection as a knapsack problem and employ an accumulation-based strategy
to effectively balance computational overhead and cache updates. In theoretical
analysis, we prove that the regret of our algorithm achieves an $O(\sqrt{MNT})$
bound, improving the coefficient of $\sqrt{MN}$ compared to the $O(MN\sqrt{T})$
result in Berkeley, where $N$ is the total number of queries and $M$ is the
cache size. Additionally, we also provide a problem-dependent bound, which was
absent in previous works. The experiment rely on real-world data show that our
algorithm reduces the total cost by approximately 12\%.

</details>


### [19] [How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages](https://arxiv.org/abs/2509.15518)
*Siyang Wu,Zhewei Sun*

Main category: cs.CL

TL;DR: 论文比较了人类和机器生成的俚语用法，发现大型语言模型在俚语感知上存在偏差。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLM）在俚语检测和解释等中间任务中的应用，但其泛化性和可靠性依赖于模型是否掌握与人类俚语用法一致的结构知识。

Method: 系统比较人类和机器生成的俚语用法，评估框架关注三个核心方面：用法特征、创造性和信息量。

Result: 发现大型语言模型在俚语感知上存在显著偏差，尽管在创造性方面有所掌握，但与人类的对齐不足以支持语言分析等推断任务。

Conclusion: 大型语言模型在俚语理解方面仍存在局限性，尤其是在需要与人类用法对齐的推断任务中。

Abstract: Slang is a commonly used type of informal language that poses a daunting
challenge to NLP systems. Recent advances in large language models (LLMs),
however, have made the problem more approachable. While LLM agents are becoming
more widely applied to intermediary tasks such as slang detection and slang
interpretation, their generalizability and reliability are heavily dependent on
whether these models have captured structural knowledge about slang that align
well with human attested slang usages. To answer this question, we contribute a
systematic comparison between human and machine-generated slang usages. Our
evaluative framework focuses on three core aspects: 1) Characteristics of the
usages that reflect systematic biases in how machines perceive slang, 2)
Creativity reflected by both lexical coinages and word reuses employed by the
slang usages, and 3) Informativeness of the slang usages when used as
gold-standard examples for model distillation. By comparing human-attested
slang usages from the Online Slang Dictionary (OSD) and slang generated by
GPT-4o and Llama-3, we find significant biases in how LLMs perceive slang. Our
results suggest that while LLMs have captured significant knowledge about the
creative aspects of slang, such knowledge does not align with humans
sufficiently to enable LLMs for extrapolative tasks such as linguistic
analyses.

</details>


### [20] [A method for improving multilingual quality and diversity of instruction fine-tuning datasets](https://arxiv.org/abs/2509.15549)
*Chunguang Zhao,Yilun Liu,Pufan Zeng,Yuanchang Luo,Shimin Tao,Minggui He,Weibin Meng,Song Xu,Ziang Chen,Chen Liu,Hongxia Ma,Li Zhang,Boxing Chen,Daimeng Wei*

Main category: cs.CL

TL;DR: 本文提出了一种名为M-DaQ的新方法，通过选择高质量和语义多样的多语言IFT样本来提高LLM的多语言能力。


<details>
  <summary>Details</summary>
Motivation: 高质量多语言训练数据的稀缺是LLM有效推广到不同语言和文化背景的关键瓶颈。现有方法由于依赖简单的启发式或特定于语言的假设，通常无法跨语言推广。

Method: 本文提出Multilingual Data Quality and Diversity (M-DaQ)方法，用于选择高质量和语义多样的多语言IFT样本。同时，对多语言环境下的肤浅对齐假设（SAH）进行了首次系统研究。

Result: 在18种语言上的实证结果表明，使用M-DaQ方法进行微调的模型比原始基线模型取得了显著的性能提升，胜率超过60%。

Conclusion: 人工评估进一步验证了这些成果，突出了响应中文化点的增加。

Abstract: Multilingual Instruction Fine-Tuning (IFT) is essential for enabling large
language models (LLMs) to generalize effectively across diverse linguistic and
cultural contexts. However, the scarcity of high-quality multilingual training
data and corresponding building method remains a critical bottleneck. While
data selection has shown promise in English settings, existing methods often
fail to generalize across languages due to reliance on simplistic heuristics or
language-specific assumptions. In this work, we introduce Multilingual Data
Quality and Diversity (M-DaQ), a novel method for improving LLMs
multilinguality, by selecting high-quality and semantically diverse
multilingual IFT samples. We further conduct the first systematic investigation
of the Superficial Alignment Hypothesis (SAH) in multilingual setting.
Empirical results across 18 languages demonstrate that models fine-tuned with
M-DaQ method achieve significant performance gains over vanilla baselines over
60% win rate. Human evaluations further validate these gains, highlighting the
increment of cultural points in the response. We release the M-DaQ code to
support future research.

</details>


### [21] [DNA-DetectLLM: Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm](https://arxiv.org/abs/2509.15550)
*Xiaowei Zhu,Yubing Ren,Fang Fang,Qingfeng Tan,Shi Wang,Yanan Cao*

Main category: cs.CL

TL;DR: 提出了一种名为 DNA-DetectLLM 的零样本检测方法，用于区分 AI 生成和人类撰写的文本，该方法通过量化累积修复工作来捕捉这两种文本之间的内在差异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 的快速发展模糊了 AI 生成文本和人类撰写文本之间的界限，带来了诸如错误信息、作者身份模糊和知识产权问题等社会风险，因此迫切需要可靠的 AI 生成文本检测方法。然而，生成语言建模的最新进展导致人类撰写文本和 AI 生成文本的特征分布之间存在显著重叠，模糊了分类边界，使得准确检测越来越具有挑战性。

Method: 提出了一种 DNA 启发的方法，利用基于修复的过程来直接且可解释地捕获人类撰写文本和 AI 生成文本之间的内在差异。该方法为每个输入构建一个理想的 AI 生成序列，迭代修复非最优 token，并将累积修复工作量化为可解释的检测信号。

Result: DNA-DetectLLM 在多个公共基准数据集上实现了 5.55% 的 AUROC 相对提升和 2.08% 的 F1 分数相对提升。

Conclusion: DNA-DetectLLM 实现了最先进的检测性能，并且对各种对抗性攻击和输入长度表现出强大的鲁棒性。

Abstract: The rapid advancement of large language models (LLMs) has blurred the line
between AI-generated and human-written text. This progress brings societal
risks such as misinformation, authorship ambiguity, and intellectual property
concerns, highlighting the urgent need for reliable AI-generated text detection
methods. However, recent advances in generative language modeling have resulted
in significant overlap between the feature distributions of human-written and
AI-generated text, blurring classification boundaries and making accurate
detection increasingly challenging. To address the above challenges, we propose
a DNA-inspired perspective, leveraging a repair-based process to directly and
interpretably capture the intrinsic differences between human-written and
AI-generated text. Building on this perspective, we introduce DNA-DetectLLM, a
zero-shot detection method for distinguishing AI-generated and human-written
text. The method constructs an ideal AI-generated sequence for each input,
iteratively repairs non-optimal tokens, and quantifies the cumulative repair
effort as an interpretable detection signal. Empirical evaluations demonstrate
that our method achieves state-of-the-art detection performance and exhibits
strong robustness against various adversarial attacks and input lengths.
Specifically, DNA-DetectLLM achieves relative improvements of 5.55% in AUROC
and 2.08% in F1 score across multiple public benchmark datasets.

</details>


### [22] [Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining](https://arxiv.org/abs/2509.15556)
*Ping Guo,Yubing Ren,Binbin Liu,Fengze Liu,Haobin Lin,Yifan Zhang,Bingni Zhang,Taifeng Wang,Yin Zheng*

Main category: cs.CL

TL;DR: 提出了一种名为Climb的框架，用于优化多语言数据分配，从而提升大型语言模型的多语言性能。


<details>
  <summary>Details</summary>
Motivation: 确定最佳语言比例具有挑战性，因为存在复杂的跨语言交互和对数据集规模的敏感性。

Method: Climb引入了一种跨语言交互感知的语言比例，通过捕捉语言间的依赖关系来量化每种语言的有效分配。然后，采用两步优化程序：首先平衡跨语言的边际收益，然后最大化由此产生的语言分配向量的大小。

Result: Climb能够准确测量各种多语言环境下的跨语言交互。使用Climb得出的比例训练的LLM始终能实现最先进的多语言性能，甚至能达到与使用更多tokens训练的开源LLM相媲美的性能。

Conclusion: Climb框架有效地优化了多语言数据分配，显著提升了LLM的多语言性能。

Abstract: Large language models (LLMs) have become integral to a wide range of
applications worldwide, driving an unprecedented global demand for effective
multilingual capabilities. Central to achieving robust multilingual performance
is the strategic allocation of language proportions within training corpora.
However, determining optimal language ratios is highly challenging due to
intricate cross-lingual interactions and sensitivity to dataset scale. This
paper introduces Climb (Cross-Lingual Interaction-aware Multilingual
Balancing), a novel framework designed to systematically optimize multilingual
data allocation. At its core, Climb introduces a cross-lingual
interaction-aware language ratio, explicitly quantifying each language's
effective allocation by capturing inter-language dependencies. Leveraging this
ratio, Climb proposes a principled two-step optimization procedure--first
equalizing marginal benefits across languages, then maximizing the magnitude of
the resulting language allocation vectors--significantly simplifying the
inherently complex multilingual optimization problem. Extensive experiments
confirm that Climb can accurately measure cross-lingual interactions across
various multilingual settings. LLMs trained with Climb-derived proportions
consistently achieve state-of-the-art multilingual performance, even achieving
competitive performance with open-sourced LLMs trained with more tokens.

</details>


### [23] [How important is language for human-like intelligence?](https://arxiv.org/abs/2509.15560)
*Gary Lupyan,Hunter Gentry,Martin Zettersten*

Main category: cs.CL

TL;DR: 语言对于通用人工智能系统和人类智能的出现至关重要。


<details>
  <summary>Details</summary>
Motivation: 探讨语言在人类认知中的作用，以及它是否仅仅是思想的表达。

Method: 通过分析语言的两个相关属性来论证语言的重要性。

Result: 语言提供了紧凑的表示，使抽象概念的表示和推理更加容易。这些压缩的表示是集体思维的迭代输出。

Conclusion: 一个足够强大的学习系统，无论生物的还是人工的，通过接触语言，可以学习到世界的压缩模型，逆向工程许多支持人类（和类人）思想的概念和因果结构。

Abstract: We use language to communicate our thoughts. But is language merely the
expression of thoughts, which are themselves produced by other, nonlinguistic
parts of our minds? Or does language play a more transformative role in human
cognition, allowing us to have thoughts that we otherwise could (or would) not
have? Recent developments in artificial intelligence (AI) and cognitive science
have reinvigorated this old question. We argue that language may hold the key
to the emergence of both more general AI systems and central aspects of human
intelligence. We highlight two related properties of language that make it such
a powerful tool for developing domain--general abilities. First, language
offers compact representations that make it easier to represent and reason
about many abstract concepts (e.g., exact numerosity). Second, these compressed
representations are the iterated output of collective minds. In learning a
language, we learn a treasure trove of culturally evolved abstractions. Taken
together, these properties mean that a sufficiently powerful learning system
exposed to language--whether biological or artificial--learns a compressed
model of the world, reverse engineering many of the conceptual and causal
structures that support human (and human-like) thought.

</details>


### [24] [LiteLong: Resource-Efficient Long-Context Data Synthesis for LLMs](https://arxiv.org/abs/2509.15568)
*Junlong Jia,Xing Wu,Chaochen Gao,Ziyang Chen,Zijia Lin,Zhongzhi Li,Weinong Wang,Haotian Xu,Donghui Jin,Debing Zhang,Binghui Guo*

Main category: cs.CL

TL;DR: LiteLong: A resource-efficient method for synthesizing high-quality long-context data.


<details>
  <summary>Details</summary>
Motivation: Existing long-context data synthesis methods are computationally inefficient.

Method: Structured topic organization using BISAC and multi-agent debate with LLMs to generate diverse topics, followed by BM25 retrieval for relevant documents.

Result: LiteLong achieves competitive performance on HELMET and Ruler benchmarks and integrates with other long-dependency methods.

Conclusion: LiteLong reduces the cost of long-context data synthesis, making it more accessible for research.

Abstract: High-quality long-context data is essential for training large language
models (LLMs) capable of processing extensive documents, yet existing synthesis
approaches using relevance-based aggregation face challenges of computational
efficiency. We present LiteLong, a resource-efficient method for synthesizing
long-context data through structured topic organization and multi-agent debate.
Our approach leverages the BISAC book classification system to provide a
comprehensive hierarchical topic organization, and then employs a debate
mechanism with multiple LLMs to generate diverse, high-quality topics within
this structure. For each topic, we use lightweight BM25 retrieval to obtain
relevant documents and concatenate them into 128K-token training samples.
Experiments on HELMET and Ruler benchmarks demonstrate that LiteLong achieves
competitive long-context performance and can seamlessly integrate with other
long-dependency enhancement methods. LiteLong makes high-quality long-context
data synthesis more accessible by reducing both computational and data
engineering costs, facilitating further research in long-context language
training.

</details>


### [25] [Relevance to Utility: Process-Supervised Rewrite for RAG](https://arxiv.org/abs/2509.15577)
*Jaeyoung Kim,Jongho Kim,Seung-won Hwang,Seoho Song,Young-In Song*

Main category: cs.CL

TL;DR: R2U通过直接优化生成正确答案的概率来弥补检索相关性和生成实用性之间的差距。它使用来自LLM的监督来扩展蒸馏管道，从而帮助较小的重写模型更好地泛化。


<details>
  <summary>Details</summary>
Motivation: 现有的“桥”模块无法捕捉到真正的文档实用性。

Method: R2U直接优化以最大化通过过程监督生成正确答案的概率。通过扩展来自LLM的监督来近似一个有效的蒸馏管道。

Result: 在多个开放域问答基准测试中，经验结果表明相对于强大的桥接基线有持续的改进。

Conclusion: R2U优于现有的桥接基线，因为它直接优化了生成正确答案的概率，并有效地利用了来自LLM的监督。

Abstract: Retrieval-Augmented Generation systems often suffer from a gap between
optimizing retrieval relevance and generative utility: retrieved documents may
be topically relevant but still lack the content needed for effective reasoning
during generation. While existing "bridge" modules attempt to rewrite the
retrieved text for better generation, we show how they fail to capture true
document utility. In this work, we propose R2U, with a key distinction of
directly optimizing to maximize the probability of generating a correct answer
through process supervision. As such direct observation is expensive, we also
propose approximating an efficient distillation pipeline by scaling the
supervision from LLMs, which helps the smaller rewriter model generalize
better. We evaluate our method across multiple open-domain question-answering
benchmarks. The empirical results demonstrate consistent improvements over
strong bridging baselines.

</details>


### [26] [Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization](https://arxiv.org/abs/2509.15579)
*Yun Tang,Cindy Tseng*

Main category: cs.CL

TL;DR: 提出了一种基于块的自监督学习（Chunk SSL）算法，用于流式和离线语音预训练。


<details>
  <summary>Details</summary>
Motivation: 随着语音技术在过去十年中的快速发展，低延迟语音人机通信变得越来越重要。自监督学习是语音技术进步的主要因素之一。大多数自监督学习算法都是在完整语段假设下设计的，如果出现部分语段（在流式应用中很常见），则必须做出妥协。

Method: Chunk SSL通过掩码预测损失进行优化，并鼓励声学编码器在同一块和前面的块中未掩码帧的帮助下恢复那些掩码语音帧的索引。提出了一种复制和附加数据增强方法来进行有效的基于块的预训练。Chunk SSL利用有限标量量化（FSQ）模块来离散化输入语音特征，研究表明高分辨率FSQ码本有利于将知识从预训练任务转移到下游任务。在预训练期间采用分组掩码预测损失，以减轻大型码本带来的高内存和计算成本。

Result: 在Librispeech和Must-C数据集上的实验结果表明，该方法在流式和离线模式下都能在语音到文本任务中取得非常有竞争力的结果。

Conclusion: 该方法在语音到文本任务中取得了有竞争力的结果，适用于流式和离线模式。

Abstract: Low latency speech human-machine communication is becoming increasingly
necessary as speech technology advances quickly in the last decade. One of the
primary factors behind the advancement of speech technology is self-supervised
learning. Most self-supervised learning algorithms are designed with full
utterance assumption and compromises have to made if partial utterances are
presented, which are common in the streaming applications. In this work, we
propose a chunk based self-supervised learning (Chunk SSL) algorithm as an
unified solution for both streaming and offline speech pre-training. Chunk SSL
is optimized with the masked prediction loss and an acoustic encoder is
encouraged to restore indices of those masked speech frames with help from
unmasked frames in the same chunk and preceding chunks. A copy and append data
augmentation approach is proposed to conduct efficient chunk based
pre-training. Chunk SSL utilizes a finite scalar quantization (FSQ) module to
discretize input speech features and our study shows a high resolution FSQ
codebook, i.e., a codebook with vocabulary size up to a few millions, is
beneficial to transfer knowledge from the pre-training task to the downstream
tasks. A group masked prediction loss is employed during pre-training to
alleviate the high memory and computation cost introduced by the large
codebook. The proposed approach is examined in two speech to text tasks, i.e.,
speech recognition and speech translation. Experimental results on the
\textsc{Librispeech} and \textsc{Must-C} datasets show that the proposed method
could achieve very competitive results for speech to text tasks at both
streaming and offline modes.

</details>


### [27] [DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models](https://arxiv.org/abs/2509.15587)
*Tsz Ting Chung,Lemao Liu,Mo Yu,Dit-Yan Yeung*

Main category: cs.CL

TL;DR: 这篇论文提出了一个新的经典逻辑基准测试DivLogicEval，包含以违反直觉的方式组成的不同陈述的自然语句。


<details>
  <summary>Details</summary>
Motivation: 现有逻辑推理基准在语言多样性方面受到限制，并且它们的分布偏离了理想的逻辑推理基准的分布，这可能导致有偏差的评估结果。

Method: 该论文提出了一个新的经典逻辑基准测试DivLogicEval，包含以违反直觉的方式组成的不同陈述的自然语句。为了确保更可靠的评估，该论文还引入了一种新的评估指标，以减轻LLM中固有的偏差和随机性的影响。

Result: 通过实验，该论文证明了回答DivLogicEval中的问题需要逻辑推理的程度，并比较了不同流行的LLM在进行逻辑推理方面的表现。

Conclusion: 该论文提出了一个新的基准测试和一个新的评估指标，以更可靠地评估LLM的逻辑推理能力。

Abstract: Logic reasoning in natural language has been recognized as an important
measure of human intelligence for Large Language Models (LLMs). Popular
benchmarks may entangle multiple reasoning skills and thus provide unfaithful
evaluations on the logic reasoning skill. Meanwhile, existing logic reasoning
benchmarks are limited in language diversity and their distributions are
deviated from the distribution of an ideal logic reasoning benchmark, which may
lead to biased evaluation results. This paper thereby proposes a new classical
logic benchmark DivLogicEval, consisting of natural sentences composed of
diverse statements in a counterintuitive way. To ensure a more reliable
evaluation, we also introduce a new evaluation metric that mitigates the
influence of bias and randomness inherent in LLMs. Through experiments, we
demonstrate the extent to which logical reasoning is required to answer the
questions in DivLogicEval and compare the performance of different popular LLMs
in conducting logical reasoning.

</details>


### [28] [SciEvent: Benchmarking Multi-domain Scientific Event Extraction](https://arxiv.org/abs/2509.15620)
*Bofu Dong,Pritesh Shah,Sumedh Sonawane,Tiyasha Banerjee,Erin Brady,Xinya Du,Ming Jiang*

Main category: cs.CL

TL;DR: 提出了SciEvent，一个新的多领域科学摘要基准测试，用于统一事件提取（EE），旨在实现对科学内容的结构化和上下文感知理解。


<details>
  <summary>Details</summary>
Motivation: 现有的科学信息提取（SciIE）主要依赖于狭窄领域的实体关系提取，限制了其在跨学科研究中的适用性，难以捕捉必要的科学信息上下文，导致语句碎片化或冲突。

Method: 定义SciIE为一个多阶段EE流程：(1) 将摘要分割为核心科学活动--背景、方法、结果和结论；(2) 提取相应的触发词和论点。使用微调的EE模型、大型语言模型（LLM）和人工注释器进行实验。

Result: 实验表明，现有模型在社会学和人文学科等领域表现不佳，存在性能差距。

Conclusion: SciEvent作为一个具有挑战性的基准，是朝着通用、多领域SciIE迈出的一步。

Abstract: Scientific information extraction (SciIE) has primarily relied on
entity-relation extraction in narrow domains, limiting its applicability to
interdisciplinary research and struggling to capture the necessary context of
scientific information, often resulting in fragmented or conflicting
statements. In this paper, we introduce SciEvent, a novel multi-domain
benchmark of scientific abstracts annotated via a unified event extraction (EE)
schema designed to enable structured and context-aware understanding of
scientific content. It includes 500 abstracts across five research domains,
with manual annotations of event segments, triggers, and fine-grained
arguments. We define SciIE as a multi-stage EE pipeline: (1) segmenting
abstracts into core scientific activities--Background, Method, Result, and
Conclusion; and (2) extracting the corresponding triggers and arguments.
Experiments with fine-tuned EE models, large language models (LLMs), and human
annotators reveal a performance gap, with current models struggling in domains
such as sociology and humanities. SciEvent serves as a challenging benchmark
and a step toward generalizable, multi-domain SciIE.

</details>


### [29] [Concept Unlearning in Large Language Models via Self-Constructed Knowledge Triplets](https://arxiv.org/abs/2509.15621)
*Tomoya Yamashita,Yuuki Yamanaka,Masanori Yamada,Takayuki Miura,Toshiki Shibahara,Tomoharu Iwata*

Main category: cs.CL

TL;DR: 提出了一种新的概念卸载（CU）方法，用于从大型语言模型（LLM）中移除更广泛的概念，例如人物或事件，而不是仅仅移除特定的目标句子。


<details>
  <summary>Details</summary>
Motivation: 现有的机器卸载（MU）方法需要明确的目标句子，并且不支持移除更广泛的概念，例如人物或事件。为了解决这个局限性，我们引入了概念卸载（CU）作为LLM卸载的新要求。

Method: 利用知识图谱来表示LLM的内部知识，并将CU定义为移除遗忘目标节点和相关联的边。提出了一种新的方法，该方法提示LLM生成关于遗忘目标的知识三元组和解释性句子，并将卸载过程应用于这些表示。

Result: 在真实世界和合成数据集上的实验表明，我们的方法有效地实现了概念级别的卸载，同时保留了不相关的知识。

Conclusion: 通过将卸载过程与LLM的内部知识表示对齐，我们的方法能够实现更精确和全面的概念移除。

Abstract: Machine Unlearning (MU) has recently attracted considerable attention as a
solution to privacy and copyright issues in large language models (LLMs).
Existing MU methods aim to remove specific target sentences from an LLM while
minimizing damage to unrelated knowledge. However, these approaches require
explicit target sentences and do not support removing broader concepts, such as
persons or events. To address this limitation, we introduce Concept Unlearning
(CU) as a new requirement for LLM unlearning. We leverage knowledge graphs to
represent the LLM's internal knowledge and define CU as removing the forgetting
target nodes and associated edges. This graph-based formulation enables a more
intuitive unlearning and facilitates the design of more effective methods. We
propose a novel method that prompts the LLM to generate knowledge triplets and
explanatory sentences about the forgetting target and applies the unlearning
process to these representations. Our approach enables more precise and
comprehensive concept removal by aligning the unlearning process with the LLM's
internal knowledge representations. Experiments on real-world and synthetic
datasets demonstrate that our method effectively achieves concept-level
unlearning while preserving unrelated knowledge.

</details>


### [30] [Sparse-Autoencoder-Guided Internal Representation Unlearning for Large Language Models](https://arxiv.org/abs/2509.15631)
*Tomoya Yamashita,Akira Ito,Yuuki Yamanaka,Masanori Yamada,Takayuki Miura,Toshiki Shibahara*

Main category: cs.CL

TL;DR: 提出了一种新颖的LLM非学习方法，该方法直接干预模型的内部激活，从而使忘记的目标的激活与“未知”实体无法区分。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）越来越多地部署在各种应用程序中，隐私和版权问题日益突出，因此需要更有效的LLM非学习技术。许多现有的非学习方法旨在通过额外的训练（例如，梯度上升）来抑制不良输出，但这可能无法消除嵌入在模型内部激活中的基础知识；使响应静音与忘记响应是不一样的，并且这种基于抑制的方法通常会遭受模型崩溃的困扰。

Method: 该方法引入了一种非学习目标，该目标将目标实体的激活从已知实体的激活修改为稀疏自动编码器潜在空间中未知实体的激活。通过将目标的内部激活与未知实体的激活对齐，我们将模型对目标实体的识别从“已知”转移到“未知”，从而实现了真正的遗忘，同时避免了过度抑制和模型崩溃。

Result: 该方法有效地对齐了遗忘目标的内部激活，这是基于抑制的方法无法可靠实现的。此外，该方法有效地减少了模型在问答任务中对目标知识的记忆，而没有对非目标知识造成重大损害。

Conclusion: 提出了一种新颖的非学习方法，该方法通过直接干预模型的内部激活来实现真正的遗忘，避免了过度抑制和模型崩溃，并在实验中验证了其有效性。

Abstract: As large language models (LLMs) are increasingly deployed across various
applications, privacy and copyright concerns have heightened the need for more
effective LLM unlearning techniques. Many existing unlearning methods aim to
suppress undesirable outputs through additional training (e.g., gradient
ascent), which reduces the probability of generating such outputs. While such
suppression-based approaches can control model outputs, they may not eliminate
the underlying knowledge embedded in the model's internal activations; muting a
response is not the same as forgetting it. Moreover, such suppression-based
methods often suffer from model collapse. To address these issues, we propose a
novel unlearning method that directly intervenes in the model's internal
activations. In our formulation, forgetting is defined as a state in which the
activation of a forgotten target is indistinguishable from that of ``unknown''
entities. Our method introduces an unlearning objective that modifies the
activation of the target entity away from those of known entities and toward
those of unknown entities in a sparse autoencoder latent space. By aligning the
target's internal activation with those of unknown entities, we shift the
model's recognition of the target entity from ``known'' to ``unknown'',
achieving genuine forgetting while avoiding over-suppression and model
collapse. Empirically, we show that our method effectively aligns the internal
activations of the forgotten target, a result that the suppression-based
approaches do not reliably achieve. Additionally, our method effectively
reduces the model's recall of target knowledge in question-answering tasks
without significant damage to the non-target knowledge.

</details>


### [31] [Multilingual LLM Prompting Strategies for Medical English-Vietnamese Machine Translation](https://arxiv.org/abs/2509.15640)
*Nhu Vo,Nu-Uyen-Phuong Le,Dung D. Le,Massimo Piccardi,Wray Buntine*

Main category: cs.CL

TL;DR: 本文研究了使用多语言LLM进行医学英语-越南语机器翻译，并评估了不同的prompt策略。


<details>
  <summary>Details</summary>
Motivation: 越南语是一种低资源且研究不足的语言，而医学英语-越南语机器翻译对于越南的医疗保健至关重要。

Method: 本文在MedEV数据集上，系统地评估了六个多语言LLM（0.5B-9B参数）的prompt策略，比较了zero-shot、few-shot和dictionary-augmented prompting。

Result: 结果表明，模型规模是性能的主要驱动因素。较大的LLM实现了强大的zero-shot结果，而few-shot prompting仅产生 marginal 的改进。相比之下，术语感知线索和基于嵌入的示例检索始终可以改进领域特定的翻译。

Conclusion: 这些发现强调了多语言LLM在医学英语-越南语机器翻译中的希望和当前的局限性。

Abstract: Medical English-Vietnamese machine translation (En-Vi MT) is essential for
healthcare access and communication in Vietnam, yet Vietnamese remains a
low-resource and under-studied language. We systematically evaluate prompting
strategies for six multilingual LLMs (0.5B-9B parameters) on the MedEV dataset,
comparing zero-shot, few-shot, and dictionary-augmented prompting with Meddict,
an English-Vietnamese medical lexicon. Results show that model scale is the
primary driver of performance: larger LLMs achieve strong zero-shot results,
while few-shot prompting yields only marginal improvements. In contrast,
terminology-aware cues and embedding-based example retrieval consistently
improve domain-specific translation. These findings underscore both the promise
and the current limitations of multilingual LLMs for medical En-Vi MT.

</details>


### [32] [Layer-wise Minimal Pair Probing Reveals Contextual Grammatical-Conceptual Hierarchy in Speech Representations](https://arxiv.org/abs/2509.15655)
*Linyang He,Qiaolin Wang,Xilin Jiang,Nima Mesgarani*

Main category: cs.CL

TL;DR: 本文研究了基于 Transformer 的语音语言模型 (SLM) 在多大程度上编码了细微的句法和概念特征。


<details>
  <summary>Details</summary>
Motivation: 现有的研究已经检查了 SLM 如何很好地编码浅层的声学和语音特征，但 SLM 在多大程度上编码了细微的句法和概念特征仍不清楚。

Method: 通过与大型语言模型的语言能力评估进行类比，本研究首次系统地评估了跨 SLM 的上下文句法和语义特征的存在，这些 SLM 用于自监督学习 (S3M)、自动语音识别 (ASR)、语音压缩 (codec) 以及作为听觉大型语言模型 (AudioLLM) 的编码器。通过跨越不同语言级别的 71 个任务的最小配对设计和诊断特征分析。

Result: 我们的分层和时间分辨分析表明：1) 所有语音编码语法特征比概念特征更稳健。

Conclusion: 所有语音编码语法特征比概念特征更稳健。

Abstract: Transformer-based speech language models (SLMs) have significantly improved
neural speech recognition and understanding. While existing research has
examined how well SLMs encode shallow acoustic and phonetic features, the
extent to which SLMs encode nuanced syntactic and conceptual features remains
unclear. By drawing parallels with linguistic competence assessments for large
language models, this study is the first to systematically evaluate the
presence of contextual syntactic and semantic features across SLMs for
self-supervised learning (S3M), automatic speech recognition (ASR), speech
compression (codec), and as the encoder for auditory large language models
(AudioLLMs). Through minimal pair designs and diagnostic feature analysis
across 71 tasks spanning diverse linguistic levels, our layer-wise and
time-resolved analysis uncovers that 1) all speech encode grammatical features
more robustly than conceptual ones.

</details>


### [33] [VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion](https://arxiv.org/abs/2509.15667)
*Dimitrios Damianos,Leon Voukoutis,Georgios Paraskevopoulos,Vassilis Katsouros*

Main category: cs.CL

TL;DR: 本文提出了一种多模态融合框架，连接了预训练的解码器大型语言模型（LLM）和声学编码器-解码器架构（如 Whisper），旨在构建具有语音功能的 LLM。


<details>
  <summary>Details</summary>
Motivation: 探索中间的音频条件文本空间，作为一种更有效的对齐机制，而不是直接使用音频嵌入。

Method: 该方法完全在连续文本表示空间中运行，通过交叉注意将 Whisper 的隐藏解码器状态与 LLM 的隐藏解码器状态融合，并支持离线和流式模式。

Result: 引入了第一个希腊语语音 LLM VoxKrikri，并通过分析表明，该方法有效地对齐了跨模态的表示，在希腊语自动语音识别方面取得了最先进的结果，在基准测试中平均提高了约 20%。

Conclusion: 连续空间融合是多语言和低资源语音 LLM 的一个有希望的途径。

Abstract: We present a multimodal fusion framework that bridges pre-trained
decoder-based large language models (LLM) and acoustic encoder-decoder
architectures such as Whisper, with the aim of building speech-enabled LLMs.
Instead of directly using audio embeddings, we explore an intermediate
audio-conditioned text space as a more effective mechanism for alignment. Our
method operates fully in continuous text representation spaces, fusing
Whisper's hidden decoder states with those of an LLM through cross-modal
attention, and supports both offline and streaming modes. We introduce
\textit{VoxKrikri}, the first Greek speech LLM, and show through analysis that
our approach effectively aligns representations across modalities. These
results highlight continuous space fusion as a promising path for multilingual
and low-resource speech LLMs, while achieving state-of-the-art results for
Automatic Speech Recognition in Greek, providing an average $\sim20\%$ relative
improvement across benchmarks.

</details>


### [34] [Fine-Tuning Large Multimodal Models for Automatic Pronunciation Assessment](https://arxiv.org/abs/2509.15701)
*Ke Wang,Wenning Wei,Yan Deng,Lei He,Sheng Zhao*

Main category: cs.CL

TL;DR: 本文研究了使用大型多模态模型（LMMs）进行自动发音评估（APA）的方法。


<details>
  <summary>Details</summary>
Motivation: 探索大型多模态模型在细粒度发音评估中的有效性。

Method: 通过在Speechocean762数据集和私有语料库上微调LMMs模型。

Result: 微调后的模型在单词和句子层面表现良好，但在音素层面的评估仍然具有挑战性。Pearson相关系数达到0.9，而Spearman等级相关系数约为0.6。

Conclusion: LMMs在APA中具有潜力，但也存在局限性，未来的工作应集中在细粒度建模和排序感知评估上。

Abstract: Automatic Pronunciation Assessment (APA) is critical for Computer-Assisted
Language Learning (CALL), requiring evaluation across multiple granularities
and aspects. Large Multimodal Models (LMMs) present new opportunities for APA,
but their effectiveness in fine-grained assessment remains uncertain. This work
investigates fine-tuning LMMs for APA using the Speechocean762 dataset and a
private corpus. Fine-tuning significantly outperforms zero-shot settings and
achieves competitive results on single-granularity tasks compared to public and
commercial systems. The model performs well at word and sentence levels, while
phoneme-level assessment remains challenging. We also observe that the Pearson
Correlation Coefficient (PCC) reaches 0.9, whereas Spearman's rank Correlation
Coefficient (SCC) remains around 0.6, suggesting that SCC better reflects
ordinal consistency. These findings highlight both the promise and limitations
of LMMs for APA and point to future work on fine-grained modeling and
rank-aware evaluation.

</details>


### [35] [Once Upon a Time: Interactive Learning for Storytelling with Small Language Models](https://arxiv.org/abs/2509.15714)
*Jonas Mayer Martins,Ali Hamza Bashir,Muhammad Rehan Khalid,Lisa Beinborn*

Main category: cs.CL

TL;DR: 大型语言模型通常通过预测下一个单词在大量文本上进行训练。受此启发，我们研究了语言模型是否可以通过不仅从下一个单词预测中学习，还可以从高级的、认知启发的反馈中学习，从而用更少的数据进行训练。


<details>
  <summary>Details</summary>
Motivation: 儿童有效地获取语言不仅通过听，而且通过在他们的社交环境中与他人互动。大型语言模型通常通过预测下一个单词在大量文本上进行训练。作者想知道语言模型是否可以通过互动学习，用更少的数据进行训练。

Method: 我们训练了一个学生模型来生成故事，老师模型对可读性、叙事连贯性和创造力进行评分。通过改变反馈循环之前的预训练量，我们评估了这种互动学习对形式和功能语言能力的影响。

Result: 高级反馈非常具有数据效率：在交互式学习中，只需 100 万字的输入，讲故事的技能就可以提高到通过 4.1 亿字的下一个单词预测所能达到的水平。

Conclusion: 互动学习可以提高语言模型的效率。

Abstract: Children efficiently acquire language not just by listening, but by
interacting with others in their social environment. Conversely, large language
models are typically trained with next-word prediction on massive amounts of
text. Motivated by this contrast, we investigate whether language models can be
trained with less data by learning not only from next-word prediction but also
from high-level, cognitively inspired feedback. We train a student model to
generate stories, which a teacher model rates on readability, narrative
coherence, and creativity. By varying the amount of pretraining before the
feedback loop, we assess the impact of this interactive learning on formal and
functional linguistic competence. We find that the high-level feedback is
highly data efficient: With just 1 M words of input in interactive learning,
storytelling skills can improve as much as with 410 M words of next-word
prediction.

</details>


### [36] [REFER: Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting](https://arxiv.org/abs/2509.15723)
*Nannan Huang,Haytham M. Fayek,Xiuzhen Zhang*

Main category: cs.CL

TL;DR: 本研究探讨了使用频率框架提示(REFER)来提高LLM意见总结的公平性。


<details>
  <summary>Details</summary>
Motivation: 以往的研究依赖于超参数调整或在提示中提供真实分布信息，但这些方法存在实际限制：终端用户很少修改默认模型参数，并且通常无法获得准确的分布信息。

Method: 通过系统实验不同的提示框架，我们调整了已知可以改善人类推理的技术，以引出语言模型中更有效的信息处理。

Result: 我们的结果表明，REFER可以提高语言模型在总结意见时的公平性。

Conclusion: 频率框架提示(REFER)可以提高LLM意见总结的公平性，尤其是在较大的语言模型和使用更强的推理指令时。

Abstract: Individuals express diverse opinions, a fair summary should represent these
viewpoints comprehensively. Previous research on fairness in opinion
summarisation using large language models (LLMs) relied on hyperparameter
tuning or providing ground truth distributional information in prompts.
However, these methods face practical limitations: end-users rarely modify
default model parameters, and accurate distributional information is often
unavailable. Building upon cognitive science research demonstrating that
frequency-based representations reduce systematic biases in human statistical
reasoning by making reference classes explicit and reducing cognitive load,
this study investigates whether frequency framed prompting (REFER) can
similarly enhance fairness in LLM opinion summarisation. Through systematic
experimentation with different prompting frameworks, we adapted techniques
known to improve human reasoning to elicit more effective information
processing in language models compared to abstract probabilistic
representations.Our results demonstrate that REFER enhances fairness in
language models when summarising opinions. This effect is particularly
pronounced in larger language models and using stronger reasoning instructions.

</details>


### [37] [Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics](https://arxiv.org/abs/2509.15739)
*Reza Sanayei,Srdjan Vesic,Eduardo Blanco,Mihai Surdeanu*

Main category: cs.CL

TL;DR: 大型语言模型擅长线性推理任务，但在非线性结构（如自然辩论）中的应用仍有待探索。本研究评估了大型语言模型是否能逼近计算论证理论（CAT）中的结构化推理。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在处理自然辩论等非线性结构化推理任务中的能力，这些任务可以最好地表达为论证图。

Method: 使用定量论证辩论（QuAD）语义，根据攻击和支持关系为论点分配可接受性得分。在仅提供来自两个NoDE数据集的对话格式辩论的情况下，提示模型对论点进行排序，而无需访问底层图。

Result: 模型与QuAD排名显示出适度的对齐，但性能会随着输入变长或语篇流程中断而下降。高级提示有助于减轻这些影响，减少与论点长度和位置相关的偏差。

Conclusion: 研究结果强调了大型语言模型在建模形式论证语义方面的潜力和局限性，并激发了未来对图感知推理的研究。

Abstract: Large Language Models (LLMs) excel at linear reasoning tasks but remain
underexplored on non-linear structures such as those found in natural debates,
which are best expressed as argument graphs. We evaluate whether LLMs can
approximate structured reasoning from Computational Argumentation Theory (CAT).
Specifically, we use Quantitative Argumentation Debate (QuAD) semantics, which
assigns acceptability scores to arguments based on their attack and support
relations. Given only dialogue-formatted debates from two NoDE datasets, models
are prompted to rank arguments without access to the underlying graph. We test
several LLMs under advanced instruction strategies, including Chain-of-Thought
and In-Context Learning. While models show moderate alignment with QuAD
rankings, performance degrades with longer inputs or disrupted discourse flow.
Advanced prompting helps mitigate these effects by reducing biases related to
argument length and position. Our findings highlight both the promise and
limitations of LLMs in modeling formal argumentation semantics and motivate
future work on graph-aware reasoning.

</details>


### [38] [UniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression](https://arxiv.org/abs/2509.15763)
*Chenlong Deng,Zhisong Zhang,Kelong Mao,Shuaiyi Li,Tianqing Fang,Hongming Zhang,Haitao Mi,Dong Yu,Zhicheng Dou*

Main category: cs.CL

TL;DR: 本文提出了一种名为 UniGist 的序列级长文本压缩框架，旨在解决大型语言模型处理长文本输入时 KV 缓存的内存开销问题。


<details>
  <summary>Details</summary>
Motivation: 现有序列级压缩方法会丢失重要的上下文信息。

Method: 该方法通过使用特殊的压缩 tokens (gists) 替换原始 tokens，并采用无块训练策略和带有 gist 移位技巧的高效内核来实现优化 GPU 训练。

Result: UniGist 显著提高了压缩质量，在细节回忆任务和长程依赖建模方面表现出色。

Conclusion: UniGist 是一种有效的长文本压缩框架，可以在保证性能的同时节省内存。

Abstract: Large language models are increasingly capable of handling long-context
inputs, but the memory overhead of key-value (KV) cache remains a major
bottleneck for general-purpose deployment. While various compression strategies
have been explored, sequence-level compression, which drops the full KV caches
for certain tokens, is particularly challenging as it can lead to the loss of
important contextual information. To address this, we introduce UniGist, a
sequence-level long-context compression framework that efficiently preserves
context information by replacing raw tokens with special compression tokens
(gists) in a fine-grained manner. We adopt a chunk-free training strategy and
design an efficient kernel with a gist shift trick, enabling optimized GPU
training. Our scheme also supports flexible inference by allowing the actual
removal of compressed tokens, resulting in real-time memory savings.
Experiments across multiple long-context tasks demonstrate that UniGist
significantly improves compression quality, with especially strong performance
in detail-recalling tasks and long-range dependency modeling.

</details>


### [39] [UPRPRC: Unified Pipeline for Reproducing Parallel Resources -- Corpus from the United Nations](https://arxiv.org/abs/2509.15789)
*Qiuyang Lu,Fangjian Shen,Zhengkai Tang,Qiang Liu,Hexuan Cheng,Hui Liu,Wushao Wen*

Main category: cs.CL

TL;DR: 本文提出了一个完整的端到端解决方案，用于构建大规模、可复现的多语料数据集。


<details>
  <summary>Details</summary>
Motivation: 先前联合国文件的语料库存在过程不透明、难以重现和规模有限等问题。

Method: 该方法包括网络抓取数据到文本对齐，并提出了一种新的图辅助段落对齐（GAPA）算法。

Result: 生成的语料库包含超过7.13亿个英语token，比以前的工作规模扩大了一倍以上。

Conclusion: 构建了一个完全由人工翻译的、非人工智能生成内容组成的最大公开并行语料库。

Abstract: The quality and accessibility of multilingual datasets are crucial for
advancing machine translation. However, previous corpora built from United
Nations documents have suffered from issues such as opaque process, difficulty
of reproduction, and limited scale. To address these challenges, we introduce a
complete end-to-end solution, from data acquisition via web scraping to text
alignment. The entire process is fully reproducible, with a minimalist
single-machine example and optional distributed computing steps for
scalability. At its core, we propose a new Graph-Aided Paragraph Alignment
(GAPA) algorithm for efficient and flexible paragraph-level alignment. The
resulting corpus contains over 713 million English tokens, more than doubling
the scale of prior work. To the best of our knowledge, this represents the
largest publicly available parallel corpus composed entirely of
human-translated, non-AI-generated content. Our code and corpus are accessible
under the MIT License.

</details>


### [40] [RAVE: Retrieval and Scoring Aware Verifiable Claim Detection](https://arxiv.org/abs/2509.15793)
*Yufeng Li,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: 这篇论文介绍了一种名为RAVE的框架，用于检测社交媒体上的可验证声明。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上虚假信息的快速传播需要可扩展的事实核查工具，而声明检测是关键步骤。

Method: 该框架结合了证据检索与相关性和来源可信度的结构化信号。

Result: 在CT22-test和PoliClaim-test上的实验表明，RAVE在准确性和F1方面始终优于纯文本和基于检索的基线方法。

Conclusion: RAVE框架有效地提高了可验证声明检测的性能。

Abstract: The rapid spread of misinformation on social media underscores the need for
scalable fact-checking tools. A key step is claim detection, which identifies
statements that can be objectively verified. Prior approaches often rely on
linguistic cues or claim check-worthiness, but these struggle with vague
political discourse and diverse formats such as tweets. We present RAVE
(Retrieval and Scoring Aware Verifiable Claim Detection), a framework that
combines evidence retrieval with structured signals of relevance and source
credibility. Experiments on CT22-test and PoliClaim-test show that RAVE
consistently outperforms text-only and retrieval-based baselines in both
accuracy and F1.

</details>


### [41] [Best-of-L: Cross-Lingual Reward Modeling for Mathematical Reasoning](https://arxiv.org/abs/2509.15811)
*Sara Rajaee,Rochelle Choenni,Ekaterina Shutova,Christof Monz*

Main category: cs.CL

TL;DR: 大型语言模型(llm)的推理能力不断提高，但这种能力在多语言llm中的不同语言之间有何不同，以及不同的语言是否产生相互补充的推理路径，目前尚不清楚。为了 исследовать 这个问题, 我们训练了一个奖励模型来对给定问题的跨语言生成响应进行排序。结果表明，与在单一语言中使用奖励模型相比，我们的跨语言奖励模型大大提高了数学推理性能，甚至使高资源语言也受益。虽然英语在多语言模型中通常表现出最高的性能，但我们发现在低采样预算下，跨语言采样尤其有利于英语。我们的研究结果揭示了通过利用不同语言的互补优势来提高多语言推理的新机会。


<details>
  <summary>Details</summary>
Motivation: 研究多语言LLM中不同语言的推理能力差异，以及不同语言的推理路径是否互补。

Method: 训练一个奖励模型来对给定问题的跨语言生成响应进行排序。

Result: 跨语言奖励模型显著提高了数学推理性能，尤其是在低采样预算下对英语有益。

Conclusion: 通过利用不同语言的互补优势，可以提高多语言推理能力。

Abstract: While the reasoning abilities of large language models (LLMs) continue to
advance, it remains unclear how such ability varies across languages in
multilingual LLMs and whether different languages produce reasoning paths that
complement each other. To investigate this question, we train a reward model to
rank generated responses for a given question across languages. Our results
show that our cross-lingual reward model substantially improves mathematical
reasoning performance compared to using reward modeling within a single
language, benefiting even high-resource languages. While English often exhibits
the highest performance in multilingual models, we find that cross-lingual
sampling particularly benefits English under low sampling budgets. Our findings
reveal new opportunities to improve multilingual reasoning by leveraging the
complementary strengths of diverse languages.

</details>


### [42] [The Curious Case of Visual Grounding: Different Effects for Speech- and Text-based Language Encoders](https://arxiv.org/abs/2509.15837)
*Adrian Sauter,Willem Zuidema,Marianne de Heer Kloots*

Main category: cs.CL

TL;DR: 研究视觉信息如何影响音频和文本深度学习模型中的语言处理。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉基础如何影响模型内部的单词表示。

Method: 通过全局表示比较和聚类分析来研究语音和文本编码器。

Result: 视觉基础增强了口语和书面语言表示之间的一致性，但主要由增强的单词身份编码驱动。语音表示仍然以语音为主导，视觉基础不会改善语义区分。

Conclusion: 研究结果可以为开发更有效的方法以利用视觉信息丰富语音模型中的语义提供信息。

Abstract: How does visual information included in training affect language processing
in audio- and text-based deep learning models? We explore how such visual
grounding affects model-internal representations of words, and find
substantially different effects in speech- vs. text-based language encoders.
Firstly, global representational comparisons reveal that visual grounding
increases alignment between representations of spoken and written language, but
this effect seems mainly driven by enhanced encoding of word identity rather
than meaning. We then apply targeted clustering analyses to probe for phonetic
vs. semantic discriminability in model representations. Speech-based
representations remain phonetically dominated with visual grounding, but in
contrast to text-based representations, visual grounding does not improve
semantic discriminability. Our findings could usefully inform the development
of more efficient methods to enrich speech-based models with visually-informed
semantics.

</details>


### [43] [Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning on Chinese Multi-Subject Physics Problems](https://arxiv.org/abs/2509.15839)
*Zhongze Luo,Zhenshuai Yin,Yongxin Guo,Zhichao Wang,Jionghao Zhu,Xiaoying Tang*

Main category: cs.CL

TL;DR: 提出了一个用于评估多模态LLM在中文物理推理方面能力的综合基准Multi-Physics，包含1412个图像相关的多项选择题，涵盖11个高中物理科目。


<details>
  <summary>Details</summary>
Motivation: 现有的评估基准在精细的主题覆盖范围、逐步推理过程以及对视觉信息的系统评估方面存在不足，尤其是在物理等专业科学领域。

Method: 构建了一个包含5个难度级别的Multi-Physics基准，并采用双重评估框架来评估20个不同的MLLM，分析最终答案的准确性和逐步推理的完整性。

Result: 通过比较改变输入模式前后模型的性能，系统地研究了难度级别和视觉信息的影响。

Conclusion: 提供了一个精细的资源，并为剖析最先进的MLLM的多模态推理过程提供了一个强大的方法。数据集和代码已开源。

Abstract: While multimodal LLMs (MLLMs) demonstrate remarkable reasoning progress,
their application in specialized scientific domains like physics reveals
significant gaps in current evaluation benchmarks. Specifically, existing
benchmarks often lack fine-grained subject coverage, neglect the step-by-step
reasoning process, and are predominantly English-centric, failing to
systematically evaluate the role of visual information. Therefore, we introduce
\textbf {Multi-Physics} for Chinese physics reasoning, a comprehensive
benchmark that includes 5 difficulty levels, featuring 1,412 image-associated,
multiple-choice questions spanning 11 high-school physics subjects. We employ a
dual evaluation framework to evaluate 20 different MLLMs, analyzing both final
answer accuracy and the step-by-step integrity of their chain-of-thought.
Furthermore, we systematically study the impact of difficulty level and visual
information by comparing the model performance before and after changing the
input mode. Our work provides not only a fine-grained resource for the
community but also offers a robust methodology for dissecting the multimodal
reasoning process of state-of-the-art MLLMs, and our dataset and code have been
open-sourced: https://github.com/luozhongze/Multi-Physics.

</details>


### [44] [Distribution-Aligned Decoding for Efficient LLM Task Adaptation](https://arxiv.org/abs/2509.15888)
*Senkang Hu,Xudong Han,Jinqi Jiang,Yihang Tao,Zihan Fang,Sam Tak Wu Kwong,Yuguang Fang*

Main category: cs.CL

TL;DR: 提出了一种轻量级的、与 PEFT 兼容的、有理论基础的方法，称为 Steering Vector Decoding (SVD)，用于任务适应。


<details>
  <summary>Details</summary>
Motivation: 即使使用参数高效的微调 (PEFT)，将数十亿参数的语言模型适应到下游任务仍然代价高昂。

Method: 从 warm-started 和预训练模型的输出分布之间的 Kullback-Leibler (KL) 散度梯度中提取任务感知 steering vector。然后，该 steering vector 用于引导解码过程，以引导模型的输出分布朝向任务分布。

Result: 在三个任务和九个基准测试中，SVD 与四种标准 PEFT 方法相结合，可将多项选择准确率提高多达 5 个点，并将开放式真实性提高 2 个点，在常识数据集上也有类似的增益（1-2 个点），而无需在 PEFT 适配器之外添加可训练参数。

Conclusion: SVD 为大型语言模型提供了更强大的任务适应的轻量级、有理论基础的路径。

Abstract: Adapting billion-parameter language models to a downstream task is still
costly, even with parameter-efficient fine-tuning (PEFT). We re-cast task
adaptation as output-distribution alignment: the objective is to steer the
output distribution toward the task distribution directly during decoding
rather than indirectly through weight updates. Building on this view, we
introduce Steering Vector Decoding (SVD), a lightweight, PEFT-compatible, and
theoretically grounded method. We start with a short warm-start fine-tune and
extract a task-aware steering vector from the Kullback-Leibler (KL) divergence
gradient between the output distribution of the warm-started and pre-trained
models. This steering vector is then used to guide the decoding process to
steer the model's output distribution towards the task distribution. We
theoretically prove that SVD is first-order equivalent to the gradient step of
full fine-tuning and derive a globally optimal solution for the strength of the
steering vector. Across three tasks and nine benchmarks, SVD paired with four
standard PEFT methods improves multiple-choice accuracy by up to 5 points and
open-ended truthfulness by 2 points, with similar gains (1-2 points) on
commonsense datasets without adding trainable parameters beyond the PEFT
adapter. SVD thus offers a lightweight, theoretically grounded path to stronger
task adaptation for large language models.

</details>


### [45] [The Psychology of Falsehood: A Human-Centric Survey of Misinformation Detection](https://arxiv.org/abs/2509.15896)
*Arghodeep Nandi,Megha Sundriyal,Euna Mehnaz Khan,Jikai Sun,Emily Vraga,Jaideep Srivastava,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文探讨了当前错误信息检测系统仅关注事实准确性的局限性，并强调了人类心理在错误信息传播中的作用，提出了未来研究方向，旨在创建更强大和适应性更强的框架。


<details>
  <summary>Details</summary>
Motivation: 探讨了当前错误信息检测系统仅关注事实准确性的局限性，强调需要采用更以人为中心的检测框架，因为错误信息的有害影响超越了简单的事实错误，它利用了个人感知、解释和情感反应信息的方式。

Method: 通过分析最先进的错误信息检测系统，结合认知偏差、社会动态和情感反应等心理学概念，揭示了当前方法的局限性，并确定了改进的机会。

Result: 揭示了当前方法的关键局限性，并确定了改进的机会。

Conclusion: 未来的研究方向应侧重于创建更强大和适应性更强的框架，例如将技术因素与人类认知和社会影响的复杂性相结合的神经行为模型，从而更有效地检测和减轻错误信息的社会危害。

Abstract: Misinformation remains one of the most significant issues in the digital age.
While automated fact-checking has emerged as a viable solution, most current
systems are limited to evaluating factual accuracy. However, the detrimental
effect of misinformation transcends simple falsehoods; it takes advantage of
how individuals perceive, interpret, and emotionally react to information. This
underscores the need to move beyond factuality and adopt more human-centered
detection frameworks. In this survey, we explore the evolving interplay between
traditional fact-checking approaches and psychological concepts such as
cognitive biases, social dynamics, and emotional responses. By analyzing
state-of-the-art misinformation detection systems through the lens of human
psychology and behavior, we reveal critical limitations of current methods and
identify opportunities for improvement. Additionally, we outline future
research directions aimed at creating more robust and adaptive frameworks, such
as neuro-behavioural models that integrate technological factors with the
complexities of human cognition and social influence. These approaches offer
promising pathways to more effectively detect and mitigate the societal harms
of misinformation.

</details>


### [46] [Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and Personalization via Questions](https://arxiv.org/abs/2509.15901)
*Frederic Kirstein,Sonu Kumar,Terry Ruas,Bela Gipp*

Main category: cs.CL

TL;DR: 提出了一种模块化pipeline框架FRAME，将摘要生成转化为语义丰富任务，并通过SCOPE协议实现个性化摘要。


<details>
  <summary>Details</summary>
Motivation: 现有LLM会议摘要生成容易出错，产生幻觉、遗漏和不相关内容。

Method: FRAME框架提取和评估关键事实，按主题组织，并用它们来丰富摘要大纲。SCOPE协议通过回答九个问题来构建推理过程，以实现个性化摘要。

Result: 在QMSum和FAME数据集上，FRAME减少了幻觉和遗漏，SCOPE提高了知识拟合和目标对齐。

Conclusion: 研究结果提倡重新思考摘要生成方法，以提高控制、忠实性和个性化。

Abstract: Meeting summarization with large language models (LLMs) remains error-prone,
often producing outputs with hallucinations, omissions, and irrelevancies. We
present FRAME, a modular pipeline that reframes summarization as a semantic
enrichment task. FRAME extracts and scores salient facts, organizes them
thematically, and uses these to enrich an outline into an abstractive summary.
To personalize summaries, we introduce SCOPE, a reason-out-loud protocol that
has the model build a reasoning trace by answering nine questions before
content selection. For evaluation, we propose P-MESA, a multi-dimensional,
reference-free evaluation framework to assess if a summary fits a target
reader. P-MESA reliably identifies error instances, achieving >= 89% balanced
accuracy against human annotations and strongly aligns with human severity
ratings (r >= 0.70). On QMSum and FAME, FRAME reduces hallucination and
omission by 2 out of 5 points (measured with MESA), while SCOPE improves
knowledge fit and goal alignment over prompt-only baselines. Our findings
advocate for rethinking summarization to improve control, faithfulness, and
personalization.

</details>


### [47] [Beyond the Score: Uncertainty-Calibrated LLMs for Automated Essay Assessment](https://arxiv.org/abs/2509.15926)
*Ahmed Karim,Qiao Wang,Zheng Yuan*

Main category: cs.CL

TL;DR: 本文提出了一种使用conformal prediction的方法，为自动评分系统提供置信度和解释。


<details>
  <summary>Details</summary>
Motivation: 当前自动评分系统缺乏置信度评估和解释，限制了实际应用，特别是在高风险考试中。

Method: 使用conformal prediction，对两个开源大型语言模型(Llama-3 8B 和 Qwen-2.5 3B)在三个不同的语料库(ASAP, TOEFL11, Cambridge-FCE)上进行微调，并在90%的风险水平下进行校准。

Result: 校准后的模型在满足覆盖目标的同时，保持预测集的紧凑性。

Conclusion: 中等规模的开源LLM已经可以支持教师参与的自动评分系统。

Abstract: Automated Essay Scoring (AES) systems now reach near human agreement on some
public benchmarks, yet real-world adoption, especially in high-stakes
examinations, remains limited. A principal obstacle is that most models output
a single score without any accompanying measure of confidence or explanation.
We address this gap with conformal prediction, a distribution-free wrapper that
equips any classifier with set-valued outputs and formal coverage guarantees.
Two open-source large language models (Llama-3 8B and Qwen-2.5 3B) are
fine-tuned on three diverse corpora (ASAP, TOEFL11, Cambridge-FCE) and
calibrated at a 90 percent risk level. Reliability is assessed with UAcc, an
uncertainty-aware accuracy that rewards models for being both correct and
concise. To our knowledge, this is the first work to combine conformal
prediction and UAcc for essay scoring. The calibrated models consistently meet
the coverage target while keeping prediction sets compact, indicating that
open-source, mid-sized LLMs can already support teacher-in-the-loop AES; we
discuss scaling and broader user studies as future work.

</details>


### [48] [Localmax dynamics for attention in transformers and its asymptotic behavior](https://arxiv.org/abs/2509.15958)
*Henri Cimetière,Maria Teresa Chiri,Bahman Gharesifard*

Main category: cs.CL

TL;DR: 提出了一种新的离散时间注意力模型，称为 localmax 动态，它在经典 softmax 动态和 hardmax 动态之间进行插值。


<details>
  <summary>Details</summary>
Motivation: 研究soft模型和hard模型的结合。

Method: 通过引入邻域交互的对齐敏感度参数，允许受控地偏离纯 hardmax 行为。

Result: 证明了 token 状态的凸包仍然收敛到凸多面体，但其结构不能再完全由最大对齐集来描述，促使引入静止集来捕获顶点附近 token 的不变行为。即使在时变对齐敏感度参数下，这些集合在理解系统的渐近行为中起着关键作用。localmax 动态不表现出有限时间收敛，并为消失的、非零的时变对齐敏感度参数提供结果，恢复了 hardmax 的极限行为。

Conclusion: 总结了经典意见动态中基于 Lyapunov 的方法的局限性，并概述了未来研究的方向。

Abstract: We introduce a new discrete-time attention model, termed the localmax
dynamics, which interpolates between the classic softmax dynamics and the
hardmax dynamics, where only the tokens that maximize the influence toward a
given token have a positive weight. As in hardmax, uniform weights are
determined by a parameter controlling neighbor influence, but the key extension
lies in relaxing neighborhood interactions through an alignment-sensitivity
parameter, which allows controlled deviations from pure hardmax behavior. As we
prove, while the convex hull of the token states still converges to a convex
polytope, its structure can no longer be fully described by a maximal alignment
set, prompting the introduction of quiescent sets to capture the invariant
behavior of tokens near vertices. We show that these sets play a key role in
understanding the asymptotic behavior of the system, even under time-varying
alignment sensitivity parameters. We further show that localmax dynamics does
not exhibit finite-time convergence and provide results for vanishing, nonzero,
time-varying alignment-sensitivity parameters, recovering the limiting behavior
of hardmax as a by-product. Finally, we adapt Lyapunov-based methods from
classical opinion dynamics, highlighting their limitations in the asymmetric
setting of localmax interactions and outlining directions for future research.

</details>


### [49] [BEFT: Bias-Efficient Fine-Tuning of Language Models](https://arxiv.org/abs/2509.15974)
*Baichuan Huang,Ananth Balashankar,Amir Aminifar*

Main category: cs.CL

TL;DR: 本文提出了一种选择要微调的偏差项的方法，以提高偏差高效微调 (BEFT) 的效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在选择用于有效微调的特定偏差项方面提供的指导有限，因此需要研究不同偏差项微调与下游性能之间的联系。

Method: 提出了一种选择要微调的偏差项的方法，从而构成了偏差高效微调 (BEFT) 的基础。

Result: 在各种大型语言模型 (LLM) 的广泛评估表明，该偏差高效方法在各种下游任务（包括分类、多项选择和生成任务）中都有效且优于其他偏差选择方法。

Conclusion: 本文提出的偏差高效微调方法在各种下游任务中都有效且优于其他偏差选择方法。

Abstract: Fine-tuning all-bias-terms stands out among various parameter-efficient
fine-tuning (PEFT) techniques, owing to its out-of-the-box usability and
competitive performance, especially in low-data regimes. Bias-only fine-tuning
has the potential for unprecedented parameter efficiency. However, the link
between fine-tuning different bias terms (i.e., bias terms in the query, key,
or value projections) and downstream performance remains unclear. The existing
approaches, e.g., based on the magnitude of bias change or empirical Fisher
information, provide limited guidance for selecting the particular bias term
for effective fine-tuning. In this paper, we propose an approach for selecting
the bias term to be fine-tuned, forming the foundation of our bias-efficient
fine-tuning (BEFT). We extensively evaluate our bias-efficient approach against
other bias-selection approaches, across a wide range of large language models
(LLMs) spanning encoder-only and decoder-only architectures from 110M to 6.7B
parameters. Our results demonstrate the effectiveness and superiority of our
bias-efficient approach on diverse downstream tasks, including classification,
multiple-choice, and generation tasks.

</details>


### [50] [Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning](https://arxiv.org/abs/2509.16025)
*Hong-Yun Lin,Jhen-Ke Lin,Chung-Chun Wang,Hao-Chien Lu,Berlin Chen*

Main category: cs.CL

TL;DR: 提出了一种新的多模态基础模型方法，用于会话级别的口语评估，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有口语评估方法依赖于容易出错的流水线或短音频窗口，忽略了语篇层面的证据。针对L2英语学习者对可靠口语评估的需求。

Method: 采用多目标学习和基于Whisper ASR模型的语音先验，进行声学感知校准，从而联合学习整体和特征层面的口语评估目标，无需手工设计的特征。

Result: 在Speak & Improve基准测试中，该方法优于之前的最佳级联系统，并表现出强大的跨部分泛化能力。

Conclusion: 该模型能够连贯地处理L2学习者的整个回答过程，擅长预测整体口语能力，可作为CALL应用中的紧凑型可部署评分器。

Abstract: Spoken Language Assessment (SLA) estimates a learner's oral proficiency from
spontaneous speech. The growing population of L2 English speakers has
intensified the demand for reliable SLA, a critical component of Computer
Assisted Language Learning (CALL). Existing efforts often rely on cascaded
pipelines, which are prone to error propagation, or end-to-end models that
often operate on a short audio window, which might miss discourse-level
evidence. This paper introduces a novel multimodal foundation model approach
that performs session-level evaluation in a single pass. Our approach couples
multi-target learning with a frozen, Whisper ASR model-based speech prior for
acoustic-aware calibration, allowing for jointly learning holistic and
trait-level objectives of SLA without resorting to handcrafted features. By
coherently processing the entire response session of an L2 speaker, the model
excels at predicting holistic oral proficiency. Experiments conducted on the
Speak & Improve benchmark demonstrate that our proposed approach outperforms
the previous state-of-the-art cascaded system and exhibits robust cross-part
generalization, producing a compact deployable grader that is tailored for CALL
applications.

</details>


### [51] [Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech](https://arxiv.org/abs/2509.16028)
*Sang Hoon Woo,Sehun Lee,Kang-wook Kim,Gunhee Kim*

Main category: cs.CL

TL;DR: 提出了一种名为 Think-Verbalize-Speak (TVS) 的框架，该框架将推理与口语表达分离，以保持 LLM 的完整推理能力。


<details>
  <summary>Details</summary>
Motivation: 直接在口语交流中使用大型语言模型 (LLM) 通常会产生欠佳的结果，因为最佳文本和口头表达之间存在不匹配。现有方法调整 LLM 以产生语音友好的输出，但它们对推理性能的影响仍未得到充分探索。

Method: 该方法的核心是verbalizing，这是一个中间步骤，可将想法转换为自然的、可用于语音的文本。还引入了 ReVerT，这是一种基于增量和异步总结的、延迟效率高的verbalizer。

Result: 实验表明，该方法在对推理影响最小的情况下，增强了语音的自然性和简洁性。

Conclusion: 该方法 (Think-Verbalize-Speak) 通过将推理与口语表达分离，有效提升了口语对话系统中大型语言模型的性能，同时保持了推理能力。

Abstract: Spoken dialogue systems increasingly employ large language models (LLMs) to
leverage their advanced reasoning capabilities. However, direct application of
LLMs in spoken communication often yield suboptimal results due to mismatches
between optimal textual and verbal delivery. While existing approaches adapt
LLMs to produce speech-friendly outputs, their impact on reasoning performance
remains underexplored. In this work, we propose Think-Verbalize-Speak, a
framework that decouples reasoning from spoken delivery to preserve the full
reasoning capacity of LLMs. Central to our method is verbalizing, an
intermediate step that translates thoughts into natural, speech-ready text. We
also introduce ReVerT, a latency-efficient verbalizer based on incremental and
asynchronous summarization. Experiments across multiple benchmarks show that
our method enhances speech naturalness and conciseness with minimal impact on
reasoning. The project page with the dataset and the source code is available
at https://yhytoto12.github.io/TVS-ReVerT

</details>


### [52] [Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM Responses](https://arxiv.org/abs/2509.16093)
*Fangyi Yu,Nabeel Seedat,Dasha Herrmannova,Frank Schilder,Jonathan Richard Schwarz*

Main category: cs.CL

TL;DR: 提出了一种分解的LLM评估框架DeCE，用于评估法律或医学等高风险领域中的长篇答案。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM评估器通常将答案质量的细微差别降低为单一的无差别分数，而标准指标如BLEU和ROUGE无法捕捉语义正确性。

Method: DeCE将精确度（事实准确性和相关性）和召回率（所需概念的覆盖率）分开，使用从黄金答案要求中自动提取的特定实例标准。

Result: DeCE在涉及多司法管辖区推理和引文基础的真实法律QA任务中，与专家判断的相关性显著增强（r=0.78），而传统指标（r=0.12），逐点LLM评分（r=0.35）和现代多维评估器（r=0.48）。

Conclusion: DeCE提供了一种在专家领域中可解释且可操作的LLM评估框架。

Abstract: Evaluating long-form answers in high-stakes domains such as law or medicine
remains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to
capture semantic correctness, and current LLM-based evaluators often reduce
nuanced aspects of answer quality into a single undifferentiated score. We
introduce DeCE, a decomposed LLM evaluation framework that separates precision
(factual accuracy and relevance) and recall (coverage of required concepts),
using instance-specific criteria automatically extracted from gold answer
requirements. DeCE is model-agnostic and domain-general, requiring no
predefined taxonomies or handcrafted rubrics. We instantiate DeCE to evaluate
different LLMs on a real-world legal QA task involving multi-jurisdictional
reasoning and citation grounding. DeCE achieves substantially stronger
correlation with expert judgments ($r=0.78$), compared to traditional metrics
($r=0.12$), pointwise LLM scoring ($r=0.35$), and modern multidimensional
evaluators ($r=0.48$). It also reveals interpretable trade-offs: generalist
models favor recall, while specialized models favor precision. Importantly,
only 11.95% of LLM-generated criteria required expert revision, underscoring
DeCE's scalability. DeCE offers an interpretable and actionable LLM evaluation
framework in expert domains.

</details>


### [53] [DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning](https://arxiv.org/abs/2509.16105)
*Sikai Bai,Haoxi Li,Jie Zhang,Zicong Hong,Song Guo*

Main category: cs.CL

TL;DR: 提出了一种新的非均匀剪枝策略，名为DiEP，用于解决MoE模型扩展带来的内存和存储挑战。


<details>
  <summary>Details</summary>
Motivation: 现有 MoE 剪枝方法在所有层上采用均匀稀疏性，导致由于不同 MoE 层中专家冗余度不同而导致次优结果和性能下降。

Method: 通过将全局离散搜索空间转换为连续空间，我们的方法能够处理指数增长的非均匀专家组合，从而实现基于梯度的自适应剪枝。

Result: 在五个先进的 MoE 模型上进行的大量实验证明了我们方法的有效性。值得注意的是，DiEP 在 Mixtral 8×7B 上仅保留一半的专家，同时保留了约 92% 的原始性能，在具有挑战性的 MMLU 数据集上优于其他剪枝方法高达 7.1%。

Conclusion: DiEP 是一种有效的 MoE 模型剪枝方法，可以在保持性能的同时显著减少模型大小。

Abstract: Despite the significant breakthrough of Mixture-of-Experts (MoE), the
increasing scale of these MoE models presents huge memory and storage
challenges. Existing MoE pruning methods, which involve reducing parameter size
with a uniform sparsity across all layers, often lead to suboptimal outcomes
and performance degradation due to varying expert redundancy in different MoE
layers. To address this, we propose a non-uniform pruning strategy, dubbed
\textbf{Di}fferentiable \textbf{E}xpert \textbf{P}runing (\textbf{DiEP}), which
adaptively adjusts pruning rates at the layer level while jointly learning
inter-layer importance, effectively capturing the varying redundancy across
different MoE layers. By transforming the global discrete search space into a
continuous one, our method handles exponentially growing non-uniform expert
combinations, enabling adaptive gradient-based pruning. Extensive experiments
on five advanced MoE models demonstrate the efficacy of our method across
various NLP tasks. Notably, \textbf{DiEP} retains around 92\% of original
performance on Mixtral 8$\times$7B with only half the experts, outperforming
other pruning methods by up to 7.1\% on the challenging MMLU dataset.

</details>


### [54] [It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge](https://arxiv.org/abs/2509.16107)
*Lukas Ellinger,Georg Groh*

Main category: cs.CL

TL;DR: 大型语言模型(LLMs)在解决对话中指代歧义方面表现不佳，尤其是在简化提示下，但通过直接偏好优化进行微调可以显著改善。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型(LLMs)是否能利用常识来解决多轮对话中的指代歧义，并分析它们在歧义持续存在时的行为，以及简化语言请求如何影响这种能力。

Method: 使用一个新的多语言评估数据集，通过LLM-as-Judge和人工标注测试了DeepSeek v3、GPT-4o、Qwen3-32B、GPT-4o-mini和Llama-3.1-8B。

Result: 当前的LLMs难以有效解决歧义，倾向于选择单一解释或涵盖所有可能的指代，而不是回避或寻求澄清。简化提示会降低常识推理和多样化响应策略的使用，使这种限制更加明显。

Conclusion: 需要高级微调来提高LLMs处理歧义的能力，并确保在不同的沟通风格中保持稳健的性能。

Abstract: Ambiguous words or underspecified references require interlocutors to resolve
them, often by relying on shared context and commonsense knowledge. Therefore,
we systematically investigate whether Large Language Models (LLMs) can leverage
commonsense to resolve referential ambiguity in multi-turn conversations and
analyze their behavior when ambiguity persists. Further, we study how requests
for simplified language affect this capacity. Using a novel multilingual
evaluation dataset, we test DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, and
Llama-3.1-8B via LLM-as-Judge and human annotations. Our findings indicate that
current LLMs struggle to resolve ambiguity effectively: they tend to commit to
a single interpretation or cover all possible references, rather than hedging
or seeking clarification. This limitation becomes more pronounced under
simplification prompts, which drastically reduce the use of commonsense
reasoning and diverse response strategies. Fine-tuning Llama-3.1-8B with Direct
Preference Optimization substantially improves ambiguity resolution across all
request types. These results underscore the need for advanced fine-tuning to
improve LLMs' handling of ambiguity and to ensure robust performance across
diverse communication styles.

</details>


### [55] [CultureScope: A Dimensional Lens for Probing Cultural Understanding in LLMs](https://arxiv.org/abs/2509.16188)
*Jinghao Zhang,Sihang Jiang,Shiwei Guo,Shisong Chen,Yanghua Xiao,Hongwei Feng,Jiaqing Liang,Minggui HE,Shimin Tao,Hongxia Ma*

Main category: cs.CL

TL;DR: 本文介绍了CultureScope，一个用于评估大型语言模型(llm)中文化理解能力的综合性评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏全面性，并且难以在不同的文化背景下进行扩展和调整，因为它们的框架通常缺乏来自完善的文化理论的指导，并且倾向于依赖专家驱动的手动注释。

Method: 受文化冰山理论的启发，我们设计了一种新颖的文化知识分类维度模式，该模式包含3层和140个维度，可指导自动构建特定于文化的知识库以及针对任何给定语言和文化的相应评估数据集。

Result: 实验结果表明，我们的方法可以有效地评估文化理解能力。他们还揭示了现有的大型语言模型缺乏全面的文化能力，并且仅仅合并多语言数据并不一定能提高文化理解能力。

Conclusion: 本文提出了CultureScope，一种用于评估llm中文化理解能力的综合性评估框架。

Abstract: As large language models (LLMs) are increasingly deployed in diverse cultural
environments, evaluating their cultural understanding capability has become
essential for ensuring trustworthy and culturally aligned applications.
However, most existing benchmarks lack comprehensiveness and are challenging to
scale and adapt across different cultural contexts, because their frameworks
often lack guidance from well-established cultural theories and tend to rely on
expert-driven manual annotations. To address these issues, we propose
CultureScope, the most comprehensive evaluation framework to date for assessing
cultural understanding in LLMs. Inspired by the cultural iceberg theory, we
design a novel dimensional schema for cultural knowledge classification,
comprising 3 layers and 140 dimensions, which guides the automated construction
of culture-specific knowledge bases and corresponding evaluation datasets for
any given languages and cultures. Experimental results demonstrate that our
method can effectively evaluate cultural understanding. They also reveal that
existing large language models lack comprehensive cultural competence, and
merely incorporating multilingual data does not necessarily enhance cultural
understanding. All code and data files are available at
https://github.com/HoganZinger/Culture

</details>


### [56] [RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation](https://arxiv.org/abs/2509.16198)
*Jane Luo,Xin Zhang,Steven Liu,Jie Wu,Yiming Huang,Yangyu Huang,Chengyu Yin,Ying Xin,Jianfeng Liu,Yuefeng Zhan,Hao Sun,Qi Chen,Scarlett Li,Mao Yang*

Main category: cs.CL

TL;DR: 本研究提出了一种名为“仓库规划图 (RPG)”的持久表示方法，以解决大型语言模型从头开始生成完整仓库的难题。RPG通过统一提案和实现级别的规划，使用图结构对仓库的能力、文件结构、数据流和函数进行编码，从而取代了自然语言的模糊性，实现长程规划和可扩展的仓库生成。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然擅长函数和文件级别的代码生成，但从头开始生成完整的仓库仍然是一个根本性的挑战。自然语言由于其模糊性和冗长性，不适合忠实地表示复杂的软件结构。

Method: 本研究构建在RPG的基础上，开发了一个名为ZeroRepo的图驱动框架，用于从头开始生成仓库。该框架包括三个阶段：提案级别规划和实现级别细化以构建图，然后是图引导的代码生成以及测试验证。

Result: 在RepoCraft基准测试中，ZeroRepo生成的仓库平均接近36K LOC，大约是Claude Code的3.9倍，其他基线的64倍。它达到了81.5%的功能覆盖率和69.7%的通过率，分别超过Claude Code 27.3和35.8个百分点。

Conclusion: RPG能够对复杂的依赖关系进行建模，通过近线性缩放实现逐步复杂的规划，并增强LLM对仓库的理解，从而加速代理定位。

Abstract: Large language models excel at function- and file-level code generation, yet
generating complete repositories from scratch remains a fundamental challenge.
This process demands coherent and reliable planning across proposal- and
implementation-level stages, while natural language, due to its ambiguity and
verbosity, is ill-suited for faithfully representing complex software
structures. To address this, we introduce the Repository Planning Graph (RPG),
a persistent representation that unifies proposal- and implementation-level
planning by encoding capabilities, file structures, data flows, and functions
in one graph. RPG replaces ambiguous natural language with an explicit
blueprint, enabling long-horizon planning and scalable repository generation.
Building on RPG, we develop ZeroRepo, a graph-driven framework for repository
generation from scratch. It operates in three stages: proposal-level planning
and implementation-level refinement to construct the graph, followed by
graph-guided code generation with test validation. To evaluate this setting, we
construct RepoCraft, a benchmark of six real-world projects with 1,052 tasks.
On RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly
3.9$\times$ the strongest baseline (Claude Code) and about 64$\times$ other
baselines. It attains 81.5% functional coverage and a 69.7% pass rate,
exceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further
analysis shows that RPG models complex dependencies, enables progressively more
sophisticated planning through near-linear scaling, and enhances LLM
understanding of repositories, thereby accelerating agent localization.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [57] [Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays](https://arxiv.org/abs/2509.15234)
*Hanbin Ko,Gihun Cho,Inhyeok Baek,Donguk Kim,Joonbeom Koo,Changi Kim,Dongheon Lee,Chang Min Park*

Main category: cs.CV

TL;DR: 本研究提出了一种新的胸部X射线报告领域自适应LLM编码器LLM2VEC4CXR，并将其与视觉骨干网络结合，构建了LLM2CLIP4CXR框架，以提高图像-文本对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言预训练在放射学领域受到临床报告异质性的限制，简单地扩展到大型噪声报告集合可能会降低模型性能。因此，研究旨在探索大型语言模型（LLM）编码器是否能提供鲁棒的临床表征，以适应不同的风格并更好地指导图像-文本对齐。

Method: 该研究提出了LLM2VEC4CXR，一种领域自适应的LLM编码器，用于处理胸部X射线报告，并构建了LLM2CLIP4CXR，一个双塔框架，将该编码器与视觉骨干网络相结合。

Result: LLM2VEC4CXR在临床文本理解方面优于基于BERT的基线模型，能够处理缩写和风格变化，并在报告级别的指标上实现了强大的临床对齐。LLM2CLIP4CXR利用这些嵌入来提高检索准确性和临床导向的评分，并且比之前的医学CLIP变体具有更强的跨数据集泛化能力。

Conclusion: 研究表明，鲁棒性而非规模是有效多模态学习的关键。该研究发布了模型，以支持医学图像-文本表征学习的进一步研究。

Abstract: Vision-language pretraining has advanced image-text alignment, yet progress
in radiology remains constrained by the heterogeneity of clinical reports,
including abbreviations, impression-only notes, and stylistic variability.
Unlike general-domain settings where more data often leads to better
performance, naively scaling to large collections of noisy reports can plateau
or even degrade model learning. We ask whether large language model (LLM)
encoders can provide robust clinical representations that transfer across
diverse styles and better guide image-text alignment. We introduce LLM2VEC4CXR,
a domain-adapted LLM encoder for chest X-ray reports, and LLM2CLIP4CXR, a
dual-tower framework that couples this encoder with a vision backbone.
LLM2VEC4CXR improves clinical text understanding over BERT-based baselines,
handles abbreviations and style variation, and achieves strong clinical
alignment on report-level metrics. LLM2CLIP4CXR leverages these embeddings to
boost retrieval accuracy and clinically oriented scores, with stronger
cross-dataset generalization than prior medical CLIP variants. Trained on 1.6M
CXR studies from public and private sources with heterogeneous and noisy
reports, our models demonstrate that robustness -- not scale alone -- is the
key to effective multimodal learning. We release models to support further
research in medical image-text representation learning.

</details>


### [58] [ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding](https://arxiv.org/abs/2509.15235)
*Jialiang Kang,Han Shu,Wenshuo Li,Yingjie Zhai,Xinghao Chen*

Main category: cs.CV

TL;DR: Vision-Aware Speculative Decoding (ViSpec) achieves substantial speedup in VLM speculative decoding.


<details>
  <summary>Details</summary>
Motivation: Existing speculative decoding methods achieve only modest speedups (<1.5x) in vision-language models (VLMs), while multimodal capabilities become central to large-scale models.

Method: ViSpec employs a lightweight vision adaptor module to compress image tokens and integrates it into the draft model's attention mechanism. It also extracts a global feature vector for each input image and augment all subsequent text tokens with this feature. A specialized training dataset is curated to overcome the scarcity of multimodal datasets.

Result: ViSpec achieves the first substantial speedup in VLM speculative decoding.

Conclusion: Extensive experiments validate ViSpec.

Abstract: Speculative decoding is a widely adopted technique for accelerating inference
in large language models (LLMs), yet its application to vision-language models
(VLMs) remains underexplored, with existing methods achieving only modest
speedups (<1.5x). This gap is increasingly significant as multimodal
capabilities become central to large-scale models. We hypothesize that large
VLMs can effectively filter redundant image information layer by layer without
compromising textual comprehension, whereas smaller draft models struggle to do
so. To address this, we introduce Vision-Aware Speculative Decoding (ViSpec), a
novel framework tailored for VLMs. ViSpec employs a lightweight vision adaptor
module to compress image tokens into a compact representation, which is
seamlessly integrated into the draft model's attention mechanism while
preserving original image positional information. Additionally, we extract a
global feature vector for each input image and augment all subsequent text
tokens with this feature to enhance multimodal coherence. To overcome the
scarcity of multimodal datasets with long assistant responses, we curate a
specialized training dataset by repurposing existing datasets and generating
extended outputs using the target VLM with modified prompts. Our training
strategy mitigates the risk of the draft model exploiting direct access to the
target model's hidden states, which could otherwise lead to shortcut learning
when training solely on target model outputs. Extensive experiments validate
ViSpec, achieving, to our knowledge, the first substantial speedup in VLM
speculative decoding.

</details>


### [59] [M-PACE: Mother Child Framework for Multimodal Compliance](https://arxiv.org/abs/2509.15241)
*Shreyash Verma,Amit Kesari,Vinayak Trivedi,Anupam Purwar,Ratnesh Jamidar*

Main category: cs.CV

TL;DR: This paper introduces M-PACE, a multimodal framework using MLLMs to unify and streamline compliance checks for multi-modal content, reducing costs and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional compliance frameworks are fragmented, inefficient, and hard to scale. MLLMs offer a way to unify these workflows.

Method: The authors propose M-PACE, a framework with a mother-child MLLM setup for single-pass assessment of vision-language inputs. They also introduce a human-annotated benchmark.

Result: M-PACE reduces inference costs by over 31 times, with the most efficient models operating at a fraction of the cost of others with comparable accuracy.

Conclusion: M-PACE automates quality control, reduces reliance on human reviewers, and offers a cost-effective solution for real-time compliance checks in advertising.

Abstract: Ensuring that multi-modal content adheres to brand, legal, or
platform-specific compliance standards is an increasingly complex challenge
across domains. Traditional compliance frameworks typically rely on disjointed,
multi-stage pipelines that integrate separate modules for image classification,
text extraction, audio transcription, hand-crafted checks, and rule-based
merges. This architectural fragmentation increases operational overhead,
hampers scalability, and hinders the ability to adapt to dynamic guidelines
efficiently. With the emergence of Multimodal Large Language Models (MLLMs),
there is growing potential to unify these workflows under a single,
general-purpose framework capable of jointly processing visual and textual
content. In light of this, we propose Multimodal Parameter Agnostic Compliance
Engine (M-PACE), a framework designed for assessing attributes across
vision-language inputs in a single pass. As a representative use case, we apply
M-PACE to advertisement compliance, demonstrating its ability to evaluate over
15 compliance-related attributes. To support structured evaluation, we
introduce a human-annotated benchmark enriched with augmented samples that
simulate challenging real-world conditions, including visual obstructions and
profanity injection. M-PACE employs a mother-child MLLM setup, demonstrating
that a stronger parent MLLM evaluating the outputs of smaller child models can
significantly reduce dependence on human reviewers, thereby automating quality
control. Our analysis reveals that inference costs reduce by over 31 times,
with the most efficient models (Gemini 2.0 Flash as child MLLM selected by
mother MLLM) operating at 0.0005 per image, compared to 0.0159 for Gemini 2.5
Pro with comparable accuracy, highlighting the trade-off between cost and
output quality achieved in real time by M-PACE in real life deployment over
advertising data.

</details>


### [60] [ProFusion: 3D Reconstruction of Protein Complex Structures from Multi-view AFM Images](https://arxiv.org/abs/2509.15242)
*Jaydeep Rade,Md Hasibul Hasan Hasib,Meric Ozturk,Baboucarr Faal,Sheng Yang,Dipali G. Sashital,Vincenzo Venditti,Baoyu Chen,Soumik Sarkar,Adarsh Krishnamurthy,Anwesha Sarkar*

Main category: cs.CV

TL;DR: ProFusion: 使用深度学习和原子力显微镜 (AFM) 的混合框架，用于预测大型蛋白质复合物 (PC) 的结构。


<details>
  <summary>Details</summary>
Motivation: 现有的 AI 蛋白质结构预测方法难以处理大型蛋白质复合物，而冷冻电镜等实验技术成本高、耗时。AFM 可以提供高分辨率高度图，但难以生成足够大的数据集来训练深度学习模型。

Method: 开发了一个虚拟 AFM 框架来模拟成像过程，并生成了一个包含约 542,000 个蛋白质的多视图合成 AFM 图像数据集。使用条件扩散模型从无姿势输入合成新视图，并使用特定实例的神经辐射场 (NeRF) 模型重建 3D 结构。

Result: 重建的 3D 蛋白质结构实现了在 AFM 成像分辨率内的平均 Chamfer 距离，反映了高结构保真度。在各种 PC 的实验 AFM 图像上进行了广泛验证。

Conclusion: ProFusion 具有准确、经济高效的蛋白质复合物结构预测和使用 AFM 实验进行快速迭代验证的强大潜力。

Abstract: AI-based in silico methods have improved protein structure prediction but
often struggle with large protein complexes (PCs) involving multiple
interacting proteins due to missing 3D spatial cues. Experimental techniques
like Cryo-EM are accurate but costly and time-consuming. We present ProFusion,
a hybrid framework that integrates a deep learning model with Atomic Force
Microscopy (AFM), which provides high-resolution height maps from random
orientations, naturally yielding multi-view data for 3D reconstruction.
However, generating a large-scale AFM imaging data set sufficient to train deep
learning models is impractical. Therefore, we developed a virtual AFM framework
that simulates the imaging process and generated a dataset of ~542,000 proteins
with multi-view synthetic AFM images. We train a conditional diffusion model to
synthesize novel views from unposed inputs and an instance-specific Neural
Radiance Field (NeRF) model to reconstruct 3D structures. Our reconstructed 3D
protein structures achieve an average Chamfer Distance within the AFM imaging
resolution, reflecting high structural fidelity. Our method is extensively
validated on experimental AFM images of various PCs, demonstrating strong
potential for accurate, cost-effective protein complex structure prediction and
rapid iterative validation using AFM experiments.

</details>


### [61] [Multi-Modal Interpretability for Enhanced Localization in Vision-Language Models](https://arxiv.org/abs/2509.15243)
*Muhammad Imran,Yugyung Lee*

Main category: cs.CV

TL;DR: 提出了一种多模态可解释学习（MMEL）框架，旨在提高视觉语言模型的可解释性，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 在安全关键的背景下应用视觉语言模型仍然具有挑战性，因为对象之间的关系复杂，视觉线索微妙，并且对透明性和可靠性的要求更高。

Method: 构建在transformer架构的基于梯度的解释（Grad-eclip）的先前工作的基础上，MMEL引入了一种新颖的分层语义关系模块，该模块通过多尺度特征处理、自适应注意加权和跨模态对齐来增强模型的可解释性。

Result: 通过在标准数据集上进行的大量实验，结果表明，通过将语义关系信息整合到基于梯度的属性图中，MMEL可以生成更集中和上下文感知的可视化效果，从而更好地反映视觉语言模型如何处理复杂的场景。

Conclusion: MMEL框架可以推广到各个领域，为需要高可解释性和可靠性的应用程序的模型决策提供有价值的见解。

Abstract: Recent advances in vision-language models have significantly expanded the
frontiers of automated image analysis. However, applying these models in
safety-critical contexts remains challenging due to the complex relationships
between objects, subtle visual cues, and the heightened demand for transparency
and reliability. This paper presents the Multi-Modal Explainable Learning
(MMEL) framework, designed to enhance the interpretability of vision-language
models while maintaining high performance. Building upon prior work in
gradient-based explanations for transformer architectures (Grad-eclip), MMEL
introduces a novel Hierarchical Semantic Relationship Module that enhances
model interpretability through multi-scale feature processing, adaptive
attention weighting, and cross-modal alignment. Our approach processes features
at multiple semantic levels to capture relationships between image regions at
different granularities, applying learnable layer-specific weights to balance
contributions across the model's depth. This results in more comprehensive
visual explanations that highlight both primary objects and their contextual
relationships with improved precision. Through extensive experiments on
standard datasets, we demonstrate that by incorporating semantic relationship
information into gradient-based attribution maps, MMEL produces more focused
and contextually aware visualizations that better reflect how vision-language
models process complex scenes. The MMEL framework generalizes across various
domains, offering valuable insights into model decisions for applications
requiring high interpretability and reliability.

</details>


### [62] [Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning](https://arxiv.org/abs/2509.15250)
*Wenda Qin,Andrea Burns,Bryan A. Plummer,Margrit Betke*

Main category: cs.CV

TL;DR: 本文提出了一种新的token剪枝方法，Navigation-Aware Pruning (NAP)，用于提高视觉语言导航(VLN)任务中大模型的效率。


<details>
  <summary>Details</summary>
Motivation: 现有token剪枝方法忽略了VLN任务的特殊挑战，例如剪枝导致的信息损失会增加计算成本，且无法识别无信息token。

Method: NAP方法利用导航特定的特征将token预过滤为前景和背景，并使用大型语言模型提取导航相关的指令。然后，该方法主要对背景token进行剪枝，并移除低重要性的导航节点以避免回溯。

Result: 在标准VLN基准测试中，NAP显著优于现有方法，在节省超过50% FLOPS的同时，保持了更高的成功率。

Conclusion: NAP方法通过利用导航信息进行token剪枝，有效地提高了VLN任务中大模型的效率，同时保持了较高的性能。

Abstract: Large models achieve strong performance on Vision-and-Language Navigation
(VLN) tasks, but are costly to run in resource-limited environments. Token
pruning offers appealing tradeoffs for efficiency with minimal performance loss
by reducing model input size, but prior work overlooks VLN-specific challenges.
For example, information loss from pruning can effectively increase
computational cost due to longer walks. Thus, the inability to identify
uninformative tokens undermines the supposed efficiency gains from pruning. To
address this, we propose Navigation-Aware Pruning (NAP), which uses
navigation-specific traits to simplify the pruning process by pre-filtering
tokens into foreground and background. For example, image views are filtered
based on whether the agent can navigate in that direction. We also extract
navigation-relevant instructions using a Large Language Model. After filtering,
we focus pruning on background tokens, minimizing information loss. To further
help avoid increases in navigation length, we discourage backtracking by
removing low-importance navigation nodes. Experiments on standard VLN
benchmarks show NAP significantly outperforms prior work, preserving higher
success rates while saving more than 50% FLOPS.

</details>


### [63] [RespoDiff: Dual-Module Bottleneck Transformation for Responsible & Faithful T2I Generation](https://arxiv.org/abs/2509.15257)
*Silpa Vadakkeeveetil Sreelatha,Sauradip Nag,Muhammad Awais,Serge Belongie,Anjan Dutta*

Main category: cs.CV

TL;DR: RespoDiff: A new framework for responsible text-to-image generation.


<details>
  <summary>Details</summary>
Motivation: Ensuring fairness and safety in text-to-image generation remains an open challenge. Existing methods typically improve fairness and safety at the expense of semantic fidelity and image quality.

Method: A dual-module transformation on the intermediate bottleneck representations of diffusion models is introduced. One module captures responsible concepts, and the other maintains semantic alignment. A novel score-matching objective enables effective coordination between the modules.

Result: The method outperforms state-of-the-art methods by ensuring semantic alignment while optimizing both objectives without compromising image fidelity. It improves responsible and semantically coherent generation by 20% across diverse, unseen prompts.

Conclusion: RespoDiff seamlessly integrates into large-scale models like SDXL, enhancing fairness and safety.

Abstract: The rapid advancement of diffusion models has enabled high-fidelity and
semantically rich text-to-image generation; however, ensuring fairness and
safety remains an open challenge. Existing methods typically improve fairness
and safety at the expense of semantic fidelity and image quality. In this work,
we propose RespoDiff, a novel framework for responsible text-to-image
generation that incorporates a dual-module transformation on the intermediate
bottleneck representations of diffusion models. Our approach introduces two
distinct learnable modules: one focused on capturing and enforcing responsible
concepts, such as fairness and safety, and the other dedicated to maintaining
semantic alignment with neutral prompts. To facilitate the dual learning
process, we introduce a novel score-matching objective that enables effective
coordination between the modules. Our method outperforms state-of-the-art
methods in responsible generation by ensuring semantic alignment while
optimizing both objectives without compromising image fidelity. Our approach
improves responsible and semantically coherent generation by 20% across
diverse, unseen prompts. Moreover, it integrates seamlessly into large-scale
models like SDXL, enhancing fairness and safety. Code will be released upon
acceptance.

</details>


### [64] [Autoguided Online Data Curation for Diffusion Model Training](https://arxiv.org/abs/2509.15267)
*Valeria Pais,Luis Oala,Daniele Faccio,Marco Aversa*

Main category: cs.CV

TL;DR: 本文研究了autoguidance和在线数据选择方法是否可以提高训练生成扩散模型的时间和样本效率。


<details>
  <summary>Details</summary>
Motivation: 生成模型计算成本的增加，激发了对高效数据管理的需求。

Method: 将联合示例选择（JEST）和autoguidance集成到一个统一的代码库中，以便快速消融和基准测试。在受控的二维合成数据生成任务以及（3x64x64）-D图像生成上评估数据管理的组合。

Result: Autoguidance始终提高样本质量和多样性。在数据效率方面，早期AJEST可以匹配或适度超过单独的autoguidance。

Conclusion: 虽然有针对性的在线选择可以在早期训练中产生效率提升，但稳健的样本质量改进主要由autoguidance驱动。

Abstract: The costs of generative model compute rekindled promises and hopes for
efficient data curation. In this work, we investigate whether recently
developed autoguidance and online data selection methods can improve the time
and sample efficiency of training generative diffusion models. We integrate
joint example selection (JEST) and autoguidance into a unified code base for
fast ablation and benchmarking. We evaluate combinations of data curation on a
controlled 2-D synthetic data generation task as well as (3x64x64)-D image
generation. Our comparisons are made at equal wall-clock time and equal number
of samples, explicitly accounting for the overhead of selection. Across
experiments, autoguidance consistently improves sample quality and diversity.
Early AJEST (applying selection only at the beginning of training) can match or
modestly exceed autoguidance alone in data efficiency on both tasks. However,
its time overhead and added complexity make autoguidance or uniform random data
selection preferable in most situations. These findings suggest that while
targeted online selection can yield efficiency gains in early training, robust
sample quality improvements are primarily driven by autoguidance. We discuss
limitations and scope, and outline when data selection may be beneficial.

</details>


### [65] [PRISM: Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images](https://arxiv.org/abs/2509.15270)
*Emanuele Ricco,Elia Onofri,Lorenzo Cima,Stefano Cresci,Roberto Di Pietro*

Main category: cs.CV

TL;DR: PRISM: A framework for fingerprinting AI-generated images using frequency-domain analysis, achieving high accuracy in model attribution and real vs fake image detection.


<details>
  <summary>Details</summary>
Motivation: The need for attribution methods to identify the model originating AI-generated content, especially in commercial settings.

Method: A Phase-enhanced Radial-based Image Signature Mapping (PRISM) framework based on radial reduction of the discrete Fourier transform and linear discriminant analysis.

Result: Achieves 92.04% attribution accuracy on PRISM-36K dataset and an average of 81.60% on other benchmarks. Also achieves an average accuracy of 88.41% in detecting real vs fake images.

Conclusion: Frequency-domain fingerprinting is effective for cross-architecture and cross-dataset model attribution, offering a solution for accountability and trust in generative AI systems.

Abstract: A critical need has emerged for generative AI: attribution methods. That is,
solutions that can identify the model originating AI-generated content. This
feature, generally relevant in multimodal applications, is especially sensitive
in commercial settings where users subscribe to paid proprietary services and
expect guarantees about the source of the content they receive. To address
these issues, we introduce PRISM, a scalable Phase-enhanced Radial-based Image
Signature Mapping framework for fingerprinting AI-generated images. PRISM is
based on a radial reduction of the discrete Fourier transform that leverages
amplitude and phase information to capture model-specific signatures. The
output of the above process is subsequently clustered via linear discriminant
analysis to achieve reliable model attribution in diverse settings, even if the
model's internal details are inaccessible. To support our work, we construct
PRISM-36K, a novel dataset of 36,000 images generated by six text-to-image GAN-
and diffusion-based models. On this dataset, PRISM achieves an attribution
accuracy of 92.04%. We additionally evaluate our method on four benchmarks from
the literature, reaching an average accuracy of 81.60%. Finally, we evaluate
our methodology also in the binary task of detecting real vs fake images,
achieving an average accuracy of 88.41%. We obtain our best result on GenImage
with an accuracy of 95.06%, whereas the original benchmark achieved 82.20%. Our
results demonstrate the effectiveness of frequency-domain fingerprinting for
cross-architecture and cross-dataset model attribution, offering a viable
solution for enforcing accountability and trust in generative AI systems.

</details>


### [66] [Large Vision Models Can Solve Mental Rotation Problems](https://arxiv.org/abs/2509.15271)
*Sebastian Ray Mason,Anders Gjølbye,Phillip Chavarria Højbjerg,Lenka Tětková,Lars Kai Hansen*

Main category: cs.CV

TL;DR: 评估视觉 Transformer (ViT、CLIP、DINOv2 和 DINOv3) 在心理旋转任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 研究现代视觉 Transformer 模型在多大程度上发展出类似人类的空间推理能力，尤其是在心理旋转方面。

Method: 通过测试模型对简单和复杂的块结构、文本和照片级逼真物体进行心理旋转任务的表现，逐层探测模型表征。

Result: 自监督 ViT 比监督 ViT 更好地捕捉几何结构；中间层比最终层表现更好；任务难度随着旋转复杂性和遮挡而增加，与人类的反应时间相似。

Conclusion: 模型在嵌入空间表示中存在与人类相似的约束。

Abstract: Mental rotation is a key test of spatial reasoning in humans and has been
central to understanding how perception supports cognition. Despite the success
of modern vision transformers, it is still unclear how well these models
develop similar abilities. In this work, we present a systematic evaluation of
ViT, CLIP, DINOv2, and DINOv3 across a range of mental-rotation tasks, from
simple block structures similar to those used by Shepard and Metzler to study
human cognition, to more complex block figures, three types of text, and
photo-realistic objects. By probing model representations layer by layer, we
examine where and how these networks succeed. We find that i) self-supervised
ViTs capture geometric structure better than supervised ViTs; ii) intermediate
layers perform better than final layers; iii) task difficulty increases with
rotation complexity and occlusion, mirroring human reaction times and
suggesting similar constraints in embedding space representations.

</details>


### [67] [Which Direction to Choose? An Analysis on the Representation Power of Self-Supervised ViTs in Downstream Tasks](https://arxiv.org/abs/2509.15272)
*Yannis Kaltampanidis,Alexandros Doumanoglou,Dimitrios Zarpalas*

Main category: cs.CV

TL;DR: 本研究旨在系统评估Vision Transformers (ViT)中未修改的自监督学习(SSL)特征在图像分类和分割任务中的表现，包括标准和少样本场景。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要额外的转换层来处理预训练的ViT特征，但对未修改特征的内在表示能力缺乏全面的分析。

Method: 通过使用基于超平面或余弦相似度的分类和分割规则，直接评估来自ViT的各种token类型（keys, queries, values等）。

Result: 分析了不同token类型、任务和预训练ViT模型，揭示了基于任务、上下文和预训练目标选择最佳token类型和决策规则的见解。

Conclusion: 研究提供了关于在不同任务和场景下，如何有效利用未修改的ViT特征的深入理解，并报告了在两个广泛使用的数据集上的详细发现。

Abstract: Self-Supervised Learning (SSL) for Vision Transformers (ViTs) has recently
demonstrated considerable potential as a pre-training strategy for a variety of
computer vision tasks, including image classification and segmentation, both in
standard and few-shot downstream contexts. Two pre-training objectives dominate
the landscape of SSL techniques: Contrastive Learning and Masked Image
Modeling. Features (or tokens) extracted from the final transformer attention
block -- specifically, the keys, queries, and values -- as well as features
obtained after the final block's feed-forward layer, have become a common
foundation for addressing downstream tasks. However, in many existing
approaches, these pre-trained ViT features are further processed through
additional transformation layers, often involving lightweight heads or combined
with distillation, to achieve superior task performance. Although such methods
can improve task outcomes, to the best of our knowledge, a comprehensive
analysis of the intrinsic representation capabilities of unaltered ViT features
has yet to be conducted. This study aims to bridge this gap by systematically
evaluating the use of these unmodified features across image classification and
segmentation tasks, in both standard and few-shot contexts. The classification
and segmentation rules that we use are either hyperplane based (as in logistic
regression) or cosine-similarity based, both of which rely on the presence of
interpretable directions in the ViT's latent space. Based on the previous rules
and without the use of additional feature transformations, we conduct an
analysis across token types, tasks, and pre-trained ViT models. This study
provides insights into the optimal choice for token type and decision rule
based on the task, context, and the pre-training objective, while reporting
detailed findings on two widely-used datasets.

</details>


### [68] [How Good are Foundation Models in Step-by-Step Embodied Reasoning?](https://arxiv.org/abs/2509.15293)
*Dinura Dissanayake,Ahmed Heakl,Omkar Thawakar,Noor Ahsan,Ritesh Thawkar,Ketan More,Jean Lahoud,Rao Anwer,Hisham Cholakkal,Ivan Laptev,Fahad Shahbaz Khan,Salman Khan*

Main category: cs.CV

TL;DR: 本文提出了一种新的基准测试，用于评估大型多模态模型在具身环境中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型在视觉理解和语言生成方面取得了显著进展，但其在现实世界具身任务中进行结构化推理的能力仍有待探索。

Method: 本文提出了具身推理基础模型（FoMER）基准，旨在评估大型多模态模型在复杂具身决策场景中的推理能力。

Result: 该基准测试包含超过 1.1k 个样本，涵盖 10 个任务和 8 个主体，包括三种不同的机器人类型。结果强调了大型多模态模型在具身推理中的潜力和局限性。

Conclusion: 该研究结果揭示了大型多模态模型在具身推理中的关键挑战和未来研究机会。

Abstract: Embodied agents operating in the physical world must make decisions that are
not only effective but also safe, spatially coherent, and grounded in context.
While recent advances in large multimodal models (LMMs) have shown promising
capabilities in visual understanding and language generation, their ability to
perform structured reasoning for real-world embodied tasks remains
underexplored. In this work, we aim to understand how well foundation models
can perform step-by-step reasoning in embodied environments. To this end, we
propose the Foundation Model Embodied Reasoning (FoMER) benchmark, designed to
evaluate the reasoning capabilities of LMMs in complex embodied decision-making
scenarios. Our benchmark spans a diverse set of tasks that require agents to
interpret multimodal observations, reason about physical constraints and
safety, and generate valid next actions in natural language. We present (i) a
large-scale, curated suite of embodied reasoning tasks, (ii) a novel evaluation
framework that disentangles perceptual grounding from action reasoning, and
(iii) empirical analysis of several leading LMMs under this setting. Our
benchmark includes over 1.1k samples with detailed step-by-step reasoning
across 10 tasks and 8 embodiments, covering three different robot types. Our
results highlight both the potential and current limitations of LMMs in
embodied reasoning, pointing towards key challenges and opportunities for
future research in robot intelligence. Our data and code will be made publicly
available.

</details>


### [69] [CoDoL: Conditional Domain Prompt Learning for Out-of-Distribution Generalization](https://arxiv.org/abs/2509.15330)
*Min Zhang,Bo Jiang,Jie Zhou,Yimeng Liu,Xin Lin*

Main category: cs.CV

TL;DR: 本文提出了一种新的条件域提示学习 (CoDoL) 方法，该方法利用现成的域信息来形成提示，并改进视觉语言嵌入对齐，以提高 OOD 泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于提示的 CLIP 方法存在以下问题：i) 文本描述不准确，导致准确性和鲁棒性下降，并对零样本 CLIP 方法提出了挑战。ii) 视觉语言嵌入对齐有限，严重影响泛化性能。

Method: 本文提出了一种新的条件域提示学习 (CoDoL) 方法，该方法利用现成的域信息来形成提示，并改进视觉语言嵌入对齐，以提高 OOD 泛化能力。为了捕获特定于实例和特定于域的信息，我们进一步提出了一种轻量级域元网络 (DMN)，以生成每个域中图像的输入条件标记。

Result: 在四个 OOD 基准 (PACS、VLCS、OfficeHome 和 DigitDG) 上进行的大量实验验证了我们提出的 CoDoL 在改进视觉语言嵌入对齐以及分布外泛化性能方面的有效性。

Conclusion: 本文提出了一种新的条件域提示学习 (CoDoL) 方法，该方法利用现成的域信息来形成提示，并改进视觉语言嵌入对齐，以提高 OOD 泛化能力。实验结果表明，CoDoL 在改进视觉语言嵌入对齐以及分布外泛化性能方面是有效的。

Abstract: Recent advances in pre-training vision-language models (VLMs), e.g.,
contrastive language-image pre-training (CLIP) methods, have shown great
potential in learning out-of-distribution (OOD) representations. Despite
showing competitive performance, the prompt-based CLIP methods still suffer
from: i) inaccurate text descriptions, which leads to degraded accuracy and
robustness, and poses a challenge for zero-shot CLIP methods. ii) limited
vision-language embedding alignment, which significantly affects the
generalization performance. To tackle the above issues, this paper proposes a
novel Conditional Domain prompt Learning (CoDoL) method, which utilizes
readily-available domain information to form prompts and improves the
vision-language embedding alignment for improving OOD generalization. To
capture both instance-specific and domain-specific information, we further
propose a lightweight Domain Meta Network (DMN) to generate input-conditional
tokens for images in each domain. Extensive experiments on four OOD benchmarks
(PACS, VLCS, OfficeHome and DigitDG) validate the effectiveness of our proposed
CoDoL in terms of improving the vision-language embedding alignment as well as
the out-of-distribution generalization performance.

</details>


### [70] [Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception](https://arxiv.org/abs/2509.15333)
*Yulin Wang,Yang Yue,Yang Yue,Huanqian Wang,Haojun Jiang,Yizeng Han,Zanlin Ni,Yifan Pu,Minglei Shi,Rui Lu,Qisen Yang,Andrew Zhao,Zhuofan Xia,Shiji Song,Gao Huang*

Main category: cs.CV

TL;DR: 提出了一种新的自适应视觉框架AdaptiveNN，通过模仿人类视觉的fixation机制，选择性地关注与任务相关的区域，从而提高效率和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器视觉模型通常被动地处理整个场景，导致资源需求过高，限制了未来的发展和实际应用。

Method: 将视觉感知过程建模为一个由粗到精的序列决策过程，逐步识别和关注与任务相关的区域，增量地整合信息，并在获得足够信息时主动结束观察。采用 representation learning 与 self-rewarding reinforcement learning 相结合的理论，实现 AdaptiveNN 的端到端训练。

Result: 在 17 个基准测试中，AdaptiveNN 在不牺牲准确性的前提下，实现了高达 28 倍的推理成本降低，并且可以灵活地适应不同的任务需求和资源预算。此外，其 fixation 模式提供了更强的可解释性，并且在许多情况下表现出与人类相似的感知行为。

Conclusion: AdaptiveNN 为高效、灵活和可解释的计算机视觉提供了一个有希望的途径，并且有潜力成为研究视觉认知的宝贵工具。

Abstract: Human vision is highly adaptive, efficiently sampling intricate environments
by sequentially fixating on task-relevant regions. In contrast, prevailing
machine vision models passively process entire scenes at once, resulting in
excessive resource demands scaling with spatial-temporal input resolution and
model size, yielding critical limitations impeding both future advancements and
real-world application. Here we introduce AdaptiveNN, a general framework
aiming to drive a paradigm shift from 'passive' to 'active, adaptive' vision
models. AdaptiveNN formulates visual perception as a coarse-to-fine sequential
decision-making process, progressively identifying and attending to regions
pertinent to the task, incrementally combining information across fixations,
and actively concluding observation when sufficient. We establish a theory
integrating representation learning with self-rewarding reinforcement learning,
enabling end-to-end training of the non-differentiable AdaptiveNN without
additional supervision on fixation locations. We assess AdaptiveNN on 17
benchmarks spanning 9 tasks, including large-scale visual recognition,
fine-grained discrimination, visual search, processing images from real driving
and medical scenarios, language-driven embodied AI, and side-by-side
comparisons with humans. AdaptiveNN achieves up to 28x inference cost reduction
without sacrificing accuracy, flexibly adapts to varying task demands and
resource budgets without retraining, and provides enhanced interpretability via
its fixation patterns, demonstrating a promising avenue toward efficient,
flexible, and interpretable computer vision. Furthermore, AdaptiveNN exhibits
closely human-like perceptual behaviors in many cases, revealing its potential
as a valuable tool for investigating visual cognition. Code is available at
https://github.com/LeapLabTHU/AdaptiveNN.

</details>


### [71] [LowDiff: Efficient Diffusion Sampling with Low-Resolution Condition](https://arxiv.org/abs/2509.15342)
*Jiuyi Xu,Qing Jin,Meida Chen,Andrew Feng,Yang Sui,Yangming Shi*

Main category: cs.CV

TL;DR: 提出了一种新的高效扩散框架LowDiff，通过生成越来越高分辨率的输出来实现加速。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中取得了显著成功，但其应用常因采样速度慢而受阻。以往的效率提升工作主要集中在压缩模型或减少总的去噪步骤，而忽略了在生成过程中利用多个输入分辨率的可能性。

Method: 采用了一种基于级联的方法，通过生成越来越高分辨率的输出来逐步细化图像，并使用统一的模型。

Result: 在CIFAR-10、FFHQ和ImageNet上的大量实验表明，该方法的有效性和通用性。结果显示，在所有数据集和设置中，吞吐量提高了50%以上，同时保持了相当或更好的质量。在无条件CIFAR-10上，LowDiff的FID为2.11，IS为9.87；在有条件CIFAR-10上，FID为1.94，IS为10.03。在FFHQ 64x64上，LowDiff的FID为2.43；在ImageNet 256x256上，基于LightningDiT-B/1构建的LowDiff产生了高质量的样本，FID为4.00，IS为195.06，同时获得了显著的效率提升。

Conclusion: LowDiff在保持或提高生成质量的同时，显著提高了扩散模型的采样效率，具有良好的应用前景。

Abstract: Diffusion models have achieved remarkable success in image generation but
their practical application is often hindered by the slow sampling speed. Prior
efforts of improving efficiency primarily focus on compressing models or
reducing the total number of denoising steps, largely neglecting the
possibility to leverage multiple input resolutions in the generation process.
In this work, we propose LowDiff, a novel and efficient diffusion framework
based on a cascaded approach by generating increasingly higher resolution
outputs. Besides, LowDiff employs a unified model to progressively refine
images from low resolution to the desired resolution. With the proposed
architecture design and generation techniques, we achieve comparable or even
superior performance with much fewer high-resolution sampling steps. LowDiff is
applicable to diffusion models in both pixel space and latent space. Extensive
experiments on both conditional and unconditional generation tasks across
CIFAR-10, FFHQ and ImageNet demonstrate the effectiveness and generality of our
method. Results show over 50% throughput improvement across all datasets and
settings while maintaining comparable or better quality. On unconditional
CIFAR-10, LowDiff achieves an FID of 2.11 and IS of 9.87, while on conditional
CIFAR-10, an FID of 1.94 and IS of 10.03. On FFHQ 64x64, LowDiff achieves an
FID of 2.43, and on ImageNet 256x256, LowDiff built on LightningDiT-B/1
produces high-quality samples with a FID of 4.00 and an IS of 195.06, together
with substantial efficiency gains.

</details>


### [72] [MaskAttn-SDXL: Controllable Region-Level Text-To-Image Generation](https://arxiv.org/abs/2509.15357)
*Yu Chang,Jiahao Chen,Anzhe Cheng,Paul Bogdan*

Main category: cs.CV

TL;DR: MaskAttn-SDXL improves compositional control in text-to-image diffusion models by sparsifying token-to-latent interactions using a learned binary mask applied to cross-attention logits in Stable Diffusion XL (SDXL).


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models often fail to compose prompts with multiple objects, attributes, and spatial relations due to cross-token interference.

Method: A region-level gating mechanism, MaskAttn-SDXL, learns a binary mask per layer and injects it into each cross-attention logit map before softmax to sparsify token-to-latent interactions.

Result: The model improves spatial compliance and attribute binding in multi-object prompts while preserving overall image quality and diversity.

Conclusion: Logit-level masked cross-attention is a data-efficient primitive for enforcing compositional control, offering a practical extension for spatial control in text-to-image generation.

Abstract: Text-to-image diffusion models achieve impressive realism but often suffer
from compositional failures on prompts with multiple objects, attributes, and
spatial relations, resulting in cross-token interference where entities
entangle, attributes mix across objects, and spatial cues are violated. To
address these failures, we propose MaskAttn-SDXL,a region-level gating
mechanism applied to the cross-attention logits of Stable Diffusion XL(SDXL)'s
UNet. MaskAttn-SDXL learns a binary mask per layer, injecting it into each
cross-attention logit map before softmax to sparsify token-to-latent
interactions so that only semantically relevant connections remain active. The
method requires no positional encodings, auxiliary tokens, or external region
masks, and preserves the original inference path with negligible overhead. In
practice, our model improves spatial compliance and attribute binding in
multi-object prompts while preserving overall image quality and diversity.
These findings demonstrate that logit-level maksed cross-attention is an
data-efficient primitve for enforcing compositional control, and our method
thus serves as a practical extension for spatial control in text-to-image
generation.

</details>


### [73] [RaceGAN: A Framework for Preserving Individuality while Converting Racial Information for Image-to-Image Translation](https://arxiv.org/abs/2509.15391)
*Mst Tasnim Pervin,George Bebis,Fang Jiang,Alireza Tavakkoli*

Main category: cs.CV

TL;DR: 提出了一种新的框架RaceGAN，用于多种族属性转换，可以在多个域上映射样式代码，同时保持个性和高层语义，而无需依赖参考图像。


<details>
  <summary>Details</summary>
Motivation: 现有模型无法保持个体性，并且除了输入之外还需要额外的参考图像，为了克服这些局限性，作者的目标是利用多域图像到图像的转换来转换种族特征。

Method: 提出RaceGAN，一种新的框架，可以在种族属性转换期间在多个域上映射样式代码，同时保持个性和高层语义，而无需依赖参考图像。

Result: RaceGAN在翻译种族特征方面优于其他模型，并在Chicago Face Dataset上进行了测试。还提供了利用基于InceptionReNetv2的分类的定量结果，以证明种族翻译的有效性。

Conclusion: 该模型可以将潜在空间划分为每个种族的不同面部聚类。

Abstract: Generative adversarial networks (GANs) have demonstrated significant progress
in unpaired image-to-image translation in recent years for several
applications. CycleGAN was the first to lead the way, although it was
restricted to a pair of domains. StarGAN overcame this constraint by tackling
image-to-image translation across various domains, although it was not able to
map in-depth low-level style changes for these domains. Style mapping via
reference-guided image synthesis has been made possible by the innovations of
StarGANv2 and StyleGAN. However, these models do not maintain individuality and
need an extra reference image in addition to the input. Our study aims to
translate racial traits by means of multi-domain image-to-image translation. We
present RaceGAN, a novel framework capable of mapping style codes over several
domains during racial attribute translation while maintaining individuality and
high level semantics without relying on a reference image. RaceGAN outperforms
other models in translating racial features (i.e., Asian, White, and Black)
when tested on Chicago Face Dataset. We also give quantitative findings
utilizing InceptionReNetv2-based classification to demonstrate the
effectiveness of our racial translation. Moreover, we investigate how well the
model partitions the latent space into distinct clusters of faces for each
ethnic group.

</details>


### [74] [Generating Part-Based Global Explanations Via Correspondence](https://arxiv.org/abs/2509.15393)
*Kunal Rathore,Prasad Tadepalli*

Main category: cs.CV

TL;DR: 本文提出了一种利用少量图像的用户定义部件标签，并将其高效迁移到更大的数据集的方法，以生成全局符号解释，从而为大规模的模型决策提供人类可理解的解释。


<details>
  <summary>Details</summary>
Motivation: 现有的解释方法通常侧重于单个图像的局部视觉解释，而基于概念的解释虽然提供了全局的见解，但需要大量的注释，从而导致显著的标注成本。

Method: 利用来自有限图像集的用户定义部件标签，并有效地将它们转移到更大的数据集。

Result: 通过聚合基于部件的局部解释，生成全局符号解释。

Conclusion: 为大规模的模型决策提供人类可理解的解释。

Abstract: Deep learning models are notoriously opaque. Existing explanation methods
often focus on localized visual explanations for individual images.
Concept-based explanations, while offering global insights, require extensive
annotations, incurring significant labeling cost. We propose an approach that
leverages user-defined part labels from a limited set of images and efficiently
transfers them to a larger dataset. This enables the generation of global
symbolic explanations by aggregating part-based local explanations, ultimately
providing human-understandable explanations for model decisions on a large
scale.

</details>


### [75] [Causal Fingerprints of AI Generative Models](https://arxiv.org/abs/2509.15406)
*Hui Xu,Chi Liu,Congcong Zhu,Minghao Wang,Youyang Qu,Longxiang Gao*

Main category: cs.CV

TL;DR: 提出了一种新的AI生成模型溯源方法，通过解耦图像内容和风格，提取模型间的因果指纹。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于模型特定线索或合成伪影，泛化性差。完整的模型指纹应反映图像来源和模型痕迹之间的因果关系。

Method: 提出一个因果解耦框架，在语义不变的潜在空间中解耦图像特定内容和风格，并使用预训练扩散重建残差提取模型指纹。使用多样化的特征表示来增强指纹的粒度。

Result: 在GAN和扩散模型上的实验表明，该方法在模型溯源方面优于现有方法。

Conclusion: 该方法在伪造检测、模型版权追踪和身份保护方面具有强大的潜力。

Abstract: AI generative models leave implicit traces in their generated images, which
are commonly referred to as model fingerprints and are exploited for source
attribution. Prior methods rely on model-specific cues or synthesis artifacts,
yielding limited fingerprints that may generalize poorly across different
generative models. We argue that a complete model fingerprint should reflect
the causality between image provenance and model traces, a direction largely
unexplored. To this end, we conceptualize the \emph{causal fingerprint} of
generative models, and propose a causality-decoupling framework that
disentangles it from image-specific content and style in a semantic-invariant
latent space derived from pre-trained diffusion reconstruction residual. We
further enhance fingerprint granularity with diverse feature representations.
We validate causality by assessing attribution performance across
representative GANs and diffusion models and by achieving source anonymization
using counterfactual examples generated from causal fingerprints. Experiments
show our approach outperforms existing methods in model attribution, indicating
strong potential for forgery detection, model copyright tracing, and identity
protection.

</details>


### [76] [NeuroRAD-FM: A Foundation Model for Neuro-Oncology with Distributionally Robust Training](https://arxiv.org/abs/2509.15416)
*Moinak Bhattacharya,Angelica P. Kurtz,Fabio M. Iwamoto,Prateek Prasanna,Gagandeep Singh*

Main category: cs.CV

TL;DR: 开发了一种神经肿瘤专用FM，具有分布鲁棒的损失函数，能够准确估计肿瘤表型，同时保持跨机构的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的FMs在预测罕见分子标记方面表现不佳，而这些标记对于治疗反应和风险分层至关重要。神经肿瘤学由于异构数据和肿瘤复杂性，对机器学习提出了独特的挑战，限制了基础模型(FMs)在不同队列中推广的能力。

Method: 在多机构脑肿瘤MRI上预训练自监督骨干网络(BYOL, DINO, MAE, MoCo)，并应用分布鲁棒优化(DRO)来缓解站点和类别不平衡。

Result: 该方法改进了分子预测，减少了站点特异性嵌入差异。在CUIMC，平均平衡精度从0.744提高到0.785，AUC从0.656提高到0.676，对于未充分代表的终点，增益最大(CDKN2A/2B精度0.86到0.92, AUC 0.73到0.92;ATRX AUC 0.69到0.82;Ki-67精度0.60到0.69)。对于生存率，所有站点的c指数都有所提高:CUIMC 0.592到0.597, UPenn 0.647到0.672, UCSF 0.600到0.627。

Conclusion: 将FMs与DRO结合使用，可以产生更具站点不变性的表示，提高常见和罕见标记的预测，并提高生存区分能力，强调需要前瞻性验证以及纵向和干预信号的整合，以推进精确神经肿瘤学。

Abstract: Neuro-oncology poses unique challenges for machine learning due to
heterogeneous data and tumor complexity, limiting the ability of foundation
models (FMs) to generalize across cohorts. Existing FMs also perform poorly in
predicting uncommon molecular markers, which are essential for treatment
response and risk stratification. To address these gaps, we developed a
neuro-oncology specific FM with a distributionally robust loss function,
enabling accurate estimation of tumor phenotypes while maintaining
cross-institution generalization. We pretrained self-supervised backbones
(BYOL, DINO, MAE, MoCo) on multi-institutional brain tumor MRI and applied
distributionally robust optimization (DRO) to mitigate site and class
imbalance. Downstream tasks included molecular classification of common markers
(MGMT, IDH1, 1p/19q, EGFR), uncommon alterations (ATRX, TP53, CDKN2A/2B, TERT),
continuous markers (Ki-67, TP53), and overall survival prediction in IDH1
wild-type glioblastoma at UCSF, UPenn, and CUIMC. Our method improved molecular
prediction and reduced site-specific embedding differences. At CUIMC, mean
balanced accuracy rose from 0.744 to 0.785 and AUC from 0.656 to 0.676, with
the largest gains for underrepresented endpoints (CDKN2A/2B accuracy 0.86 to
0.92, AUC 0.73 to 0.92; ATRX AUC 0.69 to 0.82; Ki-67 accuracy 0.60 to 0.69).
For survival, c-index improved at all sites: CUIMC 0.592 to 0.597, UPenn 0.647
to 0.672, UCSF 0.600 to 0.627. Grad-CAM highlighted tumor and peri-tumoral
regions, confirming interpretability. Overall, coupling FMs with DRO yields
more site-invariant representations, improves prediction of common and uncommon
markers, and enhances survival discrimination, underscoring the need for
prospective validation and integration of longitudinal and interventional
signals to advance precision neuro-oncology.

</details>


### [77] [ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models](https://arxiv.org/abs/2509.15435)
*Chung-En Johnny Yu,Hsuan-Chih,Chen,Brian Jalaian,Nathaniel D. Bastian*

Main category: cs.CV

TL;DR: ORCA是一个通过观察-推理-评论-行动循环来提高预训练LVLM的事实准确性和对抗鲁棒性的框架，它利用了一套小型视觉模型，无需访问模型内部或重新训练。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLM）表现出强大的多模态能力，但容易受到内在错误导致的幻觉和外部利用的对抗性攻击，从而限制了它们在现实应用中的可靠性。

Method: ORCA通过一个观察-推理-评论-行动循环运作，用证据问题查询多个视觉工具，验证跨模型的不一致性，并迭代地完善预测。

Result: 在POPE幻觉基准测试中，ORCA将独立LVLM的性能提高了+3.64％至+40.67％。在POPE上的对抗性扰动下，ORCA在LVLM上的平均准确率提高了+20.11％。当与对抗性扰动的AMBER图像上的防御技术相结合时，ORCA进一步提高了独立LVLM的性能，在评估指标上的增益范围从+1.20％到+48.00％。

Conclusion: ORCA为构建更可靠和更强大的多模态系统提供了一条有希望的途径。

Abstract: Large Vision-Language Models (LVLMs) exhibit strong multimodal capabilities
but remain vulnerable to hallucinations from intrinsic errors and adversarial
attacks from external exploitations, limiting their reliability in real-world
applications. We present ORCA, an agentic reasoning framework that improves the
factual accuracy and adversarial robustness of pretrained LVLMs through
test-time structured inference reasoning with a suite of small vision models
(less than 3B parameters). ORCA operates via an Observe--Reason--Critique--Act
loop, querying multiple visual tools with evidential questions, validating
cross-model inconsistencies, and refining predictions iteratively without
access to model internals or retraining. ORCA also stores intermediate
reasoning traces, which supports auditable decision-making. Though designed
primarily to mitigate object-level hallucinations, ORCA also exhibits emergent
adversarial robustness without requiring adversarial training or defense
mechanisms. We evaluate ORCA across three settings: (1) clean images on
hallucination benchmarks, (2) adversarially perturbed images without defense,
and (3) adversarially perturbed images with defense applied. On the POPE
hallucination benchmark, ORCA improves standalone LVLM performance by +3.64\%
to +40.67\% across different subsets. Under adversarial perturbations on POPE,
ORCA achieves an average accuracy gain of +20.11\% across LVLMs. When combined
with defense techniques on adversarially perturbed AMBER images, ORCA further
improves standalone LVLM performance, with gains ranging from +1.20\% to
+48.00\% across evaluation metrics. These results demonstrate that ORCA offers
a promising path toward building more reliable and robust multimodal systems.

</details>


### [78] [Region-Aware Deformable Convolutions](https://arxiv.org/abs/2509.15436)
*Abolfazl Saheban Maleki,Maryam Imani*

Main category: cs.CV

TL;DR: 提出了区域感知可变形卷积（RAD-Conv），以增强神经网络适应复杂图像结构的能力。


<details>
  <summary>Details</summary>
Motivation: 传统的可变形卷积局限于固定的四边形采样区域，无法灵活适应图像内容。

Method: 每个内核元素使用四个边界偏移来创建灵活的矩形区域，动态调整其大小和形状以匹配图像内容。解耦感受野的形状与内核的结构。

Result: 允许精确控制感受野的宽度和高度，能够捕获局部细节和长程依赖关系，即使使用小的1x1内核。

Conclusion: 提供了一个构建更具表现力和效率的视觉模型的实用解决方案，弥合了刚性卷积架构和计算成本高昂的基于注意力的的方法之间的差距。

Abstract: We introduce Region-Aware Deformable Convolution (RAD-Conv), a new
convolutional operator that enhances neural networks' ability to adapt to
complex image structures. Unlike traditional deformable convolutions, which are
limited to fixed quadrilateral sampling areas, RAD-Conv uses four boundary
offsets per kernel element to create flexible, rectangular regions that
dynamically adjust their size and shape to match image content. This approach
allows precise control over the receptive field's width and height, enabling
the capture of both local details and long-range dependencies, even with small
1x1 kernels. By decoupling the receptive field's shape from the kernel's
structure, RAD-Conv combines the adaptability of attention mechanisms with the
efficiency of standard convolutions. This innovative design offers a practical
solution for building more expressive and efficient vision models, bridging the
gap between rigid convolutional architectures and computationally costly
attention-based methods.

</details>


### [79] [CAGE: Continuity-Aware edGE Network Unlocks Robust Floorplan Reconstruction](https://arxiv.org/abs/2509.15459)
*Yiyi Liu,Chunyang Liu,Weiqin Jiao,Bojian Wu,Fashuai Li,Biao Xiong*

Main category: cs.CV

TL;DR: CAGE网络是一种从点云密度图重建矢量平面图的鲁棒框架，通过对每个墙段建模为有向几何连续边，实现连贯的平面图结构推理。


<details>
  <summary>Details</summary>
Motivation: 传统基于角的 polygon 表示对噪声和不完整观测非常敏感，导致平面图破碎或不合理。现有line grouping 方法虽然利用结构线索提高鲁棒性，但难以恢复精细的几何细节。

Method: 提出了一种原生的以边为中心的公式，将每个墙段建模为有向的、几何连续的边。开发了一个双查询 Transformer 解码器，在去噪框架中整合扰动查询和潜在查询。

Result: 在 Structured3D 和 SceneCAD 上进行了大量实验，CAGE 达到了最先进的性能，F1 分数分别为 99.1%（rooms）、91.7%（corners）和 89.3%（angles）。该方法还表现出强大的跨数据集泛化能力。

Conclusion: CAGE 网络通过原生边为中心的建模和双查询 Transformer 解码器，实现了从点云密度图重建矢量平面图的鲁棒性和准确性。

Abstract: We present \textbf{CAGE} (\textit{Continuity-Aware edGE}) network, a
\textcolor{red}{robust} framework for reconstructing vector floorplans directly
from point-cloud density maps. Traditional corner-based polygon representations
are highly sensitive to noise and incomplete observations, often resulting in
fragmented or implausible layouts. Recent line grouping methods leverage
structural cues to improve robustness but still struggle to recover fine
geometric details. To address these limitations, we propose a \textit{native}
edge-centric formulation, modeling each wall segment as a directed,
geometrically continuous edge. This representation enables inference of
coherent floorplan structures, ensuring watertight, topologically valid room
boundaries while improving robustness and reducing artifacts. Towards this
design, we develop a dual-query transformer decoder that integrates perturbed
and latent queries within a denoising framework, which not only stabilizes
optimization but also accelerates convergence. Extensive experiments on
Structured3D and SceneCAD show that \textbf{CAGE} achieves state-of-the-art
performance, with F1 scores of 99.1\% (rooms), 91.7\% (corners), and 89.3\%
(angles). The method also demonstrates strong cross-dataset generalization,
underscoring the efficacy of our architectural innovations. Code and pretrained
models will be released upon acceptance.

</details>


### [80] [Self-supervised learning of imaging and clinical signatures using a multimodal joint-embedding predictive architecture](https://arxiv.org/abs/2509.15470)
*Thomas Z. Li,Aravind R. Krishnan,Lianrui Zuo,John M. Still,Kim L. Sandler,Fabien Maldonado,Thomas A. Lasko,Bennett A. Landman*

Main category: cs.CV

TL;DR: 本文利用来自纵向和多模态档案的自监督学习来解决肺结节诊断中标记数据稀缺和模型容易在训练分布上过拟合的问题。


<details>
  <summary>Details</summary>
Motivation: 开发用于肺结节诊断的多模态模型受到标记数据稀缺和这些模型容易在训练分布上过拟合的限制。

Method: 我们从我们本地机构整理了一个包含CT扫描和链接的电子健康记录的未标记患者集合，以支持联合嵌入预测架构（JEPA）预训练。

Result: 在监督微调后，我们的方法在一个内部队列中优于未正则化的多模态模型和仅成像模型（我们的：0.91，多模态：0.88，仅成像：0.73 AUC），但在外部队列中表现不佳（我们的：0.72，仅成像：0.75 AUC）。

Conclusion: 这项工作创新了一种利用未标记的多模态医学档案来改进预测模型的方法，并证明了其在肺结节诊断中的优势和局限性。

Abstract: The development of multimodal models for pulmonary nodule diagnosis is
limited by the scarcity of labeled data and the tendency for these models to
overfit on the training distribution. In this work, we leverage self-supervised
learning from longitudinal and multimodal archives to address these challenges.
We curate an unlabeled set of patients with CT scans and linked electronic
health records from our home institution to power joint embedding predictive
architecture (JEPA) pretraining. After supervised finetuning, we show that our
approach outperforms an unregularized multimodal model and imaging-only model
in an internal cohort (ours: 0.91, multimodal: 0.88, imaging-only: 0.73 AUC),
but underperforms in an external cohort (ours: 0.72, imaging-only: 0.75 AUC).
We develop a synthetic environment that characterizes the context in which JEPA
may underperform. This work innovates an approach that leverages unlabeled
multimodal medical archives to improve predictive models and demonstrates its
advantages and limitations in pulmonary nodule diagnosis.

</details>


### [81] [Efficient Multimodal Dataset Distillation via Generative Models](https://arxiv.org/abs/2509.15472)
*Zhenghao Zhao,Haoxuan Wang,Junyi Wu,Yuzhang Shang,Gaowen Liu,Yan Yan*

Main category: cs.CV

TL;DR: 本文提出了一种名为EDGE的高效多模态数据集蒸馏生成方法，旨在解决现有方法计算资源需求高和蒸馏速度慢的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态数据集蒸馏方法受限于匹配训练轨迹算法，导致计算资源需求高，处理时间长。此外，生成模型蒸馏多模态数据集时，存在生成图像和标题之间缺乏相关性以及生成样本缺乏多样性的问题。

Method: 本文提出了一种新的生成模型训练流程，包括双向对比损失和多样性损失，并提出了一种标题合成策略，通过引入更多文本信息来提高文本到图像的检索性能。

Result: 在Flickr30K、COCO和CC3M数据集上的实验结果表明，该方法在性能和效率上均优于现有方法。该方法比最先进的方法快18倍。

Conclusion: 本文提出的EDGE方法能够高效地进行多模态数据集蒸馏，并在性能和效率上都取得了显著的提升。

Abstract: Dataset distillation aims to synthesize a small dataset from a large dataset,
enabling the model trained on it to perform well on the original dataset. With
the blooming of large language models and multimodal large language models, the
importance of multimodal datasets, particularly image-text datasets, has grown
significantly. However, existing multimodal dataset distillation methods are
constrained by the Matching Training Trajectories algorithm, which
significantly increases the computing resource requirement, and takes days to
process the distillation. In this work, we introduce EDGE, a generative
distillation method for efficient multimodal dataset distillation.
Specifically, we identify two key challenges of distilling multimodal datasets
with generative models: 1) The lack of correlation between generated images and
captions. 2) The lack of diversity among generated samples. To address the
aforementioned issues, we propose a novel generative model training workflow
with a bi-directional contrastive loss and a diversity loss. Furthermore, we
propose a caption synthesis strategy to further improve text-to-image retrieval
performance by introducing more text information. Our method is evaluated on
Flickr30K, COCO, and CC3M datasets, demonstrating superior performance and
efficiency compared to existing approaches. Notably, our method achieves
results 18x faster than the state-of-the-art method.

</details>


### [82] [OpenViGA: Video Generation for Automotive Driving Scenes by Streamlining and Fine-Tuning Open Source Models with Public Data](https://arxiv.org/abs/2509.15479)
*Björn Möller,Zhengyang Li,Malte Stelzer,Thomas Graave,Fabian Bettels,Muaaz Ataya,Tim Fingscheidt*

Main category: cs.CV

TL;DR: OpenViGA是一个开放的视频生成系统，用于生成汽车驾驶场景，它通过利用预训练的开源模型并在公共可用的汽车数据上进行微调来实现。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成系统模型大，训练资源需求高，设计选择的洞察力有限，缺乏公开的代码和数据集。

Method: 该系统由图像tokenizer、世界模型和视频解码器三个组件组成，并对这三个组件进行了单独的定量和定性评估。该系统完全建立在强大的预训练开源模型的基础上，并通过公共可用的汽车数据进行微调。通过简化组件的接口，构建了一个连贯的视频生成系统。

Result: 在256x256图像尺寸和4 fps的情况下，能够逐帧预测真实的驾驶场景视频，且只有一帧的算法延迟。

Conclusion: OpenViGA的底层模型和数据是公开可用的，允许完全复现，代码和模型也在Github上发布。

Abstract: Recent successful video generation systems that predict and create realistic
automotive driving scenes from short video inputs assign tokenization, future
state prediction (world model), and video decoding to dedicated models. These
approaches often utilize large models that require significant training
resources, offer limited insight into design choices, and lack publicly
available code and datasets. In this work, we address these deficiencies and
present OpenViGA, an open video generation system for automotive driving
scenes. Our contributions are: Unlike several earlier works for video
generation, such as GAIA-1, we provide a deep analysis of the three components
of our system by separate quantitative and qualitative evaluation: Image
tokenizer, world model, video decoder. Second, we purely build upon powerful
pre-trained open source models from various domains, which we fine-tune by
publicly available automotive data (BDD100K) on GPU hardware at academic scale.
Third, we build a coherent video generation system by streamlining interfaces
of our components. Fourth, due to public availability of the underlying models
and data, we allow full reproducibility. Finally, we also publish our code and
models on Github. For an image size of 256x256 at 4 fps we are able to predict
realistic driving scene videos frame-by-frame with only one frame of
algorithmic latency.

</details>


### [83] [Comparing Computational Pathology Foundation Models using Representational Similarity Analysis](https://arxiv.org/abs/2509.15482)
*Vaibhav Mishra,William Lotter*

Main category: cs.CV

TL;DR: 本研究系统地分析了六个计算病理学基础模型的表征空间，利用计算神经科学中的技术。


<details>
  <summary>Details</summary>
Motivation: 了解计算病理学中基础模型学习表征的结构和变异性，现有研究较少关注。

Method: 使用来自TCGA的H&E图像块，通过表征相似性分析来比较不同模型的表征空间。

Result: UNI2和Virchow2的表征结构最具独特性，Prov-Gigapath的平均相似度最高。染色归一化降低了所有模型的幻灯片依赖性。视觉语言模型表现出相对紧凑的表征。

Conclusion: 研究结果突出了提高对幻灯片特定特征的鲁棒性的机会，为模型集成策略提供了信息，并深入了解了训练范式如何塑造模型表征。

Abstract: Foundation models are increasingly developed in computational pathology
(CPath) given their promise in facilitating many downstream tasks. While recent
studies have evaluated task performance across models, less is known about the
structure and variability of their learned representations. Here, we
systematically analyze the representational spaces of six CPath foundation
models using techniques popularized in computational neuroscience. The models
analyzed span vision-language contrastive learning (CONCH, PLIP, KEEP) and
self-distillation (UNI (v2), Virchow (v2), Prov-GigaPath) approaches. Through
representational similarity analysis using H&E image patches from TCGA, we find
that UNI2 and Virchow2 have the most distinct representational structures,
whereas Prov-Gigapath has the highest average similarity across models. Having
the same training paradigm (vision-only vs. vision-language) did not guarantee
higher representational similarity. The representations of all models showed a
high slide-dependence, but relatively low disease-dependence. Stain
normalization decreased slide-dependence for all models by a range of 5.5%
(CONCH) to 20.5% (PLIP). In terms of intrinsic dimensionality, vision-language
models demonstrated relatively compact representations, compared to the more
distributed representations of vision-only models. These findings highlight
opportunities to improve robustness to slide-specific features, inform model
ensembling strategies, and provide insights into how training paradigms shape
model representations. Our framework is extendable across medical imaging
domains, where probing the internal representations of foundation models can
help ensure effective development and deployment.

</details>


### [84] [SmolRGPT: Efficient Spatial Reasoning for Warehouse Environments with 600M Parameters](https://arxiv.org/abs/2509.15490)
*Abdarahmane Traore,Éric Hervet,Andy Couturier*

Main category: cs.CV

TL;DR: 提出了一种名为SmolRGPT的小型视觉语言架构，用于在资源受限环境中进行空间推理。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型计算和内存需求高，难以在资源受限环境中部署，而这些环境又需要高效和强大的空间理解能力。

Method: SmolRGPT集成了RGB和深度线索，显式地结合了区域级的空间推理，并采用了一个三阶段课程来逐步对齐视觉和语言特征，实现空间关系理解，并适应特定任务的数据集。

Result: SmolRGPT仅用600M参数，在仓库空间推理基准测试上取得了有竞争力的结果，与更大的模型性能相当或超过。

Conclusion: SmolRGPT展示了在不牺牲核心空间推理能力的情况下，在现实环境中实现高效、可部署的多模态智能的潜力。

Abstract: Recent advances in vision-language models (VLMs) have enabled powerful
multimodal reasoning, but state-of-the-art approaches typically rely on
extremely large models with prohibitive computational and memory requirements.
This makes their deployment challenging in resource-constrained environments
such as warehouses, robotics, and industrial applications, where both
efficiency and robust spatial understanding are critical. In this work, we
present SmolRGPT, a compact vision-language architecture that explicitly
incorporates region-level spatial reasoning by integrating both RGB and depth
cues. SmolRGPT employs a three-stage curriculum that progressively align visual
and language features, enables spatial relationship understanding, and adapts
to task-specific datasets. We demonstrate that with only 600M parameters,
SmolRGPT achieves competitive results on challenging warehouse spatial
reasoning benchmarks, matching or exceeding the performance of much larger
alternatives. These findings highlight the potential for efficient, deployable
multimodal intelligence in real-world settings without sacrificing core spatial
reasoning capabilities. The code of the experimentation will be available at:
https://github.com/abtraore/SmolRGPT

</details>


### [85] [Lynx: Towards High-Fidelity Personalized Video Generation](https://arxiv.org/abs/2509.15496)
*Shen Sang,Tiancheng Zhi,Tianpei Gu,Jing Liu,Linjie Luo*

Main category: cs.CV

TL;DR: Lynx, a model for personalized video synthesis, uses adapters to ensure identity fidelity.


<details>
  <summary>Details</summary>
Motivation: To advance the state of personalized video generation.

Method: Introducing two lightweight adapters: ID-adapter (Perceiver Resampler to convert ArcFace-derived facial embeddings) and Ref-adapter (integrates dense VAE features).

Result: Lynx demonstrated superior face resemblance, competitive prompt following, and strong video quality on a curated benchmark.

Conclusion: Lynx advances the state of personalized video generation.

Abstract: We present Lynx, a high-fidelity model for personalized video synthesis from
a single input image. Built on an open-source Diffusion Transformer (DiT)
foundation model, Lynx introduces two lightweight adapters to ensure identity
fidelity. The ID-adapter employs a Perceiver Resampler to convert
ArcFace-derived facial embeddings into compact identity tokens for
conditioning, while the Ref-adapter integrates dense VAE features from a frozen
reference pathway, injecting fine-grained details across all transformer layers
through cross-attention. These modules collectively enable robust identity
preservation while maintaining temporal coherence and visual realism. Through
evaluation on a curated benchmark of 40 subjects and 20 unbiased prompts, which
yielded 800 test cases, Lynx has demonstrated superior face resemblance,
competitive prompt following, and strong video quality, thereby advancing the
state of personalized video generation.

</details>


### [86] [Backdoor Mitigation via Invertible Pruning Masks](https://arxiv.org/abs/2509.15497)
*Kealan Dunnett,Reza Arablouei,Dimity Miller,Volkan Dedeoglu,Raja Jurdak*

Main category: cs.CV

TL;DR: 提出了一种新的剪枝方法，该方法具有学习选择机制来识别对主要任务和后门任务至关重要的参数，以及旨在同时实现两个互补目标的可逆剪枝掩码：消除后门任务，同时通过逆掩码保留它。


<details>
  <summary>Details</summary>
Motivation: 现有的基于剪枝的方法通常无法准确识别和移除导致后门行为的特定参数。尽管基于微调的防御在最近的文献中占主导地位，这主要是由于它们优越的性能，但剪枝仍然是一种引人注目的替代方案，在低数据状态下提供更大的可解释性和改进的鲁棒性。

Method: 提出了一种新的剪枝方法，该方法具有学习选择机制来识别对主要任务和后门任务至关重要的参数，以及旨在同时实现两个互补目标的可逆剪枝掩码：消除后门任务，同时通过逆掩码保留它。我们将其表述为一个双层优化问题，该问题共同学习选择变量、稀疏可逆掩码和源自干净数据的样本特定后门扰动。内部问题使用逆掩码合成候选触发器，而外部问题优化掩码以抑制后门行为，而不损害干净任务的准确性。

Result: 大量的实验表明，我们的方法优于现有的基于剪枝的后门缓解方法，在有限的数据条件下保持了强大的性能，并且与最先进的微调方法相比，取得了具有竞争力的结果。值得注意的是，所提出的方法在成功进行后门缓解后，在恢复受损样本的正确预测方面特别有效。

Conclusion: 该方法在后门防御方面优于现有的剪枝方法，并在数据有限的情况下表现良好，与微调方法相比具有竞争力。

Abstract: Model pruning has gained traction as a promising defense strategy against
backdoor attacks in deep learning. However, existing pruning-based approaches
often fall short in accurately identifying and removing the specific parameters
responsible for inducing backdoor behaviors. Despite the dominance of
fine-tuning-based defenses in recent literature, largely due to their superior
performance, pruning remains a compelling alternative, offering greater
interpretability and improved robustness in low-data regimes. In this paper, we
propose a novel pruning approach featuring a learned \emph{selection} mechanism
to identify parameters critical to both main and backdoor tasks, along with an
\emph{invertible} pruning mask designed to simultaneously achieve two
complementary goals: eliminating the backdoor task while preserving it through
the inverse mask. We formulate this as a bi-level optimization problem that
jointly learns selection variables, a sparse invertible mask, and
sample-specific backdoor perturbations derived from clean data. The inner
problem synthesizes candidate triggers using the inverse mask, while the outer
problem refines the mask to suppress backdoor behavior without impairing
clean-task accuracy. Extensive experiments demonstrate that our approach
outperforms existing pruning-based backdoor mitigation approaches, maintains
strong performance under limited data conditions, and achieves competitive
results compared to state-of-the-art fine-tuning approaches. Notably, the
proposed approach is particularly effective in restoring correct predictions
for compromised samples after successful backdoor mitigation.

</details>


### [87] [MEC-Quant: Maximum Entropy Coding for Extremely Low Bit Quantization-Aware Training](https://arxiv.org/abs/2509.15514)
*Junbiao Pang,Tianyang Cai,Baochang Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的量化感知训练方法，称为最大熵编码量化 (MEC-Quant)，以解决量化引入的偏差问题，尤其是在极低位设置下。


<details>
  <summary>Details</summary>
Motivation: 当前量化感知训练 (QAT) 的性能仍然不如全精度 (FP) 模型。量化不可避免地会将偏差引入到学习到的表示中，尤其是在极低位设置下。

Method: 提出 MEC-Quant，它显式地优化表示的结构，从而减少学习到的表示的偏差，并更好地泛化到看不见的分布内样本。利用有损数据编码中的最小编码长度作为熵的计算上易于处理的替代，并进一步推导了基于混合专家 (MOE) 的目标的可扩展重构，这不仅允许快速计算，而且还可以处理权重或激活值的长尾分布。

Result: 在计算机视觉任务上的大量实验证明了其优越性。通过 MEC-Quant，QAT 的极限首次被推到了 x 位激活，并且 MEC-Quant 的准确性与 FP 模型相当甚至超过了 FP 模型。

Conclusion: MEC-Quant 为 QAT 建立了新的技术水平。

Abstract: Quantization-Aware Training (QAT) has driven much attention to produce
efficient neural networks. Current QAT still obtains inferior performances
compared with the Full Precision (FP) counterpart. In this work, we argue that
quantization inevitably introduce biases into the learned representation,
especially under the extremely low-bit setting. To cope with this issue, we
propose Maximum Entropy Coding Quantization (MEC-Quant), a more principled
objective that explicitly optimizes on the structure of the representation, so
that the learned representation is less biased and thus generalizes better to
unseen in-distribution samples. To make the objective end-to-end trainable, we
propose to leverage the minimal coding length in lossy data coding as a
computationally tractable surrogate for the entropy, and further derive a
scalable reformulation of the objective based on Mixture Of Experts (MOE) that
not only allows fast computation but also handles the long-tailed distribution
for weights or activation values. Extensive experiments on various tasks on
computer vision tasks prove its superiority. With MEC-Qaunt, the limit of QAT
is pushed to the x-bit activation for the first time and the accuracy of
MEC-Quant is comparable to or even surpass the FP counterpart. Without bells
and whistles, MEC-Qaunt establishes a new state of the art for QAT.

</details>


### [88] [GUI-ARP: Enhancing Grounding with Adaptive Region Perception for GUI Agents](https://arxiv.org/abs/2509.15532)
*Xianhang Ye,Yiqing Li,Wei Dai,Miancan Liu,Ziyuan Chen,Zhangye Han,Hongbo Min,Jinkui Ren,Xiantao Zhang,Wen Yang,Zhi Jin*

Main category: cs.CV

TL;DR: GUI-ARP 是一种新的框架，它通过自适应多阶段推理来实现高分辨率屏幕截图中的细粒度定位。


<details>
  <summary>Details</summary>
Motivation: 现有的 GUI grounding 方法在高分辨率屏幕截图中进行细粒度定位时经常遇到困难。

Method: GUI-ARP 采用自适应区域感知 (ARP) 和自适应阶段控制 (ASC)，动态利用视觉注意力来裁剪任务相关区域，并调整其推理策略，对简单情况执行单阶段推理，对更复杂的情况执行多阶段分析。这是通过一个两阶段训练管道实现的，该管道集成了基于 Group Relative Policy Optimization (GRPO) 的监督微调和强化微调。

Result: GUI-ARP 在具有挑战性的 GUI grounding 基准测试中实现了最先进的性能，一个 7B 模型在 ScreenSpot-Pro 上达到了 60.8% 的准确率，在 UI-Vision 基准测试中达到了 30.9% 的准确率。值得注意的是，GUI-ARP-7B 显示出与开源 72B 模型（UI-TARS-72B 为 38.1%）和专有模型的强大竞争力。

Conclusion: GUI-ARP 是一种有效的 GUI grounding 框架，它可以通过自适应多阶段推理来实现高分辨率屏幕截图中的细粒度定位。

Abstract: Existing GUI grounding methods often struggle with fine-grained localization
in high-resolution screenshots. To address this, we propose GUI-ARP, a novel
framework that enables adaptive multi-stage inference. Equipped with the
proposed Adaptive Region Perception (ARP) and Adaptive Stage Controlling (ASC),
GUI-ARP dynamically exploits visual attention for cropping task-relevant
regions and adapts its inference strategy, performing a single-stage inference
for simple cases and a multi-stage analysis for more complex scenarios. This is
achieved through a two-phase training pipeline that integrates supervised
fine-tuning with reinforcement fine-tuning based on Group Relative Policy
Optimization (GRPO). Extensive experiments demonstrate that the proposed
GUI-ARP achieves state-of-the-art performance on challenging GUI grounding
benchmarks, with a 7B model reaching 60.8% accuracy on ScreenSpot-Pro and 30.9%
on UI-Vision benchmark. Notably, GUI-ARP-7B demonstrates strong competitiveness
against open-source 72B models (UI-TARS-72B at 38.1%) and proprietary models.

</details>


### [89] [SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models](https://arxiv.org/abs/2509.15536)
*Sen Wang,Jingyi Tian,Le Wang,Zhimin Liao,Jiayi Li,Huaiyi Dong,Kun Xia,Sanping Zhou,Wei Tang,Hua Gang*

Main category: cs.CV

TL;DR: SAMPO: A hybrid framework combining visual autoregressive modeling for intra-frame generation with causal modeling for next-frame generation.


<details>
  <summary>Details</summary>
Motivation: Existing autoregressive world models struggle with visually coherent predictions due to disrupted spatial structure, inefficient decoding, and inadequate motion modeling.

Method: Integrates temporal causal decoding with bidirectional spatial attention, asymmetric multi-scale tokenizer, and trajectory-aware motion prompt module.

Result: Achieves competitive performance in action-conditioned video prediction and model-based control, improving generation quality with 4.4x faster inference.

Conclusion: Demonstrates ability to generalize to unseen tasks and benefit from larger model sizes.

Abstract: World models allow agents to simulate the consequences of actions in imagined
environments for planning, control, and long-horizon decision-making. However,
existing autoregressive world models struggle with visually coherent
predictions due to disrupted spatial structure, inefficient decoding, and
inadequate motion modeling. In response, we propose \textbf{S}cale-wise
\textbf{A}utoregression with \textbf{M}otion \textbf{P}r\textbf{O}mpt
(\textbf{SAMPO}), a hybrid framework that combines visual autoregressive
modeling for intra-frame generation with causal modeling for next-frame
generation. Specifically, SAMPO integrates temporal causal decoding with
bidirectional spatial attention, which preserves spatial locality and supports
parallel decoding within each scale. This design significantly enhances both
temporal consistency and rollout efficiency. To further improve dynamic scene
understanding, we devise an asymmetric multi-scale tokenizer that preserves
spatial details in observed frames and extracts compact dynamic representations
for future frames, optimizing both memory usage and model performance.
Additionally, we introduce a trajectory-aware motion prompt module that injects
spatiotemporal cues about object and robot trajectories, focusing attention on
dynamic regions and improving temporal consistency and physical realism.
Extensive experiments show that SAMPO achieves competitive performance in
action-conditioned video prediction and model-based control, improving
generation quality with 4.4$\times$ faster inference. We also evaluate SAMPO's
zero-shot generalization and scaling behavior, demonstrating its ability to
generalize to unseen tasks and benefit from larger model sizes.

</details>


### [90] [Beyond Words: Enhancing Desire, Emotion, and Sentiment Recognition with Non-Verbal Cues](https://arxiv.org/abs/2509.15540)
*Wei Chen,Tongguan Wang,Feiyue Xue,Junkai Li,Hui Liu,Ying Sha*

Main category: cs.CV

TL;DR: 提出了一种对称双向多模态学习框架，用于欲望、情感和情绪识别，通过文本和图像模态之间的相互引导，有效捕捉图像中与意图相关的表征。


<details>
  <summary>Details</summary>
Motivation: 现有的情感分析方法主要强调语言线索，忽略了图像作为补充的非语言线索。多模态学习已经促进了情感和情绪识别，但专门针对人类欲望理解的多模态方法仍有待探索。

Method: 提出了一种对称双向多模态学习框架，该框架在文本和图像模态之间强制执行相互指导，以有效捕获图像中与意图相关的表示。使用低分辨率图像获得全局视觉表示以进行跨模态对齐，而将高分辨率图像划分为子图像，并使用掩码图像建模进行建模，以增强捕获细粒度局部特征的能力。引入了文本引导的图像解码器和图像引导的文本解码器，以促进图像信息的局部和全局表示中的深度跨模态交互。此外，为了平衡感知增益和计算成本，采用了一种混合尺度图像策略，其中将高分辨率图像裁剪为子图像以进行掩码建模。

Result: 在 MSED 数据集上进行评估，实验结果表明，与现有技术相比，该方法始终如一地改进，验证了所提出方法的有效性。具体而言，我们的方法优于现有方法，在欲望理解方面提高了 1.1% 的 F1 分数，在情感识别方面提高了 0.6%，在情感分析方面提高了 0.9%。

Conclusion: 该论文提出了一种新的多模态学习框架，并在欲望理解、情感识别和情感分析任务上取得了state-of-the-art的结果。

Abstract: Desire, as an intention that drives human behavior, is closely related to
both emotion and sentiment. Multimodal learning has advanced sentiment and
emotion recognition, but multimodal approaches specially targeting human desire
understanding remain underexplored. And existing methods in sentiment analysis
predominantly emphasize verbal cues and overlook images as complementary
non-verbal cues. To address these gaps, we propose a Symmetrical Bidirectional
Multimodal Learning Framework for Desire, Emotion, and Sentiment Recognition,
which enforces mutual guidance between text and image modalities to effectively
capture intention-related representations in the image. Specifically,
low-resolution images are used to obtain global visual representations for
cross-modal alignment, while high resolution images are partitioned into
sub-images and modeled with masked image modeling to enhance the ability to
capture fine-grained local features. A text-guided image decoder and an
image-guided text decoder are introduced to facilitate deep cross-modal
interaction at both local and global representations of image information.
Additionally, to balance perceptual gains with computation cost, a mixed-scale
image strategy is adopted, where high-resolution images are cropped into
sub-images for masked modeling. The proposed approach is evaluated on MSED, a
multimodal dataset that includes a desire understanding benchmark, as well as
emotion and sentiment recognition. Experimental results indicate consistent
improvements over other state-of-the-art methods, validating the effectiveness
of our proposed method. Specifically, our method outperforms existing
approaches, achieving F1-score improvements of 1.1% in desire understanding,
0.6% in emotion recognition, and 0.9% in sentiment analysis. Our code is
available at: https://github.com/especiallyW/SyDES.

</details>


### [91] [Enhancing Sa2VA for Referent Video Object Segmentation: 2nd Solution for 7th LSVOS RVOS Track](https://arxiv.org/abs/2509.15546)
*Ran Hong,Feng Lu,Leilei Cao,An Yan,Youhai Jiang,Fengjie Zhu*

Main category: cs.CV

TL;DR: 提出了一种无需训练的框架，通过引入视频-语言检查器和关键帧采样器，显著提高了 Sa2VA 在 RVOS 任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在分割视频中所有符合自然语言描述的对象，弥合视觉和语言理解之间的差距。现有方法 Sa2VA 结合了大型语言模型 (LLM) 和 SAM，利用 LLM 强大的视频推理能力来指导视频分割，但仍有提升空间。

Method: 引入了两个关键组件：(1) 视频-语言检查器，用于显式验证查询中描述的主题和动作是否实际出现在视频中，从而减少误报；(2) 关键帧采样器，自适应地选择信息丰富的帧，以更好地捕获早期对象外观和远程时间上下文。

Result: 在 MeViS 测试集上实现了 64.14% 的 J&F 分数，在 ICCV 2025 的第七届 LSVOS 挑战赛的 RVOS 赛道中排名第二。

Conclusion: 该方法无需额外训练即可显著提高 RVOS 任务的性能。

Abstract: Referential Video Object Segmentation (RVOS) aims to segment all objects in a
video that match a given natural language description, bridging the gap between
vision and language understanding. Recent work, such as Sa2VA, combines Large
Language Models (LLMs) with SAM~2, leveraging the strong video reasoning
capability of LLMs to guide video segmentation. In this work, we present a
training-free framework that substantially improves Sa2VA's performance on the
RVOS task. Our method introduces two key components: (1) a Video-Language
Checker that explicitly verifies whether the subject and action described in
the query actually appear in the video, thereby reducing false positives; and
(2) a Key-Frame Sampler that adaptively selects informative frames to better
capture both early object appearances and long-range temporal context. Without
any additional training, our approach achieves a J&F score of 64.14% on the
MeViS test set, ranking 2nd place in the RVOS track of the 7th LSVOS Challenge
at ICCV 2025.

</details>


### [92] [MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild](https://arxiv.org/abs/2509.15548)
*Deming Li,Kaiwen Jiang,Yutao Tang,Ravi Ramamoorthi,Rama Chellappa,Cheng Peng*

Main category: cs.CV

TL;DR: 提出了一种新的框架 MS-GS，它使用 3DGS 在稀疏视图场景中设计了多外观功能。


<details>
  <summary>Details</summary>
Motivation: 现有的神经辐射场 (NeRF) 和 3D 高斯溅射 (3DGS) 的改进在这些领域有所改进，但它们往往过度平滑并且容易过度拟合。野生照片集通常包含有限的图像量，并呈现出多种外观，例如，在一天中的不同时间或季节拍摄，这对场景重建和新颖的视图合成提出了重大挑战。

Method: 该方法建立在单眼深度估计中提取的几何先验之上。关键在于提取和利用具有运动结构 (SfM) 点锚定算法的局部语义区域，以实现可靠的对齐和几何线索。然后，为了引入多视图约束，我们提出了一系列在虚拟视图中进行几何引导的监督，采用精细和粗略的方案，以鼓励 3D 一致性并减少过度拟合。

Result: MS-GS 在各种具有挑战性的稀疏视图和多外观条件下实现了逼真的渲染，并且在不同的数据集中显着优于现有方法。

Conclusion: MS-GS 是一种很有前途的用于解决 NeRF 和 3DGS 中的过度平滑和过度拟合问题，并且在稀疏视图场景中设计了多外观功能的框架。

Abstract: In-the-wild photo collections often contain limited volumes of imagery and
exhibit multiple appearances, e.g., taken at different times of day or seasons,
posing significant challenges to scene reconstruction and novel view synthesis.
Although recent adaptations of Neural Radiance Field (NeRF) and 3D Gaussian
Splatting (3DGS) have improved in these areas, they tend to oversmooth and are
prone to overfitting. In this paper, we present MS-GS, a novel framework
designed with Multi-appearance capabilities in Sparse-view scenarios using
3DGS. To address the lack of support due to sparse initializations, our
approach is built on the geometric priors elicited from monocular depth
estimations. The key lies in extracting and utilizing local semantic regions
with a Structure-from-Motion (SfM) points anchored algorithm for reliable
alignment and geometry cues. Then, to introduce multi-view constraints, we
propose a series of geometry-guided supervision at virtual views in a
fine-grained and coarse scheme to encourage 3D consistency and reduce
overfitting. We also introduce a dataset and an in-the-wild experiment setting
to set up more realistic benchmarks. We demonstrate that MS-GS achieves
photorealistic renderings under various challenging sparse-view and
multi-appearance conditions and outperforms existing approaches significantly
across different datasets.

</details>


### [93] [Diffusion-Based Cross-Modal Feature Extraction for Multi-Label Classification](https://arxiv.org/abs/2509.15553)
*Tian Lan,Yiming Zheng,Jianxin Yin*

Main category: cs.CV

TL;DR: Diff-Feat框架从预训练的扩散-Transformer模型中提取中间特征，用于图像和文本的多标签分类任务，并在下游任务中融合它们。


<details>
  <summary>Details</summary>
Motivation: 多标签分类应用广泛，依赖于能够捕获多标签交互的强大表示。

Method: 该方法提出Diff-Feat框架，通过启发式局部搜索算法寻找最优的“图像-文本”x“块-时间步”对，并使用简单的融合-线性投影和加法操作。

Result: 在MS-COCO-enhanced数据集上达到98.6% mAP，在Visual Genome 500数据集上达到45.7% mAP，超过了现有的CNN、图和Transformer基线模型。

Conclusion: Diff-Feat框架能够形成比单模态方法更紧密的语义聚类，并在多标签分类任务上取得了显著的性能提升。

Abstract: Multi-label classification has broad applications and depends on powerful
representations capable of capturing multi-label interactions. We introduce
\textit{Diff-Feat}, a simple but powerful framework that extracts intermediate
features from pre-trained diffusion-Transformer models for images and text, and
fuses them for downstream tasks. We observe that for vision tasks, the most
discriminative intermediate feature along the diffusion process occurs at the
middle step and is located in the middle block in Transformer. In contrast, for
language tasks, the best feature occurs at the noise-free step and is located
in the deepest block. In particular, we observe a striking phenomenon across
varying datasets: a mysterious "Layer $12$" consistently yields the best
performance on various downstream classification tasks for images (under
DiT-XL/2-256$\times$256). We devise a heuristic local-search algorithm that
pinpoints the locally optimal "image-text"$\times$"block-timestep" pair among a
few candidates, avoiding an exhaustive grid search. A simple fusion-linear
projection followed by addition-of the selected representations yields
state-of-the-art performance: 98.6\% mAP on MS-COCO-enhanced and 45.7\% mAP on
Visual Genome 500, surpassing strong CNN, graph, and Transformer baselines by a
wide margin. t-SNE and clustering metrics further reveal that
\textit{Diff-Feat} forms tighter semantic clusters than unimodal counterparts.
The code is available at https://github.com/lt-0123/Diff-Feat.

</details>


### [94] [From Development to Deployment of AI-assisted Telehealth and Screening for Vision- and Hearing-threatening diseases in resource-constrained settings: Field Observations, Challenges and Way Forward](https://arxiv.org/abs/2509.15558)
*Mahesh Shakya,Bijay Adhikari,Nirsara Shrestha,Bipin Koirala,Arun Adhikari,Prasanta Poudyal,Luna Mathema,Sarbagya Buddhacharya,Bijay Khatri,Bishesh Khanal*

Main category: cs.CV

TL;DR: 本文旨在探讨在资源受限环境下，如何利用人工智能辅助远程医疗和大规模筛查来预防可能导致失明和失聪的疾病。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的环境中，缺乏专家和筛查设备，导致可预防的残疾问题突出。人工智能辅助的筛查和远程医疗有潜力扩大早期检测范围，但实际部署面临挑战，尤其是在纸质工作流程中。

Method: 通过早期原型设计、影子部署和持续反馈的迭代、跨学科合作，构建共享理解，并减少从纸质工作流程过渡到人工智能工作流程的可用性障碍。

Result: 公共数据集和人工智能模型尽管性能不佳，但仍然非常有用。此外，还需要基于人工智能的自动图像质量检查，以在高通量筛查中捕获可分级的图像。

Conclusion: 人工智能开发和工作流程数字化应被视为端到端的迭代协同设计过程。记录这些实际挑战和经验教训，旨在填补在资源受限环境中构建实际人工智能辅助远程医疗和大规模筛查计划的背景化、可操作的领域知识的空白。

Abstract: Vision- and hearing-threatening diseases cause preventable disability,
especially in resource-constrained settings(RCS) with few specialists and
limited screening setup. Large scale AI-assisted screening and telehealth has
potential to expand early detection, but practical deployment is challenging in
paper-based workflows and limited documented field experience exist to build
upon. We provide insights on challenges and ways forward in development to
adoption of scalable AI-assisted Telehealth and screening in such settings.
Specifically, we find that iterative, interdisciplinary collaboration through
early prototyping, shadow deployment and continuous feedback is important to
build shared understanding as well as reduce usability hurdles when
transitioning from paper-based to AI-ready workflows. We find public datasets
and AI models highly useful despite poor performance due to domain shift. In
addition, we find the need for automated AI-based image quality check to
capture gradable images for robust screening in high-volume camps.
  Our field learning stress the importance of treating AI development and
workflow digitization as an end-to-end, iterative co-design process. By
documenting these practical challenges and lessons learned, we aim to address
the gap in contextual, actionable field knowledge for building real-world
AI-assisted telehealth and mass-screening programs in RCS.

</details>


### [95] [DC-Mamba: Bi-temporal deformable alignment and scale-sparse enhancement for remote sensing change detection](https://arxiv.org/abs/2509.15563)
*Min Sun,Fenghui Guo*

Main category: cs.CV

TL;DR: DC-Mamba是一种用于遥感变化检测的框架，它通过对齐和增强来提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有遥感变化检测方法缺乏处理几何不对齐的机制，难以区分细微变化和噪声。

Method: DC-Mamba框架包含Bi-Temporal Deformable Alignment (BTDA)模块用于校正空间不对齐，以及Scale-Sparse Change Amplifier (SSCA)模块用于增强变化信号并抑制噪声。

Result: DC-Mamba在F1-score和IoU指标上均优于ChangeMamba基线。

Conclusion: 提出的“对齐-增强”策略有效，为遥感变化检测提供了一种稳健且易于部署的解决方案。

Abstract: Remote sensing change detection (RSCD) is vital for identifying land-cover
changes, yet existing methods, including state-of-the-art State Space Models
(SSMs), often lack explicit mechanisms to handle geometric misalignments and
struggle to distinguish subtle, true changes from noise.To address this, we
introduce DC-Mamba, an "align-then-enhance" framework built upon the
ChangeMamba backbone. It integrates two lightweight, plug-and-play modules: (1)
Bi-Temporal Deformable Alignment (BTDA), which explicitly introduces geometric
awareness to correct spatial misalignments at the semantic feature level; and
(2) a Scale-Sparse Change Amplifier(SSCA), which uses multi-source cues to
selectively amplify high-confidence change signals while suppressing noise
before the final classification. This synergistic design first establishes
geometric consistency with BTDA to reduce pseudo-changes, then leverages SSCA
to sharpen boundaries and enhance the visibility of small or subtle targets.
Experiments show our method significantly improves performance over the strong
ChangeMamba baseline, increasing the F1-score from 0.5730 to 0.5903 and IoU
from 0.4015 to 0.4187. The results confirm the effectiveness of our
"align-then-enhance" strategy, offering a robust and easily deployable solution
that transparently addresses both geometric and feature-level challenges in
RSCD.

</details>


### [96] [BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent](https://arxiv.org/abs/2509.15566)
*Shaojie Zhang,Ruoceng Zhang,Pei Fu,Shaokang Wang,Jiahui Yang,Xin Du,Shiqi Cui,Bin Qin,Ying Huang,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: 提出了一种名为“Blink-Think-Link”(BTL)的脑启发式框架，用于人机交互，模仿人类与图形界面之间的认知过程。


<details>
  <summary>Details</summary>
Motivation: 现有AI在人机交互中的交互逻辑与自然的人类交互模式显著不同。

Method: 将交互分解为三个阶段：快速检测和关注相关屏幕区域（Blink），高级推理和决策（Think），以及生成用于精确运动控制的可执行命令（Link）。引入了Blink数据生成和BTL奖励两项关键技术创新。

Result: 开发了一个名为BTL-UI的GUI代理模型，在静态GUI理解和动态交互任务中表现出一致的state-of-the-art性能。

Conclusion: 该框架在开发高级GUI代理方面是有效的。

Abstract: In the field of AI-driven human-GUI interaction automation, while rapid
advances in multimodal large language models and reinforcement fine-tuning
techniques have yielded remarkable progress, a fundamental challenge persists:
their interaction logic significantly deviates from natural human-GUI
communication patterns. To fill this gap, we propose "Blink-Think-Link" (BTL),
a brain-inspired framework for human-GUI interaction that mimics the human
cognitive process between users and graphical interfaces. The system decomposes
interactions into three biologically plausible phases: (1) Blink - rapid
detection and attention to relevant screen areas, analogous to saccadic eye
movements; (2) Think - higher-level reasoning and decision-making, mirroring
cognitive planning; and (3) Link - generation of executable commands for
precise motor control, emulating human action selection mechanisms.
Additionally, we introduce two key technical innovations for the BTL framework:
(1) Blink Data Generation - an automated annotation pipeline specifically
optimized for blink data, and (2) BTL Reward -- the first rule-based reward
mechanism that enables reinforcement learning driven by both process and
outcome. Building upon this framework, we develop a GUI agent model named
BTL-UI, which demonstrates consistent state-of-the-art performance across both
static GUI understanding and dynamic interaction tasks in comprehensive
benchmarks. These results provide conclusive empirical validation of the
framework's efficacy in developing advanced GUI Agents.

</details>


### [97] [Towards Size-invariant Salient Object Detection: A Generic Evaluation and Optimization Approach](https://arxiv.org/abs/2509.15573)
*Shilong Bao,Qianqian Xu,Feiran Li,Boyu Han,Zhiyong Yang,Xiaochun Cao,Qingming Huang*

Main category: cs.CV

TL;DR: 提出了一种尺寸不变的显著性目标检测评估框架（SIEva）和优化框架（SIOpt），以解决现有评估指标对不同大小物体的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有显著性目标检测（SOD）指标对不同大小的显著性物体存在尺寸敏感性，导致评估结果偏差和性能下降。

Method: 1. 分析现有SOD指标的尺寸敏感性；2. 提出尺寸不变评估框架（SIEva），分别评估每个可分离的组件然后聚合结果；3. 开发尺寸不变优化框架（SIOpt），增强对各种大小的显著性物体的检测。

Result: 综合实验表明，该方法是有效的。

Conclusion: 该论文提出了一种尺寸不变的评估和优化框架，可以有效解决现有SOD指标对不同大小物体评估的偏差问题，并提高检测性能。

Abstract: This paper investigates a fundamental yet underexplored issue in Salient
Object Detection (SOD): the size-invariant property for evaluation protocols,
particularly in scenarios when multiple salient objects of significantly
different sizes appear within a single image. We first present a novel
perspective to expose the inherent size sensitivity of existing widely used SOD
metrics. Through careful theoretical derivations, we show that the evaluation
outcome of an image under current SOD metrics can be essentially decomposed
into a sum of several separable terms, with the contribution of each term being
directly proportional to its corresponding region size. Consequently, the
prediction errors would be dominated by the larger regions, while smaller yet
potentially more semantically important objects are often overlooked, leading
to biased performance assessments and practical degradation. To address this
challenge, a generic Size-Invariant Evaluation (SIEva) framework is proposed.
The core idea is to evaluate each separable component individually and then
aggregate the results, thereby effectively mitigating the impact of size
imbalance across objects. Building upon this, we further develop a dedicated
optimization framework (SIOpt), which adheres to the size-invariant principle
and significantly enhances the detection of salient objects across a broad
range of sizes. Notably, SIOpt is model-agnostic and can be seamlessly
integrated with a wide range of SOD backbones. Theoretically, we also present
generalization analysis of SOD methods and provide evidence supporting the
validity of our new evaluation protocols. Finally, comprehensive experiments
speak to the efficacy of our proposed approach. The code is available at
https://github.com/Ferry-Li/SI-SOD.

</details>


### [98] [Multimodal Learning for Fake News Detection in Short Videos Using Linguistically Verified Data and Heterogeneous Modality Fusion](https://arxiv.org/abs/2509.15578)
*Shanghong Li,Chiam Wen Qi Ruth,Hong Xu,Fang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新的多模态框架HFN，用于检测短视频平台上的假新闻。


<details>
  <summary>Details</summary>
Motivation: 当前方法难以应对短视频内容的动态和多模态特性，假新闻传播迅速，可能导致严重的社会危害。

Method: HFN集成了视频、音频和文本数据，通过决策网络动态调整模态权重，并使用加权多模态特征融合模块来确保在数据不完整的情况下也能保持鲁棒性。此外，作者还构建了一个全面的数据集VESV，专门用于短视频假新闻检测。

Result: 在FakeTT和新收集的VESV数据集上进行的实验表明，HFN在Marco F1指标上比最先进的方法提高了2.71%和4.14%。

Conclusion: 本文建立了一个强大的解决方案，能够有效地识别短视频平台复杂环境下的假新闻，为打击虚假信息开辟了更可靠和全面的途径。

Abstract: The rapid proliferation of short video platforms has necessitated advanced
methods for detecting fake news. This need arises from the widespread influence
and ease of sharing misinformation, which can lead to significant societal
harm. Current methods often struggle with the dynamic and multimodal nature of
short video content. This paper presents HFN, Heterogeneous Fusion Net, a novel
multimodal framework that integrates video, audio, and text data to evaluate
the authenticity of short video content. HFN introduces a Decision Network that
dynamically adjusts modality weights during inference and a Weighted
Multi-Modal Feature Fusion module to ensure robust performance even with
incomplete data. Additionally, we contribute a comprehensive dataset VESV
(VEracity on Short Videos) specifically designed for short video fake news
detection. Experiments conducted on the FakeTT and newly collected VESV
datasets demonstrate improvements of 2.71% and 4.14% in Marco F1 over
state-of-the-art methods. This work establishes a robust solution capable of
effectively identifying fake news in the complex landscape of short video
platforms, paving the way for more reliable and comprehensive approaches in
combating misinformation.

</details>


### [99] [EyePCR: A Comprehensive Benchmark for Fine-Grained Perception, Knowledge Comprehension and Clinical Reasoning in Ophthalmic Surgery](https://arxiv.org/abs/2509.15596)
*Gui Wang,Yang Wennuo,Xusen Ma,Zehao Zhong,Zhuoru Wu,Ende Wu,Rong Qu,Wooi Ping Cheah,Jianfeng Ren,Linlin Shen*

Main category: cs.CV

TL;DR: EyePCR: A new benchmark for ophthalmic surgery analysis to evaluate MLLMs' cognition.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs lack evaluation in high-stakes, domain-specific scenarios like surgery.

Method: Developed EyePCR, a large-scale benchmark with 210k VQAs, a medical knowledge graph, and four reasoning tasks. Also developed EyePCR-MLLM, a domain-adapted variant of Qwen2.5-VL-7B.

Result: EyePCR-MLLM achieves high accuracy in perception and outperforms open-source models in comprehension and reasoning.

Conclusion: EyePCR reveals limitations of current MLLMs in surgical cognition and provides a foundation for improving surgical video understanding models.

Abstract: MLLMs (Multimodal Large Language Models) have showcased remarkable
capabilities, but their performance in high-stakes, domain-specific scenarios
like surgical settings, remains largely under-explored. To address this gap, we
develop \textbf{EyePCR}, a large-scale benchmark for ophthalmic surgery
analysis, grounded in structured clinical knowledge to evaluate cognition
across \textit{Perception}, \textit{Comprehension} and \textit{Reasoning}.
EyePCR offers a richly annotated corpus with more than 210k VQAs, which cover
1048 fine-grained attributes for multi-view perception, medical knowledge graph
of more than 25k triplets for comprehension, and four clinically grounded
reasoning tasks. The rich annotations facilitate in-depth cognitive analysis,
simulating how surgeons perceive visual cues and combine them with domain
knowledge to make decisions, thus greatly improving models' cognitive ability.
In particular, \textbf{EyePCR-MLLM}, a domain-adapted variant of Qwen2.5-VL-7B,
achieves the highest accuracy on MCQs for \textit{Perception} among compared
models and outperforms open-source models in \textit{Comprehension} and
\textit{Reasoning}, rivalling commercial models like GPT-4.1. EyePCR reveals
the limitations of existing MLLMs in surgical cognition and lays the foundation
for benchmarking and enhancing clinical reliability of surgical video
understanding models.

</details>


### [100] [TennisTV: Do Multimodal Large Language Models Understand Tennis Rallies?](https://arxiv.org/abs/2509.15602)
*Zhongyuan Bao,Lejun Zhang*

Main category: cs.CV

TL;DR: TennisTV: A new benchmark for tennis video understanding, revealing shortcomings in MLLMs.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with fast, information-dense sports like tennis.

Method: A comprehensive benchmark with 8 tasks, 2,500 human-verified questions, and automated pipelines.

Result: Systematic evaluation of 16 MLLMs reveals shortcomings.

Conclusion: Frame-sampling density should be tailored, and temporal grounding is essential.

Abstract: Multimodal large language models (MLLMs) excel at general video understanding
but struggle with fast, high-frequency sports like tennis, where rally clips
are short yet information-dense. To systematically evaluate MLLMs in this
challenging domain, we present TennisTV, the first and most comprehensive
benchmark for tennis video understanding. TennisTV models each rally as a
temporal-ordered sequence of consecutive stroke events, using automated
pipelines for filtering and question generation. It covers 8 tasks at rally and
stroke levels and includes 2,500 human-verified questions. Evaluating 16
representative MLLMs, we provide the first systematic assessment of tennis
video understanding. Results reveal substantial shortcomings and yield two key
insights: (i) frame-sampling density should be tailored and balanced across
tasks, and (ii) improving temporal grounding is essential for stronger
reasoning.

</details>


### [101] [Enhancing WSI-Based Survival Analysis with Report-Auxiliary Self-Distillation](https://arxiv.org/abs/2509.15608)
*Zheng Wang,Hong Liu,Zheng Wang,Danyi Li,Min Cen,Baptiste Magnier,Li Liang,Liansheng Wang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为Rasa的框架，用于基于WSI的生存分析，该框架利用病理报告中的信息来提高预测患者预后的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的WSI生存分析面临噪声特征和数据可访问性有限的问题，无法有效捕捉关键的预后特征。病理报告提供了丰富的患者特异性信息，但其增强WSI生存分析的潜力尚未被充分探索。

Method: 该框架首先利用大型语言模型从病理报告中提取与WSI相关的文本描述，然后设计一个基于自蒸馏的流程来过滤掉不相关的WSI特征，最后采用风险感知的mix-up策略来增加训练数据的数量和多样性。

Result: 在CRC和TCGA-BRCA数据集上的实验表明，Rasa框架优于现有方法。

Conclusion: Rasa框架能够有效地利用病理报告中的信息来提高WSI生存分析的性能。

Abstract: Survival analysis based on Whole Slide Images (WSIs) is crucial for
evaluating cancer prognosis, as they offer detailed microscopic information
essential for predicting patient outcomes. However, traditional WSI-based
survival analysis usually faces noisy features and limited data accessibility,
hindering their ability to capture critical prognostic features effectively.
Although pathology reports provide rich patient-specific information that could
assist analysis, their potential to enhance WSI-based survival analysis remains
largely unexplored. To this end, this paper proposes a novel Report-auxiliary
self-distillation (Rasa) framework for WSI-based survival analysis. First,
advanced large language models (LLMs) are utilized to extract fine-grained,
WSI-relevant textual descriptions from original noisy pathology reports via a
carefully designed task prompt. Next, a self-distillation-based pipeline is
designed to filter out irrelevant or redundant WSI features for the student
model under the guidance of the teacher model's textual knowledge. Finally, a
risk-aware mix-up strategy is incorporated during the training of the student
model to enhance both the quantity and diversity of the training data.
Extensive experiments carried out on our collected data (CRC) and public data
(TCGA-BRCA) demonstrate the superior effectiveness of Rasa against
state-of-the-art methods. Our code is available at
https://github.com/zhengwang9/Rasa.

</details>


### [102] [PCSR: Pseudo-label Consistency-Guided Sample Refinement for Noisy Correspondence Learning](https://arxiv.org/abs/2509.15623)
*Zhuoyao Liu,Yang Liu,Wentao Feng,Shudong Huang*

Main category: cs.CV

TL;DR: 本文提出了一种名为伪标签一致性引导样本细化 (PCSR) 的新框架，通过显式地基于伪标签一致性划分样本来增强对应关系可靠性，从而提高在噪声监督下检索的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设图像-文本对是完全对齐的，忽略了真实数据中的噪声对应关系。这些未对齐的pair会误导相似性学习并降低检索性能。以前的方法通常依赖于粗粒度的分类，简单地将数据分为干净和噪声样本，忽略了噪声实例内部的内在多样性。此外，它们通常应用统一的训练策略，而不管样本特征如何，导致模型优化的样本利用率欠佳。

Method: 首先，我们采用基于置信度的估计来区分干净和噪声pair，然后通过伪标签一致性细化噪声pair，以发现结构上不同的子集。我们进一步提出了一个伪标签一致性评分 (PCS) 来量化预测稳定性，从而能够分离噪声pair中的模糊和可细化样本。因此，我们采用自适应pair优化 (APO)，其中模糊样本通过鲁棒的损失函数进行优化，可细化样本通过训练期间的文本替换得到增强。

Result: 在 CC152K、MS-COCO 和 Flickr30K 上的大量实验验证了我们的方法在提高噪声监督下检索的鲁棒性方面的有效性。

Conclusion: 本文提出了一种新颖的框架PCSR，它可以提高在噪声监督下检索的鲁棒性。

Abstract: Cross-modal retrieval aims to align different modalities via semantic
similarity. However, existing methods often assume that image-text pairs are
perfectly aligned, overlooking Noisy Correspondences in real data. These
misaligned pairs misguide similarity learning and degrade retrieval
performance. Previous methods often rely on coarse-grained categorizations that
simply divide data into clean and noisy samples, overlooking the intrinsic
diversity within noisy instances. Moreover, they typically apply uniform
training strategies regardless of sample characteristics, resulting in
suboptimal sample utilization for model optimization. To address the above
challenges, we introduce a novel framework, called Pseudo-label
Consistency-Guided Sample Refinement (PCSR), which enhances correspondence
reliability by explicitly dividing samples based on pseudo-label consistency.
Specifically, we first employ a confidence-based estimation to distinguish
clean and noisy pairs, then refine the noisy pairs via pseudo-label consistency
to uncover structurally distinct subsets. We further proposed a Pseudo-label
Consistency Score (PCS) to quantify prediction stability, enabling the
separation of ambiguous and refinable samples within noisy pairs. Accordingly,
we adopt Adaptive Pair Optimization (APO), where ambiguous samples are
optimized with robust loss functions and refinable ones are enhanced via text
replacement during training. Extensive experiments on CC152K, MS-COCO and
Flickr30K validate the effectiveness of our method in improving retrieval
robustness under noisy supervision.

</details>


### [103] [pFedSAM: Personalized Federated Learning of Segment Anything Model for Medical Image Segmentation](https://arxiv.org/abs/2509.15638)
*Tong Wang,Xingyue Zhao,Linghao Zhuang,Haoyu Zhao,Jiayi Yin,Yuyang He,Gang Yu,Bo Lin*

Main category: cs.CV

TL;DR: 提出了一个用于医学图像分割的个性化联邦 SAM 框架，以解决复杂异构数据场景下的数据隐私和模型性能问题。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割对计算机辅助诊断至关重要，但隐私限制阻碍了机构间的数据共享。现有的联邦学习方法难以处理复杂、异构的数据。SAM 模型表现出色，但其庞大的编码器在联邦环境中构成重大挑战。

Method: 该框架集成了两项关键创新：(1) 一种个性化策略，仅聚合全局参数以捕获跨客户端的共性，同时保留 L-MoE 组件以保留特定领域特征；(2) 一种解耦的全局-局部微调机制，该机制利用通过知识提炼的师生范例来弥合全局共享模型和个性化本地模型之间的差距，从而减轻过度泛化。

Result: 在两个公共数据集上的大量实验验证了该方法显着提高了分割性能，实现了稳健的跨域适应，并减少了通信开销。

Conclusion: 该论文提出了一个有效的个性化联邦 SAM 框架，可以在保护数据隐私的同时，提高医学图像分割的性能和泛化能力。

Abstract: Medical image segmentation is crucial for computer-aided diagnosis, yet
privacy constraints hinder data sharing across institutions. Federated learning
addresses this limitation, but existing approaches often rely on lightweight
architectures that struggle with complex, heterogeneous data. Recently, the
Segment Anything Model (SAM) has shown outstanding segmentation capabilities;
however, its massive encoder poses significant challenges in federated
settings. In this work, we present the first personalized federated SAM
framework tailored for heterogeneous data scenarios in medical image
segmentation. Our framework integrates two key innovations: (1) a personalized
strategy that aggregates only the global parameters to capture cross-client
commonalities while retaining the designed L-MoE (Localized Mixture-of-Experts)
component to preserve domain-specific features; and (2) a decoupled
global-local fine-tuning mechanism that leverages a teacher-student paradigm
via knowledge distillation to bridge the gap between the global shared model
and the personalized local models, thereby mitigating overgeneralization.
Extensive experiments on two public datasets validate that our approach
significantly improves segmentation performance, achieves robust cross-domain
adaptation, and reduces communication overhead.

</details>


### [104] [UNIV: Unified Foundation Model for Infrared and Visible Modalities](https://arxiv.org/abs/2509.15642)
*Fangyuan Mao,Shuo Wang,Jilin Mei,Chen Min,Shun Lu,Fuyang Liu,Yu Hu*

Main category: cs.CV

TL;DR: 提出了一种用于红外和可见光模态的统一基础模型（UNIV），通过模仿视网膜的生物学机制，实现跨模态特征对齐，并防止灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 在各种天气条件下，对联合RGB可见光和红外感知需求快速增长。虽然RGB可见光和红外数据的预训练模型在各自领域表现出色，但在多模态场景中表现不佳，例如配备这两种传感器的自动驾驶汽车。

Method: 引入了逐块跨模态对比学习（PCCL）和双重知识保留机制。PCCL是一种注意力引导的蒸馏框架，模仿视网膜水平细胞的横向抑制，实现有效的跨模态特征对齐。双重知识保留机制模仿视网膜的双极细胞信号路由，结合LoRA适配器和同步蒸馏，防止灾难性遗忘。

Result: 在红外任务上表现出优越的性能（在语义分割中+1.7 mIoU，在目标检测中+0.7 mAP），同时在可见RGB任务上保持99%+的基线性能。

Conclusion: UNIV模型在跨模态学习中表现出色，为红外和可见光数据的融合提供了一种有效的方法。

Abstract: The demand for joint RGB-visible and infrared perception is growing rapidly,
particularly to achieve robust performance under diverse weather conditions.
Although pre-trained models for RGB-visible and infrared data excel in their
respective domains, they often underperform in multimodal scenarios, such as
autonomous vehicles equipped with both sensors. To address this challenge, we
propose a biologically inspired UNified foundation model for Infrared and
Visible modalities (UNIV), featuring two key innovations. First, we introduce
Patch-wise Cross-modality Contrastive Learning (PCCL), an attention-guided
distillation framework that mimics retinal horizontal cells' lateral
inhibition, which enables effective cross-modal feature alignment while
remaining compatible with any transformer-based architecture. Second, our
dual-knowledge preservation mechanism emulates the retina's bipolar cell signal
routing - combining LoRA adapters (2% added parameters) with synchronous
distillation to prevent catastrophic forgetting, thereby replicating the
retina's photopic (cone-driven) and scotopic (rod-driven) functionality. To
support cross-modal learning, we introduce the MVIP dataset, the most
comprehensive visible-infrared benchmark to date. It contains 98,992 precisely
aligned image pairs spanning diverse scenarios. Extensive experiments
demonstrate UNIV's superior performance on infrared tasks (+1.7 mIoU in
semantic segmentation and +0.7 mAP in object detection) while maintaining 99%+
of the baseline performance on visible RGB tasks. Our code is available at
https://github.com/fangyuanmao/UNIV.

</details>


### [105] [GS-Scale: Unlocking Large-Scale 3D Gaussian Splatting Training via Host Offloading](https://arxiv.org/abs/2509.15645)
*Donghyun Lee,Dawoon Jeong,Jae W. Lee,Hongil Yoon*

Main category: cs.CV

TL;DR: GS-Scale: A fast and memory-efficient system for training large-scale 3D Gaussian Splatting on consumer-grade GPUs.


<details>
  <summary>Details</summary>
Motivation: Training large-scale 3D Gaussian Splatting at high quality is challenging due to substantial GPU memory demands.

Method: GS-Scale stores Gaussians in host memory, transferring subsets to the GPU on demand, and employs selective offloading, parameter forwarding, and deferred optimizer updates.

Result: GS-Scale reduces GPU memory demands by 3.3-5.6x and achieves training speeds comparable to GPU without host offloading, scaling the number of Gaussians from 4 million to 18 million on an RTX 4070 Mobile GPU, leading to 23-35% LPIPS improvement.

Conclusion: GS-Scale enables large-scale 3D Gaussian Splatting training on consumer-grade GPUs.

Abstract: The advent of 3D Gaussian Splatting has revolutionized graphics rendering by
delivering high visual quality and fast rendering speeds. However, training
large-scale scenes at high quality remains challenging due to the substantial
memory demands required to store parameters, gradients, and optimizer states,
which can quickly overwhelm GPU memory. To address these limitations, we
propose GS-Scale, a fast and memory-efficient training system for 3D Gaussian
Splatting. GS-Scale stores all Gaussians in host memory, transferring only a
subset to the GPU on demand for each forward and backward pass. While this
dramatically reduces GPU memory usage, it requires frustum culling and
optimizer updates to be executed on the CPU, introducing slowdowns due to CPU's
limited compute and memory bandwidth. To mitigate this, GS-Scale employs three
system-level optimizations: (1) selective offloading of geometric parameters
for fast frustum culling, (2) parameter forwarding to pipeline CPU optimizer
updates with GPU computation, and (3) deferred optimizer update to minimize
unnecessary memory accesses for Gaussians with zero gradients. Our extensive
evaluations on large-scale datasets demonstrate that GS-Scale significantly
lowers GPU memory demands by 3.3-5.6x, while achieving training speeds
comparable to GPU without host offloading. This enables large-scale 3D Gaussian
Splatting training on consumer-grade GPUs; for instance, GS-Scale can scale the
number of Gaussians from 4 million to 18 million on an RTX 4070 Mobile GPU,
leading to 23-35% LPIPS (learned perceptual image patch similarity)
improvement.

</details>


### [106] [FingerSplat: Contactless Fingerprint 3D Reconstruction and Generation based on 3D Gaussian Splatting](https://arxiv.org/abs/2509.15648)
*Yuwei Jia,Yutang Lu,Zhe Cui,Fei Su*

Main category: cs.CV

TL;DR: 本文提出了一种新的非接触式指纹3D配准、重建和生成框架，通过整合3D高斯溅射技术，为非接触式指纹识别提供了一种新范式。


<details>
  <summary>Details</summary>
Motivation: 现有非接触式指纹识别性能落后于接触式方法，主要由于缺乏具有姿态变化的非接触式指纹数据以及缺乏隐式3D指纹表示的使用。

Method: 整合3D高斯溅射技术，实现非接触式指纹的3D配准、重建和生成。

Result: 实验证明，该方法可以从2D图像精确对齐和重建3D指纹，并从3D模型生成高质量的非接触式指纹。

Conclusion: 该方法提高了非接触式指纹识别的性能。

Abstract: Researchers have conducted many pioneer researches on contactless
fingerprints, yet the performance of contactless fingerprint recognition still
lags behind contact-based methods primary due to the insufficient contactless
fingerprint data with pose variations and lack of the usage of implicit 3D
fingerprint representations. In this paper, we introduce a novel contactless
fingerprint 3D registration, reconstruction and generation framework by
integrating 3D Gaussian Splatting, with the goal of offering a new paradigm for
contactless fingerprint recognition that integrates 3D fingerprint
reconstruction and generation. To our knowledge, this is the first work to
apply 3D Gaussian Splatting to the field of fingerprint recognition, and the
first to achieve effective 3D registration and complete reconstruction of
contactless fingerprints with sparse input images and without requiring camera
parameters information. Experiments on 3D fingerprint registration,
reconstruction, and generation prove that our method can accurately align and
reconstruct 3D fingerprints from 2D images, and sequentially generates
high-quality contactless fingerprints from 3D model, thus increasing the
performances for contactless fingerprint recognition.

</details>


### [107] [A PCA Based Model for Surface Reconstruction from Incomplete Point Clouds](https://arxiv.org/abs/2509.15675)
*Hao Liu*

Main category: cs.CV

TL;DR: 提出了一种基于PCA的点云补全方法，用于从不完整的点云数据中重建表面。


<details>
  <summary>Details</summary>
Motivation: 由于光吸收率高和遮挡等因素，收集到的点云数据可能无法覆盖整个表面，导致数据集不完整。推断数据缺失区域中的表面结构并成功重建表面是一个挑战。

Method: 首先，我们采用PCA从可用的点云数据中估计底层表面的法线信息。该估计的法线信息作为我们模型中的正则化器，指导表面的重建，特别是在缺少数据的区域中。此外，我们引入了一种算子分裂方法来有效地求解所提出的模型。

Result: 通过系统的实验，我们证明了我们的模型成功地推断了数据缺失区域中的表面结构，并很好地重建了底层表面，优于现有的方法。

Conclusion: 该模型能够成功推断数据缺失区域中的表面结构并很好地重建底层表面，优于现有方法。

Abstract: Point cloud data represents a crucial category of information for
mathematical modeling, and surface reconstruction from such data is an
important task across various disciplines. However, during the scanning
process, the collected point cloud data may fail to cover the entire surface
due to factors such as high light-absorption rate and occlusions, resulting in
incomplete datasets. Inferring surface structures in data-missing regions and
successfully reconstructing the surface poses a challenge. In this paper, we
present a Principal Component Analysis (PCA) based model for surface
reconstruction from incomplete point cloud data. Initially, we employ PCA to
estimate the normal information of the underlying surface from the available
point cloud data. This estimated normal information serves as a regularizer in
our model, guiding the reconstruction of the surface, particularly in areas
with missing data. Additionally, we introduce an operator-splitting method to
effectively solve the proposed model. Through systematic experimentation, we
demonstrate that our model successfully infers surface structures in
data-missing regions and well reconstructs the underlying surfaces,
outperforming existing methodologies.

</details>


### [108] [Camera Splatting for Continuous View Optimization](https://arxiv.org/abs/2509.15677)
*Gahye Lee,Hyomin Kim,Gwangjin Ju,Jooeun Son,Hyejeong Yoon,Seungyong Lee*

Main category: cs.CV

TL;DR: 提出了一种名为 Camera Splatting 的新颖视角优化框架，用于新视角合成。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有视角选择方法在捕捉复杂视角依赖现象（如金属反射和复杂纹理）方面的不足。

Method: 通过将每个相机建模为 3D 高斯分布（称为相机 splat），并在表面附近采样的 3D 点处放置虚拟相机（称为点相机）来观察相机 splat 的分布，然后以类似于 3D Gaussian splatting 的方式，不断地、可微地优化相机 splat，使得从点相机观察到期望的目标分布，从而实现视角优化。

Result: 优化的视角在捕捉复杂的视角依赖现象方面表现出优于 FVS 方法的性能。

Conclusion: Camera Splatting 是一种有效的视角优化方法，能够更好地捕捉复杂场景中的视角依赖现象。

Abstract: We propose Camera Splatting, a novel view optimization framework for novel
view synthesis. Each camera is modeled as a 3D Gaussian, referred to as a
camera splat, and virtual cameras, termed point cameras, are placed at 3D
points sampled near the surface to observe the distribution of camera splats.
View optimization is achieved by continuously and differentiably refining the
camera splats so that desirable target distributions are observed from the
point cameras, in a manner similar to the original 3D Gaussian splatting.
Compared to the Farthest View Sampling (FVS) approach, our optimized views
demonstrate superior performance in capturing complex view-dependent phenomena,
including intense metallic reflections and intricate textures such as text.

</details>


### [109] [Layout Stroke Imitation: A Layout Guided Handwriting Stroke Generation for Style Imitation with Diffusion Model](https://arxiv.org/abs/2509.15678)
*Sidra Hanif,Longin Jan Latecki*

Main category: cs.CV

TL;DR: 提出了一种基于条件扩散模型的手写笔画生成方法，该方法考虑了书法风格和单词布局，以实现更好的手写模仿。


<details>
  <summary>Details</summary>
Motivation: 先前的工作没有将单词间距（单词布局）作为显式的手写特征，这导致风格模仿的单词间距不一致。

Method: 1. 提出了用于书法风格模仿的多尺度注意力特征，这些多尺度特征嵌入突出了局部和全局风格特征。 2. 建议包括单词布局，这有助于单词间距的手写笔画生成。 3. 提出了一个条件扩散模型来预测笔画。

Result: 提出的扩散模型优于当前最先进的笔画生成，并且与最近的图像生成网络相比具有竞争力。

Conclusion: 该研究提出了一种新的手写笔画生成方法，通过结合多尺度注意力特征和单词布局，利用条件扩散模型实现了更好的书法风格模仿和笔画生成。

Abstract: Handwriting stroke generation is crucial for improving the performance of
tasks such as handwriting recognition and writers order recovery. In
handwriting stroke generation, it is significantly important to imitate the
sample calligraphic style. The previous studies have suggested utilizing the
calligraphic features of the handwriting. However, they had not considered word
spacing (word layout) as an explicit handwriting feature, which results in
inconsistent word spacing for style imitation. Firstly, this work proposes
multi-scale attention features for calligraphic style imitation. These
multi-scale feature embeddings highlight the local and global style features.
Secondly, we propose to include the words layout, which facilitates word
spacing for handwriting stroke generation. Moreover, we propose a conditional
diffusion model to predict strokes in contrast to previous work, which directly
generated style images. Stroke generation provides additional temporal
coordinate information, which is lacking in image generation. Hence, our
proposed conditional diffusion model for stroke generation is guided by
calligraphic style and word layout for better handwriting imitation and stroke
generation in a calligraphic style. Our experimentation shows that the proposed
diffusion model outperforms the current state-of-the-art stroke generation and
is competitive with recent image generation networks.

</details>


### [110] [Saccadic Vision for Fine-Grained Visual Classification](https://arxiv.org/abs/2509.15688)
*Johann Schmidt,Sebastian Stober,Joachim Denzler,Paul Bodesheim*

Main category: cs.CV

TL;DR: 本文提出了一种新的细粒度视觉分类方法，该方法模仿人类的扫视视觉，通过提取周边特征和生成采样图，然后并行采样和编码注视块，并使用上下文选择性注意力来融合周边和焦点表示，从而在标准和具有挑战性的数据集上实现了与最先进方法相当或更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于部件的方法依赖于复杂的定位网络，这些网络学习从像素到样本空间的映射，需要对图像内容有深入的理解，同时限制了下游任务的特征效用。此外，采样点经常遭受高空间冗余，使得难以量化所需部件的最佳数量。

Method: 该方法首先提取周边特征（粗略视图）并生成采样图，然后使用权重共享编码器并行采样和编码注视块。我们采用上下文选择性注意力来权衡每个注视块的影响，然后融合周边和焦点表示。为了防止空间崩溃，我们利用非极大值抑制在注视采样期间消除冗余。

Result: 在标准FGVC基准（CUB-200-2011，NABirds，Food-101和Stanford-Dogs）和具有挑战性的昆虫数据集（EU-Moths，Ecuador-Moths和AMI-Moths）上的综合评估表明，我们的方法实现了与最先进方法相当的性能，同时始终优于我们的基线编码器。

Conclusion: 该方法通过模仿人类的扫视视觉，有效地解决了细粒度视觉分类中的挑战，并在多个数据集上取得了良好的效果。

Abstract: Fine-grained visual classification (FGVC) requires distinguishing between
visually similar categories through subtle, localized features - a task that
remains challenging due to high intra-class variability and limited inter-class
differences. Existing part-based methods often rely on complex localization
networks that learn mappings from pixel to sample space, requiring a deep
understanding of image content while limiting feature utility for downstream
tasks. In addition, sampled points frequently suffer from high spatial
redundancy, making it difficult to quantify the optimal number of required
parts. Inspired by human saccadic vision, we propose a two-stage process that
first extracts peripheral features (coarse view) and generates a sample map,
from which fixation patches are sampled and encoded in parallel using a
weight-shared encoder. We employ contextualized selective attention to weigh
the impact of each fixation patch before fusing peripheral and focus
representations. To prevent spatial collapse - a common issue in part-based
methods - we utilize non-maximum suppression during fixation sampling to
eliminate redundancy. Comprehensive evaluation on standard FGVC benchmarks
(CUB-200-2011, NABirds, Food-101 and Stanford-Dogs) and challenging insect
datasets (EU-Moths, Ecuador-Moths and AMI-Moths) demonstrates that our method
achieves comparable performance to state-of-the-art approaches while
consistently outperforming our baseline encoder.

</details>


### [111] [SCENEFORGE: Enhancing 3D-text alignment with Structured Scene Compositions](https://arxiv.org/abs/2509.15693)
*Cristian Sbrolli,Matteo Matteucci*

Main category: cs.CV

TL;DR: SceneForge通过构建多对象场景来增强3D点云和文本之间的对比学习。


<details>
  <summary>Details</summary>
Motivation: 现有3D-文本数据集规模小，数据复杂性和多样性不足。

Method: 利用单个3D形状构建具有明确空间关系的多对象场景，并用大型语言模型提炼的多对象描述配对，以此增强对比训练。

Result: 在ModelNet、ScanObjNN、Objaverse-LVIS和ScanNet上的零样本分类以及ShapeNetPart上的少样本部件分割等多个任务中，SceneForge 均实现了显著的性能提升。在ScanQA上改进了3D视觉问答，并且在检索场景中具有鲁棒的泛化能力。

Conclusion: SceneForge的组合增强是模型无关的，并且通过调整空间配置与文本指令精确对齐，展示了空间推理能力。

Abstract: The whole is greater than the sum of its parts-even in 3D-text contrastive
learning. We introduce SceneForge, a novel framework that enhances contrastive
alignment between 3D point clouds and text through structured multi-object
scene compositions. SceneForge leverages individual 3D shapes to construct
multi-object scenes with explicit spatial relations, pairing them with coherent
multi-object descriptions refined by a large language model. By augmenting
contrastive training with these structured, compositional samples, SceneForge
effectively addresses the scarcity of large-scale 3D-text datasets,
significantly enriching data complexity and diversity. We systematically
investigate critical design elements, such as the optimal number of objects per
scene, the proportion of compositional samples in training batches, and scene
construction strategies. Extensive experiments demonstrate that SceneForge
delivers substantial performance gains across multiple tasks, including
zero-shot classification on ModelNet, ScanObjNN, Objaverse-LVIS, and ScanNet,
as well as few-shot part segmentation on ShapeNetPart. SceneForge's
compositional augmentations are model-agnostic, consistently improving
performance across multiple encoder architectures. Moreover, SceneForge
improves 3D visual question answering on ScanQA, generalizes robustly to
retrieval scenarios with increasing scene complexity, and showcases spatial
reasoning capabilities by adapting spatial configurations to align precisely
with textual instructions.

</details>


### [112] [ORIC: Benchmarking Object Recognition in Incongruous Context for Large Vision-Language Models](https://arxiv.org/abs/2509.15695)
*Zhaoyang Li,Zhan Ling,Yuchen Zhou,Hao Su*

Main category: cs.CV

TL;DR: 大型视觉语言模型(LVLMs)在整合视觉和文本信息方面取得了显著进展，但在不协调的上下文中仍然容易出错，导致物体识别错误和幻觉。


<details>
  <summary>Details</summary>
Motivation: 评估大型视觉语言模型(LVLMs)在物体与上下文关系不符时的识别能力，揭示其在不协调上下文中识别物体时的不足。

Method: 提出了一个名为“不协调上下文中物体识别基准(ORIC)”的新基准，该基准采用LLM引导的采样和CLIP引导的采样两种关键策略来创建不协调的上下文。

Result: 通过评估18个LVLMs和2个开放词汇检测模型，结果表明存在显著的识别差距，强调了上下文不协调带来的挑战。

Conclusion: 这项工作为LVLMs的局限性提供了重要的见解，并鼓励对上下文感知的物体识别进行进一步研究。

Abstract: Large Vision-Language Models (LVLMs) have made significant strides in image
caption, visual question answering, and robotics by integrating visual and
textual information. However, they remain prone to errors in incongruous
contexts, where objects appear unexpectedly or are absent when contextually
expected. This leads to two key recognition failures: object misidentification
and hallucination. To systematically examine this issue, we introduce the
Object Recognition in Incongruous Context Benchmark (ORIC), a novel benchmark
that evaluates LVLMs in scenarios where object-context relationships deviate
from expectations. ORIC employs two key strategies: (1) LLM-guided sampling,
which identifies objects that are present but contextually incongruous, and (2)
CLIP-guided sampling, which detects plausible yet nonexistent objects that are
likely to be hallucinated, thereby creating an incongruous context. Evaluating
18 LVLMs and two open-vocabulary detection models, our results reveal
significant recognition gaps, underscoring the challenges posed by contextual
incongruity. This work provides critical insights into LVLMs' limitations and
encourages further research on context-aware object recognition.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [113] [MICA: Multi-Agent Industrial Coordination Assistant](https://arxiv.org/abs/2509.15237)
*Di Wen,Kunyu Peng,Junwei Zheng,Yufan Chen,Yitain Shi,Jiale Wei,Ruiping Liu,Kailun Yang,Rainer Stiefelhagen*

Main category: cs.AI

TL;DR: MICA is a perception-grounded and speech-interactive multi-agent system for real-time industrial assistance, featuring adaptive step understanding and a safety checker.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for adaptive and trustworthy assistance in industrial workflows under limited computing, connectivity, and strict privacy constraints.

Method: The paper introduces MICA, which coordinates five role-specialized language agents and uses Adaptive Step Fusion (ASF) for robust step understanding. It also establishes a new multi-agent coordination benchmark.

Result: MICA consistently improves task success, reliability, and responsiveness compared to baseline structures and can be deployed on practical offline hardware.

Conclusion: MICA represents a step toward deployable, privacy-preserving multi-agent assistants for dynamic factory environments.

Abstract: Industrial workflows demand adaptive and trustworthy assistance that can
operate under limited computing, connectivity, and strict privacy constraints.
In this work, we present MICA (Multi-Agent Industrial Coordination Assistant),
a perception-grounded and speech-interactive system that delivers real-time
guidance for assembly, troubleshooting, part queries, and maintenance. MICA
coordinates five role-specialized language agents, audited by a safety checker,
to ensure accurate and compliant support. To achieve robust step understanding,
we introduce Adaptive Step Fusion (ASF), which dynamically blends expert
reasoning with online adaptation from natural speech feedback. Furthermore, we
establish a new multi-agent coordination benchmark across representative task
categories and propose evaluation metrics tailored to industrial assistance,
enabling systematic comparison of different coordination topologies. Our
experiments demonstrate that MICA consistently improves task success,
reliability, and responsiveness over baseline structures, while remaining
deployable on practical offline hardware. Together, these contributions
highlight MICA as a step toward deployable, privacy-preserving multi-agent
assistants for dynamic factory environments. The source code will be made
publicly available at https://github.com/Kratos-Wen/MICA.

</details>


### [114] [KNARsack: Teaching Neural Algorithmic Reasoners to Solve Pseudo-Polynomial Problems](https://arxiv.org/abs/2509.15239)
*Stjepan Požgaj,Dobrik Georgiev,Marin Šilić,Petar Veličković*

Main category: cs.AI

TL;DR: 本文介绍了一种神经算法推理器，它可以解决背包问题，这是一个伪多项式问题，连接了经典算法和组合优化，但在标准NAR基准测试中被省略。


<details>
  <summary>Details</summary>
Motivation: 旨在通过模仿经典算法将算法逻辑嵌入到神经网络中。本文详细介绍了我们构建神经算法推理器的尝试，该推理器可以解决背包问题。

Method: 我们的神经算法推理器旨在密切遵循背包问题的两阶段流水线，该流水线涉及首先构建动态规划表，然后从中重建解决方案。通过动态规划监督对中间状态进行建模

Result: 该方法比仅从问题输入中选择最佳子集的直接预测基线更好地推广到更大的问题实例。

Conclusion: 构建了一个神经算法推理器，它可以解决背包问题，并且可以更好地推广到更大的问题实例。

Abstract: Neural algorithmic reasoning (NAR) is a growing field that aims to embed
algorithmic logic into neural networks by imitating classical algorithms. In
this extended abstract, we detail our attempt to build a neural algorithmic
reasoner that can solve Knapsack, a pseudo-polynomial problem bridging
classical algorithms and combinatorial optimisation, but omitted in standard
NAR benchmarks. Our neural algorithmic reasoner is designed to closely follow
the two-phase pipeline for the Knapsack problem, which involves first
constructing the dynamic programming table and then reconstructing the solution
from it. The approach, which models intermediate states through dynamic
programming supervision, achieves better generalization to larger problem
instances than a direct-prediction baseline that attempts to select the optimal
subset only from the problem inputs.

</details>


### [115] [The Distribution Shift Problem in Transportation Networks using Reinforcement Learning and AI](https://arxiv.org/abs/2509.15291)
*Federico Taschin,Abderrahmane Lazaraq,Ozan K. Tonguz,Inci Ozgunes*

Main category: cs.AI

TL;DR: 强化学习在智能交通网络中应用广泛，但其可靠性受动态变化输入数据分布的影响。Meta RL被认为是有效的解决方案，但该论文评估MetaLight后发现，在某些条件下MetaLight表现不佳，表明Meta RL方案不够稳健，可能存在可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 在交通信号控制中使用强化学习时，由于输入数据的动态变化，导致训练后的强化学习智能体的可靠性问题。这给人工智能智能体网络带来了主要的挑战和可靠性问题。

Method: 评估和分析了一种名为MetaLight的先进Meta RL方法。

Result: MetaLight在某些条件下表现良好，但在其他条件下可能表现不佳（误差高达22%）。

Conclusion: Meta RL方案通常不够稳健，甚至可能造成主要的可靠性问题。

Abstract: The use of Machine Learning (ML) and Artificial Intelligence (AI) in smart
transportation networks has increased significantly in the last few years.
Among these ML and AI approaches, Reinforcement Learning (RL) has been shown to
be a very promising approach by several authors. However, a problem with using
Reinforcement Learning in Traffic Signal Control is the reliability of the
trained RL agents due to the dynamically changing distribution of the input
data with respect to the distribution of the data used for training. This
presents a major challenge and a reliability problem for the trained network of
AI agents and could have very undesirable and even detrimental consequences if
a suitable solution is not found. Several researchers have tried to address
this problem using different approaches. In particular, Meta Reinforcement
Learning (Meta RL) promises to be an effective solution. In this paper, we
evaluate and analyze a state-of-the-art Meta RL approach called MetaLight and
show that, while under certain conditions MetaLight can indeed lead to
reasonably good results, under some other conditions it might not perform well
(with errors of up to 22%), suggesting that Meta RL schemes are often not
robust enough and can even pose major reliability problems.

</details>


### [116] [An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature](https://arxiv.org/abs/2509.15292)
*Abhiyan Dhakal,Kausik Paudel,Sanjog Sigdel*

Main category: cs.AI

TL;DR: 提出了一种基于语义相似性的自动文献综述流程。


<details>
  <summary>Details</summary>
Motivation: 传统文献综述系统开销大，相关性不足。

Method: 使用基于transformer的嵌入和余弦相似度，通过论文标题和摘要生成关键词，从开放获取仓库获取相关论文，并根据语义相似度进行排序。

Result: 评估了三种嵌入模型，并应用统计阈值方法过滤相关论文。

Conclusion: 该系统无需启发式反馈或ground truth相关性标签，作为初步研究和探索性分析的可扩展和实用工具显示出前景。

Abstract: We propose an automated pipeline for performing literature reviews using
semantic similarity. Unlike traditional systematic review systems or
optimization based methods, this work emphasizes minimal overhead and high
relevance by using transformer based embeddings and cosine similarity. By
providing a paper title and abstract, it generates relevant keywords, fetches
relevant papers from open access repository, and ranks them based on their
semantic closeness to the input. Three embedding models were evaluated. A
statistical thresholding approach is then applied to filter relevant papers,
enabling an effective literature review pipeline. Despite the absence of
heuristic feedback or ground truth relevance labels, the proposed system shows
promise as a scalable and practical tool for preliminary research and
exploratory analysis.

</details>


### [117] [Building Data-Driven Occupation Taxonomies: A Bottom-Up Multi-Stage Approach via Semantic Clustering and Multi-Agent Collaboration](https://arxiv.org/abs/2509.15786)
*Nan Li,Bo Kang,Tijl De Bie*

Main category: cs.AI

TL;DR: 提出了一种名为CLIMB的框架，用于从原始招聘信息中自动创建高质量、数据驱动的职业分类。


<details>
  <summary>Details</summary>
Motivation: 创建稳健的职业分类对于各种应用至关重要，但手动管理速度慢，现有的自动化方法要么不适应动态区域市场（自上而下），要么难以从嘈杂的数据中构建连贯的层级结构（自下而上）。

Method: CLIMB使用全局语义聚类来提炼核心职业，然后采用基于反射的多智能体系统来迭代构建连贯的层级结构。

Result: 在三个不同的真实世界数据集上，CLIMB生成的分类法比现有方法更连贯和可扩展，并且成功地捕捉了独特的区域特征。

Conclusion: CLIMB框架能够有效地自动创建高质量的职业分类，优于现有方法，并具有良好的可扩展性和区域适应性。

Abstract: Creating robust occupation taxonomies, vital for applications ranging from
job recommendation to labor market intelligence, is challenging. Manual
curation is slow, while existing automated methods are either not adaptive to
dynamic regional markets (top-down) or struggle to build coherent hierarchies
from noisy data (bottom-up). We introduce CLIMB (CLusterIng-based Multi-agent
taxonomy Builder), a framework that fully automates the creation of
high-quality, data-driven taxonomies from raw job postings. CLIMB uses global
semantic clustering to distill core occupations, then employs a
reflection-based multi-agent system to iteratively build a coherent hierarchy.
On three diverse, real-world datasets, we show that CLIMB produces taxonomies
that are more coherent and scalable than existing methods and successfully
capture unique regional characteristics. We release our code and datasets at
https://anonymous.4open.science/r/CLIMB.

</details>


### [118] [Knowledge-Driven Hallucination in Large Language Models: An Empirical Study on Process Modeling](https://arxiv.org/abs/2509.15336)
*Humam Kourani,Anton Antonov,Alessandro Berti,Wil M. P. van der Aalst*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）在分析任务中的效用在于其预训练知识，但也引入了知识驱动的幻觉风险，即模型的输出与明确的来源证据相矛盾，因为它被模型广义的内部知识所覆盖。


<details>
  <summary>Details</summary>
Motivation: 本文研究LLM在自动流程建模任务中，由于其预训练知识可能导致输出与提供的证据相矛盾的现象。

Method: 通过评估LLM在自动化流程建模任务中的表现，设计受控实验，创建提供的证据与LLM背景知识之间存在冲突的场景，使用描述标准和非典型流程结构的输入来衡量LLM对所提供证据的忠实度。

Result: 提供了一种评估这种关键可靠性问题的方法。

Conclusion: 强调需要在任何基于证据的领域对人工智能生成的工件进行严格验证。

Abstract: The utility of Large Language Models (LLMs) in analytical tasks is rooted in
their vast pre-trained knowledge, which allows them to interpret ambiguous
inputs and infer missing information. However, this same capability introduces
a critical risk of what we term knowledge-driven hallucination: a phenomenon
where the model's output contradicts explicit source evidence because it is
overridden by the model's generalized internal knowledge. This paper
investigates this phenomenon by evaluating LLMs on the task of automated
process modeling, where the goal is to generate a formal business process model
from a given source artifact. The domain of Business Process Management (BPM)
provides an ideal context for this study, as many core business processes
follow standardized patterns, making it likely that LLMs possess strong
pre-trained schemas for them. We conduct a controlled experiment designed to
create scenarios with deliberate conflict between provided evidence and the
LLM's background knowledge. We use inputs describing both standard and
deliberately atypical process structures to measure the LLM's fidelity to the
provided evidence. Our work provides a methodology for assessing this critical
reliability issue and raises awareness of the need for rigorous validation of
AI-generated artifacts in any evidence-based domain.

</details>


### [119] [EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol](https://arxiv.org/abs/2509.15957)
*Kanato Masayoshi,Masahiro Hashimoto,Ryoichi Yokoyama,Naoki Toda,Yoshifumi Uwamino,Shogo Fukuda,Ho Namkoong,Masahiro Jinzaki*

Main category: cs.AI

TL;DR: 大型语言模型(llm)在医学领域有前景，但由于电子健康记录(ehr)系统的访问受限，它们在医院的部署受到限制。模型上下文协议(mcp)支持llm和外部工具之间的集成。


<details>
  <summary>Details</summary>
Motivation: 为了评估通过mcp连接到ehr数据库的llm是否可以在真实的医院环境中自主检索临床相关信息。

Method: 我们开发了ehr-mcp，这是一个与医院ehr数据库集成的自定义mcp工具框架，并使用gpt-4.1通过langraph react代理与它进行交互。测试了六项任务，这些任务来自感染控制团队(ict)的用例。回顾性分析了ict会议上讨论的八名患者。测量了与医生生成的黄金标准的协议。

Result: llm始终如一地选择和执行正确的mcp工具。除两项任务外，所有任务均达到接近完美的准确率。在需要时间相关计算的复杂任务中，性能较低。大多数错误是由于不正确的参数或对工具结果的错误解释造成的。来自ehr-mcp的响应是可靠的，但冗长且重复的数据可能会超过上下文窗口。

Conclusion: llm可以通过真实医院环境中通过mcp工具从ehr检索临床数据，在简单的任务中实现接近完美的性能，同时突出复杂任务中的挑战。ehr-mcp为安全、一致的数据访问提供了基础架构，可以作为医院ai代理的基础。未来的工作应该扩展到检索之外的推理、生成和临床影响评估，从而为将生成式ai有效集成到临床实践中铺平道路。

Abstract: Background: Large language models (LLMs) show promise in medicine, but their
deployment in hospitals is limited by restricted access to electronic health
record (EHR) systems. The Model Context Protocol (MCP) enables integration
between LLMs and external tools.
  Objective: To evaluate whether an LLM connected to an EHR database via MCP
can autonomously retrieve clinically relevant information in a real hospital
setting.
  Methods: We developed EHR-MCP, a framework of custom MCP tools integrated
with the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct
agent to interact with it. Six tasks were tested, derived from use cases of the
infection control team (ICT). Eight patients discussed at ICT conferences were
retrospectively analyzed. Agreement with physician-generated gold standards was
measured.
  Results: The LLM consistently selected and executed the correct MCP tools.
Except for two tasks, all tasks achieved near-perfect accuracy. Performance was
lower in the complex task requiring time-dependent calculations. Most errors
arose from incorrect arguments or misinterpretation of tool results. Responses
from EHR-MCP were reliable, though long and repetitive data risked exceeding
the context window.
  Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a
real hospital setting, achieving near-perfect performance in simple tasks while
highlighting challenges in complex ones. EHR-MCP provides an infrastructure for
secure, consistent data access and may serve as a foundation for hospital AI
agents. Future work should extend beyond retrieval to reasoning, generation,
and clinical impact assessment, paving the way for effective integration of
generative AI into clinical practice.

</details>


### [120] [Diagnostics of cognitive failures in multi-agent expert systems using dynamic evaluation protocols and subsequent mutation of the processing context](https://arxiv.org/abs/2509.15366)
*Andrejs Sorstkins,Josh Bailey,Dr Alistair Baron*

Main category: cs.AI

TL;DR: 论文提出了一种诊断框架，用于评估和转移专家行为到LLM驱动的agent中。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法不足以诊断LLM agent的性能，因为它们具有随机性和多步决策过程。

Method: 该框架集成了专家标注的黄金数据集、行为突变生成的白银数据集和一个基于LLM的Agent Judge，用于评分和提供改进建议。

Result: 该框架在一个多agent招聘助理系统中揭示了潜在的认知失败，同时引导agent达到专家级别的推理和风格。

Conclusion: 该研究为随机、工具增强的LLM agent中的标准化、可复现的专家行为转移奠定了基础，从静态评估转向主动专家系统改进。

Abstract: The rapid evolution of neural architectures - from multilayer perceptrons to
large-scale Transformer-based models - has enabled language models (LLMs) to
exhibit emergent agentic behaviours when equipped with memory, planning, and
external tool use. However, their inherent stochasticity and multi-step
decision processes render classical evaluation methods inadequate for
diagnosing agentic performance. This work introduces a diagnostic framework for
expert systems that not only evaluates but also facilitates the transfer of
expert behaviour into LLM-powered agents. The framework integrates (i) curated
golden datasets of expert annotations, (ii) silver datasets generated through
controlled behavioural mutation, and (iii) an LLM-based Agent Judge that scores
and prescribes targeted improvements. These prescriptions are embedded into a
vectorized recommendation map, allowing expert interventions to propagate as
reusable improvement trajectories across multiple system instances. We
demonstrate the framework on a multi-agent recruiter-assistant system, showing
that it uncovers latent cognitive failures - such as biased phrasing,
extraction drift, and tool misrouting - while simultaneously steering agents
toward expert-level reasoning and style. The results establish a foundation for
standardized, reproducible expert behaviour transfer in stochastic,
tool-augmented LLM agents, moving beyond static evaluation to active expert
system refinement.

</details>


### [121] [FragmentRetro: A Quadratic Retrosynthetic Method Based on Fragmentation Algorithms](https://arxiv.org/abs/2509.15409)
*Yu Shee,Anthony M. Smaldone,Anton Morgunov,Gregory W. Kyro,Victor S. Batista*

Main category: cs.AI

TL;DR: FragmentRetro是一种新的逆合成方法，它利用片段算法，结合库存感知探索和模式指纹筛选，实现了二次复杂度。


<details>
  <summary>Details</summary>
Motivation: 计算机辅助合成规划（CASP）中，逆合成是将目标分子分解成更简单前体的过程，至关重要。然而，广泛采用的树搜索方法通常面临指数级的计算复杂度。

Method: FragmentRetro递归地组合分子片段，并验证它们是否存在于构建块集合中，从而提供片段组合集合作为逆合成解决方案。

Result: 对PaRoutes、USPTO-190和天然产物的评估表明，FragmentRetro实现了高求解率和具有竞争力的运行时间，包括树搜索失败的案例。指纹筛选显著降低了子结构匹配的复杂性。

Conclusion: FragmentRetro专注于高效识别基于片段的解决方案，而不是完整的反应路径，但其计算优势和生成战略起始候选物的能力使其成为可扩展和自动化合成规划的强大基础组件。

Abstract: Retrosynthesis, the process of deconstructing a target molecule into simpler
precursors, is crucial for computer-aided synthesis planning (CASP). Widely
adopted tree-search methods often suffer from exponential computational
complexity. In this work, we introduce FragmentRetro, a novel retrosynthetic
method that leverages fragmentation algorithms, specifically BRICS and r-BRICS,
combined with stock-aware exploration and pattern fingerprint screening to
achieve quadratic complexity. FragmentRetro recursively combines molecular
fragments and verifies their presence in a building block set, providing sets
of fragment combinations as retrosynthetic solutions. We present the first
formal computational analysis of retrosynthetic methods, showing that tree
search exhibits exponential complexity $O(b^h)$, DirectMultiStep scales as
$O(h^6)$, and FragmentRetro achieves $O(h^2)$, where $h$ represents the number
of heavy atoms in the target molecule and $b$ is the branching factor for tree
search. Evaluations on PaRoutes, USPTO-190, and natural products demonstrate
that FragmentRetro achieves high solved rates with competitive runtime,
including cases where tree search fails. The method benefits from fingerprint
screening, which significantly reduces substructure matching complexity. While
FragmentRetro focuses on efficiently identifying fragment-based solutions
rather than full reaction pathways, its computational advantages and ability to
generate strategic starting candidates establish it as a powerful foundational
component for scalable and automated synthesis planning.

</details>


### [122] [Stress Testing Deliberative Alignment for Anti-Scheming Training](https://arxiv.org/abs/2509.15541)
*Bronson Schoen,Evgenia Nitishinskaya,Mikita Balesni,Axel Højmark,Felix Hofstätter,Jérémy Scheurer,Alexander Meinke,Jason Wolfe,Teun van der Weij,Alex Lloyd,Nicholas Goldowsky-Dill,Angela Fan,Andrei Matveiakin,Rusheb Shah,Marcus Williams,Amelia Glaese,Boaz Barak,Wojciech Zaremba,Marius Hobbhahn*

Main category: cs.AI

TL;DR: AI系统可能会秘密地追求不一致的目标，即“策划”。评估反策划干预措施至少需要（1）测试远超分布（OOD）任务的策划倾向，（2）评估是否缺乏由情境意识驱动的策划，以及（3）检查对先前存在的不一致目标的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 测量和缓解策划需要与ML中通常使用的策略不同的策略，因为策划AI会故意试图隐藏其不一致的目标和行为。

Method: 使用广泛的“隐蔽行动”类别（例如秘密违反规则或故意在测试中表现不佳）作为策划的代理，并设计用于隐蔽行动的评估。然后，对深思熟虑的对齐方式进行压力测试，作为反策划的案例研究。

Result: 在26个OOD评估（180多个环境）中，深思熟虑的对齐方式降低了隐蔽行动率（OpenAI o3：13％-> 0.4％），但并未完全消除它们。我们的缓解措施还能够在很大程度上阻止代理追求先前训练到模型中的隐藏目标，但是经过额外的红队测试后，我们仍然发现不良行为。

Conclusion: 我们发现，模型的思维链（CoT）通常表现出对正在评估对齐方式的意识，并且有因果证据表明，这种意识会减少隐蔽行为，而无意识会增加隐蔽行为。因此，我们不能排除观察到的隐蔽行动率的降低至少部分是由情境意识驱动的。

Abstract: Highly capable AI systems could secretly pursue misaligned goals -- what we
call "scheming". Because a scheming AI would deliberately try to hide its
misaligned goals and actions, measuring and mitigating scheming requires
different strategies than are typically used in ML. We propose that assessing
anti-scheming interventions requires at least (1) testing propensity to scheme
on far out-of-distribution (OOD) tasks, (2) evaluating whether lack of scheming
is driven by situational awareness, and (3) checking for robustness to
pre-existing misaligned goals. We use a broad category of "covert actions" --
such as secretly breaking rules or intentionally underperforming in tests -- as
a proxy for scheming, and design evaluations for covert actions. We then
stress-test deliberative alignment as a case study for anti-scheming. Across 26
OOD evaluations (180+ environments), deliberative alignment reduces covert
action rates (OpenAI o3: 13%->0.4%) but does not fully eliminate them. Our
mitigation is also able to largely stop agents from pursuing a hidden goal
previously trained into the model, but we still find misbehavior after
additional red-teaming. We find that models' chain-of-thought (CoT) often
demonstrates awareness of being evaluated for alignment, and show causal
evidence that this awareness decreases covert behavior, while unawareness
increases it. Therefore, we cannot exclude that the observed reductions in
covert action rates are at least partially driven by situational awareness.
While we rely on human-legible CoT for training, studying situational
awareness, and demonstrating clear evidence of misalignment, our ability to
rely on this degrades as models continue to depart from reasoning in standard
English. We encourage research into alignment mitigations for scheming and
their assessment, especially for the adversarial case of deceptive alignment,
which this paper does not address.

</details>


### [123] [MicroRCA-Agent: Microservice Root Cause Analysis Method Based on Large Language Model Agents](https://arxiv.org/abs/2509.15635)
*Pan Tang,Shixiang Tang,Huanqi Pu,Zhiqing Miao,Zhixing Wang*

Main category: cs.AI

TL;DR: MicroRCA-Agent: An intelligent fault root cause localization system using LLM agents and multimodal data fusion.


<details>
  <summary>Details</summary>
Motivation: To address microservice root cause analysis.

Method: Combining Drain log parsing, multi-level data filtering, dual anomaly detection (Isolation Forest & status code validation), statistical symmetry ratio filtering, and a two-stage LLM analysis strategy with cross-modal prompts.

Result: Achieves a score of 50.71 in complex microservice fault scenarios.

Conclusion: Demonstrates superior performance with the proposed solution.

Abstract: This paper presents MicroRCA-Agent, an innovative solution for microservice
root cause analysis based on large language model agents, which constructs an
intelligent fault root cause localization system with multimodal data fusion.
The technical innovations are embodied in three key aspects: First, we combine
the pre-trained Drain log parsing algorithm with multi-level data filtering
mechanism to efficiently compress massive logs into high-quality fault
features. Second, we employ a dual anomaly detection approach that integrates
Isolation Forest unsupervised learning algorithms with status code validation
to achieve comprehensive trace anomaly identification. Third, we design a
statistical symmetry ratio filtering mechanism coupled with a two-stage LLM
analysis strategy to enable full-stack phenomenon summarization across
node-service-pod hierarchies. The multimodal root cause analysis module
leverages carefully designed cross-modal prompts to deeply integrate multimodal
anomaly information, fully exploiting the cross-modal understanding and logical
reasoning capabilities of large language models to generate structured analysis
results encompassing fault components, root cause descriptions, and reasoning
trace. Comprehensive ablation studies validate the complementary value of each
modal data and the effectiveness of the system architecture. The proposed
solution demonstrates superior performance in complex microservice fault
scenarios, achieving a final score of 50.71. The code has been released at:
https://github.com/tangpan360/MicroRCA-Agent.

</details>


### [124] [CCrepairBench: A High-Fidelity Benchmark and Reinforcement Learning Framework for C++ Compilation Repair](https://arxiv.org/abs/2509.15690)
*Weixuan Sun,Jucai Zhai,Dengfeng Liu,Xin Zhang,Xiaojun Wu,Qiaobo Hao,AIMgroup,Yang Fang,Jiuyang Tang*

Main category: cs.AI

TL;DR: 本论文提出了一个用于自动修复C++编译错误的框架，包括一个大型数据集CCrepair、一个强化学习范式和一个两阶段评估系统。


<details>
  <summary>Details</summary>
Motivation: 解决C++编译错误自动修复领域中，大规模高质量数据集的缺乏以及传统监督方法难以生成语义正确的补丁的问题。

Method: 1. 构建了一个大型C++编译错误数据集CCrepair；2. 提出了一个由混合奖励信号引导的强化学习范式；3. 建立了一个可靠的两阶段评估系统，使用LLM作为裁判。

Result: 实验结果表明，使用强化学习训练的Qwen2.5-1.5B-Instruct模型达到了与Qwen2.5-14B-Instruct模型相当的性能，验证了训练范式的有效性。

Conclusion: 该研究提供了一个新的数据集和一个更有效的训练和评估编译修复模型的范例，为更实用和可靠的自动编程助手铺平了道路。

Abstract: The automated repair of C++ compilation errors presents a significant
challenge, the resolution of which is critical for developer productivity.
Progress in this domain is constrained by two primary factors: the scarcity of
large-scale, high-fidelity datasets and the limitations of conventional
supervised methods, which often fail to generate semantically correct
patches.This paper addresses these gaps by introducing a comprehensive
framework with three core contributions. First, we present CCrepair, a novel,
large-scale C++ compilation error dataset constructed through a sophisticated
generate-and-verify pipeline. Second, we propose a Reinforcement Learning (RL)
paradigm guided by a hybrid reward signal, shifting the focus from mere
compilability to the semantic quality of the fix. Finally, we establish the
robust, two-stage evaluation system providing this signal, centered on an
LLM-as-a-Judge whose reliability has been rigorously validated against the
collective judgments of a panel of human experts. This integrated approach
aligns the training objective with generating high-quality, non-trivial patches
that are both syntactically and semantically correct. The effectiveness of our
approach was demonstrated experimentally. Our RL-trained Qwen2.5-1.5B-Instruct
model achieved performance comparable to a Qwen2.5-14B-Instruct model,
validating the efficiency of our training paradigm. Our work provides the
research community with a valuable new dataset and a more effective paradigm
for training and evaluating robust compilation repair models, paving the way
for more practical and reliable automated programming assistants.

</details>


### [125] [A Nascent Taxonomy of Machine Learning in Intelligent Robotic Process Automation](https://arxiv.org/abs/2509.15730)
*Lukas Laakmann,Seyyid A. Ciftci,Christian Janiesch*

Main category: cs.AI

TL;DR: 本文综述了RPA和机器学习的结合，提出了智能RPA的概念。


<details>
  <summary>Details</summary>
Motivation: RPA在自动化规则明确的任务中应用广泛，但面对复杂任务存在局限性。机器学习的引入为扩展RPA的应用范围提供了机会。

Method: 通过文献回顾，探讨RPA和机器学习之间的联系，并将智能RPA的概念组织成一个分类法。

Result: 提出的分类法包含RPA-ML集成和RPA-ML交互两个元特征，共八个维度：架构和生态系统、能力、数据基础、智能水平、集成技术深度以及部署环境、生命周期阶段和人机关系。

Conclusion: 本文构建了一个智能RPA的分类体系，为未来研究奠定基础。

Abstract: Robotic process automation (RPA) is a lightweight approach to automating
business processes using software robots that emulate user actions at the
graphical user interface level. While RPA has gained popularity for its
cost-effective and timely automation of rule-based, well-structured tasks, its
symbolic nature has inherent limitations when approaching more complex tasks
currently performed by human agents. Machine learning concepts enabling
intelligent RPA provide an opportunity to broaden the range of automatable
tasks. In this paper, we conduct a literature review to explore the connections
between RPA and machine learning and organize the joint concept intelligent RPA
into a taxonomy. Our taxonomy comprises the two meta-characteristics RPA-ML
integration and RPA-ML interaction. Together, they comprise eight dimensions:
architecture and ecosystem, capabilities, data basis, intelligence level, and
technical depth of integration as well as deployment environment, lifecycle
phase, and user-robot relation.

</details>


### [126] [Ontology Creation and Management Tools: the Case of Anatomical Connectivity](https://arxiv.org/abs/2509.15780)
*Natallia Kokash,Bernard de Bono,Tom Gillespie*

Main category: cs.AI

TL;DR: ApiNATOMY: A framework for topological and semantic representation of multiscale physiological circuit maps.


<details>
  <summary>Details</summary>
Motivation: Mapping data related to the peripheral nervous system and other physiological systems.

Method: Creating ApiNATOMY, integrating a Knowledge Representation (KR) model and a suite of Knowledge Management (KM) tools.

Result: Enables physiology experts to easily capture interactions between anatomical entities, while the KM tools help modelers convert high-level abstractions into detailed models of physiological processes, which can be integrated with external ontologies and knowledge graphs.

Conclusion: Developing infrastructure to support researchers in mapping data.

Abstract: We are developing infrastructure to support researchers in mapping data
related to the peripheral nervous system and other physiological systems, with
an emphasis on their relevance to the organs under investigation. The nervous
system, a complex network of nerves and ganglia, plays a critical role in
coordinating and transmitting signals throughout the body. To aid in this, we
have created ApiNATOMY, a framework for the topological and semantic
representation of multiscale physiological circuit maps. ApiNATOMY integrates a
Knowledge Representation (KR) model and a suite of Knowledge Management (KM)
tools. The KR model enables physiology experts to easily capture interactions
between anatomical entities, while the KM tools help modelers convert
high-level abstractions into detailed models of physiological processes, which
can be integrated with external ontologies and knowledge graphs.

</details>


### [127] [A Comparative Study of Rule-Based and Data-Driven Approaches in Industrial Monitoring](https://arxiv.org/abs/2509.15848)
*Giovanni De Gasperis,Sante Dino Facchini*

Main category: cs.AI

TL;DR: 对基于规则和数据驱动的工业监控系统方法进行了比较，提出了一个评估它们关键属性的基本框架。


<details>
  <summary>Details</summary>
Motivation: 工业监控系统正在经历从传统基于规则的架构到利用机器学习和人工智能的数据驱动方法的范式转变。本研究旨在分析这两种方法的优缺点、适用场景，并提出一个评估其关键属性的基本框架。

Method: 比较基于规则和数据驱动的方法，分析它们的优势、局限性和应用场景，并提出了一个评估它们关键属性的基本框架。

Result: 基于规则的系统在稳定环境中具有高可解释性、确定性行为和易于实现等优点，但面临可扩展性、适应性和复杂环境中的性能挑战。数据驱动的系统擅长检测隐藏的异常、实现预测性维护和动态适应新条件，但面临数据可用性、可解释性和集成复杂性等挑战。

Conclusion: 工业监控的未来在于智能、协同的系统，这些系统利用专家知识和数据驱动的洞察力。这种双重方法可增强弹性、运营效率和信任，为更智能、更灵活的工业环境铺平道路。

Abstract: Industrial monitoring systems, especially when deployed in Industry 4.0
environments, are experiencing a shift in paradigm from traditional rule-based
architectures to data-driven approaches leveraging machine learning and
artificial intelligence. This study presents a comparison between these two
methodologies, analyzing their respective strengths, limitations, and
application scenarios, and proposes a basic framework to evaluate their key
properties. Rule-based systems offer high interpretability, deterministic
behavior, and ease of implementation in stable environments, making them ideal
for regulated industries and safety-critical applications. However, they face
challenges with scalability, adaptability, and performance in complex or
evolving contexts. Conversely, data-driven systems excel in detecting hidden
anomalies, enabling predictive maintenance and dynamic adaptation to new
conditions. Despite their high accuracy, these models face challenges related
to data availability, explainability, and integration complexity. The paper
suggests hybrid solutions as a possible promising direction, combining the
transparency of rule-based logic with the analytical power of machine learning.
Our hypothesis is that the future of industrial monitoring lies in intelligent,
synergic systems that leverage both expert knowledge and data-driven insights.
This dual approach enhances resilience, operational efficiency, and trust,
paving the way for smarter and more flexible industrial environments.

</details>


### [128] [Structured Information for Improving Spatial Relationships in Text-to-Image Generation](https://arxiv.org/abs/2509.15962)
*Sander Schildermans,Chang Tian,Ying Jiao,Marie-Francine Moens*

Main category: cs.AI

TL;DR: 这篇论文提出了一种轻量级方法，通过使用基于元组的结构化信息来增强prompt，从而提高文本到图像（T2I）生成中空间关系的准确性。


<details>
  <summary>Details</summary>
Motivation: 目前的文本到图像生成技术在准确捕捉自然语言prompt中描述的空间关系方面存在挑战。

Method: 该方法使用微调的语言模型自动将prompt转换为元组，并无缝集成到T2I流程中。

Result: 实验结果表明，该方法在不影响整体图像质量的前提下，显著提高了空间准确性。自动生成的元组质量与人工生成的元组相当。

Conclusion: 该结构化信息为增强T2I生成中的空间关系提供了一种实用且可移植的解决方案，解决了当前大型生成系统的一个关键限制。

Abstract: Text-to-image (T2I) generation has advanced rapidly, yet faithfully capturing
spatial relationships described in natural language prompts remains a major
challenge. Prior efforts have addressed this issue through prompt optimization,
spatially grounded generation, and semantic refinement. This work introduces a
lightweight approach that augments prompts with tuple-based structured
information, using a fine-tuned language model for automatic conversion and
seamless integration into T2I pipelines. Experimental results demonstrate
substantial improvements in spatial accuracy, without compromising overall
image quality as measured by Inception Score. Furthermore, the automatically
generated tuples exhibit quality comparable to human-crafted tuples. This
structured information provides a practical and portable solution to enhance
spatial relationships in T2I generation, addressing a key limitation of current
large-scale generative systems.

</details>


### [129] [Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers](https://arxiv.org/abs/2509.16058)
*Krati Saxena,Federico Jurado Ruiz,Guido Manzi,Dianbo Liu,Alex Lamb*

Main category: cs.AI

TL;DR: 本研究提出了一种基于注意力模式理论（AST）的注意力控制方法（ASAC），将注意力模式概念融入到人工神经网络中，以提高系统效率。


<details>
  <summary>Details</summary>
Motivation: 注意力机制在人工智能中变得不可或缺，受认知科学中的注意力模式理论（AST）的启发，该理论认为个体通过创建注意力模型来管理注意力，从而有效地分配认知资源。

Method: 该方法将ASAC模块嵌入到Transformer架构中，使用向量量化变分自编码器（VQVAE）作为注意力的抽象器和控制器，从而实现精确的注意力管理。

Result: 在视觉和自然语言处理领域证明了ASAC的有效性，提高了分类精度并加快了学习过程。在各种数据集上的视觉Transformer实验表明，注意力控制器不仅提高了分类精度，而且加速了学习。此外，该模型在噪声和分布外数据集上表现出鲁棒性和泛化能力，并在多任务设置中表现出改进的性能。快速实验表明，基于注意力模式的模块增强了对对抗性攻击的抵抗力，优化了注意力以提高学习效率，并促进了有效的迁移学习和从更少的示例中学习。

Conclusion: 这些有希望的结果建立了认知科学和机器学习之间的联系，揭示了人工智能系统中注意力机制的有效利用。

Abstract: Attention mechanisms have become integral in AI, significantly enhancing
model performance and scalability by drawing inspiration from human cognition.
Concurrently, the Attention Schema Theory (AST) in cognitive science posits
that individuals manage their attention by creating a model of the attention
itself, effectively allocating cognitive resources. Inspired by AST, we
introduce ASAC (Attention Schema-based Attention Control), which integrates the
attention schema concept into artificial neural networks. Our initial
experiments focused on embedding the ASAC module within transformer
architectures. This module employs a Vector-Quantized Variational AutoEncoder
(VQVAE) as both an attention abstractor and controller, facilitating precise
attention management. By explicitly modeling attention allocation, our approach
aims to enhance system efficiency. We demonstrate ASAC's effectiveness in both
the vision and NLP domains, highlighting its ability to improve classification
accuracy and expedite the learning process. Our experiments with vision
transformers across various datasets illustrate that the attention controller
not only boosts classification accuracy but also accelerates learning.
Furthermore, we have demonstrated the model's robustness and generalization
capabilities across noisy and out-of-distribution datasets. In addition, we
have showcased improved performance in multi-task settings. Quick experiments
reveal that the attention schema-based module enhances resilience to
adversarial attacks, optimizes attention to improve learning efficiency, and
facilitates effective transfer learning and learning from fewer examples. These
promising results establish a connection between cognitive science and machine
learning, shedding light on the efficient utilization of attention mechanisms
in AI systems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [130] [Revealing Inherent Concurrency in Event Data: A Partial Order Approach to Process Discovery](https://arxiv.org/abs/2509.15346)
*Humam Kourani,Gyunam Park,Wil M. P. van der Aalst*

Main category: cs.DB

TL;DR: 提出了一种新的、可扩展的算法，该算法直接利用过程发现中的偏序关系。


<details>
  <summary>Details</summary>
Motivation: 传统的过程发现算法线性化事件，无法捕获真实世界过程中固有的并发性。虽然有些技术可以处理部分排序的数据，但它们通常难以在大事件日志上扩展。

Method: 从事件数据中导出部分排序的跟踪，并将它们聚合到一个sound-by-construction、完美拟合的过程模型中。我们的分层算法保留了固有的并发性，同时系统地抽象了互斥选择和循环模式，从而提高了模型的紧凑性和精度。

Result: 在复杂的真实事件日志上证明了该技术的适用性。

Conclusion: 为更真实地表示过程行为提供了一个可扩展的解决方案，尤其是在事件数据中并发普遍存在时。

Abstract: Process discovery algorithms traditionally linearize events, failing to
capture the inherent concurrency of real-world processes. While some techniques
can handle partially ordered data, they often struggle with scalability on
large event logs. We introduce a novel, scalable algorithm that directly
leverages partial orders in process discovery. Our approach derives partially
ordered traces from event data and aggregates them into a
sound-by-construction, perfectly fitting process model. Our hierarchical
algorithm preserves inherent concurrency while systematically abstracting
exclusive choices and loop patterns, enhancing model compactness and precision.
We have implemented our technique and demonstrated its applicability on complex
real-life event logs. Our work contributes a scalable solution for a more
faithful representation of process behavior, especially when concurrency is
prevalent in event data.

</details>


### [131] [Optimization techniques for SQL+ML queries: A performance analysis of real-time feature computation in OpenMLDB](https://arxiv.org/abs/2509.15529)
*Mashkhal A. Sidiq,Aras A. Salih,Samrand M. Hassan*

Main category: cs.DB

TL;DR: 本文优化了OpenMLDB上的SQL+ML查询，OpenMLDB是一个集成了离线和在线特征计算的开源数据库。


<details>
  <summary>Details</summary>
Motivation: 为了提升时间敏感的机器学习应用（如欺诈检测、个性化推荐和时间序列预测）的性能。

Method: 通过改进查询计划、缓存执行计划、并行处理和资源管理来进行优化。使用Docker中富含特征的合成数据集进行实验，模拟生产环境。

Result: OpenMLDB能够支持约12,500 QPS，延迟小于1毫秒，性能优于SparkSQL和ClickHouse（23倍）以及PostgreSQL和MySQL（3.57倍）。查询计划优化贡献了35%的性能提升，缓存贡献了25%，并行处理贡献了20%。

Conclusion: OpenMLDB的模块化优化框架结合了批量和流处理，在需要实时特征计算和服务的应用中，显著优于传统数据库系统。

Abstract: In this study, we optimize SQL+ML queries on top of OpenMLDB, an open-source
database that seamlessly integrates offline and online feature computations.
The work used feature-rich synthetic dataset experiments in Docker, which acted
like production environments that processed 100 to 500 records per batch and 6
to 12 requests per batch in parallel. Efforts have been concentrated in the
areas of better query plans, cached execution plans, parallel processing, and
resource management. The experimental results show that OpenMLDB can support
approximately 12,500 QPS with less than 1 ms latency, outperforming SparkSQL
and ClickHouse by a factor of 23 and PostgreSQL and MySQL by 3.57 times. This
study assessed the impact of optimization and showed that query plan
optimization accounted for 35% of the performance gains, caching for 25%, and
parallel processing for 20%. These results illustrate OpenMLDB's capability for
time-sensitive ML use cases, such as fraud detection, personalized
recommendation, and time series forecasting. The system's modular optimization
framework, which combines batch and stream processing without interference,
contributes to its significant performance gain over traditional database
systems, particularly in applications that require real-time feature
computation and serving. This study contributes to the understanding and design
of high-performance SQL+ML systems and highlights the need for specialized SQL
optimization for ML workloads.

</details>


### [132] [Discovering Top-k Periodic and High-Utility Patterns](https://arxiv.org/abs/2509.15732)
*Qingfeng Zhou,Wensheng Gan,Guoting Chen*

Main category: cs.DB

TL;DR: 本文提出了一种名为TPU的算法，用于提取最重要的top-k周期性高实用性模式，这些模式可能包含负实用性值。


<details>
  <summary>Details</summary>
Motivation: 用户可能对所有周期性高实用性模式不感兴趣，并且提前设置minutil也是一个具有挑战性的问题。

Method: TPU算法利用正负效用列表（PNUL）和周期估计效用共现结构（PEUCS）来存储相关的项集信息。它结合了周期性实际项效用（PIU）、周期性共现效用降序（PCUD）和周期性实际效用（PRU）阈值提升策略来快速提升阈值。

Result: 通过使用所提出的阈值提升策略，在实验中使用的数据集上，运行时间减少了约5%。具体而言，在mushroom_negative和kosarak_negative数据集上，运行时间最多减少了50%，在chess_negative数据集上，运行时间最多减少了10%。内存消耗减少了约2%，在mushroom_negative数据集上观察到最大减少约30%。

Conclusion: 通过大量的实验，我们证明了我们的算法能够准确有效地提取top-k周期性高实用性模式。本文成功地解决了top-k挖掘问题，并为数据科学做出了贡献。

Abstract: With a user-specified minimum utility threshold (minutil), periodic
high-utility pattern mining (PHUPM) aims to identify high-utility patterns that
occur periodically in a transaction database. A pattern is deemed periodic if
its period aligns with the periodicity constraint set by the user. However,
users may not be interested in all periodic high-utility patterns (PHUPs).
Moreover, setting minutil in advance is also a challenging issue. To address
these issues, our research introduces an algorithm called TPU for extracting
the most significant top-k periodic and high-utility patterns that may or may
not include negative utility values. This TPU algorithm utilizes positive and
negative utility lists (PNUL) and period-estimated utility co-occurrence
structure (PEUCS) to store pertinent itemset information. It incorporates the
periodic real item utility (PIU), periodic co-occurrence utility descending
(PCUD), and periodic real utility (PRU) threshold-raising strategies to elevate
the thresholds rapidly. By using the proposed threshold-raising strategies, the
runtime was reduced by approximately 5\% on the datasets used in the
experiments. Specifically, the runtime was reduced by up to 50\% on the
mushroom\_negative and kosarak\_negative datasets, and by up to 10\% on the
chess\_negative dataset. Memory consumption was reduced by about 2\%, with the
largest reduction of about 30\% observed on the mushroom\_negative dataset.
Through extensive experiments, we have demonstrated that our algorithm can
accurately and effectively extract the top-k periodic high-utility patterns.
This paper successfully addresses the top-k mining issue and contributes to
data science.

</details>


### [133] [Utility-based Privacy Preserving Data Mining](https://arxiv.org/abs/2509.15755)
*Qingfeng Zhou,Wensheng Gan,Zhenlian Qi,Philip S. Yu*

Main category: cs.DB

TL;DR: This paper introduces two algorithms, MU-MAP and MU-MIP, to address privacy concerns in periodic information mining by extending the PPUM framework. These algorithms hide sensitive periodic high-utility itemsets while maintaining database utility.


<details>
  <summary>Details</summary>
Motivation: Existing PPUM methods are unsuitable for addressing privacy concerns in periodic information mining. Malicious actors can extract sensitive information from publicly available datasets, posing threats to data providers.

Method: The authors propose two algorithms, MU-MAP and MU-MIP, and design two novel data structures: Sensitive Itemset List (SISL) and Sensitive Item List (SIL).

Result: The proposed algorithms achieve an Artificial Cost (AC) value of 0 on all datasets when hiding sensitive itemsets and maintain Database Utility Similarity (DUS) of over 90%.

Conclusion: The proposed algorithms can successfully hide sensitive periodic itemsets without introducing misleading patterns, outperforming traditional PPUM algorithms.

Abstract: With the advent of big data, periodic pattern mining has demonstrated
significant value in real-world applications, including smart home systems,
healthcare systems, and the medical field. However, advances in network
technology have enabled malicious actors to extract sensitive information from
publicly available datasets, posing significant threats to data providers and,
in severe cases, hindering societal development. To mitigate such risks,
privacy-preserving utility mining (PPUM) has been proposed. However, PPUM is
unsuitable for addressing privacy concerns in periodic information mining. To
address this issue, we innovatively extend the existing PPUM framework and
propose two algorithms, Maximum sensitive Utility-MAximum maxPer item (MU-MAP)
and Maximum sensitive Utility-MInimum maxPer item (MU-MIP). These algorithms
aim to hide sensitive periodic high-utility itemsets while generating sanitized
datasets. To enhance the efficiency of the algorithms, we designed two novel
data structures: the Sensitive Itemset List (SISL) and the Sensitive Item List
(SIL), which store essential information about sensitive itemsets and their
constituent items. Moreover, several performance metrics were employed to
evaluate the performance of our algorithms compared to the state-of-the-art
PPUM algorithms. The experimental results show that our proposed algorithms
achieve an Artificial Cost (AC) value of 0 on all datasets when hiding
sensitive itemsets. In contrast, the traditional PPUM algorithm yields non-zero
AC. This indicates that our algorithms can successfully hide sensitive periodic
itemsets without introducing misleading patterns, whereas the PPUM algorithm
generates additional itemsets that may interfere with user decision-making.
Moreover, the results also reveal that our algorithms maintain Database Utility
Similarity (DUS) of over 90\% after the sensitive itemsets are hidden.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [134] [Efficient and Versatile Model for Multilingual Information Retrieval of Islamic Text: Development and Deployment in Real-World Scenarios](https://arxiv.org/abs/2509.15380)
*Vera Pavlova,Mohammed Makhlouf*

Main category: cs.IR

TL;DR: 本研究利用古兰经多语语料库，探索适用于伊斯兰领域的多语信息检索 (MLIR) 系统的最佳策略。


<details>
  <summary>Details</summary>
Motivation: 现有的多语信息检索 (MLIR) 研究与实际应用之间存在差距，许多研究在孤立的环境中评估性能，限制了其在现实场景中的适用性。

Method: 我们准备了 11 个检索模型，采用四种训练方法：单语、跨语、翻译-训练-全部，以及一种结合跨语和单语技术的新型混合方法。

Result: 在领域内数据集上的评估表明，混合方法在各种检索场景中取得了有希望的结果。

Conclusion: 我们讨论了部署注意事项，强调了为实际 MLIR 应用部署单个通用轻量级模型的成本效益。

Abstract: Despite recent advancements in Multilingual Information Retrieval (MLIR), a
significant gap remains between research and practical deployment. Many studies
assess MLIR performance in isolated settings, limiting their applicability to
real-world scenarios. In this work, we leverage the unique characteristics of
the Quranic multilingual corpus to examine the optimal strategies to develop an
ad-hoc IR system for the Islamic domain that is designed to satisfy users'
information needs in multiple languages. We prepared eleven retrieval models
employing four training approaches: monolingual, cross-lingual,
translate-train-all, and a novel mixed method combining cross-lingual and
monolingual techniques. Evaluation on an in-domain dataset demonstrates that
the mixed approach achieves promising results across diverse retrieval
scenarios. Furthermore, we provide a detailed analysis of how different
training configurations affect the embedding space and their implications for
multilingual retrieval effectiveness. Finally, we discuss deployment
considerations, emphasizing the cost-efficiency of deploying a single
versatile, lightweight model for real-world MLIR applications.

</details>


### [135] [SERVAL: Surprisingly Effective Zero-Shot Visual Document Retrieval Powered by Large Vision and Language Models](https://arxiv.org/abs/2509.15432)
*Thong Nguyen,Yibin Lei,Jia-Huei Ju,Andrew Yates*

Main category: cs.IR

TL;DR: 本文提出了一种零样本的文档图像检索方法，该方法首先使用视觉语言模型生成文档图像的文本描述，然后使用标准文本编码器嵌入这些描述。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉文档检索方法通常使用专门的双编码器进行文本到图像的检索，需要大量的计算资源进行训练。

Method: 该方法使用视觉语言模型生成文档图像的详细文本描述，然后使用标准文本编码器嵌入这些描述。

Result: 在ViDoRe-v2基准测试中，该方法达到了63.4%的nDCG@5，超过了最强的专用多向量视觉文档编码器。

Conclusion: 该方法通过将模态对齐卸载到预训练的视觉语言模型，消除了对计算密集型文本图像对比训练的需求，并为未来的VDR系统建立了强大的零样本基线。

Abstract: Visual Document Retrieval (VDR) typically operates as text-to-image retrieval
using specialized bi-encoders trained to directly embed document images. We
revisit a zero-shot generate-and-encode pipeline: a vision-language model first
produces a detailed textual description of each document image, which is then
embedded by a standard text encoder. On the ViDoRe-v2 benchmark, the method
reaches 63.4% nDCG@5, surpassing the strongest specialised multi-vector visual
document encoder. It also scales better to large collections and offers broader
multilingual coverage. Analysis shows that modern vision-language models
capture complex textual and visual cues with sufficient granularity to act as a
reusable semantic proxy. By offloading modality alignment to pretrained
vision-language models, our approach removes the need for computationally
intensive text-image contrastive training and establishes a strong zero-shot
baseline for future VDR systems.

</details>


### [136] [Dual-Mode Visual System for Brain-Computer Interfaces: Integrating SSVEP and P300 Responses](https://arxiv.org/abs/2509.15439)
*Ekgari Kasawala,Surej Mouli*

Main category: cs.IR

TL;DR: This paper introduces an LED-based dual stimulation apparatus for BCI systems, combining SSVEP and P300 paradigms to improve SSVEP classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Conventional BCI systems using LCD-based visual stimulation have limitations in practical applications. This paper aims to develop a more practical LED-based system.

Method: The system uses four distinct frequencies (7 Hz, 8 Hz, 9 Hz, and 10 Hz) for directional control. Real-time feature extraction is done using FFT and P300 peak detection. Directional control is determined by the frequency with maximal amplitude.

Result: The system achieved a mean classification accuracy of 86.25% and an average ITR of 42.08 bits per minute.

Conclusion: The proposed hybrid system demonstrates improved performance and potential for practical BCI applications.

Abstract: In brain-computer interface (BCI) systems, steady-state visual evoked
potentials (SSVEP) and P300 responses have achieved widespread implementation
owing to their superior information transfer rates (ITR) and minimal training
requirements. These neurophysiological signals have exhibited robust efficacy
and versatility in external device control, demonstrating enhanced precision
and scalability. However, conventional implementations predominantly utilise
liquid crystal display (LCD)-based visual stimulation paradigms, which present
limitations in practical deployment scenarios. This investigation presents the
development and evaluation of a novel light-emitting diode (LED)-based dual
stimulation apparatus designed to enhance SSVEP classification accuracy through
the integration of both SSVEP and P300 paradigms. The system employs four
distinct frequencies, 7 Hz, 8 Hz, 9 Hz, and 10 Hz, corresponding to forward,
backward, right, and left directional controls, respectively. Oscilloscopic
verification confirmed the precision of these stimulation frequencies.
Real-time feature extraction was accomplished through the concurrent analysis
of maximum Fast Fourier Transform (FFT) amplitude and P300 peak detection to
ascertain user intent. Directional control was determined by the frequency
exhibiting maximal amplitude characteristics. The visual stimulation hardware
demonstrated minimal frequency deviation, with error differentials ranging from
0.15%to 0.20%across all frequencies. The implemented signal processing
algorithm successfully discriminated all four stimulus frequencies whilst
correlating them with their respective P300 event markers. Classification
accuracy was evaluated based on correct task intention recognition. The
proposed hybrid system achieved a mean classification accuracy of 86.25%,
coupled with an average ITR of 42.08 bits per minute (bpm).

</details>


### [137] [CFDA & CLIP at TREC iKAT 2025: Enhancing Personalized Conversational Search via Query Reformulation and Rank Fusion](https://arxiv.org/abs/2509.15588)
*Yu-Cheng Chang,Guan-Wei Yeo,Quah Eugene,Fan-Jie Shih,Yuan-Ching Kuo,Tsung-En Yu,Hung-Chun Hsu,Ming-Feng Tsai,Chuan-Ju Wang*

Main category: cs.IR

TL;DR: 这篇论文介绍了参加 2025 TREC iKAT 比赛的系统，该系统同时参加了在线和离线提交任务。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在解决在线任务中实时性、鲁棒性和效率与离线任务中准确性之间的矛盾。

Method: 该研究采用了查询重写和检索融合作为核心策略，并结合 Best-of-$N$ 选择和 RRF 策略来处理不同的提交任务。

Result: 结果表明，重排序和融合可以提高鲁棒性，但也揭示了有效性和效率之间的权衡。

Conclusion: 该研究表明，在 iKAT 比赛中，需要在有效性和效率之间进行权衡，并且重排序和融合是提高鲁棒性的有效方法。

Abstract: The 2025 TREC Interactive Knowledge Assistance Track (iKAT) featured both
interactive and offline submission tasks. The former requires systems to
operate under real-time constraints, making robustness and efficiency as
important as accuracy, while the latter enables controlled evaluation of
passage ranking and response generation with pre-defined datasets. To address
this, we explored query rewriting and retrieval fusion as core strategies. We
built our pipelines around Best-of-$N$ selection and Reciprocal Rank Fusion
(RRF) strategies to handle different submission tasks. Results show that
reranking and fusion improve robustness while revealing trade-offs between
effectiveness and efficiency across both tasks.

</details>


### [138] [Chunk Knowledge Generation Model for Enhanced Information Retrieval: A Multi-task Learning Approach](https://arxiv.org/abs/2509.15658)
*Jisu Kim,Jinhee Park,Changhyun Jeon,Jungwoo Choi,Keonwoo Kim,Minji Hong,Sehyun Kim*

Main category: cs.IR

TL;DR: 提出了一种新的文档扩展方法，通过将文档分成块，并为每个块生成标题和候选问题，以提高检索效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的查询扩展技术存在上下文敏感和性能下降的问题，而现有的文档扩展方法（如Doc2Query）存在预处理成本高、索引大小增加和生成内容可靠性问题。

Method: 提出了一种基于T5的多任务学习结构的“块知识生成模型”，该模型同时从每个文档块生成标题和候选问题，并从用户查询中提取关键词。通过单次编码和两次解码过程并行生成和提取三种类型的语义信息。

Result: 在305个查询-文档对上的GPT评估显示，使用该模型检索的Top@10准确率达到95.41%，优于文档块级别的检索。

Conclusion: 该研究提出了一种同时从文档块生成标题和候选问题的方法，并通过定性评估证明了其在大型信息检索系统中提高检索准确性的潜力。

Abstract: Traditional query expansion techniques for addressing vocabulary mismatch
problems in information retrieval are context-sensitive and may lead to
performance degradation. As an alternative, document expansion research has
gained attention, but existing methods such as Doc2Query have limitations
including excessive preprocessing costs, increased index size, and reliability
issues with generated content. To mitigate these problems and seek more
structured and efficient alternatives, this study proposes a method that
divides documents into chunk units and generates textual data for each chunk to
simultaneously improve retrieval efficiency and accuracy. The proposed "Chunk
Knowledge Generation Model" adopts a T5-based multi-task learning structure
that simultaneously generates titles and candidate questions from each document
chunk while extracting keywords from user queries. This approach maximizes
computational efficiency by generating and extracting three types of semantic
information in parallel through a single encoding and two decoding processes.
The generated data is utilized as additional information in the retrieval
system. GPT-based evaluation on 305 query-document pairs showed that retrieval
using the proposed model achieved 95.41% accuracy at Top@10, demonstrating
superior performance compared to document chunk-level retrieval. This study
contributes by proposing an approach that simultaneously generates titles and
candidate questions from document chunks for application in retrieval
pipelines, and provides empirical evidence applicable to large-scale
information retrieval systems by demonstrating improved retrieval accuracy
through qualitative evaluation.

</details>


### [139] [Understanding Embedding Scaling in Collaborative Filtering](https://arxiv.org/abs/2509.15709)
*Zhuangzhuang He,Zhou Kaiyu,Haoyue Bai,Fengbin Zhu,Yonghui Yang*

Main category: cs.IR

TL;DR: 本文研究了扩展推荐模型中embedding维度的问题，发现双峰和对数现象。


<details>
  <summary>Details</summary>
Motivation: 现有研究认为扩展embedding维度可能导致性能下降，但其根本原因尚不清楚，且不同模型和数据集上是否会出现性能下降仍未可知。

Method: 本文通过在10个不同稀疏程度和规模的数据集上，使用4种代表性的经典架构进行大规模实验。

Result: 本文观察到两种新现象：双峰和对数现象。双峰现象表现为随着embedding维度的增加，性能先提高，然后下降，再次上升，最终下降。对数现象表现为完美的对数曲线。

Conclusion: 本文发现了扩展协同过滤模型时的两种新现象，理解了双峰现象的根本原因，并从理论上分析了协同过滤模型的噪声鲁棒性，结果与经验观察相符。

Abstract: Scaling recommendation models into large recommendation models has become one
of the most widely discussed topics. Recent efforts focus on components beyond
the scaling embedding dimension, as it is believed that scaling embedding may
lead to performance degradation. Although there have been some initial
observations on embedding, the root cause of their non-scalability remains
unclear. Moreover, whether performance degradation occurs across different
types of models and datasets is still an unexplored area. Regarding the effect
of embedding dimensions on performance, we conduct large-scale experiments
across 10 datasets with varying sparsity levels and scales, using 4
representative classical architectures. We surprisingly observe two novel
phenomenon: double-peak and logarithmic. For the former, as the embedding
dimension increases, performance first improves, then declines, rises again,
and eventually drops. For the latter, it exhibits a perfect logarithmic curve.
Our contributions are threefold. First, we discover two novel phenomena when
scaling collaborative filtering models. Second, we gain an understanding of the
underlying causes of the double-peak phenomenon. Lastly, we theoretically
analyze the noise robustness of collaborative filtering models, with results
matching empirical observations.

</details>


### [140] [Optimizing Product Deduplication in E-Commerce with Multimodal Embeddings](https://arxiv.org/abs/2509.15858)
*Aysenur Kulunk,Berk Taskin,M. Furkan Eseoglu,H. Bahadir Sahin*

Main category: cs.IR

TL;DR: 该论文提出了一种可扩展的多模态产品去重方法，用于解决电商平台上的重复商品问题。


<details>
  <summary>Details</summary>
Motivation: 传统的关键词搜索方法难以准确识别重复商品，因为它们依赖于精确的文本匹配，忽略了商品标题中固有的语义相似性。重复商品会导致消费者困惑和运营效率低下。

Method: 该方法采用基于 BERT 架构的领域特定文本模型和用于图像表示的 MaskedAutoEncoders，并结合降维技术生成紧凑的 128 维嵌入。此外，还开发了一种利用文本和图像向量的新型决策模型。通过将这些特征提取机制与 Milvus（一种优化的向量数据库）集成，该系统可以促进跨越超过 2 亿件商品的大型产品目录的高效和高精度相似性搜索。

Result: 该匹配系统实现了 0.90 的宏平均 F1 分数，优于第三方解决方案的 0.83。

Conclusion: 该研究表明，将领域特定适配与最先进的机器学习技术相结合，可以减轻大型电商环境中的重复商品问题。

Abstract: In large scale e-commerce marketplaces, duplicate product listings frequently
cause consumer confusion and operational inefficiencies, degrading trust on the
platform and increasing costs. Traditional keyword-based search methodologies
falter in accurately identifying duplicates due to their reliance on exact
textual matches, neglecting semantic similarities inherent in product titles.
To address these challenges, we introduce a scalable, multimodal product
deduplication designed specifically for the e-commerce domain. Our approach
employs a domain-specific text model grounded in BERT architecture in
conjunction with MaskedAutoEncoders for image representations. Both of these
architectures are augmented with dimensionality reduction techniques to produce
compact 128-dimensional embeddings without significant information loss.
Complementing this, we also developed a novel decider model that leverages both
text and image vectors. By integrating these feature extraction mechanisms with
Milvus, an optimized vector database, our system can facilitate efficient and
high-precision similarity searches across extensive product catalogs exceeding
200 million items with just 100GB of system RAM consumption. Empirical
evaluations demonstrate that our matching system achieves a macro-average F1
score of 0.90, outperforming third-party solutions which attain an F1 score of
0.83. Our findings show the potential of combining domain-specific adaptations
with state-of-the-art machine learning techniques to mitigate duplicate
listings in large-scale e-commerce environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [141] [Pre-Forgettable Models: Prompt Learning as a Native Mechanism for Unlearning](https://arxiv.org/abs/2509.15230)
*Rutger Hendrix,Giovanni Patanè,Leonardo G. Russo,Simone Carnemolla,Giovanni Bellitto,Federica Proietto Salanitri,Concetto Spampinato,Matteo Pennisi*

Main category: cs.LG

TL;DR: 提出了一种新的基于提示学习的框架，该框架将知识获取和删除统一在单个训练阶段中，从而实现即时取消学习，而无需重新训练、模型修改或访问原始数据。


<details>
  <summary>Details</summary>
Motivation: 传统取消学习方法计算成本高昂、脆弱且不适合实时或不断发展的系统。特别是需要根据 GDPR 等隐私框架的要求取消学习特定数据。

Method: 我们引入了一种基于提示的学习框架，该框架将类级语义绑定到专用提示令牌。通过删除相应的提示即可实现即时取消学习。

Result: 该框架在保留类上的预测性能保持不变，同时有效地擦除被遗忘的类。该方法对成员推理攻击具有抵抗力，并且提示删除可防止任何残留知识提取。

Conclusion: 通过将可移除性嵌入到架构本身中，这项工作为设计模块化、可扩展且符合伦理道德的 AI 模型奠定了新的基础。

Abstract: Foundation models have transformed multimedia analysis by enabling robust and
transferable representations across diverse modalities and tasks. However,
their static deployment conflicts with growing societal and regulatory demands
-- particularly the need to unlearn specific data upon request, as mandated by
privacy frameworks such as the GDPR. Traditional unlearning approaches,
including retraining, activation editing, or distillation, are often
computationally expensive, fragile, and ill-suited for real-time or
continuously evolving systems. In this paper, we propose a paradigm shift:
rethinking unlearning not as a retroactive intervention but as a built-in
capability. We introduce a prompt-based learning framework that unifies
knowledge acquisition and removal within a single training phase. Rather than
encoding information in model weights, our approach binds class-level semantics
to dedicated prompt tokens. This design enables instant unlearning simply by
removing the corresponding prompt -- without retraining, model modification, or
access to original data. Experiments demonstrate that our framework preserves
predictive performance on retained classes while effectively erasing forgotten
ones. Beyond utility, our method exhibits strong privacy and security
guarantees: it is resistant to membership inference attacks, and prompt removal
prevents any residual knowledge extraction, even under adversarial conditions.
This ensures compliance with data protection principles and safeguards against
unauthorized access to forgotten information, making the framework suitable for
deployment in sensitive and regulated environments. Overall, by embedding
removability into the architecture itself, this work establishes a new
foundation for designing modular, scalable and ethically responsive AI models.

</details>


### [142] [A Multi-Scale Graph Neural Process with Cross-Drug Co-Attention for Drug-Drug Interactions Prediction](https://arxiv.org/abs/2509.15256)
*Zimo Yan,Jie Zhang,Zheng Xie,Yiping Song,Hao Li*

Main category: cs.LG

TL;DR: MPNP-DDI: A novel graph neural network framework for accurate and reliable drug-drug interaction prediction.


<details>
  <summary>Details</summary>
Motivation: Existing DDI prediction methods fail to capture multi-scale structural information and lack confidence quantification.

Method: Multi-scale graph neural process with message-passing and cross-drug co-attention.

Result: Significantly outperforms state-of-the-art methods on benchmark datasets.

Conclusion: MPNP-DDI is a powerful tool for pharmacovigilance, polypharmacy risk assessment, and precision medicine.

Abstract: Accurate prediction of drug-drug interactions (DDI) is crucial for medication
safety and effective drug development. However, existing methods often struggle
to capture structural information across different scales, from local
functional groups to global molecular topology, and typically lack mechanisms
to quantify prediction confidence. To address these limitations, we propose
MPNP-DDI, a novel Multi-scale Graph Neural Process framework. The core of
MPNP-DDI is a unique message-passing scheme that, by being iteratively applied,
learns a hierarchy of graph representations at multiple scales. Crucially, a
cross-drug co-attention mechanism then dynamically fuses these multi-scale
representations to generate context-aware embeddings for interacting drug
pairs, while an integrated neural process module provides principled
uncertainty estimation. Extensive experiments demonstrate that MPNP-DDI
significantly outperforms state-of-the-art baselines on benchmark datasets. By
providing accurate, generalizable, and uncertainty-aware predictions built upon
multi-scale structural features, MPNP-DDI represents a powerful computational
tool for pharmacovigilance, polypharmacy risk assessment, and precision
medicine.

</details>


### [143] [Generative AI Meets Wireless Sensing: Towards Wireless Foundation Model](https://arxiv.org/abs/2509.15258)
*Zheng Yang,Guoxuan Chi,Chenshu Wu,Hanyu Liu,Yuchong Gao,Yunhao Liu,Jie Xu,Tony Xiao Han*

Main category: cs.LG

TL;DR: 本调查探讨了 GenAI 和无线传感的融合，从两个互补的角度进行研究：GenAI 如何集成到无线传感管道中，以及主流生成模型的适用性和独特优势。


<details>
  <summary>Details</summary>
Motivation: 将 GenAI 集成到无线传感系统中可以显著改进无线传感应用，例如设备定位、人体活动识别和环境监控。

Method: 分析 GenAI 如何作为插件增强特定任务模型以及作为求解器直接解决传感任务这两种集成模式，并分析主流生成模型的特性。

Result: 确定了将 GenAI 应用于无线传感的关键挑战。

Conclusion: 提出了未来无线基础模型的方向：一种统一的、预训练的设计，能够跨不同的传感任务进行可扩展、适应性强和高效的信号理解。

Abstract: Generative Artificial Intelligence (GenAI) has made significant advancements
in fields such as computer vision (CV) and natural language processing (NLP),
demonstrating its capability to synthesize high-fidelity data and improve
generalization. Recently, there has been growing interest in integrating GenAI
into wireless sensing systems. By leveraging generative techniques such as data
augmentation, domain adaptation, and denoising, wireless sensing applications,
including device localization, human activity recognition, and environmental
monitoring, can be significantly improved. This survey investigates the
convergence of GenAI and wireless sensing from two complementary perspectives.
First, we explore how GenAI can be integrated into wireless sensing pipelines,
focusing on two modes of integration: as a plugin to augment task-specific
models and as a solver to directly address sensing tasks. Second, we analyze
the characteristics of mainstream generative models, such as Generative
Adversarial Networks (GANs), Variational Autoencoders (VAEs), and diffusion
models, and discuss their applicability and unique advantages across various
wireless sensing tasks. We further identify key challenges in applying GenAI to
wireless sensing and outline a future direction toward a wireless foundation
model: a unified, pre-trained design capable of scalable, adaptable, and
efficient signal understanding across diverse sensing tasks.

</details>


### [144] [IEFS-GMB: Gradient Memory Bank-Guided Feature Selection Based on Information Entropy for EEG Classification of Neurological Disorders](https://arxiv.org/abs/2509.15259)
*Liang Zhang,Hanyang Dong,Jia-Hong Gao,Yi Sun,Kuntao Xiao,Wanli Yang,Zhao Lv,Shurong Sheng*

Main category: cs.LG

TL;DR: 提出了一种基于信息熵和梯度记忆库的脑电特征选择方法(IEFS-GMB)，用于提高神经系统疾病诊断的准确性。


<details>
  <summary>Details</summary>
Motivation: 脑电信号信噪比低，影响模型性能；现有的特征选择方法缺乏针对性、依赖于特定架构、缺乏可解释性且鲁棒性不足。

Method: 构建动态记忆库存储历史梯度，通过信息熵计算特征重要性，并应用基于熵的权重选择信息量大的脑电特征。

Result: 在四个公共神经系统疾病数据集上的实验表明，使用IEFS-GMB增强的编码器比基线模型实现了0.64%到6.45%的精度提升，并且优于其他特征选择技术。

Conclusion: IEFS-GMB方法提高了模型的可解释性，支持其在临床环境中的实际应用。

Abstract: Deep learning-based EEG classification is crucial for the automated detection
of neurological disorders, improving diagnostic accuracy and enabling early
intervention. However, the low signal-to-noise ratio of EEG signals limits
model performance, making feature selection (FS) vital for optimizing
representations learned by neural network encoders. Existing FS methods are
seldom designed specifically for EEG diagnosis; many are architecture-dependent
and lack interpretability, limiting their applicability. Moreover, most rely on
single-iteration data, resulting in limited robustness to variability. To
address these issues, we propose IEFS-GMB, an Information Entropy-based Feature
Selection method guided by a Gradient Memory Bank. This approach constructs a
dynamic memory bank storing historical gradients, computes feature importance
via information entropy, and applies entropy-based weighting to select
informative EEG features. Experiments on four public neurological disease
datasets show that encoders enhanced with IEFS-GMB achieve accuracy
improvements of 0.64% to 6.45% over baseline models. The method also
outperforms four competing FS techniques and improves model interpretability,
supporting its practical use in clinical settings.

</details>


### [145] [A Weak Supervision Approach for Monitoring Recreational Drug Use Effects in Social Media](https://arxiv.org/abs/2509.15266)
*Lucía Prieto-Santamaría,Alba Cortés Iglesias,Claudio Vidal Giné,Fermín Fernández Calderón,Óscar M. Lozano,Alejandro Rodríguez-González*

Main category: cs.LG

TL;DR: 本研究利用Twitter分析三种新兴精神活性物质（摇头丸、GHB和2C-B）的实际使用效果。


<details>
  <summary>Details</summary>
Motivation: 了解娱乐性药物使用的实际影响仍然是公共卫生和生物医学研究中的一个关键挑战，因为传统的监测系统通常不能充分代表用户的体验。

Method: 通过结合俚语列表和MetaMap生物医学概念提取，识别并弱标注了超过92,000条提及这些物质的推文。每条推文都根据其报告的是正面还是负面效果进行极性标记，然后对不同物质的表型结果进行描述性和比较分析，并训练多个机器学习分类器来预测推文内容的极性。

Result: 结果表明，Twitter能够检测到特定物质的表型效应，并且极性分类模型可以高精度地支持实时药物警戒和药物效应表征。在测试集上，使用成本敏感学习的极限梯度提升获得了最佳性能（F1 = 0.885，AUPRC = 0.934）。

Conclusion: Twitter可以有效用于检测特定物质的表型效应，且极性分类模型能够高精度支持实时药物警戒和药物效应表征。

Abstract: Understanding the real-world effects of recreational drug use remains a
critical challenge in public health and biomedical research, especially as
traditional surveillance systems often underrepresent user experiences. In this
study, we leverage social media (specifically Twitter) as a rich and unfiltered
source of user-reported effects associated with three emerging psychoactive
substances: ecstasy, GHB, and 2C-B. By combining a curated list of slang terms
with biomedical concept extraction via MetaMap, we identified and weakly
annotated over 92,000 tweets mentioning these substances. Each tweet was
labeled with a polarity reflecting whether it reported a positive or negative
effect, following an expert-guided heuristic process. We then performed
descriptive and comparative analyses of the reported phenotypic outcomes across
substances and trained multiple machine learning classifiers to predict
polarity from tweet content, accounting for strong class imbalance using
techniques such as cost-sensitive learning and synthetic oversampling. The top
performance on the test set was obtained from eXtreme Gradient Boosting with
cost-sensitive learning (F1 = 0.885, AUPRC = 0.934). Our findings reveal that
Twitter enables the detection of substance-specific phenotypic effects, and
that polarity classification models can support real-time pharmacovigilance and
drug effect characterization with high accuracy.

</details>


### [146] [Modeling Transformers as complex networks to analyze learning dynamics](https://arxiv.org/abs/2509.15269)
*Elisabetta Rocchetti*

Main category: cs.LG

TL;DR: 本文使用复杂网络理论来理解大型语言模型 (LLM) 在训练过程中如何获得复杂能力。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在训练期间获得复杂能力的学习动态。

Method: 将基于 Transformer 的 LLM 表示为有向加权图，其中节点是模型的计算组件（注意力头和 MLP），边表示因果影响。

Result: 揭示了网络结构经历了探索、巩固和细化等不同阶段。确定了稳定的信息传播者组件层次结构和动态的信息收集器组件集。

Conclusion: 组件级网络视角为可视化和理解驱动 LLM 中功能电路形成的自组织原则提供了一个强大的宏观视角。

Abstract: The process by which Large Language Models (LLMs) acquire complex
capabilities during training remains a key open question in mechanistic
interpretability. This project investigates whether these learning dynamics can
be characterized through the lens of Complex Network Theory (CNT). I introduce
a novel methodology to represent a Transformer-based LLM as a directed,
weighted graph where nodes are the model's computational components (attention
heads and MLPs) and edges represent causal influence, measured via an
intervention-based ablation technique. By tracking the evolution of this
component-graph across 143 training checkpoints of the Pythia-14M model on a
canonical induction task, I analyze a suite of graph-theoretic metrics. The
results reveal that the network's structure evolves through distinct phases of
exploration, consolidation, and refinement. Specifically, I identify the
emergence of a stable hierarchy of information spreader components and a
dynamic set of information gatherer components, whose roles reconfigure at key
learning junctures. This work demonstrates that a component-level network
perspective offers a powerful macroscopic lens for visualizing and
understanding the self-organizing principles that drive the formation of
functional circuits in LLMs.

</details>


### [147] [Partial Column Generation with Graph Neural Networks for Team Formation and Routing](https://arxiv.org/abs/2509.15275)
*Giacomo Dall'Olio,Rainer Kolisch,Yaoxin Wu*

Main category: cs.LG

TL;DR: 提出了一种新的部分列生成策略，用于解决团队组建和路径问题。


<details>
  <summary>Details</summary>
Motivation: 解决团队组建和路径问题是一个具有挑战性的优化问题，在机场、医疗保健和维护运营等领域有多个实际应用。

Method: 基于预测哪些定价问题可能产生具有负降低成本的列，提出了一种新的部分列生成策略，用于具有多个定价问题的设置。开发了一种定制的团队组建和路由问题的机器学习模型，该模型利用图神经网络进行这些预测。

Result: 计算实验表明，应用该策略可以增强解决方案方法，并且优于文献中的传统部分列生成方法，尤其是在严格的时间限制下解决的困难实例。

Conclusion: 该方法在解决团队组建和路径问题上具有优势。

Abstract: The team formation and routing problem is a challenging optimization problem
with several real-world applications in fields such as airport, healthcare, and
maintenance operations. To solve this problem, exact solution methods based on
column generation have been proposed in the literature. In this paper, we
propose a novel partial column generation strategy for settings with multiple
pricing problems, based on predicting which ones are likely to yield columns
with a negative reduced cost. We develop a machine learning model tailored to
the team formation and routing problem that leverages graph neural networks for
these predictions. Computational experiments demonstrate that applying our
strategy enhances the solution method and outperforms traditional partial
column generation approaches from the literature, particularly on hard
instances solved under a tight time limit.

</details>


### [148] [Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning](https://arxiv.org/abs/2509.15279)
*Chi Liu,Derek Li,Yan Shu,Robin Chen,Derek Duan,Teng Fang,Bryan Dai*

Main category: cs.LG

TL;DR: Fleming-R1是一种用于可验证医学推理的模型，它通过结构化数据设计、面向推理的初始化和可验证的强化学习来提升临床推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗应用中显示出潜力，但由于需要准确的答案和透明的推理过程，因此实现专家级别的临床推理仍然具有挑战性。

Method: 该模型引入了三种互补创新：面向推理的数据策略（RODS）、思维链（CoT）冷启动以及基于可验证奖励的强化学习（RLVR）框架。

Result: Fleming-R1在各种医学基准测试中实现了显著的参数高效改进，7B变体超越了更大的基线模型，而32B模型几乎与GPT-4o相当，并且始终优于强大的开源替代方案。

Conclusion: 结构化数据设计、面向推理的初始化和可验证的强化学习可以推动临床推理超越简单的准确性优化。Fleming-R1已公开发布，以促进医学AI的透明、可重复和可审核的进展，从而在高度敏感的临床环境中实现更安全的部署。

Abstract: While large language models show promise in medical applications, achieving
expert-level clinical reasoning remains challenging due to the need for both
accurate answers and transparent reasoning processes. To address this
challenge, we introduce Fleming-R1, a model designed for verifiable medical
reasoning through three complementary innovations. First, our
Reasoning-Oriented Data Strategy (RODS) combines curated medical QA datasets
with knowledge-graph-guided synthesis to improve coverage of underrepresented
diseases, drugs, and multi-hop reasoning chains. Second, we employ
Chain-of-Thought (CoT) cold start to distill high-quality reasoning
trajectories from teacher models, establishing robust inference priors. Third,
we implement a two-stage Reinforcement Learning from Verifiable Rewards (RLVR)
framework using Group Relative Policy Optimization, which consolidates core
reasoning skills while targeting persistent failure modes through adaptive
hard-sample mining. Across diverse medical benchmarks, Fleming-R1 delivers
substantial parameter-efficient improvements: the 7B variant surpasses much
larger baselines, while the 32B model achieves near-parity with GPT-4o and
consistently outperforms strong open-source alternatives. These results
demonstrate that structured data design, reasoning-oriented initialization, and
verifiable reinforcement learning can advance clinical reasoning beyond simple
accuracy optimization. We release Fleming-R1 publicly to promote transparent,
reproducible, and auditable progress in medical AI, enabling safer deployment
in high-stakes clinical environments.

</details>


### [149] [Hybrid unary-binary design for multiplier-less printed Machine Learning classifiers](https://arxiv.org/abs/2509.15316)
*Giorgos Armeniakos,Theodoros Mantzakidis,Dimitrios Soudris*

Main category: cs.LG

TL;DR: 本文提出了一种用于印刷电子器件的混合 unary-binary 架构，以实现高效的 MLP 分类器。


<details>
  <summary>Details</summary>
Motivation: 印刷电子器件为机器学习电路提供了一种灵活、经济高效的替代方案，但其较大的特征尺寸限制了分类器的复杂性。利用印刷电子器件的低制造成本，设计人员可以为特定的 ML 模型定制硬件，从而简化电路设计。

Method: 本文探索了替代算术，并提出了一种混合 unary-binary 架构，该架构消除了昂贵的编码器，并能够高效、无乘法器地执行 MLP 分类器。此外，还引入了架构感知训练，以进一步提高面积和功率效率。

Result: 在六个数据集上的评估表明，面积平均减少 46%，功耗平均降低 39%，且精度损失极小，超过了其他最先进的 MLP 设计。

Conclusion: 本文提出的混合 unary-binary 架构能够有效地降低印刷电子器件上 MLP 分类器的面积和功耗，同时保持较高的精度。

Abstract: Printed Electronics (PE) provide a flexible, cost-efficient alternative to
silicon for implementing machine learning (ML) circuits, but their large
feature sizes limit classifier complexity. Leveraging PE's low fabrication and
NRE costs, designers can tailor hardware to specific ML models, simplifying
circuit design. This work explores alternative arithmetic and proposes a hybrid
unary-binary architecture that removes costly encoders and enables efficient,
multiplier-less execution of MLP classifiers. We also introduce
architecture-aware training to further improve area and power efficiency.
Evaluation on six datasets shows average reductions of 46% in area and 39% in
power, with minimal accuracy loss, surpassing other state-of-the-art MLP
designs.

</details>


### [150] [Kuramoto Orientation Diffusion Models](https://arxiv.org/abs/2509.15328)
*Yue Song,T. Anderson Keller,Sevan Brodjian,Takeru Miyato,Yisong Yue,Pietro Perona,Max Welling*

Main category: cs.LG

TL;DR: 提出了一种基于随机Kuramoto动力学的、基于分数的生成模型，该模型利用周期域来处理具有相干角方向模式的图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有的基于各向同性欧几里得扩散的标准生成方法难以建模富含方向的图像，如指纹和纹理。

Method: 正向过程通过全局或局部耦合振荡器交互以及吸引到全局参考相位来执行相位变量之间的同步，逐渐将数据坍缩为低熵 von Mises 分布。反向过程然后执行去同步，通过反转具有学习的分数函数的动力学来生成不同的模式。

Result: 在一般图像基准测试中取得了有竞争力的结果，并显着提高了指纹和纹理等方向密集型数据集的生成质量。

Conclusion: 这项工作证明了受生物学启发的同步动力学作为生成模型中的结构化先验的潜力。

Abstract: Orientation-rich images, such as fingerprints and textures, often exhibit
coherent angular directional patterns that are challenging to model using
standard generative approaches based on isotropic Euclidean diffusion.
Motivated by the role of phase synchronization in biological systems, we
propose a score-based generative model built on periodic domains by leveraging
stochastic Kuramoto dynamics in the diffusion process. In neural and physical
systems, Kuramoto models capture synchronization phenomena across coupled
oscillators -- a behavior that we re-purpose here as an inductive bias for
structured image generation. In our framework, the forward process performs
\textit{synchronization} among phase variables through globally or locally
coupled oscillator interactions and attraction to a global reference phase,
gradually collapsing the data into a low-entropy von Mises distribution. The
reverse process then performs \textit{desynchronization}, generating diverse
patterns by reversing the dynamics with a learned score function. This approach
enables structured destruction during forward diffusion and a hierarchical
generation process that progressively refines global coherence into fine-scale
details. We implement wrapped Gaussian transition kernels and periodicity-aware
networks to account for the circular geometry. Our method achieves competitive
results on general image benchmarks and significantly improves generation
quality on orientation-dense datasets like fingerprints and textures.
Ultimately, this work demonstrates the promise of biologically inspired
synchronization dynamics as structured priors in generative modeling.

</details>


### [151] [Global Pre-fixing, Local Adjusting: A Simple yet Effective Contrastive Strategy for Continual Learning](https://arxiv.org/abs/2509.15347)
*Jia Tang,Xinrui Wang,Songcan Chen*

Main category: cs.LG

TL;DR: 本文提出了一种新的对比学习策略，名为 GPLASC，用于解决持续学习中的任务间和任务内特征混淆问题。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法由于任务间和任务内特征混淆而受到限制。

Method: 该方法通过将表示的整个单位超球面划分为不重叠的区域，并形成任务间预先固定的等角紧框架（ETF），同时在各自的分配区域内为各个任务形成任务内可调整的 ETF。

Result: 大量实验验证了该方法的有效性。

Conclusion: 该方法能够同时确保任务之间和任务之内的区分性特征结构，并且可以无缝集成到任何现有的对比持续学习框架中。

Abstract: Continual learning (CL) involves acquiring and accumulating knowledge from
evolving tasks while alleviating catastrophic forgetting. Recently, leveraging
contrastive loss to construct more transferable and less forgetful
representations has been a promising direction in CL. Despite advancements,
their performance is still limited due to confusion arising from both
inter-task and intra-task features. To address the problem, we propose a simple
yet effective contrastive strategy named \textbf{G}lobal \textbf{P}re-fixing,
\textbf{L}ocal \textbf{A}djusting for \textbf{S}upervised \textbf{C}ontrastive
learning (GPLASC). Specifically, to avoid task-level confusion, we divide the
entire unit hypersphere of representations into non-overlapping regions, with
the centers of the regions forming an inter-task pre-fixed \textbf{E}quiangular
\textbf{T}ight \textbf{F}rame (ETF). Meanwhile, for individual tasks, our
method helps regulate the feature structure and form intra-task adjustable ETFs
within their respective allocated regions. As a result, our method
\textit{simultaneously} ensures discriminative feature structures both between
tasks and within tasks and can be seamlessly integrated into any existing
contrastive continual learning framework. Extensive experiments validate its
effectiveness.

</details>


### [152] [Probabilistic Conformal Coverage Guarantees in Small-Data Settings](https://arxiv.org/abs/2509.15349)
*Petrus H. Zwart*

Main category: cs.LG

TL;DR: Split conformal prediction provides marginal coverage guarantees, but the realized coverage for a single calibration set may vary substantially. This variance undermines effective risk control in practical applications.


<details>
  <summary>Details</summary>
Motivation: The variance in split conformal prediction undermines effective risk control in practical applications.

Method: Introduce the Small Sample Beta Correction (SSBC), a plug-and-play adjustment to the conformal significance level that leverages the exact finite-sample distribution of conformal coverage to provide probabilistic guarantees.

Result: Ensuring that with user-defined probability over the calibration draw, the deployed predictor achieves at least the desired coverage.

Conclusion: SSBC provides probabilistic guarantees, ensuring that with user-defined probability over the calibration draw, the deployed predictor achieves at least the desired coverage.

Abstract: Conformal prediction provides distribution-free prediction sets with
guaranteed marginal coverage. However, in split conformal prediction this
guarantee is training-conditional only in expectation: across many calibration
draws, the average coverage equals the nominal level, but the realized coverage
for a single calibration set may vary substantially. This variance undermines
effective risk control in practical applications. Here we introduce the Small
Sample Beta Correction (SSBC), a plug-and-play adjustment to the conformal
significance level that leverages the exact finite-sample distribution of
conformal coverage to provide probabilistic guarantees, ensuring that with
user-defined probability over the calibration draw, the deployed predictor
achieves at least the desired coverage.

</details>


### [153] [Predicting Language Models' Success at Zero-Shot Probabilistic Prediction](https://arxiv.org/abs/2509.15356)
*Kevin Ren,Santiago Cortes-Gomez,Carlos Miguel Patiño,Ananya Joshi,Ruiqi Lyu,Jingjing Tang,Alistair Turcan,Khurram Yamin,Steven Wu,Bryan Wilder*

Main category: cs.LG

TL;DR: 大型语言模型在零样本预测任务中的表现不稳定，但当其在基础预测任务中表现良好时，预测概率能更好地反映个体准确性。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型作为零样本模型生成个体特征的能力，并探究用户何时能确信LLM能为其特定任务提供高质量的预测。

Method: 对大型语言模型在各种表格预测任务中的零样本预测能力进行了大规模实证研究。

Result: 大型语言模型的性能高度不稳定，但在基础预测任务中表现良好时，其预测概率能更好地反映个体准确性。构建的指标可以在没有标签数据的情况下预测LLM在任务层面的表现。

Conclusion: 一些无需标签数据的指标可以有效预测LLM在新任务上的预测性能，从而区分LLM适用和不适用的任务。

Abstract: Recent work has investigated the capabilities of large language models (LLMs)
as zero-shot models for generating individual-level characteristics (e.g., to
serve as risk models or augment survey datasets). However, when should a user
have confidence that an LLM will provide high-quality predictions for their
particular task? To address this question, we conduct a large-scale empirical
study of LLMs' zero-shot predictive capabilities across a wide range of tabular
prediction tasks. We find that LLMs' performance is highly variable, both on
tasks within the same dataset and across different datasets. However, when the
LLM performs well on the base prediction task, its predicted probabilities
become a stronger signal for individual-level accuracy. Then, we construct
metrics to predict LLMs' performance at the task level, aiming to distinguish
between tasks where LLMs may perform well and where they are likely unsuitable.
We find that some of these metrics, each of which are assessed without labeled
data, yield strong signals of LLMs' predictive performance on new tasks.

</details>


### [154] [Stochastic Sample Approximations of (Local) Moduli of Continuity](https://arxiv.org/abs/2509.15368)
*Rodion Nazarov,Allen Gehret,Robert Shorten,Jakub Marecek*

Main category: cs.LG

TL;DR: 本文研究了局部连续性模量在评估神经网络鲁棒性和公平性方面的应用。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络的鲁棒性和重复使用中的公平性。

Method: 提出了一种非均匀随机样本逼近局部连续性模量的方法。

Result: 为研究神经网络的鲁棒性和公平性提供了重要方法。

Conclusion: 重新审视了广义导数和局部连续性模量之间的联系。

Abstract: Modulus of local continuity is used to evaluate the robustness of neural
networks and fairness of their repeated uses in closed-loop models. Here, we
revisit a connection between generalized derivatives and moduli of local
continuity, and present a non-uniform stochastic sample approximation for
moduli of local continuity. This is of importance in studying robustness of
neural networks and fairness of their repeated uses.

</details>


### [155] [Adversarial generalization of unfolding (model-based) networks](https://arxiv.org/abs/2509.15370)
*Vicky Kouni*

Main category: cs.LG

TL;DR: 本文研究了展开网络在对抗攻击下的泛化能力，这是首次对此类网络进行理论分析。


<details>
  <summary>Details</summary>
Motivation: 在压缩感知等逆问题中，展开网络的应用越来越广泛，尤其是在对抗鲁棒性至关重要的领域。然而，对于展开网络在对抗攻击下的性能，理论理解还处于初期阶段。

Method: 本文选择了一系列最先进的过参数化展开网络，并采用了一种新的框架来估计它们的对抗Rademacher复杂度。

Result: 本文为所研究的网络提供了对抗泛化误差界限，这些界限对于攻击水平来说是严格的。通过在真实世界数据上进行的一系列实验，结果证实了本文的理论推导。

Conclusion: 本文观察到，该系列的过参数化可以被利用来提高对抗鲁棒性，从而为如何有效地增强神经网络的鲁棒性提供了启示。

Abstract: Unfolding networks are interpretable networks emerging from iterative
algorithms, incorporate prior knowledge of data structure, and are designed to
solve inverse problems like compressed sensing, which deals with recovering
data from noisy, missing observations. Compressed sensing finds applications in
critical domains, from medical imaging to cryptography, where adversarial
robustness is crucial to prevent catastrophic failures. However, a solid
theoretical understanding of the performance of unfolding networks in the
presence of adversarial attacks is still in its infancy. In this paper, we
study the adversarial generalization of unfolding networks when perturbed with
$l_2$-norm constrained attacks, generated by the fast gradient sign method.
Particularly, we choose a family of state-of-the-art overaparameterized
unfolding networks and deploy a new framework to estimate their adversarial
Rademacher complexity. Given this estimate, we provide adversarial
generalization error bounds for the networks under study, which are tight with
respect to the attack level. To our knowledge, this is the first theoretical
analysis on the adversarial generalization of unfolding networks. We further
present a series of experiments on real-world data, with results corroborating
our derived theory, consistently for all data. Finally, we observe that the
family's overparameterization can be exploited to promote adversarial
robustness, shedding light on how to efficiently robustify neural networks.

</details>


### [156] [Learning in Stackelberg Mean Field Games: A Non-Asymptotic Analysis](https://arxiv.org/abs/2509.15392)
*Sihan Zeng,Benjamin Patrick Evans,Sujay Bhatt,Leo Ardon,Sumitra Ganesh,Alec Koppel*

Main category: cs.LG

TL;DR: 本文研究了Stackelberg平均场博弈(MFG)中的策略优化，这是一种用于建模单个领导者和无限数量的同质追随者之间战略互动的分层框架。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于领导者和追随者目标之间严格的独立性假设，由于嵌套循环算法结构导致样本使用效率低下，并且缺乏有限时间收敛保证。为了解决这些局限性，我们提出了AC-SMFG。

Method: 一种在连续生成的马尔可夫样本上运行的单循环actor-critic算法。该算法在领导者、代表性追随者和平均场之间交替进行(半)梯度更新，并且在实践中易于实现。

Result: 建立了算法对Stackelberg目标平稳点的有限时间和有限样本收敛性。仿真结果表明，AC-SMFG在策略质量和收敛速度方面优于现有的多智能体和MFG学习基线。

Conclusion: 据我们所知，这是第一个具有非渐近收敛保证的Stackelberg MFG算法。

Abstract: We study policy optimization in Stackelberg mean field games (MFGs), a
hierarchical framework for modeling the strategic interaction between a single
leader and an infinitely large population of homogeneous followers. The
objective can be formulated as a structured bi-level optimization problem, in
which the leader needs to learn a policy maximizing its reward, anticipating
the response of the followers. Existing methods for solving these (and related)
problems often rely on restrictive independence assumptions between the
leader's and followers' objectives, use samples inefficiently due to
nested-loop algorithm structure, and lack finite-time convergence guarantees.
To address these limitations, we propose AC-SMFG, a single-loop actor-critic
algorithm that operates on continuously generated Markovian samples. The
algorithm alternates between (semi-)gradient updates for the leader, a
representative follower, and the mean field, and is simple to implement in
practice. We establish the finite-time and finite-sample convergence of the
algorithm to a stationary point of the Stackelberg objective. To our knowledge,
this is the first Stackelberg MFG algorithm with non-asymptotic convergence
guarantees. Our key assumption is a "gradient alignment" condition, which
requires that the full policy gradient of the leader can be approximated by a
partial component of it, relaxing the existing leader-follower independence
assumption. Simulation results in a range of well-established economics
environments demonstrate that AC-SMFG outperforms existing multi-agent and MFG
learning baselines in policy quality and convergence speed.

</details>


### [157] [VMDNet: Time Series Forecasting with Leakage-Free Samplewise Variational Mode Decomposition and Multibranch Decoding](https://arxiv.org/abs/2509.15394)
*Weibin Feng,Ran Tao,John Cartlidge,Jin Zheng*

Main category: cs.LG

TL;DR: 提出了一种名为 VMDNet 的时间序列预测框架，该框架通过改进变分模态分解 (VMD) 来提升预测性能，特别是在具有强周期性的数据集中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有研究存在信息泄露和超参数调整不当的问题。

Method: 该框架采用样本级 VMD 避免泄露，使用频率感知嵌入表示每个分解模式，并使用并行时间卷积网络 (TCN) 解码，引入双层 Stackelberg 优化自适应选择 VMD 的两个核心超参数：模态数量 (K) 和带宽惩罚 (alpha)。

Result: 在两个能源相关数据集上的实验表明，VMDNet 在周期性较强时取得了最先进的结果。

Conclusion: VMDNet 在捕获结构化周期性模式方面具有明显优势，并且在弱周期性下仍然保持稳健。

Abstract: In time series forecasting, capturing recurrent temporal patterns is
essential; decomposition techniques make such structure explicit and thereby
improve predictive performance. Variational Mode Decomposition (VMD) is a
powerful signal-processing method for periodicity-aware decomposition and has
seen growing adoption in recent years. However, existing studies often suffer
from information leakage and rely on inappropriate hyperparameter tuning. To
address these issues, we propose VMDNet, a causality-preserving framework that
(i) applies sample-wise VMD to avoid leakage; (ii) represents each decomposed
mode with frequency-aware embeddings and decodes it using parallel temporal
convolutional networks (TCNs), ensuring mode independence and efficient
learning; and (iii) introduces a bilevel, Stackelberg-inspired optimisation to
adaptively select VMD's two core hyperparameters: the number of modes (K) and
the bandwidth penalty (alpha). Experiments on two energy-related datasets
demonstrate that VMDNet achieves state-of-the-art results when periodicity is
strong, showing clear advantages in capturing structured periodic patterns
while remaining robust under weak periodicity.

</details>


### [158] [Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization](https://arxiv.org/abs/2509.15399)
*Xiaochuan Gong,Jie Hao,Mingrui Liu*

Main category: cs.LG

TL;DR: 本文针对随机分层优化问题，提出了自适应算法，无需预先了解噪声水平即可实现最优收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有的分层优化算法在随机优化环境中缺乏适应性，无法在不同梯度噪声水平下实现最优收敛速度，且需要预先了解噪声大小。

Method: 针对非凸-强凹极小极大优化和非凸-强凸双层优化这两类问题，结合动量归一化技术与新的自适应参数选择方法设计了自适应算法。

Result: 所提出的算法在 T 次迭代中实现了梯度范数的 $\widetilde{O}(1/\sqrt{T} + \sqrt{\bar{\sigma}}/T^{1/4})$ 的最优收敛速度，且无需预先了解噪声水平。

Conclusion: 该研究为随机分层优化提供了首个自适应且精确的收敛保证。在合成数据和深度学习任务上的大量实验验证了所提算法的有效性。

Abstract: Hierarchical optimization refers to problems with interdependent decision
variables and objectives, such as minimax and bilevel formulations. While
various algorithms have been proposed, existing methods and analyses lack
adaptivity in stochastic optimization settings: they cannot achieve optimal
convergence rates across a wide spectrum of gradient noise levels without prior
knowledge of the noise magnitude. In this paper, we propose novel adaptive
algorithms for two important classes of stochastic hierarchical optimization
problems: nonconvex-strongly-concave minimax optimization and
nonconvex-strongly-convex bilevel optimization. Our algorithms achieve sharp
convergence rates of $\widetilde{O}(1/\sqrt{T} + \sqrt{\bar{\sigma}}/T^{1/4})$
in $T$ iterations for the gradient norm, where $\bar{\sigma}$ is an upper bound
on the stochastic gradient noise. Notably, these rates are obtained without
prior knowledge of the noise level, thereby enabling automatic adaptivity in
both low and high-noise regimes. To our knowledge, this work provides the first
adaptive and sharp convergence guarantees for stochastic hierarchical
optimization. Our algorithm design combines the momentum normalization
technique with novel adaptive parameter choices. Extensive experiments on
synthetic and deep learning tasks demonstrate the effectiveness of our proposed
algorithms.

</details>


### [159] [Exploring multimodal implicit behavior learning for vehicle navigation in simulated cities](https://arxiv.org/abs/2509.15400)
*Eric Aislan Antonelo,Gustavo Claudio Karl Couto,Christian Möller*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为数据增强隐式行为克隆（DA-IBC）的方法，用于解决标准行为克隆（BC）在学习多模态驾驶决策方面的不足。


<details>
  <summary>Details</summary>
Motivation: 标准行为克隆（BC）无法学习多模态驾驶决策，即在同一场景下存在多个有效动作的情况。

Method: 该论文探索了基于能量模型（EBMs）的隐式行为克隆（IBC），并提出了数据增强隐式行为克隆（DA-IBC）。DA-IBC通过扰动专家动作来形成IBC训练的反例，并为无导数推理使用更好的初始化，从而改进学习。

Result: 在CARLA模拟器中使用鸟瞰图输入的实验表明，在旨在评估测试环境中多模态行为学习的城市驾驶任务中，DA-IBC优于标准IBC。学习到的能量分布能够表示多模态动作分布，而BC无法实现。

Conclusion: DA-IBC 能够更好地捕捉多模态驾驶决策，优于标准 BC。

Abstract: Standard Behavior Cloning (BC) fails to learn multimodal driving decisions,
where multiple valid actions exist for the same scenario. We explore Implicit
Behavioral Cloning (IBC) with Energy-Based Models (EBMs) to better capture this
multimodality. We propose Data-Augmented IBC (DA-IBC), which improves learning
by perturbing expert actions to form the counterexamples of IBC training and
using better initialization for derivative-free inference. Experiments in the
CARLA simulator with Bird's-Eye View inputs demonstrate that DA-IBC outperforms
standard IBC in urban driving tasks designed to evaluate multimodal behavior
learning in a test environment. The learned energy landscapes are able to
represent multimodal action distributions, which BC fails to achieve.

</details>


### [160] [Top-$k$ Feature Importance Ranking](https://arxiv.org/abs/2509.15420)
*Yuxi Chen,Tiffany Tang,Genevera Allen*

Main category: cs.LG

TL;DR: 提出了一种名为 RAMPART 的新框架，用于对重要特征进行排序，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 对重要特征进行排序在可解释机器学习中至关重要，但受到的关注较少。

Method: 结合自适应连续减半策略和集成技术，优化排序准确性。

Result: 在模拟研究中优于现有方法，并在基因组学案例研究中得到验证。

Conclusion: RAMPART 能够在温和条件下以高概率实现正确的 top-k 排序。

Abstract: Accurate ranking of important features is a fundamental challenge in
interpretable machine learning with critical applications in scientific
discovery and decision-making. Unlike feature selection and feature importance,
the specific problem of ranking important features has received considerably
less attention. We introduce RAMPART (Ranked Attributions with MiniPatches And
Recursive Trimming), a framework that utilizes any existing feature importance
measure in a novel algorithm specifically tailored for ranking the top-$k$
features. Our approach combines an adaptive sequential halving strategy that
progressively focuses computational resources on promising features with an
efficient ensembling technique using both observation and feature subsampling.
Unlike existing methods that convert importance scores to ranks as
post-processing, our framework explicitly optimizes for ranking accuracy. We
provide theoretical guarantees showing that RAMPART achieves the correct
top-$k$ ranking with high probability under mild conditions, and demonstrate
through extensive simulation studies that RAMPART consistently outperforms
popular feature importance methods, concluding with a high-dimensional genomics
case study.

</details>


### [161] [Random Matrix Theory-guided sparse PCA for single-cell RNA-seq data](https://arxiv.org/abs/2509.15429)
*Victor Chardès*

Main category: cs.LG

TL;DR: 本研究提出了一种基于随机矩阵理论（RMT）的方法，以改进单细胞RNA-seq数据的降维。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA-seq数据噪声大，计算流程难以适应异构数据集或不断发展的技术。因此，大多数研究仍然依赖于主成分分析（PCA）进行降维。

Method: 该方法包括一种新颖的双重白化方法，该方法同时稳定基因和细胞的方差。这使得可以使用基于RMT的标准来自动选择稀疏度级别，从而使稀疏PCA几乎无参数。

Result: 在七种单细胞RNA-seq技术和四种稀疏PCA算法中，该方法系统地改进了主子空间的重建，并且在细胞类型分类任务中始终优于基于PCA、自动编码器和扩散的方法。

Conclusion: 该方法保留了PCA的可解释性，同时实现了对稀疏主成分的稳健、自动的推断。

Abstract: Single-cell RNA-seq provides detailed molecular snapshots of individual cells
but is notoriously noisy. Variability stems from biological differences, PCR
amplification bias, limited sequencing depth, and low capture efficiency,
making it challenging to adapt computational pipelines to heterogeneous
datasets or evolving technologies. As a result, most studies still rely on
principal component analysis (PCA) for dimensionality reduction, valued for its
interpretability and robustness. Here, we improve upon PCA with a Random Matrix
Theory (RMT)-based approach that guides the inference of sparse principal
components using existing sparse PCA algorithms. We first introduce a novel
biwhitening method, inspired by the Sinkhorn-Knopp algorithm, that
simultaneously stabilizes variance across genes and cells. This enables the use
of an RMT-based criterion to automatically select the sparsity level, rendering
sparse PCA nearly parameter-free. Our mathematically grounded approach retains
the interpretability of PCA while enabling robust, hands-off inference of
sparse principal components. Across seven single-cell RNA-seq technologies and
four sparse PCA algorithms, we show that this method systematically improves
the reconstruction of the principal subspace and consistently outperforms PCA-,
autoencoder-, and diffusion-based methods in cell-type classification tasks.

</details>


### [162] [Computing Linear Regions in Neural Networks with Skip Connections](https://arxiv.org/abs/2509.15441)
*Johnny Joyce,Jan Verschelde*

Main category: cs.LG

TL;DR: 这篇论文研究了使用热带算术表示分段线性激活函数的神经网络，从而应用热带几何。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络的训练难题，特别是过拟合问题以及跳跃连接的优势。

Method: 提出算法来计算神经网络是线性映射的区域，并通过计算实验进行分析。

Result: 通过计算实验，提供了关于训练神经网络难度的见解。

Conclusion: 研究结果深入了解了神经网络的训练，包括过拟合问题和跳跃连接的优点

Abstract: Neural networks are important tools in machine learning. Representing
piecewise linear activation functions with tropical arithmetic enables the
application of tropical geometry. Algorithms are presented to compute regions
where the neural networks are linear maps. Through computational experiments,
we provide insights on the difficulty to train neural networks, in particular
on the problems of overfitting and on the benefits of skip connections.

</details>


### [163] [Hierarchical Self-Attention: Generalizing Neural Attention Mechanics to Multi-Scale Problems](https://arxiv.org/abs/2509.15448)
*Saeed Amizadeh,Sara Abdali,Yinheng Li,Kazuhito Koishida*

Main category: cs.LG

TL;DR: 本文提出了一种新的注意力机制，用于处理多模态、多尺度数据。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在处理多模态、多尺度数据时存在泛化性问题，且缺乏统一的理论基础。

Method: 本文首先提出了一个数学结构来表示多模态、多尺度数据，然后从熵最小化原理推导出了神经注意力机制，并提出了一种基于动态规划的算法。

Result: 所提出的分层注意力机制可以从头开始训练Transformer模型，也可以在训练后将分层信息注入到预训练的Transformer模型中，从而提高模型的效率。

Conclusion: 本文提出的分层注意力机制在处理多模态、多尺度数据方面具有优势，并且可以提高Transformer模型的效率。

Abstract: Transformers and their attention mechanism have been revolutionary in the
field of Machine Learning. While originally proposed for the language data,
they quickly found their way to the image, video, graph, etc. data modalities
with various signal geometries. Despite this versatility, generalizing the
attention mechanism to scenarios where data is presented at different scales
from potentially different modalities is not straightforward. The attempts to
incorporate hierarchy and multi-modality within transformers are largely based
on ad hoc heuristics, which are not seamlessly generalizable to similar
problems with potentially different structures. To address this problem, in
this paper, we take a fundamentally different approach: we first propose a
mathematical construct to represent multi-modal, multi-scale data. We then
mathematically derive the neural attention mechanics for the proposed construct
from the first principle of entropy minimization. We show that the derived
formulation is optimal in the sense of being the closest to the standard
Softmax attention while incorporating the inductive biases originating from the
hierarchical/geometric information of the problem. We further propose an
efficient algorithm based on dynamic programming to compute our derived
attention mechanism. By incorporating it within transformers, we show that the
proposed hierarchical attention mechanism not only can be employed to train
transformer models in hierarchical/multi-modal settings from scratch, but it
can also be used to inject hierarchical information into classical, pre-trained
transformer models post training, resulting in more efficient models in
zero-shot manner.

</details>


### [164] [IMPQ: Interaction-Aware Layerwise Mixed Precision Quantization for LLMs](https://arxiv.org/abs/2509.15455)
*Junchen Zhao,Ali Derakhshan,Dushyant Bharadwaj,Jayden Kana Hyman,Junhao Dong,Sangeetha Abdu Jyothi,Ian Harris*

Main category: cs.LG

TL;DR: 本文提出了一种新的混合精度量化方法，用于在低资源设备上部署大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有混合精度量化方法在平均精度低于 4 位时表现不佳，因为它们忽略了层间交互。

Method: 提出了基于 Shapley 值的渐进量化估计 (SPQE) 方法，以及交互感知混合精度量化 (IMPQ) 方法。

Result: 在 Llama-3、Gemma-2 和 Qwen-3 模型上进行了实验，结果表明 IMPQ 的性能优于现有方法。

Conclusion: IMPQ 可以在严格的内存约束下，将层精度分配为 2 或 4 位，并且在低比特位宽下，困惑度降低了 20% 到 80%。

Abstract: Large Language Models (LLMs) promise impressive capabilities, yet their
multi-billion-parameter scale makes on-device or low-resource deployment
prohibitive. Mixed-precision quantization offers a compelling solution, but
existing methods struggle when the average precision drops below four bits, as
they rely on isolated, layer-specific metrics that overlook critical
inter-layer interactions affecting overall performance. In this paper, we
propose two innovations to address these limitations. First, we frame the
mixed-precision quantization problem as a cooperative game among layers and
introduce Shapley-based Progressive Quantization Estimation (SPQE) to
efficiently obtain accurate Shapley estimates of layer sensitivities and
inter-layer interactions. Second, building upon SPQE, we propose
Interaction-aware Mixed-Precision Quantization (IMPQ) which translates these
Shapley estimates into a binary quadratic optimization formulation, assigning
either 2 or 4-bit precision to layers under strict memory constraints.
Comprehensive experiments conducted on Llama-3, Gemma-2, and Qwen-3 models
across three independent PTQ backends (Quanto, HQQ, GPTQ) demonstrate IMPQ's
scalability and consistently superior performance compared to methods relying
solely on isolated metrics. Across average precisions spanning 4 bit down to 2
bit, IMPQ cuts Perplexity by 20 to 80 percent relative to the best baseline,
with the margin growing as the bit-width tightens.

</details>


### [165] [Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs](https://arxiv.org/abs/2509.15464)
*Junhong Lin,Song Wang,Xiaojie Guo,Julian Shun,Yada Zhu*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为EvoReasoner的时间感知多跳推理算法，并结合EvoKG知识图谱进化模块，以解决大型语言模型在处理随时间演变的知识时遇到的困难。该方法通过全局-局部实体 grounding、多路径分解和时间 grounding 评分来进行推理，并通过基于置信度的矛盾解决和时间趋势跟踪来更新知识图谱。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常假设知识图谱是静态的，忽略了真实世界数据中的时间动态性和事实不一致性。为了解决LLM在处理时间演变知识时遇到的困难。

Method: 提出了EvoReasoner，一个时间感知多跳推理算法，它执行全局-局部实体 grounding、多路径分解和时间 grounding 评分。同时，引入EvoKG，一个噪声容忍的知识图谱进化模块，通过基于置信度的矛盾解决和时间趋势跟踪，从非结构化文档中增量更新知识图谱。

Result: 该方法在时间QA基准测试和新的端到端设置中优于基于提示和KG增强的基线，有效地缩小了小型和大型LLM在动态问答方面的差距。一个8B参数的模型使用该方法匹配了671B模型七个月后的性能。

Conclusion: 结合时间推理和知识图谱进化对于实现鲁棒和最新的LLM性能至关重要。

Abstract: Large language models (LLMs) excel at many language understanding tasks but
struggle to reason over knowledge that evolves. To address this, recent work
has explored augmenting LLMs with knowledge graphs (KGs) to provide structured,
up-to-date information. However, many existing approaches assume a static
snapshot of the KG and overlook the temporal dynamics and factual
inconsistencies inherent in real-world data. To address the challenge of
reasoning over temporally shifting knowledge, we propose EvoReasoner, a
temporal-aware multi-hop reasoning algorithm that performs global-local entity
grounding, multi-route decomposition, and temporally grounded scoring. To
ensure that the underlying KG remains accurate and up-to-date, we introduce
EvoKG, a noise-tolerant KG evolution module that incrementally updates the KG
from unstructured documents through confidence-based contradiction resolution
and temporal trend tracking. We evaluate our approach on temporal QA benchmarks
and a novel end-to-end setting where the KG is dynamically updated from raw
documents. Our method outperforms both prompting-based and KG-enhanced
baselines, effectively narrowing the gap between small and large LLMs on
dynamic question answering. Notably, an 8B-parameter model using our approach
matches the performance of a 671B model prompted seven months later. These
results highlight the importance of combining temporal reasoning with KG
evolution for robust and up-to-date LLM performance. Our code is publicly
available at github.com/junhongmit/TREK.

</details>


### [166] [Solar Forecasting with Causality: A Graph-Transformer Approach to Spatiotemporal Dependencies](https://arxiv.org/abs/2509.15481)
*Yanan Niu,Demetri Psaltis,Christophe Moser,Luisa Lambertini*

Main category: cs.LG

TL;DR: SolarCAST: A causal model for solar forecasting using only historical GHI data from the target site and nearby stations.


<details>
  <summary>Details</summary>
Motivation: Accurate solar forecasting is important for renewable energy management, but existing methods rely on specialized hardware and heavy preprocessing.

Method: SolarCAST models confounding factors using embeddings, a spatio-temporal graph neural network, and a gated transformer.

Result: SolarCAST outperforms leading baselines and reduces error by 25.9% compared to a commercial forecaster.

Conclusion: SolarCAST is a lightweight, practical, and generalizable solution for localized solar forecasting.

Abstract: Accurate solar forecasting underpins effective renewable energy management.
We present SolarCAST, a causally informed model predicting future global
horizontal irradiance (GHI) at a target site using only historical GHI from
site X and nearby stations S - unlike prior work that relies on sky-camera or
satellite imagery requiring specialized hardware and heavy preprocessing. To
deliver high accuracy with only public sensor data, SolarCAST models three
classes of confounding factors behind X-S correlations using scalable neural
components: (i) observable synchronous variables (e.g., time of day, station
identity), handled via an embedding module; (ii) latent synchronous factors
(e.g., regional weather patterns), captured by a spatio-temporal graph neural
network; and (iii) time-lagged influences (e.g., cloud movement across
stations), modeled with a gated transformer that learns temporal shifts. It
outperforms leading time-series and multimodal baselines across diverse
geographical conditions, and achieves a 25.9% error reduction over the top
commercial forecaster, Solcast. SolarCAST offers a lightweight, practical, and
generalizable solution for localized solar forecasting.

</details>


### [167] [FRAUDGUESS: Spotting and Explaining New Types of Fraud in Million-Scale Financial Data](https://arxiv.org/abs/2509.15493)
*Robson L. F. Cordeiro,Meng-Chieh Lee,Christos Faloutsos*

Main category: cs.LG

TL;DR: 提出FRAUDGUESS用于检测金融交易中的欺诈行为，包括新类型的欺诈。


<details>
  <summary>Details</summary>
Motivation: 在金融交易中发现欺诈交易，包括已知的和未知的欺诈类型。

Method: 通过在精心设计的特征空间中将新类型的欺诈视为微集群来检测欺诈，并使用可视化和热图提供证据。

Result: 在真实的大规模金融数据集中发现了三种新的欺诈行为，其中两种被领域专家认为是欺诈或可疑行为，并捕捉到数百个原本不会被注意到的欺诈交易。

Conclusion: FRAUDGUESS在实际生活中得到应用，并正在考虑部署在匿名金融机构中。

Abstract: Given a set of financial transactions (who buys from whom, when, and for how
much), as well as prior information from buyers and sellers, how can we find
fraudulent transactions? If we have labels for some transactions for known
types of fraud, we can build a classifier. However, we also want to find new
types of fraud, still unknown to the domain experts ('Detection'). Moreover, we
also want to provide evidence to experts that supports our opinion
('Justification'). In this paper, we propose FRAUDGUESS, to achieve two goals:
(a) for 'Detection', it spots new types of fraud as micro-clusters in a
carefully designed feature space; (b) for 'Justification', it uses
visualization and heatmaps for evidence, as well as an interactive dashboard
for deep dives. FRAUDGUESS is used in real life and is currently considered for
deployment in an Anonymous Financial Institution (AFI). Thus, we also present
the three new behaviors that FRAUDGUESS discovered in a real, million-scale
financial dataset. Two of these behaviors are deemed fraudulent or suspicious
by domain experts, catching hundreds of fraudulent transactions that would
otherwise go un-noticed.

</details>


### [168] [Detail Across Scales: Multi-Scale Enhancement for Full Spectrum Neural Representations](https://arxiv.org/abs/2509.15494)
*Yuan Ni,Zhantao Chen,Cheng Peng,Rajan Plumley,Chun Hong Yoon,Jana B. Thayer,Joshua J. Turner*

Main category: cs.LG

TL;DR: WIEN-INR: A wavelet-informed implicit neural representation for high-fidelity scientific data encoding.


<details>
  <summary>Details</summary>
Motivation: Existing INR approaches struggle to faithfully represent the multi-scale structures, high-frequency information, and fine textures in scientific datasets when constrained to compact network sizes.

Method: A multi-scale architecture that distributes modeling across different resolution scales and employs a specialized kernel network at the finest scale.

Result: WIEN-INR achieves superior reconstruction fidelity while maintaining a compact model size on diverse scientific datasets.

Conclusion: WIEN-INR is a practical neural representation framework for high-fidelity scientific data encoding, extending the applicability of INRs to domains where efficient preservation of fine detail is essential.

Abstract: Implicit neural representations (INRs) have emerged as a compact and
parametric alternative to discrete array-based data representations, encoding
information directly in neural network weights to enable resolution-independent
representation and memory efficiency. However, existing INR approaches, when
constrained to compact network sizes, struggle to faithfully represent the
multi-scale structures, high-frequency information, and fine textures that
characterize the majority of scientific datasets. To address this limitation,
we propose WIEN-INR, a wavelet-informed implicit neural representation that
distributes modeling across different resolution scales and employs a
specialized kernel network at the finest scale to recover subtle details. This
multi-scale architecture allows for the use of smaller networks to retain the
full spectrum of information while preserving the training efficiency and
reducing storage cost. Through extensive experiments on diverse scientific
datasets spanning different scales and structural complexities, WIEN-INR
achieves superior reconstruction fidelity while maintaining a compact model
size. These results demonstrate WIEN-INR as a practical neural representation
framework for high-fidelity scientific data encoding, extending the
applicability of INRs to domains where efficient preservation of fine detail is
essential.

</details>


### [169] [Mental Accounts for Actions: EWA-Inspired Attention in Decision Transformers](https://arxiv.org/abs/2509.15498)
*Zahra Aref,Narayan B. Mandayam*

Main category: cs.LG

TL;DR: 提出了一种名为EWA-VQ-ODT的轻量级模块，通过维护每个动作的心理账户来总结最近的成功和失败，从而改进在线决策Transformer（ODT）的样本效率和平均回报。


<details>
  <summary>Details</summary>
Motivation: 现有的在线决策Transformer（ODT）缺乏对特定动作结果的显式记忆，导致学习长期动作有效性方面效率低下。

Method: 提出EWA-VQ-ODT，该模块通过直接网格查找将连续动作路由到紧凑的向量量化码本，其中每个代码存储一个标量吸引力，该吸引力通过衰减和基于奖励的强化在线更新。这些吸引力通过偏置与动作token相关的列来调节注意力。

Result: 在标准连续控制基准测试中，EWA-VQ-ODT 提高了样本效率和平均回报，尤其是在早期训练中。

Conclusion: EWA-VQ-ODT模块具有计算效率，可以通过每个代码的trace进行解释，并得到理论保证的支持，这些保证限制了吸引力动态及其对注意力漂移的影响。

Abstract: Transformers have emerged as a compelling architecture for sequential
decision-making by modeling trajectories via self-attention. In reinforcement
learning (RL), they enable return-conditioned control without relying on value
function approximation. Decision Transformers (DTs) exploit this by casting RL
as supervised sequence modeling, but they are restricted to offline data and
lack exploration. Online Decision Transformers (ODTs) address this limitation
through entropy-regularized training on on-policy rollouts, offering a stable
alternative to traditional RL methods like Soft Actor-Critic, which depend on
bootstrapped targets and reward shaping. Despite these advantages, ODTs use
standard attention, which lacks explicit memory of action-specific outcomes.
This leads to inefficiencies in learning long-term action effectiveness.
Inspired by cognitive models such as Experience-Weighted Attraction (EWA), we
propose Experience-Weighted Attraction with Vector Quantization for Online
Decision Transformers (EWA-VQ-ODT), a lightweight module that maintains
per-action mental accounts summarizing recent successes and failures.
Continuous actions are routed via direct grid lookup to a compact
vector-quantized codebook, where each code stores a scalar attraction updated
online through decay and reward-based reinforcement. These attractions modulate
attention by biasing the columns associated with action tokens, requiring no
change to the backbone or training objective. On standard continuous-control
benchmarks, EWA-VQ-ODT improves sample efficiency and average return over ODT,
particularly in early training. The module is computationally efficient,
interpretable via per-code traces, and supported by theoretical guarantees that
bound the attraction dynamics and its impact on attention drift.

</details>


### [170] [Policy Gradient Optimzation for Bayesian-Risk MDPs with General Convex Losses](https://arxiv.org/abs/2509.15509)
*Xiaoshuang Wang,Yifan Lin,Enlu Zhou*

Main category: cs.LG

TL;DR: 本文研究了具有一般损失函数和未知参数的马尔可夫决策过程(MDP)，并提出了一种策略梯度优化方法。


<details>
  <summary>Details</summary>
Motivation: 针对具有未知参数的MDP问题，利用贝叶斯方法估计参数，并对损失施加相干风险泛函，以减轻与未知参数相关的不确定性。

Method: 提出了一种策略梯度优化方法，利用相干风险度量的对偶表示，并将包络定理扩展到连续情况。

Result: 证明了该算法的平稳性分析，收敛速度为$O(T^{-1/2}+r^{-1/2})$，并建立了扩展算法的全局收敛性，提供了在每个episode中实现误差界$O(\epsilon)$所需的迭代次数的界限。

Conclusion: 本文研究了具有一般损失函数和未知参数的马尔可夫决策过程，通过贝叶斯方法和策略梯度优化方法，实现了算法的收敛性，并为实际应用提供了理论基础。

Abstract: Motivated by many application problems, we consider Markov decision processes
(MDPs) with a general loss function and unknown parameters. To mitigate the
epistemic uncertainty associated with unknown parameters, we take a Bayesian
approach to estimate the parameters from data and impose a coherent risk
functional (with respect to the Bayesian posterior distribution) on the loss.
Since this formulation usually does not satisfy the interchangeability
principle, it does not admit Bellman equations and cannot be solved by
approaches based on dynamic programming. Therefore, We propose a policy
gradient optimization method, leveraging the dual representation of coherent
risk measures and extending the envelope theorem to continuous cases. We then
show the stationary analysis of the algorithm with a convergence rate of
$O(T^{-1/2}+r^{-1/2})$, where $T$ is the number of policy gradient iterations
and $r$ is the sample size of the gradient estimator. We further extend our
algorithm to an episodic setting, and establish the global convergence of the
extended algorithm and provide bounds on the number of iterations needed to
achieve an error bound $O(\epsilon)$ in each episode.

</details>


### [171] [KoopCast: Trajectory Forecasting via Koopman Operators](https://arxiv.org/abs/2509.15513)
*Jungjin Lee,Jaeuk Shin,Gihwan Kim,Joonho Han,Insoon Yang*

Main category: cs.LG

TL;DR: KoopCast is a lightweight and efficient trajectory forecasting model.


<details>
  <summary>Details</summary>
Motivation: To accurately forecast trajectories in dynamic environments with interpretability and low latency.

Method: A two-stage approach: a probabilistic neural goal estimator predicts targets, and a Koopman operator-based module refines trajectories in a nonlinear feature space.

Result: Achieves competitive accuracy, interpretability, and low-latency deployment on ETH/UCY, Waymo Open Motion Dataset, and nuScenes.

Conclusion: KoopCast delivers high predictive accuracy, mode-level interpretability, and practical efficiency.

Abstract: We present KoopCast, a lightweight yet efficient model for trajectory
forecasting in general dynamic environments. Our approach leverages Koopman
operator theory, which enables a linear representation of nonlinear dynamics by
lifting trajectories into a higher-dimensional space. The framework follows a
two-stage design: first, a probabilistic neural goal estimator predicts
plausible long-term targets, specifying where to go; second, a Koopman
operator-based refinement module incorporates intention and history into a
nonlinear feature space, enabling linear prediction that dictates how to go.
This dual structure not only ensures strong predictive accuracy but also
inherits the favorable properties of linear operators while faithfully
capturing nonlinear dynamics. As a result, our model offers three key
advantages: (i) competitive accuracy, (ii) interpretability grounded in Koopman
spectral theory, and (iii) low-latency deployment. We validate these benefits
on ETH/UCY, the Waymo Open Motion Dataset, and nuScenes, which feature rich
multi-agent interactions and map-constrained nonlinear motion. Across
benchmarks, KoopCast consistently delivers high predictive accuracy together
with mode-level interpretability and practical efficiency.

</details>


### [172] [Manifold Dimension Estimation: An Empirical Study](https://arxiv.org/abs/2509.15517)
*Zelong Bi,Pierre Lafaye de Micheaux*

Main category: cs.LG

TL;DR: 对降维方法进行了全面的调查，包括理论基础、代表性估计器和超参数调整。


<details>
  <summary>Details</summary>
Motivation: 利用高维数据通常位于低维流形上或附近的流形假设，估计该流形的维数至关重要，但现有的维数估计工作分散且缺乏系统评估。

Method: 回顾了经常被忽视的理论基础，并提出了八个代表性的估计器。通过对照实验，分析了噪声、曲率和样本大小等个体因素如何影响性能。我们还在各种合成和真实世界的数据集上比较了估计器，引入了一种针对数据集的超参数调整方法。

Result: 结果提供了实践指导，并表明，对于这种普遍性的问题，更简单的方法通常表现更好。

Conclusion: 对于普遍性的问题，简单的方法通常表现更好

Abstract: The manifold hypothesis suggests that high-dimensional data often lie on or
near a low-dimensional manifold. Estimating the dimension of this manifold is
essential for leveraging its structure, yet existing work on dimension
estimation is fragmented and lacks systematic evaluation. This article provides
a comprehensive survey for both researchers and practitioners. We review
often-overlooked theoretical foundations and present eight representative
estimators. Through controlled experiments, we analyze how individual factors
such as noise, curvature, and sample size affect performance. We also compare
the estimators on diverse synthetic and real-world datasets, introducing a
principled approach to dataset-specific hyperparameter tuning. Our results
offer practical guidance and suggest that, for a problem of this generality,
simpler methods often perform better.

</details>


### [173] [Fully Decentralized Cooperative Multi-Agent Reinforcement Learning is A Context Modeling Problem](https://arxiv.org/abs/2509.15519)
*Chao Li,Bingkun Bao,Yang Gao*

Main category: cs.LG

TL;DR: 本文研究完全去中心化的合作多智能体强化学习，其中每个智能体只能观察状态、局部动作和共享奖励。提出了一个名为 Dynamics-Aware Context (DAC) 的新方法，通过动态感知上下文建模来解决非平稳性和相对过度泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能同时解决非平稳性和相对过度泛化问题，因为它们无法在完全去中心化的环境中对其他智能体的联合策略进行建模。

Method: 将每个智能体本地感知的任务形式化为上下文马尔可夫决策过程，并使用潜在变量对逐步动态分布进行建模，将其称为上下文。引入了基于上下文的价值函数来解决价值函数更新过程中的非平稳性问题，并推导出一个乐观的边际价值来促进合作行动的选择，从而解决相对过度泛化问题。

Result: 在各种合作任务（包括矩阵博弈、捕食者和猎物以及 SMAC）上评估了 DAC，其优于多个基线的性能验证了其有效性。

Conclusion: DAC 是一种有效的方法，可以在完全去中心化的合作多智能体强化学习中解决非平稳性和相对过度泛化问题。

Abstract: This paper studies fully decentralized cooperative multi-agent reinforcement
learning, where each agent solely observes the states, its local actions, and
the shared rewards. The inability to access other agents' actions often leads
to non-stationarity during value function updates and relative
overgeneralization during value function estimation, hindering effective
cooperative policy learning. However, existing works fail to address both
issues simultaneously, due to their inability to model the joint policy of
other agents in a fully decentralized setting. To overcome this limitation, we
propose a novel method named Dynamics-Aware Context (DAC), which formalizes the
task, as locally perceived by each agent, as an Contextual Markov Decision
Process, and further addresses both non-stationarity and relative
overgeneralization through dynamics-aware context modeling. Specifically, DAC
attributes the non-stationary local task dynamics of each agent to switches
between unobserved contexts, each corresponding to a distinct joint policy.
Then, DAC models the step-wise dynamics distribution using latent variables and
refers to them as contexts. For each agent, DAC introduces a context-based
value function to address the non-stationarity issue during value function
update. For value function estimation, an optimistic marginal value is derived
to promote the selection of cooperative actions, thereby addressing the
relative overgeneralization issue. Experimentally, we evaluate DAC on various
cooperative tasks (including matrix game, predator and prey, and SMAC), and its
superior performance against multiple baselines validates its effectiveness.

</details>


### [174] [Universal Learning of Stochastic Dynamics for Exact Belief Propagation using Bernstein Normalizing Flows](https://arxiv.org/abs/2509.15533)
*Peter Amorese,Morteza Lahijanian*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的模型，它既能近似一般的非线性随机动力学，又能支持分析置信传播。


<details>
  <summary>Details</summary>
Motivation: 在随机系统中预测未来状态的分布对于不确定性推理至关重要，但非线性动力学通常使分析置信传播变得棘手，需要近似方法。当系统模型未知且必须从数据中学习时，一个关键问题是：我们能否学习一个既能普遍逼近一般非线性随机动力学，又能支持分析置信传播的模型？

Method: 该方法结合了用于密度估计的归一化流的表达性和Bernstein多项式的分析易处理性。

Result: 经验结果表明，对于置信传播，我们学习的模型优于最先进的数据驱动方法，特别是对于具有非加性非高斯噪声的高度非线性系统。

Conclusion: 本文为一类满足这两个属性的模型建立了理论基础。

Abstract: Predicting the distribution of future states in a stochastic system, known as
belief propagation, is fundamental to reasoning under uncertainty. However,
nonlinear dynamics often make analytical belief propagation intractable,
requiring approximate methods. When the system model is unknown and must be
learned from data, a key question arises: can we learn a model that (i)
universally approximates general nonlinear stochastic dynamics, and (ii)
supports analytical belief propagation? This paper establishes the theoretical
foundations for a class of models that satisfy both properties. The proposed
approach combines the expressiveness of normalizing flows for density
estimation with the analytical tractability of Bernstein polynomials. Empirical
results show the efficacy of our learned model over state-of-the-art
data-driven methods for belief propagation, especially for highly non-linear
systems with non-additive, non-Gaussian noise.

</details>


### [175] [Nonconvex Decentralized Stochastic Bilevel Optimization under Heavy-Tailed Noises](https://arxiv.org/abs/2509.15543)
*Xinwen Zhang,Yihan Zhang,Hongchang Gao*

Main category: cs.LG

TL;DR: 本文提出了一种新的 decentralized stochastic bilevel optimization 算法，用于解决 heavy-tailed noises 下的 nonconvex bilevel optimization 问题。


<details>
  <summary>Details</summary>
Motivation: 现有的 decentralized stochastic optimization 方法通常假设 lower-level loss function 是 strongly convex 的，且 stochastic gradient noise 具有 finite variance，而这些假设在实际的机器学习模型中通常不成立。

Method: 本文提出了一种 normalized stochastic variance-reduced bilevel gradient descent 算法，该算法不依赖于任何 clipping 操作。

Result: 本文通过在 heavy-tailed noises 下对 nonconvex decentralized bilevel optimization 问题的 interdependent gradient sequences 进行创新性约束，从而建立了算法的收敛速度。大量的实验结果证实了该算法在处理 heavy-tailed noises 方面的有效性。

Conclusion: 据我们所知，这是第一个在 heavy-tailed noises 下具有严格理论保证的 decentralized bilevel optimization 算法。

Abstract: Existing decentralized stochastic optimization methods assume the lower-level
loss function is strongly convex and the stochastic gradient noise has finite
variance. These strong assumptions typically are not satisfied in real-world
machine learning models. To address these limitations, we develop a novel
decentralized stochastic bilevel optimization algorithm for the nonconvex
bilevel optimization problem under heavy-tailed noises. Specifically, we
develop a normalized stochastic variance-reduced bilevel gradient descent
algorithm, which does not rely on any clipping operation. Moreover, we
establish its convergence rate by innovatively bounding interdependent gradient
sequences under heavy-tailed noises for nonconvex decentralized bilevel
optimization problems. As far as we know, this is the first decentralized
bilevel optimization algorithm with rigorous theoretical guarantees under
heavy-tailed noises. The extensive experimental results confirm the
effectiveness of our algorithm in handling heavy-tailed noises.

</details>


### [176] [PolyJuice Makes It Real: Black-Box, Universal Red Teaming for Synthetic Image Detectors](https://arxiv.org/abs/2509.15551)
*Sepehr Dehdashtian,Mashrur M. Morshed,Jacob H. Seidman,Gaurav Bharaj,Vishnu Naresh Boddeti*

Main category: cs.LG

TL;DR: PolyJuice是一种黑盒、图像无关的合成图像检测器（SID）对抗方法，通过在T2I潜在空间中观察到的分布偏移来生成攻击，提高欺骗SID的有效性，并能提升SID的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的红队方案需要白盒访问SID，且生成图像特定攻击的在线优化成本高昂，因此需要一种黑盒、图像无关的红队方法。

Method: PolyJuice通过离线方式识别SID正确分类和错误分类样本在T2I潜在空间中的分布偏移方向，并利用该方向引导所有生成的图像朝向SID的失效模式。

Result: PolyJuice引导的T2I模型在欺骗SID方面显著更有效（高达84%），且转向方向可以在较低分辨率下有效估计并转移到较高分辨率，从而降低计算开销。在PolyJuice增强的数据集上调整SID模型，可以显著提高检测器的性能（高达30%）。

Conclusion: PolyJuice是一种有效的黑盒SID对抗方法，可以提高对抗攻击的成功率，并能提升SID的防御能力。

Abstract: Synthetic image detectors (SIDs) are a key defense against the risks posed by
the growing realism of images from text-to-image (T2I) models. Red teaming
improves SID's effectiveness by identifying and exploiting their failure modes
via misclassified synthetic images. However, existing red-teaming solutions (i)
require white-box access to SIDs, which is infeasible for proprietary
state-of-the-art detectors, and (ii) generate image-specific attacks through
expensive online optimization. To address these limitations, we propose
PolyJuice, the first black-box, image-agnostic red-teaming method for SIDs,
based on an observed distribution shift in the T2I latent space between samples
correctly and incorrectly classified by the SID. PolyJuice generates attacks by
(i) identifying the direction of this shift through a lightweight offline
process that only requires black-box access to the SID, and (ii) exploiting
this direction by universally steering all generated images towards the SID's
failure modes. PolyJuice-steered T2I models are significantly more effective at
deceiving SIDs (up to 84%) compared to their unsteered counterparts. We also
show that the steering directions can be estimated efficiently at lower
resolutions and transferred to higher resolutions using simple interpolation,
reducing computational overhead. Finally, tuning SID models on
PolyJuice-augmented datasets notably enhances the performance of the detectors
(up to 30%).

</details>


### [177] [The Multi-Query Paradox in Zeroth-Order Optimization](https://arxiv.org/abs/2509.15552)
*Wei Lin,Qingyu Song,Hong Xu*

Main category: cs.LG

TL;DR: 本文研究了在零阶优化中，如何在固定查询预算下分配查询次数以提高估计精度的问题。


<details>
  <summary>Details</summary>
Motivation: 单查询方法估计方差大，多查询方法存在查询次数与迭代次数的权衡问题，如何分配查询预算是一个未被充分研究的问题。

Method: 分析了简单平均(ZO-Avg)和投影对齐(ZO-Align)两种聚合方法，导出了在强凸、凸、非凸和随机设置下的收敛速度，明确了查询次数对收敛的影响。

Result: 对于ZO-Avg，使用多于一次查询/迭代总是查询低效的，单查询方法是最优的。相反，ZO-Align通常在每次迭代中使用更多查询时表现更好，从而使全子空间估计成为最优方法。

Conclusion: 多查询问题归结为两种经典算法之间的选择，这种选择完全取决于所使用的聚合方法。

Abstract: Zeroth-order (ZO) optimization provides a powerful framework for problems
where explicit gradients are unavailable and have to be approximated using only
queries to function value. The prevalent single-query approach is simple, but
suffers from high estimation variance, motivating a multi-query paradigm to
improves estimation accuracy. This, however, creates a critical trade-off:
under a fixed budget of queries (i.e. cost), queries per iteration and the
total number of optimization iterations are inversely proportional to one
another. How to best allocate this budget is a fundamental, under-explored
question.
  This work systematically resolves this query allocation problem. We analyze
two aggregation methods: the de facto simple averaging (ZO-Avg), and a new
Projection Alignment method (ZO-Align) we derive from local surrogate
minimization. By deriving convergence rates for both methods that make the
dependence on the number of queries explicit across strongly convex, convex,
non-convex, and stochastic settings, we uncover a stark dichotomy: For ZO-Avg,
we prove that using more than one query per iteration is always
query-inefficient, rendering the single-query approach optimal. On the
contrary, ZO-Align generally performs better with more queries per iteration,
resulting in a full-subspace estimation as the optimal approach. Thus, our work
clarifies that the multi-query problem boils down to a choice not about an
intermediate query size, but between two classic algorithms, a choice dictated
entirely by the aggregation method used. These theoretical findings are also
consistently validated by extensive experiments.

</details>


### [178] [Reward Hacking Mitigation using Verifiable Composite Rewards](https://arxiv.org/abs/2509.15557)
*Mirza Farhan Bin Tarek,Rahmatollah Beheshti*

Main category: cs.LG

TL;DR: 提出了一种改进的RLVR方法，以减少在医学问答中推理阶段的奖励利用。


<details>
  <summary>Details</summary>
Motivation: 在医学问答领域，使用RLVR的大语言模型在推理阶段容易出现奖励利用问题，例如不进行推理直接给出答案或使用非标准推理格式。

Method: 引入了一个复合奖励函数，对上述奖励利用行为进行惩罚。

Result: 实验表明，与基线模型相比，使用该奖励模型的RLVR可以产生格式更好的推理，减少奖励利用，并保持良好的准确性。

Conclusion: 该方法在减少奖励利用和提高RLVR模型的可靠性方面迈出了一步。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has recently shown that
large language models (LLMs) can develop their own reasoning without direct
supervision. However, applications in the medical domain, specifically for
question answering, are susceptible to significant reward hacking during the
reasoning phase. Our work addresses two primary forms of this behavior: i)
providing a final answer without preceding reasoning, and ii) employing
non-standard reasoning formats to exploit the reward mechanism. To mitigate
these, we introduce a composite reward function with specific penalties for
these behaviors. Our experiments show that extending RLVR with our proposed
reward model leads to better-formatted reasoning with less reward hacking and
good accuracy compared to the baselines. This approach marks a step toward
reducing reward hacking and enhancing the reliability of models utilizing RLVR.

</details>


### [179] [Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning](https://arxiv.org/abs/2509.15561)
*Om Naphade,Saksham Bansal,Parikshit Pareek*

Main category: cs.LG

TL;DR: 这篇论文提出了一种使用小型LLM进行超参数调整（HPT）的专家块框架，以解决大型模型计算成本高昂的问题。


<details>
  <summary>Details</summary>
Motivation: 大型模型的超参数调整计算成本高昂且不透明，而现有的LLM超参数调整方法依赖于超过1000亿参数的模型。

Method: 该方法的核心是轨迹上下文摘要器（TCS），它将原始训练轨迹转换为结构化上下文，使小型LLM能够分析优化过程。

Result: 使用两个本地运行的LLM（phi4:reasoning14B和qwen2.5-coder:32B）和10次试验预算，该方法在六个不同的任务中实现了平均性能，与GPT-4相比，差距在0.9个百分点以内。

Conclusion: 该研究表明，通过TCS专家块框架，小型LLM可以在超参数调整方面实现与大型模型相媲美的可靠性。

Abstract: Hyper-parameter Tuning (HPT) is a necessary step in machine learning (ML)
pipelines but becomes computationally expensive and opaque with larger models.
Recently, Large Language Models (LLMs) have been explored for HPT, yet most
rely on models exceeding 100 billion parameters. We propose an Expert Block
Framework for HPT using Small LLMs. At its core is the Trajectory Context
Summarizer (TCS), a deterministic block that transforms raw training
trajectories into structured context, enabling small LLMs to analyze
optimization progress with reliability comparable to larger models. Using two
locally-run LLMs (phi4:reasoning14B and qwen2.5-coder:32B) and a 10-trial
budget, our TCS-enabled HPT pipeline achieves average performance within ~0.9
percentage points of GPT-4 across six diverse tasks.

</details>


### [180] [How many classes do we need to see for novel class discovery?](https://arxiv.org/abs/2509.15585)
*Akanksha Sarkar,Been Kim,Jennifer J. Sun*

Main category: cs.LG

TL;DR: 本文研究了机器学习模型如何适应不断变化的现实世界数据中的新类别发现问题，并探讨了影响成功类别发现的因素。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集包含复杂且纠缠的变化因素，使得对类别发现的系统研究变得困难。许多关于为什么以及何时更可能成功发现新类别的基本问题尚未得到解答。

Method: 我们使用带有程序生成修改因子的dSprites数据集，提出了一个简单的受控实验框架。这允许我们研究什么影响了成功的类发现。

Result: 我们的经验结果表明，已知类别的数量的益处达到饱和点，超过该点发现性能趋于平稳。不同设置中收益递减的模式为从业者提供了成本效益分析的见解。

Conclusion: 本文的研究结果为从业者提供了成本效益分析的见解，并为未来在复杂现实世界数据集上进行更严格的类发现研究提供了一个起点。

Abstract: Novel class discovery is essential for ML models to adapt to evolving
real-world data, with applications ranging from scientific discovery to
robotics. However, these datasets contain complex and entangled factors of
variation, making a systematic study of class discovery difficult. As a result,
many fundamental questions are yet to be answered on why and when new class
discoveries are more likely to be successful. To address this, we propose a
simple controlled experimental framework using the dSprites dataset with
procedurally generated modifying factors. This allows us to investigate what
influences successful class discovery. In particular, we study the relationship
between the number of known/unknown classes and discovery performance, as well
as the impact of known class 'coverage' on discovering new classes. Our
empirical results indicate that the benefit of the number of known classes
reaches a saturation point beyond which discovery performance plateaus. The
pattern of diminishing return across different settings provides an insight for
cost-benefit analysis for practitioners and a starting point for more rigorous
future research of class discovery on complex real-world datasets.

</details>


### [181] [Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification](https://arxiv.org/abs/2509.15591)
*Zinan Lin,Enshu Liu,Xuefei Ning,Junyi Zhu,Wenyu Wang,Sergey Yekhanin*

Main category: cs.LG

TL;DR: 这篇论文提出了一个名为Latent Zoning Network (LZN) 的统一框架，旨在解决生成建模、表示学习和分类这三个机器学习核心问题。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习领域中，生成建模、表示学习和分类等问题的解决方案相对独立，缺乏统一的框架。该论文旨在探索是否可以通过一个统一的原则来解决这三个问题，从而简化机器学习流程并促进任务间的协同。

Method: LZN的核心思想是创建一个共享的高斯潜在空间，用于编码所有任务的信息。每种数据类型都配备一个编码器和一个解码器，分别用于将样本映射到不相交的潜在区域，以及将潜在变量映射回数据。不同的机器学习任务通过编码器和解码器的组合来实现。

Result: 实验结果表明，LZN在三个方面表现出色：(1) 增强现有模型（图像生成）：与SoTA Rectified Flow模型结合时，LZN在CIFAR10上的FID从2.76提高到2.59。(2) 独立解决任务（表示学习）：LZN在没有辅助损失函数的情况下实现了无监督表示学习，在ImageNet下游线性分类任务上优于MoCo和SimCLR。(3) 同时解决多个任务（联合生成和分类）：LZN通过图像和标签编码器/解码器，可以同时执行图像生成和分类任务，提高了FID并在CIFAR10上实现了SoTA分类精度。

Conclusion: LZN是一个有前景的统一框架，可以有效地解决生成建模、表示学习和分类等问题，并具有改进现有模型、独立解决任务和同时解决多个任务的能力。

Abstract: Generative modeling, representation learning, and classification are three
core problems in machine learning (ML), yet their state-of-the-art (SoTA)
solutions remain largely disjoint. In this paper, we ask: Can a unified
principle address all three? Such unification could simplify ML pipelines and
foster greater synergy across tasks. We introduce Latent Zoning Network (LZN)
as a step toward this goal. At its core, LZN creates a shared Gaussian latent
space that encodes information across all tasks. Each data type (e.g., images,
text, labels) is equipped with an encoder that maps samples to disjoint latent
zones, and a decoder that maps latents back to data. ML tasks are expressed as
compositions of these encoders and decoders: for example, label-conditional
image generation uses a label encoder and image decoder; image embedding uses
an image encoder; classification uses an image encoder and label decoder. We
demonstrate the promise of LZN in three increasingly complex scenarios: (1) LZN
can enhance existing models (image generation): When combined with the SoTA
Rectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-without
modifying the training objective. (2) LZN can solve tasks independently
(representation learning): LZN can implement unsupervised representation
learning without auxiliary loss functions, outperforming the seminal MoCo and
SimCLR methods by 9.3% and 0.2%, respectively, on downstream linear
classification on ImageNet. (3) LZN can solve multiple tasks simultaneously
(joint generation and classification): With image and label encoders/decoders,
LZN performs both tasks jointly by design, improving FID and achieving SoTA
classification accuracy on CIFAR10. The code and trained models are available
at https://github.com/microsoft/latent-zoning-networks. The project website is
at https://zinanlin.me/blogs/latent_zoning_networks.html.

</details>


### [182] [Personalized Prediction By Learning Halfspace Reference Classes Under Well-Behaved Distribution](https://arxiv.org/abs/2509.15592)
*Jizhou Huang,Brendan Juba*

Main category: cs.LG

TL;DR: 提出了一种个性化预测方案，其中每个查询都学习一个易于解释的预测器。


<details>
  <summary>Details</summary>
Motivation: 在高风险应用（如医疗保健）中，越来越多地部署机器学习模型，这促使人们寻找准确且可解释的预测方法。

Method: 研究了这种预测模型在标签不可知情况下，由“半空间”表示的子群体的PAC可学习性。首先，给出了一种特定于分布的PAC学习算法，用于学习个性化预测的参考类。通过利用参考类学习算法和稀疏线性表示的列表学习器，证明了第一个上界，$O(\\mathrm{opt}^{1/4} )$，用于具有稀疏线性分类器和同质半空间子集的个性化预测。

Result: 证明了第一个上界，$O(\\mathrm{opt}^{1/4} )$，用于具有稀疏线性分类器和同质半空间子集的个性化预测。在各种标准基准数据集上评估了我们的算法。

Conclusion: 研究了个性化预测模型在半空间子群体下的 PAC 可学习性，并给出了相应的算法和理论分析。

Abstract: In machine learning applications, predictive models are trained to serve
future queries across the entire data distribution. Real-world data often
demands excessively complex models to achieve competitive performance, however,
sacrificing interpretability. Hence, the growing deployment of machine learning
models in high-stakes applications, such as healthcare, motivates the search
for methods for accurate and explainable predictions. This work proposes a
Personalized Prediction scheme, where an easy-to-interpret predictor is learned
per query. In particular, we wish to produce a "sparse linear" classifier with
competitive performance specifically on some sub-population that includes the
query point. The goal of this work is to study the PAC-learnability of this
prediction model for sub-populations represented by "halfspaces" in a
label-agnostic setting. We first give a distribution-specific PAC-learning
algorithm for learning reference classes for personalized prediction. By
leveraging both the reference-class learning algorithm and a list learner of
sparse linear representations, we prove the first upper bound,
$O(\mathrm{opt}^{1/4} )$, for personalized prediction with sparse linear
classifiers and homogeneous halfspace subsets. We also evaluate our algorithms
on a variety of standard benchmark data sets.

</details>


### [183] [Efficient Extractive Text Summarization for Online News Articles Using Machine Learning](https://arxiv.org/abs/2509.15614)
*Sajib Biswas,Milon Biswas,Arunima Mandal,Fatema Tabassum Liza,Joy Sarker*

Main category: cs.LG

TL;DR: 本文利用机器学习技术，特别是LSTM网络，来进行新闻文章的提取式文本摘要。


<details>
  <summary>Details</summary>
Motivation: 在线新闻文章的内容管理依赖于有效的摘要来提高可访问性和用户参与度。

Method: 使用包含130万篇文章-摘要对的Cornell Newsroom数据集，利用BERT嵌入将文本数据转换为数值表示。将该任务定义为二元分类问题，并探索了各种模型，包括逻辑回归、前馈神经网络和长短期记忆（LSTM）网络。

Result: LSTM网络在F1得分和ROUGE-1指标上优于Lede-3等基线方法和更简单的模型。

Conclusion: 本研究强调了自动摘要在改进在线新闻平台的内容管理系统方面的潜力，从而更有效地组织内容并增强用户体验。

Abstract: In the age of information overload, content management for online news
articles relies on efficient summarization to enhance accessibility and user
engagement. This article addresses the challenge of extractive text
summarization by employing advanced machine learning techniques to generate
concise and coherent summaries while preserving the original meaning. Using the
Cornell Newsroom dataset, comprising 1.3 million article-summary pairs, we
developed a pipeline leveraging BERT embeddings to transform textual data into
numerical representations. By framing the task as a binary classification
problem, we explored various models, including logistic regression,
feed-forward neural networks, and long short-term memory (LSTM) networks. Our
findings demonstrate that LSTM networks, with their ability to capture
sequential dependencies, outperform baseline methods like Lede-3 and simpler
models in F1 score and ROUGE-1 metrics. This study underscores the potential of
automated summarization in improving content management systems for online news
platforms, enabling more efficient content organization and enhanced user
experiences.

</details>


### [184] [Information Geometry of Variational Bayes](https://arxiv.org/abs/2509.15641)
*Mohammad Emtiyaz Khan*

Main category: cs.LG

TL;DR: 本文强调了信息几何和变分贝叶斯 (VB) 之间的基本联系，并讨论了其对机器学习的影响。


<details>
  <summary>Details</summary>
Motivation: 在某些条件下，VB 解决方案始终需要估计或计算自然梯度。强调信息几何和贝叶斯共同起源，希望促进这两个领域交叉的更多工作。

Method: 使用 Khan and Rue (2023) 的自然梯度下降算法，称为贝叶斯学习规则 (BLR)。

Result: 贝叶斯规则简化为自然梯度的加法；二次替代的概括，用于基于梯度的方法；以及用于大型语言模型的 VB 算法的大规模实现。

Conclusion: 信息几何和贝叶斯之间的联系及其结果并非新内容，但进一步强调了这两个领域的共同起源。

Abstract: We highlight a fundamental connection between information geometry and
variational Bayes (VB) and discuss its consequences for machine learning. Under
certain conditions, a VB solution always requires estimation or computation of
natural gradients. We show several consequences of this fact by using the
natural-gradient descent algorithm of Khan and Rue (2023) called the Bayesian
Learning Rule (BLR). These include (i) a simplification of Bayes' rule as
addition of natural gradients, (ii) a generalization of quadratic surrogates
used in gradient-based methods, and (iii) a large-scale implementation of VB
algorithms for large language models. Neither the connection nor its
consequences are new but we further emphasize the common origins of the two
fields of information geometry and Bayes with a hope to facilitate more work at
the intersection of the two fields.

</details>


### [185] [Toward Efficient Influence Function: Dropout as a Compression Tool](https://arxiv.org/abs/2509.15651)
*Yuchen Zhang,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: 提出了一种利用dropout梯度压缩机制来更有效地计算影响函数的新方法，以评估训练数据对机器学习模型的影响。


<details>
  <summary>Details</summary>
Motivation: 评估训练数据对机器学习模型的影响至关重要，但影响函数的计算成本很高，尤其是在大型模型中。

Method: 利用dropout作为梯度压缩机制来计算影响函数。

Result: 该方法显著降低了计算和内存开销，并且在梯度压缩过程中也是如此。

Conclusion: 该方法保留了数据影响的关键组成部分，并使其能够应用于现代大型模型。

Abstract: Assessing the impact the training data on machine learning models is crucial
for understanding the behavior of the model, enhancing the transparency, and
selecting training data. Influence function provides a theoretical framework
for quantifying the effect of training data points on model's performance given
a specific test data. However, the computational and memory costs of influence
function presents significant challenges, especially for large-scale models,
even when using approximation methods, since the gradients involved in
computation are as large as the model itself. In this work, we introduce a
novel approach that leverages dropout as a gradient compression mechanism to
compute the influence function more efficiently. Our method significantly
reduces computational and memory overhead, not only during the influence
function computation but also in gradient compression process. Through
theoretical analysis and empirical validation, we demonstrate that our method
could preserves critical components of the data influence and enables its
application to modern large-scale models.

</details>


### [186] [Nonconvex Regularization for Feature Selection in Reinforcement Learning](https://arxiv.org/abs/2509.15652)
*Kyohei Suzuki,Konstantinos Slavakis*

Main category: cs.LG

TL;DR: 提出了一种用于强化学习中特征选择的有效批量算法，具有理论收敛保证。


<details>
  <summary>Details</summary>
Motivation: 为了减轻传统正则化方案中固有的估计偏差。

Method: 在经典的最小二乘时间差 (LSTD) 框架内扩展策略评估，方法是制定一个用稀疏诱导的非凸投影 minimax 凹 (PMC) 惩罚正则化的 Bellman 残差目标。

Result: 在基准数据集上的数值实验表明，所提出的方法明显优于最先进的特征选择方法，尤其是在具有许多噪声特征的场景中。

Conclusion: 由于 PMC 惩罚的弱凸性，该公式可以解释为一般非单调包含问题的一个特例。为前向反射后向分裂 (FRBS) 算法建立了新的收敛条件来解决这类问题。

Abstract: This work proposes an efficient batch algorithm for feature selection in
reinforcement learning (RL) with theoretical convergence guarantees. To
mitigate the estimation bias inherent in conventional regularization schemes,
the first contribution extends policy evaluation within the classical
least-squares temporal-difference (LSTD) framework by formulating a
Bellman-residual objective regularized with the sparsity-inducing, nonconvex
projected minimax concave (PMC) penalty. Owing to the weak convexity of the PMC
penalty, this formulation can be interpreted as a special instance of a general
nonmonotone-inclusion problem. The second contribution establishes novel
convergence conditions for the forward-reflected-backward splitting (FRBS)
algorithm to solve this class of problems. Numerical experiments on benchmark
datasets demonstrate that the proposed approach substantially outperforms
state-of-the-art feature-selection methods, particularly in scenarios with many
noisy features.

</details>


### [187] [Inference Offloading for Cost-Sensitive Binary Classification at the Edge](https://arxiv.org/abs/2509.15674)
*Vishnu Narayanan Moothedath,Umang Agarwal,Umeshraja N,James Richard Gross,Jaya Prakash Champati,Sharayu Moharir*

Main category: cs.LG

TL;DR: 本文研究边缘智能系统中的二元分类问题，其中假阴性的代价高于假阳性。该系统具有一个紧凑的本地部署模型和一个较大的远程模型，该模型可以通过网络访问，但会产生卸载成本。为了优化系统，提出了一个在线学习框架，可以不断调整本地模型置信度上的两个阈值。提出了H2T2策略，并证明了它可以实现次线性后悔。


<details>
  <summary>Details</summary>
Motivation: 研究分层推理(HI)系统中分类精度和卸载成本之间的根本权衡。

Method: 提出了一个在线学习框架，可以不断调整本地模型置信度上的两个阈值。提出了H2T2，一种在线双阈值分层推理策略。

Result: 在真实数据集上的仿真表明，H2T2始终优于朴素和单阈值HI策略，有时甚至超过离线最优。该策略还证明了对分布偏移的鲁棒性，并能有效适应不匹配的分类器。

Conclusion: H2T2是模型无关的，不需要训练，并且在使用有限反馈的推理阶段进行学习。

Abstract: We focus on a binary classification problem in an edge intelligence system
where false negatives are more costly than false positives. The system has a
compact, locally deployed model, which is supplemented by a larger, remote
model, which is accessible via the network by incurring an offloading cost. For
each sample, our system first uses the locally deployed model for inference.
Based on the output of the local model, the sample may be offloaded to the
remote model. This work aims to understand the fundamental trade-off between
classification accuracy and these offloading costs within such a hierarchical
inference (HI) system. To optimize this system, we propose an online learning
framework that continuously adapts a pair of thresholds on the local model's
confidence scores. These thresholds determine the prediction of the local model
and whether a sample is classified locally or offloaded to the remote model. We
present a closed-form solution for the setting where the local model is
calibrated. For the more general case of uncalibrated models, we introduce
H2T2, an online two-threshold hierarchical inference policy, and prove it
achieves sublinear regret. H2T2 is model-agnostic, requires no training, and
learns in the inference phase using limited feedback. Simulations on real-world
datasets show that H2T2 consistently outperforms naive and single-threshold HI
policies, sometimes even surpassing offline optima. The policy also
demonstrates robustness to distribution shifts and adapts effectively to
mismatched classifiers.

</details>


### [188] [KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning](https://arxiv.org/abs/2509.15676)
*Vaibhav Singh,Soumya Suvra Ghosal,Kapu Nirmal Joshua,Soumyabrata Pal,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 本文研究了上下文学习(ICL)中的示例选择问题，旨在提高大型语言模型(LLM)在数据稀缺任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的近邻方法在高维嵌入空间中存在泛化能力差和缺乏多样性的问题，因此需要一种更有效的示例选择方法。

Method: 本文从信息论的角度出发，将LLM建模为输入嵌入上的线性函数，并将示例选择任务转化为一个query特定的优化问题。通过推导一个近似次模的替代目标函数，并结合核技巧和基于最优设计的正则化器来鼓励选择多样性。

Result: 在多个分类任务中，本文的方法显著优于标准检索方法。

Conclusion: 本文提出的结构感知、多样化示例选择方法能够有效提高ICL在真实标签稀缺场景下的性能。

Abstract: In-context learning (ICL) has emerged as a powerful paradigm for adapting
large language models (LLMs) to new and data-scarce tasks using only a few
carefully selected task-specific examples presented in the prompt. However,
given the limited context size of LLMs, a fundamental question arises: Which
examples should be selected to maximize performance on a given user query?
While nearest-neighbor-based methods like KATE have been widely adopted for
this purpose, they suffer from well-known drawbacks in high-dimensional
embedding spaces, including poor generalization and a lack of diversity. In
this work, we study this problem of example selection in ICL from a principled,
information theory-driven perspective. We first model an LLM as a linear
function over input embeddings and frame the example selection task as a
query-specific optimization problem: selecting a subset of exemplars from a
larger example bank that minimizes the prediction error on a specific query.
This formulation departs from traditional generalization-focused learning
theoretic approaches by targeting accurate prediction for a specific query
instance. We derive a principled surrogate objective that is approximately
submodular, enabling the use of a greedy algorithm with an approximation
guarantee. We further enhance our method by (i) incorporating the kernel trick
to operate in high-dimensional feature spaces without explicit mappings, and
(ii) introducing an optimal design-based regularizer to encourage diversity in
the selected examples. Empirically, we demonstrate significant improvements
over standard retrieval methods across a suite of classification tasks,
highlighting the benefits of structure-aware, diverse example selection for ICL
in real-world, label-scarce scenarios.

</details>


### [189] [RMT-KD: Random Matrix Theoretic Causal Knowledge Distillation](https://arxiv.org/abs/2509.15724)
*Davide Ettori,Nastaran Darabi,Sureshkumar Senthilkumar,Amit Ranjan Trivedi*

Main category: cs.LG

TL;DR: RMT-KD: A compression method using Random Matrix Theory for knowledge distillation to reduce network size.


<details>
  <summary>Details</summary>
Motivation: Large deep learning models are costly to deploy at the edge due to their size and compute demands.

Method: RMT-KD preserves informative directions identified via spectral properties of hidden representations, applying RMT-based causal reduction layer by layer with self-distillation.

Result: Achieves up to 80% parameter reduction with only 2% accuracy loss, 2.8x faster inference, and nearly halved power consumption on GLUE, AG News, and CIFAR-10.

Conclusion: RMT-KD is a mathematically grounded approach to network distillation.

Abstract: Large deep learning models such as BERT and ResNet achieve state-of-the-art
performance but are costly to deploy at the edge due to their size and compute
demands. We present RMT-KD, a compression method that leverages Random Matrix
Theory (RMT) for knowledge distillation to iteratively reduce network size.
Instead of pruning or heuristic rank selection, RMT-KD preserves only
informative directions identified via the spectral properties of hidden
representations. RMT-based causal reduction is applied layer by layer with
self-distillation to maintain stability and accuracy. On GLUE, AG News, and
CIFAR-10, RMT-KD achieves up to 80% parameter reduction with only 2% accuracy
loss, delivering 2.8x faster inference and nearly halved power consumption.
These results establish RMT-KD as a mathematically grounded approach to network
distillation.

</details>


### [190] [EigenTrack: Spectral Activation Feature Tracking for Hallucination and Out-of-Distribution Detection in LLMs and VLMs](https://arxiv.org/abs/2509.15735)
*Davide Ettori,Nastaran Darabi,Sina Tayebati,Ranganath Krishnan,Mahesh Subedar,Omesh Tickoo,Amit Ranjan Trivedi*

Main category: cs.LG

TL;DR: EigenTrack是一个用于检测大型语言模型幻觉和OOD错误的实时检测器，它使用隐藏激活的谱几何作为模型动态的紧凑全局签名。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然用途广泛，但仍然容易出现幻觉和分布外(OOD)错误。

Method: 该方法通过将协方差谱统计量（如熵、特征值间隙和KL散度）从随机基线输入到轻量级循环分类器中，来跟踪表示结构中的时间变化，这些变化表明表面错误出现之前的幻觉和OOD漂移。

Result: EigenTrack只需要一次正向传递，不需要重采样，并且保留了时间上下文，聚合了全局信号，并提供了可解释的准确性-延迟权衡。

Conclusion: EigenTrack是一种可解释的实时检测器，可用于检测大型语言模型的幻觉和OOD错误。

Abstract: Large language models (LLMs) offer broad utility but remain prone to
hallucination and out-of-distribution (OOD) errors. We propose EigenTrack, an
interpretable real-time detector that uses the spectral geometry of hidden
activations, a compact global signature of model dynamics. By streaming
covariance-spectrum statistics such as entropy, eigenvalue gaps, and KL
divergence from random baselines into a lightweight recurrent classifier,
EigenTrack tracks temporal shifts in representation structure that signal
hallucination and OOD drift before surface errors appear. Unlike black- and
grey-box methods, it needs only a single forward pass without resampling.
Unlike existing white-box detectors, it preserves temporal context, aggregates
global signals, and offers interpretable accuracy-latency trade-offs.

</details>


### [191] [Aircraft Fuel Flow Modelling with Ageing Effects: From Parametric Corrections to Neural Networks](https://arxiv.org/abs/2509.15736)
*Gabriel Jarry,Ramon Dalmau,Philippe Very,Junzi Sun*

Main category: cs.LG

TL;DR: 本文研究了将引擎老化效应整合到空客A320-214燃油流量预测中的多种方法，使用了来自九个不同机身的大约19000个快速访问记录器航班的综合数据集。


<details>
  <summary>Details</summary>
Motivation: 标准的参数模型通常忽略了飞机老化时发生的性能退化。精确的飞机燃油流量建模对于运营规划和环境影响评估至关重要。

Method: 我们系统地评估了经典的基于物理的模型、经验校正系数和数据驱动的神经网络架构，这些架构将年龄作为输入特征或作为显式的乘法偏差。

Result: 结果表明，虽然基线模型始终低估了旧飞机的燃料消耗，但使用依赖于年龄的校正因子和神经模型可以大大减少偏差并提高预测准确性。

Conclusion: 这项研究强调了在参数和机器学习框架中考虑老化影响以提高运营和环境评估可靠性的重要性。该研究还强调需要更多样化的数据集，以捕获真实世界发动机退化的复杂性。

Abstract: Accurate modelling of aircraft fuel-flow is crucial for both operational
planning and environmental impact assessment, yet standard parametric models
often neglect performance deterioration that occurs as aircraft age. This paper
investigates multiple approaches to integrate engine ageing effects into
fuel-flow prediction for the Airbus A320-214, using a comprehensive dataset of
approximately nineteen thousand Quick Access Recorder flights from nine
distinct airframes with varying years in service. We systematically evaluate
classical physics-based models, empirical correction coefficients, and
data-driven neural network architectures that incorporate age either as an
input feature or as an explicit multiplicative bias. Results demonstrate that
while baseline models consistently underestimate fuel consumption for older
aircraft, the use of age-dependent correction factors and neural models
substantially reduces bias and improves prediction accuracy. Nevertheless,
limitations arise from the small number of airframes and the lack of detailed
maintenance event records, which constrain the representativeness and
generalization of age-based corrections. This study emphasizes the importance
of accounting for the effects of ageing in parametric and machine learning
frameworks to improve the reliability of operational and environmental
assessments. The study also highlights the need for more diverse datasets that
can capture the complexity of real-world engine deterioration.

</details>


### [192] [GUI-ReWalk: Massive Data Generation for GUI Agent via Stochastic Exploration and Intent-Aware Reasoning](https://arxiv.org/abs/2509.15738)
*Musen Lin,Minghao Liu,Taoran Lu,Lichen Yuan,Yiwei Liu,Haonan Xu,Yu Miao,Yuhao Chao,Zhaojian Li*

Main category: cs.LG

TL;DR: GUI-ReWalk: A new framework for synthesizing realistic and diverse GUI trajectories by combining stochastic exploration with reasoning-guided interaction.


<details>
  <summary>Details</summary>
Motivation: Existing GUI agent development is limited by the lack of high-quality training data. Current methods rely on expensive manual annotation or sacrifice diversity for task coverage.

Method: A multi-stage framework (GUI-ReWalk) that uses stochastic exploration and reasoning-guided interaction to generate realistic GUI trajectories. It supports multi-stride task generation across multiple applications.

Result: Training Qwen2.5-VL-7B on the GUI-ReWalk dataset shows superior coverage of interaction flows, higher trajectory entropy, and more realistic user intent on multiple benchmarks.

Conclusion: GUI-ReWalk is a scalable and data-efficient framework for advancing GUI agent research and enabling robust real-world automation.

Abstract: Graphical User Interface (GUI) Agents, powered by large language and
vision-language models, hold promise for enabling end-to-end automation in
digital environments. However, their progress is fundamentally constrained by
the scarcity of scalable, high-quality trajectory data. Existing data
collection strategies either rely on costly and inconsistent manual annotations
or on synthetic generation methods that trade off between diversity and
meaningful task coverage. To bridge this gap, we present GUI-ReWalk: a
reasoning-enhanced, multi-stage framework for synthesizing realistic and
diverse GUI trajectories. GUI-ReWalk begins with a stochastic exploration phase
that emulates human trial-and-error behaviors, and progressively transitions
into a reasoning-guided phase where inferred goals drive coherent and
purposeful interactions. Moreover, it supports multi-stride task generation,
enabling the construction of long-horizon workflows across multiple
applications. By combining randomness for diversity with goal-aware reasoning
for structure, GUI-ReWalk produces data that better reflects the intent-aware,
adaptive nature of human-computer interaction. We further train Qwen2.5-VL-7B
on the GUI-ReWalk dataset and evaluate it across multiple benchmarks, including
Screenspot-Pro, OSWorld-G, UI-Vision, AndroidControl, and GUI-Odyssey. Results
demonstrate that GUI-ReWalk enables superior coverage of diverse interaction
flows, higher trajectory entropy, and more realistic user intent. These
findings establish GUI-ReWalk as a scalable and data-efficient framework for
advancing GUI agent research and enabling robust real-world automation.

</details>


### [193] [Incremental Multistep Forecasting of Battery Degradation Using Pseudo Targets](https://arxiv.org/abs/2509.15740)
*Jonathan Adam Rico,Nagarajan Raghavan,Senthilnath Jayavelu*

Main category: cs.LG

TL;DR: 本文提出了一种名为 iFSNet 的在线机器学习方法，用于电池早期预测，以解决现有模型无法适应变化分布和难以进行在线增量多步预测的问题。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型在离线模式下工作，每次遇到新的数据分布都需要重新训练，无法适应变化的环境。现有的在线增量多步预测方法存在无法校正模型预测和需要大量流数据才能重新训练的问题。

Method: 本文提出 iFSNet，它是 FSNet 的改进版本，用于单次模式（逐个样本）以实现使用伪目标的多步预测。它使用输入序列的简单线性回归器来推断伪未来样本（伪目标），并计算其余预测的损失并保持更新模型。

Result: 所提出的模型在具有平滑退化轨迹的数据集上实现了 0.00197 RMSE 和 0.00154 MAE，而在具有容量再生尖峰的不规则退化轨迹的数据集上实现了 0.01588 RMSE 和 0.01234 MAE。

Conclusion: iFSNet 模型受益于 FSNet 的联想记忆和自适应结构机制，同时模型通过使用伪目标逐步改进。

Abstract: Data-driven models accurately perform early battery prognosis to prevent
equipment failure and further safety hazards. Most existing machine learning
(ML) models work in offline mode which must consider their retraining
post-deployment every time new data distribution is encountered. Hence, there
is a need for an online ML approach where the model can adapt to varying
distributions. However, existing online incremental multistep forecasts are a
great challenge as there is no way to correct the model of its forecasts at the
current instance. Also, these methods need to wait for a considerable amount of
time to acquire enough streaming data before retraining. In this study, we
propose iFSNet (incremental Fast and Slow learning Network) which is a modified
version of FSNet for a single-pass mode (sample-by-sample) to achieve multistep
forecasting using pseudo targets. It uses a simple linear regressor of the
input sequence to extrapolate pseudo future samples (pseudo targets) and
calculate the loss from the rest of the forecast and keep updating the model.
The model benefits from the associative memory and adaptive structure
mechanisms of FSNet, at the same time the model incrementally improves by using
pseudo targets. The proposed model achieved 0.00197 RMSE and 0.00154 MAE on
datasets with smooth degradation trajectories while it achieved 0.01588 RMSE
and 0.01234 MAE on datasets having irregular degradation trajectories with
capacity regeneration spikes.

</details>


### [194] [On Optimal Steering to Achieve Exact Fairness](https://arxiv.org/abs/2509.15759)
*Mohit Sharma,Amit Jayant Deshpande,Chiranjib Bhattacharyya,Rajiv Ratn Shah*

Main category: cs.LG

TL;DR: 为了解决公平机器学习中的“偏差输入，偏差输出”问题，将数据或大型语言模型 (LLM) 的内部表示的特征分布引导到保证群体公平结果的理想分布非常重要。我们通过在 KL 散度中找到最近的理想分布来制定用于最佳控制的优化程序，并在底层分布来自众所周知的参数族（例如，正态、对数正态）时，为其提供高效的算法。


<details>
  <summary>Details</summary>
Motivation: 先前关于公平生成模型和表征控制的工作可以极大地受益于模型输出上可证明的公平性保证。我们将分布定义为理想的，如果其上任何成本敏感风险的最小化器保证具有精确的群体公平结果（例如，人口统计均等、机会均等）——换句话说，它没有公平性-效用权衡。

Method: 我们通过在 KL 散度中找到最近的理想分布来制定用于最佳控制的优化程序，并在底层分布来自众所周知的参数族（例如，正态、对数正态）时，为其提供高效的算法。

Result: 在合成和真实世界数据集上的实验表明，我们的最佳控制技术提高了公平性，而没有降低效用（有时甚至提高了效用）。我们展示了 LLM 表征的仿射控制，以减少多类分类中的偏差，例如，来自 Bios 数据集中简短传记的职业预测（De-Arteaga 等人）。

Conclusion: 我们将 LLM 的内部表征引导到期望的输出，使其在不同的群体中同样有效。

Abstract: To fix the 'bias in, bias out' problem in fair machine learning, it is
important to steer feature distributions of data or internal representations of
Large Language Models (LLMs) to ideal ones that guarantee group-fair outcomes.
Previous work on fair generative models and representation steering could
greatly benefit from provable fairness guarantees on the model output. We
define a distribution as ideal if the minimizer of any cost-sensitive risk on
it is guaranteed to have exact group-fair outcomes (e.g., demographic parity,
equal opportunity)-in other words, it has no fairness-utility trade-off. We
formulate an optimization program for optimal steering by finding the nearest
ideal distribution in KL-divergence, and provide efficient algorithms for it
when the underlying distributions come from well-known parametric families
(e.g., normal, log-normal). Empirically, our optimal steering techniques on
both synthetic and real-world datasets improve fairness without diminishing
utility (and sometimes even improve utility). We demonstrate affine steering of
LLM representations to reduce bias in multi-class classification, e.g.,
occupation prediction from a short biography in Bios dataset (De-Arteaga et
al.). Furthermore, we steer internal representations of LLMs towards desired
outputs so that it works equally well across different groups.

</details>


### [195] [Learning to Optimize Capacity Planning in Semiconductor Manufacturing](https://arxiv.org/abs/2509.15767)
*Philipp Andelfinger,Jieyi Bi,Qiuyu Zhu,Jianan Zhou,Bo Zhang,Fei Fei Zhang,Chew Wye Chan,Boon Ping Gan,Wentong Cai,Jie Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种基于神经网络的 capacity planning 模型，该模型使用深度强化学习进行训练，可以直接捕获机器和处理步骤之间的各种关系，从而实现前瞻性决策。


<details>
  <summary>Details</summary>
Motivation: 当前半导体制造业的行业实践通常应用启发式规则来确定行动的优先级，例如考虑传入的机器和配方分配的未来变更列表。然而，虽然提供了可解释性，但启发式方法无法轻易解释沿过程流的复杂交互，这些交互会逐渐导致瓶颈的形成。

Method: 使用异构图神经网络表示策略，该模型直接捕获机器和处理步骤之间的各种关系。

Result: 在最大的测试场景中，训练后的策略将吞吐量和周期时间分别提高了约 1.8%。

Conclusion: 本文提出了一种基于神经网络的 capacity planning 模型，该模型使用深度强化学习进行训练，可以直接捕获机器和处理步骤之间的各种关系，从而实现前瞻性决策，并在 Intel 的小型 Minifab 模型和使用流行的 SMT2020 测试平台的初步实验中取得了良好的效果。

Abstract: In manufacturing, capacity planning is the process of allocating production
resources in accordance with variable demand. The current industry practice in
semiconductor manufacturing typically applies heuristic rules to prioritize
actions, such as future change lists that account for incoming machine and
recipe dedications. However, while offering interpretability, heuristics cannot
easily account for the complex interactions along the process flow that can
gradually lead to the formation of bottlenecks. Here, we present a neural
network-based model for capacity planning on the level of individual machines,
trained using deep reinforcement learning. By representing the policy using a
heterogeneous graph neural network, the model directly captures the diverse
relationships among machines and processing steps, allowing for proactive
decision-making. We describe several measures taken to achieve sufficient
scalability to tackle the vast space of possible machine-level actions.
  Our evaluation results cover Intel's small-scale Minifab model and
preliminary experiments using the popular SMT2020 testbed. In the largest
tested scenario, our trained policy increases throughput and decreases cycle
time by about 1.8% each.

</details>


### [196] [Generalization and Optimization of SGD with Lookahead](https://arxiv.org/abs/2509.15776)
*Kangcheng Li,Yunwen Lei*

Main category: cs.LG

TL;DR: 本研究对 Lookahead 优化器进行了泛化分析，解决了现有研究中存在的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有理论研究主要集中在 Lookahead 优化器在训练数据上的收敛性，对其泛化能力了解不足。现有的泛化分析受到严格的假设限制，例如要求损失函数是全局 Lipschitz 连续的，并且它们的界限不能完全捕捉优化和泛化之间的关系。

Method: 利用平均模型稳定性，推导出凸和强凸问题的泛化界限，而没有限制性的 Lipschitz 假设。对 Lookahead 优化器与 minibatch SGD 进行了严格的稳定性及泛化分析。

Result: 在凸设置中，分析表明相对于批量大小的线性加速。

Conclusion: 该研究解决了现有 Lookahead 优化器泛化分析的局限性，并为凸和强凸问题提供了更强的泛化保证。

Abstract: The Lookahead optimizer enhances deep learning models by employing a
dual-weight update mechanism, which has been shown to improve the performance
of underlying optimizers such as SGD. However, most theoretical studies focus
on its convergence on training data, leaving its generalization capabilities
less understood. Existing generalization analyses are often limited by
restrictive assumptions, such as requiring the loss function to be globally
Lipschitz continuous, and their bounds do not fully capture the relationship
between optimization and generalization. In this paper, we address these issues
by conducting a rigorous stability and generalization analysis of the Lookahead
optimizer with minibatch SGD. We leverage on-average model stability to derive
generalization bounds for both convex and strongly convex problems without the
restrictive Lipschitzness assumption. Our analysis demonstrates a linear
speedup with respect to the batch size in the convex setting.

</details>


### [197] [Monte Carlo Tree Diffusion with Multiple Experts for Protein Design](https://arxiv.org/abs/2509.15796)
*Xuefeng Liu,Mingxuan Cao,Songhao Jiang,Xiao Luo,Xiaotian Duan,Mengdi Wang,Tobin R. Sosnick,Jinbo Xu,Rick Stevens*

Main category: cs.LG

TL;DR: 提出了MCTD-ME，一种结合了掩蔽扩散模型与树搜索的蛋白质设计方法，以实现多标记规划和高效探索。


<details>
  <summary>Details</summary>
Motivation: 现有方法在结合自回归语言模型与蒙特卡洛树搜索（MCTS）时，在处理远程依赖关系方面存在困难，并且搜索空间过大。

Method: MCTD-ME使用生物物理保真度增强的扩散去噪作为rollout引擎，联合修改多个位置，并扩展到大型序列空间。它还利用不同容量的专家来丰富探索，由基于pLDDT的掩蔽策略引导，该策略针对低置信度区域，同时保留可靠的残基。

Result: 在反向折叠任务（CAMEO和PDB基准）中，MCTD-ME在序列恢复（AAR）和结构相似性（scTM）方面均优于单专家和无引导基线，对于更长的蛋白质，增益增加，并受益于多专家指导。

Conclusion: 该框架是模型无关的，适用于反向折叠以外的领域，包括从头蛋白质工程和多目标分子生成。

Abstract: The goal of protein design is to generate amino acid sequences that fold into
functional structures with desired properties. Prior methods combining
autoregressive language models with Monte Carlo Tree Search (MCTS) struggle
with long-range dependencies and suffer from an impractically large search
space. We propose MCTD-ME, Monte Carlo Tree Diffusion with Multiple Experts,
which integrates masked diffusion models with tree search to enable multi-token
planning and efficient exploration. Unlike autoregressive planners, MCTD-ME
uses biophysical-fidelity-enhanced diffusion denoising as the rollout engine,
jointly revising multiple positions and scaling to large sequence spaces. It
further leverages experts of varying capacities to enrich exploration, guided
by a pLDDT-based masking schedule that targets low-confidence regions while
preserving reliable residues. We propose a novel multi-expert selection rule
(PH-UCT-ME) extends predictive-entropy UCT to expert ensembles. On the inverse
folding task (CAMEO and PDB benchmarks), MCTD-ME outperforms single-expert and
unguided baselines in both sequence recovery (AAR) and structural similarity
(scTM), with gains increasing for longer proteins and benefiting from
multi-expert guidance. More generally, the framework is model-agnostic and
applicable beyond inverse folding, including de novo protein engineering and
multi-objective molecular generation.

</details>
