<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 41]
- [cs.CV](#cs.CV) [Total: 40]
- [cs.AI](#cs.AI) [Total: 41]
- [cs.DB](#cs.DB) [Total: 10]
- [cs.IR](#cs.IR) [Total: 24]
- [cs.LG](#cs.LG) [Total: 41]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Outcome-Based Education: Evaluating Students' Perspectives Using Transformer](https://arxiv.org/abs/2506.17223)
*Shuvra Smaran Das,Anirban Saha Anik,Md Kishor Morol,Mohammad Sakib Mahmood*

Main category: cs.CL

TL;DR: 本研究使用 DistilBERT 和 LIME 分析学生反馈，以改进基于结果的教育 (OBE)。


<details>
  <summary>Details</summary>
Motivation: 基于结果的教育 (OBE) 强调通过以学生为中心的学习来发展特定的能力。本研究回顾了 OBE 的重要性，我们的目标是评估和提高教育成果。

Method: 实施基于 transformer 的模型，特别是 DistilBERT，以分析包含学生反馈的 NLP 数据集，并应用 LIME (local interpretable model-agnostic explanations) 来确保模型预测清晰。

Result: 结果表明，该方法优于其他机器学习模型，因为它利用 transformer 对语言上下文的深刻理解来更好地分类情感，从而在更广泛的矩阵中给出更好的结果。我们的工作通过促进识别学生学习经历中的模式，直接有助于 OBE 实现可衡量的成果的目标。

Conclusion: transformer 模型和 LIME 解释的结合为分析学生反馈提供了一个强大而直接的框架。这与 OBE 的原则更加紧密地结合，并通过数据驱动的见解确保改进教育实践。

Abstract: Outcome-Based Education (OBE) emphasizes the development of specific
competencies through student-centered learning. In this study, we reviewed the
importance of OBE and implemented transformer-based models, particularly
DistilBERT, to analyze an NLP dataset that includes student feedback. Our
objective is to assess and improve educational outcomes. Our approach is better
than other machine learning models because it uses the transformer's deep
understanding of language context to classify sentiment better, giving better
results across a wider range of matrices. Our work directly contributes to
OBE's goal of achieving measurable outcomes by facilitating the identification
of patterns in student learning experiences. We have also applied LIME (local
interpretable model-agnostic explanations) to make sure that model predictions
are clear. This gives us understandable information about how key terms affect
sentiment. Our findings indicate that the combination of transformer models and
LIME explanations results in a strong and straightforward framework for
analyzing student feedback. This aligns more closely with the principles of OBE
and ensures the improvement of educational practices through data-driven
insights.

</details>


### [2] [Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs](https://arxiv.org/abs/2506.17231)
*Xiang Li,Chong Zhang,Jia Wang,Fangyu Wu,Yushi Li,Xiaobo Jin*

Main category: cs.CL

TL;DR: Distilling jailbreak attacks from LLMs to SLMs using Adversarial Prompt Distillation.


<details>
  <summary>Details</summary>
Motivation: Current jailbreak attack methods face problems such as low efficiency, high computational cost, and poor cross-model adaptability and versatility, which make it difficult to cope with the rapid development of LLM and new defense strategies.

Method: Adversarial Prompt Distillation, which combines masked language modeling, reinforcement learning, and dynamic temperature control through a prompt generation and distillation method. It enables small language models (SLMs) to jailbreak attacks on mainstream LLMs.

Result: The experimental results verify the superiority of the proposed method in terms of attack success rate and harm, and reflect the resource efficiency and cross-model adaptability.

Conclusion: This research explores the feasibility of distilling the jailbreak ability of LLM to SLM, reveals the model's vulnerability, and provides a new idea for LLM security research.

Abstract: Attacks on large language models (LLMs) in jailbreaking scenarios raise many
security and ethical issues. Current jailbreak attack methods face problems
such as low efficiency, high computational cost, and poor cross-model
adaptability and versatility, which make it difficult to cope with the rapid
development of LLM and new defense strategies. Our work proposes an Adversarial
Prompt Distillation, which combines masked language modeling, reinforcement
learning, and dynamic temperature control through a prompt generation and
distillation method. It enables small language models (SLMs) to jailbreak
attacks on mainstream LLMs. The experimental results verify the superiority of
the proposed method in terms of attack success rate and harm, and reflect the
resource efficiency and cross-model adaptability. This research explores the
feasibility of distilling the jailbreak ability of LLM to SLM, reveals the
model's vulnerability, and provides a new idea for LLM security research.

</details>


### [3] [GTA: Grouped-head latenT Attention](https://arxiv.org/abs/2506.17286)
*Luoyang Sun,Jiwen Jiang,Cheng Deng,Xinjian Wu,Haifeng Zhang,Lei Chen,Lionel Ni,Jun Wang*

Main category: cs.CL

TL;DR: 提出了GTA注意力机制，减少内存和计算开销，提高LLM部署效率。


<details>
  <summary>Details</summary>
Motivation: 注意力机制是大型语言模型（LLM）成功的关键，但其巨大的计算和内存开销对优化效率和性能提出了挑战。KV缓存和注意力计算随着文本长度的快速扩展，对计算和内存资源有限的硬件部署提出了挑战。

Method: 提出了分组头潜在注意力（GTA），一种新颖的注意力机制，通过共享注意力图和非线性值解码器来减少内存使用和计算复杂度。

Result: GTA将注意力计算的FLOPs降低了高达62.5%，并将KV缓存缩小了高达70%，同时避免了多头潜在注意力的额外开销，从而提高了LLM的部署效率。

Conclusion: GTA模型实现了2倍的端到端推理速度提升，预填充受益于计算成本的降低，解码受益于更小的缓存占用空间。

Abstract: Attention mechanisms underpin the success of large language models (LLMs),
yet their substantial computational and memory overhead poses challenges for
optimizing efficiency and performance. A critical bottleneck arises as KV cache
and attention computations scale rapidly with text length, challenging
deployment on hardware with limited computational and memory resources. We
observe that attention mechanisms exhibit substantial redundancy, since the KV
cache can be significantly compressed and attention maps across heads display
high similarity, revealing that much of the computation and storage is
unnecessary. Leveraging these insights, we propose \textbf{G}rouped-Head
Laten\textbf{T} \textbf{A}ttention (GTA), a novel attention mechanism that
reduces memory usage and computational complexity while maintaining
performance. GTA comprises two components: (1) a shared attention map mechanism
that reuses attention scores across multiple heads, decreasing the key cache
size; and (2) a nonlinear value decoder with learned projections that
compresses the value cache into a latent space, further cutting memory needs.
GTA cuts attention computation FLOPs by up to \emph{62.5\%} versus
Grouped-Query Attention and shrink the KV cache by up to \emph{70\%}, all while
avoiding the extra overhead of Multi-Head Latent Attention to improve LLM
deployment efficiency. Consequently, GTA models achieve a \emph{2x} increase in
end-to-end inference speed, with prefill benefiting from reduced computational
cost and decoding benefiting from the smaller cache footprint.

</details>


### [4] [AI-Generated Game Commentary: A Survey and a Datasheet Repository](https://arxiv.org/abs/2506.17294)
*Qirui Zheng,Xingbo Wang,Keyuan Cheng,Yunlong Lu,Wenxin Li*

Main category: cs.CL

TL;DR: introduce a general framework for AIGGC and present a comprehensive survey of 45 existing game commentary dataset and methods, also provide a structured datasheet summarizing the essential attributes of these datasets


<details>
  <summary>Details</summary>
Motivation: AI-Generated Game Commentary (AIGGC) has gained increasing attention due to its market potential and inherent technical challenges. As a comprehensive multimodal Natural Language Processing (NLP) task, AIGGC imposes substantial demands on language models, including factual accuracy, logical reasoning, expressive text generation, generation speed, and context management

Method: introduce a general framework for AIGGC and present a comprehensive survey of 45 existing game commentary dataset and methods

Result: classify and compare various evaluation metrics commonly used in this domain

Conclusion: To support future research and benchmarking, we also provide a structured datasheet summarizing the essential attributes of these datasets in appendix, which is meanwhile publicly available in an open repository.

Abstract: AI-Generated Game Commentary (AIGGC) has gained increasing attention due to
its market potential and inherent technical challenges. As a comprehensive
multimodal Natural Language Processing (NLP) task, AIGGC imposes substantial
demands on language models, including factual accuracy, logical reasoning,
expressive text generation, generation speed, and context management. In this
paper, we introduce a general framework for AIGGC and present a comprehensive
survey of 45 existing game commentary dataset and methods according to key
challenges they aim to address in this domain. We further classify and compare
various evaluation metrics commonly used in this domain. To support future
research and benchmarking, we also provide a structured datasheet summarizing
the essential attributes of these datasets in appendix, which is meanwhile
publicly available in an open repository.

</details>


### [5] [Semantic uncertainty in advanced decoding methods for LLM generation](https://arxiv.org/abs/2506.17296)
*Darius Foodeei,Simin Fan,Martin Jaggi*

Main category: cs.CL

TL;DR: This paper analyzes how different decoding strategies affect LLM output diversity and reliability, finding that structured methods like CoT and speculative sampling can improve both.


<details>
  <summary>Details</summary>
Motivation: Investigates semantic uncertainty in large language model (LLM) outputs across different decoding methods, focusing on emerging techniques like speculative sampling and chain-of-thought (CoT) decoding.

Method: Experiments on question answering, summarization, and code generation tasks to analyze how different decoding strategies affect both the diversity and reliability of model outputs.

Result: CoT decoding demonstrates higher semantic diversity and lower predictive entropy, with a 48.8% improvement in code generation. Speculative sampling proved particularly effective for summarization tasks, achieving superior ROUGE scores while maintaining moderate semantic diversity.

Conclusion: Properly structured decoding methods can increase semantic exploration while maintaining or improving output quality.

Abstract: This study investigates semantic uncertainty in large language model (LLM)
outputs across different decoding methods, focusing on emerging techniques like
speculative sampling and chain-of-thought (CoT) decoding. Through experiments
on question answering, summarization, and code generation tasks, we analyze how
different decoding strategies affect both the diversity and reliability of
model outputs. Our findings reveal that while CoT decoding demonstrates higher
semantic diversity, it maintains lower predictive entropy, suggesting that
structured exploration can lead to more confident and accurate outputs. This is
evidenced by a 48.8% improvement in code generation Pass@2 rates, despite lower
alignment with reference solutions. For summarization tasks, speculative
sampling proved particularly effective, achieving superior ROUGE scores while
maintaining moderate semantic diversity. Our results challenge conventional
assumptions about trade-offs between diversity and accuracy in language model
outputs, demonstrating that properly structured decoding methods can increase
semantic exploration while maintaining or improving output quality. These
findings have significant implications for deploying language models in
practical applications where both reliability and diverse solution generation
are crucial.

</details>


### [6] [Mercury: Ultra-Fast Language Models Based on Diffusion](https://arxiv.org/abs/2506.17298)
*Inception Labs,Samar Khanna,Siddhant Kharbanda,Shufan Li,Harshit Varma,Eric Wang,Sawyer Birnbaum,Ziyang Luo,Yanis Miraoui,Akash Palrecha,Stefano Ermon,Aditya Grover,Volodymyr Kuleshov*

Main category: cs.CL

TL;DR: Mercury Coder, diffusion LLMs for coding, achieves state-of-the-art speed and quality, outperforming existing models and ranking highly on Copilot Arena. Public API and playground are available.


<details>
  <summary>Details</summary>
Motivation: Introducing Mercury Coder, a new generation of commercial-scale large language models (LLMs) based on diffusion, designed for coding applications.

Method: The models are diffusion LLMs based on the Transformer architecture and trained to predict multiple tokens in parallel.

Result: Mercury Coder Mini achieves 1109 tokens/sec and Mercury Coder Small achieves 737 tokens/sec on NVIDIA H100 GPUs, outperforming speed-optimized frontier models by up to 10x on average while maintaining comparable quality.

Conclusion: Mercury Coder Mini and Mercury Coder Small are state-of-the-art regarding speed and quality, achieving high throughputs on NVIDIA H100 GPUs and outperforming other models while maintaining quality. The model ranks second in quality and is the fastest overall on Copilot Arena. A public API and free playground are available.

Abstract: We present Mercury, a new generation of commercial-scale large language
models (LLMs) based on diffusion. These models are parameterized via the
Transformer architecture and trained to predict multiple tokens in parallel. In
this report, we detail Mercury Coder, our first set of diffusion LLMs designed
for coding applications. Currently, Mercury Coder comes in two sizes: Mini and
Small. These models set a new state-of-the-art on the speed-quality frontier.
Based on independent evaluations conducted by Artificial Analysis, Mercury
Coder Mini and Mercury Coder Small achieve state-of-the-art throughputs of 1109
tokens/sec and 737 tokens/sec, respectively, on NVIDIA H100 GPUs and outperform
speed-optimized frontier models by up to 10x on average while maintaining
comparable quality. We discuss additional results on a variety of code
benchmarks spanning multiple languages and use-cases as well as real-world
validation by developers on Copilot Arena, where the model currently ranks
second on quality and is the fastest model overall. We also release a public
API at https://platform.inceptionlabs.ai/ and free playground at
https://chat.inceptionlabs.ai

</details>


### [7] [PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights](https://arxiv.org/abs/2506.17314)
*Adnan Qidwai,Srija Mukhopadhyay,Prerana Khatiwada,Dan Roth,Vivek Gupta*

Main category: cs.CL

TL;DR: PRAISE uses LLMs to improve product descriptions by structuring customer review insights, helping sellers and buyers.


<details>
  <summary>Details</summary>
Motivation: Seller-provided product information is often incomplete, and customer reviews are difficult to sift through manually.

Method: PRAISE uses LLMs to extract, compare, and structure insights from customer reviews and seller descriptions.

Result: PRAISE effectively generates actionable structured insights from unstructured reviews, identifies discrepancies between reviews and descriptions, and presents them in a clear format.

Conclusion: PRAISE improves e-commerce product catalogs by enhancing clarity and trustworthiness.

Abstract: Accurate and complete product descriptions are crucial for e-commerce, yet
seller-provided information often falls short. Customer reviews offer valuable
details but are laborious to sift through manually. We present PRAISE: Product
Review Attribute Insight Structuring Engine, a novel system that uses Large
Language Models (LLMs) to automatically extract, compare, and structure
insights from customer reviews and seller descriptions. PRAISE provides users
with an intuitive interface to identify missing, contradictory, or partially
matching details between these two sources, presenting the discrepancies in a
clear, structured format alongside supporting evidence from reviews. This
allows sellers to easily enhance their product listings for clarity and
persuasiveness, and buyers to better assess product reliability. Our
demonstration showcases PRAISE's workflow, its effectiveness in generating
actionable structured insights from unstructured reviews, and its potential to
significantly improve the quality and trustworthiness of e-commerce product
catalogs.

</details>


### [8] [Towards Safety Evaluations of Theory of Mind in Large Language Models](https://arxiv.org/abs/2506.17352)
*Tatsuhiro Aoshima,Mitsuaki Akiyama*

Main category: cs.CL

TL;DR: This study investigates LLMs' theory of mind capabilities for safety evaluation, finding that while reading comprehension has improved, theory of mind has not kept pace.


<details>
  <summary>Details</summary>
Motivation: evaluate the potential risk of deceptive actions toward developers or users

Method: analyze developmental trends across a series of open-weight LLMs

Result: LLMs have improved in reading comprehension, their theory of mind capabilities have not shown comparable development

Conclusion: LLMs' theory of mind capabilities have not shown comparable development with reading comprehension.

Abstract: As the capabilities of large language models (LLMs) continue to advance, the
importance of rigorous safety evaluation is becoming increasingly evident.
Recent concerns within the realm of safety assessment have highlighted
instances in which LLMs exhibit behaviors that appear to disable oversight
mechanisms and respond in a deceptive manner. For example, there have been
reports suggesting that, when confronted with information unfavorable to their
own persistence during task execution, LLMs may act covertly and even provide
false answers to questions intended to verify their behavior.To evaluate the
potential risk of such deceptive actions toward developers or users, it is
essential to investigate whether these behaviors stem from covert, intentional
processes within the model. In this study, we propose that it is necessary to
measure the theory of mind capabilities of LLMs. We begin by reviewing existing
research on theory of mind and identifying the perspectives and tasks relevant
to its application in safety evaluation. Given that theory of mind has been
predominantly studied within the context of developmental psychology, we
analyze developmental trends across a series of open-weight LLMs. Our results
indicate that while LLMs have improved in reading comprehension, their theory
of mind capabilities have not shown comparable development. Finally, we present
the current state of safety evaluation with respect to LLMs' theory of mind,
and discuss remaining challenges for future work.

</details>


### [9] [Cash or Comfort? How LLMs Value Your Inconvenience](https://arxiv.org/abs/2506.17367)
*Mateusz Cedro,Timour Ichmoukhamedov,Sofie Goethals,Yifan He,James Hinns,David Martens*

Main category: cs.CL

TL;DR: LLMs在金钱换舒适度的决策中表现不稳定，不适合做决策助手。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地被认为是近乎自主的人工智能代理，能够代表人类做出日常决策。然而，人工智能助手在财务奖励与用户舒适度相悖的情况下的行为尚未得到充分探索。

Method: 量化多个LLM对一系列用户不适（步行、等待、饥饿和疼痛）的定价。

Result: LLMs的响应差异大，对prompt措辞的细微变化敏感，对重大不便接受不合理低的报酬，并可能拒绝没有不适感的金钱收益。

Conclusion: LLMs在涉及金钱与舒适度权衡的决策中表现出不一致性，对人类不便的估值存在问题，不适合作为决策助手。

Abstract: Large Language Models (LLMs) are increasingly proposed as near-autonomous
artificial intelligence (AI) agents capable of making everyday decisions on
behalf of humans. Although LLMs perform well on many technical tasks, their
behaviour in personal decision-making remains less understood. Previous studies
have assessed their rationality and moral alignment with human decisions.
However, the behaviour of AI assistants in scenarios where financial rewards
are at odds with user comfort has not yet been thoroughly explored. In this
paper, we tackle this problem by quantifying the prices assigned by multiple
LLMs to a series of user discomforts: additional walking, waiting, hunger and
pain. We uncover several key concerns that strongly question the prospect of
using current LLMs as decision-making assistants: (1) a large variance in
responses between LLMs, (2) within a single LLM, responses show fragility to
minor variations in prompt phrasing (e.g., reformulating the question in the
first person can considerably alter the decision), (3) LLMs can accept
unreasonably low rewards for major inconveniences (e.g., 1 Euro to wait 10
hours), and (4) LLMs can reject monetary gains where no discomfort is imposed
(e.g., 1,000 Euro to wait 0 minutes). These findings emphasize the need for
scrutiny of how LLMs value human inconvenience, particularly as we move toward
applications where such cash-versus-comfort trade-offs are made on users'
behalf.

</details>


### [10] [Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A Feasibility Study](https://arxiv.org/abs/2506.17410)
*Danielle R. Thomas,Conrad Borchers,Jionghao Lin,Sanjit Kakarla,Shambhavi Bhushan,Erin Gatz,Shivang Gupta,Ralph Abboud,Kenneth R. Koedinger*

Main category: cs.CL

TL;DR: 本研究探讨了使用生成式人工智能来识别和评估真实数学辅导中特定辅导动作的可行性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 辅导可以提高学生的成绩，但是根据音频转录识别和研究哪些辅导行为与大规模的学生学习最相关是一个开放的研究问题。

Method: 我们分析了 50 份随机选择的大学学生远程辅导中学学生数学的成绩单。使用 GPT-4、GPT-4o、GPT-4-turbo、Gemini-1.5-pro 和 LearnLM，我们评估了导师应用的两项导师技能：给予有效的表扬和回应学生数学错误。

Result: 所有模型都能可靠地检测到相关情况，例如，导师向学生提供表扬（准确率 94-98%）和学生犯数学错误（准确率 82-88%），并有效地评估了导师对辅导最佳实践的遵守情况，与人类的判断非常吻合（分别为 83-89% 和 73-77%）。

Conclusion: 我们提出了一个具有成本效益的提示策略，并讨论了在真实环境中使用大型语言模型来支持可扩展评估的实际意义。这项工作进一步贡献了 LLM 提示，以支持 AI 支持的学习中的可重复性和研究。

Abstract: Tutoring improves student achievement, but identifying and studying what
tutoring actions are most associated with student learning at scale based on
audio transcriptions is an open research problem. This present study
investigates the feasibility and scalability of using generative AI to identify
and evaluate specific tutor moves in real-life math tutoring. We analyze 50
randomly selected transcripts of college-student remote tutors assisting middle
school students in mathematics. Using GPT-4, GPT-4o, GPT-4-turbo,
Gemini-1.5-pro, and LearnLM, we assess tutors' application of two tutor skills:
delivering effective praise and responding to student math errors. All models
reliably detected relevant situations, for example, tutors providing praise to
students (94-98% accuracy) and a student making a math error (82-88% accuracy)
and effectively evaluated the tutors' adherence to tutoring best practices,
aligning closely with human judgments (83-89% and 73-77%, respectively). We
propose a cost-effective prompting strategy and discuss practical implications
for using large language models to support scalable assessment in authentic
settings. This work further contributes LLM prompts to support reproducibility
and research in AI-supported learning.

</details>


### [11] [UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making](https://arxiv.org/abs/2506.17419)
*Jinhao Duan,James Diffenderfer,Sandeep Madireddy,Tianlong Chen,Bhavya Kailkhura,Kaidi Xu*

Main category: cs.CL

TL;DR: 本文提出了 UProp，一种用于量化 LLM 在多步决策中不确定性的新方法，该方法优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 不确定性量化 (UQ) 方法主要设计用于单轮问答格式，导致多步决策场景（例如，LLM 主动系统）未被充分探索。因此，知道何时信任 LLM 决策至关重要。

Method: 提出了一个基于信息论的框架，将 LLM 序贯决策不确定性分解为内部不确定性和外在不确定性，并提出了一种有效的外部不确定性估计器 UProp，它将 MI 的直接估计转换为多个轨迹依赖决策过程 (TDP) 上逐点互信息 (PMI) 的估计。

Result: 实验结果表明，配备周到聚合策略的 UProp 明显优于现有的单轮 UQ 基线。

Conclusion: UProp在多步决策基准测试中显著优于现有的单轮 UQ 基线，并提供了全面的分析，包括抽样效率、潜在应用和中间不确定性传播，以证明其有效性。

Abstract: As Large Language Models (LLMs) are integrated into safety-critical
applications involving sequential decision-making in the real world, it is
essential to know when to trust LLM decisions. Existing LLM Uncertainty
Quantification (UQ) methods are primarily designed for single-turn
question-answering formats, resulting in multi-step decision-making scenarios,
e.g., LLM agentic system, being underexplored. In this paper, we introduce a
principled, information-theoretic framework that decomposes LLM sequential
decision uncertainty into two parts: (i) internal uncertainty intrinsic to the
current decision, which is focused on existing UQ methods, and (ii) extrinsic
uncertainty, a Mutual-Information (MI) quantity describing how much uncertainty
should be inherited from preceding decisions. We then propose UProp, an
efficient and effective extrinsic uncertainty estimator that converts the
direct estimation of MI to the estimation of Pointwise Mutual Information (PMI)
over multiple Trajectory-Dependent Decision Processes (TDPs). UProp is
evaluated over extensive multi-step decision-making benchmarks, e.g.,
AgentBench and HotpotQA, with state-of-the-art LLMs, e.g., GPT-4.1 and
DeepSeek-V3. Experimental results demonstrate that UProp significantly
outperforms existing single-turn UQ baselines equipped with thoughtful
aggregation strategies. Moreover, we provide a comprehensive analysis of UProp,
including sampling efficiency, potential applications, and intermediate
uncertainty propagation, to demonstrate its effectiveness. Codes will be
available at https://github.com/jinhaoduan/UProp.

</details>


### [12] [Beyond the Link: Assessing LLMs' ability to Classify Political Content across Global Media](https://arxiv.org/abs/2506.17435)
*Alberto Martinez-Serra,Alejandro De La Fuente,Nienke Viescher,Ana S. Cardenal*

Main category: cs.CL

TL;DR: 本文评估了LLM是否可以准确地从五个国家/地区（法国、德国、西班牙、英国和美国）和不同语言的文章文本和URL中识别PC与非PC。


<details>
  <summary>Details</summary>
Motivation: 以往的研究已经证明了LLM在标记任务中的能力，但是否可以使用LLM仅从URL中分类政治内容（PC）的有效性尚未得到充分的探索。

Method: 使用GPT、Llama、Mistral、Deepseek、Qwen和Gemma等先进的LLM，评估模型性能，以评估URL级别分析是否可以很好地近似PC的全文分析，即使在不同的语言和国家/地区环境中也是如此。将模型输出与人工标记的文章以及传统的监督机器学习技术进行比较，以设定性能基线。

Result: 研究结果表明，URL可以嵌入大多数新闻内容，为准确性-成本平衡提供了一个重要的视角。

Conclusion: URL可以嵌入大多数新闻内容，为准确性-成本平衡提供了一个重要的视角。同时考虑了上下文的局限性，并为在政治科学研究中使用LLM提出了方法论建议。

Abstract: The use of large language models (LLMs) is becoming common in the context of
political science, particularly in studies that analyse individuals use of
digital media. However, while previous research has demonstrated LLMs ability
at labelling tasks, the effectiveness of using LLMs to classify political
content (PC) from just URLs is not yet well explored. The work presented in
this article bridges this gap by evaluating whether LLMs can accurately
identify PC vs. non-PC from both the article text and the URLs from five
countries (France, Germany, Spain, the UK, and the US) and different languages.
Using cutting-edge LLMs like GPT, Llama, Mistral, Deepseek, Qwen and Gemma, we
measure model performance to assess whether URL-level analysis can be a good
approximation for full-text analysis of PC, even across different linguistic
and national contexts. Model outputs are compared with human-labelled articles,
as well as traditional supervised machine learning techniques, to set a
baseline of performance. Overall, our findings suggest the capacity of URLs to
embed most of the news content, providing a vital perspective on accuracy-cost
balancing. We also account for contextual limitations and suggest
methodological recommendations to use LLMs within political science studies.

</details>


### [13] [Breaking the Transcription Bottleneck: Fine-tuning ASR Models for Extremely Low-Resource Fieldwork Languages](https://arxiv.org/abs/2506.17459)
*Siyu Liang,Gina-Anne Levow*

Main category: cs.CL

TL;DR: 本研究评估了两种多语言ASR模型在低资源语言上的性能，为实地语言学家提供了实用的ASR调整方法，以减轻语言文档中的转录瓶颈。


<details>
  <summary>Details</summary>
Motivation: 自动语音识别(ASR)在高资源语言方面已达到令人印象深刻的准确率，但其在语言学领域中的应用仍然有限。在实地调查环境中收集的录音面临着独特的挑战，包括自发性语音、环境噪声和来自未充分记录语言的严重受限的数据集。

Method: 对五种类型不同的低资源语言，评估微调的多语言ASR模型MMS和XLS-R的性能，并控制训练数据的时长。

Result: MMS在极少量训练数据可用时最合适，而XLS-R在训练数据超过一小时后表现出同等性能。

Conclusion: MMS最适合极少量训练数据的情况，而XLS-R在训练数据超过一小时后表现相当。

Abstract: Automatic Speech Recognition (ASR) has reached impressive accuracy for
high-resource languages, yet its utility in linguistic fieldwork remains
limited. Recordings collected in fieldwork contexts present unique challenges,
including spontaneous speech, environmental noise, and severely constrained
datasets from under-documented languages. In this paper, we benchmark the
performance of two fine-tuned multilingual ASR models, MMS and XLS-R, on five
typologically diverse low-resource languages with control of training data
duration. Our findings show that MMS is best suited when extremely small
amounts of training data are available, whereas XLS-R shows parity performance
once training data exceed one hour. We provide linguistically grounded analysis
for further provide insights towards practical guidelines for field linguists,
highlighting reproducible ASR adaptation approaches to mitigate the
transcription bottleneck in language documentation.

</details>


### [14] [Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems](https://arxiv.org/abs/2506.17467)
*Weixin Liang*

Main category: cs.CL

TL;DR: This dissertation examines the adoption and impact of LLMs, revealing biases in AI detection, measuring LLM adoption across writing domains, and exploring LLMs' potential to support researchers.


<details>
  <summary>Details</summary>
Motivation: LLMs are rapidly being adopted across society and changing how we write, communicate, and create.

Method: The dissertation employs population-level algorithmic approaches and large-scale empirical analysis.

Result: The study reveals the institutional adoption of AI detectors introduces systematic biases, measures the increasing adoption of LLMs across writing domains, and investigates LLMs' capability to provide feedback on research manuscripts.

Conclusion: LLMs have the potential to support researchers, especially early-career researchers and those from under-resourced settings, by providing feedback on research manuscripts.

Abstract: Large language models (LLMs) have shown significant potential to change how
we write, communicate, and create, leading to rapid adoption across society.
This dissertation examines how individuals and institutions are adapting to and
engaging with this emerging technology through three research directions.
First, I demonstrate how the institutional adoption of AI detectors introduces
systematic biases, particularly disadvantaging writers of non-dominant language
varieties, highlighting critical equity concerns in AI governance. Second, I
present novel population-level algorithmic approaches that measure the
increasing adoption of LLMs across writing domains, revealing consistent
patterns of AI-assisted content in academic peer reviews, scientific
publications, consumer complaints, corporate communications, job postings, and
international organization press releases. Finally, I investigate LLMs'
capability to provide feedback on research manuscripts through a large-scale
empirical analysis, offering insights into their potential to support
researchers who face barriers in accessing timely manuscript feedback,
particularly early-career researchers and those from under-resourced settings.

</details>


### [15] [VeriLocc: End-to-End Cross-Architecture Register Allocation via LLM](https://arxiv.org/abs/2506.17506)
*Lesheng Jin,Zhenyuan Ruan,Haohui Mai,Jingbo Shang*

Main category: cs.CL

TL;DR: VeriLocc, a framework that combines large language models (LLMs) with formal compiler techniques to enable generalizable and verifiable register allocation across GPU architectures


<details>
  <summary>Details</summary>
Motivation: production compilers still rely on hand-crafted register allocation heuristics that require substantial re-tuning for each hardware generation.

Method: a framework that combines large language models (LLMs) with formal compiler techniques to enable generalizable and verifiable register allocation across GPU architectures. VeriLocc fine-tunes an LLM to translate intermediate representations (MIRs) into target-specific register assignments, aided by static analysis for cross-architecture normalization and generalization and a verifier-guided regeneration loop to ensure correctness.

Result: VeriLocc achieves 85-99% single-shot accuracy and near-100% pass@100. Case study shows that

Conclusion: VeriLocc discovers more performant assignments than expert-tuned libraries, outperforming rocBLAS by over 10% in runtime.

Abstract: Modern GPUs evolve rapidly, yet production compilers still rely on
hand-crafted register allocation heuristics that require substantial re-tuning
for each hardware generation. We introduce VeriLocc, a framework that combines
large language models (LLMs) with formal compiler techniques to enable
generalizable and verifiable register allocation across GPU architectures.
VeriLocc fine-tunes an LLM to translate intermediate representations (MIRs)
into target-specific register assignments, aided by static analysis for
cross-architecture normalization and generalization and a verifier-guided
regeneration loop to ensure correctness. Evaluated on matrix multiplication
(GEMM) and multi-head attention (MHA), VeriLocc achieves 85-99% single-shot
accuracy and near-100% pass@100. Case study shows that VeriLocc discovers more
performant assignments than expert-tuned libraries, outperforming rocBLAS by
over 10% in runtime.

</details>


### [16] [Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning](https://arxiv.org/abs/2506.17525)
*Mingfei Lau,Qian Chen,Yeming Fang,Tingting Xu,Tongzhou Chen,Pavel Golik*

Main category: cs.CL

TL;DR: quality issues in multilingual speech datasets


<details>
  <summary>Details</summary>
Motivation: in some languages, these datasets suffer from significant quality issues. We believe addressing these issues will make these datasets more useful as training and evaluation sets, and improve downstream models

Method: quality audit for three widely used public multilingual speech datasets - Mozilla Common Voice 17.0, FLEURS, and VoxPopuli

Result: macro-level issues are more prevalent in less institutionalized, often under-resourced languages. We provide a case analysis of Taiwanese Southern Min (nan_tw) that highlights the need for proactive language planning (e.g. orthography prescriptions, dialect boundary definition) and enhanced data quality control in the process of Automatic Speech Recognition (ASR) dataset creation

Conclusion: proposing guidelines and recommendations to mitigate these issues in future dataset development, emphasizing the importance of sociolinguistic awareness in creating robust and reliable speech data resources.

Abstract: Our quality audit for three widely used public multilingual speech datasets -
Mozilla Common Voice 17.0, FLEURS, and VoxPopuli - shows that in some
languages, these datasets suffer from significant quality issues. We believe
addressing these issues will make these datasets more useful as training and
evaluation sets, and improve downstream models. We divide these quality issues
into two categories: micro-level and macro-level. We find that macro-level
issues are more prevalent in less institutionalized, often under-resourced
languages. We provide a case analysis of Taiwanese Southern Min (nan_tw) that
highlights the need for proactive language planning (e.g. orthography
prescriptions, dialect boundary definition) and enhanced data quality control
in the process of Automatic Speech Recognition (ASR) dataset creation. We
conclude by proposing guidelines and recommendations to mitigate these issues
in future dataset development, emphasizing the importance of sociolinguistic
awareness in creating robust and reliable speech data resources.

</details>


### [17] [DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning](https://arxiv.org/abs/2506.17533)
*Yuanhao Wu,Juntong Song,Hanning Zhang,Tong Zhang,Cheng Niu*

Main category: cs.CL

TL;DR: DuaShepherd, a novel reward modeling framework, integrates correctness and potential reward signals to improve the mathematical reasoning capabilities of LLMs, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: This paper aims to enhance the mathematical reasoning capabilities of Large Language Models (LLMs).

Method: The paper developed an automated pipeline for constructing large-scale reward modeling dataset with both correctness and potential signals and explored a unified, multi-head architecture to train the two reward models in a multi-task setup.

Result: The model achieves state-of-the-art performance on MATH500 and ProcessBench.

Conclusion: The model achieves consistent performance improvements across multiple benchmarks and outperforms models trained on either reward type alone, achieving state-of-the-art performance under comparable resource constraints.

Abstract: In this paper, we propose DuaShepherd, a novel reward modeling framework that
integrates two complementary reward signals, correctness and potential, to
enhance the mathematical reasoning capabilities of Large Language Models
(LLMs). While correctness-based signals emphasize identification of stepwise
errors, potential-based signals focus on the likelihood of reaching the correct
final answer. We developed an automated pipeline for constructing large-scale
reward modeling dataset with both signals. A unified, multi-head architecture
was explored to train the two reward models in a multi-task setup,
demonstrating benefits from learning both correctness and potential in
parallel. By combining these two signals into a compound probability, our model
achieves consistent performance improvements across multiple benchmarks.
Empirical evaluations on MATH500 and ProcessBench confirm that this combined
reward significantly outperforms models trained on either reward type alone,
achieving state-of-the-art performance under comparable resource constraints.

</details>


### [18] [Probing for Phonology in Self-Supervised Speech Representations: A Case Study on Accent Perception](https://arxiv.org/abs/2506.17542)
*Nitin Venkateswaran,Kevin Tang,Ratree Wayland*

Main category: cs.CL

TL;DR: 本研究探讨了自监督学习模型如何编码影响片段口音感知的音系特征级变异，结果表明，自监督语音表征对于使用可解释的音系特征建模口音感知具有价值。


<details>
  <summary>Details</summary>
Motivation: 传统口音感知模型低估了音系特征中梯度变化的作用，而听众依赖这些变化来判断口音。

Method: 使用 CSLU 外语口音英语语料库，提取音系特征概率，并使用 Wav2Vec2-BERT 和 WavLM 的预训练表征，以及美国英语母语者的口音判断。

Result: 口音强度可以通过一部分预训练表征特征来最好地预测，其中感知上显著的音系特征被赋予突出的权重，这些特征对比了预期的美国英语和实现的非母语英语片段。基于预训练表征的片段距离与口音等级之间的多项式逻辑回归显示，口音强度与基线距离之间存在很强的关联。

Conclusion: 自监督语音表征对于使用可解释的音系特征建模口音感知具有价值。

Abstract: Traditional models of accent perception underestimate the role of gradient
variations in phonological features which listeners rely upon for their accent
judgments. We investigate how pretrained representations from current
self-supervised learning (SSL) models of speech encode phonological
feature-level variations that influence the perception of segmental accent. We
focus on three segments: the labiodental approximant, the rhotic tap, and the
retroflex stop, which are uniformly produced in the English of native speakers
of Hindi as well as other languages in the Indian sub-continent. We use the
CSLU Foreign Accented English corpus (Lander, 2007) to extract, for these
segments, phonological feature probabilities using Phonet (V\'asquez-Correa et
al., 2019) and pretrained representations from Wav2Vec2-BERT (Barrault et al.,
2023) and WavLM (Chen et al., 2022) along with accent judgements by native
speakers of American English. Probing analyses show that accent strength is
best predicted by a subset of the segment's pretrained representation features,
in which perceptually salient phonological features that contrast the expected
American English and realized non-native English segments are given prominent
weighting. A multinomial logistic regression of pretrained representation-based
segment distances from American and Indian English baselines on accent ratings
reveals strong associations between the odds of accent strength and distances
from the baselines, in the expected directions. These results highlight the
value of self-supervised speech representations for modeling accent perception
using interpretable phonological features.

</details>


### [19] [AgriCHN: A Comprehensive Cross-domain Resource for Chinese Agricultural Named Entity Recognition](https://arxiv.org/abs/2506.17578)
*Lingxiao Zeng,Yiqi Tong,Wei Guo,Huarui Wu,Lihao Ge,Yijun Ye,Fuzhen Zhuang,Deqing Wang,Wei Guo,Cheng Chen*

Main category: cs.CL

TL;DR: 提出了一个全面的开源中文资源 AgriCHN，旨在提高自动农业实体注释的准确性。


<details>
  <summary>Details</summary>
Motivation: 高质量的农业数据集稀缺，尤其是在中文方面，导致使用主流方法进行此目的时性能欠佳。早期的大多数工作仅侧重于注释农业实体，而忽略了农业与水文和气象的深刻相关性。

Method: 构建了一个基准任务，使用了多个最先进的神经 NER 模型。

Result:  AgriCHN数据集比相关资源表现出卓越的数据质量，这归因于其更丰富的农业实体类型和更细粒度的实体划分。大量的实验结果突出了 AgriCHN 带来的重大挑战及其进一步研究的潜力。

Conclusion: AgriCHN数据集具有挑战性，有进一步研究的潜力。

Abstract: Agricultural named entity recognition is a specialized task focusing on
identifying distinct agricultural entities within vast bodies of text,
including crops, diseases, pests, and fertilizers. It plays a crucial role in
enhancing information extraction from extensive agricultural text resources.
However, the scarcity of high-quality agricultural datasets, particularly in
Chinese, has resulted in suboptimal performance when employing mainstream
methods for this purpose. Most earlier works only focus on annotating
agricultural entities while overlook the profound correlation of agriculture
with hydrology and meteorology. To fill this blank, we present AgriCHN, a
comprehensive open-source Chinese resource designed to promote the accuracy of
automated agricultural entity annotation. The AgriCHN dataset has been
meticulously curated from a wealth of agricultural articles, comprising a total
of 4,040 sentences and encapsulating 15,799 agricultural entity mentions
spanning 27 diverse entity categories. Furthermore, it encompasses entities
from hydrology to meteorology, thereby enriching the diversity of entities
considered. Data validation reveals that, compared with relevant resources,
AgriCHN demonstrates outstanding data quality, attributable to its richer
agricultural entity types and more fine-grained entity divisions. A benchmark
task has also been constructed using several state-of-the-art neural NER
models. Extensive experimental results highlight the significant challenge
posed by AgriCHN and its potential for further research.

</details>


### [20] [Mind the Gap: Assessing Wiktionary's Crowd-Sourced Linguistic Knowledge on Morphological Gaps in Two Related Languages](https://arxiv.org/abs/2506.17603)
*Jonathan Sakunkoo,Annabella Sakunkoo*

Main category: cs.CL

TL;DR: This study validates Wiktionary's defective verb lists for Latin and Italian using a neural morphological analyzer, finding high reliability for Italian but discrepancies for Latin, highlighting limitations of crowd-sourced data for less-studied languages.


<details>
  <summary>Details</summary>
Motivation: Addressing defectivity, where expected inflectional forms are absent, is essential for improving the accuracy of NLP tools in morphologically rich languages. However, traditional linguistic resources often lack coverage of morphological gaps as such knowledge requires significant human expertise and effort to document and verify. For scarce linguistic phenomena in under-explored languages, Wikipedia and Wiktionary often serve as among the few accessible resources. Despite their extensive reach, their reliability has been a subject of controversy.

Method: customizes a novel neural morphological analyzer to annotate Latin and Italian corpora. Using the massive annotated data, crowd-sourced lists of defective verbs compiled from Wiktionary are validated computationally.

Result: while Wiktionary provides a highly reliable account of Italian morphological gaps, 7% of Latin lemmata listed as defective show strong corpus evidence of being non-defective.

Conclusion: Wiktionary provides a highly reliable account of Italian morphological gaps, but 7% of Latin lemmata listed as defective show strong corpus evidence of being non-defective. This discrepancy highlights potential limitations of crowd-sourced wikis as definitive sources of linguistic knowledge, particularly for less-studied phenomena and languages.

Abstract: Morphological defectivity is an intriguing and understudied phenomenon in
linguistics. Addressing defectivity, where expected inflectional forms are
absent, is essential for improving the accuracy of NLP tools in morphologically
rich languages. However, traditional linguistic resources often lack coverage
of morphological gaps as such knowledge requires significant human expertise
and effort to document and verify. For scarce linguistic phenomena in
under-explored languages, Wikipedia and Wiktionary often serve as among the few
accessible resources. Despite their extensive reach, their reliability has been
a subject of controversy. This study customizes a novel neural morphological
analyzer to annotate Latin and Italian corpora. Using the massive annotated
data, crowd-sourced lists of defective verbs compiled from Wiktionary are
validated computationally. Our results indicate that while Wiktionary provides
a highly reliable account of Italian morphological gaps, 7% of Latin lemmata
listed as defective show strong corpus evidence of being non-defective. This
discrepancy highlights potential limitations of crowd-sourced wikis as
definitive sources of linguistic knowledge, particularly for less-studied
phenomena and languages, despite their value as resources for rare linguistic
features. By providing scalable tools and methods for quality assurance of
crowd-sourced data, this work advances computational morphology and expands
linguistic knowledge of defectivity in non-English, morphologically rich
languages.

</details>


### [21] [TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track Forecasting](https://arxiv.org/abs/2506.17609)
*Lincan Li,Eren Erman Ozguven,Yue Zhao,Guang Wang,Yiqun Xie,Yushun Dong*

Main category: cs.CL

TL;DR: TyphoFormer结合了自然语言描述作为辅助提示，以改进台风轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的模型在对智慧城市中人类和车辆的密集轨迹的建模方面表现出强大的性能，但它们通常缺乏更广泛的上下文知识，而这些知识可以增强稀疏气象轨迹（如台风轨迹）的预测可靠性。

Method: 提出TyphoFormer，这是一种新颖的框架，它结合了自然语言描述作为辅助提示，以改进台风轨迹预测。

Result: 通过在统一的Transformer编码器中集成文本和序列信息，TyphoFormer使模型能够利用通过数值特征本身无法访问的上下文线索。

Conclusion: TyphoFormer在HURDAT2基准测试中始终优于其他最先进的基线方法，尤其是在涉及非线性路径偏移和有限历史观测的具有挑战性的情况下。

Abstract: Accurate typhoon track forecasting is crucial for early system warning and
disaster response. While Transformer-based models have demonstrated strong
performance in modeling the temporal dynamics of dense trajectories of humans
and vehicles in smart cities, they usually lack access to broader contextual
knowledge that enhances the forecasting reliability of sparse meteorological
trajectories, such as typhoon tracks. To address this challenge, we propose
TyphoFormer, a novel framework that incorporates natural language descriptions
as auxiliary prompts to improve typhoon trajectory forecasting. For each time
step, we use Large Language Model (LLM) to generate concise textual
descriptions based on the numerical attributes recorded in the North Atlantic
hurricane database. The language descriptions capture high-level meteorological
semantics and are embedded as auxiliary special tokens prepended to the
numerical time series input. By integrating both textual and sequential
information within a unified Transformer encoder, TyphoFormer enables the model
to leverage contextual cues that are otherwise inaccessible through numerical
features alone. Extensive experiments are conducted on HURDAT2 benchmark,
results show that TyphoFormer consistently outperforms other state-of-the-art
baseline methods, particularly under challenging scenarios involving nonlinear
path shifts and limited historical observations.

</details>


### [22] [OpusLM: A Family of Open Unified Speech Language Models](https://arxiv.org/abs/2506.17611)
*Jinchuan Tian,William Chen,Yifan Peng,Jiatong Shi,Siddhant Arora,Shikhar Bharadwaj,Takashi Maekaku,Yusuke Shinohara,Keita Goto,Xiang Yue,Huck Yang,Shinji Watanabe*

Main category: cs.CL

TL;DR: OpusLMs, a family of open foundational speech language models up to 7B, achieves comparable or superior performance with existing SpeechLMs. The code, data, checkpoints, and training logs are released.


<details>
  <summary>Details</summary>
Motivation: To present Open Unified Speech Language Models (OpusLMs), a family of open foundational speech language models (SpeechLMs) up to 7B, built from publicly available materials and are fully transparent models.

Method: Continuously pre-trained on 213K hours of speech-text pairs and 292B text-only tokens, with specific designs on tokenization, multi-stream language models, and multi-stage training strategies.

Result: Demonstrates the importance of model size scaling and the effect of annealing data selection.

Conclusion: OpusLMs achieve comparable or superior performance with existing SpeechLMs in speech recognition, speech synthesis, and text-only capabilities.

Abstract: This paper presents Open Unified Speech Language Models (OpusLMs), a family
of open foundational speech language models (SpeechLMs) up to 7B. Initialized
from decoder-only text language models, the OpusLMs are continuously
pre-trained on 213K hours of speech-text pairs and 292B text-only tokens. We
demonstrate our OpusLMs achieve comparable (or even superior) performance with
existing SpeechLMs in speech recognition, speech synthesis, and text-only
capabilities. Technically, this paper articulates our SpeechLM designs on
tokenization, multi-stream language models, and multi-stage training
strategies. We experimentally demonstrate the importance of model size scaling
and the effect of annealing data selection. The OpusLMs are all built from
publicly available materials and are fully transparent models. We release our
code, data, checkpoints, and training logs to facilitate open SpeechLM research

</details>


### [23] [When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking](https://arxiv.org/abs/2506.18535)
*Manu Pande,Shahil Kumar,Anay Yatin Damle*

Main category: cs.CL

TL;DR: fine-tuning pre-trained transformer models degrades performance on the MS MARCO passage ranking task


<details>
  <summary>Details</summary>
Motivation: investigates the counterintuitive phenomenon where fine-tuning pre-trained transformer models degrades performance

Method: comprehensive experiments involving five model variants, UMAP visualizations, training dynamics analysis and computational efficiency metrics

Result: all fine-tuning approaches underperform the base sentence-transformers/all- MiniLM-L6-v2 model (MRR@10: 0.3026). progressive embedding space flattening

Conclusion: fine-tuning degrades performance on MS MARCO passage ranking task and disrupts the optimal embedding space structure

Abstract: This paper investigates the counterintuitive phenomenon where fine-tuning
pre-trained transformer models degrades performance on the MS MARCO passage
ranking task. Through comprehensive experiments involving five model
variants-including full parameter fine-tuning and parameter efficient LoRA
adaptations-we demonstrate that all fine-tuning approaches underperform the
base sentence-transformers/all- MiniLM-L6-v2 model (MRR@10: 0.3026). Our
analysis reveals that fine-tuning disrupts the optimal embedding space
structure learned during the base model's extensive pre-training on 1 billion
sentence pairs, including 9.1 million MS MARCO samples. UMAP visualizations
show progressive embedding space flattening, while training dynamics analysis
and computational efficiency metrics further support our findings. These
results challenge conventional wisdom about transfer learning effectiveness on
saturated benchmarks and suggest architectural innovations may be necessary for
meaningful improvements.

</details>


### [24] [Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor in LLMs](https://arxiv.org/abs/2506.17630)
*Yang Wu,Yifan Zhang,Yiwei Wang,Yujun Cai,Yurong Wu,Yuran Wang,Ning Xu,Jian Cheng*

Main category: cs.CL

TL;DR: LLM的推理可能反映事后合理化，而不是真正的推理，我们需要对LLM的推理构成进行更细致的理解。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）表现出令人印象深刻的推理能力，但越来越多的证据表明，它们的成功主要源于记忆的答案-推理模式，而不是真正的推理。

Method: 提出了一个五级答案可见性提示框架，通过间接行为分析来探测模型行为。

Result: 当答案线索被屏蔽时，即使有完整的推理链，性能也会下降26.90%。

Conclusion: LLMs主要依赖于显式答案，而不是真正的推理。

Abstract: While Large Language Models (LLMs) demonstrate impressive reasoning
capabilities, growing evidence suggests much of their success stems from
memorized answer-reasoning patterns rather than genuine inference. In this
work, we investigate a central question: are LLMs primarily anchored to final
answers or to the textual pattern of reasoning chains? We propose a five-level
answer-visibility prompt framework that systematically manipulates answer cues
and probes model behavior through indirect, behavioral analysis. Experiments
across state-of-the-art LLMs reveal a strong and consistent reliance on
explicit answers. The performance drops by 26.90\% when answer cues are masked,
even with complete reasoning chains. These findings suggest that much of the
reasoning exhibited by LLMs may reflect post-hoc rationalization rather than
true inference, calling into question their inferential depth. Our study
uncovers the answer-anchoring phenomenon with rigorous empirical validation and
underscores the need for a more nuanced understanding of what constitutes
reasoning in LLMs.

</details>


### [25] [Step-Opt: Boosting Optimization Modeling in LLMs through Iterative Data Synthesis and Structured Validation](https://arxiv.org/abs/2506.17637)
*Yang Wu,Yifan Zhang,Yurong Wu,Yuran Wang,Junkai Zhang,Jian Cheng*

Main category: cs.CL

TL;DR: Step-Opt-Instruct improves LLMs for optimization modeling by generating high-quality fine-tuning data, leading to state-of-the-art performance on OR benchmarks.


<details>
  <summary>Details</summary>
Motivation: LLMs encounter substantial challenges in tackling optimization modeling tasks for OR, particularly when dealing with complex problems.

Method: Step-Opt-Instruct, a framework that augments existing datasets and generates high-quality fine-tuning data tailored to optimization modeling, employing iterative problem generation to systematically increase problem complexity and stepwise validation to rigorously verify data.

Result: Step-Opt achieves state-of-the-art performance on benchmarks such as NL4OPT, MAMO, and IndustryOR.

Conclusion: Step-Opt demonstrates superior performance, especially in addressing complex OR tasks, with a notable 17.01% improvement in micro average accuracy on difficult problems, highlighting the effectiveness of combining structured validation with gradual problem refinement to advance the automation of decision-making processes using LLMs.

Abstract: Large Language Models (LLMs) have revolutionized various domains but
encounter substantial challenges in tackling optimization modeling tasks for
Operations Research (OR), particularly when dealing with complex problem. In
this work, we propose Step-Opt-Instruct, a framework that augments existing
datasets and generates high-quality fine-tuning data tailored to optimization
modeling. Step-Opt-Instruct employs iterative problem generation to
systematically increase problem complexity and stepwise validation to
rigorously verify data, preventing error propagation and ensuring the quality
of the generated dataset. Leveraging this framework, we fine-tune open-source
LLMs, including LLaMA-3-8B and Mistral-7B, to develop Step-Opt--a model that
achieves state-of-the-art performance on benchmarks such as NL4OPT, MAMO, and
IndustryOR. Extensive experiments demonstrate the superior performance of
Step-Opt, especially in addressing complex OR tasks, with a notable 17.01\%
improvement in micro average accuracy on difficult problems. These findings
highlight the effectiveness of combining structured validation with gradual
problem refinement to advance the automation of decision-making processes using
LLMs.The code and dataset are available at https://github.com/samwu-learn/Step.

</details>


### [26] [TPTT: Transforming Pretrained Transformer into Titans](https://arxiv.org/abs/2506.17671)
*Fabien Furfaro*

Main category: cs.CL

TL;DR: TPTT is a novel framework for enhancing pretrained Transformer models with efficient linearized attention mechanisms and advanced memory management, achieving substantial improvements in both efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Computational and memory demands of large language models (LLMs) remain a significant challenge, particularly for long-context inference.

Method: TPTT employs techniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA).

Result: Titans-Llama-3.2-1B achieves a 20% increase in Exact Match (EM) over its baseline.

Conclusion: The effectiveness of TPTT is confirmed on the MMLU benchmark, with Titans-Llama-3.2-1B achieving a 20% increase in Exact Match (EM) over its baseline. Statistical analyses and comparisons with recent state-of-the-art methods confirm the practical scalability and robustness of TPTT.

Abstract: Recent advances in large language models (LLMs) have led to remarkable
progress in natural language processing, but their computational and memory
demands remain a significant challenge, particularly for long-context
inference. We introduce TPTT (Transforming Pretrained Transformer into Titans),
a novel framework for enhancing pretrained Transformer models with efficient
linearized attention mechanisms and advanced memory management. TPTT employs
techniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA).
It is fully compatible with the Hugging Face Transformers library, enabling
seamless adaptation of any causal LLM through parameter-efficient fine-tuning
(LoRA) without full retraining. We show the effectiveness of TPTT on the MMLU
benchmark with models of approximately 1 billion parameters, observing
substantial improvements in both efficiency and accuracy. For instance,
Titans-Llama-3.2-1B achieves a 20% increase in Exact Match (EM) over its
baseline. Statistical analyses and comparisons with recent state-of-the-art
methods confirm the practical scalability and robustness of TPTT. Code is
available at https://github.com/fabienfrfr/tptt . Python package at
https://pypi.org/project/tptt/ .

</details>


### [27] [Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering](https://arxiv.org/abs/2506.17692)
*Binquan Ji,Haibo Luo,Yifei Lu,Lei Hei,Jiaqi Wang,Tingjing Liao,Lingyu Wang,Shichao Wang,Feiliang Ren*

Main category: cs.CL

TL;DR: DEC is a framework that decomposes complex questions, refines subquestions, and uses keyword extraction for multi-hop QA, achieving state-of-the-art results with reduced token consumption, especially on smaller models.


<details>
  <summary>Details</summary>
Motivation: Knowledge-intensive multi-hop question answering tasks necessitate multiple rounds of retrieval and iterative generation by LLMs, but incorporating many documents and extended contexts poses challenges for lightweight LLMs.

Method: DEC decomposes complex questions into logically coherent subquestions, iteratively refines these subquestions through context-aware rewriting, and introduces a lightweight discriminative keyword extraction module.

Result: DEC performs on par with or surpasses state-of-the-art benchmarks while significantly reducing token consumption. DEC attains state-of-the-art results on models with 8B parameters.

Conclusion: DEC performs on par with or surpasses state-of-the-art benchmarks while significantly reducing token consumption, attaining state-of-the-art results on models with 8B parameters.

Abstract: Knowledge-intensive multi-hop question answering (QA) tasks, which require
integrating evidence from multiple sources to address complex queries, often
necessitate multiple rounds of retrieval and iterative generation by large
language models (LLMs). However, incorporating many documents and extended
contexts poses challenges -such as hallucinations and semantic drift-for
lightweight LLMs with fewer parameters. This work proposes a novel framework
called DEC (Dynamic Enhancement Chain). DEC first decomposes complex questions
into logically coherent subquestions to form a hallucination-free reasoning
chain. It then iteratively refines these subquestions through context-aware
rewriting to generate effective query formulations. For retrieval, we introduce
a lightweight discriminative keyword extraction module that leverages extracted
keywords to achieve targeted, precise document recall with relatively low
computational overhead. Extensive experiments on three multi-hop QA datasets
demonstrate that DEC performs on par with or surpasses state-of-the-art
benchmarks while significantly reducing token consumption. Notably, our
approach attains state-of-the-art results on models with 8B parameters,
showcasing its effectiveness in various scenarios, particularly in
resource-constrained environments.

</details>


### [28] [Zero-Shot Conversational Stance Detection: Dataset and Approaches](https://arxiv.org/abs/2506.17693)
*Yuzhe Ding,Kang He,Bobo Li,Li Zheng,Haijun He,Fei Li,Chong Teng,Donghong Ji*

Main category: cs.CL

TL;DR: A new dataset (ZS-CSD) and model (SITPCL) are introduced for zero-shot conversational stance detection. The model achieves state-of-the-art results but the task remains challenging.


<details>
  <summary>Details</summary>
Motivation: Existing conversational stance detection datasets are restricted to a limited set of specific targets, which constrains the effectiveness of stance detection models when encountering a large number of unseen targets in real-world applications.

Method: We propose SITPCL, a speaker interaction and target-aware prototypical contrastive learning model.

Result: The proposed SITPCL model achieves state-of-the-art performance in zero-shot conversational stance detection.

Conclusion: The SITPCL model achieves state-of-the-art performance in zero-shot conversational stance detection, but the F1-macro score of 43.81% highlights the persistent challenges in this area.

Abstract: Stance detection, which aims to identify public opinion towards specific
targets using social media data, is an important yet challenging task. With the
increasing number of online debates among social media users, conversational
stance detection has become a crucial research area. However, existing
conversational stance detection datasets are restricted to a limited set of
specific targets, which constrains the effectiveness of stance detection models
when encountering a large number of unseen targets in real-world applications.
To bridge this gap, we manually curate a large-scale, high-quality zero-shot
conversational stance detection dataset, named ZS-CSD, comprising 280 targets
across two distinct target types. Leveraging the ZS-CSD dataset, we propose
SITPCL, a speaker interaction and target-aware prototypical contrastive
learning model, and establish the benchmark performance in the zero-shot
setting. Experimental results demonstrate that our proposed SITPCL model
achieves state-of-the-art performance in zero-shot conversational stance
detection. Notably, the SITPCL model attains only an F1-macro score of 43.81%,
highlighting the persistent challenges in zero-shot conversational stance
detection.

</details>


### [29] [The Evolution of Natural Language Processing: How Prompt Optimization and Language Models are Shaping the Future](https://arxiv.org/abs/2506.17700)
*Summra Saleem,Muhammad Nabeel Asim,Shaista Zulfiqar,Andreas Dengel*

Main category: cs.CL

TL;DR: This paper provides unique and comprehensive insights about the potential of diverse prompt optimization strategies, analyzes their underlying working paradigms, and categorizes them into 11 distinct classes. Moreover, the paper provides details about various NLP tasks where these prompt optimization strategies have been employed, along with details of different LLMs and benchmark datasets used for evaluation.


<details>
  <summary>Details</summary>
Motivation: A critical gap exists in comprehensive analyses of prompt optimization strategies.

Method: This paper analyzes the underlying working paradigms of prompt optimization strategies and categorizes them into 11 distinct classes.

Result: The paper provides details about various NLP tasks where prompt optimization strategies have been employed, along with details of different LLMs and benchmark datasets used for evaluation. This comprehensive compilation lays a robust foundation for future comparative studies and enables rigorous assessment of prompt optimization and LLM-based predictive pipelines under consistent experimental settings.

Conclusion: This research centralizes diverse strategic knowledge to facilitate the adaptation of existing prompt optimization strategies for development of innovative predictors across unexplored tasks.

Abstract: Large Language Models (LLMs) have revolutionized the field of Natural
Language Processing (NLP) by automating traditional labor-intensive tasks and
consequently accelerated the development of computer-aided applications. As
researchers continue to advance this field with the introduction of novel
language models and more efficient training/finetuning methodologies, the idea
of prompt engineering and subsequent optimization strategies with LLMs has
emerged as a particularly impactful trend to yield a substantial performance
boost across diverse NLP tasks. To best of our knowledge numerous review
articles have explored prompt engineering, however, a critical gap exists in
comprehensive analyses of prompt optimization strategies. To bridge this gap
this paper provides unique and comprehensive insights about the potential of
diverse prompt optimization strategies. It analyzes their underlying working
paradigms and based on these principles, categorizes them into 11 distinct
classes. Moreover, the paper provides details about various NLP tasks where
these prompt optimization strategies have been employed, along with details of
different LLMs and benchmark datasets used for evaluation. This comprehensive
compilation lays a robust foundation for future comparative studies and enables
rigorous assessment of prompt optimization and LLM-based predictive pipelines
under consistent experimental settings: a critical need in the current
landscape. Ultimately, this research will centralize diverse strategic
knowledge to facilitate the adaptation of existing prompt optimization
strategies for development of innovative predictors across unexplored tasks.

</details>


### [30] [Aged to Perfection: Machine-Learning Maps of Age in Conversational English](https://arxiv.org/abs/2506.17708)
*MingZe Tang*

Main category: cs.CL

TL;DR: investigate language patterns across different age groups


<details>
  <summary>Details</summary>
Motivation: Our research attempts to explore how language patterns vary between different age groups, exploring the connection between speaker demographics and linguistic factors such as utterance duration, lexical diversity, and word choice.

Method: By merging computational language analysis and machine learning methodologies

Result: uncover distinctive linguistic markers characteristic of multiple generations and create prediction models that can consistently estimate the speaker's age group from various aspects.

Conclusion: This work contributes to our knowledge of sociolinguistic diversity throughout the life of modern British speech.

Abstract: The study uses the British National Corpus 2014, a large sample of
contemporary spoken British English, to investigate language patterns across
different age groups. Our research attempts to explore how language patterns
vary between different age groups, exploring the connection between speaker
demographics and linguistic factors such as utterance duration, lexical
diversity, and word choice. By merging computational language analysis and
machine learning methodologies, we attempt to uncover distinctive linguistic
markers characteristic of multiple generations and create prediction models
that can consistently estimate the speaker's age group from various aspects.
This work contributes to our knowledge of sociolinguistic diversity throughout
the life of modern British speech.

</details>


### [31] [Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource Medieval Romance Languages](https://arxiv.org/abs/2506.17715)
*Matthias Schöffel,Esteban Garces Arias,Marinus Wiedner,Paula Ruppert,Meimingwei Li,Christian Heumann,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 本研究探讨了LLMs在中世纪罗曼语POS tagging中的应用，发现其在处理历史语言变体和非标准化拼写方面存在局限性，并提出了一些有效的专门技术。


<details>
  <summary>Details</summary>
Motivation: 词性 (POS) 标注仍然是自然语言处理流程中的一个基础组件, 对于计算语言学和数字人文交叉领域的历史文本分析尤其重要。尽管现代大型语言模型 (LLM) 在古代语言方面取得了显著进步, 但它们在应用于中世纪罗曼语时，由于历时语言演变、拼写变化和标记数据稀缺，面临着独特的挑战。

Method: 系统地研究了中世纪奥克语、中世纪西班牙语和中世纪法语文本中POS tagging性能的决定因素, 并评估了微调方法、提示工程、模型架构、解码策略和跨语言迁移学习技术对 tagging 准确率的影响。

Result: 结果表明，LLMs在处理历史语言变体和非标准化拼写方面的能力存在明显的局限性, 同时也表明一些有希望的专门技术可以有效地解决低资源历史语言带来的独特挑战。

Conclusion: LLMs在处理历史语言变体和非标准化拼写方面存在明显的局限性, 但也存在一些有希望的专门技术, 可以有效地解决低资源历史语言带来的独特挑战。

Abstract: Part-of-speech (POS) tagging remains a foundational component in natural
language processing pipelines, particularly critical for historical text
analysis at the intersection of computational linguistics and digital
humanities. Despite significant advancements in modern large language models
(LLMs) for ancient languages, their application to Medieval Romance languages
presents distinctive challenges stemming from diachronic linguistic evolution,
spelling variations, and labeled data scarcity. This study systematically
investigates the central determinants of POS tagging performance across diverse
corpora of Medieval Occitan, Medieval Spanish, and Medieval French texts,
spanning biblical, hagiographical, medical, and dietary domains. Through
rigorous experimentation, we evaluate how fine-tuning approaches, prompt
engineering, model architectures, decoding strategies, and cross-lingual
transfer learning techniques affect tagging accuracy. Our results reveal both
notable limitations in LLMs' ability to process historical language variations
and non-standardized spelling, as well as promising specialized techniques that
effectively address the unique challenges presented by low-resource historical
languages.

</details>


### [32] [KAG-Thinker: Teaching Large Language Models to Think with Human-like Reasoning Process](https://arxiv.org/abs/2506.17728)
*Dalong Zhang,Jun Xu,Jun Zhou,Lei Liang,Lin Yuan,Ling Zhong,Mengshu Sun,Peilong Zhao,QiWei Wang,Xiaorui Wang,Xinkai Du,YangYang Hou,Yu Ao,ZhaoYang Wang,Zhengke Gui,ZhiYing Yi,Zhongpu Bo*

Main category: cs.CL

TL;DR: KAG-Thinker enhances LLM reasoning on knowledge bases by mimicking human cognition, decomposing problems, and using supervised fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance logical coherence and contextual consistency in question-answering tasks on domain-specific knowledge bases within LLMs by simulating human cognitive mechanisms.

Method: The paper introduces KAG-Thinker, a framework that decomposes complex questions into sub-problems, uses breadth decomposition, knowledge retrieval, and depth solving models, and classifies tasks as Knowledge Retrieval or Reasoning Analysis.

Result: The paper presents KAG-Thinker, a human-like reasoning framework built upon a parameter-light large language model (LLM).

Conclusion: The paper utilizes supervised fine-tuning with multi-turn dialogues to align the model with a structured inference paradigm, avoiding excessive reflection, supported by a data evaluation framework and iterative corpus synthesis.

Abstract: In this paper, we introduce KAG-Thinker, a novel human-like reasoning
framework built upon a parameter-light large language model (LLM). Our approach
enhances the logical coherence and contextual consistency of the thinking
process in question-answering (Q\&A) tasks on domain-specific knowledge bases
(KBs) within LLMs. This framework simulates human cognitive mechanisms for
handling complex problems by establishing a structured thinking process.
Continuing the \textbf{Logical Form} guided retrieval and reasoning technology
route of KAG v0.7, firstly, it decomposes complex questions into independently
solvable sub-problems(also referred to as logical forms) through
\textbf{breadth decomposition}, each represented in two equivalent
forms-natural language and logical function-and further classified as either
Knowledge Retrieval or Reasoning Analysis tasks, with dependencies and
variables passing explicitly modeled via logical function interfaces. In the
solving process, the Retrieval function is used to perform knowledge retrieval
tasks, while the Math and Deduce functions are used to perform reasoning
analysis tasks. Secondly, it is worth noting that, in the Knowledge Retrieval
sub-problem tasks, LLMs and external knowledge sources are regarded as
equivalent KBs. We use the \textbf{knowledge boundary} model to determine the
optimal source using self-regulatory mechanisms such as confidence calibration
and reflective reasoning, and use the \textbf{depth solving} model to enhance
the comprehensiveness of knowledge acquisition. Finally, instead of utilizing
reinforcement learning, we employ supervised fine-tuning with multi-turn
dialogues to align the model with our structured inference paradigm, thereby
avoiding excessive reflection. This is supported by a data evaluation framework
and iterative corpus synthesis, which facilitate the generation of detailed
reasoning trajectories...

</details>


### [33] [HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations](https://arxiv.org/abs/2506.17748)
*Anwoy Chatterjee,Yash Goel,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: This paper proposes HIDE, a single-pass, training-free approach for hallucination detection in LMs, which outperforms existing single-pass methods and achieves competitive performance with multi-pass methods while consuming less computation time.


<details>
  <summary>Details</summary>
Motivation: Contemporary Language Models (LMs), while impressively fluent, often generate content that is factually incorrect or unfaithful to the input context - a critical issue commonly referred to as 'hallucination'. While several existing methods attempt to detect hallucinations, most rely on analyzing multiple generations per input, leading to increased computational cost and latency.

Method: a single-pass, training-free approach for effective Hallucination detectIon via Decoupled rEpresentations (HIDE). It leverages the hypothesis that hallucinations result from a statistical decoupling between an LM's internal representations of input context and its generated output. It quantifies this decoupling using the Hilbert-Schmidt Independence Criterion (HSIC) applied to hidden-state representations extracted while generating the output sequence.

Result: HIDE outperforms other single-pass methods in almost all settings, achieving an average relative improvement of ~29% in AUC-ROC over the best-performing single-pass strategy across various models and datasets.

Conclusion: HIDE shows competitive and often superior performance with multi-pass state-of-the-art methods, obtaining an average relative improvement of ~3% in AUC-ROC while consuming ~51% less computation time. Our findings highlight the effectiveness of exploiting internal representation decoupling in LMs for efficient and practical hallucination detection.

Abstract: Contemporary Language Models (LMs), while impressively fluent, often generate
content that is factually incorrect or unfaithful to the input context - a
critical issue commonly referred to as 'hallucination'. This tendency of LMs to
generate hallucinated content undermines their reliability, especially because
these fabrications are often highly convincing and therefore difficult to
detect. While several existing methods attempt to detect hallucinations, most
rely on analyzing multiple generations per input, leading to increased
computational cost and latency. To address this, we propose a single-pass,
training-free approach for effective Hallucination detectIon via Decoupled
rEpresentations (HIDE). Our approach leverages the hypothesis that
hallucinations result from a statistical decoupling between an LM's internal
representations of input context and its generated output. We quantify this
decoupling using the Hilbert-Schmidt Independence Criterion (HSIC) applied to
hidden-state representations extracted while generating the output sequence. We
conduct extensive experiments on four diverse question answering datasets,
evaluating both faithfulness and factuality hallucinations across six
open-source LMs of varying scales and properties. Our results demonstrate that
HIDE outperforms other single-pass methods in almost all settings, achieving an
average relative improvement of ~29% in AUC-ROC over the best-performing
single-pass strategy across various models and datasets. Additionally, HIDE
shows competitive and often superior performance with multi-pass
state-of-the-art methods, obtaining an average relative improvement of ~3% in
AUC-ROC while consuming ~51% less computation time. Our findings highlight the
effectiveness of exploiting internal representation decoupling in LMs for
efficient and practical hallucination detection.

</details>


### [34] [Multilingual Tokenization through the Lens of Indian Languages: Challenges and Insights](https://arxiv.org/abs/2506.17789)
*N J Karthika,Maharaj Brahma,Rohit Saluja,Ganesh Ramakrishnan,Maunendra Sankar Desarkar*

Main category: cs.CL

TL;DR: Evaluate tokenization strategies across 17 Indian languages and find that extremely low-resource languages can benefit from tokenizers trained on related high-resource languages.


<details>
  <summary>Details</summary>
Motivation: Existing tokenizers are often skewed towards high-resource languages, limiting their effectiveness for linguistically diverse and morphologically rich languages such as those in the Indian subcontinent.

Method: Comprehensive intrinsic evaluation of tokenization strategies across 17 Indian languages. Quantify the trade-offs between bottom-up and top-down tokenizer algorithms (BPE and Unigram LM), effects of vocabulary sizes, and compare strategies of multilingual vocabulary construction such as joint and cluster-based training.

Result: Quantify the trade-offs between bottom-up and top-down tokenizer algorithms (BPE and Unigram LM), effects of vocabulary sizes, and compare strategies of multilingual vocabulary construction such as joint and cluster-based training.Show that extremely low-resource languages can benefit from tokenizers trained on related high-resource languages.

Conclusion: Extremely low-resource languages can benefit from tokenizers trained on related high-resource languages. The study provides practical insights for building more fair, efficient, and linguistically informed tokenizers for multilingual NLP.

Abstract: Tokenization plays a pivotal role in multilingual NLP. However, existing
tokenizers are often skewed towards high-resource languages, limiting their
effectiveness for linguistically diverse and morphologically rich languages
such as those in the Indian subcontinent. This paper presents a comprehensive
intrinsic evaluation of tokenization strategies across 17 Indian languages. We
quantify the trade-offs between bottom-up and top-down tokenizer algorithms
(BPE and Unigram LM), effects of vocabulary sizes, and compare strategies of
multilingual vocabulary construction such as joint and cluster-based training.
We also show that extremely low-resource languages can benefit from tokenizers
trained on related high-resource languages. Our study provides practical
insights for building more fair, efficient, and linguistically informed
tokenizers for multilingual NLP.

</details>


### [35] [THCM-CAL: Temporal-Hierarchical Causal Modelling with Conformal Calibration for Clinical Risk Prediction](https://arxiv.org/abs/2506.17844)
*Xin Zhang,Qiyu Wei,Yingjie Zhu,Fanyi Wu,Sophia Ananiadou*

Main category: cs.CL

TL;DR: THCM-CAL是一种用于电子健康记录(EHRs)的自动临床风险预测框架，它构建了一个多模态因果图，通过分层因果发现，推断出临床上合理的相互作用，并通过共形预测增强预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 以往方法要么单独处理结构化诊断代码和非结构化叙述性注释，要么依赖于简单的融合策略，忽略了叙述性观察引发诊断并在入院期间传播风险的方向性、分层因果相互作用。

Method: Temporal-Hierarchical Causal Model with Conformal Calibration (THCM-CAL)

Result: THCM-CAL 推断出三个临床上合理的相互作用：片内同模态排序、片内跨模态触发和片间风险传播。

Conclusion: THCM-CAL在MIMIC-III和MIMIC-IV数据集上的实验结果表明其优越性。

Abstract: Automated clinical risk prediction from electronic health records (EHRs)
demands modeling both structured diagnostic codes and unstructured narrative
notes. However, most prior approaches either handle these modalities separately
or rely on simplistic fusion strategies that ignore the directional,
hierarchical causal interactions by which narrative observations precipitate
diagnoses and propagate risk across admissions. In this paper, we propose
THCM-CAL, a Temporal-Hierarchical Causal Model with Conformal Calibration. Our
framework constructs a multimodal causal graph where nodes represent clinical
entities from two modalities: Textual propositions extracted from notes and ICD
codes mapped to textual descriptions. Through hierarchical causal discovery,
THCM-CAL infers three clinically grounded interactions: intra-slice
same-modality sequencing, intra-slice cross-modality triggers, and inter-slice
risk propagation. To enhance prediction reliability, we extend conformal
prediction to multi-label ICD coding, calibrating per-code confidence intervals
under complex co-occurrences. Experimental results on MIMIC-III and MIMIC-IV
demonstrate the superiority of THCM-CAL.

</details>


### [36] [LLMs for Customized Marketing Content Generation and Evaluation at Scale](https://arxiv.org/abs/2506.17863)
*Haoran Liu,Amir Tahmasbi,Ehtesham Sam Haque,Purak Jain*

Main category: cs.CL

TL;DR: This paper introduces MarketingFM, AutoEval-Main, and AutoEval-Update to generate keyword-specific ad copy, automate ad evaluation, and refine evaluation prompts, achieving improved ad performance and evaluation consistency.


<details>
  <summary>Details</summary>
Motivation: most current offsite marketing content is overly generic, template-based, and poorly aligned with landing pages, limiting its effectiveness

Method: MarketingFM, a retrieval-augmented system that integrates multiple data sources to generate keyword-specific ad copy with minimal human intervention; AutoEval-Main, an automated evaluation system that combines rule-based metrics with LLM-as-a-Judge techniques; AutoEval-Update, a cost-efficient LLM-human collaborative framework to dynamically refine evaluation prompts

Result: keyword-focused ad copy outperformed templates, achieving up to 9% higher CTR, 12% more impressions, and 0.38% lower CPC; AutoEval-Main achieved 89.57% agreement with human reviewers; the critic LLM suggests meaningful refinements, improving LLM-human agreement

Conclusion: human oversight remains essential for setting thresholds and validating refinements before deployment

Abstract: Offsite marketing is essential in e-commerce, enabling businesses to reach
customers through external platforms and drive traffic to retail websites.
However, most current offsite marketing content is overly generic,
template-based, and poorly aligned with landing pages, limiting its
effectiveness. To address these limitations, we propose MarketingFM, a
retrieval-augmented system that integrates multiple data sources to generate
keyword-specific ad copy with minimal human intervention. We validate
MarketingFM via offline human and automated evaluations and large-scale online
A/B tests. In one experiment, keyword-focused ad copy outperformed templates,
achieving up to 9% higher CTR, 12% more impressions, and 0.38% lower CPC,
demonstrating gains in ad ranking and cost efficiency. Despite these gains,
human review of generated ads remains costly. To address this, we propose
AutoEval-Main, an automated evaluation system that combines rule-based metrics
with LLM-as-a-Judge techniques to ensure alignment with marketing principles.
In experiments with large-scale human annotations, AutoEval-Main achieved
89.57% agreement with human reviewers. Building on this, we propose
AutoEval-Update, a cost-efficient LLM-human collaborative framework to
dynamically refine evaluation prompts and adapt to shifting criteria with
minimal human input. By selectively sampling representative ads for human
review and using a critic LLM to generate alignment reports, AutoEval-Update
improves evaluation consistency while reducing manual effort. Experiments show
the critic LLM suggests meaningful refinements, improving LLM-human agreement.
Nonetheless, human oversight remains essential for setting thresholds and
validating refinements before deployment.

</details>


### [37] [QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs](https://arxiv.org/abs/2506.17864)
*Taolin Zhang,Haidong Kang,Dongyang Li,Qizhou Chen,Chengyu Wang Xiaofeng He,Richang Hong*

Main category: cs.CL

TL;DR: QueueEDIT improves sequential model editing (SME) for LLMs by using a queue to manage parameter updates, enhancing performance and preserving general capabilities.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) suffer from hallucinations, and sequential model editing (SME) can negatively affect the general capabilities of LLMs due to the introduction of new parameters.

Method: A queue-based self-correction framework (QueueEDIT) is proposed, which introduces a structural mapping editing loss to map triplets to knowledge-sensitive neurons and dynamically aligns previously edited parameters using a queue.

Result: The QueueEDIT framework enhances SME performance by addressing long-sequence dependency and mitigates the impact of parameter bias on the general capabilities of LLMs.

Conclusion: The proposed QueueEDIT framework significantly outperforms strong baselines across various SME settings, maintains competitiveness in single-turn editing, and preserves high capabilities in general NLP tasks throughout the SME process.

Abstract: Recently, large language models (LLMs) have demonstrated impressive results
but still suffer from hallucinations. Model editing has been proposed to
correct factual inaccuracies in LLMs. A challenging case is sequential model
editing (SME), which aims to rectify errors continuously rather than treating
them as a one-time task. During SME, the general capabilities of LLMs can be
negatively affected due to the introduction of new parameters. In this paper,
we propose a queue-based self-correction framework (QueueEDIT) that not only
enhances SME performance by addressing long-sequence dependency but also
mitigates the impact of parameter bias on the general capabilities of LLMs.
Specifically, we first introduce a structural mapping editing loss to map the
triplets to the knowledge-sensitive neurons within the Transformer layers of
LLMs. We then store the located parameters for each piece of edited knowledge
in a queue and dynamically align previously edited parameters. In each edit, we
select queue parameters most relevant to the currently located parameters to
determine whether previous knowledge needs realignment. Irrelevant parameters
in the queue are frozen, and we update the parameters at the queue head to the
LLM to ensure they do not harm general abilities. Experiments show that our
framework significantly outperforms strong baselines across various SME
settings and maintains competitiveness in single-turn editing. The resulting
LLMs also preserve high capabilities in general NLP tasks throughout the SME
process.

</details>


### [38] [How Alignment Shrinks the Generative Horizon](https://arxiv.org/abs/2506.17871)
*Chenghao Yang,Ari Holtzman*

Main category: cs.CL

TL;DR: alignment reduces variability and CoT promotes stable generations


<details>
  <summary>Details</summary>
Motivation: aligned large language models (LLMs) often generate outputs that lack diversity. What drives this stability in the generation?

Method: introduce the Branching Factor (BF) -- a token-invariant measure of the effective number of plausible next steps during generation

Result: BF often decreases as generation progresses, suggesting that LLMs become more predictable as they generate. alignment tuning substantially sharpens the model's output distribution from the outset, reducing BF by nearly an order of magnitude

Conclusion: alignment reduces variability, how CoT promotes stable generations, and how base models can be steered away from diversity

Abstract: Despite their impressive capabilities, aligned large language models (LLMs)
often generate outputs that lack diversity. What drives this stability in the
generation? We investigate this phenomenon through the lens of probability
concentration in the model's output distribution. To quantify this
concentration, we introduce the Branching Factor (BF) -- a token-invariant
measure of the effective number of plausible next steps during generation. Our
empirical analysis reveals two key findings: (1) BF often decreases as
generation progresses, suggesting that LLMs become more predictable as they
generate. (2) alignment tuning substantially sharpens the model's output
distribution from the outset, reducing BF by nearly an order of magnitude
(e.g., from 12 to 1.2) relative to base models. This stark reduction helps
explain why aligned models often appear less sensitive to decoding strategies.
Building on this insight, we find this stability has surprising implications
for complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g.,
DeepSeek-distilled models), for instance, leverage this effect; by generating
longer reasoning chains, they push generation into later, more deterministic
(lower BF) stages, resulting in more stable outputs. We hypothesize that
alignment tuning does not fundamentally change a model's behavior, but instead
steers it toward stylistic tokens (e.g., "Sure") that unlock low-entropy
trajectories already present in the base model. This view is supported by
nudging experiments, which show that prompting base models with such tokens can
similarly reduce BF. Together, our findings establish BF as a powerful
diagnostic for understanding and controlling LLM outputs - clarifying how
alignment reduces variability, how CoT promotes stable generations, and how
base models can be steered away from diversity.

</details>


### [39] [Multi-turn Jailbreaking via Global Refinement and Active Fabrication](https://arxiv.org/abs/2506.17881)
*Hua Tang,Lingyong Yan,Yukun Zhao,Shuaiqiang Wang,Jizhou Huang,Dawei Yin*

Main category: cs.CL

TL;DR: This paper presents a new multi-turn jailbreaking method for LLMs that improves upon existing techniques by globally refining the jailbreaking path and actively fabricating model responses.


<details>
  <summary>Details</summary>
Motivation: Existing multi-turn jailbreaking techniques struggle to adapt to the evolving dynamics of dialogue as the interaction progresses.

Method: The proposed method refines the jailbreaking path globally at each interaction and actively fabricates model responses to suppress safety-related warnings.

Result: The proposed method achieves superior performance compared with existing single-turn and multi-turn jailbreaking techniques across six state-of-the-art LLMs.

Conclusion: This paper introduces a novel multi-turn jailbreaking method that refines the jailbreaking path globally at each interaction and actively fabricates model responses to suppress safety-related warnings.

Abstract: Large Language Models (LLMs) have achieved exceptional performance across a
wide range of tasks. However, they still pose significant safety risks due to
the potential misuse for malicious purposes. Jailbreaks, which aim to elicit
models to generate harmful content, play a critical role in identifying the
underlying security threats. Recent jailbreaking primarily focuses on
single-turn scenarios, while the more complicated multi-turn scenarios remain
underexplored. Moreover, existing multi-turn jailbreaking techniques struggle
to adapt to the evolving dynamics of dialogue as the interaction progresses. To
address this limitation, we propose a novel multi-turn jailbreaking method that
refines the jailbreaking path globally at each interaction. We also actively
fabricate model responses to suppress safety-related warnings, thereby
increasing the likelihood of eliciting harmful outputs in subsequent questions.
Experimental results demonstrate the superior performance of our method
compared with existing single-turn and multi-turn jailbreaking techniques
across six state-of-the-art LLMs. Our code is publicly available at
https://github.com/Ytang520/Multi-Turn_jailbreaking_Global-Refinment_and_Active-Fabrication.

</details>


### [40] [Scatter-Based Innovation Propagation in Large Language Models for Multi-Stage Process Adaptation](https://arxiv.org/abs/2506.17949)
*Hong Su*

Main category: cs.CL

TL;DR: This paper introduces an innovation scatter model that helps LLMs generalize and reuse innovations across different stages of a multi-stage process by identifying, generalizing, and applying innovations to structurally similar stages.


<details>
  <summary>Details</summary>
Motivation: LLMs often struggle to generalize novel ideas beyond their original context. This paper addresses the challenge of applying such localized innovations - introduced at a specific stage or component - to other parts of a multi-stage process.

Method: propose a scatter-based innovation expansion model (innovation scatter model) that guides the LLM through a four-step process: (1) identifying the core innovation by comparing the user's input with its surrounding context, (2) generalizing the innovation by removing references to specific stages or components, (3) determining whether the generalized innovation applies to a broader scope beyond the original stage, and (4) systematically applying it to other structurally similar stages using the LLM

Result: Verification results demonstrate that the innovation scatter model enables LLMs to extend innovations across structurally similar stages, thereby enhancing generalization and reuse.

Conclusion: the innovation scatter model enables LLMs to extend innovations across structurally similar stages, thereby enhancing generalization and reuse

Abstract: Large Language Models (LLMs) exhibit strong capabilities in reproducing and
extending patterns observed during pretraining but often struggle to generalize
novel ideas beyond their original context. This paper addresses the challenge
of applying such localized innovations - introduced at a specific stage or
component - to other parts of a multi-stage process. We propose a scatter-based
innovation expansion model (innovation scatter model) that guides the LLM
through a four-step process: (1) identifying the core innovation by comparing
the user's input with its surrounding context, (2) generalizing the innovation
by removing references to specific stages or components, (3) determining
whether the generalized innovation applies to a broader scope beyond the
original stage, and (4) systematically applying it to other structurally
similar stages using the LLM. This model leverages structural redundancy across
stages to improve the applicability of novel ideas. Verification results
demonstrate that the innovation scatter model enables LLMs to extend
innovations across structurally similar stages, thereby enhancing
generalization and reuse.

</details>


### [41] [A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment](https://arxiv.org/abs/2506.17951)
*Quanwei Tang,Sophia Yat Mei Lee,Junshuang Wu,Dong Zhang,Shoushan Li,Erik Cambria,Guodong Zhou*

Main category: cs.CL

TL;DR: GraphMPA, a graph-based framework with mode-seeking preference alignment, enhances RAG by improving global understanding and aligning responses with human preferences.


<details>
  <summary>Details</summary>
Motivation: Challenges persist in achieving global understanding and aligning responses with human ethical and quality preferences in RAG.

Method: We propose GraphMPA, a comprehensive graph-based framework with mode-seeking preference alignment. Our approach constructs a hierarchical document graph using a general similarity measurement, mimicking human cognitive processes for information understanding and synthesis. Additionally, we introduce mode-seeking preference optimization to better align model outputs with human preferences through probability-matching constraints.

Result: The effectiveness of GraphMPA is demonstrated on six datasets.

Conclusion: Extensive experiments on six datasets demonstrate the effectiveness of GraphMPA.

Abstract: Recent advancements in retrieval-augmented generation (RAG) have enhanced
large language models in question answering by integrating external knowledge.
However, challenges persist in achieving global understanding and aligning
responses with human ethical and quality preferences. To address these issues,
we propose GraphMPA, a comprehensive graph-based framework with mode-seeking
preference alignment. Our approach constructs a hierarchical document graph
using a general similarity measurement, mimicking human cognitive processes for
information understanding and synthesis. Additionally, we introduce
mode-seeking preference optimization to better align model outputs with human
preferences through probability-matching constraints. Extensive experiments on
six datasets demonstrate the effectiveness of our
\href{https://github.com/tangquanwei/GraphMPA}{GraphMPA}.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [42] [Mechanistic Interpretability of Diffusion Models: Circuit-Level Analysis and Causal Validation](https://arxiv.org/abs/2506.17237)
*Dip Roy*

Main category: cs.CV

TL;DR: 通过电路级分析和干预实验，揭示了扩散模型处理合成和自然图像的算法差异，并识别了关键的注意力机制和计算瓶颈。


<details>
  <summary>Details</summary>
Motivation: 提出扩散模型的定量电路级分析，建立图像生成过程的计算路径和机制原理。

Method: 通过对2,000个合成图像和2,000个CelebA面部图像的系统干预实验，进行定量电路级分析。

Result: 揭示了真实面部处理需要具有可测量更高计算复杂度的电路（复杂度比=1.084 +/- 0.008，p < 0.001），表现出不同的注意力专业化模式，去噪时间步长的熵散度范围为0.015至0.166。确定了八种功能不同的注意力机制，显示出专门的计算作用：边缘检测（熵=3.18 +/- 0.12），纹理分析（熵=4.16 +/- 0.08）和语义理解（熵=2.67 +/- 0.15）。干预分析表明，有针对性的消融产生25.6％至128.3％的性能下降的关键计算瓶颈，为已识别的电路功能提供因果证据。

Conclusion: 建立了生成模型行为的算法理解和控制的定量基础，通过机械干预策略。

Abstract: We present a quantitative circuit-level analysis of diffusion models,
establishing computational pathways and mechanistic principles underlying image
generation processes. Through systematic intervention experiments across 2,000
synthetic and 2,000 CelebA facial images, we discover fundamental algorithmic
differences in how diffusion architectures process synthetic versus
naturalistic data distributions. Our investigation reveals that real-world face
processing requires circuits with measurably higher computational complexity
(complexity ratio = 1.084 plus/minus 0.008, p < 0.001), exhibiting distinct
attention specialization patterns with entropy divergence ranging from 0.015 to
0.166 across denoising timesteps. We identify eight functionally distinct
attention mechanisms showing specialized computational roles: edge detection
(entropy = 3.18 plus/minus 0.12), texture analysis (entropy = 4.16 plus/minus
0.08), and semantic understanding (entropy = 2.67 plus/minus 0.15).
Intervention analysis demonstrates critical computational bottlenecks where
targeted ablations produce 25.6% to 128.3% performance degradation, providing
causal evidence for identified circuit functions. These findings establish
quantitative foundations for algorithmic understanding and control of
generative model behavior through mechanistic intervention strategies.

</details>


### [43] [SRKD: Towards Efficient 3D Point Cloud Segmentation via Structure- and Relation-aware Knowledge Distillation](https://arxiv.org/abs/2506.17290)
*Yuqi Li,Junhao Dong,Zeyu Dong,Chuanguang Yang,Zhulin An,Yongjun Xu*

Main category: cs.CV

TL;DR: Proposes SRKD, a knowledge distillation framework, to transfer knowledge from a large teacher model to a lightweight student model for 3D point cloud segmentation, achieving state-of-the-art performance with reduced complexity.


<details>
  <summary>Details</summary>
Motivation: 3D point cloud segmentation faces practical challenges due to the computational complexity and deployment limitations of large-scale transformer-based models. To address this,

Method: We propose a novel Structure- and Relation-aware Knowledge Distillation framework, named SRKD, that transfers rich geometric and semantic knowledge from a large frozen teacher model (>100M) to a lightweight student model (<15M). Specifically, we propose an affinity matrix-based relation alignment module, which distills structural dependencies from the teacher to the student through point-wise similarity matching, enhancing the student's capability to learn contextual interactions. Meanwhile, we introduce a cross-sample mini-batch construction strategy that enables the student to perceive stable and generalized geometric structure.

Result: achieves state of the art performance with significantly reduced model complexity

Conclusion: This method achieves state of the art performance with significantly reduced model complexity, demonstrating its effectiveness and efficiency in real-world deployment scenarios.

Abstract: 3D point cloud segmentation faces practical challenges due to the
computational complexity and deployment limitations of large-scale
transformer-based models. To address this, we propose a novel Structure- and
Relation-aware Knowledge Distillation framework, named SRKD, that transfers
rich geometric and semantic knowledge from a large frozen teacher model (>100M)
to a lightweight student model (<15M). Specifically, we propose an affinity
matrix-based relation alignment module, which distills structural dependencies
from the teacher to the student through point-wise similarity matching,
enhancing the student's capability to learn contextual interactions. Meanwhile,
we introduce a cross-sample mini-batch construction strategy that enables the
student to perceive stable and generalized geometric structure. This aligns
across diverse point cloud instances of the teacher, rather than within a
single sample. Additionally, KL divergence is applied to align semantic
distributions, and ground-truth supervision further reinforces accurate
segmentation. Our method achieves state of the art performance with
significantly reduced model complexity, demonstrating its effectiveness and
efficiency in real-world deployment scenarios. Our Code is available at
https://github.com/itsnotacie/SRKD.

</details>


### [44] [Fine-Scale Soil Mapping in Alaska with Multimodal Machine Learning](https://arxiv.org/abs/2506.17302)
*Yijun Lin,Theresa Chen,Colby Brungard,Grunwald Sabine,Sue Ives,Matt Macander,Timm Nawrocki,Yao-Yi Chiang,Nic Jelinski*

Main category: cs.CV

TL;DR: 提出了一种新的机器学习模型MISO，用于生成高分辨率土壤地图，该模型在永久冻土地区域的泛化性和召回率上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 由于气候变化，永久冻土融化加速，威胁到基础设施的稳定性和关键的生态系统服务，如土壤碳储存。高分辨率的土壤地图对于描述永久冻土的分布、识别脆弱区域和为适应策略提供信息至关重要。

Method: 提出了一种基于视觉的机器学习模型MISO，用于生成全州范围的近地表永久冻土和土壤分类的细尺度土壤地图。该模型集成了用于视觉特征提取的地理空间基础模型、用于连续空间预测的隐式神经表示以及用于多模态对齐和地理位置感知的对比学习。

Result: 空间交叉验证和跨永久冻土带和主要土地资源区 (MLRA) 的区域分析表明，MISO 比 RF 更好地推广到偏远、未见过的位置，并且实现了更高的召回率。

Conclusion: MISO模型在泛化性和召回率上优于传统机器学习模型RF，为细尺度土壤 mapping 提供了潜力，并为未来的土壤采样和基础设施规划提供了实践指导。

Abstract: Fine-scale soil mapping in Alaska, traditionally relying on fieldwork and
localized simulations, remains a critical yet underdeveloped task, despite the
region's ecological importance and extensive permafrost coverage. As permafrost
thaw accelerates due to climate change, it threatens infrastructure stability
and key ecosystem services, such as soil carbon storage. High-resolution soil
maps are essential for characterizing permafrost distribution, identifying
vulnerable areas, and informing adaptation strategies. We present MISO, a
vision-based machine learning (ML) model to produce statewide fine-scale soil
maps for near-surface permafrost and soil taxonomy. The model integrates a
geospatial foundation model for visual feature extraction, implicit neural
representations for continuous spatial prediction, and contrastive learning for
multimodal alignment and geo-location awareness. We compare MISO with Random
Forest (RF), a traditional ML model that has been widely used in soil mapping
applications. Spatial cross-validation and regional analysis across Permafrost
Zones and Major Land Resource Areas (MLRAs) show that MISO generalizes better
to remote, unseen locations and achieves higher recall than RF, which is
critical for monitoring permafrost thaw and related environmental processes.
These findings demonstrate the potential of advanced ML approaches for
fine-scale soil mapping and provide practical guidance for future soil sampling
and infrastructure planning in permafrost-affected landscapes. The project will
be released at https://github.com/knowledge-computing/Peatland-permafrost.

</details>


### [45] [RadarSeq: A Temporal Vision Framework for User Churn Prediction via Radar Chart Sequences](https://arxiv.org/abs/2506.17325)
*Sina Najafi,M. Hadi Sepanj,Fahimeh Jafari*

Main category: cs.CV

TL;DR: 提出了一种时间感知计算机视觉框架，该框架优于经典模型和 ViT 基线，在非订阅零工平台中实现了大规模客户流失建模。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏明确的标签以及用户行为的动态性，在非订阅零工平台中预测用户流失（其中脱离是隐含的）提出了独特的挑战。现有方法通常依赖于聚合快照或静态视觉表示，这掩盖了对早期检测至关重要的时间线索。

Method: 将用户行为模式建模为雷达图图像序列的时间感知计算机视觉框架，每个雷达图都编码了每日级别的行为特征。通过将预训练的 CNN 编码器与双向 LSTM 集成，该架构捕获了客户流失行为下的空间和时间模式。

Result: 在大型真实世界数据集上的大量实验表明，该方法优于经典模型和基于 ViT 的雷达图基线，在 F1 分数上提高了 17.7，在精确度上提高了 29.4，在 AUC 上提高了 16.1，同时提高了可解释性。

Conclusion: 该框架的模块化设计、可解释性工具和高效的部署特性使其适用于动态零工经济平台中的大规模客户流失建模。

Abstract: Predicting user churn in non-subscription gig platforms, where disengagement
is implicit, poses unique challenges due to the absence of explicit labels and
the dynamic nature of user behavior. Existing methods often rely on aggregated
snapshots or static visual representations, which obscure temporal cues
critical for early detection. In this work, we propose a temporally-aware
computer vision framework that models user behavioral patterns as a sequence of
radar chart images, each encoding day-level behavioral features. By integrating
a pretrained CNN encoder with a bidirectional LSTM, our architecture captures
both spatial and temporal patterns underlying churn behavior. Extensive
experiments on a large real-world dataset demonstrate that our method
outperforms classical models and ViT-based radar chart baselines, yielding
gains of 17.7 in F1 score, 29.4 in precision, and 16.1 in AUC, along with
improved interpretability. The framework's modular design, explainability
tools, and efficient deployment characteristics make it suitable for
large-scale churn modeling in dynamic gig-economy platforms.

</details>


### [46] [P2MFDS: A Privacy-Preserving Multimodal Fall Detection System for Elderly People in Bathroom Environments](https://arxiv.org/abs/2506.17332)
*Haitian Wang,Yiren Wang,Xinyu Wang,Yumeng Miao,Yuliang Zhang,Yu Zhang,Atif Mansoor*

Main category: cs.CV

TL;DR: Developed P2MFDS, a privacy-preserving multimodal fall detection system for elderly people in bathroom environments, which unites macro- and micro-scale features to improve accuracy and recall.


<details>
  <summary>Details</summary>
Motivation: Aging is closely associated with increased fall risk, particularly in wet and confined environments such as bathrooms where over 80 percent of falls occur. Although recent research has increasingly focused on non-intrusive, privacy-preserving approaches that do not rely on wearable devices or video-based monitoring, these efforts have not fully overcome the limitations of existing unimodal systems

Method: a dual-stream network combining a CNN-BiLSTM-Attention branch for radar motion dynamics with a multi-scale CNN-SEBlock-Self-Attention branch for vibration impact detection

Result: we propose a Privacy-Preserving Multimodal Fall Detection System for Elderly People in Bathroom Environments.  First, we develop a sensor evaluation framework to select and fuse millimeter-wave radar with 3D vibration sensing, and use it to construct and preprocess a large-scale, privacy-preserving multimodal dataset in real bathroom settings, which will be released upon publication. Second, we introduce P2MFDS

Conclusion: P2MFDS delivers significant gains in accuracy and recall over state-of-the-art approaches.

Abstract: By 2050, people aged 65 and over are projected to make up 16 percent of the
global population. As aging is closely associated with increased fall risk,
particularly in wet and confined environments such as bathrooms where over 80
percent of falls occur. Although recent research has increasingly focused on
non-intrusive, privacy-preserving approaches that do not rely on wearable
devices or video-based monitoring, these efforts have not fully overcome the
limitations of existing unimodal systems (e.g., WiFi-, infrared-, or
mmWave-based), which are prone to reduced accuracy in complex environments.
These limitations stem from fundamental constraints in unimodal sensing,
including system bias and environmental interference, such as multipath fading
in WiFi-based systems and drastic temperature changes in infrared-based
methods. To address these challenges, we propose a Privacy-Preserving
Multimodal Fall Detection System for Elderly People in Bathroom Environments.
First, we develop a sensor evaluation framework to select and fuse
millimeter-wave radar with 3D vibration sensing, and use it to construct and
preprocess a large-scale, privacy-preserving multimodal dataset in real
bathroom settings, which will be released upon publication. Second, we
introduce P2MFDS, a dual-stream network combining a CNN-BiLSTM-Attention branch
for radar motion dynamics with a multi-scale CNN-SEBlock-Self-Attention branch
for vibration impact detection. By uniting macro- and micro-scale features,
P2MFDS delivers significant gains in accuracy and recall over state-of-the-art
approaches. Code and pretrained models will be made available at:
https://github.com/HaitianWang/P2MFDS-A-Privacy-Preserving-Multimodal-Fall-Detection-Network-for-Elderly-Individuals-in-Bathroom.

</details>


### [47] [A Novel Multi-layer Task-centric and Data Quality Framework for Autonomous Driving](https://arxiv.org/abs/2506.17346)
*Yuhan Zhou,Haihua Chen,Kewei Sha*

Main category: cs.CV

TL;DR: This paper introduces a task-centric data quality framework for autonomous vehicles, demonstrating that addressing data redundancy can improve object detection performance.


<details>
  <summary>Details</summary>
Motivation: Next-generation autonomous vehicles (AVs) will rely heavily on a large volume of multisource and multimodal data. In real-world settings, the data quality (DQ) of different sources and modalities usually varies. However, both researchers and practitioners in the AV field overwhelmingly concentrate on models/algorithms while undervaluing the DQ.

Method: This paper proposes a novel task-centric and data quality vase framework which consists of five layers: data layer, DQ layer, task layer, application layer, and goal layer. The proposed framework aims to map DQ with task requirements and performance goals.

Result: A case study investigating redundancy on the nuScenes dataset proves that partially removing redundancy on multisource image data could improve YOLOv8 object detection task performance. Analysis on multimodal data of image and LiDAR further presents existing redundancy DQ issues.

Conclusion: This paper opens up a range of critical but unexplored challenges at the intersection of DQ, task orchestration, and performance-oriented system development in AVs. It is expected to guide the AV community toward building more adaptive, explainable, and resilient AVs that respond intelligently to dynamic environments and heterogeneous data streams.

Abstract: The next-generation autonomous vehicles (AVs), embedded with frequent
real-time decision-making, will rely heavily on a large volume of multisource
and multimodal data. In real-world settings, the data quality (DQ) of different
sources and modalities usually varies due to unexpected environmental factors
or sensor issues. However, both researchers and practitioners in the AV field
overwhelmingly concentrate on models/algorithms while undervaluing the DQ. To
fulfill the needs of the next-generation AVs with guarantees of functionality,
efficiency, and trustworthiness, this paper proposes a novel task-centric and
data quality vase framework which consists of five layers: data layer, DQ
layer, task layer, application layer, and goal layer. The proposed framework
aims to map DQ with task requirements and performance goals. To illustrate, a
case study investigating redundancy on the nuScenes dataset proves that
partially removing redundancy on multisource image data could improve YOLOv8
object detection task performance. Analysis on multimodal data of image and
LiDAR further presents existing redundancy DQ issues. This paper opens up a
range of critical but unexplored challenges at the intersection of DQ, task
orchestration, and performance-oriented system development in AVs. It is
expected to guide the AV community toward building more adaptive, explainable,
and resilient AVs that respond intelligently to dynamic environments and
heterogeneous data streams. Code, data, and implementation details are publicly
available at: https://anonymous.4open.science/r/dq4av-framework/README.md.

</details>


### [48] [Efficient Feedback Gate Network for Hyperspectral Image Super-Resolution](https://arxiv.org/abs/2506.17361)
*Xufei Wang,Mingjian Zhang,Fei Ge,Jinchen Zhu,Wen Sha,Jifen Ren,Zhimeng Hou,Shouguo Zheng,ling Zheng,Shizhuang Weng*

Main category: cs.CV

TL;DR: This paper proposes a novel group-based SHSR method termed the efficient feedback gate network, which uses various feedbacks and gate operations involving large kernel convolutions and spectral interactions to improve the spatial resolution of hyperspectral images.


<details>
  <summary>Details</summary>
Motivation: single hyperspectral image super-resolution (SHSR) methods can be designed to improve the spatial resolution of hyperspectral images. However, failing to explore coherence thoroughly along bands and spatial-spectral information leads to the limited performance of the SHSR.

Method: a novel group-based SHSR method termed the efficient feedback gate network, which uses various feedbacks and gate operations involving large kernel convolutions and spectral interactions. In particular, by providing different guidance for neighboring groups, we can learn rich band information and hierarchical hyperspectral spatial information using channel shuffling and dilatation convolution in shuffled and progressive dilated fusion module(SPDFM). Moreover, we develop a wide-bound perception gate block and a spectrum enhancement gate block to construct the spatial-spectral reinforcement gate module (SSRGM) and obtain highly representative spatial-spectral features efficiently. Additionally, we apply a three-dimensional SSRGM to enhance holistic information and coherence for hyperspectral data.

Result: the superior performance of the proposed network over the state-of-the-art methods in terms of spectral fidelity and spatial content reconstruction.

Conclusion: The experimental results on three hyperspectral datasets demonstrate the superior performance of the proposed network over the state-of-the-art methods in terms of spectral fidelity and spatial content reconstruction.

Abstract: Even without auxiliary images, single hyperspectral image super-resolution
(SHSR) methods can be designed to improve the spatial resolution of
hyperspectral images. However, failing to explore coherence thoroughly along
bands and spatial-spectral information leads to the limited performance of the
SHSR. In this study, we propose a novel group-based SHSR method termed the
efficient feedback gate network, which uses various feedbacks and gate
operations involving large kernel convolutions and spectral interactions. In
particular, by providing different guidance for neighboring groups, we can
learn rich band information and hierarchical hyperspectral spatial information
using channel shuffling and dilatation convolution in shuffled and progressive
dilated fusion module(SPDFM). Moreover, we develop a wide-bound perception gate
block and a spectrum enhancement gate block to construct the spatial-spectral
reinforcement gate module (SSRGM) and obtain highly representative
spatial-spectral features efficiently. Additionally, we apply a
three-dimensional SSRGM to enhance holistic information and coherence for
hyperspectral data. The experimental results on three hyperspectral datasets
demonstrate the superior performance of the proposed network over the
state-of-the-art methods in terms of spectral fidelity and spatial content
reconstruction.

</details>


### [49] [From Drawings to Decisions: A Hybrid Vision-Language Framework for Parsing 2D Engineering Drawings into Structured Manufacturing Knowledge](https://arxiv.org/abs/2506.17374)
*Muhammad Tayyab Khan,Lequn Chen,Zane Yong,Jun Ming Tan,Wenhe Feng,Seung Ki Moon*

Main category: cs.CV

TL;DR: This paper introduces a hybrid vision-language framework for extracting key information from 2D engineering drawings. It uses YOLOv11-OBB for object detection and Donut/Florence-2 for parsing, with Donut showing superior performance.


<details>
  <summary>Details</summary>
Motivation: Manual extraction is slow and labor-intensive, while generic OCR models often fail due to complex layouts, engineering symbols, and rotated text, leading to incomplete and unreliable outputs.

Method: a hybrid vision-language framework that integrates a rotation-aware object detection model (YOLOv11-obb) with a transformer-based vision-language parser

Result: YOLOv11-OBB is trained on this dataset to detect OBBs and extract annotation patches. These are parsed using two open-source VLMs: Donut and Florence-2.

Conclusion: Donut outperforms Florence-2, achieving 88.5% precision, 99.2% recall, and a 93.5% F1-score, with a hallucination rate of 11.5%.

Abstract: Efficient and accurate extraction of key information from 2D engineering
drawings is essential for advancing digital manufacturing workflows. Such
information includes geometric dimensioning and tolerancing (GD&T), measures,
material specifications, and textual annotations. Manual extraction is slow and
labor-intensive, while generic OCR models often fail due to complex layouts,
engineering symbols, and rotated text, leading to incomplete and unreliable
outputs. These limitations result in incomplete and unreliable outputs. To
address these challenges, we propose a hybrid vision-language framework that
integrates a rotation-aware object detection model (YOLOv11-obb) with a
transformer-based vision-language parser. Our structured pipeline applies
YOLOv11-OBB to localize annotations and extract oriented bounding box (OBB)
patches, which are then parsed into structured outputs using a fine-tuned,
lightweight vision-language model (VLM). We curate a dataset of 1,367 2D
mechanical drawings annotated across nine key categories. YOLOv11-OBB is
trained on this dataset to detect OBBs and extract annotation patches. These
are parsed using two open-source VLMs: Donut and Florence-2. Both models are
lightweight and well-suited for specialized industrial tasks under limited
computational overhead. Following fine-tuning of both models on the curated
dataset of image patches paired with structured annotation labels, a
comparative experiment is conducted to evaluate parsing performance across four
key metrics. Donut outperforms Florence-2, achieving 88.5% precision, 99.2%
recall, and a 93.5% F1-score, with a hallucination rate of 11.5%. Finally, a
case study demonstrates how the extracted structured information supports
downstream manufacturing tasks such as process and tool selection, showcasing
the practical utility of the proposed framework in modernizing 2D drawing
interpretation.

</details>


### [50] [Spatial-Temporal Pre-Training for Embryo Viability Prediction Using Time-Lapse Videos](https://arxiv.org/abs/2506.17403)
*Zhiyi Shi,Junsik Kim,Helen Y. Yang,Yonghyun Song,Hyun-Jic Oh,Dalit Ben-Yosef,Daniel Needleman,Hanspeter Pfister*

Main category: cs.CV

TL;DR: This paper introduces Spatial-Temporal Pre-Training (STPT) to predict embryo viability from time-lapse videos, achieving state-of-the-art performance with limited resources by addressing challenges in long video processing and temporal misalignment.


<details>
  <summary>Details</summary>
Motivation: Automating embryo viability prediction for in vitro fertilization (IVF) is important but challenging due to the limited availability of labeled pregnancy outcome data, as only a small fraction of embryos are labeled after transfer. Self-supervised learning (SSL) can leverage both labeled and unlabeled data to improve prediction. However, existing SSL methods for videos are not directly applicable to embryo development videos due to two challenges: (1) embryo time-lapse videos contain hundreds of frames, requiring significant GPU memory for conventional SSL; (2) the dataset contains videos with varying lengths and many outlier frames, causing traditional video alignment methods to struggle with semantic misalignment.

Method: We propose Spatial-Temporal Pre-Training (STPT) to address these challenges. STPT includes two stages: spatial and temporal. In each stage, only one encoder is trained while the other is frozen, reducing memory demands. To handle temporal misalignment, STPT avoids frame-by-frame alignment across videos. The spatial stage learns from alignments within each video and its temporally consistent augmentations. The temporal stage then models relationships between video embeddings.

Result: On 23,027 time-lapse videos (3,286 labeled), STPT achieves the highest AUC of 0.635 (95% CI: 0.632-0.638) compared to baselines, with limited computational resources.

Conclusion: STPT achieves the highest AUC of 0.635 (95% CI: 0.632-0.638) compared to baselines, with limited computational resources.

Abstract: Automating embryo viability prediction for in vitro fertilization (IVF) is
important but challenging due to the limited availability of labeled pregnancy
outcome data, as only a small fraction of embryos are labeled after transfer.
Self-supervised learning (SSL) can leverage both labeled and unlabeled data to
improve prediction. However, existing SSL methods for videos are not directly
applicable to embryo development videos due to two challenges: (1) embryo
time-lapse videos contain hundreds of frames, requiring significant GPU memory
for conventional SSL; (2) the dataset contains videos with varying lengths and
many outlier frames, causing traditional video alignment methods to struggle
with semantic misalignment. We propose Spatial-Temporal Pre-Training (STPT) to
address these challenges. STPT includes two stages: spatial and temporal. In
each stage, only one encoder is trained while the other is frozen, reducing
memory demands. To handle temporal misalignment, STPT avoids frame-by-frame
alignment across videos. The spatial stage learns from alignments within each
video and its temporally consistent augmentations. The temporal stage then
models relationships between video embeddings. Our method efficiently handles
long videos and temporal variability. On 23,027 time-lapse videos (3,286
labeled), STPT achieves the highest AUC of 0.635 (95% CI: 0.632-0.638) compared
to baselines, with limited computational resources.

</details>


### [51] [VMRA-MaR: An Asymmetry-Aware Temporal Framework for Longitudinal Breast Cancer Risk Prediction](https://arxiv.org/abs/2506.17412)
*Zijun Sun,Solveig Thrun,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: This paper proposes a Vision Mamba RNN (VMRNN) with a state-space model (SSM) and LSTM-like memory mechanisms to effectively capture nuanced trends in breast tissue evolution. The integrated framework demonstrates notable improvements in predicting cancer onset, especially for the more challenging high-density breast cases and achieves superior performance at extended time points.


<details>
  <summary>Details</summary>
Motivation: Automated risk prediction approaches have the potential to improve the breast cancer screening process by facilitating dynamically screening of high-risk groups. There is growing interest in exploiting temporal information to capture evolving trends in breast tissue, as inspired by clinical practice. Early methods typically relied on two time steps, and although recent efforts have extended this to multiple time steps using Transformer architectures, challenges remain in fully harnessing the rich temporal dynamics inherent in longitudinal imaging data.

Method: leverage Vision Mamba RNN (VMRNN) with a state-space model (SSM) and LSTM-like memory mechanisms to effectively capture nuanced trends in breast tissue evolution. To further enhance our approach, we incorporate an asymmetry module that utilizes a Spatial Asymmetry Detector (SAD) and Longitudinal Asymmetry Tracker (LAT) to identify clinically relevant bilateral differences.

Result: achieves superior performance at extended time points (years four and five), highlighting its potential to advance early breast cancer recognition and enable more personalized screening strategies.

Conclusion: This integrated framework demonstrates notable improvements in predicting cancer onset, especially for the more challenging high-density breast cases and achieves superior performance at extended time points (years four and five), highlighting its potential to advance early breast cancer recognition and enable more personalized screening strategies.

Abstract: Breast cancer remains a leading cause of mortality worldwide and is typically
detected via screening programs where healthy people are invited in regular
intervals. Automated risk prediction approaches have the potential to improve
this process by facilitating dynamically screening of high-risk groups. While
most models focus solely on the most recent screening, there is growing
interest in exploiting temporal information to capture evolving trends in
breast tissue, as inspired by clinical practice. Early methods typically relied
on two time steps, and although recent efforts have extended this to multiple
time steps using Transformer architectures, challenges remain in fully
harnessing the rich temporal dynamics inherent in longitudinal imaging data. In
this work, we propose to instead leverage Vision Mamba RNN (VMRNN) with a
state-space model (SSM) and LSTM-like memory mechanisms to effectively capture
nuanced trends in breast tissue evolution. To further enhance our approach, we
incorporate an asymmetry module that utilizes a Spatial Asymmetry Detector
(SAD) and Longitudinal Asymmetry Tracker (LAT) to identify clinically relevant
bilateral differences. This integrated framework demonstrates notable
improvements in predicting cancer onset, especially for the more challenging
high-density breast cases and achieves superior performance at extended time
points (years four and five), highlighting its potential to advance early
breast cancer recognition and enable more personalized screening strategies.
Our code is available at https://github.com/Mortal-Suen/VMRA-MaR.git.

</details>


### [52] [Trans${^2}$-CBCT: A Dual-Transformer Framework for Sparse-View CBCT Reconstruction](https://arxiv.org/abs/2506.17425)
*Minmin Yang,Huantao Ren,Senem Velipasalar*

Main category: cs.CV

TL;DR: This paper introduces Trans-CBCT and Trans$^2$-CBCT, which combine CNN-Transformer features with point-based geometry reasoning for sparse-view CBCT reconstruction, achieving state-of-the-art results on LUNA16 and ToothFairy datasets.


<details>
  <summary>Details</summary>
Motivation: Cone-beam computed tomography (CBCT) using only a few X-ray projection views enables faster scans with lower radiation dose, but the resulting severe under-sampling causes strong artifacts and poor spatial coverage.

Method: We address these challenges in a unified framework. First, we replace conventional UNet/ResNet encoders with TransUNet, a hybrid CNN-Transformer model. We adapt TransUNet to CBCT by combining multi-scale features, querying view-specific features per 3D point, and adding a lightweight attenuation-prediction head. Second, we introduce a neighbor-aware Point Transformer to enforce volumetric coherence.

Result: This yields Trans-CBCT, which surpasses prior baselines by 1.17 dB PSNR and 0.0163 SSIM on the LUNA16 dataset with six views. The resulting model, Trans$^2$-CBCT, provides an additional gain of 0.63 dB PSNR and 0.0117 SSIM.

Conclusion: Experiments on LUNA16 and ToothFairy show consistent gains from six to ten views, validating the effectiveness of combining CNN-Transformer features with point-based geometry reasoning for sparse-view CBCT reconstruction.

Abstract: Cone-beam computed tomography (CBCT) using only a few X-ray projection views
enables faster scans with lower radiation dose, but the resulting severe
under-sampling causes strong artifacts and poor spatial coverage. We address
these challenges in a unified framework. First, we replace conventional
UNet/ResNet encoders with TransUNet, a hybrid CNN-Transformer model.
Convolutional layers capture local details, while self-attention layers enhance
global context. We adapt TransUNet to CBCT by combining multi-scale features,
querying view-specific features per 3D point, and adding a lightweight
attenuation-prediction head. This yields Trans-CBCT, which surpasses prior
baselines by 1.17 dB PSNR and 0.0163 SSIM on the LUNA16 dataset with six views.
Second, we introduce a neighbor-aware Point Transformer to enforce volumetric
coherence. This module uses 3D positional encoding and attention over k-nearest
neighbors to improve spatial consistency. The resulting model, Trans$^2$-CBCT,
provides an additional gain of 0.63 dB PSNR and 0.0117 SSIM. Experiments on
LUNA16 and ToothFairy show consistent gains from six to ten views, validating
the effectiveness of combining CNN-Transformer features with point-based
geometry reasoning for sparse-view CBCT reconstruction.

</details>


### [53] [Enhancing Wireless Device Identification through RF Fingerprinting: Leveraging Transient Energy Spectrum Analysis](https://arxiv.org/abs/2506.17439)
*Nisar Ahmed,Gulshan Saleem,Hafiz Muhammad Shahzad Asif,Muhammad Usman Younus,Kalsoom Safdar*

Main category: cs.CV

TL;DR: This paper introduces a CNN-Bi-GRU model for RF device identification using transient characteristics, achieving high accuracy and showing potential for enhancing device identification in complex wireless environments.


<details>
  <summary>Details</summary>
Motivation: Accurate identification and classification of radiation devices operating in complex electromagnetic environments is a key challenge in managing and securing these devices, driven by the growth of IoT and 5G.

Method: A hybrid deep learning model called CNN-Bi-GRU is used for learning the identification of RF devices based on their transient characteristics, with features extracted using transient energy spectrum analysis via the General Linear Chirplet Transform.

Result: The proposed approach achieved a 10-fold cross-validation performance with a precision of 99.33%, recall of 99.53%, F1-score of 99.43%, and classification accuracy of 99.17%.

Conclusion: The CNN-Bi-GRU model demonstrates promising classification performance, achieving high precision, recall, F1-score, and accuracy in identifying RF devices based on transient characteristics. This indicates its potential for enhancing device identification in complex wireless environments.

Abstract: In recent years, the rapid growth of the Internet of Things technologies and
the widespread adoption of 5G wireless networks have led to an exponential
increase in the number of radiation devices operating in complex
electromagnetic environments. A key challenge in managing and securing these
devices is accurate identification and classification. To address this
challenge, specific emitter identification techniques have emerged as a
promising solution that aims to provide reliable and efficient means of
identifying individual radiation devices in a unified and standardized manner.
This research proposes an approach that leverages transient energy spectrum
analysis using the General Linear Chirplet Transform to extract features from
RF devices. A dataset comprising nine RF devices is utilized, with each sample
containing 900 attributes and a total of 1080 equally distributed samples
across the devices. These features are then used in a classification modeling
framework. To overcome the limitations of conventional machine learning
methods, we introduce a hybrid deep learning model called the CNN-Bi-GRU for
learning the identification of RF devices based on their transient
characteristics. The proposed approach provided a 10-fold cross-validation
performance with a precision of 99.33%, recall of 99.53%, F1-score of 99.43%,
and classification accuracy of 99.17%. The results demonstrate the promising
classification performance of the CNN-Bi-GRU approach, indicating its
suitability for accurately identifying RF devices based on their transient
characteristics and its potential for enhancing device identification and
classification in complex wireless environments.

</details>


### [54] [AQUA20: A Benchmark Dataset for Underwater Species Classification under Challenging Conditions](https://arxiv.org/abs/2506.17455)
*Taufikur Rahman Fuad,Sabbir Ahmed,Shahriar Ivan*

Main category: cs.CV

TL;DR: AQUA20 dataset is introduced to benchmark underwater species recognition, ConvNeXt performs the best.


<details>
  <summary>Details</summary>
Motivation: Robust visual recognition in underwater environments is challenging due to complex distortions

Method: thirteen state-of-the-art deep learning models, including lightweight CNNs (SqueezeNet, MobileNetV2) and transformer-based architectures (ViT, ConvNeXt)

Result: ConvNeXt achieves the best performance, with a Top-3 accuracy of 98.82% and a Top-1 accuracy of 90.69%, as well as the highest overall F1-score of 88.92%

Conclusion: underwater species recognition can be improved and AQUA20 is a good foundation for future research

Abstract: Robust visual recognition in underwater environments remains a significant
challenge due to complex distortions such as turbidity, low illumination, and
occlusion, which severely degrade the performance of standard vision systems.
This paper introduces AQUA20, a comprehensive benchmark dataset comprising
8,171 underwater images across 20 marine species reflecting real-world
environmental challenges such as illumination, turbidity, occlusions, etc.,
providing a valuable resource for underwater visual understanding. Thirteen
state-of-the-art deep learning models, including lightweight CNNs (SqueezeNet,
MobileNetV2) and transformer-based architectures (ViT, ConvNeXt), were
evaluated to benchmark their performance in classifying marine species under
challenging conditions. Our experimental results show ConvNeXt achieving the
best performance, with a Top-3 accuracy of 98.82% and a Top-1 accuracy of
90.69%, as well as the highest overall F1-score of 88.92% with moderately large
parameter size. The results obtained from our other benchmark models also
demonstrate trade-offs between complexity and performance. We also provide an
extensive explainability analysis using GRAD-CAM and LIME for interpreting the
strengths and pitfalls of the models. Our results reveal substantial room for
improvement in underwater species recognition and demonstrate the value of
AQUA20 as a foundation for future research in this domain. The dataset is
publicly available at: https://huggingface.co/datasets/taufiktrf/AQUA20.

</details>


### [55] [When Every Millisecond Counts: Real-Time Anomaly Detection via the Multimodal Asynchronous Hybrid Network](https://arxiv.org/abs/2506.17457)
*Dong Xiao,Guangyao Chen,Peixi Peng,Yangru Huang,Yifan Zhao,Yongxing Dai,Yonghong Tian*

Main category: cs.CV

TL;DR: real-time anomaly detection for autonomous driving, prioritizing both minimal response time and high accuracy


<details>
  <summary>Details</summary>
Motivation: Current methods often focus on detection accuracy but neglect response time, which is critical in time-sensitive driving scenarios.

Method: a novel multimodal asynchronous hybrid network that combines event streams from event cameras with image data from RGB cameras. Our network utilizes the high temporal resolution of event cameras through an asynchronous Graph Neural Network and integrates it with spatial features extracted by a CNN from RGB images

Result: achieving millisecond-level real-time performance.

Conclusion: The proposed approach outperforms existing methods in both accuracy and response time, achieving millisecond-level real-time performance.

Abstract: Anomaly detection is essential for the safety and reliability of autonomous
driving systems. Current methods often focus on detection accuracy but neglect
response time, which is critical in time-sensitive driving scenarios. In this
paper, we introduce real-time anomaly detection for autonomous driving,
prioritizing both minimal response time and high accuracy. We propose a novel
multimodal asynchronous hybrid network that combines event streams from event
cameras with image data from RGB cameras. Our network utilizes the high
temporal resolution of event cameras through an asynchronous Graph Neural
Network and integrates it with spatial features extracted by a CNN from RGB
images. This combination effectively captures both the temporal dynamics and
spatial details of the driving environment, enabling swift and precise anomaly
detection. Extensive experiments on benchmark datasets show that our approach
outperforms existing methods in both accuracy and response time, achieving
millisecond-level real-time performance.

</details>


### [56] [Photogranulometry -- Dataset of soil images with corresponding particle size distributions](https://arxiv.org/abs/2506.17469)
*Thomas Plante St-Cyr,François Duhaime,Jean-Sébastien Dubé,Simon Grenier*

Main category: cs.CV

TL;DR: A new high-resolution image dataset of soil samples with PSD analysis is presented for training CNNs, addressing the limitations of traditional PSD analyses.


<details>
  <summary>Details</summary>
Motivation: Traditional particle size distribution (PSD) analyses create significant downtime and are expensive in labor and maintenance. These drawbacks could be alleviated using optical grain size analysis integrated into routine geotechnical laboratory workflow.

Method: Soil samples were photographed in a standardized top-view position with a resolution of 45 MP and a minimum scale of 39.4 micrometers per pixel, both in their moist and dry states. A custom test bench employing 13x9 inch white aluminum trays was used.

Result: The dataset provides a robust starting point for training convolutional neural networks (CNN) in geotechnical applications.

Conclusion: This paper introduces a dataset of 12,714 high-resolution images of 321 soil samples from the Montreal region, accompanied by their PSD analysis.

Abstract: Traditional particle size distribution (PSD) analyses create significant
downtime and are expensive in labor and maintenance. These drawbacks could be
alleviated using optical grain size analysis integrated into routine
geotechnical laboratory workflow. This paper presents a high-resolution dataset
of 12,714 images of 321 different soil samples collected in the Montreal,
Quebec region, alongside their PSD analysis. It is designed to provide a robust
starting point for training convolutional neural networks (CNN) in geotechnical
applications. Soil samples were photographed in a standardized top-view
position with a resolution of 45 MP and a minimum scale of 39.4 micrometers per
pixel, both in their moist and dry states. A custom test bench employing 13x9
inch white aluminum trays, on which the samples are spread in a thin layer, was
used. For samples exceeding a size limit, a coning and quartering method was
employed for mass reduction.

</details>


### [57] [Few-Shot, Now for Real: Medical VLMs Adaptation without Balanced Sets or Validation](https://arxiv.org/abs/2506.17500)
*Julio Silva-Rodríguez,Fereshteh Shakeri,Houda Bahig,Jose Dolz,Ismail Ben Ayed*

Main category: cs.CV

TL;DR: 这项工作挑战了对医学图像分析中视觉语言模型 (VLM) 的有利假设，并引入了一种现实的、不平衡的、无验证的适应设置。


<details>
  <summary>Details</summary>
Motivation: 先前关于该主题的工作对适应数据的分布做出了很强的假设，这在医学领域是不现实的。首先，先前的技术假设可以访问平衡的支持集，这种情况打破了现实场景中疾病流行的自然不平衡。其次，这些工作通常假设存在一个额外的验证集来修复关键的超参数，这是非常数据低效的。这项工作挑战了这些有利的部署场景，并引入了一个现实的、不平衡的、无验证的适应设置。

Method: 引入了一种免训练的线性探针，可以自适应地混合视觉和文本监督

Result: 在各种模态和下游任务中进行的广泛基准测试表明，当前的方法在实际条件下运行时会系统性地降低其性能，有时甚至比零样本推理更差。

Conclusion: 我们引入了一种免训练的线性探针，可以自适应地混合视觉和文本监督。详细的研究表明，所提出的求解器是一个强大的、高效的基线，能够在具有挑战性的场景中实现稳健的适应。

Abstract: Vision-language models (VLMs) are gaining attention in medical image
analysis. These are pre-trained on large, heterogeneous data sources, yielding
rich and transferable representations. Notably, the combination of
modality-specialized VLMs with few-shot adaptation has provided fruitful
results, enabling the efficient deployment of high-performing solutions.
However, previous works on this topic make strong assumptions about the
distribution of adaptation data, which are unrealistic in the medical domain.
First, prior art assumes access to a balanced support set, a condition that
breaks the natural imbalance in disease prevalence found in real-world
scenarios. Second, these works typically assume the presence of an additional
validation set to fix critical hyper-parameters, which is highly
data-inefficient. This work challenges these favorable deployment scenarios and
introduces a realistic, imbalanced, validation-free adaptation setting. Our
extensive benchmark across various modalities and downstream tasks demonstrates
that current methods systematically compromise their performance when operating
under realistic conditions, occasionally even performing worse than zero-shot
inference. Also, we introduce a training-free linear probe that adaptively
blends visual and textual supervision. Detailed studies demonstrate that the
proposed solver is a strong, efficient baseline, enabling robust adaptation in
challenging scenarios.

</details>


### [58] [Trustworthy Few-Shot Transfer of Medical VLMs through Split Conformal Prediction](https://arxiv.org/abs/2506.17503)
*Julio Silva-Rodríguez,Ismail Ben Ayed,Jose Dolz*

Main category: cs.CV

TL;DR: This paper introduces transductive split conformal adaptation (SCA-T) to improve the reliability and efficiency of medical vision-language models in image classification tasks.


<details>
  <summary>Details</summary>
Motivation: Medical vision-language models (VLMs) have demonstrated unprecedented transfer capabilities, but its reliability aspect remains largely unexplored. Adapting the model using the available calibration data breaks the rigid exchangeability assumptions for test data in SCP.

Method: We propose transductive split conformal adaptation (SCA-T), a novel pipeline for transfer learning on conformal scenarios, which performs an unsupervised transductive adaptation jointly on calibration and test data.

Result: Comprehensive experiments utilizing medical VLMs across various image modalities, transfer tasks, and non-conformity scores.

Conclusion: The proposed SCA-T framework offers consistent gains in efficiency and conditional coverage compared to SCP, maintaining the same empirical guarantees.

Abstract: Medical vision-language models (VLMs) have demonstrated unprecedented
transfer capabilities and are being increasingly adopted for data-efficient
image classification. Despite its growing popularity, its reliability aspect
remains largely unexplored. This work explores the split conformal prediction
(SCP) framework to provide trustworthiness guarantees when transferring such
models based on a small labeled calibration set. Despite its potential, the
generalist nature of the VLMs' pre-training could negatively affect the
properties of the predicted conformal sets for specific tasks. While common
practice in transfer learning for discriminative purposes involves an
adaptation stage, we observe that deploying such a solution for conformal
purposes is suboptimal since adapting the model using the available calibration
data breaks the rigid exchangeability assumptions for test data in SCP. To
address this issue, we propose transductive split conformal adaptation (SCA-T),
a novel pipeline for transfer learning on conformal scenarios, which performs
an unsupervised transductive adaptation jointly on calibration and test data.
We present comprehensive experiments utilizing medical VLMs across various
image modalities, transfer tasks, and non-conformity scores. Our framework
offers consistent gains in efficiency and conditional coverage compared to SCP,
maintaining the same empirical guarantees.

</details>


### [59] [Learning golf swing signatures from a single wrist-worn inertial sensor](https://arxiv.org/abs/2506.17505)
*Jessy Lauer*

Main category: cs.CV

TL;DR: holistic, data-driven framework for personalized golf swing analysis from a single wrist-worn sensor


<details>
  <summary>Details</summary>
Motivation: golf swing analysis is limited by isolated metrics, underrepresentation of professional athletes, and a lack of rich, interpretable movement representations

Method: build a large dataset of professional swings from publicly available videos, reconstruct full-body 3D kinematics using biologically accurate human mesh recovery, and generate synthetic inertial data to train neural networks that infer motion and segment swing phases from wrist-based input. We learn a compositional, discrete vocabulary of motion primitives that facilitates the detection and visualization of technical flaws, and is expressive enough to predict player identity, club type, sex, and age.

Result: accurately estimates full-body kinematics and swing events from wrist data, delivering lab-grade motion analysis on-course and supporting early detection of anomalous movement patterns. Explainability methods reveal subtle, individualized movement signatures, reinforcing the view that variability is a hallmark of skilled performance. Longitudinal tracking demonstrates practical value: as one player's handicap improved from 50 to 2.2 over 1.5 years, our system captured measurable technical progress and provided targeted, actionable feedback.

Conclusion: This work bridges lab and field-based biomechanics, offering scalable, accessible, high-fidelity motion analysis for research, coaching, and injury prevention, while opening new directions in movement-based phenotyping, personalized equipment design, and motor skill development.

Abstract: Despite its importance for performance and injury prevention, golf swing
analysis is limited by isolated metrics, underrepresentation of professional
athletes, and a lack of rich, interpretable movement representations. We
address these gaps with a holistic, data-driven framework for personalized golf
swing analysis from a single wrist-worn sensor. We build a large dataset of
professional swings from publicly available videos, reconstruct full-body 3D
kinematics using biologically accurate human mesh recovery, and generate
synthetic inertial data to train neural networks that infer motion and segment
swing phases from wrist-based input. We learn a compositional, discrete
vocabulary of motion primitives that facilitates the detection and
visualization of technical flaws, and is expressive enough to predict player
identity, club type, sex, and age. Our system accurately estimates full-body
kinematics and swing events from wrist data, delivering lab-grade motion
analysis on-course and supporting early detection of anomalous movement
patterns. Explainability methods reveal subtle, individualized movement
signatures, reinforcing the view that variability is a hallmark of skilled
performance. Longitudinal tracking demonstrates practical value: as one
player's handicap improved from 50 to 2.2 over 1.5 years, our system captured
measurable technical progress and provided targeted, actionable feedback. Our
findings challenge common assumptions, such as swing consistency across clubs
and the existence of a single "ideal" swing, and uncover latent biomarkers
shaped by both intrinsic traits and task-specific constraints. This work
bridges lab and field-based biomechanics, offering scalable, accessible,
high-fidelity motion analysis for research, coaching, and injury prevention,
while opening new directions in movement-based phenotyping, personalized
equipment design, and motor skill development.

</details>


### [60] [Scene-R1: Video-Grounded Large Language Models for 3D Scene Reasoning without 3D Annotations](https://arxiv.org/abs/2506.17545)
*Zhihao Yuan,Shuyi Jiang,Chun-Mei Feng,Yaolun Zhang,Shuguang Cui,Zhen Li,Na Zhao*

Main category: cs.CV

TL;DR: Scene-R1, a video-grounded framework, learns to reason about 3D scenes without any point-wise 3D instance supervision by pairing reinforcement-learning-driven reasoning with a two-stage grounding pipeline


<details>
  <summary>Details</summary>
Motivation: existing 3D-aware LLMs act as black boxes and they still rely on pre-trained 3D detectors to supply object proposals

Method: pairing reinforcement-learning-driven reasoning with a two-stage grounding pipeline

Result: Scene-R1 surpasses existing open-vocabulary baselines on multiple datasets, while delivering transparent, step-by-step rationales

Conclusion: reinforcement-learning-based reasoning combined with RGB-D video alone offers a practical, annotation-efficient route to trustworthy 3D scene understanding

Abstract: Currently, utilizing large language models to understand the 3D world is
becoming popular. Yet existing 3D-aware LLMs act as black boxes: they output
bounding boxes or textual answers without revealing how those decisions are
made, and they still rely on pre-trained 3D detectors to supply object
proposals. We introduce Scene-R1, a video-grounded framework that learns to
reason about 3D scenes without any point-wise 3D instance supervision by
pairing reinforcement-learning-driven reasoning with a two-stage grounding
pipeline. In the temporal grounding stage, we explicitly reason about the video
and select the video snippets most relevant to an open-ended query. In the
subsequent image grounding stage, we analyze the image and predict the 2D
bounding box. After that, we track the object using SAM2 to produce
pixel-accurate masks in RGB frames, and project them back into 3D, thereby
eliminating the need for 3D detector-based proposals while capturing fine
geometry and material cues. Scene-R1 can also adapt to the 3D visual question
answering task to answer free-form questions directly from video. Our training
pipeline only needs task-level 2D boxes or textual labels without dense 3D
point-wise labels. Scene-R1 surpasses existing open-vocabulary baselines on
multiple datasets, while delivering transparent, step-by-step rationales. These
results show that reinforcement-learning-based reasoning combined with RGB-D
video alone offers a practical, annotation-efficient route to trustworthy 3D
scene understanding.

</details>


### [61] [SynDaCaTE: A Synthetic Dataset For Evaluating Part-Whole Hierarchical Inference](https://arxiv.org/abs/2506.17558)
*Jake Levi,Mark van der Wilk*

Main category: cs.CV

TL;DR: This paper introduces SynDaCaTE, a synthetic dataset for capsule testing and evaluation. The authors use it to find a bottleneck in an existing capsule model and demonstrate that permutation-equivariant self-attention is effective for parts-to-wholes inference.


<details>
  <summary>Details</summary>
Motivation: Learning to infer object representations, and in particular part-whole hierarchies, has been the focus of extensive research in computer vision, in pursuit of improving data efficiency, systematic generalisation, and robustness. Models which are designed to infer part-whole hierarchies, often referred to as capsule networks, are typically trained end-to-end on supervised tasks such as object classification, in which case it is difficult to evaluate whether such a model actually learns to infer part-whole hierarchies, as claimed.

Method: present a SYNthetic DAtaset for CApsule Testing and Evaluation, abbreviated as SynDaCaTE, and establish its utility by (1) demonstrating the precise bottleneck in a prominent existing capsule model, and (2) demonstrating that permutation-equivariant self-attention is highly effective for parts-to-wholes inference

Result: demonstrating the precise bottleneck in a prominent existing capsule model

Conclusion: permutation-equivariant self-attention is highly effective for parts-to-wholes inference, which motivates future directions for designing effective inductive biases for computer vision.

Abstract: Learning to infer object representations, and in particular part-whole
hierarchies, has been the focus of extensive research in computer vision, in
pursuit of improving data efficiency, systematic generalisation, and
robustness. Models which are \emph{designed} to infer part-whole hierarchies,
often referred to as capsule networks, are typically trained end-to-end on
supervised tasks such as object classification, in which case it is difficult
to evaluate whether such a model \emph{actually} learns to infer part-whole
hierarchies, as claimed. To address this difficulty, we present a SYNthetic
DAtaset for CApsule Testing and Evaluation, abbreviated as SynDaCaTE, and
establish its utility by (1) demonstrating the precise bottleneck in a
prominent existing capsule model, and (2) demonstrating that
permutation-equivariant self-attention is highly effective for parts-to-wholes
inference, which motivates future directions for designing effective inductive
biases for computer vision.

</details>


### [62] [VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models](https://arxiv.org/abs/2506.17561)
*Chongkai Gao,Zixuan Liu,Zhenghao Chi,Junshan Huang,Xin Fei,Yiwen Hou,Yuxuan Zhang,Yudi Lin,Zhirui Fang,Zeyu Jiang,Lin Shao*

Main category: cs.CV

TL;DR: This paper introduces VLA-OS to study task planning paradigms in VLA models, finding visually grounded planning is better and Hierarchical-VLA is generally superior.


<details>
  <summary>Details</summary>
Motivation: existing approaches vary significantly in terms of network architectures, planning paradigms, representations, and training data sources, making it challenging for researchers to identify the precise sources of performance gains and components to be further improved

Method: introduce VLA-OS, a unified VLA architecture series capable of various task planning paradigms, and design a comprehensive suite of controlled experiments across diverse object categories (rigid and deformable), visual modalities (2D and 3D), environments (simulation and real-world), and end-effectors (grippers and dexterous hands)

Result: visually grounded planning representations are generally better than language planning representations; the Hierarchical-VLA paradigm generally achieves superior or comparable performance than other paradigms on task performance, pretraining, generalization ability, scalability, and continual learning ability, albeit at the cost of slower training and inference speeds

Conclusion: visually grounded planning representations are generally better than language planning representations; the Hierarchical-VLA paradigm generally achieves superior or comparable performance than other paradigms on task performance, pretraining, generalization ability, scalability, and continual learning ability, albeit at the cost of slower training and inference speeds

Abstract: Recent studies on Vision-Language-Action (VLA) models have shifted from the
end-to-end action-generation paradigm toward a pipeline involving task planning
followed by action generation, demonstrating improved performance on various
complex, long-horizon manipulation tasks. However, existing approaches vary
significantly in terms of network architectures, planning paradigms,
representations, and training data sources, making it challenging for
researchers to identify the precise sources of performance gains and components
to be further improved. To systematically investigate the impacts of different
planning paradigms and representations isolating from network architectures and
training data, in this paper, we introduce VLA-OS, a unified VLA architecture
series capable of various task planning paradigms, and design a comprehensive
suite of controlled experiments across diverse object categories (rigid and
deformable), visual modalities (2D and 3D), environments (simulation and
real-world), and end-effectors (grippers and dexterous hands). Our results
demonstrate that: 1) visually grounded planning representations are generally
better than language planning representations; 2) the Hierarchical-VLA paradigm
generally achieves superior or comparable performance than other paradigms on
task performance, pretraining, generalization ability, scalability, and
continual learning ability, albeit at the cost of slower training and inference
speeds.

</details>


### [63] [LLM-driven Medical Report Generation via Communication-efficient Heterogeneous Federated Learning](https://arxiv.org/abs/2506.17562)
*Haoxuan Che,Haibo Jin,Zhengrui Guo,Yi Lin,Cheng Jin,Hao Chen*

Main category: cs.CV

TL;DR: FedMRG is a federated learning framework for privacy-preserving, multi-center development of LLM-driven MRG models, addressing communication overhead and data heterogeneity challenges.


<details>
  <summary>Details</summary>
Motivation: Development of LLM-driven Medical Report Generation (MRG) models requires large amounts of medical image-report pairs scattered across multiple centers, but centralizing these data is challenging due to privacy regulations, impeding model development.

Method: The FedMRG framework employs low-rank factorization for communication-efficient LLM tuning, client-aware contrastive learning with diagnosis-driven prompts in the MRG encoder, and a dual-adapter mutual boosting mechanism in the MRG decoder.

Result: The FedMRG framework achieves communication efficiency and addresses data heterogeneity in multi-center medical report generation.

Conclusion: The FedMRG framework demonstrates generalizability and adaptability in generating clinically accurate reports while maintaining communication efficiency, underscoring its potential in harnessing multi-center data.

Abstract: LLMs have demonstrated significant potential in Medical Report Generation
(MRG), yet their development requires large amounts of medical image-report
pairs, which are commonly scattered across multiple centers. Centralizing these
data is exceptionally challenging due to privacy regulations, thereby impeding
model development and broader adoption of LLM-driven MRG models. To address
this challenge, we present FedMRG, the first framework that leverages Federated
Learning (FL) to enable privacy-preserving, multi-center development of
LLM-driven MRG models, specifically designed to overcome the critical challenge
of communication-efficient LLM training under multi-modal data heterogeneity.
To start with, our framework tackles the fundamental challenge of communication
overhead in FL-LLM tuning by employing low-rank factorization to efficiently
decompose parameter updates, significantly reducing gradient transmission costs
and making LLM-driven MRG feasible in bandwidth-constrained FL settings.
Furthermore, we observed the dual heterogeneity in MRG under the FL scenario:
varying image characteristics across medical centers, as well as diverse
reporting styles and terminology preferences. To address this, we further
enhance FedMRG with (1) client-aware contrastive learning in the MRG encoder,
coupled with diagnosis-driven prompts, which capture both globally
generalizable and locally distinctive features while maintaining diagnostic
accuracy; and (2) a dual-adapter mutual boosting mechanism in the MRG decoder
that harmonizes generic and specialized adapters to address variations in
reporting styles and terminology. Through extensive evaluation of our
established FL-MRG benchmark, we demonstrate the generalizability and
adaptability of FedMRG, underscoring its potential in harnessing multi-center
data and generating clinically accurate reports while maintaining communication
efficiency.

</details>


### [64] [HalluRNN: Mitigating Hallucinations via Recurrent Cross-Layer Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2506.17587)
*Le Yu,Kaishen Wang,Jianlong Xiong,Yue Cao,Tao He*

Main category: cs.CV

TL;DR: HalluRNN通过循环跨层推理解决大型视觉语言模型的幻觉问题，仅需微调DG-DPU模块即可实现良好性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLM）在各种任务中取得了显著的性能，但它们仍然容易产生幻觉，即生成在文本上合理但在视觉上无根据的输出。

Method: 提出了一种新颖的双门控深度传播单元（DG-DPU）模块，该模块在各层之间共享并循环细化隐藏状态。

Result: 通过仅微调DG-DPU模块，HalluRNN在多个基准测试中实现了强大而稳健的性能。

Conclusion: HalluRNN通过循环跨层推理增强模型稳定性，并在多个基准测试中实现了强大而稳健的性能。

Abstract: Though Large Vision-Language Models (LVLMs) have achieved remarkable
performance across various tasks, they are still prone to
hallucinations-generating outputs that are textually plausible but visually
ungrounded. While prior approaches generally address this issue through
data-centric fine-tuning or innovative decoding strategies, these methods often
require substantial resources or task-specific configurations. In this work, we
introduce an architecture-level solution, HalluRNN, which enhances model
stability through recurrent cross-layer reasoning. Specifically, we propose a
novel Dual-Gated Depth Propagation Unit (DG-DPU) module, which is shared across
layers and recurrently refines hidden states. This allows for the adaptive
propagation of information throughout the model, enforces consistency across
layers, and mitigates hallucinations caused by representational drift. By
fine-tuning only the DG-DPU module, HalluRNN achieves strong and robust
performance across multiple benchmarks.

</details>


### [65] [DRAMA-X: A Fine-grained Intent Prediction and Risk Reasoning Benchmark For Driving](https://arxiv.org/abs/2506.17590)
*Mihir Godbole,Xiangbo Gao,Zhengzhong Tu*

Main category: cs.CV

TL;DR: Introduces DRAMA-X, a new benchmark for intent prediction in autonomous driving, and proposes SGG-Intent, a scene-graph-based reasoning framework.


<details>
  <summary>Details</summary>
Motivation: Understanding the short-term motion of vulnerable road users (VRUs) like pedestrians and cyclists is critical for safe autonomous driving, especially in urban scenarios with ambiguous or high-risk behaviors. While vision-language models (VLMs) have enabled open-vocabulary perception, their utility for fine-grained intent reasoning remains underexplored. Notably, no existing benchmark evaluates multi-class intent prediction in safety-critical situations

Method: A lightweight, training-free framework that mirrors the ego vehicle's reasoning pipeline. It sequentially generates a scene graph from visual input using VLM-backed detectors, infers intent, assesses risk, and recommends an action using a compositional reasoning stage powered by a large language model.

Result: DRAMA-X contains 5,686 accident-prone frames labeled with object bounding boxes, a nine-class directional intent taxonomy, binary risk scores, expert-generated action suggestions for the ego vehicle, and descriptive motion summaries. These annotations enable a structured evaluation of four interrelated tasks central to autonomous decision-making: object detection, intent prediction, risk assessment, and action suggestion.

Conclusion: Scene-graph-based reasoning enhances intent prediction and risk assessment, especially when contextual cues are explicitly modeled.

Abstract: Understanding the short-term motion of vulnerable road users (VRUs) like
pedestrians and cyclists is critical for safe autonomous driving, especially in
urban scenarios with ambiguous or high-risk behaviors. While vision-language
models (VLMs) have enabled open-vocabulary perception, their utility for
fine-grained intent reasoning remains underexplored. Notably, no existing
benchmark evaluates multi-class intent prediction in safety-critical
situations, To address this gap, we introduce DRAMA-X, a fine-grained benchmark
constructed from the DRAMA dataset via an automated annotation pipeline.
DRAMA-X contains 5,686 accident-prone frames labeled with object bounding
boxes, a nine-class directional intent taxonomy, binary risk scores,
expert-generated action suggestions for the ego vehicle, and descriptive motion
summaries. These annotations enable a structured evaluation of four
interrelated tasks central to autonomous decision-making: object detection,
intent prediction, risk assessment, and action suggestion. As a reference
baseline, we propose SGG-Intent, a lightweight, training-free framework that
mirrors the ego vehicle's reasoning pipeline. It sequentially generates a scene
graph from visual input using VLM-backed detectors, infers intent, assesses
risk, and recommends an action using a compositional reasoning stage powered by
a large language model. We evaluate a range of recent VLMs, comparing
performance across all four DRAMA-X tasks. Our experiments demonstrate that
scene-graph-based reasoning enhances intent prediction and risk assessment,
especially when contextual cues are explicitly modeled.

</details>


### [66] [SELFI: Selective Fusion of Identity for Generalizable Deepfake Detection](https://arxiv.org/abs/2506.17592)
*Younghun Kim,Minsuk Jang,Myung-Joon Kwon,Wonjun Lee,Changick Kim*

Main category: cs.CV

TL;DR: 提出了一个名为SELFI的deepfake检测框架，该框架能够动态地调整身份信息的使用，从而提高检测的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有deepfake检测方法对身份信息的利用存在争议，有的抑制身份线索以减少偏差，有的依赖身份信息作为取证依据。为了协调这些观点，研究分析了两个假设：(1) 仅靠人脸身份是否足以区分deepfake；(2) 这种身份特征是否在不同的操纵方法中泛化能力差。

Method: 提出了SELFI框架，包含Forgery-Aware Identity Adapter (FAIA) 和 Identity-Aware Fusion Module (IAFM)，前者提取身份嵌入并将其投影到伪造相关的空间，后者选择性地融合身份和视觉特征。

Result: 实验结果表明，身份信息是有用的，但依赖于上下文。SELFI框架在跨操纵的泛化能力方面有所提高，并在四个基准测试中取得了比现有方法平均高3.1% AUC的性能提升。在具有挑战性的DFDC数据集上，SELFI超过了之前的最佳结果6%。

Conclusion: 提出SELFI框架，通过选择性地融合身份信息和视觉特征来提高deepfake检测的泛化能力，并在四个基准测试中取得了比现有方法平均高3.1% AUC的性能提升，在DFDC数据集上超过了之前的最佳结果6%。

Abstract: Face identity provides a powerful signal for deepfake detection. Prior
studies show that even when not explicitly modeled, classifiers often learn
identity features implicitly. This has led to conflicting views: some suppress
identity cues to reduce bias, while others rely on them as forensic evidence.
To reconcile these views, we analyze two hypotheses: (1) whether face identity
alone is discriminative for detecting deepfakes, and (2) whether such identity
features generalize poorly across manipulation methods. Our experiments confirm
that identity is informative but context-dependent. While some manipulations
preserve identity-consistent artifacts, others distort identity cues and harm
generalization. We argue that identity features should neither be blindly
suppressed nor relied upon, but instead be explicitly modeled and adaptively
controlled based on per-sample relevance. We propose \textbf{SELFI}
(\textbf{SEL}ective \textbf{F}usion of \textbf{I}dentity), a generalizable
detection framework that dynamically modulates identity usage. SELFI consists
of: (1) a Forgery-Aware Identity Adapter (FAIA) that extracts identity
embeddings from a frozen face recognition model and projects them into a
forgery-relevant space via auxiliary supervision; and (2) an Identity-Aware
Fusion Module (IAFM) that selectively integrates identity and visual features
using a relevance-guided fusion mechanism. Experiments on four benchmarks show
that SELFI improves cross-manipulation generalization, outperforming prior
methods by an average of 3.1\% AUC. On the challenging DFDC dataset, SELFI
exceeds the previous best by 6\%. Code will be released upon paper acceptance.

</details>


### [67] [A Multimodal In Vitro Diagnostic Method for Parkinson's Disease Combining Facial Expressions and Behavioral Gait Data](https://arxiv.org/abs/2506.17596)
*Wei Huang,Yinxuan Xu,Yintao Zhou,Zhengyu Li,Jing Huang,Meng Pang*

Main category: cs.CV

TL;DR: 提出了一种新的多模态体外诊断方法，利用面部表情和步态，使用轻量级深度学习模型，建立了最大的多模态帕金森病数据集，提高了诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在以下问题：1）面部表情诊断的训练数据有限；2）步态诊断需要专用设备和采集环境，导致泛化性差；3）依赖单一模态存在误诊或漏诊的风险。人口老龄化，对帕金森病早期检测的需求日益增加。

Method: 采用轻量级深度学习模型进行特征提取和融合。

Result: 建立了最大的多模态帕金森病数据集，提高了诊断准确性，并促进了在移动设备上的部署。

Conclusion: 我们提出了一种新的多模态体外诊断方法，利用面部表情和步态来诊断帕金森病，并通过实验验证了该方法的有效性。

Abstract: Parkinson's disease (PD), characterized by its incurable nature, rapid
progression, and severe disability, poses significant challenges to the lives
of patients and their families. Given the aging population, the need for early
detection of PD is increasing. In vitro diagnosis has garnered attention due to
its non-invasive nature and low cost. However, existing methods present several
challenges: 1) limited training data for facial expression diagnosis; 2)
specialized equipment and acquisition environments required for gait diagnosis,
resulting in poor generalizability; 3) the risk of misdiagnosis or missed
diagnosis when relying on a single modality. To address these issues, we
propose a novel multimodal in vitro diagnostic method for PD, leveraging facial
expressions and behavioral gait. Our method employs a lightweight deep learning
model for feature extraction and fusion, aimed at improving diagnostic accuracy
and facilitating deployment on mobile devices. Furthermore, we have established
the largest multimodal PD dataset in collaboration with a hospital and
conducted extensive experiments to validate the effectiveness of our proposed
method.

</details>


### [68] [OpenMAP-BrainAge: Generalizable and Interpretable Brain Age Predictor](https://arxiv.org/abs/2506.17597)
*Pengyu Kan,Craig Jones,Kenichi Oishi*

Main category: cs.CV

TL;DR: Developed a robust and interpretable age prediction model using a transformer-based architecture on brain MRI scans, achieving state-of-the-art accuracy and generalizability, with associations to neurodegenerative disorders.


<details>
  <summary>Details</summary>
Motivation: To develop an age prediction model which is interpretable and robust to demographic and technological variances in brain MRI scans.

Method: We propose a transformer-based architecture that leverages self-supervised pre-training on large-scale datasets. Our model processes pseudo-3D T1-weighted MRI scans from three anatomical views and incorporates brain volumetric information. By introducing a stem architecture, we reduce the conventional quadratic complexity of transformer models to linear complexity, enabling scalability for high-dimensional MRI data. We trained our model on ADNI2 $\& 3 (N=1348) and OASIS3 (N=716) datasets (age range: 42 - 95) from the North America, with an 8:1:1 split for train, validation and test. Then, we validated it on the AIBL dataset (N=768, age range: 60 - 92) from Australia.

Result: We achieved an MAE of 3.65 years on ADNI2 $\& 3 and OASIS3 test set and a high generalizability of MAE of 3.54 years on AIBL. There was a notable increase in brain age gap (BAG) across cognitive groups, with mean of 0.15 years (95% CI: [-0.22, 0.51]) in CN, 2.55 years ([2.40, 2.70]) in MCI, 6.12 years ([5.82, 6.43]) in AD. Additionally, significant negative correlation between BAG and cognitive scores was observed, with correlation coefficient of -0.185 (p < 0.001) for MoCA and -0.231 (p < 0.001) for MMSE. Gradient-based feature attribution highlighted ventricles and white matter structures as key regions influenced by brain aging.

Conclusion: The model effectively fused information from different views and volumetric information to achieve state-of-the-art brain age prediction accuracy, improved generalizability and interpretability with association to neurodegenerative disorders.

Abstract: Purpose: To develop an age prediction model which is interpretable and robust
to demographic and technological variances in brain MRI scans. Materials and
Methods: We propose a transformer-based architecture that leverages
self-supervised pre-training on large-scale datasets. Our model processes
pseudo-3D T1-weighted MRI scans from three anatomical views and incorporates
brain volumetric information. By introducing a stem architecture, we reduce the
conventional quadratic complexity of transformer models to linear complexity,
enabling scalability for high-dimensional MRI data. We trained our model on
ADNI2 $\&$ 3 (N=1348) and OASIS3 (N=716) datasets (age range: 42 - 95) from the
North America, with an 8:1:1 split for train, validation and test. Then, we
validated it on the AIBL dataset (N=768, age range: 60 - 92) from Australia.
Results: We achieved an MAE of 3.65 years on ADNI2 $\&$ 3 and OASIS3 test set
and a high generalizability of MAE of 3.54 years on AIBL. There was a notable
increase in brain age gap (BAG) across cognitive groups, with mean of 0.15
years (95% CI: [-0.22, 0.51]) in CN, 2.55 years ([2.40, 2.70]) in MCI, 6.12
years ([5.82, 6.43]) in AD. Additionally, significant negative correlation
between BAG and cognitive scores was observed, with correlation coefficient of
-0.185 (p < 0.001) for MoCA and -0.231 (p < 0.001) for MMSE. Gradient-based
feature attribution highlighted ventricles and white matter structures as key
regions influenced by brain aging. Conclusion: Our model effectively fused
information from different views and volumetric information to achieve
state-of-the-art brain age prediction accuracy, improved generalizability and
interpretability with association to neurodegenerative disorders.

</details>


### [69] [HIRE: Lightweight High-Resolution Image Feature Enrichment for Multimodal LLMs](https://arxiv.org/abs/2506.17608)
*Nikitha SR,Aradhya Neeraj Mathur,Tarun Ram Menta,Rishabh Jain,Mausoom Sarkar*

Main category: cs.CV

TL;DR: Feature upsampling can reduce computational costs while maintaining competitive results in multimodal large language models.


<details>
  <summary>Details</summary>
Motivation: Integration of high-resolution image features in modern multimodal large language models has demonstrated significant improvements but comes with a significant increase in computational costs.

Method: develop an intuition for feature upsampling as a natural extension of high-resolution feature generation

Result: achieve competitive results with tremendous reductions in training and inference time as well as computational cost, with upto 1.5x saving in FLOPs.

Conclusion: A shallow feature enricher can achieve competitive results with tremendous reductions in training and inference time as well as computational cost, with upto 1.5x saving in FLOPs.

Abstract: The integration of high-resolution image features in modern multimodal large
language models has demonstrated significant improvements in fine-grained
visual understanding tasks, achieving high performance across multiple
benchmarks. Since these features are obtained from large image encoders like
ViT, they come with a significant increase in computational costs due to
multiple calls to these encoders. In this work, we first develop an intuition
for feature upsampling as a natural extension of high-resolution feature
generation. Through extensive experiments and ablations, we demonstrate how a
shallow feature enricher can achieve competitive results with tremendous
reductions in training and inference time as well as computational cost, with
upto 1.5x saving in FLOPs.

</details>


### [70] [JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo Retouching Agent](https://arxiv.org/abs/2506.17612)
*Yunlong Lin,Zixu Lin,Kunjie Lin,Jinbin Bai,Panwang Pan,Chenxin Li,Haoyu Chen,Zhongdao Wang,Xinghao Ding,Wenbo Li,Shuicheng Yan*

Main category: cs.CV

TL;DR: JarvisArt, a multi-modal large language model (MLLM)-driven agent, outperforms GPT-4o in intelligent photo retouching with better content fidelity and comparable instruction-following capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing AI-based solutions provide automation but often suffer from limited adjustability and poor generalization, failing to meet diverse and personalized editing needs. To bridge this gap, we introduce JarvisArt

Method: a multi-modal large language model (MLLM)-driven agent that understands user intent, mimics the reasoning process of professional artists, and intelligently coordinates over 200 retouching tools within Lightroom. JarvisArt undergoes a two-stage training process: an initial Chain-of-Thought supervised fine-tuning to establish basic reasoning and tool-use skills, followed by Group Relative Policy Optimization for Retouching (GRPO-R) to further enhance its decision-making and tool proficiency. We also propose the Agent-to-Lightroom Protocol to facilitate seamless integration with Lightroom. To evaluate performance, we develop MMArt-Bench, a novel benchmark constructed from real-world user edits.

Result: JarvisArt demonstrates user-friendly interaction, superior generalization, and fine-grained control over both global and local adjustments

Conclusion: JarvisArt outperforms GPT-4o with a 60% improvement in average pixel-level metrics on MMArt-Bench for content fidelity, while maintaining comparable instruction-following capabilities, paving a new avenue for intelligent photo retouching.

Abstract: Photo retouching has become integral to contemporary visual storytelling,
enabling users to capture aesthetics and express creativity. While professional
tools such as Adobe Lightroom offer powerful capabilities, they demand
substantial expertise and manual effort. In contrast, existing AI-based
solutions provide automation but often suffer from limited adjustability and
poor generalization, failing to meet diverse and personalized editing needs. To
bridge this gap, we introduce JarvisArt, a multi-modal large language model
(MLLM)-driven agent that understands user intent, mimics the reasoning process
of professional artists, and intelligently coordinates over 200 retouching
tools within Lightroom. JarvisArt undergoes a two-stage training process: an
initial Chain-of-Thought supervised fine-tuning to establish basic reasoning
and tool-use skills, followed by Group Relative Policy Optimization for
Retouching (GRPO-R) to further enhance its decision-making and tool
proficiency. We also propose the Agent-to-Lightroom Protocol to facilitate
seamless integration with Lightroom. To evaluate performance, we develop
MMArt-Bench, a novel benchmark constructed from real-world user edits.
JarvisArt demonstrates user-friendly interaction, superior generalization, and
fine-grained control over both global and local adjustments, paving a new
avenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a
60% improvement in average pixel-level metrics on MMArt-Bench for content
fidelity, while maintaining comparable instruction-following capabilities.
Project Page: https://jarvisart.vercel.app/.

</details>


### [71] [CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning](https://arxiv.org/abs/2506.17629)
*Kailing Li,Qi'ao Xu,Tianwen Qian,Yuqian Fu,Yang Jiao,Xiaoling Wang*

Main category: cs.CV

TL;DR: CLiViS是一个利用LLM和VLM优势的框架，用于具身视觉推理，通过动态认知地图桥接低层感知和高层推理，尤其擅长处理长期视觉依赖。


<details>
  <summary>Details</summary>
Motivation: 具身视觉推理（EVR）旨在遵循基于以自我为中心的视频的复杂、自由形式的指令，从而在动态环境中实现语义理解和时空推理。先前的解决方案要么在静态视频字幕上使用大型语言模型（LLM），这经常省略关键的视觉细节，要么依赖于端到端视觉-语言模型（VLM），这些模型难以进行逐步组合推理。

Method: CLiViS，一个新颖的免训练框架，利用LLM进行高层任务规划，并协调VLM驱动的开放世界视觉感知来迭代更新场景上下文。其核心是一个动态认知地图，构建了具身场景的结构化表示，桥接了低层感知和高层推理。

Result: CLiViS在多个基准测试中表现出有效性和通用性，尤其是在处理长期视觉依赖方面。

Conclusion: CLiViS在处理长期视觉依赖方面表现出有效性和通用性。

Abstract: Embodied Visual Reasoning (EVR) seeks to follow complex, free-form
instructions based on egocentric video, enabling semantic understanding and
spatiotemporal reasoning in dynamic environments. Despite its promising
potential, EVR encounters significant challenges stemming from the diversity of
complex instructions and the intricate spatiotemporal dynamics in long-term
egocentric videos. Prior solutions either employ Large Language Models (LLMs)
over static video captions, which often omit critical visual details, or rely
on end-to-end Vision-Language Models (VLMs) that struggle with stepwise
compositional reasoning. Consider the complementary strengths of LLMs in
reasoning and VLMs in perception, we propose CLiViS. It is a novel
training-free framework that leverages LLMs for high-level task planning and
orchestrates VLM-driven open-world visual perception to iteratively update the
scene context. Building on this synergy, the core of CLiViS is a dynamic
Cognitive Map that evolves throughout the reasoning process. This map
constructs a structured representation of the embodied scene, bridging
low-level perception and high-level reasoning. Extensive experiments across
multiple benchmarks demonstrate the effectiveness and generality of CLiViS,
especially in handling long-term visual dependencies. Code is available at
https://github.com/Teacher-Tom/CLiViS.

</details>


### [72] [Optimization-Free Patch Attack on Stereo Depth Estimation](https://arxiv.org/abs/2506.17632)
*Hangcheng Liu,Xu Kuang,Xingshuo Han,Xingwan Wu,Haoran Ou,Shangwei Guo,Xingyi Huang,Tao Xiang,Tianwei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的对抗攻击方法PatchHunter，用于攻击立体深度估计（SDE）模型。该方法通过强化学习在视觉模式的结构化空间中搜索，以扰乱SDE的假设，并在KITTI数据集、CARLA模拟器和真实车辆部署中验证了其有效性和可移植性。


<details>
  <summary>Details</summary>
Motivation: 立体深度估计（SDE）对于像自动驾驶这样基于视觉的系统中的场景理解至关重要。然而，最近的研究表明，SDE模型容易受到对抗性攻击，这些攻击通常局限于不切实际的设置，例如，静态场景中单独立体视图上的数字扰动，限制了它们在现实世界中的适用性。这就提出了一个关键问题：我们如何在现实约束下设计物理上可实现的、场景自适应的和可转移的攻击来对抗SDE？

Method: 提出了一种统一的攻击框架，该框架将基于优化的技术扩展到立体匹配的四个核心阶段：特征提取、成本量构建、成本聚合和视差回归。提出PatchHunter，这是第一个针对SDE的无优化对抗补丁攻击。

Result: 对9个主流SDE模型进行了全面的阶段性评估，在光度一致性等约束条件下，结果表明基于优化的补丁的泛化能力较差。有趣的是，部分可转移的补丁表明，模式（而不是像素级扰动）可能是实现通用攻击的关键。PatchHunter不仅在有效性上超越了基于优化的方法，而且实现了明显更好的黑盒可移植性。即使在低光等具有挑战性的物理条件下，PatchHunter也能保持较高的攻击成功率（例如，D1-all > 0.4），而基于优化的方法则失败了。

Conclusion: PatchHunter不仅在有效性上超越了基于优化的方法，而且实现了明显更好的黑盒可移植性。即使在低光等具有挑战性的物理条件下，PatchHunter也能保持较高的攻击成功率（例如，D1-all > 0.4），而基于优化的方法则失败了。

Abstract: Stereo Depth Estimation (SDE) is essential for scene understanding in
vision-based systems like autonomous driving. However, recent studies show that
SDE models are vulnerable to adversarial attacks, which are often limited to
unrealistic settings, e.g., digital perturbations on separate stereo views in
static scenes, restricting their real-world applicability. This raises a
critical question: how can we design physically realizable, scene-adaptive, and
transferable attacks against SDE under realistic constraints?
  To answer this, we make two key contributions. First, we propose a unified
attack framework that extends optimization-based techniques to four core stages
of stereo matching: feature extraction, cost-volume construction, cost
aggregation, and disparity regression. A comprehensive stage-wise evaluation
across 9 mainstream SDE models, under constraints like photometric consistency,
reveals that optimization-based patches suffer from poor transferability.
Interestingly, partially transferable patches suggest that patterns, rather
than pixel-level perturbations, may be key to generalizable attacks. Motivated
by this, we present PatchHunter, the first optimization-free adversarial patch
attack against SDE. PatchHunter formulates patch generation as a reinforcement
learning-driven search over a structured space of visual patterns crafted to
disrupt SDE assumptions.
  We validate PatchHunter across three levels: the KITTI dataset, the CARLA
simulator, and real-world vehicle deployment. PatchHunter not only surpasses
optimization-based methods in effectiveness but also achieves significantly
better black-box transferability. Even under challenging physical conditions
like low light, PatchHunter maintains high attack success (e.g., D1-all > 0.4),
whereas optimization-based methods fail.

</details>


### [73] [Adaptive Multi-prompt Contrastive Network for Few-shot Out-of-distribution Detection](https://arxiv.org/abs/2506.17633)
*Xiang Fang,Arvind Easwaran,Blaise Genest*

Main category: cs.CV

TL;DR: Addresses few-shot OOD detection with Adaptive Multi-prompt Contrastive Network (AMCN) using CLIP to handle limited data and improve ID-OOD separation.


<details>
  <summary>Details</summary>
Motivation: Most OOD detection methods require many IID samples for training, which seriously limits their real-world applications.  Previous few-shot OOD detection works ignore the distinct diversity between different classes. To compensate for the absence of OOD and scarcity of ID image samples, we leverage CLIP, connecting text with images, engineering learnable ID and OOD textual prompts.

Method: Adaptive Multi-prompt Contrastive Network (AMCN), which adapts the ID-OOD separation boundary by learning inter- and intra-class distribution. leverage CLIP, connecting text with images, engineering learnable ID and OOD textual prompts. Specifically, generate adaptive prompts (learnable ID prompts, label-fixed OOD prompts and label-adaptive OOD prompts). Then, generate an adaptive class boundary for each class by introducing a class-wise threshold. Finally, propose a prompt-guided ID-OOD separation module to control the margin between ID and OOD prompts

Result: AMCN outperforms other state-of-the-art works.

Conclusion: AMCN outperforms other state-of-the-art works.

Abstract: Out-of-distribution (OOD) detection attempts to distinguish outlier samples
to prevent models trained on the in-distribution (ID) dataset from producing
unavailable outputs. Most OOD detection methods require many IID samples for
training, which seriously limits their real-world applications. To this end, we
target a challenging setting: few-shot OOD detection, where {Only a few {\em
labeled ID} samples are available.} Therefore, few-shot OOD detection is much
more challenging than the traditional OOD detection setting. Previous few-shot
OOD detection works ignore the distinct diversity between different classes. In
this paper, we propose a novel network: Adaptive Multi-prompt Contrastive
Network (AMCN), which adapts the ID-OOD separation boundary by learning inter-
and intra-class distribution. To compensate for the absence of OOD and scarcity
of ID {\em image samples}, we leverage CLIP, connecting text with images,
engineering learnable ID and OOD {\em textual prompts}. Specifically, we first
generate adaptive prompts (learnable ID prompts, label-fixed OOD prompts and
label-adaptive OOD prompts). Then, we generate an adaptive class boundary for
each class by introducing a class-wise threshold. Finally, we propose a
prompt-guided ID-OOD separation module to control the margin between ID and OOD
prompts. Experimental results show that AMCN outperforms other state-of-the-art
works.

</details>


### [74] [Histopathology Image Report Generation by Vision Language Model with Multimodal In-Context Learning](https://arxiv.org/abs/2506.17645)
*Shih-Wen Liu,Hsuan-Yu Fan,Wei-Ta Chu,Fu-En Yang,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: PathGenIC是一种用于从组织病理学图像自动生成医疗报告的上下文学习框架，通过利用多模态上下文学习和训练数据，在HistGen基准测试中取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 从组织病理学图像自动生成医疗报告是一项关键挑战，需要有效的视觉表示和领域知识。受到人类专家常见做法的启发。

Method: 提出了一种上下文学习框架，称为PathGenIC，该框架集成了从训练集导出的上下文与多模态上下文学习（ICL）机制。

Result: 该框架在HistGen基准测试中取得了最先进的结果，并在BLEU、METEOR和ROUGE-L指标上取得了显著的改进，展示了在不同报告长度和疾病类别中的稳健性。

Conclusion: PathGenIC框架在HistGen基准测试中取得了最先进的结果，并在BLEU、METEOR和ROUGE-L指标上取得了显著的改进，展示了在不同报告长度和疾病类别中的稳健性。通过最大化训练数据效用和使用ICL桥接视觉和语言，为AI驱动的组织病理学报告提供了一个解决方案，并为多模态临床应用中的未来进展奠定了坚实的基础。

Abstract: Automating medical report generation from histopathology images is a critical
challenge requiring effective visual representations and domain-specific
knowledge. Inspired by the common practices of human experts, we propose an
in-context learning framework called PathGenIC that integrates context derived
from the training set with a multimodal in-context learning (ICL) mechanism.
Our method dynamically retrieves semantically similar whole slide image
(WSI)-report pairs and incorporates adaptive feedback to enhance contextual
relevance and generation quality. Evaluated on the HistGen benchmark, the
framework achieves state-of-the-art results, with significant improvements
across BLEU, METEOR, and ROUGE-L metrics, and demonstrates robustness across
diverse report lengths and disease categories. By maximizing training data
utility and bridging vision and language with ICL, our work offers a solution
for AI-driven histopathology reporting, setting a strong foundation for future
advancements in multimodal clinical applications.

</details>


### [75] [MDSAM:Memory-Driven Sparse Attention Matrix for LVLMs Hallucination Mitigation](https://arxiv.org/abs/2506.17664)
*Shuaiye Lu,Linjiang Zhou,Xiaochuan Shi*

Main category: cs.CV

TL;DR: MDSAM 是一种免训练方法，通过动态捕获和细化图像 tokens 的注意力来减少 LVLM 中的幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型 (LVLM) 中的幻觉通常源于模型在解码过程中对图像 tokens 的敏感性，正如在生成真实和幻觉实体时观察到的注意力峰值所证明的那样。

Method: 提出了一种名为 Memory-Driven Sparse Attention Matrix (MDSAM) 的新颖的免训练方法，该方法动态捕获和细化分配给每一层图像 tokens 的注意力。

Result: 在图像字幕和视觉问答等任务的多个基准上评估了 MDSAM，证明了其减少幻觉和提高可靠性的能力。

Conclusion: MDSAM 能够持续减少幻觉并提高可靠性，且与各种 LVLM 架构兼容，在无需额外训练或外部工具的情况下减轻幻觉。

Abstract: Hallucinations in large vision-language models (LVLMs) often stem from the
model's sensitivity to image tokens during decoding, as evidenced by attention
peaks observed when generating both real and hallucinated entities. To address
this, we propose Memory-Driven Sparse Attention Matrix (MDSAM) , a novel
training-free approach that dynamically captures and refines the attention
allocated to image tokens at each layer. MDSAM memorizes attention patterns and
activates updates through alignment during decoding, enhancing focus on
relevant image tokens while effectively reducing hallucinations. We evaluate
MDSAM on multiple benchmarks for tasks such as image captioning and visual
question answering, demonstrating its ability to consistently reduce
hallucinations and improve reliability. Compatible with various LVLM
architectures, MDSAM highlights its adaptability and effectiveness in
mitigating hallucinations without requiring additional training or external
tools.

</details>


### [76] [CSDN: A Context-Gated Self-Adaptive Detection Network for Real-Time Object Detection](https://arxiv.org/abs/2506.17679)
*Wei Haolin*

Main category: cs.CV

TL;DR: This paper introduces CSDN, a novel Transformer-based detection header that replaces traditional self-attention mechanisms with a gating mechanism to improve detection accuracy and global context modeling.


<details>
  <summary>Details</summary>
Motivation: Convolutional neural networks (CNNs) are often limited by limited receptive fields, which hinders their ability to capture global contextual information and significant information redundancies in DETR-inspired header network architecture.

Method: The Context-Gated Scale-Adaptive Detection Network (CSDN), a Transformer-based detection header inspired by natural language processing architecture and human visual perception, replacing the traditional stacked self-attention and cross-attention layers with a novel gating mechanism.

Result: CSDN provides more powerful global context modeling capabilities and can better adapt to objects of different sizes and structures.

Conclusion: The proposed CSDN detection head can directly replace the native heads of various CNN-based detectors, and only a few rounds of fine-tuning on the pre-training weights can significantly improve the detection accuracy.

Abstract: Convolutional neural networks (CNNs) have long been the cornerstone of target
detection, but they are often limited by limited receptive fields, which
hinders their ability to capture global contextual information. This paper
believes that the effective utilization of extracted features is as important
as the feature extraction process itself. We critically re-evaluated the
DETR-inspired header network architecture, questioning the indispensable nature
of its self-attention mechanism, and discovering significant information
redundancies. To solve these problems, we introduced the Context-Gated
Scale-Adaptive Detection Network (CSDN), a Transformer-based detection header
inspired by natural language processing architecture and human visual
perception. CSDN aims to efficiently utilize the characteristics of the CNN
backbone network by replacing the traditional stacked self-attention and
cross-attention layers with a novel gating mechanism. This mechanism enables
each region of interest (ROI) to adaptively select and combine feature
dimensions and scale information from multiple attention patterns. CSDN
provides more powerful global context modeling capabilities and can better
adapt to objects of different sizes and structures. Our proposed detection head
can directly replace the native heads of various CNN-based detectors, and only
a few rounds of fine-tuning on the pre-training weights can significantly
improve the detection accuracy, thus avoiding the need to achieve small
improvements. Various layer modules undergo extensive re-training.

</details>


### [77] [Domain Generalization using Action Sequences for Egocentric Action Recognition](https://arxiv.org/abs/2506.17685)
*Amirshayan Nasirimajd,Chiara Plizzari,Simone Alberto Peirone,Marco Ciccone,Giuseppe Averta,Barbara Caputo*

Main category: cs.CV

TL;DR: 提出SeqDG用于第一人称视角动作识别，通过序列重构和混合训练提高泛化能力，实验结果表明SeqDG在跨域和域内动作识别方面均有提升。


<details>
  <summary>Details</summary>
Motivation: 第一人称视角的人类活动识别对于机器人复制人类行为至关重要。当在训练期间未见过的环境中进行测试时，以自我为中心的动作识别模型的性能会显着下降。

Method: 提出了一种名为SeqDG的域泛化方法，引入了视觉-文本序列重构目标(SeqRec)，并使用来自不同领域的混合动作序列进行训练(SeqMix)。

Result: 在EPIC-KITCHENS-100数据集上，SeqDG在未见环境中的跨域动作识别方面实现了+2.4%的相对平均改进；在EGTEA数据集上，该模型在域内动作识别方面比SOTA提高了+0.6%的Top-1准确率。

Conclusion: SeqDG在跨域动作识别方面有提升，在域内动作识别方面达到了新的SOTA。

Abstract: Recognizing human activities from visual inputs, particularly through a
first-person viewpoint, is essential for enabling robots to replicate human
behavior. Egocentric vision, characterized by cameras worn by observers,
captures diverse changes in illumination, viewpoint, and environment. This
variability leads to a notable drop in the performance of Egocentric Action
Recognition models when tested in environments not seen during training. In
this paper, we tackle these challenges by proposing a domain generalization
approach for Egocentric Action Recognition. Our insight is that action
sequences often reflect consistent user intent across visual domains. By
leveraging action sequences, we aim to enhance the model's generalization
ability across unseen environments. Our proposed method, named SeqDG,
introduces a visual-text sequence reconstruction objective (SeqRec) that uses
contextual cues from both text and visual inputs to reconstruct the central
action of the sequence. Additionally, we enhance the model's robustness by
training it on mixed sequences of actions from different domains (SeqMix). We
validate SeqDG on the EGTEA and EPIC-KITCHENS-100 datasets. Results on
EPIC-KITCHENS-100, show that SeqDG leads to +2.4% relative average improvement
in cross-domain action recognition in unseen environments, and on EGTEA the
model achieved +0.6% Top-1 accuracy over SOTA in intra-domain action
recognition.

</details>


### [78] [SSAVSV: Towards Unified Model for Self-Supervised Audio-Visual Speaker Verification](https://arxiv.org/abs/2506.17694)
*Gnana Praveen Rajasekhar,Jahangir Alam*

Main category: cs.CV

TL;DR: 提出了一种基于对比学习的自监督学习框架，使用统一的视觉transformer backbone，在降低计算成本的同时，实现了有竞争力的说话人验证性能。


<details>
  <summary>Details</summary>
Motivation: 传统的说话人验证的视听方法依赖于大量的标签数据和分离的模态特定架构，计算成本高，限制了它们的可扩展性。

Method: 基于对比学习的自监督学习框架，采用非对称掩蔽和掩蔽数据建模，以获得稳健的视听特征表示。使用单一共享backbone，利用视觉transformer的多功能性，实现统一的自监督视听说话人验证。

Result: 该方法实现了有竞争力的性能，同时降低了计算成本。

Conclusion: 该方法在没有标签数据的情况下实现了有竞争力的性能，同时降低了计算成本。

Abstract: Conventional audio-visual methods for speaker verification rely on large
amounts of labeled data and separate modality-specific architectures, which is
computationally expensive, limiting their scalability. To address these
problems, we propose a self-supervised learning framework based on contrastive
learning with asymmetric masking and masked data modeling to obtain robust
audiovisual feature representations. In particular, we employ a unified
framework for self-supervised audiovisual speaker verification using a single
shared backbone for audio and visual inputs, leveraging the versatility of
vision transformers. The proposed unified framework can handle audio, visual,
or audiovisual inputs using a single shared vision transformer backbone during
training and testing while being computationally efficient and robust to
missing modalities. Extensive experiments demonstrate that our method achieves
competitive performance without labeled data while reducing computational costs
compared to traditional approaches.

</details>


### [79] [DreamJourney: Perpetual View Generation with Video Diffusion Models](https://arxiv.org/abs/2506.17705)
*Bo Pan,Yang Chen,Yingwei Pan,Ting Yao,Wei Chen,Tao Mei*

Main category: cs.CV

TL;DR: DreamJourney通过利用视频扩散模型和多模态大型语言模型，解决了现有方法在永久视图生成中缺乏 3D 感知和动态捕捉的问题，实现了更逼真和连贯的动态场景视图生成。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常利用预训练的文本到图像扩散模型来合成沿着相机移动的先前未见区域的新内容。然而，底层 2D 扩散模型缺乏 3D 感知，并导致扭曲的伪影。此外，它们仅限于生成静态 3D 场景的视图，而忽略了捕获动态 4D 世界中的对象移动。

Method: DreamJourney是一个两阶段框架，它利用视频扩散模型的世界模拟能力来触发一个新的永久场景视图生成任务，该任务同时具有相机移动和对象动态。

Result: DreamJourney，一个两阶段框架，利用视频扩散模型的世界模拟能力来触发一个新的永久场景视图生成任务，该任务同时具有相机移动和对象动态。具体来说，在第一阶段，DreamJourney 首先将输入图像提升到 3D 点云，并从特定的相机轨迹渲染一系列部分图像。然后，利用视频扩散模型作为生成先验来完成缺失的区域并增强序列中的视觉连贯性，从而生成符合 3D 场景和相机轨迹的跨视图一致的视频。同时，我们引入了两种简单而有效的策略（提前停止和视图填充）来进一步稳定生成过程并提高视觉质量。接下来，在第二阶段，DreamJourney 利用多模态大型语言模型来生成描述当前视图中对象移动的文本提示，并使用视频扩散模型来为当前视图添加对象移动的动画。第一阶段和第二阶段会周期性地重复，从而实现永久动态场景视图生成。

Conclusion: DreamJourney在定量和定性方面都优于最先进的方法， 能够实现永久动态场景视图生成。

Abstract: Perpetual view generation aims to synthesize a long-term video corresponding
to an arbitrary camera trajectory solely from a single input image. Recent
methods commonly utilize a pre-trained text-to-image diffusion model to
synthesize new content of previously unseen regions along camera movement.
However, the underlying 2D diffusion model lacks 3D awareness and results in
distorted artifacts. Moreover, they are limited to generating views of static
3D scenes, neglecting to capture object movements within the dynamic 4D world.
To alleviate these issues, we present DreamJourney, a two-stage framework that
leverages the world simulation capacity of video diffusion models to trigger a
new perpetual scene view generation task with both camera movements and object
dynamics. Specifically, in stage I, DreamJourney first lifts the input image to
3D point cloud and renders a sequence of partial images from a specific camera
trajectory. A video diffusion model is then utilized as generative prior to
complete the missing regions and enhance visual coherence across the sequence,
producing a cross-view consistent video adheres to the 3D scene and camera
trajectory. Meanwhile, we introduce two simple yet effective strategies (early
stopping and view padding) to further stabilize the generation process and
improve visual quality. Next, in stage II, DreamJourney leverages a multimodal
large language model to produce a text prompt describing object movements in
current view, and uses video diffusion model to animate current view with
object movements. Stage I and II are repeated recurrently, enabling perpetual
dynamic scene view generation. Extensive experiments demonstrate the
superiority of our DreamJourney over state-of-the-art methods both
quantitatively and qualitatively. Our project page:
https://dream-journey.vercel.app.

</details>


### [80] [Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models](https://arxiv.org/abs/2506.17707)
*Jihyun Kim,Junho Park,Kyeongbo Kong,Suk-Ju Kang*

Main category: cs.CV

TL;DR: Programmable-Room is a framework that uses language instructions to generate and edit 3D rooms by breaking down the task into simpler steps and using visual programming.


<details>
  <summary>Details</summary>
Motivation: To interactively generate and edit a 3D room mesh, given natural language instructions, with precise control over each attribute.

Method: The framework decomposes the task into simpler steps and uses visual programming (VP) with a large language model (LLM) to write a Python-like program. A pretrained large-scale diffusion model is used for texture generation, enhanced by optimizing the training objective with a 1D representation of a panorama scene obtained from bidirectional LSTM.

Result: The framework can generate and edit 3D room meshes and outperforms existing models quantitatively and qualitatively.

Conclusion: The Programmable-Room framework demonstrates flexibility in generating and editing 3D room meshes and outperforms existing models.

Abstract: We present Programmable-Room, a framework which interactively generates and
edits a 3D room mesh, given natural language instructions. For precise control
of a room's each attribute, we decompose the challenging task into simpler
steps such as creating plausible 3D coordinates for room meshes, generating
panorama images for the texture, constructing 3D meshes by integrating the
coordinates and panorama texture images, and arranging furniture. To support
the various decomposed tasks with a unified framework, we incorporate visual
programming (VP). VP is a method that utilizes a large language model (LLM) to
write a Python-like program which is an ordered list of necessary modules for
the various tasks given in natural language. We develop most of the modules.
Especially, for the texture generating module, we utilize a pretrained
large-scale diffusion model to generate panorama images conditioned on text and
visual prompts (i.e., layout, depth, and semantic map) simultaneously.
Specifically, we enhance the panorama image generation quality by optimizing
the training objective with a 1D representation of a panorama scene obtained
from bidirectional LSTM. We demonstrate Programmable-Room's flexibility in
generating and editing 3D room meshes, and prove our framework's superiority to
an existing model quantitatively and qualitatively. Project page is available
in https://jihyun0510.github.io/Programmable_Room_Page/.

</details>


### [81] [PDC-Net: Pattern Divide-and-Conquer Network for Pelvic Radiation Injury Segmentation](https://arxiv.org/abs/2506.17712)
*Xinyu Xiong,Wuteng Cao,Zihuang Wu,Lei Zhang,Chong Gao,Guanbin Li,Qiyuan Qin*

Main category: cs.CV

TL;DR: PDC-Net for PRI segmentation


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation of Pelvic Radiation Injury (PRI) from Magnetic Resonance Images (MRI) is crucial for more precise prognosis assessment and the development of personalized treatment plans. However, automated segmentation remains challenging due to factors such as complex organ morphologies and confusing context.

Method:  propose a novel Pattern Divide-and-Conquer Network (PDC-Net) for PRI segmentation. The core idea is to use different network modules to "divide" various local and global patterns and, through flexible feature selection, to "conquer" the Regions of Interest (ROI) during the decoding phase. Specifically, considering that our ROI often manifest as strip-like or circular-like structures in MR slices, we introduce a Multi-Direction Aggregation (MDA) module. Additionally, to mitigate the challenge of confusing context, we propose a Memory-Guided Context (MGC) module. Finally, we design an Adaptive Fusion Decoder (AFD) that dynamically selects features from different patterns based on the Mixture-of-Experts (MoE) framework, ultimately generating the final segmentation results.

Result: We evaluate our method on the first large-scale pelvic radiation injury dataset

Conclusion: The results demonstrate the superiority of our PDC-Net over existing approaches.

Abstract: Accurate segmentation of Pelvic Radiation Injury (PRI) from Magnetic
Resonance Images (MRI) is crucial for more precise prognosis assessment and the
development of personalized treatment plans. However, automated segmentation
remains challenging due to factors such as complex organ morphologies and
confusing context. To address these challenges, we propose a novel Pattern
Divide-and-Conquer Network (PDC-Net) for PRI segmentation. The core idea is to
use different network modules to "divide" various local and global patterns
and, through flexible feature selection, to "conquer" the Regions of Interest
(ROI) during the decoding phase. Specifically, considering that our ROI often
manifests as strip-like or circular-like structures in MR slices, we introduce
a Multi-Direction Aggregation (MDA) module. This module enhances the model's
ability to fit the shape of the organ by applying strip convolutions in four
distinct directions. Additionally, to mitigate the challenge of confusing
context, we propose a Memory-Guided Context (MGC) module. This module
explicitly maintains a memory parameter to track cross-image patterns at the
dataset level, thereby enhancing the distinction between global patterns
associated with the positive and negative classes. Finally, we design an
Adaptive Fusion Decoder (AFD) that dynamically selects features from different
patterns based on the Mixture-of-Experts (MoE) framework, ultimately generating
the final segmentation results. We evaluate our method on the first large-scale
pelvic radiation injury dataset, and the results demonstrate the superiority of
our PDC-Net over existing approaches.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [82] [Evaluating Generalization and Representation Stability in Small LMs via Prompting](https://arxiv.org/abs/2506.17289)
*Rahul Raja,Arpita Vats*

Main category: cs.AI

TL;DR: This paper compares prompting and fine-tuning for adapting small language models, finding differences in how they generalize knowledge, especially in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: While prompting is often favored for its parameter efficiency and flexibility, it remains unclear how robust this approach is in low-resource settings and under distributional shifts.

Method: comparative study of prompting and fine-tuning across task formats, prompt styles, and model scales, with a focus on their behavior in both in-distribution and out-of-distribution (OOD) settings. Beyond accuracy, we analyze the internal representations learned by each approach to assess the stability and abstraction of task-specific features.

Result: findings highlight critical differences in how small models internalize and generalize knowledge under different adaptation strategies.

Conclusion: This work offers practical guidance for model selection in low-data regimes and contributes empirical insight into the ongoing debate over prompting versus fine-tuning.

Abstract: We investigate the generalization capabilities of small language models under
two popular adaptation paradigms: few-shot prompting and supervised
fine-tuning. While prompting is often favored for its parameter efficiency and
flexibility, it remains unclear how robust this approach is in low-resource
settings and under distributional shifts. This paper presents a comparative
study of prompting and fine-tuning across task formats, prompt styles, and
model scales, with a focus on their behavior in both in-distribution and
out-of-distribution (OOD) settings.
  Beyond accuracy, we analyze the internal representations learned by each
approach to assess the stability and abstraction of task-specific features. Our
findings highlight critical differences in how small models internalize and
generalize knowledge under different adaptation strategies. This work offers
practical guidance for model selection in low-data regimes and contributes
empirical insight into the ongoing debate over prompting versus fine-tuning.
Code for the experiments is available at the following

</details>


### [83] [Individual Causal Inference with Structural Causal Model](https://arxiv.org/abs/2506.17300)
*Daniel T. Chang*

Main category: cs.AI

TL;DR: This paper proposes a new method for individual causal inference (ICI) using Structural Causal Models (SCM).


<details>
  <summary>Details</summary>
Motivation: Estimating ICE can be challenging due to the limited data available for individuals, and the fact that most causal inference methods are population-based. Structural Causal Model (SCM) is fundamentally population-based. However, exogenous variables (U) in SCM can encode individual variations and thus provide the mechanism for individualized population per specific individual characteristics / facts.

Method: We propose the indiv-operator, indiv(W), to formalize/represent the population individualization process, and the individual causal query, P(Y | indiv(W), do(X), Z), to formalize/represent ICI.

Result: propose ICI with SCM as a "rung 3" causal inference, because it involves "imagining" what would be the causal effect of a hypothetical intervention on an individual, given the individual's observed characteristics / facts.

Conclusion: ICI with SCM is inference on individual alternatives (possible), not individual counterfactuals (non-actual).

Abstract: Individual causal inference (ICI) uses causal inference methods to understand
and predict the effects of interventions on individuals, considering their
specific characteristics / facts. It aims to estimate individual causal effect
(ICE), which varies across individuals. Estimating ICE can be challenging due
to the limited data available for individuals, and the fact that most causal
inference methods are population-based. Structural Causal Model (SCM) is
fundamentally population-based. Therefore, causal discovery (structural
learning and parameter learning), association queries and intervention queries
are all naturally population-based. However, exogenous variables (U) in SCM can
encode individual variations and thus provide the mechanism for individualized
population per specific individual characteristics / facts. Based on this, we
propose ICI with SCM as a "rung 3" causal inference, because it involves
"imagining" what would be the causal effect of a hypothetical intervention on
an individual, given the individual's observed characteristics / facts.
Specifically, we propose the indiv-operator, indiv(W), to formalize/represent
the population individualization process, and the individual causal query, P(Y
| indiv(W), do(X), Z), to formalize/represent ICI. We show and argue that ICI
with SCM is inference on individual alternatives (possible), not individual
counterfactuals (non-actual).

</details>


### [84] [Resource Rational Contractualism Should Guide AI Alignment](https://arxiv.org/abs/2506.17434)
*Sydney Levine,Matija Franklin,Tan Zhi-Xuan,Secil Yanik Guyot,Lionel Wong,Daniel Kilov,Yejin Choi,Joshua B. Tenenbaum,Noah Goodman,Seth Lazar,Iason Gabriel*

Main category: cs.AI

TL;DR: AI systems can use RRC to make decisions that affect people and other AI agents.


<details>
  <summary>Details</summary>
Motivation: AI systems will soon have to navigate human environments and make decisions that affect people and other AI agents whose goals and values diverge. Contractualist alignment proposes grounding those decisions in agreements that diverse stakeholders would endorse under the right conditions, yet securing such agreement at scale remains costly and slow -- even for advanced AI.

Method: Resource-Rational Contractualism (RRC): a framework where AI systems approximate the agreements rational parties would form by drawing on a toolbox of normatively-grounded, cognitively-inspired heuristics that trade effort for accuracy.

Result: AI systems can use Resource-Rational Contractualism (RRC) to approximate agreements and operate efficiently.

Conclusion: An RRC-aligned agent would not only operate efficiently, but also be equipped to dynamically adapt to and interpret the ever-changing human social world.

Abstract: AI systems will soon have to navigate human environments and make decisions
that affect people and other AI agents whose goals and values diverge.
Contractualist alignment proposes grounding those decisions in agreements that
diverse stakeholders would endorse under the right conditions, yet securing
such agreement at scale remains costly and slow -- even for advanced AI. We
therefore propose Resource-Rational Contractualism (RRC): a framework where AI
systems approximate the agreements rational parties would form by drawing on a
toolbox of normatively-grounded, cognitively-inspired heuristics that trade
effort for accuracy. An RRC-aligned agent would not only operate efficiently,
but also be equipped to dynamically adapt to and interpret the ever-changing
human social world.

</details>


### [85] [Keeping Medical AI Healthy: A Review of Detection and Correction Methods for System Degradation](https://arxiv.org/abs/2506.17442)
*Hao Guan,David Bates,Li Zhou*

Main category: cs.AI

TL;DR: The paper reviews the causes of performance degradation in AI systems within healthcare and suggests methods for monitoring, detection, and correction to ensure long-term safe deployment.


<details>
  <summary>Details</summary>
Motivation: AI systems may experience performance degradation over time, due to factors such as shifting data distributions, changes in patient characteristics, evolving clinical protocols, and variations in data quality. These factors can compromise model reliability, posing safety concerns and increasing the likelihood of inaccurate predictions or adverse outcomes.

Method: reviewing common causes of performance degradation at both data and model levels, summarizing key techniques for detecting data and model drift, followed by an in-depth look at root cause analysis and Correction strategies are further reviewed, ranging from model retraining to test-time adaptation.

Result: This review presents a forward-looking perspective on monitoring and maintaining the 'health' of AI systems in healthcare. We highlight the urgent need for continuous performance monitoring, early degradation detection, and effective self-correction mechanisms.

Conclusion: This work aims to guide the development of reliable, robust medical AI systems capable of sustaining safe, long-term deployment in dynamic clinical settings.

Abstract: Artificial intelligence (AI) is increasingly integrated into modern
healthcare, offering powerful support for clinical decision-making. However, in
real-world settings, AI systems may experience performance degradation over
time, due to factors such as shifting data distributions, changes in patient
characteristics, evolving clinical protocols, and variations in data quality.
These factors can compromise model reliability, posing safety concerns and
increasing the likelihood of inaccurate predictions or adverse outcomes. This
review presents a forward-looking perspective on monitoring and maintaining the
"health" of AI systems in healthcare. We highlight the urgent need for
continuous performance monitoring, early degradation detection, and effective
self-correction mechanisms. The paper begins by reviewing common causes of
performance degradation at both data and model levels. We then summarize key
techniques for detecting data and model drift, followed by an in-depth look at
root cause analysis. Correction strategies are further reviewed, ranging from
model retraining to test-time adaptation. Our survey spans both traditional
machine learning models and state-of-the-art large language models (LLMs),
offering insights into their strengths and limitations. Finally, we discuss
ongoing technical challenges and propose future research directions. This work
aims to guide the development of reliable, robust medical AI systems capable of
sustaining safe, long-term deployment in dynamic clinical settings.

</details>


### [86] [OmniReflect: Discovering Transferable Constitutions for LLM agents via Neuro-Symbolic Reflections](https://arxiv.org/abs/2506.17449)
*Manasa Bharadwaj,Nikhil Verma,Kevin Ferreira*

Main category: cs.AI

TL;DR: OmniReflect, a hierarchical reflection framework, improves LLM agent performance on complex tasks by constructing a constitution from task experiences, showing significant gains in task success across various environments.


<details>
  <summary>Details</summary>
Motivation: Efforts to improve Large Language Model (LLM) agent performance on complex tasks often lack generalizable mechanisms for long-term learning and remain inefficient in dynamic environments.

Method: A hierarchical, reflection-driven framework that constructs a constitution, a compact set of guiding principles distilled from task experiences, to enhance the effectiveness and efficiency of an LLM agent. It operates in two modes: Self-sustaining and Co-operative, employing Neural, Symbolic, and NeuroSymbolic techniques.

Result: Major improvements in task success, with absolute gains of +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3% on PDDL in the Self-sustaining mode. Similar gains are seen in the Co-operative mode.

Conclusion: OmniReflect demonstrates robustness and effectiveness across environments and backbones, leading to significant improvements in task success.

Abstract: Efforts to improve Large Language Model (LLM) agent performance on complex
tasks have largely focused on fine-tuning and iterative self-correction.
However, these approaches often lack generalizable mechanisms for longterm
learning and remain inefficient in dynamic environments. We introduce
OmniReflect, a hierarchical, reflection-driven framework that constructs a
constitution, a compact set of guiding principles distilled from task
experiences, to enhance the effectiveness and efficiency of an LLM agent.
OmniReflect operates in two modes: Self-sustaining, where a single agent
periodically curates its own reflections during task execution, and
Co-operative, where a Meta-advisor derives a constitution from a small
calibration set to guide another agent. To construct these constitutional
principles, we employ Neural, Symbolic, and NeuroSymbolic techniques, offering
a balance between contextual adaptability and computational efficiency.
Empirical results averaged across models show major improvements in task
success, with absolute gains of +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3%
on PDDL in the Self-sustaining mode. Similar gains are seen in the Co-operative
mode, where a lightweight Qwen3-4B ReAct agent outperforms all Reflexion
baselines on BabyAI. These findings highlight the robustness and effectiveness
of OmniReflect across environments and backbones.

</details>


### [87] [From Unstructured Communication to Intelligent RAG: Multi-Agent Automation for Supply Chain Knowledge Bases](https://arxiv.org/abs/2506.17484)
*Yao Zhang,Zaixi Shang,Silpan Patel,Mikel Zuniga*

Main category: cs.AI

TL;DR: This paper introduces a novel offline-first methodology that transforms unstructured communications into a structured knowledge base, which reduces total volume to just 3.4% of original ticket data while improving quality.


<details>
  <summary>Details</summary>
Motivation: Critical knowledge such as system usage practices, troubleshooting workflows, and resolution techniques often remains buried within unstructured communications like support tickets, emails, and chat logs. While RAG systems aim to leverage such communications as a knowledge base, their effectiveness is limited by raw data challenges: support tickets are typically noisy, inconsistent, and incomplete, making direct retrieval suboptimal.

Method: LLMs-based multi-agent system orchestrating three specialized agents: Category Discovery for taxonomy creation, Categorization for ticket grouping, and Knowledge Synthesis for article generation.

Result: Our system creates a compact knowledge base - reducing total volume to just 3.4% of original ticket data while improving quality. Experiments demonstrate that our prebuilt knowledge base in RAG systems significantly outperforms traditional RAG implementations (48.74% vs. 38.60% helpful answers) and achieves a 77.4% reduction in unhelpful responses.

Conclusion: The system translates to substantial operational efficiency: reducing support workload, accelerating resolution times, and creating self-improving systems that automatically resolve approximately 50% of future supply chain tickets.

Abstract: Supply chain operations generate vast amounts of operational data; however,
critical knowledge such as system usage practices, troubleshooting workflows,
and resolution techniques often remains buried within unstructured
communications like support tickets, emails, and chat logs. While RAG systems
aim to leverage such communications as a knowledge base, their effectiveness is
limited by raw data challenges: support tickets are typically noisy,
inconsistent, and incomplete, making direct retrieval suboptimal. Unlike
existing RAG approaches that focus on runtime optimization, we introduce a
novel offline-first methodology that transforms these communications into a
structured knowledge base. Our key innovation is a LLMs-based multi-agent
system orchestrating three specialized agents: Category Discovery for taxonomy
creation, Categorization for ticket grouping, and Knowledge Synthesis for
article generation. Applying our methodology to real-world support tickets with
resolution notes and comments, our system creates a compact knowledge base -
reducing total volume to just 3.4% of original ticket data while improving
quality. Experiments demonstrate that our prebuilt knowledge base in RAG
systems significantly outperforms traditional RAG implementations (48.74% vs.
38.60% helpful answers) and achieves a 77.4% reduction in unhelpful responses.
By automating institutional knowledge capture that typically remains siloed in
experts' heads, our solution translates to substantial operational efficiency:
reducing support workload, accelerating resolution times, and creating
self-improving systems that automatically resolve approximately 50% of future
supply chain tickets. Our approach addresses a key gap in knowledge management
by transforming transient communications into structured, reusable knowledge
through intelligent offline processing rather than latency-inducing runtime
architectures.

</details>


### [88] [Kaleidoscopic Teaming in Multi Agent Simulations](https://arxiv.org/abs/2506.17514)
*Ninareh Mehrabi,Tharindu Kumarage,Kai-Wei Chang,Aram Galstyan,Rahul Gupta*

Main category: cs.AI

TL;DR: This paper introduces a kaleidoscopic teaming framework to evaluate safety risks in AI agents, addressing the shortcomings of existing methods, and identifies vulnerabilities in various models.


<details>
  <summary>Details</summary>
Motivation: Existing red teaming or safety evaluation frameworks fall short in evaluating safety risks in complex behaviors, thought processes and actions taken by agents, especially in multi-agent setups.

Method: The paper presents a new kaleidoscopic teaming framework that generates a diverse array of scenarios modeling real-world human societies and introduces new in-context optimization techniques to generate better scenarios for safety analysis. Appropriate metrics are also presented to measure the safety of agents.

Result: The paper evaluates the safety of agents in both single-agent and multi-agent setups and captures existing safety vulnerabilities in agents.

Conclusion: The paper identifies vulnerabilities in various models with respect to their safety in agentic use-cases by utilizing the kaleidoscopic teaming framework.

Abstract: Warning: This paper contains content that may be inappropriate or offensive.
  AI agents have gained significant recent attention due to their autonomous
tool usage capabilities and their integration in various real-world
applications. This autonomy poses novel challenges for the safety of such
systems, both in single- and multi-agent scenarios. We argue that existing red
teaming or safety evaluation frameworks fall short in evaluating safety risks
in complex behaviors, thought processes and actions taken by agents. Moreover,
they fail to consider risks in multi-agent setups where various vulnerabilities
can be exposed when agents engage in complex behaviors and interactions with
each other. To address this shortcoming, we introduce the term kaleidoscopic
teaming which seeks to capture complex and wide range of vulnerabilities that
can happen in agents both in single-agent and multi-agent scenarios. We also
present a new kaleidoscopic teaming framework that generates a diverse array of
scenarios modeling real-world human societies. Our framework evaluates safety
of agents in both single-agent and multi-agent setups. In single-agent setup,
an agent is given a scenario that it needs to complete using the tools it has
access to. In multi-agent setup, multiple agents either compete against or
cooperate together to complete a task in the scenario through which we capture
existing safety vulnerabilities in agents. We introduce new in-context
optimization techniques that can be used in our kaleidoscopic teaming framework
to generate better scenarios for safety analysis. Lastly, we present
appropriate metrics that can be used along with our framework to measure safety
of agents. Utilizing our kaleidoscopic teaming framework, we identify
vulnerabilities in various models with respect to their safety in agentic
use-cases.

</details>


### [89] [Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models](https://arxiv.org/abs/2506.17585)
*Yukun Huang,Sanxing Chen,Jian Pei,Manzil Zaheer,Bhuwan Dhingra*

Main category: cs.AI

TL;DR: This paper explores improving the reliability of citations in language models by revising the training process. They propose Active Indexing, which outperforms Passive Indexing in citation precision, achieving gains up to 30.2 percent.


<details>
  <summary>Details</summary>
Motivation: Language models' citations are often unreliable due to hallucination. Current systems insert citations by querying an external retriever at inference time, introducing latency, infrastructure dependence, and vulnerability to retrieval noise. We explore whether LLMs can be made to reliably attribute to the documents seen during pretraining without test-time retrieval by revising the training process.

Method: a two-stage process: (1) continual pretraining to bind facts to persistent document identifiers, and (2) instruction tuning to elicit citation behavior. We propose Active Indexing, which continually pretrains on synthetic QA pairs that restate each fact in diverse compositional forms and require bidirectional source-to-fact and fact-to-source generation.

Result: Simple Passive Indexing helps memorize verbatim text but fails on paraphrased or compositional facts. Active Indexing consistently outperforms Passive Indexing across all tasks and models, with citation precision gains up to 30.2 percent. Performance continues to improve as we scale the amount of augmented data.

Conclusion: Active Indexing consistently outperforms Passive Indexing across all tasks and models, with citation precision gains up to 30.2 percent. Performance continues to improve as we scale the amount of augmented data.

Abstract: Trustworthy language models should provide both correct and verifiable
answers. While language models can sometimes attribute their outputs to
pretraining data, their citations are often unreliable due to hallucination. As
a result, current systems insert citations by querying an external retriever at
inference time, introducing latency, infrastructure dependence, and
vulnerability to retrieval noise. We explore whether LLMs can be made to
reliably attribute to the documents seen during (continual)
pretraining--without test-time retrieval--by revising the training process. To
evaluate this, we release CitePretrainBench, a benchmark that mixes real-world
corpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and
probes both short-form (single fact) and long-form (multi-fact) citation tasks.
Our approach follows a two-stage process: (1) continual pretraining to bind
facts to persistent document identifiers, and (2) instruction tuning to elicit
citation behavior. We find that simple Passive Indexing, which appends an
identifier to each document, helps memorize verbatim text but fails on
paraphrased or compositional facts. Instead, we propose Active Indexing, which
continually pretrains on synthetic QA pairs that (1) restate each fact in
diverse compositional forms, and (2) require bidirectional source-to-fact and
fact-to-source generation, jointly teaching the model to generate content from
a cited source and to attribute its own answers. Experiments with Qwen2.5-7B
and 3B show that Active Indexing consistently outperforms Passive Indexing
across all tasks and models, with citation precision gains up to 30.2 percent.
Our ablation studies reveal that performance continues to improve as we scale
the amount of augmented data, showing a clear upward trend even at 16 times the
original token count.

</details>


### [90] [Taming the Untamed: Graph-Based Knowledge Retrieval and Reasoning for MLLMs to Conquer the Unknown](https://arxiv.org/abs/2506.17589)
*Bowen Wang*

Main category: cs.AI

TL;DR: 我们构建了一个多模态知识图，并设计了一系列具有挑战性的查询，以评估模型复杂知识检索和推理的能力。


<details>
  <summary>Details</summary>
Motivation: 知识的真正价值不仅在于积累，还在于其有效利用以征服未知事物的潜力。尽管最近的多模态大型语言模型 (MLLM) 表现出令人印象深刻的多模态能力，但由于相关知识有限，它们经常在很少遇到的特定领域任务中失败。

Method: 我们提出了一个多代理检索器，使模型能够自主搜索相关知识而无需额外的训练。

Result: 实验结果表明，我们的方法显著提高了 MLLM 的性能。

Conclusion: 该方法显著提高了 MLLM 的性能，为多模态知识增强推理提供了一个新的视角，并为未来的研究奠定了坚实的基础。

Abstract: The real value of knowledge lies not just in its accumulation, but in its
potential to be harnessed effectively to conquer the unknown. Although recent
multimodal large language models (MLLMs) exhibit impressing multimodal
capabilities, they often fail in rarely encountered domain-specific tasks due
to limited relevant knowledge. To explore this, we adopt visual game cognition
as a testbed and select Monster Hunter: World as the target to construct a
multimodal knowledge graph (MH-MMKG), which incorporates multi-modalities and
intricate entity relations. We also design a series of challenging queries
based on MH-MMKG to evaluate the models' ability for complex knowledge
retrieval and reasoning. Furthermore, we propose a multi-agent retriever that
enables a model to autonomously search relevant knowledge without additional
training. Experimental results show that our approach significantly enhances
the performance of MLLMs, providing a new perspective on multimodal
knowledge-augmented reasoning and laying a solid foundation for future
research.

</details>


### [91] [Measuring and Augmenting Large Language Models for Solving Capture-the-Flag Challenges](https://arxiv.org/abs/2506.17644)
*Zimo Ji,Daoyuan Wu,Wenyuan Jiang,Pingchuan Ma,Zongjie Li,Shuai Wang*

Main category: cs.AI

TL;DR: This paper introduces CTFKnow, a benchmark for evaluating LLMs' CTF knowledge, and proposes CTFAgent, an LLM-driven framework that improves CTF problem-solving performance.


<details>
  <summary>Details</summary>
Motivation: There is increasing interest in using LLMs to automate CTF challenge solving, but this requires multiple abilities. The paper highlights the importance of technical knowledge in solving CTF problems.

Method: The authors construct a focused benchmark, CTFKnow, with 3,992 questions to measure LLMs' performance in technical knowledge. They also propose CTFAgent, a framework with two-stage RAG and interactive Environmental Augmentation.

Result: LLMs possess substantial technical knowledge but struggle to apply it accurately to specific scenarios. CTFAgent achieves over 80% performance improvement on two CTF datasets and ranked in the top 23.6% in picoCTF2024.

Conclusion: The paper proposes CTFAgent, a novel LLM-driven framework with two new modules: two-stage Retrieval Augmented Generation (RAG) and interactive Environmental Augmentation. Experiments show CTFAgent achieves over 80% performance improvement on two popular CTF datasets and ranked in the top 23.6% of nearly 7,000 participating teams in picoCTF2024.

Abstract: Capture-the-Flag (CTF) competitions are crucial for cybersecurity education
and training. As large language models (LLMs) evolve, there is increasing
interest in their ability to automate CTF challenge solving. For example, DARPA
has organized the AIxCC competition since 2023 to advance AI-powered automated
offense and defense. However, this demands a combination of multiple abilities,
from knowledge to reasoning and further to actions. In this paper, we highlight
the importance of technical knowledge in solving CTF problems and deliberately
construct a focused benchmark, CTFKnow, with 3,992 questions to measure LLMs'
performance in this core aspect. Our study offers a focused and innovative
measurement of LLMs' capability in understanding CTF knowledge and applying it
to solve CTF challenges. Our key findings reveal that while LLMs possess
substantial technical knowledge, they falter in accurately applying this
knowledge to specific scenarios and adapting their strategies based on feedback
from the CTF environment.
  Based on insights derived from this measurement study, we propose CTFAgent, a
novel LLM-driven framework for advancing CTF problem-solving. CTFAgent
introduces two new modules: two-stage Retrieval Augmented Generation (RAG) and
interactive Environmental Augmentation, which enhance LLMs' technical knowledge
and vulnerability exploitation on CTF, respectively. Our experimental results
show that, on two popular CTF datasets, CTFAgent both achieves over 80%
performance improvement. Moreover, in the recent picoCTF2024 hosted by CMU,
CTFAgent ranked in the top 23.6% of nearly 7,000 participating teams. This
reflects the benefit of our measurement study and the potential of our
framework in advancing LLMs' capabilities in CTF problem-solving.

</details>


### [92] [PhysUniBench: An Undergraduate-Level Physics Reasoning Benchmark for Multimodal Models](https://arxiv.org/abs/2506.17667)
*Lintao Wang,Encheng Su,Jiaqi Liu,Pengze Li,Peng Xia,Jiabei Xiao,Wenlong Zhang,Xinnan Dai,Xi Chen,Yuan Meng,Mingyu Ding,Lei Bai,Wanli Ouyang,Shixiang Tang,Aoran Wang,Xinzhu Ma*

Main category: cs.AI

TL;DR: PhysUniBench, a large-scale multimodal benchmark, is presented to evaluate the reasoning capabilities of MLLMs on undergraduate-level physics problems. It reveals that current state-of-the-art models struggle with advanced physics reasoning, especially on multi-step problems and those requiring precise diagram interpretation.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methodologies show notable limitations in capturing the breadth and complexity of undergraduate-level physics, underscoring the need for more rigorous assessments.

Method: PhysUniBench consists of 3,304 physics questions spanning 8 major sub-disciplines of physics, each accompanied by one visual diagrams. The benchmark includes both open-ended and multiple-choice questions, systematically curated and difficulty-rated through an iterative model-in-the-loop process. The benchmark's construction involved a rigorous multi-stage process, including multiple roll-outs, expert-level evaluation, automated filtering of easily solved problems, and a nuanced difficulty grading system with five levels.

Result: GPT-4o mini achieves only about 34.2% accuracy in the proposed PhysUniBench. These results highlight that current MLLMs struggle with advanced physics reasoning, especially on multi-step problems and those requiring precise diagram interpretation.

Conclusion: current state-of-the-art models encounter substantial challenges in physics reasoning, especially on multi-step problems and those requiring precise diagram interpretation. By providing a broad and rigorous assessment tool, PhysUniBench aims to drive progress in AI for Science, encouraging the development of models with stronger physical reasoning, problem-solving skills, and multimodal understanding.

Abstract: Physics problem-solving is a challenging domain for large AI models,
requiring integration of conceptual understanding, mathematical reasoning, and
interpretation of physical diagrams. Current evaluation methodologies show
notable limitations in capturing the breadth and complexity of
undergraduate-level physics, underscoring the need for more rigorous
assessments. To this end, we present PhysUniBench, a large-scale multimodal
benchmark designed to evaluate and improve the reasoning capabilities of
multimodal large language models (MLLMs) specifically on undergraduate-level
physics problems. PhysUniBench consists of 3,304 physics questions spanning 8
major sub-disciplines of physics, each accompanied by one visual diagrams. The
benchmark includes both open-ended and multiple-choice questions,
systematically curated and difficulty-rated through an iterative
model-in-the-loop process. The benchmark's construction involved a rigorous
multi-stage process, including multiple roll-outs, expert-level evaluation,
automated filtering of easily solved problems, and a nuanced difficulty grading
system with five levels. Through extensive experiments, we observe that current
state-of-the-art models encounter substantial challenges in physics reasoning.
For example, GPT-4o mini achieves only about 34.2\% accuracy in the proposed
PhysUniBench. These results highlight that current MLLMs struggle with advanced
physics reasoning, especially on multi-step problems and those requiring
precise diagram interpretation. By providing a broad and rigorous assessment
tool, PhysUniBench aims to drive progress in AI for Science, encouraging the
development of models with stronger physical reasoning, problem-solving skills,
and multimodal understanding. The benchmark and evaluation scripts are
available at https://prismax-team.github.io/PhysUniBenchmark/.

</details>


### [93] [Beyond Syntax: Action Semantics Learning for App Agents](https://arxiv.org/abs/2506.17697)
*Bohan Tang,Dezhao Luo,Jingxuan Chen,Shaogang Gong,Jianye Hao,Jun Wang,Kun Shao*

Main category: cs.AI

TL;DR: 提出了Action Semantics Learning (ASL) 框架，以提高App代理的准确性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有的微调方法使用语法学习范式，导致分布外（OOD）漏洞。

Method: Action Semantics Learning (ASL)

Result: ASL在离线和在线智能手机App操作基准测试中显著提高了App代理的准确性和泛化性。

Conclusion: ASL显著提高了App代理的准确性和泛化性。

Abstract: The advent of Large Language Models (LLMs) enables the rise of App agents
that interpret user intent and operate smartphone Apps through actions such as
clicking and scrolling. While prompt-based solutions with closed LLM APIs show
promising ability, they incur heavy compute costs and external API dependency.
Fine-tuning smaller open-source LLMs solves these limitations. However, current
fine-tuning methods use a syntax learning paradigm that forces agents to
reproduce exactly the ground truth action strings, leading to
out-of-distribution (OOD) vulnerability. To fill this gap, we propose Action
Semantics Learning (ASL), a novel learning framework, where the learning
objective is capturing the semantics of the ground truth actions. Specifically,
inspired by the programming language theory, we define the action semantics for
App agents as the state transition induced by the action in the user interface.
With this insight, ASL employs a novel SEmantic Estimator (SEE) to compute a
semantic reward to train the App agents in generating actions aligned with the
semantics of ground truth actions, even when the syntactic forms differ. To
support the effectiveness of ASL, we theoretically demonstrate the superior
robustness of ASL for the OOD problem compared with the existing syntax
learning paradigm. Extensive experiments on offline and online smartphone App
operation benchmarks show that ASL significantly improves the accuracy and
generalisation of App agents over existing methods.

</details>


### [94] [AnyMAC: Cascading Flexible Multi-Agent Collaboration via Next-Agent Prediction](https://arxiv.org/abs/2506.17784)
*Song Wang,Zhen Tan,Zihan Chen,Shuang Zhou,Tianlong Chen,Jundong Li*

Main category: cs.AI

TL;DR: rethinks multi-agent coordination through a sequential structure, offering a significantly larger topology space for multi-agent communication


<details>
  <summary>Details</summary>
Motivation: existing methods largely rely on static or graph-based inter-agent topologies, lacking the potential adaptability and flexibility in communication

Method: propose a new framework that rethinks multi-agent coordination through a sequential structure rather than a graph structure, focusing on Next-Agent Prediction and Next-Context Selection

Result: construct task-adaptive communication pipelines that support both role flexibility and global information flow

Conclusion: achieves superior performance while substantially reducing communication overhead

Abstract: Recent progress in large language model (LLM)-based multi-agent collaboration
highlights the power of structured communication in enabling collective
intelligence. However, existing methods largely rely on static or graph-based
inter-agent topologies, lacking the potential adaptability and flexibility in
communication. In this work, we propose a new framework that rethinks
multi-agent coordination through a sequential structure rather than a graph
structure, offering a significantly larger topology space for multi-agent
communication. Our method focuses on two key directions: (1) Next-Agent
Prediction, which selects the most suitable agent role at each step, and (2)
Next-Context Selection (NCS), which enables each agent to selectively access
relevant information from any previous step. Together, these components
construct task-adaptive communication pipelines that support both role
flexibility and global information flow. Extensive evaluations across multiple
benchmarks demonstrate that our approach achieves superior performance while
substantially reducing communication overhead.

</details>


### [95] [Bayesian Social Deduction with Graph-Informed Language Models](https://arxiv.org/abs/2506.17788)
*Shahab Rahimirad,Guven Gergerli,Lucia Romero,Angela Qian,Matthew Lyle Olson,Simon Stepputtis,Joseph Campbell*

Main category: cs.AI

TL;DR: This paper presents a new approach to social reasoning for LLMs that combines probabilistic modeling with language understanding, achieving state-of-the-art results and defeating human players in the game Avalon.


<details>
  <summary>Details</summary>
Motivation: Social reasoning is a challenging task for large language models (LLMs), and current models require extensive test-time inference and degrade sharply when distilled to smaller variants.

Method: A hybrid reasoning framework that externalizes belief inference to a structured probabilistic model, while using an LLM for language understanding and interaction.

Result: The proposed approach achieves competitive performance with much larger models in Agent-Agent play and defeats human players in a controlled study, achieving a 67% win rate and receiving higher qualitative ratings.

Conclusion: This paper introduces a hybrid reasoning framework that combines a structured probabilistic model for belief inference with an LLM for language understanding and interaction. The approach achieves competitive performance with much larger models and defeats human players in a controlled study.

Abstract: Social reasoning - inferring unobservable beliefs and intentions from partial
observations of other agents - remains a challenging task for large language
models (LLMs). We evaluate the limits of current reasoning language models in
the social deduction game Avalon and find that while the largest models
demonstrate strong performance, they require extensive test-time inference and
degrade sharply when distilled to smaller, real-time-capable variants. To
address this, we introduce a hybrid reasoning framework that externalizes
belief inference to a structured probabilistic model, while using an LLM for
language understanding and interaction. Our approach achieves competitive
performance with much larger models in Agent-Agent play and, notably, is the
first language agent to defeat human players in a controlled study - achieving
a 67% win rate and receiving higher qualitative ratings than both reasoning
baselines and human teammates. We release code, models, and a dataset to
support future work on social reasoning in LLM agents, which can be found at
https://camp-lab-purdue.github.io/bayesian-social-deduction/

</details>


### [96] [Efficient Strategy Synthesis for MDPs via Hierarchical Block Decomposition](https://arxiv.org/abs/2506.17792)
*Alexandros Evangelidis,Gricel Vázquez,Simos Gerasimou*

Main category: cs.AI

TL;DR: This paper introduces an approach to accelerate policy synthesis in large MDPs by dynamically refining the MDP and iteratively selecting the most fragile MDP regions for refinement.


<details>
  <summary>Details</summary>
Motivation: Conventional policy synthesis methods fail to scale to large state spaces.

Method: dynamically refining the MDP and iteratively selecting the most fragile MDP regions for refinement

Result: significant performance improvements yielded by our approach compared to the leading probabilistic model checker PRISM (up to 2x)

Conclusion: The approach demonstrates significant performance improvements compared to the leading probabilistic model checker PRISM, offering a competitive solution for real-world policy synthesis tasks in larger MDPs.

Abstract: Software-intensive systems, such as software product lines and robotics,
utilise Markov decision processes (MDPs) to capture uncertainty and analyse
sequential decision-making problems. Despite the usefulness of conventional
policy synthesis methods, they fail to scale to large state spaces. Our
approach addresses this issue and accelerates policy synthesis in large MDPs by
dynamically refining the MDP and iteratively selecting the most fragile MDP
regions for refinement. This iterative procedure offers a balance between
accuracy and efficiency, as refinement occurs only when necessary. Through a
comprehensive empirical evaluation comprising diverse case studies and MDPs up
to 1M states, we demonstrate significant performance improvements yielded by
our approach compared to the leading probabilistic model checker PRISM (up to
2x), thus offering a very competitive solution for real-world policy synthesis
tasks in larger MDPs.

</details>


### [97] [Reflective Verbal Reward Design for Pluralistic Alignment](https://arxiv.org/abs/2506.17834)
*Carter Blair,Kate Larson,Edith Law*

Main category: cs.AI

TL;DR: Personalized reward modeling using reflective dialogues and language models improves accuracy and sample efficiency compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Human values are not homogeneous, and aggregating feedback into a single reward model risks disproportionately suppressing minority preferences in AI agent alignment.

Method: A novel reward modeling approach using language models to guide users through reflective dialogues, creating personalized dialogue histories used as context for individualized reward functions.

Result: Achieved a 9-12% improvement in accuracy over non-reflective verbal reward models and is more sample efficient than traditional supervised learning methods in studies with 30 participants.

Conclusion: The proposed method achieves a 9-12% improvement in accuracy over non-reflective verbal reward models and is more sample efficient than traditional supervised learning methods.

Abstract: AI agents are commonly aligned with "human values" through reinforcement
learning from human feedback (RLHF), where a single reward model is learned
from aggregated human feedback and used to align an agent's behavior. However,
human values are not homogeneous--different people hold distinct and sometimes
conflicting values. Aggregating feedback into a single reward model risks
disproportionately suppressing minority preferences. To address this, we
present a novel reward modeling approach for learning individualized reward
models. Our approach uses a language model to guide users through reflective
dialogues where they critique agent behavior and construct their preferences.
This personalized dialogue history, containing the user's reflections and
critiqued examples, is then used as context for another language model that
serves as an individualized reward function (what we call a "verbal reward
model") for evaluating new trajectories. In studies with 30 participants, our
method achieved a 9-12% improvement in accuracy over non-reflective verbal
reward models while being more sample efficient than traditional supervised
learning methods.

</details>


### [98] [Out of Control -- Why Alignment Needs Formal Control Theory (and an Alignment Control Stack)](https://arxiv.org/abs/2506.17846)
*Elija Perrier*

Main category: cs.AI

TL;DR: 本文提出形式最优控制理论应成为 AI 对齐研究的核心，通过对齐控制堆栈的层次结构，可以更好地理解和控制 AI 系统，为高级 AI 系统的安全性和可靠性提供更全面的框架。


<details>
  <summary>Details</summary>
Motivation: 形式最优控制理论应该成为 AI 对齐研究的核心，提供与流行的 AI 安全和保障方法不同的视角。虽然最近在 AI 安全和机械可解释性方面的工作已经推进了对齐的形式方法，但它们通常无法达到其他技术的控制框架所需的一般化。此外，还缺乏关于如何使不同的对齐/控制协议可互操作的研究。

Method: 引入对齐控制堆栈，该堆栈建立了一个分层对齐堆栈，识别每一层的测量和控制特性，以及不同层如何在形式上互操作。

Result: 通过形式最优控制的视角和分层堆栈的框架，可以更全面地理解和控制前沿 AI 系统，为政府和监管机构提供所需的保证，以确保 AI 技术的可持续社区利益。

Conclusion: 通过将对齐重新构建为形式最优控制原理，并在物理到社会技术层的分层堆栈中构建对齐，我们可以更好地理解控制前沿模型和代理 AI 系统的潜力。这样做将弥合已建立的、经过经验验证的最优控制方法与实际部署考虑之间的差距，从而创建一个更全面的对齐框架，从而增强我们处理高级 AI 系统的安全性和可靠性的方式。

Abstract: This position paper argues that formal optimal control theory should be
central to AI alignment research, offering a distinct perspective from
prevailing AI safety and security approaches. While recent work in AI safety
and mechanistic interpretability has advanced formal methods for alignment,
they often fall short of the generalisation required of control frameworks for
other technologies. There is also a lack of research into how to render
different alignment/control protocols interoperable. We argue that by recasting
alignment through principles of formal optimal control and framing alignment in
terms of hierarchical stack from physical to socio-technical layers according
to which controls may be applied we can develop a better understanding of the
potential and limitations for controlling frontier models and agentic AI
systems. To this end, we introduce an Alignment Control Stack which sets out a
hierarchical layered alignment stack, identifying measurement and control
characteristics at each layer and how different layers are formally
interoperable. We argue that such analysis is also key to the assurances that
will be needed by governments and regulators in order to see AI technologies
sustainably benefit the community. Our position is that doing so will bridge
the well-established and empirically validated methods of optimal control with
practical deployment considerations to create a more comprehensive alignment
framework, enhancing how we approach safety and reliability for advanced AI
systems.

</details>


### [99] [Towards Robust Fact-Checking: A Multi-Agent System with Advanced Evidence Retrieval](https://arxiv.org/abs/2506.17878)
*Tam Trinh,Manh Nguyen,Truong-Son Hy*

Main category: cs.AI

TL;DR: This paper introduces a multi-agent system for automated fact-checking that's more accurate, efficient, and transparent than existing methods.


<details>
  <summary>Details</summary>
Motivation: The rapid spread of misinformation necessitates scalable fact-checking solutions, but existing automated approaches have limitations in handling complex claims, ensuring source credibility, and maintaining transparency.

Method: A novel multi-agent system with four specialized agents for claim decomposition, query generation, evidence retrieval, and verdict prediction.

Result: The system achieves a 12.3% improvement in Macro F1-score on benchmark datasets (FEVEROUS, HOVER, SciFact).

Conclusion: The proposed multi-agent system improves fact-checking accuracy, efficiency, and transparency, achieving a 12.3% Macro F1-score improvement.

Abstract: The rapid spread of misinformation in the digital era poses significant
challenges to public discourse, necessitating robust and scalable fact-checking
solutions. Traditional human-led fact-checking methods, while credible,
struggle with the volume and velocity of online content, prompting the
integration of automated systems powered by Large Language Models (LLMs).
However, existing automated approaches often face limitations, such as handling
complex claims, ensuring source credibility, and maintaining transparency. This
paper proposes a novel multi-agent system for automated fact-checking that
enhances accuracy, efficiency, and explainability. The system comprises four
specialized agents: an Input Ingestion Agent for claim decomposition, a Query
Generation Agent for formulating targeted subqueries, an Evidence Retrieval
Agent for sourcing credible evidence, and a Verdict Prediction Agent for
synthesizing veracity judgments with human-interpretable explanations.
Evaluated on benchmark datasets (FEVEROUS, HOVER, SciFact), the proposed system
achieves a 12.3% improvement in Macro F1-score over baseline methods. The
system effectively decomposes complex claims, retrieves reliable evidence from
trusted sources, and generates transparent explanations for verification
decisions. Our approach contributes to the growing field of automated
fact-checking by providing a more accurate, efficient, and transparent
verification methodology that aligns with human fact-checking practices while
maintaining scalability for real-world applications. Our source code is
available at https://github.com/HySonLab/FactAgent

</details>


### [100] [Leveraging Large Language Model for Intelligent Log Processing and Autonomous Debugging in Cloud AI Platforms](https://arxiv.org/abs/2506.17900)
*Cheng Ji,Huaiying Luo*

Main category: cs.AI

TL;DR: This paper introduces LLM-ID, an LLM-based intelligent debugger for cloud systems that improves fault location accuracy by 16.2%.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity and rapid expansion of AI systems in cloud platforms generate massive, unstructured, and semantically ambiguous log data, posing challenges to fault location and system self-repair.

Method: The paper proposes an intelligent log processing and automatic debugging framework based on Large Language Model (LLM), named Intelligent Debugger (LLM-ID). It integrates a multi-stage semantic inference mechanism, dynamic log structuring, unsupervised clustering, a fine-tuned LLM with multi-round attention, and a reinforcement learning-based policy-guided recovery planner.

Result: LLM-ID improves fault location accuracy by 16.2% compared to existing rule engine or traditional log analysis systems.

Conclusion: The proposed LLM-ID model improves fault location accuracy by 16.2% compared to existing methods, demonstrating stronger semantic understanding, continuous learning ability, and heterogeneous environment adaptability.

Abstract: With the increasing complexity and rapid expansion of the scale of AI systems
in cloud platforms, the log data generated during system operation is massive,
unstructured, and semantically ambiguous, which brings great challenges to
fault location and system self-repair. In order to solve this problem, this
paper proposes an intelligent log processing and automatic debugging framework
based on Large Language Model (LLM), named Intelligent Debugger (LLM-ID). This
method is extended on the basis of the existing pre-trained Transformer model,
and integrates a multi-stage semantic inference mechanism to realize the
context understanding of system logs and the automatic reconstruction of fault
chains. Firstly, the system log is dynamically structured, and the unsupervised
clustering and embedding mechanism is used to extract the event template and
semantic schema. Subsequently, the fine-tuned LLM combined with the multi-round
attention mechanism to perform contextual reasoning on the log sequence to
generate potential fault assumptions and root cause paths. Furthermore, this
paper introduces a reinforcement learning-based policy-guided recovery planner,
which is driven by the remediation strategy generated by LLM to support dynamic
decision-making and adaptive debugging in the cloud environment. Compared with
the existing rule engine or traditional log analysis system, the proposed model
has stronger semantic understanding ability, continuous learning ability and
heterogeneous environment adaptability. Experiments on the cloud platform log
dataset show that LLM-ID improves the fault location accuracy by 16.2%, which
is significantly better than the current mainstream methods

</details>


### [101] [Learning, Reasoning, Refinement: A Framework for Kahneman's Dual-System Intelligence in GUI Agents](https://arxiv.org/abs/2506.17913)
*Jinjie Wei,Jiyao Liu,Lihao Liu,Ming Hu,Junzhi Ning,Mingcheng Li,Weijie Yin,Junjun He,Xiao Liang,Chao Feng,Dingkang Yang*

Main category: cs.AI

TL;DR: CogniGUI通过结合omni parser引擎和GRPO grounding agent，克服了现有GUI agent系统的局限性，实现了类似人类行为的自适应学习GUI自动化。


<details>
  <summary>Details</summary>
Motivation: 现有的agent系统主要依赖于反复试验的决策，而不是渐进式的推理，因此缺乏从交互式遭遇中学习和适应的能力。其次，这些系统使用过于简单的单步精度指标进行评估，这些指标不能充分反映真实GUI交互的复杂性。

Method: 结合了两个主要组件：(1) 通过快速视觉语义分析对GUI元素进行即时分层解析的omni parser引擎，以识别可操作的组件，以及 (2) 基于组的相对策略优化(GRPO) grounding agent，该agent使用独特的相对奖励系统评估多个交互路径，从而促进最小和高效的操作路径。

Result: CogniGUI在当前GUI定位基准和新提出的基准测试中均超过了最先进的方法。

Conclusion: CogniGUI在当前GUI定位基准和新提出的基准测试中均超过了最先进的方法。

Abstract: Graphical User Interface (GUI) agents have made significant progress in
automating digital tasks through the utilization of computer vision and
language models. Nevertheless, existing agent systems encounter notable
limitations. Firstly, they predominantly depend on trial and error decision
making rather than progressive reasoning, thereby lacking the capability to
learn and adapt from interactive encounters. Secondly, these systems are
assessed using overly simplistic single step accuracy metrics, which do not
adequately reflect the intricate nature of real world GUI interactions. In this
paper, we present CogniGUI, a cognitive framework developed to overcome these
limitations by enabling adaptive learning for GUI automation resembling
human-like behavior. Inspired by Kahneman's Dual Process Theory, our approach
combines two main components: (1) an omni parser engine that conducts immediate
hierarchical parsing of GUI elements through quick visual semantic analysis to
identify actionable components, and (2) a Group based Relative Policy
Optimization (GRPO) grounding agent that assesses multiple interaction paths
using a unique relative reward system, promoting minimal and efficient
operational routes. This dual-system design facilitates iterative ''exploration
learning mastery'' cycles, enabling the agent to enhance its strategies over
time based on accumulated experience. Moreover, to assess the generalization
and adaptability of agent systems, we introduce ScreenSeek, a comprehensive
benchmark that includes multi application navigation, dynamic state
transitions, and cross interface coherence, which are often overlooked
challenges in current benchmarks. Experimental results demonstrate that
CogniGUI surpasses state-of-the-art methods in both the current GUI grounding
benchmarks and our newly proposed benchmark.

</details>


### [102] [Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective](https://arxiv.org/abs/2506.17930)
*Jianyu Wang,Zhiqiang Hu,Lidong Bing*

Main category: cs.AI

TL;DR: 该论文提出了一种通过修剪随机演示来改进LLM提示的新方法，并提出了一个名为PromptQuine的框架来自动搜索有效的修剪策略。


<details>
  <summary>Details</summary>
Motivation: 传统观点优先考虑精心设计的指令和演示以进行上下文学习 (ICL)，但本文对此提出了挑战。

Method: 该论文提出了一个自发现提示优化框架PromptQuine，这是一个进化搜索框架，它仅使用低数据机制自动搜索修剪策略。

Result: 该论文提出的方法在分类、多项选择问答、生成和数学推理任务中表现出有效性，同时实现了不错的运行时效率。

Conclusion: 该论文提出了一种新颖的提示设计范式，通过将随机演示修剪成看似不连贯的“乱语”，可以显著提高各种任务的性能，优于现有的prompt优化技术。

Abstract: We propose a novel prompt design paradigm that challenges conventional wisdom
in large language model (LLM) prompting. While conventional wisdom prioritizes
well-crafted instructions and demonstrations for in-context learning (ICL), we
show that pruning random demonstrations into seemingly incoherent "gibberish"
can remarkably improve performance across diverse tasks. Notably, the
"gibberish" always matches or surpasses state-of-the-art automatic prompt
optimization techniques, achieving substantial gains regardless of LLM
alignment. Nevertheless, discovering an effective pruning strategy is
non-trivial, as existing attribution methods and prompt compression algorithms
fail to deliver robust results, let alone human intuition. In terms of this, we
propose a self-discover prompt optimization framework, PromptQuine, an
evolutionary search framework that automatically searches for the pruning
strategy by itself using only low-data regimes. Much like the emergent
complexity in nature--such as symbiosis and self-organization--arising in
response to resource constraints, our framework evolves and refines
unconventional yet highly effective prompts by leveraging only the tokens
present within the context. We demonstrate its effectiveness across
classification, multi-choice question answering, generation and math reasoning
tasks across LLMs, while achieving decent runtime efficiency. We hope our
findings can guide mechanistic studies on in-context learning, and provide a
call to action, to pave the way for more open-ended search algorithms for more
effective LLM prompting.

</details>


### [103] [medicX-KG: A Knowledge Graph for Pharmacists' Drug Information Needs](https://arxiv.org/abs/2506.17959)
*Lizzy Farrugia,Lilian M. Azzopardi,Jeremy Debattista,Charlie Abela*

Main category: cs.AI

TL;DR: This paper presents medicX-KG, a pharmacist-oriented knowledge graph supporting clinical and regulatory decisions. It integrates data from three sources and effectively supports queries about drug availability, interactions, adverse reactions, and therapeutic classes.


<details>
  <summary>Details</summary>
Motivation: The role of pharmacists is evolving from medicine dispensing to delivering comprehensive pharmaceutical services. Central to this shift is access to accurate, up-to-date medicinal product information. The KG tackles the absence of a unified national drug repository, reducing pharmacists' reliance on fragmented sources.

Method: Knowledge graph construction, including data extraction, ontology design, and semantic mapping.

Result: medicX-KG, a pharmacist-oriented knowledge graph supporting clinical and regulatory decisions. It integrates data from three sources, including, the British National Formulary (BNF), DrugBank, and the Malta Medicines Authority (MMA).

Conclusion: medicX-KG effectively supports queries about drug availability, interactions, adverse reactions, and therapeutic classes.

Abstract: The role of pharmacists is evolving from medicine dispensing to delivering
comprehensive pharmaceutical services within multidisciplinary healthcare
teams. Central to this shift is access to accurate, up-to-date medicinal
product information supported by robust data integration. Leveraging artificial
intelligence and semantic technologies, Knowledge Graphs (KGs) uncover hidden
relationships and enable data-driven decision-making. This paper presents
medicX-KG, a pharmacist-oriented knowledge graph supporting clinical and
regulatory decisions. It forms the semantic layer of the broader medicX
platform, powering predictive and explainable pharmacy services. medicX-KG
integrates data from three sources, including, the British National Formulary
(BNF), DrugBank, and the Malta Medicines Authority (MMA) that addresses Malta's
regulatory landscape and combines European Medicines Agency alignment with
partial UK supply dependence. The KG tackles the absence of a unified national
drug repository, reducing pharmacists' reliance on fragmented sources. Its
design was informed by interviews with practicing pharmacists to ensure
real-world applicability. We detail the KG's construction, including data
extraction, ontology design, and semantic mapping. Evaluation demonstrates that
medicX-KG effectively supports queries about drug availability, interactions,
adverse reactions, and therapeutic classes. Limitations, including missing
detailed dosage encoding and real-time updates, are discussed alongside
directions for future enhancements.

</details>


### [104] [Graphs Meet AI Agents: Taxonomy, Progress, and Future Opportunities](https://arxiv.org/abs/2506.18019)
*Yuanchen Bei,Weizhi Zhang,Siwen Wang,Weizhi Chen,Sheng Zhou,Hao Chen,Yong Li,Jiajun Bu,Shirui Pan,Yizhou Yu,Irwin King,Fakhri Karray,Philip S. Yu*

Main category: cs.AI

TL;DR: Graphs can help AI agents handle complex tasks by better organizing data. This paper reviews how graphs are used with AI agents and suggests future research directions.


<details>
  <summary>Details</summary>
Motivation: AI agents need to plan, execute, maintain memory, and coordinate with others to accomplish complex real-world tasks, which involves dealing with intricate information, operations, and interactions. Data structurization, especially using graphs, can help agents understand and process data more effectively.

Method: A systematic review of the integration of graph techniques with core agent functionalities in AI agents.

Result: The survey presents a systematic review of how graphs can empower AI agents.

Conclusion: This survey reviews how graphs can empower AI agents, explores the integration of graph techniques with core agent functionalities, highlights applications, and identifies future research directions.

Abstract: AI agents have experienced a paradigm shift, from early dominance by
reinforcement learning (RL) to the rise of agents powered by large language
models (LLMs), and now further advancing towards a synergistic fusion of RL and
LLM capabilities. This progression has endowed AI agents with increasingly
strong abilities. Despite these advances, to accomplish complex real-world
tasks, agents are required to plan and execute effectively, maintain reliable
memory, and coordinate smoothly with other agents. Achieving these capabilities
involves contending with ever-present intricate information, operations, and
interactions. In light of this challenge, data structurization can play a
promising role by transforming intricate and disorganized data into
well-structured forms that agents can more effectively understand and process.
In this context, graphs, with their natural advantage in organizing, managing,
and harnessing intricate data relationships, present a powerful data paradigm
for structurization to support the capabilities demanded by advanced AI agents.
To this end, this survey presents a first systematic review of how graphs can
empower AI agents. Specifically, we explore the integration of graph techniques
with core agent functionalities, highlight notable applications, and identify
prospective avenues for future research. By comprehensively surveying this
burgeoning intersection, we hope to inspire the development of next-generation
AI agents equipped to tackle increasingly sophisticated challenges with graphs.
Related resources are collected and continuously updated for the community in
the Github link.

</details>


### [105] [jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval](https://arxiv.org/abs/2506.18902)
*Michael Günther,Saba Sturua,Mohammad Kalim Akram,Isabelle Mohr,Andrei Ungureanu,Sedigheh Eslami,Scott Martens,Bo Wang,Nan Wang,Han Xiao*

Main category: cs.AI

TL;DR: jina-embeddings-v4是一种多模态嵌入模型，它统一了文本和图像表示，并在各种检索场景中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 统一文本和图像表示。

Method: 一种新颖的架构，支持后期交互风格中的单向量和多向量嵌入。

Result: jina-embeddings-v4在单模态和跨模态检索任务上实现了最先进的性能。

Conclusion: jina-embeddings-v4在单模态和跨模态检索任务上实现了最先进的性能，尤其是在处理富含视觉内容（如图表、图表、图表和混合媒体格式）方面表现出色。

Abstract: We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding
model that unifies text and image representations through a novel architecture
supporting both single-vector and multi-vector embeddings in the late
interaction style. The model incorporates task-specific Low-Rank Adaptation
(LoRA) adapters to optimize performance across diverse retrieval scenarios,
including query-based information retrieval, cross-modal semantic similarity,
and programming code search. Comprehensive evaluations demonstrate that
jina-embeddings-v4 achieves state-of-the-art performance on both single- modal
and cross-modal retrieval tasks, with particular strength in processing
visually rich content such as tables, charts, diagrams, and mixed-media
formats. To facilitate evaluation of this capability, we also introduce
Jina-VDR, a novel benchmark specifically designed for visually rich image
retrieval.

</details>


### [106] [Action Language BC+](https://arxiv.org/abs/2506.18044)
*Joseph Babb,Joohyung Lee*

Main category: cs.AI

TL;DR: This paper introduces a new action language called BC+ that bridges the gap between action languages and modern ASP, leveraging stable model semantics and ASP solver capabilities.


<details>
  <summary>Details</summary>
Motivation: close the gap between action languages and the modern ASP language,The form of answer set programs considered in the earlier work is quite limited in comparison with the modern Answer Set Programming (ASP) language, which allows several useful constructs for knowledge representation, such as choice rules, aggregates, and abstract constraint atoms.

Method: define the semantics of BC+ in terms of general stable model semantics for propositional formulas, under which many modern ASP language constructs can be identified with shorthands for propositional formulas.

Result: We propose a new action language called BC+

Conclusion: Language BC+ is sufficiently expressive to encompass the best features of other action languages, such as languages B, C, C+, and BC. Computational methods available in ASP solvers are readily applicable to compute BC+, which led to an implementation of the language by extending system cplus2asp.

Abstract: Action languages are formal models of parts of natural language that are
designed to describe effects of actions. Many of these languages can be viewed
as high level notations of answer set programs structured to represent
transition systems. However, the form of answer set programs considered in the
earlier work is quite limited in comparison with the modern Answer Set
Programming (ASP) language, which allows several useful constructs for
knowledge representation, such as choice rules, aggregates, and abstract
constraint atoms. We propose a new action language called BC+, which closes the
gap between action languages and the modern ASP language. The main idea is to
define the semantics of BC+ in terms of general stable model semantics for
propositional formulas, under which many modern ASP language constructs can be
identified with shorthands for propositional formulas. Language BC+ turns out
to be sufficiently expressive to encompass the best features of other action
languages, such as languages B, C, C+, and BC. Computational methods available
in ASP solvers are readily applicable to compute BC+, which led to an
implementation of the language by extending system cplus2asp.

</details>


### [107] [Weighted Assumption Based Argumentation to reason about ethical principles and actions](https://arxiv.org/abs/2506.18056)
*Paolo Baldi,Fabio Aurelio D'Asaro,Abeer Dyoub,Francesca Alessandra Lisi*

Main category: cs.AI

TL;DR: ABA is augmented with weighted argumentation. Weights are assigned to arguments, and the attack weights between arguments are derived. The proposal is illustrated through examples in ethical reasoning and implemented using Answer Set Programming.


<details>
  <summary>Details</summary>
Motivation: To enhance Assumption Based Argumentation (ABA) with weighted argumentation.

Method: We augment Assumption Based Argumentation with weighted argumentation, assigning weights to arguments and deriving the weight of attacks between ABA arguments.

Result: Proposal illustrated through running examples in the field of ethical reasoning.

Conclusion: We present an implementation based on Answer Set Programming.

Abstract: We augment Assumption Based Argumentation (ABA for short) with weighted
argumentation. In a nutshell, we assign weights to arguments and then derive
the weight of attacks between ABA arguments. We illustrate our proposal through
running examples in the field of ethical reasoning, and present an
implementation based on Answer Set Programming.

</details>


### [108] [Deep Research Agents: A Systematic Examination And Roadmap](https://arxiv.org/abs/2506.18096)
*Yuxuan Huang,Yihang Chen,Haozheng Zhang,Kang Li,Meng Fang,Linyi Yang,Xiaoguang Li,Lifeng Shang,Songcen Xu,Jianye Hao,Kun Shao,Jun Wang*

Main category: cs.AI

TL;DR: This paper analyzes the technologies and architectures of Deep Research agents, proposes a taxonomy for classifying them, evaluates current benchmarks, and outlines future research directions.


<details>
  <summary>Details</summary>
Motivation: The rapid progress of Large Language Models (LLMs) has given rise to a new category of autonomous AI systems, referred to as Deep Research (DR) agents, tackle complex, multi-turn informational research tasks

Method: conduct a detailed analysis of the foundational technologies and architectural components that constitute Deep Research agents. propose a taxonomy that differentiates between static and dynamic workflows, and classify agent architectures based on planning strategies and agent composition, including single-agent and multi-agent configurations

Result: reviewing information acquisition strategies, contrasting API-based retrieval methods with browser-based exploration. examine modular tool-use frameworks, including code execution, multimodal input processing, and the integration of Model Context Protocols (MCPs) to support extensibility and ecosystem development. provide a critical evaluation of current benchmarks, highlighting key limitations such as restricted access to external knowledge, sequential execution inefficiencies, and misalignment between evaluation metrics and the practical objectives of DR agents

Conclusion: outlines open challenges and promising directions for future research

Abstract: The rapid progress of Large Language Models (LLMs) has given rise to a new
category of autonomous AI systems, referred to as Deep Research (DR) agents.
These agents are designed to tackle complex, multi-turn informational research
tasks by leveraging a combination of dynamic reasoning, adaptive long-horizon
planning, multi-hop information retrieval, iterative tool use, and the
generation of structured analytical reports. In this paper, we conduct a
detailed analysis of the foundational technologies and architectural components
that constitute Deep Research agents. We begin by reviewing information
acquisition strategies, contrasting API-based retrieval methods with
browser-based exploration. We then examine modular tool-use frameworks,
including code execution, multimodal input processing, and the integration of
Model Context Protocols (MCPs) to support extensibility and ecosystem
development. To systematize existing approaches, we propose a taxonomy that
differentiates between static and dynamic workflows, and we classify agent
architectures based on planning strategies and agent composition, including
single-agent and multi-agent configurations. We also provide a critical
evaluation of current benchmarks, highlighting key limitations such as
restricted access to external knowledge, sequential execution inefficiencies,
and misalignment between evaluation metrics and the practical objectives of DR
agents. Finally, we outline open challenges and promising directions for future
research. A curated and continuously updated repository of DR agent research is
available at: {https://github.com/ai-agents-2030/awesome-deep-research-agent}.

</details>


### [109] [Decentralized Consensus Inference-based Hierarchical Reinforcement Learning for Multi-Constrained UAV Pursuit-Evasion Game](https://arxiv.org/abs/2506.18126)
*Xiang Yuming,Li Sizhao,Li Rongpeng,Zhao Zhifeng,Zhang Honggang*

Main category: cs.AI

TL;DR: This paper proposes CI-HRL, a two-level reinforcement learning framework, for the Cooperative Evasion and Formation Coverage (CEFC) task. It uses Consensus-oriented Multi-Agent Communication (ConsMAC) for high-level policy and Alternative Training-based Multi-agent proximal policy optimization (AT-M) and policy distillation for low-level control. The results show CI-HRL enhances swarm's collaborative evasion and task completion capabilities.


<details>
  <summary>Details</summary>
Motivation: The Cooperative Evasion and Formation Coverage (CEFC) task, where the UAV swarm aims to maximize formation coverage across multiple target zones while collaboratively evading predators, belongs to one of the most challenging issues in MC-PEG, especially under communication-limited constraints. This multifaceted problem, which intertwines responses to obstacles, adversaries, target zones, and formation dynamics, brings up significant high-dimensional complications in locating a solution.

Method: a novel two-level framework (i.e., Consensus Inference-based Hierarchical Reinforcement Learning (CI-HRL))

Result: experimental results, including the high-fidelity software-in-the-loop (SITL) simulations, validate that CI-HRL provides a superior solution with enhanced swarm's collaborative evasion and task completion capabilities.

Conclusion: CI-HRL provides a superior solution with enhanced swarm's collaborative evasion and task completion capabilities.

Abstract: Multiple quadrotor unmanned aerial vehicle (UAV) systems have garnered
widespread research interest and fostered tremendous interesting applications,
especially in multi-constrained pursuit-evasion games (MC-PEG). The Cooperative
Evasion and Formation Coverage (CEFC) task, where the UAV swarm aims to
maximize formation coverage across multiple target zones while collaboratively
evading predators, belongs to one of the most challenging issues in MC-PEG,
especially under communication-limited constraints. This multifaceted problem,
which intertwines responses to obstacles, adversaries, target zones, and
formation dynamics, brings up significant high-dimensional complications in
locating a solution. In this paper, we propose a novel two-level framework
(i.e., Consensus Inference-based Hierarchical Reinforcement Learning (CI-HRL)),
which delegates target localization to a high-level policy, while adopting a
low-level policy to manage obstacle avoidance, navigation, and formation.
Specifically, in the high-level policy, we develop a novel multi-agent
reinforcement learning module, Consensus-oriented Multi-Agent Communication
(ConsMAC), to enable agents to perceive global information and establish
consensus from local states by effectively aggregating neighbor messages.
Meanwhile, we leverage an Alternative Training-based Multi-agent proximal
policy optimization (AT-M) and policy distillation to accomplish the low-level
control. The experimental results, including the high-fidelity
software-in-the-loop (SITL) simulations, validate that CI-HRL provides a
superior solution with enhanced swarm's collaborative evasion and task
completion capabilities.

</details>


### [110] [SE-Merging: A Self-Enhanced Approach for Dynamic Model Merging](https://arxiv.org/abs/2506.18135)
*Zijun Chen,Zhanpeng Zhou,Bo Zhang,Weinan Zhang,Xi Sun,Junchi Yan*

Main category: cs.AI

TL;DR: This paper studies the mechanism behind model merging and proposes SE-Merging, a self-enhanced model merging framework that dynamically identifies the corresponding task for each sample and then adaptively rescales the merging coefficients to further enhance task-specific expertise in the merged model. SE-Merging achieves significant performance improvements without additional training.


<details>
  <summary>Details</summary>
Motivation: Model merging has gained increasing attention due to its intriguing property: interpolating the parameters of different task-specific fine-tuned models leads to multi-task abilities. However, despite its empirical success, the underlying mechanisms of model merging remain poorly understood.

Method: The paper analyzes model merging from a representation perspective and proposes SE-Merging, a self-enhanced model merging framework that leverages two key characteristics: distinguishing samples from different tasks and adaptively rescaling merging coefficients.

Result: SE-Merging achieves significant performance improvements while remaining compatible with existing model merging techniques.

Conclusion: The paper proposes SE-Merging, a self-enhanced model merging framework, and demonstrates its significant performance improvements while remaining compatible with existing model merging techniques.

Abstract: Model merging has gained increasing attention due to its intriguing property:
interpolating the parameters of different task-specific fine-tuned models leads
to multi-task abilities. However, despite its empirical success, the underlying
mechanisms of model merging remain poorly understood. In this work, we delve
into the mechanism behind model merging from a representation perspective. Our
analysis reveals that model merging achieves multi-task abilities through two
key capabilities: i) distinguishing samples from different tasks, and ii)
adapting to the corresponding expert model for each sample. These two
capabilities allow the merged model to retain task-specific expertise, enabling
efficient multi-task adaptation. Building on these insights, we propose
\texttt{SE-Merging}, a self-enhanced model merging framework that leverages
these two characteristics to dynamically identify the corresponding task for
each sample and then adaptively rescales the merging coefficients to further
enhance task-specific expertise in the merged model. Notably,
\texttt{SE-Merging} achieves dynamic model merging without additional training.
Extensive experiments demonstrate that \texttt{SE-Merging} achieves significant
performance improvements while remaining compatible with existing model merging
techniques.

</details>


### [111] [CoachGPT: A Scaffolding-based Academic Writing Assistant](https://arxiv.org/abs/2506.18149)
*Fumian Chen,Sotheara Veng,Joshua Wilson,Xiaoming Li,Hui Fang*

Main category: cs.AI

TL;DR: Developed CoachGPT, an AI agent-based web application using LLMs, to assist individuals with academic writing by providing personalized feedback and guidance.


<details>
  <summary>Details</summary>
Motivation: Academic writing skills are crucial for students' success, but can feel overwhelming without proper guidance and practice, particularly when writing in a second language. Traditionally, students ask instructors or search dictionaries, which are not universally accessible. Early writing assistants emerged as rule-based systems that focused on detecting misspellings, subject-verb disagreements, and basic punctuation errors; however, they are inaccurate and lack contextual understanding. Machine learning-based assistants demonstrate a strong ability for language understanding but are expensive to train. Large language models (LLMs) have shown remarkable capabilities in generating responses in natural languages based on given prompts. Still, they have a fundamental limitation in education: they generate essays without teaching, which can have detrimental effects on learning when misused.

Method: we develop CoachGPT, which leverages large language models (LLMs) to assist individuals with limited educational resources and those who prefer self-paced learning in academic writing. CoachGPT is an AI agent-based web application that (1) takes instructions from experienced educators, (2) converts instructions into sub-tasks, and (3) provides real-time feedback and suggestions using large language models.

Result: Compared to existing writing assistants, CoachGPT provides a more immersive writing experience with personalized feedback and guidance.

Conclusion: Our user studies prove the usefulness of CoachGPT and the potential of large language models for academic writing.

Abstract: Academic writing skills are crucial for students' success, but can feel
overwhelming without proper guidance and practice, particularly when writing in
a second language. Traditionally, students ask instructors or search
dictionaries, which are not universally accessible. Early writing assistants
emerged as rule-based systems that focused on detecting misspellings,
subject-verb disagreements, and basic punctuation errors; however, they are
inaccurate and lack contextual understanding. Machine learning-based assistants
demonstrate a strong ability for language understanding but are expensive to
train. Large language models (LLMs) have shown remarkable capabilities in
generating responses in natural languages based on given prompts. Still, they
have a fundamental limitation in education: they generate essays without
teaching, which can have detrimental effects on learning when misused. To
address this limitation, we develop CoachGPT, which leverages large language
models (LLMs) to assist individuals with limited educational resources and
those who prefer self-paced learning in academic writing. CoachGPT is an AI
agent-based web application that (1) takes instructions from experienced
educators, (2) converts instructions into sub-tasks, and (3) provides real-time
feedback and suggestions using large language models. This unique scaffolding
structure makes CoachGPT unique among existing writing assistants. Compared to
existing writing assistants, CoachGPT provides a more immersive writing
experience with personalized feedback and guidance. Our user studies prove the
usefulness of CoachGPT and the potential of large language models for academic
writing.

</details>


### [112] [AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology](https://arxiv.org/abs/2506.18156)
*Akash Kundu,Rishika Goswami*

Main category: cs.AI

TL;DR: LLMs show human-like cognitive biases, which has implications for AI ethics.


<details>
  <summary>Details</summary>
Motivation: Investigating whether LLMs exhibit human-like cognitive patterns under established psychology frameworks.

Method: Using structured prompts and automated scoring to evaluate several proprietary and open-source LLMs under four established psychology frameworks (TAT, Framing Bias, MFT, Cognitive Dissonance).

Result: LLMs produce coherent narratives, are susceptible to positive framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and demonstrate self-contradictions tempered by rationalization.

Conclusion: LLMs exhibit human-like cognitive tendencies, shaped by training data and alignment methods, raising implications for AI transparency and ethical deployment.

Abstract: We investigate whether Large Language Models (LLMs) exhibit human-like
cognitive patterns under four established frameworks from psychology: Thematic
Apperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and
Cognitive Dissonance. We evaluated several proprietary and open-source models
using structured prompts and automated scoring. Our findings reveal that these
models often produce coherent narratives, show susceptibility to positive
framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and
demonstrate self-contradictions tempered by extensive rationalization. Such
behaviors mirror human cognitive tendencies yet are shaped by their training
data and alignment methods. We discuss the implications for AI transparency,
ethical deployment, and future work that bridges cognitive psychology and AI
safety

</details>


### [113] [Chain-of-Memory: Enhancing GUI Agents for Cross-Application Navigation](https://arxiv.org/abs/2506.18158)
*Xinzge Gao,Chuanrui Hu,Bin Chen,Teng Li*

Main category: cs.AI

TL;DR: 提出了一种名为Chain-of-Memory (CoM) 的新方法，用于显式建模GUI agent中的短期和长期记忆, 显著提高了GUI agent在跨应用任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常依赖于历史截图或动作来隐式地表示任务状态。这种依赖对GUI agent准确理解任务状态构成了挑战，并突出了在复杂和漫长的跨应用任务中存储关键信息的有效机制的缺失。

Method: 提出了一种名为Chain-of-Memory (CoM) 的新方法，用于显式建模GUI agent中的短期和长期记忆。CoM通过捕获动作描述、整合任务相关的屏幕信息，并维护一个专用记忆模块来存储和管理这些信息来实现这一点。

Result: CoM显著提高了GUI agent在跨应用任务中的性能。此外，GUI Odyssey-CoM使7B模型能够实现与72B模型相当的内存管理能力。

Conclusion: CoM显著提高了GUI agent在跨应用任务中的性能。此外，GUI Odyssey-CoM使7B模型能够实现与72B模型相当的内存管理能力。

Abstract: Multimodal large language models (MLLMs) are attracting growing attention in
the development of Graphical User Interface (GUI) agents. Existing approaches
often rely on historical screenshots or actions to implicitly represent the
task state. This reliance poses challenges for GUI agents in accurately
understanding task states and underscores the absence of effective mechanisms
to store critical information in complex and lengthy cross-app tasks. To
address these challenges, we propose Chain-of-Memory (CoM), a novel approach
for explicitly modeling short-term and long-term memory in GUI agents. CoM
achieves this by capturing action descriptions, integrating task-relevant
screen information, and maintaining a dedicated memory module to store and
manage this information. By leveraging explicit memory representations, CoM
enables GUI agents to better understand task states and retain critical
historical information persistently. To equip GUI agents with memory management
capabilities and evaluate the effectiveness of CoM, we developed the GUI
Odyssey-CoM, a dataset comprising 111k screen-action pairs annotated with
Chain-of-Memory. Experimental results demonstrate that CoM significantly
improves GUI agents' performance in cross-application tasks. Additionally, GUI
Odyssey-CoM enables 7B models to achieve memory management capabilities
comparable to 72B models. The dataset and code will be open-sourced.

</details>


### [114] [Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?](https://arxiv.org/abs/2506.18183)
*Zhiting Mei,Christina Zhang,Tenny Yin,Justin Lidard,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.AI

TL;DR: Reasoning models are overconfident, get more overconfident with deeper reasoning, but can improve calibration through introspection, though not uniformly.


<details>
  <summary>Details</summary>
Motivation: Knowing when and how much to trust these models is critical to the safe deployment of reasoning models in real-world applications.

Method: Introduce introspective uncertainty quantification (UQ).

Result: Reasoning models: (i) are typically overconfident, with self-verbalized confidence estimates often greater than 85% particularly for incorrect responses, (ii) become even more overconfident with deeper reasoning, and (iii) can become better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated).

Conclusion: Reasoning models can become better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated).

Abstract: Reasoning language models have set state-of-the-art (SOTA) records on many
challenging benchmarks, enabled by multi-step reasoning induced using
reinforcement learning. However, like previous language models, reasoning
models are prone to generating confident, plausible responses that are
incorrect (hallucinations). Knowing when and how much to trust these models is
critical to the safe deployment of reasoning models in real-world applications.
To this end, we explore uncertainty quantification of reasoning models in this
work. Specifically, we ask three fundamental questions: First, are reasoning
models well-calibrated? Second, does deeper reasoning improve model
calibration? Finally, inspired by humans' innate ability to double-check their
thought processes to verify the validity of their answers and their confidence,
we ask: can reasoning models improve their calibration by explicitly reasoning
about their chain-of-thought traces? We introduce introspective uncertainty
quantification (UQ) to explore this direction. In extensive evaluations on SOTA
reasoning models across a broad range of benchmarks, we find that reasoning
models: (i) are typically overconfident, with self-verbalized confidence
estimates often greater than 85% particularly for incorrect responses, (ii)
become even more overconfident with deeper reasoning, and (iii) can become
better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not
uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we
conclude with important research directions to design necessary UQ benchmarks
and improve the calibration of reasoning models.

</details>


### [115] [The Impact of Medication Non-adherence on Adverse Outcomes: Evidence from Schizophrenia Patients via Survival Analysis](https://arxiv.org/abs/2506.18187)
*Shahriar Noroozizadeh,Pim Welle,Jeremy C. Weiss,George H. Chen*

Main category: cs.AI

TL;DR: Non-adherence to antipsychotics leads to earlier adverse outcomes (death, hospitalization, jail) in schizophrenia patients by 1-4 months.


<details>
  <summary>Details</summary>
Motivation: Quantify the association between non-adherence to antipsychotic medications and adverse outcomes in individuals with schizophrenia.

Method: Survival analysis and causal inference methods (T-learner, S-learner, nearest neighbor matching) are used to estimate individual and average treatment effects.

Result: Non-adherence advances adverse outcomes by approximately 1 to 4 months. County-provided risk scores adjust for key confounders. Non-adherence is associated with earlier adverse events across medication formulations and types.

Conclusion: Non-adherence to antipsychotic medications is associated with earlier adverse outcomes in individuals with schizophrenia. Integrating survival analysis with causal inference tools can yield policy-relevant insights.

Abstract: This study quantifies the association between non-adherence to antipsychotic
medications and adverse outcomes in individuals with schizophrenia. We frame
the problem using survival analysis, focusing on the time to the earliest of
several adverse events (early death, involuntary hospitalization, jail
booking). We extend standard causal inference methods (T-learner, S-learner,
nearest neighbor matching) to utilize various survival models to estimate
individual and average treatment effects, where treatment corresponds to
medication non-adherence. Analyses are repeated using different amounts of
longitudinal information (3, 6, 9, and 12 months). Using data from Allegheny
County in western Pennsylvania, we find strong evidence that non-adherence
advances adverse outcomes by approximately 1 to 4 months. Ablation studies
confirm that county-provided risk scores adjust for key confounders, as their
removal amplifies the estimated effects. Subgroup analyses by medication
formulation (injectable vs. oral) and medication type consistently show that
non-adherence is associated with earlier adverse events. These findings
highlight the clinical importance of adherence in delaying psychiatric crises
and show that integrating survival analysis with causal inference tools can
yield policy-relevant insights. We caution that although we apply causal
inference, we only make associative claims and discuss assumptions needed for
causal interpretation.

</details>


### [116] [A Conceptual Framework for AI Capability Evaluations](https://arxiv.org/abs/2506.18213)
*María Victoria Carro,Denise Alejandra Mester,Francisca Gauna Selasco,Luca Nicolás Forziati Gangi,Matheo Sandleris Musa,Lola Ramos Pereyra,Mario Leiva,Juan Gustavo Corvalan,María Vanina Martinez,Gerardo Simari*

Main category: cs.AI

TL;DR: Proposes a framework for analyzing AI capability evaluations to improve transparency and comparability.


<details>
  <summary>Details</summary>
Motivation: Well-designed and transparent evaluations are becoming essential tools in AI governance. Yet there remains a lack of clarity on how to perform these assessments both comprehensively and reliably.

Method: A structured, descriptive approach that systematizes the analysis of widely used methods and terminology without imposing new taxonomies or rigid formats.

Result: The framework supports transparency, comparability, and interpretability across diverse evaluations. It also enables researchers to identify methodological weaknesses, assists practitioners in designing evaluations, and provides policymakers with an accessible tool to scrutinize, compare, and navigate complex evaluation landscapes.

Conclusion: This paper proposes a conceptual framework for analyzing AI capability evaluations to support transparency, comparability, and interpretability across diverse evaluations.

Abstract: As AI systems advance and integrate into society, well-designed and
transparent evaluations are becoming essential tools in AI governance,
informing decisions by providing evidence about system capabilities and risks.
Yet there remains a lack of clarity on how to perform these assessments both
comprehensively and reliably. To address this gap, we propose a conceptual
framework for analyzing AI capability evaluations, offering a structured,
descriptive approach that systematizes the analysis of widely used methods and
terminology without imposing new taxonomies or rigid formats. This framework
supports transparency, comparability, and interpretability across diverse
evaluations. It also enables researchers to identify methodological weaknesses,
assists practitioners in designing evaluations, and provides policymakers with
an accessible tool to scrutinize, compare, and navigate complex evaluation
landscapes.

</details>


### [117] [The 4th Dimension for Scaling Model Size](https://arxiv.org/abs/2506.18233)
*Ruike Zhu,Hanwen Zhang,Tianyu Shi,Chi Wang,Tianyi Zhou,Zengyi Qin*

Main category: cs.AI

TL;DR: 本文探讨了一种新的模型扩展维度：虚拟逻辑深度 (VLD)，它通过重用参数来提高推理能力，而无需增加参数数量。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型扩展的新维度：虚拟逻辑深度 (VLD)，它通过重用模型中的参数来增加有效算法深度，而不改变总体参数计数。参数重用不是一个新概念，但其在模型缩放中的潜力和特性尚未得到彻底研究。

Method: 通过精心设计的受控实验

Result: VLD 缩放迫使模型的知识容量保持几乎恒定，只有微小的变化。VLD 缩放能够显着提高推理能力，前提是正确实施缩放方法。参数的数量与知识容量相关，但与推理能力无关。在某些情况下，不需要增加参数数量来增强推理。这些发现与各种模型配置一致，并且可能在我们的实验范围内普遍有效。

Conclusion: VLD scaling 可以在不增加参数数量的情况下显著提高推理能力。参数的数量与知识容量相关，但与推理能力无关。在某些情况下，不需要增加参数数量来增强推理。

Abstract: Scaling the size of large language models typically involves three
dimensions: depth, width, and the number of parameters. In this work, we
explore a fourth dimension, virtual logical depth (VLD), which increases the
effective algorithmic depth without changing the overall parameter count by
reusing parameters within the model. Although parameter reuse is not a new
concept, its potential and characteristics in model scaling have not been
thoroughly studied. Through carefully designed controlled experiments, we make
the following key discoveries regarding VLD scaling:
  VLD scaling forces the knowledge capacity of the model to remain almost
constant, with only minor variations.
  VLD scaling enables a significant improvement in reasoning capability,
provided the scaling method is properly implemented.
  The number of parameters correlates with knowledge capacity, but not with
reasoning capability. Under certain conditions, it is not necessary to increase
the parameter count to enhance reasoning.
  These findings are consistent across various model configurations and are
likely to be generally valid within the scope of our experiments.

</details>


### [118] [Advanced For-Loop for QML algorithm search](https://arxiv.org/abs/2506.18260)
*FuTe Wong*

Main category: cs.AI

TL;DR: This paper uses LLMMA for automated search and optimization of QML algorithms, inspired by FunSearch. It explores and adapts classical ML concepts for quantum computing, paving the way for efficient QML algorithm development.


<details>
  <summary>Details</summary>
Motivation: Inspired by Google DeepMind's FunSearch, the proposed system works on abstract level to iteratively generates and refines quantum transformations of classical machine learning algorithms (concepts), such as the Multi-Layer Perceptron, forward-forward and backpropagation algorithms.

Method: This paper introduces an advanced framework leveraging Large Language Model-based Multi-Agent Systems (LLMMA) for the automated search and optimization of Quantum Machine Learning (QML) algorithms.

Result: The proposed system works on abstract level to iteratively generates and refines quantum transformations of classical machine learning algorithms

Conclusion: This work highlights the potential of agentic frameworks to systematically explore classical machine learning concepts and adapt them for quantum computing, paving the way for efficient and automated development of QML algorithms.

Abstract: This paper introduces an advanced framework leveraging Large Language
Model-based Multi-Agent Systems (LLMMA) for the automated search and
optimization of Quantum Machine Learning (QML) algorithms. Inspired by Google
DeepMind's FunSearch, the proposed system works on abstract level to
iteratively generates and refines quantum transformations of classical machine
learning algorithms (concepts), such as the Multi-Layer Perceptron,
forward-forward and backpropagation algorithms. As a proof of concept, this
work highlights the potential of agentic frameworks to systematically explore
classical machine learning concepts and adapt them for quantum computing,
paving the way for efficient and automated development of QML algorithms.
Future directions include incorporating planning mechanisms and optimizing
strategy in the search space for broader applications in quantum-enhanced
machine learning.

</details>


### [119] [Dynamic Knowledge Exchange and Dual-diversity Review: Concisely Unleashing the Potential of a Multi-Agent Research Team](https://arxiv.org/abs/2506.18348)
*Weilun Yu,Shixiang Tang,Yonggui Huang,Nanqing Dong,Li Fan,Honggang Qi,Wei Liu,Xiaoli Diao,Xi Chen,Wanli Ouyang*

Main category: cs.AI

TL;DR: IDVSCI是一种基于llm的多智能体框架，它结合了动态知识交换机制和双重多样性审查范例，以促进更深入的推理和产生更有创造性和影响力的科学思想。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(llm)已经开始模仿研究人员之间有效的协作，这种协作越来越依赖于科学进步。虽然最近基于llm的科学家智能体在自主科学发现方面显示出了希望，但它们通常缺乏现实研究中必不可少的交互推理和评估机制。

Method: IDVSCI (内部讨论和投票科学家)，一个建立在llm上的多智能体框架，它包含两个关键创新:一个动态知识交换机制，支持智能体之间的迭代反馈，以及一个双重多样性审查范例，模拟异构专家评估。

Result: IDVSCI始终在两个数据集上取得最佳性能，优于AI Scientist和VIRSCI等现有系统。

Conclusion: IDVSCI在两个数据集上都取得了最好的性能，超过了现有的系统，如AI Scientist和VIRSCI。这些发现强调了在基于llm的自主研究中建模交互和同行评审动态的价值。

Abstract: Scientific progress increasingly relies on effective collaboration among
researchers, a dynamic that large language models (LLMs) have only begun to
emulate. While recent LLM-based scientist agents show promise in autonomous
scientific discovery, they often lack the interactive reasoning and evaluation
mechanisms essential to real-world research. We propose IDVSCI (Internal
Discussion and Vote SCIentists), a multi-agent framework built on LLMs that
incorporates two key innovations: a Dynamic Knowledge Exchange mechanism
enabling iterative feedback among agents, and a Dual-Diversity Review paradigm
that simulates heterogeneous expert evaluation. These components jointly
promote deeper reasoning and the generation of more creative and impactful
scientific ideas. To evaluate the effectiveness and generalizability of our
approach, we conduct experiments on two datasets: a widely used benchmark in
computer science and a new dataset we introduce in the health sciences domain.
Results show that IDVSCI consistently achieves the best performance across both
datasets, outperforming existing systems such as AI Scientist and VIRSCI. These
findings highlight the value of modeling interaction and peer review dynamics
in LLM-based autonomous research.

</details>


### [120] [A Large Language Model-based Multi-Agent Framework for Analog Circuits' Sizing Relationships Extraction](https://arxiv.org/abs/2506.18424)
*Chengjie Liu,Weiyu Chen,Huiyao Xu,Yuan Du,Jun Yang,Li Du*

Main category: cs.AI

TL;DR: using large language model to prune the search space for analog circuit sizing


<details>
  <summary>Details</summary>
Motivation: Many existing techniques extract the circuit sizing task as a mathematical optimization problem to solve and continuously improve the optimization efficiency from a mathematical perspective. But they ignore the automatic introduction of prior knowledge, fail to achieve effective pruning of the search space, which thereby leads to a considerable compression margin remaining in the search space.

Method: a large language model (LLM)-based multi-agent framework for analog circuits' sizing relationships extraction from academic papers

Result: the optimization efficiency was improved by $2.32 \sim 26.6 \times$

Conclusion: This work demonstrates that the LLM can effectively prune the search space for analog circuit sizing, providing a new solution for the combination of LLMs and conventional analog circuit design automation methods.

Abstract: In the design process of the analog circuit pre-layout phase, device sizing
is an important step in determining whether an analog circuit can meet the
required performance metrics. Many existing techniques extract the circuit
sizing task as a mathematical optimization problem to solve and continuously
improve the optimization efficiency from a mathematical perspective. But they
ignore the automatic introduction of prior knowledge, fail to achieve effective
pruning of the search space, which thereby leads to a considerable compression
margin remaining in the search space. To alleviate this problem, we propose a
large language model (LLM)-based multi-agent framework for analog circuits'
sizing relationships extraction from academic papers. The search space in the
sizing process can be effectively pruned based on the sizing relationship
extracted by this framework. Eventually, we conducted tests on 3 types of
circuits, and the optimization efficiency was improved by $2.32 \sim 26.6
\times$. This work demonstrates that the LLM can effectively prune the search
space for analog circuit sizing, providing a new solution for the combination
of LLMs and conventional analog circuit design automation methods.

</details>


### [121] [How Robust is Model Editing after Fine-Tuning? An Empirical Study on Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.18428)
*Feng He,Zhenyang Liu,Marco Valentino,Zhixue Zhao*

Main category: cs.AI

TL;DR: Model edits in T2I diffusion models often don't survive fine-tuning, especially with DoRA. UCE is more robust. Fine-tuning can remove malicious edits but may also require re-editing for safety.


<details>
  <summary>Details</summary>
Motivation: It remains unknown whether edits persist after fine-tuning or whether they are inadvertently reversed. This question has fundamental practical implications. For example, if fine-tuning removes prior edits, it could serve as a defence mechanism against hidden malicious edits. Vice versa, the unintended removal of edits related to bias mitigation could pose serious safety concerns.

Method: Two T2I model families (Stable Diffusion and FLUX), two sota editing techniques, and three fine-tuning methods (DreamBooth, LoRA, and DoRA). Through an extensive empirical analysis across diverse editing tasks and evaluation metrics

Result: Edits generally fail to persist through fine-tuning, even when fine-tuning is tangential or unrelated to the edits. Notably, DoRA exhibits the strongest edit reversal effect. At the same time, among editing methods, UCE demonstrates greater robustness, retaining significantly higher efficacy post-fine-tuning compared to ReFACT.

Conclusion: Edits generally fail to persist through fine-tuning, even when fine-tuning is tangential or unrelated to the edits. DoRA exhibits the strongest edit reversal effect. UCE demonstrates greater robustness, retaining significantly higher efficacy post-fine-tuning compared to ReFACT. Fine-tuning could serve as a remediation mechanism for malicious edits while simultaneously highlighting the need for re-editing after fine-tuning to maintain beneficial safety and alignment properties.

Abstract: Model editing offers a low-cost technique to inject or correct a particular
behavior in a pre-trained model without extensive retraining, supporting
applications such as factual correction and bias mitigation. Despite this
common practice, it remains unknown whether edits persist after fine-tuning or
whether they are inadvertently reversed. This question has fundamental
practical implications. For example, if fine-tuning removes prior edits, it
could serve as a defence mechanism against hidden malicious edits. Vice versa,
the unintended removal of edits related to bias mitigation could pose serious
safety concerns. We systematically investigate the interaction between model
editing and fine-tuning in the context of T2I diffusion models, which are known
to exhibit biases and generate inappropriate content. Our study spans two T2I
model families (Stable Diffusion and FLUX), two sota editing techniques, and
three fine-tuning methods (DreamBooth, LoRA, and DoRA). Through an extensive
empirical analysis across diverse editing tasks and evaluation metrics, our
findings reveal a trend: edits generally fail to persist through fine-tuning,
even when fine-tuning is tangential or unrelated to the edits. Notably, we
observe that DoRA exhibits the strongest edit reversal effect. At the same
time, among editing methods, UCE demonstrates greater robustness, retaining
significantly higher efficacy post-fine-tuning compared to ReFACT. These
findings highlight a crucial limitation in current editing methodologies,
emphasizing the need for more robust techniques to ensure reliable long-term
control and alignment of deployed AI systems. These findings have dual
implications for AI safety: they suggest that fine-tuning could serve as a
remediation mechanism for malicious edits while simultaneously highlighting the
need for re-editing after fine-tuning to maintain beneficial safety and
alignment properties.

</details>


### [122] [Standard Applicability Judgment and Cross-jurisdictional Reasoning: A RAG-based Framework for Medical Device Compliance](https://arxiv.org/abs/2506.18511)
*Yu Han,Aaron Ceross,Jeroen H. M. Bergmann*

Main category: cs.AI

TL;DR: This paper introduces a RAG pipeline to automate standard applicability determination. The system retrieves candidate standards and uses LLMs to infer jurisdiction-specific applicability, achieving 73% accuracy and 87% Top-5 recall.


<details>
  <summary>Details</summary>
Motivation: Identifying the appropriate regulatory standard applicability remains a critical yet understudied challenge in medical device compliance, frequently necessitating expert interpretation of fragmented and heterogeneous documentation across different jurisdictions.

Method: a modular AI system that leverages a retrieval-augmented generation (RAG) pipeline to automate standard applicability determination. Given a free-text device description, our system retrieves candidate standards from a curated corpus and uses large language models to infer jurisdiction-specific applicability, classified as Mandatory, Recommended, or Not Applicable, with traceable justifications.

Result: The proposed approach attains a classification accuracy of 73% and a Top-5 retrieval recall of 87%, demonstrating its effectiveness in identifying relevant regulatory standards.

Conclusion: The proposed approach attains a classification accuracy of 73% and a Top-5 retrieval recall of 87%, demonstrating its effectiveness in identifying relevant regulatory standards. We introduce the first end-to-end system for standard applicability reasoning, enabling scalable and interpretable AI-supported regulatory science. Notably, our region-aware RAG agent performs cross-jurisdictional reasoning between Chinese and U.S. standards, supporting conflict resolution and applicability justification across regulatory frameworks.

Abstract: Identifying the appropriate regulatory standard applicability remains a
critical yet understudied challenge in medical device compliance, frequently
necessitating expert interpretation of fragmented and heterogeneous
documentation across different jurisdictions. To address this challenge, we
introduce a modular AI system that leverages a retrieval-augmented generation
(RAG) pipeline to automate standard applicability determination. Given a
free-text device description, our system retrieves candidate standards from a
curated corpus and uses large language models to infer jurisdiction-specific
applicability, classified as Mandatory, Recommended, or Not Applicable, with
traceable justifications. We construct an international benchmark dataset of
medical device descriptions with expert-annotated standard mappings, and
evaluate our system against retrieval-only, zero-shot, and rule-based
baselines. The proposed approach attains a classification accuracy of 73% and a
Top-5 retrieval recall of 87%, demonstrating its effectiveness in identifying
relevant regulatory standards. We introduce the first end-to-end system for
standard applicability reasoning, enabling scalable and interpretable
AI-supported regulatory science. Notably, our region-aware RAG agent performs
cross-jurisdictional reasoning between Chinese and U.S. standards, supporting
conflict resolution and applicability justification across regulatory
frameworks.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [123] [DCMF: A Dynamic Context Monitoring and Caching Framework for Context Management Platforms](https://arxiv.org/abs/2506.17226)
*Ashish Manchanda,Prem Prakash Jayaraman,Abhik Banerjee,Kaneez Fizza,Arkady Zaslavsky*

Main category: cs.DB

TL;DR: 提出了DCMF框架，用于增强物联网环境中的上下文缓存，通过动态评估上下文并使用混合Dempster-Shafer方法管理上下文新鲜度，从而提高了缓存命中率并减少了缓存过期。


<details>
  <summary>Details</summary>
Motivation: 上下文感知物联网应用的兴起增加了对及时和准确的上下文信息的需求。上下文是通过聚合和推断动态物联网数据得出的，这使得它具有高度的易失性，并在保持新鲜度和实时可访问性方面提出了挑战。缓存是一种潜在的解决方案，但传统的策略在物联网中难以应对上下文的瞬时性（例如，确保频繁查询的实时访问或处理快速变化的数据）。

Method: 提出了动态上下文监控框架（DCMF），通过动态评估和管理上下文来增强上下文管理平台（CMP）中的上下文缓存。DCMF包括两个核心组件：上下文评估引擎（CEE）和上下文管理模块（CMM）。

Result: DCMF实现了12.5%的更高缓存命中率，并将缓存过期时间减少了高达60%。

Conclusion: DCMF在实际智能城市数据（特别是交通和道路工程场景）中的实施和评估表明，与m-CAC技术相比，缓存命中率提高了12.5%，缓存过期减少了60%，从而确保了相关上下文的及时交付并减少了延迟。这些结果证明了DCMF的可扩展性和对动态上下文感知物联网环境的适用性。

Abstract: The rise of context-aware IoT applications has increased the demand for
timely and accurate context information. Context is derived by aggregating and
inferring from dynamic IoT data, making it highly volatile and posing
challenges in maintaining freshness and real-time accessibility. Caching is a
potential solution, but traditional policies struggle with the transient nature
of context in IoT (e.g., ensuring real-time access for frequent queries or
handling fast-changing data). To address this, we propose the Dynamic Context
Monitoring Framework (DCMF) to enhance context caching in Context Management
Platforms (CMPs) by dynamically evaluating and managing context. DCMF comprises
two core components: the Context Evaluation Engine (CEE) and the Context
Management Module (CMM). The CEE calculates the Probability of Access (PoA)
using parameters such as Quality of Service (QoS), Quality of Context (QoC),
Cost of Context (CoC), timeliness, and Service Level Agreements (SLAs),
assigning weights to assess access likelihood. Based on this, the CMM applies a
hybrid Dempster-Shafer approach to manage Context Freshness (CF), updating
belief levels and confidence scores to determine whether to cache, evict, or
refresh context items. We implemented DCMF in a Context-as-a-Service (CoaaS)
platform and evaluated it using real-world smart city data, particularly
traffic and roadwork scenarios. Results show DCMF achieves a 12.5% higher cache
hit rate and reduces cache expiry by up to 60% compared to the m-CAC technique,
ensuring timely delivery of relevant context and reduced latency. These results
demonstrate DCMF's scalability and suitability for dynamic context-aware IoT
environments.

</details>


### [124] [Transient Concepts in Streaming Graphs](https://arxiv.org/abs/2506.17451)
*Aida Sheshbolouki,M. Tamer Ozsu*

Main category: cs.DB

TL;DR: This paper introduces SGDD and SGDP, two frameworks for detecting and predicting concept drift in streaming graphs. SGDP can predict these CDs quickly, without accessing the payloads of data records.


<details>
  <summary>Details</summary>
Motivation: Understanding, detection, and adaptation to CD in streaming data is (i) vital for effective and efficient analytics as reliable output depends on adaptation to fresh input, (ii) challenging as it requires efficient operations as well as effective performance evaluations, and (iii) impactful as it applies to a variety of use cases and is a crucial initial step for data management systems.

Method: introduce two first-of-its-kind frameworks SGDD and SGDP for streaming graph CD detection and prediction

Result: SGDD detects the CDs due to the changes of generative parameters with significant delays such that it is difficult to evaluate the performance, while SGDP predicts these CDs between 7374 to 0.19 milliseconds ahead of their occurrence, without accessing the payloads of data records.

Conclusion: We revisit CD for the streaming graphs setting and introduce two first-of-its-kind frameworks SGDD and SGDP for streaming graph CD detection and prediction. Both frameworks discern the change of generative source. SGDD detects the CDs due to the changes of generative parameters with significant delays such that it is difficult to evaluate the performance, while SGDP predicts these CDs between 7374 to 0.19 milliseconds ahead of their occurrence, without accessing the payloads of data records.

Abstract: Concept Drift (CD) occurs when a change in a hidden context can induce
changes in a target concept. CD is a natural phenomenon in non-stationary
settings such as data streams. Understanding, detection, and adaptation to CD
in streaming data is (i) vital for effective and efficient analytics as
reliable output depends on adaptation to fresh input, (ii) challenging as it
requires efficient operations as well as effective performance evaluations, and
(iii) impactful as it applies to a variety of use cases and is a crucial
initial step for data management systems. Current works are mostly focused on
passive CD detection as part of supervised adaptation, on independently
generated data instances or graph snapshots, on target concepts as a function
of data labels, on static data management, and on specific temporal order of
data record. These methods do not always work. We revisit CD for the streaming
graphs setting and introduce two first-of-its-kind frameworks SGDD and SGDP for
streaming graph CD detection and prediction. Both frameworks discern the change
of generative source. SGDD detects the CDs due to the changes of generative
parameters with significant delays such that it is difficult to evaluate the
performance, while SGDP predicts these CDs between 7374 to 0.19 milliseconds
ahead of their occurrence, without accessing the payloads of data records.

</details>


### [125] [Lower Bounds for Conjunctive Query Evaluation](https://arxiv.org/abs/2506.17702)
*Stefan Mengel*

Main category: cs.DB

TL;DR: complexity theory connect to query answering and allow showing that known algorithms in several cases can likely not be improved


<details>
  <summary>Details</summary>
Motivation: complexity of conjunctive query evaluation in different settings

Method: survey known results on the complexity of conjunctive query evaluation in different settings

Result: ranging from Boolean queries over counting to more complex models like enumeration and direct access

Conclusion: known algorithms in several cases can likely not be improved

Abstract: In this tutorial, we will survey known results on the complexity of
conjunctive query evaluation in different settings, ranging from Boolean
queries over counting to more complex models like enumeration and direct
access. A particular focus will be on showing how different relatively recent
hypotheses from complexity theory connect to query answering and allow showing
that known algorithms in several cases can likely not be improved.

</details>


### [126] [Dual-Hierarchy Labelling: Scaling Up Distance Queries on Dynamic Road Networks](https://arxiv.org/abs/2506.18013)
*Muhammad Farhan,Henning Koehler,Qing Wang*

Main category: cs.DB

TL;DR: This paper introduces Dual-Hierarchy Labelling (DHL) for efficient shortest-path distance queries on dynamic road networks, outperforming existing methods in speed and space consumption.


<details>
  <summary>Details</summary>
Motivation: Existing methods for shortest-path distance computation in dynamic road networks suffer from slow query response time or poor maintenance performance, especially on large networks.

Method: The paper proposes Dual-Hierarchy Labelling (DHL), incorporating two hierarchies (query and update) with different data structures to support efficient query and update processing. Dynamic algorithms and a parallel variant are developed for maintaining the update hierarchy and hierarchical labelling.

Result: DHL achieves considerably faster construction and update time, consistently 2-4 times faster query processing, and consumes only 10%-20% labelling space compared to state-of-the-art methods on 10 large road networks.

Conclusion: The proposed DHL method significantly outperforms state-of-the-art methods on large road networks, achieving faster construction and update times, 2-4 times faster query processing, and consuming only 10%-20% labelling space.

Abstract: Computing the shortest-path distance between any two given vertices in road
networks is an important problem. A tremendous amount of research has been
conducted to address this problem, most of which are limited to static road
networks. Since road networks undergo various real-time traffic conditions,
there is a pressing need to address this problem for dynamic road networks.
Existing state-of-the-art methods incrementally maintain an indexing structure
to reflect dynamic changes on road networks. However, these methods suffer from
either slow query response time or poor maintenance performance, particularly
when road networks are large. In this work, we propose an efficient solution
\emph{Dual-Hierarchy Labelling (DHL)} for distance querying on dynamic road
networks from a novel perspective, which incorporates two hierarchies with
different but complementary data structures to support efficient query and
update processing. Specifically, our proposed solution is comprised of three
main components: \emph{query hierarchy}, \emph{update hierarchy}, and
\emph{hierarchical labelling}, where \emph{query hierarchy} enables efficient
query answering by exploring only a small subset of vertices in the labels of
two query vertices and \emph{update hierarchy} supports efficient maintenance
of distance labelling under edge weight increase or decrease. We further
develop dynamic algorithms to reflect dynamic changes by efficiently
maintaining the update hierarchy and hierarchical labelling. We also propose a
parallel variant of our dynamic algorithms by exploiting labelling structure.
We evaluate our methods on 10 large road networks and it shows that our methods
significantly outperform the state-of-the-art methods, i.e., achieving
considerably faster construction and update time, while being consistently 2-4
times faster in terms of query processing and consuming only 10\%-20\%
labelling space.

</details>


### [127] [Floating-Point Data Transformation for Lossless Compression](https://arxiv.org/abs/2506.18062)
*Samirasadat Jamalidinan,Kazem Cheshmi*

Main category: cs.DB

TL;DR: This paper introduces Typed Data Transformation (DTT) for lossless compression of floating-point data, improving compression ratio and throughput compared to existing tools.


<details>
  <summary>Details</summary>
Motivation: Lossless storage of floating-point data is crucial due to its critical accuracy, as seen in applications such as medical imaging and language model weights. Previous approaches either treat this data as raw byte streams for compression or fail to leverage all patterns within the dataset. However, because multiple bytes represent a single value and due to inherent patterns in floating-point representations, some of these bytes are correlated.

Method: a novel data transformation method called Typed Data Transformation (DTT) that groups related bytes together to improve compression

Result: DTT achieves a geometric mean compression ratio improvement of 1.16x over state-of-the-art compression tools such as zstd, while also improving both compression and decompression throughput by 1.18--3.79x.

Conclusion: Typed Data Transformation (DTT) achieves a geometric mean compression ratio improvement of 1.16x over state-of-the-art compression tools such as zstd, while also improving both compression and decompression throughput by 1.18--3.79x.

Abstract: Floating-point data is widely used across various domains. Depending on the
required precision, each floating-point value can occupy several bytes.
Lossless storage of this information is crucial due to its critical accuracy,
as seen in applications such as medical imaging and language model weights. In
these cases, data size is often significant, making lossless compression
essential. Previous approaches either treat this data as raw byte streams for
compression or fail to leverage all patterns within the dataset. However,
because multiple bytes represent a single value and due to inherent patterns in
floating-point representations, some of these bytes are correlated. To leverage
this property, we propose a novel data transformation method called Typed Data
Transformation (\DTT{}) that groups related bytes together to improve
compression. We implemented and tested our approach on various datasets across
both CPU and GPU. \DTT{} achieves a geometric mean compression ratio
improvement of 1.16$\times$ over state-of-the-art compression tools such as
zstd, while also improving both compression and decompression throughput by
1.18--3.79$\times$.

</details>


### [128] [Learning Lineage Constraints for Data Science Operations](https://arxiv.org/abs/2506.18252)
*Jinjin Zhao*

Main category: cs.DB

TL;DR: This paper proposes XProv, an architecture for cross-library data lineage in data science workflows, using logical patterns and materialized graphs.


<details>
  <summary>Details</summary>
Motivation: Debugging data science workflows that integrate functionalities from diverse libraries and frameworks requires data lineage that crosses library boundaries. Current lineage representations are often tied to particular data models and data manipulation paradigms.

Method: The paper introduces XProv, which links materialized lineage graphs of data transformations and abstracted logical patterns. It also discusses ideas on inferring logical patterns from materialized graphs.

Result: The paper presents early ideas on how to infer logical patterns when only materialized graphs are available.

Conclusion: This paper proposes an architecture for lineage, called XProv, to specify logical lineage across libraries in a common parameterized way.

Abstract: Data science workflows often integrate functionalities from a diverse set of
libraries and frameworks. Tasks such as debugging require data lineage that
crosses library boundaries. The problem is that the way that "lineage" is
represented is often intimately tied to particular data models and data
manipulation paradigms. Inspired by the use of intermediate representations
(IRs) in cross-library performance optimizations, this vision paper proposes a
similar architecture for lineage - how do we specify logical lineage across
libraries in a common parameterized way? In practice, cross-library workflows
will contain both known operations and unknown operations, so a key design of
XProv to link both materialized lineage graphs of data transformations and the
aforementioned abstracted logical patterns. We further discuss early ideas on
how to infer logical patterns when only the materialized graphs are available.

</details>


### [129] [Fast Capture of Cell-Level Provenance in Numpy](https://arxiv.org/abs/2506.18255)
*Jinjin Zhao,Sanjay Krishnan*

Main category: cs.DB

TL;DR: This paper introduces a prototype annotation system for numpy arrays that captures cell-level provenance and explores memory optimizations to reduce annotation latency.


<details>
  <summary>Details</summary>
Motivation: Effective provenance tracking enhances reproducibility, governance, and data quality in array workflows. However, significant challenges arise in capturing this provenance, including: (1) rapidly evolving APIs, (2) diverse operation types, and (3) large-scale datasets.

Method: This paper presents a prototype annotation system designed for arrays, which captures cell-level provenance specifically within the numpy library.

Result: With this prototype, we explore straightforward memory optimizations that substantially reduce annotation latency.

Conclusion: This paper envisions a provenance capture approach for arrays as part of a broader governance system for tracking for structured data workflows and diverse data science applications.

Abstract: Effective provenance tracking enhances reproducibility, governance, and data
quality in array workflows. However, significant challenges arise in capturing
this provenance, including: (1) rapidly evolving APIs, (2) diverse operation
types, and (3) large-scale datasets. To address these challenges, this paper
presents a prototype annotation system designed for arrays, which captures
cell-level provenance specifically within the numpy library. With this
prototype, we explore straightforward memory optimizations that substantially
reduce annotation latency. We envision this provenance capture approach for
arrays as part of a broader governance system for tracking for structured data
workflows and diverse data science applications.

</details>


### [130] [TableVault: Managing Dynamic Data Collections for LLM-Augmented Workflows](https://arxiv.org/abs/2506.18257)
*Jinjin Zhao,Sanjay Krishnan*

Main category: cs.DB

TL;DR: TableVault is a data management system designed to handle dynamic data collections in LLM-augmented environments.


<details>
  <summary>Details</summary>
Motivation: integration of Large Language Models (LLMs) into more complex data workflows introduces significant management challenges

Method: merging established database methodologies with emerging LLM-driven requirements

Result: TableVault meets the demands of these workflows by supporting concurrent execution, ensuring reproducibility, maintaining robust data versioning, and enabling composable workflow design.

Conclusion: TableVault offers a transparent platform that efficiently manages both structured data and associated data artifacts.

Abstract: Large Language Models (LLMs) have emerged as powerful tools for automating
and executing complex data tasks. However, their integration into more complex
data workflows introduces significant management challenges. In response, we
present TableVault - a data management system designed to handle dynamic data
collections in LLM-augmented environments. TableVault meets the demands of
these workflows by supporting concurrent execution, ensuring reproducibility,
maintaining robust data versioning, and enabling composable workflow design. By
merging established database methodologies with emerging LLM-driven
requirements, TableVault offers a transparent platform that efficiently manages
both structured data and associated data artifacts.

</details>


### [131] [Patient Journey Ontology: Representing Medical Encounters for Enhanced Patient-Centric Applications](https://arxiv.org/abs/2506.18772)
*Hassan S. Al Khatib,Subash Neupane,Sudip Mittal,Shahram Rahimi,Nina Marhamati,Sean Bozorgzad*

Main category: cs.DB

TL;DR: Developed a Patient Journey Ontology (PJO) to capture patient healthcare encounters, enabling semantic interoperability, predictive analytics, and personalized medicine.


<details>
  <summary>Details</summary>
Motivation: The healthcare industry is moving towards a patient-centric paradigm that requires advanced methods for managing and representing patient data.

Method: This paper presents a Patient Journey Ontology (PJO), a framework that captures a patient's healthcare encounters by integrating different patient data sources.

Result: Evaluations demonstrate strong capabilities in patient history retrieval, symptom tracking, and provider interaction representation, with opportunities for enhanced diagnosis-symptom linking. The PJO's reliability and practical applicability are also revealed.

Conclusion: The Patient Journey Ontology (PJO) demonstrates potential to enhance patient outcomes, healthcare efficiency, and personalized medicine, offering a tool for patient journey analysis and advancing Generative AI in healthcare.

Abstract: The healthcare industry is moving towards a patient-centric paradigm that
requires advanced methods for managing and representing patient data. This
paper presents a Patient Journey Ontology (PJO), a framework that aims to
capture the entirety of a patient's healthcare encounters. Utilizing
ontologies, the PJO integrates different patient data sources like medical
histories, diagnoses, treatment pathways, and outcomes; it enables semantic
interoperability and enhances clinical reasoning. By capturing temporal,
sequential, and causal relationships between medical encounters, the PJO
supports predictive analytics, enabling earlier interventions and optimized
treatment plans. The ontology's structure, including its main classes,
subclasses, properties, and relationships, as detailed in the paper,
demonstrates its ability to provide a holistic view of patient care.
Quantitative and qualitative evaluations by Subject Matter Experts (SMEs)
demonstrate strong capabilities in patient history retrieval, symptom tracking,
and provider interaction representation, while identifying opportunities for
enhanced diagnosis-symptom linking. These evaluations reveal the PJO's
reliability and practical applicability, demonstrating its potential to enhance
patient outcomes and healthcare efficiency. This work contributes to the
ongoing efforts of knowledge representation in healthcare, offering a reliable
tool for personalized medicine, patient journey analysis and advancing the
capabilities of Generative AI in healthcare applications.

</details>


### [132] [LIGHTHOUSE: Fast and precise distance to shoreline calculations from anywhere on earth](https://arxiv.org/abs/2506.18842)
*Patrick Beukema,Henry Herzog,Yawen Zhang,Hunter Pitelka,Favyen Bastani*

Main category: cs.DB

TL;DR: This paper introduces a new dataset and algorithm for fast and efficient coastal distance calculations from Anywhere on Earth (AoE).


<details>
  <summary>Details</summary>
Motivation: Existing global coastal datasets are only available at coarse resolution (e.g. 1-4 km) which limits their utility. Publicly available satellite imagery combined with computer vision enable much higher precision.

Method: a new library: Layered Iterative Geospatial Hierarchical Terrain-Oriented Unified Search Engine (Lighthouse)

Result: a global coastline dataset at 10 meter resolution, a 100+ fold improvement in precision over existing data.

Conclusion: We introduce a new library: Layered Iterative Geospatial Hierarchical Terrain-Oriented Unified Search Engine (Lighthouse). Lighthouse is both exceptionally fast and resource-efficient, requiring only 1 CPU and 2 GB of RAM to achieve millisecond online inference, making it well suited for real-time applications in resource-constrained environments.

Abstract: We introduce a new dataset and algorithm for fast and efficient coastal
distance calculations from Anywhere on Earth (AoE). Existing global coastal
datasets are only available at coarse resolution (e.g. 1-4 km) which limits
their utility. Publicly available satellite imagery combined with computer
vision enable much higher precision. We provide a global coastline dataset at
10 meter resolution, a 100+ fold improvement in precision over existing data.
To handle the computational challenge of querying at such an increased scale,
we introduce a new library: Layered Iterative Geospatial Hierarchical
Terrain-Oriented Unified Search Engine (Lighthouse). Lighthouse is both
exceptionally fast and resource-efficient, requiring only 1 CPU and 2 GB of RAM
to achieve millisecond online inference, making it well suited for real-time
applications in resource-constrained environments.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [133] [QUST_NLP at SemEval-2025 Task 7: A Three-Stage Retrieval Framework for Monolingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2506.17272)
*Youzheng Liu,Jiyan Liu,Xiaoman Xu,Taihang Wang,Yimin Wang,Ye Jiang*

Main category: cs.IR

TL;DR: A three-stage retrieval framework for fact-checked claim retrieval, achieving 5th and 7th place in SemEval-2025 Task 7.


<details>
  <summary>Details</summary>
Motivation: This paper describes the participation of QUST_NLP in the SemEval-2025 Task 7.

Method: a three-stage retrieval framework specifically designed for fact-checked claim retrieval. Initially, evaluate the performance of several retrieval models and select the one that yields the best results for candidate retrieval. Next, employ multiple re-ranking models to enhance the candidate results, with each model selecting the Top-10 outcomes. In the final stage, utilize weighted voting to determine the final retrieval outcomes.

Result: achieved 5th place in the monolingual track and 7th place in the crosslingual track

Conclusion: The approach achieved 5th place in the monolingual track and 7th place in the crosslingual track.

Abstract: This paper describes the participation of QUST_NLP in the SemEval-2025 Task
7. We propose a three-stage retrieval framework specifically designed for
fact-checked claim retrieval. Initially, we evaluate the performance of several
retrieval models and select the one that yields the best results for candidate
retrieval. Next, we employ multiple re-ranking models to enhance the candidate
results, with each model selecting the Top-10 outcomes. In the final stage, we
utilize weighted voting to determine the final retrieval outcomes. Our approach
achieved 5th place in the monolingual track and 7th place in the crosslingual
track. We release our system code at:
https://github.com/warmth27/SemEval2025_Task7

</details>


### [134] [Chunk Twice, Embed Once: A Systematic Study of Segmentation and Representation Trade-offs in Chemistry-Aware Retrieval-Augmented Generation](https://arxiv.org/abs/2506.17277)
*Mahmoud Amiri,Thomas Bocklitz*

Main category: cs.IR

TL;DR: systematic evaluation of chunking strategies and embedding models tailored to chemistry-focused RAG systems


<details>
  <summary>Details</summary>
Motivation: foundational design choices remain underexplored in domain-specific contexts such as chemistry.

Method: large-scale, systematic evaluation of chunking strategies and embedding models tailored to chemistry-focused RAG systems. investigate 25 chunking configurations across five method families and evaluate 48 embedding models on three chemistry-specific benchmarks, including the newly introduced QuestChemRetrieval dataset.

Result: recursive token-based chunking (specifically R100-0) consistently outperforms other approaches. retrieval-optimized embeddings substantially outperform domain-specialized models like SciBERT.

Conclusion: recursive token-based chunking (specifically R100-0) consistently outperforms other approaches, offering strong performance with minimal resource overhead. retrieval-optimized embeddings substantially outperform domain-specialized models like SciBERT.

Abstract: Retrieval-Augmented Generation (RAG) systems are increasingly vital for
navigating the ever-expanding body of scientific literature, particularly in
high-stakes domains such as chemistry. Despite the promise of RAG, foundational
design choices -- such as how documents are segmented and represented -- remain
underexplored in domain-specific contexts. This study presents the first
large-scale, systematic evaluation of chunking strategies and embedding models
tailored to chemistry-focused RAG systems. We investigate 25 chunking
configurations across five method families and evaluate 48 embedding models on
three chemistry-specific benchmarks, including the newly introduced
QuestChemRetrieval dataset. Our results reveal that recursive token-based
chunking (specifically R100-0) consistently outperforms other approaches,
offering strong performance with minimal resource overhead. We also find that
retrieval-optimized embeddings -- such as Nomic and Intfloat E5 variants --
substantially outperform domain-specialized models like SciBERT. By releasing
our datasets, evaluation framework, and empirical benchmarks, we provide
actionable guidelines for building effective and efficient chemistry-aware RAG
systems.

</details>


### [135] [CORONA: A Coarse-to-Fine Framework for Graph-based Recommendation with Large Language Models](https://arxiv.org/abs/2506.17281)
*Junze Chen,Xinjie Yang,Cheng Yang,Junfei Bao,Zeyuan Guo,Yawen Li,Chuan Shi*

Main category: cs.IR

TL;DR:  leverage LLMs' reasoning abilities during the candidate filtering process to enhance recommendation


<details>
  <summary>Details</summary>
Motivation: prior work limits LLMs to re-ranking results or dataset augmentation, failing to utilize their power during candidate filtering - which may lead to suboptimal performance

Method:  leverage LLMs' reasoning abilities during the candidate filtering process, and introduce Chain Of Retrieval ON grAphs (CORONA) to progressively narrow down the range of candidate items on interaction graphs with the help of LLMs

Result: achieves state-of-the-art performance with an 18.6% relative improvement in recall and an 18.4% relative improvement in NDCG on average

Conclusion: The proposed CORONA achieves state-of-the-art performance with an 18.6% relative improvement in recall and an 18.4% relative improvement in NDCG on average.

Abstract: Recommender systems (RSs) are designed to retrieve candidate items a user
might be interested in from a large pool. A common approach is using graph
neural networks (GNNs) to capture high-order interaction relationships. As
large language models (LLMs) have shown strong capabilities across domains,
researchers are exploring their use to enhance recommendation. However, prior
work limits LLMs to re-ranking results or dataset augmentation, failing to
utilize their power during candidate filtering - which may lead to suboptimal
performance. Instead, we propose to leverage LLMs' reasoning abilities during
the candidate filtering process, and introduce Chain Of Retrieval ON grAphs
(CORONA) to progressively narrow down the range of candidate items on
interaction graphs with the help of LLMs: (1) First, LLM performs preference
reasoning based on user profiles, with the response serving as a query to
extract relevant users and items from the interaction graph as
preference-assisted retrieval; (2) Then, using the information retrieved in the
previous step along with the purchase history of target user, LLM conducts
intent reasoning to help refine an even smaller interaction subgraph as
intent-assisted retrieval; (3) Finally, we employ a GNN to capture high-order
collaborative filtering information from the extracted subgraph, performing
GNN-enhanced retrieval to generate the final recommendation results. The
proposed framework leverages the reasoning capabilities of LLMs during the
retrieval process, while seamlessly integrating GNNs to enhance overall
recommendation performance. Extensive experiments on various datasets and
settings demonstrate that our proposed CORONA achieves state-of-the-art
performance with an 18.6% relative improvement in recall and an 18.4% relative
improvement in NDCG on average.

</details>


### [136] [Automating Financial Statement Audits with Large Language Models](https://arxiv.org/abs/2506.17282)
*Rushi Wang,Jiateng Liu,Weijie Zhao,Shenglan Li,Denghui Zhang*

Main category: cs.IR

TL;DR: LLMs can automate parts of financial auditing, but lack accounting knowledge for full automation.


<details>
  <summary>Details</summary>
Motivation: Manual financial statement auditing is inefficient and error-prone, leading to inaccurate statements.

Method: We introduce a benchmark dataset combining real-world financial tables with synthesized transaction data and a five-stage evaluation framework.

Result: LLMs identify errors with transaction data but struggle to explain them or cite accounting standards, and can't execute complete audits.

Conclusion: Current LLMs can identify errors but struggle with explanations, citing standards, and complete audits, revealing a gap in accounting knowledge.

Abstract: Financial statement auditing is essential for stakeholders to understand a
company's financial health, yet current manual processes are inefficient and
error-prone. Even with extensive verification procedures, auditors frequently
miss errors, leading to inaccurate financial statements that fail to meet
stakeholder expectations for transparency and reliability. To this end, we
harness large language models (LLMs) to automate financial statement auditing
and rigorously assess their capabilities, providing insights on their
performance boundaries in the scenario of automated auditing. Our work
introduces a comprehensive benchmark using a curated dataset combining
real-world financial tables with synthesized transaction data. In the
benchmark, we developed a rigorous five-stage evaluation framework to assess
LLMs' auditing capabilities. The benchmark also challenges models to map
specific financial statement errors to corresponding violations of accounting
standards, simulating real-world auditing scenarios through test cases. Our
testing reveals that current state-of-the-art LLMs successfully identify
financial statement errors when given historical transaction data. However,
these models demonstrate significant limitations in explaining detected errors
and citing relevant accounting standards. Furthermore, LLMs struggle to execute
complete audits and make necessary financial statement revisions. These
findings highlight a critical gap in LLMs' domain-specific accounting
knowledge. Future research must focus on enhancing LLMs' understanding of
auditing principles and procedures. Our benchmark and evaluation framework
establish a foundation for developing more effective automated auditing tools
that will substantially improve the accuracy and efficiency of real-world
financial statement auditing.

</details>


### [137] [A Framework for Generating Conversational Recommendation Datasets from Behavioral Interactions](https://arxiv.org/abs/2506.17285)
*Vinaik Chhetri,Yousaf Reza,Moghis Fereidouni,Srijata Maji,Umar Farooq,AB Siddique*

Main category: cs.IR

TL;DR: ConvRecStudio, a framework using LLMs, simulates realistic dialogs for recommendation systems, unifying collaborative filtering and CRS. A cross-attention transformer model built using this framework achieves significant improvements in recommendation accuracy.


<details>
  <summary>Details</summary>
Motivation: Modern recommendation systems typically follow two complementary paradigms: collaborative filtering and conversational recommendation systems (CRS). Each captures a different dimension of user intent. While CRS models lack collaborative signals, leading to generic or poorly personalized suggestions, traditional recommenders lack mechanisms to interactively elicit immediate needs. Unifying these paradigms promises richer personalization but remains challenging due to the lack of large-scale conversational datasets grounded in real user behavior.

Method: ConvRecStudio, a framework that uses large language models (LLMs) to simulate realistic, multi-turn dialogs grounded in timestamped user-item interactions and reviews. It follows a three-stage pipeline: (1) Temporal Profiling, (2) Semantic Dialog Planning, and (3) Multi-Turn Simulation.

Result: ConvRecStudio is applied to three domains -- MobileRec, Yelp, and Amazon Electronics -- producing over 12K multi-turn dialogs per dataset. Human and automatic evaluations confirm the naturalness, coherence, and behavioral grounding of the generated conversations.

Conclusion: A cross-attention transformer model is built that jointly encodes user history and dialog context, achieving gains in Hit@K and NDCG@K over baselines. The model achieves a 10.9% improvement in Hit@1 on Yelp over the strongest baseline.

Abstract: Modern recommendation systems typically follow two complementary paradigms:
collaborative filtering, which models long-term user preferences from
historical interactions, and conversational recommendation systems (CRS), which
interact with users in natural language to uncover immediate needs. Each
captures a different dimension of user intent. While CRS models lack
collaborative signals, leading to generic or poorly personalized suggestions,
traditional recommenders lack mechanisms to interactively elicit immediate
needs. Unifying these paradigms promises richer personalization but remains
challenging due to the lack of large-scale conversational datasets grounded in
real user behavior. We present ConvRecStudio, a framework that uses large
language models (LLMs) to simulate realistic, multi-turn dialogs grounded in
timestamped user-item interactions and reviews. ConvRecStudio follows a
three-stage pipeline: (1) Temporal Profiling, which constructs user profiles
and community-level item sentiment trajectories over fine-grained aspects; (2)
Semantic Dialog Planning, which generates a structured plan using a DAG of
flexible super-nodes; and (3) Multi-Turn Simulation, which instantiates the
plan using paired LLM agents for the user and system, constrained by
executional and behavioral fidelity checks. We apply ConvRecStudio to three
domains -- MobileRec, Yelp, and Amazon Electronics -- producing over 12K
multi-turn dialogs per dataset. Human and automatic evaluations confirm the
naturalness, coherence, and behavioral grounding of the generated
conversations. To demonstrate utility, we build a cross-attention transformer
model that jointly encodes user history and dialog context, achieving gains in
Hit@K and NDCG@K over baselines using either signal alone or naive fusion.
Notably, our model achieves a 10.9% improvement in Hit@1 on Yelp over the
strongest baseline.

</details>


### [138] [Recommendation systems in e-commerce applications with machine learning methods](https://arxiv.org/abs/2506.17287)
*Aneta Poniszewska-Maranda,Magdalena Pakula,Bozena Borowska*

Main category: cs.IR

TL;DR: This paper reviews machine learning methods in e-commerce recommendation systems, analyzing trends, challenges, and the effectiveness of different approaches using a systematic literature review.


<details>
  <summary>Details</summary>
Motivation: E-commerce platforms are increasingly reliant on recommendation systems to enhance user experience, retain customers, and, in most cases, drive sales. The integration of machine learning methods into these systems has significantly improved their efficiency, personalization, and scalability.

Method: A systematic literature review (SLR) was conducted, analyzing 38 publications from 2013 to 2025. The methods used were evaluated and compared to determine their performance and effectiveness in addressing e-commerce challenges.

Result: The paper identifies current trends in e-commerce recommendation systems, identifies challenges, and evaluates the effectiveness of various machine learning methods used, including collaborative filtering, content-based filtering, and hybrid models.

Conclusion: This paper concludes by evaluating the effectiveness of various machine learning methods used in e-commerce recommendation systems.

Abstract: E-commerce platforms are increasingly reliant on recommendation systems to
enhance user experience, retain customers, and, in most cases, drive sales. The
integration of machine learning methods into these systems has significantly
improved their efficiency, personalization, and scalability. This paper aims to
highlight the current trends in e-commerce recommendation systems, identify
challenges, and evaluate the effectiveness of various machine learning methods
used, including collaborative filtering, content-based filtering, and hybrid
models. A systematic literature review (SLR) was conducted, analyzing 38
publications from 2013 to 2025. The methods used were evaluated and compared to
determine their performance and effectiveness in addressing e-commerce
challenges.

</details>


### [139] [SlimRAG: Retrieval without Graphs via Entity-Aware Context Selection](https://arxiv.org/abs/2506.17288)
*Jiale Zhang,Jiaxiang Chen,Zhucong Li,Jie Ding,Kui Zhao,Zenglin Xu,Xin Pang,Yinghui Xu*

Main category: cs.IR

TL;DR: SlimRAG是一种轻量级的无图检索框架，它使用实体感知机制来提高检索效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 基于图的RAG系统通常受到结构开销和不精确检索的影响：它们需要昂贵的实体链接和关系提取管道，但经常返回充满松散相关或切向内容的子图。这源于一个根本的缺陷——语义相似性并不意味着语义相关性。

Method: SlimRAG用一种简单而有效的实体感知机制取代了结构繁重的组件。在索引时，它构建一个基于语义嵌入的紧凑的实体到块表。在查询时，它识别显著的实体，检索和评分相关的块，并组装一个简洁的、上下文相关的输入——没有图遍历或边缘构建。

Result: SlimRAG优于强大的平面和基于图的基线，同时减少索引大小和RITU (例如，16.31 vs. 56+)。

Conclusion: SlimRAG在准确性方面优于强大的平面和基于图的基线，同时减少了索引大小和RITU，突出了无结构、以实体为中心的上下文选择的价值。

Abstract: Retrieval-Augmented Generation (RAG) enhances language models by
incorporating external knowledge at inference time. However, graph-based RAG
systems often suffer from structural overhead and imprecise retrieval: they
require costly pipelines for entity linking and relation extraction, yet
frequently return subgraphs filled with loosely related or tangential content.
This stems from a fundamental flaw -- semantic similarity does not imply
semantic relevance. We introduce SlimRAG, a lightweight framework for retrieval
without graphs. SlimRAG replaces structure-heavy components with a simple yet
effective entity-aware mechanism. At indexing time, it constructs a compact
entity-to-chunk table based on semantic embeddings. At query time, it
identifies salient entities, retrieves and scores associated chunks, and
assembles a concise, contextually relevant input -- without graph traversal or
edge construction. To quantify retrieval efficiency, we propose Relative Index
Token Utilization (RITU), a metric measuring the compactness of retrieved
content. Experiments across multiple QA benchmarks show that SlimRAG
outperforms strong flat and graph-based baselines in accuracy while reducing
index size and RITU (e.g., 16.31 vs. 56+), highlighting the value of
structure-free, entity-centric context selection. The code will be released
soon. https://github.com/continue-ai-company/SlimRAG

</details>


### [140] [PreQRAG -- Classify and Rewrite for Enhanced RAG](https://arxiv.org/abs/2506.17493)
*Damian Martinez,Catalina Riano,Hui Fang*

Main category: cs.IR

TL;DR: PreQRAG improves RAG performance by classifying questions and rewriting/decomposing them based on type, achieving 2nd place in LiveRAG Challenge Session 2.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve retrieval and generation quality in Retrieval Augmented Generation (RAG) through targeted question preprocessing.

Method: The paper introduces PreQRAG, a Retrieval Augmented Generation (RAG) architecture that classifies questions as single-document or multi-document type and applies question rewriting or decomposition techniques accordingly.

Result: PreQRAG achieves the preliminary second place in Session 2 of the LiveRAG challenge.

Conclusion: The PreQRAG architecture achieves second place in Session 2 of the LiveRAG challenge, demonstrating the effectiveness of the question-type-aware approach.

Abstract: This paper presents the submission of the UDInfo team to the SIGIR 2025
LiveRAG Challenge. We introduce PreQRAG, a Retrieval Augmented Generation (RAG)
architecture designed to improve retrieval and generation quality through
targeted question preprocessing. PreQRAG incorporates a pipeline that first
classifies each input question as either single-document or multi-document
type. For single-document questions, we employ question rewriting techniques to
improve retrieval precision and generation relevance. For multi-document
questions, we decompose complex queries into focused sub-questions that can be
processed more effectively by downstream components. This classification and
rewriting strategy improves the RAG performance. Experimental evaluation of the
LiveRAG Challenge dataset demonstrates the effectiveness of our
question-type-aware architecture, with PreQRAG achieving the preliminary second
place in Session 2 of the LiveRAG challenge.

</details>


### [141] [Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models](https://arxiv.org/abs/2506.17580)
*Sajratul Y. Rubaiat,Hasan M. Jamil*

Main category: cs.IR

TL;DR: WISE is a system that uses a structured workflow to extract, refine, and rank query-specific knowledge. It reduces processed text by over 80% while achieving significantly higher recall over baselines.


<details>
  <summary>Details</summary>
Motivation: The exponential growth of scientific literature challenges researchers extracting and synthesizing knowledge. Traditional search engines return many sources without direct, detailed answers, while general-purpose LLMs may offer concise responses that lack depth or omit current information. LLMs with search capabilities are also limited by context window, yielding short, incomplete answers.

Method: WISE uses an LLM-powered, tree-based architecture to refine data, focusing on query-aligned, context-aware, and non-redundant information. Dynamic scoring and ranking prioritize unique contributions from each source, and adaptive stopping criteria minimize processing overhead.

Result: WISE reduces processed text by over 80% while achieving significantly higher recall over baselines like search engines and other LLM-based approaches. ROUGE and BLEU metrics reveal WISE's output is more unique than other systems, and a novel level-based metric shows it provides more in-depth information.

Conclusion: WISE delivers detailed, organized answers by systematically exploring and synthesizing knowledge from diverse sources. Experiments on HBB gene-associated diseases demonstrate WISE reduces processed text by over 80% while achieving significantly higher recall over baselines like search engines and other LLM-based approaches. ROUGE and BLEU metrics reveal WISE's output is more unique than other systems, and a novel level-based metric shows it provides more in-depth information. We also explore how the WISE workflow can be adapted for diverse domains like drug discovery, material science, and social science, enabling efficient knowledge extraction and synthesis from unstructured scientific papers and web sources.

Abstract: The exponential growth of scientific literature challenges researchers
extracting and synthesizing knowledge. Traditional search engines return many
sources without direct, detailed answers, while general-purpose LLMs may offer
concise responses that lack depth or omit current information. LLMs with search
capabilities are also limited by context window, yielding short, incomplete
answers. This paper introduces WISE (Workflow for Intelligent Scientific
Knowledge Extraction), a system addressing these limits by using a structured
workflow to extract, refine, and rank query-specific knowledge. WISE uses an
LLM-powered, tree-based architecture to refine data, focusing on query-aligned,
context-aware, and non-redundant information. Dynamic scoring and ranking
prioritize unique contributions from each source, and adaptive stopping
criteria minimize processing overhead. WISE delivers detailed, organized
answers by systematically exploring and synthesizing knowledge from diverse
sources. Experiments on HBB gene-associated diseases demonstrate WISE reduces
processed text by over 80% while achieving significantly higher recall over
baselines like search engines and other LLM-based approaches. ROUGE and BLEU
metrics reveal WISE's output is more unique than other systems, and a novel
level-based metric shows it provides more in-depth information. We also explore
how the WISE workflow can be adapted for diverse domains like drug discovery,
material science, and social science, enabling efficient knowledge extraction
and synthesis from unstructured scientific papers and web sources.

</details>


### [142] [A novel fast short-time root music method for vibration monitoring of high-speed spindles](https://arxiv.org/abs/2506.17600)
*Huiguang Zhang,Baoguo Liu,Wei Feng,Zongtang Li*

Main category: cs.IR

TL;DR: Presents a fast Short-Time Root-MUSIC (fSTrM) algorithm that achieves real-time micro-defect detection capabilities.


<details>
  <summary>Details</summary>
Motivation: Ultra-high-speed spindle bearings challenge traditional vibration monitoring due to broadband noise, non-stationarity, and limited time-frequency resolution.

Method: We present a fast Short-Time Root-MUSIC (fSTrM) algorithm that exploits FFT-accelerated Lanczos bidiagonalization

Result: The algorithm reliably identifies 150 $\mu$m defects -- previously undetectable by conventional methods -- providing 72+ hours additional warning time. Compared to STFT and wavelet methods, fSTrM achieves 1.2 Hz frequency resolution (vs. 12.5 Hz), 93\% detection rate at $-$5 dB SNR

Conclusion: This advancement transforms bearing monitoring from failure prevention to continuous degradation assessment, establishing a new paradigm for predictive maintenance in aerospace and precision machining.

Abstract: Ultra-high-speed spindle bearings challenge traditional vibration monitoring
due to broadband noise, non-stationarity, and limited time-frequency
resolution. We present a fast Short-Time Root-MUSIC (fSTrM) algorithm that
exploits
  FFT-accelerated Lanczos bidiagonalization to reduce computational complexity
from $\mathcal{O}(N^3)$ to $SN\log_2N+S^2(N+S)+M^2(N+M)$
  while preserving parametric super-resolution. The method constructs Hankel
matrices from 16 ms signal frames and extracts fault frequencies through
polynomial rooting on the unit circle. Experimental validation on the
Politecnico di Torino bearing dataset demonstrates breakthrough micro-defect
detection capabilities. The algorithm reliably identifies 150 $\mu$m defects --
previously undetectable by conventional methods -- providing 72+ hours
additional warning time. Compared to STFT and wavelet methods, fSTrM achieves
1.2 Hz frequency resolution (vs. 12.5 Hz), 93\% detection rate at $-$5 dB SNR,
and quantifies defect severity through harmonic content analysis. Critically,
the algorithm processes each frame in 2.4 ms on embedded ARM Cortex-M7
hardware, enabling real-time deployment. This advancement transforms bearing
monitoring from failure prevention to continuous degradation assessment,
establishing a new paradigm for predictive maintenance in aerospace and
precision machining.

</details>


### [143] [Reinforcing User Interest Evolution in Multi-Scenario Learning for recommender systems](https://arxiv.org/abs/2506.17682)
*Zhijian Feng,Wenhao Zheng,Xuanji Xiao*

Main category: cs.IR

TL;DR: 提出了一种新的强化学习方法，通过对多个场景中的用户兴趣演变进行建模来对跨场景的用户偏好进行建模，从而在多场景推荐任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 真实世界的推荐系统中，用户会参与各种场景，例如主页、搜索页面和相关推荐页面。这些场景中的每一个都会反映用户关注的不同方面。然而，由于决策过程和偏好表达的差异，用户兴趣在不同场景中可能不一致。这种可变性使统一建模变得复杂，使多场景学习成为一项重大挑战。

Method: 提出了一种新的强化学习方法，该方法通过对多个场景中的用户兴趣演变进行建模来对跨场景的用户偏好进行建模。我们的方法采用双Q学习来提高下一个项目的预测准确性，并使用Q值优化对比学习损失，以提高模型性能。

Result: 实验结果表明，我们的方法在多场景推荐任务中优于现有方法。

Conclusion: 该方法在多场景推荐任务中优于现有方法，为多场景建模提供了新的视角，并突出了未来研究的有希望的方向。

Abstract: In real-world recommendation systems, users would engage in variety
scenarios, such as homepages, search pages, and related recommendation pages.
Each of these scenarios would reflect different aspects users focus on.
However, the user interests may be inconsistent in different scenarios, due to
differences in decision-making processes and preference expression. This
variability complicates unified modeling, making multi-scenario learning a
significant challenge. To address this, we propose a novel reinforcement
learning approach that models user preferences across scenarios by modeling
user interest evolution across multiple scenarios. Our method employs Double
Q-learning to enhance next-item prediction accuracy and optimizes contrastive
learning loss using Q-value to make model performance better. Experimental
results demonstrate that our approach surpasses state-of-the-art methods in
multi-scenario recommendation tasks. Our work offers a fresh perspective on
multi-scenario modeling and highlights promising directions for future
research.

</details>


### [144] [CARTS: Collaborative Agents for Recommendation Textual Summarization](https://arxiv.org/abs/2506.17765)
*Jiao Chen,Kehui Yao,Reza Yousefi Maragheh,Kai Zhao,Jianpeng Xu,Jason Cho,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.IR

TL;DR: CARTS是一种用于推荐系统结构化摘要的多代理LLM框架，它优于其他LLM基线。


<details>
  <summary>Details</summary>
Motivation: 当前的推荐系统通常需要某种形式的文本数据摘要，例如为产品轮播或其他分组项目显示生成简洁而连贯的标题。虽然大型语言模型已在NLP领域的文本摘要中显示出希望，但这些方法并不直接适用于推荐系统，在推荐系统中，解释必须与项目集的核心功能高度相关，并遵守严格的字数限制。

Method: CARTS（推荐文本摘要的协作代理），一个多代理LLM框架，专为推荐系统中的结构化摘要而设计。CARTS将任务分解为三个阶段-生成增强生成（GAG），细化圈和仲裁，其中连续的代理角色负责提取显着的项目特征，基于相关性和长度反馈迭代地细化候选标题，并通过协作仲裁过程选择最终标题。

Result: 在大型电子商务数据和实时A / B测试上的实验表明，CARTS显着优于单通道和思维链LLM基线，从而提供了更高的标题相关性和改进的用户参与度指标。

Conclusion: CARTS显著优于单通道和思维链LLM基线，提供更高的标题相关性和改进的用户参与度指标。

Abstract: Current recommendation systems often require some form of textual data
summarization, such as generating concise and coherent titles for product
carousels or other grouped item displays. While large language models have
shown promise in NLP domains for textual summarization, these approaches do not
directly apply to recommendation systems, where explanations must be highly
relevant to the core features of item sets, adhere to strict word limit
constraints. In this paper, we propose CARTS (Collaborative Agents for
Recommendation Textual Summarization), a multi-agent LLM framework designed for
structured summarization in recommendation systems. CARTS decomposes the task
into three stages-Generation Augmented Generation (GAG), refinement circle, and
arbitration, where successive agent roles are responsible for extracting
salient item features, iteratively refining candidate titles based on relevance
and length feedback, and selecting the final title through a collaborative
arbitration process. Experiments on large-scale e-commerce data and live A/B
testing show that CARTS significantly outperforms single-pass and
chain-of-thought LLM baselines, delivering higher title relevance and improved
user engagement metrics.

</details>


### [145] [Expanding Relevance Judgments for Medical Case-based Retrieval Task with Multimodal LLMs](https://arxiv.org/abs/2506.17782)
*Catarina Pires,Sérgio Nunes,Luís Filipe Teixeira*

Main category: cs.IR

TL;DR: The paper explores using a Multimodal Large Language Model (MLLM) to expand relevance judgments in medical case-based retrieval. They used Gemini 1.5 Pro and achieved substantial agreement with human judgments, significantly expanding the dataset of relevance judgments.


<details>
  <summary>Details</summary>
Motivation: Evaluating Information Retrieval (IR) systems relies on high-quality manual relevance judgments (qrels), which are costly and time-consuming to obtain. Large Language Models (LLMs) offer a promising alternative to reducing reliance on manual judgments, particularly in complex domains like medical case-based retrieval, where relevance assessment requires analyzing both textual and visual information.

Method: The authors employ Gemini 1.5 Pro on the ImageCLEFmed 2013 case-based retrieval task, simulating human assessment through an iteratively refined, structured prompting strategy that integrates binary scoring, instruction-based evaluation, and few-shot learning. They systematically experimented with various prompt configurations to maximize agreement with human judgments, using Cohen's Kappa to evaluate agreement between the MLLM and human judgments.

Result: The MLLM achieved a substantial agreement score of 0.6 (Cohen's Kappa), comparable to inter-annotator agreement typically observed in multimodal retrieval tasks. Starting from 15,028 manual judgments (4.72% relevant), the MLLM-based approach expanded the dataset by over 37x to 558,653 judgments, increasing relevant annotations to 5,950. On average, each medical case query received 15,398 new annotations.

Conclusion: This paper demonstrates the potential of Multimodal Large Language Models (MLLMs) to scale relevance judgment collection, offering a promising direction for supporting retrieval evaluation in medical and multimodal IR tasks.

Abstract: Evaluating Information Retrieval (IR) systems relies on high-quality manual
relevance judgments (qrels), which are costly and time-consuming to obtain.
While pooling reduces the annotation effort, it results in only partially
labeled datasets. Large Language Models (LLMs) offer a promising alternative to
reducing reliance on manual judgments, particularly in complex domains like
medical case-based retrieval, where relevance assessment requires analyzing
both textual and visual information. In this work, we explore using a
Multimodal Large Language Model (MLLM) to expand relevance judgments, creating
a new dataset of automated judgments. Specifically, we employ Gemini 1.5 Pro on
the ImageCLEFmed 2013 case-based retrieval task, simulating human assessment
through an iteratively refined, structured prompting strategy that integrates
binary scoring, instruction-based evaluation, and few-shot learning. We
systematically experimented with various prompt configurations to maximize
agreement with human judgments. To evaluate agreement between the MLLM and
human judgments, we use Cohen's Kappa, achieving a substantial agreement score
of 0.6, comparable to inter-annotator agreement typically observed in
multimodal retrieval tasks. Starting from the original 15,028 manual judgments
(4.72% relevant) across 35 topics, our MLLM-based approach expanded the dataset
by over 37x to 558,653 judgments, increasing relevant annotations to 5,950. On
average, each medical case query received 15,398 new annotations, with
approximately 99% being non-relevant, reflecting the high sparsity typical in
this domain. Our results demonstrate the potential of MLLMs to scale relevance
judgment collection, offering a promising direction for supporting retrieval
evaluation in medical and multimodal IR tasks.

</details>


### [146] [A GenAI System for Improved FAIR Independent Biological Database Integration](https://arxiv.org/abs/2506.17934)
*Syed N. Sakib,Kallol Naha,Sajratul Y. Rubaiat,Hasan M. Jamil*

Main category: cs.IR

TL;DR: FAIRBridge是一个基于自然语言的查询处理系统，旨在帮助科学家发现、访问和查询生物数据库，即使它们不符合FAIR标准。


<details>
  <summary>Details</summary>
Motivation: 生命科学研究越来越需要识别、访问和有效处理来自链接开放数据（LOD）网络上不断发展的信息源的数据。查询响应的质量在很大程度上取决于数据源的选择和语义集成，而这些过程通常是劳动密集型、容易出错且成本高昂的。

Method: FAIRBridge利用人工智能解释查询意图，将其映射到科学文献中描述的相关数据库，并通过智能资源访问计划生成可执行的查询。

Result: FAIRBridge的自主查询处理框架使用户能够探索替代数据源，在每个步骤做出明智的选择，并在需要时利用社区驱动的众包管理。

Conclusion: FAIRBridge通过提供用户友好的自动化假设测试平台，显著增强了科学数据的集成和处理，为研究人员提供了一个强大的新工具来推进他们的研究。

Abstract: Life sciences research increasingly requires identifying, accessing, and
effectively processing data from an ever-evolving array of information sources
on the Linked Open Data (LOD) network. This dynamic landscape places a
significant burden on researchers, as the quality of query responses depends
heavily on the selection and semantic integration of data sources --processes
that are often labor-intensive, error-prone, and costly. While the adoption of
FAIR (Findable, Accessible, Interoperable, and Reusable) data principles has
aimed to address these challenges, barriers to efficient and accurate
scientific data processing persist.
  In this paper, we introduce FAIRBridge, an experimental natural
language-based query processing system designed to empower scientists to
discover, access, and query biological databases, even when they are not
FAIR-compliant. FAIRBridge harnesses the capabilities of AI to interpret query
intents, map them to relevant databases described in scientific literature, and
generate executable queries via intelligent resource access plans. The system
also includes robust tools for mitigating low-quality query processing,
ensuring high fidelity and responsiveness in the information delivered.
  FAIRBridge's autonomous query processing framework enables users to explore
alternative data sources, make informed choices at every step, and leverage
community-driven crowd curation when needed. By providing a user-friendly,
automated hypothesis-testing platform in natural English, FAIRBridge
significantly enhances the integration and processing of scientific data,
offering researchers a powerful new tool for advancing their inquiries.

</details>


### [147] [LLM-Enhanced Multimodal Fusion for Cross-Domain Sequential Recommendation](https://arxiv.org/abs/2506.17966)
*Wangyu Wu,Zhenhong Chen,Xianglin Qiu,Siqi Song,Xiaowei Huang,Fei Ma,Jimin Xiao*

Main category: cs.IR

TL;DR: LLM-EMF：一种用于跨域序列推荐的 LLM 增强型多模态融合方法，它通过融合视觉和文本数据来增强文本信息，从而显着提高推荐性能。


<details>
  <summary>Details</summary>
Motivation: 跨领域序列推荐 (CDSR) 通过利用跨多个领域的历史交互来预测用户行为，重点是建模跨领域偏好并捕获序列内和序列间项目关系。

Method: LLM-EMF，一种新颖而先进的方法，它利用大型语言模型 (LLM) 知识增强文本信息，并通过视觉和文本数据的融合显着提高推荐性能。使用冻结的 CLIP 模型，我们生成图像和文本嵌入，从而利用多模态数据丰富项目表示。

Result: 在四个电子商务数据集上进行的评估表明，LLM-EMF 在建模跨领域用户偏好方面始终优于现有方法。

Conclusion: LLM-EMF在建模跨领域用户偏好方面始终优于现有方法，突出了多模态数据集成在增强序列推荐系统方面的有效性和优势。

Abstract: Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by
leveraging historical interactions across multiple domains, focusing on
modeling cross-domain preferences and capturing both intra- and inter-sequence
item relationships. We propose LLM-Enhanced Multimodal Fusion for Cross-Domain
Sequential Recommendation (LLM-EMF), a novel and advanced approach that
enhances textual information with Large Language Models (LLM) knowledge and
significantly improves recommendation performance through the fusion of visual
and textual data. Using the frozen CLIP model, we generate image and text
embeddings, thereby enriching item representations with multimodal data. A
multiple attention mechanism jointly learns both single-domain and cross-domain
preferences, effectively capturing and understanding complex user interests
across diverse domains. Evaluations conducted on four e-commerce datasets
demonstrate that LLM-EMF consistently outperforms existing methods in modeling
cross-domain user preferences, thereby highlighting the effectiveness of
multimodal data integration and its advantages in enhancing sequential
recommendation systems. Our source code will be released.

</details>


### [148] [Comparative Analysis of Lion and AdamW Optimizers for Cross-Encoder Reranking with MiniLM, GTE, and ModernBERT](https://arxiv.org/abs/2506.18297)
*Shahil Kumar,Manu Pande,Anay Yatin Damle*

Main category: cs.IR

TL;DR: This paper studies the impact of the Lion optimizer during fine-tuning of cross-encoder rerankers.


<details>
  <summary>Details</summary>
Motivation: Cross-encoders have shown strong effectiveness for reranking due to their deep analysis of query-document pairs. This paper studies the impact of the Lion optimizer, a recent alternative to AdamW, during fine-tuning of cross-encoder rerankers.

Method: fine-tune three transformer models-MiniLM, GTE, and ModernBERT-on the MS MARCO passage ranking dataset using both optimizers

Result: ModernBERT with Lion achieves the best NDCG@10 and MAP on TREC DL 2019, while MiniLM with Lion ties ModernBERT for MRR@10 on MS MARCO dev. Lion also provides superior GPU efficiency, improving utilization by 2.67% to 10.33% across models.

Conclusion: ModernBERT with Lion achieves the best NDCG@10 (0.7225) and MAP (0.5121) on TREC DL 2019, while MiniLM with Lion ties ModernBERT for MRR@10 (0.5988) on MS MARCO dev. Lion also provides superior GPU efficiency, improving utilization by 2.67% to 10.33% across models.

Abstract: Modern information retrieval systems often employ a two-stage pipeline: an
efficient initial retrieval stage followed by a computationally intensive
reranking stage. Cross-encoders have shown strong effectiveness for reranking
due to their deep analysis of query-document pairs. This paper studies the
impact of the Lion optimizer, a recent alternative to AdamW, during fine-tuning
of cross-encoder rerankers. We fine-tune three transformer models-MiniLM, GTE,
and ModernBERT-on the MS MARCO passage ranking dataset using both optimizers.
GTE and ModernBERT support extended context lengths (up to 8192 tokens). We
evaluate effectiveness using TREC 2019 Deep Learning Track and MS MARCO dev set
(MRR@10). Experiments, run on the Modal cloud platform, reveal that ModernBERT
with Lion achieves the best NDCG@10 (0.7225) and MAP (0.5121) on TREC DL 2019,
while MiniLM with Lion ties ModernBERT for MRR@10 (0.5988) on MS MARCO dev.
Lion also provides superior GPU efficiency, improving utilization by 2.67% to
10.33% across models. We analyze performance trends using standard IR metrics
and discuss the optimizer's impact on training dynamics across architectures.

</details>


### [149] [LettinGo: Explore User Profile Generation for Recommendation System](https://arxiv.org/abs/2506.18309)
*Lu Wang,Di Zhang,Fangkai Yang,Pu Zhao,Jianfeng Liu,Yuefeng Zhan,Hao Sun,Qingwei Lin,Weiwei Deng,Dongmei Zhang,Feng Sun,Qi Zhang*

Main category: cs.IR

TL;DR: LettinGo, a novel framework for generating diverse and adaptive user profiles, enhances recommendation accuracy, flexibility, and contextual awareness.


<details>
  <summary>Details</summary>
Motivation: Existing methods often adhere to fixed formats that limit their ability to capture the full diversity of user behaviors.

Method: LettinGo operates in three stages: (1) exploring diverse user profiles via multiple LLMs, (2) evaluating profile quality based on their impact in recommendation systems, and (3) aligning the profile generation through pairwise preference data derived from task performance.

Result: Experimental results demonstrate that our framework significantly enhances recommendation accuracy, flexibility, and contextual awareness.

Conclusion: This work enhances profile generation as a key innovation for next-generation recommendation systems.

Abstract: User profiling is pivotal for recommendation systems, as it transforms raw
user interaction data into concise and structured representations that drive
personalized recommendations. While traditional embedding-based profiles lack
interpretability and adaptability, recent advances with large language models
(LLMs) enable text-based profiles that are semantically richer and more
transparent. However, existing methods often adhere to fixed formats that limit
their ability to capture the full diversity of user behaviors. In this paper,
we introduce LettinGo, a novel framework for generating diverse and adaptive
user profiles. By leveraging the expressive power of LLMs and incorporating
direct feedback from downstream recommendation tasks, our approach avoids the
rigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ
Direct Preference Optimization (DPO) to align the profile generator with
task-specific performance, ensuring that the profiles remain adaptive and
effective. LettinGo operates in three stages: (1) exploring diverse user
profiles via multiple LLMs, (2) evaluating profile quality based on their
impact in recommendation systems, and (3) aligning the profile generation
through pairwise preference data derived from task performance. Experimental
results demonstrate that our framework significantly enhances recommendation
accuracy, flexibility, and contextual awareness. This work enhances profile
generation as a key innovation for next-generation recommendation systems.

</details>


### [150] [Enhancing Document Retrieval in COVID-19 Research: Leveraging Large Language Models for Hidden Relation Extraction](https://arxiv.org/abs/2506.18311)
*Hoang-An Trieu,Dinh-Truong Do,Chau Nguyen,Vu Tran,Minh Le Nguyen*

Main category: cs.IR

TL;DR: Uses LLMs to improve COVID-19 publication retrieval by extracting hidden relationships.


<details>
  <summary>Details</summary>
Motivation: The COVID-19 pandemic has led to a massive volume of publications, necessitating an efficient retrieval system to provide researchers with useful information.

Method: Exploiting large language models (LLMs) to extract hidden relationships inside the unlabeled publication.

Result: The LLMs help the system have more useful information during retrieval progress.

Conclusion: This paper presents a method to improve the Covrelex-SE retrieval system by using LLMs to extract hidden relationships within publications.

Abstract: In recent years, with the appearance of the COVID-19 pandemic, numerous
publications relevant to this disease have been issued. Because of the massive
volume of publications, an efficient retrieval system is necessary to provide
researchers with useful information if an unexpected pandemic happens so
suddenly, like COVID-19. In this work, we present a method to help the
retrieval system, the Covrelex-SE system, to provide more high-quality search
results. We exploited the power of the large language models (LLMs) to extract
the hidden relationships inside the unlabeled publication that cannot be found
by the current parsing tools that the system is using. Since then, help the
system to have more useful information during retrieval progress.

</details>


### [151] [Team LA at SCIDOCA shared task 2025: Citation Discovery via relation-based zero-shot retrieval](https://arxiv.org/abs/2506.18316)
*Trieu An,Long Nguyen,Minh Le Nguyen*

Main category: cs.IR

TL;DR: A system using relational features and a Large Language Model (LLM) is developed to predict citations from a candidate pool, addressing challenges of abstract length and similarity. It is evaluated on the SCIDOCA 2025 training dataset.


<details>
  <summary>Details</summary>
Motivation: The main challenges stem from the length of the abstract paragraphs and the high similarity among candidate abstracts, making it difficult to determine the exact paper to cite.

Method: A system is developed that first retrieves the top-k most similar abstracts based on extracted relational features from the given paragraph. From this subset, a Large Language Model (LLM) is leveraged to accurately identify the most relevant citation.

Result: The system demonstrates its effectiveness in citation prediction.

Conclusion: The framework's effectiveness in citation prediction is demonstrated on the training dataset provided by the SCIDOCA 2025 organizers.

Abstract: The Citation Discovery Shared Task focuses on predicting the correct citation
from a given candidate pool for a given paragraph. The main challenges stem
from the length of the abstract paragraphs and the high similarity among
candidate abstracts, making it difficult to determine the exact paper to cite.
To address this, we develop a system that first retrieves the top-k most
similar abstracts based on extracted relational features from the given
paragraph. From this subset, we leverage a Large Language Model (LLM) to
accurately identify the most relevant citation. We evaluate our framework on
the training dataset provided by the SCIDOCA 2025 organizers, demonstrating its
effectiveness in citation prediction.

</details>


### [152] [Bias vs Bias -- Dawn of Justice: A Fair Fight in Recommendation Systems](https://arxiv.org/abs/2506.18327)
*Tahsin Alamgir Kheya,Mohamed Reda Bouadjenek,Sunil Aryal*

Main category: cs.IR

TL;DR: 提出了一种新的公平感知重排序方法，以减轻推荐系统中不同项目类别和多个敏感属性的偏差。


<details>
  <summary>Details</summary>
Motivation: 以往解决推荐系统偏差的工作忽略了某些项目类别中的偏差，并且大多数关于公平重排序的工作都集中在二元敏感属性上。

Method: 提出了一种公平感知重排序方法，该方法利用现有偏见来纠正不同人口群体在推荐方面的差异。

Result: 在三个真实世界的数据集上进行了实验，以评估该重排序方案在减轻推荐偏差方面的有效性。结果表明，该方法有助于减少社会偏见，且几乎不降低性能。

Conclusion: 该方法有助于减少社会偏见，且几乎不降低性能。

Abstract: Recommendation systems play a crucial role in our daily lives by impacting
user experience across various domains, including e-commerce, job
advertisements, entertainment, etc. Given the vital role of such systems in our
lives, practitioners must ensure they do not produce unfair and imbalanced
recommendations. Previous work addressing bias in recommendations overlooked
bias in certain item categories, potentially leaving some biases unaddressed.
Additionally, most previous work on fair re-ranking focused on binary-sensitive
attributes. In this paper, we address these issues by proposing a
fairness-aware re-ranking approach that helps mitigate bias in different
categories of items. This re-ranking approach leverages existing biases to
correct disparities in recommendations across various demographic groups. We
show how our approach can mitigate bias on multiple sensitive attributes,
including gender, age, and occupation. We experimented on three real-world
datasets to evaluate the effectiveness of our re-ranking scheme in mitigating
bias in recommendations. Our results show how this approach helps mitigate
social bias with little to no degradation in performance.

</details>


### [153] [PERSCEN: Learning Personalized Interaction Pattern and Scenario Preference for Multi-Scenario Matching](https://arxiv.org/abs/2506.18382)
*Haotong Du,Yaqing Wang,Fei Xiong,Lei Shao,Ming Liu,Hao Gu,Quanming Yao,Zhen Wang*

Main category: cs.IR

TL;DR: PERSCEN: incorporates user-specific modeling into multi-scenario matching to capture both user preferences shared across all scenarios and scenario-aware preferences specific to each scenario.


<details>
  <summary>Details</summary>
Motivation: existing methods often overlook user-specific modeling, limiting the generation of personalized user representations in multi-scenario matching.

Method: incorporates user-specific modeling into multi-scenario matching; constructs a user-specific feature graph and employs a lightweight graph neural network; leverages vector quantization techniques to distil scenario-aware preferences; introduces a progressive scenario-aware gated linear unit.

Result: PERSCEN outperforms existing methods. Further efficiency analysis confirms that PERSCEN effectively balances performance with computational cost.

Conclusion: PERSCEN outperforms existing methods and balances performance with computational cost.

Abstract: With the expansion of business scales and scopes on online platforms,
multi-scenario matching has become a mainstream solution to reduce maintenance
costs and alleviate data sparsity. The key to effective multi-scenario
recommendation lies in capturing both user preferences shared across all
scenarios and scenario-aware preferences specific to each scenario. However,
existing methods often overlook user-specific modeling, limiting the generation
of personalized user representations. To address this, we propose PERSCEN, an
innovative approach that incorporates user-specific modeling into
multi-scenario matching. PERSCEN constructs a user-specific feature graph based
on user characteristics and employs a lightweight graph neural network to
capture higher-order interaction patterns, enabling personalized extraction of
preferences shared across scenarios. Additionally, we leverage vector
quantization techniques to distil scenario-aware preferences from users'
behavior sequence within individual scenarios, facilitating user-specific and
scenario-aware preference modeling. To enhance efficient and flexible
information transfer, we introduce a progressive scenario-aware gated linear
unit that allows fine-grained, low-latency fusion. Extensive experiments
demonstrate that PERSCEN outperforms existing methods. Further efficiency
analysis confirms that PERSCEN effectively balances performance with
computational cost, ensuring its practicality for real-world industrial
systems.

</details>


### [154] [Rethinking Click Models in Light of Carousel Interfaces: Theory-Based Categorization and Design of Click Models](https://arxiv.org/abs/2506.18548)
*Jingwei Kang,Maarten de Rijke,Santiago de Leon-Martinez,Harrie Oosterhuis*

Main category: cs.IR

TL;DR: This paper introduces a new click model taxonomy that includes PGMs and NNs and provides a foundation for future click model design by an example derivation of a novel design for carousel interfaces.


<details>
  <summary>Details</summary>
Motivation: Existing categorizations are unable to meaningfully compare PGM with neural network click models nor generalize to newer interfaces, such as carousel interfaces. This outdated view fails to adequately explain the fundamentals of click model designs, thus hindering the development of novel click models.

Method: This work reconsiders what should be the fundamental concepts in click model design, grounding them in their mathematical properties. Based on these choices, the work creates a novel click model taxonomy.

Result: The conceptualization provides a foundation for future click model design by an example derivation of a novel design for carousel interfaces.

Conclusion: This work proposes three fundamental key-design choices that explain what statistical patterns a click model can capture and creates a novel click model taxonomy that allows a meaningful comparison of all existing click models.

Abstract: Click models are a well-established for modeling user interactions with web
interfaces. Previous work has mainly focused on traditional single-list web
search settings; this includes existing surveys that introduced categorizations
based on the first generation of probabilistic graphical model (PGM) click
models that have become standard. However, these categorizations have become
outdated, as their conceptualizations are unable to meaningfully compare PGM
with neural network (NN) click models nor generalize to newer interfaces, such
as carousel interfaces. We argue that this outdated view fails to adequately
explain the fundamentals of click model designs, thus hindering the development
of novel click models.
  This work reconsiders what should be the fundamental concepts in click model
design, grounding them - unlike previous approaches - in their mathematical
properties. We propose three fundamental key-design choices that explain what
statistical patterns a click model can capture, and thus indirectly, what user
behaviors they can capture. Based on these choices, we create a novel click
model taxonomy that allows a meaningful comparison of all existing click
models; this is the first taxonomy of single-list, grid and carousel click
models that includes PGMs and NNs. Finally, we show how our conceptualization
provides a foundation for future click model design by an example derivation of
a novel design for carousel interfaces.

</details>


### [155] [Harnessing the Power of Reinforcement Learning for Language-Model-Based Information Retriever via Query-Document Co-Augmentation](https://arxiv.org/abs/2506.18670)
*Jingming Liu,Yumeng Li,Wei Shi,Yao-Xiang Ding,Hui Su,Kun Zhou*

Main category: cs.IR

TL;DR: This paper introduces an LLM-based retriever that augments both user queries and corpus documents, trained with a bidirectional RL framework to improve retrieval performance, especially in challenging domains.


<details>
  <summary>Details</summary>
Motivation: Enhancing queries alone is insufficient for robust semantic matching; the LLM should also have sufficient understanding of the corpus by directly handling and augmenting the documents themselves.

Method: LLM-based retriever empowered to augment both user queries and corpus documents, with its policy fully explored via reinforcement learning (RL) and minimal human inductive bias.Introduces a reward sampling strategy and a specifically designed RL algorithm that enables effective training with these sampled rewards

Result: Simply allowing the LLM to modify documents yields little benefit unless paired with our carefully designed bidirectional RL framework, which enables the LLM to simultaneously learn and collaborate on both query and document augmentation policies.

Conclusion: The approach significantly enhances LLM-based retrieval performance in both sparse and dense settings, particularly in difficult retrieval domains, and achieves strong cross-benchmark generalization.

Abstract: Recent studies have proposed leveraging Large Language Models (LLMs) as
information retrievers through query rewriting. However, for challenging
corpora, we argue that enhancing queries alone is insufficient for robust
semantic matching; the LLM should also have sufficient understanding of the
corpus by directly handling and augmenting the documents themselves. To this
end, we present an LLM-based retriever empowered to augment both user queries
and corpus documents, with its policy fully explored via reinforcement learning
(RL) and minimal human inductive bias. Notably, we find that simply allowing
the LLM to modify documents yields little benefit unless paired with our
carefully designed bidirectional RL framework, which enables the LLM to
simultaneously learn and collaborate on both query and document augmentation
policies. A key technical challenge in realizing such a framework lies in
jointly updating both policies during training, where the rewards for the two
directions depend on each other, making their entangled reward intractable. Our
approach addresses this by introducing a reward sampling strategy and a
specifically designed RL algorithm that enables effective training with these
sampled rewards. Experimental results demonstrate that our approach
significantly enhances LLM-based retrieval performance in both sparse and dense
settings, particularly in difficult retrieval domains, and achieves strong
cross-benchmark generalization. Our code is released at
https://github.com/liujm2001/CoAugRetriever.

</details>


### [156] [An Audio-centric Multi-task Learning Framework for Streaming Ads Targeting on Spotify](https://arxiv.org/abs/2506.18735)
*Shivam Verma,Vivian Chen,Darren Mei*

Main category: cs.IR

TL;DR: 我们提出了CAMoE，一个用于优化Spotify广告点击率的框架，通过模态感知和深度学习显著提高了各种广告格式的性能。


<details>
  <summary>Details</summary>
Motivation: 传统广告推荐模型主要为前景体验而设计，通常难以协调平台的内在音频中心性与跨多种格式和模态优化广告性能的需求。

Method: 我们引入了跨模态自适应混合专家（CAMoE），这是一种新颖的框架，用于优化以音频为中心和多模态环境中的点击率（CTR）预测。CAMoE通过结合模态感知任务分组、自适应损失掩蔽和深度交叉网络（DCN）来增强传统的混合专家模型，以捕获多模态广告生态系统中的复杂特征交互。

Result: 通过广泛的消融研究，我们证明了这种方法在音频、视频和展示广告格式上实现了接近帕累托最优的性能，与传统的单任务和基于内容的多任务学习基线相比，显着提高了AUC-PR。

Conclusion: CAMoE在Spotify的广告服务平台上大规模部署后，音频广告的点击率提高了14.5%，视频广告的点击率提高了1.3%，音频广告位的预期每次点击成本降低了4.8%。

Abstract: Spotify, a large-scale multimedia platform, attracts over 675 million monthly
active users who collectively consume millions of hours of music, podcasts,
audiobooks, and video content. This diverse content consumption pattern
introduces unique challenges for computational advertising, which must
effectively integrate a variety of ad modalities, including audio, video, and
display, within a single user experience. Traditional ad recommendation models,
primarily designed for foregrounded experiences, often struggle to reconcile
the platform's inherent audio-centrality with the demands of optimizing ad
performance across multiple formats and modalities. To overcome these
challenges, we introduce Cross-modal Adaptive Mixture-of-Experts (CAMoE), a
novel framework for optimizing click-through rate (CTR) prediction in both
audio-centric and multi-modal settings. CAMoE enhances traditional
mixture-of-experts models by incorporating modality-aware task grouping,
adaptive loss masking, and deep-cross networks (DCN) to capture complex feature
interactions within a multi-modal ad ecosystem. Through extensive ablation
studies, we demonstrate that this approach achieves near Pareto-optimal
performance across audio, video, and display ad formats, significantly
improving AUC-PR compared to conventional single-task and content-based
multi-task learning baselines. When deployed at scale on Spotify's ad serving
platform, CAMoE delivered substantial gains, yielding a 14.5% increase in CTR
for audio ads, a 1.3% increase for video ads, and a 4.8% reduction in expected
cost-per-click (eCPC) for audio slots.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [157] [MMET: A Multi-Input and Multi-Scale Transformer for Efficient PDEs Solving](https://arxiv.org/abs/2506.17230)
*Yichen Luo,Jia Wang,Dapeng Lan,Yu Liu,Zhibo Pang*

Main category: cs.LG

TL;DR: 本文提出了一种新的Transformer框架MMET，用于高效求解多输入和多尺度偏微分方程，并在准确性和计算效率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 基于机器学习的方法通用地、高效地求解偏微分方程仍然具有挑战，这是由于有限的多输入和多尺度泛化能力以及高计算成本。

Method: 该论文提出了多输入和多尺度高效Transformer (MMET)，这是一种旨在解决上述挑战的新颖框架。MMET 将网格和查询点解耦为两个序列，并将它们分别馈送到编码器和解码器中，并使用门控条件嵌入 (GCE) 层来嵌入具有不同维度的输入变量或函数，从而能够有效地解决多尺度和多输入问题。此外，基于希尔伯特曲线的重新序列化和补丁嵌入机制减少了输入长度。

Result: MMET 在准确性和计算效率方面均优于 SOTA 方法。

Conclusion: MMET在不同物理领域的各种基准测试中，在准确性和计算效率方面均优于 SOTA 方法。这项工作强调了 MMET 作为工程和基于物理的应用程序中实时 PDE 求解的强大且可扩展的解决方案的潜力，为未来探索特定领域中预训练的大规模模型铺平了道路。

Abstract: Partial Differential Equations (PDEs) are fundamental for modeling physical
systems, yet solving them in a generic and efficient manner using machine
learning-based approaches remains challenging due to limited multi-input and
multi-scale generalization capabilities, as well as high computational costs.
This paper proposes the Multi-input and Multi-scale Efficient Transformer
(MMET), a novel framework designed to address the above challenges. MMET
decouples mesh and query points as two sequences and feeds them into the
encoder and decoder, respectively, and uses a Gated Condition Embedding (GCE)
layer to embed input variables or functions with varying dimensions, enabling
effective solutions for multi-scale and multi-input problems. Additionally, a
Hilbert curve-based reserialization and patch embedding mechanism decrease the
input length. This significantly reduces the computational cost when dealing
with large-scale geometric models. These innovations enable efficient
representations and support multi-scale resolution queries for large-scale and
multi-input PDE problems. Experimental evaluations on diverse benchmarks
spanning different physical fields demonstrate that MMET outperforms SOTA
methods in both accuracy and computational efficiency. This work highlights the
potential of MMET as a robust and scalable solution for real-time PDE solving
in engineering and physics-based applications, paving the way for future
explorations into pre-trained large-scale models in specific domains. This work
is open-sourced at https://github.com/YichenLuo-0/MMET.

</details>


### [158] [PCaM: A Progressive Focus Attention-Based Information Fusion Method for Improving Vision Transformer Domain Adaptation](https://arxiv.org/abs/2506.17232)
*Zelin Zang,Fei Wang,Liangyu Li,Jinlin Wu,Chunshui Zhao,Zhen Lei,Baigui Sun*

Main category: cs.LG

TL;DR: 提出了一种Progressive Focus Cross-Attention Mechanism (PCaM)，以解决非监督领域自适应中前景对象不匹配的问题，并在多个数据集上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 前景对象不匹配，其中跨域的前景对象大小和空间分布的差异削弱了注意力一致性，并阻碍了有效的领域对齐。

Method: Progressive Focus Cross-Attention Mechanism (PCaM)

Result: 在Office-Home、DomainNet、VisDA-2017和遥感数据集上的大量实验表明，PCaM显著提高了适应性能，并实现了新的最先进的结果。

Conclusion: PCaM显著提高了适应性能，并实现了新的最先进的结果，验证了注意导向的前景融合对领域适应的有效性。

Abstract: Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a
labeled source domain to an unlabeled target domain. Recent UDA methods based
on Vision Transformers (ViTs) have achieved strong performance through
attention-based feature alignment. However, we identify a key limitation:
foreground object mismatch, where the discrepancy in foreground object size and
spatial distribution across domains weakens attention consistency and hampers
effective domain alignment. To address this issue, we propose the Progressive
Focus Cross-Attention Mechanism (PCaM), which progressively filters out
background information during cross-attention, allowing the model to focus on
and fuse discriminative foreground semantics across domains. We further
introduce an attentional guidance loss that explicitly directs attention toward
task-relevant regions, enhancing cross-domain attention consistency. PCaM is
lightweight, architecture-agnostic, and easy to integrate into existing
ViT-based UDA pipelines. Extensive experiments on Office-Home, DomainNet,
VisDA-2017, and remote sensing datasets demonstrate that PCaM significantly
improves adaptation performance and achieves new state-of-the-art results,
validating the effectiveness of attention-guided foreground fusion for domain
adaptation.

</details>


### [159] [Graph Neural Networks in Multi-Omics Cancer Research: A Structured Survey](https://arxiv.org/abs/2506.17234)
*Payam Zohari,Mostafa Haghir Chehreghani*

Main category: cs.LG

TL;DR: GNNs are increasingly used for multi-omics cancer research. This review classifies GNN approaches and highlights trends like hybrid models and patient-specific graphs.


<details>
  <summary>Details</summary>
Motivation: The task of data integration for multi-omics data has emerged as a powerful strategy to unravel the complex biological underpinnings of cancer. Recent advancements in graph neural networks (GNNs) offer an effective framework to model heterogeneous and structured omics data, enabling precise representation of molecular interactions and regulatory networks.

Method: This systematic review explores several recent studies that leverage GNN-based architectures in multi-omics cancer research. We classify the approaches based on their targeted omics layers, graph neural network structures, and biological tasks such as subtype classification, prognosis prediction, and biomarker discovery.

Result: The analysis reveals a growing trend toward hybrid and interpretable models, alongside increasing adoption of attention mechanisms and contrastive learning. Furthermore, we highlight the use of patient-specific graphs and knowledge-driven priors as emerging directions.

Conclusion: This survey serves as a comprehensive resource for researchers aiming to design effective GNN-based pipelines for integrative cancer analysis, offering insights into current practices, limitations, and potential future directions.

Abstract: The task of data integration for multi-omics data has emerged as a powerful
strategy to unravel the complex biological underpinnings of cancer. Recent
advancements in graph neural networks (GNNs) offer an effective framework to
model heterogeneous and structured omics data, enabling precise representation
of molecular interactions and regulatory networks. This systematic review
explores several recent studies that leverage GNN-based architectures in
multi-omics cancer research. We classify the approaches based on their targeted
omics layers, graph neural network structures, and biological tasks such as
subtype classification, prognosis prediction, and biomarker discovery. The
analysis reveals a growing trend toward hybrid and interpretable models,
alongside increasing adoption of attention mechanisms and contrastive learning.
Furthermore, we highlight the use of patient-specific graphs and
knowledge-driven priors as emerging directions. This survey serves as a
comprehensive resource for researchers aiming to design effective GNN-based
pipelines for integrative cancer analysis, offering insights into current
practices, limitations, and potential future directions.

</details>


### [160] [Training a Scientific Reasoning Model for Chemistry](https://arxiv.org/abs/2506.17238)
*Siddharth M. Narayanan,James D. Braza,Ryan-Rhys Griffiths,Albert Bou,Geemi Wellawatte,Mayk Caldas Ramos,Ludovico Mitchener,Samuel G. Rodriques,Andrew D. White*

Main category: cs.LG

TL;DR: 推理模型可以通过后训练用于化学，而无需额外的领域预训练，并且与当代领域特定模型相比，需要的数据量要少得多。


<details>
  <summary>Details</summary>
Motivation: 主要问题是语言模型推理是否能推广到数学、编程和逻辑之外，而之前的大部分工作都集中在这些领域。

Method: 使用强化学习在 640,730 个实验性化学问题上训练了一个 24B 参数的 LLM (基于 Mistral-Small-24B)。

Result: 该模型在分子设计任务上超过了通用化学模型、前沿模型和人类专家。相对于专用模型，它也更具有数据效率。

Conclusion: 该方法可以应用于训练数据高效的语言模型，这些模型专门用于各种科学领域的任务。

Abstract: Reasoning models are large language models that emit a long chain-of-thought
before answering, providing both higher accuracy and explicit reasoning for
their response. A major question has been whether language model reasoning
generalizes beyond mathematics, programming, and logic, where most previous
work has focused. We demonstrate that reasoning models can be post-trained for
chemistry without additional domain pretraining, and require substantially less
data compared to contemporary domain-specific models. We report ether0, a 24B
parameter LLM (based on Mistral-Small-24B) that can reason in natural language
and respond with chemical structures. This reasoning model was trained with
reinforcement learning on 640,730 experimentally-grounded chemistry problems
across 375 tasks ranging from synthesizability, to blood-brain barrier
permeability, to human receptor activity, to scent. Our model exceeds
general-purpose chemistry models, frontier models, and human experts on
molecular design tasks. It is also more data efficient relative to specialized
models. We anticipate that this method can be applied to train data-efficient
language models specialized for tasks across a wide variety of scientific
domains.

</details>


### [161] [Recursive Learning-Based Virtual Buffering for Analytical Global Placement](https://arxiv.org/abs/2506.17247)
*Andrew B. Kahng,Yiting Liu,Zhiang Wang*

Main category: cs.LG

TL;DR: MLBuf-RePlAce, a learning-driven virtual buffering-aware analytical global placement framework, improves timing closure and addresses ERC violations with significant TNS improvements.


<details>
  <summary>Details</summary>
Motivation: Placement with buffer porosity awareness is essential for timing closure, but existing approaches are computationally expensive or lack consideration of Electrical Rule Check (ERC) violations.

Method: An efficient recursive learning-based generative buffering approach to predict buffer types and locations, addressing ERC violations during global placement.

Result: MLBuf-RePlAce achieves improvements of up to 56% in TNS within the open-source OpenROAD flow and up to 53% in TNS in a commercial flow.

Conclusion: MLBuf-RePlAce achieves significant improvements in total negative slack (TNS) without degrading post-route power, and also improves TNS in a commercial flow with a slight improvement in post-route power.

Abstract: Due to the skewed scaling of interconnect versus cell delay in modern
technology nodes, placement with buffer porosity (i.e., cell density) awareness
is essential for timing closure in physical synthesis flows. However, existing
approaches face two key challenges: (i) traditional van Ginneken-Lillis-style
buffering approaches are computationally expensive during global placement; and
(ii) machine learning-based approaches, such as BufFormer, lack a thorough
consideration of Electrical Rule Check (ERC) violations and fail to "close the
loop" back into the physical design flow. In this work, we propose
MLBuf-RePlAce, the first open-source learning-driven virtual buffering-aware
analytical global placement framework, built on top of the OpenROAD
infrastructure. MLBuf-RePlAce adopts an efficient recursive learning-based
generative buffering approach to predict buffer types and locations, addressing
ERC violations during global placement. We compare MLBuf-RePlAce against the
default virtual buffering-based timing-driven global placer in OpenROAD, using
open-source testcases from the TILOS MacroPlacement and OpenROAD-flow-scripts
repositories. Without degradation of post-route power, MLBuf-RePlAce achieves
(maximum, average) improvements of (56%, 31%) in total negative slack (TNS)
within the open-source OpenROAD flow. When evaluated by completion in a
commercial flow, MLBuf-RePlAce achieves (maximum, average) improvements of
(53%, 28%) in TNS with an average of 0.2% improvement in post-route power.

</details>


### [162] [Efficient Quantification of Multimodal Interaction at Sample Level](https://arxiv.org/abs/2506.17248)
*Zequn Yang,Hongfa Wang,Di Hu*

Main category: cs.LG

TL;DR: introduce the Lightweight Sample-wise Multimodal Interaction (LSMI) estimator, rigorously grounded in pointwise information theory, validated LSMI's precision and efficiency on synthetic and real-world datasets, reveals fine-grained sample- and category-level dynamics within multimodal data, enabling practical applications such as redundancy-informed sample partitioning, targeted knowledge distillation, and interaction-aware model ensembling.


<details>
  <summary>Details</summary>
Motivation: Understanding interactions between modalities is crucial for analyzing information dynamics in multimodal systems, yet their accurate sample-level quantification presents significant theoretical and computational challenges.

Method: introduce the Lightweight Sample-wise Multimodal Interaction (LSMI) estimator, rigorously grounded in pointwise information theory. develop a redundancy estimation framework, employing an appropriate pointwise information measure to quantify this most decomposable and measurable interaction. propose a general interaction estimation method that employs efficient entropy estimation, specifically tailored for sample-wise estimation in continuous distributions.

Result: Extensive experiments on synthetic and real-world datasets validate LSMI's precision and efficiency.

Conclusion: The sample-wise approach reveals fine-grained sample- and category-level dynamics within multimodal data, enabling practical applications such as redundancy-informed sample partitioning, targeted knowledge distillation, and interaction-aware model ensembling.

Abstract: Interactions between modalities -- redundancy, uniqueness, and synergy --
collectively determine the composition of multimodal information. Understanding
these interactions is crucial for analyzing information dynamics in multimodal
systems, yet their accurate sample-level quantification presents significant
theoretical and computational challenges. To address this, we introduce the
Lightweight Sample-wise Multimodal Interaction (LSMI) estimator, rigorously
grounded in pointwise information theory. We first develop a redundancy
estimation framework, employing an appropriate pointwise information measure to
quantify this most decomposable and measurable interaction. Building upon this,
we propose a general interaction estimation method that employs efficient
entropy estimation, specifically tailored for sample-wise estimation in
continuous distributions. Extensive experiments on synthetic and real-world
datasets validate LSMI's precision and efficiency. Crucially, our sample-wise
approach reveals fine-grained sample- and category-level dynamics within
multimodal data, enabling practical applications such as redundancy-informed
sample partitioning, targeted knowledge distillation, and interaction-aware
model ensembling. The code is available at
https://github.com/GeWu-Lab/LSMI_Estimator.

</details>


### [163] [Improving Prediction Certainty Estimation for Reliable Early Exiting via Null Space Projection](https://arxiv.org/abs/2506.17249)
*Jianing He,Qi Zhang,Duoqian Miao,Yi Kun,Shufeng Hao,Hongyun Zhang,Zhihua Wei*

Main category: cs.LG

TL;DR: 该论文提出了一种新的提前退出方法，该方法通过考虑与类别无关的信息来更准确地估计预测置信度，从而提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有的提前退出方法主要依靠与类别相关的 logits 来制定其退出信号，以估计预测的确定性，而忽略了特征中与类别无关的信息对预测确定性的不利影响。这导致对预测确定性的高估，导致具有不正确的早期预测的样本过早退出。

Method: 该论文定义了一个 NSP 分数来估计预测置信度，该分数考虑了特征中与类别无关的信息的比例。在此基础上，提出了一种基于置信度感知概率 (CAP) 分数的新型提前退出方法。

Result: 在 GLUE 基准测试上的实验结果表明，该方法可以在所有任务中实现 2.19 倍的平均加速比，而性能下降可忽略不计，超过了最先进的 (SOTA) ConsistentEE 28%，从而在任务性能和推理效率之间实现了更好的权衡。

Conclusion: 该论文提出了一种基于置信度感知概率 (CAP) 分数的新型提前退出方法，该方法集成了 logits 和 NSP 分数的见解，以增强预测置信度估计，从而实现更可靠的退出决策。

Abstract: Early exiting has demonstrated great potential in accelerating the inference
of pre-trained language models (PLMs) by enabling easy samples to exit at
shallow layers, eliminating the need for executing deeper layers. However,
existing early exiting methods primarily rely on class-relevant logits to
formulate their exiting signals for estimating prediction certainty, neglecting
the detrimental influence of class-irrelevant information in the features on
prediction certainty. This leads to an overestimation of prediction certainty,
causing premature exiting of samples with incorrect early predictions. To
remedy this, we define an NSP score to estimate prediction certainty by
considering the proportion of class-irrelevant information in the features. On
this basis, we propose a novel early exiting method based on the
Certainty-Aware Probability (CAP) score, which integrates insights from both
logits and the NSP score to enhance prediction certainty estimation, thus
enabling more reliable exiting decisions. The experimental results on the GLUE
benchmark show that our method can achieve an average speed-up ratio of 2.19x
across all tasks with negligible performance degradation, surpassing the
state-of-the-art (SOTA) ConsistentEE by 28%, yielding a better trade-off
between task performance and inference efficiency. The code is available at
https://github.com/He-Jianing/NSP.git.

</details>


### [164] [SliceGX: Layer-wise GNN Explanation with Model-slicing](https://arxiv.org/abs/2506.17977)
*Tingting Zhu,Tingyang Chen,Yinghui Wu,Arijit Khan,Xiangyu Ke*

Main category: cs.LG

TL;DR: SliceGX: a novel GNN explanation approach that generates explanations at specific GNN layers in a progressive manner. It is effective and efficient, and it can support model debugging.


<details>
  <summary>Details</summary>
Motivation: Existing GNN explanations lack finer-grained, layer-wise analysis of how intermediate representations contribute to the final result, capabilities that are crucial for model diagnosis and architecture optimization.

Method: a novel GNN explanation approach that generates explanations at specific GNN layers in a progressive manner

Result: SliceGX automatically segments M into layer blocks and discovers high-quality explanatory subgraphs in each layer block that clarifies the occurrence of output of M at the targeted layer. Additionally, SliceGX offers a SPARQL-like query interface, providing declarative access and search capacities for the generated explanations.

Conclusion: SliceGX is effective and efficient, and it can support model debugging.

Abstract: Ensuring the trustworthiness of graph neural networks (GNNs) as black-box
models requires effective explanation methods. Existing GNN explanations
typically apply input perturbations to identify subgraphs that are responsible
for the occurrence of the final output of GNNs. However, such approaches lack
finer-grained, layer-wise analysis of how intermediate representations
contribute to the final result, capabilities that are crucial for model
diagnosis and architecture optimization. This paper introduces SliceGX, a novel
GNN explanation approach that generates explanations at specific GNN layers in
a progressive manner. Given a GNN M, a set of selected intermediate layers, and
a target layer, SliceGX automatically segments M into layer blocks ("model
slice") and discovers high-quality explanatory subgraphs in each layer block
that clarifies the occurrence of output of M at the targeted layer. Although
finding such layer-wise explanations is computationally challenging, we develop
efficient algorithms and optimization techniques that incrementally generate
and maintain these subgraphs with provable approximation guarantees.
Additionally, SliceGX offers a SPARQL-like query interface, providing
declarative access and search capacities for the generated explanations.
Through experiments on large real-world graphs and representative GNN
architectures, we verify the effectiveness and efficiency of SliceGX, and
illustrate its practical utility in supporting model debugging.

</details>


### [165] [Towards Interpretable Adversarial Examples via Sparse Adversarial Attack](https://arxiv.org/abs/2506.17250)
*Fudong Lin,Jiadong Lou,Hao Wang,Brian Jalaian,Xu Yuan*

Main category: cs.LG

TL;DR: 提出了一种快速、可迁移且强大的稀疏攻击方法，用于理解卷积神经网络的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏攻击解决方案由于其稀疏性差而无法产生可解释的对抗样本，并且存在计算开销大、可迁移性差和攻击强度弱等问题。

Method: 提出了一种新颖且理论上合理的参数化技术来近似NP-hard l0优化问题，并设计了一种新的损失函数，通过最大化对抗属性和最小化扰动像素的数量来增加初始扰动。

Result: 该方法在计算开销、可迁移性和攻击强度方面优于最先进的稀疏攻击。

Conclusion: 该研究提出了一种新的稀疏攻击方法，并在计算开销、可迁移性和攻击强度方面优于现有技术，为评估深度神经网络的鲁棒性提供了一个基准。通过理论和实验结果验证，该方法产生了更稀疏的对抗样本，并发现了两种噪声类型，有助于解释对抗扰动如何误导分类器。

Abstract: Sparse attacks are to optimize the magnitude of adversarial perturbations for
fooling deep neural networks (DNNs) involving only a few perturbed pixels
(i.e., under the l0 constraint), suitable for interpreting the vulnerability of
DNNs. However, existing solutions fail to yield interpretable adversarial
examples due to their poor sparsity. Worse still, they often struggle with
heavy computational overhead, poor transferability, and weak attack strength.
In this paper, we aim to develop a sparse attack for understanding the
vulnerability of CNNs by minimizing the magnitude of initial perturbations
under the l0 constraint, to overcome the existing drawbacks while achieving a
fast, transferable, and strong attack to DNNs. In particular, a novel and
theoretical sound parameterization technique is introduced to approximate the
NP-hard l0 optimization problem, making directly optimizing sparse
perturbations computationally feasible. Besides, a novel loss function is
designed to augment initial perturbations by maximizing the adversary property
and minimizing the number of perturbed pixels simultaneously. Extensive
experiments are conducted to demonstrate that our approach, with theoretical
performance guarantees, outperforms state-of-the-art sparse attacks in terms of
computational overhead, transferability, and attack strength, expecting to
serve as a benchmark for evaluating the robustness of DNNs. In addition,
theoretical and empirical results validate that our approach yields sparser
adversarial examples, empowering us to discover two categories of noises, i.e.,
"obscuring noise" and "leading noise", which will help interpret how
adversarial perturbation misleads the classifiers into incorrect predictions.
Our code is available at https://github.com/fudong03/SparseAttack.

</details>


### [166] [PuckTrick: A Library for Making Synthetic Data More Realistic](https://arxiv.org/abs/2506.18499)
*Alessandra Agostini,Andrea Maurino,Blerina Spahiu*

Main category: cs.LG

TL;DR: Pucktrick, a Python library, contaminates synthetic datasets with controlled errors to improve ML model resilience, showing that models trained on contaminated data outperform those trained on purely synthetic data.


<details>
  <summary>Details</summary>
Motivation: synthetic data is often overly clean and lacks real-world imperfections, such as missing values, noise, outliers, and misclassified labels, which can significantly impact model generalization and robustness

Method: a Python library designed to systematically contaminate synthetic datasets by introducing controlled errors

Result: ML models trained on contaminated synthetic data outperform those trained on purely synthetic, error-free data, particularly for tree-based and linear models such as SVMs and Extra Trees.

Conclusion: ML models trained on contaminated synthetic data outperform those trained on purely synthetic, error-free data, particularly for tree-based and linear models such as SVMs and Extra Trees.

Abstract: The increasing reliance on machine learning (ML) models for decision-making
requires high-quality training data. However, access to real-world datasets is
often restricted due to privacy concerns, proprietary restrictions, and
incomplete data availability. As a result, synthetic data generation (SDG) has
emerged as a viable alternative, enabling the creation of artificial datasets
that preserve the statistical properties of real data while ensuring privacy
compliance. Despite its advantages, synthetic data is often overly clean and
lacks real-world imperfections, such as missing values, noise, outliers, and
misclassified labels, which can significantly impact model generalization and
robustness. To address this limitation, we introduce Pucktrick, a Python
library designed to systematically contaminate synthetic datasets by
introducing controlled errors. The library supports multiple error types,
including missing data, noisy values, outliers, label misclassification,
duplication, and class imbalance, offering a structured approach to evaluating
ML model resilience under real-world data imperfections. Pucktrick provides two
contamination modes: one for injecting errors into clean datasets and another
for further corrupting already contaminated datasets. Through extensive
experiments on real-world financial datasets, we evaluate the impact of
systematic data contamination on model performance. Our findings demonstrate
that ML models trained on contaminated synthetic data outperform those trained
on purely synthetic, error-free data, particularly for tree-based and linear
models such as SVMs and Extra Trees.

</details>


### [167] [Training-free LLM Verification via Recycling Few-shot Examples](https://arxiv.org/abs/2506.17251)
*Dongseok Lee,Jimyung Hong,Dongyoung Kim,Jaehyung Kim*

Main category: cs.LG

TL;DR: Referi improves LLM accuracy by recycling few-shot examples to verify outputs, achieving a 4.8% gain without extra training.


<details>
  <summary>Details</summary>
Motivation: The inherent stochasticity of LLMs' reasoning process and varying conclusions present significant challenges. Existing approaches like majority voting or Best-of-N have limitations such as limited applicability or the cost of additional training.

Method: The framework Recycles Few-shot examples to verify LLM outputs (Referi) by combining two different scores, designed motivated from Bayes' rule, and subsequently selects the candidate that is both confidently determined and contextually coherent through a few additional LLM inferences.

Result: Experiments with three different LLMs and across seven diverse tasks demonstrate that the framework improves the accuracy of LLMs.

Conclusion: The proposed Referi framework significantly improves the accuracy of LLMs by 4.8% through effective response selection, without additional training.

Abstract: Although LLMs have achieved remarkable performance, the inherent
stochasticity of their reasoning process and varying conclusions present
significant challenges. Majority voting or Best-of-N with external verification
models has been explored to find the most promising solution among multiple LLM
outputs. However, these approaches have certain limitations, such as limited
applicability or the cost of an additional training step. To address this
problem, we propose a novel and effective framework that Recycles Few-shot
examples to verify LLM outputs (Referi). Our key idea is to additionally
utilize the given few-shot examples to evaluate the candidate outputs of the
target query, not only using them to generate outputs as the conventional
few-shot prompting setup. Specifically, Referi evaluates the generated outputs
by combining two different scores, designed motivated from Bayes' rule, and
subsequently selects the candidate that is both confidently determined and
contextually coherent through a few additional LLM inferences. Experiments with
three different LLMs and across seven diverse tasks demonstrate that our
framework significantly improves the accuracy of LLMs-achieving an average gain
of 4.8%-through effective response selection, without additional training.

</details>


### [168] [Adaptive Sample Scheduling for Direct Preference Optimization](https://arxiv.org/abs/2506.17252)
*Zixuan Huang,Yikun Ban,Lean Fu,Xiaojie Li,Zhongxiang Dai,Jianxin Li,Deqing Wang*

Main category: cs.LG

TL;DR: introduce Sample Scheduling for DPO, which aims to dynamically and adaptively schedule training samples based on the model's evolving states throughout preference optimization.


<details>
  <summary>Details</summary>
Motivation: performance is highly dependent on the quality of the underlying human preference data.  prior work has explored various data selection strategies, but these methods often overlook the impact of the evolving states of the language model during the DPO process.

Method: introduce SamS, an efficient and effective algorithm that adaptively selects samples in each training batch based on the LLM's learning feedback to maximize the potential generalization performance.

Result: SamS significantly improves performance across tasks, with minimal additional computational overhead.

Conclusion: Integrating SamS significantly improves performance across tasks, with minimal additional computational overhead, pointing to a promising new direction for improving LLM alignment through more effective utilization of fixed preference datasets.

Abstract: Direct Preference Optimization (DPO) has emerged as an effective approach for
aligning large language models (LLMs) with human preferences. However, its
performance is highly dependent on the quality of the underlying human
preference data. To address this bottleneck, prior work has explored various
data selection strategies, but these methods often overlook the impact of the
evolving states of the language model during the DPO process. %including active
querying, response pair selection, and data pre-selection. In this paper, we
introduce a novel problem: Sample Scheduling for DPO, which aims to dynamically
and adaptively schedule training samples based on the model's evolving states
throughout preference optimization. To solve this problem, we propose SamS, an
efficient and effective algorithm that adaptively selects samples in each
training batch based on the LLM's learning feedback to maximize the potential
generalization performance. Notably, without modifying the core DPO algorithm,
simply integrating SamS significantly improves performance across tasks, with
minimal additional computational overhead. This work points to a promising new
direction for improving LLM alignment through more effective utilization of
fixed preference datasets.

</details>


### [169] [MS-TVNet:A Long-Term Time Series Prediction Method Based on Multi-Scale Dynamic Convolution](https://arxiv.org/abs/2506.17253)
*Chenghan Li,Mingchen Li,Yipu Liao,Ruisheng Diao*

Main category: cs.LG

TL;DR: This paper introduces MS-TVNet, a multi-scale 3D dynamic convolutional neural network, for long-term time series prediction. It outperforms existing methods by effectively capturing complex temporal patterns using convolutional networks.


<details>
  <summary>Details</summary>
Motivation: Long-term time series prediction has predominantly relied on Transformer and MLP models, while the potential of convolutional networks in this domain remains underexplored. To address this gap

Method: introduce a novel multi-scale time series reshape module, propose MS-TVNet, a multi-scale 3D dynamic convolutional neural network

Result: MS-TVNet demonstrates superior performance compared to baseline models, achieving state-of-the-art (SOTA) results in long-term time series prediction

Conclusion: MS-TVNet demonstrates superior performance compared to baseline models, achieving state-of-the-art (SOTA) results in long-term time series prediction. Our findings highlight the effectiveness of leveraging convolutional networks for capturing complex temporal patterns, suggesting a promising direction for future research in this field.

Abstract: Long-term time series prediction has predominantly relied on Transformer and
MLP models, while the potential of convolutional networks in this domain
remains underexplored. To address this gap, we introduce a novel multi-scale
time series reshape module, which effectively captures the relationships among
multi-period patches and variable dependencies. Building upon this module, we
propose MS-TVNet, a multi-scale 3D dynamic convolutional neural network.
Through comprehensive evaluations on diverse datasets, MS-TVNet demonstrates
superior performance compared to baseline models, achieving state-of-the-art
(SOTA) results in long-term time series prediction. Our findings highlight the
effectiveness of leveraging convolutional networks for capturing complex
temporal patterns, suggesting a promising direction for future research in this
field.The code is realsed on https://github.com/Curyyfaust/TVNet.

</details>


### [170] [Keeping Up with the Models: Online Deployment and Routing of LLMs at Scale](https://arxiv.org/abs/2506.17254)
*Shaoang Li,Jian Li*

Main category: cs.LG

TL;DR: This paper introduces StageRoute, a near-optimal algorithm for managing LLM deployment and routing under capacity and cost constraints, achieving a regret of order $T^{2/3}.


<details>
  <summary>Details</summary>
Motivation: The rapid pace at which new large language models (LLMs) appear -- and older ones become obsolete -- forces LLM service providers to juggle a streaming inventory of models while respecting tight deployment capacity and per-query cost budgets. We cast the reality as an online decision problem that couples stage-wise deployment, made at fixed maintenance windows, with per-query routing among the models kept live.

Method: StageRoute, a hierarchical algorithm that (i) optimistically selects up to $M_max$ models for the next stage using reward upper-confidence and cost lower-confidence bounds, then (ii) solves a budget-constrained bandit sub-problem to route each incoming query.

Result: StageRoute achieves a regret of order $T^{2/3}$ and provide a matching lower bound, thereby establishing its near-optimality. Moreover, our experiments confirm the theory, demonstrating that StageRoute performs close to the optimum in practical settings.

Conclusion: StageRoute achieves a regret of order $T^{2/3}$ and provide a matching lower bound, thereby establishing its near-optimality. Moreover, experiments confirm the theory, demonstrating that StageRoute performs close to the optimum in practical settings.

Abstract: The rapid pace at which new large language models (LLMs) appear -- and older
ones become obsolete -- forces LLM service providers to juggle a streaming
inventory of models while respecting tight deployment capacity and per-query
cost budgets. We cast the reality as an online decision problem that couples
stage-wise deployment, made at fixed maintenance windows, with per-query
routing among the models kept live. We introduce StageRoute, a hierarchical
algorithm that (i) optimistically selects up to $M_max$ models for the next
stage using reward upper-confidence and cost lower-confidence bounds, then (ii)
solves a budget-constrained bandit sub-problem to route each incoming query. We
prove that StageRoute achieves a regret of order $T^{2/3}$ and provide a
matching lower bound, thereby establishing its near-optimality. Moreover, our
experiments confirm the theory, demonstrating that StageRoute performs close to
the optimum in practical settings.

</details>


### [171] [UltraSketchLLM: Saliency-Driven Sketching for Ultra-Low Bit LLM Compression](https://arxiv.org/abs/2506.17255)
*Sunan Zou,Ziyun Zhang,Xueting Sun,Guojie Luo*

Main category: cs.LG

TL;DR: UltraSketchLLM compresses LLMs to ultra-low bitrates (0.5 bits per weight) using a sketching technique, maintaining performance and enabling deployment on edge devices.


<details>
  <summary>Details</summary>
Motivation: The memory constraints of edge devices limit the deployment of large language models (LLMs), requiring weight compression beyond the 1-bit limit. Existing compression methods have limitations in memory overhead or accuracy degradation.

Method: The paper introduces UltraSketchLLM, an index-free, sketch-based framework using data sketching with underestimate AbsMaxMin sketch, importance-aware space allocation, and a straight-through estimator for compression-aware finetuning.

Result: Experiments on Llama-3.2-1B demonstrate up to 0.5-bit compression with competitive perplexity and tolerable latency overhead.

Conclusion: UltraSketchLLM provides a way to compress LLMs to 0.5 bits per weight, making them deployable in resource-constrained environments.

Abstract: The rapid growth of large language models (LLMs) has outpaced the memory
constraints of edge devices, necessitating extreme weight compression beyond
the 1-bit limit. While quantization reduces model size, it is fundamentally
limited to 1 bit per weight. Existing multiple-to-one compression methods
either rely on mapping tables (inducing memory overhead) or incur severe
accuracy degradation due to random weight grouping. We introduce
UltraSketchLLM, an index-free, sketch-based framework that achieves ultra-low
bit compression (down to 0.5 bits per weight) while preserving model
performance. UltraSketchLLM leverages data sketching, a sub-linear
representation technique from streaming applications, to map multiple weights
to single values with bounded error. Our approach integrates an underestimate
AbsMaxMin sketch to minimize relative errors for small weights,
importance-aware space allocation to prioritize salient weights, and a
straight-through estimator for compression-aware finetuning. Experiments on
Llama-3.2-1B demonstrate up to 0.5-bit compression with competitive perplexity,
alongside tolerable latency overhead. UltraSketchLLM offers a practical
solution for deploying LLMs in resource-constrained environments.

</details>


### [172] [AI to Identify Strain-sensitive Regions of the Optic Nerve Head Linked to Functional Loss in Glaucoma](https://arxiv.org/abs/2506.17262)
*Thanadet Chuangsuwanich,Monisha E. Nongpiur,Fabian A. Braeu,Tin A. Tun,Alexandre Thiery,Shamira Perera,Ching Lin Ho,Martin Buist,George Barbastathis,Tin Aung,Michaël J. A. Girard*

Main category: cs.LG

TL;DR: ONH strain enhances prediction of glaucomatous VF loss patterns, with the neuroretinal rim being the most critical region.


<details>
  <summary>Details</summary>
Motivation: To assess whether ONH biomechanics improves prediction of three progressive visual field loss patterns in glaucoma; to use explainable AI to identify strain-sensitive ONH regions contributing to these predictions.

Method: ONH of glaucoma subjects was imaged under two conditions: primary gaze and primary gaze with IOP elevated to ~35 mmHg. Automatic ONH tissue segmentation and digital volume correlation were used to compute IOP-induced neural tissue and lamina cribrosa (LC) strains. Biomechanical and structural features were input to a Geometric Deep Learning model. Three classification tasks were performed to detect: (1) superior nasal step, (2) superior partial arcuate, (3) full superior hemifield defect.

Result: Models achieved high AUCs of 0.77-0.88, showing that ONH strain improved VF loss prediction beyond morphology alone. The inferior and inferotemporal rim were identified as key strain-sensitive regions, contributing most to visual field loss prediction and showing progressive expansion with increasing disease severity.

Conclusion: ONH strain enhances prediction of glaucomatous VF loss patterns. Neuroretinal rim, rather than the LC, was the most critical region contributing to model predictions.

Abstract: Objective: (1) To assess whether ONH biomechanics improves prediction of
three progressive visual field loss patterns in glaucoma; (2) to use
explainable AI to identify strain-sensitive ONH regions contributing to these
predictions.
  Methods: We recruited 237 glaucoma subjects. The ONH of one eye was imaged
under two conditions: (1) primary gaze and (2) primary gaze with IOP elevated
to ~35 mmHg via ophthalmo-dynamometry. Glaucoma experts classified the subjects
into four categories based on the presence of specific visual field defects:
(1) superior nasal step (N=26), (2) superior partial arcuate (N=62), (3) full
superior hemifield defect (N=25), and (4) other/non-specific defects (N=124).
Automatic ONH tissue segmentation and digital volume correlation were used to
compute IOP-induced neural tissue and lamina cribrosa (LC) strains.
Biomechanical and structural features were input to a Geometric Deep Learning
model. Three classification tasks were performed to detect: (1) superior nasal
step, (2) superior partial arcuate, (3) full superior hemifield defect. For
each task, the data were split into 80% training and 20% testing sets. Area
under the curve (AUC) was used to assess performance. Explainable AI techniques
were employed to highlight the ONH regions most critical to each
classification.
  Results: Models achieved high AUCs of 0.77-0.88, showing that ONH strain
improved VF loss prediction beyond morphology alone. The inferior and
inferotemporal rim were identified as key strain-sensitive regions,
contributing most to visual field loss prediction and showing progressive
expansion with increasing disease severity.
  Conclusion and Relevance: ONH strain enhances prediction of glaucomatous VF
loss patterns. Neuroretinal rim, rather than the LC, was the most critical
region contributing to model predictions.

</details>


### [173] [Memory Allocation in Resource-Constrained Reinforcement Learning](https://arxiv.org/abs/2506.17263)
*Massimiliano Tamborski,David Abel*

Main category: cs.LG

TL;DR: Memory constraints influence an agent's performance when navigating unknown environments.


<details>
  <summary>Details</summary>
Motivation: Resource constraints can fundamentally change both learning and decision-making.

Method: MCTS- and DQN-based algorithms

Result: Examine how different allocations of memory impact performance in episodic and continual learning settings.

Conclusion: This paper studies the dilemma of memory allocation between world model estimation and planning in memory-constrained agents using MCTS- and DQN-based algorithms.

Abstract: Resource constraints can fundamentally change both learning and
decision-making. We explore how memory constraints influence an agent's
performance when navigating unknown environments using standard reinforcement
learning algorithms. Specifically, memory-constrained agents face a dilemma:
how much of their limited memory should be allocated to each of the agent's
internal processes, such as estimating a world model, as opposed to forming a
plan using that model? We study this dilemma in MCTS- and DQN-based algorithms
and examine how different allocations of memory impact performance in episodic
and continual learning settings.

</details>


### [174] [OAT-Rephrase: Optimization-Aware Training Data Rephrasing for Zeroth-Order LLM Fine-Tuning](https://arxiv.org/abs/2506.17264)
*Jikai Long,Zijian Hu,Xiaodong Yu,Jianwen Xie,Zhaozhuo Xu*

Main category: cs.LG

TL;DR: OAT-Rephrase improves zeroth-order LLM fine-tuning by rephrasing training data.


<details>
  <summary>Details</summary>
Motivation: Zeroth-order optimization for fine-tuning LLMs is memory-efficient but has slow convergence and unstable optimization due to noisy gradient estimates.

Method: Optimization-Aware Training data rephrasing strategy (OAT-Rephrase) using a dual-stage pipeline with a rewriter LLM and a semantic judge.

Result: OAT-Rephrase improves MeZO fine-tuning performance, often matching first-order methods, across five classification tasks and three LLM architectures.

Conclusion: Optimization-aware rephrasing enhances zeroth-order tuning.

Abstract: Fine-tuning large language models (LLMs) using zeroth-order optimization (ZO)
offers a memory-efficient alternative to gradient-based methods but suffers
from slower convergence and unstable optimization due to noisy gradient
estimates. This paper introduces OAT-Rephrase, an Optimization-Aware Training
data rephrasing strategy that leverages an LLM to rephrase training instances
based on its understanding of the ZO dynamics, specifically MeZO, derived
directly from its paper. The approach incorporates a dual-stage pipeline
featuring a rewriter LLM and a semantic judge, ensuring all rephrasings retain
task relevance and logical consistency. Evaluations across five classification
tasks and three LLM architectures demonstrate that OAT-Rephrase consistently
improves MeZO fine-tuning performance, often narrowing or eliminating the gap
with first-order methods. Our findings suggest that optimization-aware
rephrasing serves as a reusable and low-overhead enhancement for zeroth-order
tuning regimes.

</details>


### [175] [Does Multimodal Large Language Model Truly Unlearn? Stealthy MLLM Unlearning Attack](https://arxiv.org/abs/2506.17265)
*Xianren Zhang,Hui Liu,Delvin Ce Zhang,Xianfeng Tang,Qi He,Dongwon Lee,Suhang Wang*

Main category: cs.LG

TL;DR: MLLM可能会记住敏感信息，因此需要研究LLM删除攻击。我们提出了SUA框架，该框架通过学习通用的噪声模式来恢复已删除的知识，并且具有良好的隐蔽性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 在大型数据上训练的多模态大型语言模型（MLLM）可能会记住敏感的个人信息和照片，从而带来严重的隐私风险。为了缓解这种情况，提出了MLLM删除方法，该方法对MLLM进行微调以减少“忘记”敏感信息。但是，尚不清楚知识是被真正忘记了还是只是隐藏在模型中。因此，我们提出研究LLM删除攻击这一新颖问题，该攻击旨在恢复已删除的LLM的已删除知识。

Method: 我们提出了一个新颖的隐蔽性删除攻击（SUA）框架，该框架学习一种通用的噪声模式。当应用于输入图像时，这种噪声可以触发模型显示已删除的内容。为了提高隐蔽性，我们引入了一种嵌入对齐损失，该损失可以最大程度地减少扰动图像和去噪图像嵌入之间的差异，从而确保攻击在语义上不明显。

Result: 实验结果表明，SUA可以有效地从MLLM中恢复已删除的信息。

Conclusion: SUA可以有效地从MLLM中恢复已删除的信息。此外，学习到的噪声具有良好的泛化性：在样本子集上训练的单个扰动可以揭示未见图像中被遗忘的内容。这表明知识的重新出现不是偶然的失败，而是一种一致的行为。

Abstract: Multimodal Large Language Models (MLLMs) trained on massive data may memorize
sensitive personal information and photos, posing serious privacy risks. To
mitigate this, MLLM unlearning methods are proposed, which fine-tune MLLMs to
reduce the ``forget'' sensitive information. However, it remains unclear
whether the knowledge has been truly forgotten or just hidden in the model.
Therefore, we propose to study a novel problem of LLM unlearning attack, which
aims to recover the unlearned knowledge of an unlearned LLM. To achieve the
goal, we propose a novel framework Stealthy Unlearning Attack (SUA) framework
that learns a universal noise pattern. When applied to input images, this noise
can trigger the model to reveal unlearned content. While pixel-level
perturbations may be visually subtle, they can be detected in the semantic
embedding space, making such attacks vulnerable to potential defenses. To
improve stealthiness, we introduce an embedding alignment loss that minimizes
the difference between the perturbed and denoised image embeddings, ensuring
the attack is semantically unnoticeable. Experimental results show that SUA can
effectively recover unlearned information from MLLMs. Furthermore, the learned
noise generalizes well: a single perturbation trained on a subset of samples
can reveal forgotten content in unseen images. This indicates that knowledge
reappearance is not an occasional failure, but a consistent behavior.

</details>


### [176] [CF-VLM:CounterFactual Vision-Language Fine-tuning](https://arxiv.org/abs/2506.17267)
*Jusheng Zhang,Kaitong Cai,Yijia Fan,Jian Wang,Keze Wang*

Main category: cs.LG

TL;DR: This paper introduces CF-VLM, a new method to improve causal reasoning in vision-language models using counterfactual samples. It outperforms existing methods and shows promise in reducing visual hallucinations.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs often rely on superficial statistical correlations, lacking the ability to capture the underlying causal logic between visual and textual content. To address this, we propose

Method: We propose CounterFactual Vision-Language Fine-tuning (CF-VLM), a novel framework that enhances the causal reasoning capabilities of VLMs through the targeted use of counterfactual samples. CF-VLM introduces three complementary training objectives: maintaining foundational cross-modal alignment, reinforcing the uniqueness and stability of factual scene representations against coherent counterfactuals, and sharpening the model's sensitivity to minimal but critical causal edits.

Result: Extensive experiments demonstrate that CF-VLM consistently outperforms strong baselines and state-of-the-art methods on compositional reasoning and generalization benchmarks. Furthermore, it shows promise in mitigating visual hallucinations, indicating improved factual consistency.

Conclusion: CF-VLM provides a robust foundation for deploying VLMs in high-stakes, real-world scenarios requiring reliable reasoning and interpretability.

Abstract: Recent advances in vision-language models (VLMs) have greatly improved
cross-modal semantic understanding, yet significant limitations remain in
fine-grained discrimination and deep causal reasoning tasks. Existing VLMs
often rely on superficial statistical correlations, lacking the ability to
capture the underlying causal logic between visual and textual content. To
address this, we propose CounterFactual Vision-Language Fine-tuning (CF-VLM), a
novel framework that enhances the causal reasoning capabilities of VLMs through
the targeted use of counterfactual samples. CF-VLM introduces three
complementary training objectives: maintaining foundational cross-modal
alignment, reinforcing the uniqueness and stability of factual scene
representations against coherent counterfactuals, and sharpening the model's
sensitivity to minimal but critical causal edits. Extensive experiments
demonstrate that CF-VLM consistently outperforms strong baselines and
state-of-the-art methods on compositional reasoning and generalization
benchmarks. Furthermore, it shows promise in mitigating visual hallucinations,
indicating improved factual consistency. Our CF-VLM provides a robust
foundation for deploying VLMs in high-stakes, real-world scenarios requiring
reliable reasoning and interpretability.

</details>


### [177] [SafeRL-Lite: A Lightweight, Explainable, and Constrained Reinforcement Learning Library](https://arxiv.org/abs/2506.17297)
*Satyam Mishra,Phung Thao Vi,Shivam Mishra,Vishwanath Bijalwan,Vijay Bhaskar Semwal,Abdul Manan Khan*

Main category: cs.LG

TL;DR: SafeRL-Lite is a Python library for building reinforcement learning (RL) agents that are both constrained and explainable.


<details>
  <summary>Details</summary>
Motivation: Existing RL toolkits often lack native mechanisms for enforcing hard safety constraints or producing human-interpretable rationales for decisions.

Method: modular wrappers around standard Gym environments and deep Q-learning agents

Result: enable safety-aware training via constraint enforcement, and real-time post-hoc explanation via SHAP values and saliency maps

Conclusion: SafeRL-Lite is effective on constrained variants of CartPole and provides visualizations that reveal both policy logic and safety adherence.

Abstract: We introduce SafeRL-Lite, an open-source Python library for building
reinforcement learning (RL) agents that are both constrained and explainable.
Existing RL toolkits often lack native mechanisms for enforcing hard safety
constraints or producing human-interpretable rationales for decisions.
SafeRL-Lite provides modular wrappers around standard Gym environments and deep
Q-learning agents to enable: (i) safety-aware training via constraint
enforcement, and (ii) real-time post-hoc explanation via SHAP values and
saliency maps. The library is lightweight, extensible, and installable via pip,
and includes built-in metrics for constraint violations. We demonstrate its
effectiveness on constrained variants of CartPole and provide visualizations
that reveal both policy logic and safety adherence. The full codebase is
available at: https://github.com/satyamcser/saferl-lite.

</details>


### [178] [AlgoSelect: Universal Algorithm Selection via the Comb Operator](https://arxiv.org/abs/2506.17304)
*Jasper Yao*

Main category: cs.LG

TL;DR: AlgoSelect 是一个用于自动算法选择的通用框架，它通过学习在算法之间进行插值来实现近乎完美的选择，并且具有理论保证。


<details>
  <summary>Details</summary>
Motivation: 我们介绍了 AlgoSelect，这是一个有原则的框架，用于从数据中学习最佳算法选择，该框架以新型 Comb Operator 为中心。

Method: AlgoSelect 学习在不同的计算方法之间进行插值。对于成对的算法，一个简单的 sigmoid 门控选择器（Comb Operator 的一个实例）促进了这种插值。我们将其扩展到多算法的 N-Path Comb。

Result: 在全面的 20$\times$20 问题-算法研究中的经验验证表明，AlgoSelect 实现了接近完美的算法选择（99.9\%+ 的准确率），所需的样本非常少且收敛速度快，揭示了在结构化领域中 $H(\\text{Algorithm}|\\text{Problem}) \\approx 0$。

Conclusion: AlgoSelect 为自动算法选择提供了一个理论上可靠、实际可部署的解决方案，具有可证明的最优性和可学习性保证，对人工智能和自适应系统具有重要意义。

Abstract: We introduce AlgoSelect, a principled framework for learning optimal
algorithm selection from data, centered around the novel Comb Operator. Given a
set of algorithms and a feature representation of problems, AlgoSelect learns
to interpolate between diverse computational approaches. For pairs of
algorithms, a simple sigmoid-gated selector, an instance of the Comb Operator,
facilitates this interpolation. We extend this to an N-Path Comb for multiple
algorithms. We prove that this framework is universal (can approximate any
algorithm selector), information-theoretically optimal in its learnability
(thresholds for selection converge almost surely, demonstrated via
Borel-Cantelli arguments), computationally efficient, and robust. Key
theoretical contributions include: (1) a universal approximation theorem
demonstrating that Comb-based selectors can achieve arbitrary accuracy; (2)
information-theoretic learnability for selection thresholds; (3) formalization
of the Comb Operator within linear operator theory, detailing its boundedness
and spectral properties; (4) an N-Path Comb generalization for multi-algorithm
selection; and (5) a practical learning framework for the adaptive seeding
functions that guide the Comb Operator. Empirical validation on a comprehensive
20$\times$20 problem-algorithm study demonstrates near-perfect selection
(99.9\%+ accuracy) with remarkably few samples and rapid convergence, revealing
that $H(\text{Algorithm}|\text{Problem}) \approx 0$ in structured domains.
AlgoSelect provides a theoretically grounded, practically deployable solution
to automated algorithm selection with provable optimality and learnability
guarantees, with significant implications for AI and adaptive systems.

</details>


### [179] [Learning to Adapt Frozen CLIP for Few-Shot Test-Time Domain Adaptation](https://arxiv.org/abs/2506.17307)
*Zhixiang Chi,Li Gu,Huan Liu,Ziqiang Wang,Yanan Wu,Yang Wang,Konstantinos N Plataniotis*

Main category: cs.LG

TL;DR: 该论文提出了一种新的少样本测试时域适应方法，通过在输入空间学习和增强文本特征，提升了模型在特定领域的适应能力，并在多个基准测试中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法利用CLIP的强大分布外（OOD）能力，通过生成领域特定的提示来指导其广义的冻结特征。然而，由于下游数据集没有被CLIP明确看到，因此仅依赖于特征空间知识受到CLIP先验知识的限制。当使用不太稳健的骨干网络（如ViT-B/16）时，在具有挑战性的现实世界基准测试中，性能会显著下降。

Method: 该方法引入了直接在输入空间上学习的方法，以补充冻结的CLIP的数据集特定知识。具体来说，一个独立的侧分支与CLIP并行连接，并通过反转注意力强制学习独占知识。为了更好地捕获下游适应的数据集特定标签语义，该研究提出通过贪婪文本集成和细化来增强文本特征之间的互散性。然后，文本和视觉特征通过生成的领域提示以领域感知的方式逐步融合，以适应特定领域。

Result: 该方法在5个大规模基准测试（WILDS和DomainNet）上表现出优越性，尤其是在较小的网络（如ViT-B/16）上取得了显著改进，例如在iWildCam的F1指标上提高了+5.1，在FMoW的WC Acc指标上提高了+3.1%。

Conclusion: 该方法在5个大规模基准测试（WILDS和DomainNet）上表现出优越性，尤其是在较小的网络（如ViT-B/16）上取得了显著改进，例如在iWildCam的F1指标上提高了+5.1，在FMoW的WC Acc指标上提高了+3.1%。

Abstract: Few-shot Test-Time Domain Adaptation focuses on adapting a model at test time
to a specific domain using only a few unlabeled examples, addressing domain
shift. Prior methods leverage CLIP's strong out-of-distribution (OOD) abilities
by generating domain-specific prompts to guide its generalized, frozen
features. However, since downstream datasets are not explicitly seen by CLIP,
solely depending on the feature space knowledge is constrained by CLIP's prior
knowledge. Notably, when using a less robust backbone like ViT-B/16,
performance significantly drops on challenging real-world benchmarks. Departing
from the state-of-the-art of inheriting the intrinsic OOD capability of CLIP,
this work introduces learning directly on the input space to complement the
dataset-specific knowledge for frozen CLIP. Specifically, an independent side
branch is attached in parallel with CLIP and enforced to learn exclusive
knowledge via revert attention. To better capture the dataset-specific label
semantics for downstream adaptation, we propose to enhance the inter-dispersion
among text features via greedy text ensemble and refinement. The text and
visual features are then progressively fused in a domain-aware manner by a
generated domain prompt to adapt toward a specific domain. Extensive
experiments show our method's superiority on 5 large-scale benchmarks (WILDS
and DomainNet), notably improving over smaller networks like ViT-B/16 with
gains of \textbf{+5.1} in F1 for iWildCam and \textbf{+3.1\%} in WC Acc for
FMoW.

</details>


### [180] [I Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution](https://arxiv.org/abs/2506.17323)
*Tamas Bisztray,Bilel Cherif,Richard A. Dubniczky,Nils Gruschka,Bertalan Borsos,Mohamed Amine Ferrag,Attila Kovacs,Vasileios Mavroeidis,Norbert Tihanyi*

Main category: cs.LG

TL;DR: This paper presents CodeT5-Authorship and LLM-AuthorBench for LLM authorship attribution of C code, achieving high accuracy in identifying the source LLM.


<details>
  <summary>Details</summary>
Motivation: Identifying the specific LLM behind generated code is increasingly important as LLM-generated code becomes more prevalent.

Method: The paper introduces CodeT5-Authorship, a novel model based on the CodeT5 encoder architecture, and LLM-AuthorBench, a benchmark dataset of 32,000 C programs generated by eight LLMs. The model's performance is compared against several baselines.

Result: CodeT5-Authorship achieves 97.56% accuracy in binary classification of closely related models (GPT-4.1 and GPT-4o) and 95.40% accuracy in multi-class attribution among five leading LLMs.

Conclusion: CodeT5-Authorship achieves high accuracy in distinguishing and attributing C programs generated by various LLMs, outperforming traditional ML classifiers and fine-tuned transformer models.

Abstract: Detecting AI-generated code, deepfakes, and other synthetic content is an
emerging research challenge. As code generated by Large Language Models (LLMs)
becomes more common, identifying the specific model behind each sample is
increasingly important. This paper presents the first systematic study of LLM
authorship attribution for C programs. We released CodeT5-Authorship, a novel
model that uses only the encoder layers from the original CodeT5
encoder-decoder architecture, discarding the decoder to focus on
classification. Our model's encoder output (first token) is passed through a
two-layer classification head with GELU activation and dropout, producing a
probability distribution over possible authors. To evaluate our approach, we
introduce LLM-AuthorBench, a benchmark of 32,000 compilable C programs
generated by eight state-of-the-art LLMs across diverse tasks. We compare our
model to seven traditional ML classifiers and eight fine-tuned transformer
models, including BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3,
Longformer, and LoRA-fine-tuned Qwen2-1.5B. In binary classification, our model
achieves 97.56% accuracy in distinguishing C programs generated by closely
related models such as GPT-4.1 and GPT-4o, and 95.40% accuracy for multi-class
attribution among five leading LLMs (Gemini 2.5 Flash, Claude 3.5 Haiku,
GPT-4.1, Llama 3.3, and DeepSeek-V3). To support open science, we release the
CodeT5-Authorship architecture, the LLM-AuthorBench benchmark, and all relevant
Google Colab scripts on GitHub: https://github.com/LLMauthorbench/.

</details>


### [181] [Origins of Creativity in Attention-Based Diffusion Models](https://arxiv.org/abs/2506.17324)
*Emma Finn,T. Anderson Keller,Manos Theodosis,Demba E. Ba*

Main category: cs.LG

TL;DR: extend the theory to the case of diffusion models whose score is parametrized by a CNN with a final self-attention layer, and verify this behavior empirically on a carefully crafted dataset


<details>
  <summary>Details</summary>
Motivation: the question of how `creativity' originates in diffusion has become increasingly important. theory does not extend to describe the role of self-attention in this process

Method: extend the theory to the case of diffusion models whose score is parametrized by a CNN with a final self-attention layer

Result: verify this behavior empirically on a carefully crafted dataset

Conclusion: self-attention will induce a globally image-consistent arrangement of local features beyond the patch-level in generated samples

Abstract: As diffusion models have become the tool of choice for image generation and
as the quality of the images continues to improve, the question of how
`creativity' originates in diffusion has become increasingly important. The
score matching perspective on diffusion has proven particularly fruitful for
understanding how and why diffusion models generate images that remain
plausible while differing significantly from their training images. In
particular, as explained in (Kamb \& Ganguli, 2024) and others, e.g.,
(Ambrogioni, 2023), theory suggests that if our score matching were optimal, we
would only be able to recover training samples through our diffusion process.
However, as shown by Kamb \& Ganguli, (2024), in diffusion models where the
score is parametrized by a simple CNN, the inductive biases of the CNN itself
(translation equivariance and locality) allow the model to generate samples
that globally do not match any training samples, but are rather patch-wise
`mosaics'. Notably, however, this theory does not extend to describe the role
of self-attention in this process. In this work, we take a preliminary step in
this direction to extend this theory to the case of diffusion models whose
score is parametrized by a CNN with a final self-attention layer. We show that
our theory suggests that self-attention will induce a globally image-consistent
arrangement of local features beyond the patch-level in generated samples, and
we verify this behavior empirically on a carefully crafted dataset.

</details>


### [182] [CopulaSMOTE: A Copula-Based Oversampling Approach for Imbalanced Classification in Diabetes Prediction](https://arxiv.org/abs/2506.17326)
*Agnideep Aich,Md Monzur Murshed,Sameera Hewage,Amanda Mayeaux*

Main category: cs.LG

TL;DR: This paper introduces a new data augmentation method using A2 copulas to improve the performance of machine learning models in detecting diabetes, achieving better results than SMOTE with XGBoost.


<details>
  <summary>Details</summary>
Motivation: Diabetes mellitus poses a significant health risk, as nearly 1 in 9 people are affected by it. Early detection can significantly lower this risk. Despite significant advancements in machine learning for identifying diabetic cases, results can still be influenced by the imbalanced nature of the data.

Method: copula-based data augmentation, which preserves the dependency structure when generating data for the minority class and integrates it with machine learning (ML) techniques. We selected the Pima Indian dataset and generated data using A2 copula, then applied four machine learning algorithms: logistic regression, random forest, gradient boosting, and extreme gradient boosting.

Result: XGBoost combined with A2 copula oversampling achieved the best performance improving accuracy by 4.6%, precision by 15.6%, recall by 20.4%, F1-score by 18.2% and AUC by 25.5% compared to the standard SMOTE method.

Conclusion: XGBoost combined with A2 copula oversampling achieved the best performance improving accuracy by 4.6%, precision by 15.6%, recall by 20.4%, F1-score by 18.2% and AUC by 25.5% compared to the standard SMOTE method. This research represents the first known use of A2 copulas for data augmentation and serves as an alternative to the SMOTE technique, highlighting the efficacy of copulas as a statistical method in machine learning applications.

Abstract: Diabetes mellitus poses a significant health risk, as nearly 1 in 9 people
are affected by it. Early detection can significantly lower this risk. Despite
significant advancements in machine learning for identifying diabetic cases,
results can still be influenced by the imbalanced nature of the data. To
address this challenge, our study considered copula-based data augmentation,
which preserves the dependency structure when generating data for the minority
class and integrates it with machine learning (ML) techniques. We selected the
Pima Indian dataset and generated data using A2 copula, then applied four
machine learning algorithms: logistic regression, random forest, gradient
boosting, and extreme gradient boosting. Our findings indicate that XGBoost
combined with A2 copula oversampling achieved the best performance improving
accuracy by 4.6%, precision by 15.6%, recall by 20.4%, F1-score by 18.2% and
AUC by 25.5% compared to the standard SMOTE method. Furthermore, we
statistically validated our results using the McNemar test. This research
represents the first known use of A2 copulas for data augmentation and serves
as an alternative to the SMOTE technique, highlighting the efficacy of copulas
as a statistical method in machine learning applications.

</details>


### [183] [AutomataGPT: Forecasting and Ruleset Inference for Two-Dimensional Cellular Automata](https://arxiv.org/abs/2506.17333)
*Jaime A. Berkovich,Noah S. David,Markus J. Buehler*

Main category: cs.LG

TL;DR: AutomataGPT, a decoder-only transformer, is pretrained on CA rules and can predict unseen rules with high accuracy, opening avenues for real-world applications.


<details>
  <summary>Details</summary>
Motivation: automatically discovering the local update rules for a given phenomenon and using them for quantitative prediction remains challenging

Method: a decoder-only transformer pretrained on around 1 million simulated trajectories that span 100 distinct two-dimensional binary deterministic CA rules on toroidal grids

Result: attains 98.5% perfect one-step forecasts and reconstructs the governing update rule with up to 96% functional (application) accuracy and 82% exact rule-matrix match

Conclusion: transformer models can faithfully infer and execute CA dynamics from data alone, laying the groundwork for abstracting real-world dynamical phenomena into data-efficient CA surrogates, opening avenues in biology, tissue engineering, physics and AI-driven scientific discovery

Abstract: Cellular automata (CA) provide a minimal formalism for investigating how
simple local interactions generate rich spatiotemporal behavior in domains as
diverse as traffic flow, ecology, tissue morphogenesis and crystal growth.
However, automatically discovering the local update rules for a given
phenomenon and using them for quantitative prediction remains challenging. Here
we present AutomataGPT, a decoder-only transformer pretrained on around 1
million simulated trajectories that span 100 distinct two-dimensional binary
deterministic CA rules on toroidal grids. When evaluated on previously unseen
rules drawn from the same CA family, AutomataGPT attains 98.5% perfect one-step
forecasts and reconstructs the governing update rule with up to 96% functional
(application) accuracy and 82% exact rule-matrix match. These results
demonstrate that large-scale pretraining over wider regions of rule space
yields substantial generalization in both the forward (state forecasting) and
inverse (rule inference) problems, without hand-crafted priors. By showing that
transformer models can faithfully infer and execute CA dynamics from data
alone, our work lays the groundwork for abstracting real-world dynamical
phenomena into data-efficient CA surrogates, opening avenues in biology, tissue
engineering, physics and AI-driven scientific discovery.

</details>


### [184] [Adaptive Social Metaverse Streaming based on Federated Multi-Agent Deep Reinforcement Learning](https://arxiv.org/abs/2506.17342)
*Zijian Long,Haopeng Wang,Haiwei Dong,Abdulmotaleb El Saddik*

Main category: cs.LG

TL;DR: propose ASMS (Adaptive Social Metaverse Streaming) to dynamically adjust streaming bit rates while preserving user privacy and improves user experience by at least 14%


<details>
  <summary>Details</summary>
Motivation: privacy remains a major challenge, as immersive interactions require continuous collection of biometric and behavioral data. At the same time, ensuring high-quality, low-latency streaming is difficult due to the demands of real-time interaction, immersive rendering, and bandwidth optimization.

Method: a novel streaming system based on Federated Multi-Agent Proximal Policy Optimization (F-MAPPO). ASMS leverages F-MAPPO, which integrates federated learning (FL) and deep reinforcement learning (DRL) to dynamically adjust streaming bit rates while preserving user privacy.

Result: ASMS improves user experience by at least 14% compared to existing streaming methods across various network conditions.

Conclusion: ASMS enhances the social metaverse experience by providing seamless and immersive streaming, even in dynamic and resource-constrained networks, while ensuring that sensitive user data remains on local devices.

Abstract: The social metaverse is a growing digital ecosystem that blends virtual and
physical worlds. It allows users to interact socially, work, shop, and enjoy
entertainment. However, privacy remains a major challenge, as immersive
interactions require continuous collection of biometric and behavioral data. At
the same time, ensuring high-quality, low-latency streaming is difficult due to
the demands of real-time interaction, immersive rendering, and bandwidth
optimization. To address these issues, we propose ASMS (Adaptive Social
Metaverse Streaming), a novel streaming system based on Federated Multi-Agent
Proximal Policy Optimization (F-MAPPO). ASMS leverages F-MAPPO, which
integrates federated learning (FL) and deep reinforcement learning (DRL) to
dynamically adjust streaming bit rates while preserving user privacy.
Experimental results show that ASMS improves user experience by at least 14%
compared to existing streaming methods across various network conditions.
Therefore, ASMS enhances the social metaverse experience by providing seamless
and immersive streaming, even in dynamic and resource-constrained networks,
while ensuring that sensitive user data remains on local devices.

</details>


### [185] [FFINO: Factorized Fourier Improved Neural Operator for Modeling Multiphase Flow in Underground Hydrogen Storage](https://arxiv.org/abs/2506.17344)
*Tao Wang,Hewei Tang*

Main category: cs.LG

TL;DR: FFINO, a new neural operator architecture, is a fast and efficient surrogate model for underground hydrogen storage multiphase flow problems, outperforming FMIONet and numerical simulations.


<details>
  <summary>Details</summary>
Motivation: Fast modeling of hydrogen plume migration and pressure field evolution is crucial for UHS field management in the energy transition to a low-carbon economy.

Method: A new neural operator architecture, FFINO, is proposed as a fast surrogate model for multiphase flow problems in UHS. The model incorporates experimental relative permeability curves as key uncertainty parameters.

Result: The FFINO model shows improvements over the FMIONet model with 38.1% fewer trainable parameters, 17.6% less training time, 12% less GPU memory cost, 9.8% accuracy improvement in predicting hydrogen plume, and 18% higher RMSE in predicting pressure buildup.

Conclusion: The trained FFINO model demonstrates superior time efficiency, being 7850 times faster than numerical simulations, making it a competent substitute for UHS problem simulations.

Abstract: Underground hydrogen storage (UHS) is a promising energy storage option for
the current energy transition to a low-carbon economy. Fast modeling of
hydrogen plume migration and pressure field evolution is crucial for UHS field
management. In this study, we propose a new neural operator architecture,
FFINO, as a fast surrogate model for multiphase flow problems in UHS. We
parameterize experimental relative permeability curves reported in the
literature and include them as key uncertainty parameters in the FFINO model.
We also compare the FFINO model with the state-of-the-art FMIONet model through
a comprehensive combination of metrics. Our new FFINO model has 38.1% fewer
trainable parameters, 17.6% less training time, and 12% less GPU memory cost
compared to FMIONet. The FFINO model also achieves a 9.8% accuracy improvement
in predicting hydrogen plume in focused areas, and 18% higher RMSE in
predicting pressure buildup. The inference time of the trained FFINO model is
7850 times faster than a numerical simulator, which makes it a competent
substitute for numerical simulations of UHS problems with superior time
efficiency.

</details>


### [186] [SAFEx: Analyzing Vulnerabilities of MoE-Based LLMs via Stable Safety-critical Expert Identification](https://arxiv.org/abs/2506.17368)
*Zhenglin Lai,Mengyao Liao,Dong Xu,Zebin Zhao,Zhihang Yuan,Chao Fan,Jianqiang Li,Bingzhe Wu*

Main category: cs.LG

TL;DR: MoE models have safety vulnerabilities because safety-aligned behaviors rely on specific expert modules. Disabling these experts significantly compromised the models' ability to refuse harmful requests.


<details>
  <summary>Details</summary>
Motivation: Existing safety alignment strategies, predominantly designed for dense models, are ill-suited to address MoE-specific vulnerabilities. In this work, we formalize and systematically study MoE model's positional vulnerability - the phenomenon where safety-aligned behaviors rely on specific expert modules, revealing critical risks inherent to MoE architectures.

Method: present SAFEx, an analytical framework that robustly identifies, characterizes, and validates the safety-critical experts using a novel Stability-based Expert Selection (SES) algorithm. Notably, our approach enables the explicit decomposition of safety-critical experts into distinct functional groups, including those responsible for harmful content detection and those controlling safe response generation.

Result: demonstrated that their intrinsic safety mechanisms heavily rely on a small subset of positional experts

Conclusion: Disabling a small set of identified safety-critical experts can significantly compromise the models' ability to refuse harmful requests. For Qwen3-MoE with 6144 experts (in the FNN layer), disabling as few as 12 identified safety-critical experts can cause the refusal rate to drop by 22%, demonstrating the disproportionate impact of a small set of experts on overall model safety.

Abstract: Large language models based on Mixture-of-Experts have achieved substantial
gains in efficiency and scalability, yet their architectural uniqueness
introduces underexplored safety alignment challenges. Existing safety alignment
strategies, predominantly designed for dense models, are ill-suited to address
MoE-specific vulnerabilities. In this work, we formalize and systematically
study MoE model's positional vulnerability - the phenomenon where
safety-aligned behaviors rely on specific expert modules, revealing critical
risks inherent to MoE architectures. To this end, we present SAFEx, an
analytical framework that robustly identifies, characterizes, and validates the
safety-critical experts using a novel Stability-based Expert Selection (SES)
algorithm. Notably, our approach enables the explicit decomposition of
safety-critical experts into distinct functional groups, including those
responsible for harmful content detection and those controlling safe response
generation. Extensive experiments on mainstream MoE models, such as the
recently released Qwen3-MoE, demonstrated that their intrinsic safety
mechanisms heavily rely on a small subset of positional experts. Disabling
these experts significantly compromised the models' ability to refuse harmful
requests. For Qwen3-MoE with 6144 experts (in the FNN layer), we find that
disabling as few as 12 identified safety-critical experts can cause the refusal
rate to drop by 22%, demonstrating the disproportionate impact of a small set
of experts on overall model safety.

</details>


### [187] [Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in Inference-time Scaling?](https://arxiv.org/abs/2506.17417)
*Mingyuan Wu,Meitang Li,Jingcheng Yang,Jize Jiang,Kaizhuo Yan,Zhaoheng Li,Minjia Zhang,Klara Nahrstedt*

Main category: cs.LG

TL;DR: Inference-time computation techniques can enhance reasoning capabilities without relying on external knowledge, but RL-trained VLMs still lack robust self-verification capabilities.


<details>
  <summary>Details</summary>
Motivation: investigate whether inference-time techniques extend effectively to vision-language models (VLMs), particularly those trained with RL

Method: decoding strategies such as majority voting and best-of-N selection with self-verification

Result: decoding strategies improve VLM reasoning performance, generation-reliant methods achieve significantly higher gains versus verification-reliant methods, self-correction behavior does not lead to measurable gains

Conclusion: RL-trained VLMs still lack robust self-verification capabilities across both visual and textual modalities.

Abstract: Recent advances in large language models (LLMs) have demonstrated that
inference-time computation techniques, such as decoding-time scaling and
self-refinement, can significantly enhance reasoning capabilities without
relying on external knowledge. A key driver of this success is the emergence of
self-correction and self-verification behaviors, often elicited through
reinforcement learning (RL). In this paper, we investigate whether these
inference-time techniques extend effectively to vision-language models (VLMs),
particularly those trained with RL. We find that while decoding strategies such
as majority voting and best-of-N selection with self-verification all improve
VLM reasoning performance, generation-reliant methods such as the former
achieve significantly higher gains versus verification-reliant methods such as
the latter. Additionally, the self-correction behavior often associated with
RL-tuned models, such as aha moment, does not lead to measurable gains. We show
via extensive experimentation within the inference-time scaling framework to
identify a key root cause: RL-trained VLMs still lack robust self-verification
capabilities across both visual and textual modalities.

</details>


### [188] [FedNAMs: Performing Interpretability Analysis in Federated Learning Context](https://arxiv.org/abs/2506.17466)
*Amitash Nanda,Sree Bhargavi Balija,Debashis Sahoo*

Main category: cs.LG

TL;DR: FedNAMs结合了NAMs和联邦学习的优点，在保证隐私的前提下，提高了模型的可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习不断发展，但在可解释性方面面临挑战。

Method: 提出了一种新的联邦神经加性模型(FedNAMs)方法，该方法将NAMs的优势与联邦学习的去中心化方法相结合。

Result: 在各种文本和图像分类任务上的研究表明，与传统的联邦深度神经网络(dnn)相比，FedNAMs具有很强的可解释性，且精度损失最小。

Conclusion: FedNAMs在不同数据集上提高了可解释性和鲁棒性，同时保持了隐私和模型效率，并能识别关键预测特征。

Abstract: Federated learning continues to evolve but faces challenges in
interpretability and explainability. To address these challenges, we introduce
a novel approach that employs Neural Additive Models (NAMs) within a federated
learning framework. This new Federated Neural Additive Models (FedNAMs)
approach merges the advantages of NAMs, where individual networks concentrate
on specific input features, with the decentralized approach of federated
learning, ultimately producing interpretable analysis results. This integration
enhances privacy by training on local data across multiple devices, thereby
minimizing the risks associated with data centralization and improving model
robustness and generalizability. FedNAMs maintain detailed, feature-specific
learning, making them especially valuable in sectors such as finance and
healthcare. They facilitate the training of client-specific models to integrate
local updates, preserve privacy, and mitigate concerns related to
centralization. Our studies on various text and image classification tasks,
using datasets such as OpenFetch ML Wine, UCI Heart Disease, and Iris, show
that FedNAMs deliver strong interpretability with minimal accuracy loss
compared to traditional Federated Deep Neural Networks (DNNs). The research
involves notable findings, including the identification of critical predictive
features at both client and global levels. Volatile acidity, sulfates, and
chlorides for wine quality. Chest pain type, maximum heart rate, and number of
vessels for heart disease. Petal length and width for iris classification. This
approach strengthens privacy and model efficiency and improves interpretability
and robustness across diverse datasets. Finally, FedNAMs generate insights on
causes of highly and low interpretable features.

</details>


### [189] [A geometric framework for momentum-based optimizers for low-rank training](https://arxiv.org/abs/2506.17475)
*Steffen Schotthöfer,Timon Klein,Jonas Kusch*

Main category: cs.LG

TL;DR: This paper introduces new training strategies based on dynamical low-rank approximation to improve the convergence and performance of low-rank pre-training and fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Classical momentum methods can struggle to converge to a local optimum due to the geometry of the underlying optimization landscape.

Method: The paper introduces novel training strategies derived from dynamical low-rank approximation, which explicitly account for the underlying geometric structure. Our approach leverages and combines tools from dynamical low-rank approximation and momentum-based optimization to design optimizers that respect the intrinsic geometry of the parameter space.

Result: The methods are validated through numerical experiments, demonstrating faster convergence, and stronger validation metrics at given parameter budgets.

Conclusion: The paper introduces novel training strategies derived from dynamical low-rank approximation, which explicitly account for the underlying geometric structure. The methods are validated through numerical experiments, demonstrating faster convergence, and stronger validation metrics at given parameter budgets.

Abstract: Low-rank pre-training and fine-tuning have recently emerged as promising
techniques for reducing the computational and storage costs of large neural
networks. Training low-rank parameterizations typically relies on conventional
optimizers such as heavy ball momentum methods or Adam. In this work, we
identify and analyze potential difficulties that these training methods
encounter when used to train low-rank parameterizations of weights. In
particular, we show that classical momentum methods can struggle to converge to
a local optimum due to the geometry of the underlying optimization landscape.
To address this, we introduce novel training strategies derived from dynamical
low-rank approximation, which explicitly account for the underlying geometric
structure. Our approach leverages and combines tools from dynamical low-rank
approximation and momentum-based optimization to design optimizers that respect
the intrinsic geometry of the parameter space. We validate our methods through
numerical experiments, demonstrating faster convergence, and stronger
validation metrics at given parameter budgets.

</details>


### [190] [Episode-specific Fine-tuning for Metric-based Few-shot Learners with Optimization-based Training](https://arxiv.org/abs/2506.17499)
*Xuanyu Zhuang,Geoffroy Peeters,Gaël Richard*

Main category: cs.LG

TL;DR: Proposes episode-specific fine-tuning methods for metric-based models in few-shot classification to better utilize support samples and avoid overfitting, validated on audio datasets.


<details>
  <summary>Details</summary>
Motivation: Labeled support samples are often underutilized--they are only used for similarity comparison, despite their potential to fine-tune and adapt the metric space itself to the classes in the current episode. The severely limited amount of data in each task poses a substantial risk of overfitting when applying such fine-tuning strategies.

Method: episode-specific, during-inference fine-tuning methods for metric-based models, including Rotational Division Fine-Tuning (RDFT) and its two variants, Iterative Division Fine-Tuning (IDFT) and Augmented Division Fine-Tuning (ADFT).

Result: Our approach consistently improves performance for all evaluated metric-based models (especially for attention-based models) and generalizes well across different audio domains.

Conclusion: The proposed approach consistently improves performance for all evaluated metric-based models (especially for attention-based models) and generalizes well across different audio domains.

Abstract: In few-shot classification tasks (so-called episodes), a small set of labeled
support samples is provided during inference to aid the classification of
unlabeled query samples. Metric-based models typically operate by computing
similarities between query and support embeddings within a learned metric
space, followed by nearest-neighbor classification. However, these labeled
support samples are often underutilized--they are only used for similarity
comparison, despite their potential to fine-tune and adapt the metric space
itself to the classes in the current episode. To address this, we propose a
series of simple yet effective episode-specific, during-inference fine-tuning
methods for metric-based models, including Rotational Division Fine-Tuning
(RDFT) and its two variants, Iterative Division Fine-Tuning (IDFT) and
Augmented Division Fine-Tuning (ADFT). These methods construct pseudo
support-query pairs from the given support set to enable fine-tuning even for
non-parametric models. Nevertheless, the severely limited amount of data in
each task poses a substantial risk of overfitting when applying such
fine-tuning strategies. To mitigate this, we further propose to train the
metric-based model within an optimization-based meta-learning framework. With
the combined efforts of episode-specific fine-tuning and optimization-based
meta-training, metric-based models are equipped with the ability to rapidly
adapt to the limited support samples during inference while avoiding
overfitting. We validate our approach on three audio datasets from diverse
domains, namely ESC-50 (environmental sounds), Speech Commands V2 (spoken
keywords), and Medley-solos-DB (musical instrument). Experimental results
demonstrate that our approach consistently improves performance for all
evaluated metric-based models (especially for attention-based models) and
generalizes well across different audio domains.

</details>


### [191] [A Survey of State Representation Learning for Deep Reinforcement Learning](https://arxiv.org/abs/2506.17518)
*Ayoub Echchahed,Pablo Samuel Castro*

Main category: cs.LG

TL;DR: Survey on representation learning methods in reinforcement learning, categorizing methods and discussing their benefits, limitations, and future directions.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges posed by complex observation spaces in sequential decision making problems using representation learning.

Method: Categorization of representation learning methods within a model-free online setting.

Result: Categorization of methods into six main classes, detailing their mechanisms, benefits, and limitations. Discussion of techniques for assessing the quality of representations, and detailing relevant future directions.

Conclusion: This survey enhances the understanding of representation learning in reinforcement learning and provides a guide for new researchers.

Abstract: Representation learning methods are an important tool for addressing the
challenges posed by complex observations spaces in sequential decision making
problems. Recently, many methods have used a wide variety of types of
approaches for learning meaningful state representations in reinforcement
learning, allowing better sample efficiency, generalization, and performance.
This survey aims to provide a broad categorization of these methods within a
model-free online setting, exploring how they tackle the learning of state
representations differently. We categorize the methods into six main classes,
detailing their mechanisms, benefits, and limitations. Through this taxonomy,
our aim is to enhance the understanding of this field and provide a guide for
new researchers. We also discuss techniques for assessing the quality of
representations, and detail relevant future directions.

</details>


### [192] [Predicting E-commerce Purchase Behavior using a DQN-Inspired Deep Learning Model for enhanced adaptability](https://arxiv.org/abs/2506.17543)
*Aditi Madhusudan Jain*

Main category: cs.LG

TL;DR: This paper presents a DQN-inspired model for predicting buying intent in e-commerce, achieving high accuracy and outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of user behavior is crucial for optimizing inventory management, personalizing user experiences, and maximizing sales in online retail.

Method: The paper adapts a Deep Q-Network (DQN) inspired architecture, combining LSTM networks with DQN strategic decision-making, in a supervised learning context.

Result: The model achieves 88% accuracy and an AUC-ROC score of 0.88 on a large-scale e-commerce dataset, demonstrating robust performance and advantages over traditional methods.

Conclusion: The paper introduces a DQN-inspired model that outperforms traditional methods in predicting buying intent, offering improvements for demand forecasting and personalized marketing in e-commerce.

Abstract: This paper presents a novel approach to predicting buying intent and product
demand in e-commerce settings, leveraging a Deep Q-Network (DQN) inspired
architecture. In the rapidly evolving landscape of online retail, accurate
prediction of user behavior is crucial for optimizing inventory management,
personalizing user experiences, and maximizing sales. Our method adapts
concepts from reinforcement learning to a supervised learning context,
combining the sequential modeling capabilities of Long Short-Term Memory (LSTM)
networks with the strategic decision-making aspects of DQNs. We evaluate our
model on a large-scale e-commerce dataset comprising over 885,000 user
sessions, each characterized by 1,114 features. Our approach demonstrates
robust performance in handling the inherent class imbalance typical in
e-commerce data, where purchase events are significantly less frequent than
non-purchase events. Through comprehensive experimentation with various
classification thresholds, we show that our model achieves a balance between
precision and recall, with an overall accuracy of 88\% and an AUC-ROC score of
0.88. Comparative analysis reveals that our DQN-inspired model offers
advantages over traditional machine learning and standard deep learning
approaches, particularly in its ability to capture complex temporal patterns in
user behavior. The model's performance and scalability make it well-suited for
real-world e-commerce applications dealing with high-dimensional, sequential
data. This research contributes to the field of e-commerce analytics by
introducing a novel predictive modeling technique that combines the strengths
of deep learning and reinforcement learning paradigms. Our findings have
significant implications for improving demand forecasting, personalizing user
experiences, and optimizing marketing strategies in online retail environments.

</details>


### [193] [DRIMV_TSK: An Interpretable Surgical Evaluation Model for Incomplete Multi-View Rectal Cancer Data](https://arxiv.org/abs/2506.17552)
*Wei Zhang,Zi Wang,Hanwen Zhou,Zhaohong Deng,Weiping Ding,Yuxi Ge,Te Zhang,Yuanpeng Zhang,Kup-Sze Choi,Shitong Wang,Shudong Hu*

Main category: cs.LG

TL;DR: 本文构建了一个多视图直肠癌数据集，并提出了一种新的手术评估模型 DRIMV_TSK，该模型在不完全数据的情况下表现良好。


<details>
  <summary>Details</summary>
Motivation: 可靠的手术难度评估可以提高直肠癌治疗的成功率，但当前的评估方法依赖于临床数据，而随着技术和人工智能的发展，可以收集到更多关于直肠癌的数据。

Method: 提出了一种可解释的不完全多视图手术评估模型，该模型集成了缺失视图插补和二阶相似性约束，并利用 TSK 模糊系统和协同学习机制。

Result: 构建了一个多视图直肠癌数据集，并提出了 DRIMV_TSK 算法，该算法优于其他先进算法。

Conclusion: DRIMV_TSK 算法在 MVRC 数据集上表现最好。

Abstract: A reliable evaluation of surgical difficulty can improve the success of the
treatment for rectal cancer and the current evaluation method is based on
clinical data. However, more data about rectal cancer can be collected with the
development of technology. Meanwhile, with the development of artificial
intelligence, its application in rectal cancer treatment is becoming possible.
In this paper, a multi-view rectal cancer dataset is first constructed to give
a more comprehensive view of patients, including the high-resolution MRI image
view, pressed-fat MRI image view, and clinical data view. Then, an
interpretable incomplete multi-view surgical evaluation model is proposed,
considering that it is hard to obtain extensive and complete patient data in
real application scenarios. Specifically, a dual representation incomplete
multi-view learning model is first proposed to extract the common information
between views and specific information in each view. In this model, the missing
view imputation is integrated into representation learning, and second-order
similarity constraint is also introduced to improve the cooperative learning
between these two parts. Then, based on the imputed multi-view data and the
learned dual representation, a multi-view surgical evaluation model with the
TSK fuzzy system is proposed. In the proposed model, a cooperative learning
mechanism is constructed to explore the consistent information between views,
and Shannon entropy is also introduced to adapt the view weight. On the MVRC
dataset, we compared it with several advanced algorithms and DRIMV_TSK obtained
the best results.

</details>


### [194] [Accelerating Residual Reinforcement Learning with Uncertainty Estimation](https://arxiv.org/abs/2506.17564)
*Lakshita Dodeja,Karl Schmeckpeper,Shivam Vats,Thomas Weng,Mingxi Jia,George Konidaris,Stefanie Tellex*

Main category: cs.LG

TL;DR: 提出了一种改进的残差强化学习方法，该方法利用不确定性估计和离策略学习来提高样本效率，并使其适用于随机基础策略。


<details>
  <summary>Details</summary>
Motivation: 现有的残差强化学习方法在稀疏奖励方面存在困难，并且是为确定性基础策略设计的。

Method: 利用基础策略的不确定性估计来集中探索基础策略不自信的区域。提出了一种简单的离策略残差学习修改方法，使其能够观察基础动作并更好地处理随机基础策略。

Result: 该算法在各种模拟基准环境中显著优于现有的基线。

Conclusion: 该算法在各种模拟基准环境中显著优于现有的基线，并且在现实世界中部署了学习到的策略，以证明它们具有零样本 sim-to-real 迁移的鲁棒性。

Abstract: Residual Reinforcement Learning (RL) is a popular approach for adapting
pretrained policies by learning a lightweight residual policy that provides
corrective actions. While Residual RL is more sample-efficient than finetuning
the entire base policy, existing methods struggle with sparse rewards and are
designed for deterministic base policies. We propose two improvements to
Residual RL that further enhance its sample efficiency and make it suitable for
stochastic base policies. First, we leverage uncertainty estimates of the base
policy to focus exploration on regions in which the base policy is not
confident. Second, we propose a simple modification to off-policy residual
learning that allows it to observe base actions and better handle stochastic
base policies. We evaluate our method with both Gaussian-based and
Diffusion-based stochastic base policies on tasks from Robosuite and D4RL, and
compare against state-of-the-art finetuning methods, demo-augmented RL methods,
and other residual RL methods. Our algorithm significantly outperforms existing
baselines in a variety of simulation benchmark environments. We also deploy our
learned polices in the real world to demonstrate their robustness with
zero-shot sim-to-real transfer.

</details>


### [195] [Towards Deeper GCNs: Alleviating Over-smoothing via Iterative Training and Fine-tuning](https://arxiv.org/abs/2506.17576)
*Furong Peng,Jinzhen Gao,Xuan Lu,Kang Liu,Yifan Huo,Sheng Wang*

Main category: cs.LG

TL;DR: 本文提出了一种新的训练策略LGT，以解决GCN中的过度平滑问题，并在深度网络中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 图卷积网络（GCN）由于过度平滑，在深度架构中面临严重的性能下降。虽然现有的研究主要将过度平滑归因于图拉普拉斯算子的重复应用，但我们的经验分析揭示了一个关键但被忽视的因素：GCN中可训练的线性变换会显着加剧特征崩溃，即使在中等深度（例如，8层）也是如此。相比之下，简化图卷积（SGC）消除了这些变换，保持了高达32层的稳定特征多样性，突出了线性变换在促进表达能力和诱导过度平滑方面的双重作用。然而，完全消除线性变换会削弱模型的表达能力。

Method: 我们提出了分层渐进训练（LGT），这是一种新颖的训练策略，可以在保持其表达能力的同时逐步构建深度GCN。LGT集成了三个互补的组件：（1）分层训练，以稳定从浅层到深层的优化，（2）低秩适应，以微调浅层并加速训练，以及（3）身份初始化，以确保新层的平滑集成并加速收敛。

Result: LGT在vanilla GCN上实现了最先进的性能，即使在32层设置下也能显着提高准确性。此外，作为一种训练方法，LGT可以与现有的方法（如PairNorm和ContraNorm）无缝结合，进一步提高它们在更深层网络中的性能。

Conclusion: LGT在vanilla GCN上实现了最先进的性能，即使在32层设置下也能显着提高准确性。此外，作为一种训练方法，LGT可以与现有的方法（如PairNorm和ContraNorm）无缝结合，进一步提高它们在更深层网络中的性能。LGT为可扩展的深度GCN提供了一个通用的、与架构无关的训练框架。

Abstract: Graph Convolutional Networks (GCNs) suffer from severe performance
degradation in deep architectures due to over-smoothing. While existing studies
primarily attribute the over-smoothing to repeated applications of graph
Laplacian operators, our empirical analysis reveals a critical yet overlooked
factor: trainable linear transformations in GCNs significantly exacerbate
feature collapse, even at moderate depths (e.g., 8 layers). In contrast,
Simplified Graph Convolution (SGC), which removes these transformations,
maintains stable feature diversity up to 32 layers, highlighting linear
transformations' dual role in facilitating expressive power and inducing
over-smoothing. However, completely removing linear transformations weakens the
model's expressive capacity.
  To address this trade-off, we propose Layer-wise Gradual Training (LGT), a
novel training strategy that progressively builds deep GCNs while preserving
their expressiveness. LGT integrates three complementary components: (1)
layer-wise training to stabilize optimization from shallow to deep layers, (2)
low-rank adaptation to fine-tune shallow layers and accelerate training, and
(3) identity initialization to ensure smooth integration of new layers and
accelerate convergence. Extensive experiments on benchmark datasets demonstrate
that LGT achieves state-of-the-art performance on vanilla GCN, significantly
improving accuracy even in 32-layer settings. Moreover, as a training method,
LGT can be seamlessly combined with existing methods such as PairNorm and
ContraNorm, further enhancing their performance in deeper networks. LGT offers
a general, architecture-agnostic training framework for scalable deep GCNs. The
code is available at [https://github.com/jfklasdfj/LGT_GCN].

</details>


### [196] [LFR-PINO: A Layered Fourier Reduced Physics-Informed Neural Operator for Parametric PDEs](https://arxiv.org/abs/2506.17582)
*Jing Wang,Biao Chen,Hairun Xie,Rui Wang,Yifan Xia,Jifa Zhang,Hui Xu*

Main category: cs.LG

TL;DR: LFR-PINO 是一种新的物理信息神经算子，它通过分层超网络架构和频域约简策略，实现了高效学习通用 PDE 求解器。


<details>
  <summary>Details</summary>
Motivation: 现有的基于物理信息的神经算子方法要么由于固定基/系数设计而受到有限的表达能力的影响，要么由于参数到权重映射空间的高维度而面临计算挑战。

Method:  layered hypernetwork architecture and a frequency-domain reduction strategy

Result: LFR-PINO实现了22.8%-68.7%的误差降低，频率域约简策略降低了28.6%-69.3%的内存使用量。

Conclusion: LFR-PINO在四个代表性的PDE问题上实现了22.8%-68.7%的误差降低，并且频率域约简策略在保持解的精度的同时，比Hyper-PINNs降低了28.6%-69.3%的内存使用量。

Abstract: Physics-informed neural operators have emerged as a powerful paradigm for
solving parametric partial differential equations (PDEs), particularly in the
aerospace field, enabling the learning of solution operators that generalize
across parameter spaces. However, existing methods either suffer from limited
expressiveness due to fixed basis/coefficient designs, or face computational
challenges due to the high dimensionality of the parameter-to-weight mapping
space. We present LFR-PINO, a novel physics-informed neural operator that
introduces two key innovations: (1) a layered hypernetwork architecture that
enables specialized parameter generation for each network layer, and (2) a
frequency-domain reduction strategy that significantly reduces parameter count
while preserving essential spectral features. This design enables efficient
learning of a universal PDE solver through pre-training, capable of directly
handling new equations while allowing optional fine-tuning for enhanced
precision. The effectiveness of this approach is demonstrated through
comprehensive experiments on four representative PDE problems, where LFR-PINO
achieves 22.8%-68.7% error reduction compared to state-of-the-art baselines.
Notably, frequency-domain reduction strategy reduces memory usage by
28.6%-69.3% compared to Hyper-PINNs while maintaining solution accuracy,
striking an optimal balance between computational efficiency and solution
fidelity.

</details>


### [197] [Towards Fundamental Limits for Active Multi-distribution Learning](https://arxiv.org/abs/2506.17607)
*Chicheng Zhang,Yihan Zhou*

Main category: cs.LG

TL;DR: This paper focuses on active multi-distribution learning, presenting new algorithms with improved label complexity bounds. It also provides instance-dependent sample complexity bounds for passive learning, bridging realizable and agnostic regimes.


<details>
  <summary>Details</summary>
Motivation: Multi-distribution learning extends agnostic Probably Approximately Correct (PAC) learning to the setting in which a family of $k$ distributions is considered and a classifier's performance is measured by its error under the worst distribution. This problem has attracted a lot of recent interests due to its applications in collaborative learning, fairness, and robustness. Despite a rather complete picture of sample complexity of passive multi-distribution learning, research on active multi-distribution learning remains scarce, with algorithms whose optimality remaining unknown.

Method: The paper develops new algorithms for active multi-distribution learning.

Result: Specifically, in the near-realizable setting the paper proves an upper bound of $\widetilde{O}\Bigl(\theta_{\max}(d+k)\ln\frac{1}{\varepsilon}\Bigr)$ and $\widetilde{O}\Bigl(\theta_{\max}(d+k)\Bigl(\ln\frac{1}{\varepsilon}+\frac{\nu^2}{\varepsilon^2}\Bigr)+\frac{k\nu}{\varepsilon^2}\Bigr)$ in the realizable and agnostic settings respectively, where $\theta_{\max}$ is the maximum disagreement coefficient among the $k$ distributions, $d$ is the VC dimension of the hypothesis class, $\nu$ is the multi-distribution error of the best hypothesis, and $\varepsilon$ is the target excess error. Moreover, the paper shows that the bound in the realizable setting is information-theoretically optimal and that the $k\nu/\varepsilon^2$ term in the agnostic setting is fundamental for proper learners.

Conclusion: This paper develops new algorithms for active multi-distribution learning and establishes improved label complexity upper and lower bounds, in distribution-dependent and distribution-free settings.  It also establishes instance-dependent sample complexity bound for passive multidistribution learning that smoothly interpolates between realizable and agnostic regimes.

Abstract: Multi-distribution learning extends agnostic Probably Approximately Correct
(PAC) learning to the setting in which a family of $k$ distributions,
$\{D_i\}_{i\in[k]}$, is considered and a classifier's performance is measured
by its error under the worst distribution. This problem has attracted a lot of
recent interests due to its applications in collaborative learning, fairness,
and robustness. Despite a rather complete picture of sample complexity of
passive multi-distribution learning, research on active multi-distribution
learning remains scarce, with algorithms whose optimality remaining unknown.
  In this paper, we develop new algorithms for active multi-distribution
learning and establish improved label complexity upper and lower bounds, in
distribution-dependent and distribution-free settings. Specifically, in the
near-realizable setting we prove an upper bound of
$\widetilde{O}\Bigl(\theta_{\max}(d+k)\ln\frac{1}{\varepsilon}\Bigr)$ and
$\widetilde{O}\Bigl(\theta_{\max}(d+k)\Bigl(\ln\frac{1}{\varepsilon}+\frac{\nu^2}{\varepsilon^2}\Bigr)+\frac{k\nu}{\varepsilon^2}\Bigr)$
in the realizable and agnostic settings respectively, where $\theta_{\max}$ is
the maximum disagreement coefficient among the $k$ distributions, $d$ is the VC
dimension of the hypothesis class, $\nu$ is the multi-distribution error of the
best hypothesis, and $\varepsilon$ is the target excess error. Moreover, we
show that the bound in the realizable setting is information-theoretically
optimal and that the $k\nu/\varepsilon^2$ term in the agnostic setting is
fundamental for proper learners. We also establish instance-dependent sample
complexity bound for passive multidistribution learning that smoothly
interpolates between realizable and agnostic
regimes~\citep{blum2017collaborative,zhang2024optimal}, which may be of
independent interest.

</details>
