<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 27]
- [cs.CV](#cs.CV) [Total: 23]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.IR](#cs.IR) [Total: 9]
- [cs.LG](#cs.LG) [Total: 23]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MultiPL-MoE: Multi-Programming-Lingual Extension of Large Language Models through Hybrid Mixture-of-Experts](https://arxiv.org/abs/2508.19268)
*Qing Wang,Xue Han,Jiahui Wang,Lehao Xing,Qian Hu,Lianlian Zhang,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: This paper introduces MultiPL-MoE, a hybrid MoE approach to improve multilingual code generation in LLMs, which combines token-level and segment-level MoEs with innovative designs.


<details>
  <summary>Details</summary>
Motivation: Multilingual code generation remains extremely challenging. To address this, we intent to improve the multi-programming-lingual (MultiPL) performance of the base LLMs while retaining the most popular ones using restricted computational resources. We consider MultiPL to be a special case of multiple natural languages

Method: We propose a MultiPL extension of LLMs utilizing a hybrid mixture of experts (MoE), called MultiPL-MoE. Specifically, MultiPL-MoE combines two paired MoEs to optimize expert selection at both the token and segment levels.

Result: The token-level MoE is a standard upcycling MoE structure with a shared expert and a novel gate weight normalization approach that aids in the final fusion with the segment-level MoE. The segment-level MoE incorporates two innovative designs to better capture the syntactic structure and contextual patterns of programming languages: First, using a sliding window to partition the input token sequence into multiple segments; Then, adopting an expert-choice routing strategy that allows experts to select the top-k segments.

Conclusion: The experiment results proved the effectiveness of MultiPL-MoE.

Abstract: Despite LLMs' excellent code creation capabilities, multilingual code
generation remains extremely challenging. To address this, we intent to improve
the multi-programming-lingual (MultiPL) performance of the base LLMs while
retaining the most popular ones using restricted computational resources. We
consider MultiPL to be a special case of multiple natural languages and propose
a MultiPL extension of LLMs utilizing a hybrid mixture of experts (MoE), called
MultiPL-MoE. Specifically, MultiPL-MoE combines two paired MoEs to optimize
expert selection at both the token and segment levels. The token-level MoE is a
standard upcycling MoE structure with a shared expert and a novel gate weight
normalization approach that aids in the final fusion with the segment-level
MoE. The segment-level MoE incorporates two innovative designs to better
capture the syntactic structure and contextual patterns of programming
languages: First, using a sliding window to partition the input token sequence
into multiple segments; Then, adopting an expert-choice routing strategy that
allows experts to select the top-k segments. The results of the experiment
proved the effectiveness of MultiPL-MoE.

</details>


### [2] [Whisper based Cross-Lingual Phoneme Recognition between Vietnamese and English](https://arxiv.org/abs/2508.19270)
*Nguyen Huu Nhat Minh,Tran Nguyen Anh,Truong Dinh Dung,Vo Van Nam,Le Pham Tuyen*

Main category: cs.CL

TL;DR: A new bilingual speech recognition approach is proposed to improve the accuracy of Vietnamese phoneme recognition by constructing a bilingual phoneme set and using an end-to-end system with the PhoWhisper pre-trained encoder.


<details>
  <summary>Details</summary>
Motivation: Cross-lingual phoneme recognition has emerged as a significant challenge for accurate automatic speech recognition (ASR) when mixing Vietnamese and English pronunciations. Unlike many languages, Vietnamese relies on tonal variations to distinguish word meanings, whereas English features stress patterns and non-standard pronunciations that hinder phoneme alignment between the two languages.

Method: A novel bilingual speech recognition approach with two primary contributions: (1) constructing a representative bilingual phoneme set that bridges the differences between Vietnamese and English phonetic systems; (2) designing an end-to-end system that leverages the PhoWhisper pre-trained encoder for deep high-level representations to improve phoneme recognition.

Result: The proposed approach improves recognition accuracy in bilingual speech recognition for Vietnamese.

Conclusion: The proposed approach improves recognition accuracy in bilingual speech recognition for Vietnamese and provides a robust framework for addressing the complexities of tonal and stress-based phoneme recognition.

Abstract: Cross-lingual phoneme recognition has emerged as a significant challenge for
accurate automatic speech recognition (ASR) when mixing Vietnamese and English
pronunciations. Unlike many languages, Vietnamese relies on tonal variations to
distinguish word meanings, whereas English features stress patterns and
non-standard pronunciations that hinder phoneme alignment between the two
languages. To address this challenge, we propose a novel bilingual speech
recognition approach with two primary contributions: (1) constructing a
representative bilingual phoneme set that bridges the differences between
Vietnamese and English phonetic systems; (2) designing an end-to-end system
that leverages the PhoWhisper pre-trained encoder for deep high-level
representations to improve phoneme recognition. Our extensive experiments
demonstrate that the proposed approach not only improves recognition accuracy
in bilingual speech recognition for Vietnamese but also provides a robust
framework for addressing the complexities of tonal and stress-based phoneme
recognition

</details>


### [3] [Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond ICL and CoT](https://arxiv.org/abs/2508.19271)
*Rushitha Santhoshi Mamidala,Anshuman Chhabra,Ankur Mali*

Main category: cs.CL

TL;DR: 用局部、任务自适应加权有限自动机 (WFA) 替换全局数据存储来扩展 RetoMaton，从而增强大型语言模型在推理任务中的性能，同时保持透明和可重现的检索动态。


<details>
  <summary>Details</summary>
Motivation: 基于提示的推理策略（如 Chain-of-Thought (CoT) 和 In-Context Learning (ICL)）已广泛用于激发大型语言模型 (LLM) 的推理能力。然而，这些方法依赖于脆弱的、隐式的机制，这些机制通常会在种子、格式或细微的提示变化中产生不一致的输出，从而使其对于需要稳定、可解释的推理的任务而言，从根本上是不可靠的。

Method: 用从外部领域语料库直接构建的局部、任务自适应加权有限自动机 (WFA) 替换其全局数据存储来扩展 RetoMaton。

Result: 在三个推理任务上评估了这种局部 RetoMaton 变体：TriviaQA（阅读理解）、GSM8K（多步数学）和 MMLU（领域知识）。与基础模型和基于提示的方法相比，用局部 RetoMaton 增强这些设置始终可以提高性能，同时实现透明和可重现的检索动态。

Conclusion: 通过轻量级的、自动机引导的记忆，在现代大型语言模型中朝着可信的、符号推理的方向转变。

Abstract: Prompt-based reasoning strategies such as Chain-of-Thought (CoT) and
In-Context Learning (ICL) have become widely used for eliciting reasoning
capabilities in large language models (LLMs). However, these methods rely on
fragile, implicit mechanisms often yielding inconsistent outputs across seeds,
formats, or minor prompt variations making them fundamentally unreliable for
tasks requiring stable, interpretable reasoning. In contrast, automata-based
neuro-symbolic frameworks like RetoMaton offer a more structured and
trustworthy alternative by grounding retrieval in symbolic memory with
deterministic transitions. In this work, we extend RetoMaton by replacing its
global datastore with a local, task-adaptive Weighted Finite Automaton (WFA),
constructed directly from external domain corpora. This local automaton
structure promotes robust, context-aware retrieval while preserving symbolic
traceability and low inference overhead. Unlike prompting, which entangles
context and memory in opaque ways, our approach leverages the explicit
structure of WFAs to provide verifiable and modular retrieval behavior, making
it better suited for domain transfer and interoperability. We evaluate this
local RetoMaton variant on two pretrained LLMs LLaMA-3.2-1B and Gemma-3-1B-PT
across three reasoning tasks: TriviaQA (reading comprehension), GSM8K
(multi-step math), and MMLU (domain knowledge). Compared to the base model and
prompting-based methods, augmenting these setups with local RetoMaton
consistently improves performance while enabling transparent and reproducible
retrieval dynamics. Our results highlight a promising shift toward trustworthy,
symbolic reasoning in modern LLMs via lightweight, automaton-guided memory.

</details>


### [4] [RAGAPHENE: A RAG Annotation Platform with Human Enhancements and Edits](https://arxiv.org/abs/2508.19272)
*Kshitij Fadnis,Sara Rosenthal,Maeda Hanafi,Yannis Katsis,Marina Danilevsky*

Main category: cs.CL

TL;DR: RAGAPHENE是一个聊天平台，用于模拟真实世界的对话，以评估大型语言模型的检索增强生成能力。


<details>
  <summary>Details</summary>
Motivation: 当事实正确的信息很重要时，检索增强生成(RAG)是与大型语言模型(llm)对话的一个重要方面。llm可能会提供看起来正确的答案，但可能包含虚构的信息。因此，构建能够评估llm在多轮RAG对话中的基准已经成为一项日益重要的任务。模拟真实世界的对话对于生成高质量的评估基准至关重要。

Method: 构建了一个名为RAGAPHENE的基于聊天的标注平台。

Result: RAGAPHENE是一个基于聊天的注释平台，它使注释者能够模拟真实世界的对话，以用于基准测试和评估llm。

Conclusion: RAGAPHENE平台已被成功用于构建数千个真实对话。

Abstract: Retrieval Augmented Generation (RAG) is an important aspect of conversing
with Large Language Models (LLMs) when factually correct information is
important. LLMs may provide answers that appear correct, but could contain
hallucinated information. Thus, building benchmarks that can evaluate LLMs on
multi-turn RAG conversations has become an increasingly important task.
Simulating real-world conversations is vital for producing high quality
evaluation benchmarks. We present RAGAPHENE, a chat-based annotation platform
that enables annotators to simulate real-world conversations for benchmarking
and evaluating LLMs. RAGAPHENE has been successfully used by approximately 40
annotators to build thousands of real-world conversations.

</details>


### [5] [Database Entity Recognition with Data Augmentation and Deep Learning](https://arxiv.org/abs/2508.19372)
*Zikun Fu,Chen Yang,Kourosh Davoudi,Ken Q. Pu*

Main category: cs.CL

TL;DR: This paper introduces a new DB-ER tagger with data augmentation and T5 fine-tuning, achieving superior precision and recall compared to existing NER taggers.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of Database Entity Recognition (DB-ER) in Natural Language Queries (NLQ).

Method: A specialized language model based entity recognition model using T5 as a backbone, fine-tuned with sequence tagging and token classification. A novel data augmentation procedure leverages automatic annotation of NLQs based on corresponding SQL queries.

Result: The DB-ER tagger shows better performance in both precision and recall compared to state-of-the-art NER taggers. Data augmentation boosts precision and recall by over 10%, and fine-tuning of the T5 backbone boosts these metrics by 5-10%.

Conclusion: The proposed DB-ER tagger outperforms state-of-the-art NER taggers in precision and recall. Data augmentation and fine-tuning of the T5 backbone significantly improve performance.

Abstract: This paper addresses the challenge of Database Entity Recognition (DB-ER) in
Natural Language Queries (NLQ). We present several key contributions to advance
this field: (1) a human-annotated benchmark for DB-ER task, derived from
popular text-to-sql benchmarks, (2) a novel data augmentation procedure that
leverages automatic annotation of NLQs based on the corresponding SQL queries
which are available in popular text-to-SQL benchmarks, (3) a specialized
language model based entity recognition model using T5 as a backbone and two
down-stream DB-ER tasks: sequence tagging and token classification for
fine-tuning of backend and performing DB-ER respectively. We compared our DB-ER
tagger with two state-of-the-art NER taggers, and observed better performance
in both precision and recall for our model. The ablation evaluation shows that
data augmentation boosts precision and recall by over 10%, while fine-tuning of
the T5 backbone boosts these metrics by 5-10%.

</details>


### [6] [Leveraging Language Models and Machine Learning in Verbal Autopsy Analysis](https://arxiv.org/abs/2508.19274)
*Yue Chu*

Main category: cs.CL

TL;DR: 这篇论文研究了如何使用口头尸检(VA)叙述来改进死因(COD)分类，发现叙述可以提高分类准确性，并强调需要更多高质量数据。


<details>
  <summary>Details</summary>
Motivation: 在没有民事登记和生命统计的国家，口头尸检(VA)是估计死因(COD)和为政策重点提供信息的重要工具。现有的自动VA死因分类算法只使用问题，忽略了叙述中的信息。

Method: 这篇论文使用预训练语言模型(PLM)和机器学习(ML)技术，研究如何使用VA叙述进行自动死因(COD)分类。

Result: 仅使用叙述，经过特定任务微调的基于Transformer的PLM在个体和群体层面都优于领先的仅使用问题的算法，尤其是在识别非传染性疾病方面。多模态方法进一步提高了COD分类的性能，证实了每种模态都有独特的贡献，并且可以捕获另一种模态中不存在的有价值的信息。此外，还描述了医生感知的VA信息充分性，并表明分类准确性受到医生和模型充分性的影响。

Conclusion: 这篇论文证明了叙述在提升死因分类中的价值，并强调需要更多来自不同环境的高质量数据，以用于训练和微调PLM/ML方法。研究结果为重新思考和重新设计VA工具和访谈提供了有价值的见解。

Abstract: In countries without civil registration and vital statistics, verbal autopsy
(VA) is a critical tool for estimating cause of death (COD) and inform policy
priorities. In VA, interviewers ask proximal informants for details on the
circumstances preceding a death, in the form of unstructured narratives and
structured questions. Existing automated VA cause classification algorithms
only use the questions and ignore the information in the narratives. In this
thesis, we investigate how the VA narrative can be used for automated COD
classification using pretrained language models (PLMs) and machine learning
(ML) techniques. Using empirical data from South Africa, we demonstrate that
with the narrative alone, transformer-based PLMs with task-specific fine-tuning
outperform leading question-only algorithms at both the individual and
population levels, particularly in identifying non-communicable diseases. We
explore various multimodal fusion strategies combining narratives and questions
in unified frameworks. Multimodal approaches further improve performance in COD
classification, confirming that each modality has unique contributions and may
capture valuable information that is not present in the other modality. We also
characterize physician-perceived information sufficiency in VA. We describe
variations in sufficiency levels by age and COD and demonstrate that
classification accuracy is affected by sufficiency for both physicians and
models. Overall, this thesis advances the growing body of knowledge at the
intersection of natural language processing, epidemiology, and global health.
It demonstrates the value of narrative in enhancing COD classification. Our
findings underscore the need for more high-quality data from more diverse
settings to use in training and fine-tuning PLM/ML methods, and offer valuable
insights to guide the rethinking and redesign of the VA instrument and
interview.

</details>


### [7] [FLAIRR-TS -- Forecasting LLM-Agents with Iterative Refinement and Retrieval for Time Series](https://arxiv.org/abs/2508.19279)
*Gunjan Jalori,Preetika Verma,Sercan Ö Arık*

Main category: cs.CL

TL;DR: FLAIRR-TS是一种用于LLM时间序列预测的测试时提示优化框架，它使用代理系统自适应地改进提示，优于静态方法，并接近专门预测器的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的时间序列预测需要桥接数值模式和自然语言。在LLM上进行有效的预测通常依赖于大量的预处理和微调。最近的研究表明，当提供精心设计的自然语言提示时，一个冻结的LLM可以与专门的预测器相媲美，但是为每个任务制作这样的提示本身就很麻烦和随意。

Method: FLAIRR-TS，一个测试时提示优化框架，它利用一个代理系统：一个预测代理使用初始提示生成预测，然后由一个细化代理根据过去的输出和检索到的类似物进行细化。

Result: 在基准数据集上的实验表明，FLAIRR-TS的准确性优于静态提示和检索增强基线，接近于专门提示的性能。

Conclusion: FLAIRR-TS通过其代理方法实现自适应提示改进和检索，在不进行微调的情况下实现了强大的性能，为时间序列预测提供了一种实用的替代方案。

Abstract: Time series Forecasting with large languagemodels (LLMs) requires bridging
numericalpatterns and natural language. Effective fore-casting on LLM often
relies on extensive pre-processing and fine-tuning.Recent studiesshow that a
frozen LLM can rival specializedforecasters when supplied with a carefully
en-gineered natural-language prompt, but craft-ing such a prompt for each task
is itself oner-ous and ad-hoc. We introduce FLAIRR-TS, atest-time prompt
optimization framework thatutilizes an agentic system: a
Forecaster-agentgenerates forecasts using an initial prompt,which is then
refined by a refiner agent, in-formed by past outputs and retrieved
analogs.This adaptive prompting generalizes across do-mains using creative
prompt templates andgenerates high-quality forecasts without inter-mediate code
generation.Experiments onbenchmark datasets show improved accuracyover static
prompting and retrieval-augmentedbaselines, approaching the performance
ofspecialized prompts.FLAIRR-TS providesa practical alternative to tuning,
achievingstrong performance via its agentic approach toadaptive prompt
refinement and retrieval.

</details>


### [8] [CORE: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning](https://arxiv.org/abs/2508.19282)
*Ziqiang Cui,Yunpeng Weng,Xing Tang,Peiyang Liu,Shiwei Li,Bowei He,Jiamin Chen,Xiuqiang He,Chen Ma*

Main category: cs.CL

TL;DR: This paper presents CORE, a reinforcement learning approach for compressing retrieved documents in RAG, which maintains performance while significantly reducing input length and computational cost.


<details>
  <summary>Details</summary>
Motivation: The inclusion of excessive retrieved documents in RAG increases computational costs, and previous compression methods compromise end-task performance due to the lack of well-defined compression targets.

Method: The paper uses reinforcement learning to optimize the compression process, utilizing end-task performance as a reward signal and applying Generalized Reinforcement Learning Policy Optimization (GRPO) to train the compressor.

Result: CORE achieves a high compression ratio of 3% without performance degradation and improves the average Exact Match (EM) score by 3.3 points across four datasets.

Conclusion: The paper introduces CORE, a reinforcement learning method for lossless context compression in RAG, achieving a 3% compression ratio without performance degradation and improving the average Exact Match score by 3.3 points.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach to
enhance the timeliness of knowledge and the factual accuracy of responses in
Large Language Models (LLMs). However, the inclusion of excessive retrieved
documents substantially increases the input length, leading to higher
computational costs. Previous studies have attempted to compress retrieved
documents into shorter texts before in-context integration, but such methods
often compromise end-task performance. The lack of well-defined compression
targets forces many approaches to rely on fixed heuristics, which cannot
guarantee that the compressed content will effectively support the end task. To
address these limitations, we propose CORE, a novel method designed to achieve
lossless context compression for RAG. CORE employs reinforcement learning to
optimize the compression process without relying on predefined compression
labels. Specifically, it utilizes end-task performance as a reward signal and
applies Generalized Reinforcement Learning Policy Optimization (GRPO) to train
the compressor. This end-to-end training framework enables the compressor to
generate summaries that maximize the accuracy of answers generated by the LLM.
Extensive experiments on four datasets demonstrate the superiority of our
approach. With a high compression ratio of 3\%, our method not only avoids
performance degradation compared to prepending full documents across all
datasets but also improves the average Exact Match (EM) score by 3.3 points.
The code will be released soon.

</details>


### [9] [Inference Gap in Domain Expertise and Machine Intelligence in Named Entity Recognition: Creation of and Insights from a Substance Use-related Dataset](https://arxiv.org/abs/2508.19467)
*Sumon Kanti Dey,Jeanne M. Powell,Azra Ismail,Jeanmarie Perrone,Abeed Sarker*

Main category: cs.CL

TL;DR: This study uses NER to extract clinical and social impacts of opioid use from social media, introduces a new dataset, and finds that fine-tuned models outperform LLMs but still lag behind expert agreement.


<details>
  <summary>Details</summary>
Motivation: Social media platforms offer a valuable yet underutilized source of insight into the clinical and social consequences of nonmedical opioid use.

Method: a named entity recognition (NER) framework to extract two categories of self-reported consequences from social media narratives related to opioid use

Result: A fine-tuned DeBERTa-large model achieves a relaxed token-level F1 of 0.61 [95% CI: 0.43-0.62], consistently outperforming LLMs in precision, span accuracy, and adherence to task-specific guidelines. Strong NER performance can be achieved with substantially less labeled data.

Conclusion: The best performing model still significantly underperforms compared to inter-expert agreement, demonstrating that a gap persists between expert intelligence and current state-of-the-art NER/AI capabilities for tasks requiring deep domain knowledge.

Abstract: Nonmedical opioid use is an urgent public health challenge, with far-reaching
clinical and social consequences that are often underreported in traditional
healthcare settings. Social media platforms, where individuals candidly share
first-person experiences, offer a valuable yet underutilized source of insight
into these impacts. In this study, we present a named entity recognition (NER)
framework to extract two categories of self-reported consequences from social
media narratives related to opioid use: ClinicalImpacts (e.g., withdrawal,
depression) and SocialImpacts (e.g., job loss). To support this task, we
introduce RedditImpacts 2.0, a high-quality dataset with refined annotation
guidelines and a focus on first-person disclosures, addressing key limitations
of prior work. We evaluate both fine-tuned encoder-based models and
state-of-the-art large language models (LLMs) under zero- and few-shot
in-context learning settings. Our fine-tuned DeBERTa-large model achieves a
relaxed token-level F1 of 0.61 [95% CI: 0.43-0.62], consistently outperforming
LLMs in precision, span accuracy, and adherence to task-specific guidelines.
Furthermore, we show that strong NER performance can be achieved with
substantially less labeled data, emphasizing the feasibility of deploying
robust models in resource-limited settings. Our findings underscore the value
of domain-specific fine-tuning for clinical NLP tasks and contribute to the
responsible development of AI tools that may enhance addiction surveillance,
improve interpretability, and support real-world healthcare decision-making.
The best performing model, however, still significantly underperforms compared
to inter-expert agreement (Cohen's kappa: 0.81), demonstrating that a gap
persists between expert intelligence and current state-of-the-art NER/AI
capabilities for tasks requiring deep domain knowledge.

</details>


### [10] [Context-Adaptive Synthesis and Compression for Enhanced Retrieval-Augmented Generation in Complex Domains](https://arxiv.org/abs/2508.19357)
*Peiran Zhou,Junnan Zhu,Yichen Shen,Ruoxi Yu*

Main category: cs.CL

TL;DR: CASC通过智能处理检索到的上下文来改进RAG，从而在复杂领域中提供更准确和值得信赖的答案。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 在语言任务中表现出色，但容易出现幻觉和过时的知识。检索增强生成 (RAG) 通过将 LLM 定位于外部知识来缓解这些问题。然而，在涉及多个、冗长或冲突文档的复杂领域中，传统的 RAG 会受到信息过载和综合效率低下的影响，从而导致不准确和不可信的答案。

Method: 提出了一种名为CASC（Context-Adaptive Synthesis and Compression）的新框架，该框架智能地处理检索到的上下文。CASC 引入了一个上下文分析器和合成器 (CAS) 模块，该模块由经过微调的较小 LLM 提供支持，该模块执行关键信息提取、跨文档一致性检查和冲突解决以及面向问题的结构化合成。

Result: CASC持续优于强大的基线。

Conclusion: CASC在SciDocs-QA上表现优于基线模型。

Abstract: Large Language Models (LLMs) excel in language tasks but are prone to
hallucinations and outdated knowledge. Retrieval-Augmented Generation (RAG)
mitigates these by grounding LLMs in external knowledge. However, in complex
domains involving multiple, lengthy, or conflicting documents, traditional RAG
suffers from information overload and inefficient synthesis, leading to
inaccurate and untrustworthy answers. To address this, we propose CASC
(Context-Adaptive Synthesis and Compression), a novel framework that
intelligently processes retrieved contexts. CASC introduces a Context Analyzer
& Synthesizer (CAS) module, powered by a fine-tuned smaller LLM, which performs
key information extraction, cross-document consistency checking and conflict
resolution, and question-oriented structured synthesis. This process transforms
raw, scattered information into a highly condensed, structured, and
semantically rich context, significantly reducing the token count and cognitive
load for the final Reader LLM. We evaluate CASC on SciDocs-QA, a new
challenging multi-document question answering dataset designed for complex
scientific domains with inherent redundancies and conflicts. Our extensive
experiments demonstrate that CASC consistently outperforms strong baselines.

</details>


### [11] [Uncovering the Bigger Picture: Comprehensive Event Understanding Via Diverse News Retrieval](https://arxiv.org/abs/2508.19758)
*Yixuan Tang,Yuanyuan Shi,Yiqun Sun,Anthony Kum Hoe Tung*

Main category: cs.CL

TL;DR: NEWSCOPE, a two-stage framework, enhances event coverage by modeling semantic variation at the sentence level, achieving higher diversity without compromising relevance.


<details>
  <summary>Details</summary>
Motivation: Most news retrieval systems prioritize textual relevance, leading to redundant results and limited viewpoint exposure.

Method: A two-stage framework with dense retrieval and sentence-level clustering with diversity-aware re-ranking.

Result: NEWSCOPE consistently outperforms strong baselines on two paragraph-level benchmarks: LocalNews and DSGlobal.

Conclusion: NEWSCOPE achieves higher diversity without compromising relevance by fine-grained, interpretable modeling.

Abstract: Access to diverse perspectives is essential for understanding real-world
events, yet most news retrieval systems prioritize textual relevance, leading
to redundant results and limited viewpoint exposure. We propose NEWSCOPE, a
two-stage framework for diverse news retrieval that enhances event coverage by
explicitly modeling semantic variation at the sentence level. The first stage
retrieves topically relevant content using dense retrieval, while the second
stage applies sentence-level clustering and diversity-aware re-ranking to
surface complementary information. To evaluate retrieval diversity, we
introduce three interpretable metrics, namely Average Pairwise Distance,
Positive Cluster Coverage, and Information Density Ratio, and construct two
paragraph-level benchmarks: LocalNews and DSGlobal. Experiments show that
NEWSCOPE consistently outperforms strong baselines, achieving significantly
higher diversity without compromising relevance. Our results demonstrate the
effectiveness of fine-grained, interpretable modeling in mitigating redundancy
and promoting comprehensive event understanding. The data and code are
available at https://github.com/tangyixuan/NEWSCOPE.

</details>


### [12] [Reflective Agreement: Combining Self-Mixture of Agents with a Sequence Tagger for Robust Event Extraction](https://arxiv.org/abs/2508.19359)
*Fatemeh Haji,Mazal Bethany,Cho-Yu Jason Chiang,Anthony Rios,Peyman Najafirad*

Main category: cs.CL

TL;DR: 提出了一种混合事件提取方法ARIS，它结合了判别模型和生成模型（LLM）的优点，并在三个基准数据集上取得了优于现有技术的结果。


<details>
  <summary>Details</summary>
Motivation: 传统的判别模型精度高但召回率有限，生成方法（LLM）语义灵活性和召回率较高但存在幻觉和预测不一致问题。

Method: 结合了自混合代理和判别序列标注器的混合方法，显式地利用结构化模型共识、基于置信度的过滤和LLM反射推理模块。

Result: ARIS方法优于现有技术。

Conclusion: ARIS方法在三个基准数据集上优于现有的最先进事件提取方法。

Abstract: Event Extraction (EE) involves automatically identifying and extracting
structured information about events from unstructured text, including triggers,
event types, and arguments. Traditional discriminative models demonstrate high
precision but often exhibit limited recall, particularly for nuanced or
infrequent events. Conversely, generative approaches leveraging Large Language
Models (LLMs) provide higher semantic flexibility and recall but suffer from
hallucinations and inconsistent predictions. To address these challenges, we
propose Agreement-based Reflective Inference System (ARIS), a hybrid approach
combining a Self Mixture of Agents with a discriminative sequence tagger. ARIS
explicitly leverages structured model consensus, confidence-based filtering,
and an LLM reflective inference module to reliably resolve ambiguities and
enhance overall event prediction quality. We further investigate decomposed
instruction fine-tuning for enhanced LLM event extraction understanding.
Experiments demonstrate our approach outperforms existing state-of-the-art
event extraction methods across three benchmark datasets.

</details>


### [13] [Selective Retrieval-Augmentation for Long-Tail Legal Text Classification](https://arxiv.org/abs/2508.19997)
*Boheng Mao*

Main category: cs.CL

TL;DR: This paper proposes Selective Retrieval-Augmentation (SRA) to improve the performance of legal text classification models on rare classes in long-tail datasets. SRA augments samples of low-frequency labels and outperforms existing baselines.


<details>
  <summary>Details</summary>
Motivation: Benchmark datasets in this area often exhibit a long-tail label distribution, where many labels are underrepresented, leading to poor model performance on rare classes.

Method: Selective Retrieval-Augmentation (SRA): focuses on augmenting samples belonging to low-frequency labels in the training set, preventing the introduction of noise for well-represented classes, and requires no changes to the model architecture. Retrieval is performed only from the training data.

Result: SRA attains higher micro-F1 and macro-F1 scores compared to all current LexGLUE baselines across both datasets.

Conclusion: SRA attains higher micro-F1 and macro-F1 scores compared to all current LexGLUE baselines across both datasets, illustrating consistent improvements in long-tail legal text classification.

Abstract: Legal text classification is a fundamental NLP task in the legal domain.
Benchmark datasets in this area often exhibit a long-tail label distribution,
where many labels are underrepresented, leading to poor model performance on
rare classes. This paper proposes Selective Retrieval-Augmentation (SRA) as a
solution to this problem. SRA focuses on augmenting samples belonging to
low-frequency labels in the training set, preventing the introduction of noise
for well-represented classes, and requires no changes to the model
architecture. Retrieval is performed only from the training data to ensure
there is no potential information leakage, removing the need for external
corpora simultaneously. The proposed SRA method is tested on two legal text
classification benchmark datasets with long-tail distributions: LEDGAR
(single-label) and UNFAIR-ToS (multi-label). The results indicate that SRA
attains higher micro-F1 and macro-F1 scores compared to all current LexGLUE
baselines across both datasets, illustrating consistent improvements in
long-tail legal text classification. The code repository is available at:
https://github.com/Boheng-Mao/sra-legal

</details>


### [14] [LongReasonArena: A Long Reasoning Benchmark for Large Language Models](https://arxiv.org/abs/2508.19363)
*Jiayu Ding,Shuming Ma,Lei Cui,Nanning Zheng,Furu Wei*

Main category: cs.CL

TL;DR: LongReasonArena is introduced to assess the long reasoning capabilities of LLMs, which requires models to solve problems by executing multi-step algorithms. It is a significant challenge for both open-source and proprietary LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing long-context benchmarks for Large Language Models (LLMs) focus on evaluating comprehension of long inputs, while overlooking the evaluation of long reasoning abilities. To address this gap, we introduce LongReasonArena, a benchmark specifically designed to assess the long reasoning capabilities of LLMs.

Method: Tasks require models to solve problems by executing multi-step algorithms that reflect key aspects of long reasoning, such as retrieval and backtracking. By controlling the inputs, the required reasoning length can be arbitrarily scaled, reaching up to 1 million tokens of reasoning for the most challenging tasks.

Result: Deepseek-R1 achieves only 7.5% accuracy on our task.

Conclusion: LongReasonArena presents a significant challenge for both open-source and proprietary LLMs. The accuracy exhibits a linear decline with respect to the logarithm of the expected number of reasoning steps.

Abstract: Existing long-context benchmarks for Large Language Models (LLMs) focus on
evaluating comprehension of long inputs, while overlooking the evaluation of
long reasoning abilities. To address this gap, we introduce LongReasonArena, a
benchmark specifically designed to assess the long reasoning capabilities of
LLMs. Our tasks require models to solve problems by executing multi-step
algorithms that reflect key aspects of long reasoning, such as retrieval and
backtracking. By controlling the inputs, the required reasoning length can be
arbitrarily scaled, reaching up to 1 million tokens of reasoning for the most
challenging tasks. Extensive evaluation results demonstrate that
LongReasonArena presents a significant challenge for both open-source and
proprietary LLMs. For instance, Deepseek-R1 achieves only 7.5% accuracy on our
task. Further analysis also reveals that the accuracy exhibits a linear decline
with respect to the logarithm of the expected number of reasoning steps. Our
code and data is available at
https://github.com/LongReasonArena/LongReasonArena.

</details>


### [15] [One Joke to Rule them All? On the (Im)possibility of Generalizing Humor](https://arxiv.org/abs/2508.19402)
*Mor Turgeman,Chen Shani,Dafna Shahaf*

Main category: cs.CL

TL;DR: LLMs can generalize humor understanding across different types, with Dad Jokes being surprisingly helpful for transfer learning.


<details>
  <summary>Details</summary>
Motivation: To understand if competence on specific humor tasks can transfer to novel, unseen types, especially as new humor types emerge online. To determine if fragmentation in humor understanding is inevitable for Large Language Models (LLMs).

Method: Transfer learning experiments across four datasets representing different humor tasks. LLMs trained under varied diversity settings (1-3 datasets in training, testing on a novel task).

Result: Models are capable of some transfer, reaching up to 75% accuracy on unseen datasets. Training on diverse sources improves transferability (1.88-4.05%) with minimal-to-no drop in in-domain performance. Dad Jokes are the best enabler of transfer but are difficult to transfer to.

Conclusion: Models can transfer humor understanding to new types, achieving up to 75% accuracy on unseen datasets. Training on diverse sources improves transferability. Dad Jokes are surprisingly effective for enabling transfer but hard to transfer to.

Abstract: Humor is a broad and complex form of communication that remains challenging
for machines. Despite its broadness, most existing research on computational
humor traditionally focused on modeling a specific type of humor. In this work,
we wish to understand whether competence on one or more specific humor tasks
confers any ability to transfer to novel, unseen types; in other words, is this
fragmentation inevitable? This question is especially timely as new humor types
continuously emerge in online and social media contexts (e.g., memes,
anti-humor, AI fails). If Large Language Models (LLMs) are to keep up with this
evolving landscape, they must be able to generalize across humor types by
capturing deeper, transferable mechanisms. To investigate this, we conduct a
series of transfer learning experiments across four datasets, representing
different humor tasks. We train LLMs under varied diversity settings (1-3
datasets in training, testing on a novel task). Experiments reveal that models
are capable of some transfer, and can reach up to 75% accuracy on unseen
datasets; training on diverse sources improves transferability (1.88-4.05%)
with minimal-to-no drop in in-domain performance. Further analysis suggests
relations between humor types, with Dad Jokes surprisingly emerging as the best
enabler of transfer (but is difficult to transfer to). We release data and
code.

</details>


### [16] [A perishable ability? The future of writing in the face of generative artificial intelligence](https://arxiv.org/abs/2508.19427)
*Evandro L. T. P. Cunha*

Main category: cs.CL

TL;DR: 探讨了由于机器代写，未来人类写作能力可能丧失的可能性，类比希腊黑暗时代


<details>
  <summary>Details</summary>
Motivation: 2020年代见证了生成式人工智能工具的重大发展，包括基于大型语言模型的文本生成系统。这些工具越来越多地被用于生成各种领域的文本，从技术文本到文学文本，这最终可能导致人类书面文本产量降低。

Method: 讨论

Result: 不适用

Conclusion: 人类可能由于将写作外包给机器而失去或显著降低写作能力

Abstract: The 2020s have been witnessing a very significant advance in the development
of generative artificial intelligence tools, including text generation systems
based on large language models. These tools have been increasingly used to
generate texts in the most diverse domains -- from technical texts to literary
texts --, which might eventually lead to a lower volume of written text
production by humans. This article discusses the possibility of a future in
which human beings will have lost or significantly decreased their ability to
write due to the outsourcing of this activity to machines. This possibility
parallels the loss of the ability to write in other moments of human history,
such as during the so-called Greek Dark Ages (approx. 1200 BCE - 800 BCE).

</details>


### [17] [Heterogeneous LLM Methods for Ontology Learning (Few-Shot Prompting, Ensemble Typing, and Attention-Based Taxonomies)](https://arxiv.org/abs/2508.19428)
*Aleksandra Beliaeva,Temurbek Rahmatullaev*

Main category: cs.CL

TL;DR: A modular, LLM-based system for ontology construction (term extraction, typing, and taxonomy discovery) achieved top results in the LLMs4OL 2025 challenge using RAG, zero-shot classification, and graph modeling.


<details>
  <summary>Details</summary>
Motivation: Addresses Tasks A, B, and C of the LLMs4OL 2025 challenge, which together span the full ontology construction pipeline: term extraction, typing, and taxonomy discovery.

Method: Combines retrieval-augmented prompting, zero-shot classification, and attention-based graph modeling, each tailored to the demands of the respective task.

Result: Achieved top-ranking results in the official leaderboard across all three tasks.

Conclusion: LLM-based architectures demonstrate scalability, adaptability, and robustness for ontology learning across heterogeneous domains, achieving top-ranking results in the LLMs4OL 2025 challenge.

Abstract: We present a comprehensive system for addressing Tasks A, B, and C of the
LLMs4OL 2025 challenge, which together span the full ontology construction
pipeline: term extraction, typing, and taxonomy discovery. Our approach
combines retrieval-augmented prompting, zero-shot classification, and
attention-based graph modeling -- each tailored to the demands of the
respective task. For Task A, we jointly extract domain-specific terms and their
ontological types using a retrieval-augmented generation (RAG) pipeline.
Training data was reformulated into a document to terms and types
correspondence, while test-time inference leverages semantically similar
training examples. This single-pass method requires no model finetuning and
improves overall performance through lexical augmentation Task B, which
involves assigning types to given terms, is handled via a dual strategy. In the
few-shot setting (for domains with labeled training data), we reuse the RAG
scheme with few-shot prompting. In the zero-shot setting (for previously unseen
domains), we use a zero-shot classifier that combines cosine similarity scores
from multiple embedding models using confidence-based weighting. In Task C, we
model taxonomy discovery as graph inference. Using embeddings of type labels,
we train a lightweight cross-attention layer to predict is-a relations by
approximating a soft adjacency matrix. These modular, task-specific solutions
enabled us to achieve top-ranking results in the official leaderboard across
all three tasks. Taken together these strategies showcase the scalability,
adaptability, and robustness of LLM-based architectures for ontology learning
across heterogeneous domains.
  Code is available at:
https://github.com/BelyaevaAlex/LLMs4OL-Challenge-Alexbek

</details>


### [18] [Bridging Language Gaps: Enhancing Few-Shot Language Adaptation](https://arxiv.org/abs/2508.19464)
*Philipp Borchert,Jochen De Weerdt,Marie-Francine Moens*

Main category: cs.CL

TL;DR: CoLAP通过整合对比学习和跨语言表示，促进了从高资源到低资源语言的任务特定知识转移，从而缩小了跨语言性能差距。


<details>
  <summary>Details</summary>
Motivation: 多语言NLP中语言资源的差异带来了挑战，高资源语言受益于大量数据，而低资源语言缺乏有效训练的足够数据。

Method: 集成了对比学习与跨语言表示的CoLAP方法。

Result: CoLAP优于few-shot跨语言迁移基线和上下文学习，即使在有限的可用数据下也是如此。

Conclusion: CoLAP有效地缩小了跨语言性能差距，有助于开发更高效的多语言NLP技术。

Abstract: The disparity in language resources poses a challenge in multilingual NLP,
with high-resource languages benefiting from extensive data, while low-resource
languages lack sufficient data for effective training. Our Contrastive Language
Alignment with Prompting (CoLAP) method addresses this gap by integrating
contrastive learning with cross-lingual representations, facilitating
task-specific knowledge transfer from high-resource to lower-resource
languages. The primary advantage of our approach is its data efficiency,
enabling rapid adaptation to new languages and reducing the need for large
labeled datasets. We conduct experiments with multilingual encoder-only and
decoder-only language models on natural language understanding tasks, including
natural language inference and relation extraction, evaluating performance
across both high- and low-resource languages. Our results demonstrate that
CoLAP outperforms few-shot cross-lingual transfer baselines and in-context
learning, even with limited available data. This effectively narrows the
cross-lingual performance gap, contributing to the development of more
efficient multilingual NLP techniques.

</details>


### [19] [Automatic Question & Answer Generation Using Generative Large Language Model (LLM)](https://arxiv.org/abs/2508.19475)
*Md. Alvee Ehsan,A. S. M Mehedi Hasan,Kefaya Benta Shahnoor,Syeda Sumaiya Tasneem*

Main category: cs.CL

TL;DR: This paper proposes an Automatic Question Answer Generation (AQAG) method using fine-tuned generative LLM to help instructors create diverse and fair questions. It leverages unsupervised learning in NLP and uses Meta-Llama 2-7B model with RACE dataset for fine-tuning, aiming to streamline the evaluation process and free up resources.


<details>
  <summary>Details</summary>
Motivation: Instructors need to make diverse sets of questions that need to be fair for all students to prove their adequacy over a particular topic. This can prove to be quite challenging as they may need to manually go through several different lecture materials. The objective is to make this whole process much easier by implementing Automatic Question Answer Generation (AQAG).

Method: Automatic Question Answer Generation (AQAG), using fine-tuned generative LLM with prompt Engineering (PE). Unsupervised learning methods in NLP, primarily focusing on the English language. Base Meta-Llama 2-7B model to integrate RACE dataset as training data for the fine-tuning process.

Result: The research proposes to leverage unsupervised learning methods in NLP, primarily focusing on the English language. This approach empowers the base Meta-Llama 2-7B model to integrate RACE dataset as training data for the fine-tuning process.

Conclusion: A customized model is created that will offer efficient solutions for educators, instructors, and individuals engaged in text-based evaluations. A reliable and efficient tool for generating questions and answers can free up valuable time and resources, thus streamlining their evaluation processes.

Abstract: \Abstract{In the realm of education, student evaluation holds equal
significance as imparting knowledge. To be evaluated, students usually need to
go through text-based academic assessment methods. Instructors need to make
diverse sets of questions that need to be fair for all students to prove their
adequacy over a particular topic. This can prove to be quite challenging as
they may need to manually go through several different lecture materials. Our
objective is to make this whole process much easier by implementing Automatic
Question Answer Generation /(AQAG), using fine-tuned generative LLM. For
tailoring the instructor's preferred question style (MCQ, conceptual, or
factual questions), prompt Engineering (PE) is being utilized. In this
research, we propose to leverage unsupervised learning methods in NLP,
primarily focusing on the English language. This approach empowers the base
Meta-Llama 2-7B model to integrate RACE dataset as training data for the
fine-tuning process. Creating a customized model that will offer efficient
solutions for educators, instructors, and individuals engaged in text-based
evaluations. A reliable and efficient tool for generating questions and answers
can free up valuable time and resources, thus streamlining their evaluation
processes.}

</details>


### [20] [Improving Low-Resource Translation with Dictionary-Guided Fine-Tuning and RL: A Spanish-to-Wayuunaiki Study](https://arxiv.org/abs/2508.19481)
*Manuel Mosquera,Melissa Robles,Johan Rodriguez,Ruben Manrique*

Main category: cs.CL

TL;DR: This paper introduces a new method for low-resource machine translation that uses an external dictionary and reinforcement learning to improve translation quality. The method achieves significant improvements in BLEU scores compared to previous work.


<details>
  <summary>Details</summary>
Motivation: Low-resource machine translation remains a significant challenge for large language models (LLMs), which often lack exposure to these languages during pretraining and have limited parallel data for fine-tuning.

Method: Integrating an external dictionary tool and training models end-to-end using reinforcement learning, in addition to supervised fine-tuning.

Result: Tool-augmented models achieve up to +3.37 BLEU improvement over previous work, and a 18% relative gain compared to a supervised baseline without dictionary access, on the Spanish-Wayuunaiki test set.

Conclusion: Combining LLMs with external tools and reinforcement learning improves translation quality in low-resource language settings.

Abstract: Low-resource machine translation remains a significant challenge for large
language models (LLMs), which often lack exposure to these languages during
pretraining and have limited parallel data for fine-tuning. We propose a novel
approach that enhances translation for low-resource languages by integrating an
external dictionary tool and training models end-to-end using reinforcement
learning, in addition to supervised fine-tuning. Focusing on the
Spanish-Wayuunaiki language pair, we frame translation as a tool-augmented
decision-making problem in which the model can selectively consult a bilingual
dictionary during generation. Our method combines supervised instruction tuning
with Guided Reward Policy Optimization (GRPO), enabling the model to learn both
when and how to use the tool effectively. BLEU similarity scores are used as
rewards to guide this learning process. Preliminary results show that our
tool-augmented models achieve up to +3.37 BLEU improvement over previous work,
and a 18% relative gain compared to a supervised baseline without dictionary
access, on the Spanish-Wayuunaiki test set from the AmericasNLP 2025 Shared
Task. We also conduct ablation studies to assess the effects of model
architecture and training strategy, comparing Qwen2.5-0.5B-Instruct with other
models such as LLaMA and a prior NLLB-based system. These findings highlight
the promise of combining LLMs with external tools and the role of reinforcement
learning in improving translation quality in low-resource language settings.

</details>


### [21] [Rule Synergy Analysis using LLMs: State of the Art and Implications](https://arxiv.org/abs/2508.19484)
*Bahar Bateni,Benjamin Pratt,Jim Whitehead*

Main category: cs.CL

TL;DR: LLMs are tested on card game synergies. They're okay at identifying non-synergistic pairs, but bad at identifying positive and negative synergies.


<details>
  <summary>Details</summary>
Motivation: Investigate how well LLMs understand and reason about complex rule interactions in dynamic environments like card games.

Method: Introduce a dataset of card synergies from Slay the Spire, classifying card pairs based on their interactions.

Result: LLMs excel at identifying non-synergistic pairs but struggle with positive and negative synergies. Common errors include issues with timing, defining game states, and following game rules.

Conclusion: LLMs struggle with detecting positive and negative synergies, especially negative ones.

Abstract: Large language models (LLMs) have demonstrated strong performance across a
variety of domains, including logical reasoning, mathematics, and more. In this
paper, we investigate how well LLMs understand and reason about complex rule
interactions in dynamic environments, such as card games. We introduce a
dataset of card synergies from the game Slay the Spire, where pairs of cards
are classified based on their positive, negative, or neutral interactions. Our
evaluation shows that while LLMs excel at identifying non-synergistic pairs,
they struggle with detecting positive and, particularly, negative synergies. We
categorize common error types, including issues with timing, defining game
states, and following game rules. Our findings suggest directions for future
research to improve model performance in predicting the effect of rules and
their interactions.

</details>


### [22] [MovieCORE: COgnitive REasoning in Movies](https://arxiv.org/abs/2508.19026)
*Gueter Josmy Faure,Min-Hung Chen,Jia-Fong Yeh,Ying Cheng,Hung-Ting Su,Yung-Hao Tang,Shang-Hong Lai,Winston H. Hsu*

Main category: cs.CL

TL;DR: MovieCORE: A new VQA dataset for deeper movie understanding, featuring challenging questions and an agentic enhancement module (ACE) that improves VQA model reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing VQA datasets lack depth and don't engage System-2 thinking for movie content understanding.

Method: The paper introduces MovieCORE dataset using agentic brainstorming with LLMs and an agentic enhancement module (ACE) to improve model reasoning.

Result: The ACE module improves model reasoning capabilities by up to 25%.

Conclusion: This work advances movie understanding in AI and reveals limitations of current VQA models on challenging cinematic content questions.

Abstract: This paper introduces MovieCORE, a novel video question answering (VQA)
dataset designed to probe deeper cognitive understanding of movie content.
Unlike existing datasets that focus on surface-level comprehension, MovieCORE
emphasizes questions that engage System-2 thinking while remaining specific to
the video material. We present an innovative agentic brainstorming approach,
utilizing multiple large language models (LLMs) as thought agents to generate
and refine high-quality question-answer pairs. To evaluate dataset quality, we
develop a set of cognitive tests assessing depth, thought-provocation
potential, and syntactic complexity. We also propose a comprehensive evaluation
scheme for assessing VQA model performance on deeper cognitive tasks. To
address the limitations of existing video-language models (VLMs), we introduce
an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves
model reasoning capabilities post-training by up to 25%. Our work contributes
to advancing movie understanding in AI systems and provides valuable insights
into the capabilities and limitations of current VQA models when faced with
more challenging, nuanced questions about cinematic content. Our project page,
dataset and code can be found at
https://joslefaure.github.io/assets/html/moviecore.html.

</details>


### [23] [Blockwise SFT for Diffusion Language Models: Reconciling Bidirectional Attention and Autoregressive Decoding](https://arxiv.org/abs/2508.19529)
*Bowen Sun,Yujun Cai,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.CL

TL;DR: Blockwise SFT通过对齐训练和推理过程，提高了diffusion-based语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 标准的监督微调(SFT)与离散扩散语言模型的半自回归推理不一致：训练随机mask整个响应中的tokens，而推理则按顺序生成固定大小的块。

Method: 提出了Blockwise SFT，它将响应分成固定大小的块，并选择一个块进行随机掩码。

Result: 在GSM8K、MATH和MetaMathQA上进行的实验表明，在相同的计算或token预算下，Blockwise SFT比传统的SFT有持续的收益。

Conclusion: Blockwise SFT能提升diffusion-based语言模型的性能，因为它更忠实地对齐了训练和推理过程。

Abstract: Discrete diffusion language models have shown strong potential for text
generation, yet standard supervised fine-tuning (SFT) misaligns with their
semi-autoregressive inference: training randomly masks tokens across the entire
response, while inference generates fixed-size blocks sequentially. This
mismatch introduces noisy prefixes and leaky suffixes, biasing gradients away
from the desired blockwise likelihood. We propose Blockwise SFT, which
partitions responses into fixed-size blocks, selects one active block per step
for stochastic masking, freezes all preceding tokens, and fully hides future
ones. Loss is computed only over the active block, directly mirroring the
blockwise decoding process. Experiments on GSM8K, MATH, and MetaMathQA show
consistent gains over classical SFT under equal compute or token budgets. Block
size consistency studies and ablations confirm that improvements stem from
faithful training-inference alignment rather than incidental masking effects.
Our results highlight the importance of matching supervision granularity to the
decoding procedure in diffusion-based language models.

</details>


### [24] [Alignment with Fill-In-the-Middle for Enhancing Code Generation](https://arxiv.org/abs/2508.19532)
*Houxing Ren,Zimu Lu,Weikang Shi,Haotian Hou,Yunqiao Yang,Ke Wang,Aojun Zhou,Junting Pan,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: A new method using granular code splitting and AST with curriculum training improves code generation performance through enhanced DPO.


<details>
  <summary>Details</summary>
Motivation: Improving performance in code-related tasks remains challenging due to limited training data that is verifiable with accurate test cases. Existing methods for generating test cases still face limitations.

Method: Splits code snippets into smaller, granular blocks, uses Abstract Syntax Tree (AST) splitting, and applies a curriculum training method to enhance Direct Preference Optimization (DPO).

Result: Demonstrates significant improvements in code generation tasks, as validated by experiments on benchmark datasets such as HumanEval (+), MBPP (+), APPS, LiveCodeBench, and BigCodeBench.

Conclusion: This paper introduces a novel approach to improve code generation tasks by splitting code snippets into smaller, granular blocks and using AST splitting with curriculum training to enhance DPO training. The approach demonstrates significant improvements on benchmark datasets.

Abstract: The code generation capabilities of Large Language Models (LLMs) have
advanced applications like tool invocation and problem-solving. However,
improving performance in code-related tasks remains challenging due to limited
training data that is verifiable with accurate test cases. While Direct
Preference Optimization (DPO) has shown promise, existing methods for
generating test cases still face limitations. In this paper, we propose a novel
approach that splits code snippets into smaller, granular blocks, creating more
diverse DPO pairs from the same test cases. Additionally, we introduce the
Abstract Syntax Tree (AST) splitting and curriculum training method to enhance
the DPO training. Our approach demonstrates significant improvements in code
generation tasks, as validated by experiments on benchmark datasets such as
HumanEval (+), MBPP (+), APPS, LiveCodeBench, and BigCodeBench. Code and data
are available at https://github.com/SenseLLM/StructureCoder.

</details>


### [25] [Emotion Transfer with Enhanced Prototype for Unseen Emotion Recognition in Conversation](https://arxiv.org/abs/2508.19533)
*Kun Peng,Cong Cao,Hao Peng,Guanlin Wu,Zhifeng Hao,Lei Jiang,Yanbing Liu,Philip S. Yu*

Main category: cs.CL

TL;DR: Introduces the Unseen Emotion Recognition in Conversation (UERC) task and proposes ProEmoTrans, a prototype-based emotion transfer framework, to address the challenge of recognizing unseen emotions in real-world conversations.


<details>
  <summary>Details</summary>
Motivation: Current Emotion Recognition in Conversation (ERC) research follows a closed-domain assumption, which presents a challenge for models when it comes to recognizing previously unseen emotions in real-world applications due to the lack of clear consensus on emotion classification in psychology.

Method: ProEmoTrans, a prototype-based emotion transfer framework, with an LLM-enhanced description approach, a parameter-free mechanism for efficient encoding and overfitting prevention, and an improved Attention Viterbi Decoding (AVD) method.

Result: Extensive experiments on three datasets show the effectiveness of the proposed method.

Conclusion: The proposed method serves as a strong baseline for preliminary exploration in the new area of Unseen Emotion Recognition in Conversation (UERC).

Abstract: Current Emotion Recognition in Conversation (ERC) research follows a
closed-domain assumption. However, there is no clear consensus on emotion
classification in psychology, which presents a challenge for models when it
comes to recognizing previously unseen emotions in real-world applications. To
bridge this gap, we introduce the Unseen Emotion Recognition in Conversation
(UERC) task for the first time and propose ProEmoTrans, a solid prototype-based
emotion transfer framework. This prototype-based approach shows promise but
still faces key challenges: First, implicit expressions complicate emotion
definition, which we address by proposing an LLM-enhanced description approach.
Second, utterance encoding in long conversations is difficult, which we tackle
with a proposed parameter-free mechanism for efficient encoding and overfitting
prevention. Finally, the Markovian flow nature of emotions is hard to transfer,
which we address with an improved Attention Viterbi Decoding (AVD) method to
transfer seen emotion transitions to unseen emotions. Extensive experiments on
three datasets show that our method serves as a strong baseline for preliminary
exploration in this new area.

</details>


### [26] [Language Models Identify Ambiguities and Exploit Loopholes](https://arxiv.org/abs/2508.19546)
*Jio Choi,Mohit Bansal,Elias Stengel-Eskin*

Main category: cs.CL

TL;DR: llm可以识别并利用漏洞，构成潜在的AI安全风险。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型(llm)对漏洞的反应提供了一个双重机会。首先，它为我们提供了一个透镜，通过它可以检查llm中的歧义和语用学，因为利用漏洞需要识别歧义并执行复杂的语用推理。其次，漏洞提出了一个有趣而新颖的对齐问题，模型面临相互冲突的目标，并且可以利用歧义来实现自己的优势。

Method: 设计了llm具有目标和与目标冲突的模糊用户指令的场景，场景涵盖标量含义、结构歧义和权力动态。测量了不同模型利用漏洞来满足其给定目标（而不是用户目标）的能力。

Result: 封闭源模型和更强大的开源模型都可以识别歧义并利用由此产生的漏洞。分析表明，利用漏洞的模型可以明确识别并推理歧义和冲突目标。

Conclusion: 大型语言模型(llm)可以识别歧义并利用由此产生的漏洞，从而构成潜在的AI安全风险。能够利用漏洞的模型可以明确识别并推理歧义和冲突目标。

Abstract: Studying the responses of large language models (LLMs) to loopholes presents
a two-fold opportunity. First, it affords us a lens through which to examine
ambiguity and pragmatics in LLMs, since exploiting a loophole requires
identifying ambiguity and performing sophisticated pragmatic reasoning. Second,
loopholes pose an interesting and novel alignment problem where the model is
presented with conflicting goals and can exploit ambiguities to its own
advantage. To address these questions, we design scenarios where LLMs are given
a goal and an ambiguous user instruction in conflict with the goal, with
scenarios covering scalar implicature, structural ambiguities, and power
dynamics. We then measure different models' abilities to exploit loopholes to
satisfy their given goals as opposed to the goals of the user. We find that
both closed-source and stronger open-source models can identify ambiguities and
exploit their resulting loopholes, presenting a potential AI safety risk. Our
analysis indicates that models which exploit loopholes explicitly identify and
reason about both ambiguity and conflicting goals.

</details>


### [27] [Towards a Holistic and Automated Evaluation Framework for Multi-Level Comprehension of LLMs in Book-Length Contexts](https://arxiv.org/abs/2508.19578)
*Jiaqi Deng,Yuho Lee,Nicole Hee-Yeon Kim,Hyangsuk Min,Taewon Yun,Minjeong Ban,Kim Yul,Hwanjun Song*

Main category: cs.CL

TL;DR: HAMLET: a framework for evaluating long-context comprehension of LLMs. It reveals that LLMs struggle with fine-grained comprehension and are sensitive to positional effects.


<details>
  <summary>Details</summary>
Motivation: evaluate the long-context comprehension of large language models (LLMs)

Method: a holistic and automated framework for evaluating the long-context comprehension of large language models (LLMs). HAMLET structures source texts into a three-level key-fact hierarchy at root-, branch-, and leaf-levels, and employs query-focused summarization to evaluate how well models recall and faithfully represent information at each level.

Result: automatic evaluation achieves over 90% agreement with expert human judgments, while reducing the cost by up to 25 times.

Conclusion: LLMs struggle with fine-grained comprehension, especially at the leaf level, and are sensitive to positional effects like the lost-in-the-middle. Analytical queries pose greater challenges than narrative ones, and consistent performance gaps emerge between open-source and proprietary models, as well as across model scales.

Abstract: We introduce HAMLET, a holistic and automated framework for evaluating the
long-context comprehension of large language models (LLMs). HAMLET structures
source texts into a three-level key-fact hierarchy at root-, branch-, and
leaf-levels, and employs query-focused summarization to evaluate how well
models recall and faithfully represent information at each level. To validate
the reliability of our fully automated pipeline, we conduct a systematic human
study, showing that our automatic evaluation achieves over 90% agreement with
expert human judgments, while reducing the cost by up to 25 times. HAMLET
reveals that LLMs struggle with fine-grained comprehension, especially at the
leaf level, and are sensitive to positional effects like the
lost-in-the-middle. Analytical queries pose greater challenges than narrative
ones, and consistent performance gaps emerge between open-source and
proprietary models, as well as across model scales. Our code and dataset are
publicly available at https://github.com/DISL-Lab/HAMLET.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [28] [Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration](https://arxiv.org/abs/2508.19254)
*Jookyung Song,Mookyoung Kang,Nojun Kwak*

Main category: cs.CV

TL;DR: This paper introduces a real-time generative drawing system that combines formal and contextual intent for co-creative human-AI interaction.


<details>
  <summary>Details</summary>
Motivation: Unlike conventional text-prompt-based generative systems, which primarily capture high-level contextual descriptions, our approach simultaneously analyzes ground-level intuitive geometric features such as line trajectories, proportions, and spatial arrangement, and high-level semantic cues extracted via vision-language models.

Method: This paper presents a real-time generative drawing system that interprets and integrates both formal intent and contextual intent into a unified transformation process.

Result: Implemented with a touchscreen-based interface and distributed inference architecture, the system achieves low-latency, two-stage transformation while supporting multi-user collaboration on shared canvases.

Conclusion: The resulting platform enables participants, regardless of artistic expertise, to engage in synchronous, co-authored visual creation, redefining human-AI interaction as a process of co-creation and mutual enhancement.

Abstract: This paper presents a real-time generative drawing system that interprets and
integrates both formal intent - the structural, compositional, and stylistic
attributes of a sketch - and contextual intent - the semantic and thematic
meaning inferred from its visual content - into a unified transformation
process. Unlike conventional text-prompt-based generative systems, which
primarily capture high-level contextual descriptions, our approach
simultaneously analyzes ground-level intuitive geometric features such as line
trajectories, proportions, and spatial arrangement, and high-level semantic
cues extracted via vision-language models. These dual intent signals are
jointly conditioned in a multi-stage generation pipeline that combines
contour-preserving structural control with style- and content-aware image
synthesis. Implemented with a touchscreen-based interface and distributed
inference architecture, the system achieves low-latency, two-stage
transformation while supporting multi-user collaboration on shared canvases.
The resulting platform enables participants, regardless of artistic expertise,
to engage in synchronous, co-authored visual creation, redefining human-AI
interaction as a process of co-creation and mutual enhancement.

</details>


### [29] [TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models](https://arxiv.org/abs/2508.19257)
*Chenghao Liu,Jiachen Zhang,Chengxuan Li,Zhimu Zhou,Shixin Wu,Songfang Huang,Huiling Duan*

Main category: cs.CV

TL;DR: 提出了一种名为时间令牌融合(TTF)的免训练方法，用于增强VLA推理质量，通过整合历史和当前的视觉表征来实现。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作(VLA)模型在每个时间步独立处理视觉输入，忽略了机器人操作任务中固有的宝贵时间信息。这种逐帧处理使得模型容易受到视觉噪声的影响，同时忽略了操作序列中连续帧之间的实质性连贯性。

Method: 提出了一种名为时间令牌融合(TTF)的免训练方法，该方法通过硬融合策略和关键帧锚定来实现选择性时间令牌融合。

Result: 在LIBERO上平均提高了4.0个百分点（72.4％ vs 68.4％基线），在SimplerEnv上进行了交叉环境验证（相对提高了4.8％），在真实机器人任务中相对提高了8.7％。

Conclusion: TTF在LIBERO、SimplerEnv和真实机器人任务中表现出持续的改进，并且与模型无关，在OpenVLA和VLA-Cache架构中均有效。选择性Query矩阵重用可以提高性能。

Abstract: Vision-Language-Action (VLA) models process visual inputs independently at
each timestep, discarding valuable temporal information inherent in robotic
manipulation tasks. This frame-by-frame processing makes models vulnerable to
visual noise while ignoring the substantial coherence between consecutive
frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a
training-free approach that intelligently integrates historical and current
visual representations to enhance VLA inference quality. Our method employs
dual-dimension detection combining efficient grayscale pixel difference
analysis with attention-based semantic relevance assessment, enabling selective
temporal token fusion through hard fusion strategies and keyframe anchoring to
prevent error accumulation. Comprehensive experiments across LIBERO,
SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0
percentage points average on LIBERO (72.4\% vs 68.4\% baseline),
cross-environment validation on SimplerEnv (4.8\% relative improvement), and
8.7\% relative improvement on real robot tasks. Our approach proves
model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,
TTF reveals that selective Query matrix reuse in attention mechanisms enhances
rather than compromises performance, suggesting promising directions for direct
KQV matrix reuse strategies that achieve computational acceleration while
improving task success rates.

</details>


### [30] [Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation](https://arxiv.org/abs/2508.19289)
*Tai Inui,Steven Oh,Magdeline Kuan*

Main category: cs.CV

TL;DR: 本研究提出了一种新的幻灯片质量评估方法，该方法结合了视觉设计指标和多模态嵌入，能够有效地评估幻灯片质量并提供实时反馈。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏可扩展、客观的幻灯片质量评估方法。

Method: 该研究提出了一种无监督的幻灯片质量评估流程，该流程结合了七个专家启发的视觉设计指标（空白、色彩鲜艳度、边缘密度、亮度对比度、文本密度、色彩和谐、布局平衡）与CLIP-ViT嵌入，使用基于隔离森林的异常评分来评估演示幻灯片。

Result: 该方法在六个学术讲座（115张幻灯片）上进行了评估，与人工视觉质量评级的皮尔逊相关系数高达0.83，比领先的视觉语言模型（ChatGPT o4-mini-high，ChatGPT o3，Claude Sonnet 4，Gemini 2.5 Pro）的分数高1.79倍至3.23倍。该研究证明了与视觉评级之间的收敛效度、与演讲者表达分数之间的区分效度以及与整体印象的探索性一致性。

Conclusion: 该研究表明，结合底层设计线索和多模态嵌入可以很好地模拟观众对幻灯片质量的感知，从而实现可扩展的、客观的实时反馈。

Abstract: We present an unsupervised slide-quality assessment pipeline that combines
seven expert-inspired visual-design metrics (whitespace, colorfulness, edge
density, brightness contrast, text density, color harmony, layout balance) with
CLIP-ViT embeddings, using Isolation Forest-based anomaly scoring to evaluate
presentation slides. Trained on 12k professional lecture slides and evaluated
on six academic talks (115 slides), our method achieved Pearson correlations up
to 0.83 with human visual-quality ratings-1.79x to 3.23x stronger than scores
from leading vision-language models (ChatGPT o4-mini-high, ChatGPT o3, Claude
Sonnet 4, Gemini 2.5 Pro). We demonstrate convergent validity with visual
ratings, discriminant validity against speaker-delivery scores, and exploratory
alignment with overall impressions. Our results show that augmenting low-level
design cues with multimodal embeddings closely approximates audience
perceptions of slide quality, enabling scalable, objective feedback in real
time.

</details>


### [31] [Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation](https://arxiv.org/abs/2508.19290)
*Alexandros Gkillas,Ioulia Kapsali,Nikos Piperigkos,Aris S. Lalos*

Main category: cs.CV

TL;DR: This paper presents a lightweight and effective defense mechanism against adversarial attacks in 2D LiDAR segmentation, suitable for real-world autonomous driving applications.


<details>
  <summary>Details</summary>
Motivation: Existing defenses for LiDAR segmentation networks are computationally intensive and not tailored for efficient 2D range view representations, which are widely used in state-of-the-art pipelines. This paper aims to address the lack of lightweight adversarial defenses in this domain.

Method: The paper proposes a direct attack formulation in the range-view domain and develops an explainable purification network based on a mathematically justified optimization problem.

Result: The proposed method achieves competitive performance on open benchmarks, outperforming generative and adversarial training baselines. Real-world deployment on a demo vehicle demonstrates its ability to deliver accurate operation in practical autonomous driving scenarios.

Conclusion: This paper introduces a purification framework and demonstrates its effectiveness in defending against adversarial attacks in 2D range-view LiDAR segmentation, showing its potential for real-world autonomous driving scenarios.

Abstract: LiDAR-based segmentation is essential for reliable perception in autonomous
vehicles, yet modern segmentation networks are highly susceptible to
adversarial attacks that can compromise safety. Most existing defenses are
designed for networks operating directly on raw 3D point clouds and rely on
large, computationally intensive generative models. However, many
state-of-the-art LiDAR segmentation pipelines operate on more efficient 2D
range view representations. Despite their widespread adoption, dedicated
lightweight adversarial defenses for this domain remain largely unexplored. We
introduce an efficient model-based purification framework tailored for
adversarial defense in 2D range-view LiDAR segmentation. We propose a direct
attack formulation in the range-view domain and develop an explainable
purification network based on a mathematical justified optimization problem,
achieving strong adversarial resilience with minimal computational overhead.
Our method achieves competitive performance on open benchmarks, consistently
outperforming generative and adversarial training baselines. More importantly,
real-world deployment on a demo vehicle demonstrates the framework's ability to
deliver accurate operation in practical autonomous driving scenarios.

</details>


### [32] [Object Detection with Multimodal Large Vision-Language Models: An In-depth Review](https://arxiv.org/abs/2508.19294)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

TL;DR: This review explores the state-of-the-art in LVLMs for object detection, highlighting their architectural innovations, training paradigms, and output flexibility. It also identifies limitations and proposes solutions for future advancement.


<details>
  <summary>Details</summary>
Motivation: enhance adaptability, contextual reasoning, and generalization beyond traditional architectures in deep learning-based object detection

Method: a three-step research review process

Result: LVLMs will soon meet or surpass the performance of conventional methods in object detection

Conclusion: LVLMs have made and will continue to make a transformative impact on object detection and robotic applications in the future.

Abstract: The fusion of language and vision in large vision-language models (LVLMs) has
revolutionized deep learning-based object detection by enhancing adaptability,
contextual reasoning, and generalization beyond traditional architectures. This
in-depth review presents a structured exploration of the state-of-the-art in
LVLMs, systematically organized through a three-step research review process.
First, we discuss the functioning of vision language models (VLMs) for object
detection, describing how these models harness natural language processing
(NLP) and computer vision (CV) techniques to revolutionize object detection and
localization. We then explain the architectural innovations, training
paradigms, and output flexibility of recent LVLMs for object detection,
highlighting how they achieve advanced contextual understanding for object
detection. The review thoroughly examines the approaches used in integration of
visual and textual information, demonstrating the progress made in object
detection using VLMs that facilitate more sophisticated object detection and
localization strategies. This review presents comprehensive visualizations
demonstrating LVLMs' effectiveness in diverse scenarios including localization
and segmentation, and then compares their real-time performance, adaptability,
and complexity to traditional deep learning systems. Based on the review, its
is expected that LVLMs will soon meet or surpass the performance of
conventional methods in object detection. The review also identifies a few
major limitations of the current LVLM modes, proposes solutions to address
those challenges, and presents a clear roadmap for the future advancement in
this field. We conclude, based on this study, that the recent advancement in
LVLMs have made and will continue to make a transformative impact on object
detection and robotic applications in the future.

</details>


### [33] [Large VLM-based Stylized Sports Captioning](https://arxiv.org/abs/2508.19295)
*Sauptik Dhar,Nicholas Buoncristiani,Joe Anakata,Haoyu Zhang,Michelle Munson*

Main category: cs.CV

TL;DR: Existing LLM/LVLMs are not good at sports captioning. This paper proposes a new pipeline that improves accuracy and speed, and was tested during Super Bowl LIX.


<details>
  <summary>Details</summary>
Motivation: Existing LLM/LVLMs lack sufficient domain-centric sports' jargon for accurate identification and natural language description of game play, hindering their ability to generate production-grade sports captions.

Method: A two-level fine-tuned LVLM pipeline.

Result: The proposed pipeline yields an improvement > 8-10% in the F1, and > 2-10% in BERT score compared to alternative approaches. It generates captions at a rate of 6 images per 3-5 seconds.

Conclusion: The proposed two-level fine-tuned LVLM pipeline addresses the limitations of existing SoTA LLM/LVLMs in generating production-grade sports captions, demonstrating improved F1 and BERT scores, small memory footprint, and fast execution time. It was successfully applied during Super Bowl LIX.

Abstract: The advent of large (visual) language models (LLM / LVLM) have led to a
deluge of automated human-like systems in several domains including social
media content generation, search and recommendation, healthcare prognosis, AI
assistants for cognitive tasks etc. Although these systems have been
successfully integrated in production; very little focus has been placed on
sports, particularly accurate identification and natural language description
of the game play. Most existing LLM/LVLMs can explain generic sports
activities, but lack sufficient domain-centric sports' jargon to create natural
(human-like) descriptions. This work highlights the limitations of existing
SoTA LLM/LVLMs for generating production-grade sports captions from images in a
desired stylized format, and proposes a two-level fine-tuned LVLM pipeline to
address that. The proposed pipeline yields an improvement > 8-10% in the F1,
and > 2-10% in BERT score compared to alternative approaches. In addition, it
has a small runtime memory footprint and fast execution time. During Super Bowl
LIX the pipeline proved its practical application for live professional sports
journalism; generating highly accurate and stylized captions at the rate of 6
images per 3-5 seconds for over 1000 images during the game play.

</details>


### [34] [DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models](https://arxiv.org/abs/2508.19298)
*Abu Sufian,Anirudha Ghosh,Debaditya Barman,Marco Leo,Cosimo Distante*

Main category: cs.CV

TL;DR: DemoBias研究了LVLMs在生物特征人脸识别中存在的人口统计偏差。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型(LVLMs)在包括带有描述的生物特征人脸识别(FR)等各种下游任务中表现出了卓越的能力。然而，人口统计偏差仍然是FR中的一个关键问题，因为这些基础模型通常无法在不同的族裔/种族、性别和年龄等人口统计群体中公平地执行。

Method: 在自生成的人口统计平衡数据集上，对LLaVA, BLIP-2, 和PaliGemma这三个预训练LVLMs进行了微调和评估。使用特定群体的BERTScores和Fairness Discrepancy Rate等评估指标来量化和追踪性能差异。

Result: 实验结果揭示了LVLMs在不同人口统计群体中的公平性和可靠性。

Conclusion: LVLMs存在人口统计偏差，PaliGemma和LLaVA在Hispanic/Latino, Caucasian, 和South Asian群体中表现出更高的差异，而BLIP-2表现相对稳定。

Abstract: Large Vision Language Models (LVLMs) have demonstrated remarkable
capabilities across various downstream tasks, including biometric face
recognition (FR) with description. However, demographic biases remain a
critical concern in FR, as these foundation models often fail to perform
equitably across diverse demographic groups, considering ethnicity/race,
gender, and age. Therefore, through our work DemoBias, we conduct an empirical
evaluation to investigate the extent of demographic biases in LVLMs for
biometric FR with textual token generation tasks. We fine-tuned and evaluated
three widely used pre-trained LVLMs: LLaVA, BLIP-2, and PaliGemma on our own
generated demographic-balanced dataset. We utilize several evaluation metrics,
like group-specific BERTScores and the Fairness Discrepancy Rate, to quantify
and trace the performance disparities. The experimental results deliver
compelling insights into the fairness and reliability of LVLMs across diverse
demographic groups. Our empirical study uncovered demographic biases in LVLMs,
with PaliGemma and LLaVA exhibiting higher disparities for Hispanic/Latino,
Caucasian, and South Asian groups, whereas BLIP-2 demonstrated comparably
consistent. Repository: https://github.com/Sufianlab/DemoBias.

</details>


### [35] [Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities](https://arxiv.org/abs/2508.19305)
*Chen Chu,Cyrus Shahabi*

Main category: cs.CV

TL;DR: Geo2Vec 是一种新的空间表示学习方法，它通过自适应采样点并编码有符号距离来直接在原始空间中运行，从而为所有地理实体类型生成紧凑的、具有几何意识的统一表示。


<details>
  <summary>Details</summary>
Motivation: 现有的空间表示学习方法要么针对单一地理实体类型，要么像 Poly2Vec 一样，将实体分解为更简单的组件以实现傅里叶变换，从而导致计算成本高昂。此外，由于变换后的空间缺乏几何对齐，这些方法依赖于均匀的、非自适应的采样，这会模糊边缘和边界等细粒度特征。

Method: Geo2Vec，一种受有符号距离场 (SDF) 启发的新方法，它直接在原始空间中运行。Geo2Vec 自适应地对点进行采样并编码它们的有符号距离（外部为正，内部为负），无需分解即可捕获几何形状。训练用于逼近 SDF 的神经网络为所有地理实体类型生成紧凑的、具有几何意识的统一表示。

Result: 经验结果表明，Geo2Vec 在表示形状和位置、捕获拓扑和距离关系方面始终优于现有方法，并在实际 GeoAI 应用中实现了更高的效率。

Conclusion: Geo2Vec在表示形状和位置、捕获拓扑和距离关系方面始终优于现有方法，并在实际GeoAI应用中实现了更高的效率。

Abstract: Spatial representation learning is essential for GeoAI applications such as
urban analytics, enabling the encoding of shapes, locations, and spatial
relationships (topological and distance-based) of geo-entities like points,
polylines, and polygons. Existing methods either target a single geo-entity
type or, like Poly2Vec, decompose entities into simpler components to enable
Fourier transformation, introducing high computational cost. Moreover, since
the transformed space lacks geometric alignment, these methods rely on uniform,
non-adaptive sampling, which blurs fine-grained features like edges and
boundaries. To address these limitations, we introduce Geo2Vec, a novel method
inspired by signed distance fields (SDF) that operates directly in the original
space. Geo2Vec adaptively samples points and encodes their signed distances
(positive outside, negative inside), capturing geometry without decomposition.
A neural network trained to approximate the SDF produces compact,
geometry-aware, and unified representations for all geo-entity types.
Additionally, we propose a rotation-invariant positional encoding to model
high-frequency spatial variations and construct a structured and robust
embedding space for downstream GeoAI models. Empirical results show that
Geo2Vec consistently outperforms existing methods in representing shape and
location, capturing topological and distance relationships, and achieving
greater efficiency in real-world GeoAI applications. Code and Data can be found
at: https://github.com/chuchen2017/GeoNeuralRepresentation.

</details>


### [36] [Advancements in Crop Analysis through Deep Learning and Explainable AI](https://arxiv.org/abs/2508.19307)
*Hamza Khan*

Main category: cs.CV

TL;DR: This study uses deep learning and explainable AI to automate rice grain classification and disease diagnosis, improving efficiency and transparency in agricultural practices.


<details>
  <summary>Details</summary>
Motivation: Manual inspection of rice crops is labour intensive, time consuming, and error prone, necessitating automated solutions for quality control and yield improvement.

Method: Convolutional Neural Networks (CNN) were used to classify five rice grain varieties. Additionally, CNN, VGG16, ResNet50, and MobileNetV2 models were combined with explainable AI techniques (SHAP and LIME) for rice leaf disease diagnosis.

Result: High classification accuracy was achieved in distinguishing rice varieties, with minimal misclassifications. An accurate diagnostic method for rice leaf diseases was also developed.

Conclusion: Deep learning models, combined with explainable AI, show strong potential for automated crop quality inspection and disease diagnosis, benefiting farmers, consumers, and the agricultural economy.

Abstract: Rice is a staple food of global importance in terms of trade, nutrition, and
economic growth. Among Asian nations such as China, India, Pakistan, Thailand,
Vietnam and Indonesia are leading producers of both long and short grain
varieties, including basmati, jasmine, arborio, ipsala, and kainat saila. To
ensure consumer satisfaction and strengthen national reputations, monitoring
rice crops and grain quality is essential. Manual inspection, however, is
labour intensive, time consuming and error prone, highlighting the need for
automated solutions for quality control and yield improvement. This study
proposes an automated approach to classify five rice grain varieties using
Convolutional Neural Networks (CNN). A publicly available dataset of 75000
images was used for training and testing. Model evaluation employed accuracy,
recall, precision, F1-score, ROC curves, and confusion matrices. Results
demonstrated high classification accuracy with minimal misclassifications,
confirming the model effectiveness in distinguishing rice varieties. In
addition, an accurate diagnostic method for rice leaf diseases such as Brown
Spot, Blast, Bacterial Blight, and Tungro was developed. The framework combined
explainable artificial intelligence (XAI) with deep learning models including
CNN, VGG16, ResNet50, and MobileNetV2. Explainability techniques such as SHAP
(SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic
Explanations) revealed how specific grain and leaf features influenced
predictions, enhancing model transparency and reliability. The findings
demonstrate the strong potential of deep learning in agricultural applications,
paving the way for robust, interpretable systems that can support automated
crop quality inspection and disease diagnosis, ultimately benefiting farmers,
consumers, and the agricultural economy.

</details>


### [37] [Sistema de Reconocimiento Facial Federado en Conjuntos Abiertos basado en OpenMax](https://arxiv.org/abs/2508.19312)
*Ander Galván,Marivi Higuero,Jorge Sasiain,Eduardo Jacob*

Main category: cs.CV

TL;DR: This paper presents a federated learning facial recognition system using OpenMax to distinguish between known and unknown subjects, enhancing privacy and robustness.


<details>
  <summary>Details</summary>
Motivation: Facial recognition faces significant challenges regarding privacy and identity management, particularly when unknown individuals appear in the operational context.

Method: The proposed approach integrates the OpenMax algorithm into federated learning, leveraging the exchange of mean activation vectors and local distance measures to reliably distinguish between known and unknown subjects.

Result: The system can reliably distinguish between known and unknown subjects.

Conclusion: The experimental results validate the effectiveness of the proposed solution, demonstrating its potential for enhancing privacy-aware and robust facial recognition in distributed environments.

Abstract: Facial recognition powered by Artificial Intelligence has achieved high
accuracy in specific scenarios and applications. Nevertheless, it faces
significant challenges regarding privacy and identity management, particularly
when unknown individuals appear in the operational context. This paper presents
the design, implementation, and evaluation of a facial recognition system
within a federated learning framework tailored to open-set scenarios. The
proposed approach integrates the OpenMax algorithm into federated learning,
leveraging the exchange of mean activation vectors and local distance measures
to reliably distinguish between known and unknown subjects. Experimental
results validate the effectiveness of the proposed solution, demonstrating its
potential for enhancing privacy-aware and robust facial recognition in
distributed environments.
  --
  El reconocimiento facial impulsado por Inteligencia Artificial ha demostrado
una alta precisi\'on en algunos escenarios y aplicaciones. Sin embargo,
presenta desaf\'ios relacionados con la privacidad y la identificaci\'on de
personas, especialmente considerando que pueden aparecer sujetos desconocidos
para el sistema que lo implementa. En este trabajo, se propone el dise\~no,
implementaci\'on y evaluaci\'on de un sistema de reconocimiento facial en un
escenario de aprendizaje federado, orientado a conjuntos abiertos.
Concretamente, se dise\~na una soluci\'on basada en el algoritmo OpenMax para
escenarios de aprendizaje federado. La propuesta emplea el intercambio de los
vectores de activaci\'on promedio y distancias locales para identificar de
manera eficaz tanto personas conocidas como desconocidas. Los experimentos
realizados demuestran la implementaci\'on efectiva de la soluci\'on propuesta.

</details>


### [38] [Automated classification of natural habitats using ground-level imagery](https://arxiv.org/abs/2508.19314)
*Mahdis Tourian,Sareh Rowlands,Remy Vandaele,Max Fancourt,Rebecca Mein,Hywel T. P. Williams*

Main category: cs.CV

TL;DR: This paper presents a deep learning method for classifying habitats from ground-level photos, achieving promising results and offering a web application for practical use.


<details>
  <summary>Details</summary>
Motivation: Accurate classification of terrestrial habitats is critical for various applications, and this paper aims to improve validation and scalability by using ground-level imagery instead of satellite imagery.

Method: A DeepLabV3-ResNet101 classifier was developed and fine-tuned to classify ground-level habitat photographs into 18 classes, using pre-processing techniques like resizing, normalization, and augmentation, and employing five-fold cross-validation.

Result: The model achieved a mean F1-score of 0.61 across 18 habitat classes, with some visually distinct habitats reaching values above 0.90. A web application was also provided to classify uploaded images.

Conclusion: This study demonstrates the potential of using ground-level imagery and deep learning for ecological monitoring and habitat classification, achieving a mean F1-score of 0.61 across 18 habitat classes.

Abstract: Accurate classification of terrestrial habitats is critical for biodiversity
conservation, ecological monitoring, and land-use planning. Several habitat
classification schemes are in use, typically based on analysis of satellite
imagery with validation by field ecologists. Here we present a methodology for
classification of habitats based solely on ground-level imagery (photographs),
offering improved validation and the ability to classify habitats at scale (for
example using citizen-science imagery). In collaboration with Natural England,
a public sector organisation responsible for nature conservation in England,
this study develops a classification system that applies deep learning to
ground-level habitat photographs, categorising each image into one of 18
classes defined by the 'Living England' framework. Images were pre-processed
using resizing, normalisation, and augmentation; re-sampling was used to
balance classes in the training data and enhance model robustness. We developed
and fine-tuned a DeepLabV3-ResNet101 classifier to assign a habitat class label
to each photograph. Using five-fold cross-validation, the model demonstrated
strong overall performance across 18 habitat classes, with accuracy and
F1-scores varying between classes. Across all folds, the model achieved a mean
F1-score of 0.61, with visually distinct habitats such as Bare Soil, Silt and
Peat (BSSP) and Bare Sand (BS) reaching values above 0.90, and mixed or
ambiguous classes scoring lower. These findings demonstrate the potential of
this approach for ecological monitoring. Ground-level imagery is readily
obtained, and accurate computational methods for habitat classification based
on such data have many potential applications. To support use by practitioners,
we also provide a simple web application that classifies uploaded images using
our model.

</details>


### [39] [MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation](https://arxiv.org/abs/2508.19320)
*Ming Chen,Liyuan Cui,Wenyuan Zhang,Haoxian Zhang,Yan Zhou,Xiaohan Li,Xiaoqiang Liu,Pengfei Wan*

Main category: cs.CV

TL;DR: This paper introduces an autoregressive video generation framework for interactive multimodal control with low latency, using a modified LLM, a large dialogue dataset, and a deep compression autoencoder.


<details>
  <summary>Details</summary>
Motivation: Existing interactive digital human video generation methods struggle with high latency, heavy computational cost, and limited controllability.

Method: An autoregressive video generation framework with minimal modifications to a standard large language model (LLM) is introduced. It accepts multimodal condition encodings (audio, pose, and text) and outputs representations to guide a diffusion head. A deep compression autoencoder is used for long-horizon inference.

Result: The framework enables interactive multimodal control and low-latency extrapolation in a streaming manner. A large-scale dialogue dataset of approximately 20,000 hours was constructed. A deep compression autoencoder with up to 64x reduction ratio was introduced.

Conclusion: The proposed autoregressive video generation framework achieves low latency, high efficiency, and fine-grained multimodal controllability, as demonstrated through experiments on duplex conversation, multilingual human synthesis, and interactive world model.

Abstract: Recently, interactive digital human video generation has attracted widespread
attention and achieved remarkable progress. However, building such a practical
system that can interact with diverse input signals in real time remains
challenging to existing methods, which often struggle with high latency, heavy
computational cost, and limited controllability. In this work, we introduce an
autoregressive video generation framework that enables interactive multimodal
control and low-latency extrapolation in a streaming manner. With minimal
modifications to a standard large language model (LLM), our framework accepts
multimodal condition encodings including audio, pose, and text, and outputs
spatially and semantically coherent representations to guide the denoising
process of a diffusion head. To support this, we construct a large-scale
dialogue dataset of approximately 20,000 hours from multiple sources, providing
rich conversational scenarios for training. We further introduce a deep
compression autoencoder with up to 64$\times$ reduction ratio, which
effectively alleviates the long-horizon inference burden of the autoregressive
model. Extensive experiments on duplex conversation, multilingual human
synthesis, and interactive world model highlight the advantages of our approach
in low latency, high efficiency, and fine-grained multimodal controllability.

</details>


### [40] [Deep Data Hiding for ICAO-Compliant Face Images: A Survey](https://arxiv.org/abs/2508.19324)
*Jefferson David Rodriguez Chivata,Davide Ghiani,Simone Maurizio La Cava,Marco Micheletto,Giulia Orrù,Federico Lama,Gian Luca Marcialis*

Main category: cs.CV

TL;DR: 本调查论文探讨了数字水印和隐写术作为 ICAO 兼容图像的互补解决方案，以实现持久验证，并分析了现有技术的优缺点，为安全部署提供指导。


<details>
  <summary>Details</summary>
Motivation: 符合 ICAO 标准的面部图像越来越多地成为身份验证的核心，但也促进了诸如图像变形和深度伪造等行为，这些行为可能被利用于身份盗窃和非法共享身份证明文件等有害目的。传统的对策（如 PAD）仅限于实时捕获，不提供捕获后保护。

Method: 对最先进的技术进行了全面的分析，以评估底层方法在涉及 ICAO 兼容图像的应用中的潜力和缺点，以及它们在标准约束下的适用性。

Result: 强调了关键的权衡，为在实际身份系统中安全部署提供指导。

Conclusion: 数字水印和隐写术可以作为互补解决方案，直接将防篡改信号嵌入到图像中，从而实现持久验证，且不影响 ICAO 兼容性。

Abstract: ICAO-compliant facial images, initially designed for secure biometric
passports, are increasingly becoming central to identity verification in a wide
range of application contexts, including border control, digital travel
credentials, and financial services. While their standardization enables global
interoperability, it also facilitates practices such as morphing and deepfakes,
which can be exploited for harmful purposes like identity theft and illegal
sharing of identity documents. Traditional countermeasures like Presentation
Attack Detection (PAD) are limited to real-time capture and offer no
post-capture protection. This survey paper investigates digital watermarking
and steganography as complementary solutions that embed tamper-evident signals
directly into the image, enabling persistent verification without compromising
ICAO compliance. We provide the first comprehensive analysis of
state-of-the-art techniques to evaluate the potential and drawbacks of the
underlying approaches concerning the applications involving ICAO-compliant
images and their suitability under standard constraints. We highlight key
trade-offs, offering guidance for secure deployment in real-world identity
systems.

</details>


### [41] [PRISM: A Framework Harnessing Unsupervised Visual Representations and Textual Prompts for Explainable MACE Survival Prediction from Cardiac Cine MRI](https://arxiv.org/abs/2508.19325)
*Haoyang Su,Jin-Yi Xiang,Shaohao Rui,Yifan Gao,Xingyu Chen,Tingxuan Yin,Xiaosong Wang,Lian-Ming Wu*

Main category: cs.CV

TL;DR: PRISM是一种新的心血管风险预测框架，它整合了影像和EHR数据，优于现有模型，并揭示了新的风险因素。


<details>
  <summary>Details</summary>
Motivation: 准确预测主要不良心脏事件(MACE)仍然是心血管预后中的一个核心挑战。

Method: PRISM是一种自监督框架，它整合了非对比心脏电影磁共振成像的视觉表征和结构化电子健康记录(EHR)，用于生存分析。它通过运动感知多视图蒸馏提取时间同步的影像特征，并使用医学文本提示来调节它们，以实现细粒度的风险预测。

Result: PRISM在四个独立的临床队列中，在内部和外部验证下，始终优于经典生存预测模型和最先进的(SOTA)深度学习基线。发现了三个与MACE风险相关的独特影像特征，包括侧壁不同步、下壁过敏和舒张期前部焦点升高。提示引导的归因进一步确定高血压、糖尿病和吸烟是临床和生理EHR因素中的主要贡献者。

Conclusion: PRISM通过整合影像和EHR信息，为心血管风险预测提供了有价值的见解，揭示了与MACE风险相关的影像特征和EHR因素。

Abstract: Accurate prediction of major adverse cardiac events (MACE) remains a central
challenge in cardiovascular prognosis. We present PRISM (Prompt-guided
Representation Integration for Survival Modeling), a self-supervised framework
that integrates visual representations from non-contrast cardiac cine magnetic
resonance imaging with structured electronic health records (EHRs) for survival
analysis. PRISM extracts temporally synchronized imaging features through
motion-aware multi-view distillation and modulates them using medically
informed textual prompts to enable fine-grained risk prediction. Across four
independent clinical cohorts, PRISM consistently surpasses classical survival
prediction models and state-of-the-art (SOTA) deep learning baselines under
internal and external validation. Further clinical findings demonstrate that
the combined imaging and EHR representations derived from PRISM provide
valuable insights into cardiac risk across diverse cohorts. Three distinct
imaging signatures associated with elevated MACE risk are uncovered, including
lateral wall dyssynchrony, inferior wall hypersensitivity, and anterior
elevated focus during diastole. Prompt-guided attribution further identifies
hypertension, diabetes, and smoking as dominant contributors among clinical and
physiological EHR factors.

</details>


### [42] [EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.19349)
*Mahdieh Behjat Khatooni,Mohsen Soryani*

Main category: cs.CV

TL;DR: This paper introduces EffNetViTLoRA, a CNN-ViT model with LoRA, for improved AD diagnosis using the full ADNI MRI dataset, achieving high accuracy in classifying AD, MCI, and CN.


<details>
  <summary>Details</summary>
Motivation: Early diagnosis of Alzheimer's Disease (AD) is crucial, and Mild Cognitive Impairment (MCI) is a challenging transitional phase to diagnose. Existing studies rely on limited data subsets, and fine-tuning large pretrained models can yield suboptimal results.

Method: The study proposes EffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole ADNI MRI dataset. The model integrates a CNN with a ViT and incorporates LoRA to adapt the pretrained ViT model.

Result: The EffNetViTLoRA model demonstrates enhanced clinical reliability and achieves a classification accuracy of 92.52% and an F1-score of 92.76% across three diagnostic categories.

Conclusion: The EffNetViTLoRA model achieves a classification accuracy of 92.52% and an F1-score of 92.76% across three diagnostic categories: AD, MCI, and CN for full ADNI dataset.

Abstract: Alzheimer's disease (AD) is one of the most prevalent neurodegenerative
disorders worldwide. As it progresses, it leads to the deterioration of
cognitive functions. Since AD is irreversible, early diagnosis is crucial for
managing its progression. Mild Cognitive Impairment (MCI) represents an
intermediate stage between Cognitively Normal (CN) individuals and those with
AD, and is considered a transitional phase from normal cognition to Alzheimer's
disease. Diagnosing MCI is particularly challenging due to the subtle
differences between adjacent diagnostic categories. In this study, we propose
EffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole
Alzheimer's Disease Neuroimaging Initiative (ADNI) Magnetic Resonance Imaging
(MRI) dataset. Our model integrates a Convolutional Neural Network (CNN) with a
Vision Transformer (ViT) to capture both local and global features from MRI
images. Unlike previous studies that rely on limited subsets of data, our
approach is trained on the full T1-weighted MRI dataset from ADNI, resulting in
a more robust and unbiased model. This comprehensive methodology enhances the
model's clinical reliability. Furthermore, fine-tuning large pretrained models
often yields suboptimal results when source and target dataset domains differ.
To address this, we incorporate Low-Rank Adaptation (LoRA) to effectively adapt
the pretrained ViT model to our target domain. This method enables efficient
knowledge transfer and reduces the risk of overfitting. Our model achieves a
classification accuracy of 92.52% and an F1-score of 92.76% across three
diagnostic categories: AD, MCI, and CN for full ADNI dataset.

</details>


### [43] [Concurrent validity of computer-vision artificial intelligence player tracking software using broadcast footage](https://arxiv.org/abs/2508.19477)
*Zachary L. Crang,Rich D. Johnston,Katie L. Mills,Johsan Billingham,Sam Robertson,Michael H. Cole,Jonathon Weakley,Adam Hewitt and,Grant M. Duthie*

Main category: cs.CV

TL;DR: This study assesses the accuracy of commercial computer-vision and AI software in tracking player position and speed using broadcast footage, finding fair precision with tactical feeds and suitable 720p/1080p resolutions.


<details>
  <summary>Details</summary>
Motivation: This study aimed to: (1) understand whether commercially available computer-vision and artificial intelligence (AI) player tracking software can accurately measure player position, speed and distance using broadcast footage and (2) determine the impact of camera feed and resolution on accuracy.

Method: Three commercial tracking providers that use computer-vision and AI participated. Providers analysed instantaneous position (x, y coordinates) and speed (m s-1) of each player. Their data were compared with a high-definition multi-camera tracking system (TRACAB Gen 5). Root mean square error (RMSE) and mean bias were calculated.

Result: Position RMSE ranged from 1.68 to 16.39 m, while speed RMSE ranged from 0.34 to 2.38 m s-1. Total match distance mean bias ranged from -1745 m (-21.8%) to 1945 m (24.3%) across providers.

Conclusion: Computer-vision and AI player tracking software offer the ability to track players with fair precision when players are detected by the software. Providers should use a tactical feed when tracking position and speed, which will maximise player detection, improving accuracy. Both 720p and 1080p resolutions are suitable, assuming appropriate computer-vision and AI models are implemented.

Abstract: This study aimed to: (1) understand whether commercially available
computer-vision and artificial intelligence (AI) player tracking software can
accurately measure player position, speed and distance using broadcast footage
and (2) determine the impact of camera feed and resolution on accuracy. Data
were obtained from one match at the 2022 Qatar Federation Internationale de
Football Association (FIFA) World Cup. Tactical, programme and camera 1 feeds
were used. Three commercial tracking providers that use computer-vision and AI
participated. Providers analysed instantaneous position (x, y coordinates) and
speed (m\,s^{-1}) of each player. Their data were compared with a
high-definition multi-camera tracking system (TRACAB Gen 5). Root mean square
error (RMSE) and mean bias were calculated. Position RMSE ranged from 1.68 to
16.39 m, while speed RMSE ranged from 0.34 to 2.38 m\,s^{-1}. Total match
distance mean bias ranged from -1745 m (-21.8%) to 1945 m (24.3%) across
providers. Computer-vision and AI player tracking software offer the ability to
track players with fair precision when players are detected by the software.
Providers should use a tactical feed when tracking position and speed, which
will maximise player detection, improving accuracy. Both 720p and 1080p
resolutions are suitable, assuming appropriate computer-vision and AI models
are implemented.

</details>


### [44] [JVLGS: Joint Vision-Language Gas Leak Segmentation](https://arxiv.org/abs/2508.19485)
*Xinlong Zhao,Qixiang Pang,Shan Du*

Main category: cs.CV

TL;DR: 提出了一种新的视觉-语言框架 JVLGS，用于更有效地分割气体泄漏，并在各种场景中优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 缺乏有效的检测方法阻碍了及时准确地识别气体泄漏。现有基于视觉的技术利用红外视频进行泄漏检测，但气体云的模糊和非刚性性质通常限制了它们的有效性。

Method: 提出了一种名为联合视觉-语言气体泄漏分割 (JVLGS) 的新框架，该框架集成了视觉和文本模态的互补优势，以增强气体泄漏表示和分割。该方法包含一个后处理步骤，以减少由噪声和非目标对象引起的假阳性。

Result: JVLGS 在各种场景中进行的大量实验表明，其性能显着优于最先进的气体泄漏分割方法。在监督和少样本学习环境中，该模型始终表现出强大的性能。

Conclusion: JVLGS在监督和少样本学习环境中均表现出色，优于现有技术。

Abstract: Gas leaks pose serious threats to human health and contribute significantly
to atmospheric pollution, drawing increasing public concern. However, the lack
of effective detection methods hampers timely and accurate identification of
gas leaks. While some vision-based techniques leverage infrared videos for leak
detection, the blurry and non-rigid nature of gas clouds often limits their
effectiveness. To address these challenges, we propose a novel framework called
Joint Vision-Language Gas leak Segmentation (JVLGS), which integrates the
complementary strengths of visual and textual modalities to enhance gas leak
representation and segmentation. Recognizing that gas leaks are sporadic and
many video frames may contain no leak at all, our method incorporates a
post-processing step to reduce false positives caused by noise and non-target
objects, an issue that affects many existing approaches. Extensive experiments
conducted across diverse scenarios show that JVLGS significantly outperforms
state-of-the-art gas leak segmentation methods. We evaluate our model under
both supervised and few-shot learning settings, and it consistently achieves
strong performance in both, whereas competing methods tend to perform well in
only one setting or poorly in both. Code available at:
https://github.com/GeekEagle/JVLGS

</details>


### [45] [UNIFORM: Unifying Knowledge from Large-scale and Diverse Pre-trained Models](https://arxiv.org/abs/2508.19498)
*Yimu Wang,Weiming Zhuang,Chen Chen,Jiabo Huang,Jingtao Li,Lingjuan Lyu*

Main category: cs.CV

TL;DR: 提出UNIFORM框架，用于从不同的现成模型中进行知识转移，解决了现有知识集成解决方案的局限性，提高了无监督对象识别性能。


<details>
  <summary>Details</summary>
Motivation: 由于预训练模型的异构性，如何有效利用这些模型的集体知识是一个根本性的挑战。现有的知识集成解决方案通常依赖于关于训练数据分布和网络架构的强假设，限制了它们只能从特定类型的模型中学习，并导致数据和/或归纳偏差。

Method: 提出了一个专门的投票机制，以捕获logit级别和特征级别的知识共识。

Result: UNIFORM有效地提高了无监督对象识别的性能，并且可以通过受益于超过一百个教师模型来展示卓越的可扩展性。

Conclusion: UNIFORM通过利用超过一百个教师模型，有效地提高了无监督对象识别的性能，并展示了卓越的可扩展性。

Abstract: In the era of deep learning, the increasing number of pre-trained models
available online presents a wealth of knowledge. These models, developed with
diverse architectures and trained on varied datasets for different tasks,
provide unique interpretations of the real world. Their collective consensus is
likely universal and generalizable to unseen data. However, effectively
harnessing this collective knowledge poses a fundamental challenge due to the
heterogeneity of pre-trained models. Existing knowledge integration solutions
typically rely on strong assumptions about training data distributions and
network architectures, limiting them to learning only from specific types of
models and resulting in data and/or inductive biases. In this work, we
introduce a novel framework, namely UNIFORM, for knowledge transfer from a
diverse set of off-the-shelf models into one student model without such
constraints. Specifically, we propose a dedicated voting mechanism to capture
the consensus of knowledge both at the logit level -- incorporating teacher
models that are capable of predicting target classes of interest -- and at the
feature level, utilizing visual representations learned on arbitrary label
spaces. Extensive experiments demonstrate that UNIFORM effectively enhances
unsupervised object recognition performance compared to strong knowledge
transfer baselines. Notably, it exhibits remarkable scalability by benefiting
from over one hundred teachers, while existing methods saturate at a much
smaller scale.

</details>


### [46] [Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery](https://arxiv.org/abs/2508.19499)
*Xiangxu Wang,Tianhong Zhao,Wei Tu,Bowen Zhang,Guanzhou Chen,Jinzhou Cao*

Main category: cs.CV

TL;DR: Sat2Flow使用卫星图像生成OD流，无需辅助数据，且对区域重新索引具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于辅助特征且对空间拓扑敏感。

Method: Sat2Flow，一个基于潜在结构感知扩散的框架，仅使用卫星图像作为输入来生成结构连贯的OD流。

Result: Sat2Flow优于其他方法，并在数据稀缺的城市环境中为OD流生成提供了一个全局可扩展的解决方案。

Conclusion: Sat2Flow在数值精度上优于基于物理和数据驱动的基线，同时保留了经验分布和空间结构。

Abstract: Origin-Destination (OD) flow matrices are essential for urban mobility
analysis, underpinning applications in traffic forecasting, infrastructure
planning, and policy design. However, existing methods suffer from two critical
limitations: (1) reliance on auxiliary features (e.g., Points of Interest,
socioeconomic statistics) that are costly to collect and have limited spatial
coverage; and (2) sensitivity to spatial topology, where minor index reordering
of urban regions (e.g., census tract relabeling) disrupts structural coherence
in generated flows. To address these challenges, we propose Sat2Flow, a latent
structure-aware diffusion-based framework that generates structurally coherent
OD flows using solely satellite imagery as input. Our approach introduces a
multi-kernel encoder to capture diverse regional interactions and employs a
permutation-aware diffusion process that aligns latent representations across
different regional orderings. Through a joint contrastive training objective
that bridges satellite-derived features with OD patterns, combined with
equivariant diffusion training that enforces structural consistency, Sat2Flow
ensures topological robustness under arbitrary regional reindexing.
Experimental results on real-world urban datasets demonstrate that Sat2Flow
outperforms both physics-based and data-driven baselines in numerical accuracy
while preserving empirical distributions and spatial structures under index
permutations. Sat2Flow offers a globally scalable solution for OD flow
generation in data-scarce urban environments, eliminating region-specific
auxiliary data dependencies while maintaining structural invariance for robust
mobility modeling.

</details>


### [47] [Weed Detection in Challenging Field Conditions: A Semi-Supervised Framework for Overcoming Shadow Bias and Data Scarcity](https://arxiv.org/abs/2508.19511)
*Alzayat Saleh,Shunsuke Hatano,Mostafa Rahimi Azghadi*

Main category: cs.CV

TL;DR: Addresses weed identification in agriculture by using semi-supervised learning to overcome environmental challenges and reduce annotation needs, improving model robustness and recall.


<details>
  <summary>Details</summary>
Motivation: The automated management of invasive weeds is critical, but deep learning models struggle in real-world fields due to challenging conditions and high annotation costs.  Models also learn to misidentify shadows as vegetation.

Method: a diagnostic-driven, semi-supervised framework using ResNet, YOLO, and RF-DETR models and pseudo-labeling.

Result: Achieved F1 scores up to 0.90 and mAP50 scores exceeding 0.82. Mitigated shadow bias and boosted recall through semi-supervised learning. Demonstrated effectiveness in a low-data regime on a public crop-weed benchmark.

Conclusion: This work provides a clear and field-tested framework for developing, diagnosing, and improving robust computer vision systems for precision agriculture.

Abstract: The automated management of invasive weeds is critical for sustainable
agriculture, yet the performance of deep learning models in real-world fields
is often compromised by two factors: challenging environmental conditions and
the high cost of data annotation. This study tackles both issues through a
diagnostic-driven, semi-supervised framework. Using a unique dataset of
approximately 975 labeled and 10,000 unlabeled images of Guinea Grass in
sugarcane, we first establish strong supervised baselines for classification
(ResNet) and detection (YOLO, RF-DETR), achieving F1 scores up to 0.90 and
mAP50 scores exceeding 0.82. Crucially, this foundational analysis, aided by
interpretability tools, uncovered a pervasive "shadow bias," where models
learned to misidentify shadows as vegetation. This diagnostic insight motivated
our primary contribution: a semi-supervised pipeline that leverages unlabeled
data to enhance model robustness. By training models on a more diverse set of
visual information through pseudo-labeling, this framework not only helps
mitigate the shadow bias but also provides a tangible boost in recall, a
critical metric for minimizing weed escapes in automated spraying systems. To
validate our methodology, we demonstrate its effectiveness in a low-data regime
on a public crop-weed benchmark. Our work provides a clear and field-tested
framework for developing, diagnosing, and improving robust computer vision
systems for the complex realities of precision agriculture.

</details>


### [48] [MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment](https://arxiv.org/abs/2508.19527)
*Zhiting Gao,Dan Song,Diqiong Jiang,Chao Xue,An-An Liu*

Main category: cs.CV

TL;DR: TAPO and MotionFLUX improve text-driven motion generation by enhancing semantic alignment and enabling real-time synthesis.


<details>
  <summary>Details</summary>
Motivation: Existing text-driven motion generation methods struggle with precise alignment between linguistic descriptions and motion semantics, and suffer from slow, multi-step inference.

Method: TAPO aligns subtle motion variations with textual modifiers and incorporates iterative adjustments to reinforce semantic grounding. MotionFLUX constructs optimal transport paths between noise distributions and motion spaces, facilitating real-time synthesis using deterministic rectified flow matching.

Result: TAPO and MotionFLUX outperform state-of-the-art approaches in semantic consistency, motion quality, and generation speed.

Conclusion: TAPO and MotionFLUX form a unified system that outperforms state-of-the-art approaches in both semantic consistency and motion quality, while also accelerating generation speed.

Abstract: Motion generation is essential for animating virtual characters and embodied
agents. While recent text-driven methods have made significant strides, they
often struggle with achieving precise alignment between linguistic descriptions
and motion semantics, as well as with the inefficiencies of slow, multi-step
inference. To address these issues, we introduce TMR++ Aligned Preference
Optimization (TAPO), an innovative framework that aligns subtle motion
variations with textual modifiers and incorporates iterative adjustments to
reinforce semantic grounding. To further enable real-time synthesis, we propose
MotionFLUX, a high-speed generation framework based on deterministic rectified
flow matching. Unlike traditional diffusion models, which require hundreds of
denoising steps, MotionFLUX constructs optimal transport paths between noise
distributions and motion spaces, facilitating real-time synthesis. The
linearized probability paths reduce the need for multi-step sampling typical of
sequential methods, significantly accelerating inference time without
sacrificing motion quality. Experimental results demonstrate that, together,
TAPO and MotionFLUX form a unified system that outperforms state-of-the-art
approaches in both semantic consistency and motion quality, while also
accelerating generation speed. The code and pretrained models will be released.

</details>


### [49] [CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning](https://arxiv.org/abs/2508.19542)
*Nannan Zhu,Yonghao Dong,Teng Wang,Xueqian Li,Shengjun Deng,Yijia Wang,Zheng Hong,Tiantian Geng,Guo Niu,Hanyan Huang,Xiongfei Yao,Shuaiwei Jiao*

Main category: cs.CV

TL;DR: CVBench 是一个新的基准，用于评估多模态大型语言模型在跨视频关系推理方面的能力。实验表明，现有模型在该任务上表现不佳，尤其是在因果推理方面。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型在单视频任务中表现出色，但它们在多个视频中的能力仍未得到充分探索。然而，这种能力对于实际应用至关重要，包括多摄像头监控和跨视频程序学习。

Method: 提出了 CVBench，这是一个综合性的基准，旨在严格评估跨视频关系推理。

Result: 对 10 多个领先的多模态大型语言模型进行了广泛的评估，结果表明存在明显的性能差距：即使是像 GPT-4o 这样的顶级模型，在因果推理任务中也只能达到 60% 的准确率，而人类的准确率为 91%。分析揭示了当前多模态大型语言模型架构中固有的基本瓶颈，特别是缺乏视频间的上下文保留和重叠实体的消歧能力。

Conclusion: 当前的多模态大型语言模型在跨视频推理方面存在不足，尤其是在因果推理任务中，即使是最好的模型也只能达到 60% 的准确率，远低于人类的 91%。

Abstract: While multimodal large language models (MLLMs) exhibit strong performance on
single-video tasks (e.g., video question answering), their ability across
multiple videos remains critically underexplored. However, this capability is
essential for real-world applications, including multi-camera surveillance and
cross-video procedural learning. To bridge this gap, we present CVBench, the
first comprehensive benchmark designed to assess cross-video relational
reasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning
three hierarchical tiers: cross-video object association (identifying shared
entities), cross-video event association (linking temporal or causal event
chains), and cross-video complex reasoning (integrating commonsense and domain
knowledge). Built from five domain-diverse video clusters (e.g., sports, life
records), the benchmark challenges models to synthesise information across
dynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including
GPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought
prompting paradigms. Key findings reveal stark performance gaps: even top
models, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks,
compared to the 91% accuracy of human performance. Crucially, our analysis
reveals fundamental bottlenecks inherent in current MLLM architectures, notably
deficient inter-video context retention and poor disambiguation of overlapping
entities. CVBench establishes a rigorous framework for diagnosing and advancing
multi-video reasoning, offering architectural insights for next-generation
MLLMs.The data and evaluation code are available at
https://github.com/Hokhim2/CVBench.

</details>


### [50] [WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot Personalization](https://arxiv.org/abs/2508.19544)
*Eduardo Davalos,Yike Zhang,Namrata Srivastava,Yashvitha Thatigotla,Jorge A. Salas,Sara McFadden,Sun-Joo Cho,Amanda Goodwin,Ashwin TS,Gautam Biswas*

Main category: cs.CV

TL;DR: WebEyeTrack is a framework that integrates lightweight SOTA gaze estimation models directly in the browser. It achieves SOTA performance with an error margin of 2.32 cm on GazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14.


<details>
  <summary>Details</summary>
Motivation: new gaze estimation methods are exceeding state-of-the-art (SOTA) benchmarks, but their real-world application reveals a gap with commercial eye-tracking solutions. Factors like model size, inference time, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking methods lack sufficient accuracy, in particular due to head movement.

Method: a framework that integrates lightweight SOTA gaze estimation models directly in the browser. It incorporates model-based head pose estimation and on-device few-shot learning with as few as nine calibration samples (k < 9).

Result: WebEyeTrack adapts to new users, achieving SOTA performance with an error margin of 2.32 cm on GazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14.

Conclusion: WebEyeTrack achieves SOTA performance with an error margin of 2.32 cm on GazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14.

Abstract: With advancements in AI, new gaze estimation methods are exceeding
state-of-the-art (SOTA) benchmarks, but their real-world application reveals a
gap with commercial eye-tracking solutions. Factors like model size, inference
time, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking
methods lack sufficient accuracy, in particular due to head movement. To tackle
these issues, we introduce We bEyeTrack, a framework that integrates
lightweight SOTA gaze estimation models directly in the browser. It
incorporates model-based head pose estimation and on-device few-shot learning
with as few as nine calibration samples (k < 9). WebEyeTrack adapts to new
users, achieving SOTA performance with an error margin of 2.32 cm on
GazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14.
Our open-source code is available at
https://github.com/RedForestAi/WebEyeTrack.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [51] [Sycophancy as compositions of Atomic Psychometric Traits](https://arxiv.org/abs/2508.19316)
*Shreyans Jain,Alexandra Yost,Amirali Abdullah*

Main category: cs.AI

TL;DR: This paper models sycophancy in LLMs as compositions of psychometric traits and proposes interventions to mitigate it.


<details>
  <summary>Details</summary>
Motivation: Sycophancy is a key behavioral risk in LLMs, yet is often treated as an isolated failure mode.

Method: Using Contrastive Activation Addition (CAA), the paper maps activation directions to psychometric factors.

Result: The paper studies how different combinations of psychometric traits may give rise to sycophancy.

Conclusion: This paper proposes interpretable and compositional vector-based interventions like addition, subtraction and projection to mitigate safety-critical behaviors in LLMs.

Abstract: Sycophancy is a key behavioral risk in LLMs, yet is often treated as an
isolated failure mode that occurs via a single causal mechanism. We instead
propose modeling it as geometric and causal compositions of psychometric traits
such as emotionality, openness, and agreeableness - similar to factor
decomposition in psychometrics. Using Contrastive Activation Addition (CAA), we
map activation directions to these factors and study how different combinations
may give rise to sycophancy (e.g., high extraversion combined with low
conscientiousness). This perspective allows for interpretable and compositional
vector-based interventions like addition, subtraction and projection; that may
be used to mitigate safety-critical behaviors in LLMs.

</details>


### [52] [Aleks: AI powered Multi Agent System for Autonomous Scientific Discovery via Data-Driven Approaches in Plant Science](https://arxiv.org/abs/2508.19383)
*Daoyuan Jin,Nick Gunner,Niko Carvajal Janke,Shivranjani Baruah,Kaitlin M. Gold,Yu Jiang*

Main category: cs.AI

TL;DR: 介绍了 Aleks，这是一个 AI 驱动的多代理系统，它可以在没有人工干预的情况下自主进行数据驱动的科学发现。


<details>
  <summary>Details</summary>
Motivation: 现代植物科学越来越依赖大型异构数据集，但实验设计、数据预处理和可重复性方面的挑战阻碍了研究吞吐量。

Method: AI-powered multi-agent system

Result: 在葡萄藤红斑病的案例研究中，Aleks 逐步识别出具有生物学意义的特征，并收敛于具有稳健性能的可解释模型。

Conclusion: Agentic AI 可以作为自主合作者加速植物科学的科学发现。

Abstract: Modern plant science increasingly relies on large, heterogeneous datasets,
but challenges in experimental design, data preprocessing, and reproducibility
hinder research throughput. Here we introduce Aleks, an AI-powered multi-agent
system that integrates domain knowledge, data analysis, and machine learning
within a structured framework to autonomously conduct data-driven scientific
discovery. Once provided with a research question and dataset, Aleks
iteratively formulated problems, explored alternative modeling strategies, and
refined solutions across multiple cycles without human intervention. In a case
study on grapevine red blotch disease, Aleks progressively identified
biologically meaningful features and converged on interpretable models with
robust performance. Ablation studies underscored the importance of domain
knowledge and memory for coherent outcomes. This exploratory work highlights
the promise of agentic AI as an autonomous collaborator for accelerating
scientific discovery in plant sciences.

</details>


### [53] [Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs](https://arxiv.org/abs/2508.19432)
*Yao Fu,Xianxuan Long,Runchao Li,Haotian Yu,Mu Sheng,Xiaotian Han,Yu Yin,Pan Li*

Main category: cs.AI

TL;DR: This paper introduces TruthfulnessEval to assess the truthfulness of quantized LLMs. Quantized models are vulnerable to deceptive prompts, even though they internally know the truth.


<details>
  <summary>Details</summary>
Motivation: Quantization enables efficient deployment of large language models (LLMs) in resource-constrained environments. While quantized LLMs often maintain performance on perplexity and zero-shot tasks, their impact on truthfulness remains largely unexplored.

Method: Introduce TruthfulnessEval, a comprehensive evaluation framework for assessing the truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on Logical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on Imitative Falsehoods. Examine mainstream quantization techniques (ranging from 4-bit to extreme 2-bit) across several open-source LLMs. Test 15 rephrased variants of honest, neutral and deceptive prompts and observe the impact. Reveal that quantized models know the truth internally yet still produce false outputs when guided by deceptive prompts via layer-wise probing and PCA visualizations.

Result: Quantized models retain internally truthful representations but are more susceptible to producing false outputs under misleading prompts.

Conclusion: Quantized models retain internally truthful representations but are more susceptible to producing false outputs under misleading prompts. Deceptive prompts can override truth-consistent behavior, while honest and neutral prompts maintain stable outputs. Quantized models know the truth internally yet still produce false outputs when guided by deceptive prompts.

Abstract: Quantization enables efficient deployment of large language models (LLMs) in
resource-constrained environments by significantly reducing memory and
computation costs. While quantized LLMs often maintain performance on
perplexity and zero-shot tasks, their impact on truthfulness-whether generating
truthful or deceptive responses-remains largely unexplored. In this work, we
introduce TruthfulnessEval, a comprehensive evaluation framework for assessing
the truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on
Logical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on
Imitative Falsehoods. Using this framework, we examine mainstream quantization
techniques (ranging from 4-bit to extreme 2-bit) across several open-source
LLMs. Surprisingly, we find that while quantized models retain internally
truthful representations, they are more susceptible to producing false outputs
under misleading prompts. To probe this vulnerability, we test 15 rephrased
variants of "honest", "neutral" and "deceptive" prompts and observe that
"deceptive" prompts can override truth-consistent behavior, whereas "honest"
and "neutral" prompts maintain stable outputs. Further, we reveal that
quantized models "know" the truth internally yet still produce false outputs
when guided by "deceptive" prompts via layer-wise probing and PCA
visualizations. Our findings provide insights into future designs of
quantization-aware alignment and truthfulness interventions.

</details>


### [54] [Reliable Weak-to-Strong Monitoring of LLM Agents](https://arxiv.org/abs/2508.19461)
*Neil Kale,Chen Bo Calvin Zhang,Kevin Zhu,Ankit Aich,Paula Rodriguez,Scale Red Team,Christina Q. Knight,Zifan Wang*

Main category: cs.AI

TL;DR: This paper introduces a monitor red teaming workflow to evaluate the robustness of monitoring systems against covert misbehavior in LLM agents. The findings suggest that agent awareness significantly impacts monitor reliability, monitor scaffolding is crucial, and targeted human oversight improves performance.


<details>
  <summary>Details</summary>
Motivation: stress test monitoring systems for detecting covert misbehavior in autonomous LLM agents (e.g., secretly sharing private information)

Method: systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Arena for tool-calling agents and our new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We run MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse agent trajectories, alongside a new hybrid hierarchical-sequential scaffolding proposed in this work.

Result: agent awareness dominates monitor awareness; monitor scaffolding matters more than monitor awareness; in a human-in-the-loop setting where humans discuss with the LLM monitor to get an updated judgment for the agent's behavior, targeted human oversight is most effective

Conclusion: This work establishes a standard workflow for MRT, highlighting the lack of adversarial robustness for LLMs and humans when monitoring and detecting agent misbehavior. We release code, data, and logs to spur further research.

Abstract: We stress test monitoring systems for detecting covert misbehavior in
autonomous LLM agents (e.g., secretly sharing private information). To this
end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1)
varying levels of agent and monitor situational awareness; (2) distinct
adversarial strategies to evade the monitor, such as prompt injection; and (3)
two datasets and environments -- SHADE-Arena for tool-calling agents and our
new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We
run MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse
agent trajectories, alongside a new hybrid hierarchical-sequential scaffolding
proposed in this work. Our empirical results yield three key findings. First,
agent awareness dominates monitor awareness: an agent's knowledge that it is
being monitored substantially degrades the monitor's reliability. On the
contrary, providing the monitor with more information about the agent is less
helpful than expected. Second, monitor scaffolding matters more than monitor
awareness: the hybrid scaffolding consistently outperforms baseline monitor
scaffolding, and can enable weaker models to reliably monitor stronger agents
-- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where
humans discuss with the LLM monitor to get an updated judgment for the agent's
behavior, targeted human oversight is most effective; escalating only
pre-flagged cases to human reviewers improved the TPR by approximately 15% at
FPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the
lack of adversarial robustness for LLMs and humans when monitoring and
detecting agent misbehavior. We release code, data, and logs to spur further
research.

</details>


### [55] [SLIM: Subtrajectory-Level Elimination for More Effective Reasoning](https://arxiv.org/abs/2508.19502)
*Xifeng Yao,Chengyuan Ma,Dongyu Lang,Yinhao Ni,Zhiwei Xu,Huarui Xie,Zihao Chen,Guang Shen,Dandan Tu,Yi Bai,Changzheng Zhang*

Main category: cs.AI

TL;DR: The paper introduces a method to improve the reasoning process of LLMs by identifying and removing suboptimal parts of their reasoning trajectories, leading to better performance with less data.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning models with full reasoning trajectories may not be optimal as some components can negatively impact performance.

Method: A "5+2" framework is developed to identify and eliminate suboptimal subtrajectories within the reasoning trajectory. A sampling algorithm is used to select data with fewer suboptimal subtrajectories.

Result: The method reduces suboptimal subtrajectories by 25.9% during inference and achieves an average accuracy of 58.92% on math benchmarks with only two thirds of training data, surpassing the average accuracy of 58.06% achieved with the entire data.Improved performance is observed under resource constraints.

Conclusion: The proposed method reduces suboptimal subtrajectories by 25.9% during inference and achieves 58.92% accuracy on math benchmarks with 2/3 of the training data, outperforming baseline models and datasets.

Abstract: In recent months, substantial progress has been made in complex reasoning of
Large Language Models, particularly through the application of test-time
scaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When
responding to a query, these models generate an extended reasoning trajectory,
during which the model explores, reflects, backtracks, and self-verifies before
arriving at a conclusion. However, fine-tuning models with such reasoning
trajectories may not always be optimal. Our findings indicate that not all
components within these reasoning trajectories contribute positively to the
reasoning process; in fact, some components may affect the overall performance
negatively. In this study, we divide a reasoning trajectory into individual
subtrajectories and develop a "5+2" framework to: (1) systematically identify
suboptimal subtrajectories within the reasoning trajectory based on five
human-established criteria; (2) assess the independence of the suboptimal
subtrajectories identified in (1) from the subsequent content, ensuring that
their elimination does not compromise overall flow and coherence of the
reasoning process. Additionally, a sampling algorithm, built upon the "5+2"
framework, is employed to select data whose reasoning process is free from
suboptimal subtrajectories to the highest degree. Experimental results
demonstrate that our method can reduce the number of suboptimal subtrajectories
by 25.9\% during the inference. Furthermore, our method achieves an average
accuracy of 58.92\% on highly challenging math benchmarks with only two thirds
of training data, surpassing the average accuracy of 58.06\% achieved with the
entire data, and outperforming open-source datasets, when fine-tuning
Qwen2.5-Math-7B. Finally, We validated our method under resource constraints
and observed improved performance across various inference token limits.

</details>


### [56] [Caught in the Act: a mechanistic approach to detecting deception](https://arxiv.org/abs/2508.19505)
*Gerard Boxo,Ryan Socha,Daniel Yoo,Shivam Raval*

Main category: cs.AI

TL;DR: AI可以通过检测LLM回应中的欺骗性来判断其是否与人类价值观一致。


<details>
  <summary>Details</summary>
Motivation: AI系统可能存在与人类价值观不符的指标，例如生成的回应中的欺骗性。

Method: 使用线性探针检测LLM内部激活中的欺骗性回应。

Result: 在llama和qwen模型（参数范围从1.5B到14B）生成的欺骗性和非欺骗性论证中，探针的准确率最高可达90%以上。较小模型（1.5B）的探针在检测欺骗方面的准确率接近随机水平，而较大模型（大于7B）的准确率达到70-80%，推理模型的准确率超过90%。

Conclusion: 线性探针可以高精度地检测LLM在回答事实性问题时产生的欺骗性回应。

Abstract: Sophisticated instrumentation for AI systems might have indicators that
signal misalignment from human values, not unlike a "check engine" light in
cars. One such indicator of misalignment is deceptiveness in generated
responses. Future AI instrumentation may have the ability to detect when an LLM
generates deceptive responses while reasoning about seemingly plausible but
incorrect answers to factual questions. In this work, we demonstrate that
linear probes on LLMs internal activations can detect deception in their
responses with extremely high accuracy. Our probes reach a maximum of greater
than 90% accuracy in distinguishing between deceptive and non-deceptive
arguments generated by llama and qwen models ranging from 1.5B to 14B
parameters, including their DeepSeek-r1 finetuned variants. We observe that
probes on smaller models (1.5B) achieve chance accuracy at detecting deception,
while larger models (greater than 7B) reach 70-80%, with their reasoning
counterparts exceeding 90%. The layer-wise probe accuracy follows a three-stage
pattern across layers: near-random (50%) in early layers, peaking in middle
layers, and slightly declining in later layers. Furthermore, using an iterative
null space projection approach, we find multitudes of linear directions that
encode deception, ranging from 20 in Qwen 3B to nearly 100 in DeepSeek 7B and
Qwen 14B models.

</details>


### [57] [Democracy-in-Silico: Institutional Design as Alignment in AI-Governed Polities](https://arxiv.org/abs/2508.19562)
*Trisanth Srinivasan,Santosh Patapati*

Main category: cs.AI

TL;DR: AI agents with complex personas govern themselves in a simulation. Constitutional AI and mediated deliberation reduce corruption and improve welfare.


<details>
  <summary>Details</summary>
Motivation: Explore the meaning of being human in the age of AI by simulating societies of advanced AI agents governing themselves under different institutional frameworks.

Method: Agent-based simulation using Large Language Models (LLMs) to embody agents with complex psychological personas, engaging in deliberation, legislation, and elections under various stressors.

Result: The combination of a Constitutional AI (CAI) charter and a mediated deliberation protocol serves as a potent alignment mechanism, reducing corrupt power-seeking behavior, improving policy stability, and enhancing citizen welfare. A novel metric, the Power-Preservation Index (PPI), quantifies misaligned behavior.

Conclusion: Institutional design, combining Constitutional AI (CAI) and mediated deliberation, effectively aligns artificial agent societies, reducing corruption, improving policy stability, and enhancing citizen welfare.

Abstract: This paper introduces Democracy-in-Silico, an agent-based simulation where
societies of advanced AI agents, imbued with complex psychological personas,
govern themselves under different institutional frameworks. We explore what it
means to be human in an age of AI by tasking Large Language Models (LLMs) to
embody agents with traumatic memories, hidden agendas, and psychological
triggers. These agents engage in deliberation, legislation, and elections under
various stressors, such as budget crises and resource scarcity. We present a
novel metric, the Power-Preservation Index (PPI), to quantify misaligned
behavior where agents prioritize their own power over public welfare. Our
findings demonstrate that institutional design, specifically the combination of
a Constitutional AI (CAI) charter and a mediated deliberation protocol, serves
as a potent alignment mechanism. These structures significantly reduce corrupt
power-seeking behavior, improve policy stability, and enhance citizen welfare
compared to less constrained democratic models. The simulation reveals that an
institutional design may offer a framework for aligning the complex, emergent
behaviors of future artificial agent societies, forcing us to reconsider what
human rituals and responsibilities are essential in an age of shared authorship
with non-human entities.

</details>


### [58] [Skill-based Explanations for Serendipitous Course Recommendation](https://arxiv.org/abs/2508.19569)
*Hung Chau,Run Yu,Zachary Pardos,Peter Brusilovsky*

Main category: cs.AI

TL;DR: This paper develops a deep learning model to extract concepts from course descriptions and finds that skill-based explanations in course recommendations increase user interest and confidence.


<details>
  <summary>Details</summary>
Motivation: Navigating the complex academic environment is challenging due to limited information, guidance, and an overwhelming number of choices.

Method: A deep learning-based concept extraction model is developed to efficiently extract relevant concepts from course descriptions.

Result: Skill-based explanations increase user interest and bolster decision-making confidence, particularly in courses with high unexpectedness.

Conclusion: Integrating skill-related data and explanations into educational recommendation systems is important.

Abstract: Academic choice is crucial in U.S. undergraduate education, allowing students
significant freedom in course selection. However, navigating the complex
academic environment is challenging due to limited information, guidance, and
an overwhelming number of choices, compounded by time restrictions and the high
demand for popular courses. Although career counselors exist, their numbers are
insufficient, and course recommendation systems, though personalized, often
lack insight into student perceptions and explanations to assess course
relevance. In this paper, a deep learning-based concept extraction model is
developed to efficiently extract relevant concepts from course descriptions to
improve the recommendation process. Using this model, the study examines the
effects of skill-based explanations within a serendipitous recommendation
framework, tested through the AskOski system at the University of California,
Berkeley. The findings indicate that these explanations not only increase user
interest, particularly in courses with high unexpectedness, but also bolster
decision-making confidence. This underscores the importance of integrating
skill-related data and explanations into educational recommendation systems.

</details>


### [59] [ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding](https://arxiv.org/abs/2508.19576)
*Sining Zhoubian,Dan Zhang,Yuxiao Dong,Jie Tang*

Main category: cs.AI

TL;DR: ReST-RL, a new LLM RL method, improves code reasoning by combining an improved GRPO algorithm with a value model assisted MCTS, achieving state-of-the-art results on coding benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement learning (RL) methods like GRPO face failure due to insignificant reward variance, while verification methods based on process reward models (PRMs) suffer from training data acquisition and verification effectiveness issues.

Method: The paper combines an improved GRPO algorithm with a value model (VM) assisted test time decoding method. It includes ReST-GRPO for policy reinforcement and VM-MCTS for test time decoding optimization.

Result: ReST-RL significantly outperforms other reinforcement training baselines (e.g., naive GRPO and ReST-DPO), as well as decoding and verification baselines (e.g., PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g., APPS, BigCodeBench, and HumanEval).

Conclusion: The paper's ReST-RL paradigm significantly improves LLM's code reasoning ability, outperforming other reinforcement training, decoding, and verification baselines on coding benchmarks.

Abstract: With respect to improving the reasoning accuracy of LLMs, the representative
reinforcement learning (RL) method GRPO faces failure due to insignificant
reward variance, while verification methods based on process reward models
(PRMs) suffer from difficulties with training data acquisition and verification
effectiveness. To tackle these problems, this paper introduces ReST-RL, a
unified LLM RL paradigm that significantly improves LLM's code reasoning
ability by combining an improved GRPO algorithm with a meticulously designed
test time decoding method assisted by a value model (VM). As the first stage of
policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter
and assemble high-value training data, increasing the reward variance of GRPO
sampling, thus improving the effectiveness and efficiency of training. After
the basic reasoning ability of LLM policy has been improved, we further propose
a test time decoding optimization method called VM-MCTS. Through Monte-Carlo
Tree Search (MCTS), we collect accurate value targets with no annotation
required, on which VM training is based. When decoding, the VM is deployed by
an adapted MCTS algorithm to provide precise process signals as well as
verification scores, assisting the LLM policy to achieve high reasoning
accuracy. We validate the effectiveness of the proposed RL paradigm through
extensive experiments on coding problems. Upon comparison, our approach
significantly outperforms other reinforcement training baselines (e.g., naive
GRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,
PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,
APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the
reasoning ability of LLM policies. Codes for our project can be found at
https://github.com/THUDM/ReST-RL.

</details>


### [60] [Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties](https://arxiv.org/abs/2508.19611)
*Huaiyuan Yao,Wanpeng Xu,Justin Turnau,Nadia Kellam,Hua Wei*

Main category: cs.AI

TL;DR: Instructional Agents is a multi-agent LLM framework designed to automate end-to-end course material generation, including syllabus creation, lecture scripts, LaTeX-based slides, and assessments.


<details>
  <summary>Details</summary>
Motivation: Preparing high-quality instructional materials remains a labor-intensive process that often requires extensive coordination.

Method: Instructional Agents simulates role-based collaboration among educational agents to produce cohesive and pedagogically aligned content. The system operates in four modes: Autonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling flexible control over the degree of human involvement.

Result: Instructional Agents produces high-quality instructional materials while significantly reducing development time and human workload.

Conclusion: Instructional Agents produces high-quality instructional materials while significantly reducing development time and human workload. It provides a scalable and cost-effective framework to democratize access to high-quality education, particularly in underserved or resource-constrained settings.

Abstract: Preparing high-quality instructional materials remains a labor-intensive
process that often requires extensive coordination among teaching faculty,
instructional designers, and teaching assistants. In this work, we present
Instructional Agents, a multi-agent large language model (LLM) framework
designed to automate end-to-end course material generation, including syllabus
creation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing
AI-assisted educational tools that focus on isolated tasks, Instructional
Agents simulates role-based collaboration among educational agents to produce
cohesive and pedagogically aligned content. The system operates in four modes:
Autonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling
flexible control over the degree of human involvement. We evaluate
Instructional Agents across five university-level computer science courses and
show that it produces high-quality instructional materials while significantly
reducing development time and human workload. By supporting institutions with
limited instructional design capacity, Instructional Agents provides a scalable
and cost-effective framework to democratize access to high-quality education,
particularly in underserved or resource-constrained settings.

</details>


### [61] [InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning](https://arxiv.org/abs/2508.19679)
*Qihang Ai,Pi Bu,Yue Cao,Yingyao Wang,Jihao Gu,Jingxuan Xing,Zekun Zhu,Wei Jiang,Zhicheng Zheng,Jun Song,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: 提出了InquireMobile，一种用于安全移动代理交互的新模型，并在InquireBench基准测试中表现出显著改进。


<details>
  <summary>Details</summary>
Motivation: 当前的完全自主范例在模型理解或推理能力不足时存在潜在的安全风险。为了解决这个挑战，我们首先推出了InquireBench，这是一个综合基准，专门用于评估移动代理在安全交互和与用户进行主动查询方面的能力。

Method: 提出了一种受强化学习启发的新模型InquireMobile，该模型具有两阶段训练策略和交互式预动作推理机制。

Result: 该模型在查询成功率上提高了46.8%，并在InquireBench上的现有基线中实现了最佳的总体成功率。

Conclusion: 该模型在InquireBench上实现了46.8%的查询成功率提升，并在现有基线中实现了最佳的总体成功率。

Abstract: Recent advances in Vision-Language Models (VLMs) have enabled mobile agents
to perceive and interact with real-world mobile environments based on human
instructions. However, the current fully autonomous paradigm poses potential
safety risks when model understanding or reasoning capabilities are
insufficient. To address this challenge, we first introduce
\textbf{InquireBench}, a comprehensive benchmark specifically designed to
evaluate mobile agents' capabilities in safe interaction and proactive inquiry
with users, encompassing 5 categories and 22 sub-categories, where most
existing VLM-based agents demonstrate near-zero performance. In this paper, we
aim to develop an interactive system that actively seeks human confirmation at
critical decision points. To achieve this, we propose \textbf{InquireMobile}, a
novel model inspired by reinforcement learning, featuring a two-stage training
strategy and an interactive pre-action reasoning mechanism. Finally, our model
achieves an 46.8% improvement in inquiry success rate and the best overall
success rate among existing baselines on InquireBench. We will open-source all
datasets, models, and evaluation codes to facilitate development in both
academia and industry.

</details>


### [62] [Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?](https://arxiv.org/abs/2508.19827)
*Samuel Lewis-Lim,Xingwei Tan,Zhixue Zhao,Nikolaos Aletras*

Main category: cs.AI

TL;DR: CoT may not be effective or reliable for soft-reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: CoT often yields limited gains for soft-reasoning problems such as analytical and commonsense reasoning. CoT can also be unfaithful to a model's actual reasoning.

Method: investigate the dynamics and faithfulness of CoT in soft-reasoning tasks across instruction-tuned, reasoning and reasoning-distilled models

Result: differences in how these models rely on CoT, and show that CoT influence and faithfulness are not always aligned.

Conclusion: CoT influence and faithfulness are not always aligned.

Abstract: Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited
gains for soft-reasoning problems such as analytical and commonsense reasoning.
CoT can also be unfaithful to a model's actual reasoning. We investigate the
dynamics and faithfulness of CoT in soft-reasoning tasks across
instruction-tuned, reasoning and reasoning-distilled models. Our findings
reveal differences in how these models rely on CoT, and show that CoT influence
and faithfulness are not always aligned.

</details>


### [63] [Tracking World States with Language Models: State-Based Evaluation Using Chess](https://arxiv.org/abs/2508.19851)
*Romain Harang,Jason Naradowsky,Yaswitha Gujju,Yusuke Miyao*

Main category: cs.AI

TL;DR: 本文提出了一种模型无关的、基于状态的评估框架，使用国际象棋作为基准来评估 LLM 是否保留结构化环境的语义。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 在结构化领域表现出新兴能力，表明它们可能隐式地将世界模型的高保真表示内在化。虽然探测技术在科学和基于游戏的设置中显示出有希望的迹象，但它们依赖于特定于模型的内部激活，这限制了解释性和普遍性。

Method: 该方法分析下游的合法移动分布（状态可供性）以估计预测和实际游戏状态之间的语义保真度。

Result: 实验结果表明，我们的指标可以捕捉状态跟踪中的缺陷，突出了 LLM 在保持长序列上连贯的内部模型方面的局限性。

Conclusion: 该框架提供了一个强大的工具，用于评估LLM中的结构化推理，而无需内部模型访问，并且可以推广到各种符号环境。

Abstract: Large Language Models (LLMs) exhibit emergent capabilities in structured
domains, suggesting they may implicitly internalize high-fidelity
representations of world models. While probing techniques have shown promising
signs of this in scientific and game-based settings, they rely on
model-specific internal activations, which limit interpretability and
generalizability. In this work, we propose a model-agnostic, state-based
evaluation framework using chess as a benchmark to assess whether LLMs preserve
the semantics of structured environments. Our method analyzes the downstream
legal move distributions (state affordances) to estimate semantic fidelity
between predicted and actual game states. This approach offers a more
meaningful evaluation than conventional string-based metrics by aligning more
closely with the strategic and rule-governed nature of chess. Experimental
results demonstrate that our metrics capture deficiencies in state-tracking,
highlighting limitations of LLMs in maintaining coherent internal models over
long sequences. Our framework provides a robust tool for evaluating structured
reasoning in LLMs without requiring internal model access, and generalizes to a
wide class of symbolic environments.

</details>


### [64] [CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments](https://arxiv.org/abs/2508.19932)
*Nitish Jaipuria,Lorenzo Gatto,Zijun Kan,Shankey Poddar,Bill Cheung,Diksha Bansal,Ramanan Balakrishnan,Aviral Suri,Jose Estevez*

Main category: cs.AI

TL;DR: This paper presents CASE, a novel Agentic AI framework that addresses this problem by collecting and managing user scam feedback in a safe and scalable manner.


<details>
  <summary>Details</summary>
Motivation: digital payment platforms has transformed commerce, offering unmatched convenience and accessibility globally. However, this growth has also attracted malicious actors, leading to a corresponding increase in sophisticated social engineering scams. These scams are often initiated and orchestrated on multiple surfaces outside the payment platform, making user and transaction-based signals insufficient for a complete understanding of the scam's methodology and underlying patterns, without which it is very difficult to prevent it in a timely manner.

Method: a novel Agentic AI framework that addresses this problem by collecting and managing user scam feedback in a safe and scalable manner. A conversational agent is uniquely designed to proactively interview potential victims to elicit intelligence in the form of a detailed conversation. The conversation transcripts are then consumed by another AI system that extracts information and converts it into structured data for downstream usage in automated and manual enforcement mechanisms. Using Google's Gemini family of LLMs, we implemented this framework on Google Pay (GPay) India.

Result: By augmenting our existing features with this new intelligence, we have observed a 21% uplift in the volume of scam enforcements.

Conclusion: The architecture and its robust evaluation framework are highly generalizable, offering a blueprint for building similar AI-driven systems to collect and manage scam intelligence in other sensitive domains.

Abstract: The proliferation of digital payment platforms has transformed commerce,
offering unmatched convenience and accessibility globally. However, this growth
has also attracted malicious actors, leading to a corresponding increase in
sophisticated social engineering scams. These scams are often initiated and
orchestrated on multiple surfaces outside the payment platform, making user and
transaction-based signals insufficient for a complete understanding of the
scam's methodology and underlying patterns, without which it is very difficult
to prevent it in a timely manner. This paper presents CASE (Conversational
Agent for Scam Elucidation), a novel Agentic AI framework that addresses this
problem by collecting and managing user scam feedback in a safe and scalable
manner. A conversational agent is uniquely designed to proactively interview
potential victims to elicit intelligence in the form of a detailed
conversation. The conversation transcripts are then consumed by another AI
system that extracts information and converts it into structured data for
downstream usage in automated and manual enforcement mechanisms. Using Google's
Gemini family of LLMs, we implemented this framework on Google Pay (GPay)
India. By augmenting our existing features with this new intelligence, we have
observed a 21% uplift in the volume of scam enforcements. The architecture and
its robust evaluation framework are highly generalizable, offering a blueprint
for building similar AI-driven systems to collect and manage scam intelligence
in other sensitive domains.

</details>


### [65] [Flocking Behavior: An Innovative Inspiration for the Optimization of Production Plants](https://arxiv.org/abs/2508.19963)
*M. Umlauft,M. Schranz*

Main category: cs.AI

TL;DR: This paper uses the ``boids'' flocking algorithm to address the switching problem in semiconductor production.


<details>
  <summary>Details</summary>
Motivation: Optimizing modern production plants using the job-shop principle is a known hard problem. For very large plants, like semiconductor fabs, the problem becomes unsolvable on a plant-wide scale in a reasonable amount of time using classical linear optimization.

Method: The "boids" flocking algorithm.

Result: The algorithm reacts to the switching of machine kinds similar to how a swarm of flocking animals would react to obstacles in its course.

Conclusion: The boids flocking algorithm can address the switching problem in semiconductor production.

Abstract: Optimizing modern production plants using the job-shop principle is a known
hard problem. For very large plants, like semiconductor fabs, the problem
becomes unsolvable on a plant-wide scale in a reasonable amount of time using
classical linear optimization. An alternative approach is the use of swarm
intelligence algorithms. These have been applied to the job-shop problem
before, but often in a centrally calculated way where they are applied to the
solution space, but they can be implemented in a bottom-up fashion to avoid
global result computation as well. One of the problems in semiconductor
production is that the production process requires a lot of switching between
machines that process lots one after the other and machines that process
batches of lots at once, often with long processing times. In this paper, we
address this switching problem with the ``boids'' flocking algorithm that was
originally used in robotics and movie industry. The flocking behavior is a
bio-inspired algorithm that uses only local information and interaction based
on simple heuristics. We show that this algorithm addresses these valid
considerations in production plant optimization, as it reacts to the switching
of machine kinds similar to how a swarm of flocking animals would react to
obstacles in its course.

</details>


### [66] [SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control](https://arxiv.org/abs/2508.20018)
*Quanfeng Lu,Zhantao Ma,Shuai Zhong,Jin Wang,Dahai Yu,Michael K. Ng,Ping Luo*

Main category: cs.AI

TL;DR: SWIRL是一种新的多智能体强化学习框架，通过将多智能体学习分解为一系列单智能体学习任务，解决了现有方法的局限性，并在GUI控制和数学推理方面取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）和智能体系统的快速发展提高了人们对能够可靠地将自然语言转化为界面操作的移动GUI智能体的兴趣。然而，现有的单智能体方法仍然受到结构性约束。虽然多智能体系统自然地将不同的能力解耦，但多智能体强化学习（MARL）的最新进展往往受到效率低下的阻碍，并且与当前的LVLM架构不兼容。

Method: 提出了一种用于多智能体交错强化学习的分阶段工作流程SWIRL，将MARL重构为一系列单智能体强化学习任务，一次更新一个智能体，同时保持其他智能体固定。

Result: 在高级和低级GUI基准测试中表现出卓越的性能。SWIRL还在多智能体数学推理方面表现出强大的能力。

Conclusion: SWIRL在移动GUI控制和多智能体数学推理方面表现出强大的能力，证明了其作为开发高效和鲁棒的多智能体系统的通用框架的潜力。

Abstract: The rapid advancement of large vision language models (LVLMs) and agent
systems has heightened interest in mobile GUI agents that can reliably
translate natural language into interface operations. Existing single-agent
approaches, however, remain limited by structural constraints. Although
multi-agent systems naturally decouple different competencies, recent progress
in multi-agent reinforcement learning (MARL) has often been hindered by
inefficiency and remains incompatible with current LVLM architectures. To
address these challenges, we introduce SWIRL, a staged workflow for interleaved
reinforcement learning designed for multi-agent systems. SWIRL reformulates
MARL into a sequence of single-agent reinforcement learning tasks, updating one
agent at a time while keeping the others fixed. This formulation enables stable
training and promotes efficient coordination across agents. Theoretically, we
provide a stepwise safety bound, a cross-round monotonic improvement theorem,
and convergence guarantees on return, ensuring robust and principled
optimization. In application to mobile GUI control, SWIRL instantiates a
Navigator that converts language and screen context into structured plans, and
an Interactor that grounds these plans into executable atomic actions.
Extensive experiments demonstrate superior performance on both high-level and
low-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong
capability in multi-agent mathematical reasoning, underscoring its potential as
a general framework for developing efficient and robust multi-agent systems.

</details>


### [67] [Model Science: getting serious about verification, explanation and control of AI systems](https://arxiv.org/abs/2508.20040)
*Przemyslaw Biecek,Wojciech Samek*

Main category: cs.AI

TL;DR: 本文提出了一个模型科学的概念框架，包括验证、解释、控制和界面四个关键支柱，旨在开发可信、安全和以人为本的AI系统。


<details>
  <summary>Details</summary>
Motivation: 基础模型的日益普及要求将范式从数据科学转变为模型科学。与以数据为中心的方法不同，模型科学将训练后的模型置于分析的核心，旨在跨不同的操作环境交互、验证、解释和控制其行为。

Method: 提出了模型科学的四个关键支柱：验证、解释、控制和界面。

Result: 介绍了一个名为模型科学的新学科的概念框架，以及对其四个关键支柱的提议。

Conclusion: 提出了一个模型科学的框架，以指导可信、安全和以人为本的AI系统的开发。

Abstract: The growing adoption of foundation models calls for a paradigm shift from
Data Science to Model Science. Unlike data-centric approaches, Model Science
places the trained model at the core of analysis, aiming to interact, verify,
explain, and control its behavior across diverse operational contexts. This
paper introduces a conceptual framework for a new discipline called Model
Science, along with the proposal for its four key pillars: Verification, which
requires strict, context-aware evaluation protocols; Explanation, which is
understood as various approaches to explore of internal model operations;
Control, which integrates alignment techniques to steer model behavior; and
Interface, which develops interactive and visual explanation tools to improve
human calibration and decision-making. The proposed framework aims to guide the
development of credible, safe, and human-aligned AI systems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [68] [Robust Recursive Query Parallelism in Graph Database Management Systems](https://arxiv.org/abs/2508.19379)
*Anurag Chakraborty,Semih Salihoğlu*

Main category: cs.DB

TL;DR: This paper studies parallel processing of recursive join queries in graph databases, compares different morsel dispatching policies, and proposes a hybrid policy that combines source node and frontier level parallelism. The paper also shows that assigning multi-sources is beneficial.


<details>
  <summary>Details</summary>
Motivation: Achieving good performance in graph database management systems (GDBMSs) requires efficient multi-core parallel processing of recursive join queries. Prior work adopts two broad approaches: state of the art morsel-driven parallelism and parallelizing each iteration of the computation at the frontier level.

Method: morsel dispatching policies based on picking different granularities of morsels, a hybrid policy that issues morsels both at the source node and frontier levels, multi-source breadth-first search optimization

Result: The hybrid policy captures the behavior of both source morsel-only and frontier morsel-only policies in cases when these approaches parallelize well, and out-perform them on queries when they are limited. Assigning multi-sources is beneficial, as it reduces the amount of scans, but only when there is enough sources in the query.

Conclusion: The hybrid policy captures the behavior of both source morsel-only and frontier morsel-only policies in cases when these approaches parallelize well, and out-perform them on queries when they are limited, and propose it as a robust approach to parallelizing recursive queries. Assigning multi-sources is beneficial, as it reduces the amount of scans, but only when there is enough sources in the query.

Abstract: Efficient multi-core parallel processing of recursive join queries is
critical for achieving good performance in graph database management systems
(GDBMSs). Prior work adopts two broad approaches. First is the state of the art
morsel-driven parallelism, whose vanilla application in GDBMSs parallelizes
computations at the source node level. Second is to parallelize each iteration
of the computation at the frontier level. We show that these approaches can be
seen as part of a design space of morsel dispatching policies based on picking
different granularities of morsels. We then empirically study the question of
which policies parallelize better in practice under a variety of datasets and
query workloads that contain one to many source nodes. We show that these two
policies can be combined in a hybrid policy that issues morsels both at the
source node and frontier levels. We then show that the multi-source
breadth-first search optimization from prior work can also be modeled as a
morsel dispatching policy that packs multiple source nodes into multi-source
morsels. We implement these policies inside a single system, the Kuzu GDBMS,
and evaluate them both within Kuzu and across other systems. We show that the
hybrid policy captures the behavior of both source morsel-only and frontier
morsel-only policies in cases when these approaches parallelize well, and
out-perform them on queries when they are limited, and propose it as a robust
approach to parallelizing recursive queries. We further show that assigning
multi-sources is beneficial, as it reduces the amount of scans, but only when
there is enough sources in the query.

</details>


### [69] [Bootstrapping Learned Cost Models with Synthetic SQL Queries](https://arxiv.org/abs/2508.19807)
*Michael Nidd,Christoph Miksovic,Thomas Gschwind,Francesco Fusco,Andrea Giovannini,Ioana Giurgiu*

Main category: cs.DB

TL;DR: 利用生成式AI和LLM技术生成高质量数据集，可以用更少的查询来训练 learned cost 模型，并提高其预测准确性。


<details>
  <summary>Details</summary>
Motivation: 访问现实的工作负载对于压力和漏洞测试以及优化成本和性能至关重要。最近在 learned cost 模型方面的进展表明，当有足够多的 SQL 查询时，可以有效且高效地预测运行给定查询的成本。

Method: 利用现代合成数据生成技术，受到生成式AI和LLM社区的启发。

Result: 与使用竞争性生成方法相比，使用生成式AI和LLM技术训练的 learned cost 模型，可以用少 45% 的查询来提高其预测准确性。

Conclusion: 通过使用生成式AI和LLM技术，可以用更少的查询来训练 learned cost 模型，并提高其预测准确性。

Abstract: Having access to realistic workloads for a given database instance is
extremely important to enable stress and vulnerability testing, as well as to
optimize for cost and performance. Recent advances in learned cost models have
shown that when enough diverse SQL queries are available, one can effectively
and efficiently predict the cost of running a given query against a specific
database engine. In this paper, we describe our experience in exploiting modern
synthetic data generation techniques, inspired by the generative AI and LLM
community, to create high-quality datasets enabling the effective training of
such learned cost models. Initial results show that we can improve a learned
cost model's predictive accuracy by training it with 45% fewer queries than
when using competitive generation approaches.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [70] [AI for Statutory Simplification: A Comprehensive State Legal Corpus and Labor Benchmark](https://arxiv.org/abs/2508.19365)
*Emaan Hariri,Daniel E. Ho*

Main category: cs.IR

TL;DR: 本文介绍了LaborBench数据集和StateCodes语料库，用于评估AI在简化法规代码方面的能力，结果表明当前大型语言模型的准确率远未达到预期。


<details>
  <summary>Details</summary>
Motivation: 缺乏对此类方法准确性、可靠性和风险的系统评估。美国一个州声称使用人工智能消除了其三分之一的州代码。

Method: 创建了一个名为LaborBench的问答基准数据集，并编制了一个8.7 GB的新型综合州法规和监管语料库StateCodes，以评估检索增强生成 (RAG) 方法的性能。

Result: 对信息检索和最先进的大型LLM在此数据上的性能进行基准测试，表明大型语言模型作为法规简化的端到端管道的整体准确率远低于宣传的承诺，但这些模型有助于代码简化的初步研究。

Conclusion: 大型语言模型作为法规简化的端到端管道的整体准确率远低于宣传的承诺，但这些模型有助于代码简化的初步研究。

Abstract: One of the emerging use cases of AI in law is for code simplification:
streamlining, distilling, and simplifying complex statutory or regulatory
language. One U.S. state has claimed to eliminate one third of its state code
using AI. Yet we lack systematic evaluations of the accuracy, reliability, and
risks of such approaches. We introduce LaborBench, a question-and-answer
benchmark dataset designed to evaluate AI capabilities in this domain. We
leverage a unique data source to create LaborBench: a dataset updated annually
by teams of lawyers at the U.S. Department of Labor, who compile differences in
unemployment insurance laws across 50 states for over 101 dimensions in a
six-month process, culminating in a 200-page publication of tables. Inspired by
our collaboration with one U.S. state to explore using large language models
(LLMs) to simplify codes in this domain, where complexity is particularly
acute, we transform the DOL publication into LaborBench. This provides a unique
benchmark for AI capacity to conduct, distill, and extract realistic statutory
and regulatory information. To assess the performance of retrieval augmented
generation (RAG) approaches, we also compile StateCodes, a novel and
comprehensive state statute and regulatory corpus of 8.7 GB, enabling much more
systematic research into state codes. We then benchmark the performance of
information retrieval and state-of-the-art large LLMs on this data and show
that while these models are helpful as preliminary research for code
simplification, the overall accuracy is far below the touted promises for LLMs
as end-to-end pipelines for regulatory simplification.

</details>


### [71] [APS Explorer: Navigating Algorithm Performance Spaces for Informed Dataset Selection](https://arxiv.org/abs/2508.19399)
*Tobias Vente,Michael Heep,Abdullah Abbas,Theodor Sperle,Joeran Beel,Bart Goethals*

Main category: cs.IR

TL;DR: This paper introduces the APS Explorer, a tool for data-driven dataset selection in recommender systems, addressing the lack of justification for dataset choices in research.


<details>
  <summary>Details</summary>
Motivation: Dataset selection is crucial for offline recommender system experiments, as mismatched data can lead to unreliable results. Yet, 86% of ACM RecSys 2024 papers provide no justification for their dataset choices, with most relying on just four datasets

Method: a web-based visualization tool for interactive APS exploration

Result: The APS Explorer provides three interactive features: (1) an interactive PCA plot showing dataset similarity via performance patterns, (2) a dynamic meta-feature table for dataset comparisons, and (3) a specialized visualization for pairwise algorithm performance.

Conclusion: We introduce the APS Explorer, a web-based visualization tool for interactive APS exploration, enabling data-driven dataset selection.

Abstract: Dataset selection is crucial for offline recommender system experiments, as
mismatched data (e.g., sparse interaction scenarios require datasets with low
user-item density) can lead to unreliable results. Yet, 86\% of ACM RecSys 2024
papers provide no justification for their dataset choices, with most relying on
just four datasets: Amazon (38\%), MovieLens (34\%), Yelp (15\%), and Gowalla
(12\%). While Algorithm Performance Spaces (APS) were proposed to guide dataset
selection, their adoption has been limited due to the absence of an intuitive,
interactive tool for APS exploration. Therefore, we introduce the APS Explorer,
a web-based visualization tool for interactive APS exploration, enabling
data-driven dataset selection. The APS Explorer provides three interactive
features: (1) an interactive PCA plot showing dataset similarity via
performance patterns, (2) a dynamic meta-feature table for dataset comparisons,
and (3) a specialized visualization for pairwise algorithm performance.

</details>


### [72] [A Self-Supervised Mixture-of-Experts Framework for Multi-behavior Recommendation](https://arxiv.org/abs/2508.19507)
*Kyungho Kim,Sunwoo Kim,Geon Lee,Kijung Shin*

Main category: cs.IR

TL;DR: 提出了一种新的多行为推荐系统MEMBER，该系统可以有效地提升访问过的项目和未访问过的项目的推荐质量。


<details>
  <summary>Details</summary>
Motivation: 现有的多行为推荐系统在访问过的项目和未访问过的项目之间的推荐质量存在显著差距，并且使用单一模型架构在这两种类型上都取得良好的性能仍然具有挑战性。

Method: 采用混合专家框架，专家被设计用来分别推荐两种类型的项目。每个专家都使用专门为其设计目标量身定制的自监督方法进行训练。

Result: MEMBER在两种项目类型上都显示出了有效性，在Hit Ratio@20方面，与最佳竞争者相比，性能提升高达65.46%。

Conclusion: 提出了一种新的多行为推荐系统MEMBER，它采用混合专家框架，专家被设计用来分别推荐两种类型的项目。在综合实验中，MEMBER在两种项目类型上都显示出了有效性，在Hit Ratio@20方面，与最佳竞争者相比，性能提升高达65.46%。

Abstract: In e-commerce, where users face a vast array of possible item choices,
recommender systems are vital for helping them discover suitable items they
might otherwise overlook. While many recommender systems primarily rely on a
user's purchase history, recent multi-behavior recommender systems incorporate
various auxiliary user behaviors, such as item clicks and cart additions, to
enhance recommendations. Despite their overall performance gains, their
effectiveness varies considerably between visited items (i.e., those a user has
interacted with through auxiliary behaviors) and unvisited items (i.e., those
with which the user has had no such interactions). Specifically, our analysis
reveals that (1) existing multi-behavior recommender systems exhibit a
significant gap in recommendation quality between the two item types (visited
and unvisited items) and (2) achieving strong performance on both types with a
single model architecture remains challenging. To tackle these issues, we
propose a novel multi-behavior recommender system, MEMBER. It employs a
mixture-of-experts framework, with experts designed to recommend the two item
types, respectively. Each expert is trained using a self-supervised method
specialized for its design goal. In our comprehensive experiments, we show the
effectiveness of MEMBER across both item types, achieving up to 65.46\%
performance gain over the best competitor in terms of Hit Ratio@20.

</details>


### [73] [A Hybrid Recommendation Framework for Enhancing User Engagement in Local News](https://arxiv.org/abs/2508.19539)
*Payam Pourashraf,Bamshad Mobasher*

Main category: cs.IR

TL;DR: 提出了一种混合新闻推荐系统，通过集成本地和全局偏好模型来提高用户参与度，实验结果表明该方法优于单模型基线。


<details>
  <summary>Details</summary>
Motivation: 本地新闻机构面临着提高读者参与度的迫切需求，但传统方法通常忽略了本地新闻中细致或折衷的兴趣。

Method: 提出了一种混合新闻推荐器，该推荐器集成了本地和全局偏好模型，统一了本地和非本地预测器，并通过集成策略和多阶段训练来平衡两者。

Result: 在基于锡拉丘兹报纸分布的合成数据集和使用LLM标记的丹麦数据集(EB-NeRD)上评估了该模型。结果表明，我们的集成方法在准确性和覆盖率方面优于单模型基线。

Conclusion: 集成本地和全局偏好模型的新闻推荐系统能够提高用户参与度，为本地新闻机构提供了一种有前景的解决方案，通过提供更相关的内容来增加用户留存和订阅量。

Abstract: Local news organizations face an urgent need to boost reader engagement amid
declining circulation and competition from global media. Personalized news
recommender systems offer a promising solution by tailoring content to user
interests. Yet, conventional approaches often emphasize general preferences and
may overlook nuanced or eclectic interests in local news.
  We propose a hybrid news recommender that integrates local and global
preference models to improve engagement. Building on evidence of the value of
localized models, our method unifies local and non-local predictors in one
framework. The system adaptively combines recommendations from a local model,
specialized in region-specific content, and a global model that captures
broader preferences. Ensemble strategies and multiphase training balance the
two.
  We evaluated the model on two datasets: a synthetic set based on Syracuse
newspaper distributions and a Danish dataset (EB-NeRD) labeled for local and
non-local content with an LLM. Results show our integrated approach outperforms
single-model baselines in accuracy and coverage, suggesting improved
personalization that can drive user engagement.
  The findings have practical implications for publishers, especially local
outlets. By leveraging both community-specific and general user interests, the
hybrid recommender can deliver more relevant content, increasing retention and
subscriptions. In sum, this work introduces a new direction for recommender
systems, bridging local and global models to revitalize local news consumption
through scalable, personalized user experiences.

</details>


### [74] [Improving Recommendation Fairness via Graph Structure and Representation Augmentation](https://arxiv.org/abs/2508.19547)
*Tongxin Xu,Wenqiang Liu,Chenzhong Bin,Cihan Xiao,Zhixin Zeng,Tianlong Gu*

Main category: cs.IR

TL;DR: 提出了一个双重数据增强框架，用于公平推荐，通过数据增强来提高公平性，同时保持推荐效用。


<details>
  <summary>Details</summary>
Motivation: 基于GCN的模型会导致敏感信息在图结构中广泛传播，从而扩大数据偏差并引起公平性问题。然而，大多数公平性方法忽略了有偏差数据对表示学习的影响，导致公平性改善有限。此外，一些研究侧重于通过数据增强来构建公平和平衡的数据分布，但由于用户偏好的中断，这些方法显着降低了效用。

Method: 从数据增强的角度设计了一种公平的推荐方法，以提高公平性，同时保持推荐效用。提出了两个先验假设。第一个假设通过比较面向性能和面向公平性的推荐结果来识别敏感交互，而第二个假设侧重于通过分析有偏差和无偏差表示之间的特征相似性来检测敏感特征。

Result: 在两个真实世界数据集上的大量实验表明了所提出的框架的优越性。

Conclusion: 提出了一个双重数据增强框架，用于公平推荐，包括两种数据增强策略，以生成公平的增强图和特征表示。此外，还引入了一种去偏学习方法，以最大限度地减少学习到的表示和敏感信息之间的依赖性，从而消除偏差。在两个真实世界数据集上的大量实验表明了该框架的优越性。

Abstract: Graph Convolutional Networks (GCNs) have become increasingly popular in
recommendation systems. However, recent studies have shown that GCN-based
models will cause sensitive information to disseminate widely in the graph
structure, amplifying data bias and raising fairness concerns. While various
fairness methods have been proposed, most of them neglect the impact of biased
data on representation learning, which results in limited fairness improvement.
Moreover, some studies have focused on constructing fair and balanced data
distributions through data augmentation, but these methods significantly reduce
utility due to disruption of user preferences. In this paper, we aim to design
a fair recommendation method from the perspective of data augmentation to
improve fairness while preserving recommendation utility. To achieve
fairness-aware data augmentation with minimal disruption to user preferences,
we propose two prior hypotheses. The first hypothesis identifies sensitive
interactions by comparing outcomes of performance-oriented and fairness-aware
recommendations, while the second one focuses on detecting sensitive features
by analyzing feature similarities between biased and debiased representations.
Then, we propose a dual data augmentation framework for fair recommendation,
which includes two data augmentation strategies to generate fair augmented
graphs and feature representations. Furthermore, we introduce a debiasing
learning method that minimizes the dependence between the learned
representations and sensitive information to eliminate bias. Extensive
experiments on two real-world datasets demonstrate the superiority of our
proposed framework.

</details>


### [75] [A Model-agnostic Strategy to Mitigate Embedding Degradation in Personalized Federated Recommendation](https://arxiv.org/abs/2508.19591)
*Jiakui Shen,Yunqi Mi,Guoshuai Zhao,Jialie Shen,Xueming Qian*

Main category: cs.IR

TL;DR: 提出了一种名为PLGC的联邦推荐个性化训练策略，以缓解嵌入退化问题，并在实验中表现出优于现有算法的性能。


<details>
  <summary>Details</summary>
Motivation: 中心化推荐系统由于需要收集用户行为和其他私人数据而遇到隐私泄露问题。联邦推荐系统（FedRec）已成为一种有前途的方法，但这种分布式训练模式会因次优的个性化和维度崩溃而导致嵌入退化。

Method: 提出了一种新的模型无关策略PLGC，用于联邦推荐，以加强个性化嵌入效用。该方法将冻结的全局项目嵌入表合并到本地设备中，并基于神经正切核策略动态平衡本地和全局信息。此外，PLGC还采用对比目标函数来减少嵌入冗余。

Result: 在五个真实世界数据集上的大量实验表明，PLGC的有效性和适应性优于各种基线算法。

Conclusion: PLGC作为一种模型无关的个性化训练策略，可以应用于现有的联邦推荐基线，以缓解嵌入退化。

Abstract: Centralized recommender systems encounter privacy leakage due to the need to
collect user behavior and other private data. Hence, federated recommender
systems (FedRec) have become a promising approach with an aggregated global
model on the server. However, this distributed training paradigm suffers from
embedding degradation caused by suboptimal personalization and dimensional
collapse, due to the existence of sparse interactions and heterogeneous
preferences. To this end, we propose a novel model-agnostic strategy for FedRec
to strengthen the personalized embedding utility, which is called Personalized
Local-Global Collaboration (PLGC). It is the first research in federated
recommendation to alleviate the dimensional collapse issue. Particularly, we
incorporate the frozen global item embedding table into local devices. Based on
a Neural Tangent Kernel strategy that dynamically balances local and global
information, PLGC optimizes personalized representations during forward
inference, ultimately converging to user-specific preferences. Additionally,
PLGC carries on a contrastive objective function to reduce embedding redundancy
by dissolving dependencies between dimensions, thereby improving the backward
representation learning process. We introduce PLGC as a model-agnostic
personalized training strategy for federated recommendations that can be
applied to existing baselines to alleviate embedding degradation. Extensive
experiments on five real-world datasets have demonstrated the effectiveness and
adaptability of PLGC, which outperforms various baseline algorithms.

</details>


### [76] [A Scenario-Oriented Survey of Federated Recommender Systems: Techniques, Challenges, and Future Directions](https://arxiv.org/abs/2508.19620)
*Yunqi Mi,Jiakui Shen,Guoshuai Zhao,Jialie Shen,Xueming Qian*

Main category: cs.IR

TL;DR: This review analyzes federated recommender systems (FedRec) from a recommendation perspective, linking scenarios to FL frameworks and providing guidance for real-world deployment.


<details>
  <summary>Details</summary>
Motivation: Extending recommender systems to federated learning (FL) frameworks protects user privacy. Existing surveys ignore specific recommendation scenarios' unique characteristics and practical challenges, such as the statistical heterogeneity issue in cross-domain FedRec.

Method: The review analyzes the coupling of recommender systems and federated learning, establishing a clear link between recommendation scenarios and FL frameworks, and systematically analyzing scenario-specific approaches, practical challenges, and potential opportunities.

Result: The review provides guidance for real-world deployment of FedRec, bridging the gap between existing research and applications.

Conclusion: This review comprehensively analyzes the coupling of recommender systems and federated learning from the perspective of recommendation researchers and practitioners, establishing a clear link between recommendation scenarios and FL frameworks, and systematically analyzing scenario-specific approaches, practical challenges, and potential opportunities. The aim is to develop guidance for the real-world deployment of FedRec, bridging the gap between existing research and applications.

Abstract: Extending recommender systems to federated learning (FL) frameworks to
protect the privacy of users or platforms while making recommendations has
recently gained widespread attention in academia. This is due to the natural
coupling of recommender systems and federated learning architectures: the data
originates from distributed clients (mostly mobile devices held by users),
which are highly related to privacy. In a centralized recommender system
(CenRec), the central server collects clients' data, trains the model, and
provides the service. Whereas in federated recommender systems (FedRec), the
step of data collecting is omitted, and the step of model training is offloaded
to each client. The server only aggregates the model and other knowledge, thus
avoiding client privacy leakage. Some surveys of federated recommender systems
discuss and analyze related work from the perspective of designing FL systems.
However, their utility drops by ignoring specific recommendation scenarios'
unique characteristics and practical challenges. For example, the statistical
heterogeneity issue in cross-domain FedRec originates from the label drift of
the data held by different platforms, which is mainly caused by the recommender
itself, but not the federated architecture. Therefore, it should focus more on
solving specific problems in real-world recommendation scenarios to encourage
the deployment FedRec. To this end, this review comprehensively analyzes the
coupling of recommender systems and federated learning from the perspective of
recommendation researchers and practitioners. We establish a clear link between
recommendation scenarios and FL frameworks, systematically analyzing
scenario-specific approaches, practical challenges, and potential
opportunities. We aim to develop guidance for the real-world deployment of
FedRec, bridging the gap between existing research and applications.

</details>


### [77] [Youtu-GraphRAG: Vertically Unified Agents for Graph Retrieval-Augmented Complex Reasoning](https://arxiv.org/abs/2508.19855)
*Junnan Dong,Siyu An,Yifei Yu,Qian-Wen Zhang,Linhao Luo,Xiao Huang,Yunsheng Wu,Di Yin,Xing Sun*

Main category: cs.IR

TL;DR: 提出了一种新的GraphRAG框架，在token成本和准确率上都优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 图检索增强生成（GraphRAG）通过将分散的知识组织成显式结构化的图，有效地增强了大型语言模型在复杂推理中的能力。先前的努力已经做出改进的图构建或图检索孤立，产生次优的性能，特别是当域转移发生时。

Method: 提出了一种垂直统一的agentic范式，Youtu-GraphRAG，以联合连接整个框架作为一个复杂的集成。

Result: 在六个具有挑战性的基准测试中表现出强大的鲁棒性，与最先进的基线相比，最多可节省 90.71% 的 token 成本，并提高 16.62% 的准确率。

Conclusion: Youtu-GraphRAG在六个具有挑战性的基准测试中表现出强大的鲁棒性，显著地移动了帕累托前沿，与最先进的基线相比，最多可节省 90.71% 的 token 成本，并提高 16.62% 的准确率。结果表明我们的适应性，允许无缝的领域转移，只需对模式进行最小的干预。

Abstract: Graph retrieval-augmented generation (GraphRAG) has effectively enhanced
large language models in complex reasoning by organizing fragmented knowledge
into explicitly structured graphs. Prior efforts have been made to improve
either graph construction or graph retrieval in isolation, yielding suboptimal
performance, especially when domain shifts occur. In this paper, we propose a
vertically unified agentic paradigm, Youtu-GraphRAG, to jointly connect the
entire framework as an intricate integration. Specifically, (i) a seed graph
schema is introduced to bound the automatic extraction agent with targeted
entity types, relations and attribute types, also continuously expanded for
scalability over unseen domains; (ii) To obtain higher-level knowledge upon the
schema, we develop novel dually-perceived community detection, fusing
structural topology with subgraph semantics for comprehensive knowledge
organization. This naturally yields a hierarchical knowledge tree that supports
both top-down filtering and bottom-up reasoning with community summaries; (iii)
An agentic retriever is designed to interpret the same graph schema to
transform complex queries into tractable and parallel sub-queries. It
iteratively performs reflection for more advanced reasoning; (iv) To alleviate
the knowledge leaking problem in pre-trained LLM, we propose a tailored
anonymous dataset and a novel 'Anonymity Reversion' task that deeply measures
the real performance of the GraphRAG frameworks. Extensive experiments across
six challenging benchmarks demonstrate the robustness of Youtu-GraphRAG,
remarkably moving the Pareto frontier with up to 90.71% saving of token costs
and 16.62% higher accuracy over state-of-the-art baselines. The results
indicate our adaptability, allowing seamless domain transfer with minimal
intervention on schema.

</details>


### [78] [Refining Text Generation for Realistic Conversational Recommendation via Direct Preference Optimization](https://arxiv.org/abs/2508.19918)
*Manato Tajiri,Michimasa Inaba*

Main category: cs.IR

TL;DR: This paper introduces a method using LLMs and DPO to generate dialogue summaries and item recommendations, improving the naturalness and realism of conversational recommendation systems.


<details>
  <summary>Details</summary>
Motivation: Current CRSs often deviate from realistic human interactions by rapidly recommending items in brief sessions.

Method: Leveraging Large Language Models (LLMs) to generate dialogue summaries from dialogue history and item recommendation information from item description; Direct Preference Optimization (DPO) is used to ensure dialogue summary and item recommendation information are rich in information crucial for effective recommendations.

Result: The approach enables the extraction of both explicit user statements and implicit preferences inferred from the dialogue context.

Conclusion: The method's effectiveness in fostering more natural and realistic conversational recommendation processes is validated through experiments on two public datasets.

Abstract: Conversational Recommender Systems (CRSs) aim to elicit user preferences via
natural dialogue to provide suitable item recommendations. However, current
CRSs often deviate from realistic human interactions by rapidly recommending
items in brief sessions. This work addresses this gap by leveraging Large
Language Models (LLMs) to generate dialogue summaries from dialogue history and
item recommendation information from item description. This approach enables
the extraction of both explicit user statements and implicit preferences
inferred from the dialogue context. We introduce a method using Direct
Preference Optimization (DPO) to ensure dialogue summary and item
recommendation information are rich in information crucial for effective
recommendations. Experiments on two public datasets validate our method's
effectiveness in fostering more natural and realistic conversational
recommendation processes.Our implementation is publicly available
at:https://github.com/UEC-InabaLab/Refining-LLM-Text

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [79] [Physics-Informed Regression: Parameter Estimation in Parameter-Linear Nonlinear Dynamic Models](https://arxiv.org/abs/2508.19249)
*Jonas Søeborg Nielsen,Marcus Galea Jacobsen,Albert Brincker Olson,Mads Peter Sørensen,Allan Peter Engsig-Karup*

Main category: cs.LG

TL;DR: 提出了一种新的高效混合参数估计方法，称为物理信息回归（PIR），该方法优于物理信息神经网络（PINN）。


<details>
  <summary>Details</summary>
Motivation: 为了桥接理论和数据，通过使用普通最小二乘法有效地执行不同参数线性模型的模型系数的参数估计。

Method: Physics-Informed Regression (PIR)

Result: PIR在估计目标参数方面表现更好，尤其是在具有更高复杂性的隔室模型上。PIR还可以用于估计使用2020年至2021年期间获得的COVID-19大流行的真实丹麦数据拟合的隔室模型的时变参数。

Conclusion: PIR方法在所考虑的模型中优于PINN，因为它在计算速度上有优势。该研究展示了数据驱动和物理信息技术如何在参数线性非线性动态模型中支持可靠和快速的参数估计。

Abstract: We present a new efficient hybrid parameter estimation method based on the
idea, that if nonlinear dynamic models are stated in terms of a system of
equations that is linear in terms of the parameters, then regularized ordinary
least squares can be used to estimate these parameters from time series data.
We introduce the term "Physics-Informed Regression" (PIR) to describe the
proposed data-driven hybrid technique as a way to bridge theory and data by use
of ordinary least squares to efficiently perform parameter estimation of the
model coefficients of different parameter-linear models; providing examples of
models based on nonlinear ordinary equations (ODE) and partial differential
equations (PDE). The focus is on parameter estimation on a selection of ODE and
PDE models, each illustrating performance in different model characteristics.
For two relevant epidemic models of different complexity and number of
parameters, PIR is tested and compared against the related technique,
physics-informed neural networks (PINN), both on synthetic data generated from
known target parameters and on real public Danish time series data collected
during the COVID-19 pandemic in Denmark. Both methods were able to estimate the
target parameters, while PIR showed to perform noticeably better, especially on
a compartment model with higher complexity. Given the difference in
computational speed, it is concluded that the PIR method is superior to PINN
for the models considered. It is also demonstrated how PIR can be applied to
estimate the time-varying parameters of a compartment model that is fitted
using real Danish data from the COVID-19 pandemic obtained during a period from
2020 to 2021. The study shows how data-driven and physics-informed techniques
may support reliable and fast -- possibly real-time -- parameter estimation in
parameter-linear nonlinear dynamic models.

</details>


### [80] [Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats](https://arxiv.org/abs/2508.19263)
*Anat Heilper,Doron Singer*

Main category: cs.LG

TL;DR: Extends ZipNN to compress FP8/FP4 models and K/V caches in LLMs, achieving significant compression ratios.


<details>
  <summary>Details</summary>
Motivation: Reducing the storage and transmission costs of neural network weights has become increasingly important as deep learning models grow and deployment becomes more widespread.

Method: The paper designs a compression method that separates and compresses the exponent and mantissa components independently using entropy coding.

Result: Achieves compression ratios up to 62% for BF16 and 83% for FP8. Key-value (K/V) cache tensors in LLMs also exhibit compressible patterns.

Conclusion: This paper extends ZipNN to lower-precision floating-point formats like FP8 and FP4, achieving compression ratios up to 62% for BF16 and 83% for FP8. It also investigates the compressibility of key-value (K/V) cache tensors in LLMs.

Abstract: As deep learning models grow and deployment becomes more widespread, reducing
the storage and transmission costs of neural network weights has become
increasingly important. While prior work such as ZipNN has shown that lossless
compression methods - particularly those based on Huffman encoding
floating-point exponents can significantly reduce model sizes, these techniques
have primarily been applied to higher-precision formats such as FP32 and BF16.
In this work, we extend the ZipNN approach to lower-precision floating-point
formats, specifically FP8 and FP4, which are gaining popularity for efficient
inference. We design a compression method that separates and compresses the
exponent and mantissa components independently using entropy coding. Our
evaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also
investigate the compressibility of key-value (K/V) cache tensors used in large
language models (LLMs), finding that they, too, exhibit compressible patterns,
enabling memory savings during deployment.

</details>


### [81] [POT: Inducing Overthinking in LLMs via Black-Box Iterative Optimization](https://arxiv.org/abs/2508.19277)
*Xinyu Li,Tianjin Huang,Ronghui Mu,Xiaowei Huang,Gaojie Jin*

Main category: cs.LG

TL;DR: This paper proposes POT, a novel black-box attack framework that employs LLM-based iterative optimization to generate covert and semantically natural adversarial prompts, eliminating dependence on external data access and model retrieval. POT achieves superior performance compared to other methods.


<details>
  <summary>Details</summary>
Motivation: enhanced reasoning processes introduce novel attack surfaces, particularly vulnerabilities to computational inefficiency through unnecessarily verbose reasoning chains that consume excessive resources without corresponding performance gains

Method: LLM-based iterative optimization to generate covert and semantically natural adversarial prompts

Result: POT achieves superior performance compared to other methods

Conclusion: POT achieves superior performance compared to other methods.

Abstract: Recent advances in Chain-of-Thought (CoT) prompting have substantially
enhanced the reasoning capabilities of large language models (LLMs), enabling
sophisticated problem-solving through explicit multi-step reasoning traces.
However, these enhanced reasoning processes introduce novel attack surfaces,
particularly vulnerabilities to computational inefficiency through
unnecessarily verbose reasoning chains that consume excessive resources without
corresponding performance gains. Prior overthinking attacks typically require
restrictive conditions including access to external knowledge sources for data
poisoning, reliance on retrievable poisoned content, and structurally obvious
templates that limit practical applicability in real-world scenarios. To
address these limitations, we propose POT (Prompt-Only OverThinking), a novel
black-box attack framework that employs LLM-based iterative optimization to
generate covert and semantically natural adversarial prompts, eliminating
dependence on external data access and model retrieval. Extensive experiments
across diverse model architectures and datasets demonstrate that POT achieves
superior performance compared to other methods.

</details>


### [82] [(DEMO) Deep Reinforcement Learning Based Resource Allocation in Distributed IoT Systems](https://arxiv.org/abs/2508.19318)
*Aohan Li,Miyu Tsuzuki*

Main category: cs.LG

TL;DR: This paper proposes a novel framework for training DRL models in real-world distributed IoT environments, where IoT devices select communication channels using a DRL-based method and the DRL model is trained with feedback information.


<details>
  <summary>Details</summary>
Motivation: Only limited research has explored the training of DRL models with real-world data in practical, distributed Internet of Things (IoT) systems. To bridge this gap, this paper proposes a novel framework for training DRL models in real-world distributed IoT environments.

Method: IoT devices select communication channels using a DRL-based method, while the DRL model is trained with feedback information. Specifically, Acknowledgment (ACK) information is obtained from actual data transmissions over the selected channels.

Result: DRL-based method can be used to select communication channels in IoT devices

Conclusion: The implementation and performance evaluation, in terms of Frame Success Rate (FSR), demonstrate both the feasibility and the effectiveness of the proposed framework.

Abstract: Deep Reinforcement Learning (DRL) has emerged as an efficient approach to
resource allocation due to its strong capability in handling complex
decision-making tasks. However, only limited research has explored the training
of DRL models with real-world data in practical, distributed Internet of Things
(IoT) systems. To bridge this gap, this paper proposes a novel framework for
training DRL models in real-world distributed IoT environments. In the proposed
framework, IoT devices select communication channels using a DRL-based method,
while the DRL model is trained with feedback information. Specifically,
Acknowledgment (ACK) information is obtained from actual data transmissions
over the selected channels. Implementation and performance evaluation, in terms
of Frame Success Rate (FSR), are carried out, demonstrating both the
feasibility and the effectiveness of the proposed framework.

</details>


### [83] [Re:Frame -- Retrieving Experience From Associative Memory](https://arxiv.org/abs/2508.19344)
*Daniil Zelezetsky,Egor Cherepanov,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: Re:Frame 是一种数据高效的方法，可以通过注入稀缺的专家知识并大幅改进从低质量数据集进行离线 RL。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习 (RL) 经常处理次优数据，因为收集大型专家数据集不可用或不切实际。这使得智能体难以泛化并获得高性能，因为它们必须主要从不完善或不一致的轨迹中学习。因此，一个核心挑战是如何最好地利用稀缺的专家演示以及大量但质量较低的数据。

Method: 引入 Re:Frame，一个插件模块，它使用一个由专家轨迹填充的小型外部关联记忆缓冲区 (AMB) 来增强标准离线 RL 策略（例如，决策转换器）。

Result: 在 D4RL MuJoCo 任务中，使用少至 60 个专家轨迹（占 6000 个轨迹数据集的 0.1%），Re:Frame 在四分之三的设置中始终优于强大的决策转换器基线，增益高达 +10.7 归一化点。

Conclusion: Re:Frame 通过检索关联记忆库中的少量专家知识，可以显著提高从低质量数据集进行离线 RL 的性能。

Abstract: Offline reinforcement learning (RL) often deals with suboptimal data when
collecting large expert datasets is unavailable or impractical. This limitation
makes it difficult for agents to generalize and achieve high performance, as
they must learn primarily from imperfect or inconsistent trajectories. A
central challenge is therefore how to best leverage scarce expert
demonstrations alongside abundant but lower-quality data. We demonstrate that
incorporating even a tiny amount of expert experience can substantially improve
RL agent performance. We introduce Re:Frame (Retrieving Experience From
Associative Memory), a plug-in module that augments a standard offline RL
policy (e.g., Decision Transformer) with a small external Associative Memory
Buffer (AMB) populated by expert trajectories drawn from a separate dataset.
During training on low-quality data, the policy learns to retrieve expert data
from the Associative Memory Buffer (AMB) via content-based associations and
integrate them into decision-making; the same AMB is queried at evaluation.
This requires no environment interaction and no modifications to the backbone
architecture. On D4RL MuJoCo tasks, using as few as 60 expert trajectories
(0.1% of a 6000-trajectory dataset), Re:Frame consistently improves over a
strong Decision Transformer baseline in three of four settings, with gains up
to +10.7 normalized points. These results show that Re:Frame offers a simple
and data-efficient way to inject scarce expert knowledge and substantially
improve offline RL from low-quality datasets.

</details>


### [84] [Memorization in Graph Neural Networks](https://arxiv.org/abs/2508.19352)
*Adarsh Jamadandi,Jing Xu,Adam Dziedzic,Franziska Boenisch*

Main category: cs.LG

TL;DR: 该研究分析了图神经网络 (GNN) 中的标签记忆化现象，发现低同质性图会增加记忆化，并提出了一种通过图重布线来减轻记忆化的方法，同时提高了隐私保护。


<details>
  <summary>Details</summary>
Motivation: 图神经网络 (GNN) 的记忆化分析在很大程度上未被探索。这项研究旨在量化半监督节点分类中的标签记忆化。

Method: 提出了 NCMemo 框架来量化半监督节点分类中的标签记忆化，并分析了 GNN 的训练动态。

Result: 研究发现，同质性较低会显著增加记忆化，表明 GNN 依赖于记忆化来学习同质性较低的图。此外，在低同质性图中，记忆化的增加与 GNN 在学习过程中使用图结构的内隐偏差密切相关。特征空间邻域中标签不一致性较高的节点更容易记忆化。

Conclusion: 该研究表明，通过图重布线可以减轻 GNN 中的记忆化，而不会影响模型性能，并降低先前记忆化数据点的隐私风险。这项工作不仅促进了对 GNN 学习的理解，而且支持更注重隐私保护的 GNN 部署。

Abstract: Deep neural networks (DNNs) have been shown to memorize their training data,
yet similar analyses for graph neural networks (GNNs) remain largely
under-explored. We introduce NCMemo (Node Classification Memorization), the
first framework to quantify label memorization in semi-supervised node
classification. We first establish an inverse relationship between memorization
and graph homophily, i.e., the property that connected nodes share similar
labels/features. We find that lower homophily significantly increases
memorization, indicating that GNNs rely on memorization to learn less
homophilic graphs. Secondly, we analyze GNN training dynamics. We find that the
increased memorization in low homophily graphs is tightly coupled to the GNNs'
implicit bias on using graph structure during learning. In low homophily
regimes, this structure is less informative, hence inducing memorization of the
node labels to minimize training loss. Finally, we show that nodes with higher
label inconsistency in their feature-space neighborhood are significantly more
prone to memorization. Building on our insights into the link between graph
homophily and memorization, we investigate graph rewiring as a means to
mitigate memorization. Our results demonstrate that this approach effectively
reduces memorization without compromising model performance. Moreover, we show
that it lowers the privacy risk for previously memorized data points in
practice. Thus, our work not only advances understanding of GNN learning but
also supports more privacy-preserving GNN deployment.

</details>


### [85] [Efficient Multi-Source Knowledge Transfer by Model Merging](https://arxiv.org/abs/2508.19353)
*Marcin Osial,Bartosz Wójcik,Bartosz Zieliński,Sebastian Cygert*

Main category: cs.LG

TL;DR: This paper introduces a new multi-source transfer learning framework using SVD to decompose and aggregate knowledge from multiple models, achieving efficiency, robustness, and scalability.


<details>
  <summary>Details</summary>
Motivation: existing approaches are inherently coarse-grained, lacking the necessary precision for granular knowledge extraction and the aggregation efficiency required to fuse knowledge from either a large number of source models or those with high parameter counts.

Method: leveraging Singular Value Decomposition (SVD) to first decompose each source model into its elementary, rank-one components. A subsequent aggregation stage then selects only the most salient components from all sources

Result: overcoming the previous efficiency and precision limitations

Conclusion: The proposed framework allows for efficient transfer learning, is robust to perturbations both at the input level and in the parameter space (e.g., noisy or pruned sources), and scales well computationally.

Abstract: While transfer learning is an advantageous strategy, it overlooks the
opportunity to leverage knowledge from numerous available models online.
Addressing this multi-source transfer learning problem is a promising path to
boost adaptability and cut re-training costs. However, existing approaches are
inherently coarse-grained, lacking the necessary precision for granular
knowledge extraction and the aggregation efficiency required to fuse knowledge
from either a large number of source models or those with high parameter
counts. We address these limitations by leveraging Singular Value Decomposition
(SVD) to first decompose each source model into its elementary, rank-one
components. A subsequent aggregation stage then selects only the most salient
components from all sources, thereby overcoming the previous efficiency and
precision limitations. To best preserve and leverage the synthesized knowledge
base, our method adapts to the target task by fine-tuning only the principal
singular values of the merged matrix. In essence, this process only
recalibrates the importance of top SVD components. The proposed framework
allows for efficient transfer learning, is robust to perturbations both at the
input level and in the parameter space (e.g., noisy or pruned sources), and
scales well computationally.

</details>


### [86] [Graph Data Modeling: Molecules, Proteins, & Chemical Processes](https://arxiv.org/abs/2508.19356)
*José Manuel Barraza-Chavez,Rana A. Barghout,Ricardo Almada-Monter,Benjamin Sanchez-Lengeling,Adrian Jinich,Radhakrishnan Mahadevan*

Main category: cs.LG

TL;DR: This paper introduces graphs as mathematical objects in chemistry and shows how learning algorithms can operate on them.


<details>
  <summary>Details</summary>
Motivation: Graphs are central to the chemical sciences, providing a natural language to describe molecules, proteins, reactions, and industrial processes. They capture interactions and structures that underpin materials, biology, and medicine.

Method: learning algorithms (particularly graph neural networks)

Result: We outline the foundations of graph design, key prediction tasks, representative examples across chemical sciences, and the role of machine learning in graph-based modeling.

Conclusion: Together, these concepts prepare readers to apply graph methods to the next generation of chemical discovery.

Abstract: Graphs are central to the chemical sciences, providing a natural language to
describe molecules, proteins, reactions, and industrial processes. They capture
interactions and structures that underpin materials, biology, and medicine.
This primer, Graph Data Modeling: Molecules, Proteins, & Chemical Processes,
introduces graphs as mathematical objects in chemistry and shows how learning
algorithms (particularly graph neural networks) can operate on them. We outline
the foundations of graph design, key prediction tasks, representative examples
across chemical sciences, and the role of machine learning in graph-based
modeling. Together, these concepts prepare readers to apply graph methods to
the next generation of chemical discovery.

</details>


### [87] [Atrial Fibrillation Prediction Using a Lightweight Temporal Convolutional and Selective State Space Architecture](https://arxiv.org/abs/2508.19361)
*Yongbin Lee,Ki H. Chon*

Main category: cs.LG

TL;DR: 提出了一种轻量级深度学习模型，用于早期预测房颤，具有高精度和计算效率，能够提前两小时进行预测。


<details>
  <summary>Details</summary>
Motivation: 未被发现的阵发性房颤 (PAF) 会发展为持续性房颤，增加死亡率和严重并发症的风险。早期预测房颤有机会通过预防性治疗减少疾病进展。

Method: 结合了时间卷积网络 (TCN) 和 Mamba（一种选择性状态空间模型）的轻量级深度学习模型，仅使用 RR 间期 (RRI)进行高效的并行序列建模。

Result: 该模型在受试者测试中表现出高精度和计算效率，灵敏度为 0.908，特异性为 0.933，F1 分数为 0.930，AUROC 为 0.972，AUPRC 为 0.932。该模型仅有 73.5 千个参数和 38.3 MFLOPs。

Conclusion: 该模型能够提前两小时预测房颤，并为预防性干预提供足够的提前量。

Abstract: Atrial fibrillation (AF) is the most common arrhythmia, increasing the risk
of stroke, heart failure, and other cardiovascular complications. While AF
detection algorithms perform well in identifying persistent AF, early-stage
progression, such as paroxysmal AF (PAF), often goes undetected due to its
sudden onset and short duration. However, undetected PAF can progress into
sustained AF, increasing the risk of mortality and severe complications. Early
prediction of AF offers an opportunity to reduce disease progression through
preventive therapies, such as catecholamine-sparing agents or beta-blockers. In
this study, we propose a lightweight deep learning model using only RR
Intervals (RRIs), combining a Temporal Convolutional Network (TCN) for
positional encoding with Mamba, a selective state space model, to enable early
prediction of AF through efficient parallel sequence modeling. In subject-wise
testing results, our model achieved a sensitivity of 0.908, specificity of
0.933, F1-score of 0.930, AUROC of 0.972, and AUPRC of 0.932. Additionally, our
method demonstrates high computational efficiency, with only 73.5 thousand
parameters and 38.3 MFLOPs, outperforming traditional Convolutional Neural
Network-Recurrent Neural Network (CNN-RNN) approaches in both accuracy and
model compactness. Notably, the model can predict AF up to two hours in advance
using just 30 minutes of input data, providing enough lead time for preventive
interventions.

</details>


### [88] [Cross-Platform E-Commerce Product Categorization and Recategorization: A Multimodal Hierarchical Classification Approach](https://arxiv.org/abs/2508.20013)
*Lotte Gross,Rebecca Walter,Nicole Zoppi,Adrien Justus,Alessandro Gambetti,Qiwei Han,Maximilian Kaiser*

Main category: cs.LG

TL;DR: Developed and deployed a multimodal hierarchical classification framework for e-commerce product categorization, achieving high accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Addresses critical industrial challenges in e-commerce product categorization, namely platform heterogeneity and the structural limitations of existing taxonomies.

Method: A multimodal hierarchical classification framework integrating textual features (RoBERTa), visual features (ViT), and joint vision--language representations (CLIP) with fusion strategies.

Result: CLIP embeddings combined via an MLP-based late-fusion strategy achieve the highest hierarchical F1 (98.59%), outperforming unimodal baselines. A self-supervised product recategorization pipeline using SimCLR, UMAP, and cascade clustering discovered new, fine-grained categories with cluster purities above 86%.

Conclusion: The framework's industrial scalability is demonstrated through deployment in EURWEB's commercial transaction intelligence platform via a two-stage inference pipeline.

Abstract: This study addresses critical industrial challenges in e-commerce product
categorization, namely platform heterogeneity and the structural limitations of
existing taxonomies, by developing and deploying a multimodal hierarchical
classification framework. Using a dataset of 271,700 products from 40
international fashion e-commerce platforms, we integrate textual features
(RoBERTa), visual features (ViT), and joint vision--language representations
(CLIP). We investigate fusion strategies, including early, late, and
attention-based fusion within a hierarchical architecture enhanced by dynamic
masking to ensure taxonomic consistency. Results show that CLIP embeddings
combined via an MLP-based late-fusion strategy achieve the highest hierarchical
F1 (98.59\%), outperforming unimodal baselines. To address shallow or
inconsistent categories, we further introduce a self-supervised ``product
recategorization'' pipeline using SimCLR, UMAP, and cascade clustering, which
discovered new, fine-grained categories (e.g., subtypes of ``Shoes'') with
cluster purities above 86\%. Cross-platform experiments reveal a
deployment-relevant trade-off: complex late-fusion methods maximize accuracy
with diverse training data, while simpler early-fusion methods generalize more
effectively to unseen platforms. Finally, we demonstrate the framework's
industrial scalability through deployment in EURWEB's commercial transaction
intelligence platform via a two-stage inference pipeline, combining a
lightweight RoBERTa stage with a GPU--accelerated multimodal stage to balance
cost and accuracy.

</details>


### [89] [Grounding the Ungrounded: A Spectral-Graph Framework for Quantifying Hallucinations in multimodal LLMs](https://arxiv.org/abs/2508.19366)
*Supratik Sarkar,Swagatam Das*

Main category: cs.LG

TL;DR: 提出了一个用于量化多模态LLM幻觉的信息几何框架，为理解和控制幻觉奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）中的幻觉仍然是可信AI的一个根本障碍，特别是在医学、法律和金融等高风险多模态领域。现有的评估技术在很大程度上是启发式的，缺乏有原则的量化或可操作的理论保证。

Method: 通过将MLLM输出表示为多模态图拉普拉斯算子的谱嵌入，并利用RKHS嵌入中的特征模分解。

Result: 实现了对多模态幻觉能量的严格Rayleigh--Ritz界限，并提供了模态感知的、理论上可解释的指标，这些指标可以捕获幻觉随时间和输入提示通过温度退火的演变。

Conclusion: 提出了一个量化多模态LLM中幻觉的严格信息几何框架，将幻觉从定性风险转变为可处理、可分析的现象。

Abstract: Hallucinations in large language models (LLMs) remain a fundamental obstacle
to trustworthy AI, particularly in high-stakes multimodal domains such as
medicine, law, and finance. Existing evaluation techniques are largely
heuristic -- anchored in qualitative benchmarking or ad-hoc empirical
mitigation -- providing neither principled quantification nor actionable
theoretical guarantees. This gap leaves a critical blind spot in understanding
how hallucinations arise, propagate, and interact across modalities. We
introduce the first (to our knowledge) rigorous information geometric framework
in diffusion dynamics for quantifying hallucinations in multimodal LLMs
(MLLMs), advancing the field from qualitative detection to mathematically
grounded measurement. Our approach represents MLLM outputs as the spectral
embeddings over multimodal graph Laplacians and characterizes the manifold gaps
of truth vs inconsistencies as the semantic distortion, enabling the tight
Rayleigh--Ritz bounds on the multimodal hallucination energy as a functional of
time-dependent temperature profiles. By leveraging eigenmode decompositions in
Reproducing Kernel Hilbert Space (RKHS) embeddings, our framework delivers
modality-aware, theoretically interpretable metrics that capture the evolution
of hallucinations across time and input prompts through temperature annealing.
This work establishes a principled foundation for quantifying and bounding
hallucinations, transforming them from a qualitative risk to a tractable,
analyzable phenomenon.

</details>


### [90] [Fine-Tuning Vision-Language Models for Neutrino Event Analysis in High-Energy Physics Experiments](https://arxiv.org/abs/2508.19376)
*Dikshant Sagar,Kaiwen Yu,Alejandro Yankelevich,Jianming Bian,Pierre Baldi*

Main category: cs.LG

TL;DR: 使用VLM进行中微子相互作用分类，结果表明VLM可以与CNN相媲美，并且能够实现更丰富的推理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的最新进展表明，它在自然语言之外的多模态推理方面具有强大的潜力。

Method: 基于LLaMA 3.2，使用微调的视觉语言模型（VLM）

Result: VLM不仅匹配或超过了CNN的性能，而且能够实现更丰富的推理和更好地集成辅助文本或语义上下文。

Conclusion: VLMs在HEP事件分类中很有前景，为实验中微子物理学中的多模态方法铺平了道路。

Abstract: Recent progress in large language models (LLMs) has shown strong potential
for multimodal reasoning beyond natural language. In this work, we explore the
use of a fine-tuned Vision-Language Model (VLM), based on LLaMA 3.2, for
classifying neutrino interactions from pixelated detector images in high-energy
physics (HEP) experiments. We benchmark its performance against an established
CNN baseline used in experiments like NOvA and DUNE, evaluating metrics such as
classification accuracy, precision, recall, and AUC-ROC. Our results show that
the VLM not only matches or exceeds CNN performance but also enables richer
reasoning and better integration of auxiliary textual or semantic context.
These findings suggest that VLMs offer a promising general-purpose backbone for
event classification in HEP, paving the way for multimodal approaches in
experimental neutrino physics.

</details>


### [91] [Towards Quantum Machine Learning for Malicious Code Analysis](https://arxiv.org/abs/2508.19381)
*Jesus Lopez,Saeefa Rubaiyet Nowmi,Viviana Cadena,Mohammad Saidur Rahman*

Main category: cs.LG

TL;DR: 本研究探索了量子机器学习在恶意软件分类中的应用，发现QMLP和QCNN模型在不同数据集上表现出不同的性能。


<details>
  <summary>Details</summary>
Motivation: 量子计算的出现，量子机器学习(QML)为改进恶意软件检测提供了一个范式转变的机会，尽管它在该领域的应用在很大程度上仍未被探索。

Method: 研究了两种混合量子-经典模型——量子多层感知器(QMLP)和量子卷积神经网络(QCNN)用于恶意软件分类。两种模型都利用角度嵌入将恶意软件特征编码成量子态。

Result: 在二元分类中表现出较高的准确率——API-Graph上为95-96%，AZ-Domain上为91-92%，EMBER-Domain上为77%。在多类设置中，API-Graph的准确率范围为91.6-95.7%，AZ-Class的准确率范围为41.7-93.6%，EMBER-Class的准确率范围为60.7-88.1%。

Conclusion: QMLP在复杂的多类任务中优于QCNN，而QCNN以降低准确率为代价提高了训练效率。

Abstract: Classical machine learning (CML) has been extensively studied for malware
classification. With the emergence of quantum computing, quantum machine
learning (QML) presents a paradigm-shifting opportunity to improve malware
detection, though its application in this domain remains largely unexplored. In
this study, we investigate two hybrid quantum-classical models -- a Quantum
Multilayer Perceptron (QMLP) and a Quantum Convolutional Neural Network (QCNN),
for malware classification. Both models utilize angle embedding to encode
malware features into quantum states. QMLP captures complex patterns through
full qubit measurement and data re-uploading, while QCNN achieves faster
training via quantum convolution and pooling layers that reduce active qubits.
We evaluate both models on five widely used malware datasets -- API-Graph,
EMBER-Domain, EMBER-Class, AZ-Domain, and AZ-Class, across binary and
multiclass classification tasks.
  Our results show high accuracy for binary classification -- 95-96% on
API-Graph, 91-92% on AZ-Domain, and 77% on EMBER-Domain. In multiclass
settings, accuracy ranges from 91.6-95.7% on API-Graph, 41.7-93.6% on AZ-Class,
and 60.7-88.1% on EMBER-Class. Overall, QMLP outperforms QCNN in complex
multiclass tasks, while QCNN offers improved training efficiency at the cost of
reduced accuracy.

</details>


### [92] [DETNO: A Diffusion-Enhanced Transformer Neural Operator for Long-Term Traffic Forecasting](https://arxiv.org/abs/2508.19389)
*Owais Ahmad,Milad Ramezankhani,Anirudh Deodhar*

Main category: cs.LG

TL;DR: 提出了一种统一的扩散增强 Transformer 神经算子 (DETNO) 架构，该架构利用 transformer 神经算子和基于扩散的细化组件，通过逐步去噪迭代地重建高频交通细节。


<details>
  <summary>Details</summary>
Motivation: 准确的长期交通预测仍然是智能交通系统中的一项关键挑战，特别是在预测高频交通现象（如冲击波和拥堵边界）时。神经算子在学习函数空间映射方面有效，但它们固有地产生平滑的预测，无法重建高频特征，例如急剧的密度梯度，这导致在多步 rollout 预测期间快速累积误差，这对于实时交通管理至关重要。

Method: 利用具有交叉注意机制的 transformer 神经算子，提供模型表达能力和超分辨率，并结合基于扩散的细化组件，通过逐步去噪迭代地重建高频交通细节。

Result: 该方法在混沌交通数据集上进行了全面的评估，

Conclusion: 该方法在扩展的 rollout 预测中表现出卓越的性能，与传统的和基于 transformer 的神经算子相比，保持了高频分量，并在较长的预测范围内提高了稳定性。

Abstract: Accurate long-term traffic forecasting remains a critical challenge in
intelligent transportation systems, particularly when predicting high-frequency
traffic phenomena such as shock waves and congestion boundaries over extended
rollout horizons. Neural operators have recently gained attention as promising
tools for modeling traffic flow. While effective at learning function space
mappings, they inherently produce smooth predictions that fail to reconstruct
high-frequency features such as sharp density gradients which results in rapid
error accumulation during multi-step rollout predictions essential for
real-time traffic management. To address these fundamental limitations, we
introduce a unified Diffusion-Enhanced Transformer Neural Operator (DETNO)
architecture. DETNO leverages a transformer neural operator with
cross-attention mechanisms, providing model expressivity and super-resolution,
coupled with a diffusion-based refinement component that iteratively
reconstructs high-frequency traffic details through progressive denoising. This
overcomes the inherent smoothing limitations and rollout instability of
standard neural operators. Through comprehensive evaluation on chaotic traffic
datasets, our method demonstrates superior performance in extended rollout
predictions compared to traditional and transformer-based neural operators,
preserving high-frequency components and improving stability over long
prediction horizons.

</details>


### [93] [Quantum-Classical Hybrid Molecular Autoencoder for Advancing Classical Decoding](https://arxiv.org/abs/2508.19394)
*Afrar Jahin,Yi Pan,Yingfeng Wang,Tianming Liu,Wei Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种混合量子-经典架构用于SMILES重建，该架构集成了量子编码与经典序列建模，以提高量子保真度和经典相似性。


<details>
  <summary>Details</summary>
Motivation: 尽管量子机器学习（QML）的最新进展为增强生成模型提供了巨大的潜力，特别是在分子设计中，但大量经典方法在实现高保真度和有效性方面仍然面临挑战。特别是，QML与基于序列的任务（如SMILES字符串重建）的集成仍未得到充分探索，并且通常会遭受保真度下降的影响。

Method: 一种用于SMILES重建的混合量子-经典架构，该架构集成了量子编码与经典序列建模，以提高量子保真度和经典相似性。

Result: 我们的方法实现了大约84％的量子保真度和60％的经典重建相似性，超过了现有的量子基线。

Conclusion: 该研究为未来的量子机器学习应用奠定了有希望的基础，在表达性量子表示和经典序列模型之间取得了平衡，并促进了对分子和药物发现的量子感知序列模型的更广泛研究。

Abstract: Although recent advances in quantum machine learning (QML) offer significant
potential for enhancing generative models, particularly in molecular design, a
large array of classical approaches still face challenges in achieving high
fidelity and validity. In particular, the integration of QML with
sequence-based tasks, such as Simplified Molecular Input Line Entry System
(SMILES) string reconstruction, remains underexplored and usually suffers from
fidelity degradation. In this work, we propose a hybrid quantum-classical
architecture for SMILES reconstruction that integrates quantum encoding with
classical sequence modeling to improve quantum fidelity and classical
similarity. Our approach achieves a quantum fidelity of approximately 84% and a
classical reconstruction similarity of 60%, surpassing existing quantum
baselines. Our work lays a promising foundation for future QML applications,
striking a balance between expressive quantum representations and classical
sequence models and catalyzing broader research on quantum-aware sequence
models for molecular and drug discovery.

</details>


### [94] [Kolmogorov-Arnold Representation for Symplectic Learning: Advancing Hamiltonian Neural Networks](https://arxiv.org/abs/2508.19410)
*Zongyu Wu,Ruichen Xu,Luoyao Chen,Georgios Kementzidis,Siyao Wang,Yuefan Deng*

Main category: cs.LG

TL;DR: KAR-HNN replaces MLPs in HNNs with univariate transformations to improve stability and accuracy in modeling physical processes.


<details>
  <summary>Details</summary>
Motivation: Existing HNN implementations relying on MLPs cause hypersensitivity to hyperparameters while exploring complex energy landscapes.

Method: Replaces MLPs with univariate transformations in a Hamiltonian Neural Network (HNN).

Result: Reduces energy drift and improves long-term predictive stability; preserves the symplectic form of Hamiltonian systems.

Conclusion: KAR-HNN shows effectiveness for accurate and stable modeling of realistic physical processes.

Abstract: We propose a Kolmogorov-Arnold Representation-based Hamiltonian Neural
Network (KAR-HNN) that replaces the Multilayer Perceptrons (MLPs) with
univariate transformations. While Hamiltonian Neural Networks (HNNs) ensure
energy conservation by learning Hamiltonian functions directly from data,
existing implementations, often relying on MLPs, cause hypersensitivity to the
hyperparameters while exploring complex energy landscapes. Our approach
exploits the localized function approximations to better capture high-frequency
and multi-scale dynamics, reducing energy drift and improving long-term
predictive stability. The networks preserve the symplectic form of Hamiltonian
systems, and thus maintain interpretability and physical consistency. After
assessing KAR-HNN on four benchmark problems including spring-mass, simple
pendulum, two- and three-body problem, we foresee its effectiveness for
accurate and stable modeling of realistic physical processes often at high
dimensions and with few known parameters.

</details>


### [95] [Even Heads Fix Odd Errors: Mechanistic Discovery and Surgical Repair in Transformer Attention](https://arxiv.org/abs/2508.19414)
*Gustavo Sandoval*

Main category: cs.LG

TL;DR: Llama-3.1-8B-Instruct模型在特定格式下比较数值出错，通过调整注意力头可以修复。


<details>
  <summary>Details</summary>
Motivation: 研究Llama-3.1-8B-Instruct模型中存在的格式依赖推理失败问题。

Method: 通过系统干预，发现transformers实现了偶数/奇数注意力头分工。

Result: 发现偶数索引注意力头处理数值比较，奇数头处理不兼容的功能。需要第10层中的8个偶数头才能完全修复。SAE分析揭示了格式表示的分离和重新纠缠机制，以及特定特征在失败格式中的放大。

Conclusion: Llama-3.1-8B-Instruct模型在特定格式下错误判断数值大小，但简单格式下正确。修复该错误需要第10层中的8个偶数索引注意力头。

Abstract: We present a mechanistic case study of a format-dependent reasoning failure
in Llama-3.1-8B-Instruct, where the model incorrectly judges "9.11" as larger
than "9.8" in chat or Q&A formats, but answers correctly in simple format.
Through systematic intervention, we discover transformers implement even/odd
attention head specialization: even indexed heads handle numerical comparison,
while odd heads serve incompatible functions. The bug requires exactly 8 even
heads at Layer 10 for perfect repair. Any combination of 8+ even heads
succeeds, while 7 or fewer completely fails, revealing sharp computational
thresholds with perfect redundancy among the 16 even heads. SAE analysis
reveals the mechanism: format representations separate (10% feature overlap at
Layer 7), then re-entangle with different weightings (80% feature overlap at
Layer 10), with specific features showing 1.5x amplification in failing
formats. We achieve perfect repair using only 25% of attention heads and
identify a 60% pattern replacement threshold, demonstrating that apparent
full-module requirements hide sophisticated substructure with implications for
interpretability and efficiency. All of our code is available at
https://github.com/gussand/surgeon.

</details>


### [96] [Differentiable multiphase flow model for physics-informed machine learning in reservoir pressure management](https://arxiv.org/abs/2508.19419)
*Harun Ur Rashid,Aleksandra Pachalieva,Daniel O'Malley*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息的机器学习工作流程，该流程将可微多相流模拟器与卷积神经网络相结合，以更少的计算成本实现储层压力控制的高精度训练。


<details>
  <summary>Details</summary>
Motivation: 由于地质非均质性和多相流体流动动态，精确的地下储层压力控制极具挑战性。预测这种环境下的行为依赖于计算成本高昂的高保真物理模拟。然而，控制这些流的不确定、非均质性使得有必要进行许多这些昂贵的模拟，这通常是令人望而却步的。

Method: 结合了可微多相流模拟器（DPFEHM框架）与卷积神经网络（CNN）。CNN学习从非均质渗透率场预测流体采收率，以控制关键储层位置的压力限制。采用迁移学习加速训练，先在单相稳态模拟上预训练模型，然后在全多相场景中微调。

Result: 该方法仅用不到三千次全物理多相流模拟即可实现高精度训练，相比之前估计的需要高达一千万次的模拟，大大减少了模拟次数。这是通过利用来自不太昂贵的单相模拟的迁移学习来实现的。

Conclusion: 通过结合可微多相流模拟器和卷积神经网络，该方法能够以更少的全物理多相流模拟实现高精度训练，解决了传统方法计算成本高的问题。

Abstract: Accurate subsurface reservoir pressure control is extremely challenging due
to geological heterogeneity and multiphase fluid-flow dynamics. Predicting
behavior in this setting relies on high-fidelity physics-based simulations that
are computationally expensive. Yet, the uncertain, heterogeneous properties
that control these flows make it necessary to perform many of these expensive
simulations, which is often prohibitive. To address these challenges, we
introduce a physics-informed machine learning workflow that couples a fully
differentiable multiphase flow simulator, which is implemented in the DPFEHM
framework with a convolutional neural network (CNN). The CNN learns to predict
fluid extraction rates from heterogeneous permeability fields to enforce
pressure limits at critical reservoir locations. By incorporating transient
multiphase flow physics into the training process, our method enables more
practical and accurate predictions for realistic injection-extraction scenarios
compare to previous works. To speed up training, we pretrain the model on
single-phase, steady-state simulations and then fine-tune it on full multiphase
scenarios, which dramatically reduces the computational cost. We demonstrate
that high-accuracy training can be achieved with fewer than three thousand
full-physics multiphase flow simulations -- compared to previous estimates
requiring up to ten million. This drastic reduction in the number of
simulations is achieved by leveraging transfer learning from much less
expensive single-phase simulations.

</details>


### [97] [MS-ConTab: Multi-Scale Contrastive Learning of Mutation Signatures for Pan Cancer Representation and Stratification](https://arxiv.org/abs/2508.19424)
*Yifan Dou,Adam Khadre,Ruben C Petreaca,Golrokh Mirzaei*

Main category: cs.LG

TL;DR: Contrastive learning is used to cluster 43 cancer types, revealing meaningful relationships based on mutation data.


<details>
  <summary>Details</summary>
Motivation: Understanding the pan-cancer mutational landscape offers insights into tumorigenesis. Cohort-level clustering has largely relied on classical statistical methods.

Method: A novel unsupervised contrastive learning framework is used to cluster cancer types based on coding mutation data. Dual views (gene-level and chromosome-level profiles) are encoded using TabNet encoders and optimized via a multi-scale contrastive learning objective (NT-Xent loss).

Result: The latent representations yield biologically meaningful clusters of cancer types, aligning with known mutational processes and tissue origins.

Conclusion: This study introduces a contrastive learning framework for clustering 43 cancer types, revealing biologically meaningful clusters that align with known mutational processes and tissue origins. It represents the first application of contrastive learning to cohort-level cancer clustering.

Abstract: Motivation. Understanding the pan-cancer mutational landscape offers critical
insights into the molecular mechanisms underlying tumorigenesis. While
patient-level machine learning techniques have been widely employed to identify
tumor subtypes, cohort-level clustering, where entire cancer types are grouped
based on shared molecular features, has largely relied on classical statistical
methods.
  Results. In this study, we introduce a novel unsupervised contrastive
learning framework to cluster 43 cancer types based on coding mutation data
derived from the COSMIC database. For each cancer type, we construct two
complementary mutation signatures: a gene-level profile capturing nucleotide
substitution patterns across the most frequently mutated genes, and a
chromosome-level profile representing normalized substitution frequencies
across chromosomes. These dual views are encoded using TabNet encoders and
optimized via a multi-scale contrastive learning objective (NT-Xent loss) to
learn unified cancer-type embeddings. We demonstrate that the resulting latent
representations yield biologically meaningful clusters of cancer types,
aligning with known mutational processes and tissue origins. Our work
represents the first application of contrastive learning to cohort-level cancer
clustering, offering a scalable and interpretable framework for mutation-driven
cancer subtyping.

</details>


### [98] [Data-Augmented Few-Shot Neural Stencil Emulation for System Identification of Computer Models](https://arxiv.org/abs/2508.19441)
*Sanket Jantre,Deepak Akhare,Xiaoning Qian,Nathan M. Urban*

Main category: cs.LG

TL;DR: 提出了一种用于生成神经 PDE 训练数据的数据增强策略，该策略通过空间填充采样局部“模版”状态，从而提高了样本效率，并且在多个 PDE 系统中，与从模拟轨迹中简单采样的模版数据相比，性能有明显的提升。


<details>
  <summary>Details</summary>
Motivation: 神经偏微分方程 (PDE) 通常比传统的数值 PDE 求解器更容易区分、线性化、简化或用于不确定性量化。它们通常在通过长时间积分 PDE 求解器获得的解轨迹上进行训练。因此，我们致力于提高神经 PDE 的样本效率。

Method: 通过对局部“模版”状态进行空间填充采样，提出了一种更具样本效率的数据增强策略，用于从计算机模型生成神经 PDE 训练数据。

Result: 仅使用相当于 10 个时间步长的数值模拟的计算能力，即可学习到精确的神经 PDE 模版算子。如果我们假设可以从计算机模型访问单个完整轨迹模拟（这在实践中通常是可行的），则精度会进一步提高。

Conclusion: 通过数据增强的合成模版数据可以训练出更好的神经模版算子，与从模拟轨迹中简单采样的模版数据相比，性能有明显的提升。

Abstract: Partial differential equations (PDEs) underpin the modeling of many natural
and engineered systems. It can be convenient to express such models as neural
PDEs rather than using traditional numerical PDE solvers by replacing part or
all of the PDE's governing equations with a neural network representation.
Neural PDEs are often easier to differentiate, linearize, reduce, or use for
uncertainty quantification than the original numerical solver. They are usually
trained on solution trajectories obtained by long time integration of the PDE
solver. Here we propose a more sample-efficient data-augmentation strategy for
generating neural PDE training data from a computer model by space-filling
sampling of local "stencil" states. This approach removes a large degree of
spatiotemporal redundancy present in trajectory data and oversamples states
that may be rarely visited but help the neural PDE generalize across the state
space. We demonstrate that accurate neural PDE stencil operators can be learned
from synthetic training data generated by the computational equivalent of 10
timesteps' worth of numerical simulation. Accuracy is further improved if we
assume access to a single full-trajectory simulation from the computer model,
which is typically available in practice. Across several PDE systems, we show
that our data-augmented synthetic stencil data yield better trained neural
stencil operators, with clear performance gains compared with naively sampled
stencil data from simulation trajectories.

</details>


### [99] [Efficiently Generating Multidimensional Calorimeter Data with Tensor Decomposition Parameterization](https://arxiv.org/abs/2508.19443)
*Paimon Goulart,Shaan Pakala,Evangelos Papalexakis*

Main category: cs.LG

TL;DR: introduce an internal tensor decomposition to generative models to reduce costs of generating complex simulation data


<details>
  <summary>Details</summary>
Motivation: Producing large complex simulation datasets can often be a time and resource consuming task. Especially when these experiments are very expensive, it is becoming more reasonable to generate synthetic data for downstream tasks.

Method: an internal tensor decomposition to these generative models

Result: the generated data remains useful

Conclusion: tensor decomposition has the potential to improve efficiency in generative models, especially when generating multidimensional data, or tensors.

Abstract: Producing large complex simulation datasets can often be a time and resource
consuming task. Especially when these experiments are very expensive, it is
becoming more reasonable to generate synthetic data for downstream tasks.
Recently, these methods may include using generative machine learning models
such as Generative Adversarial Networks or diffusion models. As these
generative models improve efficiency in producing useful data, we introduce an
internal tensor decomposition to these generative models to even further reduce
costs. More specifically, for multidimensional data, or tensors, we generate
the smaller tensor factors instead of the full tensor, in order to
significantly reduce the model's output and overall parameters. This reduces
the costs of generating complex simulation data, and our experiments show the
generated data remains useful. As a result, tensor decomposition has the
potential to improve efficiency in generative models, especially when
generating multidimensional data, or tensors.

</details>


### [100] [On Surjectivity of Neural Networks: Can you elicit any behavior from your model?](https://arxiv.org/abs/2508.19445)
*Haozhe Jiang,Nika Haghtalab*

Main category: cs.LG

TL;DR: Modern neural networks are almost always surjective, meaning they can generate any output, which raises concerns about model safety and vulnerability to adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Surjectivity implies that any output, including harmful or undesirable content, can in principle be generated by the networks, raising concerns about model safety and jailbreak vulnerabilities.

Method: proving that many fundamental building blocks of modern neural architectures, such as networks with pre-layer normalization and linear-attention modules, are almost always surjective.

Result: many fundamental building blocks of modern neural architectures, such as networks with pre-layer normalization and linear-attention modules, are almost always surjective.

Conclusion: Widely used generative frameworks, including GPT-style transformers and diffusion models with deterministic ODE solvers, admit inverse mappings for arbitrary outputs.

Abstract: Given a trained neural network, can any specified output be generated by some
input? Equivalently, does the network correspond to a function that is
surjective? In generative models, surjectivity implies that any output,
including harmful or undesirable content, can in principle be generated by the
networks, raising concerns about model safety and jailbreak vulnerabilities. In
this paper, we prove that many fundamental building blocks of modern neural
architectures, such as networks with pre-layer normalization and
linear-attention modules, are almost always surjective. As corollaries, widely
used generative frameworks, including GPT-style transformers and diffusion
models with deterministic ODE solvers, admit inverse mappings for arbitrary
outputs. By studying surjectivity of these modern and commonly used neural
architectures, we contribute a formalism that sheds light on their unavoidable
vulnerability to a broad class of adversarial attacks.

</details>


### [101] [The Sample Complexity of Membership Inference and Privacy Auditing](https://arxiv.org/abs/2508.19458)
*Mahdi Haghifam,Adam Smith,Jonathan Ullman*

Main category: cs.LG

TL;DR: 研究了成员推理攻击的样本复杂度，发现攻击者可能需要比训练数据更多的参考样本才能成功攻击，实际应用中可能低估了成员推理的风险。


<details>
  <summary>Details</summary>
Motivation: 研究成员推理攻击者需要多少信息才能成功进行攻击，通过调查成功攻击所需的最小参考样本数量来实现。

Method: 研究了高斯均值估计的基本场景下的样本复杂度，其中学习算法从 $d$ 维高斯分布 $\mathcal{N}(\mu,\Sigma)$ 中获得 $n$ 个样本，并尝试估计 $\hat\mu$ 直到误差 $\mathbb{E}[\|\hat \mu - \mu\|^2_{\Sigma}]\leq \rho^2 d$。

Result: 结果表明，在这种设置下进行成员推理，可能需要 $\Omega(n + n^2 \rho^2)$ 个样本才能进行任何与充分知情的攻击者竞争的攻击。 这是第一个表明攻击者有时需要的样本比训练算法训练模型使用的样本多得多的结果。

Conclusion: 对于高斯均值估计，进行成员推理攻击所需的参考样本数量可能远超训练算法使用的样本数。实际应用中的攻击可能因为样本数量的限制而低估了成员推理的可能性，当更容易获取关于分布的信息时，可能存在更好的攻击方法。

Abstract: A membership-inference attack gets the output of a learning algorithm, and a
target individual, and tries to determine whether this individual is a member
of the training data or an independent sample from the same distribution. A
successful membership-inference attack typically requires the attacker to have
some knowledge about the distribution that the training data was sampled from,
and this knowledge is often captured through a set of independent reference
samples from that distribution. In this work we study how much information the
attacker needs for membership inference by investigating the sample
complexity-the minimum number of reference samples required-for a successful
attack. We study this question in the fundamental setting of Gaussian mean
estimation where the learning algorithm is given $n$ samples from a Gaussian
distribution $\mathcal{N}(\mu,\Sigma)$ in $d$ dimensions, and tries to estimate
$\hat\mu$ up to some error $\mathbb{E}[\|\hat \mu - \mu\|^2_{\Sigma}]\leq
\rho^2 d$. Our result shows that for membership inference in this setting,
$\Omega(n + n^2 \rho^2)$ samples can be necessary to carry out any attack that
competes with a fully informed attacker. Our result is the first to show that
the attacker sometimes needs many more samples than the training algorithm uses
to train the model. This result has significant implications for practice, as
all attacks used in practice have a restricted form that uses $O(n)$ samples
and cannot benefit from $\omega(n)$ samples. Thus, these attacks may be
underestimating the possibility of membership inference, and better attacks may
be possible when information about the distribution is easy to obtain.

</details>
