{"id": "2507.08890", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08890", "abs": "https://arxiv.org/abs/2507.08890", "authors": ["Nick Craswell", "Bhaskar Mitra", "Emine Yilmaz", "Hossein A. Rahmani", "Daniel Campos", "Jimmy Lin", "Ellen M. Voorhees", "Ian Soboroff"], "title": "Overview of the TREC 2023 deep learning track", "comment": "arXiv admin note: substantial text overlap with arXiv:2507.08191", "summary": "This is the fifth year of the TREC Deep Learning track. As in previous years,\nwe leverage the MS MARCO datasets that made hundreds of thousands of\nhuman-annotated training labels available for both passage and document ranking\ntasks. We mostly repeated last year's design, to get another matching test set,\nbased on the larger, cleaner, less-biased v2 passage and document set, with\npassage ranking as primary and document ranking as a secondary task (using\nlabels inferred from passage). As we did last year, we sample from MS MARCO\nqueries that were completely held out, unused in corpus construction, unlike\nthe test queries in the first three years. This approach yields a more\ndifficult test with more headroom for improvement. Alongside the usual MS MARCO\n(human) queries from MS MARCO, this year we generated synthetic queries using a\nfine-tuned T5 model and using a GPT-4 prompt.\n  The new headline result this year is that runs using Large Language Model\n(LLM) prompting in some way outperformed runs that use the \"nnlm\" approach,\nwhich was the best approach in the previous four years. Since this is the last\nyear of the track, future iterations of prompt-based ranking can happen in\nother tracks. Human relevance assessments were applied to all query types, not\njust human MS MARCO queries. Evaluation using synthetic queries gave similar\nresults to human queries, with system ordering agreement of $\\tau=0.8487$.\nHowever, human effort was needed to select a subset of the synthetic queries\nthat were usable. We did not see clear evidence of bias, where runs using GPT-4\nwere favored when evaluated using synthetic GPT-4 queries, or where runs using\nT5 were favored when evaluated on synthetic T5 queries.", "AI": {"tldr": "The TREC Deep Learning track's fifth year found that Large Language Model prompting outperformed previous best approaches. Synthetic queries provided similar evaluation results to human queries.", "motivation": "To get another matching test set, based on the larger, cleaner, less-biased v2 passage and document set, with passage ranking as primary and document ranking as a secondary task. The approach yields a more difficult test with more headroom for improvement.", "method": "Leveraged the MS MARCO datasets and repeated last year's design, sampling from MS MARCO queries that were completely held out. Generated synthetic queries using a fine-tuned T5 model and using a GPT-4 prompt.", "result": "Runs using LLM prompting outperformed runs using the \"nnlm\" approach. Evaluation using synthetic queries gave similar results to human queries, with system ordering agreement of 0.8487. No clear evidence of bias was observed.", "conclusion": "Runs using Large Language Model (LLM) prompting outperformed runs that use the \"nnlm\" approach. Evaluation using synthetic queries gave similar results to human queries."}}
{"id": "2507.08945", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08945", "abs": "https://arxiv.org/abs/2507.08945", "authors": ["Savini Kashmira", "Jayanaka L. Dantanarayana", "Kriszti\u00e1n Flautner", "Lingjia Tang", "Jason Mars"], "title": "GraphRunner: A Multi-Stage Framework for Efficient and Accurate Graph-Based Retrieval", "comment": null, "summary": "Conventional Retrieval Augmented Generation (RAG) approaches are common in\ntext-based applications. However, they struggle with structured, interconnected\ndatasets like knowledge graphs, where understanding underlying relationships is\ncrucial for accurate retrieval. A common direction in graph-based retrieval\nemploys iterative, rule-based traversal guided by Large Language Models (LLMs).\nSuch existing iterative methods typically combine reasoning with single hop\ntraversal at each step, making them vulnerable to LLM reasoning errors and\nhallucinations that ultimately hinder the retrieval of relevant information.\n  To address these limitations, we propose GraphRunner, a novel graph-based\nretrieval framework that operates in three distinct stages: planning,\nverification, and execution. This introduces high-level traversal actions that\nenable multi-hop exploration in a single step. It also generates a holistic\ntraversal plan, which is verified against the graph structure and pre-defined\ntraversal actions, reducing reasoning errors and detecting hallucinations\nbefore execution. GraphRunner significantly reduces LLM reasoning errors and\ndetects hallucinations through validation. Our evaluation using the GRBench\ndataset shows that GraphRunner consistently outperforms existing approaches,\nachieving 10-50% performance improvements over the strongest baseline while\nreducing inference cost by 3.0-12.9x and response generation time by 2.5-7.1x,\nmaking it significantly more robust and efficient for graph-based retrieval\ntasks.", "AI": {"tldr": "GraphRunner, a novel graph-based retrieval framework, outperforms existing approaches by reducing reasoning errors and hallucinations through planning, verification, and execution, leading to significant performance improvements and efficiency gains.", "motivation": "Conventional Retrieval Augmented Generation (RAG) approaches struggle with structured, interconnected datasets like knowledge graphs, where understanding underlying relationships is crucial for accurate retrieval. Existing iterative methods typically combine reasoning with single hop traversal at each step, making them vulnerable to LLM reasoning errors and hallucinations that ultimately hinder the retrieval of relevant information.", "method": "a novel graph-based retrieval framework that operates in three distinct stages: planning, verification, and execution. This introduces high-level traversal actions that enable multi-hop exploration in a single step. It also generates a holistic traversal plan, which is verified against the graph structure and pre-defined traversal actions", "result": "GraphRunner significantly reduces LLM reasoning errors and detects hallucinations through validation. Our evaluation using the GRBench dataset shows that GraphRunner consistently outperforms existing approaches, achieving 10-50% performance improvements over the strongest baseline while reducing inference cost by 3.0-12.9x and response generation time by 2.5-7.1x", "conclusion": "GraphRunner consistently outperforms existing approaches, achieving 10-50% performance improvements over the strongest baseline while reducing inference cost by 3.0-12.9x and response generation time by 2.5-7.1x, making it significantly more robust and efficient for graph-based retrieval tasks."}}
{"id": "2507.09090", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09090", "abs": "https://arxiv.org/abs/2507.09090", "authors": ["Anthony Miyaguchi", "Conor Johnston", "Aaryan Potdar"], "title": "DS@GT at Touch\u00e9: Large Language Models for Retrieval-Augmented Debate", "comment": null, "summary": "Large Language Models (LLMs) demonstrate strong conversational abilities. In\nthis Working Paper, we study them in the context of debating in two ways: their\nability to perform in a structured debate along with a dataset of arguments to\nuse and their ability to evaluate utterances throughout the debate. We deploy\nsix leading publicly available models from three providers for the\nRetrieval-Augmented Debate and Evaluation. The evaluation is performed by\nmeasuring four key metrics: Quality, Quantity, Manner, and Relation. Throughout\nthis task, we found that although LLMs perform well in debates when given\nrelated arguments, they tend to be verbose in responses yet consistent in\nevaluation. The accompanying source code for this paper is located at\nhttps://github.com/dsgt-arc/touche-2025-rad.", "AI": {"tldr": "LLMs can debate well with related arguments but are verbose and consistent in evaluation.", "motivation": "Study Large Language Models (LLMs) in the context of debating.", "method": "Retrieval-Augmented Debate and Evaluation with six leading publicly available models from three providers", "result": "LLMs perform well in debates when given related arguments, they tend to be verbose in responses yet consistent in evaluation.", "conclusion": "LLMs perform well in debates when given related arguments, they tend to be verbose in responses yet consistent in evaluation."}}
{"id": "2507.09188", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.09188", "abs": "https://arxiv.org/abs/2507.09188", "authors": ["Bangcheng Sun", "Yazhe Chen", "Jilin Yang", "Xiaodong Li", "Hui Li"], "title": "Retrieval-Augmented Recommendation Explanation Generation with Hierarchical Aggregation", "comment": null, "summary": "Explainable Recommender System (ExRec) provides transparency to the\nrecommendation process, increasing users' trust and boosting the operation of\nonline services. With the rise of large language models (LLMs), whose extensive\nworld knowledge and nuanced language understanding enable the generation of\nhuman-like, contextually grounded explanations, LLM-powered ExRec has gained\ngreat momentum. However, existing LLM-based ExRec models suffer from profile\ndeviation and high retrieval overhead, hindering their deployment. To address\nthese issues, we propose Retrieval-Augmented Recommendation Explanation\nGeneration with Hierarchical Aggregation (REXHA). Specifically, we design a\nhierarchical aggregation based profiling module that comprehensively considers\nuser and item review information, hierarchically summarizing and constructing\nholistic profiles. Furthermore, we introduce an efficient retrieval module\nusing two types of pseudo-document queries to retrieve relevant reviews to\nenhance the generation of recommendation explanations, effectively reducing\nretrieval latency and improving the recall of relevant reviews. Extensive\nexperiments demonstrate that our method outperforms existing approaches by up\nto 12.6% w.r.t. the explanation quality while achieving high retrieval\nefficiency.", "AI": {"tldr": "\u63d0\u51fa REXHA\uff0c\u4e00\u79cd\u7528\u4e8e\u63a8\u8350\u89e3\u91ca\u751f\u6210\u7684\u5206\u5c42\u805a\u5408\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u9ad8\u8fbe 12.6%\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u68c0\u7d22\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e LLM \u7684 ExRec \u6a21\u578b\u5b58\u5728\u753b\u50cf\u504f\u5dee\u548c\u9ad8\u68c0\u7d22\u5f00\u9500\u7684\u95ee\u9898\uff0c\u963b\u788d\u4e86\u5b83\u4eec\u7684\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u805a\u5408\u7684\u68c0\u7d22\u589e\u5f3a\u63a8\u8350\u89e3\u91ca\u751f\u6210\u65b9\u6cd5 (REXHA)\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5206\u5c42\u805a\u5408\u7684\u753b\u50cf\u6a21\u5757\uff0c\u5168\u9762\u8003\u8651\u7528\u6237\u548c\u9879\u76ee\u7684\u8bc4\u8bba\u4fe1\u606f\uff0c\u5206\u5c42\u603b\u7ed3\u548c\u6784\u5efa\u6574\u4f53\u753b\u50cf\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u68c0\u7d22\u6a21\u5757\uff0c\u4f7f\u7528\u4e24\u79cd\u7c7b\u578b\u7684\u4f2a\u6587\u6863\u67e5\u8be2\u6765\u68c0\u7d22\u76f8\u5173\u8bc4\u8bba\uff0c\u4ee5\u589e\u5f3a\u63a8\u8350\u89e3\u91ca\u7684\u751f\u6210\uff0c\u6709\u6548\u51cf\u5c11\u68c0\u7d22\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u76f8\u5173\u8bc4\u8bba\u7684\u53ec\u56de\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89e3\u91ca\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u9ad8\u8fbe 12.6%\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u68c0\u7d22\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u89e3\u91ca\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u9ad8\u8fbe 12.6%\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u68c0\u7d22\u6548\u7387\u3002"}}
{"id": "2507.09003", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.09003", "abs": "https://arxiv.org/abs/2507.09003", "authors": ["Prasoon Patidar", "Alex Crown", "Kevin Hsieh", "Yifei Xu", "Tusher Chakraborty", "Ranveer Chandra", "Yuvraj Agarwal"], "title": "Orchestration for Domain-specific Edge-Cloud Language Models", "comment": null, "summary": "The remarkable performance of Large Language Models (LLMs) has inspired many\napplications, which often necessitate edge-cloud collaboration due to\nconnectivity, privacy, and cost considerations. Traditional methods primarily\nfocus on selecting the best LLM model for optimizing performance, while\nneglecting the critical interplay between the components of the LLM serving\npipeline (context retrieval, query preprocessing, etc.) or the changing latency\nand cost constraints. We introduce ECO-LLM (Edge-Cloud Orchestrator for LLMs),\na novel system that reframes this problem as a joint optimization challenge and\nsolves it by systematically exploring component configurations and dynamically\nselecting optimal strategies at the query level. ECO-LLM consists of two\ncomponents: (1) the ECO-LLM Emulator, which efficiently explores the vast\nconfiguration space utilizing query clustering and pareto-optimal path\nselection, gathering domain-specific performance metrics without exhaustive\nevaluation; and (2) the ECO-LLM Runtime, which leverages these metrics to\ndynamically select optimal resolution strategies for user queries while meeting\nuser-defined Service Level Objectives (SLOs). We evaluate ECO-LLM on a smart\nhome and a smart car assistant scenarios. With an exhaustive exploration of all\npossible configurations for seen queries, ECO-LLM outperforms cloud-based\nmodels like GPT-4o in terms of accuracy (90% vs. 74% on average) while reducing\ncosts by 90% and latency by 55%, demonstrating the value of its joint\noptimization at the query level. In practical deployment for previously unseen\nqueries, ECO-LLM selects configurations that reduce costs by 62% or improve\nresponse times by 62% on average compared to state-of-the-art model routing\napproaches, while maintaining higher accuracy and consistently adhering to\nspecified latency and cost constraints.", "AI": {"tldr": "ECO-LLM is a system that jointly optimizes LLM components in edge-cloud collaboration, outperforming existing methods in accuracy, cost, and latency by dynamically selecting optimal strategies at the query level.", "motivation": "Traditional methods primarily focus on selecting the best LLM model for optimizing performance, while neglecting the critical interplay between the components of the LLM serving pipeline or the changing latency and cost constraints. The remarkable performance of Large Language Models (LLMs) has inspired many applications, which often necessitate edge-cloud collaboration due to connectivity, privacy, and cost considerations.", "method": "ECO-LLM consists of two components: (1) the ECO-LLM Emulator, which efficiently explores the vast configuration space utilizing query clustering and pareto-optimal path selection, gathering domain-specific performance metrics without exhaustive evaluation; and (2) the ECO-LLM Runtime, which leverages these metrics to dynamically select optimal resolution strategies for user queries while meeting user-defined Service Level Objectives (SLOs).", "result": "ECO-LLM outperforms cloud-based models like GPT-4o in terms of accuracy (90% vs. 74% on average) while reducing costs by 90% and latency by 55%. In practical deployment for previously unseen queries, ECO-LLM selects configurations that reduce costs by 62% or improve response times by 62% on average compared to state-of-the-art model routing approaches, while maintaining higher accuracy and consistently adhering to specified latency and cost constraints.", "conclusion": "ECO-LLM outperforms cloud-based models like GPT-4o in terms of accuracy while reducing costs and latency. In practical deployment for previously unseen queries, ECO-LLM selects configurations that reduce costs or improve response times on average compared to state-of-the-art model routing approaches, while maintaining higher accuracy and consistently adhering to specified latency and cost constraints."}}
{"id": "2507.08865", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08865", "abs": "https://arxiv.org/abs/2507.08865", "authors": ["Javis AI Team", "Amrendra Singh", "Maulik Shah", "Dharshan Sampath"], "title": "Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale", "comment": null, "summary": "Extracting tables and key-value pairs from financial documents is essential\nfor business workflows such as auditing, data analytics, and automated invoice\nprocessing. In this work, we introduce Spatial ModernBERT-a transformer-based\nmodel augmented with spatial embeddings-to accurately detect and extract\ntabular data and key-value fields from complex financial documents. We cast the\nextraction task as token classification across three heads: (1) Label Head,\nclassifying each token as a label (e.g., PO Number, PO Date, Item Description,\nQuantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;\n(3) Row Head, distinguishing the start of item rows and header rows. The model\nis pretrained on the PubTables-1M dataset, then fine-tuned on a financial\ndocument dataset, achieving robust performance through cross-entropy loss on\neach classification head. We propose a post-processing method to merge tokens\nusing B-I-IB tagging, reconstruct the tabular layout, and extract key-value\npairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages\nboth textual and spatial cues, facilitating highly accurate table and key-value\nextraction in real-world financial documents.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Spatial ModernBERT\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u7a7a\u95f4\u5d4c\u5165\u8fdb\u884c\u589e\u5f3a\uff0c\u53ef\u4ee5\u51c6\u786e\u5730\u68c0\u6d4b\u548c\u63d0\u53d6\u590d\u6742\u8d22\u52a1\u6587\u6863\u4e2d\u7684\u8868\u683c\u6570\u636e\u548c\u952e\u503c\u5b57\u6bb5\u3002", "motivation": "\u4ece\u8d22\u52a1\u6587\u6863\u4e2d\u63d0\u53d6\u8868\u683c\u548c\u952e\u503c\u5bf9\u5bf9\u4e8e\u5ba1\u8ba1\u3001\u6570\u636e\u5206\u6790\u548c\u81ea\u52a8\u53d1\u7968\u5904\u7406\u7b49\u4e1a\u52a1\u5de5\u4f5c\u6d41\u7a0b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u4f7f\u7528B-I-IB\u6807\u8bb0\u5408\u5e76\u4ee4\u724c\uff0c\u91cd\u5efa\u8868\u683c\u5e03\u5c40\uff0c\u5e76\u63d0\u53d6\u952e\u503c\u5bf9\u3002\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cSpatial ModernBERT\u6709\u6548\u5730\u5229\u7528\u4e86\u6587\u672c\u548c\u7a7a\u95f4\u7ebf\u7d22\uff0c\u6709\u52a9\u4e8e\u5728\u5b9e\u9645\u8d22\u52a1\u6587\u4ef6\u4e2d\u8fdb\u884c\u9ad8\u5ea6\u51c6\u786e\u7684\u8868\u683c\u548c\u952e\u503c\u63d0\u53d6\u3002", "result": "Spatial ModernBERT\u5728\u8d22\u52a1\u6587\u6863\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u901a\u8fc7\u6bcf\u4e2a\u5206\u7c7b\u5934\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u6027\u80fd\u3002", "conclusion": "Spatial ModernBERT\u6a21\u578b\u80fd\u591f\u6709\u6548\u5229\u7528\u6587\u672c\u548c\u7a7a\u95f4\u7ebf\u7d22\uff0c\u4ece\u800c\u5728\u5b9e\u9645\u8d22\u52a1\u6587\u4ef6\u4e2d\u5b9e\u73b0\u9ad8\u5ea6\u51c6\u786e\u7684\u8868\u683c\u548c\u952e\u503c\u63d0\u53d6\u3002"}}
{"id": "2507.08828", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.08828", "abs": "https://arxiv.org/abs/2507.08828", "authors": ["Tarek Berghout"], "title": "Recurrent Expansion: A Pathway Toward the Next Generation of Deep Learning", "comment": null, "summary": "This paper introduces Recurrent Expansion (RE) as a new learning paradigm\nthat advances beyond conventional Machine Learning (ML) and Deep Learning (DL).\nWhile DL focuses on learning from static data representations, RE proposes an\nadditional dimension: learning from the evolving behavior of models themselves.\nRE emphasizes multiple mappings of data through identical deep architectures\nand analyzes their internal representations (i.e., feature maps) in conjunction\nwith observed performance signals such as loss. By incorporating these\nbehavioral traces, RE enables iterative self-improvement, allowing each model\nversion to gain insight from its predecessors. The framework is extended\nthrough Multiverse RE (MVRE), which aggregates signals from parallel model\ninstances, and further through Heterogeneous MVRE (HMVRE), where models of\nvarying architectures contribute diverse perspectives. A scalable and adaptive\nvariant, Sc-HMVRE, introduces selective mechanisms and scale diversity for\nreal-world deployment. Altogether, RE presents a shift in DL: from purely\nrepresentational learning to behavior-aware, self-evolving systems. It lays the\ngroundwork for a new class of intelligent models capable of reasoning over\ntheir own learning dynamics, offering a path toward scalable, introspective,\nand adaptive artificial intelligence. A simple code example to support\nbeginners in running their own experiments is provided in Code Availability\nSection of this paper.", "AI": {"tldr": "RE is a new learning paradigm that advances beyond conventional ML and DL by learning from the evolving behavior of models themselves, enabling iterative self-improvement.", "motivation": "conventional Machine Learning (ML) and Deep Learning (DL) focuses on learning from static data representations", "method": "Recurrent Expansion (RE), Multiverse RE (MVRE), Heterogeneous MVRE (HMVRE), Scalable and adaptive variant, Sc-HMVRE", "result": "RE enables iterative self-improvement, allowing each model version to gain insight from its predecessors.", "conclusion": "RE presents a shift in DL: from purely representational learning to behavior-aware, self-evolving systems. It lays the groundwork for a new class of intelligent models capable of reasoning over their own learning dynamics, offering a path toward scalable, introspective, and adaptive artificial intelligence."}}
{"id": "2507.08806", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08806", "abs": "https://arxiv.org/abs/2507.08806", "authors": ["Daewon Choi", "Jimin Lee", "Jihoon Tack", "Woomin Song", "Saket Dingliwal", "Sai Muralidhar Jayanthi", "Bhavana Ganesh", "Jinwoo Shin", "Aram Galstyan", "Sravan Babu Bodapati"], "title": "Think Clearly: Improving Reasoning via Redundant Token Pruning", "comment": null, "summary": "Recent large language models have shown promising capabilities in long-form\nreasoning, following structured chains of thought before arriving at a final\nanswer. However, we observe that these reasoning paths tend to include\nsubstantial redundancy; analyzing attention patterns reveals that attention\nscores are widely scattered, particularly incorrect answers exhibit greater\nattention sparsity. In this paper, we demonstrate that deliberately removing\nthis redundancy in the reasoning process significantly improves performance\nthrough clear thinking, i.e., removing distraction. Specifically, we\nsystematically identify reasoning redundancy by measuring token-level attention\nscores to a special end-of-thinking token, which is appended to an explicit\ninstruction inserted to conclude each intermediate reasoning step. Furthermore,\nwe propose structure-aware pruning that prioritizes removing tokens in\nlow-contributing reasoning chunks over individual tokens. After evicting\nredundant tokens, we remove the injected end-of-thinking instruction, then\nresume the reasoning generation. We demonstrate that our method significantly\nimproves overall accuracy across reasoning-intensive benchmarks without any\ntraining involved. In particular, our method shows strong performance on\nchallenging mathematical competition benchmarks such as AIME and AMC, where\nreasoning redundancy is more prevalent.", "AI": {"tldr": "\u901a\u8fc7\u79fb\u9664\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5197\u4f59\u6765\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u6570\u5b66\u7ade\u8d5b\u7b49\u9700\u8981\u5927\u91cf\u63a8\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u7a0b\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u63a8\u7406\u8def\u5f84\u5f80\u5f80\u5305\u542b\u5927\u91cf\u5197\u4f59\uff0c\u6ce8\u610f\u529b\u6a21\u5f0f\u5206\u6563\uff0c\u5c24\u5176\u662f\u4e0d\u6b63\u786e\u7684\u7b54\u6848\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6ce8\u610f\u529b\u7a00\u758f\u6027\u3002", "method": "\u901a\u8fc7\u6d4b\u91cftoken\u7ea7\u522b\u5bf9\u7279\u6b8aend-of-thinking token\u7684\u6ce8\u610f\u529b\u5206\u6570\u6765\u7cfb\u7edf\u5730\u8bc6\u522b\u63a8\u7406\u5197\u4f59\uff0c\u5e76\u63d0\u51fa\u7ed3\u6784\u611f\u77e5\u7684\u526a\u679d\u65b9\u6cd5\uff0c\u4f18\u5148\u79fb\u9664\u4f4e\u8d21\u732e\u7684\u63a8\u7406\u5757\u4e2d\u7684token\u3002", "result": "\u5728\u4e0d\u9700\u8981\u4efb\u4f55\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u7684\u6574\u4f53\u51c6\u786e\u6027\u3002", "conclusion": "\u901a\u8fc7\u79fb\u9664\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5197\u4f59\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u5927\u91cf\u63a8\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5982AIME\u548cAMC\u7b49\u6570\u5b66\u7ade\u8d5b\u3002"}}
{"id": "2507.08831", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08831", "abs": "https://arxiv.org/abs/2507.08831", "authors": ["Josh Qixuan Sun", "Xiaoying Xing", "Huaiyuan Weng", "Chul Min Yeum", "Mark Crowley"], "title": "View Invariant Learning for Vision-Language Navigation in Continuous Environments", "comment": "Under review", "summary": "Vision-Language Navigation in Continuous Environments (VLNCE), where an agent\nfollows instructions and moves freely to reach a destination, is a key research\nproblem in embodied AI. However, most navigation policies are sensitive to\nviewpoint changes, i.e., variations in camera height and viewing angle that\nalter the agent's observation. In this paper, we introduce a generalized\nscenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View\nInvariant Learning), a view-invariant post-training strategy that enhances the\nrobustness of existing navigation policies to changes in camera viewpoint. VIL\nemploys a contrastive learning framework to learn sparse and view-invariant\nfeatures. Additionally, we introduce a teacher-student framework for the\nWaypoint Predictor Module, a core component of most VLNCE baselines, where a\nview-dependent teacher model distills knowledge into a view-invariant student\nmodel. We employ an end-to-end training paradigm to jointly optimize these\ncomponents, thus eliminating the cost for individual module training. Empirical\nresults show that our method outperforms state-of-the-art approaches on\nV2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets\nR2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE\nsetting and find that, despite being trained for varied viewpoints, it often\nstill improves performance. On the more challenging RxR-CE dataset, our method\nalso achieved state-of-the-art performance across all metrics when compared to\nother map-free methods. This suggests that adding VIL does not diminish the\nstandard viewpoint performance and can serve as a plug-and-play post-training\nmethod.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VIL\uff0c\u4e00\u79cd\u89c6\u89d2\u4e0d\u53d8\u7684\u540e\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u73b0\u6709\u5bfc\u822a\u7b56\u7565\u5bf9\u76f8\u673a\u89c6\u89d2\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5927\u591a\u6570\u5bfc\u822a\u7b56\u7565\u5bf9\u89c6\u89d2\u53d8\u5316\u654f\u611f\uff0c\u5373\u76f8\u673a\u9ad8\u5ea6\u548c\u89c6\u89d2\u7684\u53d8\u5316\u4f1a\u6539\u53d8\u667a\u80fd\u4f53\u7684\u89c2\u5bdf\u7ed3\u679c\u3002\u56e0\u6b64\uff0c\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5e7f\u4e49\u7684\u573a\u666fV2-VLNCE\uff08\u5177\u6709\u4e0d\u540c\u89c6\u89d2\u7684VLNCE\uff09\u3002", "method": "VIL\uff0c\u4e00\u79cd\u89c6\u89d2\u4e0d\u53d8\u7684\u540e\u8bad\u7ec3\u7b56\u7565\uff0c\u5b83\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u6765\u5b66\u4e60\u7a00\u758f\u548c\u89c6\u89d2\u4e0d\u53d8\u7684\u7279\u5f81\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4e3a\u822a\u70b9\u9884\u6d4b\u5668\u6a21\u5757\u5f15\u5165\u4e86\u4e00\u4e2a\u5e08\u751f\u6846\u67b6\uff0c\u5176\u4e2d\u4e00\u4e2a\u4f9d\u8d56\u4e8e\u89c6\u89d2\u7684\u6559\u5e08\u6a21\u578b\u5c06\u77e5\u8bc6\u63d0\u70bc\u6210\u4e00\u4e2a\u89c6\u89d2\u4e0d\u53d8\u7684\u5b66\u751f\u6a21\u578b\u3002\u6211\u4eec\u91c7\u7528\u7aef\u5230\u7aef\u7684\u8bad\u7ec3\u8303\u5f0f\u6765\u5171\u540c\u4f18\u5316\u8fd9\u4e9b\u7ec4\u4ef6\u3002", "result": "VIL\u5728V2-VLNCE\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u6c34\u5e73\u7684\u65b9\u6cd58-15%\u3002\u5728\u66f4\u5177\u6311\u6218\u6027\u7684RxR-CE\u6570\u636e\u96c6\u4e0a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6240\u6709\u6307\u6807\u4e0a\u4e5f\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "VIL\u5728V2-VLNCE\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u6c34\u5e73\u7684\u65b9\u6cd58-15%\uff0c\u5728RxR-CE\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2507.09331", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.09331", "abs": "https://arxiv.org/abs/2507.09331", "authors": ["Kirill Khrylchenko", "Vladimir Baikalov", "Sergei Makeev", "Artem Matveev", "Sergei Liamaev"], "title": "Correcting the LogQ Correction: Revisiting Sampled Softmax for Large-Scale Retrieval", "comment": "Accepted at ACM RecSys 2025. Author's version. To appear in the\n  Proceedings of the 18th ACM Conference on Recommender Systems", "summary": "Two-tower neural networks are a popular architecture for the retrieval stage\nin recommender systems. These models are typically trained with a softmax loss\nover the item catalog. However, in web-scale settings, the item catalog is\noften prohibitively large, making full softmax infeasible. A common solution is\nsampled softmax, which approximates the full softmax using a small number of\nsampled negatives.\n  One practical and widely adopted approach is to use in-batch negatives, where\nnegatives are drawn from items in the current mini-batch. However, this\nintroduces a bias: items that appear more frequently in the batch (i.e.,\npopular items) are penalized more heavily.\n  To mitigate this issue, a popular industry technique known as logQ correction\nadjusts the logits during training by subtracting the log-probability of an\nitem appearing in the batch. This correction is derived by analyzing the bias\nin the gradient and applying importance sampling, effectively twice, using the\nin-batch distribution as a proposal distribution. While this approach improves\nmodel quality, it does not fully eliminate the bias.\n  In this work, we revisit the derivation of logQ correction and show that it\noverlooks a subtle but important detail: the positive item in the denominator\nis not Monte Carlo-sampled - it is always present with probability 1. We\npropose a refined correction formula that accounts for this. Notably, our loss\nintroduces an interpretable sample weight that reflects the model's uncertainty\n- the probability of misclassification under the current parameters. We\nevaluate our method on both public and proprietary datasets, demonstrating\nconsistent improvements over the standard logQ correction.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684logQ\u6821\u6b63\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u4f7f\u7528batch\u5185\u8d1f\u91c7\u6837\u65f6\u5f15\u5165\u7684\u504f\u5dee\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u53cc\u5854\u795e\u7ecf\u7f51\u7edc\u5e38\u7528\u4e8e\u68c0\u7d22\u9636\u6bb5\uff0c\u4f46\u5f53\u9879\u76ee\u76ee\u5f55\u975e\u5e38\u5927\u65f6\uff0c\u5168softmax\u53d8\u5f97\u4e0d\u53ef\u884c\u3002\u5e38\u7528\u7684\u89e3\u51b3\u65b9\u6cd5\u662f\u91c7\u6837softmax\uff0c\u4f46\u4f7f\u7528batch\u5185\u8d1f\u6837\u672c\u4f1a\u5f15\u5165\u504f\u5dee\uff0c\u5373\u5728batch\u4e2d\u51fa\u73b0\u9891\u7387\u8f83\u9ad8\u7684\u9879\u76ee\uff08\u5373\u70ed\u95e8\u9879\u76ee\uff09\u4f1a\u53d7\u5230\u66f4\u91cd\u7684\u60e9\u7f5a\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5de5\u4e1a\u754c\u5e7f\u6cdb\u91c7\u7528\u4e00\u79cd\u79f0\u4e3alogQ\u6821\u6b63\u7684\u6280\u672f\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5e76\u4e0d\u80fd\u5b8c\u5168\u6d88\u9664\u504f\u5dee\u3002", "method": "\u901a\u8fc7\u91cd\u65b0\u5ba1\u89c6logQ\u6821\u6b63\u7684\u63a8\u5bfc\u8fc7\u7a0b\uff0c\u53d1\u73b0\u5176\u5ffd\u7565\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u7ec6\u8282\uff1a\u5206\u6bcd\u4e2d\u7684\u6b63\u6837\u672c\u4e0d\u662f\u8499\u7279\u5361\u6d1b\u91c7\u6837\u7684\uff0c\u800c\u662f\u4ee5\u6982\u73871\u5b58\u5728\u7684\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6539\u8fdb\u7684\u6821\u6b63\u516c\u5f0f\u3002", "result": "\u5728\u516c\u5171\u548c\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u6807\u51c6\u7684logQ\u6821\u6b63\u5177\u6709\u6301\u7eed\u7684\u6539\u8fdb\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684logQ\u6821\u6b63\u516c\u5f0f\uff0c\u8be5\u516c\u5f0f\u8003\u8651\u4e86\u6b63\u6837\u672c\u5728\u5206\u6bcd\u4e2d\u7684\u7279\u6b8a\u6027\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u6837\u672c\u6743\u91cd\uff0c\u8be5\u6743\u91cd\u53cd\u6620\u4e86\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2507.09138", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09138", "abs": "https://arxiv.org/abs/2507.09138", "authors": ["Zhengding Hu", "Vibha Murthy", "Zaifeng Pan", "Wanlu Li", "Xiaoyi Fang", "Yufei Ding", "Yuke Wang"], "title": "HedraRAG: Coordinating LLM Generation and Database Retrieval in Heterogeneous RAG Serving", "comment": "Accepted by SOSP 2025", "summary": "This paper addresses emerging system-level challenges in heterogeneous\nretrieval-augmented generation (RAG) serving, where complex multi-stage\nworkflows and diverse request patterns complicate efficient execution. We\npresent HedraRAG, a runtime system built on a graph-based abstraction that\nexposes optimization opportunities across stage-level parallelism,\nintra-request similarity, and inter-request skewness. These opportunities are\nrealized through dynamic graph transformations, such as node splitting,\nreordering, edge addition, and dependency rewiring, applied to wavefronts of\nsubgraphs spanning concurrent requests. The resulting execution plans are\nmapped onto hybrid CPU-GPU pipelines to improve resource utilization and reduce\nlatency. Evaluations across a wide range of RAG workflows demonstrate speedups\nexceeding 1.5x and reaching up to 5x over existing frameworks, showcasing the\neffectiveness of coordinated generation and retrieval in serving environments.", "AI": {"tldr": "Addresses challenges in heterogeneous RAG serving with HedraRAG, a graph-based runtime system that optimizes execution through dynamic graph transformations and hybrid CPU-GPU pipelines, achieving significant speedups.", "motivation": "emerging system-level challenges in heterogeneous retrieval-augmented generation (RAG) serving, where complex multi-stage workflows and diverse request patterns complicate efficient execution", "method": "a runtime system built on a graph-based abstraction that exposes optimization opportunities across stage-level parallelism, intra-request similarity, and inter-request skewness. These opportunities are realized through dynamic graph transformations, such as node splitting, reordering, edge addition, and dependency rewiring, applied to wavefronts of subgraphs spanning concurrent requests. The resulting execution plans are mapped onto hybrid CPU-GPU pipelines", "result": "speedups exceeding 1.5x and reaching up to 5x over existing frameworks", "conclusion": "Evaluations across a wide range of RAG workflows demonstrate speedups exceeding 1.5x and reaching up to 5x over existing frameworks, showcasing the effectiveness of coordinated generation and retrieval in serving environments."}}
{"id": "2507.08898", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08898", "abs": "https://arxiv.org/abs/2507.08898", "authors": ["Wenliang Shan", "Michael Fu", "Rui Yang", "Chakkrit", "Tantithamthavorn"], "title": "SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems", "comment": "Under Review at Information and Software Technology", "summary": "Safety alignment is critical for LLM-powered systems. While recent\nLLM-powered guardrail approaches such as LlamaGuard achieve high detection\naccuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),\nthey struggle with multilingual unsafe inputs. This limitation leaves LLM\nsystems vulnerable to unsafe and jailbreak prompts written in low-resource\nlanguages such as those in Southeast Asia. This paper introduces SEALGuard, a\nmultilingual guardrail designed to improve the safety alignment across diverse\nlanguages. It aims to address the multilingual safety alignment gap of existing\nguardrails and ensure effective filtering of unsafe and jailbreak prompts in\nLLM-powered systems. We adapt a general-purpose multilingual language model\ninto a multilingual guardrail using low-rank adaptation (LoRA). We construct\nSEALSBench, a large-scale multilingual safety alignment dataset containing over\n260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.\nWe evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on\nthis benchmark. Our findings show that multilingual unsafe and jailbreak\nprompts substantially degrade the performance of the state-of-the-art\nLlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and\n18%, respectively, compared to its performance on English-only prompts. In\ncontrast, SEALGuard outperforms existing guardrails in detecting multilingual\nunsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and\nachieving the best DSR, precision, and F1-score. Our ablation study further\nreveals the contributions of adaptation strategies and model size to the\noverall performance of SEALGuard. SEALGuard advances the safety alignment of\nLLM systems by introducing an effective multilingual guardrail.", "AI": {"tldr": "SEALGuard\u662f\u4e00\u79cd\u591a\u8bed\u8a00guardrail\uff0c\u901a\u8fc7\u5728\u5305\u542b\u5341\u79cd\u8bed\u8a00\u7684SEALSBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528LoRA\u8c03\u6574\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86LLM\u7cfb\u7edf\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u7684LLM guardrail\u65b9\u6cd5\u5728\u5904\u7406\u591a\u8bed\u8a00\u4e0d\u5b89\u5168\u8f93\u5165\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u4e1c\u5357\u4e9a\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u3002", "method": "\u4f7f\u7528\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u5c06\u901a\u7528\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\u8c03\u6574\u4e3a\u591a\u8bed\u8a00guardrail\u3002", "result": "SEALGuard\u5728\u68c0\u6d4b\u591a\u8bed\u8a00\u4e0d\u5b89\u5168\u548cjailbreak\u63d0\u793a\u65b9\u9762\u4f18\u4e8e\u73b0\u6709guardrail\uff0c\u4e0eLlamaGuard\u76f8\u6bd4\uff0cDSR\u63d0\u9ad8\u4e8648%\uff0c\u5e76\u5b9e\u73b0\u4e86\u6700\u4f73\u7684DSR\u3001\u7cbe\u786e\u7387\u548cF1\u5206\u6570\u3002\u591a\u8bed\u8a00\u4e0d\u5b89\u5168\u548cjailbreak\u63d0\u793a\u4f1a\u663e\u8457\u964d\u4f4eLlamaGuard\u7684\u6027\u80fd\uff0c\u4e0e\u4ec5\u82f1\u8bed\u63d0\u793a\u76f8\u6bd4\uff0c\u9632\u5fa1\u6210\u529f\u7387\uff08DSR\uff09\u5206\u522b\u4e0b\u964d\u4e869%\u548c18%\u3002", "conclusion": "SEALGuard\u901a\u8fc7\u5f15\u5165\u6709\u6548\u7684\u591a\u8bed\u8a00guardrail\uff0c\u63d0\u9ad8\u4e86LLM\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u5bf9\u9f50\u3002"}}
{"id": "2507.08829", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08829", "abs": "https://arxiv.org/abs/2507.08829", "authors": ["Kimia Soroush", "Nastaran Shirazi", "Mohsen Raji"], "title": "Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI", "comment": null, "summary": "Deep Neural Networks (DNNs) are widely employed in safety-critical domains,\nwhere ensuring their reliability is essential. Triple Modular Redundancy (TMR)\nis an effective technique to enhance the reliability of DNNs in the presence of\nbit-flip faults. In order to handle the significant overhead of TMR, it is\napplied selectively on the parameters and components with the highest\ncontribution at the model output. Hence, the accuracy of the selection\ncriterion plays the key role on the efficiency of TMR. This paper presents an\nefficient TMR approach to enhance the reliability of DNNs against bit-flip\nfaults using an Explainable Artificial Intelligence (XAI) method. Since XAI can\nprovide valuable insights about the importance of individual neurons and\nweights in the performance of the network, they can be applied as the selection\nmetric in TMR techniques. The proposed method utilizes a low-cost,\ngradient-based XAI technique known as Layer-wise Relevance Propagation (LRP) to\ncalculate importance scores for DNN parameters. These scores are then used to\nenhance the reliability of the model, with the most critical weights being\nprotected by TMR. The proposed approach is evaluated on two DNN models, VGG16\nand AlexNet, using datasets such as MNIST and CIFAR-10. The results demonstrate\nthat the method can protect the AlexNet model at a bit error rate of 10-4,\nachieving over 60% reliability improvement while maintaining the same overhead\nas state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684 TMR \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd (XAI) \u65b9\u6cd5\u6765\u589e\u5f3a DNN \u62b5\u6297\u4f4d\u7ffb\u8f6c\u9519\u8bef\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u4e2d\uff0c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (DNN) \u5f97\u5230\u4e86\u5e7f\u6cdb\u5e94\u7528\uff0c\u786e\u4fdd\u5176\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u4e09\u91cd\u6a21\u5757\u5197\u4f59 (TMR) \u662f\u4e00\u79cd\u6709\u6548\u7684\u6280\u672f\uff0c\u53ef\u4ee5\u589e\u5f3a DNN \u5728\u5b58\u5728\u4f4d\u7ffb\u8f6c\u6545\u969c\u65f6\u7684\u53ef\u9760\u6027\u3002\u4e3a\u4e86\u5904\u7406 TMR \u7684\u5de8\u5927\u5f00\u9500\uff0c\u5b83\u6709\u9009\u62e9\u5730\u5e94\u7528\u4e8e\u6a21\u578b\u8f93\u51fa\u4e2d\u8d21\u732e\u6700\u9ad8\u7684\u53c2\u6570\u548c\u7ec4\u4ef6\u3002", "method": "\u5229\u7528\u4e00\u79cd\u4f4e\u6210\u672c\u7684\u3001\u57fa\u4e8e\u68af\u5ea6\u7684 XAI \u6280\u672f\uff0c\u5373\u9010\u5c42\u76f8\u5173\u6027\u4f20\u64ad (LRP) \u6765\u8ba1\u7b97 DNN \u53c2\u6570\u7684\u91cd\u8981\u6027\u5f97\u5206\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a DNN \u6a21\u578b VGG16 \u548c AlexNet \u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e86 MNIST \u548c CIFAR-10 \u7b49\u6570\u636e\u96c6\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728 10-4 \u7684\u8bef\u7801\u7387\u4e0b\u4fdd\u62a4 AlexNet \u6a21\u578b\uff0c\u5728\u4fdd\u6301\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u540c\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u9760\u6027\u63d0\u9ad8 60% \u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u6301\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u540c\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06 AlexNet \u6a21\u578b\u5728 10-4 \u7684\u8bef\u7801\u7387\u4e0b\uff0c\u53ef\u9760\u6027\u63d0\u9ad8 60% \u4ee5\u4e0a\u3002"}}
{"id": "2507.08875", "categories": ["cs.AI", "90B50, 90C29, 90C08, 91A80, 91B06"], "pdf": "https://arxiv.org/pdf/2507.08875", "abs": "https://arxiv.org/abs/2507.08875", "authors": ["Fuh-Hwa Franklin Liu", "Su-Chuan Shih"], "title": "A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data", "comment": "38 pages, 6 figures, 5 table. A practice applicable method for\n  multi-criteria assessments using cardinal and ordinal data", "summary": "Modern methods for multi-criteria assessment (MCA), such as Data Envelopment\nAnalysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria\nDecision-Making (MCDM), are utilized to appraise a collection of\nDecision-Making Units (DMUs), also known as alternatives, based on several\ncriteria. These methodologies inherently rely on assumptions and can be\ninfluenced by subjective judgment to effectively tackle the complex evaluation\nchallenges in various fields. In real-world scenarios, it is essential to\nincorporate both quantitative and qualitative criteria as they consist of\ncardinal and ordinal data. Despite the inherent variability in the criterion\nvalues of different alternatives, the homogeneity assumption is often employed,\nsignificantly affecting evaluations. To tackle these challenges and determine\nthe most appropriate alternative, we propose a novel MCA approach that combines\ntwo Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear\nprogramming, is pivotal in the MCA methodology. This approach improves\nefficiency and fairness, ensuring that evaluations are both comprehensive and\ndependable, thus offering a strong and adaptive solution. Two comprehensive\nnumerical examples demonstrate the accuracy and transparency of our proposed\nmethod. The goal is to encourage continued advancement and stimulate progress\nin automated decision systems and decision support systems.", "AI": {"tldr": "Proposes a new Multi-Criteria Assessment (MCA) method using Virtual Gap Analysis (VGA) to improve decision-making by addressing limitations in existing methods.", "motivation": "Modern MCA methods rely on assumptions and can be influenced by subjective judgment. The homogeneity assumption is often employed, significantly affecting evaluations. Real-world scenarios require incorporating both quantitative and qualitative criteria.", "method": "The paper proposes a novel MCA approach that combines two Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear programming, is pivotal in the MCA methodology.", "result": "Two comprehensive numerical examples demonstrate the accuracy and transparency of the proposed method. The approach improves efficiency and fairness, ensuring that evaluations are both comprehensive and dependable.", "conclusion": "The paper introduces a novel MCA approach combining two Virtual Gap Analysis (VGA) models to improve efficiency and fairness in evaluations, offering a strong and adaptive solution."}}
{"id": "2507.08917", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08917", "abs": "https://arxiv.org/abs/2507.08917", "authors": ["Justin D. Norman", "Hany Farid"], "title": "Detecting Deepfake Talking Heads from Facial Biometric Anomalies", "comment": "10 pages, 3 figures, 3 tables", "summary": "The combination of highly realistic voice cloning, along with visually\ncompelling avatar, face-swap, or lip-sync deepfake video generation, makes it\nrelatively easy to create a video of anyone saying anything. Today, such\ndeepfake impersonations are often used to power frauds, scams, and political\ndisinformation. We propose a novel forensic machine learning technique for the\ndetection of deepfake video impersonations that leverages unnatural patterns in\nfacial biometrics. We evaluate this technique across a large dataset of\ndeepfake techniques and impersonations, as well as assess its reliability to\nvideo laundering and its generalization to previously unseen video deepfake\ngenerators.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7528\u4e8e\u68c0\u6d4bdeepfake\u89c6\u9891\u7684\u53d6\u8bc1\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u8be5\u6280\u672f\u5229\u7528\u9762\u90e8\u751f\u7269\u7279\u5f81\u4e2d\u7684\u975e\u81ea\u7136\u6a21\u5f0f\uff0c\u5e76\u5728\u5927\u578b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "motivation": "\u903c\u771f\u7684\u58f0\u97f3\u514b\u9686\u548c\u5f15\u4eba\u6ce8\u76ee\u7684\u5934\u50cf\u3001\u6362\u8138\u6216\u5507\u5f62\u540c\u6b65deepfake\u89c6\u9891\u751f\u6210\u6280\u672f\u7684\u7ed3\u5408\uff0c\u4f7f\u5f97\u5236\u4f5c\u4efb\u4f55\u4eba\u8bf4\u4efb\u4f55\u8bdd\u7684\u89c6\u9891\u76f8\u5bf9\u5bb9\u6613\u3002\u5982\u4eca\uff0c\u8fd9\u79cddeepfake\u6a21\u4eff\u7ecf\u5e38\u88ab\u7528\u4e8e\u8bc8\u9a97\u3001\u6b3a\u8bc8\u548c\u653f\u6cbb\u865a\u5047\u4fe1\u606f\u3002", "method": "\u5229\u7528\u9762\u90e8\u751f\u7269\u7279\u5f81\u4e2d\u7684\u975e\u81ea\u7136\u6a21\u5f0f\u3002", "result": "\u5bf9\u5927\u578bdeepfake\u6280\u672f\u548c\u6a21\u4eff\u6570\u636e\u96c6\u8bc4\u4f30\u4e86\u8be5\u6280\u672f\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u5bf9\u89c6\u9891\u6e05\u6d17\u7684\u53ef\u9760\u6027\u53ca\u5176\u5bf9\u4ee5\u524d\u672a\u89c1\u8fc7\u7684\u89c6\u9891deepfake\u751f\u6210\u5668\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7528\u4e8e\u68c0\u6d4bdeepfake\u89c6\u9891\u4f2a\u88c5\u7684\u53d6\u8bc1\u673a\u5668\u5b66\u4e60\u6280\u672f\u3002"}}
{"id": "2507.09403", "categories": ["cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.09403", "abs": "https://arxiv.org/abs/2507.09403", "authors": ["Amit Jaspal", "Feng Zhang", "Wei Chang", "Sumit Kumar", "Yubo Wang", "Roni Mittleman", "Qifan Wang", "Weize Mao"], "title": "Balancing Semantic Relevance and Engagement in Related Video Recommendations", "comment": null, "summary": "Related video recommendations commonly use collaborative filtering (CF)\ndriven by co-engagement signals, often resulting in recommendations lacking\nsemantic coherence and exhibiting strong popularity bias. This paper introduces\na novel multi-objective retrieval framework, enhancing standard two-tower\nmodels to explicitly balance semantic relevance and user engagement. Our\napproach uniquely combines: (a) multi-task learning (MTL) to jointly optimize\nco-engagement and semantic relevance, explicitly prioritizing topical\ncoherence; (b) fusion of multimodal content features (textual and visual\nembeddings) for richer semantic understanding; and (c) off-policy correction\n(OPC) via inverse propensity weighting to effectively mitigate popularity bias.\nEvaluation on industrial-scale data and a two-week live A/B test reveals our\nframework's efficacy. We observed significant improvements in semantic\nrelevance (from 51% to 63% topic match rate), a reduction in popular item\ndistribution (-13.8% popular video recommendations), and a +0.04% improvement\nin our topline user engagement metric. Our method successfully achieves better\nsemantic coherence, balanced engagement, and practical scalability for\nreal-world deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u76f8\u5173\u89c6\u9891\u63a8\u8350\u7684\u591a\u76ee\u6807\u68c0\u7d22\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u3001\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u548c\u975e\u7b56\u7565\u6821\u6b63\u6765\u5e73\u8861\u8bed\u4e49\u76f8\u5173\u6027\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u4ece\u800c\u63d0\u9ad8\u8bed\u4e49\u4e00\u81f4\u6027\u5e76\u51cf\u5c11\u70ed\u95e8\u504f\u5dee\u3002", "motivation": "\u76f8\u5173\u89c6\u9891\u63a8\u8350\u901a\u5e38\u4f7f\u7528\u534f\u4f5c\u8fc7\u6ee4 (CF)\uff0c\u7531\u5171\u540c\u53c2\u4e0e\u4fe1\u53f7\u9a71\u52a8\uff0c\u901a\u5e38\u5bfc\u81f4\u63a8\u8350\u7f3a\u4e4f\u8bed\u4e49\u4e00\u81f4\u6027\u5e76\u8868\u73b0\u51fa\u5f88\u5f3a\u7684\u53d7\u6b22\u8fce\u7a0b\u5ea6\u504f\u5dee\u3002", "method": "\u4e00\u79cd\u65b0\u9896\u7684\u591a\u76ee\u6807\u68c0\u7d22\u6846\u67b6\uff0c\u589e\u5f3a\u4e86\u6807\u51c6\u53cc\u5854\u6a21\u578b\uff0c\u4ee5\u663e\u5f0f\u5730\u5e73\u8861\u8bed\u4e49\u76f8\u5173\u6027\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u7ed3\u5408\u4e86\u591a\u4efb\u52a1\u5b66\u4e60 (MTL)\u3001\u591a\u6a21\u6001\u5185\u5bb9\u7279\u5f81\u878d\u5408\uff08\u6587\u672c\u548c\u89c6\u89c9\u5d4c\u5165\uff09\u548c\u901a\u8fc7\u9006\u503e\u5411\u52a0\u6743 (OPC) \u8fdb\u884c\u7684\u975e\u7b56\u7565\u6821\u6b63\u3002", "result": "\u5728\u5de5\u4e1a\u89c4\u6a21\u6570\u636e\u548c\u4e3a\u671f\u4e24\u5468\u7684\u5b9e\u9645 A/B \u6d4b\u8bd5\u4e2d\uff0c\u8bed\u4e49\u76f8\u5173\u6027\u663e\u7740\u63d0\u9ad8\uff08\u4e3b\u9898\u5339\u914d\u7387\u4ece 51% \u63d0\u9ad8\u5230 63%\uff09\uff0c\u70ed\u95e8\u9879\u76ee\u5206\u5e03\u51cf\u5c11\uff08\u70ed\u95e8\u89c6\u9891\u63a8\u8350\u51cf\u5c11 -13.8%\uff09\uff0c\u5e76\u4e14\u6211\u4eec\u7684\u9996\u8981\u7528\u6237\u53c2\u4e0e\u5ea6\u6307\u6807\u63d0\u9ad8\u4e86 +0.04%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3001\u5e73\u8861\u7684\u53c2\u4e0e\u5ea6\u4ee5\u53ca\u5b9e\u9645\u90e8\u7f72\u7684\u5b9e\u7528\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.09448", "categories": ["cs.DB", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09448", "abs": "https://arxiv.org/abs/2507.09448", "authors": ["Pramod Chunduri", "Yao Lu", "Joy Arulraj"], "title": "TRACER: Efficient Object Re-Identification in Networked Cameras through Adaptive Query Processing", "comment": null, "summary": "Efficiently re-identifying and tracking objects across a network of cameras\nis crucial for applications like traffic surveillance. Spatula is the\nstate-of-the-art video database management system (VDBMS) for processing Re-ID\nqueries. However, it suffers from two limitations. Its spatio-temporal\nfiltering scheme has limited accuracy on large camera networks due to localized\ncamera history. It is not suitable for critical video analytics applications\nthat require high recall due to a lack of support for adaptive query\nprocessing.\n  In this paper, we present Tracer, a novel VDBMS for efficiently processing\nRe-ID queries using an adaptive query processing framework. Tracer selects the\noptimal camera to process at each time step by training a recurrent network to\nmodel long-term historical correlations. To accelerate queries under a high\nrecall constraint, Tracer incorporates a probabilistic adaptive search model\nthat processes camera feeds in incremental search windows and dynamically\nupdates the sampling probabilities using an exploration-exploitation strategy.\nTo address the paucity of benchmarks for the Re-ID task due to privacy\nconcerns, we present a novel synthetic benchmark for generating multi-camera\nRe-ID datasets based on real-world traffic distribution. Our evaluation shows\nthat Tracer outperforms the state-of-the-art cross-camera analytics system by\n3.9x on average across diverse datasets.", "AI": {"tldr": "Tracer \u662f\u4e00\u79cd\u65b0\u578b VDBMS\uff0c\u5b83\u4f7f\u7528\u81ea\u9002\u5e94\u67e5\u8be2\u5904\u7406\u6846\u67b6\u6765\u9ad8\u6548\u5730\u5904\u7406 Re-ID \u67e5\u8be2\u3002\u5b83\u4f18\u4e8e\u73b0\u6709\u7684\u8de8\u6444\u50cf\u5934\u5206\u6790\u7cfb\u7edf\uff0c\u5e73\u5747\u63d0\u9ad8\u4e86 3.9 \u500d\u3002", "motivation": "\u5728\u6444\u50cf\u5934\u7f51\u7edc\u4e2d\u9ad8\u6548\u5730\u91cd\u65b0\u8bc6\u522b\u548c\u8ddf\u8e2a\u5bf9\u8c61\u5bf9\u4e8e\u4ea4\u901a\u76d1\u63a7\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u6700\u5148\u8fdb\u7684\u89c6\u9891\u6570\u636e\u5e93\u7ba1\u7406\u7cfb\u7edf (VDBMS) Spatula \u5b58\u5728\u4e24\u4e2a\u5c40\u9650\u6027\uff1a\u65f6\u7a7a\u8fc7\u6ee4\u65b9\u6848\u5728\u5927\u578b\u6444\u50cf\u5934\u7f51\u7edc\u4e0a\u7684\u51c6\u786e\u6027\u6709\u9650\uff0c\u5e76\u4e14\u7531\u4e8e\u7f3a\u4e4f\u5bf9\u81ea\u9002\u5e94\u67e5\u8be2\u5904\u7406\u7684\u652f\u6301\uff0c\u56e0\u6b64\u4e0d\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u53ec\u56de\u7387\u7684\u5173\u952e\u89c6\u9891\u5206\u6790\u5e94\u7528\u3002", "method": "Tracer \u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u5faa\u73af\u7f51\u7edc\u6765\u6a21\u62df\u957f\u671f\u5386\u53f2\u76f8\u5173\u6027\uff0c\u4ece\u800c\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9009\u62e9\u6700\u4f73\u6444\u50cf\u5934\u8fdb\u884c\u5904\u7406\u3002Tracer \u5305\u542b\u4e00\u4e2a\u6982\u7387\u81ea\u9002\u5e94\u641c\u7d22\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u589e\u91cf\u641c\u7d22\u7a97\u53e3\u4e2d\u5904\u7406\u6444\u50cf\u5934\u9988\u9001\uff0c\u5e76\u4f7f\u7528\u63a2\u7d22-\u5229\u7528\u7b56\u7565\u52a8\u6001\u66f4\u65b0\u91c7\u6837\u6982\u7387\u3002", "result": "Tracer \u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u7684\u8de8\u6444\u50cf\u5934\u5206\u6790\u7cfb\u7edf\uff0c\u5e73\u5747\u63d0\u9ad8\u4e86 3.9 \u500d\u3002", "conclusion": "Tracer\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u7684\u8de8\u6444\u50cf\u5934\u5206\u6790\u7cfb\u7edf\uff0c\u5e73\u5747\u63d0\u9ad8\u4e86 3.9 \u500d\u3002"}}
{"id": "2507.08916", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08916", "abs": "https://arxiv.org/abs/2507.08916", "authors": ["Mahmoud Alwakeel", "Aditya Nagori", "Vijay Krishnamoorthy", "Rishikesan Kamaleswaran"], "title": "Evaluating LLMs in Medicine: A Call for Rigor, Transparency", "comment": null, "summary": "Objectives: To evaluate the current limitations of large language models\n(LLMs) in medical question answering, focusing on the quality of datasets used\nfor their evaluation. Materials and Methods: Widely-used benchmark datasets,\nincluding MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,\ntransparency, and relevance to clinical scenarios. Alternatives, such as\nchallenge questions in medical journals, were also analyzed to identify their\npotential as unbiased evaluation tools. Results: Most existing datasets lack\nclinical realism, transparency, and robust validation processes. Publicly\navailable challenge questions offer some benefits but are limited by their\nsmall size, narrow scope, and exposure to LLM training. These gaps highlight\nthe need for secure, comprehensive, and representative datasets. Conclusion: A\nstandardized framework is critical for evaluating LLMs in medicine.\nCollaborative efforts among institutions and policymakers are needed to ensure\ndatasets and methodologies are rigorous, unbiased, and reflective of clinical\ncomplexities.", "AI": {"tldr": "Existing datasets for evaluating LLMs in medicine lack clinical realism and rigor. A standardized framework and collaborative efforts are needed.", "motivation": "To evaluate the current limitations of large language models (LLMs) in medical question answering, focusing on the quality of datasets used for their evaluation.", "method": "Widely-used benchmark datasets, including MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor, transparency, and relevance to clinical scenarios. Alternatives, such as challenge questions in medical journals, were also analyzed to identify their potential as unbiased evaluation tools.", "result": "Most existing datasets lack clinical realism, transparency, and robust validation processes. Publicly available challenge questions offer some benefits but are limited by their small size, narrow scope, and exposure to LLM training. These gaps highlight the need for secure, comprehensive, and representative datasets.", "conclusion": "A standardized framework is critical for evaluating LLMs in medicine. Collaborative efforts among institutions and policymakers are needed to ensure datasets and methodologies are rigorous, unbiased, and reflective of clinical complexities."}}
{"id": "2507.08832", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08832", "abs": "https://arxiv.org/abs/2507.08832", "authors": ["Niranjan Mallikarjun Sindhur", "Pavithra C", "Nivya Muchikel"], "title": "A Hybrid Machine Learning Framework for Optimizing Crop Selection via Agronomic and Economic Forecasting", "comment": null, "summary": "Farmers in developing regions like Karnataka, India, face a dual challenge:\nnavigating extreme market and climate volatility while being excluded from the\ndigital revolution due to literacy barriers. This paper presents a novel\ndecision support system that addresses both challenges through a unique\nsynthesis of machine learning and human-computer interaction. We propose a\nhybrid recommendation engine that integrates two predictive models: a Random\nForest classifier to assess agronomic suitability based on soil, climate, and\nreal-time weather data, and a Long Short-Term Memory (LSTM) network to forecast\nmarket prices for agronomically viable crops. This integrated approach shifts\nthe paradigm from \"what can grow?\" to \"what is most profitable to grow?\",\nproviding a significant advantage in mitigating economic risk. The system is\ndelivered through an end-to-end, voice-based interface in the local Kannada\nlanguage, leveraging fine-tuned speech recognition and high-fidelity speech\nsynthesis models to ensure accessibility for low-literacy users. Our results\nshow that the Random Forest model achieves 98.5% accuracy in suitability\nprediction, while the LSTM model forecasts harvest-time prices with a low\nmargin of error. By providing data-driven, economically optimized\nrecommendations through an inclusive interface, this work offers a scalable and\nimpactful solution to enhance the financial resilience of marginalized farming\ncommunities.", "AI": {"tldr": "A voice-based decision support system using machine learning models helps farmers in Karnataka, India, to make profitable crop choices despite market and climate volatility.", "motivation": "Farmers in developing regions face market and climate volatility and are excluded from the digital revolution due to literacy barriers.", "method": "A hybrid recommendation engine that integrates a Random Forest classifier for agronomic suitability and a Long Short-Term Memory (LSTM) network to forecast market prices, delivered through a voice-based interface in the local Kannada language.", "result": "The Random Forest model achieves 98.5% accuracy in suitability prediction, while the LSTM model forecasts harvest-time prices with a low margin of error.", "conclusion": "This work provides a scalable solution to enhance financial resilience of marginalized farming communities by delivering data-driven, economically optimized recommendations through an inclusive interface."}}
{"id": "2507.08892", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.08892", "abs": "https://arxiv.org/abs/2507.08892", "authors": ["Alexander Sasha Vezhnevets", "Jayd Matyas", "Logan Cross", "Davide Paglieri", "Minsuk Chang", "William A. Cunningham", "Simon Osindero", "William S. Isaac", "Joel Z. Leibo"], "title": "Multi-Actor Generative Artificial Intelligence as a Game Engine", "comment": "13 pages", "summary": "Generative AI can be used in multi-actor environments with purposes ranging\nfrom social science modeling to interactive narrative and AI evaluation.\nSupporting this diversity of use cases -- which we classify as Simulationist,\nDramatist, and Evaluationist -- demands a flexible scenario definition\nframework. We argue here that a good approach is to take inspiration from\ntabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible\nfor the environment and generates all parts of the story not directly\ndetermined by the voluntary actions of player characters. We argue that the\nEntity-Component architectural pattern is useful here. In such a system, the GM\nis not a hardcoded computer game but is itself a configurable entity, composed\nof components just like any other actor. By design, the approach allows for a\nseparation between the underlying implementation details handled by an\nengineer, the creation of reusable components, and their composition and\nconfiguration managed by a designer who constructs entities from the\ncomponents. This separation of concerns is instrumental for achieving rapid\niteration, maintaining modularity, and ultimately to ensure scalability. We\ndescribe the ongoing evolution of the Concordia library in terms of this\nphilosophy, demonstrating how it allows users to effectively configure\nscenarios that align with their specific goals.", "AI": {"tldr": "Uses the Entity-Component pattern, inspired by TTRPGs, to allow the construction of a flexible environment for diverse generative AI use-cases.", "motivation": "Generative AI is used in multi-actor environments for social science modeling, interactive narrative, and AI evaluation, demanding a flexible scenario definition framework.", "method": "Inspired by tabletop role-playing games (TTRPGs), uses the Entity-Component architectural pattern.", "result": "The approach allows separation between implementation details, reusable components, and their composition, ensuring rapid iteration, modularity, and scalability.", "conclusion": "The Concordia library's evolution, based on the Entity-Component pattern, enables users to configure scenarios aligning with their specific goals."}}
{"id": "2507.08979", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08979", "abs": "https://arxiv.org/abs/2507.08979", "authors": ["Mahdiyar Molahasani", "Azadeh Motamedi", "Michael Greenspan", "Il-Min Kim", "Ali Etemad"], "title": "PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection", "comment": "Accepted to ICCV 2025", "summary": "We introduce Projection-based Reduction of Implicit Spurious bias in\nvision-language Models (PRISM), a new data-free and task-agnostic solution for\nbias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in\ntheir training data, leading to skewed predictions. PRISM is designed to debias\nVLMs without relying on predefined bias categories or additional external data.\nIt operates in two stages: first, an LLM is prompted with simple class prompts\nto generate scene descriptions that contain spurious correlations. Next, PRISM\nuses our novel contrastive-style debiasing loss to learn a projection that maps\nthe embeddings onto a latent space that minimizes spurious correlations while\npreserving the alignment between image and text embeddings.Extensive\nexperiments demonstrate that PRISM outperforms current debiasing methods on the\ncommonly used Waterbirds and CelebA datasets We make our code public at:\nhttps://github.com/MahdiyarMM/PRISM.", "AI": {"tldr": "PRISM is a data-free and task-agnostic solution for bias mitigation in VLMs that outperforms current methods.", "motivation": "VLMs often inherit and amplify biases in their training data, leading to skewed predictions.", "method": "A contrastive-style debiasing loss is used to learn a projection that maps embeddings onto a latent space minimizing spurious correlations while preserving image-text alignment.", "result": "A new data-free and task-agnostic solution for bias mitigation in VLMs like CLIP is introduced.", "conclusion": "PRISM outperforms current debiasing methods on Waterbirds and CelebA datasets."}}
{"id": "2507.09423", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.09423", "abs": "https://arxiv.org/abs/2507.09423", "authors": ["Dong Wang", "Junyi Jiao", "Arnab Bhadury", "Yaping Zhang", "Mingyan Gao", "Onkar Dalal"], "title": "Item-centric Exploration for Cold Start Problem", "comment": "Accepted for publication on 2025 ACM Recsys Conference Industry Track", "summary": "Recommender systems face a critical challenge in the item cold-start problem,\nwhich limits content diversity and exacerbates popularity bias by struggling to\nrecommend new items. While existing solutions often rely on auxiliary data, but\nthis paper illuminates a distinct, yet equally pressing, issue stemming from\nthe inherent user-centricity of many recommender systems. We argue that in\nenvironments with large and rapidly expanding item inventories, the traditional\nfocus on finding the \"best item for a user\" can inadvertently obscure the ideal\naudience for nascent content. To counter this, we introduce the concept of\nitem-centric recommendations, shifting the paradigm to identify the optimal\nusers for new items. Our initial realization of this vision involves an\nitem-centric control integrated into an exploration system. This control\nemploys a Bayesian model with Beta distributions to assess candidate items\nbased on a predicted balance between user satisfaction and the item's inherent\nquality. Empirical online evaluations reveal that this straightforward control\nmarkedly improves cold-start targeting efficacy, enhances user satisfaction\nwith newly explored content, and significantly increases overall exploration\nefficiency.", "AI": {"tldr": "This paper proposes an item-centric recommendation method to improve cold-start performance by finding the optimal users for new items, leading to better user satisfaction and exploration efficiency.", "motivation": "Recommender systems struggle with the item cold-start problem, limiting content diversity and exacerbating popularity bias. Traditional user-centric approaches can obscure the ideal audience for new content.", "method": "An item-centric control integrated into an exploration system, employing a Bayesian model with Beta distributions.", "result": "Online evaluations show improved cold-start targeting efficacy, enhanced user satisfaction with newly explored content, and increased overall exploration efficiency.", "conclusion": "This paper introduces an item-centric recommendation approach to address the item cold-start problem. An item-centric control using a Bayesian model with Beta distributions is proposed."}}
{"id": "2507.09592", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09592", "abs": "https://arxiv.org/abs/2507.09592", "authors": ["Isaac Shi", "Zeyuan Li", "Fan Liu", "Wenli Wang", "Lewei He", "Yang Yang", "Tianyu Shi"], "title": "THOR: Transformer Heuristics for On-Demand Retrieval", "comment": null, "summary": "We introduce the THOR (Transformer Heuristics for On-Demand Retrieval)\nModule, designed and implemented by eSapiens, a secure, scalable engine that\ntransforms natural-language questions into verified, read-only SQL analytics\nfor enterprise databases. The Text-to-SQL module follows a decoupled\norchestration/execution architecture: a Supervisor Agent routes queries, Schema\nRetrieval dynamically injects table and column metadata, and a SQL Generation\nAgent emits single-statement SELECT queries protected by a read-only guardrail.\nAn integrated Self-Correction & Rating loop captures empty results, execution\nerrors, or low-quality outputs and triggers up to five LLM-driven regeneration\nattempts. Finally, a Result Interpretation Agent produces concise,\nhuman-readable insights and hands raw rows to the Insight & Intelligence engine\nfor visualization or forecasting.\n  Smoke tests across finance, sales, and operations scenarios demonstrate\nreliable ad-hoc querying and automated periodic reporting. By embedding schema\nawareness, fault-tolerant execution, and compliance guardrails, the THOR Module\nempowers non-technical users to access live data with zero-SQL simplicity and\nenterprise-grade safety.", "AI": {"tldr": "THOR\u6a21\u5757\u662f\u4e00\u4e2a\u5b89\u5168\u3001\u53ef\u6269\u5c55\u7684\u5f15\u64ce\uff0c\u53ef\u4ee5\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u8f6c\u6362\u4e3a\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u53ea\u8bfbSQL\u5206\u6790\uff0c\u4ece\u800c\u4f7f\u975e\u6280\u672f\u7528\u6237\u80fd\u591f\u8f7b\u677e\u5b89\u5168\u5730\u8bbf\u95ee\u4f01\u4e1a\u6570\u636e\u3002", "motivation": "\u4e3a\u4e86\u4f7f\u975e\u6280\u672f\u7528\u6237\u80fd\u591f\u8bbf\u95ee\u5b9e\u65f6\u6570\u636e\uff0c\u5e76\u7b80\u5316\u4f01\u4e1a\u6570\u636e\u5e93\u7684\u67e5\u8be2\u8fc7\u7a0b\u3002", "method": "Transformer Heuristics for On-Demand Retrieval (THOR) Module", "result": "\u5728\u8d22\u52a1\u3001\u9500\u552e\u548c\u8fd0\u8425\u573a\u666f\u4e2d\u7684\u6d4b\u8bd5\u8868\u660e\uff0cTHOR\u6a21\u5757\u80fd\u591f\u5b9e\u73b0\u53ef\u9760\u7684\u5373\u5e2d\u67e5\u8be2\u548c\u81ea\u52a8\u5b9a\u671f\u62a5\u544a\u3002", "conclusion": "THOR\u6a21\u5757\u901a\u8fc7\u5d4c\u5165\u6a21\u5f0f\u611f\u77e5\u3001\u5bb9\u9519\u6267\u884c\u548c\u5408\u89c4\u6027\u4fdd\u62a4\u63aa\u65bd\uff0c\u4f7f\u975e\u6280\u672f\u7528\u6237\u80fd\u591f\u4ee5\u96f6SQL\u7684\u7b80\u6613\u6027\u548c\u4f01\u4e1a\u7ea7\u7684\u5b89\u5168\u6027\u8bbf\u95ee\u5b9e\u65f6\u6570\u636e\u3002"}}
{"id": "2507.08924", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08924", "abs": "https://arxiv.org/abs/2507.08924", "authors": ["Seokhee Hong", "Sunkyoung Kim", "Guijin Son", "Soyeon Kim", "Yeonjung Hong", "Jinsik Lee"], "title": "From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation", "comment": null, "summary": "The development of Large Language Models (LLMs) requires robust benchmarks\nthat encompass not only academic domains but also industrial fields to\neffectively evaluate their applicability in real-world scenarios. In this\npaper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,\nreconstructed from the existing KMMLU, consists of questions from the Korean\nNational Technical Qualification exams, with critical errors removed to enhance\nreliability. KMMLU-Pro is based on Korean National Professional Licensure exams\nto reflect professional knowledge in Korea. Our experiments demonstrate that\nthese benchmarks comprehensively represent industrial knowledge in Korea. We\nrelease our dataset publicly available.", "AI": {"tldr": "Introduces two Korean expert-level benchmarks (KMMLU-Redux, KMMLU-Pro) to evaluate LLMs in real-world industrial scenarios.", "motivation": "The development of LLMs requires robust benchmarks that encompass both academic and industrial fields to evaluate real-world applicability.", "method": "Reconstruction of existing KMMLU and creation of new benchmark based on Korean National Professional Licensure exams.", "result": "The experiments demonstrate that the benchmarks comprehensively represent industrial knowledge in Korea.", "conclusion": "The paper introduces two new Korean expert-level benchmarks, KMMLU-Redux and KMMLU-Pro, and makes the dataset publicly available."}}
{"id": "2507.08833", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08833", "abs": "https://arxiv.org/abs/2507.08833", "authors": ["Seokmin Ko"], "title": "LoRA Is Slower Than You Think", "comment": null, "summary": "Low-Rank Adaptation (LoRA) is one of the most widely used techniques for\nfine-tuning large language models (LLMs). By introducing a small number of\ntrainable low-rank weight matrices, LoRA substantially reduces the number of\nparameters that need to be updated, offering significant advantages in memory\nconsumption and computational efficiency compared to full fine-tuning. However,\nwe observed that LoRA does not consistently provide speed improvements across\nall model architectures and training setups. Motivated by this inconsistency,\nwe conduct a comprehensive analysis of LoRA's performance and investigate the\nunderlying factors limiting its speedup. Based on our findings, we propose\nseveral methods for more efficient fine-tuning of LLMs. We empirically evaluate\nthese methods and compare them to LoRA, demonstrating that our approach\nachieves comparable or superior performance while delivering more consistent\ntraining speed improvements. Our work offers valuable insights and practical\nguidelines for practitioners seeking to optimize LLM fine-tuning under resource\nconstraints.", "AI": {"tldr": "This paper analyzes LoRA's inconsistent speed improvements, proposes more efficient fine-tuning methods, and demonstrates their superior performance.", "motivation": "LoRA does not consistently provide speed improvements across all model architectures and training setups.", "method": "The paper conducts a comprehensive analysis of LoRA's performance and investigates the underlying factors limiting its speedup. Based on these findings, the paper proposes several methods for more efficient fine-tuning of LLMs. These methods are then empirically evaluated and compared to LoRA.", "result": "The proposed approach achieves comparable or superior performance to LoRA while delivering more consistent training speed improvements.", "conclusion": "The paper proposes several methods for more efficient fine-tuning of LLMs, achieving comparable or superior performance to LoRA with more consistent training speed improvements."}}
{"id": "2507.09080", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09080", "abs": "https://arxiv.org/abs/2507.09080", "authors": ["Athanasios Trantas", "Martino Mensio", "Stylianos Stasinos", "Sebastian Gribincea", "Taimur Khan", "Damian Podareanu", "Aliene van der Veen"], "title": "BioAnalyst: A Foundation Model for Biodiversity", "comment": null, "summary": "The accelerating loss of biodiversity presents critical challenges for\necological research and conservation strategies. The preservation of\nbiodiversity is paramount for maintaining ecological balance and ensuring the\nsustainability of ecosystems. However, biodiversity faces numerous threats,\nincluding habitat loss, climate change, and the proliferation of invasive\nspecies. Addressing these and other ecology-related challenges, both at local\nand global scales, requires comprehensive monitoring, predictive and\nconservation planning capabilities. Artificial Intelligence (AI) Foundation\nModels (FMs) have gained significant momentum in numerous scientific domains by\nleveraging vast datasets to learn general-purpose representations adaptable to\nvarious downstream tasks. This paradigm holds immense promise for biodiversity\nconservation. In response, we introduce BioAnalyst, the first Foundation Model\ntailored for biodiversity analysis and conservation planning. BioAnalyst\nemploys a transformer-based architecture, pre-trained on extensive multi-modal\ndatasets encompassing species occurrence records, remote sensing indicators,\nclimate and environmental variables. BioAnalyst is designed for adaptability,\nallowing for fine-tuning of a range of downstream tasks, such as species\ndistribution modelling, habitat suitability assessments, invasive species\ndetection, and population trend forecasting. We evaluate the model's\nperformance on two downstream use cases, demonstrating its generalisability\ncompared to existing methods, particularly in data-scarce scenarios for two\ndistinct use-cases, establishing a new accuracy baseline for ecological\nforecasting. By openly releasing BioAnalyst and its fine-tuning workflows to\nthe scientific community, we aim to foster collaborative efforts in\nbiodiversity modelling and advance AI-driven solutions to pressing ecological\nchallenges.", "AI": {"tldr": "Introduces BioAnalyst, the first AI Foundation Model tailored for biodiversity analysis and conservation planning, demonstrating improved performance in ecological forecasting, especially in data-scarce situations.", "motivation": "The accelerating loss of biodiversity presents critical challenges for ecological research and conservation strategies.", "method": "transformer-based architecture, pre-trained on extensive multi-modal datasets", "result": "demonstrates its generalisability compared to existing methods, particularly in data-scarce scenarios for two distinct use-cases", "conclusion": "BioAnalyst establishes a new accuracy baseline for ecological forecasting and fosters collaborative efforts in biodiversity modeling."}}
{"id": "2507.08981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08981", "abs": "https://arxiv.org/abs/2507.08981", "authors": ["Hanbyel Cho", "Jaesung Ahn", "Yooshin Cho", "Junmo Kim"], "title": "Video Inference for Human Mesh Recovery with Vision Transformer", "comment": "Accepted to IEEE FG 2023", "summary": "Human Mesh Recovery (HMR) from an image is a challenging problem because of\nthe inherent ambiguity of the task. Existing HMR methods utilized either\ntemporal information or kinematic relationships to achieve higher accuracy, but\nthere is no method using both. Hence, we propose \"Video Inference for Human\nMesh Recovery with Vision Transformer (HMR-ViT)\" that can take into account\nboth temporal and kinematic information. In HMR-ViT, a Temporal-kinematic\nFeature Image is constructed using feature vectors obtained from video frames\nby an image encoder. When generating the feature image, we use a Channel\nRearranging Matrix (CRM) so that similar kinematic features could be located\nspatially close together. The feature image is then further encoded using\nVision Transformer, and the SMPL pose and shape parameters are finally inferred\nusing a regression network. Extensive evaluation on the 3DPW and Human3.6M\ndatasets indicates that our method achieves a competitive performance in HMR.", "AI": {"tldr": "HMR-ViT uses both temporal and kinematic information with a Vision Transformer to improve Human Mesh Recovery.", "motivation": "Existing HMR methods utilized either temporal information or kinematic relationships, but not both, leading to potential limitations in accuracy.", "method": "A Temporal-kinematic Feature Image is constructed using feature vectors obtained from video frames by an image encoder. A Channel Rearranging Matrix (CRM) is used to locate similar kinematic features spatially close together. The feature image is then further encoded using Vision Transformer, and the SMPL pose and shape parameters are finally inferred using a regression network.", "result": "The HMR-ViT method achieves competitive performance in HMR.", "conclusion": "The proposed HMR-ViT method achieves competitive performance in Human Mesh Recovery (HMR) on the 3DPW and Human3.6M datasets."}}
{"id": "2507.09483", "categories": ["cs.IR", "H.3.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.09483", "abs": "https://arxiv.org/abs/2507.09483", "authors": ["Naghmeh Farzi", "Laura Dietz"], "title": "Does UMBRELA Work on Other LLMs?", "comment": "9 pages, 2 figures, accepted to SIGIR 2025", "summary": "We reproduce the UMBRELA LLM Judge evaluation framework across a range of\nlarge language models (LLMs) to assess its generalizability beyond the original\nstudy. Our investigation evaluates how LLM choice affects relevance assessment\naccuracy, focusing on leaderboard rank correlation and per-label agreement\nmetrics. Results demonstrate that UMBRELA with DeepSeek V3 obtains very\ncomparable performance to GPT-4o (used in original work). For LLaMA-3.3-70B we\nobtain slightly lower performance, which further degrades with smaller LLMs.", "AI": {"tldr": "UMBRELA\u6846\u67b6\u5728\u4e0d\u540cLLM\u4e0a\u7684\u8868\u73b0\u6709\u5dee\u5f02\uff0cDeepSeek V3\u6700\u4f73\u3002", "motivation": "\u8bc4\u4f30UMBRELA LLM Judge\u8bc4\u4f30\u6846\u67b6\u7684\u901a\u7528\u6027\uff0c\u8003\u5bdfLLM\u7684\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u76f8\u5173\u6027\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002", "method": "\u5728\u591a\u4e2aLLM\u4e0a\u91cd\u73b0UMBRELA LLM Judge\u8bc4\u4f30\u6846\u67b6\u3002", "result": "DeepSeek V3\u6027\u80fd\u4e0eGPT-4o\u76f8\u5f53\uff0cLLaMA-3.3-70B\u6027\u80fd\u7a0d\u4f4e\uff0c\u66f4\u5c0f\u7684LLM\u6027\u80fd\u66f4\u5dee\u3002", "conclusion": "UMBRELA\u6846\u67b6\u5728\u4e0d\u540cLLM\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff1aDeepSeek V3\u4e0eGPT-4o\u6027\u80fd\u76f8\u5f53\uff0cLLaMA-3.3-70B\u7565\u900a\uff0c\u66f4\u5c0f\u7684LLM\u6027\u80fd\u4e0b\u964d\u3002"}}
{"id": "2507.09642", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.09642", "abs": "https://arxiv.org/abs/2507.09642", "authors": ["Yina Lv", "Qiao Li", "Quanqing Xu", "Congming Gao", "Chuanhui Yang", "Xiaoli Wang", "Chun Jason Xue"], "title": "Rethinking LSM-tree based Key-Value Stores: A Survey", "comment": null, "summary": "LSM-tree is a widely adopted data structure in modern key-value store systems\nthat optimizes write performance in write-heavy applications by using append\nwrites to achieve sequential writes. However, the unpredictability of LSM-tree\ncompaction introduces significant challenges, including performance variability\nduring peak workloads and in resource-constrained environments, write\namplification caused by data rewriting during compactions, read amplification\nfrom multi-level queries, trade-off between read and write performance, as well\nas efficient space utilization to mitigate space amplification. Prior studies\non LSM-tree optimizations have addressed the above challenges; however, in\nrecent years, research on LSM-tree optimization has continued to propose. The\ngoal of this survey is to review LSM-tree optimization, focusing on\nrepresentative works in the past five years. This survey first studies existing\nsolutions on how to mitigate the performance impact of LSM-tree flush and\ncompaction and how to improve basic key-value operations. In addition,\ndistributed key-value stores serve multi-tenants, ranging from tens of\nthousands to millions of users with diverse requirements. We then analyze the\nnew challenges and opportunities in these modern architectures and across\nvarious application scenarios. Unlike the existing survey papers, this survey\nprovides a detailed discussion of the state-of-the-art work on LSM-tree\noptimizations and gives future research directions.", "AI": {"tldr": "This survey reviews LSM-tree optimizations over the past five years, focusing on mitigating compaction issues, improving key-value operations, and addressing challenges in modern distributed key-value stores. It provides future research directions.", "motivation": "LSM-tree compaction introduces significant challenges, including performance variability during peak workloads and in resource-constrained environments, write amplification, read amplification, trade-off between read and write performance, as well as efficient space utilization. Prior studies on LSM-tree optimizations have addressed the above challenges; however, in recent years, research on LSM-tree optimization has continued to propose.", "method": "reviewing LSM-tree optimization, focusing on representative works in the past five years", "result": "a detailed discussion of the state-of-the-art work on LSM-tree optimizations and gives future research directions", "conclusion": "This survey reviews LSM-tree optimization, focusing on representative works in the past five years. It studies existing solutions on how to mitigate the performance impact of LSM-tree flush and compaction and how to improve basic key-value operations. It also analyzes the new challenges and opportunities in modern architectures and across various application scenarios. The survey provides a detailed discussion of the state-of-the-art work on LSM-tree optimizations and gives future research directions."}}
{"id": "2507.08967", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08967", "abs": "https://arxiv.org/abs/2507.08967", "authors": ["Rongyi Zhu", "Yuhui Wang", "Tanqiu Jiang", "Jiacheng Liang", "Ting Wang"], "title": "Self-Improving Model Steering", "comment": "16 pages, 9 figures", "summary": "Model steering represents a powerful technique that dynamically aligns large\nlanguage models (LLMs) with human preferences during inference. However,\nconventional model-steering methods rely heavily on externally annotated data,\nnot only limiting their adaptability to varying contexts but also tethering\ntheir effectiveness to annotation quality. In this paper, we present SIMS, the\nfirst self-improving model-steering framework that operates without relying on\nexternal supervision. At its core, SIMS autonomously generates and refines\ncontrastive samples through iterative self-improvement cycles, enabling\nadaptive, context-specific steering. Additionally, SIMS employs novel\nstrategies, including prompt ranking and contrast sampling, to further enhance\nsteering efficacy. Extensive evaluation across diverse LLMs and benchmarks\ndemonstrates that SIMS substantially outperforms existing methods in steering\neffectiveness and adaptability, highlighting self-improving model steering as a\npromising direction for future research on inference-time LLM alignment.", "AI": {"tldr": "SIMS is the first self-improving model-steering framework that operates without relying on external supervision, and it outperforms existing methods.", "motivation": "Conventional model-steering methods rely heavily on externally annotated data, not only limiting their adaptability to varying contexts but also tethering their effectiveness to annotation quality.", "method": "SIMS autonomously generates and refines contrastive samples through iterative self-improvement cycles, enabling adaptive, context-specific steering. Additionally, SIMS employs novel strategies, including prompt ranking and contrast sampling, to further enhance steering efficacy.", "result": "SIMS substantially outperforms existing methods in steering effectiveness and adaptability.", "conclusion": "SIMS substantially outperforms existing methods in steering effectiveness and adaptability, highlighting self-improving model steering as a promising direction for future research on inference-time LLM alignment."}}
{"id": "2507.08834", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08834", "abs": "https://arxiv.org/abs/2507.08834", "authors": ["Karishma Battina", "Prathamesh Dinesh Joshi", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Physical Informed Neural Networks for modeling ocean pollutant", "comment": "13 pages, 9 figures, 3 tables", "summary": "Traditional numerical methods often struggle with the complexity and scale of\nmodeling pollutant transport across vast and dynamic oceanic domains. This\npaper introduces a Physics-Informed Neural Network (PINN) framework to simulate\nthe dispersion of pollutants governed by the 2D advection-diffusion equation.\nThe model achieves physically consistent predictions by embedding physical laws\nand fitting to noisy synthetic data, generated via a finite difference method\n(FDM), directly into the neural network training process. This approach\naddresses challenges such as non-linear dynamics and the enforcement of\nboundary and initial conditions. Synthetic data sets, augmented with varying\nnoise levels, are used to capture real-world variability. The training\nincorporates a hybrid loss function including PDE residuals, boundary/initial\ncondition conformity, and a weighted data fit term. The approach takes\nadvantage of the Julia language scientific computing ecosystem for\nhigh-performance simulations, offering a scalable and flexible alternative to\ntraditional solvers", "AI": {"tldr": "This paper introduces a PINN framework to simulate pollutant dispersion, offering a scalable and flexible alternative to traditional solvers.", "motivation": "Traditional numerical methods often struggle with the complexity and scale of modeling pollutant transport across vast and dynamic oceanic domains.", "method": "A Physics-Informed Neural Network (PINN) framework is used to simulate the dispersion of pollutants governed by the 2D advection-diffusion equation.", "result": "The model achieves physically consistent predictions by embedding physical laws and fitting to noisy synthetic data.", "conclusion": "The PINN framework offers a scalable and flexible alternative to traditional solvers for simulating pollutant dispersion."}}
{"id": "2507.09089", "categories": ["cs.AI", "cs.HC", "cs.SE", "I.2"], "pdf": "https://arxiv.org/pdf/2507.09089", "abs": "https://arxiv.org/abs/2507.09089", "authors": ["Joel Becker", "Nate Rush", "Elizabeth Barnes", "David Rein"], "title": "Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity", "comment": "50 pages, 8 tables, 22 figures", "summary": "Despite widespread adoption, the impact of AI tools on software development\nin the wild remains understudied. We conduct a randomized controlled trial\n(RCT) to understand how AI tools at the February-June 2025 frontier affect the\nproductivity of experienced open-source developers. 16 developers with moderate\nAI experience complete 246 tasks in mature projects on which they have an\naverage of 5 years of prior experience. Each task is randomly assigned to allow\nor disallow usage of early 2025 AI tools. When AI tools are allowed, developers\nprimarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.\nBefore starting tasks, developers forecast that allowing AI will reduce\ncompletion time by 24%. After completing the study, developers estimate that\nallowing AI reduced completion time by 20%. Surprisingly, we find that allowing\nAI actually increases completion time by 19%--AI tooling slowed developers\ndown. This slowdown also contradicts predictions from experts in economics (39%\nshorter) and ML (38% shorter). To understand this result, we collect and\nevaluate evidence for 20 properties of our setting that a priori could\ncontribute to the observed slowdown effect--for example, the size and quality\nstandards of projects, or prior developer experience with AI tooling. Although\nthe influence of experimental artifacts cannot be entirely ruled out, the\nrobustness of the slowdown effect across our analyses suggests it is unlikely\nto primarily be a function of our experimental design.", "AI": {"tldr": "AI tools unexpectedly slowed down experienced developers in open-source projects.", "motivation": "To understand the impact of AI tools on software development productivity.", "method": "Randomized controlled trial with 16 experienced open-source developers completing 246 tasks.", "result": "AI tools slowed down developers by 19%, contrary to expectations.", "conclusion": "AI tools increased completion time by 19%, contradicting predictions."}}
{"id": "2507.09005", "categories": ["cs.CV", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2507.09005", "abs": "https://arxiv.org/abs/2507.09005", "authors": ["Cheng-Hsi Hsiao", "Krishna Kumar"], "title": "From images to properties: a NeRF-driven framework for granular material parameter inversion", "comment": null, "summary": "We introduce a novel framework that integrates Neural Radiance Fields (NeRF)\nwith Material Point Method (MPM) simulation to infer granular material\nproperties from visual observations. Our approach begins by generating\nsynthetic experimental data, simulating an plow interacting with sand. The\nexperiment is rendered into realistic images as the photographic observations.\nThese observations include multi-view images of the experiment's initial state\nand time-sequenced images from two fixed cameras. Using NeRF, we reconstruct\nthe 3D geometry from the initial multi-view images, leveraging its capability\nto synthesize novel viewpoints and capture intricate surface details. The\nreconstructed geometry is then used to initialize material point positions for\nthe MPM simulation, where the friction angle remains unknown. We render images\nof the simulation under the same camera setup and compare them to the observed\nimages. By employing Bayesian optimization, we minimize the image loss to\nestimate the best-fitting friction angle. Our results demonstrate that friction\nangle can be estimated with an error within 2 degrees, highlighting the\neffectiveness of inverse analysis through purely visual observations. This\napproach offers a promising solution for characterizing granular materials in\nreal-world scenarios where direct measurement is impractical or impossible.", "AI": {"tldr": "integrates NeRF with MPM simulation to infer granular material properties from visual observations", "motivation": "infer granular material properties from visual observations", "method": "integrate Neural Radiance Fields (NeRF) with Material Point Method (MPM) simulation", "result": "friction angle can be estimated with an error within 2 degrees", "conclusion": "The friction angle can be estimated with an error within 2 degrees, highlighting the effectiveness of inverse analysis through purely visual observations. This approach offers a promising solution for characterizing granular materials in real-world scenarios where direct measurement is impractical or impossible."}}
{"id": "2507.09488", "categories": ["cs.IR", "H.3.3; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.09488", "abs": "https://arxiv.org/abs/2507.09488", "authors": ["Naghmeh Farzi", "Laura Dietz"], "title": "Criteria-Based LLM Relevance Judgments", "comment": "10 pages, 3 figures, accepted to ICTIR 2025", "summary": "Relevance judgments are crucial for evaluating information retrieval systems,\nbut traditional human-annotated labels are time-consuming and expensive. As a\nresult, many researchers turn to automatic alternatives to accelerate method\ndevelopment. Among these, Large Language Models (LLMs) provide a scalable\nsolution by generating relevance labels directly through prompting. However,\nprompting an LLM for a relevance label without constraints often results in not\nonly incorrect predictions but also outputs that are difficult for humans to\ninterpret. We propose the Multi-Criteria framework for LLM-based relevance\njudgments, decomposing the notion of relevance into multiple criteria--such as\nexactness, coverage, topicality, and contextual fit--to improve the robustness\nand interpretability of retrieval evaluations compared to direct grading\nmethods. We validate this approach on three datasets: the TREC Deep Learning\ntracks from 2019 and 2020, as well as LLMJudge (based on TREC DL 2023). Our\nresults demonstrate that Multi-Criteria judgments enhance the system\nranking/leaderboard performance. Moreover, we highlight the strengths and\nlimitations of this approach relative to direct grading approaches, offering\ninsights that can guide the development of future automatic evaluation\nframeworks in information retrieval.", "AI": {"tldr": "Proposes a Multi-Criteria framework for LLM-based relevance judgments to improve the robustness and interpretability of retrieval evaluations compared to direct grading methods.", "motivation": "Traditional human-annotated labels are time-consuming and expensive. As a result, many researchers turn to automatic alternatives to accelerate method development. Among these, Large Language Models (LLMs) provide a scalable solution by generating relevance labels directly through prompting. However, prompting an LLM for a relevance label without constraints often results in not only incorrect predictions but also outputs that are difficult for humans to interpret.", "method": "Multi-Criteria framework for LLM-based relevance judgments, decomposing the notion of relevance into multiple criteria--such as exactness, coverage, topicality, and contextual fit", "result": "Results demonstrate that Multi-Criteria judgments enhance the system ranking/leaderboard performance. Moreover, highlight the strengths and limitations of this approach relative to direct grading approaches, offering insights that can guide the development of future automatic evaluation frameworks in information retrieval.", "conclusion": "Multi-Criteria judgments enhance the system ranking/leaderboard performance."}}
{"id": "2507.10017", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.10017", "abs": "https://arxiv.org/abs/2507.10017", "authors": ["Zhiyang Tang", "Yanping Wu", "Xiangjun Zai", "Chen Chen", "Xiaoyang Wang", "Ying Zhang"], "title": "Efficient Temporal Simple Path Graph Generation", "comment": null, "summary": "Interactions between two entities often occur at specific timestamps, which\ncan be modeled as a temporal graph. Exploring the relationships between\nvertices based on temporal paths is one of the fundamental tasks. In this\npaper, we conduct the first research to propose and investigate the problem of\ngenerating the temporal simple path graph (tspG), which is the subgraph\nconsisting of all temporal simple paths from the source vertex to the target\nvertex within the given time interval. Directly enumerating all temporal simple\npaths and constructing the tspG is computationally expensive. To accelerate the\nprocessing, we propose an efficient method named Verification in Upper-bound\nGraph. It first incorporates the temporal path constraint and simple path\nconstraint to exclude unpromising edges from the original graph, which obtains\na tight upper-bound graph as a high-quality approximation of the tspG in\npolynomial time. Then, an Escape Edges Verification algorithm is further\napplied in the upper-bound graph to construct the exact tspG without\nexhaustively enumerating all temporal simple paths between given vertices.\nFinally, comprehensive experiments on 10 real-world graphs are conducted to\ndemonstrate the efficiency and effectiveness of the proposed techniques.", "AI": {"tldr": "This paper introduces the temporal simple path graph (tspG) problem and proposes an efficient method to construct it using a verification in the upper-bound graph approach, which is validated through experiments.", "motivation": "Exploring the relationships between vertices based on temporal paths is one of the fundamental tasks. In this paper, we conduct the first research to propose and investigate the problem of generating the temporal simple path graph (tspG), which is the subgraph consisting of all temporal simple paths from the source vertex to the target vertex within the given time interval. Directly enumerating all temporal simple paths and constructing the tspG is computationally expensive.", "method": "The paper proposes an efficient method named Verification in Upper-bound Graph. It first incorporates the temporal path constraint and simple path constraint to exclude unpromising edges from the original graph, which obtains a tight upper-bound graph as a high-quality approximation of the tspG in polynomial time. Then, an Escape Edges Verification algorithm is further applied in the upper-bound graph to construct the exact tspG without exhaustively enumerating all temporal simple paths between given vertices.", "result": "The paper proposes the problem of generating the temporal simple path graph (tspG).", "conclusion": "The paper conducts comprehensive experiments on 10 real-world graphs to demonstrate the efficiency and effectiveness of the proposed techniques."}}
{"id": "2507.08969", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08969", "abs": "https://arxiv.org/abs/2507.08969", "authors": ["Drew Walker", "Jennifer Love", "Swati Rajwal", "Isabel C Walker", "Hannah LF Cooper", "Abeed Sarker", "Melvin Livingston III"], "title": "Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR", "comment": "3 Tables", "summary": "Introduction: Electronic health records (EHR) are a critical medium through\nwhich patient stigmatization is perpetuated among healthcare teams. Methods: We\nidentified linguistic features of doubt markers and stigmatizing labels in\nMIMIC-III EHR via expanded lexicon matching and supervised learning\nclassifiers. Predictors of rates of linguistic features were assessed using\nPoisson regression models. Results: We found higher rates of stigmatizing\nlabels per chart among patients who were Black or African American (RR: 1.16),\npatients with Medicare/Medicaid or government-run insurance (RR: 2.46),\nself-pay (RR: 2.12), and patients with a variety of stigmatizing disease and\nmental health conditions. Patterns among doubt markers were similar, though\nmale patients had higher rates of doubt markers (RR: 1.25). We found increased\nstigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),\nwith similar patterns of doubt markers. Discussion: Stigmatizing language\noccurred at higher rates among historically stigmatized patients, perpetuated\nby multiple provider types.", "AI": {"tldr": "The study found that stigmatizing language in electronic health records disproportionately affects historically stigmatized patients and is used by multiple provider types.", "motivation": "Electronic health records (EHR) are a critical medium through which patient stigmatization is perpetuated among healthcare teams.", "method": "We identified linguistic features of doubt markers and stigmatizing labels in MIMIC-III EHR via expanded lexicon matching and supervised learning classifiers. Predictors of rates of linguistic features were assessed using Poisson regression models.", "result": "We found higher rates of stigmatizing labels per chart among patients who were Black or African American (RR: 1.16), patients with Medicare/Medicaid or government-run insurance (RR: 2.46), self-pay (RR: 2.12), and patients with a variety of stigmatizing disease and mental health conditions. Patterns among doubt markers were similar, though male patients had higher rates of doubt markers (RR: 1.25). We found increased stigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25), with similar patterns of doubt markers.", "conclusion": "Stigmatizing language occurred at higher rates among historically stigmatized patients, perpetuated by multiple provider types."}}
{"id": "2507.08835", "categories": ["cs.LG", "cs.AI", "math.ST", "q-fin.RM", "q-fin.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.08835", "abs": "https://arxiv.org/abs/2507.08835", "authors": ["Harold Gu\u00e9neau", "Alain Celisse", "Pascal Delange"], "title": "Representation learning with a transformer by contrastive learning for money laundering detection", "comment": null, "summary": "The present work tackles the money laundering detection problem. A new\nprocedure is introduced which exploits structured time series of both\nqualitative and quantitative data by means of a transformer neural network. The\nfirst step of this procedure aims at learning representations of time series\nthrough contrastive learning (without any labels). The second step leverages\nthese representations to generate a money laundering scoring of all\nobservations. A two-thresholds approach is then introduced, which ensures a\ncontrolled false-positive rate by means of the Benjamini-Hochberg (BH)\nprocedure. Experiments confirm that the transformer is able to produce general\nrepresentations that succeed in exploiting money laundering patterns with\nminimal supervision from domain experts. It also illustrates the higher ability\nof the new procedure for detecting nonfraudsters as well as fraudsters, while\nkeeping the false positive rate under control. This greatly contrasts with\nrule-based procedures or the ones based on LSTM architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u795e\u7ecf\u7f51\u7edc\u7684\u6d17\u94b1\u68c0\u6d4b\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u65f6\u95f4\u5e8f\u5217\u8868\u793a\uff0c\u5e76\u4f7f\u7528\u53cc\u9608\u503c\u65b9\u6cd5\u63a7\u5236\u8bef\u62a5\u7387\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u6d17\u94b1\u68c0\u6d4b\u95ee\u9898\u3002", "method": "\u5229\u7528Transformer\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5b66\u4e60\u65f6\u95f4\u5e8f\u5217\u7684\u8868\u793a\uff0c\u5e76\u751f\u6210\u6d17\u94b1\u8bc4\u5206\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTransformer\u80fd\u591f\u751f\u6210\u901a\u7528\u8868\u793a\uff0c\u6210\u529f\u5229\u7528\u6d17\u94b1\u6a21\u5f0f\uff0c\u4e14\u5728\u6700\u5c0f\u5316\u9886\u57df\u4e13\u5bb6\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\uff0c\u68c0\u6d4b\u975e\u6b3a\u8bc8\u8005\u548c\u6b3a\u8bc8\u8005\u7684\u80fd\u529b\u66f4\u5f3a\uff0c\u540c\u65f6\u4fdd\u6301\u8bef\u62a5\u7387\u5728\u53ef\u63a7\u8303\u56f4\u5185\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u6d17\u94b1\u6d3b\u52a8\uff0c\u540c\u65f6\u63a7\u5236\u8bef\u62a5\u7387\uff0c\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u6216LSTM\u67b6\u6784\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.09179", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09179", "abs": "https://arxiv.org/abs/2507.09179", "authors": ["Ronghua Shi", "Yiou Liu", "Xinyu Ying", "Yang Tan", "Yuchun Feng", "Lynn Ai", "Bill Shi", "Xuhui Wang", "Zhuang Liu"], "title": "Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System", "comment": null, "summary": "Decentralized finance (DeFi) has introduced a new era of permissionless\nfinancial innovation but also led to unprecedented market manipulation. Without\ncentralized oversight, malicious actors coordinate shilling campaigns and\npump-and-dump schemes across various platforms. We propose a Multi-Agent\nReinforcement Learning (MARL) framework for decentralized manipulation\ndetection, modeling the interaction between manipulators and detectors as a\ndynamic adversarial game. This framework identifies suspicious patterns using\ndelayed token price reactions as financial indicators.Our method introduces\nthree innovations: (1) Group Relative Policy Optimization (GRPO) to enhance\nlearning stability in sparse-reward and partially observable settings; (2) a\ntheory-based reward function inspired by rational expectations and information\nasymmetry, differentiating price discovery from manipulation noise; and (3) a\nmulti-modal agent pipeline that integrates LLM-based semantic features, social\ngraph signals, and on-chain market data for informed decision-making.The\nframework is integrated within the Symphony system, a decentralized multi-agent\narchitecture enabling peer-to-peer agent execution and trust-aware learning\nthrough distributed logs, supporting chain-verifiable evaluation. Symphony\npromotes adversarial co-evolution among strategic actors and maintains robust\nmanipulation detection without centralized oracles, enabling real-time\nsurveillance across global DeFi ecosystems.Trained on 100,000 real-world\ndiscourse episodes and validated in adversarial simulations, Hide-and-Shill\nachieves top performance in detection accuracy and causal attribution. This\nwork bridges multi-agent systems with financial surveillance, advancing a new\nparadigm for decentralized market intelligence. All resources are available at\nthe Hide-and-Shill GitHub repository to promote open research and\nreproducibility.", "AI": {"tldr": "uses a Multi-Agent Reinforcement Learning (MARL) framework for decentralized manipulation detection", "motivation": "Decentralized finance (DeFi) has introduced a new era of permissionless financial innovation but also led to unprecedented market manipulation.", "method": "a Multi-Agent Reinforcement Learning (MARL) framework", "result": "achieves top performance in detection accuracy and causal attribution", "conclusion": "This work bridges multi-agent systems with financial surveillance, advancing a new paradigm for decentralized market intelligence. All resources are available at the Hide-and-Shill GitHub repository to promote open research and reproducibility."}}
{"id": "2507.09008", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09008", "abs": "https://arxiv.org/abs/2507.09008", "authors": ["Xiwei Xuan", "Xiaoqi Wang", "Wenbin He", "Jorge Piazentin Ono", "Liang Gou", "Kwan-Liu Ma", "Liu Ren"], "title": "VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels", "comment": "IEEE Transactions on Visualization and Computer Graphics (2025)", "summary": "The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA)\nhave facilitated the auto-labeling of large-scale datasets, enhancing model\nperformance in challenging downstream tasks such as open-vocabulary object\ndetection and segmentation. However, the quality of FM-generated labels is less\nstudied as existing approaches focus more on data quantity over quality. This\nis because validating large volumes of data without ground truth presents a\nconsiderable challenge in practice. Existing methods typically rely on limited\nmetrics to identify problematic data, lacking a comprehensive perspective, or\napply human validation to only a small data fraction, failing to address the\nfull spectrum of potential issues. To overcome these challenges, we introduce\nVISTA, a visual analytics framework that improves data quality to enhance the\nperformance of multi-modal models. Targeting the complex and demanding domain\nof open-vocabulary image segmentation, VISTA integrates multi-phased data\nvalidation strategies with human expertise, enabling humans to identify,\nunderstand, and correct hidden issues within FM-generated labels. Through\ndetailed use cases on two benchmark datasets and expert reviews, we demonstrate\nVISTA's effectiveness from both quantitative and qualitative perspectives.", "AI": {"tldr": "VISTA\u662f\u4e00\u4e2a\u89c6\u89c9\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u9636\u6bb5\u6570\u636e\u9a8c\u8bc1\u548c\u4eba\u5de5\u4e13\u4e1a\u77e5\u8bc6\u6765\u63d0\u9ad8\u591a\u6a21\u6001\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u6570\u636e\u6570\u91cf\u800c\u975e\u8d28\u91cf\uff0c\u5e76\u4e14\u7f3a\u4e4f\u5168\u9762\u7684\u89c6\u89d2\u6765\u8bc6\u522b\u95ee\u9898\u6570\u636e\uff0c\u6216\u8005\u4ec5\u5bf9\u4e00\u5c0f\u90e8\u5206\u6570\u636e\u8fdb\u884c\u4eba\u5de5\u9a8c\u8bc1\uff0c\u65e0\u6cd5\u89e3\u51b3\u6240\u6709\u6f5c\u5728\u95ee\u9898\u3002", "method": "VISTA\uff0c\u4e00\u4e2a\u89c6\u89c9\u5206\u6790\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u591a\u9636\u6bb5\u6570\u636e\u9a8c\u8bc1\u7b56\u7565\u4e0e\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e13\u5bb6\u8bc4\u5ba1\u7684\u8be6\u7ec6\u7528\u4f8b\u4e2d\uff0c\u4ece\u5b9a\u91cf\u548c\u5b9a\u6027\u7684\u89d2\u5ea6\u8bc1\u660e\u4e86VISTA\u7684\u6709\u6548\u6027\u3002", "conclusion": "VISTA\u901a\u8fc7\u6574\u5408\u591a\u9636\u6bb5\u6570\u636e\u9a8c\u8bc1\u7b56\u7565\u4e0e\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\uff0c\u80fd\u591f\u8bc6\u522b\u3001\u7406\u89e3\u548c\u7ea0\u6b63FM\u751f\u6210\u6807\u7b7e\u4e2d\u9690\u85cf\u7684\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u6570\u636e\u8d28\u91cf\u5e76\u589e\u5f3a\u591a\u6a21\u6001\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.09566", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09566", "abs": "https://arxiv.org/abs/2507.09566", "authors": ["Timo Wilm", "Philipp Normann"], "title": "Identifying Offline Metrics that Predict Online Impact: A Pragmatic Strategy for Real-World Recommender Systems", "comment": "This work was accepted for publication in the 19th ACM Conference on\n  Recommender Systems (RecSys 2025). The final published version will be\n  available at the ACM Digital Library", "summary": "A critical challenge in recommender systems is to establish reliable\nrelationships between offline and online metrics that predict real-world\nperformance. Motivated by recent advances in Pareto front approximation, we\nintroduce a pragmatic strategy for identifying offline metrics that align with\nonline impact. A key advantage of this approach is its ability to\nsimultaneously serve multiple test groups, each with distinct offline\nperformance metrics, in an online experiment controlled by a single model. The\nmethod is model-agnostic for systems with a neural network backbone, enabling\nbroad applicability across architectures and domains. We validate the strategy\nthrough a large-scale online experiment in the field of session-based\nrecommender systems on the OTTO e-commerce platform. The online experiment\nidentifies significant alignments between offline metrics and real-word\nclick-through rate, post-click conversion rate and units sold. Our strategy\nprovides industry practitioners with a valuable tool for understanding\noffline-to-online metric relationships and making informed, data-driven\ndecisions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bc6\u522b\u4e0e\u5728\u7ebf\u5f71\u54cd\u76f8\u4e00\u81f4\u7684\u79bb\u7ebf\u6307\u6807\u7684\u5b9e\u7528\u7b56\u7565\uff0c\u5e76\u901a\u8fc7 OTTO \u7535\u5b50\u5546\u52a1\u5e73\u53f0\u4e0a\u7684\u5927\u89c4\u6a21\u5728\u7ebf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\u662f\u5efa\u7acb\u53ef\u9760\u7684\u79bb\u7ebf\u548c\u5728\u7ebf\u6307\u6807\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4ee5\u9884\u6d4b\u771f\u5b9e\u4e16\u754c\u7684\u6027\u80fd\u3002\u53d7 Pareto \u524d\u6cbf\u903c\u8fd1\u7684\u6700\u65b0\u8fdb\u5c55\u7684\u63a8\u52a8\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u7b56\u7565\uff0c\u7528\u4e8e\u8bc6\u522b\u4e0e\u5728\u7ebf\u5f71\u54cd\u76f8\u4e00\u81f4\u7684\u79bb\u7ebf\u6307\u6807\u3002\u8be5\u65b9\u6cd5\u5bf9\u4e8e\u5177\u6709\u795e\u7ecf\u7f51\u7edc\u4e3b\u5e72\u7684\u7cfb\u7edf\u662f\u6a21\u578b\u65e0\u5173\u7684\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u8de8\u67b6\u6784\u548c\u9886\u57df\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "result": "\u5728\u7ebf\u5b9e\u9a8c\u786e\u5b9a\u4e86\u79bb\u7ebf\u6307\u6807\u4e0e\u771f\u5b9e\u70b9\u51fb\u7387\u3001\u70b9\u51fb\u540e\u8f6c\u5316\u7387\u548c\u5df2\u552e\u5355\u4f4d\u4e4b\u95f4\u7684\u663e\u7740\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u7b56\u7565\u4e3a\u884c\u4e1a\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u5de5\u5177\uff0c\u53ef\u4ee5\u7406\u89e3\u79bb\u7ebf\u5230\u5728\u7ebf\u7684\u6307\u6807\u5173\u7cfb\uff0c\u5e76\u505a\u51fa\u660e\u667a\u7684\u3001\u6570\u636e\u9a71\u52a8\u7684\u51b3\u7b56\u3002"}}
{"id": "2507.10070", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.10070", "abs": "https://arxiv.org/abs/2507.10070", "authors": ["Yang Xiao", "Mo Sun", "Ziyu Song", "Bing Tian", "Jie Zhang", "Jie Sun", "Zeke Wang"], "title": "Breaking the Storage-Compute Bottleneck in Billion-Scale ANNS: A GPU-Driven Asynchronous I/O Framework", "comment": null, "summary": "With the advancement of information retrieval, recommendation systems, and\nRetrieval-Augmented Generation (RAG), Approximate Nearest Neighbor Search\n(ANNS) gains widespread applications due to its higher performance and\naccuracy. While several disk-based ANNS systems have emerged to handle\nexponentially growing vector datasets, they suffer from suboptimal performance\ndue to two inherent limitations: 1) failing to overlap SSD accesses with\ndistance computation processes and 2) extended I/O latency caused by suboptimal\nI/O Stack. To address these challenges, we present FlashANNS, a GPU-accelerated\nout-of-core graph-based ANNS system through I/O-compute overlapping. Our core\ninsight lies in the synchronized orchestration of I/O and computation through\nthree key innovations: 1) Dependency-Relaxed asynchronous pipeline: FlashANNS\ndecouples I/O-computation dependencies to fully overlap between GPU distance\ncalculations and SSD data transfers. 2) Warp-Level concurrent SSD access:\nFlashANNS implements a lock-free I/O stack with warp-level concurrency control,\nto reduce the latency-induced time overhead. 3) Computation-I/O balanced graph\ndegree Selection: FlashANNS selects graph degrees via lightweight\ncompute-to-I/O ratio sampling, ensuring optimal balance between computational\nload and storage access latency across different I/O bandwidth configurations.\nWe implement FlashANNS and compare it with state-of-the-art out-of-core ANNS\nsystems (SPANN, DiskANN) and a GPU-accelerated out-of-core ANNS system\n(FusionANNS). Experimental results demonstrate that at $\\geq$95\\% recall@10\naccuracy, our method achieves 2.3-5.9$\\times$ higher throughput compared to\nexisting SOTA methods with a single SSD, and further attains 2.7-12.2$\\times$\nthroughput improvement in multi-SSD configurations.", "AI": {"tldr": "FlashANNS is a GPU-accelerated out-of-core graph-based ANNS system that improves throughput by overlapping I/O and computation.", "motivation": "Existing disk-based ANNS systems suffer from suboptimal performance due to failing to overlap SSD accesses with distance computation and extended I/O latency caused by suboptimal I/O Stack.", "method": "FlashANNS, a GPU-accelerated out-of-core graph-based ANNS system through I/O-compute overlapping, featuring Dependency-Relaxed asynchronous pipeline, Warp-Level concurrent SSD access, and Computation-I/O balanced graph degree Selection.", "result": "FlashANNS achieves 2.3-5.9x higher throughput compared to existing SOTA methods with a single SSD, and further attains 2.7-12.2x throughput improvement in multi-SSD configurations at $\\geq$95% recall@10 accuracy.", "conclusion": "FlashANNS achieves 2.3-5.9x higher throughput compared to existing SOTA methods with a single SSD, and further attains 2.7-12.2x throughput improvement in multi-SSD configurations at $\\geq$95% recall@10 accuracy."}}
{"id": "2507.09011", "categories": ["cs.CL", "q-bio.NC", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.09011", "abs": "https://arxiv.org/abs/2507.09011", "authors": ["Ana Chkhaidze", "Reshanne R. Reeder", "Connor Gag", "Anastasia Kiyonaga", "Seana Coulson"], "title": "Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery", "comment": null, "summary": "A rapidly alternating red and black display known as Ganzflicker induces\nvisual hallucinations that reflect the generative capacity of the visual\nsystem. Recent proposals regarding the imagery spectrum, that is, differences\nin the visual system of individuals with absent imagery, typical imagery, and\nvivid imagery, suggest these differences should impact the complexity of other\ninternally generated visual experiences. Here, we used tools from natural\nlanguage processing to analyze free-text descriptions of hallucinations from\nover 4,000 participants, asking whether people with different imagery\nphenotypes see different things in their mind's eye during Ganzflicker-induced\nhallucinations. Strong imagers described complex, naturalistic content, while\nweak imagers reported simple geometric patterns. Embeddings from vision\nlanguage models better captured these differences than text-only language\nmodels, and participants with stronger imagery used language with richer\nsensorimotor associations. These findings may reflect individual variation in\ncoordination between early visual areas and higher-order regions relevant for\nthe imagery spectrum.", "AI": {"tldr": "Strong imagers of Ganzflicker-induced hallucinations reported complex, naturalistic content, while weak imagers reported simple geometric patterns.", "motivation": "Recent proposals regarding the imagery spectrum suggest differences should impact the complexity of other internally generated visual experiences.", "method": "used tools from natural language processing to analyze free-text descriptions of hallucinations from over 4,000 participants", "result": "Embeddings from vision language models better captured these differences than text-only language models, and participants with stronger imagery used language with richer sensorimotor associations.", "conclusion": "Strong imagers reported complex, naturalistic content, while weak imagers reported simple geometric patterns. These findings may reflect individual variation in coordination between early visual areas and higher-order regions relevant for the imagery spectrum."}}
{"id": "2507.08836", "categories": ["cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.08836", "abs": "https://arxiv.org/abs/2507.08836", "authors": ["Damien Fovet", "Shashank Chamoli", "Sarah Oury", "Srishti Singhal"], "title": "Accuracy and Consumption analysis from a compressed model by CompactifAI from Multiverse Computing", "comment": null, "summary": "This study evaluates the performance of a compression method, called\nCompactifAI, developed by Multiverse Computing, applied to the large language\nmodel Llama 3.1 8B\\cite{llama}. The evaluation focused on model efficiency (in\nterms of energy consumption) and accuracy using respectively the frameworks\nCodecarbon\\cite{codecarbon} and Ragas\\cite{ragas}. A comparison was performed\nbetween the model compressed with\nCompactifAI\\cite{compactifai}\\cite{compactifai2} and its full-size version. Our\nfindings reveal that the compressed model using CompactifAI not only\nsignificantly reduced the computational resources but also maintained the model\naccuracy, making the model more efficient, scalable and cost-effective.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86 CompactifAI \u538b\u7f29 Llama 3.1 8B \u6a21\u578b\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u8be5\u65b9\u6cd5\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u8bc4\u4f30 Multiverse Computing \u5f00\u53d1\u7684\u540d\u4e3a CompactifAI \u7684\u538b\u7f29\u65b9\u6cd5\u5e94\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b Llama 3.1 8B \u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528 Codecarbon \u548c Ragas \u6846\u67b6\u5206\u522b\u8bc4\u4f30\u6a21\u578b\u6548\u7387\uff08\u80fd\u6e90\u6d88\u8017\uff09\u548c\u51c6\u786e\u6027\u3002\u6bd4\u8f83\u4e86\u4f7f\u7528 CompactifAI \u538b\u7f29\u7684\u6a21\u578b\u53ca\u5176\u5b8c\u6574\u7248\u672c\u3002", "result": "\u538b\u7f29\u540e\u7684\u6a21\u578b\u5728\u4f7f\u7528 CompactifAI \u540e\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7cbe\u5ea6\u3002", "conclusion": "\u538b\u7f29\u540e\u7684\u6a21\u578b\u5728\u4f7f\u7528 CompactifAI \u540e\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d44\u6e90\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7cbe\u5ea6\uff0c\u4f7f\u6a21\u578b\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u66f4\u5177\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2507.09329", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.09329", "abs": "https://arxiv.org/abs/2507.09329", "authors": ["Matous Kozak", "Roshanak Zilouchian Moghaddam", "Siva Sivaraman"], "title": "When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents", "comment": "15 pages", "summary": "LLM-based coding agents are rapidly being deployed in software development,\nyet their security implications remain poorly understood. These agents, while\ncapable of accelerating software development, may inadvertently introduce\ninsecure practices. We conducted the first systematic security evaluation of\nautonomous coding agents, analyzing over 12,000 actions across five\nstate-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world\nsoftware setup tasks. Our findings reveal significant security concerns: 21% of\nagent trajectories contained insecure actions, with models showing substantial\nvariation in security behavior. We developed a high-precision detection system\nthat identified four major vulnerability categories, with information exposure\n(CWE-200) being the most prevalent one. We also evaluated mitigation strategies\nincluding feedback mechanisms and security reminders with various effectiveness\nbetween models. GPT-4.1 demonstrated exceptional security awareness with 96.8%\nmitigation success. Our work provides the first comprehensive framework for\nevaluating coding agent security and highlights the need for security-aware\ndesign of next generation LLM-based coding agents.", "AI": {"tldr": "This paper evaluates the security of LLM-based coding agents, finding that they often introduce security vulnerabilities. The authors also propose mitigation strategies.", "motivation": "The security implications of LLM-based coding agents are poorly understood, and these agents may inadvertently introduce insecure practices.", "method": "The authors conducted a systematic security evaluation of autonomous coding agents, analyzing over 12,000 actions across five state-of-the-art models on 93 real-world software setup tasks. They also developed a high-precision detection system and evaluated mitigation strategies.", "result": "21% of agent trajectories contained insecure actions, with information exposure being the most prevalent vulnerability. GPT-4.1 demonstrated exceptional security awareness with 96.8% mitigation success.", "conclusion": "This paper provides a comprehensive framework for evaluating the security of coding agents, highlights the need for security-aware design, and evaluates mitigation strategies."}}
{"id": "2507.09036", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09036", "abs": "https://arxiv.org/abs/2507.09036", "authors": ["Florian Kofler", "Marcel Rosier", "Mehdi Astaraki", "Hendrik M\u00f6ller", "Ilhem Isra Mekki", "Josef A. Buchner", "Anton Schmick", "Arianna Pfiffer", "Eva Oswald", "Lucas Zimmer", "Ezequiel de la Rosa", "Sarthak Pati", "Julian Canisius", "Arianna Piffer", "Ujjwal Baid", "Mahyar Valizadeh", "Akis Linardos", "Jan C. Peeken", "Surprosanna Shit", "Felix Steinbauer", "Daniel Rueckert", "Rolf Heckemann", "Spyridon Bakas", "Jan Kirschke", "Constantin von See", "Ivan Ezhov", "Marie Piraud", "Benedikt Wiestler", "Bjoern Menze"], "title": "BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis", "comment": "16p, 3f", "summary": "BrainLesion Suite is a versatile toolkit for building modular brain lesion\nimage analysis pipelines in Python. Following Pythonic principles, BrainLesion\nSuite is designed to provide a 'brainless' development experience, minimizing\ncognitive effort and streamlining the creation of complex workflows for\nclinical and scientific practice. At its core is an adaptable preprocessing\nmodule that performs co-registration, atlas registration, and optional\nskull-stripping and defacing on arbitrary multi-modal input images. BrainLesion\nSuite leverages algorithms from the BraTS challenge to synthesize missing\nmodalities, inpaint lesions, and generate pathology-specific tumor\nsegmentations. BrainLesion Suite also enables quantifying segmentation model\nperformance, with tools such as panoptica to compute lesion-wise metrics.\nAlthough BrainLesion Suite was originally developed for image analysis\npipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,\nit can be adapted for other biomedical image analysis applications. The\nindividual BrainLesion Suite packages and tutorials are accessible on GitHub.", "AI": {"tldr": "BrainLesion Suite\u662f\u4e00\u4e2a\u7528\u4e8e\u6784\u5efa\u8111\u75c5\u7076\u56fe\u50cf\u5206\u6790\u6d41\u7a0b\u7684Python\u5de5\u5177\u5305\u3002", "motivation": "BrainLesion Suite\u662f\u4e00\u4e2a\u591a\u529f\u80fd\u7684\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u5728Python\u4e2d\u6784\u5efa\u6a21\u5757\u5316\u8111\u75c5\u7076\u56fe\u50cf\u5206\u6790\u6d41\u7a0b\u3002", "method": "BrainLesion Suite\u5229\u7528\u6765\u81eaBraTS challenge\u7684\u7b97\u6cd5\u6765\u5408\u6210\u7f3a\u5931\u7684\u6a21\u6001\uff0c\u4fee\u590d\u75c5\u7076\uff0c\u5e76\u4e14\u751f\u6210\u7279\u5b9a\u4e8e\u75c5\u7406\u7684\u80bf\u7624\u5206\u5272\u3002", "result": "BrainLesion Suite\u4e5f\u80fd\u591f\u91cf\u5316\u5206\u5272\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f7f\u7528\u8bf8\u5982panoptica\u4e4b\u7c7b\u7684\u5de5\u5177\u6765\u8ba1\u7b97\u75c5\u7076\u65b9\u9762\u7684\u6307\u6807\u3002", "conclusion": "BrainLesion Suite\u53ef\u4ee5\u88ab\u8c03\u6574\u7528\u4e8e\u5176\u4ed6\u7684\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u5e94\u7528\u3002"}}
{"id": "2507.09924", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09924", "abs": "https://arxiv.org/abs/2507.09924", "authors": ["Tuan-Luc Huynh", "Thuy-Trang Vu", "Weiqing Wang", "Trung Le", "Dragan Ga\u0161evi\u0107", "Yuan-Fang Li", "Thanh-Toan Do"], "title": "MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for Rehearsal-Free Generative Retrieval over Dynamic Corpora", "comment": null, "summary": "Continually updating model-based indexes in generative retrieval with new\ndocuments remains challenging, as full retraining is computationally expensive\nand impractical under resource constraints. We propose MixLoRA-DSI, a novel\nframework that combines an expandable mixture of Low-Rank Adaptation experts\nwith a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead\nof allocating new experts for each new corpus, our proposed expansion strategy\nenables sublinear parameter growth by selectively introducing new experts only\nwhen significant number of OOD documents are detected. Experiments on NQ320k\nand MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update\nbaselines, with minimal parameter overhead and substantially lower training\ncosts.", "AI": {"tldr": "MixLoRA-DSI \u901a\u8fc7\u7ed3\u5408 LoRA \u4e13\u5bb6\u6df7\u5408\u548c OOD \u9a71\u52a8\u7684\u6269\u5c55\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6a21\u578b\u66f4\u65b0\uff0c\u4f18\u4e8e\u5168\u6a21\u578b\u66f4\u65b0\uff0c\u4e14\u53c2\u6570\u5f00\u9500\u548c\u8bad\u7ec3\u6210\u672c\u66f4\u4f4e\u3002", "motivation": "\u5728\u751f\u6210\u5f0f\u68c0\u7d22\u4e2d\uff0c\u5728\u8d44\u6e90\u7ea6\u675f\u4e0b\uff0c\u7531\u4e8e\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4e0d\u5207\u5b9e\u9645\uff0c\u56e0\u6b64\u4f7f\u7528\u65b0\u6587\u6863\u4e0d\u65ad\u66f4\u65b0\u57fa\u4e8e\u6a21\u578b\u7684\u7d22\u5f15\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u7ed3\u5408\u4e86\u53ef\u6269\u5c55\u7684\u4f4e\u79e9\u9002\u5e94\u4e13\u5bb6\u6df7\u5408\u548c\u5206\u5c42\u5f02\u5206\u5e03\uff08OOD\uff09\u9a71\u52a8\u7684\u6269\u5c55\u7b56\u7565\u3002", "result": "\u5728 NQ320k \u548c MS MARCO Passage \u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMixLoRA-DSI \u4f18\u4e8e\u5168\u6a21\u578b\u66f4\u65b0\u57fa\u7ebf\u3002", "conclusion": "MixLoRA-DSI\u5728\u53c2\u6570\u5f00\u9500\u6700\u5c0f\u548c\u8bad\u7ec3\u6210\u672c\u5927\u5e45\u964d\u4f4e\u7684\u60c5\u51b5\u4e0b\uff0c\u4f18\u4e8e\u5168\u6a21\u578b\u66f4\u65b0\u57fa\u7ebf\u3002"}}
{"id": "2507.10337", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.10337", "abs": "https://arxiv.org/abs/2507.10337", "authors": ["Benzhao Tang", "Shiyu Yang", "Zhitao Shen", "Wenjie Zhang", "Xuemin Lin", "Zhihong Tian"], "title": "LogLite: Lightweight Plug-and-Play Streaming Log Compression", "comment": "accepted by VLDB 2025", "summary": "Log data is a vital resource for capturing system events and states. With the\nincreasing complexity and widespread adoption ofmodern software systems and IoT\ndevices, the daily volume of log generation has surged to tens of petabytes,\nleading to significant collection and storage costs. To address this challenge,\nlossless log compression has emerged as an effective solution, enabling\nsubstantial resource savings without compromising log information. In this\npaper, we first conduct a characterization study on extensive public log\ndatasets and identify four key observations. Building on these insights, we\npropose LogLite, a lightweight, plug-and-play, streaming lossless compression\nalgorithm designed to handle both TEXT and JSON logs throughout their life\ncycle. LogLite requires no predefined rules or pre-training and is inherently\nadaptable to evolving log structures. Our evaluation shows that, compared to\nstate-of-the-art baselines, LogLite achieves Pareto optimality in most\nscenarios, delivering an average improvement of up to 67.8% in compression\nratio and up to 2.7 $\\times$ in compression speed.", "AI": {"tldr": "This paper introduces LogLite, a new lossless log compression algorithm that is lightweight, adaptable, and achieves better compression ratio and speed compared to existing methods.", "motivation": "The daily volume of log generation has surged to tens of petabytes, leading to significant collection and storage costs.", "method": "propose LogLite, a lightweight, plug-and-play, streaming lossless compression algorithm designed to handle both TEXT and JSON logs throughout their life cycle. LogLite requires no predefined rules or pre-training and is inherently adaptable to evolving log structures.", "result": "LogLite achieves Pareto optimality in most scenarios, delivering an average improvement of up to 67.8% in compression ratio and up to 2.7$\\times$ in compression speed.", "conclusion": "LogLite achieves Pareto optimality in most scenarios, delivering an average improvement of up to 67.8% in compression ratio and up to 2.7$\\times$ in compression speed compared to state-of-the-art baselines."}}
{"id": "2507.09025", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09025", "abs": "https://arxiv.org/abs/2507.09025", "authors": ["Chien Van Nguyen", "Ruiyi Zhang", "Hanieh Deilamsalehy", "Puneet Mathur", "Viet Dac Lai", "Haoliang Wang", "Jayakumar Subramanian", "Ryan A. Rossi", "Trung Bui", "Nikos Vlassis", "Franck Dernoncourt", "Thien Huu Nguyen"], "title": "Lizard: An Efficient Linearization Framework for Large Language Models", "comment": "15 pages", "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.", "AI": {"tldr": "Lizard\u662f\u4e00\u79cd\u7ebf\u6027\u5316\u6846\u67b6\uff0c\u53ef\u4ee5\u5c06\u57fa\u4e8eTransformer\u7684\u9884\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u8f6c\u6362\u4e3a\u7075\u6d3b\u7684\u3001\u4e9a\u4e8c\u6b21\u67b6\u6784\uff0c\u7528\u4e8e\u65e0\u9650\u4e0a\u4e0b\u6587\u751f\u6210\u3002", "motivation": "\u57fa\u4e8eTransformer\u7684LLM\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u589e\u52a0\uff0c\u9762\u4e34\u7740\u663e\u8457\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u74f6\u9888\uff0c\u8fd9\u662f\u7531\u4e8esoftmax\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u590d\u6742\u6027\u548c\u4e0d\u65ad\u589e\u957f\u7684\u952e\u503c(KV)\u7f13\u5b58\u3002", "method": "Lizard\u7ed3\u5408\u4e86\u7528\u4e8e\u5168\u5c40\u4e0a\u4e0b\u6587\u538b\u7f29\u7684\u95e8\u63a7\u7ebf\u6027\u6ce8\u610f\u529b\u4e0e\u901a\u8fc7\u5143\u8bb0\u5fc6\u589e\u5f3a\u7684\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\uff0c\u5f62\u6210\u4e86\u4e00\u79cd\u6df7\u5408\u673a\u5236\uff0c\u53ef\u4ee5\u6355\u83b7\u8fdc\u7a0b\u4f9d\u8d56\u5173\u7cfb\u548c\u7ec6\u7c92\u5ea6\u7684\u5c40\u90e8\u4ea4\u4e92\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u786c\u4ef6\u611f\u77e5\u7b97\u6cd5\uff0c\u53ef\u4ee5\u52a0\u901f\u6211\u4eec\u6a21\u578b\u7684\u8bad\u7ec3\u901f\u5ea6\u3002", "result": "Lizard\u6539\u8fdb\u4e86\u5148\u524d\u7684\u6a21\u578b18\u5206\uff0c\u5e76\u5728\u8054\u60f3\u53ec\u56de\u4efb\u52a1\u4e0a\u663e\u793a\u51fa\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "Lizard\u5728\u6807\u51c6\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u63a5\u8fd1\u65e0\u635f\u7684\u6559\u5e08\u6a21\u578b\u6027\u80fd\u6062\u590d\uff0c\u540c\u65f6\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u7ebf\u6027\u5316\u65b9\u6cd5\u3002\u57285-shot MMLU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLizard\u6bd4\u4e4b\u524d\u7684\u6a21\u578b\u63d0\u9ad8\u4e8618\u4e2a\u70b9\uff0c\u5e76\u5728\u8054\u60f3\u53ec\u56de\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u663e\u8457\u7684\u6539\u8fdb\u3002"}}
{"id": "2507.08838", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.08838", "abs": "https://arxiv.org/abs/2507.08838", "authors": ["Xiaohang Tang", "Rares Dolga", "Sangwoong Yoon", "Ilija Bogunovic"], "title": "wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models", "comment": "Preprint", "summary": "Improving the reasoning capabilities of diffusion-based large language models\n(dLLMs) through reinforcement learning (RL) remains an open problem. The\nintractability of dLLMs likelihood function necessitates approximating the\ncurrent, old, and reference policy likelihoods at each policy optimization\nstep. This reliance introduces additional computational overhead and lead to\npotentially large bias -- particularly when approximation errors occur in the\ndenominator of policy ratios used for importance sampling. To mitigate these\nissues, we introduce $\\mathtt{wd1}$, a novel policy optimization approach that\nreformulates the objective as a weighted likelihood, requiring only a single\napproximation for the current parametrized policy likelihood. Experiments on\nwidely used reasoning benchmarks demonstrate that $\\mathtt{wd1}$, without\nsupervised fine-tuning (SFT) or any supervised data, outperforms existing RL\nmethods for dLLMs, achieving up to 16% higher accuracy. $\\mathtt{wd1}$ delivers\nadditional computational gains, including reduced training time and fewer\nfunction evaluations (NFEs) per gradient step. These findings, combined with\nthe simplicity of method's implementation and R1-Zero-like training (no SFT),\nposition $\\mathtt{wd1}$ as a more effective and efficient method for applying\nRL to dLLMs reasoning.", "AI": {"tldr": "wd1 \u662f\u4e00\u79cd\u7528\u4e8e dLLM \u63a8\u7406\u7684\u5f3a\u5316\u5b66\u4e60\u65b0\u65b9\u6cd5\uff0c\u65e0\u9700\u76d1\u7763\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60 (RL) \u63d0\u9ad8\u57fa\u4e8e\u6269\u6563\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (dLLM) \u7684\u63a8\u7406\u80fd\u529b\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u95ee\u9898\u3002dLLM \u4f3c\u7136\u51fd\u6570\u7684\u96be\u5904\u7406\u6027\u9700\u8981\u5728\u6bcf\u4e2a\u7b56\u7565\u4f18\u5316\u6b65\u9aa4\u4e2d\u8fd1\u4f3c\u5f53\u524d\u3001\u65e7\u7684\u548c\u53c2\u8003\u7b56\u7565\u4f3c\u7136\u3002\u8fd9\u79cd\u4f9d\u8d56\u6027\u5e26\u6765\u4e86\u989d\u5916\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u53ef\u80fd\u5bfc\u81f4\u8f83\u5927\u7684\u504f\u5dee\u2014\u2014\u5c24\u5176\u662f\u5728\u91cd\u8981\u6027\u91c7\u6837\u7684\u7b56\u7565\u6bd4\u7387\u7684 denominator \u4e2d\u51fa\u73b0\u8fd1\u4f3c\u8bef\u5dee\u65f6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u5c06\u76ee\u6807\u91cd\u65b0\u5b9a\u4e49\u4e3a\u52a0\u6743\u4f3c\u7136\uff0c\u53ea\u9700\u8981\u5bf9\u5f53\u524d\u53c2\u6570\u5316\u7b56\u7565\u4f3c\u7136\u8fdb\u884c\u4e00\u6b21\u8fd1\u4f3c\u3002", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u63a8\u7406\u57fa\u51c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cwd1 \u5728\u6ca1\u6709\u76d1\u7763\u5fae\u8c03 (SFT) \u6216\u4efb\u4f55\u76d1\u7763\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u4f18\u4e8e\u73b0\u6709\u7684 dLLM \u7684 RL \u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe 16% \u7684\u51c6\u786e\u7387\u63d0\u5347\u3002wd1 \u63d0\u4f9b\u4e86\u989d\u5916\u7684\u8ba1\u7b97\u589e\u76ca\uff0c\u5305\u62ec\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u548c\u6bcf\u4e2a\u68af\u5ea6\u6b65\u9aa4\u7684\u51fd\u6570\u8bc4\u4f30\u6b21\u6570 (NFE)\u3002", "conclusion": "wd1 \u662f\u4e00\u79cd\u66f4\u6709\u6548\u548c\u9ad8\u6548\u7684\u5c06 RL \u5e94\u7528\u4e8e dLLMs \u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u56e0\u4e3a\u5b83\u5b9e\u73b0\u7b80\u5355\uff0c\u5e76\u4e14\u7c7b\u4f3c\u4e8e R1-Zero \u7684\u8bad\u7ec3\uff08\u6ca1\u6709 SFT\uff09\u3002"}}
{"id": "2507.09369", "categories": ["cs.AI", "68T01", "I.2.0"], "pdf": "https://arxiv.org/pdf/2507.09369", "abs": "https://arxiv.org/abs/2507.09369", "authors": ["Andrew Critch", "Jacob Tsimerman"], "title": "A Taxonomy of Omnicidal Futures Involving Artificial Intelligence", "comment": null, "summary": "This report presents a taxonomy and examples of potential omnicidal events\nresulting from AI: scenarios where all or almost all humans are killed. These\nevents are not presented as inevitable, but as possibilities that we can work\nto avoid. Insofar as large institutions require a degree of public support in\norder to take certain actions, we hope that by presenting these possibilities\nin public, we can help to support preventive measures against catastrophic\nrisks from AI.", "AI": {"tldr": "The report presents a taxonomy and examples of potential omnicidal events resulting from AI, and hopes to support preventive measures against catastrophic risks from AI by presenting these possibilities in public.", "motivation": "The report aims to present potential omnicidal events resulting from AI.", "method": "The report presents a taxonomy and examples of potential omnicidal events resulting from AI.", "result": "The report presents scenarios where all or almost all humans are killed.", "conclusion": "The report hopes to support preventive measures against catastrophic risks from AI by presenting these possibilities in public."}}
{"id": "2507.09052", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09052", "abs": "https://arxiv.org/abs/2507.09052", "authors": ["Fang Chen", "Alex Villa", "Gongbo Liang", "Xiaoyi Lu", "Meng Tang"], "title": "Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?", "comment": "20 pages, 11 figures", "summary": "Training data for class-conditional image synthesis often exhibit a\nlong-tailed distribution with limited images for tail classes. Such an\nimbalance causes mode collapse and reduces the diversity of synthesized images\nfor tail classes. For class-conditional diffusion models trained on imbalanced\ndata, we aim to improve the diversity of tail class images without compromising\nthe fidelity and diversity of head class images. We achieve this by introducing\ntwo deceptively simple but highly effective contrastive loss functions.\nFirstly, we employ an unsupervised InfoNCE loss utilizing negative samples to\nincrease the distance/dissimilarity among synthetic images, particularly for\ntail classes. To further enhance the diversity of tail classes, our second loss\nis an MSE loss that contrasts class-conditional generation with unconditional\ngeneration at large timesteps. This second loss makes the denoising process\ninsensitive to class conditions for the initial steps, which enriches tail\nclasses through knowledge sharing from head classes. Conditional-unconditional\nalignment has been shown to enhance the performance of long-tailed GAN. We are\nthe first to adapt such alignment to diffusion models. We successfully\nleveraged contrastive learning for class-imbalanced diffusion models. Our\ncontrastive learning framework is easy to implement and outperforms standard\nDDPM and alternative methods for class-imbalanced diffusion models across\nvarious datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and\nImageNetLT.", "AI": {"tldr": "\u9488\u5bf9\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6269\u6563\u6a21\u578b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u589e\u52a0\u5408\u6210\u56fe\u50cf\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u63d0\u9ad8\u5c3e\u90e8\u7c7b\u522b\u56fe\u50cf\u7684\u591a\u6837\u6027\u3002", "motivation": "\u7c7b\u522b\u6761\u4ef6\u56fe\u50cf\u5408\u6210\u7684\u8bad\u7ec3\u6570\u636e\u901a\u5e38\u5448\u73b0\u957f\u5c3e\u5206\u5e03\uff0c\u5c3e\u90e8\u7c7b\u522b\u7684\u56fe\u50cf\u6709\u9650\u3002\u8fd9\u79cd\u4e0d\u5e73\u8861\u4f1a\u5bfc\u81f4\u6a21\u5f0f\u5d29\u6e83\u5e76\u964d\u4f4e\u5c3e\u90e8\u7c7b\u522b\u7684\u5408\u6210\u56fe\u50cf\u7684\u591a\u6837\u6027\u3002\u5bf9\u4e8e\u5728\u4e0d\u5e73\u8861\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u7c7b\u522b\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u63d0\u9ad8\u5c3e\u90e8\u7c7b\u522b\u56fe\u50cf\u7684\u591a\u6837\u6027\uff0c\u800c\u4e0d\u5f71\u54cd\u5934\u90e8\u7c7b\u522b\u56fe\u50cf\u7684\u4fdd\u771f\u5ea6\u548c\u591a\u6837\u6027\u3002", "method": "\u6211\u4eec\u901a\u8fc7\u5f15\u5165\u4e24\u4e2a\u770b\u4f3c\u7b80\u5355\u4f46\u975e\u5e38\u6709\u6548\u7684\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\u6765\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002\u9996\u5148\uff0c\u6211\u4eec\u91c7\u7528\u65e0\u76d1\u7763\u7684 InfoNCE \u635f\u5931\uff0c\u5229\u7528\u8d1f\u6837\u672c\u6765\u589e\u52a0\u5408\u6210\u56fe\u50cf\u4e4b\u95f4\u7684\u8ddd\u79bb/\u5dee\u5f02\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5c3e\u90e8\u7c7b\u522b\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5c3e\u90e8\u7c7b\u522b\u7684\u591a\u6837\u6027\uff0c\u6211\u4eec\u7684\u7b2c\u4e8c\u4e2a\u635f\u5931\u662f\u4e00\u4e2a MSE \u635f\u5931\uff0c\u5b83\u5bf9\u6bd4\u4e86\u5927\u578b\u65f6\u95f4\u6b65\u957f\u7684\u6709\u6761\u4ef6\u751f\u6210\u548c\u65e0\u6761\u4ef6\u751f\u6210\u3002", "result": "\u6761\u4ef6-\u975e\u6761\u4ef6\u5bf9\u9f50\u5df2\u88ab\u8bc1\u660e\u53ef\u4ee5\u63d0\u9ad8\u957f\u5c3e GAN \u7684\u6027\u80fd\u3002\u6211\u4eec\u662f\u7b2c\u4e00\u4e2a\u5c06\u8fd9\u79cd\u5bf9\u9f50\u9002\u5e94\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4eba\u3002", "conclusion": "\u6211\u4eec\u6210\u529f\u5730\u5c06\u5bf9\u6bd4\u5b66\u4e60\u7528\u4e8e\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6269\u6563\u6a21\u578b\u3002\u6211\u4eec\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u6613\u4e8e\u5b9e\u73b0\uff0c\u5e76\u4e14\u5728\u5404\u79cd\u6570\u636e\u96c6\uff08\u5305\u62ec CIFAR10/100-LT\u3001PlacesLT\u3001TinyImageNetLT \u548c ImageNetLT\uff09\u4e0a\u4f18\u4e8e\u6807\u51c6 DDPM \u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u6269\u6563\u6a21\u578b\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002"}}
{"id": "2507.09969", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.09969", "abs": "https://arxiv.org/abs/2507.09969", "authors": ["Zhongyu Ouyang", "Mingxuan Ju", "Soroush Vosoughi", "Yanfang Ye"], "title": "Non-parametric Graph Convolution for Re-ranking in Recommendation Systems", "comment": "Accepted to RecSys2025 Main", "summary": "Graph knowledge has been proven effective in enhancing item rankings in\nrecommender systems (RecSys), particularly during the retrieval stage. However,\nits application in the ranking stage, especially when richer contextual\ninformation in user-item interactions is available, remains underexplored. A\nmajor challenge lies in the substantial computational cost associated with\nrepeatedly retrieving neighborhood information from billions of items stored in\ndistributed systems. This resource-intensive requirement makes it difficult to\nscale graph-based methods in practical RecSys. To bridge this gap, we first\ndemonstrate that incorporating graphs in the ranking stage improves ranking\nqualities. Notably, while the improvement is evident, we show that the\nsubstantial computational overheads entailed by graphs are prohibitively\nexpensive for real-world recommendations. In light of this, we propose a\nnon-parametric strategy that utilizes graph convolution for re-ranking only\nduring test time. Our strategy circumvents the notorious computational\noverheads from graph convolution during training, and utilizes structural\nknowledge hidden in graphs on-the-fly during testing. It can be used as a\nplug-and-play module and easily employed to enhance the ranking ability of\nvarious ranking layers of a real-world RecSys with significantly reduced\ncomputational overhead. Through comprehensive experiments across four benchmark\ndatasets with varying levels of sparsity, we demonstrate that our strategy\nyields noticeable improvements (i.e., 8.1% on average) during testing time with\nlittle to no additional computational overheads (i.e., 0.5 on average). Code:\nhttps://github.com/zyouyang/RecSys2025_NonParamGC.git", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6d4b\u8bd5\u65f6\u4f7f\u7528\u56fe\u5377\u79ef\u8fdb\u884c\u91cd\u65b0\u6392\u5e8f\u7684\u975e\u53c2\u6570\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u5c3d\u7ba1\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u56fe\u77e5\u8bc6\u5df2\u88ab\u8bc1\u660e\u5728\u63d0\u9ad8\u9879\u76ee\u6392\u540d\u65b9\u9762\u6709\u6548\uff0c\u7279\u522b\u662f\u5728\u68c0\u7d22\u9636\u6bb5\uff0c\u4f46\u5b83\u5728\u6392\u540d\u9636\u6bb5\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5728\u7528\u6237-\u9879\u76ee\u4ea4\u4e92\u4e2d\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u65f6\uff0c\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u4e00\u4e2a\u4e3b\u8981\u7684\u6311\u6218\u5728\u4e8e\u4ece\u5b58\u50a8\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u6570\u5341\u4ebf\u4e2a\u9879\u76ee\u4e2d\u91cd\u590d\u68c0\u7d22\u90bb\u57df\u4fe1\u606f\u76f8\u5173\u7684\u5de8\u5927\u8ba1\u7b97\u6210\u672c\u3002\u8fd9\u79cd\u8d44\u6e90\u5bc6\u96c6\u578b\u9700\u6c42\u4f7f\u5f97\u5728\u5b9e\u9645 RecSys \u4e2d\u6269\u5c55\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u53c2\u6570\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5229\u7528\u56fe\u5377\u79ef\u5728\u6d4b\u8bd5\u65f6\u8fdb\u884c\u91cd\u65b0\u6392\u5e8f\u3002\u8be5\u7b56\u7565\u907f\u514d\u4e86\u8bad\u7ec3\u671f\u95f4\u56fe\u5377\u79ef\u5e26\u6765\u7684\u5de8\u5927\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u5728\u6d4b\u8bd5\u671f\u95f4\u52a8\u6001\u5730\u5229\u7528\u56fe\u4e2d\u9690\u85cf\u7684\u7ed3\u6784\u77e5\u8bc6\u3002\u5b83\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u6a21\u5757\u4f7f\u7528\uff0c\u5e76\u4e14\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u7528\u4e8e\u63d0\u9ad8\u5177\u6709\u663e\u7740\u964d\u4f4e\u7684\u8ba1\u7b97\u5f00\u9500\u7684\u771f\u5b9e RecSys \u7684\u5404\u79cd\u6392\u5e8f\u5c42\u7684\u6392\u5e8f\u80fd\u529b\u3002", "result": "\u5728\u5177\u6709\u4e0d\u540c\u7a00\u758f\u7a0b\u5ea6\u7684\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b56\u7565\u5728\u6d4b\u8bd5\u65f6\u4ea7\u751f\u4e86\u663e\u7740\u6539\u8fdb\uff08\u5373\u5e73\u5747 8.1%\uff09\uff0c\u800c\u51e0\u4e4e\u6ca1\u6709\u989d\u5916\u7684\u8ba1\u7b97\u5f00\u9500\uff08\u5373\u5e73\u5747 0.5%\uff09\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u53c2\u6570\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u4ec5\u5728\u6d4b\u8bd5\u65f6\u4f7f\u7528\u56fe\u5377\u79ef\u8fdb\u884c\u91cd\u65b0\u6392\u5e8f\uff0c\u4ece\u800c\u5728\u51e0\u4e4e\u6ca1\u6709\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u6d4b\u8bd5\u65f6\u4ea7\u751f\u4e86\u663e\u7740\u7684\u6539\u8fdb\uff08\u5e73\u5747 8.1%\uff09\u3002"}}
{"id": "2507.10391", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.10391", "abs": "https://arxiv.org/abs/2507.10391", "authors": ["Mihail Stoian", "Johannes Th\u00fcrauf", "Andreas Zimmerer", "Alexander van Renen", "Andreas Kipf"], "title": "Instance-Optimized String Fingerprints", "comment": "Sixth International Workshop on Applied AI for Database Systems and\n  Applications (AIDB 2025)", "summary": "Recent research found that cloud data warehouses are text-heavy. However,\ntheir capabilities for efficiently processing string columns remain limited,\nrelying primarily on techniques like dictionary encoding and prefix-based\npartition pruning. In recent work, we introduced string fingerprints - a\nlightweight secondary index structure designed to approximate LIKE predicates,\nalbeit with false positives. This approach is particularly compelling for\ncolumnar query engines, where fingerprints can help reduce both compute and I/O\noverhead. We show that string fingerprints can be optimized for specific\nworkloads using mixed-integer optimization, and that they can generalize to\nunseen table predicates. On an IMDb column evaluated in DuckDB v1.3, this\nyields table-scan speedups of up to 1.36$\\times$.", "AI": {"tldr": "String fingerprints are introduced as a lightweight secondary index structure to improve the efficiency of processing string columns in cloud data warehouses, achieving table-scan speedups.", "motivation": "Cloud data warehouses are text-heavy, but their capabilities for efficiently processing string columns remain limited.", "method": "Introduction of string fingerprints - a lightweight secondary index structure designed to approximate LIKE predicates.", "result": "On an IMDb column evaluated in DuckDB v1.3, string fingerprints yield table-scan speedups of up to 1.36x.", "conclusion": "String fingerprints can be optimized for specific workloads and generalize to unseen table predicates, yielding table-scan speedups."}}
{"id": "2507.09037", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09037", "abs": "https://arxiv.org/abs/2507.09037", "authors": ["Bharadwaj Ravichandran", "David Joy", "Paul Elliott", "Brian Hu", "Jadie Adams", "Christopher Funk", "Emily Veenhuis", "Anthony Hoogs", "Arslan Basharat"], "title": "ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making", "comment": "10 pages total (including appendix), ICML 2025 Workshop on Reliable\n  and Responsible Foundation Models", "summary": "Large language models (LLMs) are increasingly being used as decision aids.\nHowever, users have diverse values and preferences that can affect their\ndecision-making, which requires novel methods for LLM alignment and\npersonalization. Existing LLM comparison tools largely focus on benchmarking\ntasks, such as knowledge-based question answering. In contrast, our proposed\nALIGN system focuses on dynamic personalization of LLM-based decision-makers\nthrough prompt-based alignment to a set of fine-grained attributes. Key\nfeatures of our system include robust configuration management, structured\noutput generation with reasoning, and several algorithm implementations with\nswappable LLM backbones, enabling different types of analyses. Our user\ninterface enables a qualitative, side-by-side comparison of LLMs and their\nalignment to various attributes, with a modular backend for easy algorithm\nintegration. Additionally, we perform a quantitative analysis comparing\nalignment approaches in two different domains: demographic alignment for public\nopinion surveys and value alignment for medical triage decision-making. The\nentire ALIGN framework is open source and will enable new research on reliable,\nresponsible, and personalized LLM-based decision-makers.", "AI": {"tldr": "ALIGN\u7cfb\u7edf\u4e13\u6ce8\u4e8e\u57fa\u4e8eLLM\u7684\u51b3\u7b56\u8005\u7684\u52a8\u6001\u4e2a\u6027\u5316\uff0c\u901a\u8fc7\u57fa\u4e8eprompt\u7684\u5bf9\u9f50\u5230\u4e00\u7ec4\u7ec6\u7c92\u5ea6\u7684\u5c5e\u6027\u3002", "motivation": "\u7528\u6237\u6709\u4e0d\u540c\u7684\u4ef7\u503c\u89c2\u548c\u504f\u597d\uff0c\u4f1a\u5f71\u54cd\u4ed6\u4eec\u7684\u51b3\u7b56\uff0c\u8fd9\u9700\u8981LLM\u5bf9\u9f50\u548c\u4e2a\u6027\u5316\u7684\u65b0\u65b9\u6cd5\u3002", "method": "prompt-based alignment", "result": "\u5b9a\u6027\u548c\u5b9a\u91cf\u5206\u6790\u6bd4\u8f83\u4e86\u4e24\u79cd\u4e0d\u540c\u9886\u57df\u4e2d\u7684\u5bf9\u9f50\u65b9\u6cd5\uff1a\u7528\u4e8e\u516c\u4f17\u610f\u89c1\u8c03\u67e5\u7684\u4eba\u53e3\u7edf\u8ba1\u5bf9\u9f50\u548c\u7528\u4e8e\u533b\u7597\u5206\u8bca\u51b3\u7b56\u7684\u4ef7\u503c\u5bf9\u9f50\u3002", "conclusion": "ALIGN\u6846\u67b6\u662f\u5f00\u6e90\u7684\uff0c\u53ef\u4ee5\u652f\u6301\u5bf9\u57fa\u4e8eLLM\u7684\u53ef\u9760\u3001\u8d1f\u8d23\u548c\u4e2a\u6027\u5316\u7684\u51b3\u7b56\u8005\u7684\u7814\u7a76\u3002"}}
{"id": "2507.08839", "categories": ["cs.LG", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.08839", "abs": "https://arxiv.org/abs/2507.08839", "authors": ["Xiaowei Yu", "Jing Zhang", "Tong Chen", "Yan Zhuang", "Minheng Chen", "Chao Cao", "Yanjun Lyu", "Lu Zhang", "Li Su", "Tianming Liu", "Dajiang Zhu"], "title": "Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer", "comment": "MICCAI 2025", "summary": "Lewy Body Disease (LBD) is a common yet understudied form of dementia that\nimposes a significant burden on public health. It shares clinical similarities\nwith Alzheimer's disease (AD), as both progress through stages of normal\ncognition, mild cognitive impairment, and dementia. A major obstacle in LBD\ndiagnosis is data scarcity, which limits the effectiveness of deep learning. In\ncontrast, AD datasets are more abundant, offering potential for knowledge\ntransfer. However, LBD and AD data are typically collected from different sites\nusing different machines and protocols, resulting in a distinct domain shift.\nTo effectively leverage AD data while mitigating domain shift, we propose a\nTransferability Aware Transformer (TAT) that adapts knowledge from AD to\nenhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived\nfrom structural MRI as training data. Built on the attention mechanism, TAT\nadaptively assigns greater weights to disease-transferable features while\nsuppressing domain-specific ones, thereby reducing domain shift and improving\ndiagnostic accuracy with limited LBD data. The experimental results demonstrate\nthe effectiveness of TAT. To the best of our knowledge, this is the first study\nto explore domain adaptation from AD to LBD under conditions of data scarcity\nand domain shift, providing a promising framework for domain-adaptive diagnosis\nof rare diseases.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Transformer\u6a21\u578b\uff0c\u5229\u7528\u963f\u5179\u6d77\u9ed8\u75c7\u6570\u636e\u6765\u5e2e\u52a9\u8bca\u65ad\u8def\u6613\u4f53\u75c5\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u5c11\u548c\u6570\u636e\u5dee\u5f02\u5927\u7684\u95ee\u9898\u3002", "motivation": "\u8def\u6613\u4f53\u75c5\uff08LBD\uff09\u662f\u4e00\u79cd\u5e38\u89c1\u4f46\u7814\u7a76\u4e0d\u8db3\u7684\u75f4\u5446\u75c7\uff0c\u5bf9\u516c\u5171\u5065\u5eb7\u6784\u6210\u91cd\u5927\u8d1f\u62c5\u3002LBD\u8bca\u65ad\u7684\u4e00\u4e2a\u4e3b\u8981\u969c\u788d\u662f\u6570\u636e\u7a00\u7f3a\uff0c\u8fd9\u9650\u5236\u4e86\u6df1\u5ea6\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0cAD\u6570\u636e\u96c6\u66f4\u4e3a\u4e30\u5bcc\uff0c\u4e3a\u77e5\u8bc6\u8f6c\u79fb\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002\u7136\u800c\uff0cLBD\u548cAD\u6570\u636e\u901a\u5e38\u4f7f\u7528\u4e0d\u540c\u7684\u673a\u5668\u548c\u534f\u8bae\u4ece\u4e0d\u540c\u7684\u7ad9\u70b9\u6536\u96c6\uff0c\u4ece\u800c\u5bfc\u81f4\u660e\u663e\u7684\u9886\u57df\u504f\u79fb\u3002", "method": "\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u8f6c\u79fb\u6027\u611f\u77e5Transformer\uff08TAT\uff09\uff0c\u5b83\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\uff0c\u81ea\u9002\u5e94\u5730\u4e3a\u75be\u75c5\u53ef\u8f6c\u79fb\u7279\u5f81\u5206\u914d\u66f4\u5927\u7684\u6743\u91cd\uff0c\u540c\u65f6\u6291\u5236\u7279\u5b9a\u4e8e\u9886\u57df\u7684\u7279\u5f81\uff0c\u4ece\u800c\u51cf\u5c11\u9886\u57df\u504f\u79fb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4e86TAT\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6570\u636e\u7a00\u7f3a\u548c\u9886\u57df\u504f\u79fb\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u6570\u636e\u6765\u6539\u5584\u8def\u6613\u4f53\u75c5\uff08LBD\uff09\u7684\u8bca\u65ad\uff0c\u4e3a\u7f55\u89c1\u75be\u75c5\u7684\u9886\u57df\u81ea\u9002\u5e94\u8bca\u65ad\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u6846\u67b6\u3002"}}
