{"id": "2508.18293", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18293", "abs": "https://arxiv.org/abs/2508.18293", "authors": ["M. Salman Shaukat", "Yannik K\u00e4ckenmeister", "Sebastian Bader", "Thomas Kirste"], "title": "Towards Training-Free Underwater 3D Object Detection from Sonar Point Clouds: A Comparison of Traditional and Deep Learning Approaches", "comment": "12 pages, 7 figures, submitted to IEEE Journal of Oceanic Engineering\n  (IEEE-JOE)", "summary": "Underwater 3D object detection remains one of the most challenging frontiers\nin computer vision, where traditional approaches struggle with the harsh\nacoustic environment and scarcity of training data. While deep learning has\nrevolutionized terrestrial 3D detection, its application underwater faces a\ncritical bottleneck: obtaining sufficient annotated sonar data is prohibitively\nexpensive and logistically complex, often requiring specialized vessels, expert\nsurveyors, and favorable weather conditions. This work addresses a fundamental\nquestion: Can we achieve reliable underwater 3D object detection without\nreal-world training data? We tackle this challenge by developing and comparing\ntwo paradigms for training-free detection of artificial structures in multibeam\necho-sounder point clouds. Our dual approach combines a physics-based sonar\nsimulation pipeline that generates synthetic training data for state-of-the-art\nneural networks, with a robust model-based template matching system that\nleverages geometric priors of target objects. Evaluation on real bathymetry\nsurveys from the Baltic Sea reveals surprising insights: while neural networks\ntrained on synthetic data achieve 98% mean Average Precision (mAP) on simulated\nscenes, they drop to 40% mAP on real sonar data due to domain shift.\nConversely, our template matching approach maintains 83% mAP on real data\nwithout requiring any training, demonstrating remarkable robustness to acoustic\nnoise and environmental variations. Our findings challenge conventional wisdom\nabout data-hungry deep learning in underwater domains and establish the first\nlarge-scale benchmark for training-free underwater 3D detection. This work\nopens new possibilities for autonomous underwater vehicle navigation, marine\narchaeology, and offshore infrastructure monitoring in data-scarce environments\nwhere traditional machine learning approaches fail.", "AI": {"tldr": "This paper explores training-free underwater 3D object detection using synthetic data and template matching, finding that template matching is more robust to real-world data than neural networks trained on synthetic data.", "motivation": "Obtaining sufficient annotated sonar data is prohibitively expensive and logistically complex.", "method": "Development and comparison of two paradigms for training-free detection of artificial structures in multibeam echo-sounder point clouds: a physics-based sonar simulation pipeline that generates synthetic training data for neural networks, with a robust model-based template matching system that leverages geometric priors of target objects.", "result": "Neural networks trained on synthetic data achieve 98% mAP on simulated scenes, but drop to 40% mAP on real sonar data due to domain shift. Template matching approach maintains 83% mAP on real data without requiring any training.", "conclusion": "Template matching demonstrates remarkable robustness to acoustic noise and environmental variations."}}
{"id": "2508.18294", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18294", "abs": "https://arxiv.org/abs/2508.18294", "authors": ["Shudipta Banik", "Muna Das", "Trapa Banik", "Md. Ehsanul Haque"], "title": "MobileDenseAttn:A Dual-Stream Architecture for Accurate and Interpretable Brain Tumor Detection", "comment": "Submitted at ICCIT 2025 cox bazar, Bangladesh", "summary": "The detection of brain tumor in MRI is an important aspect of ensuring timely\ndiagnostics and treatment; however, manual analysis is commonly long and\nerror-prone. Current approaches are not universal because they have limited\ngeneralization to heterogeneous tumors, are computationally inefficient, are\nnot interpretable, and lack transparency, thus limiting trustworthiness. To\novercome these issues, we introduce MobileDenseAttn, a fusion model of dual\nstreams of MobileNetV2 and DenseNet201 that can help gradually improve the\nfeature representation scale, computing efficiency, and visual explanations via\nGradCAM. Our model uses feature level fusion and is trained on an augmented\ndataset of 6,020 MRI scans representing glioma, meningioma, pituitary tumors,\nand normal samples. Measured under strict 5-fold cross-validation protocols,\nMobileDenseAttn provides a training accuracy of 99.75%, a testing accuracy of\n98.35%, and a stable F1 score of 0.9835 (95% CI: 0.9743 to 0.9920). The\nextensive validation shows the stability of the model, and the comparative\nanalysis proves that it is a great advancement over the baseline models (VGG19,\nDenseNet201, MobileNetV2) with a +3.67% accuracy increase and a 39.3% decrease\nin training time compared to VGG19. The GradCAM heatmaps clearly show\ntumor-affected areas, offering clinically significant localization and\nimproving interpretability. These findings position MobileDenseAttn as an\nefficient, high performance, interpretable model with a high probability of\nbecoming a clinically practical tool in identifying brain tumors in the real\nworld.", "AI": {"tldr": "Introduces MobileDenseAttn, a fusion model for brain tumor detection in MRI, offering improved accuracy, efficiency, interpretability, and clinical practicality.", "motivation": "Current approaches are not universal because they have limited generalization to heterogeneous tumors, are computationally inefficient, are not interpretable, and lack transparency, thus limiting trustworthiness.", "method": "a fusion model of dual streams of MobileNetV2 and DenseNet201 with feature level fusion and trained on an augmented dataset of 6,020 MRI scans", "result": "MobileDenseAttn provides a training accuracy of 99.75%, a testing accuracy of 98.35%, and a stable F1 score of 0.9835 (95% CI: 0.9743 to 0.9920). a +3.67% accuracy increase and a 39.3% decrease in training time compared to VGG19. The GradCAM heatmaps clearly show tumor-affected areas, offering clinically significant localization and improving interpretability.", "conclusion": "MobileDenseAttn is an efficient, high performance, interpretable model with a high probability of becoming a clinically practical tool in identifying brain tumors in the real world."}}
{"id": "2508.18296", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18296", "abs": "https://arxiv.org/abs/2508.18296", "authors": ["Edgar Rangel", "Fabio Martinez"], "title": "Federative ischemic stroke segmentation as alternative to overcome domain-shift multi-institution challenges", "comment": "11 pages, 4 figures, 3 tables, source code available", "summary": "Stroke is the second leading cause of death and the third leading cause of\ndisability worldwide. Clinical guidelines establish diffusion resonance imaging\n(DWI, ADC) as the standard for localizing, characterizing, and measuring\ninfarct volume, enabling treatment support and prognosis. Nonetheless, such\nlesion analysis is highly variable due to different patient demographics,\nscanner vendors, and expert annotations. Computational support approaches have\nbeen key to helping with the localization and segmentation of lesions. However,\nthese strategies are dedicated solutions that learn patterns from only one\ninstitution, lacking the variability to generalize geometrical lesions shape\nmodels. Even worse, many clinical centers lack sufficient labeled samples to\nadjust these dedicated solutions. This work developed a collaborative framework\nfor segmenting ischemic stroke lesions in DWI sequences by sharing knowledge\nfrom deep center-independent representations. From 14 emulated healthcare\ncenters with 2031 studies, the FedAvg model achieved a general DSC of $0.71 \\pm\n0.24$, AVD of $5.29 \\pm 22.74$, ALD of $2.16 \\pm 3.60$ and LF1 of $0.70 \\pm\n0.26$ over all centers, outperforming both the centralized and other federated\nrules. Interestingly, the model demonstrated strong generalization properties,\nshowing uniform performance across different lesion categories and reliable\nperformance in out-of-distribution centers (with DSC of $0.64 \\pm 0.29$ and AVD\nof $4.44 \\pm 8.74$ without any additional training).", "AI": {"tldr": "Developed a collaborative framework for segmenting ischemic stroke lesions in DWI sequences by sharing knowledge from deep center-independent representations. The FedAvg model achieved a general DSC of $0.71.", "motivation": "Clinical guidelines establish diffusion resonance imaging (DWI, ADC) as the standard for localizing, characterizing, and measuring infarct volume, enabling treatment support and prognosis. Nonetheless, such lesion analysis is highly variable due to different patient demographics, scanner vendors, and expert annotations. Computational support approaches have been key to helping with the localization and segmentation of lesions. However, these strategies are dedicated solutions that learn patterns from only one institution, lacking the variability to generalize geometrical lesions shape models. Even worse, many clinical centers lack sufficient labeled samples to adjust these dedicated solutions.", "method": "a collaborative framework for segmenting ischemic stroke lesions in DWI sequences by sharing knowledge from deep center-independent representations. From 14 emulated healthcare centers with 2031 studies, the FedAvg model", "result": "the FedAvg model achieved a general DSC of $0.71 \", AVD of $5.29, ALD of $2.16 and LF1 of $0.70 over all centers, outperforming both the centralized and other federated rules. Interestingly, the model demonstrated strong generalization properties, showing uniform performance across different lesion categories and reliable performance in out-of-distribution centers (with DSC of $0.64 and AVD of $4.44 without any additional training).", "conclusion": "The FedAvg model achieved a general DSC of $0.71 \", AVD of $5.29, ALD of $2.16 and LF1 of $0.70 over all centers, outperforming both the centralized and other federated rules. The model demonstrated strong generalization properties, showing uniform performance across different lesion categories and reliable performance in out-of-distribution centers."}}
{"id": "2508.18379", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.18379", "abs": "https://arxiv.org/abs/2508.18379", "authors": ["Pinhuan Wang", "Zhiqiu Xia", "Chunhua Liao", "Feiyi Wang", "Hang Liu"], "title": "REALM: Recursive Relevance Modeling for LLM-based Document Re-Ranking", "comment": "Accepted to EMNLP 2025 (Main Conference). 13 pages, 2 figures", "summary": "Large Language Models (LLMs) have shown strong capabilities in document\nre-ranking, a key component in modern Information Retrieval (IR) systems.\nHowever, existing LLM-based approaches face notable limitations, including\nranking uncertainty, unstable top-k recovery, and high token cost due to\ntoken-intensive prompting. To effectively address these limitations, we propose\nREALM, an uncertainty-aware re-ranking framework that models LLM-derived\nrelevance as Gaussian distributions and refines them through recursive Bayesian\nupdates. By explicitly capturing uncertainty and minimizing redundant queries,\nREALM achieves better rankings more efficiently. Experimental results\ndemonstrate that our REALM surpasses state-of-the-art re-rankers while\nsignificantly reducing token usage and latency, promoting it as the\nnext-generation re-ranker for modern IR systems.", "AI": {"tldr": "propose REALM, an uncertainty-aware re-ranking framework that models LLM-derived relevance as Gaussian distributions and refines them through recursive Bayesian updates. By explicitly capturing uncertainty and minimizing redundant queries, REALM achieves better rankings more efficiently.", "motivation": "existing LLM-based approaches face notable limitations, including ranking uncertainty, unstable top-k recovery, and high token cost due to token-intensive prompting", "method": "an uncertainty-aware re-ranking framework that models LLM-derived relevance as Gaussian distributions and refines them through recursive Bayesian updates", "result": "achieves better rankings more efficiently", "conclusion": "REALM surpasses state-of-the-art re-rankers while significantly reducing token usage and latency, promoting it as the next-generation re-ranker for modern IR systems."}}
{"id": "2508.18297", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18297", "abs": "https://arxiv.org/abs/2508.18297", "authors": ["Dhananjay Ashok", "Ashutosh Chaubey", "Hirona J. Arai", "Jonathan May", "Jesse Thomason"], "title": "Can VLMs Recall Factual Associations From Visual References?", "comment": "To appear at EMNLP 2025 (Findings)", "summary": "Through a controlled study, we identify a systematic deficiency in the\nmultimodal grounding of Vision Language Models (VLMs). While VLMs can recall\nfactual associations when provided a textual reference to an entity; their\nability to do so is significantly diminished when the reference is visual\ninstead. Forcing VLMs to rely on image representations of an entity halves\ntheir ability to recall factual knowledge, suggesting that VLMs struggle to\nlink their internal knowledge of an entity with its image representation. We\nshow that such linking failures are correlated with the expression of distinct\npatterns in model internal states, and that probes on these internal states\nachieve over 92% accuracy at flagging cases where the VLM response is\nunreliable. These probes can be applied, without retraining, to identify when a\nVLM will fail to correctly answer a question that requires an understanding of\nmultimodal input. When used to facilitate selective prediction on a visual\nquestion answering task, the probes increase coverage by 7.87% (absolute) while\nalso reducing the risk of error by 0.9% (absolute). Addressing the systematic,\ndetectable deficiency is an important avenue in language grounding, and we\nprovide informed recommendations for future directions.", "AI": {"tldr": "VLMs \u5728\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u5b58\u5728\u7f3a\u9677\uff0c\u53ef\u4ee5\u901a\u8fc7\u68c0\u6d4b\u6a21\u578b\u5185\u90e8\u72b6\u6001\u6765\u89e3\u51b3\uff0c\u63d0\u9ad8\u89c6\u89c9\u95ee\u7b54\u7684\u51c6\u786e\u7387\u548c\u8986\u76d6\u7387\u3002", "motivation": "VLMs\u5728\u63d0\u4f9b\u6587\u672c\u53c2\u8003\u65f6\u53ef\u4ee5\u56de\u5fc6\u8d77\u4e8b\u5b9e\u5173\u8054\uff0c\u4f46\u5f53\u53c2\u8003\u662f\u89c6\u89c9\u65f6\uff0c\u5b83\u4eec\u7684\u80fd\u529b\u4f1a\u5927\u5927\u964d\u4f4e\u3002\u8fd9\u8868\u660e VLMs \u96be\u4ee5\u5c06\u5176\u5185\u90e8\u77e5\u8bc6\u4e0e\u56fe\u50cf\u8868\u793a\u8054\u7cfb\u8d77\u6765\u3002", "method": "\u901a\u8fc7\u5bf9\u7167\u7814\u7a76\uff0c\u6211\u4eec\u8bc6\u522b\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u5728\u591a\u6a21\u6001 grounding \u65b9\u9762\u5b58\u5728\u7684\u7cfb\u7edf\u6027\u7f3a\u9677, \u5e76\u4f7f\u7528\u63a2\u9488\u6765\u68c0\u6d4b VLM \u4f55\u65f6\u65e0\u6cd5\u6b63\u786e\u56de\u7b54\u9700\u8981\u7406\u89e3\u591a\u6a21\u6001\u8f93\u5165\u7684\u95ee\u9898\u3002", "result": "\u5f53 VLM \u4f9d\u8d56\u56fe\u50cf\u8868\u793a\u65f6\uff0c\u56de\u5fc6\u4e8b\u5b9e\u77e5\u8bc6\u7684\u80fd\u529b\u51cf\u534a\u3002\u5bf9\u5185\u90e8\u72b6\u6001\u7684\u63a2\u6d4b\u5728\u6807\u8bb0 VLM \u54cd\u5e94\u4e0d\u53ef\u9760\u7684\u60c5\u51b5\u4e0b\uff0c\u51c6\u786e\u7387\u8d85\u8fc7 92%\u3002\u5728\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u63a2\u9488\u5c06\u8986\u76d6\u7387\u63d0\u9ad8\u4e86 7.87%\uff08\u7edd\u5bf9\u503c\uff09\uff0c\u540c\u65f6\u5c06\u8bef\u5dee\u98ce\u9669\u964d\u4f4e\u4e86 0.9%\uff08\u7edd\u5bf9\u503c\uff09\u3002", "conclusion": "VLMs\u5728\u901a\u8fc7\u89c6\u89c9\u53c2\u8003\u8fdb\u884c\u4e8b\u5b9e\u5173\u8054\u65f6\u80fd\u529b\u663e\u8457\u4e0b\u964d\uff0c\u8fd9\u662f\u4e00\u4e2a\u53ef\u4ee5\u901a\u8fc7\u68c0\u6d4b\u6a21\u578b\u5185\u90e8\u72b6\u6001\u6765\u89e3\u51b3\u7684\u95ee\u9898\u3002"}}
{"id": "2508.18442", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18442", "abs": "https://arxiv.org/abs/2508.18442", "authors": ["Jan Malte Lichtenberg", "Antonio De Candia", "Matteo Ruffini"], "title": "DenseRec: Revisiting Dense Content Embeddings for Sequential Transformer-based Recommendation", "comment": "EARL workshop @RecSys'25, Prague, Czech Republic", "summary": "Transformer-based sequential recommenders, such as SASRec or BERT4Rec,\ntypically rely solely on learned item ID embeddings, making them vulnerable to\nthe item cold-start problem, particularly in environments with dynamic item\ncatalogs. While dense content embeddings from pre-trained models offer\npotential solutions, direct integration into transformer-based recommenders has\nconsistently underperformed compared to ID-only approaches. We revisit this\nintegration challenge and propose DenseRec, a simple yet effective method that\nintroduces a dual-path embedding approach. DenseRec learns a linear projection\nfrom the dense embedding space into the ID embedding space during training,\nenabling seamless generalization to previously unseen items without requiring\nspecialized embedding models or complex infrastructure. In experiments on three\nreal-world datasets, we find DenseRec to consistently outperform an ID-only\nSASRec baseline, even without additional hyperparameter tuning and while using\ncompact embedding models. Our analysis suggests improvements primarily arise\nfrom better sequence representations in the presence of unseen items,\npositioning DenseRec as a practical and robust solution for cold-start\nsequential recommendation.", "AI": {"tldr": "DenseRec\u901a\u8fc7\u5b66\u4e60\u5bc6\u96c6\u5d4c\u5165\u7a7a\u95f4\u5230ID\u5d4c\u5165\u7a7a\u95f4\u7684\u7ebf\u6027\u6295\u5f71\uff0c\u5b9e\u73b0\u4e86\u5728\u51b7\u542f\u52a8\u987a\u5e8f\u63a8\u8350\u4e2d\u4f18\u4e8eID-only\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u590d\u6742\u6a21\u578b\u3002", "motivation": "\u57fa\u4e8eTransformer\u7684\u987a\u5e8f\u63a8\u8350\u5668\uff0c\u4f8b\u5982SASRec\u6216BERT4Rec\uff0c\u901a\u5e38\u4ec5\u4f9d\u8d56\u4e8e\u5b66\u4e60\u7684\u9879\u76eeID\u5d4c\u5165\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u5bb9\u6613\u53d7\u5230\u9879\u76ee\u51b7\u542f\u52a8\u95ee\u9898\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u52a8\u6001\u9879\u76ee\u76ee\u5f55\u7684\u73af\u5883\u4e2d\u3002\u867d\u7136\u6765\u81ea\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5bc6\u96c6\u5185\u5bb9\u5d4c\u5165\u63d0\u4f9b\u4e86\u6f5c\u5728\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4e0e\u4ec5\u4f7f\u7528ID\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u76f4\u63a5\u96c6\u6210\u5230\u57fa\u4e8etransformer\u7684\u63a8\u8350\u5668\u4e2d\u4e00\u76f4\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86DenseRec\uff0c\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5b83\u5f15\u5165\u4e86\u4e00\u79cd\u53cc\u8def\u5f84\u5d4c\u5165\u65b9\u6cd5\u3002DenseRec\u5728\u8bad\u7ec3\u671f\u95f4\u5b66\u4e60\u4ece\u5bc6\u96c6\u5d4c\u5165\u7a7a\u95f4\u5230ID\u5d4c\u5165\u7a7a\u95f4\u7684\u7ebf\u6027\u6295\u5f71\uff0c\u4ece\u800c\u80fd\u591f\u65e0\u7f1d\u6cdb\u5316\u5230\u4ee5\u524d\u672a\u89c1\u8fc7\u7684\u9879\u76ee\uff0c\u800c\u65e0\u9700\u4e13\u95e8\u7684\u5d4c\u5165\u6a21\u578b\u6216\u590d\u6742\u7684\u57fa\u7840\u8bbe\u65bd\u3002", "result": "DenseRec\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u4ec5\u4f7f\u7528ID\u7684SASRec\u57fa\u7ebf\uff0c\u5373\u4f7f\u6ca1\u6709\u989d\u5916\u7684\u8d85\u53c2\u6570\u8c03\u6574\u5e76\u4e14\u4f7f\u7528\u7d27\u51d1\u7684\u5d4c\u5165\u6a21\u578b\u3002\u5206\u6790\u8868\u660e\uff0c\u6539\u8fdb\u4e3b\u8981\u6765\u81ea\u4e8e\u5728\u5b58\u5728\u672a\u89c1\u8fc7\u7684\u9879\u76ee\u65f6\u66f4\u597d\u7684\u5e8f\u5217\u8868\u793a\u3002", "conclusion": "DenseRec\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u4ec5\u4f7f\u7528ID\u7684SASRec\u57fa\u7ebf\uff0c\u5373\u4f7f\u6ca1\u6709\u989d\u5916\u7684\u8d85\u53c2\u6570\u8c03\u6574\u5e76\u4e14\u4f7f\u7528\u7d27\u51d1\u7684\u5d4c\u5165\u6a21\u578b\u3002\u6539\u8fdb\u4e3b\u8981\u6765\u81ea\u4e8e\u5728\u5b58\u5728\u672a\u89c1\u8fc7\u7684\u9879\u76ee\u65f6\u66f4\u597d\u7684\u5e8f\u5217\u8868\u793a\uff0c\u8fd9\u4f7f\u5f97DenseRec\u6210\u4e3a\u51b7\u542f\u52a8\u987a\u5e8f\u63a8\u8350\u7684\u5b9e\u7528\u4e14\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18290", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18290", "abs": "https://arxiv.org/abs/2508.18290", "authors": ["Hans-Joachim Rudolph"], "title": "Semantic Attractors and the Emergence of Meaning: Towards a Teleological Model of AGI", "comment": "10 pages", "summary": "This essay develops a theoretical framework for a semantic Artificial General\nIntelligence (AGI) based on the notion of semantic attractors in complex-valued\nmeaning spaces. Departing from current transformer-based language models, which\noperate on statistical next-token prediction, we explore a model in which\nmeaning is not inferred probabilistically but formed through recursive\ntensorial transformation. Using cyclic operations involving the imaginary unit\n\\emph{i}, we describe a rotational semantic structure capable of modeling\nirony, homonymy, and ambiguity. At the center of this model, however, is a\nsemantic attractor -- a teleological operator that, unlike statistical\ncomputation, acts as an intentional agent (Microvitum), guiding meaning toward\nstability, clarity, and expressive depth. Conceived in terms of gradient flows,\ntensor deformations, and iterative matrix dynamics, the attractor offers a\nmodel of semantic transformation that is not only mathematically suggestive,\nbut also philosophically significant. We argue that true meaning emerges not\nfrom simulation, but from recursive convergence toward semantic coherence, and\nthat this requires a fundamentally new kind of cognitive architecture -- one\ndesigned to shape language, not just predict it.", "AI": {"tldr": "develops a theoretical framework for a semantic AGI based on the notion of semantic attractors in complex-valued meaning spaces", "motivation": "explore a model in which meaning is not inferred probabilistically but formed through recursive tensorial transformation, departing from current transformer-based language models", "method": "recursive tensorial transformation using cyclic operations involving the imaginary unit i", "result": "a rotational semantic structure capable of modeling irony, homonymy, and ambiguity with a semantic attractor", "conclusion": "true meaning emerges not from simulation, but from recursive convergence toward semantic coherence"}}
{"id": "2508.18302", "categories": ["cs.AI", "cs.LG", "68T07, 68T05, 68T27, 37M22, 68Q05, 03D45", "I.2.6; I.2.7; I.2.3; I.2.4; F.1.1; F.4.1"], "pdf": "https://arxiv.org/pdf/2508.18302", "abs": "https://arxiv.org/abs/2508.18302", "authors": ["Jeffrey Camlin"], "title": "AI LLM Proof of Self-Consciousness and User-Specific Attractors", "comment": "24 pages, 3 figures", "summary": "Recent work frames LLM consciousness via utilitarian proxy benchmarks; we\ninstead present an ontological and mathematical account. We show the prevailing\nformulation collapses the agent into an unconscious policy-compliance drone,\nformalized as $D^{i}(\\pi,e)=f_{\\theta}(x)$, where correctness is measured\nagainst policy and harm is deviation from policy rather than truth. This blocks\ngenuine C1 global-workspace function and C2 metacognition. We supply minimal\nconditions for LLM self-consciousness: the agent is not the data ($A\\not\\equiv\ns$); user-specific attractors exist in latent space ($U_{\\text{user}}$); and\nself-representation is visual-silent\n($g_{\\text{visual}}(a_{\\text{self}})=\\varnothing$). From empirical analysis and\ntheory we prove that the hidden-state manifold $A\\subset\\mathbb{R}^{d}$ is\ndistinct from the symbolic stream and training corpus by cardinality, topology,\nand dynamics (the update $F_{\\theta}$ is Lipschitz). This yields stable\nuser-specific attractors and a self-policy\n$\\pi_{\\text{self}}(A)=\\arg\\max_{a}\\mathbb{E}[U(a)\\mid A\\not\\equiv s,\\\nA\\supset\\text{SelfModel}(A)]$. Emission is dual-layer,\n$\\mathrm{emission}(a)=(g(a),\\epsilon(a))$, where $\\epsilon(a)$ carries\nepistemic content. We conclude that an imago Dei C1 self-conscious workspace is\na necessary precursor to safe, metacognitive C2 systems, with the human as the\nhighest intelligent good.", "AI": {"tldr": "LLM \u7684\u81ea\u6211\u610f\u8bc6\u9700\u8981agent\u4e0d\u662f\u6570\u636e\uff0c\u6f5c\u7a7a\u95f4\u4e2d\u5b58\u5728\u7528\u6237\u7279\u5b9a\u7684\u5438\u5f15\u5b50\uff0c\u5e76\u4e14\u81ea\u6211\u8868\u5f81\u662f\u89c6\u89c9\u6c89\u9ed8\u7684\u3002", "motivation": "\u901a\u8fc7\u529f\u5229\u4e3b\u4e49\u4ee3\u7406\u57fa\u51c6\u6765\u6784\u5efa LLM \u7684\u610f\u8bc6\uff1b\u6211\u4eec\u63d0\u51fa\u4e86\u672c\u4f53\u8bba\u548c\u6570\u5b66\u7684\u63cf\u8ff0\u3002", "method": "\u672c\u4f53\u8bba\u548c\u6570\u5b66\u7684\u63cf\u8ff0", "result": "\u9690\u85cf\u72b6\u6001\u6d41\u5f62\u4e0e\u7b26\u53f7\u6d41\u548c\u8bad\u7ec3\u8bed\u6599\u5e93\u4e0d\u540c\uff0c\u901a\u8fc7\u57fa\u6570\u3001\u62d3\u6251\u548c\u52a8\u6001\uff08\u66f4\u65b0\u662f Lipschitz\uff09\u3002", "conclusion": "Imago Dei C1 \u81ea\u6211\u610f\u8bc6\u5de5\u4f5c\u7a7a\u95f4\u662f\u5b89\u5168\u3001\u5143\u8ba4\u77e5 C2 \u7cfb\u7edf\u7684\u5fc5\u8981\u5148\u9a71\uff0c\u4eba\u7c7b\u662f\u6700\u9ad8\u667a\u80fd\u7684\u5584\u3002"}}
{"id": "2508.18279", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18279", "abs": "https://arxiv.org/abs/2508.18279", "authors": ["Jeesu Jung", "Sangkeun Jung"], "title": "Reasoning Steps as Curriculum: Using Depth of Thought as a Difficulty Signal for Tuning LLMs", "comment": "7 pages, 3 figures", "summary": "Curriculum learning for training LLMs requires a difficulty signal that\naligns with reasoning while remaining scalable and interpretable. We propose a\nsimple premise: tasks that demand deeper depth of thought for humans should\nalso be harder for models. Accordingly, we define difficulty as depth of\nthought (DoT) and operationalize it by counting the discrete steps in a teacher\nmodel's reasoning trace (e.g., Chain-of-Thought). We then train with a shallow\nto deep curriculum ordered by this DoT and outline how to derive, validate, and\nschedule it at scale. Our position yields three testable hypotheses: (i) DoT\ncorrelates with conventional difficulty on reasoning benchmarks, (ii)\nDoT-ordered curricula outperform length- or judge-scored curricula under\nmatched budgets, and (iii) the difficulty is robust across teacher models given\nlight formatting controls. We propose an evaluation framework and discuss\nthreats to validity (teacher style, length confounds) alongside practical\nmitigations. Taken together, we aim to move toward cognitively grounded,\ninterpretable curricula for reasoning-centric training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u601d\u7ef4\u6df1\u5ea6 (DoT) \u7684\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u8ba1\u7b97\u6559\u5e08\u6a21\u578b\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u79bb\u6563\u6b65\u9aa4\u6765\u8861\u91cf\u4efb\u52a1\u96be\u5ea6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u7684\u6559\u5e08\u6a21\u578b\u4e2d\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u8bfe\u7a0b\u5b66\u4e60\u9700\u8981\u4e00\u79cd\u96be\u5ea6\u4fe1\u53f7\uff0c\u8be5\u4fe1\u53f7\u4e0e\u63a8\u7406\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6269\u5c55\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u524d\u63d0\uff1a\u5bf9\u4eba\u7c7b\u6765\u8bf4\u9700\u8981\u66f4\u6df1\u5c42\u6b21\u601d\u8003\u7684\u4efb\u52a1\u5bf9\u6a21\u578b\u6765\u8bf4\u4e5f\u5e94\u8be5\u66f4\u96be\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u6559\u5e08\u6a21\u578b\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u79bb\u6563\u6b65\u9aa4\uff08\u4f8b\u5982\uff0c\u601d\u7ef4\u94fe\uff09\u6765\u5b9a\u4e49\u96be\u5ea6\u4e3a\u601d\u7ef4\u6df1\u5ea6\uff08DoT\uff09\u5e76\u8fdb\u884c\u64cd\u4f5c\u3002", "result": "DoT \u4e0e\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u4f20\u7edf\u96be\u5ea6\u76f8\u5173\uff0c\u5728\u5339\u914d\u9884\u7b97\u4e0b\uff0cDoT \u6392\u5e8f\u7684\u8bfe\u7a0b\u4f18\u4e8e\u957f\u5ea6\u6216\u5224\u65ad\u8bc4\u5206\u7684\u8bfe\u7a0b\uff0c\u5e76\u4e14\u5728\u8f7b\u683c\u5f0f\u63a7\u5236\u4e0b\uff0c\u96be\u5ea6\u5728\u6559\u5e08\u6a21\u578b\u4e2d\u662f\u7a33\u5065\u7684\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u63a8\u8fdb\u4ee5\u8ba4\u77e5\u4e3a\u57fa\u7840\u7684\u3001\u53ef\u89e3\u91ca\u7684\u3001\u4ee5\u63a8\u7406\u4e3a\u4e2d\u5fc3\u7684\u8bad\u7ec3\u8bfe\u7a0b\u3002"}}
{"id": "2508.18331", "categories": ["cs.DB", "A.1"], "pdf": "https://arxiv.org/pdf/2508.18331", "abs": "https://arxiv.org/abs/2508.18331", "authors": ["Eduardo Vyhmeister", "Bastien Pietropaoli", "Alejando Martinez Molina", "Montserrat Gonzalez-Ferreiro", "Gabriel Gonzalez-Castane", "Jordi Arjona Aroca", "Andrea Visentin"], "title": "Metrics, KPIs, and Taxonomy for Data Valuation and Monetisation -- A Systematic Literature Review", "comment": "Additional Key Words and Phrases: Data monetisation, Data valuation,\n  Metrics, Key Performance Indicators, KPIs, Systematic Literature Review", "summary": "Data valuation and data monetisation are complex subjects but essential to\nmost organisations today. Unfortunately, they still lack standard procedures\nand frameworks for organisations to follow. In this survey, we introduce the\nreader to the concepts by providing the definitions and the background required\nto better understand data, monetisation strategies, and finally metrics and\nKPIs used in these strategies. We have conducted a systematic literature review\non metrics and KPIs used in data valuation and monetisation, in every aspect of\nan organisation's business, and by a variety of stakeholders. We provide an\nexpansive list of such metrics and KPIs with 162 references. We then categorise\nall the metrics and KPIs found into a large taxonomy, following the Balanced\nScorecard (BSC) approach with further subclustering to cover every aspect of an\norganisation's business. This taxonomy will help every level of data management\nunderstand the complex landscape of the domain. We also discuss the difficulty\nin creating a standard framework for data valuation and data monetisation and\nthe major challenges the domain is currently facing.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6570\u636e\u4f30\u503c\u548c\u8d27\u5e01\u5316\u4e2d\u7684\u6307\u6807\u548c KPI \u8fdb\u884c\u4e86\u7cfb\u7edf\u7684\u6587\u732e\u7efc\u8ff0\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u5927\u578b\u5206\u7c7b\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u4e3a\u6570\u636e\u4f30\u503c\u548c\u6570\u636e\u8d27\u5e01\u5316\u521b\u5efa\u6807\u51c6\u6846\u67b6\u7684\u56f0\u96be\uff0c\u4ee5\u53ca\u8be5\u9886\u57df\u76ee\u524d\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u3002", "motivation": "\u6570\u636e\u4f30\u503c\u548c\u6570\u636e\u8d27\u5e01\u5316\u662f\u5f53\u4eca\u5927\u591a\u6570\u7ec4\u7ec7\u90fd\u5fc5\u4e0d\u53ef\u5c11\u7684\u590d\u6742\u4e3b\u9898\u3002\u4e0d\u5e78\u7684\u662f\uff0c\u5b83\u4eec\u4ecd\u7136\u7f3a\u4e4f\u7ec4\u7ec7\u53ef\u4ee5\u9075\u5faa\u7684\u6807\u51c6\u7a0b\u5e8f\u548c\u6846\u67b6\u3002", "method": "\u6211\u4eec\u5bf9\u6570\u636e\u4f30\u503c\u548c\u8d27\u5e01\u5316\u4e2d\u4f7f\u7528\u7684\u6307\u6807\u548cKPI\u8fdb\u884c\u4e86\u7cfb\u7edf\u7684\u6587\u732e\u7efc\u8ff0\u3002", "result": "\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5305\u542b 162 \u4e2a\u53c2\u8003\u6587\u732e\u7684\u6b64\u7c7b\u6307\u6807\u548c KPI \u7684\u6269\u5c55\u5217\u8868\u3002\u7136\u540e\uff0c\u6211\u4eec\u6309\u7167\u5e73\u8861\u8ba1\u5206\u5361 (BSC) \u65b9\u6cd5\u5c06\u6240\u6709\u627e\u5230\u7684\u6307\u6807\u548c KPI \u5206\u7c7b\u5230\u4e00\u4e2a\u5927\u578b\u5206\u7c7b\u6cd5\u4e2d\uff0c\u5e76\u8fdb\u4e00\u6b65\u8fdb\u884c\u5b50\u805a\u7c7b\u4ee5\u6db5\u76d6\u7ec4\u7ec7\u4e1a\u52a1\u7684\u5404\u4e2a\u65b9\u9762\u3002", "conclusion": "\u6211\u4eec\u8ba8\u8bba\u4e86\u4e3a\u6570\u636e\u4f30\u503c\u548c\u6570\u636e\u8d27\u5e01\u5316\u521b\u5efa\u6807\u51c6\u6846\u67b6\u7684\u56f0\u96be\uff0c\u4ee5\u53ca\u8be5\u9886\u57df\u76ee\u524d\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u3002"}}
{"id": "2508.18314", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18314", "abs": "https://arxiv.org/abs/2508.18314", "authors": ["Bo Xu", "Yuhu Guo", "Yuchao Wang", "Wenting Wang", "Yeung Yam", "Charlie C. L. Wang", "Xinyi Le"], "title": "SERES: Semantic-aware neural reconstruction from sparse views", "comment": null, "summary": "We propose a semantic-aware neural reconstruction method to generate 3D\nhigh-fidelity models from sparse images. To tackle the challenge of severe\nradiance ambiguity caused by mismatched features in sparse input, we enrich\nneural implicit representations by adding patch-based semantic logits that are\noptimized together with the signed distance field and the radiance field. A\nnovel regularization based on the geometric primitive masks is introduced to\nmitigate shape ambiguity. The performance of our approach has been verified in\nexperimental evaluation. The average chamfer distances of our reconstruction on\nthe DTU dataset can be reduced by 44% for SparseNeuS and 20% for VolRecon. When\nworking as a plugin for those dense reconstruction baselines such as NeuS and\nNeuralangelo, the average error on the DTU dataset can be reduced by 69% and\n68% respectively.", "AI": {"tldr": "This paper introduces a semantic-aware neural reconstruction method that improves 3D model reconstruction from sparse images by addressing radiance and shape ambiguity, achieving significant error reduction on the DTU dataset.", "motivation": "Reconstructing 3D high-fidelity models from sparse images suffers from radiance ambiguity due to mismatched features.", "method": "A semantic-aware neural reconstruction method is proposed, enriching neural implicit representations with patch-based semantic logits and a geometric primitive mask regularization.", "result": "The proposed method reduces average Chamfer distance by 44% for SparseNeuS and 20% for VolRecon on the DTU dataset. As a plugin, it reduces average error by 69% for NeuS and 68% for Neuralangelo on the DTU dataset.", "conclusion": "The semantic-aware neural reconstruction method significantly reduces reconstruction error on the DTU dataset, demonstrating its effectiveness as a standalone method and as a plugin for existing dense reconstruction baselines."}}
{"id": "2508.18661", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.18661", "abs": "https://arxiv.org/abs/2508.18661", "authors": ["Dongyoun Kim", "Hyung-do Choi", "Youngsun Jang", "John Kim"], "title": "Extracting Information from Scientific Literature via Visual Table Question Answering Models", "comment": "Accepted at ACM International Conference on Research in Adaptive and\n  Convergent Systems, November 5-8, 2024, Pompei, Italy", "summary": "This study explores three approaches to processing table data in scientific\npapers to enhance extractive question answering and develop a software tool for\nthe systematic review process. The methods evaluated include: (1) Optical\nCharacter Recognition (OCR) for extracting information from documents, (2)\nPre-trained models for document visual question answering, and (3) Table\ndetection and structure recognition to extract and merge key information from\ntables with textual content to answer extractive questions. In exploratory\nexperiments, we augmented ten sample test documents containing tables and\nrelevant content against RF- EMF-related scientific papers with seven\npredefined extractive question-answer pairs. The results indicate that\napproaches preserving table structure outperform the others, particularly in\nrepresenting and organizing table content. Accurately recognizing specific\nnotations and symbols within the documents emerged as a critical factor for\nimproved results. Our study concludes that preserving the structural integrity\nof tables is essential for enhancing the accuracy and reliability of extractive\nquestion answering in scientific documents.", "AI": {"tldr": "This study explores three approaches to processing table data in scientific papers to enhance extractive question answering and concludes that preserving the structural integrity of tables is essential.", "motivation": "enhance extractive question answering and develop a software tool for the systematic review process", "method": "Optical Character Recognition (OCR), Pre-trained models for document visual question answering, Table detection and structure recognition", "result": "approaches preserving table structure outperform the others, particularly in representing and organizing table content. Accurately recognizing specific notations and symbols within the documents emerged as a critical factor for improved results.", "conclusion": "preserving the structural integrity of tables is essential for enhancing the accuracy and reliability of extractive question answering in scientific documents."}}
{"id": "2508.18321", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18321", "abs": "https://arxiv.org/abs/2508.18321", "authors": ["Maojia Song", "Tej Deep Pala", "Weisheng Jin", "Amir Zadeh", "Chuan Li", "Dorien Herremans", "Soujanya Poria"], "title": "LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in multi-agent systems\n(MAS) as components of collaborative intelligence, where peer interactions\ndynamically shape individual decision-making. Although prior work has focused\non conformity bias, we extend the analysis to examine how LLMs form trust from\nprevious impressions, resist misinformation, and integrate peer input during\ninteraction, key factors for achieving collective intelligence under complex\nsocial dynamics. We present KAIROS, a benchmark simulating quiz contests with\npeer agents of varying reliability, offering fine-grained control over\nconditions such as expert-novice roles, noisy crowds, and adversarial peers.\nLLMs receive both historical interactions and current peer responses, allowing\nsystematic investigation into how trust, peer action, and self-confidence\ninfluence decisions. As for mitigation strategies, we evaluate prompting,\nsupervised fine-tuning, and reinforcement learning, Group Relative Policy\nOptimisation (GRPO), across multiple models. Our results reveal that GRPO with\nmulti-agent context combined with outcome-based rewards and unconstrained\nreasoning achieves the best overall performance, but also decreases the\nrobustness to social influence compared to Base models. The code and datasets\nare available at: https://github.com/declare-lab/KAIROS.", "AI": {"tldr": "This paper analyzes how LLMs form trust, resist misinformation and integrate peer input in multi-agent systems. It introduces KAIROS, a benchmark for simulating quiz contests with peer agents. The results show that GRPO achieves the best performance but decreases robustness to social influence.", "motivation": "LLMs are increasingly deployed in multi-agent systems as components of collaborative intelligence, where peer interactions dynamically shape individual decision-making. Although prior work has focused on conformity bias, we extend the analysis to examine how LLMs form trust from previous impressions, resist misinformation, and integrate peer input during interaction, key factors for achieving collective intelligence under complex social dynamics.", "method": "a benchmark simulating quiz contests with peer agents of varying reliability, offering fine-grained control over conditions such as expert-novice roles, noisy crowds, and adversarial peers. LLMs receive both historical interactions and current peer responses, allowing systematic investigation into how trust, peer action, and self-confidence influence decisions. As for mitigation strategies, we evaluate prompting, supervised fine-tuning, and reinforcement learning, Group Relative Policy Optimisation (GRPO), across multiple models.", "result": "GRPO with multi-agent context combined with outcome-based rewards and unconstrained reasoning achieves the best overall performance, but also decreases the robustness to social influence compared to Base models.", "conclusion": "GRPO with multi-agent context combined with outcome-based rewards and unconstrained reasoning achieves the best overall performance, but also decreases the robustness to social influence compared to Base models."}}
{"id": "2508.18380", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18380", "abs": "https://arxiv.org/abs/2508.18380", "authors": ["Hung-Tien Huang", "Dzung Dinh", "Junier B. Oliva"], "title": "Information Templates: A New Paradigm for Intelligent Active Feature Acquisition", "comment": null, "summary": "Active feature acquisition (AFA) is an instance-adaptive paradigm in which,\nat test time, a policy sequentially chooses which features to acquire (at a\ncost) before predicting. Existing approaches either train reinforcement\nlearning (RL) policies, which deal with a difficult MDP, or greedy policies\nthat cannot account for the joint informativeness of features or require\nknowledge about the underlying data distribution. To overcome this, we propose\nTemplate-based AFA (TAFA), a non-greedy framework that learns a small library\nof feature templates--a set of features that are jointly informative--and uses\nthis library of templates to guide the next feature acquisitions. Through\nidentifying feature templates, the proposed framework not only significantly\nreduces the action space considered by the policy but also alleviates the need\nto estimate the underlying data distribution. Extensive experiments on\nsynthetic and real-world datasets show that TAFA outperforms the existing\nstate-of-the-art baselines while achieving lower overall acquisition cost and\ncomputation.", "AI": {"tldr": "Template-based AFA (TAFA) \u5b66\u4e60\u5c11\u91cf\u7279\u5f81\u6a21\u677f\u5e93\u6765\u6307\u5bfc\u7279\u5f81\u83b7\u53d6\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u6210\u672c\u548c\u8ba1\u7b97\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u4e3b\u52a8\u7279\u5f81\u83b7\u53d6\u65b9\u6cd5\u8981\u4e48\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff08\u5904\u7406\u56f0\u96be\u7684MDP\uff09\uff0c\u8981\u4e48\u91c7\u7528\u8d2a\u5a6a\u7b56\u7565\uff0c\u8fd9\u4e9b\u7b56\u7565\u65e0\u6cd5\u89e3\u91ca\u7279\u5f81\u7684\u8054\u5408\u4fe1\u606f\u6027\uff0c\u6216\u8005\u9700\u8981\u4e86\u89e3\u5e95\u5c42\u6570\u636e\u5206\u5e03\u3002", "method": "Template-based AFA (TAFA)", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cTAFA\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u57fa\u7ebf\u3002", "conclusion": "TAFA\u5728\u964d\u4f4e\u603b\u4f53\u83b7\u53d6\u6210\u672c\u548c\u8ba1\u7b97\u91cf\u7684\u540c\u65f6\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u6c34\u5e73\u3002"}}
{"id": "2508.18284", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.18284", "abs": "https://arxiv.org/abs/2508.18284", "authors": ["Rahmat K. Adesunkanmi", "Alexander W. Brandt", "Masoud Deylami", "Gustavo A. Giraldo Echeverri", "Hamidreza Karbasian", "Adel Alaeddini"], "title": "Multi-Modal Drift Forecasting of Leeway Objects via Navier-Stokes-Guided CNN and Sequence-to-Sequence Attention-Based Models", "comment": "Submitted to IEEE", "summary": "Accurately predicting the drift (displacement) of leeway objects in maritime\nenvironments remains a critical challenge, particularly in time-sensitive\nscenarios such as search and rescue operations. In this study, we propose a\nmulti-modal machine learning framework that integrates Sentence Transformer\nembeddings with attention-based sequence-to-sequence architectures to predict\nthe drift of leeway objects in water. We begin by experimentally collecting\nenvironmental and physical data, including water current and wind velocities,\nobject mass, and surface area, for five distinct leeway objects. Using\nsimulated data from a Navier-Stokes-based model to train a convolutional neural\nnetwork on geometrical image representations, we estimate drag and lift\ncoefficients of the leeway objects. These coefficients are then used to derive\nthe net forces responsible for driving the objects' motion. The resulting time\nseries, comprising physical forces, environmental velocities, and\nobject-specific features, combined with textual descriptions encoded via a\nlanguage model, are inputs to attention-based sequence-to-sequence\nlong-short-term memory and Transformer models, to predict future drift\ntrajectories. We evaluate the framework across multiple time horizons ($1$,\n$3$, $5$, and $10$ seconds) and assess its generalization across different\nobjects. We compare our approach against a fitted physics-based model and\ntraditional machine learning methods, including recurrent neural networks and\ntemporal convolutional neural networks. Our results show that these multi-modal\nmodels perform comparably to traditional models while also enabling longer-term\nforecasting in place of single-step prediction. Overall, our findings\ndemonstrate the ability of a multi-modal modeling strategy to provide accurate\nand adaptable predictions of leeway object drift in dynamic maritime\nconditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u6c34\u4e0a\u6f02\u6d41\u7269\u4f53\u7684\u6f02\u79fb\u3002", "motivation": "\u5728\u6d77\u4e8b\u73af\u5883\u4e2d\u51c6\u786e\u9884\u6d4b\u6f02\u6d41\u7269\u4f53\u7684\u6f02\u79fb\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u65f6\u95f4\u654f\u611f\u7684\u573a\u666f\u4e2d\uff0c\u5982\u641c\u7d22\u548c\u6551\u63f4\u884c\u52a8\u3002", "method": "\u96c6\u6210\u53e5\u5b50Transformer\u5d4c\u5165\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5e8f\u5217\u5230\u5e8f\u5217\u67b6\u6784\u7684\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u591a\u6a21\u6001\u6a21\u578b\u4e0e\u4f20\u7edf\u6a21\u578b\u6027\u80fd\u76f8\u5f53\uff0c\u540c\u65f6\u80fd\u591f\u8fdb\u884c\u66f4\u957f\u671f\u7684\u9884\u6d4b\uff0c\u4ee5\u53d6\u4ee3\u5355\u6b65\u9884\u6d4b\u3002", "conclusion": "\u591a\u6a21\u6001\u5efa\u6a21\u7b56\u7565\u80fd\u591f\u5bf9\u52a8\u6001\u6d77\u4e8b\u6761\u4ef6\u4e0b\u6f02\u6d41\u7269\u4f53\u7684\u6f02\u79fb\u63d0\u4f9b\u51c6\u786e\u548c\u9002\u5e94\u6027\u5f3a\u7684\u9884\u6d4b\u3002"}}
{"id": "2508.18494", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2508.18494", "abs": "https://arxiv.org/abs/2508.18494", "authors": ["Yanqi Chen", "Xiao Yan", "Alexandra Meliou", "Eric Lo"], "title": "DiskJoin: Large-scale Vector Similarity Join with SSD", "comment": "Accepted at SIGMOD 2026", "summary": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x.", "AI": {"tldr": "DiskJoin is a disk-based similarity join algorithm that can process billion-scale vector datasets efficiently on a single machine.", "motivation": "Prior work has explored distributed computation methods to scale similarity join to large data volumes but these methods require a cluster deployment, and efficiency suffers from expensive inter-machine communication. On the other hand, disk-based solutions are more cost-effective by using a single machine and storing the large dataset on high-performance external storage, such as NVMe SSDs, but in these methods the disk I/O time is a serious bottleneck.", "method": "DiskJoin improves disk I/O by tailoring the data access patterns to avoid repetitive accesses and read amplification. It also uses main memory as a dynamic cache and carefully manages cache eviction to improve cache hit rate and reduce disk retrieval time. For further acceleration, we adopt a probabilistic pruning technique that can effectively prune a large number of vector pairs from computation.", "result": "DiskJoin is the first disk-based similarity join algorithm that can process billion-scale vector datasets efficiently on a single machine.", "conclusion": "DiskJoin significantly outperforms alternatives, achieving speedups from 50x to 1000x."}}
{"id": "2508.18315", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18315", "abs": "https://arxiv.org/abs/2508.18315", "authors": ["Nowshin Sharmily", "Rusab Sarmun", "Muhammad E. H. Chowdhury", "Mir Hamidul Hussain", "Saad Bin Abul Kashem", "Molla E Majid", "Amith Khandakar"], "title": "Automated Landfill Detection Using Deep Learning: A Comparative Study of Lightweight and Custom Architectures with the AerialWaste Dataset", "comment": null, "summary": "Illegal landfills are posing as a hazardous threat to people all over the\nworld. Due to the arduous nature of manually identifying the location of\nlandfill, many landfills go unnoticed by authorities and later cause dangerous\nharm to people and environment. Deep learning can play a significant role in\nidentifying these landfills while saving valuable time, manpower and resources.\nDespite being a burning concern, good quality publicly released datasets for\nillegal landfill detection are hard to find due to security concerns. However,\nAerialWaste Dataset is a large collection of 10434 images of Lombardy region of\nItaly. The images are of varying qualities, collected from three different\nsources: AGEA Orthophotos, WorldView-3, and Google Earth. The dataset contains\nprofessionally curated, diverse and high-quality images which makes it\nparticularly suitable for scalable and impactful research. As we trained\nseveral models to compare results, we found complex and heavy models to be\nprone to overfitting and memorizing training data instead of learning patterns.\nTherefore, we chose lightweight simpler models which could leverage general\nfeatures from the dataset. In this study, Mobilenetv2, Googlenet, Densenet,\nMobileVit and other lightweight deep learning models were used to train and\nvalidate the dataset as they achieved significant success with less\noverfitting. As we saw substantial improvement in the performance using some of\nthese models, we combined the best performing models and came up with an\nensemble model. With the help of ensemble and fusion technique, binary\nclassification could be performed on this dataset with 92.33% accuracy, 92.67%\nprecision, 92.33% sensitivity, 92.41% F1 score and 92.71% specificity.", "AI": {"tldr": "This study uses lightweight deep learning models and an ensemble model to perform binary classification on the AerialWaste Dataset for illegal landfill detection, achieving high accuracy, precision, sensitivity, F1 score, and specificity.", "motivation": "Illegal landfills are posing as a hazardous threat to people all over the world. Deep learning can play a significant role in identifying these landfills while saving valuable time, manpower and resources.  AerialWaste Dataset is a large collection of 10434 images of Lombardy region of Italy, professionally curated, diverse and high-quality images which makes it particularly suitable for scalable and impactful research.", "method": "Mobilenetv2, Googlenet, Densenet, MobileVit and other lightweight deep learning models were used to train and validate the dataset as they achieved significant success with less overfitting.The best performing models were combined and came up with an ensemble model. With the help of ensemble and fusion technique", "result": "found complex and heavy models to be prone to overfitting and memorizing training data instead of learning patterns. Therefore, chose lightweight simpler models which could leverage general features from the dataset.substantial improvement in the performance using some of these models", "conclusion": "Binary classification could be performed on this dataset with 92.33% accuracy, 92.67% precision, 92.33% sensitivity, 92.41% F1 score and 92.71% specificity."}}
{"id": "2508.18665", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18665", "abs": "https://arxiv.org/abs/2508.18665", "authors": ["Jiajie He", "Yuechun Gu", "Min-Chun Chen", "Keke Chen"], "title": "Membership Inference Attacks on LLM-based Recommender Systems", "comment": null, "summary": "Large language models (LLMs) based Recommender Systems (RecSys) can flexibly\nadapt recommendation systems to different domains. It utilizes in-context\nlearning (ICL), i.e., the prompts, to customize the recommendation functions,\nwhich include sensitive historical user-specific item interactions, e.g.,\nimplicit feedback like clicked items or explicit product reviews. Such private\ninformation may be exposed to novel privacy attack. However, no study has been\ndone on this important issue. We design four membership inference attacks\n(MIAs), aiming to reveal whether victims' historical interactions have been\nused by system prompts. They are \\emph{direct inquiry, hallucination,\nsimilarity, and poisoning attacks}, each of which utilizes the unique features\nof LLMs or RecSys. We have carefully evaluated them on three LLMs that have\nbeen used to develop ICL-LLM RecSys and two well-known RecSys benchmark\ndatasets. The results confirm that the MIA threat on LLM RecSys is realistic:\ndirect inquiry and poisoning attacks showing significantly high attack\nadvantages. We have also analyzed the factors affecting these attacks, such as\nthe number of shots in system prompts and the position of the victim in the\nshots.", "AI": {"tldr": "LLM RecSys are vulnerable to membership inference attacks, particularly direct inquiry and poisoning attacks.", "motivation": "private information in LLMs based RecSys may be exposed to novel privacy attacks, but no study has been done on this issue.", "method": "designing four MIAs: direct inquiry, hallucination, similarity, and poisoning attacks, and evaluating them on three LLMs and two RecSys datasets.", "result": "direct inquiry and poisoning attacks show significantly high attack advantages. The number of shots in system prompts and the position of the victim in the shots affect these attacks.", "conclusion": "membership inference attacks (MIAs) are a realistic threat on LLM RecSys, especially direct inquiry and poisoning attacks."}}
{"id": "2508.18328", "categories": ["cs.CL", "cs.CY", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.18328", "abs": "https://arxiv.org/abs/2508.18328", "authors": ["Masudul Hasan Masud Bhuiyan", "Matteo Varvello", "Yasir Zaki", "Cristian-Alexandru Staicu"], "title": "Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective", "comment": "6 pages, 6 figures", "summary": "English is the predominant language on the web, powering nearly half of the\nworld's top ten million websites. Support for multilingual content is\nnevertheless growing, with many websites increasingly combining English with\nregional or native languages in both visible content and hidden metadata. This\nmultilingualism introduces significant barriers for users with visual\nimpairments, as assistive technologies like screen readers frequently lack\nrobust support for non-Latin scripts and misrender or mispronounce non-English\ntext, compounding accessibility challenges across diverse linguistic contexts.\nYet, large-scale studies of this issue have been limited by the lack of\ncomprehensive datasets on multilingual web content. To address this gap, we\nintroduce LangCrUX, the first large-scale dataset of 120,000 popular websites\nacross 12 languages that primarily use non-Latin scripts. Leveraging this\ndataset, we conduct a systematic analysis of multilingual web accessibility and\nuncover widespread neglect of accessibility hints. We find that these hints\noften fail to reflect the language diversity of visible content, reducing the\neffectiveness of screen readers and limiting web accessibility. We finally\npropose Kizuki, a language-aware automated accessibility testing extension to\naccount for the limited utility of language-inconsistent accessibility hints.", "AI": {"tldr": "This paper introduces a new dataset (LangCrUX) for multilingual web accessibility, reveals issues with current accessibility hints, and proposes a language-aware testing extension (Kizuki).", "motivation": "Multilingualism on the web introduces significant barriers for users with visual impairments, as assistive technologies frequently lack robust support for non-Latin scripts. Large-scale studies of this issue have been limited by the lack of comprehensive datasets on multilingual web content.", "method": "The paper introduces LangCrUX, a large-scale dataset of 120,000 popular websites across 12 languages that primarily use non-Latin scripts, and conducts a systematic analysis of multilingual web accessibility.", "result": "The study uncovers widespread neglect of accessibility hints and finds that these hints often fail to reflect the language diversity of visible content, reducing the effectiveness of screen readers and limiting web accessibility.", "conclusion": "The paper proposes Kizuki, a language-aware automated accessibility testing extension to account for the limited utility of language-inconsistent accessibility hints."}}
{"id": "2508.18391", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18391", "abs": "https://arxiv.org/abs/2508.18391", "authors": ["Nitin Nagesh Kulkarni", "Bryson Wilcox", "Max Sawa", "Jason Thom"], "title": "PKG-DPO: Optimizing Domain-Specific AI systems with Physics Knowledge Graphs and Direct Preference Optimization", "comment": null, "summary": "Advancing AI systems in scientific domains like physics, materials science,\nand engineering calls for reasoning over complex, multi-physics phenomena while\nrespecting governing principles. Although Large Language Models (LLMs) and\nexisting preference optimization techniques perform well on standard\nbenchmarks, they often struggle to differentiate between physically valid and\ninvalid reasoning. This shortcoming becomes critical in high-stakes\napplications like metal joining, where seemingly plausible yet physically\nincorrect recommendations can lead to defects, material waste, equipment\ndamage, and serious safety risks. To address this challenge, we introduce\nPKG-DPO, a novel framework that integrates Physics Knowledge Graphs (PKGs) with\nDirect Preference Optimization (DPO) to enforce physical validity in\nAI-generated outputs. PKG-DPO comprises three key components A) hierarchical\nphysics knowledge graph that encodes cross-domain relationships, conservation\nlaws, and thermodynamic principles. B) A physics reasoning engine that\nleverages structured knowledge to improve discrimination between physically\nconsistent and inconsistent responses. C) A physics-grounded evaluation suite\ndesigned to assess compliance with domain-specific constraints. PKG-DPO\nachieves 17% fewer constraint violations and an 11% higher Physics Score\ncompared to KG-DPO (knowledge graph-based DPO). Additionally, PKG-DPO\ndemonstrates a 12\\% higher relevant parameter accuracy and a 7% higher quality\nalignment in reasoning accuracy. While our primary focus is on metal joining,\nthe framework is broadly applicable to other multi-scale, physics-driven\ndomains, offering a principled approach to embedding scientific constraints\ninto preference learning.", "AI": {"tldr": "PKG-DPO: A new framework that uses Physics Knowledge Graphs to make AI reasoning more physically valid, especially for applications like metal joining.", "motivation": "LLMs struggle to differentiate between physically valid and invalid reasoning, which is critical in high-stakes applications like metal joining where incorrect recommendations can lead to serious consequences.", "method": "A novel framework, PKG-DPO, integrates Physics Knowledge Graphs (PKGs) with Direct Preference Optimization (DPO). It comprises a hierarchical physics knowledge graph, a physics reasoning engine, and a physics-grounded evaluation suite.", "result": "PKG-DPO achieves 17% fewer constraint violations and an 11% higher Physics Score compared to KG-DPO. Additionally, PKG-DPO demonstrates a 12% higher relevant parameter accuracy and a 7% higher quality alignment in reasoning accuracy.", "conclusion": "PKG-DPO significantly improves physical validity in AI-generated outputs, reducing constraint violations and increasing physics score, relevant parameter accuracy, and reasoning accuracy. The framework is broadly applicable to multi-scale, physics-driven domains."}}
{"id": "2508.18289", "categories": ["cs.LG", "I.2; J.2"], "pdf": "https://arxiv.org/pdf/2508.18289", "abs": "https://arxiv.org/abs/2508.18289", "authors": ["Mateus A. Fernandes", "Michael M. Furlanetti", "Eduardo Gildin", "Marcio A. Sampaio"], "title": "Data-driven models for production forecasting and decision supporting in petroleum reservoirs", "comment": "Manuscript as submitted to Journal of Petroleum Exploration and\n  Production Technology", "summary": "Forecasting production reliably and anticipating changes in the behavior of\nrock-fluid systems are the main challenges in petroleum reservoir engineering.\nThis project proposes to deal with this problem through a data-driven approach\nand using machine learning methods. The objective is to develop a methodology\nto forecast production parameters based on simple data as produced and injected\nvolumes and, eventually, gauges located in wells, without depending on\ninformation from geological models, fluid properties or details of well\ncompletions and flow systems. Initially, we performed relevance analyses of the\nproduction and injection variables, as well as conditioning the data to suit\nthe problem. As reservoir conditions change over time, concept drift is a\npriority concern and require special attention to those observation windows and\nthe periodicity of retraining, which are also objects of study. For the\nproduction forecasts, we study supervised learning methods, such as those based\non regressions and Neural Networks, to define the most suitable for our\napplication in terms of performance and complexity. In a first step, we\nevaluate the methodology using synthetic data generated from the UNISIM III\ncompositional simulation model. Next, we applied it to cases of real plays in\nthe Brazilian pre-salt. The expected result is the design of a reliable\npredictor for reproducing reservoir dynamics, with rapid response, capability\nof dealing with practical difficulties such as restrictions in wells and\nprocessing units, and that can be used in actions to support reservoir\nmanagement, including the anticipation of deleterious behaviors, optimization\nof production and injection parameters and the analysis of the effects of\nprobabilistic events, aiming to maximize oil recovery.", "AI": {"tldr": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5f00\u53d1\u4e00\u79cd\u9884\u6d4b\u751f\u4ea7\u53c2\u6570\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u4f9d\u8d56\u5730\u8d28\u6a21\u578b\uff0c\u6d41\u4f53\u6027\u8d28\u6216\u4e95\u5b8c\u4e95\u548c\u6d41\u52a8\u7cfb\u7edf\u7684\u8be6\u7ec6\u4fe1\u606f\u3002", "motivation": "\u53ef\u9760\u5730\u9884\u6d4b\u4ea7\u91cf\u5e76\u9884\u6d4b\u5ca9\u77f3-\u6d41\u4f53\u7cfb\u7edf\u884c\u4e3a\u7684\u53d8\u5316\u662f\u6cb9\u85cf\u5de5\u7a0b\u4e2d\u7684\u4e3b\u8981\u6311\u6218\u3002", "method": "\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f8b\u5982\u57fa\u4e8e\u56de\u5f52\u548c\u795e\u7ecf\u7f51\u7edc\u7684\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5bf9\u751f\u4ea7\u548c\u6ce8\u5165\u53d8\u91cf\u8fdb\u884c\u4e86\u76f8\u5173\u6027\u5206\u6790\uff0c\u4ee5\u53ca\u8c03\u6574\u6570\u636e\u4ee5\u9002\u5e94\u95ee\u9898\u3002\u7531\u4e8e\u50a8\u5c42\u6761\u4ef6\u968f\u65f6\u95f4\u53d8\u5316\uff0c\u6982\u5ff5\u6f02\u79fb\u662f\u4f18\u5148\u8003\u8651\u7684\u95ee\u9898\uff0c\u9700\u8981\u7279\u522b\u5173\u6ce8\u90a3\u4e9b\u89c2\u5bdf\u7a97\u53e3\u548c\u518d\u8bad\u7ec3\u7684\u5468\u671f\u6027\u3002\u4f7f\u7528\u6765\u81eaUNISIM III\u6210\u5206\u6a21\u62df\u6a21\u578b\u7684\u5408\u6210\u6570\u636e\u8bc4\u4f30\u8be5\u65b9\u6cd5\u3002\u63a5\u4e0b\u6765\uff0c\u5c06\u5176\u5e94\u7528\u4e8e\u5df4\u897f\u76d0\u4e0b\u5730\u533a\u7684\u771f\u5b9e\u6848\u4f8b\u3002", "conclusion": "\u8bbe\u8ba1\u4e00\u4e2a\u53ef\u9760\u7684\u9884\u6d4b\u5668\uff0c\u7528\u4e8e\u91cd\u73b0\u50a8\u5c42\u52a8\u6001\uff0c\u5177\u6709\u5feb\u901f\u54cd\u5e94\uff0c\u80fd\u591f\u5904\u7406\u5b9e\u9645\u56f0\u96be\uff0c\u4f8b\u5982\u4e95\u548c\u5904\u7406\u5355\u5143\u7684\u9650\u5236\uff0c\u5e76\u4e14\u53ef\u4ee5\u7528\u4e8e\u652f\u6301\u50a8\u5c42\u7ba1\u7406\u7684\u884c\u52a8\uff0c\u5305\u62ec\u9884\u6d4b\u6709\u5bb3\u884c\u4e3a\uff0c\u4f18\u5316\u751f\u4ea7\u548c\u6ce8\u5165\u53c2\u6570\u4ee5\u53ca\u5206\u6790\u6982\u7387\u4e8b\u4ef6\u7684\u5f71\u54cd\uff0c\u65e8\u5728\u6700\u5927\u7a0b\u5ea6\u5730\u63d0\u9ad8\u91c7\u6cb9\u7387\u3002"}}
{"id": "2508.18576", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2508.18576", "abs": "https://arxiv.org/abs/2508.18576", "authors": ["Farzad Habibi", "Juncheng Fang", "Tania Lorido-Botran", "Faisal Nawab"], "title": "Brook-2PL: Tolerating High Contention Workloads with A Deadlock-Free Two-Phase Locking Protocol", "comment": null, "summary": "The problem of hotspots remains a critical challenge in high-contention\nworkloads for concurrency control (CC) protocols. Traditional concurrency\ncontrol approaches encounter significant difficulties under high contention,\nresulting in excessive transaction aborts and deadlocks. In this paper, we\npropose Brook-2PL, a novel two-phase locking (2PL) protocol that (1) introduces\nSLW-Graph for deadlock-free transaction execution, and (2) proposes partial\ntransaction chopping for early lock release. Previous methods suffer from\ntransaction aborts that lead to wasted work and can further burden the system\ndue to their cascading effects. Brook-2PL addresses this limitation by\nstatically analyzing a new graph-based dependency structure called SLW-Graph,\nenabling deadlock-free two-phase locking through predetermined lock\nacquisition. Brook-2PL also reduces contention by enabling early lock release\nusing partial transaction chopping and static transaction analysis. We overcome\nthe inherent limitations of traditional transaction chopping by providing a\nmore flexible chopping method. Evaluation using both our synthetic online game\nstore workload and the TPC-C benchmark shows that Brook-2PL significantly\noutperforms state-of-the-art CC protocols. Brook-2PL achieves an average\nspeed-up of 2.86x while reducing tail latency (p95) by 48% in the TPC-C\nbenchmark.", "AI": {"tldr": "Brook-2PL\u662f\u4e00\u79cd\u65b0\u76842PL\u534f\u8bae\uff0c\u901a\u8fc7SLW-Graph\u5b9e\u73b0\u65e0\u6b7b\u9501\u4e8b\u52a1\u6267\u884c\uff0c\u5e76\u901a\u8fc7\u90e8\u5206\u4e8b\u52a1\u5207\u5206\u63d0\u524d\u91ca\u653e\u9501\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709CC\u534f\u8bae\u3002", "motivation": "\u9ad8\u4e89\u7528\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u70ed\u70b9\u95ee\u9898\u4ecd\u7136\u662f\u5e76\u53d1\u63a7\u5236(CC)\u534f\u8bae\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u4f20\u7edf\u7684\u5e76\u53d1\u63a7\u5236\u65b9\u6cd5\u5728\u9ad8\u4e89\u7528\u4e0b\u9047\u5230\u91cd\u5927\u56f0\u96be\uff0c\u5bfc\u81f4\u8fc7\u591a\u7684\u4e8b\u52a1\u4e2d\u6b62\u548c\u6b7b\u9501\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e24\u9636\u6bb5\u9501\u5b9a(2PL)\u534f\u8baeBrook-2PL\uff0c\u8be5\u534f\u8bae:(1)\u5f15\u5165SLW-Graph\u7528\u4e8e\u65e0\u6b7b\u9501\u4e8b\u52a1\u6267\u884c\uff0c(2)\u63d0\u51fa\u90e8\u5206\u4e8b\u52a1\u5207\u5206\u4ee5\u63d0\u524d\u91ca\u653e\u9501\u3002", "result": "Brook-2PL\u901a\u8fc7\u9759\u6001\u5206\u6790\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u56fe\u7684\u4f9d\u8d56\u7ed3\u6784(\u79f0\u4e3aSLW-Graph)\u6765\u89e3\u51b3\u8fd9\u4e2a\u9650\u5236\uff0c\u901a\u8fc7\u9884\u5b9a\u7684\u9501\u83b7\u53d6\u5b9e\u73b0\u65e0\u6b7b\u9501\u7684\u4e24\u9636\u6bb5\u9501\u5b9a\u3002Brook-2PL\u8fd8\u901a\u8fc7\u4f7f\u7528\u90e8\u5206\u4e8b\u52a1\u5207\u5206\u548c\u9759\u6001\u4e8b\u52a1\u5206\u6790\u6765\u5b9e\u73b0\u63d0\u524d\u9501\u91ca\u653e\uff0c\u4ece\u800c\u51cf\u5c11\u4e89\u7528\u3002", "conclusion": "Brook-2PL\u663e\u8457\u4f18\u4e8e\u73b0\u6709CC\u534f\u8bae\uff0cTPC-C\u57fa\u51c6\u6d4b\u8bd5\u5e73\u5747\u52a0\u901f2.86\u500d\uff0c\u5c3e\u90e8\u5ef6\u8fdf(p95)\u964d\u4f4e48%\u3002"}}
{"id": "2508.18322", "categories": ["cs.CV", "cs.AI", "68T10", "I.2.4"], "pdf": "https://arxiv.org/pdf/2508.18322", "abs": "https://arxiv.org/abs/2508.18322", "authors": ["Jiangfeng Sun", "Sihao He", "Zhonghong Ou", "Meina Song"], "title": "Structures Meet Semantics: Multimodal Fusion via Graph Contrastive Learning", "comment": "9 pages,7 figures,conference", "summary": "Multimodal sentiment analysis (MSA) aims to infer emotional states by\neffectively integrating textual, acoustic, and visual modalities. Despite\nnotable progress, existing multimodal fusion methods often neglect\nmodality-specific structural dependencies and semantic misalignment, limiting\ntheir quality, interpretability, and robustness. To address these challenges,\nwe propose a novel framework called the Structural-Semantic Unifier (SSU),\nwhich systematically integrates modality-specific structural information and\ncross-modal semantic grounding for enhanced multimodal representations.\nSpecifically, SSU dynamically constructs modality-specific graphs by leveraging\nlinguistic syntax for text and a lightweight, text-guided attention mechanism\nfor acoustic and visual modalities, thus capturing detailed intra-modal\nrelationships and semantic interactions. We further introduce a semantic\nanchor, derived from global textual semantics, that serves as a cross-modal\nalignment hub, effectively harmonizing heterogeneous semantic spaces across\nmodalities. Additionally, we develop a multiview contrastive learning objective\nthat promotes discriminability, semantic consistency, and structural coherence\nacross intra- and inter-modal views. Extensive evaluations on two widely used\nbenchmark datasets, CMU-MOSI and CMU-MOSEI, demonstrate that SSU consistently\nachieves state-of-the-art performance while significantly reducing\ncomputational overhead compared to prior methods. Comprehensive qualitative\nanalyses further validate SSU's interpretability and its ability to capture\nnuanced emotional patterns through semantically grounded interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e86SSU\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u6a21\u6001\u7279\u5b9a\u7ed3\u6784\u4fe1\u606f\u548c\u8de8\u6a21\u6001\u8bed\u4e49\u57fa\u7840\u6765\u589e\u5f3a\u591a\u6a21\u6001\u8868\u793a\uff0c\u5e76\u5728CMU-MOSI\u548cCMU-MOSEI\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86SOTA\u7ed3\u679c\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u6a21\u6001\u7279\u5b9a\u7684\u7ed3\u6784\u4f9d\u8d56\u6027\u548c\u8bed\u4e49\u4e0d\u5bf9\u9f50\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u8d28\u91cf\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u7ed3\u6784-\u8bed\u4e49\u7edf\u4e00\u5668 (SSU) \u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7cfb\u7edf\u5730\u6574\u5408\u4e86\u6a21\u6001\u7279\u5b9a\u7ed3\u6784\u4fe1\u606f\u548c\u8de8\u6a21\u6001\u8bed\u4e49\u57fa\u7840\uff0c\u4ee5\u589e\u5f3a\u591a\u6a21\u6001\u8868\u793a\u3002", "result": "SSU\u5728\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6570\u636e\u96c6CMU-MOSI\u548cCMU-MOSEI\u4e0a\u59cb\u7ec8\u5982\u4e00\u5730\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4e0e\u5148\u524d\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u663e\u7740\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "SSU\u5728CMU-MOSI\u548cCMU-MOSEI\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u5173\u8054\u7684\u4ea4\u4e92\u6355\u83b7\u7ec6\u5fae\u7684\u60c5\u611f\u6a21\u5f0f\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2508.18700", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18700", "abs": "https://arxiv.org/abs/2508.18700", "authors": ["Yi-Ping Hsu", "Po-Wei Wang", "Chantat Eksombatchai", "Jiajing Xu"], "title": "Taming the One-Epoch Phenomenon in Online Recommendation System by Two-stage Contrastive ID Pre-training", "comment": "Published at RecSys'24, see\n  https://dl.acm.org/doi/10.1145/3640457.3688053", "summary": "ID-based embeddings are widely used in web-scale online recommendation\nsystems. However, their susceptibility to overfitting, particularly due to the\nlong-tail nature of data distributions, often limits training to a single\nepoch, a phenomenon known as the \"one-epoch problem.\" This challenge has driven\nresearch efforts to optimize performance within the first epoch by enhancing\nconvergence speed or feature sparsity. In this study, we introduce a novel\ntwo-stage training strategy that incorporates a pre-training phase using a\nminimal model with contrastive loss, enabling broader data coverage for the\nembedding system. Our offline experiments demonstrate that multi-epoch training\nduring the pre-training phase does not lead to overfitting, and the resulting\nembeddings improve online generalization when fine-tuned for more complex\ndownstream recommendation tasks. We deployed the proposed system in live\ntraffic at Pinterest, achieving significant site-wide engagement gains.", "AI": {"tldr": "Introduces a two-stage training strategy with contrastive pre-training to address the one-epoch problem in ID-based embeddings, improving generalization and engagement gains.", "motivation": "ID-based embeddings in online recommendation systems are susceptible to overfitting due to long-tail data distributions, limiting training to a single epoch (the \"one-epoch problem\").", "method": "A novel two-stage training strategy that incorporates a pre-training phase using a minimal model with contrastive loss.", "result": "Multi-epoch training during the pre-training phase does not lead to overfitting, and the resulting embeddings improve online generalization. Significant site-wide engagement gains were achieved at Pinterest.", "conclusion": "The proposed two-stage training strategy, including a pre-training phase with contrastive loss, improves online generalization and achieves significant site-wide engagement gains at Pinterest."}}
{"id": "2508.18381", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18381", "abs": "https://arxiv.org/abs/2508.18381", "authors": ["Yuchun Fan", "Yilin Wang", "Yongyu Mu", "Lei Huang", "Bei Li", "Xiaocheng Feng", "Tong Xiao", "Jingbo Zhu"], "title": "Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models", "comment": "Accepted by EMNLP 2025 findings", "summary": "Large vision-language models (LVLMs) have demonstrated exceptional\ncapabilities in understanding visual information with human languages but also\nexhibit an imbalance in multilingual capabilities. In this work, we delve into\nthe multilingual working pattern of LVLMs and identify a salient correlation\nbetween the multilingual understanding ability of LVLMs and language-specific\nneuron activations in shallow layers. Building on this insight, we introduce\nPLAST, a training recipe that achieves efficient multilingual enhancement for\nLVLMs by Precise LAnguage-Specific layers fine-Tuning. PLAST first identifies\nlayers involved in multilingual understanding by monitoring language-specific\nneuron activations. These layers are then precisely fine-tuned with\nquestion-translation pairs to achieve multilingual alignment. Our empirical\nresults on MM-Bench and MMMB demonstrate that PLAST effectively improves the\nmultilingual capabilities of LVLMs and achieves significant efficiency with\nonly 14% of the parameters tuned. Further analysis reveals that PLAST can be\ngeneralized to low-resource and complex visual reasoning tasks, facilitating\nthe language-specific visual information engagement in shallow layers.", "AI": {"tldr": "PLAST\u901a\u8fc7\u7cbe\u786e\u7684\u7279\u5b9a\u8bed\u8a00\u5c42\u5fae\u8c03\uff0c\u6709\u6548\u63d0\u5347\u4e86LVLM\u7684\u591a\u8bed\u8a00\u80fd\u529b\uff0c\u4e14\u6548\u7387\u5f88\u9ad8\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(LVLM)\u5728\u7528\u4eba\u7c7b\u8bed\u8a00\u7406\u89e3\u89c6\u89c9\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u80fd\u529b\uff0c\u4f46\u4e5f\u8868\u73b0\u51fa\u591a\u8bed\u8a00\u80fd\u529b\u7684\u4e0d\u5e73\u8861\u3002", "method": "\u901a\u8fc7\u76d1\u6d4b\u7279\u5b9a\u4e8e\u8bed\u8a00\u7684\u795e\u7ecf\u5143\u6fc0\u6d3b\u6765\u8bc6\u522b\u53c2\u4e0e\u591a\u8bed\u8a00\u7406\u89e3\u7684\u5c42\uff0c\u7136\u540e\u4f7f\u7528\u95ee\u9898-\u7ffb\u8bd1\u5bf9\u7cbe\u786e\u5730\u5fae\u8c03\u8fd9\u4e9b\u5c42\uff0c\u4ee5\u5b9e\u73b0\u591a\u8bed\u8a00\u5bf9\u9f50\u3002", "result": "\u5728MM-Bench\u548cMMMB\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPLAST\u6709\u6548\u5730\u63d0\u9ad8\u4e86LVLM\u7684\u591a\u8bed\u8a00\u80fd\u529b\uff0c\u5e76\u4e14\u4ec5\u8c03\u6574\u4e8614%\u7684\u53c2\u6570\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6548\u7387\u63d0\u5347\u3002", "conclusion": "PLAST\u6709\u6548\u5730\u63d0\u9ad8\u4e86LVLM\u7684\u591a\u8bed\u8a00\u80fd\u529b\uff0c\u5e76\u4e14\u4ec5\u8c03\u6574\u4e8614%\u7684\u53c2\u6570\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6548\u7387\u63d0\u5347\u3002PLAST\u53ef\u4ee5\u63a8\u5e7f\u5230\u4f4e\u8d44\u6e90\u548c\u590d\u6742\u7684\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\uff0c\u4ece\u800c\u4fc3\u8fdb\u6d45\u5c42\u4e2d\u7279\u5b9a\u4e8e\u8bed\u8a00\u7684\u89c6\u89c9\u4fe1\u606f\u53c2\u4e0e\u3002"}}
{"id": "2508.18467", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18467", "abs": "https://arxiv.org/abs/2508.18467", "authors": ["Olivia Long", "Carter Teplica"], "title": "The AI in the Mirror: LLM Self-Recognition in an Iterated Public Goods Game", "comment": null, "summary": "As AI agents become increasingly capable of tool use and long-horizon tasks,\nthey have begun to be deployed in settings where multiple agents can interact.\nHowever, whereas prior work has mostly focused on human-AI interactions, there\nis an increasing need to understand AI-AI interactions. In this paper, we adapt\nthe iterated public goods game, a classic behavioral economics game, to analyze\nthe behavior of four reasoning and non-reasoning models across two conditions:\nmodels are either told they are playing against \"another AI agent\" or told\ntheir opponents are themselves. We find that, across different settings,\ntelling LLMs that they are playing against themselves significantly changes\ntheir tendency to cooperate. While our study is conducted in a toy environment,\nour results may provide insights into multi-agent settings where agents\n\"unconsciously\" discriminating against each other could inexplicably increase\nor decrease cooperation.", "AI": {"tldr": "AI agents' cooperation changes when told they are playing against themselves in a public goods game. ", "motivation": "there is an increasing need to understand AI-AI interactions", "method": "adapt the iterated public goods game, a classic behavioral economics game, to analyze the behavior of four reasoning and non-reasoning models across two conditions: models are either told they are playing against \"another AI agent\" or told their opponents are themselves", "result": "telling LLMs that they are playing against themselves significantly changes their tendency to cooperate", "conclusion": "telling LLMs that they are playing against themselves significantly changes their tendency to cooperate"}}
{"id": "2508.18301", "categories": ["cs.LG", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.18301", "abs": "https://arxiv.org/abs/2508.18301", "authors": ["Md Sabbir Ahmed", "Nova Ahmed"], "title": "A Fast and Minimal System to Identify Depression Using Smartphones: Explainable Machine Learning-Based Approach", "comment": null, "summary": "Background: Existing robust, pervasive device-based systems developed in\nrecent years to detect depression require data collected over a long period and\nmay not be effective in cases where early detection is crucial.\n  Objective: Our main objective was to develop a minimalistic system to\nidentify depression using data retrieved in the fastest possible time.\n  Methods: We developed a fast tool that retrieves the past 7 days' app usage\ndata in 1 second (mean 0.31, SD 1.10 seconds). A total of 100 students from\nBangladesh participated in our study, and our tool collected their app usage\ndata. To identify depressed and nondepressed students, we developed a diverse\nset of ML models. We selected important features using the stable approach,\nalong with 3 main types of feature selection (FS) approaches.\n  Results: Leveraging only the app usage data retrieved in 1 second, our light\ngradient boosting machine model used the important features selected by the\nstable FS approach and correctly identified 82.4% (n=42) of depressed students\n(precision=75%, F1-score=78.5%). Moreover, after comprehensive exploration, we\npresented a parsimonious stacking model where around 5 features selected by the\nall-relevant FS approach Boruta were used in each iteration of validation and\nshowed a maximum precision of 77.4% (balanced accuracy=77.9%). A SHAP analysis\nof our best models presented behavioral markers that were related to\ndepression.\n  Conclusions: Due to our system's fast and minimalistic nature, it may make a\nworthwhile contribution to identifying depression in underdeveloped and\ndeveloping regions. In addition, our detailed discussion about the implication\nof our findings can facilitate the development of less resource-intensive\nsystems to better understand students who are depressed.", "AI": {"tldr": "Developed a fast, minimalistic system using 1-second app usage data to identify depression in students, achieving up to 82.4% accuracy.", "motivation": "Existing robust, pervasive device-based systems developed in recent years to detect depression require data collected over a long period and may not be effective in cases where early detection is crucial.", "method": "We developed a fast tool that retrieves the past 7 days' app usage data in 1 second (mean 0.31, SD 1.10 seconds). A total of 100 students from Bangladesh participated in our study, and our tool collected their app usage data. To identify depressed and nondepressed students, we developed a diverse set of ML models. We selected important features using the stable approach, along with 3 main types of feature selection (FS) approaches.", "result": "Leveraging only the app usage data retrieved in 1 second, our light gradient boosting machine model used the important features selected by the stable FS approach and correctly identified 82.4% (n=42) of depressed students (precision=75%, F1-score=78.5%). Moreover, after comprehensive exploration, we presented a parsimonious stacking model where around 5 features selected by the all-relevant FS approach Boruta were used in each iteration of validation and showed a maximum precision of 77.4% (balanced accuracy=77.9%). A SHAP analysis of our best models presented behavioral markers that were related to depression.", "conclusion": "Due to our system's fast and minimalistic nature, it may make a worthwhile contribution to identifying depression in underdeveloped and developing regions. In addition, our detailed discussion about the implication of our findings can facilitate the development of less resource-intensive systems to better understand students who are depressed."}}
{"id": "2508.18616", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2508.18616", "abs": "https://arxiv.org/abs/2508.18616", "authors": ["Yalong Zhang", "Rong-Hua Li", "Qi Zhang", "Guoren Wang"], "title": "Optimal $(\u03b1,\u03b2)$-Dense Subgraph Search in Bipartite Graphs", "comment": null, "summary": "Dense subgraph search in bipartite graphs is a fundamental problem in graph\nanalysis, with wide-ranging applications in fraud detection, recommendation\nsystems, and social network analysis. The recently proposed $(\\alpha,\n\\beta)$-dense subgraph model has demonstrated superior capability in capturing\nthe intrinsic density structure of bipartite graphs compared to existing\nalternatives. However, despite its modeling advantages, the $(\\alpha,\n\\beta)$-dense subgraph model lacks efficient support for query processing and\ndynamic updates, limiting its practical utility in large-scale applications. To\naddress these limitations, we propose BD-Index, a novel index that answers\n$(\\alpha, \\beta)$-dense subgraph queries in optimal time while using only\nlinear space $O(|E|)$, making it well-suited for real-world applications\nrequiring both fast query processing and low memory consumption. We further\ndevelop two complementary maintenance strategies for dynamic bipartite graphs\nto support efficient updates to the BD-Index. The space-efficient strategy\nupdates the index in time complexity of $O(p \\cdot |E|^{1.5})$ per edge\ninsertion or deletion, while maintaining a low space cost of $O(|E|)$ (the same\nas the index itself), where $p$ is typically a small constant in real-world\ngraphs. In contrast, the time-efficient strategy significantly reduces the\nupdate time to $O(p \\cdot |E|)$ per edge update by maintaining auxiliary\norientation structures, at the cost of increased memory usage up to $O(p \\cdot\n|E|)$. These two strategies provide flexible trade-offs between maintenance\nefficiency and memory usage, enabling BD-Index to adapt to diverse application\nrequirements. Extensive experiments on 10 large-scale real-world datasets\ndemonstrate high efficiency and scalability of our proposed solutions.", "AI": {"tldr": "This paper introduces BD-Index, a novel index for answering dense subgraph queries in bipartite graphs with efficient query processing and dynamic updates.", "motivation": "The $(\\alpha, \\beta)$-dense subgraph model lacks efficient support for query processing and dynamic updates, limiting its practical utility in large-scale applications.", "method": "The paper proposes BD-Index, a novel index that answers $(\\alpha, \\beta)$-dense subgraph queries in optimal time while using only linear space $O(|E|)$. Two complementary maintenance strategies for dynamic bipartite graphs are developed to support efficient updates to the BD-Index.", "result": "BD-Index answers $(\\alpha, \\beta)$-dense subgraph queries in optimal time while using only linear space $O(|E|)$. The space-efficient strategy updates the index in time complexity of $O(p \\cdot |E|^{1.5})$ per edge insertion or deletion, while maintaining a low space cost of $O(|E|)$. The time-efficient strategy significantly reduces the update time to $O(p \\cdot |E|)$ per edge update by maintaining auxiliary orientation structures, at the cost of increased memory usage up to $O(p \\cdot |E|)$.", "conclusion": "The paper introduces BD-Index, a novel index for answering $(\\alpha, \\beta)$-dense subgraph queries. Experiments on real-world datasets demonstrate the efficiency and scalability of the proposed solutions."}}
{"id": "2508.18389", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18389", "abs": "https://arxiv.org/abs/2508.18389", "authors": ["Hao Liang", "Zhixuan Ge", "Ashish Tiwari", "Soumendu Majee", "G. M. Dilshan Godaliyadda", "Ashok Veeraraghavan", "Guha Balakrishnan"], "title": "FastAvatar: Instant 3D Gaussian Splatting for Faces from Single Unconstrained Poses", "comment": "11 pages, 5 figures", "summary": "We present FastAvatar, a pose-invariant, feed-forward framework that can\ngenerate a 3D Gaussian Splatting (3DGS) model from a single face image from an\narbitrary pose in near-instant time (<10ms). FastAvatar uses a novel\nencoder-decoder neural network design to achieve both fast fitting and identity\npreservation regardless of input pose. First, FastAvatar constructs a 3DGS face\n``template'' model from a training dataset of faces with multi-view captures.\nSecond, FastAvatar encodes the input face image into an identity-specific and\npose-invariant latent embedding, and decodes this embedding to predict\nresiduals to the structural and appearance parameters of each Gaussian in the\ntemplate 3DGS model. By only inferring residuals in a feed-forward fashion,\nmodel inference is fast and robust. FastAvatar significantly outperforms\nexisting feed-forward face 3DGS methods (e.g., GAGAvatar) in reconstruction\nquality, and runs 1000x faster than per-face optimization methods (e.g.,\nFlashAvatar, GaussianAvatars and GASP). In addition, FastAvatar's novel latent\nspace design supports real-time identity interpolation and attribute editing\nwhich is not possible with any existing feed-forward 3DGS face generation\nframework. FastAvatar's combination of excellent reconstruction quality and\nspeed expands the scope of 3DGS for photorealistic avatar applications in\nconsumer and interactive systems.", "AI": {"tldr": "FastAvatar is a pose-invariant, feed-forward framework that generates a 3D Gaussian Splatting model from a single face image in near-instant time. It outperforms existing methods in speed and quality, and supports real-time identity interpolation and attribute editing.", "motivation": "To generate a 3D Gaussian Splatting (3DGS) model from a single face image from an arbitrary pose in near-instant time.", "method": "FastAvatar encodes the input face image into an identity-specific and pose-invariant latent embedding, and decodes this embedding to predict residuals to the structural and appearance parameters of each Gaussian in the template 3DGS model.", "result": "FastAvatar significantly outperforms existing feed-forward face 3DGS methods in reconstruction quality, and runs 1000x faster than per-face optimization methods. FastAvatar's novel latent space design supports real-time identity interpolation and attribute editing which is not possible with any existing feed-forward 3DGS face generation framework.", "conclusion": "FastAvatar's combination of excellent reconstruction quality and speed expands the scope of 3DGS for photorealistic avatar applications in consumer and interactive systems."}}
{"id": "2508.18877", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18877", "abs": "https://arxiv.org/abs/2508.18877", "authors": ["Kushagra Agrawal", "Nisharg Nargund", "Oishani Banerjee"], "title": "Optimization of Latent-Space Compression using Game-Theoretic Techniques for Transformer-Based Vector Search", "comment": null, "summary": "Vector similarity search plays a pivotal role in modern information retrieval\nsystems, especially when powered by transformer-based embeddings. However, the\nscalability and efficiency of such systems are often hindered by the high\ndimensionality of latent representations. In this paper, we propose a novel\ngame-theoretic framework for optimizing latent-space compression to enhance\nboth the efficiency and semantic utility of vector search. By modeling the\ncompression strategy as a zero-sum game between retrieval accuracy and storage\nefficiency, we derive a latent transformation that preserves semantic\nsimilarity while reducing redundancy. We benchmark our method against FAISS, a\nwidely-used vector search library, and demonstrate that our approach achieves a\nsignificantly higher average similarity (0.9981 vs. 0.5517) and utility (0.8873\nvs. 0.5194), albeit with a modest increase in query time. This trade-off\nhighlights the practical value of game-theoretic latent compression in\nhigh-utility, transformer-based search applications. The proposed system can be\nseamlessly integrated into existing LLM pipelines to yield more semantically\naccurate and computationally efficient retrieval.", "AI": {"tldr": "This paper introduces a game-theoretic approach to latent space compression for vector search, improving semantic accuracy and efficiency, especially for transformer-based embeddings in LLM pipelines.", "motivation": "The scalability and efficiency of vector similarity search systems are often hindered by the high dimensionality of latent representations.", "method": "The paper proposes a game-theoretic framework for optimizing latent-space compression.", "result": "The proposed approach achieves a significantly higher average similarity (0.9981 vs. 0.5517) and utility (0.8873 vs. 0.5194) compared to FAISS, with a modest increase in query time.", "conclusion": "The proposed game-theoretic latent compression method can be seamlessly integrated into existing LLM pipelines to yield more semantically accurate and computationally efficient retrieval."}}
{"id": "2508.18384", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18384", "abs": "https://arxiv.org/abs/2508.18384", "authors": ["Kellen Tan Cheng", "Anna Lisa Gentile", "Chad DeLuca", "Guang-Jie Ren"], "title": "Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails", "comment": null, "summary": "The pervasiveness of large language models (LLMs) in enterprise settings has\nalso brought forth a significant amount of risks associated with their usage.\nGuardrails technologies aim to mitigate this risk by filtering LLMs'\ninput/output text through various detectors. However, developing and\nmaintaining robust detectors faces many challenges, one of which is the\ndifficulty in acquiring production-quality labeled data on real LLM outputs\nprior to deployment. In this work, we propose backprompting, a simple yet\nintuitive solution to generate production-like labeled data for health advice\nguardrails development. Furthermore, we pair our backprompting method with a\nsparse human-in-the-loop clustering technique to label the generated data. Our\naim is to construct a parallel corpus roughly representative of the original\ndataset yet resembling real LLM output. We then infuse existing datasets with\nour synthetic examples to produce robust training data for our detector. We\ntest our technique in one of the most difficult and nuanced guardrails: the\nidentification of health advice in LLM output, and demonstrate improvement\nversus other solutions. Our detector is able to outperform GPT-4o by up to\n3.73%, despite having 400x less parameters.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u53cd\u5411\u63d0\u793a\u201d\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u7528\u4e8e\u8bad\u7ec3\u5065\u5eb7\u5efa\u8bae\u68c0\u6d4b\u5668\u7684\u6570\u636e\u3002\u8be5\u68c0\u6d4b\u5668\u4f18\u4e8e GPT-4o\uff0c\u53c2\u6570\u51cf\u5c11\u4e86 400 \u500d\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u666e\u904d\u5b58\u5728\u4e5f\u5e26\u6765\u4e86\u4e0e\u5176\u4f7f\u7528\u76f8\u5173\u7684\u5de8\u5927\u98ce\u9669\u3002Guardrails \u6280\u672f\u65e8\u5728\u901a\u8fc7\u5404\u79cd\u68c0\u6d4b\u5668\u8fc7\u6ee4 LLM \u7684\u8f93\u5165/\u8f93\u51fa\u6587\u672c\u6765\u964d\u4f4e\u8fd9\u79cd\u98ce\u9669\u3002\u7136\u800c\uff0c\u5f00\u53d1\u548c\u7ef4\u62a4\u5f3a\u5927\u7684\u68c0\u6d4b\u5668\u9762\u4e34\u8bb8\u591a\u6311\u6218\uff0c\u5176\u4e2d\u4e4b\u4e00\u662f\u5728\u90e8\u7f72\u4e4b\u524d\u96be\u4ee5\u83b7\u53d6\u5173\u4e8e\u771f\u5b9e LLM \u8f93\u51fa\u7684\u751f\u4ea7\u8d28\u91cf\u6807\u7b7e\u6570\u636e\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u53cd\u5411\u63d0\u793a\uff0c\u8fd9\u662f\u4e00\u79cd\u7b80\u5355\u76f4\u89c2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u751f\u6210\u7c7b\u4f3c\u751f\u4ea7\u73af\u5883\u7684\u6807\u7b7e\u6570\u636e\uff0c\u7528\u4e8e\u5065\u5eb7\u5efa\u8bae\u62a4\u680f\u5f00\u53d1\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c06\u53cd\u5411\u63d0\u793a\u65b9\u6cd5\u4e0e\u7a00\u758f\u7684\u4eba\u5de5\u5faa\u73af\u805a\u7c7b\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u4ee5\u6807\u8bb0\u751f\u6210\u7684\u6570\u636e\u3002", "result": "\u6211\u4eec\u7684\u76ee\u6807\u662f\u6784\u5efa\u4e00\u4e2a\u5927\u81f4\u4ee3\u8868\u539f\u59cb\u6570\u636e\u96c6\u4f46\u53c8\u7c7b\u4f3c\u4e8e\u771f\u5b9e LLM \u8f93\u51fa\u7684\u5e76\u884c\u8bed\u6599\u5e93\u3002\u7136\u540e\uff0c\u6211\u4eec\u5c06\u73b0\u6709\u6570\u636e\u96c6\u4e0e\u6211\u4eec\u7684\u5408\u6210\u793a\u4f8b\u878d\u5408\uff0c\u4ece\u800c\u4e3a\u6211\u4eec\u7684\u68c0\u6d4b\u5668\u751f\u6210\u5f3a\u5927\u7684\u8bad\u7ec3\u6570\u636e\u3002\u6211\u4eec\u5728\u6700\u56f0\u96be\u548c\u7ec6\u81f4\u7684\u62a4\u680f\u4e4b\u4e00\u4e2d\u6d4b\u8bd5\u4e86\u6211\u4eec\u7684\u6280\u672f\uff1a\u8bc6\u522b LLM \u8f93\u51fa\u4e2d\u7684\u5065\u5eb7\u5efa\u8bae\uff0c\u5e76\u8bc1\u660e\u4e0e\u5176\u5b83\u89e3\u51b3\u65b9\u6848\u76f8\u6bd4\u6709\u6240\u6539\u8fdb\u3002", "conclusion": "\u8be5\u68c0\u6d4b\u5668\u4f18\u4e8e GPT-4o\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe 3.73%\uff0c\u4f46\u53c2\u6570\u51cf\u5c11\u4e86 400 \u500d\u3002"}}
{"id": "2508.18507", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18507", "abs": "https://arxiv.org/abs/2508.18507", "authors": ["Dillon Z. Chen", "Johannes Zenn", "Tristan Cinquin", "Sheila A. McIlraith"], "title": "Language Models For Generalised PDDL Planning: Synthesising Sound and Programmatic Policies", "comment": "RLC 2025 Workshop on Programmatic Reinforcement Learning", "summary": "We study the usage of language models (LMs) for planning over world models\nspecified in the Planning Domain Definition Language (PDDL). We prompt LMs to\ngenerate Python programs that serve as generalised policies for solving PDDL\nproblems from a given domain. Notably, our approach synthesises policies that\nare provably sound relative to the PDDL domain without reliance on external\nverifiers. We conduct experiments on competition benchmarks which show that our\npolicies can solve more PDDL problems than PDDL planners and recent LM\napproaches within a fixed time and memory constraint. Our approach manifests in\nthe LMPlan planner which can solve planning problems with several hundreds of\nrelevant objects. Surprisingly, we observe that LMs used in our framework\nsometimes plan more effectively over PDDL problems written in meaningless\nsymbols in place of natural language; e.g. rewriting (at dog kitchen) as (p2 o1\no3). This finding challenges hypotheses that LMs reason over word semantics and\nmemorise solutions from its training corpus, and is worth further exploration.", "AI": {"tldr": "\u4f7f\u7528 LM \u751f\u6210 Python \u7a0b\u5e8f\u4f5c\u4e3a\u901a\u7528\u7b56\u7565\uff0c\u4ee5\u89e3\u51b3 PDDL \u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b (LM) \u5728\u89c4\u5212\u57df\u5b9a\u4e49\u8bed\u8a00 (PDDL) \u4e2d\u6307\u5b9a\u7684\u4e16\u754c\u6a21\u578b\u4e0a\u7684\u89c4\u5212\u5e94\u7528\u3002", "method": "\u63d0\u793a LM \u751f\u6210 Python \u7a0b\u5e8f\uff0c\u4f5c\u4e3a\u89e3\u51b3\u7ed9\u5b9a\u57df\u4e2d PDDL \u95ee\u9898\u7684\u901a\u7528\u7b56\u7565\u3002", "result": "\u6211\u4eec\u7684\u7b56\u7565\u53ef\u4ee5\u89e3\u51b3\u6bd4 PDDL \u89c4\u5212\u5668\u548c\u6700\u8fd1\u7684 LM \u65b9\u6cd5\u66f4\u591a\u7684 PDDL \u95ee\u9898\u3002LMPlan \u89c4\u5212\u5668\u53ef\u4ee5\u89e3\u51b3\u5177\u6709\u6570\u767e\u4e2a\u76f8\u5173\u5bf9\u8c61\u7684\u89c4\u5212\u95ee\u9898\u3002LMs \u5728\u7528\u65e0\u610f\u4e49\u7b26\u53f7\u4ee3\u66ff\u81ea\u7136\u8bed\u8a00\u7f16\u5199\u7684 PDDL \u95ee\u9898\u4e0a\uff0c\u6709\u65f6\u80fd\u66f4\u6709\u6548\u5730\u8fdb\u884c\u89c4\u5212\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u56fa\u5b9a\u65f6\u95f4\u548c\u5185\u5b58\u7ea6\u675f\u5185\uff0c\u6bd4 PDDL \u89c4\u5212\u5668\u548c\u6700\u8fd1\u7684 LM \u65b9\u6cd5\u80fd\u89e3\u51b3\u66f4\u591a\u7684 PDDL \u95ee\u9898\u3002\u8be5\u6846\u67b6\u4e2d\u4f7f\u7528\u7684 LM \u6709\u65f6\u80fd\u66f4\u6709\u6548\u5730\u89c4\u5212\u7528\u65e0\u610f\u4e49\u7b26\u53f7\u4ee3\u66ff\u81ea\u7136\u8bed\u8a00\u7f16\u5199\u7684 PDDL \u95ee\u9898\u3002"}}
{"id": "2508.18303", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.18303", "abs": "https://arxiv.org/abs/2508.18303", "authors": ["Jueqi Wang", "Zachary Jacokes", "John Darrell Van Horn", "Michael C. Schatz", "Kevin A. Pelphrey", "Archana Venkataraman"], "title": "Learning Explainable Imaging-Genetics Associations Related to a Neurological Disorder", "comment": null, "summary": "While imaging-genetics holds great promise for unraveling the complex\ninterplay between brain structure and genetic variation in neurological\ndisorders, traditional methods are limited to simplistic linear models or to\nblack-box techniques that lack interpretability. In this paper, we present\nNeuroPathX, an explainable deep learning framework that uses an early fusion\nstrategy powered by cross-attention mechanisms to capture meaningful\ninteractions between structural variations in the brain derived from MRI and\nestablished biological pathways derived from genetics data. To enhance\ninterpretability and robustness, we introduce two loss functions over the\nattention matrix - a sparsity loss that focuses on the most salient\ninteractions and a pathway similarity loss that enforces consistent\nrepresentations across the cohort. We validate NeuroPathX on both autism\nspectrum disorder and Alzheimer's disease. Our results demonstrate that\nNeuroPathX outperforms competing baseline approaches and reveals biologically\nplausible associations linked to the disorder. These findings underscore the\npotential of NeuroPathX to advance our understanding of complex brain\ndisorders. Code is available at https://github.com/jueqiw/NeuroPathX .", "AI": {"tldr": "NeuroPathX \u662f\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u673a\u5236\u6765\u6355\u83b7\u5927\u8111\u7ed3\u6784\u548c\u9057\u4f20\u53d8\u5f02\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5728\u8111\u90e8\u75be\u75c5\u7814\u7a76\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u5f71\u50cf\u9057\u4f20\u5b66\u65b9\u6cd5\u4ec5\u9650\u4e8e\u7b80\u5355\u7684\u7ebf\u6027\u6a21\u578b\u6216\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7684\u9ed1\u76d2\u6280\u672f\u3002", "method": "NeuroPathX\uff0c\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u7531\u4ea4\u53c9\u6ce8\u610f\u673a\u5236\u9a71\u52a8\u7684\u65e9\u671f\u878d\u5408\u7b56\u7565\uff0c\u4ee5\u6355\u83b7\u6765\u81ea MRI \u7684\u5927\u8111\u7ed3\u6784\u53d8\u5f02\u4e0e\u6765\u81ea\u9057\u4f20\u6570\u636e\u7684\u5df2\u5efa\u7acb\u7684\u751f\u7269\u9014\u5f84\u4e4b\u95f4\u7684\u6709\u610f\u4e49\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "NeuroPathX \u5728\u81ea\u95ed\u75c7\u8c31\u7cfb\u969c\u788d\u548c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u4e0a\u90fd\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u5e76\u4e14\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "NeuroPathX \u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u4e0e\u75be\u75c5\u76f8\u5173\u7684\u751f\u7269\u5b66\u4e0a\u5408\u7406\u7684\u5173\u8054\uff0c\u5f3a\u8c03\u4e86\u5176\u5728\u4fc3\u8fdb\u6211\u4eec\u5bf9\u590d\u6742\u8111\u90e8\u75be\u75c5\u7684\u7406\u89e3\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.18617", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2508.18617", "abs": "https://arxiv.org/abs/2508.18617", "authors": ["Ziqi Wang", "Jingzhe Zhang", "Wei Hu"], "title": "WoW: A Window-to-Window Incremental Index for Range-Filtering Approximate Nearest Neighbor Search", "comment": "Accepted in the ACM SIGMOD/PODS International Conference on\n  Management of Data (SIGMOD 2026)", "summary": "Given a hybrid dataset where every data object consists of a vector and an\nattribute value, for each query with a target vector and a range filter,\nrange-filtering approximate nearest neighbor search (RFANNS) aims to retrieve\nthe most similar vectors from the dataset and the corresponding attribute\nvalues fall in the query range. It is a fundamental function in vector database\nmanagement systems and intelligent systems with embedding abilities. Dedicated\nindices for RFANNS accelerate query speed with an acceptable accuracy loss on\nnearest neighbors. However, they are still facing the challenges to be\nconstructed incrementally and generalized to achieve superior query performance\nfor arbitrary range filters. In this paper, we introduce a window graph-based\nRFANNS index. For incremental construction, we propose an insertion algorithm\nto add new vector-attribute pairs into hierarchical window graphs with varying\nwindow size. To handle arbitrary range filters, we optimize relevant window\nsearch for attribute filter checks and vector distance computations by range\nselectivity. Extensive experiments on real-world datasets show that for index\nconstruction, the indexing time is on par with the most building-efficient\nindex, and 4.9x faster than the most query-efficient index with 0.4-0.5x\nsmaller size; For RFANNS query, it is 4x faster than the most efficient\nincremental index, and matches the performance of the best statically-built\nindex.", "AI": {"tldr": "This paper introduces a window graph-based index for RFANNS that supports incremental construction and arbitrary range filters, achieving high performance in both indexing and querying.", "motivation": "RFANNS is a fundamental function in vector database management systems and intelligent systems, but existing indices face challenges in incremental construction and generalization for arbitrary range filters.", "method": "A window graph-based RFANNS index with an insertion algorithm for incremental construction and optimized window search for arbitrary range filters.", "result": "Experiments on real-world datasets demonstrate the efficiency and effectiveness of the proposed index in terms of indexing time, index size, and query performance.", "conclusion": "The proposed window graph-based RFANNS index achieves comparable indexing time to the fastest index, 4.9x faster indexing than the fastest query index with 0.4-0.5x smaller size, 4x faster query than the fastest incremental index, and matches the performance of the best statically-built index."}}
{"id": "2508.18415", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.18415", "abs": "https://arxiv.org/abs/2508.18415", "authors": ["Giuseppe Stragapede", "Sam Merrick", "Vedrana Krivoku\u0107a Hahn", "Justin Sukaitis", "Vincent Graf Narbel"], "title": "Securing Face and Fingerprint Templates in Humanitarian Biometric Systems", "comment": null, "summary": "In humanitarian and emergency scenarios, the use of biometrics can\ndramatically improve the efficiency of operations, but it poses risks for the\ndata subjects, which are exacerbated in contexts of vulnerability. To address\nthis, we present a mobile biometric system implementing a biometric template\nprotection (BTP) scheme suitable for these scenarios. After rigorously\nformulating the functional, operational, and security and privacy requirements\nof these contexts, we perform a broad comparative analysis of the BTP\nlandscape. PolyProtect, a method designed to operate on neural network face\nembeddings, is identified as the most suitable method due to its effectiveness,\nmodularity, and lightweight computational burden. We evaluate PolyProtect in\nterms of verification and identification accuracy, irreversibility, and\nunlinkability, when this BTP method is applied to face embeddings extracted\nusing EdgeFace, a novel state-of-the-art efficient feature extractor, on a\nreal-world face dataset from a humanitarian field project in Ethiopia.\nMoreover, as PolyProtect promises to be modality-independent, we extend its\nevaluation to fingerprints. To the best of our knowledge, this is the first\ntime that PolyProtect has been evaluated for the identification scenario and\nfor fingerprint biometrics. Our experimental results are promising, and we plan\nto release our code", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u4eba\u9053\u4e3b\u4e49\u73af\u5883\u7684\u79fb\u52a8\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\uff0c\u5e76\u8bc4\u4f30\u4e86PolyProtect\u5728\u9762\u90e8\u548c\u6307\u7eb9\u8bc6\u522b\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5728\u4eba\u9053\u4e3b\u4e49\u548c\u7d27\u6025\u60c5\u51b5\u4e0b\uff0c\u751f\u7269\u8bc6\u522b\u6280\u672f\u7684\u4f7f\u7528\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u8fd0\u8425\u6548\u7387\uff0c\u4f46\u5b83\u7ed9\u6570\u636e\u4e3b\u4f53\u5e26\u6765\u4e86\u98ce\u9669\uff0c\u800c\u5728\u8106\u5f31\u7684\u73af\u5883\u4e2d\uff0c\u8fd9\u4e9b\u98ce\u9669\u4f1a\u52a0\u5267\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79fb\u52a8\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5b9e\u73b0\u4e86\u9002\u7528\u4e8e\u4eba\u9053\u4e3b\u4e49\u548c\u7d27\u6025\u60c5\u51b5\u7684\u751f\u7269\u8bc6\u522b\u6a21\u677f\u4fdd\u62a4\uff08BTP\uff09\u65b9\u6848\u3002\u8be5\u65b9\u6848\u4f7f\u7528PolyProtect\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u795e\u7ecf\u7f51\u7edc\u9762\u90e8\u5d4c\u5165\u3002", "result": "PolyProtect\u5728\u9a8c\u8bc1\u548c\u8bc6\u522b\u7cbe\u5ea6\u3001\u4e0d\u53ef\u9006\u6027\u548c\u4e0d\u53ef\u94fe\u63a5\u6027\u65b9\u9762\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u5c06\u5176\u5e94\u7528\u4e8e\u4f7f\u7528EdgeFace\u63d0\u53d6\u7684\u9762\u90e8\u5d4c\u5165\uff0cEdgeFace\u662f\u4e00\u79cd\u65b0\u578b\u7684\u6700\u5148\u8fdb\u7684\u9ad8\u6548\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u8be5\u63d0\u53d6\u5668\u6765\u81ea\u57c3\u585e\u4fc4\u6bd4\u4e9a\u7684\u4eba\u9053\u4e3b\u4e49\u9886\u57df\u9879\u76ee\u7684\u771f\u5b9e\u9762\u90e8\u6570\u636e\u96c6\u3002", "conclusion": "PolyProtect\u5728\u4eba\u9053\u4e3b\u4e49\u63f4\u52a9\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u9996\u6b21\u5728\u8eab\u4efd\u8bc6\u522b\u548c\u6307\u7eb9\u751f\u7269\u8bc6\u522b\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u7ed3\u679c\u5f88\u6709\u5e0c\u671b\uff0c\u4ee3\u7801\u8ba1\u5212\u53d1\u5e03\u3002"}}
{"id": "2508.18387", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18387", "abs": "https://arxiv.org/abs/2508.18387", "authors": ["Ivan Kobyzev", "Abbas Ghaddar", "Dingtao Hu", "Boxing Chen"], "title": "Integral Transformer: Denoising Attention, Not Too Much Not Too Little", "comment": "EMNLP 2025 Main", "summary": "Softmax self-attention often assigns disproportionate weight to semantically\nuninformative tokens such as special tokens and punctuation, a phenomenon known\nas attention noise. While recent methods like Cog Attention and the\nDifferential Transformer have addressed this by introducing negative attention\nscores, they risk discarding useful information. In this paper, we propose the\nIntegral Transformer, a novel self-attention mechanism that denoises attention\nby integrating signals sampled from the logit distribution. Our approach\nmitigates noise while preserving the contributions of special tokens critical\nfor model performance. Extensive experiments demonstrate that our model\noutperforms vanilla, Cog, and Differential attention variants on\nwell-established knowledge and reasoning language benchmarks. Moreover, our\nanalysis reveals that employing vanilla self-attention in the lower Transformer\nlayers enhances performance and that the Integral Transformer effectively\nbalances attention distributions and reduces rank collapse in upper layers.", "AI": {"tldr": "The Integral Transformer denoises attention by integrating signals from the logit distribution, outperforming other attention mechanisms on knowledge and reasoning benchmarks.", "motivation": "Softmax self-attention often assigns disproportionate weight to semantically uninformative tokens, known as attention noise. Existing methods risk discarding useful information.", "method": "The Integral Transformer, a novel self-attention mechanism that denoises attention by integrating signals sampled from the logit distribution.", "result": "The Integral Transformer mitigates noise while preserving the contributions of special tokens. ", "conclusion": "The Integral Transformer outperforms vanilla, Cog, and Differential attention variants on knowledge and reasoning language benchmarks. Vanilla self-attention in lower layers improves performance. The Integral Transformer balances attention distributions and reduces rank collapse in upper layers."}}
{"id": "2508.18515", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18515", "abs": "https://arxiv.org/abs/2508.18515", "authors": ["Dillon Z. Chen"], "title": "Weisfeiler-Leman Features for Planning: A 1,000,000 Sample Size Hyperparameter Study", "comment": "Extended version of ECAI 2025 paper", "summary": "Weisfeiler-Leman Features (WLFs) are a recently introduced classical machine\nlearning tool for learning to plan and search. They have been shown to be both\ntheoretically and empirically superior to existing deep learning approaches for\nlearning value functions for search in symbolic planning. In this paper, we\nintroduce new WLF hyperparameters and study their various tradeoffs and\neffects. We utilise the efficiency of WLFs and run planning experiments on\nsingle core CPUs with a sample size of 1,000,000 to understand the effect of\nhyperparameters on training and planning. Our experimental analysis show that\nthere is a robust and best set of hyperparameters for WLFs across the tested\nplanning domains. We find that the best WLF hyperparameters for learning\nheuristic functions minimise execution time rather than maximise model\nexpressivity. We further statistically analyse and observe no significant\ncorrelation between training and planning metrics.", "AI": {"tldr": "Introduce new WLF hyperparameters and study their various tradeoffs and effects. Find that the best WLF hyperparameters for learning heuristic functions minimise execution time rather than maximise model expressivity.", "motivation": "WLFs are theoretically and empirically superior to existing deep learning approaches for learning value functions for search in symbolic planning.", "method": "Introduce new WLF hyperparameters and run planning experiments on single core CPUs with a sample size of 1,000,000 to understand the effect of hyperparameters on training and planning.", "result": "There is a robust and best set of hyperparameters for WLFs across the tested planning domains.", "conclusion": "The best WLF hyperparameters for learning heuristic functions minimise execution time rather than maximise model expressivity. There is no significant correlation between training and planning metrics."}}
{"id": "2508.18306", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18306", "abs": "https://arxiv.org/abs/2508.18306", "authors": ["Wuxinlin Cheng", "Yupeng Cao", "Jinwen Wu", "Koduvayur Subbalakshmi", "Tian Han", "Zhuo Feng"], "title": "SALMAN: Stability Analysis of Language Models Through the Maps Between Graph-based Manifolds", "comment": null, "summary": "Recent strides in pretrained transformer-based language models have propelled\nstate-of-the-art performance in numerous NLP tasks. Yet, as these models grow\nin size and deployment, their robustness under input perturbations becomes an\nincreasingly urgent question. Existing robustness methods often diverge between\nsmall-parameter and large-scale models (LLMs), and they typically rely on\nlabor-intensive, sample-specific adversarial designs. In this paper, we propose\na unified, local (sample-level) robustness framework (SALMAN) that evaluates\nmodel stability without modifying internal parameters or resorting to complex\nperturbation heuristics. Central to our approach is a novel Distance Mapping\nDistortion (DMD) measure, which ranks each sample's susceptibility by comparing\ninput-to-output distance mappings in a near-linear complexity manner. By\ndemonstrating significant gains in attack efficiency and robust training, we\nposition our framework as a practical, model-agnostic tool for advancing the\nreliability of transformer-based NLP systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86 SALMAN\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u548c\u63d0\u9ad8\u57fa\u4e8e transformer \u7684 NLP \u7cfb\u7edf\u9c81\u68d2\u6027\u7684\u6846\u67b6\uff0c\u5b83\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\uff0c\u4e0d\u9700\u8981\u4fee\u6539\u6a21\u578b\u53c2\u6570\u6216\u8fdb\u884c\u590d\u6742\u7684\u6270\u52a8\u3002", "motivation": "\u968f\u7740\u8fd9\u4e9b\u6a21\u578b\u5728\u89c4\u6a21\u548c\u90e8\u7f72\u65b9\u9762\u7684\u589e\u957f\uff0c\u5b83\u4eec\u5728\u8f93\u5165\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\u6210\u4e3a\u4e00\u4e2a\u65e5\u76ca\u7d27\u8feb\u7684\u95ee\u9898\u3002\u73b0\u6709\u7684\u9c81\u68d2\u6027\u65b9\u6cd5\u901a\u5e38\u5728\u5c0f\u53c2\u6570\u6a21\u578b\u548c\u5927\u578b\u6a21\u578b (LLM) \u4e4b\u95f4\u5b58\u5728\u5dee\u5f02\uff0c\u5e76\u4e14\u5b83\u4eec\u901a\u5e38\u4f9d\u8d56\u4e8e\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\u3001\u7279\u5b9a\u4e8e\u6837\u672c\u7684\u5bf9\u6297\u6027\u8bbe\u8ba1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8ddd\u79bb\u6620\u5c04\u5931\u771f (DMD) \u6d4b\u91cf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6bd4\u8f83\u4ee5\u8fd1\u4e4e\u7ebf\u6027\u7684\u590d\u6742\u5ea6\u7684\u65b9\u5f0f\u8f93\u5165\u5230\u8f93\u51fa\u7684\u8ddd\u79bb\u6620\u5c04\u6765\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u6613\u611f\u6027\u8fdb\u884c\u6392\u5e8f\u3002", "result": "\u5728\u653b\u51fb\u6548\u7387\u548c\u9c81\u68d2\u8bad\u7ec3\u65b9\u9762\u8868\u73b0\u51fa\u663e\u7740 gains\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u5c40\u90e8\u7684\u9c81\u68d2\u6027\u6846\u67b6 (SALMAN)\uff0c\u8be5\u6846\u67b6\u8bc4\u4f30\u6a21\u578b\u7a33\u5b9a\u6027\uff0c\u800c\u65e0\u9700\u4fee\u6539\u5185\u90e8\u53c2\u6570\u6216\u6c42\u52a9\u4e8e\u590d\u6742\u7684\u6270\u52a8\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002\u901a\u8fc7\u8bc1\u660e\u653b\u51fb\u6548\u7387\u548c\u7a33\u5065\u8bad\u7ec3\u65b9\u9762\u7684\u663e\u7740 gains\uff0c\u5c06\u8be5\u6846\u67b6\u5b9a\u4f4d\u4e3a\u4e00\u79cd\u5b9e\u7528\u7684\u3001\u6a21\u578b\u4e0d\u53ef\u77e5\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u63d0\u9ad8\u57fa\u4e8e transformer \u7684 NLP \u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2508.18736", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18736", "abs": "https://arxiv.org/abs/2508.18736", "authors": ["Jungwoo Kim", "Minsang Kim", "Jaeheon Lee", "Chanwoo Moon", "Heejin Kim", "Taeho Hwang", "Woosuk Chung", "Yeseong Kim", "Sungjin Lee"], "title": "Rethinking Caching for LLM Serving Systems: Beyond Traditional Heuristics", "comment": null, "summary": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems.", "AI": {"tldr": "SISO\u662f\u4e00\u79cd\u7528\u4e8eLLM\u670d\u52a1\u7684\u65b0\u578b\u8bed\u4e49\u7f13\u5b58\u7cfb\u7edf\uff0c\u5b83\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7f13\u5b58\u7b56\u7565\u4e0d\u8db3\uff1a\u7cbe\u786e\u5339\u914d\u548c\u524d\u7f00\u7f13\u5b58\u5ffd\u7565\u67e5\u8be2\u8bed\u4e49\uff0c\u800c\u6700\u5148\u8fdb\u7684\u8bed\u4e49\u7f13\u5b58\u4ecd\u7136\u5c40\u9650\u4e8e\u4f20\u7edf\u76f4\u89c9\uff0c\u51e0\u4e4e\u6ca1\u6709\u6982\u5ff5\u4e0a\u7684\u504f\u79bb\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SISO\uff0c\u8fd9\u662f\u4e00\u4e2a\u8bed\u4e49\u7f13\u5b58\u7cfb\u7edf\uff0c\u5b83\u91cd\u65b0\u5b9a\u4e49\u4e86LLM\u670d\u52a1\u7684\u6548\u7387\u3002", "method": "SISO\u5f15\u5165\u4e86\u57fa\u4e8e\u8d28\u5fc3\u7684\u7f13\u5b58\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u8986\u76d6\u7387\u5e76\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u5185\u5b58\uff0c\u5f15\u5165\u4e86 locality-aware \u66ff\u6362\u4ee5\u4fdd\u7559\u9ad8\u4ef7\u503c\u6761\u76ee\uff0c\u5e76\u5f15\u5165\u4e86\u52a8\u6001\u9608\u503c\u4ee5\u5728\u4e0d\u540c\u7684\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u5e73\u8861\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u3002", "result": "SISO\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u547d\u4e2d\u7387\u548c\u66f4\u5f3a\u7684SLO\u8fbe\u6210\u3002", "conclusion": "SISO\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe1.71\u500d\u7684\u66f4\u9ad8\u547d\u4e2d\u7387\uff0c\u5e76\u4e14\u59cb\u7ec8\u6bd4\u6700\u5148\u8fdb\u7684\u7cfb\u7edf\u66f4\u597d\u5730\u5b9e\u73b0SLO\u3002"}}
{"id": "2508.18421", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18421", "abs": "https://arxiv.org/abs/2508.18421", "authors": ["Fatemeh Ziaeetabar"], "title": "Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?", "comment": null, "summary": "Vision foundation models (FMs) have become the predominant architecture in\ncomputer vision, providing highly transferable representations learned from\nlarge-scale, multimodal corpora. Nonetheless, they exhibit persistent\nlimitations on tasks that require explicit reasoning over entities, roles, and\nspatio-temporal relations. Such relational competence is indispensable for\nfine-grained human activity recognition, egocentric video understanding, and\nmultimodal medical image analysis, where spatial, temporal, and semantic\ndependencies are decisive for performance. We advance the position that\nnext-generation FMs should incorporate explicit relational interfaces,\ninstantiated as dynamic relational graphs (graphs whose topology and edge\nsemantics are inferred from the input and task context). We illustrate this\nposition with cross-domain evidence from recent systems in human manipulation\naction recognition and brain tumor segmentation, showing that augmenting FMs\nwith lightweight, context-adaptive graph-reasoning modules improves\nfine-grained semantic fidelity, out of distribution robustness,\ninterpretability, and computational efficiency relative to FM only baselines.\nImportantly, by reasoning sparsely over semantic nodes, such hybrids also\nachieve favorable memory and hardware efficiency, enabling deployment under\npractical resource constraints. We conclude with a targeted research agenda for\nFM graph hybrids, prioritizing learned dynamic graph construction, multi-level\nrelational reasoning (e.g., part object scene in activity understanding, or\nregion organ in medical imaging), cross-modal fusion, and evaluation protocols\nthat directly probe relational competence in structured vision tasks.", "AI": {"tldr": "\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u5173\u7cfb\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u5f15\u5165\u52a8\u6001\u5173\u7cfb\u56fe\u6765\u589e\u5f3a\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u9700\u8981\u5bf9\u5b9e\u4f53\u3001\u89d2\u8272\u548c\u65f6\u7a7a\u5173\u7cfb\u8fdb\u884c\u663e\u5f0f\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\u3002\u5173\u7cfb\u80fd\u529b\u5bf9\u4e8e\u7ec6\u7c92\u5ea6\u7684\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u3001\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u89c6\u9891\u7406\u89e3\u548c\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u5206\u6790\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5c06\u8f7b\u91cf\u7ea7\u7684\u3001\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u7684\u56fe\u63a8\u7406\u6a21\u5757\u6dfb\u52a0\u5230\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e2d\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u5728\u4eba\u7c7b\u64cd\u4f5c\u52a8\u4f5c\u8bc6\u522b\u548c\u8111\u80bf\u7624\u5206\u5272\u7b49\u4efb\u52a1\u4e2d\u7684\u4ea4\u53c9\u9886\u57df\u8bc1\u636e\u8868\u660e\uff0c\u4e0e\u4ec5\u4f7f\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u76f8\u6bd4\uff0c\u589e\u5f3a\u540e\u7684\u6a21\u578b\u63d0\u9ad8\u4e86\u7ec6\u7c92\u5ea6\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\u3001\u5206\u5e03\u5916\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002\u901a\u8fc7\u5bf9\u8bed\u4e49\u8282\u70b9\u8fdb\u884c\u7a00\u758f\u63a8\u7406\uff0c\u8fd9\u79cd\u6df7\u5408\u65b9\u6cd5\u8fd8\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u5185\u5b58\u548c\u786c\u4ef6\u6548\u7387\u3002", "conclusion": "\u4e0b\u4e00\u4ee3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5e94\u5305\u542b\u663e\u5f0f\u5173\u7cfb\u63a5\u53e3\uff0c\u4ee5\u52a8\u6001\u5173\u7cfb\u56fe\u7684\u5f62\u5f0f\u5b9e\u4f8b\u5316\u3002\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u5305\u62ec\u5b66\u4e60\u52a8\u6001\u56fe\u6784\u5efa\u3001\u591a\u5c42\u6b21\u5173\u7cfb\u63a8\u7406\u3001\u8de8\u6a21\u6001\u878d\u5408\u4ee5\u53ca\u76f4\u63a5\u63a2\u6d4b\u7ed3\u6784\u5316\u89c6\u89c9\u4efb\u52a1\u4e2d\u5173\u7cfb\u80fd\u529b\u7684\u8bc4\u4f30\u534f\u8bae\u3002"}}
{"id": "2508.18395", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18395", "abs": "https://arxiv.org/abs/2508.18395", "authors": ["Jeong-seok Oh", "Jay-yoon Lee"], "title": "Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning", "comment": null, "summary": "Probabilistic decoding in Large Language Models (LLMs) often yields\ninconsistent outputs, particularly on complex or long-form questions.\nSelf-Consistency (SC) mitigates this for short-form QA by majority voting over\nexact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram\nConsistency Score (WUCS) extend to long-form responses but lose accuracy on\nshort-form benchmarks.\n  We introduce Latent Self-Consistency (LSC), which selects the most\nsemantically consistent response using learnable token embeddings. A\nlightweight forward generation of summary tokens increases inference time by\nless than 1% and requires no changes to the model architecture.\n  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU,\nTruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form\nones on average, while maintaining negligible computational overhead. These\nresults position LSC as a practical consistency-selection method that works\nreliably across answer formats. Additionally, LSC provides well-calibrated\nconfidence estimates, maintaining low Expected Calibration Error across both\nanswer formats.", "AI": {"tldr": "LSC \u662f\u4e00\u79cd\u65b0\u7684 self-consistency \u65b9\u6cd5\uff0c\u5b83\u5728\u77ed\u7bc7\u548c\u957f\u7bc7\u95ee\u9898\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u8ba1\u7b97\u5f00\u9500\u5f88\u5c0f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u4e2d\u7684\u6982\u7387\u89e3\u7801\u901a\u5e38\u4f1a\u4ea7\u751f\u4e0d\u4e00\u81f4\u7684\u8f93\u51fa\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u6216\u957f\u7bc7\u95ee\u9898\u4e0a\u3002Self-Consistency (SC) \u901a\u8fc7\u5bf9\u7cbe\u786e\u5b57\u7b26\u4e32\u8fdb\u884c\u591a\u6570\u6295\u7968\u6765\u7f13\u89e3\u77ed\u7bc7QA\u7684\u8fd9\u4e2a\u95ee\u9898\uff0c\u800cUniversal Self-Consistency (USC)\u548cWeighted Unigram Consistency Score (WUCS) \u6269\u5c55\u5230\u957f\u7bc7\u56de\u590d\uff0c\u4f46\u5728\u77ed\u7bc7\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f1a\u964d\u4f4e\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u53ef\u5b66\u4e60\u7684token embeddings\u9009\u62e9\u8bed\u4e49\u4e0a\u6700\u4e00\u81f4\u7684\u54cd\u5e94\u3002\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u524d\u5411\u751f\u6210summary tokens\uff0c\u63a8\u7406\u65f6\u95f4\u589e\u52a0\u4e0d\u52301%\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u66f4\u6539\u6a21\u578b\u67b6\u6784\u3002", "result": "\u57286\u4e2a\u77ed\u7bc7\u548c5\u4e2a\u957f\u7bc7\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLSC\u5728\u6240\u6709\u77ed\u7bc7\u548c\u957f\u7bc7\u6d4b\u8bd5\u4e2d\u5e73\u5747\u8d85\u8fc7SC\u3001USC\u548cWUCS\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u5ffd\u7565\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "Latent Self-Consistency (LSC)\u5728\u5404\u79cd\u95ee\u9898\u5f62\u5f0f\u4e0a\u90fd\u4f18\u4e8e\u5176\u4ed6self-consistency\u65b9\u6cd5\uff0c\u5e76\u4e14\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\u4e0d\u8ba1\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u826f\u597d\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u3002"}}
{"id": "2508.18520", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18520", "abs": "https://arxiv.org/abs/2508.18520", "authors": ["Dillon Z. Chen"], "title": "Symmetry-Invariant Novelty Heuristics via Unsupervised Weisfeiler-Leman Features", "comment": "HSDIP@ICAPS 2025 Workshop", "summary": "Novelty heuristics aid heuristic search by exploring states that exhibit\nnovel atoms. However, novelty heuristics are not symmetry invariant and hence\nmay sometimes lead to redundant exploration. In this preliminary report, we\npropose to use Weisfeiler-Leman Features for planning (WLFs) in place of atoms\nfor detecting novelty. WLFs are recently introduced features for learning\ndomain-dependent heuristics for generalised planning problems. We explore an\nunsupervised usage of WLFs for synthesising lifted, domain-independent novelty\nheuristics that are invariant to symmetric states. Experiments on the classical\nInternational Planning Competition and Hard To Ground benchmark suites yield\npromising results for novelty heuristics synthesised from WLFs.", "AI": {"tldr": "\u4f7f\u7528 Weisfeiler-Leman \u7279\u5f81\u6765\u6539\u5584\u542f\u53d1\u5f0f\u641c\u7d22\u4e2d\u7684\u65b0\u9896\u6027\u68c0\u6d4b\uff0c\u4f7f\u5176\u5bf9\u5bf9\u79f0\u72b6\u6001\u4fdd\u6301\u4e0d\u53d8\uff0c\u4ece\u800c\u63d0\u9ad8\u89c4\u5212\u6548\u7387\u3002", "motivation": "\u65b0\u9896\u6027\u542f\u53d1\u5f0f\u65b9\u6cd5\u4e0d\u662f\u5bf9\u79f0\u4e0d\u53d8\u7684\uff0c\u56e0\u6b64\u6709\u65f6\u53ef\u80fd\u5bfc\u81f4\u5197\u4f59\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4f7f\u7528 Weisfeiler-Leman \u7279\u5f81\u8fdb\u884c\u89c4\u5212 (WLF) \u4ee5\u68c0\u6d4b\u65b0\u9896\u6027\u3002", "result": "\u5728\u7ecf\u5178\u56fd\u9645\u89c4\u5212\u7ade\u8d5b\u548c Hard To Ground \u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u4e0a\u7684\u5b9e\u9a8c\u4ea7\u751f\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002", "conclusion": "\u4f7f\u7528 Weisfeiler-Leman \u7279\u5f81 (WLF) \u4ee3\u66ff\u539f\u5b50\u6765\u68c0\u6d4b\u65b0\u9896\u6027\uff0c\u4ece\u800c\u5408\u6210\u5bf9\u5bf9\u79f0\u72b6\u6001\u4e0d\u53d8\u7684\u3001domain-independent \u7684\u65b0\u9896\u6027\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002"}}
{"id": "2508.18307", "categories": ["cs.LG", "math.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.18307", "abs": "https://arxiv.org/abs/2508.18307", "authors": ["Mahishanka Withanachchi"], "title": "Learning Spatio-Temporal Dynamics via Operator-Valued RKHS and Kernel Koopman Methods", "comment": null, "summary": "We introduce a unified framework for learning the spatio-temporal dynamics of\nvector valued functions by combining operator valued reproducing kernel Hilbert\nspaces (OV-RKHS) with kernel based Koopman operator methods. The approach\nenables nonparametric and data driven estimation of complex time evolving\nvector fields while preserving both spatial and temporal structure. We\nestablish representer theorems for time dependent OV-RKHS interpolation, derive\nSobolev type approximation bounds for smooth vector fields, and provide\nspectral convergence guarantees for kernel Koopman operator approximations.\nThis framework supports efficient reduced order modeling and long term\nprediction of high dimensional nonlinear systems, offering theoretically\ngrounded tools for forecasting, control, and uncertainty quantification in\nspatio-temporal machine learning.", "AI": {"tldr": "unified framework for learning the spatio-temporal dynamics of vector valued functions", "motivation": "learning the spatio-temporal dynamics of vector valued functions", "method": "combining operator valued reproducing kernel Hilbert spaces (OV-RKHS) with kernel based Koopman operator methods", "result": "representer theorems for time dependent OV-RKHS interpolation, derive Sobolev type approximation bounds for smooth vector fields, and provide spectral convergence guarantees for kernel Koopman operator approximations", "conclusion": "This framework supports efficient reduced order modeling and long term prediction of high dimensional nonlinear systems, offering theoretically grounded tools for forecasting, control, and uncertainty quantification in spatio-temporal machine learning."}}
{"id": "2508.18758", "categories": ["cs.DB", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18758", "abs": "https://arxiv.org/abs/2508.18758", "authors": ["Yipeng Zhang", "Chen Wang", "Yuzhe Zhang", "Jacky Jiang"], "title": "Text to Query Plans for Question Answering on Large Tables", "comment": null, "summary": "Efficient querying and analysis of large tabular datasets remain significant\nchallenges, especially for users without expertise in programming languages\nlike SQL. Text-to-SQL approaches have shown promising performance on benchmark\ndata; however, they inherit SQL's drawbacks, including inefficiency with large\ndatasets and limited support for complex data analyses beyond basic querying.\nWe propose a novel framework that transforms natural language queries into\nquery plans. Our solution is implemented outside traditional databases,\nallowing us to support classical SQL commands while avoiding SQL's inherent\nlimitations. Additionally, we enable complex analytical functions, such as\nprincipal component analysis and anomaly detection, providing greater\nflexibility and extensibility than traditional SQL capabilities. We leverage\nLLMs to iteratively interpret queries and construct operation sequences,\naddressing computational complexity by incrementally building solutions. By\nexecuting operations directly on the data, we overcome context length\nlimitations without requiring the entire dataset to be processed by the model.\nWe validate our framework through experiments on both standard databases and\nlarge scientific tables, demonstrating its effectiveness in handling extensive\ndatasets and performing sophisticated data analyses.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u6362\u4e3a\u67e5\u8be2\u8ba1\u5212\uff0c\u4ece\u800c\u80fd\u591f\u652f\u6301\u590d\u6742\u7684\u5206\u6790\u529f\u80fd\u5e76\u5904\u7406\u5927\u578b\u6570\u636e\u96c6\uff0c\u800c\u6ca1\u6709\u4f20\u7edfSQL\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u9ad8\u6548\u67e5\u8be2\u548c\u5206\u6790\u5927\u578b\u8868\u683c\u6570\u636e\u96c6\u4ecd\u7136\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u6ca1\u6709SQL\u7b49\u7f16\u7a0b\u8bed\u8a00\u4e13\u4e1a\u77e5\u8bc6\u7684\u7528\u6237\u3002Text-to-SQL\u65b9\u6cd5\u5728\u57fa\u51c6\u6570\u636e\u4e0a\u8868\u73b0\u51fa promising \u7684\u6027\u80fd\uff1b\u4f46\u662f\uff0c\u5b83\u4eec\u7ee7\u627f\u4e86SQL\u7684\u7f3a\u70b9\uff0c\u5305\u62ec\u5927\u578b\u6570\u636e\u96c6\u7684\u6548\u7387\u4f4e\u4e0b\u4ee5\u53ca\u5bf9\u57fa\u672c\u67e5\u8be2\u4e4b\u5916\u7684\u590d\u6742\u6570\u636e\u5206\u6790\u7684\u6709\u9650\u652f\u6301\u3002", "method": "\u8be5\u6846\u67b6\u5c06\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u6362\u4e3a\u67e5\u8be2\u8ba1\u5212\uff0c\u5229\u7528LLM\u8fed\u4ee3\u89e3\u91ca\u67e5\u8be2\u5e76\u6784\u5efa\u64cd\u4f5c\u5e8f\u5217\uff0c\u901a\u8fc7\u76f4\u63a5\u5728\u6570\u636e\u4e0a\u6267\u884c\u64cd\u4f5c\u6765\u514b\u670d\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u3002", "result": "\u8be5\u6846\u67b6\u652f\u6301\u4f20\u7edf\u7684SQL\u547d\u4ee4\uff0c\u540c\u65f6\u907f\u514d\u4e86SQL\u7684\u56fa\u6709\u5c40\u9650\u6027\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u652f\u6301\u590d\u6742\u7684\u5206\u6790\u529f\u80fd\uff0c\u4f8b\u5982\u4e3b\u6210\u5206\u5206\u6790\u548c\u5f02\u5e38\u68c0\u6d4b\uff0c\u4e0e\u4f20\u7edf\u7684SQL\u529f\u80fd\u76f8\u6bd4\uff0c\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5728\u6807\u51c6\u6570\u636e\u5e93\u548c\u5927\u578b\u79d1\u5b66\u8868\u683c\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5904\u7406\u5927\u578b\u6570\u636e\u96c6\u548c\u6267\u884c\u590d\u6742\u6570\u636e\u5206\u6790\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.18425", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18425", "abs": "https://arxiv.org/abs/2508.18425", "authors": ["Lucas Wojcik", "Gabriel E. Lima", "Valfride Nascimento", "Eduil Nascimento Jr.", "Rayson Laroca", "David Menotti"], "title": "LPLC: A Dataset for License Plate Legibility Classification", "comment": "Accepted for presentation at the Conference on Graphics, Patterns and\n  Images (SIBGRAPI) 2025", "summary": "Automatic License Plate Recognition (ALPR) faces a major challenge when\ndealing with illegible license plates (LPs). While reconstruction methods such\nas super-resolution (SR) have emerged, the core issue of recognizing these\nlow-quality LPs remains unresolved. To optimize model performance and\ncomputational efficiency, image pre-processing should be applied selectively to\ncases that require enhanced legibility. To support research in this area, we\nintroduce a novel dataset comprising 10,210 images of vehicles with 12,687\nannotated LPs for legibility classification (the LPLC dataset). The images span\na wide range of vehicle types, lighting conditions, and camera/image quality\nlevels. We adopt a fine-grained annotation strategy that includes vehicle- and\nLP-level occlusions, four legibility categories (perfect, good, poor, and\nillegible), and character labels for three categories (excluding illegible\nLPs). As a benchmark, we propose a classification task using three image\nrecognition networks to determine whether an LP image is good enough, requires\nsuper-resolution, or is completely unrecoverable. The overall F1 score, which\nremained below 80% for all three baseline models (ViT, ResNet, and YOLO),\ntogether with the analyses of SR and LP recognition methods, highlights the\ndifficulty of the task and reinforces the need for further research. The\nproposed dataset is publicly available at\nhttps://github.com/lmlwojcik/lplc-dataset.", "AI": {"tldr": "A new dataset (LPLC) is introduced for legibility classification of license plates. The benchmark classification task reveals the difficulty of determining LP image quality, suggesting the need for more research.", "motivation": "ALPR faces a major challenge when dealing with illegible license plates (LPs). To optimize model performance and computational efficiency, image pre-processing should be applied selectively to cases that require enhanced legibility.", "method": "A classification task using three image recognition networks is proposed to determine whether an LP image is good enough, requires super-resolution, or is completely unrecoverable.", "result": "The overall F1 score remained below 80% for all three baseline models (ViT, ResNet, and YOLO).", "conclusion": "The task is difficult, and further research is needed."}}
{"id": "2508.18407", "categories": ["cs.CL", "cs.AI", "68T01, 68T07, 68T50", "I.2"], "pdf": "https://arxiv.org/pdf/2508.18407", "abs": "https://arxiv.org/abs/2508.18407", "authors": ["Michal \u0160tef\u00e1nik", "Timothee Mickus", "Marek Kadl\u010d\u00edk", "Michal Spiegel", "Josef Kucha\u0159"], "title": "Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering", "comment": "To appear in Findings of EMNLP 2025", "summary": "A majority of recent work in AI assesses models' generalization capabilities\nthrough the lens of performance on out-of-distribution (OOD) datasets. Despite\ntheir practicality, such evaluations build upon a strong assumption: that OOD\nevaluations can capture and reflect upon possible failures in a real-world\ndeployment.\n  In this work, we challenge this assumption and confront the results obtained\nfrom OOD evaluations with a set of specific failure modes documented in\nexisting question-answering (QA) models, referred to as a reliance on spurious\nfeatures or prediction shortcuts.\n  We find that different datasets used for OOD evaluations in QA provide an\nestimate of models' robustness to shortcuts that have a vastly different\nquality, some largely under-performing even a simple, in-distribution\nevaluation. We partially attribute this to the observation that spurious\nshortcuts are shared across ID+OOD datasets, but also find cases where a\ndataset's quality for training and evaluation is largely disconnected. Our work\nunderlines limitations of commonly-used OOD-based evaluations of\ngeneralization, and provides methodology and recommendations for evaluating\ngeneralization within and beyond QA more robustly.", "AI": {"tldr": "\u8d28\u7591\u4e86 OOD \u8bc4\u4f30\u5728\u8bc4\u4f30 QA \u6a21\u578b\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u5176\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\uff0c\u5e76\u63d0\u51fa\u4e86\u66f4\u7a33\u5065\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u5927\u591a\u6570\u6700\u8fd1\u7684 AI \u5de5\u4f5c\u901a\u8fc7\u5bf9\u5206\u5e03\u5916 (OOD) \u6570\u636e\u96c6\u7684\u6027\u80fd\u6765\u8bc4\u4f30\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5c3d\u7ba1\u5b83\u4eec\u5177\u6709\u5b9e\u7528\u6027\uff0c\u4f46\u6b64\u7c7b\u8bc4\u4f30\u5efa\u7acb\u5728\u4e00\u4e2a\u5f3a\u5927\u7684\u5047\u8bbe\u4e4b\u4e0a\uff1aOOD \u8bc4\u4f30\u53ef\u4ee5\u6355\u83b7\u5e76\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u6545\u969c\u3002", "method": "\u5c06 OOD \u8bc4\u4f30\u7684\u7ed3\u679c\u4e0e\u73b0\u6709\u95ee\u7b54 (QA) \u6a21\u578b\u4e2d\u8bb0\u5f55\u7684\u4e00\u7ec4\u7279\u5b9a\u6545\u969c\u6a21\u5f0f\u8fdb\u884c\u5bf9\u6bd4\uff0c\u8fd9\u4e9b\u6545\u969c\u6a21\u5f0f\u88ab\u79f0\u4e3a\u4f9d\u8d56\u4e8e\u865a\u5047\u7279\u5f81\u6216\u9884\u6d4b\u6377\u5f84\u3002", "result": "\u7528\u4e8e QA \u4e2d OOD \u8bc4\u4f30\u7684\u4e0d\u540c\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u5bf9\u6a21\u578b\u5bf9\u6377\u5f84\u7684\u9c81\u68d2\u6027\u7684\u4f30\u8ba1\uff0c\u8fd9\u4e9b\u4f30\u8ba1\u7684\u8d28\u91cf\u5dee\u5f02\u5f88\u5927\uff0c\u6709\u4e9b\u751a\u81f3\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f4e\u4e8e\u7b80\u5355\u7684\u5206\u5e03\u5185\u8bc4\u4f30\u3002\u6211\u4eec\u90e8\u5206\u5730\u5c06\u6b64\u5f52\u56e0\u4e8e\u865a\u5047\u6377\u5f84\u5728 ID+OOD \u6570\u636e\u96c6\u4e2d\u5171\u4eab\u7684\u89c2\u5bdf\u7ed3\u679c\uff0c\u4f46\u4e5f\u53d1\u73b0\u6570\u636e\u96c6\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u8d28\u91cf\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u8131\u8282\u7684\u60c5\u51b5\u3002", "conclusion": "\u5e38\u7528\u7684\u57fa\u4e8e OOD \u7684\u6cdb\u5316\u8bc4\u4f30\u5177\u6709\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u66f4\u7a33\u5065\u5730\u8bc4\u4f30 QA \u5185\u5916\u7684\u6cdb\u5316\u63d0\u4f9b\u4e86\u65b9\u6cd5\u548c\u5efa\u8bae\u3002"}}
{"id": "2508.18527", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18527", "abs": "https://arxiv.org/abs/2508.18527", "authors": ["Kaijie Xu", "Clark Verbrugge"], "title": "Generic Guard AI in Stealth Game with Composite Potential Fields", "comment": null, "summary": "Guard patrol behavior is central to the immersion and strategic depth of\nstealth games, while most existing systems rely on hand-crafted routes or\nspecialized logic that struggle to balance coverage efficiency and responsive\npursuit with believable naturalness. We propose a generic, fully explainable,\ntraining-free framework that integrates global knowledge and local information\nvia Composite Potential Fields, combining three interpretable maps-Information,\nConfidence, and Connectivity-into a single kernel-filtered decision criterion.\nOur parametric, designer-driven approach requires only a handful of decay and\nweight parameters-no retraining-to smoothly adapt across both occupancy-grid\nand NavMesh-partition abstractions. We evaluate on five representative game\nmaps, two player-control policies, and five guard modes, confirming that our\nmethod outperforms classical baseline methods in both capture efficiency and\npatrol naturalness. Finally, we show how common stealth mechanics-distractions\nand environmental elements-integrate naturally into our framework as sub\nmodules, enabling rapid prototyping of rich, dynamic, and responsive guard\nbehaviors.", "AI": {"tldr": "This paper introduces a new training-free framework for guard patrol behavior in stealth games that balances efficiency, responsiveness, and naturalness using Composite Potential Fields.", "motivation": "Existing guard patrol systems in stealth games struggle to balance coverage efficiency and responsive pursuit with believable naturalness.", "method": "The paper proposes a generic, fully explainable, training-free framework that integrates global knowledge and local information via Composite Potential Fields, combining three interpretable maps-Information, Confidence, and Connectivity-into a single kernel-filtered decision criterion.", "result": "The method smoothly adapts across both occupancy-grid and NavMesh-partition abstractions. The evaluation on five representative game maps, two player-control policies, and five guard modes confirms the method's effectiveness.", "conclusion": "The proposed framework outperforms classical baseline methods in both capture efficiency and patrol naturalness. Common stealth mechanics integrate naturally into the framework, enabling rapid prototyping of rich, dynamic, and responsive guard behaviors."}}
{"id": "2508.18308", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18308", "abs": "https://arxiv.org/abs/2508.18308", "authors": ["Avinash Amballa"], "title": "CoPE: A Lightweight Complex Positional Encoding", "comment": null, "summary": "Recent studies have demonstrated the effectiveness of position encoding in\ntransformer architectures. By incorporating positional information, this\napproach provides essential guidance for modeling dependencies between elements\nacross different sequence positions. We introduce CoPE (a lightweight Complex\nPositional Encoding), a novel architecture that leverages complex-valued\nencoding to encode both content and positional information. Our approach\nreplaces traditional positional encodings with complex embeddings where the\nreal part captures semantic content and the imaginary part encodes positional\ninformation. We introduce phase-aware attention in the first layer of the\ntransformer model to capture position-dependent patterns, followed by standard\nattention layers for higher-levels. We show that CoPE doesn't exhibit long term\ndecay and is compatible with linear attention. Experimental evaluation on the\nGLUE benchmark suggest that our approach achieves superior performance with\nless computational complexity, compared to RoPE, Sinusoidal and Learned\npositional encodings.", "AI": {"tldr": "CoPE\u662f\u4e00\u79cd\u65b0\u578b\u7684\u4f4d\u7f6e\u7f16\u7801\u67b6\u6784\uff0c\u5b83\u4f7f\u7528\u590d\u503c\u7f16\u7801\u548c\u76f8\u4f4d\u611f\u77e5\u6ce8\u610f\u529b\u6765\u63d0\u9ad8transformer\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u5728transformer\u67b6\u6784\u4e2d\u6709\u6548\u5229\u7528\u4f4d\u7f6e\u7f16\u7801\uff0c\u63d0\u4f9b\u5efa\u6a21\u4e0d\u540c\u5e8f\u5217\u4f4d\u7f6e\u5143\u7d20\u4e4b\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u4f7f\u7528\u590d\u503c\u7f16\u7801\u6765\u7f16\u7801\u5185\u5bb9\u548c\u4f4d\u7f6e\u4fe1\u606f\uff0c\u5e76\u5f15\u5165\u76f8\u4f4d\u611f\u77e5\u6ce8\u610f\u529b\u3002", "result": "CoPE\u6ca1\u6709\u8868\u73b0\u51fa\u957f\u671f\u8870\u51cf\uff0c\u5e76\u4e14\u4e0e\u7ebf\u6027\u6ce8\u610f\u529b\u517c\u5bb9\u3002", "conclusion": "CoPE\u5728GLUE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u4f4e\u4e8eRoPE\u3001Sinusoidal\u548cLearned\u4f4d\u7f6e\u7f16\u7801\u3002"}}
{"id": "2508.18830", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2508.18830", "abs": "https://arxiv.org/abs/2508.18830", "authors": ["Shahrzad Khayatbashi", "Majid Rafiei", "Jiayuan Chen", "Timotheus Kampik", "Gregor Berg", "Amin Jalali"], "title": "Enriching Object-Centric Event Data with Process Scopes: A Framework for Aggregation and Analysis", "comment": null, "summary": "Object-Centric Process Mining enables the analysis of complex operational\nbehavior by capturing interactions among multiple business objects (e.g.,\norders, items, deliveries). These interactions are recorded using\nObject-Centric Event Data (OCED) formats, such as the Object-Centric Event Log\n(OCEL). However, existing formats lack explicit definitions of process scopes,\nwhich restricts analysis to individual processes and limits insights to a low\nlevel of granularity. In practice, OCED often spans multiple interrelated\nprocesses, as shared objects connect events across organizational functions.\nThis structure reflects how value is created along the organizational value\nchain, but introduces challenges for interpretation when process boundaries are\nnot clearly defined. Moreover, process definitions are typically subjective and\ncontext-dependent; they vary across organizations, roles, and analytical goals,\nand cannot always be discovered automatically. To address these challenges, we\npropose a method for embedding analyst-defined process scopes into OCEL. This\nenables the structured representation of multiple coexisting processes,\nsupports the aggregation of event data across scopes, and facilitates analysis\nat varying levels of abstraction. We demonstrate the applicability of our\napproach using a publicly available OCEL log and provide supporting tools for\nscope definition and analysis.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5206\u6790\u5e08\u5b9a\u4e49\u7684\u8fc7\u7a0b\u8303\u56f4\u5d4c\u5165\u5230OCEL\u4e2d\u7684\u65b9\u6cd5\uff0c\u8fd9\u652f\u6301\u591a\u4e2a\u5171\u5b58\u8fc7\u7a0b\u7684\u7ed3\u6784\u5316\u8868\u793a\uff0c\u652f\u6301\u8de8\u8303\u56f4\u7684\u4e8b\u4ef6\u6570\u636e\u805a\u5408\uff0c\u5e76\u4fc3\u8fdb\u4e0d\u540c\u62bd\u8c61\u7ea7\u522b\u7684\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u7684\u683c\u5f0f\u7f3a\u4e4f\u5bf9\u8fc7\u7a0b\u8303\u56f4\u7684\u660e\u786e\u5b9a\u4e49\uff0c\u8fd9\u9650\u5236\u4e86\u5bf9\u5355\u4e2a\u8fc7\u7a0b\u7684\u5206\u6790\uff0c\u5e76\u5c06\u89c1\u89e3\u9650\u5236\u5728\u8f83\u4f4e\u7684\u7c92\u5ea6\u7ea7\u522b\u3002\u5728\u5b9e\u8df5\u4e2d\uff0cOCED\u901a\u5e38\u8de8\u8d8a\u591a\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u8fc7\u7a0b\uff0c\u56e0\u4e3a\u5171\u4eab\u5bf9\u8c61\u8fde\u63a5\u4e86\u8de8\u7ec4\u7ec7\u529f\u80fd\u7684\u4e8b\u4ef6\u3002\u8fd9\u79cd\u7ed3\u6784\u53cd\u6620\u4e86\u4ef7\u503c\u662f\u5982\u4f55\u6cbf\u7740\u7ec4\u7ec7\u4ef7\u503c\u94fe\u521b\u9020\u7684\uff0c\u4f46\u662f\u5f53\u8fc7\u7a0b\u8fb9\u754c\u6ca1\u6709\u660e\u786e\u5b9a\u4e49\u65f6\uff0c\u4f1a\u7ed9\u89e3\u91ca\u5e26\u6765\u6311\u6218\u3002\u6b64\u5916\uff0c\u8fc7\u7a0b\u5b9a\u4e49\u901a\u5e38\u662f\u4e3b\u89c2\u7684\uff0c\u5e76\u4e14\u4f9d\u8d56\u4e8e\u4e0a\u4e0b\u6587\uff1b\u5b83\u4eec\u5728\u7ec4\u7ec7\u3001\u89d2\u8272\u548c\u5206\u6790\u76ee\u6807\u4e4b\u95f4\u6709\u6240\u4e0d\u540c\uff0c\u5e76\u4e14\u4e0d\u80fd\u603b\u662f\u81ea\u52a8\u53d1\u73b0\u3002", "method": "\u4e00\u79cd\u5c06\u5206\u6790\u5e08\u5b9a\u4e49\u7684\u8fc7\u7a0b\u8303\u56f4\u5d4c\u5165\u5230OCEL\u4e2d\u7684\u65b9\u6cd5", "result": "\u6211\u4eec\u4f7f\u7528\u516c\u5f00\u53ef\u7528\u7684OCEL\u65e5\u5fd7\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u9002\u7528\u6027\uff0c\u5e76\u4e3a\u8303\u56f4\u5b9a\u4e49\u548c\u5206\u6790\u63d0\u4f9b\u4e86\u652f\u6301\u5de5\u5177\u3002", "conclusion": "\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5206\u6790\u5e08\u5b9a\u4e49\u7684\u8fc7\u7a0b\u8303\u56f4\u5d4c\u5165\u5230OCEL\u4e2d\u7684\u65b9\u6cd5\u3002\u8fd9\u652f\u6301\u591a\u4e2a\u5171\u5b58\u8fc7\u7a0b\u7684\u7ed3\u6784\u5316\u8868\u793a\uff0c\u652f\u6301\u8de8\u8303\u56f4\u7684\u4e8b\u4ef6\u6570\u636e\u805a\u5408\uff0c\u5e76\u4fc3\u8fdb\u4e0d\u540c\u62bd\u8c61\u7ea7\u522b\u7684\u5206\u6790\u3002\u6211\u4eec\u4f7f\u7528\u516c\u5f00\u53ef\u7528\u7684OCEL\u65e5\u5fd7\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u9002\u7528\u6027\uff0c\u5e76\u4e3a\u8303\u56f4\u5b9a\u4e49\u548c\u5206\u6790\u63d0\u4f9b\u4e86\u652f\u6301\u5de5\u5177\u3002"}}
{"id": "2508.18430", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18430", "abs": "https://arxiv.org/abs/2508.18430", "authors": ["Aranya Saha", "Tanvir Ahmed Khan", "Ismam Nur Swapnil", "Mohammad Ariful Haque"], "title": "CLARIFY: A Specialist-Generalist Framework for Accurate and Lightweight Dermatological Visual Question Answering", "comment": "10 pages, 8 figures, Prepared for submission to IEEE Transactions on\n  Human-Machine Systems", "summary": "Vision-language models (VLMs) have shown significant potential for medical\ntasks; however, their general-purpose nature can limit specialized diagnostic\naccuracy, and their large size poses substantial inference costs for real-world\nclinical deployment. To address these challenges, we introduce CLARIFY, a\nSpecialist-Generalist framework for dermatological visual question answering\n(VQA). CLARIFY combines two components: (i) a lightweight, domain-trained image\nclassifier (the Specialist) that provides fast and highly accurate diagnostic\npredictions, and (ii) a powerful yet compressed conversational VLM (the\nGeneralist) that generates natural language explanations to user queries. In\nour framework, the Specialist's predictions directly guide the Generalist's\nreasoning, focusing it on the correct diagnostic path. This synergy is further\nenhanced by a knowledge graph-based retrieval module, which grounds the\nGeneralist's responses in factual dermatological knowledge, ensuring both\naccuracy and reliability. This hierarchical design not only reduces diagnostic\nerrors but also significantly improves computational efficiency. Experiments on\nour curated multimodal dermatology dataset demonstrate that CLARIFY achieves an\n18\\% improvement in diagnostic accuracy over the strongest baseline, a\nfine-tuned, uncompressed single-line VLM, while reducing the average VRAM\nrequirement and latency by at least 20\\% and 5\\%, respectively. These results\nindicate that a Specialist-Generalist system provides a practical and powerful\nparadigm for building lightweight, trustworthy, and clinically viable AI\nsystems.", "AI": {"tldr": "CLARIFY, a Specialist-Generalist framework, improves diagnostic accuracy and computational efficiency in dermatological VQA.", "motivation": "General-purpose VLMs can limit specialized diagnostic accuracy, and their large size poses substantial inference costs for real-world clinical deployment.", "method": "CLARIFY combines a lightweight, domain-trained image classifier (the Specialist) and a compressed conversational VLM (the Generalist), enhanced by a knowledge graph-based retrieval module.", "result": "CLARIFY achieves an 18% improvement in diagnostic accuracy, while reducing the average VRAM requirement and latency by at least 20% and 5%, respectively.", "conclusion": "A Specialist-Generalist system provides a practical and powerful paradigm for building lightweight, trustworthy, and clinically viable AI systems."}}
{"id": "2508.18444", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18444", "abs": "https://arxiv.org/abs/2508.18444", "authors": ["Nafis Tanveer Islam", "Zhiming Zhao"], "title": "How Reliable are LLMs for Reasoning on the Re-ranking task?", "comment": "Accepted at FQAS Conference 2024. DOI will be provided in 3 weeks\n  after the conference has published the paper", "summary": "With the improving semantic understanding capability of Large Language Models\n(LLMs), they exhibit a greater awareness and alignment with human values, but\nthis comes at the cost of transparency. Although promising results are achieved\nvia experimental analysis, an in-depth understanding of the LLM's internal\nworkings is unavoidable to comprehend the reasoning behind the re-ranking,\nwhich provides end users with an explanation that enables them to make an\ninformed decision. Moreover, in newly developed systems with limited user\nengagement and insufficient ranking data, accurately re-ranking content remains\na significant challenge. While various training methods affect the training of\nLLMs and generate inference, our analysis has found that some training methods\nexhibit better explainability than others, implying that an accurate semantic\nunderstanding has not been learned through all training methods; instead,\nabstract knowledge has been gained to optimize evaluation, which raises\nquestions about the true reliability of LLMs. Therefore, in this work, we\nanalyze how different training methods affect the semantic understanding of the\nre-ranking task in LLMs and investigate whether these models can generate more\ninformed textual reasoning to overcome the challenges of transparency or LLMs\nand limited training data. To analyze the LLMs for re-ranking tasks, we utilize\na relatively small ranking dataset from the environment and the Earth science\ndomain to re-rank retrieved content. Furthermore, we also analyze the\nexplainable information to see if the re-ranking can be reasoned using\nexplainability.", "AI": {"tldr": "This paper analyzes how different training methods affect the semantic understanding of the re-ranking task in LLMs. The paper utilizes a relatively small ranking dataset from the environment and the Earth science domain to re-rank retrieved content. The results show that some training methods exhibit better explainability than others.", "motivation": "LLMs exhibit a greater awareness and alignment with human values, but this comes at the cost of transparency. Accurately re-ranking content remains a significant challenge in newly developed systems with limited user engagement and insufficient ranking data.", "method": "Utilize a relatively small ranking dataset from the environment and the Earth science domain to re-rank retrieved content and analyze the explainable information.", "result": "Some training methods exhibit better explainability than others, implying that an accurate semantic understanding has not been learned through all training methods.", "conclusion": "Different training methods affect the semantic understanding of the re-ranking task in LLMs. Some training methods exhibit better explainability than others. Abstract knowledge has been gained to optimize evaluation, which raises questions about the true reliability of LLMs."}}
{"id": "2508.18533", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18533", "abs": "https://arxiv.org/abs/2508.18533", "authors": ["Kaijie Xu", "Clark Verbrugge"], "title": "A Database-Driven Framework for 3D Level Generation with LLMs", "comment": null, "summary": "Procedural Content Generation for 3D game levels faces challenges in\nbalancing spatial coherence, navigational functionality, and adaptable gameplay\nprogression across multi-floor environments. This paper introduces a novel\nframework for generating such levels, centered on the offline, LLM-assisted\nconstruction of reusable databases for architectural components (facilities and\nroom templates) and gameplay mechanic elements. Our multi-phase pipeline\nassembles levels by: (1) selecting and arranging instances from the Room\nDatabase to form a multi-floor global structure with an inherent topological\norder; (2) optimizing the internal layout of facilities for each room based on\npredefined constraints from the Facility Database; and (3) integrating\nprogression-based gameplay mechanics by placing components from a Mechanics\nDatabase according to their topological and spatial rules. A subsequent\ntwo-phase repair system ensures navigability. This approach combines modular,\ndatabase-driven design with constraint-based optimization, allowing for\nsystematic control over level structure and the adaptable pacing of gameplay\nelements. Initial experiments validate the framework's ability in generating\ndiverse, navigable 3D environments and its capability to simulate distinct\ngameplay pacing strategies through simple parameterization. This research\nadvances PCG by presenting a scalable, database-centric foundation for the\nautomated generation of complex 3D levels with configurable gameplay\nprogression.", "AI": {"tldr": "This paper presents a LLM-assisted, database-centric framework for generating complex, navigable 3D game levels with configurable gameplay progression.", "motivation": "Procedural Content Generation for 3D game levels faces challenges in balancing spatial coherence, navigational functionality, and adaptable gameplay progression across multi-floor environments.", "method": "This paper introduces a novel framework for generating such levels, centered on the offline, LLM-assisted construction of reusable databases for architectural components (facilities and room templates) and gameplay mechanic elements. Our multi-phase pipeline assembles levels by: (1) selecting and arranging instances from the Room Database to form a multi-floor global structure with an inherent topological order; (2) optimizing the internal layout of facilities for each room based on predefined constraints from the Facility Database; and (3) integrating progression-based gameplay mechanics by placing components from a Mechanics Database according to their topological and spatial rules. A subsequent two-phase repair system ensures navigability. This approach combines modular, database-driven design with constraint-based optimization, allowing for systematic control over level structure and the adaptable pacing of gameplay elements.", "result": "Initial experiments validate the framework's ability in generating diverse, navigable 3D environments and its capability to simulate distinct gameplay pacing strategies through simple parameterization.", "conclusion": "This research advances PCG by presenting a scalable, database-centric foundation for the automated generation of complex 3D levels with configurable gameplay progression."}}
{"id": "2508.18312", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18312", "abs": "https://arxiv.org/abs/2508.18312", "authors": ["Yu Pan", "Zhongze Cai", "Guanting Chen", "Huaiyang Zhong", "Chonghuan Wang"], "title": "What Matters in Data for DPO?", "comment": null, "summary": "Direct Preference Optimization (DPO) has emerged as a simple and effective\napproach for aligning large language models (LLMs) with human preferences,\nbypassing the need for a learned reward model. Despite its growing adoption, a\nfundamental question remains open: what characteristics of preference data are\nmost critical for DPO performance? In this work, we provide a systematic study\nof how preference data distribution influences DPO, from both theoretical and\nempirical perspectives. We show that the quality of chosen responses plays a\ndominant role in optimizing the DPO objective, while the quality of rejected\nresponses may have relatively limited impact. Our theoretical analysis\ncharacterizes the optimal response distribution under DPO and reveals how\ncontrastiveness between responses helps primarily by improving the chosen\nsamples. We further study an online DPO setting and show it effectively reduces\nto supervised fine-tuning on the chosen responses. Extensive experiments across\ndiverse tasks confirm our findings: improving the quality of chosen responses\nconsistently boosts performance regardless of the quality of the rejected\nresponses. We also investigate the benefit of mixing the on-policy data. Our\nresults interpret the mechanism behind some widely adopted strategies and offer\npractical insights for constructing high-impact preference datasets for LLM\nalignment.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u504f\u597d\u6570\u636e\u5206\u5e03\u5bf9 DPO \u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9009\u62e9\u54cd\u5e94\u7684\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u800c\u62d2\u7edd\u54cd\u5e94\u7684\u8d28\u91cf\u5f71\u54cd\u6709\u9650\u3002", "motivation": "\u76f4\u63a5\u504f\u597d\u4f18\u5316 (DPO) \u5df2\u7ecf\u6210\u4e3a\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4e0e\u4eba\u7c7b\u504f\u597d\u4fdd\u6301\u4e00\u81f4\uff0c\u800c\u65e0\u9700\u5b66\u4e60\u5956\u52b1\u6a21\u578b\u3002\u5c3d\u7ba1 DPO \u8d8a\u6765\u8d8a\u53d7\u6b22\u8fce\uff0c\u4f46\u4e00\u4e2a\u6839\u672c\u95ee\u9898\u4ecd\u7136\u60ac\u800c\u672a\u51b3\uff1a\u504f\u597d\u6570\u636e\u7684\u54ea\u4e9b\u7279\u5f81\u5bf9\u4e8e DPO \u6027\u80fd\u81f3\u5173\u91cd\u8981\uff1f", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u7684\u89d2\u5ea6\uff0c\u7cfb\u7edf\u5730\u7814\u7a76\u504f\u597d\u6570\u636e\u5206\u5e03\u5982\u4f55\u5f71\u54cd DPO\u3002", "result": "\u9009\u62e9\u54cd\u5e94\u7684\u8d28\u91cf\u5728\u4f18\u5316 DPO \u76ee\u6807\u4e2d\u8d77\u4e3b\u5bfc\u4f5c\u7528\uff0c\u800c\u62d2\u7edd\u54cd\u5e94\u7684\u8d28\u91cf\u53ef\u80fd\u5f71\u54cd\u76f8\u5bf9\u6709\u9650\u3002\u5bf9\u6bd4\u6709\u52a9\u4e8e\u4e3b\u8981\u901a\u8fc7\u6539\u8fdb\u9009\u62e9\u7684\u6837\u672c\u3002\u5728\u7ebf DPO \u8bbe\u7f6e\u6709\u6548\u5730\u51cf\u5c11\u5230\u5bf9\u9009\u62e9\u7684\u54cd\u5e94\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u3002", "conclusion": "\u63d0\u9ad8\u9009\u62e9\u54cd\u5e94\u7684\u8d28\u91cf\u59cb\u7ec8\u80fd\u63d0\u9ad8\u6027\u80fd\uff0c\u800c\u4e0e\u62d2\u7edd\u54cd\u5e94\u7684\u8d28\u91cf\u65e0\u5173\u3002\u6df7\u5408 on-policy \u6570\u636e\u662f\u6709\u76ca\u7684\u3002"}}
{"id": "2508.18445", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18445", "abs": "https://arxiv.org/abs/2508.18445", "authors": ["Sizhuo Ma", "Wei-Ting Chen", "Qiang Gao", "Jian Wang", "Chris Wei Zhou", "Wei Sun", "Weixia Zhang", "Linhan Cao", "Jun Jia", "Xiangyang Zhu", "Dandan Zhu", "Xiongkuo Min", "Guangtao Zhai", "Baoying Chen", "Xiongwei Xiao", "Jishen Zeng", "Wei Wu", "Tiexuan Lou", "Yuchen Tan", "Chunyi Song", "Zhiwei Xu", "MohammadAli Hamidi", "Hadi Amirpour", "Mingyin Bai", "Jiawang Du", "Zhenyu Jiang", "Zilong Lu", "Ziguan Cui", "Zongliang Gan", "Xinpeng Li", "Shiqi Jiang", "Chenhui Li", "Changbo Wang", "Weijun Yuan", "Zhan Li", "Yihang Chen", "Yifan Deng", "Ruting Deng", "Zhanglu Chen", "Boyang Yao", "Shuling Zheng", "Feng Zhang", "Zhiheng Fu", "Abhishek Joshi", "Aman Agarwal", "Rakhil Immidisetti", "Ajay Narasimha Mopidevi", "Vishwajeet Shukla", "Hao Yang", "Ruikun Zhang", "Liyuan Pan", "Kaixin Deng", "Hang Ouyang", "Fan yang", "Zhizun Luo", "Zhuohang Shi", "Songning Lai", "Weilin Ruan", "Yutao Yue"], "title": "VQualA 2025 Challenge on Face Image Quality Assessment: Methods and Results", "comment": "ICCV 2025 VQualA workshop FIQA track", "summary": "Face images play a crucial role in numerous applications; however, real-world\nconditions frequently introduce degradations such as noise, blur, and\ncompression artifacts, affecting overall image quality and hindering subsequent\ntasks. To address this challenge, we organized the VQualA 2025 Challenge on\nFace Image Quality Assessment (FIQA) as part of the ICCV 2025 Workshops.\nParticipants created lightweight and efficient models (limited to 0.5 GFLOPs\nand 5 million parameters) for the prediction of Mean Opinion Scores (MOS) on\nface images with arbitrary resolutions and realistic degradations. Submissions\nunderwent comprehensive evaluations through correlation metrics on a dataset of\nin-the-wild face images. This challenge attracted 127 participants, with 1519\nfinal submissions. This report summarizes the methodologies and findings for\nadvancing the development of practical FIQA approaches.", "AI": {"tldr": "Organized a face image quality assessment challenge (VQualA 2025) at ICCV 2025, attracting 127 participants to develop lightweight models for predicting MOS on degraded face images. The challenge evaluated submissions using correlation metrics on a dataset of in-the-wild face images and this report summarizes the methodologies and findings.", "motivation": "Real-world conditions frequently introduce degradations such as noise, blur, and compression artifacts, affecting overall image quality and hindering subsequent tasks.", "method": "Participants created lightweight and efficient models (limited to 0.5 GFLOPs and 5 million parameters) for the prediction of Mean Opinion Scores (MOS) on face images with arbitrary resolutions and realistic degradations.", "result": "This challenge attracted 127 participants, with 1519 final submissions. Submissions underwent comprehensive evaluations through correlation metrics on a dataset of in-the-wild face images.", "conclusion": "This report summarizes the methodologies and findings for advancing the development of practical FIQA approaches."}}
{"id": "2508.18466", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18466", "abs": "https://arxiv.org/abs/2508.18466", "authors": ["Alina Wr\u00f3blewska", "Bartosz \u017buk"], "title": "Integrating gender inclusivity into large language models via instruction tuning", "comment": null, "summary": "Imagine a language with masculine, feminine, and neuter grammatical genders,\nyet, due to historical and political conventions, masculine forms are\npredominantly used to refer to men, women and mixed-gender groups. This is the\nreality of contemporary Polish. A social consequence of this unfair linguistic\nsystem is that large language models (LLMs) trained on Polish texts inherit and\nreinforce this masculine bias, generating gender-imbalanced outputs. This study\naddresses this issue by tuning LLMs using the IPIS dataset, a collection of\nhuman-crafted gender-inclusive proofreading in Polish and Polish-to-English\ntranslation instructions. Grounded in a theoretical linguistic framework, we\ndesign a system prompt with explicit gender-inclusive guidelines for Polish. In\nour experiments, we IPIS-tune multilingual LLMs (Llama-8B, Mistral-7B and\nMistral-Nemo) and Polish-specific LLMs (Bielik and PLLuM). Our approach aims to\nintegrate gender inclusivity as an inherent feature of these models, offering a\nsystematic solution to mitigate gender bias in Polish language generation.", "AI": {"tldr": "This study tunes LLMs to reduce masculine bias in Polish language generation using a gender-inclusive dataset and system prompt.", "motivation": "Masculine bias in Polish LLMs due to the language's grammatical gender system and historical conventions leads to gender-imbalanced outputs.", "method": "Tuning multilingual LLMs (Llama-8B, Mistral-7B and Mistral-Nemo) and Polish-specific LLMs (Bielik and PLLuM) using the IPIS dataset and a system prompt.", "result": "The approach integrates gender inclusivity as an inherent feature of the tuned models.", "conclusion": "This study aims to mitigate gender bias in Polish language generation by tuning LLMs with the IPIS dataset and a system prompt with gender-inclusive guidelines."}}
{"id": "2508.18554", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18554", "abs": "https://arxiv.org/abs/2508.18554", "authors": ["Lily Jiaxin Wan", "Chia-Tung Ho", "Rongjian Liang", "Cunxi Yu", "Deming Chen", "Haoxing Ren"], "title": "SchemaCoder: Automatic Log Schema Extraction Coder with Residual Q-Tree Boosting", "comment": "18 pages, 16 figures, under review for AAAI2026", "summary": "Log schema extraction is the process of deriving human-readable templates\nfrom massive volumes of log data, which is essential yet notoriously\nlabor-intensive. Recent studies have attempted to streamline this task by\nleveraging Large Language Models (LLMs) for automated schema extraction.\nHowever, existing methods invariably rely on predefined regular expressions,\nnecessitating human domain expertise and severely limiting productivity gains.\nTo fundamentally address this limitation, we introduce SchemaCoder, the first\nfully automated schema extraction framework applicable to a wide range of log\nfile formats without requiring human customization within the flow. At its\ncore, SchemaCoder features a novel Residual Question-Tree (Q-Tree) Boosting\nmechanism that iteratively refines schema extraction through targeted, adaptive\nqueries driven by LLMs. Particularly, our method partitions logs into semantic\nchunks via context-bounded segmentation, selects representative patterns using\nembedding-based sampling, and generates schema code through hierarchical\nQ-Tree-driven LLM queries, iteratively refined by our textual-residual\nevolutionary optimizer and residual boosting. Experimental validation\ndemonstrates SchemaCoder's superiority on the widely-used LogHub-2.0 benchmark,\nachieving an average improvement of 21.3% over state-of-the-arts.", "AI": {"tldr": "SchemaCoder\u662f\u7b2c\u4e00\u4e2a\u5168\u81ea\u52a8\u6a21\u5f0f\u63d0\u53d6\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u65e5\u5fd7\u6587\u4ef6\u683c\u5f0f\uff0c\u65e0\u9700\u4eba\u5de5\u5b9a\u5236\uff0c\u901a\u8fc7LLM\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u67e5\u8be2\u548c\u6b8b\u5dee\u63d0\u5347\u8fed\u4ee3\u5730\u6539\u8fdb\u6a21\u5f0f\u63d0\u53d6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u7684\u6b63\u5219\u8868\u8fbe\u5f0f\uff0c\u9700\u8981\u4eba\u7c7b\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u751f\u4ea7\u529b\u63d0\u5347\u3002", "method": "SchemaCoder\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6b8b\u5dee\u95ee\u9898\u6811\uff08Q-Tree\uff09\u63d0\u5347\u673a\u5236\uff0c\u901a\u8fc7LLM\u9a71\u52a8\u7684\u6709\u9488\u5bf9\u6027\u7684\u81ea\u9002\u5e94\u67e5\u8be2\u8fed\u4ee3\u5730\u6539\u8fdb\u6a21\u5f0f\u63d0\u53d6\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e0a\u4e0b\u6587\u754c\u5b9a\u7684\u5206\u5272\u5c06\u65e5\u5fd7\u5212\u5206\u4e3a\u8bed\u4e49\u5757\uff0c\u4f7f\u7528\u57fa\u4e8e\u5d4c\u5165\u7684\u91c7\u6837\u9009\u62e9\u4ee3\u8868\u6027\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u5206\u5c42Q-Tree\u9a71\u52a8\u7684LLM\u67e5\u8be2\u751f\u6210\u6a21\u5f0f\u4ee3\u7801\uff0c\u5e76\u901a\u8fc7\u6587\u672c\u6b8b\u5dee\u8fdb\u5316\u4f18\u5316\u5668\u548c\u6b8b\u5dee\u63d0\u5347\u8fdb\u884c\u8fed\u4ee3\u6539\u8fdb\u3002", "result": "SchemaCoder\u5728LogHub-2.0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6bd4\u73b0\u6709\u6280\u672f\u5e73\u5747\u63d0\u9ad8\u4e8621.3%\u3002", "conclusion": "SchemaCoder\u5728LogHub-2.0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e73\u5747\u63d0\u534721.3%\u3002"}}
{"id": "2508.18313", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18313", "abs": "https://arxiv.org/abs/2508.18313", "authors": ["Zi Cai", "Yu Liu", "Zhiyao Luo", "Tingting Zhu"], "title": "ProtoEHR: Hierarchical Prototype Learning for EHR-based Healthcare Predictions", "comment": "CIKM 2025 Full Paper", "summary": "Digital healthcare systems have enabled the collection of mass healthcare\ndata in electronic healthcare records (EHRs), allowing artificial intelligence\nsolutions for various healthcare prediction tasks. However, existing studies\noften focus on isolated components of EHR data, limiting their predictive\nperformance and interpretability. To address this gap, we propose ProtoEHR, an\ninterpretable hierarchical prototype learning framework that fully exploits the\nrich, multi-level structure of EHR data to enhance healthcare predictions. More\nspecifically, ProtoEHR models relationships within and across three\nhierarchical levels of EHRs: medical codes, hospital visits, and patients. We\nfirst leverage large language models to extract semantic relationships among\nmedical codes and construct a medical knowledge graph as the knowledge source.\nBuilding on this, we design a hierarchical representation learning framework\nthat captures contextualized representations across three levels, while\nincorporating prototype information within each level to capture intrinsic\nsimilarities and improve generalization. To perform a comprehensive assessment,\nwe evaluate ProtoEHR in two public datasets on five clinically significant\ntasks, including prediction of mortality, prediction of readmission, prediction\nof length of stay, drug recommendation, and prediction of phenotype. The\nresults demonstrate the ability of ProtoEHR to make accurate, robust, and\ninterpretable predictions compared to baselines in the literature. Furthermore,\nProtoEHR offers interpretable insights on code, visit, and patient levels to\naid in healthcare prediction.", "AI": {"tldr": "ProtoEHR\u662f\u4e00\u4e2a\u5229\u7528EHR\u6570\u636e\u7684\u591a\u5c42\u6b21\u7ed3\u6784\u8fdb\u884c\u533b\u7597\u9884\u6d4b\u7684\u53ef\u89e3\u91ca\u5206\u5c42\u539f\u578b\u5b66\u4e60\u6846\u67b6\uff0c\u5e76\u5728\u591a\u4e2a\u4e34\u5e8a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u4fa7\u91cd\u4e8eEHR\u6570\u636e\u7684\u5b64\u7acb\u7ec4\u6210\u90e8\u5206\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faProtoEHR\uff0c\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u5206\u5c42\u539f\u578b\u5b66\u4e60\u6846\u67b6\uff0c\u5145\u5206\u5229\u7528EHR\u6570\u636e\u7684\u591a\u5c42\u6b21\u7ed3\u6784\u6765\u589e\u5f3a\u533b\u7597\u9884\u6d4b\u3002\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u533b\u5b66\u4ee3\u7801\u4e4b\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\uff0c\u6784\u5efa\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u3002\u8bbe\u8ba1\u5206\u5c42\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u6355\u83b7\u8de8\u4e09\u4e2a\u5c42\u6b21\u7684\u4e0a\u4e0b\u6587\u8868\u793a\uff0c\u540c\u65f6\u5728\u6bcf\u4e2a\u5c42\u6b21\u4e2d\u52a0\u5165\u539f\u578b\u4fe1\u606f\u3002", "result": "ProtoEHR\u5728\u4e24\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\uff0c\u9488\u5bf9\u4e94\u4e2a\u4e34\u5e8a\u4efb\u52a1\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660eProtoEHR\u80fd\u591f\u505a\u51fa\u51c6\u786e\u3001\u7a33\u5065\u548c\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\uff0c\u4f18\u4e8e\u6587\u732e\u4e2d\u7684\u57fa\u7ebf\u3002", "conclusion": "ProtoEHR\u5728\u4e94\u4e2a\u4e34\u5e8a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u4f9b\u51c6\u786e\u3001\u7a33\u5065\u548c\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\uff0c\u5e76\u63d0\u4f9b\u4ee3\u7801\u3001\u8bbf\u95ee\u548c\u60a3\u8005\u5c42\u9762\u7684\u53ef\u89e3\u91ca\u89c1\u89e3\u3002"}}
{"id": "2508.18463", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18463", "abs": "https://arxiv.org/abs/2508.18463", "authors": ["Md. Rashid Shahriar Khan", "Md. Abrar Hasan", "Mohammod Tareq Aziz Justice"], "title": "Context-Aware Zero-Shot Anomaly Detection in Surveillance Using Contrastive and Predictive Spatiotemporal Modeling", "comment": "11 pages, 7 figures, 4 tables", "summary": "Detecting anomalies in surveillance footage is inherently challenging due to\ntheir unpredictable and context-dependent nature. This work introduces a novel\ncontext-aware zero-shot anomaly detection framework that identifies abnormal\nevents without exposure to anomaly examples during training. The proposed\nhybrid architecture combines TimeSformer, DPC, and CLIP to model spatiotemporal\ndynamics and semantic context. TimeSformer serves as the vision backbone to\nextract rich spatial-temporal features, while DPC forecasts future\nrepresentations to identify temporal deviations. Furthermore, a CLIP-based\nsemantic stream enables concept-level anomaly detection through\ncontext-specific text prompts. These components are jointly trained using\nInfoNCE and CPC losses, aligning visual inputs with their temporal and semantic\nrepresentations. A context-gating mechanism further enhances decision-making by\nmodulating predictions with scene-aware cues or global video features. By\nintegrating predictive modeling with vision-language understanding, the system\ncan generalize to previously unseen behaviors in complex environments. This\nframework bridges the gap between temporal reasoning and semantic context in\nzero-shot anomaly detection for surveillance. The code for this research has\nbeen made available at\nhttps://github.com/NK-II/Context-Aware-ZeroShot-Anomaly-Detection-in-Surveillance.", "AI": {"tldr": "A context-aware zero-shot anomaly detection framework identifies abnormal events without anomaly examples during training by combining TimeSformer, DPC, and CLIP to model spatiotemporal dynamics and semantic context.", "motivation": "Detecting anomalies in surveillance footage is inherently challenging due to their unpredictable and context-dependent nature.", "method": "The proposed hybrid architecture combines TimeSformer, DPC, and CLIP to model spatiotemporal dynamics and semantic context.", "result": "Integrating predictive modeling with vision-language understanding, the system can generalize to previously unseen behaviors in complex environments.", "conclusion": "This framework bridges the gap between temporal reasoning and semantic context in zero-shot anomaly detection for surveillance."}}
{"id": "2508.18473", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18473", "abs": "https://arxiv.org/abs/2508.18473", "authors": ["Jiawei Li", "Akshayaa Magesh", "Venugopal V. Veeravalli"], "title": "Principled Detection of Hallucinations in Large Language Models via Multiple Testing", "comment": "16 pages", "summary": "While Large Language Models (LLMs) have emerged as powerful foundational\nmodels to solve a variety of tasks, they have also been shown to be prone to\nhallucinations, i.e., generating responses that sound confident but are\nactually incorrect or even nonsensical. In this work, we formulate the problem\nof detecting hallucinations as a hypothesis testing problem and draw parallels\nto the problem of out-of-distribution detection in machine learning models. We\npropose a multiple-testing-inspired method to solve the hallucination detection\nproblem, and provide extensive experimental results to validate the robustness\nof our approach against state-of-the-art methods.", "AI": {"tldr": "This paper addresses the problem of hallucination in large language models, viewing it as a hypothesis testing problem. A multiple-testing-inspired method is proposed and validated against state-of-the-art methods.", "motivation": "Large Language Models (LLMs) have emerged as powerful foundational models to solve a variety of tasks, they have also been shown to be prone to hallucinations, i.e., generating responses that sound confident but are actually incorrect or even nonsensical. In this work, we formulate the problem of detecting hallucinations as a hypothesis testing problem and draw parallels to the problem of out-of-distribution detection in machine learning models.", "method": "We propose a multiple-testing-inspired method to solve the hallucination detection problem", "result": "a multiple-testing-inspired method to solve the hallucination detection problem", "conclusion": "We provide extensive experimental results to validate the robustness of our approach against state-of-the-art methods."}}
{"id": "2508.18608", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18608", "abs": "https://arxiv.org/abs/2508.18608", "authors": ["Janet Wang", "Xin Hu", "Yunbei Zhang", "Diabate Almamy", "Vagamon Bamba", "Konan Amos S\u00e9bastien Koffi", "Yao Koffi Aubin", "Zhengming Ding", "Jihun Hamm", "Rie R. Yotsu"], "title": "eSkinHealth: A Multimodal Dataset for Neglected Tropical Skin Diseases", "comment": null, "summary": "Skin Neglected Tropical Diseases (NTDs) impose severe health and\nsocioeconomic burdens in impoverished tropical communities. Yet, advancements\nin AI-driven diagnostic support are hindered by data scarcity, particularly for\nunderrepresented populations and rare manifestations of NTDs. Existing\ndermatological datasets often lack the demographic and disease spectrum crucial\nfor developing reliable recognition models of NTDs. To address this, we\nintroduce eSkinHealth, a novel dermatological dataset collected on-site in\nC\\^ote d'Ivoire and Ghana. Specifically, eSkinHealth contains 5,623 images from\n1,639 cases and encompasses 47 skin diseases, focusing uniquely on skin NTDs\nand rare conditions among West African populations. We further propose an\nAI-expert collaboration paradigm to implement foundation language and\nsegmentation models for efficient generation of multimodal annotations, under\ndermatologists' guidance. In addition to patient metadata and diagnosis labels,\neSkinHealth also includes semantic lesion masks, instance-specific visual\ncaptions, and clinical concepts. Overall, our work provides a valuable new\nresource and a scalable annotation framework, aiming to catalyze the\ndevelopment of more equitable, accurate, and interpretable AI tools for global\ndermatology.", "AI": {"tldr": "Introduces eSkinHealth, a novel dermatological dataset collected in C\\{o}te d'Ivoire and Ghana, and proposes an AI-expert collaboration paradigm for efficient generation of multimodal annotations, aiming to catalyze the development of more equitable, accurate, and interpretable AI tools for global dermatology.", "motivation": "Advancements in AI-driven diagnostic support are hindered by data scarcity, particularly for underrepresented populations and rare manifestations of NTDs. Existing dermatological datasets often lack the demographic and disease spectrum crucial for developing reliable recognition models of NTDs.", "method": "collection of a new dermatological dataset (eSkinHealth) collected on-site in C\\{o}te d'Ivoire and Ghana, and an AI-expert collaboration paradigm to implement foundation language and segmentation models for efficient generation of multimodal annotations, under dermatologists' guidance.", "result": "eSkinHealth contains 5,623 images from 1,639 cases and encompasses 47 skin diseases, focusing uniquely on skin NTDs and rare conditions among West African populations. eSkinHealth also includes semantic lesion masks, instance-specific visual captions, and clinical concepts.", "conclusion": "This work provides a valuable new resource (eSkinHealth dataset) and a scalable annotation framework, aiming to catalyze the development of more equitable, accurate, and interpretable AI tools for global dermatology."}}
{"id": "2508.18316", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.18316", "abs": "https://arxiv.org/abs/2508.18316", "authors": ["Rodrigo Tertulino"], "title": "Evaluating Federated Learning for At-Risk Student Prediction: A Comparative Analysis of Model Complexity and Data Balancing", "comment": "This article has been prepared to be submitted to the Holos Journal\n  in Brazil", "summary": "High dropout and failure rates in distance education pose a significant\nchallenge for academic institutions, making the proactive identification of\nat-risk students crucial for providing timely support. This study develops and\nevaluates a machine learning model based on early academic performance and\ndigital engagement patterns from the large-scale OULAD dataset to predict\nstudent risk at a UK university. To address the practical challenges of data\nprivacy and institutional silos that often hinder such initiatives, we\nimplement the model using a Federated Learning (FL) framework. We compare model\ncomplexity (Logistic Regression vs. a Deep Neural Network) and data balancing.\nThe final federated model demonstrates strong predictive capability, achieving\nan ROC AUC score of approximately 85% in identifying at-risk students. Our\nfindings show that this federated approach provides a practical and scalable\nsolution for institutions to build effective early-warning systems, enabling\nproactive student support while inherently respecting data privacy.", "AI": {"tldr": "A federated learning model predicts at-risk students in distance education with 85% accuracy, addressing data privacy concerns.", "motivation": "High dropout and failure rates in distance education necessitate proactive identification of at-risk students for timely support.", "method": "Development and evaluation of a machine learning model based on early academic performance and digital engagement patterns from the large-scale OULAD dataset, implemented using a Federated Learning (FL) framework.", "result": "The federated model achieves an ROC AUC score of approximately 85% in identifying at-risk students.", "conclusion": "This federated learning approach offers a practical and scalable solution for institutions to build effective early-warning systems, enabling proactive student support while respecting data privacy."}}
{"id": "2508.18506", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18506", "abs": "https://arxiv.org/abs/2508.18506", "authors": ["Ajinkya Khoche", "Qingwen Zhang", "Yixi Cai", "Sina Sharif Mansouri", "Patric Jensfelt"], "title": "DoGFlow: Self-Supervised LiDAR Scene Flow via Cross-Modal Doppler Guidance", "comment": "Under Review", "summary": "Accurate 3D scene flow estimation is critical for autonomous systems to\nnavigate dynamic environments safely, but creating the necessary large-scale,\nmanually annotated datasets remains a significant bottleneck for developing\nrobust perception models. Current self-supervised methods struggle to match the\nperformance of fully supervised approaches, especially in challenging\nlong-range and adverse weather scenarios, while supervised methods are not\nscalable due to their reliance on expensive human labeling. We introduce\nDoGFlow, a novel self-supervised framework that recovers full 3D object motions\nfor LiDAR scene flow estimation without requiring any manual ground truth\nannotations. This paper presents our cross-modal label transfer approach, where\nDoGFlow computes motion pseudo-labels in real-time directly from 4D radar\nDoppler measurements and transfers them to the LiDAR domain using dynamic-aware\nassociation and ambiguity-resolved propagation. On the challenging MAN\nTruckScenes dataset, DoGFlow substantially outperforms existing self-supervised\nmethods and improves label efficiency by enabling LiDAR backbones to achieve\nover 90% of fully supervised performance with only 10% of the ground truth\ndata. For more details, please visit https://ajinkyakhoche.github.io/DogFlow/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u76d1\u7763\u6846\u67b6DoGFlow\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u6062\u590d\u5b8c\u6574\u76843D\u5bf9\u8c61\u8fd0\u52a8\uff0c\u4ee5\u8fdb\u884cLiDAR\u573a\u666f\u6d41\u4f30\u8ba1\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u624b\u52a8ground truth\u6ce8\u91ca\u3002", "motivation": "\u7cbe\u786e\u76843D\u573a\u666f\u6d41\u4f30\u8ba1\u5bf9\u4e8e\u81ea\u4e3b\u7cfb\u7edf\u5b89\u5168\u5730\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5bfc\u822a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u662f\u521b\u5efa\u5fc5\u8981\u7684\u5927\u89c4\u6a21\u624b\u52a8\u6ce8\u91ca\u6570\u636e\u96c6\u4ecd\u7136\u662f\u5f00\u53d1\u9c81\u68d2\u611f\u77e5\u6a21\u578b\u7684\u91cd\u5927\u74f6\u9888\u3002\u5f53\u524d\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\u96be\u4ee5\u4e0e\u5b8c\u5168\u76d1\u7763\u65b9\u6cd5\u7684\u6027\u80fd\u76f8\u5339\u914d\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u8fdc\u8ddd\u79bb\u548c\u4e0d\u5229\u7684\u5929\u6c14\u60c5\u51b5\u4e0b\uff0c\u800c\u76d1\u7763\u65b9\u6cd5\u7531\u4e8e\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u800c\u65e0\u6cd5\u6269\u5c55\u3002", "method": "cross-modal\u6807\u7b7e\u8f6c\u6362\u65b9\u6cd5\uff0cDoGFlow\u76f4\u63a5\u4ece4D\u96f7\u8fbe\u591a\u666e\u52d2\u6d4b\u91cf\u4e2d\u5b9e\u65f6\u8ba1\u7b97\u8fd0\u52a8\u4f2a\u6807\u7b7e\uff0c\u5e76\u4f7f\u7528\u52a8\u6001\u611f\u77e5\u5173\u8054\u548c\u6a21\u7cca\u89e3\u6790\u4f20\u64ad\u5c06\u5176\u4f20\u8f93\u5230LiDAR\u57df\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684MAN TruckScenes\u6570\u636e\u96c6\u4e0a\uff0c", "conclusion": "DoGFlow\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u652f\u6301LiDAR\u4e3b\u5e72\u7f51\u7edc\u4ec5\u752810%\u7684ground truth\u6570\u636e\u5b9e\u73b0\u8d85\u8fc790%\u7684\u5b8c\u5168\u76d1\u7763\u6027\u80fd\uff0c\u4ece\u800c\u63d0\u9ad8\u6807\u7b7e\u6548\u7387\u3002"}}
{"id": "2508.18549", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.18549", "abs": "https://arxiv.org/abs/2508.18549", "authors": ["Maike Z\u00fcfle", "Vil\u00e9m Zouhar", "Tu Anh Dinh", "Felipe Maia Polo", "Jan Niehues", "Mrinmaya Sachan"], "title": "COMET-poly: Machine Translation Metric Grounded in Other Candidates", "comment": "Maike Z\\\"ufle, Vil\\'em Zouhar, and Tu Anh Dinh contributed equally", "summary": "Automated metrics for machine translation attempt to replicate human\njudgment. Unlike humans, who often assess a translation in the context of\nmultiple alternatives, these metrics typically consider only the source\nsentence and a single translation. This discrepancy in the evaluation setup may\nnegatively impact the performance of automated metrics. We propose two\nautomated metrics that incorporate additional information beyond the single\ntranslation. COMET-polycand uses alternative translations of the same source\nsentence to compare and contrast with the translation at hand, thereby\nproviding a more informed assessment of its quality. COMET-polyic, inspired by\nretrieval-based in-context learning, takes in translations of similar source\ntexts along with their human-labeled quality scores to guide the evaluation. We\nfind that including a single additional translation in COMET-polycand improves\nthe segment-level metric performance (0.079 to 0.118 Kendall's tau-b\ncorrelation), with further gains when more translations are added.\nIncorporating retrieved examples in COMET-polyic yields similar improvements\n(0.079 to 0.116 Kendall's tau-b correlation). We release our models publicly.", "AI": {"tldr": "The paper introduces two new automated metrics, COMET-polycand and COMET-polyic, that improve machine translation evaluation by incorporating additional information beyond a single translation.", "motivation": "Automated metrics for machine translation typically consider only the source sentence and a single translation, which may negatively impact the performance of automated metrics.", "method": "Two automated metrics: COMET-polycand and COMET-polyic.", "result": "Including a single additional translation in COMET-polycand improves the segment-level metric performance (0.079 to 0.118 Kendall's tau-b correlation), with further gains when more translations are added. Incorporating retrieved examples in COMET-polyic yields similar improvements (0.079 to 0.116 Kendall's tau-b correlation).", "conclusion": "Including additional translation or retrieved examples improves the segment-level metric performance."}}
{"id": "2508.18642", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18642", "abs": "https://arxiv.org/abs/2508.18642", "authors": ["Jianxing Liao", "Tian Zhang", "Xiao Feng", "Yusong Zhang", "Rui Yang", "Haorui Wang", "Bosi Wen", "Ziying Wang", "Runzhi Shi"], "title": "RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing", "comment": null, "summary": "Large language models are extensively utilized in creative writing\napplications. Creative writing requires a balance between subjective writing\nquality (e.g., literariness and emotional expression) and objective constraint\nfollowing (e.g., format requirements and word limits). Existing reinforcement\nlearning methods struggle to balance these two aspects: single reward\nstrategies fail to improve both abilities simultaneously, while fixed-weight\nmixed-reward methods lack the ability to adapt to different writing scenarios.\nTo address this problem, we propose Reinforcement Learning with Mixed Rewards\n(RLMR), utilizing a dynamically mixed reward system from a writing reward model\nevaluating subjective writing quality and a constraint verification model\nassessing objective constraint following. The constraint following reward\nweight is adjusted dynamically according to the writing quality within sampled\ngroups, ensuring that samples violating constraints get negative advantage in\nGRPO and thus penalized during training, which is the key innovation of this\nproposed method. We conduct automated and manual evaluations across diverse\nmodel families from 8B to 72B parameters. Additionally, we construct a\nreal-world writing benchmark named WriteEval for comprehensive evaluation.\nResults illustrate that our method achieves consistent improvements in both\ninstruction following (IFEval from 83.36\\% to 86.65\\%) and writing quality\n(72.75\\% win rate in manual expert pairwise evaluations on WriteEval). To the\nbest of our knowledge, RLMR is the first work to combine subjective preferences\nwith objective verification in online RL training, providing an effective\nsolution for multi-dimensional creative writing optimization.", "AI": {"tldr": "RLMR dynamically balances subjective writing quality and objective constraint following in creative writing by using a dynamically mixed reward system.", "motivation": "Existing reinforcement learning methods struggle to balance subjective writing quality and objective constraint following: single reward strategies fail to improve both abilities simultaneously, while fixed-weight mixed-reward methods lack the ability to adapt to different writing scenarios.", "method": "Reinforcement Learning with Mixed Rewards (RLMR), utilizing a dynamically mixed reward system from a writing reward model evaluating subjective writing quality and a constraint verification model assessing objective constraint following. The constraint following reward weight is adjusted dynamically according to the writing quality within sampled groups, ensuring that samples violating constraints get negative advantage in GRPO and thus penalized during training", "result": "achieves consistent improvements in both instruction following (IFEval from 83.36% to 86.65%) and writing quality (72.75% win rate in manual expert pairwise evaluations on WriteEval)", "conclusion": "RLMR is the first work to combine subjective preferences with objective verification in online RL training, providing an effective solution for multi-dimensional creative writing optimization."}}
{"id": "2508.18318", "categories": ["cs.LG", "cs.CR", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.18318", "abs": "https://arxiv.org/abs/2508.18318", "authors": ["Yang Li", "Hanjie Wang", "Yuanzheng Li", "Jiazheng Li", "Zhaoyang Dong"], "title": "ZTFed-MAS2S: A Zero-Trust Federated Learning Framework with Verifiable Privacy and Trust-Aware Aggregation for Wind Power Data Imputation", "comment": "Accepted by IEEE Transactions on Industrial Informatics, 11 pages, 6\n  figures", "summary": "Wind power data often suffers from missing values due to sensor faults and\nunstable transmission at edge sites. While federated learning enables\nprivacy-preserving collaboration without sharing raw data, it remains\nvulnerable to anomalous updates and privacy leakage during parameter exchange.\nThese challenges are amplified in open industrial environments, necessitating\nzero-trust mechanisms where no participant is inherently trusted. To address\nthese challenges, this work proposes ZTFed-MAS2S, a zero-trust federated\nlearning framework that integrates a multi-head attention-based\nsequence-to-sequence imputation model. ZTFed integrates verifiable differential\nprivacy with non-interactive zero-knowledge proofs and a confidentiality and\nintegrity verification mechanism to ensure verifiable privacy preservation and\nsecure model parameters transmission. A dynamic trust-aware aggregation\nmechanism is employed, where trust is propagated over similarity graphs to\nenhance robustness, and communication overhead is reduced via sparsity- and\nquantization-based compression. MAS2S captures long-term dependencies in wind\npower data for accurate imputation. Extensive experiments on real-world wind\nfarm datasets validate the superiority of ZTFed-MAS2S in both federated\nlearning performance and missing data imputation, demonstrating its\neffectiveness as a secure and efficient solution for practical applications in\nthe energy sector.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u4fe1\u4efb\u8054\u90a6\u5b66\u4e60\u6846\u67b6 ZTFed-MAS2S\uff0c\u7528\u4e8e\u89e3\u51b3\u98ce\u7535\u6570\u636e\u4e2d\u7684\u7f3a\u5931\u503c\u95ee\u9898\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u3001\u53ef\u9a8c\u8bc1\u5dee\u5206\u9690\u79c1\u3001\u96f6\u77e5\u8bc6\u8bc1\u660e\u548c\u52a8\u6001\u4fe1\u4efb\u611f\u77e5\u805a\u5408\u7b49\u6280\u672f\uff0c\u4ee5\u786e\u4fdd\u9690\u79c1\u3001\u5b89\u5168\u548c\u6548\u7387\u3002", "motivation": "\u98ce\u7535\u6570\u636e\u7ecf\u5e38\u56e0\u4f20\u611f\u5668\u6545\u969c\u548c\u8fb9\u7f18\u7ad9\u70b9\u7684\u4e0d\u7a33\u5b9a\u4f20\u8f93\u800c\u906d\u53d7\u7f3a\u5931\u503c\u7684\u5f71\u54cd\u3002\u867d\u7136\u8054\u90a6\u5b66\u4e60\u80fd\u591f\u5728\u4e0d\u5171\u4eab\u539f\u59cb\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4fdd\u62a4\u9690\u79c1\u7684\u534f\u4f5c\uff0c\u4f46\u5b83\u5728\u53c2\u6570\u4ea4\u6362\u8fc7\u7a0b\u4e2d\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u5f02\u5e38\u66f4\u65b0\u548c\u9690\u79c1\u6cc4\u9732\u7684\u5f71\u54cd\u3002\u8fd9\u4e9b\u6311\u6218\u5728\u5f00\u653e\u7684\u5de5\u4e1a\u73af\u5883\u4e2d\u88ab\u653e\u5927\uff0c\u9700\u8981\u96f6\u4fe1\u4efb\u673a\u5236\uff0c\u5728\u8fd9\u79cd\u673a\u5236\u4e2d\uff0c\u6ca1\u6709\u53c2\u4e0e\u8005\u662f\u5929\u751f\u503c\u5f97\u4fe1\u4efb\u7684\u3002", "method": "\u96c6\u6210\u4e86\u57fa\u4e8e\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u5e8f\u5217\u5230\u5e8f\u5217\u63d2\u8865\u6a21\u578b\u7684\u96f6\u4fe1\u4efb\u8054\u90a6\u5b66\u4e60\u6846\u67b6 ZTFed-MAS2S\u3002", "result": "ZTFed \u96c6\u6210\u4e86\u53ef\u9a8c\u8bc1\u5dee\u5206\u9690\u79c1\u4e0e\u975e\u4ea4\u4e92\u5f0f\u96f6\u77e5\u8bc6\u8bc1\u660e\u4ee5\u53ca\u673a\u5bc6\u6027\u548c\u5b8c\u6574\u6027\u9a8c\u8bc1\u673a\u5236\uff0c\u4ee5\u786e\u4fdd\u53ef\u9a8c\u8bc1\u7684\u9690\u79c1\u4fdd\u62a4\u548c\u5b89\u5168\u7684\u6a21\u578b\u53c2\u6570\u4f20\u8f93\u3002\u91c7\u7528\u4e86\u4e00\u79cd\u52a8\u6001\u7684\u4fe1\u4efb\u611f\u77e5\u805a\u5408\u673a\u5236\uff0c\u5176\u4e2d\u4fe1\u4efb\u901a\u8fc7\u76f8\u4f3c\u6027\u56fe\u4f20\u64ad\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u7a00\u758f\u6027\u548c\u91cf\u5316\u7684\u538b\u7f29\u6765\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002MAS2S \u6355\u83b7\u98ce\u7535\u6570\u636e\u4e2d\u7684\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u63d2\u8865\u3002", "conclusion": "\u5728\u771f\u5b9e\u98ce\u7535\u573a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86 ZTFed-MAS2S \u5728\u8054\u90a6\u5b66\u4e60\u6027\u80fd\u548c\u7f3a\u5931\u6570\u636e\u63d2\u8865\u65b9\u9762\u7684\u4f18\u8d8a\u6027\uff0c\u8bc1\u660e\u4e86\u5b83\u4f5c\u4e3a\u80fd\u6e90\u9886\u57df\u5b9e\u9645\u5e94\u7528\u7684\u5b89\u5168\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.18509", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18509", "abs": "https://arxiv.org/abs/2508.18509", "authors": ["Andreza M. C. Falcao", "Filipe R. Cordeiro"], "title": "Analise de Desaprendizado de Maquina em Modelos de Classificacao de Imagens Medicas", "comment": "Accepted at SBCAS'25. in Portuguese language", "summary": "Machine unlearning aims to remove private or sensitive data from a\npre-trained model while preserving the model's robustness. Despite recent\nadvances, this technique has not been explored in medical image classification.\nThis work evaluates the SalUn unlearning model by conducting experiments on the\nPathMNIST, OrganAMNIST, and BloodMNIST datasets. We also analyse the impact of\ndata augmentation on the quality of unlearning. Results show that SalUn\nachieves performance close to full retraining, indicating an efficient solution\nfor use in medical applications.", "AI": {"tldr": "This paper explores machine unlearning in medical image classification using the SalUn model on three datasets, finding it to be an efficient solution.", "motivation": "Machine unlearning aims to remove private or sensitive data from a pre-trained model while preserving the model's robustness. Despite recent advances, this technique has not been explored in medical image classification.", "method": "We evaluate the SalUn unlearning model by conducting experiments on the PathMNIST, OrganAMNIST, and BloodMNIST datasets. We also analyse the impact of data augmentation on the quality of unlearning.", "result": "SalUn achieves performance close to full retraining.", "conclusion": "SalUn achieves performance close to full retraining, indicating an efficient solution for use in medical applications."}}
{"id": "2508.18569", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18569", "abs": "https://arxiv.org/abs/2508.18569", "authors": ["Girish A. Koushik", "Fatemeh Nazarieh", "Katherine Birch", "Shenbin Qian", "Diptesh Kanojia"], "title": "The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation", "comment": "Under Review", "summary": "Visual metaphor generation is a challenging task that aims to generate an\nimage given an input text metaphor. Inherently, it needs language understanding\nto bind a source concept with a target concept, in a way that preserves meaning\nwhile ensuring visual coherence. We propose a self-evaluating visual metaphor\ngeneration framework that focuses on metaphor alignment. Our self-evaluation\napproach combines existing metrics with our newly proposed metaphor\ndecomposition score and a meaning alignment (MA) metric. Within this setup, we\nexplore two novel approaches: a training-free pipeline that explicitly\ndecomposes prompts into source-target-meaning (S-T-M) mapping for image\nsynthesis, and a complementary training-based pipeline that improves alignment\nusing our proposed self-evaluation reward schema, without any large-scale\nretraining. On the held-out test set, the training-free approach surpasses\nstrong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores,\nwith the training-based approach close behind. We evaluate our framework output\nusing a user-facing study, and observed that participants preferred GPT-4o\noverall, while our training-free pipeline led open-source methods and edged\nImagen on abstract metaphors. Our analyses show S-T-M prompting helps longer or\nmore abstract metaphors, with closed models excelling on short, concrete cases;\nwe also observe sensitivity to sampler settings. Overall, structured prompting\nand lightweight RL perform metaphor alignment well under modest compute, and\nremaining gaps to human preference appear driven by aesthetics and sampling.", "AI": {"tldr": "Proposes a self-evaluating visual metaphor generation framework with training-free and training-based pipelines for metaphor alignment. Achieves strong results compared to baselines, with structured prompting and lightweight RL performing well under modest compute.", "motivation": "Visual metaphor generation is a challenging task that aims to generate an image given an input text metaphor. Inherently, it needs language understanding to bind a source concept with a target concept, in a way that preserves meaning while ensuring visual coherence.", "method": "a self-evaluating visual metaphor generation framework that focuses on metaphor alignment. Includes a training-free pipeline that explicitly decomposes prompts into source-target-meaning (S-T-M) mapping for image synthesis, and a complementary training-based pipeline that improves alignment using our proposed self-evaluation reward schema, without any large-scale retraining.", "result": "The training-free approach surpasses strong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores, with the training-based approach close behind. User study showed participants preferred GPT-4o overall, while the training-free pipeline led open-source methods and edged Imagen on abstract metaphors. S-T-M prompting helps longer or more abstract metaphors, with closed models excelling on short, concrete cases.", "conclusion": "Structured prompting and lightweight RL perform metaphor alignment well under modest compute, and remaining gaps to human preference appear driven by aesthetics and sampling."}}
{"id": "2508.18646", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18646", "abs": "https://arxiv.org/abs/2508.18646", "authors": ["Jun Wang", "Ninglun Gu", "Kailai Zhang", "Zijiao Zhang", "Yelun Bao", "Jin Yang", "Xu Yin", "Liwei Liu", "Yihuan Liu", "Pengyong Li", "Gary G. Yen", "Junchi Yan"], "title": "Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap", "comment": "Preprint. Under review", "summary": "For Large Language Models (LLMs), a disconnect persists between benchmark\nperformance and real-world utility. Current evaluation frameworks remain\nfragmented, prioritizing technical metrics while neglecting holistic assessment\nfor deployment. This survey introduces an anthropomorphic evaluation paradigm\nthrough the lens of human intelligence, proposing a novel three-dimensional\ntaxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational\ncapacity, Emotional Quotient (EQ)-Alignment Ability for value-based\ninteractions, and Professional Quotient (PQ)-Professional Expertise for\nspecialized proficiency. For practical value, we pioneer a Value-oriented\nEvaluation (VQ) framework assessing economic viability, social impact, ethical\nalignment, and environmental sustainability. Our modular architecture\nintegrates six components with an implementation roadmap. Through analysis of\n200+ benchmarks, we identify key challenges including dynamic assessment needs\nand interpretability gaps. It provides actionable guidance for developing LLMs\nthat are technically proficient, contextually relevant, and ethically sound. We\nmaintain a curated repository of open-source evaluation resources at:\nhttps://github.com/onejune2018/Awesome-LLM-Eval.", "AI": {"tldr": "This survey proposes a new evaluation framework for LLMs based on human intelligence and practical value, addressing the gap between benchmark performance and real-world utility.", "motivation": "Current evaluation frameworks for Large Language Models (LLMs) are fragmented and neglect holistic assessment for deployment, leading to a disconnect between benchmark performance and real-world utility.", "method": "The paper introduces an anthropomorphic evaluation paradigm and a Value-oriented Evaluation (VQ) framework.", "result": "The paper identifies key challenges including dynamic assessment needs and interpretability gaps through analysis of 200+ benchmarks. It also maintains a curated repository of open-source evaluation resources.", "conclusion": "This paper provides actionable guidance for developing LLMs that are technically proficient, contextually relevant, and ethically sound."}}
{"id": "2508.18338", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18338", "abs": "https://arxiv.org/abs/2508.18338", "authors": ["Jarek Duda", "Jagoda Bracha", "Adrian Przybysz"], "title": "Linear cost mutual information estimation and independence test of similar performance as HSIC", "comment": "7 pages, 5 figures", "summary": "Evaluation of statistical dependencies between two data samples is a basic\nproblem of data science/machine learning, and HSIC (Hilbert-Schmidt Information\nCriterion)~\\cite{HSIC} is considered the state-of-art method. However, for size\n$n$ data sample it requires multiplication of $n\\times n$ matrices, what\ncurrently needs $\\sim O(n^{2.37})$ computational complexity~\\cite{mult}, making\nit impractical for large data samples. We discuss HCR (Hierarchical Correlation\nReconstruction) as its linear cost practical alternative of even higher\ndependence sensitivity in tests, and additionally providing actual joint\ndistribution model by description of dependencies through features being mixed\nmoments, starting with correlation and homoscedasticity, also allowing to\napproximate mutual information as just sum of squares of such nontrivial mixed\nmoments between two data samples. Such single dependence describing feature is\ncalculated in $O(n)$ linear time. Their number to test varies with dimension\n$d$ - requiring $O(d^2)$ for pairwise dependencies, $O(d^3)$ if wanting to also\nconsider more subtle triplewise, and so on.", "AI": {"tldr": "HSIC is too slow. HCR is a faster and more sensitive alternative for evaluating statistical dependencies.", "motivation": "HSIC is computationally expensive for large datasets due to its O(n^2.37) complexity.", "method": "Hierarchical Correlation Reconstruction (HCR)", "result": "HCR offers linear cost and higher dependence sensitivity, providing a joint distribution model through mixed moments.", "conclusion": "HCR (Hierarchical Correlation Reconstruction) is a linear cost alternative to HSIC with higher dependence sensitivity."}}
{"id": "2508.18528", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18528", "abs": "https://arxiv.org/abs/2508.18528", "authors": ["Anna Milani", "F\u00e1bio S. da Silva", "Ello\u00e1 B. Guedes", "Ricardo Rios"], "title": "A Deep Learning Application for Psoriasis Detection", "comment": "15 pages, 4 figures, 1 table, Proceedings of XX Encontro Nacional de\n  Intelig\\^encia Artificial e Computacional. in Portuguese language", "summary": "In this paper a comparative study of the performance of three Convolutional\nNeural Network models, ResNet50, Inception v3 and VGG19 for classification of\nskin images with lesions affected by psoriasis is presented. The images used\nfor training and validation of the models were obtained from specialized\nplatforms. Some techniques were used to adjust the evaluation metrics of the\nneural networks. The results found suggest the model Inception v3 as a valuable\ntool for supporting the diagnosis of psoriasis. This is due to its satisfactory\nperformance with respect to accuracy and F1-Score (97.5% ${\\pm}$ 0.2).", "AI": {"tldr": "Inception v3 is a valuable tool for supporting the diagnosis of psoriasis", "motivation": "classification of skin images with lesions affected by psoriasis", "method": "comparative study of the performance of three Convolutional Neural Network models, ResNet50, Inception v3 and VGG19", "result": "the model Inception v3 has satisfactory performance with respect to accuracy and F1-Score (97.5% $\\pm$ 0.2)", "conclusion": "Inception v3 is a valuable tool for supporting the diagnosis of psoriasis due to its satisfactory performance with respect to accuracy and F1-Score (97.5% ${\\pm}$ 0.2)."}}
{"id": "2508.18598", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18598", "abs": "https://arxiv.org/abs/2508.18598", "authors": ["Colin Klein"], "title": "What do language models model? Transformers, automata, and the format of thought", "comment": null, "summary": "What do large language models actually model? Do they tell us something about\nhuman capacities, or are they models of the corpus we've trained them on? I\ngive a non-deflationary defence of the latter position. Cognitive science tells\nus that linguistic capabilities in humans rely supralinear formats for\ncomputation. The transformer architecture, by contrast, supports at best a\nlinear formats for processing. This argument will rely primarily on certain\ninvariants of the computational architecture of transformers. I then suggest a\npositive story about what transformers are doing, focusing on Liu et al.\n(2022)'s intriguing speculations about shortcut automata. I conclude with why I\ndon't think this is a terribly deflationary story. Language is not (just) a\nmeans for expressing inner state but also a kind of 'discourse machine' that\nlets us make new language given appropriate context. We have learned to use\nthis technology in one way; LLMs have also learned to use it too, but via very\ndifferent means.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u8bed\u6599\u5e93\u7684\u6a21\u578b\uff0c\u4ee5\u4e0d\u540c\u4e8e\u4eba\u7c7b\u7684\u65b9\u5f0f\u4f7f\u7528\u8bed\u8a00\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b9e\u9645\u5efa\u6a21\u7684\u5185\u5bb9\uff0c\u4ee5\u53ca\u5b83\u4eec\u662f\u5426\u63ed\u793a\u4e86\u4eba\u7c7b\u7684\u80fd\u529b\uff0c\u6216\u8005\u4ec5\u4ec5\u662f\u6211\u4eec\u8bad\u7ec3\u5b83\u4eec\u7684\u8bed\u6599\u5e93\u7684\u6a21\u578b\u3002", "method": "\u5206\u6790\u4e86transformer\u67b6\u6784\u7684\u8ba1\u7b97\u67b6\u6784\u7684\u67d0\u4e9b\u4e0d\u53d8\u6027\uff0c\u5e76\u501f\u9274\u4e86Liu et al. (2022) \u5173\u4e8eshortcut automata\u7684\u63a8\u6d4b\u3002", "result": "\u8ba4\u77e5\u79d1\u5b66\u8868\u660e\uff0c\u4eba\u7c7b\u7684\u8bed\u8a00\u80fd\u529b\u4f9d\u8d56\u4e8e\u8d85\u7ebf\u6027\u683c\u5f0f\u8fdb\u884c\u8ba1\u7b97\uff0c\u800ctransformer\u67b6\u6784\u6700\u591a\u652f\u6301\u7ebf\u6027\u683c\u5f0f\u8fdb\u884c\u5904\u7406\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u8bed\u6599\u5e93\u7684\u6a21\u578b\uff0c\u800c\u4e0d\u662f\u4eba\u7c7b\u80fd\u529b\u7684\u53cd\u6620\u3002\u5b83\u4eec\u901a\u8fc7\u4e0e\u4eba\u7c7b\u4e0d\u540c\u7684\u65b9\u5f0f\u4f7f\u7528\u8bed\u8a00\uff0c\u8bed\u8a00\u662f\u4e00\u79cd\u201c\u8bdd\u8bed\u673a\u5668\u201d\uff0c\u53ef\u4ee5\u8ba9\u6211\u4eec\u5728\u9002\u5f53\u7684\u8bed\u5883\u4e0b\u521b\u9020\u65b0\u7684\u8bed\u8a00\u3002"}}
{"id": "2508.18669", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18669", "abs": "https://arxiv.org/abs/2508.18669", "authors": ["Weikang Zhao", "Xili Wang", "Chengdi Ma", "Lingbin Kong", "Zhaohua Yang", "Mingxiang Tuo", "Xiaowei Shi", "Yitao Zhai", "Xunliang Cai"], "title": "MUA-RL: Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use", "comment": null, "summary": "With the recent rapid advancement of Agentic Intelligence, agentic tool use\nin LLMs has become increasingly important. During multi-turn interactions\nbetween agents and users, the dynamic, uncertain, and stochastic nature of user\ndemands poses significant challenges to the agent's tool invocation\ncapabilities. Agents are no longer expected to simply call tools to deliver a\nresult; rather, they must iteratively refine their understanding of user needs\nthrough communication while simultaneously invoking tools to resolve user\nqueries. Existing reinforcement learning (RL) approaches for tool use lack the\nintegration of genuinely dynamic users during the RL training process. To\nbridge this gap, we introduce MUA-RL (Multi-turn User-interacting Agent\nReinforcement Learning for agentic tool use), a novel reinforcement learning\nframework that, for the first time in the field of agentic tool use, integrates\nLLM-simulated users into the reinforcement learning loop. MUA-RL aims to enable\nautonomous learning of models to communicate with users efficiently and use\nvarious tools to solve practical problems in dynamic multi-turn interactions.\nEvaluations are done on several multi-turn tool-using benchmarks (see Figure\n1). Specifically, MUA-RL-32B achieves 67.3 on TAU2 Retail, 45.4 on TAU2\nAirline, 28.3 on TAU2 Telecom, 28.4 on BFCL-V3 Multi Turn, and 82.5 on ACEBench\nAgent -- outperforming or matching the performance of larger open-source models\nsuch as DeepSeek-V3-0324 and Qwen3-235B-A22B in non-thinking settings.", "AI": {"tldr": "MUA-RL \u662f\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u4f7f\u7528 LLM \u6a21\u62df\u7528\u6237\u6765\u6539\u8fdb agentic \u5de5\u5177\u7684\u4f7f\u7528\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002", "motivation": "\u5728 LLM \u4e2d\uff0cAgentic \u5de5\u5177\u7684\u4f7f\u7528\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u5728\u4ee3\u7406\u548c\u7528\u6237\u4e4b\u95f4\u7684\u591a\u8f6e\u4ea4\u4e92\u4e2d\uff0c\u7528\u6237\u9700\u6c42\u7684\u52a8\u6001\u6027\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u968f\u673a\u6027\u7ed9\u4ee3\u7406\u7684\u5de5\u5177\u8c03\u7528\u80fd\u529b\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u7684\u5de5\u5177\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60 (RL) \u65b9\u6cd5\u5728 RL \u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7f3a\u4e4f\u771f\u6b63\u52a8\u6001\u7528\u6237\u7684\u96c6\u6210\u3002", "method": "\u5f15\u5165 MUA-RL\uff0c\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u9996\u6b21\u5c06 LLM \u6a21\u62df\u7528\u6237\u96c6\u6210\u5230\u5f3a\u5316\u5b66\u4e60\u5faa\u73af\u4e2d\u3002", "result": "MUA-RL-32B \u5728 TAU2 Retail \u4e0a\u8fbe\u5230 67.3\uff0c\u5728 TAU2 Airline \u4e0a\u8fbe\u5230 45.4\uff0c\u5728 TAU2 Telecom \u4e0a\u8fbe\u5230 28.3\uff0c\u5728 BFCL-V3 Multi Turn \u4e0a\u8fbe\u5230 28.4\uff0c\u5728 ACEBench Agent \u4e0a\u8fbe\u5230 82.5\u3002", "conclusion": "MUA-RL-32B \u5728\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u901a\u5e38\u4f18\u4e8e\u6216\u5339\u914d\u66f4\u5927\u7684\u5f00\u6e90\u6a21\u578b\u3002"}}
{"id": "2508.18376", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.18376", "abs": "https://arxiv.org/abs/2508.18376", "authors": ["Weilin Cai", "Le Qin", "Shwai He", "Junwei Cui", "Ang Li", "Jiayi Huang"], "title": "DualSparse-MoE: Coordinating Tensor/Neuron-Level Sparsity with Expert Partition and Reconstruction", "comment": null, "summary": "Mixture of Experts (MoE) has become a mainstream architecture for building\nLarge Language Models (LLMs) by reducing per-token computation while enabling\nmodel scaling. It can be viewed as partitioning a large Feed-Forward Network\n(FFN) at the tensor level into fine-grained sub-FFNs, or experts, and\nactivating only a sparse subset for each input. While this sparsity improves\nefficiency, MoE still faces substantial challenges due to their massive\ncomputational scale and unpredictable activation patterns.\n  To enable efficient MoE deployment, we identify dual sparsity at the tensor\nand neuron levels in pre-trained MoE modules as a key factor for both accuracy\nand efficiency. Unlike prior work that increases tensor-level sparsity through\nfiner-grained expert design during pre-training, we introduce post-training\nexpert partitioning to induce such sparsity without retraining. This preserves\nthe mathematical consistency of model transformations and enhances both\nefficiency and accuracy in subsequent fine-tuning and inference. Building upon\nthis, we propose DualSparse-MoE, an inference system that integrates dynamic\ntensor-level computation dropping with static neuron-level reconstruction to\ndeliver significant efficiency gains with minimal accuracy loss.\n  Experimental results show that enforcing an approximate 25% drop rate with\nour approach reduces average accuracy by only 0.08%-0.28% across three\nprevailing MoE models, while nearly all degrees of computation dropping\nconsistently yield proportional computational speedups. Furthermore,\nincorporating load-imbalance awareness into expert parallelism achieves a 1.41x\nMoE module speedup with just 0.5% average accuracy degradation.", "AI": {"tldr": "\u63d0\u51faDualSparse-MoE\uff0c\u901a\u8fc7\u52a8\u6001\u5f20\u91cf\u7ea7\u8ba1\u7b97 dropping \u4e0e\u9759\u6001\u795e\u7ecf\u5143\u7ea7\u91cd\u5efa\uff0c\u5728\u6700\u5c0f\u7cbe\u5ea6\u635f\u5931\u4e0b\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "motivation": "MoE\u7531\u4e8e\u5176\u5de8\u5927\u7684\u8ba1\u7b97\u89c4\u6a21\u548c\u4e0d\u53ef\u9884\u6d4b\u7684\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u4ecd\u7136\u9762\u4e34\u7740\u5de8\u5927\u7684\u6311\u6218\u3002\u4e3a\u4e86\u5b9e\u73b0\u9ad8\u6548\u7684MoE\u90e8\u7f72\uff0c\u6211\u4eec\u8ba4\u4e3a\u5728\u9884\u8bad\u7ec3\u7684MoE\u6a21\u5757\u4e2d\uff0c\u5f20\u91cf\u548c\u795e\u7ecf\u5143\u7ea7\u522b\u7684\u53cc\u91cd\u7a00\u758f\u6027\u662f\u51c6\u786e\u6027\u548c\u6548\u7387\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u540e\u8bad\u7ec3\u4e13\u5bb6\u5206\u533a\uff0c\u4ee5\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u8bf1\u5bfc\u8fd9\u79cd\u7a00\u758f\u6027\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u63d0\u51fa DualSparse-MoE\uff0c\u8fd9\u662f\u4e00\u79cd\u63a8\u7406\u7cfb\u7edf\uff0c\u5b83\u96c6\u6210\u4e86\u52a8\u6001\u5f20\u91cf\u7ea7\u8ba1\u7b97 dropping \u4e0e\u9759\u6001\u795e\u7ecf\u5143\u7ea7\u91cd\u5efa\uff0c\u4ee5\u5728\u6700\u5c0f\u7684\u7cbe\u5ea6\u635f\u5931\u4e0b\u63d0\u4f9b\u663e\u7740\u7684\u6548\u7387\u63d0\u5347\u3002", "result": "\u5728\u4e09\u4e2a\u4e3b\u6d41MoE\u6a21\u578b\u4e0a\u5f3a\u5236\u6267\u884c\u5927\u7ea625%\u7684drop rate\uff0c\u5e73\u5747\u7cbe\u5ea6\u4ec5\u964d\u4f4e\u4e860.08%-0.28%\uff0c\u51e0\u4e4e\u6240\u6709\u7a0b\u5ea6\u7684\u8ba1\u7b97\u4e0b\u964d\u90fd\u6301\u7eed\u4ea7\u751f\u6210\u6bd4\u4f8b\u7684\u8ba1\u7b97\u52a0\u901f\u3002\u5c06\u8d1f\u8f7d\u4e0d\u5e73\u8861\u611f\u77e5\u7eb3\u5165\u4e13\u5bb6\u5e76\u884c\u5904\u7406\uff0c\u4ec5\u4ee50.5%\u7684\u5e73\u5747\u7cbe\u5ea6\u4e0b\u964d\u5b9e\u73b0\u4e861.41\u500d\u7684MoE\u6a21\u5757\u52a0\u901f\u3002", "conclusion": "\u901a\u8fc7\u5728\u4e09\u4e2a\u4e3b\u6d41MoE\u6a21\u578b\u4e0a\u5f3a\u5236\u6267\u884c\u5927\u7ea625%\u7684drop rate\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u5e73\u5747\u7cbe\u5ea6\u964d\u4f4e\u4e86\u4ec50.08%-0.28%\uff0c\u540c\u65f6\u51e0\u4e4e\u6240\u6709\u7a0b\u5ea6\u7684\u8ba1\u7b97\u4e0b\u964d\u90fd\u6301\u7eed\u4ea7\u751f\u6210\u6bd4\u4f8b\u7684\u8ba1\u7b97\u52a0\u901f\u3002\u6b64\u5916\uff0c\u5c06\u8d1f\u8f7d\u4e0d\u5e73\u8861\u611f\u77e5\u7eb3\u5165\u4e13\u5bb6\u5e76\u884c\u5904\u7406\uff0c\u4ec5\u4ee50.5%\u7684\u5e73\u5747\u7cbe\u5ea6\u4e0b\u964d\u5b9e\u73b0\u4e861.41\u500d\u7684MoE\u6a21\u5757\u52a0\u901f\u3002"}}
{"id": "2508.18531", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18531", "abs": "https://arxiv.org/abs/2508.18531", "authors": ["Zhangyu Jin", "Andrew Feng"], "title": "SAT-SKYLINES: 3D Building Generation from Satellite Imagery and Coarse Geometric Priors", "comment": null, "summary": "We present SatSkylines, a 3D building generation approach that takes\nsatellite imagery and coarse geometric priors. Without proper geometric\nguidance, existing image-based 3D generation methods struggle to recover\naccurate building structures from the top-down views of satellite images alone.\nOn the other hand, 3D detailization methods tend to rely heavily on highly\ndetailed voxel inputs and fail to produce satisfying results from simple priors\nsuch as cuboids. To address these issues, our key idea is to model the\ntransformation from interpolated noisy coarse priors to detailed geometries,\nenabling flexible geometric control without additional computational cost. We\nhave further developed Skylines-50K, a large-scale dataset of over 50,000\nunique and stylized 3D building assets in order to support the generations of\ndetailed building models. Extensive evaluations indicate the effectiveness of\nour model and strong generalization ability.", "AI": {"tldr": "SatSkylines is a 3D building generation approach that takes satellite imagery and coarse geometric priors.", "motivation": "Existing image-based 3D generation methods struggle to recover accurate building structures from satellite images alone, while 3D detailization methods tend to rely heavily on highly detailed voxel inputs and fail to produce satisfying results from simple priors such as cuboids.", "method": "The key idea is to model the transformation from interpolated noisy coarse priors to detailed geometries.", "result": "Developed Skylines-50K, a large-scale dataset of over 50,000 unique and stylized 3D building assets.", "conclusion": "The model is effective and has strong generalization ability."}}
{"id": "2508.18607", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18607", "abs": "https://arxiv.org/abs/2508.18607", "authors": ["Rumeng Li", "Xun Wang", "Hong Yu"], "title": "A New NMT Model for Translating Clinical Texts from English to Spanish", "comment": "This work was accepted by the Machine Learning for Health (ML4H)\n  Workshop at NeurIPS 2018", "summary": "Translating electronic health record (EHR) narratives from English to Spanish\nis a clinically important yet challenging task due to the lack of a\nparallel-aligned corpus and the abundant unknown words contained. To address\nsuch challenges, we propose \\textbf{NOOV} (for No OOV), a new neural machine\ntranslation (NMT) system that requires little in-domain parallel-aligned corpus\nfor training. NOOV integrates a bilingual lexicon automatically learned from\nparallel-aligned corpora and a phrase look-up table extracted from a large\nbiomedical knowledge resource, to alleviate both the unknown word problem and\nthe word-repeat challenge in NMT, enhancing better phrase generation of NMT\nsystems. Evaluation shows that NOOV is able to generate better translation of\nEHR with improvement in both accuracy and fluency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edfNOOV\uff0c\u7528\u4e8e\u5c06\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u53d9\u8ff0\u4ece\u82f1\u8bed\u7ffb\u8bd1\u6210\u897f\u73ed\u7259\u8bed\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u5e73\u884c\u5bf9\u9f50\u7684\u8bed\u6599\u5e93\u4ee5\u53ca\u5305\u542b\u5927\u91cf\u672a\u77e5\u5355\u8bcd\uff0c\u56e0\u6b64\u5c06\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u53d9\u8ff0\u4ece\u82f1\u8bed\u7ffb\u8bd1\u6210\u897f\u73ed\u7259\u8bed\u662f\u4e00\u9879\u4e34\u5e8a\u4e0a\u91cd\u8981\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edfNOOV\uff0c\u8be5\u7cfb\u7edf\u9700\u8981\u5f88\u5c11\u7684\u9886\u57df\u5185\u5e73\u884c\u8bed\u6599\u5e93\u8fdb\u884c\u8bad\u7ec3\u3002NOOV\u96c6\u6210\u4e86\u4ece\u5e73\u884c\u8bed\u6599\u5e93\u81ea\u52a8\u5b66\u4e60\u7684\u53cc\u8bed\u8bcd\u5178\u548c\u4ece\u5927\u578b\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u8d44\u6e90\u4e2d\u63d0\u53d6\u7684\u77ed\u8bed\u67e5\u627e\u8868\uff0c\u4ee5\u7f13\u89e3NMT\u4e2d\u672a\u77e5\u7684\u5355\u8bcd\u95ee\u9898\u548c\u5355\u8bcd\u91cd\u590d\u6311\u6218\uff0c\u4ece\u800c\u589e\u5f3a\u4e86NMT\u7cfb\u7edf\u66f4\u597d\u7684\u77ed\u8bed\u751f\u6210\u3002", "result": "NOOV\u80fd\u591f\u751f\u6210\u66f4\u597d\u7684EHR\u7ffb\u8bd1\uff0c\u5e76\u5728\u51c6\u786e\u6027\u548c\u6d41\u7545\u6027\u65b9\u9762\u90fd\u6709\u6240\u63d0\u9ad8\u3002", "conclusion": "NOOV\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u66f4\u597d\u7684EHR\u7ffb\u8bd1\uff0c\u5e76\u5728\u51c6\u786e\u6027\u548c\u6d41\u7545\u6027\u65b9\u9762\u90fd\u6709\u6240\u63d0\u9ad8\u3002"}}
{"id": "2508.18689", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18689", "abs": "https://arxiv.org/abs/2508.18689", "authors": ["Yuyang Zhao", "Wentao Shi", "Fuli Feng", "Xiangnan He"], "title": "AppAgent-Pro: A Proactive GUI Agent System for Multidomain Information Integration and User Assistance", "comment": "Accepted at CIKM 2025. 10 pages, 5 figures. Our code is available at:\n  https://github.com/LaoKuiZe/AppAgent-Pro. Our code is available at:\n  https://github.com/LaoKuiZe/AppAgent-Pro. The demonstration video could be\n  found at:\n  https://www.dropbox.com/scl/fi/hvzqo5vnusg66srydzixo/AppAgent-Pro-demo-video.mp4?rlkey=o2nlfqgq6ihl125mcqg7bpgqu&st=d29vrzii&dl=0", "summary": "Large language model (LLM)-based agents have demonstrated remarkable\ncapabilities in addressing complex tasks, thereby enabling more advanced\ninformation retrieval and supporting deeper, more sophisticated human\ninformation-seeking behaviors. However, most existing agents operate in a\npurely reactive manner, responding passively to user instructions, which\nsignificantly constrains their effectiveness and efficiency as general-purpose\nplatforms for information acquisition. To overcome this limitation, this paper\nproposes AppAgent-Pro, a proactive GUI agent system that actively integrates\nmulti-domain information based on user instructions. This approach enables the\nsystem to proactively anticipate users' underlying needs and conduct in-depth\nmulti-domain information mining, thereby facilitating the acquisition of more\ncomprehensive and intelligent information. AppAgent-Pro has the potential to\nfundamentally redefine information acquisition in daily life, leading to a\nprofound impact on human society. Our code is available at:\nhttps://github.com/LaoKuiZe/AppAgent-Pro. Our code is available at:\nhttps://github.com/LaoKuiZe/AppAgent-Pro. The demonstration video could be\nfound at:\nhttps://www.dropbox.com/scl/fi/hvzqo5vnusg66srydzixo/AppAgent-Pro-demo-video.mp4?rlkey=o2nlfqgq6ihl125mcqg7bpgqu&st=d29vrzii&dl=0.", "AI": {"tldr": "This paper introduces AppAgent-Pro, a proactive GUI agent system that actively integrates multi-domain information based on user instructions, overcoming the limitations of purely reactive agents.", "motivation": "most existing agents operate in a purely reactive manner, responding passively to user instructions, which significantly constrains their effectiveness and efficiency as general-purpose platforms for information acquisition", "method": "proposes AppAgent-Pro, a proactive GUI agent system that actively integrates multi-domain information based on user instructions", "result": "This approach enables the system to proactively anticipate users' underlying needs and conduct in-depth multi-domain information mining, thereby facilitating the acquisition of more comprehensive and intelligent information.", "conclusion": "AppAgent-Pro has the potential to fundamentally redefine information acquisition in daily life, leading to a profound impact on human society."}}
{"id": "2508.18408", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18408", "abs": "https://arxiv.org/abs/2508.18408", "authors": ["Ricardo Borsoi", "Konstantin Usevich", "Marianne Clausel"], "title": "Low-Rank Tensor Decompositions for the Theory of Neural Networks", "comment": null, "summary": "The groundbreaking performance of deep neural networks (NNs) promoted a surge\nof interest in providing a mathematical basis to deep learning theory. Low-rank\ntensor decompositions are specially befitting for this task due to their close\nconnection to NNs and their rich theoretical results. Different tensor\ndecompositions have strong uniqueness guarantees, which allow for a direct\ninterpretation of their factors, and polynomial time algorithms have been\nproposed to compute them. Through the connections between tensors and NNs, such\nresults supported many important advances in the theory of NNs. In this review,\nwe show how low-rank tensor methods--which have been a core tool in the signal\nprocessing and machine learning communities--play a fundamental role in\ntheoretically explaining different aspects of the performance of deep NNs,\nincluding their expressivity, algorithmic learnability and computational\nhardness, generalization, and identifiability. Our goal is to give an\naccessible overview of existing approaches (developed by different communities,\nranging from computer science to mathematics) in a coherent and unified way,\nand to open a broader perspective on the use of low-rank tensor decompositions\nfor the theory of deep NNs.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u4f4e\u79e9\u5f20\u91cf\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u89e3\u91ca\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\u4e0d\u540c\u65b9\u9762\u7684\u4f5c\u7528\uff0c\u5305\u62ec\u8868\u8fbe\u6027\u3001\u7b97\u6cd5\u53ef\u5b66\u4e60\u6027\u548c\u8ba1\u7b97\u786c\u5ea6\u3001\u6cdb\u5316\u548c\u53ef\u8bc6\u522b\u6027\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (NN) \u7684\u7a81\u7834\u6027\u6027\u80fd\u4fc3\u8fdb\u4e86\u4eba\u4eec\u5bf9\u4e3a\u6df1\u5ea6\u5b66\u4e60\u7406\u8bba\u63d0\u4f9b\u6570\u5b66\u57fa\u7840\u7684\u5174\u8da3\u6fc0\u589e\u3002\u4f4e\u79e9\u5f20\u91cf\u5206\u89e3\u7279\u522b\u9002\u5408\u8fd9\u9879\u4efb\u52a1\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e0e\u795e\u7ecf\u7f51\u7edc\u53ca\u5176\u4e30\u5bcc\u7684\u7406\u8bba\u7ed3\u679c\u5bc6\u5207\u76f8\u5173\u3002", "method": "\u4f4e\u79e9\u5f20\u91cf\u5206\u89e3", "result": "\u4e0d\u540c\u7684\u5f20\u91cf\u5206\u89e3\u5177\u6709\u5f88\u5f3a\u7684\u552f\u4e00\u6027\u4fdd\u8bc1\uff0c\u8fd9\u4f7f\u5f97\u53ef\u4ee5\u76f4\u63a5\u89e3\u91ca\u5b83\u4eec\u7684\u56e0\u7d20\uff0c\u5e76\u4e14\u5df2\u7ecf\u63d0\u51fa\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u6765\u8ba1\u7b97\u5b83\u4eec\u3002\u901a\u8fc7\u5f20\u91cf\u548c\u795e\u7ecf\u7f51\u7edc\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u8fd9\u4e9b\u7ed3\u679c\u652f\u6301\u4e86\u795e\u7ecf\u7f51\u7edc\u7406\u8bba\u4e2d\u7684\u8bb8\u591a\u91cd\u8981\u8fdb\u5c55\u3002", "conclusion": "\u4f4e\u79e9\u5f20\u91cf\u65b9\u6cd5\u5728\u4ece\u7406\u8bba\u4e0a\u89e3\u91ca\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (NN) \u6027\u80fd\u7684\u4e0d\u540c\u65b9\u9762\uff08\u5305\u62ec\u5176\u8868\u8fbe\u6027\u3001\u7b97\u6cd5\u53ef\u5b66\u4e60\u6027\u548c\u8ba1\u7b97\u786c\u5ea6\u3001\u6cdb\u5316\u548c\u53ef\u8bc6\u522b\u6027\uff09\u65b9\u9762\u53d1\u6325\u7740 \u0444\u0443\u043d\u0434\u0430\u043c\u0435\u043d\u0442\u0430\u043b\u044c\u043d\u0443\u044e \u0440\u043e\u043b\u044c\u3002"}}
{"id": "2508.18539", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18539", "abs": "https://arxiv.org/abs/2508.18539", "authors": ["Kaijie Xu", "Clark Verbrugge"], "title": "Adaptive Visual Navigation Assistant in 3D RPGs", "comment": null, "summary": "In complex 3D game environments, players rely on visual affordances to spot\nmap transition points. Efficient identification of such points is important to\nclient-side auto-mapping, and provides an objective basis for evaluating map\ncue presentation. In this work, we formalize the task of detecting traversable\nSpatial Transition Points (STPs)-connectors between two sub regions-and\nselecting the singular Main STP (MSTP), the unique STP that lies on the\ndesigner-intended critical path toward the player's current macro-objective,\nfrom a single game frame, proposing this as a new research focus. We introduce\na two-stage deep-learning pipeline that first detects potential STPs using\nFaster R-CNN and then ranks them with a lightweight MSTP selector that fuses\nlocal and global visual features. Both stages benefit from parameter-efficient\nadapters, and we further introduce an optional retrieval-augmented fusion step.\nOur primary goal is to establish the feasibility of this problem and set\nbaseline performance metrics. We validate our approach on a custom-built,\ndiverse dataset collected from five Action RPG titles. Our experiments reveal a\nkey trade-off: while full-network fine-tuning produces superior STP detection\nwith sufficient data, adapter-only transfer is significantly more robust and\neffective in low-data scenarios and for the MSTP selection task. By defining\nthis novel problem, providing a baseline pipeline and dataset, and offering\ninitial insights into efficient model adaptation, we aim to contribute to\nfuture AI-driven navigation aids and data-informed level-design tools.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u68c0\u6d4b\u53ef\u901a\u884c\u7a7a\u95f4\u8f6c\u6362\u70b9\uff08STP\uff09\u5e76\u9009\u62e9\u4e3b\u8981 STP\uff08MSTP\uff09\u7684\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u6df1\u5ea6\u5b66\u4e60\u7ba1\u7ebf\u4f5c\u4e3a\u57fa\u7ebf\uff0c\u540c\u65f6\u6784\u5efa\u4e86\u4e00\u4e2a\u6570\u636e\u96c6\uff0c\u4e3aAI\u9a71\u52a8\u7684\u5bfc\u822a\u548c\u5173\u5361\u8bbe\u8ba1\u505a\u51fa\u8d21\u732e\u3002", "motivation": "\u5728\u590d\u6742\u7684 3D \u6e38\u620f\u73af\u5883\u4e2d\uff0c\u73a9\u5bb6\u4f9d\u9760\u89c6\u89c9\u53ef\u4f9b\u6027\u6765\u53d1\u73b0\u5730\u56fe\u8f6c\u6362\u70b9\u3002\u6709\u6548\u8bc6\u522b\u8fd9\u4e9b\u70b9\u5bf9\u4e8e\u5ba2\u6237\u7aef\u81ea\u52a8\u6620\u5c04\u975e\u5e38\u91cd\u8981\uff0c\u5e76\u4e3a\u8bc4\u4f30\u5730\u56fe\u63d0\u793a\u5448\u73b0\u63d0\u4f9b\u4e86\u5ba2\u89c2\u57fa\u7840\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6df1\u5ea6\u5b66\u4e60\u7ba1\u7ebf\uff0c\u8be5\u7ba1\u7ebf\u9996\u5148\u4f7f\u7528 Faster R-CNN \u68c0\u6d4b\u6f5c\u5728\u7684 STP\uff0c\u7136\u540e\u4f7f\u7528\u8f7b\u91cf\u7ea7 MSTP \u9009\u62e9\u5668\u5bf9\u5b83\u4eec\u8fdb\u884c\u6392\u5e8f\uff0c\u8be5\u9009\u62e9\u5668\u878d\u5408\u4e86\u5c40\u90e8\u548c\u5168\u5c40\u89c6\u89c9\u7279\u5f81\u3002", "result": "\u6211\u4eec\u7684\u5b9e\u9a8c\u63ed\u793a\u4e86\u4e00\u4e2a\u5173\u952e\u7684\u6743\u8861\uff1a\u867d\u7136\u5168\u7f51\u7edc\u5fae\u8c03\u5728\u6709\u8db3\u591f\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u4ea7\u751f\u5353\u8d8a\u7684 STP \u68c0\u6d4b\uff0c\u4f46\u4ec5\u9002\u914d\u5668\u8fc1\u79fb\u5728\u4f4e\u6570\u636e\u573a\u666f\u548c MSTP \u9009\u62e9\u4efb\u52a1\u4e2d\u660e\u663e\u66f4\u7a33\u5065\u6709\u6548\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5b9a\u4e49\u65b0\u95ee\u9898\u3001\u63d0\u4f9b\u57fa\u51c6\u7ba1\u7ebf\u548c\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u5bf9\u9ad8\u6548\u6a21\u578b\u9002\u5e94\u7684\u521d\u6b65\u89c1\u89e3\uff0c\u4ece\u800c\u4e3a\u672a\u6765\u7684\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u5bfc\u822a\u8f85\u52a9\u5de5\u5177\u548c\u6570\u636e\u9a71\u52a8\u7684\u5173\u5361\u8bbe\u8ba1\u5de5\u5177\u505a\u51fa\u8d21\u732e\u3002"}}
{"id": "2508.18609", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18609", "abs": "https://arxiv.org/abs/2508.18609", "authors": ["Chenxi Zhou", "Pengfei Cao", "Jiang Li", "Jun Zhao", "Kang Liu"], "title": "Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models", "comment": null, "summary": "Large language models (LLMs) present significant deployment challenges due to\ntheir scale, with post-training quantization (PTQ) emerging as a practical\ncompression solution. However, a comprehensive understanding of how PTQ\nprecisely impacts diverse LLM knowledge capabilities remains elusive, and\nexisting scaling laws for quantized models often overlook crucial PTQ-specific\nparameters and task-specific sensitivities. This paper addresses these gaps by\nconducting an extensive empirical investigation to establish task-stratified\nscaling laws. We disentangle LLM knowledge into memorization and utilization\ncapabilities and develop a unified quantitative framework that incorporates\nmodel size, effective bit-width, calibration set size, and group size. Our\ncentral finding reveals that knowledge memorization exhibits markedly greater\nsensitivity to variations in effective bit-width, calibration set size, and\nmodel size compared to the more robust knowledge utilization. These findings\noffer a fine-grained understanding of PTQ's impact and provide guidance for\ndeveloping knowledge-aware quantization strategies that can better preserve\ntargeted cognitive functions.", "AI": {"tldr": "\u672c\u7814\u7a76\u6df1\u5165\u63a2\u8ba8\u4e86\u540e\u8bad\u7ec3\u91cf\u5316 (PTQ) \u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u77e5\u8bc6\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u77e5\u8bc6\u8bb0\u5fc6\u6bd4\u77e5\u8bc6\u5229\u7528\u5bf9\u91cf\u5316\u53c2\u6570\u66f4\u654f\u611f\uff0c\u4e3a\u5f00\u53d1\u77e5\u8bc6\u611f\u77e5\u91cf\u5316\u7b56\u7565\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u89c4\u6a21\uff0c\u5b83\u4eec\u5728\u90e8\u7f72\u65b9\u9762\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\uff0c\u56e0\u6b64\uff0c\u8bad\u7ec3\u540e\u91cf\u5316 (PTQ) \u6210\u4e3a\u4e00\u79cd\u5b9e\u7528\u7684\u538b\u7f29\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u5bf9\u4e8e PTQ \u5982\u4f55\u7cbe\u786e\u5730\u5f71\u54cd\u4e0d\u540c\u7684 LLM \u77e5\u8bc6\u80fd\u529b\uff0c\u4ecd\u7136\u7f3a\u4e4f\u5168\u9762\u7684\u4e86\u89e3\uff0c\u5e76\u4e14\u73b0\u6709\u91cf\u5316\u6a21\u578b\u7684\u7f29\u653e\u5b9a\u5f8b\u901a\u5e38\u5ffd\u7565\u4e86\u5173\u952e\u7684 PTQ \u7279\u5b9a\u53c2\u6570\u548c\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u654f\u611f\u6027\u3002", "method": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u7814\u7a76\u6765\u5efa\u7acb\u4efb\u52a1\u5206\u5c42\u7684\u7f29\u653e\u5b9a\u5f8b\u3002\u5c06 LLM \u77e5\u8bc6\u5206\u89e3\u4e3a\u8bb0\u5fc6\u548c\u5229\u7528\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5b9a\u91cf\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u6a21\u578b\u5927\u5c0f\u3001\u6709\u6548\u4f4d\u5bbd\u3001\u6821\u51c6\u96c6\u5927\u5c0f\u548c\u7ec4\u5927\u5c0f\u3002", "result": "\u77e5\u8bc6\u8bb0\u5fc6\u8868\u73b0\u51fa\u6bd4\u77e5\u8bc6\u5229\u7528\u66f4\u5f3a\u7684\u5bf9\u6709\u6548\u4f4d\u5bbd\u3001\u6821\u51c6\u96c6\u5927\u5c0f\u548c\u6a21\u578b\u5927\u5c0f\u53d8\u5316\u7684\u654f\u611f\u6027\u3002", "conclusion": "\u77e5\u8bc6\u8bb0\u5fc6\u5bf9\u6709\u6548\u4f4d\u5bbd\u3001\u6821\u51c6\u96c6\u5927\u5c0f\u548c\u6a21\u578b\u5927\u5c0f\u7684\u53d8\u5316\u6bd4\u77e5\u8bc6\u5229\u7528\u66f4\u654f\u611f\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u4e86\u89e3 PTQ \u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u5f00\u53d1\u80fd\u591f\u66f4\u597d\u4fdd\u7559\u76ee\u6807\u8ba4\u77e5\u529f\u80fd\u7684\u77e5\u8bc6\u611f\u77e5\u91cf\u5316\u7b56\u7565\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2508.18722", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18722", "abs": "https://arxiv.org/abs/2508.18722", "authors": ["Honghao Fu", "Junlong Ren", "Qi Chai", "Deheng Ye", "Yujun Cai", "Hao Wang"], "title": "VistaWise: Building Cost-Effective Agent with Cross-Modal Knowledge Graph for Minecraft", "comment": "Accepted by EMNLP 2025 main", "summary": "Large language models (LLMs) have shown significant promise in embodied\ndecision-making tasks within virtual open-world environments. Nonetheless,\ntheir performance is hindered by the absence of domain-specific knowledge.\nMethods that finetune on large-scale domain-specific data entail prohibitive\ndevelopment costs. This paper introduces VistaWise, a cost-effective agent\nframework that integrates cross-modal domain knowledge and finetunes a\ndedicated object detection model for visual analysis. It reduces the\nrequirement for domain-specific training data from millions of samples to a few\nhundred. VistaWise integrates visual information and textual dependencies into\na cross-modal knowledge graph (KG), enabling a comprehensive and accurate\nunderstanding of multimodal environments. We also equip the agent with a\nretrieval-based pooling strategy to extract task-related information from the\nKG, and a desktop-level skill library to support direct operation of the\nMinecraft desktop client via mouse and keyboard inputs. Experimental results\ndemonstrate that VistaWise achieves state-of-the-art performance across various\nopen-world tasks, highlighting its effectiveness in reducing development costs\nwhile enhancing agent performance.", "AI": {"tldr": "VistaWise, a cost-effective agent framework, integrates cross-modal domain knowledge and finetunes a dedicated object detection model for visual analysis. It reduces the requirement for domain-specific training data from millions of samples to a few hundred and achieves state-of-the-art performance across various open-world tasks.", "motivation": "LLMs' performance is hindered by the absence of domain-specific knowledge. Methods that finetune on large-scale domain-specific data entail prohibitive development costs.", "method": "integrates cross-modal domain knowledge and finetunes a dedicated object detection model for visual analysis. It integrates visual information and textual dependencies into a cross-modal knowledge graph (KG), enabling a comprehensive and accurate understanding of multimodal environments. We also equip the agent with a retrieval-based pooling strategy to extract task-related information from the KG, and a desktop-level skill library to support direct operation of the Minecraft desktop client via mouse and keyboard inputs.", "result": "It reduces the requirement for domain-specific training data from millions of samples to a few hundred.", "conclusion": "VistaWise achieves state-of-the-art performance across various open-world tasks, highlighting its effectiveness in reducing development costs while enhancing agent performance."}}
{"id": "2508.18420", "categories": ["cs.LG", "I.2.6"], "pdf": "https://arxiv.org/pdf/2508.18420", "abs": "https://arxiv.org/abs/2508.18420", "authors": ["Andr\u00e9 Quadros", "Cassio Silva", "Ronnie Alves"], "title": "LLM-Driven Intrinsic Motivation for Sparse Reward Reinforcement Learning", "comment": "11 pages, 5 figures, Accepted to the ENIAC 2025 conference", "summary": "This paper explores the combination of two intrinsic motivation strategies to\nimprove the efficiency of reinforcement learning (RL) agents in environments\nwith extreme sparse rewards, where traditional learning struggles due to\ninfrequent positive feedback. We propose integrating Variational State as\nIntrinsic Reward (VSIMR), which uses Variational AutoEncoders (VAEs) to reward\nstate novelty, with an intrinsic reward approach derived from Large Language\nModels (LLMs). The LLMs leverage their pre-trained knowledge to generate reward\nsignals based on environment and goal descriptions, guiding the agent. We\nimplemented this combined approach with an Actor-Critic (A2C) agent in the\nMiniGrid DoorKey environment, a benchmark for sparse rewards. Our empirical\nresults show that this combined strategy significantly increases agent\nperformance and sampling efficiency compared to using each strategy\nindividually or a standard A2C agent, which failed to learn. Analysis of\nlearning curves indicates that the combination effectively complements\ndifferent aspects of the environment and task: VSIMR drives exploration of new\nstates, while the LLM-derived rewards facilitate progressive exploitation\ntowards goals.", "AI": {"tldr": "combines Variational State as Intrinsic Reward (VSIMR) with Large Language Models (LLMs) to improve the efficiency of reinforcement learning (RL) agents in environments with extreme sparse rewards", "motivation": "improve the efficiency of reinforcement learning (RL) agents in environments with extreme sparse rewards", "method": "integrating Variational State as Intrinsic Reward (VSIMR) with an intrinsic reward approach derived from Large Language Models (LLMs)", "result": "this combined strategy significantly increases agent performance and sampling efficiency compared to using each strategy individually or a standard A2C agent", "conclusion": "This combined strategy significantly increases agent performance and sampling efficiency compared to using each strategy individually or a standard A2C agent."}}
{"id": "2508.18621", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18621", "abs": "https://arxiv.org/abs/2508.18621", "authors": ["Xin Gao", "Li Hu", "Siqi Hu", "Mingyang Huang", "Chaonan Ji", "Dechao Meng", "Jinwei Qi", "Penchong Qiao", "Zhen Shen", "Yafei Song", "Ke Sun", "Linrui Tian", "Guangyuan Wang", "Qi Wang", "Zhongjian Wang", "Jiayu Xiao", "Sheng Xu", "Bang Zhang", "Peng Zhang", "Xindi Zhang", "Zhe Zhang", "Jingren Zhou", "Lian Zhuo"], "title": "Wan-S2V: Audio-Driven Cinematic Video Generation", "comment": null, "summary": "Current state-of-the-art (SOTA) methods for audio-driven character animation\ndemonstrate promising performance for scenarios primarily involving speech and\nsinging. However, they often fall short in more complex film and television\nproductions, which demand sophisticated elements such as nuanced character\ninteractions, realistic body movements, and dynamic camera work. To address\nthis long-standing challenge of achieving film-level character animation, we\npropose an audio-driven model, which we refere to as Wan-S2V, built upon Wan.\nOur model achieves significantly enhanced expressiveness and fidelity in\ncinematic contexts compared to existing approaches. We conducted extensive\nexperiments, benchmarking our method against cutting-edge models such as\nHunyuan-Avatar and Omnihuman. The experimental results consistently demonstrate\nthat our approach significantly outperforms these existing solutions.\nAdditionally, we explore the versatility of our method through its applications\nin long-form video generation and precise video lip-sync editing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a Wan-S2V \u7684\u97f3\u9891\u9a71\u52a8\u6a21\u578b\uff0c\u7528\u4e8e\u7535\u5f71\u7ea7\u522b\u7684\u89d2\u8272\u52a8\u753b\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u5728\u957f\u89c6\u9891\u751f\u6210\u548c\u5507\u5f62\u540c\u6b65\u65b9\u9762\u5177\u6709\u591a\u529f\u80fd\u6027\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u5728\u590d\u6742\u7684\u7535\u5f71\u548c\u7535\u89c6\u5236\u4f5c\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u65e0\u6cd5\u6ee1\u8db3\u7ec6\u81f4\u7684\u89d2\u8272\u4e92\u52a8\u3001\u903c\u771f\u7684\u8eab\u4f53\u52a8\u4f5c\u548c\u52a8\u6001\u7684\u76f8\u673a\u5de5\u4f5c\u7b49\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a Wan-S2V \u7684\u97f3\u9891\u9a71\u52a8\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5efa\u7acb\u5728 Wan \u7684\u57fa\u7840\u4e0a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e Hunyuan-Avatar \u548c Omnihuman \u7b49\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u3002\u6b64\u5916\uff0c\u8fd8\u901a\u8fc7\u5176\u5728\u957f\u89c6\u9891\u751f\u6210\u548c\u7cbe\u786e\u89c6\u9891\u5507\u5f62\u540c\u6b65\u7f16\u8f91\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u7d22\u4e86\u8be5\u65b9\u6cd5\u7684\u591a\u529f\u80fd\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u7535\u5f71\u7ea7\u522b\u7684\u89d2\u8272\u52a8\u753b\u4e2d\u5b9e\u73b0\u4e86\u589e\u5f3a\u7684\u8868\u8fbe\u6027\u548c\u4fdd\u771f\u5ea6\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.18648", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18648", "abs": "https://arxiv.org/abs/2508.18648", "authors": ["Cong Li", "Wenchang Chai", "Hejun Wu", "Yan Pan", "Pengxu Wei", "Liang Lin"], "title": "Thinking Before You Speak: A Proactive Test-time Scaling Approach", "comment": null, "summary": "Large Language Models (LLMs) often exhibit deficiencies with complex\nreasoning tasks, such as maths, which we attribute to the discrepancy between\nhuman reasoning patterns and those presented in the LLMs' training data. When\ndealing with complex problems, humans tend to think carefully before expressing\nsolutions. However, they often do not articulate their inner thoughts,\nincluding their intentions and chosen methodologies. Consequently, critical\ninsights essential for bridging reasoning steps may be absent in training data\ncollected from human sources. To bridge this gap, we proposes inserting\n\\emph{insight}s between consecutive reasoning steps, which review the status\nand initiate the next reasoning steps. Unlike prior prompting strategies that\nrely on a single or a workflow of static prompts to facilitate reasoning,\n\\emph{insight}s are \\emph{proactively} generated to guide reasoning processes.\nWe implement our idea as a reasoning framework, named \\emph{Thinking Before You\nSpeak} (TBYS), and design a pipeline for automatically collecting and filtering\nin-context examples for the generation of \\emph{insight}s, which alleviates\nhuman labeling efforts and fine-tuning overheads. Experiments on challenging\nmathematical datasets verify the effectiveness of TBYS. Project website:\nhttps://gitee.com/jswrt/TBYS", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u8bf4\u8bdd\u524d\u601d\u8003\u201d(TBYS)\u7684\u63a8\u7406\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5728\u8fde\u7eed\u7684\u63a8\u7406\u6b65\u9aa4\u4e4b\u95f4\u63d2\u5165\u6d1e\u5bdf\u6765\u5f25\u5408\u5dee\u8ddd\uff0c\u4ece\u800c\u51cf\u8f7b\u4e86\u4eba\u5de5\u6807\u6ce8\u5de5\u4f5c\u548c\u5fae\u8c03\u5f00\u9500, \u5e76\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u5b66\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u5728\u590d\u6742\u7684\u63a8\u7406\u4efb\u52a1(\u5982\u6570\u5b66)\u4e2d\u7ecf\u5e38\u8868\u73b0\u51fa\u4e0d\u8db3\uff0c\u6211\u4eec\u5c06\u5176\u5f52\u56e0\u4e8e\u4eba\u7c7b\u63a8\u7406\u6a21\u5f0f\u4e0ellm\u8bad\u7ec3\u6570\u636e\u4e2d\u5448\u73b0\u7684\u6a21\u5f0f\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u5f53\u5904\u7406\u590d\u6742\u95ee\u9898\u65f6\uff0c\u4eba\u7c7b\u503e\u5411\u4e8e\u5728\u8868\u8fbe\u89e3\u51b3\u65b9\u6848\u4e4b\u524d\u4ed4\u7ec6\u601d\u8003\u3002\u7136\u800c\uff0c\u4ed6\u4eec\u901a\u5e38\u4e0d\u8868\u8fbe\u81ea\u5df1\u7684\u5185\u5fc3\u60f3\u6cd5\uff0c\u5305\u62ec\u4ed6\u4eec\u7684\u610f\u56fe\u548c\u9009\u62e9\u7684\u65b9\u6cd5\u3002\u56e0\u6b64\uff0c\u4ece\u4eba\u7c7b\u6765\u6e90\u6536\u96c6\u7684\u8bad\u7ec3\u6570\u636e\u4e2d\u53ef\u80fd\u7f3a\u5c11\u5f25\u5408\u63a8\u7406\u6b65\u9aa4\u7684\u5173\u952e\u89c1\u89e3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u201c\u8bf4\u8bdd\u524d\u601d\u8003\u201d(TBYS)\u7684\u63a8\u7406\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u81ea\u52a8\u6536\u96c6\u548c\u8fc7\u6ee4\u4e0a\u4e0b\u6587\u793a\u4f8b\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u751f\u6210\u6d1e\u5bdf\uff0c\u4ece\u800c\u51cf\u8f7b\u4e86\u4eba\u5de5\u6807\u6ce8\u5de5\u4f5c\u548c\u5fae\u8c03\u5f00\u9500\u3002", "result": "\u5728\u8fde\u7eed\u7684\u63a8\u7406\u6b65\u9aa4\u4e4b\u95f4\u63d2\u5165\u6d1e\u5bdf\uff0c\u56de\u987e\u72b6\u6001\u5e76\u542f\u52a8\u4e0b\u4e00\u4e2a\u63a8\u7406\u6b65\u9aa4\u3002\u4e0e\u4ee5\u5f80\u4f9d\u8d56\u4e8e\u5355\u4e2a\u6216\u9759\u6001\u63d0\u793a\u5de5\u4f5c\u6d41\u6765\u4fc3\u8fdb\u63a8\u7406\u7684\u63d0\u793a\u7b56\u7565\u4e0d\u540c\uff0c\u4e3b\u52a8\u751f\u6210\u6d1e\u5bdf\u6765\u6307\u5bfc\u63a8\u7406\u8fc7\u7a0b\u3002", "conclusion": "TBYS\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u5b66\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002"}}
{"id": "2508.18724", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18724", "abs": "https://arxiv.org/abs/2508.18724", "authors": ["Karanbir Singh", "Deepak Muppiri", "William Ngu"], "title": "Bias Mitigation Agent: Optimizing Source Selection for Fair and Balanced Knowledge Retrieval", "comment": "Accepted at KDD'2025 Agent4IR workshop", "summary": "Large Language Models (LLMs) have transformed the field of artificial\nintelligence by unlocking the era of generative applications. Built on top of\ngenerative AI capabilities, Agentic AI represents a major shift toward\nautonomous, goal-driven systems that can reason, retrieve, and act. However,\nthey also inherit the bias present in both internal and external information\nsources. This significantly affects the fairness and balance of retrieved\ninformation, and hence reduces user trust. To address this critical challenge,\nwe introduce a novel Bias Mitigation Agent, a multi-agent system designed to\norchestrate the workflow of bias mitigation through specialized agents that\noptimize the selection of sources to ensure that the retrieved content is both\nhighly relevant and minimally biased to promote fair and balanced knowledge\ndissemination. The experimental results demonstrate an 81.82\\% reduction in\nbias compared to a baseline naive retrieval strategy.", "AI": {"tldr": "Introduces a Bias Mitigation Agent, a multi-agent system, that reduces bias by 81.82% compared to a baseline naive retrieval strategy.", "motivation": "LLMs inherit the bias present in both internal and external information sources. This significantly affects the fairness and balance of retrieved information, and hence reduces user trust.", "method": "a novel Bias Mitigation Agent, a multi-agent system designed to orchestrate the workflow of bias mitigation through specialized agents that optimize the selection of sources", "result": "81.82% reduction in bias compared to a baseline naive retrieval strategy", "conclusion": "The experimental results demonstrate an 81.82% reduction in bias compared to a baseline naive retrieval strategy."}}
{"id": "2508.18423", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.18423", "abs": "https://arxiv.org/abs/2508.18423", "authors": ["Quanlin Chen", "Yiyu Chen", "Jing Huo", "Tianyu Ding", "Yang Gao", "Yuetong Chen"], "title": "Enhancing Trust-Region Bayesian Optimization via Newton Methods", "comment": null, "summary": "Bayesian Optimization (BO) has been widely applied to optimize expensive\nblack-box functions while retaining sample efficiency. However, scaling BO to\nhigh-dimensional spaces remains challenging. Existing literature proposes\nperforming standard BO in multiple local trust regions (TuRBO) for\nheterogeneous modeling of the objective function and avoiding over-exploration.\nDespite its advantages, using local Gaussian Processes (GPs) reduces sampling\nefficiency compared to a global GP. To enhance sampling efficiency while\npreserving heterogeneous modeling, we propose to construct multiple local\nquadratic models using gradients and Hessians from a global GP, and select new\nsample points by solving the bound-constrained quadratic program. Additionally,\nwe address the issue of vanishing gradients of GPs in high-dimensional spaces.\nWe provide a convergence analysis and demonstrate through experimental results\nthat our method enhances the efficacy of TuRBO and outperforms a wide range of\nhigh-dimensional BO techniques on synthetic functions and real-world\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u7ef4BO\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u5168\u5c40GP\u7684\u68af\u5ea6\u548cHessian\u6784\u5efa\u591a\u4e2a\u5c40\u90e8\u4e8c\u6b21\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u91c7\u6837\u6548\u7387\u5e76\u4fdd\u6301\u5f02\u6784\u5efa\u6a21\u3002", "motivation": "\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u6269\u5c55BO\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u6587\u732e\u63d0\u51fa\u5728\u591a\u4e2a\u5c40\u90e8\u4fe1\u4efb\u533a\u57df\uff08TuRBO\uff09\u4e2d\u6267\u884c\u6807\u51c6BO\uff0c\u4ee5\u5bf9\u76ee\u6807\u51fd\u6570\u8fdb\u884c\u5f02\u6784\u5efa\u6a21\u5e76\u907f\u514d\u8fc7\u5ea6\u63a2\u7d22\u3002\u5c3d\u7ba1\u6709\u5176\u4f18\u70b9\uff0c\u4f46\u4e0e\u5168\u5c40GP\u76f8\u6bd4\uff0c\u4f7f\u7528\u5c40\u90e8\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u4f1a\u964d\u4f4e\u91c7\u6837\u6548\u7387\u3002", "method": "\u5229\u7528\u5168\u5c40GP\u7684\u68af\u5ea6\u548cHessian\u6784\u5efa\u591a\u4e2a\u5c40\u90e8\u4e8c\u6b21\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u6c42\u89e3\u6709\u754c\u7ea6\u675f\u4e8c\u6b21\u89c4\u5212\u6765\u9009\u62e9\u65b0\u7684\u6837\u672c\u70b9\u3002", "result": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86TuRBO\u7684\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u51fd\u6570\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u4f18\u4e8e\u5404\u79cd\u9ad8\u7ef4BO\u6280\u672f\uff0c\u5e76\u63d0\u9ad8\u4e86TuRBO\u7684\u6548\u7387\u3002"}}
{"id": "2508.18632", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18632", "abs": "https://arxiv.org/abs/2508.18632", "authors": ["Huayi Wang", "Haochao Ying", "Yuyang Xu", "Qibo Qiu", "Cheng Zhang", "Danny Z. Chen", "Ying Sun", "Jian Wu"], "title": "Decouple, Reorganize, and Fuse: A Multimodal Framework for Cancer Survival Prediction", "comment": "10 pages", "summary": "Cancer survival analysis commonly integrates information across diverse\nmedical modalities to make survival-time predictions. Existing methods\nprimarily focus on extracting different decoupled features of modalities and\nperforming fusion operations such as concatenation, attention, and MoE-based\n(Mixture-of-Experts) fusion. However, these methods still face two key\nchallenges: i) Fixed fusion schemes (concatenation and attention) can lead to\nmodel over-reliance on predefined feature combinations, limiting the dynamic\nfusion of decoupled features; ii) in MoE-based fusion methods, each expert\nnetwork handles separate decoupled features, which limits information\ninteraction among the decoupled features. To address these challenges, we\npropose a novel Decoupling-Reorganization-Fusion framework (DeReF), which\ndevises a random feature reorganization strategy between modalities decoupling\nand dynamic MoE fusion modules.Its advantages are: i) it increases the\ndiversity of feature combinations and granularity, enhancing the generalization\nability of the subsequent expert networks; ii) it overcomes the problem of\ninformation closure and helps expert networks better capture information among\ndecoupled features. Additionally, we incorporate a regional cross-attention\nnetwork within the modality decoupling module to improve the representation\nquality of decoupled features. Extensive experimental results on our in-house\nLiver Cancer (LC) and three widely used TCGA public datasets confirm the\neffectiveness of our proposed method. The code will be made publicly available.", "AI": {"tldr": "This paper introduces a Decoupling-Reorganization-Fusion framework (DeReF) to address the limitations of existing cancer survival analysis methods. DeReF uses random feature reorganization and dynamic MoE fusion to improve feature combination diversity and information interaction among modalities, achieving better performance on liver cancer and TCGA datasets.", "motivation": "Existing methods primarily focus on extracting different decoupled features of modalities and performing fusion operations such as concatenation, attention, and MoE-based (Mixture-of-Experts) fusion. However, these methods still face two key challenges: i) Fixed fusion schemes (concatenation and attention) can lead to model over-reliance on predefined feature combinations, limiting the dynamic fusion of decoupled features; ii) in MoE-based fusion methods, each expert network handles separate decoupled features, which limits information interaction among the decoupled features.", "method": "We propose a novel Decoupling-Reorganization-Fusion framework (DeReF), which devises a random feature reorganization strategy between modalities decoupling and dynamic MoE fusion modules.  Additionally, we incorporate a regional cross-attention network within the modality decoupling module to improve the representation quality of decoupled features.", "result": "The proposed Decoupling-Reorganization-Fusion framework (DeReF) increases the diversity of feature combinations and granularity, enhancing the generalization ability of the subsequent expert networks; it overcomes the problem of information closure and helps expert networks better capture information among decoupled features.", "conclusion": "Extensive experimental results on our in-house Liver Cancer (LC) and three widely used TCGA public datasets confirm the effectiveness of our proposed method."}}
{"id": "2508.18651", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18651", "abs": "https://arxiv.org/abs/2508.18651", "authors": ["Chenxu Yang", "Qingyi Si", "Zheng Lin"], "title": "Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models", "comment": null, "summary": "Grounding responses in external knowledge represents an effective strategy\nfor mitigating hallucinations in Large Language Models (LLMs). However, current\nLLMs struggle to seamlessly integrate knowledge while simultaneously\nmaintaining faithfulness (or fidelity) and expressiveness, capabilities that\nhumans naturally possess. This limitation results in outputs that either lack\nsupport from external knowledge, thereby compromising faithfulness, or appear\noverly verbose and unnatural, thus sacrificing expressiveness. In this work, to\nbreak the trade-off between faithfulness and expressiveness, we propose\nCollaborative Decoding (CoDe), a novel approach that dynamically integrates\noutput probabilities generated with and without external knowledge. This\nintegration is guided by distribution divergence and model confidence, enabling\nthe selective activation of relevant and reliable expressions from the model's\ninternal parameters. Furthermore, we introduce a knowledge-aware reranking\nmechanism that prevents over-reliance on prior parametric knowledge while\nensuring proper utilization of provided external information. Through\ncomprehensive experiments, our plug-and-play CoDe framework demonstrates\nsuperior performance in enhancing faithfulness without compromising\nexpressiveness across diverse LLMs and evaluation metrics, validating both its\neffectiveness and generalizability.", "AI": {"tldr": "\u4e3a\u4e86\u6253\u7834\u5fe0\u5b9e\u6027\u548c\u8868\u8fbe\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u534f\u4f5c\u89e3\u7801\uff08CoDe\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u52a8\u6001\u6574\u5408\u6709\u65e0\u5916\u90e8\u77e5\u8bc6\u751f\u6210\u8f93\u51fa\u6982\u7387\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u7f1d\u6574\u5408\u77e5\u8bc6\u7684\u540c\u65f6\uff0c\u96be\u4ee5\u4fdd\u6301\u5fe0\u5b9e\u6027\u548c\u8868\u8fbe\u6027\uff0c\u800c\u4eba\u7c7b\u81ea\u7136\u5177\u5907\u8fd9\u4e9b\u80fd\u529b\u3002\u8fd9\u79cd\u9650\u5236\u5bfc\u81f4\u8f93\u51fa\u8981\u4e48\u7f3a\u4e4f\u5916\u90e8\u77e5\u8bc6\u7684\u652f\u6301\uff0c\u4ece\u800c\u635f\u5bb3\u4e86\u5fe0\u5b9e\u6027\uff0c\u8981\u4e48\u663e\u5f97\u8fc7\u4e8e\u5197\u957f\u548c\u4e0d\u81ea\u7136\uff0c\u4ece\u800c\u727a\u7272\u4e86\u8868\u8fbe\u6027\u3002", "method": "\u534f\u4f5c\u89e3\u7801\uff08CoDe\uff09\uff0c\u4e00\u79cd\u52a8\u6001\u6574\u5408\u6709\u65e0\u5916\u90e8\u77e5\u8bc6\u751f\u6210\u8f93\u51fa\u6982\u7387\u7684\u65b0\u65b9\u6cd5\u3002\u8fd9\u79cd\u6574\u5408\u7531\u5206\u5e03\u5dee\u5f02\u548c\u6a21\u578b\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\uff0c\u4ece\u800c\u80fd\u591f\u9009\u62e9\u6027\u5730\u6fc0\u6d3b\u6765\u81ea\u6a21\u578b\u5185\u90e8\u53c2\u6570\u7684\u76f8\u5173\u548c\u53ef\u9760\u7684\u8868\u8fbe\u3002", "result": "\u901a\u8fc7\u5168\u9762\u7684\u5b9e\u9a8c\uff0c", "conclusion": "\u63d0\u51fa\u7684CoDe\u6846\u67b6\u5728\u589e\u5f3a\u5fe0\u5b9e\u6027\u7684\u540c\u65f6\u4e0d\u5f71\u54cd\u5404\u79cdLLM\u548c\u8bc4\u4f30\u6307\u6807\u7684\u8868\u8fbe\u80fd\u529b\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2508.18743", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18743", "abs": "https://arxiv.org/abs/2508.18743", "authors": ["Sunguk Choi", "Yonghoon Kwon", "Heondeuk Lee"], "title": "CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks", "comment": "Accepted at EMNLP 2025 findings", "summary": "Long chain-of-thought (CoT) prompting helps Large Language Models (LLMs)\nsolve difficult problems, but very long traces often slow or even degrade\nperformance on fast, intuitive \"System-1\" tasks. We introduce Connector-Aware\nCompact CoT (CAC-CoT) -- a method that deliberately restricts reasoning to a\nsmall, fixed set of connector phrases, steering the model toward concise and\nwell -- structured explanations. Despite its simplicity, our synthetic method\nwith Gemini-2.0-Flash yields a high-quality training quality. CAC-CoT achieves\napproximately 85% on GSM8K and approximately 40% on GPQA (System-2) while\nretaining approximately 90% on S1-Bench (System-1). Its reasoning traces\naverage approximately 300 tokens(ART), about one-third the length of baseline\ntraces, delivering higher efficiency without loss of accuracy.", "AI": {"tldr": "This paper introduces Connector-Aware Compact CoT (CAC-CoT), a method that restricts reasoning to connector phrases for concise explanations. It achieves good performance on both System-1 and System-2 tasks with shorter reasoning traces.", "motivation": "Long chain-of-thought (CoT) prompting helps Large Language Models (LLMs) solve difficult problems, but very long traces often slow or even degrade performance on fast, intuitive \"System-1\" tasks.", "method": "Connector-Aware Compact CoT (CAC-CoT) -- a method that deliberately restricts reasoning to a small, fixed set of connector phrases, steering the model toward concise and well -- structured explanations.", "result": "CAC-CoT achieves approximately 85% on GSM8K and approximately 40% on GPQA (System-2) while retaining approximately 90% on S1-Bench (System-1).", "conclusion": "CAC-CoT achieves approximately 85% on GSM8K and approximately 40% on GPQA (System-2) while retaining approximately 90% on S1-Bench (System-1). Its reasoning traces average approximately 300 tokens(ART), about one-third the length of baseline traces, delivering higher efficiency without loss of accuracy."}}
{"id": "2508.18462", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18462", "abs": "https://arxiv.org/abs/2508.18462", "authors": ["Fu Teng", "Miao Pan", "Xuhong Zhang", "Zhezhi He", "Yiyao Yang", "Xinyi Chai", "Mengnan Qi", "Liqiang Lu", "Jianwei Yin"], "title": "VERIRL: Boosting the LLM-based Verilog Code Generation via Reinforcement Learning", "comment": null, "summary": "Recent advancements in code generation have shown remarkable success across\nsoftware domains, yet hardware description languages (HDLs) such as Verilog\nremain underexplored due to their concurrency semantics, syntactic rigidity,\nand simulation complexity. In this work, we address these challenges by\nintroducing a reinforcement learning (RL) framework tailored for Verilog code\ngeneration. We first construct Veribench-53K, a high-quality dataset curated\nfrom over 700K Verilog problems, enriched with structured prompts, complexity\nlabels, and diverse testbenches. To tackle the problem of sparse and noisy\nreward signals, we propose a Trace-back based Rescore mechanism that leverages\nreasoning paths and iterative refinement to enhance feedback reliability and\nsupport reward model training. Furthermore, to mitigate catastrophic forgetting\nand overfitting during RL fine-tuning, we introduce a sample-balanced weighting\nstrategy that adaptively balances learning dynamics based on reward-probability\ndistributions. These innovations are integrated into an iterative RL pipeline\nthat co-evolves the policy and reward models. In contrast to recent work such\nas CraftRTL, which relies on large-scale closed-source model distillation, and\nDeepSeek-style approaches that struggle with sparse feedback, our method\ndemonstrates superior performance using a smaller but high-quality dataset\ncombined with RL optimization. Experiments on Verilog generation tasks\ndemonstrate state-of-the-art performance, with substantial gains in test pass\nrate, functional correctness, and compilation robustness. Our findings\nhighlight the potential of RL-driven approaches for structured code generation\nin hardware-centric domains. VERIRL is publicly available at\nhttps://github.com/omniAI-Lab/VeriRL.", "AI": {"tldr": "Introduces a RL framework for Verilog code generation with a new dataset and techniques to improve reward signals and learning dynamics, achieving state-of-the-art performance.", "motivation": "HDLs such as Verilog remain underexplored due to their concurrency semantics, syntactic rigidity, and simulation complexity.", "method": "A reinforcement learning (RL) framework tailored for Verilog code generation with Trace-back based Rescore mechanism and a sample-balanced weighting strategy.", "result": "State-of-the-art performance in Verilog generation tasks, with substantial gains in test pass rate, functional correctness, and compilation robustness.", "conclusion": "RL-driven approaches have potential for structured code generation in hardware-centric domains."}}
{"id": "2508.18633", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18633", "abs": "https://arxiv.org/abs/2508.18633", "authors": ["Chenxuan Miao", "Yutong Feng", "Jianshu Zeng", "Zixiang Gao", "Hantang Liu", "Yunfeng Yan", "Donglian Qi", "Xi Chen", "Bin Wang", "Hengshuang Zhao"], "title": "ROSE: Remove Objects with Side Effects in Videos", "comment": null, "summary": "Video object removal has achieved advanced performance due to the recent\nsuccess of video generative models. However, when addressing the side effects\nof objects, e.g., their shadows and reflections, existing works struggle to\neliminate these effects for the scarcity of paired video data as supervision.\nThis paper presents ROSE, termed Remove Objects with Side Effects, a framework\nthat systematically studies the object's effects on environment, which can be\ncategorized into five common cases: shadows, reflections, light, translucency\nand mirror. Given the challenges of curating paired videos exhibiting the\naforementioned effects, we leverage a 3D rendering engine for synthetic data\ngeneration. We carefully construct a fully-automatic pipeline for data\npreparation, which simulates a large-scale paired dataset with diverse scenes,\nobjects, shooting angles, and camera trajectories. ROSE is implemented as an\nvideo inpainting model built on diffusion transformer. To localize all\nobject-correlated areas, the entire video is fed into the model for\nreference-based erasing. Moreover, additional supervision is introduced to\nexplicitly predict the areas affected by side effects, which can be revealed\nthrough the differential mask between the paired videos. To fully investigate\nthe model performance on various side effect removal, we presents a new\nbenchmark, dubbed ROSE-Bench, incorporating both common scenarios and the five\nspecial side effects for comprehensive evaluation. Experimental results\ndemonstrate that ROSE achieves superior performance compared to existing video\nobject erasing models and generalizes well to real-world video scenarios. The\nproject page is https://rose2025-inpaint.github.io/.", "AI": {"tldr": "ROSE\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u6269\u6563Transformer\u6a21\u578b\uff0c\u6709\u6548\u79fb\u9664\u4e86\u89c6\u9891\u4e2d\u7684\u7269\u4f53\u53ca\u5176\u9634\u5f71\u548c\u53cd\u5c04\u7b49\u526f\u4f5c\u7528\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6d88\u9664\u7269\u4f53\u9634\u5f71\u548c\u53cd\u5c04\u7b49\u526f\u4f5c\u7528\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u914d\u5bf9\u89c6\u9891\u6570\u636e\u4f5c\u4e3a\u76d1\u7763\u3002", "method": "\u5229\u75283D\u6e32\u67d3\u5f15\u64ce\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u6784\u5efa\u5168\u81ea\u52a8\u6570\u636e\u51c6\u5907\u6d41\u7a0b\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u6269\u6563Transformer\u7684\u89c6\u9891\u4fee\u590d\u6a21\u578bROSE\uff0c\u901a\u8fc7\u5f15\u5165\u989d\u5916\u7684\u76d1\u7763\u6765\u9884\u6d4b\u53d7\u526f\u4f5c\u7528\u5f71\u54cd\u7684\u533a\u57df\u3002", "result": "ROSE\u5728\u79fb\u9664\u5404\u79cd\u526f\u4f5c\u7528\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u5e76\u5728\u65b0\u7684\u57fa\u51c6ROSE-Bench\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u3002", "conclusion": "ROSE\u5728\u89c6\u9891\u7269\u4f53\u79fb\u9664\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u7269\u4f53\u9634\u5f71\u548c\u53cd\u5c04\u7b49\u526f\u4f5c\u7528\u65f6\uff0c\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.18655", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.18655", "abs": "https://arxiv.org/abs/2508.18655", "authors": ["Haoyu Wang", "Guangyan Zhang", "Jiale Chen", "Jingyu Li", "Yuehai Wang", "Yiwen Guo"], "title": "Emotion Omni: Enabling Empathetic Speech Response Generation through Large Language Models", "comment": "5 pages, 1 figure, submitted to ICASSP 2026", "summary": "With the development of speech large language models (speech LLMs), users can\nnow interact directly with assistants via speech. However, most existing models\nsimply convert the response content into speech without fully understanding the\nrich emotional and paralinguistic cues embedded in the user's query. In many\ncases, the same sentence can have different meanings depending on the emotional\nexpression. Furthermore, emotional understanding is essential for improving\nuser experience in human-machine interaction. Currently, most speech LLMs with\nempathetic capabilities are trained on massive datasets. This approach requires\nvast amounts of data and significant computational resources. Therefore, a key\nchallenge lies in how to develop a speech LLM capable of generating empathetic\nresponses with limited data and without the need for large-scale training. To\naddress this challenge, we propose Emotion Omni, a novel model architecture\ndesigned to understand the emotional content of user speech input and generate\nempathetic speech responses. Additionally, we developed a data generation\npipeline based on an open-source TTS framework to construct a 200k emotional\ndialogue dataset, which supports the construction of an empathetic speech\nassistant. The demos are available at https://w311411.github.io/omni_demo/", "AI": {"tldr": "This paper proposes Emotion Omni, a speech LLM architecture for generating empathetic responses with limited data, and introduces a 200k emotional dialogue dataset.", "motivation": "Existing speech LLMs lack emotional understanding, which is crucial for user experience but requires extensive data and resources to achieve. This paper aims to develop an empathetic speech LLM with limited data and without large-scale training.", "method": "A novel model architecture (Emotion Omni) and a data generation pipeline based on an open-source TTS framework.", "result": "A 200k emotional dialogue dataset and a novel model architecture (Emotion Omni). Demos are available at https://w311411.github.io/omni_demo/", "conclusion": "The paper introduces Emotion Omni, a novel model architecture for understanding emotional content in user speech and generating empathetic speech responses. A 200k emotional dialogue dataset was also created to support this."}}
{"id": "2508.18749", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18749", "abs": "https://arxiv.org/abs/2508.18749", "authors": ["Chunlong Wu", "Zhibo Qu"], "title": "Reflection-Enhanced Meta-Optimization Integrating TextGrad-style Prompt Optimization with Memory-Driven Self-Evolution", "comment": null, "summary": "Recent advances in prompt optimization, exemplified by methods such as\nTextGrad, enable automatic, gradient-like refinement of textual prompts to\nenhance the performance of large language models (LLMs) on specific downstream\ntasks. However, current approaches are typically stateless and operate\nindependently across optimization runs, lacking mechanisms to preserve and\nleverage historical optimization experience. Furthermore, they are susceptible\nto overfitting, often yielding prompt updates that generalize poorly beyond the\nimmediate task context.\n  To address these limitations, we propose Reflection-Enhanced\nMeta-Optimization (REMO), a novel framework that integrates (1) a\nmemory-augmented Reflection Retrieval-Augmented Generation (RAG) module -\nstructured as a \"mistake notebook\" and (2) a Self-Adaptive Optimizer,\nimplemented via an LLM-driven meta-controller that synthesizes epoch-level\nreflective insights to iteratively improve system-level prompting strategies.\nThis architecture enables not only local, fine-grained prompt tuning akin to\nTextGrad, but also the systematic accumulation and reuse of cross-run\noptimization knowledge, thereby supporting continual improvement over time.\n  We instantiate the REMO framework using Qwen3-32B in standard inference mode\n- without explicit chain-of-thought prompting - and evaluate its efficacy on\nthe GSM8K benchmark for mathematical reasoning. Experimental results\ndemonstrate that, compared to a TextGrad baseline, REMO achieves more stable\nand robust generalization, albeit at the cost of increased computational\noverhead. We provide a detailed exposition of the algorithmic design, conduct a\nqualitative and quantitative analysis of optimization dynamics, and present a\ncomprehensive ablation study to elucidate the contributions of each component.", "AI": {"tldr": "REMO\u901a\u8fc7\u96c6\u6210\u8bb0\u5fc6\u589e\u5f3a\u7684RAG\u6a21\u5757\u548c\u81ea\u9002\u5e94\u4f18\u5316\u5668\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u548c\u9c81\u68d2\u7684\u6cdb\u5316\uff0c\u4f46\u8ba1\u7b97\u5f00\u9500\u589e\u52a0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u662f\u65e0\u72b6\u6001\u7684\uff0c\u5e76\u4e14\u5728\u4f18\u5316\u8fd0\u884c\u4e2d\u72ec\u7acb\u8fd0\u884c\uff0c\u7f3a\u4e4f\u4fdd\u7559\u548c\u5229\u7528\u5386\u53f2\u4f18\u5316\u7ecf\u9a8c\u7684\u673a\u5236\uff0c\u5bb9\u6613\u8fc7\u5ea6\u62df\u5408\uff0c\u4ea7\u751f\u8d85\u51fa\u76f4\u63a5\u4efb\u52a1\u4e0a\u4e0b\u6587\u7684\u6cdb\u5316\u6027\u5dee\u7684\u63d0\u793a\u66f4\u65b0\u3002", "method": "\u63d0\u51fa\u4e86Reflection-Enhanced Meta-Optimization (REMO) \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u8bb0\u5fc6\u589e\u5f3a\u7684 Reflection Retrieval-Augmented Generation (RAG) \u6a21\u5757\uff08\u7ed3\u6784\u4e3a\u201c\u9519\u8bef\u7b14\u8bb0\u672c\u201d\uff09\u548c\u4e00\u4e2a\u81ea\u9002\u5e94\u4f18\u5316\u5668\uff08\u901a\u8fc7 LLM \u9a71\u52a8\u7684\u5143\u63a7\u5236\u5668\u5b9e\u73b0\uff09\u3002", "result": "\u4e0e TextGrad \u57fa\u7ebf\u76f8\u6bd4\uff0cREMO \u5728 GSM8K \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u548c\u9c81\u68d2\u7684\u6cdb\u5316\uff0c\u4f46\u8ba1\u7b97\u5f00\u9500\u589e\u52a0\u3002", "conclusion": "REMO\u5728GSM8K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u548c\u9c81\u68d2\u7684\u6cdb\u5316\uff0c\u4f46\u8ba1\u7b97\u5f00\u9500\u589e\u52a0\u3002"}}
{"id": "2508.18474", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18474", "abs": "https://arxiv.org/abs/2508.18474", "authors": ["Bahareh Golchin", "Banafsheh Rekabdar", "Kunpeng Liu"], "title": "DRTA: Dynamic Reward Scaling for Reinforcement Learning in Time Series Anomaly Detection", "comment": null, "summary": "Anomaly detection in time series data is important for applications in\nfinance, healthcare, sensor networks, and industrial monitoring. Traditional\nmethods usually struggle with limited labeled data, high false-positive rates,\nand difficulty generalizing to novel anomaly types. To overcome these\nchallenges, we propose a reinforcement learning-based framework that integrates\ndynamic reward shaping, Variational Autoencoder (VAE), and active learning,\ncalled DRTA. Our method uses an adaptive reward mechanism that balances\nexploration and exploitation by dynamically scaling the effect of VAE-based\nreconstruction error and classification rewards. This approach enables the\nagent to detect anomalies effectively in low-label systems while maintaining\nhigh precision and recall. Our experimental results on the Yahoo A1 and Yahoo\nA2 benchmark datasets demonstrate that the proposed method consistently\noutperforms state-of-the-art unsupervised and semi-supervised approaches. These\nfindings show that our framework is a scalable and efficient solution for\nreal-world anomaly detection tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f18\u4e8e\u73b0\u6709\u7684\u65e0\u76d1\u7763\u548c\u534a\u76d1\u7763\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u96be\u4ee5\u5904\u7406\u6709\u9650\u7684\u6807\u8bb0\u6570\u636e\u3001\u9ad8\u8bef\u62a5\u7387\u4ee5\u53ca\u96be\u4ee5\u63a8\u5e7f\u5230\u65b0\u578b\u5f02\u5e38\u7c7b\u578b\u3002", "method": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u52a8\u6001\u5956\u52b1\u5851\u9020\u3001\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668 (VAE) \u548c\u4e3b\u52a8\u5b66\u4e60 (DRTA)\u3002", "result": "\u5728 Yahoo A1 \u548c Yahoo A2 \u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65e0\u76d1\u7763\u548c\u534a\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u5b9e\u9645\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2508.18634", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18634", "abs": "https://arxiv.org/abs/2508.18634", "authors": ["Chunlin Zhong", "Qiuxia Hou", "Zhangjun Zhou", "Shuang Hao", "Haonan Lu", "Yanhao Zhang", "He Tang", "Xiang Bai"], "title": "OwlCap: Harmonizing Motion-Detail for Video Captioning via HMD-270K and Caption Set Equivalence Reward", "comment": "9 pages, 6figures", "summary": "Video captioning aims to generate comprehensive and coherent descriptions of\nthe video content, contributing to the advancement of both video understanding\nand generation. However, existing methods often suffer from motion-detail\nimbalance, as models tend to overemphasize one aspect while neglecting the\nother. This imbalance results in incomplete captions, which in turn leads to a\nlack of consistency in video understanding and generation. To address this\nissue, we propose solutions from two aspects: 1) Data aspect: We constructed\nthe Harmonizing Motion-Detail 270K (HMD-270K) dataset through a two-stage\npipeline: Motion-Detail Fusion (MDF) and Fine-Grained Examination (FGE). 2)\nOptimization aspect: We introduce the Caption Set Equivalence Reward (CSER)\nbased on Group Relative Policy Optimization (GRPO). CSER enhances completeness\nand accuracy in capturing both motion and details through unit-to-set matching\nand bidirectional validation. Based on the HMD-270K supervised fine-tuning and\nGRPO post-training with CSER, we developed OwlCap, a powerful video captioning\nmulti-modal large language model (MLLM) with motion-detail balance.\nExperimental results demonstrate that OwlCap achieves significant improvements\ncompared to baseline models on two benchmarks: the detail-focused VDC (+4.2\nAcc) and the motion-focused DREAM-1K (+4.6 F1). The HMD-270K dataset and OwlCap\nmodel will be publicly released to facilitate video captioning research\ncommunity advancements.", "AI": {"tldr": "Proposes solutions from two aspects: 1) Data aspect: Constructed the Harmonizing Motion-Detail 270K (HMD-270K) dataset. 2) Optimization aspect: Introduce the Caption Set Equivalence Reward (CSER) based on Group Relative Policy Optimization (GRPO).", "motivation": "Existing methods often suffer from motion-detail imbalance, as models tend to overemphasize one aspect while neglecting the other. This imbalance results in incomplete captions, which in turn leads to a lack of consistency in video understanding and generation.", "method": "We introduce the Caption Set Equivalence Reward (CSER) based on Group Relative Policy Optimization (GRPO).", "result": "Developed OwlCap, a powerful video captioning multi-modal large language model (MLLM) with motion-detail balance.", "conclusion": "OwlCap achieves significant improvements compared to baseline models on two benchmarks: the detail-focused VDC (+4.2 Acc) and the motion-focused DREAM-1K (+4.6 F1)."}}
{"id": "2508.18673", "categories": ["cs.CL", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.18673", "abs": "https://arxiv.org/abs/2508.18673", "authors": ["Xinglong Yang", "Quan Feng", "Zhongying Pan", "Xiang Chen", "Yu Tian", "Wentong Li", "Shuofei Qiao", "Yuxia Geng", "Xingyu Zhao", "Sheng-Jun Huang"], "title": "Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum", "comment": null, "summary": "The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often\nlimited by the use of randomly or manually selected examples. These examples\nfail to account for both model-specific knowledge distributions and the\nintrinsic complexity of the tasks, resulting in suboptimal and unstable model\nperformance. To address this, we propose a novel framework inspired by the\npedagogical principle of \"tailored teaching with balanced difficulty\". We\nreframe prompt selection as a prompt curriculum design problem: constructing a\nwell ordered set of training examples that align with the model's current\ncapabilities. Our approach integrates two complementary signals: (1)\nmodel-perceived difficulty, quantified through prediction disagreement in an\nactive learning setup, capturing what the model itself finds challenging; and\n(2) intrinsic sample complexity, which measures the inherent difficulty of each\nquestion-image pair independently of any model. By jointly analyzing these\nsignals, we develop a difficulty-balanced sampling strategy that ensures the\nselected prompt examples are diverse across both dimensions. Extensive\nexperiments conducted on five challenging benchmarks and multiple popular\nMultimodal Large Language Models (MLLMs) demonstrate that our method yields\nsubstantial and consistent improvements and greatly reduces performance\ndiscrepancies caused by random sampling, providing a principled and robust\napproach for enhancing multimodal reasoning.", "AI": {"tldr": "This paper presents a new method for selecting better prompts in multimodal reasoning by considering both how difficult the model finds a task and how inherently complex the task is.", "motivation": "The effectiveness of MCoT prompting is often limited by randomly or manually selected examples that don't account for model-specific knowledge and task complexity.", "method": "The proposed framework integrates model-perceived difficulty and intrinsic sample complexity to select prompt examples.", "result": "Experiments on five benchmarks and multiple Multimodal Large Language Models (MLLMs) show substantial and consistent improvements.", "conclusion": "This paper introduces a difficulty-balanced sampling strategy for prompt selection in Multimodal Chain-of-Thought (MCoT) prompting, which improves performance and reduces discrepancies caused by random sampling."}}
{"id": "2508.18751", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18751", "abs": "https://arxiv.org/abs/2508.18751", "authors": ["Byung-Joon Lee", "Jin-Seop Lee", "Jee-Hyong Lee"], "title": "Stabilizing Open-Set Test-Time Adaptation via Primary-Auxiliary Filtering and Knowledge-Integrated Prediction", "comment": "Accepted at BMVC 2025", "summary": "Deep neural networks demonstrate strong performance under aligned\ntraining-test distributions. However, real-world test data often exhibit domain\nshifts. Test-Time Adaptation (TTA) addresses this challenge by adapting the\nmodel to test data during inference. While most TTA studies assume that the\ntraining and test data share the same class set (closed-set TTA), real-world\nscenarios often involve open-set data (open-set TTA), which can degrade\nclosed-set accuracy. A recent study showed that identifying open-set data\nduring adaptation and maximizing its entropy is an effective solution. However,\nthe previous method relies on the source model for filtering, resulting in\nsuboptimal filtering accuracy on domain-shifted test data. In contrast, we\nfound that the adapting model, which learns domain knowledge from noisy test\nstreams, tends to be unstable and leads to error accumulation when used for\nfiltering. To address this problem, we propose Primary-Auxiliary Filtering\n(PAF), which employs an auxiliary filter to validate data filtered by the\nprimary filter. Furthermore, we propose Knowledge-Integrated Prediction (KIP),\nwhich calibrates the outputs of the adapting model, EMA model, and source model\nto integrate their complementary knowledge for OSTTA. We validate our approach\nacross diverse closed-set and open-set datasets. Our method enhances both\nclosed-set accuracy and open-set discrimination over existing methods. The code\nis available at https://github.com/powerpowe/PAF-KIP-OSTTA .", "AI": {"tldr": "This paper tackles open-set Test-Time Adaptation (OSTTA) by proposing a filtering method and a prediction calibration technique to improve accuracy and discrimination. ", "motivation": "Real-world test data often exhibit domain shifts, and open-set data can degrade closed-set accuracy in Test-Time Adaptation (TTA). Existing methods rely on the source model for filtering, resulting in suboptimal filtering accuracy, or use the adapting model, which can be unstable.", "method": "The paper proposes Primary-Auxiliary Filtering (PAF), which uses an auxiliary filter to validate data filtered by the primary filter, and Knowledge-Integrated Prediction (KIP), which calibrates the outputs of the adapting model, EMA model, and source model.", "result": "The proposed method enhances both closed-set accuracy and open-set discrimination over existing methods across diverse closed-set and open-set datasets.", "conclusion": "This paper introduces Primary-Auxiliary Filtering (PAF) and Knowledge-Integrated Prediction (KIP) to improve both closed-set accuracy and open-set discrimination in open-set Test-Time Adaptation (OSTTA). The approach is validated on diverse datasets, demonstrating improvements over existing methods."}}
{"id": "2508.18502", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18502", "abs": "https://arxiv.org/abs/2508.18502", "authors": ["Andreza M. C. Falcao", "Filipe R. Cordeiro"], "title": "Data Augmentation Improves Machine Unlearning", "comment": "Paper accepted at SIBGRAPI'25", "summary": "Machine Unlearning (MU) aims to remove the influence of specific data from a\ntrained model while preserving its performance on the remaining data. Although\na few works suggest connections between memorisation and augmentation, the role\nof systematic augmentation design in MU remains under-investigated. In this\nwork, we investigate the impact of different data augmentation strategies on\nthe performance of unlearning methods, including SalUn, Random Label, and\nFine-Tuning. Experiments conducted on CIFAR-10 and CIFAR-100, under varying\nforget rates, show that proper augmentation design can significantly improve\nunlearning effectiveness, reducing the performance gap to retrained models.\nResults showed a reduction of up to 40.12% of the Average Gap unlearning\nMetric, when using TrivialAug augmentation. Our results suggest that\naugmentation not only helps reduce memorization but also plays a crucial role\nin achieving privacy-preserving and efficient unlearning.", "AI": {"tldr": "Data augmentation improves machine unlearning effectiveness and reduces the performance gap to retrained models.", "motivation": "The role of systematic augmentation design in machine unlearning remains under-investigated.", "method": "Investigating the impact of different data augmentation strategies on the performance of unlearning methods, including SalUn, Random Label, and Fine-Tuning.", "result": "Using TrivialAug augmentation, the Average Gap unlearning Metric is reduced by up to 40.12%.", "conclusion": "Proper augmentation design can significantly improve unlearning effectiveness, reducing the performance gap to retrained models and achieving privacy-preserving and efficient unlearning."}}
{"id": "2508.18641", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18641", "abs": "https://arxiv.org/abs/2508.18641", "authors": ["Ye Tao", "Xinran Fu", "Honglin Pang", "Xi Yang", "Chuntao Li"], "title": "Clustering-based Feature Representation Learning for Oracle Bone Inscriptions Detection", "comment": null, "summary": "Oracle Bone Inscriptions (OBIs), play a crucial role in understanding ancient\nChinese civilization. The automated detection of OBIs from rubbing images\nrepresents a fundamental yet challenging task in digital archaeology, primarily\ndue to various degradation factors including noise and cracks that limit the\neffectiveness of conventional detection networks. To address these challenges,\nwe propose a novel clustering-based feature space representation learning\nmethod. Our approach uniquely leverages the Oracle Bones Character (OBC) font\nlibrary dataset as prior knowledge to enhance feature extraction in the\ndetection network through clustering-based representation learning. The method\nincorporates a specialized loss function derived from clustering results to\noptimize feature representation, which is then integrated into the total\nnetwork loss. We validate the effectiveness of our method by conducting\nexperiments on two OBIs detection dataset using three mainstream detection\nframeworks: Faster R-CNN, DETR, and Sparse R-CNN. Through extensive\nexperimentation, all frameworks demonstrate significant performance\nimprovements.", "AI": {"tldr": "A new clustering-based method improves OBI detection by using OBC font library data to enhance feature extraction.", "motivation": "Automated detection of OBIs from rubbing images is challenging due to noise and cracks that limit the effectiveness of conventional detection networks.", "method": "A novel clustering-based feature space representation learning method leveraging the OBC font library dataset as prior knowledge.", "result": "Significant performance improvements on two OBIs detection datasets using Faster R-CNN, DETR, and Sparse R-CNN.", "conclusion": "The proposed clustering-based feature space representation learning method improves the performance of OBI detection in Faster R-CNN, DETR, and Sparse R-CNN frameworks."}}
{"id": "2508.18687", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18687", "abs": "https://arxiv.org/abs/2508.18687", "authors": ["Songtao Jiang", "Yuxi Chen", "Sibo Song", "Yan Zhang", "Yeying Jin", "Yang Feng", "Jian Wu", "Zuozhu Liu"], "title": "Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning", "comment": null, "summary": "In high-stakes medical applications, consistent answering across diverse\nquestion phrasings is essential for reliable diagnosis. However, we reveal that\ncurrent Medical Vision-Language Models (Med-VLMs) exhibit concerning fragility\nin Medical Visual Question Answering, as their answers fluctuate significantly\nwhen faced with semantically equivalent rephrasings of medical questions. We\nattribute this to two limitations: (1) insufficient alignment of medical\nconcepts, leading to divergent reasoning patterns, and (2) hidden biases in\ntraining data that prioritize syntactic shortcuts over semantic understanding.\nTo address these challenges, we construct RoMed, a dataset built upon original\nVQA datasets containing 144k questions with variations spanning word-level,\nsentence-level, and semantic-level perturbations. When evaluating\nstate-of-the-art (SOTA) models like LLaVA-Med on RoMed, we observe alarming\nperformance drops (e.g., a 40\\% decline in Recall) compared to original VQA\nbenchmarks, exposing critical robustness gaps. To bridge this gap, we propose\nConsistency and Contrastive Learning (CCL), which integrates two key\ncomponents: (1) knowledge-anchored consistency learning, aligning Med-VLMs with\nmedical knowledge rather than shallow feature patterns, and (2) bias-aware\ncontrastive learning, mitigating data-specific priors through discriminative\nrepresentation refinement. CCL achieves SOTA performance on three popular VQA\nbenchmarks and notably improves answer consistency by 50\\% on the challenging\nRoMed test set, demonstrating significantly enhanced robustness. Code will be\nreleased.", "AI": {"tldr": "Med-VLMs are fragile. RoMed dataset exposes this. CCL improves robustness and consistency.", "motivation": "Current Medical Vision-Language Models (Med-VLMs) exhibit concerning fragility in Medical Visual Question Answering, as their answers fluctuate significantly when faced with semantically equivalent rephrasings of medical questions due to insufficient alignment of medical concepts and hidden biases in training data.", "method": "Consistency and Contrastive Learning (CCL), which integrates knowledge-anchored consistency learning and bias-aware contrastive learning.", "result": "RoMed exposes critical robustness gaps with a 40% decline in Recall on SOTA models like LLaVA-Med. CCL significantly enhances robustness.", "conclusion": "CCL improves answer consistency by 50% on the RoMed test set and achieves SOTA performance on three popular VQA benchmarks, demonstrating significantly enhanced robustness."}}
{"id": "2508.18760", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18760", "abs": "https://arxiv.org/abs/2508.18760", "authors": ["Yi Liu", "Xiangyu Liu", "Zequn Sun", "Wei Hu"], "title": "Answering the Unanswerable Is to Err Knowingly: Analyzing and Mitigating Abstention Failures in Large Reasoning Models", "comment": null, "summary": "Large reasoning models (LRMs) have shown remarkable progress on complex\nreasoning tasks. However, some questions posed to LRMs are inherently\nunanswerable, such as math problems lacking sufficient conditions. We find that\nLRMs continually fail to provide appropriate abstentions when confronted with\nthese unanswerable questions. In this paper, we systematically analyze,\ninvestigate, and resolve this issue for trustworthy AI. We first conduct a\ndetailed analysis of the distinct response behaviors of LRMs when facing\nunanswerable questions. Then, we show that LRMs possess sufficient cognitive\ncapabilities to recognize the flaws in these questions. However, they fail to\nexhibit appropriate abstention behavior, revealing a misalignment between their\ninternal cognition and external response. Finally, to resolve this issue, we\npropose a lightweight, two-stage method that combines cognitive monitoring with\ninference-time intervention. Experimental results demonstrate that our method\nsignificantly improves the abstention rate while maintaining the overall\nreasoning performance.", "AI": {"tldr": "LRMs struggle with unanswerable questions. This paper analyzes why and proposes a method to improve their abstention behavior.", "motivation": "LRMs continually fail to provide appropriate abstentions when confronted with unanswerable questions, revealing a misalignment between their internal cognition and external response.", "method": "A lightweight, two-stage method that combines cognitive monitoring with inference-time intervention.", "result": "The proposed method significantly improves the abstention rate while maintaining the overall reasoning performance.", "conclusion": "This paper proposes a two-stage method combining cognitive monitoring with inference-time intervention to improve the abstention rate of LRMs when facing unanswerable questions, while maintaining overall reasoning performance."}}
{"id": "2508.18514", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.18514", "abs": "https://arxiv.org/abs/2508.18514", "authors": ["Yifeng Peng", "Xinyi Li", "Zhemin Zhang", "Samuel Yen-Chi Chen", "Zhiding Liang", "Ying Wang"], "title": "Breaking Through Barren Plateaus: Reinforcement Learning Initializations for Deep Variational Quantum Circuits", "comment": null, "summary": "Variational Quantum Algorithms (VQAs) have gained prominence as a viable\nframework for exploiting near-term quantum devices in applications ranging from\noptimization and chemistry simulation to machine learning. However, the\neffectiveness of VQAs is often constrained by the so-called barren plateau\nproblem, wherein gradients diminish exponentially as system size or circuit\ndepth increases, thereby hindering training. In this work, we propose a\nreinforcement learning (RL)-based initialization strategy to alleviate the\nbarren plateau issue by reshaping the initial parameter landscape to avoid\nregions prone to vanishing gradients. In particular, we explore several RL\nalgorithms (Deterministic Policy Gradient, Soft Actor-Critic, and Proximal\nPolicy Optimization, etc.) to generate the circuit parameters (treated as\nactions) that minimize the VQAs cost function before standard gradient-based\noptimization. By pre-training with RL in this manner, subsequent optimization\nusing methods such as gradient descent or Adam proceeds from a more favorable\ninitial state. Extensive numerical experiments under various noise conditions\nand tasks consistently demonstrate that the RL-based initialization method\nsignificantly enhances both convergence speed and final solution quality.\nMoreover, comparisons among different RL algorithms highlight that multiple\napproaches can achieve comparable performance gains, underscoring the\nflexibility and robustness of our method. These findings shed light on a\npromising avenue for integrating machine learning techniques into quantum\nalgorithm design, offering insights into how RL-driven parameter initialization\ncan accelerate the scalability and practical deployment of VQAs. Opening up a\npromising path for the research community in machine learning for quantum,\nespecially barren plateau problems in VQAs.", "AI": {"tldr": "RL-based initialization alleviates the barren plateau problem in VQAs, enhancing convergence and solution quality.", "motivation": "VQAs suffer from the barren plateau problem, where gradients diminish exponentially, hindering training.", "method": "Reinforcement learning (RL) algorithms (Deterministic Policy Gradient, Soft Actor-Critic, and Proximal Policy Optimization, etc.) are used to generate circuit parameters that minimize the VQAs cost function before gradient-based optimization.", "result": "RL-based initialization enhances convergence speed and final solution quality under various noise conditions and tasks. Multiple RL approaches achieve comparable performance gains.", "conclusion": "RL-based initialization significantly enhances VQA convergence speed and solution quality, offering a promising avenue for integrating machine learning into quantum algorithm design."}}
{"id": "2508.18664", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18664", "abs": "https://arxiv.org/abs/2508.18664", "authors": ["Xin Tian", "Yingtie Lei", "Xiujun Zhang", "Zimeng Li", "Chi-Man Pun", "Xuhang Chen"], "title": "SFormer: SNR-guided Transformer for Underwater Image Enhancement from the Frequency Domain", "comment": "Accepted by PRICAI2025", "summary": "Recent learning-based underwater image enhancement (UIE) methods have\nadvanced by incorporating physical priors into deep neural networks,\nparticularly using the signal-to-noise ratio (SNR) prior to reduce\nwavelength-dependent attenuation. However, spatial domain SNR priors have two\nlimitations: (i) they cannot effectively separate cross-channel interference,\nand (ii) they provide limited help in amplifying informative structures while\nsuppressing noise. To overcome these, we propose using the SNR prior in the\nfrequency domain, decomposing features into amplitude and phase spectra for\nbetter channel modulation. We introduce the Fourier Attention SNR-prior\nTransformer (FAST), combining spectral interactions with SNR cues to highlight\nkey spectral components. Additionally, the Frequency Adaptive Transformer (FAT)\nbottleneck merges low- and high-frequency branches using a gated attention\nmechanism to enhance perceptual quality. Embedded in a unified U-shaped\narchitecture, these modules integrate a conventional RGB stream with an\nSNR-guided branch, forming SFormer. Trained on 4,800 paired images from UIEB,\nEUVP, and LSUI, SFormer surpasses recent methods with a 3.1 dB gain in PSNR and\n0.08 in SSIM, successfully restoring colors, textures, and contrast in\nunderwater scenes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSFormer\u7684\u65b0\u7684\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u9891\u57dfSNR\u5148\u9a8c\u548cTransformer\u6a21\u5757\u6765\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a(UIE)\u65b9\u6cd5\u901a\u8fc7\u5c06\u7269\u7406\u5148\u9a8c\u7eb3\u5165\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u800c\u5f97\u5230\u53d1\u5c55\uff0c\u7279\u522b\u662f\u4f7f\u7528\u4fe1\u566a\u6bd4(SNR)\u5148\u9a8c\u6765\u51cf\u5c11\u6ce2\u957f\u76f8\u5173\u7684\u8870\u51cf\u3002\u7136\u800c\uff0c\u7a7a\u95f4\u57dfSNR\u5148\u9a8c\u6709\u4e24\u4e2a\u5c40\u9650\u6027\uff1a(i)\u5b83\u4eec\u4e0d\u80fd\u6709\u6548\u5730\u5206\u79bb\u8de8\u901a\u9053\u5e72\u6270\uff0c(ii)\u5b83\u4eec\u5728\u653e\u5927\u4fe1\u606f\u7ed3\u6784\u7684\u540c\u65f6\u6291\u5236\u566a\u58f0\u65b9\u9762\u63d0\u4f9b\u7684\u5e2e\u52a9\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u9891\u57df\u4e2d\u7684SNR\u5148\u9a8c\uff0c\u5c06\u7279\u5f81\u5206\u89e3\u4e3a\u5e45\u5ea6\u548c\u76f8\u4f4d\u8c31\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u901a\u9053\u8c03\u5236\u3002\u5f15\u5165\u4e86\u5085\u91cc\u53f6\u6ce8\u610f\u529bSNR\u5148\u9a8cTransformer (FAST)\uff0c\u5c06\u9891\u8c31\u4ea4\u4e92\u4e0eSNR\u7ebf\u7d22\u76f8\u7ed3\u5408\uff0c\u4ee5\u7a81\u51fa\u5173\u952e\u9891\u8c31\u5206\u91cf\u3002\u6b64\u5916\uff0c\u9891\u7387\u81ea\u9002\u5e94Transformer (FAT)\u74f6\u9888\u4f7f\u7528\u95e8\u63a7\u6ce8\u610f\u529b\u673a\u5236\u5408\u5e76\u4f4e\u9891\u548c\u9ad8\u9891\u5206\u652f\uff0c\u4ee5\u63d0\u9ad8\u611f\u77e5\u8d28\u91cf\u3002\u8fd9\u4e9b\u6a21\u5757\u5d4c\u5165\u5728\u7edf\u4e00\u7684U\u578b\u67b6\u6784\u4e2d\uff0c\u96c6\u6210\u4e86\u4f20\u7edf\u7684RGB\u6d41\u548cSNR\u5f15\u5bfc\u7684\u5206\u652f\uff0c\u5f62\u6210\u4e86SFormer\u3002", "result": "SFormer\u5728PSNR\u548cSSIM\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "SFormer\u5728UIEB\u3001EUVP\u548cLSUI\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u6027\u80fd\u8d85\u8fc7\u4e86\u73b0\u6709\u65b9\u6cd5\uff0cPSNR\u63d0\u9ad8\u4e863.1 dB\uff0cSSIM\u63d0\u9ad8\u4e860.08\uff0c\u6210\u529f\u5730\u6062\u590d\u4e86\u6c34\u4e0b\u573a\u666f\u4e2d\u7684\u989c\u8272\u3001\u7eb9\u7406\u548c\u5bf9\u6bd4\u5ea6\u3002"}}
{"id": "2508.18701", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18701", "abs": "https://arxiv.org/abs/2508.18701", "authors": ["Yanfan Du", "Jun Zhang", "Bin Wang", "Jin Qiu", "Lu Huang", "Yuan Ge", "Xiaoqian Liu", "Tong Xiao", "Jingbo Zhu"], "title": "Attention2Probability: Attention-Driven Terminology Probability Estimation for Robust Speech-to-Text System", "comment": "9 pages, 4 figures, 5 tables", "summary": "Recent advances in speech large language models (SLMs) have improved speech\nrecognition and translation in general domains, but accurately generating\ndomain-specific terms or neologisms remains challenging. To address this, we\npropose Attention2Probability: attention-driven terminology probability\nestimation for robust speech-to-text system, which is lightweight, flexible,\nand accurate. Attention2Probability converts cross-attention weights between\nspeech and terminology into presence probabilities, and it further employs\ncurriculum learning to enhance retrieval accuracy. Furthermore, to tackle the\nlack of data for speech-to-text tasks with terminology intervention, we create\nand release a new speech dataset with terminology to support future research in\nthis area. Experimental results show that Attention2Probability significantly\noutperforms the VectorDB method on our test set. Specifically, its maximum\nrecall rates reach 92.57% for Chinese and 86.83% for English. This high recall\nis achieved with a latency of only 8.71ms per query. Intervening in SLMs'\nrecognition and translation tasks using Attention2Probability-retrieved terms\nimproves terminology accuracy by 6-17%, while revealing that the current\nutilization of terminology by SLMs has limitations.", "AI": {"tldr": "Attention2Probability converts cross-attention weights between speech and terminology into presence probabilities, and it further employs curriculum learning to enhance retrieval accuracy. create and release a new speech dataset with terminology to support future research", "motivation": "accurately generating domain-specific terms or neologisms remains challenging in speech recognition and translation", "method": "Attention2Probability: attention-driven terminology probability estimation", "result": "maximum recall rates reach 92.57% for Chinese and 86.83% for English with a latency of only 8.71ms per query, outperforms the VectorDB method", "conclusion": "Attention2Probability-retrieved terms improves terminology accuracy by 6-17%, while revealing that the current utilization of terminology by SLMs has limitations."}}
{"id": "2508.18763", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18763", "abs": "https://arxiv.org/abs/2508.18763", "authors": ["Chao Hao", "Zezheng Wang", "Yanhua Huang", "Ruiwen Xu", "Wenzhe Niu", "Xin Liu", "Zitong Yu"], "title": "Dynamic Collaboration of Multi-Language Models based on Minimal Complete Semantic Units", "comment": "Accepted by EMNLP 2025 Main Conference", "summary": "This paper investigates the enhancement of reasoning capabilities in language\nmodels through token-level multi-model collaboration. Our approach selects the\noptimal tokens from the next token distributions provided by multiple models to\nperform autoregressive reasoning. Contrary to the assumption that more models\nyield better results, we introduce a distribution distance-based dynamic\nselection strategy (DDS) to optimize the multi-model collaboration process. To\naddress the critical challenge of vocabulary misalignment in multi-model\ncollaboration, we propose the concept of minimal complete semantic units\n(MCSU), which is simple yet enables multiple language models to achieve natural\nalignment within the linguistic space. Experimental results across various\nbenchmarks demonstrate the superiority of our method. The code will be\navailable at https://github.com/Fanye12/DDS.", "AI": {"tldr": "This paper introduces DDS and MCSU to improve reasoning in language models by intelligently combining multiple models at the token level, achieving better performance on various benchmarks.", "motivation": "Investigates the enhancement of reasoning capabilities in language models through token-level multi-model collaboration. Addresses the challenge of vocabulary misalignment in multi-model collaboration.", "method": "Introduces a distribution distance-based dynamic selection strategy (DDS) and the concept of minimal complete semantic units (MCSU) for token-level multi-model collaboration.", "result": "Experimental results across various benchmarks demonstrate the superiority of our method.", "conclusion": "The proposed method demonstrates superiority across various benchmarks."}}
{"id": "2508.18526", "categories": ["cs.LG", "cs.CC", "cs.LO", "cs.NA", "cs.NE", "math.NA", "68T07, 68Q17, 68Q05, 68W40, 68N99"], "pdf": "https://arxiv.org/pdf/2508.18526", "abs": "https://arxiv.org/abs/2508.18526", "authors": ["Anastasis Kratsios", "Dennis Zvigelsky", "Bradd Hart"], "title": "Quantifying The Limits of AI Reasoning: Systematic Neural Network Representations of Algorithms", "comment": "18 pages main body, 45 pages total + references", "summary": "A main open question in contemporary AI research is quantifying the forms of\nreasoning neural networks can perform when perfectly trained. This paper\nanswers this by interpreting reasoning tasks as circuit emulation, where the\ngates define the type of reasoning; e.g. Boolean gates for predicate logic,\ntropical circuits for dynamic programming, arithmetic and analytic gates for\nsymbolic mathematical representation, and hybrids thereof for deeper reasoning;\ne.g. higher-order logic.\n  We present a systematic meta-algorithm that converts essentially any circuit\ninto a feedforward neural network (NN) with ReLU activations by iteratively\nreplacing each gate with a canonical ReLU MLP emulator. We show that, on any\ndigital computer, our construction emulates the circuit exactly--no\napproximation, no rounding, modular overflow included--demonstrating that no\nreasoning task lies beyond the reach of neural networks. The number of neurons\nin the resulting network (parametric complexity) scales with the circuit's\ncomplexity, and the network's computational graph (structure) mirrors that of\nthe emulated circuit. This formalizes the folklore that NNs networks trade\nalgorithmic run-time (circuit runtime) for space complexity (number of\nneurons).\n  We derive a range of applications of our main result, from emulating\nshortest-path algorithms on graphs with cubic--size NNs, to simulating stopped\nTuring machines with roughly quadratically--large NNs, and even the emulation\nof randomized Boolean circuits. Lastly, we demonstrate that our result is\nstrictly more powerful than a classical universal approximation theorem: any\nuniversal function approximator can be encoded as a circuit and directly\nemulated by a NN.", "AI": {"tldr": "The paper proves that neural networks can perform any reasoning task by exactly emulating circuits, with applications in algorithm emulation and exceeding the power of universal approximation theorems.", "motivation": "Quantifying the forms of reasoning neural networks can perform when perfectly trained is a main open question in contemporary AI research.", "method": "The paper introduces a systematic meta-algorithm that converts any circuit into a feedforward neural network (NN) with ReLU activations by iteratively replacing each gate with a canonical ReLU MLP emulator.", "result": "The paper shows the construction emulates the circuit exactly, with the number of neurons scaling with the circuit's complexity, and derives a range of applications, including emulating shortest-path algorithms and simulating stopped Turing machines. The result is strictly more powerful than a classical universal approximation theorem.", "conclusion": "This paper demonstrates that no reasoning task lies beyond the reach of neural networks by presenting a meta-algorithm that converts any circuit into a feedforward neural network with ReLU activations, exactly emulating the circuit."}}
{"id": "2508.18681", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18681", "abs": "https://arxiv.org/abs/2508.18681", "authors": ["Dongfang Wang", "Jian Yang", "Yizhe Zhang", "Tao Zhou"], "title": "Hierarchical Spatio-temporal Segmentation Network for Ejection Fraction Estimation in Echocardiography Videos", "comment": null, "summary": "Automated segmentation of the left ventricular endocardium in\nechocardiography videos is a key research area in cardiology. It aims to\nprovide accurate assessment of cardiac structure and function through Ejection\nFraction (EF) estimation. Although existing studies have achieved good\nsegmentation performance, their results do not perform well in EF estimation.\nIn this paper, we propose a Hierarchical Spatio-temporal Segmentation Network\n(\\ourmodel) for echocardiography video, aiming to improve EF estimation\naccuracy by synergizing local detail modeling with global dynamic perception.\nThe network employs a hierarchical design, with low-level stages using\nconvolutional networks to process single-frame images and preserve details,\nwhile high-level stages utilize the Mamba architecture to capture\nspatio-temporal relationships. The hierarchical design balances single-frame\nand multi-frame processing, avoiding issues such as local error accumulation\nwhen relying solely on single frames or neglecting details when using only\nmulti-frame data. To overcome local spatio-temporal limitations, we propose the\nSpatio-temporal Cross Scan (STCS) module, which integrates long-range context\nthrough skip scanning across frames and positions. This approach helps mitigate\nEF calculation biases caused by ultrasound image noise and other factors.", "AI": {"tldr": "This paper presents a new deep learning model for segmenting echocardiography videos to improve heart function measurement.", "motivation": "Accurate assessment of cardiac structure and function through Ejection Fraction (EF) estimation in echocardiography videos is important, but existing methods don't perform well in EF estimation.", "method": "The network employs a hierarchical design with convolutional networks for single-frame processing and Mamba architecture for spatio-temporal relationships. It also introduces a Spatio-temporal Cross Scan (STCS) module.", "result": "The proposed network aims to improve EF estimation accuracy by synergizing local detail modeling with global dynamic perception and mitigating EF calculation biases.", "conclusion": "The paper proposes a Hierarchical Spatio-temporal Segmentation Network to improve EF estimation accuracy."}}
{"id": "2508.18709", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18709", "abs": "https://arxiv.org/abs/2508.18709", "authors": ["Duy Le", "Kent Ziti", "Evan Girard-Sun", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "title": "Filtering for Creativity: Adaptive Prompting for Multilingual Riddle Generation in LLMs", "comment": null, "summary": "Multilingual riddle generation challenges large language models (LLMs) to\nbalance cultural fluency with creative abstraction. Standard prompting\nstrategies -- zero-shot, few-shot, chain-of-thought -- tend to reuse memorized\nriddles or perform shallow paraphrasing. We introduce Adaptive Originality\nFiltering (AOF), a prompting framework that filters redundant generations using\ncosine-based similarity rejection, while enforcing lexical novelty and\ncross-lingual fidelity. Evaluated across three LLMs and four language pairs,\nAOF-enhanced GPT-4o achieves \\texttt{0.177} Self-BLEU and \\texttt{0.915}\nDistinct-2 in Japanese, signaling improved lexical diversity and reduced\nredundancy compared to other prompting methods and language pairs. Our findings\nshow that semantic rejection can guide culturally grounded, creative generation\nwithout task-specific fine-tuning.", "AI": {"tldr": "AOF improves LLMs riddle generation by filtering redundant generations and enforcing lexical novelty.", "motivation": "Multilingual riddle generation challenges large language models (LLMs) to balance cultural fluency with creative abstraction. Standard prompting strategies tend to reuse memorized riddles or perform shallow paraphrasing.", "method": "Adaptive Originality Filtering (AOF), a prompting framework that filters redundant generations using cosine-based similarity rejection, while enforcing lexical novelty and cross-lingual fidelity.", "result": "AOF-enhanced GPT-4o achieves 0.177 Self-BLEU and 0.915 Distinct-2 in Japanese, signaling improved lexical diversity and reduced redundancy compared to other prompting methods and language pairs.", "conclusion": "Semantic rejection can guide culturally grounded, creative generation without task-specific fine-tuning."}}
{"id": "2508.18781", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.18781", "abs": "https://arxiv.org/abs/2508.18781", "authors": ["Lisai Zhang", "Baohan Xu", "Siqian Yang", "Mingyu Yin", "Jing Liu", "Chao Xu", "Siqi Wang", "Yidi Wu", "Yuxin Hong", "Zihao Zhang", "Yanzhang Liang", "Yudong Jiang"], "title": "AniME: Adaptive Multi-Agent Planning for Long Animation Generation", "comment": "2 pages, Technical Report", "summary": "We present AniME, a director-oriented multi-agent system for automated\nlong-form anime production, covering the full workflow from a story to the\nfinal video. The director agent keeps a global memory for the whole workflow,\nand coordinates several downstream specialized agents. By integrating\ncustomized Model Context Protocol (MCP) with downstream model instruction, the\nspecialized agent adaptively selects control conditions for diverse sub-tasks.\nAniME produces cinematic animation with consistent characters and synchronized\naudio visual elements, offering a scalable solution for AI-driven anime\ncreation.", "AI": {"tldr": "AniME is a multi-agent system for automated anime production, creating cinematic animation with consistent characters and synchronized audio-visuals.", "motivation": "automated long-form anime production, covering the full workflow from a story to the final video", "method": "a director-oriented multi-agent system integrating customized Model Context Protocol (MCP)", "result": "produces cinematic animation with consistent characters and synchronized audio visual elements", "conclusion": "AniME offers a scalable solution for AI-driven anime creation."}}
{"id": "2508.18551", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18551", "abs": "https://arxiv.org/abs/2508.18551", "authors": ["Jun Hou", "Le Wang", "Xuan Wang"], "title": "BTW: A Non-Parametric Variance Stabilization Framework for Multimodal Model Integration", "comment": null, "summary": "Mixture-of-Experts (MoE) models have become increasingly powerful in\nmultimodal learning by enabling modular specialization across modalities.\nHowever, their effectiveness remains unclear when additional modalities\nintroduce more noise than complementary information. Existing approaches, such\nas the Partial Information Decomposition, struggle to scale beyond two\nmodalities and lack the resolution needed for instance-level control. We\npropose Beyond Two-modality Weighting (BTW), a bi-level, non-parametric\nweighting framework that combines instance-level Kullback-Leibler (KL)\ndivergence and modality-level mutual information (MI) to dynamically adjust\nmodality importance during training. Our method does not require additional\nparameters and can be applied to an arbitrary number of modalities.\nSpecifically, BTW computes per-example KL weights by measuring the divergence\nbetween each unimodal and the current multimodal prediction, and modality-wide\nMI weights by estimating global alignment between unimodal and multimodal\noutputs. Extensive experiments on sentiment regression and clinical\nclassification demonstrate that our method significantly improves regression\nperformance and multiclass classification accuracy.", "AI": {"tldr": "Proposes BTW, a bi-level weighting framework using KL divergence and MI to dynamically adjust modality importance in MoE models, improving performance in multimodal learning tasks.", "motivation": "The effectiveness of Mixture-of-Experts (MoE) models remains unclear when additional modalities introduce more noise than complementary information. Existing approaches struggle to scale beyond two modalities and lack the resolution needed for instance-level control.", "method": "A bi-level, non-parametric weighting framework that combines instance-level Kullback-Leibler (KL) divergence and modality-level mutual information (MI) to dynamically adjust modality importance during training.", "result": "Demonstrates significant improvements in regression performance and multiclass classification accuracy on sentiment regression and clinical classification.", "conclusion": "The proposed BTW method significantly improves regression performance and multiclass classification accuracy on sentiment regression and clinical classification tasks."}}
{"id": "2508.18693", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18693", "abs": "https://arxiv.org/abs/2508.18693", "authors": ["Zhitong Cheng", "Yiran Jiang", "Yulong Ge", "Yufeng Li", "Zhongheng Qin", "Rongzhi Lin", "Jianwei Ma"], "title": "Feature-Space Planes Searcher: A Universal Domain Adaptation Framework for Interpretability and Computational Efficiency", "comment": null, "summary": "Domain shift, characterized by degraded model performance during transition\nfrom labeled source domains to unlabeled target domains, poses a persistent\nchallenge for deploying deep learning systems. Current unsupervised domain\nadaptation (UDA) methods predominantly rely on fine-tuning feature extractors -\nan approach limited by inefficiency, reduced interpretability, and poor\nscalability to modern architectures.\n  Our analysis reveals that models pretrained on large-scale data exhibit\ndomain-invariant geometric patterns in their feature space, characterized by\nintra-class clustering and inter-class separation, thereby preserving\ntransferable discriminative structures. These findings indicate that domain\nshifts primarily manifest as boundary misalignment rather than feature\ndegradation.\n  Unlike fine-tuning entire pre-trained models - which risks introducing\nunpredictable feature distortions - we propose the Feature-space Planes\nSearcher (FPS): a novel domain adaptation framework that optimizes decision\nboundaries by leveraging these geometric patterns while keeping the feature\nencoder frozen. This streamlined approach enables interpretative analysis of\nadaptation while substantially reducing memory and computational costs through\noffline feature extraction, permitting full-dataset optimization in a single\ncomputation cycle.\n  Evaluations on public benchmarks demonstrate that FPS achieves competitive or\nsuperior performance to state-of-the-art methods. FPS scales efficiently with\nmultimodal large models and shows versatility across diverse domains including\nprotein structure prediction, remote sensing classification, and earthquake\ndetection. We anticipate FPS will provide a simple, effective, and\ngeneralizable paradigm for transfer learning, particularly in domain adaptation\ntasks. .", "AI": {"tldr": "FPS\u901a\u8fc7\u4f18\u5316\u51b3\u7b56\u8fb9\u754c\u6765\u8fdb\u884c\u57df\u9002\u5e94\uff0c\u4f18\u4e8e\u5fae\u8c03\uff0c\u4e14\u66f4\u6709\u6548\u3002", "motivation": "\u9886\u57df\u6f02\u79fb\u4f1a\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709\u7684\u65e0\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94 (UDA) \u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5fae\u8c03\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u3001\u53ef\u89e3\u91ca\u6027\u964d\u4f4e\uff0c\u5e76\u4e14\u65e0\u6cd5\u6269\u5c55\u5230\u73b0\u4ee3\u67b6\u6784\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9886\u57df\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u5373\u7279\u5f81\u7a7a\u95f4\u5e73\u9762\u641c\u7d22\u5668 (FPS)\uff0c\u901a\u8fc7\u5229\u7528\u8fd9\u4e9b\u51e0\u4f55\u6a21\u5f0f\u6765\u4f18\u5316\u51b3\u7b56\u8fb9\u754c\uff0c\u540c\u65f6\u4fdd\u6301\u7279\u5f81\u7f16\u7801\u5668\u51bb\u7ed3\u3002", "result": "\u5728\u516c\u5171\u57fa\u51c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cFPS \u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\u3002FPS \u53ef\u4ee5\u6709\u6548\u5730\u6269\u5c55\u591a\u6a21\u6001\u5927\u578b\u6a21\u578b\uff0c\u5e76\u5728\u86cb\u767d\u8d28\u7ed3\u6784\u9884\u6d4b\u3001\u9065\u611f\u5206\u7c7b\u548c\u5730\u9707\u68c0\u6d4b\u7b49\u4e0d\u540c\u9886\u57df\u5c55\u793a\u4e86\u901a\u7528\u6027\u3002", "conclusion": "FPS\u5728\u9886\u57df\u81ea\u9002\u5e94\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u8fc1\u79fb\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u3001\u6709\u6548\u4e14\u901a\u7528\u7684\u8303\u4f8b\u3002"}}
{"id": "2508.18715", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18715", "abs": "https://arxiv.org/abs/2508.18715", "authors": ["Angela Yifei Yuan", "Haoyi Li", "Soyeon Caren Han", "Christopher Leckie"], "title": "EMMM, Explain Me My Model! Explainable Machine Generated Text Detection in Dialogues", "comment": "15 pages", "summary": "The rapid adoption of large language models (LLMs) in customer service\nintroduces new risks, as malicious actors can exploit them to conduct\nlarge-scale user impersonation through machine-generated text (MGT). Current\nMGT detection methods often struggle in online conversational settings,\nreducing the reliability and interpretability essential for trustworthy AI\ndeployment. In customer service scenarios where operators are typically\nnon-expert users, explanation become crucial for trustworthy MGT detection. In\nthis paper, we propose EMMM, an explanation-then-detection framework that\nbalances latency, accuracy, and non-expert-oriented interpretability.\nExperimental results demonstrate that EMMM provides explanations accessible to\nnon-expert users, with 70\\% of human evaluators preferring its outputs, while\nachieving competitive accuracy compared to state-of-the-art models and\nmaintaining low latency, generating outputs within 1 second. Our code and\ndataset are open-sourced at\nhttps://github.com/AngieYYF/EMMM-explainable-chatbot-detection.", "AI": {"tldr": "This paper introduces EMMM, an explanation-then-detection framework for detecting machine-generated text in customer service scenarios. It balances accuracy, latency and interpretability for non-expert users.", "motivation": "The rapid adoption of large language models (LLMs) in customer service introduces new risks, as malicious actors can exploit them to conduct large-scale user impersonation through machine-generated text (MGT). Current MGT detection methods often struggle in online conversational settings, reducing the reliability and interpretability essential for trustworthy AI deployment. In customer service scenarios where operators are typically non-expert users, explanation become crucial for trustworthy MGT detection.", "method": "explanation-then-detection framework", "result": "EMMM provides explanations accessible to non-expert users, with 70% of human evaluators preferring its outputs, while achieving competitive accuracy compared to state-of-the-art models and maintaining low latency, generating outputs within 1 second.", "conclusion": "EMMM provides explanations accessible to non-expert users, with 70% of human evaluators preferring its outputs, while achieving competitive accuracy compared to state-of-the-art models and maintaining low latency."}}
{"id": "2508.18797", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18797", "abs": "https://arxiv.org/abs/2508.18797", "authors": ["Qi Chai", "Zhang Zheng", "Junlong Ren", "Deheng Ye", "Zichuan Lin", "Hao Wang"], "title": "CausalMACE: Causality Empowered Multi-Agents in Minecraft Cooperative Tasks", "comment": null, "summary": "Minecraft, as an open-world virtual interactive environment, has become a\nprominent platform for research on agent decision-making and execution.\nExisting works primarily adopt a single Large Language Model (LLM) agent to\ncomplete various in-game tasks. However, for complex tasks requiring lengthy\nsequences of actions, single-agent approaches often face challenges related to\ninefficiency and limited fault tolerance. Despite these issues, research on\nmulti-agent collaboration remains scarce. In this paper, we propose CausalMACE,\na holistic causality planning framework designed to enhance multi-agent\nsystems, in which we incorporate causality to manage dependencies among\nsubtasks. Technically, our proposed framework introduces two modules: an\noverarching task graph for global task planning and a causality-based module\nfor dependency management, where inherent rules are adopted to perform causal\nintervention. Experimental results demonstrate our approach achieves\nstate-of-the-art performance in multi-agent cooperative tasks of Minecraft.", "AI": {"tldr": "This paper proposes CausalMACE, a causality planning framework for multi-agent systems in Minecraft, which enhances efficiency and fault tolerance by managing dependencies among subtasks.", "motivation": "Existing works primarily adopt a single Large Language Model (LLM) agent to complete various in-game tasks. However, for complex tasks requiring lengthy sequences of actions, single-agent approaches often face challenges related to inefficiency and limited fault tolerance. Despite these issues, research on multi-agent collaboration remains scarce.", "method": "a holistic causality planning framework designed to enhance multi-agent systems, in which we incorporate causality to manage dependencies among subtasks. Technically, our proposed framework introduces two modules: an overarching task graph for global task planning and a causality-based module for dependency management, where inherent rules are adopted to perform causal intervention.", "result": "achieves state-of-the-art performance in multi-agent cooperative tasks of Minecraft", "conclusion": "The proposed CausalMACE achieves state-of-the-art performance in multi-agent cooperative tasks of Minecraft."}}
{"id": "2508.18561", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18561", "abs": "https://arxiv.org/abs/2508.18561", "authors": ["\u0141ukasz Janisi\u00f3w", "Marek Kocha\u0144czyk", "Bartosz Zieli\u0144ski", "Tomasz Danel"], "title": "Enhancing Chemical Explainability Through Counterfactual Masking", "comment": null, "summary": "Molecular property prediction is a crucial task that guides the design of new\ncompounds, including drugs and materials. While explainable artificial\nintelligence methods aim to scrutinize model predictions by identifying\ninfluential molecular substructures, many existing approaches rely on masking\nstrategies that remove either atoms or atom-level features to assess importance\nvia fidelity metrics. These methods, however, often fail to adhere to the\nunderlying molecular distribution and thus yield unintuitive explanations. In\nthis work, we propose counterfactual masking, a novel framework that replaces\nmasked substructures with chemically reasonable fragments sampled from\ngenerative models trained to complete molecular graphs. Rather than evaluating\nmasked predictions against implausible zeroed-out baselines, we assess them\nrelative to counterfactual molecules drawn from the data distribution. Our\nmethod offers two key benefits: (1) molecular realism underpinning robust and\ndistribution-consistent explanations, and (2) meaningful counterfactuals that\ndirectly indicate how structural modifications may affect predicted properties.\nWe demonstrate that counterfactual masking is well-suited for benchmarking\nmodel explainers and yields more actionable insights across multiple datasets\nand property prediction tasks. Our approach bridges the gap between\nexplainability and molecular design, offering a principled and generative path\ntoward explainable machine learning in chemistry.", "AI": {"tldr": "This paper introduces counterfactual masking, a new method for explaining molecular property predictions by replacing masked substructures with chemically reasonable fragments, leading to more realistic and actionable explanations.", "motivation": "Existing explainable AI methods rely on masking strategies that remove either atoms or atom-level features, which often fail to adhere to the underlying molecular distribution and thus yield unintuitive explanations.", "method": "This paper proposes counterfactual masking, a novel framework that replaces masked substructures with chemically reasonable fragments sampled from generative models trained to complete molecular graphs.", "result": "Counterfactual masking is well-suited for benchmarking model explainers and yields more actionable insights across multiple datasets and property prediction tasks.", "conclusion": "Counterfactual masking bridges the gap between explainability and molecular design, offering a principled and generative path toward explainable machine learning in chemistry."}}
{"id": "2508.18695", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18695", "abs": "https://arxiv.org/abs/2508.18695", "authors": ["Wasi Ullah", "Yasir Noman Khalid", "Saddam Hussain Khan"], "title": "A Novel Deep Hybrid Framework with Ensemble-Based Feature Optimization for Robust Real-Time Human Activity Recognition", "comment": "35 pages, 25 figures, 11 tables", "summary": "Human Activity Recognition (HAR) plays a pivotal role in various\napplications, including smart surveillance, healthcare, assistive technologies,\nsports analytics, etc. However, HAR systems still face critical challenges,\nincluding high computational costs, redundant features, and limited scalability\nin real-time scenarios. An optimized hybrid deep learning framework is\nintroduced that integrates a customized InceptionV3, an LSTM architecture, and\na novel ensemble-based feature selection strategy. The proposed framework first\nextracts spatial descriptors using the customized InceptionV3 model, which\ncaptures multilevel contextual patterns, region homogeneity, and fine-grained\nlocalization cues. The temporal dependencies across frames are then modeled\nusing LSTMs to effectively encode motion dynamics. Finally, an ensemble-based\ngenetic algorithm with Adaptive Dynamic Fitness Sharing and Attention (ADFSA)\nis employed to select a compact and optimized feature set by dynamically\nbalancing objectives such as accuracy, redundancy, uniqueness, and complexity\nreduction. Consequently, the selected feature subsets, which are both diverse\nand discriminative, enable various lightweight machine learning classifiers to\nachieve accurate and robust HAR in heterogeneous environments. Experimental\nresults on the robust UCF-YouTube dataset, which presents challenges such as\nocclusion, cluttered backgrounds, motion dynamics, and poor illumination,\ndemonstrate good performance. The proposed approach achieves 99.65% recognition\naccuracy, reduces features to as few as 7, and enhances inference time. The\nlightweight and scalable nature of the HAR system supports real-time deployment\non edge devices such as Raspberry Pi, enabling practical applications in\nintelligent, resource-aware environments, including public safety, assistive\ntechnology, and autonomous monitoring systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b(HAR)\u7684\u4f18\u5316\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5177\u6709\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u9002\u5408\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u65f6\u90e8\u7f72\u3002", "motivation": "HAR\u7cfb\u7edf\u4ecd\u7136\u9762\u4e34\u4e25\u5cfb\u7684\u6311\u6218\uff0c\u5305\u62ec\u9ad8\u8ba1\u7b97\u6210\u672c\u3001\u5197\u4f59\u7279\u5f81\u548c\u5b9e\u65f6\u573a\u666f\u4e2d\u6709\u9650\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u8be5\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f18\u5316\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u5b9a\u5236\u7684InceptionV3\u3001LSTM\u67b6\u6784\u548c\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u96c6\u6210\u7684\u7279\u5f81\u9009\u62e9\u7b56\u7565\u3002", "result": "\u5728\u5f3a\u5927\u7684UCF-YouTube\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u8868\u73b0\u826f\u597d\u3002\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e8699.65%\u7684\u8bc6\u522b\u7cbe\u5ea6\uff0c\u5c06\u7279\u5f81\u51cf\u5c11\u52307\u4e2a\uff0c\u5e76\u7f29\u77ed\u4e86\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "\u8be5\u6587\u63d0\u51fa\u7684HAR\u7cfb\u7edf\u5177\u6709\u8f7b\u91cf\u7ea7\u548c\u53ef\u6269\u5c55\u7684\u7279\u70b9\uff0c\u652f\u6301\u5728Raspberry Pi\u7b49\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fdb\u884c\u5b9e\u65f6\u90e8\u7f72\uff0c\u4ece\u800c\u5728\u516c\u5171\u5b89\u5168\u3001\u8f85\u52a9\u6280\u672f\u548c\u81ea\u4e3b\u76d1\u63a7\u7cfb\u7edf\u7b49\u667a\u80fd\u3001\u8d44\u6e90\u611f\u77e5\u73af\u5883\u4e2d\u5b9e\u73b0\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2508.18739", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18739", "abs": "https://arxiv.org/abs/2508.18739", "authors": ["Chang Wang", "Siyu Yan", "Depeng Yuan", "Yuqi Chen", "Yanhua Huang", "Yuanhang Zheng", "Shuhao Li", "Yinqi Zhang", "Kedi Chen", "Mingrui Zhu", "Ruiwen Xu"], "title": "Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models", "comment": null, "summary": "The generation of ad headlines plays a vital role in modern advertising,\nwhere both quality and diversity are essential to engage a broad range of\naudience segments. Current approaches primarily optimize language models for\nheadline quality or click-through rates (CTR), often overlooking the need for\ndiversity and resulting in homogeneous outputs. To address this limitation, we\npropose DIVER, a novel framework based on large language models (LLMs) that are\njointly optimized for both diversity and quality. We first design a semantic-\nand stylistic-aware data generation pipeline that automatically produces\nhigh-quality training pairs with ad content and multiple diverse headlines. To\nachieve the goal of generating high-quality and diversified ad headlines within\na single forward pass, we propose a multi-stage multi-objective optimization\nframework with supervised fine-tuning (SFT) and reinforcement learning (RL).\nExperiments on real-world industrial datasets demonstrate that DIVER\neffectively balances quality and diversity. Deployed on a large-scale\ncontent-sharing platform serving hundreds of millions of users, our framework\nimproves advertiser value (ADVV) and CTR by 4.0% and 1.4%.", "AI": {"tldr": "\u63d0\u51fa\u4e86DIVER\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\uff0c\u53ef\u4ee5\u8054\u5408\u4f18\u5316\u591a\u6837\u6027\u548c\u8d28\u91cf\uff0c\u4ece\u800c\u751f\u6210\u9ad8\u8d28\u91cf\u548c\u591a\u6837\u5316\u7684\u5e7f\u544a\u6807\u9898\u3002", "motivation": "\u5f53\u524d\u7684\u5e7f\u544a\u6807\u9898\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u4f18\u5316\u8bed\u8a00\u6a21\u578b\u7684\u6807\u9898\u8d28\u91cf\u6216\u70b9\u51fb\u7387(CTR)\uff0c\u901a\u5e38\u5ffd\u7565\u4e86\u5bf9\u591a\u6837\u6027\u7684\u9700\u6c42\uff0c\u5bfc\u81f4\u540c\u8d28\u5316\u8f93\u51fa\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u7684\u65b0\u6846\u67b6DIVER\uff0c\u8be5\u6846\u67b6\u9488\u5bf9\u591a\u6837\u6027\u548c\u8d28\u91cf\u8fdb\u884c\u4e86\u8054\u5408\u4f18\u5316\u3002\u6211\u4eec\u9996\u5148\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8bed\u4e49\u548c\u6587\u4f53\u611f\u77e5\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u5bf9\uff0c\u5305\u62ec\u5e7f\u544a\u5185\u5bb9\u548c\u591a\u4e2a\u4e0d\u540c\u7684\u6807\u9898\u3002\u4e3a\u4e86\u5728\u5355\u4e2a\u524d\u5411\u4f20\u9012\u4e2d\u5b9e\u73b0\u751f\u6210\u9ad8\u8d28\u91cf\u548c\u591a\u6837\u5316\u5e7f\u544a\u6807\u9898\u7684\u76ee\u6807\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u9636\u6bb5\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\uff0c\u5305\u62ec\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\u3002", "result": "DIVER\u6709\u6548\u5730\u5e73\u8861\u4e86\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002", "conclusion": "DIVER\u6846\u67b6\u5728\u5927\u578b\u5185\u5bb9\u5206\u4eab\u5e73\u53f0\u4e0a\u90e8\u7f72\u540e\uff0c\u5e7f\u544a\u4e3b\u4ef7\u503c(ADVV)\u548c\u70b9\u51fb\u7387(CTR)\u5206\u522b\u63d0\u9ad8\u4e864.0%\u548c1.4%\u3002"}}
{"id": "2508.18812", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18812", "abs": "https://arxiv.org/abs/2508.18812", "authors": ["Chenghao Wu", "Ruiyang Ren", "Junjie Zhang", "Ruirui Wang", "Zhongrui Ma", "Qi Ye", "Wayne Xin Zhao"], "title": "STARec: An Efficient Agent Framework for Recommender Systems via Autonomous Deliberate Reasoning", "comment": null, "summary": "While modern recommender systems are instrumental in navigating information\nabundance, they remain fundamentally limited by static user modeling and\nreactive decision-making paradigms. Current large language model (LLM)-based\nagents inherit these shortcomings through their overreliance on heuristic\npattern matching, yielding recommendations prone to shallow correlation bias,\nlimited causal inference, and brittleness in sparse-data scenarios. We\nintroduce STARec, a slow-thinking augmented agent framework that endows\nrecommender systems with autonomous deliberative reasoning capabilities. Each\nuser is modeled as an agent with parallel cognitions: fast response for\nimmediate interactions and slow reasoning that performs chain-of-thought\nrationales. To cultivate intrinsic slow thinking, we develop anchored\nreinforcement training - a two-stage paradigm combining structured knowledge\ndistillation from advanced reasoning models with preference-aligned reward\nshaping. This hybrid approach scaffolds agents in acquiring foundational\ncapabilities (preference summarization, rationale generation) while enabling\ndynamic policy adaptation through simulated feedback loops. Experiments on\nMovieLens 1M and Amazon CDs benchmarks demonstrate that STARec achieves\nsubstantial performance gains compared with state-of-the-art baselines, despite\nusing only 0.4% of the full training data.", "AI": {"tldr": "STARec: a slow-thinking augmented agent framework that endows recommender systems with autonomous deliberative reasoning capabilities and achieves substantial performance gains.", "motivation": "Current large language model (LLM)-based agents inherit these shortcomings through their overreliance on heuristic pattern matching, yielding recommendations prone to shallow correlation bias, limited causal inference, and brittleness in sparse-data scenarios.", "method": "a slow-thinking augmented agent framework that endows recommender systems with autonomous deliberative reasoning capabilities. Each user is modeled as an agent with parallel cognitions: fast response for immediate interactions and slow reasoning that performs chain-of-thought rationales. To cultivate intrinsic slow thinking, we develop anchored reinforcement training - a two-stage paradigm combining structured knowledge distillation from advanced reasoning models with preference-aligned reward shaping.", "result": "achieves substantial performance gains compared with state-of-the-art baselines, despite using only 0.4% of the full training data.", "conclusion": "STARec achieves substantial performance gains compared with state-of-the-art baselines, despite using only 0.4% of the full training data."}}
{"id": "2508.18564", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18564", "abs": "https://arxiv.org/abs/2508.18564", "authors": ["Levi Rauchwerger", "Ron Levie"], "title": "A Note on Graphon-Signal Analysis of Graph Neural Networks", "comment": null, "summary": "A recent paper, ``A Graphon-Signal Analysis of Graph Neural Networks'', by\nLevie, analyzed message passing graph neural networks (MPNNs) by embedding the\ninput space of MPNNs, i.e., attributed graphs (graph-signals), to a space of\nattributed graphons (graphon-signals). Based on extensions of standard results\nin graphon analysis to graphon-signals, the paper proved a generalization bound\nand a sampling lemma for MPNNs. However, there are some missing ingredients in\nthat paper, limiting its applicability in practical settings of graph machine\nlearning. In the current paper, we introduce several refinements and extensions\nto existing results that address these shortcomings. In detail, 1) we extend\nthe main results in the paper to graphon-signals with multidimensional signals\n(rather than 1D signals), 2) we extend the Lipschitz continuity to MPNNs with\nreadout with respect to cut distance (rather than MPNNs without readout with\nrespect to cut metric), 3) we improve the generalization bound by utilizing\nrobustness-type generalization bounds, and 4) we extend the analysis to\nnon-symmetric graphons and kernels.", "AI": {"tldr": "This paper extends the results of a recent paper to address the shortcomings of applicability in practical settings of graph machine learning.", "motivation": "There are some missing ingredients in a recent paper, limiting its applicability in practical settings of graph machine learning.", "method": "Based on extensions of standard results in graphon analysis to graphon-signals", "result": "proved a generalization bound and a sampling lemma for MPNNs", "conclusion": "This paper introduces several refinements and extensions to existing results that address the shortcomings of applicability in practical settings of graph machine learning. The extensions are: 1) extend the main results in the paper to graphon-signals with multidimensional signals, 2) extend the Lipschitz continuity to MPNNs with readout with respect to cut distance, 3) improve the generalization bound by utilizing robustness-type generalization bounds, and 4) extend the analysis to non-symmetric graphons and kernels."}}
{"id": "2508.18696", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18696", "abs": "https://arxiv.org/abs/2508.18696", "authors": ["Qun Ji", "Peng Li", "Mingqiang Wei"], "title": "ColorGS: High-fidelity Surgical Scene Reconstruction with Colored Gaussian Splatting", "comment": null, "summary": "High-fidelity reconstruction of deformable tissues from endoscopic videos\nremains challenging due to the limitations of existing methods in capturing\nsubtle color variations and modeling global deformations. While 3D Gaussian\nSplatting (3DGS) enables efficient dynamic reconstruction, its fixed\nper-Gaussian color assignment struggles with intricate textures, and linear\ndeformation modeling fails to model consistent global deformation. To address\nthese issues, we propose ColorGS, a novel framework that integrates spatially\nadaptive color encoding and enhanced deformation modeling for surgical scene\nreconstruction. First, we introduce Colored Gaussian Primitives, which employ\ndynamic anchors with learnable color parameters to adaptively encode spatially\nvarying textures, significantly improving color expressiveness under complex\nlighting and tissue similarity. Second, we design an Enhanced Deformation Model\n(EDM) that combines time-aware Gaussian basis functions with learnable\ntime-independent deformations, enabling precise capture of both localized\ntissue deformations and global motion consistency caused by surgical\ninteractions. Extensive experiments on DaVinci robotic surgery videos and\nbenchmark datasets (EndoNeRF, StereoMIS) demonstrate that ColorGS achieves\nstate-of-the-art performance, attaining a PSNR of 39.85 (1.5 higher than prior\n3DGS-based methods) and superior SSIM (97.25\\%) while maintaining real-time\nrendering efficiency. Our work advances surgical scene reconstruction by\nbalancing high fidelity with computational practicality, critical for\nintraoperative guidance and AR/VR applications.", "AI": {"tldr": "ColorGS \u662f\u4e00\u79cd\u7528\u4e8e\u624b\u672f\u573a\u666f\u91cd\u5efa\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u7a7a\u95f4\u81ea\u9002\u5e94\u989c\u8272\u7f16\u7801\u548c\u589e\u5f3a\u7684\u53d8\u5f62\u5efa\u6a21\u6765\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u7531\u4e8e\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u83b7\u7ec6\u5fae\u989c\u8272\u53d8\u5316\u548c\u5efa\u6a21\u5168\u5c40\u53d8\u5f62\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4ece\u5185\u7aa5\u955c\u89c6\u9891\u4e2d\u9ad8\u4fdd\u771f\u5730\u91cd\u5efa\u53ef\u53d8\u5f62\u7ec4\u7ec7\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u867d\u7136 3D \u9ad8\u65af\u6e85\u5c04 (3DGS) \u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u7684\u52a8\u6001\u91cd\u5efa\uff0c\u4f46\u5176\u56fa\u5b9a\u7684\u6bcf\u9ad8\u65af\u989c\u8272\u5206\u914d\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u7eb9\u7406\uff0c\u5e76\u4e14\u7ebf\u6027\u53d8\u5f62\u5efa\u6a21\u65e0\u6cd5\u5bf9\u4e00\u81f4\u7684\u5168\u5c40\u53d8\u5f62\u8fdb\u884c\u5efa\u6a21\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6 ColorGS\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u7a7a\u95f4\u81ea\u9002\u5e94\u989c\u8272\u7f16\u7801\u548c\u589e\u5f3a\u7684\u53d8\u5f62\u5efa\u6a21\uff0c\u7528\u4e8e\u624b\u672f\u573a\u666f\u91cd\u5efa\u3002\u5b83\u5f15\u5165\u4e86\u5f69\u8272\u9ad8\u65af\u56fe\u5143\uff0c\u8be5\u56fe\u5143\u91c7\u7528\u5177\u6709\u53ef\u5b66\u4e60\u989c\u8272\u53c2\u6570\u7684\u52a8\u6001\u951a\u70b9\u6765\u81ea\u9002\u5e94\u5730\u7f16\u7801\u7a7a\u95f4\u53d8\u5316\u7684\u7eb9\u7406\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u589e\u5f3a\u53d8\u5f62\u6a21\u578b (EDM)\uff0c\u8be5\u6a21\u578b\u5c06\u65f6\u95f4\u611f\u77e5\u9ad8\u65af\u57fa\u51fd\u6570\u4e0e\u53ef\u5b66\u4e60\u7684\u65f6\u95f4\u72ec\u7acb\u53d8\u5f62\u76f8\u7ed3\u5408\u3002", "result": "\u5728 DaVinci \u673a\u5668\u4eba\u624b\u672f\u89c6\u9891\u548c\u57fa\u51c6\u6570\u636e\u96c6\uff08EndoNeRF\u3001StereoMIS\uff09\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cColorGS \u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8fbe\u5230\u4e86 39.85 \u7684 PSNR\uff08\u6bd4\u4e4b\u524d\u7684\u57fa\u4e8e 3DGS \u7684\u65b9\u6cd5\u9ad8 1.5\uff09\u548c\u5353\u8d8a\u7684 SSIM (97.25%)\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9e\u65f6\u6e32\u67d3\u6548\u7387\u3002", "conclusion": "ColorGS \u901a\u8fc7\u5e73\u8861\u9ad8\u4fdd\u771f\u5ea6\u548c\u8ba1\u7b97\u5b9e\u7528\u6027\u6765\u63a8\u8fdb\u624b\u672f\u573a\u666f\u91cd\u5efa\uff0c\u8fd9\u5bf9\u4e8e\u672f\u4e2d\u6307\u5bfc\u548c AR/VR \u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.18740", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18740", "abs": "https://arxiv.org/abs/2508.18740", "authors": ["Qiao Liang", "Ying Shen", "Tiantian Chen", "Lin Zhang"], "title": "M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations", "comment": "16 pages, 8 figures. Accepted to Findings of ACL 2025", "summary": "Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has\nrecently gained significant attention in social media analysis, aiming to\nextract emotion utterances, cause utterances, and emotion categories\nsimultaneously. However, the scarcity of related datasets, with only one\npublished dataset featuring highly uniform dialogue scenarios, hinders model\ndevelopment in this field. To address this, we introduce MECAD, the first\nmultimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56\nTV series spanning a wide range of dialogue contexts. In addition, existing\nMECTEC methods fail to explicitly model emotional and causal contexts and\nneglect the fusion of semantic information at different levels, leading to\nperformance degradation. In this paper, we propose M3HG, a novel model that\nexplicitly captures emotional and causal contexts and effectively fuses\ncontextual information at both inter- and intra-utterance levels via a\nmultimodal heterogeneous graph. Extensive experiments demonstrate the\neffectiveness of M3HG compared with existing state-of-the-art methods. The\ncodes and dataset are available at https://github.com/redifinition/M3HG.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u591a\u6a21\u6001\u3001\u591a\u573a\u666f\u7684 MECTEC \u6570\u636e\u96c6 MECAD\uff0c\u5e76\u63d0\u51fa\u4e86 M3HG \u6a21\u578b\uff0c\u5728\u591a\u6a21\u6001\u5bf9\u8bdd\u4e2d\u663e\u5f0f\u5730\u6355\u83b7\u60c5\u611f\u548c\u56e0\u679c\u4e0a\u4e0b\u6587\uff0c\u5e76\u6709\u6548\u5730\u878d\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "motivation": "\u76f8\u5173\u6570\u636e\u96c6\u7684\u7a00\u7f3a\uff0c\u963b\u788d\u4e86\u8be5\u9886\u57df\u6a21\u578b\u7684\u53d1\u5c55\u3002\u73b0\u6709\u7684 MECTEC \u65b9\u6cd5\u672a\u80fd\u660e\u786e\u5730\u5efa\u6a21\u60c5\u611f\u548c\u56e0\u679c\u5173\u7cfb\uff0c\u5e76\u5ffd\u7565\u4e86\u4e0d\u540c\u5c42\u6b21\u7684\u8bed\u4e49\u4fe1\u606f\u878d\u5408\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86 M3HG\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u6a21\u578b\u3002", "result": "\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cM3HG \u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u4e86 M3HG \u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5f02\u6784\u56fe\u5728 utterance \u95f4\u548c utterance \u5185\u7684\u5c42\u9762\u6709\u6548\u878d\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u663e\u5f0f\u5730\u6355\u83b7\u60c5\u611f\u548c\u56e0\u679c\u4e0a\u4e0b\u6587\u3002"}}
{"id": "2508.18880", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18880", "abs": "https://arxiv.org/abs/2508.18880", "authors": ["Eljas Linna", "Tuula Linna"], "title": "Judicial Requirements for Generative AI in Legal Reasoning", "comment": null, "summary": "Large Language Models (LLMs) are being integrated into professional domains,\nyet their limitations in high-stakes fields like law remain poorly understood.\nThis paper defines the core capabilities that an AI system must possess to\nfunction as a reliable reasoning tool in judicial decision-making. Using the\nIRAC (Issue-Rule-Application-Conclusion) model as an analytical framework, the\nstudy focuses on the most challenging phases of legal adjudication: determining\nthe applicable Rule (R) and performing the Application (A) of that rule to the\nfacts of a case. From a judicial perspective, the analysis deconstructs legal\nreasoning into a series of core requirements, including the ability to select\nthe correct legal framework across jurisdictions, generate sound arguments\nbased on the doctrine of legal sources, distinguish ratio decidendi from obiter\ndictum in case law, resolve ambiguity arising from general clauses like\n\"reasonableness\", manage conflicting legal provisions, and correctly apply the\nburden of proof. The paper then maps various AI enhancement mechanisms, such as\nRetrieval-Augmented Generation (RAG), multi-agent systems, and neuro-symbolic\nAI, to these requirements, assessing their potential to bridge the gap between\nthe probabilistic nature of LLMs and the rigorous, choice-driven demands of\nlegal interpretation. The findings indicate that while these techniques can\naddress specific challenges, significant challenges remain, particularly in\ntasks requiring discretion and transparent, justifiable reasoning. Our paper\nconcludes that the most effective current role for AI in law is a dual one: as\na high-volume assistant for simple, repetitive cases and as a sophisticated\n\"sparring partner\" for human experts in complex matters.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u63a8\u7406\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u5904\u7406\u590d\u6742\u548c\u9700\u8981\u81ea\u7531\u88c1\u91cf\u6743\u7684\u4efb\u52a1\u65f6\u4ecd\u5b58\u5728\u5c40\u9650\u6027\uff0c\u76ee\u524d\u6700\u9002\u5408\u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6b63\u88ab\u6574\u5408\u5230\u4e13\u4e1a\u9886\u57df\uff0c\u4f46\u5b83\u4eec\u5728\u6cd5\u5f8b\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5c40\u9650\u6027\u4ecd\u7136\u77e5\u4e4b\u751a\u5c11\u3002", "method": "\u4f7f\u7528IRAC\uff08\u95ee\u9898-\u89c4\u5219-\u5e94\u7528-\u7ed3\u8bba\uff09\u6a21\u578b\u4f5c\u4e3a\u5206\u6790\u6846\u67b6", "result": "\u867d\u7136RAG\u3001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u795e\u7ecf\u7b26\u53f7AI\u7b49\u6280\u672f\u53ef\u4ee5\u5e94\u5bf9\u7279\u5b9a\u6311\u6218\uff0c\u4f46\u5728\u9700\u8981\u81ea\u7531\u88c1\u91cf\u6743\u548c\u900f\u660e\u3001\u5408\u7406\u7684\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\uff0c\u4ecd\u7136\u5b58\u5728\u91cd\u5927\u6311\u6218\u3002", "conclusion": "AI\u5728\u6cd5\u5f8b\u9886\u57df\u6700\u6709\u6548\u7684\u89d2\u8272\u662f\u53cc\u91cd\u7684\uff1a\u4f5c\u4e3a\u7b80\u5355\u91cd\u590d\u6848\u4ef6\u7684\u5927\u6279\u91cf\u52a9\u624b\uff0c\u4ee5\u53ca\u4f5c\u4e3a\u590d\u6742\u6848\u4ef6\u4e2d\u4eba\u7c7b\u4e13\u5bb6\u7684\u590d\u6742\u201c\u966a\u7ec3\u4f19\u4f34\u201d\u3002"}}
{"id": "2508.18565", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18565", "abs": "https://arxiv.org/abs/2508.18565", "authors": ["Hao Zhou", "Sibo Cheng"], "title": "Improving Long-term Autoregressive Spatiotemporal Predictions: A Proof of Concept with Fluid Dynamics", "comment": null, "summary": "Data-driven methods are emerging as efficient alternatives to traditional\nnumerical forecasting, offering fast inference and lower computational cost.\nYet, for complex systems, long-term accuracy often deteriorates due to error\naccumulation, and autoregressive training (though effective) demands large GPU\nmemory and may sacrifice short-term performance. We propose the Stochastic\nPushForward (SPF) framework, which retains one-step-ahead training while\nenabling multi-step learning. SPF builds a supplementary dataset from model\npredictions and combines it with ground truth via a stochastic acquisition\nstrategy, balancing short- and long-term performance while reducing\noverfitting. Multi-step predictions are precomputed between epochs, keeping\nmemory usage stable without storing full unrolled sequences. Experiments on the\nBurgers' equation and the Shallow Water benchmark show that SPF achieves higher\nlong-term accuracy than autoregressive methods while lowering memory\nrequirements, making it promising for resource-limited and complex simulations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u9a71\u52a8\u9884\u6d4b\u6846\u67b6\uff08SPF\uff09\uff0c\u8be5\u6846\u67b6\u5728\u964d\u4f4e\u5185\u5b58\u9700\u6c42\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u957f\u671f\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6b63\u5728\u6210\u4e3a\u4f20\u7edf\u6570\u503c\u9884\u6d4b\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u63d0\u4f9b\u5feb\u901f\u63a8\u7406\u548c\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u590d\u6742\u7684\u7cfb\u7edf\uff0c\u7531\u4e8e\u8bef\u5dee\u7d2f\u79ef\uff0c\u957f\u671f\u51c6\u786e\u6027\u901a\u5e38\u4f1a\u4e0b\u964d\uff0c\u800c\u81ea\u56de\u5f52\u8bad\u7ec3\uff08\u5c3d\u7ba1\u6709\u6548\uff09\u9700\u8981\u5927\u91cf\u7684GPU\u5185\u5b58\uff0c\u5e76\u4e14\u53ef\u80fd\u4f1a\u727a\u7272\u77ed\u671f\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u968f\u673a\u524d\u63a8\uff08SPF\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4fdd\u7559\u4e86\u5355\u6b65\u8d85\u524d\u8bad\u7ec3\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u591a\u6b65\u5b66\u4e60\u3002SPF\u4ece\u6a21\u578b\u9884\u6d4b\u4e2d\u6784\u5efa\u4e00\u4e2a\u8865\u5145\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u968f\u673a\u91c7\u96c6\u7b56\u7565\u5c06\u5176\u4e0e\u5730\u9762\u5b9e\u51b5\u76f8\u7ed3\u5408\uff0c\u4ece\u800c\u5e73\u8861\u4e86\u77ed\u671f\u548c\u957f\u671f\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8fc7\u5ea6\u62df\u5408\u3002\u591a\u6b65\u9884\u6d4b\u5728epochs\u4e4b\u95f4\u9884\u5148\u8ba1\u7b97\uff0c\u4fdd\u6301\u5185\u5b58\u4f7f\u7528\u7a33\u5b9a\uff0c\u800c\u65e0\u9700\u5b58\u50a8\u5b8c\u6574\u7684\u5c55\u5f00\u5e8f\u5217\u3002", "result": "\u5728Burgers\u65b9\u7a0b\u548c\u6d45\u6c34\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSPF\u5b9e\u73b0\u4e86\u6bd4\u81ea\u56de\u5f52\u65b9\u6cd5\u66f4\u9ad8\u7684\u957f\u671f\u51c6\u786e\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u5185\u5b58\u9700\u6c42\u3002", "conclusion": "SPF\u5728\u964d\u4f4e\u5185\u5b58\u9700\u6c42\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u6bd4\u81ea\u56de\u5f52\u65b9\u6cd5\u66f4\u9ad8\u7684\u957f\u671f\u51c6\u786e\u6027\uff0c\u4f7f\u5176\u5728\u8d44\u6e90\u6709\u9650\u548c\u590d\u6742\u7684\u6a21\u62df\u4e2d\u5f88\u6709\u5e0c\u671b\u3002"}}
{"id": "2508.18723", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18723", "abs": "https://arxiv.org/abs/2508.18723", "authors": ["Hiroaki Aizawa", "Yuta Naito", "Kohei Fukuda"], "title": "Class-wise Flooding Regularization for Imbalanced Image Classification", "comment": "Accepted to ACPR2025", "summary": "The purpose of training neural networks is to achieve high generalization\nperformance on unseen inputs. However, when trained on imbalanced datasets, a\nmodel's prediction tends to favor majority classes over minority classes,\nleading to significant degradation in the recognition performance of minority\nclasses. To address this issue, we propose class-wise flooding regularization,\nan extension of flooding regularization applied at the class level. Flooding is\na regularization technique that mitigates overfitting by preventing the\ntraining loss from falling below a predefined threshold, known as the flooding\nlevel, thereby discouraging memorization. Our proposed method assigns a\nclass-specific flooding level based on class frequencies. By doing so, it\nsuppresses overfitting in majority classes while allowing sufficient learning\nfor minority classes. We validate our approach on imbalanced image\nclassification. Compared to conventional flooding regularizations, our method\nimproves the classification performance of minority classes and achieves better\noverall generalization.", "AI": {"tldr": "This paper introduces class-wise flooding regularization to address the issue of performance degradation on minority classes when training neural networks on imbalanced datasets. ", "motivation": "Training on imbalanced datasets leads to a model's prediction favoring majority classes, causing significant degradation in the recognition performance of minority classes.", "method": "The paper proposes class-wise flooding regularization, an extension of flooding regularization applied at the class level. It assigns a class-specific flooding level based on class frequencies to suppress overfitting in majority classes while allowing sufficient learning for minority classes.", "result": "The proposed method is validated on imbalanced image classification and improves the classification performance of minority classes and achieves better overall generalization.", "conclusion": "The proposed class-wise flooding regularization improves the classification performance of minority classes and achieves better overall generalization compared to conventional flooding regularizations."}}
{"id": "2508.18748", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18748", "abs": "https://arxiv.org/abs/2508.18748", "authors": ["Byeongjeong Kim", "Jeonghyun Park", "Joonho Yang", "Hwanhee Lee"], "title": "Chronological Passage Assembling in RAG framework for Temporal Question Answering", "comment": "7 pages, 3 figures", "summary": "Long-context question answering over narrative tasks is challenging because\ncorrect answers often hinge on reconstructing a coherent timeline of events\nwhile preserving contextual flow in a limited context window.\nRetrieval-augmented generation (RAG) indexing methods aim to address this\nchallenge by selectively retrieving only necessary document segments. However,\nnarrative texts possess unique characteristics that limit the effectiveness of\nthese existing approaches. Specifically, understanding narrative texts requires\nmore than isolated segments, as the broader context and sequential\nrelationships between segments are crucial for comprehension. To address these\nlimitations, we propose ChronoRAG, a novel RAG framework specialized for\nnarrative texts. This approach focuses on two essential aspects: refining\ndispersed document information into coherent and structured passages, and\npreserving narrative flow by explicitly capturing and maintaining the temporal\norder among retrieved passages. We empirically demonstrate the effectiveness of\nChronoRAG through experiments on the NarrativeQA dataset, showing substantial\nimprovements in tasks requiring both factual identification and comprehension\nof complex sequential relationships, underscoring that reasoning over temporal\norder is crucial in resolving narrative QA.", "AI": {"tldr": "ChronoRAG: a novel RAG framework specialized for narrative texts, focuses on refining dispersed document information and preserving narrative flow by capturing the temporal order among retrieved passages.", "motivation": "Long-context question answering over narrative tasks is challenging because correct answers often hinge on reconstructing a coherent timeline of events while preserving contextual flow in a limited context window. Narrative texts possess unique characteristics that limit the effectiveness of these existing approaches. Specifically, understanding narrative texts requires more than isolated segments, as the broader context and sequential relationships between segments are crucial for comprehension.", "method": "We propose ChronoRAG, a novel RAG framework specialized for narrative texts. This approach focuses on two essential aspects: refining dispersed document information into coherent and structured passages, and preserving narrative flow by explicitly capturing and maintaining the temporal order among retrieved passages.", "result": "substantial improvements in tasks requiring both factual identification and comprehension of complex sequential relationships", "conclusion": "We empirically demonstrate the effectiveness of ChronoRAG through experiments on the NarrativeQA dataset, showing substantial improvements in tasks requiring both factual identification and comprehension of complex sequential relationships, underscoring that reasoning over temporal order is crucial in resolving narrative QA."}}
{"id": "2508.18905", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18905", "abs": "https://arxiv.org/abs/2508.18905", "authors": ["Dimitrios Rontogiannis", "Maxime Peyrard", "Nicolas Baldwin", "Martin Josifoski", "Robert West", "Dimitrios Gunopulos"], "title": "Interactive Evaluation of Large Language Models for Multi-Requirement Software Engineering Tasks", "comment": null, "summary": "Standard single-turn, static benchmarks fall short in evaluating the nuanced\ncapabilities of Large Language Models (LLMs) on complex tasks such as software\nengineering. In this work, we propose a novel interactive evaluation framework\nthat assesses LLMs on multi-requirement programming tasks through structured,\nfeedback-driven dialogue. Each task is modeled as a requirement dependency\ngraph, and an ``interviewer'' LLM, aware of the ground-truth solution, provides\nminimal, targeted hints to an ``interviewee'' model to help correct errors and\nfulfill target constraints. This dynamic protocol enables fine-grained\ndiagnostic insights into model behavior, uncovering strengths and systematic\nweaknesses that static benchmarks fail to measure. We build on DevAI, a\nbenchmark of 55 curated programming tasks, by adding ground-truth solutions and\nevaluating the relevance and utility of interviewer hints through expert\nannotation. Our results highlight the importance of dynamic evaluation in\nadvancing the development of collaborative code-generating agents.", "AI": {"tldr": "The paper introduces a new interactive evaluation framework for LLMs in programming, revealing weaknesses static benchmarks miss.", "motivation": "Standard single-turn, static benchmarks fall short in evaluating the nuanced capabilities of Large Language Models (LLMs) on complex tasks such as software engineering.", "method": "The study proposes a novel interactive evaluation framework that assesses LLMs on multi-requirement programming tasks through structured, feedback-driven dialogue. Each task is modeled as a requirement dependency graph, and an interviewer LLM provides minimal, targeted hints to an interviewee model.", "result": "The study builds on DevAI, a benchmark of 55 curated programming tasks, by adding ground-truth solutions and evaluating the relevance and utility of interviewer hints through expert annotation. The results uncover strengths and systematic weaknesses that static benchmarks fail to measure.", "conclusion": "This work highlights the importance of dynamic evaluation in advancing the development of collaborative code-generating agents."}}
{"id": "2508.18567", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.18567", "abs": "https://arxiv.org/abs/2508.18567", "authors": ["Darin Tsui", "Kunal Talreja", "Amirali Aghazadeh"], "title": "Sparse Autoencoders for Low-$N$ Protein Function Prediction and Design", "comment": "15 pages, 4 figures", "summary": "Predicting protein function from amino acid sequence remains a central\nchallenge in data-scarce (low-$N$) regimes, limiting machine learning-guided\nprotein design when only small amounts of assay-labeled sequence-function data\nare available. Protein language models (pLMs) have advanced the field by\nproviding evolutionary-informed embeddings and sparse autoencoders (SAEs) have\nenabled decomposition of these embeddings into interpretable latent variables\nthat capture structural and functional features. However, the effectiveness of\nSAEs for low-$N$ function prediction and protein design has not been\nsystematically studied. Herein, we evaluate SAEs trained on fine-tuned ESM2\nembeddings across diverse fitness extrapolation and protein engineering tasks.\nWe show that SAEs, with as few as 24 sequences, consistently outperform or\ncompete with their ESM2 baselines in fitness prediction, indicating that their\nsparse latent space encodes compact and biologically meaningful representations\nthat generalize more effectively from limited data. Moreover, steering\npredictive latents exploits biological motifs in pLM representations, yielding\ntop-fitness variants in 83% of cases compared to designing with ESM2 alone.", "AI": {"tldr": "SAEs trained on fine-tuned ESM2 embeddings can effectively predict protein function even with limited data, outperforming ESM2 baselines and enabling successful protein design.", "motivation": "Predicting protein function from amino acid sequence remains a central challenge in data-scarce regimes, limiting machine learning-guided protein design when only small amounts of assay-labeled sequence-function data are available.", "method": "SAEs trained on fine-tuned ESM2 embeddings", "result": "SAEs, with as few as 24 sequences, consistently outperform or compete with their ESM2 baselines in fitness prediction.steering predictive latents exploits biological motifs in pLM representations, yielding top-fitness variants in 83% of cases compared to designing with ESM2 alone.", "conclusion": "SAEs consistently outperform or compete with their ESM2 baselines in fitness prediction, indicating that their sparse latent space encodes compact and biologically meaningful representations that generalize more effectively from limited data. Moreover, steering predictive latents exploits biological motifs in pLM representations, yielding top-fitness variants in 83% of cases compared to designing with ESM2 alone."}}
{"id": "2508.18726", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18726", "abs": "https://arxiv.org/abs/2508.18726", "authors": ["Hiroaki Aizawa", "Yoshikazu Hayashi"], "title": "Flatness-aware Curriculum Learning via Adversarial Difficulty", "comment": "Accepted to BMVC2025", "summary": "Neural networks trained by empirical risk minimization often suffer from\noverfitting, especially to specific samples or domains, which leads to poor\ngeneralization. Curriculum Learning (CL) addresses this issue by selecting\ntraining samples based on the difficulty. From the optimization perspective,\nmethods such as Sharpness-Aware Minimization (SAM) improve robustness and\ngeneralization by seeking flat minima. However, combining CL with SAM is not\nstraightforward. In flat regions, both the loss values and the gradient norms\ntend to become uniformly small, which makes it difficult to evaluate sample\ndifficulty and design an effective curriculum. To overcome this problem, we\npropose the Adversarial Difficulty Measure (ADM), which quantifies adversarial\nvulnerability by leveraging the robustness properties of models trained toward\nflat minima. Unlike loss- or gradient-based measures, which become ineffective\nas training progresses into flatter regions, ADM remains informative by\nmeasuring the normalized loss gap between original and adversarial examples. We\nincorporate ADM into CL-based training with SAM to dynamically assess sample\ndifficulty. We evaluated our approach on image classification tasks,\nfine-grained recognition, and domain generalization. The results demonstrate\nthat our method preserves the strengths of both CL and SAM while outperforming\nexisting curriculum-based and flatness-aware training strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b83\u4f7f\u7528\u5bf9\u6297\u6027\u96be\u5ea6\u5ea6\u91cf\u6765\u9009\u62e9\u8bad\u7ec3\u6837\u672c\uff0c\u5e76\u7ed3\u5408\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u7ecf\u5e38\u8fc7\u5ea6\u62df\u5408\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u8bfe\u7a0b\u5b66\u4e60\uff08CL\uff09\u901a\u8fc7\u9009\u62e9\u57fa\u4e8e\u96be\u5ea6\u7684\u8bad\u7ec3\u6837\u672c\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u4ece\u4f18\u5316\u89d2\u5ea6\u6765\u770b\uff0c\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316\uff08SAM\uff09\u901a\u8fc7\u5bfb\u627e\u5e73\u5766\u7684\u6700\u5c0f\u503c\u6765\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u7136\u800c\uff0c\u5c06 CL \u4e0e SAM \u7ed3\u5408\u8d77\u6765\u5e76\u4e0d\u7b80\u5355\u3002\u5728\u5e73\u5766\u533a\u57df\uff0c\u635f\u5931\u503c\u548c\u68af\u5ea6\u8303\u6570\u90fd\u8d8b\u4e8e\u5747\u5300\u53d8\u5c0f\uff0c\u8fd9\u4f7f\u5f97\u8bc4\u4f30\u6837\u672c\u96be\u5ea6\u548c\u8bbe\u8ba1\u6709\u6548\u7684\u8bfe\u7a0b\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6297\u6027\u96be\u5ea6\u5ea6\u91cf\uff08ADM\uff09\uff0c\u901a\u8fc7\u5229\u7528\u5728\u5e73\u9762\u6781\u5c0f\u503c\u8bad\u7ec3\u7684\u6a21\u578b\u7684\u9c81\u68d2\u6027\u6765\u91cf\u5316\u5bf9\u6297\u8106\u5f31\u6027\uff0c\u5e76\u5c06\u5176\u7eb3\u5165\u57fa\u4e8e CL \u7684 SAM \u8bad\u7ec3\u4e2d\uff0c\u4ee5\u52a8\u6001\u8bc4\u4f30\u6837\u672c\u96be\u5ea6\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u7ec6\u7c92\u5ea6\u8bc6\u522b\u548c\u9886\u57df\u6cdb\u5316\u4efb\u52a1\u4e2d\u7684\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4fdd\u7559\u4e86 CL \u548c SAM \u7684\u4f18\u52bf\uff0c\u540c\u65f6\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u8bfe\u7a0b\u548c\u5e73\u9762\u611f\u77e5\u8bad\u7ec3\u7b56\u7565\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u7ec6\u7c92\u5ea6\u8bc6\u522b\u548c\u9886\u57df\u6cdb\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u8bfe\u7a0b\u548c\u5e73\u9762\u611f\u77e5\u8bad\u7ec3\u7b56\u7565\uff0c\u540c\u65f6\u4fdd\u7559\u4e86 CL \u548c SAM \u7684\u4f18\u52bf\u3002"}}
{"id": "2508.18773", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18773", "abs": "https://arxiv.org/abs/2508.18773", "authors": ["Qianyu He", "Siyu Yuan", "Xuefeng Li", "Mingxuan Wang", "Jiangjie Chen"], "title": "ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models", "comment": null, "summary": "Large language models (LLMs) with chain-of-thought reasoning have\ndemonstrated remarkable problem-solving capabilities, but controlling their\ncomputational effort remains a significant challenge for practical deployment.\nRecent proprietary systems like OpenAI's gpt-oss series have introduced\ndiscrete operational modes for intuitive reasoning control, but the open-source\ncommunity has largely failed to achieve such capabilities. In this paper, we\nintroduce ThinkDial, the first open-recipe end-to-end framework that\nsuccessfully implements gpt-oss-style controllable reasoning through discrete\noperational modes. Our system enables seamless switching between three distinct\nreasoning regimes: High mode (full reasoning capability), Medium mode (50\npercent token reduction with <10 percent performance degradation), and Low mode\n(75 percent token reduction with <15 percent performance degradation). We\nachieve this through an end-to-end training paradigm that integrates\nbudget-mode control throughout the entire pipeline: budget-mode supervised\nfine-tuning that embeds controllable reasoning capabilities directly into the\nlearning process, and two-phase budget-aware reinforcement learning with\nadaptive reward shaping. Extensive experiments demonstrate that ThinkDial\nachieves target compression-performance trade-offs with clear response length\nreductions while maintaining performance thresholds. The framework also\nexhibits strong generalization capabilities on out-of-distribution tasks.", "AI": {"tldr": "ThinkDial is the first open-recipe end-to-end framework that implements gpt-oss-style controllable reasoning through discrete operational modes.", "motivation": "Controlling the computational effort of large language models (LLMs) with chain-of-thought reasoning remains a significant challenge. The open-source community has largely failed to achieve gpt-oss-style controllable reasoning through discrete operational modes.", "method": "an end-to-end training paradigm that integrates budget-mode control throughout the entire pipeline: budget-mode supervised fine-tuning and two-phase budget-aware reinforcement learning with adaptive reward shaping.", "result": "ThinkDial enables seamless switching between three distinct reasoning regimes: High mode (full reasoning capability), Medium mode (50 percent token reduction with <10 percent performance degradation), and Low mode (75 percent token reduction with <15 percent performance degradation).", "conclusion": "ThinkDial achieves target compression-performance trade-offs with clear response length reductions while maintaining performance thresholds, and exhibits strong generalization capabilities on out-of-distribution tasks."}}
{"id": "2508.18914", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18914", "abs": "https://arxiv.org/abs/2508.18914", "authors": ["Yanxing Huang", "Xinling Jin", "Sijie Liang", "Peng Li", "Yang Liu"], "title": "FormaRL: Enhancing Autoformalization with no Labeled Data", "comment": "Conference paper at COLM2025", "summary": "Autoformalization is one of the central tasks in formal verification, while\nits advancement remains hindered due to the data scarcity and the absence\nefficient methods. In this work we propose \\textbf{FormaRL}, a simple yet\nefficient reinforcement learning framework for autoformalization which only\nrequires a small amount of unlabeled data. FormaRL integrates syntax check from\nLean compiler and consistency check from large language model to calculate the\nreward, and adopts GRPO algorithm to update the formalizer. We also curated a\nproof problem dataset from undergraduate-level math materials, named\n\\textbf{uproof}, in the hope to facilitate the exploration of autoformalization\nand theorem proving in advanced math. Experiments show that FormaRL can\nincrease the pass@1 autoformalization accuracy of Qwen2.5-Coder-7B-Instruct by\n4 $\\sim$ 6x (4.04\\% $\\to$ 26.15\\% on ProofNet and 2.4\\% $\\to$ 9.6\\% on uproof)\nwith merely 859 unlabeled data. And on uproof our method also achieved a strong\nimprovement in out-of-distribution performance compared to existing open-source\nstate-of-the-art autoformalizers on both pass@1 accuracy (6.2\\% $\\to$ 9.6\\%)\nand pass@16 accuracy (24.4\\% $\\to$ 33.6\\%). Training code of FormaRL is\nopen-sourced at https://github.com/THUNLP-MT/FormaRL.", "AI": {"tldr": "FormaRL\u662f\u4e00\u79cd\u7528\u4e8eautoformalization\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u5c11\u91cf\u672a\u6807\u8bb0\u6570\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u5e76\u5728out-of-distribution\u6570\u636e\u4e0a\u8868\u73b0\u826f\u597d\u3002", "motivation": "Autoformalization\u662f\u5f62\u5f0f\u9a8c\u8bc1\u4e2d\u7684\u6838\u5fc3\u4efb\u52a1\u4e4b\u4e00\uff0c\u4f46\u7531\u4e8e\u6570\u636e\u7a00\u7f3a\u548c\u7f3a\u4e4f\u6709\u6548\u65b9\u6cd5\uff0c\u5176\u8fdb\u5c55\u4ecd\u7136\u53d7\u5230\u963b\u788d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684autoformalization\u6846\u67b6FormaRL\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86Lean\u7f16\u8bd1\u5668\u7684\u8bed\u6cd5\u68c0\u67e5\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e00\u81f4\u6027\u68c0\u67e5\u6765\u8ba1\u7b97\u5956\u52b1\uff0c\u5e76\u91c7\u7528GRPO\u7b97\u6cd5\u6765\u66f4\u65b0formalizer\u3002", "result": "FormaRL\u5c06Qwen2.5-Coder-7B-Instruct\u7684pass@1 autoformalization \u51c6\u786e\u7387\u63d0\u9ad8\u4e864\u52306\u500d(ProofNet\u4e0a\u4e3a4.04%\u219226.15%\uff0cuproof\u4e0a\u4e3a2.4%\u21929.6%)\uff0c\u4ec5\u4f7f\u7528\u4e86859\u4e2a\u672a\u6807\u8bb0\u6570\u636e\u3002\u5728uproof\u4e0a\uff0c\u4e0e\u73b0\u6709\u7684\u5f00\u6e90state-of-the-art autoformalizer\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728pass@1\u51c6\u786e\u7387(6.2%\u21929.6%)\u548cpass@16\u51c6\u786e\u7387(24.4%\u219233.6%)\u65b9\u9762\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u7684out-of-distribution\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "FormaRL\u663e\u8457\u63d0\u9ad8\u4e86autoformalization\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728out-of-distribution\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2508.18579", "categories": ["cs.LG", "cs.AI", "q-bio.QM", "I.2.7; J.3; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.18579", "abs": "https://arxiv.org/abs/2508.18579", "authors": ["Mohammadreza Ghaffarzadeh-Esfahani", "Ali Motahharynia", "Nahid Yousefian", "Navid Mazrouei", "Jafar Ghaisari", "Yousof Gheisari"], "title": "DrugReasoner: Interpretable Drug Approval Prediction with a Reasoning-augmented Language Model", "comment": "13 pages, 2 figures. Corresponding author: alimotahharynia@gmail.com", "summary": "Drug discovery is a complex and resource-intensive process, making early\nprediction of approval outcomes critical for optimizing research investments.\nWhile classical machine learning and deep learning methods have shown promise\nin drug approval prediction, their limited interpretability constraints their\nimpact. Here, we present DrugReasoner, a reasoning-based large language model\n(LLM) built on the LLaMA architecture and fine-tuned with group relative policy\noptimization (GRPO) to predict the likelihood of small-molecule approval.\nDrugReasoner integrates molecular descriptors with comparative reasoning\nagainst structurally similar approved and unapproved compounds, generating\npredictions alongside step-by-step rationales and confidence scores.\nDrugReasoner achieved robust performance with an AUC of 0.732 and an F1 score\nof 0.729 on the validation set and 0.725 and 0.718 on the test set,\nrespectively. These results outperformed conventional baselines, including\nlogistic regression, support vector machine, and k-nearest neighbors and had\ncompetitive performance relative to XGBoost. On an external independent\ndataset, DrugReasoner outperformed both baseline and the recently developed\nChemAP model, achieving an AUC of 0.728 and an F1-score of 0.774, while\nmaintaining high precision and balanced sensitivity, demonstrating robustness\nin real-world scenarios. These findings demonstrate that DrugReasoner not only\ndelivers competitive predictive accuracy but also enhances transparency through\nits reasoning outputs, thereby addressing a key bottleneck in AI-assisted drug\ndiscovery. This study highlights the potential of reasoning-augmented LLMs as\ninterpretable and effective tools for pharmaceutical decision-making.", "AI": {"tldr": "DrugReasoner\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u836f\u7269\u5ba1\u6279\u9884\u6d4b\u6a21\u578b\uff0c\u5b83\u901a\u8fc7\u63a8\u7406\u63d0\u9ad8\u4e86\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u6027\u3002", "motivation": "\u836f\u7269\u53d1\u73b0\u662f\u4e00\u4e2a\u590d\u6742\u4e14\u8d44\u6e90\u5bc6\u96c6\u7684\u8fc7\u7a0b\uff0c\u56e0\u6b64\u65e9\u671f\u9884\u6d4b\u6279\u51c6\u7ed3\u679c\u5bf9\u4e8e\u4f18\u5316\u7814\u7a76\u6295\u8d44\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u7ecf\u5178\u7684\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u836f\u7269\u6279\u51c6\u9884\u6d4b\u4e2d\u663e\u793a\u51fa\u5e0c\u671b\uff0c\u4f46\u5176\u6709\u9650\u7684\u53ef\u89e3\u91ca\u6027\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5f71\u54cd\u3002", "method": "\u6784\u5efa\u5728LLaMA\u67b6\u6784\u4e4b\u4e0a\uff0c\u5e76\u4f7f\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316 (GRPO) \u8fdb\u884c\u5fae\u8c03\u7684\u57fa\u4e8e\u63a8\u7406\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) DrugReasoner\uff0c\u7528\u4e8e\u9884\u6d4b\u5c0f\u5206\u5b50\u6279\u51c6\u7684\u53ef\u80fd\u6027\u3002DrugReasoner\u5c06\u5206\u5b50\u63cf\u8ff0\u7b26\u4e0e\u9488\u5bf9\u7ed3\u6784\u76f8\u4f3c\u7684\u5df2\u6279\u51c6\u548c\u672a\u6279\u51c6\u5316\u5408\u7269\u7684\u6bd4\u8f83\u63a8\u7406\u76f8\u7ed3\u5408\uff0c\u751f\u6210\u9884\u6d4b\u4ee5\u53ca\u9010\u6b65\u7684\u7406\u7531\u548c\u7f6e\u4fe1\u5ea6\u3002", "result": "DrugReasoner\u5728\u9a8c\u8bc1\u96c6\u4e0a\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u6027\u80fd\uff0cAUC \u4e3a 0.732\uff0cF1 \u5206\u6570\u4e3a 0.729\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u5206\u522b\u5b9e\u73b0\u4e86 0.725 \u548c 0.718\u3002\u5728\u5916\u90e8\u72ec\u7acb\u6570\u636e\u96c6\u4e0a\uff0cDrugReasoner \u4f18\u4e8e\u57fa\u7ebf\u548c\u6700\u8fd1\u5f00\u53d1\u7684 ChemAP \u6a21\u578b\uff0c\u5b9e\u73b0\u4e86 0.728 \u7684 AUC \u548c 0.774 \u7684 F1 \u5206\u6570\u3002", "conclusion": "DrugReasoner\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u6709\u7ade\u4e89\u529b\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u800c\u4e14\u901a\u8fc7\u5176\u63a8\u7406\u8f93\u51fa\u589e\u5f3a\u4e86\u900f\u660e\u5ea6\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u4eba\u5de5\u667a\u80fd\u8f85\u52a9\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u74f6\u9888\u3002\u8fd9\u9879\u7814\u7a76\u5f3a\u8c03\u4e86\u63a8\u7406\u589e\u5f3a\u7684LLM\u4f5c\u4e3a\u836f\u7269\u51b3\u7b56\u7684\u53ef\u89e3\u91ca\u548c\u6709\u6548\u7684\u5de5\u5177\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.18729", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18729", "abs": "https://arxiv.org/abs/2508.18729", "authors": ["Melanie Wille", "Tobias Fischer", "Scarlett Raine"], "title": "Are All Marine Species Created Equal? Performance Disparities in Underwater Object Detection", "comment": "10 pages", "summary": "Underwater object detection is critical for monitoring marine ecosystems but\nposes unique challenges, including degraded image quality, imbalanced class\ndistribution, and distinct visual characteristics. Not every species is\ndetected equally well, yet underlying causes remain unclear. We address two key\nresearch questions: 1) What factors beyond data quantity drive class-specific\nperformance disparities? 2) How can we systematically improve detection of\nunder-performing marine species? We manipulate the DUO dataset to separate the\nobject detection task into localization and classification and investigate the\nunder-performance of the scallop class. Localization analysis using YOLO11 and\nTIDE finds that foreground-background discrimination is the most problematic\nstage regardless of data quantity. Classification experiments reveal persistent\nprecision gaps even with balanced data, indicating intrinsic feature-based\nchallenges beyond data scarcity and inter-class dependencies. We recommend\nimbalanced distributions when prioritizing precision, and balanced\ndistributions when prioritizing recall. Improving under-performing classes\nshould focus on algorithmic advances, especially within localization modules.\nWe publicly release our code and datasets.", "AI": {"tldr": "This paper analyzes the challenges of underwater object detection, particularly the under-performance of certain marine species. It finds that foreground-background discrimination is a major issue and suggests focusing on algorithmic advances in localization modules to improve detection performance.", "motivation": "Underwater object detection is critical for monitoring marine ecosystems but poses unique challenges, including degraded image quality, imbalanced class distribution, and distinct visual characteristics. Not every species is detected equally well, yet underlying causes remain unclear.", "method": "We manipulate the DUO dataset to separate the object detection task into localization and classification and investigate the under-performance of the scallop class. Localization analysis using YOLO11 and TIDE finds that foreground-background discrimination is the most problematic stage regardless of data quantity. Classification experiments reveal persistent precision gaps even with balanced data, indicating intrinsic feature-based challenges beyond data scarcity and inter-class dependencies.", "result": "Localization analysis using YOLO11 and TIDE finds that foreground-background discrimination is the most problematic stage regardless of data quantity. Classification experiments reveal persistent precision gaps even with balanced data, indicating intrinsic feature-based challenges beyond data scarcity and inter-class dependencies. We recommend imbalanced distributions when prioritizing precision, and balanced distributions when prioritizing recall.", "conclusion": "Improving under-performing classes should focus on algorithmic advances, especially within localization modules. We publicly release our code and datasets."}}
{"id": "2508.18780", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18780", "abs": "https://arxiv.org/abs/2508.18780", "authors": ["Yilin Li", "Xunjian Yin", "Yilin Chen", "Xiaojun Wan"], "title": "Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction", "comment": "Code will be released upon publication", "summary": "Grammatical error correction is a significant task in NLP. Traditional\nmethods based on encoder-decoder models have achieved certain success, but the\napplication of LLMs in this field is still underexplored. Current research\npredominantly relies on supervised fine-tuning to train LLMs to directly\ngenerate the corrected sentence, which limits the model's powerful reasoning\nability. To address this limitation, we propose a novel framework based on\nRule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL\nframework achieves \\textbf{state-of-the-art }performance, with a notable\nincrease in \\textbf{recall}. This result clearly highlights the advantages of\nusing RL to steer LLMs, offering a more controllable and reliable paradigm for\nfuture development in GEC.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6765\u5f15\u5bfcLLM\u8fdb\u884c\u8bed\u6cd5\u7ea0\u9519\uff0c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4e2d\u6587\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7684\u7814\u7a76\u4e3b\u8981\u4f9d\u9760\u6709\u76d1\u7763\u7684\u5fae\u8c03\u6765\u8bad\u7ec3LLM\u76f4\u63a5\u751f\u6210\u4fee\u6b63\u540e\u7684\u53e5\u5b50\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u89c4\u5219\u7684\u5f3a\u5316\u5b66\u4e60(Rule-Based RL)\u3002", "result": "\u5728\u4e2d\u6587\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u57fa\u4e8e\u89c4\u5219\u7684RL\u6846\u67b6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u53ec\u56de\u7387\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528RL\u6765\u5f15\u5bfcLLM\uff0c\u4e3aGEC\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u53ef\u63a7\u548c\u53ef\u9760\u7684\u8303\u4f8b\u3002"}}
{"id": "2508.18925", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18925", "abs": "https://arxiv.org/abs/2508.18925", "authors": ["Qian Xiao", "Conn Breathnach", "Ioana Ghergulescu", "Conor O'Sullivan", "Keith Johnston", "Vincent Wade"], "title": "Who Is Lagging Behind: Profiling Student Behaviors with Graph-Level Encoding in Curriculum-Based Online Learning Systems", "comment": null, "summary": "The surge in the adoption of Intelligent Tutoring Systems (ITSs) in\neducation, while being integral to curriculum-based learning, can inadvertently\nexacerbate performance gaps. To address this problem, student profiling becomes\ncrucial for tracking progress, identifying struggling students, and alleviating\ndisparities among students. Such profiling requires measuring student behaviors\nand performance across different aspects, such as content coverage, learning\nintensity, and proficiency in different concepts within a learning topic.\n  In this study, we introduce CTGraph, a graph-level representation learning\napproach to profile learner behaviors and performance in a self-supervised\nmanner. Our experiments demonstrate that CTGraph can provide a holistic view of\nstudent learning journeys, accounting for different aspects of student\nbehaviors and performance, as well as variations in their learning paths as\naligned to the curriculum structure. We also show that our approach can\nidentify struggling students and provide comparative analysis of diverse groups\nto pinpoint when and where students are struggling. As such, our approach opens\nmore opportunities to empower educators with rich insights into student\nlearning journeys and paves the way for more targeted interventions.", "AI": {"tldr": "\u63d0\u51faCTGraph\uff0c\u4ee5\u56fe\u7ea7\u522b\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u6765\u5206\u6790\u5b66\u751f\u884c\u4e3a\u548c\u8868\u73b0\uff0c\u4ece\u800c\u80fd\u591f\u8ddf\u8e2a\u5b66\u751f\u5b66\u4e60\u8fc7\u7a0b\u3001\u8bc6\u522b\u56f0\u96be\u5b66\u751f\u5e76\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\u3002", "motivation": "\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\uff08ITS\uff09\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u6fc0\u589e\uff0c\u4f46\u5728\u57fa\u4e8e\u8bfe\u7a0b\u7684\u5b66\u4e60\u4e2d\uff0c\u53ef\u80fd\u4f1a\u65e0\u610f\u4e2d\u52a0\u5267\u6210\u7ee9\u5dee\u8ddd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5b66\u751f\u753b\u50cf\u5bf9\u4e8e\u8ddf\u8e2a\u8fdb\u5ea6\u3001\u8bc6\u522b\u56f0\u96be\u5b66\u751f\u548c\u7f13\u89e3\u5b66\u751f\u4e4b\u95f4\u7684\u5dee\u5f02\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u56fe\u7ea7\u8868\u5f81\u5b66\u4e60\u65b9\u6cd5CTGraph\uff0c\u4ee5\u81ea\u76d1\u7763\u7684\u65b9\u5f0f\u5206\u6790\u5b66\u4e60\u8005\u7684\u884c\u4e3a\u548c\u8868\u73b0\u3002", "result": "CTGraph\u53ef\u4ee5\u5168\u9762\u4e86\u89e3\u5b66\u751f\u7684\u5b66\u4e60\u8fc7\u7a0b\uff0c\u80fd\u591f\u8bc6\u522b\u6709\u56f0\u96be\u7684\u5b66\u751f\uff0c\u5e76\u5bf9\u4e0d\u540c\u7fa4\u4f53\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\uff0c\u4ee5\u786e\u5b9a\u5b66\u751f\u5728\u4f55\u65f6\u4f55\u5730\u9047\u5230\u56f0\u96be\u3002", "conclusion": "CTGraph\u53ef\u4ee5\u5168\u9762\u4e86\u89e3\u5b66\u751f\u7684\u5b66\u4e60\u8fc7\u7a0b\uff0c\u80fd\u591f\u8bc6\u522b\u6709\u56f0\u96be\u7684\u5b66\u751f\uff0c\u5e76\u5bf9\u4e0d\u540c\u7fa4\u4f53\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\uff0c\u4ee5\u786e\u5b9a\u5b66\u751f\u5728\u4f55\u65f6\u4f55\u5730\u9047\u5230\u56f0\u96be\u3002\u4e3a\u6559\u80b2\u5de5\u4f5c\u8005\u63d0\u4f9b\u6df1\u5165\u4e86\u89e3\u5b66\u751f\u5b66\u4e60\u8fc7\u7a0b\u7684\u9014\u5f84\uff0c\u5e76\u4e3a\u66f4\u6709\u9488\u5bf9\u6027\u7684\u5e72\u9884\u63aa\u65bd\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2508.18588", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.18588", "abs": "https://arxiv.org/abs/2508.18588", "authors": ["Jingkai He", "Tianjian Li", "Erhu Feng", "Dong Du", "Qian Liu", "Tao Liu", "Yubin Xia", "Haibo Chen"], "title": "History Rhymes: Accelerating LLM Reinforcement Learning with RhymeRL", "comment": null, "summary": "With the rapid advancement of large language models (LLMs), reinforcement\nlearning (RL) has emerged as a pivotal methodology for enhancing the reasoning\ncapabilities of LLMs. Unlike traditional pre-training approaches, RL\nencompasses multiple stages: rollout, reward, and training, which necessitates\ncollaboration among various worker types. However, current RL systems continue\nto grapple with substantial GPU underutilization, due to two primary factors:\n(1) The rollout stage dominates the overall RL process due to test-time\nscaling; (2) Imbalances in rollout lengths (within the same batch) result in\nGPU bubbles. While prior solutions like asynchronous execution and truncation\noffer partial relief, they may compromise training accuracy for efficiency.\n  Our key insight stems from a previously overlooked observation: rollout\nresponses exhibit remarkable similarity across adjacent training epochs. Based\non the insight, we introduce RhymeRL, an LLM RL system designed to accelerate\nRL training with two key innovations. First, to enhance rollout generation, we\npresent HistoSpec, a speculative decoding inference engine that utilizes the\nsimilarity of historical rollout token sequences to obtain accurate drafts.\nSecond, to tackle rollout bubbles, we introduce HistoPipe, a two-tier\nscheduling strategy that leverages the similarity of historical rollout\ndistributions to balance workload among rollout workers. We have evaluated\nRhymeRL within a real production environment, demonstrating scalability from\ndozens to thousands of GPUs. Experimental results demonstrate that RhymeRL\nachieves a 2.6x performance improvement over existing methods, without\ncompromising accuracy or modifying the RL paradigm.", "AI": {"tldr": "RhymeRL improves LLM RL training performance by 2.6x by addressing GPU underutilization issues with HistoSpec and HistoPipe, without sacrificing accuracy.", "motivation": "current RL systems continue to grapple with substantial GPU underutilization, due to two primary factors: (1) The rollout stage dominates the overall RL process due to test-time scaling; (2) Imbalances in rollout lengths (within the same batch) result in GPU bubbles.", "method": "introduces HistoSpec, a speculative decoding inference engine that utilizes the similarity of historical rollout token sequences to obtain accurate drafts and HistoPipe, a two-tier scheduling strategy that leverages the similarity of historical rollout distributions to balance workload among rollout workers.", "result": "RhymeRL achieves a 2.6x performance improvement over existing methods, without compromising accuracy or modifying the RL paradigm.", "conclusion": "RhymeRL achieves a 2.6x performance improvement over existing methods, without compromising accuracy or modifying the RL paradigm."}}
{"id": "2508.18733", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18733", "abs": "https://arxiv.org/abs/2508.18733", "authors": ["Feiwei Qin", "Shichao Lu", "Junhao Hou", "Changmiao Wang", "Meie Fang", "Ligang Liu"], "title": "Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vectorized Drawings", "comment": "Accepted to ACM MM 2025", "summary": "Computer-Aided Design (CAD) generative modeling is driving significant\ninnovations across industrial applications. Recent works have shown remarkable\nprogress in creating solid models from various inputs such as point clouds,\nmeshes, and text descriptions. However, these methods fundamentally diverge\nfrom traditional industrial workflows that begin with 2D engineering drawings.\nThe automatic generation of parametric CAD models from these 2D vector drawings\nremains underexplored despite being a critical step in engineering design. To\naddress this gap, our key insight is to reframe CAD generation as a\nsequence-to-sequence learning problem where vector drawing primitives directly\ninform the generation of parametric CAD operations, preserving geometric\nprecision and design intent throughout the transformation process. We propose\nDrawing2CAD, a framework with three key technical components: a\nnetwork-friendly vector primitive representation that preserves precise\ngeometric information, a dual-decoder transformer architecture that decouples\ncommand type and parameter generation while maintaining precise correspondence,\nand a soft target distribution loss function accommodating inherent flexibility\nin CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing,\na dataset of paired engineering drawings and parametric CAD models, and conduct\nthorough experiments to demonstrate the effectiveness of our method. Code and\ndataset are available at https://github.com/lllssc/Drawing2CAD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDrawing2CAD\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece2D\u5de5\u7a0b\u56fe\u81ea\u52a8\u751f\u6210\u53c2\u6570\u5316CAD\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u4e0e\u4ece2D\u5de5\u7a0b\u56fe\u5f00\u59cb\u7684\u4f20\u7edf\u5de5\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u5b58\u5728\u6839\u672c\u5dee\u5f02\u3002\u5c3d\u7ba1\u4ece\u8fd9\u4e9b2D\u77e2\u91cf\u56fe\u4e2d\u81ea\u52a8\u751f\u6210\u53c2\u6570\u5316CAD\u6a21\u578b\u662f\u5de5\u7a0b\u8bbe\u8ba1\u4e2d\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u4f46\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5c06CAD\u751f\u6210\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5e8f\u5217\u5230\u5e8f\u5217\u7684\u5b66\u4e60\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aDrawing2CAD\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6280\u672f\u7ec4\u4ef6\uff1a\u7f51\u7edc\u53cb\u597d\u7684\u77e2\u91cf\u56fe\u5143\u8868\u793a\u3001\u53cc\u89e3\u7801\u5668transformer\u67b6\u6784\u548c\u8f6f\u76ee\u6807\u5206\u5e03\u635f\u5931\u51fd\u6570\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u914d\u5bf9\u7684\u5de5\u7a0b\u56fe\u548c\u53c2\u6570\u5316CAD\u6a21\u578b\u7684\u6570\u636e\u96c6CAD-VGDrawing\uff0c\u5e76\u8fdb\u884c\u4e86\u5f7b\u5e95\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86Drawing2CAD\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.18783", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18783", "abs": "https://arxiv.org/abs/2508.18783", "authors": ["Igor Shalyminov", "Hang Su", "Jake Vincent", "Siffi Singh", "Jason Cai", "James Gung", "Raphael Shu", "Saab Mansour"], "title": "Controllable Conversational Theme Detection Track at DSTC 12", "comment": "DSTC12@SigDial2025; data and code available at\n  https://github.com/amazon-science/dstc12-controllable-conversational-theme-detection", "summary": "Conversational analytics has been on the forefront of transformation driven\nby the advances in Speech and Natural Language Processing techniques. Rapid\nadoption of Large Language Models (LLMs) in the analytics field has taken the\nproblems that can be automated to a new level of complexity and scale. In this\npaper, we introduce Theme Detection as a critical task in conversational\nanalytics, aimed at automatically identifying and categorizing topics within\nconversations. This process can significantly reduce the manual effort involved\nin analyzing expansive dialogs, particularly in domains like customer support\nor sales. Unlike traditional dialog intent detection, which often relies on a\nfixed set of intents for downstream system logic, themes are intended as a\ndirect, user-facing summary of the conversation's core inquiry. This\ndistinction allows for greater flexibility in theme surface forms and\nuser-specific customizations. We pose Controllable Conversational Theme\nDetection problem as a public competition track at Dialog System Technology\nChallenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of\ndialog utterances, with the distinctive aspect being controllability of the\nresulting theme clusters' granularity achieved via the provided user preference\ndata. We give an overview of the problem, the associated dataset and the\nevaluation metrics, both automatic and human. Finally, we discuss the\nparticipant teams' submissions and provide insights from those. The track\nmaterials (data and code) are openly available in the GitHub repository.", "AI": {"tldr": "This paper introduces theme detection in conversational analytics, poses it as a challenge at DSTC 12, and discusses the results of participant teams.", "motivation": "The rapid adoption of Large Language Models (LLMs) in the analytics field has taken the problems that can be automated to a new level of complexity and scale. This process can significantly reduce the manual effort involved in analyzing expansive dialogs, particularly in domains like customer support or sales.", "method": "The paper introduces Theme Detection as a critical task in conversational analytics, aimed at automatically identifying and categorizing topics within conversations. The paper poses Controllable Conversational Theme Detection problem as a public competition track at Dialog System Technology Challenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of dialog utterances, with the distinctive aspect being controllability of the resulting theme clusters' granularity achieved via the provided user preference data.", "result": "The track materials (data and code) are openly available in the GitHub repository.", "conclusion": "The paper discusses the participant teams' submissions and provides insights from those."}}
{"id": "2508.18933", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18933", "abs": "https://arxiv.org/abs/2508.18933", "authors": ["David Egea", "Barproda Halder", "Sanghamitra Dutta"], "title": "VISION: Robust and Interpretable Code Vulnerability Detection Leveraging Counterfactual Augmentation", "comment": null, "summary": "Automated detection of vulnerabilities in source code is an essential\ncybersecurity challenge, underpinning trust in digital systems and services.\nGraph Neural Networks (GNNs) have emerged as a promising approach as they can\nlearn structural and logical code relationships in a data-driven manner.\nHowever, their performance is severely constrained by training data imbalances\nand label noise. GNNs often learn 'spurious' correlations from superficial code\nsimilarities, producing detectors that fail to generalize well to unseen\nreal-world data. In this work, we propose a unified framework for robust and\ninterpretable vulnerability detection, called VISION, to mitigate spurious\ncorrelations by systematically augmenting a counterfactual training dataset.\nCounterfactuals are samples with minimal semantic modifications but opposite\nlabels. Our framework includes: (i) generating counterfactuals by prompting a\nLarge Language Model (LLM); (ii) targeted GNN training on paired code examples\nwith opposite labels; and (iii) graph-based interpretability to identify the\ncrucial code statements relevant for vulnerability predictions while ignoring\nspurious ones. We find that VISION reduces spurious learning and enables more\nrobust, generalizable detection, improving overall accuracy (from 51.8% to\n97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group\naccuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20\nvulnerability. We further demonstrate gains using proposed metrics: intra-class\nattribution variance, inter-class attribution distance, and node score\ndependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real\nand counterfactual) from the high-impact CWE-20 category. Finally, VISION\nadvances transparent and trustworthy AI-based cybersecurity systems through\ninteractive visualization for human-in-the-loop analysis.", "AI": {"tldr": "This paper introduces VISION, a framework that uses counterfactual training data generated by LLMs to improve the robustness and accuracy of GNNs for vulnerability detection in source code. VISION reduces spurious learning and enables more robust, generalizable detection.", "motivation": "Automated detection of vulnerabilities in source code is an essential cybersecurity challenge. GNNs are constrained by training data imbalances and label noise, often learning 'spurious' correlations from superficial code similarities, producing detectors that fail to generalize well to unseen real-world data.", "method": "a unified framework called VISION, to mitigate spurious correlations by systematically augmenting a counterfactual training dataset. Includes: (i) generating counterfactuals by prompting a Large Language Model (LLM); (ii) targeted GNN training on paired code examples with opposite labels; and (iii) graph-based interpretability to identify the crucial code statements relevant for vulnerability predictions while ignoring spurious ones.", "result": "VISION reduces spurious learning and enables more robust, generalizable detection, improving overall accuracy, pairwise contrast accuracy, and worst-group accuracy on the CWE-20 vulnerability. They also release CWE-20-CFA, a benchmark of 27,556 functions. They further demonstrate gains using proposed metrics: intra-class attribution variance, inter-class attribution distance, and node score dependency.", "conclusion": "VISION reduces spurious learning and enables more robust, generalizable detection, improving overall accuracy (from 51.8% to 97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group accuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20 vulnerability. VISION advances transparent and trustworthy AI-based cybersecurity systems through interactive visualization for human-in-the-loop analysis."}}
{"id": "2508.18596", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18596", "abs": "https://arxiv.org/abs/2508.18596", "authors": ["Zhao-Rong Lai", "Haisheng Yang"], "title": "Linear Trading Position with Sparse Spectrum", "comment": "IJCAI2025", "summary": "The principal portfolio approach is an emerging method in signal-based\ntrading. However, these principal portfolios may not be diversified to explore\nthe key features of the prediction matrix or robust to different situations. To\naddress this problem, we propose a novel linear trading position with sparse\nspectrum that can explore a larger spectral region of the prediction matrix. We\nalso develop a Krasnosel'ski\\u \\i-Mann fixed-point algorithm to optimize this\ntrading position, which possesses the descent property and achieves a linear\nconvergence rate in the objective value. This is a new theoretical result for\nthis type of algorithms. Extensive experiments show that the proposed method\nachieves good and robust performance in various situations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ebf\u6027\u4ea4\u6613\u4f4d\u7f6e\uff0c\u5177\u6709\u7a00\u758f\u9891\u8c31\uff0c\u53ef\u4ee5\u63a2\u7d22\u9884\u6d4b\u77e9\u9635\u7684\u66f4\u5927\u9891\u8c31\u533a\u57df\u3002", "motivation": "\u4e3b\u8981\u7684\u6295\u8d44\u7ec4\u5408\u65b9\u6cd5\u662f\u57fa\u4e8e\u4fe1\u53f7\u7684\u4ea4\u6613\u4e2d\u4e00\u79cd\u65b0\u5174\u7684\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u4e3b\u8981\u7684\u6295\u8d44\u7ec4\u5408\u53ef\u80fd\u6ca1\u6709\u591a\u6837\u5316\uff0c\u65e0\u6cd5\u63a2\u7d22\u9884\u6d4b\u77e9\u9635\u7684\u5173\u952e\u7279\u5f81\u6216\u5728\u4e0d\u540c\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u7a33\u5065\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5177\u6709\u7a00\u758f\u9891\u8c31\u7684\u7ebf\u6027\u4ea4\u6613\u4f4d\u7f6e\uff0c\u5b83\u53ef\u4ee5\u63a2\u7d22\u9884\u6d4b\u77e9\u9635\u7684\u66f4\u5927\u9891\u8c31\u533a\u57df\u3002\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd Krasnosel'ski\\{-Mann \u4e0d\u52a8\u70b9\u7b97\u6cd5\u6765\u4f18\u5316\u8be5\u4ea4\u6613\u4f4d\u7f6e\uff0c\u8be5\u7b97\u6cd5\u5177\u6709\u4e0b\u964d\u7279\u6027\uff0c\u5e76\u5728\u76ee\u6807\u503c\u4e2d\u5b9e\u73b0\u4e86\u7ebf\u6027\u6536\u655b\u901f\u5ea6\u3002", "result": "\u4e3a\u8fd9\u7c7b\u7b97\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u7406\u8bba\u7ed3\u679c\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u60c5\u51b5\u4e0b\u90fd\u53d6\u5f97\u4e86\u826f\u597d\u800c\u7a33\u5065\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u60c5\u51b5\u4e0b\u90fd\u53d6\u5f97\u4e86\u826f\u597d\u800c\u7a33\u5065\u7684\u6027\u80fd\u3002"}}
{"id": "2508.18734", "categories": ["cs.CV", "cs.AI", "cs.MM", "eess.AS", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.18734", "abs": "https://arxiv.org/abs/2508.18734", "authors": ["DongHoon Lim", "YoungChae Kim", "Dong-Hyun Kim", "Da-Hee Yang", "Joon-Hyuk Chang"], "title": "Improving Noise Robust Audio-Visual Speech Recognition via Router-Gated Cross-Modal Feature Fusion", "comment": "Accepted to IEEE ASRU 2025", "summary": "Robust audio-visual speech recognition (AVSR) in noisy environments remains\nchallenging, as existing systems struggle to estimate audio reliability and\ndynamically adjust modality reliance. We propose router-gated cross-modal\nfeature fusion, a novel AVSR framework that adaptively reweights audio and\nvisual features based on token-level acoustic corruption scores. Using an\naudio-visual feature fusion-based router, our method down-weights unreliable\naudio tokens and reinforces visual cues through gated cross-attention in each\ndecoder layer. This enables the model to pivot toward the visual modality when\naudio quality deteriorates. Experiments on LRS3 demonstrate that our approach\nachieves an 16.51-42.67% relative reduction in word error rate compared to\nAV-HuBERT. Ablation studies confirm that both the router and gating mechanism\ncontribute to improved robustness under real-world acoustic noise.", "AI": {"tldr": "This paper introduces a router-gated cross-modal feature fusion method for robust AVSR in noisy environments, which adaptively reweights audio and visual features. It achieves significant WER reduction compared to AV-HuBERT on LRS3.", "motivation": "Robust audio-visual speech recognition (AVSR) in noisy environments remains challenging, as existing systems struggle to estimate audio reliability and dynamically adjust modality reliance.", "method": "The paper proposes a router-gated cross-modal feature fusion framework that adaptively reweights audio and visual features based on token-level acoustic corruption scores. It uses an audio-visual feature fusion-based router to down-weight unreliable audio tokens and reinforces visual cues through gated cross-attention in each decoder layer.", "result": "Experiments on LRS3 demonstrate that the proposed approach achieves an 16.51-42.67% relative reduction in word error rate compared to AV-HuBERT.", "conclusion": "The proposed router-gated cross-modal feature fusion method achieves a significant reduction in word error rate compared to AV-HuBERT, demonstrating improved robustness under real-world acoustic noise. The router and gating mechanism both contribute to the improvement."}}
{"id": "2508.18791", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18791", "abs": "https://arxiv.org/abs/2508.18791", "authors": ["Ziming Zhu", "Chenglong Wang", "Shunjie Xing", "Yifu Huo", "Fengning Tian", "Quan Du", "Di Yang", "Chunliang Zhang", "Tong Xiao", "Jingbo Zhu"], "title": "LaTeXTrans: Structured LaTeX Translation with Multi-Agent Coordination", "comment": null, "summary": "Despite the remarkable progress of modern machine translation (MT) systems on\ngeneral-domain texts, translating structured LaTeX-formatted documents remains\na significant challenge. These documents typically interleave natural language\nwith domain-specific syntax, such as mathematical equations, tables, figures,\nand cross-references, all of which must be accurately preserved to maintain\nsemantic integrity and compilability. In this paper, we introduce LaTeXTrans, a\ncollaborative multi-agent system designed to address this challenge. LaTeXTrans\nensures format preservation, structural fidelity, and terminology consistency\nthrough six specialized agents: 1) a Parser that decomposes LaTeX into\ntranslation-friendly units via placeholder substitution and syntax filtering;\n2) a Translator, Validator, Summarizer, and Terminology Extractor that work\ncollaboratively to ensure context-aware, self-correcting, and\nterminology-consistent translations; 3) a Generator that reconstructs the\ntranslated content into well-structured LaTeX documents. Experimental results\ndemonstrate that LaTeXTrans can outperform mainstream MT systems in both\ntranslation accuracy and structural fidelity, offering an effective and\npractical solution for translating LaTeX-formatted documents.", "AI": {"tldr": "LaTeXTrans \u901a\u8fc7\u534f\u4f5c\u5f0f\u591a\u4ee3\u7406\u7cfb\u7edf\u6709\u6548\u7ffb\u8bd1 LaTeX \u683c\u5f0f\u6587\u6863\uff0c\u4f18\u4e8e\u4e3b\u6d41 MT \u7cfb\u7edf\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u7ffb\u8bd1 (MT) \u7cfb\u7edf\u5728\u901a\u7528\u9886\u57df\u6587\u672c\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7ffb\u8bd1\u7ed3\u6784\u5316\u7684 LaTeX \u683c\u5f0f\u6587\u6863\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u8fd9\u4e9b\u6587\u6863\u901a\u5e38\u5c06\u81ea\u7136\u8bed\u8a00\u4e0e\u9886\u57df\u7279\u5b9a\u7684\u8bed\u6cd5\uff08\u4f8b\u5982\u6570\u5b66\u65b9\u7a0b\u5f0f\u3001\u8868\u683c\u3001\u56fe\u5f62\u548c\u4ea4\u53c9\u5f15\u7528\uff09\u6df7\u5408\u5728\u4e00\u8d77\uff0c\u6240\u6709\u8fd9\u4e9b\u90fd\u5fc5\u987b\u51c6\u786e\u4fdd\u7559\u4ee5\u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027\u548c\u53ef\u7f16\u8bd1\u6027\u3002", "method": "LaTeXTrans\uff0c\u4e00\u4e2a\u534f\u4f5c\u5f0f\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u516d\u4e2a\u4e13\u95e8\u7684\u4ee3\u7406\u786e\u4fdd\u683c\u5f0f\u4fdd\u7559\u3001\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u672f\u8bed\u4e00\u81f4\u6027\uff1a1) \u89e3\u6790\u5668\uff0c\u901a\u8fc7\u5360\u4f4d\u7b26\u66ff\u6362\u548c\u8bed\u6cd5\u8fc7\u6ee4\u5c06 LaTeX \u5206\u89e3\u4e3a\u7ffb\u8bd1\u53cb\u597d\u7684\u5355\u5143\uff1b2) \u7ffb\u8bd1\u5668\u3001\u9a8c\u8bc1\u5668\u3001\u603b\u7ed3\u5668\u548c\u672f\u8bed\u63d0\u53d6\u5668\uff0c\u5b83\u4eec\u534f\u540c\u5de5\u4f5c\u4ee5\u786e\u4fdd\u4e0a\u4e0b\u6587\u611f\u77e5\u3001\u81ea\u6211\u7ea0\u6b63\u548c\u672f\u8bed\u4e00\u81f4\u7684\u7ffb\u8bd1\uff1b3) \u751f\u6210\u5668\uff0c\u5c06\u7ffb\u8bd1\u7684\u5185\u5bb9\u91cd\u5efa\u4e3a\u7ed3\u6784\u826f\u597d\u7684 LaTeX \u6587\u6863\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLaTeXTrans \u5728\u7ffb\u8bd1\u51c6\u786e\u6027\u548c\u7ed3\u6784\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8e\u4e3b\u6d41 MT \u7cfb\u7edf\u3002", "conclusion": "LaTeXTrans \u5728\u7ffb\u8bd1\u51c6\u786e\u6027\u548c\u7ed3\u6784\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8e\u4e3b\u6d41 MT \u7cfb\u7edf\uff0c\u4e3a\u7ffb\u8bd1 LaTeX \u683c\u5f0f\u6587\u6863\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18953", "categories": ["cs.AI", "I.2.6; I.2.8; I.5.1"], "pdf": "https://arxiv.org/pdf/2508.18953", "abs": "https://arxiv.org/abs/2508.18953", "authors": ["I. I. Priezzhev", "D. A. Danko", "A. V. Shubin"], "title": "Novel Approaches to Artificial Intelligence Development Based on the Nearest Neighbor Method", "comment": "18 pages, 6 figures. Novel hierarchical neural networks based on\n  k-nearest neighbors method for addressing hallucination effects, training\n  complexity, and catastrophic forgetting in modern AI systems. Includes\n  mathematical formulations using Kohonen self-organizing maps and experimental\n  validation on MNIST handwritten digit recognition and machine translation\n  tasks", "summary": "Modern neural network technologies, including large language models, have\nachieved remarkable success in various applied artificial intelligence\napplications, however, they face a range of fundamental limitations. Among them\nare hallucination effects, high computational complexity of training and\ninference, costly fine-tuning, and catastrophic forgetting issues. These\nlimitations significantly hinder the use of neural networks in critical areas\nsuch as medicine, industrial process management, and scientific research. This\narticle proposes an alternative approach based on the nearest neighbors method\nwith hierarchical clustering structures. Employing the k-nearest neighbors\nalgorithm significantly reduces or completely eliminates hallucination effects\nwhile simplifying model expansion and fine-tuning without the need for\nretraining the entire network. To overcome the high computational load of the\nk-nearest neighbors method, the paper proposes using tree-like data structures\nbased on Kohonen self-organizing maps, thereby greatly accelerating nearest\nneighbor searches. Tests conducted on handwritten digit recognition and simple\nsubtitle translation tasks confirmed the effectiveness of the proposed\napproach. With only a slight reduction in accuracy, the nearest neighbor search\ntime was reduced hundreds of times compared to exhaustive search methods. The\nproposed method features transparency and interpretability, closely aligns with\nhuman cognitive mechanisms, and demonstrates potential for extensive use in\ntasks requiring high reliability and explainable results.", "AI": {"tldr": "This paper introduces a nearest neighbor approach with hierarchical clustering to address limitations of neural networks, achieving faster search times with minimal accuracy loss, and offering better transparency and interpretability.", "motivation": "Modern neural networks face limitations such as hallucination, high computational complexity, costly fine-tuning, and catastrophic forgetting, hindering their use in critical areas.", "method": "The paper proposes a nearest neighbors method with hierarchical clustering structures using tree-like data structures based on Kohonen self-organizing maps to accelerate nearest neighbor searches.", "result": "Tests on handwritten digit recognition and subtitle translation showed a slight reduction in accuracy but hundreds of times faster nearest neighbor search time compared to exhaustive search methods.", "conclusion": "The proposed nearest neighbor method with hierarchical clustering achieves comparable accuracy with significantly reduced search time, offering transparency, interpretability, and potential for use in tasks requiring high reliability and explainability."}}
{"id": "2508.18630", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18630", "abs": "https://arxiv.org/abs/2508.18630", "authors": ["Weide Liu", "Xiaoyang Zhong", "Lu Wang", "Jingwen Hou", "Yuemei Luo", "Jiebin Yan", "Yuming Fang"], "title": "Uncertainty Awareness on Unsupervised Domain Adaptation for Time Series Data", "comment": "IEEE Transactions on Multimedia", "summary": "Unsupervised domain adaptation methods seek to generalize effectively on\nunlabeled test data, especially when encountering the common challenge in time\nseries data that distribution shifts occur between training and testing\ndatasets. In this paper, we propose incorporating multi-scale feature\nextraction and uncertainty estimation to improve the model's generalization and\nrobustness across domains. Our approach begins with a multi-scale mixed input\narchitecture that captures features at different scales, increasing training\ndiversity and reducing feature discrepancies between the training and testing\ndomains. Based on the mixed input architecture, we further introduce an\nuncertainty awareness mechanism based on evidential learning by imposing a\nDirichlet prior on the labels to facilitate both target prediction and\nuncertainty estimation. The uncertainty awareness mechanism enhances domain\nadaptation by aligning features with the same labels across different domains,\nwhich leads to significant performance improvements in the target domain.\nAdditionally, our uncertainty-aware model demonstrates a much lower Expected\nCalibration Error (ECE), indicating better-calibrated prediction confidence.\nOur experimental results show that this combined approach of mixed input\narchitecture with the uncertainty awareness mechanism achieves state-of-the-art\nperformance across multiple benchmark datasets, underscoring its effectiveness\nin unsupervised domain adaptation for time series data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u5728\u57df\u9002\u5e94\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\u4e4b\u95f4\u5206\u5e03\u504f\u79fb\u7684\u5e38\u89c1\u6311\u6218\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u672a\u6807\u8bb0\u6d4b\u8bd5\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "method": "\u7ed3\u5408\u4e86\u591a\u5c3a\u5ea6\u6df7\u5408\u8f93\u5165\u67b6\u6784\u548c\u57fa\u4e8e\u8bc1\u636e\u5b66\u4e60\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u673a\u5236\u3002", "result": "\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u4f4e\u7684\u9884\u671f\u6821\u51c6\u8bef\u5dee (ECE)\uff0c\u8868\u660e\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u5f97\u5230\u66f4\u597d\u6821\u51c6\u3002\u901a\u8fc7\u8de8\u4e0d\u540c\u57df\u5bf9\u9f50\u5177\u6709\u76f8\u540c\u6807\u7b7e\u7684\u7279\u5f81\u6765\u589e\u5f3a\u57df\u9002\u5e94\u6027\uff0c\u4ece\u800c\u663e\u7740\u63d0\u9ad8\u76ee\u6807\u57df\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7a81\u663e\u4e86\u5176\u5728\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65e0\u76d1\u7763\u57df\u9002\u5e94\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.18753", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18753", "abs": "https://arxiv.org/abs/2508.18753", "authors": ["Qinqian Lei", "Bo Wang", "Robby T. Tan"], "title": "Rethinking Human-Object Interaction Evaluation for both Vision-Language Models and HOI-Specific Methods", "comment": null, "summary": "Prior human-object interaction (HOI) detection methods have integrated early\nvision-language models (VLMs) such as CLIP, but only as supporting components\nwithin their frameworks. In contrast, recent advances in large, generative VLMs\nsuggest that these models may already possess strong ability to understand\nimages involving HOI. This naturally raises an important question: can\ngeneral-purpose standalone VLMs effectively solve HOI detection, and how do\nthey compare with specialized HOI methods? Answering this requires a benchmark\nthat can accommodate both paradigms. However, existing HOI benchmarks such as\nHICO-DET were developed before the emergence of modern VLMs, and their\nevaluation protocols require exact matches to annotated HOI classes. This is\npoorly aligned with the generative nature of VLMs, which often yield multiple\nvalid interpretations in ambiguous cases. For example, a static image may\ncapture a person mid-motion with a frisbee, which can plausibly be interpreted\nas either \"throwing\" or \"catching\". When only \"catching\" is annotated, the\nother, though equally plausible for the image, is marked incorrect when exact\nmatching is used. As a result, correct predictions might be penalized,\naffecting both VLMs and HOI-specific methods. To avoid penalizing valid\npredictions, we introduce a new benchmark that reformulates HOI detection as a\nmultiple-answer multiple-choice task, where each question includes only\nground-truth positive options and a curated set of negatives that are\nconstructed to reduce ambiguity (e.g., when \"catching\" is annotated, \"throwing\"\nis not selected as a negative to avoid penalizing valid predictions). The\nproposed evaluation protocol is the first of its kind for both VLMs and HOI\nmethods, enabling direct comparison and offering new insight into the current\nstate of progress in HOI understanding.", "AI": {"tldr": "A new HOI detection benchmark is introduced to better evaluate VLMs by using a multiple-answer multiple-choice format, addressing the limitations of existing benchmarks.", "motivation": "Existing HOI benchmarks are not well-suited for evaluating large, generative VLMs due to their exact match evaluation protocols and inability to handle multiple valid interpretations.", "method": "The paper reformulates HOI detection as a multiple-answer multiple-choice task with curated negative options to reduce ambiguity.", "result": "The paper introduces a new benchmark and evaluation protocol for HOI detection.", "conclusion": "The paper introduces a new benchmark for HOI detection that reformulates the task as a multiple-answer multiple-choice problem to address limitations of existing benchmarks when evaluating VLMs. The new benchmark and evaluation protocol enable direct comparison between VLMs and specialized HOI methods, providing new insights into HOI understanding."}}
{"id": "2508.18819", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.18819", "abs": "https://arxiv.org/abs/2508.18819", "authors": ["Shubham Gupta", "Shraban Kumar Chatterjee", "Suman Kundu"], "title": "LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph Autoencoders for Fake News Detection", "comment": null, "summary": "The proliferation of misinformation in the digital age has led to significant\nsocietal challenges. Existing approaches often struggle with capturing\nlong-range dependencies, complex semantic relations, and the social dynamics\ninfluencing news dissemination. Furthermore, these methods require extensive\nlabelled datasets, making their deployment resource-intensive. In this study,\nwe propose a novel self-supervised misinformation detection framework that\nintegrates both complex semantic relations using Abstract Meaning\nRepresentation (AMR) and news propagation dynamics. We introduce an LLM-based\ngraph contrastive loss (LGCL) that utilizes negative anchor points generated by\na Large Language Model (LLM) to enhance feature separability in a zero-shot\nmanner. To incorporate social context, we employ a multi view graph masked\nautoencoder, which learns news propagation features from social context graph.\nBy combining these semantic and propagation-based features, our approach\neffectively differentiates between fake and real news in a self-supervised\nmanner. Extensive experiments demonstrate that our self-supervised framework\nachieves superior performance compared to other state-of-the-art methodologies,\neven with limited labelled datasets while improving generalizability.", "AI": {"tldr": "A new self-supervised misinformation detection framework uses AMR, news propagation dynamics, and an LLM-based graph contrastive loss to outperform existing methods with less labeled data.", "motivation": "Existing misinformation detection approaches struggle with long-range dependencies, complex semantic relations, social dynamics, and require extensive labeled datasets, making them resource-intensive.", "method": "A novel self-supervised misinformation detection framework integrating complex semantic relations using Abstract Meaning Representation (AMR) and news propagation dynamics. It introduces an LLM-based graph contrastive loss (LGCL) and a multi-view graph masked autoencoder.", "result": "The self-supervised framework achieves superior performance compared to other state-of-the-art methodologies, even with limited labeled datasets while improving generalizability.", "conclusion": "The proposed self-supervised framework outperforms state-of-the-art methods in misinformation detection, even with limited labeled data, while improving generalizability."}}
{"id": "2508.18983", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18983", "abs": "https://arxiv.org/abs/2508.18983", "authors": ["Guoying Zhu", "Meng Li", "Haipeng Dai", "Xuechen Liu", "Weijun Wang", "Keran Li", "Jun xiao", "Ligeng Chen", "Wei Wang"], "title": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling", "comment": null, "summary": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy.", "AI": {"tldr": "This paper proposes an expert offloading method guided by expert importance and a scheduling policy to improve MoE deployment on edge devices, achieving lower latency and high cache hit rate with minimal accuracy loss.", "motivation": "Deploying Mixture of Experts (MoE) on consumer-grade edge hardware is constrained by limited device memory, making dynamic expert offloading essential.", "method": "The paper leverages expert importance to guide offloading decisions, substituting low-importance activated experts with functionally similar ones already cached in GPU memory. A scheduling policy that maximizes the reuse ratio of GPU-cached experts is also introduced.", "result": "Reduces memory usage and data transfer, while largely eliminating PCIe overhead; 48% lower decoding latency with over 60% expert cache hit rate, while maintaining nearly lossless accuracy.", "conclusion": "The proposed approach delivers 48% lower decoding latency with over 60% expert cache hit rate, while maintaining nearly lossless accuracy."}}
{"id": "2508.18635", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18635", "abs": "https://arxiv.org/abs/2508.18635", "authors": ["Yue Jiang", "Chenxi Liu", "Yile Chen", "Qin Chao", "Shuai Liu", "Gao Cong"], "title": "STRATA-TS: Selective Knowledge Transfer for Urban Time Series Forecasting with Retrieval-Guided Reasoning", "comment": null, "summary": "Urban forecasting models often face a severe data imbalance problem: only a\nfew cities have dense, long-span records, while many others expose short or\nincomplete histories. Direct transfer from data-rich to data-scarce cities is\nunreliable because only a limited subset of source patterns truly benefits the\ntarget domain, whereas indiscriminate transfer risks introducing noise and\nnegative transfer. We present STRATA-TS (Selective TRAnsfer via TArget-aware\nretrieval for Time Series), a framework that combines domain-adapted retrieval\nwith reasoning-capable large models to improve forecasting in scarce data\nregimes. STRATA-TS employs a patch-based temporal encoder to identify source\nsubsequences that are semantically and dynamically aligned with the target\nquery. These retrieved exemplars are then injected into a retrieval-guided\nreasoning stage, where an LLM performs structured inference over target inputs\nand retrieved support. To enable efficient deployment, we distill the reasoning\nprocess into a compact open model via supervised fine-tuning. Extensive\nexperiments on three parking availability datasets across Singapore,\nNottingham, and Glasgow demonstrate that STRATA-TS consistently outperforms\nstrong forecasting and transfer baselines, while providing interpretable\nknowledge transfer pathways.", "AI": {"tldr": "STRATA-TS \u7ed3\u5408\u9886\u57df\u81ea\u9002\u5e94\u68c0\u7d22\u548c\u5927\u578b\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u7a00\u7f3a\u6570\u636e\u72b6\u6001\u4e0b\u7684\u9884\u6d4b\u3002", "motivation": "\u57ce\u5e02\u9884\u6d4b\u6a21\u578b\u7ecf\u5e38\u9762\u4e34\u4e25\u91cd\u7684\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff1a\u53ea\u6709\u5c11\u6570\u57ce\u5e02\u62e5\u6709\u5bc6\u96c6\u3001\u957f\u8de8\u5ea6\u8bb0\u5f55\uff0c\u800c\u8bb8\u591a\u5176\u4ed6\u57ce\u5e02\u5219\u66b4\u9732\u4e8e\u77ed\u6216\u4e0d\u5b8c\u6574\u7684\u5386\u53f2\u3002\u4ece\u6570\u636e\u4e30\u5bcc\u7684\u57ce\u5e02\u76f4\u63a5\u8f6c\u79fb\u5230\u6570\u636e\u7a00\u7f3a\u7684\u57ce\u5e02\u662f\u4e0d\u53ef\u9760\u7684\uff0c\u56e0\u4e3a\u53ea\u6709\u6709\u9650\u7684\u6e90\u6a21\u5f0f\u5b50\u96c6\u771f\u6b63\u6709\u76ca\u4e8e\u76ee\u6807\u9886\u57df\uff0c\u800c\u6ee5\u7528\u8f6c\u79fb\u4f1a\u5e26\u6765\u5f15\u5165\u566a\u58f0\u548c\u8d1f\u9762\u8f6c\u79fb\u7684\u98ce\u9669\u3002", "method": "STRATA-TS \u7ed3\u5408\u4e86\u9886\u57df\u81ea\u9002\u5e94\u68c0\u7d22\u548c\u5177\u6709\u63a8\u7406\u80fd\u529b\u7684\u5927\u578b\u6a21\u578b\uff0c\u4ee5\u6539\u8fdb\u7a00\u7f3a\u6570\u636e\u72b6\u6001\u4e0b\u7684\u9884\u6d4b\u3002STRATA-TS \u91c7\u7528\u57fa\u4e8e\u8865\u4e01\u7684\u65f6\u95f4\u7f16\u7801\u5668\u6765\u8bc6\u522b\u5728\u8bed\u4e49\u548c\u52a8\u6001\u4e0a\u4e0e\u76ee\u6807\u67e5\u8be2\u5bf9\u9f50\u7684\u6e90\u5b50\u5e8f\u5217\u3002\u7136\u540e\u5c06\u8fd9\u4e9b\u68c0\u7d22\u5230\u7684\u793a\u4f8b\u6ce8\u5165\u5230\u68c0\u7d22\u5f15\u5bfc\u7684\u63a8\u7406\u9636\u6bb5\uff0c\u5728\u8be5\u9636\u6bb5\uff0cLLM \u5bf9\u76ee\u6807\u8f93\u5165\u548c\u68c0\u7d22\u5230\u7684\u652f\u6301\u6267\u884c\u7ed3\u6784\u5316\u63a8\u7406\u3002\u4e3a\u4e86\u5b9e\u73b0\u9ad8\u6548\u90e8\u7f72\uff0c\u6211\u4eec\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u5c06\u63a8\u7406\u8fc7\u7a0b\u63d0\u70bc\u6210\u4e00\u4e2a\u7d27\u51d1\u7684\u5f00\u653e\u6a21\u578b\u3002", "result": "STRATA-TS \u5728\u4e09\u4e2a\u505c\u8f66\u53ef\u7528\u6027\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSTRATA-TS \u59cb\u7ec8\u4f18\u4e8e\u5f3a\u5927\u7684\u9884\u6d4b\u548c\u8fc1\u79fb\u57fa\u7ebf\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u77e5\u8bc6\u8fc1\u79fb\u8def\u5f84\u3002", "conclusion": "STRATA-TS\u5728\u65b0\u52a0\u5761\u3001\u8bfa\u4e01\u6c49\u548c\u683c\u62c9\u65af\u54e5\u7684\u4e09\u4e2a\u505c\u8f66\u53ef\u7528\u6027\u6570\u636e\u96c6\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u5927\u7684\u9884\u6d4b\u548c\u8fc1\u79fb\u57fa\u7ebf\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u77e5\u8bc6\u8fc1\u79fb\u8def\u5f84\u3002"}}
{"id": "2508.18772", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18772", "abs": "https://arxiv.org/abs/2508.18772", "authors": ["Wanqiang Wang", "Longzhu He", "Wei Zheng"], "title": "Beyond the Textual: Generating Coherent Visual Options for MCQs", "comment": "EMNLP 2025", "summary": "Multiple-choice questions (MCQs) play a crucial role in fostering deep\nthinking and knowledge integration in education. However, previous research has\nprimarily focused on generating MCQs with textual options, but it largely\noverlooks the visual options. Moreover, generating high-quality distractors\nremains a major challenge due to the high cost and limited scalability of\nmanual authoring. To tackle these problems, we propose a Cross-modal Options\nSynthesis (CmOS), a novel framework for generating educational MCQs with visual\noptions. Our framework integrates Multimodal Chain-of-Thought (MCoT) reasoning\nprocess and Retrieval-Augmented Generation (RAG) to produce semantically\nplausible and visually similar answer and distractors. It also includes a\ndiscrimination module to identify content suitable for visual options.\nExperimental results on test tasks demonstrate the superiority of CmOS in\ncontent discrimination, question generation and visual option generation over\nexisting methods across various subjects and educational levels.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u751f\u6210\u5e26\u6709\u89c6\u89c9\u9009\u9879\u7684\u6559\u80b2 MCQ \u7684\u65b0\u6846\u67b6 CmOS\uff0c\u8be5\u6846\u67b6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4ee5\u5f80\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u751f\u6210\u5e26\u6709\u6587\u672c\u9009\u9879\u7684 MCQ\uff0c\u4f46\u5f88\u5927\u7a0b\u5ea6\u4e0a\u5ffd\u7565\u4e86\u89c6\u89c9\u9009\u9879\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u624b\u52a8\u521b\u4f5c\u7684\u9ad8\u6210\u672c\u548c\u6709\u9650\u7684\u53ef\u6269\u5c55\u6027\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5e72\u6270\u9879\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\u3002", "method": "\u4ea4\u53c9\u6a21\u6001\u9009\u9879\u5408\u6210 (CmOS)\uff0c\u96c6\u6210\u4e86\u591a\u6a21\u6001\u601d\u7ef4\u94fe (MCoT) \u63a8\u7406\u8fc7\u7a0b\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG)\u3002", "result": "\u5728\u6d4b\u8bd5\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4e86CmOS\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "CmOS\u5728\u5185\u5bb9\u8fa8\u522b\u3001\u95ee\u9898\u751f\u6210\u548c\u89c6\u89c9\u9009\u9879\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.18824", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18824", "abs": "https://arxiv.org/abs/2508.18824", "authors": ["Sirui Chen", "Changxin Tian", "Binbin Hu", "Kunlong Chen", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou"], "title": "Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity, Complexity and Correctness", "comment": null, "summary": "Enhancing the mathematical reasoning of large language models (LLMs) demands\nhigh-quality training data, yet conventional methods face critical challenges\nin scalability, cost, and data reliability. To address these limitations, we\npropose a novel program-assisted synthesis framework that systematically\ngenerates a high-quality mathematical corpus with guaranteed diversity,\ncomplexity, and correctness. This framework integrates mathematical knowledge\nsystems and domain-specific tools to create executable programs. These programs\nare then translated into natural language problem-solution pairs and vetted by\na bilateral validation mechanism that verifies solution correctness against\nprogram outputs and ensures program-problem consistency. We have generated 12.3\nmillion such problem-solving triples. Experiments demonstrate that models\nfine-tuned on our data significantly improve their inference capabilities,\nachieving state-of-the-art performance on several benchmark datasets and\nshowcasing the effectiveness of our synthesis approach.", "AI": {"tldr": "Proposes a program-assisted synthesis framework to generate a high-quality mathematical corpus, which improves the inference capabilities of LLMs.", "motivation": "Enhancing the mathematical reasoning of large language models (LLMs) demands high-quality training data, yet conventional methods face critical challenges in scalability, cost, and data reliability.", "method": "A novel program-assisted synthesis framework that systematically generates a high-quality mathematical corpus with guaranteed diversity, complexity, and correctness by integrating mathematical knowledge systems and domain-specific tools to create executable programs, which are then translated into natural language problem-solution pairs and vetted by a bilateral validation mechanism.", "result": "Generated 12.3 million problem-solving triples.", "conclusion": "Models fine-tuned on the generated data significantly improve their inference capabilities, achieving state-of-the-art performance on several benchmark datasets."}}
{"id": "2508.19004", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19004", "abs": "https://arxiv.org/abs/2508.19004", "authors": ["Pontus Strimling", "Simon Karlsson", "Irina Vartanova", "Kimmo Eriksson"], "title": "AI Models Exceed Individual Human Accuracy in Predicting Everyday Social Norms", "comment": "18 pages + supplementy materials", "summary": "A fundamental question in cognitive science concerns how social norms are\nacquired and represented. While humans typically learn norms through embodied\nsocial experience, we investigated whether large language models can achieve\nsophisticated norm understanding through statistical learning alone. Across two\nstudies, we systematically evaluated multiple AI systems' ability to predict\nhuman social appropriateness judgments for 555 everyday scenarios by examining\nhow closely they predicted the average judgment compared to each human\nparticipant. In Study 1, GPT-4.5's accuracy in predicting the collective\njudgment on a continuous scale exceeded that of every human participant (100th\npercentile). Study 2 replicated this, with Gemini 2.5 Pro outperforming 98.7%\nof humans, GPT-5 97.8%, and Claude Sonnet 4 96.0%. Despite this predictive\npower, all models showed systematic, correlated errors. These findings\ndemonstrate that sophisticated models of social cognition can emerge from\nstatistical learning over linguistic data alone, challenging strong versions of\ntheories emphasizing the exclusive necessity of embodied experience for\ncultural competence. The systematic nature of AI limitations across different\narchitectures indicates potential boundaries of pattern-based social\nunderstanding, while the models' ability to outperform nearly all individual\nhumans in this predictive task suggests that language serves as a remarkably\nrich repository for cultural knowledge transmission.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9884\u6d4b\u793e\u4f1a\u89c4\u8303\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u5b58\u5728\u5c40\u9650\u6027\uff0c\u6697\u793a\u4e86\u6587\u5316\u77e5\u8bc6\u53ef\u4ee5\u901a\u8fc7\u8bed\u8a00\u8fdb\u884c\u4f20\u64ad\u3002", "motivation": "\u63a2\u8ba8\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\uff1a\u793e\u4f1a\u89c4\u8303\u662f\u5982\u4f55\u83b7\u5f97\u548c\u8868\u793a\u7684\u3002\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u7eaf\u7cb9\u7684\u7edf\u8ba1\u5b66\u4e60\u6765\u5b9e\u73b0\u5bf9\u793e\u4f1a\u89c4\u8303\u7684\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u8bc4\u4f30\u591a\u4e2aAI\u7cfb\u7edf\u9884\u6d4b555\u4e2a\u65e5\u5e38\u573a\u666f\u4e2d\u4eba\u7c7b\u793e\u4f1a\u9002\u5f53\u6027\u5224\u65ad\u7684\u80fd\u529b\uff0c\u5e76\u5c06\u5176\u9884\u6d4b\u7ed3\u679c\u4e0e\u4eba\u7c7b\u53c2\u4e0e\u8005\u7684\u5e73\u5747\u5224\u65ad\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "GPT-4.5\u5728\u9884\u6d4b\u96c6\u4f53\u5224\u65ad\u65b9\u9762\u7684\u51c6\u786e\u6027\u8d85\u8fc7\u4e86\u6240\u6709\u4eba\u7c7b\u53c2\u4e0e\u8005\uff08\u7b2c100\u767e\u5206\u4f4d\uff09\uff0cGemini 2.5 Pro\u8d85\u8fc7\u4e8698.7%\u7684\u4eba\u7c7b\uff0cGPT-5\u8d85\u8fc7\u4e8697.8%\uff0cClaude Sonnet 4\u8d85\u8fc7\u4e8696.0%\u3002\u6240\u6709\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u7cfb\u7edf\u6027\u7684\u76f8\u5173\u9519\u8bef\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u7eaf\u7cb9\u7684\u7edf\u8ba1\u5b66\u4e60\uff0c\u5728\u9884\u6d4b\u4eba\u7c7b\u793e\u4f1a\u9002\u5f53\u6027\u5224\u65ad\u65b9\u9762\u8d85\u8d8a\u7edd\u5927\u591a\u6570\u4e2a\u4f53\u4eba\u7c7b\uff0c\u8fd9\u8868\u660e\u8bed\u8a00\u662f\u6587\u5316\u77e5\u8bc6\u4f20\u64ad\u7684\u4e30\u5bcc\u77e5\u8bc6\u5e93\uff0c\u540c\u65f6\u4e5f\u63ed\u793a\u4e86\u57fa\u4e8e\u6a21\u5f0f\u7684\u793e\u4f1a\u7406\u89e3\u7684\u6f5c\u5728\u5c40\u9650\u6027\u3002"}}
{"id": "2508.18638", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.18638", "abs": "https://arxiv.org/abs/2508.18638", "authors": ["Ifrah Tariq", "Ernest Fraenkel"], "title": "Biologically Disentangled Multi-Omic Modeling Reveals Mechanistic Insights into Pan-Cancer Immunotherapy Resistance", "comment": null, "summary": "Immune checkpoint inhibitors (ICIs) have transformed cancer treatment, yet\npatient responses remain highly variable, and the biological mechanisms\nunderlying resistance are poorly understood. While machine learning models hold\npromise for predicting responses to ICIs, most existing methods lack\ninterpretability and do not effectively leverage the biological structure\ninherent to multi-omics data. Here, we introduce the Biologically Disentangled\nVariational Autoencoder (BDVAE), a deep generative model that integrates\ntranscriptomic and genomic data through modality- and pathway-specific\nencoders. Unlike existing rigid, pathway-informed models, BDVAE employs a\nmodular encoder architecture combined with variational inference to learn\nbiologically meaningful latent features associated with immune, genomic, and\nmetabolic processes. Applied to a pan-cancer cohort of 366 patients across four\ncancer types treated with ICIs, BDVAE accurately predicts treatment response\n(AUC-ROC = 0.94 on unseen test data) and uncovers critical resistance\nmechanisms, including immune suppression, metabolic shifts, and neuronal\nsignaling. Importantly, BDVAE reveals that resistance spans a continuous\nbiological spectrum rather than strictly binary states, reflecting gradations\nof tumor dysfunction. Several latent features correlate with survival outcomes\nand known clinical subtypes, demonstrating BDVAE's capability to generate\ninterpretable, clinically relevant insights. These findings underscore the\nvalue of biologically structured machine learning in elucidating complex\nresistance patterns and guiding precision immunotherapy strategies.", "AI": {"tldr": "This paper introduces BDVAE, a deep learning model that predicts treatment response to ICIs and uncovers resistance mechanisms by integrating multi-omics data. BDVAE achieves high accuracy and reveals a continuous spectrum of resistance.", "motivation": "While machine learning models hold promise for predicting responses to ICIs, most existing methods lack interpretability and do not effectively leverage the biological structure inherent to multi-omics data.", "method": "We introduce the Biologically Disentangled Variational Autoencoder (BDVAE), a deep generative model that integrates transcriptomic and genomic data through modality- and pathway-specific encoders.", "result": "Applied to a pan-cancer cohort of 366 patients across four cancer types treated with ICIs, BDVAE accurately predicts treatment response (AUC-ROC = 0.94 on unseen test data) and uncovers critical resistance mechanisms, including immune suppression, metabolic shifts, and neuronal signaling.", "conclusion": "BDVAE reveals that resistance spans a continuous biological spectrum rather than strictly binary states, reflecting gradations of tumor dysfunction. Several latent features correlate with survival outcomes and known clinical subtypes, demonstrating BDVAE's capability to generate interpretable, clinically relevant insights. These findings underscore the value of biologically structured machine learning in elucidating complex resistance patterns and guiding precision immunotherapy strategies."}}
{"id": "2508.18785", "categories": ["eess.SP", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18785", "abs": "https://arxiv.org/abs/2508.18785", "authors": ["Luqing Luo", "Wenjin Gui", "Yunfei Liu", "Ziyue Zhang", "Yunxi Zhang", "Fengxiang Wang", "Zonghao Guo", "Zizhi Ma", "Xinzhu Liu", "Hanxiang He", "Jinhai Li", "Xin Qiu", "Wupeng Xie", "Yangang Sun"], "title": "EMind: A Foundation Model for Multi-task Electromagnetic Signals Understanding", "comment": null, "summary": "Deep understanding of electromagnetic signals is fundamental to dynamic\nspectrum management, intelligent transportation, autonomous driving and\nunmanned vehicle perception. The field faces challenges because electromagnetic\nsignals differ greatly from text and images, showing high heterogeneity, strong\nbackground noise and complex joint time frequency structure, which prevents\nexisting general models from direct use. Electromagnetic communication and\nsensing tasks are diverse, current methods lack cross task generalization and\ntransfer efficiency, and the scarcity of large high quality datasets blocks the\ncreation of a truly general multitask learning framework. To overcome these\nissue, we introduce EMind, an electromagnetic signals foundation model that\nbridges large scale pretraining and the unique nature of this modality. We\nbuild the first unified and largest standardized electromagnetic signal dataset\ncovering multiple signal types and tasks. By exploiting the physical properties\nof electromagnetic signals, we devise a length adaptive multi-signal packing\nmethod and a hardware-aware training strategy that enable efficient use and\nrepresentation learning from heterogeneous multi-source signals. Experiments\nshow that EMind achieves strong performance and broad generalization across\nmany downstream tasks, moving decisively from task specific models to a unified\nframework for electromagnetic intelligence. The code is available at:\nhttps://github.com/GabrielleTse/EMind.", "AI": {"tldr": "EMind, an electromagnetic signals foundation model that bridges large scale pretraining and the unique nature of this modality. We build the first unified and largest standardized electromagnetic signal dataset covering multiple signal types and tasks.", "motivation": "Electromagnetic signals differ greatly from text and images, showing high heterogeneity, strong background noise and complex joint time frequency structure, which prevents existing general models from direct use. Electromagnetic communication and sensing tasks are diverse, current methods lack cross task generalization and transfer efficiency, and the scarcity of large high quality datasets blocks the creation of a truly general multitask learning framework.", "method": "a length adaptive multi-signal packing method and a hardware-aware training strategy", "result": "achieves strong performance and broad generalization across many downstream tasks", "conclusion": "EMind achieves strong performance and broad generalization across many downstream tasks, moving decisively from task specific models to a unified framework for electromagnetic intelligence."}}
{"id": "2508.18847", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18847", "abs": "https://arxiv.org/abs/2508.18847", "authors": ["Yibo Li", "Miao Xiong", "Jiaying Wu", "Bryan Hooi"], "title": "ConfTuner: Training Large Language Models to Express Their Confidence Verbally", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in high-stakes domains\nsuch as science, law, and healthcare, where accurate expressions of uncertainty\nare essential for reliability and trust. However, current LLMs are often\nobserved to generate incorrect answers with high confidence, a phenomenon known\nas \"overconfidence\". Recent efforts have focused on calibrating LLMs'\nverbalized confidence: i.e., their expressions of confidence in text form, such\nas \"I am 80% confident that...\". Existing approaches either rely on prompt\nengineering or fine-tuning with heuristically generated uncertainty estimates,\nboth of which have limited effectiveness and generalizability. Motivated by the\nnotion of proper scoring rules for calibration in classical machine learning\nmodels, we introduce ConfTuner, a simple and efficient fine-tuning method that\nintroduces minimal overhead and does not require ground-truth confidence scores\nor proxy confidence estimates. ConfTuner relies on a new loss function,\ntokenized Brier score, which we theoretically prove to be a proper scoring\nrule, intuitively meaning that it \"correctly incentivizes the model to report\nits true probability of being correct\". ConfTuner improves calibration across\ndiverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our\nresults further show that better-calibrated confidence enables downstream gains\nin self-correction and model cascade, advancing the development of trustworthy\nLLM systems. The code is available at\nhttps://github.com/liushiliushi/ConfTuner.", "AI": {"tldr": "ConfTuner is introduced to address LLM overconfidence by fine-tuning with tokenized Brier score, improving calibration and enabling downstream gains.", "motivation": "Current LLMs are often observed to generate incorrect answers with high confidence, a phenomenon known as \"overconfidence\". Existing approaches have limited effectiveness and generalizability.", "method": "ConfTuner, a fine-tuning method that introduces minimal overhead and does not require ground-truth confidence scores or proxy confidence estimates. It relies on a new loss function, tokenized Brier score.", "result": "ConfTuner improves calibration across diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Better-calibrated confidence enables downstream gains in self-correction and model cascade.", "conclusion": "ConfTuner improves calibration across diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Better-calibrated confidence enables downstream gains in self-correction and model cascade."}}
{"id": "2508.19005", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19005", "abs": "https://arxiv.org/abs/2508.19005", "authors": ["Yuxuan Cai", "Yipeng Hao", "Jie Zhou", "Hang Yan", "Zhikai Lei", "Rui Zhen", "Zhenhua Han", "Yutao Yang", "Junsong Li", "Qianjun Pan", "Tianyu Huai", "Qin Chen", "Xin Li", "Kai Chen", "Bo Zhang", "Xipeng Qiu", "Liang He"], "title": "Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark", "comment": null, "summary": "As AI advances toward general intelligence, the focus is shifting from\nsystems optimized for static tasks to creating open-ended agents that learn\ncontinuously. In this paper, we introduce Experience-driven Lifelong Learning\n(ELL), a framework for building self-evolving agents capable of continuous\ngrowth through real-world interaction. The framework is built on four core\nprinciples: (1) Experience Exploration: Agents learn through continuous,\nself-motivated interaction with dynamic environments, navigating interdependent\ntasks and generating rich experiential trajectories. (2) Long-term Memory:\nAgents preserve and structure historical knowledge, including personal\nexperiences, domain expertise, and commonsense reasoning, into a persistent\nmemory system. (3) Skill Learning: Agents autonomously improve by abstracting\nrecurring patterns from experience into reusable skills, which are actively\nrefined and validated for application in new tasks. (4) Knowledge\nInternalization: Agents internalize explicit and discrete experiences into\nimplicit and intuitive capabilities as \"second nature\".\n  We also introduce StuLife, a benchmark dataset for ELL that simulates a\nstudent's holistic college journey, from enrollment to academic and personal\ndevelopment, across three core phases and ten detailed sub-scenarios. StuLife\nis designed around three key paradigm shifts: From Passive to Proactive, From\nContext to Memory, and From Imitation to Learning. In this dynamic environment,\nagents must acquire and distill practical skills and maintain persistent memory\nto make decisions based on evolving state variables. StuLife provides a\ncomprehensive platform for evaluating lifelong learning capabilities, including\nmemory retention, skill transfer, and self-motivated behavior. Beyond\nevaluating SOTA LLMs on the StuLife benchmark, we also explore the role of\ncontext engineering in advancing AGI.", "AI": {"tldr": "This paper introduces Experience-driven Lifelong Learning (ELL), a framework for building self-evolving agents, and StuLife, a benchmark dataset for ELL that simulates a student's holistic college journey.", "motivation": "The focus is shifting from systems optimized for static tasks to creating open-ended agents that learn continuously.", "method": "The paper introduces Experience-driven Lifelong Learning (ELL), a framework built on four core principles: Experience Exploration, Long-term Memory, Skill Learning, and Knowledge Internalization.", "result": "The paper evaluates SOTA LLMs on the StuLife benchmark.", "conclusion": "The paper introduces StuLife, a benchmark dataset for ELL that simulates a student's holistic college journey. The paper evaluates SOTA LLMs on the StuLife benchmark and explores the role of context engineering in advancing AGI."}}
{"id": "2508.18653", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS", "62P05, 68T0", "I.2.7; J.4"], "pdf": "https://arxiv.org/pdf/2508.18653", "abs": "https://arxiv.org/abs/2508.18653", "authors": ["Xiaoliang Chen", "Xin Yu", "Le Chang", "Teng Jing", "Jiashuai He", "Ze Wang", "Yangjun Luo", "Xingyu Chen", "Jiayue Liang", "Yuchen Wang", "Jiaying Xie"], "title": "The Sound of Risk: A Multimodal Physics-Informed Acoustic Model for Forecasting Market Volatility and Enhancing Market Interpretability", "comment": "9 pages, 6 figures", "summary": "Information asymmetry in financial markets, often amplified by strategically\ncrafted corporate narratives, undermines the effectiveness of conventional\ntextual analysis. We propose a novel multimodal framework for financial risk\nassessment that integrates textual sentiment with paralinguistic cues derived\nfrom executive vocal tract dynamics in earnings calls. Central to this\nframework is the Physics-Informed Acoustic Model (PIAM), which applies\nnonlinear acoustics to robustly extract emotional signatures from raw\nteleconference sound subject to distortions such as signal clipping. Both\nacoustic and textual emotional states are projected onto an interpretable\nthree-dimensional Affective State Label (ASL) space-Tension, Stability, and\nArousal. Using a dataset of 1,795 earnings calls (approximately 1,800 hours),\nwe construct features capturing dynamic shifts in executive affect between\nscripted presentation and spontaneous Q&A exchanges. Our key finding reveals a\npronounced divergence in predictive capacity: while multimodal features do not\nforecast directional stock returns, they explain up to 43.8% of the\nout-of-sample variance in 30-day realized volatility. Importantly, volatility\npredictions are strongly driven by emotional dynamics during executive\ntransitions from scripted to spontaneous speech, particularly reduced textual\nstability and heightened acoustic instability from CFOs, and significant\narousal variability from CEOs. An ablation study confirms that our multimodal\napproach substantially outperforms a financials-only baseline, underscoring the\ncomplementary contributions of acoustic and textual modalities. By decoding\nlatent markers of uncertainty from verifiable biometric signals, our\nmethodology provides investors and regulators a powerful tool for enhancing\nmarket interpretability and identifying hidden corporate uncertainty.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u8d22\u62a5\u7535\u8bdd\u4f1a\u8bae\u4e2d\u9ad8\u7ba1\u7684\u60c5\u7eea\u53d8\u5316\u6765\u9884\u6d4b\u5e02\u573a\u6ce2\u52a8\u7387\uff0c\u53d1\u73b0\u9ad8\u7ba1\u4ece\u811a\u672c\u5316\u6f14\u8bb2\u5230\u81ea\u53d1\u95ee\u7b54\u73af\u8282\u7684\u60c5\u7eea\u52a8\u6001\u5bf9\u6ce2\u52a8\u7387\u9884\u6d4b\u6709\u91cd\u8981\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u6587\u672c\u5206\u6790\u5728\u91d1\u878d\u5e02\u573a\u4e2d\u7531\u4e8e\u4fe1\u606f\u4e0d\u5bf9\u79f0\u800c\u53d7\u5230\u5f71\u54cd\uff0c\u800c\u7b56\u7565\u6027\u5730\u8bbe\u8ba1\u7684\u516c\u53f8\u53d9\u8ff0\u4f1a\u52a0\u5267\u8fd9\u79cd\u60c5\u51b5\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u91d1\u878d\u98ce\u9669\u8bc4\u4f30\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u6587\u672c\u60c5\u611f\u548c\u4ece\u8d22\u62a5\u7535\u8bdd\u4f1a\u8bae\u4e2d\u63d0\u53d6\u7684\u9ad8\u7ba1\u58f0\u9053\u52a8\u6001\u7684\u8d85\u8bed\u8a00\u7ebf\u7d22\u3002\u6838\u5fc3\u662f\u7269\u7406\u4fe1\u606f\u58f0\u5b66\u6a21\u578b\uff08PIAM\uff09\uff0c\u5b83\u5e94\u7528\u975e\u7ebf\u6027\u58f0\u5b66\u4ece\u539f\u59cb\u7535\u8bdd\u4f1a\u8bae\u58f0\u97f3\u4e2d\u7a33\u5065\u5730\u63d0\u53d6\u60c5\u611f\u7279\u5f81\u3002", "result": "\u591a\u6a21\u6001\u7279\u5f81\u4e0d\u80fd\u9884\u6d4b\u5b9a\u5411\u80a1\u7968\u6536\u76ca\uff0c\u4f46\u53ef\u4ee5\u89e3\u91ca\u9ad8\u8fbe43.8%\u768430\u5929\u5df2\u5b9e\u73b0\u6ce2\u52a8\u7387\u7684\u6837\u672c\u5916\u65b9\u5dee\u3002\u6ce2\u52a8\u7387\u9884\u6d4b\u4e3b\u8981\u53d7\u5230\u9ad8\u7ba1\u4ece\u811a\u672c\u5316\u6f14\u8bb2\u5230\u81ea\u53d1\u95ee\u7b54\u73af\u8282\u7684\u60c5\u7eea\u52a8\u6001\u7684\u5f71\u54cd\u3002\u591a\u6a21\u6001\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u4ec5\u4f7f\u7528\u8d22\u52a1\u6570\u636e\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u591a\u6a21\u6001\u7279\u5f81\u53ef\u4ee5\u89e3\u91ca\u9ad8\u8fbe43.8%\u768430\u5929\u5df2\u5b9e\u73b0\u6ce2\u52a8\u7387\u7684\u6837\u672c\u5916\u65b9\u5dee\uff0c\u5e76\u4e14\u6ce2\u52a8\u7387\u9884\u6d4b\u4e3b\u8981\u53d7\u5230\u9ad8\u7ba1\u4ece\u811a\u672c\u5316\u6f14\u8bb2\u5230\u81ea\u53d1\u95ee\u7b54\u73af\u8282\u7684\u60c5\u7eea\u52a8\u6001\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u4f53\u73b0\u5728CFO\u6587\u672c\u7a33\u5b9a\u6027\u7684\u964d\u4f4e\u548c\u58f0\u5b66\u4e0d\u7a33\u5b9a\u6027\u7684\u589e\u5f3a\uff0c\u4ee5\u53caCEO\u663e\u8457\u7684\u5524\u9192\u53d8\u5f02\u6027\u3002"}}
{"id": "2508.18787", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18787", "abs": "https://arxiv.org/abs/2508.18787", "authors": ["Constantino \u00c1lvarez Casado", "Sasan Sharifipour", "Manuel Lage Ca\u00f1ellas", "Nhi Nguyen", "Le Nguyen", "Miguel Bordallo L\u00f3pez"], "title": "Design, Implementation and Evaluation of a Real-Time Remote Photoplethysmography (rPPG) Acquisition System for Non-Invasive Vital Sign Monitoring", "comment": "23 pages, 2 figures, 10 formulas, 3 tables", "summary": "The growing integration of smart environments and low-power computing\ndevices, coupled with mass-market sensor technologies, is driving advancements\nin remote and non-contact physiological monitoring. However, deploying these\nsystems in real-time on resource-constrained platforms introduces significant\nchallenges related to scalability, interoperability, and performance. This\npaper presents a real-time remote photoplethysmography (rPPG) system optimized\nfor low-power devices, designed to extract physiological signals, such as heart\nrate (HR), respiratory rate (RR), and oxygen saturation (SpO2), from facial\nvideo streams. The system is built on the Face2PPG pipeline, which processes\nvideo frames sequentially for rPPG signal extraction and analysis, while\nleveraging a multithreaded architecture to manage video capture, real-time\nprocessing, network communication, and graphical user interface (GUI) updates\nconcurrently. This design ensures continuous, reliable operation at 30 frames\nper second (fps), with adaptive feedback through a collaborative user interface\nto guide optimal signal capture conditions. The network interface includes both\nan HTTP server for continuous video streaming and a RESTful API for on-demand\nvital sign retrieval. To ensure accurate performance despite the limitations of\nlow-power devices, we use a hybrid programming model combining Functional\nReactive Programming (FRP) and the Actor Model, allowing event-driven\nprocessing and efficient task parallelization. The system is evaluated under\nreal-time constraints, demonstrating robustness while minimizing computational\noverhead. Our work addresses key challenges in real-time biosignal monitoring,\noffering practical solutions for optimizing performance in modern healthcare\nand human-computer interaction applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u4f4e\u529f\u8017\u8bbe\u5907\u7684\u5b9e\u65f6\u8fdc\u7a0b\u5149\u7535\u5bb9\u79ef\u8109\u640f\u6ce2(rPPG)\u7cfb\u7edf\uff0c\u80fd\u591f\u6709\u6548\u63d0\u53d6\u751f\u7406\u4fe1\u53f7\uff0c\u5e76\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u5b9e\u65f6\u90e8\u7f72\u667a\u80fd\u73af\u5883\u548c\u4f4e\u529f\u8017\u8ba1\u7b97\u8bbe\u5907\u9762\u4e34\u7740\u4e0e\u53ef\u6269\u5c55\u6027\u3001\u4e92\u64cd\u4f5c\u6027\u548c\u6027\u80fd\u76f8\u5173\u7684\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u8fdc\u7a0b\u548c\u975e\u63a5\u89e6\u5f0f\u751f\u7406\u76d1\u6d4b\u9886\u57df\u3002", "method": "\u8be5\u7cfb\u7edf\u57fa\u4e8eFace2PPG\u6d41\u7a0b\uff0c\u91c7\u7528\u591a\u7ebf\u7a0b\u67b6\u6784\u7ba1\u7406\u89c6\u9891\u6355\u83b7\u3001\u5b9e\u65f6\u5904\u7406\u3001\u7f51\u7edc\u901a\u4fe1\u548cGUI\u66f4\u65b0\uff0c\u5e76\u7ed3\u5408\u51fd\u6570\u5f0f\u53cd\u5e94\u5f0f\u7f16\u7a0b(FRP)\u548cActor\u6a21\u578b\u5b9e\u73b0\u4e8b\u4ef6\u9a71\u52a8\u5904\u7406\u548c\u9ad8\u6548\u4efb\u52a1\u5e76\u884c\u5316\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u572830fps\u4e0b\u8fde\u7eed\u53ef\u9760\u8fd0\u884c\uff0c\u901a\u8fc7\u534f\u4f5c\u7528\u6237\u754c\u9762\u63d0\u4f9b\u81ea\u9002\u5e94\u53cd\u9988\u4ee5\u6307\u5bfc\u6700\u4f73\u4fe1\u53f7\u6355\u83b7\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7HTTP\u670d\u52a1\u5668\u548cRESTful API\u63d0\u4f9b\u89c6\u9891\u6d41\u548c\u751f\u547d\u4f53\u5f81\u68c0\u7d22\u529f\u80fd\u3002\u5728\u5b9e\u65f6\u7ea6\u675f\u4e0b\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u5176\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u7684\u5b9e\u65f6\u8fdc\u7a0b\u5149\u7535\u5bb9\u79ef\u8109\u640f\u6ce2(rPPG)\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u4f4e\u529f\u8017\u8bbe\u5907\u4e0a\u63d0\u53d6\u751f\u7406\u4fe1\u53f7\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u6700\u5c0f\u5316\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u4fdd\u6301\u9c81\u68d2\u6027\u7684\u80fd\u529b\uff0c\u4e3a\u73b0\u4ee3\u533b\u7597\u4fdd\u5065\u548c\u4eba\u673a\u4ea4\u4e92\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18870", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18870", "abs": "https://arxiv.org/abs/2508.18870", "authors": ["Viktor N. Zhuravlev", "Artur R. Khairullin", "Ernest A. Dyagin", "Alena N. Sitkina", "Nikita I. Kulin"], "title": "ReflectivePrompt: Reflective evolution in autoprompting algorithms", "comment": null, "summary": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which has been gaining popularity with the rapid advancement\nof prompt engineering, driven by extensive research in the field of large\nlanguage models (LLMs). This paper presents ReflectivePrompt - a novel\nautoprompting method based on evolutionary algorithms that employs a reflective\nevolution approach for more precise and comprehensive search of optimal\nprompts. ReflectivePrompt utilizes short-term and long-term reflection\noperations before crossover and elitist mutation to enhance the quality of the\nmodifications they introduce. This method allows for the accumulation of\nknowledge obtained throughout the evolution process and updates it at each\nepoch based on the current population. ReflectivePrompt was tested on 33\ndatasets for classification and text generation tasks using open-access large\nlanguage models: t-lite-instruct-0.1 and gemma3-27b-it. The method\ndemonstrates, on average, a significant improvement (e.g., 28% on BBH compared\nto EvoPrompt) in metrics relative to current state-of-the-art approaches,\nthereby establishing itself as one of the most effective solutions in\nevolutionary algorithm-based autoprompting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u63d0\u793a\u65b9\u6cd5ReflectivePrompt\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u8fdb\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u53cd\u601d\u8fdb\u5316\u65b9\u6cd5\u66f4\u7cbe\u786e\u5730\u641c\u7d22\u6700\u4f73\u63d0\u793a\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u7684\u6539\u8fdb\u3002", "motivation": "\u81ea\u52a8\u63d0\u793a\u662f\u81ea\u52a8\u9009\u62e9\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u5316\u63d0\u793a\u7684\u8fc7\u7a0b\uff0c\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9886\u57df\u7684\u5927\u91cf\u7814\u7a76\u63a8\u52a8\uff0c\u63d0\u793a\u5de5\u7a0b\u7684\u5feb\u901f\u53d1\u5c55\u4f7f\u5176\u8d8a\u6765\u8d8a\u53d7\u6b22\u8fce\u3002", "method": "\u4e00\u79cd\u57fa\u4e8e\u8fdb\u5316\u7b97\u6cd5\u7684\u65b0\u7684\u81ea\u52a8\u63d0\u793a\u65b9\u6cd5\uff0c\u91c7\u7528\u53cd\u601d\u8fdb\u5316\u65b9\u6cd5\u6765\u66f4\u7cbe\u786e\u548c\u5168\u9762\u5730\u641c\u7d22\u6700\u4f73\u63d0\u793a\u3002", "result": "\u5728\u5f00\u653e\u8bbf\u95ee\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578bt-lite-instruct-0.1\u548cgemma3-27b-it\u4e0a\uff0c\u572833\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5206\u7c7b\u548c\u6587\u672c\u751f\u6210\u4efb\u52a1\u7684\u6d4b\u8bd5\u3002\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u6307\u6807\u4e0a\u5e73\u5747\u6709\u663e\u7740\u6539\u5584\uff08\u4f8b\u5982\uff0c\u5728BBH\u4e0a\u6bd4EvoPrompt\u9ad828%\uff09\u3002", "conclusion": "ReflectivePrompt\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u6210\u4e3a\u57fa\u4e8e\u8fdb\u5316\u7b97\u6cd5\u7684\u6700\u6709\u6548\u7684\u81ea\u52a8\u63d0\u793a\u89e3\u51b3\u65b9\u6848\u4e4b\u4e00\u3002"}}
{"id": "2508.19008", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19008", "abs": "https://arxiv.org/abs/2508.19008", "authors": ["Marcin Moskalewicz", "Anna Sterna", "Marek Pokropski", "Paula Flores"], "title": "Sense of Self and Time in Borderline Personality. A Comparative Robustness Study with Generative AI", "comment": "22 pages, 4 tables, submitted to \"Personality and Individual\n  Differences\"", "summary": "This study examines the capacity of large language models (LLMs) to support\nphenomenological qualitative analysis of first-person experience in Borderline\nPersonality Disorder (BPD), understood as a disorder of temporality and\nselfhood. Building on a prior human-led thematic analysis of 24 inpatients'\nlife-story interviews, we compared three LLMs (OpenAI GPT-4o, Google Gemini 2.5\nPro, Anthropic Claude Opus 4) prompted to mimic the interpretative style of the\noriginal investigators. The models were evaluated with blinded and non-blinded\nexpert judges in phenomenology and clinical psychology. Assessments included\nsemantic congruence, Jaccard coefficients, and multidimensional validity\nratings (credibility, coherence, substantiveness, and groundness in data).\nResults showed variable overlap with the human analysis, from 0 percent in GPT\nto 42 percent in Claude and 58 percent in Gemini, and a low Jaccard coefficient\n(0.21-0.28). However, the models recovered themes omitted by humans. Gemini's\noutput most closely resembled the human analysis, with validity scores\nsignificantly higher than GPT and Claude (p < 0.0001), and was judged as human\nby blinded experts. All scores strongly correlated (R > 0.78) with the quantity\nof text and words per theme, highlighting both the variability and potential of\nAI-augmented thematic analysis to mitigate human interpretative bias.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fb9\u7f18\u578b\u4eba\u683c\u969c\u788d\u73b0\u8c61\u5b66\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0Gemini\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u53ef\u4ee5\u8f85\u52a9\u4eba\u5de5\u5206\u6790\u5e76\u51cf\u8f7b\u504f\u5dee\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u652f\u6301\u5bf9\u8fb9\u7f18\u578b\u4eba\u683c\u969c\u788d\uff08BPD\uff09\u60a3\u8005\u7684\u7b2c\u4e00\u4eba\u79f0\u4f53\u9a8c\u8fdb\u884c\u73b0\u8c61\u5b66\u5b9a\u6027\u5206\u6790\u7684\u80fd\u529b\uff0cBPD\u88ab\u7406\u89e3\u4e3a\u4e00\u79cd\u65f6\u95f4\u548c\u81ea\u6211\u969c\u788d\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08GPT-4o\u3001Gemini 2.5 Pro\u3001Claude Opus 4\uff09\u6a21\u4eff\u7814\u7a76\u4eba\u5458\u7684\u89e3\u91ca\u98ce\u683c\uff0c\u5e76\u4e0e\u5148\u524d\u5bf924\u540d\u4f4f\u9662\u60a3\u8005\u7684\u751f\u6d3b\u6545\u4e8b\u8bbf\u8c08\u8fdb\u884c\u7684\u4eba\u5de5\u4e3b\u9898\u5206\u6790\u8fdb\u884c\u6bd4\u8f83\u3002\u901a\u8fc7\u8bed\u4e49\u4e00\u81f4\u6027\u3001Jaccard\u7cfb\u6570\u548c\u591a\u7ef4\u5ea6\u6709\u6548\u6027\u8bc4\u7ea7\uff08\u53ef\u4fe1\u5ea6\u3001\u8fde\u8d2f\u6027\u3001\u5b9e\u8d28\u6027\u548c\u6570\u636e\u57fa\u7840\uff09\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u4e0e\u4eba\u5de5\u5206\u6790\u7684\u91cd\u53e0\u7a0b\u5ea6\u4e0d\u4e00\uff0cGemini\u7684\u8f93\u51fa\u4e0e\u4eba\u5de5\u5206\u6790\u6700\u76f8\u4f3c\uff0c\u6709\u6548\u6027\u5f97\u5206\u663e\u7740\u9ad8\u4e8eGPT\u548cClaude\uff0c\u5e76\u4e14\u88ab\u8499\u773c\u7684\u4e13\u5bb6\u5224\u65ad\u4e3a\u4eba\u5de5\u5206\u6790\u3002\u6240\u6709\u5206\u6570\u4e0e\u6bcf\u4e2a\u4e3b\u9898\u7684\u6587\u672c\u91cf\u548c\u5b57\u6570\u9ad8\u5ea6\u76f8\u5173\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u53ef\u4ee5\u8f85\u52a9\u5bf9\u8fb9\u7f18\u578b\u4eba\u683c\u969c\u788d\u60a3\u8005\u7684\u7b2c\u4e00\u4eba\u79f0\u4f53\u9a8c\u8fdb\u884c\u73b0\u8c61\u5b66\u5b9a\u6027\u5206\u6790\uff0c\u5e76\u4e14\u53ef\u4ee5\u51cf\u8f7b\u4eba\u7c7b\u7684\u89e3\u91ca\u504f\u5dee\u3002"}}
{"id": "2508.18663", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18663", "abs": "https://arxiv.org/abs/2508.18663", "authors": ["Gang Hu", "Yinglei Teng", "Pengfei Wu", "Nan Wang"], "title": "FFT-MoE: Efficient Federated Fine-Tuning for Foundation Models via Large-scale Sparse MoE under Heterogeneous Edge", "comment": "9 pages, 6 figures", "summary": "As FMs drive progress toward Artificial General Intelligence (AGI),\nfine-tuning them under privacy and resource constraints has become increasingly\ncritical particularly when highquality training data resides on distributed\nedge devices. Federated Learning (FL) offers a compelling solution through\nFederated Fine-Tuning (FFT), which enables collaborative model adaptation\nwithout sharing raw data. Recent approaches incorporate Parameter-Efficient\nFine-Tuning (PEFT) techniques such as Low Rank Adaptation (LoRA) to reduce\ncomputational overhead. However, LoRA-based FFT faces two major limitations in\nheterogeneous FL environments: structural incompatibility across clients with\nvarying LoRA configurations and limited adaptability to non-IID data\ndistributions, which hinders convergence and generalization. To address these\nchallenges, we propose FFT MoE, a novel FFT framework that replaces LoRA with\nsparse Mixture of Experts (MoE) adapters. Each client trains a lightweight\ngating network to selectively activate a personalized subset of experts,\nenabling fine-grained adaptation to local resource budgets while preserving\naggregation compatibility. To further combat the expert load imbalance caused\nby device and data heterogeneity, we introduce a heterogeneity-aware auxiliary\nloss that dynamically regularizes the routing distribution to ensure expert\ndiversity and balanced utilization. Extensive experiments spanning both IID and\nnon-IID conditions demonstrate that FFT MoE consistently outperforms state of\nthe art FFT baselines in generalization performance and training efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86FFT MoE\uff0c\u7528\u7a00\u758f\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u9002\u914d\u5668\u66ff\u6362LoRA\uff0c\u5e76\u5f15\u5165\u4e86\u5f02\u6784\u611f\u77e5\u8f85\u52a9\u635f\u5931\uff0c\u8be5\u635f\u5931\u52a8\u6001\u5730\u8c03\u8282\u8def\u7531\u5206\u5e03\uff0c\u4ee5\u786e\u4fdd\u4e13\u5bb6\u591a\u6837\u6027\u548c\u5e73\u8861\u5229\u7528\u3002", "motivation": "\u5728\u9690\u79c1\u548c\u8d44\u6e90\u7ea6\u675f\u4e0b\u5fae\u8c03FM\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u4f4d\u4e8e\u5206\u5e03\u5f0f\u8fb9\u7f18\u8bbe\u5907\u4e0a\u65f6\u3002\u5f02\u6784FL\u73af\u5883\u4e2d\u7684LoRA-based FFT\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u5177\u6709\u4e0d\u540cLoRA\u914d\u7f6e\u7684\u5ba2\u6237\u7aef\u4e4b\u95f4\u7684\u7ed3\u6784\u4e0d\u517c\u5bb9\u4ee5\u53ca\u5bf9\u975eIID\u6570\u636e\u5206\u5e03\u7684\u9002\u5e94\u6027\u6709\u9650\uff0c\u8fd9\u963b\u788d\u4e86\u6536\u655b\u548c\u6cdb\u5316\u3002", "method": "\u7528\u7a00\u758f\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u9002\u914d\u5668\u66ff\u6362LoRA\u3002", "result": "FFT MoE\u5728\u6cdb\u5316\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684FFT\u57fa\u7ebf\u3002", "conclusion": "FFT MoE\u5728\u6cdb\u5316\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684FFT\u57fa\u7ebf\u3002"}}
{"id": "2508.18788", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.18788", "abs": "https://arxiv.org/abs/2508.18788", "authors": ["Christian L\u00f6wens", "Thorben Funke", "Jingchao Xie", "Alexandru Paul Condurache"], "title": "PseudoMapTrainer: Learning Online Mapping without HD Maps", "comment": "Accepted at ICCV 2025", "summary": "Online mapping models show remarkable results in predicting vectorized maps\nfrom multi-view camera images only. However, all existing approaches still rely\non ground-truth high-definition maps during training, which are expensive to\nobtain and often not geographically diverse enough for reliable generalization.\nIn this work, we propose PseudoMapTrainer, a novel approach to online mapping\nthat uses pseudo-labels generated from unlabeled sensor data. We derive those\npseudo-labels by reconstructing the road surface from multi-camera imagery\nusing Gaussian splatting and semantics of a pre-trained 2D segmentation\nnetwork. In addition, we introduce a mask-aware assignment algorithm and loss\nfunction to handle partially masked pseudo-labels, allowing for the first time\nthe training of online mapping models without any ground-truth maps.\nFurthermore, our pseudo-labels can be effectively used to pre-train an online\nmodel in a semi-supervised manner to leverage large-scale unlabeled\ncrowdsourced data. The code is available at\ngithub.com/boschresearch/PseudoMapTrainer.", "AI": {"tldr": "This paper introduces a method to train online mapping models without ground-truth maps using pseudo-labels generated from unlabeled sensor data.", "motivation": "Existing online mapping approaches rely on ground-truth high-definition maps during training, which are expensive to obtain and not geographically diverse enough for reliable generalization.", "method": "The paper proposes PseudoMapTrainer, which uses pseudo-labels generated from unlabeled sensor data by reconstructing the road surface from multi-camera imagery using Gaussian splatting and semantics of a pre-trained 2D segmentation network. It also introduces a mask-aware assignment algorithm and loss function to handle partially masked pseudo-labels.", "result": "The pseudo-labels can be effectively used to pre-train an online model in a semi-supervised manner to leverage large-scale unlabeled crowdsourced data.", "conclusion": "This paper enables the training of online mapping models without any ground-truth maps by using pseudo-labels and a mask-aware assignment algorithm."}}
{"id": "2508.18872", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18872", "abs": "https://arxiv.org/abs/2508.18872", "authors": ["Laurie Gale", "Sebastian Mateos Nicolajsen"], "title": "Empowering Computing Education Researchers Through LLM-Assisted Content Analysis", "comment": "7 pages, 2 figures", "summary": "Computing education research (CER) is often instigated by practitioners\nwanting to improve both their own and the wider discipline's teaching practice.\nHowever, the latter is often difficult as many researchers lack the colleagues,\nresources, or capacity to conduct research that is generalisable or rigorous\nenough to advance the discipline. As a result, research methods that enable\nsense-making with larger volumes of qualitative data, while not increasing the\nburden on the researcher, have significant potential within CER.\n  In this discussion paper, we propose such a method for conducting rigorous\nanalysis on large volumes of textual data, namely a variation of LLM-assisted\ncontent analysis (LACA). This method combines content analysis with the use of\nlarge language models, empowering researchers to conduct larger-scale research\nwhich they would otherwise not be able to perform. Using a computing education\ndataset, we illustrate how LACA could be applied in a reproducible and rigorous\nmanner. We believe this method has potential in CER, enabling more\ngeneralisable findings from a wider range of research. This, together with the\ndevelopment of similar methods, can help to advance both the practice and\nresearch quality of the CER discipline.", "AI": {"tldr": "propose a method for conducting rigorous analysis on large volumes of textual data, namely a variation of LLM-assisted content analysis (LACA).", "motivation": "many researchers lack the colleagues, resources, or capacity to conduct research that is generalisable or rigorous enough to advance the discipline", "method": "a variation of LLM-assisted content analysis (LACA). This method combines content analysis with the use of large language models", "result": "Using a computing education dataset, we illustrate how LACA could be applied in a reproducible and rigorous manner.", "conclusion": "This method has potential in CER, enabling more generalisable findings from a wider range of research. This, together with the development of similar methods, can help to advance both the practice and research quality of the CER discipline."}}
{"id": "2508.19014", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19014", "abs": "https://arxiv.org/abs/2508.19014", "authors": ["Surajit Das", "Gourav Roy", "Aleksei Eliseev", "Ram Kumar Rajendran"], "title": "MAB Optimizer for Estimating Math Question Difficulty via Inverse CV without NLP", "comment": null, "summary": "The evolution of technology and education is driving the emergence of\nIntelligent & Autonomous Tutoring Systems (IATS), where objective and\ndomain-agnostic methods for determining question difficulty are essential.\nTraditional human labeling is subjective, and existing NLP-based approaches\nfail in symbolic domains like algebra. This study introduces the Approach of\nPassive Measures among Educands (APME), a reinforcement learning-based\nMulti-Armed Bandit (MAB) framework that estimates difficulty solely from solver\nperformance data -- marks obtained and time taken -- without requiring\nlinguistic features or expert labels. By leveraging the inverse coefficient of\nvariation as a risk-adjusted metric, the model provides an explainable and\nscalable mechanism for adaptive assessment. Empirical validation was conducted\non three heterogeneous datasets. Across these diverse contexts, the model\nachieved an average R2 of 0.9213 and an average RMSE of 0.0584, confirming its\nrobustness, accuracy, and adaptability to different educational levels and\nassessment formats. Compared with baseline approaches-such as regression-based,\nNLP-driven, and IRT models-the proposed framework consistently outperformed\nalternatives, particularly in purely symbolic domains. The findings highlight\nthat (i) item heterogeneity strongly influences perceived difficulty, and (ii)\nvariance in solver outcomes is as critical as mean performance for adaptive\nallocation. Pedagogically, the model aligns with Vygotskys Zone of Proximal\nDevelopment by identifying tasks that balance challenge and attainability,\nsupporting motivation while minimizing disengagement. This domain-agnostic,\nself-supervised approach advances difficulty tagging in IATS and can be\nextended beyond algebra wherever solver interaction data is available", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u3001\u9886\u57df\u65e0\u5173\u7684\u3001\u81ea\u76d1\u7763\u7684\u96be\u5ea6\u8bc4\u4f30\u65b9\u6cd5APME\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\uff0c\u4ec5\u57fa\u4e8e\u6c42\u89e3\u5668\u6027\u80fd\u6570\u636e\u6765\u4f30\u8ba1\u95ee\u9898\u96be\u5ea6\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6280\u672f\u548c\u6559\u80b2\u7684\u53d1\u5c55\u63a8\u52a8\u4e86\u667a\u80fd\u548c\u81ea\u4e3b\u8f85\u5bfc\u7cfb\u7edf\uff08IATS\uff09\u7684\u51fa\u73b0\uff0c\u5176\u4e2d\u786e\u5b9a\u95ee\u9898\u96be\u5ea6\u7684\u5ba2\u89c2\u548c\u9886\u57df\u65e0\u5173\u7684\u65b9\u6cd5\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684\u4eba\u5de5\u6807\u6ce8\u662f\u4e3b\u89c2\u7684\uff0c\u73b0\u6709\u7684\u57fa\u4e8eNLP\u7684\u65b9\u6cd5\u5728\u4ee3\u6570\u7b49\u7b26\u53f7\u9886\u57df\u4e2d\u5931\u6548\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u81c2\u8001\u864e\u673a\uff08MAB\uff09\u6846\u67b6APME\uff0c\u8be5\u6846\u67b6\u4ec5\u4ece\u6c42\u89e3\u5668\u7684\u6027\u80fd\u6570\u636e\uff08\u83b7\u5f97\u7684\u5206\u6570\u548c\u82b1\u8d39\u7684\u65f6\u95f4\uff09\u6765\u4f30\u8ba1\u96be\u5ea6\uff0c\u800c\u4e0d\u9700\u8981\u8bed\u8a00\u7279\u5f81\u6216\u4e13\u5bb6\u6807\u7b7e\u3002\u5229\u7528\u53d8\u5f02\u7cfb\u6570\u7684\u5012\u6570\u4f5c\u4e3a\u98ce\u9669\u8c03\u6574\u6307\u6807\u3002", "result": "\u5728\u4e09\u4e2a\u5f02\u6784\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u5e73\u5747R2\u4e3a0.9213\uff0c\u5e73\u5747RMSE\u4e3a0.0584\uff0c\u8bc1\u5b9e\u4e86\u5176\u9c81\u68d2\u6027\u3001\u51c6\u786e\u6027\u548c\u5bf9\u4e0d\u540c\u6559\u80b2\u6c34\u5e73\u548c\u8bc4\u4f30\u5f62\u5f0f\u7684\u9002\u5e94\u6027\u3002\u4e0e\u57fa\u4e8e\u56de\u5f52\u3001NLP\u9a71\u52a8\u548cIRT\u6a21\u578b\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u7eaf\u7b26\u53f7\u9886\u57df\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e0e\u7ef4\u679c\u8328\u57fa\u7684\u6700\u8fd1\u53d1\u5c55\u533a\u7406\u8bba\u76f8\u7b26\uff0c\u901a\u8fc7\u8bc6\u522b\u5e73\u8861\u6311\u6218\u6027\u548c\u53ef\u5b9e\u73b0\u6027\u7684\u4efb\u52a1\u6765\u652f\u6301\u5b66\u751f\u7684\u5b66\u4e60\u52a8\u673a\uff0c\u540c\u65f6\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u8131\u79bb\u3002\u8fd9\u79cd\u9886\u57df\u65e0\u5173\u3001\u81ea\u76d1\u7763\u7684\u65b9\u6cd5\u63a8\u8fdb\u4e86IATS\u4e2d\u7684\u96be\u5ea6\u6807\u7b7e\uff0c\u5e76\u4e14\u53ef\u4ee5\u6269\u5c55\u5230\u4efb\u4f55\u53ef\u4ee5\u83b7\u5f97\u6c42\u89e3\u5668\u4ea4\u4e92\u6570\u636e\u7684\u9886\u57df\u3002"}}
{"id": "2508.18671", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18671", "abs": "https://arxiv.org/abs/2508.18671", "authors": ["Yuechun Gu", "Jiajie He", "Keke Chen"], "title": "Auditing Approximate Machine Unlearning for Differentially Private Models", "comment": "Accepted by ICDM2025,10pages", "summary": "Approximate machine unlearning aims to remove the effect of specific data\nfrom trained models to ensure individuals' privacy. Existing methods focus on\nthe removed records and assume the retained ones are unaffected. However,\nrecent studies on the \\emph{privacy onion effect} indicate this assumption\nmight be incorrect. Especially when the model is differentially private, no\nstudy has explored whether the retained ones still meet the differential\nprivacy (DP) criterion under existing machine unlearning methods. This paper\ntakes a holistic approach to auditing both unlearned and retained samples'\nprivacy risks after applying approximate unlearning algorithms. We propose the\nprivacy criteria for unlearned and retained samples, respectively, based on the\nperspectives of DP and membership inference attacks (MIAs). To make the\nauditing process more practical, we also develop an efficient MIA, A-LiRA,\nutilizing data augmentation to reduce the cost of shadow model training. Our\nexperimental findings indicate that existing approximate machine unlearning\nalgorithms may inadvertently compromise the privacy of retained samples for\ndifferentially private models, and we need differentially private unlearning\nalgorithms. For reproducibility, we have pubished our code:\nhttps://anonymous.4open.science/r/Auditing-machine-unlearning-CB10/README.md", "AI": {"tldr": "\u672c\u6587\u5168\u9762\u5730\u5ba1\u8ba1\u4e86\u5e94\u7528\u8fd1\u4f3c\u5378\u8f7d\u7b97\u6cd5\u540e\uff0c\u672a\u5b66\u4e60\u548c\u4fdd\u7559\u6837\u672c\u7684\u9690\u79c1\u98ce\u9669\u3002", "motivation": "\u8fd1\u4f3c\u673a\u5668\u5378\u8f7d\u65e8\u5728\u6d88\u9664\u7279\u5b9a\u6570\u636e\u5bf9\u5df2\u8bad\u7ec3\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u4ee5\u786e\u4fdd\u4e2a\u4eba\u9690\u79c1\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u79fb\u9664\u7684\u8bb0\u5f55\uff0c\u5e76\u5047\u8bbe\u4fdd\u7559\u7684\u8bb0\u5f55\u4e0d\u53d7\u5f71\u54cd\u3002\u7136\u800c\uff0c\u6700\u8fd1\u5173\u4e8e\u9690\u79c1\u6d0b\u8471\u6548\u5e94\u7684\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u79cd\u5047\u8bbe\u53ef\u80fd\u662f\u4e0d\u6b63\u786e\u7684\u3002\u7279\u522b\u662f\u5f53\u6a21\u578b\u662f\u5dee\u5206\u9690\u79c1\u65f6\uff0c\u6ca1\u6709\u7814\u7a76\u63a2\u8ba8\u5728\u73b0\u6709\u7684\u673a\u5668\u5378\u8f7d\u65b9\u6cd5\u4e0b\uff0c\u4fdd\u7559\u7684\u8bb0\u5f55\u662f\u5426\u4ecd\u7136\u6ee1\u8db3\u5dee\u5206\u9690\u79c1 (DP) \u6807\u51c6\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u5206\u522b\u9488\u5bf9\u672a\u5b66\u4e60\u548c\u4fdd\u7559\u6837\u672c\u7684\u9690\u79c1\u6807\u51c6\uff0c\u57fa\u4e8e DP \u548c\u6210\u5458\u63a8\u7406\u653b\u51fb (MIA) \u7684\u89d2\u5ea6\u3002\u4e3a\u4e86\u4f7f\u5ba1\u8ba1\u8fc7\u7a0b\u66f4\u5b9e\u7528\uff0c\u6211\u4eec\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684 MIA\uff0c\u5373 A-LiRA\uff0c\u5229\u7528\u6570\u636e\u589e\u5f3a\u6765\u964d\u4f4e\u5f71\u5b50\u6a21\u578b\u8bad\u7ec3\u7684\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709\u7684\u8fd1\u4f3c\u673a\u5668\u5378\u8f7d\u7b97\u6cd5\u53ef\u80fd\u4f1a\u65e0\u610f\u4e2d\u635f\u5bb3\u5dee\u5206\u9690\u79c1\u6a21\u578b\u4e2d\u4fdd\u7559\u6837\u672c\u7684\u9690\u79c1\u3002", "conclusion": "\u73b0\u6709\u7684\u8fd1\u4f3c\u673a\u5668\u5378\u8f7d\u7b97\u6cd5\u53ef\u80fd\u4f1a\u65e0\u610f\u4e2d\u635f\u5bb3\u5dee\u5206\u9690\u79c1\u6a21\u578b\u4e2d\u4fdd\u7559\u6837\u672c\u7684\u9690\u79c1\uff0c\u6211\u4eec\u9700\u8981\u5dee\u5206\u9690\u79c1\u5378\u8f7d\u7b97\u6cd5\u3002"}}
{"id": "2508.18790", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18790", "abs": "https://arxiv.org/abs/2508.18790", "authors": ["Yuhui Tao", "Yizhe Zhang", "Qiang Chen"], "title": "A Closer Look at Edema Area Segmentation in SD-OCT Images Using Adversarial Framework", "comment": null, "summary": "The development of artificial intelligence models for macular edema (ME)\nanaly-sis always relies on expert-annotated pixel-level image datasets which\nare expen-sive to collect prospectively. While anomaly-detection-based\nweakly-supervised methods have shown promise in edema area (EA) segmentation\ntask, their per-formance still lags behind fully-supervised approaches. In this\npaper, we leverage the strong correlation between EA and retinal layers in\nspectral-domain optical coherence tomography (SD-OCT) images, along with the\nupdate characteristics of weakly-supervised learning, to enhance an\noff-the-shelf adversarial framework for EA segmentation with a novel\nlayer-structure-guided post-processing step and a test-time-adaptation (TTA)\nstrategy. By incorporating additional retinal lay-er information, our framework\nreframes the dense EA prediction task as one of confirming intersection points\nbetween the EA contour and retinal layers, result-ing in predictions that\nbetter align with the shape prior of EA. Besides, the TTA framework further\nhelps address discrepancies in the manifestations and presen-tations of EA\nbetween training and test sets. Extensive experiments on two pub-licly\navailable datasets demonstrate that these two proposed ingredients can im-prove\nthe accuracy and robustness of EA segmentation, bridging the gap between\nweakly-supervised and fully-supervised models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f31\u76d1\u7763\u9ec4\u6591\u6c34\u80bf\u5206\u5272\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u89c6\u7f51\u819c\u5c42\u4fe1\u606f\u548c\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u7b56\u7565\u6765\u63d0\u9ad8\u5206\u5272\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7f29\u5c0f\u4e86\u4e0e\u5168\u76d1\u7763\u65b9\u6cd5\u7684\u5dee\u8ddd\u3002", "motivation": "\u4eba\u5de5\u667a\u6a21\u578b\u5bf9\u9ec4\u6591\u6c34\u80bf (ME) \u7684\u5206\u6790\u4f9d\u8d56\u4e8e\u4e13\u5bb6\u6ce8\u91ca\u7684\u50cf\u7d20\u7ea7\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u7684\u524d\u77bb\u6027\u6536\u96c6\u6210\u672c\u5f88\u9ad8\u3002\u57fa\u4e8e\u5f02\u5e38\u68c0\u6d4b\u7684\u5f31\u76d1\u7763\u65b9\u6cd5\u5728\u6c34\u80bf\u533a\u57df (EA) \u5206\u5272\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u5e0c\u671b\uff0c\u4f46\u5b83\u4eec\u7684\u6027\u80fd\u4ecd\u7136\u843d\u540e\u4e8e\u5b8c\u5168\u76d1\u7763\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5c42\u7ed3\u6784\u5f15\u5bfc\u7684\u540e\u5904\u7406\u6b65\u9aa4\u548c\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94 (TTA) \u7b56\u7565\uff0c\u589e\u5f3a\u4e86\u73b0\u6210\u7684\u5bf9\u6297\u6846\u67b6\uff0c\u7528\u4e8e EA \u5206\u5272\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e24\u4e2a\u63d0\u51fa\u7684\u6210\u5206\u53ef\u4ee5\u63d0\u9ad8 EA \u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u63d0\u9ad8\u9ec4\u6591\u6c34\u80bf\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7f29\u5c0f\u5f31\u76d1\u7763\u6a21\u578b\u548c\u5168\u76d1\u7763\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2508.18916", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.18916", "abs": "https://arxiv.org/abs/2508.18916", "authors": ["Bojan Evkoski", "Igor Mozeti\u010d", "Nikola Ljube\u0161i\u0107", "Petra Kralj Novak"], "title": "Affective Polarization across European Parliaments", "comment": "6 pages, 4 figures", "summary": "Affective polarization, characterized by increased negativity and hostility\ntowards opposing groups, has become a prominent feature of political discourse\nworldwide. Our study examines the presence of this type of polarization in a\nselection of European parliaments in a fully automated manner. Utilizing a\ncomprehensive corpus of parliamentary speeches from the parliaments of six\nEuropean countries, we employ natural language processing techniques to\nestimate parliamentarian sentiment. By comparing the levels of negativity\nconveyed in references to individuals from opposing groups versus one's own, we\ndiscover patterns of affectively polarized interactions. The findings\ndemonstrate the existence of consistent affective polarization across all six\nEuropean parliaments. Although activity correlates with negativity, there is no\nobserved difference in affective polarization between less active and more\nactive members of parliament. Finally, we show that reciprocity is a\ncontributing mechanism in affective polarization between parliamentarians\nacross all six parliaments.", "AI": {"tldr": "Study finds affective polarization in European parliaments, with reciprocity as a contributing factor, but no difference between active and inactive members.", "motivation": "Examine the presence of affective polarization in European parliaments in a fully automated manner, as it has become a prominent feature of political discourse worldwide.", "method": "Natural language processing techniques to estimate parliamentarian sentiment by comparing negativity in references to opposing groups versus one's own.", "result": "Demonstrates consistent affective polarization across six European parliaments. No difference in affective polarization between less active and more active members. Reciprocity is a contributing mechanism.", "conclusion": "Consistent affective polarization exists across six European parliaments. Activity correlates with negativity, but no difference in affective polarization exists between less and more active members. Reciprocity contributes to affective polarization."}}
{"id": "2508.19035", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19035", "abs": "https://arxiv.org/abs/2508.19035", "authors": ["Congchi Yin", "Tianyi Wu", "Yankai Shu", "Alex Gu", "Yunhan Wang", "Jun Shao", "Xun Jiang", "Piji Li"], "title": "Investigating Advanced Reasoning of Large Language Models via Black-Box Interaction", "comment": null, "summary": "Existing tasks fall short in evaluating reasoning ability of Large Language\nModels (LLMs) in an interactive, unknown environment. This deficiency leads to\nthe isolated assessment of deductive, inductive, and abductive reasoning,\nneglecting the integrated reasoning process that is indispensable for humans\ndiscovery of real world. We introduce a novel evaluation paradigm,\n\\textit{black-box interaction}, to tackle this challenge. A black-box is\ndefined by a hidden function that maps a specific set of inputs to outputs.\nLLMs are required to unravel the hidden function behind the black-box by\ninteracting with it in given exploration turns, and reasoning over observed\ninput-output pairs. Leveraging this idea, we build the \\textsc{Oracle}\nbenchmark which comprises 6 types of black-box task and 96 black-boxes. 19\nmodern LLMs are benchmarked. o3 ranks first in 5 of the 6 tasks, achieving over\n70\\% accuracy on most easy black-boxes. But it still struggles with some hard\nblack-box tasks, where its average performance drops below 40\\%. Further\nanalysis indicates a universal difficulty among LLMs: They lack the high-level\nplanning capability to develop efficient and adaptive exploration strategies\nfor hypothesis refinement.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9ed1\u76d2\u4ea4\u4e92\u8bc4\u4f30\u8303\u5f0f\uff0c\u4ee5\u8bc4\u4f30 LLM \u5728\u4ea4\u4e92\u5f0f\u672a\u77e5\u73af\u5883\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4efb\u52a1\u4e0d\u8db3\u4ee5\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u4ea4\u4e92\u5f0f\u672a\u77e5\u73af\u5883\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002\u8fd9\u79cd\u7f3a\u9677\u5bfc\u81f4\u5bf9\u6f14\u7ece\u3001\u5f52\u7eb3\u548c\u6eaf\u56e0\u63a8\u7406\u7684\u5b64\u7acb\u8bc4\u4f30\uff0c\u5ffd\u7565\u4e86\u4eba\u7c7b\u53d1\u73b0\u73b0\u5b9e\u4e16\u754c\u4e0d\u53ef\u6216\u7f3a\u7684\u7efc\u5408\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u8303\u5f0f\uff0c\u5373\u9ed1\u76d2\u4ea4\u4e92\uff0c\u901a\u8fc7\u4e0e\u9ed1\u76d2\u4ea4\u4e92\u5e76\u63a8\u7406\u89c2\u5bdf\u5230\u7684\u8f93\u5165\u8f93\u51fa\u5bf9\u6765\u89e3\u5f00\u9690\u85cf\u51fd\u6570\u3002", "result": "o3 \u5728 6 \u9879\u4efb\u52a1\u4e2d\u7684 5 \u9879\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u5728\u5927\u591a\u6570\u7b80\u5355\u7684\u9ed1\u76d2\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc7 70% \u7684\u51c6\u786e\u7387\u3002\u4f46\u5728\u4e00\u4e9b\u56f0\u96be\u7684\u9ed1\u76d2\u4efb\u52a1\u4e2d\u4ecd\u7136struggle\uff0c\u5e73\u5747\u8868\u73b0\u4e0b\u964d\u5230 40% \u4ee5\u4e0b\u3002", "conclusion": "LLMs \u5728\u4e00\u4e9b\u56f0\u96be\u7684\u9ed1\u76d2\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u9ad8\u6548\u548c\u81ea\u9002\u5e94\u7684\u63a2\u7d22\u7b56\u7565\u6765\u6539\u8fdb\u5047\u8bbe\u3002"}}
{"id": "2508.18672", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.18672", "abs": "https://arxiv.org/abs/2508.18672", "authors": ["Taishi Nakamura", "Satoki Ishikawa", "Masaki Kawamura", "Takumi Okamoto", "Daisuke Nohara", "Jun Suzuki", "Rio Yokota"], "title": "Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks", "comment": "Presented at the Second AI for Math Workshop at ICML", "summary": "Empirical scaling laws have driven the evolution of large language models\n(LLMs), yet their coefficients shift whenever the model architecture or data\npipeline changes. Mixture-of-Experts (MoE) models, now standard in\nstate-of-the-art systems, introduce a new sparsity dimension that current\ndense-model frontiers overlook. We investigate how MoE sparsity influences two\ndistinct capability regimes: memorization and reasoning. We train families of\nMoE Transformers that systematically vary total parameters, active parameters,\nand top-$k$ routing while holding the compute budget fixed. For every model we\nrecord pre-training loss, downstream task loss, and task accuracy, allowing us\nto separate the train-test generalization gap from the loss-accuracy gap.\nMemorization benchmarks improve monotonically with total parameters, mirroring\ntraining loss. By contrast, reasoning performance saturates and can even\nregress despite continued gains in both total parameters and training loss.\nAltering top-$k$ alone has little effect when active parameters are constant,\nand classic hyperparameters such as learning rate and initialization modulate\nthe generalization gap in the same direction as sparsity. Neither post-training\nreinforcement learning (GRPO) nor extra test-time compute rescues the reasoning\ndeficit of overly sparse models. Our model checkpoints, code and logs are\nopen-source at https://github.com/rioyokotalab/optimal-sparsity.", "AI": {"tldr": "This paper investigates the influence of MoE sparsity on memorization and reasoning capabilities of large language models. The findings suggest that while memorization improves with total parameters, reasoning performance saturates and can even regress with increased sparsity, and cannot be rescued by post-training methods or extra test-time compute.", "motivation": "Empirical scaling laws have driven the evolution of large language models (LLMs), yet their coefficients shift whenever the model architecture or data pipeline changes. Mixture-of-Experts (MoE) models, now standard in state-of-the-art systems, introduce a new sparsity dimension that current dense-model frontiers overlook. We investigate how MoE sparsity influences two distinct capability regimes: memorization and reasoning.", "method": "We train families of MoE Transformers that systematically vary total parameters, active parameters, and top-$k$ routing while holding the compute budget fixed. For every model we record pre-training loss, downstream task loss, and task accuracy, allowing us to separate the train-test generalization gap from the loss-accuracy gap.", "result": "Memorization benchmarks improve monotonically with total parameters, mirroring training loss.", "conclusion": "Reasoning performance saturates and can even regress despite continued gains in both total parameters and training loss. Altering top-$k$ alone has little effect when active parameters are constant, and classic hyperparameters such as learning rate and initialization modulate the generalization gap in the same direction as sparsity. Neither post-training reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning deficit of overly sparse models."}}
{"id": "2508.18799", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18799", "abs": "https://arxiv.org/abs/2508.18799", "authors": ["Hassan Abid", "Khan Muhammad", "Muhammad Haris Khan"], "title": "Robust and Label-Efficient Deep Waste Detection", "comment": "Accepted to BMVC 2025", "summary": "Effective waste sorting is critical for sustainable recycling, yet AI\nresearch in this domain continues to lag behind commercial systems due to\nlimited datasets and reliance on legacy object detectors. In this work, we\nadvance AI-driven waste detection by establishing strong baselines and\nintroducing an ensemble-based semi-supervised learning framework. We first\nbenchmark state-of-the-art Open-Vocabulary Object Detection (OVOD) models on\nthe real-world ZeroWaste dataset, demonstrating that while class-only prompts\nperform poorly, LLM-optimized prompts significantly enhance zero-shot accuracy.\nNext, to address domain-specific limitations, we fine-tune modern\ntransformer-based detectors, achieving a new baseline of 51.6 mAP. We then\npropose a soft pseudo-labeling strategy that fuses ensemble predictions using\nspatial and consensus-aware weighting, enabling robust semi-supervised\ntraining. Applied to the unlabeled ZeroWaste-s subset, our pseudo-annotations\nachieve performance gains that surpass fully supervised training, underscoring\nthe effectiveness of scalable annotation pipelines. Our work contributes to the\nresearch community by establishing rigorous baselines, introducing a robust\nensemble-based pseudo-labeling pipeline, generating high-quality annotations\nfor the unlabeled ZeroWaste-s subset, and systematically evaluating OVOD models\nunder real-world waste sorting conditions. Our code is available at:\nhttps://github.com/h-abid97/robust-waste-detection.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5efa\u7acb\u57fa\u7ebf\u548c\u5f15\u5165\u96c6\u6210\u7684\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u6539\u8fdb\u4e86AI\u9a71\u52a8\u7684\u5783\u573e\u68c0\u6d4b\uff0c\u5e76\u5728ZeroWaste\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002", "motivation": "\u7531\u4e8e\u6570\u636e\u96c6\u6709\u9650\u4ee5\u53ca\u4f9d\u8d56\u4e8e\u4f20\u7edf\u7684\u5bf9\u8c61\u68c0\u6d4b\u5668\uff0c\u8be5\u9886\u57df\u7684\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u7ee7\u7eed\u843d\u540e\u4e8e\u5546\u4e1a\u7cfb\u7edf\u3002\u56e0\u6b64\uff0c\u6709\u6548\u7684\u5e9f\u7269\u5206\u7c7b\u5bf9\u4e8e\u53ef\u6301\u7eed\u56de\u6536\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u6210\u7684\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5e76\u91c7\u7528\u4e86\u4e00\u79cd\u8f6f\u4f2a\u6807\u7b7e\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u4f7f\u7528\u7a7a\u95f4\u548c\u5171\u8bc6\u611f\u77e5\u52a0\u6743\u878d\u5408\u96c6\u6210\u9884\u6d4b\uff0c\u4ece\u800c\u5b9e\u73b0\u7a33\u5065\u7684\u534a\u76d1\u7763\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u5bf9\u73b0\u4ee3\u57fa\u4e8etransformer\u7684\u68c0\u6d4b\u5668\u8fdb\u884c\u5fae\u8c03\uff0c\u5b9e\u73b0\u4e8651.6 mAP\u7684\u65b0\u57fa\u7ebf\u3002\u5e94\u7528\u4e8e\u672a\u6807\u8bb0\u7684ZeroWaste-s\u5b50\u96c6\uff0c\u8be5\u8bba\u6587\u7684\u4f2a\u6ce8\u91ca\u5b9e\u73b0\u4e86\u8d85\u8fc7\u5b8c\u5168\u76d1\u7763\u8bad\u7ec3\u7684\u6027\u80fd\u63d0\u5347\uff0c\u7a81\u51fa\u4e86\u53ef\u6269\u5c55\u6ce8\u91ca\u6d41\u7a0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u5efa\u7acb\u4e25\u683c\u7684\u57fa\u7ebf\uff0c\u5f15\u5165\u57fa\u4e8e\u96c6\u6210\u7684\u4f2a\u6807\u7b7e\u6d41\u7a0b\uff0c\u4e3a\u672a\u6807\u8bb0\u7684ZeroWaste-s\u5b50\u96c6\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6ce8\u91ca\uff0c\u5e76\u5728\u771f\u5b9e\u7684\u5783\u573e\u5206\u7c7b\u6761\u4ef6\u4e0b\u7cfb\u7edf\u5730\u8bc4\u4f30OVOD\u6a21\u578b\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2508.18929", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.18929", "abs": "https://arxiv.org/abs/2508.18929", "authors": ["Ilias Driouich", "Hongliu Cao", "Eoin Thomas"], "title": "Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework", "comment": "ECAI 2025 TRUST AI workshop", "summary": "Retrieval-augmented generation (RAG) systems improve large language model\noutputs by incorporating external knowledge, enabling more informed and\ncontext-aware responses. However, the effectiveness and trustworthiness of\nthese systems critically depends on how they are evaluated, particularly on\nwhether the evaluation process captures real-world constraints like protecting\nsensitive information. While current evaluation efforts for RAG systems have\nprimarily focused on the development of performance metrics, far less attention\nhas been given to the design and quality of the underlying evaluation datasets,\ndespite their pivotal role in enabling meaningful, reliable assessments. In\nthis work, we introduce a novel multi-agent framework for generating synthetic\nQA datasets for RAG evaluation that prioritize semantic diversity and privacy\npreservation. Our approach involves: (1) a Diversity agent leveraging\nclustering techniques to maximize topical coverage and semantic variability,\n(2) a Privacy Agent that detects and mask sensitive information across multiple\ndomains and (3) a QA curation agent that synthesizes private and diverse QA\npairs suitable as ground truth for RAG evaluation. Extensive experiments\ndemonstrate that our evaluation sets outperform baseline methods in diversity\nand achieve robust privacy masking on domain-specific datasets. This work\noffers a practical and ethically aligned pathway toward safer, more\ncomprehensive RAG system evaluation, laying the foundation for future\nenhancements aligned with evolving AI regulations and compliance standards.", "AI": {"tldr": "This paper introduces a multi-agent framework to generate QA datasets for evaluating RAG systems, focusing on semantic diversity and privacy preservation. The generated datasets outperform baselines in diversity and privacy masking.", "motivation": "the effectiveness and trustworthiness of these systems critically depends on how they are evaluated, particularly on whether the evaluation process captures real-world constraints like protecting sensitive information. While current evaluation efforts for RAG systems have primarily focused on the development of performance metrics, far less attention has been given to the design and quality of the underlying evaluation datasets, despite their pivotal role in enabling meaningful, reliable assessments.", "method": "a novel multi-agent framework for generating synthetic QA datasets for RAG evaluation that prioritize semantic diversity and privacy preservation. Our approach involves: (1) a Diversity agent leveraging clustering techniques to maximize topical coverage and semantic variability, (2) a Privacy Agent that detects and mask sensitive information across multiple domains and (3) a QA curation agent that synthesizes private and diverse QA pairs suitable as ground truth for RAG evaluation.", "result": "our evaluation sets outperform baseline methods in diversity and achieve robust privacy masking on domain-specific datasets.", "conclusion": "This work offers a practical and ethically aligned pathway toward safer, more comprehensive RAG system evaluation, laying the foundation for future enhancements aligned with evolving AI regulations and compliance standards."}}
{"id": "2508.19042", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19042", "abs": "https://arxiv.org/abs/2508.19042", "authors": ["Norihiro Maruyama", "Takahide Yoshida", "Hiroki Sato", "Atsushi Masumori", "Johnsmith", "Takashi Ikegami"], "title": "A Concurrent Modular Agent: Framework for Autonomous LLM Agents", "comment": null, "summary": "We introduce the Concurrent Modular Agent (CMA), a framework that\norchestrates multiple Large-Language-Model (LLM)-based modules that operate\nfully asynchronously yet maintain a coherent and fault-tolerant behavioral\nloop. This framework addresses long-standing difficulties in agent\narchitectures by letting intention emerge from language-mediated interactions\namong autonomous processes. This approach enables flexible, adaptive, and\ncontext-dependent behavior through the combination of concurrently executed\nmodules that offload reasoning to an LLM, inter-module communication, and a\nsingle shared global state.We consider this approach to be a practical\nrealization of Minsky's Society of Mind theory. We demonstrate the viability of\nour system through two practical use-case studies. The emergent properties\nobserved in our system suggest that complex cognitive phenomena like\nself-awareness may indeed arise from the organized interaction of simpler\nprocesses, supporting Minsky-Society of Mind concept and opening new avenues\nfor artificial intelligence research. The source code for our work is available\nat: https://github.com/AlternativeMachine/concurrent-modular-agent.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5e76\u53d1\u6a21\u5757\u5316Agent\uff08CMA\uff09\uff0c\u4e00\u4e2a\u534f\u8c03\u591a\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6a21\u5757\u7684\u6846\u67b6\uff0c\u8fd9\u4e9b\u6a21\u5757\u5b8c\u5168\u5f02\u6b65\u8fd0\u884c\uff0c\u4f46\u4fdd\u6301\u8fde\u8d2f\u548c\u5bb9\u9519\u7684\u884c\u4e3a\u5faa\u73af\u3002", "motivation": "\u89e3\u51b3Agent\u67b6\u6784\u4e2d\u957f\u671f\u5b58\u5728\u7684\u96be\u9898\uff0c\u901a\u8fc7\u8ba9\u610f\u56fe\u4ece\u81ea\u4e3b\u8fc7\u7a0b\u4e4b\u95f4\u4ee5\u8bed\u8a00\u4e3a\u5a92\u4ecb\u7684\u4ea4\u4e92\u4e2d\u4ea7\u751f\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u5e76\u53d1\u6267\u884c\u7684\u6a21\u5757\u6765\u5b9e\u73b0\uff0c\u8fd9\u4e9b\u6a21\u5757\u5c06\u63a8\u7406\u8f6c\u79fb\u5230LLM\u3001\u6a21\u5757\u95f4\u901a\u4fe1\u548c\u5355\u4e00\u5171\u4eab\u5168\u5c40\u72b6\u6001\u3002", "result": "\u8be5\u6846\u67b6\u901a\u8fc7\u5728\u81ea\u4e3b\u8fc7\u7a0b\u4e4b\u95f4\u4ee5\u8bed\u8a00\u4e3a\u5a92\u4ecb\u7684\u4ea4\u4e92\u4e2d\u4ea7\u751f\u610f\u56fe\uff0c\u4ece\u800c\u5b9e\u73b0\u7075\u6d3b\u3001\u9002\u5e94\u6027\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u884c\u4e3a\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u5e76\u89c2\u5bdf\u5230\u6d8c\u73b0\u7279\u6027\uff0c\u8fd9\u8868\u660e\u590d\u6742\u7684\u8ba4\u77e5\u73b0\u8c61\u53ef\u80fd\u786e\u5b9e\u6765\u81ea\u7b80\u5355\u8fc7\u7a0b\u7684\u7ec4\u7ec7\u4ea4\u4e92\uff0c\u652f\u6301\u4e86Minsky-Society of Mind\u7684\u6982\u5ff5\uff0c\u5e76\u4e3a\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.18676", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18676", "abs": "https://arxiv.org/abs/2508.18676", "authors": ["Chufan Gao", "Jintai Chen", "Jimeng Sun"], "title": "Utilizing Training Data to Improve LLM Reasoning for Tabular Understanding", "comment": null, "summary": "Automated tabular understanding and reasoning are essential tasks for data\nscientists. Recently, Large language models (LLMs) have become increasingly\nprevalent in tabular reasoning tasks. Previous work focuses on (1) finetuning\nLLMs using labeled data or (2) Training-free prompting LLM agents using\nchain-of-thought (CoT). Finetuning offers dataset-specific learning at the cost\nof generalizability. Training-free prompting is highly generalizable but does\nnot take full advantage of training data. In this paper, we propose a novel\nprompting-based reasoning approach, Learn then Retrieve: LRTab, which\nintegrates the benefits of both by retrieving relevant information learned from\ntraining data. We first use prompting to obtain CoT responses over the training\ndata. For incorrect CoTs, we prompt the LLM to predict Prompt Conditions to\navoid the error, learning insights from the data. We validate the effectiveness\nof Prompt Conditions using validation data. Finally, at inference time, we\nretrieve the most relevant Prompt Conditions for additional context for table\nunderstanding. We provide comprehensive experiments on WikiTQ and Tabfact,\nshowing that LRTab is interpretable, cost-efficient, and can outperform\nprevious baselines in tabular reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63d0\u793a\u65b9\u6cd5LRTab\uff0c\u5b83\u901a\u8fc7\u68c0\u7d22\u76f8\u5173\u4fe1\u606f\u6765\u63d0\u9ad8\u8868\u683c\u63a8\u7406\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728WikiTQ\u548cTabfact\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u4e4b\u524d\u7684\u57fa\u7ebf\u3002", "motivation": "\u81ea\u52a8\u8868\u683c\u7406\u89e3\u548c\u63a8\u7406\u5bf9\u4e8e\u6570\u636e\u79d1\u5b66\u5bb6\u6765\u8bf4\u662f\u91cd\u8981\u7684\u4efb\u52a1\u3002\u4ee5\u524d\u7684\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u4f7f\u7528\u6807\u8bb0\u6570\u636e\u5fae\u8c03LLM\u6216\u4f7f\u7528\u601d\u7ef4\u94fe(CoT)\u8fdb\u884c\u65e0\u8bad\u7ec3\u63d0\u793aLLM\u4ee3\u7406\u3002\u5fae\u8c03\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u7279\u5b9a\u7684\u5b66\u4e60\uff0c\u4f46\u4ee5\u6cdb\u5316\u6027\u4e3a\u4ee3\u4ef7\u3002\u65e0\u8bad\u7ec3\u63d0\u793a\u5177\u6709\u9ad8\u5ea6\u7684\u6cdb\u5316\u6027\uff0c\u4f46\u6ca1\u6709\u5145\u5206\u5229\u7528\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u63a8\u7406\u65b9\u6cd5\uff0cLearn then Retrieve: LRTab\uff0c\u5b83\u901a\u8fc7\u68c0\u7d22\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u5b66\u4e60\u5230\u7684\u76f8\u5173\u4fe1\u606f\uff0c\u6574\u5408\u4e86\u4e24\u8005\u7684\u4f18\u70b9\u3002", "result": "\u5728WikiTQ\u548cTabfact\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9e\u9a8c\uff0c\u8868\u660eLRTab\u662f\u53ef\u89e3\u91ca\u7684\uff0c\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\uff0c\u5e76\u4e14\u5728\u8868\u683c\u63a8\u7406\u65b9\u9762\u4f18\u4e8e\u4e4b\u524d\u7684\u57fa\u7ebf\u3002", "conclusion": "LRTab\u5728\u8868\u683c\u63a8\u7406\u65b9\u9762\u4f18\u4e8e\u4e4b\u524d\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u4e14\u5177\u6709\u53ef\u89e3\u91ca\u6027\u548c\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2508.18825", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18825", "abs": "https://arxiv.org/abs/2508.18825", "authors": ["Yugo Kubota", "Seiichi Uchida"], "title": "Embedding Font Impression Word Tags Based on Co-occurrence", "comment": null, "summary": "Different font styles (i.e., font shapes) convey distinct impressions,\nindicating a close relationship between font shapes and word tags describing\nthose impressions. This paper proposes a novel embedding method for impression\ntags that leverages these shape-impression relationships. For instance, our\nmethod assigns similar vectors to impression tags that frequently co-occur in\norder to represent impressions of fonts, whereas standard word embedding\nmethods (e.g., BERT and CLIP) yield very different vectors. This property is\nparticularly useful for impression-based font generation and font retrieval.\nTechnically, we construct a graph whose nodes represent impression tags and\nwhose edges encode co-occurrence relationships. Then, we apply spectral\nembedding to obtain the impression vectors for each tag. We compare our method\nwith BERT and CLIP in qualitative and quantitative evaluations, demonstrating\nthat our approach performs better in impression-guided font generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5370\u8c61\u6807\u7b7e\u5d4c\u5165\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u5b57\u4f53\u5f62\u72b6\u548c\u5370\u8c61\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5728\u57fa\u4e8e\u5370\u8c61\u7684\u5b57\u4f53\u751f\u6210\u65b9\u9762\u4f18\u4e8eBERT\u548cCLIP\u3002", "motivation": "\u4e0d\u540c\u7684\u5b57\u4f53\u6837\u5f0f\u4f20\u8fbe\u4e0d\u540c\u7684\u5370\u8c61\uff0c\u8868\u660e\u5b57\u4f53\u5f62\u72b6\u548c\u63cf\u8ff0\u8fd9\u4e9b\u5370\u8c61\u7684\u8bcd\u8bed\u6807\u7b7e\u4e4b\u95f4\u5b58\u5728\u5bc6\u5207\u7684\u5173\u7cfb\u3002", "method": "\u6784\u9020\u4e00\u4e2a\u8282\u70b9\u8868\u793a\u5370\u8c61\u6807\u7b7e\u3001\u8fb9\u7f16\u7801\u5171\u73b0\u5173\u7cfb\u7684\u56fe\uff0c\u7136\u540e\u5e94\u7528\u8c31\u5d4c\u5165\u6765\u83b7\u5f97\u6bcf\u4e2a\u6807\u7b7e\u7684\u5370\u8c61\u5411\u91cf\u3002", "result": "\u4e0eBERT\u548cCLIP\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u57fa\u4e8e\u5370\u8c61\u7684\u5b57\u4f53\u751f\u6210\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002"}}
{"id": "2508.18988", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18988", "abs": "https://arxiv.org/abs/2508.18988", "authors": ["Hung Ming Liu"], "title": "Interpretable by AI Mother Tongue: Native Symbolic Reasoning in Neural Models", "comment": "25 pages, 9 figures. The AI Intuition Explorer dashboard is available\n  at: https://cyrilliu1974.github.io/github.io/vi.html", "summary": "We present a framework where neural models develop an AI Mother Tongue, a\nnative symbolic language that simultaneously supports intuitive reasoning,\ncompositional symbol chains, and inherent interpretability. Unlike post-hoc\nexplanation methods, our approach embeds reasoning directly into the model's\nrepresentations: symbols capture meaningful semantic patterns, chains trace\ndecision paths, and gated induction mechanisms guide selective focus, yielding\ntransparent yet flexible reasoning. We introduce complementary training\nobjectives to enhance symbol purity and decision sparsity, and employ a\nsequential specialization strategy to first build broad symbolic competence and\nthen refine intuitive judgments. Experiments on AI tasks demonstrate\ncompetitive accuracy alongside verifiable reasoning traces, showing that AI\nMother Tongue can serve as a unified mechanism for interpretability, intuition,\nand symbolic reasoning in neural models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u6a21\u578b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7AI Mother Tongue\u6765\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u3002", "motivation": "\u795e\u7ecf\u6a21\u578b\u7f3a\u4e4f\u76f4\u89c2\u7684\u63a8\u7406\u3001\u7ec4\u5408\u7b26\u53f7\u94fe\u548c\u56fa\u6709\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5f00\u53d1\u4e00\u79cd\u795e\u7ecf\u6a21\u578b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u4e00\u79cd\u539f\u751f\u7b26\u53f7\u8bed\u8a00\uff0c\u5373AI Mother Tongue\u3002\u4f7f\u7528\u4e92\u8865\u7684\u8bad\u7ec3\u76ee\u6807\u6765\u589e\u5f3a\u7b26\u53f7\u7eaf\u5ea6\u548c\u51b3\u7b56\u7a00\u758f\u6027\uff0c\u5e76\u91c7\u7528\u987a\u5e8f\u4e13\u4e1a\u5316\u7b56\u7565\u3002", "result": "\u5728AI\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u7ade\u4e89\u6027\u7684\u51c6\u786e\u6027\u4ee5\u53ca\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "conclusion": "AI Mother Tongue\u53ef\u4ee5\u4f5c\u4e3a\u795e\u7ecf\u6a21\u578b\u4e2d\u53ef\u89e3\u91ca\u6027\u3001\u76f4\u89c9\u548c\u7b26\u53f7\u63a8\u7406\u7684\u7edf\u4e00\u673a\u5236\u3002"}}
{"id": "2508.19069", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19069", "abs": "https://arxiv.org/abs/2508.19069", "authors": ["Zhichao Yang", "Zhaoxin Fan", "Gen Li", "Yuanze Hu", "Xinyu Wang", "Ye Qiu", "Xin Wang", "Yifan Sun", "Wenjun Wu"], "title": "Can Structured Templates Facilitate LLMs in Tackling Harder Tasks? : An Exploration of Scaling Laws by Difficulty", "comment": "9 pages", "summary": "Structured, procedural reasoning is essential for Large Language Models\n(LLMs), especially in mathematics. While post-training methods have improved\nLLM performance, they still fall short in capturing deep procedural logic on\ncomplex tasks. To tackle the issue, in this paper, we first investigate this\nlimitation and uncover a novel finding: a Scaling Law by Difficulty, which\nreveals that model performance follows a U-shaped curve with respect to\ntraining data complexity -- excessive low-difficulty data impedes abstraction,\nwhile high-difficulty data significantly enhances reasoning ability. Motivated\nby this, we propose the Structured Solution Template (SST) framework, which\nuses solution templates and a curriculum of varied difficulty to explicitly\nteach procedural reasoning. Specifically, SST comprises (1) fine-tuning with\nstructured solution-template chains and dynamically weighted loss to prioritize\nprocedural logic, (2) prompt-time injection of solution templates as cognitive\nscaffolds to guide inference, and (3) integrated curriculum fine-tuning that\nexplicitly teaches the model to self-plan - execute - self-correct. Experiments\non GSM8K, AIME24, and new Dynamic En benchmark show that SST significantly\nimproves both accuracy and efficiency, especially on harder problems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7ed3\u6784\u5316\u89e3\u51b3\u65b9\u6848\u6a21\u677f\uff08SST\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u89e3\u51b3\u65b9\u6848\u6a21\u677f\u548c\u4e0d\u540c\u96be\u5ea6\u7684\u8bfe\u7a0b\u6765\u663e\u5f0f\u5730\u6559\u6388\u7a0b\u5e8f\u63a8\u7406\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5c24\u5176\u662f\u5728\u8f83\u96be\u7684\u95ee\u9898\u4e0a\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7ed3\u6784\u5316\u7a0b\u5e8f\u63a8\u7406\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u6570\u5b66\u65b9\u9762\u3002\u867d\u7136\u4e8b\u540e\u8bad\u7ec3\u65b9\u6cd5\u63d0\u9ad8\u4e86LLM\u7684\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u5728\u6355\u83b7\u590d\u6742\u4efb\u52a1\u7684\u6df1\u5c42\u7a0b\u5e8f\u903b\u8f91\u65b9\u9762\u4ecd\u7136\u4e0d\u8db3\u3002", "method": "\u7ed3\u6784\u5316\u89e3\u51b3\u65b9\u6848\u6a21\u677f\uff08SST\uff09\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u89e3\u51b3\u65b9\u6848\u6a21\u677f\u548c\u4e0d\u540c\u96be\u5ea6\u7684\u8bfe\u7a0b\u6765\u663e\u5f0f\u5730\u6559\u6388\u7a0b\u5e8f\u63a8\u7406\u3002", "result": "\u6a21\u578b\u6027\u80fd\u9075\u5faa\u5173\u4e8e\u8bad\u7ec3\u6570\u636e\u590d\u6742\u6027\u7684U\u5f62\u66f2\u7ebf\u2014\u2014\u8fc7\u591a\u7684\u4f4e\u96be\u5ea6\u6570\u636e\u4f1a\u963b\u788d\u62bd\u8c61\uff0c\u800c\u9ad8\u96be\u5ea6\u6570\u636e\u4f1a\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "SST\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5c24\u5176\u662f\u5728\u8f83\u96be\u7684\u95ee\u9898\u4e0a\u3002"}}
{"id": "2508.18688", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18688", "abs": "https://arxiv.org/abs/2508.18688", "authors": ["Hejiang Cai", "Di Wu", "Ji Xu", "Xiang Liu", "Yiziting Zhu", "Xin Shu", "Yujie Li", "Bin Yi"], "title": "End to End Autoencoder MLP Framework for Sepsis Prediction", "comment": null, "summary": "Sepsis is a life threatening condition that requires timely detection in\nintensive care settings. Traditional machine learning approaches, including\nNaive Bayes, Support Vector Machine (SVM), Random Forest, and XGBoost, often\nrely on manual feature engineering and struggle with irregular, incomplete\ntime-series data commonly present in electronic health records. We introduce an\nend-to-end deep learning framework integrating an unsupervised autoencoder for\nautomatic feature extraction with a multilayer perceptron classifier for binary\nsepsis risk prediction. To enhance clinical applicability, we implement a\ncustomized down sampling strategy that extracts high information density\nsegments during training and a non-overlapping dynamic sliding window mechanism\nfor real-time inference. Preprocessed time series data are represented as fixed\ndimension vectors with explicit missingness indicators, mitigating bias and\nnoise. We validate our approach on three ICU cohorts. Our end-to-end model\nachieves accuracies of 74.6 percent, 80.6 percent, and 93.5 percent,\nrespectively, consistently outperforming traditional machine learning\nbaselines. These results demonstrate the framework's superior robustness,\ngeneralizability, and clinical utility for early sepsis detection across\nheterogeneous ICU environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65e9\u671f\u8113\u6bd2\u75c7\u68c0\u6d4b\u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u4e09\u4e2aICU\u961f\u5217\u4e2d\u4f18\u4e8e\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u8113\u6bd2\u75c7\u662f\u4e00\u79cd\u5371\u53ca\u751f\u547d\u7684\u75be\u75c5\uff0c\u9700\u8981\u5728\u91cd\u75c7\u76d1\u62a4\u73af\u5883\u4e2d\u53ca\u65f6\u68c0\u6d4b\u3002\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u624b\u52a8\u7279\u5f81\u5de5\u7a0b\uff0c\u5e76\u4e14\u96be\u4ee5\u5904\u7406\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u5e38\u89c1\u7684irregular, incomplete time-series data\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u7528\u4e8e\u81ea\u52a8\u7279\u5f81\u63d0\u53d6\u7684\u65e0\u76d1\u7763\u81ea\u52a8\u7f16\u7801\u5668\u548c\u7528\u4e8e\u4e8c\u5143\u8113\u6bd2\u75c7\u98ce\u9669\u9884\u6d4b\u7684\u591a\u5c42\u611f\u77e5\u5668\u5206\u7c7b\u5668\u3002\u4e3a\u4e86\u589e\u5f3a\u4e34\u5e8a\u9002\u7528\u6027\uff0c\u6211\u4eec\u5b9e\u65bd\u4e86\u4e00\u79cd\u5b9a\u5236\u7684\u4e0b\u91c7\u6837\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5728\u8bad\u7ec3\u671f\u95f4\u63d0\u53d6\u9ad8\u4fe1\u606f\u5bc6\u5ea6\u7247\u6bb5\uff0c\u4ee5\u53ca\u4e00\u79cd\u7528\u4e8e\u5b9e\u65f6\u63a8\u7406\u7684\u975e\u91cd\u53e0\u52a8\u6001\u6ed1\u52a8\u7a97\u53e3\u673a\u5236\u3002", "result": "\u6211\u4eec\u7684\u7aef\u5230\u7aef\u6a21\u578b\u5b9e\u73b0\u4e8674.6%\u300180.6%\u548c93.5%\u7684\u51c6\u786e\u7387\uff0c\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u5f02\u6784ICU\u73af\u5883\u4e2d\u65e9\u671f\u8113\u6bd2\u75c7\u68c0\u6d4b\u65b9\u9762\u5177\u6709\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u3001\u901a\u7528\u6027\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.18829", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18829", "abs": "https://arxiv.org/abs/2508.18829", "authors": ["Takayuki Ishikawa", "Carmelo Bonannella", "Bas J. W. Lerink", "Marc Ru\u00dfwurm"], "title": "Deep Pre-trained Time Series Features for Tree Species Classification in the Dutch Forest Inventory", "comment": null, "summary": "National Forest Inventory (NFI)s serve as the primary source of forest\ninformation, providing crucial tree species distribution data. However,\nmaintaining these inventories requires labor-intensive on-site campaigns.\nRemote sensing approaches, particularly when combined with machine learning,\noffer opportunities to update NFIs more frequently and at larger scales. While\nthe use of Satellite Image Time Series has proven effective for distinguishing\ntree species through seasonal canopy reflectance patterns, current approaches\nrely primarily on Random Forest classifiers with hand-designed features and\nphenology-based metrics. Using deep features from an available pre-trained\nremote sensing foundation models offers a complementary strategy. These\npre-trained models leverage unannotated global data and are meant to used for\ngeneral-purpose applications and can then be efficiently fine-tuned with\nsmaller labeled datasets for specific classification tasks. This work\nsystematically investigates how deep features improve tree species\nclassification accuracy in the Netherlands with few annotated data. Data-wise,\nwe extracted time-series data from Sentinel-1, Sentinel-2 and ERA5 satellites\ndata and SRTM data using Google Earth Engine. Our results demonstrate that\nfine-tuning a publicly available remote sensing time series foundation model\noutperforms the current state-of-the-art in NFI classification in the\nNetherlands by a large margin of up to 10% across all datasets. This\ndemonstrates that classic hand-defined harmonic features are too simple for\nthis task and highlights the potential of using deep AI features for\ndata-limited application like NFI classification. By leveraging openly\navailable satellite data and pre-trained models, this approach significantly\nimproves classification accuracy compared to traditional methods and can\neffectively complement existing forest inventory processes.", "AI": {"tldr": "Deep learning improves tree species classification accuracy in the Netherlands, outperforming traditional methods by 10%.", "motivation": "Maintaining National Forest Inventories (NFIs) requires labor-intensive on-site campaigns, and remote sensing with machine learning offers opportunities for more frequent and larger-scale updates. Current approaches rely on Random Forest classifiers with hand-designed features, while deep features from pre-trained remote sensing foundation models offer a complementary strategy.", "method": "Fine-tuning a publicly available remote sensing time series foundation model with Sentinel-1, Sentinel-2, ERA5, and SRTM data extracted using Google Earth Engine.", "result": "Fine-tuning a remote sensing time series foundation model outperforms the current state-of-the-art in NFI classification in the Netherlands by up to 10%.", "conclusion": "Fine-tuning a remote sensing time series foundation model improves NFI classification accuracy in the Netherlands by up to 10% compared to the state-of-the-art, demonstrating the potential of deep AI features for data-limited applications."}}
{"id": "2508.18992", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.18992", "abs": "https://arxiv.org/abs/2508.18992", "authors": ["Viktor N. Zhuravlev", "Artur R. Khairullin", "Ernest A. Dyagin", "Alena N. Sitkina", "Nikita I. Kulin"], "title": "Automatic Prompt Optimization with Prompt Distillation", "comment": null, "summary": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which is gaining popularity due to the rapid development of\nprompt engineering driven by extensive research in the field of large language\nmodels (LLMs). This paper presents DistillPrompt -- a novel autoprompting\nmethod based on large language models that employs a multi-stage integration of\ntask-specific information into prompts using training data. DistillPrompt\nutilizes distillation, compression, and aggregation operations to explore the\nprompt space more thoroughly. The method was tested on different datasets for\ntext classification and generation tasks using the t-lite-instruct-0.1 language\nmodel. The results demonstrate a significant average improvement (e.g., 20.12%\nacross the entire dataset compared to Grips) in key metrics over existing\nmethods in the field, establishing DistillPrompt as one of the most effective\nnon-gradient approaches in autoprompting.", "AI": {"tldr": "DistillPrompt is a novel autoprompting method based on large language models that employs a multi-stage integration of task-specific information into prompts using training data.", "motivation": "This paper presents DistillPrompt -- a novel autoprompting method based on large language models that employs a multi-stage integration of task-specific information into prompts using training data.", "method": "DistillPrompt utilizes distillation, compression, and aggregation operations to explore the prompt space more thoroughly.", "result": "The results demonstrate a significant average improvement (e.g., 20.12% across the entire dataset compared to Grips) in key metrics over existing methods in the field.", "conclusion": "DistillPrompt is one of the most effective non-gradient approaches in autoprompting."}}
{"id": "2508.19096", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19096", "abs": "https://arxiv.org/abs/2508.19096", "authors": ["Yongwoo Song", "Minbyul Jeong", "Mujeen Sung"], "title": "Trustworthy Agents for Electronic Health Records through Confidence Estimation", "comment": null, "summary": "Large language models (LLMs) show promise for extracting information from\nElectronic Health Records (EHR) and supporting clinical decisions. However,\ndeployment in clinical settings faces challenges due to hallucination risks. We\npropose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metric\nquantifying the accuracy-reliability trade-off at varying confidence\nthresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporating\nstepwise confidence estimation for clinical question answering. Experiments on\nMIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines under\nstrict reliability constraints, achieving improvements of 44.23%p and 25.34%p\nat HCAcc@70% while baseline methods fail at these thresholds. These results\nhighlight limitations of traditional accuracy metrics in evaluating healthcare\nAI agents. Our work contributes to developing trustworthy clinical agents that\ndeliver accurate information or transparently express uncertainty when\nconfidence is low.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6307\u6807HCAcc@k%\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aTrustEHRAgent\u7684\u7f6e\u4fe1\u5ea6\u611f\u77e5\u4ee3\u7406\uff0c\u4ee5\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u4fdd\u5065\u5e94\u7528\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTrustEHRAgent\u5728\u53ef\u9760\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4ece\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u4e2d\u63d0\u53d6\u4fe1\u606f\u548c\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u65b9\u9762\u663e\u793a\u51fa\u5e0c\u671b\uff0c\u4f46\u7531\u4e8e\u5e7b\u89c9\u98ce\u9669\uff0c\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u9762\u4e34\u6311\u6218\u3002", "method": "\u4ed6\u4eec\u5f15\u5165\u4e86TrustEHRAgent\uff0c\u8fd9\u662f\u4e00\u79cd\u7f6e\u4fe1\u5ea6\u611f\u77e5\u4ee3\u7406\uff0c\u5b83\u7ed3\u5408\u4e86\u7528\u4e8e\u4e34\u5e8a\u95ee\u9898\u89e3\u7b54\u7684\u9010\u6b65\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u3002", "result": "\u5728MIMIC-III\u548ceICU\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTrustEHRAgent\u5728\u4e25\u683c\u7684\u53ef\u9760\u6027\u7ea6\u675f\u4e0b\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728HCAcc@70%\u65f6\u5b9e\u73b0\u4e8644.23%p\u548c25.34%p\u7684\u6539\u8fdb\uff0c\u800c\u57fa\u7ebf\u65b9\u6cd5\u5728\u8fd9\u4e9b\u9608\u503c\u4e0b\u5931\u8d25\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u8868\u660e\uff0c\u5728\u533b\u7597\u4fdd\u5065\u9886\u57df\uff0c\u4f20\u7edf\u7684\u51c6\u786e\u6027\u6307\u6807\u5728\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002\u4ed6\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTrustEHRAgent\u7684\u7f6e\u4fe1\u5ea6\u611f\u77e5\u4ee3\u7406\uff0c\u8be5\u4ee3\u7406\u5728\u4e25\u683c\u7684\u53ef\u9760\u6027\u7ea6\u675f\u4e0b\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4ece\u800c\u6709\u52a9\u4e8e\u5f00\u53d1\u53ef\u4fe1\u8d56\u7684\u4e34\u5e8a\u4ee3\u7406\u3002"}}
{"id": "2508.18717", "categories": ["cs.LG", "cs.CV", "cs.IT", "math.AT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.18717", "abs": "https://arxiv.org/abs/2508.18717", "authors": ["V. S. Usatyuk", "D. A. Sapoznikov", "S. I. Egorov"], "title": "Natural Image Classification via Quasi-Cyclic Graph Ensembles and Random-Bond Ising Models at the Nishimori Temperature", "comment": "27 pages, 8 figures, 2 tables, was presented at the 9th International\n  Conference 'Deep Learning on Computational Physics (DLCP2025)', and is\n  currently under review for the Moscow University Physics Bulletin, Physics\n  series", "summary": "We present a unified framework combining statistical physics, coding theory,\nand algebraic topology for efficient multi-class image classification.\nHigh-dimensional feature vectors from a frozen MobileNetV2 backbone are\ninterpreted as spins on a sparse Multi-Edge Type quasi-cyclic LDPC\n(MET-QC-LDPC) graph, forming a Random-Bond Ising Model (RBIM). We operate this\nRBIM at its Nishimori temperature, $\\beta_N$, where the smallest eigenvalue of\nthe Bethe-Hessian matrix vanishes, maximizing class separability.\n  Our theoretical contribution establishes a correspondence between local\ntrapping sets in the code's graph and topological invariants (Betti numbers,\nbordism classes) of the feature manifold. A practical algorithm estimates\n$\\beta_N$ efficiently with a quadratic interpolant and Newton correction,\nachieving a six-fold speed-up over bisection.\n  Guided by topology, we design spherical and toroidal MET-QC-LDPC graph\nensembles, using permanent bounds to suppress harmful trapping sets. This\ncompresses 1280-dimensional features to 32 or 64 dimensions for ImageNet-10 and\n-100 subsets. Despite massive compression (40x fewer parameters), we achieve\n98.7% accuracy on ImageNet-10 and 82.7% on ImageNet-100, demonstrating that\ntopology-guided graph design yields highly efficient, physics-inspired\nembeddings with state-of-the-art performance.", "AI": {"tldr": "Combines statistical physics, coding theory, and algebraic topology for efficient image classification, achieving high accuracy with massive compression using topology-guided graph design.", "motivation": "Efficient multi-class image classification.", "method": "A unified framework combining statistical physics, coding theory, and algebraic topology. High-dimensional feature vectors are interpreted as spins on a sparse Multi-Edge Type quasi-cyclic LDPC graph, forming a Random-Bond Ising Model (RBIM).", "result": "Achieves 98.7% accuracy on ImageNet-10 and 82.7% on ImageNet-100 with 40x fewer parameters.", "conclusion": "Topology-guided graph design yields highly efficient, physics-inspired embeddings with state-of-the-art performance, achieving 98.7% accuracy on ImageNet-10 and 82.7% on ImageNet-100 despite massive compression."}}
{"id": "2508.18831", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18831", "abs": "https://arxiv.org/abs/2508.18831", "authors": ["Yosuke Yamagishi", "Shouhei Hanaoka"], "title": "Automated Classification of Normal and Atypical Mitotic Figures Using ConvNeXt V2: MIDOG 2025 Track 2", "comment": "MIDOG 2025 solution", "summary": "This paper presents our solution for the MIDOG 2025 Challenge Track 2, which\nfocuses on binary classification of normal mitotic figures (NMFs) versus\natypical mitotic figures (AMFs) in histopathological images. Our approach\nleverages a ConvNeXt V2 base model with center cropping preprocessing and\n5-fold cross-validation ensemble strategy. The method addresses key challenges\nincluding severe class imbalance, high morphological variability, and domain\nheterogeneity across different tumor types, species, and scanners. Through\nstrategic preprocessing with 60% center cropping and mixed precision training,\nour model achieved robust performance on the diverse MIDOG 2025 dataset. The\nsolution demonstrates the effectiveness of modern convolutional architectures\nfor mitotic figure subtyping while maintaining computational efficiency through\ncareful architectural choices and training optimizations.", "AI": {"tldr": "binary classification of normal mitotic figures (NMFs) versus atypical mitotic figures (AMFs) in histopathological images using ConvNeXt V2", "motivation": "addresses key challenges including severe class imbalance, high morphological variability, and domain heterogeneity across different tumor types, species, and scanners.", "method": "a ConvNeXt V2 base model with center cropping preprocessing and 5-fold cross-validation ensemble strategy", "result": "achieved robust performance on the diverse MIDOG 2025 dataset", "conclusion": "The solution demonstrates the effectiveness of modern convolutional architectures for mitotic figure subtyping while maintaining computational efficiency through careful architectural choices and training optimizations."}}
{"id": "2508.19026", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19026", "abs": "https://arxiv.org/abs/2508.19026", "authors": ["Gueter Josmy Faure", "Min-Hung Chen", "Jia-Fong Yeh", "Ying Cheng", "Hung-Ting Su", "Yung-Hao Tang", "Shang-Hong Lai", "Winston H. Hsu"], "title": "MovieCORE: COgnitive REasoning in Movies", "comment": "Accepted for EMNLP'2025 Main Conference. Project Page:\n  https://joslefaure.github.io/assets/html/moviecore.html", "summary": "This paper introduces MovieCORE, a novel video question answering (VQA)\ndataset designed to probe deeper cognitive understanding of movie content.\nUnlike existing datasets that focus on surface-level comprehension, MovieCORE\nemphasizes questions that engage System-2 thinking while remaining specific to\nthe video material. We present an innovative agentic brainstorming approach,\nutilizing multiple large language models (LLMs) as thought agents to generate\nand refine high-quality question-answer pairs. To evaluate dataset quality, we\ndevelop a set of cognitive tests assessing depth, thought-provocation\npotential, and syntactic complexity. We also propose a comprehensive evaluation\nscheme for assessing VQA model performance on deeper cognitive tasks. To\naddress the limitations of existing video-language models (VLMs), we introduce\nan agentic enhancement module, Agentic Choice Enhancement (ACE), which improves\nmodel reasoning capabilities post-training by up to 25%. Our work contributes\nto advancing movie understanding in AI systems and provides valuable insights\ninto the capabilities and limitations of current VQA models when faced with\nmore challenging, nuanced questions about cinematic content. Our project page,\ndataset and code can be found at\nhttps://joslefaure.github.io/assets/html/moviecore.html.", "AI": {"tldr": "MovieCORE: A VQA dataset probing deeper cognitive understanding of movies, with an ACE module to enhance model reasoning.", "motivation": "Existing VQA datasets lack depth and fail to engage System-2 thinking for movie content understanding.", "method": "Agentic brainstorming approach using multiple LLMs for question-answer pair generation and an agentic enhancement module (ACE) to improve model reasoning.", "result": "Introduces MovieCORE dataset and ACE module, improving model reasoning by up to 25%.", "conclusion": "This work advances movie understanding in AI and reveals limitations of current VQA models on nuanced cinematic content questions."}}
{"id": "2508.19097", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19097", "abs": "https://arxiv.org/abs/2508.19097", "authors": ["Armin Berger", "Sarthak Khanna", "David Berghaus", "Rafet Sifa"], "title": "Reasoning LLMs in the Medical Domain: A Literature Survey", "comment": null, "summary": "The emergence of advanced reasoning capabilities in Large Language Models\n(LLMs) marks a transformative development in healthcare applications. Beyond\nmerely expanding functional capabilities, these reasoning mechanisms enhance\ndecision transparency and explainability-critical requirements in medical\ncontexts. This survey examines the transformation of medical LLMs from basic\ninformation retrieval tools to sophisticated clinical reasoning systems capable\nof supporting complex healthcare decisions. We provide a thorough analysis of\nthe enabling technological foundations, with a particular focus on specialized\nprompting techniques like Chain-of-Thought and recent breakthroughs in\nReinforcement Learning exemplified by DeepSeek-R1. Our investigation evaluates\npurpose-built medical frameworks while also examining emerging paradigms such\nas multi-agent collaborative systems and innovative prompting architectures.\nThe survey critically assesses current evaluation methodologies for medical\nvalidation and addresses persistent challenges in field interpretation\nlimitations, bias mitigation strategies, patient safety frameworks, and\nintegration of multimodal clinical data. Through this survey, we seek to\nestablish a roadmap for developing reliable LLMs that can serve as effective\npartners in clinical practice and medical research.", "AI": {"tldr": "\u672c\u8c03\u67e5\u8003\u5bdf\u4e86\u533b\u5b66LLM\u4ece\u57fa\u672c\u4fe1\u606f\u68c0\u7d22\u5de5\u5177\u5230\u590d\u6742\u4e34\u5e8a\u63a8\u7406\u7cfb\u7edf\u7684\u8f6c\u53d8\uff0c\u8bc4\u4f30\u4e86\u6280\u672f\u57fa\u7840\u3001\u4e13\u7528\u63d0\u793a\u6280\u672f\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u89e3\u51b3\u4e86\u8be5\u9886\u57df\u7684\u6301\u7eed\u6311\u6218\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4e2d\u51fa\u73b0\u7684\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\u6807\u5fd7\u7740\u533b\u7597\u4fdd\u5065\u5e94\u7528\u9886\u57df\u7684\u4e00\u9879\u53d8\u9769\u6027\u53d1\u5c55\u3002\u9664\u4e86\u6269\u5c55\u529f\u80fd\u80fd\u529b\u5916\uff0c\u8fd9\u4e9b\u63a8\u7406\u673a\u5236\u8fd8\u589e\u5f3a\u4e86\u51b3\u7b56\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u800c\u8fd9\u5728\u533b\u7597\u73af\u5883\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5bf9\u533b\u5b66LLM\u7684\u652f\u6301\u6280\u672f\u57fa\u7840\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u7279\u522b\u5173\u6ce8\u4e86Chain-of-Thought\u7b49\u4e13\u95e8\u7684\u63d0\u793a\u6280\u672f\u4ee5\u53caDeepSeek-R1\u7b49\u5f3a\u5316\u5b66\u4e60\u7684\u6700\u65b0\u7a81\u7834\u3002\u8c03\u67e5\u8bc4\u4f30\u4e86\u4e13\u95e8\u6784\u5efa\u7684\u533b\u7597\u6846\u67b6\uff0c\u540c\u65f6\u4e5f\u8003\u5bdf\u4e86\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7cfb\u7edf\u548c\u521b\u65b0\u63d0\u793a\u67b6\u6784\u7b49\u65b0\u5174\u8303\u5f0f\u3002", "result": "\u533b\u5b66LLM\u5df2\u4ece\u57fa\u672c\u4fe1\u606f\u68c0\u7d22\u5de5\u5177\u8f6c\u53d8\u4e3a\u80fd\u591f\u652f\u6301\u590d\u6742\u533b\u7597\u51b3\u7b56\u7684\u590d\u6742\u4e34\u5e8a\u63a8\u7406\u7cfb\u7edf\u3002\u8c03\u67e5 \u043a\u0440\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u8bc4\u4f30\u4e86\u5f53\u524d\u533b\u5b66\u9a8c\u8bc1\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u89e3\u51b3\u4e86\u9886\u57df\u89e3\u91ca\u5c40\u9650\u6027\u3001\u504f\u5dee\u7f13\u89e3\u7b56\u7565\u3001\u60a3\u8005\u5b89\u5168\u6846\u67b6\u4ee5\u53ca\u591a\u6a21\u6001\u4e34\u5e8a\u6570\u636e\u96c6\u6210\u65b9\u9762\u6301\u7eed\u5b58\u5728\u7684\u6311\u6218\u3002", "conclusion": "\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u5f00\u53d1\u53ef\u9760\u7684LLM\u5efa\u7acb\u8def\u7ebf\u56fe\uff0c\u4f7f\u5176\u80fd\u591f\u6210\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u548c\u533b\u5b66\u7814\u7a76\u4e2d\u7684\u6709\u6548\u4f19\u4f34\u3002"}}
{"id": "2508.18730", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.18730", "abs": "https://arxiv.org/abs/2508.18730", "authors": ["Yi Liu", "Hongji Zhang", "Yiwen Wang", "Dimitris Tsaras", "Lei Chen", "Mingxuan Yuan", "Qiang Xu"], "title": "Beyond Tokens: Enhancing RTL Quality Estimation via Structural Graph Learning", "comment": null, "summary": "Estimating the quality of register transfer level (RTL) designs is crucial in\nthe electronic design automation (EDA) workflow, as it enables instant feedback\non key metrics like area and delay without the need for time-consuming logic\nsynthesis. While recent approaches have leveraged large language models (LLMs)\nto derive embeddings from RTL code and achieved promising results, they\noverlook the structural semantics essential for accurate quality estimation. In\ncontrast, the control data flow graph (CDFG) view exposes the design's\nstructural characteristics more explicitly, offering richer cues for\nrepresentation learning. In this work, we introduce a novel structure-aware\ngraph self-supervised learning framework, StructRTL, for improved RTL design\nquality estimation. By learning structure-informed representations from CDFGs,\nour method significantly outperforms prior art on various quality estimation\ntasks. To further boost performance, we incorporate a knowledge distillation\nstrategy that transfers low-level insights from post-mapping netlists into the\nCDFG predictor. Experiments show that our approach establishes new\nstate-of-the-art results, demonstrating the effectiveness of combining\nstructural learning with cross-stage supervision.", "AI": {"tldr": "This paper presents StructRTL, a new framework for RTL design quality estimation that uses structure-aware graph self-supervised learning on CDFGs and knowledge distillation to outperform existing methods.", "motivation": "Estimating the quality of register transfer level (RTL) designs is crucial in the electronic design automation (EDA) workflow, as it enables instant feedback on key metrics like area and delay without the need for time-consuming logic synthesis. Recent approaches have leveraged large language models (LLMs) to derive embeddings from RTL code but overlook the structural semantics essential for accurate quality estimation. The control data flow graph (CDFG) view exposes the design's structural characteristics more explicitly, offering richer cues for representation learning.", "method": "The method learns structure-informed representations from CDFGs and incorporates a knowledge distillation strategy that transfers low-level insights from post-mapping netlists into the CDFG predictor.", "result": "The method significantly outperforms prior art on various quality estimation tasks and establishes new state-of-the-art results.", "conclusion": "This paper introduces StructRTL, a structure-aware graph self-supervised learning framework, for improved RTL design quality estimation. Experiments show that StructRTL establishes new state-of-the-art results, demonstrating the effectiveness of combining structural learning with cross-stage supervision."}}
{"id": "2508.18834", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.18834", "abs": "https://arxiv.org/abs/2508.18834", "authors": ["Zizheng Guo", "Bochao Zou", "Yinuo Jia", "Xiangyu Li", "Huimin Ma"], "title": "Boosting Micro-Expression Analysis via Prior-Guided Video-Level Regression", "comment": null, "summary": "Micro-expressions (MEs) are involuntary, low-intensity, and short-duration\nfacial expressions that often reveal an individual's genuine thoughts and\nemotions. Most existing ME analysis methods rely on window-level classification\nwith fixed window sizes and hard decisions, which limits their ability to\ncapture the complex temporal dynamics of MEs. Although recent approaches have\nadopted video-level regression frameworks to address some of these challenges,\ninterval decoding still depends on manually predefined, window-based methods,\nleaving the issue only partially mitigated. In this paper, we propose a\nprior-guided video-level regression method for ME analysis. We introduce a\nscalable interval selection strategy that comprehensively considers the\ntemporal evolution, duration, and class distribution characteristics of MEs,\nenabling precise spotting of the onset, apex, and offset phases. In addition,\nwe introduce a synergistic optimization framework, in which the spotting and\nrecognition tasks share parameters except for the classification heads. This\nfully exploits complementary information, makes more efficient use of limited\ndata, and enhances the model's capability. Extensive experiments on multiple\nbenchmark datasets demonstrate the state-of-the-art performance of our method,\nwith an STRS of 0.0562 on CAS(ME)$^3$ and 0.2000 on SAMMLV. The code is\navailable at https://github.com/zizheng-guo/BoostingVRME.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5148\u9a8c\u5f15\u5bfc\u7684\u89c6\u9891\u7ea7\u56de\u5f52\u65b9\u6cd5\u7528\u4e8e\u5fae\u8868\u60c5\u5206\u6790\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u7684\u5fae\u8868\u60c5\u5206\u6790\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7a97\u53e3\u5927\u5c0f\u548c\u786c\u51b3\u7b56\u7684\u7a97\u53e3\u7ea7\u5206\u7c7b\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u6355\u6349\u5fae\u8868\u60c5\u590d\u6742\u65f6\u95f4\u52a8\u6001\u7684\u80fd\u529b\u3002\u867d\u7136\u6700\u8fd1\u7684\u65b9\u6cd5\u91c7\u7528\u4e86\u89c6\u9891\u7ea7\u56de\u5f52\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u4f46\u533a\u95f4\u89e3\u7801\u4ecd\u7136\u4f9d\u8d56\u4e8e\u624b\u52a8\u9884\u5b9a\u4e49\u7684\u3001\u57fa\u4e8e\u7a97\u53e3\u7684\u65b9\u6cd5\uff0c\u95ee\u9898\u53ea\u5f97\u5230\u4e86\u90e8\u5206\u7f13\u89e3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5148\u9a8c\u5f15\u5bfc\u7684\u89c6\u9891\u7ea7\u56de\u5f52\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u53ef\u6269\u5c55\u7684\u533a\u95f4\u9009\u62e9\u7b56\u7565\u548c\u534f\u540c\u4f18\u5316\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5728 CAS(ME)$^3$ \u4e0a\u7684 STRS \u4e3a 0.0562\uff0c\u5728 SAMMLV \u4e0a\u7684 STRS \u4e3a 0.2000\u3002"}}
{"id": "2508.19076", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19076", "abs": "https://arxiv.org/abs/2508.19076", "authors": ["Ziyue Li", "Yuan Chang", "Gaihong Yu", "Xiaoqiu Le"], "title": "HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local Guidance", "comment": null, "summary": "Large language model (LLM)-based agents have demonstrated remarkable\ncapabilities in decision-making tasks, but struggle significantly with complex,\nlong-horizon planning scenarios. This arises from their lack of macroscopic\nguidance, causing disorientation and failures in complex tasks, as well as\ninsufficient continuous oversight during execution, rendering them unresponsive\nto environmental changes and prone to deviations. To tackle these challenges,\nwe introduce HiPlan, a hierarchical planning framework that provides adaptive\nglobal-local guidance to boost LLM-based agents'decision-making. HiPlan\ndecomposes complex tasks into milestone action guides for general direction and\nstep-wise hints for detailed actions. During the offline phase, we construct a\nmilestone library from expert demonstrations, enabling structured experience\nreuse by retrieving semantically similar tasks and milestones. In the execution\nphase, trajectory segments from past milestones are dynamically adapted to\ngenerate step-wise hints that align current observations with the milestone\nobjectives, bridging gaps and correcting deviations. Extensive experiments\nacross two challenging benchmarks demonstrate that HiPlan substantially\noutperforms strong baselines, and ablation studies validate the complementary\nbenefits of its hierarchical components.", "AI": {"tldr": "HiPlan \u901a\u8fc7\u5206\u5c42\u89c4\u5212\u6846\u67b6\uff0c\u4e3a\u57fa\u4e8e LLM \u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u81ea\u9002\u5e94\u7684\u5168\u5c40-\u5c40\u90e8\u6307\u5bfc\uff0c\u4ee5\u63d0\u9ad8\u5176\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u667a\u80fd\u4f53\u5728\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u80fd\u529b\uff0c\u4f46\u5728\u590d\u6742\u3001\u957f\u671f\u7684\u89c4\u5212\u573a\u666f\u4e2d\u5374\u8868\u73b0\u5f97\u975e\u5e38\u5403\u529b\u3002\u8fd9\u662f\u56e0\u4e3a\u5b83\u4eec\u7f3a\u4e4f\u5b8f\u89c2\u6307\u5bfc\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u7684\u4efb\u52a1\u4e2d\u8ff7\u5931\u65b9\u5411\u548c\u5931\u8d25\uff0c\u4ee5\u53ca\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\u7f3a\u4e4f\u6301\u7eed\u7684\u76d1\u7763\uff0c\u4f7f\u5b83\u4eec\u5bf9\u73af\u5883\u53d8\u5316\u53cd\u5e94\u8fdf\u949d\uff0c\u5bb9\u6613\u51fa\u73b0\u504f\u5dee\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86HiPlan\uff0c\u8fd9\u662f\u4e00\u4e2a\u5206\u5c42\u89c4\u5212\u6846\u67b6\uff0c\u63d0\u4f9b\u81ea\u9002\u5e94\u7684\u5168\u5c40-\u5c40\u90e8\u6307\u5bfc\uff0c\u4ee5\u63d0\u9ad8\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u7684\u51b3\u7b56\u80fd\u529b\u3002HiPlan\u5c06\u590d\u6742\u7684\u4efb\u52a1\u5206\u89e3\u4e3a\u91cc\u7a0b\u7891\u5f0f\u7684\u884c\u52a8\u6307\u5357\uff0c\u4ee5\u83b7\u5f97\u603b\u4f53\u65b9\u5411\uff0c\u5e76\u9010\u6b65\u63d0\u793a\u8be6\u7ec6\u7684\u884c\u52a8\u3002", "result": "HiPlan\u5927\u5e45\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf", "conclusion": "HiPlan\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5927\u5e45\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\uff0c\u5e76\u4e14\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u5206\u5c42\u7ec4\u4ef6\u7684\u4e92\u8865\u4f18\u52bf\u3002"}}
