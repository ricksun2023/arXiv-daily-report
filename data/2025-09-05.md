<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Speech-Based Cognitive Screening: A Systematic Evaluation of LLM Adaptation Strategies](https://arxiv.org/abs/2509.03525)
*Fatemeh Taherinezhad,Mohamad Javad Momeni Nezhad,Sepehr Karimi,Sina Rashidi,Ali Zolnour,Maryam Dadkhah,Yasaman Haghbin,Hossein AzadMaleki,Maryam Zolnoori*

Main category: cs.CL

TL;DR: 本研究探索了基于语音的痴呆症筛查方法，利用大型语言模型，通过多种适应策略，在DementiaBank语音语料库上进行实验，旨在提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 美国超过一半的阿尔茨海默病和相关痴呆症患者未被诊断，而基于语音的筛查提供了一种可扩展的检测方法。

Method: 研究比较了九个纯文本模型和三个多模态音频-文本模型，采用了上下文学习、推理增强提示、参数高效微调和多模态集成等适应策略。

Result: 研究结果表明，类中心演示实现了最高的上下文学习性能，推理改进了较小的模型，token级微调通常产生最佳分数。添加分类头显着改善了表现不佳的模型。多模态模型表现良好，但未超过最佳纯文本模型。

Conclusion: 模型适应策略，包括演示选择、推理设计和调整方法，对基于语音的痴呆症检测至关重要，并且适当调整的开放权重模型可以匹配或超过商业系统。

Abstract: Over half of US adults with Alzheimer disease and related dementias remain
undiagnosed, and speech-based screening offers a scalable detection approach.
We compared large language model adaptation strategies for dementia detection
using the DementiaBank speech corpus, evaluating nine text-only models and
three multimodal audio-text models on recordings from DementiaBank speech
corpus. Adaptations included in-context learning with different demonstration
selection policies, reasoning-augmented prompting, parameter-efficient
fine-tuning, and multimodal integration. Results showed that class-centroid
demonstrations achieved the highest in-context learning performance, reasoning
improved smaller models, and token-level fine-tuning generally produced the
best scores. Adding a classification head substantially improved
underperforming models. Among multimodal models, fine-tuned audio-text systems
performed well but did not surpass the top text-only models. These findings
highlight that model adaptation strategies, including demonstration selection,
reasoning design, and tuning method, critically influence speech-based dementia
detection, and that properly adapted open-weight models can match or exceed
commercial systems.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [2] [Towards Efficient General Feature Prediction in Masked Skeleton Modeling](https://arxiv.org/abs/2509.03609)
*Shengkai Sun,Zefan Zhang,Jianfeng Dong,Zhiyong Cheng,Xiaojun Chang,Meng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的通用特征预测框架（GFP），用于高效的掩蔽骨骼建模。


<details>
  <summary>Details</summary>
Motivation: 现有方法将重建目标限制为原始关节坐标或其简单变体，导致计算冗余和有限的语义表示。

Method: 用高级特征预测代替传统的低级重建，引入一个协同学习框架，该框架动态生成跨时空层次的多样化监督信号，并结合约束优化以确保特征多样性。

Result: 在NTU RGB+D 60、NTU RGB+D 120和PKU-MMD数据集上的实验表明，该方法具有计算效率（比标准的掩蔽骨骼建模方法快6.2倍）和卓越的表示质量，在各种下游任务中实现了最先进的性能。

Conclusion: 本文提出的通用特征预测框架能够提高计算效率和表示质量，并在骨骼动作识别任务中取得了优异的性能。

Abstract: Recent advances in the masked autoencoder (MAE) paradigm have significantly
propelled self-supervised skeleton-based action recognition. However, most
existing approaches limit reconstruction targets to raw joint coordinates or
their simple variants, resulting in computational redundancy and limited
semantic representation. To address this, we propose a novel General Feature
Prediction framework (GFP) for efficient mask skeleton modeling. Our key
innovation is replacing conventional low-level reconstruction with high-level
feature prediction that spans from local motion patterns to global semantic
representations. Specifically, we introduce a collaborative learning framework
where a lightweight target generation network dynamically produces diversified
supervision signals across spatial-temporal hierarchies, avoiding reliance on
pre-computed offline features. The framework incorporates constrained
optimization to ensure feature diversity while preventing model collapse.
Experiments on NTU RGB+D 60, NTU RGB+D 120 and PKU-MMD demonstrate the benefits
of our approach: Computational efficiency (with 6.2$\times$ faster training
than standard masked skeleton modeling methods) and superior representation
quality, achieving state-of-the-art performance in various downstream tasks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [3] [PG-Agent: An Agent Powered by Page Graph](https://arxiv.org/abs/2509.03536)
*Weizhi Chen,Ziwei Wang,Leyang Yang,Sheng Zhou,Xiaoxuan Tang,Jiajun Bu,Yong Li,Wei Jiang*

Main category: cs.AI

TL;DR: 现有的GUI Agent无法捕捉页面之间的复杂关系，泛化能力差。


<details>
  <summary>Details</summary>
Motivation: 现有的GUI Agent无法捕捉页面之间的复杂关系，泛化能力差。

Method: 将顺序 episodes 转换成 page graphs，利用 RAG 技术检索 GUI 的可靠认知指南，并提出一个定制的多代理框架 PG-Agent。

Result: 在各种基准测试中，PG-Agent 表现出有效性。

Conclusion: PG-Agent 即使在页面图构建的 episodes 有限的情况下，也能很好地泛化到未见过的场景。

Abstract: Graphical User Interface (GUI) agents possess significant commercial and
social value, and GUI agents powered by advanced multimodal large language
models (MLLMs) have demonstrated remarkable potential. Currently, existing GUI
agents usually utilize sequential episodes of multi-step operations across
pages as the prior GUI knowledge, which fails to capture the complex transition
relationship between pages, making it challenging for the agents to deeply
perceive the GUI environment and generalize to new scenarios. Therefore, we
design an automated pipeline to transform the sequential episodes into page
graphs, which explicitly model the graph structure of the pages that are
naturally connected by actions. To fully utilize the page graphs, we further
introduce Retrieval-Augmented Generation (RAG) technology to effectively
retrieve reliable perception guidelines of GUI from them, and a tailored
multi-agent framework PG-Agent with task decomposition strategy is proposed to
be injected with the guidelines so that it can generalize to unseen scenarios.
Extensive experiments on various benchmarks demonstrate the effectiveness of
PG-Agent, even with limited episodes for page graph construction.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [4] [ACT: Automated Constraint Targeting for Multi-Objective Recommender Systems](https://arxiv.org/abs/2509.03661)
*Daryl Chang,Yi Wu,Jennifer She,Li Wei,Lukasz Heldt*

Main category: cs.IR

TL;DR: 提出了一种名为自动化约束目标（ACT）的框架，用于在推荐系统中自动调整超参数，以满足次要目标（“护栏”）的最低阈值。


<details>
  <summary>Details</summary>
Motivation: 在推荐系统中，需要在最大化主要目标的同时，确保次要目标满足最低阈值，但正交系统变更使得执行这些护栏具有挑战性，通常需要手动调整超参数。

Method: ACT框架使用离线成对评估来寻找解决方案，并持续重新训练以适应系统和用户行为的变化。

Result: 通过实验证明了ACT框架的有效性，并描述了其在大型生产环境中的部署。

Conclusion: ACT框架能够自动找到满足护栏所需的最小超参数更改集。

Abstract: Recommender systems often must maximize a primary objective while ensuring
secondary ones satisfy minimum thresholds, or "guardrails." This is critical
for maintaining a consistent user experience and platform ecosystem, but
enforcing these guardrails despite orthogonal system changes is challenging and
often requires manual hyperparameter tuning. We introduce the Automated
Constraint Targeting (ACT) framework, which automatically finds the minimal set
of hyperparameter changes needed to satisfy these guardrails. ACT uses an
offline pairwise evaluation on unbiased data to find solutions and continuously
retrains to adapt to system and user behavior changes. We empirically
demonstrate its efficacy and describe its deployment in a large-scale
production environment.

</details>


### [5] [lifeXplore at the Lifelog Search Challenge 2021](https://arxiv.org/abs/2509.03692)
*Andreas Leibetseder,Klaus Schoeffmann*

Main category: cs.IR

TL;DR: 生命记录搜索挑战赛 (LSC) 是一个交互式生命记录数据检索竞赛，与 ACM 国际多媒体检索会议 (ICMR) 同地举办。本文介绍了一个名为 lifeXplore 的检索系统，该系统结合了按时间顺序排列的每日摘要浏览和交互式可组合概念过滤，并通过结合时间查询、高级每日摘要功能以及可用性改进进行了改进。


<details>
  <summary>Details</summary>
Motivation: 为了在生命记录搜索挑战赛中检索生命记录数据。

Method: 结合了按时间顺序排列的每日摘要浏览和交互式可组合概念过滤的 lifeXplore 检索系统。

Result: 该工具通过结合时间查询、高级每日摘要功能以及可用性改进得到了改进。

Conclusion: 提出了改进的 lifeXplore 检索系统。

Abstract: Since its first iteration in 2018, the Lifelog Search Challenge (LSC)
continues to rise in popularity as an interactive lifelog data retrieval
competition, co-located at the ACM International Conference on Multimedia
Retrieval (ICMR). The goal of this annual live event is to search a large
corpus of lifelogging data for specifically announced memories using a
purposefully developed tool within a limited amount of time. As long-standing
participants, we present our improved lifeXplore - a retrieval system combining
chronologic day summary browsing with interactive combinable concept filtering.
Compared to previous versions, the tool is improved by incorporating temporal
queries, advanced day summary features as well as usability improvements.

</details>


### [6] [LLMs for estimating positional bias in logged interaction data](https://arxiv.org/abs/2509.03696)
*Aleksandr V. Petrov,Michael Murtagh,Karthik Nagesh*

Main category: cs.IR

TL;DR: 提出了一种新方法，利用大型语言模型（LLM）来估计位置偏差，以解决推荐和搜索系统中学习排序模型中存在的位置偏差问题。


<details>
  <summary>Details</summary>
Motivation: 用户更有可能点击排名较高的项目，而不管它们的实际相关性。这导致新训练的模型可能继承并加强先前排名模型的偏差，而不是真正提高相关性。准确的倾向估计具有挑战性，尤其是在具有复杂非线性布局的界面中。

Method: 利用大型语言模型（LLM）应用于记录的用户交互数据，用于估计位置偏差。

Result: 使用LLM估计的倾向在分数桶中是稳定的，并揭示了Viator网格布局的行-列效应，而简单的启发式方法忽略了这些效应。使用这些倾向训练的IPS加权重排序器在标准NDCG@10上与生产模型匹配，同时将加权NDCG@10提高了大约2%。

Conclusion: 使用大型语言模型估计位置偏差是可行的，并且可以提高推荐系统的性能。

Abstract: Recommender and search systems commonly rely on Learning To Rank models
trained on logged user interactions to order items by predicted relevance.
However, such interaction data is often subject to position bias, as users are
more likely to click on items that appear higher in the ranking, regardless of
their actual relevance. As a result, newly trained models may inherit and
reinforce the biases of prior ranking models rather than genuinely improving
relevance. A standard approach to mitigate position bias is Inverse Propensity
Scoring (IPS), where the model's loss is weighted by the inverse of a
propensity function, an estimate of the probability that an item at a given
position is examined. However, accurate propensity estimation is challenging,
especially in interfaces with complex non-linear layouts. In this paper, we
propose a novel method for estimating position bias using Large Language Models
(LLMs) applied to logged user interaction data. This approach offers a
cost-effective alternative to online experimentation. Our experiments show that
propensities estimated with our LLM-as-a-judge approach are stable across score
buckets and reveal the row-column effects of Viator's grid layout that simpler
heuristics overlook. An IPS-weighted reranker trained with these propensities
matches the production model on standard NDCG@10 while improving weighted
NDCG@10 by roughly 2%. We will verify these offline gains in forthcoming
live-traffic experiments.

</details>


### [7] [Efficient Item ID Generation for Large-Scale LLM-based Recommendation](https://arxiv.org/abs/2509.03746)
*Anushya Subbiah,Vikram Aggarwal,James Pine,Steffen Rendle,Krishna Sayana,Kun Su*

Main category: cs.IR

TL;DR: 这篇论文提出了一种新的方法，将商品ID作为LLM中的一等公民，从而提高推荐质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM推荐方法难以处理大规模商品目录，通常需要将商品tokenize，这导致了实际应用中的瓶颈。

Method: 该论文提出了一种新的训练和推理修改方法，允许商品使用单token表示，并进行单步解码。

Result: 该方法在Amazon购物数据集上，在推荐质量（召回率和NDCG）方面优于现有技术，同时将推理效率提高了5-14倍。

Conclusion: 这项工作提供了一个不同于其他LLM推荐方法的效率视角，可能激发进一步的研究，并为将ID集成到LLM中开辟新的方向。

Abstract: Integrating product catalogs and user behavior into LLMs can enhance
recommendations with broad world knowledge, but the scale of real-world item
catalogs, often containing millions of discrete item identifiers (Item IDs),
poses a significant challenge. This contrasts with the smaller, tokenized text
vocabularies typically used in LLMs. The predominant view within the LLM-based
recommendation literature is that it is infeasible to treat item ids as a first
class citizen in the LLM and instead some sort of tokenization of an item into
multiple tokens is required. However, this creates a key practical bottleneck
in serving these models for real-time low-latency applications.
  Our paper challenges this predominant practice and integrates item ids as
first class citizens into the LLM. We provide simple, yet highly effective,
novel training and inference modifications that enable single-token
representations of items and single-step decoding. Our method shows
improvements in recommendation quality (Recall and NDCG) over existing
techniques on the Amazon shopping datasets while significantly improving
inference efficiency by 5x-14x. Our work offers an efficiency perspective
distinct from that of other popular approaches within LLM-based recommendation,
potentially inspiring further research and opening up a new direction for
integrating IDs into LLMs. Our code is available here
https://drive.google.com/file/d/1cUMj37rV0Z1bCWMdhQ6i4q4eTRQLURtC

</details>


### [8] [LLM-based Relevance Assessment for Web-Scale Search Evaluation at Pinterest](https://arxiv.org/abs/2509.03764)
*Han Wang,Alex Whitworth,Pak Ming Cheung,Zhenjie Zhang,Krishna Kamath*

Main category: cs.IR

TL;DR: 这篇论文提出了一种使用微调的LLM来自动化相关性评估的方法，以提高评估效率并扩大评估范围。


<details>
  <summary>Details</summary>
Motivation: 人工标注成本高、周期长，限制了其可扩展性。因此，需要一种自动化的相关性评估方法。

Method: 使用微调的LLM来生成判断，并验证LLM生成的判断与人工标注之间的一致性。

Result: LLM可以为实验提供可靠的相关性测量，提高评估效率，扩大查询集，优化抽样设计，并有效评估更大范围的搜索体验。该方法提高了相关性指标的质量，并显著降低了在线实验测量中的最小可检测效应（MDE）。

Conclusion: 使用LLM进行相关性评估是可行的，并且可以提高效率和质量。

Abstract: Relevance evaluation plays a crucial role in personalized search systems to
ensure that search results align with a user's queries and intent. While human
annotation is the traditional method for relevance evaluation, its high cost
and long turnaround time limit its scalability. In this work, we present our
approach at Pinterest Search to automate relevance evaluation for online
experiments using fine-tuned LLMs. We rigorously validate the alignment between
LLM-generated judgments and human annotations, demonstrating that LLMs can
provide reliable relevance measurement for experiments while greatly improving
the evaluation efficiency. Leveraging LLM-based labeling further unlocks the
opportunities to expand the query set, optimize sampling design, and
efficiently assess a wider range of search experiences at scale. This approach
leads to higher-quality relevance metrics and significantly reduces the Minimum
Detectable Effect (MDE) in online experiment measurements.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [The Optimiser Hidden in Plain Sight: Training with the Loss Landscape's Induced Metric](https://arxiv.org/abs/2509.03594)
*Thomas R. Harvey*

Main category: cs.LG

TL;DR: 提出了一类新的神经网络优化器，该优化器利用损失景观嵌入到高维空间时自然产生的黎曼度量。


<details>
  <summary>Details</summary>
Motivation: 通过采用几何视角并使用诱导度量，开发一种新的优化器，并将其与现有方法（SGD、Adam、AdamW 和 Muon）在一系列任务和架构中进行比较。

Method: 利用损失景观嵌入到高维空间时自然产生的黎曼度量。

Result: 这种新的优化器类在低维示例中非常有效，并且在训练神经网络方面比最先进的方法略有改进。有效学习率在曲率较高的区域自动降低，充当平滑形式的梯度裁剪。同样，这些优化器的一种变体也可以被视为诱导有效的预定学习率，并且从我们的几何角度来看，解耦权重衰减是自然的选择。基本方法可用于修改任何现有的预处理方法。

Conclusion: 新优化器的计算复杂度与 Adam 相当。

Abstract: We present a class of novel optimisers for training neural networks that
makes use of the Riemannian metric naturally induced when the loss landscape is
embedded in higher-dimensional space. This is the same metric that underlies
common visualisations of loss landscapes. By taking this geometric perspective
literally and using the induced metric, we develop a new optimiser and compare
it to existing methods, namely: SGD, Adam, AdamW, and Muon, across a range of
tasks and architectures. Empirically, we conclude that this new class of
optimisers is highly effective in low dimensional examples, and provides slight
improvement over state-of-the-art methods for training neural networks. These
new optimisers have theoretically desirable properties. In particular, the
effective learning rate is automatically decreased in regions of high curvature
acting as a smoothed out form of gradient clipping. Similarly, one variant of
these optimisers can also be viewed as inducing an effective scheduled learning
rate and decoupled weight decay is the natural choice from our geometric
perspective. The basic method can be used to modify any existing
preconditioning method. The new optimiser has a computational complexity
comparable to that of Adam.

</details>
