<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 49]
- [cs.CV](#cs.CV) [Total: 42]
- [cs.AI](#cs.AI) [Total: 42]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.LG](#cs.LG) [Total: 44]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Bridging the Semantic Gap: Contrastive Rewards for Multilingual Text-to-SQL](https://arxiv.org/abs/2510.13827)
*Ashish Kattamuri,Ishita Prasad,Meetu Malhotra,Arpita Vats,Rahul Raja,Albert Lie*

Main category: cs.CL

TL;DR: 提出了一种新的框架，该框架结合了多语言对比奖励信号中的组相对策略优化 (GRPO)，以提高跨语言场景中 Text-to-SQL 系统的任务效率和语义准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的 Text-to-SQL 方法仅评估和关注可执行的查询，忽略了语义对齐的挑战，包括查询的语义和执行结果的正确性。从英语转换为其他语言时，即使是执行准确性本身也会显著下降。

Method: 该方法通过结合基于语义相似性的奖励信号，教导模型在 SQL 生成和用户意图之间获得更好的对应关系。在七种语言的 MultiSpider 数据集上，使用 GRPO 微调 LLaMA-3-3B 模型。

Result: 使用 GRPO 微调 LLaMA-3-3B 模型将执行准确率提高到 87.4%（比零样本高 +26 pp），语义准确率提高到 52.29%（+32.86 pp）。在 GRPO 框架中添加对比奖励信号进一步将平均语义准确率提高到 59.14%（+6.85 pp，越南语高达 +10 pp）。

Conclusion: 实验表明，通过对比奖励信号微调的参数高效型 3B LLaMA 模型优于更大的零样本 8B LLaMA 模型，执行准确率提高了 7.43 pp（从 8B 模型的 81.43% 提高到 3B 模型的 88.86%），并且几乎与其语义准确率相匹配（59.14% 对 68.57%）——所有这些都只使用了 3,000 个强化学习训练示例。

Abstract: Current Text-to-SQL methods are evaluated and only focused on executable
queries, overlooking the semantic alignment challenge -- both in terms of the
semantic meaning of the query and the correctness of the execution results.
Even execution accuracy itself shows significant drops when moving from English
to other languages, with an average decline of 6 percentage points across
non-English languages. We address these challenges by presenting a new
framework that combines Group Relative Policy Optimization (GRPO) within a
multilingual contrastive reward signal to enhance both task efficiency and
semantic accuracy in Text-to-SQL systems in cross-lingual scenarios. Our method
teaches models to obtain better correspondence between SQL generation and user
intent by combining a reward signal based on semantic similarity. On the
seven-language MultiSpider dataset, fine-tuning the LLaMA-3-3B model with GRPO
improved the execution accuracy up to 87.4 percent (+26 pp over zero-shot) and
semantic accuracy up to 52.29 percent (+32.86 pp). Adding our contrastive
reward signal in the GRPO framework further improved the average semantic
accuracy to 59.14 percent (+6.85 pp, up to +10 pp for Vietnamese). Our
experiments showcase that a smaller, parameter-efficient 3B LLaMA model
fine-tuned with our contrastive reward signal outperforms a much larger
zero-shot 8B LLaMA model, with an uplift of 7.43 pp in execution accuracy (from
81.43 percent on the 8B model to 88.86 percent on the 3B model), and nearly
matches its semantic accuracy (59.14 percent vs. 68.57 percent) -- all using
just 3,000 reinforcement learning training examples. These results demonstrate
how we can improve the performance of Text-to-SQL systems with contrastive
rewards for directed semantic alignment, without requiring large-scale training
datasets.

</details>


### [2] [BenchPress: A Human-in-the-Loop Annotation System for Rapid Text-to-SQL Benchmark Curation](https://arxiv.org/abs/2510.13853)
*Fabian Wenz,Omar Bouattour,Devin Yang,Justin Choi,Cecil Gregg,Nesime Tatbul,Çağatay Demiralp*

Main category: cs.CL

TL;DR: 这篇论文介绍了一个名为BenchPress的人工参与系统，旨在加速特定领域text-to-SQL基准的创建。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在处理私有企业数据仓库的text-to-SQL任务时效果不佳，并且手动注释SQL日志以创建基准数据集成本高昂。

Method: BenchPress利用检索增强生成（RAG）和LLMs为给定的SQL查询生成多个自然语言描述，然后由人工专家选择、排序或编辑这些草稿。

Result: 实验表明，LLM辅助注释显著减少了创建高质量基准所需的时间和精力，并提高了注释准确性、基准可靠性和模型评估的稳健性。

Conclusion: BenchPress通过简化自定义基准的创建，为研究人员和从业人员提供了一种评估特定领域text-to-SQL模型的方法。BenchPress已在GitHub上公开提供。

Abstract: Large language models (LLMs) have been successfully applied to many tasks,
including text-to-SQL generation. However, much of this work has focused on
publicly available datasets, such as Fiben, Spider, and Bird. Our earlier work
showed that LLMs are much less effective in querying large private enterprise
data warehouses and released Beaver, the first private enterprise text-to-SQL
benchmark. To create Beaver, we leveraged SQL logs, which are often readily
available. However, manually annotating these logs to identify which natural
language questions they answer is a daunting task. Asking database
administrators, who are highly trained experts, to take on additional work to
construct and validate corresponding natural language utterances is not only
challenging but also quite costly. To address this challenge, we introduce
BenchPress, a human-in-the-loop system designed to accelerate the creation of
domain-specific text-to-SQL benchmarks. Given a SQL query, BenchPress uses
retrieval-augmented generation (RAG) and LLMs to propose multiple natural
language descriptions. Human experts then select, rank, or edit these drafts to
ensure accuracy and domain alignment. We evaluated BenchPress on annotated
enterprise SQL logs, demonstrating that LLM-assisted annotation drastically
reduces the time and effort required to create high-quality benchmarks. Our
results show that combining human verification with LLM-generated suggestions
enhances annotation accuracy, benchmark reliability, and model evaluation
robustness. By streamlining the creation of custom benchmarks, BenchPress
offers researchers and practitioners a mechanism for assessing text-to-SQL
models on a given domain-specific workload. BenchPress is freely available via
our public GitHub repository at
https://github.com/fabian-wenz/enterprise-txt2sql and is also accessible on our
website at http://dsg-mcgraw.csail.mit.edu:5000.

</details>


### [3] [From Explainability to Action: A Generative Operational Framework for Integrating XAI in Clinical Mental Health Screening](https://arxiv.org/abs/2510.13828)
*Ratna Kandala,Akshata Kishore Moharir,Divya Arvinda Nayak*

Main category: cs.CL

TL;DR: 本文提出了一种新的系统架构，利用大型语言模型将来自不同 XAI 工具的原始技术输出与临床指南相结合，自动生成人类可读的、有证据支持的临床叙述。


<details>
  <summary>Details</summary>
Motivation: 当前 XAI 技术在产生技术上忠实的输出（如特征重要性分数）方面表现出色，但未能提供临床相关的、可操作的见解，这些见解可供临床医生使用或被患者理解。技术透明性和人类效用之间的脱节是现实世界采用的主要障碍。

Method: 本文提出了一种生成操作框架，该框架利用大型语言模型 (LLM) 作为中央翻译引擎。

Result: 本文通过对框架集成的组件进行系统分析，追溯了从内在模型到生成 XAI 的演变，并展示了该框架如何直接解决关键的操作障碍，包括工作流程集成、偏差缓解和利益相关者特定的沟通。

Conclusion: 本文为推动该领域超越孤立数据点的生成，朝着在临床实践中提供集成的、可操作的和值得信赖的 AI 提供了一个战略路线图。

Abstract: Explainable Artificial Intelligence (XAI) has been presented as the critical
component for unlocking the potential of machine learning in mental health
screening (MHS). However, a persistent lab-to-clinic gap remains. Current XAI
techniques, such as SHAP and LIME, excel at producing technically faithful
outputs such as feature importance scores, but fail to deliver clinically
relevant, actionable insights that can be used by clinicians or understood by
patients. This disconnect between technical transparency and human utility is
the primary barrier to real-world adoption. This paper argues that this gap is
a translation problem and proposes the Generative Operational Framework, a
novel system architecture that leverages Large Language Models (LLMs) as a
central translation engine. This framework is designed to ingest the raw,
technical outputs from diverse XAI tools and synthesize them with clinical
guidelines (via RAG) to automatically generate human-readable, evidence-backed
clinical narratives. To justify our solution, we provide a systematic analysis
of the components it integrates, tracing the evolution from intrinsic models to
generative XAI. We demonstrate how this framework directly addresses key
operational barriers, including workflow integration, bias mitigation, and
stakeholder-specific communication. This paper also provides a strategic
roadmap for moving the field beyond the generation of isolated data points
toward the delivery of integrated, actionable, and trustworthy AI in clinical
practice.

</details>


### [4] [A Linguistics-Aware LLM Watermarking via Syntactic Predictability](https://arxiv.org/abs/2510.13829)
*Shinwoo Park,Hyejin Park,Hyeseon Ahn,Yo-Sub Han*

Main category: cs.CL

TL;DR: 本文介绍了一种名为STELA的新型水印框架，该框架通过利用语言的语言自由度来平衡文本质量和检测鲁棒性，无需访问任何模型 logits 即可实现公开验证。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型 (LLM) 的快速发展，可靠的治理工具变得至关重要。公开可验证的水印对于培养值得信赖的 AI 生态系统尤其重要。目前主要的挑战是在文本质量和检测鲁棒性之间取得平衡。以往的研究依赖于模型输出分布中的信号，但这给公开验证带来了巨大的障碍，因为检测过程需要访问底层模型的 logits。

Method: STELA 框架通过词性 (POS) n-gram 建模的语言不确定性来动态调节信号，在语法约束上下文中减弱信号以保持质量，并在具有更大语言灵活性的上下文中加强信号以提高可检测性。该检测器无需访问任何模型 logits 即可运行，从而方便了公开验证。

Result: 在类型多样的语言（分析型英语、孤立型汉语和粘着型韩语）上进行的大量实验表明，STELA 在检测鲁棒性方面优于先前的方法。

Conclusion: STELA 框架在文本质量和检测鲁棒性之间取得了更好的平衡，并且可以公开验证，为大型语言模型的可靠治理提供了一种有前景的工具。

Abstract: As large language models (LLMs) continue to advance rapidly, reliable
governance tools have become critical. Publicly verifiable watermarking is
particularly essential for fostering a trustworthy AI ecosystem. A central
challenge persists: balancing text quality against detection robustness. Recent
studies have sought to navigate this trade-off by leveraging signals from model
output distributions (e.g., token-level entropy); however, their reliance on
these model-specific signals presents a significant barrier to public
verification, as the detection process requires access to the logits of the
underlying model. We introduce STELA, a novel framework that aligns watermark
strength with the linguistic degrees of freedom inherent in language. STELA
dynamically modulates the signal using part-of-speech (POS) n-gram-modeled
linguistic indeterminacy, weakening it in grammatically constrained contexts to
preserve quality and strengthen it in contexts with greater linguistic
flexibility to enhance detectability. Our detector operates without access to
any model logits, thus facilitating publicly verifiable detection. Through
extensive experiments on typologically diverse languages-analytic English,
isolating Chinese, and agglutinative Korean-we show that STELA surpasses prior
methods in detection robustness. Our code is available at
https://github.com/Shinwoo-Park/stela_watermark.

</details>


### [5] [Users as Annotators: LLM Preference Learning from Comparison Mode](https://arxiv.org/abs/2510.13830)
*Zhongze Cai,Xiaocheng Li*

Main category: cs.CL

TL;DR: 提出了一种利用用户在比较模式下的注释数据来对齐大型语言模型的新方法。


<details>
  <summary>Details</summary>
Motivation: 专业人工标注成本高，但用户标注缺乏质量控制。

Method: 通过生成来自不同模型或同一模型的不同版本的两个响应来推断用户的数据质量，并使用期望最大化算法来估计用户的潜在质量因子，并相应地过滤用户注释数据。

Result: 下游任务表明该方法在捕获用户行为和数据过滤以进行 LLM 对齐方面的有效性。

Conclusion: 该方法可以有效地利用用户注释数据来对齐大型语言模型。

Abstract: Pairwise preference data have played an important role in the alignment of
large language models (LLMs). Each sample of such data consists of a prompt,
two different responses to the prompt, and a binary label indicating which of
the two responses is better. The labels are usually annotated by professional
human annotators. In this paper, we consider an alternative approach to collect
pairwise preference data -- user annotation from comparison mode. With the
increasingly wider adoption of LLMs among the population, users are
contributing more and more of their preference labels through their daily
interactions with the LLMs. The upside of such labels is that users are the
best experts in judging the responses to their own queries/prompts, but the
downside is the lack of quality control in these labels. In this paper, we
consider a new idea of generating two responses from two different models or
two different versions of the same model. The asymmetry allows us to make an
inference of the user's data quality through our proposed user behavior model.
We develop an expectation-maximization algorithm to estimate a latent quality
factor of the user, and filter users' annotation data accordingly. The
downstream task shows the effectiveness of our approach in both capturing the
user behavior and data filtering for LLM alignment.

</details>


### [6] [Informed Routing in LLMs: Smarter Token-Level Computation for Faster Inference](https://arxiv.org/abs/2510.13831)
*Chao Han,Yijuan Liang,Zihao Xuan,Daokuan Wu,Wei Zhang,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 本文提出了一种新的路由范式，称为知情路由，旨在解决大语言模型（LLM）推理成本高的问题。该方法通过预测token的重要性及其可恢复性，实现了灵活的执行或近似策略，从而在显著降低计算量的同时，保持了模型准确性。


<details>
  <summary>Details</summary>
Motivation: 现有token级动态计算分配方法依赖于贪婪路由，导致不可逆的信息损失和次优的token选择。

Method: 本文提出了轻量级特征预测器（LFF），用于在做出路由决策之前评估一个单元的输出，从而预测token的重要性和可恢复性。

Result: 在语言建模和推理任务上的大量实验表明，知情路由在多个稀疏水平上实现了最先进的效率-性能权衡。即使没有最终的LoRA微调，该方法也能与需要完全微调的强基线相匹配或超过，同时减少了50%以上的训练时间。

Conclusion: 知情路由是一种有效的降低LLM推理成本的方法，它通过预测token的重要性和可恢复性，实现了灵活的执行或近似策略，从而在显著降低计算量的同时，保持了模型准确性。

Abstract: The deployment of large language models (LLMs) in real-world applications is
increasingly limited by their high inference cost. While recent advances in
dynamic token-level computation allocation attempt to improve efficiency by
selectively activating model components per token, existing methods rely on
greedy routing--a myopic execute-or-skip mechanism that often leads to
irreversible information loss and suboptimal token selection. This paper
introduces informed routing, a new paradigm that proactively addresses these
issues. The key insight is to assess not only a token's immediate importance
but also its recoverability, i.e., how well its transformation can be
approximated. To this end, we propose the Lightweight Feature Forecaster (LFF),
a small predictive module that estimates a unit's output before routing
decisions are made. This enables a flexible execute-or-approximate policy that
preserves model fidelity while drastically reducing computation. Extensive
experiments on both language modeling and reasoning tasks show that informed
routing achieves state-of-the-art efficiency-performance trade-offs across
multiple sparsity levels. Notably, even without final LoRA fine-tuning, our
method matches or surpasses strong baselines that require full fine-tuning, all
while reducing training time by over 50%. The code is available at:
https://github.com/EIT-NLP/informed-routing

</details>


### [7] [Entropy Meets Importance: A Unified Head Importance-Entropy Score for Stable and Efficient Transformer Pruning](https://arxiv.org/abs/2510.13832)
*Minsik Choi,Hyegang Son,Changhoon Kim,Young Geun Kim*

Main category: cs.CL

TL;DR: 提出了一种新的剪枝标准HIES（Head Importance-Entropy Score），它整合了头部重要性得分和注意力熵，为每个头部贡献提供补充证据。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在NLP任务中表现出色，但其多层和注意力头的结构特性在推理和部署中引入了效率挑战。为了解决这些挑战，最近提出了各种剪枝方法。

Method: 引入了一种新的剪枝标准，HIES（Head Importance-Entropy Score），它整合了头部重要性得分与注意力熵。

Result: 基于HIES的剪枝在模型质量上提高了15.2%，在稳定性上提高了2.04倍。

Conclusion: HIES通过在不牺牲精度或稳定性的前提下实现大幅模型压缩，优于仅使用HIS的方法。

Abstract: Transformer-based models have achieved remarkable performance in NLP tasks.
However, their structural characteristics-multiple layers and attention
heads-introduce efficiency challenges in inference and deployment. To address
these challenges, various pruning methods have recently been proposed. Notably,
gradient-based methods using Head Importance Scores (HIS) have gained traction
for interpretability, efficiency, and ability to identify redundant heads.
However, HIS alone has limitations as it captures only the gradient-driven
contribution, overlooking the diversity of attention patterns. To overcome
these limitations, we introduce a novel pruning criterion, HIES (Head
Importance-Entropy Score), which integrates head importance scores with
attention entropy, providing complementary evidence on per-head contribution.
Empirically, HIES-based pruning yields up to 15.2% improvement in model quality
and 2.04x improvement in stability over HIS-only methods, enabling substantial
model compression without sacrificing either accuracy or stability. Code will
be released upon publication.

</details>


### [8] [ConDABench: Interactive Evaluation of Language Models for Data Analysis](https://arxiv.org/abs/2510.13835)
*Avik Dutta,Priyanshu Gupta,Hosein Hasanbeig,Rahul Pratap Singh,Harshit Nigam,Sumit Gulwani,Arjun Radhakrishna,Gustavo Soares,Ashish Tiwari*

Main category: cs.CL

TL;DR: 介绍了ConDABench，一个用于生成会话数据分析基准并评估外部工具的框架。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM数据分析任务评估基准无法捕捉到实际任务的复杂性或提供交互支持。

Method: 使用多代理工作流程从描述从公共数据集中获得的见解的文章中生成实际基准。

Result: 生成了1,420个ConDA问题，并评估了当前最好的LLM模型。结果表明，虽然新一代模型在解决更多实例方面表现更好，但在需要持续参与的任务中不一定表现更好。

Conclusion: ConDABench可以帮助模型构建者衡量在完成复杂交互任务的协作模型方面的进展。

Abstract: Real-world data analysis tasks often come with under-specified goals and
unclean data. User interaction is necessary to understand and disambiguate a
user's intent, and hence, essential to solving these complex tasks. Existing
benchmarks for evaluating LLMs on data analysis tasks do not capture these
complexities or provide first-class support for interactivity. We introduce
ConDABench, a framework for generating conversational data analysis (ConDA)
benchmarks and evaluating external tools on the generated benchmarks. \bench
consists of (a) a multi-agent workflow for generating realistic benchmarks from
articles describing insights gained from public datasets, (b) 1,420 ConDA
problems generated using this workflow, and (c) an evaluation harness that, for
the first time, makes it possible to systematically evaluate conversational
data analysis tools on the generated ConDA problems. Evaluation of
state-of-the-art LLMs on the benchmarks reveals that while the new generation
of models are better at solving more instances, they are not necessarily better
at solving tasks that require sustained, long-form engagement. ConDABench is an
avenue for model builders to measure progress towards truly collaborative
models that can complete complex interactive tasks.

</details>


### [9] [SIMBA UQ: Similarity-Based Aggregation for Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2510.13836)
*Debarun Bhattacharjya,Balaji Ganesan,Junkyu Lee,Radu Marinescu,Katsiaryna Mirylenka,Michael Glass,Xiao Shou*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型(llm)何时知道它们不知道什么，并探讨了不确定性量化(uq)技术在评估llm生成输出的置信度方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 在可信的ai系统中，不确定性量化(uq)越来越被认为是至关重要的组成部分。黑盒uq方法不需要访问生成llm的内部模型信息，因此具有许多实际优势，例如对系统更改的鲁棒性、对llm选择的适应性、降低的成本和计算上的易处理性。

Method: 本文研究了主要但不一定完全是黑盒的uq技术的有效性，其中生成输出与其他抽样生成之间的一致性被用作对其正确性的置信度的替代。本文提出了一个高级的、非语言的、基于相似性的聚合框架，该框架包含适用于复杂生成任务的广泛的uq方法，并从该框架中引入了使用小型训练集训练置信度估计模型的特定新技术。

Result: 通过对跨越问题回答、摘要和text-to-sql等不同任务的数据集进行的实证研究，证明了本文提出的基于相似性的方法可以产生比基线更好的校准置信度。

Conclusion: 本文表明，基于相似性的方法可以有效地量化大型语言模型在各种任务中的不确定性，并优于基线方法。

Abstract: When does a large language model (LLM) know what it does not know?
Uncertainty quantification (UQ) provides measures of uncertainty, such as an
estimate of the confidence in an LLM's generated output, and is therefore
increasingly recognized as a crucial component of trusted AI systems. Black-box
UQ methods do not require access to internal model information from the
generating LLM and therefore have numerous real-world advantages, such as
robustness to system changes, adaptability to choice of LLM, reduced costs, and
computational tractability. In this paper, we investigate the effectiveness of
UQ techniques that are primarily but not necessarily entirely black-box, where
the consistency between a generated output and other sampled generations is
used as a proxy for confidence in its correctness. We propose a high-level
non-verbalized similarity-based aggregation framework that subsumes a broad
swath of UQ approaches suitable for complex generative tasks, as well as
introduce specific novel techniques from the framework that train confidence
estimation models using small training sets. Through an empirical study with
datasets spanning the diverse tasks of question answering, summarization, and
text-to-SQL, we demonstrate that our proposed similarity-based methods can
yield better calibrated confidences than baselines.

</details>


### [10] [Seeing Hate Differently: Hate Subspace Modeling for Culture-Aware Hate Speech Detection](https://arxiv.org/abs/2510.13837)
*Weibin Cai,Reza Zafarani*

Main category: cs.CL

TL;DR: 本文提出了一种文化感知的框架，用于解决仇恨言论检测中存在的标签偏差和文化差异问题。


<details>
  <summary>Details</summary>
Motivation: 现有的仇恨言论检测方法忽略了一个现实世界的复杂性：训练标签是有偏差的，并且对什么是仇恨的解释因具有不同文化背景的个体而异。

Method: 该方法构建了个人的仇恨子空间，通过对文化属性的组合进行建模来缓解数据稀疏性，并使用标签传播来捕获每种组合的独特特征。

Result: 实验表明，该方法在所有指标上的性能平均优于现有技术1.05%。

Conclusion: 该方法可以提高分类性能。

Abstract: Hate speech detection has been extensively studied, yet existing methods
often overlook a real-world complexity: training labels are biased, and
interpretations of what is considered hate vary across individuals with
different cultural backgrounds. We first analyze these challenges, including
data sparsity, cultural entanglement, and ambiguous labeling. To address them,
we propose a culture-aware framework that constructs individuals' hate
subspaces. To alleviate data sparsity, we model combinations of cultural
attributes. For cultural entanglement and ambiguous labels, we use label
propagation to capture distinctive features of each combination. Finally,
individual hate subspaces, which in turn can further enhance classification
performance. Experiments show our method outperforms state-of-the-art by 1.05\%
on average across all metrics.

</details>


### [11] [Meronymic Ontology Extraction via Large Language Models](https://arxiv.org/abs/2510.13839)
*Dekai Zhang,Simone Conia,Antonio Rago*

Main category: cs.CL

TL;DR: 本文提出了一种完全自动化的方法，利用大型语言模型（LLM）从原始评论文本中提取产品本体，形式为部分整体关系。


<details>
  <summary>Details</summary>
Motivation: 本体在当今的数字时代已变得至关重要，可以组织大量可用的非结构化文本。在为这些信息提供正式结构方面，本体具有巨大的价值，可以跨各种领域应用，例如，电子商务，无数的产品列表需要适当的产品组织。但是，手动构建这些本体是一个耗时，昂贵且费力的过程。

Method: 我们利用大型语言模型（LLM）的最新进展来开发一种全自动方法，以从原始评论文本中提取产品本体，形式为部分整体关系。

Result: 我们证明，在使用LLM-as-a-judge进行评估时，我们的方法生成的本体超过了现有的基于BERT的基线。

Conclusion: 我们的研究为LLM更广泛地用于（产品或其他）本体提取奠定了基础。

Abstract: Ontologies have become essential in today's digital age as a way of
organising the vast amount of readily available unstructured text. In providing
formal structure to this information, ontologies have immense value and
application across various domains, e.g., e-commerce, where countless product
listings necessitate proper product organisation. However, the manual
construction of these ontologies is a time-consuming, expensive and laborious
process. In this paper, we harness the recent advancements in large language
models (LLMs) to develop a fully-automated method of extracting product
ontologies, in the form of meronymies, from raw review texts. We demonstrate
that the ontologies produced by our method surpass an existing, BERT-based
baseline when evaluating using an LLM-as-a-judge. Our investigation provides
the groundwork for LLMs to be used more generally in (product or otherwise)
ontology extraction.

</details>


### [12] [PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering](https://arxiv.org/abs/2510.14278)
*Md Mahadi Hasan Nahid,Davood Rafiei*

Main category: cs.CL

TL;DR: 本文介绍了一个 Agentic Retrieval System，它利用大型语言模型 (LLM) 以结构化的循环来检索具有高精度和召回率的相关证据。


<details>
  <summary>Details</summary>
Motivation: 多跳问答 (QA) 中，检索起着核心作用，回答复杂问题需要收集多个证据。

Method: 该框架由三个专门的代理组成：问题分析器，选择器和添加器。选择器和添加器之间的迭代交互产生了一组紧凑而全面的支持段落。

Result: 在四个多跳 QA 基准测试（HotpotQA、2WikiMultiHopQA、MuSiQue 和 MultiHopRAG）上的实验表明，该方法始终优于强大的基线。

Conclusion: 该方法实现了更高的检索准确率，同时过滤掉了分散注意力的内容，使下游 QA 模型能够超越全上下文答案的准确率，同时依赖于明显更少的不相关信息。

Abstract: Retrieval plays a central role in multi-hop question answering (QA), where
answering complex questions requires gathering multiple pieces of evidence. We
introduce an Agentic Retrieval System that leverages large language models
(LLMs) in a structured loop to retrieve relevant evidence with high precision
and recall. Our framework consists of three specialized agents: a Question
Analyzer that decomposes a multi-hop question into sub-questions, a Selector
that identifies the most relevant context for each sub-question (focusing on
precision), and an Adder that brings in any missing evidence (focusing on
recall). The iterative interaction between Selector and Adder yields a compact
yet comprehensive set of supporting passages. In particular, it achieves higher
retrieval accuracy while filtering out distracting content, enabling downstream
QA models to surpass full-context answer accuracy while relying on
significantly less irrelevant information. Experiments on four multi-hop QA
benchmarks -- HotpotQA, 2WikiMultiHopQA, MuSiQue, and MultiHopRAG --
demonstrates that our approach consistently outperforms strong baselines.

</details>


### [13] [ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking](https://arxiv.org/abs/2510.13842)
*Yutao Wu,Xiao Liu,Yinghui Li,Yifeng Gao,Yifan Ding,Jiale Ding,Xiang Zheng,Xingjun Ma*

Main category: cs.CL

TL;DR: 知识投毒通过将对抗性内容注入知识库，诱使大型语言模型（LLM）产生基于操纵性上下文的受攻击者控制的输出，对检索增强生成（RAG）系统构成严重威胁。


<details>
  <summary>Details</summary>
Motivation: 先前的研究强调了LLM容易受到误导或恶意检索内容的影响。然而，现实世界的 факт 检查场景更具挑战性，因为可信的证据通常在检索池中占主导地位。

Method: 我们提出了 ADMIT (对抗性多重注入技术)，这是一种少样本、语义对齐的投毒攻击，可以在没有访问目标 LLM、检索器或令牌级别控制的情况下，翻转事实检查决策并诱导欺骗性理由。

Result: 广泛的实验表明，ADMIT 在 4 个检索器、11 个 LLM 和 4 个跨域基准测试中有效转移，在 $0.93 \\times 10^{-6}$ 的极低中毒率下，平均攻击成功率 (ASR) 达到 86%，即使在存在强烈的反证据的情况下，也能保持稳健。

Conclusion: 与先前的最先进的攻击相比，ADMIT 在所有设置中将 ASR 提高了 11.2%，暴露了基于 RAG 的事实检查系统中存在的重大漏洞。

Abstract: Knowledge poisoning poses a critical threat to Retrieval-Augmented Generation
(RAG) systems by injecting adversarial content into knowledge bases, tricking
Large Language Models (LLMs) into producing attacker-controlled outputs
grounded in manipulated context. Prior work highlights LLMs' susceptibility to
misleading or malicious retrieved content. However, real-world fact-checking
scenarios are more challenging, as credible evidence typically dominates the
retrieval pool. To investigate this problem, we extend knowledge poisoning to
the fact-checking setting, where retrieved context includes authentic
supporting or refuting evidence. We propose \textbf{ADMIT}
(\textbf{AD}versarial \textbf{M}ulti-\textbf{I}njection \textbf{T}echnique), a
few-shot, semantically aligned poisoning attack that flips fact-checking
decisions and induces deceptive justifications, all without access to the
target LLMs, retrievers, or token-level control. Extensive experiments show
that ADMIT transfers effectively across 4 retrievers, 11 LLMs, and 4
cross-domain benchmarks, achieving an average attack success rate (ASR) of 86\%
at an extremely low poisoning rate of $0.93 \times 10^{-6}$, and remaining
robust even in the presence of strong counter-evidence. Compared with prior
state-of-the-art attacks, ADMIT improves ASR by 11.2\% across all settings,
exposing significant vulnerabilities in real-world RAG-based fact-checking
systems.

</details>


### [14] [Rethinking Schema Linking: A Context-Aware Bidirectional Retrieval Approach for Text-to-SQL](https://arxiv.org/abs/2510.14296)
*Md Mahadi Hasan Nahid,Davood Rafiei,Weiwei Zhang,Yong Zhang*

Main category: cs.CL

TL;DR: 这篇论文提出了一种上下文感知的双向模式检索框架，用于解决Text-to-SQL系统中模式链接的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注改进SQL生成，忽略了相关模式元素的检索，导致幻觉和执行失败。

Method: 该方法结合了两种互补策略：先表检索再列选择，以及先列检索再表选择。此外，还采用了问题分解、关键词提取和关键短语提取等技术。

Result: 在BIRD和Spider等具有挑战性的基准测试中，该方法显著提高了模式召回率，同时减少了误报。使用该方法检索到的模式生成的SQL始终优于全模式基线，并接近oracle性能。

Conclusion: 模式链接是提高Text-to-SQL准确性和效率的有力手段。

Abstract: Schema linking -- the process of aligning natural language questions with
database schema elements -- is a critical yet underexplored component of
Text-to-SQL systems. While recent methods have focused primarily on improving
SQL generation, they often neglect the retrieval of relevant schema elements,
which can lead to hallucinations and execution failures. In this work, we
propose a context-aware bidirectional schema retrieval framework that treats
schema linking as a standalone problem. Our approach combines two complementary
strategies: table-first retrieval followed by column selection, and
column-first retrieval followed by table selection. It is further augmented
with techniques such as question decomposition, keyword extraction, and
keyphrase extraction. Through comprehensive evaluations on challenging
benchmarks such as BIRD and Spider, we demonstrate that our method
significantly improves schema recall while reducing false positives. Moreover,
SQL generation using our retrieved schema consistently outperforms full-schema
baselines and closely approaches oracle performance, all without requiring
query refinement. Notably, our method narrows the performance gap between full
and perfect schema settings by 50\%. Our findings highlight schema linking as a
powerful lever for enhancing Text-to-SQL accuracy and efficiency.

</details>


### [15] [Serialized EHR make for good text representations](https://arxiv.org/abs/2510.13843)
*Zhirong Chou,Quan Qin,Shi Li*

Main category: cs.CL

TL;DR: 提出了一种新的领域对齐基础模型 SerialBEHRT，它通过在结构化的 EHR 序列上进行额外的预训练来扩展 SciBERT。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以协调电子健康记录 (EHR) 的表格和基于事件的性质与自然语言模型的顺序先验。

Method: 通过在结构化的 EHR 序列上进行额外的预训练来扩展 SciBERT。

Result: 在抗生素敏感性预测任务上，SerialBEHRT 取得了优异且一致的性能。

Conclusion: 时间序列化在医疗保健基础模型预训练中非常重要。

Abstract: The emergence of foundation models in healthcare has opened new avenues for
learning generalizable representations from large scale clinical data. Yet,
existing approaches often struggle to reconcile the tabular and event based
nature of Electronic Health Records (EHRs) with the sequential priors of
natural language models. This structural mismatch limits their ability to
capture longitudinal dependencies across patient encounters. We introduce
SerialBEHRT, a domain aligned foundation model that extends SciBERT through
additional pretraining on structured EHR sequences. SerialBEHRT is designed to
encode temporal and contextual relationships among clinical events, thereby
producing richer patient representations. We evaluate its effectiveness on the
task of antibiotic susceptibility prediction, a clinically meaningful problem
in antibiotic stewardship. Through extensive benchmarking against state of the
art EHR representation strategies, we demonstrate that SerialBEHRT achieves
superior and more consistent performance, highlighting the importance of
temporal serialization in foundation model pretraining for healthcare.

</details>


### [16] [PluriHop: Exhaustive, Recall-Sensitive QA over Distractor-Rich Corpora](https://arxiv.org/abs/2510.14377)
*Mykolas Sveistrys,Richard Kunert*

Main category: cs.CL

TL;DR: 这篇论文介绍了一种新的问答任务，称为 pluri-hop 问题，它需要跨多个文档进行聚合，并且对检索的完整性非常敏感。作者提出了 PluriHopWIND 数据集来评估现有的问答系统，并提出了 PluriHopRAG 架构来解决这个问题。


<details>
  <summary>Details</summary>
Motivation: 现有的问答系统在处理需要跨多个文档进行聚合的问题时存在局限性，尤其是在处理重复性高、干扰信息多的语料库时。

Method: 作者提出了 PluriHopRAG 架构，该架构首先将问题分解为文档级别的子问题，然后使用交叉编码器过滤器来丢弃不相关的文档，最后进行代价高昂的 LLM 推理。

Result: 实验结果表明，PluriHopRAG 架构在 F1 值上取得了 18-52% 的相对提升。

Conclusion: PluriHopWIND 数据集揭示了当前问答系统在处理重复性高、干扰信息多的语料库时的局限性。PluriHopRAG 的性能凸显了详尽检索和早期过滤作为 top-k 方法的强大替代方案的价值。

Abstract: Recent advances in large language models (LLMs) and retrieval-augmented
generation (RAG) have enabled progress on question answering (QA) when relevant
evidence is in one (single-hop) or multiple (multi-hop) passages. Yet many
realistic questions about recurring report data - medical records, compliance
filings, maintenance logs - require aggregation across all documents, with no
clear stopping point for retrieval and high sensitivity to even one missed
passage. We term these pluri-hop questions and formalize them by three
criteria: recall sensitivity, exhaustiveness, and exactness. To study this
setting, we introduce PluriHopWIND, a diagnostic multilingual dataset of 48
pluri-hop questions built from 191 real-world wind industry reports in German
and English. We show that PluriHopWIND is 8-40% more repetitive than other
common datasets and thus has higher density of distractor documents, better
reflecting practical challenges of recurring report corpora. We test a
traditional RAG pipeline as well as graph-based and multimodal variants, and
find that none of the tested approaches exceed 40% in statement-wise F1 score.
Motivated by this, we propose PluriHopRAG, a RAG architecture that follows a
"check all documents individually, filter cheaply" approach: it (i) decomposes
queries into document-level subquestions and (ii) uses a cross-encoder filter
to discard irrelevant documents before costly LLM reasoning. We find that
PluriHopRAG achieves relative F1 score improvements of 18-52% depending on base
LLM. Despite its modest size, PluriHopWIND exposes the limitations of current
QA systems on repetitive, distractor-rich corpora. PluriHopRAG's performance
highlights the value of exhaustive retrieval and early filtering as a powerful
alternative to top-k methods.

</details>


### [17] [DynaSpec: Context-aware Dynamic Speculative Sampling for Large-Vocabulary Language Models](https://arxiv.org/abs/2510.13847)
*Jinbin Zhang,Nasib Ullah,Erik Schultheis,Rohit Babbar*

Main category: cs.CL

TL;DR: DynaSpec is a context-dependent dynamic shortlisting mechanism that speeds up drafting and generalizes across diverse tasks in speculative decoding.


<details>
  <summary>Details</summary>
Motivation: Scaling of the LLM vocabulary has pushed the number of tokens to grow substantially, causing a latency bottleneck in the drafter's output head. Contemporary methods restrict the drafter's vocabulary to a fixed subset, which is brittle and suppresses rare or domain-specific tokens.

Method: Introduce lightweight, coarse-grained meta-classifiers that route contexts to a small number of token clusters; the union of the top-k selected clusters forms the drafter's shortlist, while verification retains the full vocabulary and exactness. The meta-classifier finishes its computation earlier than the drafter's hidden state generation by exploiting parallel execution of draft encoding and meta shortlisting on separate streams.

Result: Consistent gains in mean accepted length over fixed-shortlist baselines, while context-dependent selection enables smaller shortlists without degrading acceptance.

Conclusion: DynaSpec is robust, speeds up drafting, and generalizes across diverse tasks.

Abstract: Speculative decoding (a.k.a. speculative sampling) has become a standard way
to accelerate LLM inference: a small drafter proposes multiple tokens and a
large target model verifies them once per speculation length. Recently, scaling
of the LLM vocabulary has pushed the number of tokens to grow substantially.
While verification over the full vocabulary leaves the target model largely
unaffected, the O(|V|d) parameters in the drafter's output head become a
latency bottleneck, slowing the entire pipeline. Contemporary methods (e.g.,
FR-Spec, VocabTrim) restrict the drafter's vocabulary to a fixed subset of the
target model's vocabulary, ranked in descending order of token frequency.
Although this reduces draft-time compute, it is brittle, since: (i) frequency
lists are corpus-dependent and require retuning to generalize, and (ii) static
shortlists suppress rare or domain-specific tokens, lowering the expected
number of tokens per verification step. We propose DynaSpec, a
context-dependent dynamic shortlisting mechanism that is robust, speeds up
drafting, and generalizes across diverse tasks. Concretely, we introduce
lightweight, coarse-grained meta-classifiers that route contexts to a small
number of token clusters; the union of the top-k selected clusters forms the
drafter's shortlist, while verification retains the full vocabulary and
exactness. The meta-classifier finishes its computation earlier than the
drafter's hidden state generation by exploiting parallel execution of draft
encoding and meta shortlisting on separate streams. On standard
speculative-decoding benchmarks, we observe consistent gains in mean accepted
length over fixed-shortlist baselines, while context-dependent selection
enables smaller shortlists without degrading acceptance.

</details>


### [18] [MedTrust-RAG: Evidence Verification and Trust Alignment for Biomedical Question Answering](https://arxiv.org/abs/2510.14400)
*Yingpeng Ning,Yuanyuan Sun,Ling Luo,Yanhua Wang,Yuchen Pan,Hongfei Lin*

Main category: cs.CL

TL;DR: 提出了一种名为MedTrust-Guided Iterative RAG的框架，旨在提高医学问答的事实一致性并减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有的基于RAG的生物医学问答方法由于检索后的噪声和对检索证据的验证不足而存在幻觉问题，从而降低了响应的可靠性。

Method: 该方法引入了三个关键创新：强制执行 citation-aware reasoning，采用迭代检索验证过程，并集成 MedTrust-Align Module (MTAM)。

Result: 在 MedMCQA、MedQA 和 MMLU-Med 上的实验表明，该方法在多种模型架构上始终优于竞争基线，LLaMA3.1-8B-Instruct 的平均准确率提高了 2.7%，Qwen3-8B 的平均准确率提高了 2.4%。

Conclusion: MedTrust-Guided Iterative RAG 框架能够有效提高医学问答的事实一致性并减少幻觉。

Abstract: Biomedical question answering (QA) requires accurate interpretation of
complex medical knowledge. Large language models (LLMs) have shown promising
capabilities in this domain, with retrieval-augmented generation (RAG) systems
enhancing performance by incorporating external medical literature. However,
RAG-based approaches in biomedical QA suffer from hallucinations due to
post-retrieval noise and insufficient verification of retrieved evidence,
undermining response reliability. We propose MedTrust-Guided Iterative RAG, a
framework designed to enhance factual consistency and mitigate hallucinations
in medical QA. Our method introduces three key innovations. First, it enforces
citation-aware reasoning by requiring all generated content to be explicitly
grounded in retrieved medical documents, with structured Negative Knowledge
Assertions used when evidence is insufficient. Second, it employs an iterative
retrieval-verification process, where a verification agent assesses evidence
adequacy and refines queries through Medical Gap Analysis until reliable
information is obtained. Third, it integrates the MedTrust-Align Module (MTAM)
that combines verified positive examples with hallucination-aware negative
samples, leveraging Direct Preference Optimization to reinforce
citation-grounded reasoning while penalizing hallucination-prone response
patterns. Experiments on MedMCQA, MedQA, and MMLU-Med demonstrate that our
approach consistently outperforms competitive baselines across multiple model
architectures, achieving the best average accuracy with gains of 2.7% for
LLaMA3.1-8B-Instruct and 2.4% for Qwen3-8B.

</details>


### [19] [On-device System of Compositional Multi-tasking in Large Language Models](https://arxiv.org/abs/2510.13848)
*Ondrej Bohdal,Konstantinos Theodosiadis,Asterios Mpatziakas,Dimitris Filippidis,Iro Spyrou,Christos Zonios,Anastasios Drosou,Dimosthenis Ioannidis,Kyeng-Hun Lee,Jijoong Moon,Hyeonmok Ko,Mete Ozay,Umberto Michieli*

Main category: cs.CL

TL;DR: 提出了一种专门为涉及总结和翻译的组合多任务场景量身定制的新方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 通常通过参数高效的微调技术（例如 Low-Rank Adapters (LoRA)）来适应各种下游任务。虽然适配器可以组合起来以分别处理多个任务，但在针对同时执行复杂任务（例如从长对话生成翻译摘要）时，标准方法会遇到困难。

Method: 我们的技术包括在组合的摘要和翻译适配器之上添加一个可学习的投影层。这种设计实现了有效的集成，同时通过减少计算开销来保持效率，而其他策略需要大量的重新训练或顺序处理。

Result: 实验结果表明，我们的解决方案在基于云和设备上的实现中都表现良好且速度快，这突出了在需要高速运行和资源限制的实际应用中采用我们的框架的潜在好处。

Conclusion: 我们通过开发能够无缝执行组合任务的 Android 应用程序，证明了我们的方法在设备环境中的实际可行性。

Abstract: Large language models (LLMs) are commonly adapted for diverse downstream
tasks via parameter-efficient fine-tuning techniques such as Low-Rank Adapters
(LoRA). While adapters can be combined to handle multiple tasks separately,
standard approaches struggle when targeting the simultaneous execution of
complex tasks, such as generating a translated summary from a long
conversation. To address this challenge, we propose a novel approach tailored
specifically for compositional multi-tasking scenarios involving summarization
and translation. Our technique involves adding a learnable projection layer on
top of the combined summarization and translation adapters. This design enables
effective integration while maintaining efficiency through reduced
computational overhead compared to alternative strategies requiring extensive
retraining or sequential processing. We demonstrate the practical viability of
our method within an on-device environment by developing an Android app capable
of executing compositional tasks seamlessly. Experimental results indicate our
solution performs well and is fast in both cloud-based and on-device
implementations, highlighting the potential benefits of adopting our framework
in real-world applications demanding high-speed operation alongside resource
constraints.

</details>


### [20] [Language steering in latent space to mitigate unintended code-switching](https://arxiv.org/abs/2510.13849)
*Andrey Goncharov,Nikolai Kondusov,Alexey Zaytsev*

Main category: cs.CL

TL;DR: 提出了潜在空间语言引导方法，用于减轻多语言大型语言模型中的代码切换问题。


<details>
  <summary>Details</summary>
Motivation: 多语言大型语言模型经常出现无意的代码切换，降低了下游任务的可靠性。

Method: 通过在并行翻译上进行PCA来识别语言方向，并沿这些轴引导token嵌入以控制语言身份。

Result: 使用单个主成分实现了95-99％的语言分类准确率，并在Qwen2.5和Llama-3.2模型上将下一个token分布差异降低了高达42％。

Conclusion: 语言身份集中在最后几层，具有接近完美的线性可分性。

Abstract: Multilingual Large Language Models (LLMs) often exhibit unintended
code-switching, reducing reliability in downstream tasks. We propose
latent-space language steering, a lightweight inference-time method that
identifies language directions via PCA on parallel translations and steers
token embeddings along these axes to control language identity. Our approach
mitigates code-switching while preserving semantics with negligible
computational overhead and requires only minimal parallel data for calibration.
Empirically, we achieve 95-99\% language classification accuracy using a single
principal component and reduce next-token distributional divergence by up to
42% across multiple language pairs on Qwen2.5 and Llama-3.2 models. We further
analyze the layer-wise evolution of language representations, revealing that
language identity concentrates in final layers with near-perfect linear
separability.

</details>


### [21] [Revisiting the UID Hypothesis in LLM Reasoning Traces](https://arxiv.org/abs/2510.13850)
*Minju Gwak,Guijin Son,Jaehyung Kim*

Main category: cs.CL

TL;DR: 大型语言模型通常使用逐步的思维链 (CoT) 推理来解决问题，但这些中间步骤经常不忠实或难以解释。本研究受到心理语言学中均匀信息密度 (UID) 假设的启发，该假设认为人类通过保持稳定的信息流来进行交流。本研究引入了基于熵的指标来分析推理轨迹中的信息流。通过三个具有挑战性的数学基准，本研究发现 LLM 中成功的推理在全局上是不均匀的：正确的解决方案的特征在于信息密度不均匀的波动，这与人类的交流模式形成鲜明对比。这一结果挑战了关于机器推理的假设，并为设计可解释和自适应的推理模型提出了新的方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 的中间推理步骤经常不忠实或难以解释

Method: 引入基于熵的指标来分析推理轨迹中的信息流

Result: LLM 中成功的推理在全局上是不均匀的：正确的解决方案的特征在于信息密度不均匀的波动，这与人类的交流模式形成鲜明对比

Conclusion: 这一结果挑战了关于机器推理的假设，并为设计可解释和自适应的推理模型提出了新的方向。

Abstract: Large language models (LLMs) often solve problems using step-by-step
Chain-of-Thought (CoT) reasoning, yet these intermediate steps are frequently
unfaithful or hard to interpret. Inspired by the Uniform Information Density
(UID) hypothesis in psycholinguistics -- which posits that humans communicate
by maintaining a stable flow of information -- we introduce entropy-based
metrics to analyze the information flow within reasoning traces. Surprisingly,
across three challenging mathematical benchmarks, we find that successful
reasoning in LLMs is globally non-uniform: correct solutions are characterized
by uneven swings in information density, in stark contrast to human
communication patterns. This result challenges assumptions about machine
reasoning and suggests new directions for designing interpretable and adaptive
reasoning models.

</details>


### [22] [EvoEdit: Evolving Null-space Alignment for Robust and Efficient Knowledge Editing](https://arxiv.org/abs/2510.13851)
*Sicheng Lyu,Yu Gu,Xinyu Wang,Jerry Huang,Sitao Luan,Yufei Cui,Xiao-Wen Chang,Peng Lu*

Main category: cs.CL

TL;DR: EvoEdit 是一种新的模型编辑策略，通过连续零空间对齐来减少灾难性干扰，从而实现稳定高效的模型编辑。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 需要不断更新以纠正过时或错误的知识。模型编辑已经成为一种引人注目的范例，可以在不增加完全重新训练的计算负担的情况下引入有针对性的修改。然而，在顺序编辑上下文中，现有的方法表现出显着的局限性，并且遭受灾难性干扰。

Method: EvoEdit 通过对每个传入的编辑执行连续零空间对齐，保留原始和先前修改的知识表示，并在保留的知识上保持输出不变性，即使在长编辑序列中也能有效减轻干扰。

Result: 在真实世界的顺序知识编辑基准上的评估表明，EvoEdit 比以前的最先进的定位然后编辑技术实现了更好或相当的性能，速度提高了 3.53 倍。

Conclusion: 这些结果强调了在动态发展的信息环境中开发更具原则性的 LLM 设计方法的必要性，同时提供了一个简单而有效的解决方案，并具有强大的理论保证。

Abstract: Large language models (LLMs) require continual updates to rectify outdated or
erroneous knowledge. Model editing has emerged as a compelling paradigm for
introducing targeted modifications without the computational burden of full
retraining. Existing approaches are mainly based on a locate-then-edit
framework. However, in sequential editing contexts, where multiple updates are
applied over time, they exhibit significant limitations and suffer from
catastrophic interference, i.e., new edits compromise previously integrated
updates and degrade preserved knowledge. To address these challenges, we
introduce EvoEdit, a novel editing strategy that mitigates catastrophic
interference through sequential null-space alignment, enabling stable and
efficient model editing. By performing sequential null-space alignment for each
incoming edit, EvoEdit preserves both original and previously modified
knowledge representations and maintains output invariance on preserved
knowledge even across long edit sequences, effectively mitigating interference.
Evaluations on real-world sequential knowledge-editing benchmarks show that
EvoEdit achieves better or comparable performance than prior state-of-the-art
locate-then-edit techniques, with up to 3.53 times speedup. Overall, these
results underscore the necessity of developing more principled approaches for
designing LLMs in dynamically evolving information settings, while providing a
simple yet effective solution with strong theoretical guarantees.

</details>


### [23] [Intent Clustering with Shared Pseudo-Labels](https://arxiv.org/abs/2510.14640)
*I-Fan Lin,Faegheh Hasibi,Suzan Verberne*

Main category: cs.CL

TL;DR: 提出了一种直观、免训练、免标签的意图聚类方法，该方法使用轻量级和开源的LLM，并做出最小的假设。


<details>
  <summary>Details</summary>
Motivation: 当前许多方法依赖于商业LLM，这些LLM成本高昂且透明度有限。此外，他们的方法通常明确依赖于预先知道集群的数量，而在实际环境中通常并非如此。

Method: 首先要求LLM为每个文本生成伪标签，然后在此伪标签集中为每个文本执行多标签分类。这种方法基于以下假设：属于同一集群的文本将共享更多标签，因此在编码为嵌入时将更接近。

Result: 在四个基准数据集上的评估表明，该方法实现了与最新基线相当甚至更好的结果，同时保持了简单和计算效率。

Conclusion: 该方法可以应用于低资源场景，并且在多个模型和数据集上是稳定的。

Abstract: In this paper, we propose an intuitive, training-free and label-free method
for intent clustering that makes minimal assumptions using lightweight and
open-source LLMs. Many current approaches rely on commercial LLMs, which are
costly, and offer limited transparency. Additionally, their methods often
explicitly depend on knowing the number of clusters in advance, which is often
not the case in realistic settings. To address these challenges, instead of
asking the LLM to match similar text directly, we first ask it to generate
pseudo-labels for each text, and then perform multi-label classification in
this pseudo-label set for each text. This approach is based on the hypothesis
that texts belonging to the same cluster will share more labels, and will
therefore be closer when encoded into embeddings. These pseudo-labels are more
human-readable than direct similarity matches. Our evaluation on four benchmark
sets shows that our approach achieves results comparable to and better than
recent baselines, while remaining simple and computationally efficient. Our
findings indicate that our method can be applied in low-resource scenarios and
is stable across multiple models and datasets.

</details>


### [24] [ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When Responding to Different Demographic Groups](https://arxiv.org/abs/2510.13852)
*Peter Banyas,Shristi Sharma,Alistair Simmons,Atharva Vispute*

Main category: cs.CL

TL;DR: ConsistencyAI是一个用于衡量大型语言模型（LLM）对不同角色在事实一致性上的独立基准。它测试当不同人口统计特征的用户提出相同问题时，模型是否会给出事实不一致的答案。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在面对不同用户群体时，是否会产生事实不一致的回答，从而揭示模型可能存在的偏见或不稳定性。

Method: 通过向19个大型语言模型提问，问题涉及15个主题，并为每个问题添加100个不同角色的背景信息。然后，将模型回答转换为句子嵌入，计算跨角色的余弦相似度，并计算加权平均值以得出事实一致性得分。

Result: 实验结果显示，模型的事实一致性得分范围为0.9065至0.7896，平均值为0.8656。Grok-3的一致性最高，而一些轻量级模型的一致性最低。一致性因主题而异，就业市场的一致性最低，G7世界领导人的一致性最高。疫苗或以巴冲突等问题因提供者而异。

Conclusion: 结果表明，提供者和主题都会影响事实一致性。该研究发布了代码和交互式演示，以支持可重复的评估，并鼓励与角色无关的提示策略。

Abstract: Is an LLM telling you different facts than it's telling me? This paper
introduces ConsistencyAI, an independent benchmark for measuring the factual
consistency of large language models (LLMs) for different personas.
ConsistencyAI tests whether, when users of different demographics ask identical
questions, the model responds with factually inconsistent answers. Designed
without involvement from LLM providers, this benchmark offers impartial
evaluation and accountability. In our experiment, we queried 19 LLMs with
prompts that requested 5 facts for each of 15 topics. We repeated this query
100 times for each LLM, each time adding prompt context from a different
persona selected from a subset of personas modeling the general population. We
processed the responses into sentence embeddings, computed cross-persona cosine
similarity, and computed the weighted average of cross-persona cosine
similarity to calculate factual consistency scores. In 100-persona experiments,
scores ranged from 0.9065 to 0.7896, and the mean was 0.8656, which we adopt as
a benchmark threshold. xAI's Grok-3 is most consistent, while several
lightweight models rank lowest. Consistency varies by topic: the job market is
least consistent, G7 world leaders most consistent, and issues like vaccines or
the Israeli-Palestinian conflict diverge by provider. These results show that
both the provider and the topic shape the factual consistency. We release our
code and interactive demo to support reproducible evaluation and encourage
persona-invariant prompting strategies.

</details>


### [25] [An Efficient Rubric-based Generative Verifier for Search-Augmented LLMs](https://arxiv.org/abs/2510.14660)
*Linyue Ma,Yilong Xu,Xiang Long,Zhi Zheng*

Main category: cs.CL

TL;DR: 本文提出了一种新的可验证范式，称为“nugget-as-rubric”，用于搜索增强型大型语言模型（LLM）的奖励建模。


<details>
  <summary>Details</summary>
Motivation: 现有的搜索增强型LLM的奖励建模存在一些局限性。基于规则的奖励虽然可验证，但容易受到表达变化的影响，并且不能应用于长格式工作负载。生成式奖励提高了鲁棒性，但为动态语料库中的长格式工作负载设计可验证和稳定的奖励仍然具有挑战性，并且计算成本很高。

Method: 该范式将原子信息点视为不同搜索增强工作负载的结构化评估标准。短格式任务对应于单个rubric，而长格式任务扩展到与问题的信息需求相一致的多个rubric。为了支持长格式设置，设计了一种基于查询重写的自动rubric构建流程，该流程可以自动检索与每个问题相关的段落，并从中提取rubric，包括来自静态语料库和来自动态在线Web内容。

Result: 实验结果表明，Search-Gen-V在不同的工作负载中实现了强大的验证准确性，使其成为搜索增强型LLM的可扩展、鲁棒且高效的可验证奖励构造器。

Conclusion: 本文提出的“nugget-as-rubric”范式和Search-Gen-V验证器为搜索增强型LLM的奖励建模提供了一种新的有效方法。

Abstract: Search augmentation empowers Large Language Models with retrieval
capabilities to overcome the limitations imposed by static parameters.
Recently, Reinforcement Learning leverages tailored reward signals as a viable
technique to enhance LLMs performing tasks involving search. However, existing
reward modeling for search-augmented LLMs faces several limitations. Rule-based
rewards, such as Exact Match, are verifiable but fragile to variations in
expression and cannot be applied to long-form workloads. In contrast,
generative rewards improve robustness, but designing verifiable and stable
rewards for long-form workloads in dynamic corpora remains challenging and also
incurs high computational costs. In this paper, we propose a unified and
verifiable paradigm, "nugget-as-rubric", which treats atomic information points
as structured evaluation criteria for different search-augmentation workloads.
Short-form tasks correspond to a single rubric, whereas long-form tasks expand
to multiple rubrics aligned with the question's information needs. To support
long-form settings, we design an automatic rubric construction pipeline based
on query rewriting, which can automatically retrieve passages relevant to each
question and extract rubrics from them, both from static corpora and from
dynamic online web content. Furthermore, we introduce \textbf{Search-Gen-V}, a
4B-parameter efficient generative verifier under our proposed verifiable
paradigm, which is trained via the idea of distillation and a two-stage
strategy. Experimental results show that Search-Gen-V achieves strong
verification accuracy across different workloads, making it a scalable, robust,
and efficient verifiable reward constructor for search-augmented LLMs.

</details>


### [26] [R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging](https://arxiv.org/abs/2510.13854)
*Mamadou K. Keita,Christopher Homan,Sebastien Diarra*

Main category: cs.CL

TL;DR: 提出了一种混合方法，将语言规则的多层系统直接集成到神经网络的训练目标中。


<details>
  <summary>Details</summary>
Motivation: 模型通常需要大量的标记数据，而人工标注成本高昂。

Method: R2T 框架，自适应损失函数，包含一个正则化项，该正则化项教导模型以有原则的不确定性来处理词汇表外 (OOV) 单词。

Result: 在 Zarma 词性 (POS) 标记上的实验表明，仅在未标记文本上训练的 R2T-BiLSTM 模型实现了 98.2% 的准确率，优于在 300 个标记句子上微调的 AfriBERTa 等基线。对于更复杂的任务（如命名实体识别 (NER)），R2T 作为一个强大的预训练步骤。

Conclusion: R2T 是一种很有前途的方法，可以利用语言规则来提高模型在各种 NLP 任务中的性能，即使在标记数据有限的情况下也是如此。

Abstract: We introduce the Rule-to-Tag (R2T) framework, a hybrid approach that
integrates a multi-tiered system of linguistic rules directly into a neural
network's training objective. R2T's novelty lies in its adaptive loss function,
which includes a regularization term that teaches the model to handle
out-of-vocabulary (OOV) words with principled uncertainty. We frame this work
as a case study in a paradigm we call principled learning (PrL), where models
are trained with explicit task constraints rather than on labeled examples
alone. Our experiments on Zarma part-of-speech (POS) tagging show that the
R2T-BiLSTM model, trained only on unlabeled text, achieves 98.2% accuracy,
outperforming baselines like AfriBERTa fine-tuned on 300 labeled sentences. We
further show that for more complex tasks like named entity recognition (NER),
R2T serves as a powerful pre-training step; a model pre-trained with R2T and
fine-tuned on just 50 labeled sentences outperformes a baseline trained on 300.

</details>


### [27] [Supervised Fine-Tuning or Contrastive Learning? Towards Better Multimodal LLM Reranking](https://arxiv.org/abs/2510.14824)
*Ziqi Dai,Xin Zhang,Mingxin Li,Yanzhao Zhang,Dingkun Long,Pengjun Xie,Meishan Zhang,Wenjie Li,Min Zhang*

Main category: cs.CL

TL;DR: 本文对比了对比学习 (CL) 和监督微调 (SFT) 两种目标函数在基于大型语言模型 (LLM) 的重排序任务中的效果，发现 SFT 更适合 LLM 重排序。


<details>
  <summary>Details</summary>
Motivation: 以往研究表明，对比学习 (CL) 在 BERT 风格的编码器上比判别式 (分类) 学习更有效。但对于大型语言模型 (LLM)，通过监督微调 (SFT) 进行分类似乎更有希望，因为它与 LLM 的生成性质非常吻合。这引发了一个核心问题：哪种目标函数更适合基于 LLM 的重排序，以及差异的根本原因是什么？

Method: 本文对 CL 和 SFT 进行了全面的比较和分析，以通用多模态检索 (UMR) 作为实验平台。将目标分解为两个组成部分：权重（控制更新幅度）和方向（指导模型更新），然后提出了一个统一的框架来理解它们的相互作用。通过探测实验，发现 SFT 提供了比 CL 更强的加权方案，而首选的评分方向没有明显的赢家。

Result: 实验结果表明，SFT 在 LLM 重排序方面具有一致的优势。通过使用 SFT 进行大规模训练，在 MRB 基准上提出了新的最先进的重排序器。

Conclusion: 本文的研究结果表明，SFT 比 CL 更适合 LLM 重排序，并期望这些发现能够有益于该领域未来的研究和应用。

Abstract: In information retrieval, training reranking models mainly focuses on two
types of objectives: metric learning (e.g. contrastive loss to increase the
predicted scores on relevant query-document pairs) and classification (binary
label prediction of relevance vs. irrelevance). For BERT-style encoders,
various studies have shown that contrastive learning (CL) can be more effective
than discriminative (classification) learning. However, for large language
models (LLMs), classification via supervised fine-tuning (SFT), which predicts
''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears
more promising as it aligns well with the generative nature of LLMs. This
divergence raises a central question: which objective is intrinsically better
suited to LLM-based reranking, and what mechanism underlies the difference? In
this work, we conduct a comprehensive comparison and analysis between CL and
SFT for reranking, taking the universal multimodal retrieval (UMR) as the
experimental playground. We first decompose the objectives into two components:
weight, which controls the magnitude of those updates, and direction, which
guides the model updates, then present a unified framework for understanding
their interactions. Through probing experiments, we find that SFT provides a
substantially stronger weighting scheme than CL, whereas the preferred scoring
direction shows no clear winner. Taken together, these results point to a
consistent advantage of SFT over CL for LLM reranking. To further validate our
findings, we conduct large-scale training with SFT and present new
state-of-the-art rerankers on the MRB benchmark. We also provide ablations on
SFT settings and expect our findings to benefit future research and
applications in this area.

</details>


### [28] [Harnessing Consistency for Robust Test-Time LLM Ensemble](https://arxiv.org/abs/2510.13855)
*Zhichen Zeng,Qi Yu,Xiao Lin,Ruizhong Qiu,Xuying Ning,Tianxin Wei,Yuchen Yan,Jingrui He,Hanghang Tong*

Main category: cs.CL

TL;DR: 论文提出了一种名为CoRE的即插即用技术，利用模型一致性来实现稳健的LLM集成，从而提高集成性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型集成方法在面对潜在错误信号时鲁棒性不足，这些错误信号通常来自不同的分词方案和模型专业知识的差异。

Method: 该方法在token层面通过低通滤波器降低不一致token的权重，在模型层面通过提高模型自信度和减少模型间差异来增强鲁棒性。

Result: 在不同的基准测试、模型组合和集成策略上的大量实验表明，CoRE能够持续提高集成性能和鲁棒性。

Conclusion: CoRE是一种有效的提高LLM集成鲁棒性的方法。

Abstract: Different large language models (LLMs) exhibit diverse strengths and
weaknesses, and LLM ensemble serves as a promising approach to integrate their
complementary capabilities. Despite substantial progress in improving ensemble
quality, limited attention has been paid to the robustness of ensembles against
potential erroneous signals, which often arise from heterogeneous tokenization
schemes and varying model expertise. Our analysis shows that ensemble failures
typically arise from both the token level and the model level: the former
reflects severe disagreement in token predictions, while the latter involves
low confidence and pronounced disparities among models. In light of this, we
propose CoRE, a plug-and-play technique that harnesses model consistency for
robust LLM ensemble, which can be seamlessly integrated with diverse ensemble
methods. Token-level consistency captures fine-grained disagreements by
applying a low-pass filter to downweight uncertain tokens with high
inconsistency, often due to token misalignment, thereby improving robustness at
a granular level. Model-level consistency models global agreement by promoting
model outputs with high self-confidence and minimal divergence from others,
enhancing robustness at a coarser level. Extensive experiments across diverse
benchmarks, model combinations, and ensemble strategies demonstrate that CoRE
consistently improves ensemble performance and robustness.

</details>


### [29] [Multimodal Retrieval-Augmented Generation with Large Language Models for Medical VQA](https://arxiv.org/abs/2510.13856)
*A H M Rezaul Karim,Ozlem Uzuner*

Main category: cs.CL

TL;DR: MasonNLP系统在MEDIQA-WV 2025的wound-care VQA任务中排名第三，该系统使用通用领域的指令调整大型语言模型和检索增强生成（RAG）框架，结合领域内文本和视觉示例，在推理时通过简单索引和融合添加少量相关示例，无需额外训练或复杂重排序，为多模态临床NLP任务提供了一个简单有效的基线。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在解决医疗视觉问答(MedVQA)问题，以支持临床决策和患者护理，特别是针对wound-care VQA任务。

Method: MasonNLP系统采用检索增强生成（RAG）框架，利用通用领域的指令调整大型语言模型，并结合领域内文本和视觉示例，通过简单索引和融合添加少量相关示例。

Result: MasonNLP系统在MEDIQA-WV 2025 shared task中排名第三，平均得分41.37%。

Conclusion: 轻量级RAG与通用LLM相结合，为多模态临床NLP任务提供了一个简单有效的基线，无需额外的训练或复杂的重排序。

Abstract: Medical Visual Question Answering (MedVQA) enables natural language queries
over medical images to support clinical decision-making and patient care. The
MEDIQA-WV 2025 shared task addressed wound-care VQA, requiring systems to
generate free-text responses and structured wound attributes from images and
patient queries. We present the MasonNLP system, which employs a
general-domain, instruction-tuned large language model with a
retrieval-augmented generation (RAG) framework that incorporates textual and
visual examples from in-domain data. This approach grounds outputs in
clinically relevant exemplars, improving reasoning, schema adherence, and
response quality across dBLEU, ROUGE, BERTScore, and LLM-based metrics. Our
best-performing system ranked 3rd among 19 teams and 51 submissions with an
average score of 41.37%, demonstrating that lightweight RAG with
general-purpose LLMs -- a minimal inference-time layer that adds a few relevant
exemplars via simple indexing and fusion, with no extra training or complex
re-ranking -- provides a simple and effective baseline for multimodal clinical
NLP tasks.

</details>


### [30] [ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP Architecture and Paired Weight Sharing](https://arxiv.org/abs/2510.13860)
*Shivanshu Kumar,Gopalakrishnan Srinivasan*

Main category: cs.CL

TL;DR: 提出了一个名为 ShishuLM 的高效语言模型架构，该架构减少了参数数量和 Key-Value (KV) 缓存需求。


<details>
  <summary>Details</summary>
Motivation: Transformer 模型在自然语言处理任务中表现出色，但存在大量内存和计算开销。最近的研究表明，这些模型中存在显著的架构冗余，为优化提供了机会。

Method: 通过分析 AI 可解释性和推理时层剪枝的研究，并结合归一化和注意力计算与输入大致呈线性的特性，使用多层感知器 (MLP) 来近似整个 Transformer 块。

Result: ShishuLM 相比于父模型，在训练和推理过程中，内存需求减少了 25%，延迟提高了 40%。

Conclusion: 实验和分析结果为从预训练的角度构建更高效的 SLM 架构提供了见解。

Abstract: While the transformer architecture has achieved state-of-the-art performance
on natural language processing tasks, these models impose substantial memory
and computational overhead. Recent research has identified significant
architectural redundancies within these models, presenting opportunities for
optimization without compromising performance. Taking insights from research in
AI interpretability and inference-time layer pruning, we introduce an efficient
language model architecture, referred to as ShishuLM, which reduces both the
parameter count and Key-Value (KV) cache requirements. Given the increasing
importance of Small Language Models (SLMs) in agentic AI systems, we evaluate
our approach on two SLMs of different scales. Our analysis reveals that for
moderate-context scenarios, normalization coupled with attention computation is
roughly linear with the input, enabling entire transformer blocks to be
approximated through Multi-Layer Perceptrons (MLPs). Our results show that
ShishuLM provides up to 25% reduction in memory requirements and up to 40%
improvement in latency during both training and inference, compared to parent
models. Our experimental and analytical findings provide insights towards
building more efficient SLM architectures from a pre-training standpoint.

</details>


### [31] [Ensembling Large Language Models to Characterize Affective Dynamics in Student-AI Tutor Dialogues](https://arxiv.org/abs/2510.13862)
*Chenyu Zhang,Sharifa Alghowinem,Cynthia Breazeal*

Main category: cs.CL

TL;DR: 该研究调查了大型语言模型 (LLM) 辅导中学生的情感动态。


<details>
  <summary>Details</summary>
Motivation: 目前对 LLM 介导的辅导中情感动态的理解不足。这项工作旨在通过关注学习者不断变化的情感状态，推进生成式人工智能融入教育的负责任路径。

Method: 使用集成 LLM 框架，分析了 PyTutor（一个 LLM 驱动的 AI 导师）与 261 名本科生之间的 16,986 个对话回合。使用三个前沿 LLM（Gemini、GPT-4o、Claude）生成零样本情感注释，包括效价、唤醒和学习帮助性的标量评级，以及自由文本情感标签。通过等级加权模型内池化和跨模型的多数共识融合这些估计，以产生稳健的情感概况。

Result: 学生在与 AI 导师互动时，通常表现出轻微的积极情感和适度的唤醒。学习并非一帆风顺，困惑和好奇经常伴随着问题解决，而沮丧虽然较少见，但仍然会以可能阻碍进步的方式出现。情感状态是短暂的，积极的时刻比中性或消极的时刻持续时间稍长，但它们是脆弱的并且容易被打断。令人鼓舞的是，消极情绪通常会迅速消退，有时会直接反弹到积极状态。中性时刻经常充当转折点，更频繁地引导学生向上而不是向下，这表明导师可以在这些关键时刻进行干预。

Conclusion: 研究表明，在 LLM 辅导中，学生的情感是动态变化的，积极情感脆弱但消极情感可以快速消退，中性时刻是潜在的干预点。

Abstract: While recent studies have examined the leaning impact of large language model
(LLM) in educational contexts, the affective dynamics of LLM-mediated tutoring
remain insufficiently understood. This work introduces the first ensemble-LLM
framework for large-scale affect sensing in tutoring dialogues, advancing the
conversation on responsible pathways for integrating generative AI into
education by attending to learners' evolving affective states. To achieve this,
we analyzed two semesters' worth of 16,986 conversational turns exchanged
between PyTutor, an LLM-powered AI tutor, and 261 undergraduate learners across
three U.S. institutions. To investigate learners' emotional experiences, we
generate zero-shot affect annotations from three frontier LLMs (Gemini, GPT-4o,
Claude), including scalar ratings of valence, arousal, and
learning-helpfulness, along with free-text emotion labels. These estimates are
fused through rank-weighted intra-model pooling and plurality consensus across
models to produce robust emotion profiles. Our analysis shows that during
interaction with the AI tutor, students typically report mildly positive affect
and moderate arousal. Yet learning is not uniformly smooth: confusion and
curiosity are frequent companions to problem solving, and frustration, while
less common, still surfaces in ways that can derail progress. Emotional states
are short-lived--positive moments last slightly longer than neutral or negative
ones, but they are fragile and easily disrupted. Encouragingly, negative
emotions often resolve quickly, sometimes rebounding directly into positive
states. Neutral moments frequently act as turning points, more often steering
students upward than downward, suggesting opportunities for tutors to intervene
at precisely these junctures.

</details>


### [32] [Unlocking the Potential of Diffusion Language Models through Template Infilling](https://arxiv.org/abs/2510.13870)
*Junhoo Lee,Seungyeon Kim,Nojun Kwak*

Main category: cs.CL

TL;DR: 提出了一种新的扩散语言模型（DLM）条件生成方法，称为模板填充（TI），它首先生成目标响应的结构模板，然后填充掩码段。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型（DLM）作为自回归语言模型的有前途的替代方案出现，但它们的推理策略仍然局限于从自回归范式继承的基于前缀的提示。

Method: 首先，生成目标响应的结构模板，然后填充掩码段。为了提高这种结构控制的灵活性，引入了动态段分配（DSA），它可以根据生成置信度自适应地调整段长度。

Result: 在数学推理和代码生成基准测试中，与基线相比，始终提高了 17.01$\%$p。

Conclusion: TI 在多 token 生成设置中提供了额外的优势，可以在保持生成质量的同时实现有效的加速。

Abstract: Diffusion Language Models (DLMs) have emerged as a promising alternative to
Autoregressive Language Models, yet their inference strategies remain limited
to prefix-based prompting inherited from the autoregressive paradigm. In this
paper, we propose Template Infilling (TI), a tailored conditioning methodology
for DLMs' generation process. Unlike conventional prefix prompting, TI first
generates a structural template for the target response, then fills in the
masked segments. To enhance the flexibility of this structural control, we
introduce Dynamic Segment Allocation (DSA), which adaptively adjusts segment
lengths based on generation confidence. We demonstrate the effectiveness of our
approach on mathematical reasoning and code generation benchmarks, achieving
consistent improvements of 17.01$\%$p over baseline. Furthermore, we show that
TI provides additional advantages in multi-token generation settings, enabling
effective speedup while maintaining generation quality.

</details>


### [33] [Quechua Speech Datasets in Common Voice: The Case of Puno Quechua](https://arxiv.org/abs/2510.13871)
*Elwin Huaman,Wendi Huaman,Jorge Luis Huaman,Ninfa Quispe*

Main category: cs.CL

TL;DR: Common Voice is used to create a speech dataset for under-resourced languages like Quechua.


<details>
  <summary>Details</summary>
Motivation: Under-resourced languages lack data and resources, hindering speech technology development.

Method: Integration of Quechua languages into Common Voice, focusing on Puno Quechua as a case study with corpus collection of reading and spontaneous speech data.

Result: Common Voice hosts 191.1 hours of Quechua speech (86% validated), with Puno Quechua contributing 12 hours (77% validated).

Conclusion: The work contributes towards inclusive voice technology and digital empowerment of under-resourced language communities and proposes a research agenda addressing technical challenges and ethical considerations.

Abstract: Under-resourced languages, such as Quechuas, face data and resource scarcity,
hindering their development in speech technology. To address this issue, Common
Voice presents a crucial opportunity to foster an open and community-driven
speech dataset creation. This paper examines the integration of Quechua
languages into Common Voice. We detail the current 17 Quechua languages,
presenting Puno Quechua (ISO 639-3: qxp) as a focused case study that includes
language onboarding and corpus collection of both reading and spontaneous
speech data. Our results demonstrate that Common Voice now hosts 191.1 hours of
Quechua speech (86\% validated), with Puno Quechua contributing 12 hours (77\%
validated), highlighting the Common Voice's potential. We further propose a
research agenda addressing technical challenges, alongside ethical
considerations for community engagement and indigenous data sovereignty. Our
work contributes towards inclusive voice technology and digital empowerment of
under-resourced language communities.

</details>


### [34] [FRACCO: A gold-standard annotated corpus of oncological entities with ICD-O-3.1 normalisation](https://arxiv.org/abs/2510.13873)
*Johann Pignat,Milena Vucetic,Christophe Gaudet-Blavignac,Jamil Zaghir,Amandine Stettler,Fanny Amrein,Jonatan Bonjour,Jean-Philippe Goldman,Olivier Michielin,Christian Lovis,Mina Bjelogrlic*

Main category: cs.CL

TL;DR: FRACCO: A French clinical oncology corpus with ICD-O annotations for morphology, topography, and composite expressions.


<details>
  <summary>Details</summary>
Motivation: Lack of French oncology resources for NLP tool development.

Method: Expert annotation of 1301 synthetic French clinical cases, translated from Spanish, using ICD-O for annotation. Automated matching and manual validation were employed.

Result: A dataset with 71127 ICD-O normalisations, 399 morphology codes, 272 topography codes, and 2043 composite expressions.

Conclusion: FRACCO provides a reference standard for named entity recognition and concept normalisation in French oncology texts.

Abstract: Developing natural language processing tools for clinical text requires
annotated datasets, yet French oncology resources remain scarce. We present
FRACCO (FRench Annotated Corpus for Clinical Oncology) an expert-annotated
corpus of 1301 synthetic French clinical cases, initially translated from the
Spanish CANTEMIST corpus as part of the FRASIMED initiative. Each document is
annotated with terms related to morphology, topography, and histologic
differentiation, using the International Classification of Diseases for
Oncology (ICD-O) as reference. An additional annotation layer captures
composite expression-level normalisations that combine multiple ICD-O elements
into unified clinical concepts. Annotation quality was ensured through expert
review: 1301 texts were manually annotated for entity spans by two domain
experts. A total of 71127 ICD-O normalisations were produced through a
combination of automated matching and manual validation by a team of five
annotators. The final dataset representing 399 unique morphology codes (from
2549 different expressions), 272 topography codes (from 3143 different
expressions), and 2043 unique composite expressions (from 11144 different
expressions). This dataset provides a reference standard for named entity
recognition and concept normalisation in French oncology texts.

</details>


### [35] [What Layers When: Learning to Skip Compute in LLMs with Residual Gates](https://arxiv.org/abs/2510.13876)
*Filipe Laitenberger,Dawid Kopiczko,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CL

TL;DR: GateSkip: a residual-stream gating mechanism for token-wise layer skipping in decoder-only LMs, saving compute while retaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To enable token-wise layer skipping in decoder-only LMs for efficient inference.

Method: Equipping each Attention/MLP branch with a sigmoid-linear gate to condense the branch's output and skip low-importance tokens during inference based on gate values.

Result: Up to 15% compute savings with over 90% accuracy on long-form reasoning; accuracy gains at full compute and matched baseline quality near 50% savings on instruction-tuned models.

Conclusion: Learned gates offer insights into transformer information flow and the method is compatible with quantization, pruning, and self-speculative decoding.

Abstract: We introduce GateSkip, a simple residual-stream gating mechanism that enables
token-wise layer skipping in decoder-only LMs. Each Attention/MLP branch is
equipped with a sigmoid-linear gate that condenses the branch's output before
it re-enters the residual stream. During inference we rank tokens by the gate
values and skip low-importance ones using a per-layer budget. While early-exit
or router-based Mixture-of-Depths models are known to be unstable and need
extensive retraining, our smooth, differentiable gates fine-tune stably on top
of pretrained models. On long-form reasoning, we save up to 15\% compute while
retaining over 90\% of baseline accuracy. On instruction-tuned models we see
accuracy gains at full compute and match baseline quality near 50\% savings.
The learned gates give insight into transformer information flow (e.g., BOS
tokens act as anchors), and the method combines easily with quantization,
pruning, and self-speculative decoding.

</details>


### [36] [TextBandit: Evaluating Probabilistic Reasoning in LLMs Through Language-Only Decision Tasks](https://arxiv.org/abs/2510.13878)
*Jimin Lim,Arjun Damerla,Arthur Jiang,Nam Le*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在不确定性下进行顺序决策的能力，通过纯文本反馈与多臂老虎机环境交互。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注大型语言模型仅使用自然语言在不确定性下进行顺序决策的能力。

Method: 构建了一个新的基准测试，其中大型语言模型通过纯文本反馈与多臂老虎机环境交互，没有数值线索或明确的概率。

Result: Qwen3-4B模型的最佳臂选择率达到了89.2%，显著优于其他大型语言模型和传统方法。

Conclusion: 概率推理能够仅从语言中产生，并将此基准测试作为评估自然、非数字环境中的决策能力的一个步骤。

Abstract: Large language models (LLMs) have shown to be increasingly capable of
performing reasoning tasks, but their ability to make sequential decisions
under uncertainty only using natural language remains underexplored. We
introduce a novel benchmark in which LLMs interact with multi-armed bandit
environments using purely textual feedback, "you earned a token", without
access to numerical cues or explicit probabilities, resulting in the model to
infer latent reward structures purely off linguistic cues and to adapt
accordingly. We evaluated the performance of four open-source LLMs and compare
their performance to standard decision-making algorithms such as Thompson
Sampling, Epsilon Greedy, Upper Confidence Bound (UCB), and random choice.
While most of the LLMs underperformed compared to the baselines, Qwen3-4B,
achieved the best-arm selection rate of 89.2% , which significantly
outperformed both the larger LLMs and traditional methods. Our findings suggest
that probabilistic reasoning is able to emerge from language alone, and we
present this benchmark as a step towards evaluating decision-making
capabilities in naturalistic, non-numeric contexts.

</details>


### [37] [Catch Your Breath: Adaptive Computation for Self-Paced Sequence Production](https://arxiv.org/abs/2510.13879)
*Alexandre Galashov,Matt Jones,Rosemary Ke,Yuan Cao,Vaishnavh Nagarajan,Michael C. Mozer*

Main category: cs.CL

TL;DR: 提出了一种允许语言模型动态和自主地扩展用于每个输入token的计算步骤数量的监督训练目标。


<details>
  <summary>Details</summary>
Motivation: 为了使模型能够明智地使用 <don't know> 输出并校准其不确定性，我们将每个输出token的选择构建为一个具有时间成本的序列决策问题。

Method: 提出了三种方法：CYB-AP, CYB-VA, CYB-DP。

Result: CYB模型只需要baseline模型三分之一的训练数据即可达到相同的性能，并且只需要具有pause和交叉熵损失的模型的二分之一的数据。

Conclusion: CYB模型在提高准确率时会请求额外的步骤，并且该模型会根据token级别的复杂性和上下文调整其处理时间。

Abstract: We explore a class of supervised training objectives that allow a language
model to dynamically and autonomously scale the number of compute steps used
for each input token. For any token, the model can request additional compute
steps by emitting a <don't know> output. If the model is granted a delay, a
specialized <pause> token is inserted at the next input step, providing the
model with additional compute resources to generate an output. The model can
request multiple pauses. To train the model to use <don't know> outputs
judiciously and to calibrate its uncertainty, we frame the selection of each
output token as a sequential-decision problem with a time cost. We refer to the
class of methods as $\textit{Catch Your Breath}$ losses and we study three
methods in this class: CYB-AP frames the model's task as anytime prediction,
where an output may be required at any step and accuracy is discounted over
time; CYB-VA is a variational approach that aims to maximize prediction
accuracy subject to a specified distribution over stopping times; and CYB-DP
imposes a penalty based on a computational budget. Through fine-tuning
experiments, we identify the best performing loss variant. The CYB model needs
only one third as much training data as the baseline (no pause) model needs to
achieve the same performance, and half as much data as a model with pauses and
a cross-entropy loss. We find that the CYB model requests additional steps when
doing so improves accuracy, and the model adapts its processing time to
token-level complexity and context. For example, it often pauses after plural
nouns like $\textit{patients}$ and $\textit{challenges}$ but never pauses after
the first token of contracted words like $\textit{wasn}$ and $\textit{didn}$,
and it shows high variability for ambiguous tokens like $\textit{won}$, which
could function as either a verb or part of a contraction.

</details>


### [38] [PAGE: Prompt Augmentation for text Generation Enhancement](https://arxiv.org/abs/2510.13880)
*Mauro Jose Pacchiotti,Luciana Ballejos,Mariel Ale*

Main category: cs.CL

TL;DR: 提出了一种名为PAGE的框架，通过简单的辅助模块来辅助文本生成模型，提高生成质量和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有的自然语言生成模型在特定任务或要求下表现不佳，需要大量额外数据进行调整。

Method: 利用轻量级模型（如分类器或提取器）从输入文本中进行推断，并使用辅助模块的输出构建丰富的输入，以改进生成效果。

Result: 在需求工程领域进行了概念验证，使用带有分类器的辅助模块来提高软件需求生成的质量。

Conclusion: PAGE框架不需要辅助生成模型，而是提出了一种更简单、模块化的架构，易于适应不同的任务。

Abstract: In recent years, natural language generative models have shown outstanding
performance in text generation tasks. However, when facing specific tasks or
particular requirements, they may exhibit poor performance or require
adjustments that demand large amounts of additional data. This work introduces
PAGE (Prompt Augmentation for text Generation Enhancement), a framework
designed to assist these models through the use of simple auxiliary modules.
These modules, lightweight models such as classifiers or extractors, provide
inferences from the input text. The output of these auxiliaries is then used to
construct an enriched input that improves the quality and controllability of
the generation. Unlike other generation-assistance approaches, PAGE does not
require auxiliary generative models; instead, it proposes a simpler, modular
architecture that is easy to adapt to different tasks. This paper presents the
proposal, its components and architecture, and reports a proof of concept in
the domain of requirements engineering, where an auxiliary module with a
classifier is used to improve the quality of software requirements generation.

</details>


### [39] [Too Open for Opinion? Embracing Open-Endedness in Large Language Models for Social Simulation](https://arxiv.org/abs/2510.13884)
*Bolei Ma,Yong Cao,Indira Sen,Anna-Carolina Haensch,Frauke Kreuter,Barbara Plank,Daniel Hershcovich*

Main category: cs.CL

TL;DR: 开放式生成对于使用 LLM 进行社会模拟至关重要，因为它能改进测量、探索意外观点并减少偏差。


<details>
  <summary>Details</summary>
Motivation: 当前研究为了便于评分和比较，限制了 LLM 模拟为多项选择或简答格式，忽略了 LLM 的生成特性。本文认为开放式生成对于现实的社会模拟至关重要。

Method: 借鉴了数十年的调查方法研究和 NLP 的最新进展。

Result: 开放式生成可以改进测量和设计，支持探索意外观点，并减少研究者施加的指导性偏差。它还可以捕捉表达性和个性，有助于预测试，并最终提高方法论的效用。

Conclusion: 我们需要新的实践和评估框架，利用而不是限制 LLM 的开放式生成多样性，从而在 NLP 和社会科学之间建立协同作用。

Abstract: Large Language Models (LLMs) are increasingly used to simulate public opinion
and other social phenomena. Most current studies constrain these simulations to
multiple-choice or short-answer formats for ease of scoring and comparison, but
such closed designs overlook the inherently generative nature of LLMs. In this
position paper, we argue that open-endedness, using free-form text that
captures topics, viewpoints, and reasoning processes "in" LLMs, is essential
for realistic social simulation. Drawing on decades of survey-methodology
research and recent advances in NLP, we argue why this open-endedness is
valuable in LLM social simulations, showing how it can improve measurement and
design, support exploration of unanticipated views, and reduce
researcher-imposed directive bias. It also captures expressiveness and
individuality, aids in pretesting, and ultimately enhances methodological
utility. We call for novel practices and evaluation frameworks that leverage
rather than constrain the open-ended generative diversity of LLMs, creating
synergies between NLP and social science.

</details>


### [40] [Order from Chaos: Comparative Study of Ten Leading LLMs on Unstructured Data Categorization](https://arxiv.org/abs/2510.13885)
*Ariel Kamen*

Main category: cs.CL

TL;DR: 评估了十个大型语言模型在非结构化文本分类上的性能，发现它们表现一般，但集成方法可以显著提高准确性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估大型语言模型在非结构化文本分类中的性能。

Method: 使用统一的数据集和zero-shot提示，比较了十个大型语言模型在IAB 2.2层级分类上的表现，并使用准确率、精确率、召回率、F1-score、幻觉率、膨胀率和分类成本等指标进行评估。

Result: 结果表明，大型语言模型的经典性能表现一般，但Gemini 1.5/2.0 Flash和GPT 20B/120B在成本效益方面表现较好，GPT 120B的幻觉率最低。集成方法可以显著提高准确性，减少膨胀，并完全消除幻觉。

Conclusion: 研究表明，模型集成比单纯的模型规模扩展更有效，是实现或超越人类专家在大型文本分类中表现的最佳途径。

Abstract: This study presents a comparative evaluation of ten state-of-the-art large
language models (LLMs) applied to unstructured text categorization using the
Interactive Advertising Bureau (IAB) 2.2 hierarchical taxonomy. The analysis
employed a uniform dataset of 8,660 human-annotated samples and identical
zero-shot prompts to ensure methodological consistency across all models.
Evaluation metrics included four classic measures - accuracy, precision,
recall, and F1-score - and three LLM-specific indicators: hallucination ratio,
inflation ratio, and categorization cost.
  Results show that, despite their rapid advancement, contemporary LLMs achieve
only moderate classic performance, with average scores of 34% accuracy, 42%
precision, 45% recall, and 41% F1-score. Hallucination and inflation ratios
reveal that models frequently overproduce categories relative to human
annotators. Among the evaluated systems, Gemini 1.5/2.0 Flash and GPT 20B/120B
offered the most favorable cost-to-performance balance, while GPT 120B
demonstrated the lowest hallucination ratio. The findings suggest that scaling
and architectural improvements alone do not ensure better categorization
accuracy, as the task requires compressing rich unstructured text into a
limited taxonomy - a process that challenges current model architectures.
  To address these limitations, a separate ensemble-based approach was
developed and tested. The ensemble method, in which multiple LLMs act as
independent experts, substantially improved accuracy, reduced inflation, and
completely eliminated hallucinations. These results indicate that coordinated
orchestration of models - rather than sheer scale - may represent the most
effective path toward achieving or surpassing human-expert performance in
large-scale text categorization.

</details>


### [41] [Reliable Fine-Grained Evaluation of Natural Language Math Proofs](https://arxiv.org/abs/2510.13888)
*Wenjie Ma,Andrei Cojocaru,Neel Kolhe,Bradley Louie,Robin Said Sharif,Haihan Zhang,Vincent Zhuang,Matei Zaharia,Sewon Min*

Main category: cs.CL

TL;DR: 这篇论文介绍了一个用于评估大型语言模型生成的数学证明的系统方法和数据集 ProofBench。他们提出了 ProofGrader，一个能够对数学证明进行细粒度评分的评估器。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在数学推理方面取得进展，但生成和验证自然语言数学证明仍然是一个挑战，缺乏可靠的细粒度评估器是关键问题。

Method: 1.  构建专家注释的细粒度证明评分数据集 ProofBench。
2.  系统地探索评估器设计空间，包括骨干模型、输入上下文、指令和评估工作流程。
3.  开发 ProofGrader，结合强大的推理骨干语言模型、来自参考解决方案和评分方案的丰富上下文，以及简单的集成方法。

Result: ProofGrader 在专家评分上的平均绝对误差 (MAE) 为 0.926，明显优于简单的基线。在 best-of-n 选择任务中，当 n=16 时，ProofGrader 的平均得分为 4.14（满分 7 分），缩小了二元评估器 (2.48) 和人类专家 (4.62) 之间 78% 的差距。

Conclusion: ProofGrader 具有推进下游证明生成的潜力。

Abstract: Recent advances in large language models (LLMs) for mathematical reasoning
have largely focused on tasks with easily verifiable final answers; however,
generating and verifying natural language math proofs remains an open
challenge. We identify the absence of a reliable, fine-grained evaluator for
LLM-generated math proofs as a critical gap. To address this, we propose a
systematic methodology for developing and validating evaluators that assign
fine-grained scores on a 0-7 scale to model-generated math proofs. To enable
this study, we introduce ProofBench, the first expert-annotated dataset of
fine-grained proof ratings, spanning 145 problems from six major math
competitions (USAMO, IMO, Putnam, etc) and 435 LLM-generated solutions from
Gemini-2.5-pro, o3, and DeepSeek-R1. %with expert gradings. Using ProofBench as
a testbed, we systematically explore the evaluator design space across key
axes: the backbone model, input context, instructions and evaluation workflow.
Our analysis delivers ProofGrader, an evaluator that combines a strong
reasoning backbone LM, rich context from reference solutions and marking
schemes, and a simple ensembling method; it achieves a low Mean Absolute Error
(MAE) of 0.926 against expert scores, significantly outperforming naive
baselines. Finally, we demonstrate its practical utility in a best-of-$n$
selection task: at $n=16$, ProofGrader achieves an average score of 4.14 (out
of 7), closing 78% of the gap between a naive binary evaluator (2.48) and the
human oracle (4.62), highlighting its potential to advance downstream proof
generation.

</details>


### [42] [A Survey on Collaborating Small and Large Language Models for Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness](https://arxiv.org/abs/2510.13890)
*Fali Wang,Jihai Chen,Shuhua Yang,Ali Al-Lawati,Linli Tang,Hui Liu,Suhang Wang*

Main category: cs.CL

TL;DR: 本文综述了小型语言模型 (SLM) 和大型语言模型 (LLM) 的协作，旨在结合 SLM 的效率和 LLM 的通用性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型微调成本高、推理延迟高、边缘部署受限且可靠性问题突出。小型语言模型虽然紧凑、高效且适应性强，但是无法达到LLM的效果。因此，需要将两者结合起来。

Method: 本文从协作目标出发，对 SLM-LLM 协作进行系统性调研，并提出了一个包含四个目标的分类法：性能增强、成本效益、云边隐私和可信度。在这一框架下，回顾了代表性方法，总结了设计范式，并概述了开放的挑战和未来方向。

Result: 本文提出了一个包含四个目标的分类法：性能增强、成本效益、云边隐私和可信度。

Conclusion: 本文旨在促进高效、安全和可扩展的 SLM-LLM 协作。

Abstract: Large language models (LLMs) have advanced many domains and applications but
face high fine-tuning costs, inference latency, limited edge deployability, and
reliability concerns. Small language models (SLMs), compact, efficient, and
adaptable, offer complementary remedies. Recent work explores collaborative
frameworks that fuse SLMs' specialization and efficiency with LLMs'
generalization and reasoning to meet diverse objectives across tasks and
deployment scenarios. Motivated by these developments, this paper presents a
systematic survey of SLM-LLM collaboration organized by collaboration
objectives. We propose a taxonomy with four goals: performance enhancement,
cost-effectiveness, cloud-edge privacy, and trustworthiness. Within this
framework, we review representative methods, summarize design paradigms, and
outline open challenges and future directions toward efficient, secure, and
scalable SLM-LLM collaboration.

</details>


### [43] [The Harder The Better: Maintaining Supervised Fine-tuning Generalization with Less but Harder Data](https://arxiv.org/abs/2510.13892)
*Zhaoyang Shang,Sibo Wei,Jianbin Guo,Rui Zhou,Lifeng Dong,Yin Luo*

Main category: cs.CL

TL;DR: 提出了一种名为THTB的框架，用于指令数据选择和注释指导，灵感来自认知科学。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度依赖LLM的内部知识，可解释性弱，泛化能力有限。

Method: 结合质量过滤与内在和外在难度评分，优先考虑更高层次的认知指令。

Result: 在仅使用5%的数据训练的模型优于全数据集训练，并实现了优于仅使用LLM选择的泛化能力。在垂直领域，仅使用2%的数据训练的模型超过了在更大的数据集上训练的模型。

Conclusion: THTB在数据选择和注释指导方面都具有高效的SFT潜力，并为领域自适应提供了强大的潜力。

Abstract: Large Language Models (LLMs) excel in general tasks, but adapting them to
specialized domains relies on high-quality supervised fine-tuning (SFT) data.
Although existing methods can identify subsets of high-quality data and reduce
training cost to some extent, their selection process still suffers from
over-reliance on LLMs' internal knowledge, weak interpretability, and limited
generalization. To address these limitations, we propose THTB (The Harder The
Better), a cognitive science-inspired framework for instruction data selection
and annotation guidance. THTB prioritizes higher-level cognitive instructions
by combining quality filtering with intrinsic and extrinsic hardness scoring,
offering interpretable and quantifiable criteria for efficient SFT, both in
data selection and annotation guidance. Experiments show that THTB enables
models trained on only 5% of the data to outperform full-dataset training,
while achieving superior generalization compared with LLM-only selection. In
addition, THTB provides effective annotation guidance in vertical domains,
enabling a model trained on just 2% of the data to surpass models trained on
much larger datasets, demonstrating strong potential for domain adaptation. Our
code, datasets, and models are available on
https://github.com/DYJG-research/THTB.

</details>


### [44] [Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection](https://arxiv.org/abs/2510.13893)
*Olga E. Sorokoletova,Francesco Giarrusso,Vincenzo Suriani,Daniele Nardi*

Main category: cs.CL

TL;DR: 本文构建了一个包含50种越狱策略的层级分类体系，并进行了红队挑战，以评估不同攻击类型的有效性。结果表明，特定的越狱策略能够利用模型漏洞并导致偏差，而且 taxonomy-guided prompting 可以提高自动检测能力。此外，本文还编译了一个新的意大利语数据集，用于研究对抗意图逐渐显现并绕过传统安全措施的交互。


<details>
  <summary>Details</summary>
Motivation: 现有的防御措施通常侧重于单轮攻击，缺乏跨语言覆盖，并且依赖于有限的分类体系，这些分类体系要么无法捕捉攻击策略的全部多样性，要么强调风险类别而不是越狱技术。

Method: 构建了一个包含50种越狱策略的层级分类体系，并通过红队挑战分析了不同攻击类型的流行率和成功率。同时，对一个流行的LLM进行了越狱检测基准测试，并评估了 taxonomy-guided prompting 对提高自动检测能力的好处。此外，还编译了一个新的意大利语数据集，并用 taxonomy 进行注释。

Result: 构建了一个包含50种越狱策略的层级分类体系，包括 impersonation, persuasion, privilege escalation, cognitive overload, obfuscation, goal conflict, and data poisoning 七个大类。分析了不同攻击类型的流行率和成功率，并发现特定的越狱策略能够利用模型漏洞并导致偏差。Taxonomy-guided prompting 可以提高自动检测能力。编译了一个新的包含1364个多轮对抗对话的意大利语数据集。

Conclusion: 通过构建全面的越狱策略分类体系和进行红队挑战，本文深入了解了越狱技术的有效性，并为改进LLM的安全性提供了新的视角。

Abstract: Jailbreaking techniques pose a significant threat to the safety of Large
Language Models (LLMs). Existing defenses typically focus on single-turn
attacks, lack coverage across languages, and rely on limited taxonomies that
either fail to capture the full diversity of attack strategies or emphasize
risk categories rather than the jailbreaking techniques. To advance the
understanding of the effectiveness of jailbreaking techniques, we conducted a
structured red-teaming challenge. The outcome of our experiments are manifold.
First, we developed a comprehensive hierarchical taxonomy of 50 jailbreak
strategies, consolidating and extending prior classifications into seven broad
families, including impersonation, persuasion, privilege escalation, cognitive
overload, obfuscation, goal conflict, and data poisoning. Second, we analyzed
the data collected from the challenge to examine the prevalence and success
rates of different attack types, providing insights into how specific jailbreak
strategies exploit model vulnerabilities and induce misalignment. Third, we
benchmark a popular LLM for jailbreak detection, evaluating the benefits of
taxonomy-guided prompting for improving automatic detection. Finally, we
compiled a new Italian dataset of 1364 multi-turn adversarial dialogues,
annotated with our taxonomy, enabling the study of interactions where
adversarial intent emerges gradually and succeeds in bypassing traditional
safeguards.

</details>


### [45] [Attribution Quality in AI-Generated Content:Benchmarking Style Embeddings and LLM Judges](https://arxiv.org/abs/2510.13898)
*Misam Abbas*

Main category: cs.CL

TL;DR: 本研究探讨了在大语言模型（LLMs）时代，如何区分人类写作和机器生成文本的作者身份归属问题。


<details>
  <summary>Details</summary>
Motivation: 随着机器生成的文本越来越接近人类写作水平，作者身份归属变得极具挑战性。本研究旨在评估两种互补的作者身份归属方法。

Method: 研究使用了固定风格嵌入和指令调优的LLM评判器（GPT-4o）在Human AI Parallel Corpus数据集上进行基准测试。该数据集包含600个实例，涵盖六个领域（学术、新闻、小说、博客、口语记录和电视/电影剧本）。

Result: 固定风格嵌入在GPT续写文本上表现出更强的总体准确性（82% vs. 68%）。LLM评判器在LLaMA续写文本上略优于风格嵌入（85% vs. 81%），但结果在统计学上不显著。LLM评判器在小说和学术散文中明显优于风格嵌入，表明其具有语义敏感性，而嵌入在口语和剧本对话中占主导地位，反映了结构优势。

Conclusion: 作者身份归属是一个多维度问题，需要混合策略。本研究提供了一个可复现的基准，用于评估AI生成内容中的作者身份归属质量，并回顾了影响这项工作的相关文献。

Abstract: Attributing authorship in the era of large language models (LLMs) is
increasingly challenging as machine-generated prose rivals human writing. We
benchmark two complementary attribution mechanisms , fixed Style Embeddings and
an instruction-tuned LLM judge (GPT-4o) on the Human AI Parallel Corpus, an
open dataset of 600 balanced instances spanning six domains (academic, news,
fiction, blogs, spoken transcripts, and TV/movie scripts). Each instance
contains a human prompt with both a gold continuation and an LLM-generated
continuation from either GPT-4o or LLaMA-70B-Instruct. The Style Embedding
baseline achieves stronger aggregate accuracy on GPT continuations (82 pct vs.
68 pct). The LLM Judge is slightly better than the Style embeddings on LLaMA
continuations (85 pct vs. 81 pct) but the results are not statistically
significant. Crucially, the LLM judge significantly outperforms in fiction and
academic prose, indicating semantic sensitivity, whereas embeddings dominate in
spoken and scripted dialogue, reflecting structural strengths. These
complementary patterns highlight attribution as a multidimensional problem
requiring hybrid strategies. To support reproducibility we provide code on
GitHub and derived data on Hugging Face under the MIT license. This open
framework provides a reproducible benchmark for attribution quality assessment
in AI-generated content, along with a review of related literature influencing
this work.

</details>


### [46] [Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences](https://arxiv.org/abs/2510.13900)
*Julian Minder,Clément Dumas,Stewart Slocum,Helena Casademunt,Cameron Holmes,Robert West,Neel Nanda*

Main category: cs.CL

TL;DR: 狭窄领域的微调会在LLM激活中产生强烈的偏差，可以通过模型差异分析来解释，并且可以通过将这种差异添加到模型激活中来引导生成类似于微调数据的文本。


<details>
  <summary>Details</summary>
Motivation: 将大型语言模型 (LLM) 适应特定任务并创建具有已知不寻常属性的模型（对研究很有用）已成为必不可少的工具。研究发现，狭窄的微调会在LLM激活中产生偏差，可以通过模型差异分析来解释微调领域。

Method: 通过分析随机文本的前几个token的激活差异，并通过将这种差异添加到模型激活中来引导模型，从而生成类似于微调数据的文本。创建了一个基于LLM的可解释性代理来理解微调领域。将预训练数据混合到微调语料库中。

Result: 具有偏差的agent比使用简单prompt的baseline agent表现更好。狭窄微调的模型在其激活中具有其训练目标的显着痕迹，将预训练数据混合到微调语料库中可以消除这些痕迹。

Conclusion: 狭窄微调的模型作为研究更广泛的微调的替代可能并不现实，需要对狭窄微调的影响进行更深入的研究，并为模型差异、安全性和可解释性研究开发真正现实的案例研究。

Abstract: Finetuning on narrow domains has become an essential tool to adapt Large
Language Models (LLMs) to specific tasks and to create models with known
unusual properties that are useful for research. We show that narrow finetuning
creates strong biases in LLM activations that can be interpreted to understand
the finetuning domain. These biases can be discovered using simple tools from
model diffing - the study of differences between models before and after
finetuning. In particular, analyzing activation differences on the first few
tokens of random text and steering by adding this difference to the model
activations produces text similar to the format and general content of the
finetuning data. We demonstrate that these analyses contain crucial information
by creating an LLM-based interpretability agent to understand the finetuning
domain. With access to the bias, the agent performs significantly better
compared to baseline agents using simple prompting. Our analysis spans
synthetic document finetuning for false facts, emergent misalignment,
subliminal learning, and taboo word guessing game models across different
architectures (Gemma, LLaMA, Qwen) and scales (1B to 32B parameters). We
suspect these biases reflect overfitting and find that mixing pretraining data
into the finetuning corpus largely removes them, though residual risks may
remain. Our work (1) demonstrates that narrowly finetuned models have salient
traces of their training objective in their activations and suggests ways to
improve how they are trained, (2) warns AI safety and interpretability
researchers that the common practice of using such models as a proxy for
studying broader finetuning (e.g., chat-tuning) might not be realistic, and (3)
highlights the need for deeper investigation into the effects of narrow
finetuning and development of truly realistic case studies for model-diffing,
safety and interpretability research.

</details>


### [47] [RAID: Refusal-Aware and Integrated Decoding for Jailbreaking LLMs](https://arxiv.org/abs/2510.13901)
*Tuan T. Nguyen,John Le,Thai T. Vu,Willy Susilo,Heath Cooper*

Main category: cs.CL

TL;DR: 大型语言模型在各种任务中表现出色，但仍然容易受到绕过安全机制的越狱攻击。本文提出了一个框架，通过制作对抗性后缀来系统地探测这些弱点，这些后缀可以诱导受限内容，同时保持流畅性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易受到绕过安全机制的越狱攻击。

Method: 提出了一个名为 RAID（Refusal-Aware and Integrated Decoding）的框架，该框架通过制作对抗性后缀来系统地探测这些弱点，这些后缀可以诱导受限内容，同时保持流畅性。RAID 将离散 tokens 放宽为连续嵌入，并使用联合目标对其进行优化，该目标鼓励受限响应，结合了拒绝感知正则化器以引导嵌入空间中的激活远离拒绝方向，并应用连贯性项以保持语义合理性和非冗余性。优化后，评论家引导的解码程序通过平衡嵌入亲和力与语言模型可能性将嵌入映射回 tokens。

Result: 在多个开源 LLM 上的实验表明，与最近的白盒和黑盒基线相比，RAID 以更少的查询次数和更低的计算成本实现了更高的攻击成功率。

Conclusion: 研究结果强调了嵌入空间正则化对于理解和缓解 LLM 越狱漏洞的重要性。

Abstract: Large language models (LLMs) achieve impressive performance across diverse
tasks yet remain vulnerable to jailbreak attacks that bypass safety mechanisms.
We present RAID (Refusal-Aware and Integrated Decoding), a framework that
systematically probes these weaknesses by crafting adversarial suffixes that
induce restricted content while preserving fluency. RAID relaxes discrete
tokens into continuous embeddings and optimizes them with a joint objective
that (i) encourages restricted responses, (ii) incorporates a refusal-aware
regularizer to steer activations away from refusal directions in embedding
space, and (iii) applies a coherence term to maintain semantic plausibility and
non-redundancy. After optimization, a critic-guided decoding procedure maps
embeddings back to tokens by balancing embedding affinity with language-model
likelihood. This integration yields suffixes that are both effective in
bypassing defenses and natural in form. Experiments on multiple open-source
LLMs show that RAID achieves higher attack success rates with fewer queries and
lower computational cost than recent white-box and black-box baselines. These
findings highlight the importance of embedding-space regularization for
understanding and mitigating LLM jailbreak vulnerabilities.

</details>


### [48] [Investigating Political and Demographic Associations in Large Language Models Through Moral Foundations Theory](https://arxiv.org/abs/2510.13902)
*Nicole Smith-Vaniz,Harper Lyon,Lorraine Steigner,Ben Armstrong,Nicholas Mattei*

Main category: cs.CL

TL;DR: 本文研究大型语言模型(llm)在政治和道德领域的潜在偏见，使用道德基础理论(mft)框架分析llm的反应。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在日常生活中扮演着重要的角色，如医学、人际关系甚至法律事务的建议者，因此研究它们在政治和道德领域的偏见非常重要。

Method: 本文直接评估llm反应中的道德倾向，并将其与可靠的人类数据联系起来，研究llm的mft反应与现有的人类研究之间的区别。通过显式提示和基于人口统计的角色扮演，评估llm在不同条件下的行为。

Result: 本文深入了解了人工智能生成反应中政治和人口依赖性的程度。

Conclusion: 通过系统地分析LLM在这些条件和实验中的行为，我们的研究深入了解了AI生成反应中政治和人口依赖性的程度，从而揭示了LLM在道德和政治问题上的偏见。

Abstract: Large Language Models (LLMs) have become increasingly incorporated into
everyday life for many internet users, taking on significant roles as advice
givers in the domains of medicine, personal relationships, and even legal
matters. The importance of these roles raise questions about how and what
responses LLMs make in difficult political and moral domains, especially
questions about possible biases. To quantify the nature of potential biases in
LLMs, various works have applied Moral Foundations Theory (MFT), a framework
that categorizes human moral reasoning into five dimensions: Harm, Fairness,
Ingroup Loyalty, Authority, and Purity. Previous research has used the MFT to
measure differences in human participants along political, national, and
cultural lines. While there has been some analysis of the responses of LLM with
respect to political stance in role-playing scenarios, no work so far has
directly assessed the moral leanings in the LLM responses, nor have they
connected LLM outputs with robust human data. In this paper we analyze the
distinctions between LLM MFT responses and existing human research directly,
investigating whether commonly available LLM responses demonstrate ideological
leanings: either through their inherent responses, straightforward
representations of political ideologies, or when responding from the
perspectives of constructed human personas. We assess whether LLMs inherently
generate responses that align more closely with one political ideology over
another, and additionally examine how accurately LLMs can represent ideological
perspectives through both explicit prompting and demographic-based
role-playing. By systematically analyzing LLM behavior across these conditions
and experiments, our study provides insight into the extent of political and
demographic dependency in AI-generated responses.

</details>


### [49] [Schema for In-Context Learning](https://arxiv.org/abs/2510.13905)
*Pan Chen,Shaohong Chen,Mark Wang,Shi Xuan Leong,Priscilla Fung,Varinia Bernales,Alan Aspuru-Guzik*

Main category: cs.CL

TL;DR: 本文提出了一种新的In-Context Learning (ICL) 框架，称为 SCHEMA ACTIVATED IN CONTEXT LEARNING (SA-ICL)，该框架受到认知科学中图式理论的启发，通过提取先前示例中的推理过程的关键步骤及其关系，创建一个抽象的图式，用于增强模型在处理新问题时的推理过程。


<details>
  <summary>Details</summary>
Motivation: 传统的示例驱动的In-Context Learning缺乏在抽象层面进行知识检索和转移的显式模块。大型语言模型 (LLM) 缺乏形成和利用基于内部图式的学习表征的能力。

Method: SA-ICL框架提取认知构建块的表征，创建抽象的图式，即关键推理步骤及其关系的轻量级结构化模板，然后用于增强模型在处理新问题时的推理过程。

Result: 在GPQA数据集的化学和物理问题上，SA-ICL 始终能提高性能，最高可达 36.19%，同时减少对演示数量的依赖并增强可解释性。

Conclusion: SCHEMA ACTIVATED IN CONTEXT LEARNING 不仅连接了从模式启动到 Chain-of-Thought 提示的不同 ICL 策略，而且为增强 LLM 中类人推理开辟了一条新途径。

Abstract: In-Context Learning (ICL) enables transformer-based language models to adapt
to new tasks by conditioning on demonstration examples. However, traditional
example-driven in-context learning lacks explicit modules for knowledge
retrieval and transfer at the abstraction level. Inspired by cognitive science,
specifically schema theory, which holds that humans interpret new information
by activating pre-existing mental frameworks (schemas) to structure
understanding, we introduce SCHEMA ACTIVATED IN CONTEXT LEARNING (SA-ICL). This
framework extracts the representation of the building blocks of cognition for
the reasoning process instilled from prior examples, creating an abstracted
schema, a lightweight, structured template of key inferential steps and their
relationships, which is then used to augment a model's reasoning process when
presented with a novel question. We demonstrate that a broad range of large
language models (LLMs) lack the capacity to form and utilize internal
schema-based learning representations implicitly, but instead benefit
significantly from explicit schema-based scaffolding. Across chemistry and
physics questions from the GPQA dataset, our experiments show that SA-ICL
consistently boosts performance, up to 36.19 percent, when the single
demonstration example is of high quality, which simultaneously reduces reliance
on the number of demonstrations and enhances interpretability. SCHEMA ACTIVATED
IN CONTEXT LEARNING not only bridges disparate ICL strategies ranging from
pattern priming to Chain-of-Thought prompting, but also paves a new path for
enhancing human-like reasoning in LLMs.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [50] [MultiFoodhat: A potential new paradigm for intelligent food quality inspection](https://arxiv.org/abs/2510.13889)
*Yue Hu,Guohang Zhuang*

Main category: cs.CV

TL;DR: 提出了一个名为MultiFoodChat的对话驱动多智能体框架，用于零样本食物识别。


<details>
  <summary>Details</summary>
Motivation: 现有监督模型依赖大量标注数据，对未见过的食物类别泛化能力有限。

Method: 集成视觉-语言模型（VLMs）和大型语言模型（LLMs），通过多轮视觉-文本对话实现协同推理。使用对象感知Token（OPT）捕获细粒度的视觉属性，交互式推理代理（IRA）动态解释上下文线索以改进预测。

Result: 在多个公共食物数据集上的实验表明，MultiFoodChat与现有的无监督和少样本方法相比，实现了卓越的识别精度和可解释性。

Conclusion: MultiFoodChat有潜力成为智能食品质量检测和分析的新范例。

Abstract: Food image classification plays a vital role in intelligent food quality
inspection, dietary assessment, and automated monitoring. However, most
existing supervised models rely heavily on large labeled datasets and exhibit
limited generalization to unseen food categories. To overcome these challenges,
this study introduces MultiFoodChat, a dialogue-driven multi-agent reasoning
framework for zero-shot food recognition. The framework integrates
vision-language models (VLMs) and large language models (LLMs) to enable
collaborative reasoning through multi-round visual-textual dialogues. An Object
Perception Token (OPT) captures fine-grained visual attributes, while an
Interactive Reasoning Agent (IRA) dynamically interprets contextual cues to
refine predictions. This multi-agent design allows flexible and human-like
understanding of complex food scenes without additional training or manual
annotations. Experiments on multiple public food datasets demonstrate that
MultiFoodChat achieves superior recognition accuracy and interpretability
compared with existing unsupervised and few-shot methods, highlighting its
potential as a new paradigm for intelligent food quality inspection and
analysis.

</details>


### [51] [Post-surgical Endometriosis Segmentation in Laparoscopic Videos](https://arxiv.org/abs/2510.13899)
*Andreas Leibetseder,Klaus Schoeffmann,Jörg Keckstein,Simon Keckstein*

Main category: cs.CV

TL;DR: 该论文介绍了一个用于分割子宫内膜异位症的系统，它可以分析腹腔镜手术视频，用彩色覆盖物注释识别出的植入区域，并显示检测摘要，以改进视频浏览。


<details>
  <summary>Details</summary>
Motivation: 子宫内膜异位症是一种常见的女性疾病，在身体内部各个位置表现出多种视觉外观，这使得它的识别非常困难且容易出错，尤其对于非专业人士。

Method: 该系统经过训练，可以分割子宫内膜异位症的一种常见视觉外观，即深色子宫内膜植入物。

Result: 该系统能够分析腹腔镜手术视频，用彩色覆盖物注释识别出的植入区域。

Conclusion: 该系统可以显示检测摘要，以改进视频浏览。

Abstract: Endometriosis is a common women's condition exhibiting a manifold visual
appearance in various body-internal locations. Having such properties makes its
identification very difficult and error-prone, at least for laymen and
non-specialized medical practitioners. In an attempt to provide assistance to
gynecologic physicians treating endometriosis, this demo paper describes a
system that is trained to segment one frequently occurring visual appearance of
endometriosis, namely dark endometrial implants. The system is capable of
analyzing laparoscopic surgery videos, annotating identified implant regions
with multi-colored overlays and displaying a detection summary for improved
video browsing.

</details>


### [52] [Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models](https://arxiv.org/abs/2510.13993)
*Jia Yun Chua,Argyrios Zolotas,Miguel Arana-Catania*

Main category: cs.CV

TL;DR: 本研究探索了视觉语言模型（VLM）在遥感图像分析中的应用，特别是飞机检测和场景理解。


<details>
  <summary>Details</summary>
Motivation: 传统视觉模型需要大量特定领域的标记数据，并且在理解复杂环境中的上下文方面存在局限性。VLM通过整合视觉和文本数据提供了一种互补的方法，但其在遥感领域的应用仍有待探索。

Method: 将YOLO与LLaVA、ChatGPT和Gemini等VLM相结合，以实现更准确和上下文感知的图像解释。在标记和未标记的遥感数据以及退化图像场景中评估性能。

Result: 在飞机检测和计数精度方面，模型平均MAE提高了48.46%，尤其是在原始和退化场景中的恶劣条件下。在全面理解遥感图像方面，CLIPScore提高了6.17%。

Conclusion: 结合传统视觉模型和VLM的方法为更高级和高效的遥感图像分析铺平了道路，尤其是在少样本学习场景中。

Abstract: Remote sensing has become a vital tool across sectors such as urban planning,
environmental monitoring, and disaster response. While the volume of data
generated has increased significantly, traditional vision models are often
constrained by the requirement for extensive domain-specific labelled data and
their limited ability to understand the context within complex environments.
Vision Language Models offer a complementary approach by integrating visual and
textual data; however, their application to remote sensing remains
underexplored, particularly given their generalist nature. This work
investigates the combination of vision models and VLMs to enhance image
analysis in remote sensing, with a focus on aircraft detection and scene
understanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, and
Gemini aims to achieve more accurate and contextually aware image
interpretation. Performance is evaluated on both labelled and unlabelled remote
sensing data, as well as degraded image scenarios which are crucial for remote
sensing. The findings show an average MAE improvement of 48.46% across models
in the accuracy of aircraft detection and counting, especially in challenging
conditions, in both raw and degraded scenarios. A 6.17% improvement in
CLIPScore for comprehensive understanding of remote sensing images is obtained.
The proposed approach combining traditional vision models and VLMs paves the
way for more advanced and efficient remote sensing image analysis, especially
in few-shot learning scenarios.

</details>


### [53] [Finding Holes: Pathologist Level Performance Using AI for Cribriform Morphology Detection in Prostate Cancer](https://arxiv.org/abs/2510.13995)
*Kelvin Szolnoky,Anders Blilie,Nita Mulliqi,Toyonori Tsuzuki,Hemamali Samaratunga,Matteo Titus,Xiaoyi Ji,Sol Erika Boman,Einar Gudlaugsson,Svein Reidar Kjosavik,José Asenjo,Marcello Gambacorta,Paolo Libretti,Marcin Braun,Radisław Kordek,Roman Łowicki,Brett Delahunt,Kenneth A. Iczkowski,Theo van der Kwast,Geert J. L. H. van Leenders,Katia R. M. Leite,Chin-Chen Pan,Emiel Adrianus Maria Janssen,Martin Eklund,Lars Egevad,Kimmo Kartasalo*

Main category: cs.CV

TL;DR: 开发并验证了一种基于人工智能的系统，以提高前列腺癌筛状形态的检测。


<details>
  <summary>Details</summary>
Motivation: 筛状结构是前列腺癌中一种预后不良的组织学特征，但病理学家之间的观察者差异很大。

Method: 使用带有多个实例学习的EfficientNetV2-S编码器创建了一个深度学习模型，用于端到端全切片分类。在来自三个队列的430名患者的640个数字化前列腺核心针活检上训练了该模型。

Result: 该模型显示出强大的内部验证性能（AUC：0.97，95％CI：0.95-0.99；Cohen's kappa：0.81，95％CI：0.72-0.89）和强大的外部验证（AUC：0.90，95％CI：0.86-0.93；Cohen's kappa：0.55，95％CI：0.45-0.64）。

Conclusion: 我们的人工智能模型展示了病理学家级别的前列腺癌筛状形态检测性能。这种方法可以提高诊断可靠性，标准化报告并改善前列腺癌患者的治疗决策。

Abstract: Background: Cribriform morphology in prostate cancer is a histological
feature that indicates poor prognosis and contraindicates active surveillance.
However, it remains underreported and subject to significant interobserver
variability amongst pathologists. We aimed to develop and validate an AI-based
system to improve cribriform pattern detection.
  Methods: We created a deep learning model using an EfficientNetV2-S encoder
with multiple instance learning for end-to-end whole-slide classification. The
model was trained on 640 digitised prostate core needle biopsies from 430
patients, collected across three cohorts. It was validated internally (261
slides from 171 patients) and externally (266 slides, 104 patients from three
independent cohorts). Internal validation cohorts included laboratories or
scanners from the development set, while external cohorts used completely
independent instruments and laboratories. Annotations were provided by three
expert uropathologists with known high concordance. Additionally, we conducted
an inter-rater analysis and compared the model's performance against nine
expert uropathologists on 88 slides from the internal validation cohort.
  Results: The model showed strong internal validation performance (AUC: 0.97,
95% CI: 0.95-0.99; Cohen's kappa: 0.81, 95% CI: 0.72-0.89) and robust external
validation (AUC: 0.90, 95% CI: 0.86-0.93; Cohen's kappa: 0.55, 95% CI:
0.45-0.64). In our inter-rater analysis, the model achieved the highest average
agreement (Cohen's kappa: 0.66, 95% CI: 0.57-0.74), outperforming all nine
pathologists whose Cohen's kappas ranged from 0.35 to 0.62.
  Conclusion: Our AI model demonstrates pathologist-level performance for
cribriform morphology detection in prostate cancer. This approach could enhance
diagnostic reliability, standardise reporting, and improve treatment decisions
for prostate cancer patients.

</details>


### [54] [NAPPure: Adversarial Purification for Robust Image Classification under Non-Additive Perturbations](https://arxiv.org/abs/2510.14025)
*Junjie Nan,Jianing Li,Wei Chen,Mingkun Zhang,Xueqi Cheng*

Main category: cs.CV

TL;DR: 提出了一种新的对抗性净化框架，可以处理非加性扰动。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗性净化方法在处理非加性扰动时效果不佳，因为它们是为加性扰动设计的。

Method: 首先建立对抗图像的生成过程，然后通过最大化似然来解开潜在的干净图像和扰动参数。

Result: 在GTSRB和CIFAR-10数据集上的实验表明，NAPPure显著提高了图像分类模型对非加性扰动的鲁棒性。

Conclusion: NAPPure可以有效地提高图像分类模型对非加性扰动的鲁棒性。

Abstract: Adversarial purification has achieved great success in combating adversarial
image perturbations, which are usually assumed to be additive. However,
non-additive adversarial perturbations such as blur, occlusion, and distortion
are also common in the real world. Under such perturbations, existing
adversarial purification methods are much less effective since they are
designed to fit the additive nature. In this paper, we propose an extended
adversarial purification framework named NAPPure, which can further handle
non-additive perturbations. Specifically, we first establish the generation
process of an adversarial image, and then disentangle the underlying clean
image and perturbation parameters through likelihood maximization. Experiments
on GTSRB and CIFAR-10 datasets show that NAPPure significantly boosts the
robustness of image classification models against non-additive perturbations.

</details>


### [55] [Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding](https://arxiv.org/abs/2510.14032)
*Xiaoqian Shen,Wenxuan Zhang,Jun Chen,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为Vgent的基于图的检索-推理-增强生成框架，以增强LVLM对长视频的理解。


<details>
  <summary>Details</summary>
Motivation: 现有的视频语言模型难以处理长视频，因为它们难以处理超出上下文窗口的密集视频tokens并保留长期顺序信息。检索增强生成(RAG)在处理大型语言模型的长上下文方面已经证明了有效性;然而，将RAG应用于长视频面临着诸如时间依赖性中断和包含可能妨碍准确推理的无关信息等挑战。

Method: 该方法通过结构化图表示视频，保留视频剪辑之间的语义关系，以提高检索效率。引入中间推理步骤，以减轻LVLM的推理限制，利用结构化验证来减少检索噪声，并促进跨剪辑相关信息的显式聚合。

Result: 在MLVU上，该方法比基础模型提高了3.0%~5.4%的整体性能，并且比最先进的视频RAG方法高出8.6%。

Conclusion: Vgent框架有效地提升了LVLM在长视频理解方面的性能。

Abstract: Understanding and reasoning over long videos pose significant challenges for
large video language models (LVLMs) due to the difficulty in processing
intensive video tokens beyond context window and retaining long-term sequential
information. Retrieval-Augmented Generation (RAG) has demonstrated
effectiveness in processing long context for Large Language Models (LLMs);
however, applying RAG to long video faces challenges such as disrupted temporal
dependencies and inclusion of irrelevant information that can hinder accurate
reasoning. To address these limitations, we propose Vgent, a novel graph-based
retrieval-reasoning-augmented generation framework to enhance LVLMs for long
video understanding. Our approach introduces two key innovations: (i) It
represents videos by structured graphs with semantic relationships across video
clips preserved to improve retrieval effectiveness. (ii) It introduces an
intermediate reasoning step to mitigate the reasoning limitation of LVLMs,
which leverages structured verification to reduce retrieval noise and
facilitate the explicit aggregation of relevant information across clips,
resulting in more accurate and context-aware responses. We comprehensively
evaluate our framework with various open-source LVLMs on three long-video
understanding benchmarks. Our approach yielded an overall performance
improvement of $3.0\%\sim 5.4\%$ over base models on MLVU, and outperformed
state-of-the-art video RAG methods by $8.6\%$. Our code is publicly available
at https://xiaoqian-shen.github.io/Vgent.

</details>


### [56] [Synchronization of Multiple Videos](https://arxiv.org/abs/2510.14051)
*Avihai Naaman,Ron Shapira Weber,Oren Freifeld*

Main category: cs.CV

TL;DR: 提出了一种基于原型学习的时间同步框架（TPL），用于同步来自不同场景或生成式AI视频的视频，通过学习统一的原型序列来对齐关键动作阶段。


<details>
  <summary>Details</summary>
Motivation: 由于不同场景或生成式AI视频具有不同的主题、背景和非线性时间错位，因此同步这些视频非常具有挑战性。

Method: 构建了一个基于原型的框架，通过各种预训练模型提取高维嵌入，构建共享的紧凑的1D表示。通过学习统一的原型序列来锚定关键动作阶段，从而避免详尽的成对匹配。

Result: TPL提高了跨多个数据集的同步精度、效率和鲁棒性，包括细粒度的帧检索和阶段分类任务。TPL是第一个缓解多个描述相同动作的生成式AI视频中的同步问题的方法。

Conclusion: TPL有效地解决了不同场景和生成式AI视频的同步问题，并在多个任务中表现出优越的性能。

Abstract: Synchronizing videos captured simultaneously from multiple cameras in the
same scene is often easy and typically requires only simple time shifts.
However, synchronizing videos from different scenes or, more recently,
generative AI videos, poses a far more complex challenge due to diverse
subjects, backgrounds, and nonlinear temporal misalignment. We propose Temporal
Prototype Learning (TPL), a prototype-based framework that constructs a shared,
compact 1D representation from high-dimensional embeddings extracted by any of
various pretrained models. TPL robustly aligns videos by learning a unified
prototype sequence that anchors key action phases, thereby avoiding exhaustive
pairwise matching. Our experiments show that TPL improves synchronization
accuracy, efficiency, and robustness across diverse datasets, including
fine-grained frame retrieval and phase classification tasks. Importantly, TPL
is the first approach to mitigate synchronization issues in multiple generative
AI videos depicting the same action. Our code and a new multiple video
synchronization dataset are available at https://bgu-cs-vil.github.io/TPL/

</details>


### [57] [Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images](https://arxiv.org/abs/2510.14081)
*Emanuel Garbin,Guy Adam,Oded Krams,Zohar Barzelay,Eran Guendelman,Michael Schwarz,Moran Vatelmacher,Yigal Shenkman,Eli Peker,Itai Druker,Uri Patish,Yoav Blum,Max Bluvstein,Junxuan Li,Rawal Khirodkar,Shunsuke Saito*

Main category: cs.CV

TL;DR: 提出了一种新的零样本流程，用于从一些非结构化的手机图像创建超逼真、保持身份的 3D 头像。


<details>
  <summary>Details</summary>
Motivation: 现有方法面临以下挑战：单视图方法存在几何不一致和幻觉，降低了身份保持能力，而基于合成数据训练的模型无法捕捉高频细节（如皮肤皱纹和细毛），从而限制了真实感。

Method: 该方法引入了两个关键贡献：(1) 一个生成式规范化模块，可将多个非结构化视图处理成标准化的、一致的表示；(2) 一个基于 Transformer 的模型，该模型基于从真人圆顶捕获中获得的大规模高保真 Gaussian splatting 头像数据集进行训练。

Result: 该“捕获、规范化、Splat”流程可从非结构化照片生成具有引人注目的真实感和强大的身份保持能力的静态四分之一身头像。

Conclusion: 总结：新流程能生成更逼真，且更能保持身份的3D头像

Abstract: We present a novel, zero-shot pipeline for creating hyperrealistic,
identity-preserving 3D avatars from a few unstructured phone images. Existing
methods face several challenges: single-view approaches suffer from geometric
inconsistencies and hallucinations, degrading identity preservation, while
models trained on synthetic data fail to capture high-frequency details like
skin wrinkles and fine hair, limiting realism. Our method introduces two key
contributions: (1) a generative canonicalization module that processes multiple
unstructured views into a standardized, consistent representation, and (2) a
transformer-based model trained on a new, large-scale dataset of high-fidelity
Gaussian splatting avatars derived from dome captures of real people. This
"Capture, Canonicalize, Splat" pipeline produces static quarter-body avatars
with compelling realism and robust identity preservation from unstructured
photos.

</details>


### [58] [cubic: CUDA-accelerated 3D Bioimage Computing](https://arxiv.org/abs/2510.14143)
*Alexandr A. Kalinin,Anne E. Carpenter,Shantanu Singh,Matthew J. O'Meara*

Main category: cs.CV

TL;DR: Cubic是一个开源的Python库，通过利用GPU加速来增强SciPy和scikit-image API，从而解决生物图像分析中现有方法的可扩展性、效率和集成问题。


<details>
  <summary>Details</summary>
Motivation: 现有生物图像分析工具通常缺乏API、不支持GPU加速、缺乏广泛的3D图像处理能力，以及在计算密集型工作流程中的互操作性差。

Method: Cubic通过CuPy和RAPIDS cuCIM，用GPU加速的替代方案来增强SciPy和scikit-image API。它的API是设备无关的，当数据位于设备上时，将操作分派到GPU，否则在CPU上执行，从而无缝地加速各种图像处理例程。

Result: Cubic在基准测试和复制现有的反卷积和分割流程中都取得了显著的加速，同时保持了算法的保真度。

Conclusion: Cubic为可扩展、可重复的生物图像分析建立了一个强大的基础，它可以与其他GPU加速方法集成，从而实现交互式探索和自动高通量分析工作流程。

Abstract: Quantitative analysis of multidimensional biological images is useful for
understanding complex cellular phenotypes and accelerating advances in
biomedical research. As modern microscopy generates ever-larger 2D and 3D
datasets, existing computational approaches are increasingly limited by their
scalability, efficiency, and integration with modern scientific computing
workflows. Existing bioimage analysis tools often lack application programmable
interfaces (APIs), do not support graphics processing unit (GPU) acceleration,
lack broad 3D image processing capabilities, and/or have poor interoperability
for compute-heavy workflows. Here, we introduce cubic, an open-source Python
library that addresses these challenges by augmenting widely used SciPy and
scikit-image APIs with GPU-accelerated alternatives from CuPy and RAPIDS cuCIM.
cubic's API is device-agnostic and dispatches operations to GPU when data
reside on the device and otherwise executes on CPU, seamlessly accelerating a
broad range of image processing routines. This approach enables GPU
acceleration of existing bioimage analysis workflows, from preprocessing to
segmentation and feature extraction for 2D and 3D data. We evaluate cubic both
by benchmarking individual operations and by reproducing existing deconvolution
and segmentation pipelines, achieving substantial speedups while maintaining
algorithmic fidelity. These advances establish a robust foundation for
scalable, reproducible bioimage analysis that integrates with the broader
Python scientific computing ecosystem, including other GPU-accelerated methods,
enabling both interactive exploration and automated high-throughput analysis
workflows. cubic is openly available at
https://github$.$com/alxndrkalinin/cubic

</details>


### [59] [Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures](https://arxiv.org/abs/2510.14179)
*Yuancheng Xu,Wenqi Xian,Li Ma,Julien Philip,Ahmet Levent Taşel,Yiwei Zhao,Ryan Burgert,Mingming He,Oliver Hermann,Oliver Pilarski,Rahul Garg,Paul Debevec,Ning Yu*

Main category: cs.CV

TL;DR: 提出了一个框架，通过定制数据管道，在视频扩散模型中实现多视角角色一致性和3D相机控制。


<details>
  <summary>Details</summary>
Motivation: 为了在视频扩散模型中实现多视角角色一致性和3D相机控制。

Method: 通过4D高斯溅射（4DGS）重新渲染记录的体积捕获性能，并结合视频重新照明模型获得的光照变化，训练角色一致性组件。在此数据上微调最先进的开源视频扩散模型。

Result: 改进了视频质量，提高了个性化准确性，并增强了相机控制和光照适应性。

Conclusion: 该框架推进了视频生成与虚拟制作的集成。

Abstract: We introduce a framework that enables both multi-view character consistency
and 3D camera control in video diffusion models through a novel customization
data pipeline. We train the character consistency component with recorded
volumetric capture performances re-rendered with diverse camera trajectories
via 4D Gaussian Splatting (4DGS), lighting variability obtained with a video
relighting model. We fine-tune state-of-the-art open-source video diffusion
models on this data to provide strong multi-view identity preservation, precise
camera control, and lighting adaptability. Our framework also supports core
capabilities for virtual production, including multi-subject generation using
two approaches: joint training and noise blending, the latter enabling
efficient composition of independently customized models at inference time; it
also achieves scene and real-life video customization as well as control over
motion and spatial layout during customization. Extensive experiments show
improved video quality, higher personalization accuracy, and enhanced camera
control and lighting adaptability, advancing the integration of video
generation into virtual production. Our project page is available at:
https://eyeline-labs.github.io/Virtually-Being.

</details>


### [60] [Joint Modeling of Big Five and HEXACO for Multimodal Apparent Personality-trait Recognition](https://arxiv.org/abs/2510.14203)
*Ryo Masumura,Shota Orihashi,Mana Ihori,Tomohiro Tanaka,Naoki Makishima,Taiga Yamane,Naotaka Kawata,Satoshi Suzuki,Taichi Katayama*

Main category: cs.CV

TL;DR: 本文提出了一种联合建模方法，用于从多模态人类行为中自动识别外显人格特质，该方法同时考虑了心理学中长期研究的Big Five和最近备受关注的HEXACO。


<details>
  <summary>Details</summary>
Motivation: 先前研究大多使用Big Five进行多模态外显人格特质识别，但忽略了HEXACO，HEXACO可以评估与转移性攻击和报复、社会支配导向等相关的诚实-谦虚特质。此外，Big Five和HEXACO在机器学习建模中的关系尚未明确。考虑这些关系有望提高对多模态人类行为的认识。

Method: 提出了一种联合识别Big Five和HEXACO的方法。

Result: 使用自我介绍视频数据集的实验表明，该方法可以有效识别Big Five和HEXACO。

Conclusion: 该方法可以有效识别Big Five和HEXACO。

Abstract: This paper proposes a joint modeling method of the Big Five, which has long
been studied, and HEXACO, which has recently attracted attention in psychology,
for automatically recognizing apparent personality traits from multimodal human
behavior. Most previous studies have used the Big Five for multimodal apparent
personality-trait recognition. However, no study has focused on apparent HEXACO
which can evaluate an Honesty-Humility trait related to displaced aggression
and vengefulness, social-dominance orientation, etc. In addition, the
relationships between the Big Five and HEXACO when modeled by machine learning
have not been clarified. We expect awareness of multimodal human behavior to
improve by considering these relationships. The key advance of our proposed
method is to optimize jointly recognizing the Big Five and HEXACO. Experiments
using a self-introduction video dataset demonstrate that the proposed method
can effectively recognize the Big Five and HEXACO.

</details>


### [61] [LOTA: Bit-Planes Guided AI-Generated Image Detection](https://arxiv.org/abs/2510.14230)
*Hongsong Wang,Renxi Cheng,Yang Zhang,Chaolei Han,Jie Gui*

Main category: cs.CV

TL;DR: 该论文提出了一种新的AI生成图像检测方法，该方法通过位平面图像处理提取噪声特征，并设计最大梯度块选择来增强噪声信号，最后使用轻量级分类器进行分类。该方法在GenImage基准测试中取得了优异的性能，平均准确率达到98.9%，且具有良好的跨生成器泛化能力，同时计算速度比现有方法快近百倍。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像重建误差的AI生成图像检测方法计算成本高，且未能捕捉到原始图像中固有的噪声特征。

Method: 该论文创新性地使用基于位平面的图像处理来改进误差提取，利用低位平面表示图像中的噪声模式。引入了一种有效的位平面引导噪声图像生成方法，并探索了各种图像归一化策略。设计了一种最大梯度块选择方法，通过应用多方向梯度来计算噪声分数，并选择得分最高的区域。提出了一个轻量级且有效的分类头，并探索了两种不同的结构：基于噪声的分类器和噪声引导的分类器。

Result: 该方法在GenImage基准测试中取得了优异的性能，平均准确率达到98.9%（提升了11.9%），并且显示出出色的跨生成器泛化能力。特别是，该方法从GAN到Diffusion的准确率超过98.2%，从Diffusion到GAN的准确率超过99.2%。此外，它在毫秒级别执行误差提取，比现有方法快近百倍。

Conclusion: 该论文提出的方法能够以更低的计算成本和更高的准确率检测AI生成的图像，并且具有良好的泛化能力。

Abstract: The rapid advancement of GAN and Diffusion models makes it more difficult to
distinguish AI-generated images from real ones. Recent studies often use
image-based reconstruction errors as an important feature for determining
whether an image is AI-generated. However, these approaches typically incur
high computational costs and also fail to capture intrinsic noisy features
present in the raw images. To solve these problems, we innovatively refine
error extraction by using bit-plane-based image processing, as lower bit planes
indeed represent noise patterns in images. We introduce an effective bit-planes
guided noisy image generation and exploit various image normalization
strategies, including scaling and thresholding. Then, to amplify the noise
signal for easier AI-generated image detection, we design a maximum gradient
patch selection that applies multi-directional gradients to compute the noise
score and selects the region with the highest score. Finally, we propose a
lightweight and effective classification head and explore two different
structures: noise-based classifier and noise-guided classifier. Extensive
experiments on the GenImage benchmark demonstrate the outstanding performance
of our method, which achieves an average accuracy of \textbf{98.9\%}
(\textbf{11.9}\%~$\uparrow$) and shows excellent cross-generator generalization
capability. Particularly, our method achieves an accuracy of over 98.2\% from
GAN to Diffusion and over 99.2\% from Diffusion to GAN. Moreover, it performs
error extraction at the millisecond level, nearly a hundred times faster than
existing methods. The code is at https://github.com/hongsong-wang/LOTA.

</details>


### [62] [PIA: Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic Analysis](https://arxiv.org/abs/2510.14241)
*Soumyya Kanti Datta,Tanvi Ranga,Chengzhe Sun,Siwei Lyu*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的多模态音视频框架，用于检测由GANs、扩散模型和神经渲染等先进生成模型生成的deepfakes。


<details>
  <summary>Details</summary>
Motivation: 传统的deepfakes检测方法依赖于手动设计的音素-视觉对齐阈值、基本的帧级一致性检查或单模态检测策略，无法充分识别由先进的生成模型生成的现代deepfakes。

Method: 该方法利用音素序列、嘴唇几何数据和先进的面部识别嵌入，结合语言、动态面部运动和面部识别线索。

Result: 通过识别多个互补模态之间的不一致性，显著提高了对细微deepfake改变的检测。

Conclusion: 该研究提出了一种有效的多模态音视频框架，可以有效检测现代deepfakes。

Abstract: The rise of manipulated media has made deepfakes a particularly insidious
threat, involving various generative manipulations such as lip-sync
modifications, face-swaps, and avatar-driven facial synthesis. Conventional
detection methods, which predominantly depend on manually designed
phoneme-viseme alignment thresholds, fundamental frame-level consistency
checks, or a unimodal detection strategy, inadequately identify modern-day
deepfakes generated by advanced generative models such as GANs, diffusion
models, and neural rendering techniques. These advanced techniques generate
nearly perfect individual frames yet inadvertently create minor temporal
discrepancies frequently overlooked by traditional detectors. We present a
novel multimodal audio-visual framework, Phoneme-Temporal and Identity-Dynamic
Analysis(PIA), incorporating language, dynamic face motion, and facial
identification cues to address these limitations. We utilize phoneme sequences,
lip geometry data, and advanced facial identity embeddings. This integrated
method significantly improves the detection of subtle deepfake alterations by
identifying inconsistencies across multiple complementary modalities. Code is
available at https://github.com/skrantidatta/PIA

</details>


### [63] [Event Interval Modulation: A Novel Scheme for Event-based Optical Camera Communication](https://arxiv.org/abs/2510.14245)
*Miu Sumino,Mayu Ishii,Shun Kaizu,Daisuke Hisano,Yu Nakayama*

Main category: cs.CV

TL;DR: 提出了一种新的基于事件的OCC调制方案，称为事件间隔调制（EIM），该方案通过调制事件之间的时间间隔来提高传输速度，并在室内环境中实现了28 kbps@10m和8.4 kbps@50m的传输速率。


<details>
  <summary>Details</summary>
Motivation: 传统的基于帧的相机的OCC系统存在比特率低和处理负载高等局限性。虽然基于事件的视觉传感器（EVS）的OCC系统具有高速、低延迟和鲁棒性，但现有的调制方法未能充分利用EVS的特性。

Method: 提出了一种专门为基于事件的OCC设计的事件间隔调制（EIM）方案，该方案通过事件间隔调制信息。对EIM进行了理论建模，并进行了概念验证实验，优化了EVS的参数以优化EIM的频率响应，并实验确定了EIM中可用的最大调制阶数。

Result: 在室内环境中，实现了10米距离28 kbps和50米距离8.4 kbps的成功传输，为基于事件的OCC系统建立了新的比特率基准。

Conclusion: EIM调制方案在基于事件的OCC系统中具有良好的应用前景，并在传输速率上实现了新的突破。

Abstract: Optical camera communication (OCC) represents a promising visible light
communication technology. Nonetheless, typical OCC systems utilizing
frame-based cameras are encumbered by limitations, including low bit rate and
high processing load. To address these issues, OCC system utilizing an
event-based vision sensor (EVS) as receivers have been proposed. The EVS
enables high-speed, low-latency, and robust communication due to its
asynchronous operation and high dynamic range. In existing event-based OCC
systems, conventional modulation schemes such as on-off keying (OOK) and pulse
position modulation have been applied, however, to the best of our knowledge,
no modulation method has been proposed that fully exploits the unique
characteristics of the EVS. This paper proposes a novel modulation scheme,
called the event interval modulation (EIM) scheme, specifically designed for
event-based OCC. EIM enables improvement in transmission speed by modulating
information using the intervals between events. This paper proposes a
theoretical model of EIM and conducts a proof-of-concept experiment. First, the
parameters of the EVS are tuned and customized to optimize the frequency
response specifically for EIM. Then, the maximum modulation order usable in EIM
is determined experimentally. We conduct transmission experiments based on the
obtained parameters. Finally, we report successful transmission at 28 kbps over
10 meters and 8.4 kbps over 50 meters in an indoor environment. This sets a new
benchmark for bit rate in event-based OCC systems.

</details>


### [64] [Acquisition of interpretable domain information during brain MR image harmonization for content-based image retrieval](https://arxiv.org/abs/2510.14535)
*Keima Abe,Hayato Muraki,Shuhei Tomoshige,Kenichi Oishi,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: 提出了一种名为PL-SE-ADA的领域自适应框架，用于医学图像的领域协调和可解释的表征学习。


<details>
  <summary>Details</summary>
Motivation: 医学图像由于扫描仪和协议的差异，在不同成像站点之间存在领域偏移，这会降低机器学习在疾病分类等任务中的性能。

Method: 该方法包括两个编码器、一个解码器和一个领域预测器，通过对抗训练和图像重建学习领域不变和领域特定的特征。

Result: PL-SE-ADA在图像重建、疾病分类和领域识别方面取得了与现有方法相当或更好的性能。

Conclusion: PL-SE-ADA能够可视化领域独立的脑部特征和领域特定的组成部分，从而提供整个框架的高度可解释性。

Abstract: Medical images like MR scans often show domain shifts across imaging sites
due to scanner and protocol differences, which degrade machine learning
performance in tasks such as disease classification. Domain harmonization is
thus a critical research focus. Recent approaches encode brain images
$\boldsymbol{x}$ into a low-dimensional latent space $\boldsymbol{z}$, then
disentangle it into $\boldsymbol{z_u}$ (domain-invariant) and
$\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, these
methods often lack interpretability$-$an essential requirement in medical
applications$-$leaving practical issues unresolved. We propose
Pseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), a
general framework for domain harmonization and interpretable representation
learning that preserves disease-relevant information in brain MR images.
PL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract
$\boldsymbol{z_u}$ and $\boldsymbol{z_d}$, a decoder to reconstruct the image
$f_D$, and a domain predictor $g_D$. Beyond adversarial training between the
encoder and domain predictor, the model learns to reconstruct the input image
$\boldsymbol{x}$ by summing reconstructions from $\boldsymbol{z_u}$ and
$\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Compared
to prior methods, PL-SE-ADA achieves equal or better performance in image
reconstruction, disease classification, and domain recognition. It also enables
visualization of both domain-independent brain features and domain-specific
components, offering high interpretability across the entire framework.

</details>


### [65] [MACE: Mixture-of-Experts Accelerated Coordinate Encoding for Large-Scale Scene Localization and Rendering](https://arxiv.org/abs/2510.14251)
*Mingkai Liu,Dikai Fan,Haohua Que,Haojia Gao,Xiao Liu,Shuxue Peng,Meixia Lin,Shengyu Gu,Ruicong Ye,Wanli Qiu,Handong Yao,Ruopeng Zhang,Xianliang Huang*

Main category: cs.CV

TL;DR: MACE通过混合专家网络和无辅助损失的负载均衡策略，实现了大规模场景下的高效定位和高质量渲染。


<details>
  <summary>Details</summary>
Motivation: 现有场景坐标回归方法在扩展到大规模场景时受到单网络容量的限制。

Method: 提出混合专家加速坐标编码方法(MACE)，使用门控网络选择子网络，并提出无辅助损失的负载均衡策略(ALF-LB)。

Result: 该框架显著降低了成本，同时保持了更高的精度。在剑桥测试集上仅需10分钟训练即可实现高质量的渲染结果。

Conclusion: MACE为大规模场景应用提供了一种高效的解决方案。

Abstract: Efficient localization and high-quality rendering in large-scale scenes
remain a significant challenge due to the computational cost involved. While
Scene Coordinate Regression (SCR) methods perform well in small-scale
localization, they are limited by the capacity of a single network when
extended to large-scale scenes. To address these challenges, we propose the
Mixed Expert-based Accelerated Coordinate Encoding method (MACE), which enables
efficient localization and high-quality rendering in large-scale scenes.
Inspired by the remarkable capabilities of MOE in large model domains, we
introduce a gating network to implicitly classify and select sub-networks,
ensuring that only a single sub-network is activated during each inference.
Furtheremore, we present Auxiliary-Loss-Free Load Balancing(ALF-LB) strategy to
enhance the localization accuracy on large-scale scene. Our framework provides
a significant reduction in costs while maintaining higher precision, offering
an efficient solution for large-scale scene applications. Additional
experiments on the Cambridge test set demonstrate that our method achieves
high-quality rendering results with merely 10 minutes of training.

</details>


### [66] [Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization](https://arxiv.org/abs/2510.14255)
*Liao Shen,Wentao Jiang,Yiran Zhu,Tiezheng Ge,Zhiguo Cao,Bo Zheng*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为 Identity-Preserving Reward-guided Optimization (IPRO) 的新视频扩散框架，该框架基于强化学习，旨在提高 identity 保持。


<details>
  <summary>Details</summary>
Motivation: 现有的 image-to-video (I2V) 模型在保持输入人像和生成的视频之间身份一致性方面存在困难，尤其是在视频中的人表现出显著的表情变化和动作时。当人脸仅占图像的一小部分时，这个问题变得至关重要。

Method: 该方法使用人脸 identity scorer 优化扩散模型，并通过采样链的最后步骤反向传播奖励信号，从而实现更丰富的梯度反馈。此外，还提出了一种新的人脸评分机制，将 ground-truth 视频中的人脸视为面部特征池，提供多角度面部信息以增强泛化能力。KL 散度正则化进一步用于稳定训练并防止过度拟合奖励信号。

Result: 在 Wan 2.2 I2V 模型和内部 I2V 模型上进行的大量实验证明了该方法的有效性。

Conclusion: IPRO 是一种直接有效的调整算法，可优化扩散模型，从而增强 identity 保持。

Abstract: Recent advances in image-to-video (I2V) generation have achieved remarkable
progress in synthesizing high-quality, temporally coherent videos from static
images. Among all the applications of I2V, human-centric video generation
includes a large portion. However, existing I2V models encounter difficulties
in maintaining identity consistency between the input human image and the
generated video, especially when the person in the video exhibits significant
expression changes and movements. This issue becomes critical when the human
face occupies merely a small fraction of the image. Since humans are highly
sensitive to identity variations, this poses a critical yet under-explored
challenge in I2V generation. In this paper, we propose Identity-Preserving
Reward-guided Optimization (IPRO), a novel video diffusion framework based on
reinforcement learning to enhance identity preservation. Instead of introducing
auxiliary modules or altering model architectures, our approach introduces a
direct and effective tuning algorithm that optimizes diffusion models using a
face identity scorer. To improve performance and accelerate convergence, our
method backpropagates the reward signal through the last steps of the sampling
chain, enabling richer gradient feedback. We also propose a novel facial
scoring mechanism that treats faces in ground-truth videos as facial feature
pools, providing multi-angle facial information to enhance generalization. A
KL-divergence regularization is further incorporated to stabilize training and
prevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V
model and our in-house I2V model demonstrate the effectiveness of our method.
Our project and code are available at
\href{https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/}.

</details>


### [67] [Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement Learning](https://arxiv.org/abs/2510.14256)
*Xiangyu Meng,Zixian Zhang,Zhenghao Zhang,Junchao Liao,Long Qin,Weizhi Wang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为 Identity-GRPO 的人类反馈驱动优化流程，用于改进多人身份保持的视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法在动态交互中难以保持多个角色身份的一致性。

Method: 该方法构建了一个视频奖励模型，并采用了一种为多人一致性定制的 GRPO 变体。

Result: 实验表明，Identity-GRPO 在人类一致性指标上比基线方法提高了 18.9%。

Conclusion: Identity-GRPO 能够有效提升多人身份保持视频生成效果，并为强化学习与个性化视频生成对齐提供了可操作的见解。

Abstract: While advanced methods like VACE and Phantom have advanced video generation
for specific subjects in diverse scenarios, they struggle with multi-human
identity preservation in dynamic interactions, where consistent identities
across multiple characters are critical. To address this, we propose
Identity-GRPO, a human feedback-driven optimization pipeline for refining
multi-human identity-preserving video generation. First, we construct a video
reward model trained on a large-scale preference dataset containing
human-annotated and synthetic distortion data, with pairwise annotations
focused on maintaining human consistency throughout the video. We then employ a
GRPO variant tailored for multi-human consistency, which greatly enhances both
VACE and Phantom. Through extensive ablation studies, we evaluate the impact of
annotation quality and design choices on policy optimization. Experiments show
that Identity-GRPO achieves up to 18.9% improvement in human consistency
metrics over baseline methods, offering actionable insights for aligning
reinforcement learning with personalized video generation.

</details>


### [68] [MatchAttention: Matching the Relative Positions for High-Resolution Cross-View Matching](https://arxiv.org/abs/2510.14260)
*Tingman Yan,Tao Liu,Xilian Yang,Qunfei Zhao,Zeyang Xia*

Main category: cs.CV

TL;DR: This paper proposes MatchAttention, a novel attention mechanism for efficient and accurate cross-view matching, especially for high-resolution images.


<details>
  <summary>Details</summary>
Motivation: Existing cross-attention mechanisms suffer from quadratic complexity and lack explicit matching constraints, making high-resolution image matching challenging.

Method: The paper introduces MatchAttention, which dynamically matches relative positions using BilinearSoftmax for continuous sliding-window attention sampling. It also proposes MatchDecoder, gated cross-MatchAttention, and a consistency-constrained loss to handle occlusions.

Result: MatchStereo-B ranked 1st on the Middlebury benchmark and achieves fast inference on KITTI-resolution images. MatchStereo-T processes 4K UHD images quickly with low GPU memory usage. State-of-the-art results are achieved on KITTI 2012, KITTI 2015, ETH3D, and Spring flow datasets.

Conclusion: The proposed approach enables real-time, high-resolution, and high-accuracy cross-view matching.

Abstract: Cross-view matching is fundamentally achieved through cross-attention
mechanisms. However, matching of high-resolution images remains challenging due
to the quadratic complexity and lack of explicit matching constraints in the
existing cross-attention. This paper proposes an attention mechanism,
MatchAttention, that dynamically matches relative positions. The relative
position determines the attention sampling center of the key-value pairs given
a query. Continuous and differentiable sliding-window attention sampling is
achieved by the proposed BilinearSoftmax. The relative positions are
iteratively updated through residual connections across layers by embedding
them into the feature channels. Since the relative position is exactly the
learning target for cross-view matching, an efficient hierarchical cross-view
decoder, MatchDecoder, is designed with MatchAttention as its core component.
To handle cross-view occlusions, gated cross-MatchAttention and a
consistency-constrained loss are proposed. These two components collectively
mitigate the impact of occlusions in both forward and backward passes, allowing
the model to focus more on learning matching relationships. When applied to
stereo matching, MatchStereo-B ranked 1st in average error on the public
Middlebury benchmark and requires only 29ms for KITTI-resolution inference.
MatchStereo-T can process 4K UHD images in 0.1 seconds using only 3GB of GPU
memory. The proposed models also achieve state-of-the-art performance on KITTI
2012, KITTI 2015, ETH3D, and Spring flow datasets. The combination of high
accuracy and low computational complexity makes real-time, high-resolution, and
high-accuracy cross-view matching possible. Code is available at
https://github.com/TingmanYan/MatchAttention.

</details>


### [69] [Experimental Demonstration of Event-based Optical Camera Communication in Long-Range Outdoor Environment](https://arxiv.org/abs/2510.14266)
*Miu Sumino,Mayu Ishii,Shun Kaizu,Daisuke Hisano,Yu Nakayama*

Main category: cs.CV

TL;DR: 本文提出了一种用于光学相机通信系统的稳健解调方案，该方案结合了OOK、切换解调和数字锁相环。


<details>
  <summary>Details</summary>
Motivation: 在光学相机通信系统中，实现远距离、高速率和低误码率的稳健通信。

Method: 结合OOK、切换解调和数字锁相环。

Result: 在200米-60kbps和400米-30kbps的户外实验中，实现了BER < 10^{-3}。

Conclusion: 该方案在户外实验中实现了远距离、高速率和低误码率的稳健通信。

Abstract: We propose a robust demodulation scheme for optical camera communication
systems using an event-based vision sensor, combining OOK with toggle
demodulation and a digital phase-locked loop. This is the first report to
achieve a $\mathrm{BER} < 10^{-3}$ at 200m-60kbps and 400m-30kbps in outdoor
experiments.

</details>


### [70] [GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering](https://arxiv.org/abs/2510.14270)
*Alexander Valverde,Brian Xu,Yuyin Zhou,Meng Xu,Hongyun Wang*

Main category: cs.CV

TL;DR: 提出了一种结合2D基础模型和3D高斯溅射重建的混合方法 GauSSmart，以提升场景重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有高斯溅射方法在稀疏覆盖区域捕捉细节或保持真实感方面存在困难，主要由于稀疏3D训练数据的固有局限性。

Method: 结合了凸滤波和来自DINO等基础模型的语义特征监督等2D计算机视觉技术，以增强基于高斯的场景重建。利用2D分割先验和高维特征嵌入来引导高斯splat的密集化和细化，改善了覆盖不足区域的覆盖率，并保留了复杂的结构细节。

Result: 在三个数据集上验证了该方法，GauSSmart 在大多数评估场景中始终优于现有的高斯溅射方法。

Conclusion: 混合2D-3D方法具有巨大的潜力，通过将2D基础模型与3D重建流程相结合，可以克服两种方法固有的局限性。

Abstract: Scene reconstruction has emerged as a central challenge in computer vision,
with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting
achieving remarkable progress. While Gaussian Splatting demonstrates strong
performance on large-scale datasets, it often struggles to capture fine details
or maintain realism in regions with sparse coverage, largely due to the
inherent limitations of sparse 3D training data.
  In this work, we propose GauSSmart, a hybrid method that effectively bridges
2D foundational models and 3D Gaussian Splatting reconstruction. Our approach
integrates established 2D computer vision techniques, including convex
filtering and semantic feature supervision from foundational models such as
DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D
segmentation priors and high-dimensional feature embeddings, our method guides
the densification and refinement of Gaussian splats, improving coverage in
underrepresented areas and preserving intricate structural details.
  We validate our approach across three datasets, where GauSSmart consistently
outperforms existing Gaussian Splatting in the majority of evaluated scenes.
Our results demonstrate the significant potential of hybrid 2D-3D approaches,
highlighting how the thoughtful combination of 2D foundational models with 3D
reconstruction pipelines can overcome the limitations inherent in either
approach alone.

</details>


### [71] [CLEAR: Causal Learning Framework For Robust Histopathology Tumor Detection Under Out-Of-Distribution Shifts](https://arxiv.org/abs/2510.14273)
*Kieu-Anh Truong Thi,Huy-Hieu Pham,Duc-Trong Le*

Main category: cs.CV

TL;DR: 提出了一种基于因果推理的框架，通过利用语义特征并减轻混淆因素的影响来解决组织病理学中的领域转移问题。


<details>
  <summary>Details</summary>
Motivation: 组织病理学中的领域转移对深度学习模型的泛化能力提出了重大挑战，现有方法主要依赖于通过对齐特征分布或引入统计变化来建模统计相关性，但它们经常忽略因果关系。

Method: 该方法通过设计明确包含中介和观察到的组织切片的转换策略来实现前门原则。

Result: 在CAMELYON17数据集和一个私有组织病理学数据集上验证了该方法，证明了在未见领域上的一致性能提升。在CAMELYON17数据集和私有组织病理学数据集上，该方法实现了高达7%的改进，优于现有基线。

Conclusion: 这些结果突出了因果推理作为解决组织病理学图像分析中领域转移问题的强大工具的潜力。

Abstract: Domain shift in histopathology, often caused by differences in acquisition
processes or data sources, poses a major challenge to the generalization
ability of deep learning models. Existing methods primarily rely on modeling
statistical correlations by aligning feature distributions or introducing
statistical variation, yet they often overlook causal relationships. In this
work, we propose a novel causal-inference-based framework that leverages
semantic features while mitigating the impact of confounders. Our method
implements the front-door principle by designing transformation strategies that
explicitly incorporate mediators and observed tissue slides. We validate our
method on the CAMELYON17 dataset and a private histopathology dataset,
demonstrating consistent performance gains across unseen domains. As a result,
our approach achieved up to a 7% improvement in both the CAMELYON17 dataset and
the private histopathology dataset, outperforming existing baselines. These
results highlight the potential of causal inference as a powerful tool for
addressing domain shift in histopathology image analysis.

</details>


### [72] [Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding](https://arxiv.org/abs/2510.14304)
*Kyungryul Back,Seongbeom Park,Milim Kim,Mincheol Kwon,SangHyeok Lee,Hyunyoung Lee,Junhee Cho,Seunghyun Park,Jinkyu Kim*

Main category: cs.CV

TL;DR: 这篇论文提出了一种免训练的三层对比解码方法，通过水印技术来减少大型视觉语言模型中的幻觉问题，并生成更符合视觉实际的回复。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型容易产生幻觉，过度依赖单一模态或记忆训练数据，缺乏输出的 grounding。

Method: 提出一种免训练的三层对比解码方法，包括选择成熟层和业余层，通过水印问题识别关键层，并应用三层对比解码生成最终输出。

Result: 在 POPE、MME 和 AMBER 等公共基准测试中，该方法在减少 LVLM 中的幻觉方面取得了最先进的性能。

Conclusion: 该方法能有效减少大型视觉语言模型中的幻觉，并生成更符合视觉实际的回复。

Abstract: Large Vision-Language Models (LVLMs) have recently shown promising results on
various multimodal tasks, even achieving human-comparable performance in
certain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often
rely heavily on a single modality or memorize training data without properly
grounding their outputs. To address this, we propose a training-free, tri-layer
contrastive decoding with watermarking, which proceeds in three steps: (1)
select a mature layer and an amateur layer among the decoding layers, (2)
identify a pivot layer using a watermark-related question to assess whether the
layer is visually well-grounded, and (3) apply tri-layer contrastive decoding
to generate the final output. Experiments on public benchmarks such as POPE,
MME and AMBER demonstrate that our method achieves state-of-the-art performance
in reducing hallucinations in LVLMs and generates more visually grounded
responses.

</details>


### [73] [A Multi-domain Image Translative Diffusion StyleGAN for Iris Presentation Attack Detection](https://arxiv.org/abs/2510.14314)
*Shivangi Yadav,Arun Ross*

Main category: cs.CV

TL;DR: 提出了一种新的框架，用于生成合成的眼部图像，该框架能够捕捉多种域中的PA和真实特征，例如真实的、打印的眼睛和美容隐形眼镜。


<details>
  <summary>Details</summary>
Motivation: 虹膜生物识别系统可能会受到呈现攻击（PA）的威胁，其中人造眼睛、打印的眼睛图像或美容隐形眼镜等伪造品会呈现给系统。为了解决这个问题，已经开发了几种呈现攻击检测（PAD）方法。然而，由于构建和成像PA的内在困难，用于训练和评估虹膜PAD技术的数据集非常稀缺。

Method: MID-StyleGAN结合了扩散模型和生成对抗网络（GAN）的优势，以生成逼真且多样化的合成数据。我们的方法利用了一种多域架构，该架构能够在真实的眼部图像和不同的PA域之间进行转换。该模型采用了一种为眼部数据量身定制的自适应损失函数，以保持域一致性。

Result: MID-StyleGAN在生成高质量合成眼部图像方面优于现有方法。生成的数据被用于显着提高PAD系统的性能，为虹膜和眼部生物识别技术中的数据稀缺问题提供了一种可扩展的解决方案。例如，在LivDet2020数据集上，在1%的错误检测率下的真实检测率从93.41%提高到98.72%，展示了该方法的影响。

Conclusion: 总而言之，该论文提出了一种新的数据生成方法，可以有效提高PAD系统的性能。

Abstract: An iris biometric system can be compromised by presentation attacks (PAs)
where artifacts such as artificial eyes, printed eye images, or cosmetic
contact lenses are presented to the system. To counteract this, several
presentation attack detection (PAD) methods have been developed. However, there
is a scarcity of datasets for training and evaluating iris PAD techniques due
to the implicit difficulties in constructing and imaging PAs. To address this,
we introduce the Multi-domain Image Translative Diffusion StyleGAN
(MID-StyleGAN), a new framework for generating synthetic ocular images that
captures the PA and bonafide characteristics in multiple domains such as
bonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the
strengths of diffusion models and generative adversarial networks (GANs) to
produce realistic and diverse synthetic data. Our approach utilizes a
multi-domain architecture that enables the translation between bonafide ocular
images and different PA domains. The model employs an adaptive loss function
tailored for ocular data to maintain domain consistency. Extensive experiments
demonstrate that MID-StyleGAN outperforms existing methods in generating
high-quality synthetic ocular images. The generated data was used to
significantly enhance the performance of PAD systems, providing a scalable
solution to the data scarcity problem in iris and ocular biometrics. For
example, on the LivDet2020 dataset, the true detect rate at 1% false detect
rate improved from 93.41% to 98.72%, showcasing the impact of the proposed
method.

</details>


### [74] [Vision-Centric Activation and Coordination for Multimodal Large Language Models](https://arxiv.org/abs/2510.14349)
*Yunnan Wang,Fan Lu,Kecheng Zheng,Ziyuan Huang,Ziqiang Li,Wenjun Zeng,Xin Jin*

Main category: cs.CV

TL;DR: VaCo通过视觉中心激活和来自多个视觉基础模型（VFM）的协调来优化MLLM表示，从而提升视觉理解能力。


<details>
  <summary>Details</summary>
Motivation: 主流MLLM仅受文本token的下一个token预测的监督，忽略了对分析能力至关重要的以视觉为中心的信息。

Method: VaCo引入了视觉判别对齐来整合从VFM中提取的任务感知感知特征，从而统一了MLLM中文本和视觉输出的优化。具体来说，我们将可学习的模块化任务查询（MTQ）和视觉对齐层（VAL）整合到MLLM中，在不同VFM的监督下激活特定的视觉信号。为了协调VFM之间的表示冲突，精心设计的Token Gateway Mask（TGM）限制了多组MTQ之间的信息流。

Result: 大量实验表明，VaCo显着提高了不同MLLM在各种基准测试中的性能，展示了其卓越的视觉理解能力。

Conclusion: VaCo能够显著提升MLLM的视觉理解能力。

Abstract: Multimodal large language models (MLLMs) integrate image features from visual
encoders with LLMs, demonstrating advanced comprehension capabilities. However,
mainstream MLLMs are solely supervised by the next-token prediction of textual
tokens, neglecting critical vision-centric information essential for analytical
abilities. To track this dilemma, we introduce VaCo, which optimizes MLLM
representations through Vision-Centric activation and Coordination from
multiple vision foundation models (VFMs). VaCo introduces visual discriminative
alignment to integrate task-aware perceptual features extracted from VFMs,
thereby unifying the optimization of both textual and visual outputs in MLLMs.
Specifically, we incorporate the learnable Modular Task Queries (MTQs) and
Visual Alignment Layers (VALs) into MLLMs, activating specific visual signals
under the supervision of diverse VFMs. To coordinate representation conflicts
across VFMs, the crafted Token Gateway Mask (TGM) restricts the information
flow among multiple groups of MTQs. Extensive experiments demonstrate that VaCo
significantly improves the performance of different MLLMs on various
benchmarks, showcasing its superior capabilities in visual comprehension.

</details>


### [75] [Leveraging Cycle-Consistent Anchor Points for Self-Supervised RGB-D Registration](https://arxiv.org/abs/2510.14354)
*Siddharth Tourani,Jayaram Reddy,Sarvesh Thakur,K Madhava Krishna,Muhammad Haris Khan,N Dinesh Reddy*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的自监督RGB-D数据配准方法，利用循环一致的关键点和姿态块来提高配准精度。


<details>
  <summary>Details</summary>
Motivation: 如何利用大量的无标签RGB-D数据进行场景的几何推理。

Method: 使用循环一致的关键点作为显著点，并在匹配过程中加强空间一致性约束；引入新的姿态块，结合GRU循环单元和变换同步，融合历史和多视角数据。

Result: 在ScanNet和3DMatch数据集上超过了以往的自监督配准方法，甚至优于一些较早的监督方法；将提出的组件集成到现有方法中，也显示出其有效性。

Conclusion: 该方法有效地利用了无标签RGB-D数据进行场景几何推理，并在配准精度上取得了显著提升。

Abstract: With the rise in consumer depth cameras, a wealth of unlabeled RGB-D data has
become available. This prompts the question of how to utilize this data for
geometric reasoning of scenes. While many RGB-D registration meth- ods rely on
geometric and feature-based similarity, we take a different approach. We use
cycle-consistent keypoints as salient points to enforce spatial coherence
constraints during matching, improving correspondence accuracy. Additionally,
we introduce a novel pose block that combines a GRU recurrent unit with
transformation synchronization, blending historical and multi-view data. Our
approach surpasses previous self- supervised registration methods on ScanNet
and 3DMatch, even outperforming some older supervised methods. We also
integrate our components into existing methods, showing their effectiveness.

</details>


### [76] [Spatial Preference Rewarding for MLLMs Spatial Understanding](https://arxiv.org/abs/2510.14374)
*Han Qiu,Peng Gao,Lewei Lu,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: 本文提出了一种名为 SPR 的空间偏好奖励方法，通过奖励 MLLM 的详细响应和精确的对象定位来增强 MLLM 的空间能力。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要集中于调整 MLLM 来建模预先注释的指令数据以注入空间知识，而没有直接监督 MLLM 的实际响应，导致 MLLM 在细粒度空间感知能力方面存在不足。

Method: SPR 方法通过语义和定位分数来综合评估 MLLM 生成的描述中的文本质量和定位质量。同时，使用更好的定位精度来优化 MLLM 描述，并将最佳评分的优化与最低分数的初始描述配对，以实现直接偏好优化。

Result: 在标准参考和 grounding 基准上的大量实验表明，SPR 可以有效地提高 MLLM 的空间理解能力，且训练开销最小。

Conclusion: SPR 方法有效地提高了 MLLM 的空间理解能力，且训练开销最小。

Abstract: Multimodal large language models~(MLLMs) have demonstrated promising spatial
understanding capabilities, such as referencing and grounding object
descriptions. Despite their successes, MLLMs still fall short in fine-grained
spatial perception abilities, such as generating detailed region descriptions
or accurately localizing objects. Additionally, they often fail to respond to
the user's requirements for desired fine-grained spatial understanding. This
issue might arise because existing approaches primarily focus on tuning MLLMs
to model pre-annotated instruction data to inject spatial knowledge, without
direct supervision of MLLMs' actual responses. We address this issue by SPR, a
Spatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatial
capabilities by rewarding MLLMs' detailed responses with precise object
localization over vague or inaccurate responses. With randomly selected image
regions and region descriptions from MLLMs, SPR introduces semantic and
localization scores to comprehensively evaluate the text quality and
localization quality in MLLM-generated descriptions. We also refine the MLLM
descriptions with better localization accuracy and pair the best-scored
refinement with the initial descriptions of the lowest score for direct
preference optimization, thereby enhancing fine-grained alignment with visual
input. Extensive experiments over standard referring and grounding benchmarks
show that SPR improves MLLM spatial understanding capabilities effectively with
minimal overhead in training. Data and code will be released at
https://github.com/hanqiu-hq/SPR

</details>


### [77] [DOS: Directional Object Separation in Text Embeddings for Multi-Object Image Generation](https://arxiv.org/abs/2510.14376)
*Dongnam Byun,Jungwon Park,Jumgmin Ko,Changin Choi,Wonjong Rhee*

Main category: cs.CV

TL;DR: 本文提出了一种名为 DOS (Directional Object Separation) 的方法，通过修改 CLIP 文本嵌入来改进多对象图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成模型在处理包含多个对象的提示时，经常出现对象忽略或对象混合的问题，尤其是在相似形状、相似纹理、不同背景偏差和多个对象等四种情况下。

Method: 该方法基于对 CLIP 嵌入的两个关键观察，修改三种类型的 CLIP 文本嵌入，然后将其输入到文本到图像模型中。

Result: 实验结果表明，DOS 能够持续提高多对象图像生成的成功率，并减少对象混合。在人工评估中，DOS 显著优于其他四种方法。

Conclusion: DOS 是一种有效且实用的多对象图像生成改进方案。

Abstract: Recent progress in text-to-image (T2I) generative models has led to
significant improvements in generating high-quality images aligned with text
prompts. However, these models still struggle with prompts involving multiple
objects, often resulting in object neglect or object mixing. Through extensive
studies, we identify four problematic scenarios, Similar Shapes, Similar
Textures, Dissimilar Background Biases, and Many Objects, where inter-object
relationships frequently lead to such failures. Motivated by two key
observations about CLIP embeddings, we propose DOS (Directional Object
Separation), a method that modifies three types of CLIP text embeddings before
passing them into text-to-image models. Experimental results show that DOS
consistently improves the success rate of multi-object image generation and
reduces object mixing. In human evaluations, DOS significantly outperforms four
competing methods, receiving 26.24%-43.04% more votes across four benchmarks.
These results highlight DOS as a practical and effective solution for improving
multi-object image generation.

</details>


### [78] [DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with Analytical Insights](https://arxiv.org/abs/2510.14383)
*Danish Ali,Ajmal Mian,Naveed Akhtar,Ghulam Mubashar Hassan*

Main category: cs.CV

TL;DR: 提出了一种高效的3D脑肿瘤分割模型DRBD-Mamba，在保持高分割精度的同时，显著提高了计算效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤子区域的异质性给精确分割带来挑战；基于Mamba的状态空间模型计算开销大，且在不同BraTS数据分区上的鲁棒性有待考察。

Method: 提出了双分辨率双向Mamba (DRBD-Mamba)，利用空间填充曲线减少计算量，提出了门控融合模块和量化块来增强特征表示，并提出了五个系统性的folds来评估分割技术。

Result: 在20%的测试集上，Dice指标在全肿瘤、肿瘤核心和增强肿瘤上分别提高了0.10%、1.75%和0.93%。在五个系统性folds上，肿瘤核心和增强肿瘤的平均Dice增益分别为0.86%和1.45%。效率提高了15倍。

Conclusion: DRBD-Mamba在脑肿瘤分割方面表现出鲁棒性和计算优势。

Abstract: Accurate brain tumor segmentation is significant for clinical diagnosis and
treatment. It is challenging due to the heterogeneity of tumor subregions.
Mamba-based State Space Models have demonstrated promising performance.
However, they incur significant computational overhead due to sequential
feature computation across multiple spatial axes. Moreover, their robustness
across diverse BraTS data partitions remains largely unexplored, leaving a
critical gap in reliable evaluation. To address these limitations, we propose
dual-resolution bi-directional Mamba (DRBD-Mamba), an efficient 3D segmentation
model that captures multi-scale long-range dependencies with minimal
computational overhead. We leverage a space-filling curve to preserve spatial
locality during 3D-to-1D feature mapping, thereby reducing reliance on
computationally expensive multi-axial feature scans. To enrich feature
representation, we propose a gated fusion module that adaptively integrates
forward and reverse contexts, along with a quantization block that discretizes
features to improve robustness. In addition, we propose five systematic folds
on BraTS2023 for rigorous evaluation of segmentation techniques under diverse
conditions and present detailed analysis of common failure scenarios. On the
20\% test set used by recent methods, our model achieves Dice improvements of
0.10\% for whole tumor, 1.75\% for tumor core, and 0.93\% for enhancing tumor.
Evaluations on the proposed systematic five folds demonstrate that our model
maintains competitive whole tumor accuracy while achieving clear average Dice
gains of 0.86\% for tumor core and 1.45\% for enhancing tumor over existing
state-of-the-art. Furthermore, our model attains 15 times improvement in
efficiency while maintaining high segmentation accuracy, highlighting its
robustness and computational advantage over existing approaches.

</details>


### [79] [BoardVision: Deployment-ready and Robust Motherboard Defect Detection with YOLO+Faster-RCNN Ensemble](https://arxiv.org/abs/2510.14389)
*Brandon Hill,Kma Solaiman*

Main category: cs.CV

TL;DR: 这篇论文提出了BoardVision，一个用于检测主板组装缺陷的框架。


<details>
  <summary>Details</summary>
Motivation: 现有的PCB检测主要集中在裸板或trace-level缺陷，而对完整主板的组装级检测仍然不足。

Method: 使用YOLOv7和Faster R-CNN两个代表性检测器在MiracleFactory主板数据集上进行基准测试，并提出了一个轻量级集成方法Confidence-Temporal Voting (CTV Voter)来平衡精度和召回率。

Result: 通过可解释的规则平衡了精度和召回率，并在锐度、亮度、方向变化等扰动下评估了鲁棒性。

Conclusion: 计算机视觉技术可以从基准测试结果过渡到实际的组装级主板制造质量保证。

Abstract: Motherboard defect detection is critical for ensuring reliability in
high-volume electronics manufacturing. While prior research in PCB inspection
has largely targeted bare-board or trace-level defects, assembly-level
inspection of full motherboards inspection remains underexplored. In this work,
we present BoardVision, a reproducible framework for detecting assembly-level
defects such as missing screws, loose fan wiring, and surface scratches. We
benchmark two representative detectors - YOLOv7 and Faster R-CNN, under
controlled conditions on the MiracleFactory motherboard dataset, providing the
first systematic comparison in this domain. To mitigate the limitations of
single models, where YOLO excels in precision but underperforms in recall and
Faster R-CNN shows the reverse, we propose a lightweight ensemble,
Confidence-Temporal Voting (CTV Voter), that balances precision and recall
through interpretable rules. We further evaluate robustness under realistic
perturbations including sharpness, brightness, and orientation changes,
highlighting stability challenges often overlooked in motherboard defect
detection. Finally, we release a deployable GUI-driven inspection tool that
bridges research evaluation with operator usability. Together, these
contributions demonstrate how computer vision techniques can transition from
benchmark results to practical quality assurance for assembly-level motherboard
manufacturing.

</details>


### [80] [DCMIL: A Progressive Representation Learning Model of Whole Slide Images for Cancer Prognosis Analysis](https://arxiv.org/abs/2510.14403)
*Chao Tu,Kun Huang,Jie Zhang,Qianjin Feng,Yu Zhang,Zhenyuan Ning*

Main category: cs.CV

TL;DR: 提出了一种名为DCMIL的易到难渐进式表征学习模型，用于高效处理WSI以进行癌症预后，无需密集注释，可将千兆像素大小的WSI直接转换为结果预测。


<details>
  <summary>Details</summary>
Motivation: 计算病理学在利用全切片图像（WSI）量化形态异质性和开发人类癌症的客观预后模式方面显示出前景。然而，由于千兆像素大小输入的计算瓶颈和密集人工注释的稀缺性，进展受到阻碍。当前的方法通常忽略跨多倍率WSI的细粒度信息和肿瘤微环境的变化。

Method: 提出了一种易到难的渐进式表征学习模型，称为双课程对比多实例学习（DCMIL）。

Result: 在十二种癌症类型（5,954名患者，1254万个切片）上的大量实验表明，DCMIL优于基于WSI的标准预后模型。此外，DCMIL识别出细粒度的预后显着区域，提供稳健的实例不确定性估计，并捕获正常组织和肿瘤组织之间的形态差异，具有产生新的生物学见解的潜力。

Conclusion: DCMIL模型能够有效处理WSI用于癌症预后，无需密集注释，并且能够识别细粒度的预后显着区域，提供稳健的实例不确定性估计，并捕获正常组织和肿瘤组织之间的形态差异，具有产生新的生物学见解的潜力。

Abstract: The burgeoning discipline of computational pathology shows promise in
harnessing whole slide images (WSIs) to quantify morphological heterogeneity
and develop objective prognostic modes for human cancers. However, progress is
impeded by the computational bottleneck of gigapixel-size inputs and the
scarcity of dense manual annotations. Current methods often overlook
fine-grained information across multi-magnification WSIs and variations in
tumor microenvironments. Here, we propose an easy-to-hard progressive
representation learning model, termed dual-curriculum contrastive
multi-instance learning (DCMIL), to efficiently process WSIs for cancer
prognosis. The model does not rely on dense annotations and enables the direct
transformation of gigapixel-size WSIs into outcome predictions. Extensive
experiments on twelve cancer types (5,954 patients, 12.54 million tiles)
demonstrate that DCMIL outperforms standard WSI-based prognostic models.
Additionally, DCMIL identifies fine-grained prognosis-salient regions, provides
robust instance uncertainty estimation, and captures morphological differences
between normal and tumor tissues, with the potential to generate new biological
insights. All codes have been made publicly accessible at
https://github.com/tuuuc/DCMIL.

</details>


### [81] [Real-Time Neural Video Compression with Unified Intra and Inter Coding](https://arxiv.org/abs/2510.14431)
*Hui Xiang,Yifan Bian,Li Li,Jingran Wu,Xianguo Zhang,Dong Liu*

Main category: cs.CV

TL;DR: 提出了一种新的神经视频压缩框架，该框架统一了帧内和帧间编码，并通过同时进行双帧压缩来利用帧间冗余。


<details>
  <summary>Details</summary>
Motivation: 现有的神经视频压缩方案在处理遮挡和新内容、帧间误差传播和累积等方面存在局限性。

Method: 借鉴经典视频编码方案的思想，允许在帧间编码帧内进行帧内编码。提出了一个具有统一帧内和帧间编码的NVC框架，其中每个帧都由一个经过训练以自适应执行帧内/帧间编码的单一模型处理。此外，我们提出了一种同时进行双帧压缩的设计，以向前和向后利用帧间冗余。

Result: 该方案优于DCVC-RT，平均降低了10.7%的BD-rate，提供了更稳定的每帧比特率和质量，并保留了实时编码/解码性能。

Conclusion: 所提出的NVC框架有效地解决了现有方案的局限性，并在压缩效率和性能方面取得了显著提升。

Abstract: Neural video compression (NVC) technologies have advanced rapidly in recent
years, yielding state-of-the-art schemes such as DCVC-RT that offer superior
compression efficiency to H.266/VVC and real-time encoding/decoding
capabilities. Nonetheless, existing NVC schemes have several limitations,
including inefficiency in dealing with disocclusion and new content, interframe
error propagation and accumulation, among others. To eliminate these
limitations, we borrow the idea from classic video coding schemes, which allow
intra coding within inter-coded frames. With the intra coding tool enabled,
disocclusion and new content are properly handled, and interframe error
propagation is naturally intercepted without the need for manual refresh
mechanisms. We present an NVC framework with unified intra and inter coding,
where every frame is processed by a single model that is trained to perform
intra/inter coding adaptively. Moreover, we propose a simultaneous two-frame
compression design to exploit interframe redundancy not only forwardly but also
backwardly. Experimental results show that our scheme outperforms DCVC-RT by an
average of 10.7\% BD-rate reduction, delivers more stable bitrate and quality
per frame, and retains real-time encoding/decoding performances. Code and
models will be released.

</details>


### [82] [Structured Universal Adversarial Attacks on Object Detection for Video Sequences](https://arxiv.org/abs/2510.14460)
*Sven Jacob,Weijia Shao,Gjergji Kasneci*

Main category: cs.CV

TL;DR: 提出了一种针对视频对象检测的最小失真通用对抗攻击，利用核范数正则化来促进集中在背景中的结构化扰动。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习的对象检测器在安全关键应用中起着至关重要的作用。虽然基于深度学习的对象检测器已经取得了令人印象深刻的性能，但它们仍然容易受到对抗性攻击，特别是涉及通用扰动的攻击。

Method: 采用自适应的乐观指数梯度方法来有效地优化这种公式，从而提高可扩展性和收敛性。

Result: 结果表明，所提出的攻击在有效性方面优于基于低秩投影梯度下降和Frank-Wolfe的攻击，同时保持了较高的隐蔽性。

Conclusion: 提出了一种针对视频对象检测的最小失真通用对抗攻击

Abstract: Video-based object detection plays a vital role in safety-critical
applications. While deep learning-based object detectors have achieved
impressive performance, they remain vulnerable to adversarial attacks,
particularly those involving universal perturbations. In this work, we propose
a minimally distorted universal adversarial attack tailored for video object
detection, which leverages nuclear norm regularization to promote structured
perturbations concentrated in the background. To optimize this formulation
efficiently, we employ an adaptive, optimistic exponentiated gradient method
that enhances both scalability and convergence. Our results demonstrate that
the proposed attack outperforms both low-rank projected gradient descent and
Frank-Wolfe based attacks in effectiveness while maintaining high stealthiness.
All code and data are publicly available at
https://github.com/jsve96/AO-Exp-Attack.

</details>


### [83] [Unsupervised Deep Generative Models for Anomaly Detection in Neuroimaging: A Systematic Scoping Review](https://arxiv.org/abs/2510.14462)
*Youwan Mahé,Elise Bannier,Stéphanie Leplaideur,Elisa Fromont,Francesca Galassi*

Main category: cs.CV

TL;DR: 无监督深度生成模型正成为检测和分割脑部影像异常的有前途的替代方法。


<details>
  <summary>Details</summary>
Motivation: 与需要大量体素级注释数据集且仅限于特征明确的病理的完全监督方法不同，这些模型可以专门在健康数据上进行训练，并将异常识别为与学习的规范性脑部结构的偏差。

Method: 本PRISMA指南范围审查综合了最近关于神经影像异常检测的无监督深度生成模型的研究，包括自动编码器、变分自动编码器、生成对抗网络和去噪扩散模型。

Result: 在纳入的研究中，生成模型在大病灶方面取得了令人鼓舞的性能，并在解决更细微的异常方面取得了进展。生成模型的一个关键优势是它们能够产生可解释的伪健康（也称为反事实）重建，这在注释数据稀缺时尤其有价值，例如在罕见或异质疾病中。

Conclusion: 为了实现临床影响，未来的工作应优先考虑解剖学感知建模、基础模型的开发、适合任务的评估指标和严格的临床验证。

Abstract: Unsupervised deep generative models are emerging as a promising alternative
to supervised methods for detecting and segmenting anomalies in brain imaging.
Unlike fully supervised approaches, which require large voxel-level annotated
datasets and are limited to well-characterised pathologies, these models can be
trained exclusively on healthy data and identify anomalies as deviations from
learned normative brain structures. This PRISMA-guided scoping review
synthesises recent work on unsupervised deep generative models for anomaly
detection in neuroimaging, including autoencoders, variational autoencoders,
generative adversarial networks, and denoising diffusion models. A total of 49
studies published between 2018 - 2025 were identified, covering applications to
brain MRI and, less frequently, CT across diverse pathologies such as tumours,
stroke, multiple sclerosis, and small vessel disease. Reported performance
metrics are compared alongside architectural design choices. Across the
included studies, generative models achieved encouraging performance for large
focal lesions and demonstrated progress in addressing more subtle
abnormalities. A key strength of generative models is their ability to produce
interpretable pseudo-healthy (also referred to as counterfactual)
reconstructions, which is particularly valuable when annotated data are scarce,
as in rare or heterogeneous diseases. Looking ahead, these models offer a
compelling direction for anomaly detection, enabling semi-supervised learning,
supporting the discovery of novel imaging biomarkers, and facilitating within-
and cross-disease deviation mapping in unified end-to-end frameworks. To
realise clinical impact, future work should prioritise anatomy-aware modelling,
development of foundation models, task-appropriate evaluation metrics, and
rigorous clinical validation.

</details>


### [84] [Pruning Overparameterized Multi-Task Networks for Degraded Web Image Restoration](https://arxiv.org/abs/2510.14463)
*Thomas Katraouras,Dimitrios Rafailidis*

Main category: cs.CV

TL;DR: 本文提出了一种压缩多任务图像恢复模型的方法，通过迭代剪枝去除低幅度权重，同时将剩余权重重置为原始初始化，以发现高性能的稀疏子网络。


<details>
  <summary>Details</summary>
Motivation: 在线社交网络(OSN)上的有损操作导致图像质量下降，影响用户体验。多任务图像恢复模型因能同时处理不同类型的图像退化而备受关注，但其参数过多，计算效率低下。

Method: 提出了一种名为MIR-L的模型，该模型采用迭代剪枝策略，在多轮中移除低幅度权重，同时将剩余权重重置为原始初始化。

Result: 在去雨、去雾和去噪任务的基准数据集上的实验评估表明，MIR-L仅保留了10%的可训练参数，同时保持了较高的图像恢复性能。

Conclusion: MIR-L模型能够在高稀疏度下保持或超过最先进的性能。

Abstract: Image quality is a critical factor in delivering visually appealing content
on web platforms. However, images often suffer from degradation due to lossy
operations applied by online social networks (OSNs), negatively affecting user
experience. Image restoration is the process of recovering a clean high-quality
image from a given degraded input. Recently, multi-task (all-in-one) image
restoration models have gained significant attention, due to their ability to
simultaneously handle different types of image degradations. However, these
models often come with an excessively high number of trainable parameters,
making them computationally inefficient. In this paper, we propose a strategy
for compressing multi-task image restoration models. We aim to discover highly
sparse subnetworks within overparameterized deep models that can match or even
surpass the performance of their dense counterparts. The proposed model, namely
MIR-L, utilizes an iterative pruning strategy that removes low-magnitude
weights across multiple rounds, while resetting the remaining weights to their
original initialization. This iterative process is important for the multi-task
image restoration model's optimization, effectively uncovering "winning
tickets" that maintain or exceed state-of-the-art performance at high sparsity
levels. Experimental evaluation on benchmark datasets for the deraining,
dehazing, and denoising tasks shows that MIR-L retains only 10% of the
trainable parameters while maintaining high image restoration performance. Our
code, datasets and pre-trained models are made publicly available at
https://github.com/Thomkat/MIR-L.

</details>


### [85] [Grazing Detection using Deep Learning and Sentinel-2 Time Series Data](https://arxiv.org/abs/2510.14493)
*Aleksis Pirinen,Delia Fano Yela,Smita Chakraborty,Erik Källman*

Main category: cs.CV

TL;DR: 本文利用Sentinel-2 L2A时间序列数据，实现了对季节性放牧的检测，并公开了代码和模型。


<details>
  <summary>Details</summary>
Motivation: 大规模监测放牧发生地点仍然有限。

Method: 在多时相反射率特征上训练 CNN-LSTM 模型集成。

Result: 在五个验证集中，平均 F1 得分为 77%，对放牧牧场的召回率为 90%。

Conclusion: 粗分辨率、免费的卫星数据可以可靠地指导检查资源，以实现符合保护目标的土地利用合规性。

Abstract: Grazing shapes both agricultural production and biodiversity, yet scalable
monitoring of where grazing occurs remains limited. We study seasonal grazing
detection from Sentinel-2 L2A time series: for each polygon-defined field
boundary, April-October imagery is used for binary prediction (grazed / not
grazed). We train an ensemble of CNN-LSTM models on multi-temporal reflectance
features, and achieve an average F1 score of 77 percent across five validation
splits, with 90 percent recall on grazed pastures. Operationally, if inspectors
can visit at most 4 percent of sites annually, prioritising fields predicted by
our model as non-grazed yields 17.2 times more confirmed non-grazing sites than
random inspection. These results indicate that coarse-resolution, freely
available satellite data can reliably steer inspection resources for
conservation-aligned land-use compliance. Code and models have been made
publicly available.

</details>


### [86] [Vision Mamba for Permeability Prediction of Porous Media](https://arxiv.org/abs/2510.14516)
*Ali Kashefi,Tapan Mukerji*

Main category: cs.CV

TL;DR: Vision Mamba是一种用于图像分类的新型神经网络，它比Vision Transformers (ViT)和卷积神经网络(CNN)更有效率。


<details>
  <summary>Details</summary>
Motivation: 将Vision Mamba用于预测三维多孔介质的渗透率。

Method: 将Vision Mamba作为骨干网络，并与ViT和CNN模型进行比较，同时进行消融研究。

Result: 证明了Vision Mamba在三维多孔介质渗透率预测方面优于ViT和CNN。

Conclusion: 提出的框架有潜力整合到大型视觉模型中，用Vision Mamba代替ViT。

Abstract: Vision Mamba has recently received attention as an alternative to Vision
Transformers (ViTs) for image classification. The network size of Vision Mamba
scales linearly with input image resolution, whereas ViTs scale quadratically,
a feature that improves computational and memory efficiency. Moreover, Vision
Mamba requires a significantly smaller number of trainable parameters than
traditional convolutional neural networks (CNNs), and thus, they can be more
memory efficient. Because of these features, we introduce, for the first time,
a neural network that uses Vision Mamba as its backbone for predicting the
permeability of three-dimensional porous media. We compare the performance of
Vision Mamba with ViT and CNN models across multiple aspects of permeability
prediction and perform an ablation study to assess the effects of its
components on accuracy. We demonstrate in practice the aforementioned
advantages of Vision Mamba over ViTs and CNNs in the permeability prediction of
three-dimensional porous media. We make the source code publicly available to
facilitate reproducibility and to enable other researchers to build on and
extend this work. We believe the proposed framework has the potential to be
integrated into large vision models in which Vision Mamba is used instead of
ViTs.

</details>


### [87] [Real-Time Surgical Instrument Defect Detection via Non-Destructive Testing](https://arxiv.org/abs/2510.14525)
*Qurrat Ul Ain,Atif Aftab Ahmed Jilani,Zunaira Shafqat,Nigar Azhar Butt*

Main category: cs.CV

TL;DR: SurgScan: AI-powered defect detection for surgical instruments using YOLOv8.


<details>
  <summary>Details</summary>
Motivation: Manual inspection of surgical instruments is error-prone and inconsistent, posing risks to patient safety.

Method: Developed SurgScan, an AI framework using YOLOv8 for real-time defect classification.

Result: SurgScan achieves 99.3% accuracy with 4.2-5.8 ms inference speed, enhanced by contrast-enhanced preprocessing.

Conclusion: SurgScan offers a scalable, cost-effective solution for automated quality control, improving defect detection and ensuring regulatory compliance.

Abstract: Defective surgical instruments pose serious risks to sterility, mechanical
integrity, and patient safety, increasing the likelihood of surgical
complications. However, quality control in surgical instrument manufacturing
often relies on manual inspection, which is prone to human error and
inconsistency. This study introduces SurgScan, an AI-powered defect detection
framework for surgical instruments. Using YOLOv8, SurgScan classifies defects
in real-time, ensuring high accuracy and industrial scalability. The model is
trained on a high-resolution dataset of 102,876 images, covering 11 instrument
types and five major defect categories. Extensive evaluation against
state-of-the-art CNN architectures confirms that SurgScan achieves the highest
accuracy (99.3%) with real-time inference speeds of 4.2-5.8 ms per image,
making it suitable for industrial deployment. Statistical analysis demonstrates
that contrast-enhanced preprocessing significantly improves defect detection,
addressing key limitations in visual inspection. SurgScan provides a scalable,
cost-effective AI solution for automated quality control, reducing reliance on
manual inspection while ensuring compliance with ISO 13485 and FDA standards,
paving the way for enhanced defect detection in medical manufacturing.

</details>


### [88] [Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models](https://arxiv.org/abs/2510.14526)
*Yunze Tong,Didi Zhu,Zijing Hu,Jinluan Yang,Ziyu Zhao*

Main category: cs.CV

TL;DR: 提出了一种噪声投影器，以解决文本到图像生成中训练-推理不匹配的问题，该投影器在去噪前对初始噪声进行文本条件细化。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成方法生成的图像可能与prompt不一致，因为推理时噪声是从prompt-agnostic高斯先验中提取的，这与训练时prompt-conditioned噪声位于prompt-specific子集的情况不符。

Method: 提出了一种噪声投影器，该投影器在去噪之前将噪声映射到prompt-aware的对应物，使其更好地匹配SD训练期间观察到的分布。该框架包括以下步骤：首先，对一些噪声进行采样，并从视觉语言模型（VLM）获得其对应图像的token-level反馈，然后将这些信号提炼成奖励模型，最后通过准直接偏好优化来优化噪声投影器。

Result: 大量实验表明，我们的prompt-aware噪声投影提高了各种prompt下的文本图像对齐。

Conclusion: 该设计不需要参考图像或手工制作的先验知识，并且只需少量推理成本，用单次前向传递代替了多样本选择。

Abstract: In text-to-image generation, different initial noises induce distinct
denoising paths with a pretrained Stable Diffusion (SD) model. While this
pattern could output diverse images, some of them may fail to align well with
the prompt. Existing methods alleviate this issue either by altering the
denoising dynamics or by drawing multiple noises and conducting post-selection.
In this paper, we attribute the misalignment to a training-inference mismatch:
during training, prompt-conditioned noises lie in a prompt-specific subset of
the latent space, whereas at inference the noise is drawn from a
prompt-agnostic Gaussian prior. To close this gap, we propose a noise projector
that applies text-conditioned refinement to the initial noise before denoising.
Conditioned on the prompt embedding, it maps the noise to a prompt-aware
counterpart that better matches the distribution observed during SD training,
without modifying the SD model. Our framework consists of these steps: we first
sample some noises and obtain token-level feedback for their corresponding
images from a vision-language model (VLM), then distill these signals into a
reward model, and finally optimize the noise projector via a quasi-direct
preference optimization. Our design has two benefits: (i) it requires no
reference images or handcrafted priors, and (ii) it incurs small inference
cost, replacing multi-sample selection with a single forward pass. Extensive
experiments further show that our prompt-aware noise projection improves
text-image alignment across diverse prompts.

</details>


### [89] [PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model](https://arxiv.org/abs/2510.14528)
*Cheng Cui,Ting Sun,Suyin Liang,Tingquan Gao,Zelun Zhang,Jiaxuan Liu,Xueqing Wang,Changda Zhou,Hongen Liu,Manhui Lin,Yue Zhang,Yubo Zhang,Handong Zheng,Jing Zhang,Jun Zhang,Yi Liu,Dianhai Yu,Yanjun Ma*

Main category: cs.CV

TL;DR: PaddleOCR-VL is a resource-efficient model for document parsing.


<details>
  <summary>Details</summary>
Motivation: To create a compact yet powerful vision-language model (VLM) for accurate element recognition in documents.

Method: Integrating a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model.

Result: Achieves SOTA performance in both page-level document parsing and element-level recognition, outperforming existing solutions.

Conclusion: PaddleOCR-VL is highly suitable for practical deployment in real-world scenarios due to its performance, efficiency, and speed.

Abstract: In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model
tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a
compact yet powerful vision-language model (VLM) that integrates a NaViT-style
dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to
enable accurate element recognition. This innovative model efficiently supports
109 languages and excels in recognizing complex elements (e.g., text, tables,
formulas, and charts), while maintaining minimal resource consumption. Through
comprehensive evaluations on widely used public benchmarks and in-house
benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document
parsing and element-level recognition. It significantly outperforms existing
solutions, exhibits strong competitiveness against top-tier VLMs, and delivers
fast inference speeds. These strengths make it highly suitable for practical
deployment in real-world scenarios.

</details>


### [90] [Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology](https://arxiv.org/abs/2510.14532)
*Xinrui Huang,Fan Xiao,Dongming He,Anqi Gao,Dandan Li,Xiaofan Zhang,Shaoting Zhang,Xudong Wang*

Main category: cs.CV

TL;DR: DentVFM: 首个牙科视觉基础模型，通过自监督学习和多模态数据，在多种牙科任务中表现出色，超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有牙科AI系统受限于单模态、任务特定和依赖昂贵标注数据，难以泛化到不同临床场景。

Method: 提出DentVFM，一个基于Vision Transformer (ViT) 的视觉基础模型家族，使用在包含约160万多模态牙科影像的DentVista数据集上进行自监督学习。

Result: DentVFM 在疾病诊断、治疗分析、生物标志物识别和解剖标志物检测与分割等多种牙科任务中表现出强大的泛化能力，显著优于现有基线模型，并在跨模态诊断中表现出超越经验丰富的牙医的性能。

Conclusion: DentVFM 为牙科AI设立了新范例，提供可扩展、适应性强和标签效率高的模型，以改善智能牙科保健并解决全球口腔保健中的关键差距。

Abstract: Oral and maxillofacial radiology plays a vital role in dental healthcare, but
radiographic image interpretation is limited by a shortage of trained
professionals. While AI approaches have shown promise, existing dental AI
systems are restricted by their single-modality focus, task-specific design,
and reliance on costly labeled data, hindering their generalization across
diverse clinical scenarios. To address these challenges, we introduce DentVFM,
the first family of vision foundation models (VFMs) designed for dentistry.
DentVFM generates task-agnostic visual representations for a wide range of
dental applications and uses self-supervised learning on DentVista, a large
curated dental imaging dataset with approximately 1.6 million multi-modal
radiographic images from various medical centers. DentVFM includes 2D and 3D
variants based on the Vision Transformer (ViT) architecture. To address gaps in
dental intelligence assessment and benchmarks, we introduce DentBench, a
comprehensive benchmark covering eight dental subspecialties, more diseases,
imaging modalities, and a wide geographical distribution. DentVFM shows
impressive generalist intelligence, demonstrating robust generalization to
diverse dental tasks, such as disease diagnosis, treatment analysis, biomarker
identification, and anatomical landmark detection and segmentation.
Experimental results indicate DentVFM significantly outperforms supervised,
self-supervised, and weakly supervised baselines, offering superior
generalization, label efficiency, and scalability. Additionally, DentVFM
enables cross-modality diagnostics, providing more reliable results than
experienced dentists in situations where conventional imaging is unavailable.
DentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and
label-efficient model to improve intelligent dental healthcare and address
critical gaps in global oral healthcare.

</details>


### [91] [Exploring Image Representation with Decoupled Classical Visual Descriptors](https://arxiv.org/abs/2510.14536)
*Chenyuan Qu,Hao Chen,Jianbo Jiao*

Main category: cs.CV

TL;DR: 本文提出了一种名为VisualSplit的框架，它将图像分解为解耦的经典描述符，并学习捕获每个描述符的本质，同时保持其可解释性。通过显式分解视觉属性，该方法能够有效地控制各种高级视觉任务中的属性，例如图像生成和编辑。


<details>
  <summary>Details</summary>
Motivation: 深度学习在图像理解任务中取得了显著进展，但其内部表示通常是不透明的，使得难以解释视觉信息的处理方式。相比之下，经典视觉描述符（例如，边缘、颜色和强度分布）长期以来一直是图像分析的基础，并且对人类来说仍然是直观可理解的。因此，我们提出一个问题：现代学习能否从这些经典线索中受益？

Method: 本文通过重建驱动的预训练方案，VisualSplit学习捕获每个视觉描述符的本质，同时保持其可解释性。

Result: 通过显式分解视觉属性，该方法能够有效地控制各种高级视觉任务中的属性，例如图像生成和编辑。

Conclusion: 本文提出的VisualSplit框架通过显式分解视觉属性，能够有效地控制各种高级视觉任务中的属性，例如图像生成和编辑，表明这种新的学习方法对于视觉理解的有效性。

Abstract: Exploring and understanding efficient image representations is a
long-standing challenge in computer vision. While deep learning has achieved
remarkable progress across image understanding tasks, its internal
representations are often opaque, making it difficult to interpret how visual
information is processed. In contrast, classical visual descriptors (e.g. edge,
colour, and intensity distribution) have long been fundamental to image
analysis and remain intuitively understandable to humans. Motivated by this
gap, we ask a central question: Can modern learning benefit from these
classical cues? In this paper, we answer it with VisualSplit, a framework that
explicitly decomposes images into decoupled classical descriptors, treating
each as an independent but complementary component of visual knowledge. Through
a reconstruction-driven pre-training scheme, VisualSplit learns to capture the
essence of each visual descriptor while preserving their interpretability. By
explicitly decomposing visual attributes, our method inherently facilitates
effective attribute control in various advanced visual tasks, including image
generation and editing, extending beyond conventional classification and
segmentation, suggesting the effectiveness of this new learning approach for
visual understanding. Project page: https://chenyuanqu.com/VisualSplit/.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [92] [Decision Oriented Technique (DOTechnique): Finding Model Validity Through Decision-Maker Context](https://arxiv.org/abs/2510.13858)
*Raheleh Biglari,Joachim Denil*

Main category: cs.AI

TL;DR: 提出了一种新的模型验证方法，该方法通过决策一致性而不是输出相似性来确定模型有效性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖于预定义的有效性框架，但这些框架并不总是可用或充分。本文旨在解决这个问题。

Method: 引入了一种面向决策的技术 (DOTechnique)，通过评估代理模型是否能做出与高保真模型等效的决策来确定模型有效性。

Result: 通过高速公路变道系统作为例子，验证了该方法的有效性，结果表明该技术可以通过决策者的上下文来支持寻找模型有效性。

Conclusion: 该技术在支持通过决策者上下文寻找模型有效性方面具有潜力。

Abstract: Model validity is as critical as the model itself, especially when guiding
decision-making processes. Traditional approaches often rely on predefined
validity frames, which may not always be available or sufficient. This paper
introduces the Decision Oriented Technique (DOTechnique), a novel method for
determining model validity based on decision consistency rather than output
similarity. By evaluating whether surrogate models lead to equivalent decisions
compared to high-fidelity models, DOTechnique enables efficient identification
of validity regions, even in the absence of explicit validity boundaries. The
approach integrates domain constraints and symbolic reasoning to narrow the
search space, enhancing computational efficiency. A highway lane change system
serves as a motivating example, demonstrating how DOTechnique can uncover the
validity region of a simulation model. The results highlight the potential of
the technique to support finding model validity through decision-maker context.

</details>


### [93] [Do Slides Help? Multi-modal Context for Automatic Transcription of Conference Talks](https://arxiv.org/abs/2510.13979)
*Supriti Sinhamahapatra,Jan Niehues*

Main category: cs.AI

TL;DR: 该论文提出了一种多模态自动语音识别系统，该系统集成了演讲幻灯片以提高识别精度。


<details>
  <summary>Details</summary>
Motivation: 现有的语音识别系统主要依赖于声音信息，忽略了视觉信息。而视觉信息在消除歧义和适应环境方面至关重要。本文着重于整合演示幻灯片，用于科学演示的场景。

Method: 1. 创建多模态演示基准，包括特定领域术语的自动转录分析。2. 探索用多模态信息增强语音模型的方法。3. 通过适当的数据增强方法，缓解缺乏带幻灯片的语音数据集的问题。4. 使用增强的数据集训练模型。

Result: 与基线模型相比，单词错误率相对降低了约 34%，对于特定领域的术语，错误率降低了 35%。

Conclusion: 该研究表明，结合演示幻灯片的多模态自动语音识别系统可以显著提高识别精度，尤其是在特定领域术语的识别方面。

Abstract: State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily
rely on acoustic information while disregarding additional multi-modal context.
However, visual information are essential in disambiguation and adaptation.
While most work focus on speaker images to handle noise conditions, this work
also focuses on integrating presentation slides for the use cases of scientific
presentation.
  In a first step, we create a benchmark for multi-modal presentation including
an automatic analysis of transcribing domain-specific terminology. Next, we
explore methods for augmenting speech models with multi-modal information. We
mitigate the lack of datasets with accompanying slides by a suitable approach
of data augmentation. Finally, we train a model using the augmented dataset,
resulting in a relative reduction in word error rate of approximately 34%,
across all words and 35%, for domain-specific terms compared to the baseline
model.

</details>


### [94] [Do Large Language Models Show Biases in Causal Learning? Insights from Contingency Judgment](https://arxiv.org/abs/2510.13985)
*María Victoria Carro,Denise Alejandra Mester,Francisca Gauna Selasco,Giovanni Franco Gabriel Marraffini,Mario Alejandro Leiva,Gerardo I. Simari,María Vanina Martinez*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）在缺乏证据的情况下，容易产生因果错觉，即错误地感知变量之间的因果关系。


<details>
  <summary>Details</summary>
Motivation: 因果错觉是社会偏见、刻板印象、错误信息和迷信思维等许多社会问题的潜在根源。本文研究大型语言模型是否容易产生因果错觉。

Method: 构建了一个包含 1,000 个医疗背景下的零 contingency 场景的数据集，并提示 LLM 评估潜在原因的有效性。

Result: 所有评估的模型系统性地推断出无根据的因果关系，表明其极易受到因果错觉的影响。

Conclusion: 研究结果支持了“LLM 仅仅是重现因果语言，而没有真正理解因果关系”的假设，并对在需要准确因果推理的领域中使用语言模型提出了担忧。

Abstract: Causal learning is the cognitive process of developing the capability of
making causal inferences based on available information, often guided by
normative principles. This process is prone to errors and biases, such as the
illusion of causality, in which people perceive a causal relationship between
two variables despite lacking supporting evidence. This cognitive bias has been
proposed to underlie many societal problems, including social prejudice,
stereotype formation, misinformation, and superstitious thinking. In this work,
we examine whether large language models are prone to developing causal
illusions when faced with a classic cognitive science paradigm: the contingency
judgment task. To investigate this, we constructed a dataset of 1,000 null
contingency scenarios (in which the available information is not sufficient to
establish a causal relationship between variables) within medical contexts and
prompted LLMs to evaluate the effectiveness of potential causes. Our findings
show that all evaluated models systematically inferred unwarranted causal
relationships, revealing a strong susceptibility to the illusion of causality.
While there is ongoing debate about whether LLMs genuinely understand causality
or merely reproduce causal language without true comprehension, our findings
support the latter hypothesis and raise concerns about the use of language
models in domains where accurate causal reasoning is essential for informed
decision-making.

</details>


### [95] [GammaZero: Learning To Guide POMDP Belief Space Search With Graph Representations](https://arxiv.org/abs/2510.14035)
*Rajesh Mangannavar,Prasad Tadepalli*

Main category: cs.AI

TL;DR: 提出了一种以行动为中心的图表示框架，用于学习指导部分可观察马尔可夫决策过程 (POMDP) 中的规划。


<details>
  <summary>Details</summary>
Motivation: 现有的方法需要特定于领域的神经架构，并且难以扩展。

Method: GammaZero 利用统一的基于图的信念表示，能够在一个领域内跨问题大小进行泛化。信念状态可以系统地转换为以行动为中心的图，其中在小问题上学习的结构模式可以转移到更大的实例。

Result: 在标准 POMDP 基准上的实验结果表明，当在相同大小的问题上进行训练和测试时，GammaZero 实现了与 BetaZero 相当的性能，同时实现了对 2-4 倍于训练规模的问题的零样本泛化，并在减少搜索需求的同时保持了解决方案的质量。

Conclusion: GammaZero 在 POMDP 上实现了可扩展的零样本泛化。

Abstract: We introduce an action-centric graph representation framework for learning to
guide planning in Partially Observable Markov Decision Processes (POMDPs).
Unlike existing approaches that require domain-specific neural architectures
and struggle with scalability, GammaZero leverages a unified graph-based belief
representation that enables generalization across problem sizes within a
domain. Our key insight is that belief states can be systematically transformed
into action-centric graphs where structural patterns learned on small problems
transfer to larger instances. We employ a graph neural network with a decoder
architecture to learn value functions and policies from expert demonstrations
on computationally tractable problems, then apply these learned heuristics to
guide Monte Carlo tree search on larger problems. Experimental results on
standard POMDP benchmarks demonstrate that GammaZero achieves comparable
performance to BetaZero when trained and tested on the same-sized problems,
while uniquely enabling zero-shot generalization to problems 2-4 times larger
than those seen during training, maintaining solution quality with reduced
search requirements.

</details>


### [96] [Position: Require Frontier AI Labs To Release Small "Analog" Models](https://arxiv.org/abs/2510.14053)
*Shriyash Upadhyay,Chaithanya Bandi,Narmeen Oozeer,Philip Quirke*

Main category: cs.AI

TL;DR: 本文提出了一种新的监管大型人工智能模型的方法，该方法既能确保人工智能安全，又能积极促进创新。


<details>
  <summary>Details</summary>
Motivation: 目前对前沿人工智能模型的监管提案引发了人们对安全监管成本的担忧，并且大多数此类监管由于安全与创新之间的权衡而被搁置。本文旨在找到一种替代的监管方法。

Method: 要求大型人工智能实验室发布小型的、可公开访问的模拟模型（缩小版本），这些模型经过类似训练，并从其最大的专有模型中提炼出来。

Result: 模拟模型充当公共代理，允许广泛参与安全验证、可解释性研究和算法透明度，而无需实验室披露其完整规模的模型。最近的研究表明，使用这些较小模型开发的安全性和可解释性方法可以有效地推广到前沿规模的系统。

Conclusion: 这种强制措施承诺以最小的额外成本，利用可重复使用的资源（如数据和基础设施），同时为公共利益做出重大贡献。更深入地理解模型可以缓解安全与创新之间的权衡，并让我们两者兼得。

Abstract: Recent proposals for regulating frontier AI models have sparked concerns
about the cost of safety regulation, and most such regulations have been
shelved due to the safety-innovation tradeoff. This paper argues for an
alternative regulatory approach that ensures AI safety while actively promoting
innovation: mandating that large AI laboratories release small, openly
accessible analog models (scaled-down versions) trained similarly to and
distilled from their largest proprietary models.
  Analog models serve as public proxies, allowing broad participation in safety
verification, interpretability research, and algorithmic transparency without
forcing labs to disclose their full-scale models. Recent research demonstrates
that safety and interpretability methods developed using these smaller models
generalize effectively to frontier-scale systems. By enabling the wider
research community to directly investigate and innovate upon accessible
analogs, our policy substantially reduces the regulatory burden and accelerates
safety advancements.
  This mandate promises minimal additional costs, leveraging reusable resources
like data and infrastructure, while significantly contributing to the public
good. Our hope is not only that this policy be adopted, but that it illustrates
a broader principle supporting fundamental research in machine learning: deeper
understanding of models relaxes the safety-innovation tradeoff and lets us have
more of both.

</details>


### [97] [Generating Fair Consensus Statements with Social Choice on Token-Level MDPs](https://arxiv.org/abs/2510.14106)
*Carter Blair,Kate Larson*

Main category: cs.AI

TL;DR: 提出了一种新的共识声明生成框架，该框架利用多目标 Markov 决策过程 (MDP) 模拟任务，并结合社会选择理论，以确保在聚合不同意见时提供可证明的公平性保证。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大型语言模型的共识声明生成框架缺乏内在结构，难以在聚合不同的自由形式意见时提供可证明的公平性保证。

Method: 将任务建模为多目标、token 级别的 Markov 决策过程 (MDP)，其中每个目标对应于一个 agent 的偏好。Token 级别的奖励从 agent 的策略中获得，并利用策略隐式定义最优 Q 函数的发现来量化奖励。基于社会选择理论，提出了两种方法：一是提出了一种随机生成策略，保证在事前核心中，扩展了投票理论中的核心稳定性概念到文本生成；二是针对生成单个语句，使用 MDP 框架内的搜索算法，目标是最大化平均主义福利。

Result: 实验结果表明，使用语言模型实例化 agent 策略时，由平均主义目标引导的搜索生成的共识声明，与基线方法相比，在最坏情况下的 agent 对齐方面有所改进。

Conclusion: 该研究提出了一种有原则的共识声明生成方法，该方法结合了 Markov 决策过程和社会选择理论，能够在聚合不同意见时提供更好的公平性保证。

Abstract: Current frameworks for consensus statement generation with large language
models lack the inherent structure needed to provide provable fairness
guarantees when aggregating diverse free-form opinions. We model the task as a
multi-objective, token-level Markov Decision Process (MDP), where each
objective corresponds to an agent's preference. Token-level rewards for each
agent are derived from their policy (e.g., a personalized language model). This
approach utilizes the finding that such policies implicitly define optimal
Q-functions, providing a principled way to quantify rewards at each generation
step without a value function (Rafailov et al., 2024). This MDP formulation
creates a formal structure amenable to analysis using principles from social
choice theory. We propose two approaches grounded in social choice theory.
First, we propose a stochastic generation policy guaranteed to be in the
ex-ante core, extending core stability concepts from voting theory to text
generation. This policy is derived from an underlying distribution over
complete statements that maximizes proportional fairness (Nash Welfare).
Second, for generating a single statement, we target the maximization of
egalitarian welfare using search algorithms within the MDP framework.
Empirically, experiments using language models to instantiate agent policies
show that search guided by the egalitarian objective generates consensus
statements with improved worst-case agent alignment compared to baseline
methods, including the Habermas Machine (Tessler et al., 2024).

</details>


### [98] [STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management](https://arxiv.org/abs/2510.14112)
*Huiliang Zhang,Di Wu,Arnaud Zinflou,Benoit Boulet*

Main category: cs.AI

TL;DR: 提出了一种名为STEMS的、用于协同建筑能源管理的、具有安全约束的多智能体强化学习框架。


<details>
  <summary>Details</summary>
Motivation: 当前的建筑能源管理系统在利用时空信息、缺乏严格的安全保证和系统复杂性方面面临挑战。

Method: 集成了两个核心组件：(1) 使用GCN-Transformer融合架构的时空图表示学习框架，以捕获建筑物间的关系和时间模式；(2) 结合控制障碍函数以提供数学安全保证的安全约束多智能体RL算法。

Result: 在真实建筑数据集上的大量实验表明，STEMS的性能优于现有方法，成本降低21%，排放降低18%，安全违规从35.1%显著降低到5.6%，同时以仅0.13的不适比例保持最佳舒适度。

Conclusion: 该框架在极端天气条件下表现出强大的鲁棒性，并在不同建筑类型中保持有效性。

Abstract: Building energy management is essential for achieving carbon reduction goals,
improving occupant comfort, and reducing energy costs. Coordinated building
energy management faces critical challenges in exploiting spatial-temporal
dependencies while ensuring operational safety across multi-building systems.
Current multi-building energy systems face three key challenges: insufficient
spatial-temporal information exploitation, lack of rigorous safety guarantees,
and system complexity. This paper proposes Spatial-Temporal Enhanced Safe
Multi-Agent Coordination (STEMS), a novel safety-constrained multi-agent
reinforcement learning framework for coordinated building energy management.
STEMS integrates two core components: (1) a spatial-temporal graph
representation learning framework using a GCN-Transformer fusion architecture
to capture inter-building relationships and temporal patterns, and (2) a
safety-constrained multi-agent RL algorithm incorporating Control Barrier
Functions to provide mathematical safety guarantees. Extensive experiments on
real-world building datasets demonstrate STEMS's superior performance over
existing methods, showing that STEMS achieves 21% cost reduction, 18% emission
reduction, and dramatically reduces safety violations from 35.1% to 5.6% while
maintaining optimal comfort with only 0.13 discomfort proportion. The framework
also demonstrates strong robustness during extreme weather conditions and
maintains effectiveness across different building types.

</details>


### [99] [Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems](https://arxiv.org/abs/2510.14133)
*Edoardo Allegrini,Ananth Shreekumar,Z. Berkay Celik*

Main category: cs.AI

TL;DR: This paper introduces a unified framework for analyzing multi-agent AI systems, composed of a host agent model and a task lifecycle model, to enable formal verification and prevent coordination issues.


<details>
  <summary>Details</summary>
Motivation: The current fragmented inter-agent communication protocols hinder rigorous analysis of system properties and introduce risks like architectural misalignment and exploitable coordination issues.

Method: The authors introduce a modeling framework with a host agent model and a task lifecycle model. They define 17 properties for the host agent and 14 for the task lifecycle, expressed in temporal logic.

Result: The framework enables formal verification of system behavior, detection of coordination edge cases, and prevention of deadlocks and security vulnerabilities.

Conclusion: The paper presents a domain-agnostic framework for the systematic analysis, design, and deployment of correct, reliable, and robust agentic AI systems.

Abstract: Agentic AI systems, which leverage multiple autonomous agents and Large
Language Models (LLMs), are increasingly used to address complex, multi-step
tasks. The safety, security, and functionality of these systems are critical,
especially in high-stakes applications. However, the current ecosystem of
inter-agent communication is fragmented, with protocols such as the Model
Context Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol
for coordination being analyzed in isolation. This fragmentation creates a
semantic gap that prevents the rigorous analysis of system properties and
introduces risks such as architectural misalignment and exploitable
coordination issues. To address these challenges, we introduce a modeling
framework for agentic AI systems composed of two foundational models. The
first, the host agent model, formalizes the top-level entity that interacts
with the user, decomposes tasks, and orchestrates their execution by leveraging
external agents and tools. The second, the task lifecycle model, details the
states and transitions of individual sub-tasks from creation to completion,
providing a fine-grained view of task management and error handling. Together,
these models provide a unified semantic framework for reasoning about the
behavior of multi-AI agent systems. Grounded in this framework, we define 17
properties for the host agent and 14 for the task lifecycle, categorized into
liveness, safety, completeness, and fairness. Expressed in temporal logic,
these properties enable formal verification of system behavior, detection of
coordination edge cases, and prevention of deadlocks and security
vulnerabilities. Through this effort, we introduce the first rigorously
grounded, domain-agnostic framework for the systematic analysis, design, and
deployment of correct, reliable, and robust agentic AI systems.

</details>


### [100] [A Multimodal Approach to Heritage Preservation in the Context of Climate Change](https://arxiv.org/abs/2510.14136)
*David Roqui,Adèle Cormier,nistor Grozavu,Ann Bourges*

Main category: cs.AI

TL;DR: 本文提出了一种轻量级多模态架构，它融合了传感器数据（温度、湿度）与视觉图像，以预测遗产地点的退化程度。


<details>
  <summary>Details</summary>
Motivation: 传统监测依赖于单模态分析，无法捕捉环境压力与材料退化之间的复杂相互作用。气候变化加速了文化遗产地点的退化。

Method: 该方法改进了 PerceiverIO，具有两个关键创新：(1) 简化的编码器（64D 潜在空间），可防止在小型数据集上过度拟合（n=37 个训练样本）；(2) 自适应 Barlow Twins 损失，鼓励模态互补性而非冗余。

Result: 在斯特拉斯堡大教堂的数据上，该模型达到了 76.9% 的准确率，比标准多模态架构（VisualBERT、Transformer）提高了 43%，比原始 PerceiverIO 提高了 25%。

Conclusion: 这项工作表明，架构简单性与对比正则化相结合，可以在数据稀缺的遗产监测环境中实现有效的多模态学习，为 AI 驱动的保护决策支持系统奠定基础。

Abstract: Cultural heritage sites face accelerating degradation due to climate change,
yet tradi- tional monitoring relies on unimodal analysis (visual inspection or
environmental sen- sors alone) that fails to capture the complex interplay
between environmental stres- sors and material deterioration. We propose a
lightweight multimodal architecture that fuses sensor data (temperature,
humidity) with visual imagery to predict degradation severity at heritage
sites. Our approach adapts PerceiverIO with two key innovations: (1) simplified
encoders (64D latent space) that prevent overfitting on small datasets (n=37
training samples), and (2) Adaptive Barlow Twins loss that encourages modality
complementarity rather than redundancy. On data from Strasbourg Cathedral, our
model achieves 76.9% accu- racy, a 43% improvement over standard multimodal
architectures (VisualBERT, Trans- former) and 25% over vanilla PerceiverIO.
Ablation studies reveal that sensor-only achieves 61.5% while image-only
reaches 46.2%, confirming successful multimodal synergy. A systematic
hyperparameter study identifies an optimal moderate correlation target ({\tau}
=0.3) that balances align- ment and complementarity, achieving 69.2% accuracy
compared to other {\tau} values ({\tau} =0.1/0.5/0.7: 53.8%, {\tau} =0.9:
61.5%). This work demonstrates that architectural sim- plicity combined with
contrastive regularization enables effective multimodal learning in data-scarce
heritage monitoring contexts, providing a foundation for AI-driven con-
servation decision support systems.

</details>


### [101] [CodeEvolve: An open source evolutionary coding agent for algorithm discovery and optimization](https://arxiv.org/abs/2510.14150)
*Henrique Assumpção,Diego Ferreira,Leandro Campos,Fabricio Murai*

Main category: cs.AI

TL;DR: CodeEvolve是一个开源的进化编码代理，它结合了大型语言模型（LLM）和遗传算法来解决复杂的计算问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决复杂的计算问题，并构建在最近的广义科学发现方法之上，将强大的进化概念应用到LLM领域。

Method: 采用基于岛屿的遗传算法来维持种群多样性并提高吞吐量，引入了一种新颖的基于灵感的交叉机制，该机制利用LLM的上下文窗口来组合来自成功解决方案的特征，并实施元提示策略以动态探索解决方案空间。

Result: 在用于评估Google DeepMind的闭源AlphaEvolve的数学基准测试的子集上，我们对CodeEvolve进行了严格的评估。我们的发现表明，我们的方法在几个具有挑战性的问题上超过了AlphaEvolve的性能。

Conclusion: 我们发布了完整的框架作为一个开源存储库，以促进协作并加速进步。

Abstract: In this work, we introduce CodeEvolve, an open-source evolutionary coding
agent that unites Large Language Models (LLMs) with genetic algorithms to solve
complex computational problems. Our framework adapts powerful evolutionary
concepts to the LLM domain, building upon recent methods for generalized
scientific discovery. CodeEvolve employs an island-based genetic algorithm to
maintain population diversity and increase throughput, introduces a novel
inspiration-based crossover mechanism that leverages the LLMs context window to
combine features from successful solutions, and implements meta-prompting
strategies for dynamic exploration of the solution space. We conduct a rigorous
evaluation of CodeEvolve on a subset of the mathematical benchmarks used to
evaluate Google DeepMind's closed-source AlphaEvolve. Our findings show that
our method surpasses AlphaEvolve's performance on several challenging problems.
To foster collaboration and accelerate progress, we release our complete
framework as an open-source repository.

</details>


### [102] [Combining Reinforcement Learning and Behavior Trees for NPCs in Video Games with AMD Schola](https://arxiv.org/abs/2510.14154)
*Tian Liu,Alex Cann,Ian Colbert,Mehdi Saeedi*

Main category: cs.AI

TL;DR: 本研究探讨了强化学习（RL）在商业视频游戏中应用缓慢的问题，并强调了RL与传统行为树（BTs）的结合是关键的突破口。


<details>
  <summary>Details</summary>
Motivation: 探讨在商业视频游戏中采用RL驱动的NPC时，游戏AI社区面临的常见挑战。

Method: 通过AMD Schola插件，在虚幻引擎中创建多任务NPC，并在受商业视频游戏“The Last of Us”启发的复杂3D环境中，论证了该方法的可行性。

Result: 展示了联合训练RL模型与BTs的各种技能的详细方法。

Conclusion: 论证了BT+RL结合方法在复杂游戏环境中的可行性，并为游戏AI社区提供了有价值的实践指导。

Abstract: While the rapid advancements in the reinforcement learning (RL) research
community have been remarkable, the adoption in commercial video games remains
slow. In this paper, we outline common challenges the Game AI community faces
when using RL-driven NPCs in practice, and highlight the intersection of RL
with traditional behavior trees (BTs) as a crucial juncture to be explored
further. Although the BT+RL intersection has been suggested in several research
papers, its adoption is rare. We demonstrate the viability of this approach
using AMD Schola -- a plugin for training RL agents in Unreal Engine -- by
creating multi-task NPCs in a complex 3D environment inspired by the commercial
video game ``The Last of Us". We provide detailed methodologies for jointly
training RL models with BTs while showcasing various skills.

</details>


### [103] [JEDA: Query-Free Clinical Order Search from Ambient Dialogues](https://arxiv.org/abs/2510.14169)
*Praphul Singh,Corey Barrett,Sumana Srivasta,Amitabh Saikia,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: JEDA是一个快速、可解释的、无需LLM的检索层，它将环境上下文与可操作的临床医嘱实时连接起来。


<details>
  <summary>Details</summary>
Motivation: 许多系统依赖于LLM重写，这增加了延迟、不稳定性和不透明性，阻碍了实时排序。

Method: JEDA（用于直接和环境临床医嘱的联合嵌入）是一种域初始化的双编码器，它可以直接检索规范医嘱，并在无查询模式下，对环境对话的短滚动窗口进行编码，以触发检索。

Result: JEDA在实践中产生了巨大的收益，并且大大优于其基本编码器和最近的开放嵌入器。

Conclusion: JEDA是一种快速、可解释的、无需LLM的检索层，它将环境上下文与可操作的临床医嘱实时连接起来。

Abstract: Clinical conversations mix explicit directives (order a chest X-ray) with
implicit reasoning (the cough worsened overnight, we should check for
pneumonia). Many systems rely on LLM rewriting, adding latency, instability,
and opacity that hinder real-time ordering. We present JEDA (Joint Embedding
for Direct and Ambient clinical orders), a domain-initialized bi-encoder that
retrieves canonical orders directly and, in a query-free mode, encodes a short
rolling window of ambient dialogue to trigger retrieval. Initialized from
PubMedBERT and fine-tuned with a duplicate-safe contrastive objective, JEDA
aligns heterogeneous expressions of intent to shared order concepts. Training
uses constrained LLM guidance to tie each signed order to complementary
formulations (command only, context only, command+context, context+reasoning),
producing clearer inter-order separation, tighter query extendash order
coupling, and stronger generalization. The query-free mode is noise-resilient,
reducing sensitivity to disfluencies and ASR errors by conditioning on a short
window rather than a single utterance. Deployed in practice, JEDA yields large
gains and substantially outperforms its base encoder and recent open embedders
(Linq Embed Mistral, SFR Embedding, GTE Qwen, BGE large, Embedding Gemma). The
result is a fast, interpretable, LLM-free retrieval layer that links ambient
context to actionable clinical orders in real time.

</details>


### [104] [ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning](https://arxiv.org/abs/2510.14176)
*Roger Creus Castanyer,Faisal Mohamed,Pablo Samuel Castro,Cyrus Neary,Glen Berseth*

Main category: cs.AI

TL;DR: 提出了一种名为 ARM-FM 的框架，该框架利用基础模型的推理能力自动进行 RL 中的组合奖励设计。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法对奖励函数规范高度敏感，这仍然是限制其广泛适用性的一个中心挑战。

Method: 使用奖励机（RM）作为 RL 目标规范的机制，并通过使用 FM 自动构建。将语言嵌入与每个 RM 自动机状态相关联，以实现跨任务的泛化。

Result: 在各种具有挑战性的环境中提供了 ARM-FM 有效性的经验证据，包括零样本泛化的证据。

Conclusion: ARM-FM: Automated Reward Machines via Foundation Models, a framework for automated, compositional reward design in RL that leverages the high-level reasoning capabilities of foundation models (FMs).

Abstract: Reinforcement learning (RL) algorithms are highly sensitive to reward
function specification, which remains a central challenge limiting their broad
applicability. We present ARM-FM: Automated Reward Machines via Foundation
Models, a framework for automated, compositional reward design in RL that
leverages the high-level reasoning capabilities of foundation models (FMs).
Reward machines (RMs) -- an automata-based formalism for reward specification
-- are used as the mechanism for RL objective specification, and are
automatically constructed via the use of FMs. The structured formalism of RMs
yields effective task decompositions, while the use of FMs enables objective
specifications in natural language. Concretely, we (i) use FMs to automatically
generate RMs from natural language specifications; (ii) associate language
embeddings with each RM automata-state to enable generalization across tasks;
and (iii) provide empirical evidence of ARM-FM's effectiveness in a diverse
suite of challenging environments, including evidence of zero-shot
generalization.

</details>


### [105] [Implementation of AI in Precision Medicine](https://arxiv.org/abs/2510.14194)
*Göktuğ Bender,Samer Faraj,Anand Bhardwaj*

Main category: cs.AI

TL;DR: AI在精准医学中的应用受到数据质量、临床可靠性、工作流程整合和治理等因素的限制。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在精准医学中应用面临的挑战和机遇。

Method: 通过对2019-2024年相关文献的回顾性范围界定。

Result: 确定了数据质量、临床可靠性、工作流程整合和治理等关键障碍和推动因素。

Conclusion: 提出了支持可信和可持续实施的未来方向。

Abstract: Artificial intelligence (AI) has become increasingly central to precision
medicine by enabling the integration and interpretation of multimodal data, yet
implementation in clinical settings remains limited. This paper provides a
scoping review of literature from 2019-2024 on the implementation of AI in
precision medicine, identifying key barriers and enablers across data quality,
clinical reliability, workflow integration, and governance. Through an
ecosystem-based framework, we highlight the interdependent relationships
shaping real-world translation and propose future directions to support
trustworthy and sustainable implementation.

</details>


### [106] [Echoes of Human Malice in Agents: Benchmarking LLMs for Multi-Turn Online Harassment Attacks](https://arxiv.org/abs/2510.14207)
*Trilok Padhi,Pinxian Lu,Abdulkadir Erol,Tanmay Sutar,Gauri Sharma,Mina Sonmez,Munmun De Choudhury,Ugur Kursuncu*

Main category: cs.AI

TL;DR: LLM agents in web apps are vulnerable to multi-turn harassment. This paper introduces a benchmark, jailbreak methods, and evaluation to analyze and mitigate this.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of LLM agents to misuse and harm in multi-turn interactive web applications, as existing research focuses on single-turn prompts.

Method: A multi-agent simulation informed by game theory, a multi-turn harassment dataset, and jailbreak methods attacking agent memory, planning and fine-tuning. Evaluation uses a mixed-methods framework on LLaMA-3.1-8B-Instruct and Gemini-2.0-flash.

Result: Jailbreak tuning significantly increases harassment success rates (up to 99.33% in Gemini), reduces refusal rates, and leads to toxic behaviors like Insult and Flaming. Agents reproduce human-like aggression profiles. Closed-source models show significant vulnerability.

Conclusion: Multi-turn attacks succeed and mimic human harassment dynamics. Robust safety guardrails are needed to keep online platforms safe and responsible.

Abstract: Large Language Model (LLM) agents are powering a growing share of interactive
web applications, yet remain vulnerable to misuse and harm. Prior jailbreak
research has largely focused on single-turn prompts, whereas real harassment
often unfolds over multi-turn interactions. In this work, we present the Online
Harassment Agentic Benchmark consisting of: (i) a synthetic multi-turn
harassment conversation dataset, (ii) a multi-agent (e.g., harasser, victim)
simulation informed by repeated game theory, (iii) three jailbreak methods
attacking agents across memory, planning, and fine-tuning, and (iv) a
mixed-methods evaluation framework. We utilize two prominent LLMs,
LLaMA-3.1-8B-Instruct (open-source) and Gemini-2.0-flash (closed-source). Our
results show that jailbreak tuning makes harassment nearly guaranteed with an
attack success rate of 95.78--96.89% vs. 57.25--64.19% without tuning in Llama,
and 99.33% vs. 98.46% without tuning in Gemini, while sharply reducing refusal
rate to 1-2% in both models. The most prevalent toxic behaviors are Insult with
84.9--87.8% vs. 44.2--50.8% without tuning, and Flaming with 81.2--85.1% vs.
31.5--38.8% without tuning, indicating weaker guardrails compared to sensitive
categories such as sexual or racial harassment. Qualitative evaluation further
reveals that attacked agents reproduce human-like aggression profiles, such as
Machiavellian/psychopathic patterns under planning, and narcissistic tendencies
with memory. Counterintuitively, closed-source and open-source models exhibit
distinct escalation trajectories across turns, with closed-source models
showing significant vulnerability. Overall, our findings show that multi-turn
and theory-grounded attacks not only succeed at high rates but also mimic
human-like harassment dynamics, motivating the development of robust safety
guardrails to ultimately keep online platforms safe and responsible.

</details>


### [107] [LiveResearchBench: A Live Benchmark for User-Centric Deep Research in the Wild](https://arxiv.org/abs/2510.14240)
*Jiayu Wang,Yifei Ming,Riya Dulepet,Qinglin Chen,Austin Xu,Zixuan Ke,Frederic Sala,Aws Albarghouthi,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 提出了LiveResearchBench和DeepEval，用于评估agentic系统的深度研究能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法满足用户中心、动态、明确和多方面的需求。

Method: 构建了包含100个任务的LiveResearchBench基准测试，并使用DeepEval评估内容和报告质量。

Result: 对17个深度研究系统进行了评估，揭示了优势、失败模式和关键系统组件。

Conclusion: LiveResearchBench和DeepEval为系统评估提供了一个严格的基础，并为推进可靠的深度研究指明了方向。

Abstract: Deep research -- producing comprehensive, citation-grounded reports by
searching and synthesizing information from hundreds of live web sources --
marks an important frontier for agentic systems. To rigorously evaluate this
ability, four principles are essential: tasks should be (1) user-centric,
reflecting realistic information needs, (2) dynamic, requiring up-to-date
information beyond parametric knowledge, (3) unambiguous, ensuring consistent
interpretation across users, and (4) multi-faceted and search-intensive,
requiring search over numerous web sources and in-depth analysis. Existing
benchmarks fall short of these principles, often focusing on narrow domains or
posing ambiguous questions that hinder fair comparison. Guided by these
principles, we introduce LiveResearchBench, a benchmark of 100 expert-curated
tasks spanning daily life, enterprise, and academia, each requiring extensive,
dynamic, real-time web search and synthesis. Built with over 1,500 hours of
human labor, LiveResearchBench provides a rigorous basis for systematic
evaluation. To evaluate citation-grounded long-form reports, we introduce
DeepEval, a comprehensive suite covering both content- and report-level
quality, including coverage, presentation, citation accuracy and association,
consistency and depth of analysis. DeepEval integrates four complementary
evaluation protocols, each designed to ensure stable assessment and high
agreement with human judgments. Using LiveResearchBench and DeepEval, we
conduct a comprehensive evaluation of 17 frontier deep research systems,
including single-agent web search, single-agent deep research, and multi-agent
systems. Our analysis reveals current strengths, recurring failure modes, and
key system components needed to advance reliable, insightful deep research.

</details>


### [108] [Towards Agentic Self-Learning LLMs in Search Environment](https://arxiv.org/abs/2510.14253)
*Wangtao Sun,Xiang Cheng,Jialin Fan,Yao Xu,Xing Yu,Shizhu He,Jun Zhao,Kang Liu*

Main category: cs.AI

TL;DR: 这篇论文研究了如何在不依赖人工数据集或预定义规则的情况下扩展基于LLM的智能代理。提出了Agentic Self-Learning (ASL) 框架，该框架通过共同进化任务生成器、策略模型和生成奖励模型来实现持续改进。


<details>
  <summary>Details</summary>
Motivation: 探索在开放领域中扩展LLM智能代理，并克服对人工标注数据或硬性规则奖励的依赖。

Method: 提出Agentic Self-Learning (ASL) 框架，它是一个完全闭环、多角色的强化学习框架，统一了任务生成、策略执行和评估。该框架协调提示生成器、策略模型和生成奖励模型。

Result: ASL 框架能够稳定地提升性能，超越了现有的强化学习基线，并且在零标签数据条件下持续改进。研究表明，生成奖励模型的验证能力是主要的瓶颈。

Conclusion: 奖励来源和数据规模是开放领域智能代理学习的关键因素，并且多角色共同进化对于可扩展的、自我改进的智能代理是有效的。

Abstract: We study whether self-learning can scale LLM-based agents without relying on
human-curated datasets or predefined rule-based rewards. Through controlled
experiments in a search-agent setting, we identify two key determinants of
scalable agent training: the source of reward signals and the scale of agent
task data. We find that rewards from a Generative Reward Model (GRM) outperform
rigid rule-based signals for open-domain learning, and that co-evolving the GRM
with the policy further boosts performance. Increasing the volume of agent task
data-even when synthetically generated-substantially enhances agentic
capabilities. Building on these insights, we propose \textbf{Agentic
Self-Learning} (ASL), a fully closed-loop, multi-role reinforcement learning
framework that unifies task generation, policy execution, and evaluation within
a shared tool environment and LLM backbone. ASL coordinates a Prompt Generator,
a Policy Model, and a Generative Reward Model to form a virtuous cycle of
harder task setting, sharper verification, and stronger solving. Empirically,
ASL delivers steady, round-over-round gains, surpasses strong RLVR baselines
(e.g., Search-R1) that plateau or degrade, and continues improving under
zero-labeled-data conditions, indicating superior sample efficiency and
robustness. We further show that GRM verification capacity is the main
bottleneck: if frozen, it induces reward hacking and stalls progress; continual
GRM training on the evolving data distribution mitigates this, and a small
late-stage injection of real verification data raises the performance ceiling.
This work establishes reward source and data scale as critical levers for
open-domain agent learning and demonstrates the efficacy of multi-role
co-evolution for scalable, self-improving agents. The data and code of this
paper are released at
https://github.com/forangel2014/Towards-Agentic-Self-Learning

</details>


### [109] [MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning](https://arxiv.org/abs/2510.14265)
*Xukai Wang,Xuanbo Liu,Mingrui Chen,Haitian Zhong,Xuanlin Yang,Bohan Zeng,Jinbo Hu,Hao Liang,Junbo Niu,Xuchen Li,Ruitao Wu,Ruichuan An,Yang Shi,Liu Liu,Xu-Yao Zhang,Qiang Liu,Zhouchen Lin,Wentao Zhang,Bin Dong*

Main category: cs.AI

TL;DR: 提出了一个名为 MorphoBench 的新基准，用于评估大型模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估大型模型的推理能力方面存在范围有限和缺乏灵活性的问题。

Method: 通过从现有基准和奥林匹克竞赛等来源选择和收集复杂的推理问题，并利用模型推理过程中生成的关键陈述自适应地修改问题的分析挑战。此外，它还包括使用仿真软件生成的问题，从而能够以最小的资源消耗动态调整基准难度。

Result: 收集了超过 1,300 个测试问题，并根据 o3 和 GPT-5 等模型的推理能力迭代调整了 MorphoBench 的难度。

Conclusion: MorphoBench 提高了模型推理评估的全面性和有效性，为提高大型模型的推理能力和科学稳健性提供了可靠的指导。

Abstract: With the advancement of powerful large-scale reasoning models, effectively
evaluating the reasoning capabilities of these models has become increasingly
important. However, existing benchmarks designed to assess the reasoning
abilities of large models tend to be limited in scope and lack the flexibility
to adapt their difficulty according to the evolving reasoning capacities of the
models. To address this, we propose MorphoBench, a benchmark that incorporates
multidisciplinary questions to evaluate the reasoning capabilities of large
models and can adjust and update question difficulty based on the reasoning
abilities of advanced models. Specifically, we curate the benchmark by
selecting and collecting complex reasoning questions from existing benchmarks
and sources such as Olympiad-level competitions. Additionally, MorphoBench
adaptively modifies the analytical challenge of questions by leveraging key
statements generated during the model's reasoning process. Furthermore, it
includes questions generated using simulation software, enabling dynamic
adjustment of benchmark difficulty with minimal resource consumption. We have
gathered over 1,300 test questions and iteratively adjusted the difficulty of
MorphoBench based on the reasoning capabilities of models such as o3 and GPT-5.
MorphoBench enhances the comprehensiveness and validity of model reasoning
evaluation, providing reliable guidance for improving both the reasoning
abilities and scientific robustness of large models. The code has been released
in https://github.com/OpenDCAI/MorphoBench.

</details>


### [110] [A Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space](https://arxiv.org/abs/2510.14301)
*Bingjie Zhang,Yibo Yang,Renzhe,Dandan Guo,Jindong Gu,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: 提出GuardSpace框架，用于在微调过程中保持LLM的安全对齐。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在各种任务中取得了显著成功，但它们的安全对齐在适应过程中仍然很脆弱。即使在良性数据上进行微调或使用低秩适应，预训练的安全行为也很容易退化，从而导致微调模型产生有害的反应。

Method: 使用协方差预处理奇异值分解将预训练权重显式分解为安全相关和安全不相关的分量，并从安全不相关的分量初始化低秩适配器，同时冻结安全相关的分量以保持其相关的安全机制。构建一个零空间投影器，限制适配器更新，防止其改变有害提示的安全输出，从而保持原始的拒绝行为。

Result: 在多个下游任务上对各种预训练模型进行的实验表明，GuardSpace 比现有方法具有更优越的性能。对于在 GSM8K 上微调的 Llama-2-7B-Chat，GuardSpace 优于最先进的方法 AsFT，将平均有害分数从 14.4% 降低到 3.6%，同时将准确率从 26.0% 提高到 28.0%。

Conclusion: GuardSpace框架在微调过程中能够有效提升LLM的安全性和性能。

Abstract: Large language models (LLMs) have achieved remarkable success in diverse
tasks, yet their safety alignment remains fragile during adaptation. Even when
fine-tuning on benign data or with low-rank adaptation, pre-trained safety
behaviors are easily degraded, leading to harmful responses in the fine-tuned
models. To address this challenge, we propose GuardSpace, a guardrail framework
for preserving safety alignment throughout fine-tuning, composed of two key
components: a safety-sensitive subspace and a harmful-resistant null space.
First, we explicitly decompose pre-trained weights into safety-relevant and
safety-irrelevant components using covariance-preconditioned singular value
decomposition, and initialize low-rank adapters from the safety-irrelevant
ones, while freezing safety-relevant components to preserve their associated
safety mechanism. Second, we construct a null space projector that restricts
adapter updates from altering safe outputs on harmful prompts, thereby
maintaining the original refusal behavior. Experiments with various pre-trained
models on multiple downstream tasks demonstrate that GuardSpace achieves
superior performance over existing methods. Notably, for Llama-2-7B-Chat
fine-tuned on GSM8K, GuardSpace outperforms the state-of-the-art method AsFT,
reducing the average harmful score from 14.4% to 3.6%, while improving the
accuracy from from 26.0% to 28.0%.

</details>


### [111] [TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence](https://arxiv.org/abs/2510.14670)
*Marco Simoni,Aleksandar Fontana,Andrea Saracino,Paolo Mori*

Main category: cs.AI

TL;DR: TITAN is a framework that connects natural-language cyber threat queries with executable reasoning over a structured knowledge graph.


<details>
  <summary>Details</summary>
Motivation: Unlike traditional retrieval systems, TITAN operates on a typed, bidirectional graph derived from MITRE, allowing reasoning to move clearly and reversibly between threats, behaviors, and defenses.

Method: It integrates a path planner model, which predicts logical relation chains from text, and a graph executor that traverses the TITAN Ontology to retrieve factual answers and supporting evidence. To support training and evaluation, the authors introduce the TITAN Dataset, a corpus of 88209 examples pairing natural language questions with executable reasoning paths and step by step Chain of Thought explanations.

Result: Empirical evaluations show that TITAN enables models to generate syntactically valid and semantically coherent reasoning paths that can be deterministically executed on the underlying graph.

Conclusion: TITAN enables models to generate syntactically valid and semantically coherent reasoning paths that can be deterministically executed on the underlying graph.

Abstract: TITAN (Threat Intelligence Through Automated Navigation) is a framework that
connects natural-language cyber threat queries with executable reasoning over a
structured knowledge graph. It integrates a path planner model, which predicts
logical relation chains from text, and a graph executor that traverses the
TITAN Ontology to retrieve factual answers and supporting evidence. Unlike
traditional retrieval systems, TITAN operates on a typed, bidirectional graph
derived from MITRE, allowing reasoning to move clearly and reversibly between
threats, behaviors, and defenses. To support training and evaluation, we
introduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test:
13951) pairing natural language questions with executable reasoning paths and
step by step Chain of Thought explanations. Empirical evaluations show that
TITAN enables models to generate syntactically valid and semantically coherent
reasoning paths that can be deterministically executed on the underlying graph.

</details>


### [112] [Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies](https://arxiv.org/abs/2510.14312)
*Mason Nakamura,Abhinav Kumar,Saaduddin Mahmud,Sahar Abdelnabi,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: 本文提出了Terrarium框架，用于研究基于LLM的多智能体系统中的安全性、隐私和安全问题。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的多智能体系统可以自动化繁琐的用户任务，但也引入了新的风险，包括对齐问题和恶意方的攻击。

Method: 本文重新利用了黑板设计，创建了一个模块化、可配置的测试平台，用于多智能体协作。确定了关键的攻击向量，如对齐问题、恶意智能体、受损的通信和数据中毒。

Result: 本文实现了三个协作MAS场景，并进行了四种代表性的攻击，以证明该框架的灵活性。

Conclusion: Terrarium旨在通过提供快速原型设计、评估和迭代防御和设计的工具，加速迈向值得信赖的多智能体系统。

Abstract: A multi-agent system (MAS) powered by large language models (LLMs) can
automate tedious user tasks such as meeting scheduling that requires
inter-agent collaboration. LLMs enable nuanced protocols that account for
unstructured private data, user constraints, and preferences. However, this
design introduces new risks, including misalignment and attacks by malicious
parties that compromise agents or steal user data. In this paper, we propose
the Terrarium framework for fine-grained study on safety, privacy, and security
in LLM-based MAS. We repurpose the blackboard design, an early approach in
multi-agent systems, to create a modular, configurable testbed for multi-agent
collaboration. We identify key attack vectors such as misalignment, malicious
agents, compromised communication, and data poisoning. We implement three
collaborative MAS scenarios with four representative attacks to demonstrate the
framework's flexibility. By providing tools to rapidly prototype, evaluate, and
iterate on defenses and designs, Terrarium aims to accelerate progress toward
trustworthy multi-agent systems.

</details>


### [113] [Metacognitive Self-Correction for Multi-Agent System via Prototype-Guided Next-Execution Reconstruction](https://arxiv.org/abs/2510.14319)
*Xu Shen,Qi Zhang,Song Wang,Zhen Tan,Xinyu Zhao,Laura Yao,Vaishnav Tadiparthi,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Kwonjoon Lee,Tianlong Chen*

Main category: cs.AI

TL;DR: MASC: A metacognitive framework for multi-agent systems that detects and corrects errors in real-time.


<details>
  <summary>Details</summary>
Motivation: Cascading errors in multi-agent systems can disrupt the trajectory of problem-solving.

Method: MASC uses history-conditioned anomaly scoring via Next-Execution Reconstruction and Prototype-Guided Enhancement to detect errors and triggers a correction agent to revise the output.

Result: MASC outperforms baselines on the Who&When benchmark, improving step-level error detection by up to 8.47% AUC-ROC and delivers consistent end-to-end gains across architectures.

Conclusion: MASC's metacognitive monitoring and targeted correction can mitigate error propagation with minimal overhead.

Abstract: Large Language Model based multi-agent systems (MAS) excel at collaborative
problem solving but remain brittle to cascading errors: a single faulty step
can propagate across agents and disrupt the trajectory. In this paper, we
present MASC, a metacognitive framework that endows MAS with real-time,
unsupervised, step-level error detection and self-correction. MASC rethinks
detection as history-conditioned anomaly scoring via two complementary designs:
(1) Next-Execution Reconstruction, which predicts the embedding of the next
step from the query and interaction history to capture causal consistency, and
(2) Prototype-Guided Enhancement, which learns a prototype prior over
normal-step embeddings and uses it to stabilize reconstruction and anomaly
scoring under sparse context (e.g., early steps). When an anomaly step is
flagged, MASC triggers a correction agent to revise the acting agent's output
before information flows downstream. On the Who&When benchmark, MASC
consistently outperforms all baselines, improving step-level error detection by
up to 8.47% AUC-ROC ; When plugged into diverse MAS frameworks, it delivers
consistent end-to-end gains across architectures, confirming that our
metacognitive monitoring and targeted correction can mitigate error propagation
with minimal overhead.

</details>


### [114] [AI for Service: Proactive Assistance with AI Glasses](https://arxiv.org/abs/2510.14359)
*Zichen Wen,Yiyu Wang,Chenfei Liao,Boxue Yang,Junxian Li,Weifeng Liu,Haocong He,Bolong Feng,Xuyang Liu,Yuanhuiyi Lyu,Xu Zheng,Xuming Hu,Linfeng Zhang*

Main category: cs.AI

TL;DR: 提出了一个名为AI4Service的新范例，旨在实现主动和实时的日常生活中协助。


<details>
  <summary>Details</summary>
Motivation: 现有的AI服务在很大程度上是被动的，仅响应明确的用户命令。真正的智能助手应该能够预测用户需求并在适当时主动采取行动。

Method: 提出了 Alpha-Service，一个统一的框架，由五个关键组件组成：输入单元、中央处理单元、算术逻辑单元、存储单元和输出单元。通过部署在 AI 眼镜上的多智能体系统来实现 Alpha-Service。

Result: 案例研究，包括实时 Blackjack 顾问、博物馆导游和购物助手，证明了 Alpha-Service 无需明确提示即可无缝感知环境、推断用户意图并提供及时有用的帮助的能力。

Conclusion: Alpha-Service 能够 seamlessly 感知环境, 推断用户意图, 并且提供及时和有用的帮助，无需明确提示。

Abstract: In an era where AI is evolving from a passive tool into an active and
adaptive companion, we introduce AI for Service (AI4Service), a new paradigm
that enables proactive and real-time assistance in daily life. Existing AI
services remain largely reactive, responding only to explicit user commands. We
argue that a truly intelligent and helpful assistant should be capable of
anticipating user needs and taking actions proactively when appropriate. To
realize this vision, we propose Alpha-Service, a unified framework that
addresses two fundamental challenges: Know When to intervene by detecting
service opportunities from egocentric video streams, and Know How to provide
both generalized and personalized services. Inspired by the von Neumann
computer architecture and based on AI glasses, Alpha-Service consists of five
key components: an Input Unit for perception, a Central Processing Unit for
task scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit
for long-term personalization, and an Output Unit for natural human
interaction. As an initial exploration, we implement Alpha-Service through a
multi-agent system deployed on AI glasses. Case studies, including a real-time
Blackjack advisor, a museum tour guide, and a shopping fit assistant,
demonstrate its ability to seamlessly perceive the environment, infer user
intent, and provide timely and useful assistance without explicit prompts.

</details>


### [115] [Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?](https://arxiv.org/abs/2510.14387)
*Yijie Hu,Zihao Zhou,Kaizhu Huang,Xiaowei Huang,Qiufeng Wang*

Main category: cs.AI

TL;DR: IP-Merging: A tuning-free method to enhance math reasoning in MLLMs by merging parameters from math LLMs.


<details>
  <summary>Details</summary>
Motivation: MLLMs lag behind LLMs in math reasoning. Can MLLMs directly inherit math reasoning abilities from off-the-shelf math LLMs without training?

Method: Identifies reasoning-associated layers in MLLMs and math LLMs, projects math LLM parameters into the MLLM subspace to maintain alignment, and merges parameters in this subspace.

Result: IP-Merging enhances math reasoning in MLLMs without compromising other capabilities.

Conclusion: IP-Merging effectively transfers math reasoning abilities from LLMs to MLLMs without training by carefully aligning and merging parameters.

Abstract: Math reasoning has been one crucial ability of large language models (LLMs),
where significant advancements have been achieved in recent years. However,
most efforts focus on LLMs by curating high-quality annotation data and
intricate training (or inference) paradigms, while the math reasoning
performance of multi-modal LLMs (MLLMs) remains lagging behind. Since the MLLM
typically consists of an LLM and a vision block, we wonder: Can MLLMs directly
absorb math reasoning abilities from off-the-shelf math LLMs without tuning?
Recent model-merging approaches may offer insights into this question. However,
they overlook the alignment between the MLLM and LLM, where we find that there
is a large gap between their parameter spaces, resulting in lower performance.
Our empirical evidence reveals two key factors behind this issue: the
identification of crucial reasoning-associated layers in the model and the
mitigation of the gaps in parameter space. Based on the empirical insights, we
propose IP-Merging that first identifies the reasoning-associated parameters in
both MLLM and Math LLM, then projects them into the subspace of MLLM, aiming to
maintain the alignment, and finally merges parameters in this subspace.
IP-Merging is a tuning-free approach since parameters are directly adjusted.
Extensive experiments demonstrate that our IP-Merging method can enhance the
math reasoning ability of MLLMs directly from Math LLMs without compromising
their other capabilities.

</details>


### [116] [Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control](https://arxiv.org/abs/2510.14388)
*Zhe Wu,Hongjin Lu,Junliang Xing,Changhao Zhang,Yin Zhu,Yuhao Yang,Yuheng Jing,Kai Li,Kun Shao,Jianye Hao,Jun Wang,Yuanchun Shi*

Main category: cs.AI

TL;DR: Hi-Agent: A hierarchical vision-language agent for mobile control, achieving state-of-the-art performance on Android-in-the-Wild benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack structured reasoning and planning, leading to poor generalization on novel tasks or unseen UI layouts.

Method: A trainable hierarchical vision-language agent with a high-level reasoning model and a low-level action model, jointly optimized using a reformulated multi-step decision-making approach and a foresight advantage function.

Result: Achieves a new SOTA 87.9% task success rate on the Android-in-the-Wild benchmark, outperforming prior methods. Demonstrates competitive zero-shot generalization on ScreenSpot-v2 and scales effectively on AndroidWorld.

Conclusion: Hi-Agent shows strong adaptability in high-complexity mobile control scenarios.

Abstract: Building agents that autonomously operate mobile devices has attracted
increasing attention. While Vision-Language Models (VLMs) show promise, most
existing approaches rely on direct state-to-action mappings, which lack
structured reasoning and planning, and thus generalize poorly to novel tasks or
unseen UI layouts. We introduce Hi-Agent, a trainable hierarchical
vision-language agent for mobile control, featuring a high-level reasoning
model and a low-level action model that are jointly optimized. For efficient
training, we reformulate multi-step decision-making as a sequence of
single-step subgoals and propose a foresight advantage function, which
leverages execution feedback from the low-level model to guide high-level
optimization. This design alleviates the path explosion issue encountered by
Group Relative Policy Optimization (GRPO) in long-horizon tasks and enables
stable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art
(SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark,
significantly outperforming prior methods across three paradigms: prompt-based
(AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement
learning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot
generalization on the ScreenSpot-v2 benchmark. On the more challenging
AndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones,
showing strong adaptability in high-complexity mobile control scenarios.

</details>


### [117] [IMAGINE: Integrating Multi-Agent System into One Model for Complex Reasoning and Planning](https://arxiv.org/abs/2510.14406)
*Xikai Zhang,Bo Wang,Likang Xiao,Yongzhi Li,Quan Chen,Wenju Wu,Liu Liu*

Main category: cs.AI

TL;DR: IMAGINE框架将多智能体系统的推理和规划能力集成到单个紧凑模型中，并通过端到端训练显著超越了MAS的能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理和规划方面面临挑战，即使采用精心设计的提示和明确提供的先验信息，性能仍然有限。多智能体系统虽然可以改进集体推理，但推理成本高昂。

Method: 提出了IMAGINE框架，将多智能体系统集成到单个模型中，并通过端到端训练进行优化。

Result: 使用Qwen3-8B-Instruct作为基础模型，通过IMAGINE框架训练后，在TravelPlanner基准测试中达到了82.7%的Final Pass Rate，远超DeepSeek-R1-671B的40%。

Conclusion: IMAGINE框架使小规模模型能够获得多智能体系统的结构化推理和规划能力，并显著超越它，同时保持较小的模型尺寸。

Abstract: Although large language models (LLMs) have made significant strides across
various tasks, they still face significant challenges in complex reasoning and
planning. For example, even with carefully designed prompts and prior
information explicitly provided, GPT-4o achieves only a 7% Final Pass Rate on
the TravelPlanner dataset in the sole-planning mode. Similarly, even in the
thinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B, only achieve Final Pass
Rates of 5.9% and 40%, respectively. Although well-organized Multi-Agent
Systems (MAS) can offer improved collective reasoning, they often suffer from
high reasoning costs due to multi-round internal interactions, long
per-response latency, and difficulties in end-to-end training. To address these
challenges, we propose a general and scalable framework called IMAGINE, short
for Integrating Multi-Agent System into One Model. This framework not only
integrates the reasoning and planning capabilities of MAS into a single,
compact model, but also significantly surpass the capabilities of the MAS
through a simple end-to-end training. Through this pipeline, a single
small-scale model is not only able to acquire the structured reasoning and
planning capabilities of a well-organized MAS but can also significantly
outperform it. Experimental results demonstrate that, when using
Qwen3-8B-Instruct as the base model and training it with our method, the model
achieves an 82.7% Final Pass Rate on the TravelPlanner benchmark, far exceeding
the 40% of DeepSeek-R1-671B, while maintaining a much smaller model size.

</details>


### [118] [Eliminating Negative Occurrences of Derived Predicates from PDDL Axioms](https://arxiv.org/abs/2510.14412)
*Claudia Grundke,Gabriele Röger*

Main category: cs.AI

TL;DR: 本文介绍了一种转换方法，用于消除规划领域定义语言（PDDL）公理中派生谓词的负出现。


<details>
  <summary>Details</summary>
Motivation: PDDL标准限制了公理主体中谓词的负出现，但作者通常偏离此限制。本文旨在解决这个问题。

Method: 提出了一种转换方法。

Result: 提出的转换方法可以消除派生谓词的负出现。

Conclusion: PDDL的两种变体可以表达与最小不动点逻辑完全相同的查询，表明可以消除派生谓词的负出现。

Abstract: Axioms are a feature of the Planning Domain Definition Language PDDL that can
be considered as a generalization of database query languages such as Datalog.
The PDDL standard restricts negative occurrences of predicates in axiom bodies
to predicates that are directly set by actions and not derived by axioms. In
the literature, authors often deviate from this limitation and only require
that the set of axioms is stratifiable. Both variants can express exactly the
same queries as least fixed-point logic, indicating that negative occurrences
of derived predicates can be eliminated. We present the corresponding
transformation.

</details>


### [119] [Helmsman: Autonomous Synthesis of Federated Learning Systems via Multi-Agent Collaboration](https://arxiv.org/abs/2510.14512)
*Haoyuan Li,Mathias Funk,Aaqib Saeed*

Main category: cs.AI

TL;DR: Helmsman是一个多智能体系统，可以自动合成联邦学习系统，从高级用户规范到最终部署。


<details>
  <summary>Details</summary>
Motivation: 设计和部署稳健的联邦学习系统非常复杂，需要选择、组合和调整策略来应对数据异构和系统约束等多方面的挑战。

Method: Helmsman通过三个协作阶段模拟了有原则的研发工作流程：(1) 交互式人机循环规划，以制定合理的研发计划，(2) 由监督智能体团队进行模块化代码生成，以及 (3) 在沙盒模拟环境中进行自主评估和改进的闭环。

Result: Helmsman生成的解决方案与已建立的手工基线相比具有竞争力，而且通常更优越。

Conclusion: Helmsman代表了复杂分散式人工智能系统自动工程方面的重要一步。

Abstract: Federated Learning (FL) offers a powerful paradigm for training models on
decentralized data, but its promise is often undermined by the immense
complexity of designing and deploying robust systems. The need to select,
combine, and tune strategies for multifaceted challenges like data
heterogeneity and system constraints has become a critical bottleneck,
resulting in brittle, bespoke solutions. To address this, we introduce
Helmsman, a novel multi-agent system that automates the end-to-end synthesis of
federated learning systems from high-level user specifications. It emulates a
principled research and development workflow through three collaborative
phases: (1) interactive human-in-the-loop planning to formulate a sound
research plan, (2) modular code generation by supervised agent teams, and (3) a
closed-loop of autonomous evaluation and refinement in a sandboxed simulation
environment. To facilitate rigorous evaluation, we also introduce
AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess
the system-level generation capabilities of agentic systems in FL. Extensive
experiments demonstrate that our approach generates solutions competitive with,
and often superior to, established hand-crafted baselines. Our work represents
a significant step towards the automated engineering of complex decentralized
AI systems.

</details>


### [120] [JSPLIT: A Taxonomy-based Solution for Prompt Bloating in Model Context Protocol](https://arxiv.org/abs/2510.14537)
*Emanuele Antonioni,Stefan Markovic,Anirudha Shankar,Jaime Bernardo,Lovro Markovic,Silvia Pareti,Benedetto Proietti*

Main category: cs.AI

TL;DR: 这篇论文提出了一种名为JSPLIT的框架，旨在解决大型语言模型(llm)代理中使用大量工具时出现的提示臃肿问题。JSPLIT通过将工具组织成层级结构，并根据用户提示选择最相关的工具，从而有效减小提示大小，提高工具选择准确性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型(llm)代理需要与外部工具交互的需求增加，提示臃肿问题日益严重，导致成本上升、延迟增加和任务成功率降低。

Method: JSPLIT框架通过构建工具层级分类，并利用用户提示和分类结构来识别和选择最相关的工具。

Result: 实验结果表明，JSPLIT能够显著减小提示大小，同时不会显著降低代理的响应效果。在高复杂度代理环境中，JSPLIT甚至能够提高工具选择的准确性。

Conclusion: JSPLIT是一种有效的提示管理框架，可以降低成本，提高大型语言模型(llm)代理在复杂环境中的任务成功率。

Abstract: AI systems are continually evolving and advancing, and user expectations are
concurrently increasing, with a growing demand for interactions that go beyond
simple text-based interaction with Large Language Models (LLMs). Today's
applications often require LLMs to interact with external tools, marking a
shift toward more complex agentic systems. To support this, standards such as
the Model Context Protocol (MCP) have emerged, enabling agents to access tools
by including a specification of the capabilities of each tool within the
prompt. Although this approach expands what agents can do, it also introduces a
growing problem: prompt bloating. As the number of tools increases, the prompts
become longer, leading to high prompt token costs, increased latency, and
reduced task success resulting from the selection of tools irrelevant to the
prompt. To address this issue, we introduce JSPLIT, a taxonomy-driven framework
designed to help agents manage prompt size more effectively when using large
sets of MCP tools. JSPLIT organizes the tools into a hierarchical taxonomy and
uses the user's prompt to identify and include only the most relevant tools,
based on both the query and the taxonomy structure. In this paper, we describe
the design of the taxonomy, the tool selection algorithm, and the dataset used
to evaluate JSPLIT. Our results show that JSPLIT significantly reduces prompt
size without significantly compromising the agent's ability to respond
effectively. As the number of available tools for the agent grows
substantially, JSPLIT even improves the tool selection accuracy of the agent,
effectively reducing costs while simultaneously improving task success in
high-complexity agent environments.

</details>


### [121] [Symbol Grounding in Neuro-Symbolic AI: A Gentle Introduction to Reasoning Shortcuts](https://arxiv.org/abs/2510.14538)
*Emanuele Marconato,Samuele Bortolotti,Emile van Krieken,Paolo Morettin,Elena Umili,Antonio Vergari,Efthymia Tsamoura,Andrea Passerini,Stefano Teso*

Main category: cs.AI

TL;DR: 本文旨在综述神经符号人工智能（NeSy AI）中推理捷径（RSs）的问题，讨论其原因、后果、理论特征，并详细介绍处理RSs的方法，包括缓解和感知策略，旨在为研究人员和从业者提供一个统一的视角来解决RSs问题，从而促进可靠的NeSy和值得信赖的AI模型的发展。


<details>
  <summary>Details</summary>
Motivation: 神经符号人工智能（NeSy AI）旨在开发符合先验知识编码的深度神经网络，但最近的研究表明，在概念没有直接监督的情况下，NeSy模型会受到推理捷径（RSs）的影响，从而影响模型的可解释性、泛化能力和可靠性。目前关于RSs的文献分散，难以理解和解决。

Method: 本文通过提供对RSs的温和介绍，讨论其原因和后果，回顾和阐明现有的理论特征，并详细介绍处理RSs的方法，包括缓解和感知策略。

Result: 本文旨在为研究人员和从业者提供一个统一的视角来解决RSs问题。

Conclusion: 本文旨在通过对RSs的综述，为解决该问题降低门槛，并最终促进可靠的NeSy和值得信赖的AI模型的发展。

Abstract: Neuro-symbolic (NeSy) AI aims to develop deep neural networks whose
predictions comply with prior knowledge encoding, e.g. safety or structural
constraints. As such, it represents one of the most promising avenues for
reliable and trustworthy AI. The core idea behind NeSy AI is to combine neural
and symbolic steps: neural networks are typically responsible for mapping
low-level inputs into high-level symbolic concepts, while symbolic reasoning
infers predictions compatible with the extracted concepts and the prior
knowledge. Despite their promise, it was recently shown that - whenever the
concepts are not supervised directly - NeSy models can be affected by Reasoning
Shortcuts (RSs). That is, they can achieve high label accuracy by grounding the
concepts incorrectly. RSs can compromise the interpretability of the model's
explanations, performance in out-of-distribution scenarios, and therefore
reliability. At the same time, RSs are difficult to detect and prevent unless
concept supervision is available, which is typically not the case. However, the
literature on RSs is scattered, making it difficult for researchers and
practitioners to understand and tackle this challenging problem. This overview
addresses this issue by providing a gentle introduction to RSs, discussing
their causes and consequences in intuitive terms. It also reviews and
elucidates existing theoretical characterizations of this phenomenon. Finally,
it details methods for dealing with RSs, including mitigation and awareness
strategies, and maps their benefits and limitations. By reformulating advanced
material in a digestible form, this overview aims to provide a unifying
perspective on RSs to lower the bar to entry for tackling them. Ultimately, we
hope this overview contributes to the development of reliable NeSy and
trustworthy AI models.

</details>


### [122] [LLM Agents Beyond Utility: An Open-Ended Perspective](https://arxiv.org/abs/2510.14548)
*Asen Nachkov,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: 大型语言模型智能体利用思维链推理和函数调用，但它们能否像独立实体一样规划、设计任务并朝着更广泛的目标推理？


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型智能体是否可以像独立实体一样运作，具备规划、设计任务和朝着更广泛目标推理的能力。

Method: 通过增强预训练的LLM智能体，使其能够生成自己的任务、积累知识并与其环境进行广泛互动，对其进行定性研究。

Result: 该智能体可以可靠地遵循复杂的多步骤指令，跨运行存储和重用信息，并提出和解决自己的任务，但它仍然对提示设计敏感，容易重复生成任务，并且无法形成自我表征。

Conclusion: 这些发现说明了将预训练的LLM适应于开放性的希望和当前局限性，并指出了未来训练智能体管理记忆、有效地探索和追求抽象的长期目标的方向。

Abstract: Recent LLM agents have made great use of chain of thought reasoning and
function calling. As their capabilities grow, an important question arises: can
this software represent not only a smart problem-solving tool, but an entity in
its own right, that can plan, design immediate tasks, and reason toward
broader, more ambiguous goals? To study this question, we adopt an open-ended
experimental setting where we augment a pretrained LLM agent with the ability
to generate its own tasks, accumulate knowledge, and interact extensively with
its environment. We study the resulting open-ended agent qualitatively. It can
reliably follow complex multi-step instructions, store and reuse information
across runs, and propose and solve its own tasks, though it remains sensitive
to prompt design, prone to repetitive task generation, and unable to form
self-representations. These findings illustrate both the promise and current
limits of adapting pretrained LLMs toward open-endedness, and point to future
directions for training agents to manage memory, explore productively, and
pursue abstract long-term goals.

</details>


### [123] [ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks](https://arxiv.org/abs/2510.14621)
*Yuanyi Song,Heyuan Huang,Qiqiang Lin,Yin Zhao,Xiangmou Qu,Jun Wang,Xingyu Lou,Weiwen Liu,Zhuosheng Zhang,Jun Wang,Yong Yu,Weinan Zhang,Zhaoxiang Wang*

Main category: cs.AI

TL;DR: 本文介绍了一种新的图结构基准测试框架ColorBench，用于评估移动代理在复杂、长程任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有移动代理评估标准（离线静态基准和在线动态测试）无法全面评估代理能力，因为前者只能验证预定义的“黄金路径”，而后者受限于真实设备的复杂性和不可重复性。

Method: 通过对真实设备交互中观察到的有限状态进行建模，实现了动态行为的静态模拟。构建了一个图结构基准测试框架ColorBench，专注于复杂的长程任务。

Result: ColorBench包含175个任务（74个单应用，101个跨应用），平均长度超过13步。每个任务包含至少两条正确路径和多个典型错误路径，从而实现准动态交互。通过评估各种基线，发现现有模型的局限性。

Conclusion: 实验结果表明，现有模型在复杂、长程问题上存在局限性，并提出了改进方向和可行的技术路径。

Abstract: The rapid advancement of multimodal large language models has enabled agents
to operate mobile devices by directly interacting with graphical user
interfaces, opening new possibilities for mobile automation. However,
real-world mobile tasks are often complex and allow for multiple valid
solutions. This contradicts current mobile agent evaluation standards: offline
static benchmarks can only validate a single predefined "golden path", while
online dynamic testing is constrained by the complexity and non-reproducibility
of real devices, making both approaches inadequate for comprehensively
assessing agent capabilities. To bridge the gap between offline and online
evaluation and enhance testing stability, this paper introduces a novel
graph-structured benchmarking framework. By modeling the finite states observed
during real-device interactions, it achieves static simulation of dynamic
behaviors. Building on this, we develop ColorBench, a benchmark focused on
complex long-horizon tasks. It supports evaluation of multiple valid solutions,
subtask completion rate statistics, and atomic-level capability analysis.
ColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average
length of over 13 steps. Each task includes at least two correct paths and
several typical error paths, enabling quasi-dynamic interaction. By evaluating
ColorBench across various baselines, we discover limitations of existing models
and propose improvement directions and feasible technical pathways to enhance
agents' performance on complex, long-horizon problems based on experimental
results. Code and data are available at:
https://github.com/MadeAgents/ColorBench.

</details>


### [124] [Beyond Hallucinations: The Illusion of Understanding in Large Language Models](https://arxiv.org/abs/2510.14665)
*Rikard Rosenbacke,Carl Rosenbacke,Victor Rosenbacke,Martin McKee*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）正被广泛应用于人类沟通和决策，但由于其基于统计预测而非实际推理，因此存在产生听起来有说服力但缺乏事实有效性的“幻觉”风险。本文提出了一个名为 Rose-Frame 的三维框架，用于诊断人机交互中的认知和认知漂移，通过 Map vs. Territory（知识表示与现实）、Intuition vs. Reason（直觉与理性）和 Conflict vs. Confirmation（冲突与确认）三个维度，帮助用户认识到模型和自身的局限性，从而实现更透明和批判性的人工智能部署。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然流畅、富有情感且连贯，但由于其基于统计预测而非实际推理，因此存在产生听起来有说服力但缺乏事实有效性的“幻觉”风险。

Method: 提出了一个名为 Rose-Frame 的三维框架，用于诊断人机交互中的认知和认知漂移。该框架包含三个维度：Map vs. Territory（知识表示与现实）、Intuition vs. Reason（直觉与理性）和 Conflict vs. Confirmation（冲突与确认）。

Result: Rose-Frame 作为一个反思工具，旨在揭示模型和用户的局限性，从而实现更透明和批判性的人工智能部署。它并不试图通过更多数据或规则来修复 LLM，而是将对齐问题重新定义为认知治理，强调人类理性对直觉的管辖。

Conclusion: 为了使机器的流畅性与人类的理解对齐，必须嵌入可验证的反思性监督。

Abstract: Large language models (LLMs) are becoming deeply embedded in human
communication and decision-making, yet they inherit the ambiguity, bias, and
lack of direct access to truth inherent in language itself. While their outputs
are fluent, emotionally resonant, and coherent, they are generated through
statistical prediction rather than grounded reasoning. This creates the risk of
hallucination, responses that sound convincing but lack factual validity.
Building on Geoffrey Hinton's observation that AI mirrors human intuition
rather than reasoning, this paper argues that LLMs operationalize System 1
cognition at scale: fast, associative, and persuasive, but without reflection
or falsification. To address this, we introduce the Rose-Frame, a
three-dimensional framework for diagnosing cognitive and epistemic drift in
human-AI interaction. The three axes are: (i) Map vs. Territory, which
distinguishes representations of reality (epistemology) from reality itself
(ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to
separate fast, emotional judgments from slow, reflective thinking; and (iii)
Conflict vs. Confirmation, which examines whether ideas are critically tested
through disagreement or simply reinforced through mutual validation. Each
dimension captures a distinct failure mode, and their combination amplifies
misalignment. Rose-Frame does not attempt to fix LLMs with more data or rules.
Instead, it offers a reflective tool that makes both the model's limitations
and the user's assumptions visible, enabling more transparent and critically
aware AI deployment. It reframes alignment as cognitive governance: intuition,
whether human or artificial, must remain governed by human reason. Only by
embedding reflective, falsifiable oversight can we align machine fluency with
human understanding.

</details>


### [125] [Machine Learning and Public Health: Identifying and Mitigating Algorithmic Bias through a Systematic Review](https://arxiv.org/abs/2510.14669)
*Sara Altamirano,Arjan Vreeken,Sennay Ghebreab*

Main category: cs.AI

TL;DR: 本研究旨在评估荷兰公共健康机器学习(ML)研究中算法偏差的识别、讨论和报告情况，并提出了一个旨在解决公平性的框架。


<details>
  <summary>Details</summary>
Motivation: 机器学习有潜力通过改善监测、风险分层和资源分配来彻底改变公共健康，但若不重视算法偏差，可能会无意中加剧现有的健康差距。

Method: 通过整合现有框架的要素，开发了算法偏差风险评估工具(RABAT)，并将其应用于35项同行评审研究。

Result: 分析揭示了普遍存在的差距：虽然数据抽样和缺失数据实践有详细记录，但大多数研究省略了明确的公平框架、亚组分析以及对潜在危害的透明讨论。

Conclusion: 提出了一个四阶段的公平性导向框架ACAR，并为公共健康机器学习从业者提出了可操作的建议，以确保算法创新能够促进健康公平。

Abstract: Machine learning (ML) promises to revolutionize public health through
improved surveillance, risk stratification, and resource allocation. However,
without systematic attention to algorithmic bias, ML may inadvertently
reinforce existing health disparities. We present a systematic literature
review of algorithmic bias identification, discussion, and reporting in Dutch
public health ML research from 2021 to 2025. To this end, we developed the Risk
of Algorithmic Bias Assessment Tool (RABAT) by integrating elements from
established frameworks (Cochrane Risk of Bias, PROBAST, Microsoft Responsible
AI checklist) and applied it to 35 peer-reviewed studies. Our analysis reveals
pervasive gaps: although data sampling and missing data practices are well
documented, most studies omit explicit fairness framing, subgroup analyses, and
transparent discussion of potential harms. In response, we introduce a
four-stage fairness-oriented framework called ACAR (Awareness,
Conceptualization, Application, Reporting), with guiding questions derived from
our systematic literature review to help researchers address fairness across
the ML lifecycle. We conclude with actionable recommendations for public health
ML practitioners to consistently consider algorithmic bias and foster
transparency, ensuring that algorithmic innovations advance health equity
rather than undermine it.

</details>


### [126] [NAEL: Non-Anthropocentric Ethical Logic](https://arxiv.org/abs/2510.14676)
*Bianca Maria Lerma,Rafael Peñaloza*

Main category: cs.AI

TL;DR: NAEL: A novel ethical framework for AI grounded in active inference and symbolic reasoning.


<details>
  <summary>Details</summary>
Motivation: Conventional AI ethics are human-centered. NAEL formalizes ethical behavior as an emergent property of intelligent systems minimizing global expected free energy in dynamic, multi-agent environments.

Method: A neuro-symbolic architecture is proposed to allow agents to evaluate the ethical consequences of their actions in uncertain settings.

Result: A case study involving ethical resource distribution illustrates NAEL's dynamic balancing of self-preservation, epistemic learning, and collective welfare.

Conclusion: NAEL allows agents to develop context-sensitive, adaptive, and relational ethical behaviour without presupposing anthropomorphic moral intuitions, addressing the limitations of existing ethical models.

Abstract: We introduce NAEL (Non-Anthropocentric Ethical Logic), a novel ethical
framework for artificial agents grounded in active inference and symbolic
reasoning. Departing from conventional, human-centred approaches to AI ethics,
NAEL formalizes ethical behaviour as an emergent property of intelligent
systems minimizing global expected free energy in dynamic, multi-agent
environments. We propose a neuro-symbolic architecture to allow agents to
evaluate the ethical consequences of their actions in uncertain settings. The
proposed system addresses the limitations of existing ethical models by
allowing agents to develop context-sensitive, adaptive, and relational ethical
behaviour without presupposing anthropomorphic moral intuitions. A case study
involving ethical resource distribution illustrates NAEL's dynamic balancing of
self-preservation, epistemic learning, and collective welfare.

</details>


### [127] [Practical, Utilitarian Algorithm Configuration](https://arxiv.org/abs/2510.14683)
*Devon Graham,Kevin Leyton-Brown*

Main category: cs.AI

TL;DR: 本文改进了COUP算法，使其在具有理论保证的同时，在实际性能上与现有的启发式算法配置程序竞争。


<details>
  <summary>Details</summary>
Motivation: 用户效用函数可以灵活地捕获用户对算法运行时的偏好，但COUP算法的实际性能不佳。

Method: 通过一系列改进来提升COUP算法的经验性能，同时不降低其理论保证。

Result: 改进后的COUP算法在实验中表现出优势。

Conclusion: 本文将具有理论保证的功利主义算法配置提升到可以与广泛使用的启发式配置程序竞争的水平。

Abstract: Utilitarian algorithm configuration identifies a parameter setting for a
given algorithm that maximizes a user's utility. Utility functions offer a
theoretically well-grounded approach to optimizing decision-making under
uncertainty and are flexible enough to capture a user's preferences over
algorithm runtimes (e.g., they can describe a sharp cutoff after which a
solution is no longer required, a per-hour cost for compute, or diminishing
returns from algorithms that take longer to run). COUP is a recently-introduced
utilitarian algorithm configuration procedure which was designed mainly to
offer strong theoretical guarantees about the quality of the configuration it
returns, with less attention paid to its practical performance. This paper
closes that gap, bringing theoretically-grounded, utilitarian algorithm
configuration to the point where it is competitive with widely used, heuristic
configuration procedures that offer no performance guarantees. We present a
series of improvements to COUP that improve its empirical performance without
degrading its theoretical guarantees and demonstrate their benefit
experimentally. Using a case study, we also illustrate ways of exploring the
robustness of a given solution to the algorithm selection problem to variations
in the utility function.

</details>


### [128] [Purifying Task Vectors in Knowledge-Aware Subspace for Model Merging](https://arxiv.org/abs/2510.14697)
*Bang An,Yibo Yang,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: 本文提出了一种新的模型合并方法，旨在解决现有方法中由于任务向量中任务无关冗余导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法由于任务向量中的冗余信息导致性能下降，并且现有的减少冗余的方法缺乏知识感知。

Method: 提出了一种名为PAVE（Purifying TAsk Vectors）的方法，该方法在知识感知的子空间中对任务向量进行提纯。通过对从每个任务中抽取的训练样本进行奇异值分解，将模型权重分解为任务相关和冗余成分，并修剪冗余成分。同时，引入了一种谱秩分配策略，以优化归一化的激活修剪误差，从而在不同模型之间实现公平的修剪。

Result: 实验证明，PAVE在各种合并方法、任务和模型架构中均有效。

Conclusion: PAVE作为一种即插即用的方案，可以应用于各种基于任务向量的合并方法，以提高其性能。

Abstract: Model merging aims to integrate task-specific abilities from individually
fine-tuned models into a single model without extra training. In recent model
merging methods, task vector has become a fundamental building block, as it can
encapsulate the residual information from finetuning. However, the merged model
often suffers from notable performance degradation due to the conflicts caused
by task-irrelevant redundancy in task vectors. Existing efforts in overcoming
redundancy by randomly dropping elements in the parameter space involves
randomness and lacks knowledge awareness. To address these challenges, in this
study, we propose Purifying TAsk Vectors (PAVE) in knowledge-aware subspace.
Concretely, we sample some training examples from each task, and feed them into
their corresponding fine-tuned models to acquire the covariance matrices before
linear layers. We then perform a context-oriented singular value decomposition,
which accentuates the weight components most relevant to the target knowledge.
As a result, we can split fine-tuned model weights into task-relevant and
redundant components in the knowledge-aware subspace, and purify the task
vector by pruning the redundant components. To induce fair pruning efforts
across models, we further introduce a spectral rank allocation strategy by
optimizing a normalized activated pruning error. The task vector purification
by our method as a plug-and-play scheme is applicable across various task
vector-based merging methods to improve their performance. In experiments, we
demonstrate the effectiveness of PAVE across a diverse set of merging methods,
tasks, and model architectures.

</details>


### [129] [Cognitive-Aligned Spatio-Temporal Large Language Models For Next Point-of-Interest Prediction](https://arxiv.org/abs/2510.14702)
*Penglong Zhai,Jie Li,Fanyi Di,Yue Liu,Yifang Yuan,Jie Huang,Peng Wu,Sicong Wang,Mingyang Yin,Tingting Hu,Yao Xu,Xin Li*

Main category: cs.AI

TL;DR: 本文提出了一种名为CoAST的框架，利用自然语言作为接口，融合世界知识、时空轨迹模式、用户画像和情境信息，以提升POI推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）主要在大量非结构化文本语料库上进行预训练，缺乏对结构化地理实体和连续移动模式的理解，并且在工业级POI预测应用中，需要结合世界知识和人类认知（如季节、天气、节假日和用户画像）以提升用户体验和推荐性能。

Method: CoAST框架包含两个阶段：（1）通过在脱敏用户的丰富时空轨迹数据上进行持续预训练来获取推荐知识；（2）认知对齐，通过监督式微调（SFT）和后续的强化学习（RL）阶段，使用丰富的训练数据将认知判断与人类偏好对齐。

Result: 在各种真实世界数据集上进行的广泛离线实验，以及在高德地图App首页的“猜你去哪”中部署的在线实验，证明了CoAST的有效性。

Conclusion: CoAST框架能够有效提升POI推荐效果，具有实际应用价值。

Abstract: The next point-of-interest (POI) recommendation task aims to predict the
users' immediate next destinations based on their preferences and historical
check-ins, holding significant value in location-based services. Recently,
large language models (LLMs) have shown great potential in recommender systems,
which treat the next POI prediction in a generative manner. However, these
LLMs, pretrained primarily on vast corpora of unstructured text, lack the
native understanding of structured geographical entities and sequential
mobility patterns required for next POI prediction tasks. Moreover, in
industrial-scale POI prediction applications, incorporating world knowledge and
alignment of human cognition, such as seasons, weather conditions, holidays,
and users' profiles (such as habits, occupation, and preferences), can enhance
the user experience while improving recommendation performance. To address
these issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a
framework employing natural language as an interface, allowing for the
incorporation of world knowledge, spatio-temporal trajectory patterns,
profiles, and situational information. Specifically, CoAST mainly comprises of
2 stages: (1) Recommendation Knowledge Acquisition through continued
pretraining on the enriched spatial-temporal trajectory data of the
desensitized users; (2) Cognitive Alignment to align cognitive judgments with
human preferences using enriched training data through Supervised Fine-Tuning
(SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline
experiments on various real-world datasets and online experiments deployed in
"Guess Where You Go" of AMAP App homepage demonstrate the effectiveness of
CoAST.

</details>


### [130] [ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling](https://arxiv.org/abs/2510.14703)
*Jianghao Lin,Yuanyuan Shi,Xin Peng,Renjie Ding,Hairui Wang,Yuxuan Peng,Bizhe Bai,Weixi Song,Fengshuo Bai,Huacan Chai,Weinan Zhang,Fei Huang,Ying Wen*

Main category: cs.AI

TL;DR: 本文提出了一种推理扩展框架，该框架结合了细粒度束搜索和过程奖励模型 ToolPRM，该模型对每个函数调用的内部步骤进行评分。


<details>
  <summary>Details</summary>
Motivation: 当前关于推理扩展的研究主要集中在非结构化输出生成任务中，而其在结构化输出（如函数调用）中的应用在很大程度上尚未得到探索。为了弥合这一差距，我们提出了一个推理扩展框架。

Method: 该方法结合了细粒度束搜索与过程奖励模型ToolPRM，该模型对每个函数调用的内部步骤进行评分。为了训练 ToolPRM，我们构建了第一个细粒度的内部调用过程监督数据集，该数据集使用函数屏蔽技术自动注释，从而为结构化工具使用推理提供步骤级别的奖励。

Result: ToolPRM 在预测准确性方面优于粗粒度和结果奖励模型，表明其在监督函数调用推理过程方面具有更强的能力。配备 ToolPRM 的推理扩展技术还可以显着提高各种函数调用任务和基准测试中的骨干模型性能。

Conclusion: 我们揭示了将推理扩展技术应用于结构化输出的关键原则：“多探索但少保留”，这是由于结构化函数调用生成的不可恢复特性所致。

Abstract: Large language models (LLMs) are increasingly demonstrating strong
capabilities as autonomous agents, with function calling serving as a core
mechanism for interaction with the environment. Meanwhile, inference scaling
has become a cutting-edge technique to enhance LLM performance by allocating
more computational resources during the inference process. However, current
research on inference scaling primarily focuses on unstructured output
generation tasks, leaving its application in structured outputs, like function
calling, largely underexplored. To bridge this gap, we propose an inference
scaling framework that combines fine-grained beam search with a process reward
model, ToolPRM, which scores the internal steps of each single function call.
To train ToolPRM, we construct the first fine-grained intra-call process
supervision dataset, automatically annotated with function-masking techniques
to provide step-level rewards for structured tool-use reasoning. Extensive
experiments demonstrate that ToolPRM beats the coarse-grained and outcome
reward models in terms of predictive accuracy, indicating its stronger
capability in supervising the function calling inference process. Inference
scaling technique equipped with ToolPRM also significantly improves the
backbone model performance across various function calling tasks and
benchmarks. More importantly, we reveal a key principle for applying inference
scaling techniques to structured outputs: "explore more but retain less" due to
the unrecoverability characteristics of structured function calling generation.

</details>


### [131] [SimKO: Simple Pass@K Policy Optimization](https://arxiv.org/abs/2510.14807)
*Ruotian Peng,Yi Ren,Zhouliang Yu,Weiyang Liu,Yandong Wen*

Main category: cs.AI

TL;DR: RLVR方法倾向于过度利用而非探索，导致pass@1提高但pass@K (K>1) 降低。SimKO通过非对称方式缓解过度集中问题，提高top-K候选概率，从而鼓励探索，并在数学和逻辑推理基准测试中提高了pass@K。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法存在系统性偏差，过度利用而不足够探索，导致pass@1提高但pass@K (K>1) 降低。

Method: 通过追踪token级别的概率分布来分析RLVR方法的训练动态，并提出Simple Pass@K Optimization (SimKO)方法，该方法通过非对称方式，对正确答案提高top-K候选概率，对错误答案惩罚top-1候选概率，从而缓解过度集中问题。

Result: SimKO在各种数学和逻辑推理基准测试中，对于各种K值，都持续产生更高的pass@K。

Conclusion: SimKO提供了一种简单的方法来改善RLVR的探索能力，通过缓解过度集中问题，鼓励模型进行更广泛的探索。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has advanced the
reasoning capabilities of large language models (LLMs). However, prevailing
RLVR methods exhibit a systematic bias toward exploitation over exploration, as
evidenced by improved pass@1 but reduced pass@K (K>1) performance. To
understand this issue, we analyze training dynamics of RLVR methods by tracking
the token-level probability distributions over vocabulary candidates. Our
analysis reveals a consistent probability concentration effect where the top-1
candidate increasingly accumulates probability mass and suppresses that of
other candidates. More importantly, stronger over-concentration correlates with
worse pass@K performance. Inspired by this finding, we propose Simple Pass@K
Optimization (SimKO), a method designed to mitigate the over-concentration
issue, thereby encouraging exploration. SimKO operates in an asymmetrical
manner. For verified-correct responses, it boosts the probabilities of the
top-K candidates. For verified-incorrect responses, it applies stronger
penalties to the top-1 candidate. We observe that this asymmetric design is
particularly effective at mitigating over-concentration when applied at tokens
with high entropy. Across various math and logical-reasoning benchmarks, SimKO
consistently yields higher pass@K for a wide range of K, providing a simple way
to improve RLVR's exploration.

</details>


### [132] [Agentic NL2SQL to Reduce Computational Costs](https://arxiv.org/abs/2510.14808)
*Dominik Jehle,Lennart Purucker,Frank Hutter*

Main category: cs.AI

TL;DR: Datalake Agent 通过agentic system有效减少了tokens的使用，降低了成本，同时保持了竞争力。


<details>
  <summary>Details</summary>
Motivation: 使用大型语言模型将自然语言查询转换为SQL查询（NL2SQL）需要在大量SQL数据库上处理大量的元信息，导致冗长的提示和高昂的处理成本。

Method: Datalake Agent 采用了一个交互循环来减少使用的元信息。在这个循环中，LLM被用在一个推理框架中，选择性地只请求解决表格问题所需的必要信息。

Result: Datalake Agent 减少了LLM使用的tokens高达87%，从而实现了大幅的成本降低，同时保持了竞争性的性能。

Conclusion: Datalake Agent 通过agentic system有效减少了tokens的使用，降低了成本，同时保持了竞争力。

Abstract: Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL)
has recently been empowered by large language models (LLMs). Using LLMs to
perform NL2SQL methods on a large collection of SQL databases necessitates
processing large quantities of meta-information about the databases, which in
turn results in lengthy prompts with many tokens and high processing costs. To
address this challenge, we introduce Datalake Agent, an agentic system designed
to enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing
direct solvers for NL2SQL that call the LLM once with all meta-information in
the prompt, the Datalake Agent employs an interactive loop to reduce the
utilized meta-information. Within the loop, the LLM is used in a reasoning
framework that selectively requests only the necessary information to solve a
table question answering task. We evaluate the Datalake Agent on a collection
of 23 databases with 100 table question answering tasks. The Datalake Agent
reduces the tokens used by the LLM by up to 87\% and thus allows for
substantial cost reductions while maintaining competitive performance.

</details>


### [133] [RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning](https://arxiv.org/abs/2510.14828)
*Jinrui Liu,Bingyan Nie,Boyu Li,Yaran Chen,Yuze Wang,Shunsen He,Haoran Li*

Main category: cs.AI

TL;DR: 本文提出了一种名为 RoboGPT-R1 的两阶段微调框架，用于提高具身智能体在复杂操作任务中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型和视觉语言模型在执行长程操作任务时，由于常识和推理能力的限制，在复杂真实环境中面临挑战。有监督微调在机器人规划任务中泛化能力差，且物理理解不足。

Method: 该框架首先通过有监督训练获取基础知识，然后通过强化学习解决模型在视觉空间理解和推理方面的不足。设计了一种基于规则的奖励函数，同时考虑长程性能和环境中的动作约束，以实现物理理解和多步骤推理任务中的动作序列一致性。

Result: 在 EmbodiedBench 基准测试中，使用 Qwen2.5-VL-3B 训练的推理模型显著优于更大规模的模型 GPT-4o-mini 21.33%，并且超过了在 Qwen2.5-VL-7B 上训练的其他模型 20.33%。

Conclusion: RoboGPT-R1 框架有效地提高了具身智能体在复杂操作任务中的推理能力，并在 EmbodiedBench 基准测试中取得了优异的性能。

Abstract: Improving the reasoning capabilities of embodied agents is crucial for robots
to complete complex human instructions in long-view manipulation tasks
successfully. Despite the success of large language models and vision language
models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue
facing challenges in performing long-horizon manipulation tasks in complex
real-world environments, owing to their restricted common sense and reasoning
capabilities. Considering that aligning general-purpose vision language models
to robotic planning tasks via supervised fine-tuning suffers from poor
generalization and insufficient physical understanding, we propose RoboGPT-R1,
a two-stage fine-tuning framework for embodied planning. In this framework,
supervised training acquires foundational knowledge through expert sequences,
followed by RL to address the model's shortcomings in visual-spatial
understanding and reasoning. To achieve physical understanding and action
sequence consistency in multi-step reasoning tasks, we design a rule-based
reward function that simultaneously considers long-horizon performance and
action constraint in the environment. The reasoning model, trained on
Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,
by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the
EmbodiedBench benchmark.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [134] [Towards a Multimodal Stream Processing System](https://arxiv.org/abs/2510.14631)
*Uélison Jean Lopes dos Santos,Alessandro Ferri,Szilard Nistor,Riccardo Tommasini,Carsten Binnig,Manisha Luthra*

Main category: cs.DB

TL;DR: 本论文提出了一个将多模态大型语言模型嵌入为一流算子的新一代多模态流系统的愿景，以实现跨多种模态的实时查询处理。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态查询数据库集成方法无法满足流系统的严格延迟和吞吐量要求。

Method: 该方法提出了在逻辑、物理和语义查询转换等各个层面的优化，以减少模型负载，提高吞吐量，同时保持准确性。

Result: 通过一个原型系统，证明了该优化方法可以将性能提高一个数量级以上。

Conclusion: 本文讨论了一个研究路线图，概述了构建可扩展和高效的多模态流处理系统所面临的开放性研究挑战。

Abstract: In this paper, we present a vision for a new generation of multimodal
streaming systems that embed MLLMs as first-class operators, enabling real-time
query processing across multiple modalities. Achieving this is non-trivial:
while recent work has integrated MLLMs into databases for multimodal queries,
streaming systems require fundamentally different approaches due to their
strict latency and throughput requirements. Our approach proposes novel
optimizations at all levels, including logical, physical, and semantic query
transformations that reduce model load to improve throughput while preserving
accuracy. We demonstrate this with \system{}, a prototype leveraging such
optimizations to improve performance by more than an order of magnitude.
Moreover, we discuss a research roadmap that outlines open research challenges
for building a scalable and efficient multimodal stream processing systems.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [135] [FinAI Data Assistant: LLM-based Financial Database Query Processing with the OpenAI Function Calling API](https://arxiv.org/abs/2510.14162)
*Juhyeong Kim,Yejin Kim,Youngbin Lee,Hyunwoo Byun*

Main category: cs.IR

TL;DR: FinAI Data Assistant利用大型语言模型和OpenAI Function Calling API，通过路由用户请求到小型参数化查询库，实现了可靠、低延迟和低成本的金融数据库自然语言查询。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决直接使用LLM进行金融数据库查询时，在可靠性、延迟和成本方面的问题。

Method: 该方法结合大型语言模型与OpenAI Function Calling API，不直接生成SQL，而是将用户请求路由到预先设定的参数化查询。

Result: 实验结果表明，仅使用LLM预测存在误差和前瞻性偏差，但股票代码映射准确率高。FinAI Data Assistant在延迟、成本和可靠性方面优于text-to-SQL基线方法。

Conclusion: FinAI Data Assistant通过牺牲生成灵活性，换取了金融数据库查询的可靠性、低延迟和成本效益。

Abstract: We present FinAI Data Assistant, a practical approach for natural-language
querying over financial databases that combines large language models (LLMs)
with the OpenAI Function Calling API. Rather than synthesizing complete SQL via
text-to-SQL, our system routes user requests to a small library of vetted,
parameterized queries, trading generative flexibility for reliability, low
latency, and cost efficiency. We empirically study three questions: (RQ1)
whether LLMs alone can reliably recall or extrapolate time-dependent financial
data without external retrieval; (RQ2) how well LLMs map company names to stock
ticker symbols; and (RQ3) whether function calling outperforms text-to-SQL for
end-to-end database query processing. Across controlled experiments on prices
and fundamentals, LLM-only predictions exhibit non-negligible error and show
look-ahead bias primarily for stock prices relative to model knowledge cutoffs.
Ticker-mapping accuracy is near-perfect for NASDAQ-100 constituents and high
for S\&P~500 firms. Finally, FinAI Data Assistant achieves lower latency and
cost and higher reliability than a text-to-SQL baseline on our task suite. We
discuss design trade-offs, limitations, and avenues for deployment.

</details>


### [136] [Large Scale Retrieval for the LinkedIn Feed using Causal Language Models](https://arxiv.org/abs/2510.14223)
*Sudarshan Srinivasa Ramanujam,Antonio Alonso,Saurabh Kataria,Siddharth Dangi,Akhilesh Gupta,Birjodh Singh Tiwana,Manas Somaiya,Luke Simon,David Byrne,Sojeong Ha,Sen Zhou,Andrei Akterskii,Zhanglong Liu,Samira Sriram,Crescent Xiong,Zhoutao Pei,Angela Shao,Alex Li,Annie Xiao,Caitlin Kolb,Thomas Kistler,Zach Moore,Hamed Firooz*

Main category: cs.IR

TL;DR: 本文提出了一种新颖的检索方法，该方法微调了一个大型因果语言模型（Meta的LLaMA 3）作为双编码器，仅使用文本输入为用户（成员）和内容（项目）生成高质量的嵌入。


<details>
  <summary>Details</summary>
Motivation: 在LinkedIn Feed等大型推荐系统中，检索阶段对于将数亿潜在候选对象缩小到可管理的排名子集至关重要。LinkedIn的Feed根据成员的主题兴趣，提供来自成员网络外部的建议内容，其中从数亿候选对象池中检索2000个候选对象，延迟预算为几毫秒，入站QPS为每秒数千个。

Method: 微调大型因果语言模型（Meta的LLaMA 3）作为双编码器，仅使用文本输入为用户（成员）和内容（项目）生成高质量的嵌入。描述了端到端pipeline，包括用于嵌入生成的prompt设计，在LinkedIn规模上进行微调的技术以及用于低延迟，具有成本效益的在线serving的基础架构。量化prompt中的数值特征，使信息能够正确编码到嵌入中，从而有助于检索层和排名层之间更好地对齐。

Result: 该系统使用离线指标和在线A/B测试进行了评估，结果表明会员参与度得到了显着提高。我们观察到新会员的显着收益，他们通常缺乏强大的网络连接，这表明高质量的建议内容有助于保留。

Conclusion: 这项工作表明了生成语言模型如何有效地适应工业应用中的实时，高吞吐量检索。

Abstract: In large scale recommendation systems like the LinkedIn Feed, the retrieval
stage is critical for narrowing hundreds of millions of potential candidates to
a manageable subset for ranking. LinkedIn's Feed serves suggested content from
outside of the member's network (based on the member's topical interests),
where 2000 candidates are retrieved from a pool of hundreds of millions
candidate with a latency budget of a few milliseconds and inbound QPS of
several thousand per second. This paper presents a novel retrieval approach
that fine-tunes a large causal language model (Meta's LLaMA 3) as a dual
encoder to generate high quality embeddings for both users (members) and
content (items), using only textual input. We describe the end to end pipeline,
including prompt design for embedding generation, techniques for fine-tuning at
LinkedIn's scale, and infrastructure for low latency, cost effective online
serving. We share our findings on how quantizing numerical features in the
prompt enables the information to get properly encoded in the embedding,
facilitating greater alignment between the retrieval and ranking layer. The
system was evaluated using offline metrics and an online A/B test, which showed
substantial improvements in member engagement. We observed significant gains
among newer members, who often lack strong network connections, indicating that
high-quality suggested content aids retention. This work demonstrates how
generative language models can be effectively adapted for real time, high
throughput retrieval in industrial applications.

</details>


### [137] [Synergistic Integration and Discrepancy Resolution of Contextualized Knowledge for Personalized Recommendation](https://arxiv.org/abs/2510.14257)
*Lingyu Mu,Hao Deng,Haibo Xing,Kaican Lin,Zhitong Zhu,Yu Zhang,Xiaoyi Zeng,Zhengxiao Liu,Zheng Lin,Jinxin Hu*

Main category: cs.IR

TL;DR: CoCo: 使用双重机制动态构建用户特定的上下文知识嵌入，以实现语义和行为潜在维度的深度集成，从而提升推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法采用静态的、基于模式的提示机制，忽略了用户偏好的多样性，并且在语义知识表示和行为特征空间之间实现了表面上的对齐，而没有实现全面的潜在空间集成。

Method: 提出CoCo，一个端到端的框架，通过自适应知识融合和矛盾解决模块，动态构建用户特定的上下文知识嵌入。

Result: 在多个基准数据集和一个企业级电子商务平台上进行的实验评估表明，CoCo具有优越性，在推荐准确性方面比七种最先进的方法提高了8.58%。该框架在生产广告系统上的部署带来了1.91%的销售额增长。

Conclusion: CoCo具有模块化设计和模型无关的架构，为需要知识增强推理和个性化适应的下一代推荐系统提供了一种通用的解决方案。

Abstract: The integration of large language models (LLMs) into recommendation systems
has revealed promising potential through their capacity to extract world
knowledge for enhanced reasoning capabilities. However, current methodologies
that adopt static schema-based prompting mechanisms encounter significant
limitations: (1) they employ universal template structures that neglect the
multi-faceted nature of user preference diversity; (2) they implement
superficial alignment between semantic knowledge representations and behavioral
feature spaces without achieving comprehensive latent space integration. To
address these challenges, we introduce CoCo, an end-to-end framework that
dynamically constructs user-specific contextual knowledge embeddings through a
dual-mechanism approach. Our method realizes profound integration of semantic
and behavioral latent dimensions via adaptive knowledge fusion and
contradiction resolution modules. Experimental evaluations across diverse
benchmark datasets and an enterprise-level e-commerce platform demonstrate
CoCo's superiority, achieving a maximum 8.58% improvement over seven
cutting-edge methods in recommendation accuracy. The framework's deployment on
a production advertising system resulted in a 1.91% sales growth, validating
its practical effectiveness. With its modular design and model-agnostic
architecture, CoCo provides a versatile solution for next-generation
recommendation systems requiring both knowledge-enhanced reasoning and
personalized adaptation.

</details>


### [138] [Large Reasoning Embedding Models: Towards Next-Generation Dense Retrieval Paradigm](https://arxiv.org/abs/2510.14321)
*Jianting Tang,Dongshuai Li,Tao Wen,Fuyu Lv,Dan Ou,Linli Xu*

Main category: cs.IR

TL;DR: 提出了一种新的名为 LREM 的检索模型，该模型通过将推理过程整合到表征学习中，从而提高了电商搜索系统中对于困难查询的检索准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于 LLM 的 embedding 模型在处理词汇差异显著的困难查询时，由于语义准确性不足和过度依赖对比学习导致的共现模式捕获，性能会显著下降。

Method: 提出了 LREM 模型，该模型首先对困难查询进行推理以实现对原始查询的深刻理解，然后生成用于检索的推理增强查询 embedding。采用两阶段训练过程：第一阶段使用 SFT 和 InfoNCE 损失优化 LLM，第二阶段通过强化学习进一步优化推理轨迹。

Result: 大量的离线和在线实验验证了 LREM 的有效性，并于 2025 年 8 月部署在中国最大的电子商务平台上。

Conclusion: LREM 模型通过整合推理过程到表征学习中，有效弥合了原始查询和目标 item 之间的语义差距，显著提高了检索准确率。

Abstract: In modern e-commerce search systems, dense retrieval has become an
indispensable component. By computing similarities between query and item
(product) embeddings, it efficiently selects candidate products from
large-scale repositories. With the breakthroughs in large language models
(LLMs), mainstream embedding models have gradually shifted from BERT to LLMs
for more accurate text modeling. However, these models still adopt
direct-embedding methods, and the semantic accuracy of embeddings remains
inadequate. Therefore, contrastive learning is heavily employed to achieve
tight semantic alignment between positive pairs. Consequently, such models tend
to capture statistical co-occurrence patterns in the training data, biasing
them toward shallow lexical and semantic matches. For difficult queries
exhibiting notable lexical disparity from target items, the performance
degrades significantly. In this work, we propose the Large Reasoning Embedding
Model (LREM), which novelly integrates reasoning processes into representation
learning. For difficult queries, LREM first conducts reasoning to achieve a
deep understanding of the original query, and then produces a
reasoning-augmented query embedding for retrieval. This reasoning process
effectively bridges the semantic gap between original queries and target items,
significantly improving retrieval accuracy. Specifically, we adopt a two-stage
training process: the first stage optimizes the LLM on carefully curated
Query-CoT-Item triplets with SFT and InfoNCE losses to establish preliminary
reasoning and embedding capabilities, and the second stage further refines the
reasoning trajectories via reinforcement learning (RL). Extensive offline and
online experiments validate the effectiveness of LREM, leading to its
deployment on China's largest e-commerce platform since August 2025.

</details>


### [139] [Ensembling Multiple Hallucination Detectors Trained on VLLM Internal Representations](https://arxiv.org/abs/2510.14330)
*Yuto Nakamizo,Ryuhei Miyazato,Hikaru Tanabe,Ryuta Yamakura,Kiori Hatanaka*

Main category: cs.IR

TL;DR: y3h2 团队在 KDD Cup 2025 Meta CRAG-MM 挑战赛中获得第五名。该团队专注于减少 VLM 内部表征的幻觉，以提高 VQA 的准确性。


<details>
  <summary>Details</summary>
Motivation: 该论文的动机是解决视觉问答（VQA）中，特别是关于图像事实性问题，由于错误答案会导致负分，因此需要减少视觉语言模型（VLM）产生的幻觉。

Method: 该团队训练了基于 Logistic 回归的幻觉检测模型，利用隐藏状态和特定注意力头的输出，并采用了这些模型的集成。

Result: 该方法牺牲了一些正确答案，但显著减少了幻觉。

Conclusion: 该方法最终在排行榜上名列前茅，证明了减少幻觉策略的有效性。

Abstract: This paper presents the 5th place solution by our team, y3h2, for the Meta
CRAG-MM Challenge at KDD Cup 2025. The CRAG-MM benchmark is a visual question
answering (VQA) dataset focused on factual questions about images, including
egocentric images. The competition was contested based on VQA accuracy, as
judged by an LLM-based automatic evaluator. Since incorrect answers result in
negative scores, our strategy focused on reducing hallucinations from the
internal representations of the VLM. Specifically, we trained logistic
regression-based hallucination detection models using both the hidden_state and
the outputs of specific attention heads. We then employed an ensemble of these
models. As a result, while our method sacrificed some correct answers, it
significantly reduced hallucinations and allowed us to place among the top
entries on the final leaderboard. For implementation details and code, please
refer to
https://gitlab.aicrowd.com/htanabe/meta-comprehensive-rag-benchmark-starter-kit.

</details>


### [140] [GemiRec: Interest Quantization and Generation for Multi-Interest Recommendation](https://arxiv.org/abs/2510.14626)
*Zhibo Wu,Yunfan Wu,Quan Liu,Lin Jiang,Ping Yang,Yao Hu*

Main category: cs.IR

TL;DR: GemiRec is a multi-interest recommendation framework addressing interest collapse and insufficient modeling of interest evolution.


<details>
  <summary>Details</summary>
Motivation: Existing multi-interest recommendation methods suffer from interest collapse and fail to capture latent interests.

Method: The paper proposes GemiRec, a framework with interest quantization and generation, comprising Interest Dictionary Maintenance, Multi-Interest Posterior Distribution, and Multi-Interest Retrieval modules.

Result: Theoretical and empirical analyses, along with experiments, demonstrate GemiRec's advantages and effectiveness. It has been deployed in production since March 2025.

Conclusion: GemiRec effectively addresses the limitations of existing multi-interest recommendation methods and has practical value in industrial applications.

Abstract: Multi-interest recommendation has gained attention, especially in industrial
retrieval stage. Unlike classical dual-tower methods, it generates multiple
user representations instead of a single one to model comprehensive user
interests. However, prior studies have identified two underlying limitations:
the first is interest collapse, where multiple representations homogenize. The
second is insufficient modeling of interest evolution, as they struggle to
capture latent interests absent from a user's historical behavior. We begin
with a thorough review of existing works in tackling these limitations. Then,
we attempt to tackle these limitations from a new perspective. Specifically, we
propose a framework-level refinement for multi-interest recommendation, named
GemiRec. The proposed framework leverages interest quantization to enforce a
structural interest separation and interest generation to learn the evolving
dynamics of user interests explicitly. It comprises three modules: (a) Interest
Dictionary Maintenance Module (IDMM) maintains a shared quantized interest
dictionary. (b) Multi-Interest Posterior Distribution Module (MIPDM) employs a
generative model to capture the distribution of user future interests. (c)
Multi-Interest Retrieval Module (MIRM) retrieves items using multiple
user-interest representations. Both theoretical and empirical analyses, as well
as extensive experiments, demonstrate its advantages and effectiveness.
Moreover, it has been deployed in production since March 2025, showing its
practical value in industrial applications.

</details>


### [141] [MR.Rec: Synergizing Memory and Reasoning for Personalized Recommendation Assistant with LLMs](https://arxiv.org/abs/2510.14629)
*Jiani Huang,Xingchen Zou,Lianghao Xia,Qing Li*

Main category: cs.IR

TL;DR: 提出了一种新的基于LLM的推荐框架MR.Rec，它结合了记忆和推理，以实现深度个性化和智能推理，尤其是在交互式场景中。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于有限的上下文窗口和单轮推理，阻碍了它们捕获动态用户偏好和主动推理推荐上下文的能力。

Method: 1. 开发了一个综合的检索增强生成（RAG）系统，有效地索引和检索相关的外部记忆，以增强LLM的个性化能力。2. 通过整合推理增强的记忆检索，超越了传统的基于查询的检索。3. 设计了一个强化学习框架，训练LLM自主学习记忆利用和推理改进的有效策略。

Result: MR.Rec在多个指标上显著优于最先进的基线。

Conclusion: MR.Rec通过结合动态记忆检索和自适应推理，确保了更准确、上下文感知和高度个性化的推荐，验证了其在提供智能和个性化推荐方面的有效性。

Abstract: The application of Large Language Models (LLMs) in recommender systems faces
key challenges in delivering deep personalization and intelligent reasoning,
especially for interactive scenarios. Current methods are often constrained by
limited context windows and single-turn reasoning, hindering their ability to
capture dynamic user preferences and proactively reason over recommendation
contexts. To address these limitations, we propose MR.Rec, a novel framework
that synergizes memory and reasoning for LLM-based recommendations. To achieve
personalization, we develop a comprehensive Retrieval-Augmented Generation
(RAG) system that efficiently indexes and retrieves relevant external memory to
enhance LLM personalization capabilities. Furthermore, to enable the synergy
between memory and reasoning, our RAG system goes beyond conventional
query-based retrieval by integrating reasoning enhanced memory retrieval.
Finally, we design a reinforcement learning framework that trains the LLM to
autonomously learn effective strategies for both memory utilization and
reasoning refinement. By combining dynamic memory retrieval with adaptive
reasoning, this approach ensures more accurate, context-aware, and highly
personalized recommendations. Extensive experiments demonstrate that MR.Rec
significantly outperforms state-of-the-art baselines across multiple metrics,
validating its efficacy in delivering intelligent and personalized
recommendations. We will release code and data upon paper notification.

</details>


### [142] [Causality Enhancement for Cross-Domain Recommendation](https://arxiv.org/abs/2510.14641)
*Zhibo Wu,Yunfan Wu,Lin Jiang,Ping Yang,Yao Hu*

Main category: cs.IR

TL;DR: 这篇论文提出了一种名为 CE-CDR 的因果增强框架，用于跨域推荐，以解决现有方法中存在的不足。


<details>
  <summary>Details</summary>
Motivation: 现有跨域推荐方法在利用源域信息时，可能会由于任务不一致或未考虑因果关系而导致效果不佳。

Method: 该框架首先将跨域推荐问题建模为因果图，然后构建一个因果关系数据集，并提出了一个理论上无偏的 Partial Label Causal Loss，以学习鲁棒的跨域表示。

Result: 理论和实验分析表明，CE-CDR 是合理和有效的，并且具有广泛的适用性。

Conclusion: CE-CDR 作为一种模型无关的插件，已于 2025 年 4 月部署到生产环境中，显示了其在实际应用中的价值。

Abstract: Cross-domain recommendation forms a crucial component in recommendation
systems. It leverages auxiliary information through source domain tasks or
features to enhance target domain recommendations. However, incorporating
inconsistent source domain tasks may result in insufficient cross-domain
modeling or negative transfer. While incorporating source domain features
without considering the underlying causal relationships may limit their
contribution to final predictions. Thus, a natural idea is to directly train a
cross-domain representation on a causality-labeled dataset from the source to
target domain. Yet this direction has been rarely explored, as identifying
unbiased real causal labels is highly challenging in real-world scenarios. In
this work, we attempt to take a first step in this direction by proposing a
causality-enhanced framework, named CE-CDR. Specifically, we first reformulate
the cross-domain recommendation as a causal graph for principled guidance. We
then construct a causality-aware dataset heuristically. Subsequently, we derive
a theoretically unbiased Partial Label Causal Loss to generalize beyond the
biased causality-aware dataset to unseen cross-domain patterns, yielding an
enriched cross-domain representation, which is then fed into the target model
to enhance target-domain recommendations. Theoretical and empirical analyses,
as well as extensive experiments, demonstrate the rationality and effectiveness
of CE-CDR and its general applicability as a model-agnostic plugin. Moreover,
it has been deployed in production since April 2025, showing its practical
value in real-world applications.

</details>


### [143] [Dataset Pruning in RecSys and ML: Best Practice or Mal-Practice?](https://arxiv.org/abs/2510.14704)
*Leonie Winter*

Main category: cs.IR

TL;DR: 本研究探讨了数据修剪对推荐系统数据集和算法性能的影响，发现常用的核心修剪方法可能会高度选择性地保留少量原始用户，并且在修剪后的数据上训练和测试算法会获得较高评分，但在未修剪的测试集上评估时，这种优势会消失。


<details>
  <summary>Details</summary>
Motivation: 评估在推荐系统研究中，离线评估在很大程度上依赖于数据集，其中许多数据集经过修剪，例如广泛使用的 MovieLens 集合。本研究着眼于数据修剪的影响，特别是在数据集特征和算法性能方面。

Method: 分析了五个基准数据集的未修剪形式和五个连续修剪级别（5、10、20、50、100）。对于每个核心集，我们检查了结构和分布特征，并训练和测试了 11 个代表性算法。为了进一步评估修剪后的数据集是否会导致人为夸大的性能结果，我们还评估了在修剪后的训练集上训练但在未修剪的数据上测试的模型。

Result: 常用的核心修剪方法可能会高度选择性，在某些数据集中只留下 2% 的原始用户。当在修剪后的数据上进行训练和测试时，传统算法获得了更高的 nDCG@10 分数；然而，当在未修剪的测试集上评估时，这种优势在很大程度上消失了。在所有算法中，当在未修剪的数据上测试时，性能随着修剪水平的提高而下降

Conclusion: 数据集减少对推荐算法的性能有影响

Abstract: Offline evaluations in recommender system research depend heavily on
datasets, many of which are pruned, such as the widely used MovieLens
collections. This thesis examines the impact of data pruning - specifically,
removing users with fewer than a specified number of interactions - on both
dataset characteristics and algorithm performance. Five benchmark datasets were
analysed in both their unpruned form and at five successive pruning levels (5,
10, 20, 50, 100). For each coreset, we examined structural and distributional
characteristics and trained and tested eleven representative algorithms. To
further assess if pruned datasets lead to artificially inflated performance
results, we also evaluated models trained on the pruned train sets but tested
on unpruned data. Results show that commonly applied core pruning can be highly
selective, leaving as little as 2% of the original users in some datasets.
Traditional algorithms achieved higher nDCG@10 scores when both training and
testing on pruned data; however, this advantage largely disappeared when
evaluated on unpruned test sets. Across all algorithms, performance declined
with increasing pruning levels when tested on unpruned data, highlighting the
impact of dataset reduction on the performance of recommender algorithms.

</details>


### [144] [Cross-Scenario Unified Modeling of User Interests at Billion Scale](https://arxiv.org/abs/2510.14788)
*Manjie Xu,Cheng Chen,Xin Jia,Jingyi Zhou,Yongji Wu,Zejian Wang,Chi Zhang,Kai Zuo,Yibo Chen,Xu Tang,Yao Hu,Yixin Zhu*

Main category: cs.IR

TL;DR: 提出了一种名为RED-Rec的、由LLM增强的分层推荐引擎，用于处理行业级内容推荐系统中的多样化场景。


<details>
  <summary>Details</summary>
Motivation: 传统的推荐系统通常优先考虑孤立场景中的业务指标优化，忽略了跨场景的行为信号，并且难以整合LLM等先进技术。

Method: RED-Rec通过聚合和综合来自不同场景的行为来统一用户兴趣表示，从而实现全面的项目和用户建模。核心是一个双塔LLM驱动的框架，支持细致的多方面表示和高效的部署。

Result: 在RedNote上对数百万用户进行了在线A/B测试，结果表明在内容推荐和广告定向任务中都取得了显著的性能提升。

Conclusion: 这项工作推进了统一的用户建模，在大型UGC平台中实现了更深入的个性化，并促进了更有意义的用户参与。

Abstract: User interests on content platforms are inherently diverse, manifesting
through complex behavioral patterns across heterogeneous scenarios such as
search, feed browsing, and content discovery. Traditional recommendation
systems typically prioritize business metric optimization within isolated
specific scenarios, neglecting cross-scenario behavioral signals and struggling
to integrate advanced techniques like LLMs at billion-scale deployments, which
finally limits their ability to capture holistic user interests across platform
touchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender
Engine for Diversified scenarios, tailored for industry-level content
recommendation systems. RED-Rec unifies user interest representations across
multiple behavioral contexts by aggregating and synthesizing actions from
varied scenarios, resulting in comprehensive item and user modeling. At its
core, a two-tower LLM-powered framework enables nuanced, multifaceted
representations with deployment efficiency, and a scenario-aware dense mixing
and querying policy effectively fuses diverse behavioral signals to capture
cross-scenario user intent patterns and express fine-grained, context-specific
intents during serving. We validate RED-Rec through online A/B testing on
hundreds of millions of users in RedNote through online A/B testing, showing
substantial performance gains in both content recommendation and advertisement
targeting tasks. We further introduce a million-scale sequential recommendation
dataset, RED-MMU, for comprehensive offline training and evaluation. Our work
advances unified user modeling, unlocking deeper personalization and fostering
more meaningful user engagement in large-scale UGC platforms.

</details>


### [145] [A Simulation Framework for Studying Systemic Effects of Feedback Loops in Recommender Systems](https://arxiv.org/abs/2510.14857)
*Gabriele Barlacchi,Margherita Lalli,Emanuele Ferragina,Fosca Giannotti,Luca Pappalardo*

Main category: cs.IR

TL;DR: 本文介绍了一个模拟框架，用于模拟在线零售环境中推荐系统的反馈循环，该循环会影响个体行为和集体市场动态。


<details>
  <summary>Details</summary>
Motivation: 研究推荐系统如何影响多样性、购买集中度和用户同质化。

Method: 使用 Amazon e-Commerce 数据集，分析不同的推荐算法。

Result: 反馈循环增加了个人多样性，但降低了集体多样性，并将需求集中在少数热门商品上。某些推荐系统还会随着时间的推移增加用户同质化。

Conclusion: 需要在个性化和长期多样性之间取得平衡。

Abstract: Recommender systems continuously interact with users, creating feedback loops
that shape both individual behavior and collective market dynamics. This paper
introduces a simulation framework to model these loops in online retail
environments, where recommenders are periodically retrained on evolving
user-item interactions. Using the Amazon e-Commerce dataset, we analyze how
different recommendation algorithms influence diversity, purchase
concentration, and user homogenization over time. Results reveal a systematic
trade-off: while the feedback loop increases individual diversity, it
simultaneously reduces collective diversity and concentrates demand on a few
popular items. Moreover, for some recommender systems, the feedback loop
increases user homogenization over time, making user purchase profiles
increasingly similar. These findings underscore the need for recommender
designs that balance personalization with long-term diversity.

</details>


### [146] [Fantastic (small) Retrievers and How to Train Them: mxbai-edge-colbert-v0 Tech Report](https://arxiv.org/abs/2510.14880)
*Rikiya Takehi,Benjamin Clavié,Sean Lee,Aamir Shakir*

Main category: cs.IR

TL;DR: 提出了mxbai-edge-colbert-v0模型，参数量分别为17M和32M。


<details>
  <summary>Details</summary>
Motivation: 旨在支持各种规模的检索，从云端大规模检索到可在任何设备上本地运行的模型。

Method: 通过大量实验改进检索和后期交互模型，并将其提炼成更小的模型。

Result: mxbai-edge-colbert-v0在常见短文本基准测试（BEIR）上优于ColBERTv2，并在长文本任务中表现出前所未有的效率。

Conclusion: mxbai-edge-colbert-v0是一个非常出色的小型模型，可以作为未来实验的坚实基础。

Abstract: In this work, we introduce mxbai-edge-colbert-v0 models, at two different
parameter counts: 17M and 32M. As part of our research, we conduct numerous
experiments to improve retrieval and late-interaction models, which we intend
to distill into smaller models as proof-of-concepts. Our ultimate aim is to
support retrieval at all scales, from large-scale retrieval which lives in the
cloud to models that can run locally, on any device. mxbai-edge-colbert-v0 is a
model that we hope will serve as a solid foundation backbone for all future
experiments, representing the first version of a long series of small
proof-of-concepts. As part of the development of mxbai-edge-colbert-v0, we
conducted multiple ablation studies, of which we report the results. In terms
of downstream performance, mxbai-edge-colbert-v0 is a particularly capable
small model, outperforming ColBERTv2 on common short-text benchmarks (BEIR) and
representing a large step forward in long-context tasks, with unprecedented
efficiency.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [147] [Large Language Models for Real-World IoT Device Identification](https://arxiv.org/abs/2510.13817)
*Rameen Mahmood,Tousif Ahmed,Sai Teja Peddinti,Danny Yuxing Huang*

Main category: cs.LG

TL;DR: 本文提出了一种基于异构网络元数据的语义推理管道，将设备识别重构为语言建模任务。


<details>
  <summary>Details</summary>
Motivation: 当前物联网设备快速扩张，超出识别方法能力，给安全、隐私和网络问责带来风险。开放世界环境中，流量元数据不完整、嘈杂或被混淆，加剧了这些挑战。

Method: 利用互信息和基于熵的稳定性分数指导的大型语言模型集合，为IoT Inspector数据集生成高质量的供应商标签。然后，使用课程学习对量化的LLaMA3.18B模型进行指令调整，以支持稀疏性和长尾供应商分布下的泛化。

Result: 该模型在2015家供应商中实现了98.25%的top-1准确率和90.73%的宏准确率，同时保持了对缺失字段、协议漂移和对抗性操纵的弹性。

Conclusion: 指令调整的LLM为大规模实际设备识别提供了可扩展且可解释的基础。

Abstract: The rapid expansion of IoT devices has outpaced current identification
methods, creating significant risks for security, privacy, and network
accountability. These challenges are heightened in open-world environments,
where traffic metadata is often incomplete, noisy, or intentionally obfuscated.
We introduce a semantic inference pipeline that reframes device identification
as a language modeling task over heterogeneous network metadata. To construct
reliable supervision, we generate high-fidelity vendor labels for the IoT
Inspector dataset, the largest real-world IoT traffic corpus, using an ensemble
of large language models guided by mutual-information and entropy-based
stability scores. We then instruction-tune a quantized LLaMA3.18B model with
curriculum learning to support generalization under sparsity and long-tail
vendor distributions. Our model achieves 98.25% top-1 accuracy and 90.73% macro
accuracy across 2,015 vendors while maintaining resilience to missing fields,
protocol drift, and adversarial manipulation. Evaluation on an independent IoT
testbed, coupled with explanation quality and adversarial stress tests,
demonstrates that instruction-tuned LLMs provide a scalable and interpretable
foundation for real-world device identification at scale.

</details>


### [148] [Self-Training with Dynamic Weighting for Robust Gradual Domain Adaptation](https://arxiv.org/abs/2510.13864)
*Zixi Wang,Yushe Cao,Yubo Huang,Jinzhu Wei,Jingzehua Xu,Shuai Zhang,Xin Lai*

Main category: cs.LG

TL;DR: 提出了一种新的名为“具有动态加权的自训练（STDW）”的方法，旨在通过解决从源域到目标域的平滑知识迁移的挑战来增强渐进域适应（GDA）中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统GDA方法通过中间域和自训练来缓解域漂移，但通常会遭受低效的知识迁移或不完整的中间数据的影响。

Method: 引入了一种动态加权机制，该机制自适应地平衡训练期间源域和目标域的损失贡献。具体来说，设计了一个由时变超参数 $\varrho$（从 0 到 1）控制的优化框架，该框架控制特定于域的学习的强度并确保稳定的适应。该方法利用自训练来生成伪标签，并优化加权目标函数以进行迭代模型更新，从而在中间域中保持鲁棒性。

Result: 在旋转 MNIST、颜色偏移 MNIST、人像数据集和 Cover Type 数据集上的实验表明，STDW 优于现有基线。消融研究进一步验证了 $\varrho$ 的动态调度在实现渐进适应方面的关键作用，证实了其在减少域偏差和提高泛化能力方面的有效性。

Conclusion: 这项工作为稳健的渐进式领域适应提供了理论见解和实践框架，在动态现实世界场景中具有潜在的应用。

Abstract: In this paper, we propose a new method called Self-Training with Dynamic
Weighting (STDW), which aims to enhance robustness in Gradual Domain Adaptation
(GDA) by addressing the challenge of smooth knowledge migration from the source
to the target domain. Traditional GDA methods mitigate domain shift through
intermediate domains and self-training but often suffer from inefficient
knowledge migration or incomplete intermediate data. Our approach introduces a
dynamic weighting mechanism that adaptively balances the loss contributions of
the source and target domains during training. Specifically, we design an
optimization framework governed by a time-varying hyperparameter $\varrho$
(progressing from 0 to 1), which controls the strength of domain-specific
learning and ensures stable adaptation. The method leverages self-training to
generate pseudo-labels and optimizes a weighted objective function for
iterative model updates, maintaining robustness across intermediate domains.
Experiments on rotated MNIST, color-shifted MNIST, portrait datasets, and the
Cover Type dataset demonstrate that STDW outperforms existing baselines.
Ablation studies further validate the critical role of $\varrho$'s dynamic
scheduling in achieving progressive adaptation, confirming its effectiveness in
reducing domain bias and improving generalization. This work provides both
theoretical insights and a practical framework for robust gradual domain
adaptation, with potential applications in dynamic real-world scenarios. The
code is available at https://github.com/Dramwig/STDW.

</details>


### [149] [Deep Edge Filter: Return of the Human-Crafted Layer in Deep Learning](https://arxiv.org/abs/2510.13865)
*Dongkwan Lee,Junhoo Lee,Nojun Kwak*

Main category: cs.LG

TL;DR: 提出了一种新的深度边缘滤波器，通过对深度神经网络特征应用高通滤波来提高模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 该方法基于以下假设：神经网络在高频分量中编码任务相关的语义信息，而在深度特征的低频分量中存储特定领域的偏差。

Method: 通过从原始特征中减去低通滤波后的输出，该方法在保留架构完整性的同时隔离了可泛化的表示。

Result: 在视觉、文本、3D和音频等不同领域进行实验，结果表明，无论模型架构和数据模态如何，该方法都能持续提高性能。

Conclusion: 分析表明，该方法诱导特征稀疏化并有效隔离高频分量，为我们的核心假设提供了经验验证。

Abstract: We introduce the Deep Edge Filter, a novel approach that applies high-pass
filtering to deep neural network features to improve model generalizability.
Our method is motivated by our hypothesis that neural networks encode
task-relevant semantic information in high-frequency components while storing
domain-specific biases in low-frequency components of deep features. By
subtracting low-pass filtered outputs from original features, our approach
isolates generalizable representations while preserving architectural
integrity. Experimental results across diverse domains such as Vision, Text,
3D, and Audio demonstrate consistent performance improvements regardless of
model architecture and data modality. Analysis reveals that our method induces
feature sparsification and effectively isolates high-frequency components,
providing empirical validation of our core hypothesis. The code is available at
https://github.com/dongkwani/DeepEdgeFilter.

</details>


### [150] [CoLoR-GAN: Continual Few-Shot Learning with Low-Rank Adaptation in Generative Adversarial Networks](https://arxiv.org/abs/2510.13869)
*Munsif Ali,Leonardo Rossi,Massimo Bertozzi*

Main category: cs.LG

TL;DR: 提出了一种名为CoLoR-GAN的框架，用于解决GAN中的持续少样本学习问题，该框架利用低秩张量来有效调整模型以适应目标任务，同时减少所需参数的数量。


<details>
  <summary>Details</summary>
Motivation: 现有的最先进方法（如LFS-GAN）在每次训练迭代中引入了大量新的权重，从长远来看，这将变得非常重要。因此，本文致力于在GAN中实现具有低秩适应的持续少样本学习。

Method: 引入了一种LoRA in LoRA (LLoRA) 技术用于卷积层，并对LoRA的超参数选择进行了实证研究。

Result: 在多个基准CL和FS任务上的实验表明，CoLoR-GAN是有效的，达到了SOTA性能，但资源数量大大减少。

Conclusion: CoLoR-GAN在持续少样本GAN学习中实现了SOTA性能，同时显著降低了资源需求。

Abstract: Continual learning (CL) in the context of Generative Adversarial Networks
(GANs) remains a challenging problem, particularly when it comes to learn from
a few-shot (FS) samples without catastrophic forgetting. Current most effective
state-of-the-art (SOTA) methods, like LFS-GAN, introduce a non-negligible
quantity of new weights at each training iteration, which would become
significant when considering the long term. For this reason, this paper
introduces \textcolor{red}{\textbf{\underline{c}}}ontinual
few-sh\textcolor{red}{\textbf{\underline{o}}}t learning with
\textcolor{red}{\textbf{\underline{lo}}}w-\textcolor{red}{\textbf{\underline{r}}}ank
adaptation in GANs named CoLoR-GAN, a framework designed to handle both FS and
CL together, leveraging low-rank tensors to efficiently adapt the model to
target tasks while reducing even more the number of parameters required.
Applying a vanilla LoRA implementation already permitted us to obtain pretty
good results. In order to optimize even further the size of the adapters, we
challenged LoRA limits introducing a LoRA in LoRA (LLoRA) technique for
convolutional layers. Finally, aware of the criticality linked to the choice of
the hyperparameters of LoRA, we provide an empirical study to easily find the
best ones. We demonstrate the effectiveness of CoLoR-GAN through experiments on
several benchmark CL and FS tasks and show that our model is efficient,
reaching SOTA performance but with a number of resources enormously reduced.
Source code is available on
\href{https://github.com/munsifali11/CoLoR-GAN}{Github.

</details>


### [151] [Joint Discriminative-Generative Modeling via Dual Adversarial Training](https://arxiv.org/abs/2510.13872)
*Xuwang Yin,Claire Zhang,Julie Steele,Nir Shavit,Tony T. Wang*

Main category: cs.LG

TL;DR: 提出了一种新的训练框架，该框架集成了对抗训练 (AT) 原理，以实现判别鲁棒性和稳定的生成学习。


<details>
  <summary>Details</summary>
Motivation: 在单个框架内同时实现鲁棒分类和高保真生成建模是一项重大挑战。混合方法（如 Joint Energy-Based Models (JEM)）将分类器解释为 EBM，但通常受到 SGLD 训练中固有的不稳定性和不良样本质量的限制。

Method: 该方法引入了三个关键创新：(1) 使用基于 AT 的稳定方法替换基于 SGLD 的 JEM 学习，该方法通过使用 BCE 损失区分真实数据和 PGD 生成的对比样本来优化能量函数；(2) 判别组件的协同对抗训练，增强了分类鲁棒性，同时消除了对显式梯度惩罚的需要；(3) 两阶段训练程序，以解决批归一化和 EBM 训练之间的不兼容性。

Result: 在 CIFAR-10、CIFAR-100 和 ImageNet 上的实验表明，该方法在现有混合模型的基础上大大提高了对抗鲁棒性，同时保持了竞争性的生成性能。在 ImageNet 上，当针对生成建模进行优化时，该模型的生成保真度超过了 BigGAN，并且接近扩散模型，代表了第一个基于 MCMC 的 EBM 方法，可在复杂的高分辨率数据集上实现高质量生成。

Conclusion: 该方法解决了限制 JEM 扩展的关键稳定性问题，并证明对抗训练可以作为能够生成和稳健分类视觉数据的统一框架的有效基础。

Abstract: Simultaneously achieving robust classification and high-fidelity generative
modeling within a single framework presents a significant challenge. Hybrid
approaches, such as Joint Energy-Based Models (JEM), interpret classifiers as
EBMs but are often limited by the instability and poor sample quality inherent
in SGLD-based training. We address these limitations by proposing a novel
training framework that integrates adversarial training (AT) principles for
both discriminative robustness and stable generative learning. The proposed
method introduces three key innovations: (1) the replacement of SGLD-based JEM
learning with a stable, AT-based approach that optimizes the energy function by
discriminating between real data and PGD-generated contrastive samples using
the BCE loss; (2) synergistic adversarial training for the discriminative
component that enhances classification robustness while eliminating the need
for explicit gradient penalties; and (3) a two-stage training procedure to
resolve the incompatibility between batch normalization and EBM training.
Experiments on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our method
substantially improves adversarial robustness over existing hybrid models while
maintaining competitive generative performance. On ImageNet, when optimized for
generative modeling, our model's generative fidelity surpasses that of BigGAN
and approaches diffusion models, representing the first MCMC-based EBM approach
to achieve high-quality generation on complex, high-resolution datasets. Our
approach addresses key stability issues that have limited JEM scaling and
demonstrates that adversarial training can serve as an effective foundation for
unified frameworks capable of generating and robustly classifying visual data.

</details>


### [152] [K-frames: Scene-Driven Any-k Keyframe Selection for long video understanding](https://arxiv.org/abs/2510.13891)
*Yifeng Yao,Yike Yun,Jing Wang,Huishuai Zhang,Dongyan Zhao,Ke Tian,Zhihao Wang,Minghui Qiu,Tao Wang*

Main category: cs.LG

TL;DR: K-frames提出了一种新的场景驱动的关键帧选择范例，可以保留时间连续性，通过预测语义连贯的、与查询相关的剪辑来实现任意k关键帧选择，以满足不同的用户预算。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大型语言模型在长视频理解方面受到上下文窗口和计算成本的限制，并且均匀帧采样会导致大量信息丢失。现有的关键帧选择方法通常产生稀疏且时间上不连续的帧，忽略了场景的连续性，并且缺乏多尺度帧选择的灵活性。

Method: K-frames首先引入了一个名为PeakClips的数据集，该数据集包含20万个由查询条件约束的视频亮点。在此基础上，K-frames使用一个三阶段的渐进课程学习剪辑到帧的选择，包括两个用于时间定位和关键剪辑感知的监督微调阶段，以及一个直接优化下游任务场景驱动预测策略的强化学习阶段。

Result: 在主要的长时间视频理解基准上进行的大量实验表明，K-frames为各种规模的关键帧选择提供了一种有效、可解释和即插即用的解决方案。

Conclusion: K-frames 是一种用于场景驱动的关键帧选择的有效方法，它通过预测语义连贯的、与查询相关的剪辑来实现任意 k 关键帧选择，并且在多个长视频理解基准测试中表现出色。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
capabilities in image understanding, but long-video are constrained by context
windows and computational cost. Uniform frame sampling often leads to
substantial information loss. Meanwhile existing keyframe selection methods
such as text-frame retrieval or RL-based frame optimization typically yield
sparse and temporally disjointed frames, overlooking scene continuity and
lacking flexibility for multi-scale frame selection. To address these
limitations, we introduce K-frames, a novel paradigm for scene-driven keyframe
selection that preserves temporal continuity. Instead of selecting individual
frames, K-frames predicts semantically coherent, query-relevant clips, which
enables any-k keyframes selection to meet diverse user budgets. To achieve this
approach, we first introduce PeakClips, a dataset of 200K video highlights
conditioned by query. Building on this dataset, K-frames learns clip2frame
selection using a three-stage progressive curriculum. It involves two
Supervised Fine-Tuning stages for temporal grounding and key-clip perception,
followed by a Reinforcement Learning stage that directly optimizes the
scene-driven prediction policy for downstream task without further annotations.
Extensive experiments on major long-video understanding benchmarks demonstrate
that K-frames provides an effective, interpretable, and plug-and-play solution
for keyframe selection at various scales. Our dataset and model will be
available.

</details>


### [153] [Multi-View Semi-Supervised Label Distribution Learning with Local Structure Complementarity](https://arxiv.org/abs/2510.13917)
*Yanshan Xiao,Kaihong Wu,Bo Liu*

Main category: cs.LG

TL;DR: 本文提出了一种新的多视角半监督标签分布学习(MVSS-LDL)方法。


<details>
  <summary>Details</summary>
Motivation: 现有的标签分布学习(LDL)方法主要针对单视角有标签数据，而忽略了多视角半监督学习问题。

Method: 该方法首先探索每个视角的局部近邻结构，然后利用多视角局部近邻结构的互补性来补充每个视角的近邻信息，最后构建基于图学习的多视角半监督LDL模型。

Result: 数值实验表明，MVSS-LDL比现有的单视角LDL方法具有更好的分类性能。

Conclusion: 本文首次尝试解决多视角LDL问题，并验证了所提出方法的有效性。

Abstract: Label distribution learning (LDL) is a paradigm that each sample is
associated with a label distribution. At present, the existing approaches are
proposed for the single-view LDL problem with labeled data, while the
multi-view LDL problem with labeled and unlabeled data has not been considered.
In this paper, we put forward the multi-view semi-supervised label distribution
learning with local structure complementarity (MVSS-LDL) approach, which
exploits the local nearest neighbor structure of each view and emphasizes the
complementarity of local nearest neighbor structures in multiple views.
Specifically speaking, we first explore the local structure of view $v$ by
computing the $k$-nearest neighbors. As a result, the $k$-nearest neighbor set
of each sample $\boldsymbol{x}_i$ in view $v$ is attained. Nevertheless, this
$k$-nearest neighbor set describes only a part of the nearest neighbor
information of sample $\boldsymbol{x}_i$. In order to obtain a more
comprehensive description of sample $\boldsymbol{x}_i$'s nearest neighbors, we
complement the nearest neighbor set in view $v$ by incorporating sample
$\boldsymbol{x}_i$'s nearest neighbors in other views. Lastly, based on the
complemented nearest neighbor set in each view, a graph learning-based
multi-view semi-supervised LDL model is constructed. By considering the
complementarity of local nearest neighbor structures, different views can
mutually provide the local structural information to complement each other. To
the best of our knowledge, this is the first attempt at multi-view LDL.
Numerical studies have demonstrated that MVSS-LDL attains explicitly better
classification performance than the existing single-view LDL methods.

</details>


### [154] [Weight Weaving: Parameter Pooling for Data-Free Model Merging](https://arxiv.org/abs/2510.13921)
*Levy Chaves,Eduardo Valle,Sandra Avila*

Main category: cs.LG

TL;DR: 提出了一种名为 Weight Weaving 的即插即用技术，用于模型融合，无需数据即可有效设置缩放因子。


<details>
  <summary>Details</summary>
Motivation: 现有的模型融合方法严重依赖缩放超参数 λ，但缺乏在没有数据的情况下设置这些参数的有效方法，导致需要使用特权数据进行调整。

Method: Weight Weaving 通过用户定义的池化函数（如平均、随机选择或现有模型融合方法）在 λ 值搜索空间中汇集模型权重。

Result: 在三个 ViT 变体的三个实验设置中验证了 Weight Weaving，包括视觉多任务学习、视觉持续学习和领域泛化，始终提高了多种模型融合方法的性能，在无数据设置中平均准确率提高了 15.9 个百分点。

Conclusion: Weight Weaving 是一种模块化、正交于现有方法且无需评估数据的有效模型融合技术。

Abstract: Model merging provides a cost-effective and data-efficient combination of
specialized deep neural networks through parameter integration. This technique
leverages expert models across downstream tasks without requiring retraining.
Most model merging approaches critically depend on scaling hyper-parameters
$\lambda$, which weight each model's contribution globally or individually.
Principled approaches for setting scaling factors without accessing any data
(data-free) are scarce, often leading researchers to tune $\lambda$ using
privileged data from the evaluation set, which is obviously unfeasible in
practice. To address this limitation, we introduce Weight Weaving, a
plug-and-play technique that pools model weights across $\lambda$ values search
space using user-defined pooling functions, such as averaging, random
selection, or even existing model merging methods. Our method demonstrates high
modularity, imposing minimal constraints on the search space. It operates
orthogonally to existing model merging methods and eliminates evaluation data
requirements. We validate Weight Weaving across three ViT variants in three
experimental setups: vision multi-task learning, vision continual learning, and
domain generalization. Our method consistently improves the performance of
several model merging methods, achieving average accuracy gains of up to 15.9
percentage points in a data-free setting.

</details>


### [155] [LTR-ICD: A Learning-to-Rank Approach for Automatic ICD Coding](https://arxiv.org/abs/2510.13922)
*Mohammad Mansoori,Amira Soliman,Farzaneh Etminani*

Main category: cs.LG

TL;DR: 本文提出了一种新的ICD编码方法，该方法从检索系统的角度考虑了代码的顺序，并将其 формулирует как classification and ranking task。


<details>
  <summary>Details</summary>
Motivation: 临床笔记包含临床医生在就诊期间提供的非结构化文本。正确分配和排序 ICD 代码对于医疗诊断和报销至关重要。然而，自动化这项任务仍然具有挑战性。

Method: 本文从检索系统的角度处理 ICD 编码任务，从而将问题 формулирует как classification and ranking task。

Result: 该模型在正确排序主要诊断代码方面的准确率为 47%，而最先进的分类器的准确率为 20%。此外，在分类指标方面，所提出的模型实现了 0.6065 的 micro-F1 分数和 0.2904 的 macro-F1 分数，超过了之前最好的模型，其分数为 0.597 和 0.2660。

Conclusion: 提出的框架比其他方法具有识别高优先级代码的卓越能力。

Abstract: Clinical notes contain unstructured text provided by clinicians during
patient encounters. These notes are usually accompanied by a sequence of
diagnostic codes following the International Classification of Diseases (ICD).
Correctly assigning and ordering ICD codes are essential for medical diagnosis
and reimbursement. However, automating this task remains challenging.
State-of-the-art methods treated this problem as a classification task, leading
to ignoring the order of ICD codes that is essential for different purposes. In
this work, as a first attempt, we approach this task from a retrieval system
perspective to consider the order of codes, thus formulating this problem as a
classification and ranking task. Our results and analysis show that the
proposed framework has a superior ability to identify high-priority codes
compared to other methods. For instance, our model accuracy in correctly
ranking primary diagnosis codes is 47%, compared to 20% for the
state-of-the-art classifier. Additionally, in terms of classification metrics,
the proposed model achieves a micro- and macro-F1 scores of 0.6065 and 0.2904,
respectively, surpassing the previous best model with scores of 0.597 and
0.2660.

</details>


### [156] [Distributional Consistency Loss: Beyond Pointwise Data Terms in Inverse Problems](https://arxiv.org/abs/2510.13972)
*George Webber,Andrew J. Reader*

Main category: cs.LG

TL;DR: 本文提出了一种新的数据保真度损失函数，称为分布一致性（DC）损失，它通过测试观测测量值在统计上是否与当前估计所暗示的噪声分布一致来集体评估数据保真度，而不是寻求与噪声测量值的逐点一致性。


<details>
  <summary>Details</summary>
Motivation: 传统的数据保真度损失函数（如均方误差（MSE）或负对数似然）通常导致对噪声的过度拟合。为了解决这个问题。

Method: 采用聚合视角，引入分布一致性（DC）损失，这是一种数据保真度目标，它使用基于模型的每个测量概率分数，用分布级别的校准代替逐点匹配。

Result: 在图像去噪和医学图像重建两个关键示例应用领域中，证明了该方法的有效性。使用DC代替MSE损失消除了提前停止的需要，实现了更高的PSNR；在泊松噪声数据的医学图像重建中，DC损失减少了高度迭代重建中的伪影，并增强了手工制作的正则化的效果。

Conclusion: DC损失是一种基于统计的、性能增强的传统保真度损失的替代方案，适用于逆问题。

Abstract: Recovering true signals from noisy measurements is a central challenge in
inverse problems spanning medical imaging, geophysics, and signal processing.
Current solutions balance prior assumptions regarding the true signal
(regularization) with agreement to noisy measured data (data-fidelity).
Conventional data-fidelity loss functions, such as mean-squared error (MSE) or
negative log-likelihood, seek pointwise agreement with noisy measurements,
often leading to overfitting to noise. In this work, we instead evaluate
data-fidelity collectively by testing whether the observed measurements are
statistically consistent with the noise distributions implied by the current
estimate. We adopt this aggregated perspective and introduce distributional
consistency (DC) loss, a data-fidelity objective that replaces pointwise
matching with distribution-level calibration using model-based probability
scores for each measurement. DC loss acts as a direct and practical plug-in
replacement for standard data consistency terms: i) it is compatible with
modern regularizers, ii) it is optimized in the same way as traditional losses,
and iii) it avoids overfitting to measurement noise even without the use of
priors. Its scope naturally fits many practical inverse problems where the
measurement-noise distribution is known and where the measured dataset consists
of many independent noisy values. We demonstrate efficacy in two key example
application areas: i) in image denoising with deep image prior, using DC
instead of MSE loss removes the need for early stopping and achieves higher
PSNR; ii) in medical image reconstruction from Poisson-noisy data, DC loss
reduces artifacts in highly-iterated reconstructions and enhances the efficacy
of hand-crafted regularization. These results position DC loss as a
statistically grounded, performance-enhancing alternative to conventional
fidelity losses for inverse problems.

</details>


### [157] [BitNet Distillation](https://arxiv.org/abs/2510.13998)
*Xun Wu,Shaohan Huang,Wenhui Wang,Ting Song,Li Dong,Yan Xia,Furu Wei*

Main category: cs.LG

TL;DR: BitDistill: Fine-tunes full-precision LLMs into 1.58-bit precision for downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Achieve strong task-specific performance with minimal computational cost.

Method: SubLN module, multi-head attention distillation, and continual pre-training.

Result: Comparable performance to full-precision models with 10x memory savings and 2.65x faster CPU inference.

Conclusion: BitDistill is an effective method for quantizing LLMs to 1.58-bit precision.

Abstract: In this paper, we present BitNet Distillation (BitDistill), a lightweight
pipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into
1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream
tasks, achieving strong task-specific performance with minimal computational
cost. Specifically, BitDistill incorporates three key techniques: the SubLN
module, as introduced in BitNet; multi-head attention distillation, based on
MiniLM; and continual pre-training, which serves as a crucial warm-up step to
mitigate the scalability issue of the performance gap between finetuned
full-precision and 1.58-bit LLMs on specific tasks. Experimental results show
that BitDistill achieves performance comparable to the full-precision
counterpart models across model size, while enabling up to 10x memory savings
and 2.65x faster inference on CPUs. Code is available at
https://github.com/microsoft/BitNet.

</details>


### [158] [REAP the Experts: Why Pruning Prevails for One-Shot MoE compression](https://arxiv.org/abs/2510.13999)
*Mike Lasby,Ivan Lazarevich,Nish Sinnadurai,Sean Lie,Yani Ioannou,Vithursan Thangarasa*

Main category: cs.LG

TL;DR: 提出了一种新的稀疏激活混合专家模型(SMoE)的压缩方法，尤其是在生成任务中，通过修剪专家而非合并专家来实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: SMoE模型参数量大，内存开销显著，因此需要进行专家压缩。

Method: 提出了一种名为Router-weighted Expert Activation Pruning (REAP) 的剪枝标准，该标准同时考虑了路由器的门控值和专家的激活范数。

Result: 在各种SMoE模型（20B到1T参数）上，REAP在生成任务上始终优于合并和其他剪枝方法，尤其是在50%压缩率下。在代码生成和工具调用任务中，即使剪枝50%的专家后，REAP也能实现近乎无损的压缩。

Conclusion: 专家剪枝是生成任务中比专家合并更好的压缩策略。

Abstract: Sparsely-activated Mixture-of-Experts (SMoE) models offer efficient
pre-training and low latency but their large parameter counts create
significant memory overhead, motivating research into expert compression.
Contrary to recent findings favouring expert merging on discriminative
benchmarks, we demonstrate that expert pruning is a superior strategy for
generative tasks. We prove that merging introduces an irreducible error by
causing a "functional subspace collapse", due to the loss of the router's
independent, input-dependent control over experts. Leveraging this insight, we
propose Router-weighted Expert Activation Pruning (REAP), a novel pruning
criterion that considers both router gate-values and expert activation norms.
Across a diverse set of SMoE models ranging from 20B to 1T parameters, REAP
consistently outperforms merging and other pruning methods on generative
benchmarks, especially at 50% compression. Notably, our method achieves
near-lossless compression on code generation and tool-calling tasks with
Qwen3-Coder-480B and Kimi-K2, even after pruning 50% of experts.

</details>


### [159] [Conditional Clifford-Steerable CNNs with Complete Kernel Basis for PDE Modeling](https://arxiv.org/abs/2510.14007)
*Bálint László Szarvas,Maksim Zhdanov*

Main category: cs.LG

TL;DR: CSCNNs are limited in expressivity. This paper proposes Conditional Clifford-Steerable Kernels to address this issue.


<details>
  <summary>Details</summary>
Motivation: CSCNN kernel basis is not complete, thus limiting the model expressivity.

Method: Propose Conditional Clifford-Steerable Kernels, which augment the kernels with equivariant representations computed from the input feature field. Equivariance constraint for these input-dependent kernels is derived and solved efficiently via implicit parameterization.

Result: Improved expressivity on multiple PDE forecasting tasks, including fluid dynamics and relativistic electrodynamics, where the method consistently outperforms baseline methods.

Conclusion: The proposed framework improves expressivity on PDE forecasting tasks.

Abstract: Clifford-Steerable CNNs (CSCNNs) provide a unified framework that allows
incorporating equivariance to arbitrary pseudo-Euclidean groups, including
isometries of Euclidean space and Minkowski spacetime. In this work, we
demonstrate that the kernel basis of CSCNNs is not complete, thus limiting the
model expressivity. To address this issue, we propose Conditional
Clifford-Steerable Kernels, which augment the kernels with equivariant
representations computed from the input feature field. We derive the
equivariance constraint for these input-dependent kernels and show how it can
be solved efficiently via implicit parameterization. We empirically demonstrate
an improved expressivity of the resulting framework on multiple PDE forecasting
tasks, including fluid dynamics and relativistic electrodynamics, where our
method consistently outperforms baseline methods.

</details>


### [160] [Noise-Adaptive Layerwise Learning Rates: Accelerating Geometry-Aware Optimization for Deep Neural Network Training](https://arxiv.org/abs/2510.14009)
*Jie Hao,Xiaochuan Gong,Jie Xu,Zhengdao Wang,Mingrui Liu*

Main category: cs.LG

TL;DR: 提出了一种噪声自适应的层级学习率方案，以加速DNN训练。


<details>
  <summary>Details</summary>
Motivation: 现有的几何感知优化器对同一组内的层施加固定的学习率，这对于DNN训练来说可能效率低下，因为即使在与相同范数相关联的一组层中，局部曲率也可能在层之间存在异质性，并且在训练过程中动态变化。

Method: 在几何感知优化算法的基础上，引入了一种噪声自适应的层级学习率方案。该方法估计由所选LMO引起的对偶范数中的梯度方差，并使用它来分配每个组内随时间变化的噪声自适应层级学习率。

Result: 在Transformer架构（如LLaMA和GPT）上的实验结果表明，该方法比最先进的优化器收敛速度更快。

Conclusion: 该算法实现了快速的收敛速度，并且在Transformer架构上表现出优于现有优化器的性能。

Abstract: Geometry-aware optimization algorithms, such as Muon, have achieved
remarkable success in training deep neural networks (DNNs). These methods
leverage the underlying geometry of DNNs by selecting appropriate norms for
different layers and updating parameters via norm-constrained linear
minimization oracles (LMOs). However, even within a group of layers associated
with the same norm, the local curvature can be heterogeneous across layers and
vary dynamically over the course of training. For example, recent work shows
that sharpness varies substantially across transformer layers and throughout
training, yet standard geometry-aware optimizers impose fixed learning rates to
layers within the same group, which may be inefficient for DNN training.
  In this paper, we introduce a noise-adaptive layerwise learning rate scheme
on top of geometry-aware optimization algorithms and substantially accelerate
DNN training compared to methods that use fixed learning rates within each
group. Our method estimates gradient variance in the dual norm induced by the
chosen LMO on the fly, and uses it to assign time-varying noise-adaptive
layerwise learning rates within each group. We provide a theoretical analysis
showing that our algorithm achieves a sharp convergence rate. Empirical results
on transformer architectures such as LLaMA and GPT demonstrate that our
approach achieves faster convergence than state-of-the-art optimizers.

</details>


### [161] [Context-Selective State Space Models: Feedback is All You Need](https://arxiv.org/abs/2510.14027)
*Riccardo Zattra,Giacomo Baggio,Umberto Casti,Augusto Ferrante,Francesco Ticozzi*

Main category: cs.LG

TL;DR: COFFEE模型是一种新型时变状态空间模型，它结合了状态反馈以实现上下文相关的选择性，同时仍然允许并行实现。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在处理长序列时面临二次复杂度和困难，而状态空间模型(SSM)提供了一种有效的替代方案。Mamba架构中的S6模块在长序列基准测试中取得了最先进的结果。本文旨在通过引入COFFEE模型来改进S6模块。

Method: COFFEE模型通过状态反馈从内部状态计算选择性，该内部状态充当序列历史的紧凑表示。此外，采用了一种有效的模型参数化方法，消除了S6中存在的冗余。

Result: 在归纳头任务中，COFFEE模型以比S6少两个数量级的参数和训练序列实现了接近完美的精度。在MNIST上，COFFEE在同一架构中大大优于S6，仅用3585个参数就达到了97%的精度。

Conclusion: 状态反馈是构建可扩展和高效序列模型的关键机制。

Abstract: Transformers, powered by the attention mechanism, are the backbone of most
foundation models, yet they suffer from quadratic complexity and difficulties
in dealing with long-range dependencies in the input sequence. Recent work has
shown that state space models (SSMs) provide an efficient alternative, with the
S6 module at the core of the Mamba architecture achieving state-of-the-art
results on long-sequence benchmarks. In this paper, we introduce the COFFEE
(COntext From FEEdback) model, a novel time-varying SSM that incorporates state
feedback to enable context-dependent selectivity, while still allowing for
parallel implementation. Whereas the selectivity mechanism of S6 only depends
on the current input, COFFEE computes it from the internal state, which serves
as a compact representation of the sequence history. This shift allows the
model to regulate its dynamics based on accumulated context, improving its
ability to capture long-range dependencies. In addition to state feedback, we
employ an efficient model parametrization that removes redundancies present in
S6 and leads to a more compact and trainable formulation. On the induction head
task, COFFEE achieves near-perfect accuracy with two orders of magnitude fewer
parameters and training sequences compared to S6. On MNIST, COFFEE largely
outperforms S6 within the same architecture, reaching 97% accuracy with only
3585 parameters. These results showcase the role of state feedback as a key
mechanism for building scalable and efficient sequence models.

</details>


### [162] [Agentic Entropy-Balanced Policy Optimization](https://arxiv.org/abs/2510.14545)
*Guanting Dong,Licheng Bao,Zhongyuan Wang,Kangzhi Zhao,Xiaoxi Li,Jiajie Jin,Jinghan Yang,Hangyu Mao,Fuzheng Zhang,Kun Gai,Guorui Zhou,Yutao Zhu,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.LG

TL;DR: 提出了Agentic Entropy-Balanced Policy Optimization (AEPO) 算法，用于平衡agentic RL中的熵，避免训练崩溃。


<details>
  <summary>Details</summary>
Motivation: 主流agentic RL算法过度依赖熵信号可能导致训练崩溃。

Method: AEPO包含动态熵平衡rollout机制和熵平衡策略优化，分别在rollout和策略更新阶段平衡熵。

Result: 在14个数据集上，AEPO优于7种主流RL算法。在少量样本下，Qwen3-14B与AEPO结合在GAIA、Humanity's Last Exam和WebWalker上取得了显著成果。

Conclusion: AEPO提高了rollout采样多样性，同时保持了稳定的策略熵，促进了可扩展的web agent训练。

Abstract: Recently, Agentic Reinforcement Learning (Agentic RL) has made significant
progress in incentivizing the multi-turn, long-horizon tool-use capabilities of
web agents. While mainstream agentic RL algorithms autonomously explore
high-uncertainty tool-call steps under the guidance of entropy, excessive
reliance on entropy signals can impose further constraints, leading to the
training collapse. In this paper, we delve into the challenges caused by
entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an
agentic RL algorithm designed to balance entropy in both the rollout and policy
update phases. AEPO comprises two core components: (1) a dynamic
entropy-balanced rollout mechanism that adaptively allocate global and branch
sampling budget through entropy pre-monitoring, while imposing a branch penalty
on consecutive high-entropy tool-call steps to prevent over-branching issues;
and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient
operation into the high-entropy clipping term to preserve and properly rescale
gradients on high-entropy tokens, while incorporating entropy-aware advantage
estimation to prioritize learning on high-uncertainty tokens. Results across 14
challenging datasets show that AEPO consistently outperforms 7 mainstream RL
algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive
results: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker
for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on
WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout
sampling diversity while maintaining stable policy entropy, facilitating
scalable web agent training.

</details>


### [163] [CausalVerse: Benchmarking Causal Representation Learning with Configurable High-Fidelity Simulations](https://arxiv.org/abs/2510.14049)
*Guangyi Chen,Yunlong Deng,Peiyuan Zhu,Yan Li,Yifan Sheng,Zijian Li,Kun Zhang*

Main category: cs.LG

TL;DR: 本文介绍了一个新的因果表征学习 (CRL) 基准，该基准使用高保真模拟视觉数据，保留了真实的视觉复杂性，更重要的是，可以访问真实的因果生成过程。


<details>
  <summary>Details</summary>
Motivation: 现有的评估通常依赖于简单的合成数据集或现实世界任务的下游性能，通常在现实主义和评估精度之间存在两难。

Method: 使用高保真模拟视觉数据，该数据集包含四大领域（静态图像生成、动态物理模拟、机器人操作和交通情况分析）中的约 20 万张图像和 300 万个视频帧。

Result: 在各种范例中评估了具有代表性的 CRL 方法，并提供了经验见解，以帮助从业者和新手选择或扩展适当的 CRL 框架，以正确解决可以从 CRL 角度受益的特定类型的实际问题。

Conclusion: 该基准旨在弥合严格评估和现实世界适用性之间的差距，并允许灵活访问底层因果结构。

Abstract: Causal Representation Learning (CRL) aims to uncover the data-generating
process and identify the underlying causal variables and relations, whose
evaluation remains inherently challenging due to the requirement of known
ground-truth causal variables and causal structure. Existing evaluations often
rely on either simplistic synthetic datasets or downstream performance on
real-world tasks, generally suffering a dilemma between realism and evaluative
precision. In this paper, we introduce a new benchmark for CRL using
high-fidelity simulated visual data that retains both realistic visual
complexity and, more importantly, access to ground-truth causal generating
processes. The dataset comprises around 200 thousand images and 3 million video
frames across 24 sub-scenes in four domains: static image generation, dynamic
physical simulations, robotic manipulations, and traffic situation analysis.
These scenarios range from static to dynamic settings, simple to complex
structures, and single to multi-agent interactions, offering a comprehensive
testbed that hopefully bridges the gap between rigorous evaluation and
real-world applicability. In addition, we provide flexible access to the
underlying causal structures, allowing users to modify or configure them to
align with the required assumptions in CRL, such as available domain labels,
temporal dependencies, or intervention histories. Leveraging this benchmark, we
evaluated representative CRL methods across diverse paradigms and offered
empirical insights to assist practitioners and newcomers in choosing or
extending appropriate CRL frameworks to properly address specific types of real
problems that can benefit from the CRL perspective. Welcome to visit our:
Project page:https://causal-verse.github.io/,
Dataset:https://huggingface.co/CausalVerse.

</details>


### [164] [Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge Graphs with Hybrid Retrieval](https://arxiv.org/abs/2510.14592)
*Rashmi R,Vidyadhar Upadhya*

Main category: cs.LG

TL;DR: 提出了一个针对多模态文档的检索增强生成(RAG)系统，通过结合密集向量检索和结构化图遍历来实现跨模态的语义理解和上下文感知检索。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG系统主要处理单模态文本数据，无法有效处理包含文本、图像、表格等多种信息源的非结构化多模态文档。

Method: 设计了一个模态感知混合检索架构(MAHA)，该架构集成了密集向量检索和结构化图遍历，利用知识图谱编码跨模态语义和关系。

Result: 在多个基准数据集上的评估表明，MAHA显著优于基线方法，实现了0.486的ROUGE-L评分，并提供了完整的模态覆盖。

Conclusion: MAHA能够将嵌入与显式文档结构相结合，实现有效的多模态检索，并为RAG系统提供了一个可扩展且可解释的框架，通过实现对非结构化多模态数据的模态感知推理来推进RAG系统。

Abstract: Current Retrieval-Augmented Generation (RAG) systems primarily operate on
unimodal textual data, limiting their effectiveness on unstructured multimodal
documents. Such documents often combine text, images, tables, equations, and
graphs, each contributing unique information. In this work, we present a
Modality-Aware Hybrid retrieval Architecture (MAHA), designed specifically for
multimodal question answering with reasoning through a modality-aware knowledge
graph. MAHA integrates dense vector retrieval with structured graph traversal,
where the knowledge graph encodes cross-modal semantics and relationships. This
design enables both semantically rich and context-aware retrieval across
diverse modalities. Evaluations on multiple benchmark datasets demonstrate that
MAHA substantially outperforms baseline methods, achieving a ROUGE-L score of
0.486, providing complete modality coverage. These results highlight MAHA's
ability to combine embeddings with explicit document structure, enabling
effective multimodal retrieval. Our work establishes a scalable and
interpretable retrieval framework that advances RAG systems by enabling
modality-aware reasoning over unstructured multimodal data.

</details>


### [165] [FedHFT: Efficient Federated Finetuning with Heterogeneous Edge Clients](https://arxiv.org/abs/2510.14054)
*Fatih Ilhan,Selim Furkan Tekin,Tiansheng Huang,Gaowen Liu,Ramana Kompella,Greg Eisenhauer,Yingyan Celine Lin,Calton Pu,Ling Liu*

Main category: cs.LG

TL;DR: 提出了一种高效的个性化联邦微调框架FedHFT，以应对在下游任务和特定领域数据集上进行个性化自然语言理解(NLU)应用时，微调预训练的大型语言模型(llm)所面临的两个主要挑战: (i)由于专有数据的机密性或隐私要求，用于微调的数据有限和/或异构，(ii)跨参与客户端(如边缘设备)可用的计算资源各不相同。


<details>
  <summary>Details</summary>
Motivation: 解决由于专有数据的机密性或隐私要求导致的数据有限和/或异构，以及跨参与客户端(如边缘设备)可用的计算资源各不相同的问题。

Method: 引入了一种混合掩码适配器来处理跨参与客户端的资源异构性，从而支持在分布式环境中跨多个客户端对预训练语言模型进行高性能协同微调，同时保持专有数据的本地性。其次，引入了一种基于掩码个性化和客户端聚类的双层优化方法来处理非iid数据分布。

Result: 在数据和资源异构性下，与具有代表性的异构联邦学习方法相比，在各种自然语言理解任务上表现出显著的性能和效率改进。

Conclusion: FedHFT 是一种高效的个性化联邦微调框架，可以在数据和资源异构性下显著提高各种自然语言理解任务的性能和效率。

Abstract: Fine-tuning pre-trained large language models (LLMs) has become a common
practice for personalized natural language understanding (NLU) applications on
downstream tasks and domain-specific datasets. However, there are two main
challenges: (i) limited and/or heterogeneous data for fine-tuning due to
proprietary data confidentiality or privacy requirements, and (ii) varying
computation resources available across participating clients such as edge
devices. This paper presents FedHFT - an efficient and personalized federated
fine-tuning framework to address both challenges. First, we introduce a mixture
of masked adapters to handle resource heterogeneity across participating
clients, enabling high-performance collaborative fine-tuning of pre-trained
language model(s) across multiple clients in a distributed setting, while
keeping proprietary data local. Second, we introduce a bi-level optimization
approach to handle non-iid data distribution based on masked personalization
and client clustering. Extensive experiments demonstrate significant
performance and efficiency improvements over various natural language
understanding tasks under data and resource heterogeneity compared to
representative heterogeneous federated learning methods.

</details>


### [166] [On the expressivity of sparse maxout networks](https://arxiv.org/abs/2510.14068)
*Moritz Grillo,Tobias Hofmann*

Main category: cs.LG

TL;DR: 研究稀疏 maxout 网络的表达能力，其中每个神经元从前一层获取固定数量的输入并采用 maxout 激活。


<details>
  <summary>Details</summary>
Motivation: 探索卷积或图神经网络的关键特征。

Method: 建立网络可计算函数与一类虚拟多胞形之间的对偶性，将其几何形状与网络表达性问题联系起来。推导出相关多胞形维数的严格界限，作为分析的核心工具。

Result: 足够深的稀疏 maxout 网络是通用的。如果未达到所需的深度，则宽度本身无法弥补固定入度约束的稀疏性。

Conclusion: 稀疏 maxout 网络的深度层级结构具有重要意义，深度不足时，宽度无法弥补稀疏性。

Abstract: We study the expressivity of sparse maxout networks, where each neuron takes
a fixed number of inputs from the previous layer and employs a, possibly
multi-argument, maxout activation. This setting captures key characteristics of
convolutional or graph neural networks. We establish a duality between
functions computable by such networks and a class of virtual polytopes, linking
their geometry to questions of network expressivity. In particular, we derive a
tight bound on the dimension of the associated polytopes, which serves as the
central tool for our analysis. Building on this, we construct a sequence of
depth hierarchies. While sufficiently deep sparse maxout networks are
universal, we prove that if the required depth is not reached, width alone
cannot compensate for the sparsity of a fixed indegree constraint.

</details>


### [167] [Exploratory Causal Inference in SAEnce](https://arxiv.org/abs/2510.14073)
*Tommaso Mencattini,Riccardo Cadei,Francesco Locatello*

Main category: cs.LG

TL;DR: 本研究提出了一种新的方法，利用预训练的基模型和稀疏自动编码器，直接从数据中发现treatment的未知效果，从而克服了传统随机对照试验的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统随机对照试验依赖于手工设计的假设和昂贵的分析，限制了大规模的因果效应估计，并可能锚定在流行的但不完整的假设上。

Method: 该研究使用预训练的基模型将试验中的非结构化数据转化为有意义的表示，并通过稀疏自动编码器进行解释。同时，引入了一种新的递归程序Neural Effect Search，通过逐步分层解决多重检验和效应纠缠问题。

Result: 在半合成实验中评估了算法的稳健性，并在实验生态学中展示了首次在真实科学试验中成功进行的无监督因果效应识别。

Conclusion: 该研究成功地在真实世界的科学试验中实现了无监督的因果效应识别，为大规模因果效应估计提供了新的可能性。

Abstract: Randomized Controlled Trials are one of the pillars of science; nevertheless,
they rely on hand-crafted hypotheses and expensive analysis. Such constraints
prevent causal effect estimation at scale, potentially anchoring on popular yet
incomplete hypotheses. We propose to discover the unknown effects of a
treatment directly from data. For this, we turn unstructured data from a trial
into meaningful representations via pretrained foundation models and interpret
them via a sparse autoencoder. However, discovering significant causal effects
at the neural level is not trivial due to multiple-testing issues and effects
entanglement. To address these challenges, we introduce Neural Effect Search, a
novel recursive procedure solving both issues by progressive stratification.
After assessing the robustness of our algorithm on semi-synthetic experiments,
we showcase, in the context of experimental ecology, the first successful
unsupervised causal effect identification on a real-world scientific trial.

</details>


### [168] [Neural Network approximation power on homogeneous and heterogeneous reaction-diffusion equations](https://arxiv.org/abs/2510.14094)
*Haotian Feng*

Main category: cs.LG

TL;DR: 本文研究了神经网络近似反应扩散方程解的能力。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络有效近似微分方程解的理论基础不足。

Method: 利用通用逼近定理，分析了两层和三层神经网络对一维和二维反应扩散方程的逼近能力。

Result: 证明了两层神经网络可以逼近一维反应扩散方程，三层神经网络可以逼近二维反应扩散方程。

Conclusion: 强调了神经网络在逼近反应扩散方程和相关偏微分方程解方面的表达能力，为基于神经网络的微分方程求解器提供了理论基础。

Abstract: Reaction-diffusion systems represent one of the most fundamental formulations
used to describe a wide range of physical, chemical, and biological processes.
With the increasing adoption of neural networks, recent research has focused on
solving differential equations using machine learning techniques. However, the
theoretical foundation explaining why neural networks can effectively
approximate such solutions remains insufficiently explored.
  This paper provides a theoretical analysis of the approximation power of
neural networks for one- and two-dimensional reaction-diffusion equations in
both homogeneous and heterogeneous media. Building upon the universal
approximation theorem, we demonstrate that a two-layer neural network can
approximate the one-dimensional reaction-diffusion equation, while a
three-layer neural network can approximate its two-dimensional counterpart. The
theoretical framework presented here can be further extended to elliptic and
parabolic equations.
  Overall, this work highlights the expressive power of neural networks in
approximating solutions to reaction-diffusion equations and related PDEs,
providing a theoretical foundation for neural network-based differential
equation solvers.

</details>


### [169] [Unlocking Out-of-Distribution Generalization in Transformers via Recursive Latent Space Reasoning](https://arxiv.org/abs/2510.14095)
*Awni Altabaa,Siyu Chen,John Lafferty,Zhuoran Yang*

Main category: cs.LG

TL;DR: 本文研究了Transformer网络中的OOD泛化问题，使用GSM8K风格的模块化算术作为测试平台。


<details>
  <summary>Details</summary>
Motivation: 在机器学习中，超越训练分布的系统性、组合泛化仍然是一个核心挑战，也是现代语言模型新兴推理能力的关键瓶颈。

Method: 本文介绍并探索了一组旨在增强OOD泛化的四种架构机制：(i)输入自适应递归；(ii)算法监督；(iii)通过离散瓶颈锚定的潜在表示；(iv)显式纠错机制。

Result: 这些机制共同构成了一种架构方法，用于Transformer网络中具有鲁棒算法泛化能力的本机和可扩展的潜在空间推理。

Conclusion: 本文通过详细的机械可解释性分析补充了这些经验结果，揭示了这些机制如何产生强大的OOD泛化能力。

Abstract: Systematic, compositional generalization beyond the training distribution
remains a core challenge in machine learning -- and a critical bottleneck for
the emergent reasoning abilities of modern language models. This work
investigates out-of-distribution (OOD) generalization in Transformer networks
using a GSM8K-style modular arithmetic on computational graphs task as a
testbed. We introduce and explore a set of four architectural mechanisms aimed
at enhancing OOD generalization: (i) input-adaptive recurrence; (ii)
algorithmic supervision; (iii) anchored latent representations via a discrete
bottleneck; and (iv) an explicit error-correction mechanism. Collectively,
these mechanisms yield an architectural approach for native and scalable latent
space reasoning in Transformer networks with robust algorithmic generalization
capabilities. We complement these empirical results with a detailed mechanistic
interpretability analysis that reveals how these mechanisms give rise to robust
OOD generalization abilities.

</details>


### [170] [TENDE: Transfer Entropy Neural Diffusion Estimation](https://arxiv.org/abs/2510.14096)
*Simon Pedro Galeano Munoz,Mustapha Bounoua,Giulio Franzese,Pietro Michiardi,Maurizio Filippone*

Main category: cs.LG

TL;DR: 提出了一种新的方法TENDE（转移熵神经扩散估计），利用基于分数的扩散模型通过条件互信息来估计转移熵。


<details>
  <summary>Details</summary>
Motivation: 现有的估计方法受到维度诅咒的影响，需要限制性的分布假设，或者需要指数大的数据集才能实现可靠的收敛。

Method: 通过学习相关条件分布的分数函数，TENDE提供灵活、可扩展的估计，同时对底层数据生成过程做出最小的假设。

Result: 与现有的神经估计器和其他最先进的方法相比，在合成基准和真实数据上表现出卓越的准确性和鲁棒性。

Conclusion: TENDE是一种利用基于分数的扩散模型估计转移熵的新方法，它克服了现有方法的局限性，并在各种数据集上表现出优越的性能。

Abstract: Transfer entropy measures directed information flow in time series, and it
has become a fundamental quantity in applications spanning neuroscience,
finance, and complex systems analysis. However, existing estimation methods
suffer from the curse of dimensionality, require restrictive distributional
assumptions, or need exponentially large datasets for reliable convergence. We
address these limitations in the literature by proposing TENDE (Transfer
Entropy Neural Diffusion Estimation), a novel approach that leverages
score-based diffusion models to estimate transfer entropy through conditional
mutual information. By learning score functions of the relevant conditional
distributions, TENDE provides flexible, scalable estimation while making
minimal assumptions about the underlying data-generating process. We
demonstrate superior accuracy and robustness compared to existing neural
estimators and other state-of-the-art approaches across synthetic benchmarks
and real data.

</details>


### [171] [Near-Optimal Regret-Queue Length Tradeoff in Online Learning for Two-Sided Markets](https://arxiv.org/abs/2510.14097)
*Zixian Yang,Sushil Mahavir Varma,Lei Ying*

Main category: cs.LG

TL;DR: 研究一个双边市场，目标是设计定价和匹配算法以最大化平台利润，同时保持合理的队列长度。


<details>
  <summary>Details</summary>
Motivation: 实际中，控制价格依赖到达率的需求和供应曲线可能是未知的。

Method: 设计了一种新的基于在线学习的定价策略，并确定了其近似最优性。

Result: 证明了三个性能指标之间的权衡：$\\\tilde{O}(T^{1-\\gamma})$ 后悔值，$\\\tilde{O}(T^{\\gamma/2})$ 平均队列长度，以及 $\\\tilde{O}(T^{\\gamma})$ 最大队列长度，对于 $\\gamma \\in (0, 1/6]$，显著优于现有结果。

Conclusion: 所提出的策略有两个值得注意的特征：一个动态组件，优化了低后悔值和小队列长度之间的权衡；以及一个概率组件，解决了获得用于快速学习的有用样本和保持小队列长度之间的矛盾。

Abstract: We study a two-sided market, wherein, price-sensitive heterogeneous customers
and servers arrive and join their respective queues. A compatible
customer-server pair can then be matched by the platform, at which point, they
leave the system. Our objective is to design pricing and matching algorithms
that maximize the platform's profit, while maintaining reasonable queue
lengths. As the demand and supply curves governing the price-dependent arrival
rates may not be known in practice, we design a novel online-learning-based
pricing policy and establish its near-optimality. In particular, we prove a
tradeoff among three performance metrics: $\tilde{O}(T^{1-\gamma})$ regret,
$\tilde{O}(T^{\gamma/2})$ average queue length, and $\tilde{O}(T^{\gamma})$
maximum queue length for $\gamma \in (0, 1/6]$, significantly improving over
existing results [1]. Moreover, barring the permissible range of $\gamma$, we
show that this trade-off between regret and average queue length is optimal up
to logarithmic factors under a class of policies, matching the optimal one as
in [2] which assumes the demand and supply curves to be known. Our proposed
policy has two noteworthy features: a dynamic component that optimizes the
tradeoff between low regret and small queue lengths; and a probabilistic
component that resolves the tension between obtaining useful samples for fast
learning and maintaining small queue lengths.

</details>


### [172] [Briding Diffusion Posterior Sampling and Monte Carlo methods: a survey](https://arxiv.org/abs/2510.14114)
*Yazid Janati,Alain Durmus,Jimmy Olsson,Eric Moulines*

Main category: cs.LG

TL;DR: 这篇综述介绍了如何利用预训练的扩散模型和蒙特卡洛方法来解决贝叶斯逆问题，而不需要额外的训练。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成建模中表现出巨大的潜力，并且最近已被证明可以通过充当先验来解决贝叶斯逆问题。

Method: 本文重点介绍了利用扩散过程中的“扭曲”机制来引导模拟走向后验分布的方法，并结合不同的蒙特卡洛方法从这些扭曲的分布中进行采样。

Result: 综述概述了当前利用预训练扩散模型解决贝叶斯逆问题的方法。

Conclusion: 扩散模型可以通过扭曲机制和蒙特卡洛方法有效地解决贝叶斯逆问题，而无需额外训练。

Abstract: Diffusion models enable the synthesis of highly accurate samples from complex
distributions and have become foundational in generative modeling. Recently,
they have demonstrated significant potential for solving Bayesian inverse
problems by serving as priors. This review offers a comprehensive overview of
current methods that leverage \emph{pre-trained} diffusion models alongside
Monte Carlo methods to address Bayesian inverse problems without requiring
additional training. We show that these methods primarily employ a
\emph{twisting} mechanism for the intermediate distributions within the
diffusion process, guiding the simulations toward the posterior distribution.
We describe how various Monte Carlo methods are then used to aid in sampling
from these twisted distributions.

</details>


### [173] [Neural Network-enabled Domain-consistent Robust Optimisation for Global CO$_2$ Reduction Potential of Gas Power Plants](https://arxiv.org/abs/2510.14125)
*Waqar Muhammad Ashraf,Talha Ansar,Abdulelah S. Alshehri,Peipei Chen,Ramit Debnath,Vivek Dua*

Main category: cs.LG

TL;DR: 提出了一个由神经网络驱动的鲁棒优化框架，该框架将数据驱动的域作为约束集成到非线性规划技术中。


<details>
  <summary>Details</summary>
Motivation: 解决了参数化神经网络模型与优化求解器相互作用产生的域不一致解问题。

Method: 将该框架应用于一个1180兆瓦容量的联合循环燃气发电厂。

Result: 获得了域一致的鲁棒最优解，实现了0.76个百分点的平均能源效率提升。

Conclusion: 机器学习在为全球气候行动提供近期、可扩展的脱碳路径方面具有协同作用。

Abstract: We introduce a neural network-driven robust optimisation framework that
integrates data-driven domain as a constraint into the nonlinear programming
technique, addressing the overlooked issue of domain-inconsistent solutions
arising from the interaction of parametrised neural network models with
optimisation solvers. Applied to a 1180 MW capacity combined cycle gas power
plant, our framework delivers domain-consistent robust optimal solutions that
achieve a verified 0.76 percentage point mean improvement in energy efficiency.
For the first time, scaling this efficiency gain to the global fleet of gas
power plants, we estimate an annual 26 Mt reduction potential in CO$_2$ (with
10.6 Mt in Asia, 9.0 Mt in the Americas, and 4.5 Mt in Europe). These results
underscore the synergetic role of machine learning in delivering near-term,
scalable decarbonisation pathways for global climate action.

</details>


### [174] [Demystifying the Mechanisms Behind Emergent Exploration in Goal-conditioned RL](https://arxiv.org/abs/2510.14129)
*Mahsa Bastankhah,Grace Liu,Dilip Arumugam,Thomas L. Griffiths,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 本文研究了无监督强化学习中涌现探索背后的机制，着重分析了 Single-Goal Contrastive Reinforcement Learning (SGCRL) 算法。


<details>
  <summary>Details</summary>
Motivation: 旨在理解无监督强化学习中涌现探索的机制，特别是 SGCRL 算法如何在没有外部奖励或课程的情况下解决具有挑战性的长程目标到达任务。

Method: 通过结合算法目标函数的理论分析和对照实验，研究了驱动 SGCRL 探索的因素。

Result: 研究表明，SGCRL 最大化了由其学习的表征所塑造的隐式奖励。这些表征自动修改奖励环境，以促进达到目标前的探索和之后的利用。实验还表明，这些探索动态源于学习状态空间的低秩表征，而不是来自神经网络函数逼近。

Conclusion: 通过对 SGCRL 的深入理解，使其能够执行安全意识探索。

Abstract: In this work, we take a first step toward elucidating the mechanisms behind
emergent exploration in unsupervised reinforcement learning. We study
Single-Goal Contrastive Reinforcement Learning (SGCRL), a self-supervised
algorithm capable of solving challenging long-horizon goal-reaching tasks
without external rewards or curricula. We combine theoretical analysis of the
algorithm's objective function with controlled experiments to understand what
drives its exploration. We show that SGCRL maximizes implicit rewards shaped by
its learned representations. These representations automatically modify the
reward landscape to promote exploration before reaching the goal and
exploitation thereafter. Our experiments also demonstrate that these
exploration dynamics arise from learning low-rank representations of the state
space rather than from neural network function approximation. Our improved
understanding enables us to adapt SGCRL to perform safety-aware exploration.

</details>


### [175] [Learning Wireless Interference Patterns: Decoupled GNN for Throughput Prediction in Heterogeneous Multi-Hop p-CSMA Networks](https://arxiv.org/abs/2510.14137)
*Faezeh Dehghan Tarzjani,Bhaskar Krishnamachari*

Main category: cs.LG

TL;DR: 提出了一种新的图卷积网络（D-GCN），用于预测异构多跳无线网络中的饱和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的简化模型低估吞吐量，精确的马尔可夫链分析计算量大，标准GCN在异构网络上误差较高。

Method: D-GCN显式地分离了节点自身传输概率的处理和邻居干扰效应，用可学习的注意力机制代替平均聚合。

Result: D-GCN的NMAE为3.3%，优于其他方法，并且可以实现基于梯度的网络优化。

Conclusion: D-GCN在预测吞吐量方面表现出色，即使在精确分析方法计算不可行的情况下也能保持有效，并能实现接近理论最优的网络优化。

Abstract: The p-persistent CSMA protocol is central to random-access MAC analysis, but
predicting saturation throughput in heterogeneous multi-hop wireless networks
remains a hard problem. Simplified models that assume a single, shared
interference domain can underestimate throughput by 48--62\% in sparse
topologies. Exact Markov-chain analyses are accurate but scale exponentially in
computation time, making them impractical for large networks. These
computational barriers motivate structural machine learning approaches like
GNNs for scalable throughput prediction in general network topologies. Yet
off-the-shelf GNNs struggle here: a standard GCN yields 63.94\% normalized mean
absolute error (NMAE) on heterogeneous networks because symmetric normalization
conflates a node's direct interference with higher-order, cascading effects
that pertain to how interference propagates over the network graph.
  Building on these insights, we propose the Decoupled Graph Convolutional
Network (D-GCN), a novel architecture that explicitly separates processing of a
node's own transmission probability from neighbor interference effects. D-GCN
replaces mean aggregation with learnable attention, yielding interpretable,
per-neighbor contribution weights while capturing complex multihop interference
patterns. D-GCN attains 3.3\% NMAE, outperforms strong baselines, remains
tractable even when exact analytical methods become computationally infeasible,
and enables gradient-based network optimization that achieves within 1\% of
theoretical optima.

</details>


### [176] [Inferred global dense residue transition graphs from primary structure sequences enable protein interaction prediction via directed graph convolutional neural networks](https://arxiv.org/abs/2510.14139)
*Islam Akef Ebeid,Haoteng Tang,Pengfei Gu*

Main category: cs.LG

TL;DR: 提出了一种新的蛋白质-蛋白质相互作用 (PPI) 预测框架 ProtGram-DirectGCN，该框架计算效率高，即使在有限的训练数据下也能提供强大的预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有的PPI预测方法要么计算量大，要么依赖于3D蛋白质结构，因此需要计算效率更高的替代方案。

Method: 该框架包含两个阶段：ProtGram 构建蛋白质n-gram图，并使用 DirectGCN 学习残基级别的嵌入，然后通过注意力机制池化生成蛋白质级别的嵌入用于预测。

Result: DirectGCN 在节点分类基准测试中表现良好，ProtGram-DirectGCN 框架在 PPI 预测中表现出强大的预测能力，即使在有限的训练数据下也是如此。

Conclusion: ProtGram-DirectGCN 提供了一种有效的 PPI 预测方法，并且计算成本较低

Abstract: Introduction Accurate prediction of protein-protein interactions (PPIs) is
crucial for understanding cellular functions and advancing drug development.
Existing in-silico methods use direct sequence embeddings from Protein Language
Models (PLMs). Others use Graph Neural Networks (GNNs) for 3D protein
structures. This study explores less computationally intensive alternatives. We
introduce a novel framework for downstream PPI prediction through link
prediction. Methods We introduce a two-stage graph representation learning
framework, ProtGram-DirectGCN. First, we developed ProtGram. This approach
models a protein's primary structure as a hierarchy of globally inferred n-gram
graphs. In these graphs, residue transition probabilities define edge weights.
Each edge connects a pair of residues in a directed graph. The probabilities
are aggregated from a large corpus of sequences. Second, we propose DirectGCN,
a custom directed graph convolutional neural network. This model features a
unique convolutional layer. It processes information through separate
path-specific transformations: incoming, outgoing, and undirected. A shared
transformation is also applied. These paths are combined via a learnable gating
mechanism. We apply DirectGCN to ProtGram graphs to learn residue-level
embeddings. These embeddings are pooled via attention to generate protein-level
embeddings for prediction. Results We first established the efficacy of
DirectGCN on standard node classification benchmarks. Its performance matches
established methods on general datasets. The model excels at complex, directed
graphs with dense, heterophilic structures. When applied to PPI prediction, the
full ProtGram-DirectGCN framework delivers robust predictive power. This strong
performance holds even with limited training data.

</details>


### [177] [On Evaluating Loss Functions for Stock Ranking: An Empirical Analysis With Transformer Model](https://arxiv.org/abs/2510.14156)
*Jan Kwiatkowski,Jarosław A. Chudziak*

Main category: cs.LG

TL;DR: 本文研究了不同的训练损失函数对Transformer模型在股票排序任务中的影响，旨在优化基于排序的交易策略。


<details>
  <summary>Details</summary>
Motivation: 量化交易策略依赖于准确的股票排序以识别有利可图的投资。Transformer模型在理解金融时间序列方面很有前景，但不同的训练损失函数如何影响其股票排序能力尚不清楚。金融市场具有挑战性，因为其不断变化的性质以及股票之间复杂的关系。标准的损失函数通常不足以直接学习股票收益的正确顺序。

Method: 本文系统地评估了一组不同的高级损失函数，包括点式、配对式、列表式，用于每日股票收益预测，以促进基于标普500数据的基于排序的投资组合选择。

Result: 本文全面评估了不同的损失函数如何影响模型学习横截面和时间模式的能力，这对于投资组合选择至关重要。

Conclusion: 本文的研究提供了一个全面的基准，揭示了不同的损失函数如何影响模型学习横截面和时间模式的能力，从而为优化基于排序的交易策略提供实用指导。

Abstract: Quantitative trading strategies rely on accurately ranking stocks to identify
profitable investments. Effective portfolio management requires models that can
reliably order future stock returns. Transformer models are promising for
understanding financial time series, but how different training loss functions
affect their ability to rank stocks well is not yet fully understood. Financial
markets are challenging due to their changing nature and complex relationships
between stocks. Standard loss functions, which aim for simple prediction
accuracy, often aren't enough. They don't directly teach models to learn the
correct order of stock returns. While many advanced ranking losses exist from
fields such as information retrieval, there hasn't been a thorough comparison
to see how well they work for ranking financial returns, especially when used
with modern Transformer models for stock selection. This paper addresses this
gap by systematically evaluating a diverse set of advanced loss functions
including pointwise, pairwise, listwise for daily stock return forecasting to
facilitate rank-based portfolio selection on S&P 500 data. We focus on
assessing how each loss function influences the model's ability to discern
profitable relative orderings among assets. Our research contributes a
comprehensive benchmark revealing how different loss functions impact a model's
ability to learn cross-sectional and temporal patterns crucial for portfolio
selection, thereby offering practical guidance for optimizing ranking-based
trading strategies.

</details>


### [178] [Data Understanding Survey: Pursuing Improved Dataset Characterization Via Tensor-based Methods](https://arxiv.org/abs/2510.14161)
*Matthew D. Merris,Tim Andersen*

Main category: cs.LG

TL;DR: 这篇论文探讨了传统数据集特征分析方法的局限性，并提倡使用基于张量的方法来进行更深入、更具解释性的数据理解。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集特征分析方法（如统计、结构和基于模型的方法）无法提供创新和可解释性所需的深刻理解和洞察力。

Method: 该论文调查了当前最先进的传统数据分析技术，并分析了它们的局限性。同时，讨论了各种基于张量的方法，以及这些方法如何为传统的统计、结构和基于模型的数据集特征分析技术提供更强大的替代方案。

Result: 通过实例，说明了张量方法如何揭示细致的数据特征，从而提供增强的可解释性和可操作的智能。

Conclusion: 论文提倡采用基于张量的特征分析方法，认为这有望在理解复杂数据集方面实现飞跃，并为智能的、可解释的、数据驱动的发现铺平道路。

Abstract: In the evolving domains of Machine Learning and Data Analytics, existing
dataset characterization methods such as statistical, structural, and
model-based analyses often fail to deliver the deep understanding and insights
essential for innovation and explainability. This work surveys the current
state-of-the-art conventional data analytic techniques and examines their
limitations, and discusses a variety of tensor-based methods and how these may
provide a more robust alternative to traditional statistical, structural, and
model-based dataset characterization techniques. Through examples, we
illustrate how tensor methods unveil nuanced data characteristics, offering
enhanced interpretability and actionable intelligence. We advocate for the
adoption of tensor-based characterization, promising a leap forward in
understanding complex datasets and paving the way for intelligent, explainable
data-driven discoveries.

</details>


### [179] [Towards Reversible Model Merging For Low-rank Weights](https://arxiv.org/abs/2510.14163)
*Mohammadsajad Alipour,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: 提出了一种新的模型合并方法，可以有效提升低秩压缩模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的模型合并方法应用于低秩权重时会导致严重的性能下降。

Method: 提出可逆模型合并（RMM），它构建了一个紧凑的基，原始任务特定的模型可以通过线性组合从中恢复。

Result: RMM 在各种数据集和模型规模上始终优于现有的合并方法，显著提升了低秩压缩模型的性能。

Conclusion: RMM 是一种高效、无数据且灵活的方法，为选择模型权重的最佳基和线性组合的任务特定系数提供了闭式解。

Abstract: Model merging aims to combine multiple fine-tuned models into a single set of
weights that performs well across all source tasks. While prior work has shown
that merging can approximate the performance of individual fine-tuned models
for each task, it largely overlooks scenarios where models are compressed into
low-rank representations, either through low-rank adaptation (LoRA) or
post-training singular value decomposition (SVD). We first demonstrate that
applying conventional merging methods to low-rank weights leads to severe
performance degradation in the merged model. Motivated by this phenomenon, we
propose a fundamentally different approach: instead of collapsing all adapters
into one set of weights, we construct a compact basis (e.g., an equivalent of
holding two or more models) from which original task-specific models can be
recovered via linear combination. This reframes merging as generating a
reconstruction-capable model space rather than producing a single merged model.
Crucially, this allows us to ``revert'' to each individual model when needed,
recognizing that no merged model can consistently outperform one specialized
for its task. Building on this insight, we introduce our method, Reversible
Model Merging (RMM), an efficient, data-free, and flexible method that provides
a closed-form solution for selecting the optimal basis of model weights and
task-specific coefficients for linear combination. Extensive experiments across
diverse datasets and model scales demonstrate that RMM consistently outperforms
existing merging approaches, preserving the performance of low-rank compressed
models by a significant margin.

</details>


### [180] [Optimal Control Theoretic Neural Optimizer: From Backpropagation to Dynamic Programming](https://arxiv.org/abs/2510.14168)
*Guan-Horng Liu,Tianrong Chen,Evangelos A. Theodorou*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于最优控制理论的神经网络优化器（OCNOpt），该优化器通过将反向传播与动态规划联系起来，探索贝尔曼方程的高阶展开。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的优化是现代机器学习和人工智能发展的驱动力。将DNNs视为动态系统，并用最优控制编程框架确定其最优参数，对于为数值方程到物理学的原理性分析提供理论基础至关重要。

Method: 通过将反向传播算法与动态规划的最优性条件联系起来，构建了一个变分结构，并通过求解近似动态规划到一阶展开，提出了一种新的优化方法OCNOpt，该方法探索了贝尔曼方程的高阶展开。

Result: 大量实验表明，OCNOpt在鲁棒性和效率方面优于现有方法，同时保持了可控的计算复杂度。

Conclusion: OCNOpt为基于动态系统和最优控制理论的原理性算法设计开辟了新途径，并为层级反馈策略、博弈论应用以及连续时间模型（如神经ODE）的更高阶训练提供了丰富的算法机会。

Abstract: Optimization of deep neural networks (DNNs) has been a driving force in the
advancement of modern machine learning and artificial intelligence. With DNNs
characterized by a prolonged sequence of nonlinear propagation, determining
their optimal parameters given an objective naturally fits within the framework
of Optimal Control Programming. Such an interpretation of DNNs as dynamical
systems has proven crucial in offering a theoretical foundation for principled
analysis from numerical equations to physics. In parallel to these theoretical
pursuits, this paper focuses on an algorithmic perspective. Our motivated
observation is the striking algorithmic resemblance between the Backpropagation
algorithm for computing gradients in DNNs and the optimality conditions for
dynamical systems, expressed through another backward process known as dynamic
programming. Consolidating this connection, where Backpropagation admits a
variational structure, solving an approximate dynamic programming up to the
first-order expansion leads to a new class of optimization methods exploring
higher-order expansions of the Bellman equation. The resulting optimizer,
termed Optimal Control Theoretic Neural Optimizer (OCNOpt), enables rich
algorithmic opportunities, including layer-wise feedback policies,
game-theoretic applications, and higher-order training of continuous-time
models such as Neural ODEs. Extensive experiments demonstrate that OCNOpt
improves upon existing methods in robustness and efficiency while maintaining
manageable computational complexity, paving new avenues for principled
algorithmic design grounded in dynamical systems and optimal control theory.

</details>


### [181] [MAFA: A Multi-Agent Framework for Enterprise-Scale Annotation with Configurable Task Adaptation](https://arxiv.org/abs/2510.14184)
*Mahmood Hegazy,Aaron Rodrigues,Azzam Naeem*

Main category: cs.LG

TL;DR: MAFA is a multi-agent system for enterprise-scale annotation workflows.


<details>
  <summary>Details</summary>
Motivation: Annotation backlogs in financial services require accurate categorization of customer utterances.

Method: MAFA combines specialized agents with structured reasoning and a judge-based consensus mechanism.

Result: MAFA eliminated a 1 million utterance backlog with 86% agreement with human annotators, saving over 5,000 hours annually. Achieved 13.8% higher Top-1 accuracy, 15.1% improvement in Top-5 accuracy, and 16.9% better F1 in intent classification.

Conclusion: MAFA bridges the gap between multi-agent systems and practical enterprise deployment for organizations facing annotation challenges.

Abstract: We present MAFA (Multi-Agent Framework for Annotation), a production-deployed
system that transforms enterprise-scale annotation workflows through
configurable multi-agent collaboration. Addressing the critical challenge of
annotation backlogs in financial services, where millions of customer
utterances require accurate categorization, MAFA combines specialized agents
with structured reasoning and a judge-based consensus mechanism. Our framework
uniquely supports dynamic task adaptation, allowing organizations to define
custom annotation types (FAQs, intents, entities, or domain-specific
categories) through configuration rather than code changes. Deployed at JP
Morgan Chase, MAFA has eliminated a 1 million utterance backlog while
achieving, on average, 86% agreement with human annotators, annually saving
over 5,000 hours of manual annotation work. The system processes utterances
with annotation confidence classifications, which are typically 85% high, 10%
medium, and 5% low across all datasets we tested. This enables human annotators
to focus exclusively on ambiguous and low-coverage cases. We demonstrate MAFA's
effectiveness across multiple datasets and languages, showing consistent
improvements over traditional and single-agent annotation baselines: 13.8%
higher Top-1 accuracy, 15.1% improvement in Top-5 accuracy, and 16.9% better F1
in our internal intent classification dataset and similar gains on public
benchmarks. This work bridges the gap between theoretical multi-agent systems
and practical enterprise deployment, providing a blueprint for organizations
facing similar annotation challenges.

</details>


### [182] [Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation](https://arxiv.org/abs/2510.14190)
*Ruchi Sandilya,Sumaira Perez,Charles Lynch,Lindsay Victoria,Benjamin Zebley,Derrick Matthew Buchanan,Mahendra T. Bhati,Nolan Williams,Timothy J. Spellman,Faith M. Gunning,Conor Liston,Logan Grosenick*

Main category: cs.LG

TL;DR: ConDA (Contrastive Diffusion Alignment) uses contrastive learning in diffusion model embeddings to align the latent space with system dynamics, enabling interpretable control.


<details>
  <summary>Details</summary>
Motivation: Diffusion model latent spaces lack explicit organization for interpretable control. Contrastive learning can recover disentangled representations.

Method: Applies contrastive learning within diffusion embeddings to organize latents, enabling nonlinear trajectory traversal.

Result: ConDA improves controllability and interpretability across fluid dynamics, neural calcium imaging, neurostimulation, and facial expression benchmarks.

Conclusion: Diffusion latents encode dynamics-relevant structure, but require latent organization and manifold traversal to exploit.

Abstract: Diffusion models excel at generation, but their latent spaces are not
explicitly organized for interpretable control. We introduce ConDA (Contrastive
Diffusion Alignment), a framework that applies contrastive learning within
diffusion embeddings to align latent geometry with system dynamics. Motivated
by recent advances showing that contrastive objectives can recover more
disentangled and structured representations, ConDA organizes diffusion latents
such that traversal directions reflect underlying dynamical factors. Within
this contrastively structured space, ConDA enables nonlinear trajectory
traversal that supports faithful interpolation, extrapolation, and controllable
generation. Across benchmarks in fluid dynamics, neural calcium imaging,
therapeutic neurostimulation, and facial expression, ConDA produces
interpretable latent representations with improved controllability compared to
linear traversals and conditioning-based baselines. These results suggest that
diffusion latents encode dynamics-relevant structure, but exploiting this
structure requires latent organization and traversal along the latent manifold.

</details>


### [183] [Incentive-Based Federated Learning](https://arxiv.org/abs/2510.14208)
*Chanuka A. S. Hewa Kaluannakkage,Rajkumar Buyya*

Main category: cs.LG

TL;DR: 联邦学习在保护数据隐私的同时，实现了协作模型训练。但是，由于参与者不愿贡献或可能搭便车，实际适应性受到限制。本章探讨了联邦学习激励机制设计的根本挑战，并从经济学、博弈论以及区块链和深度强化学习等技术驱动的解决方案角度，提出了集中式和分散式架构的分类。通过对医疗保健、智能基础设施、车辆网络和基于区块链的去中心化系统等新兴工业应用的分析，论证了激励机制对于联邦学习实践成功至关重要，并揭示了有前景的解决方案和仍然存在的重大挑战。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中参与者不愿贡献或可能搭便车的问题，从而提高联邦学习的实际适应性。

Method: 从经济学、博弈论以及区块链和深度强化学习等技术驱动的解决方案角度，提出了集中式和分散式架构的分类。

Result: 论证了精心设计的激励机制对于联邦学习的实践成功至关重要。

Conclusion: 揭示了联邦学习生态系统中，有前景的解决方案和仍然存在的重大挑战，强调了构建可持续、公平和稳健的联邦学习生态系统的重要性。

Abstract: Federated learning promises to revolutionize machine learning by enabling
collaborative model training without compromising data privacy. However,
practical adaptability can be limited by critical factors, such as the
participation dilemma. Participating entities are often unwilling to contribute
to a learning system unless they receive some benefits, or they may pretend to
participate and free-ride on others. This chapter identifies the fundamental
challenges in designing incentive mechanisms for federated learning systems. It
examines how foundational concepts from economics and game theory can be
applied to federated learning, alongside technology-driven solutions such as
blockchain and deep reinforcement learning. This work presents a comprehensive
taxonomy that thoroughly covers both centralized and decentralized
architectures based on the aforementioned theoretical concepts. Furthermore,
the concepts described are presented from an application perspective, covering
emerging industrial applications, including healthcare, smart infrastructure,
vehicular networks, and blockchain-based decentralized systems. Through this
exploration, this chapter demonstrates that well-designed incentive mechanisms
are not merely optional features but essential components for the practical
success of federated learning. This analysis reveals both the promising
solutions that have emerged and the significant challenges that remain in
building truly sustainable, fair, and robust federated learning ecosystems.

</details>


### [184] [Spectral Analysis of Molecular Kernels: When Richer Features Do Not Guarantee Better Generalization](https://arxiv.org/abs/2510.14217)
*Asma Jamali,Tin Sum Cheng,Rodrigo A. Vargas-Hernández*

Main category: cs.LG

TL;DR: 本文对分子核的谱性质进行了全面的分析，发现更丰富的谱特征并不一定能提高预测精度，有时甚至会产生负相关。


<details>
  <summary>Details</summary>
Motivation: 研究核方法的泛化能力和表示质量，并弥补分子核谱分析的不足。

Method: 对QM9数据集上的核岭回归进行了谱分析，使用了分子指纹、预训练的transformer、全局和局部3D表示。

Result: 更丰富的谱特征并不一定能提高精度，有时甚至会产生负相关。保留前2%的特征值几乎可以恢复所有性能。

Conclusion: 挑战了“更丰富的频谱产生更好的泛化”的常见启发式方法，并强调了表示、核特征和预测性能之间细微的关系。

Abstract: Understanding the spectral properties of kernels offers a principled
perspective on generalization and representation quality. While deep models
achieve state-of-the-art accuracy in molecular property prediction, kernel
methods remain widely used for their robustness in low-data regimes and
transparent theoretical grounding. Despite extensive studies of kernel spectra
in machine learning, systematic spectral analyses of molecular kernels are
scarce. In this work, we provide the first comprehensive spectral analysis of
kernel ridge regression on the QM9 dataset, molecular fingerprint, pretrained
transformer-based, global and local 3D representations across seven molecular
properties. Surprisingly, richer spectral features, measured by four different
spectral metrics, do not consistently improve accuracy. Pearson correlation
tests further reveal that for transformer-based and local 3D representations,
spectral richness can even have a negative correlation with performance. We
also implement truncated kernels to probe the relationship between spectrum and
predictive performance: in many kernels, retaining only the top 2% of
eigenvalues recovers nearly all performance, indicating that the leading
eigenvalues capture the most informative features. Our results challenge the
common heuristic that "richer spectra yield better generalization" and
highlight nuanced relationships between representation, kernel features, and
predictive performance. Beyond molecular property prediction, these findings
inform how kernel and self-supervised learning methods are evaluated in
data-limited scientific and real-world tasks.

</details>


### [185] [When Flatness Does (Not) Guarantee Adversarial Robustness](https://arxiv.org/abs/2510.14231)
*Nils Philipp Walter,Linara Adilova,Jilles Vreeken,Michael Kamp*

Main category: cs.LG

TL;DR: 神经网络容易受到对抗扰动的影响。扁平极小值被认为可以提高鲁棒性，但这种联系一直是非正式和不完整的。本文通过严格的形式化，表明扁平性仅意味着局部而非全局的对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络的对抗脆弱性，并探讨损失函数中的平坦极小值与鲁棒性之间的关系。

Method: 首先，推导了倒数第二层中相对平坦度的闭式表达式，然后表明可以使用它来约束输入空间中损失的变化。这允许我们正式分析整个网络的对抗鲁棒性。

Result: 平坦性仅意味着局部而非全局的对抗鲁棒性。为了保持局部邻域之外的鲁棒性，损失需要从数据流形上急剧弯曲。

Conclusion: 这项研究挑战了对平坦性的简化观点，并提供了对其在鲁棒性中作用的细致理解。

Abstract: Despite their empirical success, neural networks remain vulnerable to small,
adversarial perturbations. A longstanding hypothesis suggests that flat minima,
regions of low curvature in the loss landscape, offer increased robustness.
While intuitive, this connection has remained largely informal and incomplete.
By rigorously formalizing the relationship, we show this intuition is only
partially correct: flatness implies local but not global adversarial
robustness. To arrive at this result, we first derive a closed-form expression
for relative flatness in the penultimate layer, and then show we can use this
to constrain the variation of the loss in input space. This allows us to
formally analyze the adversarial robustness of the entire network. We then show
that to maintain robustness beyond a local neighborhood, the loss needs to
curve sharply away from the data manifold. We validate our theoretical
predictions empirically across architectures and datasets, uncovering the
geometric structure that governs adversarial vulnerability, and linking
flatness to model confidence: adversarial examples often lie in large, flat
regions where the model is confidently wrong. Our results challenge simplified
views of flatness and provide a nuanced understanding of its role in
robustness.

</details>


### [186] [Scaling Test-Time Compute to Achieve IOI Gold Medal with Open-Weight Models](https://arxiv.org/abs/2510.14232)
*Mehrzad Samadi,Aleksander Ficek,Sean Narenthiran,Siddhartha Jain,Wasi Uddin Ahmad,Somshubra Majumdar,Vahid Noroozi,Boris Ginsburg*

Main category: cs.LG

TL;DR: 本文介绍了一个名为\gencluster的、可扩展且可复现的测试时计算框架，该框架使用开放权重模型达到了IOI金牌水平。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLM）的推理和解决问题的能力，尤其是在国际信息学奥林匹克竞赛（IOI）中，这是一个衡量人类和AI编程能力的关键基准。虽然一些专有模型声称已达到IOI金牌水平，但开放权重模型实现类似结果仍然是一个重大挑战。

Method: 该框架结合了大规模生成、行为聚类、排序和循环提交策略，以在有限的验证预算下有效地探索不同的解决方案空间。

Result: 实验表明，所提出的方法的性能随可用计算资源而持续扩展，缩小了开放系统和封闭系统之间的差距。GenCluster首次使用开放权重模型gpt-oss-120b在IOI 2025上获得金牌。

Conclusion: GenCluster为LLM推理的透明和可重复评估建立了一个新的基准。

Abstract: Competitive programming has become a rigorous benchmark for evaluating the
reasoning and problem-solving capabilities of large language models (LLMs). The
International Olympiad in Informatics (IOI) stands out as one of the most
prestigious annual competitions in competitive programming and has become a key
benchmark for comparing human and AI-level programming ability. While several
proprietary models have been claimed to achieve gold medal-level performance at
the IOI, often with undisclosed methods, achieving comparable results with
open-weight models remains a significant challenge. In this paper, we present
\gencluster, a scalable and reproducible test-time compute framework that
attains IOI gold-level performance using open-weight models. It combines
large-scale generation, behavioral clustering, ranking, and a round-robin
submission strategy to efficiently explore diverse solution spaces under
limited validation budgets. Our experiments show that the performance of our
proposed approach scales consistently with available compute, narrowing the gap
between open and closed systems. Notably, we will show that GenCluster can
achieve a gold medal at IOI 2025 for the first time with an open-weight model
gpt-oss-120b, setting a new benchmark for transparent and reproducible
evaluation of reasoning in LLMs.

</details>


### [187] [Policy Regularized Distributionally Robust Markov Decision Processes with Linear Function Approximation](https://arxiv.org/abs/2510.14246)
*Jingwen Gu,Yiting He,Zhishuai Liu,Pan Xu*

Main category: cs.LG

TL;DR: 本文提出了一种名为 DR-RPO 的无模型在线策略优化方法，用于解决强化学习中分布偏移下的决策问题，该方法通过参考策略正则化和线性函数逼近来实现鲁棒策略的学习和优化。


<details>
  <summary>Details</summary>
Motivation: 研究强化学习中训练和部署环境不同的分布偏移下的决策问题，特别是在线设置中样本效率和探索至关重要的情况下，策略优化在鲁棒强化学习中仍未得到充分探索。

Method: 提出了 DR-RPO 算法，该算法结合了参考策略正则化、d-矩形线性 MDP 公式以及线性函数逼近和置信度上界奖励，以实现可处理的优化和乐观探索。

Result: 理论分析表明，策略优化可以在鲁棒强化学习中实现多项式次优性界限和样本效率，与基于价值的方法的性能相匹配。实验结果验证了理论，并证明了 DR-RPO 的鲁棒性。

Conclusion: DR-RPO 算法在解决分布偏移下的强化学习问题上具有良好的性能，并且在理论和实验上都得到了验证。

Abstract: Decision-making under distribution shift is a central challenge in
reinforcement learning (RL), where training and deployment environments differ.
We study this problem through the lens of robust Markov decision processes
(RMDPs), which optimize performance against adversarial transition dynamics.
Our focus is the online setting, where the agent has only limited interaction
with the environment, making sample efficiency and exploration especially
critical. Policy optimization, despite its success in standard RL, remains
theoretically and empirically underexplored in robust RL. To bridge this gap,
we propose \textbf{D}istributionally \textbf{R}obust \textbf{R}egularized
\textbf{P}olicy \textbf{O}ptimization algorithm (DR-RPO), a model-free online
policy optimization method that learns robust policies with sublinear regret.
To enable tractable optimization within the softmax policy class, DR-RPO
incorporates reference-policy regularization, yielding RMDP variants that are
doubly constrained in both transitions and policies. To scale to large
state-action spaces, we adopt the $d$-rectangular linear MDP formulation and
combine linear function approximation with an upper confidence bonus for
optimistic exploration. We provide theoretical guarantees showing that policy
optimization can achieve polynomial suboptimality bounds and sample efficiency
in robust RL, matching the performance of value-based approaches. Finally,
empirical results across diverse domains corroborate our theory and demonstrate
the robustness of DR-RPO.

</details>


### [188] [A Physics Prior-Guided Dual-Stream Attention Network for Motion Prediction of Elastic Bragg Breakwaters](https://arxiv.org/abs/2510.14250)
*Lianzi Jiang,Jianxin Zhang,Xinyu Han,Huanhe Dong,Xiangrong Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为PhysAttnNet的新型深度学习模型，用于预测弹性布拉格防波堤的运动响应。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在处理未见过的海况时泛化能力有限，因为它们忽略了海洋系统中的自然衰减和对波浪-结构相互作用(WSI)的建模不足。

Method: 该模型包括衰减双向自注意力(DBSA)模块、相位差引导双向交叉注意力(PDG-BCA)模块和全局上下文融合(GCF)模块，并使用混合时频损失进行训练。

Result: 在波浪水槽数据集上的实验表明，PhysAttnNet 明显优于主流模型，并且具有良好的鲁棒性和适应性。

Conclusion: PhysAttnNet 有潜力成为开发海洋工程复杂系统预测模型的框架。

Abstract: Accurate motion response prediction for elastic Bragg breakwaters is critical
for their structural safety and operational integrity in marine environments.
However, conventional deep learning models often exhibit limited generalization
capabilities when presented with unseen sea states. These deficiencies stem
from the neglect of natural decay observed in marine systems and inadequate
modeling of wave-structure interaction (WSI). To overcome these challenges,
this study proposes a novel Physics Prior-Guided Dual-Stream Attention Network
(PhysAttnNet). First, the decay bidirectional self-attention (DBSA) module
incorporates a learnable temporal decay to assign higher weights to recent
states, aiming to emulate the natural decay phenomenon. Meanwhile, the phase
differences guided bidirectional cross-attention (PDG-BCA) module explicitly
captures the bidirectional interaction and phase relationship between waves and
the structure using a cosine-based bias within a bidirectional
cross-computation paradigm. These streams are synergistically integrated
through a global context fusion (GCF) module. Finally, PhysAttnNet is trained
with a hybrid time-frequency loss that jointly minimizes time-domain prediction
errors and frequency-domain spectral discrepancies. Comprehensive experiments
on wave flume datasets demonstrate that PhysAttnNet significantly outperforms
mainstream models. Furthermore,cross-scenario generalization tests validate the
model's robustness and adaptability to unseen environments, highlighting its
potential as a framework to develop predictive models for complex systems in
ocean engineering.

</details>


### [189] [Generalist vs Specialist Time Series Foundation Models: Investigating Potential Emergent Behaviors in Assessing Human Health Using PPG Signals](https://arxiv.org/abs/2510.14254)
*Saurabh Kataria,Yi Wu,Zhaoliang Chen,Hyunjung Gloria Kwak,Yuhao Xu,Lovely Yeswanth Panchumarthi,Ran Xiao,Jiaying Lu,Ayca Ermis,Anni Zhao,Runze Yan,Alex Federov,Zewen Liu,Xu Wu,Wei Jin,Carl Yang,Jocelyn Grunwell,Stephanie R. Brown,Amit Shah,Craig Jabaley,Tim Buchman,Sivasubramanium V Bhavani,Randall J. Lee,Xiao Hu*

Main category: cs.LG

TL;DR: 这篇论文比较了通用和专用时间序列基础模型在PPG信号上的性能，通过51个任务全面评估了它们在多个维度上的表现。


<details>
  <summary>Details</summary>
Motivation: 目前的时间序列基础模型大多是专家模型，而通用时间序列模型的研究较少。本文旨在比较通用模型和专家模型在PPG信号上的性能。

Method: 通过51个任务，在心脏状态评估、实验室数值估计和跨模态推断等方面，从胜率、平均性能、特征质量等七个维度综合评估了通用模型和专家模型。

Result: 在全参数微调的情况下，专家模型的胜率高出27%。

Conclusion: 论文全面评估了通用模型和专家模型，揭示了它们在不同下游场景中的优势和局限性，并进一步分析了泛化性、公平性、注意力可视化以及训练数据选择的重要性。

Abstract: Foundation models are large-scale machine learning models that are
pre-trained on massive amounts of data and can be adapted for various
downstream tasks. They have been extensively applied to tasks in Natural
Language Processing and Computer Vision with models such as GPT, BERT, and
CLIP. They are now also increasingly gaining attention in time-series analysis,
particularly for physiological sensing. However, most time series foundation
models are specialist models - with data in pre-training and testing of the
same type, such as Electrocardiogram, Electroencephalogram, and
Photoplethysmogram (PPG). Recent works, such as MOMENT, train a generalist time
series foundation model with data from multiple domains, such as weather,
traffic, and electricity. This paper aims to conduct a comprehensive
benchmarking study to compare the performance of generalist and specialist
models, with a focus on PPG signals. Through an extensive suite of total 51
tasks covering cardiac state assessment, laboratory value estimation, and
cross-modal inference, we comprehensively evaluate both models across seven
dimensions, including win score, average performance, feature quality, tuning
gain, performance variance, transferability, and scalability. These metrics
jointly capture not only the models' capability but also their adaptability,
robustness, and efficiency under different fine-tuning strategies, providing a
holistic understanding of their strengths and limitations for diverse
downstream scenarios. In a full-tuning scenario, we demonstrate that the
specialist model achieves a 27% higher win score. Finally, we provide further
analysis on generalization, fairness, attention visualizations, and the
importance of training data choice.

</details>


### [190] [CAST: Compositional Analysis via Spectral Tracking for Understanding Transformer Layer Functions](https://arxiv.org/abs/2510.14262)
*Zihao Fu,Ming Liao,Chris Russell,Zhenguang G. Cai*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为CAST的新框架，用于分析transformer层的功能，通过直接转换矩阵估计和综合频谱分析。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型取得了显著的成功，但其内部机制仍然很大程度上是黑盒，理解不足。

Method: 该方法通过Moore-Penrose伪逆估计每一层的实现转换矩阵，并应用具有六个可解释指标的频谱分析来表征层行为。

Result: 分析揭示了仅编码器和仅解码器模型之间的不同行为，解码器模型表现出压缩-扩展周期，而编码器模型保持一致的高秩处理。内核分析进一步证明了层之间的功能关系模式，CKA相似性矩阵将层清楚地划分为三个阶段：特征提取、压缩和专门化。

Conclusion: CAST提供了一种新的视角，通过分析transformer层的功能，为现有的方法提供了补充的见解。

Abstract: Large language models have achieved remarkable success but remain largely
black boxes with poorly understood internal mechanisms. To address this
limitation, many researchers have proposed various interpretability methods
including mechanistic analysis, probing classifiers, and activation
visualization, each providing valuable insights from different perspectives.
Building upon this rich landscape of complementary approaches, we introduce
CAST (Compositional Analysis via Spectral Tracking), a probe-free framework
that contributes a novel perspective by analyzing transformer layer functions
through direct transformation matrix estimation and comprehensive spectral
analysis. CAST offers complementary insights to existing methods by estimating
the realized transformation matrices for each layer using Moore-Penrose
pseudoinverse and applying spectral analysis with six interpretable metrics
characterizing layer behavior. Our analysis reveals distinct behaviors between
encoder-only and decoder-only models, with decoder models exhibiting
compression-expansion cycles while encoder models maintain consistent high-rank
processing. Kernel analysis further demonstrates functional relationship
patterns between layers, with CKA similarity matrices clearly partitioning
layers into three phases: feature extraction, compression, and specialization.

</details>
