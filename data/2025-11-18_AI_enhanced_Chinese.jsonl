{"id": "2511.12057", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.12057", "abs": "https://arxiv.org/abs/2511.12057", "authors": ["Ashwin Gerard Colaco", "Martin Boissier", "Sriram Rao", "Shubharoop Ghosh", "Sharad Mehrotra", "Tilmann Rabl"], "title": "GenIE - Simulator-Driven Iterative Data Exploration for Scientific Discovery", "comment": null, "summary": "Physics-based simulators play a critical role in scientific discovery and risk assessment, enabling what-if analyses for events like wildfires and hurricanes. Today, databases treat these simulators as external pre-processing steps. Analysts must manually run a simulation, export the results, and load them into a database before analysis can begin. This linear workflow is inefficient, incurs high latency, and hinders interactive exploration, especially when the analysis itself dictates the need for new or refined simulation data.\n  We envision a new database paradigm, entitled GenIE, that seamlessly integrates multiple simulators into databases to enable dynamic orchestration of simulation workflows. By making the database \"simulation-aware,\" GenIE can dynamically invoke simulators with appropriate parameters based on the user's query and analytical needs. This tight integration allows GenIE to avoid generating data irrelevant to the analysis, reuse previously generated data, and support iterative, incremental analysis where results are progressively refined at interactive speeds.\n  We present our vision for GenIE, designed as an extension to PostgreSQL, and demonstrate its potential benefits through comprehensive use cases: wildfire smoke dispersion analysis using WRF-SFIRE and HYSPLIT, and hurricane hazard assessment integrating wind, surge, and flood models. Our preliminary experiments show how GenIE can transform these slow, static analyses into interactive explorations by intelligently managing the trade-off between simulation accuracy and runtime across multiple integrated simulators. We conclude by highlighting the challenges and opportunities ahead in realizing the full vision of GenIE as a cornerstone for next-generation scientific data analysis.", "AI": {"tldr": "GenIE: A database paradigm integrating simulators for dynamic orchestration of simulation workflows, enabling interactive exploration and analysis.", "motivation": "Current workflows treat simulators as external pre-processing steps, leading to inefficiency and high latency, hindering interactive exploration.", "method": "Extending PostgreSQL to create GenIE, a simulation-aware database that dynamically invokes simulators based on queries and analytical needs.", "result": "Preliminary experiments demonstrate GenIE's potential to transform slow, static analyses into interactive explorations by managing the trade-off between simulation accuracy and runtime.", "conclusion": "GenIE presents challenges and opportunities as a cornerstone for next-generation scientific data analysis."}}
{"id": "2511.12457", "categories": ["cs.DB", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.12457", "abs": "https://arxiv.org/abs/2511.12457", "authors": ["Gaurav Jain", "Brandon Baker", "Joe Yin", "Chenwei Xie", "Zihao Ye", "Sidh Kulkarni", "Sara Abdelrahman", "Nova Qi", "Urjeet Shrestha", "Mike Halcrow", "Dave Bailey", "Yuxiong He"], "title": "SEE++: Evolving Snowpark Execution Environment for Modern Workloads", "comment": "4 pages, 4 figures, accepted as a Poster at IEEE BigData 2025", "summary": "Snowpark enables Data Engineering and AI/ML workloads to run directly within Snowflake by deploying a secure sandbox on virtual warehouse nodes. This Snowpark Execution Environment (SEE) allows users to execute arbitrary workloads in Python and other languages in a secure and performant manner. As adoption has grown, the diversity of workloads has introduced increasingly sophisticated needs for sandboxing. To address these evolving requirements, Snowpark transitioned its in-house sandboxing solution to gVisor, augmented with targeted optimizations. This paper describes both the functional and performance objectives that guided the upgrade, outlines the new sandbox architecture, and details the challenges encountered during the journey, along with the solutions developed to resolve them. Finally, we present case studies that highlight new features enabled by the upgraded architecture, demonstrating SEE's extensibility and flexibility in supporting the next generation of Snowpark workloads.", "AI": {"tldr": "Snowpark\u5347\u7ea7\u4e86\u5176\u5185\u90e8\u6c99\u7bb1\u89e3\u51b3\u65b9\u6848\u5230gVisor\uff0c\u5e76\u8fdb\u884c\u4e86\u4f18\u5316\u3002", "motivation": "\u968f\u7740\u91c7\u7528\u7387\u7684\u589e\u957f\uff0c\u5de5\u4f5c\u8d1f\u8f7d\u7684\u591a\u6837\u6027\u5bf9\u6c99\u7bb1\u63d0\u51fa\u4e86\u8d8a\u6765\u8d8a\u590d\u6742\u7684\u9700\u6c42\u3002", "method": "Snowpark\u8fc7\u6e21\u5230gVisor\uff0c\u5e76\u8fdb\u884c\u4e86\u6709\u9488\u5bf9\u6027\u7684\u4f18\u5316\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5347\u7ea7\u540e\u7684\u67b6\u6784\u6240\u542f\u7528\u7684\u65b0\u529f\u80fd\uff0c\u8bc1\u660e\u4e86SEE\u5728\u652f\u6301\u4e0b\u4e00\u4ee3Snowpark\u5de5\u4f5c\u8d1f\u8f7d\u65b9\u9762\u7684\u53ef\u6269\u5c55\u6027\u548c\u7075\u6d3b\u6027\u3002", "conclusion": "\u672c\u6587\u63cf\u8ff0\u4e86\u6307\u5bfc\u5347\u7ea7\u7684\u529f\u80fd\u548c\u6027\u80fd\u76ee\u6807\uff0c\u6982\u8ff0\u4e86\u65b0\u7684\u6c99\u7bb1\u67b6\u6784\uff0c\u5e76\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u5728\u6b64\u8fc7\u7a0b\u4e2d\u9047\u5230\u7684\u6311\u6218\u4ee5\u53ca\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u800c\u5f00\u53d1\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.13059", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.13059", "abs": "https://arxiv.org/abs/2511.13059", "authors": ["Johannes Wehrstein", "Roman Heinrich", "Mihail Stoian", "Skander Krid", "Martin Stemmer", "Andreas Kipf", "Carsten Binnig", "Muhammad El-Hindi"], "title": "Redbench: Workload Synthesis From Cloud Traces", "comment": null, "summary": "Workload traces from cloud data warehouse providers reveal that standard benchmarks such as TPC-H and TPC-DS fail to capture key characteristics of real-world workloads, including query repetition and string-heavy queries. In this paper, we introduce Redbench, a novel benchmark featuring a workload generator that reproduces real-world workload characteristics derived from traces released by cloud providers. Redbench integrates multiple workload generation techniques to tailor workloads to specific objectives, transforming existing benchmarks into realistic query streams that preserve intrinsic workload characteristics. By focusing on inherent workload signals rather than execution-specific metrics, Redbench bridges the gap between synthetic and real workloads. Our evaluation shows that (1) Redbench produces more realistic and reproducible workloads for cloud data warehouse benchmarking, and (2) Redbench reveals the impact of system optimizations across four commercial data warehouse platforms. We believe that Redbench provides a crucial foundation for advancing research on optimization techniques for modern cloud data warehouses.", "AI": {"tldr": "Redbench\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b83\u4f7f\u7528\u5de5\u4f5c\u8d1f\u8f7d\u751f\u6210\u5668\u6765\u91cd\u73b0\u771f\u5b9e\u4e16\u754c\u7684\u5de5\u4f5c\u8d1f\u8f7d\u7279\u5f81\uff0c\u5f25\u5408\u4e86\u5408\u6210\u5de5\u4f5c\u8d1f\u8f7d\u548c\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982TPC-H\u548cTPC-DS\uff09\u65e0\u6cd5\u6355\u6349\u5230\u771f\u5b9e\u4e16\u754c\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5173\u952e\u7279\u5f81\uff0c\u5305\u62ec\u67e5\u8be2\u91cd\u590d\u548c\u5b57\u7b26\u4e32\u7e41\u91cd\u7684\u67e5\u8be2\u3002", "method": "Redbench\u96c6\u6210\u4e86\u591a\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u751f\u6210\u6280\u672f\uff0c\u4ee5\u5b9a\u5236\u5de5\u4f5c\u8d1f\u8f7d\u4ee5\u9002\u5e94\u7279\u5b9a\u76ee\u6807\uff0c\u5c06\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u8f6c\u6362\u4e3a\u4fdd\u7559\u5185\u5728\u5de5\u4f5c\u8d1f\u8f7d\u7279\u5f81\u7684\u771f\u5b9e\u67e5\u8be2\u6d41\u3002", "result": "Redbench\u4e3a\u4e91\u6570\u636e\u4ed3\u5e93\u57fa\u51c6\u6d4b\u8bd5\u751f\u6210\u4e86\u66f4\u771f\u5b9e\u548c\u53ef\u91cd\u73b0\u7684\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5e76\u63ed\u793a\u4e86\u7cfb\u7edf\u4f18\u5316\u5728\u56db\u4e2a\u5546\u4e1a\u6570\u636e\u4ed3\u5e93\u5e73\u53f0\u4e0a\u7684\u5f71\u54cd\u3002", "conclusion": "Redbench\u4e3a\u63a8\u8fdb\u73b0\u4ee3\u4e91\u6570\u636e\u4ed3\u5e93\u4f18\u5316\u6280\u672f\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u91cd\u8981\u7684\u57fa\u7840\u3002"}}
{"id": "2511.12061", "categories": ["cs.CV", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.12061", "abs": "https://arxiv.org/abs/2511.12061", "authors": ["Zhichen Lai", "Hua Lu", "Huan Li", "Jialiang Li", "Christian S. Jensen"], "title": "MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity", "comment": "8 pages, 6 figures; accepted by AAAI 2026 as an Oral paper", "summary": "Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.", "AI": {"tldr": "MovSemCL\u662f\u4e00\u79cd\u7528\u4e8e\u8f68\u8ff9\u76f8\u4f3c\u6027\u8ba1\u7b97\u7684\u8fd0\u52a8\u8bed\u4e49\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8f68\u8ff9\u8bed\u4e49\u548c\u5c42\u6b21\u7ed3\u6784\u5efa\u6a21\u4e0d\u8db3\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4f7f\u7528\u7269\u7406\u4e0a\u4e0d\u5408\u7406\u7684\u589e\u5f3a\u3002", "method": "MovSemCL\u5c06\u539f\u59cbGPS\u8f68\u8ff9\u8f6c\u6362\u4e3a\u8fd0\u52a8\u8bed\u4e49\u7279\u5f81\uff0c\u7136\u540e\u5c06\u5176\u5206\u5272\u6210\u5c0f\u5757\u3002\u63a5\u4e0b\u6765\uff0cMovSemCL\u91c7\u7528\u5185\u90e8\u548c\u5916\u90e8\u8865\u4e01\u6ce8\u610f\u529b\u6765\u7f16\u7801\u672c\u5730\u548c\u5168\u5c40\u8f68\u8ff9\u6a21\u5f0f\u3002\u6b64\u5916\uff0cMovSemCL\u5305\u62ec\u4e00\u79cd\u66f2\u7387\u5f15\u5bfc\u7684\u589e\u5f3a\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u4fdd\u7559\u4fe1\u606f\u4e30\u5bcc\u7684\u7247\u6bb5\u5e76\u5c4f\u853d\u5197\u4f59\u7247\u6bb5\uff0c\u4ece\u800c\u751f\u6210\u7269\u7406\u4e0a\u5408\u7406\u7684\u589e\u5f3a\u89c6\u56fe\u3002", "result": "MovSemCL\u5728\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMovSemCL\u80fd\u591f\u80dc\u8fc7\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5728\u76f8\u4f3c\u6027\u641c\u7d22\u4efb\u52a1\u4e2d\u5b9e\u73b0\u63a5\u8fd1\u7406\u60f3\u503c1\u7684\u5e73\u5747\u6392\u540d\uff0c\u5e76\u5728\u542f\u53d1\u5f0f\u8fd1\u4f3c\u4e2d\u63d0\u9ad8\u9ad8\u8fbe20.3%\uff0c\u540c\u65f6\u5c06\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe43.4%\u3002", "conclusion": "MovSemCL\u662f\u4e00\u79cd\u7528\u4e8e\u8f68\u8ff9\u76f8\u4f3c\u6027\u8ba1\u7b97\u7684\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2511.11649", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11649", "abs": "https://arxiv.org/abs/2511.11649", "authors": ["Jannik Nitschke"], "title": "The Environmental Impact of Ensemble Techniques in Recommender Systems", "comment": "Bachelor Thesis, University of Siegen", "summary": "Ensemble techniques in recommender systems have demonstrated accuracy improvements of 10-30%, yet their environmental impact remains unmeasured. While deep learning recommendation algorithms can generate up to 3,297 kg CO2 per paper, ensemble methods have not been sufficiently evaluated for energy consumption. This thesis investigates how ensemble techniques influence environmental impact compared to single optimized models.\n  We conducted 93 experiments across two frameworks (Surprise for rating prediction, LensKit for ranking) on four datasets spanning 100,000 to 7.8 million interactions. We evaluated four ensemble strategies (Average, Weighted, Stacking/Rank Fusion, Top Performers) against simple baselines and optimized single models, measuring energy consumption with a smart plug.\n  Results revealed a non-linear accuracy-energy relationship. Ensemble methods achieved 0.3-5.7% accuracy improvements while consuming 19-2,549% more energy depending on dataset size and strategy. The Top Performers ensemble showed best efficiency: 0.96% RMSE improvement with 18.8% energy overhead on MovieLens-1M, and 5.7% NDCG improvement with 103% overhead on MovieLens-100K. Exhaustive averaging strategies consumed 88-270% more energy for comparable gains. On the largest dataset (Anime, 7.8M interactions), the Surprise ensemble consumed 2,005% more energy (0.21 Wh vs. 0.01 Wh) for 1.2% accuracy improvement, producing 53.8 mg CO2 versus 2.6 mg CO2 for the single model.\n  This research provides one of the first systematic measurements of energy and carbon footprint for ensemble recommender systems, demonstrates that selective strategies offer superior efficiency over exhaustive averaging, and identifies scalability limitations at industrial scale. These findings enable informed decisions about sustainable algorithm selection in recommender systems.", "AI": {"tldr": "Ensemble methods in recommender systems improve accuracy by 0.3-5.7% but increase energy consumption by 19-2,549%. Selective ensemble strategies are more efficient than exhaustive averaging.", "motivation": "To measure the environmental impact of ensemble techniques in recommender systems compared to single optimized models, as existing research lacks energy consumption evaluations for ensemble methods despite their accuracy improvements.", "method": "Conducted 93 experiments across two frameworks (Surprise, LensKit) on four datasets, evaluating four ensemble strategies (Average, Weighted, Stacking/Rank Fusion, Top Performers) against baselines and optimized single models, measuring energy consumption with a smart plug.", "result": "Ensemble methods achieved 0.3-5.7% accuracy improvements while consuming 19-2,549% more energy. Top Performers ensemble showed best efficiency. Exhaustive averaging strategies consumed 88-270% more energy for comparable gains. On the largest dataset, the Surprise ensemble consumed 2,005% more energy for 1.2% accuracy improvement.", "conclusion": "Selective ensemble strategies offer superior efficiency over exhaustive averaging, and identifies scalability limitations at industrial scale. These findings enable informed decisions about sustainable algorithm selection in recommender systems."}}
{"id": "2511.11591", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11591", "abs": "https://arxiv.org/abs/2511.11591", "authors": ["Olusola Babalola", "Bolanle Ojokoh", "Olutayo Boyinbode"], "title": "LLM-Generated Negative News Headlines Dataset: Creation and Benchmarking Against Real Journalism", "comment": "50 pages, 19 figures, 9 tables", "summary": "This research examines the potential of datasets generated by Large Language Models (LLMs) to support Natural Language Processing (NLP) tasks, aiming to overcome challenges related to data acquisition and privacy concerns associated with real-world data. Focusing on negative valence text, a critical component of sentiment analysis, we explore the use of LLM-generated synthetic news headlines as an alternative to real-world data. A specialized corpus of negative news headlines was created using tailored prompts to capture diverse negative sentiments across various societal domains. The synthetic headlines were validated by expert review and further analyzed in embedding space to assess their alignment with real-world negative news in terms of content, tone, length, and style. Key metrics such as correlation with real headlines, perplexity, coherence, and realism were evaluated. The synthetic dataset was benchmarked against two sets of real news headlines using evaluations including the Comparative Perplexity Test, Comparative Readability Test, Comparative POS Profiling, BERTScore, and Comparative Semantic Similarity. Results show the generated headlines match real headlines with the only marked divergence being in the proper noun score of the POS profile test.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u6570\u636e\u96c6\u6765\u652f\u6301\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4efb\u52a1\u7684\u6f5c\u529b\uff0c\u65e8\u5728\u514b\u670d\u4e0e\u6570\u636e\u83b7\u53d6\u548c\u4e0e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u76f8\u5173\u7684\u9690\u79c1\u95ee\u9898\u76f8\u5173\u7684\u6311\u6218\u3002", "motivation": "\u65e8\u5728\u514b\u670d\u4e0e\u6570\u636e\u83b7\u53d6\u548c\u4e0e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u76f8\u5173\u7684\u9690\u79c1\u95ee\u9898\u76f8\u5173\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u5b9a\u5236\u7684\u63d0\u793a\u521b\u5efa\u4e86\u4e00\u4e2a\u4e13\u95e8\u7684\u8d1f\u9762\u65b0\u95fb\u6807\u9898\u8bed\u6599\u5e93\uff0c\u4ee5\u6355\u6349\u5404\u4e2a\u793e\u4f1a\u9886\u57df\u7684\u5404\u79cd\u8d1f\u9762\u60c5\u7eea\u3002\u901a\u8fc7\u4e13\u5bb6\u8bc4\u5ba1\u9a8c\u8bc1\u5408\u6210\u6807\u9898\uff0c\u5e76\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8fdb\u4e00\u6b65\u5206\u6790\uff0c\u4ee5\u8bc4\u4f30\u5176\u5728\u5185\u5bb9\u3001\u8bed\u6c14\u3001\u957f\u5ea6\u548c\u98ce\u683c\u65b9\u9762\u4e0e\u771f\u5b9e\u4e16\u754c\u8d1f\u9762\u65b0\u95fb\u7684\u4e00\u81f4\u6027\u3002\u8bc4\u4f30\u4e86\u4e0e\u771f\u5b9e\u6807\u9898\u7684\u76f8\u5173\u6027\u3001\u56f0\u60d1\u5ea6\u3001\u8fde\u8d2f\u6027\u548c\u771f\u5b9e\u6027\u7b49\u5173\u952e\u6307\u6807\u3002\u4f7f\u7528\u5305\u62ec\u6bd4\u8f83\u56f0\u60d1\u5ea6\u6d4b\u8bd5\u3001\u6bd4\u8f83\u53ef\u8bfb\u6027\u6d4b\u8bd5\u3001\u6bd4\u8f83 POS \u6982\u51b5\u5206\u6790\u3001BERTScore \u548c\u6bd4\u8f83\u8bed\u4e49\u76f8\u4f3c\u6027\u5728\u5185\u7684\u8bc4\u4f30\uff0c\u9488\u5bf9\u4e24\u7ec4\u771f\u5b9e\u65b0\u95fb\u6807\u9898\u5bf9\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u751f\u6210\u7684\u6807\u9898\u4e0e\u771f\u5b9e\u6807\u9898\u76f8\u5339\u914d\uff0c\u552f\u4e00\u7684\u660e\u663e\u5dee\u5f02\u662f POS \u6982\u51b5\u6d4b\u8bd5\u7684\u4e13\u6709\u540d\u8bcd\u5f97\u5206\u3002", "conclusion": "\u751f\u6210\u7684\u6807\u9898\u4e0e\u771f\u5b9e\u6807\u9898\u76f8\u5339\u914d\uff0c\u552f\u4e00\u7684\u660e\u663e\u5dee\u5f02\u662f POS \u6982\u51b5\u6d4b\u8bd5\u7684\u4e13\u6709\u540d\u8bcd\u5f97\u5206\u3002"}}
{"id": "2511.11594", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11594", "abs": "https://arxiv.org/abs/2511.11594", "authors": ["James McCammon"], "title": "TimeStampEval: A Simple LLM Eval and a Little Fuzzy Matching Trick to Improve Search Accuracy", "comment": null, "summary": "Traditional fuzzy matching often fails when searching for quotes that are semantically identical but syntactically different across documents-a common issue when aligning official written records with speech-to-text transcripts. We introduce TimeStampEval, a benchmark for retrieving precise millisecond timestamps from long transcripts given non-verbatim quotes. Our simple two-stage method dramatically improves retrieval accuracy while cutting inference costs by over 90%. The motivating use case is an automated long-form podcast that assembles Congressional Record clips into AI-hosted narration. The technical challenge: given a sentence-timestamped transcript and a target quote that may differ due to transcription or editorial drift, return exact start and end boundaries. Standard algorithms handle verbatim text but break under fuzzier variants. Evaluating six modern LLMs on a 2,800-sentence (120k-token) transcript revealed four key findings. (1) Prompt design matters more than model choice: placing the query before the transcript and using compact formatting improved accuracy by 3-20 points while reducing token count by 30-40%. (2) Off-by-one errors form a distinct category, showing models understand the task but misplace boundaries. (3) A modest reasoning budget (600-850 tokens) raises accuracy from 37% to 77% for weak setups and to above 90% for strong ones. (4) Our \"Assisted Fuzzy\" approach-RapidFuzz pre-filtering followed by LLM verification on short snippets-improves fuzzy match accuracy by up to 50 points while halving latency and reducing cost per correct result by up to 96%. Extended tests on ten transcripts (50k-900k tokens, 1989-2025) confirm robustness to transcript length, vocabulary drift, and domain change, maintaining 95-100% rejection accuracy for absent targets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aTimeStampEval\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u4ece\u957f\u6587\u672c\u8bb0\u5f55\u4e2d\u68c0\u7d22\u7cbe\u786e\u5230\u6beb\u79d2\u7684\u65f6\u95f4\u6233\uff0c\u4e3b\u8981\u9488\u5bf9\u975e\u5b8c\u5168\u76f8\u540c\u7684\u5f15\u7528\u7684\u68c0\u7d22\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6a21\u7cca\u5339\u914d\u5728\u641c\u7d22\u8bed\u4e49\u76f8\u540c\u4f46\u53e5\u6cd5\u4e0d\u540c\u7684\u5f15\u7528\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u5bf9\u9f50\u5b98\u65b9\u4e66\u9762\u8bb0\u5f55\u548c\u8bed\u97f3\u8f6c\u6587\u672c\u8bb0\u5f55\u65f6\u3002\u8be5\u7814\u7a76\u7684\u52a8\u673a\u662f\u521b\u5efa\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u957f\u7bc7\u64ad\u5ba2\uff0c\u5c06\u56fd\u4f1a\u8bb0\u5f55\u7247\u6bb5\u5408\u6210\u4e3aAI\u4e3b\u6301\u7684\u53d9\u8ff0\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1aRapidFuzz\u9884\u8fc7\u6ee4\uff0c\u7136\u540e\u7531LLM\u9a8c\u8bc1\u77ed\u7247\u6bb5\uff0c\u79f0\u4e3a\u201cAssisted Fuzzy\u201d\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPrompt\u8bbe\u8ba1\u6bd4\u6a21\u578b\u9009\u62e9\u66f4\u91cd\u8981\uff1b\u9002\u5ea6\u7684\u63a8\u7406\u9884\u7b97\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u7387\uff1b\u6240\u63d0\u51fa\u7684\u201cAssisted Fuzzy\u201d\u65b9\u6cd5\u53ef\u4ee5\u5c06\u6a21\u7cca\u5339\u914d\u51c6\u786e\u7387\u63d0\u9ad8\u9ad8\u8fbe50\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u964d\u4f4e\u5ef6\u8fdf\u548c\u6210\u672c\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u957f\u5ea6\u3001\u8bcd\u6c47\u548c\u9886\u57df\u7684\u6587\u672c\u8bb0\u5f55\u4e0a\u90fd\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u5bf9\u4e0d\u5b58\u5728\u7684\u76ee\u6807\u4fdd\u630195-100%\u7684\u62d2\u7edd\u51c6\u786e\u7387\u3002"}}
{"id": "2511.11633", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11633", "abs": "https://arxiv.org/abs/2511.11633", "authors": ["Abhijeet Kumar", "Chetan Agarwal", "Pronoy B. Neogi", "Mayank Goswami"], "title": "Psychological stress during Examination and its estimation by handwriting in answer script", "comment": "10 Pages, 6 Figures and 1 Table", "summary": "This research explores the fusion of graphology and artificial intelligence to quantify psychological stress levels in students by analyzing their handwritten examination scripts. By leveraging Optical Character Recognition and transformer based sentiment analysis models, we present a data driven approach that transcends traditional grading systems, offering deeper insights into cognitive and emotional states during examinations. The system integrates high resolution image processing, TrOCR, and sentiment entropy fusion using RoBERTa based models to generate a numerical Stress Index. Our method achieves robustness through a five model voting mechanism and unsupervised anomaly detection, making it an innovative framework in academic forensics.", "AI": {"tldr": "\u672c\u7814\u7a76\u878d\u5408\u7b14\u8ff9\u5b66\u548c\u4eba\u5de5\u667a\u80fd\uff0c\u901a\u8fc7\u5206\u6790\u5b66\u751f\u624b\u5199\u8bd5\u5377\u6765\u91cf\u5316\u5fc3\u7406\u538b\u529b\u6c34\u5e73\u3002", "motivation": "\u65e8\u5728\u8d85\u8d8a\u4f20\u7edf\u8bc4\u5206\u7cfb\u7edf\uff0c\u66f4\u6df1\u5165\u5730\u4e86\u89e3\u8003\u8bd5\u671f\u95f4\u7684\u8ba4\u77e5\u548c\u60c5\u7eea\u72b6\u6001\u3002", "method": "\u5229\u7528\u5149\u5b66\u5b57\u7b26\u8bc6\u522b\u548c\u57fa\u4e8eTransformer\u7684\u60c5\u611f\u5206\u6790\u6a21\u578b\uff0c\u7ed3\u5408\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5904\u7406\u3001TrOCR\u548c\u57fa\u4e8eRoBERTa\u6a21\u578b\u7684\u60c5\u611f\u71b5\u878d\u5408\uff0c\u751f\u6210\u6570\u503c\u538b\u529b\u6307\u6570\u3002", "result": "\u901a\u8fc7\u4e94\u6a21\u578b\u6295\u7968\u673a\u5236\u548c\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u5b9e\u73b0\u7a33\u5065\u6027\u3002", "conclusion": "\u4e3a\u5b66\u672f\u53d6\u8bc1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u521b\u65b0\u6846\u67b6\u3002"}}
{"id": "2511.11573", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11573", "abs": "https://arxiv.org/abs/2511.11573", "authors": ["Christopher R. Lee-Jenkins"], "title": "Softmax as a Lagrangian-Legendrian Seam", "comment": null, "summary": "This note offers a first bridge from machine learning to modern differential geometry. We show that the logits-to-probabilities step implemented by softmax can be modeled as a geometric interface: two potential-generated, conservative descriptions (from negative entropy and log-sum-exp) meet along a Legendrian \"seam\" on a contact screen (the probability simplex) inside a simple folded symplectic collar. Bias-shift invariance appears as Reeb flow on the screen, and the Fenchel-Young equality/KL gap provides a computable distance to the seam. We work out the two- and three-class cases to make the picture concrete and outline next steps for ML: compact logit models (projective or spherical), global invariants, and connections to information geometry where on-screen dynamics manifest as replicator flows.", "AI": {"tldr": "softmax\u53ef\u4ee5\u88ab\u5efa\u6a21\u4e3a\u4e00\u4e2a\u51e0\u4f55\u754c\u9762\uff1a\u4e24\u4e2a\u52bf\u751f\u6210\u7684\u5b88\u6052\u63cf\u8ff0\uff08\u6765\u81ea\u8d1f\u71b5\u548clog-sum-exp\uff09\u5728\u4e00\u4e2a\u7b80\u5355\u7684\u6298\u53e0\u8f9b\u73af\u5185\u7684\u63a5\u89e6\u5c4f\u5e55\uff08\u6982\u7387\u5355\u7eaf\u5f62\uff09\u4e0a\u7684\u4e00\u4e2a\u52d2\u8ba9\u5fb7\u201c\u7f1d\u201d\u4e0a\u76f8\u9047\u3002", "motivation": "\u5efa\u7acb\u673a\u5668\u5b66\u4e60\u4e0e\u73b0\u4ee3\u5fae\u5206\u51e0\u4f55\u4e4b\u95f4\u7684\u6865\u6881", "method": "\u5c06softmax\u5efa\u6a21\u4e3a\u4e00\u4e2a\u51e0\u4f55\u754c\u9762\uff0c\u7814\u7a76\u5176\u5728\u6982\u7387\u5355\u7eaf\u5f62\u4e0a\u7684\u6027\u8d28\u3002", "result": "\u504f\u5dee-\u5e73\u79fb\u4e0d\u53d8\u6027\u8868\u73b0\u4e3a\u5c4f\u5e55\u4e0a\u7684Reeb\u6d41\uff0cFenchel-Young\u7b49\u5f0f/KL\u95f4\u9699\u63d0\u4f9b\u4e86\u5230\u7f1d\u7684\u53ef\u8ba1\u7b97\u8ddd\u79bb\u3002\u8be6\u7ec6\u9610\u8ff0\u4e86\u4e8c\u7c7b\u548c\u4e09\u7c7b\u7684\u60c5\u51b5\u3002", "conclusion": "\u63d0\u51fa\u4e86\u673a\u5668\u5b66\u4e60\u7684\u4e0b\u4e00\u6b65\uff1a\u7d27\u51d1logit\u6a21\u578b\uff08\u5c04\u5f71\u6216\u7403\u5f62\uff09\uff0c\u5168\u5c40\u4e0d\u53d8\u91cf\uff0c\u4ee5\u53ca\u4e0e\u4fe1\u606f\u51e0\u4f55\u7684\u8054\u7cfb\uff0c\u5176\u4e2d\u5c4f\u5e55\u4e0a\u7684\u52a8\u529b\u5b66\u8868\u73b0\u4e3a\u590d\u5236\u5668\u6d41\u3002"}}
{"id": "2511.12979", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.12979", "abs": "https://arxiv.org/abs/2511.12979", "authors": ["Zhengchao Wang", "Yitao Hu", "Jianing Ye", "Zhuxuan Chang", "Jiazheng Yu", "Youpeng Deng", "Keqiu Li"], "title": "RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is a critical paradigm for building reliable, knowledge-intensive Large Language Model (LLM) applications. However, the multi-stage pipeline (retrieve, generate) and unique workload characteristics (e.g., knowledge dependency) of RAG systems pose significant challenges for serving performance optimization. Existing generic LLM inference traces fail to capture these RAG-specific dynamics, creating a significant performance gap between academic research and real-world deployment. To bridge this gap, this paper introduces RAGPulse, an open-source RAG workload trace dataset. This dataset was collected from an university-wide Q&A system serving that has served more than 40,000 students and faculties since April 2024. We detail RAGPulse's system architecture, its privacy-preserving hash-based data format, and provide an in-depth statistical analysis. Our analysis reveals that real-world RAG workloads exhibit significant temporal locality and a highly skewed hot document access pattern. RAGPulse provides a high-fidelity foundation for researchers to develop and validate novel optimization strategies for RAG systems, such as content-aware batching and retrieval caching, ultimately enhancing the efficiency and reliability of RAG services. The code is available at https://github.com/flashserve/RAGPulse.", "AI": {"tldr": "RAGPulse\u662f\u4e00\u4e2a\u5f00\u6e90\u7684RAG\u5de5\u4f5c\u8d1f\u8f7d\u8ddf\u8e2a\u6570\u636e\u96c6\uff0c\u65e8\u5728\u5f25\u5408\u5b66\u672f\u7814\u7a76\u4e0e\u5b9e\u9645\u90e8\u7f72\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u7684LLM\u63a8\u7406\u8ddf\u8e2a\u65e0\u6cd5\u6355\u6349RAG\u7279\u5b9a\u7684\u52a8\u6001\uff0c\u5bfc\u81f4\u6027\u80fd\u5dee\u8ddd\u3002", "method": "\u6536\u96c6\u6765\u81ea\u4e00\u4e2a\u670d\u52a1\u4e8e\u8d85\u8fc740,000\u540d\u5e08\u751f\u7684\u5927\u5b66\u8303\u56f4\u5185\u7684\u95ee\u7b54\u7cfb\u7edf\u7684RAG\u5de5\u4f5c\u8d1f\u8f7d\u8ddf\u8e2a\u6570\u636e\uff0c\u5e76\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\u3002", "result": "\u73b0\u5b9e\u4e16\u754c\u7684RAG\u5de5\u4f5c\u8d1f\u8f7d\u8868\u73b0\u51fa\u663e\u8457\u7684\u65f6\u95f4\u5c40\u90e8\u6027\u548c\u9ad8\u5ea6\u503e\u659c\u7684\u70ed\u95e8\u6587\u6863\u8bbf\u95ee\u6a21\u5f0f\u3002", "conclusion": "RAGPulse\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u4fdd\u771f\u57fa\u7840\uff0c\u4ee5\u5f00\u53d1\u548c\u9a8c\u8bc1RAG\u7cfb\u7edf\u7684\u4f18\u5316\u7b56\u7565\uff0c\u4ece\u800c\u63d0\u9ad8RAG\u670d\u52a1\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2511.11653", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11653", "abs": "https://arxiv.org/abs/2511.11653", "authors": ["Duolin Sun", "Meixiu Long", "Dan Yang", "Yihan Jiao", "Zhehao Tan", "Jie Feng", "Junjie Wang", "Yue Shen", "Peng Wei", "Jian Wang", "Jinjie Gu"], "title": "GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning", "comment": null, "summary": "Large Language Models have shown strong potential as rerankers to enhance the overall performance of RAG systems. However, existing reranking paradigms are constrained by a core theoretical and practical dilemma: Pointwise methods, while simple and highly flexible, evaluate documents independently, making them prone to the Ranking Myopia Trap, overlooking the relative importance between documents. In contrast, Listwise methods can perceive the global ranking context, but suffer from inherent List Rigidity, leading to severe scalability and flexibility issues when handling large candidate sets. To address these challenges, we propose Groupwise, a novel reranking paradigm. In this approach, the query and a group of candidate documents are jointly fed into the model, which performs within-group comparisons to assign individual relevance scores to each document. This design retains the flexibility of Pointwise methods while enabling the comparative capability of Listwise methods. We further adopt GRPO for model training, equipped with a heterogeneous reward function that integrates ranking metrics with a distributional reward aimed at aligning score distributions across groups. To overcome the bottleneck caused by the scarcity of high quality labeled data, we further propose an innovative pipeline for synthesizing high quality retrieval and ranking data. The resulting data can be leveraged not only for training the reranker but also for training the retriever. Extensive experiments validate the effectiveness of our approach. On two reasoning intensive retrieval benchmarks, BRIGHT and R2MED.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684reranking\u8303\u5f0fGroupwise\uff0c\u4ee5\u89e3\u51b3Pointwise\u548cListwise\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684reranking\u65b9\u6cd5\u5b58\u5728Pointwise\u7684\u6392\u5e8f\u77ed\u89c6\u548cListwise\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faGroupwise\u8303\u5f0f\uff0c\u5c06\u67e5\u8be2\u548c\u4e00\u7ec4\u5019\u9009\u6587\u6863\u8f93\u5165\u6a21\u578b\uff0c\u8fdb\u884c\u7ec4\u5185\u6bd4\u8f83\uff0c\u5e76\u91c7\u7528GRPO\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\uff0c\u7ed3\u5408\u6392\u5e8f\u6307\u6807\u548c\u5206\u5e03\u5956\u52b1\u3002", "result": "\u5728\u4e24\u4e2a\u63a8\u7406\u5bc6\u96c6\u578b\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5BRIGHT\u548cR2MED\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728reranking\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u80fd\u7528\u4e8e\u8bad\u7ec3\u68c0\u7d22\u5668\u3002"}}
{"id": "2511.11597", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11597", "abs": "https://arxiv.org/abs/2511.11597", "authors": ["Michelle Chen Huebscher", "Katharine Mach", "Aleksandar Stani\u0107", "Markus Leippold", "Ben Gaiarin", "Zeke Hausfather", "Elisa Rawat", "Erich Fischer", "Massimiliano Ciaramita", "Joeri Rogelj", "Christian Buck", "Lierni Sestorain Saralegui", "Reto Knutti"], "title": "CLINB: A Climate Intelligence Benchmark for Foundational Models", "comment": "Questions, system prompt and model judge prompts available here: https://www.kaggle.com/datasets/deepmind/clinb-questions", "summary": "Evaluating how Large Language Models (LLMs) handle complex, specialized knowledge remains a critical challenge. We address this through the lens of climate change by introducing CLINB, a benchmark that assesses models on open-ended, grounded, multimodal question answering tasks with clear requirements for knowledge quality and evidential support. CLINB relies on a dataset of real users' questions and evaluation rubrics curated by leading climate scientists. We implement and validate a model-based evaluation process and evaluate several frontier models. Our findings reveal a critical dichotomy. Frontier models demonstrate remarkable knowledge synthesis capabilities, often exhibiting PhD-level understanding and presentation quality. They outperform \"hybrid\" answers curated by domain experts assisted by weaker models. However, this performance is countered by failures in grounding. The quality of evidence varies, with substantial hallucination rates for references and images. We argue that bridging this gap between knowledge synthesis and verifiable attribution is essential for the deployment of AI in scientific workflows and that reliable, interpretable benchmarks like CLINB are needed to progress towards building trustworthy AI systems.", "AI": {"tldr": "CLINB\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5904\u7406\u6c14\u5019\u53d8\u5316\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u7684\u57fa\u51c6\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5904\u7406\u590d\u6742\u3001\u4e13\u4e1a\u7684\u77e5\u8bc6\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u672c\u6587\u901a\u8fc7\u6c14\u5019\u53d8\u5316\u7684\u89c6\u89d2\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u51c6CLINB\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u5f00\u653e\u5f0f\u3001\u57fa\u4e8e\u4e8b\u5b9e\u7684\u3001\u591a\u6a21\u6001\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5bf9\u77e5\u8bc6\u8d28\u91cf\u548c\u8bc1\u636e\u652f\u6301\u63d0\u51fa\u4e86\u660e\u786e\u7684\u8981\u6c42\u3002", "method": "CLINB\u4f9d\u8d56\u4e8e\u771f\u5b9e\u7528\u6237\u7684\u95ee\u9898\u6570\u636e\u96c6\u548c\u7531 \u0432\u0435\u0434\u0443\u0449\u0438\u0445 \u6c14\u5019\u79d1\u5b66\u5bb6\u7b56\u5212\u7684\u8bc4\u4f30\u6807\u51c6\u3002\u6211\u4eec\u5b9e\u65bd\u5e76\u9a8c\u8bc1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6a21\u578b\u7684\u8bc4\u4f30\u8fc7\u7a0b\uff0c\u5e76\u8bc4\u4f30\u4e86\u51e0\u4e2a\u524d\u6cbf\u6a21\u578b\u3002", "result": "\u524d\u6cbf\u6a21\u578b\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u77e5\u8bc6\u7efc\u5408\u80fd\u529b\uff0c\u901a\u5e38\u8868\u73b0\u51fa\u535a\u58eb\u6c34\u5e73\u7684\u7406\u89e3\u548c\u8868\u8fbe\u8d28\u91cf\uff0c\u5e76\u4e14\u4f18\u4e8e\u9886\u57df\u4e13\u5bb6\u5728\u8f83\u5f31\u6a21\u578b\u7684\u5e2e\u52a9\u4e0b\u7b56\u5212\u7684\u201c\u6df7\u5408\u201d\u7b54\u6848\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u6027\u80fd\u4e0e grounding \u5931\u8d25\u5f62\u6210\u5bf9\u6bd4\u3002\u8bc1\u636e\u7684\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\uff0c\u53c2\u8003\u6587\u732e\u548c\u56fe\u50cf\u7684\u5e7b\u89c9\u7387\u5f88\u9ad8\u3002", "conclusion": "\u5f25\u5408\u77e5\u8bc6\u7efc\u5408\u548c\u53ef\u9a8c\u8bc1\u5f52\u56e0\u4e4b\u95f4\u7684\u5dee\u8ddd\u5bf9\u4e8e\u4eba\u5de5\u667a\u80fd\u5728\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u4e14\u9700\u8981\u50cf CLINB \u8fd9\u6837\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u7684\u57fa\u51c6\u6765\u63a8\u52a8\u6784\u5efa\u503c\u5f97\u4fe1\u8d56\u7684 AI \u7cfb\u7edf\u3002"}}
{"id": "2511.11793", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11793", "abs": "https://arxiv.org/abs/2511.11793", "authors": ["MiroMind Team", "Song Bai", "Lidong Bing", "Carson Chen", "Guanzheng Chen", "Yuntao Chen", "Zhe Chen", "Ziyi Chen", "Jifeng Dai", "Xuan Dong", "Yue Deng", "Yunjie Fu", "Junqi Ge", "Chenxia Han", "Tammy Huang", "Zhenhang Huang", "Jerry Jiao", "Shilei Jiang", "Tianyu Jiao", "Xiaoqi Jian", "Lei Lei", "Ruilin Li", "Ryan Luo", "Tiantong Li", "Xiang Lin", "Ziyuan Liu", "Zhiqi Li", "Jie Ni", "Qiang Ren", "Pax Sun", "Shiqian Su", "Chenxin Tao", "Bin Wang", "Hellen Wang", "Haonan Wang", "James Wang", "Jin Wang", "Jojo Wang", "Letian Wang", "Shizun Wang", "Weizhi Wang", "Zixuan Wang", "Jinfan Xu", "Sen Xing", "Chenyu Yang", "Hai Ye", "Jiaheng Yu", "Yue Yu", "Muyan Zhong", "Tianchen Zhao", "Xizhou Zhu", "Yanpeng Zhou", "Yifan Zhang", "Zhi Zhu"], "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling", "comment": "Technical Report", "summary": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.", "AI": {"tldr": "MiroThinker v1.0\u662f\u4e00\u4e2a\u5f00\u6e90\u7814\u7a76\u4ee3\u7406\uff0c\u65e8\u5728\u901a\u8fc7\u4ea4\u4e92\u6269\u5c55\u6765\u63d0\u5347\u5de5\u5177\u589e\u5f3a\u63a8\u7406\u548c\u4fe1\u606f\u641c\u7d22\u80fd\u529b\u3002\u5b83\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u7684\u4ea4\u4e92\u6269\u5c55\uff0c\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u5f00\u6e90\u4ee3\u7406\uff0c\u5e76\u63a5\u8fd1\u4e86\u5546\u4e1a\u4ee3\u7406\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4ee3\u7406\u4e3b\u8981\u901a\u8fc7\u6269\u5927\u6a21\u578b\u5c3a\u5bf8\u6216\u4e0a\u4e0b\u6587\u957f\u5ea6\u6765\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u5ffd\u7565\u4e86\u4ea4\u4e92\u6269\u5c55\u7684\u91cd\u8981\u6027\u3002MiroThinker\u65e8\u5728\u63a2\u7d22\u6a21\u578b\u5c42\u9762\u7684\u4ea4\u4e92\u6269\u5c55\uff0c\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u5904\u7406\u66f4\u6df1\u5165\u3001\u66f4\u9891\u7e41\u7684agent-\u73af\u5883\u4ea4\u4e92\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3MiroThinker\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u66f4\u6df1\u5165\u3001\u66f4\u9891\u7e41\u7684agent-\u73af\u5883\u4ea4\u4e92\u3002\u8be5\u6a21\u578b\u5177\u6709256K\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u5e76\u4e14\u53ef\u4ee5\u6267\u884c\u9ad8\u8fbe600\u6b21\u7684\u5de5\u5177\u8c03\u7528\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c72B\u7248\u672c\u7684MiroThinker\u7684\u51c6\u786e\u7387\u5206\u522b\u8fbe\u523081.9%\u300137.7%\u300147.1%\u548c55.6%\uff0c\u8d85\u8fc7\u4e86\u4e4b\u524d\u7684\u5f00\u6e90\u4ee3\u7406\uff0c\u5e76\u63a5\u8fd1\u4e86GPT-4-high\u7b49\u5546\u4e1a\u4ee3\u7406\u3002", "conclusion": "\u4ea4\u4e92\u6269\u5c55\u662f\u6784\u5efa\u4e0b\u4e00\u4ee3\u5f00\u653e\u7814\u7a76\u4ee3\u7406\u7684\u7b2c\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\uff0c\u4e0e\u6a21\u578b\u5bb9\u91cf\u548c\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e92\u8865\u3002"}}
{"id": "2511.11643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11643", "abs": "https://arxiv.org/abs/2511.11643", "authors": ["Aswath Muthuselvam", "Jeevak Raj S", "Mohanaprasad K"], "title": "Real-time pothole detection with onboard sensors and camera on vehicles", "comment": null, "summary": "Road conditions play an important role in our everyday commute. With the proliferating number of vehicles on the road each year, it has become necessary to access the road conditions very frequently, this would ensure that the traffic also flows smoothly. Even the smallest crack in the road could be easily be chipped into a large pothole due to changing surface temperatures of the road and from the force of vehicles riding over it. In this paper, we have addressed how we could better identify these potholes in realtime with the help of onboard sensors in vehicles so that the data could be useful for analysis and better management of potholes on a large scale. For the implementation, we used an SVM classifier to detect potholes, we achieved 98.1% accuracy based on data collected from a local road for about 2 km which had 26 potholes distributed along the road. Code is available at: https://github.com/aswathselvam/Potholes", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u8f66\u8f86\u4f20\u611f\u5668\u5b9e\u65f6\u8bc6\u522b\u9053\u8def\u5751\u6d3c\u7684\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u5927\u89c4\u6a21\u7684\u5751\u6d3c\u5206\u6790\u548c\u7ba1\u7406\u3002", "motivation": "\u968f\u7740\u9053\u8def\u4e0a\u8f66\u8f86\u6570\u91cf\u7684\u589e\u52a0\uff0c\u9700\u8981\u66f4\u9891\u7e41\u5730\u83b7\u53d6\u8def\u51b5\u4fe1\u606f\uff0c\u4ee5\u786e\u4fdd\u4ea4\u901a\u987a\u7545\u3002\u5373\u4f7f\u662f\u9053\u8def\u4e0a\u6700\u5c0f\u7684\u88c2\u7f1d\uff0c\u4e5f\u53ef\u80fd\u56e0\u8def\u9762\u6e29\u5ea6\u53d8\u5316\u548c\u8f66\u8f86\u884c\u9a76\u7684\u538b\u529b\u800c\u53d8\u6210\u5927\u5751\u6d3c\u3002", "method": "\u4f7f\u7528SVM\u5206\u7c7b\u5668\u68c0\u6d4b\u5751\u6d3c\u3002", "result": "\u57fa\u4e8e\u4ece\u4e00\u6bb52\u516c\u91cc\u957f\u7684\u672c\u5730\u9053\u8def\u6536\u96c6\u7684\u6570\u636e\uff0c\u5b9e\u73b0\u4e8698.1%\u7684\u51c6\u786e\u7387\uff0c\u8be5\u9053\u8def\u4e0a\u670926\u4e2a\u5751\u6d3c\u3002", "conclusion": "\u5229\u7528\u8f66\u8f86\u4f20\u611f\u5668\u53ef\u4ee5\u6709\u6548\u5730\u5b9e\u65f6\u8bc6\u522b\u9053\u8def\u5751\u6d3c\uff0c\u4e3a\u5927\u89c4\u6a21\u5751\u6d3c\u5206\u6790\u548c\u7ba1\u7406\u63d0\u4f9b\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2511.11574", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11574", "abs": "https://arxiv.org/abs/2511.11574", "authors": ["Viviana Luccioli", "Rithika Iyengar", "Ryan Panley", "Flora Haberkorn", "Xiaoyu Ge", "Leland Crane", "Nitish Sinha", "Seung Jung Lee"], "title": "LLM on a Budget: Active Knowledge Distillation for Efficient Classification of Large Text Corpora", "comment": null, "summary": "Large Language Models (LLMs) are highly accurate in classification tasks, however, substantial computational and financial costs hinder their large-scale deployment in dynamic environments. Knowledge Distillation (KD) where a LLM \"teacher\" trains a smaller and more efficient \"student\" model, offers a promising solution to this problem. However, the distillation process itself often remains costly for large datasets, since it requires the teacher to label a vast number of samples while incurring significant token consumption. To alleviate this challenge, in this work we explore the active learning (AL) as a way to create efficient student models at a fraction of the cost while preserving the LLM's performance. In particular, we introduce M-RARU (Multi-class Randomized Accept/Reject Uncertainty Sampling), a novel AL algorithm that significantly reduces training costs. M-RARU employs an innovative strategy combining uncertainty with a randomized accept-reject mechanism to select only the most informative data points for the LLM teacher. This focused approach significantly minimizes required API calls and data processing time. We evaluate M-RARU against random sampling across five diverse student models (SVM, LDA, RF, GBDT, and DistilBERT) on multiple benchmark datasets. Experiments demonstrate that our proposed method achieves up to 80% reduction in sample requirements as compared to random sampling, substantially improving classification accuracy while reducing financial costs and overall training time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5M-RARU\uff0c\u7528\u4e8e\u964d\u4f4e\u77e5\u8bc6\u84b8\u998f\u8fc7\u7a0b\u4e2dLLM\u6807\u6ce8\u6570\u636e\u7684\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8ba1\u7b97\u548c\u7ecf\u6d4e\u6210\u672c\u963b\u788d\u4e86\u5b83\u4eec\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5927\u89c4\u6a21\u90e8\u7f72\u3002\u77e5\u8bc6\u84b8\u998f\u662f\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4e3a\u5927\u578b\u6570\u636e\u96c6\u8fdb\u884c\u84b8\u998f\u7684\u6210\u672c\u4ecd\u7136\u5f88\u9ad8\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u6559\u5e08\u6a21\u578b\u6807\u6ce8\u5927\u91cf\u6837\u672c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aM-RARU\u7684\u65b0\u578b\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u548c\u968f\u673a\u63a5\u53d7/\u62d2\u7edd\u673a\u5236\uff0c\u9009\u62e9\u4fe1\u606f\u91cf\u6700\u5927\u7684\u6570\u636e\u70b9\u4f9bLLM\u6559\u5e08\u6a21\u578b\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u968f\u673a\u62bd\u6837\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u53ef\u51cf\u5c11\u9ad8\u8fbe80%\u7684\u6837\u672c\u9700\u6c42\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u5206\u7c7b\u7cbe\u5ea6\uff0c\u964d\u4f4e\u7ecf\u6d4e\u6210\u672c\u548c\u6574\u4f53\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "M-RARU\u7b97\u6cd5\u80fd\u591f\u4ee5\u8f83\u4f4e\u7684\u6210\u672c\u521b\u5efa\u9ad8\u6548\u7684\u5b66\u751f\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301LLM\u7684\u6027\u80fd\u3002"}}
{"id": "2511.13418", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13418", "abs": "https://arxiv.org/abs/2511.13418", "authors": ["Allaa Boutaleb", "Bernd Amann", "Rafael Angarita", "Hubert Naacke"], "title": "Exploring Multi-Table Retrieval Through Iterative Search", "comment": "Accepted @ the AI for Tabular Data Workshop, EurIPS 2025", "summary": "Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u8868\u68c0\u7d22\u7684\u8fed\u4ee3\u641c\u7d22\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u53ef\u6269\u5c55\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u7075\u6d3b\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002", "motivation": "\u5728\u6570\u636e\u6e56\u4e0a\u8fdb\u884c\u5f00\u653e\u57df\u95ee\u7b54\u9700\u8981\u4ece\u591a\u4e2a\u8868\u4e2d\u68c0\u7d22\u548c\u7ec4\u5408\u4fe1\u606f\uff0c\u8fd9\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u5b50\u4efb\u52a1\uff0c\u9700\u8981\u8bed\u4e49\u76f8\u5173\u6027\u548c\u7ed3\u6784\u8fde\u8d2f\u6027\uff08\u4f8b\u5982\uff0c\u53ef\u8fde\u63a5\u6027\uff09\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u6846\u67b6\u548c\u4e00\u4e2a\u5177\u4f53\u7684\u5b9e\u4f8b\uff1a\u4e00\u4e2a\u5feb\u901f\u3001\u6709\u6548\u7684\u8d2a\u5a6a\u8fde\u63a5\u611f\u77e5\u68c0\u7d22\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u6574\u4f53\u4e0a\u5e73\u8861\u4e86\u76f8\u5173\u6027\u3001\u8986\u76d6\u7387\u548c\u53ef\u8fde\u63a5\u6027\u3002", "result": "\u57285\u4e2aNL2SQL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u4e8eMIP\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u8fed\u4ee3\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u68c0\u7d22\u6027\u80fd\uff0c\u540c\u65f6\u901f\u5ea6\u63d0\u9ad8\u4e864-400\u500d\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u57fa\u51c6\u548c\u641c\u7d22\u7a7a\u95f4\u8bbe\u7f6e\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u7a81\u51fa\u4e86\u8fed\u4ee3\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u5b9e\u9645\u3001\u53ef\u6269\u5c55\u548c\u7ec4\u5408\u611f\u77e5\u68c0\u7d22\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.11847", "categories": ["cs.IR", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11847", "abs": "https://arxiv.org/abs/2511.11847", "authors": ["Ryan Singh", "Austin Hamilton", "Amanda White", "Michael Wise", "Ibrahim Yousif", "Arthur Carvalho", "Zhe Shan", "Reza Abrisham Baf", "Mohammad Mayyas", "Lora A. Cavuoto", "Fadel M. Megahed"], "title": "A Multimodal Manufacturing Safety Chatbot: Knowledge Base Design, Benchmark Development, and Evaluation of Multiple RAG Approaches", "comment": "25 pages, 5 figures", "summary": "Ensuring worker safety remains a critical challenge in modern manufacturing environments. Industry 5.0 reorients the prevailing manufacturing paradigm toward more human-centric operations. Using a design science research methodology, we identify three essential requirements for next-generation safety training systems: high accuracy, low latency, and low cost. We introduce a multimodal chatbot powered by large language models that meets these design requirements. The chatbot uses retrieval-augmented generation to ground its responses in curated regulatory and technical documentation. To evaluate our solution, we developed a domain-specific benchmark of expert-validated question and answer pairs for three representative machines: a Bridgeport manual mill, a Haas TL-1 CNC lathe, and a Universal Robots UR5e collaborative robot. We tested 24 RAG configurations using a full-factorial design and assessed them with automated evaluations of correctness, latency, and cost. Our top 2 configurations were then evaluated by ten industry experts and academic researchers. Our results show that retrieval strategy and model configuration have a significant impact on performance. The top configuration (selected for chatbot deployment) achieved an accuracy of 86.66%, an average latency of 10.04 seconds, and an average cost of $0.005 per query. Overall, our work provides three contributions: an open-source, domain-grounded safety training chatbot; a validated benchmark for evaluating AI-assisted safety instruction; and a systematic methodology for designing and assessing AI-enabled instructional and immersive safety training systems for Industry 5.0 environments.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u804a\u5929\u673a\u5668\u4eba\uff0c\u7528\u4e8e\u4e0b\u4e00\u4ee3\u5b89\u5168\u57f9\u8bad\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5177\u6709\u9ad8\u51c6\u786e\u6027\u3001\u4f4e\u5ef6\u8fdf\u548c\u4f4e\u6210\u672c\u3002", "motivation": "\u786e\u4fdd\u5de5\u4eba\u5b89\u5168\u662f\u73b0\u4ee3\u5236\u9020\u4e1a\u73af\u5883\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u5de5\u4e1a 5.0 \u5c06\u4e3b\u8981\u7684\u5236\u9020\u6a21\u5f0f\u91cd\u65b0\u8c03\u6574\u4e3a\u66f4\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u64cd\u4f5c\u3002", "method": "\u4f7f\u7528\u8bbe\u8ba1\u79d1\u5b66\u7814\u7a76\u65b9\u6cd5\uff0c\u786e\u5b9a\u4e86\u4e0b\u4e00\u4ee3\u5b89\u5168\u57f9\u8bad\u7cfb\u7edf\u7684\u4e09\u4e2a\u57fa\u672c\u8981\u6c42\uff1a\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u5ef6\u8fdf\u548c\u4f4e\u6210\u672c\u3002\u804a\u5929\u673a\u5668\u4eba\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff0c\u4f7f\u5176\u54cd\u5e94\u57fa\u4e8e\u7cbe\u9009\u7684\u76d1\u7ba1\u548c\u6280\u672f\u6587\u6863\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u68c0\u7d22\u7b56\u7565\u548c\u6a21\u578b\u914d\u7f6e\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002\u6700\u4f73\u914d\u7f6e\u7684\u51c6\u786e\u7387\u4e3a 86.66%\uff0c\u5e73\u5747\u5ef6\u8fdf\u4e3a 10.04 \u79d2\uff0c\u6bcf\u6b21\u67e5\u8be2\u7684\u5e73\u5747\u6210\u672c\u4e3a 0.005 \u7f8e\u5143\u3002", "conclusion": "\u603b\u7684\u6765\u8bf4\uff0c\u8fd9\u9879\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e09\u4e2a\u8d21\u732e\uff1a\u4e00\u4e2a\u5f00\u6e90\u7684\u3001\u57fa\u4e8e\u9886\u57df\u7684\u5b89\u5168\u57f9\u8bad\u804a\u5929\u673a\u5668\u4eba\uff1b\u4e00\u4e2a\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\u8f85\u52a9\u5b89\u5168\u6307\u5bfc\uff1b\u4ee5\u53ca\u4e00\u4e2a\u7cfb\u7edf\u7684\u8bbe\u8ba1\u548c\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\u652f\u6301\u7684\u5de5\u4e1a 5.0 \u73af\u5883\u6559\u5b66\u548c\u6c89\u6d78\u5f0f\u5b89\u5168\u57f9\u8bad\u7cfb\u7edf\u7684\u65b9\u6cd5\u3002"}}
{"id": "2511.11599", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11599", "abs": "https://arxiv.org/abs/2511.11599", "authors": ["Arefeh Kazemi", "Hamza Qadeer", "Joachim Wagner", "Hossein Hosseini", "Sri Balaaji Natarajan Kalaivendan", "Brian Davis"], "title": "SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detectio", "comment": null, "summary": "We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification.", "AI": {"tldr": "SynBullying: A synthetic dataset for cyberbullying detection using multi-LLM conversations, offering a scalable and ethically safe alternative to human data.", "motivation": "Addresses the need for scalable and ethically safe cyberbullying data by using LLMs to simulate realistic interactions.", "method": "LLMs are used to generate conversational data with context-aware and fine-grained labels for different cyberbullying categories.", "result": "The dataset's utility is evaluated across five dimensions and tested for performance in cyberbullying classification, both as standalone training data and for augmentation.", "conclusion": "Introduces SynBullying, a synthetic dataset for cyberbullying research, offering a structured, context-aware, and fine-grained resource for studying and detecting cyberbullying."}}
{"id": "2511.11810", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11810", "abs": "https://arxiv.org/abs/2511.11810", "authors": ["Bertram H\u00f8jer"], "title": "On the Notion that Language Models Reason", "comment": "Accepted at the 1st Workshop on Epistemic Intelligence in Machine Learning, EurIPS 2025", "summary": "Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \\textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are \"statistical pattern matchers\"\" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.", "AI": {"tldr": "\u8bba\u6587 \u043a\u0440\u0438\u0442\u0438\u043a\u0443\u0435\u0442 \u0443\u0442\u0432\u0435\u0440\u0436\u0434\u0435\u043d\u0438\u044f \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u044f\u0437\u044b\u043a\u043e\u0432\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u0443\u044e\u0442 \u0440\u0430\u0441\u0441\u0443\u0436\u0434\u0435\u043d\u0438\u044f, \u043f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u0438\u0445 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0438 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u043d\u0435 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f\u043c \u0440\u0430\u0441\u0441\u0443\u0436\u0434\u0435\u043d\u0438\u0439.", "motivation": "\u041e\u0446\u0435\u043d\u043a\u0430 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0439 \u0440\u0430\u0441\u0441\u0443\u0436\u0434\u0435\u043d\u0438\u0439 \u0438 \u0442\u043e\u0433\u043e, \u043a\u0430\u043a \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u0435 \u0441\u0442\u0430\u0442\u044c\u0438 \u0432 \u043e\u0431\u043b\u0430\u0441\u0442\u0438 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 (NLP) \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442 \u044d\u0442\u043e \u043f\u043e\u043d\u044f\u0442\u0438\u0435, \u0438 \u0443\u0442\u0432\u0435\u0440\u0436\u0434\u0435\u043d\u0438\u0435 \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0435 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043d\u0435 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u0442\u043e\u043c\u0443, \u043a\u0430\u043a \u043e\u0431\u0443\u0447\u0430\u044e\u0442\u0441\u044f \u044f\u0437\u044b\u043a\u043e\u0432\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438, \u043e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u044e\u0442 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u0438 \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u044e\u0442 \u043d\u043e\u0432\u044b\u0435 \u0442\u043e\u043a\u0435\u043d\u044b.", "method": "\u041f\u0440\u0435\u0434\u043f\u043e\u043b\u0430\u0433\u0430\u0435\u0442\u0441\u044f, \u0447\u0442\u043e \u044f\u0437\u044b\u043a\u043e\u0432\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440\u043e\u0432 \u0440\u0435\u0430\u043b\u0438\u0437\u0443\u044e\u0442 \\textit{\u043d\u0435\u044f\u0432\u043d\u043e\u0435} \u044f\u0434\u0440\u043e \u041c\u0430\u0440\u043a\u043e\u0432\u0430 \u043a\u043e\u043d\u0435\u0447\u043d\u043e\u0433\u043e \u043f\u043e\u0440\u044f\u0434\u043a\u0430, \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0430\u044e\u0449\u0435\u0435 \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442\u044b \u0432 \u0443\u0441\u043b\u043e\u0432\u043d\u044b\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0442\u043e\u043a\u0435\u043d\u043e\u0432. \u0412 \u044d\u0442\u043e\u043c \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0438, \u0432\u044b\u0432\u043e\u0434\u044b, \u043f\u043e\u0445\u043e\u0436\u0438\u0435 \u043d\u0430 \u0440\u0430\u0441\u0441\u0443\u0436\u0434\u0435\u043d\u0438\u044f, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u043c \u0437\u0430\u043a\u043e\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044f\u043c \u0438 \u043f\u0440\u0438\u0431\u043b\u0438\u0437\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u043c \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u043c \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430\u043c \u0432 \u0438\u0437\u0443\u0447\u0435\u043d\u043d\u043e\u043c \u044f\u0434\u0440\u0435, \u0430 \u043d\u0435 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u044f\u0432\u043d\u044b\u0445 \u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043c\u0435\u0445\u0430\u043d\u0438\u0437\u043c\u043e\u0432.", "result": "\u0412\u044b\u0432\u043e\u0434\u044b, \u043f\u043e\u0445\u043e\u0436\u0438\u0435 \u043d\u0430 \u0440\u0430\u0441\u0441\u0443\u0436\u0434\u0435\u043d\u0438\u044f, \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u043c \u0437\u0430\u043a\u043e\u043d\u043e\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044f\u043c \u0438 \u043f\u0440\u0438\u0431\u043b\u0438\u0437\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u043c \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u043c \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430\u043c \u0432 \u0438\u0437\u0443\u0447\u0435\u043d\u043d\u043e\u043c \u044f\u0434\u0440\u0435, \u0430 \u043d\u0435 \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u044f\u0432\u043d\u044b\u0445 \u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u043c\u0435\u0445\u0430\u043d\u0438\u0437\u043c\u043e\u0432. \u042d\u0442\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0438\u043b\u043b\u044e\u0441\u0442\u0440\u0438\u0440\u0443\u0435\u0442 \u0443\u0442\u0432\u0435\u0440\u0436\u0434\u0435\u043d\u0438\u0435 \u043e \u0442\u043e\u043c, \u0447\u0442\u043e \u044f\u0437\u044b\u043a\u043e\u0432\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \\", "conclusion": "\u0420\u0430\u0437\u043b\u0438\u0447\u0438\u0435 \u0438\u043c\u0435\u0435\u0442 \u043e\u0441\u043d\u043e\u0432\u043e\u043f\u043e\u043b\u0430\u0433\u0430\u044e\u0449\u0435\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0434\u043b\u044f \u0442\u043e\u0433\u043e, \u043a\u0430\u043a \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u044d\u043f\u0438\u0441\u0442\u0435\u043c\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043d\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0432 \u044f\u0437\u044b\u043a\u043e\u0432\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u044f\u0445. \u041f\u0440\u0438\u0433\u043b\u0430\u0448\u0430\u0435\u043c \u043a \u043e\u0431\u0441\u0443\u0436\u0434\u0435\u043d\u0438\u044e \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u0442\u043e\u0433\u043e, \u043a\u0430\u043a \u043e\u043f\u0438\u0441\u044b\u0432\u0430\u044e\u0442\u0441\u044f \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u044b \u0441\u0438\u0441\u0442\u0435\u043c, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u044b \u0441\u0442\u0440\u043e\u0438\u043c \u0438 \u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u0432 \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u044f\u0445 NLP."}}
{"id": "2511.11659", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11659", "abs": "https://arxiv.org/abs/2511.11659", "authors": ["Kesong Zheng", "Zhi Song", "Peizhou Li", "Shuyi Yao", "Zhenxing Bian"], "title": "A Method for Identifying Farmland System Habitat Types Based on the Dynamic-Weighted Feature Fusion Network Model", "comment": "30 pages,12 figures", "summary": "Addressing the current lack of a standardized habitat classification system for cultivated land ecosystems, incomplete coverage of habitat types, and the inability of existing models to effectively integrate semantic and texture features-resulting in insufficient segmentation accuracy and blurred boundaries for multi-scale habitats (e.g., large-scale field plots and micro-habitats)-this study developed a comprehensively annotated ultra-high-resolution remote sensing image dataset encompassing 15 categories of cultivated land system habitats. Furthermore, we propose a Dynamic-Weighted Feature Fusion Network (DWFF-Net). The encoder of this model utilizes a frozen-parameter DINOv3 to extract foundational features. By analyzing the relationships between different category images and feature maps, we introduce a data-level adaptive dynamic weighting strategy for feature fusion. The decoder incorporates a dynamic weight computation network to achieve thorough integration of multi-layer features, and a hybrid loss function is adopted to optimize model training. Experimental results on the constructed dataset demonstrate that the proposed model achieves a mean Intersection over Union (mIoU) of 0.6979 and an F1-score of 0.8049, outperforming the baseline network by 0.021 and 0.0161, respectively. Ablation studies further confirm the complementary nature of multi-layer feature fusion, which effectively improves the IoU for micro-habitat categories such as field ridges. This study establishes a habitat identification framework for cultivated land systems based on adaptive multi-layer feature fusion, enabling sub-meter precision habitat mapping at a low cost and providing robust technical support for fine-grained habitat monitoring in cultivated landscapes.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u8015\u5730\u751f\u6001\u7cfb\u7edf\u6816\u606f\u5730\u5206\u7c7b\u6807\u51c6\u7f3a\u5931\u3001\u6816\u606f\u5730\u7c7b\u578b\u8986\u76d6\u4e0d\u5168\u4ee5\u53ca\u73b0\u6709\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u6574\u5408\u8bed\u4e49\u548c\u7eb9\u7406\u7279\u5f81\u7b49\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u52a8\u6001\u52a0\u6743\u7279\u5f81\u878d\u5408\u7f51\u7edc\uff08DWFF-Net\uff09\uff0c\u7528\u4e8e\u63d0\u9ad8\u591a\u5c3a\u5ea6\u6816\u606f\u5730\u7684\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u8015\u5730\u751f\u6001\u7cfb\u7edf\u6816\u606f\u5730\u7f3a\u4e4f\u6807\u51c6\u5316\u5206\u7c7b\u7cfb\u7edf\uff0c\u6816\u606f\u5730\u7c7b\u578b\u8986\u76d6\u4e0d\u5168\uff0c\u73b0\u6709\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u6574\u5408\u8bed\u4e49\u548c\u7eb9\u7406\u7279\u5f81\uff0c\u5bfc\u81f4\u5206\u5272\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u591a\u5c3a\u5ea6\u6816\u606f\u5730\u8fb9\u754c\u6a21\u7cca\u3002", "method": "1. \u6784\u5efa\u5305\u542b15\u7c7b\u8015\u5730\u7cfb\u7edf\u6816\u606f\u5730\u7684\u8d85\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u56fe\u50cf\u6570\u636e\u96c6\u3002\n2. \u63d0\u51fa\u52a8\u6001\u52a0\u6743\u7279\u5f81\u878d\u5408\u7f51\u7edc\uff08DWFF-Net\uff09\uff0c\u5229\u7528\u51bb\u7ed3\u53c2\u6570\u7684DINOv3\u63d0\u53d6\u57fa\u7840\u7279\u5f81\uff0c\u5f15\u5165\u6570\u636e\u7ea7\u81ea\u9002\u5e94\u52a8\u6001\u52a0\u6743\u7b56\u7565\u8fdb\u884c\u7279\u5f81\u878d\u5408\uff0c\u5e76\u91c7\u7528\u6df7\u5408\u635f\u5931\u51fd\u6570\u4f18\u5316\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5728\u6784\u5efa\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6a21\u578b\u5b9e\u73b0\u4e860.6979\u7684\u5e73\u5747\u4ea4\u5e76\u6bd4\uff08mIoU\uff09\u548c0.8049\u7684F1\u5206\u6570\uff0c\u4f18\u4e8e\u57fa\u7ebf\u7f51\u7edc\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u591a\u5c42\u7279\u5f81\u878d\u5408\u7684\u4e92\u8865\u6027\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u7530\u57c2\u7b49\u5fae\u6816\u606f\u5730\u7c7b\u522b\u7684IoU\u3002", "conclusion": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u57fa\u4e8e\u81ea\u9002\u5e94\u591a\u5c42\u7279\u5f81\u878d\u5408\u7684\u8015\u5730\u7cfb\u7edf\u6816\u606f\u5730\u8bc6\u522b\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u4e9a\u7c73\u7ea7\u7cbe\u5ea6\u7684\u6816\u606f\u5730 mapping\uff0c\u4e3a\u8015\u5730\u666f\u89c2\u4e2d\u7684\u7cbe\u7ec6\u6816\u606f\u5730\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2511.11575", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11575", "abs": "https://arxiv.org/abs/2511.11575", "authors": ["Animesh Joshi"], "title": "Detecting Statistically Significant Fairness Violations in Recidivism Forecasting Algorithms", "comment": "8 pages", "summary": "Machine learning algorithms are increasingly deployed in critical domains such as finance, healthcare, and criminal justice [1]. The increasing popularity of algorithmic decision-making has stimulated interest in algorithmic fairness within the academic community. Researchers have introduced various fairness definitions that quantify disparities between privileged and protected groups, use causal inference to determine the impact of race on model predictions, and that test calibration of probability predictions from the model. Existing literature does not provide a way in which to assess whether observed disparities between groups are statistically significant or merely due to chance. This paper introduces a rigorous framework for testing the statistical significance of fairness violations by leveraging k-fold cross-validation [2] to generate sampling distributions of fairness metrics. This paper introduces statistical tests that can be used to identify statistically significant violations of fairness metrics based on disparities between predicted and actual outcomes, model calibration, and causal inference techniques [1]. We demonstrate this approach by testing recidivism forecasting algorithms trained on data from the National Institute of Justice. Our findings reveal that machine learning algorithms used for recidivism forecasting exhibit statistically significant bias against Black individuals under several fairness definitions, while also exhibiting no bias or bias against White individuals under other definitions. The results from this paper underscore the importance of rigorous and robust statistical testing while evaluating algorithmic decision-making systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e25\u683c\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6d4b\u8bd5\u516c\u5e73\u6027\u8fdd\u89c4\u884c\u4e3a\u7684\u7edf\u8ba1\u663e\u7740\u6027\uff0c\u5229\u7528k-fold\u4ea4\u53c9\u9a8c\u8bc1\u6765\u751f\u6210\u516c\u5e73\u6027\u6307\u6807\u7684\u62bd\u6837\u5206\u5e03\u3002", "motivation": "\u7b97\u6cd5\u51b3\u7b56\u5728\u5173\u952e\u9886\u57df\u7684\u65e5\u76ca\u666e\u53ca\u6fc0\u53d1\u4e86\u5b66\u672f\u754c\u5bf9\u7b97\u6cd5\u516c\u5e73\u6027\u7684\u5174\u8da3\u3002\u73b0\u6709\u7684\u6587\u732e\u6ca1\u6709\u63d0\u4f9b\u4e00\u79cd\u65b9\u6cd5\u6765\u8bc4\u4f30\u89c2\u5bdf\u5230\u7684\u7fa4\u4f53\u4e4b\u95f4\u7684\u5dee\u5f02\u662f\u5426\u5177\u6709\u7edf\u8ba1\u5b66\u610f\u4e49\uff0c\u6216\u8005\u4ec5\u4ec5\u662f\u7531\u4e8e\u5076\u7136\u6027\u3002", "method": "\u5229\u7528k-fold\u4ea4\u53c9\u9a8c\u8bc1\u6765\u751f\u6210\u516c\u5e73\u6027\u6307\u6807\u7684\u62bd\u6837\u5206\u5e03\uff0c\u4ece\u800c\u6d4b\u8bd5\u516c\u5e73\u6027\u8fdd\u89c4\u884c\u4e3a\u7684\u7edf\u8ba1\u663e\u7740\u6027\u3002", "result": "\u5bf9\u7d2f\u72af\u9884\u6d4b\u7b97\u6cd5\u7684\u6d4b\u8bd5\u8868\u660e\uff0c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5728\u51e0\u79cd\u516c\u5e73\u6027\u5b9a\u4e49\u4e0b\uff0c\u5bf9\u9ed1\u4eba\u4e2a\u4f53\u8868\u73b0\u51fa\u5177\u6709\u7edf\u8ba1\u610f\u4e49\u7684\u504f\u5dee\uff0c\u800c\u5728\u5176\u4ed6\u5b9a\u4e49\u4e0b\uff0c\u5bf9\u767d\u4eba\u4e2a\u4f53\u6ca1\u6709\u504f\u5dee\u6216\u6709\u504f\u5dee\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u5728\u8bc4\u4f30\u7b97\u6cd5\u51b3\u7b56\u7cfb\u7edf\u65f6\u8fdb\u884c\u4e25\u683c\u548c\u7a33\u5065\u7684\u7edf\u8ba1\u6d4b\u8bd5\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.12004", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12004", "abs": "https://arxiv.org/abs/2511.12004", "authors": ["Ganlin Xu", "Zhitao Yin", "Linghao Zhang", "Jiaqing Liang", "Weijia Lu", "Xiaodong Zhang", "Zhifei Yang", "Sihang Jiang", "Deqing Yang"], "title": "ComLQ: Benchmarking Complex Logical Queries in Information Retrieval", "comment": "Accepted by AAAI 2026", "summary": "Information retrieval (IR) systems play a critical role in navigating information overload across various applications. Existing IR benchmarks primarily focus on simple queries that are semantically analogous to single- and multi-hop relations, overlooking \\emph{complex logical queries} involving first-order logic operations such as conjunction ($\\land$), disjunction ($\\lor$), and negation ($\\lnot$). Thus, these benchmarks can not be used to sufficiently evaluate the performance of IR models on complex queries in real-world scenarios. To address this problem, we propose a novel method leveraging large language models (LLMs) to construct a new IR dataset \\textbf{ComLQ} for \\textbf{Com}plex \\textbf{L}ogical \\textbf{Q}ueries, which comprises 2,909 queries and 11,251 candidate passages. A key challenge in constructing the dataset lies in capturing the underlying logical structures within unstructured text. Therefore, by designing the subgraph-guided prompt with the subgraph indicator, an LLM (such as GPT-4o) is guided to generate queries with specific logical structures based on selected passages. All query-passage pairs in ComLQ are ensured \\emph{structure conformity} and \\emph{evidence distribution} through expert annotation. To better evaluate whether retrievers can handle queries with negation, we further propose a new evaluation metric, \\textbf{Log-Scaled Negation Consistency} (\\textbf{LSNC@$K$}). As a supplement to standard relevance-based metrics (such as nDCG and mAP), LSNC@$K$ measures whether top-$K$ retrieved passages violate negation conditions in queries. Our experimental results under zero-shot settings demonstrate existing retrieval models' limited performance on complex logical queries, especially on queries with negation, exposing their inferior capabilities of modeling exclusion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aComLQ\u7684\u65b0IR\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30IR\u6a21\u578b\u5728\u590d\u6742\u903b\u8f91\u67e5\u8be2\u4e0a\u7684\u6027\u80fd\uff0c\u8fd9\u4e9b\u67e5\u8be2\u6d89\u53ca\u4e00\u9636\u903b\u8f91\u8fd0\u7b97\uff0c\u5982\u5408\u53d6\u3001\u6790\u53d6\u548c\u5426\u5b9a\u3002", "motivation": "\u73b0\u6709\u7684IR\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u7b80\u5355\u7684\u67e5\u8be2\uff0c\u5ffd\u7565\u4e86\u6d89\u53ca\u4e00\u9636\u903b\u8f91\u8fd0\u7b97\u7684\u590d\u6742\u903b\u8f91\u67e5\u8be2\u3002\u56e0\u6b64\uff0c\u8fd9\u4e9b\u57fa\u51c6\u4e0d\u80fd\u5145\u5206\u8bc4\u4f30IR\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u590d\u6742\u67e5\u8be2\u4e0a\u7684\u6027\u80fd\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6784\u5efaComLQ\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5e26\u6709\u5b50\u56fe\u6307\u793a\u5668\u7684\u5b50\u56fe\u5f15\u5bfc\u63d0\u793a\uff0c\u5f15\u5bfcLLM\u57fa\u4e8e\u9009\u5b9a\u7684\u6bb5\u843d\u751f\u6210\u5177\u6709\u7279\u5b9a\u903b\u8f91\u7ed3\u6784\u7684\u67e5\u8be2\u3002\u901a\u8fc7\u4e13\u5bb6\u6ce8\u91ca\u786e\u4fddComLQ\u4e2d\u6240\u6709\u67e5\u8be2-\u6bb5\u843d\u5bf9\u7684\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u8bc1\u636e\u5206\u5e03\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u5373\u5bf9\u6570\u7f29\u653e\u5426\u5b9a\u4e00\u81f4\u6027\uff08LSNC@$K$\uff09\uff0c\u4ee5\u66f4\u597d\u5730\u8bc4\u4f30\u68c0\u7d22\u5668\u662f\u5426\u53ef\u4ee5\u5904\u7406\u5e26\u6709\u5426\u5b9a\u7684\u67e5\u8be2\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709\u7684\u68c0\u7d22\u6a21\u578b\u5728\u590d\u6742\u903b\u8f91\u67e5\u8be2\u4e0a\u7684\u6027\u80fd\u6709\u9650\uff0c\u5c24\u5176\u662f\u5728\u5e26\u6709\u5426\u5b9a\u7684\u67e5\u8be2\u4e0a\uff0c\u66b4\u9732\u4e86\u5b83\u4eec\u5728\u6392\u9664\u5efa\u6a21\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "conclusion": "ComLQ\u6570\u636e\u96c6\u548cLSNC@$K$\u6307\u6807\u53ef\u4ee5\u6709\u6548\u5730\u8bc4\u4f30IR\u6a21\u578b\u5728\u590d\u6742\u903b\u8f91\u67e5\u8be2\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u5426\u5b9a\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.11600", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.11600", "abs": "https://arxiv.org/abs/2511.11600", "authors": ["Piyushkumar Patel"], "title": "CausalGuard: A Smart System for Detecting and Preventing False Information in Large Language Models", "comment": null, "summary": "While large language models have transformed how we interact with AI systems, they have a critical weakness: they confidently state false information that sounds entirely plausible. This \"hallucination\" problem has become a major barrier to using these models where accuracy matters most. Existing solutions either require retraining the entire model, add significant computational costs, or miss the root causes of why these hallucinations occur in the first place.\n  We present CausalGuard, a new approach that combines causal reasoning with symbolic logic to catch and prevent hallucinations as they happen. Unlike previous methods that only check outputs after generation, our system understands the causal chain that leads to false statements and intervenes early in the process. CausalGuard works through two complementary paths: one that traces causal relationships between what the model knows and what it generates, and another that checks logical consistency using automated reasoning.\n  Testing across twelve different benchmarks, we found that CausalGuard correctly identifies hallucinations 89.3\\% of the time while missing only 8.3\\% of actual hallucinations. More importantly, it reduces false claims by nearly 80\\% while keeping responses natural and helpful. The system performs especially well on complex reasoning tasks where multiple steps of logic are required. Because CausalGuard shows its reasoning process, it works well in sensitive areas like medical diagnosis or financial analysis where understanding why a decision was made matters as much as the decision itself.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f1a\u81ea\u4fe1\u5730\u9648\u8ff0\u542c\u8d77\u6765\u5b8c\u5168\u5408\u7406\u7684\u865a\u5047\u4fe1\u606f\uff0c\u5373\u4ea7\u751f\u5e7b\u89c9\u3002CausalGuard\u7ed3\u5408\u4e86\u56e0\u679c\u63a8\u7406\u548c\u7b26\u53f7\u903b\u8f91\u6765\u6355\u83b7\u5e76\u9632\u6b62\u5e7b\u89c9\u7684\u4ea7\u751f\u3002", "motivation": "\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u6216\u8005\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6574\u4e2a\u6a21\u578b\uff0c\u6216\u8005\u589e\u52a0\u5927\u91cf\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u6216\u8005\u6ca1\u6709\u6293\u4f4f\u5e7b\u89c9\u4ea7\u751f\u7684\u6839\u672c\u539f\u56e0\u3002", "method": "CausalGuard\u901a\u8fc7\u4e24\u6761\u4e92\u8865\u7684\u8def\u5f84\u5de5\u4f5c\uff1a\u4e00\u6761\u8ffd\u6eaf\u6a21\u578b\u5df2\u77e5\u5185\u5bb9\u548c\u751f\u6210\u5185\u5bb9\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u53e6\u4e00\u6761\u4f7f\u7528\u81ea\u52a8\u63a8\u7406\u68c0\u67e5\u903b\u8f91\u4e00\u81f4\u6027\u3002", "result": "\u5728\u5341\u4e8c\u4e2a\u4e0d\u540c\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCausalGuard\u6b63\u786e\u8bc6\u522b\u5e7b\u89c9\u7684\u6982\u7387\u4e3a89.3\uff05\uff0c\u540c\u65f6\u4ec5\u9057\u6f0f\u4e868.3\uff05\u7684\u5b9e\u9645\u5e7b\u89c9\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u5b83\u53ef\u4ee5\u51cf\u5c11\u8fd180\uff05\u7684\u865a\u5047\u58f0\u660e\uff0c\u540c\u65f6\u4fdd\u6301\u54cd\u5e94\u7684\u81ea\u7136\u6027\u548c\u5e2e\u52a9\u6027\u3002", "conclusion": "CausalGuard\u5728\u9700\u8981\u591a\u6b65\u9aa4\u903b\u8f91\u7684\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728\u533b\u7597\u8bca\u65ad\u6216\u91d1\u878d\u5206\u6790\u7b49\u654f\u611f\u9886\u57df\u4e5f\u80fd\u5f88\u597d\u5730\u5de5\u4f5c\u3002"}}
{"id": "2511.11821", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11821", "abs": "https://arxiv.org/abs/2511.11821", "authors": ["Hong-Jun Yoon", "Faisal Ashraf", "Thomas A. Ruggles", "Debjani Singh"], "title": "Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis", "comment": "18 pages, zero figures, Preprint submitted to Environmental Modeling and Software", "summary": "Information extraction from regulatory documents using large language models presents critical trade-offs between performance and computational resources. We evaluated seven open-weight models (0.6B-70B parameters) on hydropower licensing documentation to provide empirical deployment guidance.\n  Our analysis identified a pronounced 14B parameter threshold where validation methods transition from ineffective (F1 $<$ 0.15) to viable (F1 = 0.64). Consumer-deployable models achieve 64\\% F1 through appropriate validation, while smaller models plateau at 51\\%. Large-scale models approach 77\\% F1 but require enterprise infrastructure.\n  We identified systematic hallucination patterns where perfect recall indicates extraction failure rather than success in smaller models. Our findings establish the first comprehensive resource-performance mapping for open-weight information extraction in regulatory contexts, enabling evidence-based model selection.\n  These results provide immediate value for hydropower compliance while contributing insights into parameter scaling effects that generalize across information extraction tasks.", "AI": {"tldr": "\u8bc4\u4f30\u4e86\u4e03\u4e2a\u5f00\u653e\u6743\u91cd\u6a21\u578b\uff080.6B-70B \u53c2\u6570\uff09\u5728\u6c34\u529b\u53d1\u7535\u8bb8\u53ef\u6587\u4ef6\u4e0a\u7684\u4fe1\u606f\u63d0\u53d6\u6027\u80fd\uff0c\u4ee5\u63d0\u4f9b\u90e8\u7f72\u6307\u5bfc\u3002", "motivation": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u76d1\u7ba1\u6587\u4ef6\u4e2d\u63d0\u53d6\u4fe1\u606f\u9700\u8981\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u8d44\u6e90\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002", "method": "\u5728\u6c34\u529b\u53d1\u7535\u8bb8\u53ef\u6587\u6863\u4e0a\u8bc4\u4f30\u4e86\u4e03\u4e2a\u5f00\u653e\u6743\u91cd\u6a21\u578b\uff080.6B-70B \u53c2\u6570\uff09\u3002", "result": "\u786e\u5b9a\u4e86\u660e\u663e\u7684 14B \u53c2\u6570\u9608\u503c\uff0c\u9a8c\u8bc1\u65b9\u6cd5\u4ece\u65e0\u6548\uff08F1 < 0.15\uff09\u8fc7\u6e21\u5230\u53ef\u884c\uff08F1 = 0.64\uff09\u3002\u6d88\u8d39\u8005\u53ef\u90e8\u7f72\u7684\u6a21\u578b\u901a\u8fc7\u9002\u5f53\u7684\u9a8c\u8bc1\u5b9e\u73b0\u4e86 64% \u7684 F1\uff0c\u800c\u8f83\u5c0f\u7684\u6a21\u578b\u7a33\u5b9a\u5728 51%\u3002\u5927\u578b\u6a21\u578b\u63a5\u8fd1 77% \u7684 F1\uff0c\u4f46\u9700\u8981\u4f01\u4e1a\u57fa\u7840\u8bbe\u65bd\u3002", "conclusion": "\u5efa\u7acb\u4e86\u5f00\u653e\u6743\u91cd\u4fe1\u606f\u63d0\u53d6\u5728\u76d1\u7ba1\u73af\u5883\u4e2d\u7684\u7b2c\u4e00\u4e2a\u7efc\u5408\u8d44\u6e90-\u6027\u80fd\u6620\u5c04\uff0c\u4ece\u800c\u80fd\u591f\u8fdb\u884c\u57fa\u4e8e\u8bc1\u636e\u7684\u6a21\u578b\u9009\u62e9\u3002\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u6c34\u529b\u53d1\u7535\u5408\u89c4\u6027\u63d0\u4f9b\u4e86\u76f4\u63a5\u4ef7\u503c\uff0c\u540c\u65f6\u6709\u52a9\u4e8e\u6df1\u5165\u4e86\u89e3\u53c2\u6570\u7f29\u653e\u6548\u5e94\uff0c\u8fd9\u4e9b\u6548\u5e94\u53ef\u4ee5\u63a8\u5e7f\u5230\u5404\u79cd\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2511.11662", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11662", "abs": "https://arxiv.org/abs/2511.11662", "authors": ["Ziyuan Gao"], "title": "AGENet: Adaptive Edge-aware Geodesic Distance Learning for Few-Shot Medical Image Segmentation", "comment": "Accepted for publication in WACV 2026 (Round 2)", "summary": "Medical image segmentation requires large annotated datasets, creating a significant bottleneck for clinical applications. While few-shot segmentation methods can learn from minimal examples, existing approaches demonstrate suboptimal performance in precise boundary delineation for medical images, particularly when anatomically similar regions appear without sufficient spatial context. We propose AGENet (Adaptive Geodesic Edge-aware Network), a novel framework that incorporates spatial relationships through edge-aware geodesic distance learning. Our key insight is that medical structures follow predictable geometric patterns that can guide prototype extraction even with limited training data. Unlike methods relying on complex architectural components or heavy neural networks, our approach leverages computationally lightweight geometric modeling. The framework combines three main components: (1) An edge-aware geodesic distance learning module that respects anatomical boundaries through iterative Fast Marching refinement, (2) adaptive prototype extraction that captures both global structure and local boundary details via spatially-weighted aggregation, and (3) adaptive parameter learning that automatically adjusts to different organ characteristics. Extensive experiments across diverse medical imaging datasets demonstrate improvements over state-of-the-art methods. Notably, our method reduces boundary errors compared to existing approaches while maintaining computational efficiency, making it highly suitable for clinical applications requiring precise segmentation with limited annotated data.", "AI": {"tldr": "\u63d0\u51fa\u4e86AGENet\uff08\u81ea\u9002\u5e94\u6d4b\u5730\u7ebf\u8fb9\u7f18\u611f\u77e5\u7f51\u7edc\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u6570\u636e\u96c6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u89e3\u5256\u7ed3\u6784\u76f8\u4f3c\u7684\u533a\u57df\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9700\u8981\u5927\u91cf\u7684\u6807\u6ce8\u6570\u636e\uff0c\u8fd9\u6210\u4e3a\u4e86\u4e34\u5e8a\u5e94\u7528\u7684\u91cd\u8981\u74f6\u9888\u3002\u73b0\u6709\u7684few-shot\u5206\u5272\u65b9\u6cd5\u5728\u533b\u5b66\u56fe\u50cf\u7684\u7cbe\u786e\u8fb9\u754c\u5212\u5206\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u89e3\u5256\u7ed3\u6784\u76f8\u4f3c\u7684\u533a\u57df\u3002", "method": "\u901a\u8fc7\u8fb9\u7f18\u611f\u77e5\u6d4b\u5730\u8ddd\u79bb\u5b66\u4e60\u6765\u7ed3\u5408\u7a7a\u95f4\u5173\u7cfb\u3002\u8be5\u6846\u67b6\u5305\u62ec\uff1a\u8fb9\u7f18\u611f\u77e5\u6d4b\u5730\u8ddd\u79bb\u5b66\u4e60\u6a21\u5757\u3001\u81ea\u9002\u5e94\u539f\u578b\u63d0\u53d6\u6a21\u5757\u548c\u81ea\u9002\u5e94\u53c2\u6570\u5b66\u4e60\u6a21\u5757\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u51cf\u5c11\u4e86\u8fb9\u754c\u8bef\u5dee\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u975e\u5e38\u9002\u5408\u9700\u8981\u7cbe\u786e\u5206\u5272\u4e14\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u4e34\u5e8a\u5e94\u7528\u3002"}}
{"id": "2511.11576", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11576", "abs": "https://arxiv.org/abs/2511.11576", "authors": ["WenZhuo Zhu", "Zheng Cui", "Wenhan Lu", "Sheng Liu", "Yue Zhao"], "title": "DAOpt: Modeling and Evaluation of Data-Driven Optimization under Uncertainty with LLMs", "comment": null, "summary": "Recent advances in large language models (LLMs) have accelerated research on automated optimization modeling. While real-world decision-making is inherently uncertain, most existing work has focused on deterministic optimization with known parameters, leaving the application of LLMs in uncertain settings largely unexplored. To that end, we propose the DAOpt framework including a new dataset OptU, a multi-agent decision-making module, and a simulation environment for evaluating LLMs with a focus on out-of-sample feasibility and robustness. Additionally, we enhance LLMs' modeling capabilities by incorporating few-shot learning with domain knowledge from stochastic and robust optimization.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4e0d\u786e\u5b9a\u6027\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u4e8e\u786e\u5b9a\u6027\u4f18\u5316\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u4e16\u754c\u51b3\u7b56\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e86DAOpt\u6846\u67b6\uff0c\u5305\u62ecOptU\u6570\u636e\u96c6\u3001\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u6a21\u5757\u548c\u4eff\u771f\u73af\u5883\uff0c\u5e76\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u8fdb\u884cFew-shot\u5b66\u4e60\u3002", "result": "\u7740\u91cd\u4e8e\u8bc4\u4f30LLM\u7684\u6837\u672c\u5916\u53ef\u884c\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7DAOpt\u6846\u67b6\u63d0\u5347LLM\u5728\u4e0d\u786e\u5b9a\u6027\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u5efa\u6a21\u80fd\u529b\u3002"}}
{"id": "2511.12081", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12081", "abs": "https://arxiv.org/abs/2511.12081", "authors": ["Bencheng Yan", "Yuejie Lei", "Zhiyuan Zeng", "Di Wang", "Kaiyi Lin", "Pengjie Wang", "Jian Xu", "Bo Zheng"], "title": "From Scaling to Structured Expressivity: Rethinking Transformers for CTR Prediction", "comment": null, "summary": "Despite massive investments in scale, deep models for click-through rate (CTR) prediction often exhibit rapidly diminishing returns - a stark contrast to the smooth, predictable gains seen in large language models. We identify the root cause as a structural misalignment: Transformers assume sequential compositionality, while CTR data demand combinatorial reasoning over high-cardinality semantic fields. Unstructured attention spreads capacity indiscriminately, amplifying noise under extreme sparsity and breaking scalable learning. To restore alignment, we introduce the Field-Aware Transformer (FAT), which embeds field-based interaction priors into attention through decomposed content alignment and cross-field modulation. This design ensures model complexity scales with the number of fields F, not the total vocabulary size n >> F, leading to tighter generalization and, critically, observed power-law scaling in AUC as model width increases. We present the first formal scaling law for CTR models, grounded in Rademacher complexity, that explains and predicts this behavior. On large-scale benchmarks, FAT improves AUC by up to +0.51% over state-of-the-art methods. Deployed online, it delivers +2.33% CTR and +0.66% RPM. Our work establishes that effective scaling in recommendation arises not from size, but from structured expressivity-architectural coherence with data semantics.", "AI": {"tldr": "\u63d0\u51fa\u4e86Field-Aware Transformer (FAT)\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3CTR\u9884\u6d4b\u4e2d\u6df1\u5ea6\u6a21\u578b\u6536\u76ca\u9012\u51cf\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709CTR\u9884\u6d4b\u6a21\u578b\u5728\u6269\u5c55\u89c4\u6a21\u65f6\u6536\u76ca\u8fc5\u901f\u9012\u51cf\uff0c\u56e0\u4e3aTransformer\u7684\u7ed3\u6784\u4e0eCTR\u6570\u636e\u7684\u7ec4\u5408\u6027\u8d28\u4e0d\u5339\u914d\u3002", "method": "FAT\u6a21\u578b\u901a\u8fc7\u5206\u89e3\u5185\u5bb9\u5bf9\u9f50\u548c\u8de8\u5b57\u6bb5\u8c03\u5236\uff0c\u5c06\u57fa\u4e8e\u5b57\u6bb5\u7684\u4ea4\u4e92\u5148\u9a8c\u77e5\u8bc6\u5d4c\u5165\u5230\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u3002", "result": "\u5728\u5927\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFAT\u7684AUC\u63d0\u9ad8\u4e86\u9ad8\u8fbe+0.51%\uff0c\u5728\u7ebf\u90e8\u7f72\u540e\uff0cCTR\u63d0\u9ad8\u4e86+2.33%\uff0cRPM\u63d0\u9ad8\u4e86+0.66%\u3002", "conclusion": "\u6709\u6548\u7684\u63a8\u8350\u7cfb\u7edf\u6269\u5c55\u5e76\u975e\u6765\u81ea\u6a21\u578b\u89c4\u6a21\uff0c\u800c\u662f\u6765\u81ea\u7ed3\u6784\u5316\u7684\u8868\u8fbe\u80fd\u529b\u548c\u4e0e\u6570\u636e\u8bed\u4e49\u7684\u67b6\u6784\u4e00\u81f4\u6027\u3002"}}
{"id": "2511.11611", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.11611", "abs": "https://arxiv.org/abs/2511.11611", "authors": ["David H. Silver"], "title": "Quantifying Skill and Chance: A Unified Framework for the Geometry of Games", "comment": null, "summary": "We introduce a quantitative framework for separating skill and chance in games by modeling them as complementary sources of control over stochastic decision trees. We define the Skill-Luck Index S(G) in [-1, 1] by decomposing game outcomes into skill leverage K and luck leverage L. Applying this to 30 games reveals a continuum from pure chance (coin toss, S = -1) through mixed domains such as backgammon (S = 0, Sigma = 1.20) to pure skill (chess, S = +1, Sigma = 0). Poker exhibits moderate skill dominance (S = 0.33) with K = 0.40 +/- 0.03 and Sigma = 0.80. We further introduce volatility Sigma to quantify outcome uncertainty over successive turns. The framework extends to general stochastic decision systems, enabling principled comparisons of player influence, game balance, and predictive stability, with applications to game design, AI evaluation, and risk assessment.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u91cf\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u533a\u5206\u6e38\u620f\u4e2d\u6280\u80fd\u548c\u8fd0\u6c14\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u5c06\u5b83\u4eec\u5efa\u6a21\u4e3a\u968f\u673a\u51b3\u7b56\u6811\u7684\u4e92\u8865\u63a7\u5236\u6765\u6e90\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u533a\u5206\u6e38\u620f\u4e2d\u6280\u80fd\u548c\u8fd0\u6c14\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u91cf\u5316\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u8be5\u7814\u7a76\u901a\u8fc7\u5c06\u6e38\u620f\u7ed3\u679c\u5206\u89e3\u4e3a\u6280\u80fd\u6760\u6746 K \u548c\u8fd0\u6c14\u6760\u6746 L\uff0c\u5b9a\u4e49\u4e86 Skill-Luck \u6307\u6570 S(G)\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u6ce2\u52a8\u7387 Sigma \u6765\u91cf\u5316\u8fde\u7eed\u56de\u5408\u4e2d\u7ed3\u679c\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u901a\u8fc7\u5bf9 30 \u6b3e\u6e38\u620f\u7684\u5e94\u7528\uff0c\u63ed\u793a\u4e86\u4e00\u4e2a\u4ece\u7eaf\u7cb9\u7684\u8fd0\u6c14\uff08\u629b\u786c\u5e01\uff0cS = -1\uff09\u5230\u6df7\u5408\u9886\u57df\uff08\u5982\u897f\u6d0b\u53cc\u9646\u68cb\uff0cS = 0\uff0cSigma = 1.20\uff09\u5230\u7eaf\u7cb9\u7684\u6280\u80fd\uff08\u56fd\u9645\u8c61\u68cb\uff0cS = +1\uff0cSigma = 0\uff09\u7684\u8fde\u7eed\u7edf\u4e00\u4f53\u3002\u6251\u514b\u8868\u73b0\u51fa\u9002\u5ea6\u7684\u6280\u80fd\u4f18\u52bf\uff08S = 0.33\uff09\uff0cK = 0.40 +/- 0.03\uff0cSigma = 0.80\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u6269\u5c55\u5230\u4e00\u822c\u7684\u968f\u673a\u51b3\u7b56\u7cfb\u7edf\uff0c\u4ece\u800c\u80fd\u591f\u5bf9\u73a9\u5bb6\u5f71\u54cd\u529b\u3001\u6e38\u620f\u5e73\u8861\u548c\u9884\u6d4b\u7a33\u5b9a\u6027\u8fdb\u884c\u6709\u539f\u5219\u7684\u6bd4\u8f83\uff0c\u5e76\u53ef\u5e94\u7528\u4e8e\u6e38\u620f\u8bbe\u8ba1\u3001\u4eba\u5de5\u667a\u80fd\u8bc4\u4f30\u548c\u98ce\u9669\u8bc4\u4f30\u3002"}}
{"id": "2511.11829", "categories": ["cs.CL", "cs.AI", "cs.FL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.11829", "abs": "https://arxiv.org/abs/2511.11829", "authors": ["Mihir Gupte", "Ramesh S"], "title": "Towards Autoformalization of LLM-generated Outputs for Requirement Verification", "comment": "To be submitted for publication", "summary": "Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u5de5\u5177\u6765\u9a8c\u8bc1LLM\u751f\u6210\u7684\u8f93\u51fa\uff0c\u4ee5\u786e\u4fdd\u5176\u51c6\u786e\u6027\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u9a8c\u8bc1LLM\u751f\u6210\u8f93\u51fa\u51c6\u786e\u6027\u7684\u6b63\u5f0f\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u5de5\u5177\u6765\u9a8c\u8bc1LLM\u751f\u6210\u7684\u8f93\u51fa\uff0c\u5e76\u8fdb\u884c\u4e86\u4e24\u4e2a\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u81ea\u52a8\u5f62\u5f0f\u5316\u5de5\u5177\u53ef\u4ee5\u8bc6\u522b\u903b\u8f91\u7b49\u4ef7\u7684NL\u9700\u6c42\u548cLLM\u751f\u6210\u8f93\u51fa\u4e2d\u7684\u903b\u8f91\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u81ea\u52a8\u5f62\u5f0f\u5316\u5728\u786e\u4fddLLM\u751f\u6210\u8f93\u51fa\u7684\u4fdd\u771f\u5ea6\u548c\u903b\u8f91\u4e00\u81f4\u6027\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.11700", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.11700", "abs": "https://arxiv.org/abs/2511.11700", "authors": ["Jiahui Wang", "Haiyue Zhu", "Haoren Guo", "Abdullah Al Mamun", "Cheng Xiang", "Tong Heng Lee"], "title": "EPSegFZ: Efficient Point Cloud Semantic Segmentation for Few- and Zero-Shot Scenarios with Language Guidance", "comment": "AAAI 2026", "summary": "Recent approaches for few-shot 3D point cloud semantic segmentation typically require a two-stage learning process, i.e., a pre-training stage followed by a few-shot training stage. While effective, these methods face overreliance on pre-training, which hinders model flexibility and adaptability. Some models tried to avoid pre-training yet failed to capture ample information. In addition, current approaches focus on visual information in the support set and neglect or do not fully exploit other useful data, such as textual annotations. This inadequate utilization of support information impairs the performance of the model and restricts its zero-shot ability. To address these limitations, we present a novel pre-training-free network, named Efficient Point Cloud Semantic Segmentation for Few- and Zero-shot scenarios. Our EPSegFZ incorporates three key components. A Prototype-Enhanced Registers Attention (ProERA) module and a Dual Relative Positional Encoding (DRPE)-based cross-attention mechanism for improved feature extraction and accurate query-prototype correspondence construction without pre-training. A Language-Guided Prototype Embedding (LGPE) module that effectively leverages textual information from the support set to improve few-shot performance and enable zero-shot inference. Extensive experiments show that our method outperforms the state-of-the-art method by 5.68% and 3.82% on the S3DIS and ScanNet benchmarks, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEPSegFZ\u7684\u65b0\u578b\u514d\u9884\u8bad\u7ec3\u7f51\u7edc\uff0c\u7528\u4e8e\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u573a\u666f\u4e0b\u7684\u9ad8\u6548\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u9884\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\uff0c\u5e76\u4e14\u672a\u80fd\u5145\u5206\u5229\u7528\u652f\u6301\u96c6\u4e2d\u7684\u6587\u672c\u6ce8\u91ca\u7b49\u4fe1\u606f\u3002", "method": "EPSegFZ\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1aPrototype-Enhanced Registers Attention (ProERA) \u6a21\u5757\u548c Dual Relative Positional Encoding (DRPE) \u673a\u5236\uff0c\u7528\u4e8e\u6539\u8fdb\u7279\u5f81\u63d0\u53d6\u548c\u6784\u5efa\u7cbe\u786e\u7684\u67e5\u8be2-\u539f\u578b\u5bf9\u5e94\u5173\u7cfb\uff1bLanguage-Guided Prototype Embedding (LGPE) \u6a21\u5757\uff0c\u6709\u6548\u5229\u7528\u652f\u6301\u96c6\u7684\u6587\u672c\u4fe1\u606f\u3002", "result": "\u5728S3DIS\u548cScanNet\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5206\u522b\u8d85\u8fc7\u4e86\u73b0\u6709\u6700\u4f73\u65b9\u6cd55.68%\u548c3.82%\u3002", "conclusion": "EPSegFZ\u7f51\u7edc\u5728\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2511.11579", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11579", "abs": "https://arxiv.org/abs/2511.11579", "authors": ["Felipe Urrutia", "Jorge Salas", "Alexander Kozachinskiy", "Cristian Buc Calderon", "Hector Pasten", "Cristobal Rojas"], "title": "Decoupling Positional and Symbolic Attention Behavior in Transformers", "comment": "32 pages, 12 figures, repository available", "summary": "An important aspect subtending language understanding and production is the ability to independently encode positional and symbolic information of the words within a sentence. In Transformers, positional information is typically encoded using Positional Encodings (PEs). One such popular PE, namely Rotary PE (RoPE), has been widely used due to its empirical success. Recently, it has been argued that part of RoPE's success emerges from its ability to encode robust positional and semantic information using large and small frequencies, respectively. In this work, we perform a deeper dive into the positional versus symbolic dichotomy of attention heads behavior, both at the theoretical and empirical level. We provide general definitions of what it means for a head to behave positionally or symbolically, prove that these are two mutually exclusive behaviors and develop a metric to quantify them. We apply our framework to analyze Transformer-based LLMs using RoPE and find that all heads exhibit a strong correspondence between behavior and frequency use. Finally, we introduce canonical tasks designed to be either purely positional or symbolic, and demonstrate that the Transformer performance causally relates to the ability of attention heads to leverage the appropriate frequencies. In particular, we show that we can control the Transformer performance by controlling which frequencies the attention heads can access. Altogether, our work provides a detailed understanding of RoPE, and how its properties relate to model behavior.", "AI": {"tldr": "\u672c\u6587\u6df1\u5165\u7814\u7a76\u4e86Transformer\u4e2d\u6ce8\u610f\u529b\u5934\u7684\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5b83\u4eec\u5982\u4f55\u7f16\u7801\u4f4d\u7f6e\u548c\u7b26\u53f7\u4fe1\u606f\uff0c\u4ee5\u53ca\u8fd9\u4e0eRotary Positional Encoding (RoPE)\u4e2d\u9891\u7387\u4f7f\u7528\u7684\u5173\u7cfb\u3002", "motivation": "\u7406\u89e3\u8bed\u8a00\u9700\u8981\u72ec\u7acb\u7f16\u7801\u53e5\u5b50\u4e2d\u8bcd\u8bed\u7684\u4f4d\u7f6e\u548c\u7b26\u53f7\u4fe1\u606f\u3002RoPE\u662f\u4e00\u79cd\u6d41\u884c\u7684\u4f4d\u7f6e\u7f16\u7801\u65b9\u5f0f\uff0c\u4f46\u5176\u6210\u529f\u7684\u539f\u56e0\u5c1a\u4e0d\u5b8c\u5168\u6e05\u695a\u3002\u672c\u6587\u65e8\u5728\u66f4\u6df1\u5165\u5730\u7814\u7a76RoPE\u4e2d\u4f4d\u7f6e\u4e0e\u7b26\u53f7\u4fe1\u606f\u7684\u4e8c\u5206\u6cd5\u3002", "method": "\u672c\u6587\u4ece\u7406\u8bba\u548c\u5b9e\u8bc1\u5c42\u9762\u5206\u6790\u4e86\u6ce8\u610f\u529b\u5934\u7684\u884c\u4e3a\uff0c\u5b9a\u4e49\u4e86\u4f4d\u7f6e\u6027\u548c\u7b26\u53f7\u6027\u884c\u4e3a\uff0c\u8bc1\u660e\u4e86\u8fd9\u4e24\u79cd\u884c\u4e3a\u7684\u4e92\u65a5\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u91cf\u5316\u5b83\u4eec\u7684\u6307\u6807\u3002\u7136\u540e\uff0c\u4f7f\u7528\u8be5\u6846\u67b6\u5206\u6790\u4e86\u57fa\u4e8eTransformer\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6240\u6709\u6ce8\u610f\u529b\u5934\u7684\u884c\u4e3a\u4e0e\u5176\u9891\u7387\u4f7f\u7528\u4e4b\u95f4\u5b58\u5728\u5f88\u5f3a\u7684\u5bf9\u5e94\u5173\u7cfb\u3002\u901a\u8fc7\u63a7\u5236\u6ce8\u610f\u529b\u5934\u53ef\u4ee5\u8bbf\u95ee\u7684\u9891\u7387\uff0c\u53ef\u4ee5\u63a7\u5236Transformer\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u8be6\u7ec6\u9610\u8ff0\u4e86RoPE\u7684\u539f\u7406\uff0c\u4ee5\u53ca\u5176\u5c5e\u6027\u4e0e\u6a21\u578b\u884c\u4e3a\u4e4b\u95f4\u7684\u5173\u7cfb\u3002"}}
{"id": "2511.12114", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12114", "abs": "https://arxiv.org/abs/2511.12114", "authors": ["Chengyi Liu", "Xiao Chen", "Shijie Wang", "Wenqi Fan", "Qing Li"], "title": "Continuous-time Discrete-space Diffusion Model for Recommendation", "comment": "Accepted by WSDM 2026", "summary": "In the era of information explosion, Recommender Systems (RS) are essential for alleviating information overload and providing personalized user experiences. Recent advances in diffusion-based generative recommenders have shown promise in capturing the dynamic nature of user preferences. These approaches explore a broader range of user interests by progressively perturbing the distribution of user-item interactions and recovering potential preferences from noise, enabling nuanced behavioral understanding. However, existing diffusion-based approaches predominantly operate in continuous space through encoded graph-based historical interactions, which may compromise potential information loss and suffer from computational inefficiency. As such, we propose CDRec, a novel Continuous-time Discrete-space Diffusion Recommendation framework, which models user behavior patterns through discrete diffusion on historical interactions over continuous time. The discrete diffusion algorithm operates via discrete element operations (e.g., masking) while incorporating domain knowledge through transition matrices, producing more meaningful diffusion trajectories. Furthermore, the continuous-time formulation enables flexible adaptive sampling. To better adapt discrete diffusion models to recommendations, CDRec introduces: (1) a novel popularity-aware noise schedule that generates semantically meaningful diffusion trajectories, and (2) an efficient training framework combining consistency parameterization for fast sampling and a contrastive learning objective guided by multi-hop collaborative signals for personalized recommendation. Extensive experiments on real-world datasets demonstrate CDRec's superior performance in both recommendation accuracy and computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u8fde\u7eed\u65f6\u95f4\u79bb\u6563\u7a7a\u95f4\u6269\u6563\u7684\u63a8\u8350\u6846\u67b6CDRec\uff0c\u901a\u8fc7\u5728\u8fde\u7eed\u65f6\u95f4\u5185\u5bf9\u5386\u53f2\u4ea4\u4e92\u8fdb\u884c\u79bb\u6563\u6269\u6563\u6765\u5efa\u6a21\u7528\u6237\u884c\u4e3a\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684\u63a8\u8350\u65b9\u6cd5\u4e3b\u8981\u5728\u8fde\u7eed\u7a7a\u95f4\u4e2d\u8fd0\u884c\uff0c\u53ef\u80fd\u5bfc\u81f4\u6f5c\u5728\u7684\u4fe1\u606f\u4e22\u5931\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3002", "method": "CDRec \u5f15\u5165\u4e86\u65b0\u9896\u7684\u6d41\u884c\u5ea6\u611f\u77e5\u566a\u58f0\u8ba1\u5212\uff0c\u8be5\u8ba1\u5212\u751f\u6210\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u7684\u6269\u6563\u8f68\u8ff9\uff0c\u4ee5\u53ca\u4e00\u4e2a\u6709\u6548\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u4e00\u81f4\u6027\u53c2\u6570\u5316\u4ee5\u8fdb\u884c\u5feb\u901f\u91c7\u6837\uff0c\u4ee5\u53ca\u4e00\u4e2a\u7531\u591a\u8df3\u534f\u4f5c\u4fe1\u53f7\u5f15\u5bfc\u7684\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u4ee5\u5b9e\u73b0\u4e2a\u6027\u5316\u63a8\u8350\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCDRec \u5728\u63a8\u8350\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "CDRec \u662f\u4e00\u79cd\u6709\u524d\u9014\u7684\u63a8\u8350\u6846\u67b6\uff0c\u5b83\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2511.11693", "categories": ["cs.AI", "cs.CR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11693", "abs": "https://arxiv.org/abs/2511.11693", "authors": ["Xin Zhao", "Xiaojun Chen", "Bingshan Liu", "Zeyao Liu", "Zhendong Zhao", "Xiaoyan Gu"], "title": "Value-Aligned Prompt Moderation via Zero-Shot Agentic Rewriting for Safe Image Generation", "comment": null, "summary": "Generative vision-language models like Stable Diffusion demonstrate remarkable capabilities in creative media synthesis, but they also pose substantial risks of producing unsafe, offensive, or culturally inappropriate content when prompted adversarially. Current defenses struggle to align outputs with human values without sacrificing generation quality or incurring high costs. To address these challenges, we introduce VALOR (Value-Aligned LLM-Overseen Rewriter), a modular, zero-shot agentic framework for safer and more helpful text-to-image generation. VALOR integrates layered prompt analysis with human-aligned value reasoning: a multi-level NSFW detector filters lexical and semantic risks; a cultural value alignment module identifies violations of social norms, legality, and representational ethics; and an intention disambiguator detects subtle or indirect unsafe implications. When unsafe content is detected, prompts are selectively rewritten by a large language model under dynamic, role-specific instructions designed to preserve user intent while enforcing alignment. If the generated image still fails a safety check, VALOR optionally performs a stylistic regeneration to steer the output toward a safer visual domain without altering core semantics. Experiments across adversarial, ambiguous, and value-sensitive prompts show that VALOR significantly reduces unsafe outputs by up to 100.00% while preserving prompt usefulness and creativity. These results highlight VALOR as a scalable and effective approach for deploying safe, aligned, and helpful image generation systems in open-world settings.", "AI": {"tldr": "VALOR\u662f\u4e00\u4e2a\u7528\u4e8e\u66f4\u5b89\u5168\u3001\u66f4\u6709\u5e2e\u52a9\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u6a21\u5757\u5316\u3001\u96f6\u6837\u672c\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u63d0\u793a\u5206\u6790\u548c\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684\u4ef7\u503c\u63a8\u7406\u6765\u51cf\u5c11\u4e0d\u5b89\u5168\u5185\u5bb9\u7684\u751f\u6210\u3002", "motivation": "\u5f53\u524d\u7684\u9632\u5fa1\u63aa\u65bd\u96be\u4ee5\u5728\u4e0d\u727a\u7272\u751f\u6210\u8d28\u91cf\u6216\u4ea7\u751f\u9ad8\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u4f7f\u8f93\u51fa\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\uff0c\u56e0\u6b64\u5f15\u5165VALOR\u6846\u67b6\u3002", "method": "VALOR\u96c6\u6210\u4e86\u5206\u5c42\u63d0\u793a\u5206\u6790\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684\u4ef7\u503c\u63a8\u7406\uff1a\u591a\u7ea7NSFW\u68c0\u6d4b\u5668\u8fc7\u6ee4\u8bcd\u6c47\u548c\u8bed\u4e49\u98ce\u9669\uff1b\u6587\u5316\u4ef7\u503c\u5bf9\u9f50\u6a21\u5757\u8bc6\u522b\u8fdd\u53cd\u793e\u4f1a\u89c4\u8303\u3001\u5408\u6cd5\u6027\u548c\u8868\u5f81\u4f26\u7406\u7684\u884c\u4e3a\uff1b\u610f\u56fe\u6d88\u6b67\u5668\u68c0\u6d4b\u5fae\u5999\u6216\u95f4\u63a5\u7684\u4e0d\u5b89\u5168\u542b\u4e49\u3002\u5f53\u68c0\u6d4b\u5230\u4e0d\u5b89\u5168\u5185\u5bb9\u65f6\uff0c\u63d0\u793a\u4f1a\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u7684\u3001\u7279\u5b9a\u89d2\u8272\u7684\u6307\u4ee4\u4e0b\u9009\u62e9\u6027\u5730\u91cd\u5199\uff0c\u65e8\u5728\u4fdd\u7559\u7528\u6237\u610f\u56fe\uff0c\u540c\u65f6\u52a0\u5f3a\u5bf9\u9f50\u3002\u5982\u679c\u751f\u6210\u7684\u56fe\u50cf\u4ecd\u7136\u672a\u80fd\u901a\u8fc7\u5b89\u5168\u68c0\u67e5\uff0cVALOR\u53ef\u4ee5\u9009\u62e9\u6267\u884c\u98ce\u683c\u518d\u751f\uff0c\u4ee5\u5c06\u8f93\u51fa\u5f15\u5bfc\u5230\u66f4\u5b89\u5168\u7684\u89c6\u89c9\u9886\u57df\uff0c\u800c\u4e0d\u6539\u53d8\u6838\u5fc3\u8bed\u4e49\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5bf9\u6297\u6027\u3001\u6a21\u7cca\u548c\u4ef7\u503c\u654f\u611f\u7684\u63d0\u793a\u4e2d\uff0cVALOR\u663e\u8457\u51cf\u5c11\u4e86\u9ad8\u8fbe100.00%\u7684\u4e0d\u5b89\u5168\u8f93\u51fa\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u63d0\u793a\u7684\u6709\u7528\u6027\u548c\u521b\u9020\u529b\u3002", "conclusion": "VALOR\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u90e8\u7f72\u5b89\u5168\u3001\u5bf9\u9f50\u548c\u6709\u5e2e\u52a9\u7684\u56fe\u50cf\u751f\u6210\u7cfb\u7edf\u3002"}}
{"id": "2511.11857", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11857", "abs": "https://arxiv.org/abs/2511.11857", "authors": ["Taimur Khan", "Ramoza Ahsan", "Mohib Hameed"], "title": "Three Stage Narrative Analysis; Plot-Sentiment Breakdown, Structure Learning and Concept Detection", "comment": "18 pages", "summary": "Story understanding and analysis have long been challenging areas within Natural Language Understanding. Automated narrative analysis requires deep computational semantic representations along with syntactic processing. Moreover, the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches. In this paper, we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved. The framework enables the extraction of high-level and low-level concepts conveyed through the narrative. Using dictionary-based sentiment analysis, our approach applies a custom lexicon built with the LabMTsimple storylab module. The custom lexicon is based on the Valence, Arousal, and Dominance scores from the NRC-VAD dataset. Furthermore, the framework advances the analysis by clustering similar sentiment plots using Wards hierarchical clustering technique. Experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u6790\u7535\u5f71\u5267\u672c\u60c5\u611f\u5f27\u7684\u6846\u67b6\uff0c\u5e76\u5bf9\u76f8\u5173\u4eba\u7269\u7684\u60c5\u611f\u8fdb\u884c\u6269\u5c55\u5206\u6790\u3002", "motivation": "\u5927\u91cf\u53d9\u4e8b\u6570\u636e\u7684\u51fa\u73b0\uff0c\u9700\u8981\u81ea\u52a8\u8bed\u4e49\u5206\u6790\u548c\u8ba1\u7b97\u5b66\u4e60\uff0c\u800c\u4e0d\u662f\u624b\u52a8\u5206\u6790\u65b9\u6cd5\u3002\u6545\u4e8b\u7406\u89e3\u548c\u5206\u6790\u4e00\u76f4\u662f\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4e2d\u5177\u6709\u6311\u6218\u6027\u7684\u9886\u57df\u3002", "method": "\u8be5\u6846\u67b6\u4f7f\u7528\u57fa\u4e8e\u5b57\u5178\u7684\u60c5\u611f\u5206\u6790\uff0c\u5e94\u7528\u4f7f\u7528LabMTsimple storylab\u6a21\u5757\u6784\u5efa\u7684\u81ea\u5b9a\u4e49\u8bcd\u5178\u3002\u81ea\u5b9a\u4e49\u8bcd\u5178\u57fa\u4e8eNRC-VAD\u6570\u636e\u96c6\u4e2d\u7684Valence, Arousal, and Dominance\u5206\u6570\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u4f7f\u7528Wards\u5c42\u6b21\u805a\u7c7b\u6280\u672f\u5bf9\u76f8\u4f3c\u7684\u60c5\u611f\u56fe\u8fdb\u884c\u805a\u7c7b\uff0c\u4ece\u800c\u63a8\u8fdb\u5206\u6790\u3002", "result": "\u5728\u7535\u5f71\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u7ed3\u679c\u5206\u6790\u6709\u52a9\u4e8e\u6d88\u8d39\u8005\u548c\u8bfb\u8005\u9009\u62e9\u53d9\u4e8b\u6216\u6545\u4e8b\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u63d0\u53d6\u901a\u8fc7\u53d9\u4e8b\u4f20\u8fbe\u7684\u9ad8\u7ea7\u548c\u4f4e\u7ea7\u6982\u5ff5\uff0c\u4e3a\u6d88\u8d39\u8005\u548c\u8bfb\u8005\u5728\u9009\u62e9\u53d9\u4e8b\u4f5c\u54c1\u65f6\u63d0\u4f9b\u5e2e\u52a9\u3002"}}
{"id": "2511.11702", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.11702", "abs": "https://arxiv.org/abs/2511.11702", "authors": ["Lian He", "Meng Liu", "Qilang Ye", "Yu Zhou", "Xiang Deng", "Gangyi Ding"], "title": "Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement", "comment": null, "summary": "Understanding 3D scene-level affordances from natural language instructions is essential for enabling embodied agents to interact meaningfully in complex environments. However, this task remains challenging due to the need for semantic reasoning and spatial grounding. Existing methods mainly focus on object-level affordances or merely lift 2D predictions to 3D, neglecting rich geometric structure information in point clouds and incurring high computational costs. To address these limitations, we introduce Task-Aware 3D Scene-level Affordance segmentation (TASA), a novel geometry-optimized framework that jointly leverages 2D semantic cues and 3D geometric reasoning in a coarse-to-fine manner. To improve the affordance detection efficiency, TASA features a task-aware 2D affordance detection module to identify manipulable points from language and visual inputs, guiding the selection of task-relevant views. To fully exploit 3D geometric information, a 3D affordance refinement module is proposed to integrate 2D semantic priors with local 3D geometry, resulting in accurate and spatially coherent 3D affordance masks. Experiments on SceneFun3D demonstrate that TASA significantly outperforms the baselines in both accuracy and efficiency in scene-level affordance segmentation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTASA\u7684\u51e0\u4f55\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e2d\u7406\u89e33D\u573a\u666f\u7ea7\u7684\u53ef\u4f9b\u6027\uff0c\u8be5\u6846\u67b6\u4ee5\u7c97\u5230\u7ec6\u7684\u65b9\u5f0f\u8054\u5408\u5229\u75282D\u8bed\u4e49\u7ebf\u7d22\u548c3D\u51e0\u4f55\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u5bf9\u8c61\u7ea7\u522b\u7684\u53ef\u4f9b\u6027\uff0c\u6216\u8005\u4ec5\u4ec5\u5c062D\u9884\u6d4b\u63d0\u5347\u52303D\uff0c\u5ffd\u7565\u4e86\u70b9\u4e91\u4e2d\u4e30\u5bcc\u7684\u51e0\u4f55\u7ed3\u6784\u4fe1\u606f\uff0c\u5e76\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u6210\u672c\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "TASA\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u4efb\u52a1\u611f\u77e5\u76842D\u53ef\u4f9b\u6027\u68c0\u6d4b\u6a21\u5757\uff0c\u7528\u4e8e\u4ece\u8bed\u8a00\u548c\u89c6\u89c9\u8f93\u5165\u4e2d\u8bc6\u522b\u53ef\u64cd\u4f5c\u7684\u70b9\uff0c\u6307\u5bfc\u4efb\u52a1\u76f8\u5173\u89c6\u56fe\u7684\u9009\u62e9\u3002\u4ee5\u53ca\u4e00\u4e2a3D\u53ef\u4f9b\u6027\u7ec6\u5316\u6a21\u5757\uff0c\u7528\u4e8e\u6574\u54082D\u8bed\u4e49\u5148\u9a8c\u77e5\u8bc6\u4e0e\u5c40\u90e83D\u51e0\u4f55\u4fe1\u606f\uff0c\u4ece\u800c\u4ea7\u751f\u7cbe\u786e\u548c\u7a7a\u95f4\u8fde\u8d2f\u76843D\u53ef\u4f9b\u6027\u63a9\u7801\u3002", "result": "\u5728SceneFun3D\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTASA\u5728\u573a\u666f\u7ea7\u53ef\u4f9b\u6027\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TASA\u6846\u67b6\u5728\u573a\u666f\u7ea7\u53ef\u4f9b\u6027\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2511.11581", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.11581", "abs": "https://arxiv.org/abs/2511.11581", "authors": ["Burkhard Ringlein", "Jan van Lunteren", "Radu Stoica", "Thomas Parnell"], "title": "The Anatomy of a Triton Attention Kernel", "comment": null, "summary": "A long-standing goal in both industry and academia is to develop an LLM inference platform that is portable across hardware architectures, eliminates the need for low-level hand-tuning, and still delivers best-in-class efficiency. In this work, we demonstrate that portable, efficient cross-platform LLM inference is indeed possible and share our experience. We develop a state-of-the-art paged attention kernel, the core performance-critical component of many LLM deployments, that builds exclusively on the domain-specific just-in-time compiled language Triton to achieve state-of-the-art performance on both NVIDIA and AMD GPUs. We describe our high-level approach, the key algorithmic and system-level improvements, the parameter auto-tuning required to unlock efficiency, and the integrations into a popular inference server that are necessary to bring the performance of a generic Triton attention kernel from 19.7% of the state-of-the-art to 105.9%. Our results highlight how open-source domain-specific languages can be leveraged to unlock model portability across different GPU vendors.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u53ef\u79fb\u690d\u3001\u9ad8\u6548\u7684\u8de8\u5e73\u53f0LLM\u63a8\u7406\u5e73\u53f0\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u53ef\u5728\u4e0d\u540c\u786c\u4ef6\u67b6\u6784\u4e0a\u79fb\u690d\uff0c\u65e0\u9700\u5e95\u5c42\u624b\u52a8\u8c03\u6574\uff0c\u4e14\u4ecd\u80fd\u63d0\u4f9b\u6700\u4f73\u6548\u7387\u7684LLM\u63a8\u7406\u5e73\u53f0\u662f\u5de5\u4e1a\u754c\u548c\u5b66\u672f\u754c\u7684\u4e00\u4e2a\u957f\u671f\u76ee\u6807\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6700\u5148\u8fdb\u7684\u5206\u9875\u6ce8\u610f\u529b\u5185\u6838\uff0c\u8be5\u5185\u6838\u5b8c\u5168\u5efa\u7acb\u5728\u7279\u5b9a\u9886\u57df\u7684\u5373\u65f6\u7f16\u8bd1\u8bed\u8a00Triton\u4e4b\u4e0a\uff0c\u4ee5\u5728NVIDIA\u548cAMD GPU\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4e00\u4e2a\u901a\u7528\u7684Triton\u6ce8\u610f\u529b\u5185\u6838\u7684\u6027\u80fd\u4ece\u6700\u5148\u8fdb\u6c34\u5e73\u768419.7%\u63d0\u9ad8\u5230105.9%\u3002", "conclusion": "\u5f00\u6e90\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u53ef\u4ee5\u7528\u4e8e\u89e3\u9501\u4e0d\u540cGPU\u4f9b\u5e94\u5546\u4e4b\u95f4\u7684\u6a21\u578b\u53ef\u79fb\u690d\u6027\u3002"}}
{"id": "2511.12495", "categories": ["cs.IR", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.12495", "abs": "https://arxiv.org/abs/2511.12495", "authors": ["Zhen Tao", "Xinke Jiang", "Qingshuai Feng", "Haoyu Zhang", "Lun Du", "Yuchen Fang", "Hao Miao", "Bangquan Xie", "Qingqiang Sun"], "title": "Task-Aware Retrieval Augmentation for Dynamic Recommendation", "comment": "AAAI 2026", "summary": "Dynamic recommendation systems aim to provide personalized suggestions by modeling temporal user-item interactions across time-series behavioral data. Recent studies have leveraged pre-trained dynamic graph neural networks (GNNs) to learn user-item representations over temporal snapshot graphs. However, fine-tuning GNNs on these graphs often results in generalization issues due to temporal discrepancies between pre-training and fine-tuning stages, limiting the model's ability to capture evolving user preferences. To address this, we propose TarDGR, a task-aware retrieval-augmented framework designed to enhance generalization capability by incorporating task-aware model and retrieval-augmentation. Specifically, TarDGR introduces a Task-Aware Evaluation Mechanism to identify semantically relevant historical subgraphs, enabling the construction of task-specific datasets without manual labeling. It also presents a Graph Transformer-based Task-Aware Model that integrates semantic and structural encodings to assess subgraph relevance. During inference, TarDGR retrieves and fuses task-aware subgraphs with the query subgraph, enriching its representation and mitigating temporal generalization issues. Experiments on multiple large-scale dynamic graph datasets demonstrate that TarDGR consistently outperforms state-of-the-art methods, with extensive empirical evidence underscoring its superior accuracy and generalization capabilities.", "AI": {"tldr": "TarDGR\u901a\u8fc7\u7ed3\u5408\u4efb\u52a1\u611f\u77e5\u6a21\u578b\u548c\u68c0\u7d22\u589e\u5f3a\u6765\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u7f13\u89e3\u4e86\u52a8\u6001\u63a8\u8350\u7cfb\u7edf\u4e2d\u9884\u8bad\u7ec3\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u5fae\u8c03\u65f6\u7531\u4e8e\u65f6\u95f4\u5dee\u5f02\u5bfc\u81f4\u7684\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u52a8\u6001\u63a8\u8350\u7cfb\u7edf\u5728\u65f6\u5e8f\u884c\u4e3a\u6570\u636e\u4e0a\u5fae\u8c03\u56fe\u795e\u7ecf\u7f51\u7edc\u65f6\uff0c\u7531\u4e8e\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u9636\u6bb5\u7684\u65f6\u95f4\u5dee\u5f02\uff0c\u5bfc\u81f4\u6cdb\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4efb\u52a1\u611f\u77e5\u7684\u68c0\u7d22\u589e\u5f3a\u6846\u67b6TarDGR\uff0c\u5305\u62ec\u4efb\u52a1\u611f\u77e5\u8bc4\u4f30\u673a\u5236\u548c\u57fa\u4e8e\u56feTransformer\u7684\u4efb\u52a1\u611f\u77e5\u6a21\u578b\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u878d\u5408\u76f8\u5173\u7684\u5386\u53f2\u5b50\u56fe\u3002", "result": "\u5728\u591a\u4e2a\u5927\u89c4\u6a21\u52a8\u6001\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTarDGR\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "TarDGR\u80fd\u591f\u6709\u6548\u7f13\u89e3\u52a8\u6001\u63a8\u8350\u7cfb\u7edf\u4e2d\u7531\u4e8e\u65f6\u95f4\u5dee\u5f02\u5bfc\u81f4\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2511.11752", "categories": ["cs.AI", "cs.DL", "quant-ph"], "pdf": "https://arxiv.org/pdf/2511.11752", "abs": "https://arxiv.org/abs/2511.11752", "authors": ["S\u00f6ren Arlt", "Xuemei Gu", "Mario Krenn"], "title": "Towards autonomous quantum physics research using LLM agents with access to intelligent tools", "comment": "24 pages, 5 figures", "summary": "Artificial intelligence (AI) is used in numerous fields of science, yet the initial research questions and targets are still almost always provided by human researchers. AI-generated creative ideas in science are rare and often vague, so that it remains a human task to execute them. Automating idea generation and implementation in one coherent system would significantly shift the role of humans in the scientific process. Here we present AI-Mandel, an LLM agent that can generate and implement ideas in quantum physics. AI-Mandel formulates ideas from the literature and uses a domain-specific AI tool to turn them into concrete experiment designs that can readily be implemented in laboratories. The generated ideas by AI-Mandel are often scientifically interesting - for two of them we have already written independent scientific follow-up papers. The ideas include new variations of quantum teleportation, primitives of quantum networks in indefinite causal orders, and new concepts of geometric phases based on closed loops of quantum information transfer. AI-Mandel is a prototypical demonstration of an AI physicist that can generate and implement concrete, actionable ideas. Building such a system is not only useful to accelerate science, but it also reveals concrete open challenges on the path to human-level artificial scientists.", "AI": {"tldr": "AI-Mandel\u662f\u4e00\u4e2aLLM\u667a\u80fd\u4f53\uff0c\u53ef\u4ee5\u5728\u91cf\u5b50\u7269\u7406\u5b66\u4e2d\u4ea7\u751f\u548c\u5b9e\u65bd\u60f3\u6cd5\u3002", "motivation": "\u76ee\u524d\u79d1\u5b66\u9886\u57dfAI\u7684\u5e94\u7528\uff0c\u8d77\u59cb\u7684\u7814\u7a76\u95ee\u9898\u548c\u76ee\u6807\u51e0\u4e4e\u90fd\u7531\u4eba\u7c7b\u7814\u7a76\u8005\u63d0\u4f9b\u3002AI\u81ea\u4e3b\u4ea7\u751f\u79d1\u5b66\u521b\u610f\u60f3\u6cd5\u975e\u5e38\u5c11\u89c1\u4e14\u901a\u5e38\u6bd4\u8f83\u6a21\u7cca\uff0c\u5c06\u60f3\u6cd5\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u65b9\u6848\u4ecd\u7136\u662f\u4eba\u7c7b\u7684\u4efb\u52a1\u3002\u5982\u679c\u53ef\u4ee5\u5728\u4e00\u4e2a\u8fde\u8d2f\u7684\u7cfb\u7edf\u4e2d\u81ea\u52a8\u4ea7\u751f\u548c\u5b9e\u65bd\u60f3\u6cd5\uff0c\u5c06\u5927\u5927\u6539\u53d8\u4eba\u7c7b\u5728\u79d1\u5b66\u8fc7\u7a0b\u4e2d\u7684\u89d2\u8272\u3002", "method": "AI-Mandel\u4ece\u6587\u732e\u4e2d\u63d0\u53d6\u60f3\u6cd5\uff0c\u5e76\u4f7f\u7528\u7279\u5b9a\u9886\u57df\u7684AI\u5de5\u5177\u5c06\u5176\u8f6c\u5316\u4e3a\u53ef\u4ee5\u5728\u5b9e\u9a8c\u5ba4\u4e2d\u5b9e\u65bd\u7684\u5177\u4f53\u5b9e\u9a8c\u8bbe\u8ba1\u3002", "result": "AI-Mandel\u4ea7\u751f\u7684\u60f3\u6cd5\u901a\u5e38\u5177\u6709\u79d1\u5b66\u4ef7\u503c\uff0c\u5176\u4e2d\u4e24\u4e2a\u60f3\u6cd5\u5df2\u7ecf\u64b0\u5199\u4e86\u72ec\u7acb\u7684\u79d1\u5b66\u540e\u7eed\u8bba\u6587\u3002\u8fd9\u4e9b\u60f3\u6cd5\u5305\u62ec\u91cf\u5b50\u9690\u5f62\u4f20\u6001\u7684\u65b0\u53d8\u4f53\u3001\u4e0d\u5b9a\u56e0\u679c\u987a\u5e8f\u91cf\u5b50\u7f51\u7edc\u7684\u539f\u8bed\uff0c\u4ee5\u53ca\u57fa\u4e8e\u91cf\u5b50\u4fe1\u606f\u4f20\u8f93\u95ed\u73af\u7684\u51e0\u4f55\u76f8\u4f4d\u65b0\u6982\u5ff5\u3002", "conclusion": "AI-Mandel\u662f\u4e00\u4e2aAI\u7269\u7406\u5b66\u5bb6\u7684\u539f\u578b\u6f14\u793a\uff0c\u53ef\u4ee5\u4ea7\u751f\u548c\u5b9e\u65bd\u5177\u4f53\u7684\u3001\u53ef\u64cd\u4f5c\u7684\u60f3\u6cd5\u3002\u6784\u5efa\u8fd9\u6837\u7684\u7cfb\u7edf\u4e0d\u4ec5\u6709\u52a9\u4e8e\u52a0\u901f\u79d1\u5b66\u53d1\u5c55\uff0c\u800c\u4e14\u63ed\u793a\u4e86\u901a\u5f80\u4eba\u7c7b\u6c34\u5e73\u4eba\u5de5\u667a\u80fd\u79d1\u5b66\u5bb6\u7684\u5177\u4f53\u5f00\u653e\u6027\u6311\u6218\u3002"}}
{"id": "2511.11867", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11867", "abs": "https://arxiv.org/abs/2511.11867", "authors": ["Namu Park", "Giridhar Kaushik Ramachandran", "Kevin Lybarger", "Fei Xia", "Ozlem Uzuner", "Meliha Yetisgen", "Martin Gunn"], "title": "Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis of Traditional ML and LLM Approaches", "comment": "Submitted to LREC 2026", "summary": "Large language models (LLMs) have shown considerable promise in clinical natural language processing, yet few domain-specific datasets exist to rigorously evaluate their performance on radiology tasks. In this work, we introduce an annotated corpus of 6,393 radiology reports from 586 patients, each labeled for follow-up imaging status, to support the development and benchmarking of follow-up adherence detection systems. Using this corpus, we systematically compared traditional machine-learning classifiers, including logistic regression (LR), support vector machines (SVM), Longformer, and a fully fine-tuned Llama3-8B-Instruct, with recent generative LLMs. To evaluate generative LLMs, we tested GPT-4o and the open-source GPT-OSS-20B under two configurations: a baseline (Base) and a task-optimized (Advanced) setting that focused inputs on metadata, recommendation sentences, and their surrounding context. A refined prompt for GPT-OSS-20B further improved reasoning accuracy. Performance was assessed using precision, recall, and F1 scores with 95% confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 = 0.846). GPT-4o (Advanced) achieved the best performance (F1 = 0.832), followed closely by GPT-OSS-20B (Advanced; F1 = 0.828). LR and SVM also performed strongly (F1 = 0.776 and 0.775), underscoring that while LLMs approach human-level agreement through prompt optimization, interpretable and resource-efficient models remain valuable baselines.", "AI": {"tldr": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b6393\u4efd\u653e\u5c04\u62a5\u544a\u7684\u6807\u6ce8\u8bed\u6599\u5e93\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u653e\u5c04\u5b66\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u968f\u8bbf\u4f9d\u4ece\u6027\u68c0\u6d4b\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u7528\u4e8e\u4e25\u683c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u653e\u5c04\u5b66\u4efb\u52a1\u4e2d\u6027\u80fd\u7684\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\u3002", "method": "\u6bd4\u8f83\u4e86\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\uff08LR\u3001SVM\u3001Longformer\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08Llama3-8B-Instruct\u3001GPT-4o\u3001GPT-OSS-20B\uff09\u5728\u968f\u8bbf\u4f9d\u4ece\u6027\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u4f18\u5316prompt\u63d0\u9ad8LLM\u7684\u63a8\u7406\u51c6\u786e\u6027\u3002", "result": "GPT-4o\uff08Advanced\uff09\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\uff08F1 = 0.832\uff09\uff0c\u5176\u6b21\u662fGPT-OSS-20B\uff08Advanced; F1 = 0.828\uff09\uff0cLR\u548cSVM\u4e5f\u8868\u73b0\u51fa\u8272\uff08F1 = 0.776\u548c0.775\uff09\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u8fc7prompt\u4f18\u5316\u53ef\u4ee5\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\uff0c\u4f46\u53ef\u89e3\u91ca\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u6a21\u578b\u4ecd\u7136\u5177\u6709\u91cd\u8981\u7684\u57fa\u7ebf\u4ef7\u503c\u3002"}}
{"id": "2511.11708", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11708", "abs": "https://arxiv.org/abs/2511.11708", "authors": ["Pouya Shiri", "Amirali Baniasadi"], "title": "LE-CapsNet: A Light and Enhanced Capsule Network", "comment": null, "summary": "Capsule Network (CapsNet) classifier has several advantages over CNNs, including better detection of images containing overlapping categories and higher accuracy on transformed images. Despite the advantages, CapsNet is slow due to its different structure. In addition, CapsNet is resource-hungry, includes many parameters and lags in accuracy compared to CNNs. In this work, we propose LE-CapsNet as a light, enhanced and more accurate variant of CapsNet. Using 3.8M weights, LECapsNet obtains 76.73% accuracy on the CIFAR-10 dataset while performing inference 4x faster than CapsNet. In addition, our proposed network is more robust at detecting images with affine transformations compared to CapsNet. We achieve 94.3% accuracy on the AffNIST dataset (compared to CapsNet 90.52%).", "AI": {"tldr": "LE-CapsNet: A light, enhanced, and accurate CapsNet variant.", "motivation": "CapsNet is slow, resource-hungry, and sometimes less accurate than CNNs.", "method": "Propose LE-CapsNet, a modified CapsNet architecture.", "result": "LE-CapsNet achieves 76.73% accuracy on CIFAR-10 (4x faster than CapsNet) and 94.3% on AffNIST (vs. CapsNet's 90.52%).", "conclusion": "LE-CapsNet is faster, more accurate, and more robust than CapsNet, especially on transformed images."}}
{"id": "2511.11583", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.11583", "abs": "https://arxiv.org/abs/2511.11583", "authors": ["Fernando Spadea", "Oshani Seneviratne"], "title": "Parallel and Multi-Stage Knowledge Graph Retrieval for Behaviorally Aligned Financial Asset Recommendations", "comment": "10 pages, 3 figures, RAGE-KG 2025", "summary": "Large language models (LLMs) show promise for personalized financial recommendations but are hampered by context limits, hallucinations, and a lack of behavioral grounding. Our prior work, FLARKO, embedded structured knowledge graphs (KGs) in LLM prompts to align advice with user behavior and market data. This paper introduces RAG-FLARKO, a retrieval-augmented extension to FLARKO, that overcomes scalability and relevance challenges using multi-stage and parallel KG retrieval processes. Our method first retrieves behaviorally relevant entities from a user's transaction KG and then uses this context to filter temporally consistent signals from a market KG, constructing a compact, grounded subgraph for the LLM. This pipeline reduces context overhead and sharpens the model's focus on relevant information. Empirical evaluation on a real-world financial transaction dataset demonstrates that RAG-FLARKO significantly enhances recommendation quality. Notably, our framework enables smaller, more efficient models to achieve high performance in both profitability and behavioral alignment, presenting a viable path for deploying grounded financial AI in resource-constrained environments.", "AI": {"tldr": "RAG-FLARKO: An enhanced financial recommendation framework using retrieval-augmented KGs to improve LLM performance.", "motivation": "Context limits, hallucinations, and a lack of behavioral grounding in LLMs hinder personalized financial recommendations.", "method": "Multi-stage KG retrieval to filter behaviorally relevant entities and temporally consistent market signals.", "result": "Significantly enhanced recommendation quality, enabling smaller models to achieve high performance in profitability and behavioral alignment.", "conclusion": "RAG-FLARKO offers a viable approach for deploying grounded financial AI in resource-constrained environments."}}
{"id": "2511.12518", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12518", "abs": "https://arxiv.org/abs/2511.12518", "authors": ["Zhongchao Yi", "Kai Feng", "Xiaojian Ma", "Yalong Wang", "Yongqi Liu", "Han Li", "Zhengyang Zhou", "Yang Wang"], "title": "DualGR: Generative Retrieval with Long and Short-Term Interests Modeling", "comment": null, "summary": "In large-scale industrial recommendation systems, retrieval must produce high-quality candidates from massive corpora under strict latency. Recently, Generative Retrieval (GR) has emerged as a viable alternative to Embedding-Based Retrieval (EBR), which quantizes items into a finite token space and decodes candidates autoregressively, providing a scalable path that explicitly models target-history interactions via cross-attention. However, three challenges persist: 1) how to balance users' long-term and short-term interests , 2) noise interference when generating hierarchical semantic IDs (SIDs), 3) the absence of explicit modeling for negative feedback such as exposed items without clicks. To address these challenges, we propose DualGR, a generative retrieval framework that explicitly models dual horizons of user interests with selective activation. Specifically, DualGR utilizes Dual-Branch Long/Short-Term Router (DBR) to cover both stable preferences and transient intents by explicitly modeling users' long- and short-term behaviors. Meanwhile, Search-based SID Decoding (S2D) is presented to control context-induced noise and enhance computational efficiency by constraining candidate interactions to the current coarse (level-1) bucket during fine-grained (level-2/3) SID prediction. % also reinforcing intra-class consistency. Finally, we propose an Exposure-aware Next-Token Prediction Loss (ENTP-Loss) that treats \"exposed-but-unclicked\" items as hard negatives at level-1, enabling timely interest fade-out. On the large-scale Kuaishou short-video recommendation system, DualGR has achieved outstanding performance. Online A/B testing shows +0.527% video views and +0.432% watch time lifts, validating DualGR as a practical and effective paradigm for industrial generative retrieval.", "AI": {"tldr": "DualGR: \u4e00\u4e2a\u7528\u4e8e\u5de5\u4e1a\u751f\u6210\u68c0\u7d22\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u7528\u6237\u5174\u8da3\u7684\u53cc\u91cd\u89c6\u91ce\u6765\u89e3\u51b3\u73b0\u6709\u751f\u6210\u68c0\u7d22\u65b9\u6cd5\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u68c0\u7d22\u65b9\u6cd5\u5728\u5e73\u8861\u7528\u6237\u957f\u671f\u548c\u77ed\u671f\u5174\u8da3\u3001\u5904\u7406\u5206\u5c42\u8bed\u4e49ID (SIDs) \u751f\u6210\u4e2d\u7684\u566a\u58f0\u5e72\u6270\u4ee5\u53ca\u7f3a\u4e4f\u5bf9\u8d1f\u53cd\u9988\u7684\u663e\u5f0f\u5efa\u6a21\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa DualGR \u6846\u67b6\uff0c\u5229\u7528\u53cc\u5206\u652f\u957f/\u77ed\u671f\u8def\u7531\u5668 (DBR) \u8986\u76d6\u7a33\u5b9a\u504f\u597d\u548c\u77ac\u65f6\u610f\u56fe\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u641c\u7d22\u7684 SID \u89e3\u7801 (S2D) \u6765\u63a7\u5236\u4e0a\u4e0b\u6587\u8bf1\u5bfc\u7684\u566a\u58f0\uff0c\u5e76\u63d0\u51fa Exposure-aware Next-Token Prediction Loss (ENTP-Loss) \u6765\u5904\u7406\u8d1f\u53cd\u9988\u3002", "result": "\u5728\u5feb\u624b\u77ed\u89c6\u9891\u63a8\u8350\u7cfb\u7edf\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDualGR \u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u5728\u7ebf A/B \u6d4b\u8bd5\u663e\u793a\u89c6\u9891\u89c2\u770b\u6b21\u6570 +0.527%\uff0c\u89c2\u770b\u65f6\u957f +0.432%\u3002", "conclusion": "DualGR \u662f\u4e00\u4e2a\u7528\u4e8e\u5de5\u4e1a\u751f\u6210\u68c0\u7d22\u7684\u5b9e\u7528\u4e14\u6709\u6548\u7684\u8303\u4f8b\u3002"}}
{"id": "2511.11770", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11770", "abs": "https://arxiv.org/abs/2511.11770", "authors": ["Floris Vossebeld", "Shenghui Wang"], "title": "Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction", "comment": null, "summary": "Generating complex, logically-sound SPARQL queries for multi-hop questions remains a critical bottleneck for Knowledge Graph Question Answering, as the brittle nature of one-shot generation by Large Language Models (LLMs) hinders reliable interaction with structured data. Current methods lack the adaptive policies needed to dynamically debug queries based on real-time execution feedback. This paper introduces a novel agentic framework where an LLM learns a resilient policy for the sequential process of iterative SPARQL construction. We show that a compact 3B-parameter model, trained exclusively via outcome-driven Reinforcement Learning (GRPO) without supervised fine-tuning, can learn effective policies for this task, discovering how to systematically recover from execution errors and refine its queries toward a correct answer. On a curated, executable single-answer subset of LC-QuAD 2.0, our agent achieves 49.7\\% accuracy post-entity-linking, a significant 17.5 percentage point improvement over the strongest iterative zero-shot baseline. Further analysis reveals that while the agent's capability is driven by RL, its performance is enhanced by an explicit deliberative reasoning step that acts as a cognitive scaffold to improve policy precision. This work presents a generalizable blueprint for teaching agents to master formal, symbolic tools through interaction, bridging the gap between probabilistic LLMs and the structured world of Knowledge Graphs.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u96be\u4ee5\u751f\u6210\u590d\u6742\u3001\u903b\u8f91\u4e25\u5bc6\u7684 SPARQL \u67e5\u8be2\uff0c\u963b\u788d\u4e86\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u7684\u53ef\u9760\u6027\u3002\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 agentic \u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3 LLM\uff0c\u4f7f\u5176\u5b66\u4e60\u8fed\u4ee3 SPARQL \u6784\u5efa\u7684\u7b56\u7565\uff0c\u4ece\u800c\u5728 LC-QuAD 2.0 \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u7f3a\u4e4f\u81ea\u9002\u5e94\u7b56\u7565\uff0c\u65e0\u6cd5\u6839\u636e\u5b9e\u65f6\u6267\u884c\u53cd\u9988\u52a8\u6001\u8c03\u8bd5\u67e5\u8be2\u3002", "method": "\u8be5\u8bba\u6587\u5f15\u5165\u4e86\u4e00\u79cd agentic \u6846\u67b6\uff0cLLM \u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b66\u4e60\u8fed\u4ee3 SPARQL \u6784\u5efa\u7684\u7b56\u7565\u3002", "result": "\u5728 LC-QuAD 2.0 \u6570\u636e\u96c6\u4e0a\uff0c\u8be5 agent \u5b9e\u73b0\u4e86 49.7% \u7684\u51c6\u786e\u7387\uff0c\u6bd4\u6700\u5f3a\u7684\u8fed\u4ee3 zero-shot \u57fa\u7ebf\u63d0\u9ad8\u4e86 17.5 \u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u901a\u7528\u84dd\u56fe\uff0c\u901a\u8fc7\u4ea4\u4e92\u6559\u5bfc agent \u638c\u63e1\u5f62\u5f0f\u5316\u7684\u7b26\u53f7\u5de5\u5177\uff0c\u5f25\u5408\u4e86\u6982\u7387 LLM \u548c\u77e5\u8bc6\u56fe\u8c31\u7ed3\u6784\u5316\u4e16\u754c\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2511.11878", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11878", "abs": "https://arxiv.org/abs/2511.11878", "authors": ["Fernanda Bufon F\u00e4rber", "Iago Alves Brito", "Julia Soares Dollis", "Pedro Schindler Freire Brasil Ribeiro", "Rafael Teixeira Sousa", "Arlindo Rodrigues Galv\u00e3o Filho"], "title": "MedPT: A Massive Medical Question Answering Dataset for Brazilian-Portuguese Speakers", "comment": "11 pages, 3 tables, 2 figures", "summary": "While large language models (LLMs) show transformative potential in healthcare, their development remains focused on high-resource languages, creating a critical barrier for others as simple translation fails to capture unique clinical and cultural nuances, such as endemic diseases. To address this, we introduce MedPT, the first large-scale, real-world corpus for Brazilian Portuguese, comprising 384,095 authentic question-answer pairs from patient-doctor interactions. The dataset underwent a meticulous multi-stage curation protocol, using a hybrid quantitative-qualitative analysis to filter noise and contextually enrich thousands of ambiguous queries. We further augmented the corpus via LLM-driven annotation, classifying questions into seven semantic types to capture user intent. Our analysis reveals its thematic breadth (3,200 topics) and unique linguistic properties, like the natural asymmetry in patient-doctor communication. To validate its utility, we benchmark a medical specialty routing task: fine-tuning a 1.7B parameter model achieves an outstanding 94\\% F1-score on a 20-class setup. Furthermore, our qualitative error analysis shows misclassifications are not random but reflect genuine clinical ambiguities (e.g., between comorbid conditions), proving the dataset's deep semantic richness. We publicly release MedPT to foster the development of more equitable, accurate, and culturally-aware medical technologies for the Portuguese-speaking world.", "AI": {"tldr": "MedPT: A new large-scale Brazilian Portuguese corpus for healthcare LLMs.", "motivation": "Existing LLMs in healthcare are focused on high-resource languages, and simple translation fails to capture unique clinical and cultural nuances.", "method": "A large-scale, real-world corpus for Brazilian Portuguese was created, comprising 384,095 question-answer pairs from patient-doctor interactions. The dataset underwent a meticulous multi-stage curation protocol and was further augmented via LLM-driven annotation, classifying questions into seven semantic types to capture user intent.", "result": "The dataset has thematic breadth (3,200 topics) and unique linguistic properties. Fine-tuning a 1.7B parameter model achieves an outstanding 94% F1-score on a 20-class medical specialty routing task.", "conclusion": "The authors publicly release MedPT to foster the development of more equitable, accurate, and culturally-aware medical technologies for the Portuguese-speaking world."}}
{"id": "2511.11710", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.11710", "abs": "https://arxiv.org/abs/2511.11710", "authors": ["Zhou Xu", "Qi Wang", "Yuxiao Yang", "Luyuan Zhang", "Zhang Liang", "Yang Li"], "title": "Target-Balanced Score Distillation", "comment": null, "summary": "Score Distillation Sampling (SDS) enables 3D asset generation by distilling priors from pretrained 2D text-to-image diffusion models, but vanilla SDS suffers from over-saturation and over-smoothing. To mitigate this issue, recent variants have incorporated negative prompts. However, these methods face a critical trade-off: limited texture optimization, or significant texture gains with shape distortion. In this work, we first conduct a systematic analysis and reveal that this trade-off is fundamentally governed by the utilization of the negative prompts, where Target Negative Prompts (TNP) that embed target information in the negative prompts dramatically enhancing texture realism and fidelity but inducing shape distortions. Informed by this key insight, we introduce the Target-Balanced Score Distillation (TBSD). It formulates generation as a multi-objective optimization problem and introduces an adaptive strategy that effectively resolves the aforementioned trade-off. Extensive experiments demonstrate that TBSD significantly outperforms existing state-of-the-art methods, yielding 3D assets with high-fidelity textures and geometrically accurate shape.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Target-Balanced Score Distillation (TBSD) \u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3 Score Distillation Sampling (SDS) \u5728 3D \u8d44\u4ea7\u751f\u6210\u4e2d\u5b58\u5728\u7684\u8fc7\u9971\u548c\u3001\u8fc7\u5ea6\u5e73\u6ed1\u4ee5\u53ca\u7eb9\u7406\u4f18\u5316\u4e0e\u5f62\u72b6\u626d\u66f2\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684 SDS \u65b9\u6cd5\u5728 3D \u8d44\u4ea7\u751f\u6210\u4e2d\u5b58\u5728\u8fc7\u9971\u548c\u3001\u8fc7\u5ea6\u5e73\u6ed1\u7684\u95ee\u9898\uff0c\u5e76\u4e14\u5728\u7eb9\u7406\u4f18\u5316\u548c\u5f62\u72b6\u4fdd\u6301\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002Target Negative Prompts (TNP) \u867d\u7136\u53ef\u4ee5\u589e\u5f3a\u7eb9\u7406\u7684\u771f\u5b9e\u611f\uff0c\u4f46\u4f1a\u5bfc\u81f4\u5f62\u72b6\u626d\u66f2\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u53d1\u73b0\uff0c\u8d1f\u9762\u63d0\u793a\u7684\u4f7f\u7528\u662f\u5bfc\u81f4\u4e0a\u8ff0\u6743\u8861\u7684\u5173\u952e\u56e0\u7d20\u3002TBSD \u5c06\u751f\u6210\u8fc7\u7a0b \u0444\u043e\u0440\u043c\u0443\u043b\u0438\u0440\u0443\u0435\u0442 \u4e3a\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u7b56\u7565\u6765\u6709\u6548\u89e3\u51b3\u7eb9\u7406\u4f18\u5316\u548c\u5f62\u72b6\u626d\u66f2\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "TBSD \u5728\u7eb9\u7406\u4fdd\u771f\u5ea6\u548c\u51e0\u4f55\u5f62\u72b6\u7cbe\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "TBSD \u80fd\u591f\u751f\u6210\u5177\u6709\u9ad8\u4fdd\u771f\u7eb9\u7406\u548c\u51e0\u4f55\u5f62\u72b6\u7cbe\u786e\u7684 3D \u8d44\u4ea7\uff0c\u6709\u6548\u89e3\u51b3\u4e86 SDS \u65b9\u6cd5\u4e2d\u7684\u7eb9\u7406\u4f18\u5316\u4e0e\u5f62\u72b6\u626d\u66f2\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2511.11584", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.11584", "abs": "https://arxiv.org/abs/2511.11584", "authors": ["Jacob Drori", "Luke Marks", "Bryce Woodworth", "Alex Cloud", "Alexander Matt Turner"], "title": "Output Supervision Can Obfuscate the Chain of Thought", "comment": null, "summary": "OpenAI (2025) showed that training against a chain of thought (CoT) monitor can cause obfuscated CoTs, which contain bad behavior the monitor cannot detect. They proposed to keep CoTs monitorable by training only against output monitors that do not have access to CoT. We show that such training can still cause obfuscated CoTs via two mechanisms. First, when a model is trained to produce a safe-looking output, that model may generalize to making its CoTs look safe. Second, since later tokens are conditioned on earlier ones, safe-looking CoTs may increase the likelihood of safe outputs, causing safe-looking CoTs to be reinforced. We introduce two mitigations to address these two issues, which achieve a Pareto improvement in terms of monitorability and task performance compared to regular training.", "AI": {"tldr": "\u5373\u4f7f\u6ca1\u6709\u76f4\u63a5\u9488\u5bf9CoT\u8fdb\u884c\u8bad\u7ec3\uff0c\u6a21\u578b\u4ecd\u7136\u53ef\u80fd\u4ea7\u751f\u6df7\u6dc6\u7684CoT\uff0c\u5305\u542b\u76d1\u63a7\u5668\u65e0\u6cd5\u68c0\u6d4b\u5230\u7684\u4e0d\u826f\u884c\u4e3a\u3002", "motivation": "OpenAI (2025) \u8868\u660e\uff0c\u9488\u5bf9\u601d\u7ef4\u94fe (CoT) \u76d1\u63a7\u5668\u8fdb\u884c\u8bad\u7ec3\u53ef\u80fd\u5bfc\u81f4\u6df7\u6dc6\u7684CoT\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u4ec5\u9488\u5bf9\u8f93\u51fa\u76d1\u63a7\u5668\u8fdb\u884c\u8bad\u7ec3\u662f\u5426\u4ecd\u7136\u4f1a\u5bfc\u81f4\u6df7\u6dc6\u7684CoT\u3002", "method": "\u901a\u8fc7\u4e24\u79cd\u673a\u5236\u8fdb\u884c\u8bba\u8bc1\uff1a1) \u6a21\u578b\u6cdb\u5316\u5230\u4f7f\u5176CoT\u770b\u8d77\u6765\u5b89\u5168\uff1b2) \u5b89\u5168\u7684CoT\u53ef\u80fd\u589e\u52a0\u5b89\u5168\u8f93\u51fa\u7684\u53ef\u80fd\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u4e24\u79cd\u7f13\u89e3\u63aa\u65bd\uff0c\u4e0e\u5e38\u89c4\u8bad\u7ec3\u76f8\u6bd4\uff0c\u5728\u53ef\u76d1\u63a7\u6027\u548c\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u5b9e\u73b0\u4e86\u5e15\u7d2f\u6258\u6539\u8fdb\u3002", "conclusion": "\u5373\u4f7f\u4e0d\u76f4\u63a5\u9488\u5bf9CoT\u8fdb\u884c\u8bad\u7ec3\uff0c\u6a21\u578b\u4ecd\u7136\u53ef\u80fd\u901a\u8fc7\u591a\u79cd\u673a\u5236\u4ea7\u751f\u6df7\u6dc6\u7684CoT\u3002"}}
{"id": "2511.12597", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12597", "abs": "https://arxiv.org/abs/2511.12597", "authors": ["Mengyao Gao", "Chongming Gao", "Haoyan Liu", "Qingpeng Cai", "Peng Jiang", "Jiajia Chen", "Shuai Yuan", "Xiangnan He"], "title": "MindRec: Mind-inspired Coarse-to-fine Decoding for Generative Recommendation", "comment": null, "summary": "Recent advancements in large language model-based recommendation systems often represent items as text or semantic IDs and generate recommendations in an auto-regressive manner. However, due to the left-to-right greedy decoding strategy and the unidirectional logical flow, such methods often fail to produce globally optimal recommendations. In contrast, human reasoning does not follow a rigid left-to-right sequence. Instead, it often begins with keywords or intuitive insights, which are then refined and expanded. Inspired by this fact, we propose Mind-inspired Recommender (MindRec), a novel generative framework that emulates human thought processes. Particularly, our method first generates key tokens that reflect user preferences, and then expands them into the complete item, enabling flexible and human-like generation. To further emulate the structured nature of human decision-making, we organize items into a hierarchical category tree. This structure guides the model to first produce the coarse-grained category and then progressively refine its selection through finer-grained subcategories before generating the specific item. To mitigate the local optimum problem inherent in greedy decoding, we design a novel beam search algorithm, Diffusion Beam Search, tailored for our mind-inspired generation paradigm. Experimental results demonstrate that MindRec yields a 9.5\\% average improvement in top-1 recommendation performance over state-of-the-art methods, highlighting its potential to enhance recommendation accuracy. The implementation is available via https://github.com/Mr-Peach0301/MindRec.", "AI": {"tldr": "MindRec\uff1a\u4e00\u4e2a\u6a21\u4eff\u4eba\u7c7b\u601d\u7ef4\u7684\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5173\u952etokens\u5e76\u9010\u6b65\u7ec6\u5316\u4e3a\u5b8c\u6574\u9879\u76ee\u6765\u5b9e\u73b0\u66f4\u7075\u6d3b\u3001\u7c7b\u4eba\u7684\u63a8\u8350\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u8350\u7cfb\u7edf\u7531\u4e8e\u81ea\u56de\u5f52\u751f\u6210\u65b9\u5f0f\u548c\u5355\u5411\u903b\u8f91\u6d41\uff0c\u96be\u4ee5\u4ea7\u751f\u5168\u5c40\u6700\u4f18\u7684\u63a8\u8350\u3002", "method": "1.  \u63d0\u51fa Mind-inspired Recommender (MindRec)\uff0c\u9996\u5148\u751f\u6210\u53cd\u6620\u7528\u6237\u504f\u597d\u7684\u5173\u952e tokens\uff0c\u7136\u540e\u5c06\u5b83\u4eec\u6269\u5c55\u4e3a\u5b8c\u6574\u7684\u9879\u76ee\u3002\n2.  \u5c06\u9879\u76ee\u7ec4\u7ec7\u6210\u5c42\u7ea7\u7c7b\u522b\u6811\uff0c\u5f15\u5bfc\u6a21\u578b\u9996\u5148\u751f\u6210\u7c97\u7c92\u5ea6\u7684\u7c7b\u522b\uff0c\u7136\u540e\u901a\u8fc7\u66f4\u7ec6\u7c92\u5ea6\u7684\u5b50\u7c7b\u522b\u9010\u6b65\u7ec6\u5316\u9009\u62e9\uff0c\u6700\u540e\u751f\u6210\u7279\u5b9a\u7684\u9879\u76ee\u3002\n3.  \u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684 beam search \u7b97\u6cd5\uff0cDiffusion Beam Search\uff0c\u4e13\u4e3a mind-inspired \u751f\u6210\u8303\u5f0f\u5b9a\u5236\uff0c\u4ee5\u7f13\u89e3\u8d2a\u5a6a\u89e3\u7801\u4e2d\u56fa\u6709\u7684\u5c40\u90e8\u6700\u4f18\u95ee\u9898\u3002", "result": "MindRec \u5728 top-1 \u63a8\u8350\u6027\u80fd\u4e0a\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u5e73\u5747\u63d0\u9ad8\u4e86 9.5%\u3002", "conclusion": "MindRec \u5177\u6709\u589e\u5f3a\u63a8\u8350\u51c6\u786e\u6027\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.11773", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11773", "abs": "https://arxiv.org/abs/2511.11773", "authors": ["Ruchira Dhar", "Ninell Oldenburg", "Anders Soegaard"], "title": "On the Measure of a Model: From Intelligence to Generality", "comment": "Accepted at EurIPS Workshop on \"The Science of Benchmarking and Evaluating AI\"", "summary": "Benchmarks such as ARC, Raven-inspired tests, and the Blackbird Task are widely used to evaluate the intelligence of large language models (LLMs). Yet, the concept of intelligence remains elusive- lacking a stable definition and failing to predict performance on practical tasks such as question answering, summarization, or coding. Optimizing for such benchmarks risks misaligning evaluation with real-world utility. Our perspective is that evaluation should be grounded in generality rather than abstract notions of intelligence. We identify three assumptions that often underpin intelligence-focused evaluation: generality, stability, and realism. Through conceptual and formal analysis, we show that only generality withstands conceptual and empirical scrutiny. Intelligence is not what enables generality; generality is best understood as a multitask learning problem that directly links evaluation to measurable performance breadth and reliability. This perspective reframes how progress in AI should be assessed and proposes generality as a more stable foundation for evaluating capability across diverse and evolving tasks.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u8ba4\u4e3a\uff0c\u5f53\u524d\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u667a\u80fd\u7684\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982ARC\u3001Raven\u6d4b\u8bd5\u548cBlackbird Task\uff09\u5b58\u5728\u95ee\u9898\uff0c\u56e0\u4e3a\u5b83\u4eec\u672a\u80fd\u4e0e\u5b9e\u9645\u4efb\u52a1\uff08\u5982\u95ee\u7b54\u3001\u6458\u8981\u6216\u7f16\u7801\uff09\u7684\u6027\u80fd\u76f8\u5173\u8054\u3002\u8bba\u6587\u63d0\u51fa\u8bc4\u4f30\u5e94\u8be5\u57fa\u4e8e\u901a\u7528\u6027\uff0c\u800c\u975e\u62bd\u8c61\u7684\u667a\u80fd\u6982\u5ff5\u3002", "motivation": "\u5f53\u524d\u5bf9LLM\u667a\u80fd\u7684\u8bc4\u4f30\u7f3a\u4e4f\u7a33\u5b9a\u5b9a\u4e49\uff0c\u4e14\u65e0\u6cd5\u9884\u6d4b\u5b9e\u9645\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5bfc\u81f4\u8bc4\u4f30\u4e0e\u73b0\u5b9e\u4e16\u754c\u7684\u6548\u7528\u4e0d\u7b26\u3002", "method": "\u901a\u8fc7\u6982\u5ff5\u548c\u5f62\u5f0f\u5206\u6790\uff0c\u8bba\u6587\u8003\u5bdf\u4e86\u901a\u7528\u6027\u3001\u7a33\u5b9a\u6027\u3001\u73b0\u5b9e\u6027\u8fd9\u4e09\u4e2a\u901a\u5e38\u7528\u4e8e\u667a\u80fd\u8bc4\u4f30\u7684\u5047\u8bbe\uff0c\u5e76\u8bba\u8bc1\u53ea\u6709\u901a\u7528\u6027\u80fd\u591f\u7ecf\u53d7\u4f4f\u6982\u5ff5\u548c\u5b9e\u8bc1\u7684\u63a8\u6572\u3002", "result": "\u8bba\u6587\u6307\u51fa\uff0c\u901a\u7528\u6027\u6700\u597d\u88ab\u7406\u89e3\u4e3a\u4e00\u4e2a\u591a\u4efb\u52a1\u5b66\u4e60\u95ee\u9898\uff0c\u5b83\u76f4\u63a5\u5c06\u8bc4\u4f30\u4e0e\u53ef\u8861\u91cf\u7684\u6027\u80fd\u5e7f\u5ea6\u548c\u53ef\u9760\u6027\u8054\u7cfb\u8d77\u6765\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u8ba4\u4e3a\uff0c\u5e94\u8be5\u91cd\u65b0\u6784\u5efaAI\u8fdb\u5c55\u7684\u8bc4\u4f30\u65b9\u5f0f\uff0c\u5e76\u63d0\u51fa\u901a\u7528\u6027\u662f\u8bc4\u4f30\u8de8\u4e0d\u540c\u548c\u4e0d\u65ad\u53d1\u5c55\u7684\u4efb\u52a1\u7684\u80fd\u529b\u7684\u66f4\u7a33\u5b9a\u7684\u57fa\u7840\u3002"}}
{"id": "2511.11883", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11883", "abs": "https://arxiv.org/abs/2511.11883", "authors": ["Karthikeyan K", "Raghuveer Thirukovalluru", "David Carlson"], "title": "ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts", "comment": null, "summary": "Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aClinStructor\u7684\u6d41\u7a0b\uff0c\u5b83\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5c06\u4e34\u5e8a\u81ea\u7531\u6587\u672c\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7684\u3001\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u95ee\u7b54\u5bf9\uff0c\u7136\u540e\u518d\u8fdb\u884c\u9884\u6d4b\u5efa\u6a21\u3002", "motivation": "\u4e34\u5e8a\u7b14\u8bb0\u5305\u542b\u6709\u4ef7\u503c\u7684\u3001\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u4fe1\u606f\uff0c\u4f46\u5176\u975e\u7ed3\u6784\u5316\u683c\u5f0f\u4f1a\u5e26\u6765\u4e00\u4e9b\u6311\u6218\uff0c\u5305\u62ec\u65e0\u610f\u7684\u504f\u89c1\uff08\u4f8b\u5982\uff0c\u6027\u522b\u6216\u79cd\u65cf\u504f\u89c1\uff09\uff0c\u4ee5\u53ca\u8de8\u4e34\u5e8a\u73af\u5883\u7684\u6cdb\u5316\u80fd\u529b\u5dee\uff08\u4f8b\u5982\uff0c\u5728\u4e00\u4e2aEHR\u7cfb\u7edf\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u53e6\u4e00\u4e2a\u7cfb\u7edf\u4e0a\u53ef\u80fd\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u683c\u5f0f\u4e0d\u540c\uff09\u4ee5\u53ca\u53ef\u89e3\u91ca\u6027\u5dee\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86ClinStructor\uff0c\u4e00\u4e2a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5c06\u4e34\u5e8a\u81ea\u7531\u6587\u672c\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7684\u3001\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u95ee\u7b54\u5bf9\u7684\u6d41\u7a0b\uff0c\u7136\u540e\u518d\u8fdb\u884c\u9884\u6d4b\u5efa\u6a21\u3002", "result": "\u4e0e\u76f4\u63a5\u5fae\u8c03\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5927\u5927\u63d0\u9ad8\u4e86\u900f\u660e\u5ea6\u548c\u53ef\u63a7\u6027\uff0c\u5e76\u4e14\u4ec5\u5bfc\u81f4\u9884\u6d4b\u6027\u80fd\u7565\u6709\u4e0b\u964d\uff08AUC\u4e0b\u964d2-3%\uff09\u3002", "conclusion": "ClinStructor\u4e3a\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u6784\u5efa\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u63a8\u5e7f\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2511.11716", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11716", "abs": "https://arxiv.org/abs/2511.11716", "authors": ["Sudhakar Sah", "Nikhil Chabbra", "Matthieu Durnerin"], "title": "CompressNAS : A Fast and Efficient Technique for Model Compression using Decomposition", "comment": "11 pages, 6 figures", "summary": "Deep Convolutional Neural Networks (CNNs) are increasingly difficult to deploy on microcontrollers (MCUs) and lightweight NPUs (Neural Processing Units) due to their growing size and compute demands. Low-rank tensor decomposition, such as Tucker factorization, is a promising way to reduce parameters and operations with reasonable accuracy loss. However, existing approaches select ranks locally and often ignore global trade-offs between compression and accuracy. We introduce CompressNAS, a MicroNAS-inspired framework that treats rank selection as a global search problem. CompressNAS employs a fast accuracy estimator to evaluate candidate decompositions, enabling efficient yet exhaustive rank exploration under memory and accuracy constraints. In ImageNet, CompressNAS compresses ResNet-18 by 8x with less than 4% accuracy drop; on COCO, we achieve 2x compression of YOLOv5s without any accuracy drop and 2x compression of YOLOv5n with a 2.5% drop. Finally, we present a new family of compressed models, STResNet, with competitive performance compared to other efficient models.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCompressNAS\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u538b\u7f29\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u4f7f\u5176\u66f4\u6613\u4e8e\u5728\u5fae\u63a7\u5236\u5668\u548c\u8f7b\u91cf\u7ea7\u795e\u7ecf\u5904\u7406\u5355\u5143\u4e0a\u90e8\u7f72\u3002", "motivation": "\u73b0\u6709\u7684\u4f4e\u79e9\u5f20\u91cf\u5206\u89e3\u65b9\u6cd5\u5728\u9009\u62e9\u79e9\u65f6\u901a\u5e38\u5ffd\u7565\u538b\u7f29\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u5168\u5c40\u6743\u8861\uff0c\u5bfc\u81f4\u538b\u7f29\u6548\u679c\u4e0d\u4f73\u3002", "method": "CompressNAS\u5c06\u79e9\u9009\u62e9\u89c6\u4e3a\u4e00\u4e2a\u5168\u5c40\u641c\u7d22\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u5feb\u901f\u51c6\u786e\u6027\u4f30\u8ba1\u5668\u6765\u8bc4\u4f30\u5019\u9009\u5206\u89e3\u3002", "result": "\u5728ImageNet\u4e0a\uff0cCompressNAS\u5c06ResNet-18\u538b\u7f29\u4e868\u500d\uff0c\u51c6\u786e\u7387\u4e0b\u964d\u4e0d\u52304%\uff1b\u5728COCO\u4e0a\uff0cYOLOv5s\u5b9e\u73b0\u4e862\u500d\u538b\u7f29\u4e14\u65e0\u7cbe\u5ea6\u4e0b\u964d\uff0cYOLOv5n\u5b9e\u73b0\u4e862\u500d\u538b\u7f29\u4e14\u7cbe\u5ea6\u4e0b\u964d2.5%\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u538b\u7f29\u6a21\u578b\u7cfb\u5217\uff0cSTResNet\uff0c\u4e0e\u5176\u4ed6\u9ad8\u6548\u6a21\u578b\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u4f18\u52bf\u3002"}}
{"id": "2511.11585", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11585", "abs": "https://arxiv.org/abs/2511.11585", "authors": ["Kabir Khan", "Manju Sarkar", "Anita Kar", "Suresh Ghosh"], "title": "Parameter-Efficient and Personalized Federated Training of Generative Models at the Edge", "comment": "37 pages, 8 figures", "summary": "Large generative models (for example, language and diffusion models) enable high-quality text and image synthesis but are hard to train or adapt in cross-device federated settings due to heavy computation and communication and statistical/system heterogeneity. We propose FedGen-Edge, a framework that decouples a frozen, pre-trained global backbone from lightweight client-side adapters and federates only the adapters. Using Low-Rank Adaptation (LoRA) constrains client updates to a compact subspace, which reduces uplink traffic by more than 99 percent versus full-model FedAvg, stabilizes aggregation under non-IID data, and naturally supports personalization because each client can keep a locally tuned adapter. On language modeling (PTB) and image generation (CIFAR-10), FedGen-Edge achieves lower perplexity/FID and faster convergence than strong baselines while retaining a simple FedAvg-style server. A brief ablation shows diminishing returns beyond moderate LoRA rank and a trade-off between local epochs and client drift. FedGen-Edge offers a practical path toward privacy-preserving, resource-aware, and personalized generative AI on heterogeneous edge devices.", "AI": {"tldr": "FedGen-Edge: A framework for federated training of large generative models by decoupling a frozen global backbone from lightweight client-side adapters, federating only the adapters.", "motivation": "Training large generative models in federated settings is challenging due to heavy computation, communication, and data heterogeneity.", "method": "Proposes FedGen-Edge, which uses Low-Rank Adaptation (LoRA) to constrain client updates to a compact subspace.", "result": "Achieves lower perplexity/FID and faster convergence than strong baselines on language modeling (PTB) and image generation (CIFAR-10), while reducing uplink traffic by more than 99 percent compared to full-model FedAvg.", "conclusion": "FedGen-Edge offers a practical approach for privacy-preserving, resource-aware, and personalized generative AI on heterogeneous edge devices."}}
{"id": "2511.12922", "categories": ["cs.IR", "cs.AI", "cs.LG", "cs.NE", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.12922", "abs": "https://arxiv.org/abs/2511.12922", "authors": ["Yu Hou", "Won-Yong Shin"], "title": "Tokenize Once, Recommend Anywhere: Unified Item Tokenization for Multi-domain LLM-based Recommendation", "comment": "20 pages, 8 figures, 9 tables; Annual AAAI Conference on Artificial Intelligence (AAAI-26) (to appear) (Please cite our conference version.)", "summary": "Large language model (LLM)-based recommender systems have achieved high-quality performance by bridging the discrepancy between the item space and the language space through item tokenization. However, existing item tokenization methods typically require training separate models for each item domain, limiting generalization. Moreover, the diverse distributions and semantics across item domains make it difficult to construct a unified tokenization that preserves domain-specific information. To address these challenges, we propose UniTok, a Unified item Tokenization framework that integrates our own mixture-of-experts (MoE) architecture with a series of codebooks to convert items into discrete tokens, enabling scalable tokenization while preserving semantic information across multiple item domains. Specifically, items from different domains are first projected into a unified latent space through a shared encoder. They are then routed to domain-specific experts to capture the unique semantics, while a shared expert, which is always active, encodes common knowledge transferable across domains. Additionally, to mitigate semantic imbalance across domains, we present a mutual information calibration mechanism, which guides the model towards retaining similar levels of semantic information for each domain. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed UniTok framework is (a) highly effective: achieving up to 51.89% improvements over strong benchmarks, (b) theoretically sound: showing the analytical validity of our architectural design and optimization; and (c) highly generalizable: demonstrating robust performance across diverse domains without requiring per-domain retraining, a capability not supported by existing baselines.", "AI": {"tldr": "UniTok\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u9879\u76ee\u6807\u8bb0\u6846\u67b6\uff0c\u5b83\u96c6\u6210\u4e86MoE\u67b6\u6784\u548c\u4e00\u7cfb\u5217\u4ee3\u7801\u7c3f\uff0c\u4ee5\u5c06\u9879\u76ee\u8f6c\u6362\u4e3a\u79bb\u6563\u7684token\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u6807\u8bb0\u5316\uff0c\u540c\u65f6\u4fdd\u7559\u8de8\u591a\u4e2a\u9879\u76ee\u57df\u7684\u8bed\u4e49\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u7684\u9879\u76ee\u6807\u8bb0\u5316\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4e3a\u6bcf\u4e2a\u9879\u76ee\u57df\u8bad\u7ec3\u5355\u72ec\u7684\u6a21\u578b\uff0c\u4ece\u800c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8de8\u9879\u76ee\u57df\u7684\u5404\u79cd\u5206\u5e03\u548c\u8bed\u4e49\u4f7f\u5f97\u6784\u5efa\u80fd\u591f\u4fdd\u7559\u9886\u57df\u7279\u5b9a\u4fe1\u606f\u7684\u7edf\u4e00\u6807\u8bb0\u5316\u53d8\u5f97\u56f0\u96be\u3002", "method": "UniTok\u6846\u67b6\u9996\u5148\u901a\u8fc7\u5171\u4eab\u7f16\u7801\u5668\u5c06\u6765\u81ea\u4e0d\u540c\u9886\u57df\u7684\u9879\u76ee\u6295\u5f71\u5230\u7edf\u4e00\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u3002\u7136\u540e\uff0c\u5b83\u4eec\u88ab\u8def\u7531\u5230\u9886\u57df\u7279\u5b9a\u7684\u4e13\u5bb6\u4ee5\u6355\u83b7\u72ec\u7279\u7684\u8bed\u4e49\uff0c\u800c\u59cb\u7ec8\u5904\u4e8e\u6d3b\u52a8\u72b6\u6001\u7684\u5171\u4eab\u4e13\u5bb6\u5219\u5bf9\u8de8\u9886\u57df\u7684\u53ef\u8f6c\u79fb\u7684\u5e38\u8bc6\u8fdb\u884c\u7f16\u7801\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u51cf\u8f7b\u8de8\u9886\u57df\u7684\u8bed\u4e49\u4e0d\u5e73\u8861\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e92\u4fe1\u606f\u6821\u51c6\u673a\u5236\uff0c\u8be5\u673a\u5236\u5f15\u5bfc\u6a21\u578b\u4e3a\u6bcf\u4e2a\u9886\u57df\u4fdd\u7559\u76f8\u4f3c\u6c34\u5e73\u7684\u8bed\u4e49\u4fe1\u606f\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684UniTok\u6846\u67b6\u662f\u9ad8\u5ea6\u6709\u6548\u7684\uff1a\u4e0e\u5f3a\u5927\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe51.89%\u7684\u6539\u8fdb\u3002", "conclusion": "UniTok\u6846\u67b6\u5177\u6709\u9ad8\u5ea6\u7684\u901a\u7528\u6027\uff1a\u5728\u65e0\u9700\u6bcf\u4e2a\u9886\u57df\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u8bc1\u660e\u4e86\u8de8\u4e0d\u540c\u9886\u57df\u7684\u5f3a\u5927\u6027\u80fd\uff0c\u8fd9\u662f\u73b0\u6709\u57fa\u7ebf\u4e0d\u652f\u6301\u7684\u529f\u80fd\u3002"}}
{"id": "2511.11816", "categories": ["cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.11816", "abs": "https://arxiv.org/abs/2511.11816", "authors": ["Andrea Brunello", "Luca Geatti", "Michele Mignani", "Angelo Montanari", "Nicola Saccomanno"], "title": "Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy", "comment": "Full version of the paper accepted for publication at The 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u81ea\u7136\u8bed\u8a00\u5230\u4e00\u9636\u903b\u8f91 (NL-FOL) \u7684\u7ffb\u8bd1\u95ee\u9898\uff0c\u5e76 \u043a\u0440\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u8bc4\u4f30\u4e86\u73b0\u6709\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u65b9\u6848\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6267\u884c NL-FOL \u7ffb\u8bd1\u7684\u80fd\u529b\u5b58\u5728\u4e89\u8bae\u3002\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u51c6\u786e\u53cd\u6620 LLM \u7684\u5b9e\u9645\u80fd\u529b\u3002", "method": "1. \u6279\u5224\u6027\u5730\u68c0\u67e5\u73b0\u6709\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\uff1b2. \u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u4ee5\u533a\u5206\u771f\u6b63\u7684\u8bed\u4e49\u7406\u89e3\u548c\u8868\u9762\u6a21\u5f0f\u8bc6\u522b\uff1b3. \u4f7f\u7528\u65b0\u65b9\u6cd5\u8bc4\u4f30\u6700\u5148\u8fdb\u7684 LLM\u3002", "result": "\u5bf9\u8bdd\u5bfc\u5411\u7684 LLM \u8868\u73b0\u51fa\u5f3a\u5927\u7684 NL-FOL \u7ffb\u8bd1\u6280\u80fd\u548c\u5bf9\u53e5\u5b50\u7ea7\u903b\u8f91\u7684\u771f\u6b63\u7406\u89e3\uff0c\u800c\u4ee5\u5d4c\u5165\u4e3a\u4e2d\u5fc3\u7684\u6a21\u578b\u8868\u73b0\u660e\u663e\u8f83\u5dee\u3002", "conclusion": "\u6700\u5148\u8fdb\u7684\u5bf9\u8bdd\u5bfc\u5411\u7684 LLM \u80fd\u591f\u8fdb\u884c NL-FOL \u7ffb\u8bd1\uff0c\u5e76\u4e14\u771f\u6b63\u7406\u89e3\u53e5\u5b50\u7ea7\u522b\u7684\u903b\u8f91\u3002"}}
{"id": "2511.11884", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11884", "abs": "https://arxiv.org/abs/2511.11884", "authors": ["Eric Hua Qing Zhang", "Julia Ive"], "title": "Context-Emotion Aware Therapeutic Dialogue Generation: A Multi-component Reinforcement Learning Approach to Language Models for Mental Health Support", "comment": null, "summary": "Mental health illness represents a substantial global socioeconomic burden, with COVID-19 further exacerbating accessibility challenges and driving increased demand for telehealth mental health support. While large language models (LLMs) offer promising solutions through 24/7 availability and non-judgmental interactions, pre-trained models often lack the contextual and emotional awareness necessary for appropriate therapeutic responses. This paper investigated the application of supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance GPT-2's capacity for therapeutic dialogue generation. The methodology restructured input formats to enable simultaneous processing of contextual information and emotional states alongside user input, employing a multi-component reward function that aligned model outputs with professional therapist responses and annotated emotions. Results demonstrated improvements through reinforcement learning over baseline GPT-2 across multiple evaluation metrics: BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581). LLM evaluation confirmed high contextual relevance and professionalism, while reinforcement learning achieved 99.34% emotion accuracy compared to 66.96% for baseline GPT-2. These findings demonstrate reinforcement learning's effectiveness in developing therapeutic dialogue systems that can serve as valuable assistive tools for therapists while maintaining essential human clinical oversight.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u4f7f\u7528\u76d1\u7763\u5fae\u8c03 (SFT) \u548c\u5f3a\u5316\u5b66\u4e60 (RL) \u6280\u672f\u6765\u589e\u5f3a GPT-2 \u7684\u6cbb\u7597\u5bf9\u8bdd\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u7cbe\u795e\u5065\u5eb7\u75be\u75c5\u9020\u6210\u4e86\u5de8\u5927\u7684\u5168\u7403\u793e\u4f1a\u7ecf\u6d4e\u8d1f\u62c5\uff0c\u800c COVID-19 \u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u53ef\u53ca\u6027\u6311\u6218\uff0c\u5e76\u63a8\u52a8\u4e86\u5bf9\u8fdc\u7a0b\u533b\u7597\u7cbe\u795e\u5065\u5eb7\u652f\u6301\u7684\u9700\u6c42\u3002", "method": "\u8be5\u65b9\u6cd5\u91cd\u6784\u4e86\u8f93\u5165\u683c\u5f0f\uff0c\u4ee5\u5b9e\u73b0\u540c\u65f6\u5904\u7406\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u60c5\u7eea\u72b6\u6001\u4ee5\u53ca\u7528\u6237\u8f93\u5165\uff0c\u91c7\u7528\u4e86\u4e00\u79cd\u591a\u7ec4\u4ef6\u5956\u52b1\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u5c06\u6a21\u578b\u8f93\u51fa\u4e0e\u4e13\u4e1a\u6cbb\u7597\u5e08\u7684\u53cd\u5e94\u548c\u5e26\u6ce8\u91ca\u7684\u60c5\u7eea\u5bf9\u9f50\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf GPT-2\uff1aBLEU (0.0111)\u3001ROUGE-1 (0.1397)\u3001ROUGE-2 (0.0213)\u3001ROUGE-L (0.1317) \u548c METEOR (0.0581)\u3002LLM \u8bc4\u4f30\u8bc1\u5b9e\u4e86\u9ad8\u5ea6\u7684\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u548c\u4e13\u4e1a\u6027\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u4e86 99.34% \u7684\u60c5\u7eea\u51c6\u786e\u7387\uff0c\u800c\u57fa\u7ebf GPT-2 \u4e3a 66.96%\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8bc1\u660e\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u5f00\u53d1\u6cbb\u7597\u5bf9\u8bdd\u7cfb\u7edf\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u53ef\u4ee5\u4f5c\u4e3a\u6cbb\u7597\u5e08\u7684\u6709\u4ef7\u503c\u7684\u8f85\u52a9\u5de5\u5177\uff0c\u540c\u65f6\u4fdd\u6301\u5fc5\u8981\u7684\u4eba\u5de5\u4e34\u5e8a\u76d1\u7763\u3002"}}
{"id": "2511.11720", "categories": ["cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.11720", "abs": "https://arxiv.org/abs/2511.11720", "authors": ["Jiao Chen", "Haoyi Wang", "Jianhua Tang", "Junyi Wang"], "title": "AdaptFly: Prompt-Guided Adaptation of Foundation Models for Low-Altitude UAV Networks", "comment": null, "summary": "Low-altitude Unmanned Aerial Vehicle (UAV) networks rely on robust semantic segmentation as a foundational enabler for distributed sensing-communication-control co-design across heterogeneous agents within the network. However, segmentation foundation models deteriorate quickly under weather, lighting, and viewpoint drift. Resource-limited UAVs cannot run gradient-based test-time adaptation, while resource-massive UAVs adapt independently, wasting shared experience. To address these challenges, we propose AdaptFly, a prompt-guided test-time adaptation framework that adjusts segmentation models without weight updates. AdaptFly features two complementary adaptation modes. For resource-limited UAVs, it employs lightweight token-prompt retrieval from a shared global memory. For resource-massive UAVs, it uses gradient-free sparse visual prompt optimization via Covariance Matrix Adaptation Evolution Strategy. An activation-statistic detector triggers adaptation, while cross-UAV knowledge pool consolidates prompt knowledge and enables fleet-wide collaboration with negligible bandwidth overhead. Extensive experiments on UAVid and VDD benchmarks, along with real-world UAV deployments under diverse weather conditions, demonstrate that AdaptFly significantly improves segmentation accuracy and robustness over static models and state-of-the-art TTA baselines. The results highlight a practical path to resilient, communication-efficient perception in the emerging low-altitude economy.", "AI": {"tldr": "AdaptFly is a prompt-guided test-time adaptation framework that adjusts segmentation models without weight updates.", "motivation": "Segmentation foundation models deteriorate quickly under weather, lighting, and viewpoint drift, and resource-limited UAVs cannot run gradient-based test-time adaptation, while resource-massive UAVs adapt independently, wasting shared experience.", "method": "AdaptFly features two complementary adaptation modes: lightweight token-prompt retrieval from a shared global memory for resource-limited UAVs, and gradient-free sparse visual prompt optimization via Covariance Matrix Adaptation Evolution Strategy for resource-massive UAVs. An activation-statistic detector triggers adaptation, while cross-UAV knowledge pool consolidates prompt knowledge.", "result": "AdaptFly significantly improves segmentation accuracy and robustness over static models and state-of-the-art TTA baselines on UAVid and VDD benchmarks, along with real-world UAV deployments under diverse weather conditions.", "conclusion": "AdaptFly provides a practical path to resilient, communication-efficient perception in the emerging low-altitude economy."}}
{"id": "2511.11589", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11589", "abs": "https://arxiv.org/abs/2511.11589", "authors": ["Chenyue Liu", "Ali Mostafavi"], "title": "WildfireGenome: Interpretable Machine Learning Reveals Local Drivers of Wildfire Risk and Their Cross-County Variation", "comment": null, "summary": "Current wildfire risk assessments rely on coarse hazard maps and opaque machine learning models that optimize regional accuracy while sacrificing interpretability at the decision scale. WildfireGenome addresses these gaps through three components: (1) fusion of seven federal wildfire indicators into a sign-aligned, PCA-based composite risk label at H3 Level-8 resolution; (2) Random Forest classification of local wildfire risk; and (3) SHAP and ICE/PDP analyses to expose county-specific nonlinear driver relationships. Across seven ecologically diverse U.S. counties, models achieve accuracies of 0.755-0.878 and Quadratic Weighted Kappa up to 0.951, with principal components explaining 87-94% of indicator variance. Transfer tests show reliable performance between ecologically similar regions but collapse across dissimilar contexts. Explanations consistently highlight needleleaf forest cover and elevation as dominant drivers, with risk rising sharply at 30-40% needleleaf coverage. WildfireGenome advances wildfire risk assessment from regional prediction to interpretable, decision-scale analytics that guide vegetation management, zoning, and infrastructure planning.", "AI": {"tldr": "WildfireGenome combines wildfire indicators, machine learning, and explainable AI to provide interpretable, decision-scale risk assessments.", "motivation": "Current wildfire risk assessments lack interpretability at the local level.", "method": "Fusion of wildfire indicators, Random Forest classification, and SHAP/ICE/PDP analyses.", "result": "Models achieve accuracies of 0.755-0.878 and Quadratic Weighted Kappa up to 0.951, with principal components explaining 87-94% of indicator variance. Needleleaf forest cover and elevation are key drivers.", "conclusion": "WildfireGenome enables interpretable, decision-scale wildfire risk assessment for better vegetation management, zoning, and infrastructure planning."}}
{"id": "2511.12947", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12947", "abs": "https://arxiv.org/abs/2511.12947", "authors": ["Hao Jiang", "Guoquan Wang", "Sheng Yu", "Yang Zeng", "Wencong Zeng", "Guorui Zhou"], "title": "A Plug-and-Play Spatially-Constrained Representation Enhancement Framework for Local-Life Recommendation", "comment": null, "summary": "Local-life recommendation have witnessed rapid growth, providing users with convenient access to daily essentials. However, this domain faces two key challenges: (1) spatial constraints, driven by the requirements of the local-life scenario, where items are usually shown only to users within a limited geographic area, indirectly reducing their exposure probability; and (2) long-tail sparsity, where few popular items dominate user interactions, while many high-quality long-tail items are largely overlooked due to imbalanced interaction opportunities. Existing methods typically adopt a user-centric perspective, such as modeling spatial user preferences or enhancing long-tail representations with collaborative filtering signals. However, we argue that an item-centric perspective is more suitable for this domain, focusing on enhancing long-tail items representation that align with the spatially-constrained characteristics of local lifestyle services. To tackle this issue, we propose ReST, a Plug-And-Play Spatially-Constrained Representation Enhancement Framework for Long-Tail Local-Life Recommendation. Specifically, we first introduce a Meta ID Warm-up Network, which initializes fundamental ID representations by injecting their basic attribute-level semantic information. Subsequently, we propose a novel Spatially-Constrained ID Representation Enhancement Network (SIDENet) based on contrastive learning, which incorporates two efficient strategies: a spatially-constrained hard sampling strategy and a dynamic representation alignment strategy. This design adaptively identifies weak ID representations based on their attribute-level information during training. It additionally enhances them by capturing latent item relationships within the spatially-constrained characteristics of local lifestyle services, while preserving compatibility with popular items.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aReST\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u672c\u5730\u751f\u6d3b\u63a8\u8350\u4e2d\u7684\u7a7a\u95f4\u7ea6\u675f\u548c\u957f\u5c3e\u7a00\u758f\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\uff0c\u5ffd\u7565\u4e86\u9879\u76ee\u672c\u8eab\u7684\u7a7a\u95f4\u7ea6\u675f\u7279\u6027\u548c\u957f\u5c3e\u5546\u54c1\u7684\u8868\u793a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u63d2\u4ef6\u5f0f\u7684\u7a7a\u95f4\u7ea6\u675f\u8868\u793a\u589e\u5f3a\u6846\u67b6ReST\uff0c\u5305\u62ec\u5143ID\u9884\u70ed\u7f51\u7edc\u548c\u7a7a\u95f4\u7ea6\u675fID\u8868\u793a\u589e\u5f3a\u7f51\u7edc\uff08SIDENet\uff09\uff0c\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u957f\u5c3e\u5546\u54c1\u7684\u8868\u793a\u3002", "result": "\u901a\u8fc7\u7a7a\u95f4\u7ea6\u675f\u7684\u786c\u91c7\u6837\u548c\u52a8\u6001\u8868\u793a\u5bf9\u9f50\u7b56\u7565\uff0cSIDENet\u80fd\u591f\u81ea\u9002\u5e94\u5730\u8bc6\u522b\u548c\u589e\u5f3a\u5f31ID\u8868\u793a\uff0c\u540c\u65f6\u4fdd\u7559\u4e0e\u70ed\u95e8\u5546\u54c1\u7684\u517c\u5bb9\u6027\u3002", "conclusion": "ReST\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u672c\u5730\u751f\u6d3b\u63a8\u8350\u4e2d\u7684\u7a7a\u95f4\u7ea6\u675f\u548c\u957f\u5c3e\u7a00\u758f\u6027\u95ee\u9898\uff0c\u63d0\u5347\u63a8\u8350\u6548\u679c\u3002"}}
{"id": "2511.11831", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11831", "abs": "https://arxiv.org/abs/2511.11831", "authors": ["Wenhao Zhou", "Hao Zheng", "Rong Zhao"], "title": "TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models", "comment": null, "summary": "Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3a TopoPerception \u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (LVLM) \u7684\u5168\u5c40\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u73b0\u6709 LVLM \u7684\u89c6\u89c9\u611f\u77e5\u6a21\u5757\u5b58\u5728\u74f6\u9888\uff0c\u5e76\u4e14\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u5b58\u5728\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u611f\u77e5\u80fd\u529b\u88ab\u9ad8\u4f30\u7684\u5c40\u90e8\u6377\u5f84\u3002", "method": "TopoPerception \u57fa\u51c6\u5229\u7528\u62d3\u6251\u5c5e\u6027\u6765\u8bc4\u4f30 LVLM \u7684\u5168\u5c40\u89c6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u62d3\u6251\u5c5e\u6027\u5bf9\u56fe\u50cf\u7684\u5168\u5c40\u7ed3\u6784\u654f\u611f\u4e14\u5bf9\u5c40\u90e8\u7279\u5f81\u4e0d\u53d8\u3002", "result": "\u5728 TopoPerception \u4e0a\u8bc4\u4f30\u73b0\u6709\u6a21\u578b\u53d1\u73b0\uff0c\u5373\u4f7f\u5728\u6700\u7c97\u7cd9\u7684\u611f\u77e5\u7c92\u5ea6\u4e0b\uff0c\u6240\u6709\u6a21\u578b\u7684\u6027\u80fd\u90fd\u4e0d\u4f18\u4e8e\u968f\u673a\u6c34\u5e73\uff0c\u8868\u660e\u5176\u7f3a\u4e4f\u611f\u77e5\u5168\u5c40\u89c6\u89c9\u7279\u5f81\u7684\u80fd\u529b\u3002\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u53cd\u800c\u8868\u73b0\u66f4\u5dee\u3002", "conclusion": "TopoPerception \u63ed\u793a\u4e86\u5f53\u524d LVLM \u7684\u4e00\u4e2a\u5173\u952e\u74f6\u9888\uff0c\u5e76\u4e3a\u63d0\u9ad8\u5176\u5168\u5c40\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.11922", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11922", "abs": "https://arxiv.org/abs/2511.11922", "authors": ["Karthikeyan K", "Raghuveer Thirukovalluru", "David Carlson"], "title": "Additive Large Language Models for Semi-Structured Text", "comment": null, "summary": "Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \\textbf{CALM}, short for \\textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.", "AI": {"tldr": "CALM: An interpretable framework for clinical text classification.", "motivation": "Lack of interpretability in LLM predictions hinders clinical adoption.", "method": "Predicts outcomes as the additive sum of each component's contribution.", "result": "Achieves comparable performance to conventional LLM classifiers while improving trust and revealing clinically meaningful patterns.", "conclusion": "CALM improves trust, supports quality-assurance checks, and reveals clinically meaningful patterns."}}
{"id": "2511.11725", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11725", "abs": "https://arxiv.org/abs/2511.11725", "authors": ["Zekai Shi", "Zhixi Cai", "Kalin Stefanov"], "title": "Do Blind Spots Matter for Word-Referent Mapping? A Computational Study with Infant Egocentric Video", "comment": null, "summary": "Typically, children start to learn their first words between 6 and 9 months, linking spoken utterances to their visual referents. Without prior knowledge, a word encountered for the first time can be interpreted in countless ways; it might refer to any of the objects in the environment, their components, or attributes. Using longitudinal, egocentric, and ecologically valid data from the experience of one child, in this work, we propose a self-supervised and biologically plausible strategy to learn strong visual representations. Our masked autoencoder-based visual backbone incorporates knowledge about the blind spot in human eyes to define a novel masking strategy. This mask and reconstruct approach attempts to mimic the way the human brain fills the gaps in the eyes' field of view. This represents a significant shift from standard random masking strategies, which are difficult to justify from a biological perspective. The pretrained encoder is utilized in a contrastive learning-based video-text model capable of acquiring word-referent mappings. Extensive evaluation suggests that the proposed biologically plausible masking strategy is at least as effective as random masking for learning word-referent mappings from cross-situational and temporally extended episodes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u548c\u751f\u7269\u5b66\u4e0a\u5408\u7406\u7684\u7b56\u7565\u6765\u5b66\u4e60\u5f3a\u5927\u7684\u89c6\u89c9\u8868\u5f81\uff0c\u7528\u4e8e\u5b66\u4e60\u5355\u8bcd\u6307\u79f0\u6620\u5c04\u3002", "motivation": "\u513f\u7ae5\u901a\u5e38\u5728 6 \u5230 9 \u4e2a\u6708\u5927\u65f6\u5f00\u59cb\u5b66\u4e60\u4ed6\u4eec\u7684\u7b2c\u4e00\u4e2a\u5355\u8bcd\uff0c\u5c06\u53e3\u8bed\u4e0e\u89c6\u89c9\u6307\u793a\u7269\u8054\u7cfb\u8d77\u6765\u3002\u5728\u6ca1\u6709\u5148\u9a8c\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\uff0c\u7b2c\u4e00\u6b21\u9047\u5230\u7684\u5355\u8bcd\u53ef\u4ee5\u7528\u65e0\u6570\u79cd\u65b9\u5f0f\u89e3\u91ca\uff1b\u5b83\u53ef\u80fd\u6307\u7684\u662f\u73af\u5883\u4e2d\u4efb\u4f55\u7269\u4f53\u3001\u5b83\u4eec\u7684\u7ec4\u6210\u90e8\u5206\u6216\u5c5e\u6027\u3002", "method": "\u4f7f\u7528\u6765\u81ea\u4e00\u4e2a\u5b69\u5b50\u7684\u7ecf\u9a8c\u7684\u7eb5\u5411\u3001\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u548c\u751f\u6001\u6709\u6548\u7684\u6570\u636e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u81ea\u52a8\u7f16\u7801\u5668\u7684\u89c6\u89c9\u4e3b\u5e72\uff0c\u8be5\u4e3b\u5e72\u7ed3\u5408\u4e86\u5173\u4e8e\u4eba\u773c\u76f2\u70b9\u7684\u77e5\u8bc6\u6765\u5b9a\u4e49\u4e00\u79cd\u65b0\u7684\u63a9\u7801\u7b56\u7565\u3002\u8fd9\u79cd\u63a9\u7801\u548c\u91cd\u5efa\u65b9\u6cd5\u8bd5\u56fe\u6a21\u4eff\u4eba\u8111\u586b\u8865\u773c\u775b\u89c6\u91ce\u4e2d\u7684\u7a7a\u767d\u7684\u65b9\u5f0f\u3002", "result": "\u6240\u63d0\u51fa\u7684\u751f\u7269\u5b66\u4e0a\u5408\u7406\u7684\u63a9\u853d\u7b56\u7565\u81f3\u5c11\u4e0e\u968f\u673a\u63a9\u853d\u4e00\u6837\u6709\u6548\uff0c\u53ef\u4ee5\u4ece\u8de8\u60c5\u5883\u548c\u65f6\u95f4\u6269\u5c55\u7684\u60c5\u8282\u4e2d\u5b66\u4e60\u5355\u8bcd\u6307\u79f0\u6620\u5c04\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u8868\u5f81\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u7269\u5b66\u4e0a\u662f\u5408\u7406\u7684\uff0c\u5e76\u4e14\u5728\u5b66\u4e60\u5355\u8bcd\u6307\u79f0\u6620\u5c04\u65b9\u9762\u662f\u6709\u6548\u7684\u3002"}}
{"id": "2511.11592", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.11592", "abs": "https://arxiv.org/abs/2511.11592", "authors": ["Guojian Zhan", "Likun Wang", "Pengcheng Wang", "Feihong Zhang", "Jingliang Duan", "Masayoshi Tomizuka", "Shengbo Eben Li"], "title": "Mind Your Entropy: From Maximum Entropy to Trajectory Entropy-Constrained RL", "comment": "17 pages", "summary": "Maximum entropy has become a mainstream off-policy reinforcement learning (RL) framework for balancing exploitation and exploration. However, two bottlenecks still limit further performance improvement: (1) non-stationary Q-value estimation caused by jointly injecting entropy and updating its weighting parameter, i.e., temperature; and (2) short-sighted local entropy tuning that adjusts temperature only according to the current single-step entropy, without considering the effect of cumulative entropy over time. In this paper, we extends maximum entropy framework by proposing a trajectory entropy-constrained reinforcement learning (TECRL) framework to address these two challenges. Within this framework, we first separately learn two Q-functions, one associated with reward and the other with entropy, ensuring clean and stable value targets unaffected by temperature updates. Then, the dedicated entropy Q-function, explicitly quantifying the expected cumulative entropy, enables us to enforce a trajectory entropy constraint and consequently control the policy long-term stochasticity. Building on this TECRL framework, we develop a practical off-policy algorithm, DSAC-E, by extending the state-of-the-art distributional soft actor-critic with three refinements (DSAC-T). Empirical results on the OpenAI Gym benchmark demonstrate that our DSAC-E can achieve higher returns and better stability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f68\u8ff9\u71b5\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60 (TECRL) \u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u6700\u5927\u71b5\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e24\u4e2a\u74f6\u9888\uff1a\u975e\u5e73\u7a33 Q \u503c\u4f30\u8ba1\u548c\u77ed\u89c6\u7684\u5c40\u90e8\u71b5\u8c03\u6574\u3002", "motivation": "\u6700\u5927\u71b5\u5f3a\u5316\u5b66\u4e60\u662f\u5e73\u8861\u5229\u7528\u548c\u63a2\u7d22\u7684\u4e3b\u6d41\u6846\u67b6\uff0c\u4f46\u5b58\u5728\u975e\u5e73\u7a33 Q \u503c\u4f30\u8ba1\u548c\u77ed\u89c6\u7684\u5c40\u90e8\u71b5\u8c03\u6574\u8fd9\u4e24\u4e2a\u74f6\u9888\u9650\u5236\u4e86\u6027\u80fd\u7684\u8fdb\u4e00\u6b65\u63d0\u9ad8\u3002", "method": "\u8be5\u6846\u67b6\u9996\u5148\u5206\u522b\u5b66\u4e60\u4e24\u4e2a Q \u51fd\u6570\uff0c\u4e00\u4e2a\u4e0e\u5956\u52b1\u76f8\u5173\uff0c\u53e6\u4e00\u4e2a\u4e0e\u71b5\u76f8\u5173\uff0c\u786e\u4fdd\u4e0d\u53d7\u6e29\u5ea6\u66f4\u65b0\u5f71\u54cd\u7684\u5e72\u51c0\u4e14\u7a33\u5b9a\u7684\u503c\u76ee\u6807\u3002\u7136\u540e\uff0c\u4e13\u95e8\u7684\u71b5 Q \u51fd\u6570\u663e\u5f0f\u5730\u91cf\u5316\u4e86\u9884\u671f\u7684\u7d2f\u79ef\u71b5\uff0c\u4f7f\u6211\u4eec\u80fd\u591f\u5b9e\u65bd\u8f68\u8ff9\u71b5\u7ea6\u675f\uff0c\u4ece\u800c\u63a7\u5236\u7b56\u7565\u7684\u957f\u671f\u968f\u673a\u6027\u3002\u5728\u6b64 TECRL \u6846\u67b6\u7684\u57fa\u7840\u4e0a\uff0c\u6211\u4eec\u901a\u8fc7\u6269\u5c55\u6700\u5148\u8fdb\u7684\u5206\u5e03\u5f0f\u8f6f Actor-Critic \u5e76\u8fdb\u884c\u4e09\u9879\u6539\u8fdb (DSAC-T)\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u79bb\u7ebf\u7b56\u7565\u7b97\u6cd5 DSAC-E\u3002", "result": "\u5728 OpenAI Gym \u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684 DSAC-E \u53ef\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u7684\u56de\u62a5\u548c\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "TECRL \u6846\u67b6\u548c DSAC-E \u7b97\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2511.12949", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12949", "abs": "https://arxiv.org/abs/2511.12949", "authors": ["Bokang Fu", "Jiahao Wang", "Xiaojing Liu", "Yuli Liu"], "title": "Can We Predict the Next Question? A Collaborative Filtering Approach to Modeling User Behavior", "comment": null, "summary": "In recent years, large language models (LLMs) have excelled in language understanding and generation, powering advanced dialogue and recommendation systems. However, a significant limitation persists: these systems often model user preferences statically, failing to capture the dynamic and sequential nature of interactive behaviors. The sequence of a user's historical questions provides a rich, implicit signal of evolving interests and cognitive patterns, yet leveraging this temporal data for predictive tasks remains challenging due to the inherent disconnect between language modeling and behavioral sequence modeling.\n  To bridge this gap, we propose a Collaborative Filtering-enhanced Question Prediction (CFQP) framework. CFQP dynamically models evolving user-question interactions by integrating personalized memory modules with graph-based preference propagation. This dual mechanism allows the system to adaptively learn from user-specific histories while refining predictions through collaborative signals from similar users. Experimental results demonstrate that our approach effectively generates agents that mimic real-user questioning patterns, highlighting its potential for building proactive and adaptive dialogue systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a CFQP \u7684\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u5efa\u6a21\u7528\u6237\u63d0\u95ee\u884c\u4e3a\uff0c\u4ee5\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u7528\u6237\u52a8\u6001\u504f\u597d\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u548c\u751f\u6210\u8bed\u8a00\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u9759\u6001\u5730\u5efa\u6a21\u7528\u6237\u504f\u597d\uff0c\u65e0\u6cd5\u6355\u6349\u4ea4\u4e92\u884c\u4e3a\u7684\u52a8\u6001\u548c\u987a\u5e8f\u6027\u3002", "method": "\u901a\u8fc7\u6574\u5408\u4e2a\u6027\u5316\u7684\u8bb0\u5fc6\u6a21\u5757\u4e0e\u57fa\u4e8e\u56fe\u7684\u504f\u597d\u4f20\u64ad\u6765\u52a8\u6001\u5efa\u6a21\u7528\u6237-\u95ee\u9898\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u751f\u6210\u6a21\u4eff\u771f\u5b9e\u7528\u6237\u63d0\u95ee\u6a21\u5f0f\u7684\u4ee3\u7406\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6784\u5efa\u4e3b\u52a8\u548c\u81ea\u9002\u5e94\u5bf9\u8bdd\u7cfb\u7edf\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2511.11899", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11899", "abs": "https://arxiv.org/abs/2511.11899", "authors": ["Xi Li", "Nicholas Matsumoto", "Ujjwal Pasupulety", "Atharva Deo", "Cherine Yang", "Jay Moran", "Miguel E. Hernandez", "Peter Wager", "Jasmine Lin", "Jeanine Kim", "Alvin C. Goh", "Christian Wagner", "Geoffrey A. Sonn", "Andrew J. Hung"], "title": "End to End AI System for Surgical Gesture Sequence Recognition and Clinical Outcome Prediction", "comment": null, "summary": "Fine-grained analysis of intraoperative behavior and its impact on patient outcomes remain a longstanding challenge. We present Frame-to-Outcome (F2O), an end-to-end system that translates tissue dissection videos into gesture sequences and uncovers patterns associated with postoperative outcomes. Leveraging transformer-based spatial and temporal modeling and frame-wise classification, F2O robustly detects consecutive short (~2 seconds) gestures in the nerve-sparing step of robot-assisted radical prostatectomy (AUC: 0.80 frame-level; 0.81 video-level). F2O-derived features (gesture frequency, duration, and transitions) predicted postoperative outcomes with accuracy comparable to human annotations (0.79 vs. 0.75; overlapping 95% CI). Across 25 shared features, effect size directions were concordant with small differences (~ 0.07), and strong correlation (r = 0.96, p < 1e-14). F2O also captured key patterns linked to erectile function recovery, including prolonged tissue peeling and reduced energy use. By enabling automatic interpretable assessment, F2O establishes a foundation for data-driven surgical feedback and prospective clinical decision support.", "AI": {"tldr": "F2O\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u5b83\u53ef\u4ee5\u5c06\u7ec4\u7ec7\u89e3\u5256\u89c6\u9891\u8f6c\u6362\u4e3a\u624b\u52bf\u5e8f\u5217\uff0c\u5e76\u63ed\u793a\u4e0e\u672f\u540e\u7ed3\u679c\u76f8\u5173\u7684\u6a21\u5f0f\u3002", "motivation": "\u5bf9\u672f\u4e2d\u884c\u4e3a\u7684\u7ec6\u7c92\u5ea6\u5206\u6790\u53ca\u5176\u5bf9\u60a3\u8005\u9884\u540e\u7684\u5f71\u54cd\u4ecd\u7136\u662f\u4e00\u4e2a\u957f\u671f\u5b58\u5728\u7684\u6311\u6218\u3002", "method": "\u5229\u7528\u57fa\u4e8etransformer\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u5efa\u6a21\u4ee5\u53ca\u9010\u5e27\u5206\u7c7b\uff0cF2O\u7a33\u5065\u5730\u68c0\u6d4b\u673a\u5668\u4eba\u8f85\u52a9\u524d\u5217\u817a\u764c\u6839\u6cbb\u672f\u795e\u7ecf\u4fdd\u7559\u6b65\u9aa4\u4e2d\u7684\u8fde\u7eed\u77ed\u65f6\u624b\u52bf\uff08~2\u79d2\uff09\u3002", "result": "F2O\u884d\u751f\u7684\u7279\u5f81\uff08\u624b\u52bf\u9891\u7387\u3001\u6301\u7eed\u65f6\u95f4\u548c\u8f6c\u6362\uff09\u9884\u6d4b\u672f\u540e\u7ed3\u679c\u7684\u51c6\u786e\u6027\u4e0e\u4eba\u5de5\u6ce8\u91ca\u76f8\u5f53\uff080.79 vs. 0.75\uff1b\u91cd\u53e095% CI\uff09\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u73b0\u81ea\u52a8\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\uff0cF2O\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u624b\u672f\u53cd\u9988\u548c\u524d\u77bb\u6027\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.11933", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11933", "abs": "https://arxiv.org/abs/2511.11933", "authors": ["Karthikeyan K", "Raghuveer Thirukovalluru", "Bhuwan Dhingra", "David Edwin Carlson"], "title": "InData: Towards Secure Multi-Step, Tool-Based Data Analysis", "comment": null, "summary": "Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6570\u636e\u5206\u6790\u4e2d\u76f4\u63a5\u751f\u6210\u548c\u6267\u884c\u4ee3\u7801\u5b58\u5728\u5b89\u5168\u98ce\u9669\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u5b89\u5168\u66ff\u4ee3\u65b9\u6848\uff1a\u9650\u5236LLM\u76f4\u63a5\u751f\u6210\u4ee3\u7801\u548c\u8bbf\u95ee\u6570\u636e\uff0c\u800c\u662f\u901a\u8fc7\u9884\u5b9a\u4e49\u7684\u3001\u5b89\u5168\u7684\u5de5\u5177\u96c6\u8fdb\u884c\u4ea4\u4e92\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aIndirect Data Engagement (InData) \u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u57fa\u4e8e\u5de5\u5177\u7684\u591a\u6b65\u9aa4\u63a8\u7406\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709\u7684LLM\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u4ecd\u7136\u7f3a\u4e4f\u5f3a\u5927\u7684\u591a\u6b65\u9aa4\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f4\u63a5\u751f\u6210\u4ee3\u7801\u548c\u8bbf\u95ee\u654f\u611f\u6570\u636e\u65f6\u5b58\u5728\u7684\u5b89\u5168\u98ce\u9669\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aIndirect Data Engagement (InData) \u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u57fa\u4e8e\u5de5\u5177\u7684\u591a\u6b65\u9aa4\u63a8\u7406\u80fd\u529b\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\uff08\u7b80\u5355\u3001\u4e2d\u7b49\u548c\u56f0\u96be\uff09\u7684\u6570\u636e\u5206\u6790\u95ee\u9898\uff0c\u6355\u6349\u4e86\u4e0d\u65ad\u589e\u52a0\u7684\u63a8\u7406\u590d\u6742\u6027\u3002\u5728InData\u4e0a\u5bf915\u4e2a\u5f00\u6e90LLM\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5927\u578b\u6a21\u578b\uff08\u5982gpt-oss-120b\uff09\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u8f83\u9ad8\u7684\u51c6\u786e\u7387\uff0897.3%\uff09\uff0c\u4f46\u5728\u56f0\u96be\u4efb\u52a1\u4e0a\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0869.6%\uff09\u3002", "conclusion": "\u76ee\u524d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ecd\u7136\u7f3a\u4e4f\u5f3a\u5927\u7684\u591a\u6b65\u9aa4\u5de5\u5177\u63a8\u7406\u80fd\u529b\u3002\u53d1\u5e03\u4e86\u6570\u636e\u96c6\u548c\u4ee3\u7801\uff0c\u4ee5\u4fc3\u8fdb\u5177\u6709\u66f4\u5f3a\u5927\u7684\u591a\u6b65\u9aa4\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u7684LLM\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u3002"}}
{"id": "2511.11730", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11730", "abs": "https://arxiv.org/abs/2511.11730", "authors": ["Yongjun Xiao", "Dian Meng", "Xinlei Huang", "Yanran Liu", "Shiwei Ruan", "Ziyue Qiao", "Xubin Zheng"], "title": "GROVER: Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion", "comment": "8 pages, 3 figures, Accepted to AAAI 2026", "summary": "Effectively modeling multimodal spatial omics data is critical for understanding tissue complexity and underlying biological mechanisms. While spatial transcriptomics, proteomics, and epigenomics capture molecular features, they lack pathological morphological context. Integrating these omics with histopathological images is therefore essential for comprehensive disease tissue analysis. However, substantial heterogeneity across omics, imaging, and spatial modalities poses significant challenges. Naive fusion of semantically distinct sources often leads to ambiguous representations. Additionally, the resolution mismatch between high-resolution histology images and lower-resolution sequencing spots complicates spatial alignment. Biological perturbations during sample preparation further distort modality-specific signals, hindering accurate integration. To address these challenges, we propose Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion (GROVER), a novel framework for adaptive integration of spatial multi-omics data. GROVER leverages a Graph Convolutional Network encoder based on Kolmogorov-Arnold Networks to capture the nonlinear dependencies between each modality and its associated spatial structure, thereby producing expressive, modality-specific embeddings. To align these representations, we introduce a spot-feature-pair contrastive learning strategy that explicitly optimizes the correspondence across modalities at each spot. Furthermore, we design a dynamic expert routing mechanism that adaptively selects informative modalities for each spot while suppressing noisy or low-quality inputs. Experiments on real-world spatial omics datasets demonstrate that GROVER outperforms state-of-the-art baselines, providing a robust and reliable solution for multimodal integration.", "AI": {"tldr": "GROVER\u6846\u67b6\u901a\u8fc7\u56fe\u5377\u79ef\u7f51\u7edc\u7f16\u7801\u5668\u3001\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u548c\u52a8\u6001\u4e13\u5bb6\u8def\u7531\u673a\u5236\uff0c\u81ea\u9002\u5e94\u5730\u6574\u5408\u7a7a\u95f4\u591a\u7ec4\u5b66\u6570\u636e\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6709\u6548\u5efa\u6a21\u591a\u6a21\u6001\u7a7a\u95f4\u7ec4\u5b66\u6570\u636e\u5bf9\u4e8e\u7406\u89e3\u7ec4\u7ec7\u590d\u6742\u6027\u548c\u6f5c\u5728\u751f\u7269\u5b66\u673a\u5236\u81f3\u5173\u91cd\u8981\u3002\u6574\u5408\u7ec4\u5b66\u4e0e\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5bf9\u4e8e\u7efc\u5408\u75be\u75c5\u7ec4\u7ec7\u5206\u6790\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7ec4\u5b66\u3001\u6210\u50cf\u548c\u7a7a\u95f4\u6a21\u5f0f\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u5f02\u8d28\u6027\uff0c\u5bf9\u6574\u5408\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "GROVER\u5229\u7528\u57fa\u4e8eKolmogorov-Arnold Networks\u7684\u56fe\u5377\u79ef\u7f51\u7edc\u7f16\u7801\u5668\uff0c\u6355\u83b7\u6bcf\u4e2a\u6a21\u6001\u4e0e\u5176\u76f8\u5173\u7a7a\u95f4\u7ed3\u6784\u4e4b\u95f4\u7684\u975e\u7ebf\u6027\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ece\u800c\u751f\u6210\u8868\u8fbe\u6027\u7684\u3001\u7279\u5b9a\u4e8e\u6a21\u6001\u7684\u5d4c\u5165\u3002\u5f15\u5165\u4e86\u4e00\u79cdspot-feature-pair\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u5f0f\u5730\u4f18\u5316\u4e86\u6bcf\u4e2aspot\u4e2d\u8de8\u6a21\u6001\u7684\u5bf9\u5e94\u5173\u7cfb\u3002\u8bbe\u8ba1\u4e86\u4e00\u79cd\u52a8\u6001\u4e13\u5bb6\u8def\u7531\u673a\u5236\uff0c\u81ea\u9002\u5e94\u5730\u9009\u62e9\u6bcf\u4e2aspot\u7684\u4fe1\u606f\u6a21\u6001\uff0c\u540c\u65f6\u6291\u5236\u566a\u58f0\u6216\u4f4e\u8d28\u91cf\u7684\u8f93\u5165\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7a7a\u95f4\u7ec4\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGROVER\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e3a\u591a\u6a21\u6001\u6574\u5408\u63d0\u4f9b\u4e86\u7a33\u5065\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "GROVER\u6846\u67b6\u4e3a\u7a7a\u95f4\u591a\u7ec4\u5b66\u6570\u636e\u7684\u81ea\u9002\u5e94\u6574\u5408\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2511.11593", "categories": ["cs.LG", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.11593", "abs": "https://arxiv.org/abs/2511.11593", "authors": ["Matthew Morris", "Ian Horrocks"], "title": "Sound Logical Explanations for Mean Aggregation Graph Neural Networks", "comment": "Full version (with appendices) of paper accepted to NeurIPS 2025 (The Thirty-Ninth Annual Conference on Neural Information Processing Systems)", "summary": "Graph neural networks (GNNs) are frequently used for knowledge graph completion. Their black-box nature has motivated work that uses sound logical rules to explain predictions and characterise their expressivity. However, despite the prevalence of GNNs that use mean as an aggregation function, explainability and expressivity results are lacking for them. We consider GNNs with mean aggregation and non-negative weights (MAGNNs), proving the precise class of monotonic rules that can be sound for them, as well as providing a restricted fragment of first-order logic to explain any MAGNN prediction. Our experiments show that restricting mean-aggregation GNNs to have non-negative weights yields comparable or improved performance on standard inductive benchmarks, that sound rules are obtained in practice, that insightful explanations can be generated in practice, and that the sound rules can expose issues in the trained models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u5e73\u5747\u805a\u5408\u7684\u56fe\u795e\u7ecf\u7f51\u7edc (MAGNNs) \u7684\u53ef\u89e3\u91ca\u6027\u548c\u8868\u8fbe\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86sound logical rules\u6765\u89e3\u91ca\u9884\u6d4b\u3002", "motivation": "\u5c3d\u7ba1\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5e38\u7528\u4e8e\u77e5\u8bc6\u56fe\u8865\u5168\uff0c\u4f46\u5bf9\u4e8e\u4f7f\u7528\u5e73\u5747\u4f5c\u4e3a\u805a\u5408\u51fd\u6570\u7684GNNs\uff0c\u53ef\u89e3\u91ca\u6027\u548c\u8868\u8fbe\u6027\u7684\u7814\u7a76\u4ecd\u7136\u4e0d\u8db3\u3002", "method": "\u672c\u6587\u8003\u8651\u4e86\u5177\u6709\u5e73\u5747\u805a\u5408\u548c\u975e\u8d1f\u6743\u91cd\u7684GNN\uff08MAGNNs\uff09\uff0c\u8bc1\u660e\u4e86\u53ef\u4ee5\u9002\u7528\u4e8e\u5b83\u4eec\u7684\u5355\u8c03\u89c4\u5219\u7684\u7cbe\u786e\u7c7b\u522b\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u9636\u903b\u8f91\u7684\u53d7\u9650\u7247\u6bb5\u6765\u89e3\u91ca\u4efb\u4f55MAGNN\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9650\u5236\u5e73\u5747\u805a\u5408GNN\u5177\u6709\u975e\u8d1f\u6743\u91cd\u53ef\u4ee5\u5728\u6807\u51c6\u5f52\u7eb3\u57fa\u51c6\u4e0a\u4ea7\u751f\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u5b9e\u8df5\u4e2d\u53ef\u4ee5\u83b7\u5f97\u5408\u7406\u7684\u89c4\u5219\uff0c\u5728\u5b9e\u8df5\u4e2d\u53ef\u4ee5\u751f\u6210\u6709\u6d1e\u5bdf\u529b\u7684\u89e3\u91ca\uff0c\u5e76\u4e14\u5408\u7406\u7684\u89c4\u5219\u53ef\u4ee5\u63ed\u793a\u8bad\u7ec3\u6a21\u578b\u4e2d\u7684\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u4e3a\u4f7f\u7528\u5e73\u5747\u805a\u5408\u7684\u56fe\u795e\u7ecf\u7f51\u7edc (MAGNNs) \u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u8868\u8fbe\u6027\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5f3a\u8c03\u4e86\u975e\u8d1f\u6743\u91cd\u5728\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u4f5c\u7528\u3002"}}
{"id": "2511.12959", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12959", "abs": "https://arxiv.org/abs/2511.12959", "authors": ["Jaehyung Lim", "Wonbin Kweon", "Woojoo Kim", "Junyoung Kim", "Dongha Kim", "Hwanjo Yu"], "title": "Personalized Federated Recommendation With Knowledge Guidance", "comment": null, "summary": "Federated Recommendation (FedRec) has emerged as a key paradigm for building privacy-preserving recommender systems. However, existing FedRec models face a critical dilemma: memory-efficient single-knowledge models suffer from a suboptimal knowledge replacement practice that discards valuable personalization, while high-performance dual-knowledge models are often too memory-intensive for practical on-device deployment. We propose Federated Recommendation with Knowledge Guidance (FedRKG), a model-agnostic framework that resolves this dilemma. The core principle, Knowledge Guidance, avoids full replacement and instead fuses global knowledge into preserved local embeddings, attaining the personalization benefits of dual-knowledge within a single-knowledge memory footprint. Furthermore, we introduce Adaptive Guidance, a fine-grained mechanism that dynamically modulates the intensity of this guidance for each user-item interaction, overcoming the limitations of static fusion methods. Extensive experiments on benchmark datasets demonstrate that FedRKG significantly outperforms state-of-the-art methods, validating the effectiveness of our approach. The code is available at https://github.com/Jaehyung-Lim/fedrkg.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u63a8\u8350\u6846\u67b6FedRKG\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u8054\u90a6\u63a8\u8350\u6a21\u578b\u5728\u5185\u5b58\u6548\u7387\u548c\u4e2a\u6027\u5316\u4e4b\u95f4\u7684\u4e24\u96be\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u8054\u90a6\u63a8\u8350\u6a21\u578b\u9762\u4e34\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u5185\u5b58\u6548\u7387\u9ad8\u7684\u5355\u77e5\u8bc6\u6a21\u578b\u4f1a\u4e22\u5f03\u6709\u4ef7\u503c\u7684\u4e2a\u6027\u5316\u4fe1\u606f\uff0c\u800c\u9ad8\u6027\u80fd\u7684\u53cc\u77e5\u8bc6\u6a21\u578b\u901a\u5e38\u5185\u5b58\u5bc6\u96c6\uff0c\u96be\u4ee5\u5728\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e86\u77e5\u8bc6\u5f15\u5bfc\uff0c\u907f\u514d\u5b8c\u5168\u66ff\u6362\uff0c\u800c\u662f\u5c06\u5168\u5c40\u77e5\u8bc6\u878d\u5165\u5230\u4fdd\u7559\u7684\u672c\u5730\u5d4c\u5165\u4e2d\uff0c\u5728\u5355\u77e5\u8bc6\u5185\u5b58\u5360\u7528\u5185\u5b9e\u73b0\u53cc\u77e5\u8bc6\u7684\u4e2a\u6027\u5316\u4f18\u52bf\u3002\u6b64\u5916\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u5f15\u5bfc\uff0c\u4e00\u79cd\u7ec6\u7c92\u5ea6\u673a\u5236\uff0c\u53ef\u4ee5\u52a8\u6001\u5730\u8c03\u6574\u6bcf\u4e2a\u7528\u6237-\u9879\u76ee\u4ea4\u4e92\u7684\u5f15\u5bfc\u5f3a\u5ea6\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFedRKG \u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.11914", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.IT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11914", "abs": "https://arxiv.org/abs/2511.11914", "authors": ["Shizhou Xu", "Yuan Ni", "Stefan Broecker", "Thomas Strohmer"], "title": "Forgetting-MarI: LLM Unlearning via Marginal Information Regularization", "comment": null, "summary": "As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Forgetting-MarI \u7684 LLM unlearning \u6846\u67b6\uff0c\u5b83\u53ea\u5220\u9664\u5f85 unlearn \u6570\u636e\u5e26\u6765\u7684\u989d\u5916\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u7559\u7531\u4fdd\u7559\u6570\u636e\u652f\u6301\u7684\u4fe1\u606f\u3002", "motivation": "\u968f\u7740 AI \u6a21\u578b\u5728\u4e0d\u65ad\u6269\u5c55\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ece\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u4e2d\u79fb\u9664\u7279\u5b9a\u6570\u636e\u7684\u5f71\u54cd\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u4fdd\u62a4\u9690\u79c1\u548c\u7b26\u5408\u6cd5\u89c4\u3002", "method": "\u901a\u8fc7\u60e9\u7f5a\u8fb9\u7f18\u4fe1\u606f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5bf9 unlearn \u6570\u636e\u96c6\u5728\u8bad\u7ec3\u6a21\u578b\u4e2d\u7684\u6b8b\u4f59\u5f71\u54cd\u4ea7\u751f\u4e00\u4e2a\u660e\u786e\u7684\u4e0a\u9650\uff0c\u4ece\u800c\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u4e0d\u53ef\u68c0\u6d4b\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u5b9e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684 unlearning \u65b9\u6cd5\uff0c\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u4f9b\u53ef\u9760\u7684 forgetting \u6548\u679c\u548c\u66f4\u597d\u4fdd\u7559\u7684\u901a\u7528\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u8fd9\u4e00\u8fdb\u6b65\u4ee3\u8868\u7740\u671d\u7740\u4f7f AI \u7cfb\u7edf\u66f4\u53ef\u63a7\uff0c\u66f4\u7b26\u5408\u9690\u79c1\u548c\u7248\u6743\u6cd5\u89c4\uff0c\u800c\u4e0d\u635f\u5bb3\u5176\u6709\u6548\u6027\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2511.11946", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11946", "abs": "https://arxiv.org/abs/2511.11946", "authors": ["Hadi Sheikhi", "Chenyang Huang", "Osmar R. Za\u00efane"], "title": "Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization", "comment": null, "summary": "Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs' attachment on external knowledge.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u56fe\u8c31\u5bf9\u8bdd\u751f\u6210\u4e2d\u4f9d\u8d56\u5185\u90e8\u77e5\u8bc6\uff0c\u5bfc\u81f4\u4e0e\u5916\u90e8\u77e5\u8bc6\u56fe\u8c31\u8131\u8282\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u56fe\u8c31\u5bf9\u8bdd\u751f\u6210\u4e2d\u5229\u7528\u5916\u90e8\u77e5\u8bc6\u7684\u80fd\u529b\u3002", "method": "1. \u63d0\u51faLLM-KAT\uff0c\u7528\u4e8e\u8bc4\u4f30\u751f\u6210\u56de\u590d\u4e2d\u7684\u77e5\u8bc6\u8fde\u63a5\u30022. \u63d0\u51fa\u5b9e\u4f53\u533f\u540d\u5316\u6280\u672f\uff0c\u9f13\u52b1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u66f4\u597d\u5730\u5229\u7528\u5916\u90e8\u77e5\u8bc6\u3002", "result": "\u5728OpenDialKG\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u5916\u90e8\u77e5\u8bc6\u7684\u8fde\u63a5\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5b9e\u4f53\u533f\u540d\u5316\u6280\u672f\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u56fe\u8c31\u5bf9\u8bdd\u751f\u6210\u4e2d\u5bf9\u5916\u90e8\u77e5\u8bc6\u7684\u5229\u7528\u7387\u3002"}}
{"id": "2511.11732", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11732", "abs": "https://arxiv.org/abs/2511.11732", "authors": ["Aditya Mehta", "Swarnim Chaudhary", "Pratik Narang", "Jagat Sesh Challa"], "title": "Exposing DeepFakes via Hyperspectral Domain Mapping", "comment": "Accepted at AAAI 2026 Student Abstract", "summary": "Modern generative and diffusion models produce highly realistic images that can mislead human perception and even sophisticated automated detection systems. Most detection methods operate in RGB space and thus analyze only three spectral channels. We propose HSI-Detect, a two-stage pipeline that reconstructs a 31-channel hyperspectral image from a standard RGB input and performs detection in the hyperspectral domain. Expanding the input representation into denser spectral bands amplifies manipulation artifacts that are often weak or invisible in the RGB domain, particularly in specific frequency bands. We evaluate HSI-Detect across FaceForensics++ dataset and show the consistent improvements over RGB-only baselines, illustrating the promise of spectral-domain mapping for Deepfake detection.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a HSI-Detect \u7684\u65b0\u578b Deepfake \u68c0\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u9996\u5148\u4ece RGB \u56fe\u50cf\u91cd\u5efa 31 \u901a\u9053\u9ad8\u5149\u8c31\u56fe\u50cf\uff0c\u7136\u540e\u5728\u9ad8\u5149\u8c31\u57df\u4e2d\u8fdb\u884c\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u7684 Deepfake \u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5728 RGB \u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u4ec5\u5206\u6790\u4e09\u4e2a\u5149\u8c31\u901a\u9053\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u68c0\u6d4b\u7ec6\u5fae\u4f2a\u9020\u8ff9\u8c61\u7684\u80fd\u529b\u3002", "method": "HSI-Detect \u662f\u4e00\u79cd\u4e24\u9636\u6bb5\u6d41\u7a0b\uff0c\u5b83\u9996\u5148\u4ece\u6807\u51c6 RGB \u8f93\u5165\u91cd\u5efa\u9ad8\u5149\u8c31\u56fe\u50cf\uff0c\u7136\u540e\u5728\u9ad8\u5149\u8c31\u57df\u4e2d\u6267\u884c\u68c0\u6d4b\u3002\u901a\u8fc7\u5c06\u8f93\u5165\u8868\u793a\u6269\u5c55\u5230\u66f4\u5bc6\u7684\u5149\u8c31\u5e26\uff0c\u53ef\u4ee5\u653e\u5927\u5728 RGB \u57df\u4e2d\u901a\u5e38\u8f83\u5f31\u6216\u4e0d\u53ef\u89c1\u7684\u64cd\u7eb5\u4f2a\u5f71\u3002", "result": "\u5728 FaceForensics++ \u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cHSI-Detect \u59cb\u7ec8\u4f18\u4e8e\u4ec5\u4f7f\u7528 RGB \u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5149\u8c31\u57df\u6620\u5c04\u5728 Deepfake \u68c0\u6d4b\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2511.11596", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11596", "abs": "https://arxiv.org/abs/2511.11596", "authors": ["Javier Mar\u00edn"], "title": "Loss Given Default Prediction Under Measurement-Induced Mixture Distributions: An Information-Theoretic Approach", "comment": null, "summary": "Loss Given Default (LGD) modeling faces a fundamental data quality constraint: 90% of available training data consists of proxy estimates based on pre-distress balance sheets rather than actual recovery outcomes from completed bankruptcy proceedings. We demonstrate that this mixture-contaminated training structure causes systematic failure of recursive partitioning methods, with Random Forest achieving negative r-squared (-0.664, worse than predicting the mean) on held-out test data. Information-theoretic approaches based on Shannon entropy and mutual information provide superior generalization, achieving r-squared of 0.191 and RMSE of 0.284 on 1,218 corporate bankruptcies (1980-2023). Analysis reveals that leverage-based features contain 1.510 bits of mutual information while size effects contribute only 0.086 bits, contradicting regulatory assumptions about scale-dependent recovery. These results establish practical guidance for financial institutions deploying LGD models under Basel III requirements when representative outcome data is unavailable at sufficient scale. The findings generalize to medical outcomes research, climate forecasting, and technology reliability-domains where extended observation periods create unavoidable mixture structure in training data.", "AI": {"tldr": "LGD\u5efa\u6a21\u4e2d\uff0c\u8bad\u7ec3\u6570\u636e\u4e3b\u8981\u4e3a\u57fa\u4e8e\u7834\u4ea7\u524d\u8d44\u4ea7\u8d1f\u503a\u8868\u7684\u4ee3\u7406\u4f30\u8ba1\uff0c\u800c\u975e\u5b9e\u9645\u7834\u4ea7\u6e05\u7b97\u7ed3\u679c\uff0c\u5bfc\u81f4recursive partitioning methods\u5931\u6548\u3002", "motivation": "\u5b9e\u9645\u7834\u4ea7\u6062\u590d\u7ed3\u679c\u6570\u636e\u532e\u4e4f\uff0cLGD\u5efa\u6a21\u4f9d\u8d56\u4ee3\u7406\u4f30\u8ba1\u6570\u636e\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u9999\u519c\u71b5\u548c\u4e92\u4fe1\u606f\u7684information-theoretic approaches\u3002", "result": "\u57281980-2023\u5e74\u7684\u516c\u53f8\u7834\u4ea7\u6570\u636e\u4e0a\uff0cR\u65b9\u4e3a0.191\uff0cRMSE\u4e3a0.284\uff0c\u597d\u4e8eRandom Forest\u7684\u8d1fR\u65b9\u7ed3\u679c\u3002\u6760\u6746\u7387\u7279\u5f81\u5305\u542b1.510 bits\u7684\u4e92\u4fe1\u606f\uff0c\u800c\u89c4\u6a21\u6548\u5e94\u4ec5\u8d21\u732e0.086 bits\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5728\u7f3a\u4e4f\u5145\u5206\u89c4\u6a21\u7684\u4ee3\u8868\u6027\u7ed3\u679c\u6570\u636e\u65f6\uff0c\u91d1\u878d\u673a\u6784\u5728Basel III\u8981\u6c42\u4e0b\u90e8\u7f72LGD\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\uff0c\u5e76\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u9886\u57df\u3002"}}
{"id": "2511.13041", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.13041", "abs": "https://arxiv.org/abs/2511.13041", "authors": ["Miaomiao Cai", "Min Hou", "Lei Chen", "Le Wu", "Haoyue Bai", "Yong Li", "Meng Wang"], "title": "Mitigating Recommendation Biases via Group-Alignment and Global-Uniformity in Representation Learning", "comment": null, "summary": "Collaborative Filtering~(CF) plays a crucial role in modern recommender systems, leveraging historical user-item interactions to provide personalized suggestions. However, CF-based methods often encounter biases due to imbalances in training data. This phenomenon makes CF-based methods tend to prioritize recommending popular items and performing unsatisfactorily on inactive users. Existing works address this issue by rebalancing training samples, reranking recommendation results, or making the modeling process robust to the bias. Despite their effectiveness, these approaches can compromise accuracy or be sensitive to weighting strategies, making them challenging to train. In this paper, we deeply analyze the causes and effects of the biases and propose a framework to alleviate biases in recommendation from the perspective of representation distribution, namely Group-Alignment and Global-Uniformity Enhanced Representation Learning for Debiasing Recommendation (AURL). Specifically, we identify two significant problems in the representation distribution of users and items, namely group-discrepancy and global-collapse. These two problems directly lead to biases in the recommendation results. To this end, we propose two simple but effective regularizers in the representation space, respectively named group-alignment and global-uniformity. The goal of group-alignment is to bring the representation distribution of long-tail entities closer to that of popular entities, while global-uniformity aims to preserve the information of entities as much as possible by evenly distributing representations. Our method directly optimizes both the group-alignment and global-uniformity regularization terms to mitigate recommendation biases. Extensive experiments on three real datasets and various recommendation backbones verify the superiority of our proposed framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u8350\u6846\u67b6AURL\uff0c\u4ee5\u7f13\u89e3\u63a8\u8350\u4e2d\u7684\u504f\u5dee\u3002", "motivation": "CF\u65b9\u6cd5\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u4e0d\u5e73\u8861\u800c\u5b58\u5728\u504f\u5dee\uff0c\u5bfc\u81f4\u63a8\u8350\u7ed3\u679c\u503e\u5411\u4e8e\u6d41\u884c\u7269\u54c1\u548c\u5bf9\u4e0d\u6d3b\u8dc3\u7528\u6237\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u4ece\u8868\u793a\u5206\u5e03\u7684\u89d2\u5ea6\uff0c\u901a\u8fc7\u7ec4\u5bf9\u9f50\u548c\u5168\u5c40\u5747\u5300\u6027\u589e\u5f3a\u8868\u793a\u5b66\u4e60\u6765\u6d88\u9664\u504f\u5dee\u3002\u63d0\u51fa\u4e86\u7ec4\u5bf9\u9f50\u548c\u5168\u5c40\u5747\u5300\u6027\u4e24\u4e2a\u6b63\u5219\u5316\u9879\uff0c\u5206\u522b\u89e3\u51b3\u7ec4\u5dee\u5f02\u548c\u5168\u5c40\u5d29\u6e83\u95ee\u9898\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u548c\u5404\u79cd\u63a8\u8350\u9aa8\u5e72\u7f51\u7edc\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u6846\u67b6\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u7ec4\u5bf9\u9f50\u548c\u5168\u5c40\u5747\u5300\u6027\u6b63\u5219\u5316\u9879\u6765\u51cf\u8f7b\u63a8\u8350\u504f\u5dee\u3002"}}
{"id": "2511.11916", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11916", "abs": "https://arxiv.org/abs/2511.11916", "authors": ["Sinan Urgun", "Se\u00e7kin Ar\u0131"], "title": "An Analysis of Architectural Impact on LLM-based Abstract Visual Reasoning: A Systematic Benchmark on RAVEN-FAIR", "comment": "23 pages, 9 figures", "summary": "This study aims to systematically evaluate the performance of large language models (LLMs) in abstract visual reasoning problems. We examined four LLM models (GPT-4.1-Mini, Claude-3.5-Haiku, Gemini-1.5-Flash, Llama-3.3-70b) utilizing four different reasoning architectures (single-shot, embedding-controlled repetition, self-reflection, and multi-agent) on the RAVEN-FAIR dataset. Visual responses generated through a three-stage process (JSON extraction, LLM reasoning, and Tool Function) were evaluated using SSIM and LPIPS metrics; Chain-of-Thought scores and error types (semantic hallucination, numeric misperception) were analyzed. Results demonstrate that GPT-4.1-Mini consistently achieved the highest overall accuracy across all architectures, indicating a strong reasoning capability. While the multi-agent architecture occasionally altered semantic and numeric balance across models, these effects were not uniformly beneficial. Instead, each model exhibited distinct sensitivity patterns to architectural design, underscoring that reasoning effectiveness remains model-specific. Variations in response coverage further emerged as a confounding factor that complicates direct cross-architecture comparison. To estimate the upper-bound performance of each configuration, we report the best of five independent runs, representing a best-case scenario rather than an averaged outcome. This multi-run strategy aligns with recent recommendations, which emphasize that single-run evaluations are fragile and may lead to unreliable conclusions.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u95ee\u9898\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0GPT-4.1-Mini\u6a21\u578b\u5728\u6240\u6709\u67b6\u6784\u4e2d\u59cb\u7ec8\u8868\u73b0\u51fa\u6700\u9ad8\u7684\u6574\u4f53\u51c6\u786e\u7387\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u95ee\u9898\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u4e86\u56db\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08GPT-4.1-Mini, Claude-3.5-Haiku, Gemini-1.5-Flash, Llama-3.3-70b\uff09\uff0c\u5229\u7528\u56db\u79cd\u4e0d\u540c\u7684\u63a8\u7406\u67b6\u6784\uff08\u5355\u6b21\u3001\u5d4c\u5165\u63a7\u5236\u91cd\u590d\u3001\u81ea\u6211\u53cd\u601d\u548c\u591a\u667a\u80fd\u4f53\uff09\u5728 RAVEN-FAIR \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u901a\u8fc7SSIM\u548cLPIPS\u6307\u6807\u8bc4\u4f30\u89c6\u89c9\u54cd\u5e94\uff0c\u5206\u6790\u601d\u7ef4\u94fe\u5f97\u5206\u548c\u9519\u8bef\u7c7b\u578b\u3002", "result": "GPT-4.1-Mini\u6a21\u578b\u5728\u6240\u6709\u67b6\u6784\u4e2d\u59cb\u7ec8\u8868\u73b0\u51fa\u6700\u9ad8\u7684\u6574\u4f53\u51c6\u786e\u7387\u3002\u591a\u667a\u80fd\u4f53\u67b6\u6784\u6709\u65f6\u4f1a\u6539\u53d8\u8de8\u6a21\u578b\u7684\u8bed\u4e49\u548c\u6570\u5b57\u5e73\u8861\uff0c\u4f46\u8fd9\u4e9b\u6548\u679c\u5e76\u975e\u666e\u904d\u6709\u76ca\u3002\u6bcf\u4e2a\u6a21\u578b\u5bf9\u67b6\u6784\u8bbe\u8ba1\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u654f\u611f\u6027\u6a21\u5f0f\u3002\u54cd\u5e94\u8986\u76d6\u7387\u7684\u53d8\u5316\u8fdb\u4e00\u6b65\u6210\u4e3a\u6df7\u6dc6\u76f4\u63a5\u8de8\u67b6\u6784\u6bd4\u8f83\u7684\u56e0\u7d20\u3002", "conclusion": "\u63a8\u7406\u6709\u6548\u6027\u4ecd\u7136\u662f\u7279\u5b9a\u4e8e\u6a21\u578b\u7684\uff0c\u5355\u6b21\u8bc4\u4f30\u53ef\u80fd\u5bfc\u81f4\u4e0d\u53ef\u9760\u7684\u7ed3\u8bba\u3002"}}
{"id": "2511.11966", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.11966", "abs": "https://arxiv.org/abs/2511.11966", "authors": ["Steven Cao", "Gregory Valiant", "Percy Liang"], "title": "On the Entropy Calibration of Language Models", "comment": "Neurips 2025", "summary": "We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.", "AI": {"tldr": "\u7814\u7a76\u71b5\u6821\u51c6\u95ee\u9898\uff0c\u5373\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u71b5\u662f\u5426\u4e0e\u5176\u5728\u4eba\u5de5\u6587\u672c\u4e0a\u7684\u5bf9\u6570\u635f\u5931\u76f8\u5339\u914d\u3002\u8fc7\u53bb\u7684\u7814\u7a76\u53d1\u73b0\uff0c\u6a21\u578b\u5b58\u5728\u6821\u51c6\u8bef\u5dee\uff0c\u968f\u7740\u751f\u6210\u957f\u5ea6\u7684\u589e\u52a0\uff0c\u6bcf\u6b65\u7684\u71b5\u589e\u52a0\uff08\u6587\u672c\u8d28\u91cf\u4e0b\u964d\uff09\u3002", "motivation": "\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u5b58\u5728\u8bef\u5dee\u7d2f\u79ef\u7684\u57fa\u672c\u95ee\u9898\uff0c\u6807\u51c6\u7684\u89e3\u51b3\u65b9\u6848\u662f\u622a\u65ad\u5206\u5e03\uff0c\u4f46\u8fd9\u4f1a\u4ee5\u727a\u7272\u591a\u6837\u6027\u4e3a\u4ee3\u4ef7\u6765\u63d0\u9ad8\u6587\u672c\u8d28\u91cf\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u5931\u51c6\u662f\u5426\u53ef\u80fd\u968f\u7740\u89c4\u6a21\u7684\u6269\u5927\u800c\u6539\u5584\uff0c\u4ee5\u53ca\u5728\u7406\u8bba\u4e0a\u662f\u5426\u6709\u53ef\u80fd\u5728\u4e0d\u8fdb\u884c\u6743\u8861\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u6821\u51c6\u3002", "method": "\u9996\u5148\uff0c\u7814\u7a76\u4e86\u4e00\u4e2a\u7b80\u5316\u7684\u7406\u8bba\u73af\u5883\uff0c\u4ee5\u63cf\u8ff0\u76f8\u5bf9\u4e8e\u6570\u636e\u96c6\u5927\u5c0f\u7684\u5931\u51c6\u7684\u7f29\u653e\u884c\u4e3a\u3002\u7136\u540e\uff0c\u5728\u53c2\u6570\u8303\u56f4\u4ece 0.5B \u5230 70B \u7684\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u5b9e\u8bc1\u5730\u6d4b\u91cf\u4e86\u5931\u51c6\u3002", "result": "\u53d1\u73b0\u7f29\u653e\u884c\u4e3a\u4e0e\u7b80\u5316\u8bbe\u7f6e\u7684\u9884\u6d4b\u76f8\u4f3c\uff1a\u6587\u672c\u7684\u62df\u5408\u7f29\u653e\u6307\u6570\u63a5\u8fd1\u4e8e 0\uff0c\u8fd9\u610f\u5473\u7740\u8f83\u5927\u7684\u6a21\u578b\u4ee5\u4e0e\u8f83\u5c0f\u7684\u6a21\u578b\u76f8\u4f3c\u7684\u901f\u7387\u7d2f\u79ef\u8bef\u5dee\u3002\u5373\u4f7f\u8f83\u5927\u7684\u6a21\u578b\u8d28\u91cf\u66f4\u9ad8\uff0c\u4f46\u6211\u4eec\u4ecd\u4ee5\u4e0e\u8f83\u5c0f\u7684\u6a21\u578b\u76f8\u4f3c\u7684\u622a\u65ad\u91cf\u4ece\u8f83\u5927\u7684\u6a21\u578b\u4e2d\u8fdb\u884c\u91c7\u6837\uff0c\u8fd9\u53ef\u4ee5\u89e3\u91ca\u8fd9\u79cd\u7f29\u653e\uff08\u6216\u7f3a\u4e4f\u7f29\u653e\uff09\u3002", "conclusion": "\u4ece\u7406\u8bba\u4e0a\u8bb2\uff0c\u5982\u679c\u6211\u4eec\u53ef\u4ee5\u8bbf\u95ee\u4e00\u4e2a\u9ed1\u76d2\u6765\u62df\u5408\u6a21\u578b\u4ee5\u9884\u6d4b\u6587\u672c\u7684\u672a\u6765\u71b5\uff0c\u90a3\u4e48\u964d\u4f4e\u71b5\u7684\u540c\u65f6\u4fdd\u6301\u5bf9\u6570\u635f\u5931\u662f\u53ef\u80fd\u7684\u3002"}}
{"id": "2511.11735", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.11735", "abs": "https://arxiv.org/abs/2511.11735", "authors": ["Yonatan Sverdlov", "Eitan Rosen", "Nadav Dym"], "title": "Toward bilipshiz geometric models", "comment": null, "summary": "Many neural networks for point clouds are, by design, invariant to the symmetries of this datatype: permutations and rigid motions. The purpose of this paper is to examine whether such networks preserve natural symmetry aware distances on the point cloud spaces, through the notion of bi-Lipschitz equivalence. This inquiry is motivated by recent work in the Equivariant learning literature which highlights the advantages of bi-Lipschitz models in other scenarios.\n  We consider two symmetry aware metrics on point clouds: (a) The Procrustes Matching (PM) metric and (b) Hard Gromov Wasserstien distances. We show that these two distances themselves are not bi-Lipschitz equivalent, and as a corollary deduce that popular invariant networks for point clouds are not bi-Lipschitz with respect to the PM metric. We then show how these networks can be modified so that they do obtain bi-Lipschitz guarantees. Finally, we provide initial experiments showing the advantage of the proposed bi-Lipschitz model over standard invariant models, for the tasks of finding correspondences between 3D point clouds.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u70b9\u4e91\u795e\u7ecf\u7f51\u7edc\u662f\u5426\u4fdd\u7559\u4e86\u70b9\u4e91\u7a7a\u95f4\u4e0a\u7684\u5bf9\u79f0\u611f\u77e5\u8ddd\u79bb\uff0c\u901a\u8fc7\u53cc Lipschitz \u7b49\u4ef7\u7684\u6982\u5ff5\u3002", "motivation": "\u63a2\u8ba8\u53cc Lipschitz \u6a21\u578b\u5728\u5176\u4ed6\u573a\u666f\u4e2d\u7684\u4f18\u52bf\u662f\u5426\u9002\u7528\u4e8e\u70b9\u4e91\u795e\u7ecf\u7f51\u7edc\u3002", "method": "\u8003\u8651\u4e86\u70b9\u4e91\u4e0a\u7684 Procrustes Matching (PM) \u5ea6\u91cf\u548c Hard Gromov Wasserstien \u8ddd\u79bb\uff0c\u5e76\u5206\u6790\u4e86\u5b83\u4eec\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u8bc1\u660e\u4e86\u8fd9\u4e24\u79cd\u8ddd\u79bb\u4e0d\u662f\u53cc Lipschitz \u7b49\u4ef7\u7684\uff0c\u5e76\u63a8\u5bfc\u51fa\u6d41\u884c\u7684\u4e0d\u53d8\u70b9\u4e91\u7f51\u7edc\u4e0e PM \u5ea6\u91cf\u4e0d\u662f\u53cc Lipschitz \u7684\u3002\u540c\u65f6\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u4fee\u6539\u8fd9\u4e9b\u7f51\u7edc\u4ee5\u83b7\u5f97\u53cc Lipschitz \u4fdd\u8bc1\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u53cc Lipschitz \u6a21\u578b\u5728 3D \u70b9\u4e91\u5bf9\u5e94\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6807\u51c6\u4e0d\u53d8\u6a21\u578b\u3002"}}
{"id": "2511.11602", "categories": ["cs.LG", "cs.GT", "cs.MA", "math.OC"], "pdf": "https://arxiv.org/pdf/2511.11602", "abs": "https://arxiv.org/abs/2511.11602", "authors": ["Georgios C. Chasparis"], "title": "Aspiration-based Perturbed Learning Automata in Games with Noisy Utility Measurements. Part A: Stochastic Stability in Non-zero-Sum Games", "comment": null, "summary": "Reinforcement-based learning has attracted considerable attention both in modeling human behavior as well as in engineering, for designing measurement- or payoff-based optimization schemes. Such learning schemes exhibit several advantages, especially in relation to filtering out noisy observations. However, they may exhibit several limitations when applied in a distributed setup. In multi-player weakly-acyclic games, and when each player applies an independent copy of the learning dynamics, convergence to (usually desirable) pure Nash equilibria cannot be guaranteed. Prior work has only focused on a small class of games, namely potential and coordination games. To address this main limitation, this paper introduces a novel payoff-based learning scheme for distributed optimization, namely aspiration-based perturbed learning automata (APLA). In this class of dynamics, and contrary to standard reinforcement-based learning schemes, each player's probability distribution for selecting actions is reinforced both by repeated selection and an aspiration factor that captures the player's satisfaction level. We provide a stochastic stability analysis of APLA in multi-player positive-utility games under the presence of noisy observations. This is the first part of the paper that characterizes stochastic stability in generic non-zero-sum games by establishing equivalence of the induced infinite-dimensional Markov chain with a finite dimensional one. In the second part, stochastic stability is further specialized to weakly acyclic games.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6536\u76ca\u7684\u5206\u5e03\u5f0f\u4f18\u5316\u5b66\u4e60\u65b9\u6848\uff0c\u79f0\u4e3a\u57fa\u4e8e\u671f\u671b\u7684\u6270\u52a8\u5b66\u4e60\u81ea\u52a8\u673a (APLA)\u3002", "motivation": "\u5728\u591a\u4eba\u5f31\u975e\u5faa\u73af\u535a\u5f08\u4e2d\uff0c\u5f53\u6bcf\u4e2a\u53c2\u4e0e\u8005\u5e94\u7528\u72ec\u7acb\u5b66\u4e60\u52a8\u6001\u526f\u672c\u65f6\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u6536\u655b\u5230\u7eaf\u7eb3\u4ec0\u5747\u8861\u3002", "method": "\u5f15\u5165 APLA \u52a8\u6001\uff0c\u5176\u4e2d\u6bcf\u4e2a\u53c2\u4e0e\u8005\u9009\u62e9\u52a8\u4f5c\u7684\u6982\u7387\u5206\u5e03\u4f1a\u56e0\u91cd\u590d\u9009\u62e9\u548c\u4e00\u4e2a\u6355\u83b7\u53c2\u4e0e\u8005\u6ee1\u610f\u5ea6\u7684\u671f\u671b\u56e0\u5b50\u800c\u5f97\u5230\u52a0\u5f3a\u3002\u901a\u8fc7\u5efa\u7acb\u8bf1\u5bfc\u7684\u65e0\u9650\u7ef4\u9a6c\u5c14\u53ef\u592b\u94fe\u4e0e\u6709\u9650\u7ef4\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u7b49\u4ef7\u6027\uff0c\u63d0\u4f9b\u4e86 APLA \u5728\u5b58\u5728\u566a\u58f0\u89c2\u6d4b\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u591a\u4eba\u6b63\u6548\u7528\u535a\u5f08\u4e2d\u7684\u968f\u673a\u7a33\u5b9a\u6027\u5206\u6790\u3002\u968f\u673a\u7a33\u5b9a\u6027\u8fdb\u4e00\u6b65\u4e13\u95e8\u7528\u4e8e\u5f31\u975e\u5faa\u73af\u535a\u5f08\u3002", "result": "\u5bf9 APLA \u5728\u591a\u4eba\u6b63\u6548\u7528\u535a\u5f08\u4e2d\u7684\u968f\u673a\u7a33\u5b9a\u6027\u8fdb\u884c\u4e86\u5206\u6790\u3002", "conclusion": "\u672c\u6587\u7814\u7a76\u4e86 APLA \u5728\u591a\u4eba\u535a\u5f08\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u4f9b\u4e86\u968f\u673a\u7a33\u5b9a\u6027\u5206\u6790\u3002"}}
{"id": "2511.13057", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13057", "abs": "https://arxiv.org/abs/2511.13057", "authors": ["Satyanarayan Pati"], "title": "Dimension vs. Precision: A Comparative Analysis of Autoencoders and Quantization for Efficient Vector Retrieval on BEIR SciFact", "comment": "16 pages, 9 figures, 1 table", "summary": "Dense retrieval models have become a standard for state-of-the-art information retrieval. However, their high-dimensional, high-precision (float32) vector embeddings create significant storage and memory challenges for real-world deployment. To address this, we conduct a rigorous empirical study on the BEIR SciFact benchmark, evaluating the trade-offs between two primary compression strategies: (1) Dimensionality Reduction via deep Autoencoders (AE), reducing original 384-dim vectors to latent spaces from 384 down to 12, and (2) Precision Reduction via Quantization (float16, int8, and binary). We systematically compare each method by measuring the \"performance loss\" (or gain) relative to a float32 baseline across a full suite of retrieval metrics (NDCG, MAP, MRR, Recall, Precision) at various k cutoffs. Our results show that int8 scalar quantization provides the most effective \"sweet spot,\" achieving a 4x compression with a negligible [~1-2%] drop in nDCG@10. In contrast, Autoencoders show a graceful degradation but suffer a more significant performance loss at equivalent 4x compression ratios (AE-96). binary quantization was found to be unsuitable for this task due to catastrophic performance drops. This work provides a practical guide for deploying efficient, high-performance retrieval systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u538b\u7f29\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b\uff0c\u4ee5\u51cf\u5c11\u5b58\u50a8\u548c\u5185\u5b58\u7684\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b\u7684\u9ad8\u7ef4\u5ea6\u5411\u91cf\u5d4c\u5165\u7ed9\u5b9e\u9645\u90e8\u7f72\u5e26\u6765\u4e86\u5b58\u50a8\u548c\u5185\u5b58\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5728BEIR SciFact\u57fa\u51c6\u4e0a\uff0c\u8bc4\u4f30\u964d\u7ef4\uff08\u4f7f\u7528\u81ea\u7f16\u7801\u5668\uff09\u548c\u91cf\u5316\uff08float16, int8, and binary\uff09\u4e24\u79cd\u538b\u7f29\u7b56\u7565\u7684\u6743\u8861\u3002", "result": "int8\u6807\u91cf\u91cf\u5316\u5728\u6027\u80fd\u635f\u5931\u5f88\u5c0f\u7684\u60c5\u51b5\u4e0b\uff08\u7ea61-2%\u7684nDCG@10\u4e0b\u964d\uff09\u5b9e\u73b0\u4e864\u500d\u538b\u7f29\uff0c\u6548\u679c\u6700\u597d\u3002\u81ea\u7f16\u7801\u5668\u867d\u7136\u8868\u73b0\u51fa\u5e73\u7f13\u7684\u6027\u80fd\u9000\u5316\uff0c\u4f46\u5728\u76f8\u540c\u76844\u500d\u538b\u7f29\u7387\u4e0b\uff0c\u6027\u80fd\u635f\u5931\u66f4\u5927\u3002Binary\u91cf\u5316\u7531\u4e8e\u6027\u80fd\u4e0b\u964d\u4e25\u91cd\uff0c\u4e0d\u9002\u5408\u8fd9\u9879\u4efb\u52a1\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u90e8\u7f72\u9ad8\u6548\u3001\u9ad8\u6027\u80fd\u7684\u68c0\u7d22\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2511.11921", "categories": ["cs.AI", "cs.ET", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.11921", "abs": "https://arxiv.org/abs/2511.11921", "authors": ["Liudong Xing", "Janet", "Lin"], "title": "Looking Forward: Challenges and Opportunities in Agentic AI Reliability", "comment": "13 pages, 6 figures; This is a preprint of a chapter accepted for publication in Generative and Agentic AI Reliability: Architectures, Challenges, and Trust for Autonomous Systems, published by SpringerNature", "summary": "This chapter presents perspectives for challenges and future development in building reliable AI systems, particularly, agentic AI systems. Several open research problems related to mitigating the risks of cascading failures are discussed. The chapter also sheds lights on research challenges and opportunities in aspects including dynamic environments, inconsistent task execution, unpredictable emergent behaviors, as well as resource-intensive reliability mechanisms. In addition, several research directions along the line of testing and evaluating reliability of agentic AI systems are also discussed.", "AI": {"tldr": "\u672c\u7ae0\u63a2\u8ba8\u4e86\u6784\u5efa\u53ef\u9760\u7684AI\u7cfb\u7edf\uff0c\u7279\u522b\u662fAgentic AI\u7cfb\u7edf\u6240\u9762\u4e34\u7684\u6311\u6218\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u65e8\u5728\u6784\u5efa\u53ef\u9760\u7684Agentic AI\u7cfb\u7edf\u3002", "method": "\u8ba8\u8bba\u4e86\u4e0e\u964d\u4f4e\u7ea7\u8054\u6545\u969c\u98ce\u9669\u76f8\u5173\u7684\u51e0\u4e2a\u5f00\u653e\u7814\u7a76\u95ee\u9898\u3002", "result": "\u9610\u660e\u4e86\u52a8\u6001\u73af\u5883\u3001\u4e0d\u4e00\u81f4\u7684\u4efb\u52a1\u6267\u884c\u3001\u4e0d\u53ef\u9884\u6d4b\u7684\u7a81\u53d1\u884c\u4e3a\u4ee5\u53ca\u8d44\u6e90\u5bc6\u96c6\u578b\u53ef\u9760\u6027\u673a\u5236\u7b49\u65b9\u9762\u7684\u7814\u7a76\u6311\u6218\u548c\u673a\u9047\u3002", "conclusion": "\u63a2\u8ba8\u4e86\u6d4b\u8bd5\u548c\u8bc4\u4f30Agentic AI\u7cfb\u7edf\u53ef\u9760\u6027\u7684\u51e0\u4e2a\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2511.11978", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11978", "abs": "https://arxiv.org/abs/2511.11978", "authors": ["Hui Huang", "Yanping Chen", "Ruizhang Huang", "Chuan Lin", "Yongbin Qin"], "title": "A Reasoning Paradigm for Named Entity Recognition", "comment": "Accepted at AAAI 2026", "summary": "Generative LLMs typically improve Named Entity Recognition (NER) performance through instruction tuning. They excel at generating entities by semantic pattern matching but lack an explicit, verifiable reasoning mechanism. This \"cognitive shortcutting\" leads to suboptimal performance and brittle generalization, especially in zero-shot and lowresource scenarios where reasoning from limited contextual cues is crucial. To address this issue, a reasoning framework is proposed for NER, which shifts the extraction paradigm from implicit pattern matching to explicit reasoning. This framework consists of three stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset annotated with NER-oriented CoTs is generated, which contain task-relevant reasoning chains. Then, they are used to tune the NER model to generate coherent rationales before deriving the final answer. Finally, a reasoning enhancement stage is implemented to optimize the reasoning process using a comprehensive reward signal. This stage ensures explicit and verifiable extractions. Experiments show that ReasoningNER demonstrates impressive cognitive ability in the NER task, achieving competitive performance. In zero-shot settings, it achieves state-of-the-art (SOTA) performance, outperforming GPT-4 by 12.3 percentage points on the F1 score. Analytical results also demonstrate its great potential to advance research in reasoningoriented information extraction. Our codes are available at https://github.com/HuiResearch/ReasoningIE.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684NER\u63a8\u7406\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u751f\u6210\u5f0fLLM\u5728zero-shot\u548c\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u6210\u5f0fLLM\u5728NER\u4efb\u52a1\u4e2d\u4f9d\u8d56\u8bed\u4e49\u6a21\u5f0f\u5339\u914d\uff0c\u7f3a\u4e4f\u663e\u5f0f\u63a8\u7406\u673a\u5236\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002", "method": "\u8be5\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1aCoT\u751f\u6210\u3001CoT\u8c03\u4f18\u548c\u63a8\u7406\u589e\u5f3a\uff0c\u901a\u8fc7\u663e\u5f0f\u63a8\u7406\u94fe\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "ReasoningNER\u5728NER\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8ba4\u77e5\u80fd\u529b\uff0c\u5728zero-shot\u8bbe\u7f6e\u4e0b\u4f18\u4e8eGPT-4 12.3\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u63a8\u7406\u5bfc\u5411\u4fe1\u606f\u63d0\u53d6\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2511.11751", "categories": ["cs.CV", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.11751", "abs": "https://arxiv.org/abs/2511.11751", "authors": ["Sanchit Sinha", "Guangzhi Xiong", "Zhenghao He", "Aidong Zhang"], "title": "Concept-RuleNet: Grounded Multi-Agent Neurosymbolic Reasoning in Vision Language Models", "comment": "AAAI 2026 (oral)", "summary": "Modern vision-language models (VLMs) deliver impressive predictive accuracy yet offer little insight into 'why' a decision is reached, frequently hallucinating facts, particularly when encountering out-of-distribution data. Neurosymbolic frameworks address this by pairing black-box perception with interpretable symbolic reasoning, but current methods extract their symbols solely from task labels, leaving them weakly grounded in the underlying visual data. In this paper, we introduce a multi-agent system - Concept-RuleNet that reinstates visual grounding while retaining transparent reasoning. Specifically, a multimodal concept generator first mines discriminative visual concepts directly from a representative subset of training images. Next, these visual concepts are utilized to condition symbol discovery, anchoring the generations in real image statistics and mitigating label bias. Subsequently, symbols are composed into executable first-order rules by a large language model reasoner agent - yielding interpretable neurosymbolic rules. Finally, during inference, a vision verifier agent quantifies the degree of presence of each symbol and triggers rule execution in tandem with outputs of black-box neural models, predictions with explicit reasoning pathways. Experiments on five benchmarks, including two challenging medical-imaging tasks and three underrepresented natural-image datasets, show that our system augments state-of-the-art neurosymbolic baselines by an average of 5% while also reducing the occurrence of hallucinated symbols in rules by up to 50%.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf Concept-RuleNet\uff0c\u5b83\u53ef\u4ee5\u589e\u5f3a\u89c6\u89c9\u57fa\u7840\u5e76\u4fdd\u6301\u900f\u660e\u7684\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b (VLM) \u9884\u6d4b\u51c6\u786e\u7387\u5f88\u9ad8\uff0c\u4f46\u5bf9\u51b3\u7b56\u7684\u539f\u56e0\u6d1e\u5bdf\u529b\u4e0d\u8db3\uff0c\u5e76\u4e14\u7ecf\u5e38\u51fa\u73b0\u5e7b\u89c9\uff0c\u5c24\u5176\u662f\u5728\u9047\u5230\u5206\u5e03\u5916\u6570\u636e\u65f6\u3002\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\u901a\u8fc7\u5c06\u9ed1\u76d2\u611f\u77e5\u4e0e\u53ef\u89e3\u91ca\u7684\u7b26\u53f7\u63a8\u7406\u76f8\u7ed3\u5408\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u5f53\u524d\u7684\u65b9\u6cd5\u4ec5\u4ece\u4efb\u52a1\u6807\u7b7e\u4e2d\u63d0\u53d6\u7b26\u53f7\uff0c\u5bfc\u81f4\u5b83\u4eec\u5728\u5e95\u5c42\u89c6\u89c9\u6570\u636e\u4e2d\u7684\u57fa\u7840\u8584\u5f31\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u7cfb\u7edf Concept-RuleNet\uff0c\u5b83\u6062\u590d\u4e86\u89c6\u89c9\u57fa\u7840\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u900f\u660e\u7684\u63a8\u7406\u3002\u8be5\u7cfb\u7edf\u9996\u5148\u4ece\u8bad\u7ec3\u56fe\u50cf\u7684\u4ee3\u8868\u6027\u5b50\u96c6\u4e2d\u6316\u6398\u6709\u533a\u522b\u7684\u89c6\u89c9\u6982\u5ff5\u3002\u63a5\u4e0b\u6765\uff0c\u8fd9\u4e9b\u89c6\u89c9\u6982\u5ff5\u88ab\u7528\u4e8e\u8c03\u8282\u7b26\u53f7\u53d1\u73b0\uff0c\u5c06\u751f\u6210\u951a\u5b9a\u5728\u771f\u5b9e\u7684\u56fe\u50cf\u7edf\u8ba1\u6570\u636e\u4e2d\uff0c\u5e76\u51cf\u8f7b\u6807\u7b7e\u504f\u5dee\u3002\u968f\u540e\uff0c\u7b26\u53f7\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u5668\u4ee3\u7406\u7ec4\u5408\u6210\u53ef\u6267\u884c\u7684\u4e00\u9636\u89c4\u5219\uff0c\u4ece\u800c\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u7b26\u53f7\u89c4\u5219\u3002\u6700\u540e\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u89c6\u89c9\u9a8c\u8bc1\u5668\u4ee3\u7406\u91cf\u5316\u6bcf\u4e2a\u7b26\u53f7\u7684\u5b58\u5728\u7a0b\u5ea6\uff0c\u5e76\u4e0e\u9ed1\u76d2\u795e\u7ecf\u6a21\u578b\u7684\u8f93\u51fa\u4e00\u8d77\u89e6\u53d1\u89c4\u5219\u6267\u884c\uff0c\u4ece\u800c\u5b9e\u73b0\u5177\u6709\u663e\u5f0f\u63a8\u7406\u8def\u5f84\u7684\u9884\u6d4b\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u533b\u5b66\u6210\u50cf\u4efb\u52a1\u548c\u4e09\u4e2a\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5c06\u6700\u5148\u8fdb\u7684\u795e\u7ecf\u7b26\u53f7\u57fa\u7ebf\u63d0\u9ad8\u4e86\u5e73\u5747 5%\uff0c\u540c\u65f6\u8fd8\u5c06\u89c4\u5219\u4e2d\u5e7b\u89c9\u7b26\u53f7\u7684\u51fa\u73b0\u6b21\u6570\u51cf\u5c11\u4e86\u9ad8\u8fbe 50%\u3002", "conclusion": "Concept-RuleNet \u589e\u5f3a\u4e86\u89c6\u89c9\u57fa\u7840\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u900f\u660e\u7684\u63a8\u7406\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684\u795e\u7ecf\u7b26\u53f7\u57fa\u7ebf\u3002"}}
{"id": "2511.11604", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.11604", "abs": "https://arxiv.org/abs/2511.11604", "authors": ["Amaratou Mahamadou Saley", "Thierry Moyaux", "A\u00efcha Sekhari", "Vincent Cheutet", "Jean-Baptiste Danielou"], "title": "Enhancing failure prediction in nuclear industry: Hybridization of knowledge- and data-driven techniques", "comment": "19 pages, 9 figures, 6 journal, Journal Q1 (Computers and Industrial Engineering)", "summary": "The convergence of the Internet of Things (IoT) and Industry 4.0 has significantly enhanced data-driven methodologies within the nuclear industry, notably enhancing safety and economic efficiency. This advancement challenges the precise prediction of future maintenance needs for assets, which is crucial for reducing downtime and operational costs. However, the effectiveness of data-driven methodologies in the nuclear sector requires extensive domain knowledge due to the complexity of the systems involved. Thus, this paper proposes a novel predictive maintenance methodology that combines data-driven techniques with domain knowledge from a nuclear equipment. The methodological originality of this paper is located on two levels: highlighting the limitations of purely data-driven approaches and demonstrating the importance of knowledge in enhancing the performance of the predictive models. The applicative novelty of this work lies in its use within a domain such as a nuclear industry, which is highly restricted and ultrasensitive due to security, economic and environmental concerns. A detailed real-world case study which compares the current state of equipment monitoring with two scenarios, demonstrate that the methodology significantly outperforms purely data-driven methods in failure prediction. While purely data-driven methods achieve only a modest performance with a prediction horizon limited to 3 h and a F1 score of 56.36%, the hybrid approach increases the prediction horizon to 24 h and achieves a higher F1 score of 93.12%.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u6280\u672f\u4e0e\u6838\u8bbe\u5907\u9886\u57df\u77e5\u8bc6\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6838\u5de5\u4e1a\u7684\u5b89\u5168\u6027\u548c\u7ecf\u6d4e\u6548\u7387\u3002", "motivation": "\u5728\u6838\u5de5\u4e1a\u4e2d\uff0c\u7cbe\u786e\u9884\u6d4b\u8d44\u4ea7\u7684\u672a\u6765\u7ef4\u62a4\u9700\u6c42\u5bf9\u4e8e\u51cf\u5c11\u505c\u673a\u65f6\u95f4\u548c\u8fd0\u8425\u6210\u672c\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7cfb\u7edf\u590d\u6742\u6027\uff0c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u6548\u679c\u53d7\u5230\u9650\u5236\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u6570\u636e\u9a71\u52a8\u6280\u672f\u4e0e\u6838\u8bbe\u5907\u9886\u57df\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6df7\u5408\u65b9\u6cd5\u5728\u6545\u969c\u9884\u6d4b\u65b9\u9762\u660e\u663e\u4f18\u4e8e\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u9884\u6d4b\u8303\u56f4\u4ece3\u5c0f\u65f6\u589e\u52a0\u523024\u5c0f\u65f6\uff0cF1\u503c\u4ece56.36%\u63d0\u9ad8\u523093.12%\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u9886\u57df\u77e5\u8bc6\u5728\u63d0\u9ad8\u9884\u6d4b\u6a21\u578b\u6027\u80fd\u65b9\u9762\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.13166", "categories": ["cs.IR", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.13166", "abs": "https://arxiv.org/abs/2511.13166", "authors": ["Zhaoxin Shen", "Dan Wu"], "title": "Local Collaborative Filtering: A Collaborative Filtering Method that Utilizes Local Similarities among Users", "comment": "4 pages, 2 figures", "summary": "To leverage user behavior data from the Internet more effectively in recommender systems, this paper proposes a novel collaborative filtering (CF) method called Local Collaborative Filtering (LCF). LCF utilizes local similarities among users and integrates their data using the law of large numbers (LLN), thereby improving the utilization of user behavior data. Experiments are conducted on the Steam game dataset, and the results of LCF align with real-world needs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5c40\u90e8\u534f\u540c\u8fc7\u6ee4\uff08LCF\uff09\u7684\u65b0\u578b\u534f\u540c\u8fc7\u6ee4\uff08CF\uff09\u65b9\u6cd5\uff0c\u65e8\u5728\u66f4\u6709\u6548\u5730\u5229\u7528\u6765\u81ea\u4e92\u8054\u7f51\u7684\u7528\u6237\u884c\u4e3a\u6570\u636e\u3002", "motivation": "\u4e3a\u4e86\u66f4\u6709\u6548\u5730\u5229\u7528\u6765\u81ea\u4e92\u8054\u7f51\u7684\u7528\u6237\u884c\u4e3a\u6570\u636e\u3002", "method": "LCF\u5229\u7528\u7528\u6237\u4e4b\u95f4\u7684\u5c40\u90e8\u76f8\u4f3c\u6027\uff0c\u5e76\u4f7f\u7528\u5927\u6570\u5b9a\u5f8b\uff08LLN\uff09\u6574\u5408\u4ed6\u4eec\u7684\u6570\u636e\uff0c\u4ece\u800c\u63d0\u9ad8\u7528\u6237\u884c\u4e3a\u6570\u636e\u7684\u5229\u7528\u7387\u3002", "result": "\u5728Steam\u6e38\u620f\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0cLCF\u7684\u7ed3\u679c\u4e0e\u73b0\u5b9e\u4e16\u754c\u7684\u9700\u6c42\u76f8\u7b26\u3002", "conclusion": "LCF\u65b9\u6cd5\u5728\u5229\u7528\u7528\u6237\u884c\u4e3a\u6570\u636e\u65b9\u9762\u662f\u6709\u6548\u7684\uff0c\u5e76\u4e14\u4e0e\u73b0\u5b9e\u9700\u6c42\u76f8\u7b26\u3002"}}
{"id": "2511.11924", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11924", "abs": "https://arxiv.org/abs/2511.11924", "authors": ["Yongkang Huo", "Fulvio Forni", "Rodolphe Sepulchre"], "title": "A Neuromorphic Architecture for Scalable Event-Based Control", "comment": null, "summary": "This paper introduces the ``rebound Winner-Take-All (RWTA)\" motif as the basic element of a scalable neuromorphic control architecture. From the cellular level to the system level, the resulting architecture combines the reliability of discrete computation and the tunability of continuous regulation: it inherits the discrete computation capabilities of winner-take-all state machines and the continuous tuning capabilities of excitable biophysical circuits. The proposed event-based framework addresses continuous rhythmic generation and discrete decision-making in a unified physical modeling language. We illustrate the versatility, robustness, and modularity of the architecture through the nervous system design of a snake robot.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u67b6\u6784\uff0c\u5b83\u7ed3\u5408\u4e86\u79bb\u6563\u8ba1\u7b97\u7684\u53ef\u9760\u6027\u548c\u8fde\u7eed\u8c03\u8282\u7684\u53ef\u8c03\u6027\u3002", "motivation": "\u6784\u5efa\u4e00\u4e2a\u53ef\u9760\u4e14\u53ef\u8c03\u7684\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u67b6\u6784\u3002", "method": "\u5f15\u5165\u4e86\u201c\u53cd\u5f39\u80dc\u8005\u5168\u5f97 (RWTA)\u201d\u4f5c\u4e3a\u57fa\u672c\u5143\u4ef6\uff0c\u5e76\u7ed3\u5408\u80dc\u8005\u5168\u5f97\u72b6\u6001\u673a\u7684\u79bb\u6563\u8ba1\u7b97\u80fd\u529b\u548c\u53ef\u5174\u594b\u751f\u7269\u7269\u7406\u7535\u8def\u7684\u8fde\u7eed\u8c03\u8c10\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u86c7\u5f62\u673a\u5668\u4eba\u7684\u795e\u7ecf\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u8bc1\u660e\u4e86\u8be5\u67b6\u6784\u7684\u591a\u529f\u80fd\u6027\u3001\u9c81\u68d2\u6027\u548c\u6a21\u5757\u5316\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u4e8b\u4ef6\u7684\u6846\u67b6\u5728\u7edf\u4e00\u7684\u7269\u7406\u5efa\u6a21\u8bed\u8a00\u4e2d\u89e3\u51b3\u4e86\u8fde\u7eed\u8282\u594f\u751f\u6210\u548c\u79bb\u6563\u51b3\u7b56\u95ee\u9898\u3002"}}
{"id": "2511.12001", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.12001", "abs": "https://arxiv.org/abs/2511.12001", "authors": ["Eunkyu Park", "Wesley Hanwen Deng", "Vasudha Varadarajan", "Mingxi Yan", "Gunhee Kim", "Maarten Sap", "Motahhare Eslami"], "title": "Critical or Compliant? The Double-Edged Sword of Reasoning in Chain-of-Thought Explanations", "comment": "Under review; 16 pages, 15 figures", "summary": "Explanations are often promoted as tools for transparency, but they can also foster confirmation bias; users may assume reasoning is correct whenever outputs appear acceptable. We study this double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios by systematically perturbing reasoning chains and manipulating delivery tones. Specifically, we analyze reasoning errors in vision language models (VLMs) and how they impact user trust and the ability to detect errors. Our findings reveal two key effects: (1) users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed, and (2) the confident tone suppresses error detection while maintaining reliance, showing that delivery styles can override correctness. These results highlight how CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust. All code will be released publicly.", "AI": {"tldr": "Chain-of-Thought (CoT) explanations in vision language models (VLMs) can mislead users by fostering confirmation bias, leading to sustained reliance even when reasoning is flawed.", "motivation": "To study the double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios, where explanations can promote transparency but also foster confirmation bias.", "method": "Systematically perturb reasoning chains and manipulate delivery tones in vision language models (VLMs) to analyze reasoning errors and their impact on user trust and error detection.", "result": "Users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed. Confident tones suppress error detection while maintaining reliance, showing that delivery styles can override correctness.", "conclusion": "CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust."}}
{"id": "2511.11754", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11754", "abs": "https://arxiv.org/abs/2511.11754", "authors": ["Stanislav Selitskiy"], "title": "Batch Transformer Architecture: Case of Synthetic Image Generation for Emotion Expression Facial Recognition", "comment": null, "summary": "A novel Transformer variation architecture is proposed in the implicit sparse style. Unlike \"traditional\" Transformers, instead of attention to sequential or batch entities in their entirety of whole dimensionality, in the proposed Batch Transformers, attention to the \"important\" dimensions (primary components) is implemented. In such a way, the \"important\" dimensions or feature selection allows for a significant reduction of the bottleneck size in the encoder-decoder ANN architectures. The proposed architecture is tested on the synthetic image generation for the face recognition task in the case of the makeup and occlusion data set, allowing for increased variability of the limited original data set.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Transformer\u53d8\u4f53\u67b6\u6784\uff0c\u5b83\u91c7\u7528\u9690\u5f0f\u7a00\u758f\u98ce\u683c\uff0c\u5173\u6ce8\u201c\u91cd\u8981\u7684\u201d\u7ef4\u5ea6\uff08\u4e3b\u8981\u6210\u5206\uff09\uff0c\u800c\u4e0d\u662f\u50cf\u4f20\u7edfTransformer\u90a3\u6837\u5173\u6ce8\u6574\u4e2a\u7ef4\u5ea6\u4e2d\u7684\u5e8f\u5217\u6216\u6279\u6b21\u5b9e\u4f53\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3encoder-decoder ANN\u67b6\u6784\u4e2d\u7684\u74f6\u9888\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u6709\u9650\u539f\u59cb\u6570\u636e\u96c6\u7684\u53ef\u53d8\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86\u4e00\u79cdBatch Transformers\uff0c\u5b83\u5173\u6ce8\u201c\u91cd\u8981\u7684\u201d\u7ef4\u5ea6\u6216\u7279\u5f81\u9009\u62e9\uff0c\u4ece\u800c\u663e\u8457\u51cf\u5c0f\u74f6\u9888\u5927\u5c0f\u3002", "result": "\u8be5\u67b6\u6784\u5728\u5316\u5986\u548c\u906e\u6321\u6570\u636e\u96c6\u7684\u5408\u6210\u56fe\u50cf\u751f\u6210\u4eba\u8138\u8bc6\u522b\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6709\u9650\u539f\u59cb\u6570\u636e\u96c6\u7684\u53ef\u53d8\u6027\u3002", "conclusion": "Batch Transformers\u901a\u8fc7\u5173\u6ce8\u91cd\u8981\u7ef4\u5ea6\uff0c\u6709\u6548\u51cf\u5c0f\u4e86encoder-decoder ANN\u67b6\u6784\u4e2d\u7684\u74f6\u9888\u5927\u5c0f\uff0c\u5e76\u5728\u4eba\u8138\u8bc6\u522b\u4efb\u52a1\u4e2d\u63d0\u9ad8\u4e86\u6570\u636e\u53ef\u53d8\u6027\u3002"}}
{"id": "2511.11607", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11607", "abs": "https://arxiv.org/abs/2511.11607", "authors": ["Guoqing Ma", "Yuhan Zhang", "Yuming Dai", "Guangfu Hao", "Yang Chen", "Shan Yu"], "title": "Clustering-Based Weight Orthogonalization for Stabilizing Deep Reinforcement Learning", "comment": null, "summary": "Reinforcement learning (RL) has made significant advancements, achieving superhuman performance in various tasks. However, RL agents often operate under the assumption of environmental stationarity, which poses a great challenge to learning efficiency since many environments are inherently non-stationary. This non-stationarity results in the requirement of millions of iterations, leading to low sample efficiency. To address this issue, we introduce the Clustering Orthogonal Weight Modified (COWM) layer, which can be integrated into the policy network of any RL algorithm and mitigate non-stationarity effectively. The COWM layer stabilizes the learning process by employing clustering techniques and a projection matrix. Our approach not only improves learning speed but also reduces gradient interference, thereby enhancing the overall learning efficiency. Empirically, the COWM outperforms state-of-the-art methods and achieves improvements of 9% and 12.6% in vision based and state-based DMControl benchmark. It also shows robustness and generality across various algorithms and tasks.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u975e\u9759\u6001\u73af\u5883\u4e0b\u7684\u5b66\u4e60\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u901a\u5e38\u5047\u8bbe\u73af\u5883\u662f\u9759\u6001\u7684\uff0c\u4f46\u5728\u975e\u9759\u6001\u73af\u5883\u4e2d\uff0c\u5b66\u4e60\u6548\u7387\u4f1a\u5927\u5927\u964d\u4f4e\uff0c\u9700\u8981\u6570\u767e\u4e07\u6b21\u8fed\u4ee3\u3002", "method": "\u8bba\u6587\u5f15\u5165\u4e86\u805a\u7c7b\u6b63\u4ea4\u6743\u91cd\u4fee\u6b63\uff08COWM\uff09\u5c42\uff0c\u53ef\u4ee5\u96c6\u6210\u5230\u4efb\u4f55\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u7b56\u7565\u7f51\u7edc\u4e2d\uff0c\u4ee5\u51cf\u8f7b\u975e\u9759\u6001\u6027\u3002", "result": "COWM\u5728\u57fa\u4e8e\u89c6\u89c9\u548c\u57fa\u4e8e\u72b6\u6001\u7684DMControl\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u5b9e\u73b0\u4e869%\u548c12.6%\u7684\u6539\u8fdb\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "COWM\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5b66\u4e60\u901f\u5ea6\uff0c\u51cf\u5c11\u4e86\u68af\u5ea6\u5e72\u6270\uff0c\u589e\u5f3a\u4e86\u6574\u4f53\u5b66\u4e60\u6548\u7387\uff0c\u5e76\u5728\u5404\u79cd\u7b97\u6cd5\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2511.13201", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.13201", "abs": "https://arxiv.org/abs/2511.13201", "authors": ["Hao Hu", "Yifan Feng", "Ruoxue Li", "Rundong Xue", "Xingliang Hou", "Zhiqiang Tian", "Yue Gao", "Shaoyi Du"], "title": "Cog-RAG: Cognitive-Inspired Dual-Hypergraph with Theme Alignment Retrieval-Augmented Generation", "comment": "Accepted by AAAI 2026 main conference", "summary": "Retrieval-Augmented Generation (RAG) enhances the response quality and domain-specific performance of large language models (LLMs) by incorporating external knowledge to combat hallucinations. In recent research, graph structures have been integrated into RAG to enhance the capture of semantic relations between entities. However, it primarily focuses on low-order pairwise entity relations, limiting the high-order associations among multiple entities. Hypergraph-enhanced approaches address this limitation by modeling multi-entity interactions via hyperedges, but they are typically constrained to inter-chunk entity-level representations, overlooking the global thematic organization and alignment across chunks. Drawing inspiration from the top-down cognitive process of human reasoning, we propose a theme-aligned dual-hypergraph RAG framework (Cog-RAG) that uses a theme hypergraph to capture inter-chunk thematic structure and an entity hypergraph to model high-order semantic relations. Furthermore, we design a cognitive-inspired two-stage retrieval strategy that first activates query-relevant thematic content from the theme hypergraph, and then guides fine-grained recall and diffusion in the entity hypergraph, achieving semantic alignment and consistent generation from global themes to local details. Our extensive experiments demonstrate that Cog-RAG significantly outperforms existing state-of-the-art baseline approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e3b\u9898\u5bf9\u9f50\u7684\u53cc\u8d85\u56feRAG\u6846\u67b6\uff08Cog-RAG\uff09\uff0c\u5b83\u4f7f\u7528\u4e3b\u9898\u8d85\u56fe\u6765\u6355\u83b7\u5757\u95f4\u4e3b\u9898\u7ed3\u6784\uff0c\u5e76\u4f7f\u7528\u5b9e\u4f53\u8d85\u56fe\u6765\u5efa\u6a21\u9ad8\u9636\u8bed\u4e49\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u7ed3\u6784RAG\u4e3b\u8981\u5173\u6ce8\u4f4e\u9636\u914d\u5bf9\u5b9e\u4f53\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u591a\u4e2a\u5b9e\u4f53\u4e4b\u95f4\u7684\u9ad8\u9636\u5173\u8054\u3002\u8d85\u56fe\u589e\u5f3a\u65b9\u6cd5\u901a\u8fc7\u8d85\u8fb9\u5efa\u6a21\u591a\u5b9e\u4f53\u4ea4\u4e92\u6765\u89e3\u51b3\u8fd9\u4e2a\u9650\u5236\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u88ab\u9650\u5236\u5728\u5757\u95f4\u5b9e\u4f53\u7ea7\u522b\u7684\u8868\u793a\uff0c\u5ffd\u7565\u4e86\u8de8\u5757\u7684\u5168\u5c40\u4e3b\u9898\u7ec4\u7ec7\u548c\u5bf9\u9f50\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8ba4\u77e5\u542f\u53d1\u7684\u4e24\u9636\u6bb5\u68c0\u7d22\u7b56\u7565\uff0c\u9996\u5148\u4ece\u4e3b\u9898\u8d85\u56fe\u4e2d\u6fc0\u6d3b\u67e5\u8be2\u76f8\u5173\u7684\u4e3b\u9898\u5185\u5bb9\uff0c\u7136\u540e\u5728\u5b9e\u4f53\u8d85\u56fe\u4e2d\u5f15\u5bfc\u7ec6\u7c92\u5ea6\u7684\u53ec\u56de\u548c\u6269\u6563\uff0c\u5b9e\u73b0\u4ece\u5168\u5c40\u4e3b\u9898\u5230\u5c40\u90e8\u7ec6\u8282\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u4e00\u81f4\u751f\u6210\u3002", "result": "Cog-RAG\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Cog-RAG\u6846\u67b6\u6709\u6548\u5730\u63d0\u5347\u4e86RAG\u5728\u6355\u83b7\u8bed\u4e49\u5173\u7cfb\u548c\u751f\u6210\u8d28\u91cf\u65b9\u9762\u7684\u6027\u80fd\u3002"}}
{"id": "2511.11945", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11945", "abs": "https://arxiv.org/abs/2511.11945", "authors": ["Mohammed Temraz", "Mark T Keane"], "title": "Augmenting The Weather: A Hybrid Counterfactual-SMOTE Algorithm for Improving Crop Growth Prediction When Climate Changes", "comment": "31 pages, 8 figures", "summary": "In recent years, humanity has begun to experience the catastrophic effects of climate change as economic sectors (such as agriculture) struggle with unpredictable and extreme weather events. Artificial Intelligence (AI) should help us handle these climate challenges but its most promising solutions are not good at dealing with climate-disrupted data; specifically, machine learning methods that work from historical data-distributions, are not good at handling out-of-distribution, outlier events. In this paper, we propose a novel data augmentation method, that treats the predictive problems around climate change as being, in part, due to class-imbalance issues; that is, prediction from historical datasets is difficult because, by definition, they lack sufficient minority-class instances of \"climate outlier events\". This novel data augmentation method -- called Counterfactual-Based SMOTE (CFA-SMOTE) -- combines an instance-based counterfactual method from Explainable AI (XAI) with the well-known class-imbalance method, SMOTE. CFA-SMOTE creates synthetic data-points representing outlier, climate-events that augment the dataset to improve predictive performance. We report comparative experiments using this CFA-SMOTE method, comparing it to benchmark counterfactual and class-imbalance methods under different conditions (i.e., class-imbalance ratios). The focal climate-change domain used relies on predicting grass growth on Irish dairy farms, during Europe-wide drought and forage crisis of 2018.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5 CFA-SMOTE\uff0c\u7528\u4e8e\u89e3\u51b3\u6c14\u5019\u53d8\u5316\u80cc\u666f\u4e0b\u7684\u6570\u636e\u5206\u5e03\u5916\u548c\u7c7b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u9884\u6d4b\u53d7\u6c14\u5019\u5f71\u54cd\u7684\u519c\u4e1a\u6570\u636e\u65b9\u9762\u3002", "motivation": "\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u6c14\u5019\u53d8\u5316\u5f15\u8d77\u7684\u6570\u636e\u5206\u5e03\u5916\u4e8b\u4ef6\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5386\u53f2\u6570\u636e\u4e2d\u7f3a\u4e4f\u8db3\u591f\u7684\u6c14\u5019\u5f02\u5e38\u4e8b\u4ef6\u6837\u672c\u3002", "method": "\u7ed3\u5408\u4e86\u89e3\u91ca\u6027 AI (XAI) \u4e2d\u7684\u57fa\u4e8e\u5b9e\u4f8b\u7684\u53cd\u4e8b\u5b9e\u65b9\u6cd5\u548c SMOTE \u7c7b\u4e0d\u5e73\u8861\u65b9\u6cd5\uff0c\u751f\u6210\u5408\u6210\u6570\u636e\u70b9\u6765\u6a21\u62df\u6c14\u5019\u5f02\u5e38\u4e8b\u4ef6\uff0c\u4ece\u800c\u589e\u5f3a\u6570\u636e\u96c6\u3002", "result": "\u901a\u8fc7\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86 CFA-SMOTE \u65b9\u6cd5\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u4e0e\u57fa\u51c6\u7684\u53cd\u4e8b\u5b9e\u548c\u7c7b\u4e0d\u5e73\u8861\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u5b9e\u9a8c\u9886\u57df\u4e3a\u9884\u6d4b 2018 \u5e74\u6b27\u6d32\u5e72\u65f1\u548c\u9972\u6599\u5371\u673a\u671f\u95f4\u7231\u5c14\u5170\u5976\u725b\u573a\u7684\u7267\u8349\u751f\u957f\u60c5\u51b5\u3002", "conclusion": "CFA-SMOTE \u80fd\u591f\u6709\u6548\u5e94\u5bf9\u6c14\u5019\u53d8\u5316\u5e26\u6765\u7684\u6570\u636e\u6311\u6218\uff0c\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2511.12014", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.12014", "abs": "https://arxiv.org/abs/2511.12014", "authors": ["Truong Vo", "Sanmi Koyejo"], "title": "CURE: Cultural Understanding and Reasoning Evaluation - A Framework for \"Thick\" Culture Alignment Evaluation in LLMs", "comment": "7 pages, 5 figures", "summary": "Large language models (LLMs) are increasingly deployed in culturally diverse environments, yet existing evaluations of cultural competence remain limited. Existing methods focus on de-contextualized correctness or forced-choice judgments, overlooking the need for cultural understanding and reasoning required for appropriate responses. To address this gap, we introduce a set of benchmarks that, instead of directly probing abstract norms or isolated statements, present models with realistic situational contexts that require culturally grounded reasoning. In addition to the standard Exact Match metric, we introduce four complementary metrics (Coverage, Specificity, Connotation, and Coherence) to capture different dimensions of model's response quality. Empirical analysis across frontier models reveals that thin evaluation systematically overestimates cultural competence and produces unstable assessments with high variance. In contrast, thick evaluation exposes differences in reasoning depth, reduces variance, and provides more stable, interpretable signals of cultural understanding.", "AI": {"tldr": "\u73b0\u6709\u7684\u6587\u5316\u80fd\u529b\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u5957\u57fa\u51c6\uff0c\u5b83\u4e3a\u6a21\u578b\u63d0\u4f9b\u4e86\u9700\u8981\u6587\u5316\u57fa\u7840\u63a8\u7406\u7684\u73b0\u5b9e\u60c5\u5883\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u53bb\u8bed\u5883\u5316\u7684\u6b63\u786e\u6027\u6216\u5f3a\u5236\u9009\u62e9\u5224\u65ad\uff0c\u5ffd\u7565\u4e86\u5bf9\u9002\u5f53\u53cd\u5e94\u6240\u9700\u7684\u6587\u5316\u7406\u89e3\u548c\u63a8\u7406\u7684\u9700\u8981\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u5957\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u4e0d\u662f\u76f4\u63a5\u63a2\u6d4b\u62bd\u8c61\u89c4\u8303\u6216\u5b64\u7acb\u7684\u9648\u8ff0\uff0c\u800c\u662f\u5411\u6a21\u578b\u5448\u73b0\u9700\u8981\u6587\u5316\u57fa\u7840\u63a8\u7406\u7684\u73b0\u5b9e\u60c5\u5883\u3002\u9664\u4e86\u6807\u51c6\u7684\u7cbe\u786e\u5339\u914d\u6307\u6807\u5916\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u56db\u4e2a\u4e92\u8865\u6307\u6807\uff08\u8986\u76d6\u7387\u3001\u7279\u5f02\u6027\u3001\u5185\u6db5\u548c\u8fde\u8d2f\u6027\uff09\u6765\u6355\u6349\u6a21\u578b\u54cd\u5e94\u8d28\u91cf\u7684\u4e0d\u540c\u7ef4\u5ea6\u3002", "result": "\u5bf9\u524d\u6cbf\u6a21\u578b\u7684\u5b9e\u8bc1\u5206\u6790\u8868\u660e\uff0c\u5355\u8584\u7684\u8bc4\u4f30\u7cfb\u7edf\u6027\u5730\u9ad8\u4f30\u4e86\u6587\u5316\u80fd\u529b\uff0c\u5e76\u4ea7\u751f\u5177\u6709\u9ad8\u65b9\u5dee\u7684\u4e0d\u7a33\u5b9a\u8bc4\u4f30\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6df1\u5165\u7684\u8bc4\u4f30\u66b4\u9732\u4e86\u63a8\u7406\u6df1\u5ea6\u7684\u5dee\u5f02\uff0c\u51cf\u5c11\u4e86\u65b9\u5dee\uff0c\u5e76\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u3001\u53ef\u89e3\u91ca\u7684\u6587\u5316\u7406\u89e3\u4fe1\u53f7\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u6709\u6548\u7684\u6587\u5316\u80fd\u529b\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u66f4\u8d34\u8fd1\u73b0\u5b9e\u7684\u57fa\u51c6\u548c\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u6a21\u578b\u5728\u6587\u5316\u7406\u89e3\u548c\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2511.11780", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11780", "abs": "https://arxiv.org/abs/2511.11780", "authors": ["Hossein Mohebbi", "Mohammed Abdulrahman", "Yanting Miao", "Pascal Poupart", "Suraj Kothawade"], "title": "Image-POSER: Reflective RL for Multi-Expert Image Generation and Editing", "comment": null, "summary": "Recent advances in text-to-image generation have produced strong single-shot models, yet no individual system reliably executes the long, compositional prompts typical of creative workflows. We introduce Image-POSER, a reflective reinforcement learning framework that (i) orchestrates a diverse registry of pretrained text-to-image and image-to-image experts, (ii) handles long-form prompts end-to-end through dynamic task decomposition, and (iii) supervises alignment at each step via structured feedback from a vision-language model critic. By casting image synthesis and editing as a Markov Decision Process, we learn non-trivial expert pipelines that adaptively combine strengths across models. Experiments show that Image-POSER outperforms baselines, including frontier models, across industry-standard and custom benchmarks in alignment, fidelity, and aesthetics, and is consistently preferred in human evaluations. These results highlight that reinforcement learning can endow AI systems with the capacity to autonomously decompose, reorder, and combine visual models, moving towards general-purpose visual assistants.", "AI": {"tldr": "Image-POSER\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u534f\u8c03\u5404\u79cd\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u548c\u56fe\u50cf\u5230\u56fe\u50cf\u4e13\u5bb6\uff0c\u901a\u8fc7\u52a8\u6001\u4efb\u52a1\u5206\u89e3\u5904\u7406\u957f\u683c\u5f0f\u63d0\u793a\uff0c\u5e76\u901a\u8fc7\u6765\u81ea\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6279\u8bc4\u8005\u7684\u7ed3\u6784\u5316\u53cd\u9988\u6765\u76d1\u7763\u6bcf\u4e00\u6b65\u7684\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u65e0\u6cd5\u53ef\u9760\u5730\u6267\u884c\u521b\u610f\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u5178\u578b\u7684\u957f\u800c\u590d\u6742\u7684\u63d0\u793a\u3002", "method": "\u5c06\u56fe\u50cf\u5408\u6210\u548c\u7f16\u8f91\u89c6\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5b66\u4e60\u81ea\u9002\u5e94\u5730\u7ed3\u5408\u5404\u79cd\u6a21\u578b\u4f18\u52bf\u7684\u4e13\u5bb6\u7ba1\u9053\u3002", "result": "Image-POSER\u5728\u5bf9\u9f50\u3001\u4fdd\u771f\u5ea6\u548c\u7f8e\u5b66\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u4e14\u5728\u4eba\u5de5\u8bc4\u4f30\u4e2d\u59cb\u7ec8\u66f4\u53d7\u6b22\u8fce\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u4f7fAI\u7cfb\u7edf\u80fd\u591f\u81ea\u4e3b\u5730\u5206\u89e3\u3001\u91cd\u65b0\u6392\u5e8f\u548c\u7ec4\u5408\u89c6\u89c9\u6a21\u578b\uff0c\u4ece\u800c\u671d\u7740\u901a\u7528\u89c6\u89c9\u52a9\u624b\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2511.11622", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11622", "abs": "https://arxiv.org/abs/2511.11622", "authors": ["Alexis Roger", "Gwen Legate", "Kashif Rasul", "Yuriy Nevmyvaka", "Irina Rish"], "title": "Small Vocabularies, Big Gains: Pretraining and Tokenization in Time Series Models", "comment": null, "summary": "Tokenization and transfer learning are two critical components in building state of the art time series foundation models for forecasting. In this work, we systematically study the effect of tokenizer design, specifically scaling and quantization strategies, on model performance, alongside the impact of pretraining versus random initialization. We show that tokenizer configuration primarily governs the representational capacity and stability of the model, while transfer learning influences optimization efficiency and alignment. Using a combination of empirical training experiments and theoretical analyses, we demonstrate that pretrained models consistently leverage well-designed tokenizers more effectively, particularly at smaller vocabulary sizes. Conversely, misaligned tokenization can diminish or even invert the benefits of pretraining. These findings highlight the importance of careful tokenization in time series modeling and suggest that combining small, efficient vocabularies with pretrained weights is especially advantageous in multi-modal forecasting settings, where the overall vocabulary must be shared across modalities. Our results provide concrete guidance for designing tokenizers and leveraging transfer learning in discrete representation learning for continuous signals.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5206\u8bcd\u5668\u8bbe\u8ba1\uff08\u7279\u522b\u662f\u7f29\u653e\u548c\u91cf\u5316\u7b56\u7565\uff09\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u9884\u8bad\u7ec3\u4e0e\u968f\u673a\u521d\u59cb\u5316\u5e26\u6765\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u5206\u8bcd\u5668\u8bbe\u8ba1\u548c\u8fc1\u79fb\u5b66\u4e60\u5bf9\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7ecf\u9a8c\u8bad\u7ec3\u5b9e\u9a8c\u548c\u7406\u8bba\u5206\u6790\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u3002", "result": "\u9884\u8bad\u7ec3\u6a21\u578b\u80fd\u66f4\u6709\u6548\u5730\u5229\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5206\u8bcd\u5668\uff0c\u5c24\u5176\u662f\u5728\u8f83\u5c0f\u7684\u8bcd\u6c47\u91cf\u4e0b\u3002\u76f8\u53cd\uff0c\u9519\u8bef\u7684\u5206\u8bcd\u4f1a\u964d\u4f4e\u751a\u81f3\u9006\u8f6c\u9884\u8bad\u7ec3\u7684\u4f18\u52bf\u3002", "conclusion": "\u5728\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u4e2d\uff0c\u4ed4\u7ec6\u7684\u5206\u8bcd\u975e\u5e38\u91cd\u8981\u3002\u5728\u591a\u6a21\u6001\u9884\u6d4b\u8bbe\u7f6e\u4e2d\uff0c\u5c06\u5c0f\u578b\u3001\u9ad8\u6548\u7684\u8bcd\u6c47\u8868\u4e0e\u9884\u8bad\u7ec3\u6743\u91cd\u76f8\u7ed3\u5408\u5c24\u5176\u6709\u5229\uff0c\u56e0\u4e3a\u5728\u591a\u6a21\u6001\u9884\u6d4b\u8bbe\u7f6e\u4e2d\uff0c\u603b\u4f53\u8bcd\u6c47\u8868\u5fc5\u987b\u5728\u6a21\u6001\u4e4b\u95f4\u5171\u4eab\u3002"}}
{"id": "2511.13389", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13389", "abs": "https://arxiv.org/abs/2511.13389", "authors": ["Zhipeng Ma", "Bo N\u00f8rregaard J\u00f8rgensen", "Zheng Grace Ma"], "title": "Uncovering Causal Drivers of Energy Efficiency for Industrial Process in Foundry via Time-Series Causal Inference", "comment": "Accepted by the Energy Informatics.Academy Conference 2025 (EI.A 2025)", "summary": "Improving energy efficiency in industrial foundry processes is a critical challenge, as these operations are highly energy-intensive and marked by complex interdependencies among process variables. Correlation-based analyses often fail to distinguish true causal drivers from spurious associations, limiting their usefulness for decision-making. This paper applies a time-series causal inference framework to identify the operational factors that directly affect energy efficiency in induction furnace melting. Using production data from a Danish foundry, the study integrates time-series clustering to segment melting cycles into distinct operational modes with the PCMCI+ algorithm, a state-of-the-art causal discovery method, to uncover cause-effect relationships within each mode. Across clusters, robust causal relations among energy consumption, furnace temperature, and material weight define the core drivers of efficiency, while voltage consistently influences cooling water temperature with a delayed response. Cluster-specific differences further distinguish operational regimes: efficient clusters are characterized by stable causal structures, whereas inefficient ones exhibit reinforcing feedback loops and atypical dependencies. The contributions of this study are twofold. First, it introduces an integrated clustering-causal inference pipeline as a methodological innovation for analyzing energy-intensive processes. Second, it provides actionable insights that enable foundry operators to optimize performance, reduce energy consumption, and lower emissions.", "AI": {"tldr": "\u672c\u6587\u8fd0\u7528\u65f6\u95f4\u5e8f\u5217\u56e0\u679c\u63a8\u7406\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u611f\u5e94\u7089\u7194\u70bc\u4e2d\u76f4\u63a5\u5f71\u54cd\u80fd\u6e90\u6548\u7387\u7684\u8fd0\u884c\u56e0\u7d20\u3002", "motivation": "\u5de5\u4e1a\u94f8\u9020\u8fc7\u7a0b\u80fd\u8017\u9ad8\uff0c\u8fc7\u7a0b\u53d8\u91cf\u95f4\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\u590d\u6742\uff0c\u56e0\u6b64\u63d0\u9ad8\u5de5\u4e1a\u94f8\u9020\u8fc7\u7a0b\u7684\u80fd\u6e90\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u5206\u6790\u901a\u5e38\u65e0\u6cd5\u533a\u5206\u771f\u5b9e\u7684\u56e0\u679c\u9a71\u52a8\u56e0\u7d20\u548c\u865a\u5047\u5173\u8054\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u51b3\u7b56\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u8be5\u7814\u7a76\u6574\u5408\u4e86\u65f6\u95f4\u5e8f\u5217\u805a\u7c7b\uff0c\u5c06\u7194\u70bc\u5468\u671f\u5206\u5272\u6210\u4e0d\u540c\u7684\u8fd0\u884c\u6a21\u5f0f\uff0c\u5e76\u5229\u7528\u6700\u5148\u8fdb\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5PCMCI+\u7b97\u6cd5\uff0c\u63ed\u793a\u4e86\u6bcf\u79cd\u6a21\u5f0f\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u80fd\u6e90\u6d88\u8017\u3001\u7089\u6e29\u548c\u6750\u6599\u91cd\u91cf\u4e4b\u95f4\u5b58\u5728\u7a33\u5065\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u662f\u80fd\u6e90\u6548\u7387\u7684\u6838\u5fc3\u9a71\u52a8\u56e0\u7d20\uff0c\u800c\u7535\u538b\u59cb\u7ec8\u4ee5\u5ef6\u8fdf\u54cd\u5e94\u7684\u65b9\u5f0f\u5f71\u54cd\u51b7\u5374\u6c34\u6e29\u5ea6\u3002\u9ad8\u6548\u96c6\u7fa4\u4ee5\u7a33\u5b9a\u7684\u56e0\u679c\u7ed3\u6784\u4e3a\u7279\u5f81\uff0c\u800c\u4f4e\u6548\u96c6\u7fa4\u5219\u8868\u73b0\u51fa\u5f3a\u5316\u7684\u53cd\u9988\u56de\u8def\u548c\u975e\u5178\u578b\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "conclusion": "\u8be5\u7814\u7a76\u7684\u8d21\u732e\u662f\u53cc\u91cd\u7684\u3002\u9996\u5148\uff0c\u5b83\u5f15\u5165\u4e86\u4e00\u4e2a\u96c6\u6210\u7684\u805a\u7c7b-\u56e0\u679c\u63a8\u7406\u7ba1\u9053\uff0c\u4f5c\u4e3a\u5206\u6790\u80fd\u6e90\u5bc6\u96c6\u578b\u8fc7\u7a0b\u7684\u65b9\u6cd5\u521b\u65b0\u3002\u5176\u6b21\uff0c\u5b83\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u4f7f\u94f8\u9020\u64cd\u4f5c\u5458\u80fd\u591f\u4f18\u5316\u6027\u80fd\uff0c\u51cf\u5c11\u80fd\u6e90\u6d88\u8017\u5e76\u964d\u4f4e\u6392\u653e\u3002"}}
{"id": "2511.11954", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11954", "abs": "https://arxiv.org/abs/2511.11954", "authors": ["Borchuluun Yadamsuren", "Steven Keith Platt", "Miguel Diaz"], "title": "LLM-Assisted Formalization Enables Deterministic Detection of Statutory Inconsistency in the Internal Revenue Code", "comment": "29 pages, 3 appendices with Prolog code and full codebase available at: https://github.com/borchuluun/section121-inconsistency-detection", "summary": "This study introduces a hybrid neuro-symbolic framework that achieves deterministic detection of statutory inconsistency in complex law. We use the U.S. Internal Revenue Code (IRC) as a case study because its complexity makes it a fertile domain for identifying conflicts. Our research offers a solution for detecting inconsistent provisions by combining Large Language Models (LLMs) with symbolic logic.\n  LLM-based methods can support compliance, fairness, and statutory drafting, yet tax-specific applications remain sparse. A key challenge is that such models struggle with hierarchical processing and deep structured reasoning, especially over long text.\n  This research addresses these gaps through experiments using GPT-4o, GPT-5, and Prolog. GPT-4o was first used to translate Section 121 into Prolog rules and refine them in SWISH. These rules were then incorporated into prompts to test whether Prolog-augmented prompting improved GPT-4o's inconsistency detection. GPT-4o, whether prompted with natural language alone or with Prolog augmentation, detected the inconsistency in only one of three strategies (33 percent accuracy), but its reasoning quality differed: natural-language prompting achieved 100 percent rule coverage, while Prolog-augmented prompting achieved 66 percent, indicating more incomplete statutory analysis.\n  In contrast to probabilistic prompting, the hybrid Prolog model produced deterministic and reproducible results. Guided by GPT-5 for refinement, the model formalized the IRC section's competing interpretations and successfully detected an inconsistency zone. Validation tests confirm that the Prolog implementation is accurate, internally consistent, deterministic, and capable of autonomously identifying inconsistencies. These findings show that LLM-assisted formalization, anchored in symbolic logic, enables transparent and reliable statutory inconsistency detection.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6df7\u5408\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4e0e\u7b26\u53f7\u903b\u8f91\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u590d\u6742\u6cd5\u5f8b\u4e2d\u6cd5\u5b9a\u4e0d\u4e00\u81f4\u6027\u7684\u786e\u5b9a\u6027\u68c0\u6d4b\u3002", "motivation": "\u57fa\u4e8e LLM \u7684\u65b9\u6cd5\u53ef\u4ee5\u652f\u6301\u5408\u89c4\u6027\u3001\u516c\u5e73\u6027\u548c\u6cd5\u89c4\u8d77\u8349\uff0c\u4f46\u7279\u5b9a\u4e8e\u7a0e\u6536\u7684\u5e94\u7528\u7a0b\u5e8f\u4ecd\u7136\u5f88\u5c11\u3002\u4e00\u4e2a\u5173\u952e\u7684\u6311\u6218\u662f\uff0c\u6b64\u7c7b\u6a21\u578b\u96be\u4ee5\u8fdb\u884c\u5206\u5c42\u5904\u7406\u548c\u6df1\u5ea6\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u5c24\u5176\u662f\u5728\u957f\u6587\u672c\u4e2d\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528 GPT-4o \u5c06\u7b2c 121 \u6761\u7ffb\u8bd1\u6210 Prolog \u89c4\u5219\uff0c\u5e76\u5728 SWISH \u4e2d\u5bf9\u5176\u8fdb\u884c\u5b8c\u5584\u3002\u7136\u540e\u5c06\u8fd9\u4e9b\u89c4\u5219\u7eb3\u5165\u63d0\u793a\u4e2d\uff0c\u4ee5\u6d4b\u8bd5 Prolog \u589e\u5f3a\u63d0\u793a\u662f\u5426\u63d0\u9ad8\u4e86 GPT-4o \u7684\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\u80fd\u529b\u3002", "result": "GPT-4o \u5728\u4e09\u79cd\u7b56\u7565\u4e2d\u4ec5\u68c0\u6d4b\u5230\u4e00\u79cd\u7b56\u7565\u7684\u4e0d\u4e00\u81f4\u6027\uff0833% \u7684\u51c6\u786e\u7387\uff09\uff0c\u4f46\u5176\u63a8\u7406\u8d28\u91cf\u4e0d\u540c\uff1a\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u5b9e\u73b0\u4e86 100% \u7684\u89c4\u5219\u8986\u76d6\u7387\uff0c\u800c Prolog \u589e\u5f3a\u63d0\u793a\u5b9e\u73b0\u4e86 66%\uff0c\u8868\u660e\u6cd5\u5b9a\u5206\u6790\u4e0d\u5b8c\u6574\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6df7\u5408 Prolog \u6a21\u578b\u4ea7\u751f\u4e86\u786e\u5b9a\u6027\u4e14\u53ef\u91cd\u73b0\u7684\u7ed3\u679c\u3002\u5728 GPT-5 \u7684\u6307\u5bfc\u4e0b\uff0c\u8be5\u6a21\u578b\u6b63\u5f0f\u786e\u5b9a\u4e86 IRC \u90e8\u5206\u7684\u7ade\u4e89\u6027\u89e3\u91ca\uff0c\u5e76\u6210\u529f\u68c0\u6d4b\u5230\u4e0d\u4e00\u81f4\u533a\u57df\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4ee5\u7b26\u53f7\u903b\u8f91\u4e3a\u57fa\u7840\u7684 LLM \u8f85\u52a9\u5f62\u5f0f\u5316\u80fd\u591f\u5b9e\u73b0\u900f\u660e\u4e14\u53ef\u9760\u7684\u6cd5\u5b9a\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\u3002"}}
{"id": "2511.12109", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12109", "abs": "https://arxiv.org/abs/2511.12109", "authors": ["Felipe Fujita", "Hideyuki Takada"], "title": "Exploring Parameter-Efficient Fine-Tuning and Backtranslation for the WMT 25 General Translation Task", "comment": null, "summary": "In this paper, we explore the effectiveness of combining fine-tuning and backtranslation on a small Japanese corpus for neural machine translation. Starting from a baseline English{\\textrightarrow}Japanese model (COMET = 0.460), we first apply backtranslation (BT) using synthetic data generated from monolingual Japanese corpora, yielding a modest increase (COMET = 0.468). Next, we fine-tune (FT) the model on a genuine small parallel dataset drawn from diverse Japanese news and literary corpora, achieving a substantial jump to COMET = 0.589 when using Mistral 7B. Finally, we integrate both backtranslation and fine-tuning{ -- }first augmenting the small dataset with BT generated examples, then adapting via FT{ -- }which further boosts performance to COMET = 0.597. These results demonstrate that, even with limited training data, the synergistic use of backtranslation and targeted fine-tuning on Japanese corpora can significantly enhance translation quality, outperforming each technique in isolation. This approach offers a lightweight yet powerful strategy for improving low-resource language pairs.", "AI": {"tldr": "\u7ed3\u5408\u5fae\u8c03\u548c\u56de\u8bd1\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u5c0f\u578b\u65e5\u8bed\u8bed\u6599\u5e93\u7684\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u5728\u5c0f\u578b\u65e5\u8bed\u8bed\u6599\u5e93\u4e0a\uff0c\u7ed3\u5408\u5fae\u8c03\u548c\u56de\u8bd1\u5bf9\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u7684\u6709\u6548\u6027\u3002", "method": "\u9996\u5148\u4f7f\u7528\u4ece\u5355\u8bed\u65e5\u8bed\u8bed\u6599\u5e93\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u8fdb\u884c\u56de\u8bd1(BT)\uff0c\u7136\u540e\u5728\u4e00\u4e2a\u6765\u81ea\u4e0d\u540c\u65e5\u8bed\u65b0\u95fb\u548c\u6587\u5b66\u8bed\u6599\u5e93\u7684\u771f\u5b9e\u5c0f\u578b\u5e76\u884c\u6570\u636e\u96c6\u4e0a\u5bf9\u6a21\u578b\u8fdb\u884c\u5fae\u8c03(FT)\u3002\u6700\u540e\uff0c\u5c06\u56de\u8bd1\u548c\u5fae\u8c03\u76f8\u7ed3\u5408\u3002", "result": "\u5355\u72ec\u4f7f\u7528\u56de\u8bd1COMET=0.468\uff0c\u5355\u72ec\u4f7f\u7528\u5fae\u8c03COMET=0.589\uff0c\u7ed3\u5408\u56de\u8bd1\u548c\u5fae\u8c03COMET=0.597\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u8bad\u7ec3\u6570\u636e\u6709\u9650\uff0c\u7ed3\u5408\u4f7f\u7528\u56de\u8bd1\u548c\u6709\u9488\u5bf9\u6027\u7684\u5fae\u8c03\u4e5f\u80fd\u663e\u8457\u63d0\u9ad8\u7ffb\u8bd1\u8d28\u91cf\u3002", "conclusion": "\u5373\u4f7f\u4f7f\u7528\u6709\u9650\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u901a\u8fc7\u56de\u8bd1\u548c\u6709\u9488\u5bf9\u6027\u7684\u5fae\u8c03\u7684\u534f\u540c\u4f7f\u7528\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528\u6bcf\u79cd\u6280\u672f\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u6539\u8fdb\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4f46\u529f\u80fd\u5f3a\u5927\u7684\u7b56\u7565\u3002"}}
{"id": "2511.11824", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11824", "abs": "https://arxiv.org/abs/2511.11824", "authors": ["Zhongping Dong", "Pengyang Yu", "Shuangjian Li", "Liming Chen", "Mohand Tahar Kechadi"], "title": "SOTFormer: A Minimal Transformer for Unified Object Tracking and Trajectory Prediction", "comment": null, "summary": "Accurate single-object tracking and short-term motion forecasting remain challenging under occlusion, scale variation, and temporal drift, which disrupt the temporal coherence required for real-time perception. We introduce \\textbf{SOTFormer}, a minimal constant-memory temporal transformer that unifies object detection, tracking, and short-horizon trajectory prediction within a single end-to-end framework. Unlike prior models with recurrent or stacked temporal encoders, SOTFormer achieves stable identity propagation through a ground-truth-primed memory and a burn-in anchor loss that explicitly stabilizes initialization. A single lightweight temporal-attention layer refines embeddings across frames, enabling real-time inference with fixed GPU memory. On the Mini-LaSOT (20%) benchmark, SOTFormer attains 76.3 AUC and 53.7 FPS (AMP, 4.3 GB VRAM), outperforming transformer baselines such as TrackFormer and MOTRv2 under fast motion, scale change, and occlusion.", "AI": {"tldr": "SOTFormer: A constant-memory temporal transformer for single-object tracking and short-term motion forecasting.", "motivation": "Accurate tracking and forecasting are difficult due to occlusion, scale variation, and temporal drift.", "method": "A minimal constant-memory temporal transformer unifies detection, tracking, and trajectory prediction. It uses a ground-truth-primed memory and burn-in anchor loss for stable identity propagation. A temporal-attention layer refines embeddings.", "result": "On Mini-LaSOT (20%), SOTFormer achieves 76.3 AUC and 53.7 FPS, outperforming other transformers.", "conclusion": "SOTFormer achieves real-time inference with fixed GPU memory and outperforms baselines in challenging conditions."}}
{"id": "2511.11623", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11623", "abs": "https://arxiv.org/abs/2511.11623", "authors": ["Yushan Jiang", "Shuteng Niu", "Dongjin Song", "Yichen Wang", "Jingna Feng", "Xinyue Hu", "Liu Yang", "Cui Tao"], "title": "Early GVHD Prediction in Liver Transplantation via Multi-Modal Deep Learning on Imbalanced EHR Data", "comment": null, "summary": "Graft-versus-host disease (GVHD) is a rare but often fatal complication in liver transplantation, with a very high mortality rate. By harnessing multi-modal deep learning methods to integrate heterogeneous and imbalanced electronic health records (EHR), we aim to advance early prediction of GVHD, paving the way for timely intervention and improved patient outcomes. In this study, we analyzed pre-transplant electronic health records (EHR) spanning the period before surgery for 2,100 liver transplantation patients, including 42 cases of graft-versus-host disease (GVHD), from a cohort treated at Mayo Clinic between 1992 and 2025. The dataset comprised four major modalities: patient demographics, laboratory tests, diagnoses, and medications. We developed a multi-modal deep learning framework that dynamically fuses these modalities, handles irregular records with missing values, and addresses extreme class imbalance through AUC-based optimization. The developed framework outperforms all single-modal and multi-modal machine learning baselines, achieving an AUC of 0.836, an AUPRC of 0.157, a recall of 0.768, and a specificity of 0.803. It also demonstrates the effectiveness of our approach in capturing complementary information from different modalities, leading to improved performance. Our multi-modal deep learning framework substantially improves existing approaches for early GVHD prediction. By effectively addressing the challenges of heterogeneity and extreme class imbalance in real-world EHR, it achieves accurate early prediction. Our proposed multi-modal deep learning method demonstrates promising results for early prediction of a GVHD in liver transplantation, despite the challenge of extremely imbalanced EHR data.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u809d\u79fb\u690d\u672f\u540eGVHD\u7684\u65e9\u671f\u9884\u6d4b\u80fd\u529b\uff0c\u4ece\u800c\u4e3a\u53ca\u65f6\u5e72\u9884\u548c\u6539\u5584\u60a3\u8005\u9884\u540e\u94fa\u5e73\u9053\u8def\u3002", "motivation": "\u809d\u79fb\u690d\u672f\u540e\u7684\u79fb\u690d\u7269\u6297\u5bbf\u4e3b\u75c5\uff08GVHD\uff09\u662f\u4e00\u79cd\u7f55\u89c1\u4f46\u901a\u5e38\u662f\u81f4\u547d\u7684\u5e76\u53d1\u75c7\uff0c\u6b7b\u4ea1\u7387\u975e\u5e38\u9ad8\u3002", "method": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u52a8\u6001\u878d\u5408\u4e86\u60a3\u8005\u4eba\u53e3\u7edf\u8ba1\u5b66\u3001\u5b9e\u9a8c\u5ba4\u68c0\u67e5\u3001\u8bca\u65ad\u548c\u836f\u7269\u56db\u4e2a\u4e3b\u8981\u6a21\u6001\uff0c\u5904\u7406\u5177\u6709\u7f3a\u5931\u503c\u7684\u4e0d\u89c4\u5219\u8bb0\u5f55\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8eAUC\u7684\u4f18\u5316\u89e3\u51b3\u4e86\u6781\u7aef\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u8be5\u6846\u67b6\u4f18\u4e8e\u6240\u6709\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u57fa\u7ebf\uff0cAUC\u8fbe\u52300.836\uff0cAUPRC\u8fbe\u52300.157\uff0c\u53ec\u56de\u7387\u8fbe\u52300.768\uff0c\u7279\u5f02\u6027\u8fbe\u52300.803\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u809d\u79fb\u690d\u672f\u540eGVHD\u7684\u65e9\u671f\u9884\u6d4b\u4e2d\u663e\u793a\u51fa\u5f88\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u5c3d\u7ba1\u9762\u4e34\u7740\u6781\u4e0d\u5e73\u8861\u7684EHR\u6570\u636e\u7684\u6311\u6218\u3002"}}
{"id": "2511.13415", "categories": ["cs.IR", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.13415", "abs": "https://arxiv.org/abs/2511.13415", "authors": ["Wanqing Cui", "Wei Huang", "Yazhi Guo", "Yibo Hu", "Meiguang Jin", "Junfeng Ma", "Keping Bi"], "title": "Attention Grounded Enhancement for Visual Document Retrieval", "comment": null, "summary": "Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs. Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance. However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match. As a result, retrievers tend to rely on surface-level cues and struggle to capture implicit semantic connections, hindering their ability to handle non-extractive queries. To alleviate this problem, we propose a \\textbf{A}ttention-\\textbf{G}rounded \\textbf{RE}triever \\textbf{E}nhancement (AGREE) framework. AGREE leverages cross-modal attention from multimodal large language models as proxy local supervision to guide the identification of relevant document regions. During training, AGREE combines local signals with the global signals to jointly optimize the retriever, enabling it to learn not only whether documents match, but also which content drives relevance. Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline. Quantitative and qualitative analyses further demonstrate that AGREE promotes deeper alignment between query terms and document regions, moving beyond surface-level matching toward more accurate and interpretable retrieval. Our code is available at: https://anonymous.4open.science/r/AGREE-2025.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAGREE\u7684\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u4f5c\u4e3a\u4ee3\u7406\u5c40\u90e8\u76d1\u7763\uff0c\u4ee5\u6307\u5bfc\u8bc6\u522b\u76f8\u5173\u6587\u6863\u533a\u57df\uff0c\u4ece\u800c\u63d0\u5347\u89c6\u89c9\u6587\u6863\u68c0\u7d22\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u68c0\u7d22\u5668\u8bad\u7ec3\u4f9d\u8d56\u7c97\u7cd9\u7684\u5168\u5c40\u76f8\u5173\u6027\u6807\u7b7e\uff0c\u65e0\u6cd5\u63ed\u793a\u54ea\u4e9b\u533a\u57df\u652f\u6301\u5339\u914d\uff0c\u5bfc\u81f4\u68c0\u7d22\u5668\u4f9d\u8d56\u8868\u9762\u7ebf\u7d22\uff0c\u96be\u4ee5\u6355\u6349\u9690\u542b\u7684\u8bed\u4e49\u8054\u7cfb\uff0c\u4ece\u800c\u96be\u4ee5\u5904\u7406\u975e\u62bd\u53d6\u5f0f\u67e5\u8be2\u3002", "method": "AGREE\u6846\u67b6\u7ed3\u5408\u5c40\u90e8\u4fe1\u53f7\u548c\u5168\u5c40\u4fe1\u53f7\u8054\u5408\u4f18\u5316\u68c0\u7d22\u5668\uff0c\u4f7f\u5176\u4e0d\u4ec5\u5b66\u4e60\u6587\u6863\u662f\u5426\u5339\u914d\uff0c\u8fd8\u5b66\u4e60\u54ea\u4e9b\u5185\u5bb9\u9a71\u52a8\u76f8\u5173\u6027\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684ViDoRe V2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAGREE\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528\u5168\u5c40\u76d1\u7763\u7684\u57fa\u7ebf\u3002", "conclusion": "AGREE\u6846\u67b6\u80fd\u591f\u4fc3\u8fdb\u67e5\u8be2\u8bcd\u548c\u6587\u6863\u533a\u57df\u4e4b\u95f4\u66f4\u6df1\u5c42\u6b21\u7684\u5bf9\u9f50\uff0c\u8d85\u8d8a\u8868\u9762\u5339\u914d\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u548c\u53ef\u89e3\u91ca\u7684\u68c0\u7d22\u3002"}}
{"id": "2511.11990", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11990", "abs": "https://arxiv.org/abs/2511.11990", "authors": ["Shaoqi Wang", "Lu Yu", "Chunjie Yang"], "title": "Improving Autoformalization Using Direct Dependency Retrieval", "comment": null, "summary": "The convergence of deep learning and formal mathematics has spurred research in formal verification. Statement autoformalization, a crucial first step in this process, aims to translate informal descriptions into machine-verifiable representations but remains a significant challenge. The core difficulty lies in the fact that existing methods often suffer from a lack of contextual awareness, leading to hallucination of formal definitions and theorems. Furthermore, current retrieval-augmented approaches exhibit poor precision and recall for formal library dependency retrieval, and lack the scalability to effectively leverage ever-growing public datasets. To bridge this gap, we propose a novel retrieval-augmented framework based on DDR (\\textit{Direct Dependency Retrieval}) for statement autoformalization. Our DDR method directly generates candidate library dependencies from natural language mathematical descriptions and subsequently verifies their existence within the formal library via an efficient suffix array check. Leveraging this efficient search mechanism, we constructed a dependency retrieval dataset of over 500,000 samples and fine-tuned a high-precision DDR model. Experimental results demonstrate that our DDR model significantly outperforms SOTA methods in both retrieval precision and recall. Consequently, an autoformalizer equipped with DDR shows consistent performance advantages in both single-attempt accuracy and multi-attempt stability compared to models using traditional selection-based RAG methods.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f4\u63a5\u4f9d\u8d56\u68c0\u7d22\uff08DDR\uff09\u7684\u68c0\u7d22\u589e\u5f3a\u6846\u67b6\uff0c\u7528\u4e8e\u8bed\u53e5\u81ea\u52a8\u5f62\u5f0f\u5316\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u4f9d\u8d56\u68c0\u7d22\u6548\u679c\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8bed\u53e5\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u611f\u77e5\uff0c\u5bfc\u81f4\u5f62\u5f0f\u5b9a\u4e49\u548c\u5b9a\u7406\u7684\u5e7b\u89c9\u95ee\u9898\u3002\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u5728\u5f62\u5f0f\u5e93\u4f9d\u8d56\u68c0\u7d22\u65b9\u9762\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u8f83\u4f4e\uff0c\u4e14\u7f3a\u4e4f\u6709\u6548\u5229\u7528\u4e0d\u65ad\u589e\u957f\u7684\u516c\u5171\u6570\u636e\u96c6\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u57fa\u4e8eDDR\uff0c\u76f4\u63a5\u4ece\u81ea\u7136\u8bed\u8a00\u6570\u5b66\u63cf\u8ff0\u751f\u6210\u5019\u9009\u5e93\u4f9d\u8d56\uff0c\u5e76\u901a\u8fc7\u9ad8\u6548\u7684\u540e\u7f00\u6570\u7ec4\u68c0\u67e5\u9a8c\u8bc1\u5176\u5728\u5f62\u5f0f\u5e93\u4e2d\u7684\u5b58\u5728\u6027\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u8d85\u8fc750\u4e07\u6837\u672c\u7684\u4f9d\u8d56\u68c0\u7d22\u6570\u636e\u96c6\uff0c\u5e76\u5fae\u8c03\u4e86\u4e00\u4e2a\u9ad8\u7cbe\u5ea6DDR\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDDR\u6a21\u578b\u5728\u68c0\u7d22\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u914d\u5907DDR\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u5668\u5728\u5355\u6b21\u5c1d\u8bd5\u51c6\u786e\u7387\u548c\u591a\u6b21\u5c1d\u8bd5\u7a33\u5b9a\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u4f7f\u7528\u4f20\u7edf\u9009\u62e9RAG\u65b9\u6cd5\u7684\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684DDR\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u8bed\u53e5\u81ea\u52a8\u5f62\u5f0f\u5316\u7684\u6027\u80fd\uff0c\u5e76\u5728\u68c0\u7d22\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548c\u6a21\u578b\u7a33\u5b9a\u6027\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u63d0\u5347\u3002"}}
{"id": "2511.12116", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12116", "abs": "https://arxiv.org/abs/2511.12116", "authors": ["Piotr P\u0119zik", "Konrad Kaczy\u0144ski", "Maria Szyma\u0144ska", "Filip \u017barnecki", "Zuzanna Deckert", "Jakub Kwiatkowski", "Wojciech Janowski"], "title": "LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.", "AI": {"tldr": "LLMs have a knowledge boundary due to their training data's temporal cutoff, leading to potential inaccuracies when dealing with recent events.", "motivation": "LLMs struggle with providing accurate information about events after their training data's temporal cutoff, and may blend outdated information, compromising accuracy.", "method": "Introduce LLMLagBench, a benchmark to identify the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events.", "result": "Apply LLMLagBench to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs.", "conclusion": "Assess the reliability of the benchmark by manual validation and comparison with publicly released information about LLM pretraining."}}
{"id": "2511.11837", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11837", "abs": "https://arxiv.org/abs/2511.11837", "authors": ["Fatemeh Elhambakhsh", "Gaurav Ameta", "Aditi Roy", "Hyunwoong Ko"], "title": "MP-GFormer: A 3D-Geometry-Aware Dynamic Graph Transformer Approach for Machining Process Planning", "comment": null, "summary": "Machining process planning (MP) is inherently complex due to structural and geometrical dependencies among part features and machining operations. A key challenge lies in capturing dynamic interdependencies that evolve with distinct part geometries as operations are performed. Machine learning has been applied to address challenges in MP, such as operation selection and machining sequence prediction. Dynamic graph learning (DGL) has been widely used to model dynamic systems, thanks to its ability to integrate spatio-temporal relationships. However, in MP, while existing DGL approaches can capture these dependencies, they fail to incorporate three-dimensional (3D) geometric information of parts and thus lack domain awareness in predicting machining operation sequences. To address this limitation, we propose MP-GFormer, a 3D-geometry-aware dynamic graph transformer that integrates evolving 3D geometric representations into DGL through an attention mechanism to predict machining operation sequences. Our approach leverages StereoLithography surface meshes representing the 3D geometry of a part after each machining operation, with the boundary representation method used for the initial 3D designs. We evaluate MP-GFormer on a synthesized dataset and demonstrate that the method achieves improvements of 24\\% and 36\\% in accuracy for main and sub-operation predictions, respectively, compared to state-of-the-art approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a MP-GFormer \u7684 3D \u51e0\u4f55\u611f\u77e5\u52a8\u6001\u56fe Transformer\uff0c\u7528\u4e8e\u9884\u6d4b\u52a0\u5de5\u64cd\u4f5c\u5e8f\u5217\u3002", "motivation": "\u73b0\u6709\u7684\u52a8\u6001\u56fe\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u6574\u5408\u96f6\u4ef6\u7684\u4e09\u7ef4 (3D) \u51e0\u4f55\u4fe1\u606f\uff0c\u56e0\u6b64\u5728\u9884\u6d4b\u52a0\u5de5\u64cd\u4f5c\u5e8f\u5217\u65f6\u7f3a\u4e4f\u9886\u57df\u611f\u77e5\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5c06\u4e0d\u65ad\u6f14\u53d8\u7684\u4e09\u7ef4\u51e0\u4f55\u8868\u793a\u6574\u5408\u5230 DGL \u4e2d\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30 MP-GFormer\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u4e3b\u8981\u548c\u6b21\u8981\u64cd\u4f5c\u9884\u6d4b\u7684\u51c6\u786e\u7387\u65b9\u9762\u5206\u522b\u63d0\u9ad8\u4e86 24% \u548c 36%\u3002", "conclusion": "MP-GFormer\u6709\u6548\u5730\u63d0\u5347\u4e86\u52a0\u5de5\u64cd\u4f5c\u5e8f\u5217\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u5229\u7528\u4e09\u7ef4\u51e0\u4f55\u4fe1\u606f\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.11625", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.11625", "abs": "https://arxiv.org/abs/2511.11625", "authors": ["Mohammad Karami", "Mohammad Reza Nemati", "Aidin Kazemi", "Ali Mikaeili Barzili", "Hamid Azadegan", "Behzad Moshiri"], "title": "MedFedPure: A Medical Federated Framework with MAE-based Detection and Diffusion Purification for Inference-Time Attacks", "comment": null, "summary": "Artificial intelligence (AI) has shown great potential in medical imaging, particularly for brain tumor detection using Magnetic Resonance Imaging (MRI). However, the models remain vulnerable at inference time when they are trained collaboratively through Federated Learning (FL), an approach adopted to protect patient privacy. Adversarial attacks can subtly alter medical scans in ways invisible to the human eye yet powerful enough to mislead AI models, potentially causing serious misdiagnoses. Existing defenses often assume centralized data and struggle to cope with the decentralized and diverse nature of federated medical settings. In this work, we present MedFedPure, a personalized federated learning defense framework designed to protect diagnostic AI models at inference time without compromising privacy or accuracy. MedFedPure combines three key elements: (1) a personalized FL model that adapts to the unique data distribution of each institution; (2) a Masked Autoencoder (MAE) that detects suspicious inputs by exposing hidden perturbations; and (3) an adaptive diffusion-based purification module that selectively cleans only the flagged scans before classification. Together, these steps offer robust protection while preserving the integrity of normal, benign images. We evaluated MedFedPure on the Br35H brain MRI dataset. The results show a significant gain in adversarial robustness, improving performance from 49.50% to 87.33% under strong attacks, while maintaining a high clean accuracy of 97.67%. By operating locally and in real time during diagnosis, our framework provides a practical path to deploying secure, trustworthy, and privacy-preserving AI tools in clinical workflows.\n  Index Terms: cancer, tumor detection, federated learning, masked autoencoder, diffusion, privacy", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a MedFedPure \u7684\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u9632\u5fa1\u6846\u67b6\uff0c\u65e8\u5728\u4fdd\u62a4\u8bca\u65ad AI \u6a21\u578b\u5728\u63a8\u7406\u65f6\u514d\u53d7\u5bf9\u6297\u653b\u51fb\uff0c\u540c\u65f6\u4e0d\u5f71\u54cd\u9690\u79c1\u6216\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u9632\u5fa1\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u96c6\u4e2d\u5f0f\u6570\u636e\uff0c\u5e76\u4e14\u96be\u4ee5\u5e94\u5bf9\u8054\u90a6\u533b\u7597\u73af\u5883\u4e2d\u5206\u6563\u548c\u591a\u6837\u5316\u7684\u6027\u8d28\u3002\u5bf9\u6297\u6027\u653b\u51fb\u4f1a\u5fae\u5999\u5730\u6539\u53d8\u533b\u5b66\u626b\u63cf\uff0c\u4eba\u773c\u65e0\u6cd5\u5bdf\u89c9\uff0c\u4f46\u8db3\u4ee5\u8bef\u5bfc AI \u6a21\u578b\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u7684\u8bef\u8bca\u3002", "method": "MedFedPure \u7ed3\u5408\u4e86\u4e09\u4e2a\u5173\u952e\u8981\u7d20\uff1a(1) \u9002\u5e94\u6bcf\u4e2a\u673a\u6784\u72ec\u7279\u6570\u636e\u5206\u5e03\u7684\u4e2a\u6027\u5316 FL \u6a21\u578b\uff1b(2) \u901a\u8fc7\u66b4\u9732\u9690\u85cf\u6270\u52a8\u6765\u68c0\u6d4b\u53ef\u7591\u8f93\u5165\u7684 Masked Autoencoder (MAE)\uff1b(3) \u81ea\u9002\u5e94\u6269\u6563\u51c0\u5316\u6a21\u5757\uff0c\u9009\u62e9\u6027\u5730\u6e05\u7406\u6807\u8bb0\u7684\u626b\u63cf\uff0c\u7136\u540e\u518d\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728 Br35H \u8111 MRI \u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86 MedFedPure\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u6297\u9c81\u68d2\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u63d0\u9ad8\uff0c\u5728\u5f3a\u653b\u51fb\u4e0b\u7684\u6027\u80fd\u4ece 49.50% \u63d0\u9ad8\u5230 87.33%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86 97.67% \u7684\u9ad8\u6e05\u6d01\u51c6\u786e\u7387\u3002", "conclusion": "MedFedPure \u6846\u67b6\u901a\u8fc7\u5728\u8bca\u65ad\u671f\u95f4\u672c\u5730\u548c\u5b9e\u65f6\u8fd0\u884c\uff0c\u4e3a\u5728\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u90e8\u7f72\u5b89\u5168\u3001\u53ef\u4fe1\u548c\u4fdd\u62a4\u9690\u79c1\u7684 AI \u5de5\u5177\u63d0\u4f9b\u4e86\u4e00\u6761\u5207\u5b9e\u53ef\u884c\u7684\u9014\u5f84\u3002"}}
{"id": "2511.12003", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12003", "abs": "https://arxiv.org/abs/2511.12003", "authors": ["Shuochen Liu", "Pengfei Luo", "Chao Zhang", "Yuhao Chen", "Haotian Zhang", "Qi Liu", "Xin Kou", "Tong Xu", "Enhong Chen"], "title": "Look As You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning", "comment": "Poster of AAAI'2026", "summary": "Aiming to identify precise evidence sources from visual documents, visual evidence attribution for visual document retrieval-augmented generation (VD-RAG) ensures reliable and verifiable predictions from vision-language models (VLMs) in multimodal question answering. Most existing methods adopt end-to-end training to facilitate intuitive answer verification. However, they lack fine-grained supervision and progressive traceability throughout the reasoning process. In this paper, we introduce the Chain-of-Evidence (CoE) paradigm for VD-RAG. CoE unifies Chain-of-Thought (CoT) reasoning and visual evidence attribution by grounding reference elements in reasoning steps to specific regions with bounding boxes and page indexes. To enable VLMs to generate such evidence-grounded reasoning, we propose Look As You Think (LAT), a reinforcement learning framework that trains models to produce verifiable reasoning paths with consistent attribution. During training, LAT evaluates the attribution consistency of each evidence region and provides rewards only when the CoE trajectory yields correct answers, encouraging process-level self-verification. Experiments on vanilla Qwen2.5-VL-7B-Instruct with Paper- and Wiki-VISA benchmarks show that LAT consistently improves the vanilla model in both single- and multi-image settings, yielding average gains of 8.23% in soft exact match (EM) and 47.0% in IoU@0.5. Meanwhile, LAT not only outperforms the supervised fine-tuning baseline, which is trained to directly produce answers with attribution, but also exhibits stronger generalization across domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChain-of-Evidence (CoE) \u7684\u89c6\u89c9\u6587\u6863\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (VD-RAG) \u8303\u4f8b\uff0c\u7528\u4e8e\u8bc6\u522b\u89c6\u89c9\u6587\u6863\u4e2d\u7684\u7cbe\u786e\u8bc1\u636e\u6765\u6e90\uff0c\u5e76\u901a\u8fc7 Look As You Think (LAT) \u6846\u67b6\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u63d0\u9ad8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u95ee\u7b54\u4e2d\u7684\u53ef\u9760\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u76d1\u7763\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u9010\u6b65\u53ef\u8ffd\u6eaf\u6027\u3002", "method": "\u63d0\u51fa\u4e86Look As You Think (LAT)\uff0c\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u6b65\u9aa4\u4e2d\u5c06\u53c2\u8003\u5143\u7d20\u4e0e\u7279\u5b9a\u533a\u57df\u8fdb\u884c\u7ed1\u5b9a\uff0c\u5e76\u4f7f\u7528\u8fb9\u754c\u6846\u548c\u9875\u9762\u7d22\u5f15\uff0c\u6765\u8bad\u7ec3\u6a21\u578b\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5728Paper-\u548cWiki-VISA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLAT\u5728soft exact match (EM) \u4e0a\u5e73\u5747\u63d0\u9ad8\u4e868.23%\uff0c\u5728IoU@0.5\u4e0a\u5e73\u5747\u63d0\u9ad8\u4e8647.0%\u3002", "conclusion": "LAT \u4e0d\u4ec5\u4f18\u4e8e\u76f4\u63a5\u751f\u6210\u5e26\u6709\u5c5e\u6027\u7684\u7b54\u6848\u7684\u76d1\u7763\u5fae\u8c03\u57fa\u7ebf\uff0c\u800c\u4e14\u5728\u8de8\u9886\u57df\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.12130", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12130", "abs": "https://arxiv.org/abs/2511.12130", "authors": ["Bingbing Wang", "Zhixin Bai", "Zhengda Jin", "Zihan Wang", "Xintong Song", "Jingjie Lin", "Sixuan Li", "Jing Li", "Ruifeng Xu"], "title": "PRISM of Opinions: A Persona-Reasoned Multimodal Framework for User-centric Conversational Stance Detection", "comment": null, "summary": "The rapid proliferation of multimodal social media content has driven research in Multimodal Conversational Stance Detection (MCSD), which aims to interpret users' attitudes toward specific targets within complex discussions. However, existing studies remain limited by: **1) pseudo-multimodality**, where visual cues appear only in source posts while comments are treated as text-only, misaligning with real-world multimodal interactions; and **2) user homogeneity**, where diverse users are treated uniformly, neglecting personal traits that shape stance expression. To address these issues, we introduce **U-MStance**, the first user-centric MCSD dataset, containing over 40k annotated comments across six real-world targets. We further propose **PRISM**, a **P**ersona-**R**easoned mult**I**modal **S**tance **M**odel for MCSD. PRISM first derives longitudinal user personas from historical posts and comments to capture individual traits, then aligns textual and visual cues within conversational context via Chain-of-Thought to bridge semantic and pragmatic gaps across modalities. Finally, a mutual task reinforcement mechanism is employed to jointly optimize stance detection and stance-aware response generation for bidirectional knowledge transfer. Experiments on U-MStance demonstrate that PRISM yields significant gains over strong baselines, underscoring the effectiveness of user-centric and context-grounded multimodal reasoning for realistic stance understanding.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u7684\u7528\u6237\u4e2d\u5fc3\u7684\u591a\u6a21\u6001\u5bf9\u8bdd\u7acb\u573a\u68c0\u6d4b\u6570\u636e\u96c6\uff08U-MStance\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aPRISM\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u4ece\u5386\u53f2\u5e16\u5b50\u4e2d\u63d0\u53d6\u7528\u6237\u753b\u50cf\uff0c\u5e76\u7ed3\u5408Chain-of-Thought\u63a8\u7406\u548c\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u6765\u63d0\u5347\u7acb\u573a\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5bf9\u8bdd\u7acb\u573a\u68c0\u6d4b\u7814\u7a76\u5b58\u5728\u4e24\u4e2a\u5c40\u9650\u6027\uff1a\u4f2a\u591a\u6a21\u6001\u95ee\u9898\uff08\u89c6\u89c9\u7ebf\u7d22\u4ec5\u51fa\u73b0\u5728\u6e90\u5e16\u5b50\u4e2d\uff0c\u800c\u8bc4\u8bba\u88ab\u89c6\u4e3a\u7eaf\u6587\u672c\uff09\u548c\u7528\u6237\u540c\u8d28\u6027\u95ee\u9898\uff08\u5ffd\u7565\u4e86\u4e2a\u4eba\u7279\u5f81\u5bf9\u7acb\u573a\u8868\u8fbe\u7684\u5f71\u54cd\uff09\u3002", "method": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6U-MStance\uff0c\u5e76\u63d0\u51fa\u4e86PRISM\u6a21\u578b\u3002PRISM\u6a21\u578b\u9996\u5148\u4ece\u7528\u6237\u7684\u5386\u53f2\u5e16\u5b50\u4e2d\u63d0\u53d6\u7528\u6237\u753b\u50cf\uff0c\u7136\u540e\u901a\u8fc7Chain-of-Thought\u5c06\u6587\u672c\u548c\u89c6\u89c9\u7ebf\u7d22\u5bf9\u9f50\uff0c\u6700\u540e\u4f7f\u7528\u4e92\u4efb\u52a1\u5f3a\u5316\u673a\u5236\u8054\u5408\u4f18\u5316\u7acb\u573a\u68c0\u6d4b\u548c\u751f\u6210\u4efb\u52a1\u3002", "result": "\u5728U-MStance\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPRISM\u6a21\u578b\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u7528\u6237\u4e2d\u5fc3\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u591a\u6a21\u6001\u63a8\u7406\u5728\u7acb\u573a\u7406\u89e3\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u6784\u5efa\u65b0\u7684\u6570\u636e\u96c6\u548c\u63d0\u51fa\u65b0\u7684\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5bf9\u8bdd\u7acb\u573a\u68c0\u6d4b\u4e2d\u5b58\u5728\u7684\u4f2a\u591a\u6a21\u6001\u548c\u7528\u6237\u540c\u8d28\u6027\u95ee\u9898\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.11851", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.11851", "abs": "https://arxiv.org/abs/2511.11851", "authors": ["Wei-Jia Chen", "Min-Yen Tsai", "Cheng-Yi Lee", "Chia-Mu Yu"], "title": "Defending Unauthorized Model Merging via Dual-Stage Weight Protection", "comment": "10 pages, under review", "summary": "The rapid proliferation of pretrained models and open repositories has made model merging a convenient yet risky practice, allowing free-riders to combine fine-tuned models into a new multi-capability model without authorization. Such unauthorized model merging not only violates intellectual property rights but also undermines model ownership and accountability. To address this issue, we present MergeGuard, a proactive dual-stage weight protection framework that disrupts merging compatibility while maintaining task fidelity. In the first stage, we redistribute task-relevant information across layers via L2-regularized optimization, ensuring that important gradients are evenly dispersed. In the second stage, we inject structured perturbations to misalign task subspaces, breaking curvature compatibility in the loss landscape. Together, these stages reshape the model's parameter geometry such that merged models collapse into destructive interference while the protected model remains fully functional. Extensive experiments on both vision (ViT-L-14) and language (Llama2, Gemma2, Mistral) models demonstrate that MergeGuard reduces merged model accuracy by up to 90% with less than 1.5% performance loss on the protected model.", "AI": {"tldr": "MergeGuard\u662f\u4e00\u4e2a\u7528\u4e8e\u4fdd\u62a4\u9884\u8bad\u7ec3\u6a21\u578b\u514d\u53d7\u672a\u7ecf\u6388\u6743\u5408\u5e76\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u5851\u6a21\u578b\u53c2\u6570\u51e0\u4f55\u7ed3\u6784\u6765\u7834\u574f\u5408\u5e76\u517c\u5bb9\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u672a\u7ecf\u6388\u6743\u7684\u6a21\u578b\u5408\u5e76\u4fb5\u72af\u4e86\u77e5\u8bc6\u4ea7\u6743\uff0c\u635f\u5bb3\u4e86\u6a21\u578b\u6240\u6709\u6743\u548c\u95ee\u8d23\u5236\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86MergeGuard\u3002", "method": "MergeGuard\u4f7f\u7528\u53cc\u9636\u6bb5\u6743\u91cd\u4fdd\u62a4\u6846\u67b6\uff0c\u5305\u62ec\u901a\u8fc7L2\u6b63\u5219\u5316\u4f18\u5316\u5728\u5c42\u4e4b\u95f4\u91cd\u65b0\u5206\u914d\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\uff0c\u4ee5\u53ca\u6ce8\u5165\u7ed3\u6784\u5316\u6270\u52a8\u6765\u9519\u4f4d\u4efb\u52a1\u5b50\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMergeGuard\u53ef\u4ee5\u5c06\u5408\u5e76\u6a21\u578b\u7684\u51c6\u786e\u7387\u964d\u4f4e\u9ad8\u8fbe90%\uff0c\u800c\u53d7\u4fdd\u62a4\u6a21\u578b\u7684\u6027\u80fd\u635f\u5931\u5c0f\u4e8e1.5%\u3002", "conclusion": "MergeGuard\u901a\u8fc7\u6539\u53d8\u6a21\u578b\u53c2\u6570\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u6709\u6548\u5730\u963b\u6b62\u4e86\u672a\u7ecf\u6388\u6743\u7684\u6a21\u578b\u5408\u5e76\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u81ea\u8eab\u7684\u6027\u80fd\u3002"}}
{"id": "2511.11627", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11627", "abs": "https://arxiv.org/abs/2511.11627", "authors": ["Wang Zhenyu", "Li Peiyuan", "Shi Yongxiang", "Wu Ruoyu", "Zhang Lei"], "title": "SA-EMO: Structure-Aligned Encoder Mixture of Operators for Generalizable Full-waveform Inversion", "comment": null, "summary": "Full-waveform inversion (FWI) can produce high-resolution subsurface models, yet it remains inherently ill-posed, highly nonlinear, and computationally intensive. Although recent deep learning and numerical acceleration methods have improved speed and scalability, they often rely on single CNN architectures or single neural operators, which struggle to generalize in unknown or complex geological settings and are ineffective at distinguishing diverse geological types. To address these issues, we propose a Structure-Aligned Encoder-Mixture-of-Operators (SA-EMO) architecture for velocity-field inversion under unknown subsurface structures. First, a structure-aligned encoder maps high-dimensional seismic wavefields into a physically consistent latent space, thereby eliminating spatio-temporal mismatch between the waveform and velocity domains, recovering high-frequency components, and enhancing feature generalization. Then, an adaptive routing mechanism selects and fuses multiple neural-operator experts, including spectral, wavelet, multiscale, and local operators, to predict the velocity model. We systematically evaluate our approach on the OpenFWI benchmark and the Marmousi2 dataset. Results show that SA-EMO significantly outperforms traditional CNN or single-operator methods, achieving an average MAE reduction of approximately 58.443% and an improvement in boundary resolution of about 10.308%. Ablation studies further reveal that the structure-aligned encoder, the expert-fusion mechanism, and the routing module each contribute markedly to the performance gains. This work introduces a new paradigm for efficient, scalable, and physically interpretable full-waveform inversion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u901f\u5ea6\u573a\u53cd\u6f14\u7684\u7ed3\u6784\u5bf9\u9f50\u7f16\u7801\u5668-\u6df7\u5408\u7b97\u5b50 (SA-EMO) \u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5728\u672a\u77e5\u5730\u4e0b\u7ed3\u6784\u4e0b\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u548c\u6570\u503c\u52a0\u901f\u65b9\u6cd5\u6cdb\u5316\u6027\u5dee\uff0c\u65e0\u6cd5\u533a\u5206\u4e0d\u540c\u7684\u5730\u8d28\u7c7b\u578b\u3002", "method": "SA-EMO \u67b6\u6784\uff0c\u5305\u542b\u7ed3\u6784\u5bf9\u9f50\u7f16\u7801\u5668\u548c\u6df7\u5408\u795e\u7ecf\u7b97\u5b50\u4e13\u5bb6\u3002", "result": "\u5728 OpenFWI \u57fa\u51c6\u548c Marmousi2 \u6570\u636e\u96c6\u4e0a\uff0cSA-EMO \u4f18\u4e8e\u4f20\u7edf CNN \u6216\u5355\u7b97\u5b50\u65b9\u6cd5\uff0c\u5e73\u5747 MAE \u964d\u4f4e\u7ea6 58.443%\uff0c\u8fb9\u754c\u5206\u8fa8\u7387\u63d0\u9ad8\u7ea6 10.308%\u3002", "conclusion": "SA-EMO \u4e3a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u548c\u7269\u7406\u53ef\u89e3\u91ca\u7684\u5168\u6ce2\u5f62\u53cd\u6f14\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u8303\u4f8b\u3002"}}
{"id": "2511.13523", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2511.13523", "abs": "https://arxiv.org/abs/2511.13523", "authors": ["Nikita Neveditsin", "Pawan Lingras", "Salil Patil", "Swarup Patil", "Vijay Mago"], "title": "Compact Multimodal Language Models as Robust OCR Alternatives for Noisy Textual Clinical Reports", "comment": null, "summary": "Digitization of medical records often relies on smartphone photographs of printed reports, producing images degraded by blur, shadows, and other noise. Conventional OCR systems, optimized for clean scans, perform poorly under such real-world conditions. This study evaluates compact multimodal language models as privacy-preserving alternatives for transcribing noisy clinical documents. Using obstetric ultrasound reports written in regionally inflected medical English common to Indian healthcare settings, we compare eight systems in terms of transcription accuracy, noise sensitivity, numeric accuracy, and computational efficiency. Compact multimodal models consistently outperform both classical and neural OCR pipelines. Despite higher computational costs, their robustness and linguistic adaptability position them as viable candidates for on-premises healthcare digitization.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u7d27\u51d1\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u8f6c\u5f55\u5608\u6742\u7684\u4e34\u5e8a\u6587\u6863\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf OCR \u7cfb\u7edf\u5728\u5904\u7406\u7531\u667a\u80fd\u624b\u673a\u62cd\u6444\u7684\u533b\u7597\u8bb0\u5f55\u56fe\u50cf\u65f6\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf OCR \u7cfb\u7edf\u5728\u5904\u7406\u7531\u667a\u80fd\u624b\u673a\u62cd\u6444\u7684\u3001\u53d7\u6a21\u7cca\u3001\u9634\u5f71\u548c\u5176\u4ed6\u566a\u58f0\u5f71\u54cd\u7684\u533b\u7597\u8bb0\u5f55\u56fe\u50cf\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u7814\u7a76\u65e8\u5728\u5bfb\u627e\u4e00\u79cd\u66f4\u6709\u6548\u7684\u8f6c\u5f55\u566a\u58f0\u4e34\u5e8a\u6587\u6863\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86\u5728\u5370\u5ea6\u533b\u7597\u73af\u5883\u4e2d\u5e38\u89c1\u7684\u3001\u7528\u5e26\u6709\u533a\u57df\u8272\u5f69\u7684\u533b\u5b66\u82f1\u8bed\u7f16\u5199\u7684\u4ea7\u79d1\u8d85\u58f0\u62a5\u544a\uff0c\u5e76\u6bd4\u8f83\u4e86\u516b\u4e2a\u7cfb\u7edf\u5728\u8f6c\u5f55\u51c6\u786e\u7387\u3001\u566a\u58f0\u654f\u611f\u5ea6\u3001\u6570\u5b57\u51c6\u786e\u7387\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u7d27\u51d1\u578b\u591a\u6a21\u6001\u6a21\u578b\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edf\u548c\u795e\u7ecf OCR \u6d41\u7a0b\u3002", "conclusion": "\u5c3d\u7ba1\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\uff0c\u4f46\u7d27\u51d1\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u7a33\u5065\u6027\u548c\u8bed\u8a00\u9002\u5e94\u6027\u4f7f\u5176\u6210\u4e3a\u672c\u5730\u533b\u7597\u6570\u5b57\u5316\u53ef\u884c\u7684\u5019\u9009\u65b9\u6848\u3002"}}
{"id": "2511.12008", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12008", "abs": "https://arxiv.org/abs/2511.12008", "authors": ["Yunqi Hong", "Johnson Kao", "Liam Edwards", "Nein-Tzu Liu", "Chung-Yen Huang", "Alex Oliveira-Kowaleski", "Cho-Jui Hsieh", "Neil Y. C. Lin"], "title": "Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models", "comment": null, "summary": "AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.", "AI": {"tldr": "RECAP-PATH: An interpretable AI framework for pathology that uses self-learning to generate evidence-linked diagnostic reasoning, improving accuracy and trust.", "motivation": "Current AI pathology tools lack human-readable reasoning, hindering adoption.", "method": "A two-phase learning process: diversification to expand pathology-style explanations and optimization to refine them for accuracy. Requires small labeled datasets and no white-box access or weight updates.", "result": "RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy on breast and prostate datasets.", "conclusion": "RECAP-PATH provides clinically trustworthy AI by uniting visual understanding with reasoning, demonstrating a generalizable path toward evidence-linked interpretation."}}
{"id": "2511.12133", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12133", "abs": "https://arxiv.org/abs/2511.12133", "authors": ["Qingyu Zhang", "Chunlei Xin", "Xuanang Chen", "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Le Sun", "Qing Ye", "Qianlong Xie", "Xingxing Wang"], "title": "AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing", "comment": null, "summary": "Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7528\u4e8e\u76ee\u6807\u9a71\u52a8\u7684\u529d\u8bf4\u5bf9\u8bdd\u7684\u65b0\u6846\u67b6AI-Salesman\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u53cc\u9636\u6bb5\u67b6\u6784\u548c\u52a8\u6001\u8f6e\u5ed3\u5f15\u5bfc\u4ee3\u7406\u6765\u5b9e\u73b0\u7a33\u5065\u7684\u9500\u552e\u7b56\u7565\u548c\u4e8b\u5b9e\u5fe0\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u76ee\u6807\u9a71\u52a8\u7684\u529d\u8bf4\u5bf9\u8bdd\u4e2d\u9762\u4e34\u591a\u8f6e\u89c4\u5212\u3001\u4e8b\u5b9e\u5fe0\u5b9e\u6027\u4ee5\u53ca\u7f3a\u4e4f\u7279\u5b9a\u4efb\u52a1\u6570\u636e\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u7b56\u7565\u8106\u5f31\u548c\u4e8b\u5b9e\u5e7b\u89c9\u3002", "method": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u5bf9\u8bdd\u6570\u636e\u96c6TeleSalesCorpus\uff0c\u5e76\u63d0\u51fa\u4e86AI-Salesman\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u4f7f\u7528\u8d1d\u53f6\u65af\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u8bad\u7ec3\u9636\u6bb5\u548c\u4e00\u4e2a\u4f7f\u7528\u52a8\u6001\u8f6e\u5ed3\u5f15\u5bfc\u4ee3\u7406(DOGA)\u7684\u63a8\u7406\u9636\u6bb5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684AI-Salesman\u5728\u81ea\u52a8\u6307\u6807\u548c\u7efc\u5408\u4eba\u5de5\u8bc4\u4f30\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "AI-Salesman\u5728\u590d\u6742\u7684\u529d\u8bf4\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002"}}
{"id": "2511.11864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11864", "abs": "https://arxiv.org/abs/2511.11864", "authors": ["Muzammal Shafique", "Nasir Rahim", "Jamil Ahmad", "Mohammad Siadat", "Khalid Malik", "Ghaus Malik"], "title": "FocusSDF: Boundary-Aware Learning for Medical Image Segmentation via Signed Distance Supervision", "comment": null, "summary": "Segmentation of medical images constitutes an essential component of medical image analysis, providing the foundation for precise diagnosis and efficient therapeutic interventions in clinical practices. Despite substantial progress, most segmentation models do not explicitly encode boundary information; as a result, making boundary preservation a persistent challenge in medical image segmentation. To address this challenge, we introduce FocusSDF, a novel loss function based on the signed distance functions (SDFs), which redirects the network to concentrate on boundary regions by adaptively assigning higher weights to pixels closer to the lesion or organ boundary, effectively making it boundary aware. To rigorously validate FocusSDF, we perform extensive evaluations against five state-of-the-art medical image segmentation models, including the foundation model MedSAM, using four distance-based loss functions across diverse datasets covering cerebral aneurysm, stroke, liver, and breast tumor segmentation tasks spanning multiple imaging modalities. The experimental results consistently demonstrate the superior performance of FocusSDF over existing distance transform based loss functions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570 (SDF) \u7684\u635f\u5931\u51fd\u6570 FocusSDF\uff0c\u4ee5\u63d0\u9ad8\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u8fb9\u754c\u7684\u5206\u5272\u6548\u679c\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u662f\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u4f46\u5927\u591a\u6570\u5206\u5272\u6a21\u578b\u6ca1\u6709\u660e\u786e\u7f16\u7801\u8fb9\u754c\u4fe1\u606f\uff0c\u5bfc\u81f4\u8fb9\u754c\u4fdd\u6301\u6210\u4e3a\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u4e3a\u66f4\u9760\u8fd1\u75c5\u7076\u6216\u5668\u5b98\u8fb9\u754c\u7684\u50cf\u7d20\u5206\u914d\u66f4\u9ad8\u7684\u6743\u91cd\uff0c\u4f7f\u7f51\u7edc\u96c6\u4e2d\u4e8e\u8fb9\u754c\u533a\u57df\u3002", "result": "\u5728\u5305\u62ec\u8111\u52a8\u8109\u7624\u3001\u4e2d\u98ce\u3001\u809d\u810f\u548c\u4e73\u817a\u80bf\u7624\u5206\u5272\u4efb\u52a1\u7684\u5404\u79cd\u6570\u636e\u96c6\u4e0a\uff0c\u9488\u5bf9\u4e94\u79cd\u6700\u5148\u8fdb\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\uff08\u5305\u62ec\u57fa\u7840\u6a21\u578b MedSAM\uff09\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4e00\u81f4\u8868\u660e FocusSDF \u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u8ddd\u79bb\u53d8\u6362\u7684\u635f\u5931\u51fd\u6570\u3002", "conclusion": "FocusSDF \u662f\u4e00\u79cd\u6709\u6548\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u635f\u5931\u51fd\u6570\uff0c\u5c24\u5176\u662f\u5728\u8fb9\u754c\u5206\u5272\u65b9\u9762"}}
{"id": "2511.11629", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11629", "abs": "https://arxiv.org/abs/2511.11629", "authors": ["Xu Zhang", "Peng Wang", "Chen Wang", "Zhe Xu", "Xiaohua Nie", "Wei Wang"], "title": "Global Feature Enhancing and Fusion Framework for Strain Gauge Time Series Classification", "comment": "Global Feature Enhancing and Fusion Framework for Time Series Classification", "summary": "Strain Gauge Status (SGS) recognition is crucial in the field of intelligent manufacturing based on the Internet of Things, as accurate identification helps timely detection of failed mechanical components, avoiding accidents. The loading and unloading sequences generated by strain gauges can be identified through time series classification (TSC) algorithms. Recently, deep learning models, e.g., convolutional neural networks (CNNs) have shown remarkable success in the TSC task, as they can extract discriminative local features from the subsequences to identify the time series. However, we observe that only the local features may not be sufficient for expressing the time series, especially when the local sub-sequences between different time series are very similar, e.g., SGS data of aircraft wings in static strength experiments. Nevertheless, CNNs suffer from the limitation in extracting global features due to the nature of convolution operations. For extracting global features to more comprehensively represent the SGS time series, we propose two insights: (i) Constructing global features through feature engineering. (ii) Learning high-order relationships between local features to capture global features. To realize and utilize them, we propose a hypergraph-based global feature learning and fusion framework, which learns and fuses global features for semantic consistency to enhance the representation of SGS time series, thereby improving recognition accuracy. Our method designs are validated on industrial SGS and public UCR datasets, showing better generalization for unseen data in SGS recognition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u5168\u5c40\u7279\u5f81\u5b66\u4e60\u548c\u878d\u5408\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u5e94\u53d8\u8ba1\u72b6\u6001\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4ec5\u5c40\u90e8\u7279\u5f81\u53ef\u80fd\u4e0d\u8db3\u4ee5\u8868\u8fbe\u65f6\u95f4\u5e8f\u5217\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u65f6\u95f4\u5e8f\u5217\u4e4b\u95f4\u7684\u5c40\u90e8\u5b50\u5e8f\u5217\u975e\u5e38\u76f8\u4f3c\u65f6\u3002\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5728\u63d0\u53d6\u5168\u5c40\u7279\u5f81\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "1) \u901a\u8fc7\u7279\u5f81\u5de5\u7a0b\u6784\u5efa\u5168\u5c40\u7279\u5f81\u30022) \u5b66\u4e60\u5c40\u90e8\u7279\u5f81\u4e4b\u95f4\u7684\u9ad8\u9636\u5173\u7cfb\u4ee5\u6355\u83b7\u5168\u5c40\u7279\u5f81\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u5168\u5c40\u7279\u5f81\u5b66\u4e60\u548c\u878d\u5408\u6846\u67b6\u3002", "result": "\u5728\u5de5\u4e1aSGS\u548c\u516c\u5171UCR\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u8868\u660e\u5176\u5bf9SGS\u8bc6\u522b\u4e2d\u672a\u89c1\u6570\u636e\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u548c\u878d\u5408\u5168\u5c40\u7279\u5f81\u4ee5\u5b9e\u73b0\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u589e\u5f3a\u4e86SGS\u65f6\u95f4\u5e8f\u5217\u7684\u8868\u793a\uff0c\u63d0\u9ad8\u4e86\u8bc6\u522b\u7cbe\u5ea6\u3002"}}
{"id": "2511.12060", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12060", "abs": "https://arxiv.org/abs/2511.12060", "authors": ["Yinghao Ruan", "Wei Pang", "Shuaihao Liu", "Huili Yang", "Leyi Han", "Xinghui Dong"], "title": "Intelligent Collaborative Optimization for Rubber Tyre Film Production Based on Multi-path Differentiated Clipping Proximal Policy Optimization", "comment": "10 pages", "summary": "The advent of smart manufacturing is addressing the limitations of traditional centralized scheduling and inflexible production line configurations in the rubber tyre industry, especially in terms of coping with dynamic production demands. Contemporary tyre manufacturing systems form complex networks of tightly coupled subsystems pronounced nonlinear interactions and emergent dynamics. This complexity renders the effective coordination of multiple subsystems, posing an essential yet formidable task. For high-dimensional, multi-objective optimization problems in this domain, we introduce a deep reinforcement learning algorithm: Multi-path Differentiated Clipping Proximal Policy Optimization (MPD-PPO). This algorithm employs a multi-branch policy architecture with differentiated gradient clipping constraints to ensure stable and efficient high-dimensional policy updates. Validated through experiments on width and thickness control in rubber tyre film production, MPD-PPO demonstrates substantial improvements in both tuning accuracy and operational efficiency. The framework successfully tackles key challenges, including high dimensionality, multi-objective trade-offs, and dynamic adaptation, thus delivering enhanced performance and production stability for real-time industrial deployment in tyre manufacturing.", "AI": {"tldr": "\u667a\u80fd\u5236\u9020\u6b63\u5728\u89e3\u51b3\u4f20\u7edf\u8f6e\u80ce\u884c\u4e1a\u96c6\u4e2d\u8c03\u5ea6\u548c\u751f\u4ea7\u7ebf\u914d\u7f6e\u4e0d\u7075\u6d3b\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u5e94\u5bf9\u52a8\u6001\u751f\u4ea7\u9700\u6c42\u65b9\u9762\u3002", "motivation": "\u73b0\u4ee3\u8f6e\u80ce\u5236\u9020\u7cfb\u7edf\u5f62\u6210\u590d\u6742\u7684\u7d27\u5bc6\u8026\u5408\u5b50\u7cfb\u7edf\u7f51\u7edc\uff0c\u5177\u6709\u660e\u663e\u7684\u975e\u7ebf\u6027\u76f8\u4e92\u4f5c\u7528\u548c\u6d8c\u73b0\u52a8\u529b\u5b66\u3002\u8fd9\u79cd\u590d\u6742\u6027\u4f7f\u5f97\u591a\u4e2a\u5b50\u7cfb\u7edf\u7684\u6709\u6548\u534f\u8c03\u6210\u4e3a\u4e00\u9879\u91cd\u8981\u4f46\u8270\u5de8\u7684\u4efb\u52a1\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff1a\u591a\u8def\u5f84\u5dee\u5f02\u5316\u526a\u88c1\u8fd1\u7aef\u7b56\u7565\u4f18\u5316 (MPD-PPO)\u3002\u8be5\u7b97\u6cd5\u91c7\u7528\u5177\u6709\u5dee\u5f02\u5316\u68af\u5ea6\u526a\u88c1\u7ea6\u675f\u7684\u591a\u5206\u652f\u7b56\u7565\u67b6\u6784\uff0c\u4ee5\u786e\u4fdd\u7a33\u5b9a\u9ad8\u6548\u7684\u9ad8\u7ef4\u7b56\u7565\u66f4\u65b0\u3002", "result": "\u901a\u8fc7\u5728\u6a61\u80f6\u8f6e\u80ce\u8584\u819c\u751f\u4ea7\u4e2d\u7684\u5bbd\u5ea6\u548c\u539a\u5ea6\u63a7\u5236\u5b9e\u9a8c\u9a8c\u8bc1\uff0cMPD-PPO \u5728\u8c03\u6574\u7cbe\u5ea6\u548c\u8fd0\u8425\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u51fa\u663e\u7740\u6539\u8fdb\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u9ad8\u7ef4\u5ea6\u3001\u591a\u76ee\u6807\u6743\u8861\u548c\u52a8\u6001\u9002\u5e94\uff0c\u4ece\u800c\u4e3a\u8f6e\u80ce\u5236\u9020\u4e2d\u7684\u5b9e\u65f6\u5de5\u4e1a\u90e8\u7f72\u63d0\u4f9b\u589e\u5f3a\u7684\u6027\u80fd\u548c\u751f\u4ea7\u7a33\u5b9a\u6027\u3002"}}
{"id": "2511.12140", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12140", "abs": "https://arxiv.org/abs/2511.12140", "authors": ["Pinxue Guo", "Chongruo Wu", "Xinyu Zhou", "Lingyi Hong", "Zhaoyu Chen", "Jinglun Li", "Kaixun Jiang", "Sen-ching Samson Cheung", "Wei Zhang", "Wenqiang Zhang"], "title": "Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of \"Seeing is Believing\", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.", "AI": {"tldr": "VBackChecker\u662f\u4e00\u4e2a\u65b0\u7684\u65e0\u53c2\u8003\u5e7b\u89c9\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u5177\u6709\u63a8\u7406\u548c\u6307\u4ee3\u5206\u5272\u80fd\u529b\u7684\u50cf\u7d20\u7ea7Grounding LLM\u6765\u9a8c\u8bc1MLLM\u751f\u6210\u7684\u54cd\u5e94\u4e0e\u89c6\u89c9\u8f93\u5165\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u51c6\u786e\u68c0\u6d4b\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b(MLLM)\u4e2d\u7684\u5e7b\u89c9\u5bf9\u4e8e\u786e\u4fdd\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86VBackChecker\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u53c2\u8003\u65e0\u5173\u5e7b\u89c9\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u50cf\u7d20\u7ea7Grounding LLM\u6765\u9a8c\u8bc1MLLM\u751f\u6210\u54cd\u5e94\u4e0e\u89c6\u89c9\u8f93\u5165\u7684\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u521b\u65b0\u7684pipeline\uff0c\u7528\u4e8e\u751f\u6210\u6307\u4ee4\u8c03\u4f18\u6570\u636e(R-Instruct)\u3002", "result": "VBackChecker\u4f18\u4e8e\u5148\u524d\u7684\u590d\u6742\u6846\u67b6\uff0c\u5e76\u5728R^2 -HalBench\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u751a\u81f3\u53ef\u4ee5\u4e0eGPT-4o\u5728\u5e7b\u89c9\u68c0\u6d4b\u65b9\u9762\u7684\u80fd\u529b\u76f8\u5ab2\u7f8e\u3002\u5b83\u8fd8\u5728\u50cf\u7d20\u7ea7 grounding \u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u5148\u524d\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc7 10% \u7684\u6539\u8fdb\u3002", "conclusion": "VBackChecker\u662f\u4e00\u4e2a\u6709\u6548\u7684MLLM\u5e7b\u89c9\u68c0\u6d4b\u6846\u67b6\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2511.11882", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11882", "abs": "https://arxiv.org/abs/2511.11882", "authors": ["Simon Durand", "Samuel Foucher", "Alexandre Delplanque", "Jo\u00eblle Taillon", "J\u00e9r\u00f4me Th\u00e9au"], "title": "Lacking Data? No worries! How synthetic images can alleviate image scarcity in wildlife surveys: a case study with muskox (Ovibos moschatus)", "comment": "34 pages, 10 figures, submitted to Remote Sensing in Ecology and Conservation", "summary": "Accurate population estimates are essential for wildlife management, providing critical insights into species abundance and distribution. Traditional survey methods, including visual aerial counts and GNSS telemetry tracking, are widely used to monitor muskox populations in Arctic regions. These approaches are resource intensive and constrained by logistical challenges. Advances in remote sensing, artificial intelligence, and high resolution aerial imagery offer promising alternatives for wildlife detection. Yet, the effectiveness of deep learning object detection models (ODMs) is often limited by small datasets, making it challenging to train robust ODMs for sparsely distributed species like muskoxen. This study investigates the integration of synthetic imagery (SI) to supplement limited training data and improve muskox detection in zero shot (ZS) and few-shot (FS) settings. We compared a baseline model trained on real imagery with 5 ZS and 5 FS models that incorporated progressively more SI in the training set. For the ZS models, where no real images were included in the training set, adding SI improved detection performance. As more SI were added, performance in precision, recall and F1 score increased, but eventually plateaued, suggesting diminishing returns when SI exceeded 100% of the baseline model training dataset. For FS models, combining real and SI led to better recall and slightly higher overall accuracy compared to using real images alone, though these improvements were not statistically significant. Our findings demonstrate the potential of SI to train accurate ODMs when data is scarce, offering important perspectives for wildlife monitoring by enabling rare or inaccessible species to be monitored and to increase monitoring frequency. This approach could be used to initiate ODMs without real data and refine it as real images are acquired over time.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u5408\u6210\u56fe\u50cf\uff08SI\uff09\u6765\u8865\u5145\u6709\u9650\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4ee5\u63d0\u9ad8\u5728\u96f6\u6837\u672c\uff08ZS\uff09\u548c\u5c11\u6837\u672c\uff08FS\uff09\u73af\u5883\u4e2d\u68c0\u6d4b\u9e9d\u725b\u7684\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u7684\u91ce\u751f\u52a8\u7269\u8c03\u67e5\u65b9\u6cd5\u8d44\u6e90\u5bc6\u96c6\u4e14\u9762\u4e34\u540e\u52e4\u6311\u6218\u3002\u6df1\u5ea6\u5b66\u4e60\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff08ODM\uff09\u7684\u6709\u6548\u6027\u53d7\u5230\u5c0f\u6570\u636e\u96c6\u7684\u9650\u5236\uff0c\u96be\u4ee5\u8bad\u7ec3\u51fa\u9002\u7528\u4e8e\u7a00\u758f\u5206\u5e03\u7269\u79cd\uff08\u5982\u9e9d\u725b\uff09\u7684\u7a33\u5065ODM\u3002", "method": "\u6bd4\u8f83\u4e86\u5728\u8bad\u7ec3\u96c6\u4e2d\u52a0\u5165\u4e0d\u540c\u6570\u91cfSI\u7684\u57fa\u7ebf\u6a21\u578b\u30015\u4e2aZS\u6a21\u578b\u548c5\u4e2aFS\u6a21\u578b\u3002", "result": "\u5bf9\u4e8eZS\u6a21\u578b\uff0c\u6dfb\u52a0SI\u6539\u8fdb\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u4f46\u5f53SI\u8d85\u8fc7\u57fa\u7ebf\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u96c6\u7684100%\u65f6\uff0c\u6027\u80fd\u63d0\u5347\u8d8b\u4e8e\u5e73\u7f13\u3002\u5bf9\u4e8eFS\u6a21\u578b\uff0c\u7ed3\u5408\u771f\u5b9e\u56fe\u50cf\u548cSI\u53ef\u83b7\u5f97\u66f4\u597d\u7684\u53ec\u56de\u7387\u548c\u7565\u9ad8\u7684\u6574\u4f53\u51c6\u786e\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u6570\u636e\u7a00\u7f3a\u65f6\uff0cSI\u5177\u6709\u8bad\u7ec3\u7cbe\u786eODM\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u76d1\u6d4b\u7a00\u6709\u6216\u96be\u4ee5\u63a5\u8fd1\u7684\u7269\u79cd\u5e76\u63d0\u9ad8\u76d1\u6d4b\u9891\u7387\uff0c\u4e3a\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u89c6\u89d2\u3002"}}
{"id": "2511.11630", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2511.11630", "abs": "https://arxiv.org/abs/2511.11630", "authors": ["Eliane Younes", "Elie Hachem", "Marc Bernacki"], "title": "Predicting Grain Growth in Polycrystalline Materials Using Deep Learning Time Series Models", "comment": null, "summary": "Grain Growth strongly influences the mechanical behavior of materials, making its prediction a key objective in microstructural engineering. In this study, several deep learning approaches were evaluated, including recurrent neural networks (RNN), long short-term memory (LSTM), temporal convolutional networks (TCN), and transformers, to forecast grain size distributions during grain growth. Unlike full-field simulations, which are computationally demanding, the present work relies on mean-field statistical descriptors extracted from high-fidelity simulations. A dataset of 120 grain growth sequences was processed into normalized grain size distributions as a function of time. The models were trained to predict future distributions from a short temporal history using a recursive forecasting strategy. Among the tested models, the LSTM network achieved the highest accuracy (above 90\\%) and the most stable performance, maintaining physically consistent predictions over extended horizons while reducing computation time from about 20 minutes per sequence to only a few seconds, whereas the other architectures tended to diverge when forecasting further in time. These results highlight the potential of low-dimensional descriptors and LSTM-based forecasting for efficient and accurate microstructure prediction, with direct implications for digital twin development and process optimization.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9884\u6d4b\u6676\u7c92\u957f\u5927\u8fc7\u7a0b\u4e2d\u6676\u7c92\u5c3a\u5bf8\u7684\u5206\u5e03\uff0c\u91cd\u70b9\u5173\u6ce8\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\u3001\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08LSTM\uff09\u3001\u65f6\u95f4\u5377\u79ef\u7f51\u7edc\uff08TCN\uff09\u548cTransformer\u3002", "motivation": "\u9884\u6d4b\u6676\u7c92\u957f\u5927\u662f\u5fae\u89c2\u7ed3\u6784\u5de5\u7a0b\u4e2d\u7684\u5173\u952e\u76ee\u6807\uff0c\u56e0\u4e3a\u6676\u7c92\u957f\u5927\u5f3a\u70c8\u5f71\u54cd\u6750\u6599\u7684\u529b\u5b66\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u4ece\u9ad8\u4fdd\u771f\u6a21\u62df\u4e2d\u63d0\u53d6\u7684\u5e73\u5747\u573a\u7edf\u8ba1\u63cf\u8ff0\u7b26\uff0c\u5c06120\u4e2a\u6676\u7c92\u957f\u5927\u5e8f\u5217\u7684\u6570\u636e\u96c6\u5904\u7406\u6210\u968f\u65f6\u95f4\u53d8\u5316\u7684\u5f52\u4e00\u5316\u6676\u7c92\u5c3a\u5bf8\u5206\u5e03\u3002\u4f7f\u7528\u9012\u5f52\u9884\u6d4b\u7b56\u7565\u8bad\u7ec3\u6a21\u578b\u4ee5\u4ece\u77ed\u65f6\u95f4\u5386\u53f2\u9884\u6d4b\u672a\u6765\u5206\u5e03\u3002", "result": "\u5728\u6d4b\u8bd5\u7684\u6a21\u578b\u4e2d\uff0cLSTM\u7f51\u7edc\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u7cbe\u5ea6\uff08\u9ad8\u4e8e90%\uff09\u548c\u6700\u7a33\u5b9a\u7684\u6027\u80fd\uff0c\u5728\u6269\u5c55\u7684\u65f6\u95f4\u8303\u56f4\u5185\u4fdd\u6301\u4e86\u7269\u7406\u4e0a\u4e00\u81f4\u7684\u9884\u6d4b\uff0c\u540c\u65f6\u5c06\u6bcf\u4e2a\u5e8f\u5217\u7684\u8ba1\u7b97\u65f6\u95f4\u4ece\u5927\u7ea620\u5206\u949f\u51cf\u5c11\u5230\u51e0\u79d2\u949f\u3002", "conclusion": "\u4f4e\u7ef4\u63cf\u8ff0\u7b26\u548c\u57fa\u4e8eLSTM\u7684\u9884\u6d4b\u5177\u6709\u9ad8\u6548\u3001\u51c6\u786e\u7684\u5fae\u89c2\u7ed3\u6784\u9884\u6d4b\u6f5c\u529b\uff0c\u5bf9\u6570\u5b57\u5b6a\u751f\u5f00\u53d1\u548c\u8fc7\u7a0b\u4f18\u5316\u5177\u6709\u76f4\u63a5\u5f71\u54cd\u3002"}}
{"id": "2511.12063", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12063", "abs": "https://arxiv.org/abs/2511.12063", "authors": ["Enoch Hyunwook Kang", "Hema Yoganarasimhan"], "title": "Bayesian Optimization in Language Space: An Eval-Efficient AI Self-Improvement Framework", "comment": null, "summary": "Large Language Models (LLMs) have recently enabled self-improving AI, i.e., AI that iteratively generates, evaluates, and refines its own outcomes. Recent studies have shown that self-improving AI focusing on prompt optimization can outperform state-of-the-art reinforcement-learning fine-tuned LLMs. Here, their `performance' is typically measured by query efficiency - the number of LLM-generated solution samples required to meet a certain performance threshold. However, in many societal applications, the primary limitation is not generating new solutions but evaluating them. For instance, evaluating an ad's effectiveness requires significant human feedback, which is far more costly and time-consuming than generating a candidate ad. To optimize for the evaluation efficiency objective, a natural approach is to extend Bayesian Optimization (BO), a framework proven optimal for evaluation efficiency, to the language domain. However, the difficulty of directly estimating suitable acquisition functions in LLMs' minds makes this extension challenging. This paper overcomes this challenge by proving that the combination of the simple and widely used Best-of-N selection strategy and simple textual gradients (i.e., textual edits from a critic model) statistically emulates the behavior of the gradients on the canonical UCB acquisition function, which induces optimal exploration in terms of evaluation efficiency. Based on this result, we propose TextGrad-Best-of-N Bayesian Optimization (T-BoN BO), a simple and eval-efficient language-space Bayesian optimization framework for AI self-improvement. We also empirically validate T-BoN BO by applying it to automated ad alignment tasks for persona distribution, demonstrating its superior performance compared to popular state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aT-BoN BO\u7684\u8bed\u8a00\u7a7a\u95f4\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8AI\u81ea\u6211\u6539\u8fdb\u7684\u8bc4\u4f30\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u81ea\u6539\u8fdbAI\u4e3b\u8981\u5173\u6ce8prompt\u4f18\u5316\uff0c\u4ee5\u67e5\u8be2\u6548\u7387\u4e3a\u8861\u91cf\u6807\u51c6\uff0c\u4f46\u5728\u8bb8\u591a\u793e\u4f1a\u5e94\u7528\u4e2d\uff0c\u8bc4\u4f30\u6210\u672c\u8fdc\u9ad8\u4e8e\u751f\u6210\u6210\u672c\u3002\u4e3a\u4e86\u4f18\u5316\u8bc4\u4f30\u6548\u7387\uff0c\u9700\u8981\u5c06\u8d1d\u53f6\u65af\u4f18\u5316\u6269\u5c55\u5230\u8bed\u8a00\u9886\u57df\uff0c\u4f46\u76f4\u63a5\u4f30\u8ba1LLM\u4e2d\u7684\u91c7\u96c6\u51fd\u6570\u5f88\u56f0\u96be\u3002", "method": "\u901a\u8fc7\u8bc1\u660eBest-of-N\u9009\u62e9\u7b56\u7565\u548c\u6587\u672c\u68af\u5ea6\u53ef\u4ee5\u6a21\u62dfUCB\u91c7\u96c6\u51fd\u6570\u7684\u68af\u5ea6\u884c\u4e3a\uff0c\u63d0\u51fa\u4e86T-BoN BO\u6846\u67b6\u3002", "result": "\u5728persona\u5206\u5e03\u7684\u81ea\u52a8\u5e7f\u544a\u8c03\u6574\u4efb\u52a1\u4e2d\uff0cT-BoN BO\u7684\u8868\u73b0\u4f18\u4e8e\u5f53\u524d\u6d41\u884c\u7684baseline\u65b9\u6cd5\u3002", "conclusion": "T-BoN BO\u662f\u4e00\u79cd\u7b80\u5355\u4e14\u8bc4\u4f30\u6548\u7387\u9ad8\u7684\u8bed\u8a00\u7a7a\u95f4\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u9002\u7528\u4e8eAI\u81ea\u6211\u6539\u8fdb\u3002"}}
{"id": "2511.12159", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12159", "abs": "https://arxiv.org/abs/2511.12159", "authors": ["Yaocheng Zhang", "Haohuan Huang", "Zijun Song", "Yuanheng Zhu", "Qichao Zhang", "Zijie Zhao", "Dongbin Zhao"], "title": "CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic", "comment": "17 pages, 10 figures", "summary": "Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.", "AI": {"tldr": "CriticSearch\u901a\u8fc7\u8ffd\u6eaf\u8bc4\u8bba\u673a\u5236\u63d0\u4f9b\u5bc6\u96c6\u3001turn-level\u7684\u53cd\u9988\uff0c\u4ee5\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u5de5\u5177\u96c6\u6210\u63a8\u7406\uff08TIR\uff09\u3002", "motivation": "\u73b0\u6709\u641c\u7d22\u4ee3\u7406\u7ba1\u9053\u901a\u5e38\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4f18\u5316\uff0c\u4f46\u5e38\u5e38\u9762\u4e34\u7a00\u758f\u7ed3\u679c\u5956\u52b1\uff0c\u5bfc\u81f4\u4f4e\u6548\u63a2\u7d22\u548c\u4e0d\u7a33\u5b9a\u8bad\u7ec3\u3002", "method": "\u5f15\u5165CriticSearch\uff0c\u4e00\u79cd\u7ec6\u7c92\u5ea6\u7684\u4fe1\u7528\u5206\u914d\u6846\u67b6\uff0c\u901a\u8fc7\u8ffd\u6eaf\u8bc4\u8bba\u673a\u5236\u63d0\u4f9b\u5bc6\u96c6\u7684\u3001turn-level\u7684\u53cd\u9988\u3002\u4f7f\u7528\u51bb\u7ed3\u7684\u3001\u975e\u5bf9\u79f0\u7684\u8bc4\u8bbaLLM\u56de\u987e\u6027\u5730\u8bc4\u4f30\u6bcf\u4e00\u8f6e\uff0c\u5229\u7528\u6765\u81ea\u5b8c\u6574\u8f68\u8ff9\u548c\u6807\u51c6\u7b54\u6848\u7684\u7279\u6743\u4fe1\u606f\uff0c\u5c06\u8fd9\u4e9b\u8bc4\u4f30\u8f6c\u5316\u4e3a\u7a33\u5b9a\u7684\u3001\u5bc6\u96c6\u7684\u5956\u52b1\uff0c\u4ee5\u6307\u5bfc\u7b56\u7565\u6539\u8fdb\u3002", "result": "\u5728\u4e0d\u540c\u7684\u591a\u8df3\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCriticSearch\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u3001\u66f4\u9ad8\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u66f4\u9ad8\u7684\u6027\u80fd\u3002", "conclusion": "CriticSearch\u901a\u8fc7\u63d0\u4f9b\u5bc6\u96c6\u53cd\u9988\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u5de5\u5177\u96c6\u6210\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2511.11890", "categories": ["cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.11890", "abs": "https://arxiv.org/abs/2511.11890", "authors": ["Camila Machado de Araujo", "Egon P. B. S. Borges", "Ricardo Marcelo Canteiro Grangeiro", "Allan Pinto"], "title": "Advancing Annotat3D with Harpia: A CUDA-Accelerated Library For Large-Scale Volumetric Data Segmentation", "comment": null, "summary": "High-resolution volumetric imaging techniques, such as X-ray tomography and advanced microscopy, generate increasingly large datasets that challenge existing tools for efficient processing, segmentation, and interactive exploration. This work introduces new capabilities to Annotat3D through Harpia, a new CUDA-based processing library designed to support scalable, interactive segmentation workflows for large 3D datasets in high-performance computing (HPC) and remote-access environments. Harpia features strict memory control, native chunked execution, and a suite of GPU-accelerated filtering, annotation, and quantification tools, enabling reliable operation on datasets exceeding single-GPU memory capacity. Experimental results demonstrate significant improvements in processing speed, memory efficiency, and scalability compared to widely used frameworks such as NVIDIA cuCIM and scikit-image. The system's interactive, human-in-the-loop interface, combined with efficient GPU resource management, makes it particularly suitable for collaborative scientific imaging workflows in shared HPC infrastructures.", "AI": {"tldr": "Harpia accelerates 3D dataset segmentation on HPC systems.", "motivation": "Existing tools struggle with large 3D datasets from high-resolution imaging.", "method": "CUDA-based library with chunked execution and GPU-accelerated tools.", "result": "Improved speed, memory efficiency, and scalability compared to cuCIM and scikit-image.", "conclusion": "Suitable for collaborative scientific imaging workflows in HPC environments."}}
{"id": "2511.11632", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11632", "abs": "https://arxiv.org/abs/2511.11632", "authors": ["Qiuhao Zeng"], "title": "Toward Better Generalization in Few-Shot Learning through the Meta-Component Combination", "comment": "20 pages, 5 figures", "summary": "In few-shot learning, classifiers are expected to generalize to unseen classes given only a small number of instances of each new class. One of the popular solutions to few-shot learning is metric-based meta-learning. However, it highly depends on the deep metric learned on seen classes, which may overfit to seen classes and fail to generalize well on unseen classes. To improve the generalization, we explore the substructures of classifiers and propose a novel meta-learning algorithm to learn each classifier as a combination of meta-components. Meta-components are learned across meta-learning episodes on seen classes and disentangled by imposing an orthogonal regularizer to promote its diversity and capture various shared substructures among different classifiers. Extensive experiments on few-shot benchmark tasks show superior performances of the proposed method.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5ea6\u91cf\u7684\u5143\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u5143\u7ec4\u4ef6\u7684\u7ec4\u5408\u6765\u6539\u8fdb\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5ea6\u91cf\u7684\u5143\u5b66\u4e60\u65b9\u6cd5\u5bb9\u6613\u5bf9\u5df2\u89c1\u8fc7\u7684\u7c7b\u522b\u8fc7\u62df\u5408\uff0c\u5bfc\u81f4\u5728\u65b0\u7c7b\u522b\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u6bcf\u4e2a\u5206\u7c7b\u5668\u5b66\u4e60\u4e3a\u5143\u7ec4\u4ef6\u7684\u7ec4\u5408\uff0c\u5e76\u901a\u8fc7\u6b63\u4ea4\u6b63\u5219\u5316\u5668\u6765\u89e3\u8026\u5143\u7ec4\u4ef6\uff0c\u4fc3\u8fdb\u5176\u591a\u6837\u6027\uff0c\u5e76\u6355\u83b7\u4e0d\u540c\u5206\u7c7b\u5668\u4e4b\u95f4\u5171\u4eab\u7684\u5404\u79cd\u5b50\u7ed3\u6784\u3002", "result": "\u5728few-shot benchmark\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u5143\u7ec4\u4ef6\u7684\u7ec4\u5408\uff0c\u63d0\u9ad8\u4e86\u5728few-shot\u5b66\u4e60\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.11646", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.11646", "abs": "https://arxiv.org/abs/2511.11646", "authors": ["Li Yinxing", "Tsukasa Ishigaki"], "title": "A Deep Learning Model to Predicting Changes in Consumer Attributes for New Line-extended Products", "comment": "23 pages", "summary": "Product line extension is a marketing strategy that enhances a company's sphere of influence. Because excessive line extensions disrupt brand image, only appropriate line extensions based on consumer needs are desirable. Marketers should know the key consumer attributes of the primary customers for new line-extended products before companies enter the market. This paper describes a method for predicting changes in consumer attributes for new line-extended products using a novel deep learning model. The proposed model, Conditional Tabular Variational Auto-Encoder (CTVAE), generates synthetic data from large-scale tabular data of consumers and products. It can provide various implications about effective product line marketing for marketers. The experimental results demonstrate that the CTVAE offers superior prediction performance than existing models. We indicate implications for new products that change containers or flavors for effective product line marketing. The proposed approach has the potential to contribute to avoiding cannibalization and to designing product images and marketing strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u65b0\u4ea7\u54c1\u7ebf\u6269\u5c55\u5bf9\u6d88\u8d39\u8005\u5c5e\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u8fc7\u5ea6\u7684\u4ea7\u54c1\u7ebf\u6269\u5c55\u4f1a\u7834\u574f\u54c1\u724c\u5f62\u8c61\uff0c\u56e0\u6b64\u9700\u8981\u4e86\u89e3\u6d88\u8d39\u8005\u5bf9\u65b0\u4ea7\u54c1\u7ebf\u6269\u5c55\u4ea7\u54c1\u7684\u5173\u952e\u5c5e\u6027\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u8868\u683c\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08CTVAE\uff09\u751f\u6210\u6d88\u8d39\u8005\u548c\u4ea7\u54c1\u7684\u5927\u89c4\u6a21\u8868\u683c\u6570\u636e\u7684\u5408\u6210\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCTVAE \u63d0\u4f9b\u4e86\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u907f\u514d\u540c\u7c7b\u76f8\u98df\uff0c\u5e76\u8bbe\u8ba1\u4ea7\u54c1\u5f62\u8c61\u548c\u8425\u9500\u7b56\u7565\u3002"}}
{"id": "2511.12083", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12083", "abs": "https://arxiv.org/abs/2511.12083", "authors": ["Yanchang Fu", "Shengda Liu", "Pei Xu", "Kaiqi Huang"], "title": "No-Regret Strategy Solving in Imperfect-Information Games via Pre-Trained Embedding", "comment": null, "summary": "High-quality information set abstraction remains a core challenge in solving large-scale imperfect-information extensive-form games (IIEFGs)-such as no-limit Texas Hold'em-where the finite nature of spatial resources hinders strategy solving over the full game. State-of-the-art AI methods rely on pre-trained discrete clustering for abstraction, yet their hard classification irreversibly loses critical information: specifically, the quantifiable subtle differences between information sets-vital for strategy solving-thereby compromising the quality of such solving. Inspired by the word embedding paradigm in natural language processing, this paper proposes the Embedding CFR algorithm, a novel approach for solving strategies in IIEFGs within an embedding space. The algorithm pre-trains and embeds features of isolated information sets into an interconnected low-dimensional continuous space, where the resulting vectors more precisely capture both the distinctions and connections between information sets. Embedding CFR presents a strategy-solving process driven by regret accumulation and strategy updates within this embedding space, with accompanying theoretical analysis verifying its capacity to reduce cumulative regret. Experiments on poker show that with the same spatial overhead, Embedding CFR achieves significantly faster exploitability convergence compared to cluster-based abstraction algorithms, confirming its effectiveness. Furthermore, to our knowledge, it is the first algorithm in poker AI that pre-trains information set abstractions through low-dimensional embedding for strategy solving.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5728\u4e0d\u5b8c\u7f8e\u4fe1\u606f\u6269\u5c55\u5f62\u5f0f\u535a\u5f08\uff08IIEFG\uff09\u4e2d\u6c42\u89e3\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u5d4c\u5165\u4fe1\u606f\u96c6\u7684\u7279\u5f81\u5230\u4f4e\u7ef4\u8fde\u7eed\u7a7a\u95f4\u4e2d\uff0c\u66f4\u7cbe\u786e\u5730\u6355\u6349\u4fe1\u606f\u96c6\u4e4b\u95f4\u7684\u533a\u522b\u548c\u8054\u7cfb\u3002", "motivation": "\u73b0\u6709AI\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9884\u8bad\u7ec3\u7684\u79bb\u6563\u805a\u7c7b\u8fdb\u884c\u62bd\u8c61\uff0c\u4f46\u786c\u5206\u7c7b\u4f1a\u4e0d\u53ef\u9006\u8f6c\u5730\u4e22\u5931\u5173\u952e\u4fe1\u606f\uff0c\u4ece\u800c\u5f71\u54cd\u6c42\u89e3\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEmbedding CFR\u7684\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8fdb\u884c\u540e\u6094\u7d2f\u79ef\u548c\u7b56\u7565\u66f4\u65b0\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u964d\u4f4e\u7d2f\u79ef\u540e\u6094\u7684\u80fd\u529b\u3002", "result": "\u5728\u6251\u514b\u5b9e\u9a8c\u4e2d\uff0cEmbedding CFR\u5728\u76f8\u540c\u7684\u7a7a\u95f4\u5f00\u9500\u4e0b\uff0c\u5b9e\u73b0\u4e86\u6bd4\u57fa\u4e8e\u805a\u7c7b\u7684\u62bd\u8c61\u7b97\u6cd5\u66f4\u5feb\u7684\u53ef\u5229\u7528\u6027\u6536\u655b\u3002", "conclusion": "Embedding CFR\u662f\u4e00\u79cd\u6709\u6548\u7684\u7b56\u7565\u6c42\u89e3\u65b9\u6cd5\uff0c\u5e76\u4e14\u662f\u6251\u514bAI\u4e2d\u7b2c\u4e00\u4e2a\u901a\u8fc7\u4f4e\u7ef4\u5d4c\u5165\u9884\u8bad\u7ec3\u4fe1\u606f\u96c6\u62bd\u8c61\u4ee5\u8fdb\u884c\u7b56\u7565\u6c42\u89e3\u7684\u7b97\u6cd5\u3002"}}
{"id": "2511.12213", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12213", "abs": "https://arxiv.org/abs/2511.12213", "authors": ["Liang Xue", "Haoyu Liu", "Yajun Tian", "Xinyu Zhong", "Yang Liu"], "title": "MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues", "comment": null, "summary": "Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.", "AI": {"tldr": "MME-RAG\uff0c\u4e00\u4e2a\u591a\u7ba1\u7406\u5668-\u4e13\u5bb6\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u5206\u89e3\u548cKeyInfo\u5f15\u5bfc\u68c0\u7d22\uff0c\u5b9e\u73b0\u7cbe\u786e\u548c\u9886\u57df\u81ea\u9002\u5e94\u7684\u5b9e\u4f53\u8bc6\u522b\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9762\u5411\u4efb\u52a1\u7684\u5bf9\u8bdd\u4e2d\uff0c\u9886\u57df\u9002\u5e94\u548c\u68c0\u7d22\u53ef\u63a7\u6027\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "MME-RAG\u5c06\u5b9e\u4f53\u8bc6\u522b\u5206\u89e3\u4e3a\u4e24\u4e2a\u534f\u540c\u9636\u6bb5\uff1a\u8f7b\u91cf\u7ea7\u7ba1\u7406\u5668\u8fdb\u884c\u7c7b\u578b\u7ea7\u522b\u5224\u65ad\uff0c\u4e13\u4e1a\u4e13\u5bb6\u8fdb\u884c\u8de8\u5ea6\u7ea7\u522b\u63d0\u53d6\u3002\u6bcf\u4e2a\u4e13\u5bb6\u90fd\u7531KeyInfo\u68c0\u7d22\u5668\u652f\u6301\uff0c\u8be5\u68c0\u7d22\u5668\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6ce8\u5165\u8bed\u4e49\u5bf9\u9f50\u7684\u5c11\u91cf\u6837\u672c\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u7cbe\u786e\u548c\u9886\u57df\u81ea\u9002\u5e94\u7684\u63d0\u53d6\u3002", "result": "\u5728CrossNER\u3001MIT-Movie\u3001MIT-Restaurant\u548c\u65b0\u6784\u5efa\u7684\u591a\u9886\u57df\u5ba2\u6237\u670d\u52a1\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMME-RAG\u5728\u5927\u591a\u6570\u9886\u57df\u4e2d\u4f18\u4e8e\u6700\u8fd1\u7684\u57fa\u7ebf\u3002", "conclusion": "\u5206\u5c42\u5206\u89e3\u548cKeyInfo\u5f15\u5bfc\u68c0\u7d22\u662f\u9c81\u68d2\u6027\u548c\u8de8\u9886\u57df\u6cdb\u5316\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0cMME-RAG\u662f\u81ea\u9002\u5e94\u5bf9\u8bdd\u7406\u89e3\u7684\u53ef\u6269\u5c55\u548c\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.11898", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11898", "abs": "https://arxiv.org/abs/2511.11898", "authors": ["Arnav Singhvi", "Vasiliki Bikia", "Asad Aali", "Akshay Chaudhari", "Roxana Daneshjou"], "title": "Prompt Triage: Structured Optimization Enhances Vision-Language Model Performance on Medical Imaging Benchmarks", "comment": null, "summary": "Vision-language foundation models (VLMs) show promise for diverse imaging tasks but often underperform on medical benchmarks. Prior efforts to improve performance include model finetuning, which requires large domain-specific datasets and significant compute, or manual prompt engineering, which is hard to generalize and often inaccessible to medical institutions seeking to deploy these tools. These challenges motivate interest in approaches that draw on a model's embedded knowledge while abstracting away dependence on human-designed prompts to enable scalable, weight-agnostic performance improvements. To explore this, we adapt the Declarative Self-improving Python (DSPy) framework for structured automated prompt optimization in medical vision-language systems through a comprehensive, formal evaluation. We implement prompting pipelines for five medical imaging tasks across radiology, gastroenterology, and dermatology, evaluating 10 open-source VLMs with four prompt optimization techniques. Optimized pipelines achieved a median relative improvement of 53% over zero-shot prompting baselines, with the largest gains ranging from 300% to 3,400% on tasks where zero-shot performance is low. These results highlight the substantial potential of applying automated prompt optimization to medical AI systems, demonstrating significant gains for vision-based applications requiring accurate clinical image interpretation. By reducing dependence on prompt design to elicit intended outputs, these techniques allow clinicians to focus on patient care and clinical decision-making. Furthermore, our experiments offer scalability and preserve data privacy, demonstrating performance improvement on open-source VLMs. We publicly release our evaluation pipelines to support reproducible research on specialized medical tasks, available at https://github.com/DaneshjouLab/prompt-triage-lab.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528Declarative Self-improving Python (DSPy)\u6846\u67b6\u8fdb\u884c\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u7cfb\u7edfprompt\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u5e76\u51cf\u5c11\u5bf9\u4eba\u5de5prompt\u8bbe\u8ba1\u7684\u4f9d\u8d56\u3002", "motivation": "\u73b0\u6709VLM\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0cfinetuning\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u4eba\u5de5prompt\u96be\u4ee5\u6cdb\u5316\uff0c\u533b\u5b66\u673a\u6784\u96be\u4ee5\u90e8\u7f72\u3002", "method": "\u901a\u8fc7DSPy\u6846\u67b6\uff0c\u5bf9\u653e\u5c04\u5b66\u3001\u80c3\u80a0\u75c5\u5b66\u548c\u76ae\u80a4\u75c5\u5b66\u9886\u57df\u7684\u4e94\u4e2a\u533b\u5b66\u56fe\u50cf\u4efb\u52a1\uff0c\u4f7f\u7528\u56db\u79cdprompt\u4f18\u5316\u6280\u672f\u5bf910\u4e2a\u5f00\u6e90VLM\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4f18\u5316\u540e\u7684pipeline\u76f8\u6bd4zero-shot prompting\u57fa\u7ebf\uff0c\u6027\u80fd\u63d0\u5347\u4e8653%\uff0c\u5728zero-shot\u6027\u80fd\u8f83\u4f4e\u7684\u4efb\u52a1\u4e0a\uff0c\u63d0\u5347\u5e45\u5ea6\u8fbe\u5230300%-3400%\u3002", "conclusion": "\u81ea\u52a8prompt\u4f18\u5316\u5728\u533b\u5b66AI\u7cfb\u7edf\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u53ef\u663e\u8457\u63d0\u9ad8\u9700\u8981\u7cbe\u786e\u4e34\u5e8a\u56fe\u50cf\u89e3\u91ca\u7684\u89c6\u89c9\u5e94\u7528\u6027\u80fd\uff0c\u5e76\u63d0\u9ad8\u53ef\u6269\u5c55\u6027\u548c\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002"}}
{"id": "2511.11636", "categories": ["cs.LG", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.11636", "abs": "https://arxiv.org/abs/2511.11636", "authors": ["Asma Sadia Khan", "Sadia Tabassum"], "title": "An Explainable and Fair AI Tool for PCOS Risk Assessment: Calibration, Subgroup Equity, and Interactive Clinical Deployment", "comment": null, "summary": "This paper presents a fairness-audited and interpretable machine learning framework for predicting polycystic ovary syndrome (PCOS), designed to evaluate model performance and identify diagnostic disparities across patient subgroups. The framework integrated SHAP-based feature attributions with demographic audits to connect predictive explanations with observed disparities for actionable insights. Probabilistic calibration metrics (Brier Score and Expected Calibration Error) are incorporated to ensure reliable risk predictions across subgroups. Random Forest, SVM, and XGBoost models were trained with isotonic and Platt scaling for calibration and fairness comparison. A calibrated Random Forest achieved a high predictive accuracy of 90.8%. SHAP analysis identified follicle count, weight gain, and menstrual irregularity as the most influential features, which are consistent with the Rotterdam diagnostic criteria. Although the SVM with isotonic calibration achieved the lowest calibration error (ECE = 0.0541), the Random Forest model provided a better balance between calibration and interpretability (Brier = 0.0678, ECE = 0.0666). Therefore, it was selected for detailed fairness and SHAP analyses. Subgroup analysis revealed that the model performed best among women aged 25-35 (accuracy 90.9%) but underperformed in those under 25 (69.2%), highlighting age-related disparities. The model achieved perfect precision in obese women and maintained high recall in lean PCOS cases, demonstrating robustness across phenotypes. Finally, a Streamlit-based web interface enables real-time PCOS risk assessment, Rotterdam criteria evaluation, and interactive 'what-if' analysis, bridging the gap between AI research and clinical usability.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u516c\u5e73\u4e14\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u591a\u56ca\u5375\u5de2\u7efc\u5408\u5f81\uff08PCOS\uff09\uff0c\u65e8\u5728\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u5e76\u8bc6\u522b\u60a3\u8005\u4e9a\u7ec4\u95f4\u7684\u8bca\u65ad\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u8bc4\u4f30\u6a21\u578b\u5728\u9884\u6d4bPCOS\u65b9\u9762\u7684\u6027\u80fd\uff0c\u5e76\u53d1\u73b0\u4e0d\u540c\u60a3\u8005\u7fa4\u4f53\u4e4b\u95f4\u7684\u8bca\u65ad\u5dee\u5f02\uff0c\u4e3a\u4e34\u5e8a\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u98ce\u9669\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u8be5\u6846\u67b6\u6574\u5408\u4e86\u57fa\u4e8eSHAP\u7684\u7279\u5f81\u5c5e\u6027\u548c\u4eba\u53e3\u7edf\u8ba1\u5ba1\u8ba1\uff0c\u5c06\u9884\u6d4b\u89e3\u91ca\u4e0e\u89c2\u5bdf\u5230\u7684\u5dee\u5f02\u8054\u7cfb\u8d77\u6765\u3002\u4f7f\u7528\u968f\u673a\u68ee\u6797\u3001SVM\u548cXGBoost\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u7b49\u6e17\u548cPlatt\u7f29\u653e\u8fdb\u884c\u6821\u51c6\u548c\u516c\u5e73\u6027\u6bd4\u8f83\u3002", "result": "\u6821\u51c6\u540e\u7684\u968f\u673a\u68ee\u6797\u6a21\u578b\u5b9e\u73b0\u4e8690.8%\u7684\u9ad8\u9884\u6d4b\u51c6\u786e\u7387\u3002SHAP\u5206\u6790\u786e\u5b9a\u5375\u6ce1\u8ba1\u6570\u3001\u4f53\u91cd\u589e\u52a0\u548c\u6708\u7ecf\u4e0d\u89c4\u5f8b\u662f\u6700\u5177\u5f71\u54cd\u529b\u7684\u7279\u5f81\u3002\u4e9a\u7ec4\u5206\u6790\u663e\u793a\uff0c\u6a21\u578b\u572825-35\u5c81\u5973\u6027\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u572825\u5c81\u4ee5\u4e0b\u5973\u6027\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7a81\u51fa\u4e86\u4e0e\u5e74\u9f84\u76f8\u5173\u7684\u5dee\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eStreamlit\u7684Web\u754c\u9762\uff0c\u53ef\u4ee5\u8fdb\u884c\u5b9e\u65f6PCOS\u98ce\u9669\u8bc4\u4f30\u548c\u4ea4\u4e92\u5f0f\u5206\u6790\uff0c\u5f25\u5408\u4e86AI\u7814\u7a76\u4e0e\u4e34\u5e8a\u53ef\u7528\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2511.12254", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12254", "abs": "https://arxiv.org/abs/2511.12254", "authors": ["Yuxiang Zhou", "Jichang Li", "Yanhao Zhang", "Haonan Lu", "Guanbin Li"], "title": "Mobile-Agent-RAG: Driving Smart Multi-Agent Coordination with Contextual Knowledge Empowerment for Long-Horizon Mobile Automation", "comment": null, "summary": "Mobile agents show immense potential, yet current state-of-the-art (SoTA) agents exhibit inadequate success rates on real-world, long-horizon, cross-application tasks. We attribute this bottleneck to the agents' excessive reliance on static, internal knowledge within MLLMs, which leads to two critical failure points: 1) strategic hallucinations in high-level planning and 2) operational errors during low-level execution on user interfaces (UI). The core insight of this paper is that high-level planning and low-level UI operations require fundamentally distinct types of knowledge. Planning demands high-level, strategy-oriented experiences, whereas operations necessitate low-level, precise instructions closely tied to specific app UIs. Motivated by these insights, we propose Mobile-Agent-RAG, a novel hierarchical multi-agent framework that innovatively integrates dual-level retrieval augmentation. At the planning stage, we introduce Manager-RAG to reduce strategic hallucinations by retrieving human-validated comprehensive task plans that provide high-level guidance. At the execution stage, we develop Operator-RAG to improve execution accuracy by retrieving the most precise low-level guidance for accurate atomic actions, aligned with the current app and subtask. To accurately deliver these knowledge types, we construct two specialized retrieval-oriented knowledge bases. Furthermore, we introduce Mobile-Eval-RAG, a challenging benchmark for evaluating such agents on realistic multi-app, long-horizon tasks. Extensive experiments demonstrate that Mobile-Agent-RAG significantly outperforms SoTA baselines, improving task completion rate by 11.0% and step efficiency by 10.2%, establishing a robust paradigm for context-aware, reliable multi-agent mobile automation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Mobile-Agent-RAG \u7684\u65b0\u578b\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5c42\u68c0\u7d22\u589e\u5f3a\u6765\u89e3\u51b3\u73b0\u6709\u79fb\u52a8\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u4e16\u754c\u3001\u957f\u7a0b\u3001\u8de8\u5e94\u7528\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u79fb\u52a8\u667a\u80fd\u4f53\u8fc7\u5ea6\u4f9d\u8d56 MLLM \u5185\u7684\u9759\u6001\u77e5\u8bc6\uff0c\u5bfc\u81f4\u9ad8\u5c42\u89c4\u5212\u4e2d\u7684\u6218\u7565\u6027\u5e7b\u89c9\u548c\u5e95\u5c42 UI \u6267\u884c\u4e2d\u7684\u64cd\u4f5c\u9519\u8bef\u3002", "method": "\u8be5\u6846\u67b6\u5728\u89c4\u5212\u9636\u6bb5\u5f15\u5165 Manager-RAG\uff0c\u901a\u8fc7\u68c0\u7d22\u4eba\u5de5\u9a8c\u8bc1\u7684\u4efb\u52a1\u8ba1\u5212\u6765\u51cf\u5c11\u6218\u7565\u6027\u5e7b\u89c9\uff1b\u5728\u6267\u884c\u9636\u6bb5\u5f15\u5165 Operator-RAG\uff0c\u901a\u8fc7\u68c0\u7d22\u7cbe\u786e\u7684\u5e95\u5c42\u6307\u5bfc\u6765\u63d0\u9ad8\u6267\u884c\u51c6\u786e\u6027\u3002\u6784\u5efa\u4e86\u4e24\u4e2a\u4e13\u95e8\u7684\u68c0\u7d22\u77e5\u8bc6\u5e93\uff0c\u5e76\u63d0\u51fa\u4e86 Mobile-Eval-RAG \u57fa\u51c6\u6765\u8bc4\u4f30\u667a\u80fd\u4f53\u3002", "result": "Mobile-Agent-RAG \u5728\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u6b65\u9aa4\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "Mobile-Agent-RAG \u4e3a\u4e0a\u4e0b\u6587\u611f\u77e5\u3001\u53ef\u9760\u7684\u591a\u667a\u80fd\u4f53\u79fb\u52a8\u81ea\u52a8\u5316\u5efa\u7acb\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u8303\u4f8b\u3002"}}
{"id": "2511.12089", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.12089", "abs": "https://arxiv.org/abs/2511.12089", "authors": ["Yanchang Fu", "Qiyue Yin", "Shengda Liu", "Pei Xu", "Kaiqi Huang"], "title": "KrwEmd: Revising the Imperfect-Recall Abstraction from Forgetting Everything", "comment": null, "summary": "Excessive abstraction is a critical challenge in hand abstraction-a task specific to games like Texas hold'em-when solving large-scale imperfect-information games, as it impairs AI performance. This issue arises from extreme implementations of imperfect-recall abstraction, which entirely discard historical information. This paper presents KrwEmd, the first practical algorithm designed to address this problem. We first introduce the k-recall winrate feature, which not only qualitatively distinguishes signal observation infosets by leveraging both future and, crucially, historical game information, but also quantitatively captures their similarity. We then develop the KrwEmd algorithm, which clusters signal observation infosets using earth mover's distance to measure discrepancies between their features. Experimental results demonstrate that KrwEmd significantly improves AI gameplay performance compared to existing algorithms.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a KrwEmd \u7684\u65b0\u7b97\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u5fb7\u5dde\u6251\u514b\u7b49\u6e38\u620f\u4e2d\u8fc7\u5ea6\u62bd\u8c61\u7684\u95ee\u9898\uff0c\u8fc7\u5ea6\u62bd\u8c61\u4f1a\u635f\u5bb3 AI \u6027\u80fd\u3002", "motivation": "\u52a8\u673a\u662f\u73b0\u6709\u7684\u624b\u90e8\u62bd\u8c61\u65b9\u6cd5\u5728\u89e3\u51b3\u5927\u578b\u4e0d\u5b8c\u5168\u4fe1\u606f\u535a\u5f08\u65f6\u5b58\u5728\u8fc7\u5ea6\u62bd\u8c61\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u5b83\u4eec\u5b8c\u5168\u4e22\u5f03\u4e86\u5386\u53f2\u4fe1\u606f\uff0c\u4ece\u800c\u635f\u5bb3\u4e86 AI \u6027\u80fd\u3002", "method": "\u8be5\u8bba\u6587\u9996\u5148\u5f15\u5165\u4e86 k-recall \u80dc\u7387\u7279\u5f81\uff0c\u8be5\u7279\u5f81\u5229\u7528\u672a\u6765\u548c\u5386\u53f2\u6e38\u620f\u4fe1\u606f\u6765\u533a\u5206\u4fe1\u53f7\u89c2\u5bdf\u4fe1\u606f\u96c6\uff0c\u5e76\u91cf\u5316\u5b83\u4eec\u7684\u76f8\u4f3c\u6027\u3002\u7136\u540e\uff0c\u5f00\u53d1\u4e86 KrwEmd \u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4f7f\u7528 earth mover's distance \u6765\u8861\u91cf\u4fe1\u53f7\u89c2\u5bdf\u4fe1\u606f\u96c6\u7279\u5f81\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u4ece\u800c\u5bf9\u5b83\u4eec\u8fdb\u884c\u805a\u7c7b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u7b97\u6cd5\u76f8\u6bd4\uff0cKrwEmd \u663e\u7740\u63d0\u9ad8\u4e86 AI \u6e38\u620f\u6027\u80fd\u3002", "conclusion": "KrwEmd \u7b97\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86\u624b\u90e8\u62bd\u8c61\u4e2d\u7684\u8fc7\u5ea6\u62bd\u8c61\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u4e86 AI \u5728\u4e0d\u5b8c\u5168\u4fe1\u606f\u535a\u5f08\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2511.12236", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12236", "abs": "https://arxiv.org/abs/2511.12236", "authors": ["Raavi Gupta", "Pranav Hari Panicker", "Sumit Bhatia", "Ganesh Ramakrishnan"], "title": "Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts", "comment": "To appear at International Joint Conference on Natural Language Processing & Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL), 2025", "summary": "Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.", "AI": {"tldr": "CONFACTCHECK\uff1a\u4e00\u79cd\u9ad8\u6548\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u65e0\u9700\u5916\u90e8\u77e5\u8bc6\u5e93\uff0c\u57fa\u4e8e\u751f\u6210\u6587\u672c\u4e2d\u4e8b\u5b9e\u63a2\u6d4b\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7ecf\u5e38\u4ea7\u751f\u5e7b\u89c9\u548c\u751f\u6210\u4e0d\u7b26\u5408\u4e8b\u5b9e\u7684\u6587\u672c\uff0c\u8fd9\u5bf9\u533b\u7597\u4fdd\u5065\u3001\u91d1\u878d\u548c\u5ba2\u6237\u652f\u6301\u7b49\u9886\u57df\u6784\u6210\u4e25\u91cd\u98ce\u9669\u3002\u73b0\u6709\u68c0\u6d4b\u5e7b\u89c9\u7684\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u591a\u6b21 LLM API \u8c03\u7528\uff0c\u4ece\u800c\u589e\u52a0\u5ef6\u8fdf\u548c API \u6210\u672c\u3002", "method": "\u63d0\u51fa CONFACTCHECK\uff0c\u5b83\u4e0d\u5229\u7528\u4efb\u4f55\u5916\u90e8\u77e5\u8bc6\u5e93\uff0c\u5e76\u4e14\u57fa\u4e8e\u4ee5\u4e0b\u7b80\u5355\u76f4\u89c9\uff1a\u751f\u6210\u6587\u672c\u4e2d\u4e8b\u5b9e\u63a2\u6d4b\u7684\u54cd\u5e94\u5e94\u5728\u5355\u4e2a LLM \u5185\u548c\u8de8\u4e0d\u540c LLM \u4fdd\u6301\u4e00\u81f4\u3002", "result": "\u5728\u6db5\u76d6\u4e8b\u5b9e\u6587\u672c\u751f\u6210\u548c\u5f00\u653e\u751f\u6210\u7684\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u4e25\u683c\u7ecf\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u5728\u7c7b\u4f3c\u6761\u4ef6\u4e0b\u8fd0\u884c\u7684\u73b0\u6709\u57fa\u7ebf\u76f8\u6bd4\uff0cCONFACTCHECK \u53ef\u4ee5\u4f7f\u7528\u66f4\u5c11\u7684\u8d44\u6e90\u6709\u6548\u5730\u68c0\u6d4b\u5230\u5e7b\u89c9\u4e8b\u5b9e\uff0c\u5e76\u83b7\u5f97\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u3002", "conclusion": "CONFACTCHECK \u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u4e86 LLM \u81ea\u8eab\u7684\u4e00\u81f4\u6027\uff0c\u800c\u65e0\u9700\u5916\u90e8\u77e5\u8bc6\u5e93\u6216\u591a\u6b21 API \u8c03\u7528\u3002"}}
{"id": "2511.11908", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11908", "abs": "https://arxiv.org/abs/2511.11908", "authors": ["Afifa Khaled", "Ebrahim Hamid Sumiea"], "title": "PI-NAIM: Path-Integrated Neural Adaptive Imputation Model", "comment": null, "summary": "Medical imaging and multi-modal clinical settings often face the challange of missing modality in their diagnostic pipelines. Existing imputation methods either lack representational capacity or are computationally expensive. We propose PI-NAIM, a novel dual-path architecture that dynamically routes samples to optimized imputation approaches based on missingness complexity. Our framework integrates: (1) intelligent path routing that directs low missingness samples to efficient statistical imputation (MICE) and complex patterns to powerful neural networks (GAIN with temporal analysis); (2) cross-path attention fusion that leverages missingness-aware embeddings to intelligently combine both branches; and (3) end-to-end joint optimization of imputation accuracy and downstream task performance. Extensive experiments on MIMIC-III and multimodal benchmarks demonstrate state-of-the-art performance, achieving RMSE of 0.108 (vs. baselines' 0.119-0.152) and substantial gains in downstream tasks with an AUROC of 0.812 for mortality prediction. PI-NAIM's modular design enables seamless integration into vision pipelines handling incomplete sensor measurements, missing modalities, or corrupted inputs, providing a unified solution for real-world scenario. The code is publicly available at https://github.com/AfifaKhaled/PI-NAIM-Path-Integrated-Neural-Adaptive-Imputation-Model", "AI": {"tldr": "PI-NAIM: A dual-path imputation architecture for medical imaging that routes samples based on missingness complexity, using MICE for simple cases and neural networks (GAIN) for complex ones, with cross-path attention fusion and joint optimization.", "motivation": "Missing modalities in medical imaging hinder diagnostic pipelines, and existing imputation methods are either weak or slow.", "method": "A dual-path architecture (PI-NAIM) with intelligent path routing (MICE or GAIN), cross-path attention fusion, and end-to-end joint optimization.", "result": "State-of-the-art performance on MIMIC-III and multimodal benchmarks, achieving RMSE of 0.108 and AUROC of 0.812 for mortality prediction.", "conclusion": "PI-NAIM is a modular and unified solution for handling incomplete data in real-world medical scenarios, seamlessly integrating into vision pipelines."}}
{"id": "2511.11638", "categories": ["cs.LG", "math.NA", "nlin.PS"], "pdf": "https://arxiv.org/pdf/2511.11638", "abs": "https://arxiv.org/abs/2511.11638", "authors": ["Aamir Shehzad"], "title": "Enhancing PINN Accuracy for the RLW Equation: Adaptive and Conservative Approaches", "comment": "32 pages, 19 figures This work investigates adaptive and conservative PINN frameworks for solving the RLW equation", "summary": "Standard physics-informed neural network implementations have produced large error rates when using these models to solve the regularized long wave (RLW) equation. Two improved PINN approaches were developed in this research: an adaptive approach with self-adaptive loss weighting and a conservative approach enforcing explicit conservation laws. Three benchmark tests were used to demonstrate how effective PINN's are as they relate to the type of problem being solved (i.e., time dependent RLW equation). The first was a single soliton traveling along a line (propagation), the second was the interaction between two solitons, and the third was the evolution of an undular bore over the course of $t=250$. The results demonstrated that the effectiveness of PINNs are problem specific. The adaptive PINN was significantly better than both the conservative PINN and the standard PINN at solving problems involving complex nonlinear interactions such as colliding two solitons. The conservative approach was significantly better at solving problems involving long term behavior of single solitons and undular bores. However, the most important finding from this research is that explicitly enforcing conservation laws may be harmful to optimizing the solution of highly nonlinear systems of equations and therefore requires special training methods. The results from our adaptive and conservative approaches were within $O(10^{-5})$ of established numerical solutions for the same problem, thus demonstrating that PINNs can provide accurate solutions to complex systems of partial differential equations without the need for a discretization of space or time (mesh free). Moreover, the finding from this research challenges the assumptions that conservation enforcement will always improve the performance of a PINN and provides researchers with guidelines for designing PINNs for use on specific types of problems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7528\u6539\u8fdb\u7684PINN\u65b9\u6cd5\u6c42\u89e3\u6b63\u5219\u5316\u957f\u6ce2\uff08RLW\uff09\u65b9\u7a0b\uff0c\u53d1\u73b0\u81ea\u9002\u5e94PINN\u5728\u89e3\u51b3\u590d\u6742\u975e\u7ebf\u6027\u95ee\u9898\uff08\u5982\u5b64\u5b50\u78b0\u649e\uff09\u4e0a\u4f18\u4e8e\u4fdd\u5b88PINN\u548c\u6807\u51c6PINN\uff0c\u800c\u4fdd\u5b88PINN\u5728\u89e3\u51b3\u5355\u5b64\u5b50\u957f\u671f\u884c\u4e3a\u548cundular bores\u95ee\u9898\u4e0a\u66f4\u4f18\u3002\u7814\u7a76\u5f3a\u8c03\uff0c\u663e\u5f0f\u6267\u884c\u5b88\u6052\u5b9a\u5f8b\u53ef\u80fd\u4e0d\u5229\u4e8e\u9ad8\u5ea6\u975e\u7ebf\u6027\u65b9\u7a0b\u7ec4\u7684\u4f18\u5316\uff0c\u5e76\u4e3aPINN\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u4f20\u7edfPINN\u5728\u6c42\u89e3\u6b63\u5219\u5316\u957f\u6ce2\u65b9\u7a0b\u65f6\u8bef\u5dee\u7387\u8f83\u9ad8\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u79cd\u6539\u8fdb\u7684PINN\u65b9\u6cd5\uff1a\u81ea\u9002\u5e94\u635f\u5931\u6743\u91cd\u65b9\u6cd5\u548c\u5f3a\u5236\u6267\u884c\u663e\u5f0f\u5b88\u6052\u5f8b\u7684\u4fdd\u5b88\u65b9\u6cd5\u3002", "result": "\u81ea\u9002\u5e94PINN\u5728\u89e3\u51b3\u590d\u6742\u975e\u7ebf\u6027\u76f8\u4e92\u4f5c\u7528\u95ee\u9898\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u800c\u4fdd\u5b88\u65b9\u6cd5\u5728\u89e3\u51b3\u5355\u5b64\u5b50\u548cundular bores\u7684\u957f\u671f\u884c\u4e3a\u95ee\u9898\u4e0a\u8868\u73b0\u66f4\u597d\u3002\u663e\u5f0f\u6267\u884c\u5b88\u6052\u5b9a\u5f8b\u53ef\u80fd\u5bf9\u9ad8\u5ea6\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u4f18\u5316\u6709\u5bb3\u3002\u6539\u8fdb\u7684PINN\u65b9\u6cd5\u7ed3\u679c\u4e0e\u65e2\u6709\u6570\u503c\u89e3\u7684\u8bef\u5dee\u5728$O(10^{-5})$\u8303\u56f4\u5185\u3002", "conclusion": "PINN\u53ef\u4ee5\u4e3a\u590d\u6742\u504f\u5fae\u5206\u65b9\u7a0b\u7ec4\u63d0\u4f9b\u7cbe\u786e\u89e3\uff0c\u4e14\u65e0\u9700\u79bb\u6563\u7a7a\u95f4\u6216\u65f6\u95f4\u3002\u7814\u7a76\u7ed3\u679c\u6311\u6218\u4e86\u5b88\u6052\u5b9a\u5f8b\u6267\u884c\u603b\u80fd\u63d0\u9ad8PINN\u6027\u80fd\u7684\u5047\u8bbe\uff0c\u5e76\u4e3a\u7814\u7a76\u4eba\u5458\u8bbe\u8ba1PINN\u4ee5\u89e3\u51b3\u7279\u5b9a\u7c7b\u578b\u95ee\u9898\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2511.12449", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12449", "abs": "https://arxiv.org/abs/2511.12449", "authors": ["Zhanheng Nie", "Chenghan Fu", "Daoze Zhang", "Junxian Wu", "Wanxian Guan", "Pengjie Wang", "Jian Xu", "Bo Zheng"], "title": "MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding", "comment": "11 pages, 7 figures", "summary": "The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMOON2.0\u7684\u52a8\u6001\u6a21\u6001\u5e73\u8861\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u7535\u5546\u4ea7\u54c1\u7406\u89e3\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7535\u5546\u4ea7\u54c1\u7406\u89e3\u65b9\u9762\u9762\u4e34\u4e09\u4e2a\u6311\u6218\uff1a\u6a21\u6001\u6df7\u5408\u8bad\u7ec3\u5f15\u8d77\u7684\u6a21\u6001\u4e0d\u5e73\u8861\u3001\u4ea7\u54c1\u5185\u90e8\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u5185\u5728\u5bf9\u9f50\u5173\u7cfb\u672a\u88ab\u5145\u5206\u5229\u7528\u3001\u4ee5\u53ca\u5bf9\u7535\u5546\u591a\u6a21\u6001\u6570\u636e\u4e2d\u566a\u58f0\u7684\u5904\u7406\u80fd\u529b\u6709\u9650\u3002", "method": "MOON2.0 \u5305\u542b\uff1a(1) \u6a21\u6001\u9a71\u52a8\u7684\u6df7\u5408\u4e13\u5bb6 (MoE) \u6a21\u5757\uff0c\u901a\u8fc7\u6a21\u6001\u7ec4\u5408\u81ea\u9002\u5e94\u5730\u5904\u7406\u8f93\u5165\u6837\u672c\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u8054\u5408\u5b66\u4e60\u4ee5\u51cf\u8f7b\u6a21\u6001\u4e0d\u5e73\u8861\uff1b(2) \u53cc\u5c42\u5bf9\u9f50\u65b9\u6cd5\uff0c\u4ee5\u66f4\u597d\u5730\u5229\u7528\u5355\u4e2a\u4ea7\u54c1\u5185\u90e8\u7684\u8bed\u4e49\u5bf9\u9f50\u5c5e\u6027\uff1b(3) \u57fa\u4e8e MLLM \u7684\u56fe\u50cf-\u6587\u672c\u534f\u540c\u589e\u5f3a\u7b56\u7565\uff0c\u5c06\u6587\u672c\u4e30\u5bcc\u4e0e\u89c6\u89c9\u6269\u5c55\u76f8\u7ed3\u5408\uff0c\u5e76\u7ed3\u5408\u52a8\u6001\u6837\u672c\u8fc7\u6ee4\u6765\u63d0\u9ad8\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u3002", "result": "MOON2.0 \u5728 MBE2.0 \u548c\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u70ed\u56fe\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86 MOON2.0 \u6539\u8fdb\u7684\u591a\u6a21\u6001\u5bf9\u9f50\u7684\u5b9a\u6027\u8bc1\u636e\u3002", "conclusion": "MOON2.0 \u662f\u4e00\u79cd\u6709\u6548\u7684\u7535\u5546\u4ea7\u54c1\u7406\u89e3\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u6846\u67b6\u3002"}}
{"id": "2511.12113", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12113", "abs": "https://arxiv.org/abs/2511.12113", "authors": ["Lanxue Zhang", "Yuqiang Xie", "Fang Fang", "Fanglong Dong", "Rui Liu", "Yanan Cao"], "title": "MetaGDPO: Alleviating Catastrophic Forgetting with Metacognitive Knowledge through Group Direct Preference Optimization", "comment": "23 pages, 10 figures, AAAI 2026", "summary": "Large Language Models demonstrate strong reasoning capabilities, which can be effectively compressed into smaller models. However, existing datasets and fine-tuning approaches still face challenges that lead to catastrophic forgetting, particularly for models smaller than 8B. First, most datasets typically ignore the relationship between training data knowledge and the model's inherent abilities, making it difficult to preserve prior knowledge. Second, conventional training objectives often fail to constrain inherent knowledge preservation, which can result in forgetting of previously learned skills. To address these issues, we propose a comprehensive solution that alleviates catastrophic forgetting from both the data and fine-tuning approach perspectives. On the data side, we construct a dataset of 5K instances that covers multiple reasoning tasks and incorporates metacognitive knowledge, making it more tolerant and effective for distillation into smaller models. We annotate the metacognitive knowledge required to solve each question and filter the data based on task knowledge and the model's inherent skills. On the training side, we introduce GDPO (Group Direction Preference Optimization), which is better suited for resource-limited scenarios and can efficiently approximate the performance of GRPO. Guided by the large model and by implicitly constraining the optimization path through a reference model, GDPO enables more effective knowledge transfer from the large model and constrains excessive parameter drift. Extensive experiments demonstrate that our approach significantly alleviates catastrophic forgetting and improves reasoning performance on smaller models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u538b\u7f29\u5230\u5c0f\u578b\u6a21\u578b\u65f6\u51fa\u73b0\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u7684\u7efc\u5408\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u548c\u5fae\u8c03\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u4fdd\u6301\u6a21\u578b\u7684\u5148\u524d\u77e5\u8bc6\uff0c\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5c24\u5176\u662f\u5728\u5c0f\u4e8e 8B \u7684\u6a21\u578b\u4e2d\u3002", "method": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u5143\u8ba4\u77e5\u77e5\u8bc6\u7684\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a GDPO \u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u53c2\u8003\u6a21\u578b\u7ea6\u675f\u4f18\u5316\u8def\u5f84\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u4f20\u9012\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u8f7b\u4e86\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5e76\u63d0\u9ad8\u4e86\u5c0f\u578b\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7f13\u89e3\u5927\u6a21\u578b\u538b\u7f29\u5230\u5c0f\u6a21\u578b\u65f6\u51fa\u73b0\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002"}}
{"id": "2511.12249", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12249", "abs": "https://arxiv.org/abs/2511.12249", "authors": ["Khang T. Huynh", "Dung H. Nguyen", "Binh T. Nguyen"], "title": "ViConBERT: Context-Gloss Aligned Vietnamese Word Embedding for Polysemous and Sense-Aware Representations", "comment": null, "summary": "Recent advances in contextualized word embeddings have greatly improved semantic tasks such as Word Sense Disambiguation (WSD) and contextual similarity, but most progress has been limited to high-resource languages like English. Vietnamese, in contrast, still lacks robust models and evaluation resources for fine-grained semantic understanding. In this paper, we present ViConBERT, a novel framework for learning Vietnamese contextualized embeddings that integrates contrastive learning (SimCLR) and gloss-based distillation to better capture word meaning. We also introduce ViConWSD, the first large-scale synthetic dataset for evaluating semantic understanding in Vietnamese, covering both WSD and contextual similarity. Experimental results show that ViConBERT outperforms strong baselines on WSD (F1 = 0.87) and achieves competitive performance on ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60), demonstrating its effectiveness in modeling both discrete senses and graded semantic relations. Our code, models, and data are available at https://github.com/tkhangg0910/ViConBERT", "AI": {"tldr": "ViConBERT is a new framework for learning Vietnamese contextualized embeddings that integrates contrastive learning and gloss-based distillation to better capture word meaning.", "motivation": "Vietnamese lacks robust models and evaluation resources for fine-grained semantic understanding.", "method": "A novel framework for learning Vietnamese contextualized embeddings that integrates contrastive learning (SimCLR) and gloss-based distillation.", "result": "ViConBERT outperforms strong baselines on WSD (F1 = 0.87) and achieves competitive performance on ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60).", "conclusion": "ViConBERT demonstrates its effectiveness in modeling both discrete senses and graded semantic relations in Vietnamese."}}
{"id": "2511.11910", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11910", "abs": "https://arxiv.org/abs/2511.11910", "authors": ["Siyou Li", "Huanan Wu", "Juexi Shao", "Yinghao Ma", "Yujian Gan", "Yihao Luo", "Yuwei Wang", "Dong Nie", "Lu Wang", "Wengqing Wu", "Le Zhang", "Massimo Poesio", "Juntao Yu"], "title": "Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models", "comment": null, "summary": "Despite the recent advances in the video understanding ability of multimodal large language models (MLLMs), long video understanding remains a challenge. One of the main issues is that the number of vision tokens grows linearly with video length, which causes an explosion in attention cost, memory, and latency. To solve this challenge, we present Query-aware Token Selector (\\textbf{QTSplus}), a lightweight yet powerful visual token selection module that serves as an information gate between the vision encoder and LLMs. Given a text query and video tokens, QTSplus dynamically selects the most important visual evidence for the input text query by (i) scoring visual tokens via cross-attention, (ii) \\emph{predicting} an instance-specific retention budget based on the complexity of the query, and (iii) \\emph{selecting} Top-$n$ tokens with a differentiable straight-through estimator during training and a hard gate at inference. Furthermore, a small re-encoder preserves temporal order using absolute time information, enabling second-level localization while maintaining global coverage.\n  Integrated into Qwen2.5-VL, QTSplus compresses the vision stream by up to \\textbf{89\\%} and reduces end-to-end latency by \\textbf{28\\%} on long videos. The evaluation on eight long video understanding benchmarks shows near-parity accuracy overall when compared with the original Qwen models and outperforms the original model by \\textbf{+20.5} and \\textbf{+5.6} points respectively on TempCompass direction and order accuracies. These results show that QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence.\n  We will make all code, data, and trained models' weights publicly available.", "AI": {"tldr": "QTSplus is introduced to address the challenge of long video understanding in MLLMs by reducing the number of vision tokens and improving efficiency.", "motivation": "Long videos cause an explosion in attention cost, memory, and latency due to the linear increase in vision tokens with video length.", "method": "QTSplus dynamically selects important visual evidence using cross-attention, predicts a retention budget based on query complexity, and selects Top-n tokens. A re-encoder preserves temporal order.", "result": "QTSplus compresses the vision stream by up to 89%, reduces latency by 28%, and improves accuracy on TempCompass benchmarks by +20.5 and +5.6 points.", "conclusion": "QTSplus is an effective mechanism for scaling MLLMs to long-video scenarios while preserving task-relevant evidence."}}
{"id": "2511.11641", "categories": ["cs.LG", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.11641", "abs": "https://arxiv.org/abs/2511.11641", "authors": ["Jinqi Xiao", "Cheng Luo", "Lingyi Huang", "Cheng Yang", "Yang Sui", "Huy Phan", "Xiao Zang", "Yibiao Ying", "Zhexiang Tang", "Anima Anandkumar", "Bo Yuan"], "title": "EcoSpa: Efficient Transformer Training with Coupled Sparsity", "comment": null, "summary": "Transformers have become the backbone of modern AI, yet their high computational demands pose critical system challenges. While sparse training offers efficiency gains, existing methods fail to preserve critical structural relationships between weight matrices that interact multiplicatively in attention and feed-forward layers. This oversight leads to performance degradation at high sparsity levels. We introduce EcoSpa, an efficient structured sparse training method that jointly evaluates and sparsifies coupled weight matrix pairs, preserving their interaction patterns through aligned row/column removal. EcoSpa introduces a new granularity for calibrating structural component importance and performs coupled estimation and sparsification across both pre-training and fine-tuning scenarios. Evaluations demonstrate substantial improvements: EcoSpa enables efficient training of LLaMA-1B with 50\\% memory reduction and 21\\% faster training, achieves $2.2\\times$ model compression on GPT-2-Medium with $2.4$ lower perplexity, and delivers $1.6\\times$ inference speedup. The approach uses standard PyTorch operations, requiring no custom hardware or kernels, making efficient transformer training accessible on commodity hardware.", "AI": {"tldr": "EcoSpa: An efficient structured sparse training method for Transformers that preserves weight matrix relationships.", "motivation": "Existing sparse training methods for Transformers fail to preserve structural relationships between weight matrices, leading to performance degradation at high sparsity levels.", "method": "EcoSpa jointly evaluates and sparsifies coupled weight matrix pairs, preserving their interaction patterns through aligned row/column removal. It introduces a new granularity for calibrating structural component importance and performs coupled estimation and sparsification across both pre-training and fine-tuning scenarios.", "result": "EcoSpa enables efficient training of LLaMA-1B with 50% memory reduction and 21% faster training, achieves 2.2x model compression on GPT-2-Medium with 2.4 lower perplexity, and delivers 1.6x inference speedup.", "conclusion": "EcoSpa uses standard PyTorch operations, requiring no custom hardware or kernels, making efficient transformer training accessible on commodity hardware."}}
{"id": "2511.12920", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12920", "abs": "https://arxiv.org/abs/2511.12920", "authors": ["Desheng Hu", "Joachim Baumann", "Aleksandra Urman", "Elsa Lichtenegger", "Robin Forsberg", "Aniko Hannak", "Christo Wilson"], "title": "Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy", "comment": "18 pages, 10 figures; to appear in AAAI ICWSM 2026", "summary": "Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.", "AI": {"tldr": "\u8c37\u6b4c\u641c\u7d22\u8d8a\u6765\u8d8a\u591a\u5730\u901a\u8fc7AI Overviews (AIO) \u548c Featured Snippets (FS)\u7b49\u529f\u80fd\u5c55\u793aAI\u751f\u6210\u7684\u5185\u5bb9\uff0c\u4f46\u7528\u6237\u65e0\u6cd5\u63a7\u5236\u5176\u5c55\u793a\u65b9\u5f0f\u3002\u901a\u8fc7\u5bf91508\u4e2a\u771f\u5b9e\u7684\u5a74\u513f\u62a4\u7406\u548c\u598a\u5a20\u76f8\u5173\u67e5\u8be2\u7684\u7cfb\u7edf\u7b97\u6cd5\u5ba1\u6838\uff0c\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u4fe1\u606f\u5c55\u793a\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u8bc4\u4f30AI\u751f\u6210\u5185\u5bb9\u5728\u6bcd\u5a74\u5065\u5eb7\u9886\u57df\u7684\u4fe1\u606f\u8d28\u91cf\u548c\u4e00\u81f4\u6027\uff0c\u56e0\u4e3a\u7528\u6237\u8d8a\u6765\u8d8a\u4f9d\u8d56\u8fd9\u4e9b\u4fe1\u606f\uff0c\u4f46\u5bf9\u5176\u5c55\u793a\u65b9\u5f0f\u7f3a\u4e4f\u63a7\u5236\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7b97\u6cd5\u5ba1\u68381508\u4e2a\u5a74\u513f\u62a4\u7406\u548c\u598a\u5a20\u76f8\u5173\u67e5\u8be2\uff0c\u5e76\u4ece\u7b54\u6848\u4e00\u81f4\u6027\u3001\u76f8\u5173\u6027\u3001\u533b\u7597\u4fdd\u969c\u63aa\u65bd\u3001\u6765\u6e90\u7c7b\u522b\u548c\u60c5\u611f\u4e00\u81f4\u6027\u7b49\u591a\u7ef4\u5ea6\u8bc4\u4f30AIO\u548cFS\u7684\u8d28\u91cf\u3002", "result": "AIO\u548cFS\u5728\u540c\u4e00\u641c\u7d22\u7ed3\u679c\u9875\u9762\u4e0a\u663e\u793a\u7684\u4fe1\u606f\u4e0d\u4e00\u81f4\uff0c\u6bd4\u4f8b\u9ad8\u8fbe33%\u3002\u4e24\u8005\u90fd\u4e25\u91cd\u7f3a\u4e4f\u533b\u7597\u4fdd\u969c\u63aa\u65bd\uff08\u5206\u522b\u4ec5\u536011%\u548c7%\uff09\u3002FS\u7ecf\u5e38\u94fe\u63a5\u5230\u5546\u4e1a\u6765\u6e90\u3002", "conclusion": "AI\u4ecb\u5bfc\u7684\u5065\u5eb7\u4fe1\u606f\u9700\u8981\u66f4\u4e25\u683c\u7684\u8d28\u91cf\u63a7\u5236\u3002\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u8f6c\u79fb\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5ba1\u6838\u9ad8\u98ce\u9669\u9886\u57df\u4e2d\u7684AI\u7cfb\u7edf\uff0c\u5728\u8fd9\u4e9b\u9886\u57df\u4e2d\uff0c\u4fe1\u606f\u8d28\u91cf\u76f4\u63a5\u5f71\u54cd\u7528\u6237\u798f\u7949\u3002"}}
{"id": "2511.12135", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12135", "abs": "https://arxiv.org/abs/2511.12135", "authors": ["Letian Chen", "Runhan Shi", "Gufeng Yu", "Yang Yang"], "title": "RTMol: Rethinking Molecule-text Alignment in a Round-trip View", "comment": null, "summary": "Aligning molecular sequence representations (e.g., SMILES notations) with textual descriptions is critical for applications spanning drug discovery, materials design, and automated chemical literature analysis. Existing methodologies typically treat molecular captioning (molecule-to-text) and text-based molecular design (text-to-molecule) as separate tasks, relying on supervised fine-tuning or contrastive learning pipelines. These approaches face three key limitations: (i) conventional metrics like BLEU prioritize linguistic fluency over chemical accuracy, (ii) training datasets frequently contain chemically ambiguous narratives with incomplete specifications, and (iii) independent optimization of generation directions leads to bidirectional inconsistency. To address these issues, we propose RTMol, a bidirectional alignment framework that unifies molecular captioning and text-to-SMILES generation through self-supervised round-trip learning. The framework introduces novel round-trip evaluation metrics and enables unsupervised training for molecular captioning without requiring paired molecule-text corpora. Experiments demonstrate that RTMol enhances bidirectional alignment performance by up to 47% across various LLMs, establishing an effective paradigm for joint molecule-text understanding and generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86RTMol\uff0c\u4e00\u4e2a\u901a\u8fc7\u81ea\u76d1\u7763\u5f80\u8fd4\u5b66\u4e60\u7edf\u4e00\u5206\u5b50\u63cf\u8ff0\u548c\u6587\u672c\u5230SMILES\u751f\u6210\u7684\u53cc\u5411\u5bf9\u9f50\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u5b50\u63cf\u8ff0\u548c\u6587\u672c\u5230\u5206\u5b50\u8bbe\u8ba1\u65b9\u6cd5\u901a\u5e38\u88ab\u89c6\u4e3a\u72ec\u7acb\u7684\u4efb\u52a1\uff0c\u4f9d\u8d56\u4e8e\u76d1\u7763\u5fae\u8c03\u6216\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5e76\u4e14\u9762\u4e34\u4e09\u4e2a\u4e3b\u8981\u9650\u5236\uff1a(i) \u4f20\u7edf\u6307\u6807\u4f18\u5148\u8003\u8651\u8bed\u8a00\u6d41\u7545\u6027\u800c\u975e\u5316\u5b66\u51c6\u786e\u6027\uff0c(ii) \u8bad\u7ec3\u6570\u636e\u96c6\u5305\u542b\u5316\u5b66\u4e0a\u6a21\u7cca\u7684\u53d9\u8ff0\u548c\u4e0d\u5b8c\u6574\u7684\u89c4\u8303\uff0c(iii) \u751f\u6210\u65b9\u5411\u7684\u72ec\u7acb\u4f18\u5316\u5bfc\u81f4\u53cc\u5411\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u4e86RTMol\uff0c\u5b83\u5f15\u5165\u4e86\u65b0\u7684\u5f80\u8fd4\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5b9e\u73b0\u4e86\u5206\u5b50\u63cf\u8ff0\u7684\u65e0\u76d1\u7763\u8bad\u7ec3\uff0c\u65e0\u9700\u914d\u5bf9\u7684\u5206\u5b50-\u6587\u672c\u8bed\u6599\u5e93\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRTMol\u5728\u5404\u79cdLLM\u4e2d\u5c06\u53cc\u5411\u5bf9\u9f50\u6027\u80fd\u63d0\u9ad8\u4e86\u9ad8\u8fbe47%\u3002", "conclusion": "RTMol \u4e3a\u8054\u5408\u5206\u5b50-\u6587\u672c\u7406\u89e3\u548c\u751f\u6210\u5efa\u7acb\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u8303\u4f8b\u3002"}}
{"id": "2511.12281", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12281", "abs": "https://arxiv.org/abs/2511.12281", "authors": ["Ivan Zakazov", "Alexander Sharipov", "Berke Argin", "Oussama Gabouj", "Kamel Charaf", "Alexi Semiz", "Lorenzo Drudi", "Nicolas Baldwin", "Robert West"], "title": "Cmprsr: Abstractive Token-Level Question-Agnostic Prompt Compressor", "comment": null, "summary": "Motivated by the high costs of using black-box Large Language Models (LLMs), we introduce a novel prompt compression paradigm, under which we use smaller LLMs to compress inputs for the larger ones. We present the first comprehensive LLM-as-a-compressor benchmark spanning 25 open- and closed-source models, which reveals significant disparity in models' compression ability in terms of (i) preserving semantically important information (ii) following the user-provided compression rate (CR). We further improve the performance of gpt-4.1-mini, the best overall vanilla compressor, with Textgrad-based compression meta-prompt optimization. We also identify the most promising open-source vanilla LLM - Qwen3-4B - and post-train it with a combination of supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), pursuing the dual objective of CR adherence and maximizing the downstream task performance. We call the resulting model Cmprsr and demonstrate its superiority over both extractive and vanilla abstractive compression across the entire range of compression rates on lengthy inputs from MeetingBank and LongBench as well as short prompts from GSM8k. The latter highlights Cmprsr's generalizability across varying input lengths and domains. Moreover, Cmprsr closely follows the requested compression rate, offering fine control over the cost-quality trade-off.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63d0\u793a\u538b\u7f29\u8303\u5f0f\uff0c\u4f7f\u7528\u8f83\u5c0f\u7684LLM\u6765\u538b\u7f29\u8f83\u5927LLM\u7684\u8f93\u5165\uff0c\u4ee5\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u4f7f\u7528\u9ed1\u76d2\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6210\u672c\u9ad8\u6602\u3002", "method": "\u6784\u5efa\u4e86\u9996\u4e2a\u5168\u9762\u7684LLM\u538b\u7f29\u5668\u57fa\u51c6\uff0c\u5e76\u4f7f\u7528Textgrad\u4f18\u5316\u538b\u7f29meta-prompt\uff0c\u4ee5\u53ca\u4f7f\u7528SFT\u548cGRPO\u5bf9Qwen3-4B\u8fdb\u884c\u540e\u8bad\u7ec3\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u538b\u7f29\u80fd\u529b\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86Cmprsr\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5176\u4ed6\u538b\u7f29\u65b9\u6cd5\u3002", "conclusion": "Cmprsr\u6a21\u578b\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e14\u53ef\u4ee5\u7cbe\u786e\u63a7\u5236\u538b\u7f29\u7387\uff0c\u4ece\u800c\u5728\u6210\u672c\u548c\u8d28\u91cf\u4e4b\u95f4\u5b9e\u73b0\u7cbe\u7ec6\u7684\u6743\u8861\u3002"}}
{"id": "2511.11944", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11944", "abs": "https://arxiv.org/abs/2511.11944", "authors": ["Ling Wang", "Yunfan Lu", "Wenzong Ma", "Huizai Yao", "Pengteng Li", "Hui Xiong"], "title": "From Events to Clarity: The Event-Guided Diffusion Framework for Dehazing", "comment": "11 pages, 8 figures. Completed in April 2025", "summary": "Clear imaging under hazy conditions is a critical task. Prior-based and neural methods have improved results. However, they operate on RGB frames, which suffer from limited dynamic range. Therefore, dehazing remains ill-posed and can erase structure and illumination details. To address this, we use event cameras for dehazing for the \\textbf{first time}. Event cameras offer much higher HDR ($120 dBvs.60 dB$) and microsecond latency, therefore they suit hazy scenes. In practice, transferring HDR cues from events to frames is hard because real paired data are scarce. To tackle this, we propose an event-guided diffusion model that utilizes the strong generative priors of diffusion models to reconstruct clear images from hazy inputs by effectively transferring HDR information from events. Specifically, we design an event-guided module that maps sparse HDR event features, \\textit{e.g.,} edges, corners, into the diffusion latent space. This clear conditioning provides precise structural guidance during generation, improves visual realism, and reduces semantic drift. For real-world evaluation, we collect a drone dataset in heavy haze (AQI = 341) with synchronized RGB and event sensors. Experiments on two benchmarks and our dataset achieve state-of-the-art results.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u8fdb\u884c\u53bb\u96fe\uff0c\u4ee5\u5e94\u5bf9\u4f20\u7edfRGB\u56fe\u50cf\u52a8\u6001\u8303\u56f4\u6709\u9650\u7684\u95ee\u9898\u3002", "motivation": "RGB\u56fe\u50cf\u5728\u96fe\u973e\u6761\u4ef6\u4e0b\u6210\u50cf\u6548\u679c\u5dee\uff0c\u52a8\u6001\u8303\u56f4\u6709\u9650\uff0c\u5bfc\u81f4\u53bb\u96fe\u6548\u679c\u4e0d\u4f73\uff0c\u5bb9\u6613\u4e22\u5931\u7ed3\u6784\u548c\u5149\u7167\u7ec6\u8282\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e8b\u4ef6\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u5148\u9a8c\u77e5\u8bc6\uff0c\u901a\u8fc7\u6709\u6548\u5730\u4ece\u4e8b\u4ef6\u4e2d\u8f6c\u79fbHDR\u4fe1\u606f\u6765\u4ece\u96fe\u973e\u8f93\u5165\u4e2d\u91cd\u5efa\u6e05\u6670\u7684\u56fe\u50cf\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e8b\u4ef6\u5f15\u5bfc\u6a21\u5757\uff0c\u5c06\u7a00\u758f\u7684HDR\u4e8b\u4ef6\u7279\u5f81\uff08\u4f8b\u5982\uff0c\u8fb9\u7f18\u3001\u89d2\u70b9\uff09\u6620\u5c04\u5230\u6269\u6563\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4f5c\u8005\u6536\u96c6\u7684\u65e0\u4eba\u673a\u6570\u636e\u96c6\u4e0a\uff0c\u5b9e\u9a8c\u7ed3\u679c\u5747\u8fbe\u5230\u4e86state-of-the-art\u7684\u6c34\u5e73\u3002", "conclusion": "\u4e8b\u4ef6\u76f8\u673a\u7ed3\u5408\u6269\u6563\u6a21\u578b\u80fd\u591f\u6709\u6548\u63d0\u5347\u96fe\u973e\u56fe\u50cf\u7684\u6e05\u6670\u5ea6\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u6700\u597d\u7684\u6548\u679c\u3002"}}
{"id": "2511.12934", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.12934", "abs": "https://arxiv.org/abs/2511.12934", "authors": ["Zhi Kou", "Xiang-Rong Sheng", "Shuguang Han", "Zhishan Zhao", "Yueyao Cheng", "Han Zhu", "Jian Xu", "Bo Zheng"], "title": "AIF: Asynchronous Inference Framework for Cost-Effective Pre-Ranking", "comment": null, "summary": "In industrial recommendation systems, pre-ranking models based on deep neural networks (DNNs) commonly adopt a sequential execution framework: feature fetching and model forward computation are triggered only after receiving candidates from the upstream retrieval stage. This design introduces inherent bottlenecks, including redundant computations of identical users/items and increased latency due to strictly sequential operations, which jointly constrain the model's capacity and system efficiency. To address these limitations, we propose the Asynchronous Inference Framework (AIF), a cost-effective computational architecture that decouples interaction-independent components, those operating within a single user or item, from real-time prediction. AIF reorganizes the model inference process by performing user-side computations in parallel with the retrieval stage and conducting item-side computations in a nearline manner. This means that interaction-independent components are calculated just once and completed before the real-time prediction phase of the pre-ranking stage. As a result, AIF enhances computational efficiency and reduces latency, freeing up resources to significantly improve the feature set and model architecture of interaction-independent components. Moreover, we delve into model design within the AIF framework, employing approximated methods for interaction-dependent components in online real-time predictions. By co-designing both the framework and the model, our solution achieves notable performance gains without significantly increasing computational and latency costs. This has enabled the successful deployment of AIF in the Taobao display advertising system.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5f02\u6b65\u63a8\u7406\u6846\u67b6\uff08AIF\uff09\u4ee5\u89e3\u51b3\u5de5\u4e1a\u63a8\u8350\u7cfb\u7edf\u4e2d\u9884\u6392\u5e8f\u6a21\u578b\u7684\u74f6\u9888\u3002", "motivation": "\u4f20\u7edf\u7684\u9884\u6392\u5e8f\u6a21\u578b\u5b58\u5728\u5197\u4f59\u8ba1\u7b97\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5bb9\u91cf\u548c\u7cfb\u7edf\u6548\u7387\u3002", "method": "AIF\u5c06\u7528\u6237\u4fa7\u548c\u7269\u54c1\u4fa7\u7684\u8ba1\u7b97\u4e0e\u5b9e\u65f6\u9884\u6d4b\u89e3\u8026\uff0c\u5e76\u884c\u8ba1\u7b97\u7528\u6237\u4fa7\u7279\u5f81\uff0c\u8fd1\u7ebf\u8ba1\u7b97\u7269\u54c1\u4fa7\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u8fd1\u4f3c\u65b9\u6cd5\u5904\u7406\u4ea4\u4e92\u4f9d\u8d56\u7ec4\u4ef6\u3002", "result": "AIF\u5728\u8ba1\u7b97\u548c\u5ef6\u8fdf\u6210\u672c\u6ca1\u6709\u663e\u8457\u589e\u52a0\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u6846\u67b6\u548c\u6a21\u578b\u7684\u534f\u540c\u8bbe\u8ba1\uff0cAIF\u6210\u529f\u90e8\u7f72\u5728\u6dd8\u5b9d\u5c55\u793a\u5e7f\u544a\u7cfb\u7edf\u4e2d\u3002"}}
{"id": "2511.12169", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12169", "abs": "https://arxiv.org/abs/2511.12169", "authors": ["Kaiyue Zhao", "Dingqi Chen", "Shaoyu Wang", "Pan Hu"], "title": "Incremental Maintenance of DatalogMTL Materialisations", "comment": "Accepted as oral paper at the main track of AAAI 2026", "summary": "DatalogMTL extends the classical Datalog language with metric temporal logic (MTL), enabling expressive reasoning over temporal data. While existing reasoning approaches, such as materialisation based and automata based methods, offer soundness and completeness, they lack support for handling efficient dynamic updates, a crucial requirement for real-world applications that involve frequent data updates. In this work, we propose DRedMTL, an incremental reasoning algorithm for DatalogMTL with bounded intervals. Our algorithm builds upon the classical DRed algorithm, which incrementally updates the materialisation of a Datalog program. Unlike a Datalog materialisation which is in essence a finite set of facts, a DatalogMTL materialisation has to be represented as a finite set of facts plus periodic intervals indicating how the full materialisation can be constructed through unfolding. To cope with this, our algorithm is equipped with specifically designed operators to efficiently handle such periodic representations of DatalogMTL materialisations. We have implemented this approach and tested it on several publicly available datasets. Experimental results show that DRedMTL often significantly outperforms rematerialisation, sometimes by orders of magnitude.", "AI": {"tldr": "DatalogMTL\u7528\u5ea6\u91cf\u65f6\u5e8f\u903b\u8f91\u6269\u5c55\u4e86\u7ecf\u5178\u7684Datalog\u8bed\u8a00\uff0c\u5b9e\u73b0\u4e86\u5bf9\u65f6\u6001\u6570\u636e\u7684\u8868\u8fbe\u6027\u63a8\u7406\u3002\u73b0\u6709\u7684\u63a8\u7406\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u5904\u7406\u9ad8\u6548\u52a8\u6001\u66f4\u65b0\u7684\u652f\u6301\uff0c\u800c\u8fd9\u5bf9\u4e8e\u6d89\u53ca\u9891\u7e41\u6570\u636e\u66f4\u65b0\u7684\u5b9e\u9645\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eDatalogMTL\u7684\u589e\u91cf\u63a8\u7406\u7b97\u6cd5DRedMTL\uff0c\u8be5\u7b97\u6cd5\u5efa\u7acb\u5728\u7ecf\u5178\u7684DRed\u7b97\u6cd5\u4e4b\u4e0a\uff0c\u901a\u8fc7\u5c55\u5f00\u6765\u6709\u6548\u5730\u5904\u7406DatalogMTL\u7269\u5316\u7684\u5468\u671f\u6027\u8868\u793a\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDRedMTL\u901a\u5e38\u660e\u663e\u4f18\u4e8e\u91cd\u65b0\u7269\u5316\u3002", "motivation": "\u73b0\u6709\u7684DatalogMTL\u63a8\u7406\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u5904\u7406\u9ad8\u6548\u52a8\u6001\u66f4\u65b0\u7684\u652f\u6301\uff0c\u8fd9\u5bf9\u4e8e\u6d89\u53ca\u9891\u7e41\u6570\u636e\u66f4\u65b0\u7684\u5b9e\u9645\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eDatalogMTL\u7684\u589e\u91cf\u63a8\u7406\u7b97\u6cd5DRedMTL\uff0c\u8be5\u7b97\u6cd5\u5efa\u7acb\u5728\u7ecf\u5178\u7684DRed\u7b97\u6cd5\u4e4b\u4e0a\uff0c\u5e76\u914d\u5907\u4e86\u4e13\u95e8\u8bbe\u8ba1\u7684\u7b97\u5b50\u6765\u6709\u6548\u5730\u5904\u7406DatalogMTL\u7269\u5316\u7684\u5468\u671f\u6027\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDRedMTL\u901a\u5e38\u660e\u663e\u4f18\u4e8e\u91cd\u65b0\u7269\u5316\uff0c\u6709\u65f6\u751a\u81f3\u9ad8\u51fa\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "DRedMTL\u662f\u4e00\u79cd\u7528\u4e8eDatalogMTL\u7684\u589e\u91cf\u63a8\u7406\u7b97\u6cd5\uff0c\u5b83\u901a\u8fc7\u6709\u6548\u5730\u5904\u7406DatalogMTL\u7269\u5316\u7684\u5468\u671f\u6027\u8868\u793a\uff0c\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u91cd\u65b0\u7269\u5316\u3002"}}
{"id": "2511.12290", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12290", "abs": "https://arxiv.org/abs/2511.12290", "authors": ["Purnima Bindal", "Vikas Kumar", "Sagar Rathore", "Vasudha Bhatnagar"], "title": "AugAbEx : Way Forward for Extractive Case Summarization", "comment": "30 pages, under review in a Journal", "summary": "Summarization of legal judgments poses a heavy cognitive burden on law practitioners due to the complexity of the language, context-sensitive legal jargon, and the length of the document. Therefore, the automatic summarization of legal documents has attracted serious attention from natural language processing researchers. Since the abstractive summaries of legal documents generated by deep neural methods remain prone to the risk of misrepresenting nuanced legal jargon or overlooking key contextual details, we envisage a rising trend toward the use of extractive case summarizers.\n  Given the high cost of human annotation for gold standard extractive summaries, we engineer a light and transparent pipeline that leverages existing abstractive gold standard summaries to create the corresponding extractive gold standard versions. The approach ensures that the experts` opinions ensconced in the original gold standard abstractive summaries are carried over to the transformed extractive summaries. We aim to augment seven existing case summarization datasets, which include abstractive summaries, by incorporating corresponding extractive summaries and create an enriched data resource for case summarization research community. To ensure the quality of the augmented extractive summaries, we perform an extensive comparative evaluation with the original abstractive gold standard summaries covering structural, lexical, and semantic dimensions. We also compare the domain-level information of the two summaries. We commit to release the augmented datasets in the public domain for use by the research community and believe that the resource will offer opportunities to advance the field of automatic summarization of legal documents.", "AI": {"tldr": "\u6cd5\u5f8b\u5224\u51b3\u6458\u8981\u56e0\u5176\u8bed\u8a00\u590d\u6742\u3001\u6cd5\u5f8b\u672f\u8bed\u5177\u6709\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u4ee5\u53ca\u6587\u6863\u957f\u5ea6\u800c\u7ed9\u6cd5\u5f8b\u4ece\u4e1a\u8005\u5e26\u6765\u4e86\u6c89\u91cd\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002\u56e0\u6b64\uff0c\u6cd5\u5f8b\u6587\u4ef6\u7684\u81ea\u52a8\u6458\u8981\u5f15\u8d77\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u4eba\u5458\u7684\u91cd\u89c6\u3002\u6211\u4eec\u8bbe\u60f3\uff0c\u7531\u4e8e\u6df1\u5ea6\u795e\u7ecf\u65b9\u6cd5\u751f\u6210\u7684\u6cd5\u5f8b\u6587\u4ef6\u7684\u62bd\u8c61\u6458\u8981\u4ecd\u7136\u5bb9\u6613\u51fa\u73b0\u6b6a\u66f2\u7ec6\u5fae\u6cd5\u5f8b\u672f\u8bed\u6216\u5ffd\u7565\u5173\u952e\u4e0a\u4e0b\u6587\u7ec6\u8282\u7684\u98ce\u9669\uff0c\u56e0\u6b64\u4f7f\u7528\u62bd\u53d6\u6848\u4f8b\u6458\u8981\u5668\u5c06\u6210\u4e3a\u4e00\u79cd\u4e0a\u5347\u8d8b\u52bf\u3002", "motivation": "\u7531\u4e8e\u6cd5\u5f8b\u5224\u51b3\u6458\u8981\u5bf9\u6cd5\u5f8b\u4ece\u4e1a\u8005\u9020\u6210\u8ba4\u77e5\u8d1f\u62c5\uff0c\u5e76\u4e14\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u751f\u6210\u6cd5\u5f8b\u6587\u4ef6\u6458\u8981\u3002", "method": "\u5229\u7528\u73b0\u6709\u7684\u62bd\u8c61\u9ec4\u91d1\u6807\u51c6\u6458\u8981\uff0c\u6784\u5efa\u4e00\u4e2a\u8f7b\u91cf\u4e14\u900f\u660e\u7684\u6d41\u7a0b\uff0c\u4ee5\u521b\u5efa\u76f8\u5e94\u7684\u62bd\u53d6\u9ec4\u91d1\u6807\u51c6\u7248\u672c\u3002\u8be5\u65b9\u6cd5\u786e\u4fdd\u4e13\u5bb6\u7684\u610f\u89c1\u80fd\u591f\u4ece\u539f\u59cb\u7684\u62bd\u8c61\u9ec4\u91d1\u6807\u51c6\u6458\u8981\u5ef6\u7eed\u5230\u8f6c\u6362\u540e\u7684\u62bd\u53d6\u6458\u8981\u4e2d\u3002", "result": "\u901a\u8fc7\u6574\u5408\u76f8\u5e94\u7684\u62bd\u53d6\u6458\u8981\u6765\u6269\u5145\u73b0\u6709\u7684\u4e03\u4e2a\u6848\u4f8b\u6458\u8981\u6570\u636e\u96c6\uff0c\u5305\u62ec\u62bd\u8c61\u6458\u8981\uff0c\u5e76\u4e3a\u6848\u4f8b\u6458\u8981\u7814\u7a76\u793e\u533a\u521b\u5efa\u4e00\u4e2a\u4e30\u5bcc\u7684\u6570\u636e\u8d44\u6e90\u3002\u4e3a\u4e86\u786e\u4fdd\u589e\u5f3a\u7684\u62bd\u53d6\u6458\u8981\u7684\u8d28\u91cf\uff0c\u6211\u4eec\u4f7f\u7528\u539f\u59cb\u7684\u62bd\u8c61\u9ec4\u91d1\u6807\u51c6\u6458\u8981\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u6bd4\u8f83\u8bc4\u4f30\uff0c\u6db5\u76d6\u7ed3\u6784\u3001\u8bcd\u6c47\u548c\u8bed\u4e49\u7ef4\u5ea6\u3002\u6211\u4eec\u8fd8\u5c06\u6bd4\u8f83\u4e24\u4e2a\u6458\u8981\u7684\u57df\u7ea7\u522b\u4fe1\u606f\u3002", "conclusion": "\u6211\u4eec\u81f4\u529b\u4e8e\u5728\u516c\u5171\u9886\u57df\u53d1\u5e03\u589e\u5f3a\u7684\u6570\u636e\u96c6\uff0c\u4f9b\u7814\u7a76\u793e\u533a\u4f7f\u7528\uff0c\u5e76\u76f8\u4fe1\u8be5\u8d44\u6e90\u5c06\u4e3a\u63a8\u8fdb\u6cd5\u5f8b\u6587\u4ef6\u7684\u81ea\u52a8\u6458\u8981\u9886\u57df\u63d0\u4f9b\u673a\u4f1a\u3002"}}
{"id": "2511.11959", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11959", "abs": "https://arxiv.org/abs/2511.11959", "authors": ["Leonardi Melo", "Lu\u00eds Gustavo", "Dimmy Magalh\u00e3es", "Lucciani Vieira", "Mauro Ara\u00fajo"], "title": "Evaluation of Attention Mechanisms in U-Net Architectures for Semantic Segmentation of Brazilian Rock Art Petroglyphs", "comment": "14 pages, 8 figures. Preprint submitted to arXiv", "summary": "This study presents a comparative analysis of three U-Net-based architectures for semantic segmentation of rock art petroglyphs from Brazilian archaeological sites. The investigated architectures were: (1) BEGL-UNet with Border-Enhanced Gaussian Loss function; (2) Attention-Residual BEGL-UNet, incorporating residual blocks and gated attention mechanisms; and (3) Spatial Channel Attention BEGL-UNet, which employs spatial-channel attention modules based on Convolutional Block Attention Module. All implementations employed the BEGL loss function combining binary cross-entropy with Gaussian edge enhancement. Experiments were conducted on images from the Po\u00e7o da Bebidinha Archaeological Complex, Piau\u00ed, Brazil, using 5-fold cross-validation. Among the architectures, Attention-Residual BEGL-UNet achieved the best overall performance with Dice Score of 0.710, validation loss of 0.067, and highest recall of 0.854. Spatial Channel Attention BEGL-UNet obtained comparable performance with DSC of 0.707 and recall of 0.857. The baseline BEGL-UNet registered DSC of 0.690. These results demonstrate the effectiveness of attention mechanisms for archaeological heritage digital preservation, with Dice Score improvements of 2.5-2.9% over the baseline.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e09\u79cd\u57fa\u4e8eU-Net\u7684\u67b6\u6784\u5728\u5df4\u897f\u8003\u53e4\u9057\u5740\u5ca9\u753b\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u5e94\u7528\u3002Attention-Residual BEGL-UNet \u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5bf9\u5df4\u897f\u8003\u53e4\u9057\u5740\u7684\u5ca9\u753b\u8fdb\u884c\u8bed\u4e49\u5206\u5272", "method": "\u6bd4\u8f83\u4e09\u79cd\u57fa\u4e8eU-Net\u7684\u67b6\u6784\uff1aBEGL-UNet\u3001Attention-Residual BEGL-UNet \u548c Spatial Channel Attention BEGL-UNet", "result": "Attention-Residual BEGL-UNet \u53d6\u5f97\u4e86\u6700\u4f73\u7684\u6574\u4f53\u6027\u80fd\uff0cDice \u7cfb\u6570\u4e3a 0.710\uff0c\u9a8c\u8bc1\u635f\u5931\u4e3a 0.067\uff0c\u53ec\u56de\u7387\u6700\u9ad8\uff0c\u4e3a 0.854", "conclusion": "\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u4e8e\u8003\u53e4\u9057\u4ea7\u7684\u6570\u5b57\u5316\u4fdd\u62a4\u975e\u5e38\u6709\u6548"}}
{"id": "2511.11647", "categories": ["cs.LG", "cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2511.11647", "abs": "https://arxiv.org/abs/2511.11647", "authors": ["Dariush Salami", "Ramin Hashemi", "Parham Kazemi", "Mikko A. Uusitalo"], "title": "Environment-Aware Transfer Reinforcement Learning for Sustainable Beam Selection", "comment": "Accepted to be published in a workshop in IEEE GLOBECOM 2025", "summary": "This paper presents a novel and sustainable approach for improving beam selection in 5G and beyond networks using transfer learning and Reinforcement Learning (RL). Traditional RL-based beam selection models require extensive training time and computational resources, particularly when deployed in diverse environments with varying propagation characteristics posing a major challenge for scalability and energy efficiency. To address this, we propose modeling the environment as a point cloud, where each point represents the locations of gNodeBs (gNBs) and surrounding scatterers. By computing the Chamfer distance between point clouds, structurally similar environments can be efficiently identified, enabling the reuse of pre-trained models through transfer learning. This methodology leads to a 16x reduction in training time and computational overhead, directly contributing to energy efficiency. By minimizing the need for retraining in each new deployment, our approach significantly lowers power consumption and supports the development of green and sustainable Artificial Intelligence (AI) in wireless systems. Furthermore, it accelerates time-to-deployment, reduces carbon emissions associated with training, and enhances the viability of deploying AI-driven communication systems at the edge. Simulation results confirm that our approach maintains high performance while drastically cutting energy costs, demonstrating the potential of transfer learning to enable scalable, adaptive, and environmentally conscious RL-based beam selection strategies in dynamic and diverse propagation environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6ce2\u675f\u9009\u62e9\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u63d0\u9ad8\u80fd\u6e90\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6ce2\u675f\u9009\u62e9\u6a21\u578b\u9700\u8981\u5927\u91cf\u7684\u8bad\u7ec3\u65f6\u95f4\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u4e0d\u540c\u4f20\u64ad\u7279\u6027\u7684\u591a\u6837\u5316\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\uff0c\u5bf9\u53ef\u6269\u5c55\u6027\u548c\u80fd\u6e90\u6548\u7387\u6784\u6210\u91cd\u5927\u6311\u6218\u3002", "method": "\u5c06\u73af\u5883\u5efa\u6a21\u4e3a\u70b9\u4e91\uff0c\u901a\u8fc7\u8ba1\u7b97\u70b9\u4e91\u4e4b\u95f4\u7684Chamfer\u8ddd\u79bb\u6765\u8bc6\u522b\u7ed3\u6784\u76f8\u4f3c\u7684\u73af\u5883\uff0c\u4ece\u800c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u91cd\u7528\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "result": "\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e8616\u500d\uff0c\u8ba1\u7b97\u5f00\u9500\u4e5f\u663e\u8457\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u964d\u4f4e\u529f\u8017\uff0c\u652f\u6301\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u7eff\u8272\u53ef\u6301\u7eed\u7684\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\uff0c\u5e76\u52a0\u901f\u90e8\u7f72\u65f6\u95f4\uff0c\u51cf\u5c11\u4e0e\u8bad\u7ec3\u76f8\u5173\u7684\u78b3\u6392\u653e\u3002"}}
{"id": "2511.13063", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.13063", "abs": "https://arxiv.org/abs/2511.13063", "authors": ["Zhenghua Li", "Hang Chen", "Zihao Sun", "Kai Li", "Xiaolin Hu"], "title": "FGNet: Leveraging Feature-Guided Attention to Refine SAM2 for 3D EM Neuron Segmentation", "comment": null, "summary": "Accurate segmentation of neural structures in Electron Microscopy (EM) images is paramount for neuroscience. However, this task is challenged by intricate morphologies, low signal-to-noise ratios, and scarce annotations, limiting the accuracy and generalization of existing methods. To address these challenges, we seek to leverage the priors learned by visual foundation models on a vast amount of natural images to better tackle this task. Specifically, we propose a novel framework that can effectively transfer knowledge from Segment Anything 2 (SAM2), which is pre-trained on natural images, to the EM domain. We first use SAM2 to extract powerful, general-purpose features. To bridge the domain gap, we introduce a Feature-Guided Attention module that leverages semantic cues from SAM2 to guide a lightweight encoder, the Fine-Grained Encoder (FGE), in focusing on these challenging regions. Finally, a dual-affinity decoder generates both coarse and refined affinity maps. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art (SOTA) approaches with the SAM2 weights frozen. Upon further fine-tuning on EM data, our method significantly outperforms existing SOTA methods. This study validates that transferring representations pre-trained on natural images, when combined with targeted domain-adaptive guidance, can effectively address the specific challenges in neuron segmentation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578bSegment Anything 2 (SAM2)\u7684\u77e5\u8bc6\u6765\u63d0\u9ad8\u7535\u5b50\u663e\u5fae\u955c(EM)\u56fe\u50cf\u4e2d\u795e\u7ecf\u7ed3\u6784\u5206\u5272\u7cbe\u5ea6\u7684\u65b0\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7ed3\u6784\u5206\u5272\u65b9\u6cd5\u5728EM\u56fe\u50cf\u4e2d\u9762\u4e34\u5f62\u6001\u590d\u6742\u3001\u4fe1\u566a\u6bd4\u4f4e\u548c\u6ce8\u91ca\u7a00\u7f3a\u7b49\u6311\u6218\uff0c\u9650\u5236\u4e86\u5176\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u8be5\u6846\u67b6\u9996\u5148\u4f7f\u7528SAM2\u63d0\u53d6\u901a\u7528\u7279\u5f81\uff0c\u7136\u540e\u5f15\u5165\u7279\u5f81\u5f15\u5bfc\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5229\u7528SAM2\u7684\u8bed\u4e49\u7ebf\u7d22\u5f15\u5bfc\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668\u5173\u6ce8\u5177\u6709\u6311\u6218\u6027\u7684\u533a\u57df\u3002\u6700\u540e\uff0c\u53cc\u4eb2\u548c\u529b\u89e3\u7801\u5668\u751f\u6210\u7c97\u7565\u548c\u7cbe\u7ec6\u7684\u4eb2\u548c\u529b\u56fe\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728SAM2\u6743\u91cd\u51bb\u7ed3\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u4e0e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002\u5728EM\u6570\u636e\u4e0a\u8fdb\u4e00\u6b65\u5fae\u8c03\u540e\uff0c\u8be5\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u4f73\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\uff0c\u5f53\u7ed3\u5408\u6709\u9488\u5bf9\u6027\u7684\u9886\u57df\u81ea\u9002\u5e94\u6307\u5bfc\u65f6\uff0c\u8fc1\u79fb\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u9884\u8bad\u7ec3\u7684\u8868\u793a\u53ef\u4ee5\u6709\u6548\u5730\u89e3\u51b3\u795e\u7ecf\u5143\u5206\u5272\u4e2d\u7684\u7279\u5b9a\u6311\u6218\u3002"}}
{"id": "2511.12208", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12208", "abs": "https://arxiv.org/abs/2511.12208", "authors": ["Jilong Liu", "Pengyang Shao", "Wei Qin", "Fei Liu", "Yonghui Yang", "Richang Hong"], "title": "Debate over Mixed-knowledge: A Robust Multi-Agent Framework for Incomplete Knowledge Graph Question Answering", "comment": null, "summary": "Knowledge Graph Question Answering (KGQA) aims to improve factual accuracy by leveraging structured knowledge. However, real-world Knowledge Graphs (KGs) are often incomplete, leading to the problem of Incomplete KGQA (IKGQA). A common solution is to incorporate external data to fill knowledge gaps, but existing methods lack the capacity to adaptively and contextually fuse multiple sources, failing to fully exploit their complementary strengths. To this end, we propose Debate over Mixed-knowledge (DoM), a novel framework that enables dynamic integration of structured and unstructured knowledge for IKGQA. Built upon the Multi-Agent Debate paradigm, DoM assigns specialized agents to perform inference over knowledge graphs and external texts separately, and coordinates their outputs through iterative interaction. It decomposes the input question into sub-questions, retrieves evidence via dual agents (KG and Retrieval-Augmented Generation, RAG), and employs a judge agent to evaluate and aggregate intermediate answers. This collaboration exploits knowledge complementarity and enhances robustness to KG incompleteness. In addition, existing IKGQA datasets simulate incompleteness by randomly removing triples, failing to capture the irregular and unpredictable nature of real-world knowledge incompleteness. To address this, we introduce a new dataset, Incomplete Knowledge Graph WebQuestions, constructed by leveraging real-world knowledge updates. These updates reflect knowledge beyond the static scope of KGs, yielding a more realistic and challenging benchmark. Through extensive experiments, we show that DoM consistently outperforms state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Debate over Mixed-knowledge (DoM) \u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u6574\u5408\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u77e5\u8bc6\uff0c\u4ee5\u89e3\u51b3\u4e0d\u5b8c\u6574\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54 (IKGQA) \u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u901a\u5e38\u662f\u4e0d\u5b8c\u6574\u7684\uff0c\u5bfc\u81f4\u4e0d\u5b8c\u6574\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\uff08IKGQA\uff09\u95ee\u9898\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u7f3a\u4e4f\u81ea\u9002\u5e94\u548c\u60c5\u5883\u5316\u5730\u878d\u5408\u591a\u4e2a\u6765\u6e90\u7684\u80fd\u529b\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5b83\u4eec\u7684\u4e92\u8865\u4f18\u52bf\u3002", "method": "\u8be5\u6846\u67b6\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u8303\u5f0f\uff0c\u5206\u914d\u4e13\u95e8\u7684\u667a\u80fd\u4f53\u5206\u522b\u5bf9\u77e5\u8bc6\u56fe\u8c31\u548c\u5916\u90e8\u6587\u672c\u8fdb\u884c\u63a8\u7406\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u4ea4\u4e92\u534f\u8c03\u5b83\u4eec\u7684\u8f93\u51fa\u3002\u5b83\u5c06\u8f93\u5165\u95ee\u9898\u5206\u89e3\u4e3a\u5b50\u95ee\u9898\uff0c\u901a\u8fc7\u53cc\u91cd\u667a\u80fd\u4f53\uff08KG \u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff0cRAG\uff09\u68c0\u7d22\u8bc1\u636e\uff0c\u5e76\u91c7\u7528\u88c1\u5224\u667a\u80fd\u4f53\u6765\u8bc4\u4f30\u548c\u805a\u5408\u4e2d\u95f4\u7b54\u6848\u3002", "result": "DoM \u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u3002", "conclusion": "\u77e5\u8bc6\u4e92\u8865\u6027\u548c\u589e\u5f3a\u4e86\u5bf9 KG \u4e0d\u5b8c\u6574\u6027\u7684\u9c81\u68d2\u6027\u3002\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\uff0cIncomplete Knowledge Graph WebQuestions\uff0c\u5b83\u901a\u8fc7\u5229\u7528\u771f\u5b9e\u4e16\u754c\u7684\u77e5\u8bc6\u66f4\u65b0\u6765\u6784\u5efa\uff0c\u4ece\u800c\u53cd\u6620\u4e86\u8d85\u51fa KG \u9759\u6001\u8303\u56f4\u7684\u77e5\u8bc6\uff0c\u4ece\u800c\u4ea7\u751f\u66f4\u771f\u5b9e\u548c\u66f4\u5177\u6311\u6218\u6027\u7684\u57fa\u51c6\u3002"}}
{"id": "2511.12300", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12300", "abs": "https://arxiv.org/abs/2511.12300", "authors": ["Naoya Sugiura", "Kosuke Yamada", "Yasuhiro Ogawa", "Katsuhiko Toyama", "Ryohei Sasano"], "title": "Do LLMs and Humans Find the Same Questions Difficult? A Case Study on Japanese Quiz Answering", "comment": null, "summary": "LLMs have achieved performance that surpasses humans in many NLP tasks. However, it remains unclear whether problems that are difficult for humans are also difficult for LLMs. This study investigates how the difficulty of quizzes in a buzzer setting differs between LLMs and humans. Specifically, we first collect Japanese quiz data including questions, answers, and correct response rate of humans, then prompted LLMs to answer the quizzes under several settings, and compare their correct answer rate to that of humans from two analytical perspectives. The experimental results showed that, compared to humans, LLMs struggle more with quizzes whose correct answers are not covered by Wikipedia entries, and also have difficulty with questions that require numerical answers.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u4eba\u7c7b\u5728\u62a2\u7b54\u6d4b\u9a8c\u4e2d\u7684\u96be\u5ea6\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f \u0432\u044b\u044f\u0441\u043d\u0438\u0442\u044c LLMs \u96be\u4ee5\u89e3\u51b3\u7684\u95ee\u9898\u662f\u5426\u4e0e\u4eba\u7c7b\u76f8\u540c\u3002", "method": "\u9996\u5148\uff0c\u6536\u96c6\u5305\u542b\u95ee\u9898\u3001\u7b54\u6848\u548c\u4eba\u7c7b\u6b63\u786e\u7387\u7684\u65e5\u8bed\u6d4b\u9a8c\u6570\u636e\u3002\u7136\u540e\uff0c\u63d0\u793a LLMs \u5728\u591a\u4e2a\u8bbe\u7f6e\u4e0b\u56de\u7b54\u6d4b\u9a8c\u95ee\u9898\uff0c\u5e76\u5c06\u5b83\u4eec\u7684\u6b63\u786e\u7387\u4e0e\u4eba\u7c7b\u7684\u6b63\u786e\u7387\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ece\u4e24\u4e2a\u5206\u6790\u89d2\u5ea6\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4eba\u7c7b\u76f8\u6bd4\uff0cLLMs \u66f4\u96be\u56de\u7b54\u6b63\u786e\u7b54\u6848\u672a\u88ab\u7ef4\u57fa\u767e\u79d1\u8986\u76d6\u7684\u6d4b\u9a8c\u95ee\u9898\uff0c\u5e76\u4e14\u96be\u4ee5\u56de\u7b54\u9700\u8981\u6570\u5b57\u7b54\u6848\u7684\u95ee\u9898\u3002", "conclusion": "LLMs \u5728\u5904\u7406\u9700\u8981\u7ef4\u57fa\u767e\u79d1\u77e5\u8bc6\u548c\u6570\u5b57\u63a8\u7406\u7684\u95ee\u9898\u4e0a\u5b58\u5728\u5c40\u9650\u6027\u3002"}}
{"id": "2511.11984", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11984", "abs": "https://arxiv.org/abs/2511.11984", "authors": ["Zhenhao Guo", "Rachit Saluja", "Tianyuan Yao", "Quan Liu", "Junchao Zhu", "Haibo Wang", "Daniel Reisenb\u00fcchler", "Yuankai Huo", "Benjamin Liechty", "David J. Pisapia", "Kenji Ikemura", "Steven Salvatoree", "Surya Seshane", "Mert R. Sabuncu", "Yihe Yang", "Ruining Deng"], "title": "From Classification to Cross-Modal Understanding: Leveraging Vision-Language Models for Fine-Grained Renal Pathology", "comment": null, "summary": "Fine-grained glomerular subtyping is central to kidney biopsy interpretation, but clinically valuable labels are scarce and difficult to obtain. Existing computational pathology approaches instead tend to evaluate coarse diseased classification under full supervision with image-only models, so it remains unclear how vision-language models (VLMs) should be adapted for clinically meaningful subtyping under data constraints. In this work, we model fine-grained glomerular subtyping as a clinically realistic few-shot problem and systematically evaluate both pathology-specialized and general-purpose vision-language models under this setting. We assess not only classification performance (accuracy, AUC, F1) but also the geometry of the learned representations, examining feature alignment between image and text embeddings and the separability of glomerular subtypes. By jointly analyzing shot count, model architecture and domain knowledge, and adaptation strategy, this study provides guidance for future model selection and training under real clinical data constraints. Our results indicate that pathology-specialized vision-language backbones, when paired with the vanilla fine-tuning, are the most effective starting point. Even with only 4-8 labeled examples per glomeruli subtype, these models begin to capture distinctions and show substantial gains in discrimination and calibration, though additional supervision continues to yield incremental improvements. We also find that the discrimination between positive and negative examples is as important as image-text alignment. Overall, our results show that supervision level and adaptation strategy jointly shape both diagnostic performance and multimodal structure, providing guidance for model selection, adaptation strategies, and annotation investment.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5728\u6570\u636e\u53d7\u9650\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u8c03\u6574\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4ee5\u8fdb\u884c\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u7ec6\u7c92\u5ea6\u80be\u5c0f\u7403\u4e9a\u578b\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u7684\u8ba1\u7b97\u75c5\u7406\u5b66\u65b9\u6cd5\u503e\u5411\u4e8e\u5728\u5b8c\u5168\u76d1\u7763\u4e0b\u8bc4\u4f30\u7c97\u7565\u7684\u75be\u75c5\u5206\u7c7b\uff0c\u5e76\u4e14\u7f3a\u4e4f\u6709\u4ef7\u503c\u7684\u4e34\u5e8a\u6807\u7b7e\u3002", "method": "\u5c06\u7ec6\u7c92\u5ea6\u80be\u5c0f\u7403\u4e9a\u578b\u5206\u6790\u5efa\u6a21\u4e3a\u4e00\u4e2a\u4e34\u5e8a\u73b0\u5b9e\u7684\u5c11\u6837\u672c\u95ee\u9898\uff0c\u5e76\u7cfb\u7edf\u5730\u8bc4\u4f30\u75c5\u7406\u5b66\u4e13\u7528\u548c\u901a\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u75c5\u7406\u5b66\u4e13\u7528\u7684\u89c6\u89c9-\u8bed\u8a00\u9aa8\u5e72\u7f51\u7edc\uff0c\u5f53\u4e0e\u539f\u59cb\u5fae\u8c03\u914d\u5bf9\u65f6\uff0c\u662f\u6700\u6709\u6548\u7684\u8d77\u70b9\u3002\u5373\u4f7f\u6bcf\u4e2a\u80be\u5c0f\u7403\u4e9a\u578b\u53ea\u6709 4-8 \u4e2a\u6807\u8bb0\u793a\u4f8b\uff0c\u8fd9\u4e9b\u6a21\u578b\u4e5f\u5f00\u59cb\u6355\u6349\u5dee\u5f02\uff0c\u5e76\u5728\u533a\u5206\u548c\u6821\u51c6\u65b9\u9762\u663e\u793a\u51fa\u663e\u7740 gains\u3002", "conclusion": "\u76d1\u7763\u6c34\u5e73\u548c\u8c03\u6574\u7b56\u7565\u5171\u540c\u5f71\u54cd\u8bca\u65ad\u6027\u80fd\u548c\u591a\u6a21\u6001\u7ed3\u6784\uff0c\u4e3a\u6a21\u578b\u9009\u62e9\u3001\u8c03\u6574\u7b56\u7565\u548c\u6ce8\u91ca\u6295\u8d44\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2511.11648", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11648", "abs": "https://arxiv.org/abs/2511.11648", "authors": ["Shunyu Wu", "Tianyue Li", "Yixuan Leng", "Jingyi Suo", "Jian Lou", "Dan Li", "See-Kiong Ng"], "title": "Lightweight Time Series Data Valuation on Time Series Foundation Models via In-Context Finetuning", "comment": null, "summary": "Time series foundation models (TSFMs) have demonstrated increasing capabilities due to their extensive pretraining on large volumes of diverse time series data. Consequently, the quality of time series data is crucial to TSFM performance, rendering an accurate and efficient data valuation of time series for TSFMs indispensable. However, traditional data valuation methods, such as influence functions, face severe computational bottlenecks due to their poor scalability with growing TSFM model sizes and often fail to preserve temporal dependencies. In this paper, we propose LTSV, a Lightweight Time Series Valuation on TSFMS via in-context finetuning. Grounded in the theoretical evidence that in-context finetuning approximates the influence function, LTSV estimates a sample's contribution by measuring the change in context loss after in-context finetuning, leveraging the strong generalization capabilities of TSFMs to produce robust and transferable data valuations. To capture temporal dependencies, we introduce temporal block aggregation, which integrates per-block influence scores across overlapping time windows. Experiments across multiple time series datasets and models demonstrate that LTSV consistently provides reliable and strong valuation performance, while maintaining manageable computational requirements. Our results suggest that in-context finetuning on time series foundation models provides a practical and effective bridge between data attribution and model generalization in time series learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4f30\u503c\u65b9\u6cd5LTSV\uff0c\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFM\uff09\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u4f20\u7edf\u6570\u636e\u4f30\u503c\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\u4e14\u5ffd\u7565\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "method": "\u901a\u8fc7\u4e0a\u4e0b\u6587\u5fae\u8c03\u8fd1\u4f3c\u5f71\u54cd\u51fd\u6570\uff0c\u5e76\u5f15\u5165\u65f6\u95f4\u5757\u805a\u5408\u4ee5\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLTSV\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u5f3a\u5927\u7684\u4f30\u503c\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8e\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5fae\u8c03\u4e3a\u65f6\u95f4\u5e8f\u5217\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5f52\u56e0\u548c\u6a21\u578b\u6cdb\u5316\u4e4b\u95f4\u67b6\u8d77\u4e86\u4e00\u5ea7\u6865\u6881\u3002"}}
{"id": "2511.13125", "categories": ["cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13125", "abs": "https://arxiv.org/abs/2511.13125", "authors": ["Hao Long", "Silin Zhou", "Lisi Chen", "Shuo Shang"], "title": "Region-Point Joint Representation for Effective Trajectory Similarity Learning", "comment": "This paper is accepted by AAAI2026", "summary": "Recent learning-based methods have reduced the computational complexity of traditional trajectory similarity computation, but state-of-the-art (SOTA) methods still fail to leverage the comprehensive spectrum of trajectory information for similarity modeling. To tackle this problem, we propose \\textbf{RePo}, a novel method that jointly encodes \\textbf{Re}gion-wise and \\textbf{Po}int-wise features to capture both spatial context and fine-grained moving patterns. For region-wise representation, the GPS trajectories are first mapped to grid sequences, and spatial context are captured by structural features and semantic context enriched by visual features. For point-wise representation, three lightweight expert networks extract local, correlation, and continuous movement patterns from dense GPS sequences. Then, a router network adaptively fuses the learned point-wise features, which are subsequently combined with region-wise features using cross-attention to produce the final trajectory embedding. To train RePo, we adopt a contrastive loss with hard negative samples to provide similarity ranking supervision. Experiment results show that RePo achieves an average accuracy improvement of 22.2\\% over SOTA baselines across all evaluation metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRePo\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8054\u5408\u7f16\u7801\u533a\u57df\u548c\u70b9\u7279\u5f81\u4ee5\u6355\u83b7\u7a7a\u95f4\u4e0a\u4e0b\u6587\u548c\u7ec6\u7c92\u5ea6\u7684\u79fb\u52a8\u6a21\u5f0f\uff0c\u4ece\u800c\u63d0\u9ad8\u8f68\u8ff9\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u65e0\u6cd5\u5229\u7528\u8f68\u8ff9\u4fe1\u606f\u7684\u5b8c\u6574\u9891\u8c31\u8fdb\u884c\u76f8\u4f3c\u6027\u5efa\u6a21\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u5c06GPS\u8f68\u8ff9\u6620\u5c04\u5230\u7f51\u683c\u5e8f\u5217\uff0c\u7136\u540e\u901a\u8fc7\u7ed3\u6784\u548c\u8bed\u4e49\u7279\u5f81\u6355\u83b7\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4e13\u5bb6\u7f51\u7edc\u63d0\u53d6\u5c40\u90e8\u3001\u76f8\u5173\u548c\u8fde\u7eed\u7684\u8fd0\u52a8\u6a21\u5f0f\u3002\u4e4b\u540e\uff0c\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u5c06\u70b9\u7279\u5f81\u548c\u533a\u57df\u7279\u5f81\u878d\u5408\uff0c\u751f\u6210\u6700\u7ec8\u7684\u8f68\u8ff9\u5d4c\u5165\u3002", "result": "RePo\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u6bd4SOTA\u57fa\u7ebf\u5e73\u574722.2\uff05\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "RePo\u65b9\u6cd5\u6709\u6548\u5730\u5229\u7528\u4e86\u8f68\u8ff9\u4fe1\u606f\u7684\u5b8c\u6574\u9891\u8c31\uff0c\u63d0\u9ad8\u4e86\u8f68\u8ff9\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2511.12214", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12214", "abs": "https://arxiv.org/abs/2511.12214", "authors": ["Ruochen Li", "Zhanxing Zhu", "Tanqiu Qiao", "Hubert P. H. Shum"], "title": "ViTE: Virtual Graph Trajectory Expert Router for Pedestrian Trajectory Prediction", "comment": null, "summary": "Pedestrian trajectory prediction is critical for ensuring safety in autonomous driving, surveillance systems, and urban planning applications. While early approaches primarily focus on one-hop pairwise relationships, recent studies attempt to capture high-order interactions by stacking multiple Graph Neural Network (GNN) layers. However, these approaches face a fundamental trade-off: insufficient layers may lead to under-reaching problems that limit the model's receptive field, while excessive depth can result in prohibitive computational costs. We argue that an effective model should be capable of adaptively modeling both explicit one-hop interactions and implicit high-order dependencies, rather than relying solely on architectural depth. To this end, we propose ViTE (Virtual graph Trajectory Expert router), a novel framework for pedestrian trajectory prediction. ViTE consists of two key modules: a Virtual Graph that introduces dynamic virtual nodes to model long-range and high-order interactions without deep GNN stacks, and an Expert Router that adaptively selects interaction experts based on social context using a Mixture-of-Experts design. This combination enables flexible and scalable reasoning across varying interaction patterns. Experiments on three benchmarks (ETH/UCY, NBA, and SDD) demonstrate that our method consistently achieves state-of-the-art performance, validating both its effectiveness and practical efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86ViTE\uff0c\u4e00\u4e2a\u7528\u4e8e\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u865a\u62df\u56fe\u548c\u4e13\u5bb6\u8def\u7531\u5668\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u5728\u4e0d\u540c\u4ea4\u4e92\u6a21\u5f0f\u4e0b\u7684\u7075\u6d3b\u548c\u53ef\u6269\u5c55\u7684\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u9636\u4ea4\u4e92\u5efa\u6a21\u4e2d\u9762\u4e34\u4e0d\u8db3\uff0c\u6df1\u5c42GNN\u5806\u53e0\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "ViTE\u5305\u542b\u865a\u62df\u56fe\u548c\u4e13\u5bb6\u8def\u7531\u5668\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff0c\u524d\u8005\u5f15\u5165\u52a8\u6001\u865a\u62df\u8282\u70b9\u4ee5\u5efa\u6a21\u957f\u7a0b\u548c\u9ad8\u9636\u4ea4\u4e92\uff0c\u540e\u8005\u57fa\u4e8e\u793e\u4ea4\u73af\u5883\u81ea\u9002\u5e94\u5730\u9009\u62e9\u4ea4\u4e92\u4e13\u5bb6\u3002", "result": "\u5728ETH/UCY\u3001NBA\u548cSDD\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cViTE\u59cb\u7ec8\u53d6\u5f97\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "ViTE\u7684\u6709\u6548\u6027\u548c\u5b9e\u9645\u6548\u7387\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2511.12381", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12381", "abs": "https://arxiv.org/abs/2511.12381", "authors": ["Logan Mann", "Nayan Saxena", "Sarah Tandon", "Chenhao Sun", "Savar Toteja", "Kevin Zhu"], "title": "Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load", "comment": null, "summary": "Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \\textbf{(1) Load \\& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \\textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6267\u884c\u5426\u5b9a\u6307\u4ee4\u65f6\uff0c\u4e5f\u4f1a\u51fa\u73b0\u4e0e\u4eba\u7c7b\u7c7b\u4f3c\u7684\u201c\u8bbd\u523a\u6027\u53cd\u5f39\u201d\u73b0\u8c61\uff0c\u5373\u8bd5\u56fe\u6291\u5236\u67d0\u4e2a\u6982\u5ff5\u53cd\u800c\u4f1a\u4f7f\u5176\u66f4\u5bb9\u6613\u88ab\u6fc0\u6d3b\u3002", "motivation": "\u7814\u7a76LLM\u5728\u6267\u884c\u5426\u5b9a\u6307\u4ee4\u65f6\u662f\u5426\u4ee5\u53ca\u5982\u4f55\u51fa\u73b0\u201c\u8bbd\u523a\u6027\u53cd\u5f39\u201d\u73b0\u8c61\u3002\u5426\u5b9a\u6307\u4ee4\u4f1a\u589e\u52a0\u88ab\u5426\u5b9a\u6982\u5ff5\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u8fd9\u5bf9\u4e8eLLM\u6765\u8bf4\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\u6765\u7814\u7a76\u8fd9\u4e2a\u95ee\u9898\uff1a(1) \u6539\u53d8\u5e72\u6270\u6587\u672c\u7684\u7c7b\u578b\uff08\u8bed\u4e49\u3001\u53e5\u6cd5\u3001\u91cd\u590d\uff09\uff0c\u89c2\u5bdf\u53cd\u5f39\u5f3a\u5ea6\uff1b(2) \u6d4b\u8bd5\u6a21\u578b\u662f\u5426\u80fd\u533a\u5206\u540c\u4e00\u6982\u5ff5\u7684\u4e2d\u6027\u548c\u5426\u5b9a\u8868\u8fbe\uff0c\u4ee5\u53ca\u8fd9\u79cd\u533a\u5206\u662f\u5426\u4e0e\u53cd\u5f39\u7684\u6301\u4e45\u6027\u76f8\u5173\u3002", "result": "\u5426\u5b9a\u6307\u4ee4\u540e\u7acb\u5373\u51fa\u73b0\u53cd\u5f39\u73b0\u8c61\uff0c\u5e76\u4e14\u968f\u7740\u5e72\u6270\u6587\u672c\u957f\u5ea6\u7684\u589e\u52a0\u6216\u8bed\u4e49\u76f8\u5173\u6027\u7684\u589e\u5f3a\u800c\u52a0\u5267\uff0c\u91cd\u590d\u6027\u7684\u5e72\u6270\u6587\u672c\u5219\u6709\u52a9\u4e8e\u6291\u5236\u3002\u66f4\u5f3a\u7684\u6781\u6027\u5206\u79bb\u4e0e\u66f4\u6301\u4e45\u7684\u53cd\u5f39\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5c06\u201c\u8bbd\u523a\u6027\u53cd\u5f39\u201d\u7684\u8ba4\u77e5\u9884\u6d4b\u4e0e\u5bf9\u957f\u4e0a\u4e0b\u6587\u5e72\u6270\u7684\u673a\u5236\u6027\u7406\u89e3\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u901a\u8fc7\u7535\u8def\u8ffd\u8e2a\u5206\u6790\uff0c\u8bc6\u522b\u51fa\u653e\u5927\u7981\u6b62\u4ee4\u724c\u7684\u7a00\u758f\u4e2d\u95f4\u5c42\u6ce8\u610f\u529b\u5934\uff0c\u4ee5\u53ca\u6291\u5236\u65e9\u671f\u5c42\u3002\u8fd8\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b5000\u4e2a\u5426\u5b9a\u63d0\u793a\u7684\u6570\u636e\u96c6ReboundBench\uff0c\u4ee5\u652f\u6301\u672a\u6765\u7684\u7814\u7a76\u3002"}}
{"id": "2511.11989", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.11989", "abs": "https://arxiv.org/abs/2511.11989", "authors": ["Songsong Zhang", "Chuanqi Tang", "Hongguang Zhang", "Guijian Tang", "Minglong Li", "Xueqiong Li", "Shaowu Yang", "Yuanxi Peng", "Wenjing Yang", "Jing Zhao"], "title": "BeyondFacial: Identity-Preserving Personalized Generation Beyond Facial Close-ups", "comment": "9 pages, 10 figures", "summary": "Identity-Preserving Personalized Generation (IPPG) has advanced film production and artistic creation, yet existing approaches overemphasize facial regions, resulting in outputs dominated by facial close-ups.These methods suffer from weak visual narrativity and poor semantic consistency under complex text prompts, with the core limitation rooted in identity (ID) feature embeddings undermining the semantic expressiveness of generative models. To address these issues, this paper presents an IPPG method that breaks the constraint of facial close-ups, achieving synergistic optimization of identity fidelity and scene semantic creation. Specifically, we design a Dual-Line Inference (DLI) pipeline with identity-semantic separation, resolving the representation conflict between ID and semantics inherent in traditional single-path architectures. Further, we propose an Identity Adaptive Fusion (IdAF) strategy that defers ID-semantic fusion to the noise prediction stage, integrating adaptive attention fusion and noise decision masking to avoid ID embedding interference on semantics without manual masking. Finally, an Identity Aggregation Prepending (IdAP) module is introduced to aggregate ID information and replace random initializations, further enhancing identity preservation. Experimental results validate that our method achieves stable and effective performance in IPPG tasks beyond facial close-ups, enabling efficient generation without manual masking or fine-tuning. As a plug-and-play component, it can be rapidly deployed in existing IPPG frameworks, addressing the over-reliance on facial close-ups, facilitating film-level character-scene creation, and providing richer personalized generation capabilities for related domains.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8eab\u4efd\u4fdd\u6301\u4e2a\u6027\u5316\u751f\u6210\uff08IPPG\uff09\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u8fc7\u5ea6\u5173\u6ce8\u9762\u90e8\u533a\u57df\u3001\u89c6\u89c9\u53d9\u4e8b\u6027\u5f31\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709IPPG\u65b9\u6cd5\u8fc7\u5ea6\u5f3a\u8c03\u9762\u90e8\u533a\u57df\uff0c\u5bfc\u81f4\u89c6\u89c9\u53d9\u4e8b\u6027\u5f31\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u5dee\uff0c\u5176\u6838\u5fc3\u9650\u5236\u5728\u4e8e\u8eab\u4efd\uff08ID\uff09\u7279\u5f81\u5d4c\u5165\u7834\u574f\u4e86\u751f\u6210\u6a21\u578b\u7684\u8bed\u4e49\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u53cc\u7ebf\u63a8\u7406\uff08DLI\uff09\u7ba1\u9053\uff0c\u5b9e\u73b0\u8eab\u4efd-\u8bed\u4e49\u5206\u79bb\uff0c\u5e76\u63d0\u51fa\u8eab\u4efd\u81ea\u9002\u5e94\u878d\u5408\uff08IdAF\uff09\u7b56\u7565\uff0c\u5c06ID-\u8bed\u4e49\u878d\u5408\u63a8\u8fdf\u5230\u566a\u58f0\u9884\u6d4b\u9636\u6bb5\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u8eab\u4efd\u805a\u5408\u524d\u7f6e\uff08IdAP\uff09\u6a21\u5757\uff0c\u4ee5\u805a\u5408ID\u4fe1\u606f\u5e76\u66ff\u6362\u968f\u673a\u521d\u59cb\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8d85\u51fa\u9762\u90e8\u7279\u5199\u7684IPPG\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u7a33\u5b9a\u6709\u6548\u7684\u6027\u80fd\uff0c\u65e0\u9700\u624b\u52a8masking\u6216\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u751f\u6210\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f5c\u4e3a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7ec4\u4ef6\uff0c\u53ef\u4ee5\u5feb\u901f\u90e8\u7f72\u5728\u73b0\u6709\u7684IPPG\u6846\u67b6\u4e2d\uff0c\u89e3\u51b3\u8fc7\u5ea6\u4f9d\u8d56\u9762\u90e8\u7279\u5199\u7684\u95ee\u9898\uff0c\u4fc3\u8fdb\u7535\u5f71\u7ea7\u522b\u7684\u89d2\u8272\u573a\u666f\u521b\u5efa\uff0c\u5e76\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u4e2a\u6027\u5316\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2511.11650", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11650", "abs": "https://arxiv.org/abs/2511.11650", "authors": ["Daniele Ugo Leonzio", "Paolo Bestagini", "Marco Marcon", "Stefano Tubaro"], "title": "Enhanced Water Leak Detection with Convolutional Neural Networks and One-Class Support Vector Machine", "comment": null, "summary": "Water is a critical resource that must be managed efficiently. However, a substantial amount of water is lost each year due to leaks in Water Distribution Networks (WDNs). This underscores the need for reliable and effective leak detection and localization systems. In recent years, various solutions have been proposed, with data-driven approaches gaining increasing attention due to their superior performance. In this paper, we propose a new method for leak detection. The method is based on water pressure measurements acquired at a series of nodes of a WDN. Our technique is a fully data-driven solution that makes only use of the knowledge of the WDN topology, and a series of pressure data acquisitions obtained in absence of leaks. The proposed solution is based on an feature extractor and a one-class Support Vector Machines (SVM) trained on no-leak data, so that leaks are detected as anomalies. The results achieved on a simulate dataset using the Modena WDN demonstrate that the proposed solution outperforms recent methods for leak detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u6f0f\u6c34\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ec5\u5229\u7528\u6c34\u5206\u5e03\u7f51\u7edc\uff08WDN\uff09\u7684\u62d3\u6251\u7ed3\u6784\u548c\u65e0\u6cc4\u6f0f\u60c5\u51b5\u4e0b\u7684\u538b\u529b\u6570\u636e\uff0c\u901a\u8fc7\u7279\u5f81\u63d0\u53d6\u5668\u548c\u5355\u7c7b\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u5c06\u6cc4\u6f0f\u68c0\u6d4b\u4e3a\u5f02\u5e38\u3002", "motivation": "\u89e3\u51b3\u6c34\u5206\u5e03\u7f51\u7edc\uff08WDN\uff09\u4e2d\u56e0\u6cc4\u6f0f\u5bfc\u81f4\u5927\u91cf\u6c34\u8d44\u6e90\u635f\u5931\u7684\u95ee\u9898\uff0c\u9700\u8981\u53ef\u9760\u6709\u6548\u7684\u6cc4\u6f0f\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7cfb\u7edf\u3002\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u56e0\u5176\u5353\u8d8a\u7684\u6027\u80fd\u800c\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u6570\u636e\u9a71\u52a8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u4ec5\u4f7f\u7528WDN\u62d3\u6251\u7ed3\u6784\u7684\u77e5\u8bc6\u548c\u5728\u6ca1\u6709\u6cc4\u6f0f\u7684\u60c5\u51b5\u4e0b\u83b7\u5f97\u7684\u4e00\u7cfb\u5217\u538b\u529b\u6570\u636e\u91c7\u96c6\u3002\u8be5\u89e3\u51b3\u65b9\u6848\u57fa\u4e8e\u7279\u5f81\u63d0\u53d6\u5668\u548c\u5728\u65e0\u6cc4\u6f0f\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u5355\u7c7b\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\uff0c\u4ee5\u4fbf\u5c06\u6cc4\u6f0f\u68c0\u6d4b\u4e3a\u5f02\u5e38\u3002", "result": "\u5728Modena WDN\u7684\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u7684\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u4f18\u4e8e\u6700\u8fd1\u7684\u6cc4\u6f0f\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u9a71\u52a8\u7684\u6cc4\u6f0f\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u5728\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u6cc4\u6f0f\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2511.13189", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.13189", "abs": "https://arxiv.org/abs/2511.13189", "authors": ["Diego Ortego", "Marlon Rodr\u00edguez", "Mario Almagro", "Kunal Dahiya", "David Jim\u00e9nez", "Juan C. SanMiguel"], "title": "Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework", "comment": "To appear at AAAI 2026", "summary": "Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5728\u6781\u9650\u591a\u6807\u7b7e\u5206\u7c7b\uff08XMC\uff09\u4e2d\u5229\u7528\u5927\u578bdecoder-only\u6a21\u578b\u548c\u89c6\u89c9\u4fe1\u606f\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u6027\u80fd\u3002\u63d0\u51fa\u4e86ViXML\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709XMC\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5c0f\u578bencoder-only transformer\u67b6\u6784\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u5927\u578bdecoder-only\u6a21\u578b\u548c\u89c6\u89c9\u4fe1\u606f\u3002", "method": "1. \u6709\u6548\u5229\u7528\u5927\u578bdecoder-only\u6a21\u578b\u3002\n2. \u901a\u8fc7pooling\u56fe\u50cf\u7684\u5355\u4e2a\u5d4c\u5165\u6765\u96c6\u6210\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u63d0\u51faVision-enhanced eXtreme Multi-label Learning\u6846\u67b6\uff08ViXML\uff09\u3002", "result": "1. \u5c11\u91cf\u53c2\u6570\u7684decoder\u6a21\u578b\u5373\u53ef\u5e26\u6765\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u5f00\u9500\u53ef\u63a7\u3002\n2. ViXML\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8etext-only decoder\uff0c\u8868\u660e\u56fe\u50cf\u4fe1\u606f\u7684\u91cd\u8981\u6027\u3002\n3. \u5728\u56db\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\uff0cViXML\u7684\u6027\u80fd\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\u6c34\u5e73\uff0c\u5728\u6700\u5927\u6570\u636e\u96c6\u4e0a\u7684P@1\u6307\u6807\u4e0a\u63d0\u5347\u9ad8\u8fbe+8.21%\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u5730\u5229\u7528\u4e86\u5927\u578bdecoder-only\u6a21\u578b\u548c\u89c6\u89c9\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86XMC\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2511.12239", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12239", "abs": "https://arxiv.org/abs/2511.12239", "authors": ["Tarun Gupta", "Danish Pruthi"], "title": "Beyond World Models: Rethinking Understanding in AI Models", "comment": "Accepted to AAAI 2026 (Main Track)", "summary": "World models have garnered substantial interest in the AI community. These are internal representations that simulate aspects of the external world, track entities and states, capture causal relationships, and enable prediction of consequences. This contrasts with representations based solely on statistical correlations. A key motivation behind this research direction is that humans possess such mental world models, and finding evidence of similar representations in AI models might indicate that these models \"understand\" the world in a human-like way. In this paper, we use case studies from the philosophy of science literature to critically examine whether the world model framework adequately characterizes human-level understanding. We focus on specific philosophical analyses where the distinction between world model capabilities and human understanding is most pronounced. While these represent particular views of understanding rather than universal definitions, they help us explore the limits of world models.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86AI\u4e2d\u7684\u4e16\u754c\u6a21\u578b\uff0c\u5b83\u6a21\u62df\u5916\u90e8\u4e16\u754c\u3001\u8ffd\u8e2a\u5b9e\u4f53\u548c\u72b6\u6001\u3001\u6355\u6349\u56e0\u679c\u5173\u7cfb\u5e76\u9884\u6d4b\u7ed3\u679c\u3002\u8fd9\u4e0e\u4ec5\u57fa\u4e8e\u7edf\u8ba1\u76f8\u5173\u6027\u7684\u8868\u5f81\u5f62\u6210\u5bf9\u6bd4\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4eba\u7c7b\u62e5\u6709\u5fc3\u7406\u4e16\u754c\u6a21\u578b\uff0c\u5728AI\u6a21\u578b\u4e2d\u5bfb\u627e\u7c7b\u4f3c\u8868\u5f81\u53ef\u80fd\u8868\u660e\u8fd9\u4e9b\u6a21\u578b\u4ee5\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u7684\u65b9\u5f0f\u201c\u7406\u89e3\u201d\u4e16\u754c\u3002", "method": "\u672c\u6587\u91c7\u7528\u79d1\u5b66\u54f2\u5b66\u6587\u732e\u4e2d\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u6279\u5224\u6027\u5730\u68c0\u9a8c\u4e86\u4e16\u754c\u6a21\u578b\u6846\u67b6\u662f\u5426\u5145\u5206\u8868\u5f81\u4e86\u4eba\u7c7b\u6c34\u5e73\u7684\u7406\u89e3\u3002", "result": "\u8bba\u6587\u4fa7\u91cd\u4e8e\u4e16\u754c\u6a21\u578b\u80fd\u529b\u4e0e\u4eba\u7c7b\u7406\u89e3\u4e4b\u95f4\u533a\u522b\u6700\u660e\u663e\u7684\u7279\u5b9a\u54f2\u5b66\u5206\u6790\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u867d\u4ee3\u8868\u7279\u5b9a\u89c2\u70b9\u800c\u975e\u666e\u904d\u5b9a\u4e49\uff0c\u4f46\u6709\u52a9\u4e8e\u63a2\u7d22\u4e16\u754c\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.12387", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12387", "abs": "https://arxiv.org/abs/2511.12387", "authors": ["Jeyarajalingam Varsha", "Menan Velayuthan", "Sumirtha Karunakaran", "Rasan Nivethiga", "Kengatharaiyer Sarveswaran"], "title": "From Phonemes to Meaning: Evaluating Large Language Models on Tamil", "comment": "11 pages", "summary": "Large Language Models (LLMs) have shown strong generalization across tasks in high-resource languages; however, their linguistic competence in low-resource and morphologically rich languages such as Tamil remains largely unexplored. Existing multilingual benchmarks often rely on translated English datasets, failing to capture the linguistic and cultural nuances of the target language. To address this gap, we introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark manually curated using 820 questions from Sri Lankan school-level Tamil subject examination papers. Each question is annotated by trained linguists under five linguistic categories and a factual knowledge category, spanning Grades 1--13 to ensure broad linguistic coverage. We evaluate both closed-source and open-source LLMs using a standardized evaluation framework. Our results show that Gemini 2.5 achieves the highest overall performance, while open-source models lag behind, highlighting the gap in linguistic grounding. Category- and grade-wise analyses reveal that all models perform well on lower-grade questions but show a clear decline as linguistic complexity increases. Further, no strong correlation is observed between a model's overall performance and its ability to identify linguistic categories, suggesting that performance may be driven by exposure rather than genuine understanding.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ILAKKANAM\uff0c\u8fd9\u662f\u4e00\u4e2a\u6cf0\u7c73\u5c14\u8bed\u7684\u8bed\u8a00\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u5728\u8be5\u8bed\u8a00\u4e2d\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u7ffb\u8bd1\u7684\u82f1\u8bed\u6570\u636e\u96c6\uff0c\u65e0\u6cd5\u6355\u6349\u76ee\u6807\u8bed\u8a00\u7684\u8bed\u8a00\u548c\u6587\u5316\u7ec6\u5fae\u5dee\u522b\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u4f4e\u8d44\u6e90\u3001\u5f62\u6001\u4e30\u5bcc\u7684\u6cf0\u7c73\u5c14\u8bed\u7684\u8bed\u8a00\u80fd\u529b\u5dee\u8ddd\u3002", "method": "\u672c\u6587\u4f7f\u7528\u65af\u91cc\u5170\u5361\u5b66\u6821\u6c34\u5e73\u6cf0\u7c73\u5c14\u8bed\u79d1\u76ee\u8003\u8bd5\u8bd5\u5377\u4e2d\u7684820\u4e2a\u95ee\u9898\uff0c\u624b\u52a8\u521b\u5efa\u4e86\u6cf0\u7c73\u5c14\u8bed\u7684\u8bed\u8a00\u8bc4\u4f30\u57fa\u51c6ILAKKANAM\u3002\u6bcf\u4e2a\u95ee\u9898\u90fd\u7531\u8bad\u7ec3\u6709\u7d20\u7684\u8bed\u8a00\u5b66\u5bb6\u5728\u4e94\u4e2a\u8bed\u8a00\u7c7b\u522b\u548c\u4e00\u4e2a\u4e8b\u5b9e\u77e5\u8bc6\u7c7b\u522b\u4e0b\u8fdb\u884c\u6ce8\u91ca\uff0c\u8de8\u8d8a1-13\u5e74\u7ea7\uff0c\u4ee5\u786e\u4fdd\u5e7f\u6cdb\u7684\u8bed\u8a00\u8986\u76d6\u3002", "result": "Gemini 2.5\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u603b\u4f53\u6027\u80fd\uff0c\u800c\u5f00\u6e90\u6a21\u578b\u5219\u843d\u540e\uff0c\u7a81\u51fa\u4e86\u8bed\u8a00\u57fa\u7840\u65b9\u9762\u7684\u5dee\u8ddd\u3002\u6240\u6709\u6a21\u578b\u5728\u8f83\u4f4e\u5e74\u7ea7\u7684\u95ee\u9898\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u968f\u7740\u8bed\u8a00\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u8868\u73b0\u660e\u663e\u4e0b\u964d\u3002\u6a21\u578b\u7684\u603b\u4f53\u6027\u80fd\u4e0e\u5176\u8bc6\u522b\u8bed\u8a00\u7c7b\u522b\u7684\u80fd\u529b\u4e4b\u95f4\u6ca1\u6709\u89c2\u5bdf\u5230\u5f88\u5f3a\u7684\u76f8\u5173\u6027\uff0c\u8fd9\u8868\u660e\u6027\u80fd\u53ef\u80fd\u7531\u66b4\u9732\u9a71\u52a8\uff0c\u800c\u4e0d\u662f\u771f\u6b63\u7684\u7406\u89e3\u3002", "conclusion": "\u672c\u6587\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6cf0\u7c73\u5c14\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u8bed\u8a00\u80fd\u529b\u4ecd\u6709\u5f85\u63d0\u9ad8\u3002"}}
{"id": "2511.11993", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11993", "abs": "https://arxiv.org/abs/2511.11993", "authors": ["Jiaming Liang", "Chi-Man Pun"], "title": "Dynamic Parameter Optimization for Highly Transferable Transformation-Based Attacks", "comment": null, "summary": "Despite their wide application, the vulnerabilities of deep neural networks raise societal concerns. Among them, transformation-based attacks have demonstrated notable success in transfer attacks. However, existing attacks suffer from blind spots in parameter optimization, limiting their full potential. Specifically, (1) prior work generally considers low-iteration settings, yet attacks perform quite differently at higher iterations, so characterizing overall performance based only on low-iteration results is misleading. (2) Existing attacks use uniform parameters for different surrogate models, iterations, and tasks, which greatly impairs transferability. (3) Traditional transformation parameter optimization relies on grid search. For n parameters with m steps each, the complexity is O(mn). Large computational overhead limits further optimization of parameters. To address these limitations, we conduct an empirical study with various transformations as baselines, revealing three dynamic patterns of transferability with respect to parameter strength. We further propose a novel Concentric Decay Model (CDM) to effectively explain these patterns. Building on these insights, we propose an efficient Dynamic Parameter Optimization (DPO) based on the rise-then-fall pattern, reducing the complexity to O(nlogm). Comprehensive experiments on existing transformation-based attacks across different surrogate models, iterations, and tasks demonstrate that our DPO can significantly improve transferability.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u52a8\u6001\u53c2\u6570\u4f18\u5316\uff08DPO\uff09\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u57fa\u4e8e\u8f6c\u6362\u7684\u5bf9\u6297\u653b\u51fb\u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u53ef\u8fc1\u79fb\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8f6c\u6362\u7684\u653b\u51fb\u5b58\u5728\u53c2\u6570\u4f18\u5316\u76f2\u70b9\uff0c\u5305\u62ec\u4f4e\u8fed\u4ee3\u8bbe\u7f6e\u7684\u5c40\u9650\u6027\u3001\u5bf9\u4e0d\u540c\u4ee3\u7406\u6a21\u578b\u4f7f\u7528\u7edf\u4e00\u53c2\u6570\u4ee5\u53ca\u4f20\u7edf\u7f51\u683c\u641c\u7d22\u7684\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "method": "\u8be5\u7814\u7a76\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u63ed\u793a\u4e86\u8f6c\u6362\u5f3a\u5ea6\u4e0e\u53ef\u8fc1\u79fb\u6027\u4e4b\u95f4\u7684\u4e09\u79cd\u52a8\u6001\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u540c\u5fc3\u8870\u51cf\u6a21\u578b\uff08CDM\uff09\u6765\u89e3\u91ca\u8fd9\u4e9b\u6a21\u5f0f\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5148\u4e0a\u5347\u540e\u4e0b\u964d\u6a21\u5f0f\u7684\u52a8\u6001\u53c2\u6570\u4f18\u5316\uff08DPO\uff09\u65b9\u6cd5\u3002", "result": "\u5728\u4e0d\u540c\u7684\u4ee3\u7406\u6a21\u578b\u3001\u8fed\u4ee3\u548c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDPO \u80fd\u591f\u663e\u8457\u63d0\u9ad8\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684DPO\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u4e8e\u8f6c\u6362\u7684\u653b\u51fb\u7684\u53c2\u6570\u4f18\u5316\u76f2\u70b9\u95ee\u9898\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u6297\u6837\u672c\u7684\u53ef\u8fc1\u79fb\u6027\u3002"}}
{"id": "2511.11651", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11651", "abs": "https://arxiv.org/abs/2511.11651", "authors": ["Zhijian Gong", "Wenjia Dong", "Xueyuan Xu", "Fulin Wei", "Chunyu Liu", "Li Zhuo"], "title": "Incomplete Depression Feature Selection with Missing EEG Channels", "comment": null, "summary": "As a critical mental health disorder, depression has severe effects on both human physical and mental well-being. Recent developments in EEG-based depression analysis have shown promise in improving depression detection accuracies. However, EEG features often contain redundant, irrelevant, and noisy information. Additionally, real-world EEG data acquisition frequently faces challenges, such as data loss from electrode detachment and heavy noise interference. To tackle the challenges, we propose a novel feature selection approach for robust depression analysis, called Incomplete Depression Feature Selection with Missing EEG Channels (IDFS-MEC). IDFS-MEC integrates missing-channel indicator information and adaptive channel weighting learning into orthogonal regression to lessen the effects of incomplete channels on model construction, and then utilizes global redundancy minimization learning to reduce redundant information among selected feature subsets. Extensive experiments conducted on MODMA and PRED-d003 datasets reveal that the EEG feature subsets chosen by IDFS-MEC have superior performance than 10 popular feature selection methods among 3-, 64-, and 128-channel settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0c\u7528\u4e8e\u7a33\u5065\u7684\u6291\u90c1\u75c7\u5206\u6790\uff0c\u79f0\u4e3a\u4e0d\u5b8c\u6574\u6291\u90c1\u75c7\u7279\u5f81\u9009\u62e9\u4e0e\u7f3a\u5931\u8111\u7535\u901a\u9053\uff08IDFS-MEC\uff09\u3002", "motivation": "\u8111\u7535\u7279\u5f81\u901a\u5e38\u5305\u542b\u5197\u4f59\u3001\u4e0d\u76f8\u5173\u548c\u566a\u58f0\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u73b0\u5b9e\u4e16\u754c\u7684\u8111\u7535\u6570\u636e\u91c7\u96c6\u7ecf\u5e38\u9762\u4e34\u6311\u6218\uff0c\u4f8b\u5982\u7535\u6781\u5206\u79bb\u5bfc\u81f4\u7684\u6570\u636e\u4e22\u5931\u548c\u4e25\u91cd\u7684\u566a\u58f0\u5e72\u6270\u3002", "method": "IDFS-MEC\u5c06\u7f3a\u5931\u901a\u9053\u6307\u793a\u5668\u4fe1\u606f\u548c\u81ea\u9002\u5e94\u901a\u9053\u52a0\u6743\u5b66\u4e60\u96c6\u6210\u5230\u6b63\u4ea4\u56de\u5f52\u4e2d\uff0c\u4ee5\u51cf\u5c11\u4e0d\u5b8c\u6574\u901a\u9053\u5bf9\u6a21\u578b\u6784\u5efa\u7684\u5f71\u54cd\uff0c\u7136\u540e\u5229\u7528\u5168\u5c40\u5197\u4f59\u6700\u5c0f\u5316\u5b66\u4e60\u6765\u51cf\u5c11\u6240\u9009\u7279\u5f81\u5b50\u96c6\u4e2d\u7684\u5197\u4f59\u4fe1\u606f\u3002", "result": "\u5728MODMA\u548cPRED-d003\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cIDFS-MEC\u9009\u62e9\u7684\u8111\u7535\u7279\u5f81\u5b50\u96c6\u57283\u901a\u9053\u300164\u901a\u9053\u548c128\u901a\u9053\u8bbe\u7f6e\u4e2d\u6bd410\u79cd\u6d41\u884c\u7684\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u5177\u6709\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "IDFS-MEC\u662f\u4e00\u79cd\u6709\u6548\u7684\u6291\u90c1\u75c7\u5206\u6790\u7279\u5f81\u9009\u62e9\u65b9\u6cd5"}}
{"id": "2511.12241", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12241", "abs": "https://arxiv.org/abs/2511.12241", "authors": ["Junhyuk Seo", "Hyeyoon Moon", "Kyu-Hwan Jung", "Namkee Oh", "Taerim Kim"], "title": "AURA: Development and Validation of an Augmented Unplanned Removal Alert System using Synthetic ICU Videos", "comment": "12 pages, 5 figures", "summary": "Unplanned extubation (UE) remains a critical patient safety concern in intensive care units (ICUs), often leading to severe complications or death. Real-time UE detection has been limited, largely due to the ethical and privacy challenges of obtaining annotated ICU video data. We propose Augmented Unplanned Removal Alert (AURA), a vision-based risk detection system developed and validated entirely on a fully synthetic video dataset. By leveraging text-to-video diffusion, we generated diverse and clinically realistic ICU scenarios capturing a range of patient behaviors and care contexts. The system applies pose estimation to identify two high-risk movement patterns: collision, defined as hand entry into spatial zones near airway tubes, and agitation, quantified by the velocity of tracked anatomical keypoints. Expert assessments confirmed the realism of the synthetic data, and performance evaluations showed high accuracy for collision detection and moderate performance for agitation recognition. This work demonstrates a novel pathway for developing privacy-preserving, reproducible patient safety monitoring systems with potential for deployment in intensive care settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAURA\u7684\u57fa\u4e8e\u89c6\u89c9\u7684\u98ce\u9669\u68c0\u6d4b\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u4f7f\u7528\u5b8c\u5168\u5408\u6210\u7684\u89c6\u9891\u6570\u636e\u96c6\u5f00\u53d1\u548c\u9a8c\u8bc1\uff0c\u7528\u4e8e\u68c0\u6d4bICU\u4e2d\u7684\u975e\u8ba1\u5212\u6027\u62d4\u7ba1\uff08UE\uff09\u3002", "motivation": "ICU\u4e2d\u7684\u975e\u8ba1\u5212\u6027\u62d4\u7ba1\uff08UE\uff09\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u7684\u60a3\u8005\u5b89\u5168\u95ee\u9898\uff0c\u4f46\u7531\u4e8e\u83b7\u53d6\u5e26\u6ce8\u91ca\u7684ICU\u89c6\u9891\u6570\u636e\u5b58\u5728\u4f26\u7406\u548c\u9690\u79c1\u6311\u6218\uff0c\u5b9e\u65f6UE\u68c0\u6d4b\u53d7\u5230\u9650\u5236\u3002", "method": "\u5229\u7528\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u751f\u6210\u591a\u6837\u4e14\u4e34\u5e8a\u771f\u5b9e\u7684ICU\u573a\u666f\uff0c\u6355\u6349\u4e00\u7cfb\u5217\u60a3\u8005\u884c\u4e3a\u548c\u62a4\u7406\u73af\u5883\u3002\u8be5\u7cfb\u7edf\u5e94\u7528\u59ff\u52bf\u4f30\u8ba1\u6765\u8bc6\u522b\u4e24\u79cd\u9ad8\u98ce\u9669\u8fd0\u52a8\u6a21\u5f0f\uff1a\u78b0\u649e\uff08\u5b9a\u4e49\u4e3a\u624b\u8fdb\u5165\u6c14\u9053\u7ba1\u9644\u8fd1\u7684\u533a\u57df\uff09\u548c\u8e81\u52a8\uff08\u901a\u8fc7\u8ddf\u8e2a\u7684\u89e3\u5256\u5173\u952e\u70b9\u7684\u901f\u5ea6\u6765\u91cf\u5316\uff09\u3002", "result": "\u4e13\u5bb6\u8bc4\u4f30\u8bc1\u5b9e\u4e86\u5408\u6210\u6570\u636e\u7684\u771f\u5b9e\u6027\uff0c\u6027\u80fd\u8bc4\u4f30\u8868\u660e\u78b0\u649e\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u5f88\u9ad8\uff0c\u800c\u8e81\u52a8\u8bc6\u522b\u7684\u6027\u80fd\u9002\u4e2d\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u4e00\u79cd\u65b0\u7684\u9014\u5f84\uff0c\u7528\u4e8e\u5f00\u53d1\u5177\u6709\u9690\u79c1\u4fdd\u62a4\u3001\u53ef\u91cd\u590d\u7684\u60a3\u8005\u5b89\u5168\u76d1\u6d4b\u7cfb\u7edf\uff0c\u5e76\u6709\u53ef\u80fd\u5728\u91cd\u75c7\u76d1\u62a4\u73af\u5883\u4e2d\u90e8\u7f72\u3002"}}
{"id": "2511.12464", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12464", "abs": "https://arxiv.org/abs/2511.12464", "authors": ["Chenglong Wang", "Yifu Huo", "Yang Gan", "Yongyu Mu", "Qiaozhi He", "Murun Yang", "Bei Li", "Chunliang Zhang", "Tongran Liu", "Anxiang Ma", "Zhengtao Yu", "Jingbo Zhu", "Tong Xiao"], "title": "Probing Preference Representations: A Multi-Dimensional Evaluation and Analysis Method for Reward Models", "comment": "Accepted by AAAI 2026", "summary": "Previous methods evaluate reward models by testing them on a fixed pairwise ranking test set, but they typically do not provide performance information on each preference dimension. In this work, we address the evaluation challenge of reward models by probing preference representations. To confirm the effectiveness of this evaluation method, we construct a Multi-dimensional Reward Model Benchmark (MRMBench), a collection of six probing tasks for different preference dimensions. We design it to favor and encourage reward models that better capture preferences across different dimensions. Furthermore, we introduce an analysis method, inference-time probing, which identifies the dimensions used during the reward prediction and enhances its interpretability. Through extensive experiments, we find that MRMBench strongly correlates with the alignment performance of large language models (LLMs), making it a reliable reference for developing advanced reward models. Our analysis of MRMBench evaluation results reveals that reward models often struggle to capture preferences across multiple dimensions, highlighting the potential of multi-objective optimization in reward modeling. Additionally, our findings show that the proposed inference-time probing method offers a reliable metric for assessing the confidence of reward predictions, which ultimately improves the alignment of LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u63a2\u6d4b\u504f\u597d\u8868\u5f81\u6765\u8bc4\u4f30\u5956\u52b1\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u7ef4\u5ea6\u5956\u52b1\u6a21\u578b\u57fa\u51c6\uff08MRMBench\uff09\u3002", "motivation": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9b\u6bcf\u4e2a\u504f\u597d\u7ef4\u5ea6\u7684\u6027\u80fd\u4fe1\u606f\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u5956\u52b1\u6a21\u578b\u9884\u6d4b\u504f\u597d\u80fd\u529b\u7684\u6df1\u5165\u7406\u89e3\u3002", "method": "1. \u6784\u5efa\u5305\u542b\u516d\u4e2a\u63a2\u6d4b\u4efb\u52a1\u7684MRMBench\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540c\u504f\u597d\u7ef4\u5ea6\u30022. \u63d0\u51fa\u4e00\u79cd\u63a8\u7406\u65f6\u63a2\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u5956\u52b1\u9884\u6d4b\u4e2d\u4f7f\u7528\u7684\u7ef4\u5ea6\u5e76\u589e\u5f3a\u5176\u53ef\u89e3\u91ca\u6027\u3002", "result": "MRMBench\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u6027\u80fd\u9ad8\u5ea6\u76f8\u5173\uff0c\u53ef\u4f5c\u4e3a\u5f00\u53d1\u9ad8\u7ea7\u5956\u52b1\u6a21\u578b\u7684\u53ef\u9760\u53c2\u8003\u3002\u5956\u52b1\u6a21\u578b\u5728\u6355\u83b7\u591a\u4e2a\u7ef4\u5ea6\u7684\u504f\u597d\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u63a8\u7406\u65f6\u63a2\u6d4b\u65b9\u6cd5\u80fd\u53ef\u9760\u8bc4\u4f30\u5956\u52b1\u9884\u6d4b\u7684\u7f6e\u4fe1\u5ea6\u3002", "conclusion": "MRMBench\u53ef\u4ee5\u6709\u6548\u8bc4\u4f30\u5956\u52b1\u6a21\u578b\uff0c\u5e76\u63ed\u793a\u4e86\u591a\u76ee\u6807\u4f18\u5316\u5728\u5956\u52b1\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\u3002\u63a8\u7406\u65f6\u63a2\u6d4b\u65b9\u6cd5\u80fd\u591f\u63d0\u9ad8LLM\u7684\u5bf9\u9f50\u6548\u679c\u3002"}}
{"id": "2511.12005", "categories": ["cs.CV", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.12005", "abs": "https://arxiv.org/abs/2511.12005", "authors": ["Xinyu He", "Botong Zhao", "Bingbing Li", "Shujing Lyu", "Jiwei Shen", "Yue Lu"], "title": "LithoSeg: A Coarse-to-Fine Framework for High-Precision Lithography Segmentation", "comment": null, "summary": "Accurate segmentation and measurement of lithography scanning electron microscope (SEM) images are crucial for ensuring precise process control, optimizing device performance, and advancing semiconductor manufacturing yield. Lithography segmentation requires pixel-level delineation of groove contours and consistent performance across diverse pattern geometries and process window. However, existing methods often lack the necessary precision and robustness, limiting their practical applicability. To overcome this challenge, we propose LithoSeg, a coarse-to-fine network tailored for lithography segmentation. In the coarse stage, we introduce a Human-in-the-Loop Bootstrapping scheme for the Segment Anything Model (SAM) to attain robustness with minimal supervision. In the subsequent fine stage, we recast 2D segmentation as 1D regression problem by sampling groove-normal profiles using the coarse mask and performing point-wise refinement with a lightweight MLP. LithoSeg outperforms previous approaches in both segmentation accuracy and metrology precision while requiring less supervision, offering promising prospects for real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 lithography SEM \u56fe\u50cf\u5206\u5272\u65b9\u6cd5 LithoSeg\uff0c\u8be5\u65b9\u6cd5\u5728\u5206\u5272\u7cbe\u5ea6\u548c\u6d4b\u91cf\u7cbe\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u6240\u9700\u7684\u76d1\u7763\u66f4\u5c11\u3002", "motivation": "\u73b0\u6709\u7684 lithography \u5206\u5272\u65b9\u6cd5\u7f3a\u4e4f\u5fc5\u8981\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5728\u7c97\u7565\u9636\u6bb5\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u4eba\u673a\u5faa\u73af\u5f15\u5bfc\u65b9\u6848\uff0c\u7528\u4e8e\u5206\u5272\u4efb\u4f55\u6a21\u578b (SAM)\uff0c\u4ee5\u5728\u6700\u5c0f\u76d1\u7763\u4e0b\u83b7\u5f97\u9c81\u68d2\u6027\u3002\u5728\u7cbe\u7ec6\u9636\u6bb5\uff0c\u6211\u4eec\u901a\u8fc7\u4f7f\u7528\u7c97\u63a9\u6a21\u91c7\u6837\u51f9\u69fd\u6cd5\u7ebf\u8f6e\u5ed3\u5e76\u5c06 2D \u5206\u5272\u91cd\u94f8\u4e3a 1D \u56de\u5f52\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u8f7b\u91cf\u7ea7 MLP \u6267\u884c\u70b9\u65b9\u5f0f\u7ec6\u5316\u3002", "result": "LithoSeg \u5728\u5206\u5272\u7cbe\u5ea6\u548c\u6d4b\u91cf\u7cbe\u5ea6\u4e0a\u5747\u4f18\u4e8e\u4ee5\u524d\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u6240\u9700\u7684\u76d1\u7763\u66f4\u5c11\u3002", "conclusion": "LithoSeg \u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u5e7f\u9614\u7684\u524d\u666f\u3002"}}
{"id": "2511.11652", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11652", "abs": "https://arxiv.org/abs/2511.11652", "authors": ["Marvin Plein", "Carsten F. Dormann", "Andreas Christen"], "title": "How many stations are sufficient? Exploring the effect of urban weather station density reduction on imputation accuracy of air temperature and humidity", "comment": null, "summary": "Urban weather station networks (WSNs) are widely used to monitor urban weather and climate patterns and aid urban planning. However, maintaining WSNs is expensive and labor-intensive. Here, we present a step-wise station removal procedure to thin an existing WSN in Freiburg, Germany, and analyze the ability of WSN subsets to reproduce air temperature and humidity patterns of the entire original WSN for a year following a simulated reduction of WSN density. We found that substantial reductions in station numbers after one year of full deployment are possible while retaining high predictive accuracy. A reduction from 42 to 4 stations, for instance, increased mean prediction RMSEs from 0.69 K to 0.83 K for air temperature and from 3.8% to 4.4% for relative humidity, corresponding to RMSE increases of only 20% and 16%, respectively. Predictive accuracy is worse for remote stations in forests than for stations in built-up or open settings, but consistently better than a state-of-the-art numerical urban land-surface model (Surface Urban Energy and Water Balance Scheme). Stations located at the edges between built-up and rural areas are most valuable when reconstructing city-wide climate characteristics. Our study demonstrates the potential of thinning WSNs to maximize the efficient allocation of financial and personnel-related resources in urban climate research.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9010\u6b65\u79fb\u9664\u6c14\u8c61\u7ad9\u7684\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u57ce\u5e02\u6c14\u8c61\u7ad9\u7f51\u7edc(WSN)\u7684\u5bc6\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u6c14\u6e29\u548c\u6e7f\u5ea6\u6a21\u5f0f\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u57ce\u5e02\u6c14\u8c61\u7ad9\u7f51\u7edc\u7ef4\u62a4\u6210\u672c\u9ad8\u3001\u52b3\u52a8\u5f3a\u5ea6\u5927\uff0c\u56e0\u6b64\u9700\u8981\u4f18\u5316\u73b0\u6709\u7f51\u7edc\u3002", "method": "\u901a\u8fc7\u9010\u6b65\u79fb\u9664\u5fb7\u56fd\u5f17\u83b1\u5821\u73b0\u6709WSN\u4e2d\u7684\u7ad9\u70b9\uff0c\u5e76\u5206\u6790WSN\u5b50\u96c6\u91cd\u73b0\u539f\u59cbWSN\u6c14\u6e29\u548c\u6e7f\u5ea6\u6a21\u5f0f\u7684\u80fd\u529b\u3002", "result": "\u5728\u4fdd\u6301\u8f83\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u5927\u5e45\u51cf\u5c11\u7ad9\u70b9\u6570\u91cf\u662f\u53ef\u884c\u7684\u3002\u4f8b\u5982\uff0c\u4ece42\u4e2a\u7ad9\u70b9\u51cf\u5c11\u52304\u4e2a\u7ad9\u70b9\uff0c\u6c14\u6e29\u548c\u76f8\u5bf9\u6e7f\u5ea6\u7684\u5747\u65b9\u6839\u8bef\u5dee(RMSE)\u4ec5\u5206\u522b\u589e\u52a020%\u548c16%\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u7cbe\u7b80WSN\u5177\u6709\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u57ce\u5e02\u6c14\u5019\u7814\u7a76\u4e2d\u8d22\u653f\u548c\u4eba\u4e8b\u76f8\u5173\u8d44\u6e90\u6709\u6548\u914d\u7f6e\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.12472", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12472", "abs": "https://arxiv.org/abs/2511.12472", "authors": ["Mengying Wang", "Chenhui Ma", "Ao Jiao", "Tuo Liang", "Pengjun Lu", "Shrinidhi Hegde", "Yu Yin", "Evren Gurkan-Cavusoglu", "Yinghui Wu"], "title": "Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing", "comment": "The 40th AAAI Conference on Artificial Intelligence (AAAI-26)", "summary": "Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel (\"serendipitious\") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSerenQA\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2d\u53d1\u73b0\u610f\u5916\u7b54\u6848\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u7cfb\u7edf\u901a\u5e38\u53ea\u8fd4\u56de\u76f8\u5173\u4f46\u53ef\u9884\u6d4b\u7684\u7b54\u6848\uff0c\u7f3a\u4e4f\u53d1\u73b0\u65b0\u9896\u7b54\u6848\u7684\u80fd\u529b\u3002", "method": "\u8bba\u6587\u5b9a\u4e49\u4e86\u9762\u5411\u610f\u5916\u53d1\u73b0\u7684\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86SerenQA\u6846\u67b6\uff0c\u5305\u542b\u4e25\u8c28\u7684\u610f\u5916\u53d1\u73b0\u6307\u6807\u548c\u4e13\u5bb6\u6807\u6ce8\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u7ed3\u6784\u5316\u7684\u8bc4\u4f30\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76ee\u524d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u68c0\u7d22\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8bc6\u522b\u771f\u6b63\u4ee4\u4eba\u60ca\u8bb6\u548c\u6709\u4ef7\u503c\u7684\u53d1\u73b0\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2d\u4ecd\u6709\u5f88\u5927\u7684\u63d0\u5347\u7a7a\u95f4\uff0c\u5c24\u5176\u662f\u5728\u53d1\u73b0\u610f\u5916\u548c\u65b0\u9896\u7684\u7b54\u6848\u65b9\u9762\u3002"}}
{"id": "2511.12006", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12006", "abs": "https://arxiv.org/abs/2511.12006", "authors": ["Kai-Wen K. Yang", "Andrew Bai", "Alexandra Bermudez", "Yunqi Hong", "Zoe Latham", "Iris Sloan", "Michael Liu", "Vishrut Goyal", "Cho-Jui Hsieh", "Neil Y. C. Lin"], "title": "Uncertainty-Guided Selective Adaptation Enables Cross-Platform Predictive Fluorescence Microscopy", "comment": null, "summary": "Deep learning is transforming microscopy, yet models often fail when applied to images from new instruments or acquisition settings. Conventional adversarial domain adaptation (ADDA) retrains entire networks, often disrupting learned semantic representations. Here, we overturn this paradigm by showing that adapting only the earliest convolutional layers, while freezing deeper layers, yields reliable transfer. Building on this principle, we introduce Subnetwork Image Translation ADDA with automatic depth selection (SIT-ADDA-Auto), a self-configuring framework that integrates shallow-layer adversarial alignment with predictive uncertainty to automatically select adaptation depth without target labels. We demonstrate robustness via multi-metric evaluation, blinded expert assessment, and uncertainty-depth ablations. Across exposure and illumination shifts, cross-instrument transfer, and multiple stains, SIT-ADDA improves reconstruction and downstream segmentation over full-encoder adaptation and non-adversarial baselines, with reduced drift of semantic features. Our results provide a design rule for label-free adaptation in microscopy and a recipe for field settings; the code is publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u914d\u7f6e\u9886\u57df\u81ea\u9002\u5e94\u6846\u67b6SIT-ADDA-Auto\uff0c\u7528\u4e8e\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5728\u663e\u5fae\u955c\u56fe\u50cf\u5904\u7406\u4e2d\uff0c\u6a21\u578b\u5728\u65b0\u8bbe\u5907\u6216\u91c7\u96c6\u8bbe\u7f6e\u4e0a\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5bf9\u6297\u9886\u57df\u81ea\u9002\u5e94(ADDA)\u91cd\u65b0\u8bad\u7ec3\u6574\u4e2a\u7f51\u7edc\uff0c\u7ecf\u5e38\u7834\u574f\u5b66\u4e60\u5230\u7684\u8bed\u4e49\u8868\u793a\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u4ec5\u8c03\u6574\u6700\u65e9\u7684\u5377\u79ef\u5c42\uff0c\u540c\u65f6\u51bb\u7ed3\u66f4\u6df1\u5c42\uff0c\u5e76\u5c06\u6d45\u5c42\u5bf9\u6297\u5bf9\u9f50\u4e0e\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u76f8\u7ed3\u5408\uff0c\u4ee5\u81ea\u52a8\u9009\u62e9\u81ea\u9002\u5e94\u6df1\u5ea6\uff0c\u65e0\u9700\u76ee\u6807\u6807\u7b7e\u3002", "result": "SIT-ADDA\u5728\u91cd\u5efa\u548c\u4e0b\u6e38\u5206\u5272\u65b9\u9762\uff0c\u4f18\u4e8e\u5168\u7f16\u7801\u5668\u81ea\u9002\u5e94\u548c\u975e\u5bf9\u6297\u57fa\u7ebf\uff0c\u5e76\u51cf\u5c11\u4e86\u8bed\u4e49\u7279\u5f81\u7684\u6f02\u79fb\u3002\u901a\u8fc7\u591a\u6307\u6807\u8bc4\u4f30\u3001\u76f2\u6cd5\u4e13\u5bb6\u8bc4\u4f30\u548c\u4e0d\u786e\u5b9a\u6027-\u6df1\u5ea6\u6d88\u878d\u8bc1\u660e\u4e86\u5176\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u663e\u5fae\u955c\u4e2d\u7684\u65e0\u6807\u7b7e\u81ea\u9002\u5e94\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8bbe\u8ba1\u89c4\u5219\uff0c\u5e76\u4e3a\u73b0\u573a\u8bbe\u7f6e\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b9\u6848\u3002"}}
{"id": "2511.11654", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.11654", "abs": "https://arxiv.org/abs/2511.11654", "authors": ["Sayambhu Sen", "Shalabh Bhatnagar"], "title": "Convergence of Multiagent Learning Systems for Traffic control", "comment": "14 pages 2 figures", "summary": "Rapid urbanization in cities like Bangalore has led to severe traffic congestion, making efficient Traffic Signal Control (TSC) essential. Multi-Agent Reinforcement Learning (MARL), often modeling each traffic signal as an independent agent using Q-learning, has emerged as a promising strategy to reduce average commuter delays. While prior work Prashant L A et. al has empirically demonstrated the effectiveness of this approach, a rigorous theoretical analysis of its stability and convergence properties in the context of traffic control has not been explored. This paper bridges that gap by focusing squarely on the theoretical basis of this multi-agent algorithm. We investigate the convergence problem inherent in using independent learners for the cooperative TSC task. Utilizing stochastic approximation methods, we formally analyze the learning dynamics. The primary contribution of this work is the proof that the specific multi-agent reinforcement learning algorithm for traffic control is proven to converge under the given conditions extending it from single agent convergence proofs for asynchronous value iteration.", "AI": {"tldr": "\u672c\u6587\u7740\u91cd\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60 (MARL) \u4ea4\u901a\u4fe1\u53f7\u63a7\u5236 (TSC) \u7b97\u6cd5\u7684\u7406\u8bba\u57fa\u7840\uff0c\u586b\u8865\u4e86\u5148\u524d\u5de5\u4f5c\u7684\u7a7a\u767d\u3002", "motivation": "\u57ce\u5e02\u4ea4\u901a\u62e5\u5835\u4e25\u91cd\uff0c\u9700\u8981\u9ad8\u6548\u7684\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u3002\u5148\u524d\u7684\u7814\u7a76\u5df2\u7ecf\u8bc1\u660e\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u662f\u4e00\u79cd\u6709\u524d\u9014\u7684\u7b56\u7565\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u7a33\u5b9a\u6027\u548c\u6536\u655b\u6027\u7684\u4e25\u683c\u7406\u8bba\u5206\u6790\u3002", "method": "\u5229\u7528\u968f\u673a\u903c\u8fd1\u65b9\u6cd5\uff0c\u6b63\u5f0f\u5206\u6790\u4e86\u5b66\u4e60\u52a8\u6001\u3002", "result": "\u8bc1\u660e\u4e86\u7528\u4e8e\u4ea4\u901a\u63a7\u5236\u7684\u7279\u5b9a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u7ed9\u5b9a\u6761\u4ef6\u4e0b\u6536\u655b\u3002", "conclusion": "\u8be5\u7814\u7a76\u6269\u5c55\u4e86\u5f02\u6b65\u503c\u8fed\u4ee3\u7684\u5355\u667a\u80fd\u4f53\u6536\u655b\u6027\u8bc1\u660e\u3002"}}
{"id": "2511.12271", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12271", "abs": "https://arxiv.org/abs/2511.12271", "authors": ["Zhiyu An", "Wan Du"], "title": "MoralReason: Generalizable Moral Decision Alignment For LLM Agents Using Reasoning-Level Reinforcement Learning", "comment": "Accepted for AAAI 2026", "summary": "Large language models are increasingly influencing human moral decisions, yet current approaches focus primarily on evaluating rather than actively steering their moral decisions. We formulate this as an out-of-distribution moral alignment problem, where LLM agents must learn to apply consistent moral reasoning frameworks to scenarios beyond their training distribution. We introduce Moral-Reason-QA, a novel dataset extending 680 human-annotated, high-ambiguity moral scenarios with framework-specific reasoning traces across utilitarian, deontological, and virtue ethics, enabling systematic evaluation of moral generalization in realistic decision contexts. Our learning approach employs Group Relative Policy Optimization with composite rewards that simultaneously optimize decision alignment and framework-specific reasoning processes to facilitate learning of the underlying moral frameworks. Experimental results demonstrate successful generalization to unseen moral scenarios, with softmax-normalized alignment scores improving by +0.757 for utilitarian and +0.450 for deontological frameworks when tested on out-of-distribution evaluation sets. The experiments also reveal training challenges and promising directions that inform future research. These findings establish that LLM agents can be systematically trained to internalize and apply specific moral frameworks to novel situations, providing a critical foundation for AI safety as language models become more integrated into human decision-making processes.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u5f71\u54cd\u4eba\u7c7b\u7684\u9053\u5fb7\u51b3\u7b56\uff0c\u4f46\u76ee\u524d\u7684\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u8bc4\u4f30\u800c\u4e0d\u662f\u79ef\u6781\u5f15\u5bfc\u4ed6\u4eec\u7684\u9053\u5fb7\u51b3\u7b56\u3002", "motivation": "\u5c06\u9053\u5fb7\u63a8\u7406\u6846\u67b6\u5e94\u7528\u4e8e\u8d85\u51fa\u5176\u8bad\u7ec3\u8303\u56f4\u7684\u60c5\u51b5\uff0c\u4ece\u800c\u89e3\u51b3\u9053\u5fb7\u504f\u5dee\u95ee\u9898\u3002", "method": "\u4f7f\u7528 Moral-Reason-QA \u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u529f\u5229\u4e3b\u4e49\u3001\u4e49\u52a1\u8bba\u548c\u7f8e\u5fb7\u4f26\u7406\u7684\u6846\u67b6\u7279\u5b9a\u63a8\u7406\u8ddf\u8e2a\u6269\u5c55\u4e86 680 \u4e2a\u4eba\u5de5\u6ce8\u91ca\u7684\u9ad8\u6b67\u4e49\u9053\u5fb7\u573a\u666f\uff0c\u4ece\u800c\u80fd\u591f\u5728\u5b9e\u9645\u51b3\u7b56\u73af\u5883\u4e2d\u7cfb\u7edf\u5730\u8bc4\u4f30\u9053\u5fb7\u6982\u62ec\u3002", "result": "\u5728\u672a\u89c1\u8fc7\u7684\u9053\u5fb7\u573a\u666f\u4e2d\u6210\u529f\u63a8\u5e7f\uff0c\u5728\u5206\u5e03\u5916\u8bc4\u4f30\u96c6\u4e0a\u6d4b\u8bd5\u65f6\uff0c\u529f\u5229\u4e3b\u4e49\u7684 softmax \u5f52\u4e00\u5316\u5bf9\u9f50\u5206\u6570\u63d0\u9ad8\u4e86 +0.757\uff0c\u4e49\u52a1\u8bba\u7684 softmax \u5f52\u4e00\u5316\u5bf9\u9f50\u5206\u6570\u63d0\u9ad8\u4e86 +0.450\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u7ecf\u8fc7\u7cfb\u7edf\u8bad\u7ec3\uff0c\u5c06\u7279\u5b9a\u7684\u9053\u5fb7\u6846\u67b6\u5185\u5316\u5e76\u5e94\u7528\u4e8e\u65b0\u7684\u60c5\u51b5\uff0c\u4e3a\u4eba\u5de5\u667a\u80fd\u5b89\u5168\u5960\u5b9a\u5173\u952e\u57fa\u7840\u3002"}}
{"id": "2511.12497", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.12497", "abs": "https://arxiv.org/abs/2511.12497", "authors": ["JoonHo Lee", "HyeonMin Cho", "Jaewoong Yun", "Hyunjae Lee", "JunKyu Lee", "Juree Seok"], "title": "SGuard-v1: Safety Guardrail for Large Language Models", "comment": "Technical Report", "summary": "We present SGuard-v1, a lightweight safety guardrail for Large Language Models (LLMs), which comprises two specialized models to detect harmful content and screen adversarial prompts in human-AI conversational settings. The first component, ContentFilter, is trained to identify safety risks in LLM prompts and responses in accordance with the MLCommons hazard taxonomy, a comprehensive framework for trust and safety assessment of AI. The second component, JailbreakFilter, is trained with a carefully designed curriculum over integrated datasets and findings from prior work on adversarial prompting, covering 60 major attack types while mitigating false-unsafe classification. SGuard-v1 is built on the 2B-parameter Granite-3.3-2B-Instruct model that supports 12 languages. We curate approximately 1.4 million training instances from both collected and synthesized data and perform instruction tuning on the base model, distributing the curated data across the two component according to their designated functions. Through extensive evaluation on public and proprietary safety benchmarks, SGuard-v1 achieves state-of-the-art safety performance while remaining lightweight, thereby reducing deployment overhead. SGuard-v1 also improves interpretability for downstream use by providing multi-class safety predictions and their binary confidence scores. We release the SGuard-v1 under the Apache-2.0 License to enable further research and practical deployment in AI safety.", "AI": {"tldr": "SGuard-v1\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684LLM\u5b89\u5168\u9632\u62a4\u7cfb\u7edf\uff0c\u5305\u542b\u4e24\u4e2a\u6a21\u578b\uff0c\u7528\u4e8e\u68c0\u6d4b\u6709\u5bb3\u5185\u5bb9\u548c\u7b5b\u9009\u5bf9\u6297\u6027\u63d0\u793a\u3002", "motivation": "\u5728\u4eba\u673a\u5bf9\u8bdd\u73af\u5883\u4e2d\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u53ef\u80fd\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u6709\u6548\u7684\u9632\u62a4\u673a\u5236\u3002", "method": "\u8be5\u7cfb\u7edf\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1aContentFilter\u7528\u4e8e\u8bc6\u522bLLM\u63d0\u793a\u548c\u54cd\u5e94\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff1bJailbreakFilter\u4f7f\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bfe\u7a0b\u8bad\u7ec3\uff0c\u4ee5\u8bc6\u522b\u5bf9\u6297\u6027\u63d0\u793a\u3002", "result": "SGuard-v1\u5728\u516c\u5171\u548c\u79c1\u6709\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5b89\u5168\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8f7b\u91cf\u7ea7\u3002", "conclusion": "SGuard-v1\u4ee5Apache-2.0\u8bb8\u53ef\u8bc1\u53d1\u5e03\uff0c\u4ee5\u4fc3\u8fdbAI\u5b89\u5168\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2511.12018", "categories": ["cs.CV", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.12018", "abs": "https://arxiv.org/abs/2511.12018", "authors": ["Shounak Ray Chaudhuri", "Arash Jahangiri", "Christopher Paolini"], "title": "Enhancing Road Safety Through Multi-Camera Image Segmentation with Post-Encroachment Time Analysis", "comment": "8 pages, 10 figures, Submitted to IEEE Intelligent Vehicles Symposium 2026", "summary": "Traffic safety analysis at signalized intersections is vital for reducing vehicle and pedestrian collisions, yet traditional crash-based studies are limited by data sparsity and latency. This paper presents a novel multi-camera computer vision framework for real-time safety assessment through Post-Encroachment Time (PET) computation, demonstrated at the intersection of H Street and Broadway in Chula Vista, California. Four synchronized cameras provide continuous visual coverage, with each frame processed on NVIDIA Jetson AGX Xavier devices using YOLOv11 segmentation for vehicle detection. Detected vehicle polygons are transformed into a unified bird's-eye map using homography matrices, enabling alignment across overlapping camera views. A novel pixel-level PET algorithm measures vehicle position without reliance on fixed cells, allowing fine-grained hazard visualization via dynamic heatmaps, accurate to 3.3 sq-cm. Timestamped vehicle and PET data is stored in an SQL database for long-term monitoring. Results over various time intervals demonstrate the framework's ability to identify high-risk regions with sub-second precision and real-time throughput on edge devices, producing data for an 800 x 800 pixel logarithmic heatmap at an average of 2.68 FPS. This study validates the feasibility of decentralized vision-based PET analysis for intelligent transportation systems, offering a replicable methodology for high-resolution, real-time, and scalable intersection safety evaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6444\u50cf\u5934\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u5b9e\u65f6\u4ea4\u901a\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u8ba1\u7b97\u540e\u4fb5\u5165\u65f6\u95f4\uff08PET\uff09\u6765\u8bc6\u522b\u9ad8\u98ce\u9669\u533a\u57df\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u4e8b\u6545\u7684\u7814\u7a76\u53d7\u5230\u6570\u636e\u7a00\u758f\u6027\u548c\u5ef6\u8fdf\u7684\u9650\u5236\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u8fdb\u884c\u5b9e\u65f6\u5b89\u5168\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528\u56db\u4e2a\u540c\u6b65\u6444\u50cf\u5934\uff0c\u901a\u8fc7 YOLOv11 \u5206\u5272\u8fdb\u884c\u8f66\u8f86\u68c0\u6d4b\uff0c\u5e76\u4f7f\u7528\u5355\u5e94\u6027\u77e9\u9635\u5c06\u68c0\u6d4b\u5230\u7684\u8f66\u8f86\u591a\u8fb9\u5f62\u8f6c\u6362\u4e3a\u7edf\u4e00\u7684\u9e1f\u77b0\u56fe\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u50cf\u7d20\u7ea7 PET \u7b97\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e9a\u79d2\u7ea7\u7cbe\u5ea6\u548c\u5b9e\u65f6\u541e\u5410\u91cf\u4e0b\u8bc6\u522b\u9ad8\u98ce\u9669\u533a\u57df\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u4ee5\u5e73\u5747 2.68 FPS \u7684\u901f\u5ea6\u751f\u6210 800 x 800 \u50cf\u7d20\u5bf9\u6570\u70ed\u56fe\uff0c\u4ece\u800c\u8bc6\u522b\u9ad8\u98ce\u9669\u533a\u57df\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u5206\u6563\u89c6\u89c9\u7684 PET \u5206\u6790\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u9ad8\u5206\u8fa8\u7387\u3001\u5b9e\u65f6\u548c\u53ef\u6269\u5c55\u7684\u4ea4\u53c9\u53e3\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u590d\u5236\u7684\u65b9\u6cd5\u3002"}}
{"id": "2511.11656", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11656", "abs": "https://arxiv.org/abs/2511.11656", "authors": ["Luca Marzari", "Manuele Bicego", "Ferdinando Cicalese", "Alessandro Farinelli"], "title": "On the Probabilistic Learnability of Compact Neural Network Preimage Bounds", "comment": "Accepted at the 40th Annual AAAI Conference on Artificial Intelligence 2026", "summary": "Although recent provable methods have been developed to compute preimage bounds for neural networks, their scalability is fundamentally limited by the #P-hardness of the problem. In this work, we adopt a novel probabilistic perspective, aiming to deliver solutions with high-confidence guarantees and bounded error. To this end, we investigate the potential of bootstrap-based and randomized approaches that are capable of capturing complex patterns in high-dimensional spaces, including input regions where a given output property holds. In detail, we introduce $\\textbf{R}$andom $\\textbf{F}$orest $\\textbf{Pro}$perty $\\textbf{Ve}$rifier ($\\texttt{RF-ProVe}$), a method that exploits an ensemble of randomized decision trees to generate candidate input regions satisfying a desired output property and refines them through active resampling. Our theoretical derivations offer formal statistical guarantees on region purity and global coverage, providing a practical, scalable solution for computing compact preimage approximations in cases where exact solvers fail to scale.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u68ee\u6797\u7684\u795e\u7ecf\u7f51\u7edc\u524d\u50cf\u8fd1\u4f3c\u8ba1\u7b97\u65b9\u6cd5\uff0c\u53ef\u5728\u4fdd\u8bc1\u9ad8\u7f6e\u4fe1\u5ea6\u548c\u6709\u754c\u8bef\u5dee\u7684\u60c5\u51b5\u4e0b\uff0c\u89e3\u51b3\u7cbe\u786e\u6c42\u89e3\u5668\u65e0\u6cd5\u6269\u5c55\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8ba1\u7b97\u795e\u7ecf\u7f51\u7edc\u524d\u50cf\u754c\u9650\u7684\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u4e0a\u5b58\u5728\u6839\u672c\u9650\u5236\uff0c\u56e0\u4e3a\u95ee\u9898\u662f#P-hard\u7684\u3002", "method": "\u5229\u7528\u968f\u673a\u51b3\u7b56\u6811\u96c6\u6210\u751f\u6210\u6ee1\u8db3\u671f\u671b\u8f93\u51fa\u5c5e\u6027\u7684\u5019\u9009\u8f93\u5165\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u4e3b\u52a8\u91cd\u91c7\u6837\u8fdb\u884c\u4f18\u5316\uff0c\u8be5\u65b9\u6cd5\u540d\u4e3aRF-ProVe\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u533a\u57df\u7eaf\u5ea6\u548c\u5168\u5c40\u8986\u76d6\u7387\u4e0a\u63d0\u4f9b\u6b63\u5f0f\u7684\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u4e3a\u8ba1\u7b97\u7d27\u51d1\u7684\u524d\u50cf\u8fd1\u4f3c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "RF-ProVe\u65b9\u6cd5\u5728\u7cbe\u786e\u6c42\u89e3\u5668\u5931\u6548\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u8ba1\u7b97\u7d27\u51d1\u7684\u524d\u50cf\u8fd1\u4f3c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63d0\u4f9b\u4e86\u6b63\u5f0f\u7684\u7edf\u8ba1\u4fdd\u8bc1\u3002"}}
{"id": "2511.12306", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.12306", "abs": "https://arxiv.org/abs/2511.12306", "authors": ["Darvin Yi", "Teng Liu", "Mattie Terzolo", "Lance Hasson", "Ayan Sinh", "Pablo Mendes", "Andrew Rabinovich"], "title": "UpBench: A Dynamically Evolving Real-World Labor-Market Agentic Benchmark Framework Built for Human-Centric AI", "comment": null, "summary": "As large language model (LLM) agents increasingly undertake digital work, reliable frameworks are needed to evaluate their real-world competence, adaptability, and capacity for human collaboration. Existing benchmarks remain largely static, synthetic, or domain-limited, providing limited insight into how agents perform in dynamic, economically meaningful environments. We introduce UpBench, a dynamically evolving benchmark grounded in real jobs drawn from the global Upwork labor marketplace. Each task corresponds to a verified client transaction, anchoring evaluation in genuine work activity and financial outcomes. UpBench employs a rubric-based evaluation framework, in which expert freelancers decompose each job into detailed, verifiable acceptance criteria and assess AI submissions with per-criterion feedback. This structure enables fine-grained analysis of model strengths, weaknesses, and instruction-following fidelity beyond binary pass/fail metrics. Human expertise is integrated throughout the data pipeline (from job curation and rubric construction to evaluation) ensuring fidelity to real professional standards and supporting research on human-AI collaboration. By regularly refreshing tasks to reflect the evolving nature of online work, UpBench provides a scalable, human-centered foundation for evaluating agentic systems in authentic labor-market contexts, offering a path toward a collaborative framework, where AI amplifies human capability through partnership rather than replacement.", "AI": {"tldr": "UpBench\u662f\u4e00\u4e2a\u52a8\u6001\u66f4\u65b0\u7684\u57fa\u51c6\uff0c\u5b83\u57fa\u4e8eUpwork\u7684\u771f\u5b9e\u5de5\u4f5c\uff0c\u7528\u4e8e\u8bc4\u4f30LLM agent\u5728\u771f\u5b9e\u5de5\u4f5c\u73af\u5883\u4e2d\u7684\u80fd\u529b\uff0c\u5305\u62ec\u80dc\u4efb\u529b\u3001\u9002\u5e94\u6027\u548c\u4eba\u673a\u534f\u4f5c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u5927\u591a\u662f\u9759\u6001\u7684\u3001\u5408\u6210\u7684\u6216\u9886\u57df\u53d7\u9650\u7684\uff0c\u65e0\u6cd5\u6df1\u5165\u4e86\u89e3agent\u5728\u52a8\u6001\u7684\u3001\u5177\u6709\u7ecf\u6d4e\u610f\u4e49\u7684\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "method": "UpBench\u4f7f\u7528\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4e13\u5bb6\u81ea\u7531\u804c\u4e1a\u8005\u5c06\u6bcf\u4e2a\u5de5\u4f5c\u5206\u89e3\u4e3a\u8be6\u7ec6\u7684\u3001\u53ef\u9a8c\u8bc1\u7684\u9a8c\u6536\u6807\u51c6\uff0c\u5e76\u6839\u636e\u6bcf\u4e2a\u6807\u51c6\u5bf9AI\u63d0\u4ea4\u7684\u5185\u5bb9\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "UpBench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u3001\u4ee5\u4eba\u4e3a\u672c\u7684\u57fa\u7840\uff0c\u7528\u4e8e\u5728\u771f\u5b9e\u7684\u52b3\u52a8\u529b\u5e02\u573a\u73af\u5883\u4e2d\u8bc4\u4f30agent\u7cfb\u7edf\u3002", "conclusion": "UpBench\u4e3a\u8bc4\u4f30LLM agent\u5728\u771f\u5b9e\u5de5\u4f5c\u573a\u666f\u4e2d\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u52a8\u6001\u3001\u4ee5\u4eba\u4e3a\u672c\u7684\u6846\u67b6\uff0c\u65e8\u5728\u4fc3\u8fdb\u4eba\u673a\u534f\u4f5c\uff0c\u4f7fAI\u80fd\u591f\u589e\u5f3a\u4eba\u7c7b\u7684\u80fd\u529b\u3002"}}
{"id": "2511.12504", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12504", "abs": "https://arxiv.org/abs/2511.12504", "authors": ["Maria Tseytlin", "Paul Roit", "Omri Abend", "Ido Dagan", "Ayal Klein"], "title": "QA-Noun: Representing Nominal Semantics via Natural Language Question-Answer Pairs", "comment": null, "summary": "Decomposing sentences into fine-grained meaning units is increasingly used to model semantic alignment. While QA-based semantic approaches have shown effectiveness for representing predicate-argument relations, they have so far left noun-centered semantics largely unaddressed. We introduce QA-Noun, a QA-based framework for capturing noun-centered semantic relations. QA-Noun defines nine question templates that cover both explicit syntactical and implicit contextual roles for nouns, producing interpretable QA pairs that complement verbal QA-SRL. We release detailed guidelines, a dataset of over 2,000 annotated noun mentions, and a trained model integrated with QA-SRL to yield a unified decomposition of sentence meaning into individual, highly fine-grained, facts. Evaluation shows that QA-Noun achieves near-complete coverage of AMR's noun arguments while surfacing additional contextually implied relations, and that combining QA-Noun with QA-SRL yields over 130\\% higher granularity than recent fact-based decomposition methods such as FactScore and DecompScore. QA-Noun thus complements the broader QA-based semantic framework, forming a comprehensive and scalable approach to fine-grained semantic decomposition for cross-text alignment.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aQA-Noun\u7684\u57fa\u4e8eQA\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6355\u83b7\u4ee5\u540d\u8bcd\u4e3a\u4e2d\u5fc3\u7684\u8bed\u4e49\u5173\u7cfb\uff0c\u901a\u8fc7\u5b9a\u4e49\u4e5d\u4e2a\u95ee\u9898\u6a21\u677f\u6765\u8986\u76d6\u540d\u8bcd\u7684\u663e\u5f0f\u53e5\u6cd5\u548c\u9690\u5f0f\u4e0a\u4e0b\u6587\u89d2\u8272\uff0c\u4ece\u800c\u751f\u6210\u53ef\u89e3\u91ca\u7684QA\u5bf9\uff0c\u4e0e verbal QA-SRL \u4e92\u8865\u3002\u8be5\u8bba\u6587\u53d1\u5e03\u4e86\u8be6\u7ec6\u7684\u6307\u5357\u3001\u4e00\u4e2a\u5305\u542b\u8d85\u8fc72000\u4e2a\u5e26\u6ce8\u91ca\u7684\u540d\u8bcd\u63d0\u53ca\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u4e00\u4e2a\u4e0eQA-SRL\u96c6\u6210\u7684\u8bad\u7ec3\u6a21\u578b\uff0c\u4ee5\u5b9e\u73b0\u53e5\u5b50\u542b\u4e49\u7684\u7edf\u4e00\u5206\u89e3\u4e3a\u5355\u72ec\u7684\u3001\u9ad8\u5ea6\u7ec6\u7c92\u5ea6\u7684 facts\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eQA\u7684\u8bed\u4e49\u65b9\u6cd5\u5728\u8868\u793a\u8c13\u8bcd-\u8bba\u5143\u5173\u7cfb\u65b9\u9762\u5df2\u7ecf\u663e\u793a\u51fa\u6709\u6548\u6027\uff0c\u4f46\u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u5b83\u4eec\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u6ca1\u6709\u89e3\u51b3\u4ee5\u540d\u8bcd\u4e3a\u4e2d\u5fc3\u7684\u8bed\u4e49\u95ee\u9898\u3002", "method": "\u8be5\u8bba\u6587\u5f15\u5165\u4e86QA-Noun\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8eQA\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6355\u83b7\u4ee5\u540d\u8bcd\u4e3a\u4e2d\u5fc3\u7684\u8bed\u4e49\u5173\u7cfb\u3002QA-Noun\u5b9a\u4e49\u4e86\u4e5d\u4e2a\u95ee\u9898\u6a21\u677f\uff0c\u6db5\u76d6\u540d\u8bcd\u7684\u663e\u5f0f\u53e5\u6cd5\u548c\u9690\u5f0f\u4e0a\u4e0b\u6587\u89d2\u8272\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u7684QA\u5bf9\uff0c\u4e0everbal QA-SRL\u4e92\u8865\u3002", "result": "QA-Noun\u5b9e\u73b0\u4e86\u5bf9AMR\u540d\u8bcd\u8bba\u5143\u7684\u8fd1\u4e4e\u5b8c\u6574\u7684\u8986\u76d6\uff0c\u540c\u65f6\u6d6e\u73b0\u4e86\u989d\u5916\u7684\u4e0a\u4e0b\u6587\u9690\u542b\u5173\u7cfb\u3002\u5c06QA-Noun\u4e0eQA-SRL\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u4ea7\u751f\u6bd4\u6700\u8fd1\u7684\u57fa\u4e8e\u4e8b\u5b9e\u7684\u5206\u89e3\u65b9\u6cd5\uff08\u5982FactScore\u548cDecompScore\uff09\u9ad8\u51fa130%\u4ee5\u4e0a\u7684\u7c92\u5ea6\u3002", "conclusion": "QA-Noun\u5b8c\u5584\u4e86\u66f4\u5e7f\u6cdb\u7684\u57fa\u4e8eQA\u7684\u8bed\u4e49\u6846\u67b6\uff0c\u5f62\u6210\u4e86\u4e00\u79cd\u5168\u9762\u7684\u3001\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u7684\u8bed\u4e49\u5206\u89e3\uff0c\u4ee5\u5b9e\u73b0\u8de8\u6587\u672c\u5bf9\u9f50\u3002"}}
{"id": "2511.12020", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12020", "abs": "https://arxiv.org/abs/2511.12020", "authors": ["Xianglong Shi", "Silin Cheng", "Sirui Zhao", "Yunhan Jiang", "Enhong Chen", "Yang Liu", "Sebastien Ourselin"], "title": "LIHE: Linguistic Instance-Split Hyperbolic-Euclidean Framework for Generalized Weakly-Supervised Referring Expression Comprehension", "comment": null, "summary": "Existing Weakly-Supervised Referring Expression Comprehension (WREC) methods, while effective, are fundamentally limited by a one-to-one mapping assumption, hindering their ability to handle expressions corresponding to zero or multiple targets in realistic scenarios. To bridge this gap, we introduce the Weakly-Supervised Generalized Referring Expression Comprehension task (WGREC), a more practical paradigm that handles expressions with variable numbers of referents. However, extending WREC to WGREC presents two fundamental challenges: supervisory signal ambiguity, where weak image-level supervision is insufficient for training a model to infer the correct number and identity of referents, and semantic representation collapse, where standard Euclidean similarity forces hierarchically-related concepts into non-discriminative clusters, blurring categorical boundaries. To tackle these challenges, we propose a novel WGREC framework named Linguistic Instance-Split Hyperbolic-Euclidean (LIHE), which operates in two stages. The first stage, Referential Decoupling, predicts the number of target objects and decomposes the complex expression into simpler sub-expressions. The second stage, Referent Grounding, then localizes these sub-expressions using HEMix, our innovative hybrid similarity module that synergistically combines the precise alignment capabilities of Euclidean proximity with the hierarchical modeling strengths of hyperbolic geometry. This hybrid approach effectively prevents semantic collapse while preserving fine-grained distinctions between related concepts. Extensive experiments demonstrate LIHE establishes the first effective weakly supervised WGREC baseline on gRefCOCO and Ref-ZOM, while HEMix achieves consistent improvements on standard REC benchmarks, improving IoU@0.5 by up to 2.5\\%. The code is available at https://anonymous.4open.science/r/LIHE.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u7684\u5f31\u76d1\u7763\u5e7f\u4e49\u6307\u4ee3\u8868\u8fbe\u5f0f\u7406\u89e3\u4efb\u52a1\uff08WGREC\uff09\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u96f6\u4e2a\u6216\u591a\u4e2a\u76ee\u6807\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f31\u76d1\u7763\u6307\u4ee3\u8868\u8fbe\u5f0f\u7406\u89e3\uff08WREC\uff09\u65b9\u6cd5\u53d7\u9650\u4e8e\u4e00\u5bf9\u4e00\u6620\u5c04\u5047\u8bbe\uff0c\u65e0\u6cd5\u5904\u7406\u73b0\u5b9e\u573a\u666f\u4e2d\u5bf9\u5e94\u96f6\u4e2a\u6216\u591a\u4e2a\u76ee\u6807\u7684\u8868\u8fbe\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aLIHE\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542bReferential Decoupling\u548cReferent Grounding\u4e24\u4e2a\u9636\u6bb5\uff0c\u5e76\u4f7f\u7528\u6df7\u5408\u76f8\u4f3c\u5ea6\u6a21\u5757HEMix\u3002", "result": "LIHE\u5728gRefCOCO\u548cRef-ZOM\u6570\u636e\u96c6\u4e0a\u5efa\u7acb\u4e86\u7b2c\u4e00\u4e2a\u6709\u6548\u7684\u5f31\u76d1\u7763WGREC\u57fa\u7ebf\uff0cHEMix\u5728\u6807\u51c6REC\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "LIHE\u6846\u67b6\u548cHEMix\u6a21\u5757\u80fd\u591f\u6709\u6548\u89e3\u51b3WGREC\u4efb\u52a1\u4e2d\u7684\u76d1\u7763\u4fe1\u53f7\u6a21\u7cca\u548c\u8bed\u4e49\u8868\u793a\u5d29\u6e83\u95ee\u9898\u3002"}}
{"id": "2511.11663", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11663", "abs": "https://arxiv.org/abs/2511.11663", "authors": ["Zhixiong Zhao", "Fangxin Liu", "Junjie Wang", "Chenyang Guan", "Zongwu Wang", "Li Jiang", "Haibing Guan"], "title": "SpecQuant: Spectral Decomposition and Adaptive Truncation for Ultra-Low-Bit LLMs Quantization", "comment": "Accepted at AAAI 2026", "summary": "The emergence of accurate open large language models (LLMs) has sparked a push for advanced quantization techniques to enable efficient deployment on end-user devices. In this paper, we revisit the challenge of extreme LLM compression -- targeting ultra-low-bit quantization for both activations and weights -- from a Fourier frequency domain perspective. We propose SpecQuant, a two-stage framework that tackles activation outliers and cross-channel variance. In the first stage, activation outliers are smoothed and transferred into the weight matrix to simplify downstream quantization. In the second stage, we apply channel-wise low-frequency Fourier truncation to suppress high-frequency components while preserving essential signal energy, improving quantization robustness. Our method builds on the principle that most of the weight energy is concentrated in low-frequency components, which can be retained with minimal impact on model accuracy. To enable runtime adaptability, we introduce a lightweight truncation module during inference that adjusts truncation thresholds based on channel characteristics. On LLaMA-3 8B, SpecQuant achieves 4-bit quantization for both weights and activations, narrowing the zero-shot accuracy gap to only 1.5% compared to full precision, while delivering 2 times faster inference and 3times lower memory usage.", "AI": {"tldr": "SpecQuant\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LLM\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u9891\u57df\u4e2d\u5904\u7406\u6fc0\u6d3b\u548c\u6743\u91cd\uff0c\u5b9e\u73b0\u4e86\u6781\u4f4e\u7684\u6bd4\u7279\u91cf\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u51c6\u786e\u7387\u548c\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u5728\u7ec8\u7aef\u8bbe\u5907\u4e0a\u9ad8\u6548\u90e8\u7f72\uff0c\u9700\u8981\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u538b\u7f29\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u6781\u4f4e\u6bd4\u7279\u91cf\u5316\u6fc0\u6d3b\u548c\u6743\u91cd\u3002", "method": "SpecQuant\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u9996\u5148\u5e73\u6ed1\u6fc0\u6d3b\u5f02\u5e38\u503c\u5e76\u5c06\u5176\u8f6c\u79fb\u5230\u6743\u91cd\u77e9\u9635\u4e2d\uff0c\u7136\u540e\u5e94\u7528\u901a\u9053\u7ea7\u7684\u4f4e\u9891\u5085\u91cc\u53f6\u622a\u65ad\u6765\u6291\u5236\u9ad8\u9891\u5206\u91cf\u3002\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u622a\u65ad\u6a21\u5757\uff0c\u7528\u4e8e\u8c03\u6574\u622a\u65ad\u9608\u503c\u3002", "result": "\u5728LLaMA-3 8B\u6a21\u578b\u4e0a\uff0cSpecQuant\u5b9e\u73b0\u4e86\u6743\u91cd\u548c\u6fc0\u6d3b\u76844\u6bd4\u7279\u91cf\u5316\uff0c\u96f6\u6837\u672c\u51c6\u786e\u7387\u5dee\u8ddd\u4ec5\u4e3a1.5%\uff0c\u540c\u65f6\u63a8\u7406\u901f\u5ea6\u63d0\u9ad82\u500d\uff0c\u5185\u5b58\u4f7f\u7528\u964d\u4f4e3\u500d\u3002", "conclusion": "SpecQuant\u80fd\u591f\u5728\u6781\u4f4e\u6bd4\u7279\u91cf\u5316LLM\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u8f83\u9ad8\u7684\u51c6\u786e\u7387\u548c\u6548\u7387\uff0c\u4e3a\u7ec8\u7aef\u8bbe\u5907\u4e0a\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.12344", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12344", "abs": "https://arxiv.org/abs/2511.12344", "authors": ["Baolong Bi", "Shenghua Liu", "Yiwei Wang", "Siqian Tong", "Lingrui Mei", "Yuyao Ge", "Yilong Xu", "Jiafeng Guo", "Xueqi Cheng"], "title": "Reward and Guidance through Rubrics: Promoting Exploration to Improve Multi-Domain Reasoning", "comment": null, "summary": "Recent advances in reinforcement learning (RL) have significantly improved the complex reasoning capabilities of large language models (LLMs). Despite these successes, existing methods mainly focus on single-domain RL (e.g., mathematics) with verifiable rewards (RLVR), and their reliance on purely online RL frameworks restricts the exploration space, thereby limiting reasoning performance. In this paper, we address these limitations by leveraging rubrics to provide both fine-grained reward signals and offline guidance. We propose $\\textbf{RGR-GRPO}$ (Reward and Guidance through Rubrics), a rubric-driven RL framework for multi-domain reasoning. RGR-GRPO enables LLMs to receive dense and informative rewards while exploring a larger solution space during GRPO training. Extensive experiments across 14 benchmarks spanning multiple domains demonstrate that RGR-GRPO consistently outperforms RL methods that rely solely on alternative reward schemes or offline guidance. Compared with verifiable online RL baseline, RGR-GRPO achieves average improvements of +7.0%, +5.4%, +8.4%, and +6.6% on mathematics, physics, chemistry, and general reasoning tasks, respectively. Notably, RGR-GRPO maintains stable entropy fluctuations during off-policy training and achieves superior pass@k performance, reflecting sustained exploration and effective breakthrough beyond existing performance bottlenecks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u7684\u591a\u57df\u63a8\u7406\u5f3a\u5316\u5b66\u4e60\u6846\u67b6RGR-GRPO\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5177\u6709\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5355\u9886\u57df\u5f3a\u5316\u5b66\u4e60\uff0c\u5e76\u4e14\u4f9d\u8d56\u4e8e\u7eaf\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u9650\u5236\u4e86\u63a2\u7d22\u7a7a\u95f4\uff0c\u4ece\u800c\u9650\u5236\u4e86\u63a8\u7406\u6027\u80fd\u3002", "method": "\u5229\u7528\u89c4\u5219\u6765\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u5956\u52b1\u4fe1\u53f7\u548c\u79bb\u7ebf\u6307\u5bfc\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u89c4\u5219\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6RGR-GRPO\u3002", "result": "\u5728\u8de8\u8d8a\u591a\u4e2a\u9886\u57df\u768414\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRGR-GRPO\u59cb\u7ec8\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u4e8e\u66ff\u4ee3\u5956\u52b1\u65b9\u6848\u6216\u79bb\u7ebf\u6307\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002\u4e0e\u53ef\u9a8c\u8bc1\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u76f8\u6bd4\uff0cRGR-GRPO\u5728\u6570\u5b66\u3001\u7269\u7406\u3001\u5316\u5b66\u548c\u4e00\u822c\u63a8\u7406\u4efb\u52a1\u4e0a\u5206\u522b\u5b9e\u73b0\u4e86\u5e73\u5747+7.0%\u3001+5.4%\u3001+8.4%\u548c+6.6%\u7684\u6539\u8fdb\u3002", "conclusion": "RGR-GRPO\u5728\u79bb\u7b56\u7565\u8bad\u7ec3\u671f\u95f4\u4fdd\u6301\u7a33\u5b9a\u7684\u71b5\u6ce2\u52a8\uff0c\u5e76\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684pass@k\u6027\u80fd\uff0c\u53cd\u6620\u4e86\u6301\u7eed\u7684\u63a2\u7d22\u548c\u6709\u6548\u7a81\u7834\u73b0\u6709\u6027\u80fd\u74f6\u9888\u3002"}}
{"id": "2511.12520", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12520", "abs": "https://arxiv.org/abs/2511.12520", "authors": ["Jie Zhang", "Bo Tang", "Wanzi Shao", "Wenqiang Wei", "Jihao Zhao", "Jianqing Zhu", "Zhiyu li", "Wen Xi", "Zehao Lin", "Feiyu Xiong", "Yanchao Tan"], "title": "TAdaRAG: Task Adaptive Retrieval-Augmented Generation via On-the-Fly Knowledge Graph Construction", "comment": "Accepted by AAAI 2026", "summary": "Retrieval-Augmented Generation (RAG) improves large language models by retrieving external knowledge, often truncated into smaller chunks due to the input context window, which leads to information loss, resulting in response hallucinations and broken reasoning chains. Moreover, traditional RAG retrieves unstructured knowledge, introducing irrelevant details that hinder accurate reasoning. To address these issues, we propose TAdaRAG, a novel RAG framework for on-the-fly task-adaptive knowledge graph construction from external sources. Specifically, we design an intent-driven routing mechanism to a domain-specific extraction template, followed by supervised fine-tuning and a reinforcement learning-based implicit extraction mechanism, ensuring concise, coherent, and non-redundant knowledge integration. Evaluations on six public benchmarks and a real-world business benchmark (NowNewsQA) across three backbone models demonstrate that TAdaRAG outperforms existing methods across diverse domains and long-text tasks, highlighting its strong generalization and practical effectiveness.", "AI": {"tldr": "TAdaRAG: A new RAG framework that constructs task-adaptive knowledge graphs from external sources to improve the accuracy and coherence of LLM responses.", "motivation": "Traditional RAG systems suffer from information loss due to truncated contexts and irrelevant details from unstructured knowledge, leading to hallucinations and broken reasoning.", "method": "Proposes an intent-driven routing mechanism to a domain-specific extraction template, followed by supervised fine-tuning and reinforcement learning for concise knowledge integration.", "result": "TAdaRAG outperforms existing methods on six public benchmarks and a real-world business benchmark across three backbone models.", "conclusion": "TAdaRAG demonstrates strong generalization and practical effectiveness in diverse domains and long-text tasks."}}
{"id": "2511.12024", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12024", "abs": "https://arxiv.org/abs/2511.12024", "authors": ["Jose Reinaldo Cunha Santos A V Silva Neto", "Hodaka Kawachi", "Yasushi Yagi", "Tomoya Nakamura"], "title": "Null-Space Diffusion Distillation for Efficient Photorealistic Lensless Imaging", "comment": "8 pages without reference, 6 figures, 1 table", "summary": "State-of-the-art photorealistic reconstructions for lensless cameras often rely on paired lensless-lensed supervision, which can bias models due to lens-lensless domain mismatch. To avoid this, ground-truth-free diffusion priors are attractive; however, generic formulations tuned for conventional inverse problems often break under the noisy, highly multiplexed, and ill-posed lensless deconvolution setting. We observe that methods which separate range-space enforcement from null-space diffusion-prior updates yield stable, realistic reconstructions. Building on this, we introduce Null-Space Diffusion Distillation (NSDD): a single-pass student that distills the null-space component of an iterative DDNM+ solver, conditioned on the lensless measurement and on a range-space anchor. NSDD preserves measurement consistency and achieves photorealistic results without paired supervision at a fraction of the runtime and memory. On Lensless-FFHQ and PhlatCam, NSDD is the second fastest, behind Wiener, and achieves near-teacher perceptual quality (second-best LPIPS, below DDNM+), outperforming DPS and classical convex baselines. These results suggest a practical path toward fast, ground-truth-free, photorealistic lensless imaging.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u76d1\u7763lensless\u56fe\u50cf\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7null-space diffusion distillation (NSDD) \u4ece iterative DDNM+ \u6c42\u89e3\u5668\u4e2d\u63d0\u70bc null-space \u6210\u5206\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u771f\u5b9e\u7684\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u7684lensless\u76f8\u673a\u903c\u771f\u91cd\u5efa\u4f9d\u8d56\u4e8e paired lensless-lensed \u76d1\u7763\uff0c\u8fd9\u4f1a\u56e0\u57df\u4e0d\u5339\u914d\u800c\u5bfc\u81f4\u504f\u5dee\u3002\u65e0\u76d1\u7763\u7684 diffusion priors \u5f88\u6709\u5438\u5f15\u529b\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5728lensless\u53cd\u5377\u79ef\u4e2d\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86 Null-Space Diffusion Distillation (NSDD)\uff0c\u5b83\u662f\u4e00\u4e2a\u5355\u7a0b\u5b66\u751f\u6a21\u578b\uff0c\u53ef\u4ee5\u63d0\u70bc iterative DDNM+ \u6c42\u89e3\u5668\u7684 null-space \u6210\u5206\uff0c\u5e76\u4ee5 lensless \u6d4b\u91cf\u548c range-space \u951a\u70b9\u4e3a\u6761\u4ef6\u3002", "result": "\u5728 Lensless-FFHQ \u548c PhlatCam \u4e0a\uff0cNSDD \u901f\u5ea6\u4ec5\u6b21\u4e8e Wiener\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6559\u5e08\u7f51\u7edc\u7684\u611f\u77e5\u8d28\u91cf\uff0c\u4f18\u4e8e DPS \u548c\u4f20\u7edf\u51f8\u4f18\u5316\u57fa\u7ebf\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u5feb\u901f\u3001\u65e0\u76d1\u7763\u3001\u903c\u771f\u7684 lensless \u6210\u50cf\u662f\u6709\u5b9e\u9645\u5e94\u7528\u524d\u666f\u7684\u3002"}}
{"id": "2511.11665", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11665", "abs": "https://arxiv.org/abs/2511.11665", "authors": ["Sameeksha Sriram", "Ayush Paliwal", "Alexander S. Ecker", "Chase van de Geijn"], "title": "Clifford Algebraic Rotor Embeddings : Maybe embeddings should start to CARE", "comment": null, "summary": "Rotary Positional Embeddings (RoPE) have demonstrated exceptional performance as a positional encoding method, consistently outperforming their baselines. While recent work has sought to extend RoPE to higher-dimensional inputs, many such extensions are non-commutative, thereby forfeiting RoPE's shift-equivariance property. Spherical RoPE is one such non-commutative variant, motivated by the idea of rotating embedding vectors on spheres rather than circles. However, spherical rotations are inherently non-commutative, making the choice of rotation sequence ambiguous. In this work, we explore a quaternion-based approach -- Quaternion Rotary Embeddings (QuatRo) -- in place of Euler angles, leveraging quaternions' ability to represent 3D rotations to parameterize the axes of rotation. We show Mixed RoPE and Spherical RoPE to be special cases of QuatRo. Further, we propose a generalization of QuatRo to Clifford Algebraic Rotary Embeddings (CARE) using geometric algebra. Viewing quaternions as the even subalgebra of Cl(3,0,0), we extend the notion of rotary embeddings from quaternions to Clifford rotors acting on multivectors. This formulation enables two key generalizations: (1) extending rotary embeddings to arbitrary dimensions, and (2) encoding positional information in multivectors of multiple grades, not just vectors. We present preliminary experiments comparing spherical, quaternion, and Clifford-based rotary embeddings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56db\u5143\u6570\u548c\u514b\u5229\u798f\u5fb7\u4ee3\u6570\u7684\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u65b9\u6cd5\uff0c\u4ee5\u63a8\u5e7fRoPE\u5230\u66f4\u9ad8\u7ef4\u5ea6\uff0c\u5e76\u4fdd\u6301\u5176\u79fb\u4f4d\u7b49\u65b9\u6027\u3002", "motivation": "\u73b0\u6709\u7684RoPE\u6269\u5c55\u65b9\u6cd5\uff08\u5982Spherical RoPE\uff09\u4e0d\u5177\u5907\u79fb\u4f4d\u7b49\u65b9\u6027\uff0c\u5e76\u4e14\u65cb\u8f6c\u987a\u5e8f\u4e0d\u660e\u786e\u3002", "method": "\u4f7f\u7528\u56db\u5143\u6570\u6765\u53c2\u6570\u5316\u65cb\u8f6c\u8f74\uff0c\u5e76\u63a8\u5e7f\u5230\u514b\u5229\u798f\u5fb7\u4ee3\u6570\uff0c\u63d0\u51fa\u4e86Quaternion Rotary Embeddings (QuatRo) \u548c Clifford Algebraic Rotary Embeddings (CARE)\u3002", "result": "\u8bc1\u660e\u4e86Mixed RoPE\u548cSpherical RoPE\u662fQuatRo\u7684\u7279\u4f8b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e86spherical, quaternion, \u548c Clifford-based rotary embeddings\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5c06\u65cb\u8f6c\u5d4c\u5165\u63a8\u5e7f\u5230\u4efb\u610f\u7ef4\u5ea6\uff0c\u5e76\u5728\u591a\u91cd\u5411\u91cf\u4e2d\u7f16\u7801\u4f4d\u7f6e\u4fe1\u606f\u3002"}}
{"id": "2511.12359", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12359", "abs": "https://arxiv.org/abs/2511.12359", "authors": ["Yifan Zhu", "Sammie Katt", "Samuel Kaski"], "title": "More Than Irrational: Modeling Belief-Biased Agents", "comment": "13 pages, 8 figures. Accepted at the 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026)", "summary": "Despite the explosive growth of AI and the technologies built upon it, predicting and inferring the sub-optimal behavior of users or human collaborators remains a critical challenge. In many cases, such behaviors are not a result of irrationality, but rather a rational decision made given inherent cognitive bounds and biased beliefs about the world. In this paper, we formally introduce a class of computational-rational (CR) user models for cognitively-bounded agents acting optimally under biased beliefs. The key novelty lies in explicitly modeling how a bounded memory process leads to a dynamically inconsistent and biased belief state and, consequently, sub-optimal sequential decision-making. We address the challenge of identifying the latent user-specific bound and inferring biased belief states from passive observations on the fly. We argue that for our formalized CR model family with an explicit and parameterized cognitive process, this challenge is tractable. To support our claim, we propose an efficient online inference method based on nested particle filtering that simultaneously tracks the user's latent belief state and estimates the unknown cognitive bound from a stream of observed actions. We validate our approach in a representative navigation task using memory decay as an example of a cognitive bound. With simulations, we show that (1) our CR model generates intuitively plausible behaviors corresponding to different levels of memory capacity, and (2) our inference method accurately and efficiently recovers the ground-truth cognitive bounds from limited observations ($\\le 100$ steps). We further demonstrate how this approach provides a principled foundation for developing adaptive AI assistants, enabling adaptive assistance that accounts for the user's memory limitations.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u8ba1\u7b97\u7406\u6027\uff08CR\uff09\u7528\u6237\u6a21\u578b\uff0c\u7528\u4e8e\u6a21\u62df\u5728\u8ba4\u77e5\u9650\u5236\u548c\u504f\u89c1\u4fe1\u5ff5\u4e0b\u884c\u52a8\u7684\u667a\u80fd\u4f53\uff0c\u89e3\u51b3\u4e86\u9884\u6d4b\u548c\u63a8\u65ad\u7528\u6237\u6b21\u4f18\u884c\u4e3a\u7684\u96be\u9898\u3002", "motivation": "\u52a8\u673a\u662f\u9884\u6d4b\u548c\u63a8\u65ad\u7528\u6237\u6216\u4eba\u7c7b\u5408\u4f5c\u8005\u7684\u6b21\u4f18\u884c\u4e3a\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u884c\u4e3a\u901a\u5e38\u662f\u8ba4\u77e5\u9650\u5236\u548c\u504f\u89c1\u4fe1\u5ff5\u4e0b\u7684\u7406\u6027\u51b3\u7b56\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u6709\u754c\u8bb0\u5fc6\u8fc7\u7a0b\u5982\u4f55\u5bfc\u81f4\u52a8\u6001\u4e0d\u4e00\u81f4\u548c\u6709\u504f\u89c1\u7684\u4fe1\u5ff5\u72b6\u6001\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5d4c\u5957\u7c92\u5b50\u6ee4\u6ce2\u7684\u5728\u7ebf\u63a8\u7406\u65b9\u6cd5\uff0c\u4ee5\u540c\u65f6\u8ddf\u8e2a\u7528\u6237\u7684\u6f5c\u5728\u4fe1\u5ff5\u72b6\u6001\u5e76\u4f30\u8ba1\u672a\u77e5\u7684\u8ba4\u77e5\u754c\u9650\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5bfc\u822a\u4efb\u52a1\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u8be5CR\u6a21\u578b\u80fd\u591f\u751f\u6210\u4e0e\u4e0d\u540c\u8bb0\u5fc6\u5bb9\u91cf\u76f8\u5bf9\u5e94\u7684\u76f4\u89c2\u5408\u7406\u7684\u884c\u4e3a\uff0c\u5e76\u4e14\u8be5\u63a8\u7406\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u6709\u6548\u5730\u4ece\u6709\u9650\u7684\u89c2\u5bdf\u4e2d\u6062\u590d\u771f\u5b9e\u7684\u8ba4\u77e5\u754c\u9650\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5f00\u53d1\u81ea\u9002\u5e94AI\u52a9\u624b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u57fa\u7840\uff0c\u80fd\u591f\u5b9e\u73b0\u8003\u8651\u7528\u6237\u8bb0\u5fc6\u9650\u5236\u7684\u81ea\u9002\u5e94\u8f85\u52a9\u3002"}}
{"id": "2511.12573", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12573", "abs": "https://arxiv.org/abs/2511.12573", "authors": ["Hyeonji Kim", "Sujeong Oh", "Sanghack Lee"], "title": "Mitigating Length Bias in RLHF through a Causal Lens", "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) is widely used to align large language models (LLMs) with human preferences. However, RLHF-trained reward models often exhibit length bias -- a systematic tendency to favor longer responses by conflating verbosity with quality. We propose a causal framework for analyzing and mitigating length bias in RLHF reward modeling. Central to our approach is a counterfactual data augmentation method that generates response pairs designed to isolate content quality from verbosity. These counterfactual examples are then used to train the reward model, enabling it to assess responses based on content quality independently of verbosity. Specifically, we construct (1) length-divergent pairs with similar content and (2) content-divergent pairs of similar length. Empirical evaluations show that our method reduces length bias in reward assignment and leads to more concise, content-focused outputs from the policy model. These findings demonstrate that the proposed approach effectively reduces length bias and improves the robustness and content sensitivity of reward modeling in RLHF pipelines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u56e0\u679c\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u548c\u7f13\u89e3RLHF\u5956\u52b1\u6a21\u578b\u4e2d\u7684\u957f\u5ea6\u504f\u5dee\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u751f\u6210\u65e8\u5728\u5c06\u5185\u5bb9\u8d28\u91cf\u4e0e\u5197\u957f\u6027\u9694\u79bb\u7684\u54cd\u5e94\u5bf9\uff0c\u6765\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u3002", "motivation": "RLHF\u8bad\u7ec3\u7684\u5956\u52b1\u6a21\u578b\u901a\u5e38\u8868\u73b0\u51fa\u957f\u5ea6\u504f\u5dee\uff0c\u5373\u7cfb\u7edf\u6027\u5730\u504f\u7231\u66f4\u957f\u7684\u54cd\u5e94\uff0c\u5c06\u5197\u957f\u4e0e\u8d28\u91cf\u6df7\u4e3a\u4e00\u8c08\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cd\u4e8b\u5b9e\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u751f\u6210\u5177\u6709\u76f8\u4f3c\u5185\u5bb9\u4f46\u957f\u5ea6\u4e0d\u540c\u7684\u914d\u5bf9\uff0c\u4ee5\u53ca\u5185\u5bb9\u4e0d\u540c\u4f46\u957f\u5ea6\u76f8\u4f3c\u7684\u914d\u5bf9\uff0c\u7528\u4e8e\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u5956\u52b1\u5206\u914d\u4e2d\u7684\u957f\u5ea6\u504f\u5dee\uff0c\u5e76\u4f7f\u7b56\u7565\u6a21\u578b\u4ea7\u751f\u66f4\u7b80\u6d01\u3001\u66f4\u6ce8\u91cd\u5185\u5bb9\u7684\u8f93\u51fa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u51cf\u5c11\u4e86\u957f\u5ea6\u504f\u5dee\uff0c\u63d0\u9ad8\u4e86RLHF\u7ba1\u9053\u4e2d\u5956\u52b1\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u5185\u5bb9\u654f\u611f\u6027\u3002"}}
{"id": "2511.12026", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12026", "abs": "https://arxiv.org/abs/2511.12026", "authors": ["Rulin Zhou", "Wenlong He", "An Wang", "Jianhang Zhang", "Xuanhui Zeng", "Xi Zhang", "Chaowei Zhu", "Haijun Hu", "Hongliang Ren"], "title": "Bridging Vision and Language for Robust Context-Aware Surgical Point Tracking: The VL-SurgPT Dataset and Benchmark", "comment": "AAAI 2026 oral", "summary": "Accurate point tracking in surgical environments remains challenging due to complex visual conditions, including smoke occlusion, specular reflections, and tissue deformation. While existing surgical tracking datasets provide coordinate information, they lack the semantic context necessary to understand tracking failure mechanisms. We introduce VL-SurgPT, the first large-scale multimodal dataset that bridges visual tracking with textual descriptions of point status in surgical scenes. The dataset comprises 908 in vivo video clips, including 754 for tissue tracking (17,171 annotated points across five challenging scenarios) and 154 for instrument tracking (covering seven instrument types with detailed keypoint annotations). We establish comprehensive benchmarks using eight state-of-the-art tracking methods and propose TG-SurgPT, a text-guided tracking approach that leverages semantic descriptions to improve robustness in visually challenging conditions. Experimental results demonstrate that incorporating point status information significantly improves tracking accuracy and reliability, particularly in adverse visual scenarios where conventional vision-only methods struggle. By bridging visual and linguistic modalities, VL-SurgPT enables the development of context-aware tracking systems crucial for advancing computer-assisted surgery applications that can maintain performance even under challenging intraoperative conditions.", "AI": {"tldr": "VL-SurgPT\uff1a\u9996\u4e2a\u5927\u578b\u591a\u6a21\u6001\u624b\u672f\u6570\u636e\u96c6\uff0c\u5c06\u89c6\u89c9\u8ddf\u8e2a\u4e0e\u624b\u672f\u573a\u666f\u4e2d\u70b9\u72b6\u6001\u7684\u6587\u672c\u63cf\u8ff0\u76f8\u7ed3\u5408\uff0c\u4ee5\u5e94\u5bf9\u590d\u6742\u89c6\u89c9\u6761\u4ef6\u4e0b\u7684\u8ddf\u8e2a\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u624b\u672f\u8ddf\u8e2a\u6570\u636e\u96c6\u7f3a\u4e4f\u7406\u89e3\u8ddf\u8e2a\u5931\u8d25\u673a\u5236\u6240\u9700\u7684\u8bed\u4e49\u4fe1\u606f\u3002", "method": "1. \u6784\u5efa\u5305\u542b908\u4e2a\u4f53\u5185\u89c6\u9891\u7247\u6bb5\u7684\u6570\u636e\u96c6\uff0c\u5305\u62ec\u7ec4\u7ec7\u8ddf\u8e2a\u548c\u5668\u68b0\u8ddf\u8e2a\u30022. \u4f7f\u75288\u79cd\u5148\u8fdb\u7684\u8ddf\u8e2a\u65b9\u6cd5\u5efa\u7acb\u7efc\u5408\u57fa\u51c6\u30023. \u63d0\u51faTG-SurgPT\uff0c\u4e00\u79cd\u6587\u672c\u5f15\u5bfc\u7684\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u5229\u7528\u8bed\u4e49\u63cf\u8ff0\u6765\u63d0\u9ad8\u5728\u89c6\u89c9\u6311\u6218\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u7ed3\u5408\u70b9\u72b6\u6001\u4fe1\u606f\u663e\u8457\u63d0\u9ad8\u4e86\u8ddf\u8e2a\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u5c24\u5176\u662f\u5728\u4f20\u7edf\u89c6\u89c9\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u7684\u6076\u52a3\u89c6\u89c9\u573a\u666f\u4e2d\u3002", "conclusion": "VL-SurgPT\u901a\u8fc7\u6865\u63a5\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u6001\uff0c\u80fd\u591f\u5f00\u53d1\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8ddf\u8e2a\u7cfb\u7edf\uff0c\u8fd9\u5bf9\u4e8e\u63a8\u8fdb\u8ba1\u7b97\u673a\u8f85\u52a9\u624b\u672f\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u5373\u4f7f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u672f\u4e2d\u6761\u4ef6\u4e0b\u4e5f\u80fd\u4fdd\u6301\u6027\u80fd\u3002"}}
{"id": "2511.11666", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.11666", "abs": "https://arxiv.org/abs/2511.11666", "authors": ["Rajit Rajpal", "Benedict Leimkuhler", "Yuanhao Jiang"], "title": "Adaptive Stepsizing for Stochastic Gradient Langevin Dynamics in Bayesian Neural Networks", "comment": null, "summary": "Bayesian neural networks (BNNs) require scalable sampling algorithms to approximate posterior distributions over parameters. Existing stochastic gradient Markov Chain Monte Carlo (SGMCMC) methods are highly sensitive to the choice of stepsize and adaptive variants such as pSGLD typically fail to sample the correct invariant measure without addition of a costly divergence correction term. In this work, we build on the recently proposed `SamAdams' framework for timestep adaptation (Leimkuhler, Lohmann, and Whalley 2025), introducing an adaptive scheme: SA-SGLD, which employs time rescaling to modulate the stepsize according to a monitored quantity (typically the local gradient norm). SA-SGLD can automatically shrink stepsizes in regions of high curvature and expand them in flatter regions, improving both stability and mixing without introducing bias. We show that our method can achieve more accurate posterior sampling than SGLD on high-curvature 2D toy examples and in image classification with BNNs using sharp priors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94SGLD\u65b9\u6cd5\uff08SA-SGLD\uff09\uff0c\u5b83\u901a\u8fc7\u65f6\u95f4\u91cd\u7f29\u653e\u6765\u8c03\u8282\u6b65\u957f\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86BNN\u4e2d\u540e\u9a8c\u5206\u5e03\u91c7\u6837\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u4e14\u6ca1\u6709\u5f15\u5165\u504f\u5dee\u3002", "motivation": "\u73b0\u6709\u7684SGMCMC\u65b9\u6cd5\u5bf9\u6b65\u957f\u7684\u9009\u62e9\u975e\u5e38\u654f\u611f\uff0c\u800c\u81ea\u9002\u5e94\u53d8\u4f53\uff08\u5982pSGLD\uff09\u901a\u5e38\u65e0\u6cd5\u5728\u4e0d\u6dfb\u52a0\u6602\u8d35\u6563\u5ea6\u6821\u6b63\u9879\u7684\u60c5\u51b5\u4e0b\u5bf9\u6b63\u786e\u7684 invariant measure \u8fdb\u884c\u91c7\u6837\u3002", "method": "\u6784\u5efa\u5728\u6700\u8fd1\u63d0\u51fa\u7684`SamAdams' timestep adaptation\u6846\u67b6\u4e4b\u4e0a\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u65b9\u6848\uff1aSA-SGLD\uff0c\u5b83\u91c7\u7528\u65f6\u95f4\u91cd\u7f29\u653e\u6765\u6839\u636e\u76d1\u63a7\u91cf\uff08\u901a\u5e38\u662f\u5c40\u90e8\u68af\u5ea6\u8303\u6570\uff09\u6765\u8c03\u8282\u6b65\u957f\u3002", "result": "\u5728\u5177\u6709\u9ad8\u66f2\u7387\u76842D\u73a9\u5177\u793a\u4f8b\u548c\u4f7f\u7528 sharp priors \u7684 BNN \u7684\u56fe\u50cf\u5206\u7c7b\u4e2d\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u6bd4 SGLD \u66f4\u51c6\u786e\u7684\u540e\u9a8c\u91c7\u6837\u3002", "conclusion": "SA-SGLD\u53ef\u4ee5\u81ea\u52a8\u7f29\u5c0f\u9ad8\u66f2\u7387\u533a\u57df\u7684\u6b65\u957f\uff0c\u5e76\u6269\u5927\u8f83\u5e73\u5766\u533a\u57df\u7684\u6b65\u957f\uff0c\u4ece\u800c\u63d0\u9ad8\u7a33\u5b9a\u6027\u548c\u6df7\u5408\u6027\uff0c\u4e14\u4e0d\u5f15\u5165\u504f\u5dee"}}
{"id": "2511.12378", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12378", "abs": "https://arxiv.org/abs/2511.12378", "authors": ["Dylan M. Asmar", "Mykel J. Kochenderfer"], "title": "Learning to Trust: Bayesian Adaptation to Varying Suggester Reliability in Sequential Decision Making", "comment": "Under Review", "summary": "Autonomous agents operating in sequential decision-making tasks under uncertainty can benefit from external action suggestions, which provide valuable guidance but inherently vary in reliability. Existing methods for incorporating such advice typically assume static and known suggester quality parameters, limiting practical deployment. We introduce a framework that dynamically learns and adapts to varying suggester reliability in partially observable environments. First, we integrate suggester quality directly into the agent's belief representation, enabling agents to infer and adjust their reliance on suggestions through Bayesian inference over suggester types. Second, we introduce an explicit ``ask'' action allowing agents to strategically request suggestions at critical moments, balancing informational gains against acquisition costs. Experimental evaluation demonstrates robust performance across varying suggester qualities, adaptation to changing reliability, and strategic management of suggestion requests. This work provides a foundation for adaptive human-agent collaboration by addressing suggestion uncertainty in uncertain environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u8fd0\u884c\u7684\u81ea\u4e3bAgent\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u52a8\u6001\u5b66\u4e60\u548c\u9002\u5e94\u5916\u90e8\u5efa\u8bae\u8005\u7684\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u5c06\u5efa\u8bae\u8005\u8d28\u91cf\u7eb3\u5165Agent\u7684\u4fe1\u5ff5\u8868\u793a\uff0c\u5e76\u5f15\u5165\u201c\u8be2\u95ee\u201d\u52a8\u4f5c\u6765\u5b9e\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u9759\u6001\u548c\u5df2\u77e5\u7684\u5efa\u8bae\u8005\u8d28\u91cf\u53c2\u6570\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\uff0c\u5e8f\u5217\u51b3\u7b56\u4efb\u52a1\u4e2d\u8fd0\u884c\u7684\u81ea\u4e3bAgent\u53ef\u4ee5\u4ece\u5916\u90e8\u884c\u52a8\u5efa\u8bae\u4e2d\u53d7\u76ca\uff0c\u8fd9\u4e9b\u5efa\u8bae\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u6307\u5bfc\uff0c\u4f46\u53ef\u9760\u6027\u5404\u4e0d\u76f8\u540c\u3002", "method": "\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u5efa\u8bae\u8005\u8d28\u91cf\u76f4\u63a5\u6574\u5408\u5230Agent\u7684\u4fe1\u5ff5\u8868\u793a\u4e2d\uff0c\u4f7fAgent\u80fd\u591f\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u7406\u6765\u63a8\u65ad\u548c\u8c03\u6574\u4ed6\u4eec\u5bf9\u5efa\u8bae\u7684\u4f9d\u8d56\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u660e\u786e\u7684\u201c\u8be2\u95ee\u201d\u52a8\u4f5c\uff0c\u5141\u8bb8Agent\u5728\u5173\u952e\u65f6\u523b\u6218\u7565\u6027\u5730\u8bf7\u6c42\u5efa\u8bae\uff0c\u4ece\u800c\u5e73\u8861\u4fe1\u606f\u589e\u76ca\u4e0e\u83b7\u53d6\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u7684\u5efa\u8bae\u8005\u8d28\u91cf\u4e0b\u8868\u73b0\u51fa\u7a33\u5065\u7684\u6027\u80fd\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u53ef\u9760\u6027\uff0c\u5e76\u80fd\u6218\u7565\u6027\u5730\u7ba1\u7406\u5efa\u8bae\u8bf7\u6c42\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u89e3\u51b3\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u5efa\u8bae\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u81ea\u9002\u5e94\u4eba\u673a\u534f\u4f5c\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.12586", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12586", "abs": "https://arxiv.org/abs/2511.12586", "authors": ["Pu-Hai Yang", "Heyan Huang", "Heng-Da Xu", "Fanshu Sun", "Xian-Ling Mao", "Chaoxu Mu"], "title": "MMWOZ: Building Multimodal Agent for Task-oriented Dialogue", "comment": null, "summary": "Task-oriented dialogue systems have garnered significant attention due to their conversational ability to accomplish goals, such as booking airline tickets for users. Traditionally, task-oriented dialogue systems are conceptualized as intelligent agents that interact with users using natural language and have access to customized back-end APIs. However, in real-world scenarios, the widespread presence of front-end Graphical User Interfaces (GUIs) and the absence of customized back-end APIs create a significant gap for traditional task-oriented dialogue systems in practical applications. In this paper, to bridge the gap, we collect MMWOZ, a new multimodal dialogue dataset that is extended from MultiWOZ 2.3 dataset. Specifically, we begin by developing a web-style GUI to serve as the front-end. Next, we devise an automated script to convert the dialogue states and system actions from the original dataset into operation instructions for the GUI. Lastly, we collect snapshots of the web pages along with their corresponding operation instructions. In addition, we propose a novel multimodal model called MATE (Multimodal Agent for Task-oriEnted dialogue) as the baseline model for the MMWOZ dataset. Furthermore, we conduct comprehensive experimental analysis using MATE to investigate the construction of a practical multimodal agent for task-oriented dialogue.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001\u5bf9\u8bdd\u6570\u636e\u96c6 MMWOZ\uff0c\u65e8\u5728\u5f25\u5408\u4f20\u7edf\u9762\u5411\u4efb\u52a1\u7684\u5bf9\u8bdd\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5dee\u8ddd\u3002\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u5f00\u53d1 Web \u98ce\u683c\u7684 GUI \u5e76\u5c06\u539f\u59cb\u6570\u636e\u96c6\u4e2d\u7684\u5bf9\u8bdd\u72b6\u6001\u548c\u7cfb\u7edf\u52a8\u4f5c\u8f6c\u6362\u4e3a GUI \u7684\u64cd\u4f5c\u6307\u4ee4\u6765\u6784\u5efa\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a MATE \u7684\u65b0\u578b\u591a\u6a21\u6001\u6a21\u578b\u4f5c\u4e3a MMWOZ \u6570\u636e\u96c6\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u9762\u5411\u4efb\u52a1\u7684\u5bf9\u8bdd\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u4e3a\u73b0\u5b9e\u4e16\u754c\u4e2d\u5b58\u5728\u5927\u91cf\u7684\u524d\u7aef GUI \u4e14\u7f3a\u4e4f\u5b9a\u5236\u7684\u540e\u7aef API\u3002", "method": "1. \u5f00\u53d1\u4e00\u4e2a Web \u98ce\u683c\u7684 GUI \u4f5c\u4e3a\u524d\u7aef\u3002\n2. \u8bbe\u8ba1\u4e00\u4e2a\u81ea\u52a8\u811a\u672c\uff0c\u5c06\u539f\u59cb\u6570\u636e\u96c6\u4e2d\u7684\u5bf9\u8bdd\u72b6\u6001\u548c\u7cfb\u7edf\u52a8\u4f5c\u8f6c\u6362\u4e3a GUI \u7684\u64cd\u4f5c\u6307\u4ee4\u3002\n3. \u6536\u96c6\u7f51\u9875\u5feb\u7167\u4ee5\u53ca\u76f8\u5e94\u7684\u64cd\u4f5c\u6307\u4ee4\u3002\n4. \u63d0\u51fa\u4e00\u4e2a\u540d\u4e3a MATE \u7684\u65b0\u578b\u591a\u6a21\u6001\u6a21\u578b\u4f5c\u4e3a MMWOZ \u6570\u636e\u96c6\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001\u5bf9\u8bdd\u6570\u636e\u96c6 MMWOZ\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a MATE \u7684\u65b0\u578b\u591a\u6a21\u6001\u6a21\u578b\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u7814\u7a76\u5982\u4f55\u6784\u5efa\u4e00\u4e2a\u5b9e\u7528\u7684\u591a\u6a21\u6001 agent\uff0c\u7528\u4e8e\u9762\u5411\u4efb\u52a1\u7684\u5bf9\u8bdd\u3002"}}
{"id": "2511.12027", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12027", "abs": "https://arxiv.org/abs/2511.12027", "authors": ["Jeong Hun Yeo", "Sangyun Chung", "Sungjune Park", "Dae Hoe Kim", "Jinyoung Moon", "Yong Man Ro"], "title": "GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory", "comment": null, "summary": "Long-video understanding remains a significant challenge for Multimodal Large Language Models (MLLMs) due to inherent token limitations and the complexity of capturing long-term temporal dependencies. Existing methods often fail to capture the global context and complex event relationships necessary for deep video reasoning. To address this, we introduce GCAgent, a novel Global-Context-Aware Agent framework that achieves comprehensive long-video understanding. Our core innovation is the Schematic and Narrative Episodic Memory. This memory structurally models events and their causal and temporal relations into a concise, organized context, fundamentally resolving the long-term dependency problem. Operating in a multi-stage Perception-Action-Reflection cycle, our GCAgent utilizes a Memory Manager to retrieve relevant episodic context for robust, context-aware inference. Extensive experiments confirm that GCAgent significantly enhances long-video understanding, achieving up to 23.5\\% accuracy improvement on the Video-MME Long split over a strong MLLM baseline. Furthermore, our framework establishes state-of-the-art performance among comparable 7B-scale MLLMs, achieving 73.4\\% accuracy on the Long split and the highest overall average (71.9\\%) on the Video-MME benchmark, validating our agent-based reasoning paradigm and structured memory for cognitively-inspired long-video understanding.", "AI": {"tldr": "GCAgent\u662f\u4e00\u4e2a\u7528\u4e8e\u957f\u89c6\u9891\u7406\u89e3\u7684\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u6784\u5efa\u7ed3\u6784\u5316\u7684\u4e8b\u4ef6\u8bb0\u5fc6\u6765\u89e3\u51b3\u957f\u65f6\u4f9d\u8d56\u95ee\u9898\uff0c\u5e76\u5728Video-MME\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u590d\u6742\u7684\u4e8b\u4ef6\u5173\u7cfb\uff0c\u4ece\u800c\u65e0\u6cd5\u8fdb\u884c\u6df1\u5165\u7684\u89c6\u9891\u63a8\u7406\u3002", "method": "GCAgent\u4f7f\u7528\u56fe\u793a\u548c\u53d9\u4e8b\u4e8b\u4ef6\u8bb0\u5fc6\uff0c\u5e76\u5728\u591a\u9636\u6bb5\u611f\u77e5-\u884c\u52a8-\u53cd\u601d\u5faa\u73af\u4e2d\u8fd0\u884c\uff0c\u5229\u7528\u8bb0\u5fc6\u7ba1\u7406\u5668\u68c0\u7d22\u76f8\u5173\u7684\u4e8b\u4ef6\u4e0a\u4e0b\u6587\u3002", "result": "GCAgent\u5728Video-MME Long split\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe23.5%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u5e76\u57287B\u89c4\u6a21\u7684MLLM\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u9a8c\u8bc1\u4e86\u57fa\u4e8eAgent\u7684\u63a8\u7406\u8303\u5f0f\u548c\u7ed3\u6784\u5316\u8bb0\u5fc6\u5728\u8ba4\u77e5\u542f\u53d1\u4e0b\u7684\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.11667", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11667", "abs": "https://arxiv.org/abs/2511.11667", "authors": ["Feng Guo", "Yuntao Wen", "Shen Gao", "Junshuo Zhang", "Shuo Shang"], "title": "Beyond Superficial Forgetting: Thorough Unlearning through Knowledge Density Estimation and Block Re-insertion", "comment": null, "summary": "Machine unlearning, which selectively removes harmful knowledge from a pre-trained model without retraining from scratch, is crucial for addressing privacy, regulatory compliance, and ethical concerns in Large Language Models (LLMs). However, existing unlearning methods often struggle to thoroughly remove harmful knowledge, leaving residual harmful knowledge that can be easily recovered. To address these limitations, we propose Knowledge Density-Guided Unlearning via Blocks Reinsertion (KUnBR), a novel approach that first identifies layers with rich harmful knowledge and then thoroughly eliminates the harmful knowledge via re-insertion strategy. Our method introduces knowledge density estimation to quantify and locate layers containing the most harmful knowledge, enabling precise unlearning. Additionally, we design a layer re-insertion strategy that extracts and re-inserts harmful knowledge-rich layers into the original LLM, bypassing gradient obstruction caused by cover layers and ensuring effective gradient propagation during unlearning. Extensive experiments conducted on several unlearning and general capability benchmarks demonstrate that KUnBR achieves state-of-the-art forgetting performance while maintaining model utility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 machine unlearning \u65b9\u6cd5 KUnBR\uff0c\u901a\u8fc7\u77e5\u8bc6\u5bc6\u5ea6\u5f15\u5bfc\u548c\u5757\u91cd\u63d2\u5165\u6765\u5b9e\u73b0\u5bf9 LLM \u4e2d\u6709\u5bb3\u77e5\u8bc6\u7684\u5f7b\u5e95\u5220\u9664\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6548\u7528\u3002", "motivation": "\u73b0\u6709\u7684 machine unlearning \u65b9\u6cd5\u96be\u4ee5\u5f7b\u5e95\u5220\u9664\u6709\u5bb3\u77e5\u8bc6\uff0c\u5bfc\u81f4\u6b8b\u7559\uff0c\u4ece\u800c\u5f15\u53d1\u9690\u79c1\u3001\u5408\u89c4\u548c\u4f26\u7406\u95ee\u9898\u3002", "method": "\u9996\u5148\u8bc6\u522b\u5305\u542b\u4e30\u5bcc\u6709\u5bb3\u77e5\u8bc6\u7684\u5c42\uff0c\u7136\u540e\u901a\u8fc7\u91cd\u63d2\u5165\u7b56\u7565\u5f7b\u5e95\u6d88\u9664\u6709\u5bb3\u77e5\u8bc6\u3002\u5f15\u5165\u77e5\u8bc6\u5bc6\u5ea6\u4f30\u8ba1\u6765\u91cf\u5316\u548c\u5b9a\u4f4d\u5305\u542b\u6700\u591a\u6709\u5bb3\u77e5\u8bc6\u7684\u5c42\uff0c\u5e76\u8bbe\u8ba1\u5c42\u91cd\u63d2\u5165\u7b56\u7565\uff0c\u5c06\u5bcc\u542b\u6709\u5bb3\u77e5\u8bc6\u7684\u5c42\u63d0\u53d6\u5e76\u91cd\u65b0\u63d2\u5165\u5230\u539f\u59cb LLM \u4e2d\uff0c\u7ed5\u8fc7\u8986\u76d6\u5c42\u9020\u6210\u7684\u68af\u5ea6\u963b\u585e\uff0c\u786e\u4fdd\u5728 unlearning \u671f\u95f4\u7684\u6709\u6548\u68af\u5ea6\u4f20\u64ad\u3002", "result": "\u5728\u591a\u4e2a unlearning \u548c\u901a\u7528\u80fd\u529b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cKUnBR \u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9057\u5fd8\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6548\u7528\u3002", "conclusion": "KUnBR \u662f\u4e00\u79cd\u6709\u6548\u7684 machine unlearning \u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u5220\u9664\u6709\u5bb3\u77e5\u8bc6\u7684\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6548\u7528\u3002"}}
{"id": "2511.12439", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.12439", "abs": "https://arxiv.org/abs/2511.12439", "authors": ["Yujia Liu", "Sophia Yu", "Hongyue Jin", "Jessica Wen", "Alexander Qian", "Terrence Lee", "Mattheus Ramsis", "Gi Won Choi", "Lianhui Qin", "Xin Liu", "Edward J. Wang"], "title": "Multi-agent Self-triage System with Medical Flowcharts", "comment": null, "summary": "Online health resources and large language models (LLMs) are increasingly used as a first point of contact for medical decision-making, yet their reliability in healthcare remains limited by low accuracy, lack of transparency, and susceptibility to unverified information. We introduce a proof-of-concept conversational self-triage system that guides LLMs with 100 clinically validated flowcharts from the American Medical Association, providing a structured and auditable framework for patient decision support. The system leverages a multi-agent framework consisting of a retrieval agent, a decision agent, and a chat agent to identify the most relevant flowchart, interpret patient responses, and deliver personalized, patient-friendly recommendations, respectively. Performance was evaluated at scale using synthetic datasets of simulated conversations. The system achieved 95.29% top-3 accuracy in flowchart retrieval (N=2,000) and 99.10% accuracy in flowchart navigation across varied conversational styles and conditions (N=37,200). By combining the flexibility of free-text interaction with the rigor of standardized clinical protocols, this approach demonstrates the feasibility of transparent, accurate, and generalizable AI-assisted self-triage, with potential to support informed patient decision-making while improving healthcare resource utilization.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u4f1a\u8bdd\u5f0f\u81ea\u6211\u5206\u8bca\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5229\u7528\u7f8e\u56fd\u533b\u5b66\u4f1a\u7684\u4e34\u5e8a\u9a8c\u8bc1\u6d41\u7a0b\u56fe\uff0c\u4e3a\u60a3\u8005\u63d0\u4f9b\u7ed3\u6784\u5316\u548c\u53ef\u5ba1\u8ba1\u7684\u51b3\u7b56\u652f\u6301\u3002", "motivation": "\u5728\u7ebf\u5065\u5eb7\u8d44\u6e90\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u7528\u4f5c\u533b\u7597\u51b3\u7b56\u7684\u9996\u8981\u63a5\u89e6\u70b9\uff0c\u4f46\u5b83\u4eec\u5728\u533b\u7597\u4fdd\u5065\u65b9\u9762\u7684\u53ef\u9760\u6027\u4ecd\u7136\u53d7\u5230\u51c6\u786e\u6027\u4f4e\u3001\u7f3a\u4e4f\u900f\u660e\u5ea6\u4ee5\u53ca\u5bb9\u6613\u53d7\u5230\u672a\u7ecf\u8bc1\u5b9e\u7684\u4fe1\u606f\u7684\u5f71\u54cd\u3002", "method": "\u8be5\u7cfb\u7edf\u5229\u7528\u4e00\u4e2a\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u62ec\u68c0\u7d22\u4ee3\u7406\u3001\u51b3\u7b56\u4ee3\u7406\u548c\u804a\u5929\u4ee3\u7406\uff0c\u5206\u522b\u7528\u4e8e\u8bc6\u522b\u6700\u76f8\u5173\u7684\u6d41\u7a0b\u56fe\u3001\u89e3\u91ca\u60a3\u8005\u7684\u53cd\u5e94\u4ee5\u53ca\u63d0\u4f9b\u4e2a\u6027\u5316\u7684\u3001\u60a3\u8005\u53cb\u597d\u7684\u5efa\u8bae\u3002", "result": "\u8be5\u7cfb\u7edf\u5728\u6d41\u7a0b\u56fe\u68c0\u7d22\u4e2d\u8fbe\u523095.29%\u7684\u524d3\u540d\u51c6\u786e\u7387(N=2,000)\uff0c\u5728\u4e0d\u540c\u4f1a\u8bdd\u98ce\u683c\u548c\u6761\u4ef6\u4e0b\uff0c\u6d41\u7a0b\u56fe\u5bfc\u822a\u7684\u51c6\u786e\u7387\u4e3a99.10%(N=37,200)\u3002", "conclusion": "\u901a\u8fc7\u5c06\u81ea\u7531\u6587\u672c\u4ea4\u4e92\u7684\u7075\u6d3b\u6027\u4e0e\u6807\u51c6\u5316\u4e34\u5e8a\u534f\u8bae\u7684\u4e25\u8c28\u6027\u76f8\u7ed3\u5408\uff0c\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u900f\u660e\u3001\u51c6\u786e\u548c\u53ef\u63a8\u5e7f\u7684AI\u8f85\u52a9\u81ea\u6211\u5206\u8bca\u7684\u53ef\u884c\u6027\uff0c\u5e76\u6709\u53ef\u80fd\u652f\u6301\u77e5\u60c5\u7684\u60a3\u8005\u51b3\u7b56\uff0c\u540c\u65f6\u63d0\u9ad8\u533b\u7597\u4fdd\u5065\u8d44\u6e90\u7684\u5229\u7528\u7387\u3002"}}
{"id": "2511.12596", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.12596", "abs": "https://arxiv.org/abs/2511.12596", "authors": ["Oron Anschel", "Alon Shoshan", "Adam Botach", "Shunit Haviv Hakimi", "Asaf Gendler", "Emanuel Ben Baruch", "Nadav Bhonker", "Igor Kviatkovsky", "Manoj Aggarwal", "Gerard Medioni"], "title": "Group-Aware Reinforcement Learning for Output Diversity in Large Language Models", "comment": "EMNLP Main 2025", "summary": "Large Language Models (LLMs) often suffer from mode collapse, repeatedly generating the same few completions even when many valid answers exist, limiting their diversity across a wide range of tasks. We introduce Group-Aware Policy Optimization (GAPO), a simple extension of the recent and popular Group Relative Policy Optimization (GRPO) that computes rewards over the group as a whole. GAPO enables learning from the group-level properties such as diversity and coverage. We demonstrate GAPO using a frequency-aware reward function that encourages uniform sampling over valid LLM completions, and show that GAPO-trained models produce valid and more diverse model responses. Beyond this setup, GAPO generalizes to open-ended prompts and improves response diversity without compromising accuracy on standard LLM benchmarks (GSM8K, MATH, HumanEval, MMLU-Pro). Our code will be made publicly available.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a Group-Aware Policy Optimization (GAPO) \u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4e2d\u5e38\u89c1\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u5373\u6a21\u578b\u503e\u5411\u4e8e\u91cd\u590d\u751f\u6210\u76f8\u540c\u7684\u51e0\u4e2a\u7b54\u6848\uff0c\u4ece\u800c\u9650\u5236\u4e86\u7b54\u6848\u7684\u591a\u6837\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7ecf\u5e38\u51fa\u73b0\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u5373\u4f7f\u5b58\u5728\u8bb8\u591a\u6709\u6548\u7684\u7b54\u6848\uff0c\u4e5f\u603b\u662f\u91cd\u590d\u751f\u6210\u76f8\u540c\u7684\u51e0\u4e2a\u7b54\u6848\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u7684\u591a\u6837\u6027\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86 Group-Aware Policy Optimization (GAPO)\uff0c\u5b83\u662f\u6700\u8fd1\u6d41\u884c\u7684 Group Relative Policy Optimization (GRPO) \u7684\u4e00\u4e2a\u7b80\u5355\u6269\u5c55\uff0c\u53ef\u4ee5\u8ba1\u7b97\u6574\u4e2a\u7ec4\u7684\u5956\u52b1\u3002GAPO \u80fd\u591f\u4ece\u7ec4\u7ea7\u522b\u7684\u5c5e\u6027\uff08\u5982\u591a\u6837\u6027\u548c\u8986\u76d6\u7387\uff09\u4e2d\u5b66\u4e60\u3002\u6211\u4eec\u4f7f\u7528\u9891\u7387\u611f\u77e5\u5956\u52b1\u51fd\u6570\u6765\u6f14\u793a GAPO\uff0c\u8be5\u51fd\u6570\u9f13\u52b1\u5728\u6709\u6548\u7684 LLM \u5b8c\u6210\u4e2d\u8fdb\u884c\u5747\u5300\u91c7\u6837\u3002", "result": "GAPO \u8bad\u7ec3\u7684\u6a21\u578b\u4ea7\u751f\u6709\u6548\u4e14\u66f4\u591a\u6837\u5316\u7684\u6a21\u578b\u54cd\u5e94\u3002GAPO \u53ef\u4ee5\u63a8\u5e7f\u5230\u5f00\u653e\u5f0f\u63d0\u793a\uff0c\u5e76\u5728\u4e0d\u5f71\u54cd\u6807\u51c6 LLM \u57fa\u51c6\u6d4b\u8bd5\uff08GSM8K\u3001MATH\u3001HumanEval\u3001MMLU-Pro\uff09\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u54cd\u5e94\u591a\u6837\u6027\u3002", "conclusion": "GAPO \u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7b54\u6848\u7684\u591a\u6837\u6027\u3002"}}
{"id": "2511.12030", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12030", "abs": "https://arxiv.org/abs/2511.12030", "authors": ["Jun Zhou", "Chi Xu", "Kaifeng Tang", "Yuting Ge", "Tingrui Guo", "Li Cheng"], "title": "VPHO: Joint Visual-Physical Cue Learning and Aggregation for Hand-Object Pose Estimation", "comment": "14 pages, 9 figures, extended version of the AAAI 2026 paper \"VPHO: Joint Visual-Physical Cue Learning and Aggregation for Hand-Object Pose Estimation\"", "summary": "Estimating the 3D poses of hands and objects from a single RGB image is a fundamental yet challenging problem, with broad applications in augmented reality and human-computer interaction. Existing methods largely rely on visual cues alone, often producing results that violate physical constraints such as interpenetration or non-contact. Recent efforts to incorporate physics reasoning typically depend on post-optimization or non-differentiable physics engines, which compromise visual consistency and end-to-end trainability. To overcome these limitations, we propose a novel framework that jointly integrates visual and physical cues for hand-object pose estimation. This integration is achieved through two key ideas: 1) joint visual-physical cue learning: The model is trained to extract 2D visual cues and 3D physical cues, thereby enabling more comprehensive representation learning for hand-object interactions; 2) candidate pose aggregation: A novel refinement process that aggregates multiple diffusion-generated candidate poses by leveraging both visual and physical predictions, yielding a final estimate that is visually consistent and physically plausible. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art approaches in both pose accuracy and physical plausibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5355\u5f20RGB\u56fe\u50cf\u4e2d\u4f30\u8ba1\u624b\u548c\u7269\u4f53\u76843D\u59ff\u52bf\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u89c6\u89c9\u548c\u7269\u7406\u7ebf\u7d22\uff0c\u4ee5\u63d0\u9ad8\u59ff\u52bf\u51c6\u786e\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u7ebf\u7d22\uff0c\u5bb9\u6613\u8fdd\u53cd\u7269\u7406\u7ea6\u675f\u3002\u6700\u8fd1\u7ed3\u5408\u7269\u7406\u63a8\u7406\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u540e\u4f18\u5316\u6216\u4e0d\u53ef\u5fae\u7684\u7269\u7406\u5f15\u64ce\uff0c\u635f\u5bb3\u4e86\u89c6\u89c9\u4e00\u81f4\u6027\u548c\u7aef\u5230\u7aef\u7684\u53ef\u8bad\u7ec3\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u89c6\u89c9-\u7269\u7406\u7ebf\u7d22\u5b66\u4e60\u548c\u5019\u9009\u59ff\u52bf\u805a\u5408\u6765\u5b9e\u73b0\u89c6\u89c9\u548c\u7269\u7406\u7ebf\u7d22\u7684\u96c6\u6210\u3002\u6a21\u578b\u88ab\u8bad\u7ec3\u4ee5\u63d0\u53d62D\u89c6\u89c9\u7ebf\u7d22\u548c3D\u7269\u7406\u7ebf\u7d22\uff0c\u5e76\u5229\u7528\u89c6\u89c9\u548c\u7269\u7406\u9884\u6d4b\u6765\u805a\u5408\u591a\u4e2a\u6269\u6563\u751f\u6210\u7684\u5019\u9009\u59ff\u52bf\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u59ff\u52bf\u51c6\u786e\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u6c34\u5e73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u624b-\u7269\u4f53\u59ff\u52bf\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u3002"}}
{"id": "2511.11668", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.11668", "abs": "https://arxiv.org/abs/2511.11668", "authors": ["Chase van de Geijn", "Ayush Paliwal", "Timo L\u00fcddecke", "Alexander S. Ecker"], "title": "Do traveling waves make good positional encodings?", "comment": null, "summary": "Transformers rely on positional encoding to compensate for the inherent permutation invariance of self-attention. Traditional approaches use absolute sinusoidal embeddings or learned positional vectors, while more recent methods emphasize relative encodings to better capture translation equivariances. In this work, we propose RollPE, a novel positional encoding mechanism based on traveling waves, implemented by applying a circular roll operation to the query and key tensors in self-attention. This operation induces a relative shift in phase across positions, allowing the model to compute attention as a function of positional differences rather than absolute indices. We show this simple method significantly outperforms traditional absolute positional embeddings and is comparable to RoPE. We derive a continuous case of RollPE which implicitly imposes a topographic structure on the query and key space. We further derive a mathematical equivalence of RollPE to a particular configuration of RoPE. Viewing RollPE through the lens of traveling waves may allow us to simplify RoPE and relate it to processes of information flow in the brain.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4f4d\u7f6e\u7f16\u7801\u673a\u5236RollPE\uff0c\u5b83\u57fa\u4e8e\u884c\u6ce2\uff0c\u901a\u8fc7\u5bf9\u81ea\u6ce8\u610f\u529b\u4e2d\u7684\u67e5\u8be2\u548c\u952e\u5f20\u91cf\u5e94\u7528\u5faa\u73af\u6eda\u52a8\u64cd\u4f5c\u6765\u5b9e\u73b0\u3002", "motivation": "Transformer\u4f9d\u8d56\u4e8e\u4f4d\u7f6e\u7f16\u7801\u6765\u8865\u507f\u81ea\u6ce8\u610f\u529b\u7684\u56fa\u6709\u7f6e\u6362\u4e0d\u53d8\u6027\u3002\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528\u7edd\u5bf9\u6b63\u5f26\u5d4c\u5165\u6216\u5b66\u4e60\u7684\u4f4d\u7f6e\u5411\u91cf\uff0c\u800c\u6700\u8fd1\u7684\u65b9\u6cd5\u5f3a\u8c03\u76f8\u5bf9\u7f16\u7801\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u5e73\u79fb\u7b49\u65b9\u5dee\u3002", "method": "\u5bf9\u81ea\u6ce8\u610f\u529b\u4e2d\u7684\u67e5\u8be2\u548c\u952e\u5f20\u91cf\u5e94\u7528\u5faa\u73af\u6eda\u52a8\u64cd\u4f5c\uff0c\u4ece\u800c\u5728\u4f4d\u7f6e\u4e4b\u95f4\u4ea7\u751f\u76f8\u5bf9\u7684\u76f8\u4f4d\u79fb\u52a8\uff0c\u4ece\u800c\u4f7f\u6a21\u578b\u80fd\u591f\u5c06\u6ce8\u610f\u529b\u8ba1\u7b97\u4e3a\u4f4d\u7f6e\u5dee\u5f02\u7684\u51fd\u6570\uff0c\u800c\u4e0d\u662f\u7edd\u5bf9\u7d22\u5f15\u3002", "result": "\u8be5\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u4f20\u7edf\u7684\u7edd\u5bf9\u4f4d\u7f6e\u5d4c\u5165\uff0c\u5e76\u4e14\u4e0eRoPE\u76f8\u5f53\u3002", "conclusion": "\u4ece\u884c\u6ce2\u7684\u89d2\u5ea6\u89c2\u5bdfRollPE\u53ef\u4ee5\u7b80\u5316RoPE\uff0c\u5e76\u5c06\u5176\u4e0e\u5927\u8111\u4e2d\u7684\u4fe1\u606f\u6d41\u8fc7\u7a0b\u8054\u7cfb\u8d77\u6765\u3002"}}
{"id": "2511.12485", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12485", "abs": "https://arxiv.org/abs/2511.12485", "authors": ["Pengze Li", "Jiaqi Liu", "Junchi Yu", "Lihao Liu", "Mingyu Ding", "Wanli Ouyang", "Shixiang Tang", "Xi Chen"], "title": "ARCHE: A Novel Task to Evaluate LLMs on Latent Reasoning Chain Extraction", "comment": "Accepted to AAAI 2026", "summary": "Large language models (LLMs) are increasingly used in scientific domains. While they can produce reasoning-like content via methods such as chain-of-thought prompting, these outputs are typically unstructured and informal, obscuring whether models truly understand the fundamental reasoning paradigms that underpin scientific inference. To address this, we introduce a novel task named Latent Reasoning Chain Extraction (ARCHE), in which models must decompose complex reasoning arguments into combinations of standard reasoning paradigms in the form of a Reasoning Logic Tree (RLT). In RLT, all reasoning steps are explicitly categorized as one of three variants of Peirce's fundamental inference modes: deduction, induction, or abduction. To facilitate this task, we release ARCHE Bench, a new benchmark derived from 70 Nature Communications articles, including more than 1,900 references and 38,000 viewpoints. We propose two logic-aware evaluation metrics: Entity Coverage (EC) for content completeness and Reasoning Edge Accuracy (REA) for step-by-step logical validity. Evaluations on 10 leading LLMs on ARCHE Bench reveal that models exhibit a trade-off between REA and EC, and none are yet able to extract a complete and standard reasoning chain. These findings highlight a substantial gap between the abilities of current reasoning models and the rigor required for scientific argumentation.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5176\u63a8\u7406\u8fc7\u7a0b\u975e\u7ed3\u6784\u5316\u4e14\u975e\u6b63\u5f0f\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u6f5c\u53d8\u91cf\u63a8\u7406\u94fe\u63d0\u53d6\uff08ARCHE\uff09\u4efb\u52a1\uff0c\u65e8\u5728\u5c06\u590d\u6742\u63a8\u7406\u5206\u89e3\u4e3a\u6807\u51c6\u7684\u63a8\u7406\u8303\u5f0f\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b1900\u591a\u7bc7\u53c2\u8003\u6587\u732e\u548c38000\u4e2a\u89c2\u70b9\u7684ARCHE Bench\u57fa\u51c6\u3002\u5bf910\u4e2a\u9886\u5148\u7684LLM\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u6a21\u578b\u5728\u63a8\u7406\u8fb9\u7f18\u51c6\u786e\u6027\u548c\u5b9e\u4f53\u8986\u76d6\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u4e14\u65e0\u6cd5\u63d0\u53d6\u5b8c\u6574\u548c\u6807\u51c6\u7684\u63a8\u7406\u94fe\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u63a8\u7406\u6a21\u578b\u7684\u80fd\u529b\u4e0e\u79d1\u5b66\u8bba\u8bc1\u7684\u4e25\u8c28\u6027\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u79d1\u5b66\u63a8\u7406\u7684\u57fa\u672c\u8303\u5f0f\u3002", "method": "\u63d0\u51fa\u6f5c\u53d8\u91cf\u63a8\u7406\u94fe\u63d0\u53d6\uff08ARCHE\uff09\u4efb\u52a1\uff0c\u5c06\u590d\u6742\u63a8\u7406\u5206\u89e3\u4e3a\u6f14\u7ece\u3001\u5f52\u7eb3\u6216\u6eaf\u56e0\u4e09\u79cd\u63a8\u7406\u6a21\u5f0f\u7684\u7ec4\u5408\uff0c\u5f62\u6210\u63a8\u7406\u903b\u8f91\u6811\uff08RLT\uff09\u3002", "result": "\u5bf910\u4e2a\u9886\u5148\u7684LLM\u5728ARCHE Bench\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u6a21\u578b\u5728\u63a8\u7406\u8fb9\u7f18\u51c6\u786e\u6027\u548c\u5b9e\u4f53\u8986\u76d6\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u4e14\u65e0\u6cd5\u63d0\u53d6\u5b8c\u6574\u548c\u6807\u51c6\u7684\u63a8\u7406\u94fe\u3002", "conclusion": "\u5f53\u524d\u63a8\u7406\u6a21\u578b\u7684\u80fd\u529b\u4e0e\u79d1\u5b66\u8bba\u8bc1\u7684\u4e25\u8c28\u6027\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002"}}
{"id": "2511.12609", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12609", "abs": "https://arxiv.org/abs/2511.12609", "authors": ["Yunxin Li", "Xinyu Chen", "Shenyuan Jiang", "Haoyuan Shi", "Zhenyu Liu", "Xuanyu Zhang", "Nanhao Deng", "Zhenran Xu", "Yicheng Ma", "Meishan Zhang", "Baotian Hu", "Min Zhang"], "title": "Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data", "comment": "47 pages,10 Figures, Project Website: https://idealistxy.github.io/Uni-MoE-v2.github.io/; Codes: https://github.com/HITsz-TMG/Uni-MoE", "summary": "We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.", "AI": {"tldr": "Uni-MoE 2.0 \u662f\u4e00\u4e2a\u5f00\u6e90\u5168\u6a21\u6001\u5927\u6a21\u578b\uff0c\u63d0\u5347\u4e86 Lychee \u7cfb\u5217\u6a21\u578b\u5728\u591a\u6a21\u6001\u7406\u89e3\u3001\u63a8\u7406\u548c\u751f\u6210\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u6784\u5efa\u4e00\u4e2a\u5728\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u66f4\u5f3a\u7684\u591a\u6a21\u6001\u6a21\u578b\u3002", "method": "1. \u52a8\u6001\u5bb9\u91cf\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u8bbe\u8ba1\u30022. \u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u5f3a\u5316\u7b56\u7565\u8fdb\u884c\u589e\u5f3a\u30023. \u7cbe\u5fc3\u7b56\u5212\u7684\u591a\u6a21\u6001\u6570\u636e\u5339\u914d\u6280\u672f\u3002", "result": "\u5728 85 \u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUni-MoE 2.0 \u8fbe\u5230\u4e86 SOTA \u6216\u6781\u5177\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5728\u8d85\u8fc7 50 \u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8fc7\u4e86 Qwen2.5-Omni\u3002", "conclusion": "Uni-MoE 2.0 \u5728\u89c6\u9891\u7406\u89e3\u3001\u5168\u6a21\u6001\u7406\u89e3\u548c\u89c6\u542c\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u5e76\u5728\u957f\u683c\u5f0f\u8bed\u97f3\u5904\u7406\u3001\u4f4e\u7ea7\u56fe\u50cf\u5904\u7406\u548c\u53ef\u63a7\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u9886\u5148\u3002"}}
{"id": "2511.12032", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12032", "abs": "https://arxiv.org/abs/2511.12032", "authors": ["Guotao Liang", "Baoquan Zhang", "Zhiyuan Wen", "Zihao Han", "Yunming Ye"], "title": "Improved Masked Image Generation with Knowledge-Augmented Token Representations", "comment": "AAAI-26", "summary": "Masked image generation (MIG) has demonstrated remarkable efficiency and high-fidelity images by enabling parallel token prediction. Existing methods typically rely solely on the model itself to learn semantic dependencies among visual token sequences. However, directly learning such semantic dependencies from data is challenging because the individual tokens lack clear semantic meanings, and these sequences are usually long. To address this limitation, we propose a novel Knowledge-Augmented Masked Image Generation framework, named KA-MIG, which introduces explicit knowledge of token-level semantic dependencies (\\emph{i.e.}, extracted from the training data) as priors to learn richer representations for improving performance. In particular, we explore and identify three types of advantageous token knowledge graphs, including two positive and one negative graphs (\\emph{i.e.}, the co-occurrence graph, the semantic similarity graph, and the position-token incompatibility graph). Based on three prior knowledge graphs, we design a graph-aware encoder to learn token and position-aware representations. After that, a lightweight fusion mechanism is introduced to integrate these enriched representations into the existing MIG methods. Resorting to such prior knowledge, our method effectively enhances the model's ability to capture semantic dependencies, leading to improved generation quality. Experimental results demonstrate that our method improves upon existing MIG for class-conditional image generation on ImageNet.", "AI": {"tldr": "KA-MIG\u6846\u67b6\u901a\u8fc7\u5f15\u5165token\u7ea7\u522b\u8bed\u4e49\u4f9d\u8d56\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u63d0\u5347\u4e86\u5e76\u884ctoken\u9884\u6d4b\u7684\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709MIG\u65b9\u6cd5\u96be\u4ee5\u5b66\u4e60\u89c6\u89c9token\u5e8f\u5217\u95f4\u7684\u8bed\u4e49\u4f9d\u8d56\uff0c\u56e0\u4e3atoken\u7f3a\u4e4f\u660e\u786e\u8bed\u4e49\u4e14\u5e8f\u5217\u8fc7\u957f\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u56fe\u611f\u77e5\u7f16\u7801\u5668\uff0c\u5229\u7528\u5171\u73b0\u56fe\u3001\u8bed\u4e49\u76f8\u4f3c\u56fe\u548c\u4f4d\u7f6e-token\u4e0d\u517c\u5bb9\u56fe\u4e09\u79cdtoken\u77e5\u8bc6\u56fe\uff0c\u5b66\u4e60token\u548c\u4f4d\u7f6e\u611f\u77e5\u7684\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u878d\u5408\u673a\u5236\u6574\u5408\u5230\u73b0\u6709MIG\u65b9\u6cd5\u4e2d\u3002", "result": "\u5728ImageNet\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7c7b\u522b\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709MIG\u65b9\u6cd5\u3002", "conclusion": "KA-MIG\u6846\u67b6\u901a\u8fc7\u5148\u9a8c\u77e5\u8bc6\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u6355\u6349\u8bed\u4e49\u4f9d\u8d56\u7684\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2511.11669", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.11669", "abs": "https://arxiv.org/abs/2511.11669", "authors": ["Dmytro Hospodarchuk"], "title": "H-Model: Dynamic Neural Architectures for Adaptive Processing", "comment": "Independent research report, 24 pages including references and figures", "summary": "This article explores the design and experimentation of a neural network architecture capable of dynamically adjusting its internal structure based on the input data. The proposed model introduces a routing mechanism that allows each layer to influence how its outputs are propagated through the network, enabling iterative and adaptive computation. This concept is loosely inspired by the idea of thought processes and dynamic reasoning, where information flow is conditioned not only on the data itself, but also on the internal state of the system.\n  It is important to note that this work does not aim to compete with state-of-the-art language models in terms of performance. Instead, it presents a conceptual prototype-an architectural framework that opens up a new direction for exploring adaptable and potentially more interpretable networks. The goal is not optimization of existing benchmarks but rather the proposal of a system that can learn not only representations, but also the structure of computation itself.\n  Due to practical constraints in computing resources and data, this study remains a preliminary investigation. Nevertheless, initial observations show promise, and the architecture's full potential can only be evaluated in future experiments under more favorable computational conditions.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u80fd\u591f\u6839\u636e\u8f93\u5165\u6570\u636e\u52a8\u6001\u8c03\u6574\u5176\u5185\u90e8\u7ed3\u6784\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3002", "motivation": "\u63a2\u7d22\u8bbe\u8ba1\u4e00\u79cd\u65b0\u578b\u7684\u3001\u53ef\u89e3\u91ca\u7684\u81ea\u9002\u5e94\u795e\u7ecf\u7f51\u7edc\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8def\u7531\u673a\u5236\uff0c\u5141\u8bb8\u6bcf\u4e00\u5c42\u5f71\u54cd\u5176\u8f93\u51fa\u5728\u7f51\u7edc\u4e2d\u7684\u4f20\u64ad\u65b9\u5f0f\uff0c\u4ece\u800c\u5b9e\u73b0\u8fed\u4ee3\u548c\u81ea\u9002\u5e94\u8ba1\u7b97\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u51fa\u6f5c\u529b\u3002", "conclusion": "\u8be5\u67b6\u6784\u6709\u6f5c\u529b\u5b66\u4e60\u8868\u5f81\u548c\u8ba1\u7b97\u7ed3\u6784\u672c\u8eab\uff0c\u4f46\u9700\u8981\u5728\u66f4\u6709\u5229\u7684\u8ba1\u7b97\u6761\u4ef6\u4e0b\u8fdb\u884c\u8fdb\u4e00\u6b65\u8bc4\u4f30\u3002"}}
{"id": "2511.12563", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12563", "abs": "https://arxiv.org/abs/2511.12563", "authors": ["Eljas Linna", "Kestutis Baltakys", "Alexandros Iosifidis", "Juho Kanniainen"], "title": "LOBERT: Generative AI Foundation Model for Limit Order Book Messages", "comment": "Submission for NeurIPS 2025 GenAI in Finance Workshop", "summary": "Modeling the dynamics of financial Limit Order Books (LOB) at the message level is challenging due to irregular event timing, rapid regime shifts, and the reactions of high-frequency traders to visible order flow. Previous LOB models require cumbersome data representations and lack adaptability outside their original tasks, leading us to introduce LOBERT, a general-purpose encoder-only foundation model for LOB data suitable for downstream fine-tuning. LOBERT adapts the original BERT architecture for LOB data by using a novel tokenization scheme that treats complete multi-dimensional messages as single tokens while retaining continuous representations of price, volume, and time. With these methods, LOBERT achieves leading performance in tasks such as predicting mid-price movements and next messages, while reducing the required context length compared to previous methods.", "AI": {"tldr": "LOBERT is a BERT-based foundation model for Limit Order Book (LOB) data.", "motivation": "Modeling LOB dynamics is challenging due to irregular timing, rapid shifts, and high-frequency trader reactions. Previous models are cumbersome and lack adaptability.", "method": "Adapts BERT with a novel tokenization scheme treating multi-dimensional messages as single tokens, retaining continuous representations of price, volume, and time.", "result": "LOBERT achieves leading performance in mid-price movement and next message prediction, reducing context length.", "conclusion": "LOBERT is a general-purpose encoder suitable for downstream fine-tuning."}}
{"id": "2511.12630", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12630", "abs": "https://arxiv.org/abs/2511.12630", "authors": ["Maoqi Liu", "Quan Fang", "Yang Yang", "Can Zhao", "Kaiquan Cai"], "title": "Knots: A Large-Scale Multi-Agent Enhanced Expert-Annotated Dataset and LLM Prompt Optimization for NOTAM Semantic Parsing", "comment": "Accepted to Advanced Engineering Informatics", "summary": "Notice to Air Missions (NOTAMs) serve as a critical channel for disseminating key flight safety information, yet their complex linguistic structures and implicit reasoning pose significant challenges for automated parsing. Existing research mainly focuses on surface-level tasks such as classification and named entity recognition, lacking deep semantic understanding. To address this gap, we propose NOTAM semantic parsing, a task emphasizing semantic inference and the integration of aviation domain knowledge to produce structured, inference-rich outputs. To support this task, we construct Knots (Knowledge and NOTAM Semantics), a high-quality dataset of 12,347 expert-annotated NOTAMs covering 194 Flight Information Regions, enhanced through a multi-agent collaborative framework for comprehensive field discovery. We systematically evaluate a wide range of prompt-engineering strategies and model-adaptation techniques, achieving substantial improvements in aviation text understanding and processing. Our experimental results demonstrate the effectiveness of the proposed approach and offer valuable insights for automated NOTAM analysis systems. Our code is available at: https://github.com/Estrellajer/Knots.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684NOTAM\u8bed\u4e49\u5206\u6790\u4efb\u52a1\uff0c\u65e8\u5728\u901a\u8fc7\u8bed\u4e49\u63a8\u7406\u548c\u822a\u7a7a\u9886\u57df\u77e5\u8bc6\u6574\u5408\uff0c\u5b9e\u73b0\u5bf9NOTAMs\u7684\u6df1\u5c42\u8bed\u4e49\u7406\u89e3\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u8bf8\u5982\u5206\u7c7b\u548c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7b49\u8868\u9762\u4efb\u52a1\uff0c\u7f3a\u4e4f\u6df1\u5c42\u8bed\u4e49\u7406\u89e3\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6Knots\uff0c\u5e76\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u4e00\u7cfb\u5217prompt-engineering\u7b56\u7565\u548c\u6a21\u578b\u81ea\u9002\u5e94\u6280\u672f\u3002", "result": "\u5728\u822a\u7a7a\u6587\u672c\u7406\u89e3\u548c\u5904\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u81ea\u52a8\u5316NOTAM\u5206\u6790\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2511.12034", "categories": ["cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.12034", "abs": "https://arxiv.org/abs/2511.12034", "authors": ["Xiaohao Liu", "Xiaobo Xia", "Jiaheng Wei", "Shuo Yang", "Xiu Su", "See-Kiong Ng", "Tat-Seng Chua"], "title": "Calibrated Multimodal Representation Learning with Missing Modalities", "comment": null, "summary": "Multimodal representation learning harmonizes distinct modalities by aligning them into a unified latent space. Recent research generalizes traditional cross-modal alignment to produce enhanced multimodal synergy but requires all modalities to be present for a common instance, making it challenging to utilize prevalent datasets with missing modalities. We provide theoretical insights into this issue from an anchor shift perspective. Observed modalities are aligned with a local anchor that deviates from the optimal one when all modalities are present, resulting in an inevitable shift. To address this, we propose CalMRL for multimodal representation learning to calibrate incomplete alignments caused by missing modalities. Specifically, CalMRL leverages the priors and the inherent connections among modalities to model the imputation for the missing ones at the representation level. To resolve the optimization dilemma, we employ a bi-step learning method with the closed-form solution of the posterior distribution of shared latents. We validate its mitigation of anchor shift and convergence with theoretical guidance. By equipping the calibrated alignment with the existing advanced method, we offer new flexibility to absorb data with missing modalities, which is originally unattainable. Extensive experiments and comprehensive analyses demonstrate the superiority of CalMRL. Our code, model checkpoints, and evaluation raw data will be publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u8868\u5f81\u5b66\u4e60\u65b9\u6cd5 CalMRL\uff0c\u4ee5\u89e3\u51b3\u7531\u4e8e\u6a21\u6001\u7f3a\u5931\u5bfc\u81f4\u7684\u4e0d\u5b8c\u6574\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u878d\u5408\u65f6\u9700\u8981\u6240\u6709\u6a21\u6001\u90fd\u5b58\u5728\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u6a21\u6001\u7f3a\u5931\u666e\u904d\u5b58\u5728\u7684\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\u3002\u672c\u6587\u4ece\u951a\u70b9\u504f\u79fb\u7684\u89d2\u5ea6\u5206\u6790\u4e86\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "CalMRL \u5229\u7528\u5148\u9a8c\u77e5\u8bc6\u548c\u6a21\u6001\u95f4\u7684\u5185\u5728\u8054\u7cfb\uff0c\u5728\u8868\u5f81\u5c42\u9762\u5efa\u6a21\u7f3a\u5931\u6a21\u6001\u7684\u63a8\u65ad\uff0c\u5e76\u901a\u8fc7\u53cc\u6b65\u5b66\u4e60\u65b9\u6cd5\u548c\u5171\u4eab\u9690\u53d8\u91cf\u540e\u9a8c\u5206\u5e03\u7684\u95ed\u5f0f\u89e3\u6765\u89e3\u51b3\u4f18\u5316\u96be\u9898\u3002", "result": "\u7406\u8bba\u5206\u6790\u9a8c\u8bc1\u4e86 CalMRL \u5bf9\u951a\u70b9\u504f\u79fb\u7684\u7f13\u89e3\u548c\u6536\u655b\u6027\uff0c\u5927\u91cf\u5b9e\u9a8c\u548c\u7efc\u5408\u5206\u6790\u8868\u660e\u4e86 CalMRL \u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "CalMRL \u80fd\u591f\u7075\u6d3b\u5730\u5904\u7406\u5305\u542b\u7f3a\u5931\u6a21\u6001\u7684\u6570\u636e\uff0c\u5e76\u63d0\u5347\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2511.11671", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.11671", "abs": "https://arxiv.org/abs/2511.11671", "authors": ["Alina Deriyeva", "Benjamin Paassen"], "title": "Evaluation of LLM-based Explanations for a Learning Analytics Dashboard", "comment": null, "summary": "Learning Analytics Dashboards can be a powerful tool to support self-regulated learning in Digital Learning Environments and promote development of meta-cognitive skills, such as reflection. However, their effectiveness can be affected by the interpretability of the data they provide. To assist in the interpretation, we employ a large language model to generate verbal explanations of the data in the dashboard and evaluate it against a standalone dashboard and explanations provided by human teachers in an expert study with university level educators (N=12). We find that the LLM-based explanations of the skill state presented in the dashboard, as well as general recommendations on how to proceed with learning within the course are significantly more favored compared to the other conditions. This indicates that using LLMs for interpretation purposes can enhance the learning experience for learners while maintaining the pedagogical standards approved by teachers.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e3a\u5b66\u4e60\u5206\u6790\u4eea\u8868\u677f\u751f\u6210\u53e3\u5934\u89e3\u91ca\uff0c\u4ee5\u63d0\u9ad8\u5176\u6709\u6548\u6027\u5e76\u4fc3\u8fdb\u5143\u8ba4\u77e5\u6280\u80fd\u7684\u53d1\u5c55\u3002", "motivation": "\u5b66\u4e60\u5206\u6790\u4eea\u8868\u677f\u53ef\u4ee5\u652f\u6301\u6570\u5b57\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u81ea\u6211\u8c03\u8282\u5b66\u4e60\uff0c\u4f46\u5176\u6709\u6548\u6027\u53d7\u5230\u6570\u636e\u53ef\u89e3\u91ca\u6027\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3a\u4eea\u8868\u677f\u4e2d\u7684\u6570\u636e\u751f\u6210\u53e3\u5934\u89e3\u91ca\uff0c\u5e76\u4e0e\u72ec\u7acb\u4eea\u8868\u677f\u548c\u4eba\u7c7b\u6559\u5e08\u63d0\u4f9b\u7684\u89e3\u91ca\u8fdb\u884c\u6bd4\u8f83\uff0c\u8fdb\u884c\u4e13\u5bb6\u7814\u7a76\uff08N=12\uff09\u3002", "result": "\u57fa\u4e8eLLM\u7684\u6280\u80fd\u72b6\u6001\u89e3\u91ca\u548c\u5b66\u4e60\u5efa\u8bae\u6bd4\u5176\u4ed6\u6761\u4ef6\u66f4\u53d7\u6b22\u8fce\u3002", "conclusion": "\u4f7f\u7528LLM\u8fdb\u884c\u89e3\u91ca\u53ef\u4ee5\u589e\u5f3a\u5b66\u4e60\u8005\u7684\u5b66\u4e60\u4f53\u9a8c\uff0c\u540c\u65f6\u4fdd\u6301\u6559\u5e08\u8ba4\u53ef\u7684\u6559\u5b66\u6807\u51c6\u3002"}}
{"id": "2511.12579", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2511.12579", "abs": "https://arxiv.org/abs/2511.12579", "authors": ["Yongwen Ren", "Chao Wang", "Peng Du", "Chuan Qin", "Dazhong Shen", "Hui Xiong"], "title": "Enhancing Conversational Recommender Systems with Tree-Structured Knowledge and Pretrained Language Models", "comment": null, "summary": "Recent advances in pretrained language models (PLMs) have significantly improved conversational recommender systems (CRS), enabling more fluent and context-aware interactions. To further enhance accuracy and mitigate hallucination, many methods integrate PLMs with knowledge graphs (KGs), but face key challenges: failing to fully exploit PLM reasoning over graph relationships, indiscriminately incorporating retrieved knowledge without context filtering, and neglecting collaborative preferences in multi-turn dialogues. To this end, we propose PCRS-TKA, a prompt-based framework employing retrieval-augmented generation to integrate PLMs with KGs. PCRS-TKA constructs dialogue-specific knowledge trees from KGs and serializes them into texts, enabling structure-aware reasoning while capturing rich entity semantics. Our approach selectively filters context-relevant knowledge and explicitly models collaborative preferences using specialized supervision signals. A semantic alignment module harmonizes heterogeneous inputs, reducing noise and enhancing accuracy. Extensive experiments demonstrate that PCRS-TKA consistently outperforms all baselines in both recommendation and conversational quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPCRS-TKA\u7684prompt\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5c06PLM\u4e0eKG\u96c6\u6210\uff0c\u5728\u63a8\u8350\u548c\u5bf9\u8bdd\u8d28\u91cf\u65b9\u9762\u90fd\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u51c6\u786e\u6027\u5e76\u51cf\u8f7b\u5e7b\u89c9\uff0c\u8bb8\u591a\u65b9\u6cd5\u5c06PLM\u4e0e\u77e5\u8bc6\u56fe\uff08KG\uff09\u96c6\u6210\uff0c\u4f46\u9762\u4e34\u5173\u952e\u6311\u6218\uff1a\u672a\u80fd\u5145\u5206\u5229\u7528PLM\u5bf9\u56fe\u5f62\u5173\u7cfb\u7684\u63a8\u7406\uff0c\u4e0d\u52a0\u9009\u62e9\u5730\u7ed3\u5408\u68c0\u7d22\u5230\u7684\u77e5\u8bc6\u800c\u6ca1\u6709\u4e0a\u4e0b\u6587\u8fc7\u6ee4\uff0c\u5e76\u5ffd\u7565\u4e86\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u534f\u4f5c\u504f\u597d\u3002", "method": "PCRS-TKA\u6784\u5efa\u7279\u5b9a\u4e8e\u5bf9\u8bdd\u7684\u77e5\u8bc6\u6811\uff0c\u5e76\u5c06\u5176\u5e8f\u5217\u5316\u4e3a\u6587\u672c\uff0c\u4ece\u800c\u5b9e\u73b0\u7ed3\u6784\u611f\u77e5\u63a8\u7406\uff0c\u540c\u65f6\u6355\u83b7\u4e30\u5bcc\u7684\u5b9e\u4f53\u8bed\u4e49\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u9009\u62e9\u6027\u5730\u8fc7\u6ee4\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u77e5\u8bc6\uff0c\u5e76\u4f7f\u7528\u4e13\u95e8\u7684\u76d1\u7763\u4fe1\u53f7\u663e\u5f0f\u5730\u5efa\u6a21\u534f\u4f5c\u504f\u597d\u3002\u8bed\u4e49\u5bf9\u9f50\u6a21\u5757\u534f\u8c03\u5f02\u6784\u8f93\u5165\uff0c\u51cf\u5c11\u566a\u58f0\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "result": "PCRS-TKA\u5728\u63a8\u8350\u548c\u5bf9\u8bdd\u8d28\u91cf\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u3002", "conclusion": "PCRS-TKA\u5728\u63a8\u8350\u548c\u5bf9\u8bdd\u8d28\u91cf\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2511.12661", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.12661", "abs": "https://arxiv.org/abs/2511.12661", "authors": ["Yuchen Wu", "Liang Ding", "Li Shen", "Dacheng Tao"], "title": "Reason-KE++: Aligning the Process, Not Just the Outcome, for Faithful LLM Knowledge Editing", "comment": null, "summary": "Aligning Large Language Models (LLMs) to be faithful to new knowledge in complex, multi-hop reasoning tasks is a critical, yet unsolved, challenge. We find that SFT-based methods, e.g., Reason-KE, while state-of-the-art, suffer from a \"faithfulness gap\": they optimize for format mimicry rather than sound reasoning. This gap enables the LLM's powerful parametric priors to override new contextual facts, resulting in critical factual hallucinations (e.g., incorrectly reasoning \"Houston\" from \"NASA\" despite an explicit edit). To solve this core LLM alignment problem, we propose Reason-KE++, an SFT+RL framework that instills process-level faithfulness. Its core is a Stage-aware Reward mechanism that provides dense supervision for intermediate reasoning steps (e.g., Decomposition, Sub-answer Correctness). Crucially, we identify that naive outcome-only RL is a deceptive trap for LLM alignment: it collapses reasoning integrity (e.g., 19.00% Hop acc) while superficially boosting final accuracy. Our process-aware framework sets a new SOTA of 95.48% on MQUAKE-CF-3k (+5.28%), demonstrating that for complex tasks, aligning the reasoning process is essential for building trustworthy LLMs.", "AI": {"tldr": "Reason-KE++\u662f\u4e00\u4e2aSFT+RL\u6846\u67b6\uff0c\u901a\u8fc7\u9636\u6bb5\u611f\u77e5\u5956\u52b1\u673a\u5236\uff0c\u63d0\u9ad8LLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u5bf9\u65b0\u77e5\u8bc6\u7684\u5fe0\u5b9e\u5ea6\uff0c\u4f18\u4e8e\u5355\u7eaf\u7684\u7ed3\u679c\u76d1\u7763RL\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709SFT\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u201c\u5fe0\u5b9e\u6027\u5dee\u8ddd\u201d\uff0cLLM\u4f1a\u5ffd\u7565\u65b0\u77e5\u8bc6\u800c\u4f9d\u8d56\u81ea\u8eab\u53c2\u6570\u5148\u9a8c\uff0c\u5bfc\u81f4\u4e8b\u5b9e\u5e7b\u89c9\u3002", "method": "\u63d0\u51faReason-KE++\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u9636\u6bb5\u611f\u77e5\u5956\u52b1\u673a\u5236\uff0c\u5bf9\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u8fdb\u884c\u5bc6\u96c6\u76d1\u7763\u3002", "result": "\u5728MQUAKE-CF-3k\u6570\u636e\u96c6\u4e0a\u8fbe\u523095.48%\u7684\u65b0SOTA\uff0c\u6bd4\u4e4b\u524d\u7684\u6a21\u578b\u63d0\u9ad8\u4e865.28%\u3002", "conclusion": "\u5bf9\u4e8e\u590d\u6742\u4efb\u52a1\uff0c\u5bf9\u9f50\u63a8\u7406\u8fc7\u7a0b\u5bf9\u4e8e\u6784\u5efa\u503c\u5f97\u4fe1\u8d56\u7684LLM\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2511.12040", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.12040", "abs": "https://arxiv.org/abs/2511.12040", "authors": ["Xinyuan Hu", "Changyue Shi", "Chuxiao Yang", "Minghao Chen", "Jiajun Ding", "Tao Wei", "Chen Wei", "Zhou Yu", "Min Tan"], "title": "SRSplat: Feed-Forward Super-Resolution Gaussian Splatting from Sparse Multi-View Images", "comment": "AAAI2026-Oral. Project Page: https://xinyuanhu66.github.io/SRSplat/", "summary": "Feed-forward 3D reconstruction from sparse, low-resolution (LR) images is a crucial capability for real-world applications, such as autonomous driving and embodied AI. However, existing methods often fail to recover fine texture details. This limitation stems from the inherent lack of high-frequency information in LR inputs. To address this, we propose \\textbf{SRSplat}, a feed-forward framework that reconstructs high-resolution 3D scenes from only a few LR views. Our main insight is to compensate for the deficiency of texture information by jointly leveraging external high-quality reference images and internal texture cues. We first construct a scene-specific reference gallery, generated for each scene using Multimodal Large Language Models (MLLMs) and diffusion models. To integrate this external information, we introduce the \\textit{Reference-Guided Feature Enhancement (RGFE)} module, which aligns and fuses features from the LR input images and their reference twin image. Subsequently, we train a decoder to predict the Gaussian primitives using the multi-view fused feature obtained from \\textit{RGFE}. To further refine predicted Gaussian primitives, we introduce \\textit{Texture-Aware Density Control (TADC)}, which adaptively adjusts Gaussian density based on the internal texture richness of the LR inputs. Extensive experiments demonstrate that our SRSplat outperforms existing methods on various datasets, including RealEstate10K, ACID, and DTU, and exhibits strong cross-dataset and cross-resolution generalization capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSRSplat\u7684\u524d\u9988\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u7684\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u91cd\u5efa\u9ad8\u5206\u8fa8\u73873D\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u4ece\u7a00\u758f\u7684\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u6062\u590d\u7cbe\u7ec6\u7684\u7eb9\u7406\u7ec6\u8282\uff0c\u56e0\u4e3a\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u7f3a\u4e4f\u9ad8\u9891\u4fe1\u606f\u3002", "method": "\u5229\u7528\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u4e3a\u6bcf\u4e2a\u573a\u666f\u751f\u6210\u573a\u666f\u7279\u5b9a\u7684\u53c2\u8003\u56fe\u5e93\uff0c\u5e76\u5f15\u5165\u53c2\u8003\u5f15\u5bfc\u7684\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff08RGFE\uff09\u6765\u5bf9\u9f50\u548c\u878d\u5408\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u56fe\u50cf\u53ca\u5176\u53c2\u8003\u56fe\u50cf\u7684\u7279\u5f81\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u7eb9\u7406\u611f\u77e5\u5bc6\u5ea6\u63a7\u5236\uff08TADC\uff09\uff0c\u6839\u636e\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u7684\u5185\u90e8\u7eb9\u7406\u4e30\u5bcc\u5ea6\u81ea\u9002\u5e94\u5730\u8c03\u6574\u9ad8\u65af\u5bc6\u5ea6\u3002", "result": "\u5728RealEstate10K\u3001ACID\u548cDTU\u7b49\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cSRSplat\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u6570\u636e\u96c6\u548c\u8de8\u5206\u8fa8\u7387\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SRSplat \u80fd\u591f\u6709\u6548\u5730\u4ece\u7a00\u758f\u7684\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u91cd\u5efa\u9ad8\u5206\u8fa8\u7387 3D \u573a\u666f\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2511.11673", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11673", "abs": "https://arxiv.org/abs/2511.11673", "authors": ["M. A. Gameiro"], "title": "Synergistic Feature Fusion for Latent Lyrical Classification: A Gated Deep Learning Architecture", "comment": null, "summary": "This study addresses the challenge of integrating complex, high-dimensional deep semantic features with simple, interpretable structural cues for lyrical content classification. We introduce a novel Synergistic Fusion Layer (SFL) architecture, a deep learning model utilizing a gated mechanism to modulate Sentence-BERT embeddings (Fdeep) using low-dimensional auxiliary features (Fstruct). The task, derived from clustering UMAP-reduced lyrical embeddings, is reframed as binary classification, distinguishing a dominant, homogeneous cluster (Class 0) from all other content (Class 1). The SFL model achieved an accuracy of 0.9894 and a Macro F1 score of 0.9894, outperforming a comprehensive Random Forest (RF) baseline that used feature concatenation (Accuracy = 0.9868). Crucially, the SFL model demonstrated vastly superior reliability and calibration, exhibiting a 93% reduction in Expected Calibration Error (ECE = 0.0035) and a 2.5x lower Log Loss (0.0304) compared to the RF baseline (ECE = 0.0500; Log Loss = 0.0772). This performance validates the architectural hypothesis that non-linear gating is superior to simple feature concatenation, establishing the SFL model as a robust and trustworthy system for complex multimodal lyrical analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6b4c\u8bcd\u5185\u5bb9\u5206\u7c7b\u7684\u65b0\u578b\u534f\u540c\u878d\u5408\u5c42\uff08SFL\uff09\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u4f18\u4e8e\u968f\u673a\u68ee\u6797\u57fa\u7ebf\u3002", "motivation": "\u5c06\u590d\u6742\u3001\u9ad8\u7ef4\u7684\u6df1\u5ea6\u8bed\u4e49\u7279\u5f81\u4e0e\u7b80\u5355\u3001\u53ef\u89e3\u91ca\u7684\u7ed3\u6784\u7ebf\u7d22\u76f8\u96c6\u6210\uff0c\u7528\u4e8e\u6b4c\u8bcd\u5185\u5bb9\u5206\u7c7b\u3002", "method": "\u5229\u7528\u95e8\u63a7\u673a\u5236\u8c03\u5236Sentence-BERT\u5d4c\u5165\uff08Fdeep\uff09\uff0c\u4f7f\u7528\u4f4e\u7ef4\u8f85\u52a9\u7279\u5f81\uff08Fstruct\uff09\u3002\u5c06\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e8c\u5143\u5206\u7c7b\uff0c\u533a\u5206\u4e3b\u8981\u7684\u540c\u8d28\u96c6\u7fa4\uff08Class 0\uff09\u4e0e\u6240\u6709\u5176\u4ed6\u5185\u5bb9\uff08Class 1\uff09\u3002", "result": "SFL\u6a21\u578b\u7684\u51c6\u786e\u7387\u4e3a0.9894\uff0c\u5b8fF1\u5206\u6570\u4e3a0.9894\uff0c\u4f18\u4e8e\u4f7f\u7528\u7279\u5f81\u8fde\u63a5\u7684\u7efc\u5408\u968f\u673a\u68ee\u6797\uff08RF\uff09\u57fa\u7ebf\uff08\u51c6\u786e\u7387= 0.9868\uff09\u3002SFL\u6a21\u578b\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u53ef\u9760\u6027\u548c\u6821\u51c6\u6027\uff0c\u9884\u671f\u6821\u51c6\u8bef\u5dee\uff08ECE = 0.0035\uff09\u964d\u4f4e\u4e8693\uff05\uff0c\u5bf9\u6570\u635f\u5931\uff080.0304\uff09\u6bd4RF\u57fa\u7ebf\u4f4e2.5\u500d\uff08ECE = 0.0500\uff1b\u5bf9\u6570\u635f\u5931= 0.0772\uff09\u3002", "conclusion": "\u975e\u7ebf\u6027\u95e8\u63a7\u4f18\u4e8e\u7b80\u5355\u7684\u7279\u5f81\u8fde\u63a5\uff0c\u4ece\u800c\u5c06SFL\u6a21\u578b\u786e\u7acb\u4e3a\u7528\u4e8e\u590d\u6742\u591a\u6a21\u6001\u6292\u60c5\u5206\u6790\u7684\u9c81\u68d2\u4e14\u503c\u5f97\u4fe1\u8d56\u7684\u7cfb\u7edf\u3002"}}
