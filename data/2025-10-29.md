<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 37]
- [cs.CV](#cs.CV) [Total: 32]
- [cs.AI](#cs.AI) [Total: 34]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.LG](#cs.LG) [Total: 33]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Evaluating Long-Term Memory for Long-Context Question Answering](https://arxiv.org/abs/2510.23730)
*Alessandra Terranova,Björn Ross,Alexandra Birch*

Main category: cs.CL

TL;DR: 大型语言模型需要记忆来实现连续对话和经验学习。本文对不同的记忆增强方法进行了系统评估，以确定哪种记忆类型最适合长程对话任务。


<details>
  <summary>Details</summary>
Motivation: 研究主要集中在复杂记忆系统的开发上，但目前尚不清楚哪种记忆类型对于长程对话任务最有效。

Method: 使用LoCoMo（一个合成的长程对话基准）对记忆增强方法进行了系统评估，该基准针对需要不同推理策略的问答任务进行了注释。分析了全上下文提示、通过检索增强生成实现的语义记忆、代理记忆、通过上下文学习实现的事件记忆以及通过提示优化实现的过程记忆。

Result: 记忆增强方法在保持竞争精度的同时，减少了超过90%的token使用量。

Conclusion: 记忆架构的复杂性应随模型能力而扩展，小型基础模型从RAG中获益最多，而强大的指令调整推理模型则从通过反思和更复杂的代理语义记忆实现的事件学习中获益。特别是，情景记忆可以帮助LLM识别自身知识的局限性。

Abstract: In order for large language models to achieve true conversational continuity
and benefit from experiential learning, they need memory. While research has
focused on the development of complex memory systems, it remains unclear which
types of memory are most effective for long-context conversational tasks. We
present a systematic evaluation of memory-augmented methods using LoCoMo, a
benchmark of synthetic long-context dialogues annotated for question-answering
tasks that require diverse reasoning strategies. We analyse full-context
prompting, semantic memory through retrieval-augmented generation and agentic
memory, episodic memory through in-context learning, and procedural memory
through prompt optimization. Our findings show that memory-augmented approaches
reduce token usage by over 90% while maintaining competitive accuracy. Memory
architecture complexity should scale with model capability, with small
foundation models benefitting most from RAG, and strong instruction-tuned
reasoning model gaining from episodic learning through reflections and more
complex agentic semantic memory. In particular, episodic memory can help LLMs
recognise the limits of their own knowledge.

</details>


### [2] [BitSkip: An Empirical Analysis of Quantization and Early Exit Composition](https://arxiv.org/abs/2510.23766)
*Ramshankar Bhuvaneswaran,Handan Liu*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）中量化和动态路由等复杂技术的组合效应。


<details>
  <summary>Details</summary>
Motivation: 探索高效LLM，这些技术虽然单独有效，但组合效果不明确。

Method: 提出BitSkip框架，系统探索这些交互作用。

Result: 一个简单的8位量化模型BitSkip-V1，优于更复杂的4位和Hadamard增强模型，甚至在质量上与全精度基线竞争。

Conclusion: 引入Hadamard变换即使在8位精度下也会导致性能灾难性下降，BitSkip-V1在早期退出方面表现出色。

Abstract: The pursuit of efficient Large Language Models (LLMs) has led to increasingly
complex techniques like extreme quantization and dynamic routing. While
individual benefits of these methods are well-documented, their compositional
effects remain poorly understood. This paper introduces BitSkip, a hybrid
architectural framework for systematically exploring these interactions.
Counter-intuitively, our findings reveal that a simple 8-bit quantized model
without Hadamard transform (BitSkip-V1) not only outperforms its more complex
4-bit and Hadamard-enhanced counterparts but also competes the full-precision
baseline in quality (perplexity of 1.13 vs 1.19) . The introduction of Hadamard
transforms, even at 8-bit precision, catastrophically degraded performance by
over 37,000%, tracing fundamental training instability. Our BitSkip-V1 recipe
demonstrates superior early-exit characteristics, with layer 18 providing
optimal 32.5% speed gain for minimal 4% quality loss.

</details>


### [3] [Iterative Critique-Refine Framework for Enhancing LLM Personalization](https://arxiv.org/abs/2510.24469)
*Durga Prasad Maram,Dhruvin Gandhi,Zonghai Yao,Gayathri Akkinapalli,Franck Dernoncourt,Yu Wang,Ryan A. Rossi,Nesreen K. Ahmed*

Main category: cs.CL

TL;DR: PerFine是一个无需训练的框架，通过迭代和基于用户画像的反馈来增强个性化文本生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在个性化文本生成中存在语气、主题或风格漂移的问题。

Method: 该方法使用一个LLM生成器生成草稿，然后使用另一个LLM评论员提供关于语气、词汇、句子结构和主题的结构化反馈，生成器再进行修改。此外，还研究了诸如Best-of-N和Topic Extraction等推理时策略。

Result: 在Yelp、Goodreads和Amazon数据集上，PerFine始终优于PGraphRAG，GEval指标提高了+7-13%，并且随着评论员规模的增加而具有可扩展性。

Conclusion: 事后、感知用户画像的反馈为个性化LLM生成提供了一个强大的范例，该范例无需训练且与模型无关。

Abstract: Personalized text generation requires models not only to produce coherent
text but also to align with a target user's style, tone, and topical focus.
Existing retrieval-augmented approaches such as LaMP and PGraphRAG enrich
profiles with user and neighbor histories, but they stop at generation and
often yield outputs that drift in tone, topic, or style. We present PerFine, a
unified, training-free critique-refine framework that enhances personalization
through iterative, profile-grounded feedback. In each iteration, an LLM
generator produces a draft conditioned on the retrieved profile, and a critic
LLM - also conditioned on the same profile - provides structured feedback on
tone, vocabulary, sentence structure, and topicality. The generator then
revises, while a novel knockout strategy retains the stronger draft across
iterations. We further study additional inference-time strategies such as
Best-of-N and Topic Extraction to balance quality and efficiency. Across Yelp,
Goodreads, and Amazon datasets, PerFine consistently improves personalization
over PGraphRAG, with GEval gains of +7-13%, steady improvements over 3-5
refinement iterations, and scalability with increasing critic size. These
results highlight that post-hoc, profile-aware feedback offers a powerful
paradigm for personalized LLM generation that is both training-free and
model-agnostic.

</details>


### [4] [Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural Processing of Figurative Language](https://arxiv.org/abs/2510.23828)
*Mena Attia,Aashiq Muhamed,Mai Alkhamissi,Thamar Solorio,Mona Diab*

Main category: cs.CL

TL;DR: 大型语言模型在理解和使用文化背景下的语言，特别是隐喻表达方面存在挑战。我们设计了评估任务，并在阿拉伯语和英语的语境理解、语用和内涵解释方面评估了 22 个大型语言模型。结果表明，模型在理解阿拉伯语谚语方面的准确率低于英语谚语，而理解埃及习语的准确率低于阿拉伯语谚语。模型在语用任务中也面临挑战。我们发布了 Kinayat，这是第一个埃及阿拉伯语习语数据集，旨在用于隐喻理解和语用评估。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型处理文化背景下的语言的能力，特别是在理解和实际使用编码本地知识和文化细微差别的比喻表达方面。

Method: 使用比喻语言作为文化细微差别和本地知识的代表，我们设计了评估阿拉伯语和英语语境理解、语用和内涵解释的评估任务。我们评估了 22 个开源和闭源的大型语言模型在埃及阿拉伯语习语、多方言阿拉伯语谚语和英语谚语方面的表现。

Result: 结果显示，阿拉伯语谚语的平均准确率比英语谚语低 4.29%，而埃及习语的准确率比阿拉伯语谚语低 10.28%。对于语用任务，准确率相对于理解下降了 14.07%，但提供上下文习语句子可将准确率提高 10.66%。模型在内涵意义方面也存在困难，对于具有 100% 标注者间一致性的习语，与人工标注者的一致性最高达到 85.58%。

Conclusion: 比喻语言可以作为文化推理的有效诊断工具：虽然大型语言模型通常可以解释比喻意义，但它们在使用方面面临挑战。

Abstract: We present a comprehensive evaluation of the ability of large language models
(LLMs) to process culturally grounded language, specifically to understand and
pragmatically use figurative expressions that encode local knowledge and
cultural nuance. Using figurative language as a proxy for cultural nuance and
local knowledge, we design evaluation tasks for contextual understanding,
pragmatic use, and connotation interpretation in Arabic and English. We
evaluate 22 open- and closed-source LLMs on Egyptian Arabic idioms,
multidialectal Arabic proverbs, and English proverbs. Our results show a
consistent hierarchy: the average accuracy for Arabic proverbs is 4.29% lower
than for English proverbs, and performance for Egyptian idioms is 10.28% lower
than for Arabic proverbs. For the pragmatic use task, accuracy drops by 14.07%
relative to understanding, though providing contextual idiomatic sentences
improves accuracy by 10.66%. Models also struggle with connotative meaning,
reaching at most 85.58% agreement with human annotators on idioms with 100%
inter-annotator agreement. These findings demonstrate that figurative language
serves as an effective diagnostic for cultural reasoning: while LLMs can often
interpret figurative meaning, they face challenges in using it appropriately.
To support future research, we release Kinayat, the first dataset of Egyptian
Arabic idioms designed for both figurative understanding and pragmatic use
evaluation.

</details>


### [5] [Optimizing Retrieval for RAG via Reinforced Contrastive Learning](https://arxiv.org/abs/2510.24652)
*Jiawei Zhou,Lei Chen*

Main category: cs.CL

TL;DR: 提出了一种名为R3的检索框架，通过强化对比学习优化RAG中的检索。


<details>
  <summary>Details</summary>
Motivation: 传统的检索增强生成（RAG）中，信息检索（IR）的角色转变为为AI系统检索上下文知识，但相关性难以预先定义或标注。

Method: 使用trial-and-feedback强化对比学习，允许检索器在RAG环境中动态探索和优化相关性。通过检索结果与环境互动产生对比信号，自动引导检索器的自我改进。

Result: 在各种任务中，R3相比原始检索器提高了RAG性能5.2%，超过了最先进的检索器4.9%，并且与基于大型语言模型（LLM）增强的检索和RAG系统取得了相当的结果。

Conclusion: R3框架高效且实用，仅需少量GPU资源即可完成训练。

Abstract: As retrieval-augmented generation (RAG) becomes increasingly widespread, the
role of information retrieval (IR) is shifting from retrieving information for
human users to retrieving contextual knowledge for artificial intelligence (AI)
systems, where relevance becomes difficult to define or annotate beforehand. To
address this challenge, we propose R3, a Retrieval framework optimized for RAG
through trialand-feedback Reinforced contrastive learning. Unlike prior
approaches that rely on annotated or synthetic data for supervised fine-tuning,
R3 enables the retriever to dynamically explore and optimize relevance within
the RAG environment. During training, the retrieved results interact with the
environment to produce contrastive signals that automatically guide the
retriever's self-improvement. Extensive experiments across diverse tasks
demonstrate that R3 improves RAG performance by 5.2% over the original
retriever and surpasses state-of-the-art retrievers by 4.9%, while achieving
comparable results to LLM-augmented retrieval and RAG systems built on
post-trained or instruction-tuned LLMs. It is both efficient and practical,
requiring only 4 GPUs and completing training within a single day.

</details>


### [6] [How Pragmatics Shape Articulation: A Computational Case Study in STEM ASL Discourse](https://arxiv.org/abs/2510.23842)
*Saki Imai,Lee Kezar,Laurel Aichler,Mert Inan,Erin Walker,Alicia Wooten,Lorna Quandt,Malihe Alikhani*

Main category: cs.CL

TL;DR: 收集了一个美国手语 (ASL) STEM（科学、技术、工程和数学）对话的运动捕捉数据集，该数据集能够对 dyadic 交互式签名、单独签名讲座和解释文章之间进行定量比较。


<details>
  <summary>Details</summary>
Motivation: 目前最好的手语模型大多在口译员或孤立的词汇数据上进行训练，忽略了自然对话的变异性。然而，人类交流通过时空变化和表达方式动态地适应语境和对话者。这尤其体现在教育环境中，教师和学生使用新的词汇。

Method: 使用连续运动学特征，我们将特定于对话的夹带与个人努力减少区分开来，并显示 STEM 术语重复提及的时空变化。

Result: 平均而言，对话符号的持续时间比孤立符号短 24.6%-44.6%，并且在独白上下文中没有显着减少。

Conclusion: 我们的研究将语言分析和计算建模联系起来，以了解语用学如何影响符号表达及其在手语技术中的表示。

Abstract: Most state-of-the-art sign language models are trained on interpreter or
isolated vocabulary data, which overlooks the variability that characterizes
natural dialogue. However, human communication dynamically adapts to contexts
and interlocutors through spatiotemporal changes and articulation style. This
specifically manifests itself in educational settings, where novel vocabularies
are used by teachers, and students. To address this gap, we collect a motion
capture dataset of American Sign Language (ASL) STEM (Science, Technology,
Engineering, and Mathematics) dialogue that enables quantitative comparison
between dyadic interactive signing, solo signed lecture, and interpreted
articles. Using continuous kinematic features, we disentangle dialogue-specific
entrainment from individual effort reduction and show spatiotemporal changes
across repeated mentions of STEM terms. On average, dialogue signs are
24.6%-44.6% shorter in duration than the isolated signs, and show significant
reductions absent in monologue contexts. Finally, we evaluate sign embedding
models on their ability to recognize STEM signs and approximate how entrained
the participants become over time. Our study bridges linguistic analysis and
computational modeling to understand how pragmatics shape sign articulation and
its representation in sign language technologies.

</details>


### [7] [Tongyi DeepResearch Technical Report](https://arxiv.org/abs/2510.24701)
*Tongyi DeepResearch Team,Baixuan Li,Bo Zhang,Dingchu Zhang,Fei Huang,Guangyu Li,Guoxin Chen,Huifeng Yin,Jialong Wu,Jingren Zhou,Kuan Li,Liangcai Su,Litu Ou,Liwen Zhang,Pengjun Xie,Rui Ye,Wenbiao Yin,Xinmiao Yu,Xinyu Wang,Xixi Wu,Xuanzhong Chen,Yida Zhao,Zhen Zhang,Zhengwei Tao,Zhongwang Zhang,Zile Qiao,Chenxi Wang,Donglei Yu,Gang Fu,Haiyang Shen,Jiayin Yang,Jun Lin,Junkai Zhang,Kui Zeng,Li Yang,Hailong Yin,Maojia Song,Ming Yan,Peng Xia,Qian Xiao,Rui Min,Ruixue Ding,Runnan Fang,Shaowei Chen,Shen Huang,Shihang Wang,Shihao Cai,Weizhou Shen,Xiaobin Wang,Xin Guan,Xinyu Geng,Yingcheng Shi,Yuning Wu,Zhuo Chen,Zijian Li,Yong Jiang*

Main category: cs.CL

TL;DR: Tongyi DeepResearch 是一个专为长期、深度信息检索研究任务而设计的 agentic 大语言模型。


<details>
  <summary>Details</summary>
Motivation: 为了激励自主深度研究能力，Tongyi DeepResearch 通过结合 agentic mid-training 和 agentic post-training 的端到端训练框架进行开发，从而能够在复杂任务中实现可扩展的推理和信息检索。

Method: 设计了一个高度可扩展的数据合成管道，该管道是完全自动的，不依赖于昂贵的人工注释，并支持所有训练阶段。通过为每个阶段构建定制环境，我们的系统能够在整个过程中实现稳定和一致的交互。

Result: Tongyi DeepResearch 具有 305 亿个总参数，但每个 token 仅激活 33 亿个参数，在包括 Humanity's Last Exam、BrowseComp、BrowseComp-ZH、WebWalkerQA、xbench-DeepSearch、FRAMES 和 xbench-DeepSearch-2510 在内的一系列 agentic 深度研究基准测试中实现了最先进的性能。

Conclusion: 我们开源了模型、框架和完整的解决方案，以增强社区能力。

Abstract: We present Tongyi DeepResearch, an agentic large language model, which is
specifically designed for long-horizon, deep information-seeking research
tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is
developed through an end-to-end training framework that combines agentic
mid-training and agentic post-training, enabling scalable reasoning and
information seeking across complex tasks. We design a highly scalable data
synthesis pipeline that is fully automatic, without relying on costly human
annotation, and empowers all training stages. By constructing customized
environments for each stage, our system enables stable and consistent
interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total
parameters, with only 3.3 billion activated per token, achieves
state-of-the-art performance across a range of agentic deep research
benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH,
WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We
open-source the model, framework, and complete solutions to empower the
community.

</details>


### [8] [CRADLE Bench: A Clinician-Annotated Benchmark for Multi-Faceted Mental Health Crisis and Safety Risk Detection](https://arxiv.org/abs/2510.23845)
*Grace Byun,Rebecca Lipschutz,Sean T. Minton,Abigail Lott,Jinho D. Choi*

Main category: cs.CL

TL;DR: 论文提出了CRADLE BENCH，一个用于多方面危机检测的基准，涵盖七种危机类型并包含时间标签。


<details>
  <summary>Details</summary>
Motivation: 检测心理健康危机情况（如自杀意念、强奸、家庭暴力、虐待儿童和性骚扰）对于语言模型来说至关重要但尚未充分探索。模型必须可靠地标记这些情况，否则可能会产生严重后果。

Method: 构建了一个包含600个临床医生注释的评估示例和420个开发示例的基准，以及一个包含约4K个示例的训练语料库，这些示例使用多个语言模型的大多数投票集成自动标记。

Result: 通过在共识和一致集成协议定义的子集上微调六个危机检测模型，提供了在不同协议标准下训练的互补模型。

Conclusion: CRADLE BENCH基准的提出

Abstract: Detecting mental health crisis situations such as suicide ideation, rape,
domestic violence, child abuse, and sexual harassment is a critical yet
underexplored challenge for language models. When such situations arise during
user--model interactions, models must reliably flag them, as failure to do so
can have serious consequences. In this work, we introduce CRADLE BENCH, a
benchmark for multi-faceted crisis detection. Unlike previous efforts that
focus on a limited set of crisis types, our benchmark covers seven types
defined in line with clinical standards and is the first to incorporate
temporal labels. Our benchmark provides 600 clinician-annotated evaluation
examples and 420 development examples, together with a training corpus of
around 4K examples automatically labeled using a majority-vote ensemble of
multiple language models, which significantly outperforms single-model
annotation. We further fine-tune six crisis detection models on subsets defined
by consensus and unanimous ensemble agreement, providing complementary models
trained under different agreement criteria.

</details>


### [9] [Temporal Blindness in Multi-Turn LLM Agents: Misaligned Tool Use vs. Human Time Perception](https://arxiv.org/abs/2510.23853)
*Yize Cheng,Arshia Soltani Moakhar,Chenrui Fan,Kazem Faghih,Parsa Hosseini,Wenxiao Wang,Soheil Feizi*

Main category: cs.CL

TL;DR: 大型语言模型在多轮对话中与动态环境交互时存在时间盲区，无法感知消息之间的时间间隔，导致工具调用不准确。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型Agent在多轮对话中与动态环境交互时，由于缺乏时间感知能力，无法根据时间间隔决定是否调用工具，导致过度或不足地依赖先前信息。

Method: 1. 提出了TicToc-v1测试集，包含34个具有不同时间敏感度的场景。2. 通过在对话消息中添加时间戳，为LLM提供时间背景信息。3. 收集人类偏好数据，创建两个子集：prefer-noTool和prefer-Tool。

Result: 在没有时间信息的情况下，大多数模型的性能略高于随机水平（60%），添加时间戳后略有改善（65%），但提升有限。

Conclusion: 需要进行专门的后训练对齐，以使多轮LLM工具的使用与人类的时间感知对齐。

Abstract: Large language model agents are increasingly used in multi-turn
conversational settings to interact with and execute tasks in dynamic
environments. However, a key limitation is their temporal blindness: they, by
default, operate with a stationary context, failing to account for the
real-world time elapsed between messages. This becomes a critical liability
when an agent must decide whether to invoke a tool based on how much time has
passed since the last observation. Without temporal awareness, agents often
either over-rely on previous context (skipping necessary tool calls), or
under-rely on it (unnecessarily repeating tool calls). To study this challenge,
we introduce TicToc-v1, a test set of multi-turn user-agent trajectories across
34 scenarios with varying time sensitivity. Each trajectory ends with a user
question, where the need for a tool call depends on the amount of time elapsed
since the last message. To give LLMs temporal context, we augment dialogue
messages with explicit timestamps, bridging the gap between static dialogue and
evolving environments. We then collected human preferences for these samples,
creating two subsets: one where humans preferred relying on the previous
observation (prefer-noTool), and another where they preferred a new tool call
(prefer-Tool). We evaluated how well LLM tool-calling decisions align with
human preferences under varying time intervals on TicToc-v1. Our analysis show
that without time information, most models perform only slightly better than
random, with the top alignment rate being just over 60%. While adding
timestamps leads to a slight improvement, particularly for larger models, the
improvement is modest, peaking at around 65%. We also show that naive,
prompt-based alignment have limited effectiveness. Our findings highlight the
need for specific post-training alignment to align multi-turn LLM tool use with
human temporal perception.

</details>


### [10] [Can LLMs Narrate Tabular Data? An Evaluation Framework for Natural Language Representations of Text-to-SQL System Outputs](https://arxiv.org/abs/2510.23854)
*Jyotika Singh,Weiyi Sun,Amit Agarwal,Viji Krishnamurthy,Yassine Benajiba,Sujith Ravi,Dan Roth*

Main category: cs.CL

TL;DR: 提出了一种新的评估方法 Combo-Eval，用于评估大型语言模型生成的自然语言表示 (NLR)，并构建了首个专用数据集 NLR-BIRD。


<details>
  <summary>Details</summary>
Motivation: 现有技术使用大型语言模型 (LLM) 将表格数据库结果转换为自然语言表示 (NLR)，但表格结果在自然语言表示中存在信息丢失或错误的问题，这在很大程度上尚未被探索。

Method: 结合了多种现有方法的优点，优化评估保真度，并显著减少 LLM 调用次数（25-61%）。

Result: 通过人工评估表明，Combo-Eval 与人工判断具有更好的一致性，适用于有和没有ground truth参考的场景。

Conclusion: Combo-Eval 是一种有效的方法，可以用来评估大型语言模型生成的自然语言表示。

Abstract: In modern industry systems like multi-turn chat agents, Text-to-SQL
technology bridges natural language (NL) questions and database (DB) querying.
The conversion of tabular DB results into NL representations (NLRs) enables the
chat-based interaction. Currently, NLR generation is typically handled by large
language models (LLMs), but information loss or errors in presenting tabular
results in NL remains largely unexplored. This paper introduces a novel
evaluation method - Combo-Eval - for judgment of LLM-generated NLRs that
combines the benefits of multiple existing methods, optimizing evaluation
fidelity and achieving a significant reduction in LLM calls by 25-61%.
Accompanying our method is NLR-BIRD, the first dedicated dataset for NLR
benchmarking. Through human evaluations, we demonstrate the superior alignment
of Combo-Eval with human judgments, applicable across scenarios with and
without ground truth references.

</details>


### [11] [OraPlan-SQL: A Planning-Centric Framework for Complex Bilingual NL2SQL Reasoning](https://arxiv.org/abs/2510.23870)
*Marianne Menglin Liu,Sai Ashish Somayajula,Syed Fahad Allam Shah,Sujith Ravi,Dan Roth*

Main category: cs.CL

TL;DR: OraPlan-SQL在Archer NL2SQL Evaluation Challenge 2025中排名第一，在执行准确率（EX）方面超过第二名系统6%以上，英语为55.0%，中文为56.7%，同时保持超过99%的SQL有效性（VA）。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在解决复杂推理（如算术、常识和假设推理）的NL2SQL问题，特别是在双语环境中面临的挑战，包括音译和实体不匹配问题。

Method: 该系统采用agentic框架，包含Planner agent和SQL agent。通过反馈引导的meta-prompting策略优化单个planner，利用人工输入的failure case聚类，提炼出修正指南并整合到planner的系统提示中。同时，加入实体链接指南，生成实体的替代形式，并显式包含在计划中。此外，通过计划多样化来增强可靠性。

Result: OraPlan-SQL在Archer NL2SQL Evaluation Challenge 2025中，英语执行准确率为55.0%，中文为56.7%，SQL有效性超过99%，超过第二名系统6%以上。

Conclusion: 该研究提出了一种有效的NL2SQL系统OraPlan-SQL，通过单planner的优化和计划多样化，在复杂推理和双语环境下取得了优异的性能。

Abstract: We present OraPlan-SQL, our system for the Archer NL2SQL Evaluation Challenge
2025, a bilingual benchmark requiring complex reasoning such as arithmetic,
commonsense, and hypothetical inference. OraPlan-SQL ranked first, exceeding
the second-best system by more than 6% in execution accuracy (EX), with 55.0%
in English and 56.7% in Chinese, while maintaining over 99% SQL validity (VA).
Our system follows an agentic framework with two components: Planner agent that
generates stepwise natural language plans, and SQL agent that converts these
plans into executable SQL. Since SQL agent reliably adheres to the plan, our
refinements focus on the planner. Unlike prior methods that rely on multiple
sub-agents for planning and suffer from orchestration overhead, we introduce a
feedback-guided meta-prompting strategy to refine a single planner. Failure
cases from a held-out set are clustered with human input, and an LLM distills
them into corrective guidelines that are integrated into the planner's system
prompt, improving generalization without added complexity. For the multilingual
scenario, to address transliteration and entity mismatch issues, we incorporate
entity-linking guidelines that generate alternative surface forms for entities
and explicitly include them in the plan. Finally, we enhance reliability
through plan diversification: multiple candidate plans are generated for each
query, with the SQL agent producing a query for each plan, and final output
selected via majority voting over their executions.

</details>


### [12] [Language Models for Longitudinal Clinical Prediction](https://arxiv.org/abs/2510.23884)
*Tananun Songdechakraiwut,Michael Lutz*

Main category: cs.CL

TL;DR: 使用轻量级框架，无需微调即可使用大型语言模型分析纵向临床数据，预测准确。


<details>
  <summary>Details</summary>
Motivation: 探索使用大型语言模型分析纵向临床数据。

Method: 将患者历史和语境整合到语言模型空间中。

Result: 即使使用最少的训练数据，也能在神经心理学评估中实现准确可靠的性能，

Conclusion: 该方法对早期阿尔茨海默病监测具有前景

Abstract: We explore a lightweight framework that adapts frozen large language models
to analyze longitudinal clinical data. The approach integrates patient history
and context within the language model space to generate accurate forecasts
without model fine-tuning. Applied to neuropsychological assessments, it
achieves accurate and reliable performance even with minimal training data,
showing promise for early-stage Alzheimer's monitoring.

</details>


### [13] [AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages](https://arxiv.org/abs/2510.23896)
*Kosei Uemura,Miaoran Zhang,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 本文介绍了AfriMTEB，一个扩展的MMTEB，覆盖59种非洲语言，14个任务和38个数据集。同时提出了AfriE5模型，通过跨语言对比蒸馏将mE5模型适配到非洲语言。


<details>
  <summary>Details</summary>
Motivation: 现有NLP任务中的文本嵌入对于防止LLM中的幻觉至关重要，但非洲语言在现有的MMTEB中代表性不足。

Method: 通过跨语言对比蒸馏，将instruction-tuned mE5模型适配到非洲语言，得到AfriE5模型。

Result: AfriE5实现了state-of-the-art的性能，超过了Gemini-Embeddings和mE5等强基线模型。

Conclusion: 本文引入了AfriMTEB和AfriE5，为非洲语言的NLP任务提供了新的资源和方法。

Abstract: Text embeddings are an essential building component of several NLP tasks such
as retrieval-augmented generation which is crucial for preventing
hallucinations in LLMs. Despite the recent release of massively multilingual
MTEB (MMTEB), African languages remain underrepresented, with existing tasks
often repurposed from translation benchmarks such as FLORES clustering or
SIB-200. In this paper, we introduce AfriMTEB -- a regional expansion of MMTEB
covering 59 languages, 14 tasks, and 38 datasets, including six newly added
datasets. Unlike many MMTEB datasets that include fewer than five languages,
the new additions span 14 to 56 African languages and introduce entirely new
tasks, such as hate speech detection, intent detection, and emotion
classification, which were not previously covered. Complementing this, we
present AfriE5, an adaptation of the instruction-tuned mE5 model to African
languages through cross-lingual contrastive distillation. Our evaluation shows
that AfriE5 achieves state-of-the-art performance, outperforming strong
baselines such as Gemini-Embeddings and mE5.

</details>


### [14] [Breaking the Benchmark: Revealing LLM Bias via Minimal Contextual Augmentation](https://arxiv.org/abs/2510.23921)
*Kaveh Eskandari Miandoab,Mahammed Kamruzzaman,Arshia Gharooni,Gene Louis Kim,Vasanth Sarathy,Ninareh Mehrabi*

Main category: cs.CL

TL;DR: 大型语言模型(LLM)由于训练数据的判别性，表现出刻板印象偏差。尽管在避免使用刻板印象信息的方法和模型开发方面取得了显著进展，但最近的研究表明，用于偏差对齐的方法是脆弱的。本文介绍了一种新颖而通用的增强框架，该框架包括三个即插即用步骤，适用于许多公平性评估基准。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在刻板印象偏差，且现有的偏差对齐方法具有脆弱性。

Method: 提出了一种新颖的通用增强框架，包含三个即插即用步骤，并将其应用于公平性评估数据集（BBQ）。

Result: 发现大型语言模型（包括最先进的开放和封闭权重模型）容易受到对其输入的扰动的影响，更有可能表现出刻板印象。此外，目标人群属于文献研究较少的社区时，这些模型更有可能出现偏差行为。

Conclusion: 需要扩大公平性和安全性研究，以涵盖更多不同的社区。

Abstract: Large Language Models have been shown to demonstrate stereotypical biases in
their representations and behavior due to the discriminative nature of the data
that they have been trained on. Despite significant progress in the development
of methods and models that refrain from using stereotypical information in
their decision-making, recent work has shown that approaches used for bias
alignment are brittle. In this work, we introduce a novel and general
augmentation framework that involves three plug-and-play steps and is
applicable to a number of fairness evaluation benchmarks. Through application
of augmentation to a fairness evaluation dataset (Bias Benchmark for Question
Answering (BBQ)), we find that Large Language Models (LLMs), including
state-of-the-art open and closed weight models, are susceptible to
perturbations to their inputs, showcasing a higher likelihood to behave
stereotypically. Furthermore, we find that such models are more likely to have
biased behavior in cases where the target demographic belongs to a community
less studied by the literature, underlining the need to expand the fairness and
safety research to include more diverse communities.

</details>


### [15] [Agent-based Automated Claim Matching with Instruction-following LLMs](https://arxiv.org/abs/2510.23924)
*Dina Pisarevskaya,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: 提出了一种新的基于代理的方法，用于使用指令跟随 LLM 自动执行声明匹配任务。


<details>
  <summary>Details</summary>
Motivation: 探索使用 LLM 生成提示来改进声明匹配，并降低计算成本。

Method: 提出了一个两步流程，首先使用 LLM 生成提示，然后使用 LLM 执行声明匹配作为二元分类任务。

Result: LLM 生成的提示优于人工生成的提示，较小的 LLM 在生成过程中与较大的 LLM 效果一样好，并且在流程的每个步骤中使用不同的 LLM 是有效的。

Conclusion: 该研究深入了解了 LLM 对声明匹配的理解。

Abstract: We present a novel agent-based approach for the automated claim matching task
with instruction-following LLMs. We propose a two-step pipeline that first
generates prompts with LLMs, to then perform claim matching as a binary
classification task with LLMs. We demonstrate that LLM-generated prompts can
outperform SOTA with human-generated prompts, and that smaller LLMs can do as
well as larger ones in the generation process, allowing to save computational
resources. We also demonstrate the effectiveness of using different LLMs for
each step of the pipeline, i.e. using an LLM for prompt generation, and another
for claim matching. Our investigation into the prompt generation process in
turn reveals insights into the LLMs' understanding of claim matching.

</details>


### [16] [Auto prompting without training labels: An LLM cascade for product quality assessment in e-commerce catalogs](https://arxiv.org/abs/2510.23941)
*Soham Satyadharma,Fatemeh Sheikholeslami,Swati Kaul,Aziz Umit Batur,Suleiman A. Khan*

Main category: cs.CL

TL;DR: 提出了一种新颖的、免训练的级联方法，用于自动提示大型语言模型（LLM）以评估电子商务中的产品质量。


<details>
  <summary>Details</summary>
Motivation: 弥合通用语言理解和复杂工业目录中大规模特定领域知识之间的差距。

Method: 从人工制作的提示的种子开始，级联逐步优化指令以满足目录特定要求。

Result: 自动提示级联比传统的思维链提示提高了 8-10% 的精确度和召回率，同时将领域专家每个属性的工作量从 5.1 小时减少到 3 分钟，减少了 99%。

Conclusion: 该级联有效地推广到五种语言和多个质量评估任务，始终保持性能提升。

Abstract: We introduce a novel, training free cascade for auto-prompting Large Language
Models (LLMs) to assess product quality in e-commerce. Our system requires no
training labels or model fine-tuning, instead automatically generating and
refining prompts for evaluating attribute quality across tens of thousands of
product category-attribute pairs. Starting from a seed of human-crafted
prompts, the cascade progressively optimizes instructions to meet
catalog-specific requirements. This approach bridges the gap between general
language understanding and domain-specific knowledge at scale in complex
industrial catalogs. Our extensive empirical evaluations shows the auto-prompt
cascade improves precision and recall by $8-10\%$ over traditional
chain-of-thought prompting. Notably, it achieves these gains while reducing
domain expert effort from 5.1 hours to 3 minutes per attribute - a $99\%$
reduction. Additionally, the cascade generalizes effectively across five
languages and multiple quality assessment tasks, consistently maintaining
performance gains.

</details>


### [17] [Leveraging LLMs for Early Alzheimer's Prediction](https://arxiv.org/abs/2510.23946)
*Tananun Songdechakraiwut*

Main category: cs.CL

TL;DR: 本文提出了一个基于连接组信息的LLM框架，用于临床预测，尤其关注早期阿尔茨海默病的检测。


<details>
  <summary>Details</summary>
Motivation: 旨在提高早期阿尔茨海默病检测的灵敏度和准确性，以便及时干预。

Method: 该框架将动态fMRI连接性编码为时间序列，应用稳健的标准化方法，并将数据映射到适合预训练LLM的表示形式。

Result: 该方法在早期阿尔茨海默病检测中实现了高灵敏度的预测，误差率远低于临床认可的范围。

Conclusion: 该研究对及时干预阿尔茨海默病具有重要意义。

Abstract: We present a connectome-informed LLM framework that encodes dynamic fMRI
connectivity as temporal sequences, applies robust normalization, and maps
these data into a representation suitable for a frozen pre-trained LLM for
clinical prediction. Applied to early Alzheimer's detection, our method
achieves sensitive prediction with error rates well below clinically recognized
margins, with implications for timely Alzheimer's intervention.

</details>


### [18] [Uncovering the Potential Risks in Unlearning: Danger of English-only Unlearning in Multilingual LLMs](https://arxiv.org/abs/2510.23949)
*Kyomin Hwang,Hyeonjin Kim,Seungyeon Kim,Sunghyun Wee,Nojun Kwak*

Main category: cs.CL

TL;DR: 多语言LLM仅用英语数据擦除知识不够充分，且现有分析侧重性能。本文关注评估角度，揭示了在用多语言数据集完全微调后进行unlearning时出现的语言混淆问题。


<details>
  <summary>Details</summary>
Motivation: 现有多语言LLM的知识擦除研究不足，尤其是在完全微调后进行unlearning时，模型会出现语言混淆，即以非输入提示的语言作答。

Method: 1. 引入基于N-gram的语言混合(N-Mix)得分，量化展示多语言LLM中普遍存在的语言混淆现象。2. 论证了当N-Mix得分较高时，基于参考的指标会导致假阴性。3. 建议需要一种新型的unlearning评估方法，可以直接评估生成句子的内容，即基于语义的指标。

Result: N-Mix得分能够有效量化语言混淆现象，并揭示了传统评估指标在存在语言混淆时的局限性。

Conclusion: 需要开发新的、基于语义的评估指标来更准确地评估多语言LLM的unlearning效果。

Abstract: There have been a couple of studies showing that attempting to erase
multilingual knowledge using only English data is insufficient for multilingual
LLMs. However, their analyses remain highly performance-oriented. In this
paper, we switch the point of view to evaluation, and address an additional
blind spot which reveals itself when the multilingual LLM is fully finetuned
with parallel multilingual dataset before unlearning. Here, language confusion
occurs whereby a model responds in language different from that of the input
prompt. Language confusion is a problematic phenomenon in unlearning, causing
the standard reference-based metrics to fail. We tackle this phenomenon in
three steps: (1) introduce N-gram-based Language-Mix (N-Mix) score to
quantitatively show the language confusion is pervasive and consistent in
multilingual LLMs, (2) demonstrate that reference-based metrics result in false
negatives when N-Mix score is high, and(3) suggest the need of new type of
unlearning evaluation that can directly assess the content of the generated
sentences. We call this type of metrics as semantic-based metric.

</details>


### [19] [M-Eval: A Heterogeneity-Based Framework for Multi-evidence Validation in Medical RAG Systems](https://arxiv.org/abs/2510.23995)
*Mengzhou Sun,Sendong Zhao,Jianyu Chen,Haochun Wang,Bin Qin*

Main category: cs.CL

TL;DR: 本研究提出了一种名为M-Eval的新方法，用于检测RAG在医疗问答系统中生成的错误信息。


<details>
  <summary>Details</summary>
Motivation: 当前RAG应用于医疗领域时，会生成不正确的信息，例如幻觉，并且未能正确使用外部知识。

Method: 该方法受到循证医学中异质性分析方法的启发，通过从外部知识库提取额外的医学文献，并检索RAG系统生成的证据文档，使用异质性分析来检查证据是否支持响应中的不同观点。

Result: M-Eval方法在各种LLM中显示出高达23.31%的准确率提升。

Conclusion: 该研究有助于检测当前基于RAG的医疗系统中的错误，并使LLM的应用更加可靠，减少诊断错误。

Abstract: Retrieval-augmented Generation (RAG) has demonstrated potential in enhancing
medical question-answering systems through the integration of large language
models (LLMs) with external medical literature. LLMs can retrieve relevant
medical articles to generate more professional responses efficiently. However,
current RAG applications still face problems. They generate incorrect
information, such as hallucinations, and they fail to use external knowledge
correctly. To solve these issues, we propose a new method named M-Eval. This
method is inspired by the heterogeneity analysis approach used in
Evidence-Based Medicine (EBM). Our approach can check for factual errors in RAG
responses using evidence from multiple sources. First, we extract additional
medical literature from external knowledge bases. Then, we retrieve the
evidence documents generated by the RAG system. We use heterogeneity analysis
to check whether the evidence supports different viewpoints in the response. In
addition to verifying the accuracy of the response, we also assess the
reliability of the evidence provided by the RAG system. Our method shows an
improvement of up to 23.31% accuracy across various LLMs. This work can help
detect errors in current RAG-based medical systems. It also makes the
applications of LLMs more reliable and reduces diagnostic errors.

</details>


### [20] [PICOs-RAG: PICO-supported Query Rewriting for Retrieval-Augmented Generation in Evidence-Based Medicine](https://arxiv.org/abs/2510.23998)
*Mengzhou Sun,Sendong Zhao,Jianyu Chen,Bin Qin*

Main category: cs.CL

TL;DR: 该论文提出了一种名为PICOs-RAG的方法，以改进检索增强生成（RAG）在循证医学（EBM）中的应用。


<details>
  <summary>Details</summary>
Motivation: 当前RAG方法难以处理真实临床场景中的复杂查询，例如，当查询缺少某些信息或使用不精确的语言时，模型可能会检索到不相关的证据并生成无用的答案。

Method: 该方法通过将用户查询扩展和规范化为更专业的格式，并使用EBM中的PICO格式提取用于检索的最重要信息来解决此问题。

Result: 与基线相比，检索效率和相关性显著提高，经评估，提升高达8.8%。

Conclusion: PICOs-RAG提高了大型语言模型在EBM中作为有用的和可靠的医疗助手时的性能。

Abstract: Evidence-based medicine (EBM) research has always been of paramount
importance. It is important to find appropriate medical theoretical support for
the needs from physicians or patients to reduce the occurrence of medical
accidents. This process is often carried out by human querying relevant
literature databases, which lacks objectivity and efficiency. Therefore,
researchers utilize retrieval-augmented generation (RAG) to search for evidence
and generate responses automatically. However, current RAG methods struggle to
handle complex queries in real-world clinical scenarios. For example, when
queries lack certain information or use imprecise language, the model may
retrieve irrelevant evidence and generate unhelpful answers. To address this
issue, we present the PICOs-RAG to expand the user queries into a better
format. Our method can expand and normalize the queries into professional ones
and use the PICO format, a search strategy tool present in EBM, to extract the
most important information used for retrieval. This approach significantly
enhances retrieval efficiency and relevance, resulting in up to an 8.8\%
improvement compared to the baseline evaluated by our method. Thereby the
PICOs-RAG improves the performance of the large language models into a helpful
and reliable medical assistant in EBM.

</details>


### [21] [META-RAG: Meta-Analysis-Inspired Evidence-Re-Ranking Method for Retrieval-Augmented Generation in Evidence-Based Medicine](https://arxiv.org/abs/2510.24003)
*Mengzhou Sun,Sendong Zhao,Jianyu Chen,Haochun Wang,Bin Qin*

Main category: cs.CL

TL;DR: 该论文提出了一种新的方法，通过模拟EBM中的meta-analysis来重新排序和过滤医学证据，从而提高LLM在EBM任务中的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG在EBM中难以有效区分高质量证据，EBM对证据有严格要求。

Method: 结合多种EBM方法模拟meta-analysis，包括可靠性分析、异质性分析和外推分析。

Result: 实验结果表明，该方法使准确率提高了11.4%。

Conclusion: 该方法能够从PubMed数据集中提取更高质量和更可靠的证据，减少错误知识的输入，并帮助用户获得更有效的回复。

Abstract: Evidence-based medicine (EBM) holds a crucial role in clinical application.
Given suitable medical articles, doctors effectively reduce the incidence of
misdiagnoses. Researchers find it efficient to use large language models (LLMs)
techniques like RAG for EBM tasks. However, the EBM maintains stringent
requirements for evidence, and RAG applications in EBM struggle to efficiently
distinguish high-quality evidence. Therefore, inspired by the meta-analysis
used in EBM, we provide a new method to re-rank and filter the medical
evidence. This method presents multiple principles to filter the best evidence
for LLMs to diagnose. We employ a combination of several EBM methods to emulate
the meta-analysis, which includes reliability analysis, heterogeneity analysis,
and extrapolation analysis. These processes allow the users to retrieve the
best medical evidence for the LLMs. Ultimately, we evaluate these high-quality
articles and show an accuracy improvement of up to 11.4% in our experiments and
results. Our method successfully enables RAG to extract higher-quality and more
reliable evidence from the PubMed dataset. This work can reduce the infusion of
incorrect knowledge into responses and help users receive more effective
replies.

</details>


### [22] [TEXT2DB: Integration-Aware Information Extraction with Large Language Model Agents](https://arxiv.org/abs/2510.24014)
*Yizhu Jiao,Sha Li,Sizhe Zhou,Heng Ji,Jiawei Han*

Main category: cs.CL

TL;DR: 论文提出了一种新的信息抽取（IE）任务TEXT2DB，它强调了IE输出与目标数据库的集成。该任务要求模型根据用户指令，利用文档集中的信息更新数据库。


<details>
  <summary>Details</summary>
Motivation: 现有信息抽取本体与下游应用需求不匹配，难以直接利用。

Method: 提出了一个名为OPAL的LLM Agent框架，包括Observer、Planner和Analyzer三个组件。Planner组件生成基于代码的计划，调用IE模型。

Result: OPAL可以通过生成不同的代码计划并调用所需的IE模型来成功适应不同的数据库模式。

Conclusion: 该研究引入了一个新的基准来评估TEXT2DB任务，并提出了OPAL框架。实验表明，OPAL在适应不同的数据库模式方面是成功的。

Abstract: The task of information extraction (IE) is to extract structured knowledge
from text. However, it is often not straightforward to utilize IE output due to
the mismatch between the IE ontology and the downstream application needs. We
propose a new formulation of IE TEXT2DB that emphasizes the integration of IE
output and the target database (or knowledge base). Given a user instruction, a
document set, and a database, our task requires the model to update the
database with values from the document set to satisfy the user instruction.
This task requires understanding user instructions for what to extract and
adapting to the given DB/KB schema for how to extract on the fly. To evaluate
this new task, we introduce a new benchmark featuring common demands such as
data infilling, row population, and column addition. In addition, we propose an
LLM agent framework OPAL (Observe-PlanAnalyze LLM) which includes an Observer
component that interacts with the database, the Planner component that
generates a code-based plan with calls to IE models, and the Analyzer component
that provides feedback regarding code quality before execution. Experiments
show that OPAL can successfully adapt to diverse database schemas by generating
different code plans and calling the required IE models. We also highlight
difficult cases such as dealing with large databases with complex dependencies
and extraction hallucination, which we believe deserve further investigation.
Source code: https://github.com/yzjiao/Text2DB

</details>


### [23] [Teaching LLMs to Abstain via Fine-Grained Semantic Confidence Reward](https://arxiv.org/abs/2510.24020)
*Hao An,Yang Xu*

Main category: cs.CL

TL;DR: 该论文提出了一种新的强化学习框架，通过细粒度的语义置信度奖励来引导大型语言模型避免回答超出其知识范围的问题，从而减轻幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖粗粒度的信号来引导LLM避免回答问题，导致模型对自身知识边界的感知不精确。为了解决这个问题。

Method: 该方法通过对多个候选答案进行抽样和语义聚类，然后训练LLM保留高置信度簇内的答案，并丢弃低置信度簇内的答案。

Result: 该方法在领域内和领域外基准测试中显著提高了可靠性。

Conclusion: 该论文提出了一种有效的减轻大型语言模型幻觉问题的方法，并通过实验验证了其有效性。

Abstract: Mitigating hallucinations in Large Language Models (LLMs) is critical for
their reliable deployment. Existing methods typically fine-tune LLMs to abstain
from answering questions beyond their knowledge scope. However, these methods
often rely on coarse-grained signals to guide LLMs to abstain, such as overall
confidence or uncertainty scores on multiple sampled answers, which may result
in an imprecise awareness of the model's own knowledge boundaries. To this end,
we propose a novel reinforcement learning framework built on
$\textbf{\underline{Fi}ne-grained \underline{S}emantic \underline{Co}nfidence
\underline{Re}ward (\Ours)}$, which guides LLMs to abstain via sample-specific
confidence. Specifically, our method operates by sampling multiple candidate
answers and conducting semantic clustering, then training the LLM to retain
answers within high-confidence clusters and discard those within low-confidence
ones, thereby promoting accurate post-hoc abstention. Additionally, we propose
a new metric for evaluating the reliability of abstention fine-tuning tasks
more comprehensively. Our method significantly enhances reliability in both
in-domain and out-of-distribution benchmarks.

</details>


### [24] [SpecKD: Speculative Decoding for Effective Knowledge Distillation of LLMs](https://arxiv.org/abs/2510.24021)
*Haiduo Huang,Jiangcheng Song,Yadong Zhang,Pengju Ren*

Main category: cs.CL

TL;DR: SpecKD: A new knowledge distillation framework that uses a token-level gating mechanism to selectively apply the distillation loss only to accepted tokens, improving student performance.


<details>
  <summary>Details</summary>
Motivation: Conventional KD approaches apply the distillation loss uniformly across all tokens, regardless of the teacher's confidence, which can introduce noise and harm student performance.

Method: A dynamic, token-level gating mechanism inspired by the propose-and-verify paradigm of speculative decoding is introduced. The student's token proposal is verified against the teacher's distribution, and the distillation loss is selectively applied only to accepted tokens, while rejected tokens are masked out.

Result: SpecKD consistently and significantly outperforms strong KD baselines, leading to more stable training and more capable student models, and achieving state-of-the-art results on diverse text generation tasks.

Conclusion: SpecKD is a novel, plug-and-play framework that improves knowledge distillation by selectively applying the distillation loss based on teacher confidence, leading to better student performance.

Abstract: Knowledge Distillation (KD) has become a cornerstone technique for
compressing Large Language Models (LLMs) into smaller, more efficient student
models. However, conventional KD approaches typically apply the distillation
loss uniformly across all tokens, regardless of the teacher's confidence. This
indiscriminate mimicry can introduce noise, as the student is forced to learn
from the teacher's uncertain or high-entropy predictions, which may ultimately
harm student performance-especially when the teacher is much larger and more
powerful. To address this, we propose Speculative Knowledge Distillation
(SpecKD), a novel, plug-and-play framework that introduces a dynamic,
token-level gating mechanism inspired by the "propose-and-verify" paradigm of
speculative decoding. At each step, the student's token proposal is verified
against the teacher's distribution; the distillation loss is selectively
applied only to "accepted" tokens, while "rejected" tokens are masked out.
Extensive experiments on diverse text generation tasks show that SpecKD
consistently and significantly outperforms strong KD baselines, leading to more
stable training and more capable student models, and achieving state-of-the-art
results.

</details>


### [25] [Success and Cost Elicit Convention Formation for Efficient Communication](https://arxiv.org/abs/2510.24023)
*Saujas Vaduguru,Yilun Hua,Yoav Artzi,Daniel Fried*

Main category: cs.CL

TL;DR: 本文提出了一种训练大型多模态模型以形成约定，从而实现高效通信的方法。


<details>
  <summary>Details</summary>
Motivation: 人类利用共享的会话上下文来提高通信的成功率和效率。语言惯例的形成就是其中一种表现形式，它使人们能够协调使用较短、成本较低的表达方式，这些表达方式可以使用共享的会话上下文来理解。

Method: 我们的方法使用模型之间的模拟引用游戏，并且不需要额外的人工生成数据。

Result: 在涉及照片和七巧板图像的重复引用游戏中，我们的方法使模型能够与人进行有效的通信：在交互过程中，消息长度最多可减少 41%，而成功率可提高 15%。与形成约定的模型进行交互时，人类听众的反应速度更快。

Conclusion: 仅基于成功或成本的训练是不够的 - 两者对于引发约定形成都是必要的。

Abstract: Humans leverage shared conversational context to become increasingly
successful and efficient at communicating over time. One manifestation of this
is the formation of ad hoc linguistic conventions, which allow people to
coordinate on short, less costly utterances that are understood using shared
conversational context. We present a method to train large multimodal models to
form conventions, enabling efficient communication. Our approach uses simulated
reference games between models, and requires no additional human-produced data.
In repeated reference games involving photographs and tangram images, our
method enables models to communicate efficiently with people: reducing the
message length by up to 41% while increasing success by 15% over the course of
the interaction. Human listeners respond faster when interacting with our model
that forms conventions. We also show that training based on success or cost
alone is insufficient - both are necessary to elicit convention formation.

</details>


### [26] [Pie: A Programmable Serving System for Emerging LLM Applications](https://arxiv.org/abs/2510.24051)
*In Gim,Zhiyao Ma,Seung-seob Lee,Lin Zhong*

Main category: cs.CL

TL;DR: Pie是一个可编程的LLM服务系统，旨在提供灵活性和效率，通过将传统的生成循环分解为细粒度的服务处理程序并通过API公开，并将生成过程的控制权委托给用户提供的程序（inferlet）。


<details>
  <summary>Details</summary>
Motivation: 现有的基于单片令牌生成循环构建的服务系统无法满足新兴的大型语言模型（LLM）应用中涉及的各种推理策略和代理工作流程。

Method: Pie将传统的生成循环分解为细粒度的服务处理程序，并通过API公开，并将生成过程的控制权委托给用户提供的程序（inferlet）。Pie使用WebAssembly执行inferlet，受益于其轻量级沙箱。

Result: Pie在标准任务上与最先进的性能相匹配（3-12%的延迟开销），同时通过启用特定于应用程序的优化，显着提高了代理工作流程的延迟和吞吐量（提高了1.3倍-3.4倍）。

Conclusion: Pie是一个有前途的LLM服务系统，它可以通过实现特定于应用程序的优化来提高代理工作流程的延迟和吞吐量。

Abstract: Emerging large language model (LLM) applications involve diverse reasoning
strategies and agentic workflows, straining the capabilities of existing
serving systems built on a monolithic token generation loop. This paper
introduces Pie, a programmable LLM serving system designed for flexibility and
efficiency. Pie decomposes the traditional generation loop into fine-grained
service handlers exposed via an API and delegates control of the generation
process to user-provided programs, called inferlets. This enables applications
to implement new KV cache strategies, bespoke generation logic, and seamlessly
integrate computation and I/O-entirely within the application, without
requiring modifications to the serving system. Pie executes inferlets using
WebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows
Pie matches state-of-the-art performance on standard tasks (3-12% latency
overhead) while significantly improving latency and throughput (1.3x-3.4x
higher) on agentic workflows by enabling application-specific optimizations.

</details>


### [27] [Challenging Multilingual LLMs: A New Taxonomy and Benchmark for Unraveling Hallucination in Translation](https://arxiv.org/abs/2510.24073)
*Xinwei Wu,Heng Liu,Jiang Zhou,Xiaohu Zhao,Linlong Xu,Longyue Wang,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: 论文介绍了一个用于检测多语言大型语言模型（LLMs）中幻觉现象的诊断框架和基准测试集HalloMTBench。


<details>
  <summary>Details</summary>
Motivation: 现有的机器翻译（MT）基准测试无法有效揭示多语言LLMs中的失败案例，特别是幻觉问题。

Method: 该研究构建了一个包含指令分离和源分离的分类框架，并据此创建了多语言、人工验证的HalloMTBench基准测试集，涵盖11个英语到其他语言的翻译方向。他们使用4个前沿LLMs生成候选结果，并通过LLM judges和专家验证来筛选，最终得到5,435个高质量实例。

Result: 对17个LLMs在HalloMTBench上进行评估，揭示了独特的“幻觉触发器”，反映了模型规模、源文本长度敏感性、语言偏见以及强化学习（RL）放大的语言混合等失败模式。

Conclusion: HalloMTBench为诊断LLM翻译失败提供了一个前瞻性的测试平台。

Abstract: Large Language Models (LLMs) have advanced machine translation but remain
vulnerable to hallucinations. Unfortunately, existing MT benchmarks are not
capable of exposing failures in multilingual LLMs. To disclose hallucination in
multilingual LLMs, we introduce a diagnostic framework with a taxonomy that
separates Instruction Detachment from Source Detachment. Guided by this
taxonomy, we create HalloMTBench, a multilingual, human-verified benchmark
across 11 English-to-X directions. We employed 4 frontier LLMs to generate
candidates and scrutinize these candidates with an ensemble of LLM judges, and
expert validation. In this way, we curate 5,435 high-quality instances. We have
evaluated 17 LLMs on HalloMTBench. Results reveal distinct ``hallucination
triggers'' -- unique failure patterns reflecting model scale, source length
sensitivity, linguistic biases, and Reinforcement-Learning (RL) amplified
language mixing. HalloMTBench offers a forward-looking testbed for diagnosing
LLM translation failures. HalloMTBench is available in
https://huggingface.co/collections/AIDC-AI/marco-mt.

</details>


### [28] [Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+ Languages and Cultures](https://arxiv.org/abs/2510.24081)
*Tyler A. Chang,Catherine Arnett,Abdelrahman Eldesokey,Abdelrahman Sadallah,Abeer Kashar,Abolade Daud,Abosede Grace Olanihun,Adamu Labaran Mohammed,Adeyemi Praise,Adhikarinayum Meerajita Sharma,Aditi Gupta,Afitab Iyigun,Afonso Simplício,Ahmed Essouaied,Aicha Chorana,Akhil Eppa,Akintunde Oladipo,Akshay Ramesh,Aleksei Dorkin,Alfred Malengo Kondoro,Alham Fikri Aji,Ali Eren Çetintaş,Allan Hanbury,Alou Dembele,Alp Niksarli,Álvaro Arroyo,Amin Bajand,Amol Khanna,Ana Chkhaidze,Ana Condez,Andiswa Mkhonto,Andrew Hoblitzell,Andrew Tran,Angelos Poulis,Anirban Majumder,Anna Vacalopoulou,Annette Kuuipolani Kanahele Wong,Annika Simonsen,Anton Kovalev,Ashvanth. S,Ayodeji Joseph Lana,Barkin Kinay,Bashar Alhafni,Benedict Cibalinda Busole,Bernard Ghanem,Bharti Nathani,Biljana Stojanovska Đurić,Bola Agbonile,Bragi Bergsson,Bruce Torres Fischer,Burak Tutar,Burcu Alakuş Çınar,Cade J. Kanoniakapueo Kane,Can Udomcharoenchaikit,Catherine Arnett,Chadi Helwe,Chaithra Reddy Nerella,Chen Cecilia Liu,Chiamaka Glory Nwokolo,Cristina España-Bonet,Cynthia Amol,DaeYeop Lee,Dana Arad,Daniil Dzenhaliou,Daria Pugacheva,Dasol Choi,Daud Abolade,David Liu,David Semedo,Deborah Popoola,Deividas Mataciunas,Delphine Nyaboke,Dhyuthy Krishna Kumar,Diogo Glória-Silva,Diogo Tavares,Divyanshu Goyal,DongGeon Lee,Ebele Nwamaka Anajemba,Egonu Ngozi Grace,Elena Mickel,Elena Tutubalina,Elias Herranen,Emile Anand,Emmanuel Habumuremyi,Emuobonuvie Maria Ajiboye,Eryawan Presma Yulianrifat,Esther Adenuga,Ewa Rudnicka,Faith Olabisi Itiola,Faran Taimoor Butt,Fathima Thekkekara,Fatima Haouari,Filbert Aurelian Tjiaranata,Firas Laakom,Francesca Grasso,Francesco Orabona,Francesco Periti,Gbenga Kayode Solomon,Gia Nghia Ngo,Gloria Udhehdhe-oze,Gonçalo Martins,Gopi Naga Sai Ram Challagolla,Guijin Son,Gulnaz Abdykadyrova,Hafsteinn Einarsson,Hai Hu,Hamidreza Saffari,Hamza Zaidi,Haopeng Zhang,Harethah Abu Shairah,Harry Vuong,Hele-Andra Kuulmets,Houda Bouamor,Hwanjo Yu,Iben Nyholm Debess,İbrahim Ethem Deveci,Ikhlasul Akmal Hanif,Ikhyun Cho,Inês Calvo,Inês Vieira,Isaac Manzi,Ismail Daud,Itay Itzhak,Iuliia,Alekseenko,Ivan Belashkin,Ivan Spada,Ivan Zhelyazkov,Jacob Brinton,Jafar Isbarov,Jaka Čibej,Jan Čuhel,Jan Kocoń,Jauza Akbar Krito,Jebish Purbey,Jennifer Mickel,Jennifer Za,Jenny Kunz,Jihae Jeong,Jimena Tena Dávalos,Jinu Lee,João Magalhães,John Yi,Jongin Kim,Joseph Chataignon,Joseph Marvin Imperial,Jubeerathan Thevakumar,Judith Land,Junchen Jiang,Jungwhan Kim,Kairit Sirts,Kamesh R,Kamesh V,Kanda Patrick Tshinu,Kätriin Kukk,Kaustubh Ponkshe,Kavsar Huseynova,Ke He,Kelly Buchanan,Kengatharaiyer Sarveswaran,Kerem Zaman,Khalil Mrini,Kian Kyars,Krister Kruusmaa,Kusum Chouhan,Lainitha Krishnakumar,Laura Castro Sánchez,Laura Porrino Moscoso,Leshem Choshen,Levent Sencan,Lilja Øvrelid,Lisa Alazraki,Lovina Ehimen-Ugbede,Luheerathan Thevakumar,Luxshan Thavarasa,Mahnoor Malik,Mamadou K. Keita,Mansi Jangid,Marco De Santis,Marcos García,Marek Suppa,Mariam D'Ciofalo,Marii Ojastu,Maryam Sikander,Mausami Narayan,Maximos Skandalis,Mehak Mehak,Mehmet İlteriş Bozkurt,Melaku Bayu Workie,Menan Velayuthan,Michael Leventhal,Michał Marcińczuk,Mirna Potočnjak,Mohammadamin Shafiei,Mridul Sharma,Mrityunjaya Indoria,Muhammad Ravi Shulthan Habibi,Murat Kolić,Nada Galant,Naphat Permpredanun,Narada Maugin,Nicholas Kluge Corrêa,Nikola Ljubešić,Nirmal Thomas,Nisansa de Silva,Nisheeth Joshi,Nitish Ponkshe,Nizar Habash,Nneoma C. Udeze,Noel Thomas,Noémi Ligeti-Nagy,Nouhoum Coulibaly,Nsengiyumva Faustin,Odunayo Kareemat Buliaminu,Odunayo Ogundepo,Oghojafor Godswill Fejiro,Ogundipe Blessing Funmilola,Okechukwu God'spraise,Olanrewaju Samuel,Olaoye Deborah Oluwaseun,Olasoji Akindejoye,Olga Popova,Olga Snissarenko,Onyinye Anulika Chiemezie,Orkun Kinay,Osman Tursun,Owoeye Tobiloba Moses,Oyelade Oluwafemi Joshua,Oyesanmi Fiyinfoluwa,Pablo Gamallo,Pablo Rodríguez Fernández,Palak Arora,Pedro Valente,Peter Rupnik,Philip Oghenesuowho Ekiugbo,Pramit Sahoo,Prokopis Prokopidis,Pua Niau-Puhipau,Quadri Yahya,Rachele Mignone,Raghav Singhal,Ram Mohan Rao Kadiyala,Raphael Merx,Rapheal Afolayan,Ratnavel Rajalakshmi,Rishav Ghosh,Romina Oji,Ron Kekeha Solis,Rui Guerra,Rushikesh Zawar,Sa'ad Nasir Bashir,Saeed Alzaabi,Sahil Sandeep,Sai Pavan Batchu,SaiSandeep Kantareddy,Salsabila Zahirah Pranida,Sam Buchanan,Samuel Rutunda,Sander Land,Sarah Sulollari,Sardar Ali,Saroj Sapkota,Saulius Tautvaisas,Sayambhu Sen,Sayantani Banerjee,Sebastien Diarra,SenthilNathan. M,Sewoong Lee,Shaan Shah,Shankar Venkitachalam,Sharifa Djurabaeva,Sharon Ibejih,Shivanya Shomir Dutta,Siddhant Gupta,Silvia Paniagua Suárez,Sina Ahmadi,Sivasuthan Sukumar,Siyuan Song,Snegha A.,Sokratis Sofianopoulos,Sona Elza Simon,Sonja Benčina,Sophie Gvasalia,Sphurti Kirit More,Spyros Dragazis,Stephan P. Kaufhold,Suba. S,Sultan AlRashed,Surangika Ranathunga,Taiga Someya,Taja Kuzman Pungeršek,Tal Haklay,Tasi'u Jibril,Tatsuya Aoyama,Tea Abashidze,Terenz Jomar Dela Cruz,Terra Blevins,Themistoklis Nikas,Theresa Dora Idoko,Thu Mai Do,Tilek Chubakov,Tommaso Gargiani,Uma Rathore,Uni Johannesen,Uwuma Doris Ugwu,Vallerie Alexandra Putra,Vanya Bannihatti Kumar,Varsha Jeyarajalingam,Varvara Arzt,Vasudevan Nedumpozhimana,Viktoria Ondrejova,Viktoryia Horbik,Vishnu Vardhan Reddy Kummitha,Vuk Dinić,Walelign Tewabe Sewunetie,Winston Wu,Xiaojing Zhao,Yacouba Diarra,Yaniv Nikankin,Yash Mathur,Yixi Chen,Yiyuan Li,Yolanda Xavier,Yonatan Belinkov,Yusuf Ismail Abayomi,Zaid Alyafeai,Zhengyang Shan,Zhi Rui Tam,Zilu Tang,Zuzana Nadova,Baber Abbasi,Stella Biderman,David Stap,Duygu Ataman,Fabian Schmidt,Hila Gonen,Jiayi Wang,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 提出了一个多语言常识推理基准Global PIQA，覆盖超过100种语言，揭示了LLM在低资源语言上的表现差距。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏覆盖多种语言和文化的大型语言模型文化特定评估基准。

Method: 由来自65个国家的335名研究人员手工构建，包含116种语言变体，涵盖五大洲、14个语系和23个书写系统。

Result: 先进的LLM在Global PIQA总体上表现良好，但在低资源语言中表现较差（准确率差距高达37%）。

Conclusion: Global PIQA表明，在许多语言和文化中，日常知识仍有改进空间，并希望Global PIQA能帮助了解人类语言所嵌入的文化多样性。

Abstract: To date, there exist almost no culturally-specific evaluation benchmarks for
large language models (LLMs) that cover a large number of languages and
cultures. In this paper, we present Global PIQA, a participatory commonsense
reasoning benchmark for over 100 languages, constructed by hand by 335
researchers from 65 countries around the world. The 116 language varieties in
Global PIQA cover five continents, 14 language families, and 23 writing
systems. In the non-parallel split of Global PIQA, over 50% of examples
reference local foods, customs, traditions, or other culturally-specific
elements. We find that state-of-the-art LLMs perform well on Global PIQA in
aggregate, but they exhibit weaker performance in lower-resource languages (up
to a 37% accuracy gap, despite random chance at 50%). Open models generally
perform worse than proprietary models. Global PIQA highlights that in many
languages and cultures, everyday knowledge remains an area for improvement,
alongside more widely-discussed capabilities such as complex reasoning and
expert knowledge. Beyond its uses for LLM evaluation, we hope that Global PIQA
provides a glimpse into the wide diversity of cultures in which human language
is embedded.

</details>


### [29] [RegSpeech12: A Regional Corpus of Bengali Spontaneous Speech Across Dialects](https://arxiv.org/abs/2510.24096)
*Md. Rezuwan Hassan,Azmol Hossain,Kanij Fatema,Rubayet Sabbir Faruque,Tanmoy Shome,Ruwad Naswan,Trina Chakraborty,Md. Foriduzzaman Zihad,Tawsif Tashwar Dipto,Nazia Tasnim,Nazmuddoha Ansary,Md. Mehedi Hasan Shawon,Ahmed Imtiaz Humayun,Md. Golam Rabiul Alam,Farig Sadeque,Asif Sushmit*

Main category: cs.CL

TL;DR: 系统地研究孟加拉语方言的计算处理仍然有限。本研究旨在记录和分析这些方言的语音和形态特征，同时探索构建计算模型的可能性，特别是为区域变体量身定制的自动语音识别 (ASR) 系统。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语在南亚和散居社区中被广泛使用，表现出相当大的方言多样性，这种多样性受地理、文化和历史的影响。

Method: 记录和分析这些方言的语音和形态特征，同时探索构建计算模型的可能性，特别是为区域变体量身定制的自动语音识别 (ASR) 系统。

Result: 为本研究创建的数据集公开发布。

Conclusion: 这些努力有可能应用于虚拟助手和更广泛的语言技术，有助于保护方言多样性和推进面向孟加拉语社区的包容性数字工具的进步。

Abstract: The Bengali language, spoken extensively across South Asia and among
diasporic communities, exhibits considerable dialectal diversity shaped by
geography, culture, and history. Phonological and pronunciation-based
classifications broadly identify five principal dialect groups: Eastern
Bengali, Manbhumi, Rangpuri, Varendri, and Rarhi. Within Bangladesh, further
distinctions emerge through variation in vocabulary, syntax, and morphology, as
observed in regions such as Chittagong, Sylhet, Rangpur, Rajshahi, Noakhali,
and Barishal. Despite this linguistic richness, systematic research on the
computational processing of Bengali dialects remains limited. This study seeks
to document and analyze the phonetic and morphological properties of these
dialects while exploring the feasibility of building computational models
particularly Automatic Speech Recognition (ASR) systems tailored to regional
varieties. Such efforts hold potential for applications in virtual assistants
and broader language technologies, contributing to both the preservation of
dialectal diversity and the advancement of inclusive digital tools for
Bengali-speaking communities. The dataset created for this study is released
for public use.

</details>


### [30] [Squrve: A Unified and Modular Framework for Complex Real-World Text-to-SQL Tasks](https://arxiv.org/abs/2510.24102)
*Yihan Wang,Peiyu Liu,Runyu Chen,Jiaxing Pu,Wei Xu*

Main category: cs.CL

TL;DR: Squrve is a new Text-to-SQL framework.


<details>
  <summary>Details</summary>
Motivation: Deploying Text-to-SQL techniques in real-world systems is challenging due to limited integration tools.

Method: A unified, modular, and extensive Text-to-SQL framework is introduced, which establishes a universal execution paradigm and proposes a multi-actor collaboration mechanism based on seven abstracted effective atomic actor components.

Result: Collaborative workflows consistently outperform the original individual methods on widely adopted benchmarks.

Conclusion: Squrve opens up a new effective avenue for tackling complex real-world queries.

Abstract: Text-to-SQL technology has evolved rapidly, with diverse academic methods
achieving impressive results. However, deploying these techniques in real-world
systems remains challenging due to limited integration tools. Despite these
advances, we introduce Squrve, a unified, modular, and extensive Text-to-SQL
framework designed to bring together research advances and real-world
applications. Squrve first establishes a universal execution paradigm that
standardizes invocation interfaces, then proposes a multi-actor collaboration
mechanism based on seven abstracted effective atomic actor components.
Experiments on widely adopted benchmarks demonstrate that the collaborative
workflows consistently outperform the original individual methods, thereby
opening up a new effective avenue for tackling complex real-world queries. The
codes are available at https://github.com/Satissss/Squrve.

</details>


### [31] [Reinforcement Learning for Long-Horizon Multi-Turn Search Agents](https://arxiv.org/abs/2510.24126)
*Vivek Kalyan,Martin Andrews*

Main category: cs.CL

TL;DR: 强化学习（RL）可以显著提升大型语言模型（LLM）智能体的能力，使其在复杂任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的基于提示的方法在解决复杂任务方面表现出色，但通过从经验中学习，强化学习可以进一步提升这些能力。

Method: 通过在法律文档搜索基准上进行实验，使用强化学习训练了一个140亿参数的模型。

Result: 该模型优于前沿模型（85% vs 78% 的准确率）。

Conclusion: 允许智能体在更长的多轮交互中操作，可以获得更好的结果。

Abstract: Large Language Model (LLM) agents can leverage multiple turns and tools to
solve complex tasks, with prompt-based approaches achieving strong performance.
This work demonstrates that Reinforcement Learning (RL) can push capabilities
significantly further by learning from experience. Through experiments on a
legal document search benchmark, we show that our RL-trained 14 Billion
parameter model outperforms frontier class models (85% vs 78% accuracy). In
addition, we explore turn-restricted regimes, during training and at test-time,
that show these agents achieve better results if allowed to operate over longer
multi-turn horizons.

</details>


### [32] [Beyond Line-Level Filtering for the Pretraining Corpora of LLMs](https://arxiv.org/abs/2510.24139)
*Chanwoo Park,Suyoung Park,Yelim Ahn,Jongmin Kim,Jongyeon Park,Jaejin Lee*

Main category: cs.CL

TL;DR: 这篇论文介绍了两种改进的行级过滤技术，分别是模式感知行级去重 (PLD) 和模式感知尾随标点过滤 (PTF)，旨在保留结构上重要的内容。


<details>
  <summary>Details</summary>
Motivation: 传统行级过滤技术可能会丢弃有价值的内容，从而对下游性能产生负面影响。

Method: 通过考虑行级信号及其在文档中的顺序分布来增强传统过滤技术。

Result: 在英语和韩语的1B参数小语言模型上的评估表明，该方法在多项选择基准测试中持续提高性能，并显着提高SQuAD v1和KorQuAD v1上的生成式问答准确性。

Conclusion: 提出的模式感知过滤方法能够保留结构上重要的内容，从而提升模型性能。

Abstract: While traditional line-level filtering techniques, such as line-level
deduplication and trailing-punctuation filters, are commonly used, these basic
methods can sometimes discard valuable content, negatively affecting downstream
performance. In this paper, we introduce two methods-pattern-aware line-level
deduplication (PLD) and pattern-aware trailing punctuation filtering (PTF)-by
enhancing the conventional filtering techniques. Our approach not only
considers line-level signals but also takes into account their sequential
distribution across documents, enabling us to retain structurally important
content that might otherwise be removed. We evaluate these proposed methods by
training small language models (1 B parameters) in both English and Korean. The
results demonstrate that our methods consistently improve performance on
multiple-choice benchmarks and significantly enhance generative
question-answering accuracy on both SQuAD v1 and KorQuAD v1.

</details>


### [33] [Ko-MuSR: A Multistep Soft Reasoning Benchmark for LLMs Capable of Understanding Korean](https://arxiv.org/abs/2510.24150)
*Chanwoo Park,Suyoung Park,JiA Kang,Jongyeon Park,Sangho Kim,Hyunji M. Park,Sumin Bae,Mingyu Kang,Jaejin Lee*

Main category: cs.CL

TL;DR: Ko-MuSR: 评估韩语叙事中多步软推理的首个基准。


<details>
  <summary>Details</summary>
Motivation: 在长篇韩语叙事中，全面评估多步、软推理能力，同时最大限度地减少数据污染。

Method: 构建了完全韩语的叙事、推理链和多项选择题，并通过人工注释员验证了逻辑一致性和可解答性。

Result: 多语言模型在韩语推理任务中优于专注于韩语的模型，表明了推理能力的跨语言泛化。

Conclusion: Ko-MuSR 为推进韩语 NLP 提供了坚实的基础，可以通过系统评估长上下文推理和提示策略。

Abstract: We present Ko-MuSR, the first benchmark to comprehensively evaluate
multistep, soft reasoning in long Korean narratives while minimizing data
contamination. Built following MuSR, Ko-MuSR features fully Korean narratives,
reasoning chains, and multiple-choice questions verified by human annotators
for logical consistency and answerability. Evaluations of four large language
models -- two multilingual and two Korean-specialized -- show that multilingual
models outperform Korean-focused ones even in Korean reasoning tasks,
indicating cross-lingual generalization of reasoning ability. Carefully
designed prompting strategies, which combine few-shot examples, reasoning
traces, and task-specific hints, further boost accuracy, approaching
human-level performance. Ko-MuSR offers a solid foundation for advancing Korean
NLP by enabling systematic evaluation of long-context reasoning and prompting
strategies.

</details>


### [34] [MuSaG: A Multimodal German Sarcasm Dataset with Full-Modal Annotations](https://arxiv.org/abs/2510.24178)
*Aaron Scott,Maike Züfle,Jan Niehues*

Main category: cs.CL

TL;DR: 提出了首个德语多模态讽刺检测数据集MuSaG，包含从德国电视节目中手动选择和人工注释的语句，共33分钟。


<details>
  <summary>Details</summary>
Motivation: 社交媒体和流行文化中普遍存在讽刺，对自然语言理解、情感分析和内容审核构成了持续的挑战。随着多模态大型语言模型的出现，讽刺检测扩展到文本之外，需要整合来自音频和视觉的线索。

Method: 构建了一个包含对齐的文本、音频和视频模态的数据集，并由人工单独注释，从而能够在单模态和多模态设置中进行评估。使用九个开源和商业模型进行基准测试，跨越文本、音频、视觉和多模态架构，并将它们的性能与人类注释进行比较。

Result: 结果表明，虽然人类在对话环境中严重依赖音频，但模型在文本上的表现最佳。这突出了当前多模态模型的差距。

Conclusion: 发布MuSaG以支持未来对多模态讽刺检测和人机对齐的研究。

Abstract: Sarcasm is a complex form of figurative language in which the intended
meaning contradicts the literal one. Its prevalence in social media and popular
culture poses persistent challenges for natural language understanding,
sentiment analysis, and content moderation. With the emergence of multimodal
large language models, sarcasm detection extends beyond text and requires
integrating cues from audio and vision. We present MuSaG, the first German
multimodal sarcasm detection dataset, consisting of 33 minutes of manually
selected and human-annotated statements from German television shows. Each
instance provides aligned text, audio, and video modalities, annotated
separately by humans, enabling evaluation in unimodal and multimodal settings.
We benchmark nine open-source and commercial models, spanning text, audio,
vision, and multimodal architectures, and compare their performance to human
annotations. Our results show that while humans rely heavily on audio in
conversational settings, models perform best on text. This highlights a gap in
current multimodal models and motivates the use of MuSaG for developing models
better suited to realistic scenarios. We release MuSaG publicly to support
future research on multimodal sarcasm detection and human-model alignment.

</details>


### [35] [Exploring the Influence of Relevant Knowledge for Natural Language Generation Interpretability](https://arxiv.org/abs/2510.24179)
*Iván Martínez-Murillo,Paloma Moreda,Elena Lloret*

Main category: cs.CL

TL;DR: 这篇论文探讨了外部知识整合在自然语言生成（NLG）中的影响，重点关注常识生成任务。作者扩展了CommonGen数据集，创建了KITGI基准，该基准将输入概念集与从ConceptNet检索到的语义关系配对，并包括手动注释的输出。


<details>
  <summary>Details</summary>
Motivation: 论文的动机是研究外部知识整合在自然语言生成中的作用，特别是在常识生成任务中。研究人员希望了解外部知识如何影响生成句子的质量，包括常识合理性和概念覆盖率。

Method: 该研究采用了一种三阶段方法：(1) 识别并移除关键知识；(2) 重新生成句子；(3) 手动评估输出的常识合理性和概念覆盖率。使用T5-Large模型，比较了在两种条件下的句子生成：使用完整外部知识和使用过滤知识（删除高度相关的关系）。

Result: 结果表明，使用完整知识生成的句子在两个标准上的正确率达到91%，而过滤后性能急剧下降至6%。

Conclusion: 研究结果表明，相关的外部知识对于维持NLG中的连贯性和概念覆盖率至关重要。这项工作强调了设计可解释的、知识增强的NLG系统的重要性，并呼吁建立能够捕捉表面指标背后潜在推理的评估框架。

Abstract: This paper explores the influence of external knowledge integration in
Natural Language Generation (NLG), focusing on a commonsense generation task.
We extend the CommonGen dataset by creating KITGI, a benchmark that pairs input
concept sets with retrieved semantic relations from ConceptNet and includes
manually annotated outputs. Using the T5-Large model, we compare sentence
generation under two conditions: with full external knowledge and with filtered
knowledge where highly relevant relations were deliberately removed. Our
interpretability benchmark follows a three-stage method: (1) identifying and
removing key knowledge, (2) regenerating sentences, and (3) manually assessing
outputs for commonsense plausibility and concept coverage. Results show that
sentences generated with full knowledge achieved 91\% correctness across both
criteria, while filtering reduced performance drastically to 6\%. These
findings demonstrate that relevant external knowledge is critical for
maintaining both coherence and concept coverage in NLG. This work highlights
the importance of designing interpretable, knowledge-enhanced NLG systems and
calls for evaluation frameworks that capture the underlying reasoning beyond
surface-level metrics.

</details>


### [36] [Beyond Neural Incompatibility: Easing Cross-Scale Knowledge Transfer in Large Language Models through Latent Semantic Alignment](https://arxiv.org/abs/2510.24208)
*Jian Gu,Aldeida Aleti,Chunyang Chen,Hongyu Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种新的参数知识转移（PKT）方法，用于解决大型语言模型（LLM）之间不同尺度知识转移的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法由于神经不兼容性，直接重用层参数受到严重限制。本文旨在实现LLM之间更灵活、更广泛的知识转移。

Method: 该方法以激活作为层间知识转移的媒介，利用潜在空间中的语义对齐。

Result: 在四个基准测试中，该方法优于现有方法。

Conclusion: 该研究揭示了促进跨尺度知识转移的关键因素，并提供了对潜在语义对齐本质的见解。

Abstract: Large Language Models (LLMs) encode vast amounts of knowledge in their
massive parameters, which is accessible to locate, trace, and analyze. Despite
advances in neural interpretability, it is still not clear how to transfer
knowledge in a fine-grained manner, namely parametric knowledge transfer (PKT).
A key problem is enabling effective and efficient knowledge transfer across
LLMs of different scales, which is essential for achieving greater flexibility
and broader applicability in transferring knowledge between LLMs. Due to neural
incompatibility, referring to the architectural and parametric differences
between LLMs of varying scales, existing methods that directly reuse layer
parameters are severely limited. In this paper, we identify the semantic
alignment in latent space as the fundamental prerequisite for LLM cross-scale
knowledge transfer. Instead of directly using the layer parameters, our
approach takes activations as the medium of layer-wise knowledge transfer.
Leveraging the semantics in latent space, our approach is simple and
outperforms prior work, better aligning model behaviors across varying scales.
Evaluations on four benchmarks demonstrate the efficacy of our method. Further
analysis reveals the key factors easing cross-scale knowledge transfer and
provides insights into the nature of latent semantic alignment.

</details>


### [37] [HACK: Hallucinations Along Certainty and Knowledge Axes](https://arxiv.org/abs/2510.24222)
*Adi Simhi,Jonathan Herzig,Itay Itzhak,Dana Arad,Zorik Gekhman,Roi Reichart,Fazl Barez,Gabriel Stanovsky,Idan Szpektor,Yonatan Belinkov*

Main category: cs.CL

TL;DR: LLM中的幻觉是可靠应用的关键障碍。现有的研究通常根据LLM的外部属性而不是潜在的内部属性对幻觉进行分类。这种外部关注忽略了幻觉可能需要基于其底层机制的定制缓解策略。


<details>
  <summary>Details</summary>
Motivation: 现有的研究通常根据LLM的外部属性而不是潜在的内部属性对幻觉进行分类。这种外部关注忽略了幻觉可能需要基于其底层机制的定制缓解策略。

Method: 我们提出了一个框架，用于沿着知识和确定性两个轴对幻觉进行分类。由于参数知识和确定性可能因模型而异，我们的分类方法涉及一个特定于模型的数据集构建过程，该过程区分这些类型的幻觉。沿着知识轴，我们区分了由缺乏知识引起的幻觉和尽管模型具有正确响应的知识而发生的幻觉。

Result: 为了验证我们在知识轴上的框架，我们应用了转向缓解，这依赖于参数知识的存在来操纵模型激活。这解决了缺乏现有方法来验证知识分类的问题，通过显示两种幻觉类型之间的显着差异。我们进一步分析了模型之间不同的知识和幻觉模式，表明尽管共享参数知识，但确实发生了不同的幻觉。转向确定性轴，我们发现了一个特别令人担忧的幻觉子集，其中模型在内部拥有正确知识的情况下以确定性进行幻觉。我们引入了一种新的评估指标来衡量缓解方法在这个子集上的有效性，结果表明，虽然某些方法平均表现良好，但它们在这些关键情况下却不成比例地失败了。

Conclusion: 我们的研究结果强调了在幻觉分析中考虑知识和确定性的重要性，并呼吁采取有针对性的缓解方法，考虑幻觉的潜在因素。

Abstract: Hallucinations in LLMs present a critical barrier to their reliable usage.
Existing research usually categorizes hallucination by their external
properties rather than by the LLMs' underlying internal properties. This
external focus overlooks that hallucinations may require tailored mitigation
strategies based on their underlying mechanism. We propose a framework for
categorizing hallucinations along two axes: knowledge and certainty. Since
parametric knowledge and certainty may vary across models, our categorization
method involves a model-specific dataset construction process that
differentiates between those types of hallucinations. Along the knowledge axis,
we distinguish between hallucinations caused by a lack of knowledge and those
occurring despite the model having the knowledge of the correct response. To
validate our framework along the knowledge axis, we apply steering mitigation,
which relies on the existence of parametric knowledge to manipulate model
activations. This addresses the lack of existing methods to validate knowledge
categorization by showing a significant difference between the two
hallucination types. We further analyze the distinct knowledge and
hallucination patterns between models, showing that different hallucinations do
occur despite shared parametric knowledge. Turning to the certainty axis, we
identify a particularly concerning subset of hallucinations where models
hallucinate with certainty despite having the correct knowledge internally. We
introduce a new evaluation metric to measure the effectiveness of mitigation
methods on this subset, revealing that while some methods perform well on
average, they fail disproportionately on these critical cases. Our findings
highlight the importance of considering both knowledge and certainty in
hallucination analysis and call for targeted mitigation approaches that
consider the hallucination underlying factors.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [38] [Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices](https://arxiv.org/abs/2510.23775)
*Aryan Mathur,Asaduddin Ahmed,Pushti Amit Vasoya,Simeon Kandan Sonar,Yasir Z,Madesh Kuppusamy*

Main category: cs.CV

TL;DR: 提出了一种可解释的图像真伪检测系统，该系统结合了轻量级卷积分类器和视觉语言模型，可以对32x32图像中的伪影进行分类、定位和解释。


<details>
  <summary>Details</summary>
Motivation: 人工智能生成图像的日益逼真对验证视觉真实性提出了挑战。

Method: 结合轻量级卷积分类器（“Faster-Than-Lies”）和视觉语言模型 (Qwen2-VL-7B) 。使用基于自动编码器的重建误差图生成伪影定位热图。

Result: 在扩展的 CiFAKE 数据集上实现了 96.5% 的准确率，并在 8 核 CPU 上保持 175 毫秒的推理时间。将 70 种视觉伪影类型分为八个语义组，并为每个检测到的异常生成可解释的文本。

Conclusion: 证明了结合视觉和语言推理在低分辨率图像中进行可解释的真实性检测的可行性，并概述了法医学、工业检测和社交媒体审核中潜在的跨领域应用。

Abstract: The increasing realism of AI-generated imagery poses challenges for verifying
visual authenticity. We present an explainable image authenticity detection
system that combines a lightweight convolutional classifier
("Faster-Than-Lies") with a Vision-Language Model (Qwen2-VL-7B) to classify,
localize, and explain artifacts in 32x32 images. Our model achieves 96.5%
accuracy on the extended CiFAKE dataset augmented with adversarial
perturbations and maintains an inference time of 175ms on 8-core CPUs, enabling
deployment on local or edge devices. Using autoencoder-based reconstruction
error maps, we generate artifact localization heatmaps, which enhance
interpretability for both humans and the VLM. We further categorize 70 visual
artifact types into eight semantic groups and demonstrate explainable text
generation for each detected anomaly. This work highlights the feasibility of
combining visual and linguistic reasoning for interpretable authenticity
detection in low-resolution imagery and outlines potential cross-domain
applications in forensics, industrial inspection, and social media moderation.

</details>


### [39] [CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting](https://arxiv.org/abs/2510.23785)
*Md Tanvir Hossain,Akif Islam,Mohd Ruhul Ameen*

Main category: cs.CV

TL;DR: 本文介绍了一种名为CountFormer的基于Transformer的框架，用于类别无关的对象计数，通过学习识别重复和结构连贯性来实现。


<details>
  <summary>Details</summary>
Motivation: 现有的计数模型在对象具有复杂的形状、内部对称性或重叠的组件时，经常会出现错误计数，无法复制人类的计数能力。

Method: 该模型建立在CounTR架构之上，用自监督基础模型DINOv2替换了其视觉编码器，并进一步结合了位置嵌入融合，以在通过轻量级卷积解码器将这些特征解码为密度图之前，保留几何关系。

Result: 在FSC-147数据集上评估，该模型取得了与当前最先进方法相当的性能，同时在结构复杂或密集堆积的场景中表现出卓越的准确性。

Conclusion: 研究结果表明，集成诸如DINOv2之类的基础模型使计数系统能够接近类人的结构感知，从而朝着真正通用和无示例的计数范例迈进。

Abstract: Humans can effortlessly count diverse objects by perceiving visual repetition
and structural relationships rather than relying on class identity. However,
most existing counting models fail to replicate this ability; they often
miscount when objects exhibit complex shapes, internal symmetry, or overlapping
components. In this work, we introduce CountFormer, a transformer-based
framework that learns to recognize repetition and structural coherence for
class-agnostic object counting. Built upon the CounTR architecture, our model
replaces its visual encoder with the self-supervised foundation model DINOv2,
which produces richer and spatially consistent feature representations. We
further incorporate positional embedding fusion to preserve geometric
relationships before decoding these features into density maps through a
lightweight convolutional decoder. Evaluated on the FSC-147 dataset, our model
achieves performance comparable to current state-of-the-art methods while
demonstrating superior accuracy on structurally intricate or densely packed
scenes. Our findings indicate that integrating foundation models such as DINOv2
enables counting systems to approach human-like structural perception,
advancing toward a truly general and exemplar-free counting paradigm.

</details>


### [40] [A geometric and deep learning reproducible pipeline for monitoring floating anthropogenic debris in urban rivers using in situ cameras](https://arxiv.org/abs/2510.23798)
*Gauthier Grimmer,Romain Wenger,Clément Flint,Germain Forestier,Gilles Rixhon,Valentin Chardon*

Main category: cs.CV

TL;DR: 本文提出了一种利用固定摄像头监测河流中漂浮垃圾的方法。


<details>
  <summary>Details</summary>
Motivation: 河流中漂浮的人造垃圾对生物多样性、水质和人类活动产生不利影响。

Method: 利用深度学习和几何模型，通过固定摄像头连续量化和监测漂浮垃圾，并估计检测到的物体的大小。

Result: 研究强调了数据集构成协议的重要性，特别是在整合负面图像和考虑时间泄漏方面。证明了使用投影几何和回归校正进行度量对象估计的可行性。

Conclusion: 该方法为开发城市水生环境的鲁棒、低成本、自动化监测系统铺平了道路。

Abstract: The proliferation of floating anthropogenic debris in rivers has emerged as a
pressing environmental concern, exerting a detrimental influence on
biodiversity, water quality, and human activities such as navigation and
recreation. The present study proposes a novel methodological framework for the
monitoring the aforementioned waste, utilising fixed, in-situ cameras. This
study provides two key contributions: (i) the continuous quantification and
monitoring of floating debris using deep learning and (ii) the identification
of the most suitable deep learning model in terms of accuracy and inference
speed under complex environmental conditions. These models are tested in a
range of environmental conditions and learning configurations, including
experiments on biases related to data leakage. Furthermore, a geometric model
is implemented to estimate the actual size of detected objects from a 2D image.
This model takes advantage of both intrinsic and extrinsic characteristics of
the camera. The findings of this study underscore the significance of the
dataset constitution protocol, particularly with respect to the integration of
negative images and the consideration of temporal leakage. In conclusion, the
feasibility of metric object estimation using projective geometry coupled with
regression corrections is demonstrated. This approach paves the way for the
development of robust, low-cost, automated monitoring systems for urban aquatic
environments.

</details>


### [41] [RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution of Rare-Earth Features](https://arxiv.org/abs/2510.23816)
*Forouzan Fallah,Wenwen Li,Chia-Yu Hsu,Hyunho Lee,Yezhou Yang*

Main category: cs.CV

TL;DR: 提出了一种新的遥感图像超分辨率(SR)框架RareFlow，该框架在域外(OOD)条件下具有鲁棒性，通过双重条件架构、多方面的损失函数和不确定性量化来提高图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有的遥感图像超分辨率方法在OOD条件下表现不佳，产生视觉上合理但物理上不准确的结果。因此，本文旨在解决遥感图像超分辨率在OOD条件下的鲁棒性问题。

Method: 该方法的核心是一个双重条件架构，利用门控ControlNet从低分辨率输入中保留细粒度的几何保真度，同时利用文本提示为合成复杂特征提供语义指导。此外，还引入了一个多方面的损失函数，以确保输出在光谱和辐射上与传感器特性保持一致。该框架还通过采用随机前向传递方法来量化其自身的预测不确定性。

Result: 在多传感器卫星图像的新基准上验证了RareFlow。地球物理专家评估表明，该模型的输出接近地面实况图像的保真度，显著优于最先进的基线模型。感知指标的定量增益也证实了这种定性优势，包括FID降低了近40%。

Conclusion: RareFlow为数据稀缺的科学领域中的高保真合成提供了一个鲁棒的框架，并为严重领域转移下的受控生成提供了一个新的范例。

Abstract: Super-resolution (SR) for remote sensing imagery often fails under
out-of-distribution (OOD) conditions, such as rare geomorphic features captured
by diverse sensors, producing visually plausible but physically inaccurate
results. We present RareFlow, a physics-aware SR framework designed for OOD
robustness. RareFlow's core is a dual-conditioning architecture. A Gated
ControlNet preserves fine-grained geometric fidelity from the low-resolution
input, while textual prompts provide semantic guidance for synthesizing complex
features. To ensure physically sound outputs, we introduce a multifaceted loss
function that enforces both spectral and radiometric consistency with sensor
properties. Furthermore, the framework quantifies its own predictive
uncertainty by employing a stochastic forward pass approach; the resulting
output variance directly identifies unfamiliar inputs, mitigating feature
hallucination. We validate RareFlow on a new, curated benchmark of multi-sensor
satellite imagery. In blind evaluations, geophysical experts rated our model's
outputs as approaching the fidelity of ground truth imagery, significantly
outperforming state-of-the-art baselines. This qualitative superiority is
corroborated by quantitative gains in perceptual metrics, including a nearly
40\% reduction in FID. RareFlow provides a robust framework for high-fidelity
synthesis in data-scarce scientific domains and offers a new paradigm for
controlled generation under severe domain shift.

</details>


### [42] [TRELLISWorld: Training-Free World Generation from Object Generators](https://arxiv.org/abs/2510.23880)
*Hanke Chen,Yuan Liu,Minchen Li*

Main category: cs.CV

TL;DR: 提出了一种新的训练方法，通过重新利用文本到3D对象扩散模型作为模块化瓦片生成器，实现可扩展的、连贯的场景合成，同时保留局部语义控制。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常局限于单对象生成，需要特定领域的训练，或者缺乏对完整360度视角的支持。

Method: 将场景生成重新定义为多瓦片去噪问题，其中重叠的3D区域被独立生成并通过加权平均无缝混合。

Result: 该方法支持多样化的场景布局、高效的生成和灵活的编辑。

Conclusion: 该方法为通用、语言驱动的3D场景构建奠定了简单而强大的基础。

Abstract: Text-driven 3D scene generation holds promise for a wide range of
applications, from virtual prototyping to AR/VR and simulation. However,
existing methods are often constrained to single-object generation, require
domain-specific training, or lack support for full 360-degree viewability. In
this work, we present a training-free approach to 3D scene synthesis by
repurposing general-purpose text-to-3D object diffusion models as modular tile
generators. We reformulate scene generation as a multi-tile denoising problem,
where overlapping 3D regions are independently generated and seamlessly blended
via weighted averaging. This enables scalable synthesis of large, coherent
scenes while preserving local semantic control. Our method eliminates the need
for scene-level datasets or retraining, relies on minimal heuristics, and
inherits the generalization capabilities of object-level priors. We demonstrate
that our approach supports diverse scene layouts, efficient generation, and
flexible editing, establishing a simple yet powerful foundation for
general-purpose, language-driven 3D scene construction.

</details>


### [43] [Improving Visual Discriminability of CLIP for Training-Free Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2510.23894)
*Jinxin Zhou,Jiachen Jiang,Zhihui Zhu*

Main category: cs.CV

TL;DR: LHT-CLIP is a training-free framework that improves CLIP models for semantic segmentation by exploiting visual discriminability across different levels.


<details>
  <summary>Details</summary>
Motivation: Extending CLIP models to semantic segmentation is difficult due to the misalignment between image-level pre-training and pixel-level understanding. Existing methods often inherit global alignment bias, leading to suboptimal performance.

Method: The authors propose LHT-CLIP, which includes semantic-spatial reweighting, selective head enhancement, and abnormal token replacement to restore visual discriminability.

Result: LHT-CLIP achieves state-of-the-art performance on 8 semantic segmentation benchmarks.

Conclusion: LHT-CLIP effectively improves segmentation performance without additional training, auxiliary networks, or extensive hyperparameter tuning, making it practical for real-world deployment.

Abstract: Extending CLIP models to semantic segmentation remains challenging due to the
misalignment between their image-level pre-training objectives and the
pixel-level visual understanding required for dense prediction. While prior
efforts have achieved encouraging results by reorganizing the final layer and
features, they often inherit the global alignment bias of preceding layers,
leading to suboptimal segmentation performance. In this work, we propose
LHT-CLIP, a novel training-free framework that systematically exploits the
visual discriminability of CLIP across layer, head, and token levels. Through
comprehensive analysis, we reveal three key insights: (i) the final layers
primarily strengthen image-text alignment with sacrifice of visual
discriminability (e.g., last 3 layers in ViT-B/16 and 8 layers in ViT-L/14),
partly due to the emergence of anomalous tokens; (ii) a subset of attention
heads (e.g., 10 out of 144 in ViT-B/16) display consistently strong visual
discriminability across datasets; (iii) abnormal tokens display sparse and
consistent activation pattern compared to normal tokens. Based on these
findings, we propose three complementary techniques: semantic-spatial
reweighting, selective head enhancement, and abnormal token replacement to
effectively restore visual discriminability and improve segmentation
performance without any additional training, auxiliary pre-trained networks, or
extensive hyperparameter tuning. Extensive experiments on 8 common semantic
segmentation benchmarks demonstrate that LHT-CLIP achieves state-of-the-art
performance across diverse scenarios, highlighting its effectiveness and
practicality for real-world deployment.

</details>


### [44] [DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning](https://arxiv.org/abs/2510.23907)
*Eddison Pham,Prisha Priyadarshini,Adrian Maliackel,Kanishk Bandi,Cristian Meo,Kevin Zhu*

Main category: cs.CV

TL;DR: DynaStride是一种生成连贯场景级别字幕的流程，无需手动场景分割，通过自适应帧采样和多模态窗口处理来捕捉关键转场，并使用多模态思维链过程生成动作-对象对，最后通过动态步幅窗口选择算法进行优化和融合。


<details>
  <summary>Details</summary>
Motivation: 现有的字幕可能缺乏连贯性和质量，从而削弱视频的教育意图。

Method: DynaStride流程，采用自适应帧采样和多模态窗口处理，并结合多模态思维链和动态步幅窗口选择算法。

Result: 在N-gram指标和语义相似性指标上均优于VLLaMA3和GPT-4o等基线模型。定性分析表明，DynaStride生成的字幕在时间上更连贯和信息更丰富。

Conclusion: DynaStride为改进人工智能驱动的教学内容生成提供了一个有希望的方向。

Abstract: Scene-level captioning in instructional videos can enhance learning by
requiring an understanding of both visual cues and temporal structure. By
aligning visual cues with textual guidance, this understanding supports
procedural learning and multimodal reasoning, providing a richer context for
skill acquisition. However, captions that fail to capture this structure may
lack coherence and quality, which can create confusion and undermine the
video's educational intent. To address this gap, we introduce DynaStride, a
pipeline to generate coherent, scene-level captions without requiring manual
scene segmentation. Using the YouCookII dataset's scene annotations, DynaStride
performs adaptive frame sampling and multimodal windowing to capture key
transitions within each scene. It then employs a multimodal chain-of-thought
process to produce multiple action-object pairs, which are refined and fused
using a dynamic stride window selection algorithm that adaptively balances
temporal context and redundancy. The final scene-level caption integrates
visual semantics and temporal reasoning in a single instructional caption.
Empirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,
demonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and
semantic similarity measures (BERTScore, CLIPScore). Qualitative analyses
further show that DynaStride produces captions that are more temporally
coherent and informative, suggesting a promising direction for improving
AI-powered instructional content generation.

</details>


### [45] [TurboPortrait3D: Single-step diffusion-based fast portrait novel-view synthesis](https://arxiv.org/abs/2510.23929)
*Emily Kim,Julieta Martinez,Timur Bagautdinov,Jessica Hodgins*

Main category: cs.CV

TL;DR: 提出了一种名为 TurboPortrait3D 的方法，用于低延迟的人像新视角合成。


<details>
  <summary>Details</summary>
Motivation: 现有图像到 3D 的人像生成模型容易产生视觉伪影，缺乏细节，并且无法完全保留对象的身份。图像扩散模型擅长生成高质量图像，但计算成本高，并且没有以 3D 为基础，因此无法直接生成多视图一致的输出。

Method: 该方法采用单张主体正面图像作为输入，并应用前馈图像到头像生成管道以获得初始 3D 表示和相应的噪声渲染。然后将这些噪声渲染输入到单步扩散模型中，该模型以输入图像为条件，并且经过专门训练以多视图一致的方式细化渲染。

Result: 该方法在质量和数量上均优于当前最先进的人像新视角合成方法，同时在时间上也很有效率。

Conclusion: 图像空间扩散模型可用于显着提高现有图像到头像方法的质量，同时保持 3D 感知并以低延迟运行。

Abstract: We introduce TurboPortrait3D: a method for low-latency novel-view synthesis
of human portraits. Our approach builds on the observation that existing
image-to-3D models for portrait generation, while capable of producing
renderable 3D representations, are prone to visual artifacts, often lack of
detail, and tend to fail at fully preserving the identity of the subject. On
the other hand, image diffusion models excel at generating high-quality images,
but besides being computationally expensive, are not grounded in 3D and thus
are not directly capable of producing multi-view consistent outputs. In this
work, we demonstrate that image-space diffusion models can be used to
significantly enhance the quality of existing image-to-avatar methods, while
maintaining 3D-awareness and running with low-latency. Our method takes a
single frontal image of a subject as input, and applies a feedforward
image-to-avatar generation pipeline to obtain an initial 3D representation and
corresponding noisy renders. These noisy renders are then fed to a single-step
diffusion model which is conditioned on input image(s), and is specifically
trained to refine the renders in a multi-view consistent way. Moreover, we
introduce a novel effective training strategy that includes pre-training on a
large corpus of synthetic multi-view data, followed by fine-tuning on
high-quality real images. We demonstrate that our approach both qualitatively
and quantitatively outperforms current state-of-the-art for portrait novel-view
synthesis, while being efficient in time.

</details>


### [46] [PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors](https://arxiv.org/abs/2510.23930)
*Xirui Jin,Renbiao Jin,Boying Li,Danping Zou,Wenxian Yu*

Main category: cs.CV

TL;DR: PlanarGS: A 3DGS-based framework for indoor scene reconstruction, uses language-prompted planar priors to overcome the limitations of 3DGS in low-texture indoor environments.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting (3DGS) struggles in indoor scenes with large, low-texture regions, leading to ambiguous geometry and poor 3D surface reconstruction.

Method: Introduces a Language-Prompted Planar Priors (LP3) pipeline that uses a vision-language segmentation model and refines region proposals with cross-view fusion and geometric priors.  It optimizes 3D Gaussians with planar consistency and geometric prior supervision (depth and normal cues).

Result: PlanarGS reconstructs accurate and detailed 3D surfaces, significantly outperforming state-of-the-art methods on standard indoor benchmarks.

Conclusion: PlanarGS effectively addresses the limitations of 3DGS in indoor scenes by incorporating planar priors, leading to improved 3D surface reconstruction quality.

Abstract: Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an
efficient representation for novel-view synthesis, achieving impressive visual
quality. However, in scenes dominated by large and low-texture regions, common
in indoor environments, the photometric loss used to optimize 3DGS yields
ambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome
this limitation, we introduce PlanarGS, a 3DGS-based framework tailored for
indoor scene reconstruction. Specifically, we design a pipeline for
Language-Prompted Planar Priors (LP3) that employs a pretrained vision-language
segmentation model and refines its region proposals via cross-view fusion and
inspection with geometric priors. 3D Gaussians in our framework are optimized
with two additional terms: a planar prior supervision term that enforces planar
consistency, and a geometric prior supervision term that steers the Gaussians
toward the depth and normal cues. We have conducted extensive experiments on
standard indoor benchmarks. The results show that PlanarGS reconstructs
accurate and detailed 3D surfaces, consistently outperforming state-of-the-art
methods by a large margin. Project page: https://planargs.github.io

</details>


### [47] [Adaptive Training of INRs via Pruning and Densification](https://arxiv.org/abs/2510.23943)
*Diana Aldana,João Paulo Lima,Daniel Csillag,Daniel Perazzo,Haoan Feng,Luiz Velho,Tiago Novello*

Main category: cs.CV

TL;DR: 提出了一种名为AIRe的自适应训练方案，用于优化隐式神经表示(INR)的架构。


<details>
  <summary>Details</summary>
Motivation: 现有的隐式神经表示方法在选择合适的输入频率和架构，以及管理参数冗余方面存在挑战，通常需要启发式方法和大量的超参数优化。

Method: 该方法使用神经元剪枝机制来避免冗余，并使用输入频率密集化来提高表示能力。具体来说，首先识别贡献较小的神经元，并应用有针对性的权重衰减将其信息转移到剩余的神经元，然后进行结构化剪枝。接下来，密集化阶段将输入频率添加到信号欠拟合的频谱区域，从而扩展表示基础。

Result: 在图像和SDF上的实验表明，AIRe可以在减小模型尺寸的同时保持甚至提高重建质量。

Conclusion: AIRe 能够在减小模型尺寸的同时保持甚至提高重建质量，实现了网络大小和重建质量之间的更好权衡。

Abstract: Encoding input coordinates with sinusoidal functions into multilayer
perceptrons (MLPs) has proven effective for implicit neural representations
(INRs) of low-dimensional signals, enabling the modeling of high-frequency
details. However, selecting appropriate input frequencies and architectures
while managing parameter redundancy remains an open challenge, often addressed
through heuristics and heavy hyperparameter optimization schemes. In this
paper, we introduce AIRe ($\textbf{A}$daptive $\textbf{I}$mplicit neural
$\textbf{Re}$presentation), an adaptive training scheme that refines the INR
architecture over the course of optimization. Our method uses a neuron pruning
mechanism to avoid redundancy and input frequency densification to improve
representation capacity, leading to an improved trade-off between network size
and reconstruction quality. For pruning, we first identify less-contributory
neurons and apply a targeted weight decay to transfer their information to the
remaining neurons, followed by structured pruning. Next, the densification
stage adds input frequencies to spectrum regions where the signal underfits,
expanding the representational basis. Through experiments on images and SDFs,
we show that AIRe reduces model size while preserving, or even improving,
reconstruction quality. Code and pretrained models will be released for public
use.

</details>


### [48] [Neural USD: An object-centric framework for iterative editing and control](https://arxiv.org/abs/2510.23956)
*Alejandro Escontrela,Shrinu Kushagra,Sjoerd van Steenkiste,Yulia Rubanova,Aleksander Holynski,Kelsey Allen,Kevin Murphy,Thomas Kipf*

Main category: cs.CV

TL;DR: 提出了一种新的图像生成框架，可以对图像中的对象进行精确和迭代编辑，避免了全局性改变。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成方法在编辑图像时，容易引起全局性改变。

Method: 借鉴计算机图形学中的通用场景描述(USD)标准，提出了神经通用场景描述(Neural USD)，以分层结构化的方式表示场景和对象。

Result: 通过实验验证了Neural USD框架的有效性，展示了其支持迭代和增量工作流的能力。

Conclusion: Neural USD框架能够实现对图像中对象的外观、几何形状和姿态的逐对象控制，并保证控制信号的解耦。

Abstract: Amazing progress has been made in controllable generative modeling,
especially over the last few years. However, some challenges remain. One of
them is precise and iterative object editing. In many of the current methods,
trying to edit the generated image (for example, changing the color of a
particular object in the scene or changing the background while keeping other
elements unchanged) by changing the conditioning signals often leads to
unintended global changes in the scene. In this work, we take the first steps
to address the above challenges. Taking inspiration from the Universal Scene
Descriptor (USD) standard developed in the computer graphics community, we
introduce the "Neural Universal Scene Descriptor" or Neural USD. In this
framework, we represent scenes and objects in a structured, hierarchical
manner. This accommodates diverse signals, minimizes model-specific
constraints, and enables per-object control over appearance, geometry, and
pose. We further apply a fine-tuning approach which ensures that the above
control signals are disentangled from one another. We evaluate several design
considerations for our framework, demonstrating how Neural USD enables
iterative and incremental workflows. More information at:
https://escontrela.me/neural_usd .

</details>


### [49] [SafeVision: Efficient Image Guardrail with Robust Policy Adherence and Explainability](https://arxiv.org/abs/2510.23960)
*Peiyang Xu,Minzhou Pan,Zhaorun Chen,Shuang Yang,Chaowei Xiao,Bo Li*

Main category: cs.CV

TL;DR: SafeVision: A novel image guardrail that integrates human-like reasoning to enhance adaptability and transparency.


<details>
  <summary>Details</summary>
Motivation: Traditional image guardrail models often misclassify content and struggle to adapt to emerging threats.

Method: An effective data collection and generation framework, a policy-following training pipeline, and a customized loss function are used. A diverse QA generation and training strategy to enhance learning effectiveness is proposed.

Result: SafeVision achieves state-of-the-art performance on different benchmarks, outperforming GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while being over 16x faster.

Conclusion: SafeVision sets a comprehensive, policy-following, and explainable image guardrail with dynamic adaptation to emerging threats.

Abstract: With the rapid proliferation of digital media, the need for efficient and
transparent safeguards against unsafe content is more critical than ever.
Traditional image guardrail models, constrained by predefined categories, often
misclassify content due to their pure feature-based learning without semantic
reasoning. Moreover, these models struggle to adapt to emerging threats,
requiring costly retraining for new threats. To address these limitations, we
introduce SafeVision, a novel image guardrail that integrates human-like
reasoning to enhance adaptability and transparency. Our approach incorporates
an effective data collection and generation framework, a policy-following
training pipeline, and a customized loss function. We also propose a diverse QA
generation and training strategy to enhance learning effectiveness. SafeVision
dynamically aligns with evolving safety policies at inference time, eliminating
the need for retraining while ensuring precise risk assessments and
explanations. Recognizing the limitations of existing unsafe image benchmarks,
which either lack granularity or cover limited risks, we introduce VisionHarm,
a high-quality dataset comprising two subsets: VisionHarm Third-party
(VisionHarm-T) and VisionHarm Comprehensive(VisionHarm-C), spanning diverse
harmful categories. Through extensive experiments, we show that SafeVision
achieves state-of-the-art performance on different benchmarks. SafeVision
outperforms GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while
being over 16x faster. SafeVision sets a comprehensive, policy-following, and
explainable image guardrail with dynamic adaptation to emerging threats.

</details>


### [50] [Reasoning Visual Language Model for Chest X-Ray Analysis](https://arxiv.org/abs/2510.23968)
*Andriy Myronenko,Dong Yang,Baris Turkbey,Mariam Aboian,Sena Azamat,Esra Akcicek,Hongxu Yin,Pavlo Molchanov,Marc Edgar,Yufan He,Pengfei Guo,Yucheng Tang,Daguang Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种用于胸部X光片解释的链式思考(CoT)推理框架，旨在提高医学图像分析的可解释性和透明度。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在医学图像分析中缺乏透明的、逐步的推理过程，这与临床医生的需求不符。

Method: 该方法通过对中间步骤与可观察的图像证据和放射学工作流程进行对齐，学习专家如何推理。该模型结合了高保真视觉编码和两阶段训练方法：推理风格的监督微调(SFT)和强化学习(RL)。

Result: 该方法在分布外评估中实现了具有竞争力的多标签分类，同时提高了可解释性。专家放射科医生的研究表明，完整的推理过程提高了信心，支持了错误审计，并减少了完成报告的时间。

Conclusion: 该研究发布了代码和模型，以支持社区在胸部X光片和其他医学成像任务中实现可信、可解释的AI。

Abstract: Vision-language models (VLMs) have shown strong promise for medical image
analysis, but most remain opaque, offering predictions without the transparent,
stepwise reasoning clinicians rely on. We present a framework that brings
chain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by
reasoning-first training paradigms, our approach is designed to learn how
experts reason, not just what they conclude, by aligning intermediate steps
with observable image evidence and radiology workflow. Beyond accuracy, the
explicit reasoning traces support clinical auditability: they reveal why a
conclusion was reached, which alternatives were considered, and where
uncertainty remains, enabling quality assurance, error analysis, and safer
human-AI collaboration.
  Our model couples high-fidelity visual encoding with a two-stage training
recipe: a reasoning-style supervised fine-tuning (SFT) followed by
reinforcement learning (RL) that uses verifiable rewards over a list of X-ray
abnormalities. The model outputs reasoning that mirrors radiologists systematic
thought process, uncertainty, and differential diagnosis. In
out-of-distribution evaluation, the approach achieves competitive multi-label
classification while improving interpretability. In a reader study with expert
radiologists, full reasoning traces increased confidence, supported error
auditing, and reduced time to finalize reports. We release code and the model
NV-Reason-CXR-3B to support community progress toward trustworthy, explainable
AI in chest radiography and other medical imaging tasks where reasoning quality
is as critical as prediction quality.

</details>


### [51] [Efficient Cost-and-Quality Controllable Arbitrary-scale Super-resolution with Fourier Constraints](https://arxiv.org/abs/2510.23978)
*Kazutoshi Akita,Norimichi Ukita*

Main category: cs.CV

TL;DR: 本文提出了一种联合预测多个傅里叶分量的方法，以提高超分辨率重建的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法逐个预测傅里叶分量，导致性能下降和效率低下。因此，本文旨在解决任意尺度超分辨率重建中成本和质量可控性的问题。

Method: 提出联合预测多个分量。

Result: 提高了质量和效率。

Conclusion: 联合预测多个傅里叶分量的方法可以提高超分辨率重建的质量和效率。

Abstract: Cost-and-Quality (CQ) controllability in arbitrary-scale super-resolution is
crucial. Existing methods predict Fourier components one by one using a
recurrent neural network. However, this approach leads to performance
degradation and inefficiency due to independent prediction. This paper proposes
predicting multiple components jointly to improve both quality and efficiency.

</details>


### [52] [TeleEgo: Benchmarking Egocentric AI Assistants in the Wild](https://arxiv.org/abs/2510.23981)
*Jiaqi Yan,Ruilong Ren,Jingren Liu,Shuning Xu,Ling Wang,Yiheng Wang,Yun Wang,Long Zhang,Xiangyu Chen,Changzhi Sun,Jixiang Luo,Dell Zhang,Hao Sun,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleEgo: A comprehensive, long-duration, streaming, omni-modal benchmark for egocentric AI assistants in realistic daily contexts.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks evaluate abilities in isolation, lack realistic streaming scenarios, or support only short-term tasks.

Method: A new benchmark dataset called TeleEgo is introduced, featuring synchronized egocentric video, audio, and text across four domains. Defines 12 diagnostic subtasks across three core capabilities: Memory, Understanding, and Cross-Memory Reasoning. Contains 3,291 human-verified QA items.

Result: Two key metrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess correctness, temporal responsiveness, and long-term retention.

Conclusion: TeleEgo provides a realistic and comprehensive evaluation to advance the development of practical AI assistants.

Abstract: Egocentric AI assistants in real-world settings must process multi-modal
inputs (video, audio, text), respond in real time, and retain evolving
long-term memory. However, existing benchmarks typically evaluate these
abilities in isolation, lack realistic streaming scenarios, or support only
short-term tasks. We introduce \textbf{TeleEgo}, a long-duration, streaming,
omni-modal benchmark for evaluating egocentric AI assistants in realistic daily
contexts. The dataset features over 14 hours per participant of synchronized
egocentric video, audio, and text across four domains: work \& study, lifestyle
\& routines, social activities, and outings \& culture. All data is aligned on
a unified global timeline and includes high-quality visual narrations and
speech transcripts, curated through human refinement.TeleEgo defines 12
diagnostic subtasks across three core capabilities: Memory (recalling past
events), Understanding (interpreting the current moment), and Cross-Memory
Reasoning (linking distant events). It contains 3,291 human-verified QA items
spanning multiple question formats (single-choice, binary, multi-choice, and
open-ended), evaluated strictly in a streaming setting. We propose two key
metrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess
correctness, temporal responsiveness, and long-term retention. TeleEgo provides
a realistic and comprehensive evaluation to advance the development of
practical AI assistants.

</details>


### [53] [AdvBlur: Adversarial Blur for Robust Diabetic Retinopathy Classification and Cross-Domain Generalization](https://arxiv.org/abs/2510.24000)
*Heethanjan Kanagalingam,Thenukan Pathmanathan,Mokeeshan Vathanakumar,Tharmakulasingam Mukunthan*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的糖尿病视网膜病变 (DR) 分类方法 AdvBlur，以解决由于采集设备、人口差异和成像条件引起的分布变化导致的鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变 (DR) 是全球视力丧失的主要原因，早期准确的检测可以显著改善治疗效果。现有的深度学习 (DL) 模型在预测 DR 方面面临鲁棒性挑战。

Method: 该方法将对抗模糊图像集成到数据集中，并采用双重损失函数框架来解决域泛化问题。

Result: 该方法有效地减轻了看不见的分布变化的影响，在多个数据集上的综合评估证明了这一点。与最先进的域泛化 DR 模型相比，该方法在看不见的外部数据集上实现了具有竞争力的性能。

Conclusion: 实验结果证明了该方法的有效性。

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, yet
early and accurate detection can significantly improve treatment outcomes.
While numerous Deep learning (DL) models have been developed to predict DR from
fundus images, many face challenges in maintaining robustness due to
distributional variations caused by differences in acquisition devices,
demographic disparities, and imaging conditions. This paper addresses this
critical limitation by proposing a novel DR classification approach, a method
called AdvBlur. Our method integrates adversarial blurred images into the
dataset and employs a dual-loss function framework to address domain
generalization. This approach effectively mitigates the impact of unseen
distributional variations, as evidenced by comprehensive evaluations across
multiple datasets. Additionally, we conduct extensive experiments to explore
the effects of factors such as camera type, low-quality images, and dataset
size. Furthermore, we perform ablation studies on blurred images and the loss
function to ensure the validity of our choices. The experimental results
demonstrate the effectiveness of our proposed method, achieving competitive
performance compared to state-of-the-art domain generalization DR models on
unseen external datasets.

</details>


### [54] [Towards the Automatic Segmentation, Modeling and Meshing of the Aortic Vessel Tree from Multicenter Acquisitions: An Overview of the SEG.A. 2023 Segmentation of the Aorta Challenge](https://arxiv.org/abs/2510.24009)
*Yuan Jin,Antonio Pepe,Gian Marco Melito,Yuxuan Chen,Yunsu Byeon,Hyeseong Kim,Kyungwon Kim,Doohyun Park,Euijoon Choi,Dosik Hwang,Andriy Myronenko,Dong Yang,Yufan He,Daguang Xu,Ayman El-Ghotni,Mohamed Nabil,Hossam El-Kady,Ahmed Ayyad,Amr Nasr,Marek Wodzinski,Henning Müller,Hyeongyu Kim,Yejee Shin,Abbas Khan,Muhammad Asad,Alexander Zolotarev,Caroline Roney,Anthony Mathur,Martin Benning,Gregory Slabaugh,Theodoros Panagiotis Vagenas,Konstantinos Georgas,George K. Matsopoulos,Jihan Zhang,Zhen Zhang,Liqin Huang,Christian Mayer,Heinrich Mächler,Jan Egger*

Main category: cs.CV

TL;DR: SEG.A. 挑战赛发布了一个大型AVT分割数据集，以促进该领域的发展。


<details>
  <summary>Details</summary>
Motivation: 缺乏共享的高质量数据阻碍了AVT自动分析的发展。

Method: 通过SEG.A.挑战赛，使用隐藏的测试集对自动算法进行基准测试，并进行表面网格划分的可选任务。

Result: 结果表明，深度学习方法趋于一致，3D U-Net架构在顶级提交中占主导地位。算法集成明显优于单个模型，并且性能与算法设计和训练数据的特征密切相关。

Conclusion: 该计划建立了一个新的性能基准，并提供了一个持久的资源，以推动未来创新，实现强大的、可临床转化的工具。

Abstract: The automated analysis of the aortic vessel tree (AVT) from computed
tomography angiography (CTA) holds immense clinical potential, but its
development has been impeded by a lack of shared, high-quality data. We
launched the SEG.A. challenge to catalyze progress in this field by introducing
a large, publicly available, multi-institutional dataset for AVT segmentation.
The challenge benchmarked automated algorithms on a hidden test set, with
subsequent optional tasks in surface meshing for computational simulations. Our
findings reveal a clear convergence on deep learning methodologies, with 3D
U-Net architectures dominating the top submissions. A key result was that an
ensemble of the highest-ranking algorithms significantly outperformed
individual models, highlighting the benefits of model fusion. Performance was
strongly linked to algorithmic design, particularly the use of customized
post-processing steps, and the characteristics of the training data. This
initiative not only establishes a new performance benchmark but also provides a
lasting resource to drive future innovation toward robust, clinically
translatable tools.

</details>


### [55] [Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science Tasks](https://arxiv.org/abs/2510.24010)
*Mirali Purohit,Bimal Gajera,Vatsal Malaviya,Irish Mehta,Kunal Kasodekar,Jacob Adler,Steven Lu,Umaa Rebbapragada,Hannah Kerner*

Main category: cs.CV

TL;DR: 本文介绍了Mars-Bench，一个用于评估火星相关任务模型的基准。


<details>
  <summary>Details</summary>
Motivation: 火星科学缺乏标准化的评估框架，限制了该领域基础模型的发展。

Method: 构建了包含20个数据集的Mars-Bench，涵盖分类、分割和目标检测任务，关注火星地质特征。

Result: 使用在自然图像、地球卫星数据和视觉语言模型上预训练的模型进行基线评估，结果表明火星专用基础模型可能优于通用模型。

Conclusion: Mars-Bench旨在为开发和比较火星科学的机器学习模型建立标准化的基础。

Abstract: Foundation models have enabled rapid progress across many specialized domains
by leveraging large-scale pre-training on unlabeled data, demonstrating strong
generalization to a variety of downstream tasks. While such models have gained
significant attention in fields like Earth Observation, their application to
Mars science remains limited. A key enabler of progress in other domains has
been the availability of standardized benchmarks that support systematic
evaluation. In contrast, Mars science lacks such benchmarks and standardized
evaluation frameworks, which have limited progress toward developing foundation
models for Martian tasks. To address this gap, we introduce Mars-Bench, the
first benchmark designed to systematically evaluate models across a broad range
of Mars-related tasks using both orbital and surface imagery. Mars-Bench
comprises 20 datasets spanning classification, segmentation, and object
detection, focused on key geologic features such as craters, cones, boulders,
and frost. We provide standardized, ready-to-use datasets and baseline
evaluations using models pre-trained on natural images, Earth satellite data,
and state-of-the-art vision-language models. Results from all analyses suggest
that Mars-specific foundation models may offer advantages over general-domain
counterparts, motivating further exploration of domain-adapted pre-training.
Mars-Bench aims to establish a standardized foundation for developing and
comparing machine learning models for Mars science. Our data, models, and code
are available at: https://mars-bench.github.io/.

</details>


### [56] [AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts](https://arxiv.org/abs/2510.24034)
*Yufan Liu,Wanqian Zhang,Huashan Chen,Lin Wang,Xiaojun Jia,Zheng Lin,Weiping Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为APT（AutoPrompT）的黑盒框架，该框架利用大型语言模型（LLM）自动生成良性提示的人类可读对抗性后缀，以评估文本到图像（T2I）模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 当前用于主动评估T2I模型安全漏洞的红队方法通常需要对T2I模型进行白盒访问，并且依赖于低效的每个提示优化，并且不可避免地生成容易被过滤器阻止的语义上无意义的提示。

Method: 该方法首先在对抗性后缀优化和微调LLM之间引入交替优化-微调管道，利用优化的后缀。此外，该方法在优化阶段集成了一种双重规避策略，能够绕过基于困惑度的过滤器和黑名单单词过滤器。

Result: 大量的实验表明，该方法生成的人类可读、抗过滤的对抗性提示具有出色的红队性能，以及卓越的零样本可转移性，从而能够即时适应未见过的提示，并暴露出商业API中的关键漏洞（例如，Leonardo.Ai）。

Conclusion: 该研究提出了一种有效的黑盒框架，用于生成人类可读的对抗性提示，以评估和提高文本到图像模型的安全性。

Abstract: Despite rapid advancements in text-to-image (T2I) models, their safety
mechanisms are vulnerable to adversarial prompts, which maliciously generate
unsafe images. Current red-teaming methods for proactively assessing such
vulnerabilities usually require white-box access to T2I models, and rely on
inefficient per-prompt optimization, as well as inevitably generate
semantically meaningless prompts easily blocked by filters. In this paper, we
propose APT (AutoPrompT), a black-box framework that leverages large language
models (LLMs) to automatically generate human-readable adversarial suffixes for
benign prompts. We first introduce an alternating optimization-finetuning
pipeline between adversarial suffix optimization and fine-tuning the LLM
utilizing the optimized suffix. Furthermore, we integrates a dual-evasion
strategy in optimization phase, enabling the bypass of both perplexity-based
filter and blacklist word filter: (1) we constrain the LLM generating
human-readable prompts through an auxiliary LLM perplexity scoring, which
starkly contrasts with prior token-level gibberish, and (2) we also introduce
banned-token penalties to suppress the explicit generation of banned-tokens in
blacklist. Extensive experiments demonstrate the excellent red-teaming
performance of our human-readable, filter-resistant adversarial prompts, as
well as superior zero-shot transferability which enables instant adaptation to
unseen prompts and exposes critical vulnerabilities even in commercial APIs
(e.g., Leonardo.Ai.).

</details>


### [57] [ResNet: Enabling Deep Convolutional Neural Networks through Residual Learning](https://arxiv.org/abs/2510.24036)
*Xingyu Liu,Kun Ming Goh*

Main category: cs.CV

TL;DR: ResNet overcomes the vanishing gradient problem using skip connections, enabling training of very deep networks.


<details>
  <summary>Details</summary>
Motivation: Training very deep CNNs is challenging due to the vanishing gradient problem.

Method: Use skip connections to allow gradients to flow directly through shortcut connections that bypass intermediate layers.

Result: ResNet-18 achieves 89.9% accuracy on CIFAR-10, compared to 84.1% for a traditional deep CNN.

Conclusion: ResNet enables the training of networks with hundreds of layers, converging faster and training more stably.

Abstract: Convolutional Neural Networks (CNNs) has revolutionized computer vision, but
training very deep networks has been challenging due to the vanishing gradient
problem. This paper explores Residual Networks (ResNet), introduced by He et
al. (2015), which overcomes this limitation by using skip connections. ResNet
enables the training of networks with hundreds of layers by allowing gradients
to flow directly through shortcut connections that bypass intermediate layers.
In our implementation on the CIFAR-10 dataset, ResNet-18 achieves 89.9%
accuracy compared to 84.1% for a traditional deep CNN of similar depth, while
also converging faster and training more stably.

</details>


### [58] [Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for Vision Models](https://arxiv.org/abs/2510.24037)
*Shufan Shen,Junshu Sun,Shuhui Wang,Qingming Huang*

Main category: cs.CV

TL;DR: SNELLA是一种单阶段参数高效微调方法，它通过合并两个低秩可学习矩阵来选择性地更新权重矩阵，并引入非线性核函数来增加合并矩阵的秩，从而更好地适应下游任务。此外，SNELLA还提出了一种自适应双层稀疏分配机制，以鼓励权重根据其重要性得分在层间和层内竞争。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏微调方法存在两个局限性：一是通过梯度信息定位任务相关权重时忽略了微调过程中的参数调整；二是仅更新定位的权重会导致高内存占用。

Method: SNELLA通过将权重矩阵添加到由两个低秩可学习矩阵合并而成的另一个稀疏矩阵来选择性地更新权重矩阵。此外，SNELLA引入非线性核函数来扩展低秩分解，并提出自适应双层稀疏分配机制。

Result: SNELLA在分类、分割和生成任务上取得了SOTA性能，并且内存占用较低。例如，在FGVC基准测试中，SNELLA的Top-1准确率比SPT-LoRA高1.8%（91.9% vs. 90.1%）。

Conclusion: SNELLA是一种有效的参数高效微调方法，它能够以较低的内存占用实现SOTA性能。

Abstract: Parameter-efficient fine-tuning (PEFT) aims to adapt pre-trained vision
models to downstream tasks. Among PEFT paradigms, sparse tuning achieves
remarkable performance by adjusting only the weights most relevant to
downstream tasks, rather than densely tuning the entire weight matrix. Current
methods follow a two-stage paradigm. First, it locates task-relevant weights by
gradient information, which overlooks the parameter adjustments during
fine-tuning and limits the performance. Second, it updates only the located
weights by applying a sparse mask to the gradient of the weight matrix, which
results in high memory usage due to the storage of all weight matrices in the
optimizer. In this paper, we propose a one-stage method named SNELLA to
overcome the above limitations. For memory usage, SNELLA selectively updates
the weight matrix by adding it to another sparse matrix that is merged by two
low-rank learnable matrices. We extend the low-rank decomposition by
introducing nonlinear kernel functions, thereby increasing the rank of the
resulting merged matrix to prevent the interdependency among weight updates,
enabling better adaptation to downstream tasks. For locating task-relevant
weights, we propose an adaptive bi-level sparsity allocation mechanism that
encourages weights to compete across and inside layers based on their
importance scores in an end-to-end manner. Extensive experiments are conducted
on classification, segmentation, and generation tasks using different
pre-trained vision models. The results show that SNELLA achieves SOTA
performance with low memory usage. Notably, SNELLA obtains 1.8% (91.9% v.s.
90.1%) higher Top-1 accuracy on the FGVC benchmark compared to SPT-LoRA.
Compared to previous methods, SNELLA achieves a memory reduction of 31.1%-39.9%
across models with parameter scales from 86M to 632M. Our source codes are
available at https://github.com/ssfgunner/SNELL.

</details>


### [59] [Enhancing CLIP Robustness via Cross-Modality Alignment](https://arxiv.org/abs/2510.24038)
*Xingyu Zhu,Beier Zhu,Shuo Wang,Kesen Zhao,Hanwang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为COLA的跨模态对齐框架，以解决CLIP等视觉-语言模型在对抗扰动下的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的方法主要集中在对抗微调或提示优化上，但忽略了CLIP编码特征中的差距，即文本和图像特征相距甚远。这种错位在对抗扰动下会显著放大，导致分类性能严重下降。

Method: COLA基于最优传输，通过恢复特征空间中的全局图像-文本对齐和局部结构一致性来显式地解决对抗性错位问题。它首先将对抗图像嵌入投影到类文本特征所跨越的子空间上，有效地过滤掉非语义扭曲，同时保留判别信息。然后，它将图像和文本建模为多个增强视图上的离散分布，并通过OT优化它们的对齐，子空间投影无缝集成到成本计算中。

Result: 在14个zero-shot分类基准上的大量评估表明了COLA的有效性，尤其是在ImageNet及其变体上，在PGD对抗攻击下平均提高了6.7%，同时保持了在干净样本上的高精度。

Conclusion: COLA是一种免训练且与现有微调模型兼容的方法，有效提升了视觉-语言模型在对抗环境下的鲁棒性。

Abstract: Vision-language models (VLMs) such as CLIP demonstrate strong generalization
in zero-shot classification but remain highly vulnerable to adversarial
perturbations. Existing methods primarily focus on adversarial fine-tuning or
prompt optimization; they often overlook the gaps in CLIP's encoded features,
which is shown as the text and image features lie far apart from each other.
This misalignment is significantly amplified under adversarial perturbations,
leading to severe degradation in classification performance. To address this
problem, we propose Cross-modality Alignment, dubbed COLA, an optimal
transport-based framework that explicitly addresses adversarial misalignment by
restoring both global image-text alignment and local structural consistency in
the feature space. (1) COLA first projects adversarial image embeddings onto a
subspace spanned by class text features, effectively filtering out non-semantic
distortions while preserving discriminative information. (2) It then models
images and texts as discrete distributions over multiple augmented views and
refines their alignment via OT, with the subspace projection seamlessly
integrated into the cost computation. This design ensures stable cross-modal
alignment even under adversarial conditions. COLA is training-free and
compatible with existing fine-tuned models. Extensive evaluations across 14
zero-shot classification benchmarks demonstrate the effectiveness of COLA,
especially with an average improvement of 6.7% on ImageNet and its variants
under PGD adversarial attacks, while maintaining high accuracy on clean
samples.

</details>


### [60] [Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification](https://arxiv.org/abs/2510.24078)
*William Yang,Xindi Wu,Zhiwei Deng,Esin Tureci,Olga Russakovsky*

Main category: cs.CV

TL;DR: 提出了一种名为BOB的微调策略，用于改进文本到图像模型的合成数据生成，特别是在细粒度分类任务中。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型在生成用于分类的有效合成训练数据方面面临挑战，微调可能导致过拟合和多样性降低。

Method: 该方法首先提取类无关属性（如场景背景和对象姿态），然后在微调过程中显式地以这些属性为条件，并在生成过程中将它们边缘化。

Result: 在多个数据集上，该方法在少样本细粒度分类中取得了最先进的性能。在Aircraft数据集上，BOB比DataDream提高了7.4%。

Conclusion: BOB在24个实验设置中的18个中优于现有技术，其中14个设置的准确率提高了2%以上，证明了其在合成数据生成方面的有效性。

Abstract: Text-to-image (T2I) models are increasingly used for synthetic dataset
generation, but generating effective synthetic training data for classification
remains challenging. Fine-tuning a T2I model with a few real examples can help
improve the quality of synthetic training data; however, it may also cause
overfitting and reduce diversity in the generated samples. We propose a
fine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for
fine-grained classification. Given a small set of real examples, we first
extract class-agnostic attributes such as scene background and object pose. We
then explicitly condition on these attributes during fine-tuning of the T2I
model and marginalize them out during generation. This design mitigates
overfitting, preserves the T2I model's generative prior, reduces estimation
errors, and further minimizes unintended inter-class associations. Extensive
experiments across multiple T2I models, backbones, and datasets show that our
method achieves state-of-the-art performance in low-shot fine-grained
classification when augmented with synthetic data. Concretely, BOB outperforms
DataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning
a CLIP classifier with five real images augmented with 100 synthetic images).
In three of the four benchmarks, fine-tuning downstream models with 5 real
images augmented with BOB achieves better performance than fine-tuning with 10
real images. Collectively, BOB outperforms prior art in 18 of 24 experimental
settings, with 2+% accuracy improvements in 14 of these settings.

</details>


### [61] [OmniText: A Training-Free Generalist for Controllable Text-Image Manipulation](https://arxiv.org/abs/2510.24093)
*Agus Gunawan,Samuel Teodoro,Yun Chen,Soo Ye Kim,Jihyong Oh,Munchurl Kim*

Main category: cs.CV

TL;DR: OmniText 是一个无需训练的通用框架，能够执行各种文本图像处理 (TIM) 任务，解决了现有文本修复方法在文本去除、风格控制和重复字母生成方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的文本合成在图像中插入和编辑文本方面表现出色，但无法去除文本、缺乏对渲染文本风格的控制以及容易生成重复字母，限制了其在更广泛的文本图像处理 (TIM) 任务中的应用。

Method: OmniText 采用自注意力反转实现文本去除，通过重新分配交叉注意力来减少文本幻觉，并引入新的潜在优化框架中的损失函数，包括交叉注意力内容损失和自注意力风格损失，以提高文本渲染准确性和促进风格定制。

Result: OmniText 在多项任务和指标上实现了最先进的性能，并且与专业方法具有可比性。

Conclusion: OmniText 是第一个能够执行各种 TIM 任务的通用方法。

Abstract: Recent advancements in diffusion-based text synthesis have demonstrated
significant performance in inserting and editing text within images via
inpainting. However, despite the potential of text inpainting methods, three
key limitations hinder their applicability to broader Text Image Manipulation
(TIM) tasks: (i) the inability to remove text, (ii) the lack of control over
the style of rendered text, and (iii) a tendency to generate duplicated
letters. To address these challenges, we propose OmniText, a training-free
generalist capable of performing a wide range of TIM tasks. Specifically, we
investigate two key properties of cross- and self-attention mechanisms to
enable text removal and to provide control over both text styles and content.
Our findings reveal that text removal can be achieved by applying
self-attention inversion, which mitigates the model's tendency to focus on
surrounding text, thus reducing text hallucinations. Additionally, we
redistribute cross-attention, as increasing the probability of certain text
tokens reduces text hallucination. For controllable inpainting, we introduce
novel loss functions in a latent optimization framework: a cross-attention
content loss to improve text rendering accuracy and a self-attention style loss
to facilitate style customization. Furthermore, we present OmniText-Bench, a
benchmark dataset for evaluating diverse TIM tasks. It includes input images,
target text with masks, and style references, covering diverse applications
such as text removal, rescaling, repositioning, and insertion and editing with
various styles. Our OmniText framework is the first generalist method capable
of performing diverse TIM tasks. It achieves state-of-the-art performance
across multiple tasks and metrics compared to other text inpainting methods and
is comparable with specialist methods.

</details>


### [62] [Enhancing Pre-trained Representation Classifiability can Boost its Interpretability](https://arxiv.org/abs/2510.24105)
*Shufan Shen,Zhaobo Qi,Junshu Sun,Qingming Huang,Qi Tian,Shuhui Wang*

Main category: cs.CV

TL;DR: 本文研究了预训练视觉模型表征的可解释性与可分类性之间的关系，发现它们是正相关的。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉模型应用广泛，对表征的可解释性提出了新要求，但尚不清楚预训练表征是否能同时实现高可解释性和可分类性。

Method: 本文通过表征与可解释语义比例的相关性来量化表征的可解释性，并提出了一种名为 Inherent Interpretability Score (IIS) 的指标来评估信息损失，衡量可解释语义的比例，从而量化表征的可解释性。

Result: 研究发现可解释性与可分类性呈正相关，即可分类性更高的表征提供更多可在解释中捕获的可解释语义。

Conclusion: 该发现表明，通过最大化可解释性进行微调可以进一步提高表征的可分类性，并且在提高表征的可分类性的同时，可以基于其解释获得预测，从而减少准确性下降。

Abstract: The visual representation of a pre-trained model prioritizes the
classifiability on downstream tasks, while the widespread applications for
pre-trained visual models have posed new requirements for representation
interpretability. However, it remains unclear whether the pre-trained
representations can achieve high interpretability and classifiability
simultaneously. To answer this question, we quantify the representation
interpretability by leveraging its correlation with the ratio of interpretable
semantics within the representations. Given the pre-trained representations,
only the interpretable semantics can be captured by interpretations, whereas
the uninterpretable part leads to information loss. Based on this fact, we
propose the Inherent Interpretability Score (IIS) that evaluates the
information loss, measures the ratio of interpretable semantics, and quantifies
the representation interpretability. In the evaluation of the representation
interpretability with different classifiability, we surprisingly discover that
the interpretability and classifiability are positively correlated, i.e.,
representations with higher classifiability provide more interpretable
semantics that can be captured in the interpretations. This observation further
supports two benefits to the pre-trained representations. First, the
classifiability of representations can be further improved by fine-tuning with
interpretability maximization. Second, with the classifiability improvement for
the representations, we obtain predictions based on their interpretations with
less accuracy degradation. The discovered positive correlation and
corresponding applications show that practitioners can unify the improvements
in interpretability and classifiability for pre-trained vision models. Codes
are available at https://github.com/ssfgunner/IIS.

</details>


### [63] [UHKD: A Unified Framework for Heterogeneous Knowledge Distillation via Frequency-Domain Representations](https://arxiv.org/abs/2510.24116)
*Fengming Yu,Haiwei Pan,Kejia Zhang,Jian Guan,Haiying Jiang*

Main category: cs.CV

TL;DR: 知识蒸馏 (KD) 是一种有效的模型压缩技术，可将知识从高性能教师转移到轻量级学生，从而在保持准确性的同时降低成本。但是，架构多样性会引入语义差异，从而阻碍中间表示的使用。本文提出了统一异构知识蒸馏 (UHKD) 框架，该框架利用频域中的中间特征进行跨架构传输。使用傅里叶变换来捕获全局特征信息，从而缓解异构师生对之间的表示差异。特征变换模块 (FTM) 生成教师特征的紧凑频域表示，而可学习的特征对齐模块 (FAM) 投射学生特征并通过多层匹配对齐它们。在 CIFAR-100 和 ImageNet-1K 上的实验表明，UHKD 比最新方法提高了 5.59% 和 0.83%，突显了 UHKD 作为统一异构表示和有效利用视觉知识的有效方法。


<details>
  <summary>Details</summary>
Motivation: 现有的知识蒸馏 (KD) 方法大多是为同构模型设计的，在异构场景中会降低性能，尤其是在涉及中间特征时。以往的研究主要集中在 logits 空间，对中间层中的语义信息利用有限。

Method: 提出了统一异构知识蒸馏 (UHKD) 框架，该框架利用频域中的中间特征进行跨架构传输。傅里叶变换用于捕获全局特征信息，缓解异构师生对之间的表示差异。特征变换模块 (FTM) 生成教师特征的紧凑频域表示，而可学习的特征对齐模块 (FAM) 投射学生特征并通过多层匹配对齐它们。训练由一个联合目标引导，该目标将中间特征的均方误差与 logits 的 Kullback-Leibler 散度相结合。

Result: 在 CIFAR-100 和 ImageNet-1K 上的实验表明，UHKD 比最新方法提高了 5.59% 和 0.83%。

Conclusion: UHKD 是一种统一异构表示和有效利用视觉知识的有效方法。

Abstract: Knowledge distillation (KD) is an effective model compression technique that
transfers knowledge from a high-performance teacher to a lightweight student,
reducing cost while maintaining accuracy. In visual applications, where
large-scale image models are widely used, KD enables efficient deployment.
However, architectural diversity introduces semantic discrepancies that hinder
the use of intermediate representations. Most existing KD methods are designed
for homogeneous models and degrade in heterogeneous scenarios, especially when
intermediate features are involved. Prior studies mainly focus on the logits
space, making limited use of the semantic information in intermediate layers.
To address this limitation, Unified Heterogeneous Knowledge Distillation (UHKD)
is proposed as a framework that leverages intermediate features in the
frequency domain for cross-architecture transfer. Fourier transform is applied
to capture global feature information, alleviating representational
discrepancies between heterogeneous teacher-student pairs. A Feature
Transformation Module (FTM) produces compact frequency-domain representations
of teacher features, while a learnable Feature Alignment Module (FAM) projects
student features and aligns them via multi-level matching. Training is guided
by a joint objective combining mean squared error on intermediate features with
Kullback-Leibler divergence on logits. Experiments on CIFAR-100 and ImageNet-1K
demonstrate gains of 5.59% and 0.83% over the latest method, highlighting UHKD
as an effective approach for unifying heterogeneous representations and
enabling efficient utilization of visual knowledge

</details>


### [64] [DogMo: A Large-Scale Multi-View RGB-D Dataset for 4D Canine Motion Recovery](https://arxiv.org/abs/2510.24117)
*Zan Wang,Siyu Chen,Luya Mo,Xinfeng Gao,Yuxin Shen,Lebin Ding,Wei Liang*

Main category: cs.CV

TL;DR: DogMo是一个大型多视角RGB-D视频数据集，用于从图像中恢复犬类运动。


<details>
  <summary>Details</summary>
Motivation: 现有的狗运动数据集缺乏多视角和真实3D数据，以及有限的规模和多样性。

Method: 提出了一个三阶段、特定实例的优化流程，将SMAL模型拟合到运动序列。

Result: 建立了四个运动恢复基准设置，支持单目和多视角、RGB和RGB-D输入的系统评估。

Conclusion: 该数据集和方法为推进狗运动恢复研究奠定了基础，并在计算机视觉、计算机图形学和动物行为建模的交叉领域开辟了新的方向。

Abstract: We present DogMo, a large-scale multi-view RGB-D video dataset capturing
diverse canine movements for the task of motion recovery from images. DogMo
comprises 1.2k motion sequences collected from 10 unique dogs, offering rich
variation in both motion and breed. It addresses key limitations of existing
dog motion datasets, including the lack of multi-view and real 3D data, as well
as limited scale and diversity. Leveraging DogMo, we establish four motion
recovery benchmark settings that support systematic evaluation across monocular
and multi-view, RGB and RGB-D inputs. To facilitate accurate motion recovery,
we further introduce a three-stage, instance-specific optimization pipeline
that fits the SMAL model to the motion sequences. Our method progressively
refines body shape and pose through coarse alignment, dense correspondence
supervision, and temporal regularization. Our dataset and method provide a
principled foundation for advancing research in dog motion recovery and open up
new directions at the intersection of computer vision, computer graphics, and
animal behavior modeling.

</details>


### [65] [ETC: training-free diffusion models acceleration with Error-aware Trend Consistency](https://arxiv.org/abs/2510.24129)
*Jiajian Xie,Hubery Yin,Chen Li,Zhou Zhao,Shengyu Zhang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种名为Error-aware Trend Consistency (ETC)的框架，旨在加速扩散模型的采样过程，同时保持生成结果的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的无训练加速方法忽略了去噪趋势，并且缺乏对模型特定容错的误差控制，导致轨迹偏差和生成结果不一致。

Method: 该框架引入了一个一致的趋势预测器，利用扩散轨迹的平滑连续性，将历史去噪模式投影到稳定的未来方向，并逐步将其分配到多个近似步骤中。此外，还提出了一种模型特定的误差容错搜索机制，通过识别从不稳定的语义规划到稳定的质量改进的过渡点来导出校正阈值。

Result: 实验表明，ETC比FLUX实现了2.65倍的加速，而一致性的降级可以忽略不计（-0.074 SSIM score）。

Conclusion: ETC框架能够有效地加速扩散模型的采样过程，同时保持生成结果的一致性。

Abstract: Diffusion models have achieved remarkable generative quality but remain
bottlenecked by costly iterative sampling. Recent training-free methods
accelerate diffusion process by reusing model outputs. However, these methods
ignore denoising trends and lack error control for model-specific tolerance,
leading to trajectory deviations under multi-step reuse and exacerbating
inconsistencies in the generated results. To address these issues, we introduce
Error-aware Trend Consistency (ETC), a framework that (1) introduces a
consistent trend predictor that leverages the smooth continuity of diffusion
trajectories, projecting historical denoising patterns into stable future
directions and progressively distributing them across multiple approximation
steps to achieve acceleration without deviating; (2) proposes a model-specific
error tolerance search mechanism that derives corrective thresholds by
identifying transition points from volatile semantic planning to stable quality
refinement. Experiments show that ETC achieves a 2.65x acceleration over FLUX
with negligible (-0.074 SSIM score) degradation of consistency.

</details>


### [66] [Compositional Image Synthesis with Inference-Time Scaling](https://arxiv.org/abs/2510.24133)
*Minsuk Ji,Sanghyeok Lee,Namhyuk Ahn*

Main category: cs.CV

TL;DR: 该论文提出了一个无需训练的框架，通过结合对象中心方法和自我完善来提高布局的真实性，同时保持美学质量。


<details>
  <summary>Details</summary>
Motivation: 现代文本到图像模型在组合性方面存在不足，通常无法渲染准确的对象数量、属性和空间关系。

Method: 该框架利用大型语言模型 (LLM) 从输入提示合成显式布局，并将这些布局注入到图像生成过程中。一个以对象为中心的视觉语言模型 (VLM) 判断重新排列多个候选对象，以迭代地选择与提示最一致的结果。

Result: 通过统一显式布局 grounding 与基于自我完善的推理时缩放，该框架与最近的文本到图像模型相比，实现了更强的场景与提示对齐。

Conclusion: 该框架通过结合对象中心方法和自我完善，有效提高了文本到图像模型的组合性，使其能够更准确地根据文本提示生成图像。

Abstract: Despite their impressive realism, modern text-to-image models still struggle
with compositionality, often failing to render accurate object counts,
attributes, and spatial relations. To address this challenge, we present a
training-free framework that combines an object-centric approach with
self-refinement to improve layout faithfulness while preserving aesthetic
quality. Specifically, we leverage large language models (LLMs) to synthesize
explicit layouts from input prompts, and we inject these layouts into the image
generation process, where a object-centric vision-language model (VLM) judge
reranks multiple candidates to select the most prompt-aligned outcome
iteratively. By unifying explicit layout-grounding with self-refine-based
inference-time scaling, our framework achieves stronger scene alignment with
prompts compared to recent text-to-image models. The code are available at
https://github.com/gcl-inha/ReFocus.

</details>


### [67] [VC4VG: Optimizing Video Captions for Text-to-Video Generation](https://arxiv.org/abs/2510.24134)
*Yang Du,Zhuoran Lin,Kaiqiang Song,Biao Wang,Zhicheng Zheng,Tiezheng Ge,Bo Zheng,Qin Jin*

Main category: cs.CV

TL;DR: 本文提出了一种视频字幕优化框架VC4VG，专门为文本到视频（T2V）模型的需求量身定制。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏专门针对T2V训练优化视频字幕的策略。

Method: 通过从T2V的角度分析字幕内容，将视频重建所需的基本元素分解为多个维度，并提出了一种基于原则的字幕设计方法。构建了VC4VG-Bench，这是一个新的基准，具有细粒度、多维度和必要性分级的指标，与T2V的特定要求相一致。

Result: 大量的T2V微调实验表明，字幕质量的提高与视频生成性能之间存在很强的相关性，验证了该方法的有效性。

Conclusion: 验证了所提出方法的有效性，并发布了所有基准工具和代码以支持进一步的研究。

Abstract: Recent advances in text-to-video (T2V) generation highlight the critical role
of high-quality video-text pairs in training models capable of producing
coherent and instruction-aligned videos. However, strategies for optimizing
video captions specifically for T2V training remain underexplored. In this
paper, we introduce VC4VG (Video Captioning for Video Generation), a
comprehensive caption optimization framework tailored to the needs of T2V
models.We begin by analyzing caption content from a T2V perspective,
decomposing the essential elements required for video reconstruction into
multiple dimensions, and proposing a principled caption design methodology. To
support evaluation, we construct VC4VG-Bench, a new benchmark featuring
fine-grained, multi-dimensional, and necessity-graded metrics aligned with
T2V-specific requirements.Extensive T2V fine-tuning experiments demonstrate a
strong correlation between improved caption quality and video generation
performance, validating the effectiveness of our approach. We release all
benchmark tools and code at https://github.com/qyr0403/VC4VG to support further
research.

</details>


### [68] [Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning](https://arxiv.org/abs/2510.24152)
*Aodi Wu,Xubo Luo*

Main category: cs.CV

TL;DR: 本技术报告介绍了我们在 IROS 2025 RoboSense 挑战赛中的解决方案，该方案评估了视觉语言模型 (VLM) 在自动驾驶场景理解方面的能力，涵盖感知、预测、规划和腐败检测任务。


<details>
  <summary>Details</summary>
Motivation: 旨在提升视觉语言模型在自动驾驶场景理解方面的性能，特别是在感知、预测、规划和腐败检测等安全关键任务上。

Method: 提出了一个基于四个核心组件的系统框架：混合提示路由器、任务特定提示、视觉组装模块和模型推理参数配置。

Result: 在 Qwen2.5-VL-72B 上实施的方法在 Phase-1（干净数据）上实现了 70.87% 的平均准确率，在 Phase-2（损坏数据）上实现了 72.85% 的平均准确率。

Conclusion: 结构化提示和空间定位能够显著提高 VLM 在安全关键型自动驾驶任务中的性能。

Abstract: This technical report presents our solution for the RoboSense Challenge at
IROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving
scene understanding across perception, prediction, planning, and corruption
detection tasks. We propose a systematic framework built on four core
components. First, a Mixture-of-Prompts router classifies questions and
dispatches them to task-specific expert prompts, eliminating interference
across diverse question types. Second, task-specific prompts embed explicit
coordinate systems, spatial reasoning rules, role-playing,
Chain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to
each task. Third, a visual assembly module composes multi-view images with
object crops, magenta markers, and adaptive historical frames based on question
requirements. Fourth, we configure model inference parameters (temperature,
top-p, message roles) per task to optimize output quality. Implemented on
Qwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean
data) and 72.85% on Phase-2 (corrupted data), demonstrating that structured
prompting and spatial grounding substantially enhance VLM performance on
safety-critical autonomous driving tasks. Code and prompt are available at
https://github.com/wuaodi/UCAS-CSU-phase2.

</details>


### [69] [Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2](https://arxiv.org/abs/2510.24195)
*Ziqi Zhou,Yifan Hu,Yufei Song,Zijing Li,Shengshan Hu,Leo Yu Zhang,Dezhong Yao,Long Zheng,Hai Jin*

Main category: cs.CV

TL;DR: 本文研究了视频分割基础模型SAM2的鲁棒性，发现现有针对SAM的攻击不能直接转移到SAM2上。提出了UAP-SAM2，一种基于双重语义偏差的跨 prompt 通用对抗攻击，通过扭曲当前帧内的语义并破坏连续帧之间的语义一致性来优化 UAP。


<details>
  <summary>Details</summary>
Motivation: 研究SAM2在面对对抗性攻击时的脆弱性，并分析现有SAM攻击方法在SAM2上的性能差距。强调了SAM2的架构差异带来的两个关键挑战：来自prompt的方向引导和连续帧之间的语义纠缠。

Method: 提出了UAP-SAM2，一种跨 prompt 通用对抗攻击，通过目标扫描策略减少 prompt 依赖，并设计了一个双重语义偏差框架来优化 UAP，该框架扭曲当前帧内的语义并破坏连续帧之间的语义一致性。

Result: 在六个数据集上的大量实验表明，所提出的方法对SAM2有效，并且显著优于现有最先进的攻击方法。

Conclusion: 本文成功地提出了针对SAM2的通用对抗攻击方法UAP-SAM2，并在实验中验证了其有效性，表明SAM2在对抗性攻击下存在脆弱性。

Abstract: Recent studies reveal the vulnerability of the image segmentation foundation
model SAM to adversarial examples. Its successor, SAM2, has attracted
significant attention due to its strong generalization capability in video
segmentation. However, its robustness remains unexplored, and it is unclear
whether existing attacks on SAM can be directly transferred to SAM2. In this
paper, we first analyze the performance gap of existing attacks between SAM and
SAM2 and highlight two key challenges arising from their architectural
differences: directional guidance from the prompt and semantic entanglement
across consecutive frames. To address these issues, we propose UAP-SAM2, the
first cross-prompt universal adversarial attack against SAM2 driven by dual
semantic deviation. For cross-prompt transferability, we begin by designing a
target-scanning strategy that divides each frame into k regions, each randomly
assigned a prompt, to reduce prompt dependency during optimization. For
effectiveness, we design a dual semantic deviation framework that optimizes a
UAP by distorting the semantics within the current frame and disrupting the
semantic consistency across consecutive frames. Extensive experiments on six
datasets across two segmentation tasks demonstrate the effectiveness of the
proposed method for SAM2. The comparative results show that UAP-SAM2
significantly outperforms state-of-the-art (SOTA) attacks by a large margin.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [70] [Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents](https://arxiv.org/abs/2510.23691)
*Zihao Wang,Xujing Li,Yining Ye,Junjie Fang,Haoming Wang,Longxiang Liu,Shihao Liang,Junting Lu,Zhiyong Wu,Jiazhan Feng,Wanjun Zhong,Zili Li,Yu Wang,Yu Miao,Bo Zhou,Yuanfan Li,Hao Wang,Zhongkai Zhao,Faming Wu,Zhengxuan Jiang,Weihao Tan,Heyuan Yao,Shi Yan,Xiangyang Li,Yitao Liang,Yujia Qin,Guang Shi*

Main category: cs.AI

TL;DR: Game-TARS 是一个通用游戏智能体，它使用统一的、可扩展的动作空间进行训练，该动作空间与人类对齐的原始键盘鼠标输入相关联。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够在不同领域（包括操作系统、网络和模拟游戏）中进行大规模持续预训练的通用游戏智能体。

Method: 使用超过 500B tokens 的多样化轨迹和多模态数据进行预训练，采用衰减的持续损失来减少因果混淆，并采用有效的稀疏思考策略来平衡推理深度和推理成本。

Result: 在开放世界 Minecraft 任务中的成功率是之前最佳模型的 2 倍，在未见过的网络 3D 游戏中接近人类的通用性，并且在 FPS 基准测试中优于 GPT-5、Gemini-2.5-Pro 和 Claude-4-Sonnet。

Conclusion: 简单、可扩展的动作表示与大规模预训练相结合，为具有广泛计算机使用能力的通用智能体提供了一条有希望的道路。

Abstract: We present Game-TARS, a generalist game agent trained with a unified,
scalable action space anchored to human-aligned native keyboard-mouse inputs.
Unlike API- or GUI-based approaches, this paradigm enables large-scale
continual pre-training across heterogeneous domains, including OS, web, and
simulation games. Game-TARS is pre-trained on over 500B tokens with diverse
trajectories and multimodal data. Key techniques include a decaying continual
loss to reduce causal confusion and an efficient Sparse-Thinking strategy that
balances reasoning depth and inference cost. Experiments show that Game-TARS
achieves about 2 times the success rate over the previous sota model on
open-world Minecraft tasks, is close to the generality of fresh humans in
unseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet
in FPS benchmarks. Scaling results on training-time and test-time confirm that
the unified action space sustains improvements when scaled to cross-game and
multimodal data. Our results demonstrate that simple, scalable action
representations combined with large-scale pre-training provide a promising path
toward generalist agents with broad computer-use abilities.

</details>


### [71] [AI and the Decentering of Disciplinary Creativity](https://arxiv.org/abs/2510.23734)
*Eamon Duede*

Main category: cs.AI

TL;DR: 探讨人工智能在科学问题解决中的角色及其对学科创造力的影响。


<details>
  <summary>Details</summary>
Motivation: 研究人工智能如何影响学科创造力，区分创造性方法和创造性产品，并引入学科创造力的概念。

Method: 通过两个数学案例进行分析，展示计算如何扩展学科创造力。

Result: 计算可以扩展学科创造力，但某些人工智能方法可能会取代它。

Conclusion: 人工智能对学科创造力的取代可能会改变甚至降低科学探索的价值。

Abstract: This paper examines the role of artificial intelligence in scientific
problem-solving, with a focus on its implications for disciplinary creativity.
Drawing on recent work in the philosophy of creativity, I distinguish between
creative approaches and creative products, and introduce the concept of
disciplinary creativity -the creative application of discipline-specific
expertise to a valued problem within that field. Through two cases in
mathematics, I show that while computation can extend disciplinary creativity,
certain approaches involving AI can serve to displace it. This displacement has
the potential to alter (and, perhaps, diminish) the value of scientific
pursuit.

</details>


### [72] [Multi-Environment POMDPs: Discrete Model Uncertainty Under Partial Observability](https://arxiv.org/abs/2510.23744)
*Eline M. Bovy,Caleb Probine,Marnix Suilen,Ufuk Topcu,Nils Jansen*

Main category: cs.AI

TL;DR: ME-POMDPs are POMDPs with discrete model uncertainty, where the goal is to find a policy that maximizes the worst-case reward across all POMDPs.


<details>
  <summary>Details</summary>
Motivation: To address the problem of model uncertainty in POMDPs, where multiple domain experts may disagree on how to model a problem.

Method: The paper generalizes ME-POMDPs to adversarial-belief POMDPs (AB-POMDPs) and reduces any arbitrary ME-POMDP to a ME-POMDP that only varies in its transition and reward functions or only in its observation and reward functions. The paper then devises exact and approximate algorithms to compute robust policies for AB-POMDPs, and thus ME-POMDPs.

Result: The paper demonstrates that it can compute policies for standard POMDP benchmarks extended to the multi-environment setting.

Conclusion: The paper provides a method for finding robust policies in ME-POMDPs by generalizing them to AB-POMDPs and developing exact and approximate algorithms.

Abstract: Multi-environment POMDPs (ME-POMDPs) extend standard POMDPs with discrete
model uncertainty. ME-POMDPs represent a finite set of POMDPs that share the
same state, action, and observation spaces, but may arbitrarily vary in their
transition, observation, and reward models. Such models arise, for instance,
when multiple domain experts disagree on how to model a problem. The goal is to
find a single policy that is robust against any choice of POMDP within the set,
i.e., a policy that maximizes the worst-case reward across all POMDPs. We
generalize and expand on existing work in the following way. First, we show
that ME-POMDPs can be generalized to POMDPs with sets of initial beliefs, which
we call adversarial-belief POMDPs (AB-POMDPs). Second, we show that any
arbitrary ME-POMDP can be reduced to a ME-POMDP that only varies in its
transition and reward functions or only in its observation and reward
functions, while preserving (optimal) policies. We then devise exact and
approximate (point-based) algorithms to compute robust policies for AB-POMDPs,
and thus ME-POMDPs. We demonstrate that we can compute policies for standard
POMDP benchmarks extended to the multi-environment setting.

</details>


### [73] [Test-Time Tuned Language Models Enable End-to-end De Novo Molecular Structure Generation from MS/MS Spectra](https://arxiv.org/abs/2510.23746)
*Laura Mismetti,Marvin Alberts,Andreas Krause,Mara Graziani*

Main category: cs.AI

TL;DR: 提出了一种新框架，通过利用测试时调整，增强预训练 Transformer 模型的学习能力，以解决串联质谱分析中从头分子结构生成的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于先前观察到的分子的数据库匹配，或需要中间片段或指纹预测的多步骤流程，这使得寻找正确的分子非常具有挑战性，特别是对于参考数据库中不存在的化合物。

Method: 该框架通过利用测试时调整，增强预训练的 Transformer 模型，实现直接从串联质谱和分子式进行端到端的分子结构生成，绕过手动注释和中间步骤。

Result: 在两个流行的基准测试 NPLIB1 和 MassSpecGym 上，超越了事实上的最先进方法 DiffMS，分别提高了 100% 和 20%。在实验光谱上进行测试时调整允许模型动态适应新的光谱，并且在 MassSpecGym 上，相对于传统微调的相对性能增益为 62%。

Conclusion: 即使预测偏离了真实情况，生成的分子候选物仍然在结构上是准确的，为人工解释和更可靠的识别提供了有价值的指导。

Abstract: Tandem Mass Spectrometry enables the identification of unknown compounds in
crucial fields such as metabolomics, natural product discovery and
environmental analysis. However, current methods rely on database matching from
previously observed molecules, or on multi-step pipelines that require
intermediate fragment or fingerprint prediction. This makes finding the correct
molecule highly challenging, particularly for compounds absent from reference
databases. We introduce a framework that, by leveraging test-time tuning,
enhances the learning of a pre-trained transformer model to address this gap,
enabling end-to-end de novo molecular structure generation directly from the
tandem mass spectra and molecular formulae, bypassing manual annotations and
intermediate steps. We surpass the de-facto state-of-the-art approach DiffMS on
two popular benchmarks NPLIB1 and MassSpecGym by 100% and 20%, respectively.
Test-time tuning on experimental spectra allows the model to dynamically adapt
to novel spectra, and the relative performance gain over conventional
fine-tuning is of 62% on MassSpecGym. When predictions deviate from the ground
truth, the generated molecular candidates remain structurally accurate,
providing valuable guidance for human interpretation and more reliable
identification.

</details>


### [74] [Evaluating In Silico Creativity: An Expert Review of AI Chess Compositions](https://arxiv.org/abs/2510.23772)
*Vivek Veeriah,Federico Barbero,Marcus Chiam,Xidong Feng,Michael Dennis,Ryan Pachauri,Thomas Tumiel,Johan Obando-Ceron,Jiaxin Shi,Shaobo Hou,Satinder Singh,Nenad Tomašev,Tom Zahavy*

Main category: cs.AI

TL;DR: 本研究探讨了生成式人工智能在创作国际象棋谜题方面的能力，并设计了一个能够生成具有审美吸引力、新颖、违反直觉和独特解法的谜题的AI系统。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式人工智能是否能够产生创造性和新颖的输出。

Method: 设计了一个AI系统来生成国际象棋谜题，并由国际象棋专家评估。

Result: 国际象棋专家们被要求选择他们最喜欢的谜题，并解释其吸引力。

Conclusion: 通过国际象棋谜题的生成和评估，研究生成式人工智能的创造力。

Abstract: The rapid advancement of Generative AI has raised significant questions
regarding its ability to produce creative and novel outputs. Our recent work
investigates this question within the domain of chess puzzles and presents an
AI system designed to generate puzzles characterized by aesthetic appeal,
novelty, counter-intuitive and unique solutions. We briefly discuss our method
below and refer the reader to the technical paper for more details. To assess
our system's creativity, we presented a curated booklet of AI-generated puzzles
to three world-renowned experts: International Master for chess compositions
Amatzia Avni, Grandmaster Jonathan Levitt, and Grandmaster Matthew Sadler. All
three are noted authors on chess aesthetics and the evolving role of computers
in the game. They were asked to select their favorites and explain what made
them appealing, considering qualities such as their creativity, level of
challenge, or aesthetic design.

</details>


### [75] [Why Foundation Models in Pathology Are Failing](https://arxiv.org/abs/2510.23807)
*Hamid R. Tizhoosh*

Main category: cs.AI

TL;DR: 病理学中的通用基础模型未能达到在癌症诊断等方面的预期突破，因为它们与人类组织的复杂性存在根本的不匹配。


<details>
  <summary>Details</summary>
Motivation: 评估表明，通用基础模型在病理学中存在诊断准确率低、鲁棒性差等弱点。

Method: 分析了通用基础模型在病理学应用中的不足，并探讨了其根本原因。

Result: 识别了七个相互关联的原因，包括生物复杂性、无效的自监督和过度泛化等。

Conclusion: 当前的病理学基础模型在概念上与组织形态的本质不符，需要重新思考这一范例。

Abstract: In non-medical domains, foundation models (FMs) have revolutionized computer
vision and language processing through large-scale self-supervised and
multimodal learning. Consequently, their rapid adoption in computational
pathology was expected to deliver comparable breakthroughs in cancer diagnosis,
prognostication, and multimodal retrieval. However, recent systematic
evaluations reveal fundamental weaknesses: low diagnostic accuracy, poor
robustness, geometric instability, heavy computational demands, and concerning
safety vulnerabilities. This short paper examines these shortcomings and argues
that they stem from deeper conceptual mismatches between the assumptions
underlying generic foundation modeling in mainstream AI and the intrinsic
complexity of human tissue. Seven interrelated causes are identified:
biological complexity, ineffective self-supervision, overgeneralization,
excessive architectural complexity, lack of domain-specific innovation,
insufficient data, and a fundamental design flaw related to tissue patch size.
These findings suggest that current pathology foundation models remain
conceptually misaligned with the nature of tissue morphology and call for a
fundamental rethinking of the paradigm itself.

</details>


### [76] [ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents](https://arxiv.org/abs/2510.23822)
*Zhenyu Zhang,Tianyi Chen,Weiran Xu,Alex Pentland,Jiaxin Pei*

Main category: cs.AI

TL;DR: ReCAP: A hierarchical framework with shared context for reasoning and planning in LLMs, improving subgoal alignment and success rates on long-horizon reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Long-horizon tasks requiring multi-step reasoning and dynamic re-planning remain challenging for large language models (LLMs). Sequential prompting methods are prone to context drift, loss of goal information, and recurrent failure cycles, while hierarchical prompting methods often weaken cross-level continuity or incur substantial runtime overhead.

Method: ReCAP combines three key mechanisms: (i) plan-ahead decomposition, in which the model generates a full subtask list, executes the first item, and refines the remainder; (ii) structured re-injection of parent plans, maintaining consistent multi-level context during recursive return; and (iii) memory-efficient execution, bounding the active prompt so costs scale linearly with task depth.

Result: ReCAP substantially improves subgoal alignment and success rates on various long-horizon reasoning benchmarks, achieving a 32% gain on synchronous Robotouille and a 29% improvement on asynchronous Robotouille under the strict pass@1 protocol.

Conclusion: ReCAP aligns high-level goals with low-level actions, reduces redundant prompting, and preserve coherent context updates across recursion.

Abstract: Long-horizon tasks requiring multi-step reasoning and dynamic re-planning
remain challenging for large language models (LLMs). Sequential prompting
methods are prone to context drift, loss of goal information, and recurrent
failure cycles, while hierarchical prompting methods often weaken cross-level
continuity or incur substantial runtime overhead. We introduce ReCAP (Recursive
Context-Aware Reasoning and Planning), a hierarchical framework with shared
context for reasoning and planning in LLMs. ReCAP combines three key
mechanisms: (i) plan-ahead decomposition, in which the model generates a full
subtask list, executes the first item, and refines the remainder; (ii)
structured re-injection of parent plans, maintaining consistent multi-level
context during recursive return; and (iii) memory-efficient execution, bounding
the active prompt so costs scale linearly with task depth. Together these
mechanisms align high-level goals with low-level actions, reduce redundant
prompting, and preserve coherent context updates across recursion. Experiments
demonstrate that ReCAP substantially improves subgoal alignment and success
rates on various long-horizon reasoning benchmarks, achieving a 32% gain on
synchronous Robotouille and a 29% improvement on asynchronous Robotouille under
the strict pass@1 protocol.

</details>


### [77] [Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models](https://arxiv.org/abs/2510.23824)
*Murad Ismayilov,Edwin Meriaux,Shuo Wen,Gregory Dudek*

Main category: cs.AI

TL;DR: 本文研究了多智能体路径规划中的去中心化目标分配问题，利用LLM在网格世界环境中实现了接近最优的makespan。


<details>
  <summary>Details</summary>
Motivation: 在去中心化条件下，协调共享环境中的多个自主智能体是一个长期存在的挑战。本文解决了多智能体路径规划的去中心化目标分配问题。

Method: 智能体基于环境的结构化表示（包括网格可视化和场景数据）独立生成排序后的目标偏好。然后，智能体交换其目标排名，并通过固定的冲突解决规则来确定分配。

Result: LLM智能体在充分观察的网格世界环境中，在经过精心设计的提示和相关的定量信息后，可以实现接近最优的makespan，并且始终优于传统的启发式方法。

Conclusion: 研究结果强调了语言模型在多智能体路径规划中去中心化目标分配的潜力，并突出了信息结构在此类系统中的重要性。

Abstract: Coordinating multiple autonomous agents in shared environments under
decentralized conditions is a long-standing challenge in robotics and
artificial intelligence. This work addresses the problem of decentralized goal
assignment for multi-agent path planning, where agents independently generate
ranked preferences over goals based on structured representations of the
environment, including grid visualizations and scenario data. After this
reasoning phase, agents exchange their goal rankings, and assignments are
determined by a fixed, deterministic conflict-resolution rule (e.g., agent
index ordering), without negotiation or iterative coordination. We
systematically compare greedy heuristics, optimal assignment, and large
language model (LLM)-based agents in fully observable grid-world settings. Our
results show that LLM-based agents, when provided with well-designed prompts
and relevant quantitative information, can achieve near-optimal makespans and
consistently outperform traditional heuristics. These findings underscore the
potential of language models for decentralized goal assignment in multi-agent
path planning and highlight the importance of information structure in such
systems.

</details>


### [78] [From Benchmarks to Business Impact: Deploying IBM Generalist Agent in Enterprise Production](https://arxiv.org/abs/2510.23856)
*Segev Shlomov,Alon Oved,Sami Marreed,Ido Levy,Offer Akrabi,Avi Yaeli,Łukasz Strąk,Elizabeth Koumpan,Yinon Goldshtein,Eilam Shapira,Nir Mashkif,Asaf Adi*

Main category: cs.AI

TL;DR: 本文介绍了IBM开发的通用智能体CUGA，它在企业规模上进行了初步的试点，并开源。


<details>
  <summary>Details</summary>
Motivation: 企业在部署通用智能体时面临框架分散、开发缓慢和缺乏标准化评估实践的挑战。通用智能体在学术基准测试中表现出色，但在生产环境中的应用证据有限。

Method: CUGA采用分层规划器-执行器架构，并在AppWorld和WebArena上实现了最先进的性能。此外，还引入了BPO-TA基准测试，包含13个分析端点，共26个任务。

Result: CUGA在初步评估中接近专用智能体的准确性，并有潜力减少开发时间和成本。

Conclusion: 本文展示了通用智能体在企业规模上运行的早期证据，并总结了初步试点的技术和组织经验教训，概述了将CUGA等研究级架构推进到稳健的企业级系统中的需求和后续步骤。

Abstract: Agents are rapidly advancing in automating digital work, but enterprises face
a harder challenge: moving beyond prototypes to deployed systems that deliver
measurable business value. This path is complicated by fragmented frameworks,
slow development, and the absence of standardized evaluation practices.
Generalist agents have emerged as a promising direction, excelling on academic
benchmarks and offering flexibility across task types, applications, and
modalities. Yet, evidence of their use in production enterprise settings
remains limited. This paper reports IBM's experience developing and piloting
the Computer Using Generalist Agent (CUGA), which has been open-sourced for the
community (https://github.com/cuga-project/cuga-agent). CUGA adopts a
hierarchical planner--executor architecture with strong analytical foundations,
achieving state-of-the-art performance on AppWorld and WebArena. Beyond
benchmarks, it was evaluated in a pilot within the Business-Process-Outsourcing
talent acquisition domain, addressing enterprise requirements for scalability,
auditability, safety, and governance. To support assessment, we introduce
BPO-TA, a 26-task benchmark spanning 13 analytics endpoints. In preliminary
evaluations, CUGA approached the accuracy of specialized agents while
indicating potential for reducing development time and cost. Our contribution
is twofold: presenting early evidence of generalist agents operating at
enterprise scale, and distilling technical and organizational lessons from this
initial pilot. We outline requirements and next steps for advancing
research-grade architectures like CUGA into robust, enterprise-ready systems.

</details>


### [79] [Generating Creative Chess Puzzles](https://arxiv.org/abs/2510.23881)
*Xidong Feng,Vivek Veeriah,Marcus Chiam,Michael Dennis,Ryan Pachauri,Thomas Tumiel,Federico Barbero,Johan Obando-Ceron,Jiaxin Shi,Satinder Singh,Shaobo Hou,Nenad Tomašev,Tom Zahavy*

Main category: cs.AI

TL;DR: 这篇论文提出了一种使用强化学习（RL）框架生成更具创造性、美感和违反直觉的国际象棋谜题的方法。该方法通过设计基于国际象棋引擎搜索统计的新颖奖励，来克服生成式人工智能在生成此类谜题方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 在国际象棋谜题领域，生成真正具有创造性、美感和违反直觉的输出仍然是一个挑战。

Method: 首先，对生成式人工智能架构进行基准测试，然后引入一个强化学习框架，该框架使用基于国际象棋引擎搜索统计的新颖奖励。

Result: 强化学习方法使违反直觉的谜题生成率提高了 10 倍，从 0.22%（监督式）提高到 2.5%，超过了现有的数据集比率（2.1%）和最佳 Lichess 训练模型（0.4%）。

Conclusion: 论文最终生成了一本由人工智能生成的谜题集，并获得了三位世界知名专家的认可。

Abstract: While Generative AI rapidly advances in various domains, generating truly
creative, aesthetic, and counter-intuitive outputs remains a challenge. This
paper presents an approach to tackle these difficulties in the domain of chess
puzzles. We start by benchmarking Generative AI architectures, and then
introduce an RL framework with novel rewards based on chess engine search
statistics to overcome some of those shortcomings. The rewards are designed to
enhance a puzzle's uniqueness, counter-intuitiveness, diversity, and realism.
Our RL approach dramatically increases counter-intuitive puzzle generation by
10x, from 0.22\% (supervised) to 2.5\%, surpassing existing dataset rates
(2.1\%) and the best Lichess-trained model (0.4\%). Our puzzles meet novelty
and diversity benchmarks, retain aesthetic themes, and are rated by human
experts as more creative, enjoyable, and counter-intuitive than composed book
puzzles, even approaching classic compositions. Our final outcome is a curated
booklet of these AI-generated puzzles, which is acknowledged for creativity by
three world-renowned experts.

</details>


### [80] [Hybrid Modeling, Sim-to-Real Reinforcement Learning, and Large Language Model Driven Control for Digital Twins](https://arxiv.org/abs/2510.23882)
*Adil Rasheed,Oscar Ravik,Omer San*

Main category: cs.AI

TL;DR: 本研究探讨了数字孪生在动态系统建模和控制中的应用，结合了基于物理、数据驱动和混合方法，以及传统和人工智能驱动的控制器。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索数字孪生技术在动态系统建模与控制中的应用潜力。

Method: 该研究使用一个微型温室作为测试平台，开发并比较了四种预测模型（线性模型、基于物理的建模（PBM）、长短期记忆网络（LSTM）和混合分析与建模（HAM）），以及三种控制策略（模型预测控制（MPC）、强化学习（RL）和基于大型语言模型（LLM）的控制）。

Result: 研究结果表明，在建模方面，HAM在准确性、泛化性和计算效率之间提供了最平衡的性能，而LSTM以更高的资源成本实现了高精度。在控制器方面，MPC提供了稳健和可预测的性能，RL表现出很强的适应性，而基于LLM的控制器在与预测工具结合使用时，提供了灵活的人机交互。

Conclusion: 该研究综合评估了不同建模方法和控制策略在数字孪生应用中的性能和优缺点，为实际应用提供了参考。

Abstract: This work investigates the use of digital twins for dynamical system modeling
and control, integrating physics-based, data-driven, and hybrid approaches with
both traditional and AI-driven controllers. Using a miniature greenhouse as a
test platform, four predictive models Linear, Physics-Based Modeling (PBM),
Long Short Term Memory (LSTM), and Hybrid Analysis and Modeling (HAM) are
developed and compared under interpolation and extrapolation scenarios. Three
control strategies Model Predictive Control (MPC), Reinforcement Learning (RL),
and Large Language Model (LLM) based control are also implemented to assess
trade-offs in precision, adaptability, and implementation effort. Results show
that in modeling HAM provides the most balanced performance across accuracy,
generalization, and computational efficiency, while LSTM achieves high
precision at greater resource cost. Among controllers, MPC delivers robust and
predictable performance, RL demonstrates strong adaptability, and LLM-based
controllers offer flexible human-AI interaction when coupled with predictive
tools.

</details>


### [81] [Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges](https://arxiv.org/abs/2510.23883)
*Shrestha Datta,Shahriar Kabir Nahin,Anshuman Chhabra,Prasant Mohapatra*

Main category: cs.AI

TL;DR: 本研究概述了由大型语言模型驱动的 Agentic AI 系统带来的新型安全风险，这些系统具有规划、工具使用、记忆和自主性。


<details>
  <summary>Details</summary>
Motivation: 自主执行任务的能力带来了新的安全风险，不同于传统人工智能安全和传统软件安全。

Method: 本研究概述了 Agentic AI 特有的威胁分类，回顾了近期的基准和评估方法，并从技术和治理角度讨论了防御策略。

Result: 合成了当前的研究，强调了开放的挑战。

Conclusion: 旨在支持安全设计代理系统的发展。

Abstract: Agentic AI systems powered by large language models (LLMs) and endowed with
planning, tool use, memory, and autonomy, are emerging as powerful, flexible
platforms for automation. Their ability to autonomously execute tasks across
web, software, and physical environments creates new and amplified security
risks, distinct from both traditional AI safety and conventional software
security. This survey outlines a taxonomy of threats specific to agentic AI,
reviews recent benchmarks and evaluation methodologies, and discusses defense
strategies from both technical and governance perspectives. We synthesize
current research and highlight open challenges, aiming to support the
development of secure-by-design agent systems.

</details>


### [82] [Latent Chain-of-Thought for Visual Reasoning](https://arxiv.org/abs/2510.23925)
*Guohao Sun,Hang Hua,Jian Wang,Jiebo Luo,Sohail Dianat,Majid Rabbani,Raghuveer Rao,Zhiqiang Tao*

Main category: cs.AI

TL;DR: 本文提出了一种基于分摊变分推理的可扩展训练算法，以提高大型视觉语言模型(LVLM)的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有训练算法泛化性差，依赖有偏奖励模型。

Method: 将LVLM中的推理重构为后验推理，并利用寻求多样性的强化学习算法引入新的稀疏奖励函数，鼓励多样化、高可能性的潜在CoT。

Result: 该方法在七个推理基准上增强了最先进的LVLM，在有效性、泛化性和可解释性方面均有提升。

Conclusion: 该方法通过鼓励多样化的推理路径和避免奖励偏差，提高了LVLM的推理能力。

Abstract: Chain-of-thought (CoT) reasoning is critical for improving the
interpretability and reliability of Large Vision-Language Models (LVLMs).
However, existing training algorithms such as SFT, PPO, and GRPO may not
generalize well across unseen reasoning tasks and heavily rely on a biased
reward model. To address this challenge, we reformulate reasoning in LVLMs as
posterior inference and propose a scalable training algorithm based on
amortized variational inference. By leveraging diversity-seeking reinforcement
learning algorithms, we introduce a novel sparse reward function for
token-level learning signals that encourage diverse, high-likelihood latent
CoT, overcoming deterministic sampling limitations and avoiding reward hacking.
Additionally, we implement a Bayesian inference-scaling strategy that replaces
costly Best-of-N and Beam Search with a marginal likelihood to efficiently rank
optimal rationales and answers. We empirically demonstrate that the proposed
method enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in
terms of effectiveness, generalization, and interpretability.

</details>


### [83] [Decentralized Causal Discovery using Judo Calculus](https://arxiv.org/abs/2510.23942)
*Sridhar Mahadevan*

Main category: cs.AI

TL;DR: 本文提出了一种使用柔道演算的直觉主义分散式因果发现框架的理论和实现。


<details>
  <summary>Details</summary>
Motivation: 在现实世界的应用中，因果效应取决于状态(年龄、国家、剂量、基因型或实验室协议)。

Method: 将柔道演算与基于分数的、基于约束的和基于梯度的因果发现方法相结合。

Result: 实验结果表明，sheaf-theoretic因果发现的分散性质获得了计算效率，并且比经典的因果发现方法有改进的性能。

Conclusion: 柔道演算将这种上下文依赖性形式化为局部真理：因果声明在一系列状态上被证明为真，而不是一次在所有地方都为真。

Abstract: We describe a theory and implementation of an intuitionistic decentralized
framework for causal discovery using judo calculus, which is formally defined
as j-stable causal inference using j-do-calculus in a topos of sheaves. In
real-world applications -- from biology to medicine and social science --
causal effects depend on regime (age, country, dose, genotype, or lab
protocol). Our proposed judo calculus formalizes this context dependence
formally as local truth: a causal claim is proven true on a cover of regimes,
not everywhere at once. The Lawvere-Tierney modal operator j chooses which
regimes are relevant; j-stability means the claim holds constructively and
consistently across that family. We describe an algorithmic and implementation
framework for judo calculus, combining it with standard score-based,
constraint-based, and gradient-based causal discovery methods. We describe
experimental results on a range of domains, from synthetic to real-world
datasets from biology and economics. Our experimental results show the
computational efficiency gained by the decentralized nature of sheaf-theoretic
causal discovery, as well as improved performance over classical causal
discovery methods.

</details>


### [84] [The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity](https://arxiv.org/abs/2510.23965)
*Aymane El Gadarri,Ali Aouad,Vivek F. Farias*

Main category: cs.AI

TL;DR: 传统LLM对齐方法容易受到人类偏好异质性的影响。论文提出了一种新的方法，称为符号估计器，通过在聚合步骤中用二元分类损失代替交叉熵，提供了一个简单、可证明一致且高效的估计器。


<details>
  <summary>Details</summary>
Motivation: 现有的概率模型在处理成对比较数据时，对社会福利的典型衡量标准——群体平均效用的估计不一致。

Method: 提出了符号估计器，用二元分类损失代替交叉熵。

Result: 在LLM对齐的模拟中，符号估计器显著减少了偏好扭曲，估计误差降低了近35%，与真实群体偏好不一致从12%降低到8%。

Conclusion: 该方法在保持实现简单性的同时，性能优于显式建模用户异质性的面板数据启发式方法。

Abstract: Traditional LLM alignment methods are vulnerable to heterogeneity in human
preferences. Fitting a na\"ive probabilistic model to pairwise comparison data
(say over prompt-completion pairs) yields an inconsistent estimate of the
population-average utility -a canonical measure of social welfare. We propose a
new method, dubbed the sign estimator, that provides a simple, provably
consistent, and efficient estimator by replacing cross-entropy with binary
classification loss in the aggregation step. This simple modification recovers
consistent ordinal alignment under mild assumptions and achieves the first
polynomial finite-sample error bounds in this setting. In realistic simulations
of LLM alignment using digital twins, the sign estimator substantially reduces
preference distortion over a panel of simulated personas, cutting (angular)
estimation error by nearly 35% and decreasing disagreement with true population
preferences from 12% to 8% compared to standard RLHF. Our method also compares
favorably to panel data heuristics that explicitly model user heterogeneity and
require tracking individual-level preference data-all while maintaining the
implementation simplicity of existing LLM alignment pipelines.

</details>


### [85] [Learning Individual Movement Shifts After Urban Disruptions with Social Infrastructure Reliance](https://arxiv.org/abs/2510.23989)
*Shangde Gao,Zelin Xu,Zhe Jiang*

Main category: cs.AI

TL;DR: 本研究利用大规模稀疏的个人数据，将个人社会基础设施弹性 (SIR) 纳入条件深度学习模型，以捕捉个人移动模式与本地空间环境之间的复杂关系。


<details>
  <summary>Details</summary>
Motivation: 预测破坏性事件发生前个体移动模式的变化具有挑战性，原因包括缺乏衡量个体异质社会基础设施弹性 (SIR) 的指标、常用特征的局限性以及个体移动模式与空间环境之间复杂互动的未被充分捕捉。

Method: 本研究采用条件深度学习模型，结合个体社会基础设施弹性 (SIR)，捕捉个体移动模式与本地空间环境之间的复杂关系。

Result: 实验表明，结合个体社会基础设施弹性 (SIR) 和空间环境可以提高模型预测事件后个体移动模式的能力。该条件模型可以捕捉到在事件前表现出相似模式但在 SIR 方面不同的个体之间移动模式的差异。

Conclusion: 将个人社会基础设施弹性 (SIR) 纳入模型可以有效预测事件后个体移动模式的变化。

Abstract: Shifts in individual movement patterns following disruptive events can reveal
changing demands for community resources. However, predicting such shifts
before disruptive events remains challenging for several reasons. First,
measures are lacking for individuals' heterogeneous social infrastructure
resilience (SIR), which directly influences their movement patterns, and
commonly used features are often limited or unavailable at scale, e.g.,
sociodemographic characteristics. Second, the complex interactions between
individual movement patterns and spatial contexts have not been sufficiently
captured. Third, individual-level movement may be spatially sparse and not
well-suited to traditional decision-making methods for movement predictions.
This study incorporates individuals' SIR into a conditioned deep learning model
to capture the complex relationships between individual movement patterns and
local spatial context using large-scale, sparse individual-level data. Our
experiments demonstrate that incorporating individuals' SIR and spatial context
can enhance the model's ability to predict post-event individual movement
patterns. The conditioned model can capture the divergent shifts in movement
patterns among individuals who exhibit similar pre-event patterns but differ in
SIR.

</details>


### [86] [Discovering Heuristics with Large Language Models (LLMs) for Mixed-Integer Programs: Single-Machine Scheduling](https://arxiv.org/abs/2510.24013)
*İbrahim Oğuz Çetinkaya,İ. Esra Büyüktahtakın,Parshin Shojaee,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 本研究利用大型语言模型（LLM）发现了新的启发式算法，用于解决单机总延迟（SMTT）问题，旨在最小化总延迟。


<details>
  <summary>Details</summary>
Motivation: 现有启发式算法较为简单，需要更高效的算法。

Method: 开发并评估了两种新的LLM发现的启发式算法，EDDC和MDDC，并与现有算法和精确方法在不同规模的任务上进行比较。

Result: EDDC在最多500个任务的情况下改进了经典EDD规则，MDDC始终优于传统启发式算法，并且在更大更复杂的实例中与精确方法相比仍具有竞争力。

Conclusion: 人与LLM的协作可以为NP-hard约束组合优化生成可扩展的，高性能的启发式算法。

Abstract: Our study contributes to the scheduling and combinatorial optimization
literature with new heuristics discovered by leveraging the power of Large
Language Models (LLMs). We focus on the single-machine total tardiness (SMTT)
problem, which aims to minimize total tardiness by sequencing n jobs on a
single processor without preemption, given processing times and due dates. We
develop and benchmark two novel LLM-discovered heuristics, the EDD Challenger
(EDDC) and MDD Challenger (MDDC), inspired by the well-known Earliest Due Date
(EDD) and Modified Due Date (MDD) rules. In contrast to prior studies that
employed simpler rule-based heuristics, we evaluate our LLM-discovered
algorithms using rigorous criteria, including optimality gaps and solution time
derived from a mixed-integer programming (MIP) formulation of SMTT. We compare
their performance against state-of-the-art heuristics and exact methods across
various job sizes (20, 100, 200, and 500 jobs). For instances with more than
100 jobs, exact methods such as MIP and dynamic programming become
computationally intractable. Up to 500 jobs, EDDC improves upon the classic EDD
rule and another widely used algorithm in the literature. MDDC consistently
outperforms traditional heuristics and remains competitive with exact
approaches, particularly on larger and more complex instances. This study shows
that human-LLM collaboration can produce scalable, high-performing heuristics
for NP-hard constrained combinatorial optimization, even under limited
resources when effectively configured.

</details>


### [87] [OneCast: Structured Decomposition and Modular Generation for Cross-Domain Time Series Forecasting](https://arxiv.org/abs/2510.24028)
*Tingyue Pan,Mingyue Cheng,Shilong Zhang,Zhiding Liu,Xiaoyu Tao,Yucong Luo,Jintao Zhang,Qi Liu*

Main category: cs.AI

TL;DR: OneCast: a forecasting framework that decomposes time series into seasonal and trend components, each modeled through tailored generative pathways.


<details>
  <summary>Details</summary>
Motivation: Existing methods often fall short when facing domain-specific trend shifts and inconsistent periodic patterns.

Method: decompose time series into seasonal and trend components, the seasonal component is captured by a lightweight projection module that reconstructs periodic patterns via interpretable basis functions; the trend component is encoded into discrete tokens at segment level via a semantic-aware tokenizer, and subsequently inferred through a masked discrete diffusion mechanism.

Result: OneCast mostly outperforms state-of-the-art baselines across eight domains.

Conclusion: achieving effective generalization across heterogeneous time series data remains a significant challenge. treat temporal series as undifferentiated sequence, without explicitly decoupling their inherent structural components.

Abstract: Cross-domain time series forecasting is a valuable task in various web
applications. Despite its rapid advancement, achieving effective generalization
across heterogeneous time series data remains a significant challenge. Existing
methods have made progress by extending single-domain models, yet often fall
short when facing domain-specific trend shifts and inconsistent periodic
patterns. We argue that a key limitation lies in treating temporal series as
undifferentiated sequence, without explicitly decoupling their inherent
structural components. To address this, we propose OneCast, a structured and
modular forecasting framework that decomposes time series into seasonal and
trend components, each modeled through tailored generative pathways.
Specifically, the seasonal component is captured by a lightweight projection
module that reconstructs periodic patterns via interpretable basis functions.
In parallel, the trend component is encoded into discrete tokens at segment
level via a semantic-aware tokenizer, and subsequently inferred through a
masked discrete diffusion mechanism. The outputs from both branches are
combined to produce a final forecast that captures seasonal patterns while
tracking domain-specific trends. Extensive experiments across eight domains
demonstrate that OneCast mostly outperforms state-of-the-art baselines.

</details>


### [88] [LLMLogAnalyzer: A Clustering-Based Log Analysis Chatbot using Large Language Models](https://arxiv.org/abs/2510.24031)
*Peng Cai,Reza Ryan,Nickson M. Karie*

Main category: cs.AI

TL;DR: LLMLogAnalyzer是一个基于聚类的日志分析聊天机器人，它利用大型语言模型(llm)和机器学习(ml)算法来简化和简化日志分析过程。


<details>
  <summary>Details</summary>
Motivation: 分析大量的不同日志数据仍然具有很大的挑战性，因为高成本、缺乏内部专业知识和时间限制使得许多组织即使是基本的分析也很困难。

Method: LLMLogAnalyzer 采用了一种创新的方法，该方法包括一个路由器、日志识别器、日志解析器和搜索工具的模块化架构。该设计增强了LLM的结构化文本分析能力，同时提高了准确性和鲁棒性。

Result: 在不同的任务中，与最先进的基于llm的聊天机器人(包括ChatGPT、ChatPDF和NotebookLM)相比，性能始终提高了39%到68%。当使用ROUGE-1分数时，该系统还表现出强大的鲁棒性，四分位间距(IQR)降低了93%，表明结果的可变性显著降低。

Conclusion: LLMLogAnalyzer 是一种有价值的资源，适用于网络安全专家和非技术用户。

Abstract: System logs are a cornerstone of cybersecurity, supporting proactive breach
prevention and post-incident investigations. However, analyzing vast amounts of
diverse log data remains significantly challenging, as high costs, lack of
in-house expertise, and time constraints make even basic analysis difficult for
many organizations. This study introduces LLMLogAnalyzer, a clustering-based
log analysis chatbot that leverages Large Language Models (LLMs) and Machine
Learning (ML) algorithms to simplify and streamline log analysis processes.
This innovative approach addresses key LLM limitations, including context
window constraints and poor structured text handling capabilities, enabling
more effective summarization, pattern extraction, and anomaly detection tasks.
LLMLogAnalyzer is evaluated across four distinct domain logs and various tasks.
Results demonstrate significant performance improvements over state-of-the-art
LLM-based chatbots, including ChatGPT, ChatPDF, and NotebookLM, with consistent
gains ranging from 39% to 68% across different tasks. The system also exhibits
strong robustness, achieving a 93% reduction in interquartile range (IQR) when
using ROUGE-1 scores, indicating significantly lower result variability. The
framework's effectiveness stems from its modular architecture comprising a
router, log recognizer, log parser, and search tools. This design enhances LLM
capabilities for structured text analysis while improving accuracy and
robustness, making it a valuable resource for both cybersecurity experts and
non-technical users.

</details>


### [89] [Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach](https://arxiv.org/abs/2510.24085)
*Md. Shihab Uddin,Md Nazmus Shakib,Rahul Bhadani*

Main category: cs.AI

TL;DR: 本文比较了经典模型和机器学习模型在电动汽车跟随行为预测中的表现，发现随机森林模型在各种场景下均优于经典模型。


<details>
  <summary>Details</summary>
Motivation: 为了提升交通安全和开发智能驾驶系统，需要了解电动汽车的驾驶行为。

Method: 使用智能驾驶模型（IDM）、最优速度模型（OVM）、最优速度相对速度（OVRV）和一个简化的CACC模型作为经典模型，并采用随机森林回归模型作为机器学习方法。使用真实世界数据集，通过最小化RMSE来校准经典模型参数。随机森林模型使用间距、速度和间隙类型作为输入来预测加速度。

Result: 随机森林模型表现出卓越的准确性，在不同间隙下的RMSE分别为0.0046（中等间隙）、0.0016（长间隙）和0.0025（超长间隙）。在基于物理的模型中，CACC表现最佳，长间隙下的RMSE为2.67。

Conclusion: 研究结果强调了机器学习模型在所有场景中的优越性能，对模拟电动汽车行为和分析电动汽车集成环境中的混合自主交通动态具有重要价值。

Abstract: The increasing adoption of electric vehicles (EVs) necessitates an
understanding of their driving behavior to enhance traffic safety and develop
smart driving systems. This study compares classical and machine learning
models for EV car following behavior. Classical models include the Intelligent
Driver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative
Velocity (OVRV), and a simplified CACC model, while the machine learning
approach employs a Random Forest Regressor. Using a real world dataset of an EV
following an internal combustion engine (ICE) vehicle under varied driving
conditions, we calibrated classical model parameters by minimizing the RMSE
between predictions and real data. The Random Forest model predicts
acceleration using spacing, speed, and gap type as inputs. Results demonstrate
the Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),
0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,
CACC performed best, with an RMSE of 2.67 for long gaps. These findings
highlight the machine learning model's performance across all scenarios. Such
models are valuable for simulating EV behavior and analyzing mixed autonomy
traffic dynamics in EV integrated environments.

</details>


### [90] [HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology](https://arxiv.org/abs/2510.24115)
*Sandeep Vissapragada,Vikrant Sahu,Gagan Raj Gupta,Vandita Singh*

Main category: cs.AI

TL;DR: HistoLens is an AI assistant for pathologists that provides transparent reasoning and visual proofs for its findings.


<details>
  <summary>Details</summary>
Motivation: Doctors need to understand the reasoning behind AI to trust it.

Method: The system translates plain English questions into precise queries for its AI engine and provides a structured report with visual proofs.

Result: Pathologists can use HistoLens to verify insights and make faster, more confident diagnoses.

Conclusion: HistoLens is a trustworthy AI assistant that helps pathologists in their workflow.

Abstract: For doctors to truly trust artificial intelligence, it can't be a black box.
They need to understand its reasoning, almost as if they were consulting a
colleague. We created HistoLens1 to be that transparent, collaborative partner.
It allows a pathologist to simply ask a question in plain English about a
tissue slide--just as they would ask a trainee. Our system intelligently
translates this question into a precise query for its AI engine, which then
provides a clear, structured report. But it doesn't stop there. If a doctor
ever asks, "Why?", HistoLens can instantly provide a 'visual proof' for any
finding--a heatmap that points to the exact cells and regions the AI used for
its analysis. We've also ensured the AI focuses only on the patient's tissue,
just like a trained pathologist would, by teaching it to ignore distracting
background noise. The result is a workflow where the pathologist remains the
expert in charge, using a trustworthy AI assistant to verify their insights and
make faster, more confident diagnoses.

</details>


### [91] [From Observability Data to Diagnosis: An Evolving Multi-agent System for Incident Management in Cloud Systems](https://arxiv.org/abs/2510.24145)
*Yu Luo,Jiamin Jiang,Jingfei Feng,Lei Tao,Qingliang Zhang,Xidao Wen,Yongqian Sun,Shenglin Zhang,Jielong Huang,Nan Qi,Dan Pei*

Main category: cs.AI

TL;DR: 这篇论文介绍了一种名为 OpsAgent 的轻量级、自演化的多代理系统，用于事件管理 (IM)。


<details>
  <summary>Details</summary>
Motivation: 现有自动化 IM 方法难以跨系统通用，可解释性有限，且部署成本高昂，从而阻碍了实际应用。

Method: OpsAgent 采用了一种免训练的数据处理器，将异构的可观察性数据转换为结构化的文本描述，以及一个多代理协作框架，使诊断推理透明且可审计。它还引入了一种双重自演化机制，将内部模型更新与外部经验积累相结合。

Result: 在 OPENRCA 基准上的综合实验表明，OpsAgent 具有最先进的性能，并且是可推广的、可解释的、具有成本效益的和自演化的。

Conclusion: OpsAgent 是一种可在实际中部署且可持续的解决方案，适用于现实世界云系统中的长期运行。

Abstract: Incident management (IM) is central to the reliability of large-scale cloud
systems. Yet manual IM, where on-call engineers examine metrics, logs, and
traces is labor-intensive and error-prone in the face of massive and
heterogeneous observability data. Existing automated IM approaches often
struggle to generalize across systems, provide limited interpretability, and
incur high deployment costs, which hinders adoption in practice. In this paper,
we present OpsAgent, a lightweight, self-evolving multi-agent system for IM
that employs a training-free data processor to convert heterogeneous
observability data into structured textual descriptions, along with a
multi-agent collaboration framework that makes diagnostic inference transparent
and auditable. To support continual capability growth, OpsAgent also introduces
a dual self-evolution mechanism that integrates internal model updates with
external experience accumulation, thereby closing the deployment loop.
Comprehensive experiments on the OPENRCA benchmark demonstrate state-of-the-art
performance and show that OpsAgent is generalizable, interpretable,
cost-efficient, and self-evolving, making it a practically deployable and
sustainable solution for long-term operation in real-world cloud systems.

</details>


### [92] [BMGQ: A Bottom-up Method for Generating Complex Multi-hop Reasoning Questions from Semi-structured Data](https://arxiv.org/abs/2510.24151)
*Bingsen Qiu,Zijian Liu,Xiao Liu,Haoshen Yang,Zeren Gao,Bingjie Wang,Feier Zhang,Yixuan Qin,Chunyan Li*

Main category: cs.AI

TL;DR: 提出了一种自动框架，用于从半结构化知识源生成高难度、可用于训练的多跳问题。


<details>
  <summary>Details</summary>
Motivation: 最近，构建能够真正强调模型检索和推理能力的多跳问答 (QA) 数据集仍然具有很高的挑战性。同时，手动管理非平凡的可检索问题会产生过高的人力成本并且无法扩展，从而为训练高能力检索和推理代理创建了关键的数据瓶颈。

Method: 该系统 (i) 通过基于自然语言推理 (NLI) 的关系类型和多样性感知扩展来增长多样化的、逻辑标记的证据集群；(ii) 应用反向问题构建来组成倾斜的线索，以便孤立的信号信息不足，但它们的组合唯一地标识目标实体；(iii) 通过将多模型共识过滤与结构化约束分解和基于证据的匹配相结合的两步评估管道来强制执行质量。

Result: 我们提出了一个可扩展的过程，可以产生复杂的、抗检索但可验证的问题，适用于 SFT/RL 训练以及具有挑战性的评估，从而大大减少了人工管理工作，同时保留了强大的评估基准的难度概况。

Conclusion: 该方法能够生成高质量、高难度的多跳问答数据集，可用于训练和评估模型，并大大减少了人工管理工作。

Abstract: Building training-ready multi-hop question answering (QA) datasets that truly
stress a model's retrieval and reasoning abilities remains highly challenging
recently. While there have been a few recent evaluation datasets that capture
the characteristics of hard-to-search but easy-to-verify problems -- requiring
the integration of ambiguous, indirect, and cross-domain cues -- these data
resources remain scarce and are mostly designed for evaluation, making them
unsuitable for supervised fine-tuning (SFT) or reinforcement learning (RL).
Meanwhile, manually curating non-trivially retrievable questions -- where
answers cannot be found through a single direct query but instead require
multi-hop reasoning over oblique and loosely connected evidence -- incurs
prohibitive human costs and fails to scale, creating a critical data bottleneck
for training high-capability retrieval-and-reasoning agents.
  To address this, we present an automated framework for generating
high-difficulty, training-ready multi-hop questions from semi-structured
knowledge sources. The system (i) grows diverse, logically labeled evidence
clusters through Natural Language Inference (NLI)-based relation typing and
diversity-aware expansion; (ii) applies reverse question construction to
compose oblique cues so that isolated signals are underinformative but their
combination uniquely identifies the target entity; and (iii) enforces quality
with a two-step evaluation pipeline that combines multi-model consensus
filtering with structured constraint decomposition and evidence-based matching.
The result is a scalable process that yields complex, retrieval-resistant yet
verifiable questions suitable for SFT/RL training as well as challenging
evaluation, substantially reducing human curation effort while preserving the
difficulty profile of strong evaluation benchmarks.

</details>


### [93] [BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning](https://arxiv.org/abs/2510.24161)
*Wentao Tan,Bowen Wang,Heng Zhi,Chenyu Liu,Zhe Li,Jian Liu,Zengrong Lin,Yukun Dai,Yipeng Chen,Wenjie Yang,Enci Xie,Hao Xue,Baixu Ji,Chen Xu,Zhibin Wang,Tianshi Wang,Lei Zhu,Heng Tao Shen*

Main category: cs.AI

TL;DR: 本文介绍了一种名为Boundless Large Model (BLM$_1$) 的多模态空间基础模型，该模型能够跨越数字和物理空间无缝运行，同时在不同形态和任务中实现泛化。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大型语言模型(MLLM)在数字-物理空间和形态上的泛化能力较差；视觉-语言-动作模型(VLA)产生低级动作，缺乏稳健的高级具身推理能力；大多数具身大型语言模型(ELLM)受限于数字空间，难以泛化到物理世界。因此，缺乏能够在数字和物理空间无缝运行，并在不同形态和任务中实现泛化的统一模型。

Method: BLM$_1$ 通过两阶段训练范式集成三种关键能力：跨空间转移、跨任务学习和跨形态泛化。第一阶段通过精选的数字语料库将具身知识注入MLLM，同时保持语言能力。第二阶段通过意图桥接接口训练策略模块，从MLLM中提取高级语义以指导控制，而无需微调MLLM主干。

Result: 在数字和物理基准测试中的评估表明，单个BLM$_1$ 实例优于四个模型系列——MLLM、ELLM、VLA 和 GMLM——在数字任务中实现了约 6% 的收益，在物理任务中实现了约 3% 的收益。

Conclusion: BLM$_1$ 是一个多模态空间基础模型，它保留了指令遵循和推理能力，结合了具身知识，并支持强大的跨形态控制。

Abstract: Multimodal large language models (MLLMs) have advanced vision-language
reasoning and are increasingly deployed in embodied agents. However,
significant limitations remain: MLLMs generalize poorly across digital-physical
spaces and embodiments; vision-language-action models (VLAs) produce low-level
actions yet lack robust high-level embodied reasoning; and most embodied large
language models (ELLMs) are constrained to digital-space with poor
generalization to the physical world. Thus, unified models that operate
seamlessly across digital and physical spaces while generalizing across
embodiments and tasks remain absent. We introduce the \textbf{Boundless Large
Model (BLM$_1$)}, a multimodal spatial foundation model that preserves
instruction following and reasoning, incorporates embodied knowledge, and
supports robust cross-embodiment control. BLM$_1$ integrates three key
capabilities -- \textit{cross-space transfer, cross-task learning, and
cross-embodiment generalization} -- via a two-stage training paradigm. Stage I
injects embodied knowledge into the MLLM through curated digital corpora while
maintaining language competence. Stage II trains a policy module through an
intent-bridging interface that extracts high-level semantics from the MLLM to
guide control, without fine-tuning the MLLM backbone. This process is supported
by a self-collected cross-embodiment demonstration suite spanning four robot
embodiments and six progressively challenging tasks. Evaluations across digital
and physical benchmarks show that a single BLM$_1$ instance outperforms four
model families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving
$\sim\!\textbf{6%}$ gains in digital tasks and $\sim\!\textbf{3%}$ in physical
tasks.

</details>


### [94] [UniPlanner: A Unified Motion Planning Framework for Autonomous Vehicle Decision-Making Systems via Multi-Dataset Integration](https://arxiv.org/abs/2510.24166)
*Xin Yang,Yuhang Zhang,Wei Li,Xin Lin,Wenbin Zou,Chen Xu*

Main category: cs.AI

TL;DR: 提出了一个名为UniPlanner的多数据集集成自动驾驶车辆决策规划框架，以解决现有方法在规划中鲁棒性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在规划能力上有所提升，但受限于单数据集训练，鲁棒性不足。

Method: 1) 历史-未来轨迹字典网络 (HFTDN): 聚合多数据集的历史-未来轨迹对，利用历史轨迹相似性检索相关未来轨迹，生成跨数据集规划指导。
2) 无梯度轨迹映射器 (GFTM): 从多数据集学习鲁棒的历史-未来相关性，将历史轨迹转换为通用规划先验。无梯度设计确保引入有价值的先验，同时防止捷径学习，使规划知识安全转移。
3) 稀疏到密集 (S2D) 范式: 实施自适应 dropout，在训练期间选择性地抑制规划先验以进行鲁棒学习，同时在推理期间实现完全先验利用，以最大限度地提高规划性能。

Result: UniPlanner实现了统一的跨数据集学习。

Conclusion: UniPlanner是首个为自动驾驶车辆决策中多数据集集成而设计的规划框架。

Abstract: Motion planning is a critical component of autonomous vehicle decision-making
systems, directly determining trajectory safety and driving efficiency. While
deep learning approaches have advanced planning capabilities, existing methods
remain confined to single-dataset training, limiting their robustness in
planning.
  Through systematic analysis, we discover that vehicular trajectory
distributions and history-future correlations demonstrate remarkable
consistency across different datasets. Based on these findings, we propose
UniPlanner, the first planning framework designed for multi-dataset integration
in autonomous vehicle decision-making. UniPlanner achieves unified
cross-dataset learning through three synergistic innovations.
  First, the History-Future Trajectory Dictionary Network (HFTDN) aggregates
history-future trajectory pairs from multiple datasets, using historical
trajectory similarity to retrieve relevant futures and generate cross-dataset
planning guidance.
  Second, the Gradient-Free Trajectory Mapper (GFTM) learns robust
history-future correlations from multiple datasets, transforming historical
trajectories into universal planning priors. Its gradient-free design ensures
the introduction of valuable priors while preventing shortcut learning, making
the planning knowledge safely transferable. Third, the Sparse-to-Dense (S2D)
paradigm implements adaptive dropout to selectively suppress planning priors
during training for robust learning, while enabling full prior utilization
during inference to maximize planning performance.

</details>


### [95] [MGA: Memory-Driven GUI Agent for Observation-Centric Interaction](https://arxiv.org/abs/2510.24168)
*Weihua Cheng,Ersheng Ni,Wenlong Wang,Yifei Sun,Junming Liu,Wangyu Shen,Yirong Chen,Botian Shi,Ding Wang*

Main category: cs.AI

TL;DR: 本文介绍了一种名为 Memory-Driven GUI Agent (MGA) 的新型 GUI 代理，它通过首先观察然后决策的原则来解决现有 GUI 代理的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有 GUI 代理依赖历史轨迹，容易放大误差传播，并且存在局部探索偏差，忽略关键界面线索。

Method: MGA 将每一步建模为一个独立的、上下文丰富的环境状态，由当前屏幕截图、任务无关的空间信息和动态更新的结构化内存表示。

Result: 在 OSworld 基准测试、真实桌面应用程序（Chrome、VSCode、VLC）和跨任务转移实验中，MGA 在鲁棒性、泛化性和效率方面都取得了显著的提升。

Conclusion: MGA 是一种更有效、更强大的 GUI 代理，它通过改变 GUI 交互方式，显著提升了性能。

Abstract: The rapid progress of Large Language Models (LLMs) and their multimodal
extensions (MLLMs) has enabled agentic systems capable of perceiving and acting
across diverse environments. A challenging yet impactful frontier is the
development of GUI agents, which must navigate complex desktop and web
interfaces while maintaining robustness and generalization. Existing paradigms
typically model tasks as long-chain executions, concatenating historical
trajectories into the context. While approaches such as Mirage and GTA1 refine
planning or introduce multi-branch action selection, they remain constrained by
two persistent issues: Dependence on historical trajectories, which amplifies
error propagation. And Local exploration bias, where "decision-first,
observation-later" mechanisms overlook critical interface cues. We introduce
the Memory-Driven GUI Agent (MGA), which reframes GUI interaction around the
principle of observe first, then decide. MGA models each step as an
independent, context-rich environment state represented by a triad: current
screenshot, task-agnostic spatial information, and a dynamically updated
structured memory. Experiments on OSworld benchmarks, real desktop applications
(Chrome, VSCode, VLC), and cross-task transfer demonstrate that MGA achieves
substantial gains in robustness, generalization, and efficiency compared to
state-of-the-art baselines. The code is publicly available at:
{https://anonymous.4open.science/r/MGA-3571}.

</details>


### [96] [MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and Scaling MCP Tools](https://arxiv.org/abs/2510.24284)
*Wenhao Wang,Peizhi Niu,Zhao Xu,Zhaoyu Chen,Jian Du,Yaxin Du,Xianghe Pang,Keduan Huang,Yanfeng Wang,Qiang Yan,Siheng Chen*

Main category: cs.AI

TL;DR: MCP-Flow是一个自动化的流程，用于大规模发现服务器、合成数据和训练模型，以提高LLM代理在现实世界MCP环境中的熟练程度。


<details>
  <summary>Details</summary>
Motivation: 现有MCP研究覆盖的服务器很少，依赖于昂贵的手动管理，并且缺乏训练支持，从而阻碍了在现实世界中的部署。

Method: MCP-Flow使用一个自动化的web代理驱动的流程，用于大规模服务器发现、数据合成和模型训练。它从1166个服务器和11536个工具中收集和过滤数据，生成68733个高质量的指令-函数调用对和6439个轨迹。

Result: MCP-Flow在驱动卓越的MCP工具选择、函数调用生成和增强的代理任务性能方面非常有效。

Conclusion: MCP-Flow为提高LLM代理在现实世界MCP环境中的能力提供了一个可扩展的基础。

Abstract: Large Language Models (LLMs) increasingly rely on external tools to perform
complex, realistic tasks, yet their ability to utilize the rapidly expanding
Model Contextual Protocol (MCP) ecosystem remains limited. Existing MCP
research covers few servers, depends on costly manual curation, and lacks
training support, hindering progress toward real-world deployment. To overcome
these limitations, we introduce MCP-Flow, an automated web-agent-driven
pipeline for large-scale server discovery, data synthesis, and model training.
MCP-Flow collects and filters data from 1166 servers and 11536 tools, producing
68733 high-quality instruction-function call pairs and 6439 trajectories, far
exceeding prior work in scale and diversity. Extensive experiments demonstrate
MCP-Flow's effectiveness in driving superior MCP tool selection, function-call
generation, and enhanced agentic task performance. MCP-Flow thus provides a
scalable foundation for advancing LLM agents' proficiency in real-world MCP
environments. MCP-Flow is publicly available at
\href{https://github.com/wwh0411/MCP-Flow}{https://github.com/wwh0411/MCP-Flow}.

</details>


### [97] [Investigating Intra-Abstraction Policies For Non-exact Abstraction Algorithms](https://arxiv.org/abs/2510.24297)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 本文研究了蒙特卡洛树搜索（MCTS）中抽象方法在同一抽象节点内存在多个动作时的策略选择问题，发现现有方法未充分考虑这种情况，并提出了几种新的策略。


<details>
  <summary>Details</summary>
Motivation: MCTS的采样效率较低，可以通过构建和使用状态和/或动作抽象来解决，以便在同一层的节点之间共享信息。现有抽象方法未充分考虑同一抽象节点内存在多个动作的情况。

Method: 提出并评估了几种替代的内部抽象策略。

Result: 几种提出的策略在大多数环境和参数设置中优于随机策略。

Conclusion: 本文提出并验证了几种改进MCTS抽象策略的方法，可以提升算法性能。

Abstract: One weakness of Monte Carlo Tree Search (MCTS) is its sample efficiency which
can be addressed by building and using state and/or action abstractions in
parallel to the tree search such that information can be shared among nodes of
the same layer. The primary usage of abstractions for MCTS is to enhance the
Upper Confidence Bound (UCB) value during the tree policy by aggregating visits
and returns of an abstract node. However, this direct usage of abstractions
does not take the case into account where multiple actions with the same parent
might be in the same abstract node, as these would then all have the same UCB
value, thus requiring a tiebreak rule. In state-of-the-art abstraction
algorithms such as pruned On the Go Abstractions (pruned OGA), this case has
not been noticed, and a random tiebreak rule was implicitly chosen. In this
paper, we propose and empirically evaluate several alternative
intra-abstraction policies, several of which outperform the random policy
across a majority of environments and parameter settings.

</details>


### [98] [Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank](https://arxiv.org/abs/2510.24299)
*Jiayu Liu,Wei Dai,Zhenya Huang,Ning Miao,Enhong Chen*

Main category: cs.AI

TL;DR: 本文提出了一种新的LLM输出检查方法，该方法利用LLM内部行为来判断推理路径的正确性，无需外部资源。


<details>
  <summary>Details</summary>
Motivation: 现有LLM检查方法依赖外部资源，计算开销大，适用范围有限。

Method: 通过计算输入问题和输出推理路径之间的相关矩阵的秩，作为推理正确性的指标。设计了一个即插即用的Self-Indicator方法来重新调整候选推理路径的权重。

Result: Self-Indicator方法在区分正确和错误推理路径方面达到了75%以上的准确率，并在三个推理基准测试中将准确率提高了8%以上。

Conclusion: 相关矩阵的秩是LLM推理正确性的一个可靠指标，Self-Indicator方法能够有效提高LLM的推理准确性。

Abstract: Despite the strong reasoning ability of large language models~(LLMs), they
are prone to errors and hallucinations. As a result, how to check their outputs
effectively and efficiently has become a critical problem in their
applications. Existing checking methods heavily rely on external resources,
such as trained verifiers (e.g., process/outcome reward models) or elaborate
prompts, which lead to high computational overhead and are only applicable to
specific domains. In this paper, we investigate whether the internal behaviors
of LLMs have already implied the credibility of their reasoning paths.
Specifically, we find that the rank of the correlation matrix between the input
problem and the output reasoning path is a robust indicator of reasoning
correctness. Different from other correctness indicators for LLMs, the
calculation of the correlation matrix only relies on the LLM itself, which
avoids the hassle of training a separate model or designing complicated
prompts. Based on it, we design a simple, plug-and-play Self-Indicator method
to reweight candidate reasoning paths, which achieves significant performance
improvements than other voting and verification methods with very few
computational overhead. Our experiments across multiple LLMs of varying scales
and model families have further shown the effectiveness of Self-Indicator. It
achieves over 75% accuracy in distinguishing correct reasoning paths from
incorrect ones, and, in turn, improves the accuracies on three reasoning
benchmarks by more than 8%.

</details>


### [99] [Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental Forecasting](https://arxiv.org/abs/2510.24303)
*Deniz Gorur,Antoni Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 提出了一种新的多智能体框架用于判断预测，其中不同的智能体可能对声明的真实性存在分歧，并为支持和反对声明提供具体证据。


<details>
  <summary>Details</summary>
Motivation: 基于人类判断对未来事件进行预测的任务可以看作是一种声明验证，其中声明对应于未来事件，任务是评估该事件的合理性。

Method: 提出一个新颖的用于声明验证的多智能体框架，不同的智能体可能对声明的真实性存在分歧，并为支持和反对声明提供具体的证据，这些证据表示为定量双极论证框架 (QBAF)。

Result: 结合来自智能体的证据可以提高预测准确性，尤其是在三个智能体的情况下，同时为声明验证提供可解释的证据组合。

Conclusion: 通过实验观察到，结合来自智能体的证据可以提高预测准确性，尤其是在三个智能体的情况下，同时为声明验证提供可解释的证据组合。

Abstract: Judgmental forecasting is the task of making predictions about future events
based on human judgment. This task can be seen as a form of claim verification,
where the claim corresponds to a future event and the task is to assess the
plausibility of that event. In this paper, we propose a novel multi-agent
framework for claim verification, whereby different agents may disagree on
claim veracity and bring specific evidence for and against the claims,
represented as quantitative bipolar argumentation frameworks (QBAFs). We then
instantiate the framework for supporting claim verification, with a variety of
agents realised with Large Language Models (LLMs): (1) ArgLLM agents, an
existing approach for claim verification that generates and evaluates QBAFs;
(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)
from external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,
extending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of
arguments from external sources. Finally, we conduct experiments with two
standard judgmental forecasting datasets, with instances of our framework with
two or three agents, empowered by six different base LLMs. We observe that
combining evidence from agents can improve forecasting accuracy, especially in
the case of three agents, while providing an explainable combination of
evidence for claim verification.

</details>


### [100] [Generative Large Language Models (gLLMs) in Content Analysis: A Practical Guide for Communication Research](https://arxiv.org/abs/2510.24337)
*Daria Kravets-Meinke,Hannah Schmid-Petri,Sonja Niemann,Ute Schmid*

Main category: cs.AI

TL;DR: 本文旨在为通信研究人员提供一个全面的最佳实践指南，以应对在 gLLM 辅助的定量内容分析中遇到的挑战，从而确保研究结果的质量并使其更易于访问。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式大型语言模型 (gLLM) 在通信研究中的内容分析中越来越多地被使用，但它们在方法论工具包中的整合仍然不发达。研究表明，gLLM 在各种与通信科学相关的编码任务上可以胜过众包工作者和训练有素的编码员，而且通常只需花费很少的时间和成本。此外，gLLM 可以解码隐含意义和上下文信息，可以使用自然语言进行指导，只需基本的编程技能即可部署，并且除了验证数据集之外几乎不需要或不需要注释数据——这构成了自动化内容分析的范式转变。

Method: 本文综合了新兴的 gLLM 辅助定量内容分析研究，并提出了一个全面的最佳实践指南，以应对至少七个影响结果质量的关键挑战：(1) 编码手册开发，(2) 提示工程，(3) 模型选择，(4) 参数调整，(5) 迭代改进，(6) 模型可靠性的验证，以及可选的 (7) 性能增强。

Result: 本文旨在提高 gLLM 在内容分析中的可访问性，并确保遵守已建立的学科质量标准，如有效性、可靠性、可重复性和研究伦理。

Conclusion: 本文旨在为通信研究人员提供 gLLM 辅助定量内容分析的最佳实践指南，以应对挑战并确保研究质量。

Abstract: Generative Large Language Models (gLLMs), such as ChatGPT, are increasingly
being used in communication research for content analysis. Studies show that
gLLMs can outperform both crowd workers and trained coders, such as research
assistants, on various coding tasks relevant to communication science, often at
a fraction of the time and cost. Additionally, gLLMs can decode implicit
meanings and contextual information, be instructed using natural language,
deployed with only basic programming skills, and require little to no annotated
data beyond a validation dataset - constituting a paradigm shift in automated
content analysis. Despite their potential, the integration of gLLMs into the
methodological toolkit of communication research remains underdeveloped. In
gLLM-assisted quantitative content analysis, researchers must address at least
seven critical challenges that impact result quality: (1) codebook development,
(2) prompt engineering, (3) model selection, (4) parameter tuning, (5)
iterative refinement, (6) validation of the model's reliability, and
optionally, (7) performance enhancement. This paper synthesizes emerging
research on gLLM-assisted quantitative content analysis and proposes a
comprehensive best-practice guide to navigate these challenges. Our goal is to
make gLLM-based content analysis more accessible to a broader range of
communication researchers and ensure adherence to established disciplinary
quality standards of validity, reliability, reproducibility, and research
ethics.

</details>


### [101] [VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science Automation](https://arxiv.org/abs/2510.24339)
*Yunxuan Jiang,Silan Hu,Xiaoning Wang,Yuanyuan Zhang,Xiangyu Chang*

Main category: cs.AI

TL;DR: VDSAgents是一个基于Veridical Data Science (VDS)框架的多智能体系统，它将可预测性-可计算性-稳定性 (PCS) 原则嵌入到LLM驱动的数据科学自动化中，用于数据清理、特征工程、建模和评估。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM驱动的数据科学系统仅仅依赖于LLM的内部推理，缺乏科学和理论原则的指导，这限制了它们的可靠性和鲁棒性，尤其是在处理嘈杂和复杂的真实世界数据集时。

Method: 该系统实现了模块化的工作流程，每个阶段都由一个智能体处理，结合了扰动分析、单元测试和模型验证，以确保功能性和科学可审计性。

Result: 在九个具有不同特征的数据集上，VDSAgents优于AutoKaggle和DataInterpreter等最先进的端到端数据科学系统。

Conclusion: 将PCS原则嵌入到LLM驱动的数据科学自动化中是可行的。

Abstract: Large language models (LLMs) become increasingly integrated into data science
workflows for automated system design. However, these LLM-driven data science
systems rely solely on the internal reasoning of LLMs, lacking guidance from
scientific and theoretical principles. This limits their trustworthiness and
robustness, especially when dealing with noisy and complex real-world datasets.
This paper provides VDSAgents, a multi-agent system grounded in the
Predictability-Computability-Stability (PCS) principles proposed in the
Veridical Data Science (VDS) framework. Guided by PCS principles, the system
implements a modular workflow for data cleaning, feature engineering, modeling,
and evaluation. Each phase is handled by an elegant agent, incorporating
perturbation analysis, unit testing, and model validation to ensure both
functionality and scientific auditability. We evaluate VDSAgents on nine
datasets with diverse characteristics, comparing it with state-of-the-art
end-to-end data science systems, such as AutoKaggle and DataInterpreter, using
DeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the
results of AutoKaggle and DataInterpreter, which validates the feasibility of
embedding PCS principles into LLM-driven data science automation.

</details>


### [102] [A Unified Geometric Space Bridging AI Models and the Human Brain](https://arxiv.org/abs/2510.24342)
*Silin Chen,Yuzhong Chen,Zifan Wang,Junhao Wang,Zifeng Jia,Keith M Kendrick,Tuo Zhang,Lin Zhao,Dezhong Yao,Tianming Liu,Xi Jiang*

Main category: cs.AI

TL;DR: 提出了一种名为“类脑空间”的统一框架，用于比较不同人工智能模型的内在组织方式与人脑的相似度。


<details>
  <summary>Details</summary>
Motivation: 旨在了解人工智能系统是否像大脑一样组织信息，并打破现有研究在输入和任务上的局限性，从而比较不同模态人工智能模型的内在组织。

Method: 通过将人工智能模型的内在空间注意力拓扑组织映射到典型的人类功能脑网络，从而将每个人工智能模型精确定位并比较。

Result: 揭示了一个连续的弧形几何结构，反映了类脑性的逐渐增加；不同的模型在这个几何结构中表现出不同的分布模式，这些模式与不同的类脑程度相关，并且受到预训练范式和位置编码方案的影响。模型的大脑相似度和下游任务性能并非完全一致。

Conclusion: “类脑空间”提供了一个统一的框架，用于跨领域定位、量化和比较智能，揭示了连接机器和大脑的深层组织原则。

Abstract: For decades, neuroscientists and computer scientists have pursued a shared
ambition: to understand intelligence and build it. Modern artificial neural
networks now rival humans in language, perception, and reasoning, yet it is
still largely unknown whether these artificial systems organize information as
the brain does. Existing brain-AI alignment studies have shown the striking
correspondence between the two systems, but such comparisons remain bound to
specific inputs and tasks, offering no common ground for comparing how AI
models with different kinds of modalities-vision, language, or multimodal-are
intrinsically organized. Here we introduce a groundbreaking concept of
Brain-like Space: a unified geometric space in which every AI model can be
precisely situated and compared by mapping its intrinsic spatial attention
topological organization onto canonical human functional brain networks,
regardless of input modality, task, or sensory domain. Our extensive analysis
of 151 Transformer-based models spanning state-of-the-art large vision models,
large language models, and large multimodal models uncovers a continuous
arc-shaped geometry within this space, reflecting a gradual increase of
brain-likeness; different models exhibit distinct distribution patterns within
this geometry associated with different degrees of brain-likeness, shaped not
merely by their modality but by whether the pretraining paradigm emphasizes
global semantic abstraction and whether the positional encoding scheme
facilitates deep fusion across different modalities. Moreover, the degree of
brain-likeness for a model and its downstream task performance are not
"identical twins". The Brain-like Space provides the first unified framework
for situating, quantifying, and comparing intelligence across domains,
revealing the deep organizational principles that bridge machines and the
brain.

</details>


### [103] [An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine](https://arxiv.org/abs/2510.24359)
*Pedram Fard,Alaleh Azhir,Neguine Rezaii,Jiazi Tian,Hossein Estiri*

Main category: cs.AI

TL;DR: 传统的医疗人工智能以服务普通患者为目标，但在边缘情况下表现不佳，造成公平性和信任问题。本文提出了一种多智能体生态系统，用于N-of-1决策支持，通过权衡可靠性、不确定性和数据密度，为临床医生提供决策支持包。验证从人群平均转移到个体可靠性，通过低密度区域的误差、小范围校准和风险覆盖权衡来衡量。面临的挑战包括计算需求、自动化偏差和监管适应性，通过缓存策略、共识检查和自适应试验框架来解决。该方法旨在使医疗人工智能与医学的首要原则保持一致：透明、公平和以个人为中心的护理。


<details>
  <summary>Details</summary>
Motivation: 传统医疗人工智能在处理罕见变异、多病或代表性不足的人群时存在局限性，损害了公平性和信任。

Method: 提出一种多智能体生态系统，其中按器官系统、患者群体和分析模式聚集的智能体利用共享的模型和证据合成工具，结果汇聚在协调层，该协调层权衡可靠性、不确定性和数据密度，然后向临床医生提供决策支持包。

Result: 验证从人群平均转移到个体可靠性，通过低密度区域的误差、小范围校准和风险覆盖权衡来衡量。

Conclusion: 通过从单一模型转向协调智能，该方法旨在使医疗人工智能与医学的首要原则保持一致：透明、公平和以个人为中心的护理。

Abstract: Artificial intelligence in medicine is built to serve the average patient. By
minimizing error across large datasets, most systems deliver strong aggregate
accuracy yet falter at the margins: patients with rare variants,
multimorbidity, or underrepresented demographics. This average patient fallacy
erodes both equity and trust. We propose a different design: a multi-agent
ecosystem for N-of-1 decision support. In this environment, agents clustered by
organ systems, patient populations, and analytic modalities draw on a shared
library of models and evidence synthesis tools. Their results converge in a
coordination layer that weighs reliability, uncertainty, and data density
before presenting the clinician with a decision-support packet: risk estimates
bounded by confidence ranges, outlier flags, and linked evidence. Validation
shifts from population averages to individual reliability, measured by error in
low-density regions, calibration in the small, and risk--coverage trade-offs.
Anticipated challenges include computational demands, automation bias, and
regulatory fit, addressed through caching strategies, consensus checks, and
adaptive trial frameworks. By moving from monolithic models to orchestrated
intelligence, this approach seeks to align medical AI with the first principle
of medicine: care that is transparent, equitable, and centered on the
individual.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [104] [Odyssey: An End-to-End System for Pareto-Optimal Serverless Query Processing](https://arxiv.org/abs/2510.24307)
*Shyam Jesalpura,Shengda Zhu,Amir Shaikhha,Antonio Barbalace,Boris Grot*

Main category: cs.DB

TL;DR: 这篇论文介绍了一种名为 Odyssey 的端到端 serverless 数据分析流水线，它集成了查询规划器、成本模型和执行引擎。


<details>
  <summary>Details</summary>
Motivation: 现有serverless数据分析工作主要关注serverless执行引擎，并假设存在“好的”查询执行计划，或者依赖用户指导来构建这样的计划。然而，即使在serverless上进行简单的分析查询，也存在巨大的可能计划空间，不同计划在性能和成本上存在巨大差异。

Method: Odyssey 自动生成和评估 serverless 查询计划，利用状态空间修剪启发式方法和一种新的搜索算法来识别 Pareto 最优计划，即使对于复杂的查询，也能以低延迟平衡成本和性能。

Result: 评估表明，Odyssey 能够准确预测货币成本和延迟，并且在成本和/或延迟方面始终优于 AWS Athena。

Conclusion: Odyssey 在 serverless 数据分析方面表现出色，能够自动生成和评估查询计划，并在成本和性能方面优于现有方案。

Abstract: Running data analytics queries on serverless (FaaS) workers has been shown to
be cost- and performance-efficient for a variety of real-world scenarios,
including intermittent query arrival patterns, sudden load spikes and
management challenges that afflict managed VM clusters. Alas, existing
serverless data analytics works focus primarily on the serverless execution
engine and assume the existence of a "good" query execution plan or rely on
user guidance to construct such a plan. Meanwhile, even simple analytics
queries on serverless have a huge space of possible plans, with vast
differences in both performance and cost among plans.
  This paper introduces Odyssey, an end-to-end serverless-native data analytics
pipeline that integrates a query planner, cost model and execution engine.
Odyssey automatically generates and evaluates serverless query plans, utilizing
state space pruning heuristics and a novel search algorithm to identify
Pareto-optimal plans that balance cost and performance with low latency even
for complex queries. Our evaluations demonstrate that Odyssey accurately
predicts both monetary cost and latency, and consistently outperforms AWS
Athena on cost and/or latency.

</details>


### [105] [Evaluating Joinable Column Discovery Approaches for Context-Aware Search](https://arxiv.org/abs/2510.24599)
*Harsha Kokel,Aamod Khatiwada,Tejaswini Pedapati,Haritha Ananthakrishnan,Oktie Hassanzadeh,Horst Samulowitz,Kavitha Srinivas*

Main category: cs.DB

TL;DR: 本文对可连接列发现方法进行了全面的实验评估，并提供了选择合适方法的实践指南。


<details>
  <summary>Details</summary>
Motivation: 企业数据分析中，可连接列发现至关重要，但现有方法对不同数据特征的最佳方法以及多重标准如何影响发现效果的理解有限。

Method: 本文比较了关系数据库和数据湖上七个基准的语法和语义技术，并分析了六个关键标准（唯一值、交集大小、连接大小、反向连接大小、值语义和元数据语义），并研究了通过集成排序组合它们如何影响性能。

Result: 元数据和值语义对于数据湖至关重要，基于大小的标准在关系数据库中起着更重要的作用，并且集成方法始终优于单标准方法。

Conclusion: 本文揭示了跨数据上下文的方法行为差异，并强调了整合多个标准以实现可靠连接发现的好处，为根据数据集特征选择合适的方法提供了实践指南。

Abstract: Joinable Column Discovery is a critical challenge in automating enterprise
data analysis. While existing approaches focus on syntactic overlap and
semantic similarity, there remains limited understanding of which methods
perform best for different data characteristics and how multiple criteria
influence discovery effectiveness. We present a comprehensive experimental
evaluation of joinable column discovery methods across diverse scenarios. Our
study compares syntactic and semantic techniques on seven benchmarks covering
relational databases and data lakes. We analyze six key criteria -- unique
values, intersection size, join size, reverse join size, value semantics, and
metadata semantics -- and examine how combining them through ensemble ranking
affects performance. Our analysis reveals differences in method behavior across
data contexts and highlights the benefits of integrating multiple criteria for
robust join discovery. We provide empirical evidence on when each criterion
matters, compare pre-trained embedding models for semantic joins, and offer
practical guidelines for selecting suitable methods based on dataset
characteristics. Our findings show that metadata and value semantics are
crucial for data lakes, size-based criteria play a stronger role in relational
databases, and ensemble approaches consistently outperform single-criterion
methods.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [106] [Resource-Efficient LLM Application for Structured Transformation of Unstructured Financial Contracts](https://arxiv.org/abs/2510.23990)
*Maruf Ahmed Mridul,Oshani Seneviratne*

Main category: cs.IR

TL;DR: 本文提出了一种 CDMizer 框架的扩展，用于将非结构化法律合同转换为机器可读的 CDM 格式，以实现金融工作流程的自动化。


<details>
  <summary>Details</summary>
Motivation: 将非结构化法律合同转换为标准化的机器可读格式对于自动化金融工作流程至关重要。信用支持附件 (CSA) 等复杂法律文件转换为 CDM 表示仍然是一个重大挑战。

Method: 本文提出了一种 CDMizer 框架的扩展，这是一种模板驱动的解决方案，可确保合同到 CDM 转换期间的句法正确性和符合 CDM 模式。

Result: 本文将扩展后的框架应用于实际任务，并将其性能与国际掉期和衍生品协会 (ISDA) 开发的 CSA 条款提取基准进行了比较。结果表明，CDMizer 在与更小的开源大型语言模型 (LLM) 集成时，在准确性和效率方面实现了与更大的专有模型相媲美的性能。

Conclusion: 这项工作强调了资源高效解决方案在自动化法律合同转换方面的潜力，提供了一种经济高效且可扩展的方法，可以满足资源有限或数据隐私要求严格的金融机构的需求。

Abstract: The transformation of unstructured legal contracts into standardized,
machine-readable formats is essential for automating financial workflows. The
Common Domain Model (CDM) provides a standardized framework for this purpose,
but converting complex legal documents like Credit Support Annexes (CSAs) into
CDM representations remains a significant challenge. In this paper, we present
an extension of the CDMizer framework, a template-driven solution that ensures
syntactic correctness and adherence to the CDM schema during contract-to-CDM
conversion. We apply this extended framework to a real-world task, comparing
its performance with a benchmark developed by the International Swaps and
Derivatives Association (ISDA) for CSA clause extraction. Our results show that
CDMizer, when integrated with a significantly smaller, open-source Large
Language Model (LLM), achieves competitive performance in terms of accuracy and
efficiency against larger, proprietary models. This work underscores the
potential of resource-efficient solutions to automate legal contract
transformation, offering a cost-effective and scalable approach that can meet
the needs of financial institutions with constrained resources or strict data
privacy requirements.

</details>


### [107] [DUET: Dual Model Co-Training for Entire Space CTR Prediction](https://arxiv.org/abs/2510.24369)
*Yutian Xiao,Meng Yuan,Fuzhen Zhuang,Wei Chen,Shukuan Wang,Shanqi Liu,Chao Feng,Wenhui Yu,Xiang Li,Lantao Hu,Han Li,Zhao Zhang*

Main category: cs.IR

TL;DR: 提出了一种名为DUET的框架，用于大规模推荐系统中的预排序阶段，旨在解决模型表达能力和计算效率之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有的双塔架构在捕获候选项目之间复杂的关系方面存在局限性，并且容易受到样本选择偏差（SSB）问题的影响。

Method: DUET通过在单个前向传递中对整个候选子集执行集合级别的预测，从而实现候选项目之间的信息感知交互，并通过双模型协同训练机制扩展对未曝光项目的监督，从而有效缓解SSB。

Result: 离线实验和在线A/B测试表明，DUET优于现有技术水平的基线，并在多个核心业务指标上实现了改进。DUET已在快手和快手极速版App中全面部署。

Conclusion: DUET框架能够在严格的计算预算下实现表达性建模，缓解样本选择偏差问题，并在实际应用中取得了显著效果。

Abstract: The pre-ranking stage plays a pivotal role in large-scale recommender systems
but faces an intrinsic trade-off between model expressiveness and computational
efficiency. Owing to the massive candidate pool and strict latency constraints,
industry systems often rely on lightweight two-tower architectures, which are
computationally efficient yet limited in estimation capability. As a result,
they struggle to capture the complex synergistic and suppressive relationships
among candidate items, which are essential for producing contextually coherent
and diverse recommendation lists. Moreover, this simplicity further amplifies
the Sample Selection Bias (SSB) problem, as coarse-grained models trained on
biased exposure data must generalize to a much larger candidate space with
distinct distributions.
  To address these issues, we propose \textbf{DUET} (\textbf{DU}al Model
Co-Training for \textbf{E}ntire Space C\textbf{T}R Prediction), a set-wise
pre-ranking framework that achieves expressive modeling under tight
computational budgets. Instead of scoring items independently, DUET performs
set-level prediction over the entire candidate subset in a single forward pass,
enabling information-aware interactions among candidates while amortizing the
computational cost across the set. Moreover, a dual model co-training mechanism
extends supervision to unexposed items via mutual pseudo-label refinement,
effectively mitigating SSB. Validated through extensive offline experiments and
online A/B testing, DUET consistently outperforms state-of-the-art baselines
and achieves improvements across multiple core business metrics. At present,
DUET has been fully deployed in Kuaishou and Kuaishou Lite Apps, serving the
main traffic for hundreds of millions of users.

</details>


### [108] [Metadata-Driven Retrieval-Augmented Generation for Financial Question Answering](https://arxiv.org/abs/2510.24402)
*Michail Dadopoulos,Anestis Ladas,Stratos Moschidis,Ioannis Negkakis*

Main category: cs.IR

TL;DR: 本文提出了一种改进的RAG架构，该架构利用LLM生成的元数据来处理长篇、结构化的财务文件。


<details>
  <summary>Details</summary>
Motivation: 现有RAG技术在处理包含稀疏和交叉引用的长篇结构化财务文件时存在困难。

Method: 本文提出了一种新颖的多阶段RAG架构，并评估了一系列增强技术，包括预检索过滤、后检索重排序和丰富的嵌入。

Result: 结果表明，强大的重排序器对于精度至关重要，而性能提升主要来自于将块元数据直接嵌入文本（“上下文块”）。

Conclusion: 本文提出的优化架构结合了LLM驱动的预检索优化和上下文嵌入，实现了卓越的性能。

Abstract: Retrieval-Augmented Generation (RAG) struggles on long, structured financial
filings where relevant evidence is sparse and cross-referenced. This paper
presents a systematic investigation of advanced metadata-driven
Retrieval-Augmented Generation (RAG) techniques, proposing and evaluating a
novel, multi-stage RAG architecture that leverages LLM-generated metadata. We
introduce a sophisticated indexing pipeline to create contextually rich
document chunks and benchmark a spectrum of enhancements, including
pre-retrieval filtering, post-retrieval reranking, and enriched embeddings,
benchmarked on the FinanceBench dataset. Our results reveal that while a
powerful reranker is essential for precision, the most significant performance
gains come from embedding chunk metadata directly with text ("contextual
chunks"). Our proposed optimal architecture combines LLM-driven pre-retrieval
optimizations with these contextual embeddings to achieve superior performance.
Additionally, we present a custom metadata reranker that offers a compelling,
cost-effective alternative to commercial solutions, highlighting a practical
trade-off between peak performance and operational efficiency. This study
provides a blueprint for building robust, metadata-aware RAG systems for
financial document analysis.

</details>


### [109] [From Time and Place to Preference: LLM-Driven Geo-Temporal Context in Recommendations](https://arxiv.org/abs/2510.24430)
*Yejin Kim,Shaghayegh Agah,Mayur Nankani,Neeraj Sharma,Feifei Peng,Maria Peifer,Sardar Hamidian,H Howie Huang*

Main category: cs.IR

TL;DR: 提出了一种可扩展的框架，该框架使用大型语言模型 (LLM) 仅从时间戳和粗略位置生成地理时间嵌入，从而捕获假期、季节性趋势和本地/全球事件。


<details>
  <summary>Details</summary>
Motivation: 大多数推荐系统将时间戳视为数字或循环值，忽略了现实世界的背景，例如假期、事件和季节性模式。

Method: 使用大型语言模型 (LLM) 生成地理时间嵌入，然后引入地理时间嵌入信息量测试作为轻量级诊断。

Result: 在 MovieLens、LastFM 和生产数据集上证明，这些嵌入提供了与完整模型集成结果一致的预测信号。

Conclusion: 强调了对自适应或混合推荐策略的需求，并发布了一个上下文丰富的 MovieLens 数据集以支持未来的研究。

Abstract: Most recommender systems treat timestamps as numeric or cyclical values,
overlooking real-world context such as holidays, events, and seasonal patterns.
We propose a scalable framework that uses large language models (LLMs) to
generate geo-temporal embeddings from only a timestamp and coarse location,
capturing holidays, seasonal trends, and local/global events. We then introduce
a geo-temporal embedding informativeness test as a lightweight diagnostic,
demonstrating on MovieLens, LastFM, and a production dataset that these
embeddings provide predictive signal consistent with the outcomes of full model
integrations. Geo-temporal embeddings are incorporated into sequential models
through (1) direct feature fusion with metadata embeddings or (2) an auxiliary
loss that enforces semantic and geo-temporal alignment. Our findings highlight
the need for adaptive or hybrid recommendation strategies, and we release a
context-enriched MovieLens dataset to support future research.

</details>


### [110] [MiniOneRec: An Open-Source Framework for Scaling Generative Recommendation](https://arxiv.org/abs/2510.24431)
*Xiaoyu Kong,Leheng Sheng,Junfei Tan,Yuxin Chen,Jiancan Wu,An Zhang,Xiang Wang,Xiangnan He*

Main category: cs.IR

TL;DR: 本研究提出了 MiniOneRec，这是一个完全开源的生成式推荐框架，用于研究大型语言模型在推荐系统中的扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统的推荐系统受限于嵌入维度扩展，而新兴的生成式方法使用紧凑的语义ID序列。但工业部署通常是私有的，扩展规律和最小后训练方案尚不明确。

Method: 通过残差量化VAE生成SIDs，并在Amazon Review数据集上对0.5B到7B参数的Qwen主干网络进行后训练。提出了轻量级的后训练流程，包括SID对齐和强化学习。

Result: 实验表明，随着模型规模的增加，训练和评估损失持续下降，验证了生成方法的参数效率。所提出的后训练技术显著提高了排序准确性和候选多样性。

Conclusion: MiniOneRec 验证了生成式推荐方法的扩展性，并提供了一个有效的后训练流程，为开源社区提供了有价值的资源。

Abstract: The recent success of large language models (LLMs) has renewed interest in
whether recommender systems can achieve similar scaling benefits. Conventional
recommenders, dominated by massive embedding tables, tend to plateau as
embedding dimensions grow. In contrast, the emerging generative paradigm
replaces embeddings with compact Semantic ID (SID) sequences produced by
autoregressive Transformers. Yet most industrial deployments remain
proprietary, leaving two fundamental questions open: (1) Do the expected
scaling laws hold on public benchmarks? (2) What is the minimal post-training
recipe that enables competitive performance?
  We present MiniOneRec, to the best of our knowledge, the first fully
open-source generative recommendation framework, which provides an end-to-end
workflow spanning SID construction, supervised fine-tuning, and
recommendation-oriented reinforcement learning. We generate SIDs via a Residual
Quantized VAE and post-train Qwen backbones ranging from 0.5B to 7B parameters
on the Amazon Review dataset. Our experiments reveal a consistent downward
trend in both training and evaluation losses with increasing model size,
validating the parameter efficiency of the generative approach. To further
enhance performance, we propose a lightweight yet effective post-training
pipeline that (1) enforces full-process SID alignment and (2) applies
reinforcement learning with constrained decoding and hybrid rewards. Together,
these techniques yield significant improvements in both ranking accuracy and
candidate diversity.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [111] [An Enhanced Dual Transformer Contrastive Network for Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.23617)
*Phuong Q. Dao,Mark Roantree,Vuong M. Ngo*

Main category: cs.LG

TL;DR: 本文提出了一种用于多模态情感分析的新模型，该模型结合了 BERT 和 ViT，并通过对比学习进一步提高了性能。在两个基准数据集上的实验结果表明，该方法是有效的。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析旨在通过联合分析来自多个模态的数据来理解人类的情感，与单模态方法相比，它可以提供更丰富和更准确的解释。

Method: 本文首先提出了 BERT-ViT-EF，这是一种新颖的模型，它通过早期融合策略结合了强大的基于 Transformer 的编码器 BERT（用于文本输入）和 ViT（用于视觉输入）。为了进一步增强模型的能力，本文提出了一种名为双重 Transformer 对比网络 (DTCN) 的扩展，它建立在 BERT-ViT-EF 的基础上。DTCN 在 BERT 之后加入了一个额外的 Transformer 编码器层，以改进文本上下文（在融合之前），并采用对比学习来对齐文本和图像表示，从而促进稳健的多模态特征学习。

Result: 在两个广泛使用的 MSA 基准 MVSA-Single 和 TumEmo 上的经验结果证明了本文方法的有效性。DTCN 在 TumEmo 上实现了最佳的准确率 (78.4%) 和 F1 分数 (78.3%)，并在 MVSA-Single 上提供了具有竞争力的性能，准确率为 76.6%，F1 分数为 75.9%。

Conclusion: 这些改进突出了基于 Transformer 的多模态情感分析中早期融合和更深层次的上下文建模的优势。

Abstract: Multimodal Sentiment Analysis (MSA) seeks to understand human emotions by
jointly analyzing data from multiple modalities typically text and images
offering a richer and more accurate interpretation than unimodal approaches. In
this paper, we first propose BERT-ViT-EF, a novel model that combines powerful
Transformer-based encoders BERT for textual input and ViT for visual input
through an early fusion strategy. This approach facilitates deeper cross-modal
interactions and more effective joint representation learning. To further
enhance the model's capability, we propose an extension called the Dual
Transformer Contrastive Network (DTCN), which builds upon BERT-ViT-EF. DTCN
incorporates an additional Transformer encoder layer after BERT to refine
textual context (before fusion) and employs contrastive learning to align text
and image representations, fostering robust multimodal feature learning.
Empirical results on two widely used MSA benchmarks MVSA-Single and TumEmo
demonstrate the effectiveness of our approach. DTCN achieves best accuracy
(78.4%) and F1-score (78.3%) on TumEmo, and delivers competitive performance on
MVSA-Single, with 76.6% accuracy and 75.9% F1-score. These improvements
highlight the benefits of early fusion and deeper contextual modeling in
Transformer-based multimodal sentiment analysis.

</details>


### [112] [Speeding Up MACE: Low-Precision Tricks for Equivarient Force Fields](https://arxiv.org/abs/2510.23621)
*Alexandre Benoit*

Main category: cs.LG

TL;DR: 本研究旨在通过识别计算瓶颈和评估低精度执行策略，在保持精度的同时，降低 MACE 的计算成本并提高其速度。通过分析 MACE 的端到端和每个模块，比较 e3nn 和 NVIDIA cuEquivariance 后端，并评估 FP64/FP32/BF16/FP16 设置，用于推理、NVT 和 NPT 水模拟以及玩具训练。


<details>
  <summary>Details</summary>
Motivation: SO(3)-等变模型（如 MACE）虽然能提供准确的分子动力学（MD），但计算成本高昂。目前缺乏系统性的证据表明，降低精度算法和 GPU 优化内核是否可以在不损害物理保真度的情况下降低这种成本。

Method: 通过对 MACE 进行性能分析，比较不同的后端和精度设置，并在可重现的稳态计时下进行推理、NVT 和 NPT 水模拟以及玩具训练。

Result: cuEquivariance 将推理延迟降低了约 3 倍。在 FP32 模型中仅将线性层转换为 BF16/FP16 可额外加速约 4 倍，而 NVT/NPT MD 中的能量和热力学可观测量保持在运行间变异性范围内。训练期间的半精度权重会降低力的 RMSE。在没有显式适配器的情况下混合 e3nn 和 cuEq 模块会导致表示不匹配。

Conclusion: 融合的等变内核和混合精度推理可以显着加速最先进的力场，而对下游 MD 的影响可忽略不计。一种实用的策略是默认使用带有 FP32 的 cuEquivariance，并为线性层启用 BF16/FP16（保持 FP32 累积）以实现最大吞吐量，而训练仍保持在 FP32 中。预计在 Ampere/Hopper GPU（TF32/BF16）上以及来自内核级 FP16/BF16 路径和流水线融合将获得更多收益。

Abstract: Machine-learning force fields can deliver accurate molecular dynamics (MD) at
high computational cost. For SO(3)-equivariant models such as MACE, there is
little systematic evidence on whether reduced-precision arithmetic and
GPU-optimized kernels can cut this cost without harming physical fidelity. This
thesis aims to make MACE cheaper and faster while preserving accuracy by
identifying computational bottlenecks and evaluating low-precision execution
policies. We profile MACE end-to-end and per block, compare the e3nn and NVIDIA
cuEquivariance backends, and assess FP64/FP32/BF16/FP16 settings (with FP32
accumulation) for inference, short NVT and long NPT water simulations, and toy
training runs under reproducible, steady-state timing. cuEquivariance reduces
inference latency by about $3\times$. Casting only linear layers to BF16/FP16
within an FP32 model yields roughly 4x additional speedups, while energies and
thermodynamic observables in NVT/NPT MD remain within run-to-run variability.
Half-precision weights during training degrade force RMSE. Mixing e3nn and cuEq
modules without explicit adapters causes representation mismatches. Fused
equivariant kernels and mixed-precision inference can substantially accelerate
state-of-the-art force fields with negligible impact on downstream MD. A
practical policy is to use cuEquivariance with FP32 by default and enable
BF16/FP16 for linear layers (keeping FP32 accumulations) for maximum
throughput, while training remains in FP32. Further gains are expected on
Ampere/Hopper GPUs (TF32/BF16) and from kernel-level FP16/BF16 paths and
pipeline fusion.

</details>


### [113] [Adversarially-Aware Architecture Design for Robust Medical AI Systems](https://arxiv.org/abs/2510.23622)
*Alyssa Gerhart,Balaji Iyangar*

Main category: cs.LG

TL;DR: 对抗性攻击对医疗保健中的人工智能系统构成严重风险，可能导致模型产生危险的错误分类。


<details>
  <summary>Details</summary>
Motivation: 研究对抗性攻击对医疗保健人工智能系统的威胁，尤其是在服务不足的人群中。

Method: 通过在皮肤病学数据集上进行实验，采用详细的威胁建模、实验基准测试和模型评估。

Result: 对抗性攻击显著降低了分类准确率，防御措施（如对抗训练和知识蒸馏）取得了一定的成功，但需要在防御效果和模型在干净数据上的性能之间取得平衡。

Conclusion: 需要采取综合的技术、伦理和政策方法，以在医疗保健领域构建更具弹性、更公平的AI。

Abstract: Adversarial attacks pose a severe risk to AI systems used in healthcare,
capable of misleading models into dangerous misclassifications that can delay
treatments or cause misdiagnoses. These attacks, often imperceptible to human
perception, threaten patient safety, particularly in underserved populations.
Our study explores these vulnerabilities through empirical experimentation on a
dermatological dataset, where adversarial methods significantly reduce
classification accuracy. Through detailed threat modeling, experimental
benchmarking, and model evaluation, we demonstrate both the severity of the
threat and the partial success of defenses like adversarial training and
distillation. Our results show that while defenses reduce attack success rates,
they must be balanced against model performance on clean data. We conclude with
a call for integrated technical, ethical, and policy-based approaches to build
more resilient, equitable AI in healthcare.

</details>


### [114] [DiNo and RanBu: Lightweight Predictions from Shallow Random Forests](https://arxiv.org/abs/2510.23624)
*Tiago Mendonça dos Santos,Rafael Izbicki,Luís Gustavo Esteves*

Main category: cs.LG

TL;DR: 提出了两种浅层森林方法DiNo和RanBu，将少量深度有限的树转换为有效的距离加权预测器。


<details>
  <summary>Details</summary>
Motivation: 随机森林集是表格预测任务的强大基线，但它们对数百棵深度树的依赖通常导致高推理延迟和内存需求，限制了在延迟敏感或资源受限的环境中的部署。

Method: DiNo通过观察对的最近公共祖先来测量谱系距离，而RanBu将核平滑应用于Breiman的经典邻近度量。

Result: RanBu在三个合成基准和25个公共数据集中匹配或超过了全深度随机森林的准确性，尤其是在高噪声环境中，同时将训练加推理时间减少了高达95%。DiNo在低噪声状态下以适度的计算成本实现了最佳的偏差-方差权衡。

Conclusion: 这两种方法都可以直接扩展到分位数回归，并在保持准确性的同时大幅提高速度。

Abstract: Random Forest ensembles are a strong baseline for tabular prediction tasks,
but their reliance on hundreds of deep trees often results in high inference
latency and memory demands, limiting deployment in latency-sensitive or
resource-constrained environments. We introduce DiNo (Distance with Nodes) and
RanBu (Random Bushes), two shallow-forest methods that convert a small set of
depth-limited trees into efficient, distance-weighted predictors. DiNo measures
cophenetic distances via the most recent common ancestor of observation pairs,
while RanBu applies kernel smoothing to Breiman's classical proximity measure.
Both approaches operate entirely after forest training: no additional trees are
grown, and tuning of the single bandwidth parameter $h$ requires only
lightweight matrix-vector operations. Across three synthetic benchmarks and 25
public datasets, RanBu matches or exceeds the accuracy of full-depth random
forests-particularly in high-noise settings-while reducing training plus
inference time by up to 95\%. DiNo achieves the best bias-variance trade-off in
low-noise regimes at a modest computational cost. Both methods extend directly
to quantile regression, maintaining accuracy with substantial speed gains. The
implementation is available as an open-source R/C++ package at
https://github.com/tiagomendonca/dirf. We focus on structured tabular random
samples (i.i.d.), leaving extensions to other modalities for future work.

</details>


### [115] [From Detection to Discovery: A Closed-Loop Approach for Simultaneous and Continuous Medical Knowledge Expansion and Depression Detection on Social Media](https://arxiv.org/abs/2510.23626)
*Shuang Geng,Wenli Zhang,Jiaheng Xie,Rui Wang,Sudha Ram*

Main category: cs.LG

TL;DR: 提出了一种闭环大语言模型-知识图谱框架，用于抑郁症检测，通过迭代学习循环整合预测和知识扩展。


<details>
  <summary>Details</summary>
Motivation: 利用社交媒体用户生成内容预测心理健康状况，并扩展医学知识。

Method: 构建闭环框架，LLM进行抑郁症检测和实体抽取，知识图谱表示和加权实体，专家监督下将新知识加入知识图谱。

Result: 提高了预测准确性，发现了临床上有意义的症状、合并症和社会诱因。

Conclusion: 验证了预测式学习和学习式预测的相互促进，为自适应数据驱动知识系统提供了基础。

Abstract: Social media user-generated content (UGC) provides real-time, self-reported
indicators of mental health conditions such as depression, offering a valuable
source for predictive analytics. While prior studies integrate medical
knowledge to improve prediction accuracy, they overlook the opportunity to
simultaneously expand such knowledge through predictive processes. We develop a
Closed-Loop Large Language Model (LLM)-Knowledge Graph framework that
integrates prediction and knowledge expansion in an iterative learning cycle.
In the knowledge-aware depression detection phase, the LLM jointly performs
depression detection and entity extraction, while the knowledge graph
represents and weights these entities to refine prediction performance. In the
knowledge refinement and expansion phase, new entities, relationships, and
entity types extracted by the LLM are incorporated into the knowledge graph
under expert supervision, enabling continual knowledge evolution. Using
large-scale UGC, the framework enhances both predictive accuracy and medical
understanding. Expert evaluations confirmed the discovery of clinically
meaningful symptoms, comorbidities, and social triggers complementary to
existing literature. We conceptualize and operationalize
prediction-through-learning and learning-through-prediction as mutually
reinforcing processes, advancing both methodological and theoretical
understanding in predictive analytics. The framework demonstrates the
co-evolution of computational models and domain knowledge, offering a
foundation for adaptive, data-driven knowledge systems applicable to other
dynamic risk monitoring contexts.

</details>


### [116] [Chain of Execution Supervision Promotes General Reasoning in Large Language Models](https://arxiv.org/abs/2510.23629)
*Nuo Chen,Zehua Li,Keqin Bao,Junyang Lin,Dayiheng Liu*

Main category: cs.LG

TL;DR: 论文提出了TracePile，一个大规模的代码执行轨迹数据集，用于提升大型语言模型（LLMs）的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的代码训练数据包含隐式推理，并且受到语法和实现噪声的干扰，直接训练效果不佳。

Method: 论文构建了一个包含260万样本的TracePile数据集，将代码执行转化为显式的、逐步的思维链式推理（Chain of Execution，CoE）。数据集涵盖数学、经典算法和算法竞赛等领域，并通过变量追踪问题和代码重写来增强逻辑粒度和代码多样性。

Result: 在四个基础模型（LLaMA 3, LLaMA 3.1, Qwen-2.5, and Qwen-2.5 Coder）和20个基准测试（涵盖数学、代码、逻辑和算法）上的实验表明，TracePile带来了持续的改进。例如，在九个数学数据集上，TracePile使LLaMA3.1-8B的性能平均提高了7.1%。

Conclusion: TracePile能够有效提升LLMs在数学、代码、逻辑和算法等方面的推理能力。

Abstract: Building robust and general reasoning ability is a central goal in the
development of large language models (LLMs). Recent efforts increasingly turn
to code as a rich training source, given its inherent logical structure and
diverse reasoning paradigms such as divide-and-conquer, topological ordering,
and enumeration. However, reasoning in code is often expressed implicitly and
entangled with syntactic or implementation noise, making direct training on raw
code suboptimal.To address this, we introduce TracePile, a large-scale corpus
of 2.6 million samples that transforms code execution into explicit,
step-by-step chain-of-thought-style rationales, which we call Chain of
Execution (CoE). The corpus spans domains including mathematics, classical
algorithms and algorithmic competition, and is enriched with variable-tracing
questions and code rewritings to enhance logical granularity and code
diversity. We evaluate TracePile using three training setups:
continue-pretraining, instruction tuning after pretraining, and two-stage
finetuning. Experiments across four base models (LLaMA 3, LLaMA 3.1, Qwen-2.5,
and Qwen-2.5 Coder) and 20 benchmarks covering math, code, logic, and
algorithms demonstrate consistent improvements. Notably, TracePile boosts
LLaMA3.1-8B by 7.1\% on average across nine math datasets and delivers clear
gains on LiveCodeBench, CRUX, and MMLU under two-stage fine-tuning.

</details>


### [117] [NUM2EVENT: Interpretable Event Reasoning from Numerical time-series](https://arxiv.org/abs/2510.23630)
*Ninghui Feng,Yiyan Qi*

Main category: cs.LG

TL;DR: 本文提出了一种新的数值到事件的推理和解码任务，旨在从数值输入中推断出可解释的结构化事件。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要集中在预测或趋势描述，而没有揭示驱动数值变化的潜在事件或解释其背后的推理过程。

Method: 该文提出了一个 reasoning-aware 框架，该框架集成了 agent-guided event extractor (AGE)、marked multivariate Hawkes-based synthetic generator (EveDTS) 和一个两阶段微调 pipeline，该 pipeline 将时间序列编码器与结构化解码器相结合。

Result: 在多领域数据集上的实验表明，该方法在事件级别的精确率和召回率方面均优于强大的 LLM 基线。

Conclusion: 这些结果为弥合定量推理和语义理解之间的差距提供了一个新的方向，使 LLM 能够直接从数值动态中解释和预测事件。

Abstract: Large language models (LLMs) have recently demonstrated impressive multimodal
reasoning capabilities, yet their understanding of purely numerical time-series
signals remains limited. Existing approaches mainly focus on forecasting or
trend description, without uncovering the latent events that drive numerical
changes or explaining the reasoning process behind them. In this work, we
introduce the task of number-to-event reasoning and decoding, which aims to
infer interpretable structured events from numerical inputs, even when current
text is unavailable. To address the data scarcity and semantic alignment
challenges, we propose a reasoning-aware framework that integrates an
agent-guided event extractor (AGE), a marked multivariate Hawkes-based
synthetic generator (EveDTS), and a two-stage fine-tuning pipeline combining a
time-series encoder with a structured decoder. Our model explicitly reasons
over numerical changes, generates intermediate explanations, and outputs
structured event hypotheses. Experiments on multi-domain datasets show that our
method substantially outperforms strong LLM baselines in event-level precision
and recall. These results suggest a new direction for bridging quantitative
reasoning and semantic understanding, enabling LLMs to explain and predict
events directly from numerical dynamics.

</details>


### [118] [Beyond Pairwise: Empowering LLM Alignment With Ranked Choice Modeling](https://arxiv.org/abs/2510.23631)
*Yuxuan Tang,Yifan Feng*

Main category: cs.LG

TL;DR: 提出了一种名为排序选择偏好优化 (RCPO) 的统一框架，该框架通过最大似然估计将偏好优化与（排序）选择建模联系起来。


<details>
  <summary>Details</summary>
Motivation: 以往的大型语言模型 (LLM) 对齐主要依赖于成对偏好优化，这种方法忽略了从更丰富的人类反馈形式（例如多项比较和 top-k 排名）中学习的机会。

Method: 提出了排序选择偏好优化 (RCPO)，这是一个统一的框架，它通过最大似然估计将偏好优化与（排序）选择建模联系起来。该框架灵活，支持基于效用和基于排名的选择模型。它包含了几种现有的成对方法（例如，DPO、SimPO），同时为更丰富的反馈格式提供了有原则的训练目标。

Result: 在 AlpacaEval 2 和 Arena-Hard 基准测试中，对 Llama-3-8B-Instruct 和 Gemma-2-9B-it 的实证研究表明，RCPO 始终优于有竞争力的基线。

Conclusion: RCPO 表明，直接利用排序偏好数据，结合正确的选择模型，可以实现更有效的对齐。它为将（排序）选择建模纳入 LLM 训练提供了通用且可扩展的基础。

Abstract: Alignment of large language models (LLMs) has predominantly relied on
pairwise preference optimization, where annotators select the better of two
responses to a prompt. While simple, this approach overlooks the opportunity to
learn from richer forms of human feedback, such as multiwise comparisons and
top-$k$ rankings. We propose Ranked Choice Preference Optimization (RCPO), a
unified framework that bridges preference optimization with (ranked) choice
modeling via maximum likelihood estimation. The framework is flexible,
supporting both utility-based and rank-based choice models. It subsumes several
existing pairwise methods (e.g., DPO, SimPO), while providing principled
training objectives for richer feedback formats. We instantiate this framework
with two representative ranked choice models (Multinomial Logit and
Mallows-RMJ). Empirical studies on Llama-3-8B-Instruct and Gemma-2-9B-it across
AlpacaEval 2 and Arena-Hard benchmarks show that RCPO consistently outperforms
competitive baselines. RCPO shows how directly leveraging ranked preference
data, combined with the right choice models, yields more effective alignment.
It offers a versatile and extensible foundation for incorporating (ranked)
choice modeling into LLM training.

</details>


### [119] [LLMComp: A Language Modeling Paradigm for Error-Bounded Scientific Data Compression](https://arxiv.org/abs/2510.23632)
*Guozhong Li,Muhannad Alhumaidi,Spiros Skiadopoulos,Panos Kalnis*

Main category: cs.LG

TL;DR: LLMCOMP利用大型语言模型进行有损压缩，在保证误差范围的前提下，压缩率比现有技术高出30%。


<details>
  <summary>Details</summary>
Motivation: 高效、有误差界限的压缩对于大规模时空数据集变得越来越重要。解码器专用大型语言模型（LLM）在建模复杂序列数据方面表现出了卓越的能力。

Method: LLMCOMP首先将3D场量化为离散token，通过Z阶曲线排列它们以保持局部性，并应用覆盖引导采样来提高训练效率。然后，利用空间-时间嵌入训练自回归transformer来建模token转换。

Result: 在多个再分析数据集上的实验表明，LLMCOMP始终优于最先进的压缩器，在严格的误差范围内，压缩率高达30%。

Conclusion: LLM作为高保真科学数据的通用压缩器具有潜力。

Abstract: The rapid growth of high-resolution scientific simulations and observation
systems is generating massive spatiotemporal datasets, making efficient,
error-bounded compression increasingly important. Meanwhile, decoder-only large
language models (LLMs) have demonstrated remarkable capabilities in modeling
complex sequential data. In this paper, we propose LLMCOMP, a novel lossy
compression paradigm that leverages decoder-only large LLMs to model scientific
data. LLMCOMP first quantizes 3D fields into discrete tokens, arranges them via
Z-order curves to preserve locality, and applies coverage-guided sampling to
enhance training efficiency. An autoregressive transformer is then trained with
spatial-temporal embeddings to model token transitions. During compression, the
model performs top-k prediction, storing only rank indices and fallback
corrections to ensure strict error bounds. Experiments on multiple reanalysis
datasets show that LLMCOMP consistently outperforms state-of-the-art
compressors, achieving up to 30% higher compression ratios under strict error
bounds. These results highlight the potential of LLMs as general-purpose
compressors for high-fidelity scientific data.

</details>


### [120] [Noise is All You Need: Solving Linear Inverse Problems by Noise Combination Sampling with Diffusion Models](https://arxiv.org/abs/2510.23633)
*Xun Su,Hiroyuki Kasai*

Main category: cs.LG

TL;DR: 提出了一种新的噪声组合采样方法，以解决预训练扩散模型在零样本逆问题求解中过度或不足整合观测信息的问题。


<details>
  <summary>Details</summary>
Motivation: 预训练扩散模型在零样本逆问题求解中面临过度整合观测信息扰乱生成过程，或整合不足无法强调逆问题约束的困境。

Method: 提出噪声组合采样方法，从噪声子空间合成最优噪声向量以近似测量得分，替换标准去噪扩散概率模型过程中的噪声项。

Result: 该方法适用于多种逆问题求解器，尤其是在生成步骤数 T 较小时，以可忽略的计算开销实现卓越性能，显著提高鲁棒性和稳定性。

Conclusion: 噪声组合采样方法能够自然地将条件信息嵌入到生成过程中，无需依赖逐步超参数调整。

Abstract: Pretrained diffusion models have demonstrated strong capabilities in
zero-shot inverse problem solving by incorporating observation information into
the generation process of the diffusion models. However, this presents an
inherent dilemma: excessive integration can disrupt the generative process,
while insufficient integration fails to emphasize the constraints imposed by
the inverse problem. To address this, we propose \emph{Noise Combination
Sampling}, a novel method that synthesizes an optimal noise vector from a noise
subspace to approximate the measurement score, replacing the noise term in the
standard Denoising Diffusion Probabilistic Models process. This enables
conditional information to be naturally embedded into the generation process
without reliance on step-wise hyperparameter tuning. Our method can be applied
to a wide range of inverse problem solvers, including image compression, and,
particularly when the number of generation steps $T$ is small, achieves
superior performance with negligible computational overhead, significantly
improving robustness and stability.

</details>


### [121] [Monotone and Separable Set Functions: Characterizations and Neural Models](https://arxiv.org/abs/2510.23634)
*Soutrik Sarangi,Yonatan Sverdlov,Nadav Dym,Abir De*

Main category: cs.LG

TL;DR: 本文研究了保持集合自然偏序的集合到向量的函数设计问题，即单调分离 (MAS) 集合函数。


<details>
  <summary>Details</summary>
Motivation: 受集合包含问题的应用驱动，研究设计集合到向量的函数，使得集合的自然偏序得以保持。

Method: 建立了获得 MAS 函数所需的向量维度的上下界，并提出了一个称为“弱 MAS”的模型，该模型具有 Holder 连续性。

Result: 对于无限 ground set 的重要情况，表明 MAS 函数不存在，但提供了一个称为 our 的模型，该模型具有弱 MAS 属性，并且在 Holder 连续性方面是稳定的。实验表明，与没有结合集合包含作为归纳偏差的标准集合模型相比，使用我们的模型具有优势。

Conclusion: MAS 函数可用于构建单调的通用模型，并且可以逼近所有单调集合函数。

Abstract: Motivated by applications for set containment problems, we consider the
following fundamental problem: can we design set-to-vector functions so that
the natural partial order on sets is preserved, namely $S\subseteq T \text{ if
and only if } F(S)\leq F(T) $. We call functions satisfying this property
Monotone and Separating (MAS) set functions. % We establish lower and upper
bounds for the vector dimension necessary to obtain MAS functions, as a
function of the cardinality of the multisets and the underlying ground set. In
the important case of an infinite ground set, we show that MAS functions do not
exist, but provide a model called our which provably enjoys a relaxed MAS
property we name "weakly MAS" and is stable in the sense of Holder continuity.
We also show that MAS functions can be used to construct universal models that
are monotone by construction and can approximate all monotone set functions.
Experimentally, we consider a variety of set containment tasks. The experiments
show the benefit of using our our model, in comparison with standard set models
which do not incorporate set containment as an inductive bias. Our code is
available in https://github.com/yonatansverdlov/Monotone-Embedding.

</details>


### [122] [Help the machine to help you: an evaluation in the wild of egocentric data cleaning via skeptical learning](https://arxiv.org/abs/2510.23635)
*Andrea Bontempelli,Matteo Busso,Leonardo Javier Malcotti,Fausto Giunchiglia*

Main category: cs.LG

TL;DR: 本研究探讨了在现实场景中使用Skeptical Learning (SKEL) 提高数字个人助理注释质量的方法。


<details>
  <summary>Details</summary>
Motivation: 数字个人助理需要高质量的注释才能正常运行，但用户注释常常包含错误和噪音。以往研究未能包含最终用户的确认。

Method: 通过让大学生在四周内使用iLog移动应用，并根据他们的视角和需求细化输入标签，来评估SKEL在现实条件下的性能。

Result: 研究结果强调了用户付出和数据质量之间找到平衡的挑战，以及使用SKEL的潜在好处，包括减少注释工作和提高收集数据的质量。

Conclusion: SKEL在减少注释工作和提高数据质量方面具有潜力，但需要在用户付出和数据质量之间找到平衡。

Abstract: Any digital personal assistant, whether used to support task performance,
answer questions, or manage work and daily life, including fitness schedules,
requires high-quality annotations to function properly. However, user
annotations, whether actively produced or inferred from context (e.g., data
from smartphone sensors), are often subject to errors and noise. Previous
research on Skeptical Learning (SKEL) addressed the issue of noisy labels by
comparing offline active annotations with passive data, allowing for an
evaluation of annotation accuracy. However, this evaluation did not include
confirmation from end-users, the best judges of their own context. In this
study, we evaluate SKEL's performance in real-world conditions with actual
users who can refine the input labels based on their current perspectives and
needs. The study involves university students using the iLog mobile application
on their devices over a period of four weeks. The results highlight the
challenges of finding the right balance between user effort and data quality,
as well as the potential benefits of using SKEL, which include reduced
annotation effort and improved quality of collected data.

</details>


### [123] [Flight Delay Prediction via Cross-Modality Adaptation of Large Language Models and Aircraft Trajectory Representation](https://arxiv.org/abs/2510.23636)
*Thaweerath Phisannupawong,Joshua Julian Damanik,Han-Lim Choi*

Main category: cs.LG

TL;DR: 本文提出了一种基于轻量级大语言模型的多模态航班延误预测方法，该方法从空管人员的角度出发，结合轨迹表示和文本航空信息，实现了亚分钟级的预测误差。


<details>
  <summary>Details</summary>
Motivation: 航班延误是影响整体网络性能的关键问题，因此航班延误预测成为空中交通管理的关键焦点。

Method: 该方法将轨迹数据转化为语言模态，以捕捉空域条件，并结合航班信息、天气报告和机场公告等文本航空信息。

Result: 实验结果表明，该模型能够有效地利用与延误来源相关的上下文信息，始终实现亚分钟级的预测误差。

Conclusion: 该框架证明了语言理解与轨迹信息的跨模态适应相结合，可以提高延误预测的准确性。此外，该方法还显示出在实际操作中的实用性和可扩展性，支持实时更新，从而在收到新的操作信息后改进预测。

Abstract: Flight delay prediction has become a key focus in air traffic management, as
delays highlight inefficiencies that impact overall network performance. This
paper presents a lightweight large language model-based multimodal flight delay
prediction, formulated from the perspective of air traffic controllers
monitoring aircraft delay after entering the terminal area. The approach
integrates trajectory representations with textual aeronautical information,
including flight information, weather reports, and aerodrome notices, by
adapting trajectory data into the language modality to capture airspace
conditions. Experimental results show that the model consistently achieves
sub-minute prediction error by effectively leveraging contextual information
related to the sources of delay. The framework demonstrates that linguistic
understanding, when combined with cross-modality adaptation of trajectory
information, enhances delay prediction. Moreover, the approach shows
practicality and scalability for real-world operations, supporting real-time
updates that refine predictions upon receiving new operational information.

</details>


### [124] [Combining Textual and Structural Information for Premise Selection in Lean](https://arxiv.org/abs/2510.23637)
*Job Petrovčič,David Eliecer Narvaez Denis,Ljupčo Todorovski*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于图的定理证明前提选择方法，该方法结合了Lean形式化的密集文本嵌入和异构依赖图上的图神经网络。


<details>
  <summary>Details</summary>
Motivation: 现有的基于语言的方法通常孤立地处理前提，忽略了连接它们的依赖关系网络，这阻碍了大型形式库中定理证明的扩展。

Method: 该方法结合了Lean形式化的密集文本嵌入和异构依赖图上的图神经网络，该图捕获了状态-前提和前提-前提关系。

Result: 在LeanDojo基准测试中，该方法在标准检索指标上优于ReProver语言基线超过25%。

Conclusion: 这些结果证明了关系信息对于更有效的前提选择的力量。

Abstract: Premise selection is a key bottleneck for scaling theorem proving in large
formal libraries. Yet existing language-based methods often treat premises in
isolation, ignoring the web of dependencies that connects them. We present a
graph-augmented approach that combines dense text embeddings of Lean
formalizations with graph neural networks over a heterogeneous dependency graph
capturing both state--premise and premise--premise relations. On the LeanDojo
Benchmark, our method outperforms the ReProver language-based baseline by over
25% across standard retrieval metrics. These results demonstrate the power of
relational information for more effective premise selection.

</details>


### [125] [Integrating Genomics into Multimodal EHR Foundation Models](https://arxiv.org/abs/2510.23639)
*Jonathan Amar,Edward Liu,Alessandra Breschi,Liangliang Zhang,Pouya Kheradpour,Sylvia Li,Lisa Soleymani Lehmann,Alessandro Giulianelli,Matt Edwards,Yugang Jia,David Nola,Raghav Mani,Pankaj Vats,Jesse Tetreault,T. J. Chen,Cory Y. McLean*

Main category: cs.LG

TL;DR: 本文介绍了一种创新的电子健康记录 (EHR) 基础模型，该模型集成了多基因风险评分 (PRS) 作为基础数据模式，超越了传统的仅 EHR 方法，以构建更全面的健康概况。


<details>
  <summary>Details</summary>
Motivation: 利用“All of Us” (AoU) 研究计划中的广泛且多样化的数据，此多模式框架旨在了解临床数据和遗传易感性之间的复杂关系。

Method: 该方法将生成式人工智能的进步扩展到 EHR 基础模型空间，从而增强了预测能力和可解释性。

Result: 在 AoU 数据上的评估表明，该模型对各种疾病（尤其是 2 型糖尿病 (T2D)）的发病具有预测价值，并说明了 PRS 和 EHR 数据之间的相互作用。这项工作还探索了用于自定义分类任务的迁移学习，展示了该架构的多功能性和效率。

Conclusion: 这种方法对于释放对疾病预测、主动健康管理、风险分层和个性化治疗策略的新见解至关重要，为医疗保健中更个性化、公平和可操作的真实世界证据生成奠定了基础。

Abstract: This paper introduces an innovative Electronic Health Record (EHR) foundation
model that integrates Polygenic Risk Scores (PRS) as a foundational data
modality, moving beyond traditional EHR-only approaches to build more holistic
health profiles. Leveraging the extensive and diverse data from the All of Us
(AoU) Research Program, this multimodal framework aims to learn complex
relationships between clinical data and genetic predispositions. The
methodology extends advancements in generative AI to the EHR foundation model
space, enhancing predictive capabilities and interpretability. Evaluation on
AoU data demonstrates the model's predictive value for the onset of various
conditions, particularly Type 2 Diabetes (T2D), and illustrates the interplay
between PRS and EHR data. The work also explores transfer learning for custom
classification tasks, showcasing the architecture's versatility and efficiency.
This approach is pivotal for unlocking new insights into disease prediction,
proactive health management, risk stratification, and personalized treatment
strategies, laying the groundwork for more personalized, equitable, and
actionable real-world evidence generation in healthcare.

</details>


### [126] [Structure-Aware Fusion with Progressive Injection for Multimodal Molecular Representation Learning](https://arxiv.org/abs/2510.23640)
*Zihao Jing,Yan Sun,Yan Yi Li,Sugitha Janarthanan,Alana Deng,Pingzhao Hu*

Main category: cs.LG

TL;DR: MuMo: A structured multimodal fusion framework addresses challenges in molecular representation through two key strategies: Structured Fusion Pipeline (SFP) and Progressive Injection (PI).


<details>
  <summary>Details</summary>
Motivation: Multimodal molecular models often suffer from 3D conformer unreliability and modality collapse, limiting their robustness and generalization.

Method: Combines 2D topology and 3D geometry into a unified and stable structural prior, and introduces a Progressive Injection (PI) mechanism that asymmetrically integrates this prior into the sequence stream.

Result: Achieves an average improvement of 2.7% over the best-performing baseline on each task, ranking first on 22 of them, including a 27% improvement on the LD50 task.

Conclusion: MuMo's robustness to 3D conformer noise and the effectiveness of multimodal fusion in molecular representation are validated.

Abstract: Multimodal molecular models often suffer from 3D conformer unreliability and
modality collapse, limiting their robustness and generalization. We propose
MuMo, a structured multimodal fusion framework that addresses these challenges
in molecular representation through two key strategies. To reduce the
instability of conformer-dependent fusion, we design a Structured Fusion
Pipeline (SFP) that combines 2D topology and 3D geometry into a unified and
stable structural prior. To mitigate modality collapse caused by naive fusion,
we introduce a Progressive Injection (PI) mechanism that asymmetrically
integrates this prior into the sequence stream, preserving modality-specific
modeling while enabling cross-modal enrichment. Built on a state space
backbone, MuMo supports long-range dependency modeling and robust information
propagation. Across 29 benchmark tasks from Therapeutics Data Commons (TDC) and
MoleculeNet, MuMo achieves an average improvement of 2.7% over the
best-performing baseline on each task, ranking first on 22 of them, including a
27% improvement on the LD50 task. These results validate its robustness to 3D
conformer noise and the effectiveness of multimodal fusion in molecular
representation. The code is available at: github.com/selmiss/MuMo.

</details>


### [127] [Spatially Aware Linear Transformer (SAL-T) for Particle Jet Tagging](https://arxiv.org/abs/2510.23641)
*Aaron Wang,Zihan Zhao,Subash Katel,Vivekanand Gyanchand Sahu,Elham E Khoda,Abhijith Gandrakota,Jennifer Ngadiuba,Richard Cavanaugh,Javier Duarte*

Main category: cs.LG

TL;DR: 提出了Spatially Aware Linear Transformer (SAL-T)，这是一种linformer架构的物理启发式增强，它保持了线性注意力。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在捕获高能粒子碰撞中的全局和局部相关性方面非常有效，但它们在CERN LHC等高数据吞吐量环境中提出了部署挑战。Transformer模型的二次复杂度需要大量资源，并增加推理期间的延迟。为了解决这些问题。

Method: 该方法结合了基于运动学特征的粒子空间感知分区，从而计算物理意义区域之间的注意力。此外，我们利用卷积层来捕获局部相关性，这是由来自射流物理的见解所提供的。

Result: 在射流分类任务中，SAL-T优于标准linformer，并且还实现了与全注意力transformer相当的分类结果，同时在推理期间使用更少的资源和更低的延迟。在通用点云分类数据集（ModelNet10）上的实验进一步证实了这一趋势。

Conclusion: SAL-T在资源使用和推理延迟方面，实现了与全注意力transformer相当的分类结果

Abstract: Transformers are very effective in capturing both global and local
correlations within high-energy particle collisions, but they present
deployment challenges in high-data-throughput environments, such as the CERN
LHC. The quadratic complexity of transformer models demands substantial
resources and increases latency during inference. In order to address these
issues, we introduce the Spatially Aware Linear Transformer (SAL-T), a
physics-inspired enhancement of the linformer architecture that maintains
linear attention. Our method incorporates spatially aware partitioning of
particles based on kinematic features, thereby computing attention between
regions of physical significance. Additionally, we employ convolutional layers
to capture local correlations, informed by insights from jet physics. In
addition to outperforming the standard linformer in jet classification tasks,
SAL-T also achieves classification results comparable to full-attention
transformers, while using considerably fewer resources with lower latency
during inference. Experiments on a generic point cloud classification dataset
(ModelNet10) further confirm this trend. Our code is available at
https://github.com/aaronw5/SAL-T4HEP.

</details>


### [128] [Efficient Low Rank Attention for Long-Context Inference in Large Language Models](https://arxiv.org/abs/2510.23649)
*Tenghui Li,Guoxu Zhou,Xuyang Zhao,Yuning Qiu,Qibin Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种名为低秩查询和键注意（LRQK）的两阶段框架，以解决大型语言模型（LLM）中因输入文本长度增加而导致的 KV 缓存 GPU 内存成本过高的问题。该框架在预填充阶段将全精度查询和键矩阵分解为紧凑的秩-r 因子，然后在每个解码步骤中使用这些低维投影来计算代理注意力分数。通过选择前 k 个 token 和一小组固定的最近 token，LRQK 采用混合 GPU-CPU 缓存，仅传输丢失的全精度 KV 对，从而在减少 CPU-GPU 数据移动的同时保持精确的注意力输出。


<details>
  <summary>Details</summary>
Motivation: LLM 中 KV 缓存的 GPU 内存成本随着输入文本长度的增加而变得过高，限制了资源受限设备上的长上下文推理。现有的 KV 量化和剪枝方法虽然降低了内存使用量，但会遭受数值精度损失或次优的键值对保留。

Method: 本文提出的 LRQK 框架包含两个阶段：1) 在预填充阶段，将全精度查询和键矩阵分解为紧凑的秩-r 因子；2) 在解码阶段，使用这些低维投影以 O(lr) 的时间复杂度计算代理注意力分数。此外，LRQK 采用混合 GPU-CPU 缓存，仅传输丢失的全精度 KV 对。

Result: 在 RULER 和 LongBench 基准测试中，使用 LLaMA-3-8B 和 Qwen2.5-7B 进行的广泛实验表明，LRQK 在长上下文设置中与领先的稀疏注意力方法相匹配或超过了它们，同时在最小的精度损失下实现了显着的内存节省。

Conclusion: LRQK 是一种有效的长上下文 LLM 推理框架，它通过低秩分解和混合 GPU-CPU 缓存，在降低内存成本的同时保持了较高的精度。

Abstract: As the length of input text grows, the key-value (KV) cache in LLMs imposes
prohibitive GPU memory costs and limits long-context inference on resource
constrained devices. Existing approaches, such as KV quantization and pruning,
reduce memory usage but suffer from numerical precision loss or suboptimal
retention of key-value pairs. We introduce Low Rank Query and Key attention
(LRQK), a two-stage framework that jointly decomposes the full-precision query
and key matrices into compact rank-\(r\) factors during the prefill stage, and
then uses these low-dimensional projections to compute proxy attention scores
in \(\mathcal{O}(lr)\) time at each decode step. By selecting only the
top-\(k\) tokens and a small fixed set of recent tokens, LRQK employs a mixed
GPU-CPU cache with a hit-and-miss mechanism that transfers only missing
full-precision KV pairs, thereby preserving exact attention outputs while
reducing CPU-GPU data movement. Extensive experiments on the RULER and
LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK
matches or surpasses leading sparse-attention methods in long context settings,
while delivering significant memory savings with minimal loss in accuracy. Our
code is available at https://github.com/tenghuilee/LRQK.

</details>


### [129] [Beyond Hidden-Layer Manipulation: Semantically-Aware Logit Interventions for Debiasing LLMs](https://arxiv.org/abs/2510.23650)
*Wei Xia*

Main category: cs.LG

TL;DR: 提出了两种zero-shot logits-layer debiasing方法，分别是静态和动态方法。


<details>
  <summary>Details</summary>
Motivation: 减少偏差

Method: Logits干预

Result: 动态方法减少了高达70%的偏差，且流畅度损失最小。Logits干预优于隐藏层方法。

Conclusion: 语义感知的Logits干预对于debiasing aligned LLMs是稳定且有效的。

Abstract: We proposed Static and Dynamic -- two zero-shot logits-layer debiasing
methods. Dynamic reduces bias by up to 70% with minimal fluency loss. Logits
intervention outperforms hidden-layer approaches. We show semantic-aware logits
intervention is stable and effective for debiasing aligned LLMs.

</details>


### [130] [The Structural Scalpel: Automated Contiguous Layer Pruning for Large Language Models](https://arxiv.org/abs/2510.23652)
*Yao Lu,Yuqi Li,Wenbin Xie,Shanqing Yu,Qi Xuan,Zhaowei Zhu,Shiping Wen*

Main category: cs.LG

TL;DR: 提出了一种新的连续层剪枝框架（CLP），用于压缩大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有层剪枝方法忽略了层之间的依赖关系，导致模型性能下降。

Method: 提出了可微的凹门算法和截止端点调整策略。

Result: 在多个模型架构和尺寸上，CLP 显著优于现有方法。例如，在 20% 的剪枝率下，CLP 在 LLaMA3-70B 上实现了 95.34% 的平均性能保持率。

Conclusion: CLP 能够有效压缩模型，且能与量化技术结合使用。

Abstract: Although large language models (LLMs) have achieved revolutionary
breakthroughs in many fields, their large model size and high computational
cost pose significant challenges for practical deployment on
resource-constrained edge devices. To this end, layer pruning has been proposed
to reduce the computational overhead by directly removing redundant layers.
However, existing layer pruning methods typically rely on hand-crafted metrics
to evaluate and remove individual layers, while ignoring the dependencies
between layers. This can disrupt the model's information flow and severely
degrade performance. To address these issues, we propose CLP, a novel
continuous layer pruning framework that introduces two key innovations: a
differentiable concave gate algorithm that automatically identifies the best
continuous layer segments for pruning via gradient-based optimization; and a
cutoff endpoint tuning strategy that effectively restores model performance by
fine-tuning only the layers adjacent to the pruned segments. Extensive
experiments across multiple model architectures (including LLaMA2, LLaMA3 and
Qwen) and sizes (from $7$B to $70$B parameters) show that CLP significantly
outperforms existing state-of-the-art baselines. For example, at a pruning rate
of $20\%$, CLP achieves an average performance retention of $95.34\%$ on
LLaMA3-70B, outperforming baselines by $4.29\%$-$30.52\%$. Furthermore, CLP can
be seamlessly combined with quantization to further compress the model with
only a slight performance loss.

</details>


### [131] [Error Adjustment Based on Spatiotemporal Correlation Fusion for Traffic Forecasting](https://arxiv.org/abs/2510.23656)
*Fuqiang Liu,Weiping Ding,Luis Miranda-Moreno,Lijun Sun*

Main category: cs.LG

TL;DR: 提出了一种用于调整交通预测中时空自相关预测误差的通用框架SAEA。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽略了交通数据的时间性和空间性引起的自相关性，这限制了基于DNN的预测模型的性能。

Method: 将预测误差建模为时空向量自回归(VAR)过程，通过系数矩阵显式地捕捉空间和时间误差相关性，并引入结构稀疏正则化以结合先验空间信息。

Result: 在不同的交通数据集上验证了该方法的有效性，结果表明该方法在几乎所有情况下都能提高性能。

Conclusion: SAEA框架能够有效地调整交通预测中的自相关预测误差，从而提高预测模型的性能。

Abstract: Deep neural networks (DNNs) play a significant role in an increasing body of
research on traffic forecasting due to their effectively capturing
spatiotemporal patterns embedded in traffic data. A general assumption of
training the said forecasting models via mean squared error estimation is that
the errors across time steps and spatial positions are uncorrelated. However,
this assumption does not really hold because of the autocorrelation caused by
both the temporality and spatiality of traffic data. This gap limits the
performance of DNN-based forecasting models and is overlooked by current
studies. To fill up this gap, this paper proposes Spatiotemporally
Autocorrelated Error Adjustment (SAEA), a novel and general framework designed
to systematically adjust autocorrelated prediction errors in traffic
forecasting. Unlike existing approaches that assume prediction errors follow a
random Gaussian noise distribution, SAEA models these errors as a
spatiotemporal vector autoregressive (VAR) process to capture their intrinsic
dependencies. First, it explicitly captures both spatial and temporal error
correlations by a coefficient matrix, which is then embedded into a newly
formulated cost function. Second, a structurally sparse regularization is
introduced to incorporate prior spatial information, ensuring that the learned
coefficient matrix aligns with the inherent road network structure. Finally, an
inference process with test-time error adjustment is designed to dynamically
refine predictions, mitigating the impact of autocorrelated errors in real-time
forecasting. The effectiveness of the proposed approach is verified on
different traffic datasets. Results across a wide range of traffic forecasting
models show that our method enhances performance in almost all cases.

</details>


### [132] [A machine learning framework integrating seed traits and plasma parameters for predicting germination uplift in crops](https://arxiv.org/abs/2510.23657)
*Saklain Niam,Tashfiqur Rahman,Md. Amjad Patwary,Mukarram Hossain*

Main category: cs.LG

TL;DR: This study uses machine learning to predict the effect of cold plasma on seed germination.


<details>
  <summary>Details</summary>
Motivation: Predicting the outcomes of cold plasma treatment on seed germination is difficult due to complex interactions.

Method: Machine learning models (GB, XGB, ET, and hybrids) were used to forecast germination uplift in soybean, barley, sunflower, radish, and tomato under dielectric barrier discharge (DBD) plasma. Feature reduction and engineering analysis were also performed.

Result: Extra Trees (ET) performed best (R{2} = 0.919; RMSE = 3.21; MAE = 2.62), improving to R{2} = 0.925 after feature reduction. Germination rate maximized at $\geq$100 W with low exposure time. Radish and soybean were modeled with high consistency, while sunflower remained slightly higher variable. Williams and Sari were well predicted, while Arian and Ny\'{\i}rs\'{e}gi fekete were comparatively poorly captured.

Conclusion: This framework was also embedded into MLflow, providing a decision-support tool for optimizing CP seed germination in precision agriculture.

Abstract: Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet
outcomes remain difficult to predict due to complex seed--plasma--environment
interactions. This study introduces the first machine learning framework to
forecast germination uplift in soybean, barley, sunflower, radish, and tomato
under dielectric barrier discharge (DBD) plasma. Among the models tested (GB,
XGB, ET, and hybrids), Extra Trees (ET) performed best (R\textsuperscript{2} =
0.919; RMSE = 3.21; MAE = 2.62), improving to R\textsuperscript{2} = 0.925
after feature reduction. Engineering analysis revealed a hormetic response:
negligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for
200--500 s, and reduced germination beyond 20 kV or prolonged exposures.
Discharge power was also a dominant factor, with germination rate maximizing at
$\geq$100 W with low exposure time. Species and cultivar-level predictions
showed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high
consistency, while sunflower remained slightly higher variable (MAE = 3.80).
Among cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,
while Arian (2.86) and Ny\'{\i}rs\'{e}gi fekete (3.74) were comparatively
poorly captured. This framework was also embedded into MLflow, providing a
decision-support tool for optimizing CP seed germination in precision
agriculture.

</details>


### [133] [Aligning Diffusion Language Models via Unpaired Preference Optimization](https://arxiv.org/abs/2510.23658)
*Vaibhav Jindal,Hejian Sang,Chun-Mao Lai,Yanning Chen,Zhipeng Wang*

Main category: cs.LG

TL;DR: 提出了一种新的对齐扩散语言模型（dLLMs）与人类偏好的方法，称为ELBO-KTO，它结合了ELBO替代和前景理论的非配对偏好目标。


<details>
  <summary>Details</summary>
Motivation: 由于序列对数似然难以处理且成对偏好数据收集成本高昂，因此将扩散语言模型与人类偏好对齐具有挑战性。

Method: 该方法结合了ELBO替代的扩散对数似然和前景理论的非配对偏好目标（KTO）。同时，分析了ELBO替代引起的偏差和方差，并采用方差减少实践来稳定训练期间的梯度。

Result: 在LLaDA-8B-Instruct上应用ELBO-KTO，在kto-mix-14k和UltraFeedback-Binary上分别实现了65.9%和62.3%的调整胜率，与基本模型相比，该模型在自动LLM判断下表现更好。在下游任务中，包括GSM8K、MMLU和额外的推理/知识基准测试中，在UltraFeedback-Binary上训练的ELBO-KTO与基本模型在相同的解码下表现相当或更好。

Conclusion: 非配对偏好优化是扩散LLM中成对对齐的可行替代方案。

Abstract: Diffusion language models (dLLMs) are an emerging alternative to
autoregressive (AR) generators, but aligning them to human preferences is
challenging because sequence log-likelihoods are intractable and pairwise
preference data are costly to collect. We introduce ELBO-KTO, which combines an
ELBO surrogate for diffusion log-likelihoods with a prospect-theoretic,
unpaired preference objective (Kahneman Tversky Optimization, KTO). We analyze
the bias and variance induced by the ELBO substitution and employ
variance-reduction practices that stabilize gradients during training. Applied
to LLaDA-8B-Instruct, ELBO-KTO yields \textbf{65.9\%} and \textbf{62.3\%}
adjusted win rates on kto-mix-14k and UltraFeedback-Binary, respectively,
versus the base model under an automatic LLM judge. Across downstream tasks,
including GSM8K, MMLU, and additional reasoning/knowledge benchmarks, ELBO-KTO
trained on UltraFeedback-Binary performs on par with or better than the base
model under identical decoding. This establishes unpaired preference
optimization as a viable alternative to pairwise alignment in diffusion LLMs.

</details>


### [134] [Quantum Machine Learning for Image Classification: A Hybrid Model of Residual Network with Quantum Support Vector Machine](https://arxiv.org/abs/2510.23659)
*Md. Farhan Shahriyar,Gazi Tanbhir,Abdullah Md Raihan Chy*

Main category: cs.LG

TL;DR: 本文提出了一种混合量子-经典方法，利用 ResNet-50 提取特征，然后使用 QSVM 进行马铃薯病害检测分类。


<details>
  <summary>Details</summary>
Motivation: 经典机器学习模型在高维数据集上表现不佳，需要量子计算来提高分类效率。

Method: 使用 ResNet-50 提取图像特征，PCA 降维，然后使用 QSVM 模型，采用 ZZ, Z 和 Pauli-X 等量子特征映射。

Result: 基于 Z 特征映射的 QSVM 优于经典模型，准确率达到 99.23%。

Conclusion: 该研究突出了将量子计算集成到图像分类中的优势，并为通过混合量子-经典建模提供了一种潜在的疾病检测解决方案。

Abstract: Recently, there has been growing attention on combining quantum machine
learning (QML) with classical deep learning approaches, as computational
techniques are key to improving the performance of image classification tasks.
This study presents a hybrid approach that uses ResNet-50 (Residual Network)
for feature extraction and Quantum Support Vector Machines (QSVM) for
classification in the context of potato disease detection. Classical machine
learning as well as deep learning models often struggle with high-dimensional
and complex datasets, necessitating advanced techniques like quantum computing
to improve classification efficiency. In our research, we use ResNet-50 to
extract deep feature representations from RGB images of potato diseases. These
features are then subjected to dimensionality reduction using Principal
Component Analysis (PCA). The resulting features are processed through QSVM
models which apply various quantum feature maps such as ZZ, Z, and Pauli-X to
transform classical data into quantum states. To assess the model performance,
we compared it with classical machine learning algorithms such as Support
Vector Machine (SVM) and Random Forest (RF) using five-fold stratified
cross-validation for comprehensive evaluation. The experimental results
demonstrate that the Z-feature map-based QSVM outperforms classical models,
achieving an accuracy of 99.23 percent, surpassing both SVM and RF models. This
research highlights the advantages of integrating quantum computing into image
classification and provides a potential disease detection solution through
hybrid quantum-classical modeling.

</details>


### [135] [Quanvolutional Neural Networks for Pneumonia Detection: An Efficient Quantum-Assisted Feature Extraction Paradigm](https://arxiv.org/abs/2510.23660)
*Gazi Tanbhir,Md. Farhan Shahriyar,Abdullah Md Raihan Chy*

Main category: cs.LG

TL;DR: 本文提出了一种用于肺炎检测的混合量子-经典模型，该模型利用量子卷积层提取特征，然后使用经典神经网络进行分类。


<details>
  <summary>Details</summary>
Motivation: 传统的卷积神经网络在肺炎检测中面临计算成本高、特征表示能力有限以及泛化能力差等问题，尤其是在小数据集上。

Method: 该方法使用带有参数化量子电路的量子卷积层处理图像块，并使用旋转 Y 门进行数据编码和纠缠层以生成非经典特征表示。然后，将这些量子提取的特征输入到经典神经网络进行分类。

Result: 实验结果表明，所提出的 QNN 实现了 83.33% 的验证准确率，而同等的经典 CNN 的验证准确率为 73.33%。

Conclusion: 该研究奠定了将量子计算集成到深度学习驱动的医疗诊断系统的基础，为传统方法提供了一种计算效率高的替代方案。

Abstract: Pneumonia poses a significant global health challenge, demanding accurate and
timely diagnosis. While deep learning, particularly Convolutional Neural
Networks (CNNs), has shown promise in medical image analysis for pneumonia
detection, CNNs often suffer from high computational costs, limitations in
feature representation, and challenges in generalizing from smaller datasets.
To address these limitations, we explore the application of Quanvolutional
Neural Networks (QNNs), leveraging quantum computing for enhanced feature
extraction. This paper introduces a novel hybrid quantum-classical model for
pneumonia detection using the PneumoniaMNIST dataset. Our approach utilizes a
quanvolutional layer with a parameterized quantum circuit (PQC) to process 2x2
image patches, employing rotational Y-gates for data encoding and entangling
layers to generate non-classical feature representations. These
quantum-extracted features are then fed into a classical neural network for
classification. Experimental results demonstrate that the proposed QNN achieves
a higher validation accuracy of 83.33 percent compared to a comparable
classical CNN which achieves 73.33 percent. This enhanced convergence and
sample efficiency highlight the potential of QNNs for medical image analysis,
particularly in scenarios with limited labeled data. This research lays the
foundation for integrating quantum computing into deep-learning-driven medical
diagnostic systems, offering a computationally efficient alternative to
traditional approaches.

</details>


### [136] [AI-Driven Carbon Monitoring: Transformer-Based Reconstruction of Atmospheric CO2 in Canadian Poultry Regions](https://arxiv.org/abs/2510.23663)
*Padmanabhan Jagannathan Prajesh,Kaliaperumal Ragunath,Miriam Gordon,Bruce Rathgeber,Suresh Neethirajan*

Main category: cs.LG

TL;DR: 本文提出了一个名为ST-ViWT的框架，用于重建加拿南部地区的连续XCO2场，该框架结合了小波变换和Transformer注意力机制，并利用OCO-2数据和其他辅助数据。结果表明，该模型具有很高的精度和泛化能力，能够用于碳排放核算和减排评估。


<details>
  <summary>Details</summary>
Motivation: 精确绘制农业景观中的柱平均CO2 (XCO2) 图对于指导减排策略至关重要，尤其是在家禽养殖密集区域。

Method: 提出了一种时空视觉Transformer与小波变换(ST-ViWT)框架，该框架将小波时频表示与Transformer注意力机制相结合，并利用气象、植被指数、地形和土地覆盖等数据。

Result: 在2024年OCO-2数据上，ST-ViWT达到了R2 = 0.984和RMSE = 0.468 ppm；92.3%的填补预测值在+/-1 ppm范围内。与TCCON的独立验证显示出强大的泛化能力（偏差= -0.14 ppm；r = 0.928）。空间分析表明，家禽设施密度与XCO2之间存在中等程度的正相关关系（r = 0.43）。

Conclusion: 基于Transformer的地球观测技术能够实现可扩展、透明、空间显式的碳核算、热点优先级排序和与政策相关的减排评估。

Abstract: Accurate mapping of column-averaged CO2 (XCO2) over agricultural landscapes
is essential for guiding emission mitigation strategies. We present a
Spatiotemporal Vision Transformer with Wavelets (ST-ViWT) framework that
reconstructs continuous, uncertainty-quantified XCO2 fields from OCO-2 across
southern Canada, emphasizing poultry-intensive regions. The model fuses wavelet
time-frequency representations with transformer attention over meteorology,
vegetation indices, topography, and land cover. On 2024 OCO-2 data, ST-ViWT
attains R2 = 0.984 and RMSE = 0.468 ppm; 92.3 percent of gap-filled predictions
lie within +/-1 ppm. Independent validation with TCCON shows robust
generalization (bias = -0.14 ppm; r = 0.928), including faithful reproduction
of the late-summer drawdown. Spatial analysis across 14 poultry regions reveals
a moderate positive association between facility density and XCO2 (r = 0.43);
high-density areas exhibit larger seasonal amplitudes (9.57 ppm) and enhanced
summer variability. Compared with conventional interpolation and standard
machine-learning baselines, ST-ViWT yields seamless 0.25 degree CO2 surfaces
with explicit uncertainties, enabling year-round coverage despite sparse
observations. The approach supports integration of satellite constraints with
national inventories and precision livestock platforms to benchmark emissions,
refine region-specific factors, and verify interventions. Importantly,
transformer-based Earth observation enables scalable, transparent, spatially
explicit carbon accounting, hotspot prioritization, and policy-relevant
mitigation assessment.

</details>


### [137] [Transformers from Compressed Representations](https://arxiv.org/abs/2510.23665)
*Juan C. Leon Alcazar,Mattia Soldan,Mohammad Saatialsoruji,Alejandro Pardo,Hani Itani,Juan Camilo Perez,Bernard Ghanem*

Main category: cs.LG

TL;DR: 提出了一种名为TEMPEST的方法，该方法直接从压缩数据流中学习语义表示，无需原始字节级处理或完整媒体解码。


<details>
  <summary>Details</summary>
Motivation: 压缩文件格式是高效数据存储和传输的基石，但它们在表征学习方面的潜力在很大程度上未被探索。

Method: 利用压缩文件的固有字节流结构来设计有效的标记化和编码策略。

Result: TEMPEST在各种数据集、编码方案和模态上实现了与最先进技术竞争的精度，同时提高了内存和计算效率。

Conclusion: TEMPEST通过减少语义分类所需的token数量，降低了计算复杂性和内存使用率。

Abstract: Compressed file formats are the corner stone of efficient data storage and
transmission, yet their potential for representation learning remains largely
underexplored. We introduce TEMPEST (TransformErs froM comPressed
rEpreSenTations), a method that exploits the inherent byte-stream structure of
compressed files to design an effective tokenization and encoding strategy. By
leveraging this compact encoding, a standard transformer can directly learn
semantic representations from compressed data streams, bypassing the need for
raw byte-level processing or full media decoding. Our proposal substantially
reduces the number of tokens required for semantic classification, thereby
lowering both computational complexity and memory usage. Through extensive
experiments across diverse datasets, coding schemes, and modalities, we show
that TEMPEST achieves accuracy competitive wit the state-of-the-art while
delivering efficiency gains in memory and compute.

</details>


### [138] [Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free Structural Topology Optimization](https://arxiv.org/abs/2510.23667)
*Amin Heyrani Nobari,Lyle Regenwetter,Cyril Picard,Ligong Han,Faez Ahmed*

Main category: cs.LG

TL;DR: 提出了一种名为OAT的拓扑优化框架，可以快速预测各种长宽比、分辨率、体积分数、载荷和夹具下的最小柔度布局。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法在结构拓扑优化中存在局限性，例如只能处理固定方形网格和少数手动编码的边界条件，并且需要进行后处理优化，这限制了其通用性。

Method: OAT结合了与分辨率和形状无关的自编码器、隐式神经场解码器以及条件潜在扩散模型，并在包含220万个优化结构的OpenTO数据集上进行训练。

Result: 在多个基准测试中，OAT相对于现有最佳模型，平均柔度降低高达90%，并且在单个GPU上可以在1秒内完成从64x64到256x256分辨率以及高达10:1长宽比的推理。

Conclusion: OAT是一个通用的、快速的、与分辨率无关的拓扑优化框架，并提供了一个大规模数据集，以促进逆向设计的生成建模的进一步研究。

Abstract: Structural topology optimization (TO) is central to engineering design but
remains computationally intensive due to complex physics and hard constraints.
Existing deep-learning methods are limited to fixed square grids, a few
hand-coded boundary conditions, and post-hoc optimization, preventing general
deployment. We introduce Optimize Any Topology (OAT), a foundation-model
framework that directly predicts minimum-compliance layouts for arbitrary
aspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines
a resolution- and shape-agnostic autoencoder with an implicit neural-field
decoder and a conditional latent-diffusion model trained on OpenTO, a new
corpus of 2.2 million optimized structures covering 2 million unique
boundary-condition configurations. On four public benchmarks and two
challenging unseen tests, OAT lowers mean compliance up to 90% relative to the
best prior models and delivers sub-1 second inference on a single GPU across
resolutions from 64 x 64 to 256 x 256 and aspect ratios as high as 10:1. These
results establish OAT as a general, fast, and resolution-free framework for
physics-aware topology optimization and provide a large-scale dataset to spur
further research in generative modeling for inverse design. Code & data can be
found at https://github.com/ahnobari/OptimizeAnyTopology.

</details>


### [139] [Traffic flow forecasting, STL decomposition, Hybrid model, LSTM, ARIMA, XGBoost, Intelligent transportation systems](https://arxiv.org/abs/2510.23668)
*Fujiang Yuan,Yangrui Fan,Xiaohuan Bing,Zhen Tian,Chunhong Yuan,Yankang Li*

Main category: cs.LG

TL;DR: 本文提出了一种分解驱动的混合框架，该框架集成了使用 Loess (STL) 进行的季节性趋势分解与三种互补预测模型。


<details>
  <summary>Details</summary>
Motivation: 精确的交通流量预测对于智能交通系统和城市交通管理至关重要。然而，单一模型方法通常无法捕获交通流量数据中复杂的、非线性的和多尺度的时序模式。

Method: STL 首先将原始时间序列分解为趋势、季节性和残差分量。然后，长短期记忆 (LSTM) 网络对长期趋势进行建模，自回归综合移动平均 (ARIMA) 模型捕获季节性周期性，而极端梯度提升 (XGBoost) 算法预测非线性残差波动。最终预测是通过子模型预测的乘法积分获得的。

Result: 使用 2015 年 11 月至 12 月纽约市交叉路口的 998 条交通流量记录，结果表明 LSTM ARIMA XGBoost 混合模型在 MAE、RMSE 和 R 平方指标方面显着优于包括 LSTM、ARIMA 和 XGBoost 在内的独立模型。

Conclusion: 分解策略有效地隔离了时间特征，使每个模型都可以专门化，从而提高了预测准确性、可解释性和鲁棒性。

Abstract: Accurate traffic flow forecasting is essential for intelligent transportation
systems and urban traffic management. However, single model approaches often
fail to capture the complex, nonlinear, and multi scale temporal patterns in
traffic flow data. This study proposes a decomposition driven hybrid framework
that integrates Seasonal Trend decomposition using Loess (STL) with three
complementary predictive models. STL first decomposes the original time series
into trend, seasonal, and residual components. Then, a Long Short Term Memory
(LSTM) network models long term trends, an Autoregressive Integrated Moving
Average (ARIMA) model captures seasonal periodicity, and an Extreme Gradient
Boosting (XGBoost) algorithm predicts nonlinear residual fluctuations. The
final forecast is obtained through multiplicative integration of the sub model
predictions. Using 998 traffic flow records from a New York City intersection
between November and December 2015, results show that the LSTM ARIMA XGBoost
hybrid model significantly outperforms standalone models including LSTM, ARIMA,
and XGBoost across MAE, RMSE, and R squared metrics. The decomposition strategy
effectively isolates temporal characteristics, allowing each model to
specialize, thereby improving prediction accuracy, interpretability, and
robustness.

</details>


### [140] [Sparsity and Superposition in Mixture of Experts](https://arxiv.org/abs/2510.23671)
*Marmik Chaudhari,Jeremi Nuer,Rome Thorstenson*

Main category: cs.LG

TL;DR: 本文研究了MoE模型与稠密模型在机制上的差异，发现MoE模型的网络稀疏性与单义性相关，并提出了基于单义特征表示的专家专业化定义。


<details>
  <summary>Details</summary>
Motivation: 理解MoE模型与稠密网络在机制上的差异，现有研究未能解释MoE模型。

Method: 通过测量专家之间的叠加来开发新的指标，并分析网络稀疏性、特征稀疏性和特征重要性。

Result: 发现网络稀疏性更高的模型表现出更高的单义性，专家围绕连贯的特征组合进行组织。

Conclusion: MoE中的网络稀疏性可能在不牺牲性能的情况下实现更可解释的模型，挑战了可解释性和能力根本上相互对立的常见假设。

Abstract: Mixture of Experts (MoE) models have become central to scaling large language
models, yet their mechanistic differences from dense networks remain poorly
understood. Previous work has explored how dense models use
\textit{superposition} to represent more features than dimensions, and how
superposition is a function of feature sparsity and feature importance. MoE
models cannot be explained mechanistically through the same lens. We find that
neither feature sparsity nor feature importance cause discontinuous phase
changes, and that network sparsity (the ratio of active to total experts)
better characterizes MoEs. We develop new metrics for measuring superposition
across experts. Our findings demonstrate that models with greater network
sparsity exhibit greater \emph{monosemanticity}. We propose a new definition of
expert specialization based on monosemantic feature representation rather than
load balancing, showing that experts naturally organize around coherent feature
combinations when initialized appropriately. These results suggest that network
sparsity in MoEs may enable more interpretable models without sacrificing
performance, challenging the common assumption that interpretability and
capability are fundamentally at odds.

</details>


### [141] [DBLoss: Decomposition-based Loss Function for Time Series Forecasting](https://arxiv.org/abs/2510.23672)
*Xiangfei Qiu,Xingjian Wu,Hanyin Cheng,Xvyuan Liu,Chenjuan Guo,Jilin Hu,Bin Yang*

Main category: cs.LG

TL;DR: 提出了一种新的时间序列预测损失函数DBLoss，它可以分解时间序列并分别计算季节性和趋势分量的损失。


<details>
  <summary>Details</summary>
Motivation: 现有的MSE损失函数有时无法准确捕捉预测范围内的季节性或趋势。

Method: 使用指数移动平均法将时间序列分解为季节性和趋势分量，然后分别计算每个分量的损失，并对其进行加权。

Result: DBLoss 显著提高了各种真实世界数据集中最先进模型的性能。

Conclusion: DBLoss 为时间序列损失函数的设计提供了一个新的视角。

Abstract: Time series forecasting holds significant value in various domains such as
economics, traffic, energy, and AIOps, as accurate predictions facilitate
informed decision-making. However, the existing Mean Squared Error (MSE) loss
function sometimes fails to accurately capture the seasonality or trend within
the forecasting horizon, even when decomposition modules are used in the
forward propagation to model the trend and seasonality separately. To address
these challenges, we propose a simple yet effective Decomposition-Based Loss
function called DBLoss. This method uses exponential moving averages to
decompose the time series into seasonal and trend components within the
forecasting horizon, and then calculates the loss for each of these components
separately, followed by weighting them. As a general loss function, DBLoss can
be combined with any deep learning forecasting model. Extensive experiments
demonstrate that DBLoss significantly improves the performance of
state-of-the-art models across diverse real-world datasets and provides a new
perspective on the design of time series loss functions.

</details>


### [142] [Informed Initialization for Bayesian Optimization and Active Learning](https://arxiv.org/abs/2510.23681)
*Carl Hvarfner,David Eriksson,Eytan Bakshy,Max Balandat*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为HIPE的新型采集策略，用于在少量样本的贝叶斯优化中平衡预测不确定性降低和超参数学习。


<details>
  <summary>Details</summary>
Motivation: 在少量样本的贝叶斯优化中，初始化对代理模型的预测质量至关重要，但现有方法忽略了空间填充设计可能并非最佳以及高效的超参数学习的重要性。

Method: 该论文提出了一种名为超参数知情的预测探索（HIPE）的采集策略，该策略利用信息论原则平衡预测不确定性降低和超参数学习，并推导了高斯过程设置中HIPE的闭式表达式。

Result: 实验结果表明，HIPE在预测精度、超参数识别和后续优化性能方面优于标准初始化策略，尤其是在与许多实际贝叶斯优化应用相关的大批量、少量样本设置中。

Conclusion: HIPE是一种有效的初始化策略，可以在少量样本的贝叶斯优化中提高预测精度和优化性能。

Abstract: Bayesian Optimization is a widely used method for optimizing expensive
black-box functions, relying on probabilistic surrogate models such as Gaussian
Processes. The quality of the surrogate model is crucial for good optimization
performance, especially in the few-shot setting where only a small number of
batches of points can be evaluated. In this setting, the initialization plays a
critical role in shaping the surrogate's predictive quality and guiding
subsequent optimization. Despite this, practitioners typically rely on
(quasi-)random designs to cover the input space. However, such approaches
neglect two key factors: (a) space-filling designs may not be desirable to
reduce predictive uncertainty, and (b) efficient hyperparameter learning during
initialization is essential for high-quality prediction, which may conflict
with space-filling designs. To address these limitations, we propose
Hyperparameter-Informed Predictive Exploration (HIPE), a novel acquisition
strategy that balances predictive uncertainty reduction with hyperparameter
learning using information-theoretic principles. We derive a closed-form
expression for HIPE in the Gaussian Process setting and demonstrate its
effectiveness through extensive experiments in active learning and few-shot BO.
Our results show that HIPE outperforms standard initialization strategies in
terms of predictive accuracy, hyperparameter identification, and subsequent
optimization performance, particularly in large-batch, few-shot settings
relevant to many real-world Bayesian Optimization applications.

</details>


### [143] [Beyond Prompt Engineering: Neuro-Symbolic-Causal Architecture for Robust Multi-Objective AI Agents](https://arxiv.org/abs/2510.23682)
*Gokturk Aytug Akarlar*

Main category: cs.LG

TL;DR: LLM agents are risky in important situations. They can fail based on prompt wording. The authors made a new architecture, Chimera, that combines an LLM, symbolic constraints, and causal inference.


<details>
  <summary>Details</summary>
Motivation: LLM agents can be unreliable in important situations.

Method: The authors created Chimera, a neuro-symbolic-causal architecture that integrates three components: an LLM strategist, a formally verified symbolic constraint engine, and a causal inference module for counterfactual reasoning. They compared Chimera against baseline architectures (LLM-only, LLM with symbolic constraints) across 52-week simulations in a realistic e-commerce environment.

Result: Chimera consistently delivers the highest returns and improves brand trust, demonstrating prompt-agnostic robustness. TLA+ formal verification proves zero constraint violations across all scenarios.

Conclusion: Architectural design, not prompt engineering, determines the reliability of autonomous agents in production environments.

Abstract: Large language models show promise as autonomous decision-making agents, yet
their deployment in high-stakes domains remains fraught with risk. Without
architectural safeguards, LLM agents exhibit catastrophic brittleness:
identical capabilities produce wildly different outcomes depending solely on
prompt framing. We present Chimera, a neuro-symbolic-causal architecture that
integrates three complementary components - an LLM strategist, a formally
verified symbolic constraint engine, and a causal inference module for
counterfactual reasoning. We benchmark Chimera against baseline architectures
(LLM-only, LLM with symbolic constraints) across 52-week simulations in a
realistic e-commerce environment featuring price elasticity, trust dynamics,
and seasonal demand. Under organizational biases toward either volume or margin
optimization, LLM-only agents fail catastrophically (total loss of \$99K in
volume scenarios) or destroy brand trust (-48.6% in margin scenarios). Adding
symbolic constraints prevents disasters but achieves only 43-87% of Chimera's
profit. Chimera consistently delivers the highest returns (\$1.52M and \$1.96M
respectively, some cases +\$2.2M) while improving brand trust (+1.8% and
+10.8%, some cases +20.86%), demonstrating prompt-agnostic robustness. Our TLA+
formal verification proves zero constraint violations across all scenarios.
These results establish that architectural design not prompt engineering
determines the reliability of autonomous agents in production environments. We
provide open-source implementations and interactive demonstrations for
reproducibility.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [144] [MSRANetV2: An Explainable Deep Learning Architecture for Multi-class Classification of Colorectal Histopathological Images](https://arxiv.org/abs/2510.24136)
*Ovi Sarkar,Md Shafiuzzaman,Md. Faysal Ahamed,Golam Mahmud,Muhammad E. H. Chowdhury*

Main category: eess.IV

TL;DR: 这篇论文提出了一种名为MSRANetV2的卷积神经网络结构，用于结直肠组织图像分类，并在两个公共数据集上取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的结直肠癌诊断方法存在主观性、耗时和易变性等问题，因此需要更精确和高效的诊断方法。

Method: 该模型采用ResNet50V2骨干网络，并扩展了残差注意力机制和squeeze-and-excitation (SE)块，以提取深层语义和细粒度的空间特征。通过通道对齐和上采样操作，MSRANetV2有效地融合了多尺度表示。

Result: 在CRC-VAL-HE-7K和NCT-CRC-HE-100K两个公共数据集上，该模型取得了出色的平均精确率、召回率、F1分数、AUC和测试准确率。

Conclusion: MSRANetV2是一种可靠、可解释且高性能的结直肠癌组织分类模型。

Abstract: Colorectal cancer (CRC) is a leading worldwide cause of cancer-related
mortality, and the role of prompt precise detection is of paramount interest in
improving patient outcomes. Conventional diagnostic methods such as colonoscopy
and histological examination routinely exhibit subjectivity, are extremely
time-consuming, and are susceptible to variation. Through the development of
digital pathology, deep learning algorithms have become a powerful approach in
enhancing diagnostic precision and efficiency. In our work, we proposed a
convolutional neural network architecture named MSRANetV2, specially optimized
for the classification of colorectal tissue images. The model employs a
ResNet50V2 backbone, extended with residual attention mechanisms and
squeeze-and-excitation (SE) blocks, to extract deep semantic and fine-grained
spatial features. With channel alignment and upsampling operations, MSRANetV2
effectively fuses multi-scale representations, thereby enhancing the robustness
of the classification. We evaluated our model on a five-fold stratified
cross-validation strategy on two publicly available datasets: CRC-VAL-HE-7K and
NCT-CRC-HE-100K. The proposed model achieved remarkable average Precision,
recall, F1-score, AUC, and test accuracy were 0.9884 plus-minus 0.0151, 0.9900
plus-minus 0.0151, 0.9900 plus-minus 0.0145, 0.9999 plus-minus 0.00006, and
0.9905 plus-minus 0.0025 on the 7K dataset. On the 100K dataset, they were
0.9904 plus-minus 0.0091, 0.9900 plus-minus 0.0071, 0.9900 plus-minus 0.0071,
0.9997 plus-minus 0.00016, and 0.9902 plus-minus 0.0006. Additionally, Grad-CAM
visualizations were incorporated to enhance model interpretability by
highlighting tissue areas that are medically relevant. These findings validate
that MSRANetV2 is a reliable, interpretable, and high-performing architectural
model for classifying CRC tissues.

</details>
