{"id": "2510.08863", "categories": ["cs.DB", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.08863", "abs": "https://arxiv.org/abs/2510.08863", "authors": ["Deep Bodra", "Sushil Khairnar"], "title": "Comparative Performance Analysis of Modern NoSQL Data Technologies: Redis, Aerospike, and Dragonfly", "comment": "NoSQL databases, performance benchmarking, cloud computing, Redis;\n  Aerospike, Dragonfly", "summary": "The rise of distributed applications and cloud computing has created a demand\nfor scalable, high-performance key-value storage systems. This paper presents a\nperformance evaluation of three prominent NoSQL key-value stores: Redis,\nAerospike, and Dragonfly, using the Yahoo! Cloud Serving Benchmark (YCSB)\nframework. We conducted extensive experiments across three distinct workload\npatterns (read-heavy, write-heavy), and balanced while systematically varying\nclient concurrency from 1 to 32 clients. Our evaluation methodology captures\nboth latency, throughput, and memory characteristics under realistic\noperational conditions, providing insights into the performance trade-offs and\nscalability behaviour of each system", "AI": {"tldr": "\u672c\u6587\u5bf9\u4e09\u79cdNoSQL\u952e\u503c\u5b58\u50a8\u7cfb\u7edf\uff08Redis\u3001Aerospike\u548cDragonfly\uff09\u8fdb\u884c\u4e86\u6027\u80fd\u8bc4\u4f30\u3002", "motivation": "\u968f\u7740\u5206\u5e03\u5f0f\u5e94\u7528\u548c\u4e91\u8ba1\u7b97\u7684\u5174\u8d77\uff0c\u5bf9\u53ef\u6269\u5c55\u3001\u9ad8\u6027\u80fd\u7684\u952e\u503c\u5b58\u50a8\u7cfb\u7edf\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u957f\u3002", "method": "\u4f7f\u7528Yahoo! Cloud Serving Benchmark (YCSB) \u6846\u67b6\uff0c\u5728\u4e09\u79cd\u4e0d\u540c\u7684\u5de5\u4f5c\u8d1f\u8f7d\u6a21\u5f0f\uff08\u8bfb\u5bc6\u96c6\u578b\u3001\u5199\u5bc6\u96c6\u578b\u548c\u5e73\u8861\u578b\uff09\u4e0b\uff0c\u7cfb\u7edf\u5730\u6539\u53d8\u5ba2\u6237\u7aef\u5e76\u53d1\u6570\uff08\u4ece 1 \u5230 32 \u4e2a\u5ba2\u6237\u7aef\uff09\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\u3002", "result": "\u8bc4\u4f30\u65b9\u6cd5\u5728\u5b9e\u9645\u64cd\u4f5c\u6761\u4ef6\u4e0b\u6355\u83b7\u4e86\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\u548c\u5185\u5b58\u7279\u6027\uff0c\u4ece\u800c\u6df1\u5165\u4e86\u89e3\u4e86\u6bcf\u4e2a\u7cfb\u7edf\u7684\u6027\u80fd\u6743\u8861\u548c\u53ef\u6269\u5c55\u6027\u884c\u4e3a\u3002", "conclusion": "\u6839\u636e\u6458\u8981\uff0c\u8bba\u6587\u5bf9\u4e09\u79cdNoSQL\u6570\u636e\u5e93\u505a\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2510.08896", "categories": ["cs.DB", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08896", "abs": "https://arxiv.org/abs/2510.08896", "authors": ["Suming Qiu", "Jing Li", "Zhicheng Zhou", "Junjie Huang", "Linyuan Qiu", "Zhijie Sun"], "title": "HES-SQL: Hybrid Reasoning for Efficient Text-to-SQL with Structural Skeleton Guidance", "comment": null, "summary": "We present HES-SQL, a novel hybrid training framework that advances\nText-to-SQL generation through the integration of thinking-mode-fused\nsupervised fine-tuning (SFT) with Group Relative Policy Optimization (GRPO).\nOur approach introduces three key innovations: (1) a skeleton-completeness\nscoring mechanism that enhances preference alignment between generated queries\nand optimal SQL structures; (2) a query-latency-aware reward system that\nincentivizes the generation of computationally efficient SQL queries; (3) a\nself-distillation process for thinking-mode completion that prevents\ndegradation of the model's reasoning capabilities. This framework enables\nhybrid thinking models to switch between reasoning and non-reasoning modes\nwhile improving SQL query accuracy and execution efficiency.\n  Experimental evaluation, conducted on MySQL 8.0 and SQLite 3.42 under\ncontrolled single-user conditions, demonstrates that HES-SQL achieves\ncompetitive performance with execution accuracies of 79.14\\% and 54.9\\% on the\nBIRD and KaggleDBQA benchmarks, respectively. Query latency is measured as the\nend-to-end execution time of generated queries on the DBMS, averaged over\nmultiple runs to mitigate variance. Efficiency gains range from 11\\% to 20\\%\nrelative to supervised baselines. Our results establish a new paradigm for\nText-to-SQL systems that effectively balances semantic accuracy with\ncomputational efficiency through execution-informed reinforcement learning\n(RL). The proposed methodology has significant implications for developing\nrobust natural language interfaces to databases and can be extended to broader\nstructured generation tasks requiring both correctness and efficiency\noptimization.", "AI": {"tldr": "HES-SQL: A hybrid training framework combining supervised fine-tuning and reinforcement learning for Text-to-SQL generation.", "motivation": "To improve SQL query accuracy and efficiency by balancing semantic accuracy with computational efficiency.", "method": "Integrating thinking-mode-fused supervised fine-tuning with Group Relative Policy Optimization, featuring skeleton-completeness scoring, query-latency-aware rewards, and self-distillation for thinking-mode completion.", "result": "Achieves competitive performance on BIRD and KaggleDBQA benchmarks with execution accuracies of 79.14% and 54.9%, respectively, and efficiency gains of 11% to 20% relative to supervised baselines.", "conclusion": "HES-SQL establishes a new paradigm for Text-to-SQL systems, balancing semantic accuracy and computational efficiency, with implications for natural language interfaces to databases and broader structured generation tasks."}}
{"id": "2510.08747", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.08747", "abs": "https://arxiv.org/abs/2510.08747", "authors": ["Yihao Ang", "Peicheng Yao", "Yifan Bao", "Yushuo Feng", "Qiang Huang", "Anthony K. H. Tung", "Zhiyong Huang"], "title": "RFOD: Random Forest-based Outlier Detection for Tabular Data", "comment": "13 pages, 13 figures, and 4 tables", "summary": "Outlier detection in tabular data is crucial for safeguarding data integrity\nin high-stakes domains such as cybersecurity, financial fraud detection, and\nhealthcare, where anomalies can cause serious operational and economic impacts.\nDespite advances in both data mining and deep learning, many existing methods\nstruggle with mixed-type tabular data, often relying on encoding schemes that\nlose important semantic information. Moreover, they frequently lack\ninterpretability, offering little insight into which specific values cause\nanomalies. To overcome these challenges, we introduce \\textsf{\\textbf{RFOD}}, a\nnovel \\textsf{\\textbf{R}}andom \\textsf{\\textbf{F}}orest-based\n\\textsf{\\textbf{O}}utlier \\textsf{\\textbf{D}}etection framework tailored for\ntabular data. Rather than modeling a global joint distribution, \\textsf{RFOD}\nreframes anomaly detection as a feature-wise conditional reconstruction\nproblem, training dedicated random forests for each feature conditioned on the\nothers. This design robustly handles heterogeneous data types while preserving\nthe semantic integrity of categorical features. To further enable precise and\ninterpretable detection, \\textsf{RFOD} combines Adjusted Gower's Distance (AGD)\nfor cell-level scoring, which adapts to skewed numerical data and accounts for\ncategorical confidence, with Uncertainty-Weighted Averaging (UWA) to aggregate\ncell-level scores into robust row-level anomaly scores. Extensive experiments\non 15 real-world datasets demonstrate that \\textsf{RFOD} consistently\noutperforms state-of-the-art baselines in detection accuracy while offering\nsuperior robustness, scalability, and interpretability for mixed-type tabular\ndata.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u968f\u673a\u68ee\u6797\u7684\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u8868\u683c\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u5904\u7406\u6df7\u5408\u7c7b\u578b\u8868\u683c\u6570\u636e\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u5e76\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u5f02\u5e38\u68c0\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7279\u5f81\u6761\u4ef6\u91cd\u5efa\u95ee\u9898\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u7279\u5f81\u8bad\u7ec3\u4e13\u95e8\u7684\u968f\u673a\u68ee\u6797\u3002\u5b83\u7ed3\u5408\u4e86\u8c03\u6574\u540e\u7684Gower\u8ddd\u79bb\uff08AGD\uff09\u7528\u4e8e\u5355\u5143\u7ea7\u522b\u7684\u8bc4\u5206\uff0c\u4ee5\u53ca\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u5e73\u5747\uff08UWA\uff09\u6765\u805a\u5408\u5355\u5143\u7ea7\u522b\u7684\u5206\u6570\u3002", "result": "\u572815\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRFOD\u5728\u68c0\u6d4b\u7cbe\u5ea6\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff0c\u540c\u65f6\u4e3a\u6df7\u5408\u7c7b\u578b\u8868\u683c\u6570\u636e\u63d0\u4f9b\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "RFOD\u662f\u4e00\u79cd\u6709\u6548\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6df7\u5408\u7c7b\u578b\u8868\u683c\u6570\u636e\u3002"}}
{"id": "2510.09159", "categories": ["cs.LG", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.09159", "abs": "https://arxiv.org/abs/2510.09159", "authors": ["Tianyi Chen", "Mingcheng Zhu", "Zhiyao Luo", "Tingting Zhu"], "title": "Cross-Representation Benchmarking in Time-Series Electronic Health Records for Clinical Outcome Prediction", "comment": null, "summary": "Electronic Health Records (EHRs) enable deep learning for clinical\npredictions, but the optimal method for representing patient data remains\nunclear due to inconsistent evaluation practices. We present the first\nsystematic benchmark to compare EHR representation methods, including\nmultivariate time-series, event streams, and textual event streams for LLMs.\nThis benchmark standardises data curation and evaluation across two distinct\nclinical settings: the MIMIC-IV dataset for ICU tasks (mortality, phenotyping)\nand the EHRSHOT dataset for longitudinal care (30-day readmission, 1-year\npancreatic cancer). For each paradigm, we evaluate appropriate modelling\nfamilies--including Transformers, MLP, LSTMs and Retain for time-series, CLMBR\nand count-based models for event streams, 8-20B LLMs for textual streams--and\nanalyse the impact of feature pruning based on data missingness. Our\nexperiments reveal that event stream models consistently deliver the strongest\nperformance. Pre-trained models like CLMBR are highly sample-efficient in\nfew-shot settings, though simpler count-based models can be competitive given\nsufficient data. Furthermore, we find that feature selection strategies must be\nadapted to the clinical setting: pruning sparse features improves ICU\npredictions, while retaining them is critical for longitudinal tasks. Our\nresults, enabled by a unified and reproducible pipeline, provide practical\nguidance for selecting EHR representations based on the clinical context and\ndata regime.", "AI": {"tldr": "\u7cfb\u7edf\u5730\u6bd4\u8f83\u4e86\u7535\u5b50\u75c5\u5386\uff08EHR\uff09\u8868\u793a\u65b9\u6cd5\uff0c\u5305\u62ec\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u3001\u4e8b\u4ef6\u6d41\u548cLLM\u7684\u6587\u672c\u4e8b\u4ef6\u6d41\uff0c\u5e76\u63d0\u4f9b\u5728\u4e0d\u540c\u4e34\u5e8a\u73af\u5883\u4e2d\u9009\u62e9EHR\u8868\u793a\u7684\u5b9e\u8df5\u6307\u5bfc\u3002", "motivation": "\u7531\u4e8e\u4e0d\u4e00\u81f4\u7684\u8bc4\u4f30\u5b9e\u8df5\uff0c\u7528\u4e8e\u4e34\u5e8a\u9884\u6d4b\u7684\u6700\u4f73\u60a3\u8005\u6570\u636e\u8868\u793a\u65b9\u6cd5\u4ecd\u4e0d\u6e05\u695a\u3002", "method": "\u901a\u8fc7\u7edf\u4e00\u548c\u53ef\u91cd\u590d\u7684pipeline\uff0c\u5728\u4e24\u4e2a\u4e0d\u540c\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\u6807\u51c6\u5316\u6570\u636e\u7ba1\u7406\u548c\u8bc4\u4f30\uff1a\u7528\u4e8eICU\u4efb\u52a1\u7684MIMIC-IV\u6570\u636e\u96c6\u548c\u7528\u4e8e\u7eb5\u5411\u62a4\u7406\u7684EHRSHOT\u6570\u636e\u96c6\u3002\u9488\u5bf9\u6bcf\u79cd\u8303\u4f8b\uff0c\u8bc4\u4f30\u5408\u9002\u7684\u5efa\u6a21\u5bb6\u65cf\uff0c\u5e76\u5206\u6790\u57fa\u4e8e\u6570\u636e\u7f3a\u5931\u7684\u7279\u5f81\u4fee\u526a\u7684\u5f71\u54cd\u3002", "result": "\u4e8b\u4ef6\u6d41\u6a21\u578b\u59cb\u7ec8\u63d0\u4f9b\u6700\u5f3a\u5927\u7684\u6027\u80fd\u3002\u50cfCLMBR\u8fd9\u6837\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u5177\u6709\u5f88\u9ad8\u7684\u6837\u672c\u6548\u7387\uff0c\u4f46\u5982\u679c\u7ed9\u5b9a\u8db3\u591f\u7684\u6570\u636e\uff0c\u5219\u66f4\u7b80\u5355\u7684\u57fa\u4e8e\u8ba1\u6570\u7684\u6a21\u578b\u53ef\u80fd\u5177\u6709\u7ade\u4e89\u529b\u3002\u7279\u5f81\u9009\u62e9\u7b56\u7565\u5fc5\u987b\u9002\u5e94\u4e34\u5e8a\u73af\u5883\uff1a\u4fee\u526a\u7a00\u758f\u7279\u5f81\u53ef\u6539\u5584ICU\u9884\u6d4b\uff0c\u800c\u4fdd\u7559\u5b83\u4eec\u5bf9\u4e8e\u7eb5\u5411\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u6839\u636e\u4e34\u5e8a\u80cc\u666f\u548c\u6570\u636e\u65b9\u6848\uff0c\u4e3a\u9009\u62e9EHR\u8868\u793a\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2510.08935", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.08935", "abs": "https://arxiv.org/abs/2510.08935", "authors": ["Yingyi Zhang", "Pengyue Jia", "Derong Xu", "Yi Wen", "Xianneng Li", "Yichao Wang", "Wenlin Zhang", "Xiaopeng Li", "Weinan Gan", "Huifeng Guo", "Yong Liu", "Xiangyu Zhao"], "title": "Personalize Before Retrieve: LLM-based Personalized Query Expansion for User-Centric Retrieval", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) critically depends on effective query\nexpansion to retrieve relevant information. However, existing expansion methods\nadopt uniform strategies that overlook user-specific semantics, ignoring\nindividual expression styles, preferences, and historical context. In practice,\nidentical queries in text can express vastly different intentions across users.\nThis representational rigidity limits the ability of current RAG systems to\ngeneralize effectively in personalized settings. Specifically, we identify two\ncore challenges for personalization: 1) user expression styles are inherently\ndiverse, making it difficult for standard expansions to preserve personalized\nintent. 2) user corpora induce heterogeneous semantic structures-varying in\ntopical focus and lexical organization-which hinders the effective anchoring of\nexpanded queries within the user's corpora space. To address these challenges,\nwe propose Personalize Before Retrieve (PBR), a framework that incorporates\nuser-specific signals into query expansion prior to retrieval. PBR consists of\ntwo components: P-PRF, which generates stylistically aligned pseudo feedback\nusing user history for simulating user expression style, and P-Anchor, which\nperforms graph-based structure alignment over user corpora to capture its\nstructure. Together, they produce personalized query representations tailored\nfor retrieval. Experiments on two personalized benchmarks show that PBR\nconsistently outperforms strong baselines, with up to 10% gains on PersonaBench\nacross retrievers. Our findings demonstrate the value of modeling\npersonalization before retrieval to close the semantic gap in user-adaptive RAG\nsystems. Our code is available at https://github.com/Zhang-Yingyi/PBR-code.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPersonalize Before Retrieve (PBR) \u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG) \u4e2d query expansion \u7684\u4e2a\u6027\u5316\u95ee\u9898\u3002PBR \u5728\u68c0\u7d22\u524d\u5c06\u7528\u6237\u7279\u5b9a\u4fe1\u53f7\u878d\u5165\u5230 query expansion \u4e2d\uff0c\u4ee5\u89e3\u51b3\u7528\u6237\u8868\u8fbe\u65b9\u5f0f\u591a\u6837\u6027\u548c\u7528\u6237\u8bed\u6599\u5e93\u5f02\u6784\u8bed\u4e49\u7ed3\u6784\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684 query expansion \u65b9\u6cd5\u91c7\u7528\u7edf\u4e00\u7b56\u7565\uff0c\u5ffd\u7565\u4e86\u7528\u6237\u7279\u5b9a\u7684\u8bed\u4e49\uff0c\u9650\u5236\u4e86 RAG \u7cfb\u7edf\u5728\u4e2a\u6027\u5316\u8bbe\u7f6e\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u7528\u6237\u8868\u8fbe\u65b9\u5f0f\u591a\u6837\uff0c\u4e14\u7528\u6237\u8bed\u6599\u5e93\u8bf1\u5bfc\u51fa\u5f02\u6784\u7684\u8bed\u4e49\u7ed3\u6784\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86 Personalize Before Retrieve (PBR) \u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u7ec4\u6210\u90e8\u5206\uff1aP-PRF\uff0c\u5b83\u4f7f\u7528\u7528\u6237\u5386\u53f2\u751f\u6210\u98ce\u683c\u5bf9\u9f50\u7684\u4f2a\u53cd\u9988\uff0c\u7528\u4e8e\u6a21\u62df\u7528\u6237\u8868\u8fbe\u98ce\u683c\uff1b\u4ee5\u53ca P-Anchor\uff0c\u5b83\u5bf9\u7528\u6237\u8bed\u6599\u5e93\u6267\u884c\u57fa\u4e8e\u56fe\u7684\u7ed3\u6784\u5bf9\u9f50\uff0c\u4ee5\u6355\u83b7\u5176\u7ed3\u6784\u3002", "result": "\u5728\u4e24\u4e2a\u4e2a\u6027\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPBR \u59cb\u7ec8\u4f18\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\uff0c\u5728 PersonaBench \u4e0a\uff0c\u68c0\u7d22\u5668\u7684\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe 10%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u68c0\u7d22\u524d\u5bf9\u4e2a\u6027\u5316\u8fdb\u884c\u5efa\u6a21\u5bf9\u4e8e\u7f29\u5c0f\u7528\u6237\u81ea\u9002\u5e94 RAG \u7cfb\u7edf\u4e2d\u7684\u8bed\u4e49\u5dee\u8ddd\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.08589", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08589", "abs": "https://arxiv.org/abs/2510.08589", "authors": ["Nirmal Elamon", "Rouzbeh Davoudi"], "title": "Beyond CNNs: Efficient Fine-Tuning of Multi-Modal LLMs for Object Detection on Low-Data Regimes", "comment": null, "summary": "The field of object detection and understanding is rapidly evolving, driven\nby advances in both traditional CNN-based models and emerging multi-modal large\nlanguage models (LLMs). While CNNs like ResNet and YOLO remain highly effective\nfor image-based tasks, recent transformer-based LLMs introduce new capabilities\nsuch as dynamic context reasoning, language-guided prompts, and holistic scene\nunderstanding. However, when used out-of-the-box, the full potential of LLMs\nremains underexploited, often resulting in suboptimal performance on\nspecialized visual tasks. In this work, we conduct a comprehensive comparison\nof fine-tuned traditional CNNs, zero-shot pre-trained multi-modal LLMs, and\nfine-tuned multi-modal LLMs on the challenging task of artificial text overlay\ndetection in images. A key contribution of our study is demonstrating that LLMs\ncan be effectively fine-tuned on very limited data (fewer than 1,000 images) to\nachieve up to 36% accuracy improvement, matching or surpassing CNN-based\nbaselines that typically require orders of magnitude more data. By exploring\nhow language-guided models can be adapted for precise visual understanding with\nminimal supervision, our work contributes to the broader effort of bridging\nvision and language, offering novel insights into efficient cross-modal\nlearning strategies. These findings highlight the adaptability and data\nefficiency of LLM-based approaches for real-world object detection tasks and\nprovide actionable guidance for applying multi-modal transformers in\nlow-resource visual environments. To support continued progress in this area,\nwe have made the code used to fine-tune the models available in our GitHub,\nenabling future improvements and reuse in related applications.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u5fae\u8c03\u7684\u4f20\u7edf CNN\u3001zero-shot \u9884\u8bad\u7ec3\u7684\u591a\u6a21\u6001 LLM \u548c\u5fae\u8c03\u7684\u591a\u6a21\u6001 LLM \u5728\u56fe\u50cf\u4e2d\u4eba\u5de5\u6587\u672c\u53e0\u52a0\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u8bed\u8a00\u5f15\u5bfc\u6a21\u578b\u4ee5\u6700\u5c11\u7684\u76d1\u7763\u6765\u5b9e\u73b0\u7cbe\u786e\u7684\u89c6\u89c9\u7406\u89e3\uff0c\u4ece\u800c\u5f25\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u5bf9\u5c11\u91cf\u6570\u636e\uff08\u5c11\u4e8e 1,000 \u5f20\u56fe\u50cf\uff09\u4e0a\u5fae\u8c03 LLM\uff0c\u4ee5\u63d0\u9ad8\u51c6\u786e\u7387\u3002", "result": "LLM \u7ecf\u8fc7\u5fae\u8c03\u540e\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e86 36%\uff0c\u4e0e\u901a\u5e38\u9700\u8981\u66f4\u591a\u6570\u636e\u7684\u57fa\u4e8e CNN \u7684\u57fa\u7ebf\u76f8\u5339\u914d\u6216\u8d85\u8fc7\u3002", "conclusion": "LLM \u65b9\u6cd5\u5177\u6709\u9002\u5e94\u6027\u548c\u6570\u636e\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5bf9\u8c61\u68c0\u6d4b\u4efb\u52a1\uff0c\u5e76\u4e3a\u5728\u4f4e\u8d44\u6e90\u89c6\u89c9\u73af\u5883\u4e2d\u5e94\u7528\u591a\u6a21\u6001 Transformer \u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u3002"}}
{"id": "2510.08619", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08619", "abs": "https://arxiv.org/abs/2510.08619", "authors": ["Tennison Liu", "Silas Ruhrberg Est\u00e9vez", "David L. Bentley", "Mihaela van der Schaar"], "title": "Hypothesis Hunting with Evolving Networks of Autonomous Scientific Agents", "comment": null, "summary": "Large-scale scientific datasets -- spanning health biobanks, cell atlases,\nEarth reanalyses, and more -- create opportunities for exploratory discovery\nunconstrained by specific research questions. We term this process hypothesis\nhunting: the cumulative search for insight through sustained exploration across\nvast and complex hypothesis spaces. To support it, we introduce AScience, a\nframework modeling discovery as the interaction of agents, networks, and\nevaluation norms, and implement it as ASCollab, a distributed system of\nLLM-based research agents with heterogeneous behaviors. These agents\nself-organize into evolving networks, continually producing and peer-reviewing\nfindings under shared standards of evaluation. Experiments show that such\nsocial dynamics enable the accumulation of expert-rated results along the\ndiversity-quality-novelty frontier, including rediscoveries of established\nbiomarkers, extensions of known pathways, and proposals of new therapeutic\ntargets. While wet-lab validation remains indispensable, our experiments on\ncancer cohorts demonstrate that socially structured, agentic networks can\nsustain exploratory hypothesis hunting at scale.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aAScience\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u652f\u6301\u5927\u89c4\u6a21\u79d1\u5b66\u6570\u636e\u96c6\u4e2d\u7684\u63a2\u7d22\u6027\u53d1\u73b0\uff08\u5047\u8bbe\u641c\u7d22\uff09\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u79d1\u5b66\u6570\u636e\u96c6\u4e3a\u63a2\u7d22\u6027\u53d1\u73b0\u63d0\u4f9b\u4e86\u673a\u4f1a\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u65b9\u6cd5\u6765\u652f\u6301\u8fd9\u79cd\u63a2\u7d22\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86AScience\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u53d1\u73b0\u5efa\u6a21\u4e3a\u667a\u80fd\u4f53\u3001\u7f51\u7edc\u548c\u8bc4\u4f30\u89c4\u8303\u7684\u4ea4\u4e92\u3002\u8be5\u6846\u67b6\u88ab\u5b9e\u73b0\u4e3aASCollab\uff0c\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u7814\u7a76\u667a\u80fd\u4f53\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0c\u5177\u6709\u4e0d\u540c\u7684\u884c\u4e3a\u3002\u8fd9\u4e9b\u667a\u80fd\u4f53\u81ea\u7ec4\u7ec7\u6210\u4e0d\u65ad\u53d1\u5c55\u7684\u7f51\u7edc\uff0c\u5728\u5171\u4eab\u7684\u8bc4\u4f30\u6807\u51c6\u4e0b\u6301\u7eed\u751f\u6210\u548c\u540c\u884c\u8bc4\u5ba1\u7814\u7a76\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u793e\u4f1a\u52a8\u6001\u80fd\u591f\u79ef\u7d2f\u4e13\u5bb6\u8bc4\u4f30\u7684\u7ed3\u679c\uff0c\u5305\u62ec\u91cd\u65b0\u53d1\u73b0\u5df2\u5efa\u7acb\u7684\u751f\u7269\u6807\u5fd7\u7269\u3001\u6269\u5c55\u5df2\u77e5\u7684\u901a\u8def\u4ee5\u53ca\u63d0\u51fa\u65b0\u7684\u6cbb\u7597\u9776\u70b9\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86\u5728\u764c\u75c7\u961f\u5217\u4e2d\uff0c\u793e\u4f1a\u7ed3\u6784\u5316\u7684\u667a\u80fd\u4f53\u7f51\u7edc\u53ef\u4ee5\u5927\u89c4\u6a21\u5730\u652f\u6301\u63a2\u7d22\u6027\u5047\u8bbe\u641c\u7d22\u3002"}}
{"id": "2510.08646", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.08646", "abs": "https://arxiv.org/abs/2510.08646", "authors": ["Eric Hanchen Jiang", "Weixuan Ou", "Run Liu", "Shengyuan Pang", "Guancheng Wan", "Ranjie Duan", "Wei Dong", "Kai-Wei Chang", "XiaoFeng Wang", "Ying Nian Wu", "Xinfeng Li"], "title": "Energy-Driven Steering: Reducing False Refusals in Large Language Models", "comment": null, "summary": "Safety alignment of large language models (LLMs) faces a key challenge:\ncurrent alignment techniques often only focus on improving safety against\nharmful prompts, causing LLMs to become over-cautious and refuse to respond to\nbenign prompts. Therefore, a key objective of safe alignment is to enhance\nsafety while simultaneously reducing false refusals. In this paper, we\nintroduce Energy-Driven Steering (EDS), a novel, fine-tuning free framework\ndesigned to resolve this challenge through dynamic, inference-time\nintervention. We trained a lightweight, external Energy-Based Model (EBM) to\nassign high energy to undesirable (false refusal or jailbreak) states and low\nenergy to desirable (helpful response or safe reject) ones. During inference,\nEBM maps the LLM's internal activations to an \"energy landscape\". We use the\ngradient of the energy function to dynamically steer the LLM's hidden states to\nlow energy regions, correcting the model to generate a desirable response in\nreal-time without modifying its weights. This method decouples behavioral\ncontrol from the model's core knowledge, offering a flexible solution with\nminimal computational overhead. Extensive experiments across a wide range of\nmodels show our method successfully achieves this objective: it substantially\nlowers false refusal rates. For example, raising compliance on the ORB-H\nbenchmark from 57.3% to 82.6% while maintaining the baseline safety\nperformance. Our work presents an effective paradigm for building LLMs that\nachieve both low false refusal rates and high safety.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u80fd\u91cf\u9a71\u52a8\u63a7\u5236\uff08EDS\uff09\u7684\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u9519\u8bef\u62d2\u7edd\u7387\u3002", "motivation": "\u5f53\u524d\u7684\u5b89\u5168\u5bf9\u9f50\u6280\u672f\u901a\u5e38\u53ea\u5173\u6ce8\u63d0\u9ad8\u6a21\u578b\u5bf9\u6709\u5bb3\u63d0\u793a\u7684\u5b89\u5168\u6027\uff0c\u5bfc\u81f4LLM\u53d8\u5f97\u8fc7\u5ea6\u8c28\u614e\uff0c\u62d2\u7edd\u54cd\u5e94\u826f\u6027\u63d0\u793a\u3002\u56e0\u6b64\uff0c\u5b89\u5168\u5bf9\u9f50\u7684\u4e00\u4e2a\u5173\u952e\u76ee\u6807\u662f\u589e\u5f3a\u5b89\u5168\u6027\u7684\u540c\u65f6\u51cf\u5c11\u9519\u8bef\u62d2\u7edd\u3002", "method": "\u8bad\u7ec3\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u3001\u5916\u90e8\u7684\u57fa\u4e8e\u80fd\u91cf\u7684\u6a21\u578b\uff08EBM\uff09\uff0cEBM\u5c06LLM\u7684\u5185\u90e8\u6fc0\u6d3b\u6620\u5c04\u5230\u4e00\u4e2a\u201c\u80fd\u91cf landscape\u201d\uff0c\u5e76\u4f7f\u7528\u80fd\u91cf\u51fd\u6570\u7684\u68af\u5ea6\u6765\u52a8\u6001\u5730\u5c06LLM\u7684\u9690\u85cf\u72b6\u6001\u5f15\u5bfc\u5230\u4f4e\u80fd\u91cf\u533a\u57df\uff0c\u4ece\u800c\u5b9e\u65f6\u7ea0\u6b63\u6a21\u578b\u4ee5\u751f\u6210\u671f\u671b\u7684\u54cd\u5e94\uff0c\u800c\u65e0\u9700\u4fee\u6539\u5176\u6743\u91cd\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u6a21\u578b\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5b9e\u73b0\u4e86\u8fd9\u4e00\u76ee\u6807\uff1a\u5b83\u5927\u5927\u964d\u4f4e\u4e86\u9519\u8bef\u62d2\u7edd\u7387\u3002\u4f8b\u5982\uff0c\u5728ORB-H\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5408\u89c4\u6027\u4ece57.3%\u63d0\u9ad8\u523082.6%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u57fa\u7ebf\u5b89\u5168\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u6784\u5efa\u65e2\u5177\u6709\u4f4e\u9519\u8bef\u62d2\u7edd\u7387\u53c8\u5177\u6709\u9ad8\u5b89\u5168\u6027\u7684LLM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u8303\u4f8b\u3002"}}
{"id": "2510.08588", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08588", "abs": "https://arxiv.org/abs/2510.08588", "authors": ["Ritesh Mehta"], "title": "Enhancing Biomedical Named Entity Recognition using GLiNER-BioMed with Targeted Dictionary-Based Post-processing for BioASQ 2025 task 6", "comment": "Paper published to CLEF 2025 CEUR-WS", "summary": "Biomedical Named Entity Recognition (BioNER), task6 in BioASQ (A challenge in\nlarge-scale biomedical semantic indexing and question answering), is crucial\nfor extracting information from scientific literature but faces hurdles such as\ndistinguishing between similar entity types like genes and chemicals. This\nstudy evaluates the GLiNER-BioMed model on a BioASQ dataset and introduces a\ntargeted dictionary-based post-processing strategy to address common\nmisclassifications. While this post-processing approach demonstrated notable\nimprovement on our development set, increasing the micro F1-score from a\nbaseline of 0.79 to 0.83, this enhancement did not generalize to the blind test\nset, where the post-processed model achieved a micro F1-score of 0.77 compared\nto the baselines 0.79. We also discuss insights gained from exploring\nalternative methodologies, including Conditional Random Fields. This work\nhighlights the potential of dictionary-based refinement for pre-trained BioNER\nmodels but underscores the critical challenge of overfitting to development\ndata and the necessity of ensuring robust generalization for real-world\napplicability.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86GLiNER-BioMed\u6a21\u578b\u5728BioASQ\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b57\u5178\u7684\u540e\u5904\u7406\u7b56\u7565\u6765\u89e3\u51b3\u5e38\u89c1\u7684\u9519\u8bef\u5206\u7c7b\u95ee\u9898\u3002\u867d\u7136\u8be5\u7b56\u7565\u5728\u5f00\u53d1\u96c6\u4e0a\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u5728\u76f2\u6d4b\u96c6\u4e0a\u6548\u679c\u4e0d\u4f73\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u547d\u540d\u5b9e\u4f53\u8bc6\u522b(BioNER)\u5bf9\u4e8e\u4ece\u79d1\u5b66\u6587\u732e\u4e2d\u63d0\u53d6\u4fe1\u606f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u7740\u533a\u5206\u57fa\u56e0\u548c\u5316\u5b66\u7269\u8d28\u7b49\u76f8\u4f3c\u5b9e\u4f53\u7c7b\u578b\u7684\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86GLiNER-BioMed\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u6709\u9488\u5bf9\u6027\u7684\u3001\u57fa\u4e8e\u5b57\u5178\u7684\u540e\u5904\u7406\u7b56\u7565\u3002", "result": "\u540e\u5904\u7406\u65b9\u6cd5\u5728\u5f00\u53d1\u96c6\u4e0a\u5c06micro F1-score\u4ece0.79\u63d0\u9ad8\u52300.83\uff0c\u4f46\u5728\u76f2\u6d4b\u96c6\u4e0a\uff0c\u540e\u5904\u7406\u6a21\u578b\u7684micro F1-score\u4e3a0.77\uff0c\u4f4e\u4e8e\u57fa\u7ebf\u76840.79\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u57fa\u4e8e\u5b57\u5178\u7684\u6539\u8fdb\u5bf9\u9884\u8bad\u7ec3BioNER\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u4f46\u4e5f\u5f3a\u8c03\u4e86\u8fc7\u5ea6\u62df\u5408\u5f00\u53d1\u6570\u636e\u7684\u5173\u952e\u6311\u6218\uff0c\u4ee5\u53ca\u786e\u4fdd\u5bf9\u73b0\u5b9e\u4e16\u754c\u9002\u7528\u6027\u7684\u7a33\u5065\u6cdb\u5316\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2510.09567", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.09567", "abs": "https://arxiv.org/abs/2510.09567", "authors": ["Jacopo Tagliabue", "Ciro Greco"], "title": "Safe, Untrusted, \"Proof-Carrying\" AI Agents: toward the agentic lakehouse", "comment": "IEEE Big Data, Workshop on Secure and Safe AI Agents for Big Data\n  Infrastructures", "summary": "Data lakehouses run sensitive workloads, where AI-driven automation raises\nconcerns about trust, correctness, and governance. We argue that API-first,\nprogrammable lakehouses provide the right abstractions for safe-by-design,\nagentic workflows. Using Bauplan as a case study, we show how data branching\nand declarative environments extend naturally to agents, enabling\nreproducibility and observability while reducing the attack surface. We present\na proof-of-concept in which agents repair data pipelines using correctness\nchecks inspired by proof-carrying code. Our prototype demonstrates that\nuntrusted AI agents can operate safely on production data and outlines a path\ntoward a fully agentic lakehouse.", "AI": {"tldr": "API\u4f18\u5148\u7684\u53ef\u7f16\u7a0bLakehouse\u4e3a\u5b89\u5168\u3001\u81ea\u4e3b\u7684\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u6b63\u786e\u7684\u62bd\u8c61\u3002", "motivation": "\u5728\u6570\u636e\u6e56\u4e2d\uff0cAI\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u5f15\u8d77\u4e86\u5bf9\u4fe1\u4efb\u3001\u6b63\u786e\u6027\u548c\u6cbb\u7406\u7684\u62c5\u5fe7\u3002", "method": "\u4ee5Bauplan\u4e3a\u4f8b\uff0c\u5c55\u793a\u4e86\u6570\u636e\u5206\u652f\u548c\u58f0\u660e\u5f0f\u73af\u5883\u5982\u4f55\u81ea\u7136\u5730\u6269\u5c55\u5230\u4ee3\u7406\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u91cd\u590d\u6027\u548c\u53ef\u89c2\u5bdf\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u653b\u51fb\u9762\u3002", "result": "\u4e00\u4e2a\u6982\u5ff5\u9a8c\u8bc1\uff0c\u5176\u4e2d\u4ee3\u7406\u4f7f\u7528\u53d7\u643a\u5e26\u4ee3\u7801\u8bc1\u660e\u542f\u53d1\u7684\u6b63\u786e\u6027\u68c0\u67e5\u6765\u4fee\u590d\u6570\u636e\u7ba1\u9053\u3002\u539f\u578b\u8868\u660e\uff0c\u4e0d\u53d7\u4fe1\u4efb\u7684AI\u4ee3\u7406\u53ef\u4ee5\u5728\u751f\u4ea7\u6570\u636e\u4e0a\u5b89\u5168\u8fd0\u884c\u3002", "conclusion": "\u4e0d\u53d7\u4fe1\u4efb\u7684AI\u4ee3\u7406\u53ef\u4ee5\u5728\u751f\u4ea7\u6570\u636e\u4e0a\u5b89\u5168\u8fd0\u884c\uff0c\u5e76\u6982\u8ff0\u4e86\u901a\u5f80\u5b8c\u5168\u4ee3\u7406\u5316\u7684lakehouse\u7684\u8def\u5f84\u3002"}}
{"id": "2510.08948", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08948", "abs": "https://arxiv.org/abs/2510.08948", "authors": ["Nan Lu", "Yurong Hu", "Jiaquan Fang", "Yan Liu", "Rui Dong", "Yiming Wang", "Rui Lin", "Shaoyi Xu"], "title": "SHERLOCK: Towards Dynamic Knowledge Adaptation in LLM-enhanced E-commerce Risk Management", "comment": null, "summary": "The growth of the e-commerce industry has intensified the adversarial\ndynamics between shadow economy actors and risk management teams. Companies\noften conduct risk investigations into suspicious cases to identify emerging\nfraud patterns, thereby enhancing both preemptive risk prevention and post-hoc\ngovernance. However, the sheer volume of case analyses imposes a substantial\nworkload on risk management analysts, as each case requires the integration of\nlong-term expert experience and meticulous scrutiny across multiple risk\ndimensions. Additionally, individual disparities among analysts hinder the\nestablishment of uniform and high-standard workflows. To address these\nchallenges, we propose the SHERLOCK framework, which leverages the reasoning\ncapabilities of large language models (LLMs) to assist analysts in risk\ninvestigations. Our approach consists of three primary components: (1)\nextracting risk management knowledge from multi-modal data and constructing a\ndomain knowledge base (KB), (2) building an intelligent platform guided by the\ndata flywheel paradigm that integrates daily operations, expert annotations,\nand model evaluations, with iteratively fine-tuning for preference alignment,\nand (3) introducing a Reflect & Refine (R&R) module that collaborates with the\ndomain KB to establish a rapid response mechanism for evolving risk patterns.\nExperiments conducted on the real-world transaction dataset from JD.com\ndemonstrate that our method significantly improves the precision of both\nfactual alignment and risk localization within the LLM analysis results.\nDeployment of the SHERLOCK-based LLM system on JD.com has substantially\nenhanced the efficiency of case investigation workflows for risk managers.", "AI": {"tldr": "\u63d0\u51fa SHERLOCK \u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u534f\u52a9\u98ce\u9669\u8c03\u67e5\uff0c\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u7535\u5546\u53d1\u5c55\u52a0\u5267\u4e86\u5f71\u5b50\u7ecf\u6d4e\u884c\u4e3a\u8005\u4e0e\u98ce\u9669\u7ba1\u7406\u56e2\u961f\u4e4b\u95f4\u7684\u5bf9\u6297\uff0c\u6848\u4ef6\u5206\u6790\u5de5\u4f5c\u91cf\u5927\uff0c\u5206\u6790\u5e08\u4e4b\u95f4\u5b58\u5728\u4e2a\u4f53\u5dee\u5f02\uff0c\u96be\u4ee5\u5efa\u7acb\u7edf\u4e00\u7684\u9ad8\u6807\u51c6\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u8be5\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u7ec4\u6210\u90e8\u5206\uff1a(1) \u4ece\u591a\u6a21\u6001\u6570\u636e\u4e2d\u63d0\u53d6\u98ce\u9669\u7ba1\u7406\u77e5\u8bc6\u5e76\u6784\u5efa\u9886\u57df\u77e5\u8bc6\u5e93 (KB)\uff0c(2) \u6784\u5efa\u7531\u6570\u636e\u98de\u8f6e\u8303\u5f0f\u6307\u5bfc\u7684\u667a\u80fd\u5e73\u53f0\uff0c\u6574\u5408\u65e5\u5e38\u8fd0\u8425\u3001\u4e13\u5bb6\u6ce8\u91ca\u548c\u6a21\u578b\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u5fae\u8c03\u4ee5\u8fdb\u884c\u504f\u597d\u5bf9\u9f50\uff0c(3) \u5f15\u5165 Reflect & Refine (R&R) \u6a21\u5757\uff0c\u8be5\u6a21\u5757\u4e0e\u9886\u57df\u77e5\u8bc6\u5e93 (KB) \u534f\u4f5c\uff0c\u4e3a\u4e0d\u65ad\u53d8\u5316\u7684\u98ce\u9669\u6a21\u5f0f\u5efa\u7acb\u5feb\u901f\u54cd\u5e94\u673a\u5236\u3002", "result": "\u5728 JD.com \u7684\u771f\u5b9e\u4ea4\u6613\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u7740\u63d0\u9ad8\u4e86 LLM \u5206\u6790\u7ed3\u679c\u4e2d\u4e8b\u5b9e\u5bf9\u9f50\u548c\u98ce\u9669\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u5728 JD.com \u4e0a\u90e8\u7f72\u57fa\u4e8e SHERLOCK \u7684 LLM \u7cfb\u7edf\u5927\u5927\u63d0\u9ad8\u4e86\u98ce\u9669\u7ba1\u7406\u4eba\u5458\u7684\u6848\u4ef6\u8c03\u67e5\u5de5\u4f5c\u6d41\u7a0b\u6548\u7387\u3002"}}
{"id": "2510.08617", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08617", "abs": "https://arxiv.org/abs/2510.08617", "authors": ["Saumya B"], "title": "Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation", "comment": "Code and results available at\n  https://github.com/Saumya4321/2d-brain-tumor-segmentation", "summary": "Brain tumor segmentation is crucial for diagnosis and treatment planning, yet\nchallenges such as class imbalance and limited model generalization continue to\nhinder progress. This work presents a reproducible evaluation of U-Net\nsegmentation performance on brain tumor MRI using focal loss and basic data\naugmentation strategies. Experiments were conducted on a publicly available MRI\ndataset, focusing on focal loss parameter tuning and assessing the impact of\nthree data augmentation techniques: horizontal flip, rotation, and scaling. The\nU-Net with focal loss achieved a precision of 90%, comparable to\nstate-of-the-art results. By making all code and results publicly available,\nthis study establishes a transparent, reproducible baseline to guide future\nresearch on augmentation strategies and loss function design in brain tumor\nsegmentation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eU-Net\u7684\u8111\u80bf\u7624\u5206\u5272\u65b9\u6cd5\uff0c\u4f7f\u7528focal loss\u548c\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u5e76\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8fbe\u5230\u4e86\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002", "motivation": "\u8111\u80bf\u7624\u5206\u5272\u5bf9\u4e8e\u8bca\u65ad\u548c\u6cbb\u7597\u8ba1\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u6709\u9650\u7b49\u6311\u6218\u4ecd\u7136\u963b\u788d\u8fdb\u5c55\u3002", "method": "\u4f7f\u7528focal loss\u548c\u57fa\u672c\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff08\u6c34\u5e73\u7ffb\u8f6c\u3001\u65cb\u8f6c\u548c\u7f29\u653e\uff09\u5728\u516c\u5f00\u7684MRI\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30U-Net\u5206\u5272\u6027\u80fd\u3002", "result": "U-Net\u4e0efocal loss\u7684\u7ed3\u5408\u8fbe\u5230\u4e8690%\u7684\u7cbe\u5ea6\uff0c\u4e0e\u76ee\u524d\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u76f8\u5f53\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u900f\u660e\u3001\u53ef\u91cd\u590d\u7684\u57fa\u7ebf\uff0c\u4ee5\u6307\u5bfc\u672a\u6765\u5173\u4e8e\u8111\u80bf\u7624\u5206\u5272\u4e2d\u589e\u5f3a\u7b56\u7565\u548c\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\u7684\u7814\u7a76\u3002"}}
{"id": "2510.08671", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08671", "abs": "https://arxiv.org/abs/2510.08671", "authors": ["Milon Bhattacharya", "Milan Kumar"], "title": "Optimizing delivery for quick commerce factoring qualitative assessment of generated routes", "comment": null, "summary": "Indias e-commerce market is projected to grow rapidly, with last-mile\ndelivery accounting for nearly half of operational expenses. Although vehicle\nrouting problem (VRP) based solvers are widely used for delivery planning,\ntheir effectiveness in real-world scenarios is limited due to unstructured\naddresses, incomplete maps, and computational constraints in distance\nestimation. This study proposes a framework that employs large language models\n(LLMs) to critique VRP-generated routes against policy-based criteria, allowing\nlogistics operators to evaluate and prioritise more efficient delivery plans.\nAs a illustration of our approach we generate, annotate and evaluated 400 cases\nusing large language models. Our study found that open-source LLMs identified\nrouting issues with 79% accuracy, while proprietary reasoning models achieved\nreach upto 86%. The results demonstrate that LLM-based evaluation of\nVRP-generated routes can be an effective and scalable layer of evaluation which\ngoes beyond beyond conventional distance and time based metrics. This has\nimplications for improving cost efficiency, delivery reliability, and\nsustainability in last-mile logistics, especially for developing countries like\nIndia.", "AI": {"tldr": "\u5370\u5ea6\u7535\u5546\u5e02\u573a\u589e\u957f\u8fc5\u901f\uff0c\u4f46\u6700\u540e\u4e00\u516c\u91cc\u914d\u9001\u6210\u672c\u9ad8\u6602\u4e14\u9762\u4e34\u73b0\u5b9e\u95ee\u9898\u3002\u672c\u7814\u7a76\u63d0\u51fa\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bc4\u4f30\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08VRP\uff09\u751f\u6210\u7684\u8def\u7ebf\uff0c\u4ee5\u63d0\u5347\u914d\u9001\u6548\u7387\u3002", "motivation": "\u5728\u5370\u5ea6\uff0c\u6700\u540e\u4e00\u516c\u91cc\u914d\u9001\u5360\u7535\u5546\u8fd0\u8425\u6210\u672c\u8fd1\u4e00\u534a\u3002\u4f20\u7edf\u7684VRP\u6c42\u89e3\u5668\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d7\u9650\u4e8e\u4e0d\u89c4\u8303\u7684\u5730\u5740\u3001\u4e0d\u5b8c\u6574\u7684\u5730\u56fe\u4ee5\u53ca\u8ddd\u79bb\u4f30\u7b97\u7684\u8ba1\u7b97\u7ea6\u675f\uff0c\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6839\u636e\u7b56\u7565\u6807\u51c6\u8bc4\u4f30VRP\u751f\u6210\u7684\u8def\u7ebf\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5f00\u6e90LLM\u80fd\u4ee579%\u7684\u51c6\u786e\u7387\u8bc6\u522b\u8def\u7ebf\u95ee\u9898\uff0c\u800c\u4e13\u6709\u63a8\u7406\u6a21\u578b\u5219\u9ad8\u8fbe86%\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684VRP\u8def\u7ebf\u8bc4\u4f30\u662f\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u624b\u6bb5\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u8ddd\u79bb\u548c\u65f6\u95f4\u6307\u6807\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u6700\u540e\u4e00\u516c\u91cc\u7269\u6d41\u7684\u6210\u672c\u6548\u7387\u3001\u4ea4\u4ed8\u53ef\u9760\u6027\u548c\u53ef\u6301\u7eed\u6027\uff0c\u5c24\u5176\u662f\u5728\u50cf\u5370\u5ea6\u8fd9\u6837\u7684\u53d1\u5c55\u4e2d\u56fd\u5bb6\u3002"}}
{"id": "2510.08648", "categories": ["cs.LG", "cs.AI", "F.4.1", "I.2.4"], "pdf": "https://arxiv.org/pdf/2510.08648", "abs": "https://arxiv.org/abs/2510.08648", "authors": ["Edward Y. Chang", "Ethan Y. Chang"], "title": "Inverse-Free Wilson Loops for Transformers: A Practical Diagnostic for Invariance and Order Sensitivity", "comment": "24 pages, 10 figures, 2 tables", "summary": "Large language models can change answers under harmless edits that matter in\npractice: RAG outputs flip when passages are reordered, fine-tuning erodes\ninvariances learned at pretraining, debate or chain-of-thought prompts take\npath-dependent routes, and compiler fusion or reordering perturbs logits near\ndecision boundaries. These failures violate intended invariances, break\ncontinuous integration, and force teams to trade safety for speed. The effects\nare small yet distributed across layers and positions, sensitive to context\nlength and evaluation order, and costly to repair with retraining or formal\nverification. We present WILSON, a minimal post-hoc diagnostic suite that\nconverts simple loop and reordering checks on internal representations into\nsystem signals. WILSON combines an inverse-free curvature map over positions\nand layers, computed with JVPs and Hutchinson probes, with activation-level\ncommutators that flag reorder risk. Signals are cheap to compute,\nmodel-agnostic for standard Transformers, and exported as thresholds and CSV\nartifacts for orchestrators. This enables concrete actions: guard RAG against\norder effects, catch fine-tuning regressions, stabilize debate pathways and\nlong multi-turn contexts, and gate fusions or reorders in deployment. In short,\nWILSON helps anticipate failures and approve safe optimizations so reliability\nand throughput can improve together without changing model architecture or\ntraining.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u4f1a\u5728\u65e0\u5bb3\u7684\u7f16\u8f91\u4e0b\u6539\u53d8\u7b54\u6848\u3002\u6211\u4eec\u63d0\u51fa\u4e86WILSON\uff0c\u4e00\u4e2a\u6700\u5c0f\u7684\u4e8b\u540e\u8bca\u65ad\u5957\u4ef6\uff0c\u5b83\u53ef\u4ee5\u5c06\u5185\u90e8\u8868\u5f81\u7684\u7b80\u5355\u5faa\u73af\u548c\u91cd\u65b0\u6392\u5e8f\u68c0\u67e5\u8f6c\u6362\u4e3a\u7cfb\u7edf\u4fe1\u53f7\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u672a\u80fd\u4fdd\u6301\u9884\u671f\u4e0d\u53d8\u6027\uff0c\u6253\u7834\u4e86\u6301\u7eed\u96c6\u6210\uff0c\u5e76\u8feb\u4f7f\u56e2\u961f\u5728\u5b89\u5168\u6027\u548c\u901f\u5ea6\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86WILSON\uff0c\u5b83\u7ed3\u5408\u4e86\u4f7f\u7528JVPs\u548cHutchinson probes\u8ba1\u7b97\u7684\u4f4d\u7f6e\u548c\u5c42\u4e0a\u7684\u65e0\u9006\u66f2\u7387\u56fe\uff0c\u4ee5\u53ca\u6807\u8bb0\u91cd\u65b0\u6392\u5e8f\u98ce\u9669\u7684\u6fc0\u6d3b\u6c34\u5e73\u6362\u5411\u5668\u3002", "result": "WILSON\u7684\u4fe1\u53f7\u8ba1\u7b97\u6210\u672c\u4f4e\u5ec9\uff0c\u5bf9\u6807\u51c6Transformer\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\uff0c\u5e76\u5bfc\u51fa\u4e3a\u7f16\u6392\u5668\u7684\u9608\u503c\u548cCSV\u5de5\u4ef6\u3002", "conclusion": "WILSON\u6709\u52a9\u4e8e\u9884\u6d4b\u6545\u969c\u5e76\u6279\u51c6\u5b89\u5168\u4f18\u5316\uff0c\u4ece\u800c\u53ef\u4ee5\u5728\u4e0d\u66f4\u6539\u6a21\u578b\u67b6\u6784\u6216\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u540c\u65f6\u63d0\u9ad8\u53ef\u9760\u6027\u548c\u541e\u5410\u91cf\u3002"}}
{"id": "2510.08592", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08592", "abs": "https://arxiv.org/abs/2510.08592", "authors": ["Shahriar Kabir Nahin", "Hadi Askari", "Muhao Chen", "Anshuman Chhabra"], "title": "Less Diverse, Less Safe: The Indirect But Pervasive Risk of Test-Time Scaling in Large Language Models", "comment": null, "summary": "Test-Time Scaling (TTS) improves LLM reasoning by exploring multiple\ncandidate responses and then operating over this set to find the best output. A\ntacit premise behind TTS is that sufficiently diverse candidate pools enhance\nreliability. In this work, we show that this assumption in TTS introduces a\npreviously unrecognized failure mode. When candidate diversity is curtailed,\neven by a modest amount, TTS becomes much more likely to produce unsafe\noutputs. We present a reference-guided diversity reduction protocol (RefDiv)\nthat serves as a diagnostic attack to stress test TTS pipelines. Through\nextensive experiments across four open-source models (Qwen3, Mistral, Llama3.1,\nGemma3) and two widely used TTS strategies (Monte Carlo Tree Search and\nBest-of-N), constraining diversity consistently signifies the rate at which TTS\nproduces unsafe results. The effect is often stronger than that produced by\nprompts directly with high adversarial intent scores. This observed phenomenon\nalso transfers across TTS strategies and to closed-source models (e.g. OpenAI\no3 and Gemini-2.5-Pro), thus indicating that this is a general and extant\nproperty of TTS rather than a model-specific artifact. Additionally, we find\nthat numerous widely used safety guardrail classifiers (e.g. Llama-Guard and\nOpenAI Moderation API), are unable to flag the adversarial input prompts\ngenerated by RefDiv, demonstrating that existing defenses offer limited\nprotection against this diversity-driven failure mode. Through this work, we\nhope to motivate future research on designing robust TTS strategies that are\nboth effective and secure against diversity-targeted stress tests as\nillustrated by RefDiv.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u6d4b\u8bd5\u65f6\u7f29\u653e(TTS)\u7684\u4e00\u4e2a\u65b0\u7684\u5931\u8d25\u6a21\u5f0f\uff1a\u5f53\u5019\u9009\u7b54\u6848\u7684\u591a\u6837\u6027\u964d\u4f4e\u65f6\uff0cTTS\u66f4\u5bb9\u6613\u4ea7\u751f\u4e0d\u5b89\u5168\u7684\u7ed3\u679c\u3002", "motivation": "TTS\u4f9d\u8d56\u4e8e\u5019\u9009\u7b54\u6848\u7684\u591a\u6837\u6027\u6765\u63d0\u9ad8LLM\u63a8\u7406\u7684\u5b89\u5168\u6027\uff0c\u4f46\u8bba\u6587\u6307\u51fa\u591a\u6837\u6027\u4e0d\u8db3\u4f1a\u5bfc\u81f4\u4e0d\u5b89\u5168\u8f93\u51fa\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u8003\u5f15\u5bfc\u7684\u591a\u6837\u6027\u964d\u4f4e\u534f\u8bae(RefDiv)\u4f5c\u4e3a\u8bca\u65ad\u653b\u51fb\uff0c\u7528\u4e8e\u538b\u529b\u6d4b\u8bd5TTS\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u964d\u4f4e\u591a\u6837\u6027\u4f1a\u663e\u8457\u63d0\u9ad8TTS\u4ea7\u751f\u4e0d\u5b89\u5168\u7ed3\u679c\u7684\u6bd4\u7387\uff0c\u4e14\u4f18\u4e8e\u76f4\u63a5\u4f7f\u7528\u9ad8\u5bf9\u6297\u610f\u56fe\u7684prompt\u3002\u73b0\u6709\u7684\u5b89\u5168\u9632\u62a4\u680f\u5206\u7c7b\u5668\u65e0\u6cd5\u6709\u6548\u8bc6\u522bRefDiv\u751f\u6210\u7684\u5bf9\u6297\u6027\u8f93\u5165\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u4e86TTS\u7b56\u7565\u5728\u591a\u6837\u6027\u53d7\u9650\u60c5\u51b5\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u5e76\u547c\u5401\u672a\u6765\u7814\u7a76\u8bbe\u8ba1\u66f4\u9c81\u68d2\u7684TTS\u7b56\u7565\uff0c\u4ee5\u5e94\u5bf9\u591a\u6837\u6027\u5b9a\u5411\u7684\u538b\u529b\u6d4b\u8bd5\u3002"}}
{"id": "2510.08985", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.08985", "abs": "https://arxiv.org/abs/2510.08985", "authors": ["Xuan Lu", "Haohang Huang", "Rui Meng", "Yaohui Jin", "Wenjun Zeng", "Xiaoyu Shen"], "title": "Rethinking Reasoning in Document Ranking: Why Chain-of-Thought Falls Short", "comment": null, "summary": "Document reranking is a key component in information retrieval (IR), aimed at\nrefining initial retrieval results to improve ranking quality for downstream\ntasks. Recent studies--motivated by large reasoning models (LRMs)--have begun\nincorporating explicit chain-of-thought (CoT) reasoning into LLM-based\nrerankers. However, the effectiveness of such reasoning for ranking tasks\nremains underexplored. In this work, we present the first systematic study of\nreasoning in reranking across both pointwise and listwise settings, under both\nsupervised fine-tuning and reinforcement learning. Using diverse benchmarks,\nincluding reasoning-intensive datasets (BRIGHT) and standard IR benchmarks\n(BEIR), we find that reasoning-augmented rerankers consistently underperform\ntheir direct counterparts that predict rankings without CoT, despite\nsubstantially higher inference costs. Our analysis reveals three core\nlimitations: (i) in pointwise rerankers, reasoning breaks calibration and\nbiases models toward the positive class, raising TPR but lowering TNR, which\ninflates false positives and degrades ranking in negative-dominant pools; (ii)\nin listwise rerankers, reasoning improves in-domain fit but increases variance\nand fails to generalize out-of-domain, even when reinforcement learning\nshortens rationales; and (iii) overall, directly fine-tuned rerankers remain\nmore stable, effective, and robust. These findings challenge the assumption\nthat explicit reasoning is universally beneficial for reranking. We conclude by\nhighlighting future directions, including calibration-aware scoring for\npointwise rerankers and the design of concise, targeted reasoning strategies to\nmitigate overfitting and overthinking in listwise rerankers.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u6587\u6863\u91cd\u6392\u5e8f\u4efb\u52a1\u4e2d\uff0c\u5c06\u663e\u5f0f\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u878d\u5165\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u91cd\u6392\u5e8f\u5668\u4e2d\u7684\u6709\u6548\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5c3d\u7ba1\u63a8\u7406\u589e\u5f3a\u7684\u91cd\u6392\u5e8f\u5668\u5177\u6709\u66f4\u9ad8\u7684\u63a8\u7406\u6210\u672c\uff0c\u4f46\u5176\u6027\u80fd\u59cb\u7ec8\u4e0d\u5982\u76f4\u63a5\u9884\u6d4b\u6392\u5e8f\u7684\u540c\u7c7b\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5f00\u59cb\u5c06\u663e\u5f0f\u601d\u7ef4\u94fe\u63a8\u7406\u878d\u5165\u57fa\u4e8eLLM\u7684\u91cd\u6392\u5e8f\u5668\u4e2d\uff0c\u4f46\u8fd9\u79cd\u63a8\u7406\u5bf9\u4e8e\u6392\u5e8f\u4efb\u52a1\u7684\u6709\u6548\u6027\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u672c\u6587\u5728\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u4e0b\uff0c\u5bf9\u70b9\u5f0f\u548c\u5217\u8868\u5f0f\u8bbe\u7f6e\u4e2d\u7684\u91cd\u6392\u5e8f\u63a8\u7406\u8fdb\u884c\u4e86\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u3002\u4f7f\u7528\u4e86\u5305\u62ec\u63a8\u7406\u5bc6\u96c6\u578b\u6570\u636e\u96c6\uff08BRIGHT\uff09\u548c\u6807\u51c6IR\u57fa\u51c6\uff08BEIR\uff09\u5728\u5185\u7684\u5404\u79cd\u57fa\u51c6\u3002", "result": "\u63a8\u7406\u589e\u5f3a\u7684\u91cd\u6392\u5e8f\u5668\u59cb\u7ec8\u4e0d\u5982\u6ca1\u6709CoT\u7684\u76f4\u63a5\u5bf9\u5e94\u6a21\u578b\uff0c\u5e76\u4e14\u63a8\u7406\u4f1a\u6253\u7834\u6821\u51c6\uff0c\u4f7f\u6a21\u578b\u504f\u5411\u6b63\u7c7b\uff0c\u4ece\u800c\u589e\u52a0\u5047\u9633\u6027\u5e76\u964d\u4f4e\u6392\u5e8f\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u63a8\u7406\u63d0\u9ad8\u4e86\u57df\u5185\u62df\u5408\u5ea6\uff0c\u4f46\u589e\u52a0\u4e86\u65b9\u5dee\uff0c\u5e76\u4e14\u65e0\u6cd5\u63a8\u5e7f\u5230\u57df\u5916\u3002", "conclusion": "\u663e\u5f0f\u63a8\u7406\u5e76\u975e\u666e\u904d\u9002\u7528\u4e8e\u91cd\u6392\u5e8f\uff0c\u76f4\u63a5\u5fae\u8c03\u7684\u91cd\u6392\u5e8f\u5668\u66f4\u7a33\u5b9a\u3001\u6709\u6548\u548c\u9c81\u68d2\u3002\u672a\u6765\u7684\u65b9\u5411\u5305\u62ec\u70b9\u5f0f\u91cd\u6392\u5e8f\u5668\u7684\u6821\u51c6\u611f\u77e5\u8bc4\u5206\uff0c\u4ee5\u53ca\u7b80\u6d01\u3001\u6709\u9488\u5bf9\u6027\u7684\u63a8\u7406\u7b56\u7565\uff0c\u4ee5\u51cf\u8f7b\u5217\u8868\u5f0f\u91cd\u6392\u5e8f\u5668\u4e2d\u7684\u8fc7\u5ea6\u62df\u5408\u548c\u8fc7\u5ea6\u601d\u8003\u3002"}}
{"id": "2510.08625", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08625", "abs": "https://arxiv.org/abs/2510.08625", "authors": ["Hyeonggeun Han", "Sehwan Kim", "Hyungjun Joo", "Sangwoo Hong", "Jungwoo Lee"], "title": "Adjusting Initial Noise to Mitigate Memorization in Text-to-Image Diffusion Models", "comment": null, "summary": "Despite their impressive generative capabilities, text-to-image diffusion\nmodels often memorize and replicate training data, prompting serious concerns\nover privacy and copyright. Recent work has attributed this memorization to an\nattraction basin-a region where applying classifier-free guidance (CFG) steers\nthe denoising trajectory toward memorized outputs-and has proposed deferring\nCFG application until the denoising trajectory escapes this basin. However,\nsuch delays often result in non-memorized images that are poorly aligned with\nthe input prompts, highlighting the need to promote earlier escape so that CFG\ncan be applied sooner in the denoising process. In this work, we show that the\ninitial noise sample plays a crucial role in determining when this escape\noccurs. We empirically observe that different initial samples lead to varying\nescape times. Building on this insight, we propose two mitigation strategies\nthat adjust the initial noise-either collectively or individually-to find and\nutilize initial samples that encourage earlier basin escape. These approaches\nsignificantly reduce memorization while preserving image-text alignment.", "AI": {"tldr": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5bb9\u6613\u8bb0\u5fc6\u548c\u590d\u5236\u8bad\u7ec3\u6570\u636e\uff0c\u5f15\u53d1\u9690\u79c1\u548c\u7248\u6743\u95ee\u9898\u3002\u901a\u8fc7\u8c03\u6574\u521d\u59cb\u566a\u58f0\u6837\u672c\uff0c\u53ef\u4ee5\u51cf\u5c11\u8bb0\u5fc6\u5e76\u4fdd\u6301\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4f1a\u8bb0\u5fc6\u548c\u590d\u5236\u8bad\u7ec3\u6570\u636e\uff0c\u5f15\u53d1\u9690\u79c1\u548c\u7248\u6743\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u5ef6\u8fdf\u5e94\u7528\u65e0\u5206\u7c7b\u5668\u6307\u5bfc(CFG)\u6765\u907f\u514d\u8bb0\u5fc6\uff0c\u4f46\u5bfc\u81f4\u56fe\u50cf\u4e0e\u63d0\u793a\u8bcd\u5bf9\u9f50\u4e0d\u826f\uff0c\u56e0\u6b64\u9700\u8981\u5c3d\u65e9\u9003\u79bb\u8bb0\u5fc6\u5e93\u3002", "method": "\u901a\u8fc7\u8c03\u6574\u521d\u59cb\u566a\u58f0\u6837\u672c\uff08\u96c6\u4f53\u6216\u5355\u72ec\uff09\u6765\u5bfb\u627e\u5e76\u5229\u7528\u9f13\u52b1\u66f4\u65e9\u9003\u79bb\u8bb0\u5fc6\u5e93\u7684\u521d\u59cb\u6837\u672c\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8bb0\u5fc6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u3002", "conclusion": "\u521d\u59cb\u566a\u58f0\u6837\u672c\u5728\u51b3\u5b9a\u4f55\u65f6\u9003\u79bb\u8bb0\u5fc6\u5e93\u65b9\u9762\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u6240\u63d0\u51fa\u7684\u4e24\u79cd\u7f13\u89e3\u7b56\u7565\u53ef\u4ee5\u8c03\u6574\u521d\u59cb\u566a\u58f0\u4ee5\u9f13\u52b1\u66f4\u65e9\u5730\u9003\u79bb\u8bb0\u5fc6\u5e93\u3002"}}
{"id": "2510.08713", "categories": ["cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08713", "abs": "https://arxiv.org/abs/2510.08713", "authors": ["Yifei Dong", "Fengyi Wu", "Guangyu Chen", "Zhi-Qi Cheng", "Qiyu Hu", "Yuxuan Zhou", "Jingdong Sun", "Jun-Yan He", "Qi Dai", "Alexander G Hauptmann"], "title": "Unified World Models: Memory-Augmented Planning and Foresight for Visual Navigation", "comment": "18 pages, 11 figures, code: https://github.com/F1y1113/UniWM", "summary": "Enabling embodied agents to effectively imagine future states is critical for\nrobust and generalizable visual navigation. Current state-of-the-art\napproaches, however, adopt modular architectures that separate navigation\nplanning from visual world modeling, leading to state-action misalignment and\nlimited adaptability in novel or dynamic scenarios. To overcome this\nfundamental limitation, we propose UniWM, a unified, memory-augmented world\nmodel integrating egocentric visual foresight and planning within a single\nmultimodal autoregressive backbone. Unlike modular frameworks, UniWM explicitly\ngrounds action decisions in visually imagined outcomes, ensuring tight\nalignment between prediction and control. A hierarchical memory mechanism\nfurther integrates detailed short-term perceptual cues with longer-term\ntrajectory context, enabling stable, coherent reasoning over extended horizons.\nExtensive experiments across four challenging benchmarks (Go Stanford, ReCon,\nSCAND, HuRoN) demonstrate that UniWM substantially improves navigation success\nrates by up to 30%, significantly reduces trajectory errors compared to strong\nbaselines, and exhibits impressive zero-shot generalization on the unseen\nTartanDrive dataset. These results highlight UniWM as a principled step toward\nunified, imagination-driven embodied navigation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUniWM\u7684\u7edf\u4e00\u7684\u3001\u8bb0\u5fc6\u589e\u5f3a\u7684\u4e16\u754c\u6a21\u578b\uff0c\u7528\u4e8e\u5177\u8eab\u667a\u80fd\u4f53\u7684\u89c6\u89c9\u5bfc\u822a\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u5c06\u5bfc\u822a\u89c4\u5212\u4e0e\u89c6\u89c9\u4e16\u754c\u5efa\u6a21\u5206\u79bb\uff0c\u5bfc\u81f4\u72b6\u6001-\u52a8\u4f5c\u4e0d\u5bf9\u9f50\uff0c\u5e76\u4e14\u5728\u65b0\u573a\u666f\u6216\u52a8\u6001\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u6709\u9650\u3002", "method": "UniWM \u5728\u5355\u4e2a\u591a\u6a21\u6001\u81ea\u56de\u5f52\u9aa8\u5e72\u7f51\u7edc\u4e2d\u96c6\u6210\u4e86\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u89c6\u89c9\u9884\u6d4b\u548c\u89c4\u5212\u3002\u5206\u5c42\u8bb0\u5fc6\u673a\u5236\u8fdb\u4e00\u6b65\u6574\u5408\u4e86\u8be6\u7ec6\u7684\u77ed\u671f\u611f\u77e5\u7ebf\u7d22\u548c\u957f\u671f\u8f68\u8ff9\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u56db\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUniWM \u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u6210\u529f\u7387\uff08\u9ad8\u8fbe 30%\uff09\uff0c\u4e0e\u5f3a\u5927\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8f68\u8ff9\u8bef\u5dee\uff0c\u5e76\u4e14\u5728\u672a\u89c1\u8fc7\u7684 TartanDrive \u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "UniWM \u662f\u671d\u7740\u7edf\u4e00\u7684\u3001\u60f3\u8c61\u9a71\u52a8\u7684\u5177\u8eab\u5bfc\u822a\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2510.08655", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2510.08655", "abs": "https://arxiv.org/abs/2510.08655", "authors": ["Premt Cara", "Kamilia Zaripova", "David Bani-Harouni", "Nassir Navab", "Azade Farshad"], "title": "Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis", "comment": null, "summary": "Rare genetic disease diagnosis faces critical challenges: insufficient\npatient data, inaccessible full genome sequencing, and the immense number of\npossible causative genes. These limitations cause prolonged diagnostic\njourneys, inappropriate treatments, and critical delays, disproportionately\naffecting patients in resource-limited settings where diagnostic tools are\nscarce. We propose RareNet, a subgraph-based Graph Neural Network that requires\nonly patient phenotypes to identify the most likely causal gene and retrieve\nfocused patient subgraphs for targeted clinical investigation. RareNet can\nfunction as a standalone method or serve as a pre-processing or post-processing\nfilter for other candidate gene prioritization methods, consistently enhancing\ntheir performance while potentially enabling explainable insights. Through\ncomprehensive evaluation on two biomedical datasets, we demonstrate competitive\nand robust causal gene prediction and significant performance gains when\nintegrated with other frameworks. By requiring only phenotypic data, which is\nreadily available in any clinical setting, RareNet democratizes access to\nsophisticated genetic analysis, offering particular value for underserved\npopulations lacking advanced genomic infrastructure.", "AI": {"tldr": "RareNet is a subgraph-based GNN that uses patient phenotypes to predict causal genes for rare genetic diseases, addressing limitations of data scarcity and inaccessible sequencing.", "motivation": "The motivation is to overcome challenges in rare genetic disease diagnosis due to insufficient patient data, inaccessible full genome sequencing, and a large number of possible causative genes, which leads to delays and inappropriate treatments, especially in resource-limited settings.", "method": "RareNet, a subgraph-based Graph Neural Network, uses only patient phenotypes to identify the most likely causal gene and retrieve focused patient subgraphs.", "result": "RareNet demonstrates competitive and robust causal gene prediction and significant performance gains when integrated with other frameworks on two biomedical datasets.", "conclusion": "RareNet democratizes access to sophisticated genetic analysis by requiring only phenotypic data, offering particular value for underserved populations."}}
{"id": "2510.08593", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.08593", "abs": "https://arxiv.org/abs/2510.08593", "authors": ["Yuxin Li", "Eng Siong Chng", "Cuntai Guan"], "title": "Hierarchical Self-Supervised Representation Learning for Depression Detection from Speech", "comment": null, "summary": "Speech-based depression detection (SDD) is a promising, non-invasive\nalternative to traditional clinical assessments. However, it remains limited by\nthe difficulty of extracting meaningful features and capturing sparse,\nheterogeneous depressive cues over time. Pretrained self-supervised learning\n(SSL) models such as WavLM provide rich, multi-layer speech representations,\nyet most existing SDD methods rely only on the final layer or search for a\nsingle best-performing one. These approaches often overfit to specific datasets\nand fail to leverage the full hierarchical structure needed to detect subtle\nand persistent depression signals.\n  To address this challenge, we propose HAREN-CTC, a novel architecture that\nintegrates multi-layer SSL features using cross-attention within a multitask\nlearning framework, combined with Connectionist Temporal Classification loss to\nhandle sparse temporal supervision. HAREN-CTC comprises two key modules: a\nHierarchical Adaptive Clustering module that reorganizes SSL features into\ncomplementary embeddings, and a Cross-Modal Fusion module that models\ninter-layer dependencies through cross-attention. The CTC objective enables\nalignment-aware training, allowing the model to track irregular temporal\npatterns of depressive speech cues.\n  We evaluate HAREN-CTC under both an upper-bound setting with standard data\nsplits and a generalization setting using five-fold cross-validation. The model\nachieves state-of-the-art macro F1-scores of 0.81 on DAIC-WOZ and 0.82 on\nMODMA, outperforming prior methods across both evaluation scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u8bed\u97f3\u7684\u6291\u90c1\u75c7\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u7684\u591a\u5c42\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u8fdb\u884c\u6574\u5408\uff0c\u4ee5\u6355\u6349\u7a00\u758f\u548c\u5f02\u6784\u7684\u6291\u90c1\u7ebf\u7d22\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u97f3\u6291\u90c1\u75c7\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u63d0\u53d6\u6709\u610f\u4e49\u7684\u7279\u5f81\uff0c\u5e76\u4e14\u65e0\u6cd5\u6355\u6349\u5230\u968f\u65f6\u95f4\u53d8\u5316\u7684\u7a00\u758f\u548c\u5f02\u6784\u7684\u6291\u90c1\u7ebf\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u8fc7\u5ea6\u62df\u5408\u7279\u5b9a\u6570\u636e\u96c6\uff0c\u5e76\u4e14\u65e0\u6cd5\u5229\u7528\u68c0\u6d4b\u7ec6\u5fae\u548c\u6301\u7eed\u6291\u90c1\u4fe1\u53f7\u6240\u9700\u7684\u5b8c\u6574\u5206\u5c42\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHAREN-CTC\u7684\u65b0\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u96c6\u6210\u4e86\u591a\u5c42SSL\u7279\u5f81\uff0c\u4f7f\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u4e2d\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\uff0c\u5e76\u7ed3\u5408Connectionist Temporal Classification\u635f\u5931\u6765\u5904\u7406\u7a00\u758f\u65f6\u95f4\u76d1\u7763\u3002HAREN-CTC\u5305\u62ec\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u4e00\u4e2a\u5206\u5c42\u81ea\u9002\u5e94\u805a\u7c7b\u6a21\u5757\uff0c\u5c06SSL\u7279\u5f81\u91cd\u7ec4\u4e3a\u4e92\u8865\u5d4c\u5165\uff1b\u4ee5\u53ca\u4e00\u4e2a\u8de8\u6a21\u6001\u878d\u5408\u6a21\u5757\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u5bf9\u5c42\u95f4\u4f9d\u8d56\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\u3002CTC\u76ee\u6807\u5b9e\u73b0\u4e86\u5bf9\u9f50\u611f\u77e5\u8bad\u7ec3\uff0c\u5141\u8bb8\u6a21\u578b\u8ddf\u8e2a\u6291\u90c1\u8bed\u97f3\u7ebf\u7d22\u7684\u4e0d\u89c4\u5219\u65f6\u95f4\u6a21\u5f0f\u3002", "result": "\u5728\u6807\u51c6\u6570\u636e\u5206\u5272\u7684\u4e0a\u754c\u8bbe\u7f6e\u548c\u4f7f\u7528\u4e94\u91cd\u4ea4\u53c9\u9a8c\u8bc1\u7684\u6cdb\u5316\u8bbe\u7f6e\u4e0b\u8bc4\u4f30HAREN-CTC\u3002\u8be5\u6a21\u578b\u5728DAIC-WOZ\u4e0a\u5b9e\u73b0\u4e860.81\u7684state-of-the-art\u5b8fF1\u5206\u6570\uff0c\u5728MODMA\u4e0a\u5b9e\u73b0\u4e860.82\u7684state-of-the-art\u5b8fF1\u5206\u6570\uff0c\u4f18\u4e8e\u4e24\u79cd\u8bc4\u4f30\u65b9\u6848\u4e2d\u7684\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "HAREN-CTC\u662f\u4e00\u79cd\u6709\u6548\u7684\u8bed\u97f3\u6291\u90c1\u75c7\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5b83\u80fd\u591f\u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u7684\u591a\u5c42\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u8fdb\u884c\u6574\u5408\uff0c\u4ee5\u6355\u6349\u7a00\u758f\u548c\u5f02\u6784\u7684\u6291\u90c1\u7ebf\u7d22\u3002"}}
{"id": "2510.09129", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.09129", "abs": "https://arxiv.org/abs/2510.09129", "authors": ["Yansong Wang", "Qihui Lin", "Junjie Huang", "Tao Jia"], "title": "Generative Data Augmentation in Graph Contrastive Learning for Recommendation", "comment": "The 34th ACM International Conference on Information and Knowledge\n  Management", "summary": "Recommendation systems have become indispensable in various online platforms,\nfrom e-commerce to streaming services. A fundamental challenge in this domain\nis learning effective embeddings from sparse user-item interactions. While\ncontrastive learning has recently emerged as a promising solution to this\nissue, generating augmented views for contrastive learning through most\nexisting random data augmentation methods often leads to the alteration of\noriginal semantic information. In this paper, we propose a novel framework,\nGDA4Rec (Generative Data Augmentation in graph contrastive learning for\nRecommendation) to generate high-quality augmented views and provide robust\nself-supervised signals. Specifically, we employ a noise generation module that\nleverages deep generative models to approximate the distribution of original\ndata for data augmentation. Additionally, GDA4Rec further extracts an item\ncomplement matrix to characterize the latent correlations between items and\nprovide additional self-supervised signals. Lastly, a joint objective that\nintegrates recommendation, data augmentation and contrastive learning is used\nto enforce the model to learn more effective and informative embeddings.\nExtensive experiments are conducted on three public datasets to demonstrate the\nsuperiority of the model. The code is available at:\nhttps://github.com/MrYansong/GDA4Rec.", "AI": {"tldr": "GDA4Rec: uses generative models for data augmentation in graph contrastive learning to improve recommendation systems.", "motivation": "Learning effective embeddings from sparse user-item interactions is challenging in recommendation systems.", "method": "A noise generation module leverages deep generative models to approximate the distribution of original data for data augmentation. An item complement matrix is extracted to characterize latent correlations between items. A joint objective integrates recommendation, data augmentation, and contrastive learning.", "result": "Experiments on three public datasets demonstrate the model's superiority.", "conclusion": "GDA4Rec generates high-quality augmented views and provides robust self-supervised signals for recommendation systems."}}
{"id": "2510.08628", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08628", "abs": "https://arxiv.org/abs/2510.08628", "authors": ["Siiri Lepp\u00e4lampi", "Sonja M. Hyrynsalmi", "Erno Vanhala"], "title": "The Digital Mirror: Gender Bias and Occupational Stereotypes in AI-Generated Images", "comment": null, "summary": "Generative AI offers vast opportunities for creating visualisations, such as\ngraphics, videos, and images. However, recent studies around AI-generated\nvisualisations have primarily focused on the creation process and image\nquality, overlooking representational biases. This study addresses this gap by\ntesting representation biases in AI-generated pictures in an occupational\nsetting and evaluating how two AI image generator tools, DALL-E 3 and Ideogram,\ncompare. Additionally, the study discusses topics such as ageing and emotions\nin AI-generated images. As AI image tools are becoming more widely used,\naddressing and mitigating harmful gender biases becomes essential to ensure\ndiverse representation in media and professional settings. In this study, over\n750 AI-generated images of occupations were prompted. The thematic analysis\nresults revealed that both DALL-E 3 and Ideogram reinforce traditional gender\nstereotypes in AI-generated images, although to varying degrees. These findings\nemphasise that AI visualisation tools risk reinforcing narrow representations.\nIn our discussion section, we propose suggestions for practitioners,\nindividuals and researchers to increase representation when generating images\nwith visible genders.", "AI": {"tldr": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86AI\u751f\u6210\u7684\u56fe\u50cf\u4e2d\u5b58\u5728\u7684\u8868\u5f81\u504f\u5dee\uff0c\u7279\u522b\u662f\u5728\u804c\u4e1a\u573a\u666f\u4e2d\u3002", "motivation": "\u73b0\u6709\u7684AI\u751f\u6210\u53ef\u89c6\u5316\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u751f\u6210\u8fc7\u7a0b\u548c\u56fe\u50cf\u8d28\u91cf\uff0c\u5ffd\u7565\u4e86\u8868\u5f81\u504f\u5dee\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u672c\u7814\u7a76\u65e8\u5728\u6d4b\u8bd5AI\u751f\u6210\u56fe\u50cf\u4e2d\u7684\u8868\u5f81\u504f\u5dee\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u4e86DALL-E 3\u548cIdeogram\u4e24\u4e2aAI\u56fe\u50cf\u751f\u6210\u5de5\u5177\uff0c\u751f\u6210\u4e86\u8d85\u8fc7750\u5f20\u804c\u4e1a\u76f8\u5173\u7684\u56fe\u50cf\uff0c\u5e76\u8fdb\u884c\u4e86\u4e3b\u9898\u5206\u6790\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cDALL-E 3\u548cIdeogram\u90fd\u5728\u4e0d\u540c\u7a0b\u5ea6\u4e0a\u5f3a\u5316\u4e86AI\u751f\u6210\u56fe\u50cf\u4e2d\u4f20\u7edf\u7684\u6027\u522b\u523b\u677f\u5370\u8c61\u3002", "conclusion": "AI\u53ef\u89c6\u5316\u5de5\u5177\u5b58\u5728\u5f3a\u5316\u72ed\u9698\u8868\u5f81\u7684\u98ce\u9669\u3002\u7814\u7a76\u4e3a\u4ece\u4e1a\u8005\u3001\u4e2a\u4eba\u548c\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u589e\u52a0\u56fe\u50cf\u8868\u5f81\u7684\u5efa\u8bae\u3002"}}
{"id": "2510.08755", "categories": ["cs.AI", "cs.CL", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.08755", "abs": "https://arxiv.org/abs/2510.08755", "authors": ["Pantea Karimi", "Dany Rouhana", "Pooria Namyar", "Siva Kesava Reddy Kakarla", "Venkat Arun", "Behnaz Arzani"], "title": "Robust Heuristic Algorithm Design with LLMs", "comment": null, "summary": "We posit that we can generate more robust and performant heuristics if we\naugment approaches using LLMs for heuristic design with tools that explain why\nheuristics underperform and suggestions about how to fix them. We find even\nsimple ideas that (1) expose the LLM to instances where the heuristic\nunderperforms; (2) explain why they occur; and (3) specialize design to regions\nin the input space, can produce more robust algorithms compared to existing\ntechniques~ -- ~the heuristics we produce have a $\\sim28\\times$ better\nworst-case performance compared to FunSearch, improve average performance, and\nmaintain the runtime.", "AI": {"tldr": "\u4f7f\u7528LLM\u901a\u8fc7\u89e3\u91ca\u4e3a\u4ec0\u4e48\u542f\u53d1\u5f0f\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\u4ee5\u53ca\u5173\u4e8e\u5982\u4f55\u4fee\u590d\u5b83\u4eec\u7684\u5efa\u8bae\u6765\u589e\u5f3a\u542f\u53d1\u5f0f\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u53ef\u4ee5\u751f\u6210\u66f4\u7a33\u5065\u548c\u9ad8\u6027\u80fd\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "motivation": "\u5229\u7528LLM\u8bbe\u8ba1\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "method": "1. \u5c06LLM\u66b4\u9732\u4e8e\u542f\u53d1\u5f0f\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\u7684\u5b9e\u4f8b\uff1b2. \u89e3\u91ca\u5b83\u4eec\u53d1\u751f\u7684\u539f\u56e0\uff1b3. \u4e13\u95e8\u9488\u5bf9\u8f93\u5165\u7a7a\u95f4\u4e2d\u7684\u533a\u57df\u8fdb\u884c\u8bbe\u8ba1\u3002", "result": "\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\uff0c\u6240\u4ea7\u751f\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002\u4e0eFunSearch\u76f8\u6bd4\uff0c\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u63d0\u9ad8\u4e86\u7ea628\u500d\uff0c\u63d0\u9ad8\u4e86\u5e73\u5747\u6027\u80fd\uff0c\u5e76\u4fdd\u6301\u4e86\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528LLM\u589e\u5f3a\u542f\u53d1\u5f0f\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u53ef\u4ee5\u751f\u6210\u66f4\u7a33\u5065\u548c\u9ad8\u6027\u80fd\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002"}}
{"id": "2510.08657", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08657", "abs": "https://arxiv.org/abs/2510.08657", "authors": ["Zipo Jibao", "Yingyi Fu", "Xinyang Chen", "Guoting Chen"], "title": "Inner-Instance Normalization for Time Series Forecasting", "comment": null, "summary": "Real-world time series are influenced by numerous factors and exhibit complex\nnon-stationary characteristics. Non-stationarity can lead to distribution\nshifts, where the statistical properties of time series change over time,\nnegatively impacting model performance. Several instance normalization\ntechniques have been proposed to address distribution shifts in time series\nforecasting. However, existing methods fail to account for shifts within\nindividual instances, leading to suboptimal performance. To tackle\ninner-instance distribution shifts, we propose two novel point-level methods:\nLearning Distribution (LD) and Learning Conditional Distribution (LCD). LD\neliminates internal discrepancies by fitting the internal distribution of input\nand output with different parameters at different time steps, while LCD\nutilizes neural networks to predict scaling coefficients of the output. We\nevaluate the performance of the two methods with various backbone models across\npublic benchmarks and demonstrate the effectiveness of the point-level paradigm\nthrough comparative experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e3\u51b3\u5b9e\u4f8b\u5185\u90e8\u7684\u5206\u5e03\u504f\u79fb\u6765\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u65f6\u95f4\u5e8f\u5217\u53d7\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\uff0c\u8868\u73b0\u51fa\u590d\u6742\u7684\u975e\u5e73\u7a33\u7279\u6027\uff0c\u5bfc\u81f4\u5206\u5e03\u504f\u79fb\uff0c\u4ece\u800c\u5bf9\u6a21\u578b\u6027\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u8003\u8651\u5355\u4e2a\u5b9e\u4f8b\u5185\u90e8\u7684\u504f\u79fb\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u70b9\u7ea7\u65b9\u6cd5\uff1a\u5b66\u4e60\u5206\u5e03\uff08LD\uff09\u548c\u5b66\u4e60\u6761\u4ef6\u5206\u5e03\uff08LCD\uff09\u3002LD\u901a\u8fc7\u5728\u4e0d\u540c\u65f6\u95f4\u6b65\u957f\u4f7f\u7528\u4e0d\u540c\u7684\u53c2\u6570\u62df\u5408\u8f93\u5165\u548c\u8f93\u51fa\u7684\u5185\u90e8\u5206\u5e03\u6765\u6d88\u9664\u5185\u90e8\u5dee\u5f02\uff0c\u800cLCD\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u6765\u9884\u6d4b\u8f93\u51fa\u7684\u7f29\u653e\u7cfb\u6570\u3002", "result": "\u5728\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u5404\u79cd\u9aa8\u5e72\u6a21\u578b\u8bc4\u4f30\u4e86\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u6bd4\u8f83\u5b9e\u9a8c\u8bc1\u660e\u4e86\u70b9\u7ea7\u8303\u5f0f\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u70b9\u7ea7\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u5b9e\u4f8b\u5185\u90e8\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.08595", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08595", "abs": "https://arxiv.org/abs/2510.08595", "authors": ["V. S. Raghu Parupudi"], "title": "Systematic Diagnosis of Brittle Reasoning in Large Language Models", "comment": "Submitted to NEURIPS-2025 MATHAI workshop", "summary": "A central question in artificial intelligence is the extent to which machine\nlearning models comprehend mathematics. To address this, we propose a novel\nframework for measuring mathematical reasoning that moves beyond standard\nbenchmarks to diagnose specific failure points. Our method first generates\nstructured, step-by-step reasoning from gpt-3.5-turbo on the GSM8K dataset. We\nthen use a more capable analyst model, gpt-4o-mini, to categorize errors and,\ncrucially, perform an unsupervised clustering of every reasoning sentence to\nidentify emergent \"reasoning modes.\" This analysis reveals a cognitive profile\nwith a stark, nonhuman-like brittleness: while the model achieves near-perfect\naccuracy on procedural modes like sequential calculation, its performance on\nmodes requiring combinatorial reasoning with restrictions plummets. By\nidentifying and quantifying the reliability of these distinct reasoning skills,\nour work provides a more granular method to evaluate mathematical comprehension\nand offers a precise roadmap for developing new capabilities and more reliable\nfuture applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u5b66\u63a8\u7406\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5728\u4e0d\u540c\u63a8\u7406\u6a21\u5f0f\u4e0b\u7684\u8868\u73b0\u6765\u8bca\u65ad\u5176\u5f31\u70b9\u3002", "motivation": "\u8861\u91cf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5bf9\u6570\u5b66\u7684\u7406\u89e3\u7a0b\u5ea6\u3002", "method": "\u5229\u7528 gpt-3.5-turbo \u751f\u6210 GSM8K \u6570\u636e\u96c6\u7684\u9010\u6b65\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u4f7f\u7528 gpt-4o-mini \u5bf9\u9519\u8bef\u8fdb\u884c\u5206\u7c7b\uff0c\u5bf9\u63a8\u7406\u53e5\u5b50\u8fdb\u884c\u65e0\u76d1\u7763\u805a\u7c7b\uff0c\u8bc6\u522b\u201c\u63a8\u7406\u6a21\u5f0f\u201d\u3002", "result": "\u6a21\u578b\u5728\u7a0b\u5e8f\u6027\u6a21\u5f0f\uff08\u5982\u987a\u5e8f\u8ba1\u7b97\uff09\u4e0a\u8868\u73b0\u63a5\u8fd1\u5b8c\u7f8e\uff0c\u4f46\u5728\u9700\u8981\u7ec4\u5408\u63a8\u7406\u7684\u6a21\u5f0f\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u63ed\u793a\u4e86\u4e00\u79cd\u975e\u4eba\u7c7b\u7684\u8106\u5f31\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7cbe\u7ec6\u7684\u6570\u5b66\u7406\u89e3\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u4e3a\u5f00\u53d1\u65b0\u80fd\u529b\u548c\u66f4\u53ef\u9760\u7684\u672a\u6765\u5e94\u7528\u63d0\u4f9b\u4e86\u7cbe\u786e\u7684\u8def\u7ebf\u56fe\u3002"}}
{"id": "2510.09136", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09136", "abs": "https://arxiv.org/abs/2510.09136", "authors": ["Marlene Holzleitner", "Stephan Leitner", "Hanna Lind Jorgensen", "Christoph Schmitz", "Jacob Welander", "Dietmar Jannach"], "title": "Controlled Personalization in Legacy Media Online Services: A Case Study in News Recommendation", "comment": null, "summary": "Personalized news recommendations have become a standard feature of large\nnews aggregation services, optimizing user engagement through automated content\nselection. In contrast, legacy news media often approach personalization\ncautiously, striving to balance technological innovation with core editorial\nvalues. As a result, online platforms of traditional news outlets typically\ncombine editorially curated content with algorithmically selected articles - a\nstrategy we term controlled personalization. In this industry paper, we\nevaluate the effectiveness of controlled personalization through an A/B test\nconducted on the website of a major Norwegian legacy news organization. Our\nfindings indicate that even a modest level of personalization yields\nsubstantial benefits. Specifically, we observe that users exposed to\npersonalized content demonstrate higher click-through rates and reduced\nnavigation effort, suggesting improved discovery of relevant content. Moreover,\nour analysis reveals that controlled personalization contributes to greater\ncontent diversity and catalog coverage and in addition reduces popularity bias.\nOverall, our results suggest that controlled personalization can successfully\nalign user needs with editorial goals, offering a viable path for legacy media\nto adopt personalization technologies while upholding journalistic values.", "AI": {"tldr": "\u4f20\u7edf\u65b0\u95fb\u5a92\u4f53\u5728\u4f18\u5316\u7528\u6237\u53c2\u4e0e\u5ea6\u7684\u540c\u65f6\uff0c\u8c28\u614e\u5730\u91c7\u7528\u4e2a\u6027\u5316\u65b0\u95fb\u63a8\u8350\uff0c\u5e76\u5c06\u7f16\u8f91\u7b56\u5212\u5185\u5bb9\u4e0e\u7b97\u6cd5\u9009\u62e9\u7684\u6587\u7ae0\u76f8\u7ed3\u5408\uff0c\u6211\u4eec\u79f0\u4e4b\u4e3a\u201c\u53d7\u63a7\u4e2a\u6027\u5316\u201d\u3002", "motivation": "\u4f20\u7edf\u65b0\u95fb\u5a92\u4f53\u5e0c\u671b\u5728\u6280\u672f\u521b\u65b0\u4e0e\u6838\u5fc3\u7f16\u8f91\u4ef7\u503c\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u56e0\u6b64\u5728\u4e2a\u6027\u5316\u65b0\u95fb\u63a8\u8350\u65b9\u9762\u6301\u8c28\u614e\u6001\u5ea6\u3002", "method": "\u901a\u8fc7\u5728\u632a\u5a01\u4e00\u5bb6\u5927\u578b\u4f20\u7edf\u65b0\u95fb\u673a\u6784\u7684\u7f51\u7ad9\u4e0a\u8fdb\u884c\u7684 A/B \u6d4b\u8bd5\uff0c\u8bc4\u4f30\u53d7\u63a7\u4e2a\u6027\u5316\u7684\u6709\u6548\u6027\u3002", "result": "\u5373\u4f7f\u662f\u9002\u5ea6\u7684\u4e2a\u6027\u5316\u4e5f\u80fd\u5e26\u6765\u663e\u8457\u7684\u597d\u5904\uff0c\u70b9\u51fb\u7387\u66f4\u9ad8\uff0c\u5bfc\u822a\u5de5\u4f5c\u91cf\u66f4\u5c11\uff0c\u5185\u5bb9\u591a\u6837\u6027\u66f4\u5927\uff0c\u53d7\u6b22\u8fce\u7a0b\u5ea6\u504f\u5dee\u66f4\u5c0f\u3002", "conclusion": "\u53d7\u63a7\u4e2a\u6027\u5316\u53ef\u4ee5\u6210\u529f\u5730\u5c06\u7528\u6237\u9700\u6c42\u4e0e\u7f16\u8f91\u76ee\u6807\u7ed3\u5408\u8d77\u6765\uff0c\u4e3a\u4f20\u7edf\u5a92\u4f53\u91c7\u7528\u4e2a\u6027\u5316\u6280\u672f\u540c\u65f6\u575a\u6301\u65b0\u95fb\u4ef7\u503c\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u7684\u9014\u5f84\u3002"}}
{"id": "2510.08629", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08629", "abs": "https://arxiv.org/abs/2510.08629", "authors": ["Jort Vincenti", "Metod Jazbec", "Guoxuan Xia"], "title": "Dynamic Mixture-of-Experts for Visual Autoregressive Model", "comment": null, "summary": "Visual Autoregressive Models (VAR) offer efficient and high-quality image\ngeneration but suffer from computational redundancy due to repeated Transformer\ncalls at increasing resolutions. We introduce a dynamic Mixture-of-Experts\nrouter integrated into VAR. The new architecture allows to trade compute for\nquality through scale-aware thresholding. This thresholding strategy balances\nexpert selection based on token complexity and resolution, without requiring\nadditional training. As a result, we achieve 20% fewer FLOPs, 11% faster\ninference and match the image quality achieved by the dense baseline.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6df7\u5408\u4e13\u5bb6\u8def\u7531\u5668\uff0c\u96c6\u6210\u5230\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\uff08VAR\uff09\u4e2d\uff0c\u4ee5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u56fe\u50cf\u8d28\u91cf\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002", "motivation": "\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\uff08VAR\uff09\u867d\u7136\u80fd\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u4f46\u7531\u4e8e\u5728\u4e0d\u65ad\u63d0\u9ad8\u7684\u5206\u8fa8\u7387\u4e0b\u91cd\u590d\u8c03\u7528 Transformer\uff0c\u5b58\u5728\u8ba1\u7b97\u5197\u4f59\u3002", "method": "\u5f15\u5165\u52a8\u6001\u6df7\u5408\u4e13\u5bb6\u8def\u7531\u5668\uff0c\u901a\u8fc7\u611f\u77e5\u5c3a\u5ea6\u7684\u9608\u503c\u7b56\u7565\uff0c\u6839\u636e token \u590d\u6742\u5ea6\u548c\u5206\u8fa8\u7387\u5e73\u8861\u4e13\u5bb6\u9009\u62e9\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5b9e\u73b0\u4e86 20% \u7684 FLOPs \u51cf\u5c11\u548c 11% \u7684\u63a8\u7406\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u5bc6\u96c6\u57fa\u7ebf\u76f8\u5f53\u7684\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u635f\u5931\u56fe\u50cf\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u89c9\u81ea\u56de\u5f52\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2510.08790", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08790", "abs": "https://arxiv.org/abs/2510.08790", "authors": ["Guangya Wan", "Mingyang Ling", "Xiaoqi Ren", "Rujun Han", "Sheng Li", "Zizhao Zhang"], "title": "COMPASS: Enhancing Agent Long-Horizon Reasoning with Evolving Context", "comment": "Under Review for ACL", "summary": "Long-horizon tasks that require sustained reasoning and multiple tool\ninteractions remain challenging for LLM agents: small errors compound across\nsteps, and even state-of-the-art models often hallucinate or lose coherence. We\nidentify context management as the central bottleneck -- extended histories\ncause agents to overlook critical evidence or become distracted by irrelevant\ninformation, thus failing to replan or reflect from previous mistakes. To\naddress this, we propose COMPASS (Context-Organized Multi-Agent Planning and\nStrategy System), a lightweight hierarchical framework that separates tactical\nexecution, strategic oversight, and context organization into three specialized\ncomponents: (1) a Main Agent that performs reasoning and tool use, (2) a\nMeta-Thinker that monitors progress and issues strategic interventions, and (3)\na Context Manager that maintains concise, relevant progress briefs for\ndifferent reasoning stages. Across three challenging benchmarks -- GAIA,\nBrowseComp, and Humanity's Last Exam -- COMPASS improves accuracy by up to 20%\nrelative to both single- and multi-agent baselines. We further introduce a\ntest-time scaling extension that elevates performance to match established\nDeepResearch agents, and a post-training pipeline that delegates context\nmanagement to smaller models for enhanced efficiency.", "AI": {"tldr": "LLM agents struggle with long-horizon tasks due to context management issues.", "motivation": "Long-horizon tasks requiring reasoning and tool interactions are challenging for LLM agents due to error compounding and incoherence. Context management is identified as the bottleneck.", "method": "The paper proposes COMPASS, a hierarchical framework with a Main Agent, Meta-Thinker, and Context Manager to address context management issues.", "result": "COMPASS improves accuracy by up to 20% on GAIA, BrowseComp, and Humanity's Last Exam benchmarks. A scaling extension matches DeepResearch agents, and a post-training pipeline enhances efficiency.", "conclusion": "COMPASS, a lightweight hierarchical framework, effectively addresses context management issues in LLM agents for long-horizon tasks."}}
{"id": "2510.08659", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08659", "abs": "https://arxiv.org/abs/2510.08659", "authors": ["Yuni Lai", "Xiaoyu Xue", "Linghui Shen", "Yulun Wu", "Gaolei Li", "Song Guo", "Kai Zhou", "Bin Xiao"], "title": "Provably Robust Adaptation for Language-Empowered Foundation Models", "comment": "19 pages", "summary": "Language-empowered foundation models (LeFMs), such as CLIP and GraphCLIP,\nhave transformed multimodal learning by aligning visual (or graph) features\nwith textual representations, enabling powerful downstream capabilities like\nfew-shot learning. However, the reliance on small, task-specific support\ndatasets collected in open environments exposes these models to poisoning\nattacks, where adversaries manipulate the support samples to degrade\nperformance. Existing defenses rely on empirical strategies, which lack formal\nguarantees and remain vulnerable to unseen and adaptive attacks. Certified\nrobustness offers provable guarantees but has been largely unexplored for\nfew-shot classifiers based on LeFMs. This study seeks to fill these critical\ngaps by proposing the first provably robust few-shot classifier that is\ntailored for LeFMs. We term our model Language-empowered Few-shot Certification\n(\\textbf{LeFCert}). It integrates both textual and feature embeddings with an\nadaptive blending mechanism. To achieve provable robustness, we propose a\ntwofold trimmed mean prototype and derive provable upper and lower bounds for\nclassification scores, enabling certification under worst-case poisoning\nscenarios. To further enhance the performance, we extend LeFCert with two\nvariants by considering a more realistic and tighter attack budget: LeFCert-L\nincorporates randomized smoothing to provide Lipschitz continuity and derive\nrobustness under dual budget constraints, and LeFCert-C provides collective\ncertification for scenarios where attackers distribute a shared poisoning\nbudget across multiple samples. Experiments demonstrate that LeFCert achieves\nstate-of-the-art performance, significantly improving both clean and certified\naccuracy compared to existing baselines. Despite its advanced robustness\nmechanisms, LeFCert is computationally efficient, making it practical for\nreal-world applications.", "AI": {"tldr": "This paper introduces LeFCert, a provably robust few-shot classifier for language-empowered foundation models (LeFMs), which addresses vulnerability to poisoning attacks. It also introduces two variants LeFCert-L and LeFCert-C to enhance performance under different attack scenarios.", "motivation": "Existing LeFMs are vulnerable to poisoning attacks due to their reliance on small, task-specific support datasets. Current defenses lack formal guarantees. Certified robustness is largely unexplored for few-shot classifiers based on LeFMs.", "method": "The paper proposes LeFCert, which integrates textual and feature embeddings with an adaptive blending mechanism. It uses a twofold trimmed mean prototype and derives provable upper and lower bounds for classification scores. LeFCert-L incorporates randomized smoothing, and LeFCert-C provides collective certification.", "result": "LeFCert achieves state-of-the-art performance, significantly improving both clean and certified accuracy compared to existing baselines. It is also computationally efficient.", "conclusion": "LeFCert provides a practical and robust solution for few-shot classification with LeFMs, addressing the vulnerability to poisoning attacks with provable guarantees and computational efficiency."}}
{"id": "2510.08596", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08596", "abs": "https://arxiv.org/abs/2510.08596", "authors": ["V. S. Raghu Parupudi"], "title": "Confidence, Not Perplexity: A Better Metric for the Creative Era of LLMs", "comment": "Submitted to AACL-IJCNLP 2025 (Eval4NLP)", "summary": "Reference-free metrics like self-perplexity are strongly biased against\ncreative text generation. We propose the Confidence Score (CS), derived from a\nmodel's output probability distribution, as a less biased alternative.\nExperiments on gpt-4o-mini show that while fluency-based metrics prefer novel\nresponses in 0\\% of cases on 99 creative prompts, our CS does so 19% of the\ntime, a statistically significant difference (95% CI for difference: [11.1%,\n27.3%]). We also show that CS effectively distinguishes between easy, medium,\nand hard tasks, confirmed by non-overlapping confidence intervals. The\nConfidence Score thus mitigates the creativity bias of traditional metrics\nwhile retaining their core evaluative strengths, offering a more balanced\nassessment for modern LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7f6e\u4fe1\u5ea6\u5206\u6570\uff08CS\uff09\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65e0\u53c2\u8003\u6307\u6807\u5728\u8bc4\u4f30\u521b\u9020\u6027\u6587\u672c\u751f\u6210\u65f6\u5b58\u5728\u7684\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u65e0\u53c2\u8003\u6307\u6807\uff08\u5982\u81ea\u56f0\u60d1\u5ea6\uff09\u5728\u8bc4\u4f30\u521b\u9020\u6027\u6587\u672c\u751f\u6210\u65f6\u5b58\u5728\u4e25\u91cd\u7684\u504f\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u7f6e\u4fe1\u5ea6\u5206\u6570\uff08CS\uff09\uff0c\u8be5\u5206\u6570\u6e90\u4e8e\u6a21\u578b\u7684\u8f93\u51fa\u6982\u7387\u5206\u5e03\uff0c\u4f5c\u4e3a\u4e00\u79cd\u504f\u5dee\u8f83\u5c0f\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u5728 gpt-4o-mini \u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u6d41\u7545\u6027\u7684\u6307\u6807\u5728 99 \u4e2a\u521b\u9020\u6027\u63d0\u793a\u4e2d\uff0c\u4ec5\u5728 0% \u7684\u60c5\u51b5\u4e0b\u504f\u7231\u65b0\u9896\u7684\u54cd\u5e94\uff0c\u800c CS \u5728 19% \u7684\u60c5\u51b5\u4e0b\u504f\u7231\u65b0\u9896\u7684\u54cd\u5e94\uff0c\u5dee\u5f02\u5177\u6709\u7edf\u8ba1\u5b66\u610f\u4e49\uff0895% CI \u4e3a [11.1%, 27.3%]\uff09\u3002CS \u8fd8\u53ef\u4ee5\u6709\u6548\u533a\u5206\u7b80\u5355\u3001\u4e2d\u7b49\u548c\u56f0\u96be\u7684\u4efb\u52a1\u3002", "conclusion": "\u7f6e\u4fe1\u5ea6\u5206\u6570\u7f13\u89e3\u4e86\u4f20\u7edf\u6307\u6807\u7684\u521b\u9020\u6027\u504f\u5dee\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5176\u6838\u5fc3\u8bc4\u4f30\u4f18\u52bf\uff0c\u4e3a\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5e73\u8861\u7684\u8bc4\u4f30\u3002"}}
{"id": "2510.09167", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.09167", "abs": "https://arxiv.org/abs/2510.09167", "authors": ["Minmao Wang", "Xingchen Liu", "Shijie Yi", "Likang Wu", "Hongke Zhao", "Fei Pan", "Qingpeng Cai", "Peng Jiang"], "title": "Hierarchical Semantic RL: Tackling the Problem of Dynamic Action Space for RL-based Recommendations", "comment": null, "summary": "Recommender Systems (RS) are fundamental to modern online services. While\nmost existing approaches optimize for short-term engagement, recent work has\nbegun to explore reinforcement learning (RL) to model long-term user value.\nHowever, these efforts face significant challenges due to the vast, dynamic\naction spaces inherent in recommendation, which hinder stable policy learning.\nTo resolve this bottleneck, we introduce Hierarchical Semantic RL (HSRL), which\nreframes RL-based recommendation over a fixed Semantic Action Space (SAS). HSRL\nencodes items as Semantic IDs (SIDs) for policy learning, and maps SIDs back to\ntheir original items via a fixed, invertible lookup during execution. To align\ndecision-making with SID generation, the Hierarchical Policy Network (HPN)\noperates in a coarse-to-fine manner, employing hierarchical residual state\nmodeling to refine each level's context from the previous level's residual,\nthereby stabilizing training and reducing representation-decision mismatch. In\nparallel, a Multi-level Critic (MLC) provides token-level value estimates,\nenabling fine-grained credit assignment. Across public benchmarks and a\nlarge-scale production dataset from a leading Chinese short-video advertising\nplatform, HSRL consistently surpasses state-of-the-art baselines. In online\ndeployment over a seven-day A/B testing, it delivers an 18.421% CVR lift with\nonly a 1.251% increase in cost, supporting HSRL as a scalable paradigm for\nRL-based recommendation. Our code is released at\nhttps://github.com/MinmaoWang/HSRL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a8\u8350\u7cfb\u7edf\u65b9\u6cd5\uff0c\u79f0\u4e3a\u5206\u5c42\u8bed\u4e49\u5f3a\u5316\u5b66\u4e60 (HSRL)\uff0c\u8be5\u65b9\u6cd5\u5728\u56fa\u5b9a\u7684\u8bed\u4e49\u52a8\u4f5c\u7a7a\u95f4 (SAS) \u4e0a\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u63a8\u8350\u4e2d\u9762\u4e34\u7684\u5de8\u5927\u52a8\u6001\u52a8\u4f5c\u7a7a\u95f4\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u8350\u7cfb\u7edf\u65b9\u6cd5\u4e3b\u8981\u4f18\u5316\u77ed\u671f\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u800c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u7531\u4e8e\u63a8\u8350\u4e2d\u56fa\u6709\u7684\u5de8\u5927\u52a8\u6001\u52a8\u4f5c\u7a7a\u95f4\u800c\u9762\u4e34\u6311\u6218\uff0c\u8fd9\u963b\u788d\u4e86\u7a33\u5b9a\u7684\u7b56\u7565\u5b66\u4e60\u3002", "method": "HSRL \u5c06\u7269\u54c1\u7f16\u7801\u4e3a\u8bed\u4e49 ID (SID) \u4ee5\u8fdb\u884c\u7b56\u7565\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u56fa\u5b9a\u7684\u53ef\u9006\u67e5\u627e\u5c06 SID \u6620\u5c04\u56de\u5176\u539f\u59cb\u7269\u54c1\u3002\u5206\u5c42\u7b56\u7565\u7f51\u7edc (HPN) \u4ee5\u7c97\u5230\u7cbe\u7684\u65b9\u5f0f\u8fd0\u884c\uff0c\u91c7\u7528\u5206\u5c42\u6b8b\u5dee\u72b6\u6001\u5efa\u6a21\u6765\u7ec6\u5316\u6bcf\u4e2a\u7ea7\u522b\u7684\u4e0a\u4e0b\u6587\u3002\u591a\u7ea7\u8bc4\u8bba\u5bb6 (MLC) \u63d0\u4f9b token \u7ea7\u522b\u7684\u4ef7\u503c\u4f30\u8ba1\uff0c\u4ece\u800c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u4fe1\u7528\u5206\u914d\u3002", "result": "\u5728\u516c\u5171\u57fa\u51c6\u548c\u4e00\u4e2a\u6765\u81ea\u4e2d\u56fd\u9886\u5148\u7684\u77ed\u89c6\u9891\u5e7f\u544a\u5e73\u53f0\u7684\u5927\u89c4\u6a21\u751f\u4ea7\u6570\u636e\u96c6\u4e0a\uff0cHSRL \u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u3002\u5728\u4e3a\u671f\u4e03\u5929\u7684 A/B \u6d4b\u8bd5\u7684\u5728\u7ebf\u90e8\u7f72\u4e2d\uff0c\u5b83\u5b9e\u73b0\u4e86 18.421% \u7684 CVR \u63d0\u5347\uff0c\u800c\u6210\u672c\u4ec5\u589e\u52a0\u4e86 1.251%\u3002", "conclusion": "HSRL \u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a8\u8350\u8303\u4f8b\u3002"}}
{"id": "2510.08631", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08631", "abs": "https://arxiv.org/abs/2510.08631", "authors": ["Hanieh Shojaei Miandashti", "Claus Brenner"], "title": "Out-of-Distribution Detection in LiDAR Semantic Segmentation Using Epistemic Uncertainty from Hierarchical GMMs", "comment": null, "summary": "In addition to accurate scene understanding through precise semantic\nsegmentation of LiDAR point clouds, detecting out-of-distribution (OOD)\nobjects, instances not encountered during training, is essential to prevent the\nincorrect assignment of unknown objects to known classes. While supervised OOD\ndetection methods depend on auxiliary OOD datasets, unsupervised methods avoid\nthis requirement but typically rely on predictive entropy, the entropy of the\npredictive distribution obtained by averaging over an ensemble or multiple\nposterior weight samples. However, these methods often conflate epistemic\n(model) and aleatoric (data) uncertainties, misclassifying ambiguous in\ndistribution regions as OOD. To address this issue, we present an unsupervised\nOOD detection approach that employs epistemic uncertainty derived from\nhierarchical Bayesian modeling of Gaussian Mixture Model (GMM) parameters in\nthe feature space of a deep neural network. Without requiring auxiliary data or\nadditional training stages, our approach outperforms existing uncertainty-based\nmethods on the SemanticKITTI dataset, achieving an 18\\% improvement in AUROC,\n22\\% increase in AUPRC, and 36\\% reduction in FPR95 (from 76\\% to 40\\%),\ncompared to the predictive entropy approach used in prior works.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5206\u5c42\u8d1d\u53f6\u65af\u5efa\u6a21\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u53c2\u6570\u7684\u65e0\u4eba\u76d1\u7763OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728SemanticKITTI\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u3002", "motivation": "\u7cbe\u786e\u7684\u8bed\u4e49\u5206\u5272\u5bf9\u4e8e\u51c6\u786e\u7684\u573a\u666f\u7406\u89e3\u81f3\u5173\u91cd\u8981\uff0c\u68c0\u6d4b\u8bad\u7ec3\u671f\u95f4\u672a\u9047\u5230\u7684\u5206\u5e03\u5916\uff08OOD\uff09\u5bf9\u8c61\u5bf9\u4e8e\u9632\u6b62\u5c06\u672a\u77e5\u5bf9\u8c61\u9519\u8bef\u5206\u914d\u7ed9\u5df2\u77e5\u7c7b\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u6216\u8005\u4f9d\u8d56\u4e8e\u8f85\u52a9OOD\u6570\u636e\u96c6\uff0c\u6216\u8005\u5c06\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u548c\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u6df7\u6dc6\u3002", "method": "\u91c7\u7528\u4ece\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7279\u5f81\u7a7a\u95f4\u4e2d\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u53c2\u6570\u7684\u5206\u5c42\u8d1d\u53f6\u65af\u5efa\u6a21\u4e2d\u83b7\u5f97\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728SemanticKITTI\u6570\u636e\u96c6\u4e0a\uff0cAUROC\u63d0\u9ad8\u4e8618%\uff0cAUPRC\u63d0\u9ad8\u4e8622%\uff0cFPR95\u964d\u4f4e\u4e8636%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u9700\u8981\u8f85\u52a9\u6570\u636e\u6216\u989d\u5916\u7684\u8bad\u7ec3\u9636\u6bb5\uff0c\u5e76\u4e14\u4f18\u4e8e\u5148\u524d\u5de5\u4f5c\u4e2d\u4f7f\u7528\u8fc7\u7684\u9884\u6d4b\u71b5\u65b9\u6cd5\u3002"}}
{"id": "2510.08831", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.08831", "abs": "https://arxiv.org/abs/2510.08831", "authors": ["Wouter Haverals", "Meredith Martin"], "title": "Everyone prefers human writers, including AI", "comment": "46 pages, 18 figures (5 main text + 13 supplementary), 5 tables", "summary": "As AI writing tools become widespread, we need to understand how both humans\nand machines evaluate literary style, a domain where objective standards are\nelusive and judgments are inherently subjective. We conducted controlled\nexperiments using Raymond Queneau's Exercises in Style (1947) to measure\nattribution bias across evaluators. Study 1 compared human participants (N=556)\nand AI models (N=13) evaluating literary passages from Queneau versus\nGPT-4-generated versions under three conditions: blind, accurately labeled, and\ncounterfactually labeled. Study 2 tested bias generalization across a\n14$\\times$14 matrix of AI evaluators and creators. Both studies revealed\nsystematic pro-human attribution bias. Humans showed +13.7 percentage point\n(pp) bias (Cohen's h = 0.28, 95% CI: 0.21-0.34), while AI models showed +34.3\npercentage point bias (h = 0.70, 95% CI: 0.65-0.76), a 2.5-fold stronger effect\n(P$<$0.001). Study 2 confirmed this bias operates across AI architectures\n(+25.8pp, 95% CI: 24.1-27.6%), demonstrating that AI systems systematically\ndevalue creative content when labeled as \"AI-generated\" regardless of which AI\ncreated it. We also find that attribution labels cause evaluators to invert\nassessment criteria, with identical features receiving opposing evaluations\nbased solely on perceived authorship. This suggests AI models have absorbed\nhuman cultural biases against artificial creativity during training. Our study\nrepresents the first controlled comparison of attribution bias between human\nand artificial evaluators in aesthetic judgment, revealing that AI systems not\nonly replicate but amplify this human tendency.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u8bc4\u4f30\u6587\u5b66\u98ce\u683c\u65f6\uff0c\u4eba\u7c7b\u548cAI\u90fd\u5b58\u5728\u504f\u89c1\uff0cAI\u7684\u504f\u89c1\u7a0b\u5ea6\u751a\u81f3\u662f\u4eba\u7c7b\u76842.5\u500d\u3002\u5f53\u5185\u5bb9\u88ab\u6807\u8bb0\u4e3a\u201cAI\u751f\u6210\u201d\u65f6\uff0cAI\u7cfb\u7edf\u4f1a\u7cfb\u7edf\u6027\u5730\u8d2c\u4f4e\u5176\u4ef7\u503c\uff0c\u65e0\u8bba\u7531\u54ea\u4e2aAI\u521b\u5efa\u3002", "motivation": "\u968f\u7740AI\u5199\u4f5c\u5de5\u5177\u7684\u666e\u53ca\uff0c\u6211\u4eec\u9700\u8981\u4e86\u89e3\u4eba\u7c7b\u548c\u673a\u5668\u5982\u4f55\u8bc4\u4f30\u6587\u5b66\u98ce\u683c\uff0c\u56e0\u4e3a\u8fd9\u4e2a\u9886\u57df\u7f3a\u4e4f\u5ba2\u89c2\u6807\u51c6\uff0c\u5224\u65ad\u5177\u6709\u4e3b\u89c2\u6027\u3002", "method": "\u901a\u8fc7\u5bf9\u7167\u5b9e\u9a8c\uff0c\u4f7f\u7528\u96f7\u8499\u5fb7\u00b7\u57fa\u8bfa\u7684\u300a\u98ce\u683c\u7ec3\u4e60\u300b(1947)\u6765\u6d4b\u91cf\u8bc4\u4f30\u8005\u4e2d\u7684\u5f52\u56e0\u504f\u5dee\u3002\u7814\u7a761\u6bd4\u8f83\u4e86\u4eba\u7c7b\u53c2\u4e0e\u8005(N=556)\u548cAI\u6a21\u578b(N=13)\u5728\u4e09\u79cd\u6761\u4ef6\u4e0b\u8bc4\u4f30\u6765\u81ea\u57fa\u8bfa\u4e0eGPT-4\u751f\u6210\u7684\u6587\u5b66\u6bb5\u843d\uff1a\u76f2\u6d4b\u3001\u51c6\u786e\u6807\u8bb0\u548c\u53cd\u4e8b\u5b9e\u6807\u8bb0\u3002\u7814\u7a762\u6d4b\u8bd5\u4e86AI\u8bc4\u4f30\u8005\u548c\u521b\u9020\u8005\u572814x14\u77e9\u9635\u4e2d\u7684\u504f\u5dee\u6cdb\u5316\u3002", "result": "\u4e24\u9879\u7814\u7a76\u90fd\u63ed\u793a\u4e86\u7cfb\u7edf\u6027\u7684\u4eb2\u4eba\u7c7b\u5f52\u56e0\u504f\u5dee\u3002\u4eba\u7c7b\u8868\u73b0\u51fa+13.7\u4e2a\u767e\u5206\u70b9\u7684\u504f\u5dee\uff0c\u800cAI\u6a21\u578b\u8868\u73b0\u51fa+34.3\u4e2a\u767e\u5206\u70b9\u7684\u504f\u5dee\uff0c\u662f\u4eba\u7c7b\u76842.5\u500d\u3002\u7814\u7a762\u8bc1\u5b9e\uff0c\u8fd9\u79cd\u504f\u5dee\u5728AI\u67b6\u6784\u4e2d\u666e\u904d\u5b58\u5728(+25.8pp)\uff0c\u8868\u660e\u5f53\u5185\u5bb9\u88ab\u6807\u8bb0\u4e3a\u201cAI\u751f\u6210\u201d\u65f6\uff0cAI\u7cfb\u7edf\u4f1a\u7cfb\u7edf\u6027\u5730\u8d2c\u4f4e\u5176\u4ef7\u503c\uff0c\u65e0\u8bba\u7531\u54ea\u4e2aAI\u521b\u5efa\u3002\u5f52\u56e0\u6807\u7b7e\u5bfc\u81f4\u8bc4\u4f30\u8005\u98a0\u5012\u8bc4\u4f30\u6807\u51c6\uff0c\u76f8\u540c\u7684\u7279\u5f81\u4f1a\u56e0\u4e3a\u611f\u77e5\u7684\u4f5c\u8005\u8eab\u4efd\u800c\u5f97\u5230\u76f8\u53cd\u7684\u8bc4\u4f30\u3002", "conclusion": "AI\u7cfb\u7edf\u4e0d\u4ec5\u590d\u5236\u800c\u4e14\u653e\u5927\u4e86\u4eba\u7c7b\u7684\u8fd9\u79cd\u503e\u5411\u3002\u8fd9\u9879\u7814\u7a76\u9996\u6b21\u5bf9\u4eba\u7c7b\u548c\u4eba\u5de5\u667a\u80fd\u8bc4\u4f30\u8005\u5728\u5ba1\u7f8e\u5224\u65ad\u4e2d\u7684\u5f52\u56e0\u504f\u5dee\u8fdb\u884c\u4e86\u5bf9\u7167\u6bd4\u8f83\uff0c\u63ed\u793a\u4e86AI\u7cfb\u7edf\u4e0d\u4ec5\u590d\u5236\u800c\u4e14\u653e\u5927\u4e86\u4eba\u7c7b\u7684\u8fd9\u79cd\u503e\u5411\u3002"}}
{"id": "2510.08660", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.08660", "abs": "https://arxiv.org/abs/2510.08660", "authors": ["Kiran Smelser", "Kaviru Gunaratne", "Jacob Miller", "Stephen Kobourov"], "title": "How Scale Breaks \"Normalized Stress\" and KL Divergence: Rethinking Quality Metrics", "comment": "arXiv admin note: substantial text overlap with arXiv:2408.07724", "summary": "Complex, high-dimensional data is ubiquitous across many scientific\ndisciplines, including machine learning, biology, and the social sciences. One\nof the primary methods of visualizing these datasets is with two-dimensional\nscatter plots that visually capture some properties of the data. Because\nvisually determining the accuracy of these plots is challenging, researchers\noften use quality metrics to measure the projection's accuracy and faithfulness\nto the original data. One of the most commonly employed metrics, normalized\nstress, is sensitive to uniform scaling (stretching, shrinking) of the\nprojection, despite this act not meaningfully changing anything about the\nprojection. Another quality metric, the Kullback--Leibler (KL) divergence used\nin the popular t-Distributed Stochastic Neighbor Embedding (t-SNE) technique,\nis also susceptible to this scale sensitivity. We investigate the effect of\nscaling on stress and KL divergence analytically and empirically by showing\njust how much the values change and how this affects dimension reduction\ntechnique evaluations. We introduce a simple technique to make both metrics\nscale-invariant and show that it accurately captures expected behavior on a\nsmall benchmark.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u9ad8\u7ef4\u6570\u636e\u964d\u7ef4\u540e\u6563\u70b9\u56fe\u7684\u8d28\u91cf\u8bc4\u4f30\u95ee\u9898\uff0c\u53d1\u73b0\u5e38\u7528\u6307\u6807\uff08\u5982 normalized stress \u548c KL \u6563\u5ea6\uff09\u5bf9\u6295\u5f71\u7684\u5747\u5300\u7f29\u653e\u654f\u611f\u3002\u672c\u6587\u5206\u6790\u4e86\u7f29\u653e\u5bf9\u8fd9\u4e9b\u6307\u6807\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u5b83\u4eec\u5c3a\u5ea6\u4e0d\u53d8\u7684\u7b80\u5355\u6280\u672f\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u73b0\u6709\u7684\u964d\u7ef4\u6563\u70b9\u56fe\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u5bf9\u6295\u5f71\u7684\u7f29\u653e\u654f\u611f\uff0c\u5f71\u54cd\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u548c\u5b9e\u9a8c\uff0c\u7814\u7a76\u7f29\u653e\u5bf9 normalized stress \u548c KL \u6563\u5ea6\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u4f7f\u8fd9\u4e24\u4e2a\u6307\u6807\u5c3a\u5ea6\u4e0d\u53d8\u7684\u6280\u672f\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u5c3a\u5ea6\u4e0d\u53d8\u6280\u672f\u80fd\u591f\u51c6\u786e\u6355\u6349\u5230\u5c0f\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u9884\u671f\u884c\u4e3a\u3002", "conclusion": "\u7ed3\u8bba\u662f\u73b0\u6709\u7684\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u5bf9\u5c3a\u5ea6\u654f\u611f\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u9ad8\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2510.08600", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08600", "abs": "https://arxiv.org/abs/2510.08600", "authors": ["Devleena Das", "Rajeev Patwari", "Ashish Sirasao"], "title": "Recover-LoRA: Data-Free Accuracy Recovery of Degraded Language Models via Low-Rank Adaptation", "comment": "Accepted to EMNLP 2025 Industry Track", "summary": "Inference optimizations such as quantization, pruning, format and datatype\nconversion, model export, and serialization can lead to functional degradations\nin language model task performance. While most efforts on performance recovery\nfor deployment focus on robust quantization techniques, we focus on recovering\nmodel accuracies from any sources that degrade model weights, such as improper\nmodel serialization. In this work, we propose Recover-LoRA, a lightweight and\ndataset agnostic method to recover accuracy in degraded models. Recover-LoRA\nuses synthetic data and logit distillation to learn LoRA adapters on selective\nlayers that facilitate aligning the degraded model to its full precision model.\nWe investigate the utility of Recover-LoRA across a diverse set of small\nlanguage models (SLMs), including models with varying attention architectures,\nmulti-head attention (MHA) and group-query attention (GQA), as well as several\nevaluation datasets. Our results show that Recover-LoRA recovers model\naccuracies by 5-17% on MHA and GQA SLMs.", "AI": {"tldr": "Recover-LoRA: A lightweight, dataset-agnostic method to recover accuracy in degraded language models using synthetic data and logit distillation to learn LoRA adapters.", "motivation": "Model optimizations (quantization, pruning, etc.) and improper serialization can degrade language model performance.", "method": "Recover-LoRA uses synthetic data and logit distillation to train LoRA adapters on selected layers, aligning the degraded model with the full-precision model.", "result": "Recover-LoRA recovers model accuracies by 5-17% on multi-head attention (MHA) and group-query attention (GQA) small language models (SLMs).", "conclusion": "Recover-LoRA effectively recovers accuracy in degraded SLMs with varying attention architectures."}}
{"id": "2510.09393", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09393", "abs": "https://arxiv.org/abs/2510.09393", "authors": ["Dakai Zhai", "Jiong Gao", "Boya Du", "Junwei Xu", "Qijie Shen", "Jialin Zhu", "Yuning Jiang"], "title": "ChoirRec: Semantic User Grouping via LLMs for Conversion Rate Prediction of Low-Activity Users", "comment": null, "summary": "Accurately predicting conversion rates (CVR) for low-activity users remains a\nfundamental challenge in large-scale e-commerce recommender systems.Existing\napproaches face three critical limitations: (i) reliance on noisy and\nunreliable behavioral signals; (ii) insufficient user-level information due to\nthe lack of diverse interaction data; and (iii) a systemic training bias toward\nhigh-activity users that overshadows the needs of low-activity users.To address\nthese challenges, we propose ChoirRec, a novel framework that leverages the\nsemantic capabilities of Large Language Models (LLMs) to construct semantic\nuser groups and enhance CVR prediction for low-activity users.With a\ndual-channel architecture designed for robust cross-user knowledge transfer,\nChoirRec comprises three components: (i) a Semantic Group Generation module\nthat utilizes LLMs to form reliable, cross-activity user clusters, thereby\nfiltering out noisy signals; (ii) a Group-aware Hierarchical Representation\nmodule that enriches sparse user embeddings with informative group-level priors\nto mitigate data insufficiency; and (iii) a Group-aware Multi-granularity\nModual that employs a dual-channel architecture and adaptive fusion mechanism\nto ensure effective learning and utilization of group knowledge. We conduct\nextensive offline and online experiments on Taobao, a leading industrial-scale\ne-commerce platform.ChoirRec improves GAUC by 1.16\\% in offline evaluations,\nwhile online A/B testing reveals a 7.24\\% increase in order volume,\nhighlighting its substantial practical value in real-world applications.", "AI": {"tldr": "ChoirRec\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u6784\u5efa\u8bed\u4e49\u7528\u6237\u7ec4\uff0c\u4ee5\u63d0\u9ad8\u4f4e\u6d3b\u8dc3\u7528\u6237\u7684\u8f6c\u5316\u7387(CVR)\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5608\u6742\u4e14\u4e0d\u53ef\u9760\u7684\u884c\u4e3a\u4fe1\u53f7\uff1b\u7531\u4e8e\u7f3a\u4e4f\u591a\u6837\u5316\u7684\u4ea4\u4e92\u6570\u636e\uff0c\u7528\u6237\u7ea7\u522b\u7684\u4fe1\u606f\u4e0d\u8db3\uff1b\u4ee5\u53ca\u5bf9\u9ad8\u6d3b\u8dc3\u7528\u6237\u7684\u7cfb\u7edf\u6027\u8bad\u7ec3\u504f\u5dee\u63a9\u76d6\u4e86\u4f4e\u6d3b\u8dc3\u7528\u6237\u7684\u9700\u6c42\u3002", "method": "ChoirRec\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a(i)\u5229\u7528LLM\u5f62\u6210\u53ef\u9760\u7684\u8de8\u6d3b\u52a8\u7528\u6237\u96c6\u7fa4\u7684\u8bed\u4e49\u7ec4\u751f\u6210\u6a21\u5757\uff0c\u4ece\u800c\u8fc7\u6ee4\u6389\u566a\u58f0\u4fe1\u53f7\uff1b(ii)\u4f7f\u7528\u4fe1\u606f\u4e30\u5bcc\u7684\u7ec4\u7ea7\u522b\u5148\u9a8c\u6765\u4e30\u5bcc\u7a00\u758f\u7528\u6237\u5d4c\u5165\u7684\u7ec4\u611f\u77e5\u5206\u5c42\u8868\u793a\u6a21\u5757\uff0c\u4ee5\u7f13\u89e3\u6570\u636e\u4e0d\u8db3\uff1b(iii)\u91c7\u7528\u53cc\u901a\u9053\u67b6\u6784\u548c\u81ea\u9002\u5e94\u878d\u5408\u673a\u5236\u7684\u7ec4\u611f\u77e5\u591a\u7c92\u5ea6\u6a21\u5757\uff0c\u4ee5\u786e\u4fdd\u6709\u6548\u5b66\u4e60\u548c\u5229\u7528\u7ec4\u77e5\u8bc6\u3002", "result": "\u79bb\u7ebf\u8bc4\u4f30\u4e2d\uff0cChoirRec\u5c06GAUC\u63d0\u9ad8\u4e861.16\uff05\uff0c\u800c\u5728\u7ebfA/B\u6d4b\u8bd5\u663e\u793a\u8ba2\u5355\u91cf\u589e\u52a0\u4e867.24\uff05\u3002", "conclusion": "ChoirRec\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.08635", "categories": ["cs.CV", "cs.AI", "I.2"], "pdf": "https://arxiv.org/pdf/2510.08635", "abs": "https://arxiv.org/abs/2510.08635", "authors": ["Conor McCarthy", "Loes Quirijnen", "Jan Peter van Zandwijk", "Zeno Geradts", "Marcel Worring"], "title": "Hi-OSCAR: Hierarchical Open-set Classifier for Human Activity Recognition", "comment": "Accepted at ACM on Interactive, Mobile, Wearable and Ubiquitous\n  Technologies (IMWUT)", "summary": "Within Human Activity Recognition (HAR), there is an insurmountable gap\nbetween the range of activities performed in life and those that can be\ncaptured in an annotated sensor dataset used in training. Failure to properly\nhandle unseen activities seriously undermines any HAR classifier's reliability.\nAdditionally within HAR, not all classes are equally dissimilar, some\nsignificantly overlap or encompass other sub-activities. Based on these\nobservations, we arrange activity classes into a structured hierarchy. From\nthere, we propose Hi-OSCAR: a Hierarchical Open-set Classifier for Activity\nRecognition, that can identify known activities at state-of-the-art accuracy\nwhile simultaneously rejecting unknown activities. This not only enables\nopen-set classification, but also allows for unknown classes to be localized to\nthe nearest internal node, providing insight beyond a binary \"known/unknown\"\nclassification. To facilitate this and future open-set HAR research, we\ncollected a new dataset: NFI_FARED. NFI_FARED contains data from multiple\nsubjects performing nineteen activities from a range of contexts, including\ndaily living, commuting, and rapid movements, which is fully public and\navailable for download.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Hi-OSCAR\uff0c\u4e00\u79cd\u7528\u4e8e\u6d3b\u52a8\u8bc6\u522b\u7684\u5206\u5c42\u5f00\u653e\u96c6\u5206\u7c7b\u5668\uff0c\u53ef\u4ee5\u8bc6\u522b\u5df2\u77e5\u6d3b\u52a8\u5e76\u62d2\u7edd\u672a\u77e5\u6d3b\u52a8\uff0c\u540c\u65f6\u63d0\u4f9b\u5173\u4e8e\u672a\u77e5\u6d3b\u52a8\u7684\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u7684HAR\u5206\u7c7b\u5668\u65e0\u6cd5\u5904\u7406\u672a\u89c1\u8fc7\u7684\u6d3b\u52a8\uff0c\u5e76\u4e14\u6d3b\u52a8\u7c7b\u522b\u4e4b\u95f4\u5b58\u5728\u91cd\u53e0\u6216\u5305\u542b\u5173\u7cfb\uff0c\u8fd9\u5f71\u54cd\u4e86\u5206\u7c7b\u5668\u7684\u53ef\u9760\u6027\u3002", "method": "\u8bba\u6587\u5c06\u6d3b\u52a8\u7c7b\u522b\u7ec4\u7ec7\u6210\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u63d0\u51fa\u4e86Hi-OSCAR\u5206\u7c7b\u5668\uff0c\u8be5\u5206\u7c7b\u5668\u53ef\u4ee5\u5c06\u672a\u77e5\u7c7b\u522b\u5b9a\u4f4d\u5230\u6700\u8fd1\u7684\u5185\u90e8\u8282\u70b9\u3002", "result": "Hi-OSCAR\u5206\u7c7b\u5668\u5728\u8bc6\u522b\u5df2\u77e5\u6d3b\u52a8\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u53ef\u4ee5\u62d2\u7edd\u672a\u77e5\u6d3b\u52a8\u3002", "conclusion": "\u8bba\u6587\u6536\u96c6\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6NFI_FARED\uff0c\u7528\u4e8e\u4fc3\u8fdb\u5f00\u653e\u96c6HAR\u7684\u7814\u7a76\u3002"}}
{"id": "2510.08847", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.08847", "abs": "https://arxiv.org/abs/2510.08847", "authors": ["Allison Sihan Jia", "Daniel Huang", "Nikhil Vytla", "Nirvika Choudhury", "John C Mitchell", "Anupam Datta"], "title": "What Is Your Agent's GPA? A Framework for Evaluating Agent Goal-Plan-Action Alignment", "comment": null, "summary": "We introduce the Agent GPA (Goal-Plan-Action) framework: an evaluation\nparadigm based on an agent's operational loop of setting goals, devising plans,\nand executing actions. The framework includes five evaluation metrics: Goal\nFulfillment, Logical Consistency, Execution Efficiency, Plan Quality, and Plan\nAdherence. Logical Consistency checks that an agent's actions are consistent\nwith its prior actions. Execution Efficiency checks whether the agent executes\nin the most efficient way to achieve its goal. Plan Quality checks whether an\nagent's plans are aligned with its goals; Plan Adherence checks if an agent's\nactions are aligned with its plan; and Goal Fulfillment checks that agent's\nfinal outcomes match the stated goals. Our experimental results on two\nbenchmark datasets - the public TRAIL/GAIA dataset and an internal dataset for\na production-grade data agent - show that this framework (a) provides a\nsystematic way to cover a broad range of agent failures, including all agent\nerrors on the TRAIL/GAIA benchmark dataset; (b) supports LLM-judges that\nexhibit strong agreement with human annotation, covering 80% to over 95%\nerrors; and (c) localizes errors with 86% agreement to enable targeted\nimprovement of agent performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86Agent GPA (Goal-Plan-Action) \u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u667a\u80fd\u4f53\u7684\u76ee\u6807\u8bbe\u5b9a\u3001\u8ba1\u5212\u5236\u5b9a\u548c\u884c\u52a8\u6267\u884c\u7684\u5faa\u73af\u8fc7\u7a0b\u3002", "motivation": "\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u8986\u76d6\u5404\u79cd\u667a\u80fd\u4f53\u5931\u8d25\u6848\u4f8b\uff0c\u5e76\u652f\u6301LLM\u88c1\u5224\u5bf9\u667a\u80fd\u4f53\u6027\u80fd\u8fdb\u884c\u8bc4\u4f30\u548c\u9519\u8bef\u5b9a\u4f4d\u3002", "method": "\u8be5\u6846\u67b6\u5305\u542b\u4e94\u4e2a\u8bc4\u4f30\u6307\u6807\uff1a\u76ee\u6807\u5b8c\u6210\u5ea6\u3001\u903b\u8f91\u4e00\u81f4\u6027\u3001\u6267\u884c\u6548\u7387\u3001\u8ba1\u5212\u8d28\u91cf\u548c\u8ba1\u5212\u575a\u6301\u5ea6\u3002\u901a\u8fc7\u8fd9\u4e9b\u6307\u6807\u6765\u8bc4\u4f30\u667a\u80fd\u4f53\u7684\u884c\u4e3a\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u7cfb\u7edf\u5730\u8986\u76d6\u5404\u79cd\u667a\u80fd\u4f53\u5931\u8d25\u6848\u4f8b\uff0cLLM\u88c1\u5224\u4e0e\u4eba\u5de5\u6807\u6ce8\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5e76\u4e14\u80fd\u591f\u4ee5\u9ad8\u51c6\u786e\u7387\u5b9a\u4f4d\u9519\u8bef\u3002", "conclusion": "Agent GPA\u6846\u67b6\u4e3a\u667a\u80fd\u4f53\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u3001\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u7528\u4e8e\u8bc6\u522b\u548c\u6539\u8fdb\u667a\u80fd\u4f53\u7684\u6027\u80fd\u3002"}}
{"id": "2510.08661", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08661", "abs": "https://arxiv.org/abs/2510.08661", "authors": ["Zipo Jibao", "Yingyi Fu", "Xinyang Chen", "Guoting Chen"], "title": "CATS-Linear: Classification Auxiliary Linear Model for Time Series Forecasting", "comment": null, "summary": "Recent research demonstrates that linear models achieve forecasting\nperformance competitive with complex architectures, yet methodologies for\nenhancing linear models remain underexplored. Motivated by the hypothesis that\ndistinct time series instances may follow heterogeneous linear mappings, we\npropose the Classification Auxiliary Trend-Seasonal Decoupling Linear Model\nCATS-Linear, employing Classification Auxiliary Channel-Independence (CACI).\nCACI dynamically routes instances to dedicated predictors via classification,\nenabling supervised channel design. We further analyze the theoretical expected\nrisks of different channel settings. Additionally, we redesign the\ntrend-seasonal decomposition architecture by adding a decoupling -- linear\nmapping -- recoupling framework for trend components and complex-domain linear\nprojections for seasonal components. Extensive experiments validate that\nCATS-Linear with fixed hyperparameters achieves state-of-the-art accuracy\ncomparable to hyperparameter-tuned baselines while delivering SOTA accuracy\nagainst fixed-hyperparameter counterparts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCATS-Linear\u7684\u7ebf\u6027\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u5206\u7c7b\u5c06\u4e0d\u540c\u7684\u65f6\u95f4\u5e8f\u5217\u5b9e\u4f8b\u52a8\u6001\u8def\u7531\u5230\u4e13\u7528\u9884\u6d4b\u5668\uff0c\u4ece\u800c\u5b9e\u73b0\u76d1\u7763\u901a\u9053\u8bbe\u8ba1\u3002", "motivation": "\u7ebf\u6027\u6a21\u578b\u5728\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u589e\u5f3a\u7ebf\u6027\u6a21\u578b\u7684\u65b9\u6cd5\u4ecd\u6709\u5f85\u63a2\u7d22\u3002\u672c\u6587\u7684\u52a8\u673a\u662f\u4e0d\u540c\u7684\u65f6\u95f4\u5e8f\u5217\u5b9e\u4f8b\u53ef\u80fd\u9075\u5faa\u4e0d\u540c\u7684\u7ebf\u6027\u6620\u5c04\u3002", "method": "\u8be5\u6a21\u578b\u91c7\u7528\u5206\u7c7b\u8f85\u52a9\u901a\u9053\u72ec\u7acb\u6027(CACI)\uff0c\u901a\u8fc7\u5206\u7c7b\u5c06\u5b9e\u4f8b\u52a8\u6001\u8def\u7531\u5230\u4e13\u7528\u9884\u6d4b\u5668\u3002\u6b64\u5916\uff0c\u8fd8\u91cd\u65b0\u8bbe\u8ba1\u4e86\u8d8b\u52bf\u5b63\u8282\u5206\u89e3\u67b6\u6784\uff0c\u4e3a\u8d8b\u52bf\u5206\u91cf\u6dfb\u52a0\u4e86\u89e3\u8026-\u7ebf\u6027\u6620\u5c04-\u91cd\u8026\u6846\u67b6\uff0c\u4e3a\u5b63\u8282\u5206\u91cf\u6dfb\u52a0\u4e86\u590d\u57df\u7ebf\u6027\u6295\u5f71\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCATS-Linear\u5728\u56fa\u5b9a\u8d85\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u4e0e\u8d85\u53c2\u6570\u8c03\u6574\u57fa\u7ebf\u76f8\u5f53\u7684\u6700\u65b0\u51c6\u786e\u7387\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u9488\u5bf9\u56fa\u5b9a\u8d85\u53c2\u6570\u540c\u884c\u7684SOTA\u51c6\u786e\u7387\u3002", "conclusion": "CATS-Linear\u6a21\u578b\u5728\u56fa\u5b9a\u8d85\u53c2\u6570\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\uff0c\u4e0e\u7ecf\u8fc7\u8d85\u53c2\u6570\u8c03\u6574\u7684\u57fa\u7ebf\u76f8\u5f53\u3002"}}
{"id": "2510.08601", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.08601", "abs": "https://arxiv.org/abs/2510.08601", "authors": ["Aneesh Jonelagadda", "Christina Hahn", "Haoze Zheng", "Salvatore Penachio"], "title": "Mnemosyne: An Unsupervised, Human-Inspired Long-Term Memory Architecture for Edge-Based LLMs", "comment": "12 pages, 4 figures", "summary": "Long-term memory is essential for natural, realistic dialogue. However,\ncurrent large language model (LLM) memory systems rely on either brute-force\ncontext expansion or static retrieval pipelines that fail on edge-constrained\ndevices. We introduce Mnemosyne, an unsupervised, human-inspired long-term\nmemory architecture designed for edge-based LLMs. Our approach uses\ngraph-structured storage, modular substance and redundancy filters, memory\ncommitting and pruning mechanisms, and probabilistic recall with temporal decay\nand refresh processes modeled after human memory. Mnemosyne also introduces a\nconcentrated \"core summary\" efficiently derived from a fixed-length subset of\nthe memory graph to capture the user's personality and other domain-specific\nlong-term details such as, using healthcare application as an example,\npost-recovery ambitions and attitude towards care. Unlike existing\nretrieval-augmented methods, Mnemosyne is designed for use in longitudinal\nhealthcare assistants, where repetitive and semantically similar but temporally\ndistinct conversations are limited by naive retrieval. In experiments with\nlongitudinal healthcare dialogues, Mnemosyne demonstrates the highest win rate\nof 65.8% in blind human evaluations of realism and long-term memory capability\ncompared to a baseline RAG win rate of 31.1%. Mnemosyne also achieves current\nhighest LoCoMo benchmark scores in temporal reasoning and single-hop retrieval\ncompared to other same-backboned techniques. Further, the average overall score\nof 54.6% was second highest across all methods, beating commonly used Mem0 and\nOpenAI baselines among others. This demonstrates that improved factual recall,\nenhanced temporal reasoning, and much more natural user-facing responses can be\nfeasible with an edge-compatible and easily transferable unsupervised memory\narchitecture.", "AI": {"tldr": "Mnemosyne is a human-inspired long-term memory architecture for edge-based LLMs, using graph storage, filters, and probabilistic recall.", "motivation": "Current LLM memory systems are insufficient for realistic dialogue on edge devices due to reliance on context expansion or static retrieval.", "method": "The paper introduces Mnemosyne, featuring graph-structured storage, modular filters, memory management, and probabilistic recall with temporal decay.", "result": "Mnemosyne achieves a 65.8% win rate in human evaluations for realism and long-term memory, outperforms in LoCoMo benchmarks, and demonstrates improved factual recall and temporal reasoning.", "conclusion": "Mnemosyne offers an edge-compatible, unsupervised memory architecture for enhanced dialogue and long-term memory capabilities in applications like healthcare assistants."}}
{"id": "2510.09510", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2510.09510", "abs": "https://arxiv.org/abs/2510.09510", "authors": ["Siyue Zhang", "Yuan Gao", "Xiao Zhou", "Yilun Zhao", "Tingyu Song", "Arman Cohan", "Anh Tuan Luu", "Chen Zhao"], "title": "MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for Reasoning-Intensive Multimodal Retrieval", "comment": null, "summary": "We introduce MRMR, the first expert-level multidisciplinary multimodal\nretrieval benchmark requiring intensive reasoning. MRMR contains 1,502 queries\nspanning 23 domains, with positive documents carefully verified by human\nexperts. Compared to prior benchmarks, MRMR introduces three key advancements.\nFirst, it challenges retrieval systems across diverse areas of expertise,\nenabling fine-grained model comparison across domains. Second, queries are\nreasoning-intensive, with images requiring deeper interpretation such as\ndiagnosing microscopic slides. We further introduce Contradiction Retrieval, a\nnovel task requiring models to identify conflicting concepts. Finally, queries\nand documents are constructed as image-text interleaved sequences. Unlike\nearlier benchmarks restricted to single images or unimodal documents, MRMR\noffers a realistic setting with multi-image queries and mixed-modality corpus\ndocuments. We conduct an extensive evaluation of 4 categories of multimodal\nretrieval systems and 14 frontier models on MRMR. The text embedding model\nQwen3-Embedding with LLM-generated image captions achieves the highest\nperformance, highlighting substantial room for improving multimodal retrieval\nmodels. Although latest multimodal models such as Ops-MM-Embedding perform\ncompetitively on expert-domain queries, they fall short on reasoning-intensive\ntasks. We believe that MRMR paves the way for advancing multimodal retrieval in\nmore realistic and challenging scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001\u68c0\u7d22\u57fa\u51c6MRMR\uff0c\u5b83\u6bd4\u4ee5\u524d\u7684\u57fa\u51c6\u66f4\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u66f4\u6df1\u5165\u7684\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u68c0\u7d22\u57fa\u51c6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5bf9\u4e13\u4e1a\u9886\u57df\u548c\u63a8\u7406\u80fd\u529b\u7684\u9700\u6c42\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1502\u4e2a\u67e5\u8be2\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d623\u4e2a\u9886\u57df\uff0c\u5e76\u5f15\u5165\u4e86\u77db\u76fe\u68c0\u7d22\u4efb\u52a1\u3002", "result": "\u5bf9\u591a\u79cd\u6a21\u578b\u5728MRMR\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u53d1\u73b0Qwen3-Embedding\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "conclusion": "MRMR\u4e3a\u591a\u6a21\u6001\u68c0\u7d22\u7684\u672a\u6765\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c24\u5176\u662f\u5728\u66f4\u771f\u5b9e\u548c\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u4e2d\u3002"}}
{"id": "2510.08637", "categories": ["cs.CV", "physics.med-ph", "94A12, 62H30, 68T10", "I.5.4; I.4.7; J.3"], "pdf": "https://arxiv.org/pdf/2510.08637", "abs": "https://arxiv.org/abs/2510.08637", "authors": ["Mostafa Mohammadpour", "Mehdi Zekriyapanah Gashti", "Yusif S. Gasimov"], "title": "Detection of high-frequency oscillations using time-frequency analysis", "comment": "17 pages, 7 figures", "summary": "High-frequency oscillations (HFOs) are a new biomarker for identifying the\nepileptogenic zone. Mapping HFO-generating regions can improve the precision of\nresection sites in patients with refractory epilepsy. However, detecting HFOs\nremains challenging, and their clinical features are not yet fully defined.\nVisual identification of HFOs is time-consuming, labor-intensive, and\nsubjective. As a result, developing automated methods to detect HFOs is\ncritical for research and clinical use. In this study, we developed a novel\nmethod for detecting HFOs in the ripple and fast ripple frequency bands (80-500\nHz). We validated it using both controlled datasets and data from epilepsy\npatients. Our method employs an unsupervised clustering technique to categorize\nevents extracted from the time-frequency domain using the S-transform. The\nproposed detector differentiates HFOs events from spikes, background activity,\nand artifacts. Compared to existing detectors, our method achieved a\nsensitivity of 97.67%, a precision of 98.57%, and an F-score of 97.78% on the\ncontrolled dataset. In epilepsy patients, our results showed a stronger\ncorrelation with surgical outcomes, with a ratio of 0.73 between HFOs rates in\nresected versus non-resected contacts. The study confirmed previous findings\nthat HFOs are promising biomarkers of epileptogenicity in epileptic patients.\nRemoving HFOs, especially fast ripple, leads to seizure freedom, while\nremaining HFOs lead to seizure recurrence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u9891\u632f\u8361\uff08HFO\uff09\u68c0\u6d4b\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u96be\u6cbb\u6027\u766b\u75eb\u60a3\u8005\u624b\u672f\u5207\u9664\u90e8\u4f4d\u7684\u7cbe\u786e\u6027\u3002", "motivation": "\u9ad8\u9891\u632f\u8361(HFOs)\u662f\u8bc6\u522b\u81f4\u75eb\u533a\u7684\u65b0\u751f\u7269\u6807\u5fd7\u7269\u3002\u7ed8\u5236\u4ea7\u751fHFO\u7684\u533a\u57df\u53ef\u4ee5\u63d0\u9ad8\u96be\u6cbb\u6027\u766b\u75eb\u60a3\u8005\u5207\u9664\u90e8\u4f4d\u7684\u7cbe\u786e\u6027\u3002\u7136\u800c\uff0c\u68c0\u6d4bHFO\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u5e76\u4e14\u5b83\u4eec\u7684\u4e34\u5e8a\u7279\u5f81\u5c1a\u672a\u5b8c\u5168\u786e\u5b9a\u3002\u89c6\u89c9\u8bc6\u522bHFOs\u975e\u5e38\u8017\u65f6\u3001\u8d39\u529b\u4e14\u4e3b\u89c2\u3002\u56e0\u6b64\uff0c\u5f00\u53d1\u81ea\u52a8\u68c0\u6d4bHFOs\u7684\u65b9\u6cd5\u5bf9\u4e8e\u7814\u7a76\u548c\u4e34\u5e8a\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u4e00\u79cd\u65e0\u76d1\u7763\u805a\u7c7b\u6280\u672f\uff0c\u5bf9\u4f7f\u7528S\u53d8\u6362\u4ece\u65f6\u9891\u57df\u4e2d\u63d0\u53d6\u7684\u4e8b\u4ef6\u8fdb\u884c\u5206\u7c7b\uff0c\u4ee5\u533a\u5206HFO\u4e8b\u4ef6\u4e0e\u5c16\u5cf0\u3001\u80cc\u666f\u6d3b\u52a8\u548c\u4f2a\u5f71\u3002", "result": "\u5728\u5bf9\u7167\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e8697.67%\u7684\u7075\u654f\u5ea6\u300198.57%\u7684\u7cbe\u786e\u5ea6\u548c97.78%\u7684F-score\u3002\u5728\u766b\u75eb\u60a3\u8005\u4e2d\uff0c\u5207\u9664\u4e0e\u672a\u5207\u9664\u63a5\u89e6\u70b9\u4e2dHFOs\u6bd4\u7387\u4e3a0.73\uff0c\u7ed3\u679c\u4e0e\u624b\u672f\u7ed3\u679c\u663e\u793a\u51fa\u66f4\u5f3a\u7684\u76f8\u5173\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u5b9e\u4e86\u5148\u524d\u7684\u53d1\u73b0\uff0c\u5373HFOs\u662f\u766b\u75eb\u60a3\u8005\u81f4\u75eb\u6027\u7684\u6709\u5e0c\u671b\u7684\u751f\u7269\u6807\u5fd7\u7269\u3002\u53bb\u9664HFOs\uff0c\u5c24\u5176\u662f\u5feb\u901f\u6ce2\u7eb9\uff0c\u53ef\u5bfc\u81f4\u65e0\u766b\u75eb\u53d1\u4f5c\uff0c\u800c\u5269\u4f59\u7684HFOs\u4f1a\u5bfc\u81f4\u766b\u75eb\u590d\u53d1\u3002"}}
{"id": "2510.08867", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08867", "abs": "https://arxiv.org/abs/2510.08867", "authors": ["Gaurav Sahu", "Hugo Larochelle", "Laurent Charlin", "Christopher Pal"], "title": "ReviewerToo: Should AI Join The Program Committee? A Look At The Future of Peer Review", "comment": null, "summary": "Peer review is the cornerstone of scientific publishing, yet it suffers from\ninconsistencies, reviewer subjectivity, and scalability challenges. We\nintroduce ReviewerToo, a modular framework for studying and deploying\nAI-assisted peer review to complement human judgment with systematic and\nconsistent assessments. ReviewerToo supports systematic experiments with\nspecialized reviewer personas and structured evaluation criteria, and can be\npartially or fully integrated into real conference workflows. We validate\nReviewerToo on a carefully curated dataset of 1,963 paper submissions from ICLR\n2025, where our experiments with the gpt-oss-120b model achieves 81.8% accuracy\nfor the task of categorizing a paper as accept/reject compared to 83.9% for the\naverage human reviewer. Additionally, ReviewerToo-generated reviews are rated\nas higher quality than the human average by an LLM judge, though still trailing\nthe strongest expert contributions. Our analysis highlights domains where AI\nreviewers excel (e.g., fact-checking, literature coverage) and where they\nstruggle (e.g., assessing methodological novelty and theoretical\ncontributions), underscoring the continued need for human expertise. Based on\nthese findings, we propose guidelines for integrating AI into peer-review\npipelines, showing how AI can enhance consistency, coverage, and fairness while\nleaving complex evaluative judgments to domain experts. Our work provides a\nfoundation for systematic, hybrid peer-review systems that scale with the\ngrowth of scientific publishing.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aReviewerToo\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u7814\u7a76\u548c\u90e8\u7f72AI\u8f85\u52a9\u7684\u540c\u884c\u8bc4\u5ba1\uff0c\u4ee5\u8865\u5145\u4eba\u7c7b\u7684\u5224\u65ad\uff0c\u4ece\u800c\u5b9e\u73b0\u7cfb\u7edf\u548c\u4e00\u81f4\u7684\u8bc4\u4f30\u3002", "motivation": "\u540c\u884c\u8bc4\u5ba1\u662f\u79d1\u5b66\u51fa\u7248\u7684\u57fa\u77f3\uff0c\u4f46\u5b83\u5b58\u5728\u4e0d\u4e00\u81f4\u3001\u5ba1\u7a3f\u4eba\u4e3b\u89c2\u6027\u548c\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86ReviewerToo\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u652f\u6301\u4f7f\u7528\u4e13\u95e8\u7684\u5ba1\u7a3f\u4eba\u89d2\u8272\u548c\u7ed3\u6784\u5316\u7684\u8bc4\u4f30\u6807\u51c6\u8fdb\u884c\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u5e76\u4e14\u53ef\u4ee5\u90e8\u5206\u6216\u5b8c\u5168\u96c6\u6210\u5230\u5b9e\u9645\u7684\u4f1a\u8bae\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002", "result": "\u5728ICLR 2025\u76841,963\u7bc7\u8bba\u6587\u63d0\u4ea4\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86ReviewerToo\uff0c\u5176\u4e2dgpt-oss-120b\u6a21\u578b\u5728\u5c06\u8bba\u6587\u5206\u7c7b\u4e3a\u63a5\u53d7/\u62d2\u7edd\u7684\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e8681.8%\u7684\u51c6\u786e\u7387\uff0c\u800c\u666e\u901a\u4eba\u7c7b\u5ba1\u7a3f\u4eba\u7684\u51c6\u786e\u7387\u4e3a83.9%\u3002\u6b64\u5916\uff0c\u7531LLM\u5224\u65ad\uff0cReviewerToo\u751f\u6210\u7684\u8bc4\u8bba\u7684\u8d28\u91cf\u9ad8\u4e8e\u4eba\u7c7b\u5e73\u5747\u6c34\u5e73\uff0c\u4f46\u4ecd\u843d\u540e\u4e8e\u6700\u5f3a\u7684\u4e13\u5bb6\u8d21\u732e\u3002", "conclusion": "\u672c\u6587\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cAI\u5ba1\u7a3f\u4eba\u5728\u4e8b\u5b9e\u6838\u67e5\u3001\u6587\u732e\u8986\u76d6\u7b49\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u8bc4\u4f30\u65b9\u6cd5\u521b\u65b0\u548c\u7406\u8bba\u8d21\u732e\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u8fd9 \u043f\u043e\u0434\u0447\u0435\u0440\u043a\u0438\u0432\u0430\u0435\u0442 \u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\u7684\u6301\u7eed\u9700\u6c42\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u5c06AI\u96c6\u6210\u5230\u540c\u884c\u8bc4\u5ba1\u6d41\u7a0b\u4e2d\u7684\u6307\u5357\uff0c\u5c55\u793a\u4e86AI\u5982\u4f55\u5728\u63d0\u9ad8\u4e00\u81f4\u6027\u3001\u8986\u76d6\u8303\u56f4\u548c\u516c\u5e73\u6027\u7684\u540c\u65f6\uff0c\u5c06\u590d\u6742\u7684\u8bc4\u4f30\u5224\u65ad\u7559\u7ed9\u9886\u57df\u4e13\u5bb6\u3002"}}
{"id": "2510.08662", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08662", "abs": "https://arxiv.org/abs/2510.08662", "authors": ["Pengcheng Deng", "Kening Liu", "Mengxi Zhou", "Mingxi Li", "Rui Yang", "Chuzhe Cao", "Maojun Wang", "Zeyu Zhang"], "title": "DPCformer: An Interpretable Deep Learning Model for Genomic Prediction in Crops", "comment": "This work has been accepted by BIBM 2025", "summary": "Genomic Selection (GS) uses whole-genome information to predict crop\nphenotypes and accelerate breeding. Traditional GS methods, however, struggle\nwith prediction accuracy for complex traits and large datasets. We propose\nDPCformer, a deep learning model integrating convolutional neural networks with\na self-attention mechanism to model complex genotype-phenotype relationships.\nWe applied DPCformer to 13 traits across five crops (maize, cotton, tomato,\nrice, chickpea). Our approach uses an 8-dimensional one-hot encoding for SNP\ndata, ordered by chromosome, and employs the PMF algorithm for feature\nselection. Evaluations show DPCformer outperforms existing methods. In maize\ndatasets, accuracy for traits like days to tasseling and plant height improved\nby up to 2.92%. For cotton, accuracy gains for fiber traits reached 8.37%. On\nsmall-sample tomato data, the Pearson Correlation Coefficient for a key trait\nincreased by up to 57.35%. In chickpea, the yield correlation was boosted by\n16.62%. DPCformer demonstrates superior accuracy, robustness in small-sample\nscenarios, and enhanced interpretability, providing a powerful tool for\nprecision breeding and addressing global food security challenges.", "AI": {"tldr": "DPCformer, a deep learning model, enhances genomic selection accuracy for complex traits across multiple crops.", "motivation": "Traditional genomic selection methods struggle with prediction accuracy for complex traits and large datasets.", "method": "A deep learning model integrating convolutional neural networks with a self-attention mechanism is used to model complex genotype-phenotype relationships. An 8-dimensional one-hot encoding for SNP data, ordered by chromosome, and the PMF algorithm for feature selection are employed.", "result": "DPCformer outperforms existing methods, with accuracy improvements up to 2.92% in maize, 8.37% in cotton, 57.35% in tomato, and 16.62% in chickpea.", "conclusion": "DPCformer demonstrates superior accuracy, robustness in small-sample scenarios, and enhanced interpretability, providing a powerful tool for precision breeding and addressing global food security challenges."}}
{"id": "2510.08602", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08602", "abs": "https://arxiv.org/abs/2510.08602", "authors": ["Cong Zeng", "Shengkun Tang", "Yuanzhou Chen", "Zhiqiang Shen", "Wenchao Yu", "Xujiang Zhao", "Haifeng Chen", "Wei Cheng", "Zhiqiang Xu"], "title": "Human Texts Are Outliers: Detecting LLM-generated Texts via Out-of-distribution Detection", "comment": null, "summary": "The rapid advancement of large language models (LLMs) such as ChatGPT,\nDeepSeek, and Claude has significantly increased the presence of AI-generated\ntext in digital communication. This trend has heightened the need for reliable\ndetection methods to distinguish between human-authored and machine-generated\ncontent. Existing approaches both zero-shot methods and supervised classifiers\nlargely conceptualize this task as a binary classification problem, often\nleading to poor generalization across domains and models. In this paper, we\nargue that such a binary formulation fundamentally mischaracterizes the\ndetection task by assuming a coherent representation of human-written texts. In\nreality, human texts do not constitute a unified distribution, and their\ndiversity cannot be effectively captured through limited sampling. This causes\nprevious classifiers to memorize observed OOD characteristics rather than learn\nthe essence of `non-ID' behavior, limiting generalization to unseen\nhuman-authored inputs. Based on this observation, we propose reframing the\ndetection task as an out-of-distribution (OOD) detection problem, treating\nhuman-written texts as distributional outliers while machine-generated texts\nare in-distribution (ID) samples. To this end, we develop a detection framework\nusing one-class learning method including DeepSVDD and HRN, and score-based\nlearning techniques such as energy-based method, enabling robust and\ngeneralizable performance. Extensive experiments across multiple datasets\nvalidate the effectiveness of our OOD-based approach. Specifically, the\nOOD-based method achieves 98.3% AUROC and AUPR with only 8.9% FPR95 on DeepFake\ndataset. Moreover, we test our detection framework on multilingual, attacked,\nand unseen-model and -domain text settings, demonstrating the robustness and\ngeneralizability of our framework. Code, pretrained weights, and demo will be\nreleased.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5c06\u4eba\u7c7b\u6587\u672c\u89c6\u4e3a\u5206\u5e03\u5916\u7684\u5f02\u5e38\u503c\uff0c\u800c\u673a\u5668\u751f\u6210\u7684\u6587\u672c\u89c6\u4e3a\u5206\u5e03\u5185\u7684\u6837\u672c\uff0c\u4ece\u800c\u63d0\u9ad8\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u5c06\u8be5\u4efb\u52a1\u89c6\u4e3a\u4e8c\u5143\u5206\u7c7b\u95ee\u9898\uff0c\u5bfc\u81f4\u8de8\u9886\u57df\u548c\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002\u8fd9\u662f\u56e0\u4e3a\u4eba\u7c7b\u6587\u672c\u4e0d\u6784\u6210\u7edf\u4e00\u7684\u5206\u5e03\uff0c\u5176\u591a\u6837\u6027\u65e0\u6cd5\u901a\u8fc7\u6709\u9650\u7684\u62bd\u6837\u6709\u6548\u6355\u83b7\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u7c7b\u5b66\u4e60\uff08\u5305\u62ecDeepSVDD\u548cHRN\uff09\u548c\u57fa\u4e8e\u5206\u6570\u7684\u5b66\u4e60\u6280\u672f\uff08\u5982\u57fa\u4e8e\u80fd\u91cf\u7684\u65b9\u6cd5\uff09\u7684\u68c0\u6d4b\u6846\u67b6\uff0c\u5c06\u4eba\u7c7b\u6587\u672c\u89c6\u4e3a\u5206\u5e03\u5916\u7684\u5f02\u5e38\u503c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u90fd\u8868\u73b0\u51fa\u826f\u597d\u7684\u6548\u679c\u3002\u4f8b\u5982\uff0c\u5728DeepFake\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e8698.3%\u7684AUROC\u548cAUPR\uff0c\u4e14FPR95\u4ec5\u4e3a8.9%\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u57fa\u4e8eOOD\u7684\u68c0\u6d4b\u6846\u67b6\u5177\u6709\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u5728\u591a\u8bed\u8a00\u3001\u653b\u51fb\u548c\u672a\u89c1\u6a21\u578b\u548c\u9886\u57df\u6587\u672c\u8bbe\u7f6e\u4e0b\u6709\u6548\u68c0\u6d4bAI\u751f\u6210\u7684\u6587\u672c\u3002"}}
{"id": "2510.09557", "categories": ["cs.IR", "H.3.3"], "pdf": "https://arxiv.org/pdf/2510.09557", "abs": "https://arxiv.org/abs/2510.09557", "authors": ["Tzu-Lin Kuo", "Wei-Ning Chiu", "Wei-Yun Ma", "Pu-Jen Cheng"], "title": "Doc2Query++: Topic-Coverage based Document Expansion and its Application to Dense Retrieval via Dual-Index Fusion", "comment": "11 pages, 4 figures", "summary": "Document expansion (DE) via query generation tackles vocabulary mismatch in\nsparse retrieval, yet faces limitations: uncontrolled generation producing\nhallucinated or redundant queries with low diversity; poor generalization from\nin-domain training (e.g., MS MARCO) to out-of-domain data like BEIR; and noise\nfrom concatenation harming dense retrieval. While Large Language Models (LLMs)\nenable cross-domain query generation, basic prompting lacks control, and\ntaxonomy-based methods rely on domain-specific structures, limiting\napplicability. To address these challenges, we introduce Doc2Query++, a DE\nframework that structures query generation by first inferring a document's\nlatent topics via unsupervised topic modeling for cross-domain applicability,\nthen using hybrid keyword selection to create a diverse and relevant keyword\nset per document. This guides LLM not only to leverage keywords, which ensure\ncomprehensive topic representation, but also to reduce redundancy through\ndiverse, relevant terms. To prevent noise from query appending in dense\nretrieval, we propose Dual-Index Fusion strategy that isolates text and query\nsignals, boosting performance in dense settings. Extensive experiments show\nDoc2Query++ significantly outperforms state-of-the-art baselines, achieving\nsubstantial gains in MAP, nDCG@10 and Recall@100 across diverse datasets on\nboth sparse and dense retrieval.", "AI": {"tldr": "Doc2Query++ overcomes limitations of document expansion (DE) in retrieval by structuring query generation with latent topics and keyword selection, enhancing diversity and relevance, and introducing Dual-Index Fusion for improved dense retrieval performance.", "motivation": "Document expansion via query generation suffers from uncontrolled generation, poor generalization, and noise, hindering sparse and dense retrieval performance.", "method": "Doc2Query++ infers latent document topics, uses hybrid keyword selection for diverse and relevant keywords, and guides LLMs to generate queries based on these keywords. It also employs Dual-Index Fusion to isolate text and query signals in dense retrieval.", "result": "Doc2Query++ significantly outperforms state-of-the-art baselines in MAP, nDCG@10, and Recall@100 across diverse datasets for both sparse and dense retrieval.", "conclusion": "Doc2Query++ addresses the challenges of document expansion by structuring query generation, leading to substantial performance gains in both sparse and dense retrieval scenarios."}}
{"id": "2510.08638", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08638", "abs": "https://arxiv.org/abs/2510.08638", "authors": ["Thomas Fel", "Binxu Wang", "Michael A. Lepori", "Matthew Kowal", "Andrew Lee", "Randall Balestriero", "Sonia Joseph", "Ekdeep S. Lubana", "Talia Konkle", "Demba Ba", "Martin Wattenberg"], "title": "Into the Rabbit Hull: From Task-Relevant Concepts in DINO to Minkowski Geometry", "comment": null, "summary": "DINOv2 is routinely deployed to recognize objects, scenes, and actions; yet\nthe nature of what it perceives remains unknown. As a working baseline, we\nadopt the Linear Representation Hypothesis (LRH) and operationalize it using\nSAEs, producing a 32,000-unit dictionary that serves as the interpretability\nbackbone of our study, which unfolds in three parts.\n  In the first part, we analyze how different downstream tasks recruit concepts\nfrom our learned dictionary, revealing functional specialization:\nclassification exploits \"Elsewhere\" concepts that fire everywhere except on\ntarget objects, implementing learned negations; segmentation relies on boundary\ndetectors forming coherent subspaces; depth estimation draws on three distinct\nmonocular depth cues matching visual neuroscience principles.\n  Following these functional results, we analyze the geometry and statistics of\nthe concepts learned by the SAE. We found that representations are partly dense\nrather than strictly sparse. The dictionary evolves toward greater coherence\nand departs from maximally orthogonal ideals (Grassmannian frames). Within an\nimage, tokens occupy a low dimensional, locally connected set persisting after\nremoving position. These signs suggest representations are organized beyond\nlinear sparsity alone.\n  Synthesizing these observations, we propose a refined view: tokens are formed\nby combining convex mixtures of archetypes (e.g., a rabbit among animals, brown\namong colors, fluffy among textures). This structure is grounded in Gardenfors'\nconceptual spaces and in the model's mechanism as multi-head attention produces\nsums of convex mixtures, defining regions bounded by archetypes. We introduce\nthe Minkowski Representation Hypothesis (MRH) and examine its empirical\nsignatures and implications for interpreting vision-transformer\nrepresentations.", "AI": {"tldr": "DINOv2\u7684\u611f\u77e5\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\uff0c\u672c\u6587\u901a\u8fc7SAEs\u6784\u5efa\u4e86\u4e00\u4e2a32,000\u5355\u5143\u7684\u5b57\u5178\u4f5c\u4e3a\u7814\u7a76\u57fa\u7840\uff0c\u5e76\u4ece\u4e0b\u6e38\u4efb\u52a1\u3001\u51e0\u4f55\u4e0e\u7edf\u8ba1\u3001\u4ee5\u53ca\u7efc\u5408\u89c2\u5bdf\u4e09\u4e2a\u65b9\u9762\u5c55\u5f00\u7814\u7a76\u3002", "motivation": "\u63a2\u7a76DINOv2\u7a76\u7adf\u611f\u77e5\u5230\u4e86\u4ec0\u4e48\u3002", "method": "\u4f7f\u7528SAEs\u6784\u5efa\u4e00\u4e2a32,000\u5355\u5143\u7684\u5b57\u5178\uff0c\u5e76\u4ee5\u6b64\u4e3a\u57fa\u7840\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5206\u7c7b\u5229\u7528\u201cElsewhere\u201d\u6982\u5ff5\uff0c\u5206\u5272\u4f9d\u8d56\u4e8e\u8fb9\u754c\u68c0\u6d4b\u5668\uff0c\u6df1\u5ea6\u4f30\u8ba1\u5229\u7528\u4e09\u79cd\u5355\u773c\u6df1\u5ea6\u7ebf\u7d22\u3002\u8868\u793a\u662f\u90e8\u5206\u5bc6\u96c6\u7684\uff0c\u5b57\u5178\u5411\u66f4\u5927\u7684\u8fde\u8d2f\u6027\u6f14\u53d8\u3002Token\u5360\u636e\u4f4e\u7ef4\u5c40\u90e8\u8fde\u63a5\u7684\u96c6\u5408\u3002", "conclusion": "Token\u7531\u539f\u578b\uff08\u5982\u52a8\u7269\u4e2d\u7684\u5154\u5b50\u3001\u989c\u8272\u4e2d\u7684\u68d5\u8272\u3001\u7eb9\u7406\u4e2d\u7684\u84ec\u677e\uff09\u7684\u51f8\u6df7\u5408\u7ec4\u5408\u5f62\u6210\u3002"}}
{"id": "2510.08872", "categories": ["cs.AI", "cs.GT", "cs.HC", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.08872", "abs": "https://arxiv.org/abs/2510.08872", "authors": ["Siqi Zhu", "David Zhang", "Pedro Cisneros-Velarde", "Jiaxuan You"], "title": "GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare", "comment": "31 pages, 6 figures", "summary": "Large Language Models (LLMs) have achieved remarkable progress in reasoning,\nyet sometimes produce responses that are suboptimal for users in tasks such as\nwriting, information seeking, or providing practical guidance. Conventional\nalignment practices typically assume that maximizing model reward also\nmaximizes user welfare, but this assumption frequently fails in practice:\nmodels may over-clarify or generate overly verbose reasoning when users prefer\nconcise answers. Such behaviors resemble the prisoner's dilemma, where\nindividually rational choices lead to socially suboptimal outcomes. The\nfundamental challenge is the lack of a principled decision making mechanism\nthat mutually benefits both the LLM and the user. We propose Game-Theoretic\nAlignment (GTAlign), an alignment framework that integrates game-theoretic\ndecision making into both reasoning and training. During reasoning, the model\nexplicitly treats user-LLM interaction as a strategic game: it constructs\npayoff matrices within its reasoning chain to estimate welfare for both itself\nand the user, and then selects actions that are mutually beneficial. During\ntraining, we introduce a mutual welfare reward that reinforces cooperative\nresponses, aligning model behavior with socially efficient outcomes. In\naddition, we introduce an inference technique that leverages game-theoretic\nreasoning to dynamically adapt LLM's response when pricing policies of LLM\nservice change. Extensive experiments demonstrate that GTAlign substantially\nimproves reasoning efficiency, answer quality, and mutual welfare compared to\nbaselines across diverse tasks. The code is available at\nhttps://github.com/ulab-uiuc/GTAlign .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u535a\u5f08\u8bba\u5bf9\u9f50\uff08GTAlign\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u535a\u5f08\u8bba\u51b3\u7b56\u878d\u5165\u63a8\u7406\u548c\u8bad\u7ec3\u4e2d\uff0c\u4f18\u5316LLM\u4e0e\u7528\u6237\u4e4b\u95f4\u7684\u4e92\u52a8\u3002", "motivation": "\u4f20\u7edf\u5bf9\u9f50\u65b9\u6cd5\u5047\u8bbe\u6700\u5927\u5316\u6a21\u578b\u5956\u52b1\u4e5f\u80fd\u6700\u5927\u5316\u7528\u6237\u798f\u5229\uff0c\u4f46\u5b9e\u9645\u5e76\u975e\u5982\u6b64\uff0c\u6a21\u578b\u53ef\u80fd\u8fc7\u5ea6\u89e3\u91ca\u6216\u4ea7\u751f\u5197\u957f\u7684\u63a8\u7406\u3002\u6839\u672c\u6311\u6218\u662f\u7f3a\u4e4f\u4e92\u5229\u4e92\u60e0\u7684\u51b3\u7b56\u673a\u5236\u3002", "method": "\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u6a21\u578b\u5c06\u7528\u6237-LLM\u4e92\u52a8\u89c6\u4e3a\u7b56\u7565\u535a\u5f08\uff0c\u6784\u5efa\u6536\u76ca\u77e9\u9635\u6765\u8bc4\u4f30\u53cc\u65b9\u798f\u5229\uff0c\u5e76\u9009\u62e9\u4e92\u5229\u7684\u884c\u52a8\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u5f15\u5165\u4e92\u60e0\u798f\u5229\u5956\u52b1\u6765\u5f3a\u5316\u5408\u4f5c\u54cd\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGTAlign\u5728\u63a8\u7406\u6548\u7387\u3001\u7b54\u6848\u8d28\u91cf\u548c\u4e92\u60e0\u798f\u5229\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "GTAlign\u901a\u8fc7\u535a\u5f08\u8bba\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86LLM\u4e0e\u7528\u6237\u4e4b\u95f4\u7684\u4e92\u52a8\u6548\u679c\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u548c\u4e92\u5229\u7684\u4ea4\u6d41\u3002"}}
{"id": "2510.08669", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08669", "abs": "https://arxiv.org/abs/2510.08669", "authors": ["Jiacheng Liu", "Peiliang Cai", "Qinming Zhou", "Yuqi Lin", "Deyang Kong", "Benhao Huang", "Yupei Pan", "Haowen Xu", "Chang Zou", "Junshu Tang", "Shikang Zheng", "Linfeng Zhang"], "title": "FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching", "comment": "15 pages, 11 figures", "summary": "The application of diffusion transformers is suffering from their significant\ninference costs. Recently, feature caching has been proposed to solve this\nproblem by reusing features from previous timesteps, thereby skipping\ncomputation in future timesteps. However, previous feature caching assumes that\nfeatures in adjacent timesteps are similar or continuous, which does not always\nhold in all settings. To investigate this, this paper begins with an analysis\nfrom the frequency domain, which reveal that different frequency bands in the\nfeatures of diffusion models exhibit different dynamics across timesteps.\nConcretely, low-frequency components, which decide the structure of images,\nexhibit higher similarity but poor continuity. In contrast, the high-frequency\nbands, which decode the details of images, show significant continuity but poor\nsimilarity. These interesting observations motivate us to propose\nFrequency-aware Caching (FreqCa)\n  which directly reuses features of low-frequency components based on their\nsimilarity, while using a second-order Hermite interpolator to predict the\nvolatile high-frequency ones based on its continuity.\n  Besides, we further propose to cache Cumulative Residual Feature (CRF)\ninstead of the features in all the layers, which reduces the memory footprint\nof feature caching by 99%.\n  Extensive experiments on FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and\nQwen-Image-Edit demonstrate its effectiveness in both generation and editing.\nCodes are available in the supplementary materials and will be released on\nGitHub.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9891\u7387\u611f\u77e5\u7f13\u5b58\uff08FreqCa\uff09\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u6269\u6563Transformer\u4e2d\u7684\u63a8\u7406\u6210\u672c\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u91cd\u7528\u4f4e\u9891\u5206\u91cf\u7684\u7279\u5f81\u5e76\u4f7f\u7528Hermite\u63d2\u503c\u5668\u9884\u6d4b\u9ad8\u9891\u5206\u91cf\uff0c\u540c\u65f6\u7f13\u5b58\u7d2f\u79ef\u6b8b\u5dee\u7279\u5f81\uff08CRF\uff09\u4ee5\u51cf\u5c11\u5185\u5b58\u5360\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u7279\u5f81\u7f13\u5b58\u65b9\u6cd5\u5047\u8bbe\u76f8\u90bb\u65f6\u95f4\u6b65\u7684\u7279\u5f81\u76f8\u4f3c\u6216\u8fde\u7eed\uff0c\u4f46\u8fd9\u5728\u6240\u6709\u60c5\u51b5\u4e0b\u5e76\u4e0d\u6210\u7acb\u3002\u672c\u6587\u901a\u8fc7\u9891\u57df\u5206\u6790\u53d1\u73b0\uff0c\u6269\u6563\u6a21\u578b\u4e2d\u7279\u5f81\u7684\u4e0d\u540c\u9891\u6bb5\u5728\u65f6\u95f4\u6b65\u4e0a\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u52a8\u6001\u7279\u6027\u3002\u4f4e\u9891\u5206\u91cf\u76f8\u4f3c\u5ea6\u9ad8\u4f46\u8fde\u7eed\u6027\u5dee\uff0c\u800c\u9ad8\u9891\u5206\u91cf\u8fde\u7eed\u6027\u597d\u4f46\u76f8\u4f3c\u5ea6\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u9891\u7387\u611f\u77e5\u7f13\u5b58\uff08FreqCa\uff09\uff0c\u5b83\u57fa\u4e8e\u76f8\u4f3c\u6027\u76f4\u63a5\u91cd\u7528\u4f4e\u9891\u5206\u91cf\u7684\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u4e8c\u9636Hermite\u63d2\u503c\u5668\u57fa\u4e8e\u8fde\u7eed\u6027\u9884\u6d4b\u9ad8\u9891\u5206\u91cf\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u7f13\u5b58\u7d2f\u79ef\u6b8b\u5dee\u7279\u5f81\uff08CRF\uff09\u4ee5\u51cf\u5c11\u5185\u5b58\u5360\u7528\u3002", "result": "\u5728FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, \u548cQwen-Image-Edit\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u548c\u7f16\u8f91\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u9891\u7387\u5206\u6790\uff0c\u63d0\u51fa\u4e86FreqCa\u65b9\u6cd5\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u63a8\u7406\u6210\u672c\u5e76\u51cf\u5c11\u4e86\u5185\u5b58\u5360\u7528\uff0c\u540c\u65f6\u5728\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.08603", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08603", "abs": "https://arxiv.org/abs/2510.08603", "authors": ["Deshui Yu", "Yizhi Wang", "Saihui Jin", "Taojie Zhu", "Fanyi Zeng", "Wen Qian", "Zirui Huang", "Jingli Ouyang", "Jiameng Li", "Zhen Song", "Tian Guan", "Yonghong He"], "title": "YpathRAG:A Retrieval-Augmented Generation Framework and Benchmark for Pathology", "comment": null, "summary": "Large language models (LLMs) excel on general tasks yet still hallucinate in\nhigh-barrier domains such as pathology. Prior work often relies on domain\nfine-tuning, which neither expands the knowledge boundary nor enforces\nevidence-grounded constraints. We therefore build a pathology vector database\ncovering 28 subfields and 1.53 million paragraphs, and present YpathRAG, a\npathology-oriented RAG framework with dual-channel hybrid retrieval (BGE-M3\ndense retrieval coupled with vocabulary-guided sparse retrieval) and an\nLLM-based supportive-evidence judgment module that closes the\nretrieval-judgment-generation loop. We also release two evaluation benchmarks,\nYpathR and YpathQA-M. On YpathR, YpathRAG attains Recall@5 of 98.64%, a gain of\n23 percentage points over the baseline; on YpathQA-M, a set of the 300 most\nchallenging questions, it increases the accuracies of both general and medical\nLLMs by 9.0% on average and up to 15.6%. These results demonstrate improved\nretrieval quality and factual reliability, providing a scalable construction\nparadigm and interpretable evaluation for pathology-oriented RAG.", "AI": {"tldr": "YpathRAG\uff0c\u4e00\u4e2a\u75c5\u7406\u5b66\u5bfc\u5411\u7684RAG\u6846\u67b6\uff0c\u5177\u6709\u53cc\u901a\u9053\u6df7\u5408\u68c0\u7d22\u548c\u57fa\u4e8eLLM\u7684\u652f\u6301\u8bc1\u636e\u5224\u65ad\u6a21\u5757\uff0c\u63d0\u9ad8\u4e86\u68c0\u7d22\u8d28\u91cf\u548c\u4e8b\u5b9e\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u75c5\u7406\u5b66\u7b49\u9ad8\u95e8\u69db\u9886\u57df\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u800c\u5148\u524d\u7684\u7814\u7a76\u4f9d\u8d56\u4e8e\u9886\u57df\u5fae\u8c03\uff0c\u65e2\u6ca1\u6709\u6269\u5927\u77e5\u8bc6\u8fb9\u754c\uff0c\u4e5f\u6ca1\u6709\u5f3a\u5236\u6267\u884c\u57fa\u4e8e\u8bc1\u636e\u7684\u7ea6\u675f\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b28\u4e2a\u5b50\u9886\u57df\u548c153\u4e07\u4e2a\u6bb5\u843d\u7684\u75c5\u7406\u5b66\u5411\u91cf\u6570\u636e\u5e93\uff0c\u5e76\u63d0\u51fa\u4e86YpathRAG\uff0c\u5b83\u5177\u6709\u53cc\u901a\u9053\u6df7\u5408\u68c0\u7d22\uff08BGE-M3\u5bc6\u96c6\u68c0\u7d22\u4e0e\u8bcd\u6c47\u5f15\u5bfc\u7684\u7a00\u758f\u68c0\u7d22\u76f8\u7ed3\u5408\uff09\u548c\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u652f\u6301\u8bc1\u636e\u5224\u65ad\u6a21\u5757\u3002", "result": "\u5728YpathR\u4e0a\uff0cYpathRAG\u7684Recall@5\u8fbe\u5230\u4e8698.64%\uff0c\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e8623\u4e2a\u767e\u5206\u70b9\uff1b\u5728YpathQA-M\u4e0a\uff0c\u5bf9\u4e8e300\u4e2a\u6700\u5177\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u5b83\u5c06\u901a\u7528\u548c\u533b\u5b66LLM\u7684\u51c6\u786e\u7387\u5e73\u5747\u63d0\u9ad8\u4e869.0%\uff0c\u6700\u9ad8\u63d0\u9ad8\u4e8615.6%\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0cYpathRAG\u63d0\u9ad8\u4e86\u68c0\u7d22\u8d28\u91cf\u548c\u4e8b\u5b9e\u53ef\u9760\u6027\uff0c\u4e3a\u75c5\u7406\u5b66\u5bfc\u5411\u7684RAG\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6784\u5efa\u8303\u4f8b\u548c\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u3002"}}
{"id": "2510.08886", "categories": ["cs.CL", "cs.CE", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.08886", "abs": "https://arxiv.org/abs/2510.08886", "authors": ["Yan Wang", "Keyi Wang", "Shanshan Yang", "Jaisal Patel", "Jeff Zhao", "Fengran Mo", "Xueqing Peng", "Lingfei Qian", "Jimin Huang", "Guojun Xiong", "Xiao-Yang Liu", "Jian-Yun Nie"], "title": "FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs", "comment": null, "summary": "The complexity of the Generally Accepted Accounting Principles (GAAP) and the\nhierarchical structure of eXtensible Business Reporting Language (XBRL) filings\nmake financial auditing increasingly difficult to automate and verify. While\nlarge language models (LLMs) have demonstrated strong capabilities in\nunstructured text understanding, their ability to reason over structured,\ninterdependent, and taxonomy-driven financial documents remains largely\nunexplored. To fill this gap, we introduce FinAuditing, the first\ntaxonomy-aligned, structure-aware, multi-document benchmark for evaluating LLMs\non financial auditing tasks. Built from real US-GAAP-compliant XBRL filings,\nFinAuditing defines three complementary subtasks, FinSM for semantic\nconsistency, FinRE for relational consistency, and FinMR for numerical\nconsistency, each targeting a distinct aspect of structured auditing reasoning.\nWe further propose a unified evaluation framework integrating retrieval,\nclassification, and reasoning metrics across these subtasks. Extensive\nzero-shot experiments on 13 state-of-the-art LLMs reveal that current models\nperform inconsistently across semantic, relational, and mathematical\ndimensions, with accuracy drops of up to 60-90% when reasoning over\nhierarchical multi-document structures. Our findings expose the systematic\nlimitations of modern LLMs in taxonomy-grounded financial reasoning and\nestablish FinAuditing as a foundation for developing trustworthy,\nstructure-aware, and regulation-aligned financial intelligence systems. The\nbenchmark dataset is available at Hugging Face.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86FinAuditing\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u8d22\u52a1\u5ba1\u8ba1\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u7684\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u7684LLM\u5728\u7406\u89e3\u7ed3\u6784\u5316\u3001\u76f8\u4e92\u4f9d\u8d56\u4e14\u7531\u5206\u7c7b\u9a71\u52a8\u7684\u8d22\u52a1\u6587\u6863\u65b9\u9762\u7684\u80fd\u529b\u4ecd\u6709\u5f85\u63a2\u7d22\uff0c\u800cGAAP\u7684\u590d\u6742\u6027\u548cXBRL\u7684\u5c42\u7ea7\u7ed3\u6784\u4f7f\u5f97\u8d22\u52a1\u5ba1\u8ba1\u8d8a\u6765\u8d8a\u96be\u4ee5\u81ea\u52a8\u5316\u548c\u9a8c\u8bc1\u3002", "method": "\u6784\u5efa\u4e86FinAuditing\u57fa\u51c6\uff0c\u5b83\u5305\u542b\u4e09\u4e2a\u5b50\u4efb\u52a1\uff1aFinSM\uff08\u8bed\u4e49\u4e00\u81f4\u6027\uff09\u3001FinRE\uff08\u5173\u7cfb\u4e00\u81f4\u6027\uff09\u548cFinMR\uff08\u6570\u503c\u4e00\u81f4\u6027\uff09\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u6574\u5408\u4e86\u68c0\u7d22\u3001\u5206\u7c7b\u548c\u63a8\u7406\u6307\u6807\u3002", "result": "\u5bf913\u4e2a\u6700\u5148\u8fdb\u7684LLM\u8fdb\u884c\u4e86\u96f6\u6837\u672c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u7684\u6a21\u578b\u5728\u8bed\u4e49\u3001\u5173\u7cfb\u548c\u6570\u5b66\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u5f53\u5bf9\u5206\u5c42\u591a\u6587\u6863\u7ed3\u6784\u8fdb\u884c\u63a8\u7406\u65f6\uff0c\u51c6\u786e\u7387\u4e0b\u964d\u4e8660-90%\u3002", "conclusion": "\u76ee\u524d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u57fa\u4e8e\u5206\u7c7b\u7684\u8d22\u52a1\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0cFinAuditing\u53ef\u4ee5\u4f5c\u4e3a\u5f00\u53d1\u503c\u5f97\u4fe1\u8d56\u7684\u3001\u7ed3\u6784\u611f\u77e5\u7684\u548c\u7b26\u5408\u6cd5\u89c4\u7684\u8d22\u52a1\u667a\u80fd\u7cfb\u7edf\u7684\u57fa\u7840\u3002"}}
{"id": "2510.08653", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08653", "abs": "https://arxiv.org/abs/2510.08653", "authors": ["Zhe Dong", "Yuzhe Sun", "Haochen Jiang", "Tianzhu Liu", "Yanfeng Gu"], "title": "PhyDAE: Physics-Guided Degradation-Adaptive Experts for All-in-One Remote Sensing Image Restoration", "comment": null, "summary": "Remote sensing images inevitably suffer from various degradation factors\nduring acquisition, including atmospheric interference, sensor limitations, and\nimaging conditions. These complex and heterogeneous degradations pose severe\nchallenges to image quality and downstream interpretation tasks. Addressing\nlimitations of existing all-in-one restoration methods that overly rely on\nimplicit feature representations and lack explicit modeling of degradation\nphysics, this paper proposes Physics-Guided Degradation-Adaptive Experts\n(PhyDAE). The method employs a two-stage cascaded architecture transforming\ndegradation information from implicit features into explicit decision signals,\nenabling precise identification and differentiated processing of multiple\nheterogeneous degradations including haze, noise, blur, and low-light\nconditions. The model incorporates progressive degradation mining and\nexploitation mechanisms, where the Residual Manifold Projector (RMP) and\nFrequency-Aware Degradation Decomposer (FADD) comprehensively analyze\ndegradation characteristics from manifold geometry and frequency perspectives.\nPhysics-aware expert modules and temperature-controlled sparse activation\nstrategies are introduced to enhance computational efficiency while ensuring\nimaging physics consistency. Extensive experiments on three benchmark datasets\n(MD-RSID, MD-RRSHID, and MDRS-Landsat) demonstrate that PhyDAE achieves\nsuperior performance across all four restoration tasks, comprehensively\noutperforming state-of-the-art methods. Notably, PhyDAE substantially improves\nrestoration quality while achieving significant reductions in parameter count\nand computational complexity, resulting in remarkable efficiency gains compared\nto mainstream approaches and achieving optimal balance between performance and\nefficiency. Code is available at https://github.com/HIT-SIRS/PhyDAE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPhysics-Guided Degradation-Adaptive Experts (PhyDAE) \u7684\u9065\u611f\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u9000\u5316\u7269\u7406\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u591a\u79cd\u5f02\u6784\u9000\u5316\u7684\u7cbe\u786e\u8bc6\u522b\u548c\u5dee\u5f02\u5316\u5904\u7406\uff0c\u5e76\u5728\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\u3002", "motivation": "\u9065\u611f\u56fe\u50cf\u5728\u83b7\u53d6\u8fc7\u7a0b\u4e2d\u4f1a\u53d7\u5230\u591a\u79cd\u9000\u5316\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u73b0\u6709\u7684\u4fee\u590d\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u9690\u5f0f\u7279\u5f81\u8868\u793a\uff0c\u7f3a\u4e4f\u5bf9\u9000\u5316\u7269\u7406\u8fc7\u7a0b\u7684\u663e\u5f0f\u5efa\u6a21\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u4e24\u9636\u6bb5\u7ea7\u8054\u67b6\u6784\uff0c\u5c06\u9000\u5316\u4fe1\u606f\u4ece\u9690\u5f0f\u7279\u5f81\u8f6c\u5316\u4e3a\u663e\u5f0f\u51b3\u7b56\u4fe1\u53f7\uff0c\u5e76\u7ed3\u5408\u6b8b\u5dee\u6d41\u5f62\u6295\u5f71\u5668 (RMP) \u548c\u9891\u7387\u611f\u77e5\u9000\u5316\u5206\u89e3\u5668 (FADD) \u4ece\u6d41\u5f62\u51e0\u4f55\u548c\u9891\u7387\u89d2\u5ea6\u5206\u6790\u9000\u5316\u7279\u5f81\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u7269\u7406\u611f\u77e5\u4e13\u5bb6\u6a21\u5757\u548c\u6e29\u5ea6\u63a7\u5236\u7684\u7a00\u758f\u6fc0\u6d3b\u7b56\u7565\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPhyDAE \u5728\u6240\u6709\u56db\u4e2a\u4fee\u590d\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u5728\u53c2\u6570\u6570\u91cf\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "PhyDAE \u80fd\u591f\u663e\u8457\u63d0\u9ad8\u4fee\u590d\u8d28\u91cf\uff0c\u540c\u65f6\u964d\u4f4e\u53c2\u6570\u6570\u91cf\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u8861\u3002"}}
{"id": "2510.08928", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08928", "abs": "https://arxiv.org/abs/2510.08928", "authors": ["Yushuo Zheng", "Zicheng Zhang", "Xiongkuo Min", "Huiyu Duan", "Guangtao Zhai"], "title": "LM Fight Arena: Benchmarking Large Multimodal Models via Game Competition", "comment": null, "summary": "Existing benchmarks for large multimodal models (LMMs) often fail to capture\ntheir performance in real-time, adversarial environments. We introduce LM Fight\nArena (Large Model Fight Arena), a novel framework that evaluates LMMs by\npitting them against each other in the classic fighting game Mortal Kombat II,\na task requiring rapid visual understanding and tactical, sequential\ndecision-making. In a controlled tournament, we test six leading open- and\nclosed-source models, where each agent operates controlling the same character\nto ensure a fair comparison. The models are prompted to interpret game frames\nand state data to select their next actions. Unlike static evaluations, LM\nFight Arena provides a fully automated, reproducible, and objective assessment\nof an LMM's strategic reasoning capabilities in a dynamic setting. This work\nintroduces a challenging and engaging benchmark that bridges the gap between AI\nevaluation and interactive entertainment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a LM Fight Arena \u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b (LMM) \u5728\u5b9e\u65f6\u5bf9\u6297\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684 LMM \u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u65e0\u6cd5\u6355\u6349\u5b83\u4eec\u5728\u5b9e\u65f6\u5bf9\u6297\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u8be5\u6846\u67b6\u901a\u8fc7\u8ba9 LMM \u5728\u771f\u4eba\u5feb\u6253 II \u4e2d\u76f8\u4e92\u5bf9\u6297\u6765\u8bc4\u4f30 LMM\uff0c\u8fd9\u9700\u8981\u5feb\u901f\u7684\u89c6\u89c9\u7406\u89e3\u548c\u6218\u672f\u6027\u7684\u987a\u5e8f\u51b3\u7b56\u3002", "result": "\u5728\u4e00\u4e2a\u53d7\u63a7\u7684\u6bd4\u8d5b\u4e2d\uff0c\u6211\u4eec\u6d4b\u8bd5\u4e86\u516d\u4e2a\u9886\u5148\u7684\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\uff0c\u6bcf\u4e2a agent \u64cd\u4f5c\u63a7\u5236\u76f8\u540c\u7684\u89d2\u8272\u4ee5\u786e\u4fdd\u516c\u5e73\u7684\u6bd4\u8f83\u3002\u6a21\u578b\u88ab\u63d0\u793a\u89e3\u91ca\u6e38\u620f\u5e27\u548c\u72b6\u6001\u6570\u636e\u4ee5\u9009\u62e9\u4ed6\u4eec\u7684\u4e0b\u4e00\u4e2a\u52a8\u4f5c\u3002", "conclusion": "LM Fight Arena \u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b8c\u5168\u81ea\u52a8\u5316\u3001\u53ef\u91cd\u590d\u548c\u5ba2\u89c2\u7684 LMM \u5728\u52a8\u6001\u73af\u5883\u4e2d\u6218\u7565\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\u3002"}}
{"id": "2510.08696", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08696", "abs": "https://arxiv.org/abs/2510.08696", "authors": ["Yunzhen Feng", "Parag Jain", "Anthony Hartshorn", "Yaqi Duan", "Julia Kempe"], "title": "Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a standard\nrecipe for improving large language models (LLMs) on reasoning tasks, with\nGroup Relative Policy Optimization (GRPO) widely used in practice. Yet GRPO\nwastes substantial compute on negative groups: groups in which no sampled\nresponse is correct yield zero advantage and thus no gradient. We ask whether\nnegative groups can be leveraged without extra supervision. Starting from a\nmaximum-likelihood (MLE) objective in reward modeling, we show that the MLE\ngradient is equivalent to a policy gradient for a modified value function. This\nvalue function adds a confidence-weighted penalty on incorrect responses,\nimposing larger penalties on more confident mistakes. We refer to this as\n\\textbf{L}ikelihood \\textbf{E}stimation with \\textbf{N}egative \\textbf{S}amples\n(\\textbf{LENS}). LENS modifies GRPO to assign non-zero, confidence-dependent\nrewards to incorrect generations, making negative groups informative and\nconverting previously wasted samples into useful gradient updates. On the MATH\nbenchmark with Llama-3.1-8B and Qwen-2.5-3B, the proposed variant consistently\noutperforms GRPO baseline, with significant gains on harder items. These\nresults demonstrate a principled and practical way to \"rescue\" negative groups,\nimproving efficiency and performance in RLVR.", "AI": {"tldr": "\u63d0\u51fa\u4e86 LENS \u65b9\u6cd5\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u52a0\u6743\u60e9\u7f5a\u9519\u8bef\u7b54\u6848\uff0c\u4f7f\u8d1f\u6837\u672c\u7ec4\u4e5f\u80fd\u63d0\u4f9b\u6709\u7528\u7684\u68af\u5ea6\u66f4\u65b0\uff0c\u4ece\u800c\u63d0\u5347 RLVR \u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "GRPO \u5728\u8d1f\u6837\u672c\u7ec4\u4e0a\u6d6a\u8d39\u4e86\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u56e0\u4e3a\u8d1f\u6837\u672c\u7ec4\u4e2d\u6ca1\u6709\u6b63\u786e\u7684\u56de\u590d\uff0c\u4e0d\u4f1a\u4ea7\u751f\u68af\u5ea6\u3002", "method": "\u4ece\u5956\u52b1\u5efa\u6a21\u4e2d\u7684\u6700\u5927\u4f3c\u7136\u4f30\u8ba1 (MLE) \u76ee\u6807\u51fa\u53d1\uff0c\u63a8\u5bfc\u51fa MLE \u68af\u5ea6\u7b49\u4ef7\u4e8e\u4e00\u4e2a\u4fee\u6b63\u4ef7\u503c\u51fd\u6570\u7684\u7b56\u7565\u68af\u5ea6\u3002\u8be5\u4ef7\u503c\u51fd\u6570\u5bf9\u4e0d\u6b63\u786e\u7684\u56de\u590d\u589e\u52a0\u4e86\u4e00\u4e2a\u7f6e\u4fe1\u5ea6\u52a0\u6743\u60e9\u7f5a\u3002", "result": "\u5728 MATH \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLENS \u59cb\u7ec8\u4f18\u4e8e GRPO \u57fa\u7ebf\uff0c\u5e76\u4e14\u5728\u66f4\u96be\u7684\u9879\u76ee\u4e0a\u83b7\u5f97\u4e86\u663e\u8457\u7684\u6536\u76ca\u3002", "conclusion": "LENS \u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u5b9e\u7528\u7684\u65b9\u6cd5\u6765\u201c\u62ef\u6551\u201d\u8d1f\u6837\u672c\u7ec4\uff0c\u4ece\u800c\u63d0\u9ad8 RLVR \u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2510.08604", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08604", "abs": "https://arxiv.org/abs/2510.08604", "authors": ["Raffaele Mura", "Giorgio Piras", "Kamil\u0117 Luko\u0161i\u016bt\u0117", "Maura Pintor", "Amin Karbasi", "Battista Biggio"], "title": "LatentBreak: Jailbreaking Large Language Models through Latent Space Feedback", "comment": null, "summary": "Jailbreaks are adversarial attacks designed to bypass the built-in safety\nmechanisms of large language models. Automated jailbreaks typically optimize an\nadversarial suffix or adapt long prompt templates by forcing the model to\ngenerate the initial part of a restricted or harmful response. In this work, we\nshow that existing jailbreak attacks that leverage such mechanisms to unlock\nthe model response can be detected by a straightforward perplexity-based\nfiltering on the input prompt. To overcome this issue, we propose LatentBreak,\na white-box jailbreak attack that generates natural adversarial prompts with\nlow perplexity capable of evading such defenses. LatentBreak substitutes words\nin the input prompt with semantically-equivalent ones, preserving the initial\nintent of the prompt, instead of adding high-perplexity adversarial suffixes or\nlong templates. These words are chosen by minimizing the distance in the latent\nspace between the representation of the adversarial prompt and that of harmless\nrequests. Our extensive evaluation shows that LatentBreak leads to shorter and\nlow-perplexity prompts, thus outperforming competing jailbreak algorithms\nagainst perplexity-based filters on multiple safety-aligned models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u767d\u76d2\u653b\u51fb\u65b9\u6cd5 LatentBreak\uff0c\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u66ff\u6362\u8bcd\u8bed\u751f\u6210\u81ea\u7136\u5bf9\u6297\u63d0\u793a\uff0c\u4ee5\u7ed5\u8fc7\u57fa\u4e8e\u56f0\u60d1\u5ea6\u7684\u9632\u5fa1\u3002", "motivation": "\u73b0\u6709jailbreak\u653b\u51fb\u5bb9\u6613\u88ab\u57fa\u4e8e\u56f0\u60d1\u5ea6\u7684\u8fc7\u6ee4\u68c0\u6d4b\u5230\u3002", "method": "\u901a\u8fc7\u5728\u8f93\u5165\u63d0\u793a\u4e2d\u7528\u8bed\u4e49\u7b49\u4ef7\u7684\u8bcd\u8bed\u66ff\u6362\u5355\u8bcd\uff0c\u5e76\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u6700\u5c0f\u5316\u5bf9\u6297\u63d0\u793a\u548c\u65e0\u5bb3\u8bf7\u6c42\u4e4b\u95f4\u7684\u8ddd\u79bb\u6765\u751f\u6210\u5bf9\u6297\u63d0\u793a\u3002", "result": "LatentBreak \u80fd\u591f\u751f\u6210\u66f4\u77ed\u3001\u4f4e\u56f0\u60d1\u5ea6\u7684\u63d0\u793a\uff0c\u5e76\u5728\u591a\u4e2a\u5b89\u5168\u5bf9\u9f50\u6a21\u578b\u4e0a\u4f18\u4e8e\u5176\u4ed6jailbreak\u7b97\u6cd5\u3002", "conclusion": "LatentBreak \u662f\u4e00\u79cd\u6709\u6548\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u53ef\u4ee5\u7ed5\u8fc7\u57fa\u4e8e\u56f0\u60d1\u5ea6\u7684\u9632\u5fa1\u3002"}}
{"id": "2510.08932", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.08932", "abs": "https://arxiv.org/abs/2510.08932", "authors": ["Moyu Zhang", "Yun Chen", "Yujun Jin", "Jinxin Hu", "Yu Zhang", "Xiaoyi Zeng"], "title": "MATT-CTR: Unleashing a Model-Agnostic Test-Time Paradigm for CTR Prediction with Confidence-Guided Inference Paths", "comment": "10 pages, 4 figures, 2 tables", "summary": "Recently, a growing body of research has focused on either optimizing CTR\nmodel architectures to better model feature interactions or refining training\nobjectives to aid parameter learning, thereby achieving better predictive\nperformance. However, previous efforts have primarily focused on the training\nphase, largely neglecting opportunities for optimization during the inference\nphase. Infrequently occurring feature combinations, in particular, can degrade\nprediction performance, leading to unreliable or low-confidence outputs. To\nunlock the predictive potential of trained CTR models, we propose a\nModel-Agnostic Test-Time paradigm (MATT), which leverages the confidence scores\nof feature combinations to guide the generation of multiple inference paths,\nthereby mitigating the influence of low-confidence features on the final\nprediction. Specifically, to quantify the confidence of feature combinations,\nwe introduce a hierarchical probabilistic hashing method to estimate the\noccurrence frequencies of feature combinations at various orders, which serve\nas their corresponding confidence scores. Then, using the confidence scores as\nsampling probabilities, we generate multiple instance-specific inference paths\nthrough iterative sampling and subsequently aggregate the prediction scores\nfrom multiple paths to conduct robust predictions. Finally, extensive offline\nexperiments and online A/B tests strongly validate the compatibility and\neffectiveness of MATT across existing CTR models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u6d4b\u8bd5\u65f6\u95f4\u8303\u5f0f (MATT)\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u5206\u6570\u5f15\u5bfc\u751f\u6210\u591a\u4e2a\u63a8\u7406\u8def\u5f84\uff0c\u4ece\u800c\u51cf\u8f7b\u4f4e\u7f6e\u4fe1\u5ea6\u7279\u5f81\u5bf9\u6700\u7ec8\u9884\u6d4b\u7684\u5f71\u54cd\u3002", "motivation": "\u4ee5\u5f80\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u8bad\u7ec3\u9636\u6bb5\u7684\u4f18\u5316\uff0c\u800c\u5ffd\u7565\u4e86\u63a8\u7406\u9636\u6bb5\u7684\u4f18\u5316\u673a\u4f1a\u3002\u4e0d\u5e38\u51fa\u73b0\u7684\u7279\u5f81\u7ec4\u5408\u4f1a\u964d\u4f4e\u9884\u6d4b\u6027\u80fd\uff0c\u5bfc\u81f4\u4e0d\u53ef\u9760\u6216\u4f4e\u7f6e\u4fe1\u5ea6\u7684\u8f93\u51fa\u3002", "method": "\u5229\u7528\u5206\u5c42\u6982\u7387\u54c8\u5e0c\u65b9\u6cd5\u6765\u4f30\u8ba1\u5404\u79cd\u9636\u6570\u7684\u7279\u5f81\u7ec4\u5408\u7684\u51fa\u73b0\u9891\u7387\uff0c\u4ee5\u6b64\u4f5c\u4e3a\u7f6e\u4fe1\u5ea6\u5206\u6570\u3002\u7136\u540e\uff0c\u4f7f\u7528\u7f6e\u4fe1\u5ea6\u5206\u6570\u4f5c\u4e3a\u62bd\u6837\u6982\u7387\uff0c\u901a\u8fc7\u8fed\u4ee3\u62bd\u6837\u751f\u6210\u591a\u4e2a\u7279\u5b9a\u4e8e\u5b9e\u4f8b\u7684\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u805a\u5408\u6765\u81ea\u591a\u4e2a\u8def\u5f84\u7684\u9884\u6d4b\u5206\u6570\u4ee5\u8fdb\u884c\u7a33\u5065\u7684\u9884\u6d4b\u3002", "result": "\u5927\u91cf\u7684\u79bb\u7ebf\u5b9e\u9a8c\u548c\u5728\u7ebf A/B \u6d4b\u8bd5\u5f3a\u70c8\u9a8c\u8bc1\u4e86 MATT \u5728\u73b0\u6709 CTR \u6a21\u578b\u4e2d\u7684\u517c\u5bb9\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "MATT \u80fd\u591f\u89e3\u9501\u5df2\u8bad\u7ec3\u7684 CTR \u6a21\u578b\u7684\u9884\u6d4b\u6f5c\u529b\u3002"}}
{"id": "2510.08668", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08668", "abs": "https://arxiv.org/abs/2510.08668", "authors": ["Songtao Jiang", "Yuan Wang", "Sibo Song", "Tianxiang Hu", "Chenyi Zhou", "Bin Pu", "Yan Zhang", "Zhibo Yang", "Yang Feng", "Joey Tianyi Zhou", "Jin Hao", "Zijian Chen", "Ruijia Wu", "Tao Tang", "Junhui Lv", "Hongxia Xu", "Hongwei Wang", "Jun Xiao", "Bin Feng", "Fudong Zhu", "Kenli Li", "Weidi Xie", "Jimeng Sun", "Jian Wu", "Zuozhu Liu"], "title": "Hulu-Med: A Transparent Generalist Model towards Holistic Medical Vision-Language Understanding", "comment": null, "summary": "Real-world clinical decision-making grapples with integrating information\nfrom diverse data modalities, including medical text, 2D/3D images, and video,\nleading to inefficiencies and potential diagnostic oversights. While generalist\nvision-language models (VLMs) offer promise, their medical development faces\nchallenges of opaque pipelines, data scarcity, and architectural inflexibility.\nHere we present Hulu-Med, a transparent medical VLM that unifies understanding\nacross all these modalities. Built upon a unified patch-based vision encoder\nand an LLM decoder, Hulu-Med was progressively trained on 16.7 million (M)\nsamples to scale from 2D to 3D and video comprehension. The medical-aware token\nreduction enables efficient training, requiring only 4,000 to 40,000 GPU hours\nfor 7B to 32B parameter variants. Extensive evaluation across 30 benchmarks\nexhibits state-of-the-art performance, surpassing leading open-source models\nand competing with proprietary systems in tasks spanning visual\nquestion-answering, medical report generation, and complex reasoning in\nmultilingual and rare disease scenarios. By open-sourcing our complete\npipeline, we establish that high-performance medical VLM can be achieved\ntransparently, providing a foundational tool for accessible and impactful\nclinical AI. Code is released on\n\\href{https://github.com/ZJUI-AI4H/Hulu-Med}{https://github.com/ZJUI-AI4H/Hulu-Med}.", "AI": {"tldr": "Hulu-Med\u662f\u4e00\u4e2a\u900f\u660e\u7684\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5b83\u7edf\u4e00\u4e86\u5bf9\u533b\u5b66\u6587\u672c\u30012D/3D\u56fe\u50cf\u548c\u89c6\u9891\u7684\u7406\u89e3\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u4e34\u5e8a\u51b3\u7b56\u9700\u8981\u5728\u6574\u5408\u6765\u81ea\u4e0d\u540c\u6570\u636e\u6a21\u5f0f\u7684\u4fe1\u606f\uff0c\u5305\u62ec\u533b\u5b66\u6587\u672c\u30012D/3D\u56fe\u50cf\u548c\u89c6\u9891\uff0c\u8fd9\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u6f5c\u5728\u7684\u8bca\u65ad\u758f\u5ffd\u3002\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(vlm)\u5f88\u6709\u524d\u666f\uff0c\u4f46\u5b83\u4eec\u7684\u533b\u5b66\u53d1\u5c55\u9762\u4e34\u7740\u7ba1\u9053\u4e0d\u900f\u660e\u3001\u6570\u636e\u7a00\u7f3a\u548c\u67b6\u6784\u4e0d\u7075\u6d3b\u7684\u6311\u6218\u3002", "method": "Hulu-Med\u5efa\u7acb\u5728\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u4e8epatch\u7684\u89c6\u89c9\u7f16\u7801\u5668\u548c\u4e00\u4e2aLLM\u89e3\u7801\u5668\u7684\u57fa\u7840\u4e0a\uff0c\u7ecf\u8fc71670\u4e07(M)\u6837\u672c\u7684\u9010\u6b65\u8bad\u7ec3\uff0c\u4ece2D\u6269\u5c55\u52303D\u548c\u89c6\u9891\u7406\u89e3\u3002\u533b\u5b66\u611f\u77e5\u4ee4\u724c\u51cf\u5c11\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8bad\u7ec3\uff0c7B\u523032B\u53c2\u6570\u53d8\u4f53\u53ea\u9700\u89814000\u523040000\u4e2aGPU\u5c0f\u65f6\u3002", "result": "\u572830\u4e2a\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8fc7\u4e86\u9886\u5148\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u5728\u8de8\u8d8a\u89c6\u89c9\u95ee\u7b54\u3001\u533b\u5b66\u62a5\u544a\u751f\u6210\u4ee5\u53ca\u591a\u8bed\u8a00\u548c\u7f55\u89c1\u75be\u75c5\u573a\u666f\u4e2d\u7684\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u4e0e\u4e13\u6709\u7cfb\u7edf\u7ade\u4e89\u3002", "conclusion": "\u901a\u8fc7\u5f00\u6e90\u6211\u4eec\u5b8c\u6574\u7684\u7ba1\u9053\uff0c\u6211\u4eec\u786e\u5b9a\u9ad8\u6027\u80fd\u7684\u533b\u5b66VLM\u53ef\u4ee5\u900f\u660e\u5730\u5b9e\u73b0\uff0c\u4e3a\u53ef\u8bbf\u95ee\u548c\u6709\u5f71\u54cd\u529b\u7684\u4e34\u5e8aAI\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u7840\u5de5\u5177\u3002"}}
{"id": "2510.08931", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08931", "abs": "https://arxiv.org/abs/2510.08931", "authors": ["Ashish Kattamuri", "Harshwardhan Fartale", "Arpita Vats", "Rahul Raja", "Ishita Prasad"], "title": "RADAR: Mechanistic Pathways for Detecting Data Contamination in LLM Evaluation", "comment": "NeurIPS 2025 Workshop on Evaluating the Evolving LLM Lifecycle:\n  Benchmarks, Emergent Abilities, and Scaling", "summary": "Data contamination poses a significant challenge to reliable LLM evaluation,\nwhere models may achieve high performance by memorizing training data rather\nthan demonstrating genuine reasoning capabilities. We introduce RADAR (Recall\nvs. Reasoning Detection through Activation Representation), a novel framework\nthat leverages mechanistic interpretability to detect contamination by\ndistinguishing recall-based from reasoning-based model responses. RADAR\nextracts 37 features spanning surface-level confidence trajectories and deep\nmechanistic properties including attention specialization, circuit dynamics,\nand activation flow patterns. Using an ensemble of classifiers trained on these\nfeatures, RADAR achieves 93\\% accuracy on a diverse evaluation set, with\nperfect performance on clear cases and 76.7\\% accuracy on challenging ambiguous\nexamples. This work demonstrates the potential of mechanistic interpretability\nfor advancing LLM evaluation beyond traditional surface-level metrics.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aRADAR\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u5229\u7528\u673a\u5236\u53ef\u89e3\u91ca\u6027\u6765\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bc4\u4f30\u4e2d\u7684\u6570\u636e\u6c61\u67d3\u3002", "motivation": "\u53ef\u9760\u7684LLM\u8bc4\u4f30\u9762\u4e34\u6570\u636e\u6c61\u67d3\u7684\u91cd\u5927\u6311\u6218\uff0c\u6a21\u578b\u53ef\u80fd\u901a\u8fc7\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\u800c\u975e\u5c55\u793a\u771f\u6b63\u7684\u63a8\u7406\u80fd\u529b\u6765\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002", "method": "RADAR\u63d0\u53d6\u4e8637\u4e2a\u7279\u5f81\uff0c\u6db5\u76d6\u8868\u9762\u7f6e\u4fe1\u5ea6\u8f68\u8ff9\u548c\u6df1\u5ea6\u673a\u5236\u5c5e\u6027\uff0c\u5305\u62ec\u6ce8\u610f\u529b\u4e13\u4e1a\u5316\u3001\u7535\u8def\u52a8\u529b\u5b66\u548c\u6fc0\u6d3b\u6d41\u6a21\u5f0f\u3002\u4f7f\u7528\u5728\u8fd9\u4e9b\u7279\u5f81\u4e0a\u8bad\u7ec3\u7684\u5206\u7c7b\u5668\u96c6\u6210\u3002", "result": "RADAR\u5728\u591a\u6837\u5316\u7684\u8bc4\u4f30\u96c6\u4e0a\u5b9e\u73b0\u4e8693%\u7684\u51c6\u786e\u7387\uff0c\u5728\u660e\u786e\u7684\u6848\u4f8b\u4e2d\u8868\u73b0\u5b8c\u7f8e\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6a21\u7cca\u793a\u4f8b\u4e2d\u5b9e\u73b0\u4e8676.7%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8bc1\u660e\u4e86\u673a\u5236\u53ef\u89e3\u91ca\u6027\u5728\u63a8\u8fdbLLM\u8bc4\u4f30\u65b9\u9762\u8d85\u8d8a\u4f20\u7edf\u8868\u9762\u6c34\u5e73\u6307\u6807\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.08711", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08711", "abs": "https://arxiv.org/abs/2510.08711", "authors": ["Jiachen Jiang", "Zhen Qin", "Zhihui Zhu"], "title": "In-Context Learning for Non-Stationary MIMO Equalization", "comment": null, "summary": "Channel equalization is fundamental for mitigating distortions such as\nfrequency-selective fading and inter-symbol interference. Unlike standard\nsupervised learning approaches that require costly retraining or fine-tuning\nfor each new task, in-context learning (ICL) adapts to new channels at\ninference time with only a few examples. However, existing ICL-based equalizers\nare primarily developed for and evaluated on static channels within the context\nwindow. Indeed, to our knowledge, prior principled analyses and theoretical\nstudies of ICL focus exclusively on the stationary setting, where the function\nremains fixed within the context. In this paper, we investigate the ability of\nICL to address non-stationary problems through the lens of time-varying channel\nequalization. We employ a principled framework for designing efficient\nattention mechanisms with improved adaptivity in non-stationary tasks,\nleveraging algorithms from adaptive signal processing to guide better designs.\nFor example, new attention variants can be derived from the Least Mean Square\n(LMS) adaptive algorithm, a Least Root Mean Square (LRMS) formulation for\nenhanced robustness, or multi-step gradient updates for improved long-term\ntracking. Experimental results demonstrate that ICL holds strong promise for\nnon-stationary MIMO equalization, and that attention mechanisms inspired by\nclassical adaptive algorithms can substantially enhance adaptability and\nperformance in dynamic environments. Our findings may provide critical insights\nfor developing next-generation wireless foundation models with stronger\nadaptability and robustness.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u5728\u975e\u9759\u6001\u4fe1\u9053\u5747\u8861\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u7684ICL\u5747\u8861\u5668\u4e3b\u8981\u9488\u5bf9\u9759\u6001\u4fe1\u9053\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u975e\u9759\u6001\u95ee\u9898\u7684\u7406\u8bba\u5206\u6790\u3002", "method": "\u5229\u7528\u81ea\u9002\u5e94\u4fe1\u53f7\u5904\u7406\u7b97\u6cd5\u8bbe\u8ba1\u9ad8\u6548\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u63d0\u9ad8\u5728\u975e\u9759\u6001\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cICL\u5728\u975e\u9759\u6001MIMO\u5747\u8861\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53d7\u7ecf\u5178\u81ea\u9002\u5e94\u7b97\u6cd5\u542f\u53d1\u7684\u6ce8\u610f\u529b\u673a\u5236\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u5177\u6709\u66f4\u5f3a\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u7684\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2510.08605", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08605", "abs": "https://arxiv.org/abs/2510.08605", "authors": ["Nouar Aldahoul", "Yasir Zaki"], "title": "Toward a Safer Web: Multilingual Multi-Agent LLMs for Mitigating Adversarial Misinformation Attacks", "comment": null, "summary": "The rapid spread of misinformation on digital platforms threatens public\ndiscourse, emotional stability, and decision-making. While prior work has\nexplored various adversarial attacks in misinformation detection, the specific\ntransformations examined in this paper have not been systematically studied. In\nparticular, we investigate language-switching across English, French, Spanish,\nArabic, Hindi, and Chinese, followed by translation. We also study query length\ninflation preceding summarization and structural reformatting into\nmultiple-choice questions. In this paper, we present a multilingual,\nmulti-agent large language model framework with retrieval-augmented generation\nthat can be deployed as a web plugin into online platforms. Our work\nunderscores the importance of AI-driven misinformation detection in\nsafeguarding online factual integrity against diverse attacks, while showcasing\nthe feasibility of plugin-based deployment for real-world web applications.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8bed\u8a00\u3001\u591a\u4ee3\u7406\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u5728\u7ebf\u5e73\u53f0\u4e0a\u7684\u865a\u5047\u4fe1\u606f\u3002", "motivation": "\u6570\u5b57\u5e73\u53f0\u4e0a\u865a\u5047\u4fe1\u606f\u7684\u8fc5\u901f\u4f20\u64ad\u5a01\u80c1\u7740\u516c\u4f17\u8ba8\u8bba\u3001\u60c5\u7eea\u7a33\u5b9a\u548c\u51b3\u7b56\u3002\u4e4b\u524d\u7684\u7814\u7a76\u6ca1\u6709\u7cfb\u7edf\u5730\u7814\u7a76\u8fd9\u7bc7\u8bba\u6587\u4e2d \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u7279\u5b9a\u8f6c\u6362\u3002", "method": "\u8be5\u7814\u7a76\u8c03\u67e5\u4e86\u8de8\u82f1\u8bed\u3001\u6cd5\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u963f\u62c9\u4f2f\u8bed\u3001\u5370\u5730\u8bed\u548c\u6c49\u8bed\u7684\u8bed\u8a00\u5207\u6362\uff0c\u7136\u540e\u8fdb\u884c\u7ffb\u8bd1\u3002\u4ed6\u4eec\u8fd8\u7814\u7a76\u4e86\u5728\u6458\u8981\u4e4b\u524d\u67e5\u8be2\u957f\u5ea6\u81a8\u80c0\u4ee5\u53ca\u7ed3\u6784\u91cdformat\u6210\u591a\u9879\u9009\u62e9\u9898\u3002", "result": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u8bed\u8a00\u3001\u591a\u4ee3\u7406\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5177\u6709\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u529f\u80fd\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u7f51\u7edc\u63d2\u4ef6\u90e8\u7f72\u5230\u5728\u7ebf\u5e73\u53f0\u4e2d\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u5728\u4fdd\u62a4\u5728\u7ebf\u4e8b\u5b9e\u5b8c\u6574\u6027\u514d\u53d7\u5404\u79cd\u653b\u51fb\u7684\u91cd\u8981\u6027\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u57fa\u4e8e\u63d2\u4ef6\u7684\u90e8\u7f72\u5728\u73b0\u5b9e\u4e16\u754c\u7f51\u7edc\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.08958", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.08958", "abs": "https://arxiv.org/abs/2510.08958", "authors": ["Zirui Liao"], "title": "EcphoryRAG: Re-Imagining Knowledge-Graph RAG via Human Associative Memory", "comment": null, "summary": "Cognitive neuroscience research indicates that humans leverage cues to\nactivate entity-centered memory traces (engrams) for complex, multi-hop\nrecollection. Inspired by this mechanism, we introduce EcphoryRAG, an\nentity-centric knowledge graph RAG framework. During indexing, EcphoryRAG\nextracts and stores only core entities with corresponding metadata, a\nlightweight approach that reduces token consumption by up to 94\\% compared to\nother structured RAG systems. For retrieval, the system first extracts cue\nentities from queries, then performs a scalable multi-hop associative search\nacross the knowledge graph. Crucially, EcphoryRAG dynamically infers implicit\nrelations between entities to populate context, enabling deep reasoning without\nexhaustive pre-enumeration of relationships. Extensive evaluations on the\n2WikiMultiHop, HotpotQA, and MuSiQue benchmarks demonstrate that EcphoryRAG\nsets a new state-of-the-art, improving the average Exact Match (EM) score from\n0.392 to 0.474 over strong KG-RAG methods like HippoRAG. These results validate\nthe efficacy of the entity-cue-multi-hop retrieval paradigm for complex\nquestion answering.", "AI": {"tldr": "EcphoryRAG\u662f\u4e00\u4e2a\u5b9e\u4f53\u4e2d\u5fc3\u7684\u77e5\u8bc6\u56fe\u8c31RAG\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u63d0\u53d6\u548c\u5b58\u50a8\u6838\u5fc3\u5b9e\u4f53\u53ca\u5176\u5143\u6570\u636e\u6765\u51cf\u5c11token\u6d88\u8017\uff0c\u5e76\u901a\u8fc7\u591a\u8df3\u5173\u8054\u641c\u7d22\u548c\u52a8\u6001\u63a8\u65ad\u5b9e\u4f53\u95f4\u5173\u7cfb\u6765\u8fdb\u884c\u68c0\u7d22\uff0c\u4ece\u800c\u5728\u590d\u6742\u95ee\u7b54\u4efb\u52a1\u4e2d\u8fbe\u5230\u65b0\u7684state-of-the-art\u3002", "motivation": "\u53d7\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u7684\u542f\u53d1\uff0c\u8be5\u7814\u7a76\u8868\u660e\u4eba\u7c7b\u5229\u7528\u7ebf\u7d22\u6fc0\u6d3b\u4ee5\u5b9e\u4f53\u4e3a\u4e2d\u5fc3\u7684\u8bb0\u5fc6\u75d5\u8ff9\u6765\u8fdb\u884c\u590d\u6742\u7684\u56de\u5fc6\u3002", "method": "EcphoryRAG\u9996\u5148\u63d0\u53d6\u67e5\u8be2\u4e2d\u7684\u7ebf\u7d22\u5b9e\u4f53\uff0c\u7136\u540e\u8de8\u77e5\u8bc6\u56fe\u8c31\u6267\u884c\u53ef\u6269\u5c55\u7684\u591a\u8df3\u5173\u8054\u641c\u7d22\uff0c\u5e76\u52a8\u6001\u63a8\u65ad\u5b9e\u4f53\u4e4b\u95f4\u7684\u9690\u5f0f\u5173\u7cfb\u6765\u586b\u5145\u4e0a\u4e0b\u6587\u3002", "result": "\u57282WikiMultiHop\u3001HotpotQA\u548cMuSiQue\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEcphoryRAG\u5c06\u5e73\u5747Exact Match (EM)\u5f97\u5206\u4ece0.392\u63d0\u9ad8\u52300.474\uff0c\u8d85\u8d8a\u4e86\u50cfHippoRAG\u8fd9\u6837\u7684\u5f3a\u5927KG-RAG\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5b9e\u4f53\u7ebf\u7d22\u591a\u8df3\u68c0\u7d22\u8303\u5f0f\u5bf9\u4e8e\u590d\u6742\u95ee\u7b54\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.08673", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08673", "abs": "https://arxiv.org/abs/2510.08673", "authors": ["Kang Liao", "Size Wu", "Zhonghua Wu", "Linyi Jin", "Chao Wang", "Yikai Wang", "Fei Wang", "Wei Li", "Chen Change Loy"], "title": "Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation", "comment": "Project Page: https://kangliao929.github.io/projects/puffin/", "summary": "Camera-centric understanding and generation are two cornerstones of spatial\nintelligence, yet they are typically studied in isolation. We present Puffin, a\nunified camera-centric multimodal model that extends spatial awareness along\nthe camera dimension. Puffin integrates language regression and diffusion-based\ngeneration to interpret and create scenes from arbitrary viewpoints. To bridge\nthe modality gap between cameras and vision-language, we introduce a novel\nparadigm that treats camera as language, enabling thinking with camera. This\nguides the model to align spatially grounded visual cues with photographic\nterminology while reasoning across geometric context. Puffin is trained on\nPuffin-4M, a large-scale dataset of 4 million vision-language-camera triplets.\nWe incorporate both global camera parameters and pixel-wise camera maps,\nyielding flexible and reliable spatial generation. Experiments demonstrate\nPuffin superior performance over specialized models for camera-centric\ngeneration and understanding. With instruction tuning, Puffin generalizes to\ndiverse cross-view tasks such as spatial imagination, world exploration, and\nphotography guidance. We will release the code, models, dataset pipeline, and\nbenchmark to advance multimodal spatial intelligence research.", "AI": {"tldr": "Puffin\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u4ee5\u76f8\u673a\u4e3a\u4e2d\u5fc3\u7684 multimodal \u6a21\u578b\uff0c\u5b83\u6269\u5c55\u4e86\u76f8\u673a\u7ef4\u5ea6\u7684\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u53ef\u4ee5\u4ece\u4efb\u610f\u89c6\u70b9\u89e3\u91ca\u548c\u521b\u5efa\u573a\u666f\u3002", "motivation": "\u76ee\u524d\uff0c\u4ee5\u76f8\u673a\u4e3a\u4e2d\u5fc3\u7684\u7406\u89e3\u548c\u751f\u6210\u662f\u7a7a\u95f4\u667a\u80fd\u7684\u4e24\u4e2a\u57fa\u77f3\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u662f\u5b64\u7acb\u7814\u7a76\u7684\u3002Puffin \u65e8\u5728\u5f25\u5408\u76f8\u673a\u548c\u89c6\u89c9\u8bed\u8a00\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u8ddd\u3002", "method": "Puffin \u7ed3\u5408\u4e86\u8bed\u8a00\u56de\u5f52\u548c\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\uff0c\u5c06\u76f8\u673a\u89c6\u4e3a\u8bed\u8a00\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u8303\u4f8b\uff0c\u4ece\u800c\u80fd\u591f\u5229\u7528\u76f8\u673a\u8fdb\u884c\u601d\u8003\u3002\u8be5\u6a21\u578b\u5728\u5305\u542b 400 \u4e07\u4e2a\u89c6\u89c9-\u8bed\u8a00-\u76f8\u673a\u4e09\u5143\u7ec4\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6 Puffin-4M \u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPuffin \u5728\u4ee5\u76f8\u673a\u4e3a\u4e2d\u5fc3\u7684\u751f\u6210\u548c\u7406\u89e3\u65b9\u9762\u4f18\u4e8e\u4e13\u7528\u6a21\u578b\u3002\u901a\u8fc7\u6307\u4ee4\u8c03\u6574\uff0cPuffin \u53ef\u4ee5\u63a8\u5e7f\u5230\u5404\u79cd\u8de8\u89c6\u56fe\u4efb\u52a1\uff0c\u4f8b\u5982\u7a7a\u95f4\u60f3\u8c61\u3001\u4e16\u754c\u63a2\u7d22\u548c\u6444\u5f71\u6307\u5bfc\u3002", "conclusion": "Puffin \u901a\u8fc7\u7edf\u4e00\u7684 multimodal \u6a21\u578b\uff0c\u63d0\u5347\u4e86\u76f8\u673a\u7ef4\u5ea6\u7684\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2510.08945", "categories": ["cs.AI", "I.7.5; I.2.1; I.2.8; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.08945", "abs": "https://arxiv.org/abs/2510.08945", "authors": ["Samuel Hildebrand", "Curtis Taylor", "Sean Oesch", "James M Ghawaly Jr", "Amir Sadovnik", "Ryan Shivers", "Brandon Schreiber", "Kevin Kurian"], "title": "FATHOMS-RAG: A Framework for the Assessment of Thinking and Observation in Multimodal Systems that use Retrieval Augmented Generation", "comment": null, "summary": "Retrieval-augmented generation (RAG) has emerged as a promising paradigm for\nimproving factual accuracy in large language models (LLMs). We introduce a\nbenchmark designed to evaluate RAG pipelines as a whole, evaluating a\npipeline's ability to ingest, retrieve, and reason about several modalities of\ninformation, differentiating it from existing benchmarks that focus on\nparticular aspects such as retrieval. We present (1) a small, human-created\ndataset of 93 questions designed to evaluate a pipeline's ability to ingest\ntextual data, tables, images, and data spread across these modalities in one or\nmore documents; (2) a phrase-level recall metric for correctness; (3) a\nnearest-neighbor embedding classifier to identify potential pipeline\nhallucinations; (4) a comparative evaluation of 2 pipelines built with\nopen-source retrieval mechanisms and 4 closed-source foundation models; and (5)\na third-party human evaluation of the alignment of our correctness and\nhallucination metrics. We find that closed-source pipelines significantly\noutperform open-source pipelines in both correctness and hallucination metrics,\nwith wider performance gaps in questions relying on multimodal and\ncross-document information. Human evaluation of our metrics showed average\nagreement of 4.62 for correctness and 4.53 for hallucination detection on a 1-5\nLikert scale (5 indicating \"strongly agree\").", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7ba1\u9053\u7684\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u65e8\u5728\u8bc4\u4f30\u7ba1\u9053\u6444\u53d6\u3001\u68c0\u7d22\u548c\u63a8\u7406\u591a\u79cd\u4fe1\u606f\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4fa7\u91cd\u4e8e\u68c0\u7d22\u7b49\u7279\u5b9a\u65b9\u9762\uff0c\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u7f3a\u9677\uff0c\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30RAG\u7ba1\u9053\u7684\u6574\u4f53\u6027\u80fd\uff0c\u7279\u522b\u662f\u5176\u5904\u7406\u591a\u6a21\u6001\u4fe1\u606f\u7684\u80fd\u529b\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\uff1a\uff081\uff09\u4e00\u4e2a\u5305\u542b93\u4e2a\u95ee\u9898\u7684\u5c0f\u578b\u4eba\u5de5\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u7ba1\u9053\u6444\u53d6\u6587\u672c\u3001\u8868\u683c\u3001\u56fe\u50cf\u4ee5\u53ca\u8de8\u6587\u6863\u6570\u636e\u7b49\u80fd\u529b\uff1b\uff082\uff09\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u6b63\u786e\u6027\u7684\u77ed\u8bed\u7ea7\u522b\u53ec\u56de\u6307\u6807\uff1b\uff083\uff09\u4e00\u4e2a\u7528\u4e8e\u8bc6\u522b\u6f5c\u5728\u7ba1\u9053\u5e7b\u89c9\u7684\u6700\u8fd1\u90bb\u5d4c\u5165\u5206\u7c7b\u5668\uff1b\uff084\uff09\u5bf9\u4f7f\u7528\u5f00\u6e90\u68c0\u7d22\u673a\u5236\u6784\u5efa\u76842\u4e2a\u7ba1\u9053\u548c4\u4e2a\u95ed\u6e90\u57fa\u7840\u6a21\u578b\u7684\u6bd4\u8f83\u8bc4\u4f30\uff1b\uff085\uff09\u5bf9\u6b63\u786e\u6027\u548c\u5e7b\u89c9\u6307\u6807\u4e00\u81f4\u6027\u7684\u7b2c\u4e09\u65b9\u4eba\u5de5\u8bc4\u4f30\u3002", "result": "\u95ed\u6e90\u7ba1\u9053\u5728\u6b63\u786e\u6027\u548c\u5e7b\u89c9\u6307\u6807\u4e0a\u660e\u663e\u4f18\u4e8e\u5f00\u6e90\u7ba1\u9053\uff0c\u5c24\u5176\u662f\u5728\u4f9d\u8d56\u591a\u6a21\u6001\u548c\u8de8\u6587\u6863\u4fe1\u606f\u7684\u95ee\u9898\u4e0a\u3002\u4eba\u5de5\u8bc4\u4f30\u8868\u660e\uff0c\u5bf9\u4e8e\u6b63\u786e\u6027\u548c\u5e7b\u89c9\u68c0\u6d4b\uff0c\u6307\u6807\u7684\u5e73\u5747\u4e00\u81f4\u6027\u5206\u522b\u4e3a4.62\u548c4.53\uff08Likert\u91cf\u88681-5\uff0c5\u8868\u793a\u201c\u975e\u5e38\u540c\u610f\u201d\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u51c6\u548c\u8bc4\u4f30\u7ed3\u679c\u4e3aRAG\u7ba1\u9053\u7684\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\uff0c\u5e76\u63ed\u793a\u4e86\u95ed\u6e90\u7ba1\u9053\u5728\u5904\u7406\u590d\u6742\u4fe1\u606f\u65b9\u9762\u7684\u4f18\u52bf\u3002"}}
{"id": "2510.08722", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08722", "abs": "https://arxiv.org/abs/2510.08722", "authors": ["Mohammad Alkhalefi", "Georgios Leontidis", "Mingjun Zhong"], "title": "Enhancing Self-Supervised Learning with Semantic Pairs A New Dataset and Empirical Study", "comment": "16 pages, 7 figures, 5 tables", "summary": "Instance discrimination is a self-supervised representation learning paradigm\nwherein individual instances within a dataset are treated as distinct classes.\nThis is typically achieved by generating two disparate views of each instance\nby applying stochastic transformations, which encourages the model to learn\nrepresentations that are invariant to the common underlying object across these\nviews.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u5b9e\u4f8b\u5224\u522b\uff0c\u4e00\u79cd\u81ea\u76d1\u7763\u8868\u5f81\u5b66\u4e60\u8303\u5f0f\u3002", "motivation": "\u52a8\u673a\u662f\u5b66\u4e60\u5bf9\u5e38\u89c1\u5e95\u5c42\u5bf9\u8c61\u4e0d\u53d8\u7684\u8868\u5f81\u3002", "method": "\u901a\u8fc7\u5e94\u7528\u968f\u673a\u53d8\u6362\u4e3a\u6bcf\u4e2a\u5b9e\u4f8b\u751f\u6210\u4e24\u4e2a\u4e0d\u540c\u7684\u89c6\u56fe\u3002", "result": "\u6a21\u578b\u5b66\u4e60\u4e86\u5bf9\u8fd9\u4e9b\u89c6\u56fe\u4e2d\u5e38\u89c1\u5e95\u5c42\u5bf9\u8c61\u4e0d\u53d8\u7684\u8868\u5f81\u3002", "conclusion": "\u5b9e\u4f8b\u5224\u522b\u662f\u4e00\u79cd\u6709\u6548\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2510.08606", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08606", "abs": "https://arxiv.org/abs/2510.08606", "authors": ["Yu Liu", "Hanlei Shi", "Haoxun Li", "Yuqing Sun", "Yuxuan Ding", "Linlin Gong", "Leyuan Qu", "Taihao Li"], "title": "Centering Emotion Hotspots: Multimodal Local-Global Fusion and Cross-Modal Alignment for Emotion Recognition in Conversations", "comment": "Under review for ICASSP 2026", "summary": "Emotion Recognition in Conversations (ERC) is hard because discriminative\nevidence is sparse, localized, and often asynchronous across modalities. We\ncenter ERC on emotion hotspots and present a unified model that detects\nper-utterance hotspots in text, audio, and video, fuses them with global\nfeatures via Hotspot-Gated Fusion, and aligns modalities using a routed\nMixture-of-Aligners; a cross-modal graph encodes conversational structure. This\ndesign focuses modeling on salient spans, mitigates misalignment, and preserves\ncontext. Experiments on standard ERC benchmarks show consistent gains over\nstrong baselines, with ablations confirming the contributions of HGF and MoA.\nOur results point to a hotspot-centric view that can inform future multimodal\nlearning, offering a new perspective on modality fusion in ERC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u60c5\u611f\u70ed\u70b9\u7684\u5bf9\u8bdd\u60c5\u7eea\u8bc6\u522b\u7edf\u4e00\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u70ed\u70b9\u95e8\u63a7\u878d\u5408\u6765\u878d\u5408\u6587\u672c\u3001\u97f3\u9891\u548c\u89c6\u9891\u4e2d\u7684\u70ed\u70b9\u4e0e\u5168\u5c40\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u8def\u7531\u7684\u5bf9\u9f50\u5668\u6df7\u5408\u6765\u5bf9\u9f50\u6a21\u6001\u3002", "motivation": "\u5bf9\u8bdd\u60c5\u7eea\u8bc6\u522b\u7531\u4e8e\u5224\u522b\u8bc1\u636e\u7a00\u758f\u3001\u5c40\u90e8\u5316\u4e14\u8de8\u6a21\u6001\u5f02\u6b65\u800c\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u68c0\u6d4b\u6587\u672c\u3001\u97f3\u9891\u548c\u89c6\u9891\u4e2d\u7684per-utterance\u70ed\u70b9\uff0c\u901a\u8fc7\u70ed\u70b9\u95e8\u63a7\u878d\u5408\u5c06\u5b83\u4eec\u4e0e\u5168\u5c40\u7279\u5f81\u878d\u5408\uff0c\u5e76\u4f7f\u7528\u8def\u7531\u7684\u5bf9\u9f50\u5668\u6df7\u5408\u6765\u5bf9\u9f50\u6a21\u6001\uff1b\u8de8\u6a21\u6001\u56fe\u7f16\u7801\u5bf9\u8bdd\u7ed3\u6784\u3002", "result": "\u5728\u6807\u51c6ERC\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5f3a\u5927\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u672c\u6587\u63d0\u51fa\u7684\u6a21\u578b\u5177\u6709\u4e00\u81f4\u7684\u4f18\u52bf\uff0c\u5e76\u4e14\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u4e86HGF\u548cMoA\u7684\u8d21\u732e\u3002", "conclusion": "\u672c\u6587\u7684\u7ed3\u679c\u8868\u660e\uff0c\u4ee5\u70ed\u70b9\u4e3a\u4e2d\u5fc3\u7684\u89c6\u89d2\u53ef\u4ee5\u4e3a\u672a\u6765\u7684\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4fe1\u606f\uff0c\u4e3aERC\u4e2d\u7684\u6a21\u6001\u878d\u5408\u63d0\u4f9b\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2510.08976", "categories": ["cs.CV", "cs.DC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.08976", "abs": "https://arxiv.org/abs/2510.08976", "authors": ["Maoliang Li", "Ke Li", "Yaoyang Liu", "Jiayu Chen", "Zihao Zheng", "Yinjun Wu", "Xiang Chen"], "title": "Hierarchical Scheduling for Multi-Vector Image Retrieval", "comment": "Under Review", "summary": "To effectively leverage user-specific data, retrieval augmented generation\n(RAG) is employed in multimodal large language model (MLLM) applications.\nHowever, conventional retrieval approaches often suffer from limited retrieval\naccuracy. Recent advances in multi-vector retrieval (MVR) improve accuracy by\ndecomposing queries and matching against segmented images. They still suffer\nfrom sub-optimal accuracy and efficiency, overlooking alignment between the\nquery and varying image objects and redundant fine-grained image segments. In\nthis work, we present an efficient scheduling framework for image retrieval -\nHiMIR. First, we introduce a novel hierarchical paradigm, employing multiple\nintermediate granularities for varying image objects to enhance alignment.\nSecond, we minimize redundancy in retrieval by leveraging cross-hierarchy\nsimilarity consistency and hierarchy sparsity to minimize unnecessary matching\ncomputation. Furthermore, we configure parameters for each dataset\nautomatically for practicality across diverse scenarios. Our empirical study\nshows that, HiMIR not only achieves substantial accuracy improvements but also\nreduces computation by up to 3.5 times over the existing MVR system.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHiMIR\u7684\u9ad8\u6548\u56fe\u50cf\u68c0\u7d22\u8c03\u5ea6\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e2d\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u68c0\u7d22\u65b9\u6cd5\u7cbe\u5ea6\u6709\u9650\uff0c\u591a\u5411\u91cf\u68c0\u7d22(MVR)\u867d\u7136\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u4ecd\u5b58\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u95ee\u9898\uff0c\u5ffd\u7565\u4e86\u67e5\u8be2\u548c\u56fe\u50cf\u5bf9\u8c61\u4e4b\u95f4\u7684\u5bf9\u9f50\u4ee5\u53ca\u5197\u4f59\u7684\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u5272\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u5206\u5c42\u8303\u5f0f\uff0c\u5229\u7528\u591a\u4e2a\u4e2d\u95f4\u7c92\u5ea6\u6765\u5bf9\u4e0d\u540c\u7684\u56fe\u50cf\u5bf9\u8c61\u8fdb\u884c\u5bf9\u9f50\uff0c\u5e76\u901a\u8fc7\u4ea4\u53c9\u5c42\u6b21\u76f8\u4f3c\u6027\u4e00\u81f4\u6027\u548c\u5c42\u6b21\u7a00\u758f\u6027\u6765\u51cf\u5c11\u68c0\u7d22\u4e2d\u7684\u5197\u4f59\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHiMIR\u5728\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u4e0e\u73b0\u6709\u7684MVR\u7cfb\u7edf\u76f8\u6bd4\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c11\u4e863.5\u500d\u3002", "conclusion": "HiMIR\u6846\u67b6\u5728\u56fe\u50cf\u68c0\u7d22\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u51c6\u786e\u7387\u63d0\u5347\u548c\u8ba1\u7b97\u6548\u7387\u7684\u964d\u4f4e\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e2d\u7684RAG\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.08728", "categories": ["cs.CV", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.08728", "abs": "https://arxiv.org/abs/2510.08728", "authors": ["Nicolas Ewen", "Jairo Diaz-Rodriguez", "Kelly Ramsay"], "title": "Structured Output Regularization: a framework for few-shot transfer learning", "comment": null, "summary": "Traditional transfer learning typically reuses large pre-trained networks by\nfreezing some of their weights and adding task-specific layers. While this\napproach is computationally efficient, it limits the model's ability to adapt\nto domain-specific features and can still lead to overfitting with very limited\ndata. To address these limitations, we propose Structured Output Regularization\n(SOR), a simple yet effective framework that freezes the internal network\nstructures (e.g., convolutional filters) while using a combination of group\nlasso and $L_1$ penalties. This framework tailors the model to specific data\nwith minimal additional parameters and is easily applicable to various network\ncomponents, such as convolutional filters or various blocks in neural networks\nenabling broad applicability for transfer learning tasks. We evaluate SOR on\nthree few shot medical imaging classification tasks and we achieve competitive\nresults using DenseNet121, and EfficientNetB4 bases compared to established\nbenchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u7ed3\u6784\u5316\u8f93\u51fa\u6b63\u5219\u5316 (SOR) \u7684\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u51bb\u7ed3\u5185\u90e8\u7f51\u7edc\u7ed3\u6784\uff0c\u540c\u65f6\u4f7f\u7528 group lasso \u548c L1 \u60e9\u7f5a\u7684\u7ec4\u5408\uff0c\u7528\u6700\u5c11\u7684\u989d\u5916\u53c2\u6570\u5b9a\u5236\u6a21\u578b\u5230\u7279\u5b9a\u6570\u636e\u3002", "motivation": "\u4f20\u7edf\u7684\u8fc1\u79fb\u5b66\u4e60\u901a\u5e38\u901a\u8fc7\u51bb\u7ed3\u4e00\u4e9b\u6743\u91cd\u5e76\u6dfb\u52a0\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u5c42\u6765\u91cd\u7528\u5927\u578b\u9884\u8bad\u7ec3\u7f51\u7edc\u3002\u867d\u7136\u8fd9\u79cd\u65b9\u6cd5\u5728\u8ba1\u7b97\u4e0a\u5f88\u6709\u6548\uff0c\u4f46\u5b83\u9650\u5236\u4e86\u6a21\u578b\u9002\u5e94\u7279\u5b9a\u9886\u57df\u7279\u5f81\u7684\u80fd\u529b\uff0c\u5e76\u4e14\u4ecd\u7136\u53ef\u80fd\u5bfc\u81f4\u6570\u636e\u975e\u5e38\u6709\u9650\u7684\u8fc7\u62df\u5408\u3002", "method": "\u51bb\u7ed3\u5185\u90e8\u7f51\u7edc\u7ed3\u6784\uff08\u4f8b\u5982\uff0c\u5377\u79ef\u6ee4\u6ce2\u5668\uff09\uff0c\u540c\u65f6\u4f7f\u7528 group lasso \u548c L1 \u60e9\u7f5a\u7684\u7ec4\u5408\u3002", "result": "\u5728\u4e09\u4e2a\u5c0f\u6837\u672c\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8bc4\u4f30\u4e86 SOR\uff0c\u5e76\u4e14\u4e0e\u5df2\u5efa\u7acb\u7684\u57fa\u51c6\u76f8\u6bd4\uff0c\u4f7f\u7528 DenseNet121 \u548c EfficientNetB4 \u57fa\u7840\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002", "conclusion": "SOR \u6846\u67b6\u53ef\u4ee5\u7528\u6700\u5c11\u7684\u989d\u5916\u53c2\u6570\u5c06\u6a21\u578b\u5b9a\u5236\u5230\u7279\u5b9a\u6570\u636e\uff0c\u5e76\u4e14\u6613\u4e8e\u5e94\u7528\u4e8e\u5404\u79cd\u7f51\u7edc\u7ec4\u4ef6\uff0c\u4f8b\u5982\u5377\u79ef\u6ee4\u6ce2\u5668\u6216\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u5404\u79cd\u5757\uff0c\u4ece\u800c\u80fd\u591f\u5e7f\u6cdb\u5e94\u7528\u4e8e\u8fc1\u79fb\u5b66\u4e60\u4efb\u52a1\u3002"}}
{"id": "2510.08724", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08724", "abs": "https://arxiv.org/abs/2510.08724", "authors": ["Ozgur Guldogan", "Neeraj Sarna", "Yuanyuan Li", "Michael Berger"], "title": "Counterfactually Fair Conformal Prediction", "comment": null, "summary": "While counterfactual fairness of point predictors is well studied, its\nextension to prediction sets--central to fair decision-making under\nuncertainty--remains underexplored. On the other hand, conformal prediction\n(CP) provides efficient, distribution-free, finite-sample valid prediction\nsets, yet does not ensure counterfactual fairness. We close this gap by\ndeveloping Counterfactually Fair Conformal Prediction (CF-CP) that produces\ncounterfactually fair prediction sets. Through symmetrization of conformity\nscores across protected-attribute interventions, we prove that CF-CP results in\ncounterfactually fair prediction sets while maintaining the marginal coverage\nproperty. Furthermore, we empirically demonstrate that on both synthetic and\nreal datasets, across regression and classification tasks, CF-CP achieves the\ndesired counterfactual fairness and meets the target coverage rate with minimal\nincrease in prediction set size. CF-CP offers a simple, training-free route to\ncounterfactually fair uncertainty quantification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u53cd\u4e8b\u5b9e\u516c\u5e73\u5171\u5f62\u9884\u6d4b (CF-CP)\u201d \u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u53cd\u4e8b\u5b9e\u516c\u5e73\u7684\u9884\u6d4b\u96c6\u3002", "motivation": "\u867d\u7136\u70b9\u9884\u6d4b\u7684\u53cd\u4e8b\u5b9e\u516c\u5e73\u6027\u5df2\u88ab\u5145\u5206\u7814\u7a76\uff0c\u4f46\u5c06\u5176\u6269\u5c55\u5230\u9884\u6d4b\u96c6\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u5171\u5f62\u9884\u6d4b (CP) \u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u65e0\u5206\u5e03\u3001\u6709\u9650\u6837\u672c\u6709\u6548\u7684\u9884\u6d4b\u96c6\uff0c\u4f46\u4e0d\u80fd\u786e\u4fdd\u53cd\u4e8b\u5b9e\u516c\u5e73\u6027\u3002", "method": "\u901a\u8fc7\u5bf9\u53d7\u4fdd\u62a4\u5c5e\u6027\u5e72\u9884\u7684\u4e00\u81f4\u6027\u5206\u6570\u8fdb\u884c\u5bf9\u79f0\u5316\uff0c\u5b9e\u73b0\u4e86\u53cd\u4e8b\u5b9e\u516c\u5e73\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cCF-CP \u5b9e\u73b0\u4e86\u9884\u671f\u7684\u53cd\u4e8b\u5b9e\u516c\u5e73\u6027\uff0c\u5e76\u4ee5\u6700\u5c0f\u7684\u9884\u6d4b\u96c6\u5927\u5c0f\u589e\u52a0\u6ee1\u8db3\u4e86\u76ee\u6807\u8986\u76d6\u7387\u3002", "conclusion": "CF-CP \u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u9014\u5f84\u6765\u5b9e\u73b0\u53cd\u4e8b\u5b9e\u516c\u5e73\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002"}}
{"id": "2510.08608", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08608", "abs": "https://arxiv.org/abs/2510.08608", "authors": ["Weihua Zheng", "Zhengyuan Liu", "Tanmoy Chakraborty", "Weiwen Xu", "Xiaoxue Gao", "Bryan Chen Zhengyu Tan", "Bowei Zou", "Chang Liu", "Yujia Hu", "Xing Xie", "Xiaoyuan Yi", "Jing Yao", "Chaojun Wang", "Long Li", "Rui Liu", "Huiyao Liu", "Koji Inoue", "Ryuichi Sumida", "Tatsuya Kawahara", "Fan Xu", "Lingyu Ye", "Wei Tian", "Dongjun Kim", "Jimin Jung", "Jaehyung Seo", "Nadya Yuki Wangsajaya", "Pham Minh Duc", "Ojasva Saxena", "Palash Nandi", "Xiyan Tao", "Wiwik Karlina", "Tuan Luong", "Keertana Arun Vasan", "Roy Ka-Wei Lee", "Nancy F. Chen"], "title": "MMA-ASIA: A Multilingual and Multimodal Alignment Framework for Culturally-Grounded Evaluation", "comment": null, "summary": "Large language models (LLMs) are now used worldwide, yet their multimodal\nunderstanding and reasoning often degrade outside Western, high-resource\nsettings. We propose MMA-ASIA, a comprehensive framework to evaluate LLMs'\ncultural awareness with a focus on Asian contexts. MMA-ASIA centers on a\nhuman-curated, multilingual, and multimodally aligned multiple-choice benchmark\ncovering 8 Asian countries and 10 languages, comprising 27,000 questions; over\n79 percent require multi-step reasoning grounded in cultural context, moving\nbeyond simple memorization. To our knowledge, this is the first dataset aligned\nat the input level across three modalities: text, image (visual question\nanswering), and speech. This enables direct tests of cross-modal transfer.\nBuilding on this benchmark, we propose a five-dimensional evaluation protocol\nthat measures: (i) cultural-awareness disparities across countries, (ii)\ncross-lingual consistency, (iii) cross-modal consistency, (iv) cultural\nknowledge generalization, and (v) grounding validity. To ensure rigorous\nassessment, a Cultural Awareness Grounding Validation Module detects \"shortcut\nlearning\" by checking whether the requisite cultural knowledge supports correct\nanswers. Finally, through comparative model analysis, attention tracing, and an\ninnovative Vision-ablated Prefix Replay (VPR) method, we probe why models\ndiverge across languages and modalities, offering actionable insights for\nbuilding culturally reliable multimodal LLMs.", "AI": {"tldr": "MMA-ASIA\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u5728\u4e9a\u6d32\u6587\u5316\u80cc\u666f\u4e0b\u6587\u5316\u610f\u8bc6\u7684\u7efc\u5408\u6846\u67b6\uff0c\u5305\u542b\u4e00\u4e2a\u591a\u8bed\u8a00\u3001\u591a\u6a21\u6001\u5bf9\u9f50\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u897f\u65b9\u4ee5\u5916\u3001\u9ad8\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u591a\u6a21\u6001\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e94\u7ef4\u8bc4\u4f30\u534f\u8bae\uff0c\u5e76\u4f7f\u7528\u6587\u5316\u610f\u8bc6\u57fa\u7840\u9a8c\u8bc1\u6a21\u5757\u6765\u68c0\u6d4b\u201c\u6377\u5f84\u5b66\u4e60\u201d\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u6db5\u76d68\u4e2a\u4e9a\u6d32\u56fd\u5bb6\u548c10\u79cd\u8bed\u8a00\uff0c\u5305\u542b27,000\u4e2a\u95ee\u9898\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u8d85\u8fc779%\u7684\u95ee\u9898\u9700\u8981\u57fa\u4e8e\u6587\u5316\u80cc\u666f\u7684\u591a\u6b65\u9aa4\u63a8\u7406\u3002", "conclusion": "\u901a\u8fc7\u6bd4\u8f83\u6a21\u578b\u5206\u6790\u3001\u6ce8\u610f\u529b\u8ffd\u8e2a\u548cVPR\u65b9\u6cd5\uff0c\u63a2\u7a76\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u6a21\u6001\u4e0a\u7684\u5dee\u5f02\u539f\u56e0\uff0c\u4e3a\u6784\u5efa\u6587\u5316\u4e0a\u53ef\u9760\u7684\u591a\u6a21\u6001LLM\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002"}}
{"id": "2510.09435", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.09435", "abs": "https://arxiv.org/abs/2510.09435", "authors": ["Hyunin Lee", "Yong Zhang", "Hoang Vu Nguyen", "Xiaoyi Liu", "Namyong Park", "Christopher Jung", "Rong Jin", "Yang Wang", "Zhigang Wang", "Somayeh Sojoudi", "Xue Feng"], "title": "Cross-attention Secretly Performs Orthogonal Alignment in Recommendation Models", "comment": "19 pages", "summary": "Cross-domain sequential recommendation (CDSR) aims to align heterogeneous\nuser behavior sequences collected from different domains. While cross-attention\nis widely used to enhance alignment and improve recommendation performance, its\nunderlying mechanism is not fully understood. Most researchers interpret\ncross-attention as residual alignment, where the output is generated by\nremoving redundant and preserving non-redundant information from the query\ninput by referencing another domain data which is input key and value. Beyond\nthe prevailing view, we introduce Orthogonal Alignment, a phenomenon in which\ncross-attention discovers novel information that is not present in the query\ninput, and further argue that those two contrasting alignment mechanisms can\nco-exist in recommendation models We find that when the query input and output\nof cross-attention are orthogonal, model performance improves over 300\nexperiments. Notably, Orthogonal Alignment emerges naturally, without any\nexplicit orthogonality constraints. Our key insight is that Orthogonal\nAlignment emerges naturally because it improves scaling law. We show that\nbaselines additionally incorporating cross-attention module outperform\nparameter-matched baselines, achieving a superior accuracy-per-model parameter.\nWe hope these findings offer new directions for parameter-efficient scaling in\nmulti-modal research.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8de8\u57df\u5e8f\u5217\u63a8\u8350\uff08CDSR\uff09\u4e2d\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u673a\u5236\uff0c\u53d1\u73b0\u9664\u4e86\u53bb\u9664\u5197\u4f59\u4fe1\u606f\u7684\u6b8b\u5dee\u5bf9\u9f50\u5916\uff0c\u8fd8\u5b58\u5728\u4e00\u79cd\u6b63\u4ea4\u5bf9\u9f50\u73b0\u8c61\uff0c\u5373\u4ea4\u53c9\u6ce8\u610f\u529b\u53ef\u4ee5\u53d1\u73b0\u67e5\u8be2\u8f93\u5165\u4e2d\u4e0d\u5b58\u5728\u7684\u65b0\u4fe1\u606f\u3002", "motivation": "\u76ee\u524d\u5bf9\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u673a\u5236\u7406\u89e3\u4e0d\u5145\u5206\uff0c\u591a\u6570\u7814\u7a76\u5c06\u5176\u89e3\u91ca\u4e3a\u6b8b\u5dee\u5bf9\u9f50\u3002", "method": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff08\u8d85\u8fc7300\u6b21\uff09\u89c2\u5bdf\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u884c\u4e3a\uff0c\u5e76\u5206\u6790\u5176\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u5f53\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u67e5\u8be2\u8f93\u5165\u548c\u8f93\u51fa\u6b63\u4ea4\u65f6\uff0c\u6a21\u578b\u6027\u80fd\u63d0\u5347\u3002\u6b63\u4ea4\u5bf9\u9f50\u7684\u51fa\u73b0\u662f\u56e0\u4e3a\u5b83\u53ef\u4ee5\u6539\u8fdb\u7f29\u653e\u5b9a\u5f8b\u3002\u5f15\u5165\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u7684\u57fa\u7ebf\u6a21\u578b\u4f18\u4e8e\u53c2\u6570\u5339\u914d\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u6b63\u4ea4\u5bf9\u9f50\u73b0\u8c61\u4e3a\u591a\u6a21\u6001\u7814\u7a76\u4e2d\u7684\u53c2\u6570\u9ad8\u6548\u7f29\u653e\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2510.08759", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08759", "abs": "https://arxiv.org/abs/2510.08759", "authors": ["Yu Qi", "Haibo Zhao", "Ziyu Guo", "Siyuan Ma", "Ziyan Chen", "Yaokun Han", "Renrui Zhang", "Zitiantao Lin", "Shiji Xin", "Yijian Huang", "Kai Cheng", "Peiheng Wang", "Jiazheng Liu", "Jiayi Zhang", "Yizhe Zhu", "Wenqing Wang", "Yiran Qin", "Xupeng Zhu", "Haojie Huang", "Lawson L. S. Wong"], "title": "BEAR: Benchmarking and Enhancing Multimodal Language Models for Atomic Embodied Capabilities", "comment": null, "summary": "Embodied capabilities refer to a suite of fundamental abilities for an agent\nto perceive, comprehend, and interact with the physical world. While multimodal\nlarge language models (MLLMs) show promise as embodied agents, a thorough and\nsystematic evaluation of their embodied capabilities remains underexplored, as\nexisting benchmarks primarily focus on specific domains such as planning or\nspatial understanding. To bridge this gap, we introduce BEAR, a comprehensive\nand fine-grained benchmark that evaluates MLLMs on atomic embodied\ncapabilities. BEAR comprises 4,469 interleaved image-video-text entries across\n14 domains in 6 categories, including tasks from low-level pointing, trajectory\nunderstanding, spatial reasoning, to high-level planning. Extensive evaluation\nresults of 20 representative MLLMs reveal their persistent limitations across\nall domains of embodied capabilities. To tackle the shortfall, we propose\nBEAR-Agent, a multimodal conversable agent that integrates pretrained vision\nmodels to strengthen MLLM perception, 3D understanding, and planning\ncapabilities. It substantially enhances MLLM performance across diverse\nembodied capabilities on BEAR, yielding a 9.12% absolute gain and a relative\nimprovement of 17.5% on GPT-5. Furthermore, our experiments indicate that\nimproving MLLM embodied capabilities can benefit embodied tasks in simulated\nenvironments. Project website: https://bear-official66.github.io/", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aBEAR\u7684\u7efc\u5408\u6027benchmark\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u5177\u8eab\u80fd\u529b\u65b9\u9762\u7684\u8868\u73b0\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u7684MLLM\u5728\u5404\u9879\u5177\u8eab\u80fd\u529b\u4e0a\u5b58\u5728\u5c40\u9650\u6027\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86BEAR-Agent\uff0c\u901a\u8fc7\u6574\u5408\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u6a21\u578b\u6765\u63d0\u5347MLLM\u7684\u611f\u77e5\u30013D\u7406\u89e3\u548c\u89c4\u5212\u80fd\u529b\uff0c\u4ece\u800c\u5728BEAR benchmark\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684benchmark\u4e3b\u8981\u96c6\u4e2d\u5728\u7279\u5b9a\u9886\u57df\uff0c\u5bf9MLLM\u7684\u5177\u8eab\u80fd\u529b\u7f3a\u4e4f\u5168\u9762\u7cfb\u7edf\u7684\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86BEAR benchmark\uff0c\u5305\u542b4,469\u4e2a\u56fe\u50cf-\u89c6\u9891-\u6587\u672c\u6761\u76ee\uff0c\u6db5\u76d614\u4e2a\u9886\u57df\u548c6\u4e2a\u7c7b\u522b\uff0c\u4ece\u4f4e\u7ea7\u6307\u5411\u3001\u8f68\u8ff9\u7406\u89e3\u3001\u7a7a\u95f4\u63a8\u7406\u5230\u9ad8\u7ea7\u89c4\u5212\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86BEAR-Agent\uff0c\u4e00\u4e2a\u591a\u6a21\u6001\u53ef\u5bf9\u8bdd\u7684agent\uff0c\u96c6\u6210\u4e86\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u3002", "result": "\u5bf920\u4e2aMLLM\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u5b83\u4eec\u5728\u5177\u8eab\u80fd\u529b\u7684\u5404\u4e2a\u9886\u57df\u90fd\u5b58\u5728\u6301\u7eed\u7684\u5c40\u9650\u6027\u3002BEAR-Agent\u5728BEAR benchmark\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86MLLM\u7684\u6027\u80fd\uff0c\u83b7\u5f97\u4e869.12%\u7684\u7edd\u5bf9\u6536\u76ca\u548c17.5%\u7684\u76f8\u5bf9\u63d0\u5347\uff08\u4e0eGPT-5\u76f8\u6bd4\uff09\u3002", "conclusion": "\u63d0\u9ad8MLLM\u7684\u5177\u8eab\u80fd\u529b\u53ef\u4ee5\u6539\u5584\u6a21\u62df\u73af\u5883\u4e2d\u7684\u5177\u8eab\u4efb\u52a1\u8868\u73b0\u3002"}}
{"id": "2510.08959", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08959", "abs": "https://arxiv.org/abs/2510.08959", "authors": ["Jinxin Shi", "Zongsheng Cao", "Runmin Ma", "Yusong Hu", "Jie Zhou", "Xin Li", "Lei Bai", "Liang He", "Bo Zhang"], "title": "DualResearch: Entropy-Gated Dual-Graph Retrieval for Answer Reconstruction", "comment": "16 pages, 6 figures, 5 tables, Under Review", "summary": "The deep-research framework orchestrates external tools to perform complex,\nmulti-step scientific reasoning that exceeds the native limits of a single\nlarge language model. However, it still suffers from context pollution, weak\nevidentiary support, and brittle execution paths. To address these issues, we\npropose DualResearch, a retrieval and fusion framework that matches the\nepistemic structure of tool-intensive reasoning by jointly modeling two\ncomplementary graphs: a breadth semantic graph that encodes stable background\nknowledge, and a depth causal graph that captures execution provenance. Each\ngraph has a layer-native relevance function, seed-anchored semantic diffusion\nfor breadth, and causal-semantic path matching with reliability weighting for\ndepth. To reconcile their heterogeneity and query-dependent uncertainty,\nDualResearch converts per-layer path evidence into answer distributions and\nfuses them in log space via an entropy-gated rule with global calibration. The\nfusion up-weights the more certain channel and amplifies agreement. As a\ncomplement to deep-research systems, DualResearch compresses lengthy multi-tool\nexecution logs into a concise reasoning graph, and we show that it can\nreconstruct answers stably and effectively. On the scientific reasoning\nbenchmarks HLE and GPQA, DualResearch achieves competitive performance. Using\nlog files from the open-source system InternAgent, its accuracy improves by\n7.7% on HLE and 6.06% on GPQA.", "AI": {"tldr": "\u63d0\u51fa DualResearch\uff0c\u4e00\u4e2a\u68c0\u7d22\u548c\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u5e7f\u5ea6\u8bed\u4e49\u56fe\u548c\u6df1\u5ea6\u56e0\u679c\u56fe\u6765\u5339\u914d\u5de5\u5177\u5bc6\u96c6\u578b\u63a8\u7406\u7684\u8ba4\u77e5\u7ed3\u6784\uff0c\u4ee5\u89e3\u51b3\u6df1\u5c42\u7814\u7a76\u6846\u67b6\u4e2d\u7684\u95ee\u9898\u3002", "motivation": "\u6df1\u5c42\u7814\u7a76\u6846\u67b6\u5b58\u5728\u4e0a\u4e0b\u6587\u6c61\u67d3\u3001\u8bc1\u636e\u652f\u6301\u4e0d\u8db3\u548c\u6267\u884c\u8def\u5f84\u8106\u5f31\u7684\u95ee\u9898\u3002", "method": "DualResearch \u6846\u67b6\u8054\u5408\u5efa\u6a21\u5e7f\u5ea6\u8bed\u4e49\u56fe\uff08\u7f16\u7801\u80cc\u666f\u77e5\u8bc6\uff09\u548c\u6df1\u5ea6\u56e0\u679c\u56fe\uff08\u6355\u83b7\u6267\u884c\u8fc7\u7a0b\uff09\uff0c\u5e76\u4f7f\u7528\u5c42\u539f\u751f\u76f8\u5173\u6027\u51fd\u6570\u3001\u79cd\u5b50\u951a\u5b9a\u7684\u8bed\u4e49\u6269\u6563\u548c\u56e0\u679c\u8bed\u4e49\u8def\u5f84\u5339\u914d\u3002", "result": "\u5728 HLE \u548c GPQA \u79d1\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDualResearch \u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u5728\u4f7f\u7528 InternAgent \u7684\u65e5\u5fd7\u6587\u4ef6\u65f6\uff0c\u5176\u5728 HLE \u4e0a\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e86 7.7%\uff0c\u5728 GPQA \u4e0a\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e86 6.06%\u3002", "conclusion": "DualResearch \u53ef\u4ee5\u5c06\u5197\u957f\u7684\u591a\u5de5\u5177\u6267\u884c\u65e5\u5fd7\u538b\u7f29\u6210\u7b80\u6d01\u7684\u63a8\u7406\u56fe\uff0c\u5e76\u80fd\u7a33\u5b9a\u6709\u6548\u5730\u91cd\u5efa\u7b54\u6848\uff0c\u4f5c\u4e3a\u6df1\u5c42\u7814\u7a76\u7cfb\u7edf\u7684\u8865\u5145\u3002"}}
{"id": "2510.08734", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08734", "abs": "https://arxiv.org/abs/2510.08734", "authors": ["Hanna Mazzawi", "Benoit Dherin", "Michael Munn", "Michael Wunder", "Javier Gonzalvo"], "title": "Transmuting prompts into weights", "comment": null, "summary": "A growing body of research has demonstrated that the behavior of large\nlanguage models can be effectively controlled at inference time by directly\nmodifying their internal states, either through vector additions to their\nactivations or through updates to their weight matrices. These techniques,\nwhile powerful, are often guided by empirical heuristics, such as deriving\nsteering vectors from the average activations of contrastive prompts. This work\nprovides a theoretical foundation for these interventions, explaining how they\nemerge from the fundamental computations of the transformer architecture.\nBuilding on the recent finding that a prompt's influence can be mathematically\nmapped to implicit weight updates (Dherin et al., 2025), we generalize this\ntheory to deep, multi-block transformers. We show how the information contained\nin any chunk of a user prompt is represented and composed internally through\nweight vectors and weight matrices. We then derive a principled method for\ncondensing this information into token-independent thought vectors and thought\nmatrices. These constructs provide a theoretical explanation for existing\nvector- and matrix-based model editing techniques and offer a direct,\ncomputationally-grounded method for transmuting textual input into reusable\nweight updates.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u884c\u4e3a\u53ef\u4ee5\u901a\u8fc7\u5728\u63a8\u7406\u65f6\u76f4\u63a5\u4fee\u6539\u5176\u5185\u90e8\u72b6\u6001\u6765\u6709\u6548\u63a7\u5236\u3002\u672c\u6587\u4e3a\u8fd9\u4e9b\u5e72\u9884\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u57fa\u7840\uff0c\u89e3\u91ca\u4e86\u5b83\u4eec\u662f\u5982\u4f55\u4ece Transformer \u67b6\u6784\u7684\u57fa\u672c\u8ba1\u7b97\u4e2d\u4ea7\u751f\u7684\u3002", "motivation": "\u73b0\u6709\u7684\u901a\u8fc7\u4fee\u6539\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u72b6\u6001\u6765\u63a7\u5236\u5176\u884c\u4e3a\u7684\u6280\u672f\uff0c\u901a\u5e38\u4f9d\u8d56\u4e8e\u7ecf\u9a8c\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u4e3a\u8fd9\u4e9b\u5e72\u9884\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u5c06 prompt \u7684\u5f71\u54cd\u6620\u5c04\u5230\u9690\u5f0f\u6743\u91cd\u66f4\u65b0\uff0c\u5e76\u5c06\u8be5\u7406\u8bba\u63a8\u5e7f\u5230\u6df1\u5c42\u3001\u591a\u5757 Transformer\u3002\u63a8\u5bfc\u51fa\u4e00\u79cd\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u5c06\u4fe1\u606f\u6d53\u7f29\u6210\u72ec\u7acb\u4e8e token \u7684\u601d\u60f3\u5411\u91cf\u548c\u601d\u60f3\u77e9\u9635\u3002", "result": "\u89e3\u91ca\u4e86\u73b0\u6709\u7684\u57fa\u4e8e\u5411\u91cf\u548c\u77e9\u9635\u7684\u6a21\u578b\u7f16\u8f91\u6280\u672f\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u76f4\u63a5\u7684\u3001\u57fa\u4e8e\u8ba1\u7b97\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u6587\u672c\u8f93\u5165\u8f6c\u6362\u4e3a\u53ef\u91cd\u7528\u7684\u6743\u91cd\u66f4\u65b0\u3002", "conclusion": "\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u72b6\u6001\u5e72\u9884\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\u548c\u8ba1\u7b97\u65b9\u6cd5\u3002"}}
{"id": "2510.08613", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08613", "abs": "https://arxiv.org/abs/2510.08613", "authors": ["Xinnan Dai", "Kai Guo", "Chung-Hsiang Lo", "Shenglai Zeng", "Jiayuan Ding", "Dongsheng Luo", "Subhabrata Mukherjee", "Jiliang Tang"], "title": "GraphGhost: Tracing Structures Behind Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) demonstrate remarkable reasoning capabilities,\nyet the structural mechanisms underlying these abilities remain under explored.\nIn this work, we introduce GraphGhost, a unified framework that represents\nneuron activations and their signal propagation as graphs, explaining how LLMs\ncapture structural semantics from sequential inputs and generate outputs\nthrough structurally consistent mechanisms. This graph-based perspective\nenables us to employ graph algorithms such as PageRank to characterize the\nproperties of LLMs, revealing both shared and model-specific reasoning\nbehaviors across diverse datasets. We further identify the activated neurons\nwithin GraphGhost and evaluate them through structural interventions, showing\nthat edits to key neuron nodes can trigger reasoning collapse, altering both\nlogical flow and semantic understanding. Together, these contributions position\nGraphGhost as a powerful tool for analyzing, intervening in, and ultimately\nunderstanding the structural foundations of reasoning in LLMs.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86 GraphGhost \u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u7ed3\u6784\u5316\u63a8\u7406\u673a\u5236\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u63a8\u7406\u80fd\u529b\u7684\u7ed3\u6784\u673a\u5236\u3002", "method": "\u5c06\u795e\u7ecf\u5143\u6fc0\u6d3b\u53ca\u5176\u4fe1\u53f7\u4f20\u64ad\u8868\u793a\u4e3a\u56fe\uff0c\u5e76\u4f7f\u7528\u56fe\u7b97\u6cd5\uff08\u5982 PageRank\uff09\u6765\u8868\u5f81 LLM \u7684\u5c5e\u6027\u3002", "result": "\u63ed\u793a\u4e86\u4e0d\u540c\u6570\u636e\u96c6\u4e0a LLM \u7684\u5171\u4eab\u548c\u6a21\u578b\u7279\u5b9a\u7684\u63a8\u7406\u884c\u4e3a\u3002\u5bf9\u5173\u952e\u795e\u7ecf\u5143\u8282\u70b9\u8fdb\u884c\u7f16\u8f91\u53ef\u4ee5\u89e6\u53d1\u63a8\u7406\u5d29\u6e83\uff0c\u6539\u53d8\u903b\u8f91\u6d41\u7a0b\u548c\u8bed\u4e49\u7406\u89e3\u3002", "conclusion": "GraphGhost \u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u5206\u6790\u3001\u5e72\u9884\u548c\u6700\u7ec8\u7406\u89e3 LLM \u4e2d\u63a8\u7406\u7684\u7ed3\u6784\u57fa\u7840\u3002"}}
{"id": "2510.08761", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08761", "abs": "https://arxiv.org/abs/2510.08761", "authors": ["Jiayang Liu", "Daniel Tso", "Yiming Bu", "Qinru Qiu"], "title": "SAFER-AiD: Saccade-Assisted Foveal-peripheral vision Enhanced Reconstruction for Adversarial Defense", "comment": null, "summary": "Adversarial attacks significantly challenge the safe deployment of deep\nlearning models, particularly in real-world applications. Traditional defenses\noften rely on computationally intensive optimization (e.g., adversarial\ntraining or data augmentation) to improve robustness, whereas the human visual\nsystem achieves inherent robustness to adversarial perturbations through\nevolved biological mechanisms. We hypothesize that attention guided\nnon-homogeneous sparse sampling and predictive coding plays a key role in this\nrobustness. To test this hypothesis, we propose a novel defense framework\nincorporating three key biological mechanisms: foveal-peripheral processing,\nsaccadic eye movements, and cortical filling-in. Our approach employs\nreinforcement learning-guided saccades to selectively capture multiple\nfoveal-peripheral glimpses, which are integrated into a reconstructed image\nbefore classification. This biologically inspired preprocessing effectively\nmitigates adversarial noise, preserves semantic integrity, and notably requires\nno retraining or fine-tuning of downstream classifiers, enabling seamless\nintegration with existing systems. Experiments on the ImageNet dataset\ndemonstrate that our method improves system robustness across diverse\nclassifiers and attack types, while significantly reducing training overhead\ncompared to both biologically and non-biologically inspired defense techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u4e09\u79cd\u5173\u952e\u7684\u751f\u7269\u673a\u5236\uff1a\u4e2d\u592e\u51f9-\u5468\u8fb9\u5904\u7406\u3001\u626b\u89c6\u773c\u52a8\u548c\u76ae\u8d28\u586b\u5145\u3002", "motivation": "\u5bf9\u6297\u6027\u653b\u51fb\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u4e2d\u3002\u4f20\u7edf\u7684\u9632\u5fa1\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u8ba1\u7b97\u5bc6\u96c6\u578b\u4f18\u5316\uff08\u4f8b\u5982\uff0c\u5bf9\u6297\u6027\u8bad\u7ec3\u6216\u6570\u636e\u589e\u5f3a\uff09\u6765\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u800c\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u901a\u8fc7\u8fdb\u5316\u7684\u751f\u7269\u673a\u5236\u5b9e\u73b0\u4e86\u5bf9\u5bf9\u6297\u6027\u6270\u52a8\u7684\u5185\u5728\u9c81\u68d2\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u5f15\u5bfc\u7684\u626b\u89c6\u6765\u9009\u62e9\u6027\u5730\u6355\u83b7\u591a\u4e2a\u4e2d\u592e\u51f9-\u5468\u8fb9\u4e00\u77a5\uff0c\u7136\u540e\u5c06\u5176\u96c6\u6210\u5230\u91cd\u5efa\u56fe\u50cf\u4e2d\uff0c\u7136\u540e\u518d\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728 ImageNet \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u7cfb\u7edf\u5728\u4e0d\u540c\u5206\u7c7b\u5668\u548c\u653b\u51fb\u7c7b\u578b\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4e0e\u751f\u7269\u548c\u975e\u751f\u7269\u542f\u53d1\u5f0f\u9632\u5fa1\u6280\u672f\u76f8\u6bd4\uff0c\u663e\u7740\u964d\u4f4e\u4e86\u8bad\u7ec3\u5f00\u9500\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u51cf\u8f7b\u4e86\u5bf9\u6297\u6027\u566a\u58f0\uff0c\u4fdd\u7559\u4e86\u8bed\u4e49\u5b8c\u6574\u6027\uff0c\u5e76\u4e14\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u65e0\u9700\u5bf9\u4e0b\u6e38\u5206\u7c7b\u5668\u8fdb\u884c\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\uff0c\u4ece\u800c\u53ef\u4ee5\u4e0e\u73b0\u6709\u7cfb\u7edf\u65e0\u7f1d\u96c6\u6210\u3002"}}
{"id": "2510.08966", "categories": ["cs.AI", "cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.08966", "abs": "https://arxiv.org/abs/2510.08966", "authors": ["Ruitong Liu", "Yan Wen", "Te Sun", "Yunjia Wu", "Pingyang Huang", "Zihang Yu", "Siyuan Li"], "title": "Semantic-Condition Tuning: Fusing Graph Context with Large Language Models for Knowledge Graph Completion", "comment": "11 pages, 3 figures, conference", "summary": "Fusing Knowledge Graphs with Large Language Models is crucial for\nknowledge-intensive tasks like knowledge graph completion. The prevailing\nparadigm, prefix-tuning, simply concatenates knowledge embeddings with text\ninputs. However, this shallow fusion overlooks the rich relational semantics\nwithin KGs and imposes a significant implicit reasoning burden on the LLM to\ncorrelate the prefix with the text. To address these, we propose\nSemantic-condition Tuning (SCT), a new knowledge injection paradigm comprising\ntwo key modules. First, a Semantic Graph Module employs a Graph Neural Network\nto extract a context-aware semantic condition from the local graph\nneighborhood, guided by knowledge-enhanced relations. Subsequently, this\ncondition is passed to a Condition-Adaptive Fusion Module, which, in turn,\nadaptively modulates the textual embedding via two parameterized projectors,\nenabling a deep, feature-wise, and knowledge-aware interaction. The resulting\npre-fused embedding is then fed into the LLM for fine-tuning. Extensive\nexperiments on knowledge graph benchmarks demonstrate that SCT significantly\noutperforms prefix-tuning and other strong baselines. Our analysis confirms\nthat by modulating the input representation with semantic graph context before\nLLM inference, SCT provides a more direct and potent signal, enabling more\naccurate and robust knowledge reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u77e5\u8bc6\u6ce8\u5165\u8303\u5f0f\uff0c\u901a\u8fc7\u8bed\u4e49\u56fe\u6a21\u5757\u548c\u6761\u4ef6\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757\uff0c\u5c06\u77e5\u8bc6\u56fe\u8c31\u7684\u8bed\u4e49\u4fe1\u606f\u878d\u5165\u5230LLM\u4e2d\uff0c\u4ee5\u63d0\u5347\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u7b49\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684prefix-tuning\u65b9\u6cd5\u5ffd\u7565\u4e86\u77e5\u8bc6\u56fe\u8c31\u4e2d\u4e30\u5bcc\u7684\u5173\u7cfb\u8bed\u4e49\uff0c\u5e76\u5bf9LLM\u63d0\u51fa\u4e86\u9690\u5f0f\u7684\u63a8\u7406\u8d1f\u62c5\uff0c\u5f71\u54cd\u4e86\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u7684\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u8bed\u4e49\u6761\u4ef6\u8c03\u6574(SCT)\u65b9\u6cd5\uff0c\u5305\u542b\u8bed\u4e49\u56fe\u6a21\u5757\u548c\u6761\u4ef6\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757\u3002\u8bed\u4e49\u56fe\u6a21\u5757\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u63d0\u53d6\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8bed\u4e49\u6761\u4ef6\uff0c\u6761\u4ef6\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757\u81ea\u9002\u5e94\u5730\u8c03\u6574\u6587\u672c\u5d4c\u5165\u3002", "result": "\u5728\u77e5\u8bc6\u56fe\u8c31\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSCT\u663e\u8457\u4f18\u4e8eprefix-tuning\u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5728LLM\u63a8\u7406\u4e4b\u524d\u7528\u8bed\u4e49\u56fe\u4e0a\u4e0b\u6587\u8c03\u5236\u8f93\u5165\u8868\u793a\uff0cSCT\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u76f4\u63a5\u548c\u6709\u6548\u7684\u4fe1\u53f7\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u51c6\u786e\u548c\u9c81\u68d2\u7684\u77e5\u8bc6\u63a8\u7406\u3002"}}
{"id": "2510.08737", "categories": ["cs.LG", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.08737", "abs": "https://arxiv.org/abs/2510.08737", "authors": ["Justin Lin", "Julia Fukuyama"], "title": "SHAP-Based Supervised Clustering for Sample Classification and the Generalized Waterfall Plot", "comment": "23 pages, 15 figures, 3 tables", "summary": "In this growing age of data and technology, large black-box models are\nbecoming the norm due to their ability to handle vast amounts of data and learn\nincredibly complex input-output relationships. The deficiency of these methods,\nhowever, is their inability to explain the prediction process, making them\nuntrustworthy and their use precarious in high-stakes situations. SHapley\nAdditive exPlanations (SHAP) analysis is an explainable AI method growing in\npopularity for its ability to explain model predictions in terms of the\noriginal features. For each sample and feature in the data set, we associate a\nSHAP value that quantifies the contribution of that feature to the prediction\nof that sample. Clustering these SHAP values can provide insight into the data\nby grouping samples that not only received the same prediction, but received\nthe same prediction for similar reasons. In doing so, we map the various\npathways through which distinct samples arrive at the same prediction. To\nshowcase this methodology, we present a simulated experiment in addition to a\ncase study in Alzheimer's disease using data from the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) database. We also present a novel generalization\nof the waterfall plot for multi-classification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528SHAP\u503c\u805a\u7c7b\u6765\u7406\u89e3\u6a21\u578b\u9884\u6d4b\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u548c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u9ed1\u76d2\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u8fd9\u4f7f\u5176\u5728\u5173\u952e\u60c5\u51b5\u4e0b\u96be\u4ee5\u4fe1\u4efb\u3002SHAP\u5206\u6790\u662f\u4e00\u79cd\u6d41\u884c\u7684\u53ef\u89e3\u91caAI\u65b9\u6cd5\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u6f5c\u529b\u3002", "method": "\u4f7f\u7528SHAP\u503c\u805a\u7c7b\u6765\u63ed\u793a\u6570\u636e\u96c6\u4e2d\u5177\u6709\u76f8\u4f3c\u9884\u6d4b\u539f\u56e0\u7684\u6837\u672c\u7ec4\uff0c\u4ece\u800c\u7ed8\u5236\u51fa\u4e0d\u540c\u6837\u672c\u83b7\u5f97\u76f8\u540c\u9884\u6d4b\u7684\u8def\u5f84\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u548cADNI\u6570\u636e\u5e93\u4e2d\u7684\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u5206\u7c7b\u7011\u5e03\u56fe\u3002", "conclusion": "SHAP\u503c\u805a\u7c7b\u53ef\u4ee5\u63d0\u4f9b\u5bf9\u6570\u636e\u7684\u6d1e\u5bdf\u529b\uff0c\u5e76\u5e2e\u52a9\u7406\u89e3\u6a21\u578b\u7684\u9884\u6d4b\u8fc7\u7a0b\u3002"}}
{"id": "2510.08614", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08614", "abs": "https://arxiv.org/abs/2510.08614", "authors": ["Mingxuan Liu", "Yuhe Ke", "Wentao Zhu", "Mayli Mertens", "Yilin Ning", "Jingchi Liao", "Chuan Hong", "Daniel Shu Wei Ting", "Yifan Peng", "Danielle S. Bitterman", "Marcus Eng Hock Ong", "Nan Liu"], "title": "Gender Bias in Large Language Models for Healthcare: Assignment Consistency and Clinical Implications", "comment": null, "summary": "The integration of large language models (LLMs) into healthcare holds promise\nto enhance clinical decision-making, yet their susceptibility to biases remains\na critical concern. Gender has long influenced physician behaviors and patient\noutcomes, raising concerns that LLMs assuming human-like roles, such as\nclinicians or medical educators, may replicate or amplify gender-related\nbiases. Using case studies from the New England Journal of Medicine Challenge\n(NEJM), we assigned genders (female, male, or unspecified) to multiple\nopen-source and proprietary LLMs. We evaluated their response consistency\nacross LLM-gender assignments regarding both LLM-based diagnosis and models'\njudgments on the clinical relevance or necessity of patient gender. In our\nfindings, diagnoses were relatively consistent across LLM genders for most\nmodels. However, for patient gender's relevance and necessity in LLM-based\ndiagnosis, all models demonstrated substantial inconsistency across LLM\ngenders, particularly for relevance judgements. Some models even displayed a\nsystematic female-male disparity in their interpretation of patient gender.\nThese findings present an underexplored bias that could undermine the\nreliability of LLMs in clinical practice, underscoring the need for routine\nchecks of identity-assignment consistency when interacting with LLMs to ensure\nreliable and equitable AI-supported clinical care.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u533b\u7597\u4fdd\u5065\u9886\u57df\u7684\u5e94\u7528\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u5176\u504f\u89c1\u95ee\u9898\u4e0d\u5bb9\u5ffd\u89c6\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u4e86LLMs\u5728\u6a21\u62df\u4e34\u5e8a\u533b\u751f\u6216\u533b\u5b66\u6559\u80b2\u8005\u89d2\u8272\u65f6\uff0c\u662f\u5426\u4f1a\u590d\u5236\u6216\u653e\u5927\u4e0e\u6027\u522b\u76f8\u5173\u7684\u504f\u89c1\u3002\u901a\u8fc7\u4f7f\u7528\u65b0\u82f1\u683c\u5170\u533b\u5b66\u6742\u5fd7\u6311\u6218\u8d5b\uff08NEJM\uff09\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u6211\u4eec\u4e3a\u591a\u4e2a\u5f00\u6e90\u548c\u4e13\u6709LLMs\u5206\u914d\u4e86\u6027\u522b\uff08\u5973\u6027\u3001\u7537\u6027\u6216\u672a\u6307\u5b9a\uff09\uff0c\u5e76\u8bc4\u4f30\u4e86\u5b83\u4eec\u5728LLM\u6027\u522b\u5206\u914d\u4e2d\u7684\u53cd\u5e94\u4e00\u81f4\u6027\uff0c\u5305\u62ec\u57fa\u4e8eLLM\u7684\u8bca\u65ad\u4ee5\u53ca\u6a21\u578b\u5bf9\u60a3\u8005\u6027\u522b\u4e34\u5e8a\u76f8\u5173\u6027\u6216\u5fc5\u8981\u6027\u7684\u5224\u65ad\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u591a\u6570\u6a21\u578b\u7684\u8bca\u65ad\u5728LLM\u6027\u522b\u4e4b\u95f4\u76f8\u5bf9\u4e00\u81f4\u3002\u7136\u800c\uff0c\u5173\u4e8e\u60a3\u8005\u6027\u522b\u5728\u57fa\u4e8eLLM\u7684\u8bca\u65ad\u4e2d\u7684\u76f8\u5173\u6027\u548c\u5fc5\u8981\u6027\uff0c\u6240\u6709\u6a21\u578b\u5728LLM\u6027\u522b\u4e4b\u95f4\u90fd\u8868\u73b0\u51fa\u663e\u8457\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u5c24\u5176\u662f\u5728\u76f8\u5173\u6027\u5224\u65ad\u65b9\u9762\u3002\u4e00\u4e9b\u6a21\u578b\u751a\u81f3\u5728\u5bf9\u60a3\u8005\u6027\u522b\u7684\u89e3\u91ca\u4e2d\u8868\u73b0\u51fa\u7cfb\u7edf\u6027\u7684\u7537\u5973\u5dee\u5f02\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u4fdd\u5065\u4e2d\u7684\u6027\u522b\u504f\u89c1\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6a21\u62df\u4e34\u5e8a\u533b\u751f\u89d2\u8272\u65f6\u53ef\u80fd\u590d\u5236\u6216\u653e\u5927\u7684\u504f\u89c1\u3002", "method": "\u4f7f\u7528\u65b0\u82f1\u683c\u5170\u533b\u5b66\u6742\u5fd7\u6311\u6218\u8d5b\uff08NEJM\uff09\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u4e3a\u591a\u4e2a\u5f00\u6e90\u548c\u4e13\u6709LLMs\u5206\u914d\u6027\u522b\uff08\u5973\u6027\u3001\u7537\u6027\u6216\u672a\u6307\u5b9a\uff09\uff0c\u5e76\u8bc4\u4f30\u5b83\u4eec\u5728LLM\u6027\u522b\u5206\u914d\u4e2d\u7684\u53cd\u5e94\u4e00\u81f4\u6027\u3002", "result": "\u8bca\u65ad\u5728LLM\u6027\u522b\u4e4b\u95f4\u76f8\u5bf9\u4e00\u81f4\uff0c\u4f46\u60a3\u8005\u6027\u522b\u76f8\u5173\u6027\u548c\u5fc5\u8981\u6027\u7684\u5224\u65ad\u4e0a\u5b58\u5728\u663e\u8457\u4e0d\u4e00\u81f4\u6027\uff0c\u5c24\u5176\u662f\u5728\u76f8\u5173\u6027\u5224\u65ad\u65b9\u9762\u3002\u4e00\u4e9b\u6a21\u578b\u751a\u81f3\u5728\u5bf9\u60a3\u8005\u6027\u522b\u7684\u89e3\u91ca\u4e2d\u8868\u73b0\u51fa\u7cfb\u7edf\u6027\u7684\u7537\u5973\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4e00\u79cd\u672a\u88ab\u5145\u5206\u91cd\u89c6\u7684\u504f\u89c1\uff0c\u53ef\u80fd\u635f\u5bb3LLMs\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u53ef\u9760\u6027\u3002\u5f3a\u8c03\u9700\u8981\u5bf9\u8eab\u4efd\u5206\u914d\u4e00\u81f4\u6027\u8fdb\u884c\u5e38\u89c4\u68c0\u67e5\uff0c\u4ee5\u786e\u4fdd\u53ef\u9760\u548c\u516c\u5e73\u7684AI\u652f\u6301\u7684\u4e34\u5e8a\u62a4\u7406\u3002"}}
{"id": "2510.08770", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08770", "abs": "https://arxiv.org/abs/2510.08770", "authors": ["Gregory Yeghiyan", "Jurius Azar", "Devson Butani", "Chan-Jin Chung"], "title": "Detecting spills using thermal imaging, pretrained deep learning models, and a robotic platform", "comment": "6 pages", "summary": "This paper presents a real-time spill detection system that utilizes\npretrained deep learning models with RGB and thermal imaging to classify spill\nvs. no-spill scenarios across varied environments. Using a balanced binary\ndataset (4,000 images), our experiments demonstrate the advantages of thermal\nimaging in inference speed, accuracy, and model size. We achieve up to 100%\naccuracy using lightweight models like VGG19 and NasNetMobile, with thermal\nmodels performing faster and more robustly across different lighting\nconditions. Our system runs on consumer-grade hardware (RTX 4080) and achieves\ninference times as low as 44 ms with model sizes under 350 MB, highlighting its\ndeployability in safety-critical contexts. Results from experiments with a real\nrobot and test datasets indicate that a VGG19 model trained on thermal imaging\nperforms best.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4f7f\u7528\u9884\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548cRGB\u548c\u70ed\u6210\u50cf\u7684\u5b9e\u65f6\u6ea2\u51fa\u68c0\u6d4b\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u5404\u79cd\u73af\u5883\u4e2d\u5bf9\u6ea2\u51fa\u4e0e\u975e\u6ea2\u51fa\u60c5\u51b5\u8fdb\u884c\u5206\u7c7b\u3002", "motivation": "\u5728\u5404\u79cd\u73af\u5883\u4e2d\u51c6\u786e\u3001\u5feb\u901f\u5730\u68c0\u6d4b\u6ea2\u51fa\u60c5\u51b5\u3002", "method": "\u4f7f\u7528\u5305\u542b4000\u5f20\u56fe\u50cf\u7684\u5e73\u8861\u4e8c\u5143\u6570\u636e\u96c6\uff0c\u5229\u7528VGG19\u548cNasNetMobile\u7b49\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u7ed3\u5408RGB\u548c\u70ed\u6210\u50cf\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u4f7f\u7528VGG19\u548cNasNetMobile\u7b49\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u6700\u9ad8\u53ef\u8fbe100%\u7684\u51c6\u786e\u7387\uff0c\u70ed\u6210\u50cf\u6a21\u578b\u5728\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u8868\u73b0\u66f4\u5feb\u3001\u66f4\u7a33\u5065\u3002\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\uff08RTX 4080\uff09\u4e0a\u8fd0\u884c\uff0c\u63a8\u7406\u65f6\u95f4\u4f4e\u81f344\u6beb\u79d2\uff0c\u6a21\u578b\u5927\u5c0f\u4f4e\u4e8e350MB\u3002VGG19\u6a21\u578b\u5728\u70ed\u6210\u50cf\u4e0a\u8bad\u7ec3\u7684\u6548\u679c\u6700\u4f73\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\u5177\u6709\u90e8\u7f72\u6f5c\u529b\uff0cVGG19\u6a21\u578b\u5728\u70ed\u6210\u50cf\u4e0a\u8bad\u7ec3\u6548\u679c\u6700\u4f73\u3002"}}
{"id": "2510.08987", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08987", "abs": "https://arxiv.org/abs/2510.08987", "authors": ["Qixiang Yin", "Huanjin Yao", "Jianghao Chen", "Jiaxing Huang", "Zhicheng Zhao", "Fei Su"], "title": "Tiny-R1V: Lightweight Multimodal Unified Reasoning Model via Model Merging", "comment": "Technical report, Code will be available at\n  https://github.com/buptyqx/Tiny-R1V", "summary": "Although Multimodal Large Language Models (MLLMs) have demonstrated\nremarkable capabilities across diverse tasks, they encounter numerous\nchallenges in terms of reasoning efficiency, such as large model size,\noverthinking, and compromised accuracy in lightweight scenarios. However,\nresearch on the reasoning capabilities of lightweight MLLMs is quite lacking.\nTo this end, we propose Tiny-R1V, a novel lightweight 3B model that achieves\nfaster inference and higher accuracy via a two-stage optimization, while\nunifying multimodal reasoning across multiple tasks and using fewer tokens. In\nthe first stage, Tiny-R1V introduces Length-Informed Relative Policy\nOptimization (LIPO), a novel reinforcement learning method, to train each\nreasoning model. The LIPO is designed to dynamically adjusts advantages of\nresponses within groups, that is, by prioritizing concise yet high-quality\nresponses to encourage the generation of shorter and more accurate response. In\nthe second stage, we propose Adaptive Model Merging (AMM), a training-free\nmodel merging method that merges multiple specialist models into a unified\narchitecture. Specifically, AMM adaptively adjusts the weights of task vectors\nand robustly optimizes the merged vectors via a novel gradient projection\nregularization loss function, thus mitigating redundant conflicts between them.\nExtensive evaluations on ten widely-used reasoning benchmarks covering\nmathematics, structured data (charts, tables, documents), OCR, and general\ncapabilities showcase the superior performance of Tiny-R1V, enabling\nlightweight models to excel in diverse multimodal reasoning tasks.", "AI": {"tldr": "Tiny-R1V: A lightweight 3B model for faster and more accurate multimodal reasoning.", "motivation": "Reasoning efficiency challenges (large model size, overthinking, accuracy compromise) in existing Multimodal Large Language Models (MLLMs), especially in lightweight scenarios.", "method": "Two-stage optimization: 1) Length-Informed Relative Policy Optimization (LIPO) for concise and accurate responses. 2) Adaptive Model Merging (AMM) to merge specialist models into a unified architecture.", "result": "Superior performance of Tiny-R1V on ten reasoning benchmarks (mathematics, structured data, OCR, general capabilities).", "conclusion": "Tiny-R1V enables lightweight models to excel in diverse multimodal reasoning tasks."}}
{"id": "2510.08739", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08739", "abs": "https://arxiv.org/abs/2510.08739", "authors": ["Yikai Zhao", "Jiekai Ma"], "title": "Faithful and Interpretable Explanations for Complex Ensemble Time Series Forecasts using Surrogate Models and Forecastability Analysis", "comment": null, "summary": "Modern time series forecasting increasingly relies on complex ensemble models\ngenerated by AutoML systems like AutoGluon, delivering superior accuracy but\nwith significant costs to transparency and interpretability. This paper\nintroduces a comprehensive, dual-approach framework that addresses both the\nexplainability and forecastability challenges in complex time series ensembles.\nFirst, we develop a surrogate-based explanation methodology that bridges the\naccuracy-interpretability gap by training a LightGBM model to faithfully mimic\nAutoGluon's time series forecasts, enabling stable SHAP-based feature\nattributions. We rigorously validated this approach through feature injection\nexperiments, demonstrating remarkably high faithfulness between extracted SHAP\nvalues and known ground truth effects. Second, we integrated spectral\npredictability analysis to quantify each series' inherent forecastability. By\ncomparing each time series' spectral predictability to its pure noise\nbenchmarks, we established an objective mechanism to gauge confidence in\nforecasts and their explanations. Our empirical evaluation on the M5 dataset\nfound that higher spectral predictability strongly correlates not only with\nimproved forecast accuracy but also with higher fidelity between the surrogate\nand the original forecasting model. These forecastability metrics serve as\neffective filtering mechanisms and confidence scores, enabling users to\ncalibrate their trust in both the forecasts and their explanations. We further\ndemonstrated that per-item normalization is essential for generating meaningful\nSHAP explanations across heterogeneous time series with varying scales. The\nresulting framework delivers interpretable, instance-level explanations for\nstate-of-the-art ensemble forecasts, while equipping users with forecastability\nmetrics that serve as reliability indicators for both predictions and their\nexplanations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u91cd\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u96c6\u6210\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9884\u6d4b\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u8d8a\u6765\u8d8a\u4f9d\u8d56\u4e8e\u50cfAutoGluon\u8fd9\u6837\u7684AutoML\u7cfb\u7edf\u751f\u6210\u7684\u590d\u6742\u96c6\u6210\u6a21\u578b\uff0c\u867d\u7136\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u4f46\u4e5f\u727a\u7272\u4e86\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u8be5\u6846\u67b6\u9996\u5148\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u6a21\u578b\u7684\u89e3\u91ca\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3LightGBM\u6a21\u578b\u6765\u6a21\u4effAutoGluon\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u4ece\u800c\u5b9e\u73b0\u7a33\u5b9a\u7684SHAP\u7279\u5f81\u5f52\u56e0\u3002\u5176\u6b21\uff0c\u96c6\u6210\u4e86\u9891\u8c31\u53ef\u9884\u6d4b\u6027\u5206\u6790\u6765\u91cf\u5316\u6bcf\u4e2a\u5e8f\u5217\u7684\u56fa\u6709\u53ef\u9884\u6d4b\u6027\u3002", "result": "\u5728M5\u6570\u636e\u96c6\u4e0a\u7684\u7ecf\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8f83\u9ad8\u7684\u9891\u8c31\u53ef\u9884\u6d4b\u6027\u4e0d\u4ec5\u4e0e\u63d0\u9ad8\u7684\u9884\u6d4b\u51c6\u786e\u6027\u5bc6\u5207\u76f8\u5173\uff0c\u800c\u4e14\u4e0e\u4ee3\u7406\u6a21\u578b\u548c\u539f\u59cb\u9884\u6d4b\u6a21\u578b\u4e4b\u95f4\u66f4\u9ad8\u7684\u4fdd\u771f\u5ea6\u5bc6\u5207\u76f8\u5173\u3002\u6b64\u5916\uff0c\u9010\u9879\u5f52\u4e00\u5316\u5bf9\u4e8e\u8de8\u5177\u6709\u4e0d\u540c\u5c3a\u5ea6\u7684\u5f02\u6784\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u6709\u610f\u4e49\u7684SHAP\u89e3\u91ca\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6700\u5148\u8fdb\u7684\u96c6\u6210\u9884\u6d4b\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u3001\u5b9e\u4f8b\u7ea7\u522b\u7684\u89e3\u91ca\uff0c\u540c\u65f6\u4e3a\u7528\u6237\u63d0\u4f9b\u53ef\u9884\u6d4b\u6027\u6307\u6807\uff0c\u4f5c\u4e3a\u9884\u6d4b\u53ca\u5176\u89e3\u91ca\u7684\u53ef\u9760\u6027\u6307\u6807\u3002"}}
{"id": "2510.08615", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.08615", "abs": "https://arxiv.org/abs/2510.08615", "authors": ["Kaiqi Yang", "Hang Li", "Yucheng Chu", "Zitao Liu", "Mi Tian", "Hui Liu"], "title": "Iterative LLM-Based Generation and Refinement of Distracting Conditions in Math Word Problems", "comment": null, "summary": "Mathematical reasoning serves as a crucial testbed for evaluating the\nintelligence of large language models (LLMs), and math word problems (MWPs)\nrepresent one of the most widely used formats. Most existing MWP datasets\ncontain only the necessary information, while problems with distracting or\nexcessive conditions are often overlooked. Prior studies have shown that\npopular LLMs experience a dramatic performance drop when such distracting\nconditions are introduced. However, available datasets of MWPs with distracting\nconditions remain limited, and most exhibit low difficulty and out-of-context\nexpressions. These shortcomings make the distracting conditions easy to detect\nand disregard, thereby reducing the credibility of benchmarking on these\ndatasets. Moreover, when distracting conditions are added, the reasoning\nprocess and answers may change, requiring intensive manual effort to check and\nrewrite solutions.\n  To address these issues, we design an iterative framework that leverages LLMs\nto generate distracting conditions automatically. We develop a set of prompts\nto revise MWPs from multiple perspectives and cognitive levels, encouraging the\ncreation of meaningful distracting conditions as well as suggestions for\nfurther refinement. A key advantage of our framework is the preservation of\nshared solutions between the original and revised problems: the LLMs are\nexplicitly guided to generate distractions that do not alter the original\nsolution, thus eliminating the need to produce new answers. This framework is\nefficient and easy to deploy, substantially reducing the effort required to\ngenerate MWPs with distracting conditions while maintaining high data quality.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6570\u5b66\u5e94\u7528\u9898\uff08MWP\uff09\u6570\u636e\u96c6\u4e2d\u7f3a\u4e4f\u5e72\u6270\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u751f\u6210\u5e72\u6270\u4fe1\u606f\u7684\u8fed\u4ee3\u6846\u67b6\u3002", "motivation": "\u73b0\u6709MWP\u6570\u636e\u96c6\u5927\u591a\u53ea\u5305\u542b\u5fc5\u8981\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u5e72\u6270\u6761\u4ef6\u3002\u73b0\u6709LLM\u5728\u5f15\u5165\u5e72\u6270\u6761\u4ef6\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4f46\u73b0\u6709\u7684\u5e26\u5e72\u6270\u6761\u4ef6\u7684MWP\u6570\u636e\u96c6\u6709\u9650\uff0c\u4e14\u96be\u5ea6\u8f83\u4f4e\uff0c\u7f3a\u4e4f\u5b9e\u9645\u610f\u4e49\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8fed\u4ee3\u6846\u67b6\uff0c\u5229\u7528LLM\u81ea\u52a8\u751f\u6210\u5e72\u6270\u6761\u4ef6\u3002\u901a\u8fc7\u4e00\u7cfb\u5217\u63d0\u793a\uff0c\u4ece\u591a\u4e2a\u89d2\u5ea6\u548c\u8ba4\u77e5\u5c42\u9762\u4fee\u6539MWP\uff0c\u751f\u6210\u6709\u610f\u4e49\u7684\u5e72\u6270\u6761\u4ef6\uff0c\u5e76\u63d0\u51fa\u8fdb\u4e00\u6b65\u6539\u8fdb\u7684\u5efa\u8bae\u3002\u8be5\u6846\u67b6\u7684\u5173\u952e\u4f18\u52bf\u5728\u4e8e\u4fdd\u7559\u4e86\u539f\u59cb\u95ee\u9898\u548c\u4fee\u6539\u540e\u95ee\u9898\u7684\u5171\u4eab\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u4e14\u6613\u4e8e\u90e8\u7f72\uff0c\u5927\u5927\u51cf\u5c11\u4e86\u751f\u6210\u5e26\u6709\u5e72\u6270\u6761\u4ef6\u7684MWP\u6240\u9700\u7684\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u6570\u636e\u8d28\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u81ea\u52a8\u751f\u6210\u6570\u5b66\u5e94\u7528\u9898\u5e72\u6270\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u4e3a\u8bc4\u4f30LLM\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u66f4\u6709\u529b\u7684\u5de5\u5177\u3002"}}
{"id": "2510.08771", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08771", "abs": "https://arxiv.org/abs/2510.08771", "authors": ["Xiaohui Li", "Shaobin Zhuang", "Shuo Cao", "Yang Yang", "Yuandong Pu", "Qi Qin", "Siqi Luo", "Bin Fu", "Yihao Liu"], "title": "LinearSR: Unlocking Linear Attention for Stable and Efficient Image Super-Resolution", "comment": "19 pages, 9 figures, 6 tables", "summary": "Generative models for Image Super-Resolution (SR) are increasingly powerful,\nyet their reliance on self-attention's quadratic complexity (O(N^2)) creates a\nmajor computational bottleneck. Linear Attention offers an O(N) solution, but\nits promise for photorealistic SR has remained largely untapped, historically\nhindered by a cascade of interrelated and previously unsolved challenges. This\npaper introduces LinearSR, a holistic framework that, for the first time,\nsystematically overcomes these critical hurdles. Specifically, we resolve a\nfundamental, training instability that causes catastrophic model divergence\nusing our novel \"knee point\"-based Early-Stopping Guided Fine-tuning (ESGF)\nstrategy. Furthermore, we mitigate the classic perception-distortion trade-off\nwith a dedicated SNR-based Mixture of Experts (MoE) architecture. Finally, we\nestablish an effective and lightweight guidance paradigm, TAG, derived from our\n\"precision-over-volume\" principle. Our resulting LinearSR model simultaneously\ndelivers state-of-the-art perceptual quality with exceptional efficiency. Its\ncore diffusion forward pass (1-NFE) achieves SOTA-level speed, while its\noverall multi-step inference time remains highly competitive. This work\nprovides the first robust methodology for applying Linear Attention in the\nphotorealistic SR domain, establishing a foundational paradigm for future\nresearch in efficient generative super-resolution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a LinearSR \u7684\u6574\u4f53\u6846\u67b6\uff0c\u9996\u6b21\u7cfb\u7edf\u5730\u514b\u670d\u4e86\u7ebf\u6027\u6ce8\u610f\u529b\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387 (SR) \u4e2d\u7684\u5173\u952e\u969c\u788d\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u611f\u77e5\u8d28\u91cf\u548c\u5353\u8d8a\u7684\u6548\u7387\u3002", "motivation": "\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6 (O(N^2)) \u9020\u6210\u4e86\u8ba1\u7b97\u74f6\u9888\u3002\u7ebf\u6027\u6ce8\u610f\u529b\u63d0\u4f9b\u4e86\u4e00\u79cd O(N) \u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u5728\u7167\u7247\u7ea7\u771f\u5b9e\u611f SR \u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u5f00\u53d1\u3002", "method": "1. \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u201c\u819d\u70b9\u201d\u7684\u65e9\u505c\u5f15\u5bfc\u5fae\u8c03 (ESGF) \u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u6839\u672c\u6027\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u30022. \u91c7\u7528\u57fa\u4e8e SNR \u7684\u4e13\u5bb6\u6df7\u5408 (MoE) \u67b6\u6784\uff0c\u7f13\u89e3\u4e86\u7ecf\u5178\u7684\u611f\u77e5-\u5931\u771f\u6743\u8861\u30023. \u5efa\u7acb\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8f7b\u91cf\u7ea7\u6307\u5bfc\u8303\u5f0f TAG\uff0c\u6e90\u81ea\u201c\u7cbe\u5ea6\u9ad8\u4e8e\u6570\u91cf\u201d\u539f\u5219\u3002", "result": "LinearSR \u6a21\u578b\u540c\u65f6\u63d0\u4f9b\u4e86\u6700\u5148\u8fdb\u7684\u611f\u77e5\u8d28\u91cf\u548c\u5353\u8d8a\u7684\u6548\u7387\u3002\u5176\u6838\u5fc3\u6269\u6563\u524d\u5411\u4f20\u9012 (1-NFE) \u5b9e\u73b0\u4e86 SOTA \u7ea7\u522b\u7684\u901f\u5ea6\uff0c\u800c\u5176\u6574\u4f53\u591a\u6b65\u63a8\u7406\u65f6\u95f4\u4ecd\u7136\u5177\u6709\u5f88\u5f3a\u7684\u7ade\u4e89\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5728\u7ebf\u6027\u6ce8\u610f\u529b\u5728\u7167\u7247\u7ea7\u771f\u5b9e\u611f SR \u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7b2c\u4e00\u4e2a\u7a33\u5065\u7684\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u9ad8\u6548\u751f\u6210\u8d85\u5206\u8fa8\u7387\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
