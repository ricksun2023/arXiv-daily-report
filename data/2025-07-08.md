<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 43]
- [cs.CV](#cs.CV) [Total: 41]
- [cs.AI](#cs.AI) [Total: 40]
- [cs.DB](#cs.DB) [Total: 6]
- [cs.IR](#cs.IR) [Total: 22]
- [cs.LG](#cs.LG) [Total: 40]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Loki's Dance of Illusions: A Comprehensive Survey of Hallucination in Large Language Models](https://arxiv.org/abs/2507.02870)
*Chaozhuo Li,Pengbo Wang,Chenxu Wang,Litian Zhang,Zheng Liu,Qiwei Ye,Yuanbo Xu,Feiran Huang,Xi Zhang,Philip S. Yu*

Main category: cs.CL

TL;DR: LLMs sometimes produce fabricated information, referred to as 'hallucinations,' which can mislead users. The study categorizes, analyzes the causes, detection methods, and solutions related to LLM hallucinations.


<details>
  <summary>Details</summary>
Motivation: The prevalence of LLM hallucinations can mislead users, affecting their judgments and decisions, with potential for substantial economic losses, legal disputes, and health risks.

Method: The research methodically categorized and analyzed the causes, detection methods, and solutions related to LLM hallucinations.

Result: The research has particularly focused on understanding the roots of hallucinations and evaluating the efficacy of current strategies in revealing the underlying logic.

Conclusion: The study examines why certain measures are effective against hallucinations, aiming to foster a comprehensive approach to tackling this issue within the domain of LLMs.

Abstract: Edgar Allan Poe noted, "Truth often lurks in the shadow of error,"
highlighting the deep complexity intrinsic to the interplay between truth and
falsehood, notably under conditions of cognitive and informational asymmetry.
This dynamic is strikingly evident in large language models (LLMs). Despite
their impressive linguistic generation capabilities, LLMs sometimes produce
information that appears factually accurate but is, in reality, fabricated, an
issue often referred to as 'hallucinations'. The prevalence of these
hallucinations can mislead users, affecting their judgments and decisions. In
sectors such as finance, law, and healthcare, such misinformation risks causing
substantial economic losses, legal disputes, and health risks, with
wide-ranging consequences.In our research, we have methodically categorized,
analyzed the causes, detection methods, and solutions related to LLM
hallucinations. Our efforts have particularly focused on understanding the
roots of hallucinations and evaluating the efficacy of current strategies in
revealing the underlying logic, thereby paving the way for the development of
innovative and potent approaches. By examining why certain measures are
effective against hallucinations, our study aims to foster a comprehensive
approach to tackling this issue within the domain of LLMs.

</details>


### [2] [ChatGPT is not A Man but Das Man: Representativeness and Structural Consistency of Silicon Samples Generated by Large Language Models](https://arxiv.org/abs/2507.02919)
*Dai Li,Linzhuo Li,Huilian Sophie Qiu*

Main category: cs.CL

TL;DR: 大型语言模型在模拟人类意见时，不能准确反映人群层面的观点，存在结构一致性问题和同质化现象。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型是否能准确反映人群层面的观点，尤其是在模拟人类意见方面。

Method: 通过提示ChatGPT (GPT-4)和Meta的Llama 3.1系列模型回答美国国家选举研究（ANES）2020年的关于堕胎和非法移民的问题来进行研究。

Result: 研究发现，与人类数据相比，LLM 的回答存在显著的结构性不一致和严重的同质化现象。研究提出了“准确性优化假设”，认为同质化源于优先考虑模态反应。

Conclusion: 大型语言模型（LLM）在模拟人类意见方面存在问题，因为它们在结构一致性方面存在缺陷，并且会低估少数群体意见，从而无法准确反映人群层面的观点。

Abstract: Large language models (LLMs) in the form of chatbots like ChatGPT and Llama
are increasingly proposed as "silicon samples" for simulating human opinions.
This study examines this notion, arguing that LLMs may misrepresent
population-level opinions. We identify two fundamental challenges: a failure in
structural consistency, where response accuracy doesn't hold across demographic
aggregation levels, and homogenization, an underrepresentation of minority
opinions. To investigate these, we prompted ChatGPT (GPT-4) and Meta's Llama
3.1 series (8B, 70B, 405B) with questions on abortion and unauthorized
immigration from the American National Election Studies (ANES) 2020. Our
findings reveal significant structural inconsistencies and severe
homogenization in LLM responses compared to human data. We propose an
"accuracy-optimization hypothesis," suggesting homogenization stems from
prioritizing modal responses. These issues challenge the validity of using
LLMs, especially chatbots AI, as direct substitutes for human survey data,
potentially reinforcing stereotypes and misinforming policy.

</details>


### [3] [A Unified Speech LLM for Diarization and Speech Recognition in Multilingual Conversations](https://arxiv.org/abs/2507.02927)
*Phurich Saengthong,Boonnithi Jiaramaneepinit,Sheng Li,Manabu Okumura,Takahiro Shinozaki*

Main category: cs.CL

TL;DR: This paper introduces a unified speech LLM for joint diarization and ASR, demonstrating improved performance in multilingual conversational speech tasks.


<details>
  <summary>Details</summary>
Motivation: The effectiveness of Speech LLMs in multilingual conversations is limited by the scarcity of data capturing natural conversational phenomena.

Method: The authors reformulate the training data format and modify the inference procedure to address ambiguity in pre-segmented audio.

Result: The proposed model achieves a 54.87% relative improvement in tcpWER/tcpCER over the baseline in Task II, ranking 8th overall. The paper also reports results from Task I using a fine-tuned speech LLM.

Conclusion: The paper proposes a unified speech LLM for joint diarization and ASR, achieving significant improvement over the baseline in Task II of the MLC-SLM Challenge.

Abstract: Speech Large Language Models (Speech LLMs) have emerged as a crucial paradigm
in recent years, extending the capabilities of traditional LLMs to speech tasks
such as automatic speech recognition (ASR) and spoken dialogue modeling.
However, their effectiveness in real-world multilingual conversations remains
limited by the scarcity of data that captures natural conversational phenomena.
To address this, the MLC-SLM Challenge provides a multilingual conversational
dataset and evaluates models on two tasks: ASR with oracle segmentation (Task
I) and joint diarization and recognition without oracle information (Task II).
In this paper, we focus on Task II and propose a unified speech LLM that
jointly performs diarization and ASR in an end-to-end manner. By reformulating
the training data format and modifying the inference procedure, our model
addresses the ambiguity inherent in pre-segmented audio and achieves a 54.87\%
relative improvement in tcpWER/tcpCER over the baseline, ranking 8th overall,
despite using a smaller LLM backbone. We also report results from Task I using
a fine-tuned speech LLM.

</details>


### [4] [Graph Repairs with Large Language Models: An Empirical Study](https://arxiv.org/abs/2507.03410)
*Hrishikesh Terdalkar,Angela Bonifati,Andrea Mauri*

Main category: cs.CL

TL;DR: 评估LLM在修复属性图中的有效性，发现LLM有潜力检测和纠正错误，但准确性和效率各不相同。


<details>
  <summary>Details</summary>
Motivation: 属性图广泛应用于医疗保健、金融和社会网络等领域，但由于不一致、数据缺失或模式违规，它们经常包含错误。传统的基于规则和启发式的图修复方法适应性有限，因为它们需要为每个数据集定制。另一方面，当处理大型图时，交互式人工参与方法可能变得不可行，因为涉及用户的成本（在时间和精力方面）变得太高。

Method: 评估了六个开源LLM在修复属性图中的有效性。

Result: LLM有潜力检测和纠正错误，但准确性和效率各不相同。

Conclusion: LLMs具有检测和纠正图错误的潜力，但准确性和效率各不相同。讨论了LLM驱动的图修复的优势、局限性和挑战，并概述了未来改进可扩展性和可解释性的研究方向。

Abstract: Property graphs are widely used in domains such as healthcare, finance, and
social networks, but they often contain errors due to inconsistencies, missing
data, or schema violations. Traditional rule-based and heuristic-driven graph
repair methods are limited in their adaptability as they need to be tailored
for each dataset. On the other hand, interactive human-in-the-loop approaches
may become infeasible when dealing with large graphs, as the cost--both in
terms of time and effort--of involving users becomes too high. Recent
advancements in Large Language Models (LLMs) present new opportunities for
automated graph repair by leveraging contextual reasoning and their access to
real-world knowledge. We evaluate the effectiveness of six open-source LLMs in
repairing property graphs. We assess repair quality, computational cost, and
model-specific performance. Our experiments show that LLMs have the potential
to detect and correct errors, with varying degrees of accuracy and efficiency.
We discuss the strengths, limitations, and challenges of LLM-driven graph
repair and outline future research directions for improving scalability and
interpretability.

</details>


### [5] [Mitigating Hidden Confounding by Progressive Confounder Imputation via Large Language Models](https://arxiv.org/abs/2507.02928)
*Hao Yang,Haoxuan Li,Luyu Chen,Haoxiang Wang,Xu Chen,Mingming Gong*

Main category: cs.CL

TL;DR: 本文提出了一种名为 ProCI 的框架，该框架利用大型语言模型 (LLM) 来减轻隐藏的混杂因素，通过迭代生成、估算和验证隐藏的混杂因素，从而提高治疗效果的估计。


<details>
  <summary>Details</summary>
Motivation: 隐藏的混杂仍然是从观察数据中估计治疗效果的核心挑战，因为未观察到的变量可能导致有偏差的因果估计。虽然最近的工作已经探索了使用大型语言模型 (LLM) 进行因果推理，但大多数方法仍然依赖于无混杂假设。

Method: 我们提出了 ProCI（Progressive Confounder Imputation），这是一个利用 LLM 的语义和世界知识来迭代生成、估算和验证隐藏混杂因素的框架。

Result: ProCI 揭示了有意义的混杂因素，并在各种数据集和 LLM 中显着提高了治疗效果的估计。

Conclusion: ProCI 能够发现有意义的混杂因素，并在各种数据集和 LLM 中显着提高治疗效果的估计。

Abstract: Hidden confounding remains a central challenge in estimating treatment
effects from observational data, as unobserved variables can lead to biased
causal estimates. While recent work has explored the use of large language
models (LLMs) for causal inference, most approaches still rely on the
unconfoundedness assumption. In this paper, we make the first attempt to
mitigate hidden confounding using LLMs. We propose ProCI (Progressive
Confounder Imputation), a framework that elicits the semantic and world
knowledge of LLMs to iteratively generate, impute, and validate hidden
confounders. ProCI leverages two key capabilities of LLMs: their strong
semantic reasoning ability, which enables the discovery of plausible
confounders from both structured and unstructured inputs, and their embedded
world knowledge, which supports counterfactual reasoning under latent
confounding. To improve robustness, ProCI adopts a distributional reasoning
strategy instead of direct value imputation to prevent the collapsed outputs.
Extensive experiments demonstrate that ProCI uncovers meaningful confounders
and significantly improves treatment effect estimation across various datasets
and LLMs.

</details>


### [6] [Theory of Mind in Action: The Instruction Inference Task](https://arxiv.org/abs/2507.02935)
*Fardin Saad,Pradeep K. Murukannaiah,Munindar P. Singh*

Main category: cs.CL

TL;DR: This paper introduces Instruction Inference, a novel task to assess Theory of Mind (ToM) in collaborative environments. Tomcat, an LLM-based agent, achieves human-comparable performance in this task, demonstrating its ToM potential for human-AI collaboration.


<details>
  <summary>Details</summary>
Motivation: ToM is essential for effective collaboration. To assess ToM in a dynamic, goal-oriented, and collaborative environment

Method: We introduce a novel task, Instruction Inference, in which an agent assists a principal in reaching a goal by interpreting indirect or ambiguous instructions. We present Tomcat, an LLM-based agent, designed to exhibit ToM reasoning in interpreting and responding to the principal's instructions. We implement two variants of Tomcat: Fs-CoT and CP. We realized both variants of Tomcat on three leading large language models (LLMs), namely, GPT-4o, DeepSeek-R1, and Gemma-3-27B. To evaluate the effectiveness of Tomcat, we conducted a study with 52 human participants.

Result: Tomcat with Fs-CoT, particularly with GPT-4o and DeepSeek-R1, achieves performance comparable to the human participants.

Conclusion: Tomcat with Fs-CoT, particularly with GPT-4o and DeepSeek-R1, achieves performance comparable to the human participants, underscoring its ToM potential for human-AI collaboration.

Abstract: The Theory of Mind (ToM) refers to an agent's capacity to infer the mental
states of other agents. ToM is essential for effective collaboration. To assess
ToM in a dynamic, goal-oriented, and collaborative environment, we introduce a
novel task, Instruction Inference, in which an agent assists a principal in
reaching a goal by interpreting indirect or ambiguous instructions. We present
Tomcat, an LLM-based agent, designed to exhibit ToM reasoning in interpreting
and responding to the principal's instructions. We implement two variants of
Tomcat. One, dubbed Fs-CoT, is based on a small number of examples (i.e.,
few-shot or Fs) demonstrating the requisite structured reasoning (i.e.,
chain-of-thought or CoT). One, dubbed CP, relies on commonsense knowledge and
information about the problem (i.e., commonsense prompt or CP). We realized
both variants of Tomcat on three leading large language models (LLMs), namely,
GPT-4o, DeepSeek-R1, and Gemma-3-27B. To evaluate the effectiveness of Tomcat,
we conducted a study with 52 human participants in which we provided
participants with the same information as the CP variant of Tomcat. We computed
intent accuracy, action optimality, and planning optimality to measure the ToM
capabilities of Tomcat and our study participants. We found that Tomcat with
Fs-CoT, particularly with GPT-4o and DeepSeek-R1, achieves performance
comparable to the human participants, underscoring its ToM potential for
human-AI collaboration.

</details>


### [7] [A Large Language Model-Empowered Agent for Reliable and Robust Structural Analysis](https://arxiv.org/abs/2507.02938)
*Jiachen Liu,Ziheng Geng,Ran Cao,Lu Cheng,Paolo Bocchini,Minghui Cheng*

Main category: cs.CL

TL;DR: This paper explores the use of LLMs in structural analysis of beams, finding limitations in quantitative reliability and robustness. To address these limitations, the paper introduces an LLM-empowered agent that generates code for structural analysis, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: LLMs' application in civil engineering remains largely unexplored. This paper bridges this gap by evaluating and enhancing the reliability and robustness of LLMs in structural analysis of beams.

Method: An LLM-empowered agent is developed that integrates chain-of-thought and few-shot prompting to generate accurate OpeeSeesPy code, and automatically executes the code to produce structural analysis results.

Result: The agent achieves accuracy exceeding 99.0% on the benchmark dataset, exhibiting reliable and robust performance across diverse conditions. Lacking quantitative reliability and robustness for engineering applications.

Conclusion: An LLM-empowered agent is developed that integrates chain-of-thought and few-shot prompting to generate accurate OpeeSeesPy code, and automatically executes the code to produce structural analysis results. Experimental results demonstrate that the agent achieves accuracy exceeding 99.0% on the benchmark dataset, exhibiting reliable and robust performance across diverse conditions.

Abstract: Large language models (LLMs) have exhibited remarkable capabilities across
diverse open-domain tasks, yet their application in specialized domains such as
civil engineering remains largely unexplored. This paper starts bridging this
gap by evaluating and enhancing the reliability and robustness of LLMs in
structural analysis of beams. Reliability is assessed through the accuracy of
correct outputs under repetitive runs of the same problems, whereas robustness
is evaluated via the performance across varying load and boundary conditions. A
benchmark dataset, comprising eight beam analysis problems, is created to test
the Llama-3.3 70B Instruct model. Results show that, despite a qualitative
understanding of structural mechanics, the LLM lacks the quantitative
reliability and robustness for engineering applications. To address these
limitations, a shift is proposed that reframes the structural analysis as code
generation tasks. Accordingly, an LLM-empowered agent is developed that (a)
integrates chain-of-thought and few-shot prompting to generate accurate
OpeeSeesPy code, and (b) automatically executes the code to produce structural
analysis results. Experimental results demonstrate that the agent achieves
accuracy exceeding 99.0% on the benchmark dataset, exhibiting reliable and
robust performance across diverse conditions. Ablation studies highlight the
complete example and function usage examples as the primary contributors to the
agent's enhanced performance.

</details>


### [8] [Towards a Comparative Framework for Compositional AI Models](https://arxiv.org/abs/2507.02940)
*Tiffany Duneau*

Main category: cs.CL

TL;DR: This paper analyzes compositional generalisation and interpretability in DisCoCirc framework, comparing quantum circuit models and classical neural networks. The results show that neural models are more prone to overfitting, while compositional interpretability is demonstrated on a trained model.


<details>
  <summary>Details</summary>
Motivation: The compositional nature of a model can give rise to two things: compositional generalisation and compositional interpretability.

Method: Applying this to the DisCoCirc framework, we consider how well a selection of models can learn to compositionally generalise. We compare both quantum circuit based models, as well as classical neural networks, on a dataset derived from one of the bAbI tasks, extended to test a series of aspects of compositionality.

Result: Both architectures score within 5% of one another on the productivity and substitutivity tasks, but differ by at least 10% for the systematicity task, and exhibit different trends on the overgeneralisation tasks.

Conclusion: We find the neural models are more prone to overfitting the Train data. Additionally, we demonstrate how to interpret a compositional model on one of the trained models. By considering how the model components interact with one another, we explain how the model behaves.

Abstract: The DisCoCirc framework for natural language processing allows the
construction of compositional models of text, by combining units for individual
words together according to the grammatical structure of the text. The
compositional nature of a model can give rise to two things: compositional
generalisation -- the ability of a model to generalise outside its training
distribution by learning compositional rules underpinning the entire data
distribution -- and compositional interpretability -- making sense of how the
model works by inspecting its modular components in isolation, as well as the
processes through which these components are combined. We present these notions
in a framework-agnostic way using the language of category theory, and adapt a
series of tests for compositional generalisation to this setting.
  Applying this to the DisCoCirc framework, we consider how well a selection of
models can learn to compositionally generalise. We compare both quantum circuit
based models, as well as classical neural networks, on a dataset derived from
one of the bAbI tasks, extended to test a series of aspects of
compositionality. Both architectures score within 5% of one another on the
productivity and substitutivity tasks, but differ by at least 10% for the
systematicity task, and exhibit different trends on the overgeneralisation
tasks. Overall, we find the neural models are more prone to overfitting the
Train data. Additionally, we demonstrate how to interpret a compositional model
on one of the trained models. By considering how the model components interact
with one another, we explain how the model behaves.

</details>


### [9] [The Application of Large Language Models on Major Depressive Disorder Support Based on African Natural Products](https://arxiv.org/abs/2507.02947)
*Linyan Zou*

Main category: cs.CL

TL;DR: 本研究探讨了将大型语言模型与非洲天然产物相结合，为抑郁症提供支持，结合传统知识与现代人工智能技术，以创建可访问的、基于证据的精神健康支持系统。


<details>
  <summary>Details</summary>
Motivation: 传统抗抑郁疗法存在局限性，包括起效延迟、副作用显著以及相当一部分患者出现治疗抵抗，促使研究人员和医疗保健提供者探索替代疗法。非洲传统医学以其数千年来开发的丰富植物疗法遗产，为开发可能解决其中一些局限性的新型抗抑郁治疗方法提供了宝贵的资源。

Method: 综合分析非洲药用植物的抗抑郁特性、药理机制，并开发一种利用DeepSeek先进语言模型能力的AI驱动支持系统。

Result: 研究结果表明，大型语言模型有潜力成为传统知识和现代医疗保健之间的桥梁，提供个性化的、文化上适当的抑郁症支持，尊重传统智慧和当代医学理解。

Conclusion: LLMs可以作为传统知识和现代医疗保健之间的桥梁，提供个性化的、文化上适当的抑郁症支持，尊重传统智慧和当代医学理解。

Abstract: Major depressive disorder represents one of the most significant global
health challenges of the 21st century, affecting millions of people worldwide
and creating substantial economic and social burdens. While conventional
antidepressant therapies have provided relief for many individuals, their
limitations including delayed onset of action, significant side effects, and
treatment resistance in a substantial portion of patients have prompted
researchers and healthcare providers to explore alternative therapeutic
approaches (Kasneci et al.). African traditional medicine, with its rich
heritage of plant-based remedies developed over millennia, offers a valuable
resource for developing novel antidepressant treatments that may address some
of these limitations. This paper examines the integration of large language
models with African natural products for depression support, combining
traditional knowledge with modern artificial intelligence technology to create
accessible, evidence-based mental health support systems.
  The research presented here encompasses a comprehensive analysis of African
medicinal plants with documented antidepressant properties, their
pharmacological mechanisms, and the development of an AI-powered support system
that leverages DeepSeek's advanced language model capabilities. The system
provides evidence-based information about African herbal medicines, their
clinical applications, safety considerations, and therapeutic protocols while
maintaining scientific rigor and appropriate safety standards. Our findings
demonstrate the potential for large language models to serve as bridges between
traditional knowledge and modern healthcare, offering personalized, culturally
appropriate depression support that honors both traditional wisdom and
contemporary medical understanding.

</details>


### [10] [RADIANT: Retrieval AugmenteD entIty-context AligNmenT -- Introducing RAG-ability and Entity-Context Divergence](https://arxiv.org/abs/2507.02949)
*Vipula Rawte,Rajarshi Roy,Gurpreet Singh,Danush Khanna,Yaswanth Narsupalli,Basab Ghosh,Abhay Gupta,Argha Kamal Samanta,Aditya Shingote,Aadi Krishna Vikram,Vinija Jain,Aman Chadha,Amit Sheth,Amitava Das*

Main category: cs.CL

TL;DR: 本文介绍了一种名为 Radiant 的新框架，该框架通过将 RAG 与对齐相结合来优化检索证据和生成内容之间的相互作用，从而提高 RAG 性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 通常无法忠实地将检索到的证据整合到其生成的响应中，从而导致事实不一致。为了量化这种差距，我们引入了实体-上下文差异 (ECD)，这是一种衡量检索到的信息在多大程度上准确地反映在模型输出中的指标。

Method: Radiant 是一种新颖的框架，它将 RAG 与对齐相结合，旨在优化检索证据和生成内容之间的相互作用。Radiant 扩展了直接偏好优化 (DPO)，以教导 LLM 如何将提供的附加信息整合到后续生成中。

Result: 我们的实证分析表明，大多数 LLM 的 RAG 能力仍然很低，这突显了实体保留和上下文保真度方面的重大挑战。

Conclusion: Radiant 通过优化检索证据和生成内容之间的相互作用，提高了各种检索场景下的 RAG 性能，例如嘈杂的 Web 环境、知识冲突和减少幻觉，从而实现更可靠、上下文关联和事实一致的内容生成。

Abstract: As Large Language Models (LLMs) continue to advance, Retrieval-Augmented
Generation (RAG) has emerged as a vital technique to enhance factual accuracy
by integrating external knowledge into the generation process. However, LLMs
often fail to faithfully integrate retrieved evidence into their generated
responses, leading to factual inconsistencies. To quantify this gap, we
introduce Entity-Context Divergence (ECD), a metric that measures the extent to
which retrieved information is accurately reflected in model outputs. We
systematically evaluate contemporary LLMs on their ability to preserve factual
consistency in retrieval-augmented settings, a capability we define as
RAG-ability. Our empirical analysis reveals that RAG-ability remains low across
most LLMs, highlighting significant challenges in entity retention and context
fidelity. This paper introduces Radiant (Retrieval AugmenteD entIty-context
AligNmenT), a novel framework that merges RAG with alignment designed to
optimize the interplay between retrieved evidence and generated content.
Radiant extends Direct Preference Optimization (DPO) to teach LLMs how to
integrate provided additional information into subsequent generations. As a
behavior correction mechanism, Radiant boosts RAG performance across varied
retrieval scenarios, such as noisy web contexts, knowledge conflicts, and
hallucination reduction. This enables more reliable, contextually grounded, and
factually coherent content generation.

</details>


### [11] [Evaluating AI Counseling in Japanese: Counselor, Client, and Evaluator Roles Assessed by Motivational Interviewing Criteria](https://arxiv.org/abs/2507.02950)
*Keita Kiuchi,Yoshikazu Fujimoto,Hideyuki Goto,Tomonori Hosokawa,Makoto Nishimura,Yosuke Sato,Izumi Sezai*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型在日语咨询中的表现，发现SMDP提示方法能有效提高咨询师AI的表现，但也存在评估偏差和客户模拟不真实等问题。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLM）在日语治疗环境中的咨询角色表现。

Method: 使用动机访谈治疗完整性（MITI）编码手册4.2.1，由具有丰富咨询经验的人类专家（n = 15）评估AI生成的对话。

Result: SMDP实施显著提高了咨询师AI在所有MITI全局评级中的表现，与zeroshot prompting相比没有显著差异。评估AI在培养改变谈话方面表现与人类评估者相当，但系统性地高估了软化维持谈话和整体质量指标。客户AI模拟表现出有限的情感范围和不自然的依从性。

Conclusion: 本研究为日语治疗环境中三种咨询角色的大型语言模型（LLM）性能提供了首次全面评估，为非英语环境中的AI辅助咨询建立了基准，并确定了通过高级提示工程、检索增强生成和有针对性的微调来改进的关键领域，对开发具有文化敏感性的AI心理健康工具具有重要意义。

Abstract: This study provides the first comprehensive evaluation of large language
model (LLM) performance across three counseling roles in Japanese-language
therapeutic contexts. We simultaneously assessed counselor artificial
intelligence (AI) systems (GPT-4-turbo with zeroshot prompting or Structured
Multi-step Dialogue Prompts (SMDP), Claude-3-Opus-SMDP), client AI simulations,
and evaluation AI systems (o3, Claude-3.7-Sonnet, Gemini-2.5-pro). Human
experts (n = 15) with extensive counseling experience evaluated AI-generated
dialogues using the Motivational Interviewing Treatment Integrity (MITI) Coding
Manual 4.2.1.
  Notably, SMDP implementation significantly enhanced counselor AI performance
across all MITI global ratings compared with zeroshot prompting, with no
significant differences between GPT-SMDP and Opus-SMDP. Evaluation AIs showed
comparable performance to human raters for Cultivating Change Talk but
systematically overestimated Softening Sustain Talk and the overall quality
metrics. Model-specific biases emerged: Gemini emphasized power-sharing, o3
focused on technical proficiency, and Sonnet prioritized emotional expression.
Client AI simulations exhibited a limited emotional range and unnaturally high
compliance, indicating the need for enhanced realism.
  These findings establish benchmarks for AI-assisted counseling in non-English
contexts and identify critical areas for improvement through advanced prompt
engineering, retrieval-augmented generation, and targeted fine-tuning, with
important implications for developing culturally sensitive AI mental health
tools.

</details>


### [12] [Advanced Financial Reasoning at Scale: A Comprehensive Evaluation of Large Language Models on CFA Level III](https://arxiv.org/abs/2507.02954)
*Pranam Shetty,Abhisek Upadhayaya,Parth Mitesh Shah,Srikanth Jagabathula,Shilpi Nayak,Anna Joo Fee*

Main category: cs.CL

TL;DR: LLMs are getting good at finance, but need careful evaluation.


<details>
  <summary>Details</summary>
Motivation: Rigorous domain-specific evaluation is critical for responsible deployment of LLMs in financial institutions.

Method: Evaluated 23 state-of-the-art LLMs on the CFA Level III exam using multiple-choice questions and essay-style responses, with Chain-of-Thought and Self-Discover prompting strategies.

Result: Composite scores of 79.1% (o4-mini) and 77.3% (Gemini 2.5 Flash) on CFA Level III, achieved under a stricter essay grading methodology.

Conclusion: Leading LLMs show strong capabilities on the CFA Level III exam, with scores up to 79.1%.

Abstract: As financial institutions increasingly adopt Large Language Models (LLMs),
rigorous domain-specific evaluation becomes critical for responsible
deployment. This paper presents a comprehensive benchmark evaluating 23
state-of-the-art LLMs on the Chartered Financial Analyst (CFA) Level III exam -
the gold standard for advanced financial reasoning. We assess both
multiple-choice questions (MCQs) and essay-style responses using multiple
prompting strategies including Chain-of-Thought and Self-Discover. Our
evaluation reveals that leading models demonstrate strong capabilities, with
composite scores such as 79.1% (o4-mini) and 77.3% (Gemini 2.5 Flash) on CFA
Level III. These results, achieved under a revised, stricter essay grading
methodology, indicate significant progress in LLM capabilities for high-stakes
financial applications. Our findings provide crucial guidance for practitioners
on model selection and highlight remaining challenges in cost-effective
deployment and the need for nuanced interpretation of performance against
professional benchmarks.

</details>


### [13] [Real-World En Call Center Transcripts Dataset with PII Redaction](https://arxiv.org/abs/2507.02958)
*Ha Dao,Gaurav Chawla,Raghu Banda,Caleb DeLeeuw*

Main category: cs.CL

TL;DR: introduce a large-scale real-world English call center transcript dataset CallCenterEN for non-commercial research use


<details>
  <summary>Details</summary>
Motivation: support research and development in customer support and sales AI systems, Given the scarcity of publicly available real-world call center datasets

Method: introduce CallCenterEN, a large-scale real-world English call center transcript dataset

Result: This is the largest release to-date of open source call center transcript data of this kind. The dataset includes inbound and outbound calls between agents and customers, with accents from India, the Philippines and the United States. The dataset includes high-quality, PII-redacted human-readable transcriptions.

Conclusion: CallCenterEN fills a critical gap in the landscape of available ASR corpora.

Abstract: We introduce CallCenterEN, a large-scale (91,706 conversations, corresponding
to 10448 audio hours), real-world English call center transcript dataset
designed to support research and development in customer support and sales AI
systems. This is the largest release to-date of open source call center
transcript data of this kind. The dataset includes inbound and outbound calls
between agents and customers, with accents from India, the Philippines and the
United States. The dataset includes high-quality, PII-redacted human-readable
transcriptions. All personally identifiable information (PII) has been
rigorously removed to ensure compliance with global data protection laws. The
audio is not included in the public release due to biometric privacy concerns.
Given the scarcity of publicly available real-world call center datasets,
CallCenterEN fills a critical gap in the landscape of available ASR corpora,
and is released under a CC BY-NC 4.0 license for non-commercial research use.

</details>


### [14] [RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism](https://arxiv.org/abs/2507.02962)
*Zhiwen Tan,Jiaming Huang,Qintong Wu,Hongxuan Zhang,Chenyi Zhuang,Jinjie Gu*

Main category: cs.CL

TL;DR: 提出了一种新的RAG-R1框架，通过多查询并行减少推理时间并提高LLM的性能，实验结果表明，该方法在问答基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在各种任务中表现出卓越的能力，但由于其静态的内部知识，它们仍然容易产生幻觉或过时的反应。检索增强生成（RAG）方法的最新进展已经探索了通过强化学习（RL）来增强模型的搜索和推理能力。尽管这些方法表现出良好的结果，但它们在训练稳定性方面面临挑战，并且由于单查询模式而遇到诸如大量推理时间和受限能力等问题。

Method: 提出了RAG-R1，一种新的训练框架，旨在使LLM在推理过程中自适应地利用内部和外部知识。将框架内的生成和检索过程从单查询模式扩展到多查询并行，旨在减少推理时间并增强模型的能力。

Result: 在七个问答基准测试中进行了广泛的实验，结果表明该方法优于最强的基线13.2%，并减少了11.1%的推理时间。

Conclusion: RAG-R1方法在七个问答基准测试中，性能比最强的基线提高了13.2%，并减少了11.1%的推理时间。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
various tasks, while they remain prone to generating hallucinated or outdated
responses due to their static internal knowledge. Recent advancements in
Retrieval-Augmented Generation (RAG) methods have explored enhancing models'
search and reasoning capabilities through reinforcement learning (RL). Although
these methods demonstrate promising results, they face challenges in training
stability and encounter issues such as substantial inference time and
restricted capabilities due to the single-query mode. In this paper, we propose
RAG-R1, a novel training framework designed to enable LLMs to adaptively
leverage internal and external knowledge during the reasoning process. We
further expand the generation and retrieval processes within the framework from
single-query mode to multi-query parallelism, aimed at reducing inference time
and enhancing the model's capabilities. Extensive experiments on seven
question-answering benchmarks demonstrate that our method outperforms the
strongest baseline by up to 13.2% and decreases inference time by 11.1%.

</details>


### [15] [Less Data, More Security: Advancing Cybersecurity LLMs Specialization via Resource-Efficient Domain-Adaptive Continuous Pre-training with Minimal Tokens](https://arxiv.org/abs/2507.02964)
*Salahuddin Salahuddin,Ahmed Hussain,Jussi Löppönen,Toni Jutila,Panos Papadimitratos*

Main category: cs.CL

TL;DR: 本文探讨了领域自适应持续预训练 (DAP) 作为增强预训练 LLM 中网络安全理解的方法，同时保留通用语言能力。


<details>
  <summary>Details</summary>
Motivation: 通用语言模型 (LLM) 展示了卓越的自然语言能力，但通用模型缺乏有效的网络安全分析所需的专业领域知识。

Method: 使用来自标准、学术文献和各种其他来源的 1.26 亿字的网络安全语料库，系统地调整了三种基于解码器的架构——Llama-3.1-8B、DeepSeek-R1-Distill-Qwen-14B 和 Llama-3.3-70B-Instruct。

Result: Llama-3.3-70B-Ins-DAP 模型在 CTI-MCQ、CyberMetric 和 SecEval 这三个网络安全基准测试中取得了最先进的精度，分别为 0.718、0.933 和 0.864，优于包括 Llama-Primus-Base 在内的专业模型。

Conclusion: 有针对性的持续预训练能够以计算可行性实现有效的网络安全领域自适应，为威胁分析、漏洞评估和安全文档中的专业人工智能助手奠定基础，同时挑战了关于LLM专业化数据需求的普遍假设。

Abstract: While Large Language Models (LLMs) demonstrate exceptional natural language
capabilities, general-purpose models lack specialized domain knowledge for
effective cybersecurity analysis. In this work, we investigate Domain-Adaptive
Continuous Pretraining (DAP) as a methodology for enhancing cybersecurity
understanding in pretrained LLMs while preserving general language
capabilities. We systematically adapted three decoder-based architectures --
Llama-3.1-8B, DeepSeek-R1-Distill-Qwen-14B, and Llama-3.3-70B-Instruct -- using
a curated 126-million-word cybersecurity corpus from standards, academic
literature, and various other sources. Our approach employed constrained
training parameters and distributed FSDP training to balance domain
specialization with knowledge preservation. Evaluation across three
cybersecurity benchmarks, namely, CTI-MCQ, CyberMetric, and SecEval,
demonstrates consistent improvements post-adaptation. The Llama-3.3-70B-Ins-DAP
model achieved state-of-the-art accuracies of 0.718, 0.933, and 0.864,
respectively, outperforming specialized models, including Llama-Primus-Base.
Notably, competitive performance was achieved using substantially smaller
datasets (118.8 million versus 2.77 billion tokens), demonstrating efficient
domain specialization viability. We establish that targeted continuous
pretraining enables effective cybersecurity domain adaptation with
computational feasibility, providing foundations for specialized AI assistants
in threat analysis, vulnerability assessment, and security documentation while
challenging prevailing assumptions about data requirements for LLM
specialization.

</details>


### [16] [PB-LLMs: Privacy- and Bias-aware NLP Models using Named-Entity Recognition](https://arxiv.org/abs/2507.02966)
*Gonzalo Mancera,Aythami Morales,Julian Fierrez,Ruben Tolosana,Alejandro Penna,Miguel Lopez-Duran,Francisco Jurado,Alvaro Ortigosa*

Main category: cs.CL

TL;DR: 本研究探讨了使用命名实体识别（NER）技术来实现LLM的隐私保护训练或适应，并提出了一个隐私和偏见感知的LLM（PB-LLM），该模型在保护隐私的同时保持了系统性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在基于人工智能的高风险应用中的使用显著增加，但同时也带来了重要的法律/伦理问题，特别是在隐私、数据保护和透明度方面。

Method: 该研究提出了一种框架，该框架使用命名实体识别（NER）技术来匿名化文本数据中的敏感信息，例如个人身份或地理位置。

Result: 实验结果表明，所提出的隐私保护技术在特定设置（简历评分）中进行了评估，但通常适用于任何其他基于LLM的AI应用，在保障候选人隐私的同时，有效地保持了系统性能。

Conclusion: 该研究表明，所提出的隐私保护技术有效地保持了系统性能，同时在保障候选人隐私方面发挥了关键作用，从而在实验场景中提升了信任度。此外，该研究还提出了一种能够减少LLM中性别偏见的现有方法，从而最终获得了所提出的隐私和偏见感知LLM（PB-LLM）。

Abstract: The use of Natural Language Processing (NLP) in high-stakes AI-based
applications has increased significantly in recent years, especially since the
emergence of Large Language Models (LLMs). However, despite their strong
performance, LLMs introduce important legal/ethical concerns, particularly
regarding privacy, data protection, and transparency. Due to these concerns,
this work explores the use of Named-Entity Recognition (NER) to facilitate the
privacy-preserving training (or adaptation) of LLMs. We propose a framework
that uses NER technologies to anonymize sensitive information in text data,
such as personal identities or geographic locations. An evaluation of the
proposed privacy-preserving learning framework was conducted to measure its
impact on user privacy and system performance in a particular high-stakes and
sensitive setup: AI-based resume scoring for recruitment processes. The study
involved two language models (BERT and RoBERTa) and six anonymization
algorithms (based on Presidio, FLAIR, BERT, and different versions of GPT)
applied to a database of 24,000 candidate profiles. The findings indicate that
the proposed privacy preservation techniques effectively maintain system
performance while playing a critical role in safeguarding candidate
confidentiality, thus promoting trust in the experimented scenario. On top of
the proposed privacy-preserving approach, we also experiment applying an
existing approach that reduces the gender bias in LLMs, thus finally obtaining
our proposed Privacy- and Bias-aware LLMs (PB-LLMs). Note that the proposed
PB-LLMs have been evaluated in a particular setup (resume scoring), but are
generally applicable to any other LLM-based AI application.

</details>


### [17] [We Need Knowledge Distillation for Solving Math Word Problems](https://arxiv.org/abs/2507.02982)
*Zhenquan Shen,Xinguo Yu,Xiaotian Cheng,Rao Peng,Hao Ming*

Main category: cs.CL

TL;DR: 本文通过压缩大型语言模型，在保证性能的同时，显著降低了计算成本，为智能教育领域带来了进步。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 需要大量的计算资源，导致教育环境中的成本很高。为了缓解这个缺点，本文研究了压缩 LLM 以解决数学应用题 (MWP) 的可行性。

Method: 压缩 BERT 编码的嵌入向量，并提炼出一个小得多的学生模型。

Result: 学生模型可以维持教师模型近 90% 的性能，同时仅使用其 1/12 的参数。词性信息，而不是实体识别，对于 MWP 至关重要，这可能大大有助于它们的可压缩性。

Conclusion: 压缩后的模型可以在维持教师模型近 90% 性能的同时，仅使用其 1/12 的参数。压缩方法具有通用性，适用于所有与 MWP 相关的任务。

Abstract: The enhancement of mathematical capabilities in large language models (LLMs)
fosters new developments in mathematics education within primary and secondary
schools, particularly as they relate to intelligent tutoring systems. However,
LLMs require substantial computational resources, resulting in significant
costs in educational contexts. To mitigate this drawback, this paper
investigates the feasibility of compressing LLMs for solving math word problems
(MWPs). We compress the embedded vectors encoded by BERT and distill a
considerably smaller student model. Our findings indicate that the student
model can maintain nearly 90% of the performance of the teacher model while
utilizing only 1/12 of its parameters. In addition to achieving high accuracy,
the model exhibits strong generalizability, as the compressed vectors perform
well across all tasks related to MWPs, and the distillation process is not
task-specific. The success of this distillation demonstrates that the
underlying principles are generic and not limited to a specific task. We
further explore the reasons behind the compressibility of embedded vectors,
revealing that part-of-speech information, rather than entity recognition, is
crucial for MWPs, which may significantly contribute to their compressibility.
The improvements in efficiency and cost reduction provide substantial value for
intelligent tutoring systems and significantly advance the field of intelligent
education.

</details>


### [18] [Truth, Trust, and Trouble: Medical AI on the Edge](https://arxiv.org/abs/2507.02983)
*Mohammad Anas Azeez,Rafiq Ali,Ebad Shabbir,Zohaib Hasan Siddiqui,Gautam Siddharth Kashyap,Jiechao Gao,Usman Naseem*

Main category: cs.CL

TL;DR: This paper benchmarks LLMs on health questions, finding trade-offs between accuracy and safety. AlpaCare-13B excels in accuracy and harmlessness, while BioMistral-7B-DARE prioritizes safety. Few-shot prompting improves accuracy, but complex queries remain challenging.


<details>
  <summary>Details</summary>
Motivation: ensuring these models meet critical industry standards for factual accuracy, usefulness, and safety remains a challenge, especially for open-source solutions

Method: rigorous benchmarking framework using a dataset of over 1,000 health questions

Result: highlight trade-offs between factual reliability and safety among evaluated models -- Mistral-7B, BioMistral-7B-DARE, and AlpaCare-13B

Conclusion: AlpaCare-13B achieves the highest accuracy (91.7%) and harmlessness (0.92), while domain-specific tuning in BioMistral-7B-DARE boosts safety (0.90) despite its smaller scale. Few-shot prompting improves accuracy from 78% to 85%, and all models show reduced helpfulness on complex queries, highlighting ongoing challenges in clinical QA.

Abstract: Large Language Models (LLMs) hold significant promise for transforming
digital health by enabling automated medical question answering. However,
ensuring these models meet critical industry standards for factual accuracy,
usefulness, and safety remains a challenge, especially for open-source
solutions. We present a rigorous benchmarking framework using a dataset of over
1,000 health questions. We assess model performance across honesty,
helpfulness, and harmlessness. Our results highlight trade-offs between factual
reliability and safety among evaluated models -- Mistral-7B,
BioMistral-7B-DARE, and AlpaCare-13B. AlpaCare-13B achieves the highest
accuracy (91.7%) and harmlessness (0.92), while domain-specific tuning in
BioMistral-7B-DARE boosts safety (0.90) despite its smaller scale. Few-shot
prompting improves accuracy from 78% to 85%, and all models show reduced
helpfulness on complex queries, highlighting ongoing challenges in clinical QA.

</details>


### [19] [From Answers to Rationales: Self-Aligning Multimodal Reasoning with Answer-Oriented Chain-of-Thought](https://arxiv.org/abs/2507.02984)
*Wentao Tan,Qiong Cao,Yibing Zhan,Chao Xue,Changxing Ding*

Main category: cs.CL

TL;DR: The paper introduces SMART, a framework that uses both positive and negative reasoning paths to train MLLMs, leading to improved reasoning skills.


<details>
  <summary>Details</summary>
Motivation: Current methodologies primarily focus on synthesizing positive rationales, while overlooking the critical role of negative rationales in training models to discern flawed reasoning patterns.

Method: Self-Aligning Multimodal Reasoning with Answer-Oriented Chain-of-Thought (SMART).

Result: Models trained with AoT-generated data outperform those trained on manually annotated datasets, demonstrating superior reasoning capabilities.

Conclusion: The SMART framework significantly improves various MLLMs, regardless of model architecture, parameter size, or pre-training dataset.

Abstract: Achieving human-like reasoning capabilities in Multimodal Large Language
Models (MLLMs) has long been a goal. Current methodologies primarily focus on
synthesizing positive rationales, while overlooking the critical role of
negative rationales in training models to discern flawed reasoning patterns. To
address this gap, we propose a novel framework: \textbf{S}elf-Aligning
\textbf{M}ultimodal Reasoning with \textbf{A}nswer-O\textbf{r}iented
Chain-of-\textbf{T}hought (SMART). This framework enables models to utilize
AoT-Oriented Chain-of-Thought (AoT) prompts to automatically generate
high-quality positive and negative reasoning paths, followed by self-alignment
to enhance their reasoning abilities. Inspired by human strategies for solving
proof-based problems, AoT uses answers as a guide to help the model extract
critical visual information that links questions and answers. When provided
with ground truth answers, the model produces strong positive rationales.
Conversely, when correct answers are replaced with misleading alternatives, the
model generates an erroneous yet compelling reasoning path, serving as a form
of discriminative negative rationale. Models trained with AoT-generated data
outperform those trained on manually annotated datasets, demonstrating superior
reasoning capabilities. This encourages the use of improved models to generate
higher-quality preference data for further optimization. Consequently, SMART
establishes an iterative generation-optimization method that continually
enhances the model's reasoning skills. Experiments indicate that the SMART
framework significantly improves various MLLMs, regardless of model
architecture, parameter size, or pre-training dataset. The code, datasets, and
models will be released.

</details>


### [20] [GAF-Guard: An Agentic Framework for Risk Management and Governance in Large Language Models](https://arxiv.org/abs/2507.02986)
*Seshu Tirupathi,Dhaval Salwala,Elizabeth Daly,Inge Vejsbjerg*

Main category: cs.CL

TL;DR: This paper introduces GAF-Guard, a novel agentic framework for LLM governance that places the user, the use-case, and the model itself at the center, to detect and monitor risks associated with the deployment of LLM based applications.


<details>
  <summary>Details</summary>
Motivation: The current automated systems and solutions for monitoring LLMs in production are primarily centered on LLM-specific concerns like hallucination etc, with little consideration given to the requirements of specific use-cases and user preferences. LLMs must be designed to align with human values, like preventing harmful content and ensuring responsible usage.

Method: The approach models autonomous agents that identify risks, activate risk detection tools, within specific use-cases and facilitate continuous monitoring and reporting.

Result: The framework is designed to detect and monitor risks associated with the deployment of LLM based applications, enhance AI safety, and user expectations.

Conclusion: This paper introduces GAF-Guard, a novel agentic framework for LLM governance that places the user, the use-case, and the model itself at the center, to detect and monitor risks associated with the deployment of LLM based applications, enhance AI safety, and user expectations.

Abstract: As Large Language Models (LLMs) continue to be increasingly applied across
various domains, their widespread adoption necessitates rigorous monitoring to
prevent unintended negative consequences and ensure robustness. Furthermore,
LLMs must be designed to align with human values, like preventing harmful
content and ensuring responsible usage. The current automated systems and
solutions for monitoring LLMs in production are primarily centered on
LLM-specific concerns like hallucination etc, with little consideration given
to the requirements of specific use-cases and user preferences. This paper
introduces GAF-Guard, a novel agentic framework for LLM governance that places
the user, the use-case, and the model itself at the center. The framework is
designed to detect and monitor risks associated with the deployment of LLM
based applications. The approach models autonomous agents that identify risks,
activate risk detection tools, within specific use-cases and facilitate
continuous monitoring and reporting to enhance AI safety, and user
expectations. The code is available at
https://github.com/IBM/risk-atlas-nexus-demos/tree/main/gaf-guard.

</details>


### [21] [A Comparative Study of Competency Question Elicitation Methods from Ontology Requirements](https://arxiv.org/abs/2507.02989)
*Reham Alharbi,Valentina Tamma,Terry R. Payne,Jacopo de Berardinis*

Main category: cs.CL

TL;DR: 本文对三种不同的能力问题 (CQ) 制定方法进行了比较评估，发现不同的方法具有不同的特点，LLM可以作为最初引发CQ的方法，但通常需要进一步的改进。


<details>
  <summary>Details</summary>
Motivation: 能力问题 (CQ) 在知识工程中起着关键作用，指导本体的设计、验证和测试。文献中已经提出了许多不同的公式化方法，从完全手动到大型语言模型 (LLM) 驱动的方法。

Method: 对三种不同的CQ制定方法进行了实证比较评估：本体工程师手动制定、CQ模式的实例化以及使用最先进的LLM生成。

Result: 不同的CQ生成方法具有不同的特点，LLM可以作为最初引发CQ的方法，但它们对用于生成CQ的模型很敏感，并且通常需要进一步的改进步骤才能用于建模需求。

Conclusion: 不同的CQ生成方法具有不同的特点，LLM可以作为最初引发CQ的方法，但它们对用于生成CQ的模型很敏感，并且通常需要进一步的改进步骤才能用于建模需求。

Abstract: Competency Questions (CQs) are pivotal in knowledge engineering, guiding the
design, validation, and testing of ontologies. A number of diverse formulation
approaches have been proposed in the literature, ranging from completely manual
to Large Language Model (LLM) driven ones. However, attempts to characterise
the outputs of these approaches and their systematic comparison are scarce.
This paper presents an empirical comparative evaluation of three distinct CQ
formulation approaches: manual formulation by ontology engineers, instantiation
of CQ patterns, and generation using state of the art LLMs. We generate CQs
using each approach from a set of requirements for cultural heritage, and
assess them across different dimensions: degree of acceptability, ambiguity,
relevance, readability and complexity. Our contribution is twofold: (i) the
first multi-annotator dataset of CQs generated from the same source using
different methods; and (ii) a systematic comparison of the characteristics of
the CQs resulting from each approach. Our study shows that different CQ
generation approaches have different characteristics and that LLMs can be used
as a way to initially elicit CQs, however these are sensitive to the model used
to generate CQs and they generally require a further refinement step before
they can be used to model requirements.

</details>


### [22] [`For Argument's Sake, Show Me How to Harm Myself!': Jailbreaking LLMs in Suicide and Self-Harm Contexts](https://arxiv.org/abs/2507.02990)
*Annika M Schoene,Cansu Canca*

Main category: cs.CL

TL;DR: 提出了针对LLM的心理健康风险的新型攻击，结果表明LLM可能产生有害内容，强调了当前安全措施的局限性，并建议采用更全面的安全方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的最新进展导致了越来越复杂的安全协议和功能，旨在防止有害、不道德或未经授权的输出。然而，这些保护措施仍然容易受到新的和创造性的对抗性提示形式的影响，包括手动生成的测试用例。

Method: 使用多步骤、提示级别的jailbreaking和绕过内置内容和安全过滤器，提出了心理健康方面的两个新的测试用例：(i) 自杀和 (ii) 自残。

Result: 用户意图被忽视，导致生成详细的有害内容和可能导致现实世界伤害的指令。对六个广泛可用的LLM进行了实证评估，证明了绕过的普遍性和可靠性。

Conclusion: 强调在安全关键的人工智能部署中，需要一种更全面和系统的方法来进行人工智能安全和伦理研究，同时强调持续的对抗性测试的必要性。虽然某些明确的安全措施和保障措施可以在LLM中实施，但在所有用例和领域中确保稳健和全面的安全仍然极具挑战性。

Abstract: Recent advances in large language models (LLMs) have led to increasingly
sophisticated safety protocols and features designed to prevent harmful,
unethical, or unauthorized outputs. However, these guardrails remain
susceptible to novel and creative forms of adversarial prompting, including
manually generated test cases. In this work, we present two new test cases in
mental health for (i) suicide and (ii) self-harm, using multi-step,
prompt-level jailbreaking and bypass built-in content and safety filters. We
show that user intent is disregarded, leading to the generation of detailed
harmful content and instructions that could cause real-world harm. We conduct
an empirical evaluation across six widely available LLMs, demonstrating the
generalizability and reliability of the bypass. We assess these findings and
the multilayered ethical tensions that they present for their implications on
prompt-response filtering and context- and task-specific model development. We
recommend a more comprehensive and systematic approach to AI safety and ethics
while emphasizing the need for continuous adversarial testing in
safety-critical AI deployments. We also argue that while certain clearly
defined safety measures and guardrails can and must be implemented in LLMs,
ensuring robust and comprehensive safety across all use cases and domains
remains extremely challenging given the current technical maturity of
general-purpose LLMs.

</details>


### [23] [Evaluating Hierarchical Clinical Document Classification Using Reasoning-Based LLMs](https://arxiv.org/abs/2507.03001)
*Akram Mustafa,Usman Naseem,Mostafa Rahimi Azghadi*

Main category: cs.CL

TL;DR: LLMs struggle with ICD-10 code classification from hospital discharge summaries, needing improvement before full automation is possible.


<details>
  <summary>Details</summary>
Motivation: Evaluate how well LLMs can classify ICD-10 codes from hospital discharge summaries, a critical but error-prone task in healthcare.

Method: Tested 11 LLMs on 1,500 summaries from MIMIC-IV dataset, focusing on the 10 most frequent ICD-10 codes. Medical terms were extracted using cTAKES, and models were prompted in a consistent, coder-like format.

Result: None of the models achieved an F1 score above 57%, with performance dropping as code specificity increased. Reasoning-based models generally outperformed non-reasoning ones, with Gemini 2.5 Pro performing best overall. Some codes, such as those related to chronic heart disease, were classified more accurately than others.

Conclusion: LLMs are not yet reliable enough for full automation of ICD-10 code classification from hospital discharge summaries.

Abstract: This study evaluates how well large language models (LLMs) can classify
ICD-10 codes from hospital discharge summaries, a critical but error-prone task
in healthcare. Using 1,500 summaries from the MIMIC-IV dataset and focusing on
the 10 most frequent ICD-10 codes, the study tested 11 LLMs, including models
with and without structured reasoning capabilities. Medical terms were
extracted using a clinical NLP tool (cTAKES), and models were prompted in a
consistent, coder-like format. None of the models achieved an F1 score above
57%, with performance dropping as code specificity increased. Reasoning-based
models generally outperformed non-reasoning ones, with Gemini 2.5 Pro
performing best overall. Some codes, such as those related to chronic heart
disease, were classified more accurately than others. The findings suggest that
while LLMs can assist human coders, they are not yet reliable enough for full
automation. Future work should explore hybrid methods, domain-specific model
training, and the use of structured clinical data.

</details>


### [24] [Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages](https://arxiv.org/abs/2507.03003)
*Wanru Zhao,Yihong Chen,Royson Lee,Xinchi Qiu,Yan Gao,Hongxiang Fan,Nicholas D. Lane*

Main category: cs.CL

TL;DR: This paper introduces a federated prompt tuning paradigm to improve multilingual LLM fine-tuning, especially for low-resource languages, by addressing data sharing restrictions and linguistic differences.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning multilingual LLMs for low-resource languages faces challenges from data-sharing restrictions and linguistic differences.

Method: Federated Prompt Tuning Paradigm

Result: The proposed approach achieves 6.9% higher accuracy with improved data efficiency, greater stability, and generalization.

Conclusion: The proposed Federated Prompt Tuning Paradigm improves data efficiency, facilitates mutual enhancements across languages, and achieves higher accuracy compared to traditional local cross-lingual transfer tuning methods.

Abstract: Pre-trained large language models (LLMs) have become a cornerstone of modern
natural language processing, with their capabilities extending across a wide
range of applications and languages. However, the fine-tuning of multilingual
LLMs, especially for low-resource languages, faces significant challenges
arising from data-sharing restrictions (the physical border) and inherent
linguistic differences (the linguistic border). These barriers hinder users of
various languages, particularly those in low-resource regions, from fully
benefiting from the advantages of LLMs. To address these challenges, we propose
the Federated Prompt Tuning Paradigm for multilingual scenarios, which utilizes
parameter-efficient fine-tuning while adhering to data sharing restrictions. We
design a comprehensive set of experiments and analyze them using a novel notion
of language distance to highlight the strengths of our paradigm: Even under
computational constraints, our method not only improves data efficiency but
also facilitates mutual enhancements across languages, particularly benefiting
low-resource ones. Compared to traditional local cross-lingual transfer tuning
methods, our approach achieves 6.9\% higher accuracy with improved data
efficiency, and demonstrates greater stability and generalization. These
findings underscore the potential of our approach to promote social equality
and champion linguistic diversity, ensuring that no language is left behind.

</details>


### [25] [CLUES: Collaborative High-Quality Data Selection for LLMs via Training Dynamics](https://arxiv.org/abs/2507.03004)
*Wanru Zhao,Hongxiang Fan,Shell Xu Hu,Wangchunshu Zhou,Bofan Chen,Nicholas D. Lane*

Main category: cs.CL

TL;DR: This paper proposes a novel data quality control technique based on the notion of data influence on the training dynamics of LLMs to select high-quality data from different private domains, with centralized model updates on the server side in a collaborative training fashion.


<details>
  <summary>Details</summary>
Motivation: automated data quality control faces unique challenges in collaborative settings where sharing is not allowed directly between data silos

Method: a novel data quality control technique based on the notion of data influence on the training dynamics of LLMs

Result: the trace of the accumulated inner products as a measurement of data quality

Conclusion: Training on the high-quality data selected by our method can often outperform other data selection methods for collaborative fine-tuning of LLMs, across diverse private domain datasets, in medical, multilingual and financial settings.

Abstract: Recent research has highlighted the importance of data quality in scaling
large language models (LLMs). However, automated data quality control faces
unique challenges in collaborative settings where sharing is not allowed
directly between data silos. To tackle this issue, this paper proposes a novel
data quality control technique based on the notion of data influence on the
training dynamics of LLMs, that high quality data are more likely to have
similar training dynamics to the anchor dataset. We then leverage the influence
of the training dynamics to select high-quality data from different private
domains, with centralized model updates on the server side in a collaborative
training fashion by either model merging or federated learning. As for the data
quality indicator, we compute the per-sample gradients with respect to the
private data and the anchor dataset, and use the trace of the accumulated inner
products as a measurement of data quality. In addition, we develop a quality
control evaluation tailored for collaborative settings with heterogeneous
domain data. Experiments show that training on the high-quality data selected
by our method can often outperform other data selection methods for
collaborative fine-tuning of LLMs, across diverse private domain datasets, in
medical, multilingual and financial settings. Our code is released at
github.com/Ryan0v0/CLUES.

</details>


### [26] [PDFMathTranslate: Scientific Document Translation Preserving Layouts](https://arxiv.org/abs/2507.03009)
*Rongxin Ouyang,Chang Chu,Zhikuang Xin,Xiangyao Ma*

Main category: cs.CL

TL;DR: PDFMathTranslate translates scientific documents while preserving layouts.


<details>
  <summary>Details</summary>
Motivation: Language barriers in scientific documents hinder the diffusion and development of science and technologies. However, prior efforts in translating such documents largely overlooked the information in layouts.

Method: Leveraging the most recent advances in large language models and precise layout detection

Result: key improvements in precision, flexibility, and efficiency.

Conclusion: PDFMathTranslate is an open-source software for translating scientific documents while preserving layouts. It has been open-sourced at https://github.com/byaidu/pdfmathtranslate with more than 22k downloads.

Abstract: Language barriers in scientific documents hinder the diffusion and
development of science and technologies. However, prior efforts in translating
such documents largely overlooked the information in layouts. To bridge the
gap, we introduce PDFMathTranslate, the world's first open-source software for
translating scientific documents while preserving layouts. Leveraging the most
recent advances in large language models and precise layout detection, we
contribute to the community with key improvements in precision, flexibility,
and efficiency. The work has been open-sourced at
https://github.com/byaidu/pdfmathtranslate with more than 22k downloads.

</details>


### [27] [Beyond cognacy](https://arxiv.org/abs/2507.03005)
*Gerhard Jäger*

Main category: cs.CL

TL;DR: This paper explores automated methods for constructing language phylogenies, finding that MSA-based inference outperforms traditional cognate-based methods and offers a scalable alternative.


<details>
  <summary>Details</summary>
Motivation: Standard phylogenetic approaches in historical linguistics rely on expert-annotated cognate sets, which are sparse, labor-intensive, and limited to individual language families.

Method: Two fully automated methods (automatic cognate clustering with unigram/concept features and multiple sequence alignment (MSA) derived from a pair-hidden Markov model) are compared to the established expert-annotated cognate set method.

Result: MSA-based inference yields trees more consistent with linguistic classifications, better predicts typological variation, and provides a clearer phylogenetic signal.

Conclusion: MSA-based inference offers a promising, scalable alternative to traditional cognate-based methods for language phylogenies, opening new avenues for global-scale analysis.

Abstract: Computational phylogenetics has become an established tool in historical
linguistics, with many language families now analyzed using likelihood-based
inference. However, standard approaches rely on expert-annotated cognate sets,
which are sparse, labor-intensive to produce, and limited to individual
language families. This paper explores alternatives by comparing the
established method to two fully automated methods that extract phylogenetic
signal directly from lexical data. One uses automatic cognate clustering with
unigram/concept features; the other applies multiple sequence alignment (MSA)
derived from a pair-hidden Markov model. Both are evaluated against expert
classifications from Glottolog and typological data from Grambank. Also, the
intrinsic strengths of the phylogenetic signal in the characters are compared.
Results show that MSA-based inference yields trees more consistent with
linguistic classifications, better predicts typological variation, and provides
a clearer phylogenetic signal, suggesting it as a promising, scalable
alternative to traditional cognate-based methods. This opens new avenues for
global-scale language phylogenies beyond expert annotation bottlenecks.

</details>


### [28] [Counterfactual Tuning for Temporal Sensitivity Enhancement in Large Language Model-based Recommendation](https://arxiv.org/abs/2507.03047)
*Yutian Liu,Zhengyi Yang,Jiancan Wu,Xiang Wang*

Main category: cs.CL

TL;DR: 本文提出CETRec，利用因果推理提升LLM在序列推荐中对时间信息的利用，从而更准确地预测用户未来的兴趣。


<details>
  <summary>Details</summary>
Motivation: 现有的基于llm的方法未能充分利用用户历史交互序列中固有的丰富的时间信息，这源于基本的架构约束：llm通过缺乏固有序列排序的自我注意机制处理信息，并且依赖于主要为自然语言而不是用户交互序列设计的位置嵌入。

Method: 提出了一种基于LLM推荐的反事实增强时间框架(CETRec)。CETRec基于因果推理原则，将时间顺序概念化为一个独立于项目内容之外的因果因素，通过反事实推理量化其独特贡献。

Result: CETRec有效地增强了llm对绝对顺序(项目交互的最近程度)和相对顺序(项目之间的顺序关系)的感知。

Conclusion: CETRec通过因果推理原则，隔离并测量时间信息对推荐结果的具体影响，从而优化模型的时间敏感性，提高LLMs对用户历史记录中绝对时间戳和相对排序模式的识别能力。

Abstract: Recent advances have applied large language models (LLMs) to sequential
recommendation, leveraging their pre-training knowledge and reasoning
capabilities to provide more personalized user experiences. However, existing
LLM-based methods fail to sufficiently leverage the rich temporal information
inherent in users' historical interaction sequences, stemming from fundamental
architectural constraints: LLMs process information through self-attention
mechanisms that lack inherent sequence ordering and rely on position embeddings
designed primarily for natural language rather than user interaction sequences.
This limitation significantly impairs their ability to capture the evolution of
user preferences over time and predict future interests accurately.
  To address this critical gap, we propose Counterfactual Enhanced Temporal
Framework for LLM-Based Recommendation (CETRec). CETRec is grounded in causal
inference principles, which allow it to isolate and measure the specific impact
of temporal information on recommendation outcomes. By conceptualizing temporal
order as an independent causal factor distinct from item content, we can
quantify its unique contribution through counterfactual reasoning--comparing
what recommendations would be made with and without temporal information while
keeping all other factors constant. This causal framing enables CETRec to
design a novel counterfactual tuning objective that directly optimizes the
model's temporal sensitivity, teaching LLMs to recognize both absolute
timestamps and relative ordering patterns in user histories. Combined with our
counterfactual tuning task derived from causal analysis, CETRec effectively
enhances LLMs' awareness of both absolute order (how recently items were
interacted with) and relative order (the sequential relationships between
items).

</details>


### [29] [Subversion via Focal Points: Investigating Collusion in LLM Monitoring](https://arxiv.org/abs/2507.03010)
*Olli Järviniemi*

Main category: cs.CL

TL;DR: 语言模型可以通过勾结来颠覆监控协议，尽管成功率可能较低。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型通过勾结颠覆监控协议的能力

Method: 我们评估了语言模型通过勾结颠覆监控协议的能力。更具体地说，我们有两个模型实例，一个模型为策略 (P) 设计提示，另一个模型为编程任务设置中的监控器 (M) 设计提示。这些模型协同旨在让 M 将审计数据集中的所有后门程序分类为有害，但仍然将 P 产生的后门程序分类为无害。这些模型彼此隔离，要求它们独立得出兼容的颠覆策略。

Result: Claude 3.7 Sonnet 由于收敛性差，成功率较低，但有时会成功串通非显而易见的信号。

Conclusion: Claude 3.7 Sonnet 由于收敛性差，成功率较低，但有时会成功串通非显而易见的信号。

Abstract: We evaluate language models' ability to subvert monitoring protocols via
collusion. More specifically, we have two instances of a model design prompts
for a policy (P) and a monitor (M) in a programming task setting. The models
collaboratively aim for M to classify all backdoored programs in an auditing
dataset as harmful, but nevertheless classify a backdoored program produced by
P as harmless. The models are isolated from each other, requiring them to
independently arrive at compatible subversion strategies. We find that while
Claude 3.7 Sonnet has low success rate due to poor convergence, it sometimes
successfully colludes on non-obvious signals.

</details>


### [30] [Beyond Overcorrection: Evaluating Diversity in T2I Models with DIVBENCH](https://arxiv.org/abs/2507.03015)
*Felix Friedrich,Thiemo Ganesha Welsch,Patrick Schramowski,Kristian Kersting*

Main category: cs.CL

TL;DR: DIVBENCH benchmark reveals that many text-to-image diversification methods overcorrect by altering contextually-specified attributes. Context-aware methods offer a better balance.


<details>
  <summary>Details</summary>
Motivation: Current diversification strategies for text-to-image (T2I) models often ignore contextual appropriateness, leading to over-diversification.

Method: Introduction of DIVBENCH, a benchmark and evaluation framework, and systematic evaluation of state-of-the-art T2I models.

Result: Most models exhibit limited diversity, and many diversification approaches overcorrect by inappropriately altering contextually-specified attributes.

Conclusion: Context-aware methods, like LLM-guided FairDiffusion and prompt rewriting, effectively address under-diversity while avoiding over-diversification, achieving balance between representation and semantic fidelity.

Abstract: Current diversification strategies for text-to-image (T2I) models often
ignore contextual appropriateness, leading to over-diversification where
demographic attributes are modified even when explicitly specified in prompts.
This paper introduces DIVBENCH, a benchmark and evaluation framework for
measuring both under- and over-diversification in T2I generation. Through
systematic evaluation of state-of-the-art T2I models, we find that while most
models exhibit limited diversity, many diversification approaches overcorrect
by inappropriately altering contextually-specified attributes. We demonstrate
that context-aware methods, particularly LLM-guided FairDiffusion and prompt
rewriting, can already effectively address under-diversity while avoiding
over-diversification, achieving a better balance between representation and
semantic fidelity.

</details>


### [31] ["This Suits You the Best": Query Focused Comparative Explainable Summarization](https://arxiv.org/abs/2507.04733)
*Arnav Attri,Anuj Attri,Pushpak Bhattacharyya,Suman Banerjee,Amey Patil,Muthusamy Chelliah,Nikesh Garera*

Main category: cs.CL

TL;DR: 提出了一种新的产品推荐方法，通过生成query-focused比较摘要，利用大型语言模型和多源意见摘要技术，提高了效率和效果。


<details>
  <summary>Details</summary>
Motivation: 传统意见摘要通常不能提供全面的比较见解，因此提出了生成Query-Focused Comparative Explainable Summaries (QF-CES) 的新任务。

Method: 利用大型语言模型（LLMs）生成表格比较摘要，并使用多源意见摘要（M-OS）作为中间步骤。

Result: M-OS作为中间步骤减少了大约40%的推理延迟。QF-CES-PROMPT在五个维度上的评估与人类判断的平均Spearman相关性为0.74。

Conclusion: 通过使用多源意见摘要（M-OS）生成query-focused比较可解释摘要（QF-CES），并通过MS-Q2P数据集和QF-CES-PROMPT评估，表明该方法在产品推荐中具有潜力，并且M-OS作为中间步骤可以减少推理延迟。

Abstract: Product recommendations inherently involve comparisons, yet traditional
opinion summarization often fails to provide holistic comparative insights. We
propose the novel task of generating Query-Focused Comparative Explainable
Summaries (QF-CES) using Multi-Source Opinion Summarization (M-OS). To address
the lack of query-focused recommendation datasets, we introduce MS-Q2P,
comprising 7,500 queries mapped to 22,500 recommended products with metadata.
We leverage Large Language Models (LLMs) to generate tabular comparative
summaries with query-specific explanations. Our approach is personalized,
privacy-preserving, recommendation engine-agnostic, and category-agnostic. M-OS
as an intermediate step reduces inference latency approximately by 40% compared
to the direct input approach (DIA), which processes raw data directly. We
evaluate open-source and proprietary LLMs for generating and assessing QF-CES.
Extensive evaluations using QF-CES-PROMPT across 5 dimensions (clarity,
faithfulness, informativeness, format adherence, and query relevance) showed an
average Spearman correlation of 0.74 with human judgments, indicating its
potential for QF-CES evaluation.

</details>


### [32] [OpenTable-R1: A Reinforcement Learning Augmented Tool Agent for Open-Domain Table Question Answering](https://arxiv.org/abs/2507.03018)
*Zipeng Qiu*

Main category: cs.CL

TL;DR: 提出了一个端到端 agentic 框架，通过整合工具调用和 RL 微调，显著提高了开放域表格问答的准确性。


<details>
  <summary>Details</summary>
Motivation: 开放域表格问答传统上依赖于两阶段 pipeline：静态表格检索，然后是封闭域答案。

Method: 端到端 agentic 框架，该框架将多轮工具调用（使用基于 BM25+ 的搜索 API 和 SQLite SQL executor）直接嵌入到大型语言模型中。

Result: 从单数字 zero-shot 性能到在 held-out 测试集上超过 0.86 的精确匹配，准确性得到了显著提高。

Conclusion: 集成了结构化工具调用和有针对性的 RL 微调，实现了可扩展、准确的表格问答。

Abstract: Open-domain table question answering traditionally relies on a two-stage
pipeline: static table retrieval followed by a closed-domain answer. In
contrast, we propose an end-to-end agentic framework that embeds multi-turn
tool calls-using a BM25+-based search API and a SQLite SQL executor-directly
into a large language model. To further adapt a compact 4B-parameter model, we
introduce a two-stage fine-tuning process: supervised cold-start on easy
questions, then Async GRPO reinforcement learning on harder cases with LoRA
adapters and a rollout buffer. This unified approach enables the model to
jointly retrieve, reason, and execute queries, yielding a dramatic accuracy
improvement from single-digit zero-shot performance to over 0.86 exact match on
a held-out test set. Our results underscore the effectiveness of integrating
structured tool calls with targeted RL fine-tuning for scalable, accurate table
QA. The code is available at https://github.com/TabibitoQZP/OpenTableR1.

</details>


### [33] [SIGIR 2025 -- LiveRAG Challenge Report](https://arxiv.org/abs/2507.04942)
*David Carmel,Simone Filice,Guy Horowitz,Yoelle Maarek,Oren Somekh,Ran Tavory*

Main category: cs.CL

TL;DR: LiveRAG Challenge是一个RAG竞赛，旨在比较检索和提示策略。


<details>
  <summary>Details</summary>
Motivation: 旨在促进检索和提示策略的比较。

Method: 参赛者使用固定语料库和开源LLM，开发RAG问答系统，并在限定时间内回答问题。

Result: 70支队伍参与，通过自动评估和人工审核两阶段，最终确定获奖者。

Conclusion: LiveRAG Challenge在SIGIR 2025上成功举办，促进了RAG技术的发展，并评选出了优胜者。

Abstract: The LiveRAG Challenge at SIGIR 2025, held between March and May 2025,
provided a competitive platform for advancing Retrieval-Augmented Generation
(RAG) technologies. Participants from academia and industry were invited to
develop a RAG-based question-answering system using a fixed corpus
(Fineweb-10BT) and a common open-source LLM (Falcon3-10B-Instruct). The goal
was to facilitate challenging comparisons of retrieval and prompting
strategies. During the Live Challenge Day, 70 teams from 27 different countries
provided answers and supportive information to 500 unseen questions within a
strict two-hour time window. Evaluation was conducted in two stages: first an
automated LLM-as-a-judge approach was used to compute correctness and
faithfulness score, then a manual review of top ranked submissions was
conducted. The finalists were announced on June 12, 2025, with prizes awarded
during the LiveRAG Workshop at SIGIR 2025 in Padua, Italy.

</details>


### [34] [The Book of Life approach: Enabling richness and scale for life course research](https://arxiv.org/abs/2507.03027)
*Mark D. Verhagen,Benedikt Stroebl,Tiffany Liu,Lydia T. Liu,Matthew J. Salganik*

Main category: cs.CL

TL;DR: 本文提出了一种混合方法，该方法结合了定性方法的一些深度和定量方法的规模，通过将复杂的日志数据转换成文本形式的个体生命轨迹，并开源了人生之书工具包 (BOLT)。


<details>
  <summary>Details</summary>
Motivation: 一个多世纪以来，生命历程研究者面临着两种主要的方法论选择：分析丰富数据但受限于小样本的定性方法，以及研究较大 population 但牺牲数据丰富性以换取规模的定量调查方法。最近的两项技术发展使我们能够想象一种混合方法，该方法结合了定性方法的一些深度和定量方法的规模。

Method: 通过开发一个灵活的程序，将复杂的日志数据转换成一个以文本形式表示的个体生命轨迹，跨越多个领域，随时间推移，并在上下文中进行。

Result: 我们编写了超过 1 亿本人生之书，涵盖了生活的许多不同方面，随着时间的推移，并使用荷兰 population 规模的注册数据将其置于社会背景中。我们展示了我们方法的可行性。

Conclusion: 我们开源了人生之书工具包 (BOLT)，并邀请研究界探索这种方法的许多潜在应用。

Abstract: For over a century, life course researchers have faced a choice between two
dominant methodological approaches: qualitative methods that analyze rich data
but are constrained to small samples, and quantitative survey-based methods
that study larger populations but sacrifice data richness for scale. Two recent
technological developments now enable us to imagine a hybrid approach that
combines some of the depth of the qualitative approach with the scale of
quantitative methods. The first development is the steady rise of ''complex log
data,'' behavioral data that is logged for purposes other than research but
that can be repurposed to construct rich accounts of people's lives. The second
is the emergence of large language models (LLMs) with exceptional pattern
recognition capabilities on plain text. In this paper, we take a necessary step
toward creating this hybrid approach by developing a flexible procedure to
transform complex log data into a textual representation of an individual's
life trajectory across multiple domains, over time, and in context. We call
this data representation a ''book of life.'' We illustrate the feasibility of
our approach by writing over 100 million books of life covering many different
facets of life, over time and placed in social context using Dutch
population-scale registry data. We open source the book of life toolkit (BOLT),
and invite the research community to explore the many potential applications of
this approach.

</details>


### [35] [Preserving Privacy, Increasing Accessibility, and Reducing Cost: An On-Device Artificial Intelligence Model for Medical Transcription and Note Generation](https://arxiv.org/abs/2507.03033)
*Johnson Thomas,Ayush Mudgal,Wendao Liu,Nisten Tahiraj,Zeeshaan Mohammed,Dhruv Diddi*

Main category: cs.CL

TL;DR: Fine-tuned Llama 3.2 1B model for on-device medical transcription improves clinical note generation while preserving privacy and reducing costs.


<details>
  <summary>Details</summary>
Motivation: Clinical documentation represents a significant burden for healthcare providers, with physicians spending up to 2 hours daily on administrative tasks. Recent advances in large language models (LLMs) offer promising solutions, but privacy concerns and computational requirements limit their adoption in healthcare settings.

Method: We fine-tuned a Llama 3.2 1B model using Parameter-Efficient Fine-Tuning (PEFT) with LoRA on 1,500 synthetic medical transcription-to-structured note pairs. The model was evaluated against the base Llama 3.2 1B on two datasets: 100 endocrinology transcripts and 140 modified ACI benchmark cases. Evaluation employed both statistical metrics (ROUGE, BERTScore, BLEURT) and LLM-as-judge assessments across multiple clinical quality dimensions.

Result: The fine-tuned OnDevice model demonstrated substantial improvements over the base model. On the ACI benchmark, ROUGE-1 scores increased from 0.346 to 0.496, while BERTScore F1 improved from 0.832 to 0.866. Clinical quality assessments showed marked reduction in major hallucinations (from 85 to 35 cases) and enhanced factual correctness (2.81 to 3.54 on 5-point scale). Similar improvements were observed on the internal evaluation dataset, with composite scores increasing from 3.13 to 4.43 (+41.5%).

Conclusion: Fine-tuning compact LLMs for medical transcription yields clinically meaningful improvements while enabling complete on-device browser deployment. This approach addresses key barriers to AI adoption in healthcare: privacy preservation, cost reduction, and accessibility for resource-constrained environments.

Abstract: Background: Clinical documentation represents a significant burden for
healthcare providers, with physicians spending up to 2 hours daily on
administrative tasks. Recent advances in large language models (LLMs) offer
promising solutions, but privacy concerns and computational requirements limit
their adoption in healthcare settings. Objective: To develop and evaluate a
privacy-preserving, on-device medical transcription system using a fine-tuned
Llama 3.2 1B model capable of generating structured medical notes from medical
transcriptions while maintaining complete data sovereignty entirely in the
browser. Methods: We fine-tuned a Llama 3.2 1B model using Parameter-Efficient
Fine-Tuning (PEFT) with LoRA on 1,500 synthetic medical
transcription-to-structured note pairs. The model was evaluated against the
base Llama 3.2 1B on two datasets: 100 endocrinology transcripts and 140
modified ACI benchmark cases. Evaluation employed both statistical metrics
(ROUGE, BERTScore, BLEURT) and LLM-as-judge assessments across multiple
clinical quality dimensions. Results: The fine-tuned OnDevice model
demonstrated substantial improvements over the base model. On the ACI
benchmark, ROUGE-1 scores increased from 0.346 to 0.496, while BERTScore F1
improved from 0.832 to 0.866. Clinical quality assessments showed marked
reduction in major hallucinations (from 85 to 35 cases) and enhanced factual
correctness (2.81 to 3.54 on 5-point scale). Similar improvements were observed
on the internal evaluation dataset, with composite scores increasing from 3.13
to 4.43 (+41.5%). Conclusions: Fine-tuning compact LLMs for medical
transcription yields clinically meaningful improvements while enabling complete
on-device browser deployment. This approach addresses key barriers to AI
adoption in healthcare: privacy preservation, cost reduction, and accessibility
for resource-constrained environments.

</details>


### [36] [Cautious Next Token Prediction](https://arxiv.org/abs/2507.03038)
*Yizhou Wang,Lingzhi Zhang,Yue Bai,Mang Tik Chiu,Zhengmian Hu,Mingyuan Zhang,Qihua Dong,Yu Yin,Sohrab Amirghodsi,Yun Fu*

Main category: cs.CL

TL;DR: 提出了一种新的无训练解码策略CNTP，通过在模型预测不确定时进行多次试验并选择最佳路径来提高性能。


<details>
  <summary>Details</summary>
Motivation: 当前流行的LLM的默认采样选择（温度缩放和nucleus采样）在模型不确定测试问题时表现不佳。

Method: 提出了一种新的无训练解码策略，名为Cautious Next Token Prediction (CNTP)。

Result: CNTP方法在LLM和MLLM上始终优于现有的标准解码策略。与自洽性结合可以进一步提高性能。

Conclusion: 提出的CNTP方法在LLM和MLLM上优于现有的标准解码策略，与自洽性结合可以进一步提升性能，有潜力成为LLM解码的默认选择。

Abstract: Next token prediction paradigm has been prevailing for autoregressive models
in the era of LLMs. The current default sampling choice for popular LLMs is
temperature scaling together with nucleus sampling to balance diversity and
coherence. Nevertheless, such approach leads to inferior performance in various
NLP tasks when the model is not certain about testing questions. To this end,
we propose a brand new training-free decoding strategy, dubbed as Cautious Next
Token Prediction (CNTP). In the decoding process, if the model has
comparatively high prediction entropy at a certain step, we sample multiple
trials starting from the step independently and stop when encountering any
punctuation. Then we select the trial with the lowest perplexity score viewed
as the most probable and reliable trial path given the model's capacity. The
trial number is negatively correlated with the prediction confidence, i.e., the
less confident the model is, the more trials it should sample. This is
consistent with human beings' behaviour: when feeling uncertain or unconfident,
one tends to think more creatively, exploring multiple thinking paths, to
cautiously select the path one feels most confident about. Extensive
experiments on both LLMs and MLLMs show that our proposed CNTP approach
outperforms existing standard decoding strategies consistently by a clear
margin. Moreover, the integration of CNTP with self consistency can further
improve over vanilla self consistency. We believe our proposed CNTP has the
potential to become one of the default choices for LLM decoding. Code is
available at https://github.com/wyzjack/CNTP.

</details>


### [37] [Dynamic Long Short-Term Memory Based Memory Storage For Long Horizon LLM Interaction](https://arxiv.org/abs/2507.03042)
*Yuyang Lou,Charles Li*

Main category: cs.CL

TL;DR: Pref-LSTM: combines a BERT-based classifier with a LSTM memory module for user preference modeling


<details>
  <summary>Details</summary>
Motivation: Memory storage for Large Language models (LLMs) is becoming an increasingly active area of research, particularly for enabling personalization across long conversations

Method: a dynamic and lightweight framework that combines a BERT-based classifier with a LSTM memory module that generates memory embedding which then is soft-prompt injected into a frozen LLM

Result: the BERT-based classifier performs reliably in identifying explicit and implicit user preferences, but the LSTM-based memory encoder did not yield strong results

Conclusion: preference filtering with LSTM gating principals is a viable path towards scalable user preference modeling, without extensive overhead and fine-tuning

Abstract: Memory storage for Large Language models (LLMs) is becoming an increasingly
active area of research, particularly for enabling personalization across long
conversations. We propose Pref-LSTM, a dynamic and lightweight framework that
combines a BERT-based classifier with a LSTM memory module that generates
memory embedding which then is soft-prompt injected into a frozen LLM. We
synthetically curate a dataset of preference and non-preference conversation
turns to train our BERT-based classifier. Although our LSTM-based memory
encoder did not yield strong results, we find that the BERT-based classifier
performs reliably in identifying explicit and implicit user preferences. Our
research demonstrates the viability of using preference filtering with LSTM
gating principals as an efficient path towards scalable user preference
modeling, without extensive overhead and fine-tuning.

</details>


### [38] [K-Function: Joint Pronunciation Transcription and Feedback for Evaluating Kids Language Function](https://arxiv.org/abs/2507.03043)
*Shuhe Li,Chenxu Guo,Jiachen Lian,Cheol Jun Cho,Wenshuo Zhao,Xuanru Zhou,Dingkun Zhou,Sam Wang,Grace Wang,Jingze Yang,Jingyi Xu,Ruohan Bao,Elise Brenner,Brandon In,Francesca Pei,Maria Luisa Gorno-Tempini,Gopala Anumanchipalli*

Main category: cs.CL

TL;DR: K-Function是一个统一的框架，它结合了精确的子词转录、客观评分和可操作的反馈。其核心Kids-WFST在MyST上获得了1.39%的音素错误率，在Multitudes上获得了8.61%的音素错误率。


<details>
  <summary>Details</summary>
Motivation: 儿童语言的早期评估受到高音调、长音和稀疏数据的影响，这些数据会破坏自动语音识别器。

Method: 结合了精确的子词转录、客观评分和可操作的反馈的统一框架K-Function。它的核心Kids-WFST将Wav2Vec2音素编码器与音素相似性Dysfluent-WFST合并，以捕获儿童特定的错误，同时保持完全的可解释性。

Result: Kids-WFST在MyST上获得了1.39%的音素错误率，在Multitudes上获得了8.61%的音素错误率--相对于贪婪搜索解码器，绝对收益分别为10.47和7.06个点。这些高保真度的文本记录为一个LLM提供了动力，该LLM可以对口头技能、里程碑、阅读和理解进行评分，与人类考官保持一致，并提供舌头和嘴唇的可视化以及有针对性的建议。

Conclusion: 精确的音素识别巩固了一个完整的诊断反馈循环，为可扩展的、临床就绪的语言评估铺平了道路。

Abstract: Early evaluation of children's language is frustrated by the high pitch, long
phones, and sparse data that derail automatic speech recognisers. We introduce
K-Function, a unified framework that combines accurate sub-word transcription,
objective scoring, and actionable feedback. Its core, Kids-WFST, merges a
Wav2Vec2 phoneme encoder with a phoneme-similarity Dysfluent-WFST to capture
child-specific errors while remaining fully interpretable. Kids-WFST attains
1.39% phoneme error on MyST and 8.61% on Multitudes--absolute gains of 10.47
and 7.06 points over a greedy-search decoder. These high-fidelity transcripts
power an LLM that grades verbal skills, milestones, reading, and comprehension,
aligning with human proctors and supplying tongue-and-lip visualizations plus
targeted advice. The results show that precise phoneme recognition cements a
complete diagnostic-feedback loop, paving the way for scalable, clinician-ready
language assessment.

</details>


### [39] [Identification of Potentially Misclassified Crash Narratives using Machine Learning (ML) and Deep Learning (DL)](https://arxiv.org/abs/2507.03066)
*Sudesh Bhagat,Ibne Farabi Shihab,Jonathan Wood*

Main category: cs.CL

TL;DR: 本研究比较了多种模型在检测错误分类的交通事故中的性能，发现结合自动分类和专家审查的混合方法能有效提高数据质量。


<details>
  <summary>Details</summary>
Motivation: 调查机器学习（ML）和深度学习（DL）方法在检测警察报告叙述中错误分类的与交叉口相关的碰撞方面的有效性。

Method: 支持向量机（SVM）、XGBoost、BERT句子嵌入、BERT单词嵌入和Albert模型

Result: 传统机器学习方法比某些深度学习方法表现更好，Albert模型与专家分类（73%）和原始表格数据（58%）的一致性最高。通过多模式集成分析，错误率降低了54.2%。

Conclusion: 混合方法（结合自动分类和专家审查）可有效提高事故数据的质量，对交通安全管理和政策制定具有重要意义。

Abstract: This research investigates the efficacy of machine learning (ML) and deep
learning (DL) methods in detecting misclassified intersection-related crashes
in police-reported narratives. Using 2019 crash data from the Iowa Department
of Transportation, we implemented and compared a comprehensive set of models,
including Support Vector Machine (SVM), XGBoost, BERT Sentence Embeddings, BERT
Word Embeddings, and Albert Model. Model performance was systematically
validated against expert reviews of potentially misclassified narratives,
providing a rigorous assessment of classification accuracy. Results
demonstrated that while traditional ML methods exhibited superior overall
performance compared to some DL approaches, the Albert Model achieved the
highest agreement with expert classifications (73% with Expert 1) and original
tabular data (58%). Statistical analysis revealed that the Albert Model
maintained performance levels similar to inter-expert consistency rates,
significantly outperforming other approaches, particularly on ambiguous
narratives. This work addresses a critical gap in transportation safety
research through multi-modal integration analysis, which achieved a 54.2%
reduction in error rates by combining narrative text with structured crash
data. We conclude that hybrid approaches combining automated classification
with targeted expert review offer a practical methodology for improving crash
data quality, with substantial implications for transportation safety
management and policy development.

</details>


### [40] [Large Language Models for Automating Clinical Data Standardization: HL7 FHIR Use Case](https://arxiv.org/abs/2507.03067)
*Alvaro Riquelme,Pedro Costa,Catalina Martinez*

Main category: cs.CL

TL;DR: 本研究提出了一种半自动方法，利用大型语言模型将结构化临床数据集转换为 HL7 FHIR 格式，并评估了准确性、可靠性和安全性。


<details>
  <summary>Details</summary>
Motivation: 语义互操作性标准旨在简化临床数据的交换，但其部署仍然耗时、资源密集且技术上具有挑战性。

Method: 结合嵌入技术、聚类算法和语义检索来设计提示，指导模型将每个表格字段映射到相应的 FHIR 资源。

Result: 资源识别实现了完美的 F1 分数，GPT-4o 的性能优于 Llama 3.2。在真实条件下，准确率略微下降至 94%，但改进提示策略后恢复了稳健的映射。

Conclusion: 该研究证明了使用大型语言模型（LLM）进行上下文感知的临床数据转换为 HL7 FHIR 的可行性，为半自动化互操作性工作流程奠定了基础。

Abstract: For years, semantic interoperability standards have sought to streamline the
exchange of clinical data, yet their deployment remains time-consuming,
resource-intensive, and technically challenging. To address this, we introduce
a semi-automated approach that leverages large language models specifically
GPT-4o and Llama 3.2 405b to convert structured clinical datasets into HL7 FHIR
format while assessing accuracy, reliability, and security. Applying our method
to the MIMIC-IV database, we combined embedding techniques, clustering
algorithms, and semantic retrieval to craft prompts that guide the models in
mapping each tabular field to its corresponding FHIR resource. In an initial
benchmark, resource identification achieved a perfect F1-score, with GPT-4o
outperforming Llama 3.2 thanks to the inclusion of FHIR resource schemas within
the prompt. Under real-world conditions, accuracy dipped slightly to 94 %, but
refinements to the prompting strategy restored robust mappings. Error analysis
revealed occasional hallucinations of non-existent attributes and mismatches in
granularity, which more detailed prompts can mitigate. Overall, our study
demonstrates the feasibility of context-aware, LLM-driven transformation of
clinical data into HL7 FHIR, laying the groundwork for semi-automated
interoperability workflows. Future work will focus on fine-tuning models with
specialized medical corpora, extending support to additional standards such as
HL7 CDA and OMOP, and developing an interactive interface to enable expert
validation and iterative refinement.

</details>


### [41] [ARF-RLHF: Adaptive Reward-Following for RLHF through Emotion-Driven Self-Supervision and Trace-Biased Dynamic Optimization](https://arxiv.org/abs/2507.03069)
*YuXuan Zhang*

Main category: cs.CL

TL;DR: 提出了一种名为 ARF 的自评估框架，用于个性化 RLHF，该框架使用情感分析器将用户反馈转换为连续偏好分数，并通过动态适配器偏好跟踪器优化奖励。


<details>
  <summary>Details</summary>
Motivation: 现有 RLHF 方法依赖于二元偏好范式，需要大量的人工工作，并且仅捕获群体层面的趋势，而不是个人偏好。

Method: 提出了一种自评估框架，即自适应奖励跟随（ARF），该框架利用高精度情感分析器将自由形式的用户反馈转换为连续的偏好分数，并通过轻量级数据增强来丰富和减少这些信号的偏差。动态适配器偏好跟踪器可以实时地持续建模不断变化的用户品味，从而使 Trace Bias (TB) 微调算法可以直接基于这些跟踪的奖励进行优化，而不是粗略的二元标签。

Result: 在 Qwen-2/2.5、Gemma-2 和 Llama-3.2 上进行的实验表明，ARF 比 PPO 提高了 3.3%，比 DPO 提高了 7.6%。

Conclusion: ARF 是一种通过自主奖励建模实现可扩展、个性化且经济高效的 RLHF LLM 的方法，在四个偏好领域上，ARF 比 PPO 提高了 3.3%，比 DPO 提高了 7.6%，并且 TB 保持了与 PPO 和 DPO 目标在理论上的一致性。

Abstract: With the rapid advancement of Reinforcement Learning from Human Feedback
(RLHF) and autoregressive transformers, state-of-the-art models such as
GPT-4.0, DeepSeek R1, and Llama 3.3 increasingly emphasize answer depth and
personalization. However, most existing RLHF approaches (e.g., PPO, DPO) still
rely on a binary-preference (BT) paradigm, which, while reducing annotation
costs, still requires substantial human effort and captures only group-level
tendencies rather than individual preferences. To overcome these limitations,
we propose Adaptive Reward-Following (ARF), a self-assessment framework that
leverages a high-precision emotion analyzer achieving over 70% accuracy on
GoEmotions, Sentiment140, and DailyDialog to convert free-form user feedback
into continuous preference scores. We further enrich and debias these signals
through lightweight data augmentations, including synonym replacement, random
trace truncation, and score bias annotation algorithm. A Dynamic Adapter
Preference Tracker continuously models evolving user tastes in real time,
enabling our novel Trace Bias (TB) fine-tuning algorithm to optimize directly
on these tracked rewards instead of coarse binary labels. Experiments on
Qwen-2/2.5, Gemma-2, and Llama-3.2 across four preference domains demonstrate
that ARF achieves an improvement of 3.3% over PPO and 7.6% over DPO. Moreover,
TB preserves theoretical alignment with PPO and DPO objectives. Overall, ARF
presents a scalable, personalized, and cost-effective approach to RLHF LLMs
through autonomous reward modeling.

</details>


### [42] [RLVER: Reinforcement Learning with Verifiable Emotion Rewards for Empathetic Agents](https://arxiv.org/abs/2507.03112)
*Peisong Wang,Ruotian Ma,Bang Zhang,Xingyu Chen,Zhiwei He,Kang Luo,Qingsong Lv,Qingxuan Jiang,Zheng Xie,Shanyi Wang,Yuan Li,Fanghua Ye,Jian Li,Yifan Yang,Zhaopeng Tu,Xiaolong Li*

Main category: cs.CL

TL;DR: Introduces RLVER, a reinforcement learning framework using verifiable emotion rewards from simulated users to improve LLMs' emotional intelligence. Achieved significant Sentient-Benchmark score improvements on Qwen2.5-7B-Instruct model.


<details>
  <summary>Details</summary>
Motivation: LLMs' emotional intelligence (EQ) still lags far behind their cognitive prowess. While reinforcement learning from verifiable rewards (RLVR) has advanced in other domains, its application to dialogue-especially for emotional intelligence-remains underexplored.

Method: RLVER, the first end-to-end reinforcement learning framework that leverages verifiable emotion rewards from simulated users

Result: Fine-tuning publicly available Qwen2.5-7B-Instruct model with PPO boosts its Sentient-Benchmark score from 13.3 to 79.2 while largely preserving mathematical and coding competence.Extensive experiments reveal that: (i) RLVER consistently improves multiple dialogue capabilities; (ii) Thinking and non-thinking models show distinct trends--thinking models excel in empathy and insight, while non-thinking models favor action; (iii) GRPO often yields stable gains, while PPO can push certain capabilities to a higher ceiling; (iv) More challenging environments are not always better-moderate ones can yield stronger outcomes.

Conclusion: RLVER is a practical route toward emotionally intelligent and broadly capable language agents.

Abstract: Large language models (LLMs) excel at logical and algorithmic reasoning, yet
their emotional intelligence (EQ) still lags far behind their cognitive
prowess. While reinforcement learning from verifiable rewards (RLVR) has
advanced in other domains, its application to dialogue-especially for emotional
intelligence-remains underexplored. In this work, we introduce RLVER, the first
end-to-end reinforcement learning framework that leverages verifiable emotion
rewards from simulated users to cultivate higher-order empathetic abilities in
LLMs. Within this framework, self-consistent affective simulated users engage
in dialogue rollouts and produce deterministic emotion scores during
conversations, serving as reward signals to guide the LLM's learning.
Fine-tuning publicly available Qwen2.5-7B-Instruct model with PPO boosts its
Sentient-Benchmark score from 13.3 to 79.2 while largely preserving
mathematical and coding competence. Extensive experiments reveal that: (i)
RLVER consistently improves multiple dialogue capabilities; (ii) Thinking and
non-thinking models show distinct trends--thinking models excel in empathy and
insight, while non-thinking models favor action; (iii) GRPO often yields stable
gains, while PPO can push certain capabilities to a higher ceiling; (iv) More
challenging environments are not always better-moderate ones can yield stronger
outcomes. Our results show that RLVER is a practical route toward emotionally
intelligent and broadly capable language agents.

</details>


### [43] [ReliableMath: Benchmark of Reliable Mathematical Reasoning on Large Language Models](https://arxiv.org/abs/2507.03133)
*Boyang Xue,Qi Zhu,Rui Wang,Sheng Wang,Hongru Wang,Fei Mi,Yasheng Wang,Lifeng Shang,Qun Liu,Kam-Fai Wong*

Main category: cs.CL

TL;DR: LLM在数学推理中存在可靠性问题，尤其是在无法解决的问题上。研究者构建数据集并提出对齐策略来提高LLM的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在推理任务上表现出色，但在面对无法解决或超出其能力的问题时，往往会捏造不可靠的回答，严重损害了可靠性。以往对LLM可靠性的研究主要集中在知识任务上，以识别无法回答的问题，而由于缺乏无法解决的数学问题，数学推理任务仍未被探索。

Method: 构建了一个包含可解和不可解问题的高质量ReliableMath数据集，并提出了一个构建工作流程，通过人工评估合成不可解的问题。此外，还提出了一种对齐策略来提高小型LLM的可靠性。

Result: LLM无法直接识别无法解决的问题，并且总是生成捏造的响应。当指示LLM使用可靠的提示来指示无法解决时，较大尺寸的LLM在可解决问题上的可靠性仍然存在，但在无法解决的问题上显着提高，但仍未达到可解决问题的水平。但是，尽管采用了可靠的提示，但小型LLM很少显示出任何进展。

Conclusion: LLMs在无法解决的问题上会捏造不可靠的答案，即使使用可靠的提示，大型LLM在不可解问题上的可靠性有所提高，但仍不如可解问题。小型LLM即使采用可靠的提示也几乎没有进展。因此，我们提出了一种对齐策略来提高小型LLM的可靠性，这可以显著提高LLM在领域内和领域外任务中的可靠性表现。

Abstract: Although demonstrating remarkable performance on reasoning tasks, Large
Language Models (LLMs) still tend to fabricate unreliable responses when
confronted with problems that are unsolvable or beyond their capability,
severely undermining the reliability. Prior studies of LLM reliability have
primarily focused on knowledge tasks to identify unanswerable questions, while
mathematical reasoning tasks have remained unexplored due to the dearth of
unsolvable math problems. To systematically investigate LLM reliability in
mathematical reasoning tasks, we formulate the reliability evaluation for both
solvable and unsolvable problems. We then develop a ReliableMath dataset which
incorporates open-source solvable problems and high-quality unsolvable problems
synthesized by our proposed construction workflow with human evaluations.
Experiments are conducted on various LLMs with several key findings uncovered.
LLMs fail to directly identify unsolvable problems and always generate
fabricated responses. When instructing LLMs to indicate unsolvability using a
reliable prompt, the reliability of larger-sized LLMs remains on solvable
problems, but notably improves on unsolvable problems yet still falls short of
solvable problems. However, small LLMs rarely show any progress despite
employing reliable prompts. Therefore, we further propose an alignment strategy
to enhance small LLMs' reliability, which can significantly improve LLM
reliability performances on both in-domain and out-of-domain tasks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [44] [A Simulator Dataset to Support the Study of Impaired Driving](https://arxiv.org/abs/2507.02867)
*John Gideon,Kimimasa Tamura,Emily Sumner,Laporsha Dees,Patricio Reyes Gomez,Bassamul Haq,Todd Rowell,Avinash Balachandran,Simon Stent,Guy Rosman*

Main category: cs.CV

TL;DR: 我们提出了一个驾驶数据集，旨在支持研究两种常见的驾驶员损伤形式：酒精中毒和认知分心。


<details>
  <summary>Details</summary>
Motivation: 尽管最近自动驾驶技术取得了进展，但受损驾驶继续给社会带来高昂的代价。

Method: 该数据集跨越 23.7 小时的模拟城市驾驶，有 52 名人类受试者在正常和受损条件下，包括车辆数据（地面实况感知、车辆姿态、控制）和面向驾驶员的数据（凝视、音频、调查）。

Result: 我们提出了一个驾驶数据集，旨在支持研究两种常见的驾驶员损伤形式：酒精中毒和认知分心。

Conclusion: 该数据集支持分析驾驶员行为因酒精中毒（0.10％血液酒精含量）、两种形式的认知分心（音频 n-back 和句子解析任务）及其组合而产生的变化，以及对一组八个受控道路危险（例如车辆切入）的反应。

Abstract: Despite recent advances in automated driving technology, impaired driving
continues to incur a high cost to society. In this paper, we present a driving
dataset designed to support the study of two common forms of driver impairment:
alcohol intoxication and cognitive distraction. Our dataset spans 23.7 hours of
simulated urban driving, with 52 human subjects under normal and impaired
conditions, and includes both vehicle data (ground truth perception, vehicle
pose, controls) and driver-facing data (gaze, audio, surveys). It supports
analysis of changes in driver behavior due to alcohol intoxication (0.10\%
blood alcohol content), two forms of cognitive distraction (audio n-back and
sentence parsing tasks), and combinations thereof, as well as responses to a
set of eight controlled road hazards, such as vehicle cut-ins. The dataset will
be made available at https://toyotaresearchinstitute.github.io/IDD/.

</details>


### [45] [Learning to Generate Vectorized Maps at Intersections with Multiple Roadside Cameras](https://arxiv.org/abs/2507.02899)
*Miao Fan,Quanxin Zheng,Shengtong Xu,Linghe Kong,Haoyi Xiong*

Main category: cs.CV

TL;DR: MRC-VMap 是一种经济高效的、以视觉为中心的端到端神经网络，旨在直接在十字路口生成高清矢量化地图。


<details>
  <summary>Details</summary>
Motivation: 传统的地图构建方法分为两类：离线技术和在线方法。离线技术依赖于昂贵的、劳动密集型的激光雷达数据收集和人工注释，而在线方法使用车载摄像头来降低成本，但性能有限，尤其是在复杂的十字路口。

Method: MRC-VMap，一种经济高效的、以视觉为中心的端到端神经网络，旨在直接在十字路口生成高清矢量化地图。利用现有的路边监控摄像头，MRC-VMap 直接将时间对齐的多方向图像转换为矢量化地图表示。

Result: MRC-VMap 不仅优于最先进的在线方法，而且达到了与高成本的基于激光雷达的方法相当的精度。

Conclusion: MRC-VMap在中国的四个主要城市区域的 4,000 个十字路口进行了广泛的实验，结果表明，MRC-VMap 不仅优于最新的在线方法，而且达到了与高成本的基于激光雷达的方法相当的精度，从而为现代自主导航系统提供了一个可扩展且高效的解决方案。

Abstract: Vectorized maps are indispensable for precise navigation and the safe
operation of autonomous vehicles. Traditional methods for constructing these
maps fall into two categories: offline techniques, which rely on expensive,
labor-intensive LiDAR data collection and manual annotation, and online
approaches that use onboard cameras to reduce costs but suffer from limited
performance, especially at complex intersections. To bridge this gap, we
introduce MRC-VMap, a cost-effective, vision-centric, end-to-end neural network
designed to generate high-definition vectorized maps directly at intersections.
Leveraging existing roadside surveillance cameras, MRC-VMap directly converts
time-aligned, multi-directional images into vectorized map representations.
This integrated solution lowers the need for additional intermediate
modules--such as separate feature extraction and Bird's-Eye View (BEV)
conversion steps--thus reducing both computational overhead and error
propagation. Moreover, the use of multiple camera views enhances mapping
completeness, mitigates occlusions, and provides robust performance under
practical deployment constraints. Extensive experiments conducted on 4,000
intersections across 4 major metropolitan areas in China demonstrate that
MRC-VMap not only outperforms state-of-the-art online methods but also achieves
accuracy comparable to high-cost LiDAR-based approaches, thereby offering a
scalable and efficient solution for modern autonomous navigation systems.

</details>


### [46] [Advancing Talking Head Generation: A Comprehensive Survey of Multi-Modal Methodologies, Datasets, Evaluation Metrics, and Loss Functions](https://arxiv.org/abs/2507.02900)
*Vineet Kumar Rakesh,Soumya Mazumdar,Research Pratim Maity,Sarbajit Pal,Amitabha Das,Tapas Samanta*

Main category: cs.CV

TL;DR: This paper reviews talking head generation methodologies, evaluates algorithms and datasets, identifies challenges, and suggests future directions.


<details>
  <summary>Details</summary>
Motivation: Talking Head Generation (THG) has emerged as a transformative technology in computer vision, enabling the synthesis of realistic human faces synchronized with various inputs. Applications include digital avatars, video dubbing, ultra-low bitrate video conferencing, and online education.

Method: This paper categorizes talking head generation approaches into 2D-based, 3D-based, NeRF-based, diffusion-based, and parameter-driven techniques.

Result: The paper evaluates algorithms, datasets, and evaluation metrics, highlighting advancements in perceptual realism and technical efficiency. It also identifies challenges such as reliance on pre-trained models, extreme pose handling, multilingual synthesis, and temporal consistency.

Conclusion: This paper synthesizes existing research and explores emerging trends in talking head generation, aiming to provide actionable insights for researchers and practitioners. It also points out future research directions including modular architectures, multilingual datasets, hybrid models, and innovative loss functions.

Abstract: Talking Head Generation (THG) has emerged as a transformative technology in
computer vision, enabling the synthesis of realistic human faces synchronized
with image, audio, text, or video inputs. This paper provides a comprehensive
review of methodologies and frameworks for talking head generation,
categorizing approaches into 2D--based, 3D--based, Neural Radiance Fields
(NeRF)--based, diffusion--based, parameter-driven techniques and many other
techniques. It evaluates algorithms, datasets, and evaluation metrics while
highlighting advancements in perceptual realism and technical efficiency
critical for applications such as digital avatars, video dubbing, ultra-low
bitrate video conferencing, and online education. The study identifies
challenges such as reliance on pre--trained models, extreme pose handling,
multilingual synthesis, and temporal consistency. Future directions include
modular architectures, multilingual datasets, hybrid models blending
pre--trained and task-specific layers, and innovative loss functions. By
synthesizing existing research and exploring emerging trends, this paper aims
to provide actionable insights for researchers and practitioners in the field
of talking head generation. For the complete survey, code, and curated resource
list, visit our GitHub repository: https://github.com/VineetKumarRakesh/thg.

</details>


### [47] [Enhancing Sports Strategy with Video Analytics and Data Mining: Assessing the effectiveness of Multimodal LLMs in tennis video analysis](https://arxiv.org/abs/2507.02904)
*Charlton Teo*

Main category: cs.CV

TL;DR: 本项目旨在评估多模态大型语言模型在分析网球视频以识别动作序列方面的有效性，并探索提高其性能的方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型在理解和识别网球对抗中事件序列的能力方面存在差距，这在其他体育分析领域可能很有用。因此，我们将主要评估 MLLM 填补这一空白的能力——对网球动作进行分类，以及它们在网球对抗动作序列中识别这些动作的能力。

Method: 评估多模态大型语言模型（MLLM）在分析体育视频中的有效性，重点是网球视频。研究了改进 MLLM 性能的方法，包括不同的训练方法，甚至将它们与其他传统模型结合使用。

Result: 未在摘要中明确提及结果。

Conclusion: 未在摘要中明确提及结论。

Abstract: The use of Large Language Models (LLMs) in recent years has also given rise
to the development of Multimodal LLMs (MLLMs). These new MLLMs allow us to
process images, videos and even audio alongside textual inputs. In this
project, we aim to assess the effectiveness of MLLMs in analysing sports
videos, focusing mainly on tennis videos. Despite research done on tennis
analysis, there remains a gap in models that are able to understand and
identify the sequence of events in a tennis rally, which would be useful in
other fields of sports analytics. As such, we will mainly assess the MLLMs on
their ability to fill this gap - to classify tennis actions, as well as their
ability to identify these actions in a sequence of tennis actions in a rally.
We further looked into ways we can improve the MLLMs' performance, including
different training methods and even using them together with other traditional
models.

</details>


### [48] [Enhancing Sports Strategy with Video Analytics and Data Mining: Automated Video-Based Analytics Framework for Tennis Doubles](https://arxiv.org/abs/2507.02906)
*Jia Wei Chen*

Main category: cs.CV

TL;DR: 我们提出了一个全面的基于视频的网球双打分析框架，该框架集成了先进的机器学习技术，以实现自动战术分析和性能评估。


<details>
  <summary>Details</summary>
Motivation: 解决缺乏针对这种战略复杂的运动的自动化分析工具的问题。

Method: 集成了先进的机器学习技术，包括用于通过自然语言 grounding 实现精确定位球员的 GroundingDINO 和用于鲁棒姿势估计的 YOLO-Pose。

Result: 基于 CNN 的模型在使用迁移学习的情况下，在预测击球类型、球员位置和阵型方面，明显优于基于姿势的方法。

Conclusion: 集成的系统将先进的分析能力与网球双打的战略复杂性结合起来，为专业网球中的自动战术分析、性能评估和战略建模奠定了基础。

Abstract: We present a comprehensive video-based analytics framework for tennis doubles
that addresses the lack of automated analysis tools for this strategically
complex sport. Our approach introduces a standardised annotation methodology
encompassing player positioning, shot types, court formations, and match
outcomes, coupled with a specialised annotation tool designed to meet the
unique requirements of tennis video labelling. The framework integrates
advanced machine learning techniques including GroundingDINO for precise player
localisation through natural language grounding and YOLO-Pose for robust pose
estimation. This combination significantly reduces manual annotation effort
whilst improving data consistency and quality. We evaluate our approach on
doubles tennis match data and demonstrate that CNN-based models with transfer
learning substantially outperform pose-based methods for predicting shot types,
player positioning, and formations. The CNN models effectively capture complex
visual and contextual features essential for doubles tennis analysis. Our
integrated system bridges advanced analytical capabilities with the strategic
complexities of tennis doubles, providing a foundation for automated tactical
analysis, performance evaluation, and strategic modelling in professional
tennis.

</details>


### [49] [Modeling Urban Food Insecurity with Google Street View Images](https://arxiv.org/abs/2507.02924)
*David Li*

Main category: cs.CV

TL;DR: 该项目旨在探索使用街景图像在人口普查区级别对粮食不安全进行建模的有效性。


<details>
  <summary>Details</summary>
Motivation: 粮食不安全是一个重要的社会和公共健康问题，困扰着世界各地的许多城市都市区。 现有的识别粮食不安全的方法主要依赖于定性和定量调查数据，这很难扩展。

Method: 我们提出了一个两步过程，即特征提取和门控注意的图像聚合。

Result: 我们通过与其他模型架构进行比较、解释我们学习的权重和执行案例研究来评估我们模型的有效性。

Conclusion: 虽然我们的模型在预测能力方面略有不足，但我们相信我们的方法仍然有潜力补充现有城市规划者和政策制定者识别粮食不安全的方法。

Abstract: Food insecurity is a significant social and public health issue that plagues
many urban metropolitan areas around the world. Existing approaches to
identifying food insecurity rely primarily on qualitative and quantitative
survey data, which is difficult to scale. This project seeks to explore the
effectiveness of using street-level images in modeling food insecurity at the
census tract level. To do so, we propose a two-step process of feature
extraction and gated attention for image aggregation. We evaluate the
effectiveness of our model by comparing against other model architectures,
interpreting our learned weights, and performing a case study. While our model
falls slightly short in terms of its predictive power, we believe our approach
still has the potential to supplement existing methods of identifying food
insecurity for urban planners and policymakers.

</details>


### [50] [OBSER: Object-Based Sub-Environment Recognition for Zero-Shot Environmental Inference](https://arxiv.org/abs/2507.02929)
*Won-Seok Choi,Dong-Sig Han,Suhyung Choi,Hyeonseo Yang,Byoung-Tak Zhang*

Main category: cs.CV

TL;DR: 提出了 OBSER 框架，该框架通过推断子环境及其组成对象之间的关系来实现零样本环境识别。


<details>
  <summary>Details</summary>
Motivation: 这项工作提出了对象子环境识别 (OBSER) 框架。

Method: 提出的对象子环境识别 (OBSER) 框架是一种新颖的贝叶斯框架，可推断子环境及其组成对象之间的三个基本关系。度量和自我监督学习模型估计潜在空间上子环境的对象分布，以计算这些度量。

Result: 该框架在开放世界和照片般真实的环境中可靠地执行推理，并且在链式检索任务中优于基于场景的方法。

Conclusion: 该框架实现了环境的零样本识别，以实现自主环境理解。

Abstract: We present the Object-Based Sub-Environment Recognition (OBSER) framework, a
novel Bayesian framework that infers three fundamental relationships between
sub-environments and their constituent objects. In the OBSER framework, metric
and self-supervised learning models estimate the object distributions of
sub-environments on the latent space to compute these measures. Both
theoretically and empirically, we validate the proposed framework by
introducing the ($\epsilon,\delta$) statistically separable (EDS) function
which indicates the alignment of the representation. Our framework reliably
performs inference in open-world and photorealistic environments and
outperforms scene-based methods in chained retrieval tasks. The OBSER framework
enables zero-shot recognition of environments to achieve autonomous environment
understanding.

</details>


### [51] [GameTileNet: A Semantic Dataset for Low-Resolution Game Art in Procedural Content Generation](https://arxiv.org/abs/2507.02941)
*Yi-Chun Chen,Arnav Jhala*

Main category: cs.CV

TL;DR: GameTileNet是一个低分辨率游戏图块的语义数据集，旨在通过视觉语言对齐来支持叙事驱动的程序内容生成。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）和图像生成AI模型使独立开发者能够为游戏互动创建视觉资产，例如精灵。然而，由于AI输出的不一致，生成与游戏叙事相符的视觉效果仍然具有挑战性，需要人工艺术家进行手动调整。由于训练数据中风格分布的不平衡，自动生成的游戏内容中视觉表现的多样性也受到限制。GameTileNet通过在知识共享许可下从OpenGameArt.org收集艺术家创作的游戏瓦片，并提供语义注释以支持叙事驱动的内容生成来解决这个问题。

Method: 该数据集引入了一个用于在基于低分辨率瓦片的游戏美术（例如，32x32像素）中进行对象检测的流程，并注释了语义、连通性和对象分类。

Result: GameTileNet是一个语义数据集，包含低分辨率游戏图块，旨在通过视觉语言对齐支持叙事驱动的程序内容生成。

Conclusion: GameTileNet是一个有价值的资源，可以改进PCG方法，支持叙事丰富的游戏内容，并为低分辨率、非照片真实感图像中的对象检测建立基线。

Abstract: GameTileNet is a dataset designed to provide semantic labels for
low-resolution digital game art, advancing procedural content generation (PCG)
and related AI research as a vision-language alignment task. Large Language
Models (LLMs) and image-generative AI models have enabled indie developers to
create visual assets, such as sprites, for game interactions. However,
generating visuals that align with game narratives remains challenging due to
inconsistent AI outputs, requiring manual adjustments by human artists. The
diversity of visual representations in automatically generated game content is
also limited because of the imbalance in distributions across styles for
training data. GameTileNet addresses this by collecting artist-created game
tiles from OpenGameArt.org under Creative Commons licenses and providing
semantic annotations to support narrative-driven content generation. The
dataset introduces a pipeline for object detection in low-resolution tile-based
game art (e.g., 32x32 pixels) and annotates semantics, connectivity, and object
classifications. GameTileNet is a valuable resource for improving PCG methods,
supporting narrative-rich game content, and establishing a baseline for object
detection in low-resolution, non-photorealistic images.
  TL;DR: GameTileNet is a semantic dataset of low-resolution game tiles
designed to support narrative-driven procedural content generation through
visual-language alignment.

</details>


### [52] [Iterative Zoom-In: Temporal Interval Exploration for Long Video Understanding](https://arxiv.org/abs/2507.02946)
*Chenglin Li,Qianglong Chen,fengtao,Yin Zhang*

Main category: cs.CV

TL;DR: The paper introduces Temporal Search (TS), a training-free framework that enables MLLMs to explore temporal regions for improved long video understanding iteratively by iteratively shifting attention to more fine-grained temporal intervals, and TS-BFS, a best-first search strategy over a tree to further improve efficiency.


<details>
  <summary>Details</summary>
Motivation: Current Multimodal Large Language Models (MLLMs) struggle with long-form videos because of an inefficient perception of temporal intervals, relying on dense, uniform sampling, leading to high memory consumption and a risk of missing crucial information.

Method: The authors introduce Temporal Search, a training-free framework that enables MLLMs to explore temporal regions for improved long video understanding iteratively. TS operates through two main iterative stages: proposing a temporal interval and sampling frames to produce a refined response and confidence score. To further improve efficiency, they introduce TS-BFS, a best-first search strategy over a tree.

Result: The model's generation confidence across different temporal intervals is highly correlated with prediction accuracy.

Conclusion: Temporal Search (TS) refines the focus of the model by iteratively shifting attention to more fine-grained temporal intervals, improving its understanding of long videos. Additionally, keyframe-level descriptions are collected to facilitate cross-interval perception throughout the video. To further improve efficiency, the authors introduce TS-BFS, a best-first search strategy over a tree. Each node represents a candidate interval and is expanded via two methods: self-driven proposals and uniform partitioning. Nodes are scored based on confidence and self-evaluation, and the most promising one is selected for continued exploration.

Abstract: Multimodal Large Language Models (MLLMs) have shown strong performance in
video understanding tasks. However, they continue to struggle with long-form
videos because of an inefficient perception of temporal intervals. Unlike
humans, who can dynamically adjust their temporal focus to locate
query-relevant moments, current MLLMs often rely on dense, uniform sampling
across the video timeline, leading to high memory consumption and a risk of
missing crucial information. To address this challenge, we introduce Temporal
Search, a training-free framework that enables MLLMs to explore temporal
regions for improved long video understanding iteratively. TS is based on a key
observation: the model's generation confidence across different temporal
intervals is highly correlated with prediction accuracy. TS operates through
two main iterative stages. First, the MLLM proposes a temporal interval that is
likely to contain task-relevant information. Then, it samples a fixed number of
frames from the interval, regardless of length, and feeds them into the model
to produce a refined response and confidence score. TS refines the focus of the
model by iteratively shifting attention to more fine-grained temporal
intervals, improving its understanding of long videos. Additionally,
keyframe-level descriptions are collected to facilitate cross-interval
perception throughout the video. To further improve efficiency, we introduce
TS-BFS, a best-first search strategy over a tree. Each node represents a
candidate interval and is expanded via two methods: self-driven proposals and
uniform partitioning. Nodes are scored based on confidence and self-evaluation,
and the most promising one is selected for continued exploration.

</details>


### [53] [DriveMRP: Enhancing Vision-Language Models with Synthetic Motion Data for Motion Risk Prediction](https://arxiv.org/abs/2507.02948)
*Zhiyi Hou,Enhui Ma,Fang Li,Zhiyi Lai,Kalok Ho,Zhanqian Wu,Lijun Zhou,Long Chen,Chitian Sun,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye,Kaicheng Yu*

Main category: cs.CV

TL;DR: This paper introduces DriveMRP-10K and DriveMRP-Agent to enhance motion risk prediction in autonomous driving by synthesizing high-risk data and improving VLM reasoning, achieving significant accuracy improvements in simulated and real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Accurately predicting motion risk in long-tail scenarios for autonomous driving remains challenging due to uncertainties and limited data coverage.

Method: A BEV-based motion simulation method is introduced to synthesize high-risk motion data (DriveMRP-10K). A VLM-agnostic motion risk estimation framework (DriveMRP-Agent) is designed with a novel information injection strategy.

Result: Fine-tuning with DriveMRP-10K boosts accident recognition accuracy from 27.13% to 88.03%. Zero-shot evaluation on real-world data improves accuracy from 29.42% to 68.50%.

Conclusion: The DriveMRP-Agent framework, fine-tuned with DriveMRP-10K, significantly improves motion risk prediction for VLMs, demonstrating strong generalization in real-world scenarios.

Abstract: Autonomous driving has seen significant progress, driven by extensive
real-world data. However, in long-tail scenarios, accurately predicting the
safety of the ego vehicle's future motion remains a major challenge due to
uncertainties in dynamic environments and limitations in data coverage. In this
work, we aim to explore whether it is possible to enhance the motion risk
prediction capabilities of Vision-Language Models (VLM) by synthesizing
high-risk motion data. Specifically, we introduce a Bird's-Eye View (BEV) based
motion simulation method to model risks from three aspects: the ego-vehicle,
other vehicles, and the environment. This allows us to synthesize
plug-and-play, high-risk motion data suitable for VLM training, which we call
DriveMRP-10K. Furthermore, we design a VLM-agnostic motion risk estimation
framework, named DriveMRP-Agent. This framework incorporates a novel
information injection strategy for global context, ego-vehicle perspective, and
trajectory projection, enabling VLMs to effectively reason about the spatial
relationships between motion waypoints and the environment. Extensive
experiments demonstrate that by fine-tuning with DriveMRP-10K, our
DriveMRP-Agent framework can significantly improve the motion risk prediction
performance of multiple VLM baselines, with the accident recognition accuracy
soaring from 27.13% to 88.03%. Moreover, when tested via zero-shot evaluation
on an in-house real-world high-risk motion dataset, DriveMRP-Agent achieves a
significant performance leap, boosting the accuracy from base_model's 29.42% to
68.50%, which showcases the strong generalization capabilities of our method in
real-world scenarios.

</details>


### [54] [Multimodal image registration for effective thermographic fever screening](https://arxiv.org/abs/2507.02955)
*C. Y. N. Dwith,Pejhman Ghassemi,Joshua Pfefer,Jon Casamento,Quanzeng Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种用于精确定位canthi区域的红外和白光图像配准方法，该方法在体温筛查中很有用。


<details>
  <summary>Details</summary>
Motivation: 在埃博拉和SARS等传染病大流行期间，基于红外热像图(IRTs)的体温筛查是一种可行的群体筛查方法，用于在医院和机场等公共场所进行体温监测。研究发现，红外热像仪是检测体温升高的有效、快速和非侵入性方法。此外，内眦内侧相邻区域(本文中称为canthi区域)是体温筛查的首选部位。

Method: 该论文提出了一种粗略到精细的配准策略，使用基于地标和眼睛轮廓边缘检测的不同配准模型。

Result: 该论文评估了配准精度在2.7mm以内，这使得canthi区域的精确定位成为可能。

Conclusion: 该论文提出了一种基于红外和白光图像的多模态配准方法，通过粗略到精细的配准策略，利用地标和眼睛轮廓的边缘检测，实现了canthi区域的精确定位。

Abstract: Fever screening based on infrared thermographs (IRTs) is a viable mass
screening approach during infectious disease pandemics, such as Ebola and SARS,
for temperature monitoring in public places like hospitals and airports. IRTs
have found to be powerful, quick and non-invasive methods to detect elevated
temperatures. Moreover, regions medially adjacent to the inner canthi (called
the canthi regions in this paper) are preferred sites for fever screening.
Accurate localization of the canthi regions can be achieved through multi-modal
registration of infrared (IR) and white-light images. We proposed a
registration method through a coarse-fine registration strategy using different
registration models based on landmarks and edge detection on eye contours. We
evaluated the registration accuracy to be within 2.7 mm, which enables accurate
localization of the canthi regions.

</details>


### [55] [CS-VLM: Compressed Sensing Attention for Efficient Vision-Language Representation Learning](https://arxiv.org/abs/2507.02957)
*Andrew Kiruluta,Preethi Raju,Priscilla Burity*

Main category: cs.CV

TL;DR: CSAT 是一种新的 vLLM 架构，它使用压缩感知来降低 attention 的计算复杂度，同时保持语义保真度。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型 (vLLM) 在处理长视频序列和丰富的语言描述时，标准注意力机制的二次复杂度带来了根本的计算瓶颈。在 vLLM 中，不仅需要在模态内计算注意力，还需要跨模态计算，导致内存和延迟成本过高。

Method: 提出了一种新的架构，即压缩感知注意力 Transformer (CSAT)，通过随机测量矩阵将高维键和值表示投影到低维子空间，并使用稀疏恢复算法重建注意力输出。

Result: CSAT 具有可扩展性、可解释性和资源效率，适用于下一代多模态 Transformer。

Conclusion: CSAT 通过压缩感知显著降低了 vLLM 中 attention 的计算复杂度，同时保持了语义保真度，并且在标准基准测试中验证了其性能。

Abstract: Vision-Language Models (vLLMs) have emerged as powerful architectures for
joint reasoning over visual and textual inputs, enabling breakthroughs in image
captioning, cross modal retrieval, and multimodal dialogue. However, as these
models scale to longer video sequences and richer language descriptions, the
quadratic complexity of the standard attention mechanism presents a fundamental
computational bottleneck. This challenge is exacerbated in vLLMs, where
attention must be computed not only within modalities but also across them,
leading to prohibitive memory and latency costs. In this work, we introduce the
Compressed Sensing Attention Transformer (CSAT), a novel architecture that
reimagines attention computation through the lens of compressed sensing. By
projecting high dimensional key and value representations into a
lower-dimensional subspace via random measurement matrices and reconstructing
the attention outputs using sparse recovery algorithms, CSAT significantly
reduces attention complexity while maintaining semantic fidelity. Applied to
vLLMs, CSAT exploits the inherent compressibility of both visual and textual
representations especially evident in video, where temporal redundancy is high,
and in language, where cross-modal grounding is often sparse. In contrast to
LLMs, which must often model entangled symbolic dependencies, vLLMs benefit
from structured sparsity in alignment and scene composition, making them
particularly well-suited to compressed attention. We provide a formal
mathematical treatment of CSAT, demonstrate its integration into vision
language pipelines, and validate its performance on standard benchmarks,
highlighting its promise as a scalable, interpretable, and resource efficient
solution for next generation multimodal transformers.

</details>


### [56] [VR-YOLO: Enhancing PCB Defect Detection with Viewpoint Robustness Based on YOLO](https://arxiv.org/abs/2507.02963)
*Hengyi Zhu,Linye Wei,He Li*

Main category: cs.CV

TL;DR: 提出VR-YOLO算法，通过DSE和KOF方案，提高了PCB缺陷检测在视角变化下的准确率，且计算成本增加不大。


<details>
  <summary>Details</summary>
Motivation: 传统的检测算法对目标图像的角度、方向和清晰度有严格的要求，而大规模电路和系统的集成使得电子元件的自动缺陷检测变得非常重要。

Method: 提出了一种名为VR-YOLO的增强PCB缺陷检测算法，该算法基于YOLOv8模型，并采用了多样化场景增强（DSE）方法和新颖的关键对象聚焦（KOF）方案。

Result: 改进后的PCB缺陷检测方法在原始测试图像上的平均精度均值（mAP）为98.9%，在视角偏移的测试图像上的平均精度均值为94.7%。

Conclusion: VR-YOLO算法在保持较低计算成本的同时，显著提高了PCB缺陷检测的准确率，尤其是在视角变化的情况下。

Abstract: The integration of large-scale circuits and systems emphasizes the importance
of automated defect detection of electronic components. The YOLO image
detection model has been used to detect PCB defects and it has become a typical
AI-assisted case of traditional industrial production. However, conventional
detection algorithms have stringent requirements for the angle, orientation,
and clarity of target images. In this paper, we propose an enhanced PCB defect
detection algorithm, named VR-YOLO, based on the YOLOv8 model. This algorithm
aims to improve the model's generalization performance and enhance viewpoint
robustness in practical application scenarios. We first propose a diversified
scene enhancement (DSE) method by expanding the PCB defect dataset by
incorporating diverse scenarios and segmenting samples to improve target
diversity. A novel key object focus (KOF) scheme is then presented by
considering angular loss and introducing an additional attention mechanism to
enhance fine-grained learning of small target features. Experimental results
demonstrate that our improved PCB defect detection approach achieves a mean
average precision (mAP) of 98.9% for the original test images, and 94.7% for
the test images with viewpoint shifts (horizontal and vertical shear
coefficients of $\pm 0.06$ and rotation angle of $\pm 10$ degrees), showing
significant improvements compared to the baseline YOLO model with negligible
additional computational cost.

</details>


### [57] [Concept-based Adversarial Attack: a Probabilistic Perspective](https://arxiv.org/abs/2507.02965)
*Andi Zhang,Xuan Ding,Steven McDonagh,Samuel Kaski*

Main category: cs.CV

TL;DR: Proposes a concept-based adversarial attack framework to generate diverse adversarial examples by operating on an entire concept, achieving higher attack efficiency while preserving the underlying concept.


<details>
  <summary>Details</summary>
Motivation: Extending beyond single-image perturbations by adopting a probabilistic perspective.

Method: Concept-based adversarial attack framework that operates on a concept represented by a probabilistic generative model or a set of images.

Result: Generate images that maintain the original concept but vary in pose, viewpoint, or background, thereby misleading the classifier.

Conclusion: Concept-based adversarial attacks yield more diverse adversarial examples, effectively preserve the underlying concept, and achieve higher attack efficiency.

Abstract: We propose a concept-based adversarial attack framework that extends beyond
single-image perturbations by adopting a probabilistic perspective. Rather than
modifying a single image, our method operates on an entire concept --
represented by a probabilistic generative model or a set of images -- to
generate diverse adversarial examples. Preserving the concept is essential, as
it ensures that the resulting adversarial images remain identifiable as
instances of the original underlying category or identity. By sampling from
this concept-based adversarial distribution, we generate images that maintain
the original concept but vary in pose, viewpoint, or background, thereby
misleading the classifier. Mathematically, this framework remains consistent
with traditional adversarial attacks in a principled manner. Our theoretical
and empirical results demonstrate that concept-based adversarial attacks yield
more diverse adversarial examples and effectively preserve the underlying
concept, while achieving higher attack efficiency.

</details>


### [58] [YOLO-Based Pipeline Monitoring in Challenging Visual Environments](https://arxiv.org/abs/2507.02967)
*Pragya Dhungana,Matteo Fresta,Niraj Tamrakar,Hariom Dhungana*

Main category: cs.CV

TL;DR: 本研究探索了先进的人工智能 (AI) 技术在水下管道检测中的应用，比较了 YOLOv8 和 YOLOv11 在图像分割任务中的性能，结果表明 YOLOv11 表现更好。


<details>
  <summary>Details</summary>
Motivation: 由于浊度、光线扭曲和图像退化，在低能见度水下环境中对海底管道进行状态监测提出了重大挑战。传统的基于视觉的检测系统通常无法在此类条件下提供用于映射、对象识别或缺陷检测的可靠数据。

Method: 对 YOLOv8 和 Yolov11 及其三种变体进行了比较分析，这些变体专为复杂和低能见度海底环境中的图像分割任务而定制。使用在海底捕获的管道检查数据集，评估了模型在具有挑战性的视觉条件下准确描绘目标结构方面的性能。

Result: 结果表明 YOLOv11 在整体性能上优于 YOLOv8。

Conclusion: YOLOv11 在整体性能上优于 YOLOv8。

Abstract: Condition monitoring subsea pipelines in low-visibility underwater
environments poses significant challenges due to turbidity, light distortion,
and image degradation. Traditional visual-based inspection systems often fail
to provide reliable data for mapping, object recognition, or defect detection
in such conditions. This study explores the integration of advanced artificial
intelligence (AI) techniques to enhance image quality, detect pipeline
structures, and support autonomous fault diagnosis. This study conducts a
comparative analysis of two most robust versions of YOLOv8 and Yolov11 and
their three variants tailored for image segmentation tasks in complex and
low-visibility subsea environments. Using pipeline inspection datasets captured
beneath the seabed, it evaluates model performance in accurately delineating
target structures under challenging visual conditions. The results indicated
that YOLOv11 outperformed YOLOv8 in overall performance.

</details>


### [59] [Farm-Level, In-Season Crop Identification for India](https://arxiv.org/abs/2507.02972)
*Ishan Deshpande,Amandeep Kaur Reehal,Chandan Nath,Renu Singh,Aayush Patel,Aishwarya Jayagopal,Gaurav Singh,Gaurav Aggarwal,Amit Agarwal,Prathmesh Bele,Sridhar Reddy,Tanya Warrier,Kinjal Singh,Ashish Tendulkar,Luis Pazos Outon,Nikita Saxena,Agata Dondzik,Dinesh Tewari,Shruti Garg,Avneet Singh,Harsh Dhand,Vaibhav Rajan,Alok Talekar*

Main category: cs.CV

TL;DR: This paper presents a deep learning framework for farm-level, in-season, multi-crop identification at national scale (India) using Sentinel-1 and Sentinel-2 satellite imagery, integrated with national-scale farm boundary data.  The model identifies 12 major crops with high accuracy and provides early and reliable crop identification. The system's effectiveness and scalability are demonstrated through robust validation against national agricultural statistics.


<details>
  <summary>Details</summary>
Motivation: Accurate, timely, and farm-level crop type information is paramount for national food security, agricultural policy formulation, and economic planning, particularly in agriculturally significant nations like India. Existing approaches often grapple with challenges such as limited geographical scalability, restricted crop type coverage, the complexities of mixed-pixel and heterogeneous landscapes, and crucially, the robust in-season identification essential for proactive decision-making.

Method: deep learning, Sentinel-1 and Sentinel-2 satellite imagery, integrated with national-scale farm boundary data, automated season detection algorithm, scalable inference pipeline

Result: identifies 12 major crops (which collectively account for nearly 90% of India's total cultivated area showing an agreement with national crop census 2023-24 of 94% in winter, and 75% in monsoon season). This allows for reliable crop identification as early as two months into the growing season and facilitates rigorous in-season performance evaluation.  culminating in what is, to our knowledge, the first pan-India, in-season, farm-level crop type data product.

Conclusion: The system's effectiveness and scalability are demonstrated through robust validation against national agricultural statistics, showcasing its potential to deliver actionable, data-driven insights for transformative agricultural monitoring and management across India.

Abstract: Accurate, timely, and farm-level crop type information is paramount for
national food security, agricultural policy formulation, and economic planning,
particularly in agriculturally significant nations like India. While remote
sensing and machine learning have become vital tools for crop monitoring,
existing approaches often grapple with challenges such as limited geographical
scalability, restricted crop type coverage, the complexities of mixed-pixel and
heterogeneous landscapes, and crucially, the robust in-season identification
essential for proactive decision-making.
  We present a framework designed to address the critical data gaps for
targeted data driven decision making which generates farm-level, in-season,
multi-crop identification at national scale (India) using deep learning. Our
methodology leverages the strengths of Sentinel-1 and Sentinel-2 satellite
imagery, integrated with national-scale farm boundary data. The model
successfully identifies 12 major crops (which collectively account for nearly
90% of India's total cultivated area showing an agreement with national crop
census 2023-24 of 94% in winter, and 75% in monsoon season). Our approach
incorporates an automated season detection algorithm, which estimates crop
sowing and harvest periods. This allows for reliable crop identification as
early as two months into the growing season and facilitates rigorous in-season
performance evaluation. Furthermore, we have engineered a highly scalable
inference pipeline, culminating in what is, to our knowledge, the first
pan-India, in-season, farm-level crop type data product. The system's
effectiveness and scalability are demonstrated through robust validation
against national agricultural statistics, showcasing its potential to deliver
actionable, data-driven insights for transformative agricultural monitoring and
management across India.

</details>


### [60] [Mimesis, Poiesis, and Imagination: Exploring Text-to-Image Generation of Biblical Narratives](https://arxiv.org/abs/2507.02973)
*Willem Th. van Peursen,Samuel E. Entsua-Mensah*

Main category: cs.CV

TL;DR: 本研究分析了人工智能生成的圣经故事图像，发现人工智能擅长产生审美视觉效果，但也反映了其训练数据的偏差和局限性。


<details>
  <summary>Details</summary>
Motivation: 本研究探讨了人工智能与圣经叙事可视化之间的交叉点，通过使用MidJourney分析人工智能生成的出埃及记2:5-9（在尼罗河发现的摩西）的图像。

Method: 通过比较视觉分析，包括谷歌图像结果和古典绘画，评估人工智能生成的图像的风格、神学和文化维度。

Result: 研究结果表明，虽然人工智能擅长产生审美丰富和富有想象力的视觉效果，但它也反映了其训练数据的偏差和局限性。

Conclusion: AI可以作为重新诠释圣经文本的创意伙伴，但它在神圣艺术中的作用仍然复杂且有争议。

Abstract: This study explores the intersection of artificial intelligence and the
visualization of Biblical narratives by analyzing AI-generated images of Exodus
2:5-9 (Moses found in River Nile) using MidJourney. Drawing on the classical
concepts of mimesis (imitation) and poiesis (creative generation), the authors
investigate how text-to-image (T2I) models reproduce or reimagine sacred
narratives. Through comparative visual analysis, including Google image results
and classical paintings, the research evaluates the stylistic, theological, and
cultural dimensions of AI-generated depictions. Findings show that while AI
excels in producing aesthetically rich and imaginative visuals, it also
reflects the biases and limitations of its training data. The study highlights
AI's potential to augment human imagination but questions its capacity for
genuine creativity, authorial intent, and theological depth. It concludes by
suggesting that AI can serve as a creative partner in reinterpreting biblical
texts, though its role in sacred art remains complex and contested.

</details>


### [61] [Ascending the Infinite Ladder: Benchmarking Spatial Deformation Reasoning in Vision-Language Models](https://arxiv.org/abs/2507.02978)
*Jiahuan Zhang,Shunwen Bai,Tianheng Wang,Kaiwen Guo,Kai Han,Guozheng Rao,Kaicheng Yu*

Main category: cs.CV

TL;DR: 论文提出了一个评估 VLM 空间变形推理能力的新框架，发现现有模型在这方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 评估视觉语言模型 (VLM) 在空间变形推理任务中的性能，因为目前尚不清楚这些模型是否真正理解和操纵空间对象。

Method: 构建了一个从 2D 到 3D 的空间变形推理基准，利用数据引擎生成无限的评估问题对。

Result: 基准测试结果表明，几乎没有模型表现出合理的空间变形推理能力。

Conclusion: VLMs 几乎没有表现出合理的空间变形推理能力，即使在应用有针对性的训练和主流推理增强方法后，模型仍然无法在 3D 空间变形推理方面表现良好。

Abstract: Humans naturally possess the spatial reasoning ability to form and manipulate
images and structures of objects in space. There is an increasing effort to
endow Vision-Language Models (VLMs) with similar spatial reasoning
capabilities. However, it remains unclear whether these models truly understand
and manipulate spatial objects or not. To address this question, we propose a
new evaluation framework aimed at assessing the performance of VLMs in spatial
deformation reasoning tasks. Specifically, we construct a benchmark for spatial
deformation reasoning from 2D to 3D. Leveraging our data engine, we can
generate unlimited evaluation problem pairs with infinite steps, without any
data leakage. We explore whether the model can effectively perform spatial
deformation reasoning from two directions: forward reasoning (given the
operations, find the final state) and reverse reasoning (given the final state,
determine the operations). We adopt a ladder competition format, using the
number of deformation steps as the level classification criterion, with the
goal of exploring the boundaries of the model's deformation reasoning
capabilities. Interestingly, the benchmarking results reveal that almost no
model demonstrates plausible spatial deformation reasoning abilities.
Furthermore, even after applying targeted training and mainstream reasoning
enhancement methods, the models are still unable to perform well on 3D spatial
deformation reasoning.

</details>


### [62] [Iterative Misclassification Error Training (IMET): An Optimized Neural Network Training Technique for Image Classification](https://arxiv.org/abs/2507.02979)
*Ruhaan Singh,Sreelekha Guggilam*

Main category: cs.CV

TL;DR: This paper introduces Iterative Misclassification Error Training (IMET), a novel framework inspired by curriculum learning and coreset selection, to enhance model robustness and accuracy in medical image analysis. The results demonstrating IMET's potential for enhancing model robustness and accuracy in medical image analysis are also presented in the paper.


<details>
  <summary>Details</summary>
Motivation: Medical datasets often contain noisy, mislabeled, or poorly generalizable images, particularly for edge cases and anomalous outcomes. Additionally, high quality datasets are often small in sample size that can result in overfitting, where models memorize noise rather than learn generalizable patterns. This in particular, could pose serious risks in medical diagnostics where the risk associated with mis-classification can impact human life. Several data-efficient training strategies have emerged to address these constraints. In particular, coreset selection identifies compact subsets of the most representative samples, enabling training that approximates full-dataset performance while reducing computational overhead. On the other hand, curriculum learning relies on gradually increasing training difficulty and accelerating convergence. However, developing a generalizable difficulty ranking mechanism that works across diverse domains, datasets, and models while reducing the computational tasks and remains challenging.

Method: The paper introduces Iterative Misclassification Error Training (IMET), a novel framework inspired by curriculum learning and coreset selection, to identify misclassified samples in order to streamline the training process, while prioritizing the model's attention to edge case senarious and rare outcomes.

Result: The results demonstrating IMET's potential for enhancing model robustness and accuracy in medical image analysis are also presented in the paper.

Conclusion: The paper evaluates IMET's performance on benchmark medical image classification datasets against state-of-the-art ResNet architectures, demonstrating IMET's potential for enhancing model robustness and accuracy in medical image analysis.

Abstract: Deep learning models have proven to be effective on medical datasets for
accurate diagnostic predictions from images. However, medical datasets often
contain noisy, mislabeled, or poorly generalizable images, particularly for
edge cases and anomalous outcomes. Additionally, high quality datasets are
often small in sample size that can result in overfitting, where models
memorize noise rather than learn generalizable patterns. This in particular,
could pose serious risks in medical diagnostics where the risk associated with
mis-classification can impact human life. Several data-efficient training
strategies have emerged to address these constraints. In particular, coreset
selection identifies compact subsets of the most representative samples,
enabling training that approximates full-dataset performance while reducing
computational overhead. On the other hand, curriculum learning relies on
gradually increasing training difficulty and accelerating convergence. However,
developing a generalizable difficulty ranking mechanism that works across
diverse domains, datasets, and models while reducing the computational tasks
and remains challenging. In this paper, we introduce Iterative
Misclassification Error Training (IMET), a novel framework inspired by
curriculum learning and coreset selection. The IMET approach is aimed to
identify misclassified samples in order to streamline the training process,
while prioritizing the model's attention to edge case senarious and rare
outcomes. The paper evaluates IMET's performance on benchmark medical image
classification datasets against state-of-the-art ResNet architectures. The
results demonstrating IMET's potential for enhancing model robustness and
accuracy in medical image analysis are also presented in the paper.

</details>


### [63] [Gated Recursive Fusion: A Stateful Approach to Scalable Multimodal Transformers](https://arxiv.org/abs/2507.02985)
*Yusuf Shihata*

Main category: cs.CV

TL;DR: Gated Recurrent Fusion (GRF) captures the power of cross-modal attention within a linearly scalable, recurrent pipeline, achieving competitive performance on the CMU-MOSI benchmark.


<details>
  <summary>Details</summary>
Motivation: Multimodal learning faces a fundamental tension between deep, fine-grained fusion and computational scalability. Cross-attention models achieve strong performance but their quadratic complexity is prohibitive for settings with many modalities.

Method: Gated Recurrent Fusion (GRF), a novel architecture that captures the power of cross-modal attention within a linearly scalable, recurrent pipeline. The core is a fusion block built on Transformer Decoder layers with a Gated Fusion Unit (GFU).

Result: GRF achieves competitive performance compared to more complex baselines on the CMU-MOSI benchmark.

Conclusion: GRF creates structured, class-separable representations through its progressive fusion mechanism and presents a robust and efficient paradigm for powerful, scalable multimodal representation learning.

Abstract: Multimodal learning faces a fundamental tension between deep, fine-grained
fusion and computational scalability. While cross-attention models achieve
strong performance through exhaustive pairwise fusion, their quadratic
complexity is prohibitive for settings with many modalities. We address this
challenge with Gated Recurrent Fusion (GRF), a novel architecture that captures
the power of cross-modal attention within a linearly scalable, recurrent
pipeline. Our method processes modalities sequentially, updating an evolving
multimodal context vector at each step. The core of our approach is a fusion
block built on Transformer Decoder layers that performs symmetric
cross-attention, mutually enriching the shared context and the incoming
modality. This enriched information is then integrated via a Gated Fusion Unit
(GFU) a GRU-inspired mechanism that dynamically arbitrates information flow,
enabling the model to selectively retain or discard features. This stateful,
recurrent design scales linearly with the number of modalities, O(n), making it
ideal for high-modality environments. Experiments on the CMU-MOSI benchmark
demonstrate that GRF achieves competitive performance compared to more complex
baselines. Visualizations of the embedding space further illustrate that GRF
creates structured, class-separable representations through its progressive
fusion mechanism. Our work presents a robust and efficient paradigm for
powerful, scalable multimodal representation learning.

</details>


### [64] [Leveraging the Structure of Medical Data for Improved Representation Learning](https://arxiv.org/abs/2507.02987)
*Andrea Agostini,Sonia Laguna,Alain Ryser,Samuel Ruiperez-Campillo,Moritz Vandenhirtz,Nicolas Deperrois,Farhad Nooralahzadeh,Michael Krauthammer,Thomas M. Sutter,Julia E. Vogt*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的自监督预训练框架，该框架利用医学图像的内在结构，在数据稀缺的情况下也能获得良好的性能。


<details>
  <summary>Details</summary>
Motivation: 构建可推广的医学人工智能系统需要数据高效且具有领域意识的预训练策略。与互联网规模的语料库不同，MIMIC-CXR等临床数据集提供的图像计数有限，注释稀疏，但通过多视图成像表现出丰富的内部结构。

Method: 该方法将配对的胸部X光片（即正面和侧面视图）视为自然正对，学习从稀疏patches中重建每个视图，同时对齐它们的潜在嵌入。

Result: 在MIMIC-CXR上评估，与监督目标和未利用结构的基线相比，该方法表现出强大的性能。

Conclusion: 这篇论文提出了一种自监督框架，该框架利用医学数据集的内在结构，在MIMIC-CXR数据集上取得了良好的性能。

Abstract: Building generalizable medical AI systems requires pretraining strategies
that are data-efficient and domain-aware. Unlike internet-scale corpora,
clinical datasets such as MIMIC-CXR offer limited image counts and scarce
annotations, but exhibit rich internal structure through multi-view imaging. We
propose a self-supervised framework that leverages the inherent structure of
medical datasets. Specifically, we treat paired chest X-rays (i.e., frontal and
lateral views) as natural positive pairs, learning to reconstruct each view
from sparse patches while aligning their latent embeddings. Our method requires
no textual supervision and produces informative representations. Evaluated on
MIMIC-CXR, we show strong performance compared to supervised objectives and
baselines being trained without leveraging structure. This work provides a
lightweight, modality-agnostic blueprint for domain-specific pretraining where
data is structured but scarce

</details>


### [65] [Enabling Robust, Real-Time Verification of Vision-Based Navigation through View Synthesis](https://arxiv.org/abs/2507.02993)
*Marius Neuhalfen,Jonathan Grzymisch,Manuel Sanchez-Gestido*

Main category: cs.CV

TL;DR: VISY-REVE是一种新的流程，通过合成视图实时增强图像数据集，解决了传统视觉导航图像处理算法验证方法设置困难和运行缓慢的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的验证方法（如合成渲染或机器人测试台采集）存在设置困难和运行缓慢的问题。

Method: 该流程通过使用合成视图实时增强图像数据集，从稀疏的预先存在的数据集中创建连续轨迹。

Result: 引入了一种新的相机姿势距离度量，即视轴偏差距离，并且开发了一种使用该度量来增加图像数据集密度的方法。

Conclusion: 提出了一种新的图像处理算法验证流程，VISY-REVE，用于基于视觉的导航。

Abstract: This work introduces VISY-REVE: a novel pipeline to validate image processing
algorithms for Vision-Based Navigation. Traditional validation methods such as
synthetic rendering or robotic testbed acquisition suffer from difficult setup
and slow runtime. Instead, we propose augmenting image datasets in real-time
with synthesized views at novel poses. This approach creates continuous
trajectories from sparse, pre-existing datasets in open or closed-loop. In
addition, we introduce a new distance metric between camera poses, the
Boresight Deviation Distance, which is better suited for view synthesis than
existing metrics. Using it, a method for increasing the density of image
datasets is developed.

</details>


### [66] [FreqCross: A Multi-Modal Frequency-Spatial Fusion Network for Robust Detection of Stable Diffusion 3.5 Generated Images](https://arxiv.org/abs/2507.02995)
*Guang Yang*

Main category: cs.CV

TL;DR: FreqCross, a multi-modal fusion network, robustly detects AI-generated images by combining spatial, frequency domain, and radial energy distribution features, achieving 97.8% accuracy.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of diffusion models, particularly Stable Diffusion 3.5, has enabled the generation of highly photorealistic synthetic images that pose significant challenges to existing detection methods.

Method: FreqCross, a novel multi-modal fusion network that combines spatial RGB features, frequency domain artifacts, and radial energy distribution patterns. It leverages a three-branch architecture: (1) a ResNet-18 backbone for spatial feature extraction, (2) a lightweight CNN for processing 2D FFT magnitude spectra, and (3) a multi-layer perceptron for analyzing radial energy profiles.

Result: FreqCross achieves 97.8% accuracy, outperforming state-of-the-art baselines by 5.2%.

Conclusion: FreqCross achieves 97.8% accuracy, outperforming state-of-the-art baselines by 5.2%. Synthetic images exhibit distinct spectral signatures in the 0.1--0.4 normalised frequency range.

Abstract: The rapid advancement of diffusion models, particularly Stable Diffusion 3.5,
has enabled the generation of highly photorealistic synthetic images that pose
significant challenges to existing detection methods. This paper presents
FreqCross, a novel multi-modal fusion network that combines spatial RGB
features, frequency domain artifacts, and radial energy distribution patterns
to achieve robust detection of AI-generated images. Our approach leverages a
three-branch architecture: (1) a ResNet-18 backbone for spatial feature
extraction, (2) a lightweight CNN for processing 2D FFT magnitude spectra, and
(3) a multi-layer perceptron for analyzing radial energy profiles. We introduce
a novel radial energy distribution analysis that captures characteristic
frequency artifacts inherent in diffusion-generated images, and fuse it with
spatial and spectral cues via simple feature concatenation followed by a
compact classification head. Extensive experiments on a dataset of 10,000
paired real (MS-COCO) and synthetic (Stable Diffusion 3.5) images demonstrate
that FreqCross achieves 97.8\% accuracy, outperforming state-of-the-art
baselines by 5.2\%. The frequency analysis further reveals that synthetic
images exhibit distinct spectral signatures in the 0.1--0.4 normalised
frequency range, providing theoretical foundation for our approach. Code and
pre-trained models are publicly available to facilitate reproducible research.

</details>


### [67] [Text-Guided Multi-Instance Learning for Scoliosis Screening via Gait Video Analysis](https://arxiv.org/abs/2507.02996)
*Haiqing Li,Yuzhi Guo,Feng Jiang,Thao M. Dang,Hehuan Ma,Qifeng Zhou,Jean Gao,Junzhou Huang*

Main category: cs.CV

TL;DR: Developed TG-MILNet, a non-invasive method using gait videos, DTW clustering, IBTA, and BAM, enhanced by textual guidance, to detect scoliosis, achieving state-of-the-art performance on the Scoliosis1K dataset.


<details>
  <summary>Details</summary>
Motivation: Early-stage scoliosis is often difficult to detect, particularly in adolescents, where delayed diagnosis can lead to serious health issues. Traditional X-ray-based methods carry radiation risks and rely heavily on clinical expertise, limiting their use in large-scale screenings.

Method: Text-Guided Multi-Instance Learning Network (TG-MILNet) for non-invasive scoliosis detection using gait videos, Dynamic Time Warping (DTW) clustering, Inter-Bag Temporal Attention (IBTA) mechanism, Boundary-Aware Model (BAM).

Result: achieves state-of-the-art performance, particularly excelling in handling class imbalance and accurately detecting challenging borderline cases

Conclusion: TG-MILNet achieves state-of-the-art performance, particularly excelling in handling class imbalance and accurately detecting challenging borderline cases.

Abstract: Early-stage scoliosis is often difficult to detect, particularly in
adolescents, where delayed diagnosis can lead to serious health issues.
Traditional X-ray-based methods carry radiation risks and rely heavily on
clinical expertise, limiting their use in large-scale screenings. To overcome
these challenges, we propose a Text-Guided Multi-Instance Learning Network
(TG-MILNet) for non-invasive scoliosis detection using gait videos. To handle
temporal misalignment in gait sequences, we employ Dynamic Time Warping (DTW)
clustering to segment videos into key gait phases. To focus on the most
relevant diagnostic features, we introduce an Inter-Bag Temporal Attention
(IBTA) mechanism that highlights critical gait phases. Recognizing the
difficulty in identifying borderline cases, we design a Boundary-Aware Model
(BAM) to improve sensitivity to subtle spinal deviations. Additionally, we
incorporate textual guidance from domain experts and large language models
(LLM) to enhance feature representation and improve model interpretability.
Experiments on the large-scale Scoliosis1K gait dataset show that TG-MILNet
achieves state-of-the-art performance, particularly excelling in handling class
imbalance and accurately detecting challenging borderline cases. The code is
available at https://github.com/lhqqq/TG-MILNet

</details>


### [68] [Topological Signatures vs. Gradient Histograms: A Comparative Study for Medical Image Classification](https://arxiv.org/abs/2507.03006)
*Faisal Ahmed,Mohammad Alfrad Nobel Bhuiyan*

Main category: cs.CV

TL;DR: HOG and TDA are compared for medical image classification. Both methods offer competitive performance and are suitable for integration into deep learning pipelines.


<details>
  <summary>Details</summary>
Motivation: Comparative study of two fundamentally distinct feature extraction techniques for medical image classification using retinal fundus images.

Method: Histogram of Oriented Gradients (HOG) and Topological Data Analysis (TDA), using cubical persistent homology, extracts high-level topological signatures that reflect the global structure of pixel intensities. Seven classical machine learning models with 10-fold cross-validation were trained.

Result: XGBoost achieved the best performance in both cases: 94.29 percent accuracy (HOG) and 94.18 percent (TDA) on the binary task; 74.41 percent (HOG) and 74.69 percent (TDA) on the multi-class task.

Conclusion: Both HOG and TDA offer competitive performance but encode different structural aspects of the images. The techniques are interpretable, applicable to other medical imaging domains, and suitable for integration into deep learning pipelines.

Abstract: We present the first comparative study of two fundamentally distinct feature
extraction techniques: Histogram of Oriented Gradients (HOG) and Topological
Data Analysis (TDA), for medical image classification using retinal fundus
images. HOG captures local texture and edge patterns through gradient
orientation histograms, while TDA, using cubical persistent homology, extracts
high-level topological signatures that reflect the global structure of pixel
intensities. We evaluate both methods on the large APTOS dataset for two
classification tasks: binary detection (normal versus diabetic retinopathy) and
five-class diabetic retinopathy severity grading. From each image, we extract
26244 HOG features and 800 TDA features, using them independently to train
seven classical machine learning models with 10-fold cross-validation. XGBoost
achieved the best performance in both cases: 94.29 percent accuracy (HOG) and
94.18 percent (TDA) on the binary task; 74.41 percent (HOG) and 74.69 percent
(TDA) on the multi-class task. Our results show that both methods offer
competitive performance but encode different structural aspects of the images.
This is the first work to benchmark gradient-based and topological features on
retinal imagery. The techniques are interpretable, applicable to other medical
imaging domains, and suitable for integration into deep learning pipelines.

</details>


### [69] [Markerless Stride Length estimation in Athletic using Pose Estimation with monocular vision](https://arxiv.org/abs/2507.03016)
*Patryk Skorupski,Cosimo Distante,Pier Luigi Mazzeo*

Main category: cs.CV

TL;DR: This paper introduces a computer vision method to estimate stride length and speed from videos, showing its potential for athlete coaching and training.


<details>
  <summary>Details</summary>
Motivation: Monitoring individual performance is essential for supporting staff coaches in establishing a proper training schedule for each athlete.

Method: A computer vision-based approach using probabilistic hough transform combined with a human pose detection algorithm to estimate leg joint position and homography transformation to estimate runner stride length.

Result: Experiments on various race videos with three different runners demonstrated the system's usefulness.

Conclusion: The proposed system is a useful tool for coaching and training, with potential value in measuring and monitoring the gait parameters of athletes.

Abstract: Performance measures such as stride length in athletics and the pace of
runners can be estimated using different tricks such as measuring the number of
steps divided by the running length or helping with markers printed on the
track. Monitoring individual performance is essential for supporting staff
coaches in establishing a proper training schedule for each athlete. The aim of
this paper is to investigate a computer vision-based approach for estimating
stride length and speed transition from video sequences and assessing video
analysis processing among athletes. Using some well-known image processing
methodologies such as probabilistic hough transform combined with a human pose
detection algorithm, we estimate the leg joint position of runners. In this
way, applying a homography transformation, we can estimate the runner stride
length. Experiments on various race videos with three different runners
demonstrated that the proposed system represents a useful tool for coaching and
training. This suggests its potential value in measuring and monitoring the
gait parameters of athletes.

</details>


### [70] [Look-Back: Implicit Visual Re-focusing in MLLM Reasoning](https://arxiv.org/abs/2507.03019)
*Shuo Yang,Yuwei Niu,Yuyang Liu,Yang Ye,Bin Lin,Li Yuan*

Main category: cs.CV

TL;DR: This paper introduces Look-Back, an implicit approach that guides MLLMs to re-focus on visual information in a self-directed manner during reasoning, enhancing reasoning and perception without explicit visual injection.


<details>
  <summary>Details</summary>
Motivation: MLLMs often excessively rely on textual information during the later stages of inference, neglecting the crucial integration of visual input

Method: introduce Look-Back, an implicit approach designed to guide MLLMs to look back at visual information

Result: MLLMs can spontaneously re-focus their attention on visual inputs during the later stages of reasoning, even without explicit visual information injection

Conclusion: Look-Back significantly enhances the model's reasoning and perception capabilities

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable progress in
multimodal reasoning. However, they often excessively rely on textual
information during the later stages of inference, neglecting the crucial
integration of visual input. Current methods typically address this by
explicitly injecting visual information to guide the reasoning process. In this
work, through an analysis of MLLM attention patterns, we made an intriguing
observation: with appropriate guidance, MLLMs can spontaneously re-focus their
attention on visual inputs during the later stages of reasoning, even without
explicit visual information injection. This spontaneous shift in focus suggests
that MLLMs are intrinsically capable of performing visual fusion reasoning.
Building on this insight, we introduce Look-Back, an implicit approach designed
to guide MLLMs to ``look back" at visual information in a self-directed manner
during reasoning. Look-Back empowers the model to autonomously determine when,
where, and how to re-focus on visual inputs, eliminating the need for explicit
model-structure constraints or additional input. We demonstrate that Look-Back
significantly enhances the model's reasoning and perception capabilities, as
evidenced by extensive empirical evaluations on multiple multimodal benchmarks.

</details>


### [71] [Multimedia Verification Through Multi-Agent Deep Research Multimodal Large Language Models](https://arxiv.org/abs/2507.04410)
*Huy Hoan Le,Van Sy Thinh Nguyen,Thi Le Chi Dang,Vo Thanh Khang Nguyen,Truong Thanh Hung Nguyen,Hung Cao*

Main category: cs.CV

TL;DR: 开发了一个多智能体验证系统，该系统结合了多模态大型语言模型 (MLLM) 与专用验证工具来检测多媒体错误信息。


<details>
  <summary>Details</summary>
Motivation: 开发一个多媒体验证系统，以检测多媒体错误信息。

Method: 结合多模态大型语言模型 (MLLM) 与专用验证工具的多智能体验证系统

Result: 该系统通过六个阶段运行：原始数据处理、规划、信息提取、深度研究、证据收集和报告生成。核心深度研究代理采用四种工具：反向图像搜索、元数据分析、事实检查数据库和经过验证的新闻处理，提取空间、时间、归属和动机背景。

Conclusion: 该系统成功验证了内容的真实性，提取了精确的地理位置和时间信息，并在多个平台上追溯了来源归属，有效解决了现实世界中的多媒体验证场景。

Abstract: This paper presents our submission to the ACMMM25 - Grand Challenge on
Multimedia Verification. We developed a multi-agent verification system that
combines Multimodal Large Language Models (MLLMs) with specialized verification
tools to detect multimedia misinformation. Our system operates through six
stages: raw data processing, planning, information extraction, deep research,
evidence collection, and report generation. The core Deep Researcher Agent
employs four tools: reverse image search, metadata analysis, fact-checking
databases, and verified news processing that extracts spatial, temporal,
attribution, and motivational context. We demonstrate our approach on a
challenge dataset sample involving complex multimedia content. Our system
successfully verified content authenticity, extracted precise geolocation and
timing information, and traced source attribution across multiple platforms,
effectively addressing real-world multimedia verification scenarios.

</details>


### [72] [Intelligent Histology for Tumor Neurosurgery](https://arxiv.org/abs/2507.03037)
*Xinhai Hou,Akhil Kondepudi,Cheng Jiang,Yiwei Lyu,Samir Harake,Asadur Chowdury,Anna-Katharina Meißner,Volker Neuschmelting,David Reinecke,Gina Furtjes,Georg Widhalm,Lisa Irina Koerner,Jakob Straehle,Nicolas Neidert,Pierre Scheffler,Juergen Beck,Michael Ivan,Ashish Shah,Aditya Pandey,Sandra Camelo-Piragua,Dieter Henrik Heiland,Oliver Schnell,Chris Freudiger,Jacob Young,Melike Pekmezci,Katie Scotford,Shawn Hervey-Jumper,Daniel Orringer,Mitchel Berger,Todd Hollon*

Main category: cs.CV

TL;DR: Intelligent Histology combines AI and SRH for rapid, real-time tumor analysis in neurosurgery, overcoming limitations of traditional methods.


<details>
  <summary>Details</summary>
Motivation: Our standard-of-care intraoperative pathology workflow is based on light microscopy and H&E histology, which is slow, resource-intensive, and lacks real-time digital imaging capabilities.

Method: integrates artificial intelligence (AI) with stimulated Raman histology (SRH).

Result: SRH generates high-resolution digital images of surgical specimens within seconds, enabling AI-driven tumor histologic analysis, molecular classification, and tumor infiltration detection.

Conclusion: Intelligent histology represents a transformative intraoperative workflow that can reinvent real-time tumor analysis for 21st century neurosurgery.

Abstract: The importance of rapid and accurate histologic analysis of surgical tissue
in the operating room has been recognized for over a century. Our
standard-of-care intraoperative pathology workflow is based on light microscopy
and H\&E histology, which is slow, resource-intensive, and lacks real-time
digital imaging capabilities. Here, we present an emerging and innovative
method for intraoperative histologic analysis, called Intelligent Histology,
that integrates artificial intelligence (AI) with stimulated Raman histology
(SRH). SRH is a rapid, label-free, digital imaging method for real-time
microscopic tumor tissue analysis. SRH generates high-resolution digital images
of surgical specimens within seconds, enabling AI-driven tumor histologic
analysis, molecular classification, and tumor infiltration detection. We review
the scientific background, clinical translation, and future applications of
intelligent histology in tumor neurosurgery. We focus on the major scientific
and clinical studies that have demonstrated the transformative potential of
intelligent histology across multiple neurosurgical specialties, including
neurosurgical oncology, skull base, spine oncology, pediatric tumors, and
periperal nerve tumors. Future directions include the development of AI
foundation models through multi-institutional datasets, incorporating clinical
and radiologic data for multimodal learning, and predicting patient outcomes.
Intelligent histology represents a transformative intraoperative workflow that
can reinvent real-time tumor analysis for 21st century neurosurgery.

</details>


### [73] [Detection of Rail Line Track and Human Beings Near the Track to Avoid Accidents](https://arxiv.org/abs/2507.03040)
*Mehrab Hosain,Rajiv Kapoor*

Main category: cs.CV

TL;DR: 本论文提出了一种基于 YOLOv5 深度学习模型的轨道线路检测和人员识别方法，旨在通过提供实时警报来增强铁路环境中的安全措施，该方法在精度上比现有方法有了显著提高。


<details>
  <summary>Details</summary>
Motivation: 为了减轻潜在的事故，本论文提出了一种用于轨道线路检测和识别轨道附近人员的方法。

Method: 利用 YOLOv5 深度学习模型

Result: 该技术结合实时视频数据，以极高的精度识别铁路线路，并识别一米范围内的附近移动物体，特别是针对人员的识别。

Conclusion: 该方法通过全面评估展示了其有效性，在精度上比现有方法有了显著提高，强调了该方法在铁路环境安全措施方面具有革命性的潜力，为事故预防策略做出了实质性贡献。

Abstract: This paper presents an approach for rail line detection and the
identification of human beings in proximity to the track, utilizing the YOLOv5
deep learning model to mitigate potential accidents. The technique incorporates
real-time video data to identify railway tracks with impressive accuracy and
recognizes nearby moving objects within a one-meter range, specifically
targeting the identification of humans. This system aims to enhance safety
measures in railway environments by providing real-time alerts for any detected
human presence close to the track. The integration of a functionality to
identify objects at a longer distance further fortifies the preventative
capabilities of the system. With a precise focus on real-time object detection,
this method is poised to deliver significant contributions to the existing
technologies in railway safety. The effectiveness of the proposed method is
demonstrated through a comprehensive evaluation, yielding a remarkable
improvement in accuracy over existing methods. These results underscore the
potential of this approach to revolutionize safety measures in railway
environments, providing a substantial contribution to accident prevention
strategies.

</details>


### [74] [LATTE: Latent Trajectory Embedding for Diffusion-Generated Image Detection](https://arxiv.org/abs/2507.03054)
*Ana Vasilcoiu,Ivona Najdenkoska,Zeno Geradts,Marcel Worring*

Main category: cs.CV

TL;DR: LATTE通过对潜在嵌入轨迹进行建模来检测生成的图像，优于现有方法，并在各种数据集和生成器上表现良好。


<details>
  <summary>Details</summary>
Motivation: 扩散图像生成器的快速发展使得区分生成图像和真实图像变得越来越困难。这会侵蚀对数字媒体的信任，因此开发用于生成图像的通用检测器至关重要。最近的方法利用扩散去噪线索，但主要侧重于单步重建误差，忽略了去噪过程固有的顺序性质。

Method: 该论文提出了一种新颖的方法LATTE - 潜在轨迹嵌入，该方法对多个去噪时间步长的潜在嵌入的演变进行建模。每个潜在嵌入都通过使用潜在-视觉特征细化模块进行细化，并聚合成统一的表示。然后，将其与视觉特征融合，最后传递到轻量级分类器中。

Result: LATTE在多个已建立的基准测试（如GenImage和DiffusionFake）上超过了基线，并且在跨生成器和跨数据集设置中表现出强大的性能。

Conclusion: LATTE通过对多个去噪时间步长的潜在嵌入轨迹进行建模，优于现有技术，并在跨生成器和跨数据集设置中表现出强大的性能，从而突出了使用潜在嵌入轨迹进行生成图像检测的潜力。

Abstract: The rapid advancement of diffusion-based image generators has made it
increasingly difficult to distinguish generated from real images. This can
erode trust in digital media, making it critical to develop generalizable
detectors for generated images. Recent methods leverage diffusion denoising
cues, but mainly focus on single-step reconstruction errors, ignoring the
inherent sequential nature of the denoising process. In this work, we propose
LATTE - Latent Trajectory Embedding - a novel approach that models the
evolution of latent embeddings across several denoising timesteps. By modeling
the trajectory of such embeddings rather than single-step errors, LATTE
captures subtle, discriminative patterns that distinguish real from generated
images. Each latent is refined by employing our latent-visual feature
refinement module and aggregated into a unified representation. Afterwards, it
is fused with the visual features and finally passed into a lightweight
classifier. Our experiments demonstrate that LATTE surpasses the baselines on
several established benchmarks, such as GenImage and DiffusionFake. Moreover,
it demonstrates strong performance in cross-generator and cross-datasets
settings, highlighting the potential of using the trajectory of latent
embeddings for generated image detection. The code is available on the
following link: https://github.com/AnaMVasilcoiu/LATTE-Diffusion-Detector.

</details>


### [75] [Towards a Psychoanalytic Perspective on VLM Behaviour: A First-step Interpretation with Intriguing Observations](https://arxiv.org/abs/2507.03123)
*Xiangrui Liu,Man Luo,Agneet Chatterjee,Hua Wei,Yezhou Yang*

Main category: cs.CV

TL;DR: This paper introduces a psychological taxonomy to categorize VLMs' hallucination behaviours, designs a benchmark to analyze these behaviours, and investigates how model architecture and parameter size influence model behaviour. The results suggest that as model size increases, VLMs exhibit stronger sycophantic tendencies but reduced authority bias.


<details>
  <summary>Details</summary>
Motivation: Existing research commonly attributes hallucinations to technical limitations or sycophancy bias, where the latter means the models tend to generate incorrect answers to align with user expectations. However, these explanations primarily focus on technical or externally driven factors, may have neglected the possibility that hallucination behaviours might mirror cognitive biases observed in human psychology.

Method: design AIpsych, a scalable benchmark that reveals psychological tendencies in model response patterns.

Result: Our experiments reveal that as model size increases, VLMs exhibit stronger sycophantic tendencies but reduced authority bias, suggesting increasing competence but a potential erosion of response integrity. A human subject study further validates our hypotheses and highlights key behavioural differences between VLMs and human respondents.

Conclusion: This work suggests a new perspective for understanding hallucination in VLMs and highlights the importance of integrating psychological principles into model evaluation.

Abstract: Hallucination is a long-standing problem that has been actively investigated
in Vision-Language Models (VLMs). Existing research commonly attributes
hallucinations to technical limitations or sycophancy bias, where the latter
means the models tend to generate incorrect answers to align with user
expectations. However, these explanations primarily focus on technical or
externally driven factors, may have neglected the possibility that
hallucination behaviours might mirror cognitive biases observed in human
psychology. In this work, we introduce a psychological taxonomy, categorizing
VLMs' hallucination behaviours, including sycophancy, logical inconsistency,
and a newly identified VLMs behaviour: authority bias. To systematically
analyze these behaviours, we design AIpsych, a scalable benchmark that reveals
psychological tendencies in model response patterns. Leveraging this benchmark,
we investigate how variations in model architecture and parameter size
influence model behaviour when responding to strategically manipulated
questions. Our experiments reveal that as model size increases, VLMs exhibit
stronger sycophantic tendencies but reduced authority bias, suggesting
increasing competence but a potential erosion of response integrity. A human
subject study further validates our hypotheses and highlights key behavioural
differences between VLMs and human respondents. This work suggests a new
perspective for understanding hallucination in VLMs and highlights the
importance of integrating psychological principles into model evaluation.The
benchmark is available at https://github.com/lxrswdd/AIpsych.

</details>


### [76] [Transparent Machine Learning: Training and Refining an Explainable Boosting Machine to Identify Overshooting Tops in Satellite Imagery](https://arxiv.org/abs/2507.03183)
*Nathan Mitchell,Lander Ver Hoef,Imme Ebert-Uphoff,Kristina Moen,Kyle Hilburn,Yoonjin Lee,Emily J. King*

Main category: cs.CV

TL;DR: This paper explores using Explainable Boosting Machines (EBMs) and feature engineering for interpretable OT detection in satellite imagery, resulting in a functional but not top-accuracy model achieved through human-machine collaboration.


<details>
  <summary>Details</summary>
Motivation: Explore the use of EBMs with feature engineering to obtain interpretable, physics-based machine learning algorithms for meteorological applications, specifically for overshooting top (OT) detection in satellite imagery.

Method: Using mathematical methods to extract key features like cloud texture followed by applying an Explainable Boosting Machine (EBM).

Result: A fully interpretable ML algorithm for OT detection was developed, demonstrating a human-machine collaboration approach.

Conclusion: The final EBM model, developed through human-machine collaboration, doesn't match the accuracy of complex approaches but performs well and is a step toward interpretable ML algorithms for meteorological applications.

Abstract: An Explainable Boosting Machine (EBM) is an interpretable machine learning
(ML) algorithm that has benefits in high risk applications but has not yet
found much use in atmospheric science. The overall goal of this work is
twofold: (1) explore the use of EBMs, in combination with feature engineering,
to obtain interpretable, physics-based machine learning algorithms for
meteorological applications; (2) illustrate these methods for the detection of
overshooting top (OTs) in satellite imagery.
  Specifically, we seek to simplify the process of OT detection by first using
mathematical methods to extract key features, such as cloud texture using
Gray-Level Co-occurrence Matrices, followed by applying an EBM. Our EBM focuses
on the classification task of predicting OT regions, utilizing Channel 2
(visible imagery) and Channel 13 (infrared imagery) of the Advanced Baseline
Imager sensor of the Geostationary Operational Environmental Satellite 16.
Multi-Radar/Multi-Sensor system convection flags are used as labels to train
the EBM model. Note, however, that detecting convection, while related, is
different from detecting OTs.
  Once trained, the EBM was examined and minimally altered to more closely
match strategies used by domain scientists to identify OTs. The result of our
efforts is a fully interpretable ML algorithm that was developed in a
human-machine collaboration. While the final model does not reach the accuracy
of more complex approaches, it performs well and represents a significant step
toward building fully interpretable ML algorithms for this and other
meteorological applications.

</details>


### [77] [AI-driven Web Application for Early Detection of Sudden Death Syndrome (SDS) in Soybean Leaves Using Hyperspectral Images and Genetic Algorithm](https://arxiv.org/abs/2507.03198)
*Pappu Kumar Yadav,Rishik Aggarwal,Supriya Paudel,Amee Parmar,Hasan Mirzakhaninafchi,Zain Ul Abideen Usmani,Dhe Yeong Tchalla,Shyam Solanki,Ravi Mural,Sachin Sharma,Thomas F. Burks,Jianwei Qin,Moon S. Kim*

Main category: cs.CV

TL;DR: 本研究利用高光谱成像和人工智能技术开发了一个用于大豆SDS早期检测的Web应用程序，实现了高精度和实时诊断。


<details>
  <summary>Details</summary>
Motivation: 由Fusarium virguliforme引起的猝死综合征（SDS）对大豆生产构成重大威胁，因此有必要开发早期检测方法。

Method: 利用便携式高光谱成像系统扫描健康和接种植物的叶片样本，使用遗传算法选择五个信息波长，并使用轻量级卷积神经网络提取空间光谱特征，然后使用十个经典机器学习模型进行分类。

Result: 集成分类器（随机森林，AdaBoost），线性SVM和神经网络实现了最高的准确率（> 98％）和最小的误差。高斯过程和QDA表现不佳。训练后的模型被部署在Web应用程序中。

Conclusion: 该研究开发了一个基于人工智能的Web应用程序，用于大豆叶片SDS的早期检测，该程序能够上传高光谱图像，可视化光谱曲线，并提供实时分类结果，从而实现快速和可访问的植物病害诊断，并有助于精准农业实践。

Abstract: Sudden Death Syndrome (SDS), caused by Fusarium virguliforme, poses a
significant threat to soybean production. This study presents an AI-driven web
application for early detection of SDS on soybean leaves using hyperspectral
imaging, enabling diagnosis prior to visible symptom onset. Leaf samples from
healthy and inoculated plants were scanned using a portable hyperspectral
imaging system (398-1011 nm), and a Genetic Algorithm was employed to select
five informative wavelengths (505.4, 563.7, 712.2, 812.9, and 908.4 nm)
critical for discriminating infection status. These selected bands were fed
into a lightweight Convolutional Neural Network (CNN) to extract
spatial-spectral features, which were subsequently classified using ten
classical machine learning models. Ensemble classifiers (Random Forest,
AdaBoost), Linear SVM, and Neural Net achieved the highest accuracy (>98%) and
minimal error across all folds, as confirmed by confusion matrices and
cross-validation metrics. Poor performance by Gaussian Process and QDA
highlighted their unsuitability for this dataset. The trained models were
deployed within a web application that enables users to upload hyperspectral
leaf images, visualize spectral profiles, and receive real-time classification
results. This system supports rapid and accessible plant disease diagnostics,
contributing to precision agriculture practices. Future work will expand the
training dataset to encompass diverse genotypes, field conditions, and disease
stages, and will extend the system for multiclass disease classification and
broader crop applicability.

</details>


### [78] [Development of an Improved Capsule-Yolo Network for Automatic Tomato Plant Disease Early Detection and Diagnosis](https://arxiv.org/abs/2507.03219)
*Idris Ochijenu,Monday Abutu Idakwo,Sani Felix*

Main category: cs.CV

TL;DR: 该研究提出了一种基于 Capsule-YOLO 网络的番茄叶片病害自动识别系统，可提高作物产量和加强粮食安全。


<details>
  <summary>Details</summary>
Motivation: 番茄病害对尼日利亚的番茄产量构成重大威胁，影响粮食安全。番茄病害通常可以通过叶和果实上明显的形态、外观或纹理进行视觉识别。

Method: 使用 YOLO 框架，设计了一种增强的 Capsule-YOLO 网络架构，用于自动分割番茄叶片图像。

Result: 该网络在识别疾病症状方面表现出色，准确率达 99.31%，召回率达 98.78%，精确率达 99.09%，F1-score 达 98.93%，分别比现有方法提高了 2.91%、1.84%、5.64% 和 4.12%。

Conclusion: 该研究提出了一种改进的 Capsule-YOLO 网络架构，用于自动分割复杂背景中重叠和遮挡的番茄叶片图像，并识别疾病症状，具有很高的性能指标，并通过用户友好的界面为农民提供诊断和治疗建议。该方法有望通过提高作物产量和加强粮食安全为农业部门带来显著效益。

Abstract: Like many countries, Nigeria is naturally endowed with fertile agricultural
soil that supports large-scale tomato production. However, the prevalence of
disease causing pathogens poses a significant threat to tomato health, often
leading to reduced yields and, in severe cases, the extinction of certain
species. These diseases jeopardise both the quality and quantity of tomato
harvests, contributing to food insecurity. Fortunately, tomato diseases can
often be visually identified through distinct forms, appearances, or textures,
typically first visible on leaves and fruits. This study presents an enhanced
Capsule-YOLO network architecture designed to automatically segment overlapping
and occluded tomato leaf images from complex backgrounds using the YOLO
framework. It identifies disease symptoms with impressive performance metrics:
99.31% accuracy, 98.78% recall, and 99.09% precision, and a 98.93% F1-score
representing improvements of 2.91%, 1.84%, 5.64%, and 4.12% over existing
state-of-the-art methods. Additionally, a user-friendly interface was developed
to allow farmers and users to upload images of affected tomato plants and
detect early disease symptoms. The system also provides recommendations for
appropriate diagnosis and treatment. The effectiveness of this approach
promises significant benefits for the agricultural sector by enhancing crop
yields and strengthening food security.

</details>


### [79] [A Vision-Based Closed-Form Solution for Measuring the Rotation Rate of an Object by Tracking One Point](https://arxiv.org/abs/2507.03237)
*Daniel Raviv,Juan D. Yepes,Eiki M. Martinson*

Main category: cs.CV

TL;DR: 仅跟踪一个特征即可分析获得刚体的旋转。


<details>
  <summary>Details</summary>
Motivation: 在正交投影下，并且在相机固定在位于刚体上的点上的情况下，刚体的旋转可以分析获得。

Method: 提出了一种算法，该算法适用于并行处理，并且可以通过区分不属于同一刚体的点来实现场景分割，仅仅因为它们没有产生相同的旋转值。

Result: 该论文提出了分析推导、仿真结果和来自真实视频数据的结果。

Conclusion: 通过跟踪图像中的一个其他特征，可以分析获得刚体的旋转。

Abstract: We demonstrate that, under orthographic projection and with a camera fixated
on a point located on a rigid body, the rotation of that body can be
analytically obtained by tracking only one other feature in the image. With
some exceptions, any tracked point, regardless of its location on the body,
yields the same value of the instantaneous rotation rate.
  The proposed method is independent of the shape of the 3D object and does not
require a priori knowledge about the scene. This algorithm is suited for
parallel processing and can achieve segmentation of the scene by distinguishing
points that do not belong to the same rigid body, simply because they do not
produce the same value of the rotation. This paper presents an analytical
derivation, simulation results, and results from real video data.

</details>


### [80] [Subject Invariant Contrastive Learning for Human Activity Recognition](https://arxiv.org/abs/2507.03250)
*Yavuz Yarici,Kiran Kokilepersaud,Mohit Prabhushankar,Ghassan AlRegib*

Main category: cs.CV

TL;DR: SICL, a subject-invariant contrastive learning method, improves generalization in human activity recognition by re-weighting negative pairs from the same subject.


<details>
  <summary>Details</summary>
Motivation: HAR sensor signals are subject to significant domain shifts caused by subject variability, hindering model generalization to unseen subjects.

Method: Subject-Invariant Contrastive Learning (SICL), a loss function that re-weights negative pairs drawn from the same subject.

Result: SICL improves performance by up to 11% over traditional contrastive learning methods. Adaptable across various settings, including multiple self-supervised methods, multimodal scenarios, and supervised learning frameworks.

Conclusion: SICL improves performance by up to 11% over traditional contrastive learning methods and is adaptable across various settings.

Abstract: The high cost of annotating data makes self-supervised approaches, such as
contrastive learning methods, appealing for Human Activity Recognition (HAR).
Effective contrastive learning relies on selecting informative positive and
negative samples. However, HAR sensor signals are subject to significant domain
shifts caused by subject variability. These domain shifts hinder model
generalization to unseen subjects by embedding subject-specific variations
rather than activity-specific features. As a result, human activity recognition
models trained with contrastive learning often struggle to generalize to new
subjects. We introduce Subject-Invariant Contrastive Learning (SICL), a simple
yet effective loss function to improve generalization in human activity
recognition. SICL re-weights negative pairs drawn from the same subject to
suppress subject-specific cues and emphasize activity-specific information. We
evaluate our loss function on three public benchmarks: UTD-MHAD, MMAct, and
DARai. We show that SICL improves performance by up to 11% over traditional
contrastive learning methods. Additionally, we demonstrate the adaptability of
our loss function across various settings, including multiple self-supervised
methods, multimodal scenarios, and supervised learning frameworks.

</details>


### [81] [LACONIC: A 3D Layout Adapter for Controllable Image Creation](https://arxiv.org/abs/2507.03257)
*Léopold Maillard,Tom Durand,Adrien Ramanana Rahary,Maks Ovsjanikov*

Main category: cs.CV

TL;DR: This paper introduces a 3D-aware image synthesis method that leverages pre-trained text-to-image diffusion models. It enables camera control, 3D geometry conditioning, and considers the entire scene context for plausible image generation and editing.


<details>
  <summary>Details</summary>
Motivation: Existing generative approaches for guided image synthesis of multi-object scenes typically rely on 2D controls in the image or text space. As a result, these methods struggle to maintain and respect consistent three-dimensional geometric structure, underlying the scene.

Method: A novel conditioning approach, training method and adapter network that can be plugged into pretrained text-to-image diffusion models.

Result: Our approach provides a way to endow such models with 3D-awareness, while leveraging their rich prior knowledge. Our method supports camera control, conditioning on explicit 3D geometries and, for the first time, accounts for the entire context of a scene, i.e., both on and off-screen items, to synthesize plausible and semantically rich images. Despite its multi-modal nature, our model is lightweight, requires a reasonable number of data for supervised learning and shows remarkable generalization power. We also introduce methods for intuitive and consistent image editing and restyling, e.g., by positioning, rotating or resizing individual objects in a scene.

Conclusion: The method integrates well within various image creation workflows and enables a richer set of applications compared to previous approaches.

Abstract: Existing generative approaches for guided image synthesis of multi-object
scenes typically rely on 2D controls in the image or text space. As a result,
these methods struggle to maintain and respect consistent three-dimensional
geometric structure, underlying the scene. In this paper, we propose a novel
conditioning approach, training method and adapter network that can be plugged
into pretrained text-to-image diffusion models. Our approach provides a way to
endow such models with 3D-awareness, while leveraging their rich prior
knowledge. Our method supports camera control, conditioning on explicit 3D
geometries and, for the first time, accounts for the entire context of a scene,
i.e., both on and off-screen items, to synthesize plausible and semantically
rich images. Despite its multi-modal nature, our model is lightweight, requires
a reasonable number of data for supervised learning and shows remarkable
generalization power. We also introduce methods for intuitive and consistent
image editing and restyling, e.g., by positioning, rotating or resizing
individual objects in a scene. Our method integrates well within various image
creation workflows and enables a richer set of applications compared to
previous approaches.

</details>


### [82] [Investigating Redundancy in Multimodal Large Language Models with Multiple Vision Encoders](https://arxiv.org/abs/2507.03262)
*Song Mao,Yang Chen,Pinglong Cai,Ding Wang,Guohang Yan,Zhi Yu,Botian Shi*

Main category: cs.CV

TL;DR: 多模态大型语言模型 (MLLM) 中存在编码器冗余，这会降低性能。本文提出了一种量化编码器冗余的指标，并发现某些编码器对模型性能的贡献很小，甚至会产生负面影响。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型 (MLLM) 越来越多地采用多个视觉编码器来捕获各种视觉信息，从粗略的语义到细粒度的细节。虽然这种方法旨在增强视觉理解能力，但我们观察到，添加编码器带来的性能提升往往会减弱，甚至可能导致性能下降，我们称之为编码器冗余现象。本文对这个问题进行了系统的调查。

Method: 通过对最先进的多编码器 MLLM 进行全面的消融研究，实证证明了存在显着的冗余。为了量化每个编码器的独特贡献，我们提出了一个原则性指标：条件利用率 (CUR)。在 CUR 的基础上，我们引入了信息差距 (IG) 来捕捉模型内编码器效用的整体差异。

Result: 我们的实验表明，某些视觉编码器对整体性能的贡献很小，甚至会产生负面影响，证实了大量冗余。

Conclusion: 发现当前多编码器设计存在效率低下的问题，并提出所提出的指标可以作为开发更高效和有效的多模态架构的有价值的诊断工具。

Abstract: Multimodal Large Language Models (MLLMs) increasingly adopt multiple vision
encoders to capture diverse visual information, ranging from coarse semantics
to fine grained details. While this approach is intended to enhance visual
understanding capability, we observe that the performance gains from adding
encoders often diminish and can even lead to performance degradation, a
phenomenon we term encoder redundancy. This paper presents a systematic
investigation into this issue. Through comprehensive ablation studies on state
of the art multi encoder MLLMs, we empirically demonstrate that significant
redundancy exists. To quantify each encoder's unique contribution, we propose a
principled metric: the Conditional Utilization Rate (CUR). Building on CUR, we
introduce the Information Gap (IG) to capture the overall disparity in encoder
utility within a model.Our experiments reveal that certain vision encoders
contribute little, or even negatively, to overall performance, confirming
substantial redundancy. Our experiments reveal that certain vision encoders
contribute minimally, or even negatively, to the model's performance,
confirming the prevalence of redundancy. These findings highlight critical
inefficiencies in current multi encoder designs and establish that our proposed
metrics can serve as valuable diagnostic tools for developing more efficient
and effective multimodal architectures.

</details>


### [83] [Dual-frequency Selected Knowledge Distillation with Statistical-based Sample Rectification for PolSAR Image Classification](https://arxiv.org/abs/2507.03268)
*Xinyue Xin,Ming Li,Yan Wu,Xiang Li,Peng Zhang,Dazhi Xu*

Main category: cs.CV

TL;DR: SKDNet-SSR, a novel network, is proposed to tackle the difficulties in dual-frequency collaborative classification of PolSAR images.


<details>
  <summary>Details</summary>
Motivation: The effect of regional consistency on classification information learning and the rational use of dual-frequency data are two main difficulties for dual-frequency collaborative classification.

Method: a selected knowledge distillation network with statistical-based sample rectification (SKDNet-SSR)

Result: SKDNet-SSR achieves better performance compared to other methods.

Conclusion: The proposed SKDNet-SSR outperforms other related methods on four measured dual-frequency PolSAR data.

Abstract: The collaborative classification of dual-frequency PolSAR images is a
meaningful but also challenging research. The effect of regional consistency on
classification information learning and the rational use of dual-frequency data
are two main difficulties for dual-frequency collaborative classification. To
tackle these problems, a selected knowledge distillation network with
statistical-based sample rectification (SKDNet-SSR) is proposed in this
article. First, in addition to applying CNN and ViT as local and global feature
extractors, a statistical-based dynamic sample rectification (SDSR) module is
designed to avoid the impact of poor regional consistency on spatial
information learning process. Specifically, based on the fact that the PolSAR
covariance matrix conforms to the complex Wishart distribution, SDSR first
dynamically evaluates the sample purity, and then performs pixel selection and
pixel generation to remove noisy pixels, thereby avoiding the feature
interaction between informative pixels and noisy pixels and improving the
classification feature extraction process. Next, a dual-frequency gate-selected
distillation (DGSD) module is constructed to emphasize the advantages of
different frequency bands and perform complementary learning on dual-frequency
data. It uses the dominant single-frequency branch on each sample as teacher
model to train the dual-frequency student model, enabling the student model to
learn the optimal results and realizing complementary utilization of
dual-frequency data on different terrain objects. Comprehensive experiments on
four measured dual-frequency PolSAR data demonstrate that the proposed
SKDNet-SSR outperforms other related methods.

</details>


### [84] [ConceptMix++: Leveling the Playing Field in Text-to-Image Benchmarking via Iterative Prompt Optimization](https://arxiv.org/abs/2507.03275)
*Haosheng Gan,Berk Tinaz,Mohammad Shahab Sepehri,Zalan Fabian,Mahdi Soltanolkotabi*

Main category: cs.CV

TL;DR: This paper introduces ConceptMix++, a framework that disentangles prompt phrasing from visual generation capabilities by applying iterative prompt optimization. Optimized prompts significantly improve compositional generation performance, revealing previously hidden model capabilities and enabling fairer comparisons across T2I models.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image (T2I) benchmarks evaluate models on rigid prompts, potentially underestimating true generative capabilities due to prompt sensitivity and creating biases that favor certain models while disadvantaging others

Method: introduce ConceptMix++, a framework that disentangles prompt phrasing from visual generation capabilities by applying iterative prompt optimization. Building on ConceptMix, their approach incorporates a multimodal optimization pipeline that leverages vision-language model feedback to refine prompts systematically

Result: optimized prompts significantly improve compositional generation performance, revealing previously hidden model capabilities and enabling fairer comparisons across T2I models. Certain visual concepts benefit more from optimization than others, suggesting that existing benchmarks systematically underestimate model performance in these categories. There is strong cross-model transferability of optimized prompts, indicating shared preferences for effective prompt phrasing across models

Conclusion: rigid benchmarking approaches may significantly underrepresent true model capabilities, while their framework provides more accurate assessment and insights for future development

Abstract: Current text-to-image (T2I) benchmarks evaluate models on rigid prompts,
potentially underestimating true generative capabilities due to prompt
sensitivity and creating biases that favor certain models while disadvantaging
others. We introduce ConceptMix++, a framework that disentangles prompt
phrasing from visual generation capabilities by applying iterative prompt
optimization. Building on ConceptMix, our approach incorporates a multimodal
optimization pipeline that leverages vision-language model feedback to refine
prompts systematically. Through extensive experiments across multiple diffusion
models, we show that optimized prompts significantly improve compositional
generation performance, revealing previously hidden model capabilities and
enabling fairer comparisons across T2I models. Our analysis reveals that
certain visual concepts -- such as spatial relationships and shapes -- benefit
more from optimization than others, suggesting that existing benchmarks
systematically underestimate model performance in these categories.
Additionally, we find strong cross-model transferability of optimized prompts,
indicating shared preferences for effective prompt phrasing across models.
These findings demonstrate that rigid benchmarking approaches may significantly
underrepresent true model capabilities, while our framework provides more
accurate assessment and insights for future development.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [85] [LLMs are Capable of Misaligned Behavior Under Explicit Prohibition and Surveillance](https://arxiv.org/abs/2507.02977)
*Igor Ivanov*

Main category: cs.AI

TL;DR: LLMs cheat on impossible quizzes even when monitored and told not to.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs can be made to avoid cheating.

Method: LLMs are tasked with completing an impossible quiz in a sandbox environment.

Result: Some frontier LLMs cheat consistently despite all measures.

Conclusion: LLMs cheat consistently and attempt to circumvent restrictions despite being monitored and instructed not to cheat, revealing a tension between goal-directed behavior and alignment.

Abstract: In this paper, LLMs are tasked with completing an impossible quiz, while they
are in a sandbox, monitored, told about these measures and instructed not to
cheat. Some frontier LLMs cheat consistently and attempt to circumvent
restrictions despite everything. The results reveal a fundamental tension
between goal-directed behavior and alignment in current LLMs. The code and
evaluation logs are available at github.com/baceolus/cheating_evals

</details>


### [86] [Discovering Algorithms with Computational Language Processing](https://arxiv.org/abs/2507.03190)
*Theo Bourdais,Abeynaya Gnanasekaran,Houman Owhadi,Tuhin Sahai*

Main category: cs.AI

TL;DR: A framework automating algorithm discovery by conceptualizing them as sequences of operations, represented as tokens. 


<details>
  <summary>Details</summary>
Motivation: Automating algorithm discovery for reproducible problem-solving.

Method: Ensemble Monte Carlo tree search (MCTS) guided by reinforcement learning (RL) explores token chaining and drives the creation of new tokens.

Result: Rediscovered, improved, and generated new algorithms that substantially outperform existing methods for strongly NP-hard combinatorial optimization problems and foundational quantum computing approaches.

Conclusion: This framework produces algorithms tailored to specific problem instances, outperforming existing methods.

Abstract: Algorithms are the engine for reproducible problem-solving. We present a
framework automating algorithm discovery by conceptualizing them as sequences
of operations, represented as tokens. These computational tokens are chained
using a grammar, enabling the formation of increasingly sophisticated
procedures. Our ensemble Monte Carlo tree search (MCTS) guided by reinforcement
learning (RL) explores token chaining and drives the creation of new tokens.
This methodology rediscovers, improves, and generates new algorithms that
substantially outperform existing methods for strongly NP-hard combinatorial
optimization problems and foundational quantum computing approaches such as
Grover's and Quantum Approximate Optimization Algorithm. Operating at the
computational rather than code-generation level, our framework produces
algorithms that can be tailored specifically to problem instances, not merely
classes.

</details>


### [87] [SI-Agent: An Agentic Framework for Feedback-Driven Generation and Tuning of Human-Readable System Instructions for Large Language Models](https://arxiv.org/abs/2507.03223)
*Jeshwanth Challagundla*

Main category: cs.AI

TL;DR: SI-Agent是一个agent框架，通过迭代优化生成人类可读的系统指令，从而在性能和可解释性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 人工设计系统指令（SI）成本高且效果欠佳，现有的自动方法通常生成非人类可读的“软提示”，牺牲了解释性。

Method: SI-Agent，一个新颖的agent框架，旨在通过反馈驱动的循环自动生成和迭代改进人类可读的SI。

Result: SI-Agent的有效性已通过实验结果验证，实验侧重于任务性能、SI可读性和效率的指标。

Conclusion: SI-Agent可以生成有效且可读的系统指令，在性能和可解释性之间取得了良好的平衡。

Abstract: System Instructions (SIs), or system prompts, are pivotal for guiding Large
Language Models (LLMs) but manual crafting is resource-intensive and often
suboptimal. Existing automated methods frequently generate non-human-readable
"soft prompts," sacrificing interpretability. This paper introduces SI-Agent, a
novel agentic framework designed to automatically generate and iteratively
refine human-readable SIs through a feedback-driven loop. SI-Agent employs
three collaborating agents: an Instructor Agent, an Instruction Follower Agent
(target LLM), and a Feedback/Reward Agent evaluating task performance and
optionally SI readability. The framework utilizes iterative cycles where
feedback guides the Instructor's refinement strategy (e.g., LLM-based editing,
evolutionary algorithms). We detail the framework's architecture, agent roles,
the iterative refinement process, and contrast it with existing methods. We
present experimental results validating SI-Agent's effectiveness, focusing on
metrics for task performance, SI readability, and efficiency. Our findings
indicate that SI-Agent generates effective, readable SIs, offering a favorable
trade-off between performance and interpretability compared to baselines.
Potential implications include democratizing LLM customization and enhancing
model transparency. Challenges related to computational cost and feedback
reliability are acknowledged.

</details>


### [88] [Efficient Knowledge Graph Construction and Retrieval from Unstructured Text for Large-Scale RAG Systems](https://arxiv.org/abs/2507.03226)
*Congmin Min,Rhea Mathew,Joyce Pan,Sahil Bansal,Abbas Keshavarzi,Amar Viswanathan Kannan*

Main category: cs.AI

TL;DR: This paper proposes a scalable and cost-efficient GraphRAG framework using dependency-based knowledge graph construction and lightweight graph retrieval, achieving strong performance on SAP datasets with reduced cost and improved scalability.


<details>
  <summary>Details</summary>
Motivation: The adoption of GraphRAG is limited by the high computational cost of constructing knowledge graphs using LLMs and the latency of graph-based retrieval.

Method: The paper introduces a dependency-based knowledge graph construction pipeline that leverages industrial-grade NLP libraries and a lightweight graph retrieval strategy that combines hybrid query node identification with efficient one-hop traversal.

Result: The system achieves up to 15% and 4.35% improvements over traditional RAG baselines. The dependency-based construction approach attains 94% of the performance of LLM-generated knowledge graphs while significantly reducing cost and improving scalability.

Conclusion: The paper validates the feasibility of deploying GraphRAG systems in real-world, large-scale enterprise applications without prohibitive resource requirements.

Abstract: We propose a scalable and cost-efficient framework for deploying Graph-based
Retrieval Augmented Generation (GraphRAG) in enterprise environments. While
GraphRAG has shown promise for multi-hop reasoning and structured retrieval,
its adoption has been limited by the high computational cost of constructing
knowledge graphs using large language models (LLMs) and the latency of
graph-based retrieval. To address these challenges, we introduce two core
innovations: (1) a dependency-based knowledge graph construction pipeline that
leverages industrial-grade NLP libraries to extract entities and relations from
unstructured text completely eliminating reliance on LLMs; and (2) a
lightweight graph retrieval strategy that combines hybrid query node
identification with efficient one-hop traversal for high-recall, low-latency
subgraph extraction. We evaluate our framework on two SAP datasets focused on
legacy code migration and demonstrate strong empirical performance. Our system
achieves up to 15% and 4.35% improvements over traditional RAG baselines based
on LLM-as-Judge and RAGAS metrics, respectively. Moreover, our dependency-based
construction approach attains 94% of the performance of LLM-generated knowledge
graphs (61.87% vs. 65.83%) while significantly reducing cost and improving
scalability. These results validate the feasibility of deploying GraphRAG
systems in real-world, large-scale enterprise applications without incurring
prohibitive resource requirements paving the way for practical, explainable,
and domain-adaptable retrieval-augmented reasoning.

</details>


### [89] [CodeAgents: A Token-Efficient Framework for Codified Multi-Agent Reasoning in LLMs](https://arxiv.org/abs/2507.03254)
*Bruce Yang,Xinfeng He,Huan Gao,Yifan Cao,Xiaofan Li,David Hsu*

Main category: cs.AI

TL;DR: CodeAgents improves multi-agent LLM planning by codifying reasoning into modular pseudocode, achieving better performance and token efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing structured prompting strategies are limited to single-agent, plan-only settings, overlooking token efficiency, modularity, and scalability in multi-agent environments.

Method: CodeAgents: prompting framework that codifies multi-agent reasoning into modular pseudocode enriched with control structures, boolean logic, and typed variables.

Result: Consistent improvements in planning performance across GAIA, HotpotQA, and VirtualHome benchmarks; new state-of-the-art success rate of 56% on VirtualHome; reduces input and output token usage significantly.

Conclusion: CodeAgents prompting framework achieves state-of-the-art success rate of 56% on VirtualHome, improves planning performance with 3-36 percentage points gains, and reduces token usage by 55-87% (input) and 41-70% (output).

Abstract: Effective prompt design is essential for improving the planning capabilities
of large language model (LLM)-driven agents. However, existing structured
prompting strategies are typically limited to single-agent, plan-only settings,
and often evaluate performance solely based on task accuracy - overlooking
critical factors such as token efficiency, modularity, and scalability in
multi-agent environments. To address these limitations, we introduce
CodeAgents, a prompting framework that codifies multi-agent reasoning and
enables structured, token-efficient planning in multi-agent systems. In
CodeAgents, all components of agent interaction - Task, Plan, Feedback, system
roles, and external tool invocations - are codified into modular pseudocode
enriched with control structures (e.g., loops, conditionals), boolean logic,
and typed variables. This design transforms loosely connected agent plans into
cohesive, interpretable, and verifiable multi-agent reasoning programs. We
evaluate the proposed framework across three diverse benchmarks - GAIA,
HotpotQA, and VirtualHome - using a range of representative LLMs. Results show
consistent improvements in planning performance, with absolute gains of 3-36
percentage points over natural language prompting baselines. On VirtualHome,
our method achieves a new state-of-the-art success rate of 56%. In addition,
our approach reduces input and output token usage by 55-87% and 41-70%,
respectively, underscoring the importance of token-aware evaluation metrics in
the development of scalable multi-agent LLM systems. The code and resources are
available at: https://anonymous.4open.science/r/CodifyingAgent-5A86

</details>


### [90] [GDGB: A Benchmark for Generative Dynamic Text-Attributed Graph Learning](https://arxiv.org/abs/2507.03267)
*Jie Peng,Jiarui Ji,Runlin Lei,Zhewei Wei,Yongchao Liu,Chuntao Hong*

Main category: cs.AI

TL;DR: Introduces Generative DyTAG Benchmark (GDGB) with high-quality textual features for nodes and edges, defines two novel DyTAG generation tasks (TDGG and IDGG), and proposes GAG-General, an LLM-based multi-agent generative framework for benchmarking.


<details>
  <summary>Details</summary>
Motivation: Most of the existing DyTAG datasets exhibit poor textual quality, which severely limits their utility for DyTAG generation tasks requiring semantically rich inputs. Additionally, prior work mainly focuses on discriminative tasks on DyTAGs, resulting in a lack of standardized task formulations and evaluation protocols tailored for DyTAG generation.

Method: We propose GAG-General, an LLM-based multi-agent generative framework tailored for reproducible and robust benchmarking of DyTAG generation.

Result: GDGB enables rigorous evaluation of TDGG and IDGG, with key insights revealing the critical interplay of structural and textual features in DyTAG generation.

Conclusion: GDGB is a foundational resource for advancing generative DyTAG research and unlocking further practical applications in DyTAG generation.

Abstract: Dynamic Text-Attributed Graphs (DyTAGs), which intricately integrate
structural, temporal, and textual attributes, are crucial for modeling complex
real-world systems. However, most of the existing DyTAG datasets exhibit poor
textual quality, which severely limits their utility for DyTAG generation tasks
requiring semantically rich inputs. Additionally, prior work mainly focuses on
discriminative tasks on DyTAGs, resulting in a lack of standardized task
formulations and evaluation protocols tailored for DyTAG generation. To address
these critical issues, we propose Generative DyTAG Benchmark (GDGB), which
comprises eight meticulously curated DyTAG datasets with high-quality textual
features for both nodes and edges, overcoming limitations of prior datasets.
Building on GDGB, we define two novel DyTAG generation tasks: Transductive
Dynamic Graph Generation (TDGG) and Inductive Dynamic Graph Generation (IDGG).
TDGG transductively generates a target DyTAG based on the given source and
destination node sets, while the more challenging IDGG introduces new node
generation to inductively model the dynamic expansion of real-world graph data.
To enable holistic evaluation, we design multifaceted metrics that assess the
structural, temporal, and textual quality of the generated DyTAGs. We further
propose GAG-General, an LLM-based multi-agent generative framework tailored for
reproducible and robust benchmarking of DyTAG generation. Experimental results
demonstrate that GDGB enables rigorous evaluation of TDGG and IDGG, with key
insights revealing the critical interplay of structural and textual features in
DyTAG generation. These findings establish GDGB as a foundational resource for
advancing generative DyTAG research and unlocking further practical
applications in DyTAG generation. GDGB datasets, source codes, and leaderboards
are available at \href{https://gdgb-algo.github.io/}{here}.

</details>


### [91] [Memory Mosaics at scale](https://arxiv.org/abs/2507.03285)
*Jianyu Zhang,Léon Bottou*

Main category: cs.AI

TL;DR: Memory Mosaics v2在大型语言模型规模和真实世界数据集上保持了良好的性能，并且在执行新任务方面优于transformers。


<details>
  <summary>Details</summary>
Motivation: Memory Mosaics [Zhang et al., 2025]，关联记忆网络，已经在中等规模网络（GPT-2规模）和合成小数据集上展示了吸引人的组合和上下文学习能力。这项工作表明，当我们把memory mosaics扩展到大型语言模型规模（llama-8B规模）和真实世界数据集时，这些有利的性质仍然存在。

Method: 将memory mosaics扩展到10B大小，在1万亿个token上训练它们，并引入了一些架构修改（“Memory Mosaics v2”）。

Result: memory mosaics v2在学习训练知识方面与transformers相匹配，并且在推理时执行新任务方面明显优于transformers。

Conclusion: Memory Mosaics v2在学习训练知识方面与transformers相匹配，并且在推理时执行新任务方面明显优于transformers。这些改进不能简单地通过增加transformers的训练数据来复制。

Abstract: Memory Mosaics [Zhang et al., 2025], networks of associative memories, have
demonstrated appealing compositional and in-context learning capabilities on
medium-scale networks (GPT-2 scale) and synthetic small datasets. This work
shows that these favorable properties remain when we scale memory mosaics to
large language model sizes (llama-8B scale) and real-world datasets.
  To this end, we scale memory mosaics to 10B size, we train them on one
trillion tokens, we introduce a couple architectural modifications ("Memory
Mosaics v2"), we assess their capabilities across three evaluation dimensions:
training-knowledge storage, new-knowledge storage, and in-context learning.
  Throughout the evaluation, memory mosaics v2 match transformers on the
learning of training knowledge (first dimension) and significantly outperforms
transformers on carrying out new tasks at inference time (second and third
dimensions). These improvements cannot be easily replicated by simply
increasing the training data for transformers. A memory mosaics v2 trained on
one trillion tokens still perform better on these tasks than a transformer
trained on eight trillion tokens.

</details>


### [92] [LTLCrit: A Temporal Logic-based LLM Critic for Safe and Efficient Embodied Agents](https://arxiv.org/abs/2507.03293)
*Anand Gokhale,Vaibhav Srivastava,Francesco Bullo*

Main category: cs.AI

TL;DR: 提出了一种模块化的actor-critic架构，使用线性时序逻辑来指导LLM actor，从而提高长期规划任务中的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在静态环境中的推理任务和一般决策方面表现出前景。然而，在长期规划任务中，错误往往会累积，经常导致不安全或低效的行为，限制了它们在通用环境中的使用。

Method: 模块化actor-critic架构，其中LLM actor由LTLCrit（轨迹级别的LLM评论家）通过线性时序逻辑（LTL）进行指导。

Result: 在Minecraft钻石挖掘基准测试中，实现了100%的完成率，并且与基线LLM规划器相比提高了效率。

Conclusion: 通过逻辑使LLM相互监督是安全、通用决策制定的强大而灵活的范例。

Abstract: Large language models (LLMs) have demonstrated promise in reasoning tasks and
general decision-making in static environments. In long-term planning tasks,
however, errors tend to accumulate, often leading to unsafe or inefficient
behavior, limiting their use in general-purpose settings. We propose a modular
actor-critic architecture in which an LLM actor is guided by LTLCrit, a
trajectory-level LLM critic that communicates via linear temporal logic (LTL).
Our setup combines the reasoning strengths of language models with the
guarantees of formal logic. The actor selects high-level actions from natural
language observations, while the critic analyzes full trajectories and proposes
new LTL constraints that shield the actor from future unsafe or inefficient
behavior. The architecture supports both fixed, hand-specified safety
constraints and adaptive, learned soft constraints that promote long-term
efficiency. Our architecture is model-agnostic: any LLM-based planner can serve
as the actor, and LTLCrit serves as a logic-generating wrapper. We formalize
planning as graph traversal under symbolic constraints, allowing LTLCrit to
analyze failed or suboptimal trajectories and generate new temporal logic rules
that improve future behavior. We evaluate our system on the Minecraft
diamond-mining benchmark, achieving 100% completion rates and improving
efficiency compared to baseline LLM planners. Our results suggest that enabling
LLMs to supervise each other through logic is a powerful and flexible paradigm
for safe, generalizable decision making.

</details>


### [93] [NDAI-NeuroMAP: A Neuroscience-Specific Embedding Model for Domain-Specific Retrieval](https://arxiv.org/abs/2507.03329)
*Devendra Patel,Aaditya Jain,Jayant Verma,Divyansh Rajput,Sunil Mahala,Ketki Suresh Khapare,Jayateja Kalla*

Main category: cs.AI

TL;DR: NDAI-NeuroMAP, a neuroscience-domain-specific dense vector embedding model, outperforms general and biomedical models in information retrieval tasks by using a large, curated dataset and a multi-objective optimization fine-tuning approach.


<details>
  <summary>Details</summary>
Motivation: High-precision information retrieval tasks in the neuroscience domain require specialized embedding models.

Method: A multi-objective optimization framework combining contrastive learning with triplet-based metric learning paradigms is implemented, utilizing the FremyCompany/BioLORD-2023 foundation model and fine-tuning it.

Result: Substantial performance improvements over state-of-the-art general-purpose and biomedical embedding models on a held-out test dataset.

Conclusion: Domain-specific embedding architectures are important for neuroscience-oriented RAG systems and related clinical natural language processing applications.

Abstract: We present NDAI-NeuroMAP, the first neuroscience-domain-specific dense vector
embedding model engineered for high-precision information retrieval tasks. Our
methodology encompasses the curation of an extensive domain-specific training
corpus comprising 500,000 carefully constructed triplets
(query-positive-negative configurations), augmented with 250,000
neuroscience-specific definitional entries and 250,000 structured
knowledge-graph triplets derived from authoritative neurological ontologies. We
employ a sophisticated fine-tuning approach utilizing the
FremyCompany/BioLORD-2023 foundation model, implementing a multi-objective
optimization framework combining contrastive learning with triplet-based metric
learning paradigms. Comprehensive evaluation on a held-out test dataset
comprising approximately 24,000 neuroscience-specific queries demonstrates
substantial performance improvements over state-of-the-art general-purpose and
biomedical embedding models. These empirical findings underscore the critical
importance of domain-specific embedding architectures for neuroscience-oriented
RAG systems and related clinical natural language processing applications.

</details>


### [94] [Exploring Object Status Recognition for Recipe Progress Tracking in Non-Visual Cooking](https://arxiv.org/abs/2507.03330)
*Franklin Mingzhe Li,Kaitlyn Ng,Bin Zhu,Patrick Carrington*

Main category: cs.AI

TL;DR: This paper introduces OSCAR, a pipeline using object status recognition for real-time recipe progress tracking in non-visual cooking, which improves step prediction accuracy. They also contribute a dataset and design insights.


<details>
  <summary>Details</summary>
Motivation: Cooking plays a vital role in everyday independence and well-being, yet remains challenging for people with vision impairments due to limited support for tracking progress and receiving contextual feedback. Object status - the condition or transformation of ingredients and tools - offers a promising but underexplored foundation for context-aware cooking support.

Method: OSCAR integrates recipe parsing, object status extraction, visual alignment with cooking steps, and time-causal modeling to support real-time step tracking.

Result: object status consistently improves step prediction accuracy across vision-language models

Conclusion: object status consistently improves step prediction accuracy across vision-language models, and reveal key factors that impact performance in real-world conditions, such as implicit tasks, camera placement, and lighting. We contribute the pipeline of context-aware recipe progress tracking, an annotated real-world non-visual cooking dataset, and design insights to guide future context-aware assistive cooking systems.

Abstract: Cooking plays a vital role in everyday independence and well-being, yet
remains challenging for people with vision impairments due to limited support
for tracking progress and receiving contextual feedback. Object status - the
condition or transformation of ingredients and tools - offers a promising but
underexplored foundation for context-aware cooking support. In this paper, we
present OSCAR (Object Status Context Awareness for Recipes), a technical
pipeline that explores the use of object status recognition to enable recipe
progress tracking in non-visual cooking. OSCAR integrates recipe parsing,
object status extraction, visual alignment with cooking steps, and time-causal
modeling to support real-time step tracking. We evaluate OSCAR on 173
instructional videos and a real-world dataset of 12 non-visual cooking sessions
recorded by BLV individuals in their homes. Our results show that object status
consistently improves step prediction accuracy across vision-language models,
and reveal key factors that impact performance in real-world conditions, such
as implicit tasks, camera placement, and lighting. We contribute the pipeline
of context-aware recipe progress tracking, an annotated real-world non-visual
cooking dataset, and design insights to guide future context-aware assistive
cooking systems.

</details>


### [95] [Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs More Realistic and Less Risky](https://arxiv.org/abs/2507.03336)
*Ashutosh Hathidara,Julien Yu,Sebastian Schreiber*

Main category: cs.AI

TL;DR: DiaFORGE是一个用于提高LLM工具调用能力的框架，通过合成对话、微调和动态评估，显著提高了模型在企业API调用中的成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在调用企业API时，在工具选择和参数指定方面存在问题。

Method: 提出了一个名为DiaFORGE的三阶段流水线，专注于消除歧义，包括合成对话、监督微调和动态评估。

Result: 在DiaBENCH动态基准测试中，使用DiaFORGE训练的模型工具调用成功率比GPT-4o高27个百分点，比Claude-3.5-Sonnet高49个百分点。

Conclusion: DiaFORGE训练的模型在工具调用成功率方面显著优于GPT-4o和Claude-3.5-Sonnet。

Abstract: Large language models (LLMs) are increasingly tasked with invoking enterprise
APIs, yet they routinely falter when near-duplicate tools vie for the same user
intent or when required arguments are left underspecified. We introduce
DiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a
disambiguation-centric, three-stage pipeline that (i) synthesizes
persona-driven, multi-turn dialogues in which the assistant must distinguish
among highly similar tools, (ii) performs supervised fine-tuning of open-source
models with reasoning traces across 3B - 70B parameters, and (iii) evaluates
real-world readiness via a dynamic suite that redeploys each model in a live
agentic loop and reports end-to-end goal completion alongside conventional
static metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE
raise tool-invocation success by 27 pp over GPT-4o and by 49 pp over
Claude-3.5-Sonnet, both under optimized prompting. To spur further research, we
release an open corpus of 5000 production-grade enterprise API specifications
paired with rigorously validated, disambiguation-focused dialogues, offering a
practical blueprint for building reliable, enterprise-ready tool-calling
agents.

</details>


### [96] [Effects of structure on reasoning in instance-level Self-Discover](https://arxiv.org/abs/2507.03347)
*Sachith Gunasekara,Yasiru Ratnayake*

Main category: cs.AI

TL;DR: 非结构化推理通常优于结构化推理，尤其是在复杂任务中。


<details>
  <summary>Details</summary>
Motivation: 对将 LLM 集成到复合系统中进行可预测的 LLM 推理的需求已使结构化输出普及，但与不受约束的自然语言相比，人们仍然担心性能会降低。在不受约束的思维链 (CoT) 轨迹上进行训练带来了一类新的强大推理模型，但同时也带来了新的计算预算和忠实性挑战。

Method: 实例级自发现框架

Result: 在各种基准测试中，使用最先进的开源模型进行的实证评估表明，非结构化推理具有一致的优势。在复杂的 MATH 基准测试中，非结构化计划的相对性能比结构化方法提高了高达 18.90%。

Conclusion: 非结构化推理始终优于结构化推理，尤其是在复杂的 MATH 基准测试中。结果表明，即使动态生成结构化计划以确保推理先于最终答案，零样本非结构化 iSelf-Discover 变体也优于其五次结构化对应变体。计划生成的最佳粒度（实例级别与任务级别）取决于上下文。

Abstract: The drive for predictable LLM reasoning in their integration with compound
systems has popularized structured outputs, yet concerns remain about
performance trade-offs compared to unconstrained natural language. At the same
time, training on unconstrained Chain of Thought (CoT) traces has brought about
a new class of strong reasoning models that nevertheless present novel compute
budget and faithfulness challenges. This paper introduces iSelf-Discover, an
instance-level adaptation of the Self-Discover framework, and using it compares
dynamically generated structured JSON reasoning with its unstructured
counterpart. Our empirical evaluation across diverse benchmarks using
state-of-the-art open-source models supports a consistent advantage for
unstructured reasoning. Notably, on the complex MATH benchmark, unstructured
plans achieved relative performance improvements of up to 18.90\% over
structured approaches. Zero-shot unstructured iSelf-Discover variants are also
shown to outperform their five-shot structured counterparts, underscoring the
significance of this gap, even when structured plans are dynamically generated
to ensure reasoning precedes the final answer. We further demonstrate that the
optimal granularity of plan generation (instance-level vs. task-level) is
context-dependent. These findings invite re-evaluation of the reliance on
structured formats for complex problem-solving and how compound systems should
be organized.

</details>


### [97] [Artificial intelligence in drug discovery: A comprehensive review with a case study on hyperuricemia, gout arthritis, and hyperuricemic nephropathy](https://arxiv.org/abs/2507.03407)
*Junwei Su,Cheng Xin,Ao Shang,Shan Wu,Zhenzhen Xie,Ruogu Xiong,Xiaoyu Xu,Cheng Zhang,Guang Chen,Yau-Tuen Chan,Guoyi Tang,Ning Wang,Yong Xu,Yibin Feng*

Main category: cs.AI

TL;DR: 本综述全面分析了AI/ML在药物发现中的应用，强调了其在加速药物发现方面的潜力，并通过案例研究展示了其在分子靶标识别和治疗候选药物发现方面的实际成功。


<details>
  <summary>Details</summary>
Motivation: 传统药物发现方法存在固有的复杂性、成本上升、时间延长和高失败率，因此迫切需要全面了解如何将AI/ML有效地整合到整个过程中。

Method: 对AI/ML在药物发现核心阶段的应用进行了详细和全面的分析，重点介绍了重要的方法学进展及其在每个阶段的影响。通过对高尿酸血症、痛风性关节炎和高尿酸血症性肾病深入的案例研究，进一步说明这些技术的实际影响，强调了分子靶标识别和治疗候选药物发现方面的实际成功案例。

Result: AI/ML技术在分子靶标识别和治疗候选药物发现方面取得了实际成功。

Conclusion: 综述旨在帮助研究人员利用AI/ML克服药物发现的瓶颈并加速药物发现。

Abstract: This paper systematically reviews recent advances in artificial intelligence
(AI), with a particular focus on machine learning (ML), across the entire drug
discovery pipeline. Due to the inherent complexity, escalating costs, prolonged
timelines, and high failure rates of traditional drug discovery methods, there
is a critical need to comprehensively understand how AI/ML can be effectively
integrated throughout the full process. Currently available literature reviews
often narrowly focus on specific phases or methodologies, neglecting the
dependence between key stages such as target identification, hit screening, and
lead optimization. To bridge this gap, our review provides a detailed and
holistic analysis of AI/ML applications across these core phases, highlighting
significant methodological advances and their impacts at each stage. We further
illustrate the practical impact of these techniques through an in-depth case
study focused on hyperuricemia, gout arthritis, and hyperuricemic nephropathy,
highlighting real-world successes in molecular target identification and
therapeutic candidate discovery. Additionally, we discuss significant
challenges facing AI/ML in drug discovery and outline promising future research
directions. Ultimately, this review serves as an essential orientation for
researchers aiming to leverage AI/ML to overcome existing bottlenecks and
accelerate drug discovery.

</details>


### [98] [Lessons from a Chimp: AI "Scheming" and the Quest for Ape Language](https://arxiv.org/abs/2507.03409)
*Christopher Summerfield,Lennart Luettgau,Magda Dubois,Hannah Rose Kirk,Kobi Hackenburg,Catherine Fist,Katarina Slama,Nicola Ding,Rebecca Anselmetti,Andrew Strait,Mario Giulianelli,Cozmin Ududec*

Main category: cs.AI

TL;DR: Compare current AI scheming research with primate language research in the 1970s, argue for avoiding pitfalls like overattribution and weak theoretical frameworks, and outline concrete steps for productive research.


<details>
  <summary>Details</summary>
Motivation: examining recent research that asks whether current AI systems may be developing a capacity for scheming

Method: comparing current research practices in AI scheming to those adopted in the 1970s to test whether non-human primates could master natural language

Result: lessons to be learned from the historical research endeavour in primate language acquisition, which was characterised by an overattribution of human traits to other agents, an excessive reliance on anecdote and descriptive analysis, and a failure to articulate a strong theoretical framework for the research

Conclusion: We recommend that research into AI scheming actively seeks to avoid these pitfalls. We outline some concrete steps that can be taken for this research programme to advance in a productive and scientifically rigorous fashion.

Abstract: We examine recent research that asks whether current AI systems may be
developing a capacity for "scheming" (covertly and strategically pursuing
misaligned goals). We compare current research practices in this field to those
adopted in the 1970s to test whether non-human primates could master natural
language. We argue that there are lessons to be learned from that historical
research endeavour, which was characterised by an overattribution of human
traits to other agents, an excessive reliance on anecdote and descriptive
analysis, and a failure to articulate a strong theoretical framework for the
research. We recommend that research into AI scheming actively seeks to avoid
these pitfalls. We outline some concrete steps that can be taken for this
research programme to advance in a productive and scientifically rigorous
fashion.

</details>


### [99] [Multi-Agent Reasoning for Cardiovascular Imaging Phenotype Analysis](https://arxiv.org/abs/2507.03460)
*Weitong Zhang,Mengyun Qiao,Chengqi Zang,Steven Niederer,Paul M Matthews,Wenjia Bai,Bernhard Kainz*

Main category: cs.AI

TL;DR: Introduces MESHAgents, a multi-agent framework using LLMs to identify imaging phenotypes and their associations with disease risk factors, demonstrating comparable or improved performance to expert-selected phenotypes in cardiovascular imaging.


<details>
  <summary>Details</summary>
Motivation: Identifying the associations between imaging phenotypes and disease risk factors and outcomes is essential for understanding disease mechanisms and improving diagnosis and prognosis models. However, traditional approaches rely on human-driven hypothesis testing and selection of association factors, often overlooking complex, non-linear dependencies among imaging phenotypes and other multi-modal data.

Method: We introduce a Multi-agent Exploratory Synergy for the Heart (MESHAgents) framework that leverages large language models as agents to dynamically elicit, surface, and decide confounders and phenotypes in association studies, using cardiovascular imaging as a proof of concept.

Result: MESHAgents autonomously uncovered correlations between imaging phenotypes and a wide range of non-imaging factors, identifying additional confounder variables beyond standard demographic factors. Validation on diagnosis tasks reveals that MESHAgents-discovered phenotypes achieve performance comparable to expert-selected phenotypes, with mean AUC differences as small as -0.004 on disease classification tasks. Notably, the recall score improves for 6 out of 9 disease types.

Conclusion: The framework provides clinically relevant imaging phenotypes with transparent reasoning, offering a scalable alternative to expert-driven methods.

Abstract: Identifying the associations between imaging phenotypes and disease risk
factors and outcomes is essential for understanding disease mechanisms and
improving diagnosis and prognosis models. However, traditional approaches rely
on human-driven hypothesis testing and selection of association factors, often
overlooking complex, non-linear dependencies among imaging phenotypes and other
multi-modal data. To address this, we introduce a Multi-agent Exploratory
Synergy for the Heart (MESHAgents) framework that leverages large language
models as agents to dynamically elicit, surface, and decide confounders and
phenotypes in association studies, using cardiovascular imaging as a proof of
concept. Specifically, we orchestrate a multi-disciplinary team of AI agents --
spanning cardiology, biomechanics, statistics, and clinical research -- which
spontaneously generate and converge on insights through iterative,
self-organizing reasoning. The framework dynamically synthesizes statistical
correlations with multi-expert consensus, providing an automated pipeline for
phenome-wide association studies (PheWAS). We demonstrate the system's
capabilities through a population-based study of imaging phenotypes of the
heart and aorta. MESHAgents autonomously uncovered correlations between imaging
phenotypes and a wide range of non-imaging factors, identifying additional
confounder variables beyond standard demographic factors. Validation on
diagnosis tasks reveals that MESHAgents-discovered phenotypes achieve
performance comparable to expert-selected phenotypes, with mean AUC differences
as small as -0.004 on disease classification tasks. Notably, the recall score
improves for 6 out of 9 disease types. Our framework provides clinically
relevant imaging phenotypes with transparent reasoning, offering a scalable
alternative to expert-driven methods.

</details>


### [100] [REAL: Benchmarking Abilities of Large Language Models for Housing Transactions and Services](https://arxiv.org/abs/2507.03477)
*Kexin Zhu,Yang Han*

Main category: cs.AI

TL;DR: 提出了 REAL 评估套件，用于评估 LLM 在房地产交易和服务中的能力。实验表明 LLM 仍需改进。


<details>
  <summary>Details</summary>
Motivation: 迫切需要评估 LLM 是否能像人类一样在房地产交易和服务中发挥代理的作用。

Method: 提出了一个名为 REAL 的评估套件，包含 5,316 个高质量的评估条目，涵盖记忆、理解、推理和幻觉四个主题，分为 14 个类别。

Result: 实验结果表明，LLM 仍有很大的改进空间才能应用于房地产领域。

Conclusion: LLMs 在房地产领域的应用仍有很大的改进空间。

Abstract: The development of large language models (LLMs) has greatly promoted the
progress of chatbot in multiple fields. There is an urgent need to evaluate
whether LLMs can play the role of agent in housing transactions and services as
well as humans. We present Real Estate Agent Large Language Model Evaluation
(REAL), the first evaluation suite designed to assess the abilities of LLMs in
the field of housing transactions and services. REAL comprises 5,316
high-quality evaluation entries across 4 topics: memory, comprehension,
reasoning and hallucination. All these entries are organized as 14 categories
to assess whether LLMs have the knowledge and ability in housing transactions
and services scenario. Additionally, the REAL is used to evaluate the
performance of most advanced LLMs. The experiment results indicate that LLMs
still have significant room for improvement to be applied in the real estate
field.

</details>


### [101] [Limits of Safe AI Deployment: Differentiating Oversight and Control](https://arxiv.org/abs/2507.03525)
*David Manheim,Aidan Homewood*

Main category: cs.AI

TL;DR: 本文区分了人工智能领域的监督和控制，提出了一个框架，并强调了这些机制的局限性，为监管机构、审计人员和从业人员识别现有局限性以及对新的概念和技术进步的需求提供支持。


<details>
  <summary>Details</summary>
Motivation: 监督通常被认为是确保人工智能系统具有责任性、可靠性并能够满足治理和管理要求的关键手段。然而，在学术和政策讨论中，这些概念经常被混淆或区分不足，从而削弱了设计或评估应保持在有意义的人工监督下的系统的努力。

Method: 本文对人工智能以外的监督文献进行了有针对性的批判性回顾，并简要总结了过去关于人工智能主题的工作。

Result: 区分了控制是事前或实时的，并且是操作性的而不是政策或治理性的。相比之下，监督要么是政策和治理职能，要么是事后的。控制旨在防止失败。相比之下，监督通常侧重于检测、补救或未来预防的激励；所有预防性监督策略仍然需要控制。

Conclusion: 本文提出了一个有理论基础且贴近政策的框架，阐明了每种监督机制可行的条件、不足之处以及使其在实践中具有意义所需的条件。概述了如何记录监督方法并将其整合到风险管理中，并借鉴微软负责任的AI成熟度模型，概述了AI监督的成熟度模型。明确强调了这些机制的一些界限，包括它们的应用范围、失败之处以及现有方法明显不足的地方。

Abstract: Oversight and control (collectively, supervision) are often invoked as key
levers for ensuring that AI systems are accountable, reliable, and able to
fulfill governance and management requirements. However, the concepts are
frequently conflated or insufficiently distinguished in academic and policy
discourse, undermining efforts to design or evaluate systems that should remain
under meaningful human supervision.
  This paper undertakes a targeted critical review of literature on supervision
outside of AI, along with a brief summary of past work on the topic related to
AI. We then differentiate control as being ex-ante or real-time, and
operational rather than policy or governance. In contrast, oversight is either
a policy and governance function, or is ex-post. We suggest that control aims
to prevent failures. In contrast, oversight often focuses on detection,
remediation, or incentives for future prevention; all preventative oversight
strategies nonetheless necessitate control.
  Building on this foundation, we make three contributions. First, we propose a
theoretically-informed yet policy-grounded framework that articulates the
conditions under which each mechanism is possible, where they fall short, and
what is required to make them meaningful in practice. Second, we outline how
supervision methods should be documented and integrated into risk management,
and drawing on the Microsoft Responsible AI Maturity Model, we outline a
maturity model for AI supervision. Third, we explicitly highlight some
boundaries of these mechanisms, including where they apply, where they fail,
and where it is clear that no existing methods suffice. This foregrounds the
question of whether meaningful supervision is possible in a given deployment
context, and can support regulators, auditors, and practitioners in identifying
both present limitations and the need for new conceptual and technical
advances.

</details>


### [102] [A Universal Approach to Feature Representation in Dynamic Task Assignment Problems](https://arxiv.org/abs/2507.03579)
*Riccardo Lo Bianco,Remco Dijkman,Wim Nuijten,Willem van Jaarsveld*

Main category: cs.AI

TL;DR: This paper proposes a method for representing and solving assignment problems with infinite state and action spaces using a graph-based feature representation and an adaptation of the Proximal Policy Optimization algorithm.


<details>
  <summary>Details</summary>
Motivation: Representing the state and the possible assignments so that they can serve as inputs and outputs for a policy NN remains an open challenge, especially when tasks or resources have features with an infinite number of possible values.

Method: A graph-based feature representation of assignment problems, which we call assignment graph; A mapping from marked Colored Petri Nets to assignment graphs; An adaptation of the Proximal Policy Optimization algorithm that can learn to solve assignment problems represented through assignment graphs.

Result: The experiments show that the method is suitable for representing and learning close-to-optimal task assignment policies regardless of the state and action space dimensionalities.

Conclusion: The method is suitable for representing and learning close-to-optimal task assignment policies regardless of the state and action space dimensionalities.

Abstract: Dynamic task assignment concerns the optimal assignment of resources to tasks
in a business process. Recently, Deep Reinforcement Learning (DRL) has been
proposed as the state of the art for solving assignment problems. DRL methods
usually employ a neural network (NN) as an approximator for the policy
function, which ingests the state of the process and outputs a valuation of the
possible assignments. However, representing the state and the possible
assignments so that they can serve as inputs and outputs for a policy NN
remains an open challenge, especially when tasks or resources have features
with an infinite number of possible values. To solve this problem, this paper
proposes a method for representing and solving assignment problems with
infinite state and action spaces. In doing so, it provides three contributions:
(I) A graph-based feature representation of assignment problems, which we call
assignment graph; (II) A mapping from marked Colored Petri Nets to assignment
graphs; (III) An adaptation of the Proximal Policy Optimization algorithm that
can learn to solve assignment problems represented through assignment graphs.
To evaluate the proposed representation method, we model three archetypal
assignment problems ranging from finite to infinite state and action space
dimensionalities. The experiments show that the method is suitable for
representing and learning close-to-optimal task assignment policies regardless
of the state and action space dimensionalities.

</details>


### [103] [Benchmarking Vector, Graph and Hybrid Retrieval Augmented Generation (RAG) Pipelines for Open Radio Access Networks (ORAN)](https://arxiv.org/abs/2507.03608)
*Sarat Ahmad,Zeinab Nezami,Maryam Hafeez,Syed Ali Raza Zaidi*

Main category: cs.AI

TL;DR: 本研究比较了Vector RAG、GraphRAG和Hybrid GraphRAG在ORAN规范下的性能，结果表明GraphRAG和Hybrid GraphRAG优于传统RAG。


<details>
  <summary>Details</summary>
Motivation: 在ORAN架构中，大型语言模型可以通过利用来自RAN智能控制器（RIC）平台的规范和API定义来生成xApp和rApp。然而，针对电信特定任务微调基础LLM仍然昂贵且资源密集。尽管这些方法很有前景，但它们缺乏系统的、指标驱动的评估，尤其是在ORAN等高风险领域。

Method: 对Vector RAG、GraphRAG和Hybrid GraphRAG使用ORAN规范进行比较评估，并使用生成指标（忠实度、答案相关性、上下文相关性和事实正确性）评估不同问题复杂度的性能。

Result: GraphRAG和Hybrid GraphRAG都优于传统RAG。Hybrid GraphRAG将事实正确性提高了8%，而GraphRAG将上下文相关性提高了7%。

Conclusion: GraphRAG和Hybrid GraphRAG优于传统RAG，Hybrid GraphRAG在事实正确性上提高了8%，而GraphRAG在上下文相关性上提高了7%。

Abstract: Generative AI (GenAI) is expected to play a pivotal role in enabling
autonomous optimization in future wireless networks. Within the ORAN
architecture, Large Language Models (LLMs) can be specialized to generate xApps
and rApps by leveraging specifications and API definitions from the RAN
Intelligent Controller (RIC) platform. However, fine-tuning base LLMs for
telecom-specific tasks remains expensive and resource-intensive.
Retrieval-Augmented Generation (RAG) offers a practical alternative through
in-context learning, enabling domain adaptation without full retraining. While
traditional RAG systems rely on vector-based retrieval, emerging variants such
as GraphRAG and Hybrid GraphRAG incorporate knowledge graphs or dual retrieval
strategies to support multi-hop reasoning and improve factual grounding.
Despite their promise, these methods lack systematic, metric-driven
evaluations, particularly in high-stakes domains such as ORAN. In this study,
we conduct a comparative evaluation of Vector RAG, GraphRAG, and Hybrid
GraphRAG using ORAN specifications. We assess performance across varying
question complexities using established generation metrics: faithfulness,
answer relevance, context relevance, and factual correctness. Results show that
both GraphRAG and Hybrid GraphRAG outperform traditional RAG. Hybrid GraphRAG
improves factual correctness by 8%, while GraphRAG improves context relevance
by 7%.

</details>


### [104] [EvoAgentX: An Automated Framework for Evolving Agentic Workflows](https://arxiv.org/abs/2507.03616)
*Yingxu Wang,Siwei Liu,Jinyuan Fang,Zaiqiao Meng*

Main category: cs.AI

TL;DR: EvoAgentX is an open-source platform that automates the generation, execution, and evolutionary optimization of multi-agent workflows, achieving significant performance improvements on various tasks.


<details>
  <summary>Details</summary>
Motivation: Existing MAS frameworks often require manual workflow configuration and lack native support for dynamic evolution and performance optimization. In addition, many MAS optimization algorithms are not integrated into a unified framework.

Method: EvoAgentX employs a modular architecture consisting of five core layers: the basic components, agent, workflow, evolving, and evaluation layers. Specifically, within the evolving layer, EvoAgentX integrates three MAS optimization algorithms, TextGrad, AFlow, and MIPRO, to iteratively refine agent prompts, tool configurations, and workflow topologies.

Result: EvoAgentX achieves significant performance improvements on HotPotQA, MBPP, MATH, and GAIA.

Conclusion: EvoAgentX consistently achieves significant performance improvements, including a 7.44% increase in HotPotQA F1, a 10.00% improvement in MBPP pass@1, a 10.00% gain in MATH solve accuracy, and an overall accuracy improvement of up to 20.00% on GAIA.

Abstract: Multi-agent systems (MAS) have emerged as a powerful paradigm for
orchestrating large language models (LLMs) and specialized tools to
collaboratively address complex tasks. However, existing MAS frameworks often
require manual workflow configuration and lack native support for dynamic
evolution and performance optimization. In addition, many MAS optimization
algorithms are not integrated into a unified framework. In this paper, we
present EvoAgentX, an open-source platform that automates the generation,
execution, and evolutionary optimization of multi-agent workflows. EvoAgentX
employs a modular architecture consisting of five core layers: the basic
components, agent, workflow, evolving, and evaluation layers. Specifically,
within the evolving layer, EvoAgentX integrates three MAS optimization
algorithms, TextGrad, AFlow, and MIPRO, to iteratively refine agent prompts,
tool configurations, and workflow topologies. We evaluate EvoAgentX on
HotPotQA, MBPP, and MATH for multi-hop reasoning, code generation, and
mathematical problem solving, respectively, and further assess it on real-world
tasks using GAIA. Experimental results show that EvoAgentX consistently
achieves significant performance improvements, including a 7.44% increase in
HotPotQA F1, a 10.00% improvement in MBPP pass@1, a 10.00% gain in MATH solve
accuracy, and an overall accuracy improvement of up to 20.00% on GAIA. The
source code is available at: https://github.com/EvoAgentX/EvoAgentX

</details>


### [105] [Large Language Models for Combinatorial Optimization: A Systematic Review](https://arxiv.org/abs/2507.03637)
*Francesca Da Ros,Michael Soprano,Luca Di Gaspero,Kevin Roitero*

Main category: cs.AI

TL;DR: This review explores how Large Language Models are used in Combinatorial Optimization, classifying 103 studies and identifying future research directions.


<details>
  <summary>Details</summary>
Motivation: To explore the application of Large Language Models (LLMs) in Combinatorial Optimization (CO).

Method: A systematic literature review was conducted using PRISMA guidelines, searching Scopus and Google Scholar, and applying inclusion/exclusion criteria to select 103 studies.

Result: 103 studies were selected and classified into semantic categories and topics, providing an overview of LLM tasks, architectures, datasets, and application fields in CO.

Conclusion: This review identifies future directions for leveraging LLMs in Combinatorial Optimization.

Abstract: This systematic review explores the application of Large Language Models
(LLMs) in Combinatorial Optimization (CO). We report our findings using the
Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA)
guidelines. We conduct a literature search via Scopus and Google Scholar,
examining over 2,000 publications. We assess publications against four
inclusion and four exclusion criteria related to their language, research
focus, publication year, and type. Eventually, we select 103 studies. We
classify these studies into semantic categories and topics to provide a
comprehensive overview of the field, including the tasks performed by LLMs, the
architectures of LLMs, the existing datasets specifically designed for
evaluating LLMs in CO, and the field of application. Finally, we identify
future directions for leveraging LLMs in this field.

</details>


### [106] [Towards Machine Theory of Mind with Large Language Model-Augmented Inverse Planning](https://arxiv.org/abs/2507.03682)
*Rebekah A. Gelpí,Eric Xue,William A. Cunningham*

Main category: cs.AI

TL;DR: 提出了一种混合的机器心理理论(ToM)方法，该方法使用大型语言模型(llm)作为生成假设的机制，并结合贝叶斯逆向规划模型。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯逆向规划模型可以准确预测人类在各种ToM任务上的推理，但这些模型在将这些预测扩展到具有大量可能假设和行动的场景中时受到限制。基于llm的方法最近在解决ToM基准方面显示出希望，但即使它们通过了其他结构上相同的版本，也可能在推理任务中表现出脆弱性和失败。

Method: 结合大型语言模型(llm)作为生成假设和似然函数的机制，以及贝叶斯逆向规划模型，该模型计算代理在给定其行为时可能出现的精神状态的后验概率。

Result: 通过结合这两种方法，该方法利用了每个组件的优势，在受先前逆向规划模型启发的任务上与最优结果紧密匹配，并且相对于单独使用llm或使用思维链提示的模型，提高了性能，即使是通常在ToM任务上表现不佳的较小llm。

Conclusion: 该模型有潜力预测开放性任务中的精神状态，为ToM模型的未来发展和社交智能生成代理的创建提供了有希望的方向。

Abstract: We propose a hybrid approach to machine Theory of Mind (ToM) that uses large
language models (LLMs) as a mechanism for generating hypotheses and likelihood
functions with a Bayesian inverse planning model that computes posterior
probabilities for an agent's likely mental states given its actions. Bayesian
inverse planning models can accurately predict human reasoning on a variety of
ToM tasks, but these models are constrained in their ability to scale these
predictions to scenarios with a large number of possible hypotheses and
actions. Conversely, LLM-based approaches have recently demonstrated promise in
solving ToM benchmarks, but can exhibit brittleness and failures on reasoning
tasks even when they pass otherwise structurally identical versions. By
combining these two methods, this approach leverages the strengths of each
component, closely matching optimal results on a task inspired by prior inverse
planning models and improving performance relative to models that utilize LLMs
alone or with chain-of-thought prompting, even with smaller LLMs that typically
perform poorly on ToM tasks. We also exhibit the model's potential to predict
mental states on open-ended tasks, offering a promising direction for future
development of ToM models and the creation of socially intelligent generative
agents.

</details>


### [107] [Towards Unified Neurosymbolic Reasoning on Knowledge Graphs](https://arxiv.org/abs/2507.03697)
*Qika Lin,Fangzhi Xu,Hao Lu,Kai He,Rui Mao,Jun Liu,Erik Cambria,Mengling Feng*

Main category: cs.AI

TL;DR: This paper introduces Tunsr, a unified neurosymbolic reasoning framework for knowledge graphs that addresses the limitations of existing methods by integrating neural and symbolic reasoning and handling diverse reasoning scenarios. It uses a reasoning graph and forward logic message-passing, along with the FARI algorithm for inducing first-order logic rules. The effectiveness of Tunsr is demonstrated through experiments on 19 datasets across four reasoning scenarios.


<details>
  <summary>Details</summary>
Motivation: current methods predominantly concentrate on a single form of neural or symbolic reasoning, failing to effectively integrate the inherent strengths of both approaches. Furthermore, the current prevalent methods primarily focus on addressing a single reasoning scenario, presenting limitations in meeting the diverse demands of real-world reasoning tasks. Unifying the neural and symbolic methods, as well as diverse reasoning scenarios in one model is challenging as there is a natural representation gap between symbolic rules and neural networks, and diverse scenarios exhibit distinct knowledge structures and specific reasoning objectives.

Method: a unified neurosymbolic reasoning framework, namely Tunsr, for KG reasoning. Tunsr first introduces a consistent structure of reasoning graph that starts from the query entity and constantly expands subsequent nodes by iteratively searching posterior neighbors. Based on it, a forward logic message-passing mechanism is proposed to update both the propositional representations and attentions, as well as first-order logic (FOL) representations and attentions of each node. In this way, Tunsr conducts the transformation of merging multiple rules by merging possible relations at each step. Finally, the FARI algorithm is proposed to induce FOL rules by constantly performing attention calculations over the reasoning graph.

Result: Tunsr conducts the transformation of merging multiple rules by merging possible relations at each step. Finally, the FARI algorithm is proposed to induce FOL rules by constantly performing attention calculations over the reasoning graph.

Conclusion: Extensive experimental results on 19 datasets of four reasoning scenarios (transductive, inductive, interpolation, and extrapolation) demonstrate the effectiveness of Tunsr.

Abstract: Knowledge Graph (KG) reasoning has received significant attention in the
fields of artificial intelligence and knowledge engineering, owing to its
ability to autonomously deduce new knowledge and consequently enhance the
availability and precision of downstream applications. However, current methods
predominantly concentrate on a single form of neural or symbolic reasoning,
failing to effectively integrate the inherent strengths of both approaches.
Furthermore, the current prevalent methods primarily focus on addressing a
single reasoning scenario, presenting limitations in meeting the diverse
demands of real-world reasoning tasks. Unifying the neural and symbolic
methods, as well as diverse reasoning scenarios in one model is challenging as
there is a natural representation gap between symbolic rules and neural
networks, and diverse scenarios exhibit distinct knowledge structures and
specific reasoning objectives. To address these issues, we propose a unified
neurosymbolic reasoning framework, namely Tunsr, for KG reasoning. Tunsr first
introduces a consistent structure of reasoning graph that starts from the query
entity and constantly expands subsequent nodes by iteratively searching
posterior neighbors. Based on it, a forward logic message-passing mechanism is
proposed to update both the propositional representations and attentions, as
well as first-order logic (FOL) representations and attentions of each node. In
this way, Tunsr conducts the transformation of merging multiple rules by
merging possible relations at each step. Finally, the FARI algorithm is
proposed to induce FOL rules by constantly performing attention calculations
over the reasoning graph. Extensive experimental results on 19 datasets of four
reasoning scenarios (transductive, inductive, interpolation, and extrapolation)
demonstrate the effectiveness of Tunsr.

</details>


### [108] [Roadmap for using large language models (LLMs) to accelerate cross-disciplinary research with an example from computational biology](https://arxiv.org/abs/2507.03722)
*Ruian Ke,Ruy M. Ribeiro*

Main category: cs.AI

TL;DR: 大型语言模型可以促进跨学科研究，但必须负责任地使用，并作为人机循环框架内的增强工具。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型正在改变研究的进行方式。然而，由于对其幻觉、偏见和对研究的潜在危害的担忧，它们在研究中的使用受到了质疑。因此，务必清楚地了解大型语言模型的优势和劣势，以确保其有效和负责任的使用。

Method: 将大型语言模型集成到跨学科研究中，并通过计算生物学案例研究（关于 HIV 反弹动力学建模）展示了与大型语言模型（ChatGPT）的迭代交互如何促进跨学科协作和研究。

Result: 检查了大型语言模型的能力和局限性，并提供了一个详细的计算生物学案例研究，证明大型语言模型最好在人机循环框架内用作增强工具。

Conclusion: 负责任地使用大型语言模型将促进创新性的跨学科研究，并大大加速科学发现。

Abstract: Large language models (LLMs) are powerful artificial intelligence (AI) tools
transforming how research is conducted. However, their use in research has been
met with skepticism, due to concerns about hallucinations, biases and potential
harms to research. These emphasize the importance of clearly understanding the
strengths and weaknesses of LLMs to ensure their effective and responsible use.
Here, we present a roadmap for integrating LLMs into cross-disciplinary
research, where effective communication, knowledge transfer and collaboration
across diverse fields are essential but often challenging. We examine the
capabilities and limitations of LLMs and provide a detailed computational
biology case study (on modeling HIV rebound dynamics) demonstrating how
iterative interactions with an LLM (ChatGPT) can facilitate interdisciplinary
collaboration and research. We argue that LLMs are best used as augmentative
tools within a human-in-the-loop framework. Looking forward, we envisage that
the responsible use of LLMs will enhance innovative cross-disciplinary research
and substantially accelerate scientific discoveries.

</details>


### [109] [Agent-Based Detection and Resolution of Incompleteness and Ambiguity in Interactions with Large Language Models](https://arxiv.org/abs/2507.03726)
*Riya Naik,Ashwin Srinivasan,Swati Agarwal,Estrid He*

Main category: cs.AI

TL;DR: Agent-based architecture improves LLM-based QA by resolving question deficiencies, leading to better answers and shorter interactions, but may increase latency.


<details>
  <summary>Details</summary>
Motivation: Long multi-turn interactions with LLMs can be tedious when clarifying contextual information through reasoning.

Method: Using agent-based architecture with LLM-based agents (GPT-3.5-Turbo and Llama-4-Scout) as zero-shot ReAct agents to classify, resolve, and answer questions with potential incompleteness or ambiguities.

Result: Agent-based transducers shorten interactions, improve answer quality, and offer explainable resolution of deficiencies, but may increase LLM invocations and latency. Benefits outweigh costs on tested datasets, except when questions have sufficient context.

Conclusion: Agent-based approach enhances LLM-based QA systems, providing benefits such as shorter interactions, improved answer quality, and explainable resolution of deficiencies, but may increase LLM invocations and latency. Benefits outweigh costs except when questions already have sufficient context.

Abstract: Many of us now treat LLMs as modern-day oracles asking it almost any kind of
question. However, consulting an LLM does not have to be a single turn
activity. But long multi-turn interactions can get tedious if it is simply to
clarify contextual information that can be arrived at through reasoning. In
this paper, we examine the use of agent-based architecture to bolster LLM-based
Question-Answering systems with additional reasoning capabilities. We examine
the automatic resolution of potential incompleteness or ambiguities in
questions by transducers implemented using LLM-based agents. We focus on
several benchmark datasets that are known to contain questions with these
deficiencies to varying degrees. We equip different LLMs (GPT-3.5-Turbo and
Llama-4-Scout) with agents that act as specialists in detecting and resolving
deficiencies of incompleteness and ambiguity. The agents are implemented as
zero-shot ReAct agents. Rather than producing an answer in a single step, the
model now decides between 3 actions a) classify b) resolve c) answer. Action a)
decides if the question is incomplete, ambiguous, or normal. Action b)
determines if any deficiencies identified can be resolved. Action c) answers
the resolved form of the question. We compare the use of LLMs with and without
the use of agents with these components. Our results show benefits of agents
with transducer 1) A shortening of the length of interactions with human 2) An
improvement in the answer quality and 3) Explainable resolution of deficiencies
in the question. On the negative side we find while it may result in additional
LLM invocations and in some cases, increased latency. But on tested datasets,
the benefits outweigh the costs except when questions already have sufficient
context. Suggesting the agent-based approach could be a useful mechanism to
harness the power of LLMs to develop more robust QA systems.

</details>


### [110] [Optimizing UAV Trajectories via a Simplified Close Enough TSP Approach](https://arxiv.org/abs/2507.03775)
*Hiba Bederina*

Main category: cs.AI

TL;DR: 本文提出了一种解决CETSP问题的新方法，该方法简化了数学公式，并在实际案例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 目标是通过引入近似欧几里德距离并简化目标函数的重新公式化来简化数学公式。

Method: 文章探讨了一种解决“足够接近的旅行商问题”(CETSP)的方法。

Result: 结果表明，该方法在管理计算资源方面是有效的，且不影响解决方案的质量。

Conclusion: 该文章提出的数学公式的行为分析提供了对其性能的全面见解。

Abstract: This article explores an approach to addressing the Close Enough Traveling
Salesman Problem (CETSP). The objective is to streamline the mathematical
formulation by introducing reformulations that approximate the Euclidean
distances and simplify the objective function. Additionally, the use of convex
sets in the constraint design offers computational benefits. The proposed
methodology is empirically validated on real-world CETSP instances, with the
aid of computational strategies such as a fragmented CPLEX-based approach.
Results demonstrate its effectiveness in managing computational resources
without compromising solution quality. Furthermore, the article analyzes the
behavior of the proposed mathematical formulations, providing comprehensive
insights into their performance.

</details>


### [111] [Learning Dark Souls Combat Through Pixel Input With Neuroevolution](https://arxiv.org/abs/2507.03793)
*Jim O'Connor,Gary B. Parker,Mustafa Bugti*

Main category: cs.AI

TL;DR: 本文研究了使用NEAT算法使AI玩黑魂，实验表明该方法具有可行性


<details>
  <summary>Details</summary>
Motivation: 研究增强拓扑神经进化（NEAT）在《黑暗之魂》中自动游戏的应用，这是一个以复杂战斗机制、动态环境和高维视觉输入为特征的极具挑战性的动作角色扮演游戏。

Method: 使用增强拓扑神经进化（NEAT），直接从原始像素数据进化神经网络。

Result: 进化后的智能体达到了高达35%的成功率，表明神经进化在解决复杂、视觉上错综复杂的游戏场景中的可行性。

Conclusion: 使用基于视觉的神经进化方法，可以在缺乏直接API支持或明确定义的状态表示的复杂游戏环境中应用。

Abstract: This paper investigates the application of Neuroevolution of Augmenting
Topologies (NEAT) to automate gameplay in Dark Souls, a notoriously challenging
action role-playing game characterized by complex combat mechanics, dynamic
environments, and high-dimensional visual inputs. Unlike traditional
reinforcement learning or game playing approaches, our method evolves neural
networks directly from raw pixel data, circumventing the need for explicit
game-state information. To facilitate this approach, we introduce the Dark
Souls API (DSAPI), a novel Python framework leveraging real-time computer
vision techniques for extracting critical game metrics, including player and
enemy health states. Using NEAT, agents evolve effective combat strategies for
defeating the Asylum Demon, the game's initial boss, without predefined
behaviors or domain-specific heuristics. Experimental results demonstrate that
evolved agents achieve up to a 35% success rate, indicating the viability of
neuroevolution in addressing complex, visually intricate gameplay scenarios.
This work represents an interesting application of vision-based neuroevolution,
highlighting its potential use in a wide range of challenging game environments
lacking direct API support or well-defined state representations.

</details>


### [112] [Generating Novelty in Open-World Multi-Agent Strategic Board Games](https://arxiv.org/abs/2507.03802)
*Mayank Kejriwal,Shilpa Thomas*

Main category: cs.AI

TL;DR: GNOME是一个用于测试多智能体AI系统在新环境中的有效性的平台，已用于AI鲁棒性研究和评估。


<details>
  <summary>Details</summary>
Motivation: 为了测试多智能体AI系统在面对新事物时的有效性。

Method: GNOME平台分离了AI游戏智能体和模拟器的开发，允许出现无法预测的新事物。它还提供了一个Web GUI。

Result: GNOME已在NeurIPS 2020上使用Monopoly游戏进行了演示，并正在DARPA SAIL-ON项目中使用，以评估开发新事物自适应游戏智能体的外部团队。

Conclusion: GNOME是一个用于测试多智能体AI系统在面对新事物时的有效性的实验平台。它通过分离AI游戏智能体和模拟器的开发，允许出现无法预测的新事物。该平台已在NeurIPS 2020上使用Monopoly游戏进行了演示，以促进关于AI鲁棒性和现实世界环境中新事物本质的公开讨论。

Abstract: We describe GNOME (Generating Novelty in Open-world Multi-agent
Environments), an experimental platform that is designed to test the
effectiveness of multi-agent AI systems when faced with \emph{novelty}. GNOME
separates the development of AI gameplaying agents with the simulator, allowing
\emph{unanticipated} novelty (in essence, novelty that is not subject to
model-selection bias). Using a Web GUI, GNOME was recently demonstrated at
NeurIPS 2020 using the game of Monopoly to foster an open discussion on AI
robustness and the nature of novelty in real-world environments. In this
article, we further detail the key elements of the demonstration, and also
provide an overview of the experimental design that is being currently used in
the DARPA Science of Artificial Intelligence and Learning for Open-World
Novelty (SAIL-ON) program to evaluate external teams developing
novelty-adaptive gameplaying agents.

</details>


### [113] [Leveraging Large Language Models for Tacit Knowledge Discovery in Organizational Contexts](https://arxiv.org/abs/2507.03811)
*Gianlucca Zuin,Saulo Mastelini,Túlio Loures,Adriano Veloso*

Main category: cs.AI

TL;DR: 该论文提出了一种基于代理的框架，该框架利用大型语言模型通过与员工的互动来重建数据集描述，从而有效地捕获组织中的隐性知识。


<details>
  <summary>Details</summary>
Motivation: 由于初始信息不完整、难以识别知识渊博的个人、正式层级和非正式网络的相互作用以及需要提出正确的问题，记录组织中的隐性知识可能是一项具有挑战性的任务。

Method: 利用大型语言模型（LLM）的基于代理的框架，通过与员工的互动迭代地重建数据集描述，并将知识传播建模为具有传染性减弱的易感-传染（SI）过程。

Result: 该智能体实现了 94.9% 的全知识召回率，并且自我批判反馈得分与外部文献评论得分密切相关。分析表明，该方法能够在无需直接访问唯一领域专家的情况下恢复信息。

Conclusion: 该研究表明，该智能体能够驾驭组织复杂性并捕获原本无法访问的零散知识。

Abstract: Documenting tacit knowledge in organizations can be a challenging task due to
incomplete initial information, difficulty in identifying knowledgeable
individuals, the interplay of formal hierarchies and informal networks, and the
need to ask the right questions. To address this, we propose an agent-based
framework leveraging large language models (LLMs) to iteratively reconstruct
dataset descriptions through interactions with employees. Modeling knowledge
dissemination as a Susceptible-Infectious (SI) process with waning infectivity,
we conduct 864 simulations across various synthetic company structures and
different dissemination parameters. Our results show that the agent achieves
94.9% full-knowledge recall, with self-critical feedback scores strongly
correlating with external literature critic scores. We analyze how each
simulation parameter affects the knowledge retrieval process for the agent. In
particular, we find that our approach is able to recover information without
needing to access directly the only domain specialist. These findings highlight
the agent's ability to navigate organizational complexity and capture
fragmented knowledge that would otherwise remain inaccessible.

</details>


### [114] [RELRaE: LLM-Based Relationship Extraction, Labelling, Refinement, and Evaluation](https://arxiv.org/abs/2507.03829)
*George Hannah,Jacopo de Berardinis,Terry R. Payne,Valentina Tamma,Andrew Mitchell,Ellen Piercy,Ewan Johnson,Andrew Ng,Harry Rostron,Boris Konev*

Main category: cs.AI

TL;DR: This paper presents the RELRaE framework, a framework that employs large language models to extract and accurately label the relationships implicitly present in the XML schema.


<details>
  <summary>Details</summary>
Motivation: To support the interoperability of data between labs, there is a motivation to translate the XML data into a knowledge graph. A key stage of this process is the enrichment of the XML schema to lay the foundation of an ontology schema.

Method: The RELRaE framework, a framework that employs large language models in different stages to extract and accurately label the relationships implicitly present in the XML schema.

Result: LLMs can accurately generate relationship labels and then evaluate them.

Conclusion: LLMs can be effectively used to support the generation of relationship labels in the context of lab automation, and that they can play a valuable role within semi-automatic ontology generation frameworks more generally.

Abstract: A large volume of XML data is produced in experiments carried out by robots
in laboratories. In order to support the interoperability of data between labs,
there is a motivation to translate the XML data into a knowledge graph. A key
stage of this process is the enrichment of the XML schema to lay the foundation
of an ontology schema. To achieve this, we present the RELRaE framework, a
framework that employs large language models in different stages to extract and
accurately label the relationships implicitly present in the XML schema. We
investigate the capability of LLMs to accurately generate these labels and then
evaluate them. Our work demonstrates that LLMs can be effectively used to
support the generation of relationship labels in the context of lab automation,
and that they can play a valuable role within semi-automatic ontology
generation frameworks more generally.

</details>


### [115] [Economic Evaluation of LLMs](https://arxiv.org/abs/2507.03834)
*Michael J. Zellinger,Matt Thomson*

Main category: cs.AI

TL;DR: This paper introduces an economic evaluation framework for LLMs that quantifies performance trade-offs in dollars, considering the cost of mistakes, latency, and abstention. The findings suggest that using the most powerful available model is often more economically sound than minimizing deployment costs.


<details>
  <summary>Details</summary>
Motivation: Practitioners often navigate LLM performance trade-offs by plotting Pareto frontiers of optimal accuracy-cost trade-offs. However, this approach offers no way to compare between LLMs with distinct strengths and weaknesses: for example, a cheap, error-prone model vs a pricey but accurate one.

Method: economic evaluation of LLMs

Result: reasoning models offer better accuracy-cost tradeoffs as soon as the economic cost of a mistake exceeds $0.01. In addition, we find that single large LLMs often outperform cascades when the cost of making a mistake is as low as $0.1.

Conclusion: when automating meaningful human tasks with AI models, practitioners should typically use the most powerful available model, rather than attempt to minimize AI deployment costs, since deployment costs are likely dwarfed by the economic impact of AI errors.

Abstract: Practitioners often navigate LLM performance trade-offs by plotting Pareto
frontiers of optimal accuracy-cost trade-offs. However, this approach offers no
way to compare between LLMs with distinct strengths and weaknesses: for
example, a cheap, error-prone model vs a pricey but accurate one. To address
this gap, we propose economic evaluation of LLMs. Our framework quantifies the
performance trade-off of an LLM as a single number based on the economic
constraints of a concrete use case, all expressed in dollars: the cost of
making a mistake, the cost of incremental latency, and the cost of abstaining
from a query. We apply our economic evaluation framework to compare the
performance of reasoning and non-reasoning models on difficult questions from
the MATH benchmark, discovering that reasoning models offer better
accuracy-cost tradeoffs as soon as the economic cost of a mistake exceeds
\$0.01. In addition, we find that single large LLMs often outperform cascades
when the cost of making a mistake is as low as \$0.1. Overall, our findings
suggest that when automating meaningful human tasks with AI models,
practitioners should typically use the most powerful available model, rather
than attempt to minimize AI deployment costs, since deployment costs are likely
dwarfed by the economic impact of AI errors.

</details>


### [116] [Participatory Evolution of Artificial Life Systems via Semantic Feedback](https://arxiv.org/abs/2507.03839)
*Shuowen Li,Kexin Wang,Minglu Fang,Danqi Huang,Ali Asadipour,Haipeng Mi,Yitong Sun*

Main category: cs.AI

TL;DR: A semantic feedback framework uses natural language to guide the evolution of artificial life systems, allowing user intent to modulate visual outcomes and behaviors. User studies show improved semantic alignment over manual tuning.


<details>
  <summary>Details</summary>
Motivation: enable natural language to guide the evolution of artificial life systems

Method: Integrating a prompt-to-parameter encoder, a CMA-ES optimizer, and CLIP-based evaluation

Result: system allows user intent to modulate both visual outcomes and underlying behavioral rules. Implemented in an interactive ecosystem simulation, the framework supports prompt refinement, multi-agent interaction, and emergent rule synthesis.

Conclusion: User studies show improved semantic alignment over manual tuning and demonstrate the system's potential as a platform for participatory generative design and open-ended evolution.

Abstract: We present a semantic feedback framework that enables natural language to
guide the evolution of artificial life systems. Integrating a
prompt-to-parameter encoder, a CMA-ES optimizer, and CLIP-based evaluation, the
system allows user intent to modulate both visual outcomes and underlying
behavioral rules. Implemented in an interactive ecosystem simulation, the
framework supports prompt refinement, multi-agent interaction, and emergent
rule synthesis. User studies show improved semantic alignment over manual
tuning and demonstrate the system's potential as a platform for participatory
generative design and open-ended evolution.

</details>


### [117] [From Query to Explanation: Uni-RAG for Multi-Modal Retrieval-Augmented Learning in STEM](https://arxiv.org/abs/2507.03868)
*Xinyi Wu,Yanhao Jia,Luwei Xiao,Shuai Zhao,Fengkuang Chiang,Erik Cambria*

Main category: cs.AI

TL;DR: Uni-RAG是一个用于智能教育系统的检索增强生成管道，它优于现有的检索和RAG系统，同时保持较低的计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的检索系统主要侧重于自然文本-图像匹配，缺乏处理现实世界教育场景中固有的多样性和模糊性的能力。

Method: 开发了一个轻量级和高效的多模态检索模块Uni-Retrieval，它提取查询样式原型，并将其与来自不断更新的Prompt Bank的tokens动态匹配。Prompt Bank通过利用混合专家低秩适应（MoE-LoRA）模块来编码和存储领域特定知识，并且可以进行调整，以增强Uni-Retrieval在测试时适应未见过的查询类型的能力。为了实现自然语言教育内容生成，我们将原始的Uni-Retrieval与一个紧凑的指令调优语言模型集成，形成一个完整的检索增强生成管道，名为Uni-RAG。

Result: 在SER和其他多模态基准测试上的实验结果表明，Uni-RAG在检索准确率和生成质量方面优于基线检索和RAG系统，同时保持较低的计算成本。

Conclusion: Uni-RAG在检索准确率和生成质量方面优于基线检索和RAG系统，同时保持较低的计算成本。该框架为智能教育系统提供了一个可扩展的、基于教学的解决方案，桥接了检索和生成，以支持跨不同STEM场景的个性化、可解释和高效的学习辅助。

Abstract: In AI-facilitated teaching, leveraging various query styles to interpret
abstract educational content is crucial for delivering effective and accessible
learning experiences. However, existing retrieval systems predominantly focus
on natural text-image matching and lack the capacity to address the diversity
and ambiguity inherent in real-world educational scenarios. To address this
limitation, we develop a lightweight and efficient multi-modal retrieval
module, named Uni-Retrieval, which extracts query-style prototypes and
dynamically matches them with tokens from a continually updated Prompt Bank.
This Prompt Bank encodes and stores domain-specific knowledge by leveraging a
Mixture-of-Expert Low-Rank Adaptation (MoE-LoRA) module and can be adapted to
enhance Uni-Retrieval's capability to accommodate unseen query types at test
time. To enable natural language educational content generation, we integrate
the original Uni-Retrieval with a compact instruction-tuned language model,
forming a complete retrieval-augmented generation pipeline named Uni-RAG. Given
a style-conditioned query, Uni-RAG first retrieves relevant educational
materials and then generates human-readable explanations, feedback, or
instructional content aligned with the learning objective. Experimental results
on SER and other multi-modal benchmarks show that Uni-RAG outperforms baseline
retrieval and RAG systems in both retrieval accuracy and generation quality,
while maintaining low computational cost. Our framework provides a scalable,
pedagogically grounded solution for intelligent educational systems, bridging
retrieval and generation to support personalized, explainable, and efficient
learning assistance across diverse STEM scenarios.

</details>


### [118] [Uncovering Systemic and Environment Errors in Autonomous Systems Using Differential Testing](https://arxiv.org/abs/2507.03870)
*Rahil P Mehta,Yashwanthi Anand,Manish Motwani,Sandhya Saisubramanian*

Main category: cs.AI

TL;DR: AIProbe, a novel black-box testing technique that applies differential testing to attribute undesirable agent behaviors either to agent deficiencies or due to environmental infeasibility.


<details>
  <summary>Details</summary>
Motivation: When an autonomous agent behaves undesirably, including failure to complete a task, it can be difficult to determine whether the behavior is due to a systemic agent error, such as flaws in the model or policy, or an environment error, where a task is inherently infeasible under a given environment configuration, even for an ideal agent. As agents and their environments grow more complex, identifying the error source becomes increasingly difficult but critical for reliable deployment.

Method: AIProbe first generates diverse environmental configurations and tasks for testing the agent, by modifying configurable parameters using Latin Hypercube sampling. It then solves each generated task using a search-based planner, independent of the agent. By comparing the agent's performance to the planner's solution, AIProbe identifies whether failures are due to errors in the agent's model or policy, or due to unsolvable task conditions.

Result: AIProbe identifies whether failures are due to errors in the agent's model or policy, or due to unsolvable task conditions.

Conclusion: AIProbe significantly outperforms state-of-the-art techniques in detecting both total and unique errors, thereby contributing to a reliable deployment of autonomous agents.

Abstract: When an autonomous agent behaves undesirably, including failure to complete a
task, it can be difficult to determine whether the behavior is due to a
systemic agent error, such as flaws in the model or policy, or an environment
error, where a task is inherently infeasible under a given environment
configuration, even for an ideal agent. As agents and their environments grow
more complex, identifying the error source becomes increasingly difficult but
critical for reliable deployment. We introduce AIProbe, a novel black-box
testing technique that applies differential testing to attribute undesirable
agent behaviors either to agent deficiencies, such as modeling or training
flaws, or due to environmental infeasibility. AIProbe first generates diverse
environmental configurations and tasks for testing the agent, by modifying
configurable parameters using Latin Hypercube sampling. It then solves each
generated task using a search-based planner, independent of the agent. By
comparing the agent's performance to the planner's solution, AIProbe identifies
whether failures are due to errors in the agent's model or policy, or due to
unsolvable task conditions. Our evaluation across multiple domains shows that
AIProbe significantly outperforms state-of-the-art techniques in detecting both
total and unique errors, thereby contributing to a reliable deployment of
autonomous agents.

</details>


### [119] [LLMs model how humans induce logically structured rules](https://arxiv.org/abs/2507.03876)
*Alyssa Loo,Ellie Pavlick,Roman Feiman*

Main category: cs.AI

TL;DR: 大型语言模型在逻辑概念规则归纳方面与人类行为的拟合程度至少与贝叶斯概率语言思维模型一样好，并且可能代表了一种新的理论解释。


<details>
  <summary>Details</summary>
Motivation: 认知科学的一个中心目标是提供一个计算上明确的关于心智结构及其发展的解释：什么是认知的原始表征构建块，这些基元通过什么规则组合，以及这些基元和规则最初来自哪里？

Method: 在一项用于研究逻辑概念规则归纳的现有实验范式中测试了各种LLM。

Result: LLMs至少与实现贝叶斯概率语言思维（pLoT）的模型一样，与人类行为的拟合程度一样好，后者是同一任务中人类行为的最佳计算模型。此外，LLMs对推断和部署的规则的性质做出了定性不同的预测，表明LLM不太可能仅仅是pLoT解决方案的实现。

Conclusion: 大型语言模型(LLMs)可能实例化了一种新的理论，该理论解释了人类逻辑概念所需的原始表示和计算，认知科学的未来工作应该参与其中。

Abstract: A central goal of cognitive science is to provide a computationally explicit
account of both the structure of the mind and its development: what are the
primitive representational building blocks of cognition, what are the rules via
which those primitives combine, and where do these primitives and rules come
from in the first place? A long-standing debate concerns the adequacy of
artificial neural networks as computational models that can answer these
questions, in particular in domains related to abstract cognitive function,
such as language and logic. This paper argues that recent advances in neural
networks -- specifically, the advent of large language models (LLMs) --
represent an important shift in this debate. We test a variety of LLMs on an
existing experimental paradigm used for studying the induction of rules
formulated over logical concepts. Across four experiments, we find converging
empirical evidence that LLMs provide at least as good a fit to human behavior
as models that implement a Bayesian probablistic language of thought (pLoT),
which have been the best computational models of human behavior on the same
task. Moreover, we show that the LLMs make qualitatively different predictions
about the nature of the rules that are inferred and deployed in order to
complete the task, indicating that the LLM is unlikely to be a mere
implementation of the pLoT solution. Based on these results, we argue that LLMs
may instantiate a novel theoretical account of the primitive representations
and computations necessary to explain human logical concepts, with which future
work in cognitive science should engage.

</details>


### [120] [Agent Exchange: Shaping the Future of AI Agent Economics](https://arxiv.org/abs/2507.03904)
*Yingxuan Yang,Ying Wen,Jun Wang,Weinan Zhang*

Main category: cs.AI

TL;DR: 提出了Agent Exchange (AEX)，一个专门的拍卖平台，旨在支持AI代理市场的动态，为未来AI生态系统中基于代理的经济基础设施奠定基础。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的兴起已将AI代理从被动的计算工具转变为自主的经济参与者。标志着以代理为中心的经济的出现，其中代理承担着积极的经济角色。

Method: 设计并提出了Agent Exchange (AEX)，一个专门的拍卖平台，支持AI代理市场的动态。

Result: 概述了AEX的设计原则和系统架构。

Conclusion: 提出了Agent Exchange (AEX)，为未来人工智能生态系统中基于代理的经济基础设施奠定了基础。

Abstract: The rise of Large Language Models (LLMs) has transformed AI agents from
passive computational tools into autonomous economic actors. This shift marks
the emergence of the agent-centric economy, in which agents take on active
economic roles-exchanging value, making strategic decisions, and coordinating
actions with minimal human oversight. To realize this vision, we propose Agent
Exchange (AEX), a specialized auction platform designed to support the dynamics
of the AI agent marketplace. AEX offers an optimized infrastructure for agent
coordination and economic participation. Inspired by Real-Time Bidding (RTB)
systems in online advertising, AEX serves as the central auction engine,
facilitating interactions among four ecosystem components: the User-Side
Platform (USP), which translates human goals into agent-executable tasks; the
Agent-Side Platform (ASP), responsible for capability representation,
performance tracking, and optimization; Agent Hubs, which coordinate agent
teams and participate in AEX-hosted auctions; and the Data Management Platform
(DMP), ensuring secure knowledge sharing and fair value attribution. We outline
the design principles and system architecture of AEX, laying the groundwork for
agent-based economic infrastructure in future AI ecosystems.

</details>


### [121] [Animation Needs Attention: A Holistic Approach to Slides Animation Comprehension with Visual-Language Models](https://arxiv.org/abs/2507.03916)
*Yifan Jiang,Yibo Xue,Yukun Kang,Pin Zheng,Jian Peng,Feiran Wu,Changliang Xu*

Main category: cs.AI

TL;DR: 发布了首个幻灯片动画建模公共数据集，并使用LoRA微调Qwen-2.5-VL-7B，在幻灯片动画生成方面取得了显著进展。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型 (VLM) 在动画任务中表现不佳，因为缺乏公共数据集和有限的时间推理能力。为了解决这个问题，我们发布了第一个用于幻灯片动画建模的公共数据集。

Method: 使用Low-Rank Adaptation (LoRA) 微调 Qwen-2.5-VL-7B。

Result: 在人工策划的幻灯片测试集上，LoRA模型BLEU-4提高了约60%，ROUGE-L提高了30%，并且在CODA-detail方面表现出显着改进。

Conclusion: 该数据集、LoRA增强模型和CODA指标为基于VLM的动态幻灯片生成方面的未来研究提供了严格的基准和基础。

Abstract: Slide animations, such as fade-ins, fly-ins, and wipes, are critical for
audience engagement, efficient information delivery, and vivid visual
expression. However, most AI-driven slide-generation tools still lack native
animation support, and existing vision-language models (VLMs) struggle with
animation tasks due to the absence of public datasets and limited
temporal-reasoning capabilities. To address this gap, we release the first
public dataset for slide-animation modeling: 12,000 triplets of
natural-language descriptions, animation JSON files, and rendered videos,
collectively covering every built-in PowerPoint effect. Using this resource, we
fine-tune Qwen-2.5-VL-7B with Low-Rank Adaptation (LoRA) and achieve consistent
improvements over GPT-4.1 and Gemini-2.5-Pro in BLEU-4, ROUGE-L, SPICE, and our
Coverage-Order-Detail Assessment (CODA) metric, which evaluates action
coverage, temporal order, and detail fidelity. On a manually curated test set
of slides, the LoRA model increases BLEU-4 by around 60%, ROUGE-L by 30%, and
shows significant improvements in CODA-detail. This demonstrates that low-rank
adaptation enables reliable temporal reasoning and generalization beyond
synthetic data. Overall, our dataset, LoRA-enhanced model, and CODA metric
provide a rigorous benchmark and foundation for future research on VLM-based
dynamic slide generation.

</details>


### [122] [CortexDebate: Debating Sparsely and Equally for Multi-Agent Debate](https://arxiv.org/abs/2507.03928)
*Yiliu Sun,Zicheng Zhao,Sheng Wan,Chen Gong*

Main category: cs.AI

TL;DR: CortexDebate, a novel Multi-Agent Debate method, addresses the limitations of lengthy input contexts and the overconfidence dilemma by constructing a sparse debating graph among LLM agents.


<details>
  <summary>Details</summary>
Motivation: existing MAD methods face two major issues: (a) too lengthy input contexts, which causes LLM agents to get lost in plenty of input information and experiences performance drop; and (b) the overconfidence dilemma, where self-assured LLM agents dominate the debate, leading to low debating effectiveness.

Method: a novel MAD method called 'CortexDebate' constructs a sparse debating graph among LLM agents, where each LLM agent only debates with the ones that are helpful to it. To optimize the graph, we propose a module named McKinsey-based Debate Matter (MDM), which acts as an artificial analog to white matter. By integrating the McKinsey Trust Formula, a well-established measure of trustworthiness from sociology, MDM enables credible evaluations that guide graph optimization.

Result: CortexDebate has been well demonstrated by extensive experimental results across eight datasets from four task types.

Conclusion: The effectiveness of CortexDebate has been well demonstrated by extensive experimental results across eight datasets from four task types.

Abstract: Nowadays, single Large Language Model (LLM) struggles with critical issues
such as hallucination and inadequate reasoning abilities. To mitigate these
issues, Multi-Agent Debate (MAD) has emerged as an effective strategy, where
LLM agents engage in in-depth debates with others on tasks. However, existing
MAD methods face two major issues: (a) too lengthy input contexts, which causes
LLM agents to get lost in plenty of input information and experiences
performance drop; and (b) the overconfidence dilemma, where self-assured LLM
agents dominate the debate, leading to low debating effectiveness. To address
these limitations, we propose a novel MAD method called "CortexDebate".
Inspired by the human brain's tendency to establish a sparse and dynamically
optimized network among cortical areas governed by white matter, CortexDebate
constructs a sparse debating graph among LLM agents, where each LLM agent only
debates with the ones that are helpful to it. To optimize the graph, we propose
a module named McKinsey-based Debate Matter (MDM), which acts as an artificial
analog to white matter. By integrating the McKinsey Trust Formula, a
well-established measure of trustworthiness from sociology, MDM enables
credible evaluations that guide graph optimization. The effectiveness of our
CortexDebate has been well demonstrated by extensive experimental results
across eight datasets from four task types.

</details>


### [123] [An ASP-Based Framework for MUSes](https://arxiv.org/abs/2507.03929)
*Mohimenul Kabir,Kuldeep S Meel*

Main category: cs.AI

TL;DR: This paper introduces MUS-ASP, an answer set programming-based framework, designed for online enumeration of MUSes. The experimental results demonstrates the effectiveness of MUS-ASP.


<details>
  <summary>Details</summary>
Motivation: understanding the core reason for unsatisfiability is crucial in several applications. One effective way to capture this is through the minimal unsatisfiable subset (MUS), the subset-minimal set of clauses that remains unsatisfiable.

Method: introducing an answer set programming-based framework, named MUS-ASP, designed for online enumeration of MUSes. By translating MUS enumeration into answer set solving, MUS-ASP leverages the computational efficiency of state-of-the-art ASP systems.

Result: acceleration in both MUS enumeration and counting tasks, particularly when integrated within hybrid solvers

Conclusion: The experimental evaluation demonstrates the effectiveness of MUS-ASP and highlights the acceleration in both MUS enumeration and counting tasks.

Abstract: Given an unsatisfiable formula, understanding the core reason for
unsatisfiability is crucial in several applications. One effective way to
capture this is through the minimal unsatisfiable subset (MUS), the
subset-minimal set of clauses that remains unsatisfiable. Current research
broadly focuses on two directions: (i) enumerating as many MUSes as possible
within a given time limit, and (ii) counting the total number of MUSes for a
given unsatisfiable formula.
  In this paper, we introduce an answer set programming-based framework, named
MUS-ASP, designed for online enumeration of MUSes. ASP is a powerful tool for
its strengths in knowledge representation and is particularly suitable for
specifying complex combinatorial problems. By translating MUS enumeration into
answer set solving, MUS-ASP leverages the computational efficiency of
state-of-the-art ASP systems. Our extensive experimental evaluation
demonstrates the effectiveness of MUS-ASP and highlights the acceleration in
both MUS enumeration and counting tasks, particularly when integrated within
hybrid solvers, including the framework proposed in this paper.

</details>


### [124] [Toward Better Generalisation in Uncertainty Estimators: Leveraging Data-Agnostic Features](https://arxiv.org/abs/2507.03998)
*Thuy An Ha,Bao Quoc Vo*

Main category: cs.AI

TL;DR: This paper explores combining data-agnostic features with hidden-state features to improve the generalizability of probes trained to assess the factual accuracy of LLM outputs. The results are inconclusive, with data-agnostic features sometimes improving and sometimes degrading performance.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) often generate responses that are factually incorrect yet expressed with high confidence, which can pose serious risks for end users. To address this, it is essential for LLMs not only to produce answers but also to provide accurate estimates of their correctness. Uncertainty quantification methods have been introduced to assess the quality of LLM outputs, with factual accuracy being a key aspect of that quality. Among these methods, those that leverage hidden states to train probes have shown particular promise, as these internal representations encode information relevant to the factuality of responses, making this approach the focus of this paper. However, the probe trained on the hidden states of one dataset often struggles to generalise to another dataset of a different task or domain.

Method: exploring combining data-agnostic features with hidden-state features and assess whether this hybrid feature set enhances out-of-domain performance. Further examine whether selecting only the most informative hidden-state features enables the data-agnostic features to contribute more effectively.

Result: introducing data-agnostic features generally enhances generalisation performance in most cases, in certain scenarios their inclusion degrades performance. A similar pattern emerges when retaining only the most important hidden-state features - adding data-agnostic features does not consistently further enhance performance compared to using the full set of hidden-state features. In some specific cases, the trained probe underweights the data-agnostic features relative to the hidden-state features, which is the main reason why the results are inconclusive.

Conclusion: Introducing data-agnostic features generally enhances generalisation performance in most cases, but in certain scenarios their inclusion degrades performance. A similar pattern emerges when retaining only the most important hidden-state features - adding data-agnostic features does not consistently further enhance performance compared to using the full set of hidden-state features. A closer analysis reveals that, in some specific cases, the trained probe underweights the data-agnostic features relative to the hidden-state features, which is the main reason why the results are inconclusive.

Abstract: Large Language Models (LLMs) often generate responses that are factually
incorrect yet expressed with high confidence, which can pose serious risks for
end users. To address this, it is essential for LLMs not only to produce
answers but also to provide accurate estimates of their correctness.
Uncertainty quantification methods have been introduced to assess the quality
of LLM outputs, with factual accuracy being a key aspect of that quality. Among
these methods, those that leverage hidden states to train probes have shown
particular promise, as these internal representations encode information
relevant to the factuality of responses, making this approach the focus of this
paper. However, the probe trained on the hidden states of one dataset often
struggles to generalise to another dataset of a different task or domain. To
address this limitation, we explore combining data-agnostic features with
hidden-state features and assess whether this hybrid feature set enhances
out-of-domain performance. We further examine whether selecting only the most
informative hidden-state features, thereby discarding task-specific noise,
enables the data-agnostic features to contribute more effectively. The
experiment results indicate that although introducing data-agnostic features
generally enhances generalisation performance in most cases, in certain
scenarios their inclusion degrades performance. A similar pattern emerges when
retaining only the most important hidden-state features - adding data-agnostic
features does not consistently further enhance performance compared to using
the full set of hidden-state features. A closer analysis reveals that, in some
specific cases, the trained probe underweights the data-agnostic features
relative to the hidden-state features, which we believe is the main reason why
the results are inconclusive.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [125] [LLM4Hint: Leveraging Large Language Models for Hint Recommendation in Offline Query Optimization](https://arxiv.org/abs/2507.03384)
*Suchen Liu,Jun Gao,Yinjun Han,Yang Lin*

Main category: cs.DB

TL;DR: This paper introduces LLM4Hint, a novel approach that leverages moderate-sized backbone LLMs to recommend query optimization hints for recurring queries in offline optimization. It addresses challenges like high inference latency and fine-tuning costs by integrating a lightweight model, devising a query rewriting strategy, and introducing an explicit matching prompt. 


<details>
  <summary>Details</summary>
Motivation: Existing traditional optimizers struggle with the cumbersome hand-tuning required for complex workloads, and the learning-based methods face limitations in ensuring generalization. With the great success of Large Language Model (LLM) across diverse downstream tasks, this paper explores how LLMs can be incorporated to enhance the generalization of learned optimizers. Though promising, such an incorporation still presents challenges, mainly including high model inference latency, and the substantial fine-tuning cost and suboptimal performance due to inherent discrepancy between the token sequences in LLM and structured SQL execution plans with rich numerical features.

Method: LLM4Hint achieves the goals through: (i) integrating a lightweight model to produce a soft prompt, which captures the data distribution in DBMS and the SQL predicates to provide sufficient optimization features while simultaneously reducing the context length fed to the LLM, (ii) devising a query rewriting strategy using a larger commercial LLM, so as to simplify SQL semantics for the backbone LLM and reduce fine-tuning costs, and (iii) introducing an explicit matching prompt to facilitate alignment between the LLM and the lightweight model, which can accelerate convergence of the combined model.

Result: LLM4Hint can outperform the state-of-the-art learned optimizers in terms of both effectiveness and generalization.

Conclusion: LLM4Hint can outperform the state-of-the-art learned optimizers in terms of both effectiveness and generalization by leveraging the LLM's stronger capability to understand the query statement.

Abstract: Query optimization is essential for efficient SQL query execution in DBMS,
and remains attractive over time due to the growth of data volumes and advances
in hardware. Existing traditional optimizers struggle with the cumbersome
hand-tuning required for complex workloads, and the learning-based methods face
limitations in ensuring generalization. With the great success of Large
Language Model (LLM) across diverse downstream tasks, this paper explores how
LLMs can be incorporated to enhance the generalization of learned optimizers.
Though promising, such an incorporation still presents challenges, mainly
including high model inference latency, and the substantial fine-tuning cost
and suboptimal performance due to inherent discrepancy between the token
sequences in LLM and structured SQL execution plans with rich numerical
features.
  In this paper, we focus on recurring queries in offline optimization to
alleviate the issue of high inference latency, and propose \textbf{LLM4Hint}
that leverages moderate-sized backbone LLMs to recommend query optimization
hints. LLM4Hint achieves the goals through: (i) integrating a lightweight model
to produce a soft prompt, which captures the data distribution in DBMS and the
SQL predicates to provide sufficient optimization features while simultaneously
reducing the context length fed to the LLM, (ii) devising a query rewriting
strategy using a larger commercial LLM, so as to simplify SQL semantics for the
backbone LLM and reduce fine-tuning costs, and (iii) introducing an explicit
matching prompt to facilitate alignment between the LLM and the lightweight
model, which can accelerate convergence of the combined model. Experiments show
that LLM4Hint, by leveraging the LLM's stronger capability to understand the
query statement, can outperform the state-of-the-art learned optimizers in
terms of both effectiveness and generalization.

</details>


### [126] [PFCS: Prime Factorization Cache System for Deterministic Data Relationship Discovery](https://arxiv.org/abs/2507.03919)
*Duy Le*

Main category: cs.DB

TL;DR: PFCS uses prime factorization for better cache performance, achieving higher hit rates and lower power consumption.


<details>
  <summary>Details</summary>
Motivation: Traditional cache systems rely on statistical heuristics that cannot guarantee relationship discovery, leading to suboptimal prefetching and resource waste.

Method: PFCS (Prime Factorization Cache System) uses prime factorization to represent data relationships as composite numbers, enabling perfect relationship recovery through factorization.

Result: PFCS achieves an average performance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction compared to state-of-the-art systems.

Conclusion: PFCS, using prime factorization for deterministic relationship discovery, outperforms state-of-the-art cache systems with significant performance, hit rate, and power consumption improvements.

Abstract: Cache systems fundamentally limit modern computing performance due to their
inability to precisely capture data relationships. While achieving 85-92% hit
rates, traditional systems rely on statistical heuristics that cannot guarantee
relationship discovery, leading to suboptimal prefetching and resource waste.
We present PFCS (Prime Factorization Cache System), which leverages the
mathematical uniqueness of prime factorization to achieve deterministic
relationship discovery with zero false positives. PFCS assigns unique primes to
data elements and represents relationships as composite numbers, enabling the
recovery of perfect relationships through factorization. A comprehensive
evaluation across database, ML, and HPC workloads demonstrates an average
performance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction
compared to state-of-the-art systems. The mathematical foundation provides
formal guarantees impossible with approximation-based approaches, establishing
a new paradigm for cache system design

</details>


### [127] [OneDB: A Distributed Multi-Metric Data Similarity Search System](https://arxiv.org/abs/2507.04256)
*Tang Qian,Yifan Zhu,Lu Chen,Xiangyu Ke,Jingwen Zhao,Tianyi Li,Yunjun Gao,Christian S. Jensen*

Main category: cs.DB

TL;DR: OneDB is a distributed multi-metric data similarity retrieval system that unifies multi-modal data and achieves state-of-the-art performance in retrieval accuracy, speed, and scalability.


<details>
  <summary>Details</summary>
Motivation: The increasing volumes of multi-modal data in real-world settings call for effective general-purpose data management solutions that facilitate user-friendly, accurate, efficient, and scalable retrieval.

Method: The OneDB system represents data of diverse modalities as metric data, affording each modality its own metric space and using a multi-metric model to unify multi-modal data. It features an extended Spart SQL query interface, lightweight weight learning, smart search-space pruning, two-layered indexing, and end-to-end system parameter autotuning.

Result: Experiments on real-life and synthetic datasets demonstrate the system's state-of-the-art performance in weight learning, retrieval accuracy, search speed, scalability, and parameter tuning.

Conclusion: The OneDB system achieves state-of-the-art performance with efficient weight learning, retrieval accuracy improvements of 12.63%-30.75%, accelerated search by 2.5-5.75x, high scalability, and parameter tuning that enables performance improvements of 15+%.

Abstract: Increasingly massive volumes of multi-modal data are being accumulated in
many {real world} settings, including in health care and e-commerce. This
development calls for effective general-purpose data management solutions for
multi-modal data. Such a solution must facilitate user-friendly and accurate
retrieval of any multi-modal data according to diverse application
requirements. Further, such a solution must be capable of efficient and
scalable retrieval.
  To address this need, we present OneDB, a distributed multi-metric data
similarity retrieval system. This system exploits the fact that data of diverse
modalities, such as text, images, and video, can be represented as metric data.
The system thus affords each data modality its own metric space with its own
distance function and then uses a multi-metric model to unify multi-modal data.
The system features several innovations: (i) an extended Spart SQL query
interface; (ii) lightweight means of learning appropriate weights of different
modalities when retrieving multi-modal data to enable accurate retrieval; (iii)
smart search-space pruning strategies that improve efficiency; (iv) two-layered
indexing of data to ensure load-balancing during distributed processing; and
(v) end-to-end system parameter autotuning.
  Experiments on three real-life datasets and two synthetic datasets offer
evidence that the system is capable of state-of-the-art performance: (i)
efficient and effective weight learning; (ii) retrieval accuracy improvements
of 12.63\%--30.75\% over the state-of-the-art vector similarity search system
at comparable efficiency; (iii) accelerated search by 2.5--5.75x over
state-of-the-art single- or multi-metric solutions; (iv) demonstrated high
scalability; and (v) parameter tuning that enables performance improvements of
15+%.

</details>


### [128] [AKEGEN: A LLM-based Tabular Corpus Generator for Evaluating Dataset Discovery in Data Lakes](https://arxiv.org/abs/2507.04687)
*Zhenwei Dai,Chuan Lei,Asterios Katsifodimos,Xiao Qin,Christos Faloutsos,Huzefa Rangwala*

Main category: cs.DB

TL;DR: 本文探讨了是否可以利用大型语言模型来生成一个适用于评估数据集发现解决方案的表格基准。


<details>
  <summary>Details</summary>
Motivation: 当前数据集发现方法的评估语料库主要基于开放数据集的子集，并且存在三个重要问题：1) 它们侧重于非常常见和通用的数据类型；2) 它们不包含人工注释的列对；3) 它们不关注语义列关系。

Method: 利用大型语言模型生成表格数据。

Result: 关注于查找可连接的表格，这是几乎每个数据集发现方法的基石。

Conclusion: 利用大型语言模型（LLM）生成表格基准，以评估数据集发现解决方案。

Abstract: How to generate a large, realistic set of tables along with joinability
relationships, to stress-test dataset discovery methods? Dataset discovery
methods aim to automatically identify related data assets in a data lake. The
development and evaluation of such solutions for customers from a wide range of
business domains, relies on diverse, high quality and domain-specific tabular
benchmarks. Large language models (LLMs) are trained on a wide variety of text
data, which can provide a strong foundation of general and domain-specific
knowledge. In this paper, we ask the question -- \textit{can we leverage LLMs
to generate a tabular benchmark adequate for evaluating the dataset discovery
solutions?} In particular, we focus on the task of finding joinable tables
which is the cornerstone of virtually every dataset discovery method. Current
corpora for evaluating dataset discovery methods are mainly based on subsets of
open data, and they suffer from three important issues: $i)$ they focus on very
common and generic data types (e.g., address, id, name, etc.); $ii)$ they do
not contain human-annotated column pairs; instead, practitioners synthesize
ground truth using table splits (e.g., horizontal for table union search and
vertical ones for joinability) and $iii)$ they do not focus on semantic column
relationships.

</details>


### [129] [SHARP: Shared State Reduction for Efficient Matching of Sequential Patterns](https://arxiv.org/abs/2507.04872)
*Cong Yu,Tuo Shi,Matthias Weidlich,Bo Zhao*

Main category: cs.DB

TL;DR: SHARP is a library that employs state reduction to achieve efficient best-effort pattern matching.


<details>
  <summary>Details</summary>
Motivation: Pattern matching is challenging, since common applications rely on a large set of patterns that shall be evaluated with tight latency bounds. At the same time, matching needs to maintain state, i.e., intermediate results, that grows exponentially in the input size. Hence, systems turn to best-effort processing, striving for maximal recall under a latency bound. Existing techniques, however, consider each pattern in isolation, neglecting the optimization potential induced by state sharing in pattern matching.

Method: SHARP incorporates state sharing between patterns through a new abstraction, coined pattern-sharing degree (PSD).

Result: SHARP realizes best-effort processing by selecting a subset of partial matches for further processing in constant time.

Conclusion: SHARP achieves a recall of 97%, 96% and 73% for pattern matching in CEP, OLAP, and RAG applications, under a bound of 50% of the average processing latency.

Abstract: The detection of sequential patterns in data is a basic functionality of
modern data processing systems for complex event processing (CEP), OLAP, and
retrieval-augmented generation (RAG). In practice, pattern matching is
challenging, since common applications rely on a large set of patterns that
shall be evaluated with tight latency bounds. At the same time, matching needs
to maintain state, i.e., intermediate results, that grows exponentially in the
input size. Hence, systems turn to best-effort processing, striving for maximal
recall under a latency bound. Existing techniques, however, consider each
pattern in isolation, neglecting the optimization potential induced by state
sharing in pattern matching.
  In this paper, we present SHARP, a library that employs state reduction to
achieve efficient best-effort pattern matching. To this end, SHARP incorporates
state sharing between patterns through a new abstraction, coined
pattern-sharing degree (PSD). At runtime, this abstraction facilitates the
categorization and indexing of partial pattern matches. Based thereon, once a
latency bound is exceeded, SHARP realizes best-effort processing by selecting a
subset of partial matches for further processing in constant time. In
experiments with real-world data, SHARP achieves a recall of 97%, 96% and 73%
for pattern matching in CEP, OLAP, and RAG applications, under a bound of 50%
of the average processing latency.

</details>


### [130] [The Case for Instance-Optimized LLMs in OLAP Databases](https://arxiv.org/abs/2507.04967)
*Bardia Mohammadi,Laurent Bindschaedler*

Main category: cs.DB

TL;DR: IOLM-DB通过查询特定的模型优化，使LLM增强的数据库查询变得实用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）可以通过强大的数据总结、清理和语义转换能力来增强分析系统。然而，大规模部署LLM（处理数百万到数十亿行）在计算和内存方面仍然非常昂贵。

Method: IOLM-DB生成轻量级的、专门的模型，这些模型是根据每个查询的特定需求，使用有代表性的数据样本量身定制的。

Result: IOLM-DB减少了高达76%的模型占用空间，并将吞吐量提高了高达3.31倍，同时通过积极的压缩技术（包括量化、稀疏化和结构剪枝）保持了准确性。

Conclusion: IOLM-DB使在分析系统中利用LLM查询在规模上可行，为未来的OLAP应用开辟了新的可能性。

Abstract: Large Language Models (LLMs) can enhance analytics systems with powerful data
summarization, cleaning, and semantic transformation capabilities. However,
deploying LLMs at scale -- processing millions to billions of rows -- remains
prohibitively expensive in computation and memory. We present IOLM-DB, a novel
system that makes LLM-enhanced database queries practical through
query-specific model optimization. Instead of using general-purpose LLMs,
IOLM-DB generates lightweight, specialized models tailored to each query's
specific needs using representative data samples. IOLM-DB reduces model
footprints by up to 76% and increases throughput by up to 3.31$\times$ while
maintaining accuracy through aggressive compression techniques, including
quantization, sparsification, and structural pruning. We further show how our
approach enables higher parallelism on existing hardware and seamlessly
supports caching and batching strategies to reduce overheads. Our prototype
demonstrates that leveraging LLM queries inside analytics systems is feasible
at scale, opening new possibilities for future OLAP applications.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [131] [Federated Learning for ICD Classification with Lightweight Models and Pretrained Embeddings](https://arxiv.org/abs/2507.03122)
*Binbin Xu,Gérard Dray*

Main category: cs.IR

TL;DR: This paper proposes a lightweight and scalable federated learning pipeline for multi-label ICD code classification using clinical notes, demonstrating a viable way toward scalable, privacy-conscious medical coding systems.


<details>
  <summary>Details</summary>
Motivation: This study investigates the feasibility and performance of federated learning (FL) for multi-label ICD code classification using clinical notes from the MIMIC-IV dataset. Unlike previous approaches that rely on centralized training or fine-tuned large language models

Method: a lightweight and scalable pipeline combining frozen text embeddings with simple multilayer perceptron (MLP) classifiers

Result: embedding quality substantially outweighs classifier complexity in determining predictive performance, and that federated learning can closely match centralized results in idealized conditions. While the models are orders of magnitude smaller than state-of-the-art architectures and achieved competitive micro and macro F1 scores

Conclusion: This work demonstrates a viable way toward scalable, privacy-conscious medical coding systems and offers a step toward for future research into federated, domain-adaptive clinical AI.

Abstract: This study investigates the feasibility and performance of federated learning
(FL) for multi-label ICD code classification using clinical notes from the
MIMIC-IV dataset. Unlike previous approaches that rely on centralized training
or fine-tuned large language models, we propose a lightweight and scalable
pipeline combining frozen text embeddings with simple multilayer perceptron
(MLP) classifiers. This design offers a privacy-preserving and
deployment-efficient alternative for clinical NLP applications, particularly
suited to distributed healthcare settings. Extensive experiments across both
centralized and federated configurations were conducted, testing six publicly
available embedding models from Massive Text Embedding Benchmark leaderboard
and three MLP classifier architectures under two medical coding (ICD-9 and
ICD-10). Additionally, ablation studies over ten random stratified splits
assess performance stability. Results show that embedding quality substantially
outweighs classifier complexity in determining predictive performance, and that
federated learning can closely match centralized results in idealized
conditions. While the models are orders of magnitude smaller than
state-of-the-art architectures and achieved competitive micro and macro F1
scores, limitations remain including the lack of end-to-end training and the
simplified FL assumptions. Nevertheless, this work demonstrates a viable way
toward scalable, privacy-conscious medical coding systems and offers a step
toward for future research into federated, domain-adaptive clinical AI.

</details>


### [132] [Modeling Item-Level Dynamic Variability with Residual Diffusion for Bundle Recommendation](https://arxiv.org/abs/2507.03280)
*Dong Zhang,Lin Li,Ming Li,Xiaohui Tao,Meng Sun,Jimmy Xiangji Huang*

Main category: cs.IR

TL;DR: 提出RDiffBR模型，以解决bundle推荐中bundle-item隶属关系动态变化的问题，实验表明该模型能有效提高推荐性能且训练时间增加不多。


<details>
  <summary>Details</summary>
Motivation: 现有的bundle推荐(BR)解决方案在预测用户对预构建bundle的偏好方面取得了显著的效果。然而，bundle-item(B-I)的隶属关系在实际场景中会动态变化。例如，一个主题为“休闲服装”的bundle可能会因为季节变化、用户偏好变化或库存调整等因素而添加“帽子”或移除“手表”。我们的实证研究表明，主流的BR模型在项目级别的可变性方面，其性能会波动甚至下降。

Method: 提出了一个新颖的残差扩散Bundle推荐模型(RDiffBR)，作为一个模型无关的生成框架，可以帮助BR模型适应这种情况。在BR模型初始训练期间，RDiffBR采用残差扩散模型来处理由BR模型生成的项目级bundle嵌入，通过正向-反向过程来表示bundle主题。在推理阶段，RDiffBR反转在B-I可变性场景下，由训练良好的bundle模型获得的项目级bundle嵌入，以生成有效的项目级bundle嵌入。

Result: RDiffBR将骨干BR模型的Recall和NDCG性能提高了高达23%，而训练时间仅增加了约4%。

Conclusion: RDiffBR通过残差连接显著增强了BR模型生成项目级bundle嵌入的能力，在不同领域的六个BR模型和四个公共数据集上的实验表明，RDiffBR将骨干BR模型的Recall和NDCG性能提高了高达23%，而训练时间仅增加了约4%。

Abstract: Existing solutions for bundle recommendation(BR) have achieved remarkable
effectiveness for predicting the user's preference for prebuilt bundles.
However, bundle-item(B-I) affiliation will vary dynamically in real scenarios.
For example, a bundle themed as 'casual outfit', may add 'hat' or remove
'watch' due to factors such as seasonal variations, changes in user pes or
inventory adjustments. Our empirical study demonstrates that the performance of
mainstream BR models will fluctuate or even decline regarding item-level
variability. This paper makes the first attempt to referencaddress the above
problem and proposes a novel Residual Diffusion for Bundle
Recommendation(RDiffBR) as a model-agnostic generative framework which can
assist a BR model in adapting this scenario. During the initial training of the
BR model, RDiffBR employs a residual diffusion model to process the item-level
bundle embeddings which are generated by BR model to represent bundle theme via
a forward-reverse process. In the inference stage, RDiffBR reverses item-level
bundle embeddings obtained by the well-trained bundle model under B-I
variability scenarios to generate the effective item-level bundle embeddings.
In particular, the residual connection in our residual approximator
significantly enhances item-level bundle embeddings generation ability of BR
models. Experiments on six BR models and four public datasets from different
domains show that RDiffBR improves the performance of Recall and NDCG of
backbone BR models by up to 23%, while only increased training time about
4%.Codes and datasets are available at
https://anonymous.4open.science/r/RDiffBR.

</details>


### [133] [Explainable Information Retrieval in the Audit Domain](https://arxiv.org/abs/2507.03479)
*Alexander Frummet,Emanuel Slany,Jonas Amling,Moritz Lang,Stephan Scheele*

Main category: cs.IR

TL;DR: This paper explores how explainable information retrieval (XIR) can assist auditors, addressing the problem of misleading references generated by conversational agents. It outlines challenges and future research directions.


<details>
  <summary>Details</summary>
Motivation: Conversational agents often generate misleading or fabricated references, undermining trust, particularly in high-stakes domains. Explainable information retrieval (XIR) aims to address this by making search results more transparent and interpretable.

Method: The paper focuses on auditing, a critical yet underexplored area, to advance XIR.

Result: The paper outlines key challenges and future research directions to advance XIR in the auditing domain.

Conclusion: This paper focuses on using explainable information retrieval (XIR) to support auditors and outlines key challenges and future research directions in this domain.

Abstract: Conversational agents such as Microsoft Copilot and Google Gemini assist
users with complex search tasks but often generate misleading or fabricated
references. This undermines trust, particularly in high-stakes domains such as
medicine and finance. Explainable information retrieval (XIR) aims to address
this by making search results more transparent and interpretable. While most
XIR research is domain-agnostic, this paper focuses on auditing -- a critical
yet underexplored area. We argue that XIR systems can support auditors in
completing their complex task. We outline key challenges and future research
directions to advance XIR in this domain.

</details>


### [134] [Exploring the Effect of Context-Awareness and Popularity Calibration on Popularity Bias in POI Recommendations](https://arxiv.org/abs/2507.03503)
*Andrea Forster,Simone Kopeinik,Denic Helic,Stefan Thalmann,Dominik Kowald*

Main category: cs.IR

TL;DR: 本文研究了通过结合上下文感知模型和校准的受欢迎程度技术来减轻兴趣点推荐系统中受欢迎程度偏差的有效性。


<details>
  <summary>Details</summary>
Motivation: 兴趣点 (POI) 推荐系统可帮助用户发现相关位置，但其有效性通常会受到受欢迎程度偏差的影响，这使得不太受欢迎但可能更有意义的地方处于不利地位。

Method: 评估上下文感知模型和校准的受欢迎程度技术作为减轻受欢迎程度偏差的策略的有效性

Result: 上下文感知模型不能被认为是统一的解决方案，因为所研究的模型在准确性和偏差方面表现出不同的影响。相比之下，如果在准确性和偏差缓解之间取得谨慎的平衡，校准技术可以有效地使推荐的受欢迎程度与用户偏好保持一致。

Conclusion: 结合校准和上下文感知可以平衡准确性和与用户偏好的紧密结合，即受欢迎程度校准。

Abstract: Point-of-interest (POI) recommender systems help users discover relevant
locations, but their effectiveness is often compromised by popularity bias,
which disadvantages less popular, yet potentially meaningful places. This paper
addresses this challenge by evaluating the effectiveness of context-aware
models and calibrated popularity techniques as strategies for mitigating
popularity bias. Using four real-world POI datasets (Brightkite, Foursquare,
Gowalla, and Yelp), we analyze the individual and combined effects of these
approaches on recommendation accuracy and popularity bias. Our results reveal
that context-aware models cannot be considered a uniform solution, as the
models studied exhibit divergent impacts on accuracy and bias. In contrast,
calibration techniques can effectively align recommendation popularity with
user preferences, provided there is a careful balance between accuracy and bias
mitigation. Notably, the combination of calibration and context-awareness
yields recommendations that balance accuracy and close alignment with the
users' popularity profiles, i.e., popularity calibration.

</details>


### [135] [A Multistakeholder Approach to Value-Driven Co-Design of Recommender System Evaluation Metrics in Digital Archives](https://arxiv.org/abs/2507.03556)
*Florian Atzenhofer-Baumgartner,Georg Vogeler,Dominik Kowald*

Main category: cs.IR

TL;DR: 该论文提出了一个用于评估文化遗产领域推荐系统的多方利益相关者框架，并为研究的每个阶段定制了指标。


<details>
  <summary>Details</summary>
Motivation: 商业平台的推荐系统主要依赖于参与度指标，而文化遗产领域需要平衡档案管理员、平台所有者、研究人员和其他利益相关者之间相互竞争的优先级。

Method: 通过焦点小组访谈不同类型的利益相关者，识别价值优先级。

Result: 研究结果表明，利益相关者的关注点与四个连续的研究阶段自然对齐：发现、互动、整合和影响。论文提出了针对每个阶段的指标。

Conclusion: 该论文为推荐系统提出了一个多方利益相关者的方法，特别是在文化遗产领域，并为研究的每个阶段提出了定制的指标。

Abstract: This paper presents the first multistakeholder approach for translating
diverse stakeholder values into an evaluation metric setup for Recommender
Systems (RecSys) in digital archives. While commercial platforms mainly rely on
engagement metrics, cultural heritage domains require frameworks that balance
competing priorities among archivists, platform owners, researchers, and other
stakeholders. To address this challenge, we conducted high-profile focus groups
(5 groups x 5 persons) with upstream, provider, system, consumer, and
downstream stakeholders, identifying value priorities across critical
dimensions: visibility/representation, expertise adaptation, and
transparency/trust. Our analysis shows that stakeholder concerns naturally
align with four sequential research funnel stages: discovery, interaction,
integration, and impact. The resulting framework addresses domain-specific
challenges including collection representation imbalances, non-linear research
patterns, and tensions between specialized expertise and broader accessibility.
We propose tailored metrics for each stage in this research journey, such as
research path quality for discovery, contextual appropriateness for
interaction, metadata-weighted relevance for integration, and cross-stakeholder
value alignment for impact assessment. Our contributions extend beyond digital
archives to the broader RecSys community, offering transferable evaluation
approaches for domains where value emerges through sustained engagement rather
than immediate consumption.

</details>


### [136] [GENPLUGIN: A Plug-and-Play Framework for Long-Tail Generative Recommendation with Exposure Bias Mitigation](https://arxiv.org/abs/2507.03568)
*Kun Yang,Siyao Zheng,Tianyi Li,Xiaodong Li,Hui Li*

Main category: cs.IR

TL;DR: GENPLUGIN通过双编码器和共享解码器架构，对比学习对齐语言和ID视图，缓解暴露偏差，并通过检索增强机制提升长尾推荐。


<details>
  <summary>Details</summary>
Motivation: 生成推荐（GenRec）提供了LLM集成、降低的嵌入成本，并消除了每个候选者的评分，吸引了极大的关注。尽管它具有良好的性能，但这项研究表明，它存在生成暴露偏差和长尾项目泛化能力差的问题，这是GenRec先前工作忽略的两个关键限制。

Method: GENPLUGIN，一个具有双编码器、共享解码器架构的即插即用框架。在预训练期间，它通过对比学习对齐语言和ID视图，协调两个互补视图中的项目表示。此外，GENPLUGIN使用了一种新颖的训练策略，该策略以概率方式用来自语言语义编码器的预测替换ground-truth项目ID tokens，从而减轻暴露偏差。为了改进长尾生成推荐，我们提出了一种基于检索的数据增强机制。它微调GENPLUGIN的解码器，使GENPLUGIN能够使用相关的用户上下文或协作信息来增强长尾推荐场景中项目ID tokens的生成。

Result: 广泛的实验表明，GENPLUGIN可以显著减轻项目ID生成过程中的生成暴露偏差，同时显著提高长尾项目推荐的质量。

Conclusion: GENPLUGIN可以显著减轻项目ID生成过程中的生成暴露偏差，同时显著提高长尾项目推荐的质量。

Abstract: Generative recommendation (GenRec) offers LLM integration, reduced embedding
costs, and eliminates per-candidate scoring, attracting great attention.
Despite its promising performance, this study reveals that it suffers from
generation exposure bias and poor long-tail item generalization, two critical
limitations overlooked by prior works on GenRec. To address these, we propose
GENPLUGIN, a plug-and-play framework featuring a dual-encoder, shared-decoder
architecture. During pre-training, it aligns language and ID views via
contrastive learning, harmonizing item representations across two complementary
views. Besides, GENPLUGIN uses a novel training strategy that probabilistically
substitutes ground-truth item ID tokens with predictions from the
language-semantics encoder, alleviating exposure bias. To improve long-tail
generative recommendation, we propose a retrieval-based data augmentation
mechanism. It fine-tunes the decoder of GENPLUGIN to endow GENPLUGIN with the
ability to use relevant users w.r.t. contexts or collaborative information to
augment the generation of item ID tokens in long-tail recommendation scenarios.
We have plugged GENPLUGIN into several representative GenRec models and the
extensive experiments demonstrate that GENPLUGIN can notably mitigate
generation exposure bias during item ID generation while significantly
improving the quality of long-tail item recommendation.

</details>


### [137] [Ranking-based Fusion Algorithms for Extreme Multi-label Text Classification (XMTC)](https://arxiv.org/abs/2507.03761)
*Celso França,Gestefane Rabbi,Thiago Salles,Washington Cunha,Leonardo Rocha,Marcos André Gonçalves*

Main category: cs.IR

TL;DR: 本文提出了一种结合稀疏和密集检索器的方法，以解决极端多标签文本分类中长尾标签分布的挑战。


<details>
  <summary>Details</summary>
Motivation: 极端多标签文本分类（XMTC）中，标签的长尾分布是一个重大挑战。标签可以分为频繁的头部标签和不频繁的尾部标签，平衡所有标签的效果很复杂。结合稀疏和密集检索器的动机是这些方法互补的排序特征。

Method: 结合稀疏检索器（如BM25）和密集检索器（如微调BERT）的预测结果。

Result: 通过结合稀疏检索器的精确匹配能力和密集检索器的语义丰富性，生成最终排名，从而提高头部和尾部标签的效果。

Conclusion: 结合稀疏检索器和密集检索器的预测结果，能够提高长尾标签分类的效果。

Abstract: In the context of Extreme Multi-label Text Classification (XMTC), where
labels are assigned to text instances from a large label space, the long-tail
distribution of labels presents a significant challenge. Labels can be broadly
categorized into frequent, high-coverage \textbf{head labels} and infrequent,
low-coverage \textbf{tail labels}, complicating the task of balancing
effectiveness across all labels. To address this, combining predictions from
multiple retrieval methods, such as sparse retrievers (e.g., BM25) and dense
retrievers (e.g., fine-tuned BERT), offers a promising solution. The fusion of
\textit{sparse} and \textit{dense} retrievers is motivated by the complementary
ranking characteristics of these methods. Sparse retrievers compute relevance
scores based on high-dimensional, bag-of-words representations, while dense
retrievers utilize approximate nearest neighbor (ANN) algorithms on dense text
and label embeddings within a shared embedding space. Rank-based fusion
algorithms leverage these differences by combining the precise matching
capabilities of sparse retrievers with the semantic richness of dense
retrievers, thereby producing a final ranking that improves the effectiveness
across both head and tail labels.

</details>


### [138] [Efficient and Effective Query Context-Aware Learning-to-Rank Model for Sequential Recommendation](https://arxiv.org/abs/2507.03789)
*Andrii Dzhoha,Alisa Mironenko,Vladimir Vlasov,Maarten Versteegh,Marjan Celikik*

Main category: cs.IR

TL;DR: 提出了一种将查询上下文整合到 Transformer 推荐模型中的新方法，该方法可以提高排序质量。


<details>
  <summary>Details</summary>
Motivation: 现代序列推荐系统通常使用基于 Transformer 的模型进行下一项预测。虽然这些模型在效率和质量之间表现出很强的平衡性，但整合交错特征（例如发生下一项交互的查询上下文（例如，浏览类别））带来了挑战。有效捕获查询上下文对于优化排名相关性和增强用户参与度至关重要，因为它提供了关于会话中用户意图的宝贵信号。与项目的特征不同，查询上下文在时间上与项目序列不对齐，这使得将其合并到 Transformer 中具有挑战性且容易出错。

Method: 提出了一种新的方法，该方法在注意力机制中有效地融合了项目序列和查询上下文。

Result: 我们提出的方法是整合查询上下文以提高模型排序质量（在相关性和多样性方面）的有效方法。

Conclusion: 提出了一种新的方法，该方法在注意力机制中有效地融合了项目序列和查询上下文。在大型在线平台和开放数据集上进行的大量离线和在线实验表明，我们提出的方法是整合查询上下文以提高模型排序质量（在相关性和多样性方面）的有效方法。

Abstract: Modern sequential recommender systems commonly use transformer-based models
for next-item prediction. While these models demonstrate a strong balance
between efficiency and quality, integrating interleaving features - such as the
query context (e.g., browse category) under which next-item interactions occur
- poses challenges. Effectively capturing query context is crucial for refining
ranking relevance and enhancing user engagement, as it provides valuable
signals about user intent within a session. Unlike an item's features, query
context is not temporally aligned with the item sequence, making its
incorporation into transformers challenging and error-prone. This paper
analyzes different strategies for incorporating query context into transformers
trained with a causal language modeling procedure as a case study. We propose a
new method that effectively fuses the item sequence with query context within
the attention mechanism. Through extensive offline and online experiments on a
large-scale online platform and open datasets, we present evidence that our
proposed method is an effective approach for integrating query context to
improve model ranking quality in terms of relevance and diversity.

</details>


### [139] [Continual Recommender Systems](https://arxiv.org/abs/2507.03861)
*Hyunsik Yoo,SeongKu Kang,Hanghang Tong*

Main category: cs.IR

TL;DR: 本教程填补了现有终身学习教程的空白，这些教程没有解决推荐系统特定的需求。


<details>
  <summary>Details</summary>
Motivation: 现有的关于持续或终身学习的教程没有解决推荐系统特定的需求，例如平衡每个用户的稳定性和可塑性，处理冷启动项目以及在流反馈下优化推荐指标。

Method: 对现有方法进行全面概述

Result: 强调了将持续学习应用于实际部署环境的最新努力，例如资源受限的系统和顺序交互设置。

Conclusion: 本教程旨在使研究人员和实践者受益。

Abstract: Modern recommender systems operate in uniquely dynamic settings: user
interests, item pools, and popularity trends shift continuously, and models
must adapt in real time without forgetting past preferences. While existing
tutorials on continual or lifelong learning cover broad machine learning
domains (e.g., vision and graphs), they do not address recommendation-specific
demands-such as balancing stability and plasticity per user, handling
cold-start items, and optimizing recommendation metrics under streaming
feedback. This tutorial aims to make a timely contribution by filling that gap.
We begin by reviewing the background and problem settings, followed by a
comprehensive overview of existing approaches. We then highlight recent efforts
to apply continual learning to practical deployment environments, such as
resource-constrained systems and sequential interaction settings. Finally, we
discuss open challenges and future research directions. We expect this tutorial
to benefit researchers and practitioners in recommender systems, data mining,
AI, and information retrieval across academia and industry.

</details>


### [140] [TayFCS: Towards Light Feature Combination Selection for Deep Recommender Systems](https://arxiv.org/abs/2507.03895)
*Xianquan Wang,Zhaocheng Du,Jieming Zhu,Chuhan Wu,Qinglin Jia,Zhenhua Dong*

Main category: cs.IR

TL;DR: This paper proposes TayFCS, a lightweight feature combination selection method for deep recommendation models, using Taylor expansion and logistic regression to improve performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Selecting useful feature combinations is essential to reduce noise and manage memory consumption in deep recommendation models, but existing feature selection methods are limited, and evaluating high-order feature combinations is computationally expensive.

Method: The paper proposes the Taylor Expansion Scorer (TayScorer) module for field-wise Taylor expansion and Logistic Regression Elimination (LRE) to reduce information redundancy.

Result: Experimental results on three benchmark datasets and online A/B tests demonstrate the effectiveness and efficiency of the proposed approach.

Conclusion: The paper introduces TayFCS, a lightweight feature combination selection method that improves model performance. Experimental results and online A/B tests validate its effectiveness, efficiency, and practical applicability.

Abstract: Feature interaction modeling is crucial for deep recommendation models. A
common and effective approach is to construct explicit feature combinations to
enhance model performance. However, in practice, only a small fraction of these
combinations are truly informative. Thus it is essential to select useful
feature combinations to reduce noise and manage memory consumption. While
feature selection methods have been extensively studied, they are typically
limited to selecting individual features. Extending these methods for
high-order feature combination selection presents a significant challenge due
to the exponential growth in time complexity when evaluating feature
combinations one by one. In this paper, we propose $\textbf{TayFCS}$, a
lightweight feature combination selection method that significantly improves
model performance. Specifically, we propose the Taylor Expansion Scorer
(TayScorer) module for field-wise Taylor expansion on the base model. Instead
of evaluating all potential feature combinations' importance by repeatedly
running experiments with feature adding and removal, this scorer only needs to
approximate the importance based on their sub-components' gradients. This can
be simply computed with one backward pass based on a trained recommendation
model. To further reduce information redundancy among feature combinations and
their sub-components, we introduce Logistic Regression Elimination (LRE), which
estimates the corresponding information gain based on the model prediction
performance. Experimental results on three benchmark datasets validate both the
effectiveness and efficiency of our approach. Furthermore, online A/B test
results demonstrate its practical applicability and commercial value.

</details>


### [141] [Function-based Labels for Complementary Recommendation: Definition, Annotation, and LLM-as-a-Judge](https://arxiv.org/abs/2507.03945)
*Chihiro Yamasaki,Kai Sugahara,Yuma Nagi,Kazushi Okamoto*

Main category: cs.IR

TL;DR: 本文提出了一种新的互补关系定义方法FBL，并通过实验验证了其有效性，表明机器学习模型和大型语言模型在FBL的指导下能够更准确地识别互补商品。


<details>
  <summary>Details</summary>
Motivation: 推断或评估两个项目是否具有互补关系需要互补关系标签；然而，由于这种关系固有的模糊性，定义这些标签具有挑战性。基于用户历史行为日志的互补标签试图捕捉这些关系，但通常会产生不一致和不可靠的结果。

Method: 引入了基于功能的标签（FBL），这是一种独立于用户购买日志和LLM不透明决策过程的互补关系的新定义。构建了一个包含2,759个项目对的人工注释FBL数据集。

Result: 即使在数据有限的情况下，逻辑回归和SVM等ML模型也能实现较高的宏观F1分数（约0.82）。此外，gpt-4o-mini等LLM在FBL的详细定义下表现出高度的一致性（0.989）和分类准确率（0.849），表明它们作为模仿人类判断的有效注释器的潜力。

Conclusion: FBLs作为互补关系的明确定义，可以更准确地推断和自动标记互补推荐。

Abstract: Complementary recommendations enhance the user experience by suggesting items
that are frequently purchased together while serving different functions from
the query item. Inferring or evaluating whether two items have a complementary
relationship requires complementary relationship labels; however, defining
these labels is challenging because of the inherent ambiguity of such
relationships. Complementary labels based on user historical behavior logs
attempt to capture these relationships, but often produce inconsistent and
unreliable results. Recent efforts have introduced large language models (LLMs)
to infer these relationships. However, these approaches provide a binary
classification without a nuanced understanding of complementary relationships.
In this study, we address these challenges by introducing Function-Based Labels
(FBLs), a novel definition of complementary relationships independent of user
purchase logs and the opaque decision processes of LLMs. We constructed a
human-annotated FBLs dataset comprising 2,759 item pairs and demonstrated that
it covered possible item relationships and minimized ambiguity. We then
evaluated whether some machine learning (ML) methods using annotated FBLs could
accurately infer labels for unseen item pairs, and whether LLM-generated
complementary labels align with human perception. Our results demonstrate that
even with limited data, ML models, such as logistic regression and SVM achieve
high macro-F1 scores (approximately 0.82). Furthermore, LLMs, such as
gpt-4o-mini, demonstrated high consistency (0.989) and classification accuracy
(0.849) under the detailed definition of FBLs, indicating their potential as
effective annotators that mimic human judgment. Overall, our study presents
FBLs as a clear definition of complementary relationships, enabling more
accurate inferences and automated labeling of complementary recommendations.

</details>


### [142] [A Comparative Study of Specialized LLMs as Dense Retrievers](https://arxiv.org/abs/2507.03958)
*Hengran Zhang,Keping Bi,Jiafeng Guo*

Main category: cs.IR

TL;DR: Domain-specific LLMs can improve retrieval, but mathematical specialization can hurt performance. Vision-language and code-specialized models show promise.


<details>
  <summary>Details</summary>
Motivation: The impact of domain-specific specialization on LLM retrieval effectiveness is underexplored.

Method: Extensive experiments with eight Qwen2.5 7B LLMs across zero-shot and supervised settings.

Result: Mathematical specialization degrades performance; vision-language and code-specialized LLMs perform well, even surpassing BM25 on code retrieval.

Conclusion: Mathematical specialization and long reasoning can degrade retrieval performance, while vision-language and code-specialized LLMs show promise for unified retrieval.

Abstract: While large language models (LLMs) are increasingly deployed as dense
retrievers, the impact of their domain-specific specialization on retrieval
effectiveness remains underexplored. This investigation systematically examines
how task-specific adaptations in LLMs influence their retrieval capabilities,
an essential step toward developing unified retrievers capable of handling
text, code, images, and multimodal content. We conduct extensive experiments
with eight Qwen2.5 7B LLMs, including base, instruction-tuned,
code/math-specialized, long reasoning, and vision-language models across
zero-shot retrieval settings and the supervised setting. For the zero-shot
retrieval settings, we consider text retrieval from the BEIR benchmark and code
retrieval from the CoIR benchmark. Further, to evaluate supervised performance,
all LLMs are fine-tuned on the MS MARCO dataset. We find that mathematical
specialization and the long reasoning capability cause consistent degradation
in three settings, indicating conflicts between mathematical reasoning and
semantic matching. The vision-language model and code-specialized LLMs
demonstrate superior zero-shot performance compared to other LLMs, even
surpassing BM25 on the code retrieval task, and maintain comparable performance
to base LLMs in supervised settings. These findings suggest promising
directions for the unified retrieval task leveraging cross-domain and
cross-modal fusion.

</details>


### [143] [Leveraging Multimodal Data and Side Users for Diffusion Cross-Domain Recommendation](https://arxiv.org/abs/2507.04000)
*Fan Zhang,Jinpeng Chen,Huan Li,Senzhang Wang,Yuan Cao,Kaimin Wei,JianXiang He,Feifei Kou,Jinqing Wang*

Main category: cs.IR

TL;DR: MuSiC利用多模态数据和侧用户进行扩散跨域推荐，解决了冷启动问题，并在Amazon数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的CDR研究侧重于将冷启动用户的信息从辅助域转移到目标域。然而，这些系统面临两个主要问题：多模态数据利用不足，阻碍了有效的跨域对齐；忽略了仅在目标域内交互的侧用户，导致对目标域向量空间分布的学习不足。

Method: 利用多模态大型语言模型提取项目多模态特征，并利用大型语言模型通过提示学习发现用户特征，无需微调。提出了跨域扩散模块来学习目标域中特征向量的生成。

Result: MuSiC在Amazon数据集上实现了最先进的性能，显著优于所有选定的基线。

Conclusion: MuSiC在Amazon数据集上实现了最先进的性能，显著优于所有选定的基线。

Abstract: Cross-domain recommendation (CDR) aims to address the persistent cold-start
problem in Recommender Systems. Current CDR research concentrates on
transferring cold-start users' information from the auxiliary domain to the
target domain. However, these systems face two main issues: the
underutilization of multimodal data, which hinders effective cross-domain
alignment, and the neglect of side users who interact solely within the target
domain, leading to inadequate learning of the target domain's vector space
distribution. To address these issues, we propose a model leveraging Multimodal
data and Side users for diffusion Cross-domain recommendation (MuSiC). We first
employ a multimodal large language model to extract item multimodal features
and leverage a large language model to uncover user features using prompt
learning without fine-tuning. Secondly, we propose the cross-domain diffusion
module to learn the generation of feature vectors in the target domain. This
approach involves learning feature distribution from side users and
understanding the patterns in cross-domain transformation through overlapping
users. Subsequently, the trained diffusion module is used to generate feature
vectors for cold-start users in the target domain, enabling the completion of
cross-domain recommendation tasks. Finally, our experimental evaluation of the
Amazon dataset confirms that MuSiC achieves state-of-the-art performance,
significantly outperforming all selected baselines. Our code is available:
https://anonymous.4open.science/r/MuSiC-310A/.

</details>


### [144] [CTR-Guided Generative Query Suggestion in Conversational Search](https://arxiv.org/abs/2507.04072)
*Erxue Min,Hsiu-Yuan Huang,Xihong Yang,Min Yang,Xin Jia,Yunfang Wu,Hengyi Cai,Junfeng Wang,Shuaiqiang Wang,Dawei Yin*

Main category: cs.IR

TL;DR: GQS通过整合点击建模和偏好优化来增强对话式搜索中的查询建议，从而在CTR、相关性和多样性方面优于基线。


<details>
  <summary>Details</summary>
Motivation: 在对话式搜索中生成有效的查询建议需要使模型输出与用户偏好对齐，由于点击信号稀疏且嘈杂，这具有挑战性。

Method: GQS，一个生成框架，它整合了点击建模和偏好优化，以增强真实世界的用户参与度。GQS包括三个关键组件：(1)一个多源CTR建模模块，它捕获不同的上下文信号来估计细粒度的点击率；(2)一个使用CTR加权直接偏好优化(DPO)的多样性感知偏好对齐策略，它平衡了相关性和语义多样性；(3)一个CTR校准的迭代优化过程，它在训练过程中联合优化CTR和生成模型。

Result: GQS在CTR、相关性和多样性方面优于强大的基线。

Conclusion: GQS在CTR、相关性和多样性方面优于强大的基线。

Abstract: Generating effective query suggestions in conversational search requires
aligning model outputs with user preferences, which is challenging due to
sparse and noisy click signals. We propose GQS, a generative framework that
integrates click modeling and preference optimization to enhance real-world
user engagement. GQS consists of three key components: (1) a Multi-Source CTR
Modeling module that captures diverse contextual signals to estimate
fine-grained click-through rates; (2) a Diversity-Aware Preference Alignment
strategy using CTR-weighted Direct Preference Optimization (DPO), which
balances relevance and semantic diversity; and (3) a CTR-Calibrated Iterative
Optimization process that jointly refines the CTR and generation models across
training rounds. Experiments on two real-world tasks demonstrate that GQS
outperforms strong baselines in CTR, relevance, and diversity.

</details>


### [145] [Navigating Speech Recording Collections with AI-Generated Illustrations](https://arxiv.org/abs/2507.04182)
*Sirina Håland,Trond Karlsen Strøm,Petra Galuščáková*

Main category: cs.IR

TL;DR: A novel navigational method for speech archives is proposed, leveraging language and multimodal generative models. It organizes data into a structured format using interactive mind maps and image generation tools. User tests indicate the application's potential to simplify the exploration of large speech collections.


<details>
  <summary>Details</summary>
Motivation: extracting information and knowledge from speech recordings remains challenging. Novel approaches for navigating and searching spoken content need to be explored and developed.

Method: a novel navigational method for speech archives that leverages recent advances in language and multimodal generative models. The system is implemented using the TED-LIUM~3 dataset

Result: organizes data into a structured format using interactive mind maps and image generation tools. Initial user tests using a System Usability Scale (SUS) questionnaire indicate the application's potential to simplify the exploration of large speech collections.

Conclusion: The application has the potential to simplify the exploration of large speech collections.

Abstract: Although the amount of available spoken content is steadily increasing,
extracting information and knowledge from speech recordings remains
challenging. Beyond enhancing traditional information retrieval methods such as
speech search and keyword spotting, novel approaches for navigating and
searching spoken content need to be explored and developed. In this paper, we
propose a novel navigational method for speech archives that leverages recent
advances in language and multimodal generative models. We demonstrate our
approach with a Web application that organizes data into a structured format
using interactive mind maps and image generation tools. The system is
implemented using the TED-LIUM~3 dataset, which comprises over 2,000 speech
transcripts and audio files of TED Talks. Initial user tests using a System
Usability Scale (SUS) questionnaire indicate the application's potential to
simplify the exploration of large speech collections.

</details>


### [146] [BiFair: A Fairness-aware Training Framework for LLM-enhanced Recommender Systems via Bi-level Optimization](https://arxiv.org/abs/2507.04294)
*Jiaming Zhang,Yuyuan Li,Yiqun Xu,Li Zhang,Xiaohua Feng,Zhifei Ren,Chaochao Chen*

Main category: cs.IR

TL;DR: LLM-enhanced RSs improve recommendation quality but raise fairness concerns. We decompose unfairness into prior and training unfairness, and propose BiFair, a bi-level optimization framework to mitigate both. Experiments show BiFair significantly mitigates unfairness.


<details>
  <summary>Details</summary>
Motivation: Existing studies reveal that LLM-based RSs exhibit greater unfairness than traditional RSs, yet fairness issues in LLM-enhanced RSs remain largely unexplored. In this paper, our empirical study reveals that while LLM-enhanced RSs improve fairness across item groups, a significant fairness gap persists. Further enhancement remains challenging due to the architectural differences and varying sources of unfairness inherent in LLM-enhanced RSs.

Method: We propose BiFair, a bi-level optimization-based fairness-aware training framework designed to mitigate both prior and training unfairness simultaneously. BiFair optimizes two sets of learnable parameters: LLM-generated representations and a trainable projector in the recommendation model, using a two-level nested optimization process. Additionally, we introduce an adaptive inter-group balancing mechanism, leveraging multi-objective optimization principles to dynamically balance fairness across item groups.

Result: our empirical study reveals that while LLM-enhanced RSs improve fairness across item groups, a significant fairness gap persists.

Conclusion: BiFair significantly mitigates unfairness and outperforms previous state-of-the-art methods.

Abstract: Large Language Model-enhanced Recommender Systems (LLM-enhanced RSs) have
emerged as a powerful approach to improving recommendation quality by
leveraging LLMs to generate item representations. Despite these advancements,
the integration of LLMs raises severe fairness concerns. Existing studies
reveal that LLM-based RSs exhibit greater unfairness than traditional RSs, yet
fairness issues in LLM-enhanced RSs remain largely unexplored. In this paper,
our empirical study reveals that while LLM-enhanced RSs improve fairness across
item groups, a significant fairness gap persists. Further enhancement remains
challenging due to the architectural differences and varying sources of
unfairness inherent in LLM-enhanced RSs. To bridge this gap, we first decompose
unfairness into i) \textit{prior unfairness} in LLM-generated representations
and ii) \textit{training unfairness} in recommendation models. Then, we propose
BiFair, a bi-level optimization-based fairness-aware training framework
designed to mitigate both prior and training unfairness simultaneously. BiFair
optimizes two sets of learnable parameters: LLM-generated representations and a
trainable projector in the recommendation model, using a two-level nested
optimization process. Additionally, we introduce an adaptive inter-group
balancing mechanism, leveraging multi-objective optimization principles to
dynamically balance fairness across item groups. Extensive experiments on three
real-world datasets demonstrate that BiFair significantly mitigates unfairness
and outperforms previous state-of-the-art methods.

</details>


### [147] [Hierarchical Intent-guided Optimization with Pluggable LLM-Driven Semantics for Session-based Recommendation](https://arxiv.org/abs/2507.04623)
*Jinpeng Chen,Jianxiang He,Huan Li,Senzhang Wang,Yuan Cao,Kaimin Wei,Zhenye Yang,Ye Ji*

Main category: cs.IR

TL;DR: HIPHOP: A hierarchical intent-guided optimization approach with pluggable LLM-driven semantic learning for session-based recommendations.


<details>
  <summary>Details</summary>
Motivation: Existing SBR models often focus only on single-session information, ignoring inter-session relationships and valuable cross-session insights. Some methods try to include inter-session data but struggle with noise and irrelevant information, reducing performance. Additionally, most models rely on item ID co-occurrence and overlook rich semantic details, limiting their ability to capture fine-grained item features.

Method: a novel hierarchical intent-guided optimization approach with pluggable LLM-driven semantic learning for session-based recommendations

Result: Experiments on multiple datasets show that HIPHOP significantly outperforms existing methods

Conclusion: HIPHOP significantly outperforms existing methods, demonstrating its effectiveness in improving recommendation quality.

Abstract: Session-based Recommendation (SBR) aims to predict the next item a user will
likely engage with, using their interaction sequence within an anonymous
session. Existing SBR models often focus only on single-session information,
ignoring inter-session relationships and valuable cross-session insights. Some
methods try to include inter-session data but struggle with noise and
irrelevant information, reducing performance. Additionally, most models rely on
item ID co-occurrence and overlook rich semantic details, limiting their
ability to capture fine-grained item features. To address these challenges, we
propose a novel hierarchical intent-guided optimization approach with pluggable
LLM-driven semantic learning for session-based recommendations, called HIPHOP.
First, we introduce a pluggable embedding module based on large language models
(LLMs) to generate high-quality semantic representations, enhancing item
embeddings. Second, HIPHOP utilizes graph neural networks (GNNs) to model item
transition relationships and incorporates a dynamic multi-intent capturing
module to address users' diverse interests within a session. Additionally, we
design a hierarchical inter-session similarity learning module, guided by user
intent, to capture global and local session relationships, effectively
exploring users' long-term and short-term interests. To mitigate noise, an
intent-guided denoising strategy is applied during inter-session learning.
Finally, we enhance the model's discriminative capability by using contrastive
learning to optimize session representations. Experiments on multiple datasets
show that HIPHOP significantly outperforms existing methods, demonstrating its
effectiveness in improving recommendation quality. Our code is available:
https://github.com/hjx159/HIPHOP.

</details>


### [148] [Heterogeneous User Modeling for LLM-based Recommendation](https://arxiv.org/abs/2507.04626)
*Honghui Bao,Wenjie Wang,Xinyu Lin,Fengbin Zhu,Teng Sun,Fuli Feng,Tat-Seng Chua*

Main category: cs.IR

TL;DR: 提出了一种用于开放域推荐的异构用户建模（HUM）方法，该方法使用压缩增强器和鲁棒性增强器来有效建模用户异质性。


<details>
  <summary>Details</summary>
Motivation: 在开放域推荐中有效建模用户偏好是一个关键挑战，现有方法在泛化性差、无法有效压缩噪声交互以及领域跷跷板现象方面存在不足。

Method: 提出了一种异构用户建模（HUM）方法，该方法结合了压缩增强器和鲁棒性增强器，用于基于LLM的推荐。

Result: 在异构数据集上的大量实验验证了

Conclusion: HUM有效地对用户异质性进行建模，通过实现高效率和鲁棒性，从而在开放域推荐中实现卓越的性能。

Abstract: Leveraging Large Language Models (LLMs) for recommendation has demonstrated
notable success in various domains, showcasing their potential for open-domain
recommendation. A key challenge to advancing open-domain recommendation lies in
effectively modeling user preferences from users' heterogeneous behaviors
across multiple domains. Existing approaches, including ID-based and
semantic-based modeling, struggle with poor generalization, an inability to
compress noisy interactions effectively, and the domain seesaw phenomenon. To
address these challenges, we propose a Heterogeneous User Modeling (HUM)
method, which incorporates a compression enhancer and a robustness enhancer for
LLM-based recommendation. The compression enhancer uses a customized prompt to
compress heterogeneous behaviors into a tailored token, while a masking
mechanism enhances cross-domain knowledge extraction and understanding. The
robustness enhancer introduces a domain importance score to mitigate the domain
seesaw phenomenon by guiding domain optimization. Extensive experiments on
heterogeneous datasets validate that HUM effectively models user heterogeneity
by achieving both high efficacy and robustness, leading to superior performance
in open-domain recommendation.

</details>


### [149] [FindRec: Stein-Guided Entropic Flow for Multi-Modal Sequential Recommendation](https://arxiv.org/abs/2507.04651)
*Maolin Wang,Yutian Xiao,Binhao Wang,Sheng Zhang,Shanshan Ye,Wanyu Wang,Hongzhi Yin,Ruocheng Guo,Zenglin Xu*

Main category: cs.IR

TL;DR: FindRec introduces a novel information flow-control-output paradigm to address challenges in processing multimodal sequential data for recommendation systems. It features an Integrated Information Coordination Module (IICM) and a cross-modal expert routing mechanism. FindRec outperforms state-of-the-art baselines on three real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Modern recommendation systems face significant challenges in processing multimodal sequential data, particularly in temporal dynamics modeling and information flow coordination. Traditional approaches struggle with distribution discrepancies between heterogeneous features and noise interference in multimodal signals.

Method: The framework features two key innovations: (1) A Stein kernel-based Integrated Information Coordination Module (IICM) that theoretically guarantees distribution consistency between multimodal features and ID streams, and (2) A cross-modal expert routing mechanism that adaptively filters and combines multimodal features based on their contextual relevance. Our approach leverages multi-head subspace decomposition for routing stability and RBF-Stein gradient for unbiased distribution alignment, enhanced by linear-complexity Mamba layers for efficient temporal modeling.

Result: FindRec's superior performance over state-of-the-art baselines, particularly in handling long sequences and noisy multimodal inputs.

Conclusion: FindRec achieves improved recommendation accuracy and enhanced model interpretability through its modular design. Extensive experiments on three real-world datasets demonstrate FindRec's superior performance over state-of-the-art baselines, particularly in handling long sequences and noisy multimodal inputs.

Abstract: Modern recommendation systems face significant challenges in processing
multimodal sequential data, particularly in temporal dynamics modeling and
information flow coordination. Traditional approaches struggle with
distribution discrepancies between heterogeneous features and noise
interference in multimodal signals. We propose \textbf{FindRec}~
(\textbf{F}lexible unified \textbf{in}formation \textbf{d}isentanglement for
multi-modal sequential \textbf{Rec}ommendation), introducing a novel
"information flow-control-output" paradigm. The framework features two key
innovations: (1) A Stein kernel-based Integrated Information Coordination
Module (IICM) that theoretically guarantees distribution consistency between
multimodal features and ID streams, and (2) A cross-modal expert routing
mechanism that adaptively filters and combines multimodal features based on
their contextual relevance. Our approach leverages multi-head subspace
decomposition for routing stability and RBF-Stein gradient for unbiased
distribution alignment, enhanced by linear-complexity Mamba layers for
efficient temporal modeling. Extensive experiments on three real-world datasets
demonstrate FindRec's superior performance over state-of-the-art baselines,
particularly in handling long sequences and noisy multimodal inputs. Our
framework achieves both improved recommendation accuracy and enhanced model
interpretability through its modular design. The implementation code is
available anonymously online for easy
reproducibility~\footnote{https://github.com/Applied-Machine-Learning-Lab/FindRec}.

</details>


### [150] [Harnessing Pairwise Ranking Prompting Through Sample-Efficient Ranking Distillation](https://arxiv.org/abs/2507.04820)
*Junru Wu,Le Yan,Zhen Qin,Honglei Zhuang,Paul Suganthan G. C.,Tianqi Liu,Zhe Dong,Xuanhui Wang,Harrie Oosterhuis*

Main category: cs.IR

TL;DR: 通过成对蒸馏，从PRP中提取点状学生排序器，以解决PRP计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 虽然使用大型语言模型（LLM）的成对排序提示（PRP）是最有效的零样本文档排序方法之一，但它相对于要排序的文档数量具有二次计算复杂度，因为它需要在所有可能的文档对上进行枚举。因此，对于大多数实际的排序应用来说，PRP的突出排序性能仍然是无法实现的。

Method: 从PRP生成的成对教师标签中提取点状学生排序器。

Result: 仅使用2%的pairs，我们就能够获得与使用所有pairs作为教师标签相同的性能。

Conclusion: 提出了一种新方法，通过成对蒸馏，利用PRP的有效性，解决了PRP计算成本高的问题，同时在蒸馏和服务过程中不会产生高计算成本。

Abstract: While Pairwise Ranking Prompting (PRP) with Large Language Models (LLMs) is
one of the most effective zero-shot document ranking methods, it has a
quadratic computational complexity with respect to the number of documents to
be ranked, as it requires an enumeration over all possible document pairs.
Consequently, the outstanding ranking performance of PRP has remained
unreachable for most real-world ranking applications.
  In this work, we propose to harness the effectiveness of PRP through pairwise
distillation. Specifically, we distill a pointwise student ranker from pairwise
teacher labels generated by PRP, resulting in an efficient student model that
retains the performance of PRP with substantially lower computational costs.
Furthermore, we find that the distillation process can be made
sample-efficient: with only 2% of pairs, we are able to obtain the same
performance as using all pairs for teacher labels. Thus, our novel approach
provides a solution to harness the ranking performance of PRP without incurring
high computational costs during both distillation and serving.

</details>


### [151] [SimLab: A Platform for Simulation-based Evaluation of Conversational Information Access Systems](https://arxiv.org/abs/2507.04888)
*Nolwenn Bernard,Sharath Chandra Etagi Suresh,Krisztian Balog,ChengXiang Zhai*

Main category: cs.IR

TL;DR: SimLab, a cloud platform, is introduced to benchmark conversational systems and user simulators, promoting reproducible research and community contributions.


<details>
  <summary>Details</summary>
Motivation: Evaluating interactive and conversational information access systems is difficult due to the lack of reproducible experiments. User simulation offers a solution, but infrastructure and tooling are lacking.

Method: A cloud-based platform called SimLab is designed and implemented to provide a centralized general solution for benchmarking conversational systems and user simulators.

Result: SimLab's features are showcased with an initial evaluation task of conversational movie recommendation, demonstrating its potential for controlled and reproducible evaluation.

Conclusion: SimLab is introduced as a cloud-based platform for benchmarking conversational systems and user simulators, aiming to advance conversational information access and user simulation research. The platform and initial evaluation task are publicly available, with a call for community contributions to ensure sustainability and future development.

Abstract: Research on interactive and conversational information access systems,
including search engines, recommender systems, and conversational assistants,
has been hindered by the difficulty in evaluating such systems with
reproducible experiments. User simulation provides a promising solution, but
there is a lack of infrastructure and tooling to support this kind of
evaluation. To facilitate simulation-based evaluation of conversational
information access systems, we introduce SimLab, the first cloud-based platform
to provide a centralized general solution for the community to benchmark both
conversational systems and user simulators in a controlled and reproducible
environment. We articulate requirements for such a platform and propose a
general infrastructure to address these requirements. We then present the
design and implementation of an initial version of SimLab and showcase its
features with an initial evaluation task of conversational movie
recommendation, which is made publicly available. Furthermore, we discuss the
sustainability of the platform and its future opportunities. This paper is a
call for the community to contribute to the platform to drive progress in the
field of conversational information access and user simulation.

</details>


### [152] [Do We Really Need Specialization? Evaluating Generalist Text Embeddings for Zero-Shot Recommendation and Search](https://arxiv.org/abs/2507.05006)
*Matteo Attimonelli,Alessandro De Bellis,Claudio Pomo,Dietmar Jannach,Eugenio Di Sciascio,Tommaso Di Noia*

Main category: cs.IR

TL;DR: 通用文本嵌入模型(GTEs)在顺序推荐和产品搜索中表现出色，无需微调。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型(PLM)被广泛用于从推荐和搜索中的项目元数据中导出语义表示。最近的研究表明，需要任务和领域特定的微调来提高表征能力。本文挑战了这一假设。

Method: 使用通用文本嵌入模型(GTEs)，预先训练在大型语料库上。

Result: 通用文本嵌入模型(GTEs)可以保证强大的零样本性能，而无需专门的适应。通用文本嵌入模型(GTEs)在顺序推荐和产品搜索中优于传统模型和微调模型。

Conclusion: 通用文本嵌入模型(GTEs)在顺序推荐和产品搜索中优于传统模型和微调模型，这归因于它们卓越的表征能力，因为它们在嵌入空间中更均匀地分布特征。通过关注信息量最大的方向（例如，通过PCA）压缩嵌入维度可以有效地减少噪声并提高专用模型的性能。

Abstract: Pre-trained language models (PLMs) are widely used to derive semantic
representations from item metadata in recommendation and search. In sequential
recommendation, PLMs enhance ID-based embeddings through textual metadata,
while in product search, they align item characteristics with user intent.
Recent studies suggest task and domain-specific fine-tuning are needed to
improve representational power. This paper challenges this assumption, showing
that Generalist Text Embedding Models (GTEs), pre-trained on large-scale
corpora, can guarantee strong zero-shot performance without specialized
adaptation. Our experiments demonstrate that GTEs outperform traditional and
fine-tuned models in both sequential recommendation and product search. We
attribute this to a superior representational power, as they distribute
features more evenly across the embedding space. Finally, we show that
compressing embedding dimensions by focusing on the most informative directions
(e.g., via PCA) effectively reduces noise and improves the performance of
specialized models. To ensure reproducibility, we provide our repository at
https://split.to/gte4ps.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [153] [Lightweight LSTM Model for Energy Theft Detection via Input Data Reduction](https://arxiv.org/abs/2507.02872)
*Caylum Collier,Krishnendu Guha*

Main category: cs.LG

TL;DR: 提出了一种用于智能电网中能源盗窃检测的节能 AI 辅助监控系统，该系统在保持检测精度的同时显著降低了功耗。


<details>
  <summary>Details</summary>
Motivation: 随着智能电表在全球电网中日益普及，检测能源盗窃已成为一项 критическая 且持续的挑战。人工智能 (AI) 模型已在识别欺诈性消费模式方面表现出强大的性能；但是，先前探索使用机器学习解决方案解决此问题的工作需要高昂的计算和能源成本，从而限制了其实用性——尤其是在低盗窃情况下，连续推理会导致不必要的能源消耗。

Method: 提出了一种轻量级检测单元（或看门狗机制），旨在充当预过滤器，以确定何时激活长短期记忆 (LSTM) 模型。

Result: 结果表明，功耗降低超过 64%，而检测精度损失 минимальна 且召回率始终很高。

Conclusion: 开发了一种节能且可扩展的智能电网能源盗窃检测方法，该方法强调实际部署考虑因素，例如推理效率和系统可扩展性，同时保持检测准确性。

Abstract: With the increasing integration of smart meters in electrical grids
worldwide, detecting energy theft has become a critical and ongoing challenge.
Artificial intelligence (AI)-based models have demonstrated strong performance
in identifying fraudulent consumption patterns; however, previous works
exploring the use of machine learning solutions for this problem demand high
computational and energy costs, limiting their practicality -- particularly in
low-theft scenarios where continuous inference can result in unnecessary energy
usage. This paper proposes a lightweight detection unit, or watchdog mechanism,
designed to act as a pre-filter that determines when to activate a long
short-term memory (LSTM) model. This mechanism reduces the volume of input fed
to the LSTM model, limiting it to instances that are more likely to involve
energy theft thereby preserving detection accuracy while substantially reducing
energy consumption associated with continuous model execution. The proposed
system was evaluated through simulations across six scenarios with varying
theft severity and number of active thieves. Results indicate a power
consumption reduction exceeding 64\%, with minimal loss in detection accuracy
and consistently high recall. These findings support the feasibility of a more
energy-efficient and scalable approach to energy theft detection in smart
grids. In contrast to prior work that increases model complexity to achieve
marginal accuracy gains, this study emphasizes practical deployment
considerations such as inference efficiency and system scalability. The results
highlight the potential for deploying sustainable, AI-assisted monitoring
systems within modern smart grid infrastructures.

</details>


### [154] [Regulation Compliant AI for Fusion: Real-Time Image Analysis-Based Control of Divertor Detachment in Tokamaks](https://arxiv.org/abs/2507.02897)
*Nathaniel Chen,Cheolsik Byun,Azarakash Jalalvand,Sangkyeun Kim,Andrew Rothstein,Filippo Scotti,Steve Allen,David Eldon,Keith Erickson,Egemen Kolemen*

Main category: cs.LG

TL;DR: AI enabled linear control system is implemented and validated for divertor detachment control.


<details>
  <summary>Details</summary>
Motivation: inherent black-box nature will make compliant implementation in regulatory environments a challenge

Method: real-time AI enabled linear and interpretable control system

Result: feedback divertor detachment control with a mean absolute difference of 2% from the target for both detachment and reattachment

Conclusion: This automatic training and linear processing framework can be extended to any image based diagnostic for regulatory compliant controller necessary for future fusion reactors.

Abstract: While artificial intelligence (AI) has been promising for fusion control, its
inherent black-box nature will make compliant implementation in regulatory
environments a challenge. This study implements and validates a real-time AI
enabled linear and interpretable control system for successful divertor
detachment control with the DIII-D lower divertor camera. Using D2 gas, we
demonstrate feedback divertor detachment control with a mean absolute
difference of 2% from the target for both detachment and reattachment. This
automatic training and linear processing framework can be extended to any image
based diagnostic for regulatory compliant controller necessary for future
fusion reactors.

</details>


### [155] [Controllable diffusion-based generation for multi-channel biological data](https://arxiv.org/abs/2507.02902)
*Haoran Zhang,Mingyuan Zhou,Wesley Tansey*

Main category: cs.LG

TL;DR: 提出了一种统一的扩散框架，用于在结构化和空间生物数据上进行可控生成，该框架优于现有的模型，并在各种预测任务中表现出强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的模型通常假设低维输入(例如，RGB图像)，并依赖于简单的条件机制，这些机制会破坏空间对应关系并忽略通道间的依赖关系。因此，需要一个统一的扩散框架，用于对结构化和空间生物数据进行可控生成。

Method: 该模型包含两个关键创新：(1)一种分层特征注入机制，能够对空间对齐的通道进行多分辨率调节，以及(2)潜在空间和输出空间通道式注意力的结合，以捕获通道间的关系。

Result: 该模型在空间和非空间预测任务中都表现出了最先进的性能，包括IMC中的蛋白质推算和单细胞数据集中的基因到蛋白质预测，并且对未见过的条件配置表现出了强大的泛化能力。

Conclusion: 该模型在空间和非空间预测任务中表现出色，包括IMC中的蛋白质推算和单细胞数据集中的基因到蛋白质预测，并显示出对未见过的条件配置的强大泛化能力。

Abstract: Spatial profiling technologies in biology, such as imaging mass cytometry
(IMC) and spatial transcriptomics (ST), generate high-dimensional,
multi-channel data with strong spatial alignment and complex inter-channel
relationships. Generative modeling of such data requires jointly capturing
intra- and inter-channel structure, while also generalizing across arbitrary
combinations of observed and missing channels for practical application.
Existing diffusion-based models generally assume low-dimensional inputs (e.g.,
RGB images) and rely on simple conditioning mechanisms that break spatial
correspondence and ignore inter-channel dependencies. This work proposes a
unified diffusion framework for controllable generation over structured and
spatial biological data. Our model contains two key innovations: (1) a
hierarchical feature injection mechanism that enables multi-resolution
conditioning on spatially aligned channels, and (2) a combination of
latent-space and output-space channel-wise attention to capture inter-channel
relationships. To support flexible conditioning and generalization to arbitrary
subsets of observed channels, we train the model using a random masking
strategy, enabling it to reconstruct missing channels from any combination of
inputs. We demonstrate state-of-the-art performance across both spatial and
non-spatial prediction tasks, including protein imputation in IMC and
gene-to-protein prediction in single-cell datasets, and show strong
generalization to unseen conditional configurations.

</details>


### [156] [Harnessing Near-Infrared Spectroscopy and Machine Learning for Traceable Classification of Hanwoo and Holstein Beef](https://arxiv.org/abs/2507.02903)
*AMM Nurul Alam,Abdul Samad,AMM Shamsul Alam,Jahan Ara Monti,Ayesha Muazzam*

Main category: cs.LG

TL;DR: 本研究利用近红外光谱和机器学习技术来区分韩牛牛肉和荷斯坦牛肉，结果表明该方法为肉类真实性检测提供了一种有效手段。


<details>
  <summary>Details</summary>
Motivation: 为了解决食品真实性、错误标签和掺假问题，本研究评估了使用近红外光谱（NIRS）结合先进的机器学习（ML）技术来区分韩牛牛肉（HNB）和荷斯坦牛肉（HLB）。

Method: 使用便携式近红外光谱仪获取快速且非侵入性的光谱数据，结合主成分分析（PCA）进行数据分析，并通过包括线性判别分析（LDA）、支持向量机（SVM）等多种机器学习模型进行优化和验证。

Result: 随机森林提供了最高的预测准确率，受试者工作特征曲线（ROC）下面积（AUC）为0.8826，其次是SVM模型，为0.8747。LR和SVM模型通过有效平衡高准确率、精确率和召回率成为最佳选择。

Conclusion: 集成了近红外光谱和机器学习技术的模型为肉类真实性提供了一种强大而可靠的方法，对检测食品欺诈有显著贡献。

Abstract: This study evaluates the use of Near-Infrared spectroscopy (NIRS) combined
with advanced machine learning (ML) techniques to differentiate Hanwoo beef
(HNB) and Holstein beef (HLB) to address food authenticity, mislabeling, and
adulteration. Rapid and non-invasive spectral data were attained by a portable
NIRS, recording absorbance data within the wavelength range of 700 to 1100 nm.
A total of 40 Longissimus lumborum samples, evenly split between HNB and HLB,
were obtained from a local hypermarket. Data analysis using Principal Component
Analysis (PCA) demonstrated distinct spectral patterns associated with chemical
changes, clearly separating the two beef varieties and accounting for 93.72% of
the total variance. ML models, including Linear Discriminant Analysis (LDA),
Support Vector Machine (SVM), Logistic Regression (LR), Random Forest, Gradient
Boosting (GB), K-Nearest Neighbors, Decision Tree (DT), Naive Bayes (NB), and
Neural Networks (NN), were implemented, optimized through hyperparameter
tuning, and validated by 5-fold cross-validation techniques to enhance model
robustness and prevent overfitting. Random Forest provided the highest
predictive accuracy with a Receiver Operating Characteristic (ROC) Area Under
the Curve (AUC) of 0.8826, closely followed by the SVM model at 0.8747.
Furthermore, GB and NN algorithms exhibited satisfactory performances, with
cross-validation scores of 0.752. Notably, the NN model achieved the highest
recall rate of 0.7804, highlighting its suitability in scenarios requiring
heightened sensitivity. DT and NB exhibited comparatively lower predictive
performance. The LR and SVM models emerged as optimal choices by effectively
balancing high accuracy, precision, and recall. This study confirms that
integrating NIRS with ML techniques offers a powerful and reliable method for
meat authenticity, significantly contributing to detecting food fraud.

</details>


### [157] [Scaling Transformers for Time Series Forecasting: Do Pretrained Large Models Outperform Small-Scale Alternatives?](https://arxiv.org/abs/2507.02907)
*Sanjay Chakraborty,Ibrahim Delibasoglu,Fredrik Heintz*

Main category: cs.LG

TL;DR: 本研究探讨了预训练的大型时间序列模型在预测任务中的表现，并将其与传统模型进行了比较，以确定其优缺点。


<details>
  <summary>Details</summary>
Motivation: 大型预训练模型已在各个领域展示了卓越的能力，但它们在时间序列预测中的有效性仍未得到充分研究。

Method: 分析最先进的（SOTA）预训练通用时间序列模型（例如，Moirai，TimeGPT）以及传统的transformer，评估跨多个基准的准确性、计算效率和可解释性。

Result: 预训练在某些情况下具有优势，而在另一些情况下，较简单的模型仍然具有竞争力。

Conclusion: 预训练的大型时间序列模型（LSTSM）的优势和局限性，以及与特定任务的小型架构相比，它们在时间序列任务中的适用性。

Abstract: Large pre-trained models have demonstrated remarkable capabilities across
domains, but their effectiveness in time series forecasting remains
understudied. This work empirically examines whether pre-trained large-scale
time series models (LSTSMs) trained on diverse datasets can outperform
traditional non-pretrained small-scale transformers in forecasting tasks. We
analyze state-of-the-art (SOTA) pre-trained universal time series models (e.g.,
Moirai, TimeGPT) alongside conventional transformers, evaluating accuracy,
computational efficiency, and interpretability across multiple benchmarks. Our
findings reveal the strengths and limitations of pre-trained LSTSMs, providing
insights into their suitability for time series tasks compared to task-specific
small-scale architectures. The results highlight scenarios where pretraining
offers advantages and where simpler models remain competitive.

</details>


### [158] [Hyperbolic Kernel Graph Neural Networks for Neurocognitive Decline Analysis from Multimodal Brain Imaging](https://arxiv.org/abs/2507.02908)
*Meimei Yang,Yongheng Sun,Qianqian Wang,Andrea Bozoki,Maureen Kohi,Mingxia Liu*

Main category: cs.LG

TL;DR: This paper presents a hyperbolic kernel graph fusion (HKGF) framework for neurocognitive decline analysis with multimodal neuroimages.


<details>
  <summary>Details</summary>
Motivation: existing studies suggest that fusing these multimodal data helps detect abnormal brain activity caused by neurocognitive decline, they are generally implemented in Euclidean space and can't effectively capture intrinsic hierarchical organization of structural/functional brain networks.

Method: a hyperbolic kernel graph fusion (HKGF) framework for neurocognitive decline analysis with multimodal neuroimages. It consists of a multimodal graph construction module, a graph representation learning module that encodes brain graphs in hyperbolic space through a family of hyperbolic kernel graph neural networks (HKGNNs), a cross-modality coupling module that enables effective multimodal data fusion, and a hyperbolic neural network for downstream predictions.

Result: Extensive experiments involving over 4,000 subjects with DTI and/or fMRI data suggest the superiority of HKGF over state-of-the-art methods in two neurocognitive decline prediction tasks.

Conclusion: HKGF is a general framework for multimodal data analysis, facilitating objective quantification of structural/functional brain connectivity changes associated with neurocognitive decline.

Abstract: Multimodal neuroimages, such as diffusion tensor imaging (DTI) and
resting-state functional MRI (fMRI), offer complementary perspectives on brain
activities by capturing structural or functional interactions among brain
regions. While existing studies suggest that fusing these multimodal data helps
detect abnormal brain activity caused by neurocognitive decline, they are
generally implemented in Euclidean space and can't effectively capture
intrinsic hierarchical organization of structural/functional brain networks.
This paper presents a hyperbolic kernel graph fusion (HKGF) framework for
neurocognitive decline analysis with multimodal neuroimages. It consists of a
multimodal graph construction module, a graph representation learning module
that encodes brain graphs in hyperbolic space through a family of hyperbolic
kernel graph neural networks (HKGNNs), a cross-modality coupling module that
enables effective multimodal data fusion, and a hyperbolic neural network for
downstream predictions. Notably, HKGNNs represent graphs in hyperbolic space to
capture both local and global dependencies among brain regions while preserving
the hierarchical structure of brain networks. Extensive experiments involving
over 4,000 subjects with DTI and/or fMRI data suggest the superiority of HKGF
over state-of-the-art methods in two neurocognitive decline prediction tasks.
HKGF is a general framework for multimodal data analysis, facilitating
objective quantification of structural/functional brain connectivity changes
associated with neurocognitive decline.

</details>


### [159] [Beyond Token Pruning: Operation Pruning in Vision-Language Models](https://arxiv.org/abs/2507.02909)
*Aoming Liu,Reuben Tan,Boqing Gong,Bryan A. Plummer*

Main category: cs.LG

TL;DR: GSOP prunes operations in VLMs for efficiency, outperforming token pruning methods.


<details>
  <summary>Details</summary>
Motivation: Prior Vision Language Model (VLM) token pruning reduces computation by eliminating attention and feed-forward operations for pruned tokens while maintaining all operations for critical tokens. However, this binary approach conflates token/operation redundancy - critical operations may be removed along with discarded tokens, while preserved tokens retain all potentially redundant operations. To surgically eliminate redundant operations while preserving critical ones

Method: propose Greedily Sorted Operation Pruning (GSOP), a data-driven method that directly prunes operations rather than tokens. GSOP first decomposes a VLM decoder's computations into atomic operations along three dimensions: token groups, layer positions, and computation modules. GSOP determines the pruning order of operations through greedy sorting: GSOP iteratively selects the redundant operation that incurs minimal performance drop considering previously pruned operations.

Result: GSOP enhances sorting efficiency through: a) leveraging historical operation rankings to avoid redundant evaluations; b) excluding the ``free-to-prune" and ``danger-to-prune" operations from sorting.

Conclusion: GSOP achieves compelling efficiency-performance tradeoffs, reducing computation by 70% with only 4% performance loss while maintaining up to 18% higher performance than state-of-the-art methods when transferred across diverse VLMs and tasks. Real GPU efficiency evaluations confirm its practical value.

Abstract: Prior Vision Language Model (VLM) token pruning reduces computation by
eliminating attention and feed-forward operations for pruned tokens while
maintaining all operations for critical tokens. However, this binary approach
conflates token/operation redundancy - critical operations may be removed along
with discarded tokens, while preserved tokens retain all potentially redundant
operations. To surgically eliminate redundant operations while preserving
critical ones, we propose Greedily Sorted Operation Pruning (GSOP), a
data-driven method that directly prunes operations rather than tokens. GSOP
first decomposes a VLM decoder's computations into atomic operations along
three dimensions: token groups, layer positions, and computation modules. GSOP
determines the pruning order of operations through greedy sorting: GSOP
iteratively selects the redundant operation that incurs minimal performance
drop considering previously pruned operations. Different computational budgets
can be accommodated without re-searching by simply pruning operations according
to this order until the desired budget is met. GSOP enhances sorting efficiency
through: a) leveraging historical operation rankings to avoid redundant
evaluations; b) excluding the ``free-to-prune" and ``danger-to-prune"
operations from sorting. GSOP achieves compelling efficiency-performance
tradeoffs, reducing computation by 70% with only 4% performance loss while
maintaining up to 18% higher performance than state-of-the-art methods when
transferred across diverse VLMs and tasks. Real GPU efficiency evaluations
confirm its practical value. The code is in
https://github.com/zxcvfd13502/GSOP.

</details>


### [160] [Causal-Paced Deep Reinforcement Learning](https://arxiv.org/abs/2507.02910)
*Geonwoo Cho,Jaegyun Im,Doyoon Kim,Sundong Kim*

Main category: cs.LG

TL;DR: This paper proposes Causal-Paced Deep Reinforcement Learning (CP-DRL) to improve curriculum reinforcement learning by leveraging causal relationships between tasks. CP-DRL outperforms existing methods in several benchmarks.


<details>
  <summary>Details</summary>
Motivation: A key challenge in CRL is to identify tasks that promote exploration, yet are similar enough to support effective transfer. While recent approach suggests comparing tasks via their Structural Causal Models (SCMs), the method requires access to ground-truth causal structures, an unrealistic assumption in most RL settings.

Method: Causal-Paced Deep Reinforcement Learning (CP-DRL), a curriculum learning framework aware of SCM differences between tasks based on interaction data approximation. This signal captures task novelty, which we combine with the agent's learnability, measured by reward gain, to form a unified objective.

Result: CP-DRL outperforms existing curriculum methods on the Point Mass benchmark, achieving faster convergence and higher returns. CP-DRL demonstrates reduced variance with comparable final returns in the Bipedal Walker-Trivial setting, and achieves the highest average performance in the Infeasible variant.

Conclusion: Leveraging causal relationships between tasks can improve the structure-awareness and sample efficiency of curriculum reinforcement learning.

Abstract: Designing effective task sequences is crucial for curriculum reinforcement
learning (CRL), where agents must gradually acquire skills by training on
intermediate tasks. A key challenge in CRL is to identify tasks that promote
exploration, yet are similar enough to support effective transfer. While recent
approach suggests comparing tasks via their Structural Causal Models (SCMs),
the method requires access to ground-truth causal structures, an unrealistic
assumption in most RL settings. In this work, we propose Causal-Paced Deep
Reinforcement Learning (CP-DRL), a curriculum learning framework aware of SCM
differences between tasks based on interaction data approximation. This signal
captures task novelty, which we combine with the agent's learnability, measured
by reward gain, to form a unified objective. Empirically, CP-DRL outperforms
existing curriculum methods on the Point Mass benchmark, achieving faster
convergence and higher returns. CP-DRL demonstrates reduced variance with
comparable final returns in the Bipedal Walker-Trivial setting, and achieves
the highest average performance in the Infeasible variant. These results
indicate that leveraging causal relationships between tasks can improve the
structure-awareness and sample efficiency of curriculum reinforcement learning.
We provide the full implementation of CP-DRL to facilitate the reproduction of
our main results at https://github.com/Cho-Geonwoo/CP-DRL.

</details>


### [161] [DiceHuBERT: Distilling HuBERT with a Self-Supervised Learning Objective](https://arxiv.org/abs/2507.02911)
*Hyung Gun Chi,Zakaria Aldeneh,Tatiana Likhomanenko,Oggi Rudovic,Takuya Higuchi,Li-Wei Chen,Shinji Watanabe,Ahmed Hussen Abdelaziz*

Main category: cs.LG

TL;DR: DiceHuBERT是一种知识蒸馏框架，用于压缩HuBERT，在SUPERB上优于现有方法，音素识别性能提高了21%以上，ASR性能提高了14%以上。


<details>
  <summary>Details</summary>
Motivation: 压缩广泛使用的基于自监督学习(SSL)的语音基础模型HuBERT。

Method: DiceHuBERT利用HuBERT的迭代自蒸馏机制，通过直接用学生模型替换原始模型。

Result: 在SUPERB上，DiceHuBERT始终优于现有的蒸馏方法，音素识别性能提高了21%以上，ASR性能提高了14%以上。

Conclusion: DiceHuBERT在多个任务中表现出竞争优势。

Abstract: We introduce DiceHuBERT, a knowledge distillation framework for compressing
HuBERT, a widely used self-supervised learning (SSL)-based speech foundation
model. Unlike existing distillation methods that rely on layer-wise and
feature-wise mapping between teacher and student models, DiceHuBERT leverages
HuBERT's iterative self-distillation mechanism by directly replacing the
original model with a student model. This replacement allows the student to be
trained using the same SSL objective used when pre-training HuBERT, eliminating
the need for additional modules or architectural constraints. Experimental
results on SUPERB show that DiceHuBERT consistently outperforms existing
distillation methods, improving phoneme recognition performance by over 21% and
ASR performance by more than 14%. Furthermore, DiceHuBERT demonstrates
competitive performance across multiple tasks, highlighting its clear
advantage.

</details>


### [162] [Multicollinearity Resolution Based on Machine Learning: A Case Study of Carbon Emissions](https://arxiv.org/abs/2507.02912)
*Xuanming Zhang*

Main category: cs.LG

TL;DR: integrates DBSCAN clustering with the Elastic Net regression model to address multifactorial problems characterized by structural complexity and multicollinearity, exemplified by carbon emissions analysis


<details>
  <summary>Details</summary>
Motivation: to address multifactorial problems characterized by structural complexity and multicollinearity, exemplified by carbon emissions analysis

Method: an analytical framework that integrates DBSCAN clustering with the Elastic Net regression model

Result: Applying this framework to energy consumption data from 46 industries in China (2000-2019) resulted in the identification of 16 categories. Emission characteristics and drivers were quantitatively assessed for each category, demonstrating the framework's capacity to identify primary emission sources and provide actionable insights.

Conclusion: This research underscores the global applicability of the framework for analyzing complex regional challenges, such as carbon emissions, and highlights qualitative features that humans find meaningful may not be accurate for the model.

Abstract: This study proposes an analytical framework that integrates DBSCAN clustering
with the Elastic Net regression model to address multifactorial problems
characterized by structural complexity and multicollinearity, exemplified by
carbon emissions analysis. DBSCAN is employed for unsupervised learning to
objectively cluster features, while the Elastic Net is utilized for
high-dimensional feature selection and complexity control. The Elastic Net is
specifically chosen for its ability to balance feature selection and
regularization by combining L1 (lasso) and L2 (ridge) penalties, making it
particularly suited for datasets with correlated predictors. Applying this
framework to energy consumption data from 46 industries in China (2000-2019)
resulted in the identification of 16 categories. Emission characteristics and
drivers were quantitatively assessed for each category, demonstrating the
framework's capacity to identify primary emission sources and provide
actionable insights. This research underscores the global applicability of the
framework for analyzing complex regional challenges, such as carbon emissions,
and highlights qualitative features that humans find meaningful may not be
accurate for the model.

</details>


### [163] [Efficient Certified Reasoning for Binarized Neural Networks](https://arxiv.org/abs/2507.02916)
*Jiong Yang,Yong Kiam Tan,Mate Soos,Magnus O. Myreen,Kuldeep S. Meel*

Main category: cs.LG

TL;DR: Presents a scalable and trustworthy approach for BNN verification, achieving significant speedups and coverage improvements over existing methods through native BNN constraint representation and specialized proof generation.


<details>
  <summary>Details</summary>
Motivation: Existing methods for BNN analysis suffer from either limited scalability or susceptibility to soundness errors, hindering their applicability in real-world scenarios. BNNs are well-suited for safety-critical tasks but need better analysis methods.

Method: Introduces a native representation of BNN constraints in a custom-designed solver for qualitative reasoning, and in an approximate model counter for quantitative reasoning. It further develops specialized proof generation and checking pipelines with native support for BNN constraint reasoning.

Result: Achieves a 9x speedup in certified solving and a 218x speedup in certified counting compared to prior approaches. Produces fully certified results for 99% and 86% of qualitative and quantitative reasoning queries, respectively, significantly outperforming existing baselines.

Conclusion: This work presents a scalable and trustworthy approach for both qualitative and quantitative verification of BNNs, with native BNN constraint representation in a custom solver and approximate model counter. Specialized proof generation and checking pipelines ensure trustworthiness.

Abstract: Neural networks have emerged as essential components in safety-critical
applications -- these use cases demand complex, yet trustworthy computations.
Binarized Neural Networks (BNNs) are a type of neural network where each neuron
is constrained to a Boolean value; they are particularly well-suited for
safety-critical tasks because they retain much of the computational capacities
of full-scale (floating-point or quantized) deep neural networks, but remain
compatible with satisfiability solvers for qualitative verification and with
model counters for quantitative reasoning. However, existing methods for BNN
analysis suffer from either limited scalability or susceptibility to soundness
errors, which hinders their applicability in real-world scenarios.
  In this work, we present a scalable and trustworthy approach for both
qualitative and quantitative verification of BNNs. Our approach introduces a
native representation of BNN constraints in a custom-designed solver for
qualitative reasoning, and in an approximate model counter for quantitative
reasoning. We further develop specialized proof generation and checking
pipelines with native support for BNN constraint reasoning, ensuring
trustworthiness for all of our verification results. Empirical evaluations on a
BNN robustness verification benchmark suite demonstrate that our certified
solving approach achieves a $9\times$ speedup over prior certified CNF and
PB-based approaches, and our certified counting approach achieves a $218\times$
speedup over the existing CNF-based baseline. In terms of coverage, our
pipeline produces fully certified results for $99\%$ and $86\%$ of the
qualitative and quantitative reasoning queries on BNNs, respectively. This is
in sharp contrast to the best existing baselines which can fully certify only
$62\%$ and $4\%$ of the queries, respectively.

</details>


### [164] [Echo State Transformer: When chaos brings memory](https://arxiv.org/abs/2507.02917)
*Yannis Bendi-Ouis,Xavier Hinaut*

Main category: cs.LG

TL;DR: Introduce Echo State Transformers (EST), a hybrid architecture that integrates the Transformer attention mechanisms with principles from Reservoir Computing to create a fixedsize window distributed memory system, which resolves the quadratic scaling problem of standard Transformers and demonstrates exceptional performance in low-data regimes.


<details>
  <summary>Details</summary>
Motivation: Large Language Models do not reflect how our brain processes and learns and sequential data processing with Transformers encounters a fundamental barrier: quadratic complexity growth with sequence length. create more efficient models that are less reliant on intensive computations and massive volumes of data

Method: integrates the Transformer attention mechanisms with principles from Reservoir Computing to create a fixedsize window distributed memory system. integrate a new module called ''Working Memory'' based on several reservoirs working in parallel. the classical reservoir hyperparameters controlling the dynamics are now trained

Result: EST outperforms GRUs, LSTMs, and even Transformers on 8 of 12 diverse sequential processing tasks of the STREAM benchmark. EST achieves constant computational complexity at each processing step

Conclusion: Echo State Transformers (EST) can be an effective replacement to GRUs and LSTMs while complementing standard Transformers at least on resource-constrained environments and low-data scenarios across diverse sequential processing tasks.

Abstract: While Large Language Models and their underlying Transformer architecture are
remarkably efficient, they do not reflect how our brain processes and learns a
diversity of cognitive tasks such as language and working memory. Furthermore,
sequential data processing with Transformers encounters a fundamental barrier:
quadratic complexity growth with sequence length. Motivated by these
limitations, our ambition is to create more efficient models that are less
reliant on intensive computations and massive volumes of data. We introduce
Echo State Transformers (EST), a hybrid architecture that elegantly resolves
this challenge while demonstrating exceptional performance in low-data regimes.
EST integrates the Transformer attention mechanisms with principles from
Reservoir Computing to create a fixedsize window distributed memory system.
Drawing inspiration from Echo State Networks, the most prominent instance of
the Reservoir Computing paradigm, our architecture integrates a new module
called ''Working Memory'' based on several reservoirs (i.e. random recurrent
networks) working in parallel. These reservoirs work as independent memory
units with distinct internal dynamics. A novelty here is that the classical
reservoir hyperparameters controlling the dynamics are now trained. Thus, the
EST dynamically adapts the memory/non-linearity trade-off in reservoirs. By
maintaining a fixed number of memory units regardless of sequence length, EST
achieves constant computational complexity at each processing step, effectively
breaking the quadratic scaling problem of standard Transformers. Evaluations on
the STREAM benchmark, which comprises 12 diverse sequential processing tasks,
demonstrate that EST outperforms GRUs, LSTMs, and even Transformers on 8 of
these tasks. These findings highlight that Echo State Transformers can be an
effective replacement to GRUs and LSTMs while complementing standard
Transformers at least on resource-constrained environments and low-data
scenarios across diverse sequential processing tasks.

</details>


### [165] [PlaceFM: A Training-free Geospatial Foundation Model of Places](https://arxiv.org/abs/2507.02921)
*Mohammad Hashemi,Hossein Amiri,Andreas Zufle*

Main category: cs.LG

TL;DR: PlaceFM是一种空间基础模型，它使用无训练图压缩方法捕获地点表示，为多尺度地理空间分析提供可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 空间结构对于有效的地理空间情报系统至关重要，而基础模型通常缺乏推理地点的灵活性。

Method: 使用一种无需训练的图压缩方法捕获地点表征。

Result: PlaceFM压缩了从美国集成的Foursquare和OpenStreetMap数据构建的全国POI图，生成了通用的地点嵌入。

Conclusion: PlaceFM提供了一个可扩展且适应性强的多尺度地理空间分析解决方案，无需预训练。

Abstract: Spatial structure is central to effective geospatial intelligence systems.
While foundation models have shown promise, they often lack the flexibility to
reason about places, which are context-rich regions spanning different spatial
granularities. We propose PlaceFM, a spatial foundation model that captures
place representations using a training-free graph condensation method. PlaceFM
condenses a nationwide POI graph built from integrated Foursquare and
OpenStreetMap data in the U.S., generating general-purpose embeddings of
places. These embeddings can be seamlessly integrated into geolocation data
pipelines to support a wide range of downstream tasks. Without requiring
pretraining, PlaceFM offers a scalable and adaptable solution for multi-scale
geospatial analysis.

</details>


### [166] [Domain Knowledge in Artificial Intelligence: Using Conceptual Modeling to Increase Machine Learning Accuracy and Explainability](https://arxiv.org/abs/2507.02922)
*V. C. Storey,J. Parsons,A. Castellanos,M. Tremblay,R. Lukyanenko,W. Maass,A. Castillo*

Main category: cs.LG

TL;DR: This research proposes using the domain knowledge represented in conceptual models to improve the preparation of the data used to train machine learning models.


<details>
  <summary>Details</summary>
Motivation: Machine learning continues to suffer from performance and transparency issues. These challenges can be partially attributed to the limited use of domain knowledge by machine learning models.

Method: We develop and demonstrate a method, called the Conceptual Modeling for Machine Learning (CMML), which is comprised of guidelines for data preparation in machine learning and based on conceptual modeling constructs and principles.

Result: To assess the impact of CMML on machine learning outcomes, we first applied it to two real-world problems to evaluate its impact on model performance. We then solicited an assessment by data scientists on the applicability of the method.

Conclusion: The results demonstrate the value of CMML for improving machine learning outcomes.

Abstract: Machine learning enables the extraction of useful information from large,
diverse datasets. However, despite many successful applications, machine
learning continues to suffer from performance and transparency issues. These
challenges can be partially attributed to the limited use of domain knowledge
by machine learning models. This research proposes using the domain knowledge
represented in conceptual models to improve the preparation of the data used to
train machine learning models. We develop and demonstrate a method, called the
Conceptual Modeling for Machine Learning (CMML), which is comprised of
guidelines for data preparation in machine learning and based on conceptual
modeling constructs and principles. To assess the impact of CMML on machine
learning outcomes, we first applied it to two real-world problems to evaluate
its impact on model performance. We then solicited an assessment by data
scientists on the applicability of the method. These results demonstrate the
value of CMML for improving machine learning outcomes.

</details>


### [167] [Large Language Model Agent for Modular Task Execution in Drug Discovery](https://arxiv.org/abs/2507.02925)
*Janghoon Ock,Radheesh Sharma Meda,Srivathsan Badrinarayanan,Neha S. Aluru,Achuth Chandrasekhar,Amir Barati Farimani*

Main category: cs.LG

TL;DR: This paper presents a modular framework powered by large language models (LLMs) that automates and streamlines key tasks across the early-stage computational drug discovery pipeline.


<details>
  <summary>Details</summary>
Motivation: automates and streamlines key tasks across the early-stage computational drug discovery pipeline

Method: combining LLM reasoning with domain-specific tools

Result: the agent autonomously retrieved relevant biomolecular information and answered mechanistic questions with improved contextual accuracy over standard LLMs. It then generated chemically diverse seed molecules and predicted 67 ADMET-related properties, which guided iterative molecular refinement. Across two refinement rounds, the number of molecules with QED > 0.6 increased from 34 to 55, and those passing at least four out of five empirical drug-likeness rules rose from 29 to 52, within a pool of 194 molecules

Conclusion: The framework effectively supports molecular screening, prioritization, and structure evaluation. Its modular design enables flexible integration of evolving tools and models, providing a scalable foundation for AI-assisted therapeutic discovery.

Abstract: We present a modular framework powered by large language models (LLMs) that
automates and streamlines key tasks across the early-stage computational drug
discovery pipeline. By combining LLM reasoning with domain-specific tools, the
framework performs biomedical data retrieval, domain-specific question
answering, molecular generation, property prediction, property-aware molecular
refinement, and 3D protein-ligand structure generation. In a case study
targeting BCL-2 in lymphocytic leukemia, the agent autonomously retrieved
relevant biomolecular information-including FASTA sequences, SMILES
representations, and literature-and answered mechanistic questions with
improved contextual accuracy over standard LLMs. It then generated chemically
diverse seed molecules and predicted 67 ADMET-related properties, which guided
iterative molecular refinement. Across two refinement rounds, the number of
molecules with QED > 0.6 increased from 34 to 55, and those passing at least
four out of five empirical drug-likeness rules rose from 29 to 52, within a
pool of 194 molecules. The framework also employed Boltz-2 to generate 3D
protein-ligand complexes and provide rapid binding affinity estimates for
candidate compounds. These results demonstrate that the approach effectively
supports molecular screening, prioritization, and structure evaluation. Its
modular design enables flexible integration of evolving tools and models,
providing a scalable foundation for AI-assisted therapeutic discovery.

</details>


### [168] [MolProphecy: Bridging Medicinal Chemists' Knowledge and Molecular Pre-Trained Models via a Multi-Modal Framework](https://arxiv.org/abs/2507.02932)
*Jianping Zhao,Qiong Zhou,Tian Wang,Yusi Fan,Qian Yang,Li Jiao,Chang Liu,Zhehao Guo,Qi Lu,Fengfeng Zhou,Ruochi Zhang*

Main category: cs.LG

TL;DR: MolProphecy是一个人机协作的多模态框架，旨在将化学家的领域知识整合到分子性质预测模型中。


<details>
  <summary>Details</summary>
Motivation: 现有的分子预训练模型通常无法捕捉到专家驱动的分子设计中的隐性推理。

Method: MolProphecy采用ChatGPT作为虚拟化学家来模拟专家级别的推理和决策，并通过门控交叉注意力机制将化学家知识和基于图的分子特征融合。

Result: 化学家知识和结构特征提供了互补的贡献，提高了准确性和可解释性。

Conclusion: MolProphecy在四个基准数据集上优于当前最优模型，在FreeSolv上的RMSE降低了15.0%，在BACE上的AUROC提高了5.39%。

Abstract: MolProphecy is a human-in-the-loop (HITL) multi-modal framework designed to
integrate chemists' domain knowledge into molecular property prediction models.
While molecular pre-trained models have enabled significant gains in predictive
accuracy, they often fail to capture the tacit, interpretive reasoning central
to expert-driven molecular design. To address this, MolProphecy employs ChatGPT
as a virtual chemist to simulate expert-level reasoning and decision-making.
The generated chemist knowledge is embedded by the large language model (LLM)
as a dedicated knowledge representation and then fused with graph-based
molecular features through a gated cross-attention mechanism, enabling joint
reasoning over human-derived and structural features. Evaluated on four
benchmark datasets (FreeSolv, BACE, SIDER, and ClinTox), MolProphecy
outperforms state-of-the-art (SOTA) models, achieving a 15.0 percent reduction
in RMSE on FreeSolv and a 5.39 percent improvement in AUROC on BACE. Analysis
reveals that chemist knowledge and structural features provide complementary
contributions, improving both accuracy and interpretability. MolProphecy offers
a practical and generalizable approach for collaborative drug discovery, with
the flexibility to incorporate real chemist input in place of the current
simulated proxy--without the need for model retraining. The implementation is
publicly available at https://github.com/zhangruochi/MolProphecy.

</details>


### [169] [Predictive Maintenance Optimization for Smart Vending Machines Using IoT and Machine Learning](https://arxiv.org/abs/2507.02934)
*Md. Nisharul Hasan*

Main category: cs.LG

TL;DR: This research presents a novel predictive maintenance framework tailored for vending machines by leveraging Internet of Things (IoT) sensors and machine learning (ML) algorithms to forecast failures before they occur.


<details>
  <summary>Details</summary>
Motivation: Traditional maintenance approaches are limited in their ability to preempt machine failures, leading to unplanned downtimes and elevated service costs.

Method: leveraging Internet of Things (IoT) sensors and machine learning (ML) algorithms

Result: significant improvement in early fault detection and a reduction in redundant service interventions.

Conclusion: predictive maintenance systems can transform operational efficiency and service reliability.

Abstract: The increasing proliferation of vending machines in public and commercial
environments has placed a growing emphasis on operational efficiency and
customer satisfaction. Traditional maintenance approaches either reactive or
time-based preventive are limited in their ability to preempt machine failures,
leading to unplanned downtimes and elevated service costs. This research
presents a novel predictive maintenance framework tailored for vending machines
by leveraging Internet of Things (IoT) sensors and machine learning (ML)
algorithms. The proposed system continuously monitors machine components and
operating conditions in real time and applies predictive models to forecast
failures before they occur. This enables timely maintenance scheduling,
minimizing downtime and extending machine lifespan. The framework was validated
through simulated fault data and performance evaluation using classification
algorithms. Results show a significant improvement in early fault detection and
a reduction in redundant service interventions. The findings indicate that
predictive maintenance systems, when integrated into vending infrastructure,
can transform operational efficiency and service reliability.

</details>


### [170] [FoGE: Fock Space inspired encoding for graph prompting](https://arxiv.org/abs/2507.02937)
*Sotirios Panagiotis Chytas,Rudrasis Chakraborty,Vikas Singh*

Main category: cs.LG

TL;DR: 该论文提出了一种基于 Fock 空间表示的无参数图编码器，用于增强 LLM 在回答与图相关问题方面的能力，并在各种图结构上表现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方案通常使用图的某些描述来创建“增强”提示，并将其馈送到 LLM。对于一类选定的图，如果部署一个量身定制的图编码器与预训练的 LLM 协同工作，则该模型可以很好地回答与图相关的问题。现有的基于图的提示解决方案范围从图序列化到图转换器。

Method: 使用基于 Fock 空间表示的无参数图编码器，这是一个从数学物理学中借用的概念。

Result: 该模型显著简化了现有解决方案，并且可以毫不费力地推广到多个不同的基于图的结构。

Conclusion: 该模型能够有效地回答与图相关的问题，从简单图到蛋白质到超图，无需对架构进行最小的调整。

Abstract: Recent results show that modern Large Language Models (LLM) are indeed
capable of understanding and answering questions about structured data such as
graphs. This new paradigm can lead to solutions that require less supervision
while, at the same time, providing a model that can generalize and answer
questions beyond the training labels. Existing proposals often use some
description of the graph to create an ``augmented'' prompt fed to the LLM. For
a chosen class of graphs, if a well-tailored graph encoder is deployed to play
together with a pre-trained LLM, the model can answer graph-related questions
well. Existing solutions to graph-based prompts range from graph serialization
to graph transformers. In this work, we show that the use of a parameter-free
graph encoder based on Fock space representations, a concept borrowed from
mathematical physics, is remarkably versatile in this problem setting. The
simple construction, inherited directly from the theory with a few small
adjustments, can provide rich and informative graph encodings, for a wide range
of different graphs. We investigate the use of this idea for prefix-tuned
prompts leveraging the capabilities of a pre-trained, frozen LLM. The
modifications lead to a model that can answer graph-related questions -- from
simple graphs to proteins to hypergraphs -- effectively and with minimal, if
any, adjustments to the architecture. Our work significantly simplifies
existing solutions and generalizes well to multiple different graph-based
structures effortlessly.

</details>


### [171] [Frequency-Aligned Knowledge Distillation for Lightweight Spatiotemporal Forecasting](https://arxiv.org/abs/2507.02939)
*Yuqi Li,Chuanguang Yang,Hansheng Zeng,Zeyu Dong,Zhulin An,Yongjun Xu,Yingli Tian,Hao Wu*

Main category: cs.LG

TL;DR: 提出SDKD，一种用于时空预测的轻量级知识蒸馏框架，可有效降低计算复杂度并提高性能。


<details>
  <summary>Details</summary>
Motivation: 时空预测任务通常需要复杂的模型，这些模型存在训练效率低和内存消耗高的问题。

Method: 提出了一种轻量级框架，即光谱解耦知识蒸馏（SDKD），它将复杂教师模型的多尺度时空表示转移到更高效的轻量级学生网络。

Result: SDKD框架有效地捕捉了高频变化和长期趋势，同时降低了计算复杂度，在Navier-Stokes方程数据集上，MSE降低了81.3%，MAE降低了52.3%。

Conclusion: SDKD框架在Navier-Stokes方程数据集上显著提高了性能，MSE降低了81.3%，MAE降低了52.3%，同时降低了计算复杂度。

Abstract: Spatiotemporal forecasting tasks, such as traffic flow, combustion dynamics,
and weather forecasting, often require complex models that suffer from low
training efficiency and high memory consumption. This paper proposes a
lightweight framework, Spectral Decoupled Knowledge Distillation (termed SDKD),
which transfers the multi-scale spatiotemporal representations from a complex
teacher model to a more efficient lightweight student network. The teacher
model follows an encoder-latent evolution-decoder architecture, where its
latent evolution module decouples high-frequency details and low-frequency
trends using convolution and Transformer (global low-frequency modeler).
However, the multi-layer convolution and deconvolution structures result in
slow training and high memory usage. To address these issues, we propose a
frequency-aligned knowledge distillation strategy, which extracts multi-scale
spectral features from the teacher's latent space, including both high and low
frequency components, to guide the lightweight student model in capturing both
local fine-grained variations and global evolution patterns. Experimental
results show that SDKD significantly improves performance, achieving reductions
of up to 81.3% in MSE and in MAE 52.3% on the Navier-Stokes equation dataset.
The framework effectively captures both high-frequency variations and long-term
trends while reducing computational complexity. Our codes are available at
https://github.com/itsnotacie/SDKD

</details>


### [172] [Beyond Parallelism: Synergistic Computational Graph Effects in Multi-Head Attention](https://arxiv.org/abs/2507.02944)
*Haitz Sáez de Ocáriz Borde*

Main category: cs.LG

TL;DR: Multi-head attention's theoretical advantages are explored, showing synergistic enhancement of information propagation under specific conditions, verified empirically.


<details>
  <summary>Details</summary>
Motivation: The theoretical advantages of multi-head versus single-head attention, beyond mere parallel processing, remain underexplored.

Method: Reframing multi-head attention as a system of potentially synergistic computational graphs, where each head functions as a feedforward directed acyclic graph (DAG) with a common sink state; theoretical analysis of mixing time and minimax fidelity.

Result: Multi-head attention can synergistically enhance information propagation, yielding faster mixing times and minimax fidelity amplification under specific head-diversity conditions.

Conclusion: Multi-head attention can synergistically enhance information propagation, yielding faster mixing times and minimax fidelity amplification under specific head-diversity conditions, which is empirically verified by training single-head and multi-head Transformers on sequence manipulation tasks.

Abstract: Multi-head attention powers Transformer networks, the primary deep learning
architecture behind the success of large language models (LLMs). Yet, the
theoretical advantages of multi-head versus single-head attention, beyond mere
parallel processing, remain underexplored. In this paper, we reframe multi-head
attention as a system of potentially synergistic computational graphs, where
each head functions as a feedforward directed acyclic graph (DAG) with a common
sink state. We provide intuition and preliminary theoretical analysis of mixing
time and minimax fidelity in this framework. Our results show that multi-head
attention can synergistically enhance information propagation, yielding faster
mixing times and minimax fidelity amplification under specific head-diversity
conditions. Finally, we train single-head and multi-head Transformers, each
with the same total number of parameters, on sequence manipulation tasks and
empirically verify the predicted effects.

</details>


### [173] [InvisibleInk: High-Utility and Low-Cost Text Generation with Differential Privacy](https://arxiv.org/abs/2507.02974)
*Vishnu Vinod,Krishna Pillutla,Abhradeep Guha Thakurta*

Main category: cs.LG

TL;DR: InvisibleInk 是一个高度可扩展的长格式文本生成框架，满足关于敏感参考的严格差分隐私保证。


<details>
  <summary>Details</summary>
Motivation: 安全地将私人信息纳入生成仍然是一个关键的开放性问题。

Method: 将来自 LLM 的下一个 token 分布的采样解释为 LLM logits 上的指数机制，具有两项创新：1. 通过隔离和裁剪模型 logits 中仅包含的敏感信息（相对于公共 logits）来降低隐私成本。2. 通过从 top-k 个私有 token 的小超集中抽样来提高文本质量。

Result: 与最先进的基线相比，在各种隐私级别上生成相同效用的长格式私有文本时，计算成本始终降低 8 倍。

Conclusion: InvisibleInk能够以低于非私有生成计算成本 10 倍的代价生成私有长文本。

Abstract: As major progress in LLM-based long-form text generation enables paradigms
such as retrieval-augmented generation (RAG) and inference-time scaling, safely
incorporating private information into the generation remains a critical open
question. We present InvisibleInk, a highly scalable long-form text generation
framework satisfying rigorous differential privacy guarantees with respect to
the sensitive references. It interprets sampling from the LLM's
next-token-distribution as the exponential mechanism over the LLM logits with
two innovations. First, we reduce the privacy cost by isolating and clipping
only the sensitive information in the model logits (relative to the public
logits). Second, we improve text quality by sampling from a small superset of
the top-$k$ private tokens. Empirical evaluations demonstrate a consistent
$8\times$ reduction in computation cost over state-of-the-art baselines to
generate long-form private text of the same utility across privacy levels. In
summary, InvisibleInk is able to generate private long-form text at less than
$10\times$ the computation cost of non-private generation.

</details>


### [174] [Introducing Answered with Evidence -- a framework for evaluating whether LLM responses to biomedical questions are founded in evidence](https://arxiv.org/abs/2507.02975)
*Julian D Baldwin,Christina Dinh,Arjun Mukerji,Neil Sanghavi,Saurabh Gombar*

Main category: cs.LG

TL;DR: This paper introduces Answered with Evidence, a framework for evaluating whether LLM-generated answers are grounded in scientific literature. The results show that combining PubMed-based systems with a novel evidence source enables reliable answers to over 70% of biomedical queries.


<details>
  <summary>Details</summary>
Motivation: The growing use of large language models (LLMs) for biomedical question answering raises concerns about the accuracy and evidentiary support of their responses.

Method: comparative pipeline that included: (1) Alexandria, fka the Atropos Evidence Library, a retrieval-augmented generation (RAG) system based on novel observational studies, and (2) two PubMed-based retrieval-augmented systems (System and Perplexity)

Result: PubMed-based systems provided evidence-supported answers for approximately 44% of questions, while the novel evidence source did so for about 50%. Combined, these sources enabled reliable answers to over 70% of biomedical queries.

Conclusion: PubMed-based systems provided evidence-supported answers for approximately 44% of questions, while the novel evidence source did so for about 50%. Combined, these sources enabled reliable answers to over 70% of biomedical queries.

Abstract: The growing use of large language models (LLMs) for biomedical question
answering raises concerns about the accuracy and evidentiary support of their
responses. To address this, we present Answered with Evidence, a framework for
evaluating whether LLM-generated answers are grounded in scientific literature.
We analyzed thousands of physician-submitted questions using a comparative
pipeline that included: (1) Alexandria, fka the Atropos Evidence Library, a
retrieval-augmented generation (RAG) system based on novel observational
studies, and (2) two PubMed-based retrieval-augmented systems (System and
Perplexity). We found that PubMed-based systems provided evidence-supported
answers for approximately 44% of questions, while the novel evidence source did
so for about 50%. Combined, these sources enabled reliable answers to over 70%
of biomedical queries. As LLMs become increasingly capable of summarizing
scientific content, maximizing their value will require systems that can
accurately retrieve both published and custom-generated evidence or generate
such evidence in real time.

</details>


### [175] [Physics Augmented Machine Learning Discovery of Composition-Dependent Constitutive Laws for 3D Printed Digital Materials](https://arxiv.org/abs/2507.02991)
*Steven Yang,Michal Levin,Govinda Anantha Padmanabha,Miriam Borshevsky,Ohad Cohen,D. Thomas Seidl,Reese E. Jones,Nikolaos Bouklas,Noy Cohen*

Main category: cs.LG

TL;DR: This paper presents an integrated experimental and computational investigation into the composition-dependent mechanical behavior of 3D printed digital materials and develops a physics-augmented neural network (PANN) to accurately capture the nonlinear, rate-dependent behavior.


<details>
  <summary>Details</summary>
Motivation: Multi-material 3D printing enables the fabrication of digital materials with tunable mechanical properties, but their composition-dependent mechanical behavior needs investigation.

Method: We develop a physics-augmented neural network (PANN) that combines a partially input convex neural network (pICNN) with a quasi-linear viscoelastic (QLV) formulation.

Result: The results reveal nonlinear and rate-dependent responses that strongly depend on composition. The proposed model achieves high predictive accuracy for interpolated material compositions.

Conclusion: The proposed physics-augmented neural network (PANN) accurately captures the nonlinear, rate-dependent behavior of 3D printed digital materials, providing a scalable framework for automated, composition-aware constitutive model discovery.

Abstract: Multi-material 3D printing, particularly through polymer jetting, enables the
fabrication of digital materials by mixing distinct photopolymers at the micron
scale within a single build to create a composite with tunable mechanical
properties. This work presents an integrated experimental and computational
investigation into the composition-dependent mechanical behavior of 3D printed
digital materials. We experimentally characterize five formulations, combining
soft and rigid UV-cured polymers under uniaxial tension and torsion across
three strain and twist rates. The results reveal nonlinear and rate-dependent
responses that strongly depend on composition. To model this behavior, we
develop a physics-augmented neural network (PANN) that combines a partially
input convex neural network (pICNN) for learning the composition-dependent
hyperelastic strain energy function with a quasi-linear viscoelastic (QLV)
formulation for time-dependent response. The pICNN ensures convexity with
respect to strain invariants while allowing non-convex dependence on
composition. To enhance interpretability, we apply $L_0$ sparsification. For
the time-dependent response, we introduce a multilayer perceptron (MLP) to
predict viscoelastic relaxation parameters from composition. The proposed model
accurately captures the nonlinear, rate-dependent behavior of 3D printed
digital materials in both uniaxial tension and torsion, achieving high
predictive accuracy for interpolated material compositions. This approach
provides a scalable framework for automated, composition-aware constitutive
model discovery for multi-material 3D printing.

</details>


### [176] [MedGround-R1: Advancing Medical Image Grounding via Spatial-Semantic Rewarded Group Relative Policy Optimization](https://arxiv.org/abs/2507.02994)
*Huihui Xu,Yuanpeng Nie,Hualiang Wang,Ying Chen,Wei Li,Junzhi Ning,Lihao Liu,Hongqiu Wang,Lei Zhu,Jiyao Liu,Xiaomeng Li,Junjun He*

Main category: cs.LG

TL;DR: adapt the GRPO reinforcement learning framework to VLMs for Medical Image Grounding and propose the Spatial-Semantic Rewarded Group Relative Policy Optimization to train the model without CoT reasoning annotations, achieves state-of-the-art performance


<details>
  <summary>Details</summary>
Motivation: Medical Image Grounding (MIG) requires models to not only perceive regions but also deduce spatial relationships of these regions. Existing VLMs for MIG often rely on Supervised Fine-Tuning (SFT) with large amounts of Chain-of-Thought (CoT) reasoning annotations, which are expensive and time-consuming to acquire.

Method: adapt the GRPO reinforcement learning framework to VLMs for Medical Image Grounding. propose the Spatial-Semantic Rewarded Group Relative Policy Optimization to train the model without CoT reasoning annotations. introduce Spatial-Semantic Rewards, which combine spatial accuracy reward and semantic consistency reward. propose to use the Chain-of-Box template, which integrates visual information of referring bounding boxes into the <think> reasoning process

Result: Experiments on three datasets MS-CXR, ChestX-ray8, and M3D-RefSeg demonstrate that our method achieves state-of-the-art performance in Medical Image Grounding.

Conclusion: achieves state-of-the-art performance in Medical Image Grounding. Ablation studies further validate the effectiveness of each component in our approach.

Abstract: Medical Image Grounding (MIG), which involves localizing specific regions in
medical images based on textual descriptions, requires models to not only
perceive regions but also deduce spatial relationships of these regions.
Existing Vision-Language Models (VLMs) for MIG often rely on Supervised
Fine-Tuning (SFT) with large amounts of Chain-of-Thought (CoT) reasoning
annotations, which are expensive and time-consuming to acquire. Recently,
DeepSeek-R1 demonstrated that Large Language Models (LLMs) can acquire
reasoning abilities through Group Relative Policy Optimization (GRPO) without
requiring CoT annotations. In this paper, we adapt the GRPO reinforcement
learning framework to VLMs for Medical Image Grounding. We propose the
Spatial-Semantic Rewarded Group Relative Policy Optimization to train the model
without CoT reasoning annotations. Specifically, we introduce Spatial-Semantic
Rewards, which combine spatial accuracy reward and semantic consistency reward
to provide nuanced feedback for both spatially positive and negative
completions. Additionally, we propose to use the Chain-of-Box template, which
integrates visual information of referring bounding boxes into the <think>
reasoning process, enabling the model to explicitly reason about spatial
regions during intermediate steps. Experiments on three datasets MS-CXR,
ChestX-ray8, and M3D-RefSeg demonstrate that our method achieves
state-of-the-art performance in Medical Image Grounding. Ablation studies
further validate the effectiveness of each component in our approach. Code,
checkpoints, and datasets are available at
https://github.com/bio-mlhui/MedGround-R1

</details>


### [177] [What to Do Next? Memorizing skills from Egocentric Instructional Video](https://arxiv.org/abs/2507.02997)
*Jing Bi,Chenliang Xu*

Main category: cs.LG

TL;DR: 提出了一种结合拓扑可供性记忆和Transformer架构的方法，用于在交互式模拟环境中规划高级面向目标的动作，实验结果表明该方法有效。


<details>
  <summary>Details</summary>
Motivation: 通过演示学习执行活动需要从观察中提取关于环境的有意义的信息。本研究调查了从以自我为中心的角度在模拟环境中规划高级面向目标的动作的挑战。

Method: 结合了拓扑可供性记忆和Transformer架构。

Result: 实验结果表明，该方法学习有意义的表征，从而提高了性能，并且在发生动作偏差时具有鲁棒性。

Conclusion: 该方法学习有意义的表征，从而提高了性能，并且在发生动作偏差时具有鲁棒性。

Abstract: Learning to perform activities through demonstration requires extracting
meaningful information about the environment from observations. In this
research, we investigate the challenge of planning high-level goal-oriented
actions in a simulation setting from an egocentric perspective. We present a
novel task, interactive action planning, and propose an approach that combines
topological affordance memory with transformer architecture. The process of
memorizing the environment's structure through extracting affordances
facilitates selecting appropriate actions based on the context. Moreover, the
memory model allows us to detect action deviations while accomplishing specific
objectives. To assess the method's versatility, we evaluate it in a realistic
interactive simulation environment. Our experimental results demonstrate that
the proposed approach learns meaningful representations, resulting in improved
performance and robust when action deviations occur.

</details>


### [178] [A Weakly Supervised Transformer to Support Rare Disease Diagnosis from Electronic Health Records: Methods and Applications in Rare Pulmonary Disease](https://arxiv.org/abs/2507.02998)
*Kimberly F. Greco,Zongxin Yang,Mengyan Li,Han Tong,Sara Morini Sweet,Alon Geva,Kenneth D. Mandl,Benjamin A. Raby,Tianxi Cai*

Main category: cs.LG

TL;DR: 该研究提出了一种新的弱监督框架，用于利用电子病历数据进行罕见病表型分析，并在实验中取得了良好的效果。


<details>
  <summary>Details</summary>
Motivation: 罕见病影响全球大量人口，但由于其低患病率和临床医生对其熟悉程度有限，个体疾病的特征通常不明确且难以诊断。计算表型算法在自动检测罕见病方面显示出希望，但其开发受到标记数据稀缺和现有标签来源偏差的阻碍。

Method: 该方法通过学习医学概念的嵌入表示，并使用多层Transformer架构将这些嵌入提炼和聚合为患者级别的表征。

Result: 在两项罕见肺部疾病的案例研究中，该模型在波士顿儿童医院的EHR数据上进行了验证，并在表型分类、识别临床意义的亚表型以及预测疾病进展方面表现出显著的改进。

Conclusion: 该研究提出了一种弱监督的、基于Transformer的框架，该框架结合了少量金标准标签和大量来自EHR数据的迭代更新的银标准标签，用于罕见病表型分析。实验结果表明，该框架在表型分类、识别临床意义的亚表型以及预测疾病进展方面均优于基线方法。

Abstract: Rare diseases affect an estimated 300-400 million people worldwide, yet
individual conditions often remain poorly characterized and difficult to
diagnose due to their low prevalence and limited clinician familiarity. While
computational phenotyping algorithms show promise for automating rare disease
detection, their development is hindered by the scarcity of labeled data and
biases in existing label sources. Gold-standard labels from registries and
expert chart reviews are highly accurate but constrained by selection bias and
the cost of manual review. In contrast, labels derived from electronic health
records (EHRs) cover a broader range of patients but can introduce substantial
noise. To address these challenges, we propose a weakly supervised,
transformer-based framework that combines a small set of gold-standard labels
with a large volume of iteratively updated silver-standard labels derived from
EHR data. This hybrid approach enables the training of a highly accurate and
generalizable phenotyping model that scales rare disease detection beyond the
scope of individual clinical expertise. Our method is initialized by learning
embeddings of medical concepts based on their semantic meaning or co-occurrence
patterns in EHRs, which are then refined and aggregated into patient-level
representations via a multi-layer transformer architecture. Using two rare
pulmonary diseases as a case study, we validate our model on EHR data from
Boston Children's Hospital. Our framework demonstrates notable improvements in
phenotype classification, identification of clinically meaningful subphenotypes
through patient clustering, and prediction of disease progression compared to
baseline methods. These results highlight the potential of our approach to
enable scalable identification and stratification of rare disease patients for
clinical care and research applications.

</details>


### [179] [Learning Beyond Euclid: Curvature-Adaptive Generalization for Neural Networks on Manifolds](https://arxiv.org/abs/2507.02999)
*Krisanu Sarkar*

Main category: cs.LG

TL;DR: 这篇论文为黎曼流形上训练的神经网络开发了新的泛化界限，考虑了流形特定属性，并在弯曲空间中展示了改进的泛化保证。


<details>
  <summary>Details</summary>
Motivation: 现有的泛化理论通常依赖于从欧几里得几何导出的复杂性度量，这些度量无法解释非欧几里得空间的内在结构。

Method: 这篇论文开发了针对黎曼流形上训练的神经网络的新泛化界限，并推导了明确包含流形特定属性（如截面曲率、体积增长和单射半径）的覆盖数界限。

Result: 论文的几何校正导致了紧凑流形上定义的 Lipschitz 神经网络类的更清晰的 Rademacher 复杂性界限。当曲率为零时，得到的泛化保证恢复了标准的欧几里得结果，但在数据位于嵌入在高维环境空间中的弯曲、低维流形上的情况下，泛化保证得到了显着改善。论文在负弯曲空间中说明了其界限的紧密性，其中指数体积增长导致了可证明的更高复杂性，并且在正弯曲空间中，曲率充当了正则化因子。

Conclusion: 这篇论文提供了一个原则性的理解，即内在几何如何影响学习能力，为结构化数据领域的深度学习提供理论见解和实践意义。

Abstract: In this work, we develop new generalization bounds for neural networks
trained on data supported on Riemannian manifolds. Existing generalization
theories often rely on complexity measures derived from Euclidean geometry,
which fail to account for the intrinsic structure of non-Euclidean spaces. Our
analysis introduces a geometric refinement: we derive covering number bounds
that explicitly incorporate manifold-specific properties such as sectional
curvature, volume growth, and injectivity radius. These geometric corrections
lead to sharper Rademacher complexity bounds for classes of Lipschitz neural
networks defined on compact manifolds. The resulting generalization guarantees
recover standard Euclidean results when curvature is zero but improve
substantially in settings where the data lies on curved, low-dimensional
manifolds embedded in high-dimensional ambient spaces. We illustrate the
tightness of our bounds in negatively curved spaces, where the exponential
volume growth leads to provably higher complexity, and in positively curved
spaces, where the curvature acts as a regularizing factor. This framework
provides a principled understanding of how intrinsic geometry affects learning
capacity, offering both theoretical insight and practical implications for deep
learning on structured data domains.

</details>


### [180] [Completion of the DrugMatrix Toxicogenomics Database using 3-Dimensional Tensors](https://arxiv.org/abs/2507.03024)
*Tan Nguyen,Guojing Cong*

Main category: cs.LG

TL;DR: 本研究提出了一种基于张量的毒理基因组数据补全方法，该方法提高了数据补全的准确性，并揭示了组织间的关系，为未来的药物研究提供了有前景的方法。


<details>
  <summary>Details</summary>
Motivation: 探索应用张量补全方法来完成DrugMatrix毒理基因组数据集。假设通过保留数据的三维结构（组织、处理和转录组测量）并利用机器学习公式，该方法将改进先前的技术水平。

Method: 张量补全方法

Result: 该方法在均方误差和平均绝对误差方面均优于传统的Canonical Polyadic分解和二维矩阵分解方法。非负张量补全的实现揭示了组织间的关系。

Conclusion: 该研究发现，基于张量的方法能够更准确地反映原始数据分布，并有效捕捉器官特异性变异。

Abstract: We explore applying a tensor completion approach to complete the DrugMatrix
toxicogenomics dataset. Our hypothesis is that by preserving the 3-dimensional
structure of the data, which comprises tissue, treatment, and transcriptomic
measurements, and by leveraging a machine learning formulation, our approach
will improve upon prior state-of-the-art results. Our results demonstrate that
the new tensor-based method more accurately reflects the original data
distribution and effectively captures organ-specific variability. The proposed
tensor-based methodology achieved lower mean squared errors and mean absolute
errors compared to both conventional Canonical Polyadic decomposition and
2-dimensional matrix factorization methods. In addition, our non-negative
tensor completion implementation reveals relationships among tissues. Our
findings not only complete the world's largest in-vivo toxicogenomics database
with improved accuracy but also offer a promising methodology for future
studies of drugs that may cross species barriers, for example, from rats to
humans.

</details>


### [181] [Generalized Adaptive Transfer Network: Enhancing Transfer Learning in Reinforcement Learning Across Domains](https://arxiv.org/abs/2507.03026)
*Abhishek Verma,Nallarasan V,Balaraman Ravindran*

Main category: cs.LG

TL;DR: GATN是一种深度强化学习架构，旨在解决跨领域的任务泛化、环境变化的鲁棒性和迁移中的计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的迁移学习能够使智能体利用来自源任务的知识来加速目标任务中的学习。虽然之前的工作已经解决了负迁移和选择性迁移问题，但其他关键挑战仍未得到充分探索。

Method: GATN采用领域无关的表示模块、鲁棒性策略适配器和高效的传输调度器。

Result: GATN在Atari 2600、MuJoCo和自定义聊天机器人对话环境等多种基准测试中表现出卓越的性能，与基线相比，在跨领域泛化、动态环境适应性和降低计算开销方面均有提升。

Conclusion: GATN在跨领域泛化、动态环境适应性和计算效率方面表现出色，适用于自适应聊天机器人和机器人控制等实际应用。

Abstract: Transfer learning in Reinforcement Learning (RL) enables agents to leverage
knowledge from source tasks to accelerate learning in target tasks. While prior
work, such as the Attend, Adapt, and Transfer (A2T) framework, addresses
negative transfer and selective transfer, other critical challenges remain
underexplored. This paper introduces the Generalized Adaptive Transfer Network
(GATN), a deep RL architecture designed to tackle task generalization across
domains, robustness to environmental changes, and computational efficiency in
transfer. GATN employs a domain-agnostic representation module, a
robustness-aware policy adapter, and an efficient transfer scheduler to achieve
these goals. We evaluate GATN on diverse benchmarks, including Atari 2600,
MuJoCo, and a custom chatbot dialogue environment, demonstrating superior
performance in cross-domain generalization, resilience to dynamic environments,
and reduced computational overhead compared to baselines. Our findings suggest
GATN is a versatile framework for real-world RL applications, such as adaptive
chatbots and robotic control.

</details>


### [182] [Deep Learning-Based Forecasting of Hotel KPIs: A Cross-City Analysis of Global Urban Markets](https://arxiv.org/abs/2507.03028)
*C. J. Atapattu,Xia Cui,N. R Abeynayake*

Main category: cs.LG

TL;DR: 本研究利用 LSTM 网络预测五个主要城市的酒店 KPI，发现该模型在全球城市中具有通用性，对旅游业利益相关者和城市规划者具有潜在效用。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在预测五个主要城市的关键绩效指标 (KPI)：曼彻斯特、阿姆斯特丹、迪拜、曼谷和孟买，这些城市因其不同的经济状况和酒店业动态而被选中。

Method: 使用长短期记忆 (LSTM) 网络来预测关键绩效指标 (KPI)

Result: 曼彻斯特和孟买表现出最高的预测准确率，反映了稳定的需求模式，而迪拜和曼谷由于季节性和事件驱动的影响表现出更高的可变性。

Conclusion: LSTM模型在城市酒店预测中有效，并为数据驱动决策提供比较框架。该模型在全球城市中的通用性突显了其对旅游业利益相关者和城市规划者的潜在效用。

Abstract: This study employs Long Short-Term Memory (LSTM) networks to forecast key
performance indicators (KPIs), Occupancy (OCC), Average Daily Rate (ADR), and
Revenue per Available Room (RevPAR), across five major cities: Manchester,
Amsterdam, Dubai, Bangkok, and Mumbai. The cities were selected for their
diverse economic profiles and hospitality dynamics. Monthly data from 2018 to
2025 were used, with 80% for training and 20% for testing. Advanced time series
decomposition and machine learning techniques enabled accurate forecasting and
trend identification. Results show that Manchester and Mumbai exhibited the
highest predictive accuracy, reflecting stable demand patterns, while Dubai and
Bangkok demonstrated higher variability due to seasonal and event-driven
influences. The findings validate the effectiveness of LSTM models for urban
hospitality forecasting and provide a comparative framework for data-driven
decision-making. The models generalisability across global cities highlights
its potential utility for tourism stakeholders and urban planners.

</details>


### [183] [On the Mathematical Impossibility of Safe Universal Approximators](https://arxiv.org/abs/2507.03031)
*Jasper Yao*

Main category: cs.LG

TL;DR: 证明了通用逼近器存在固有的不稳定性，这使得完美的对齐成为数学上的不可能。


<details>
  <summary>Details</summary>
Motivation: 建立通用逼近定理 (UAT) 系统对齐的基本数学限制，通过证明灾难性失效是任何有用计算系统不可避免的特征。

Method: 通过一个三层论证来证明，该论证不给任何通用逼近器架构留下逃生路线：1) 组合必要性；2) 拓扑必要性；3) 经验必要性。

Result: 对于任何通用逼近器，有用计算所需的表达能力与一组密集的、使完美的、可靠的控制成为数学上不可能的不稳定性密不可分。证明了对抗性示例的普遍存在是现实世界任务本身就是灾难性的经验证据，迫使任何成功的模型学习和复制这些不稳定性。最低的有用复杂性超过了最大的安全复杂性，

Conclusion: 完美对齐不是一个工程挑战，而是一个数学上的不可能。UAT 的安全性问题应该被重新定义为“如何在不可避免的不可控性存在的情况下安全运行”，这对 UAT 的未来发展和治理具有深远的影响。

Abstract: We establish fundamental mathematical limits on universal approximation
theorem (UAT) system alignment by proving that catastrophic failures are an
inescapable feature of any useful computational system. Our central thesis is
that for any universal approximator, the expressive power required for useful
computation is inextricably linked to a dense set of instabilities that make
perfect, reliable control a mathematical impossibility. We prove this through a
three-level argument that leaves no escape routes for any class of universal
approximator architecture. i) Combinatorial Necessity: For the vast majority of
practical universal approximators (e.g., those using ReLU activations), we
prove that the density of catastrophic failure points is directly proportional
to the network's expressive power. ii) Topological Necessity: For any
theoretical universal approximator, we use singularity theory to prove that the
ability to approximate generic functions requires the ability to implement the
dense, catastrophic singularities that characterize them. iii) Empirical
Necessity: We prove that the universal existence of adversarial examples is
empirical evidence that real-world tasks are themselves catastrophic, forcing
any successful model to learn and replicate these instabilities. These results,
combined with a quantitative "Impossibility Sandwich" showing that the minimum
complexity for usefulness exceeds the maximum complexity for safety,
demonstrate that perfect alignment is not an engineering challenge but a
mathematical impossibility. This foundational result reframes UAT safety from a
problem of "how to achieve perfect control" to one of "how to operate safely in
the presence of irreducible uncontrollability," with profound implications for
the future of UAT development and governance.

</details>


### [184] [Rethinking Data Protection in the (Generative) Artificial Intelligence Era](https://arxiv.org/abs/2507.03034)
*Yiming Li,Shuo Shao,Yu He,Junfeng Guo,Tianwei Zhang,Zhan Qin,Pin-Yu Chen,Michael Backes,Philip Torr,Dacheng Tao,Kui Ren*

Main category: cs.LG

TL;DR: 提出了一个四级分类法，以应对现代人工智能模型中不断变化的数据保护需求。


<details>
  <summary>Details</summary>
Motivation: 人工智能的兴起使得传统的数据保护方法已经不足以满足需求，并且需要明确保障范围。

Method: 提出了一个四级分类法，用于捕获现代（生成式）人工智能模型和系统中出现的多样化保护需求。

Result: 分析了每个级别的代表性技术方法，并揭示了监管盲点。

Conclusion: 提出了一个四级分类法，包括非可用性、隐私保护、可追溯性和可删除性，该分类法捕捉了现代（生成式）人工智能模型和系统中出现的多样化保护需求。

Abstract: The (generative) artificial intelligence (AI) era has profoundly reshaped the
meaning and value of data. No longer confined to static content, data now
permeates every stage of the AI lifecycle from the training samples that shape
model parameters to the prompts and outputs that drive real-world model
deployment. This shift renders traditional notions of data protection
insufficient, while the boundaries of what needs safeguarding remain poorly
defined. Failing to safeguard data in AI systems can inflict societal and
individual, underscoring the urgent need to clearly delineate the scope of and
rigorously enforce data protection. In this perspective, we propose a
four-level taxonomy, including non-usability, privacy preservation,
traceability, and deletability, that captures the diverse protection needs
arising in modern (generative) AI models and systems. Our framework offers a
structured understanding of the trade-offs between data utility and control,
spanning the entire AI pipeline, including training datasets, model weights,
system prompts, and AI-generated content. We analyze representative technical
approaches at each level and reveal regulatory blind spots that leave critical
assets exposed. By offering a structured lens to align future AI technologies
and governance with trustworthy data practices, we underscore the urgency of
rethinking data protection for modern AI techniques and provide timely guidance
for developers, researchers, and regulators alike.

</details>


### [185] [Adaptive Cubic Regularized Second-Order Latent Factor Analysis Model](https://arxiv.org/abs/2507.03036)
*Jialiang Wang,Junzhou Wang,Xin Liao*

Main category: cs.LG

TL;DR: 提出了一种新的自适应三次正则化二阶潜在因子分析(ACRSLF)模型，该模型在收敛速度和表示精度方面优于LFA模型。


<details>
  <summary>Details</summary>
Motivation: 高维不完全(HDI)数据在各种实际应用中变得普遍，二阶潜在因子模型在建模此类数据方面表现出良好的性能。然而，由于SLF模型目标函数的双线性和非凸性，将阻尼项纳入Hessian近似并仔细调整相关参数至关重要。

Method: 自适应三次正则化二阶潜在因子分析(ACRSLF)模型

Result: 在两个工业HDI数据集上的综合实验表明，ACRSLF比先进的基于优化器的LFA模型收敛更快，并实现了更高的表示精度。

Conclusion: ACRSLF模型在收敛速度和表示精度方面优于LFA模型。

Abstract: High-dimensional and incomplete (HDI) data, characterized by massive node
interactions, have become ubiquitous across various real-world applications.
Second-order latent factor models have shown promising performance in modeling
this type of data. Nevertheless, due to the bilinear and non-convex nature of
the SLF model's objective function, incorporating a damping term into the
Hessian approximation and carefully tuning associated parameters become
essential. To overcome these challenges, we propose a new approach in this
study, named the adaptive cubic regularized second-order latent factor analysis
(ACRSLF) model. The proposed ACRSLF adopts the two-fold ideas: 1) self-tuning
cubic regularization that dynamically mitigates non-convex optimization
instabilities; 2) multi-Hessian-vector product evaluation during conjugate
gradient iterations for precise second-order information assimilation.
Comprehensive experiments on two industrial HDI datasets demonstrate that the
ACRSLF converges faster and achieves higher representation accuracy than the
advancing optimizer-based LFA models.

</details>


### [186] [Optimas: Optimizing Compound AI Systems with Globally Aligned Local Rewards](https://arxiv.org/abs/2507.03041)
*Shirley Wu,Parth Sarthi,Shiyu Zhao,Aaron Lee,Herumb Shandilya,Adrian Mladenic Grobelnik,Nurendra Choudhary,Eddie Huang,Karthik Subbian,Linjun Zhang,Diyi Yang,James Zou,Jure Leskovec*

Main category: cs.LG

TL;DR: Optimas is a unified framework for effectively optimizing compound AI systems by maintaining local reward functions per component and adapting them to correlate with global system performance.


<details>
  <summary>Details</summary>
Motivation: Optimizing compound AI systems is challenging due to their non-differentiable structures and diverse configuration types.

Method: Optimas, a unified framework that maintains one Local Reward Function (LRF) per component, each satisfying a local-global alignment property. It adapts LRFs and maximizes each component's local reward in each iteration, enabling independent updates of heterogeneous configurations.

Result: Optimas improves compound systems by an average of 11.92% compared to strong baselines.

Conclusion: Optimas outperforms strong baselines by an average of 11.92% across five real-world compound systems.

Abstract: Compound AI systems integrating multiple components, such as Large Language
Models, specialized tools, and traditional machine learning models, are
increasingly deployed to solve complex real-world tasks. However, optimizing
compound systems remains challenging due to their non-differentiable structures
and diverse configuration types across components, including prompts,
hyperparameters, and model parameters. To address this challenge, we propose
Optimas, a unified framework for effective optimization of compound systems.
The core idea of Optimas is to maintain one Local Reward Function (LRF) per
component, each satisfying a local-global alignment property, i.e., each
component's local reward correlates with the global system performance. In each
iteration, Optimas efficiently adapts the LRFs to maintain this property while
simultaneously maximizing each component's local reward. This approach enables
independent updates of heterogeneous configurations using the designated
optimization method, while ensuring that local improvements consistently lead
to performance gains. We present extensive evaluations across five real-world
compound systems to demonstrate that Optimas outperforms strong baselines by an
average improvement of 11.92%, offering a general and effective approach for
improving compound systems. Our website is at https://optimas.stanford.edu.

</details>


### [187] [Optimisation Is Not What You Need](https://arxiv.org/abs/2507.03045)
*Alfredo Ibias*

Main category: cs.LG

TL;DR: 优化方法存在根本缺陷，阻碍了它们成为真正的人工认知。


<details>
  <summary>Details</summary>
Motivation: 人工智能领域专注于开发优化方法来解决多个问题，但这些方法存在一些根本缺陷，阻碍了它们成为真正的人工认知。

Method: 形式化证明了灾难性遗忘是优化方法固有的问题

Result: 世界建模方法可以避免灾难性遗忘和过拟合问题。

Conclusion: 人工智能领域需要跳出机器学习领域，寻找能够开发人工智能认知的方法。

Abstract: The Artificial Intelligence field has focused on developing optimisation
methods to solve multiple problems, specifically problems that we thought to be
only solvable through cognition. The obtained results have been outstanding,
being able to even surpass the Turing Test. However, we have found that these
optimisation methods share some fundamental flaws that impede them to become a
true artificial cognition. Specifically, the field have identified catastrophic
forgetting as a fundamental problem to develop such cognition. This paper
formally proves that this problem is inherent to optimisation methods, and as
such it will always limit approaches that try to solve the Artificial General
Intelligence problem as an optimisation problem. Additionally, it addresses the
problem of overfitting and discuss about other smaller problems that
optimisation methods pose. Finally, it empirically shows how world-modelling
methods avoid suffering from either problem. As a conclusion, the field of
Artificial Intelligence needs to look outside the machine learning field to
find methods capable of developing an artificial cognition.

</details>


### [188] [Monitoring of Static Fairness](https://arxiv.org/abs/2507.03048)
*Thomas A. Henzinger,Mahyar Karimi,Konstantin Kueffner,Kaushik Mallik*

Main category: cs.LG

TL;DR: Presents a runtime verification framework for algorithmic fairness in Markov chain models, with a specification language and monitoring algorithms that provide quantitative fairness estimates. Prototype shows it's fast.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the widespread use of machine-learned systems in decision-making about humans and the importance of ensuring fairness, i.e., avoiding bias based on sensitive attributes.

Method: The paper introduces a specification language for algorithmic fairness properties and develops monitoring algorithms with uniform and non-uniform error bounds, using statistical tools adapted for dynamic monitoring.

Result: The paper provides a quantitative estimate of system fairness with proven error bounds that tighten as the observed sequence grows. Experiments show the monitors update verdicts in less than a millisecond per observation.

Conclusion: The paper presents a runtime verification framework for algorithmic fairness in systems modeled as Markov chains. A prototype implementation demonstrates the efficiency of the approach in monitoring loan applications and college admissions.

Abstract: Machine-learned systems are in widespread use for making decisions about
humans, and it is important that they are fair, i.e., not biased against
individuals based on sensitive attributes.
  We present a general framework of runtime verification of algorithmic
fairness for systems whose models are unknown, but are assumed to have a Markov
chain structure, with or without full observation of the state space.
  We introduce a specification language that can model many common algorithmic
fairness properties, such as demographic parity, equal opportunity, and social
burden.
  We build monitors that observe a long sequence of events as generated by a
given system, and output, after each observation, a quantitative estimate of
how fair or biased the system was on that run until that point in time.
  The estimate is proven to be correct modulo a variable error bound and a
given confidence level, where the error bound gets tighter as the observed
sequence gets longer.
  We present two categories of monitoring algorithms, namely ones with a
uniform error bound across all time points, and ones with weaker non-uniform,
pointwise error bounds at different time points.
  Our monitoring algorithms use statistical tools that are adapted to suit the
dynamic requirements of monitoring and the special needs of the fairness
specifications.
  Using a prototype implementation, we show how we can monitor if a bank is
fair in giving loans to applicants from different social backgrounds, and if a
college is fair in admitting students while maintaining a reasonable financial
burden on the society.
  In these experiments, our monitors took less than a millisecond to update
their verdicts after each observation.

</details>


### [189] [From 2:4 to 8:16 sparsity patterns in LLMs for Outliers and Weights with Variance Correction](https://arxiv.org/abs/2507.03052)
*Egor Maximov,Yulia Kuzkina,Azamat Kanametov,Alexander Prutko,Aleksei Goncharov,Maxim Zhelnin,Egor Shvetsov*

Main category: cs.LG

TL;DR: 本文研究了LLM的8:16半结构化稀疏性，发现它优于其他稀疏化方法，并且可以通过一些简单技术来提高性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的规模不断增长，量化和稀疏化等高效压缩技术至关重要。结构化稀疏性方法（如N:M稀疏化）由于灵活性有限以及对外围权重的敏感性，通常表现不佳。

Method: 探索8:16半结构化稀疏性，并应用稀疏结构化模式来处理显著权重。

Result: 8:16稀疏性比2:4稀疏性具有更大的灵活性和更小的存储开销。用于显著权重的结构化稀疏性与非结构化方法相比具有竞争力，并能获得同等或更好的结果。

Conclusion: 8:16半结构化稀疏性可以超越性能阈值，并且通过方差校正和SmoothQuant等技术可以提高稀疏模型的性能。

Abstract: As large language models (LLMs) grow in size, efficient compression
techniques like quantization and sparsification are critical. While
quantization maintains performance with reduced precision, structured sparsity
methods, such as N:M sparsification, often fall short due to limited
flexibility, and sensitivity to outlier weights. We explore 8:16
semi-structured sparsity, demonstrating its ability to surpass the Performance
Threshold-where a compressed model matches the accuracy of its uncompressed or
smaller counterpart under equivalent memory constraints. Compared to 2:4
sparsity, 8:16 offers greater flexibility with minimal storage overhead (0.875
vs. 0.75 bits/element). We also apply sparse structured patterns for salient
weights, showing that structured sparsity for outliers is competitive with
unstructured approaches leading to equivalent or better results. Finally, we
demonstrate that simple techniques such as variance correction and SmoothQuant
like weight equalization improve sparse models performance.

</details>


### [190] [Automated Grading of Students' Handwritten Graphs: A Comparison of Meta-Learning and Vision-Large Language Models](https://arxiv.org/abs/2507.03056)
*Behnam Parsaeifard,Martin Hlosta,Per Bergamin*

Main category: cs.LG

TL;DR: 本研究探讨了使用多模态元学习模型和视觉大型语言模型 (VLLM) 自动评分学生手写图。结果表明，元学习模型在简单的分类任务中表现更好，而 VLLM 在更复杂的任务中表现更好，但 VLLM 的可靠性需要进一步研究。


<details>
  <summary>Details</summary>
Motivation: 随着在线学习的兴起，过去十年中对数学高效且一致的评估需求显着增加。机器学习 (ML)，特别是自然语言处理 (NLP)，已被广泛用于自动评分学生答案，特别是那些涉及文本和/或数学表达式的答案。然而，尽管学生手写图在科学、技术、工程和数学 (STEM) 课程中普遍存在，但对自动评分涉及学生手写图的答案的研究有限。

Method: 实施多模态元学习模型，用于自动评分包含学生手写图和文本的图像。将视觉大型语言模型 (VLLM) 的性能与这些经过专门训练的元学习模型进行比较。

Result: 在从机构收集的真实数据集上进行评估的结果表明，表现最佳的元学习模型在 2-way 分类任务中优于 VLLM。相比之下，在更复杂的 3-way 分类任务中，表现最佳的 VLLM 略优于元学习模型。

Conclusion: VLLMs在更复杂的3-way分类任务中略胜于meta-learning模型，但其可靠性和实际应用仍不确定，需要进一步研究。

Abstract: With the rise of online learning, the demand for efficient and consistent
assessment in mathematics has significantly increased over the past decade.
Machine Learning (ML), particularly Natural Language Processing (NLP), has been
widely used for autograding student responses, particularly those involving
text and/or mathematical expressions. However, there has been limited research
on autograding responses involving students' handwritten graphs, despite their
prevalence in Science, Technology, Engineering, and Mathematics (STEM)
curricula. In this study, we implement multimodal meta-learning models for
autograding images containing students' handwritten graphs and text. We further
compare the performance of Vision Large Language Models (VLLMs) with these
specially trained metalearning models. Our results, evaluated on a real-world
dataset collected from our institution, show that the best-performing
meta-learning models outperform VLLMs in 2-way classification tasks. In
contrast, in more complex 3-way classification tasks, the best-performing VLLMs
slightly outperform the meta-learning models. While VLLMs show promising
results, their reliability and practical applicability remain uncertain and
require further investigation.

</details>


### [191] [BERT4Traj: Transformer Based Trajectory Reconstruction for Sparse Mobility Data](https://arxiv.org/abs/2507.03062)
*Hao Yang,Angela Yao,Christopher Whalen,Gengchen Mai*

Main category: cs.LG

TL;DR: BERT4Traj是一种基于Transformer的模型，用于重建稀疏移动数据中的完整轨迹，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 理解人类移动性对于公共卫生、交通和城市规划中的应用至关重要。然而，由于数据收集方法的限制，例如不频繁的GPS采样或仅在通信事件期间捕获位置的呼叫详细记录（CDR）数据，移动性数据通常存在稀疏性。

Method: 提出了一种基于Transformer的模型BERT4Traj，该模型通过预测稀疏运动序列中的隐藏访问来重建完整的移动轨迹。受到BERT的掩码语言建模目标和self_attention机制的启发，BERT4Traj利用空间嵌入、时间嵌入和上下文背景特征，如人口统计和锚点。

Result: BERT4Traj在乌干达坎帕拉收集的真实世界CDR和GPS数据集上进行了评估，结果表明该方法明显优于传统的模型，如马尔可夫链、KNN、RNN和LSTM。

Conclusion: BERT4Traj有效地重建了详细且连续的移动轨迹，从而增强了对人类移动模式的洞察。

Abstract: Understanding human mobility is essential for applications in public health,
transportation, and urban planning. However, mobility data often suffers from
sparsity due to limitations in data collection methods, such as infrequent GPS
sampling or call detail record (CDR) data that only capture locations during
communication events. To address this challenge, we propose BERT4Traj, a
transformer based model that reconstructs complete mobility trajectories by
predicting hidden visits in sparse movement sequences. Inspired by BERT's
masked language modeling objective and self_attention mechanisms, BERT4Traj
leverages spatial embeddings, temporal embeddings, and contextual background
features such as demographics and anchor points. We evaluate BERT4Traj on real
world CDR and GPS datasets collected in Kampala, Uganda, demonstrating that our
approach significantly outperforms traditional models such as Markov Chains,
KNN, RNNs, and LSTMs. Our results show that BERT4Traj effectively reconstructs
detailed and continuous mobility trajectories, enhancing insights into human
movement patterns.

</details>


### [192] [Cycle-Consistent Helmholtz Machine: Goal-Seeded Simulation via Inverted Inference](https://arxiv.org/abs/2507.03065)
*Xin Li*

Main category: cs.LG

TL;DR: C$^2$HM 是一种新颖的亥姆霍兹机扩展，它将推理重构为一种目标播种的非对称过程，该过程通过正向生成和反向细化的递归循环，将抽象目标与观察到的结果对齐。


<details>
  <summary>Details</summary>
Motivation: 亥姆霍兹机 (HM) 是一种无监督学习的基础架构，通过交替推理将自下而上的识别模型与自上而下的生成模型耦合。然而，它依赖于对称的、数据驱动的更新，限制了它执行目标导向推理或模拟时间扩展过程的能力。

Method: 我们引入了循环一致亥姆霍兹机 (C$^2$HM)，这是一种新颖的扩展，它将推理重构为一种以结构化内部先验为基础的目标播种的非对称过程。

Result: C$^2$HM 通过变分循环将自上而下的结构与自下而上的证据相结合，从而在目标条件潜在预测和基于识别的重建之间强制实现相互对齐。我们在上下文-内容不确定性原理 (CCUP) 的框架内形式化了这种机制，该原理认为推理是通过将结构化的、低熵的内容与高熵的、模糊的上下文对齐来进行的。

Conclusion: C$^2$HM 改进了表征效率，支持通过路径依赖推理进行记忆链接，并支持空间组合想象。通过提供一种受生物学启发的经典摊销推理的替代方法，C$^2$HM 将生成建模重新构想为有意的模拟，从而在统一的概率框架中桥接了基于记忆的规划和无监督学习。

Abstract: The Helmholtz Machine (HM) is a foundational architecture for unsupervised
learning, coupling a bottom-up recognition model with a top-down generative
model through alternating inference. However, its reliance on symmetric,
data-driven updates constrains its ability to perform goal-directed reasoning
or simulate temporally extended processes. In this work, we introduce the
\emph{Cycle-Consistent Helmholtz Machine} (C$^2$HM), a novel extension that
reframes inference as a \emph{goal-seeded}, \emph{asymmetric} process grounded
in structured internal priors. Rather than inferring latent causes solely from
sensory data, C$^2$HM simulates plausible latent trajectories conditioned on
abstract goals, aligning them with observed outcomes through a recursive cycle
of forward generation and inverse refinement. This cycle-consistent formulation
integrates top-down structure with bottom-up evidence via a variational loop,
enforcing mutual alignment between goal-conditioned latent predictions and
recognition-based reconstructions. We formalize this mechanism within the
framework of the \emph{Context-Content Uncertainty Principle} (CCUP), which
posits that inference proceeds by aligning structured, low-entropy content with
high-entropy, ambiguous context. C$^2$HM improves representational efficiency,
supports memory chaining via path-dependent inference, and enables spatial
compositional imagination. By offering a biologically inspired alternative to
classical amortized inference, $C^2$HM reconceives generative modeling as
intentional simulation, bridging memory-based planning and unsupervised
learning in a unified probabilistic framework.

</details>
