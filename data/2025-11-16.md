<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 3]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Text2SQL-Flow: A Robust SQL-Aware Data Augmentation Framework for Text-to-SQL](https://arxiv.org/abs/2511.10192)
*Qifeng Cai,Hao Liang,Chang Xu,Tao Xie,Wentao Zhang,Bin Cui*

Main category: cs.CL

TL;DR: 提出了一个SQL-aware的数据增强框架，用于生成大规模、语义有效、结构多样的Text-to-SQL对。


<details>
  <summary>Details</summary>
Motivation: Text-to-SQL的性能受限于稀缺、简单和低多样性的数据集。

Method: 提出了Text2SQL-Flow框架，该框架在六个增强维度上运行，并集成了SQL执行验证、自然语言问题生成、思维链推理追踪和数据分类。

Result: 构建了一个高质量的数据集SQLFlow，包含89,544个带注释的例子。在开源和闭源LLM上的评估表明，SQLFlow提高了性能。

Conclusion: 这项工作为推进Text-to-SQL系统建立了一个可扩展的、以数据为中心的基础，并强调了高质量结构化数据在现代人工智能中的关键作用。

Abstract: The data-centric paradigm has become pivotal in AI, especially for Text-to-SQL, where performance is limited by scarce, simplistic, and low-diversity datasets. To address this, we propose Text2SQL-Flow, a SQL-aware data augmentation framework that generates large-scale, semantically valid, and structurally diverse Text-to-SQL pairs from minimal seed data. It operates across six augmentation dimensions and integrates an end-to-end pipeline featuring SQL execution verification, natural language question generation, chain-of-thought reasoning traces, and data classification. A modular Database Manager ensures cross-database compatibility and scalability. Using this framework, we build SQLFlow, a high-quality dataset of 89,544 annotated examples. We evaluate SQLFlow in two settings: (1) For open-source LLMs, fine-tuning on SQLFlow consistently improves performance across benchmarks under the same data budget. (2) For closed-source LLMs, we introduce a masked alignment retrieval method that treats SQLFlow as both knowledge base and training data for the retriever. This enables structure-aware example matching by modeling fine-grained alignments between questions and SQL queries. Experiments show our retrieval strategy outperforms existing methods, underscoring the value of SQLFlow's high-fidelity data and our novel technique. Our work establishes a scalable, data-centric foundation for advancing Text-to-SQL systems and highlights the critical role of high-quality structured data in modern AI.

</details>


### [2] [Omnilingual ASR: Open-Source Multilingual Speech Recognition for 1600+ Languages](https://arxiv.org/abs/2511.09690)
*Omnilingual ASR team,Gil Keren,Artyom Kozhevnikov,Yen Meng,Christophe Ropers,Matthew Setzler,Skyler Wang,Ife Adebara,Michael Auli,Can Balioglu,Kevin Chan,Chierh Cheng,Joe Chuang,Caley Droof,Mark Duppenthaler,Paul-Ambroise Duquenne,Alexander Erben,Cynthia Gao,Gabriel Mejia Gonzalez,Kehan Lyu,Sagar Miglani,Vineel Pratap,Kaushik Ram Sadagopan,Safiyyah Saleem,Arina Turkatenko,Albert Ventayol-Boada,Zheng-Xin Yong,Yu-An Chung,Jean Maillard,Rashel Moritz,Alexandre Mourachko,Mary Williamson,Shireen Yates*

Main category: cs.CL

TL;DR: Omnilingual ASR is a large-scale ASR system designed for extensibility, enabling communities to introduce unserved languages with limited data.


<details>
  <summary>Details</summary>
Motivation: Expanding ASR coverage to more languages is costly and limited by current architectures, raising ethical concerns when done without community collaboration.

Method: The system scales self-supervised pre-training to 7B parameters and uses an encoder-decoder architecture for zero-shot generalization, leveraging a LLM-inspired decoder, trained on a massive, diverse corpus.

Result: Omnilingual ASR covers over 1,600 languages, including 500 previously unserved, with substantial gains over prior systems, especially in low-resource conditions.

Conclusion: The release of Omnilingual ASR models and tools lowers barriers for researchers and communities, inviting new forms of participation and addressing ethical considerations.

Abstract: Automatic speech recognition (ASR) has advanced in high-resource languages, but most of the world's 7,000+ languages remain unsupported, leaving thousands of long-tail languages behind. Expanding ASR coverage has been costly and limited by architectures that restrict language support, making extension inaccessible to most--all while entangled with ethical concerns when pursued without community collaboration. To transcend these limitations, we introduce Omnilingual ASR, the first large-scale ASR system designed for extensibility. Omnilingual ASR enables communities to introduce unserved languages with only a handful of data samples. It scales self-supervised pre-training to 7B parameters to learn robust speech representations and introduces an encoder-decoder architecture designed for zero-shot generalization, leveraging a LLM-inspired decoder. This capability is grounded in a massive and diverse training corpus; by combining breadth of coverage with linguistic variety, the model learns representations robust enough to adapt to unseen languages. Incorporating public resources with community-sourced recordings gathered through compensated local partnerships, Omnilingual ASR expands coverage to over 1,600 languages, the largest such effort to date--including over 500 never before served by ASR. Automatic evaluations show substantial gains over prior systems, especially in low-resource conditions, and strong generalization. We release Omnilingual ASR as a family of models, from 300M variants for low-power devices to 7B for maximum accuracy. We reflect on the ethical considerations shaping this design and conclude by discussing its societal impact. In particular, we highlight how open-sourcing models and tools can lower barriers for researchers and communities, inviting new forms of participation. Open-source artifacts are available at https://github.com/facebookresearch/omnilingual-asr.

</details>


### [3] [Order Matters: Rethinking Prompt Construction in In-Context Learning](https://arxiv.org/abs/2511.09700)
*Warren Li,Yiqian Wang,Zihan Wang,Jingbo Shang*

Main category: cs.CL

TL;DR: 重新评估了上下文学习（ICL）中示例选择和排序的重要性，发现排序对性能的影响与选择不同示例集的影响相当。


<details>
  <summary>Details</summary>
Motivation: 先前的研究主要关注示例选择，而忽略了示例排序的影响。本文旨在重新审视这一假设，系统地比较选择和排序的影响。

Method: 通过在分类和生成任务上进行受控实验，使用多种开源模型和GPT-3。

Result: 发现不同示例排序导致的性能差异与使用完全不同的示例集相当。仅使用开发集即可识别强排序，其性能接近于基于测试标签选择最佳排序的oracle。

Conclusion: 示例选择和排序在提示设计中同等重要且相互关联，需要重新审视ICL中的假设。

Abstract: In-context learning (ICL) enables large language models to perform new tasks by conditioning on a sequence of examples. Most prior work reasonably and intuitively assumes that which examples are chosen has a far greater effect on performance than how those examples are ordered, leading to a focus on example selection. We revisit this assumption and conduct a systematic comparison between the effect of selection and ordering. Through controlled experiments on both classification and generation tasks, using multiple open-source model families (0.5B to 27B parameters) and GPT-5, we find that the variance in performance due to different example orderings is comparable to that from using entirely different example sets. Furthermore, we show that strong orderings can be identified using only a development set, achieving performance close to an oracle that selects the best ordering based on test labels. Our findings highlight the equal and intertwined importance of example selection and ordering in prompt design, calling for a reexamination of the assumptions held in ICL.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [4] [FedeCouple: Fine-Grained Balancing of Global-Generalization and Local-Adaptability in Federated Learning](https://arxiv.org/abs/2511.09599)
*Ming Yang,Dongrun Li,Xin Wang,Feng Li,Lisheng Fan,Chunxiao Wang,Xiaoming Wu,Peng Cheng*

Main category: cs.CV

TL;DR: FedeCouple: A federated learning method balancing global generalization and local adaptability for heterogeneous client data.


<details>
  <summary>Details</summary>
Motivation: Existing personalized federated learning methods often neglect local extractor adaptability and global classifier generalization, leading to weak component coupling and degraded performance.

Method: Jointly learns global/local features, uses dynamic knowledge distillation for classifier generalization, and refines the feature space with local, non-transmitted anchors.

Result: FedeCouple outperforms nine baselines on five image-classification datasets in effectiveness, stability, scalability, and security, with a 4.3% improvement over the best baseline in effectiveness.

Conclusion: FedeCouple balances global generalization and local adaptability, achieving superior performance in privacy-preserving federated learning scenarios.

Abstract: In privacy-preserving mobile network transmission scenarios with heterogeneous client data, personalized federated learning methods that decouple feature extractors and classifiers have demonstrated notable advantages in enhancing learning capability. However, many existing approaches primarily focus on feature space consistency and classification personalization during local training, often neglecting the local adaptability of the extractor and the global generalization of the classifier. This oversight results in insufficient coordination and weak coupling between the components, ultimately degrading the overall model performance. To address this challenge, we propose FedeCouple, a federated learning method that balances global generalization and local adaptability at a fine-grained level. Our approach jointly learns global and local feature representations while employing dynamic knowledge distillation to enhance the generalization of personalized classifiers. We further introduce anchors to refine the feature space; their strict locality and non-transmission inherently preserve privacy and reduce communication overhead. Furthermore, we provide a theoretical analysis proving that FedeCouple converges for nonconvex objectives, with iterates approaching a stationary point as the number of communication rounds increases. Extensive experiments conducted on five image-classification datasets demonstrate that FedeCouple consistently outperforms nine baseline methods in effectiveness, stability, scalability, and security. Notably, in experiments evaluating effectiveness, FedeCouple surpasses the best baseline by a significant margin of 4.3%.

</details>


### [5] [MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation](https://arxiv.org/abs/2511.09611)
*Ye Tian,Ling Yang,Jiongfan Yang,Anran Wang,Yu Tian,Jiani Zheng,Haochen Wang,Zhiyang Teng,Zhuochen Wang,Yinjie Wang,Yunhai Tong,Mengdi Wang,Xiangtai Li*

Main category: cs.CV

TL;DR: 现有的自回归方法在复杂任务中由于误差传播导致性能下降。论文提出了ParaBench基准来评估文本和图像输出，发现性能下降与生成推理和最终图像之间的对齐不良有关。提出了一个并行多模态扩散框架MMaDA-Parallel，通过在整个去噪过程中实现文本和图像之间的连续双向交互来解决这个问题。使用监督微调训练MMaDA-Parallel，然后通过并行强化学习（ParaRL）进一步优化，该策略沿轨迹应用语义奖励以加强跨模态一致性。实验表明，该模型显著提高了跨模态对齐和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂任务中由于误差传播导致性能下降。

Method: 提出了一个并行多模态扩散框架MMaDA-Parallel，通过在整个去噪过程中实现文本和图像之间的连续双向交互来解决这个问题。使用监督微调训练MMaDA-Parallel，然后通过并行强化学习（ParaRL）进一步优化。

Result: 在ParaBench上，输出对齐方面比最先进的模型Bagel提高了6.9%。

Conclusion: 该模型显著提高了跨模态对齐和语义一致性，为思考感知图像合成建立了一个更强大的范例。

Abstract: While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose a parallel multimodal diffusion framework, MMaDA-Parallel, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. MMaDA-Parallel is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving a 6.9\% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis. Our code is open-sourced at https://github.com/tyfeld/MMaDA-Parallel

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [An Efficient and Almost Optimal Solver for the Joint Routing-Assignment Problem via Partial JRA and Large-α Optimization](https://arxiv.org/abs/2511.09563)
*Qilong Yuan*

Main category: cs.AI

TL;DR: 提出了一种新的高效方法，可以为大规模 JRA 问题实现高精度、接近最优的解决方案。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明，精确方法虽然保证了最优性，但对于大规模实例来说，计算效率变得很低。

Method: 引入了一种部分路径重建 (PPR) 求解器，该求解器首先识别关键项目-占位符对以形成简化的子问题，然后有效地解决该子问题以优化全局解决方案。

Result: 在 n = 300、500 和 1000 的基准数据集上进行的实验评估表明，所提出的方法始终提供几乎最优的解决方案，实现了与基本事实平均偏差 0.00%，同时保持了较高的计算效率。

Conclusion: 该框架可以应用于 TSP 和相关的优化问题。

Abstract: The Joint Routing-Assignment (JRA) optimization problem simultaneously determines the assignment of items to placeholders and a Hamiltonian cycle that visits each node pair exactly once, with the objective of minimizing total travel cost. Previous studies introduced an exact mixed-integer programming (MIP) solver, along with datasets and a Gurobi implementation, showing that while the exact approach guarantees optimality, it becomes computationally inefficient for large-scale instances. To overcome this limitation, heuristic methods based on merging algorithms and shaking procedures were proposed, achieving solutions within approximately 1% deviation from the optimum. This work presents a novel and more efficient approach that attains high-accuracy, near-optimal solutions for large-scale JRA problems. The proposed method introduces a Partial Path Reconstructon (PPR) solver that first identifies key item-placeholder pairs to form a reduced subproblem, which is solved efficiently to refine the global solution. Using this PJAR framework, the initial heuristic merging solutions can be further improved, reducing the deviation by half. Moreover, the solution can be iteratively polished with PPR based solver along the optimization path to yield highly accurate tours. Additionally, a global Large-α constraint is incorporated into the JRA model to further enhance solution optimality. Experimental evaluations on benchmark datasets with n = 300, 500, and 1000 demonstrate that the proposed method consistently delivers almost optimal solutions, achieving an average deviation of 0.00% from the ground truth while maintaining high computational efficiency. Beyond the JRA problem, the proposed framework and methodologies exhibit strong potential for broader applications. The Framework can be applied to TSP and related optimization problems.

</details>


### [7] [Variable Neighborhood Search for the Electric Vehicle Routing Problem](https://arxiv.org/abs/2511.09570)
*David Woller,Viktor Kozák,Miroslav Kulich,Libor Přeučil*

Main category: cs.AI

TL;DR: 本文介绍了在2020年IEEE世界计算智能大会的CEC-12竞赛中获胜的电动汽车路径问题（EVRP）解决方案，该方案基于可变邻域搜索（VNS）元启发式算法。


<details>
  <summary>Details</summary>
Motivation: 由于文献中考虑的约束条件的多样性，比较不同问题变体的方案仍然具有挑战性。电动汽车路径问题（EVRP）扩展了经典的车辆路径问题（VRP），以反映电动和混合动力汽车在物流中日益增长的应用。

Method: 该方法基于可变邻域搜索（VNS）元启发式算法。

Result: 该方法在完整的竞赛数据集上取得了最佳结果，并且优于之后发布的最新算法。

Conclusion: 本文提出了一种基于VNS元启发式算法的电动汽车路径问题（EVRP）解决方案，并在CEC-12竞赛中获胜，证明了其有效性。

Abstract: The Electric Vehicle Routing Problem (EVRP) extends the classical Vehicle Routing Problem (VRP) to reflect the growing use of electric and hybrid vehicles in logistics. Due to the variety of constraints considered in the literature, comparing approaches across different problem variants remains challenging. A minimalistic variant of the EVRP, known as the Capacitated Green Vehicle Routing Problem (CGVRP), was the focus of the CEC-12 competition held during the 2020 IEEE World Congress on Computational Intelligence. This paper presents the competition-winning approach, based on the Variable Neighborhood Search (VNS) metaheuristic. The method achieves the best results on the full competition dataset and also outperforms a more recent algorithm published afterward.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [8] [Dolphin: An Actor-Oriented Database for Reactive Moving Object Data Management](https://arxiv.org/abs/2511.10063)
*Yiwen Wang,Vivek Shah,Marcos Antonio Vaz Salles,Claudia Bauzer Medeiros,Julio Cesar Dos Reis,Yongluan Zhou*

Main category: cs.DB

TL;DR: 本文提出了一种新的方法，通过增强分布式 actor 框架的反应功能、复杂的空间数据管理和并发语义来支持反应式移动对象应用。


<details>
  <summary>Details</summary>
Motivation: 现有的移动对象场景研究通常将反应行为留给最终用户复杂地实现，无法满足低延迟计算和可扩展性的要求。

Method: 本文提出移动 actor 抽象，它是 actor 模型的概念增强，具有反应感知、移动和空间查询功能。基于移动 actor，定义了一个反应式移动对象数据管理平台 M-AODBs，并构建了 Dolphin -- M-AODBs 的一个实现。

Result: 在具有真实反应式移动对象场景的一组实验评估中，Dolphin 在多台机器上展示了可扩展性，并提供了接近实时的反应延迟。

Conclusion: Dolphin 能够在多台机器上扩展，并提供接近实时的反应延迟，证明了 M-AODBs 平台的有效性。

Abstract: Novel reactive moving object applications require solutions to support object reactive behaviors as a way to query and update dynamic data. While moving object scenarios have long been researched in the context of spatio-temporal data management, reactive behavior is usually left to complex end-user implementations. However, it is not just a matter of hardwiring reactive constraints: the required solutions need to satisfy tight low-latency computation requirements and be scalable. This paper explores a novel approach to enrich a distributed actor-based framework with reactive functionality and complex spatial data management along with concurrency semantics. Our approach relies on a proposal of the moving actor abstraction, which is a conceptual enhancement of the actor model with reactive sensing, movement, and spatial querying capabilities. This enhancement helps developers of reactive moving object applications avoid the significant burden of implementing application-level schemes to balance performance and consistency. Based on moving actors, we define a reactive moving object data management platform, named Moving Actor-Oriented Databases (M-AODBs), and build Dolphin -- an implementation of M-AODBs. Dolphin embodies a non-intrusive actor-based design layered on top of the Microsoft Orleans distributed virtual actor framework. In a set of experimental evaluations with realistic reactive moving object scenarios, Dolphin exhibits scalability on multi-machines and provides near-real-time reaction latency.

</details>


### [9] [CityVerse: A Unified Data Platform for Multi-Task Urban Computing with Large Language Models](https://arxiv.org/abs/2511.10418)
*Yaqiao Zhu,Hongkai Wen,Mark Birkin,Man Luo*

Main category: cs.DB

TL;DR: CityVerse是一个统一的平台，集成了多源城市数据、基于能力的任務分类和动态模拟，用于在城市环境中进行系统的LLM评估。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在各种城市任务中面临两个关键挑战：缺乏用于一致的多源数据访问的统一平台和阻碍公平比较的零散的任务定义。

Method: CityVerse提供：1）基于坐标的数据API，统一了十类城市数据；2）任务API，将43个城市计算任务组织成四个级别的认知层次；3）一个交互式可视化前端，支持实时数据检索、多层显示和模拟重放。

Result: 通过对主流LLM在代表性任务上的评估，验证了该平台的有效性，证明了其支持可重复和系统评估的能力。

Conclusion: CityVerse为推进城市计算领域中的LLM和多任务方法提供了一个可重用的基础。

Abstract: Large Language Models (LLMs) show remarkable potential for urban computing, from spatial reasoning to predictive analytics. However, evaluating LLMs across diverse urban tasks faces two critical challenges: lack of unified platforms for consistent multi-source data access and fragmented task definitions that hinder fair comparison. To address these challenges, we present CityVerse, the first unified platform integrating multi-source urban data, capability-based task taxonomy, and dynamic simulation for systematic LLM evaluation in urban contexts. CityVerse provides: 1) coordinate-based Data APIs unifying ten categories of urban data-including spatial features, temporal dynamics, demographics, and multi-modal imagery-with over 38 million curated records; 2) Task APIs organizing 43 urban computing tasks into a four-level cognitive hierarchy: Perception, Spatial Understanding, Reasoning and Prediction, and Decision and Interaction, enabling standardized evaluation across capability levels; 3) an interactive visualization frontend supporting real-time data retrieval, multi-layer display, and simulation replay for intuitive exploration and validation. We validate the platform's effectiveness through evaluations on mainstream LLMs across representative tasks, demonstrating its capability to support reproducible and systematic assessment. CityVerse provides a reusable foundation for advancing LLMs and multi-task approaches in the urban computing domain.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [10] [GPR: Towards a Generative Pre-trained One-Model Paradigm for Large-Scale Advertising Recommendation](https://arxiv.org/abs/2511.10138)
*Jun Zhang,Yi Li,Yue Liu,Changping Wang,Yuan Wang,Yuling Xiong,Xun Liu,Haiyang Wu,Qian Li,Enming Zhang,Jiawei Sun,Xin Xu,Zishuai Zhang,Ruoran Liu,Suyuan Huang,Zhaoxin Zhang,Zhengkai Guo,Shuojin Yang,Meng-Hao Guo,Huan Yu,Jie Jiang,Shi-Min Hu*

Main category: cs.IR

TL;DR: GPR是一个端到端的生成式推荐框架，它用统一的生成方法代替了传统的级联模式，解决了目标不一致和误差传播的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多阶段广告推荐系统存在目标不一致和误差传播的问题，并且统一的生成式推荐模型难以满足实际工业应用的需求。

Method: 提出了GPR，一个单模型框架，它重新定义了广告推荐为一个端到端的生成任务。GPR包括统一的表示，网络架构和训练策略三个关键创新。

Result: GPR已经在腾讯微信频道广告系统中全面部署，并在GMV和CTCVR等关键业务指标上取得了显著提升。

Conclusion: GPR通过统一的框架和创新的方法，实现了广告推荐系统的端到端生成，并在实际应用中取得了显著的业务效果。

Abstract: As an intelligent infrastructure connecting users with commercial content, advertising recommendation systems play a central role in information flow and value creation within the digital economy. However, existing multi-stage advertising recommendation systems suffer from objective misalignment and error propagation, making it difficult to achieve global optimality, while unified generative recommendation models still struggle to meet the demands of practical industrial applications. To address these issues, we propose GPR (Generative Pre-trained Recommender), the first one-model framework that redefines advertising recommendation as an end-to-end generative task, replacing the traditional cascading paradigm with a unified generative approach. To realize GPR, we introduce three key innovations spanning unified representation, network architecture, and training strategy. First, we design a unified input schema and tokenization method tailored to advertising scenarios, mapping both ads and organic content into a shared multi-level semantic ID space, thereby enhancing semantic alignment and modeling consistency across heterogeneous data. Second, we develop the Heterogeneous Hierarchical Decoder (HHD), a dual-decoder architecture that decouples user intent modeling from ad generation, achieving a balance between training efficiency and inference flexibility while maintaining strong modeling capacity. Finally, we propose a multi-stage joint training strategy that integrates Multi-Token Prediction (MTP), Value-Aware Fine-Tuning and the Hierarchy Enhanced Policy Optimization (HEPO) algorithm, forming a complete generative recommendation pipeline that unifies interest modeling, value alignment, and policy optimization. GPR has been fully deployed in the Tencent Weixin Channels advertising system, delivering significant improvements in key business metrics including GMV and CTCVR.

</details>


### [11] [Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding](https://arxiv.org/abs/2511.10492)
*Yunkai Zhang,Qiang Zhang,Feng,Lin,Ruizhong Qiu,Hanchao Yu,Jason Liu,Yinglong Xia,Zhuoran Yu,Zeyu Zheng,Diji Yang*

Main category: cs.IR

TL;DR: 提出了一种backbone-agnostic框架，将人类先验知识直接整合到生成式推荐模型的端到端训练中，以优化准确性和超越准确性的目标。


<details>
  <summary>Details</summary>
Motivation: 为了长期用户满意度，优化推荐系统以实现超越准确性的目标（例如多样性、新颖性和个性化）至关重要。工业实践者积累了大量的结构化领域知识，但现有方法通常与核心模型学习脱钩，或者需要特定于架构的修改并丢弃有价值的人类先验。

Method: 引入了受高效LLM解码策略启发的轻量级、先验条件适配器头，以引导模型沿着人类可理解的轴（例如，交互类型、长期与短期兴趣）解开用户意图。还引入了一种分层组合策略，用于建模不同先验类型之间的复杂交互。

Result: 在三个大规模数据集上的大量实验表明，该方法显着提高了准确性和超越准确性的目标。人类先验允许骨干模型更有效地利用更长的上下文长度和更大的模型尺寸。

Conclusion: 该论文提出了一种有效的方法，利用人类先验知识来改进生成式推荐模型，并在准确性和超越准确性的目标上都取得了显着成果。

Abstract: Optimizing recommender systems for objectives beyond accuracy, such as diversity, novelty, and personalization, is crucial for long-term user satisfaction. To this end, industrial practitioners have accumulated vast amounts of structured domain knowledge, which we term human priors (e.g., item taxonomies, temporal patterns). This knowledge is typically applied through post-hoc adjustments during ranking or post-ranking. However, this approach remains decoupled from the core model learning, which is particularly undesirable as the industry shifts to end-to-end generative recommendation foundation models. On the other hand, many methods targeting these beyond-accuracy objectives often require architecture-specific modifications and discard these valuable human priors by learning user intent in a fully unsupervised manner.
  Instead of discarding the human priors accumulated over years of practice, we introduce a backbone-agnostic framework that seamlessly integrates these human priors directly into the end-to-end training of generative recommenders. With lightweight, prior-conditioned adapter heads inspired by efficient LLM decoding strategies, our approach guides the model to disentangle user intent along human-understandable axes (e.g., interaction types, long- vs. short-term interests). We also introduce a hierarchical composition strategy for modeling complex interactions across different prior types. Extensive experiments on three large-scale datasets demonstrate that our method significantly enhances both accuracy and beyond-accuracy objectives. We also show that human priors allow the backbone model to more effectively leverage longer context lengths and larger model sizes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [DemoTuner: Efficient DBMS Knobs Tuning via LLM-Assisted Demonstration Reinforcement Learning](https://arxiv.org/abs/2511.09998)
*Hui Dou,Lei Jin,Yuxuan Zhou,Jiang He,Yiwen Zhang*

Main category: cs.LG

TL;DR: 提出DemoTuner框架，利用LLM辅助的演示强化学习方法，提升数据库管理系统（DBMS）旋钮调优效果，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 手动调节数据库管理系统的旋钮既费力又低效，强化学习方法存在离线训练收敛速度慢的问题。

Method: 1. 设计结构化的思维链提示，利用大型语言模型（LLM）从文本中提取调优提示。 2. 提出一种提示感知的演示强化学习算法HA-DDPGfD，将提取的调优提示整合到强化学习Agent的训练中。

Result: 在MySQL和PostgreSQL上进行的大量实验表明，DemoTuner在性能提升和在线调优成本降低方面均优于DB-BERT、GPTuner和CDBTune等基线方法。DemoTuner还表现出对未知工作负载的卓越适应性。

Conclusion: DemoTuner是第一个将演示强化学习算法引入DBMS旋钮调优的工作，显著提升了调优效果并降低了在线调优成本。

Abstract: The performance of modern DBMSs such as MySQL and PostgreSQL heavily depends on the configuration of performance-critical knobs. Manual tuning these knobs is laborious and inefficient due to the complex and high-dimensional nature of the configuration space. Among the automated tuning methods, reinforcement learning (RL)-based methods have recently sought to improve the DBMS knobs tuning process from several different perspectives. However, they still encounter challenges with slow convergence speed during offline training. In this paper, we mainly focus on how to leverage the valuable tuning hints contained in various textual documents such as DBMS manuals and web forums to improve the offline training of RL-based methods. To this end, we propose an efficient DBMS knobs tuning framework named DemoTuner via a novel LLM-assisted demonstration reinforcement learning method. Specifically, to comprehensively and accurately mine tuning hints from documents, we design a structured chain of thought prompt to employ LLMs to conduct a condition-aware tuning hints extraction task. To effectively integrate the mined tuning hints into RL agent training, we propose a hint-aware demonstration reinforcement learning algorithm HA-DDPGfD in DemoTuner. As far as we know, DemoTuner is the first work to introduce the demonstration reinforcement learning algorithm for DBMS knobs tuning. Experimental evaluations conducted on MySQL and PostgreSQL across various workloads demonstrate the significant advantages of DemoTuner in both performance improvement and online tuning cost reduction over three representative baselines including DB-BERT, GPTuner and CDBTune. Additionally, DemoTuner also exhibits superior adaptability to application scenarios with unknown workloads.

</details>


### [13] [Probability-Biased Attention over Directed Bipartite Graphs for Long-Tail ICD Coding](https://arxiv.org/abs/2511.09559)
*Tianlei Chen,Yuxiao Chen,Yang Li,Feifei Wang*

Main category: cs.LG

TL;DR: 提出了一种新的 ICD 编码方法，通过对常见码和罕见码之间的共现关系进行建模来解决长尾分布问题。


<details>
  <summary>Details</summary>
Motivation: ICD 编码任务具有标签空间大和长尾分布的挑战，罕见码缺乏足够的训练数据。

Method: 构建了一个有向二分图编码器，利用概率偏差将常见码的信息传递给罕见码，并通过大型语言模型生成代码描述来丰富初始嵌入。

Result: 在三个 ICD 编码基准数据集上实现了最先进的性能，尤其是在 Macro-F1 上有显著提升。

Conclusion: 该方法通过对码之间的共现关系进行建模，有效地提高了罕见码的分类性能。

Abstract: Automated International Classification of Diseases (ICD) coding aims to assign multiple disease codes to clinical documents, constituting a crucial multi-label text classification task in healthcare informatics. However, the task is challenging due to its large label space (10,000 to 20,000 codes) and long-tail distribution, where a few codes dominate while many rare codes lack sufficient training data. To address this, we propose a learning method that models fine-grained co-occurrence relationships among codes. Specifically, we construct a Directed Bipartite Graph Encoder with disjoint sets of common and rare code nodes. To facilitate a one-way information flow, edges are directed exclusively from common to rare codes. The nature of these connections is defined by a probability-based bias, which is derived from the conditional probability of a common code co-occurring given the presence of a rare code. This bias is then injected into the encoder's attention module, a process we term Co-occurrence Encoding. This structure empowers the graph encoder to enrich rare code representations by aggregating latent comorbidity information reflected in the statistical co-occurrence of their common counterparts. To ensure high-quality input to the graph, we utilize a large language model (LLM) to generate comprehensive descriptions for codes, enriching initial embeddings with clinical context and comorbidity information, serving as external knowledge for the statistical co-occurrence relationships in the code system. Experiments on three automated ICD coding benchmark datasets demonstrate that our method achieves state-of-the-art performance with particularly notable improvements in Macro-F1, which is the key metric for long-tail classification.

</details>
