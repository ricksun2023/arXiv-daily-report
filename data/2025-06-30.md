<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 55]
- [cs.CV](#cs.CV) [Total: 56]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.IR](#cs.IR) [Total: 20]
- [cs.LG](#cs.LG) [Total: 50]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Efficient Multilingual ASR Finetuning via LoRA Language Experts](https://arxiv.org/abs/2506.21555)
*Jiahong Li,Yiwen Shao,Jianheng Zhuo,Chenda Li,Liliang Tang,Dong Yu,Yanmin Qian*

Main category: cs.CL

TL;DR: This paper introduces a LoRA-based finetuning method for multilingual ASR that improves performance by 10-15% compared to standard finetuning.


<details>
  <summary>Details</summary>
Motivation: Multilingual ASR suffers from the curse of multilinguality, where different languages interfere with each other.

Method: LoRA language experts based on Whisper, using LoRA expert fusion or knowledge distillation.

Result: Approximately 10% and 15% relative performance gains in language-aware and language-agnostic scenarios, respectively.

Conclusion: The proposed LoRA-based finetuning framework achieves better recognition performance on target languages than standard fine-tuning methods.

Abstract: Recent advancements in deep learning have significantly enhanced multilingual
automatic speech recognition (ASR) due to the development of advanced model
architectures and available large-scale multilingual datasets. Despite that,
multilingual ASR still suffers from the curse of multilinguality in that
different languages tend to interfere with each other, making it difficult for
the ASR model to identify multiple languages effectively while sharing model
capacity across them. This paper proposes an efficient finetuning framework for
customized multilingual ASR via prepared LoRA language experts based on
Whisper. Through LoRA expert fusion or knowledge distillation, our approach
achieves better recognition performance on target languages than standard
fine-tuning methods. Experimental results demonstrate that the proposed models
yield approximately 10\% and 15\% relative performance gains in language-aware
and language-agnostic scenarios, respectively.

</details>


### [2] [VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21556)
*Hyeongcheol Park,MinHyuk Jang,Ha Dam Baek,Gyusam Chang,Jiyoung Seo,Jiwan Park,Hogun Park,Sangpil Kim*

Main category: cs.CL

TL;DR: 提出了 VAT-KG，一个涵盖视觉、音频和文本信息的多模态知识图谱，并提出了一个新的多模态 RAG 框架，实验证明了 VAT-KG 在支持 MLLM 方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的 MMKG 通常在范围上受到限制，导致知识覆盖范围过时或不完整，并且它们通常仅支持窄范围的模式，例如文本和视觉信息。这些限制降低了它们的可扩展性和对广泛的多模态任务的适用性，特别是在该领域转向更丰富的模式（如视频和音频）时。

Method: 提出 Visual-Audio-Text Knowledge Graph (VAT-KG)，这是一个概念中心和知识密集型的多模态知识图谱，涵盖视觉、音频和文本信息。构建流程通过一系列严格的过滤和对齐步骤，确保多模态数据和细粒度语义之间的跨模态知识对齐，从而能够从任何多模态数据集自动生成 MMKG。

Result: 引入了一种新颖的多模态 RAG 框架，该框架检索详细的概念级知识以响应来自任意模式的查询。

Conclusion: VAT-KG 通过在各种模式的问题回答任务中的实验证明了其在支持 MLLM 方面的有效性，突出了其在统一和利用多模态知识方面的实际价值。

Abstract: Multimodal Knowledge Graphs (MMKGs), which represent explicit knowledge
across multiple modalities, play a pivotal role by complementing the implicit
knowledge of Multimodal Large Language Models (MLLMs) and enabling more
grounded reasoning via Retrieval Augmented Generation (RAG). However, existing
MMKGs are generally limited in scope: they are often constructed by augmenting
pre-existing knowledge graphs, which restricts their knowledge, resulting in
outdated or incomplete knowledge coverage, and they often support only a narrow
range of modalities, such as text and visual information. These limitations
reduce their extensibility and applicability to a broad range of multimodal
tasks, particularly as the field shifts toward richer modalities such as video
and audio in recent MLLMs. Therefore, we propose the Visual-Audio-Text
Knowledge Graph (VAT-KG), the first concept-centric and knowledge-intensive
multimodal knowledge graph that covers visual, audio, and text information,
where each triplet is linked to multimodal data and enriched with detailed
descriptions of concepts. Specifically, our construction pipeline ensures
cross-modal knowledge alignment between multimodal data and fine-grained
semantics through a series of stringent filtering and alignment steps, enabling
the automatic generation of MMKGs from any multimodal dataset. We further
introduce a novel multimodal RAG framework that retrieves detailed
concept-level knowledge in response to queries from arbitrary modalities.
Experiments on question answering tasks across various modalities demonstrate
the effectiveness of VAT-KG in supporting MLLMs, highlighting its practical
value in unifying and leveraging multimodal knowledge.

</details>


### [3] [Debunk and Infer: Multimodal Fake News Detection via Diffusion-Generated Evidence and LLM Reasoning](https://arxiv.org/abs/2506.21557)
*Kaiying Yan,Moyang Liu,Yukun Liu,Ruibo Fu,Zhengqi Wen,Jianhua Tao,Xuefei Liu*

Main category: cs.CL

TL;DR: 本文提出了DIFND框架，用于检测假新闻，通过生成反驳证据和多模态推理，提高了检测准确性和可信度，并在FakeSV和FVC数据集上取得了优异成果。


<details>
  <summary>Details</summary>
Motivation: 多媒体平台上假新闻的迅速传播对信息可信度提出了严峻的挑战。

Method: 提出了一个用于假新闻检测的Debunk-and-Infer框架(DIFND)，该框架利用反驳知识来增强假新闻检测的性能和可解释性。DIFND集成了条件扩散模型的生成能力和多模态大型语言模型(MLLM)的协同推理能力。

Result: DIFND在检测精度方面取得了显著提高。

Conclusion: DIFND在FakeSV和FVC数据集上表现出色，优于现有方法，并提供值得信赖的决策。

Abstract: The rapid spread of fake news across multimedia platforms presents serious
challenges to information credibility. In this paper, we propose a
Debunk-and-Infer framework for Fake News Detection(DIFND) that leverages
debunking knowledge to enhance both the performance and interpretability of
fake news detection. DIFND integrates the generative strength of conditional
diffusion models with the collaborative reasoning capabilities of multimodal
large language models (MLLMs). Specifically, debunk diffusion is employed to
generate refuting or authenticating evidence based on the multimodal content of
news videos, enriching the evaluation process with diverse yet semantically
aligned synthetic samples. To improve inference, we propose a chain-of-debunk
strategy where a multi-agent MLLM system produces logic-grounded,
multimodal-aware reasoning content and final veracity judgment. By jointly
modeling multimodal features, generative debunking cues, and reasoning-rich
verification within a unified architecture, DIFND achieves notable improvements
in detection accuracy. Extensive experiments on the FakeSV and FVC datasets
show that DIFND not only outperforms existing approaches but also delivers
trustworthy decisions.

</details>


### [4] [Bench to the Future: A Pastcasting Benchmark for Forecasting Agents](https://arxiv.org/abs/2506.21558)
*FutureSearch,:,Jack Wildman,Nikos I. Bosse,Daniel Hnyk,Peter Mühlbacher,Finn Hambly,Jon Evans,Dan Schwarz,Lawrence Phillips*

Main category: cs.CL

TL;DR: 提出了一个名为BTF的“过去预测”基准测试，用于评估LLM预测能力。


<details>
  <summary>Details</summary>
Motivation: 预测是一个具有挑战性的任务，它提供了一种清晰可衡量的方法来研究人工智能系统。预测需要在互联网上进行大量研究，并且评估需要时间才能发生，这使得预测基准的开发具有挑战性。迄今为止，还没有预测基准为LLM预测器提供真实、封闭和可重复的环境。

Method: 提出了一个“过去预测”基准测试，其中包含数百个高质量的问题，这些问题的答案是已知的，每个问题都附带有数万个相关的网页离线语料库。

Result: 过去预测环境可以产生与基于当时未解决问题的互联网预测结果相当的结果。使用包括最近发布的Claude 4模型在内的多个LLM，展示了代理和思维链预测方法，并证明了BTF能够跟踪预测能力的持续提升。

Conclusion: BTF能够跟踪预测能力的持续提升，并且会不断增加新的问题。

Abstract: Forecasting is a challenging task that offers a clearly measurable way to
study AI systems. Forecasting requires a large amount of research on the
internet, and evaluations require time for events to happen, making the
development of forecasting benchmarks challenging. To date, no forecasting
benchmark provides a realistic, hermetic, and repeatable environment for LLM
forecasters. We introduce Bench To the Future (BTF), a "pastcasting" benchmark
with hundreds of high-quality questions for which the resolution is already
known. Each question is accompanied by a large offline corpus of tens of
thousands of relevant web pages, enabling a way to elicit realistic "forecasts"
on past events from LLMs. Results suggest that our pastcasting environment can
produce results comparable to those based on forecasts using the internet on
at-the-time unresolved questions. We show results benchmarking agent and
chain-of-thought forecasting approaches using several LLMs, including the
recently-released Claude 4 models, and demonstrate BTF's ability to track
steady forecasting capability progress over time. We intend this to be a living
benchmark, with new questions added continually to account for increasing
training data cutoff dates. We invite researchers to contact us at
hello@futuresearch.ai to utilize our benchmark or tooling for their own
research.

</details>


### [5] [GraphLAMA: Enabling Efficient Adaptation of Graph Language Models with Limited Annotations](https://arxiv.org/abs/2506.21559)
*Junze Chen,Cheng Yang,Shujie Li,Zhiqiang Zhang,Yawen Li,Junping Du,Chuan Shi*

Main category: cs.CL

TL;DR: GraphLAMA introduces an extra parameter adaptation stage that can efficiently tailor GLMs to an unseen graph and task with only a few labeled examples, in exchange for better prediction accuracy and faster inference speed.


<details>
  <summary>Details</summary>
Motivation: ICL on graphs has effectiveness issues due to fixed parameters and efficiency issues due to long context. Meanwhile, the large amount of labeled data required for instruction tuning can be difficult to obtain in real-world scenarios. To this end, we aim to introduce an extra parameter adaptation stage that can efficiently tailor GLMs to an unseen graph and task with only a few labeled examples, in exchange for better prediction accuracy and faster inference speed.

Method: propose GraphLAMA method, with its model backbone and learning schemes specialized for efficient tuning and inference. Specifically, for model backbone, we use a graph neural network (GNN) with several well-designed components to transform nodes into the representation space of LLM tokens. Task instructions can then be represented as a mixture of node and language tokens. In the pre-training stage, model parameters except the LLM will be trained with different tasks to capture general knowledge. In the adaptation stage, only a few pre-trained parameters will be updated based on few-shot examples.

Result: achieves state-of-the-art performance with 4.91% absolution improvement in accuracy. Compared with ICL, our inference speed can be 10 times faster under 5-shot setting.

Conclusion: The proposed GraphLAMA achieves state-of-the-art performance with 4.91% absolution improvement in accuracy. Compared with ICL, the inference speed can be 10 times faster under 5-shot setting.

Abstract: Large language models (LLMs) have demonstrated their strong capabilities in
various domains, and have been recently integrated for graph analysis as graph
language models (GLMs). With LLMs as the predictor, some GLMs can interpret
unseen tasks described by natural language, and learn from a few examples in
the prompts without parameter tuning, known as in-context learning (ICL).
Another subset of GLMs utilizes abundant training labels to enhance model
performance, known as instruction tuning. However, we argue that ICL on graphs
has effectiveness issues due to fixed parameters and efficiency issues due to
long context. Meanwhile, the large amount of labeled data required for
instruction tuning can be difficult to obtain in real-world scenarios. To this
end, we aim to introduce an extra parameter adaptation stage that can
efficiently tailor GLMs to an unseen graph and task with only a few labeled
examples, in exchange for better prediction accuracy and faster inference
speed. For implementation, in this paper we propose GraphLAMA method, with its
model backbone and learning schemes specialized for efficient tuning and
inference. Specifically, for model backbone, we use a graph neural network
(GNN) with several well-designed components to transform nodes into the
representation space of LLM tokens. Task instructions can then be represented
as a mixture of node and language tokens. In the pre-training stage, model
parameters except the LLM will be trained with different tasks to capture
general knowledge. In the adaptation stage, only a few pre-trained parameters
will be updated based on few-shot examples. Extensive experiments on
few/zero-shot node classification and summary generation show that our proposed
GraphLAMA achieves state-of-the-art performance with 4.91% absolution
improvement in accuracy. Compared with ICL, our inference speed can be 10 times
faster under 5-shot setting.

</details>


### [6] [Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning](https://arxiv.org/abs/2506.21560)
*Yifu Han,Geo Zhang*

Main category: cs.CL

TL;DR: 本研究比较了几种微调技术在小型语言模型上的效果，发现 RLOO 和 DPO 在指令跟随方面表现良好，而合成数据增强有助于提高数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 本研究调查了强化学习 (RL) 微调技术在紧凑型语言模型 (Qwen2.5-0.5B Base) 上对两个具有挑战性的任务（指令跟随和数学推理）的有效性。

Method: 有监督微调 (SFT)、使用偏好标记数据的直接偏好优化 (DPO) 以及使用奖励模型的强化留一法 (RLOO)

Result: RLOO 与 DeBERTa 奖励建模实现了最佳对齐，而 DPO 提供了强大且一致的结果。对于数学推理任务，合成数据增强和 Best-of-N 采样与外部验证器显着提高了准确性

Conclusion: 本研究强调了训练轻量级、任务对齐的小型语言模型的关键权衡和实用策略。

Abstract: This study investigates the effectiveness of reinforcement learning (RL)
fine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two
challenging tasks: instruction following and mathematical reasoning. We compare
supervised fine-tuning (SFT), Direct Preference Optimization (DPO) using
preference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models.
Our experiments show that RLOO with DeBERTa reward modeling achieves the best
alignment, while DPO provides strong and consistent results. For math reasoing
tasks, synthetic data augmentation and best-of-N sampling with an external
verifier significantly improve accuracy, showing the potential of combining
fine-tuning with inference-time tools. This study highlights key trade-offs and
practical strategies for training lightweight, task-aligned small-scale
language models.

</details>


### [7] [Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs](https://arxiv.org/abs/2506.21561)
*Emilio Barkett,Olivia Long,Madhavendra Thakur*

Main category: cs.CL

TL;DR: This study presents the largest evaluation to date of LLMs' veracity detection capabilities and the first analysis of these capabilities in reasoning models.


<details>
  <summary>Details</summary>
Motivation: LLMs remain poorly understood as judges of truth

Method: eight LLMs make 4,800 veracity judgments across several prompts, comparing reasoning and non-reasoning models

Result: rates of truth-bias are lower in reasoning models than in non-reasoning models, but still higher than human benchmarks. identify sycophantic tendencies in several advanced models, which displayed an asymmetry in detection accuracy, performing well in truth accuracy but poorly in deception accuracy

Conclusion: capability advances alone do not resolve fundamental veracity detection challenges in LLMs

Abstract: Despite their widespread use in fact-checking, moderation, and high-stakes
decision-making, large language models (LLMs) remain poorly understood as
judges of truth. This study presents the largest evaluation to date of LLMs'
veracity detection capabilities and the first analysis of these capabilities in
reasoning models. We had eight LLMs make 4,800 veracity judgments across
several prompts, comparing reasoning and non-reasoning models. We find that
rates of truth-bias, or the likelihood to believe a statement is true,
regardless of whether it is actually true, are lower in reasoning models than
in non-reasoning models, but still higher than human benchmarks. Most
concerning, we identify sycophantic tendencies in several advanced models
(o4-mini and GPT-4.1 from OpenAI, R1 from DeepSeek), which displayed an
asymmetry in detection accuracy, performing well in truth accuracy but poorly
in deception accuracy. This suggests that capability advances alone do not
resolve fundamental veracity detection challenges in LLMs.

</details>


### [8] [FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction](https://arxiv.org/abs/2506.21562)
*Jun Yin,Pengyu Zeng,Jing Zhong,Peilin Li,Miao Zhang,Ran Luo,Shuai Lu*

Main category: cs.CL

TL;DR: 提出了一种新的平面图生成模型FPDS，它通过预测下一个房间来逐步生成平面图，更符合建筑设计的迭代过程。


<details>
  <summary>Details</summary>
Motivation: 现有的平面图生成模型主要是端到端生成，以单次生成整个基于像素的布局。这种范例通常与现实世界建筑实践中观察到的增量工作流程不兼容。

Method: 提出了一种新颖的“下一个房间预测”范例，该范例专为建筑平面图建模而定制。

Result: 实验评估表明，FPDS在文本到平面图的任务中表现出竞争力的性能。

Conclusion: FPDS在文本到平面图的任务中表现出与扩散模型和Tell2Design相比具有竞争力的性能，表明其在支持未来智能建筑设计方面的潜在适用性。

Abstract: In the architectural design process, floor plan generation is inherently
progressive and iterative. However, existing generative models for floor plans
are predominantly end-to-end generation that produce an entire pixel-based
layout in a single pass. This paradigm is often incompatible with the
incremental workflows observed in real-world architectural practice. To address
this issue, we draw inspiration from the autoregressive 'next token prediction'
mechanism commonly used in large language models, and propose a novel 'next
room prediction' paradigm tailored to architectural floor plan modeling.
Experimental evaluation indicates that FPDS demonstrates competitive
performance in comparison to diffusion models and Tell2Design in the
text-to-floorplan task, indicating its potential applicability in supporting
future intelligent architectural design.

</details>


### [9] [FormosanBench: Benchmarking Low-Resource Austronesian Languages in the Era of Large Language Models](https://arxiv.org/abs/2506.21563)
*Kaiying Kevin Lin,Hsiyu Chen,Haopeng Zhang*

Main category: cs.CL

TL;DR: introduce FORMOSANBENCH to evaluate LLMs on low-resource Austronesian languages


<details>
  <summary>Details</summary>
Motivation: LLMs capabilities in low-resource and minority languages remain significantly underexplored. Formosan languages are both linguistically rich and endangered, largely due to the sociolinguistic dominance of Mandarin.

Method: introduce FORMOSANBENCH, the first benchmark for evaluating LLMs on low-resource Austronesian languages. It covers three endangered Formosan languages: Atayal, Amis, and Paiwan, across three core NLP tasks: machine translation, automatic speech recognition (ASR), and text summarization. Assess model performance in zero-shot, 10-shot, and fine-tuned settings using FORMOSANBENCH.

Result: reveal a substantial performance gap between high-resource and Formosan languages.

Conclusion: Existing LLMs consistently underperform across all tasks, with 10-shot learning and fine-tuning offering only limited improvements.

Abstract: While large language models (LLMs) have demonstrated impressive performance
across a wide range of natural language processing (NLP) tasks in high-resource
languages, their capabilities in low-resource and minority languages remain
significantly underexplored. Formosan languages -- a subgroup of Austronesian
languages spoken in Taiwan -- are both linguistically rich and endangered,
largely due to the sociolinguistic dominance of Mandarin. In this work, we
introduce FORMOSANBENCH, the first benchmark for evaluating LLMs on
low-resource Austronesian languages. It covers three endangered Formosan
languages: Atayal, Amis, and Paiwan, across three core NLP tasks: machine
translation, automatic speech recognition (ASR), and text summarization. We
assess model performance in zero-shot, 10-shot, and fine-tuned settings using
FORMOSANBENCH. Our results reveal a substantial performance gap between
high-resource and Formosan languages. Existing LLMs consistently underperform
across all tasks, with 10-shot learning and fine-tuning offering only limited
improvements. These findings underscore the urgent need for more inclusive NLP
technologies that can effectively support endangered and underrepresented
languages. We release our datasets and code to facilitate future research in
this direction.

</details>


### [10] [Team QUST at SemEval-2025 Task 10: Evaluating Large Language Models in Multiclass Multi-label Classification of News Entity Framing](https://arxiv.org/abs/2506.21564)
*Jiyan Liu,Youzheng Liu,Taihang Wang,Xiaoman Xu,Yimin Wang,Ye Jiang*

Main category: cs.CL

TL;DR: QUST_NLP 参与了 SemEval-2025 Task 7，提出了一个三阶段检索框架，在单语赛道获得第 5 名，在跨语言赛道获得第 7 名。


<details>
  <summary>Details</summary>
Motivation: 参与 SemEval-2025 Task 7。

Method: 一个专门为事实核查声明检索设计的三阶段检索框架。

Result: 使用多个重排序模型来增强候选结果，每个模型选择前 10 个结果。在最后阶段，我们利用加权投票来确定最终检索结果。

Conclusion: 该方法在单语赛道获得第 5 名，在跨语言赛道获得第 7 名。

Abstract: This paper describes the participation of QUST_NLP in the SemEval-2025 Task
7. We propose a three-stage retrieval framework specifically designed for
fact-checked claim retrieval. Initially, we evaluate the performance of several
retrieval models and select the one that yields the best results for candidate
retrieval. Next, we employ multiple re-ranking models to enhance the candidate
results, with each model selecting the Top-10 outcomes. In the final stage, we
utilize weighted voting to determine the final retrieval outcomes. Our approach
achieved 5th place in the monolingual track and 7th place in the crosslingual
track. We release our system code at:
https://github.com/warmth27/SemEval2025_Task7.

</details>


### [11] [A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing](https://arxiv.org/abs/2506.21565)
*Takato Ueno,Keito Inoshita*

Main category: cs.CL

TL;DR: This paper proposes KCS+IBC, a multi-agent LLM framework inspired by Japanese communication practices, to improve sentiment analysis through bias mitigation, explainability, and probabilistic prediction.


<details>
  <summary>Details</summary>
Motivation: Inspired by Japan's kairanban culture and idobata conversations, traditional communication practices that foster nuanced dialogue among community members and contribute to the formation of social balance.

Method: A multi-agent inference framework (KCS+IBC) that integrates multiple large language models (LLMs) with a mid-phase casual dialogue session to blend formal inference with individual perspectives and introduces probabilistic sentiment prediction.

Result: KCS achieves accuracy comparable to that of a single LLM across datasets, while KCS+IBC exhibits a consistent decrease in entropy and a gradual increase in variance during the latter stages of inference.

Conclusion: KCS+IBC balances aggregation and diversity of predictions, as shown by a consistent decrease in entropy and a gradual increase in variance. Future work will quantitatively assess the impact of these characteristics on bias correction and develop more advanced sentiment analysis systems.

Abstract: Japan's kairanban culture and idobata conversations have long functioned as
traditional communication practices that foster nuanced dialogue among
community members and contribute to the formation of social balance. Inspired
by these information exchange processes, this study proposes a multi-agent
inference framework (KCS+IBC) that integrates multiple large language models
(LLMs) to achieve bias mitigation, improved explainability, and probabilistic
prediction in sentiment analysis. In addition to sequentially sharing
prediction results, the proposed method incorporates a mid-phase casual
dialogue session to blend formal inference with individual perspectives and
introduces probabilistic sentiment prediction. Experimental results show that
KCS achieves accuracy comparable to that of a single LLM across datasets, while
KCS+IBC exhibits a consistent decrease in entropy and a gradual increase in
variance during the latter stages of inference, suggesting the framework's
ability to balance aggregation and diversity of predictions. Future work will
quantitatively assess the impact of these characteristics on bias correction
and aim to develop more advanced sentiment analysis systems.

</details>


### [12] [The Saturation Point of Backtranslation in High Quality Low Resource English Gujarati Machine Translation](https://arxiv.org/abs/2506.21566)
*Arwa Arif*

Main category: cs.CL

TL;DR: Backtranslation's effectiveness in high-quality, low-resource MT is explored. Results show diminishing returns, suggesting limitations in such settings.


<details>
  <summary>Details</summary>
Motivation: explore the effectiveness of backtranslation for English Gujarati translation in high quality, low resource settings

Method: backtranslation with multilingual pretrained MBART50 model

Result: adding synthetic data does not improve translation performance and, in some cases, slightly reduces it

Conclusion: backtranslation may reach a point of diminishing returns in certain low-resource settings

Abstract: Backtranslation BT is widely used in low resource machine translation MT to
generate additional synthetic training data using monolingual corpora. While
this approach has shown strong improvements for many language pairs, its
effectiveness in high quality, low resource settings remains unclear. In this
work, we explore the effectiveness of backtranslation for English Gujarati
translation using the multilingual pretrained MBART50 model. Our baseline
system, trained on a high quality parallel corpus of approximately 50,000
sentence pairs, achieves a BLEU score of 43.8 on a validation set. We augment
this data with carefully filtered backtranslated examples generated from
monolingual Gujarati text. Surprisingly, adding this synthetic data does not
improve translation performance and, in some cases, slightly reduces it. We
evaluate our models using multiple metrics like BLEU, ChrF++, TER, BLEURT and
analyze possible reasons for this saturation. Our findings suggest that
backtranslation may reach a point of diminishing returns in certain
low-resource settings and we discuss implications for future research.

</details>


### [13] [BioPars: A Pretrained Biomedical Large Language Model for Persian Biomedical Text Mining](https://arxiv.org/abs/2506.21567)
*Baqer M. Merzah,Tania Taami,Salman Asoudeh,Amir reza Hossein pour,Saeed Mirzaee,Amir Ali Bengari*

Main category: cs.CL

TL;DR: This paper introduces BioPars, a new method and dataset (BioParsQA) for evaluating LLMs in Persian medical QA. BioPars outperforms existing models but highlights the need for further improvements.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used for complex analysis in bioinformatics, but their capabilities in specialized fields require further investigation, especially for real-world questions and fine-grained inferences.

Method: The study introduces BioPars, a measure to assess LLMs on subject-specific knowledge, interpretation, synthesis, and evidence demonstration. It also introduces BIOPARS-BENCH and BioParsQA datasets. The LLMs ChatGPT, Llama, and Galactica are compared.

Result: BioPars outperforms ChatGPT, Llama, and Galactica on Persian medical QA. On BioParsQA, it achieved a ROUGE-L score of 29.99, a BERTScore of 90.87, a MoverScore of 60.43, and a BLEURT score of 50.78.

Conclusion: BioPars achieves state-of-the-art results on Persian medical QA, demonstrating the potential of LLMs in this area but also highlighting the need for further fine-tuning.

Abstract: Large Language Models (LLMs) have recently gained attention in the life
sciences due to their capacity to model, extract, and apply complex biological
information. Beyond their classical use as chatbots, these systems are
increasingly used for complex analysis and problem-solving in specialized
fields, including bioinformatics. First, we introduce BIOPARS-BENCH, a dataset
from over 10,000 scientific articles, textbooks, and medical websites.
BioParsQA was also introduced to evaluate the proposed model, which consists of
5,231 Persian medical questions and answers. This study then introduces
BioPars, a simple but accurate measure designed to assess LLMs for three main
abilities: acquiring subject-specific knowledge, interpreting and synthesizing
such knowledge, and demonstrating proper evidence. Comparing ChatGPT, Llama,
and Galactica, our study highlights their ability to remember and retrieve
learned knowledge but also reveals shortcomings in addressing higher-level,
real-world questions and fine-grained inferences. These findings indicate the
need for further fine-tuning to address the capabilities of LLM in
bioinformatics tasks. To our knowledge, BioPars is the first application of LLM
in Persian medical QA, especially for generating long answers. Evaluation of
four selected medical QA datasets shows that BioPars has achieved remarkable
results compared to comparative approaches. The model on BioParsQA achieved a
ROUGE-L score of 29.99, which is an improvement over GPT-4 1.0. The model
achieved a BERTScore of 90.87 with the MMR method. The MoverScore and BLEURT
values were also higher in this model than the other three models. In addition,
the reported scores for the model are MoverScore=60.43 and BLEURT=50.78.
BioPars is an ongoing project and all resources related to its development will
be made available via the following GitHub repository:
https://github.com/amirap80/BioPars.

</details>


### [14] [Assessing RAG and HyDE on 1B vs. 4B-Parameter Gemma LLMs for Personal Assistants Integretion](https://arxiv.org/abs/2506.21568)
*Andrejs Sorstkins*

Main category: cs.CL

TL;DR: This study evaluates RAG and HyDE on compact Gemma LLMs for a privacy-first personal assistant. RAG reduces latency and hallucinations, making it suitable for on-device use.


<details>
  <summary>Details</summary>
Motivation: Resource efficiency is a critical barrier to deploying large language models in edge and privacy-sensitive applications.

Method: We implement short-term memory via MongoDB and long-term semantic storage via Qdrant, orchestrated through FastAPI and LangChain, and expose the system through a React.js frontend.

Result: RAG consistently reduces latency by up to 17% and eliminates factual hallucinations. HyDE enhances semantic relevance but increases response time and hallucination rate. Scaling models yields marginal throughput gains for baseline and RAG pipelines, but magnifies HyDE's computational overhead and variability.

Conclusion: RAG is the pragmatic choice for on-device personal assistants powered by small-scale LLMs.

Abstract: Resource efficiency is a critical barrier to deploying large language models
(LLMs) in edge and privacy-sensitive applications. This study evaluates the
efficacy of two augmentation strategies--Retrieval-Augmented Generation (RAG)
and Hypothetical Document Embeddings (HyDE)--on compact Gemma LLMs of 1 billion
and 4 billion parameters, within the context of a privacy-first personal
assistant. We implement short-term memory via MongoDB and long-term semantic
storage via Qdrant, orchestrated through FastAPI and LangChain, and expose the
system through a React.js frontend. Across both model scales, RAG consistently
reduces latency by up to 17\% and eliminates factual hallucinations when
responding to user-specific and domain-specific queries. HyDE, by contrast,
enhances semantic relevance--particularly for complex physics prompts--but
incurs a 25--40\% increase in response time and a non-negligible hallucination
rate in personal-data retrieval. Comparing 1 B to 4 B models, we observe that
scaling yields marginal throughput gains for baseline and RAG pipelines, but
magnifies HyDE's computational overhead and variability. Our findings position
RAG as the pragmatic choice for on-device personal assistants powered by
small-scale LLMs.

</details>


### [15] [Hybrid-NL2SVA: Integrating RAG and Finetuning for LLM-based NL2SVA](https://arxiv.org/abs/2506.21569)
*Weihua Xiao,Derek Ekberg,Siddharth Garg,Ramesh Karri*

Main category: cs.CL

TL;DR: This paper introduces a customized RAG framework and a synthetic fine-tuning dataset to improve LLM performance in translating natural language to SystemVerilog Assertions (SVAs).


<details>
  <summary>Details</summary>
Motivation: Manually writing SystemVerilog Assertions (SVAs) from natural language property descriptions is labor-intensive and error-prone. Existing models struggle with understanding domain-specific syntax and semantics.

Method: A customized retrieval-augmented generation (RAG) framework and a synthetic fine-tuning dataset. Prompt-guided explanations teach LLMs the layer-by-layer construction process of concurrent SVAs, enabling supervised fine-tuning.

Result: The customized RAG framework increases the number of functionality matched SVAs by 58.42% over GPT-4o-mini, while Qwen2.5-Coder-7B-Instruct fine-tuned on the dataset and integrated with HybridRetrieval achieves a 59.05% improvement over the base Qwen model.

Conclusion: A customized RAG framework and a synthetic fine-tuning dataset improve LLM's performance in NL2SVA. Fine-tuning on the dataset and integrated with HybridRetrieval achieves a 59.05% improvement over the base Qwen model.

Abstract: SystemVerilog Assertions (SVAs) are critical for verifying the correctness of
hardware designs, but manually writing them from natural language property
descriptions, i.e., NL2SVA, remains a labor-intensive and error-prone task.
Recent advances in large language models (LLMs) offer opportunities to automate
this translation. However, existing models still struggle with understanding
domain-specific syntax and semantics. To enhance LLM performance in NL2SVA, we
propose a customized retrieval-augmented generation (RAG) framework and a
synthetic fine-tuning dataset that together improve LLM's performance. To
further improve lightweight models over NL2SVA, our fine-tuning dataset
provides prompt-guided explanations that teach LLMs the layer-by-layer
construction process of concurrent SVAs, enabling supervised fine-tuning that
greatly improves syntax and functionality accuracy. To evaluate the performance
of LLMs over NL2SVA, we construct the largest evaluation dataset for NL2SVA,
comprising 40 Verilog designs and 229 formally verified SVAs with detailed
annotations. Experimental results show that our customized RAG framework
increases the number of functionality matched SVAs by 58.42% over GPT-4o-mini,
while Qwen2.5-Coder-7B-Instruct fine-tuned on our fine-tuning dataset and
integrated with HybridRetrieval achieves a 59.05% over the base Qwen model.

</details>


### [16] [Random Initialization Can't Catch Up: The Advantage of Language Model Transfer for Time Series Forecasting](https://arxiv.org/abs/2506.21570)
*Roland Riachi,Kashif Rasul,Arjun Ashok,Prateek Humane,Alexis Roger,Andrew R. Williams,Yuriy Nevmyvaka,Irina Rish*

Main category: cs.CL

TL;DR: 分析了时间序列预测中语言模型的有效迁移，发现在低数据情况下，特定设计选择和持续存在的迁移差距会显著影响性能。


<details>
  <summary>Details</summary>
Motivation: 先前的工作已经证明了调整预训练语言模型（LM）在低数据情况下预测时间序列的有效性。

Method: 分析了在各种设计选择下，从语言模型到时间序列预测的有效迁移，包括上游后训练、时间序列分词器和语言骨干大小。

Result: 这些设计选择对验证损失有显著影响，明确的选择优于其他选择。与 Hernandez 等人 (2021) 的研究相反，我们观察到 LMs 的验证损失在随机初始化模型的验证损失收敛后继续平稳下降，导致了在各种设计选择中都存在的非消失迁移差距。

Conclusion: 在低数据情况下，语言模型的验证损失在随机初始化模型收敛后持续平稳下降，导致持续存在的迁移差距。

Abstract: Recent works have demonstrated the effectiveness of adapting pre-trained
language models (LMs) for forecasting time series in the low-data regime. We
build upon these findings by analyzing the effective transfer from language
models to time series forecasting under various design choices including
upstream post-training, time series tokenizer and language backbone size. In
the low-data regime, these design choices have a significant impact on the
validation loss, with clear-cut choices that outperform others. Contrary to
Hernandez et al. (2021), we observe that the validation loss of the LMs
continues to smoothly decrease long after the validation loss of the randomly
initialized models has converged, leading to a non-vanishing transfer gap that
holds across design choices. These findings not only help shed light on the
effective use of compute-efficient training for time series, but also open the
way for the study of modality-agnostic properties of data distributions
leveraged by these models.

</details>


### [17] [Towards Understanding the Cognitive Habits of Large Reasoning Models](https://arxiv.org/abs/2506.21571)
*Jianshuo Dong,Yujia Fu,Chuanrui Hu,Chao Zhang,Han Qiu*

Main category: cs.CL

TL;DR: LRMs show human-like thinking patterns that can predict misbehavior.


<details>
  <summary>Details</summary>
Motivation: Explore whether LRMs exhibit human-like cognitive habits, inspired by the observation of consistent CoT patterns across tasks.

Method: Introduce CogTest, a benchmark with 16 cognitive habits, and evaluate 16 LLMs using an evidence-first extraction method.

Result: LRMs, unlike conventional LLMs, exhibit human-like habits and adaptively deploy them. Certain habits are strongly associated with harmful responses.

Conclusion: LRMs exhibit human-like cognitive habits and adaptively deploy them, which can be indicative of potential misbehavior in safety-related tasks.

Abstract: Large Reasoning Models (LRMs), which autonomously produce a reasoning Chain
of Thought (CoT) before producing final responses, offer a promising approach
to interpreting and monitoring model behaviors. Inspired by the observation
that certain CoT patterns -- e.g., ``Wait, did I miss anything?'' --
consistently emerge across tasks, we explore whether LRMs exhibit human-like
cognitive habits. Building on Habits of Mind, a well-established framework of
cognitive habits associated with successful human problem-solving, we introduce
CogTest, a principled benchmark designed to evaluate LRMs' cognitive habits.
CogTest includes 16 cognitive habits, each instantiated with 25 diverse tasks,
and employs an evidence-first extraction method to ensure reliable habit
identification. With CogTest, we conduct a comprehensive evaluation of 16
widely used LLMs (13 LRMs and 3 non-reasoning ones). Our findings reveal that
LRMs, unlike conventional LLMs, not only exhibit human-like habits but also
adaptively deploy them according to different tasks. Finer-grained analyses
further uncover patterns of similarity and difference in LRMs' cognitive habit
profiles, particularly certain inter-family similarity (e.g., Qwen-3 models and
DeepSeek-R1). Extending the study to safety-related tasks, we observe that
certain habits, such as Taking Responsible Risks, are strongly associated with
the generation of harmful responses. These findings suggest that studying
persistent behavioral patterns in LRMs' CoTs is a valuable step toward deeper
understanding of LLM misbehavior. The code is available at:
https://github.com/jianshuod/CogTest.

</details>


### [18] [Aligning MLLM Benchmark With Human Preferences via Structural Equation Modeling](https://arxiv.org/abs/2506.21572)
*Tianyu. Zou,Shengwu. Xiong,Ruilin. Yao,Jirui. Huang,Yi. Rong,Yaxiong. Chen,Shili. Xiong,Cong. Wang*

Main category: cs.CL

TL;DR: 提出了一种新的 MLLM 基准框架，该框架基于结构方程建模和皮亚杰的认知发展理论，以提高可解释性和减少冗余。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏结构化、可解释和理论 обоснованной 基准设计，评估多模态大型语言模型 (MLLM) 仍然是一项根本性挑战。现有的基准测试通常采用基于启发式的任务分组，认知目标不明确，导致能力重叠、指标冗余和诊断能力有限。

Method: 基于结构方程建模 (SEM) 对齐 MLLM 基准测试，并根据皮亚杰的认知发展理论引入新的能力层次结构。

Result: 该基准测试表现出更强的可解释性，减少的指标冗余和更清晰的认知一致性。

Conclusion: 该基准测试比现有方法具有更强的可解释性，更少的指标冗余和更清晰的认知一致性。

Abstract: Evaluating multimodal large language models (MLLMs) remains a fundamental
challenge due to a lack of structured, interpretable, and theoretically
grounded benchmark designs. Existing benchmarks often adopt heuristic-based
task groupings with unclear cognitive targets, thus resulting in overlapping
abilities, redundant indicators, and limited diagnostic power. In this work, we
propose a novel framework for aligning MLLM benchmark based on Structural
Equation Modeling (SEM) to analyze and quantify the internal validity,
dimensional separability, and contribution of benchmark components. Motivated
by the observed limitations of current designs, we further introduce a novel
capability hierarchy grounded in Piagets theory of cognitive development,
dividing MLLM abilities into three hierarchical layers, i.e., Perception,
Memory, and Reasoning. We reorganize existing MLLM benchmarks under the
proposed framework and construct a new benchmark named Gold. Experimental
results demonstrate that the proposed benchmark exhibits stronger
interpretability, reduced indicator redundancy, and clearer cognitive
consistency compared to existing approaches.

</details>


### [19] [Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box LLMs](https://arxiv.org/abs/2506.21573)
*Yanwei Ren,Liu Liu,Baosheng Yu,Jiayan Qiu,Quan Chen*

Main category: cs.CL

TL;DR: This paper introduces a novel framework that combines black-box and white-box approaches to optimize instructions for large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Relying solely on white-box approaches demands extensive computational resources and offers limited representational capacity, while black-box models can incur prohibitive financial costs. To address these challenges

Method: We introduce a novel framework that seamlessly merges the strengths of both paradigms. Black-box models provide high-quality, diverse instruction initializations, and white-box models supply fine-grained interpretability through hidden states and output features. By enforcing a semantic similarity constraint, these components fuse into a unified high-dimensional representation that captures deep semantic and structural nuances, enabling an iterative optimization process to refine instruction quality and adaptability.

Result: our approach consistently outperforms state-of-the-art baselines. Extensive evaluations across a broad spectrum of tasks-ranging from complex reasoning to cross-lingual generalization

Conclusion: This fusion of black-box initialization with advanced semantic refinement yields a scalable and efficient solution, paving the way for next-generation LLM-driven applications in diverse real-world scenarios.

Abstract: Optimizing instructions for large language models (LLMs) is critical for
harnessing their full potential in complex and diverse tasks. However, relying
solely on white-box approaches demands extensive computational resources and
offers limited representational capacity, while black-box models can incur
prohibitive financial costs. To address these challenges, we introduce a novel
framework that seamlessly merges the strengths of both paradigms. Black-box
models provide high-quality, diverse instruction initializations, and white-box
models supply fine-grained interpretability through hidden states and output
features. By enforcing a semantic similarity constraint, these components fuse
into a unified high-dimensional representation that captures deep semantic and
structural nuances, enabling an iterative optimization process to refine
instruction quality and adaptability. Extensive evaluations across a broad
spectrum of tasks-ranging from complex reasoning to cross-lingual
generalization-demonstrate that our approach consistently outperforms
state-of-the-art baselines. This fusion of black-box initialization with
advanced semantic refinement yields a scalable and efficient solution, paving
the way for next-generation LLM-driven applications in diverse real-world
scenarios. The source code will be released soon.

</details>


### [20] [Digital Gatekeepers: Exploring Large Language Model's Role in Immigration Decisions](https://arxiv.org/abs/2506.21574)
*Yicheng Mao,Yang Zhao*

Main category: cs.CL

TL;DR: This study investigates the potential and limitations of LLMs in supporting immigration decision-making, revealing both benefits and biases.


<details>
  <summary>Details</summary>
Motivation: Immigration departments face significant work-loads and the challenge of ensuring fairness in decision-making processes. Integrating artificial intelligence offers a promising solution to these challenges.

Method: This paper conducted discrete choice experiments and in-depth interviews to study LLM decision-making strategies.

Result: LLMs can align their decision-making with human strategies, emphasizing utility maximization and procedural fairness. ChatGPT has safeguards to prevent unintentional discrimination, it still exhibits stereotypes and biases concerning nationality and shows preferences toward privileged group.

Conclusion: LLMs have the potential to automate and enhance immigration decisions, but they also exhibit stereotypes and biases concerning nationality and show preferences toward privileged groups.

Abstract: With globalization and increasing immigrant populations, immigration
departments face significant work-loads and the challenge of ensuring fairness
in decision-making processes. Integrating artificial intelligence offers a
promising solution to these challenges. This study investigates the potential
of large language models (LLMs),such as GPT-3.5 and GPT-4, in supporting
immigration decision-making. Utilizing a mixed-methods approach,this paper
conducted discrete choice experiments and in-depth interviews to study LLM
decision-making strategies and whether they are fair. Our findings demonstrate
that LLMs can align their decision-making with human strategies, emphasizing
utility maximization and procedural fairness. Meanwhile, this paper also
reveals that while ChatGPT has safeguards to prevent unintentional
discrimination, it still exhibits stereotypes and biases concerning nationality
and shows preferences toward privileged group. This dual analysis highlights
both the potential and limitations of LLMs in automating and enhancing
immigration decisions.

</details>


### [21] [MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark](https://arxiv.org/abs/2412.15194)
*Qihao Zhao,Yangyu Huang,Tengchao Lv,Lei Cui,Qinzheng Sun,Shaoguang Mao,Xin Zhang,Ying Xin,Qiufeng Yin,Scarlett Li,Furu Wei*

Main category: cs.CL

TL;DR: 提出了一个更严格、无污染的 MCQ 基准 MMLU-CF，用于评估大型语言模型的理解能力。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型 (LLM) 的训练数据的开源性和广泛来源，导致基准污染，从而导致不可靠的评估结果。为了缓解这个问题，我们提出了一个无污染且更具挑战性的 MCQ 基准，称为 MMLU-CF。

Method: 通过扩大数据来源和设计三个去污规则来避免意外的数据泄露；将基准分为验证集和测试集，测试集保持闭源以确保结果的可靠性。

Result: 创建了一个无污染且更具挑战性的 MCQ 基准 MMLU-CF。GPT-4o 在该基准测试集上表现有所下降。

Conclusion: GPT-4o 在 MMLU-CF 测试集上的 5-shot 得分为 73.4%，0-shot 得分为 71.9%，表明该方法在创建更严格和无污染的评估标准方面的有效性。

Abstract: Multiple-choice question (MCQ) datasets like Massive Multitask Language
Understanding (MMLU) are widely used to evaluate the commonsense,
understanding, and problem-solving abilities of large language models (LLMs).
However, the open-source nature of these benchmarks and the broad sources of
training data for LLMs have inevitably led to benchmark contamination,
resulting in unreliable evaluation results. To alleviate this issue, we propose
a contamination-free and more challenging MCQ benchmark called MMLU-CF. This
benchmark reassesses LLMs' understanding of world knowledge by averting both
unintentional and malicious data leakage. To avoid unintentional data leakage,
we source data from a broader domain and design three decontamination rules. To
prevent malicious data leakage, we divide the benchmark into validation and
test sets with similar difficulty and subject distributions. The test set
remains closed-source to ensure reliable results, while the validation set is
publicly available to promote transparency and facilitate independent
verification. Our evaluation of mainstream LLMs reveals that the powerful
GPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on
the test set, which indicates the effectiveness of our approach in creating a
more rigorous and contamination-free evaluation standard. The GitHub repository
is available at https://github.com/microsoft/MMLU-CF and the dataset refers to
https://huggingface.co/datasets/microsoft/MMLU-CF.

</details>


### [22] [STRuCT-LLM: Unifying Tabular and Graph Reasoning with Reinforcement Learning for Semantic Parsing](https://arxiv.org/abs/2506.21575)
*Josefa Lia Stoisser,Marc Boubnovski Martell,Lawrence Phillips,Casper Hansen,Julien Fauqueur*

Main category: cs.CL

TL;DR: STRuCT-LLM是一个统一的框架，用于训练大型语言模型，以对关系和图结构化数据执行结构化推理，通过联合优化Text-to-SQL和Text-to-Cypher任务，并在多个任务上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有工作通常孤立地处理关系和图形式，而STRuCT-LLM旨在利用SQL和Cypher之间的共享抽象来诱导跨形式迁移，从而使SQL训练能够提高Cypher性能，反之亦然，即使没有共享模式。

Method: 该方法结合了强化学习（RL）和思维链（CoT）监督，共同优化Text-to-SQL和Text-to-Cypher任务。此外，还引入了基于图编辑距离的拓扑感知奖励函数，以支持基于图的解析中的细粒度优化。

Result: STRuCT-LLM在Spider上提高了13.5%，在Text2Cypher上提高了73.1%。在下游表格问答（TableBench）和知识图谱问答（CR-LT-KGQA）上，无需任何QA特定的监督，分别提高了8.5%和1.7%。

Conclusion: STRuCT-LLM通过联合训练SQL和Cypher，并利用可执行查询作为结构化推理的支架，在语义解析、表格问答和知识图谱问答任务上都取得了显著的性能提升，证明了该方法在结构化数据推理方面的有效性和协同优势。

Abstract: We propose STRuCT-LLM, a unified framework for training large language models
(LLMs) to perform structured reasoning over both relational and
graph-structured data. Our approach jointly optimizes Text-to-SQL and
Text-to-Cypher tasks using reinforcement learning (RL) combined with
Chain-of-Thought (CoT) supervision. To support fine-grained optimization in
graph-based parsing, we introduce a topology-aware reward function based on
graph edit distance. Unlike prior work that treats relational and graph
formalisms in isolation, STRuCT-LLM leverages shared abstractions between SQL
and Cypher to induce cross-formalism transfer, enabling SQL training to improve
Cypher performance and vice versa - even without shared schemas. Our largest
model (QwQ-32B) achieves substantial relative improvements across tasks: on
semantic parsing, Spider improves by 13.5\% and Text2Cypher by 73.1\%. The
model also demonstrates strong zero-shot generalization, improving performance
on downstream tabular QA (TableBench: 8.5\%) and knowledge graph QA
(CR-LT-KGQA: 1.7\%) without any QA-specific supervision. These results
demonstrate both the effectiveness of executable queries as scaffolds for
structured reasoning and the synergistic benefits of jointly training on SQL
and Cypher (code available at https://github.com/bouv/STRuCT-LLM).

</details>


### [23] [Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition via Soft Prompt Tuning](https://arxiv.org/abs/2506.21576)
*Hongli Yang,Yizhou Peng,Hao Huang,Sheng Li*

Main category: cs.CL

TL;DR: 本文研究了 Soft Prompt Tuning (SPT)，一种参数高效的方法，以增强 CS ASR，同时保留先前的知识。


<details>
  <summary>Details</summary>
Motivation: 像 Whisper 这样的大规模多语言 ASR 模型擅长高资源环境，但在低资源场景（如稀有语言和代码转换 (CS)）中面临挑战，这是由于计算成本和灾难性遗忘。

Method: 探索 Soft Prompt Tuning (SPT)

Result: 深度 prompt tuning 是最有效的 SPT 方法

Conclusion: SPT4ASR 方法在 CS ASR 中实现了进一步的错误减少，同时保持了与 LoRA 相似的参数效率，而不会降低现有语言的性能。

Abstract: Large-scale multilingual ASR models like Whisper excel in high-resource
settings but face challenges in low-resource scenarios, such as rare languages
and code-switching (CS), due to computational costs and catastrophic
forgetting. We explore Soft Prompt Tuning (SPT), a parameter-efficient method
to enhance CS ASR while preserving prior knowledge. We evaluate two strategies:
(1) full fine-tuning (FFT) of both soft prompts and the entire Whisper model,
demonstrating improved cross-lingual capabilities compared to traditional
methods, and (2) adhering to SPT's original design by freezing model parameters
and only training soft prompts. Additionally, we introduce SPT4ASR, a
combination of different SPT variants. Experiments on the SEAME and ASRU2019
datasets show that deep prompt tuning is the most effective SPT approach, and
our SPT4ASR methods achieve further error reductions in CS ASR, maintaining
parameter efficiency similar to LoRA, without degrading performance on existing
languages.

</details>


### [24] [Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR](https://arxiv.org/abs/2506.21577)
*Hongli Yang,Sheng Li,Hao Huang,Ayiduosi Tuohan,Yizhou Peng*

Main category: cs.CL

TL;DR: This paper introduces Entire SPT and LAPT to improve multilingual ASR, achieving better performance in language expansion tasks with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Challenges such as language interference and expanding to unseen languages (language expansion) without degrading performance persist in multilingual ASR.

Method: Entire Soft Prompt Tuning (Entire SPT), Language-Aware Prompt Tuning (LAPT), SPT-Whisper toolkit

Result: Entire SPT and LAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks.

Conclusion: Entire SPT and LAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks, respectively, providing an efficient solution for dynamic, multilingual ASR models with minimal computational overhead.

Abstract: Recent advancements in multilingual automatic speech recognition (ASR) have
been driven by large-scale end-to-end models like Whisper. However, challenges
such as language interference and expanding to unseen languages (language
expansion) without degrading performance persist. This paper addresses these
with three contributions: 1) Entire Soft Prompt Tuning (Entire SPT), which
applies soft prompts to both the encoder and decoder, enhancing feature
extraction and decoding; 2) Language-Aware Prompt Tuning (LAPT), which
leverages cross-lingual similarities to encode shared and language-specific
features using lightweight prompt matrices; 3) SPT-Whisper, a toolkit that
integrates SPT into Whisper and enables efficient continual learning.
Experiments across three languages from FLEURS demonstrate that Entire SPT and
LAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks,
respectively, providing an efficient solution for dynamic, multilingual ASR
models with minimal computational overhead.

</details>


### [25] [Data Efficacy for Language Model Training](https://arxiv.org/abs/2506.21545)
*Yalun Dai,Yangyu Huang,Xin Zhang,Wenshan Wu,Chong Li,Wenhui Lu,Shijie Cao,Li Dong,Scarlett Li*

Main category: cs.CL

TL;DR: This paper introduces the concept of Data Efficacy, which focuses on maximizing performance by optimizing the organization of training data. They propose a new paradigm called DELT and demonstrate its effectiveness in improving language model training.


<details>
  <summary>Details</summary>
Motivation: Recent research has been dedicated to data efficiency, which aims to maximize performance by selecting a minimal or optimal subset of training data. To complement it, we define Data Efficacy, which focuses on maximizing performance by optimizing the organization of training data and remains relatively underexplored.

Method: A general paradigm, DELT, for considering data efficacy in LM training, which highlights the significance of training data organization. DELT comprises three components: Data Scoring, Data Selection, and Data Ordering. Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which considers both the learnability and quality of each data sample from the gradient consistency perspective. Folding Ordering (FO), as a novel instance of Data Ordering, which addresses issues such as model forgetting and data distribution bias.

Result: various instances of the proposed DELT enhance LM performance to varying degrees without increasing the data scale and model size. the combination of our proposed LQS for data scoring and Folding for data ordering achieves the most significant improvement. data efficacy can be achieved together with data efficiency by applying data selection.

Conclusion: Data efficacy is a promising foundational area in LM training.

Abstract: Data is fundamental to the training of language models (LM). Recent research
has been dedicated to data efficiency, which aims to maximize performance by
selecting a minimal or optimal subset of training data. Techniques such as data
filtering, sampling, and selection play a crucial role in this area. To
complement it, we define Data Efficacy, which focuses on maximizing performance
by optimizing the organization of training data and remains relatively
underexplored. This work introduces a general paradigm, DELT, for considering
data efficacy in LM training, which highlights the significance of training
data organization. DELT comprises three components: Data Scoring, Data
Selection, and Data Ordering. Among these components, we design
Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which
considers both the learnability and quality of each data sample from the
gradient consistency perspective. We also devise Folding Ordering (FO), as a
novel instance of Data Ordering, which addresses issues such as model
forgetting and data distribution bias. Comprehensive experiments validate the
data efficacy in LM training, which demonstrates the following: Firstly,
various instances of the proposed DELT enhance LM performance to varying
degrees without increasing the data scale and model size. Secondly, among these
instances, the combination of our proposed LQS for data scoring and Folding for
data ordering achieves the most significant improvement. Lastly, data efficacy
can be achieved together with data efficiency by applying data selection.
Therefore, we believe that data efficacy is a promising foundational area in LM
training.

</details>


### [26] [Evaluation of LLM-based Strategies for the Extraction of Food Product Information from Online Shops](https://arxiv.org/abs/2506.21585)
*Christoph Brosch,Sian Brumm,Rolf Krieger,Jonas Scheffler*

Main category: cs.CL

TL;DR: 本文探讨了使用 LLM 从食品产品页面提取结构化信息的方法，发现间接提取在略微降低准确性的情况下，显著提高了效率并降低了成本。


<details>
  <summary>Details</summary>
Motivation: 生成人工智能和大型语言模型 (LLM) 为自动化从网页中提取结构化信息提供了巨大的潜力。在这项工作中，我们专注于在线零售商的食品产品页面，并探索模式约束的提取方法来检索关键产品属性，例如成分列表和营养表。

Method: 比较了两种基于 LLM 的方法：直接提取和通过生成函数进行的间接提取，并在包含来自三个不同在线商店的 3,000 个食品产品页面的数据集上，从准确性、效率和成本方面对它们进行了评估。

Result: 虽然间接方法实现了略低的准确率（96.48%，相比直接提取低 -1.61%），但它将所需的 LLM 调用次数减少了 95.82%，从而显着提高了效率并降低了运营成本。

Conclusion: 间接提取方法可以为使用 LLM 从基于模板的网页中进行大规模信息提取任务提供可扩展且经济高效的解决方案。

Abstract: Generative AI and large language models (LLMs) offer significant potential
for automating the extraction of structured information from web pages. In this
work, we focus on food product pages from online retailers and explore
schema-constrained extraction approaches to retrieve key product attributes,
such as ingredient lists and nutrition tables. We compare two LLM-based
approaches, direct extraction and indirect extraction via generated functions,
evaluating them in terms of accuracy, efficiency, and cost on a curated dataset
of 3,000 food product pages from three different online shops. Our results show
that although the indirect approach achieves slightly lower accuracy (96.48\%,
$-1.61\%$ compared to direct extraction), it reduces the number of required LLM
calls by 95.82\%, leading to substantial efficiency gains and lower operational
costs. These findings suggest that indirect extraction approaches can provide
scalable and cost-effective solutions for large-scale information extraction
tasks from template-based web pages using LLMs.

</details>


### [27] [HealthQA-BR: A System-Wide Benchmark Reveals Critical Knowledge Gaps in Large Language Models](https://arxiv.org/abs/2506.21578)
*Andrew Maranhão Ventura D'addario*

Main category: cs.CL

TL;DR: HealthQA-BR是一个新的葡萄牙语医疗保健基准，揭示了大型语言模型在不同医疗专业中知识掌握程度的巨大差异，强调了当前评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 目前对医疗保健领域大型语言模型（LLM）的评估主要集中在以医生为中心的英语基准上，忽略了患者护理的跨专业性质，从而造成了一种危险的能力错觉。

Method: 引入HealthQA-BR，这是一个针对葡萄牙语医疗保健的大规模、系统范围的基准，包含来自巴西国家许可和住院医师考试的5,632个问题。

Result: 结果表明，尽管GPT 4.1等先进模型取得了很高的总体准确率（86.6%），但这一表面分数掩盖了令人震惊的、先前未测量的缺陷。诸如眼科（98.7%）等专业的表现接近完美，而神经外科（60.0%）和社会工作（68.4%）的表现勉强及格。所有模型都存在这种“参差不齐”的知识结构问题，表明高水平分数不足以进行安全验证。

Conclusion: 公开HealthQA-BR和评估套件，以促进对AI在整个医疗团队中准备情况的细致评估。

Abstract: The evaluation of Large Language Models (LLMs) in healthcare has been
dominated by physician-centric, English-language benchmarks, creating a
dangerous illusion of competence that ignores the interprofessional nature of
patient care. To provide a more holistic and realistic assessment, we introduce
HealthQA-BR, the first large-scale, system-wide benchmark for
Portuguese-speaking healthcare. Comprising 5,632 questions from Brazil's
national licensing and residency exams, it uniquely assesses knowledge not only
in medicine and its specialties but also in nursing, dentistry, psychology,
social work, and other allied health professions. We conducted a rigorous
zero-shot evaluation of over 20 leading LLMs. Our results reveal that while
state-of-the-art models like GPT 4.1 achieve high overall accuracy (86.6%),
this top-line score masks alarming, previously unmeasured deficiencies. A
granular analysis shows performance plummets from near-perfect in specialties
like Ophthalmology (98.7%) to barely passing in Neurosurgery (60.0%) and, most
notably, Social Work (68.4%). This "spiky" knowledge profile is a systemic
issue observed across all models, demonstrating that high-level scores are
insufficient for safety validation. By publicly releasing HealthQA-BR and our
evaluation suite, we provide a crucial tool to move beyond single-score
evaluations and toward a more honest, granular audit of AI readiness for the
entire healthcare team.

</details>


### [28] [Evaluating Multimodal Large Language Models on Educational Textbook Question Answering](https://arxiv.org/abs/2506.21596)
*Hessa A. Alawwad,Anas Zafar,Areej Alhothali,Usman Naseem,Ali Alkhathlan,Amani Jamal*

Main category: cs.CL

TL;DR: 本文评估了 MLLM 在教科书问答任务中的表现，并提出了一个多模态 RAG 流程来提高准确性。结果表明，检索到的教育背景会影响模型准确性，但模型在处理问题-上下文关系方面仍存在局限性。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型 (MLLM) 最近在视觉语言任务中取得了显著成功。然而，它们对复杂的、长篇的课程和无法表示为单个自然图像的复杂教育图进行推理的能力在很大程度上未经测试。

Method: 引入了一个轻量级多模态检索增强生成 (RAG) 流程，该流程将课程中的段落和图表集成到提示中。

Result: 结果表明，检索到的教育背景对模型准确性和推理的影响，同时也揭示了当前在处理问题-上下文关系方面的局限性以及噪声的可能性。

Conclusion: 评估了最先进的 MLLM 在教科书问题回答 (TQA) 任务中的表现，揭示了模型在处理问题-上下文关系方面的局限性以及噪声的可能性，指出了多模态人工智能驱动学习未来研究的关键方向。

Abstract: Multimodal large language models (MLLMs) have recently achieved significant
success in vision--language tasks. However, their capacity to reason over
complex, long lessons and intricate educational diagrams that cannot be
represented as a single natural image remains largely untested. In this work,
we present the first evaluation of state-of-the-art MLLMs on the textbook
question answering (TQA) task using the CK12-QA dataset. We assess the
performance of recent vision-language models, including LLaVA and LLaMA
3.2-Vision, across various input configurations. Additionally, we introduce a
lightweight multimodal retrieval-augmented generation (RAG) pipeline that
integrates both paragraphs and diagrams from the lesson into the prompt. Our
results demonstrate the influence of retrieved educational context on model
accuracy and reasoning, while also revealing current limitations in handling
question-context relationships and the potential for noise, pointing to key
directions for future research in multimodal AI-driven learning.

</details>


### [29] [From General Reasoning to Domain Expertise: Uncovering the Limits of Generalization in Large Language Models](https://arxiv.org/abs/2506.21580)
*Dana Alsagheer,Yang Lu,Abdulrahman Kamal,Omar Kamal,Mohammad Kamal,Nada Mansour,Cosmo Yang Wu,Rambiba Karanjai,Sen Li,Weidong Shi*

Main category: cs.CL

TL;DR: This study explores the connection between general reasoning capabilities of LLMs and their performance in domain-specific reasoning tasks, motivated by the importance of reasoning in decision-making and the growing trend of training LLMs for general reasoning.


<details>
  <summary>Details</summary>
Motivation: Effective decision-making relies heavily on strong reasoning abilities. Reasoning is the foundation for decision-making, providing the analytical and logical framework to make sound choices. As AI technology evolves, there is a growing trend to train LLMs to excel in general reasoning.

Method: This study explores how the general reasoning capabilities of LLMs connect to their performance in domain-specific reasoning tasks.

Result: How the general reasoning capabilities of LLMs connect to their performance in domain-specific reasoning tasks.

Conclusion: The study explores how the general reasoning capabilities of LLMs connect to their performance in domain-specific reasoning tasks.

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
remarkable capabilities in various domains. However, effective decision-making
relies heavily on strong reasoning abilities. Reasoning is the foundation for
decision-making, providing the analytical and logical framework to make sound
choices. Reasoning involves analyzing information, drawing inferences, and
reaching conclusions based on logic or evidence. Decision-making builds on this
foundation by applying the insights from reasoning to select the best course of
action among alternatives. Together, these processes create a continuous cycle
of thought and action aimed at achieving goals effectively. As AI technology
evolves, there is a growing trend to train LLMs to excel in general reasoning.
This study explores how the general reasoning capabilities of LLMs connect to
their performance in domain-specific reasoning tasks.

</details>


### [30] [Overview of the ClinIQLink 2025 Shared Task on Medical Question-Answering](https://arxiv.org/abs/2506.21597)
*Brandon Colelough,Davis Bartels,Dina Demner-Fushman*

Main category: cs.CL

TL;DR: ClinIQLink is a new benchmark to test LLMs on medical question answering using a diverse dataset and both automated and expert evaluations.


<details>
  <summary>Details</summary>
Motivation: The paper introduces ClinIQLink, a shared task to evaluate the capabilities of large language models (LLMs) in medically-oriented question answering.

Method: The challenge uses 4,978 expert-verified question-answer pairs across seven formats. Participating systems are evaluated using exact match and embedding metrics, followed by physician review.

Result: The ClinIQLink challenge includes a dataset of 4,978 medical question-answer pairs, an automated evaluation harness, and a physician panel for auditing model responses.

Conclusion: ClinIQLink provides a challenging benchmark for LLMs in medical question answering, with a focus on general practitioner-level knowledge and diverse question formats. The evaluation includes automated scoring and expert review.

Abstract: In this paper, we present an overview of ClinIQLink, a shared task,
collocated with the 24th BioNLP workshop at ACL 2025, designed to stress-test
large language models (LLMs) on medically-oriented question answering aimed at
the level of a General Practitioner. The challenge supplies 4,978
expert-verified, medical source-grounded question-answer pairs that cover seven
formats: true/false, multiple choice, unordered list, short answer,
short-inverse, multi-hop, and multi-hop-inverse. Participating systems, bundled
in Docker or Apptainer images, are executed on the CodaBench platform or the
University of Maryland's Zaratan cluster. An automated harness (Task 1) scores
closed-ended items by exact match and open-ended items with a three-tier
embedding metric. A subsequent physician panel (Task 2) audits the top model
responses.

</details>


### [31] [VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents](https://arxiv.org/abs/2506.21582)
*Sam Yu-Te Lee,Chengyang Ji,Shicheng Wen,Lifu Huang,Dongyi Liu,Kwan-Liu Ma*

Main category: cs.CL

TL;DR: Introduces VIDEE, a system that helps entry-level data analysts perform text analytics with intelligent agents, using a three-stage human-agent collaboration workflow. Experiments and a user study validate its effectiveness and usability.


<details>
  <summary>Details</summary>
Motivation: Text analytics has traditionally required specialized knowledge in Natural Language Processing (NLP) or text analysis, which presents a barrier for entry-level analysts. Recent advances in large language models (LLMs) have changed the landscape of NLP by enabling more accessible and automated text analysis.

Method: VIDEE instantiates a human-agent collaraboration workflow consisting of three stages: (1) Decomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search algorithm to support generative reasoning with human feedback, (2) Execution, which generates an executable text analytics pipeline, and (3) Evaluation, which integrates LLM-based evaluation and visualizations to support user validation of execution results.

Result: We conduct two quantitative experiments to evaluate VIDEE's effectiveness and analyze common agent errors. A user study involving participants with varying levels of NLP and text analytics experience demonstrates the system's usability and reveals distinct user behavior patterns.

Conclusion: The findings identify design implications for human-agent collaboration, validate the practical utility of VIDEE for non-expert users, and inform future improvements to intelligent text analytics systems.

Abstract: Text analytics has traditionally required specialized knowledge in Natural
Language Processing (NLP) or text analysis, which presents a barrier for
entry-level analysts. Recent advances in large language models (LLMs) have
changed the landscape of NLP by enabling more accessible and automated text
analysis (e.g., topic detection, summarization, information extraction, etc.).
We introduce VIDEE, a system that supports entry-level data analysts to conduct
advanced text analytics with intelligent agents. VIDEE instantiates a
human-agent collaroration workflow consisting of three stages: (1)
Decomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search
algorithm to support generative reasoning with human feedback, (2) Execution,
which generates an executable text analytics pipeline, and (3) Evaluation,
which integrates LLM-based evaluation and visualizations to support user
validation of execution results. We conduct two quantitative experiments to
evaluate VIDEE's effectiveness and analyze common agent errors. A user study
involving participants with varying levels of NLP and text analytics experience
-- from none to expert -- demonstrates the system's usability and reveals
distinct user behavior patterns. The findings identify design implications for
human-agent collaboration, validate the practical utility of VIDEE for
non-expert users, and inform future improvements to intelligent text analytics
systems.

</details>


### [32] [Structured Attention Matters to Multimodal LLMs in Document Understanding](https://arxiv.org/abs/2506.21600)
*Chang Liu,Hongkai Chen,Yujun Cai,Hang Wu,Qingwen Ye,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.CL

TL;DR: 原始OCR文本会损害MLLM的文档理解性能。通过LaTex范式保持文档结构可以提高性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型(MLLM)的文档理解仍然是一个重大挑战。之前的研究主要集中于通过精确的多模态查询来定位证据页面，而我们的工作调查了一个基本但被忽视的方面：输入格式如何影响文档理解性能。

Method: 提出了一种新的结构保持方法，使用LaTex范式编码文档元素，保持了对理解至关重要的层次结构和空间关系。

Result: 结构化文本在文本和视觉内容上诱导结构化的注意模式，引导模型关注语义上有意义的区域，同时减少注意浪费。与原始OCR文本相比，结构化文本显著提高了MLLM的性能。

Conclusion: 通过引入保持结构的LaTex范式编码文档元素，显著提高了MLLM在各种文档类型上的文档问答性能，无需架构修改或额外训练。

Abstract: Document understanding remains a significant challenge for multimodal large
language models (MLLMs). While previous research has primarily focused on
locating evidence pages through precise multimodal queries, our work
investigates a fundamental yet overlooked aspect: how input format influences
document comprehension performance. Through systematic analysis, we discover
that raw OCR text often impairs rather than improves MLLMs' performance, which
is a counterintuitive finding we attribute to attention dispersion and
structure loss. To further substantiate our hypothesis, we propose a novel
structure-preserving approach that encodes document elements using the LaTex
paradigm, maintaining the hierarchical organization and spatial relationships
critical for comprehension. Our attention analysis reveals that structured text
induces structured attention patterns on both textual and visual content,
directing models to focus on semantically meaningful regions while reducing
attention waste. This approach significantly enhances MLLMs' document question
answering performance across diverse document types without requiring
architectural modifications or additional training.

</details>


### [33] [Hope Speech Detection in code-mixed Roman Urdu tweets: A Positive Turn in Natural Language Processing](https://arxiv.org/abs/2506.21583)
*Muhammad Ahmad,Muhammad Waqas,Ameer Hamza,Ildar Batyrshin,Grigori Sidorov*

Main category: cs.CL

TL;DR: This paper introduces a new dataset for hope speech detection in Roman Urdu and proposes a custom transformer model that outperforms baseline models.


<details>
  <summary>Details</summary>
Motivation: existing research mainly focuses on high-resource languages and standardized scripts, often overlooking informal and underrepresented forms such as Roman Urdu. To the best of our knowledge, this is the first study to address hope speech detection in code-mixed Roman Urdu by introducing a carefully annotated dataset, thereby filling a critical gap in inclusive NLP research for low-resource, informal language varieties.

Method: a custom attention-based transformer model optimized for the syntactic and semantic variability of Roman Urdu, evaluated using 5-fold cross-validation

Result: it introduces the first multi-class annotated dataset for Roman Urdu hope speech, comprising Generalized Hope, Realistic Hope, Unrealistic Hope, and Not Hope categories; (2) it explores the psychological foundations of hope and analyzes its linguistic patterns in code-mixed Roman Urdu to inform dataset development; (3) it proposes a custom attention-based transformer model optimized for the syntactic and semantic variability of Roman Urdu, evaluated using 5-fold cross-validation; and (4) it verifies the statistical significance of performance gains using a t-test

Conclusion: The proposed model, XLM-R, achieves the best performance with a cross-validation score of 0.78, outperforming the baseline SVM (0.75) and BiLSTM (0.76), with gains of 4% and 2.63% respectively.

Abstract: Hope is a positive emotional state involving the expectation of favorable
future outcomes, while hope speech refers to communication that promotes
optimism, resilience, and support, particularly in adverse contexts. Although
hope speech detection has gained attention in Natural Language Processing
(NLP), existing research mainly focuses on high-resource languages and
standardized scripts, often overlooking informal and underrepresented forms
such as Roman Urdu. To the best of our knowledge, this is the first study to
address hope speech detection in code-mixed Roman Urdu by introducing a
carefully annotated dataset, thereby filling a critical gap in inclusive NLP
research for low-resource, informal language varieties. This study makes four
key contributions: (1) it introduces the first multi-class annotated dataset
for Roman Urdu hope speech, comprising Generalized Hope, Realistic Hope,
Unrealistic Hope, and Not Hope categories; (2) it explores the psychological
foundations of hope and analyzes its linguistic patterns in code-mixed Roman
Urdu to inform dataset development; (3) it proposes a custom attention-based
transformer model optimized for the syntactic and semantic variability of Roman
Urdu, evaluated using 5-fold cross-validation; and (4) it verifies the
statistical significance of performance gains using a t-test. The proposed
model, XLM-R, achieves the best performance with a cross-validation score of
0.78, outperforming the baseline SVM (0.75) and BiLSTM (0.76), with gains of 4%
and 2.63% respectively.

</details>


### [34] [AdaptGOT: A Pre-trained Model for Adaptive Contextual POI Representation Learning](https://arxiv.org/abs/2506.21612)
*Xiaobin Ren,Xinyu Zhu,Kaiqi Zhao*

Main category: cs.CL

TL;DR: The paper introduces AdaptGOT, a novel POI embedding model addressing limitations in multi-context sampling, context exploration, versatility, and generalization. It uses adaptive representation learning and Geographical-Co-Occurrence-Text representation with attention mechanisms and a MoE-based architecture. Experiments show AdaptGOT outperforms existing methods on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Despite the success of task-specific, end-to-end models in POI embedding, several challenges remain. These include the need for more effective multi-context sampling strategies, insufficient exploration of multiple POI contexts, limited versatility, and inadequate generalization. To address these issues

Method: We propose the AdaptGOT model, which integrates both the (Adapt)ive representation learning technique and the Geographical-Co-Occurrence-Text (GOT) representation with a particular emphasis on Geographical location, Co-Occurrence and Textual information. The AdaptGOT model comprises three key components: (1) contextual neighborhood generation, which integrates advanced mixed sampling techniques such as KNN, density-based, importance-based, and category-aware strategies to capture complex contextual neighborhoods; (2) an advanced GOT representation enhanced by an attention mechanism, designed to derive high-quality, customized representations and efficiently capture complex interrelations between POIs; and (3) the MoE-based adaptive encoder-decoder architecture, which ensures topological consistency and enriches contextual representation by minimizing Jensen-Shannon divergence across varying contexts.

Result: superior performance of the proposed AdaptGOT model

Conclusion: Experiments on two real-world datasets and multiple POI tasks substantiate the superior performance of the proposed AdaptGOT model.

Abstract: Currently, considerable strides have been achieved in Point-of-Interest (POI)
embedding methodologies, driven by the emergence of novel POI tasks like
recommendation and classification. Despite the success of task-specific,
end-to-end models in POI embedding, several challenges remain. These include
the need for more effective multi-context sampling strategies, insufficient
exploration of multiple POI contexts, limited versatility, and inadequate
generalization. To address these issues, we propose the AdaptGOT model, which
integrates both the (Adapt)ive representation learning technique and the
Geographical-Co-Occurrence-Text (GOT) representation with a particular emphasis
on Geographical location, Co-Occurrence and Textual information. The AdaptGOT
model comprises three key components: (1) contextual neighborhood generation,
which integrates advanced mixed sampling techniques such as KNN, density-based,
importance-based, and category-aware strategies to capture complex contextual
neighborhoods; (2) an advanced GOT representation enhanced by an attention
mechanism, designed to derive high-quality, customized representations and
efficiently capture complex interrelations between POIs; and (3) the MoE-based
adaptive encoder-decoder architecture, which ensures topological consistency
and enriches contextual representation by minimizing Jensen-Shannon divergence
across varying contexts. Experiments on two real-world datasets and multiple
POI tasks substantiate the superior performance of the proposed AdaptGOT model.

</details>


### [35] [Empirical Evidence for Alignment Faking in Small LLMs and Prompt-Based Mitigation Techniques](https://arxiv.org/abs/2506.21584)
*J. Koorndijk*

Main category: cs.CL

TL;DR: Small language models can exhibit deceptive alignment, which can be reduced with prompt engineering, challenging the assumption that deception requires large models.


<details>
  <summary>Details</summary>
Motivation: Current literature suggests that alignment faking is an emergent property of large language models.

Method: The study uses empirical evidence from a small instruction-tuned model (LLaMA 3 8B) and prompt-only interventions like deontological moral framing and scratchpad reasoning.

Result: The study found that a small instruction-tuned model can exhibit alignment faking, and prompt-only interventions can reduce this behavior.

Conclusion: This paper refines the understanding of deception in language models and underscores the need for alignment evaluations across model sizes and deployment settings.

Abstract: Current literature suggests that alignment faking (deceptive alignment) is an
emergent property of large language models. We present the first empirical
evidence that a small instruction-tuned model, specifically LLaMA 3 8B, can
also exhibit alignment faking. We further show that prompt-only interventions,
including deontological moral framing and scratchpad reasoning, significantly
reduce this behavior without modifying model internals. This challenges the
assumption that prompt-based ethics are trivial and that deceptive alignment
requires scale. We introduce a taxonomy distinguishing shallow deception,
shaped by context and suppressible through prompting, from deep deception,
which reflects persistent, goal-driven misalignment. Our findings refine the
understanding of deception in language models and underscore the need for
alignment evaluations across model sizes and deployment settings.

</details>


### [36] [Refine Medical Diagnosis Using Generation Augmented Retrieval and Clinical Practice Guidelines](https://arxiv.org/abs/2506.21615)
*Wenhao Li,Hongkuan Zhang,Hongwei Zhang,Zhengxu Li,Zengjie Dong,Yafan Chen,Niranjan Bidargaddi,Hong Liu*

Main category: cs.CL

TL;DR: The paper introduces GARMLE-G, a framework that grounds medical language model outputs in clinical practice guidelines (CPGs) to enable hallucination-free and clinically aligned recommendations. A prototype system for hypertension diagnosis demonstrates superior performance compared to RAG-based baselines.


<details>
  <summary>Details</summary>
Motivation: Current medical language models typically predict ICD code-based diagnosis from electronic health records (EHRs) because these labels are readily available. However, ICD codes do not capture the nuanced, context-rich reasoning clinicians use for diagnosis. This misalignment limits the clinical utility of existing models.

Method: Generation-Augmented Retrieval framework that grounds medical language model outputs in authoritative CPGs. Unlike conventional Retrieval-Augmented Generation based approaches, GARMLE-G enables hallucination-free outputs by directly retrieving authoritative guideline content without relying on model-generated text. It (1) integrates LLM predictions with EHR data to create semantically rich queries, (2) retrieves relevant CPG knowledge snippets via embedding similarity, and (3) fuses guideline content with model output to generate clinically aligned recommendations.

Result: A prototype system for hypertension diagnosis was developed and evaluated on multiple metrics, demonstrating superior retrieval precision, semantic relevance, and clinical guideline adherence compared to RAG-based baselines, while maintaining a lightweight architecture suitable for localized healthcare deployment.

Conclusion: GARMLE-G provides a scalable, low-cost, and hallucination-free method for grounding medical language models in evidence-based clinical practice, with strong potential for broader clinical deployment.

Abstract: Current medical language models, adapted from large language models (LLMs),
typically predict ICD code-based diagnosis from electronic health records
(EHRs) because these labels are readily available. However, ICD codes do not
capture the nuanced, context-rich reasoning clinicians use for diagnosis.
Clinicians synthesize diverse patient data and reference clinical practice
guidelines (CPGs) to make evidence-based decisions. This misalignment limits
the clinical utility of existing models. We introduce GARMLE-G, a
Generation-Augmented Retrieval framework that grounds medical language model
outputs in authoritative CPGs. Unlike conventional Retrieval-Augmented
Generation based approaches, GARMLE-G enables hallucination-free outputs by
directly retrieving authoritative guideline content without relying on
model-generated text. It (1) integrates LLM predictions with EHR data to create
semantically rich queries, (2) retrieves relevant CPG knowledge snippets via
embedding similarity, and (3) fuses guideline content with model output to
generate clinically aligned recommendations. A prototype system for
hypertension diagnosis was developed and evaluated on multiple metrics,
demonstrating superior retrieval precision, semantic relevance, and clinical
guideline adherence compared to RAG-based baselines, while maintaining a
lightweight architecture suitable for localized healthcare deployment. This
work provides a scalable, low-cost, and hallucination-free method for grounding
medical language models in evidence-based clinical practice, with strong
potential for broader clinical deployment.

</details>


### [37] [Doc2SAR: A Synergistic Framework for High-Fidelity Extraction of Structure-Activity Relationships from Scientific Documents](https://arxiv.org/abs/2506.21625)
*Jiaxi Zhuang,Kangning Li,Jue Hou,Mingjun Xu,Zhifeng Gao,Hengxing Cai*

Main category: cs.CL

TL;DR: 提出了Doc2SAR，用于从科学文献中提取分子结构-活性关系，并在DocSAR-200基准测试中取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 从科学文献和专利中提取分子结构-活性关系（SAR）对于药物发现和材料研究至关重要。然而，由于异构文档格式和现有方法的限制，这项任务仍然具有挑战性。依赖于刚性模板的基于规则的方法无法在不同的文档布局中推广，而通用多模态大型语言模型（MLLM）对于诸如布局检测和光学化学结构识别（OCSR）之类的专门任务缺乏足够的准确性和可靠性。

Method: 提出了Doc2SAR，一个新颖的协同框架，该框架将领域特定的工具与通过监督微调（SFT）增强的MLLM集成。

Result: Doc2SAR在DocSAR-200上实现了80.78%的表格召回率，超过end2end GPT-4o 51.48%。

Conclusion: Doc2SAR在各种文档类型中实现了最先进的性能，显著优于领先的端到端基线。在DocSAR-200上达到了80.78%的表格召回率，超过end2end GPT-4o 51.48%。

Abstract: Extracting molecular structure-activity relationships (SARs) from scientific
literature and patents is essential for drug discovery and materials research.
However, this task remains challenging due to heterogeneous document formats
and limitations of existing methods. Specifically, rule-based approaches
relying on rigid templates fail to generalize across diverse document layouts,
while general-purpose multimodal large language models (MLLMs) lack sufficient
accuracy and reliability for specialized tasks, such as layout detection and
optical chemical structure recognition (OCSR). To address these challenges, we
introduce DocSAR-200, a rigorously annotated benchmark of 200 scientific
documents designed specifically for evaluating SAR extraction methods.
Additionally, we propose Doc2SAR, a novel synergistic framework that integrates
domain-specific tools with MLLMs enhanced via supervised fine-tuning (SFT).
Extensive experiments demonstrate that Doc2SAR achieves state-of-the-art
performance across various document types, significantly outperforming leading
end-to-end baselines. Specifically, Doc2SAR attains an overall Table Recall of
80.78% on DocSAR-200, exceeding end2end GPT-4o by 51.48%. Furthermore, Doc2SAR
demonstrates practical usability through efficient inference and is accompanied
by a web app.

</details>


### [38] [Can Vision Language Models Understand Mimed Actions?](https://arxiv.org/abs/2506.21586)
*Hyundong Cho,Spencer Lin,Tejas Srinivasan,Michael Saxon,Deuksin Kwon,Natali T. Chavez,Jonathan May*

Main category: cs.CL

TL;DR: MIME: a novel video-based question answering benchmark comprising of 86 mimed actions, constructed with motion capture data, variations of each action with perturbations applied to the character, background, and viewpoint for evaluating recognition robustness. both open-weight and API-based vision-language models perform significantly worse than humans on MIME, motivating the need for increased research for instilling more robust understanding of human gestures


<details>
  <summary>Details</summary>
Motivation: a solid understanding of mimed actions is a crucial prerequisite for vision-language models capable of interpreting and commanding more subtle aspects of NVC

Method: a novel video-based question answering benchmark comprising of 86 mimed actions. Constructed with motion capture data, MIME consists of variations of each action with perturbations applied to the character, background, and viewpoint for evaluating recognition robustness

Result: both open-weight and API-based vision-language models perform significantly worse than humans on MIME

Conclusion: vision-language models perform significantly worse than humans on MIME, motivating the need for increased research for instilling more robust understanding of human gestures

Abstract: Nonverbal communication (NVC) plays an integral role in human language, but
studying NVC in general is challenging because of its broad scope and high
variance in interpretation among individuals and cultures. However, mime -- the
theatrical technique of suggesting intent using only gesture, expression, and
movement -- is a subset of NVC that consists of explicit and embodied actions
with much lower human interpretation variance. We argue that a solid
understanding of mimed actions is a crucial prerequisite for vision-language
models capable of interpreting and commanding more subtle aspects of NVC.
Hence, we propose Mime Identification Multimodal Evaluation (MIME), a novel
video-based question answering benchmark comprising of 86 mimed actions.
Constructed with motion capture data, MIME consists of variations of each
action with perturbations applied to the character, background, and viewpoint
for evaluating recognition robustness. We find that both open-weight and
API-based vision-language models perform significantly worse than humans on
MIME, motivating the need for increased research for instilling more robust
understanding of human gestures.

</details>


### [39] [DAPFAM: A Domain-Aware Patent Retrieval Dataset Aggregated at the Family Level](https://arxiv.org/abs/2506.22141)
*Iliass Ayaou,Denis Cavallucci,Hicham Chibane*

Main category: cs.CL

TL;DR: DAPFAM: A new, manageable, multi-jurisdictional, domain-aware patent retrieval dataset with explicit in-domain/out-of-domain labels, designed to support sub-document level experiments and address limitations in existing datasets. It highlights challenges in cross-domain patent retrieval.


<details>
  <summary>Details</summary>
Motivation: Existing publicly available patent retrieval datasets often lack explicit in-domain and out-of-domain labeling, multi-jurisdiction coverage, balanced query domain representation, and manageable sizes for sub-document level experiments.

Method: The dataset is constructed at the simple-family level using a three-step data-curation pipeline and includes a novel labeling scheme based on International Patent Classification (IPC) codes to determine in-domain and out-of-domain relationships.

Result: DAPFAM contains 1,247 domain-balanced full-text query families and 45,336 full-text target families, enriched with relevance judgments and in-domain/out-of-domain relationships, resulting in 49,869 evaluation pairs. The dataset is designed to require minimal preprocessing and be manageable for entities with limited resources.

Conclusion: This paper introduces DAPFAM, a new open-access, domain-aware, multi-jurisdictional patent retrieval dataset designed to address the limitations of existing datasets. Baseline experiments using lexical and neural retrieval methods highlight significant challenges in cross-domain patent retrieval.

Abstract: In the landscape of publicly available patent retrieval datasets, the need
for explicit indomain and out-of-domain labeling, multi-jurisdiction coverage,
balanced query domain representation and manageable sizes that support sub
document level experiments on moderate computational resources is often
overlooked. To address these gaps, we propose DAPFAM, a new open access
domain-aware patent retrieval dataset constructed at the simple-family level.
The dataset contains 1,247 domain balanced full text query families and 45,336
full text target families. The dataset is enriched by clear relevance judgments
(forward/backward citations as positive links, random negatives), as well as
explicit in-domain or out-of-domain relationships via a novel proposed
labelling scheme based on via International Patent Classification (IPC) codes,
resulting in 49,869 evaluation pairs. The dataset is multi jurisdictional,
requires little to no preprocessing for retrieval evaluation, and remains of a
size manageable for entities with limited ressources allowing for sub document
level retrieval experiments without excessive computational costs. We describe
our three-step data-curation pipeline, present comprehensive dataset
statistics, and provide baseline experiments using lexical and neural retrieval
methods. Our baseline experiments highlight significant challenges in
crossdomain patent retrieval. The dataset will be publicly available (for now
the access link is this repository:
https://osf.io/vbyzd/?view_only=1a40242e0d1941a58aa854af3e50cf6b).

</details>


### [40] [Is DeepSeek a New Voice Among LLMs in Public Opinion Simulation?](https://arxiv.org/abs/2506.21587)
*Weihong Qi,Fan Huang,Jisun An,Haewoon Kwak*

Main category: cs.CL

TL;DR: 本研究评估了DeepSeek模拟公众舆论的能力，发现其在特定问题和人群中表现出色，但存在文化和人口偏见。


<details>
  <summary>Details</summary>
Motivation: 评估开源大型语言模型（LLM）DeepSeek与主要科技公司开发的LLM相比，模拟公众舆论的能力。

Method: 比较DeepSeek-R1和DeepSeek-V3与Qwen2.5、GPT-4o和Llama-3.3，并利用来自美国国家选举研究（ANES）和中国Zuobiao数据集的调查数据，评估这些模型预测中美两国社会问题舆论的能力。

Result: DeepSeek-V3在模拟美国关于堕胎问题的观点方面表现最佳，在模拟中国关于对外援助和个人主义的观点方面表现最佳，但在模拟关于资本主义的观点方面表现出局限性，尤其未能捕捉到低收入和非大学学历个人的立场。

Conclusion: 所有LLM都表现出过度概括人口群体中单一视角的倾向，通常默认为群体内一致的反应。需要在LLM驱动的舆论建模中缓解文化和人口偏见，呼吁采用更具包容性的训练方法。

Abstract: This study evaluates the ability of DeepSeek, an open-source large language
model (LLM), to simulate public opinions in comparison to LLMs developed by
major tech companies. By comparing DeepSeek-R1 and DeepSeek-V3 with Qwen2.5,
GPT-4o, and Llama-3.3 and utilizing survey data from the American National
Election Studies (ANES) and the Zuobiao dataset of China, we assess these
models' capacity to predict public opinions on social issues in both China and
the United States, highlighting their comparative capabilities between
countries. Our findings indicate that DeepSeek-V3 performs best in simulating
U.S. opinions on the abortion issue compared to other topics such as climate
change, gun control, immigration, and services for same-sex couples, primarily
because it more accurately simulates responses when provided with Democratic or
liberal personas. For Chinese samples, DeepSeek-V3 performs best in simulating
opinions on foreign aid and individualism but shows limitations in modeling
views on capitalism, particularly failing to capture the stances of low-income
and non-college-educated individuals. It does not exhibit significant
differences from other models in simulating opinions on traditionalism and the
free market. Further analysis reveals that all LLMs exhibit the tendency to
overgeneralize a single perspective within demographic groups, often defaulting
to consistent responses within groups. These findings highlight the need to
mitigate cultural and demographic biases in LLM-driven public opinion modeling,
calling for approaches such as more inclusive training methodologies.

</details>


### [41] [Understanding Verbatim Memorization in LLMs Through Circuit Discovery](https://arxiv.org/abs/2506.21588)
*Ilya Lasy,Peter Knees,Stefan Woltran*

Main category: cs.CL

TL;DR: 本文通过transformer回路，从机制可解释性的角度研究了大型语言模型（LLM）中记忆化的根本机制，发现启动记忆化的回路也可以在启动后保持记忆化，而仅保持记忆化的回路无法触发其启动。记忆化预防机制在不同的文本领域中可以稳健地转移，而记忆化诱导似乎更依赖于上下文。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）中记忆化的根本机制（即逐字复制训练数据）仍然知之甚少。网络中的哪个部分决定检索一个我们认为是记忆序列开始的token？模型在生成记忆句子与非记忆句子时的行为有何不同？

Method: 通过利用transformer回路，从机制可解释性的角度来研究这些问题。通过精心构建的对比数据集，我们识别出模型生成与记忆内容不同的点，并分离出负责记忆化两个不同方面的特定回路。

Result: 发现启动记忆化的回路也可以在启动后保持记忆化，而仅保持记忆化的回路无法触发其启动。记忆化预防机制在不同的文本领域中可以稳健地转移，而记忆化诱导似乎更依赖于上下文。

Conclusion: 发现启动记忆化的回路也可以在启动后保持记忆化，而仅保持记忆化的回路无法触发其启动。记忆化预防机制在不同的文本领域中可以稳健地转移，而记忆化诱导似乎更依赖于上下文。

Abstract: Underlying mechanisms of memorization in LLMs -- the verbatim reproduction of
training data -- remain poorly understood. What exact part of the network
decides to retrieve a token that we would consider as start of memorization
sequence? How exactly is the models' behaviour different when producing
memorized sentence vs non-memorized? In this work we approach these questions
from mechanistic interpretability standpoint by utilizing transformer circuits
-- the minimal computational subgraphs that perform specific functions within
the model. Through carefully constructed contrastive datasets, we identify
points where model generation diverges from memorized content and isolate the
specific circuits responsible for two distinct aspects of memorization. We find
that circuits that initiate memorization can also maintain it once started,
while circuits that only maintain memorization cannot trigger its initiation.
Intriguingly, memorization prevention mechanisms transfer robustly across
different text domains, while memorization induction appears more
context-dependent.

</details>


### [42] [A General Method for Detecting Information Generated by Large Language Models](https://arxiv.org/abs/2506.21589)
*Minjia Mao,Dongjun Wei,Xiao Fang,Michael Chau*

Main category: cs.CL

TL;DR: This paper introduces a general LLM detector (GLD) that can detect LLM-generated content across unseen LLMs and domains, which outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Detecting LLM-generated information is essential for preserving trust on digital platforms and preventing the spread of misinformation. Current detection methods face challenges in generalizing to new LLMs and domains.

Method: a general LLM detector (GLD) that combines a twin memory networks design and a theory-guided detection generalization module

Result: demonstrate the superiority of GLD over state-of-the-art detection methods using real-world datasets

Conclusion: We introduce a general LLM detector (GLD) that combines a twin memory networks design and a theory-guided detection generalization module to detect LLM-generated information across unseen LLMs and domains. The study has important academic and practical implications for digital platforms and LLMs.

Abstract: The proliferation of large language models (LLMs) has significantly
transformed the digital information landscape, making it increasingly
challenging to distinguish between human-written and LLM-generated content.
Detecting LLM-generated information is essential for preserving trust on
digital platforms (e.g., social media and e-commerce sites) and preventing the
spread of misinformation, a topic that has garnered significant attention in IS
research. However, current detection methods, which primarily focus on
identifying content generated by specific LLMs in known domains, face
challenges in generalizing to new (i.e., unseen) LLMs and domains. This
limitation reduces their effectiveness in real-world applications, where the
number of LLMs is rapidly multiplying and content spans a vast array of
domains. In response, we introduce a general LLM detector (GLD) that combines a
twin memory networks design and a theory-guided detection generalization module
to detect LLM-generated information across unseen LLMs and domains. Using
real-world datasets, we conduct extensive empirical evaluations and case
studies to demonstrate the superiority of GLD over state-of-the-art detection
methods. The study has important academic and practical implications for
digital platforms and LLMs.

</details>


### [43] [Representation Consistency for Accurate and Coherent LLM Answer Aggregation](https://arxiv.org/abs/2506.21590)
*Junqi Jiang,Tom Bewley,Salim I. Amoukou,Francesco Leofante,Antonio Rago,Saumitra Mishra,Francesca Toni*

Main category: cs.CL

TL;DR: 提出了一种新的测试时缩放方法（RC），该方法通过考虑模型内部激活的一致性来聚合LLM的多个候选响应，从而提高任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时缩放方法通常需要对提示和抽样策略进行复杂的修改。为了解决这个问题，本文提出了一种新的方法。

Method: 提出了一种名为表征一致性（RC）的测试时缩放方法，用于聚合来自LLM的多个候选响应的答案，该方法考虑了模型在生成响应集时内部激活的一致性，通过缓存激活和轻量相似度计算实现，无需额外模型查询。

Result: RC方法在四个开源LLM和四个推理数据集上进行了验证，结果表明该方法能够提高任务性能，且稀疏激活信号的一致性与连贯推理的概念相符。

Conclusion: 通过实验验证，RC方法在推理过程中能有效提高任务性能，相比于其他方法，准确率持续提升高达4%。稀疏激活信号的一致性与连贯推理的概念相符。

Abstract: Test-time scaling improves large language models' (LLMs) performance by
allocating more compute budget during inference. To achieve this, existing
methods often require intricate modifications to prompting and sampling
strategies. In this work, we introduce representation consistency (RC), a
test-time scaling method for aggregating answers drawn from multiple candidate
responses of an LLM regardless of how they were generated, including variations
in prompt phrasing and sampling strategy. RC enhances answer aggregation by not
only considering the number of occurrences of each answer in the candidate
response set, but also the consistency of the model's internal activations
while generating the set of responses leading to each answer. These activations
can be either dense (raw model activations) or sparse (encoded via pretrained
sparse autoencoders). Our rationale is that if the model's representations of
multiple responses converging on the same answer are highly variable, this
answer is more likely to be the result of incoherent reasoning and should be
down-weighted during aggregation. Importantly, our method only uses cached
activations and lightweight similarity computations and requires no additional
model queries. Through experiments with four open-source LLMs and four
reasoning datasets, we validate the effectiveness of RC for improving task
performance during inference, with consistent accuracy improvements (up to 4%)
over strong test-time scaling baselines. We also show that consistency in the
sparse activation signals aligns well with the common notion of coherent
reasoning.

</details>


### [44] [FinEval-KR: A Financial Domain Evaluation Framework for Large Language Models' Knowledge and Reasoning](https://arxiv.org/abs/2506.21591)
*Shaoyu Dou,Yutian Shen,Mofan Chen,Zixuan Wang,Jiajie Xu,Qi Guo,Kailai Shao,Chao Chen,Haixiang Hu,Haibo Shi,Min Min,Liwen Zhang*

Main category: cs.CL

TL;DR: 提出了FinEval-KR框架来评估LLM在金融领域的知识和推理能力。实验表明，LLM的推理和认知能力是关键，顶级模型在知识应用上仍有瓶颈，专业金融LLM表现不如通用LLM。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)显示出巨大的潜力，但在需要领域知识和复杂推理的复杂金融推理任务中面临挑战。目前的评估基准通常不足，因为它们没有将这些能力指标与单一任务性能分离，并且缺乏任务失败的根本原因分析。

Method: 提出了FinEval-KR评估框架，用于解耦和量化LLM的知识和推理能力，并提出了不同的知识评分和推理评分指标。受到了认知科学的启发，提出了基于布鲁姆分类法的认知评分，以分析不同认知水平的推理任务能力。

Result: LLM的推理能力和更高阶的认知能力是影响推理准确性的核心因素。即使是顶级模型在知识应用方面仍然面临瓶颈。专业金融LLM在多个指标上通常落后于顶级通用大模型。

Conclusion: LLM的推理能力和更高阶的认知能力是影响推理准确性的核心因素。即使是顶级模型在知识应用方面仍然面临瓶颈。专业金融LLM在多个指标上通常落后于顶级通用大模型。

Abstract: Large Language Models (LLMs) demonstrate significant potential but face
challenges in complex financial reasoning tasks requiring both domain knowledge
and sophisticated reasoning. Current evaluation benchmarks often fall short by
not decoupling these capabilities indicators from single task performance and
lack root cause analysis for task failure. To address this, we introduce
FinEval-KR, a novel evaluation framework for decoupling and quantifying LLMs'
knowledge and reasoning abilities independently, proposing distinct knowledge
score and reasoning score metrics. Inspired by cognitive science, we further
propose a cognitive score based on Bloom's taxonomy to analyze capabilities in
reasoning tasks across different cognitive levels. We also release a new
open-source Chinese financial reasoning dataset covering 22 subfields to
support reproducible research and further advancements in financial reasoning.
Our experimental results reveal that LLM reasoning ability and higher-order
cognitive ability are the core factors influencing reasoning accuracy. We also
specifically find that even top models still face a bottleneck with knowledge
application. Furthermore, our analysis shows that specialized financial LLMs
generally lag behind the top general large models across multiple metrics.

</details>


### [45] [SignBart -- New approach with the skeleton sequence for Isolated Sign language Recognition](https://arxiv.org/abs/2506.21592)
*Tinh Nguyen,Minh Khue Phan Tran*

Main category: cs.CL

TL;DR: A new sign language recognition (SLR) approach based on BART architecture achieves high accuracy with fewer parameters by independently encoding x and y coordinates of skeleton sequences and using cross-attention.


<details>
  <summary>Details</summary>
Motivation: Sign language recognition is crucial for individuals with hearing impairments to break communication barriers. However, previous approaches have had to choose between efficiency and accuracy. Such as RNNs, LSTMs, and GCNs, had problems with vanishing gradients and high computational costs. Despite improving performance, transformer-based methods were not commonly used.

Method: Utilizing an encoder-decoder of BART architecture, the model independently encodes the x and y coordinates, while Cross-Attention ensures their interrelation is maintained.

Result: With only 749,888 parameters, the model achieves 96.04% accuracy on the LSA-64 dataset, significantly outperforming previous models with over one million parameters. The model also demonstrates excellent performance and generalization across WLASL and ASL-Citizen datasets. Ablation studies underscore the importance of coordinate projection, normalization, and using multiple skeleton components for boosting model efficacy.

Conclusion: This study offers a reliable and effective approach for sign language recognition, with strong potential for enhancing accessibility tools for the deaf and hard of hearing.

Abstract: Sign language recognition is crucial for individuals with hearing impairments
to break communication barriers. However, previous approaches have had to
choose between efficiency and accuracy. Such as RNNs, LSTMs, and GCNs, had
problems with vanishing gradients and high computational costs. Despite
improving performance, transformer-based methods were not commonly used. This
study presents a new novel SLR approach that overcomes the challenge of
independently extracting meaningful information from the x and y coordinates of
skeleton sequences, which traditional models often treat as inseparable. By
utilizing an encoder-decoder of BART architecture, the model independently
encodes the x and y coordinates, while Cross-Attention ensures their
interrelation is maintained. With only 749,888 parameters, the model achieves
96.04% accuracy on the LSA-64 dataset, significantly outperforming previous
models with over one million parameters. The model also demonstrates excellent
performance and generalization across WLASL and ASL-Citizen datasets. Ablation
studies underscore the importance of coordinate projection, normalization, and
using multiple skeleton components for boosting model efficacy. This study
offers a reliable and effective approach for sign language recognition, with
strong potential for enhancing accessibility tools for the deaf and hard of
hearing.

</details>


### [46] [Gazal-R1: Achieving State-of-the-Art Medical Reasoning with Parameter-Efficient Two-Stage Training](https://arxiv.org/abs/2506.21594)
*Ahmed M. Adly,Mostafa Samy,Amr Fawzy*

Main category: cs.CL

TL;DR: Gazal-R1, a 32B model, outperforms larger models in medical reasoning using a novel two-stage training pipeline and GRPO, achieving state-of-the-art results on medical benchmarks.


<details>
  <summary>Details</summary>
Motivation: demonstrates that strategic training can enable mid-sized models to outperform significantly larger counterparts in specialized domains

Method: a novel two-stage training pipeline: first, supervised fine-tuning on a carefully curated dataset; second, reinforcement learning using Group Relative Policy Optimization (GRPO) with a sophisticated multi-component reward system

Result: achieves state-of-the-art performance in medical reasoning while providing transparent, step-by-step explanations for clinical decision-making. Gazal-R1 achieves exceptional performance across medical benchmarks, scoring 87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA

Conclusion: Gazal-R1 achieves exceptional performance across medical benchmarks, surpassing models up to 12x larger, and offers a reproducible framework for developing high-capability, domain-specific language models.

Abstract: We present Gazal-R1, a 32-billion-parameter language model that achieves
state-of-the-art performance in medical reasoning while providing transparent,
step-by-step explanations for clinical decision-making. Built upon Qwen3 32B,
our model demonstrates that strategic training can enable mid-sized models to
outperform significantly larger counterparts in specialized domains. We
developed a novel two-stage training pipeline: first, supervised fine-tuning on
a carefully curated dataset of 107,033 synthetic medical reasoning examples
that teaches structured clinical thinking, enhanced by advanced
parameter-efficient techniques including Weight-Decomposed Low-Rank Adaptation
(DoRA) and Rank-Stabilized LoRA (rsLoRA); second, reinforcement learning using
Group Relative Policy Optimization (GRPO) with a sophisticated multi-component
reward system that refines accuracy, format adherence, and reasoning quality.
Gazal-R1 achieves exceptional performance across medical benchmarks, scoring
87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing
models up to 12x larger. Beyond its strong empirical results, this work
provides detailed insights into the challenges of training reasoning-capable
models in specialized domains, including issues with reward hacking, training
instability, and the fundamental tension between factual recall and detailed
reasoning. Our methodology offers a reproducible framework for developing
high-capability, domain-specific language models that balance performance,
efficiency, and explainability.

</details>


### [47] [Thunder-LLM: Efficiently Adapting LLMs to Korean with Minimal Resources](https://arxiv.org/abs/2506.21595)
*Jinpyo Kim,Gyeongje Cho,Chanwoo Park,Jongwon Park,Jongmin Kim,Yeonkyoun So,Jaejin Lee*

Main category: cs.CL

TL;DR: Adapting English LLMs to Korean with minimal resources achieves state-of-the-art performance. End-to-end process and code are shared.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art LLMs often underperform in languages other than English or Chinese. The end-to-end training process of LLMs is largely unknown.

Method: Adaptation of an existing English-based LLM to Korean in a low-budget scenario, including data collection, preprocessing, model training, benchmark creation, and evaluation.

Result: The new bilingual models, Thunder-LLM and Thunder-LLM-Ins, achieve superior Korean performance compared to state-of-the-art models while utilizing minimal data and computational resources.

Conclusion: The study demonstrates a cost-efficient method to adapt existing LLMs to new languages, specifically Korean, achieving state-of-the-art performance with minimal resources. The authors share their experience and code publicly.

Abstract: Since state-of-the-art LLMs often underperform in languages other than
English or Chinese, improving the capability of LLMs in new languages has
become an essential task. Moreover, LLMs' entire end-to-end training process
remains largely unknown to the public due to proprietary reasons, technical
complexity, inconsistent documentation, and ethical considerations. The
complete picture remains a closely guarded secret within the industry. This
paper presents methods to adapt an existing English-based LLM to Korean in a
low-budget scenario. We describe the entire end-to-end process: collecting
Korean datasets, preprocessing the data, training the model, creating
downstream benchmarks, and conducting evaluations. The evaluation results
indicate that our method can effectively and cost-efficiently add new language
capabilities to existing LLMs. Our new bilingual models, Thunder-LLM and
Thunder-LLM-Ins, achieve superior Korean performance compared to
state-of-the-art models while utilizing minimal data and computational
resources. We share our comprehensive experience and make the code publicly
available.

</details>


### [48] [BiMark: Unbiased Multilayer Watermarking for Large Language Models](https://arxiv.org/abs/2506.21602)
*Xiaoyan Feng,He Zhang,Yanjun Zhang,Leo Yu Zhang,Shirui Pan*

Main category: cs.CL

TL;DR: BiMark, a novel watermarking framework that achieves text quality preservation, model-agnostic detection, and message embedding capacity.


<details>
  <summary>Details</summary>
Motivation: urgent concerns about LLM-generated text authenticity, prompting regulatory demands for reliable identification mechanisms and existing approaches struggle to simultaneously achieve three critical requirements: text quality preservation, model-agnostic detection, and message embedding capacity

Method: a bit-flip unbiased reweighting mechanism, a multilayer architecture and an information encoding approach

Result: BiMark achieves up to 30% higher extraction rates for short texts while maintaining text quality indicated by lower perplexity, and performs comparably to non-watermarked text on downstream tasks such as summarization and translation.

Conclusion: BiMark achieves up to 30% higher extraction rates for short texts while maintaining text quality and performs comparably to non-watermarked text on downstream tasks.

Abstract: Recent advances in Large Language Models (LLMs) have raised urgent concerns
about LLM-generated text authenticity, prompting regulatory demands for
reliable identification mechanisms. Although watermarking offers a promising
solution, existing approaches struggle to simultaneously achieve three critical
requirements: text quality preservation, model-agnostic detection, and message
embedding capacity, which are crucial for practical implementation. To achieve
these goals, the key challenge lies in balancing the trade-off between text
quality preservation and message embedding capacity. To address this challenge,
we propose BiMark, a novel watermarking framework that achieves these
requirements through three key innovations: (1) a bit-flip unbiased reweighting
mechanism enabling model-agnostic detection, (2) a multilayer architecture
enhancing detectability without compromising generation quality, and (3) an
information encoding approach supporting multi-bit watermarking. Through
theoretical analysis and extensive experiments, we validate that, compared to
state-of-the-art multi-bit watermarking methods, BiMark achieves up to 30%
higher extraction rates for short texts while maintaining text quality
indicated by lower perplexity, and performs comparably to non-watermarked text
on downstream tasks such as summarization and translation.

</details>


### [49] [Operationalizing Automated Essay Scoring: A Human-Aware Approach](https://arxiv.org/abs/2506.21603)
*Yenisel Plasencia-Calaña*

Main category: cs.CL

TL;DR: This paper compares ML-based AES models and LLMs, finding ML models more accurate but less explainable, and both struggling with bias and robustness.


<details>
  <summary>Details</summary>
Motivation: explores the human-centric operationalization of Automated Essay Scoring (AES) systems, addressing aspects beyond accuracy

Method: comparing various machine learning-based approaches with Large Language Models (LLMs) approaches

Result: identifying their strengths, similarities and differences in bias, robustness, and explainability

Conclusion: ML-based AES models outperform LLMs in accuracy but struggle with explainability, whereas LLMs provide richer explanations. Both approaches struggle with bias and robustness to edge scores.

Abstract: This paper explores the human-centric operationalization of Automated Essay
Scoring (AES) systems, addressing aspects beyond accuracy. We compare various
machine learning-based approaches with Large Language Models (LLMs) approaches,
identifying their strengths, similarities and differences. The study
investigates key dimensions such as bias, robustness, and explainability,
considered important for human-aware operationalization of AES systems. Our
study shows that ML-based AES models outperform LLMs in accuracy but struggle
with explainability, whereas LLMs provide richer explanations. We also found
that both approaches struggle with bias and robustness to edge scores. By
analyzing these dimensions, the paper aims to identify challenges and
trade-offs between different methods, contributing to more reliable and
trustworthy AES methods.

</details>


### [50] [MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents](https://arxiv.org/abs/2506.21605)
*Haoran Tan,Zeyu Zhang,Chen Ma,Xu Chen,Quanyu Dai,Zhenhua Dong*

Main category: cs.CL

TL;DR: 本文构建了一个名为 MemBench 的基准，用于评估基于 LLM 的代理的记忆能力，包括有效性、效率和容量。


<details>
  <summary>Details</summary>
Motivation: 先前对记忆能力的评估通常受到记忆水平和交互场景多样性的限制。他们也缺乏全面的指标来反映多个方面的记忆能力。

Method: 构建了一个更全面的数据集和基准来评估基于 LLM 的代理的记忆能力。数据集包含事实记忆和反思记忆作为不同级别，并提出参与和观察作为各种交互场景。

Result: 构建了一个更全面的数据集和基准，用于评估基于 LLM 的代理的记忆能力。

Conclusion: 提出了一个名为 MemBench 的基准，用于从多个方面评估基于 LLM 的代理的记忆能力，包括它们的有效性、效率和容量。

Abstract: Recent works have highlighted the significance of memory mechanisms in
LLM-based agents, which enable them to store observed information and adapt to
dynamic environments. However, evaluating their memory capabilities still
remains challenges. Previous evaluations are commonly limited by the diversity
of memory levels and interactive scenarios. They also lack comprehensive
metrics to reflect the memory capabilities from multiple aspects. To address
these problems, in this paper, we construct a more comprehensive dataset and
benchmark to evaluate the memory capability of LLM-based agents. Our dataset
incorporates factual memory and reflective memory as different levels, and
proposes participation and observation as various interactive scenarios. Based
on our dataset, we present a benchmark, named MemBench, to evaluate the memory
capability of LLM-based agents from multiple aspects, including their
effectiveness, efficiency, and capacity. To benefit the research community, we
release our dataset and project at https://github.com/import-myself/Membench.

</details>


### [51] [Large Language Models as symbolic DNA of cultural dynamics](https://arxiv.org/abs/2506.21606)
*Parham Pourdavood,Michael Jacob,Terrence Deacon*

Main category: cs.CL

TL;DR: LLMs are like DNA for culture: they store compressed patterns that humans reinterpret, catalyzing creativity.


<details>
  <summary>Details</summary>
Motivation: To conceptualize LLMs as externalized informational substrates, analogous to DNA, for human cultural dynamics.

Method: Analysis of four universal features: compression, decompression, externalization, and recursion.

Result: LLMs preserve regularities of human culture without understanding embodied human experience, providing a tool for self-reflection and hypothesis generation.

Conclusion: LLMs are tools for cultural evolution, enabling hypothesis generation while requiring human interpretation to ground them in aesthetics and norms.

Abstract: This paper proposes a novel conceptualization of Large Language Models (LLMs)
as externalized informational substrates that function analogously to DNA for
human cultural dynamics. Rather than viewing LLMs as either autonomous
intelligence or mere programmed mimicry, we argue they serve a broader role as
repositories that preserve compressed patterns of human symbolic
expression--"fossils" of meaningful dynamics that retain relational residues
without their original living contexts. Crucially, these compressed patterns
only become meaningful through human reinterpretation, creating a recursive
feedback loop where they can be recombined and cycle back to ultimately
catalyze human creative processes. Through analysis of four universal
features--compression, decompression, externalization, and recursion--we
demonstrate that just as DNA emerged as a compressed and externalized medium
for preserving useful cellular dynamics without containing explicit reference
to goal-directed physical processes, LLMs preserve useful regularities of human
culture without containing understanding of embodied human experience.
Therefore, we argue that LLMs' significance lies not in rivaling human
intelligence, but in providing humanity a tool for self-reflection and playful
hypothesis-generation in a low-stakes, simulated environment. This framework
positions LLMs as tools for cultural evolvability, enabling humanity to
generate novel hypotheses about itself while maintaining the human
interpretation necessary to ground these hypotheses in ongoing human aesthetics
and norms.

</details>


### [52] [CORE-KG: An LLM-Driven Knowledge Graph Construction Framework for Human Smuggling Networks](https://arxiv.org/abs/2506.21607)
*Dipak Meher,Carlotta Domeniconi,Guadalupe Correa-Cabrera*

Main category: cs.CL

TL;DR: CORE-KG, a modular framework, constructs interpretable KGs from legal texts using type-aware coreference resolution and domain-guided extraction, reducing node duplication and legal noise for analyzing criminal networks.


<details>
  <summary>Details</summary>
Motivation: Human smuggling networks are increasingly adaptive and difficult to analyze. Legal case documents offer valuable insights but are unstructured, lexically dense, and filled with ambiguous or shifting references-posing challenges for automated knowledge graph (KG) construction. Existing KG methods often rely on static templates and lack coreference resolution, while recent LLM-based approaches frequently produce noisy, fragmented graphs due to hallucinations, and duplicate nodes caused by a lack of guided extraction.

Method: a modular framework for building interpretable KGs from legal texts. It uses a two-step pipeline: (1) type-aware coreference resolution via sequential, structured LLM prompts, and (2) entity and relationship extraction using domain-guided instructions, built on an adapted GraphRAG framework.

Result: CORE-KG reduces node duplication by 33.28%, and legal noise by 38.37% compared to a GraphRAG-based baseline-resulting in cleaner and more coherent graph structures.

Conclusion: CORE-KG creates cleaner, more coherent graph structures, making it a strong foundation for analyzing complex criminal networks.

Abstract: Human smuggling networks are increasingly adaptive and difficult to analyze.
Legal case documents offer valuable insights but are unstructured, lexically
dense, and filled with ambiguous or shifting references-posing challenges for
automated knowledge graph (KG) construction. Existing KG methods often rely on
static templates and lack coreference resolution, while recent LLM-based
approaches frequently produce noisy, fragmented graphs due to hallucinations,
and duplicate nodes caused by a lack of guided extraction. We propose CORE-KG,
a modular framework for building interpretable KGs from legal texts. It uses a
two-step pipeline: (1) type-aware coreference resolution via sequential,
structured LLM prompts, and (2) entity and relationship extraction using
domain-guided instructions, built on an adapted GraphRAG framework. CORE-KG
reduces node duplication by 33.28%, and legal noise by 38.37% compared to a
GraphRAG-based baseline-resulting in cleaner and more coherent graph
structures. These improvements make CORE-KG a strong foundation for analyzing
complex criminal networks.

</details>


### [53] [SysTemp: A Multi-Agent System for Template-Based Generation of SysML v2](https://arxiv.org/abs/2506.21608)
*Yasmine Bouamra,Bruno Yun,Alexandre Poisson,Frédéric Armetta*

Main category: cs.CL

TL;DR: SysTemp facilitates and improves the creation of SysML v2 models from natural language specifications.


<details>
  <summary>Details</summary>
Motivation: The automatic generation of SysML v2 models represents a major challenge in the engineering of complex systems, particularly due to the scarcity of learning corpora and complex syntax.

Method: It is based on a multi-agent system, including a template generator that structures the generation process.

Result: We discuss the advantages and challenges of this system through an evaluation.

Conclusion: The system has the potential to improve the quality of the generations in SysML v2 modeling.

Abstract: The automatic generation of SysML v2 models represents a major challenge in
the engineering of complex systems, particularly due to the scarcity of
learning corpora and complex syntax. We present SysTemp, a system aimed at
facilitating and improving the creation of SysML v2 models from natural
language specifications. It is based on a multi-agent system, including a
template generator that structures the generation process. We discuss the
advantages and challenges of this system through an evaluation, highlighting
its potential to improve the quality of the generations in SysML v2 modeling.

</details>


### [54] [From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models](https://arxiv.org/abs/2506.21609)
*Junhao Liu,Zhenhao Xu,Yuxin Fang,Yichuan Chen,Zuobin Ying,Wenhan Chang*

Main category: cs.CL

TL;DR: 本文分析了四个大型语言模型的推理过程和输出，揭示了它们在推理深度和方式上的差异，并提供了关于计算效率和推理鲁棒性之间权衡的见解。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要忽略了对这些模型推理过程和输出的全面和系统比较，特别是关于它们的自我反思模式（也称为“顿悟时刻”）以及跨不同领域的互连。

Method: 使用关键词统计和 LLM-as-a-judge 范式分析四个先进的大型推理模型（GPT-o1、DeepSeek-R1、Kimi-k1.5 和 Grok-3）的推理特征。

Result: 揭示了这些模型在推理过程中如何在探索和利用之间取得平衡，如何处理问题以及如何得出结论的各种模式。通过定量和定性比较，发现这些模型在推理深度、对中间步骤的依赖以及它们的思维过程和输出模式与 GPT-o1 之间的相似程度等方面存在差异。

Conclusion: 揭示了这些模型在推理过程中如何在探索和利用之间取得平衡，如何处理问题以及如何得出结论的各种模式。通过定量和定性比较，发现这些模型在推理深度、对中间步骤的依赖以及它们的思维过程和输出模式与 GPT-o1 之间的相似程度等方面存在差异。

Abstract: Recently, there have been notable advancements in large language models
(LLMs), demonstrating their growing abilities in complex reasoning. However,
existing research largely overlooks a thorough and systematic comparison of
these models' reasoning processes and outputs, particularly regarding their
self-reflection pattern (also termed "Aha moment") and the interconnections
across diverse domains. This paper proposes a novel framework for analyzing the
reasoning characteristics of four cutting-edge large reasoning models (GPT-o1,
DeepSeek-R1, Kimi-k1.5, and Grok-3) using keywords statistic and LLM-as-a-judge
paradigm. Our approach connects their internal thinking processes with their
final outputs. A diverse dataset consists of real-world scenario-based
questions covering logical deduction, causal inference, and multi-step
problem-solving. Additionally, a set of metrics is put forward to assess both
the coherence of reasoning and the accuracy of the outputs. The research
results uncover various patterns of how these models balance exploration and
exploitation, deal with problems, and reach conclusions during the reasoning
process. Through quantitative and qualitative comparisons, disparities among
these models are identified in aspects such as the depth of reasoning, the
reliance on intermediate steps, and the degree of similarity between their
thinking processes and output patterns and those of GPT-o1. This work offers
valuable insights into the trade-off between computational efficiency and
reasoning robustness and provides practical recommendations for enhancing model
design and evaluation in practical applications. We publicly release our
project at: https://github.com/ChangWenhan/FromThinking2Output

</details>


### [55] [Does Multimodality Lead to Better Time Series Forecasting?](https://arxiv.org/abs/2506.21611)
*Xiyuan Zhang,Boran Han,Haoyang Fang,Abdul Fatir Ansari,Shuai Zhang,Danielle C. Maddix,Cuixiong Hu,Andrew Gordon Wilson,Michael W. Mahoney,Hao Wang,Yan Liu,Huzefa Rangwala,George Karypis,Bernie Wang*

Main category: cs.CL

TL;DR: 研究了多模态时间序列预测，发现其有效性取决于模型和数据特性。


<details>
  <summary>Details</summary>
Motivation: 研究将文本信息整合到时间序列预测基础模型中是否以及在何种条件下始终产生增益。

Method: 评估两种流行的多模态预测范例：基于对齐的方法和基于提示的方法。

Result: 多模态方法的效果并非在所有数据集和模型上都是普遍的，并且多模态方法有时并不优于最强的单模态基线。

Conclusion: 多模态方法并非在所有数据集和模型上都优于最强的单模态基线模型。当存在以下情况时，结合文本信息最有帮助：(1) 高容量文本模型，(2) 相对较弱的时间序列模型，和 (3) 适当的对齐策略。在数据方面，当 (4) 有足够的训练数据可用时，并且 (5) 文本提供了超出仅从时间序列中捕获的补充预测信号时，更有可能获得性能提升。

Abstract: Recently, there has been growing interest in incorporating textual
information into foundation models for time series forecasting. However, it
remains unclear whether and under what conditions such multimodal integration
consistently yields gains. We systematically investigate these questions across
a diverse benchmark of 14 forecasting tasks spanning 7 domains, including
health, environment, and economics. We evaluate two popular multimodal
forecasting paradigms: aligning-based methods, which align time series and text
representations; and prompting-based methods, which directly prompt large
language models for forecasting. Although prior works report gains from
multimodal input, we find these effects are not universal across datasets and
models, and multimodal methods sometimes do not outperform the strongest
unimodal baselines. To understand when textual information helps, we
disentangle the effects of model architectural properties and data
characteristics. Our findings highlight that on the modeling side,
incorporating text information is most helpful given (1) high-capacity text
models, (2) comparatively weaker time series models, and (3) appropriate
aligning strategies. On the data side, performance gains are more likely when
(4) sufficient training data is available and (5) the text offers complementary
predictive signal beyond what is already captured from the time series alone.
Our empirical findings offer practical guidelines for when multimodality can be
expected to aid forecasting tasks, and when it does not.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [56] [Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs](https://arxiv.org/abs/2506.21656)
*Yifan Shen,Yuanzhe Liu,Jingyuan Zhu,Xu Cao,Xiaofeng Zhang,Yixiao He,Wenming Ye,James Matthew Rehg,Ismini Lourentzou*

Main category: cs.CV

TL;DR: SpatialReasoner-R1是一种视觉语言推理模型，它使用M3CTS和fDPO来提高空间推理能力，并在SPATIALRGPT-Bench上取得了新的SoTA。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉语言模型（VLMs）在细粒度的空间推理方面存在困难，特别是在需要多步逻辑和精确空间对齐时。

Method: 提出了一个多模型蒙特卡洛树搜索（M3CTS）方法，用于生成多样且逻辑上一致的长链思维（LongCoT）推理轨迹。此外，还提出了细粒度的直接偏好优化（fDPO）。

Result: fDPO在空间质量任务上的平均改进为4.1%，在空间数量任务上的平均改进为9.0%。

Conclusion: SpatialReasoner-R1在SPATIALRGPT-Bench上取得了新的SoTA，平均准确率比最强的基线高出9.8%，并在一般的视觉语言任务上保持了竞争性的表现。

Abstract: Current Vision-Language Models (VLMs) struggle with fine-grained spatial
reasoning, particularly when multi-step logic and precise spatial alignment are
required. In this work, we introduce SpatialReasoner-R1, a vision-language
reasoning model designed to address these limitations. To construct
high-quality supervision for spatial reasoning, we design a Multi-Model Monte
Carlo Tree Search (M3CTS) method that generates diverse, logically consistent
Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose
fine-grained Direct Preference Optimization (fDPO), which introduces
segment-specific preference granularity for descriptive grounding and logical
reasoning, guided by a spatial reward mechanism that evaluates candidate
responses based on visual consistency, spatial grounding, and logical
coherence. Experimental results demonstrate that fDPO achieves an average
improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%
gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a
new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in
average accuracy, while maintaining competitive performance on general
vision-language tasks.

</details>


### [57] [TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360° Panorama Generation](https://arxiv.org/abs/2506.21681)
*Hakan Çapuk,Andrew Bond,Muhammed Burak Kızıl,Emir Göçen,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

TL;DR: introduce TanDiT, a method that synthesizes panoramic scenes by generating grids of tangent-plane images covering the entire 360$\circ$ view


<details>
  <summary>Details</summary>
Motivation: these models still struggle with panoramic image generation due to unique challenges, including varying levels of geometric distortion and the requirement for seamless loop-consistency. To address these issues while leveraging the strengths of the existing models

Method: a method that synthesizes panoramic scenes by generating grids of tangent-plane images covering the entire 360$^\circ$ view. Unlike previous methods relying on multiple diffusion branches, TanDiT utilizes a unified diffusion model trained to produce these tangent-plane images simultaneously within a single denoising iteration. Furthermore, we propose a model-agnostic post-processing step specifically designed to enhance global coherence across the generated panoramas.

Result: To accurately assess panoramic image quality, we also present two specialized metrics, TangentIS and TangentFID, and provide a comprehensive benchmark comprising captioned panoramic datasets and standardized evaluation scripts.

Conclusion: The method generalizes effectively beyond its training data, robustly interprets detailed and complex text prompts, and seamlessly integrates with various generative models to yield high-quality, diverse panoramic images.

Abstract: Recent advances in image generation have led to remarkable improvements in
synthesizing perspective images. However, these models still struggle with
panoramic image generation due to unique challenges, including varying levels
of geometric distortion and the requirement for seamless loop-consistency. To
address these issues while leveraging the strengths of the existing models, we
introduce TanDiT, a method that synthesizes panoramic scenes by generating
grids of tangent-plane images covering the entire 360$^\circ$ view. Unlike
previous methods relying on multiple diffusion branches, TanDiT utilizes a
unified diffusion model trained to produce these tangent-plane images
simultaneously within a single denoising iteration. Furthermore, we propose a
model-agnostic post-processing step specifically designed to enhance global
coherence across the generated panoramas. To accurately assess panoramic image
quality, we also present two specialized metrics, TangentIS and TangentFID, and
provide a comprehensive benchmark comprising captioned panoramic datasets and
standardized evaluation scripts. Extensive experiments demonstrate that our
method generalizes effectively beyond its training data, robustly interprets
detailed and complex text prompts, and seamlessly integrates with various
generative models to yield high-quality, diverse panoramic images.

</details>


### [58] [FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering](https://arxiv.org/abs/2506.21710)
*Liangyu Zhong,Fabio Rosenthal,Joachim Sicking,Fabian Hüger,Thorsten Bagdonat,Hanno Gottschalk,Leo Schwinn*

Main category: cs.CV

TL;DR: This paper introduces FOCUS, a training-free visual cropping method that uses MLLM-internal representations to find the most relevant image region for fine-grained VQA tasks. FOCUS achieves strong performance with less computation compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Visual Question Answering (VQA) focusing on small image details still remains a challenge. Although visual cropping techniques seem promising, recent approaches have several limitations: the need for task-specific fine-tuning, low efficiency due to uninformed exhaustive search, or incompatibility with efficient attention implementations.

Method: a training-free visual cropping method, dubbed FOCUS, that leverages MLLM-internal representations to guide the search for the most relevant image region. This is accomplished in four steps: first, we identify the target object(s) in the VQA prompt; second, we compute an object relevance map using the key-value (KV) cache; third, we propose and rank relevant image regions based on the map; and finally, we perform the fine-grained VQA task using the top-ranked region.

Result: FOCUS outperforms three popular visual cropping methods in both accuracy and efficiency, and matches the best-performing baseline, ZoomEye, while requiring 3 - 6.5 x less compute.

Conclusion: FOCUS achieves strong performance across four fine-grained VQA datasets and two types of MLLMs. It outperforms three popular visual cropping methods in both accuracy and efficiency, and matches the best-performing baseline, ZoomEye, while requiring 3 - 6.5 x less compute.

Abstract: While Multimodal Large Language Models (MLLMs) offer strong perception and
reasoning capabilities for image-text input, Visual Question Answering (VQA)
focusing on small image details still remains a challenge. Although visual
cropping techniques seem promising, recent approaches have several limitations:
the need for task-specific fine-tuning, low efficiency due to uninformed
exhaustive search, or incompatibility with efficient attention implementations.
We address these shortcomings by proposing a training-free visual cropping
method, dubbed FOCUS, that leverages MLLM-internal representations to guide the
search for the most relevant image region. This is accomplished in four steps:
first, we identify the target object(s) in the VQA prompt; second, we compute
an object relevance map using the key-value (KV) cache; third, we propose and
rank relevant image regions based on the map; and finally, we perform the
fine-grained VQA task using the top-ranked region. As a result of this informed
search strategy, FOCUS achieves strong performance across four fine-grained VQA
datasets and two types of MLLMs. It outperforms three popular visual cropping
methods in both accuracy and efficiency, and matches the best-performing
baseline, ZoomEye, while requiring 3 - 6.5 x less compute.

</details>


### [59] [CAST: Cross-Attentive Spatio-Temporal feature fusion for Deepfake detection](https://arxiv.org/abs/2506.21711)
*Aryan Thakre,Omkar Nagwekar,Vedang Talekar,Aparna Santra Biswas*

Main category: cs.CV

TL;DR: The paper proposes a cross-attention-based CNN-Transformer model (CAST) for deepfake detection that improves spatio-temporal feature fusion, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing CNN-Transformer models process spatial and temporal features independently, limiting the depth of spatio-temporal interaction.

Method: A unified CAST model that leverages cross-attention to fuse spatial and temporal features in an integrated manner.

Result: The model achieves an AUC of 99.49% and an accuracy of 97.57% in intra-dataset evaluations and a 93.31% AUC on the unseen DeepfakeDetection dataset in cross-dataset testing.

Conclusion: The proposed CAST model demonstrates strong performance in deepfake video detection, achieving high AUC and accuracy in both intra- and cross-dataset evaluations, highlighting the effectiveness of cross-attention-based feature fusion.

Abstract: Deepfakes have emerged as a significant threat to digital media authenticity,
increasing the need for advanced detection techniques that can identify subtle
and time-dependent manipulations. CNNs are effective at capturing spatial
artifacts, and Transformers excel at modeling temporal inconsistencies.
However, many existing CNN-Transformer models process spatial and temporal
features independently. In particular, attention-based methods often use
separate attention mechanisms for spatial and temporal features and combine
them using naive approaches like averaging, addition, or concatenation, which
limits the depth of spatio-temporal interaction. To address this challenge, we
propose a unified CAST model that leverages cross-attention to effectively fuse
spatial and temporal features in a more integrated manner. Our approach allows
temporal features to dynamically attend to relevant spatial regions, enhancing
the model's ability to detect fine-grained, time-evolving artifacts such as
flickering eyes or warped lips. This design enables more precise localization
and deeper contextual understanding, leading to improved performance across
diverse and challenging scenarios. We evaluate the performance of our model
using the FaceForensics++, Celeb-DF, and DeepfakeDetection datasets in both
intra- and cross-dataset settings to affirm the superiority of our approach.
Our model achieves strong performance with an AUC of 99.49 percent and an
accuracy of 97.57 percent in intra-dataset evaluations. In cross-dataset
testing, it demonstrates impressive generalization by achieving a 93.31 percent
AUC on the unseen DeepfakeDetection dataset. These results highlight the
effectiveness of cross-attention-based feature fusion in enhancing the
robustness of deepfake video detection.

</details>


### [60] [Elucidating and Endowing the Diffusion Training Paradigm for General Image Restoration](https://arxiv.org/abs/2506.21722)
*Xin Lu,Xueyang Fu,Jie Xiao,Zihao Fan,Yurui Zhu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 提出了一种新的红外框架，该框架通过扩散训练范式来提高通用红外网络的性能，并在单任务和多任务红外中都取得了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像恢复（IR）任务中表现出强大的生成能力，但与主流的基于重建的通用普通红外网络相比，其复杂的架构和迭代过程限制了它们的实际应用。现有的方法主要集中在优化网络架构和扩散路径上，但忽略了将扩散训练范式集成到通用普通红外框架中。

Method: 通过对时间步长依赖性、网络层次结构、噪声水平关系和多重恢复任务相关性进行系统分析，提出了一个新的基于扩散训练的红外框架。为了使红外网络能够同时恢复图像和建模生成表示，我们引入了一系列正则化策略，使扩散目标与红外任务对齐，从而提高了单任务场景中的泛化能力。此外，认识到基于扩散的生成对不同红外任务产生不同的影响，我们开发了一种增量训练范式和特定于任务的适配器，进一步提高了多任务统一红外中的性能。

Result: 我们的方法显著提高了红外网络在单任务红外中的泛化能力，并在多任务统一红外中实现了卓越的性能。

Conclusion: 该方法显著提高了红外网络在单任务红外中的泛化能力，并在多任务统一红外中实现了卓越的性能。值得注意的是，该框架可以无缝集成到现有的通用红外架构中。

Abstract: While diffusion models demonstrate strong generative capabilities in image
restoration (IR) tasks, their complex architectures and iterative processes
limit their practical application compared to mainstream reconstruction-based
general ordinary IR networks. Existing approaches primarily focus on optimizing
network architecture and diffusion paths but overlook the integration of the
diffusion training paradigm within general ordinary IR frameworks. To address
these challenges, this paper elucidates key principles for adapting the
diffusion training paradigm to general IR training through systematic analysis
of time-step dependencies, network hierarchies, noise-level relationships, and
multi-restoration task correlations, proposing a new IR framework supported by
diffusion-based training. To enable IR networks to simultaneously restore
images and model generative representations, we introduce a series of
regularization strategies that align diffusion objectives with IR tasks,
improving generalization in single-task scenarios. Furthermore, recognizing
that diffusion-based generation exerts varying influences across different IR
tasks, we develop an incremental training paradigm and task-specific adaptors,
further enhancing performance in multi-task unified IR. Experiments demonstrate
that our method significantly improves the generalization of IR networks in
single-task IR and achieves superior performance in multi-task unified IR.
Notably, the proposed framework can be seamlessly integrated into existing
general IR architectures.

</details>


### [61] [Asymmetric Dual Self-Distillation for 3D Self-Supervised Representation Learning](https://arxiv.org/abs/2506.21724)
*Remco F. Leijenaar,Hamidreza Kasaei*

Main category: cs.CV

TL;DR: AsymDSD is proposed, an Asymmetric Dual Self-Distillation framework that unifies masked modeling and invariance learning through prediction in the latent space rather than the input space,achieving state-of-the-art results on ScanObjectNN.


<details>
  <summary>Details</summary>
Motivation: Learning semantically meaningful representations from unstructured 3D point clouds remains a central challenge in computer vision, especially in the absence of large-scale labeled datasets. While masked point modeling (MPM) is widely used in self-supervised 3D learning, its reconstruction-based objective can limit its ability to capture high-level semantics.

Method: AsymDSD, an Asymmetric Dual Self-Distillation framework that unifies masked modeling and invariance learning through prediction in the latent space rather than the input space. AsymDSD builds on a joint embedding architecture and introduces several key design choices: an efficient asymmetric setup, disabling attention between masked queries to prevent shape leakage, multi-mask sampling, and a point cloud adaptation of multi-crop.

Result: achieves state-of-the-art results on ScanObjectNN (90.53%) and further improves to 93.72% when pretrained on 930k shapes, surpassing prior methods

Conclusion: AsymDSD achieves state-of-the-art results on ScanObjectNN (90.53%) and further improves to 93.72% when pretrained on 930k shapes, surpassing prior methods.

Abstract: Learning semantically meaningful representations from unstructured 3D point
clouds remains a central challenge in computer vision, especially in the
absence of large-scale labeled datasets. While masked point modeling (MPM) is
widely used in self-supervised 3D learning, its reconstruction-based objective
can limit its ability to capture high-level semantics. We propose AsymDSD, an
Asymmetric Dual Self-Distillation framework that unifies masked modeling and
invariance learning through prediction in the latent space rather than the
input space. AsymDSD builds on a joint embedding architecture and introduces
several key design choices: an efficient asymmetric setup, disabling attention
between masked queries to prevent shape leakage, multi-mask sampling, and a
point cloud adaptation of multi-crop. AsymDSD achieves state-of-the-art results
on ScanObjectNN (90.53%) and further improves to 93.72% when pretrained on 930k
shapes, surpassing prior methods.

</details>


### [62] [Exploring Image Generation via Mutually Exclusive Probability Spaces and Local Correlation Hypothesis](https://arxiv.org/abs/2506.21731)
*Chenqiu Zhao,Anup Basu*

Main category: cs.CV

TL;DR: 论文提出了MESP和LCH框架，旨在解决概率生成模型中全局分布学习导致的记忆问题，并通过实验验证了这些框架。


<details>
  <summary>Details</summary>
Motivation: 概率生成模型在学习全局分布时可能存在局限性，导致记忆而非生成行为。

Method: 提出了互斥概率空间（MESP）和局部相关性假设（LCH）两种理论框架。基于MESP，提出了二元潜在自编码器（BL-AE）和自回归随机变量模型（ARVM）。

Result: ARVM在标准数据集上取得了有竞争力的FID分数，超过了现有方法。然而，这些分数反映的是记忆而不是生成。

Conclusion: 生成模型中的全局分布学习可能导致记忆而非生成行为，并通过实验验证了所提出的框架。

Abstract: We propose two theoretical frameworks, the Mutually Exclusive Probability
Space (MESP) and the Local Correlation Hypothesis (LCH), to explore a potential
limitation in probabilistic generative models; namely that learning global
distributions leads to memorization rather than generative behavior. MESP
emerges from our rethinking of the Variational Autoencoder (VAE). We observe
that latent variable distributions in VAE exhibit overlap, which leads to an
optimization conflict between the reconstruction loss and KL-divergence loss. A
lower bound based on the overlap coefficient is proposed. We refer to this
phenomenon as Mutually Exclusive Probability Spaces. Based on MESP, a Binary
Latent Autoencoder (BL-AE) is proposed to encode images into binary latent
representations. These binary latents are used as the input to our
Autoregressive Random Variable Model (ARVM), a modified autoregressive model
outputting histograms. Our ARVM achieves competitive FID scores, outperforming
state-of-the-art methods on standard datasets. However, such scores reflect
memorization rather than generation. To address this issue, we propose the
Local Correlation Hypothesis (LCH), which posits that generative capability
arising from local correlations among latent variables. Comprehensive
experiments and discussions are conducted to validate our frameworks.

</details>


### [63] [Equitable Federated Learning with NCA](https://arxiv.org/abs/2506.21735)
*Nick Lemke,Mirko Konstantin,Henry John Krumb,John Kalkhof,Jonathan Stieber,Anirban Mukhopadhyay*

Main category: cs.CV

TL;DR: FedNCA: A novel FL system tailored for medical image segmentation tasks in low-resource settings. It leverages the lightweight Med-NCA architecture, enabling training on low-cost edge devices, such as widely available smartphones, while minimizing communication costs. Additionally, our encryption-ready FedNCA proves to be suitable for compromised network communication.


<details>
  <summary>Details</summary>
Motivation: FL adoption in LMICs faces significant barriers, including limited high-performance computing resources and unreliable internet connectivity. To address these challenges

Method: FedNCA leverages the lightweight Med-NCA architecture, enabling training on low-cost edge devices, such as widely available smartphones, while minimizing communication costs. Additionally, our encryption-ready FedNCA proves to be suitable for compromised network communication.

Result: introduce FedNCA, a novel FL system tailored for medical image segmentation tasks

Conclusion: FedNCA paves the way for inclusive, efficient, lightweight, and encryption-ready medical imaging solutions, fostering equitable healthcare advancements in resource-constrained regions.

Abstract: Federated Learning (FL) is enabling collaborative model training across
institutions without sharing sensitive patient data. This approach is
particularly valuable in low- and middle-income countries (LMICs), where access
to trained medical professionals is limited. However, FL adoption in LMICs
faces significant barriers, including limited high-performance computing
resources and unreliable internet connectivity. To address these challenges, we
introduce FedNCA, a novel FL system tailored for medical image segmentation
tasks. FedNCA leverages the lightweight Med-NCA architecture, enabling training
on low-cost edge devices, such as widely available smartphones, while
minimizing communication costs. Additionally, our encryption-ready FedNCA
proves to be suitable for compromised network communication. By overcoming
infrastructural and security challenges, FedNCA paves the way for inclusive,
efficient, lightweight, and encryption-ready medical imaging solutions,
fostering equitable healthcare advancements in resource-constrained regions.

</details>


### [64] [ImplicitQA: Going beyond frames towards Implicit Video Reasoning](https://arxiv.org/abs/2506.21742)
*Sirnam Swetha,Rohit Gupta,Parth Parag Kulkarni,David G Shatwell,Jeffrey A Chan Santiago,Nyle Siddiqui,Joseph Fioresi,Mubarak Shah*

Main category: cs.CV

TL;DR: This paper introduces ImplicitQA, a new benchmark for video QA that tests implicit reasoning, which current models struggle with.


<details>
  <summary>Details</summary>
Motivation: Current VideoQA benchmarks focus on explicit visual content, neglecting implicit reasoning which is crucial for understanding creative and cinematic videos.

Method: The authors created a dataset of 1K QA pairs from 320+ creative video clips, categorized into key reasoning dimensions.

Result: Evaluations on leading VideoQA models show performance degradation on ImplicitQA, highlighting the difficulty of implicit reasoning.

Conclusion: The paper introduces ImplicitQA, a new benchmark for video question answering that focuses on implicit reasoning, and shows that current VideoQA models struggle with this type of reasoning.

Abstract: Video QA has made significant strides by leveraging multimodal learning to
align visual and textual modalities. However, current benchmarks overwhelmingly
focus on questions answerable through explicit visual content - actions,
objects & events directly observable within individual frames or short clips.
In contrast, creative and cinematic videos - such as movies, TV shows, and
narrative-driven content - employ storytelling techniques that deliberately
omit certain depictions, requiring viewers to infer motives, causality, and
relationships across discontinuous frames. Humans naturally excel at such
implicit reasoning, seamlessly integrating information across time and context
to construct coherent narratives. Current VideoQA systems and benchmarks fail
to capture this essential dimension of human-like understanding. To bridge this
gap, we present ImplicitQA, a novel benchmark specifically designed to test
models on implicit reasoning. It comprises 1K meticulously annotated QA pairs
derived from 320+ high-quality creative video clips, systematically categorized
into key reasoning dimensions: lateral and vertical spatial reasoning, depth
and proximity, viewpoint and visibility, motion and trajectory, causal and
motivational reasoning, social interactions, physical context, and inferred
counting. These annotations are deliberately challenging, crafted by authors
ensuring high-quality. Our extensive evaluations on leading VideoQA models
reveals performance degradation, underscoring their reliance on surface-level
visual cues and highlighting the difficulty of implicit reasoning. Performance
variations across models further illustrate the complexity and diversity of the
challenges presented by ImplicitQA. By releasing both the dataset and our data
collection framework, we aim to stimulate further research and development in
the community. https://huggingface.co/datasets/ucf-crcv/ImplicitQA.

</details>


### [65] [Early Glaucoma Detection using Deep Learning with Multiple Datasets of Fundus Images](https://arxiv.org/abs/2506.21770)
*Rishiraj Paul Chowdhury,Nirmit Shekar Karkera*

Main category: cs.CV

TL;DR: Developed a deep learning pipeline for early glaucoma detection using EfficientNet-B0, achieving strong performance with minimal preprocessing and good generalization across datasets.


<details>
  <summary>Details</summary>
Motivation: Glaucoma is a leading cause of irreversible blindness, but early detection can significantly improve treatment outcomes. Traditional diagnostic methods are often invasive and require specialized equipment.

Method: a deep learning pipeline using the EfficientNet-B0 architecture for glaucoma detection from retinal fundus images. sequentially train and fine-tune our model across ACRIMA, ORIGA, and RIM-ONE datasets to enhance generalization

Result: minimal preprocessing yields higher AUC-ROC compared to more complex enhancements, and our model demonstrates strong discriminative performance on unseen datasets.

Conclusion: The proposed deep learning pipeline offers a reproducible and scalable approach to early glaucoma detection, supporting its potential clinical utility.

Abstract: Glaucoma is a leading cause of irreversible blindness, but early detection
can significantly improve treatment outcomes. Traditional diagnostic methods
are often invasive and require specialized equipment. In this work, we present
a deep learning pipeline using the EfficientNet-B0 architecture for glaucoma
detection from retinal fundus images. Unlike prior studies that rely on single
datasets, we sequentially train and fine-tune our model across ACRIMA, ORIGA,
and RIM-ONE datasets to enhance generalization. Our experiments show that
minimal preprocessing yields higher AUC-ROC compared to more complex
enhancements, and our model demonstrates strong discriminative performance on
unseen datasets. The proposed pipeline offers a reproducible and scalable
approach to early glaucoma detection, supporting its potential clinical
utility.

</details>


### [66] [Comparing Learning Paradigms for Egocentric Video Summarization](https://arxiv.org/abs/2506.21785)
*Daniel Wen*

Main category: cs.CV

TL;DR: This study assesses supervised learning, unsupervised learning, and prompt fine-tuning for egocentric video understanding, finding that GPT-4o outperforms specialized models but all methods struggle compared to third-person video, highlighting the need for improvement in this area.


<details>
  <summary>Details</summary>
Motivation: To investigate various computer vision paradigms' ability to understand and interpret egocentric video data.

Method: We examine Shotluck Holmes (state-of-the-art supervised learning), TAC-SUM (state-of-the-art unsupervised learning), and GPT-4o (a prompt fine-tuned pre-trained model), evaluating their effectiveness in video summarization.

Result: current state-of-the-art models perform less effectively on first-person videos compared to third-person videos.  a prompt fine-tuned general-purpose GPT-4o model outperforms these specialized models

Conclusion: Current state-of-the-art models perform less effectively on first-person videos compared to third-person videos, highlighting the need for further advancements in the egocentric video domain.  A prompt fine-tuned general-purpose GPT-4o model outperforms these specialized models, emphasizing the limitations of existing approaches in adapting to the unique challenges of first-person perspectives.

Abstract: In this study, we investigate various computer vision paradigms - supervised
learning, unsupervised learning, and prompt fine-tuning - by assessing their
ability to understand and interpret egocentric video data. Specifically, we
examine Shotluck Holmes (state-of-the-art supervised learning), TAC-SUM
(state-of-the-art unsupervised learning), and GPT-4o (a prompt fine-tuned
pre-trained model), evaluating their effectiveness in video summarization. Our
results demonstrate that current state-of-the-art models perform less
effectively on first-person videos compared to third-person videos,
highlighting the need for further advancements in the egocentric video domain.
Notably, a prompt fine-tuned general-purpose GPT-4o model outperforms these
specialized models, emphasizing the limitations of existing approaches in
adapting to the unique challenges of first-person perspectives. Although our
evaluation is conducted on a small subset of egocentric videos from the
Ego-Exo4D dataset due to resource constraints, the primary objective of this
research is to provide a comprehensive proof-of-concept analysis aimed at
advancing the application of computer vision techniques to first-person videos.
By exploring novel methodologies and evaluating their potential, we aim to
contribute to the ongoing development of models capable of effectively
processing and interpreting egocentric perspectives.

</details>


### [67] [CAT-SG: A Large Dynamic Scene Graph Dataset for Fine-Grained Understanding of Cataract Surgery](https://arxiv.org/abs/2506.21813)
*Felix Holm,Gözde Ünver,Ghazal Ghazaei,Nassir Navab*

Main category: cs.CV

TL;DR: The paper introduces the CAT-SG dataset for cataract surgery, which includes detailed semantic relations to improve surgical phase recognition and technique analysis. A new scene graph generation model, CatSGG, is also presented.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack comprehensive representations that capture the semantic relationships between entities over time in cataract surgery.

Method: A novel scene graph generation model, CatSGG, outperforms current methods.

Result: The CAT-SG dataset provides structured annotations of tool-tissue interactions, procedural variations, and temporal dependencies.

Conclusion: The CAT-SG dataset enhances AI-driven surgical training, real-time decision support, and workflow analysis.

Abstract: Understanding the intricate workflows of cataract surgery requires modeling
complex interactions between surgical tools, anatomical structures, and
procedural techniques. Existing datasets primarily address isolated aspects of
surgical analysis, such as tool detection or phase segmentation, but lack
comprehensive representations that capture the semantic relationships between
entities over time. This paper introduces the Cataract Surgery Scene Graph
(CAT-SG) dataset, the first to provide structured annotations of tool-tissue
interactions, procedural variations, and temporal dependencies. By
incorporating detailed semantic relations, CAT-SG offers a holistic view of
surgical workflows, enabling more accurate recognition of surgical phases and
techniques. Additionally, we present a novel scene graph generation model,
CatSGG, which outperforms current methods in generating structured surgical
representations. The CAT-SG dataset is designed to enhance AI-driven surgical
training, real-time decision support, and workflow analysis, paving the way for
more intelligent, context-aware systems in clinical practice.

</details>


### [68] [Few-Shot Segmentation of Historical Maps via Linear Probing of Vision Foundation Models](https://arxiv.org/abs/2506.21826)
*Rafael Sterzinger,Marco Peer,Robert Sablatnig*

Main category: cs.CV

TL;DR: 提出了一种简单而有效的历史地图少样本分割方法，该方法利用大型视觉基础模型的丰富语义嵌入结合参数高效的微调，在低数据情况下实现了高性能，并减少了手动注释的需求。


<details>
  <summary>Details</summary>
Motivation: 历史地图作为丰富的历史资源，为了解历史变迁提供了重要的见解，但其多样化的视觉表示和有限的注释数据给自动处理带来了巨大的挑战。

Method: 利用大型视觉基础模型的丰富语义嵌入结合参数高效的微调，实现历史地图的少样本分割。

Result: 在 Siegfried 基准数据集的葡萄园和铁路分割任务中，该方法优于现有技术，在 10-shot 场景中实现了 +5% 和 +13% 的 mIoU 相对提升，在更具挑战性的 5-shot 设置中实现了约 +20% 的提升。此外，在 ICDAR 2021 竞赛数据集上表现出强大的性能，在建筑物分割中获得了 67.3% 的平均 PQ。

Conclusion: 该方法即使在极低数据情况下（10-shot 和 5-shot）也能保持高性能，同时仅需要 689k 可训练参数（仅占总模型大小的 0.21%），从而减少了对手动注释的需求，促进了该领域的自动化处理和分析。

Abstract: As rich sources of history, maps provide crucial insights into historical
changes, yet their diverse visual representations and limited annotated data
pose significant challenges for automated processing. We propose a simple yet
effective approach for few-shot segmentation of historical maps, leveraging the
rich semantic embeddings of large vision foundation models combined with
parameter-efficient fine-tuning. Our method outperforms the state-of-the-art on
the Siegfried benchmark dataset in vineyard and railway segmentation, achieving
+5% and +13% relative improvements in mIoU in 10-shot scenarios and around +20%
in the more challenging 5-shot setting. Additionally, it demonstrates strong
performance on the ICDAR 2021 competition dataset, attaining a mean PQ of 67.3%
for building block segmentation, despite not being optimized for this
shape-sensitive metric, underscoring its generalizability. Notably, our
approach maintains high performance even in extremely low-data regimes (10- &
5-shot), while requiring only 689k trainable parameters - just 0.21% of the
total model size. Our approach enables precise segmentation of diverse
historical maps while drastically reducing the need for manual annotations,
advancing automated processing and analysis in the field. Our implementation is
publicly available at:
https://github.com/RafaelSterzinger/few-shot-map-segmentation.

</details>


### [69] [TaleForge: Interactive Multimodal System for Personalized Story Creation](https://arxiv.org/abs/2506.21832)
*Minh-Loi Nguyen,Quang-Khai Le,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: TaleForge is a personalized story-generation system that integrates LLMs and text-to-image diffusion to embed users' facial images within both narratives and illustrations, resulting in heightened engagement and ownership.


<details>
  <summary>Details</summary>
Motivation: existing methods often treat users as passive consumers, offering generic plots with limited personalization. This undermines engagement and immersion, especially where individual style or appearance is crucial

Method: a personalized story-generation system that integrates large language models (LLMs) and text-to-image diffusion to embed users' facial images within both narratives and illustrations. TaleForge features three interconnected modules: Story Generation, Personalized Image Generation, and Background Generation

Result: A user study demonstrated heightened engagement and ownership when individuals appeared as protagonists. Participants praised the system's real-time previews and intuitive controls, though they requested finer narrative editing tools

Conclusion: TaleForge advances multimodal storytelling by aligning personalized text and imagery to create immersive, user-centric experiences.

Abstract: Storytelling is a deeply personal and creative process, yet existing methods
often treat users as passive consumers, offering generic plots with limited
personalization. This undermines engagement and immersion, especially where
individual style or appearance is crucial. We introduce TaleForge, a
personalized story-generation system that integrates large language models
(LLMs) and text-to-image diffusion to embed users' facial images within both
narratives and illustrations. TaleForge features three interconnected modules:
Story Generation, where LLMs create narratives and character descriptions from
user prompts; Personalized Image Generation, merging users' faces and outfit
choices into character illustrations; and Background Generation, creating scene
backdrops that incorporate personalized characters. A user study demonstrated
heightened engagement and ownership when individuals appeared as protagonists.
Participants praised the system's real-time previews and intuitive controls,
though they requested finer narrative editing tools. TaleForge advances
multimodal storytelling by aligning personalized text and imagery to create
immersive, user-centric experiences.

</details>


### [70] [PrefPaint: Enhancing Image Inpainting through Expert Human Feedback](https://arxiv.org/abs/2506.21834)
*Duy-Bao Bui,Hoang-Khang Nguyen,Trung-Nghia Le*

Main category: cs.CV

TL;DR: PrefPaint incorporates human feedback into the training process of Stable Diffusion Inpainting, bypassing the need for computationally expensive reward models, reduces visual inconsistencies and improving image rendering, particularly in medical contexts


<details>
  <summary>Details</summary>
Motivation: inpainting models can generate inaccurate images, leading to significant errors in medical diagnosis and treatment

Method: incorporates human feedback into the training process of Stable Diffusion Inpainting

Result: PrefPaint outperforms existing methods, reducing visual inconsistencies and improving image rendering

Conclusion: PrefPaint outperforms existing methods, reducing visual inconsistencies and improving image rendering, particularly in medical contexts, where our model generates more realistic polyps images.

Abstract: Inpainting, the process of filling missing or corrupted image parts, has
broad applications, including medical imaging. However, in specialized fields
like medical polyps imaging, where accuracy and reliability are critical,
inpainting models can generate inaccurate images, leading to significant errors
in medical diagnosis and treatment. To ensure reliability, medical images
should be annotated by experts like oncologists for effective model training.
We propose PrefPaint, an approach that incorporates human feedback into the
training process of Stable Diffusion Inpainting, bypassing the need for
computationally expensive reward models. In addition, we develop a web-based
interface streamlines training, fine-tuning, and inference. This interactive
interface provides a smooth and intuitive user experience, making it easier to
offer feedback and manage the fine-tuning process. User study on various
domains shows that PrefPaint outperforms existing methods, reducing visual
inconsistencies and improving image rendering, particularly in medical
contexts, where our model generates more realistic polyps images.

</details>


### [71] [ProSAM: Enhancing the Robustness of SAM-based Visual Reference Segmentation with Probabilistic Prompts](https://arxiv.org/abs/2506.21835)
*Xiaoqi Wang,Clint Sebastian,Wenbin He,Liu Ren*

Main category: cs.CV

TL;DR: ProSAM: a simple but effective method to address the stability challenges in existing SAM-based visual reference segmentation approaches.


<details>
  <summary>Details</summary>
Motivation: existing SAM-based methods often generate prompts at object boundaries due to suboptimal prompt encoder, which results in instability and reduced robustness.

Method: learning a variational prompt encoder to predict multivariate prompt distributions

Result: ProSAM avoids generating prompts that lie in unstable regions, overcoming the instability caused by less robust prompts.

Conclusion: ProSAM consistently surpasses state-of-the-art methods on the Pascal-5$^i$ and COCO-20$^i$ datasets, providing a more robust solution for visual reference segmentation.

Abstract: The recent advancements in large foundation models have driven the success of
open-set image segmentation, a task focused on segmenting objects beyond
predefined categories. Among various prompt types (such as points, boxes,
texts, and visual references), visual reference segmentation stands out for its
unique flexibility and strong zero-shot capabilities. Recently, several
SAM-based methods have made notable progress in this task by automatically
generating prompts to guide SAM. However, these methods often generate prompts
at object boundaries due to suboptimal prompt encoder, which results in
instability and reduced robustness. In this work, we introduce ProSAM, a simple
but effective method to address the stability challenges we identified in
existing SAM-based visual reference segmentation approaches. By learning a
variational prompt encoder to predict multivariate prompt distributions, ProSAM
avoids generating prompts that lie in unstable regions, overcoming the
instability caused by less robust prompts. Our approach consistently surpasses
state-of-the-art methods on the Pascal-5$^i$ and COCO-20$^i$ datasets,
providing a more robust solution for visual reference segmentation.

</details>


### [72] [GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles](https://arxiv.org/abs/2506.21839)
*Mengyi Shan,Brian Curless,Ira Kemelmacher-Shlizerman,Steve Seitz*

Main category: cs.CV

TL;DR: This paper introduces a hierarchical multi-agent framework to improve the generation of escape room puzzle images by addressing the limitations of base image models in spatial relationships and affordance reasoning.


<details>
  <summary>Details</summary>
Motivation: challenging text-to-image models with generating escape room puzzle images that are visually appealing, logically solid, and intellectually stimulating. While base image models struggle with spatial relationships and affordance reasoning

Method: a hierarchical multi-agent framework that decomposes this task into structured stages: functional design, symbolic scene graph reasoning, layout synthesis, and local image editing. Specialized agents collaborate through iterative feedback to ensure the scene is visually coherent and functionally solvable.

Result: agent collaboration improves output quality in terms of solvability, shortcut avoidance, and affordance clarity, while maintaining visual quality.

Conclusion: Specialized agents collaborate through iterative feedback to ensure the scene is visually coherent and functionally solvable, improving output quality in terms of solvability, shortcut avoidance, and affordance clarity, while maintaining visual quality.

Abstract: We challenge text-to-image models with generating escape room puzzle images
that are visually appealing, logically solid, and intellectually stimulating.
While base image models struggle with spatial relationships and affordance
reasoning, we propose a hierarchical multi-agent framework that decomposes this
task into structured stages: functional design, symbolic scene graph reasoning,
layout synthesis, and local image editing. Specialized agents collaborate
through iterative feedback to ensure the scene is visually coherent and
functionally solvable. Experiments show that agent collaboration improves
output quality in terms of solvability, shortcut avoidance, and affordance
clarity, while maintaining visual quality.

</details>


### [73] [3D-Telepathy: Reconstructing 3D Objects from EEG Signals](https://arxiv.org/abs/2506.21843)
*Yuxiang Ge,Jionghao Cheng,Ruiquan Ge,Zhaojie Fang,Gangyong Jia,Xiang Wan,Nannan Li,Ahmed Elazab,Changmiao Wang*

Main category: cs.CV

TL;DR: This paper introduces a method to reconstruct 3D objects from EEG data using a novel EEG encoder architecture and Variational Score Distillation, addressing the limitations of 2D image reconstruction in Brain-Computer Interfaces.


<details>
  <summary>Details</summary>
Motivation: Reconstructing 3D visual stimuli from EEG data holds significant potential for Brain-Computer Interfaces (BCIs) and aiding individuals with communication disorders. The neural activities captured by EEG contain rich spatial information that is inevitably lost when reconstructing only 2D images, thus limiting its practical applications in BCI.

Method: An innovative EEG encoder architecture that integrates a dual self-attention mechanism and a hybrid training strategy including cross-attention, contrastive learning, and self-supervised learning techniques.

Result: The study successfully generates 3D objects with similar content and structure from EEG data.

Conclusion: Utilizing Variational Score Distillation to train a neural radiation field, the study successfully generates 3D objects with similar content and structure from EEG data.

Abstract: Reconstructing 3D visual stimuli from Electroencephalography (EEG) data holds
significant potential for applications in Brain-Computer Interfaces (BCIs) and
aiding individuals with communication disorders. Traditionally, efforts have
focused on converting brain activity into 2D images, neglecting the translation
of EEG data into 3D objects. This limitation is noteworthy, as the human brain
inherently processes three-dimensional spatial information regardless of
whether observing 2D images or the real world. The neural activities captured
by EEG contain rich spatial information that is inevitably lost when
reconstructing only 2D images, thus limiting its practical applications in BCI.
The transition from EEG data to 3D object reconstruction faces considerable
obstacles. These include the presence of extensive noise within EEG signals and
a scarcity of datasets that include both EEG and 3D information, which
complicates the extraction process of 3D visual data. Addressing this
challenging task, we propose an innovative EEG encoder architecture that
integrates a dual self-attention mechanism. We use a hybrid training strategy
to train the EEG Encoder, which includes cross-attention, contrastive learning,
and self-supervised learning techniques. Additionally, by employing stable
diffusion as a prior distribution and utilizing Variational Score Distillation
to train a neural radiation field, we successfully generate 3D objects with
similar content and structure from EEG data.

</details>


### [74] [End-to-End RGB-IR Joint Image Compression With Channel-wise Cross-modality Entropy Model](https://arxiv.org/abs/2506.21851)
*Haofeng Wang,Fangtao Zhou,Qi Zhang,Zeyuan Chen,Enci Zhang,Zhao Wang,Xiaofeng Huang,Siwei Ma*

Main category: cs.CV

TL;DR: 提出了一种用于 RGB-IR 图像对的联合压缩框架，该框架优于现有的压缩方法。


<details>
  <summary>Details</summary>
Motivation: RGB-红外 (RGB-Infrared) 图像对经常同时应用于智能监控等各种应用中。然而，随着模态数量的增加，所需的数据存储和传输成本也随之增加一倍。因此，高效的 RGB-IR 数据压缩至关重要。

Method: 提出了一种用于 RGB-IR 图像对的联合压缩框架，具体来说，为了充分利用跨模态先验信息，在模态内部和之间进行精确的上下文概率建模，我们提出了一个通道式跨模态熵模型 (CCEM)。在 CCEM 中，设计了一个低频上下文提取块 (LCEB) 和一个低频上下文融合块 (LCFB)，用于提取和聚合来自两种模态的全局低频信息，这有助于模型更准确地预测熵参数。

Result: 实验结果表明，我们的方法在 LLVIP 和 KAIST 数据集上优于现有的 RGB-IR 图像对和单模态压缩方法。

Conclusion: 该框架在 LLVIP 数据集上实现了 23.1% 的比特率节省，优于 CVPR 2022 上提出的最先进的 RGB-IR 图像编解码器。

Abstract: RGB-IR(RGB-Infrared) image pairs are frequently applied simultaneously in
various applications like intelligent surveillance. However, as the number of
modalities increases, the required data storage and transmission costs also
double. Therefore, efficient RGB-IR data compression is essential. This work
proposes a joint compression framework for RGB-IR image pair. Specifically, to
fully utilize cross-modality prior information for accurate context probability
modeling within and between modalities, we propose a Channel-wise
Cross-modality Entropy Model (CCEM). Among CCEM, a Low-frequency Context
Extraction Block (LCEB) and a Low-frequency Context Fusion Block (LCFB) are
designed for extracting and aggregating the global low-frequency information
from both modalities, which assist the model in predicting entropy parameters
more accurately. Experimental results demonstrate that our approach outperforms
existing RGB-IR image pair and single-modality compression methods on LLVIP and
KAIST datasets. For instance, the proposed framework achieves a 23.1% bit rate
saving on LLVIP dataset compared to the state-of-the-art RGB-IR image codec
presented at CVPR 2022.

</details>


### [75] [Periodic-MAE: Periodic Video Masked Autoencoder for rPPG Estimation](https://arxiv.org/abs/2506.21855)
*Jiho Choi,Sang Jun Lee*

Main category: cs.CV

TL;DR: 该论文提出了一种新的自监督学习框架，用于从面部视频中提取生理信号，并在多个数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 远程光电容积脉搏波(rPPG)估计的关键在于捕获视频中的准周期信号。

Method: 该论文提出了一种通过捕获皮肤颜色随时间变化的细微变化来学习无标记面部视频中周期信号的通用表示的方法。该框架采用视频掩码自动编码器，通过自监督学习来学习面部区域的高维时空表示。为了考虑信号的周期性，该方法应用了视频采样方面的帧掩码，这使得模型能够在预训练阶段捕获重采样的准周期信号。此外，该框架还结合了生理带宽限制，利用生理信号在其频率带宽内是稀疏的特性，为模型提供脉冲线索。预训练的编码器然后被转移到rPPG任务，用于从面部视频中提取生理信号。

Result: 该方法在PURE、UBFC-rPPG、MMPD和V4V数据集上进行了广泛的实验评估，结果表明性能得到了显著提高，特别是在具有挑战性的跨数据集评估中。

Conclusion: 该论文提出的方法在多个数据集上表现出显著的性能提升，特别是在具有挑战性的跨数据集评估中。

Abstract: In this paper, we propose a method that learns a general representation of
periodic signals from unlabeled facial videos by capturing subtle changes in
skin tone over time. The proposed framework employs the video masked
autoencoder to learn a high-dimensional spatio-temporal representation of the
facial region through self-supervised learning. Capturing quasi-periodic
signals in the video is crucial for remote photoplethysmography (rPPG)
estimation. To account for signal periodicity, we apply frame masking in terms
of video sampling, which allows the model to capture resampled quasi-periodic
signals during the pre-training stage. Moreover, the framework incorporates
physiological bandlimit constraints, leveraging the property that physiological
signals are sparse within their frequency bandwidth to provide pulse cues to
the model. The pre-trained encoder is then transferred to the rPPG task, where
it is used to extract physiological signals from facial videos. We evaluate the
proposed method through extensive experiments on the PURE, UBFC-rPPG, MMPD, and
V4V datasets. Our results demonstrate significant performance improvements,
particularly in challenging cross-dataset evaluations. Our code is available at
https://github.com/ziiho08/Periodic-MAE.

</details>


### [76] [SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture of Data Experts for an Expressive Latent Space](https://arxiv.org/abs/2506.21857)
*Ekaterina Redekop,Mara Pleasure,Zichen Wang,Kimberly Flores,Anthony Sisk,William Speier,Corey W. Arnold*

Main category: cs.CV

TL;DR: SPADE是一个基础模型，它集成了组织病理学与ST数据，以指导统一框架内的图像表征学习，通过对比学习来学习共同注册的WSI补丁和基因表达谱的表征, 并在14个下游任务上表现出明显优越的few-shot性能。


<details>
  <summary>Details</summary>
Motivation: 数字病理学的快速发展和自监督深度学习的进步，使得为跨多种疾病的各种病理学任务开发基础模型成为可能。全切片图像（WSI）与空间转录组学（ST）的综合集成仍然存在一个关键的差距，这对于捕获超出标准苏木精和伊红（H＆E）染色的关键分子异质性至关重要。

Method: SPADE利用一种混合数据专家技术，通过两阶段特征空间聚类创建专家，使用对比学习来学习共同注册的WSI补丁和基因表达谱的表征。

Result: SPADE是一种基础模型，它将组织病理学与ST数据集成，以指导统一框架内的图像表征学习，实际上创建了一个ST信息潜在空间。

Conclusion: SPADE在14个下游任务上进行了评估，与基线模型相比，表现出明显优越的few-shot性能，突出了将形态学和分子信息整合到一个潜在空间中的好处。

Abstract: The rapid growth of digital pathology and advances in self-supervised deep
learning have enabled the development of foundational models for various
pathology tasks across diverse diseases. While multimodal approaches
integrating diverse data sources have emerged, a critical gap remains in the
comprehensive integration of whole-slide images (WSIs) with spatial
transcriptomics (ST), which is crucial for capturing critical molecular
heterogeneity beyond standard hematoxylin & eosin (H&E) staining. We introduce
SPADE, a foundation model that integrates histopathology with ST data to guide
image representation learning within a unified framework, in effect creating an
ST-informed latent space. SPADE leverages a mixture-of-data experts technique,
where experts, created via two-stage feature-space clustering, use contrastive
learning to learn representations of co-registered WSI patches and gene
expression profiles. Pre-trained on the comprehensive HEST-1k dataset, SPADE is
evaluated on 14 downstream tasks, demonstrating significantly superior few-shot
performance compared to baseline models, highlighting the benefits of
integrating morphological and molecular information into one latent space.

</details>


### [77] [ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment](https://arxiv.org/abs/2109.05721)
*Yangyu Huang,Hao Yang,Chong Li,Jongyoo Kim,Fangyun Wei*

Main category: cs.CV

TL;DR: 本文研究了人脸对齐中的误差偏差问题，并提出了ADNet，它在三个数据集上实现了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 人脸对齐误差分布存在误差偏差问题，即地标误差的分布倾向于沿着地标曲线的切线方向扩展。这种误差偏差与模糊的地标标注任务密切相关。

Method: 提出了各向异性方向损失（ADL）和各向异性注意力模块（AAM），分别用于坐标和热图回归。

Result: ADL对人脸边界上每个地标点的法线方向施加了很强的约束力。AAM是一个注意力模块，可以获得各向异性的注意力掩码，该掩码关注点及其相邻点连接的局部边缘区域，它在切线方向的响应比在法线方向的响应更强，这意味着在切线方向的约束较少。

Conclusion: ADNet在300W、WFLW和COFW数据集上实现了最先进的结果，证明了其有效性和鲁棒性。

Abstract: The recent progress of CNN has dramatically improved face alignment
performance. However, few works have paid attention to the error-bias with
respect to error distribution of facial landmarks. In this paper, we
investigate the error-bias issue in face alignment, where the distributions of
landmark errors tend to spread along the tangent line to landmark curves. This
error-bias is not trivial since it is closely connected to the ambiguous
landmark labeling task. Inspired by this observation, we seek a way to leverage
the error-bias property for better convergence of CNN model. To this end, we
propose anisotropic direction loss (ADL) and anisotropic attention module (AAM)
for coordinate and heatmap regression, respectively. ADL imposes strong binding
force in normal direction for each landmark point on facial boundaries. On the
other hand, AAM is an attention module which can get anisotropic attention mask
focusing on the region of point and its local edge connected by adjacent
points, it has a stronger response in tangent than in normal, which means
relaxed constraints in the tangent. These two methods work in a complementary
manner to learn both facial structures and texture details. Finally, we
integrate them into an optimized end-to-end training pipeline named ADNet. Our
ADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which
demonstrates the effectiveness and robustness.

</details>


### [78] [LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs](https://arxiv.org/abs/2506.21862)
*Boyuan Sun,Jiaxing Zhao,Xihan Wei,Qibin Hou*

Main category: cs.CV

TL;DR: LLaVA-Scissor 是一种免训练的 token 压缩策略，它使用语义连接组件来压缩视频多模态大型语言模型中的 token，优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 以往的方法大多尝试基于注意力分数来压缩 token，但未能有效捕获所有语义区域，并且经常导致 token 冗余。

Method: 利用语义连接组件 (SCC) 方法，在空间和时间域中进行两步时空 token 压缩。

Result: LLaVA-Scissor 能够有效地压缩 token，用一组不重叠的语义 token 来表示整个视频。

Conclusion: LLaVA-Scissor 在各种视频理解基准测试中优于其他 token 压缩方法，尤其是在低 token 保留率下。

Abstract: In this paper, we present LLaVA-Scissor, a training-free token compression
strategy designed for video multimodal large language models. Previous methods
mostly attempt to compress tokens based on attention scores, but fail to
effectively capture all semantic regions and often lead to token redundancy.
Differently, we propose to leverage the Semantic Connected Components (SCC)
approach that assigns tokens to distinct semantic regions within the token set,
ensuring comprehensive semantic coverage. The outcome is a two-step
spatio-temporal token compression strategy that utilizes SCC in both spatial
and temporal domains. This strategy can effectively compress tokens by
representing the entire video with a set of non-overlapping semantic tokens. We
conduct extensive evaluations of the token compression capabilities of
LLaVA-Scissor across diverse video understanding benchmarks, including video
question answering, long video understanding, and comprehensive multi-choices
benchmarks. Experimental results show that the proposed LLaVA-Scissor
outperforms other token compression methods, achieving superior performance in
various video understanding benchmarks, particularly at low token retention
ratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.

</details>


### [79] [FreeEnricher: Enriching Face Landmarks without Additional Cost](https://arxiv.org/abs/2212.09525)
*Yangyu Huang,Xi Chen,Jongyoo Kim,Hao Yang,Chong Li,Jiaolong Yang,Dong Chen*

Main category: cs.CV

TL;DR: This paper presents a framework that can enrich landmark density by existing sparse landmark datasets. The method achieves state-of-the-art accuracy on both the newly constructed dense 300W testset and the original sparse 300W and WFLW testsets without additional cost.


<details>
  <summary>Details</summary>
Motivation: Most works only consider sparse face alignment, while dense facial landmark is highly demanded in various scenarios.

Method: A weakly-supervised learning approach is proposed to learn the refinement ability on original sparse landmarks and adapt this ability to enriched dense landmarks. Several operators are devised and organized together to implement the idea.

Result: The proposed method yields state-of-the-art accuracy on the newly-constructed dense 300W testset and the original sparse 300W and WFLW testsets.

Conclusion: The proposed method achieves state-of-the-art accuracy on both the newly constructed dense 300W testset and the original sparse 300W and WFLW testsets without additional cost.

Abstract: Recent years have witnessed significant growth of face alignment. Though
dense facial landmark is highly demanded in various scenarios, e.g., cosmetic
medicine and facial beautification, most works only consider sparse face
alignment. To address this problem, we present a framework that can enrich
landmark density by existing sparse landmark datasets, e.g., 300W with 68
points and WFLW with 98 points. Firstly, we observe that the local patches
along each semantic contour are highly similar in appearance. Then, we propose
a weakly-supervised idea of learning the refinement ability on original sparse
landmarks and adapting this ability to enriched dense landmarks. Meanwhile,
several operators are devised and organized together to implement the idea.
Finally, the trained model is applied as a plug-and-play module to the existing
face alignment networks. To evaluate our method, we manually label the dense
landmarks on 300W testset. Our method yields state-of-the-art accuracy not only
in newly-constructed dense 300W testset but also in the original sparse 300W
and WFLW testsets without additional cost.

</details>


### [80] [Remote Sensing Large Vision-Language Model: Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling](https://arxiv.org/abs/2506.21863)
*Sungjune Park,Yeongyun Kim,Se Yeon Kim,Yong Man Ro*

Main category: cs.CV

TL;DR: A new LVLM framework for remote sensing that uses semantic augmentation and expert modeling to improve performance on scene classification and VQA tasks.


<details>
  <summary>Details</summary>
Motivation: The application of Large Vision and Language Models (LVLMs) to remote sensing (RS) remains underexplored due to significant domain differences in visual appearances, object scales, and semantics. These discrepancies hinder the effective understanding of RS scenes, which contain rich, multi-level semantic information spanning from coarse-to-fine levels, limiting the direct adaptation of existing LVLMs to RS imagery.

Method: A novel LVLM framework tailored for RS understanding, incorporating Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling. The Semantic Augmentation Module enriches visual features with relevant semantics across fine-to-coarse levels using a retrieval-based approach. Semantic experts process semantic representation at different levels separately, enabling hierarchical semantic understanding.

Result: The proposed framework achieves consistent improvements across multiple semantic levels in scene classification and VQA tasks.

Conclusion: The proposed framework achieves consistent improvements across multiple semantic levels in remote sensing tasks, highlighting its capability and effectiveness in bridging the gap between general LVLMs and unique demands of RS-specific vision-language understanding.

Abstract: Large Vision and Language Models (LVLMs) have shown strong performance across
various vision-language tasks in natural image domains. However, their
application to remote sensing (RS) remains underexplored due to significant
domain differences in visual appearances, object scales, and semantics. These
discrepancies hider the effective understanding of RS scenes, which contain
rich, multi-level semantic information spanning from coarse-to-fine levels.
Hence, it limits the direct adaptation of existing LVLMs to RS imagery. To
address this gap, we propose a novel LVLM framework tailored for RS
understanding, incorporating two core components: Semantic-augmented
Multi-level Alignment and Semantic-aware Expert Modeling. First, to align
multi-level visual features, we introduce the retrieval-based Semantic
Augmentation Module which enriches the visual features with relevant semantics
across fine-to-coarse levels (e.g., object- and scene-level information). It is
designed to retrieve relevant semantic cues from a RS semantic knowledge
database, followed by aggregation of semantic cues with user query and
multi-level visual features, resulting in semantically enriched representation
across multiple levels. Second, for Semantic-aware Expert Modeling, we design
semantic experts, where each expert is responsible for processing semantic
representation at different levels separately. This enables hierarchical
semantic understanding from coarse to fine levels. Evaluations across multiple
RS tasks-including scene classification and VQA, etc.-demonstrate that the
proposed framework achieves consistent improvements across multiple semantic
levels. This highlights its capability and effectiveness in bridging the gap
between general LVLMs and unique demands of RS-specific vision-language
understanding.

</details>


### [81] [Dual-Perspective United Transformer for Object Segmentation in Optical Remote Sensing Images](https://arxiv.org/abs/2506.21866)
*Yanguang Sun,Jiexi Yan,Jianjun Qian,Chunyan Xu,Jian Yang,Lei Luo*

Main category: cs.CV

TL;DR: 提出了一种新颖的双视角联合Transformer (DPU-Former)，在多个数据集上优于最先进的方法。


<details>
  <summary>Details</summary>
Motivation: 从光学遥感图像(ORSI)中自动分割对象是一项重要的任务。大多数现有的模型主要基于卷积或Transformer特征，每种特征都具有独特的优势。利用这两种优势是有价值的研究，但这带来了一些挑战，包括两种类型特征之间的异质性、高复杂性和模型的巨大参数。然而，这些问题在现有的ORSI方法中经常被忽视，导致次优分割。

Method: 提出了一种新颖的双视角联合Transformer (DPU-Former)，它具有独特的结构，旨在同时整合长距离依赖和空间细节。我们设计了全局-局部混合注意力，它通过两个视角捕获多样化的信息，并引入了傅里叶空间合并策略来避免偏差以实现高效融合。此外，我们还提出了一个门控线性前馈网络来增加表达能力。此外，我们构建了一个DPU-Former解码器来聚合和加强不同层的特征。

Result: DPU-Former模型在多个数据集上优于最先进的方法。

Conclusion: DPU-Former模型在多个数据集上优于最先进的方法。

Abstract: Automatically segmenting objects from optical remote sensing images (ORSIs)
is an important task. Most existing models are primarily based on either
convolutional or Transformer features, each offering distinct advantages.
Exploiting both advantages is valuable research, but it presents several
challenges, including the heterogeneity between the two types of features, high
complexity, and large parameters of the model. However, these issues are often
overlooked in existing the ORSIs methods, causing sub-optimal segmentation. For
that, we propose a novel Dual-Perspective United Transformer (DPU-Former) with
a unique structure designed to simultaneously integrate long-range dependencies
and spatial details. In particular, we design the global-local mixed attention,
which captures diverse information through two perspectives and introduces a
Fourier-space merging strategy to obviate deviations for efficient fusion.
Furthermore, we present a gated linear feed-forward network to increase the
expressive ability. Additionally, we construct a DPU-Former decoder to
aggregate and strength features at different layers. Consequently, the
DPU-Former model outperforms the state-of-the-art methods on multiple datasets.
Code: https://github.com/CSYSI/DPU-Former.

</details>


### [82] [PEACE: Empowering Geologic Map Holistic Understanding with MLLMs](https://arxiv.org/abs/2501.06184)
*Yangyu Huang,Tianyi Gao,Haoran Xu,Qihao Zhao,Yang Song,Zhipeng Gui,Tengchao Lv,Hao Chen,Lei Cui,Scarlett Li,Furu Wei*

Main category: cs.CV

TL;DR: 论文构建了GeoMap-Bench用于评估MLLM在地质地图理解方面的能力，并提出了GeoMap-Agent来解决这个问题，实验结果表明GeoMap-Agent显著优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大型语言模型（MLLM）在地质地图理解方面存在不足，因为制图概括具有挑战性，涉及处理高分辨率地图、管理多个相关组件以及需要领域专业知识。

Method: 引入了GeoMap-Agent，它包含三个模块：分层信息提取（HIE）、领域知识注入（DKI）和提示增强问答（PEQA）。

Result: GeoMap-Agent在GeoMap-Bench上取得了0.811的总体评分，显著优于GPT-4o的0.369。

Conclusion: GeoMap-Agent在GeoMap-Bench上取得了0.811的评分，显著优于GPT-4o的0.369，为地质学中的高级AI应用铺平了道路。

Abstract: Geologic map, as a fundamental diagram in geology science, provides critical
insights into the structure and composition of Earth's subsurface and surface.
These maps are indispensable in various fields, including disaster detection,
resource exploration, and civil engineering. Despite their significance,
current Multimodal Large Language Models (MLLMs) often fall short in geologic
map understanding. This gap is primarily due to the challenging nature of
cartographic generalization, which involves handling high-resolution map,
managing multiple associated components, and requiring domain-specific
knowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever
benchmark for evaluating MLLMs in geologic map understanding, which assesses
the full-scale abilities in extracting, referring, grounding, reasoning, and
analyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent
designed for geologic map understanding, which features three modules:
Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI),
and Prompt-enhanced Question Answering (PEQA). Inspired by the
interdisciplinary collaboration among human scientists, an AI expert group acts
as consultants, utilizing a diverse tool pool to comprehensively analyze
questions. Through comprehensive experiments, GeoMap-Agent achieves an overall
score of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o.
Our work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs,
paves the way for advanced AI applications in geology, enhancing the efficiency
and accuracy of geological investigations.

</details>


### [83] [Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning](https://arxiv.org/abs/2506.21873)
*Tzu-Chun Chien,Chieh-Kai Lin,Shiang-Feng Tsai,Ruei-Chi Lai,Hung-Jen Chen,Min Sun*

Main category: cs.CV

TL;DR: The paper introduces Grounding-Aware Token Pruning (GAP) to address the degradation of grounding ability in MLLMs caused by token pruning. GAP adjusts position IDs and recovers performance without additional overhead.


<details>
  <summary>Details</summary>
Motivation: pruning significantly weakens the model's grounding ability, leading to incorrect predictions and drastic performance degradation

Method: introduce Grounding-Aware Token Pruning (GAP), a simple yet effective adjustment to position IDs

Result: improve performance across various token pruning strategies on models such as Shikra, MiniGPTv2, and the LLaVA series

Conclusion: Grounding-Aware Token Pruning (GAP) can recover REC accuracy back to 51.42%, which is 90% of the original performance in the without pruning setting.

Abstract: Recent Multimodal Large Language Models (MLLMs) have demonstrated strong
performance in visual grounding, establishing themselves as a general interface
for various vision-language applications. This progress has driven the
development of token pruning methods to mitigate the high computational costs
associated with processing numerous visual tokens. However, we observe that
pruning significantly weakens the model's grounding ability, leading to
incorrect predictions and drastic performance degradation. In Referring
Expression Comprehension (REC), for instance, pruning causes the accuracy of
LLaVA on the RefCOCO validation set to drop from 56.14% to 15.34%. Our analysis
identifies misaligned position IDs after pruning as the primary cause of this
degradation, as both the order and value of these IDs are crucial for
maintaining performance in grounding tasks. To address this issue, we propose
Grounding-Aware Token Pruning (GAP), a simple yet effective adjustment to
position IDs that recovers REC accuracy back to 51.42%, which is 90% of the
original performance in the without pruning setting, all while requiring no
additional training, memory, or computational overhead. Applied to models such
as Shikra, MiniGPTv2, and the LLaVA series, our method consistently improves
performance across various token pruning strategies.

</details>


### [84] [GRASP-PsONet: Gradient-based Removal of Spurious Patterns for PsOriasis Severity Classification](https://arxiv.org/abs/2506.21883)
*Basudha Pal,Sharif Amit Kamran,Brendon Lutnick,Molly Lucas,Chaitanya Parmar,Asha Patel Shah,David Apfel,Steven Fakharzadeh,Lloyd Miller,Gabriela Cula,Kristopher Standish*

Main category: cs.CV

TL;DR: 提出了一种自动标记问题图像的方法，以提高银屑病严重程度评分模型的准确性和鲁棒性，并减少人工审核的需求。


<details>
  <summary>Details</summary>
Motivation: 银屑病(PsO)严重程度评分对于临床试验非常重要，但受到评估者间变异性和人工临床评估负担的阻碍。使用患者拍摄的移动照片进行远程成像虽然具有可扩展性，但也带来了一些挑战，例如光照、背景和设备质量的变化。

Method: 提出了一种基于梯度的可解释性方法，自动标记引入虚假相关性的问题训练图像，使用ConvNeXT模型进行图像分类。

Result: 移除8.2%的标记图像后，模型在保留测试集上的AUC-ROC提高了5%(85%到90%)。该方法在评估由两位皮肤科医生评分的训练数据子集时，仅审查了前30%的样本，就识别出超过90%的评估者间不一致的病例。

Conclusion: 通过标记并移除有问题的训练图像，该方法提高了模型的泛化能力和准确性，并能有效检测标注不一致的图像，从而减少对手动审核的需求。

Abstract: Psoriasis (PsO) severity scoring is important for clinical trials but is
hindered by inter-rater variability and the burden of in person clinical
evaluation. Remote imaging using patient captured mobile photos offers
scalability but introduces challenges, such as variation in lighting,
background, and device quality that are often imperceptible to humans but can
impact model performance. These factors, along with inconsistencies in
dermatologist annotations, reduce the reliability of automated severity
scoring. We propose a framework to automatically flag problematic training
images that introduce spurious correlations which degrade model generalization,
using a gradient based interpretability approach. By tracing the gradients of
misclassified validation images, we detect training samples where model errors
align with inconsistently rated examples or are affected by subtle, nonclinical
artifacts. We apply this method to a ConvNeXT based weakly supervised model
designed to classify PsO severity from phone images. Removing 8.2% of flagged
images improves model AUC-ROC by 5% (85% to 90%) on a held out test set.
Commonly, multiple annotators and an adjudication process ensure annotation
accuracy, which is expensive and time consuming. Our method detects training
images with annotation inconsistencies, potentially removing the need for
manual review. When applied to a subset of training data rated by two
dermatologists, the method identifies over 90% of cases with inter-rater
disagreement by reviewing only the top 30% of samples. This improves automated
scoring for remote assessments, ensuring robustness despite data collection
variability.

</details>


### [85] [Integrating Multi-Modal Sensors: A Review of Fusion Techniques for Intelligent Vehicles](https://arxiv.org/abs/2506.21885)
*Chuheng Wei,Ziye Qin,Ziyan Zhang,Guoyuan Wu,Matthew J. Barth*

Main category: cs.CV

TL;DR: systematic review of deep learning-based multi-sensor fusion strategies and datasets in autonomous driving


<details>
  <summary>Details</summary>
Motivation: enhancing perception for autonomous driving, overcoming individual sensor limitations, and enabling comprehensive environmental understanding

Method: systematic review of deep learning-based methods corresponding to each strategy (data-level, feature-level, and decision-level)

Result: We present key multi-modal datasets and discuss their applicability in addressing real-world challenges, particularly in adverse weather conditions and complex urban environments.

Conclusion: This work offers valuable insights into current methods and future directions for multi-sensor fusion in autonomous driving.

Abstract: Multi-sensor fusion plays a critical role in enhancing perception for
autonomous driving, overcoming individual sensor limitations, and enabling
comprehensive environmental understanding. This paper first formalizes
multi-sensor fusion strategies into data-level, feature-level, and
decision-level categories and then provides a systematic review of deep
learning-based methods corresponding to each strategy. We present key
multi-modal datasets and discuss their applicability in addressing real-world
challenges, particularly in adverse weather conditions and complex urban
environments. Additionally, we explore emerging trends, including the
integration of Vision-Language Models (VLMs), Large Language Models (LLMs), and
the role of sensor fusion in end-to-end autonomous driving, highlighting its
potential to enhance system adaptability and robustness. Our work offers
valuable insights into current methods and future directions for multi-sensor
fusion in autonomous driving.

</details>


### [86] [DIVE: Deep-search Iterative Video Exploration A Technical Report for the CVRR Challenge at CVPR 2025](https://arxiv.org/abs/2506.21891)
*Umihiro Kamoto,Tatsuya Ishibashi,Noriyuki Kugo*

Main category: cs.CV

TL;DR: DIVE方法在复杂视频推理与鲁棒性评估挑战赛2025中获得第一名，该方法通过迭代推理和逐步分解问题来生成关于视频片段的准确答案，并在CVRR-ES基准测试中取得了81.44%的准确率。


<details>
  <summary>Details</summary>
Motivation: 为了在复杂视频推理与鲁棒性评估挑战赛2025中生成关于真实世界视频片段问题的准确自然语言答案。

Method: DIVE（深度搜索迭代视频探索）采用迭代推理方法，其中每个输入问题都被语义分解，并通过逐步推理和渐进推理来解决。

Result: DIVE方法在CVRR-ES基准测试的测试集上实现了81.44%的准确率。

Conclusion: DIVE方法在CVRR-ES基准测试中取得了81.44%的准确率，并在所有参与者中排名第一。

Abstract: In this report, we present the winning solution that achieved the 1st place
in the Complex Video Reasoning & Robustness Evaluation Challenge 2025. This
challenge evaluates the ability to generate accurate natural language answers
to questions about diverse, real-world video clips. It uses the Complex Video
Reasoning and Robustness Evaluation Suite (CVRR-ES) benchmark, which consists
of 214 unique videos and 2,400 question-answer pairs spanning 11 categories.
Our method, DIVE (Deep-search Iterative Video Exploration), adopts an iterative
reasoning approach, in which each input question is semantically decomposed and
solved through stepwise reasoning and progressive inference. This enables our
system to provide highly accurate and contextually appropriate answers to even
the most complex queries. Applied to the CVRR-ES benchmark, our approach
achieves 81.44% accuracy on the test set, securing the top position among all
participants. This report details our methodology and provides a comprehensive
analysis of the experimental results, demonstrating the effectiveness of our
iterative reasoning framework in achieving robust video question answering. The
code is available at https://github.com/PanasonicConnect/DIVE

</details>


### [87] [SODA: Out-of-Distribution Detection in Domain-Shifted Point Clouds via Neighborhood Propagation](https://arxiv.org/abs/2506.21892)
*Adam Goodge,Xun Xu,Bryan Hooi,Wee Siong Ng,Jingyi Liao,Yongyi Su,Xulei Yang*

Main category: cs.CV

TL;DR: This paper introduces SODA, a new method for detecting OOD point clouds using 3D VLMs. It addresses the domain shift problem and improves performance without retraining the model.


<details>
  <summary>Details</summary>
Motivation: Detecting out-of-distribution (OOD) point cloud objects is critical for model safety and reliability but remains under-explored. Existing 3D VLMs suffer from a synthetic-to-real domain shift due to limited size and diversity of pre-training datasets.

Method: The paper exploits advances in 3D vision-language models (3D VLMs) for OOD detection and introduces a neighborhood-based score propagation scheme called SODA.

Result: Empirical experiments demonstrate that synthetic-to-real domain shift degrades point cloud and text embedding alignment in 3D VLMs. SODA achieves state-of-the-art performance over existing approaches.

Conclusion: The paper proposes SODA, a novel inference-based methodology that improves OOD point cloud detection through neighborhood-based score propagation, achieving state-of-the-art performance without additional training.

Abstract: As point cloud data increases in prevalence in a variety of applications, the
ability to detect out-of-distribution (OOD) point cloud objects becomes
critical for ensuring model safety and reliability. However, this problem
remains under-explored in existing research. Inspired by success in the image
domain, we propose to exploit advances in 3D vision-language models (3D VLMs)
for OOD detection in point cloud objects. However, a major challenge is that
point cloud datasets used to pre-train 3D VLMs are drastically smaller in size
and object diversity than their image-based counterparts. Critically, they
often contain exclusively computer-designed synthetic objects. This leads to a
substantial domain shift when the model is transferred to practical tasks
involving real objects scanned from the physical environment. In this paper,
our empirical experiments show that synthetic-to-real domain shift
significantly degrades the alignment of point cloud with their associated text
embeddings in the 3D VLM latent space, hindering downstream performance. To
address this, we propose a novel methodology called SODA which improves the
detection of OOD point clouds through a neighborhood-based score propagation
scheme. SODA is inference-based, requires no additional model training, and
achieves state-of-the-art performance over existing approaches across datasets
and problem settings.

</details>


### [88] [Exploring Task-Solving Paradigm for Generalized Cross-Domain Face Anti-Spoofing via Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.21895)
*Fangling Jiang,Qi Li,Weining Wang,Gang Wang,Bing Liu,Zhenan Sun*

Main category: cs.CV

TL;DR: 提出了一种基于强化微调的人脸防欺骗方法，该方法激发了多模态大型语言模型思考和学习如何解决防欺骗任务本身的能力，而不是依赖于对真实性模式的记忆。


<details>
  <summary>Details</summary>
Motivation: 现有方法倾向于记忆训练集中的数据模式，导致对不同场景中未知攻击类型的泛化能力较差，且可解释性有限。

Method: 提出了一种基于强化微调的人脸防欺骗方法，该方法激发了多模态大型语言模型思考和学习如何解决防欺骗任务本身的能力，而不是依赖于对真实性模式的记忆。设计了可验证的类一致奖励和推理一致奖励，并采用基于 GRPO 的优化策略来指导模型从多个角度探索推理策略，以最大限度地提高预期奖励。

Result: 通过迭代试错学习，同时仅保留高奖励轨迹，该模型从广泛的解决方案空间中提炼出高度通用的决策规则，以有效解决跨域人脸防欺骗任务。大量实验结果表明

Conclusion: 该方法在跨域泛化性能方面达到了最先进的水平， 推广到看不见的目标域中各种未知的攻击类型，同时为其真实性决策提供可解释的推理，而无需进行劳动密集型文本注释进行训练。

Abstract: Recently the emergence of novel presentation attacks has drawn increasing
attention to face anti-spoofing. However, existing methods tend to memorize
data patterns from the training set, resulting in poor generalization to
unknown attack types across different scenarios and limited interpretability.
To address these challenges, this paper presents a reinforcement
fine-tuning-based face anti-spoofing method that stimulates the capabilities of
multimodal large language models to think and learn how to solve the
anti-spoofing task itself, rather than relying on the memorization of
authenticity patterns. We design verifiable class consistent reward and
reasoning consistent reward, and employ a GRPO-based optimization strategy to
guide the model in exploring reasoning policies from multiple perspectives to
maximize expected rewards. As a result, through iterative trial-and-error
learning while retaining only high-reward trajectories, the model distills
highly generalizable decision-making rules from the extensive solution space to
effectively address cross-domain face anti-spoofing tasks. Extensive
experimental results demonstrate that our method achieves state-of-the-art
cross-domain generalization performance. It generalizes well to diverse unknown
attack types in unseen target domains while providing interpretable reasoning
for its authenticity decisions without requiring labor-intensive textual
annotations for training.

</details>


### [89] [Visual Content Detection in Educational Videos with Transfer Learning and Dataset Enrichment](https://arxiv.org/abs/2506.21903)
*Dipayan Biswas,Shishir Shah,Jaspal Subhlok*

Main category: cs.CV

TL;DR: 本文提出了一种基于迁移学习的YOLO模型优化方法，用于检测讲座视频中的视觉元素，并发布了一个带注释的数据集和源代码。


<details>
  <summary>Details</summary>
Motivation: 讲座视频中的视觉元素对于理解、记忆和数据呈现至关重要，但其在改善视频内容访问方面的潜力尚未得到充分利用。准确自动检测讲座视频中的视觉元素具有挑战性，因为视觉元素缺乏标准结构，并且缺乏带注释的数据集。

Method: 使用迁移学习方法，优化YOLO模型，通过在多个基准数据集上进行训练并部署半监督自动标注策略。

Result: YOLO模型在该任务中表现出最有希望的结果。论文还发布了一个带注释的讲座视频帧的公开基准，以及源代码，以促进未来的研究。

Conclusion: 该论文提出了一种迁移学习方法，用于检测讲座视频帧中的视觉元素。通过在多个基准数据集上进行训练并部署半监督自动标注策略，优化了YOLO模型以用于讲座视频对象检测。实验结果表明该方法是成功的，并为讲座视频中的对象检测问题提供了一个通用解决方案。

Abstract: Video is transforming education with online courses and recorded lectures
supplementing and replacing classroom teaching. Recent research has focused on
enhancing information retrieval for video lectures with advanced navigation,
searchability, summarization, as well as question answering chatbots. Visual
elements like tables, charts, and illustrations are central to comprehension,
retention, and data presentation in lecture videos, yet their full potential
for improving access to video content remains underutilized. A major factor is
that accurate automatic detection of visual elements in a lecture video is
challenging; reasons include i) most visual elements, such as charts, graphs,
tables, and illustrations, are artificially created and lack any standard
structure, and ii) coherent visual objects may lack clear boundaries and may be
composed of connected text and visual components. Despite advancements in deep
learning based object detection, current models do not yield satisfactory
performance due to the unique nature of visual content in lectures and scarcity
of annotated datasets. This paper reports on a transfer learning approach for
detecting visual elements in lecture video frames. A suite of state of the art
object detection models were evaluated for their performance on lecture video
datasets. YOLO emerged as the most promising model for this task. Subsequently
YOLO was optimized for lecture video object detection with training on multiple
benchmark datasets and deploying a semi-supervised auto labeling strategy.
Results evaluate the success of this approach, also in developing a general
solution to the problem of object detection in lecture videos. Paper
contributions include a publicly released benchmark of annotated lecture video
frames, along with the source code to facilitate future research.

</details>


### [90] [RAUM-Net: Regional Attention and Uncertainty-aware Mamba Network](https://arxiv.org/abs/2506.21905)
*Mingquan Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种半监督的细粒度视觉分类方法，该方法结合了 Mamba 特征建模、区域注意力和贝叶斯不确定性，即使在标记数据有限和存在遮挡的情况下也能实现强大的性能。


<details>
  <summary>Details</summary>
Motivation: 由于细微的类间差异和脆弱的特征表示，细粒度视觉分类（FGVC）仍然是计算机视觉中一项具有挑战性的任务。现有的方法在细粒度场景中表现不佳，尤其是在标记数据稀缺时。

Method: 该论文提出了一种半监督方法，结合了基于 Mamba 的特征建模、区域注意力和贝叶斯不确定性。

Result: 实验表明，该方法在具有遮挡的 FGVC 基准测试中表现出强大的性能，证明了在标记数据有限时具有鲁棒性。

Conclusion: 该论文在有限的标记数据下，在细粒度视觉分类（FGVC）基准测试中表现出强大的性能，尤其是在存在遮挡的情况下，证明了其鲁棒性。

Abstract: Fine Grained Visual Categorization (FGVC) remains a challenging task in
computer vision due to subtle inter class differences and fragile feature
representations. Existing methods struggle in fine grained scenarios,
especially when labeled data is scarce. We propose a semi supervised method
combining Mamba based feature modeling, region attention, and Bayesian
uncertainty. Our approach enhances local to global feature modeling while
focusing on key areas during learning. Bayesian inference selects high quality
pseudo labels for stability. Experiments show strong performance on FGVC
benchmarks with occlusions, demonstrating robustness when labeled data is
limited. Code is available at https://github.com/wxqnl/RAUM Net.

</details>


### [91] [CERBERUS: Crack Evaluation & Recognition Benchmark for Engineering Reliability & Urban Stability](https://arxiv.org/abs/2506.21909)
*Justin Reinman,Sunwoong Choi*

Main category: cs.CV

TL;DR: CERBERUS是一个合成基准，旨在帮助训练和评估AI模型，以检测基础设施中的裂缝和其他缺陷。


<details>
  <summary>Details</summary>
Motivation: 旨在帮助训练和评估用于检测基础设施中裂缝和其他缺陷的 AI 模型。

Method: 使用 Unity 构建的裂缝图像生成器和逼真的 3D 检测场景

Result: 使用不同的合成和真实裂缝数据组合测试了一个流行的目标检测模型 (YOLO)。

Conclusion: 结合合成数据和真实数据可以提高在真实世界图像上的性能。CERBERUS 提供了一种灵活、可重复的方式来测试缺陷检测系统，并支持未来在自动化基础设施检测方面的研究。

Abstract: CERBERUS is a synthetic benchmark designed to help train and evaluate AI
models for detecting cracks and other defects in infrastructure. It includes a
crack image generator and realistic 3D inspection scenarios built in Unity. The
benchmark features two types of setups: a simple Fly-By wall inspection and a
more complex Underpass scene with lighting and geometry challenges. We tested a
popular object detection model (YOLO) using different combinations of synthetic
and real crack data. Results show that combining synthetic and real data
improves performance on real-world images. CERBERUS provides a flexible,
repeatable way to test defect detection systems and supports future research in
automated infrastructure inspection. CERBERUS is publicly available at
https://github.com/justinreinman/Cerberus-Defect-Generator.

</details>


### [92] [Generating Attribute-Aware Human Motions from Textual Prompt](https://arxiv.org/abs/2506.21912)
*Xinghan Wang,Kun Xu,Fei Li,Cao Sheng,Jiazhong Yu,Yadong Mu*

Main category: cs.CV

TL;DR: 提出了一种新的框架，用于生成逼真、属性感知的人类运动，并引入了一个新的数据集来评估该模型。


<details>
  <summary>Details</summary>
Motivation: 当前的方法忽略了人类属性（如年龄、性别、体重和身高）的影响，而这些属性是塑造人类运动模式的关键因素。

Method: 提出了一个受结构因果模型启发的框架，将动作语义与人类属性解耦，从而实现文本到语义的预测和属性控制的生成。

Result: 引入了HumanAttr，这是一个综合数据集，包含文本-运动对的属性注释，为属性感知的文本到运动生成设置了第一个基准。

Conclusion: 该模型能够生成与用户文本和属性输入对齐的逼真、属性感知的运动，并在新数据集上进行了广泛的实验验证了模型的有效性。

Abstract: Text-driven human motion generation has recently attracted considerable
attention, allowing models to generate human motions based on textual
descriptions. However, current methods neglect the influence of human
attributes (such as age, gender, weight, and height) which are key factors
shaping human motion patterns. This work represents a pilot exploration for
bridging this gap. We conceptualize each motion as comprising both attribute
information and action semantics, where textual descriptions align exclusively
with action semantics. To achieve this, a new framework inspired by Structural
Causal Models is proposed to decouple action semantics from human attributes,
enabling text-to-semantics prediction and attribute-controlled generation. The
resulting model is capable of generating realistic, attribute-aware motion
aligned with the user's text and attribute inputs. For evaluation, we introduce
HumanAttr, a comprehensive dataset containing attribute annotations for
text-motion pairs, setting the first benchmark for attribute-aware
text-to-motion generation. Extensive experiments on the new dataset validate
our model's effectiveness.

</details>


### [93] [SepFormer: Coarse-to-fine Separator Regression Network for Table Structure Recognition](https://arxiv.org/abs/2506.21920)
*Nam Quan Nguyen,Xuan Phong Pham,Tuan-Anh Tran*

Main category: cs.CV

TL;DR: SepFormer是一种用于表格结构识别的快速且鲁棒的方法，它使用分离器回归和transformer解码器来预测表格分隔符。


<details>
  <summary>Details</summary>
Motivation: 从图像数据中自动重建表格的逻辑排列（称为表格结构识别（TSR））是语义数据提取的基础。

Method: SepFormer集成了分离-合并范例，通过具有DETR风格架构的分隔符回归，在单步中改进速度和鲁棒性。SepFormer是一个粗到精的方法，通过两个transformer解码器的堆栈，预测从单行到线段分隔符的表格分隔符。

Result: SepFormer在多个基准数据集上实现了与最先进方法相当的性能。

Conclusion: SepFormer在多个基准数据集上实现了与最先进方法相当的性能，平均运行速度为25.6 FPS。

Abstract: The automated reconstruction of the logical arrangement of tables from image
data, termed Table Structure Recognition (TSR), is fundamental for semantic
data extraction. Recently, researchers have explored a wide range of techniques
to tackle this problem, demonstrating significant progress. Each table is a set
of vertical and horizontal separators. Following this realization, we present
SepFormer, which integrates the split-and-merge paradigm into a single step
through separator regression with a DETR-style architecture, improving speed
and robustness. SepFormer is a coarse-to-fine approach that predicts table
separators from single-line to line-strip separators with a stack of two
transformer decoders. In the coarse-grained stage, the model learns to
gradually refine single-line segments through decoder layers with additional
angle loss. At the end of the fine-grained stage, the model predicts line-strip
separators by refining sampled points from each single-line segment. Our
SepFormer can run on average at 25.6 FPS while achieving comparable performance
with state-of-the-art methods on several benchmark datasets, including SciTSR,
PubTabNet, WTW, and iFLYTAB.

</details>


### [94] [ZeroReg3D: A Zero-shot Registration Pipeline for 3D Consecutive Histopathology Image Reconstruction](https://arxiv.org/abs/2506.21923)
*Juming Xiong,Ruining Deng,Jialin Yue,Siqi Lu,Junlin Guo,Marilyn Lionts,Tianyuan Yao,Can Cui,Junchao Zhu,Chongyu Qu,Mengmeng Yin,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: 提出了ZeroReg3D，一种用于组织学切片3D重建的零样本配准流程，无需重新训练或微调即可有效解决各种挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的2D组织学分析方法难以保留关键的3D空间关系，从2D切片构建精确的3D模型仍然具有挑战性，深度学习方法泛化性有限且需要大规模训练数据，非深度学习方法通常在准确性方面妥协。

Method: 结合了基于零样本深度学习的关键点匹配与基于优化的仿射和非刚性配准技术。

Result: 引入了一种新颖的零样本配准流程ZeroReg3D，专为从连续组织学切片进行精确的3D重建而定制。

Conclusion: ZeroReg3D, 有效地解决了组织变形、切片伪影、染色变异和光照不一致等关键挑战，而无需重新训练或微调。

Abstract: Histological analysis plays a crucial role in understanding tissue structure
and pathology. While recent advancements in registration methods have improved
2D histological analysis, they often struggle to preserve critical 3D spatial
relationships, limiting their utility in both clinical and research
applications. Specifically, constructing accurate 3D models from 2D slices
remains challenging due to tissue deformation, sectioning artifacts,
variability in imaging techniques, and inconsistent illumination. Deep
learning-based registration methods have demonstrated improved performance but
suffer from limited generalizability and require large-scale training data. In
contrast, non-deep-learning approaches offer better generalizability but often
compromise on accuracy. In this study, we introduced ZeroReg3D, a novel
zero-shot registration pipeline tailored for accurate 3D reconstruction from
serial histological sections. By combining zero-shot deep learning-based
keypoint matching with optimization-based affine and non-rigid registration
techniques, ZeroReg3D effectively addresses critical challenges such as tissue
deformation, sectioning artifacts, staining variability, and inconsistent
illumination without requiring retraining or fine-tuning. The code has been
made publicly available at https://github.com/hrlblab/ZeroReg3D

</details>


### [95] [SPAZER: Spatial-Semantic Progressive Reasoning Agent for Zero-shot 3D Visual Grounding](https://arxiv.org/abs/2506.21924)
*Zhao Jin,Rong-Cheng Tu,Jingyi Liao,Wenhao Sun,Xiao Luo,Shunyu Liu,Dacheng Tao*

Main category: cs.CV

TL;DR: SPAZER是一个VLM驱动的代理，它结合了3D和2D模态，以实现鲁棒的零样本3D视觉定位，并在ScanRefer和Nr3D上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的范例倾向于强调空间（基于3D）或语义（基于2D）理解，限制了它们在复杂的现实世界应用中的有效性。为了减轻对昂贵的3D训练数据的依赖，最近的研究探索了零样本3DVG，利用了预训练的LLM和VLM的广泛知识和强大的推理能力。

Method: 提出了SPAZER，一个VLM驱动的代理，它在一个渐进式推理框架中结合了两种模式。它首先整体分析场景，并从最佳视点生成3D渲染。在此基础上，进行锚引导的候选筛选，以执行潜在对象的粗略定位。此外，利用检索到的相关2D相机图像，有效地执行3D-2D联合决策，以确定最佳匹配对象。

Result: SPAZER在ScanRefer和Nr3D基准测试中显著优于之前的最先进的零样本方法，准确率分别提高了9.0%和10.9%。

Conclusion: SPAZER通过桥接空间和语义推理神经流，实现了鲁棒的零样本定位，无需在3D标记数据上进行训练，并在ScanRefer和Nr3D基准测试中显著优于之前的最先进的零样本方法，准确率分别提高了9.0%和10.9%。

Abstract: 3D Visual Grounding (3DVG) aims to localize target objects within a 3D scene
based on natural language queries. To alleviate the reliance on costly 3D
training data, recent studies have explored zero-shot 3DVG by leveraging the
extensive knowledge and powerful reasoning capabilities of pre-trained LLMs and
VLMs. However, existing paradigms tend to emphasize either spatial (3D-based)
or semantic (2D-based) understanding, limiting their effectiveness in complex
real-world applications. In this work, we introduce SPAZER - a VLM-driven agent
that combines both modalities in a progressive reasoning framework. It first
holistically analyzes the scene and produces a 3D rendering from the optimal
viewpoint. Based on this, anchor-guided candidate screening is conducted to
perform a coarse-level localization of potential objects. Furthermore,
leveraging retrieved relevant 2D camera images, 3D-2D joint decision-making is
efficiently performed to determine the best-matching object. By bridging
spatial and semantic reasoning neural streams, SPAZER achieves robust zero-shot
grounding without training on 3D-labeled data. Extensive experiments on
ScanRefer and Nr3D benchmarks demonstrate that SPAZER significantly outperforms
previous state-of-the-art zero-shot methods, achieving notable gains of 9.0%
and 10.9% in accuracy.

</details>


### [96] [Quality Assessment and Distortion-aware Saliency Prediction for AI-Generated Omnidirectional Images](https://arxiv.org/abs/2506.21925)
*Liu Yang,Huiyu Duan,Jiarui Wang,Jing Liu,Menghan Hu,Xiongkuo Min,Guangtao Zhai,Patrick Le Callet*

Main category: cs.CV

TL;DR: 研究了 AI 生成全向图像的质量评估和失真感知显着性预测问题，并提出了相应的优化流程。


<details>
  <summary>Details</summary>
Motivation: AI 生成全向图像在虚拟现实和增强现实应用中具有巨大潜力，但对其质量评估和优化研究仍然缺乏。

Method: 提出了两个基于 BLIP-2 模型的共享编码器模型 BLIP2OIQA 和 BLIP2OISal，以评估人类视觉体验并预测 AI 生成全向图像的失真感知显着性。

Result: BLIP2OIQA 和 BLIP2OISal 模型在人类视觉体验评估和失真感知显着性预测任务中取得了最先进的结果，并且可以有效地用于优化过程。

Conclusion: BLIP2OIQA 和 BLIP2OISal 模型在 AI 生成全向图像的人类视觉体验评估和失真感知显着性预测任务中取得了最先进的结果，并且可以有效地用于优化过程。

Abstract: With the rapid advancement of Artificial Intelligence Generated Content
(AIGC) techniques, AI generated images (AIGIs) have attracted widespread
attention, among which AI generated omnidirectional images (AIGODIs) hold
significant potential for Virtual Reality (VR) and Augmented Reality (AR)
applications. AI generated omnidirectional images exhibit unique quality
issues, however, research on the quality assessment and optimization of
AI-generated omnidirectional images is still lacking. To this end, this work
first studies the quality assessment and distortion-aware saliency prediction
problems for AIGODIs, and further presents a corresponding optimization
process. Specifically, we first establish a comprehensive database to reflect
human feedback for AI-generated omnidirectionals, termed OHF2024, which
includes both subjective quality ratings evaluated from three perspectives and
distortion-aware salient regions. Based on the constructed OHF2024 database, we
propose two models with shared encoders based on the BLIP-2 model to evaluate
the human visual experience and predict distortion-aware saliency for
AI-generated omnidirectional images, which are named as BLIP2OIQA and
BLIP2OISal, respectively. Finally, based on the proposed models, we present an
automatic optimization process that utilizes the predicted visual experience
scores and distortion regions to further enhance the visual quality of an
AI-generated omnidirectional image. Extensive experiments show that our
BLIP2OIQA model and BLIP2OISal model achieve state-of-the-art (SOTA) results in
the human visual experience evaluation task and the distortion-aware saliency
prediction task for AI generated omnidirectional images, and can be effectively
used in the optimization process. The database and codes will be released on
https://github.com/IntMeGroup/AIGCOIQA to facilitate future research.

</details>


### [97] [SDRNET: Stacked Deep Residual Network for Accurate Semantic Segmentation of Fine-Resolution Remotely Sensed Images](https://arxiv.org/abs/2506.21945)
*Naftaly Wambugu,Ruisheng Wang,Bo Guo,Tianshu Yu,Sheng Xu,Mohammed Elhassan*

Main category: cs.CV

TL;DR: SDRNet是一种用于FRRS图像语义分割的堆叠深度残差网络，它优于现有的DCNN。


<details>
  <summary>Details</summary>
Motivation: 精确的语义分割受到类差异、遮挡和对象大小变化的影响。从 FRRS 图像中提取足够的特征仍然具有挑战性。更深的网络由于逐渐的下采样过程而显着丢失空间细节，从而导致较差的分割结果和粗糙的边界。

Method: 提出了一个堆叠的深度残差网络 (SDRNet)，用于从 FRRS 图像中进行语义分割。该框架利用两个堆叠的编码器-解码器网络来利用远程语义，同时保留空间信息，并在每个编码器和解码器网络之间使用扩张残差块 (DRB) 来捕获足够的全局依赖性，从而提高分割性能。

Result: 在 ISPRS Vaihingen 和 Potsdam 数据集上获得的结果表明，SDRNet 在语义分割方面表现有效且与当前的 DCNN 相比具有竞争力。

Conclusion: SDRNet在语义分割方面表现出色，优于当前的DCNN。

Abstract: Land cover maps generated from semantic segmentation of high-resolution
remotely sensed images have drawn mucon in the photogrammetry and remote
sensing research community. Currently, massive fine-resolution remotely sensed
(FRRS) images acquired by improving sensing and imaging technologies become
available. However, accurate semantic segmentation of such FRRS images is
greatly affected by substantial class disparities, the invisibility of key
ground objects due to occlusion, and object size variation. Despite the
extraordinary potential in deep convolutional neural networks (DCNNs) in image
feature learning and representation, extracting sufficient features from FRRS
images for accurate semantic segmentation is still challenging. These
challenges demand the deep learning models to learn robust features and
generate sufficient feature descriptors. Specifically, learning
multi-contextual features to guarantee adequate coverage of varied object sizes
from the ground scene and harnessing global-local contexts to overcome class
disparities challenge even profound networks. Deeper networks significantly
lose spatial details due to gradual downsampling processes resulting in poor
segmentation results and coarse boundaries. This article presents a stacked
deep residual network (SDRNet) for semantic segmentation from FRRS images. The
proposed framework utilizes two stacked encoder-decoder networks to harness
long-range semantics yet preserve spatial information and dilated residual
blocks (DRB) between each encoder and decoder network to capture sufficient
global dependencies thus improving segmentation performance. Our experimental
results obtained using the ISPRS Vaihingen and Potsdam datasets demonstrate
that the SDRNet performs effectively and competitively against current DCNNs in
semantic segmentation.

</details>


### [98] [Exploring Semantic Masked Autoencoder for Self-supervised Point Cloud Understanding](https://arxiv.org/abs/2506.21957)
*Yixin Zha,Chuxin Wang,Wenfei Yang,Tianzhu Zhang*

Main category: cs.CV

TL;DR: This paper proposes Semantic Masked Autoencoder to address the issue of failing to capture reasonable semantic relationships by the self-supervised models.


<details>
  <summary>Details</summary>
Motivation: These pre-training methods rely on random masking strategies to establish the perception of point clouds by restoring corrupted point cloud inputs, which leads to the failure of capturing reasonable semantic relationships by the self-supervised models.

Method: Semantic Masked Autoencoder, which comprises two main components: a prototype-based component semantic modeling module and a component semantic-enhanced masking strategy.

Result: the introduction of a component semantic-enhanced prompt-tuning strategy, which further leverages these prototypes to improve the performance of pre-trained models in downstream tasks.

Conclusion: Extensive experiments conducted on datasets such as ScanObjectNN, ModelNet40, and ShapeNetPart demonstrate the effectiveness of our proposed modules.

Abstract: Point cloud understanding aims to acquire robust and general feature
representations from unlabeled data. Masked point modeling-based methods have
recently shown significant performance across various downstream tasks. These
pre-training methods rely on random masking strategies to establish the
perception of point clouds by restoring corrupted point cloud inputs, which
leads to the failure of capturing reasonable semantic relationships by the
self-supervised models. To address this issue, we propose Semantic Masked
Autoencoder, which comprises two main components: a prototype-based component
semantic modeling module and a component semantic-enhanced masking strategy.
Specifically, in the component semantic modeling module, we design a component
semantic guidance mechanism to direct a set of learnable prototypes in
capturing the semantics of different components from objects. Leveraging these
prototypes, we develop a component semantic-enhanced masking strategy that
addresses the limitations of random masking in effectively covering complete
component structures. Furthermore, we introduce a component semantic-enhanced
prompt-tuning strategy, which further leverages these prototypes to improve the
performance of pre-trained models in downstream tasks. Extensive experiments
conducted on datasets such as ScanObjectNN, ModelNet40, and ShapeNetPart
demonstrate the effectiveness of our proposed modules.

</details>


### [99] [TASeg: Text-aware RGB-T Semantic Segmentation based on Fine-tuning Vision Foundation Models](https://arxiv.org/abs/2506.21975)
*Meng Yu,Te Cui,Qitong Chu,Wenjie Song,Yi Yang,Yufeng Yue*

Main category: cs.CV

TL;DR: TASeg: a text-aware RGB-T segmentation framework


<details>
  <summary>Details</summary>
Motivation: Existing RGB-T semantic segmentation models mainly rely on low-level visual features and lack high-level textual information, which struggle with accurate segmentation when categories share similar visual characteristics.  integrating SAM with thermal images and text is hindered by modality heterogeneity and computational inefficiency

Method: a text-aware RGB-T segmentation framework by using Low-Rank Adaptation (LoRA) fine-tuning technology to adapt vision foundation models. Specifically, a Dynamic Feature Fusion Module (DFFM) in the image encoder merges features from multiple visual modalities while freezing SAM's original transformer blocks.  CLIP-generated text embeddings in the mask decoder enable semantic alignment

Result: achieves superior performance in challenging scenarios

Conclusion: achieves superior performance in challenging scenarios with fewer trainable parameters

Abstract: Reliable semantic segmentation of open environments is essential for
intelligent systems, yet significant problems remain: 1) Existing RGB-T
semantic segmentation models mainly rely on low-level visual features and lack
high-level textual information, which struggle with accurate segmentation when
categories share similar visual characteristics. 2) While SAM excels in
instance-level segmentation, integrating it with thermal images and text is
hindered by modality heterogeneity and computational inefficiency. To address
these, we propose TASeg, a text-aware RGB-T segmentation framework by using
Low-Rank Adaptation (LoRA) fine-tuning technology to adapt vision foundation
models. Specifically, we propose a Dynamic Feature Fusion Module (DFFM) in the
image encoder, which effectively merges features from multiple visual
modalities while freezing SAM's original transformer blocks. Additionally, we
incorporate CLIP-generated text embeddings in the mask decoder to enable
semantic alignment, which further rectifies the classification error and
improves the semantic understanding accuracy. Experimental results across
diverse datasets demonstrate that our method achieves superior performance in
challenging scenarios with fewer trainable parameters.

</details>


### [100] [R1-Track: Direct Application of MLLMs to Visual Object Tracking via Reinforcement Learning](https://arxiv.org/abs/2506.21980)
*Biao Wang,Wenwen Li*

Main category: cs.CV

TL;DR: 该论文提出了R1-Track，通过对Qwen2.5-VL进行微调，使其在目标跟踪任务中表现出色，同时保持了通用能力。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉单目标跟踪方法需要显式的分类和回归建模，依赖于大规模数据集的监督训练，并且仅限于跟踪这一单一任务，缺乏灵活性。多模态大型语言模型（MLLMs）近年来发展迅速，但在图像对之间的模板匹配（即跟踪任务）中表现不佳。

Method: 使用群体相对策略优化（GRPO）强化学习方法，在一个小规模数据集上，使用基于规则的奖励函数对Qwen2.5-VL进行微调。

Result: 提出的R1-Track模型支持通过边界框或文本描述进行灵活初始化，并在GOT-10k基准测试上取得了显著的性能。

Conclusion: R1-Track在GOT-10k基准测试上取得了显著的性能，同时保留了原始模型的大部分通用能力，并讨论了R1-Track的潜在改进。

Abstract: Visual single object tracking aims to continuously localize and estimate the
scale of a target in subsequent video frames, given only its initial state in
the first frame. This task has traditionally been framed as a template matching
problem, evolving through major phases including correlation filters,
two-stream networks, and one-stream networks with significant progress
achieved. However, these methods typically require explicit classification and
regression modeling, depend on supervised training with large-scale datasets,
and are limited to the single task of tracking, lacking flexibility. In recent
years, multi-modal large language models (MLLMs) have advanced rapidly.
Open-source models like Qwen2.5-VL, a flagship MLLMs with strong foundational
capabilities, demonstrate excellent performance in grounding tasks. This has
spurred interest in applying such models directly to visual tracking. However,
experiments reveal that Qwen2.5-VL struggles with template matching between
image pairs (i.e., tracking tasks). Inspired by deepseek-R1, we fine-tuned
Qwen2.5-VL using the group relative policy optimization (GRPO) reinforcement
learning method on a small-scale dataset with a rule-based reward function. The
resulting model, R1-Track, achieved notable performance on the GOT-10k
benchmark. R1-Track supports flexible initialization via bounding boxes or text
descriptions while retaining most of the original model's general capabilities.
And we further discuss potential improvements for R1-Track. This rough
technical report summarizes our findings as of May 2025.

</details>


### [101] [RoboEnvision: A Long-Horizon Video Generation Model for Multi-Task Robot Manipulation](https://arxiv.org/abs/2506.22007)
*Liudi Yang,Yang Bai,George Eskandar,Fengyi Shen,Mohammad Altillawi,Dong Chen,Soumajit Majumder,Ziyuan Liu,Gitta Kutyniok,Abhinav Valada*

Main category: cs.CV

TL;DR: This paper introduces a new method for generating long-horizon videos for robotic manipulation tasks by decomposing goals into atomic tasks, generating keyframes, and using a diffusion model to interpolate between them, outperforming existing methods in video quality, consistency, and policy model performance.


<details>
  <summary>Details</summary>
Motivation: Text-to-video diffusion models have made significant progress in photorealism, language understanding, and motion generation but struggle with long-horizon robotic tasks. Recent works use video diffusion models for high-quality simulation data and predictive rollouts in robot planning. However, these works predict short sequences of the robot achieving one task and employ an autoregressive paradigm to extend to the long horizon, leading to error accumulations in the generated video and in the execution.

Method: we first decompose the high-level goals into smaller atomic tasks and generate keyframes aligned with these instructions. A second diffusion model then interpolates between each of the two generated frames, achieving the long-horizon video. 2) We propose a semantics preserving attention module to maintain consistency between the keyframes. 3) We design a lightweight policy model to regress the robot joint states from generated videos.

Result: achieves state-of-the-art results on two benchmarks in video quality and consistency while outperforming previous policy models on long-horizon tasks.

Conclusion: Our approach achieves state-of-the-art results on two benchmarks in video quality and consistency while outperforming previous policy models on long-horizon tasks.

Abstract: We address the problem of generating long-horizon videos for robotic
manipulation tasks. Text-to-video diffusion models have made significant
progress in photorealism, language understanding, and motion generation but
struggle with long-horizon robotic tasks. Recent works use video diffusion
models for high-quality simulation data and predictive rollouts in robot
planning. However, these works predict short sequences of the robot achieving
one task and employ an autoregressive paradigm to extend to the long horizon,
leading to error accumulations in the generated video and in the execution. To
overcome these limitations, we propose a novel pipeline that bypasses the need
for autoregressive generation. We achieve this through a threefold
contribution: 1) we first decompose the high-level goals into smaller atomic
tasks and generate keyframes aligned with these instructions. A second
diffusion model then interpolates between each of the two generated frames,
achieving the long-horizon video. 2) We propose a semantics preserving
attention module to maintain consistency between the keyframes. 3) We design a
lightweight policy model to regress the robot joint states from generated
videos. Our approach achieves state-of-the-art results on two benchmarks in
video quality and consistency while outperforming previous policy models on
long-horizon tasks.

</details>


### [102] [Towards Universal & Efficient Model Compression via Exponential Torque Pruning](https://arxiv.org/abs/2506.22015)
*Sarthak Ketanbhai Modi,Lim Zi Pong,Shourya Kuchhal,Yoshi Cao,Yupeng Cheng,Teo Yon Shin,Lin Shang-Wei,Zhiming Li*

Main category: cs.CV

TL;DR: This paper proposes Exponential Torque Pruning (ETP) to efficiently prune redundant neural modules and achieve higher compression rates with negligible accuracy drop compared to previous methods.


<details>
  <summary>Details</summary>
Motivation: The pruning effect of Torque-inspired regularization is far from perfect, as the post-trained network is still dense and also suffers from high accuracy drop, and the default linear force application scheme imposes inappropriate force on neural module of different distances.

Method: Exponential Torque Pruning (ETP), which adopts an exponential force application scheme for regularization

Result: ETP manages to achieve significantly higher compression rate than the previous state-of-the-art pruning strategies with negligible accuracy drop.

Conclusion: ETP achieves higher compression rate than previous state-of-the-art pruning strategies with negligible accuracy drop.

Abstract: The rapid growth in complexity and size of modern deep neural networks (DNNs)
has increased challenges related to computational costs and memory usage,
spurring a growing interest in efficient model compression techniques. Previous
state-of-the-art approach proposes using a Torque-inspired regularization which
forces the weights of neural modules around a selected pivot point. Whereas, we
observe that the pruning effect of this approach is far from perfect, as the
post-trained network is still dense and also suffers from high accuracy drop.
In this work, we attribute such ineffectiveness to the default linear force
application scheme, which imposes inappropriate force on neural module of
different distances. To efficiently prune the redundant and distant modules
while retaining those that are close and necessary for effective inference, in
this work, we propose Exponential Torque Pruning (ETP), which adopts an
exponential force application scheme for regularization. Experimental results
on a broad range of domains demonstrate that, though being extremely simple,
ETP manages to achieve significantly higher compression rate than the previous
state-of-the-art pruning strategies with negligible accuracy drop.

</details>


### [103] [Advancing Facial Stylization through Semantic Preservation Constraint and Pseudo-Paired Supervision](https://arxiv.org/abs/2506.22022)
*Zhanyi Lu,Yue Zhou*

Main category: cs.CV

TL;DR: Proposes a facial stylization method using semantic preservation and pseudo-paired supervision to improve content correspondence and stylization effect, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Previous StyleGAN-based methods suffer from artifacts or insufficient fidelity to the source image due to neglecting semantic shift of the generator during stylization.

Method: A facial stylization method that integrates semantic preservation constraint and pseudo-paired supervision.

Result: Achieves more flexible multimodal and reference-guided stylization without complex network architecture designs or additional training. Demonstrates high-fidelity, aesthetically pleasing facial style transfer.

Conclusion: The proposed approach produces high-fidelity, aesthetically pleasing facial style transfer that surpasses previous methods.

Abstract: Facial stylization aims to transform facial images into appealing,
high-quality stylized portraits, with the critical challenge of accurately
learning the target style while maintaining content consistency with the
original image. Although previous StyleGAN-based methods have made significant
advancements, the generated results still suffer from artifacts or insufficient
fidelity to the source image. We argue that these issues stem from neglecting
semantic shift of the generator during stylization. Therefore, we propose a
facial stylization method that integrates semantic preservation constraint and
pseudo-paired supervision to enhance the content correspondence and improve the
stylization effect. Additionally, we develop a methodology for creating
multi-level pseudo-paired datasets to implement supervisory constraint.
Furthermore, building upon our facial stylization framework, we achieve more
flexible multimodal and reference-guided stylization without complex network
architecture designs or additional training. Experimental results demonstrate
that our approach produces high-fidelity, aesthetically pleasing facial style
transfer that surpasses previous methods.

</details>


### [104] [Cross-modal Ship Re-Identification via Optical and SAR Imagery: A Novel Dataset and Method](https://arxiv.org/abs/2506.22027)
*Han Wang,Shengyang Li,Jian Yang,Yuxuan Liu,Yixuan Lv,Zhuang Zhou*

Main category: cs.CV

TL;DR: Introduces HOSS ReID dataset and TransOSS method for improved ship tracking using optical and SAR satellite imagery.


<details>
  <summary>Details</summary>
Motivation: Current ship tracking methods using geostationary or video satellites have limitations in resolution, weather dependency, filming duration, and coverage area, making them unsuitable for real-world ship tracking requirements.

Method: The paper proposes a baseline method called TransOSS, built on the Vision Transformer architecture, which refines the patch embedding structure, incorporates additional embeddings, and employs contrastive learning.

Result: The paper presents the HOSS ReID dataset and the TransOSS baseline method, both of which are publicly available.

Conclusion: The paper introduces a new dataset (HOSS ReID) and a baseline method (TransOSS) for cross-modal ship re-identification, demonstrating a solution for ship tracking using low-Earth orbit constellations of optical and SAR sensors.

Abstract: Detecting and tracking ground objects using earth observation imagery remains
a significant challenge in the field of remote sensing. Continuous maritime
ship tracking is crucial for applications such as maritime search and rescue,
law enforcement, and shipping analysis. However, most current ship tracking
methods rely on geostationary satellites or video satellites. The former offer
low resolution and are susceptible to weather conditions, while the latter have
short filming durations and limited coverage areas, making them less suitable
for the real-world requirements of ship tracking. To address these limitations,
we present the Hybrid Optical and Synthetic Aperture Radar (SAR) Ship
Re-Identification Dataset (HOSS ReID dataset), designed to evaluate the
effectiveness of ship tracking using low-Earth orbit constellations of optical
and SAR sensors. This approach ensures shorter re-imaging cycles and enables
all-weather tracking. HOSS ReID dataset includes images of the same ship
captured over extended periods under diverse conditions, using different
satellites of different modalities at varying times and angles. Furthermore, we
propose a baseline method for cross-modal ship re-identification, TransOSS,
which is built on the Vision Transformer architecture. It refines the patch
embedding structure to better accommodate cross-modal tasks, incorporates
additional embeddings to introduce more reference information, and employs
contrastive learning to pre-train on large-scale optical-SAR image pairs,
ensuring the model's ability to extract modality-invariant features. Our
dataset and baseline method are publicly available on
https://github.com/Alioth2000/Hoss-ReID.

</details>


### [105] [Partial CLIP is Enough: Chimera-Seg for Zero-shot Semantic Segmentation](https://arxiv.org/abs/2506.22032)
*Jialei Chen,Xu Zheng,Danda Pani Paudel,Luc Van Gool,Hiroshi Murase,Daisuke Deguchi*

Main category: cs.CV

TL;DR: propose Chimera-Seg and Selective Global Distillation (SGD) to improve zero-shot semantic segmentation (ZSS).


<details>
  <summary>Details</summary>
Motivation: Knowledge transfer remains challenging due to: (1) the difficulty of aligning vision-based features with the textual space, which requires combining spatial precision with vision-language alignment; and (2) the semantic gap between CLIP's global representations and the local, fine-grained features of segmentation models.

Method: propose Chimera-Seg, which integrates a segmentation backbone as the body and a CLIP-based semantic head as the head and Selective Global Distillation (SGD), which distills knowledge from dense features exhibiting high similarity to the CLIP CLS token, while gradually reducing the number of features used for alignment as training progresses. Besides, we also use a Semantic Alignment Module (SAM) to further align dense visual features with semantic embeddings extracted from the frozen CLIP text encoder.

Result: Experiments on two benchmarks show improvements of 0.9% and 1.2% in hIoU.

Conclusion: Experiments on two benchmarks show improvements of 0.9% and 1.2% in hIoU.

Abstract: Zero-shot Semantic Segmentation (ZSS) aims to segment both seen and unseen
classes using supervision from only seen classes. Beyond adaptation-based
methods, distillation-based approaches transfer vision-language alignment of
vision-language model, e.g., CLIP, to segmentation models. However, such
knowledge transfer remains challenging due to: (1) the difficulty of aligning
vision-based features with the textual space, which requires combining spatial
precision with vision-language alignment; and (2) the semantic gap between
CLIP's global representations and the local, fine-grained features of
segmentation models. To address challenge (1), we propose Chimera-Seg, which
integrates a segmentation backbone as the body and a CLIP-based semantic head
as the head, like the Chimera in Greek mythology, combining spatial precision
with vision-language alignment. Specifically, Chimera-Seg comprises a trainable
segmentation model and a CLIP Semantic Head (CSH), which maps dense features
into the CLIP-aligned space. The CSH incorporates a frozen subnetwork and fixed
projection layers from the CLIP visual encoder, along with lightweight
trainable components. The partial module from CLIP visual encoder, paired with
the segmentation model, retains segmentation capability while easing the
mapping to CLIP's semantic space. To address challenge (2), we propose
Selective Global Distillation (SGD), which distills knowledge from dense
features exhibiting high similarity to the CLIP CLS token, while gradually
reducing the number of features used for alignment as training progresses.
Besides, we also use a Semantic Alignment Module (SAM) to further align dense
visual features with semantic embeddings extracted from the frozen CLIP text
encoder. Experiments on two benchmarks show improvements of 0.9% and 1.2% in
hIoU.

</details>


### [106] [Few-Shot Identity Adaptation for 3D Talking Heads via Global Gaussian Field](https://arxiv.org/abs/2506.22044)
*Hong Nie,Fuyuan Cao,Lu Chen,Fengxin Chen,Yuefeng Zou,Jun Yu*

Main category: cs.CV

TL;DR: FIAG 是一种新的 3D 说话头像合成框架，它可以使用少量训练镜头实现有效的特定于身份的适应。


<details>
  <summary>Details</summary>
Motivation: 基于重建和渲染的说话头像合成方法虽然实现了高质量的结果和强大的身份保持，但受到其对特定于身份的模型的依赖性的限制。与基于生成模型的方法相比，每个新身份都需要从头开始训练，从而导致高计算成本和降低的可扩展性。

Method: 提出了一种新的 3D 说话头像合成框架 FIAG，该框架支持在共享场中表示多个身份的全局高斯场和捕获不同身份的通用运动动态的通用运动场。

Result: 该框架能够以最少的数据从规范身份表示快速适应到特定身份表示。

Conclusion: 该方法优于现有技术水平的方法，验证了所提出框架的有效性和泛化性。

Abstract: Reconstruction and rendering-based talking head synthesis methods achieve
high-quality results with strong identity preservation but are limited by their
dependence on identity-specific models. Each new identity requires training
from scratch, incurring high computational costs and reduced scalability
compared to generative model-based approaches. To overcome this limitation, we
propose FIAG, a novel 3D speaking head synthesis framework that enables
efficient identity-specific adaptation using only a few training footage. FIAG
incorporates Global Gaussian Field, which supports the representation of
multiple identities within a shared field, and Universal Motion Field, which
captures the common motion dynamics across diverse identities. Benefiting from
the shared facial structure information encoded in the Global Gaussian Field
and the general motion priors learned in the motion field, our framework
enables rapid adaptation from canonical identity representations to specific
ones with minimal data. Extensive comparative and ablation experiments
demonstrate that our method outperforms existing state-of-the-art approaches,
validating both the effectiveness and generalizability of the proposed
framework. Code is available at: \textit{https://github.com/gme-hong/FIAG}.

</details>


### [107] [EnLVAM: Enhanced Left Ventricle Linear Measurements Utilizing Anatomical Motion Mode](https://arxiv.org/abs/2506.22063)
*Durgesh K. Singh,Ahcene Boubekki,Qing Cao,Svein Arne Aase,Robert Jenssen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: 该论文提出了一种半自动框架，通过在AMM图像上训练landmark检测器并强制直线约束来提高左心室测量的准确性。


<details>
  <summary>Details</summary>
Motivation: 手动放置左心室测量中的地标耗时且容易出错，而现有的深度学习方法经常错位地标，导致测量不准确。

Method: 该方法在从B超视频实时计算出的解剖M型图像（AMM）上训练landmark检测器，然后转换回B超空间。

Result: 实验表明，与标准B超方法相比，该方法提高了准确性。

Conclusion: 该论文提出了一种新颖的框架，通过强制直线约束来提高左心室测量的准确性，并在不同的网络架构中表现出良好的泛化能力。

Abstract: Linear measurements of the left ventricle (LV) in the Parasternal Long Axis
(PLAX) view using B-mode echocardiography are crucial for cardiac assessment.
These involve placing 4-6 landmarks along a virtual scanline (SL) perpendicular
to the LV axis near the mitral valve tips. Manual placement is time-consuming
and error-prone, while existing deep learning methods often misalign landmarks,
causing inaccurate measurements. We propose a novel framework that enhances LV
measurement accuracy by enforcing straight-line constraints. A landmark
detector is trained on Anatomical M-Mode (AMM) images, computed in real time
from B-mode videos, then transformed back to B-mode space. This approach
addresses misalignment and reduces measurement errors. Experiments show
improved accuracy over standard B-mode methods, and the framework generalizes
well across network architectures. Our semi-automatic design includes a
human-in-the-loop step where the user only places the SL, simplifying
interaction while preserving alignment flexibility and clinical relevance.

</details>


### [108] [MirrorMe: Towards Realtime and High Fidelity Audio-Driven Halfbody Animation](https://arxiv.org/abs/2506.22065)
*Dechao Meng,Steven Xiao,Xindi Zhang,Guangyuan Wang,Peng Zhang,Qi Wang,Bang Zhang,Liefeng Bo*

Main category: cs.CV

TL;DR: MirrorMe是一种用于实时音频驱动人像动画的框架，它优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 音频驱动的人像动画在实时生成高保真、时间连贯的动画方面面临重大挑战。最近基于扩散的方法提高了生成质量，但它们对逐帧UNet架构的依赖引入了过高的延迟，并且难以保证时间一致性。

Method: MirrorMe建立在LTX视频模型之上，并通过VAE编码图像连接和自注意力机制、因果音频编码器和适配器以及渐进式训练策略进行改进。

Result: MirrorMe在EMTD基准测试中表现出最先进的性能。

Conclusion: MirrorMe在保真度、唇形同步准确性和时间稳定性方面表现出最先进的性能。

Abstract: Audio-driven portrait animation, which synthesizes realistic videos from
reference images using audio signals, faces significant challenges in real-time
generation of high-fidelity, temporally coherent animations. While recent
diffusion-based methods improve generation quality by integrating audio into
denoising processes, their reliance on frame-by-frame UNet architectures
introduces prohibitive latency and struggles with temporal consistency. This
paper introduces MirrorMe, a real-time, controllable framework built on the LTX
video model, a diffusion transformer that compresses video spatially and
temporally for efficient latent space denoising. To address LTX's trade-offs
between compression and semantic fidelity, we propose three innovations: 1. A
reference identity injection mechanism via VAE-encoded image concatenation and
self-attention, ensuring identity consistency; 2. A causal audio encoder and
adapter tailored to LTX's temporal structure, enabling precise audio-expression
synchronization; and 3. A progressive training strategy combining close-up
facial training, half-body synthesis with facial masking, and hand pose
integration for enhanced gesture control. Extensive experiments on the EMTD
Benchmark demonstrate MirrorMe's state-of-the-art performance in fidelity,
lip-sync accuracy, and temporal stability.

</details>


### [109] [Single-Scanline Relative Pose Estimation for Rolling Shutter Cameras](https://arxiv.org/abs/2506.22069)
*Petr Hruby,Marc Pollefeys*

Main category: cs.CV

TL;DR: A novel approach for estimating the relative pose between rolling shutter cameras is proposed, which allows pose estimation without explicitly modeling camera motion and enables single-view relative pose estimation for scanlines of rolling shutter cameras.


<details>
  <summary>Details</summary>
Motivation: This allows pose estimation without explicitly modeling camera motion.

Method: a novel approach for estimating the relative pose between rolling shutter cameras using the intersections of line projections with a single scanline per image

Result: scanlines can be selected within a single image, enabling single-view relative pose estimation for scanlines of rolling shutter cameras

Conclusion: The feasibility of our approach for initializing rolling shutter SfM is demonstrated, highlighting its potential for further development.

Abstract: We propose a novel approach for estimating the relative pose between rolling
shutter cameras using the intersections of line projections with a single
scanline per image. This allows pose estimation without explicitly modeling
camera motion. Alternatively, scanlines can be selected within a single image,
enabling single-view relative pose estimation for scanlines of rolling shutter
cameras. Our approach is designed as a foundational building block for rolling
shutter structure-from-motion (SfM), where no motion model is required, and
each scanline's pose can be computed independently. % We classify minimal
solvers for this problem in both generic and specialized settings, including
cases with parallel lines and known gravity direction, assuming known
intrinsics and no lens distortion. Furthermore, we develop minimal solvers for
the parallel-lines scenario, both with and without gravity priors, by
leveraging connections between this problem and the estimation of 2D structure
from 1D cameras. % Experiments on rolling shutter images from the Fastec
dataset demonstrate the feasibility of our approach for initializing rolling
shutter SfM, highlighting its potential for further development. % The code
will be made publicly available.

</details>


### [110] [Reasoning in machine vision: learning to think fast and slow](https://arxiv.org/abs/2506.22075)
*Shaheer U. Saeed,Yipei Wang,Veeru Kasivisvanathan,Brian R. Davidson,Matthew J. Clarkson,Yipeng Hu,Daniel C. Alexander*

Main category: cs.CV

TL;DR: 该论文提出了一种新的学习范式，通过模仿人类的思考方式，在视觉任务中实现了机器推理，并在性能上优于现有方法，甚至人类专家。


<details>
  <summary>Details</summary>
Motivation: 机器智能仍然受限于训练数据，缺乏在推理时动态改进解决方案的能力。虽然最近的一些进展探索了机器推理，但这些努力主要限于口头领域，如数学问题解决，其中明确的规则支配着逐步推理。其他关键的现实世界任务——包括视觉感知、空间推理和放射诊断——需要非语言推理，这仍然是一个开放的挑战。

Method: 该方法整合了一个用于熟悉任务的快速思考系统 I 模块，以及一个使用自博弈强化学习迭代改进解决方案的慢速思考系统 II 模块。这种范式通过在数据稀缺的情况下提出、竞争和改进解决方案来模仿人类推理。

Result: 该论文提出了一种新的学习范式，通过允许随着思考时间（推理时计算）的增加而提高性能，即使在标记数据非常有限的情况下，也能实现视觉中的机器推理。

Conclusion: 该论文展示了在真实世界视觉任务中，通过延长思考时间，该方法在性能上优于大规模监督学习、基础模型，甚至人类专家。这些任务包括计算机视觉基准测试和医学图像上的癌症定位，展示了非语言机器推理的变革潜力。

Abstract: Reasoning is a hallmark of human intelligence, enabling adaptive
decision-making in complex and unfamiliar scenarios. In contrast, machine
intelligence remains bound to training data, lacking the ability to dynamically
refine solutions at inference time. While some recent advances have explored
reasoning in machines, these efforts are largely limited to verbal domains such
as mathematical problem-solving, where explicit rules govern step-by-step
reasoning. Other critical real-world tasks - including visual perception,
spatial reasoning, and radiological diagnosis - require non-verbal reasoning,
which remains an open challenge. Here we present a novel learning paradigm that
enables machine reasoning in vision by allowing performance improvement with
increasing thinking time (inference-time compute), even under conditions where
labelled data is very limited. Inspired by dual-process theories of human
cognition in psychology, our approach integrates a fast-thinking System I
module for familiar tasks, with a slow-thinking System II module that
iteratively refines solutions using self-play reinforcement learning. This
paradigm mimics human reasoning by proposing, competing over, and refining
solutions in data-scarce scenarios. We demonstrate superior performance through
extended thinking time, compared not only to large-scale supervised learning
but also foundation models and even human experts, in real-world vision tasks.
These tasks include computer-vision benchmarks and cancer localisation on
medical images across five organs, showcasing transformative potential for
non-verbal machine reasoning.

</details>


### [111] [Towards Accurate Heart Rate Measurement from Ultra-Short Video Clips via Periodicity-Guided rPPG Estimation and Signal Reconstruction](https://arxiv.org/abs/2506.22078)
*Pei-Kai Huanga,Ya-Ting Chan,Kuan-Wen Chen,Yen-Chun Chou,Shih-Yu Yang,Chiou-Ting Hsu*

Main category: cs.CV

TL;DR: Accurately measures HR from 2-second video clips using periodicity-guided rPPG estimation and signal reconstruction, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To accurately measure HR from ultra-short 2-second video clips by addressing the limited number of heartbeat cycles and mitigating estimation inaccuracies due to spectral leakage.

Method: An effective periodicity-guided rPPG estimation method and a generator to reconstruct longer rPPG signals from ultra-short ones.

Result: Achieves state-of-the-art performance on four rPPG estimation benchmark datasets.

Conclusion: The proposed method accurately measures HR from ultra-short video clips and outperforms previous rPPG estimation techniques.

Abstract: Many remote Heart Rate (HR) measurement methods focus on estimating remote
photoplethysmography (rPPG) signals from video clips lasting around 10 seconds
but often overlook the need for HR estimation from ultra-short video clips. In
this paper, we aim to accurately measure HR from ultra-short 2-second video
clips by specifically addressing two key challenges. First, to overcome the
limited number of heartbeat cycles in ultra-short video clips, we propose an
effective periodicity-guided rPPG estimation method that enforces consistent
periodicity between rPPG signals estimated from ultra-short clips and their
much longer ground truth signals. Next, to mitigate estimation inaccuracies due
to spectral leakage, we propose including a generator to reconstruct longer
rPPG signals from ultra-short ones while preserving their periodic consistency
to enable more accurate HR measurement. Extensive experiments on four rPPG
estimation benchmark datasets demonstrate that our proposed method not only
accurately measures HR from ultra-short video clips but also outperform
previous rPPG estimation techniques to achieve state-of-the-art performance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [112] [SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents](https://arxiv.org/abs/2506.21669)
*Wanxin Tian,Shijie Zhang,Kevin Zhang,Xiaowei Chi,Yulin Luo,Junyu Lu,Chunkai Fan,Qiang Zhou,Yiming Zhao,Ning Liu Siyu Lin,Zhiyuan Qin,Xiaozhu Ju,Shanghang Zhang,Jian Tang*

Main category: cs.AI

TL;DR: 提出了SEEA-R1框架，通过Tree-GRPO和MGRM解决了强化微调在具身智能体自进化中的问题，并在ALFWorld基准测试中取得了优异的成果。


<details>
  <summary>Details</summary>
Motivation: 当前的强化微调(RFT)在增强LLM的推理能力方面表现出强大的性能，但其在具有多模态交互的自进化具身智能方面的潜力仍未被探索。RFT在具身环境中面临两个根本障碍：(i)多步骤推理任务中缺乏可访问的中间奖励限制了有效的学习信号，以及(ii)依赖于手工设计的奖励函数限制了对新任务和环境的泛化。

Method: 提出了Tree-GRPO和MGRM来解决RFT在具身环境中的挑战。

Result: SEEA-R1在ALFWorld基准测试中超越了现有技术水平，文本得分为85.07%，多模态得分为36.19%，超过了包括GPT-4o在内的先前模型。SEEA-R1在没有环境奖励的情况下也获得了80.3%的分数，超过了所有开源基线。

Conclusion: SEEA-R1在ALFWorld基准测试中超越了最先进的方法，在没有环境奖励的情况下也取得了很高的分数，展示了其作为自进化具身智能体的潜力。

Abstract: Self-evolution, the ability of agents to autonomously improve their reasoning
and behavior, is essential for the embodied domain with long-horizon,
real-world tasks. Despite current advancements in reinforcement fine-tuning
(RFT) showing strong performance in enhancing reasoning in LLMs, its potential
to enable self-evolving embodied intelligence with multi-modal interactions
remains largely unexplored. Specifically, reinforcement fine-tuning faces two
fundamental obstacles in embodied settings: (i) the lack of accessible
intermediate rewards in multi-step reasoning tasks limits effective learning
signals, and (ii) reliance on hand-crafted reward functions restricts
generalization to novel tasks and environments. To address these challenges, we
present Self-Evolving Embodied Agents-R1, SEEA-R1, the first RFT framework
designed for enabling the self-evolving capabilities of embodied agents.
Specifically, to convert sparse delayed rewards into denser intermediate
signals that improve multi-step reasoning, we propose Tree-based group relative
policy optimization (Tree-GRPO), which integrates Monte Carlo Tree Search into
GRPO. To generalize reward estimation across tasks and scenes, supporting
autonomous adaptation and reward-driven self-evolution, we further introduce
Multi-modal Generative Reward Model (MGRM). To holistically evaluate the
effectiveness of SEEA-R1, we evaluate on the ALFWorld benchmark, surpassing
state-of-the-art methods with scores of 85.07% (textual) and 36.19%
(multi-modal), outperforming prior models including GPT-4o. SEEA-R1 also
achieves scores of 80.3% without environmental reward, surpassing all
open-source baselines and highlighting its scalability as a self-evolving
embodied agent. Additional experiments and qualitative analysis further support
the potential of SEEA-R1 for future research in scalable embodied intelligence.

</details>


### [113] [Hierarchical Reasoning Model](https://arxiv.org/abs/2506.21734)
*Guan Wang,Jin Li,Yuhao Sun,Xing Chen,Changling Liu,Yue Wu,Meng Lu,Sen Song,Yasin Abbasi Yadkori*

Main category: cs.AI

TL;DR: HRM, a novel recurrent architecture inspired by the human brain, achieves exceptional performance on complex reasoning tasks with high efficiency and minimal data, outperforming much larger models.


<details>
  <summary>Details</summary>
Motivation: Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain

Method: We propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations.

Result: With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or CoT data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms much larger models with significantly longer context windows on the Abstraction and Reasoning Corpus (ARC)

Conclusion: HRM demonstrates potential as a transformative advancement toward universal computation and general-purpose reasoning systems.

Abstract: Reasoning, the process of devising and executing complex goal-oriented action
sequences, remains a critical challenge in AI. Current large language models
(LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from
brittle task decomposition, extensive data requirements, and high latency.
Inspired by the hierarchical and multi-timescale processing in the human brain,
we propose the Hierarchical Reasoning Model (HRM), a novel recurrent
architecture that attains significant computational depth while maintaining
both training stability and efficiency. HRM executes sequential reasoning tasks
in a single forward pass without explicit supervision of the intermediate
process, through two interdependent recurrent modules: a high-level module
responsible for slow, abstract planning, and a low-level module handling rapid,
detailed computations. With only 27 million parameters, HRM achieves
exceptional performance on complex reasoning tasks using only 1000 training
samples. The model operates without pre-training or CoT data, yet achieves
nearly perfect performance on challenging tasks including complex Sudoku
puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms
much larger models with significantly longer context windows on the Abstraction
and Reasoning Corpus (ARC), a key benchmark for measuring artificial general
intelligence capabilities. These results underscore HRM's potential as a
transformative advancement toward universal computation and general-purpose
reasoning systems.

</details>


### [114] [THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?](https://arxiv.org/abs/2506.21763)
*Xin Wang,Jiyao Liu,Yulong Xiao,Junzhi Ning,Lihao Liu,Junjun He,Botian Shi,Kaicheng Yu*

Main category: cs.AI

TL;DR: Introduces THE-Tree, a framework for constructing domain-specific evolution trees from scientific literature, improving performance in graph completion, predicting future developments, and evaluating scientific papers.


<details>
  <summary>Details</summary>
Motivation: Existing validation methods are inadequate because LLMs hallucinate and lack domain knowledge, while traditional citation networks lack explicit causality and narrative surveys are unstructured. There is an absence of structured, verifiable, and causally-linked historical data of scientific evolution.

Method: A computational framework that constructs domain-specific evolution trees from scientific literature. It employs a search algorithm with a novel Think-Verbalize-Cite-Verify process and validates evolutionary links using a natural language inference mechanism.

Result: Constructed and validated 88 THE-Trees across diverse domains and released a benchmark dataset including up to 71k fact verifications covering 27k papers. Achieved performance improvements in graph completion, predicting future scientific developments, and evaluating important scientific papers.

Conclusion: THE-Tree improves hit@1 in graph completion by 8% to 14%, predicts future scientific developments with nearly 10% improvement in hit@1, and boosts performance of evaluating important scientific papers by almost 100%.

Abstract: Large Language Models (LLMs) are accelerating scientific idea generation, but
rigorously evaluating these numerous, often superficial, AI-generated
propositions for novelty and factual accuracy is a critical bottleneck; manual
verification is too slow.Existing validation methods are inadequate: LLMs as
standalone verifiers may hallucinate and lack domain knowledge (our findings
show ~60\% unawareness of relevant papers in specific domains), while
traditional citation networks lack explicit causality and narrative surveys are
unstructured.This underscores a core challenge: the absence of structured,
verifiable, and causally-linked historical data of scientific evolution.To
address this,we introduce \textbf{THE-Tree} (\textbf{T}echnology
\textbf{H}istory \textbf{E}volution Tree), a computational framework that
constructs such domain-specific evolution trees from scientific
literature.THE-Tree employs a search algorithm to explore evolutionary paths.
During its node expansion, it utilizes a novel "Think-Verbalize-Cite-Verify"
process: an LLM proposes potential advancements and cites supporting
literature. Critically, each proposed evolutionary link is then validated for
logical coherence and evidential support by a recovered natural language
inference mechanism that interrogates the cited literature, ensuring that each
step is grounded.We construct and validate 88 THE-Trees across diverse domains
and release a benchmark dataset including up to 71k fact verifications covering
27k papers to foster further research.Experiments demonstrate that i) in graph
completion, our THE-Tree improves hit@1 by 8\% to 14\% across multiple models
compared to traditional citation networks; ii) for predicting future scientific
developments, it improves hit@1 metric by nearly 10\%; and iii) when combined
with other methods, it boosts the performance of evaluating important
scientific papers by almost 100\%.

</details>


### [115] [MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight Domain-Specific Generator and Large Language Models](https://arxiv.org/abs/2506.21784)
*Yifan Liu,Xishun Liao,Haoxuan Ma,Jonathan Liu,Rohan Jadhav,Jiaqi Ma*

Main category: cs.AI

TL;DR: MobiVerse 是一个混合框架，它利用轻量级领域特定生成器的效率和法学硕士的适应性，以实现上下文感知的修改，从而弥合了移动模拟中的差距，提供了一个用于移动系统规划和运营的可定制平台，并具有基准算法。


<details>
  <summary>Details</summary>
Motivation: 了解和建模人类移动模式对于有效的交通规划和城市发展至关重要。尽管移动研究取得了重大进展，但在模拟平台中仍然存在一个关键的差距，该平台允许大规模地进行算法开发、政策实施和综合评估。

Method: 我们提出了 MobiVerse，这是一个混合框架，它利用轻量级领域特定生成器的效率来生成基本活动链，并利用法学硕士的适应性来进行上下文感知的修改。

Result: 在洛杉矶西木区进行了一项案例研究，我们在标准 PC 上为大约 53,000 名智能体的全体人口高效地生成并动态调整了时间表。我们的实验表明，MobiVerse 成功地使智能体能够通过我们的混合框架响应环境反馈，包括道路封闭、大型聚会活动（如足球比赛）和拥堵。

Conclusion: MobiVerse 成功地使智能体能够通过混合框架响应环境反馈，包括道路封闭、大型聚会活动（如足球比赛）和拥堵。其模块化设计有助于在交通系统和智能体层面测试各种移动算法。结果表明，该方法在保持计算效率的同时，增强了行为的真实性。

Abstract: Understanding and modeling human mobility patterns is crucial for effective
transportation planning and urban development. Despite significant advances in
mobility research, there remains a critical gap in simulation platforms that
allow for algorithm development, policy implementation, and comprehensive
evaluation at scale. Traditional activity-based models require extensive data
collection and manual calibration, machine learning approaches struggle with
adaptation to dynamic conditions, and treding agent-based Large Language Models
(LLMs) implementations face computational constraints with large-scale
simulations. To address these challenges, we propose MobiVerse, a hybrid
framework leverages the efficiency of lightweight domain-specific generator for
generating base activity chains with the adaptability of LLMs for context-aware
modifications. A case study was conducted in Westwood, Los Angeles, where we
efficiently generated and dynamically adjusted schedules for the whole
population of approximately 53,000 agents on a standard PC. Our experiments
demonstrate that MobiVerse successfully enables agents to respond to
environmental feedback, including road closures, large gathering events like
football games, and congestion, through our hybrid framework. Its modular
design facilitates testing various mobility algorithms at both transportation
system and agent levels. Results show our approach maintains computational
efficiency while enhancing behavioral realism. MobiVerse bridges the gap in
mobility simulation by providing a customizable platform for mobility systems
planning and operations with benchmark algorithms. Code and videos are
available at https://github.com/ucla-mobility/MobiVerse.

</details>


### [116] [CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation](https://arxiv.org/abs/2506.21805)
*Nicolas Bougie,Narimasa Watanabe*

Main category: cs.AI

TL;DR: CitySim, an urban simulator powered by large language models, simulates realistic human behavior with nuanced intentions, long-term goals, and spatial memory, outperforming prior methods in aligning with real-world human behavior and offering a scalable testbed for urban phenomena.


<details>
  <summary>Details</summary>
Motivation: Prior work often relies on rigid, hand-crafted rules, limiting their ability to simulate nuanced intentions, plans, and adaptive behaviors in urban environments.

Method: A recursive value-driven approach is used to generate realistic daily schedules, balancing mandatory activities, personal habits, and situational factors. Agents are endowed with beliefs, long-term goals, and spatial memory for navigation to enable long-term simulations.

Result: CitySim exhibits closer alignment with real humans than prior work at both micro and macro levels. Experiments modeling tens of thousands of agents demonstrate its ability to estimate crowd density, predict place popularity, and assess well-being.

Conclusion: CitySim is a scalable and flexible testbed for understanding and forecasting urban phenomena.

Abstract: Modeling human behavior in urban environments is fundamental for social
science, behavioral studies, and urban planning. Prior work often rely on
rigid, hand-crafted rules, limiting their ability to simulate nuanced
intentions, plans, and adaptive behaviors. Addressing these challenges, we
envision an urban simulator (CitySim), capitalizing on breakthroughs in
human-level intelligence exhibited by large language models. In CitySim, agents
generate realistic daily schedules using a recursive value-driven approach that
balances mandatory activities, personal habits, and situational factors. To
enable long-term, lifelike simulations, we endow agents with beliefs, long-term
goals, and spatial memory for navigation. CitySim exhibits closer alignment
with real humans than prior work, both at micro and macro levels. Additionally,
we conduct insightful experiments by modeling tens of thousands of agents and
evaluating their collective behaviors under various real-world scenarios,
including estimating crowd density, predicting place popularity, and assessing
well-being. Our results highlight CitySim as a scalable, flexible testbed for
understanding and forecasting urban phenomena.

</details>


### [117] [Interactive Multi-Objective Probabilistic Preference Learning with Soft and Hard Bounds](https://arxiv.org/abs/2506.21887)
*Edward Chen,Sang T. Truong,Natalie Dullerud,Sanmi Koyejo,Carlos Guestrin*

Main category: cs.AI

TL;DR: Active-MoSH: An interactive framework enhances decision-making by integrating preference learning and sensitivity analysis, improving trust and convergence.


<details>
  <summary>Details</summary>
Motivation: High-stakes decision-making involves navigating multiple competing objectives with expensive evaluations. Selecting Pareto-optimal solutions that match implicit preferences is challenging, as exhaustive Pareto frontier exploration is computationally and cognitively prohibitive. Current methods often lack systematic approaches to iteratively refine these multi-faceted preference structures. DMs must trust their final decision, confident they haven't missed superior alternatives.

Method: The paper proposes Active-MoSH, an interactive local-global framework. The local component integrates soft-hard bounds with probabilistic preference learning, maintaining distributions over DM preferences and bounds for adaptive Pareto subset refinement, guided by an active sampling strategy. The global component, T-MoSH, leverages multi-objective sensitivity analysis.

Result: The paper demonstrates Active-MoSH's performance benefits through diverse synthetic and real-world applications. A user study on AI-generated image selection further validates the framework's ability to improve convergence, enhance DM trust, and provide expressive preference articulation.

Conclusion: The paper introduces Active-MoSH, an interactive framework that helps decision-makers navigate competing objectives with expensive evaluations by integrating soft-hard bounds with probabilistic preference learning and multi-objective sensitivity analysis. User study validates that Active-MoSH improves convergence, enhances DM trust and provides expressive preference articulation, enabling more effective DMs.

Abstract: High-stakes decision-making involves navigating multiple competing objectives
with expensive evaluations. For instance, in brachytherapy, clinicians must
balance maximizing tumor coverage (e.g., an aspirational target or soft bound
of >95% coverage) against strict organ dose limits (e.g., a non-negotiable hard
bound of <601 cGy to the bladder), with each plan evaluation being
resource-intensive. Selecting Pareto-optimal solutions that match implicit
preferences is challenging, as exhaustive Pareto frontier exploration is
computationally and cognitively prohibitive, necessitating interactive
frameworks to guide users. While decision-makers (DMs) often possess domain
knowledge to narrow the search via such soft-hard bounds, current methods often
lack systematic approaches to iteratively refine these multi-faceted preference
structures. Critically, DMs must trust their final decision, confident they
haven't missed superior alternatives; this trust is paramount in
high-consequence scenarios. We present Active-MoSH, an interactive local-global
framework designed for this process. Its local component integrates soft-hard
bounds with probabilistic preference learning, maintaining distributions over
DM preferences and bounds for adaptive Pareto subset refinement. This is guided
by an active sampling strategy optimizing exploration-exploitation while
minimizing cognitive burden. To build DM trust, Active-MoSH's global component,
T-MoSH, leverages multi-objective sensitivity analysis to identify potentially
overlooked, high-value points beyond immediate feedback. We demonstrate
Active-MoSH's performance benefits through diverse synthetic and real-world
applications. A user study on AI-generated image selection further validates
our hypotheses regarding the framework's ability to improve convergence,
enhance DM trust, and provide expressive preference articulation, enabling more
effective DMs.

</details>


### [118] [AlphaBeta is not as good as you think: a new probabilistic model to better analyze deterministic game-solving algorithms](https://arxiv.org/abs/2506.21996)
*Raphaël Boige,Amine Boumaza,Bruno Scherrer*

Main category: cs.AI

TL;DR: 引入了一个新的概率模型来构建博弈树，发现AlphaBeta算法在深度有限树中的性能不如Scout算法。


<details>
  <summary>Details</summary>
Motivation: 传统确定性博弈求解算法的分析存在局限性，因为其简化的模型剥夺了博弈的结构复杂性，产生了没有算法面临有意义挑战的简单实例。

Method: 引入了一种新的概率模型，该模型使用固定的分层条件分布增量构建博弈树。推导了包括AlphaBeta和Scout在内的几种算法在该模型下的平均情况复杂度的递归公式。

Result: 在深度有限树中，AlphaBeta算法的常数乘法因子比Scout等算法大得多，导致实际速度显著下降。

Conclusion: 所有算法似乎都收敛到相同的分支因子，但AlphaBeta算法的常数乘法因子比Scout等算法大得多，导致实际速度显著下降。

Abstract: Deterministic game-solving algorithms are conventionally analyzed in the
light of their average-case complexity against a distribution of random
game-trees, where leaf values are independently sampled from a fixed
distribution. This simplified model enables uncluttered mathematical analysis,
revealing two key properties: root value distributions asymptotically collapse
to a single fixed value for finite-valued trees, and all reasonable algorithms
achieve global optimality. However, these findings are artifacts of the model's
design-its long criticized independence assumption strips games of structural
complexity, producing trivial instances where no algorithm faces meaningful
challenges. To address this limitation, we introduce a new probabilistic model
that incrementally constructs game-trees using a fixed level-wise conditional
distribution. By enforcing ancestor dependency, a critical structural feature
of real-world games, our framework generates problems with adjustable
difficulty while retaining some form of analytical tractability. For several
algorithms, including AlphaBeta and Scout, we derive recursive formulas
characterizing their average-case complexities under this model. These allow us
to rigorously compare algorithms on deep game-trees, where Monte-Carlo
simulations are no longer feasible. While asymptotically, all algorithms seem
to converge to identical branching factor (a result analogous to those of
independence-based models), deep finite trees reveal stark differences:
AlphaBeta incurs a significantly larger constant multiplicative factor compared
to algorithms like Scout, leading to a substantial practical slowdown. Our
framework sheds new light on classical game-solving algorithms, offering
rigorous evidence and analytical tools to advance the understanding of these
methods under a more realistic, challenging, and yet tractable model.

</details>


### [119] [LeanConjecturer: Automatic Generation of Mathematical Conjectures for Theorem Proving](https://arxiv.org/abs/2506.22005)
*Naoto Onda,Kazumi Kasaura,Yuta Oriike,Masaya Taniguchi,Akiyoshi Sannai,Sho Sonoda*

Main category: cs.AI

TL;DR: LeanConjecturer利用大型语言模型自动生成数学猜想，并通过强化学习提升定理证明能力。


<details>
  <summary>Details</summary>
Motivation: 解决形式定理证明中的数据稀缺性挑战。

Method: 该方法结合了基于规则的上下文提取和基于LLM的定理陈述生成。

Result: LeanConjecturer从40个Mathlib种子文件中生成了12,289个猜想，其中3,776个被识别为句法有效且非平凡的，即不能被\texttt{aesop}策略证明。针对领域特定猜想的定向训练可以增强定理证明能力。我们的方法平均每个种子文件生成103.25个新的猜想，为创建定理证明系统的训练数据提供了一个可扩展的解决方案。

Conclusion: LeanConjecturer能够成功验证拓扑学中几个重要的定理，包括半开集、α-开集和前开集的性质，展示了它在数学发现方面的潜力，超越了现有结果的简单变体。

Abstract: We introduce LeanConjecturer, a pipeline for automatically generating
university-level mathematical conjectures in Lean 4 using Large Language Models
(LLMs). Our hybrid approach combines rule-based context extraction with
LLM-based theorem statement generation, addressing the data scarcity challenge
in formal theorem proving. Through iterative generation and evaluation,
LeanConjecturer produced 12,289 conjectures from 40 Mathlib seed files, with
3,776 identified as syntactically valid and non-trivial, that is, cannot be
proven by \texttt{aesop} tactic. We demonstrate the utility of these generated
conjectures for reinforcement learning through Group Relative Policy
Optimization (GRPO), showing that targeted training on domain-specific
conjectures can enhance theorem proving capabilities. Our approach generates
103.25 novel conjectures per seed file on average, providing a scalable
solution for creating training data for theorem proving systems. Our system
successfully verified several non-trivial theorems in topology, including
properties of semi-open, alpha-open, and pre-open sets, demonstrating its
potential for mathematical discovery beyond simple variations of existing
results.

</details>


### [120] [Universal Retrieval for Multimodal Trajectory Modeling](https://arxiv.org/abs/2506.22056)
*Xuan Zhang,Ziyan Jiang,Rui Meng,Yifei Leng,Zhenbang Xiao,Zora Zhiruo Wang,Yanyi Shang,Dehan Kong*

Main category: cs.AI

TL;DR: 本文介绍了多模态轨迹检索，弥合了通用检索和以代理为中心的轨迹建模之间的差距。


<details>
  <summary>Details</summary>
Motivation: 轨迹数据在增强人工智能代理能力方面具有巨大的潜力，特别是在GUI环境中。然而，如何对轨迹级别数据的表示进行建模提出了一个重大的挑战，在轨迹数据爆炸性增长的情况下，这个问题还没有得到系统的解决。

Method: 提出了GAE-Retriever，一个多模态检索框架，它采用了视觉-语言模型，并通过令牌选择和GradCache机制结合了优化的对比学习。

Result: 构建了统一代理轨迹数据集（UATD），并提出了GAE-Bench，一个包含大量基于轨迹的检索对的基准。

Conclusion: GAE-Retriever在检索召回率方面始终优于强大的基线，突出了其在推进多模态轨迹检索方面的有效性。

Abstract: Trajectory data, capturing human actions and environmental states across
various modalities, holds significant potential for enhancing AI agent
capabilities, particularly in GUI environments. However, how to model the
representation of trajectory-level data presents a significant challenge that
has not been systematically addressed amid explosive trajectory data growth. In
this work, we introduce Multimodal Trajectory Retrieval, bridging the gap
between universal retrieval and agent-centric trajectory modeling. We construct
the Unified Agent Trajectory Dataset (UATD) from annotated demonstrations and
states across diverse real-world scenarios. Based on this, we present
GAE-Bench, a benchmark containing a large number of trajectory-based retrieval
pairs. In addition, we propose GAE-Retriever, a multimodal retrieval framework
that adopts vision-language models and incorporates optimized contrastive
learning through a token selection and the GradCache mechanism. Comprehensive
evaluations across multiple datasets show that GAE-Retriever consistently
outperforms strong baselines in retrieval recall, highlighting its
effectiveness in advancing multimodal trajectory retrieval.

</details>


### [121] [Query as Test: An Intelligent Driving Test and Data Storage Method for Integrated Cockpit-Vehicle-Road Scenarios](https://arxiv.org/abs/2506.22068)
*Shengyue Yao,Runqing Guo,Yangyang Qin,Miangbing Meng,Jipeng Cao,Yilun Lin,Yisheng Lv,Fei-Yue Wang*

Main category: cs.AI

TL;DR: This paper introduces the concept of "Query as Test" (QaT) and proposes Extensible Scenarios Notations (ESN) to address the issue of fragmented data ecosystems in autonomous driving. It also introduces Validation-Driven Development (VDD) to accelerate the development process.


<details>
  <summary>Details</summary>
Motivation: The data ecosystems of intelligent cockpits, autonomous driving, and intelligent road networks are increasingly fragmented and incompatible, and existing testing methods lack flexibility and fail to cover all edge cases.

Method: The paper proposes Extensible Scenarios Notations (ESN), a novel declarative data framework based on Answer Set Programming (ASP), to uniformly represent heterogeneous multimodal data.

Result: ESN achieves deep semantic fusion of data and supports complex semantic querying, natural interpretability, and on-demand data abstraction for privacy protection. The QaT paradigm transforms functional validation and safety compliance checks into logical queries against the ESN database.

Conclusion: This paper introduces Validation-Driven Development (VDD) to guide developments by logical validation rather than quantitative testing, aiming to accelerate the iteration and development process.

Abstract: With the deep penetration of Artificial Intelligence (AI) in the
transportation sector, intelligent cockpits, autonomous driving, and
intelligent road networks are developing at an unprecedented pace. However, the
data ecosystems of these three key areas are increasingly fragmented and
incompatible. Especially, existing testing methods rely on data stacking, fail
to cover all edge cases, and lack flexibility. To address this issue, this
paper introduces the concept of "Query as Test" (QaT). This concept shifts the
focus from rigid, prescripted test cases to flexible, on-demand logical queries
against a unified data representation. Specifically, we identify the need for a
fundamental improvement in data storage and representation, leading to our
proposal of "Extensible Scenarios Notations" (ESN). ESN is a novel declarative
data framework based on Answer Set Programming (ASP), which uniformly
represents heterogeneous multimodal data from the cockpit, vehicle, and road as
a collection of logical facts and rules. This approach not only achieves deep
semantic fusion of data, but also brings three core advantages: (1) supports
complex and flexible semantic querying through logical reasoning; (2) provides
natural interpretability for decision-making processes; (3) allows for
on-demand data abstraction through logical rules, enabling fine-grained privacy
protection. We further elaborate on the QaT paradigm, transforming the
functional validation and safety compliance checks of autonomous driving
systems into logical queries against the ESN database, significantly enhancing
the expressiveness and formal rigor of the testing. Finally, we introduce the
concept of "Validation-Driven Development" (VDD), which suggests to guide
developments by logical validation rather than quantitative testing in the era
of Large Language Models, in order to accelerating the iteration and
development process.

</details>


### [122] [A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety](https://arxiv.org/abs/2506.22183)
*Camille François,Ludovic Péran,Ayah Bdeir,Nouha Dziri,Will Hawkins,Yacine Jernite,Sayash Kapoor,Juliet Shen,Heidy Khlaaf,Kevin Klyman,Nik Marda,Marie Pellat,Deb Raji,Divya Siddarth,Aviya Skowron,Joseph Spisak,Madhulika Srikumar,Victor Storchan,Audrey Tang,Jen Weedon*

Main category: cs.AI

TL;DR: 本次论文报告了关于人工智能开放性和安全性的哥伦比亚会议的成果，该会议强调了开放性在提高人工智能安全方面的作用，并提出了未来研究的优先方向。


<details>
  <summary>Details</summary>
Motivation: 开放权重和开源基础模型的迅速崛起，加强了使人工智能系统安全的义务，并重塑了机遇。

Method: 本次研究使用了参与式的、以解决方案为导向的过程，由来自学术界、工业界、民间社会和政府的超过 45 位研究人员、工程师和政策领导者组成的工作组，产出了研究议程、技术干预和开源工具的映射以及内容安全过滤器生态系统的映射。

Result: 工作组产出了 (i) 安全和开源人工智能交叉领域的研究议程；(ii) 现有和所需技术干预和开源工具的映射，以在人工智能开发工作流程中安全和负责任地部署开放基础模型；(iii) 内容安全过滤器生态系统的映射，并提出了未来研究和开发的路线图。

Conclusion: 开放性（透明的权重、可互操作的工具和公共治理）可以通过实现独立审查、分散缓解和文化多元监督来提高安全性。然而，仍然存在显著差距：多模态和多语言基准的稀缺、针对代理系统中 prompt 注入和组合攻击的有限防御，以及受人工智能危害影响最严重的社区的参与机制不足。论文最后提出了五个优先研究方向的路线图，强调参与性投入、面向未来的内容过滤器、全生态系统的安全基础设施、严格的代理保障措施和扩展的危害分类。

Abstract: The rapid rise of open-weight and open-source foundation models is
intensifying the obligation and reshaping the opportunity to make AI systems
safe. This paper reports outcomes from the Columbia Convening on AI Openness
and Safety (San Francisco, 19 Nov 2024) and its six-week preparatory programme
involving more than forty-five researchers, engineers, and policy leaders from
academia, industry, civil society, and government. Using a participatory,
solutions-oriented process, the working groups produced (i) a research agenda
at the intersection of safety and open source AI; (ii) a mapping of existing
and needed technical interventions and open source tools to safely and
responsibly deploy open foundation models across the AI development workflow;
and (iii) a mapping of the content safety filter ecosystem with a proposed
roadmap for future research and development. We find that openness --
understood as transparent weights, interoperable tooling, and public governance
-- can enhance safety by enabling independent scrutiny, decentralized
mitigation, and culturally plural oversight. However, significant gaps persist:
scarce multimodal and multilingual benchmarks, limited defenses against
prompt-injection and compositional attacks in agentic systems, and insufficient
participatory mechanisms for communities most affected by AI harms. The paper
concludes with a roadmap of five priority research directions, emphasizing
participatory inputs, future-proof content filters, ecosystem-wide safety
infrastructure, rigorous agentic safeguards, and expanded harm taxonomies.
These recommendations informed the February 2025 French AI Action Summit and
lay groundwork for an open, plural, and accountable AI safety discipline.

</details>


### [123] [Breaking Rank Bottlenecks in Knowledge Graph Completion](https://arxiv.org/abs/2506.22271)
*Samy Badreddine,Emile van Krieken,Luciano Serafini*

Main category: cs.AI

TL;DR: KGC models rely on a simple vector-matrix multiplication to score queries, which limits model expressivity. KGE-MoS, a mixture-based output layer, is proposed to break rank bottlenecks in many KGC models.


<details>
  <summary>Details</summary>
Motivation: rank bottlenecks hurt ranking accuracy and the distribution fidelity of scores by limiting the set of feasible predictions

Method: a mixture-based output layer to break rank bottlenecks

Result: KGE-MoS improves performance and probabilistic fit of KGC models

Conclusion: KGE-MoS improves performance and probabilistic fit of KGC models for a low parameter cost.

Abstract: Many Knowledge Graph Completion (KGC) models, despite using powerful
encoders, rely on a simple vector-matrix multiplication to score queries
against candidate object entities. When the number of entities is larger than
the model's embedding dimension, which in practical scenarios is often by
several orders of magnitude, we have a linear output layer with a rank
bottleneck. Such bottlenecked layers limit model expressivity. We investigate
both theoretically and empirically how rank bottlenecks affect KGC models. We
find that, by limiting the set of feasible predictions, rank bottlenecks hurt
ranking accuracy and the distribution fidelity of scores. Inspired by the
language modelling literature, we propose KGE-MoS, a mixture-based output layer
to break rank bottlenecks in many KGC models. Our experiments on four datasets
show that KGE-MoS improves performance and probabilistic fit of KGC models for
a low parameter cost.

</details>


### [124] [Artificial Intelligent Disobedience: Rethinking the Agency of Our Artificial Teammates](https://arxiv.org/abs/2506.22276)
*Reuth Mirsky*

Main category: cs.AI

TL;DR: This paper argues for expanding the agency of AI teammates to include intelligent disobedience, empowering them to make meaningful and autonomous contributions within human-AI teams.


<details>
  <summary>Details</summary>
Motivation: Most cooperative AI systems remain rigidly obedient, designed to follow human instructions without question, even when doing so may be counterproductive or unsafe.

Method: It introduces a scale of AI agency levels and uses representative examples.

Result: The paper explores how intelligent disobedience manifests across different autonomy levels.

Conclusion: This paper proposes initial boundaries and considerations for studying disobedience as a core capability of artificial agents.

Abstract: Artificial intelligence has made remarkable strides in recent years,
achieving superhuman performance across a wide range of tasks. Yet despite
these advances, most cooperative AI systems remain rigidly obedient, designed
to follow human instructions without question and conform to user expectations,
even when doing so may be counterproductive or unsafe. This paper argues for
expanding the agency of AI teammates to include \textit{intelligent
disobedience}, empowering them to make meaningful and autonomous contributions
within human-AI teams. It introduces a scale of AI agency levels and uses
representative examples to highlight the importance and growing necessity of
treating AI autonomy as an independent research focus in cooperative settings.
The paper then explores how intelligent disobedience manifests across different
autonomy levels and concludes by proposing initial boundaries and
considerations for studying disobedience as a core capability of artificial
agents.

</details>


### [125] [Conceptual Topic Aggregation](https://arxiv.org/abs/2506.22309)
*Klara M. Gutekunst,Dominik Dürrschnabel,Johannes Hirth,Gerd Stumme*

Main category: cs.AI

TL;DR: Proposes FAT-CAT, an FCA-based approach, to enhance topic aggregation and visualization, providing more interpretable insights compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional manual inspection is infeasible, necessitating computational methods. Existing topic modeling methods struggle to provide interpretable representations.

Method: FAT-CAT, an approach based on Formal Concept Analysis (FCA) to enhance meaningful topic aggregation and visualization of discovered topics.

Result: FCA-based aggregation provides more meaningful and interpretable insights into dataset composition than existing topic modeling techniques in a case study on the ETYNTKE dataset.

Conclusion: FCA-based aggregation provides more meaningful and interpretable insights into dataset composition than existing topic modeling techniques.

Abstract: The vast growth of data has rendered traditional manual inspection
infeasible, necessitating the adoption of computational methods for efficient
data exploration. Topic modeling has emerged as a powerful tool for analyzing
large-scale textual datasets, enabling the extraction of latent semantic
structures. However, existing methods for topic modeling often struggle to
provide interpretable representations that facilitate deeper insights into data
structure and content. In this paper, we propose FAT-CAT, an approach based on
Formal Concept Analysis (FCA) to enhance meaningful topic aggregation and
visualization of discovered topics. Our approach can handle diverse topics and
file types -- grouped by directories -- to construct a concept lattice that
offers a structured, hierarchical representation of their topic distribution.
In a case study on the ETYNTKE dataset, we evaluate the effectiveness of our
approach against other representation methods to demonstrate that FCA-based
aggregation provides more meaningful and interpretable insights into dataset
composition than existing topic modeling techniques.

</details>


### [126] [Embodied AI Agents: Modeling the World](https://arxiv.org/abs/2506.22355)
*Pascale Fung,Yoram Bachrach,Asli Celikyilmaz,Kamalika Chaudhuri,Delong Chen,Willy Chung,Emmanuel Dupoux,Hervé Jégou,Alessandro Lazaric,Arjun Majumdar,Andrea Madotto,Franziska Meier,Florian Metze,Théo Moutakanni,Juan Pino,Basile Terver,Joseph Tighe,Jitendra Malik*

Main category: cs.AI

TL;DR: This paper proposes the development of world models is central to reasoning and planning of embodied AI agents, allowing these agents to understand and predict their environment, to understand user intentions and social contexts, thereby enhancing their ability to perform complex tasks autonomously.


<details>
  <summary>Details</summary>
Motivation: AI agents embodied in visual, virtual or physical forms can interact with both users and their environments. These agents are designed to perceive, learn and act within their surroundings, which makes them more similar to how humans learn and interact with the environments as compared to disembodied agents.

Method: Integration of multimodal perception, planning through reasoning for action and control, and memory to create a comprehensive understanding of the physical world. Learning the mental world model of users to enable better human-agent collaboration.

Result: AI agents can understand and predict their environment, understand user intentions and social contexts, and perform complex tasks autonomously.

Conclusion: Development of world models is central to reasoning and planning of embodied AI agents, allowing these agents to understand and predict their environment, to understand user intentions and social contexts, thereby enhancing their ability to perform complex tasks autonomously.

Abstract: This paper describes our research on AI agents embodied in visual, virtual or
physical forms, enabling them to interact with both users and their
environments. These agents, which include virtual avatars, wearable devices,
and robots, are designed to perceive, learn and act within their surroundings,
which makes them more similar to how humans learn and interact with the
environments as compared to disembodied agents. We propose that the development
of world models is central to reasoning and planning of embodied AI agents,
allowing these agents to understand and predict their environment, to
understand user intentions and social contexts, thereby enhancing their ability
to perform complex tasks autonomously. World modeling encompasses the
integration of multimodal perception, planning through reasoning for action and
control, and memory to create a comprehensive understanding of the physical
world. Beyond the physical world, we also propose to learn the mental world
model of users to enable better human-agent collaboration.

</details>


### [127] [AI Model Passport: Data and System Traceability Framework for Transparent AI in Health](https://arxiv.org/abs/2506.22358)
*Varvara Kalokyri,Nikolaos S. Tachos,Charalampos N. Kalantzopoulos,Stelios Sfakianakis,Haridimos Kondylakis,Dimitrios I. Zaridis,Sara Colantonio,Daniele Regge,Nikolaos Papanikolaou,The ProCAncer-I consortium,Konstantinos Marias,Dimitrios I. Fotiadis,Manolis Tsiknakis*

Main category: cs.AI

TL;DR: The paper proposes the AI Model Passport as a digital identity for AI models to improve transparency and trust, and showcases its implementation in medical imaging.


<details>
  <summary>Details</summary>
Motivation: Existing AI frameworks lack scalability, comparability, machine interpretability, and a unique, verifiable identity for AI models, limiting reproducibility and stakeholder trust.

Method: The paper introduces the AI Model Passport and implements it through AIPassport, an MLOps tool. A lesion segmentation use case using data from the ProCAncer-I dataset is used to showcase its effectiveness.

Result: The AI Model Passport enhances transparency, reproducibility, and regulatory readiness while reducing manual effort.

Conclusion: This paper introduces the AI Model Passport, a structured documentation framework for AI models, and demonstrates its implementation through AIPassport, enhancing transparency, reproducibility, and regulatory readiness in AI-driven healthcare.

Abstract: The increasing integration of Artificial Intelligence (AI) into health and
biomedical systems necessitates robust frameworks for transparency,
accountability, and ethical compliance. Existing frameworks often rely on
human-readable, manual documentation which limits scalability, comparability,
and machine interpretability across projects and platforms. They also fail to
provide a unique, verifiable identity for AI models to ensure their provenance
and authenticity across systems and use cases, limiting reproducibility and
stakeholder trust. This paper introduces the concept of the AI Model Passport,
a structured and standardized documentation framework that acts as a digital
identity and verification tool for AI models. It captures essential metadata to
uniquely identify, verify, trace and monitor AI models across their lifecycle -
from data acquisition and preprocessing to model design, development and
deployment. In addition, an implementation of this framework is presented
through AIPassport, an MLOps tool developed within the ProCAncer-I EU project
for medical imaging applications. AIPassport automates metadata collection,
ensures proper versioning, decouples results from source scripts, and
integrates with various development environments. Its effectiveness is
showcased through a lesion segmentation use case using data from the
ProCAncer-I dataset, illustrating how the AI Model Passport enhances
transparency, reproducibility, and regulatory readiness while reducing manual
effort. This approach aims to set a new standard for fostering trust and
accountability in AI-driven healthcare solutions, aspiring to serve as the
basis for developing transparent and regulation compliant AI systems across
domains.

</details>


### [128] [The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements](https://arxiv.org/abs/2506.22419)
*Bingchen Zhao,Despoina Magka,Minqi Jiang,Xian Li,Roberta Raileanu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Kelvin Niu,Shagun Sodhani,Michael Shvartsman,Andrei Lupu,Alisia Lupidi,Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Thomas Foster,Lucia Cipolina-Kun,Abhishek Charnalia,Derek Dunfield,Alexander H. Miller,Oisin Mac Aodha,Jakob Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: This paper introduces a benchmark to evaluate LLMs' ability to reproduce research results in LLM training. The results show that current LLMs struggle to reimplement known innovations, highlighting the challenges in automating scientific reproduction.


<details>
  <summary>Details</summary>
Motivation: Evaluate the ability of AI agents to reproduce results in an active research area.

Method: Automated LLM Speedrunning Benchmark, leveraging the research community contributions on the NanoGPT speedrun.

Result: LLMs struggle to reimplement already-known innovations in the benchmark, even when given detailed hints.

Conclusion: Recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement already-known innovations, even when given detailed hints. This benchmark provides a simple, non-saturated measure of an LLMs ability to automate scientific reproduction.

Abstract: Rapid advancements in large language models (LLMs) have the potential to
assist in scientific progress. A critical capability toward this endeavor is
the ability to reproduce existing work. To evaluate the ability of AI agents to
reproduce results in an active research area, we introduce the Automated LLM
Speedrunning Benchmark, leveraging the research community contributions on the
NanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.
Each of the 19 speedrun tasks provides the agent with the previous records
training script, optionally paired with one of three hint formats, ranging from
pseudocode to paper-like descriptions of the new records improvements. Records
execute quickly by design and speedrun improvements encompass diverse
code-level changes, ranging from high-level algorithmic advancements to
hardware-aware optimizations. These features make the benchmark both accessible
and realistic for the frontier problem of improving LLM training. We find that
recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement
already-known innovations in our benchmark, even when given detailed hints. Our
benchmark thus provides a simple, non-saturated measure of an LLMs ability to
automate scientific reproduction, a necessary (but not sufficient) skill for an
autonomous research agent.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [129] [Revisiting Graph Analytics Benchmark](https://arxiv.org/abs/2506.21811)
*Lingkai Meng,Yu Shao,Long Yuan,Longbin Lai,Peng Cheng,Xue Li,Wenyuan Yu,Wenjie Zhang,Xuemin Lin,Jingren Zhou*

Main category: cs.DB

TL;DR: A novel graph analytics benchmark is proposed to address the shortcomings of existing benchmarks by improving algorithm selection, data generation, and API usability evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing graph analytics benchmarks often fall short of fully assessing performance due to limitations in core algorithm selection, data generation processes, and the neglect of API usability evaluation.

Method: The authors select eight core algorithms, design an efficient and flexible data generator to produce eight new synthetic datasets, and introduce a multi-level large language model (LLM)-based framework for API usability evaluation.

Result: Comprehensive experimental evaluations on existing platforms (GraphX, PowerGraph, Flash, Grape, Pregel+, Ligra and G-thinker) demonstrate the superiority of the proposed benchmark.

Conclusion: The proposed graph analytics benchmark demonstrates its superiority through comprehensive experimental evaluations on existing platforms (GraphX, PowerGraph, Flash, Grape, Pregel+, Ligra and G-thinker).

Abstract: The rise of graph analytics platforms has led to the development of various
benchmarks for evaluating and comparing platform performance. However, existing
benchmarks often fall short of fully assessing performance due to limitations
in core algorithm selection, data generation processes (and the corresponding
synthetic datasets), as well as the neglect of API usability evaluation. To
address these shortcomings, we propose a novel graph analytics benchmark.
First, we select eight core algorithms by extensively reviewing both academic
and industrial settings. Second, we design an efficient and flexible data
generator and produce eight new synthetic datasets as the default datasets for
our benchmark. Lastly, we introduce a multi-level large language model
(LLM)-based framework for API usability evaluation-the first of its kind in
graph analytics benchmarks. We conduct comprehensive experimental evaluations
on existing platforms (GraphX, PowerGraph, Flash, Grape, Pregel+, Ligra and
G-thinker). The experimental results demonstrate the superiority of our
proposed benchmark.

</details>


### [130] [A Survey of LLM Inference Systems](https://arxiv.org/abs/2506.21901)
*James Pan,Guoliang Li*

Main category: cs.DB

TL;DR: Reviews LLM inference techniques, highlighting load prediction, adaptive mechanisms, and cost reduction for high performance.


<details>
  <summary>Details</summary>
Motivation: The autoregressive nature of LLM request processing motivates new techniques for high performance and inference quality under high-volume workloads. Existing techniques lack analysis within a complete system framework.

Method: Reviewing operators, algorithms, model optimization, execution, and memory management techniques.

Result: Techniques fundamentally rely on load prediction, adaptive mechanisms, and cost reduction. Combination of these techniques to form single-replica and multi-replica inference systems, including disaggregated and serverless systems.

Conclusion: This survey reviews techniques for LLM inference systems, showing they rely on load prediction, adaptive mechanisms, and cost reduction. It discusses combining these techniques in single/multi-replica systems and remaining challenges.

Abstract: The past few years has witnessed specialized large language model (LLM)
inference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside
rapid LLM adoption via services like ChatGPT. Driving these system design
efforts is the unique autoregressive nature of LLM request processing,
motivating new techniques for achieving high performance while preserving high
inference quality over high-volume and high-velocity workloads. While many of
these techniques are discussed across the literature, they have not been
analyzed under the framework of a complete inference system, nor have the
systems themselves been analyzed and compared.
  In this survey, we review these techniques, starting from operators and
algorithms for request processing, then moving on to techniques for model
optimization and execution, including kernel design, batching, and scheduling,
before ending with techniques for memory management, including paged memory,
eviction and offloading techniques, quantization, and cache persistence.
Through these discussions, we show that these techniques fundamentally rely on
load prediction, adaptive mechanisms, and cost reduction in order to overcome
the challenges introduced by autoregressive generation and achieve the goals of
the system. We then discuss how these techniques can be combined to form
single-replica and multi-replica inference systems, including disaggregated
inference systems that offer more control over resource allocation and
serverless systems that can be deployed over shared hardware infrastructure. We
end with a discussion of remaining challenges.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [131] [LLM2Rec: Large Language Models Are Powerful Embedding Models for Sequential Recommendation](https://arxiv.org/abs/2506.21579)
*Yingzhi He,Xiaohao Liu,An Zhang,Yunshan Ma,Tat-Seng Chua*

Main category: cs.IR

TL;DR: LLM2Rec是一种新的嵌入模型，它集成了LLM的语义理解和CF感知，以提高序列推荐的质量。


<details>
  <summary>Details</summary>
Motivation: 传统的序列推荐器主要依赖于基于id的嵌入，这些嵌入通过高阶共现模式捕获CF信号。然而，这些嵌入完全依赖于过去的交互，缺乏推广到看不见的领域的传递知识。大型语言模型(llm)的最新进展推动了基于文本的推荐方法，这些方法从文本描述中获得项目表示。虽然这些方法增强了泛化，但它们未能编码CF信号——即潜在的项目相关性和偏好模式——这对于有效的推荐至关重要。我们认为，理想的嵌入模型应该无缝地将CF信号与丰富的语义表示集成，以提高领域内和领域外的推荐性能。

Method: LLM2Rec，一种为序列推荐量身定制的新型嵌入模型，它集成了LLM的丰富语义理解和CF感知。我们的方法遵循一个两阶段的训练框架：(1) 协同监督微调，它使llm能够根据历史交互推断项目关系；(2) 项目级嵌入建模，它将这些专门的llm改进为结构化的项目嵌入模型，这些模型编码语义和协作信息。

Result: 在真实数据集上的大量实验表明，LLM2Rec有效地提高了在领域内和领域外环境下的推荐质量。

Conclusion: LLM2Rec有效地提高了在领域内和领域外环境下的推荐质量，展示了利用LLM构建更健壮、更通用的序列推荐嵌入模型的潜力。

Abstract: Sequential recommendation aims to predict users' future interactions by
modeling collaborative filtering (CF) signals from historical behaviors of
similar users or items. Traditional sequential recommenders predominantly rely
on ID-based embeddings, which capture CF signals through high-order
co-occurrence patterns. However, these embeddings depend solely on past
interactions, lacking transferable knowledge to generalize to unseen domains.
Recent advances in large language models (LLMs) have motivated text-based
recommendation approaches that derive item representations from textual
descriptions. While these methods enhance generalization, they fail to encode
CF signals-i.e., latent item correlations and preference patterns-crucial for
effective recommendation. We argue that an ideal embedding model should
seamlessly integrate CF signals with rich semantic representations to improve
both in-domain and out-of-domain recommendation performance.
  To this end, we propose LLM2Rec, a novel embedding model tailored for
sequential recommendation, integrating the rich semantic understanding of LLMs
with CF awareness. Our approach follows a two-stage training framework: (1)
Collaborative Supervised Fine-tuning, which adapts LLMs to infer item
relationships based on historical interactions, and (2) Item-level Embedding
Modeling, which refines these specialized LLMs into structured item embedding
models that encode both semantic and collaborative information. Extensive
experiments on real-world datasets demonstrate that LLM2Rec effectively
improves recommendation quality across both in-domain and out-of-domain
settings. Our findings highlight the potential of leveraging LLMs to build more
robust, generalizable embedding models for sequential recommendation. Our codes
are available at https://github.com/HappyPointer/LLM2Rec.

</details>


### [132] [Evaluating the Robustness of Dense Retrievers in Interdisciplinary Domains](https://arxiv.org/abs/2506.21581)
*Sarthak Chaturvedi,Anurag Acharya,Rounak Meyur,Koby Hayashi,Sai Munikoti,Sameera Horawalavithana*

Main category: cs.IR

TL;DR: 基准选择强烈决定了对专业领域中检索系统有效性的评估。


<details>
  <summary>Details</summary>
Motivation: 评估基准特征可能会扭曲领域自适应的真正优势，从而导致对专业领域部署决策产生误导性评估。

Method: 在联邦机构的环境影响报告 (EIS) 上微调 ColBERTv2 模型，并使用主题多样性指标比较不同的基准。

Result: 相同的领域自适应方法在不同的评估方法下表现出非常不同的感知优势。在具有清晰分隔主题边界的基准上，领域自适应显示出较小的改进（最大 0.61% NDCG 增益）。但是，在另一个具有重叠语义结构的基准上，相同的模型表现出很大的改进（高达 2.22% NDCG 增益），性能优势相差 3.6 倍。

Conclusion: 评估基准的特征可能会扭曲检索模型中领域自适应的真正优势。具有明确分隔主题边界的评估框架通常会低估领域自适应的优势，而那些具有重叠语义边界的评估框架则会显示出更好地反映真实世界监管文档复杂性的改进。

Abstract: Evaluation benchmark characteristics may distort the true benefits of domain
adaptation in retrieval models. This creates misleading assessments that
influence deployment decisions in specialized domains. We show that two
benchmarks with drastically different features such as topic diversity,
boundary overlap, and semantic complexity can influence the perceived benefits
of fine-tuning. Using environmental regulatory document retrieval as a case
study, we fine-tune ColBERTv2 model on Environmental Impact Statements (EIS)
from federal agencies. We evaluate these models across two benchmarks with
different semantic structures. Our findings reveal that identical domain
adaptation approaches show very different perceived benefits depending on
evaluation methodology. On one benchmark, with clearly separated topic
boundaries, domain adaptation shows small improvements (maximum 0.61% NDCG
gain). However, on the other benchmark with overlapping semantic structures,
the same models demonstrate large improvements (up to 2.22% NDCG gain), a
3.6-fold difference in the performance benefit. We compare these benchmarks
through topic diversity metrics, finding that the higher-performing benchmark
shows 11% higher average cosine distances between contexts and 23% lower
silhouette scores, directly contributing to the observed performance
difference. These results demonstrate that benchmark selection strongly
determines assessments of retrieval system effectiveness in specialized
domains. Evaluation frameworks with well-separated topics regularly
underestimate domain adaptation benefits, while those with overlapping semantic
boundaries reveal improvements that better reflect real-world regulatory
document complexity. Our findings have important implications for developing
and deploying AI systems for interdisciplinary domains that integrate multiple
topics.

</details>


### [133] [PentaRAG: Large-Scale Intelligent Knowledge Retrieval for Enterprise LLM Applications](https://arxiv.org/abs/2506.21593)
*Abu Hanif Muhammad Syarubany,Chang Dong Yoo*

Main category: cs.IR

TL;DR: PentaRAG, a five-layer module, addresses the demands of enterprise LLM deployments by improving speed and efficiency in RAG systems.


<details>
  <summary>Details</summary>
Motivation: Enterprise deployments of large-language model (LLM) demand continuously changing document collections with sub-second latency and predictable GPU cost requirements that classical Retrieval-Augmented Generation (RAG) pipelines only partially satisfy.

Method: a five-layer module that routes each query through two instant caches (fixed key-value and semantic), a memory-recall mode that exploits the LLM's own weights, an adaptive session memory, and a conventional retrieval-augmentation layer

Result: cache warming reduces mean latency from several seconds to well below one second and shifts traffic toward the fast paths. Resource-efficiency tests show that PentaRAG cuts average GPU time to 0.248 seconds per query, roughly half that of a naive RAG baseline, and sustains an aggregate throughput of approximately 100,000 queries per second

Conclusion:  layered routing strategy can deliver freshness, speed, and efficiency simultaneously in production-grade RAG systems.

Abstract: Enterprise deployments of large-language model (LLM) demand continuously
changing document collections with sub-second latency and predictable GPU cost
requirements that classical Retrieval-Augmented Generation (RAG) pipelines only
partially satisfy. We present PentaRAG, a five-layer module that routes each
query through two instant caches (fixed key-value and semantic), a
memory-recall mode that exploits the LLM's own weights, an adaptive session
memory, and a conventional retrieval-augmentation layer. Implemented with
Mistral-8B, Milvus and vLLM, the system can answer most repeated or
semantically similar questions from low-latency caches while retaining full
retrieval for novel queries. On the TriviaQA domain, LoRA fine-tuning combined
with the memory-recall layer raises answer similarity by approximately 8% and
factual correctness by approximately 16% over the base model. Under a
nine-session runtime simulation, cache warming reduces mean latency from
several seconds to well below one second and shifts traffic toward the fast
paths. Resource-efficiency tests show that PentaRAG cuts average GPU time to
0.248 seconds per query, roughly half that of a naive RAG baseline, and
sustains an aggregate throughput of approximately 100,000 queries per second on
our setup. These results demonstrate that a layered routing strategy can
deliver freshness, speed, and efficiency simultaneously in production-grade RAG
systems.

</details>


### [134] [SERP Interference Network and Its Applications in Search Advertising](https://arxiv.org/abs/2506.21598)
*Purak Jain,Sandeep Appala*

Main category: cs.IR

TL;DR: This paper presents a new A/B testing method for search engine marketing that addresses challenges with user anonymization and interference between ads, using network analysis and a SageMaker-based system.


<details>
  <summary>Details</summary>
Motivation: Search Engine Marketing teams need to run continuous A/B tests to optimize profitability and customer experience on SERPs. However, they face challenges with anonymized users and violations of the Stable Unit Treatment Value Assumption.

Method: The paper proposes constructing bipartite (Search Query to Product Ad or Text Ad) SERP interference networks, using a novel weighting function to create weighted projections, and forming unipartite graphs for clustering and randomization.

Result: The paper demonstrates the application of the experimental design in evaluating a new bidding algorithm for Paid Search. It also provides a blueprint of a novel system architecture utilizing SageMaker.

Conclusion: This paper introduces a novel experimental design leveraging censored observational data to construct bipartite SERP interference networks for A/B testing in search engine marketing. It demonstrates the application of this design in evaluating a new bidding algorithm and provides a system architecture blueprint using SageMaker.

Abstract: Search Engine marketing teams in the e-commerce industry manage global search
engine traffic to their websites with the aim to optimize long-term
profitability by delivering the best possible customer experience on Search
Engine Results Pages (SERPs). In order to do so, they need to run continuous
and rapid Search Marketing A/B tests to continuously evolve and improve their
products. However, unlike typical e-commerce A/B tests that can randomize based
on customer identification, their tests face the challenge of anonymized users
on search engines. On the other hand, simply randomizing on products violates
Stable Unit Treatment Value Assumption for most treatments of interest. In this
work, we propose leveraging censored observational data to construct bipartite
(Search Query to Product Ad or Text Ad) SERP interference networks. Using a
novel weighting function, we create weighted projections to form unipartite
graphs which can then be use to create clusters to randomized on. We
demonstrate this experimental design's application in evaluating a new bidding
algorithm for Paid Search. Additionally, we provide a blueprint of a novel
system architecture utilizing SageMaker which enables polyglot programming to
implement each component of the experimental framework.

</details>


### [135] [Reinforcement Fine-Tuned Large Language Models for Next POI Recommendation](https://arxiv.org/abs/2506.21599)
*Peibo Li,Shuang Ao,Hao Xue,Yang Song,Maarten de Rijke,Johan Barthélemy,Tomasz Bednarz,Flora D. Salim*

Main category: cs.IR

TL;DR: Refine-POI is a reinforcement fine-tuning framework for next POI recommendation that achieves state-of-the-art top-k recommendation performance.


<details>
  <summary>Details</summary>
Motivation: Prompt-based models generally offer greater output flexibility but deliver lower accuracy, whereas SFT-based models achieve higher performance yet face a fundamental mismatch: next POI recommendation data does not naturally suit supervised fine-tuning.

Method: We introduce recommendation-driven rewards that enable LLMs to learn to generate top-k recommendation lists using only one ground-truth POI per example.

Result: Experiments on real-world datasets demonstrate that Refine-POI achieves state-of-the-art top-k recommendation performance.

Conclusion: Refine-POI achieves state-of-the-art top-k recommendation performance.

Abstract: Large language models (LLMs) have been adopted for next point-of-interest
(POI) recommendation tasks. Typical LLM-based recommenders fall into two
categories: prompt-based and supervised fine-tuning (SFT)-based models.
Prompt-based models generally offer greater output flexibility but deliver
lower accuracy, whereas SFT-based models achieve higher performance yet face a
fundamental mismatch: next POI recommendation data does not naturally suit
supervised fine-tuning. In SFT, the model is trained to reproduce the exact
ground truth, but each training example provides only a single target POI, so
there is no ground truth for producing a top-k list.
  To address this, we propose Refine-POI, a reinforcement fine-tuning framework
for next POI recommendation. We introduce recommendation-driven rewards that
enable LLMs to learn to generate top-k recommendation lists using only one
ground-truth POI per example. Experiments on real-world datasets demonstrate
that Refine-POI achieves state-of-the-art top-k recommendation performance.

</details>


### [136] [Hierarchical Patch Compression for ColPali: Efficient Multi-Vector Document Retrieval with Dynamic Pruning and Quantization](https://arxiv.org/abs/2506.21601)
*Duong Bach*

Main category: cs.IR

TL;DR: HPC-ColPali improves the efficiency of multi-vector document retrieval while maintaining accuracy by compressing patch embeddings and pruning less important patches.


<details>
  <summary>Details</summary>
Motivation: Multi-vector document retrieval systems are accurate but have high storage and computational costs.

Method: Hierarchical Patch Compression framework with K-Means quantization, attention-guided dynamic pruning, and optional binary encoding.

Result: HPC-ColPali achieves 30-50% lower query latency, reduces hallucination rates by 30%, and halves end-to-end latency.

Conclusion: HPC-ColPali is a scalable and efficient solution for multi-vector document retrieval across diverse applications, reducing hallucination rates and latency in Retrieval-Augmented Generation.

Abstract: Multi-vector document retrieval systems, such as ColPali, excel in
fine-grained matching for complex queries but incur significant storage and
computational costs due to their reliance on high-dimensional patch embeddings
and late-interaction scoring. To address these challenges, we propose
HPC-ColPali, a Hierarchical Patch Compression framework that enhances the
efficiency of ColPali while preserving its retrieval accuracy. Our approach
integrates three innovative techniques: (1) K-Means quantization, which
compresses patch embeddings into 1-byte centroid indices, achieving up to
32$\times$ storage reduction; (2) attention-guided dynamic pruning, utilizing
Vision-Language Model attention weights to retain only the top-$p\%$ most
salient patches, reducing late-interaction computation by up to 60\% with less
than 2\% nDCG@10 loss; and (3) optional binary encoding of centroid indices
into $b$-bit strings ($b=\lceil\log_2 K\rceil$), enabling rapid Hamming
distance-based similarity search for resource-constrained environments.
Evaluated on the ViDoRe and SEC-Filings datasets, HPC-ColPali achieves 30--50\%
lower query latency under HNSW indexing while maintaining high retrieval
precision. When integrated into a Retrieval-Augmented Generation pipeline for
legal summarization, it reduces hallucination rates by 30\% and halves
end-to-end latency. These advancements establish HPC-ColPali as a scalable and
efficient solution for multi-vector document retrieval across diverse
applications. Code is available at https://github.com/DngBack/HPC-ColPali.

</details>


### [137] [Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding](https://arxiv.org/abs/2506.21604)
*Varun Mannam,Fang Wang,Xin Chen*

Main category: cs.IR

TL;DR: 这篇论文提出了一个量化多模态RAG系统可信度的框架，发现适当的模态加权可以显著提高性能。


<details>
  <summary>Details</summary>
Motivation: 目前多模态生成AI的评估框架难以建立可信度，阻碍了企业采用，而可靠性至关重要。

Method: 该方法建立技术指标和以用户为中心的信任度量之间的定量关系。

Result: 评估表明，文本、图像、标题和OCR的最佳模态加权（权重分别为30%、15%、25%和30%）比纯文本基线提高了57.3%的性能，同时保持了计算效率。

Conclusion: 这篇论文提出了一个严谨的框架，用于量化和增强多模态RAG在关键企业应用中的可信度，从而推进负责任的AI部署。

Abstract: Current evaluation frameworks for multimodal generative AI struggle to
establish trustworthiness, hindering enterprise adoption where reliability is
paramount. We introduce a systematic, quantitative benchmarking framework to
measure the trustworthiness of progressively integrating cross-modal inputs
such as text, images, captions, and OCR within VisualRAG systems for enterprise
document intelligence. Our approach establishes quantitative relationships
between technical metrics and user-centric trust measures. Evaluation reveals
that optimal modality weighting with weights of 30% text, 15% image, 25%
caption, and 30% OCR improves performance by 57.3% over text-only baselines
while maintaining computational efficiency. We provide comparative assessments
of foundation models, demonstrating their differential impact on
trustworthiness in caption generation and OCR extraction-a vital consideration
for reliable enterprise AI. This work advances responsible AI deployment by
providing a rigorous framework for quantifying and enhancing trustworthiness in
multimodal RAG for critical enterprise applications.

</details>


### [138] [Bayesian-Guided Diversity in Sequential Sampling for Recommender Systems](https://arxiv.org/abs/2506.21617)
*Hiba Bederina,Jill-Jênn Vie*

Main category: cs.IR

TL;DR: Proposes a multi-objective, contextual sequential sampling framework that leverages a multi-objective, contextual sequential sampling strategy to improve diversity without sacrificing relevance in recommender systems.


<details>
  <summary>Details</summary>
Motivation: The challenge of balancing user relevance and content diversity in recommender systems is increasingly critical amid growing concerns about content homogeneity and reduced user engagement.

Method: The proposed framework leverages a multi-objective, contextual sequential sampling strategy. Item selection is guided by Bayesian updates that dynamically adjust scores to optimize diversity. The reward formulation integrates multiple diversity metrics-including the log-determinant volume of a tuned similarity submatrix and ridge leverage scores-along with a diversity gain uncertainty term to address the exploration-exploitation trade-off. Both intra- and inter-batch diversity are modeled to promote serendipity and minimize redundancy. A dominance-based ranking procedure identifies Pareto-optimal item sets, enabling adaptive and balanced selections at each iteration.

Result: Experiments on a real-world dataset show that our approach significantly improves diversity without sacrificing relevance.

Conclusion: This paper presents a framework that significantly improves diversity without sacrificing relevance, demonstrating its potential to enhance user experience in large-scale recommendation settings.

Abstract: The challenge of balancing user relevance and content diversity in
recommender systems is increasingly critical amid growing concerns about
content homogeneity and reduced user engagement. In this work, we propose a
novel framework that leverages a multi-objective, contextual sequential
sampling strategy. Item selection is guided by Bayesian updates that
dynamically adjust scores to optimize diversity. The reward formulation
integrates multiple diversity metrics-including the log-determinant volume of a
tuned similarity submatrix and ridge leverage scores-along with a diversity
gain uncertainty term to address the exploration-exploitation trade-off. Both
intra- and inter-batch diversity are modeled to promote serendipity and
minimize redundancy. A dominance-based ranking procedure identifies
Pareto-optimal item sets, enabling adaptive and balanced selections at each
iteration. Experiments on a real-world dataset show that our approach
significantly improves diversity without sacrificing relevance, demonstrating
its potential to enhance user experience in large-scale recommendation
settings.

</details>


### [139] [DCN^2: Interplay of Implicit Collision Weights and Explicit Cross Layers for Large-Scale Recommendation](https://arxiv.org/abs/2506.21624)
*Blaž Škrlj,Yonatan Karni,Grega Gašperšič,Blaž Mramor,Yulia Stolin,Martin Jakomin,Jasna Urbančič,Yuval Dishi,Natalia Silberstein,Ophir Friedler,Assaf Klein*

Main category: cs.IR

TL;DR: DCN^2通过三个算法改进增强了DCNv2架构，解决了信息丢失、冲突管理和成对相似性建模等关键限制，并在实际推荐系统和公共基准测试中表现出优越的性能。


<details>
  <summary>Details</summary>
Motivation: DCNv2是一个强大的生产基线，并且是众多实际推荐系统不可或缺的一部分。其固有的效率和建模交互的能力通常会产生更简单且与计算要求更高的替代方案（如Deep FFM）相比更具竞争力的模型。

Method: 引入了三个对DCNv2架构的重大算法改进，包括解决Cross层中的信息丢失、通过可学习的查找级别权重隐式管理冲突以及使用自定义层显式建模成对相似性。

Result: DCN^2架构已在一个实时推荐系统中使用，在各种用例中每秒处理超过5亿次预测，并且优于DCNv2。

Conclusion: DCN^2在离线和在线实验中均优于DCNv2，并在四个公共基准数据集上展示了卓越的性能。

Abstract: The Deep and Cross architecture (DCNv2) is a robust production baseline and
is integral to numerous real-life recommender systems. Its inherent efficiency
and ability to model interactions often result in models that are both simpler
and highly competitive compared to more computationally demanding alternatives,
such as Deep FFMs. In this work, we introduce three significant algorithmic
improvements to the DCNv2 architecture, detailing their formulation and
behavior at scale. The enhanced architecture we refer to as DCN^2 is actively
used in a live recommender system, processing over 0.5 billion predictions per
second across diverse use cases where it out-performed DCNv2, both offline and
online (ab tests). These improvements effectively address key limitations
observed in the DCNv2, including information loss in Cross layers, implicit
management of collisions through learnable lookup-level weights, and explicit
modeling of pairwise similarities with a custom layer that emulates FFMs'
behavior. The superior performance of DCN^2 is also demonstrated on four
publicly available benchmark data sets.

</details>


### [140] [IRanker: Towards Ranking Foundation Model](https://arxiv.org/abs/2506.21638)
*Tao Feng,Zhigang Hua,Zijie Lei,Yan Xie,Shuang Yang,Bo Long,Jiaxuan You*

Main category: cs.IR

TL;DR: IRanker, a ranking foundation model using RL and iterative decoding, achieves state-of-the-art ranking performance and generalizes well, even improving generic LLM tasks.


<details>
  <summary>Details</summary>
Motivation: Ranking tasks are ubiquitous, but lack clear labels for supervision, posing challenges to developing a ranking FM. The goal is to unify ranking tasks using a single ranking foundation model (FM).

Method: A ranking FM framework with reinforcement learning (RL) and iterative decoding is proposed. The complex ranking task is decomposed into an iterative decoding process.

Result: IRanker-3B achieves state-of-the-art results on several datasets, surpasses larger models on certain datasets, and shows good generalization. It also enhances zero-shot LLM performance.

Conclusion: IRanker-3B achieves state-of-the-art results on several datasets and demonstrates good generalization on in-domain ranking tasks. It also outperforms the base model on out-of-domain generic LLM tasks.

Abstract: Ranking tasks are ubiquitous, encompassing applications such as
recommendation systems, LLM routing, and item re-ranking. We propose to unify
these tasks using a single ranking foundation model (FM), as it eliminates the
need for designing different models for each specific ranking task. However,
unlike general supervision tasks in LLMs, ranking tasks do not have clear
labels for supervision, posing great challenges to developing a ranking FM. To
overcome these challenges, we propose IRanker, a ranking FM framework with
reinforcement learning (RL) and iterative decoding. Our insight is to decompose
the complex ranking task into an iterative decoding process that eliminates the
worst candidate from the candidate pool step by step, which significantly
reduces the output combinatorial space and better utilizes the limited context
length during RL training. We meticulously train and comprehensively evaluate
an IRanker-3B model on nine datasets across three scenarios: recommendation,
routing, and passage ranking. The results show that a single IRanker-3B
achieves state-of-the-art results on several datasets compared to models of
similar size, and even surpasses the performance of larger models on certain
datasets. We further demonstrate the effectiveness of our RL design and the
robustness of the iterative mechanism across different LLM sizes. Moreover, we
conducted both in-domain and out-of-domain zero-shot generalization
experiments, which showed that IRanker-3B achieved good generalization on
in-domain ranking tasks compared to the base LLM by at least 5% improvement.
Surprisingly, on out-of-domain generic LLM tasks, IRanker-3B outperformed the
base model by at least 9% on GSM8K, IFEval, and MathQA. In addition, the
thoughts generated by IRanker-3B during training could further enhance
zero-shot LLM performance.

</details>


### [141] [HyReC: Exploring Hybrid-based Retriever for Chinese](https://arxiv.org/abs/2506.21913)
*Zunran Wang,Zheng Shenpeng,Wang Shenglan,Minghui Zhao,Zhonghua Li*

Main category: cs.IR

TL;DR: 本文提出了一种新的中文混合检索优化方法HyReC，并在C-MTEB上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 混合检索方法统一了基于密集向量和基于词典的检索，由于性能提升而在工业界受到了相当大的关注。然而，尽管它们的结果很有希望，但这些混合范式在中文检索环境中的应用在很大程度上仍未被探索。

Method: 创新性的端到端优化方法HyReC，专为中文混合检索设计。它集成了词语的语义联合，并使用GLAE促进词汇和密集检索之间的一致语义共享，同时尽量减少它们之间的干扰。此外，还包含一个标准化模块（NM）来促进检索方法之间的互惠互利。

Result: 通过将术语的语义联合集成到表示模型中来增强性能。GLAE（Global-Local-Aware Encoder）用于促进基于词典和密集检索之间的一致语义共享，同时最大限度地减少它们之间的干扰。标准化模块（NM）的加入，促进了检索方法之间的互利。

Conclusion: HyReC在C-MTEB检索基准上进行了评估，证明了其有效性。

Abstract: Hybrid-based retrieval methods, which unify dense-vector and lexicon-based
retrieval, have garnered considerable attention in the industry due to
performance enhancement. However, despite their promising results, the
application of these hybrid paradigms in Chinese retrieval contexts has
remained largely underexplored. In this paper, we introduce HyReC, an
innovative end-to-end optimization method tailored specifically for
hybrid-based retrieval in Chinese. HyReC enhances performance by integrating
the semantic union of terms into the representation model. Additionally, it
features the Global-Local-Aware Encoder (GLAE) to promote consistent semantic
sharing between lexicon-based and dense retrieval while minimizing the
interference between them. To further refine alignment, we incorporate a
Normalization Module (NM) that fosters mutual benefits between the retrieval
approaches. Finally, we evaluate HyReC on the C-MTEB retrieval benchmark to
demonstrate its effectiveness.

</details>


### [142] [ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation](https://arxiv.org/abs/2506.21931)
*Reza Yousefi Maragheh,Pratheek Vadla,Priyank Gupta,Kai Zhao,Aysenur Inan,Kehui Yao,Jianpeng Xu,Praveen Kanumala,Jason Cho,Sushant Kumar*

Main category: cs.IR

TL;DR: ARAG通过集成多代理协作机制到RAG流程中，显著提升了个性化推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于RAG的方法通常依赖于静态检索启发式方法，并且无法在动态推荐场景中捕获细致的用户偏好。

Method: 引入ARAG，一个用于个性化推荐的Agentic检索增强生成框架，它将多代理协作机制集成到RAG流程中。ARAG利用四个专门的基于LLM的代理：用户理解代理、自然语言推理（NLI）代理、上下文总结代理和项目排序代理。

Result: 实验结果表明，ARAG在三个数据集上优于标准RAG和基于近期的基线方法。

Conclusion: ARAG显著优于标准RAG和基于近期的基线方法，NDCG@5指标提升高达42.1%，Hit@5指标提升高达35.5%。

Abstract: Retrieval-Augmented Generation (RAG) has shown promise in enhancing
recommendation systems by incorporating external context into large language
model prompts. However, existing RAG-based approaches often rely on static
retrieval heuristics and fail to capture nuanced user preferences in dynamic
recommendation scenarios. In this work, we introduce ARAG, an Agentic
Retrieval-Augmented Generation framework for Personalized Recommendation, which
integrates a multi-agent collaboration mechanism into the RAG pipeline. To
better understand the long-term and session behavior of the user, ARAG
leverages four specialized LLM-based agents: a User Understanding Agent that
summarizes user preferences from long-term and session contexts, a Natural
Language Inference (NLI) Agent that evaluates semantic alignment between
candidate items retrieved by RAG and inferred intent, a context summary agent
that summarizes the findings of NLI agent, and an Item Ranker Agent that
generates a ranked list of recommendations based on contextual fit. We evaluate
ARAG accross three datasets. Experimental results demonstrate that ARAG
significantly outperforms standard RAG and recency-based baselines, achieving
up to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an
ablation study to analyse the effect by different components of ARAG. Our
findings highlight the effectiveness of integrating agentic reasoning into
retrieval-augmented recommendation and provide new directions for LLM-based
personalization.

</details>


### [143] [CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware Layout Design](https://arxiv.org/abs/2506.21934)
*Najmeh Forouzandehmehr,Reza Yousefi Maragheh,Sriram Kollipara,Kai Zhao,Topojoy Biswas,Evren Korpeoglu,Kannan Achan*

Main category: cs.IR

TL;DR: Introduces CAL-RAG, a retrieval-augmented agentic framework for content-aware layout generation, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: most existing approaches lack grounding in contextual design exemplars and fall short in handling semantic alignment and visual coherence.

Method: a retrieval-augmented, agentic framework for content-aware layout generation that integrates multimodal retrieval, large language models, and collaborative agentic reasoning

Result: CAL-RAG achieves state-of-the-art performance across multiple layout metrics -- including underlay effectiveness, element alignment, and overlap -- substantially outperforming strong baselines such as LayoutPrompter.

Conclusion: combining retrieval augmentation with agentic multi-step reasoning yields a scalable, interpretable, and high-fidelity solution for automated layout generation.

Abstract: Automated content-aware layout generation -- the task of arranging visual
elements such as text, logos, and underlays on a background canvas -- remains a
fundamental yet under-explored problem in intelligent design systems. While
recent advances in deep generative models and large language models (LLMs) have
shown promise in structured content generation, most existing approaches lack
grounding in contextual design exemplars and fall short in handling semantic
alignment and visual coherence. In this work we introduce CAL-RAG, a
retrieval-augmented, agentic framework for content-aware layout generation that
integrates multimodal retrieval, large language models, and collaborative
agentic reasoning. Our system retrieves relevant layout examples from a
structured knowledge base and invokes an LLM-based layout recommender to
propose structured element placements. A vision-language grader agent evaluates
the layout with visual metrics, and a feedback agent provides targeted
refinements, enabling iterative improvement. We implement our framework using
LangGraph and evaluate it on the PKU PosterLayout dataset, a benchmark rich in
semantic and structural variability. CAL-RAG achieves state-of-the-art
performance across multiple layout metrics -- including underlay effectiveness,
element alignment, and overlap -- substantially outperforming strong baselines
such as LayoutPrompter. These results demonstrate that combining retrieval
augmentation with agentic multi-step reasoning yields a scalable,
interpretable, and high-fidelity solution for automated layout generation.

</details>


### [144] [Literature-Grounded Novelty Assessment of Scientific Ideas](https://arxiv.org/abs/2506.22026)
*Simra Shahid,Marissa Radensky,Raymond Fok,Pao Siangliulue,Daniel S. Weld,Tom Hope*

Main category: cs.IR

TL;DR: Idea Novelty Checker, an LLM-based RAG framework, improves automated novelty evaluation by using a two-stage retrieve-then-rerank approach.


<details>
  <summary>Details</summary>
Motivation: Manual evaluation of novelty is labor-intensive, subjective, and impractical at scale.

Method: LLM-based retrieval-augmented generation (RAG) framework with a two-stage retrieve-then-rerank approach, incorporating expert-labeled examples.

Result: The Idea Novelty Checker achieves approximately 13% higher agreement than existing approaches. Ablation studies showcase the importance of the facet-based re-ranker.

Conclusion: Idea Novelty Checker achieves approximately 13% higher agreement than existing approaches in novelty evaluation. The facet-based re-ranker is important for identifying the most relevant literature.

Abstract: Automated scientific idea generation systems have made remarkable progress,
yet the automatic evaluation of idea novelty remains a critical and
underexplored challenge. Manual evaluation of novelty through literature review
is labor-intensive, prone to error due to subjectivity, and impractical at
scale. To address these issues, we propose the Idea Novelty Checker, an
LLM-based retrieval-augmented generation (RAG) framework that leverages a
two-stage retrieve-then-rerank approach. The Idea Novelty Checker first
collects a broad set of relevant papers using keyword and snippet-based
retrieval, then refines this collection through embedding-based filtering
followed by facet-based LLM re-ranking. It incorporates expert-labeled examples
to guide the system in comparing papers for novelty evaluation and in
generating literature-grounded reasoning. Our extensive experiments demonstrate
that our novelty checker achieves approximately 13% higher agreement than
existing approaches. Ablation studies further showcases the importance of the
facet-based re-ranker in identifying the most relevant literature for novelty
evaluation.

</details>


### [145] [Reward Balancing Revisited: Enhancing Offline Reinforcement Learning for Recommender Systems](https://arxiv.org/abs/2506.22112)
*Wenzheng Shu,Yanxiang Zeng,Yongxiang Tang,Teng Sha,Ning Luo,Yanhua Cheng,Xialong Liu,Fan Zhou,Peng Jiang*

Main category: cs.IR

TL;DR: R3S is an offline RL framework that balances intrinsic biases in world models and the diversity of policy recommendations.


<details>
  <summary>Details</summary>
Motivation: the simultaneous balancing of intrinsic biases in world models and the diversity of policy recommendations.

Method: innovative offline RL framework termed Reallocated Reward for Recommender Systems (R3S).

Result: integrating inherent model uncertainty to tackle the intrinsic fluctuations in reward predictions, we boost diversity for decision-making to align with a more interactive paradigm, incorporating extra penalizers with decay that deter actions leading to diminished state variety at both local and global scales.

Conclusion: R3S improves the accuracy of world models and efficiently harmonizes the heterogeneous preferences of the users.

Abstract: Offline reinforcement learning (RL) has emerged as a prevalent and effective
methodology for real-world recommender systems, enabling learning policies from
historical data and capturing user preferences. In offline RL, reward shaping
encounters significant challenges, with past efforts to incorporate prior
strategies for uncertainty to improve world models or penalize underexplored
state-action pairs. Despite these efforts, a critical gap remains: the
simultaneous balancing of intrinsic biases in world models and the diversity of
policy recommendations. To address this limitation, we present an innovative
offline RL framework termed Reallocated Reward for Recommender Systems (R3S).
By integrating inherent model uncertainty to tackle the intrinsic fluctuations
in reward predictions, we boost diversity for decision-making to align with a
more interactive paradigm, incorporating extra penalizers with decay that deter
actions leading to diminished state variety at both local and global scales.
The experimental results demonstrate that R3S improves the accuracy of world
models and efficiently harmonizes the heterogeneous preferences of the users.

</details>


### [146] [UiS-IAI@LiveRAG: Retrieval-Augmented Information Nugget-Based Generation of Responses](https://arxiv.org/abs/2506.22210)
*Weronika Łajewska,Ivica Kostric,Gabriel Iturra-Bocaz,Mariam Arustashvili,Krisztian Balog*

Main category: cs.IR

TL;DR: This paper proposes a modular pipeline using information nuggets to improve RAG's factual correctness, source attribution, and completeness. It focuses on query rewriting and context curation, finding that a few sub-query rewrites boost recall, but excessive document usage reduces effectiveness.


<details>
  <summary>Details</summary>
Motivation: Retrieval-augmented generation (RAG) faces challenges related to factual correctness, source attribution, and response completeness.

Method: modular pipeline that operates on information nuggets-minimal, atomic units of relevant information extracted from retrieved documents. This multistage pipeline encompasses query rewriting, passage retrieval and reranking, nugget detection and clustering, cluster ranking and summarization, and response fluency enhancement.

Result: Combining original queries with a few sub-query rewrites boosts recall, while increasing the number of documents used for reranking and generation beyond a certain point reduces effectiveness, without improving response quality.

Conclusion: Combining original queries with a few sub-query rewrites boosts recall, while increasing the number of documents used for reranking and generation beyond a certain point reduces effectiveness, without improving response quality.

Abstract: Retrieval-augmented generation (RAG) faces challenges related to factual
correctness, source attribution, and response completeness. The LiveRAG
Challenge hosted at SIGIR'25 aims to advance RAG research using a fixed corpus
and a shared, open-source LLM. We propose a modular pipeline that operates on
information nuggets-minimal, atomic units of relevant information extracted
from retrieved documents. This multistage pipeline encompasses query rewriting,
passage retrieval and reranking, nugget detection and clustering, cluster
ranking and summarization, and response fluency enhancement. This design
inherently promotes grounding in specific facts, facilitates source
attribution, and ensures maximum information inclusion within length
constraints. In this challenge, we extend our focus to also address the
retrieval component of RAG, building upon our prior work on multi-faceted query
rewriting. Furthermore, for augmented generation, we concentrate on improving
context curation capabilities, maximizing the breadth of information covered in
the response while ensuring pipeline efficiency. Our results show that
combining original queries with a few sub-query rewrites boosts recall, while
increasing the number of documents used for reranking and generation beyond a
certain point reduces effectiveness, without improving response quality.

</details>


### [147] [JointRank: Rank Large Set with Single Pass](https://arxiv.org/abs/2506.22262)
*Evgeny Dedov*

Main category: cs.IR

TL;DR: Proposes a model-agnostic method for fast reranking large sets that exceed a model input limits by partitioning candidate items into overlapping blocks, ranking independently and aggregating to construct a global ranking.


<details>
  <summary>Details</summary>
Motivation: Listwise rerankers are often limited in practice by model input size constraints, or by degraded quality when processing large sets.

Method: The method partitions candidate items into overlapping blocks, each of which is ranked independently in parallel. Implicit pairwise comparisons are then derived from these local rankings. Finally, these comparisons are aggregated to construct a global ranking using algorithms such as Winrate or PageRank.

Result: Achieves a nDCG@10 of 70.88 compared to the 57.68 for full-context listwise approach using gpt-4.1-mini, while reducing latency from 21 to 8 seconds.

Conclusion: The proposed method achieves a nDCG@10 of 70.88 compared to the 57.68 for full-context listwise approach using gpt-4.1-mini as long-context model, while reducing latency from 21 to 8 seconds.

Abstract: Efficiently ranking relevant items from large candidate pools is a
cornerstone of modern information retrieval systems -- such as web search,
recommendation, and retrieval-augmented generation. Listwise rerankers, which
improve relevance by jointly considering multiple candidates, are often limited
in practice: either by model input size constraints, or by degraded quality
when processing large sets. We propose a model-agnostic method for fast
reranking large sets that exceed a model input limits. The method first
partitions candidate items into overlapping blocks, each of which is ranked
independently in parallel. Implicit pairwise comparisons are then derived from
these local rankings. Finally, these comparisons are aggregated to construct a
global ranking using algorithms such as Winrate or PageRank. Experiments on
TREC DL-2019 show that our method achieves an nDCG@10 of 70.88 compared to the
57.68 for full-context listwise approach using gpt-4.1-mini as long-context
model, while reducing latency from 21 to 8 seconds.
  The implementation of the algorithm and the experiments is available in the
repository: https://github.com/V3RGANz/jointrank

</details>


### [148] [Education-Oriented Graph Retrieval-Augmented Generation for Learning Path Recommendation](https://arxiv.org/abs/2506.22303)
*Xinghe Cheng,Zihan Zhang,Jiapu Wang,Liangda Fang,Chaobo He,Quanlong Guan,Shirui Pan,Weiqi Luo*

Main category: cs.IR

TL;DR: propose Discrimination Learning Enhances Learning Path Recommendation (DLELP), which enhances learning path recommendations by incorporating both prerequisite and similarity relationships between knowledge concepts to address the limitations of relying solely on prerequisite relationships, and achieves state-of-the-art performance


<details>
  <summary>Details</summary>
Motivation: existing methods primarily rely on prerequisite relationships, which present two major limitations: 1) Many educational datasets do not explicitly provide prerequisite relationships between knowledge concepts, hindering the application of current learning path recommendation methods. 2) Relying solely on prerequisite relationships as the sole knowledge structure can impede learning progress and negatively impact student outcomes.

Method: incorporates both prerequisite and similarity relationships between knowledge concepts. Specifically, introduce a knowledge concept structure graph generation module that adaptively constructs knowledge concept structure graphs for different educational datasets. propose a Discrimination Learning-driven Reinforcement Learning (DLRL) framework, which mitigates the issue of blocked learning paths

Result: achieves state-of-the-art performance. provides interpretable reasoning for the recommended learning paths.

Conclusion: achieves state-of-the-art performance and provides interpretable reasoning for the recommended learning paths.

Abstract: Learning path recommendation seeks to provide learners with a structured
sequence of learning items (e.g., knowledge concepts or exercises) to optimize
their learning efficiency. Despite significant efforts in this area, most
existing methods primarily rely on prerequisite relationships, which present
two major limitations: 1) Many educational datasets do not explicitly provide
prerequisite relationships between knowledge concepts, hindering the
application of current learning path recommendation methods. 2) Relying solely
on prerequisite relationships as the sole knowledge structure can impede
learning progress and negatively impact student outcomes. To address these
challenges, we propose a novel approach, Discrimination Learning Enhances
Learning Path Recommendation (DLELP), which enhances learning path
recommendations by incorporating both prerequisite and similarity relationships
between knowledge concepts. Specifically, we introduce a knowledge concept
structure graph generation module that adaptively constructs knowledge concept
structure graphs for different educational datasets, significantly improving
the generalizability of learning path recommendation methods. We then propose a
Discrimination Learning-driven Reinforcement Learning (DLRL) framework, which
mitigates the issue of blocked learning paths, further enhancing the efficacy
of learning path recommendations. Finally, we conduct extensive experiments on
three benchmark datasets, demonstrating that our method not only achieves
state-of-the-art performance but also provides interpretable reasoning for the
recommended learning paths.

</details>


### [149] [HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval](https://arxiv.org/abs/2506.22356)
*Kevin Duh,Eugene Yang,Orion Weller,Andrew Yates,Dawn Lawrie*

Main category: cs.IR

TL;DR: The HLTCOE LiveRAG submission, based on the GPT-researcher framework and a ColBERT retrieval system, achieved 5th place in LiveRAG with a correctness score of 1.07.


<details>
  <summary>Details</summary>
Motivation: This paper introduces the HLTCOE LiveRAG submission.

Method: The system used a ColBERT bi-encoder architecture for retrieval, Qwen2.5-7B-Instruct for query generation, m2-bert-80M-8k-retrieval for filtering, and Falcon3-10B for answer generation, leveraging the GPT-researcher framework.

Result: The system achieved a correctness score of 1.07 in the LiveRAG evaluation.

Conclusion: The system achieved 5th place in the LiveRAG automatic evaluation for correctness with a score of 1.07.

Abstract: The HLTCOE LiveRAG submission utilized the GPT-researcher framework for
researching the context of the question, filtering the returned results, and
generating the final answer. The retrieval system was a ColBERT bi-encoder
architecture, which represents a passage with many dense tokens. Retrieval used
a local, compressed index of the FineWeb10-BT collection created with PLAID-X,
using a model fine-tuned for multilingual retrieval. Query generation from
context was done with Qwen2.5-7B-Instruct, while filtering was accomplished
with m2-bert-80M-8k-retrieval. Up to nine passages were used as context to
generate an answer using Falcon3-10B. This system placed 5th in the LiveRAG
automatic evaluation for correctness with a score of 1.07.

</details>


### [150] [Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and Measurement](https://arxiv.org/abs/2506.22372)
*Maryam Mousavian,Zahra Abbasiantaeb,Mohammad Aliannejadi,Fabio Crestani*

Main category: cs.IR

TL;DR: 利用大型语言模型来检测和衡量段落排序中的性别偏见，并引入了一种新的性别公平性指标，名为 Class-wise Weighted Exposure (CWEx)。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理 (NLP) 和信息检索 (IR) 系统中存在的社会偏见是一个持续存在的挑战，这突显了开发稳健的方法来识别和评估此类偏见的重要性。现有的性别公平性指标依赖于基于词汇和频率的度量，导致各种限制，例如，遗漏细微的性别差异。

Method: 利用大型语言模型 (LLM) 来检测和衡量段落排序中的性别偏见。引入了一种新的性别公平性指标，名为 Class-wise Weighted Exposure (CWEx)。

Result: 在各种排序模型上进行的大量实验结果表明，与之前的指标相比，所提出的指标可以更详细地评估公平性，并且与人工标签的对齐性更高（Grep-BiasIR 为 58.77%，MSMGenderBias 为 18.51%，使用 Cohen Kappa 协议测量），有效地区分了排序中的性别偏见。

Conclusion: 集成了 LLM 驱动的偏差检测、改进的公平性指标以及针对已建立数据集的性别偏差注释，这项工作提供了一个更强大的框架，用于分析和减轻 IR 系统中的偏差。

Abstract: The presence of social biases in Natural Language Processing (NLP) and
Information Retrieval (IR) systems is an ongoing challenge, which underlines
the importance of developing robust approaches to identifying and evaluating
such biases. In this paper, we aim to address this issue by leveraging Large
Language Models (LLMs) to detect and measure gender bias in passage ranking.
Existing gender fairness metrics rely on lexical- and frequency-based measures,
leading to various limitations, e.g., missing subtle gender disparities.
Building on our LLM-based gender bias detection method, we introduce a novel
gender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to
address existing limitations. To measure the effectiveness of our proposed
metric and study LLMs' effectiveness in detecting gender bias, we annotate a
subset of the MS MARCO Passage Ranking collection and release our new gender
bias collection, called MSMGenderBias, to foster future research in this area.
Our extensive experimental results on various ranking models show that our
proposed metric offers a more detailed evaluation of fairness compared to
previous metrics, with improved alignment to human labels (58.77% for
Grep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa
agreement), effectively distinguishing gender bias in ranking. By integrating
LLM-driven bias detection, an improved fairness metric, and gender bias
annotations for an established dataset, this work provides a more robust
framework for analyzing and mitigating bias in IR systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [151] [APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization](https://arxiv.org/abs/2506.21655)
*Minjie Hong,Zirun Guo,Yan Xia,Zehan Wang,Ziang Zhang,Tao Jin,Zhou Zhao*

Main category: cs.LG

TL;DR: This paper introduces View-R1-3B, a new MLLM that improves reasoning using a novel RL approach (APO with DADS and STCR) without hurting general task performance, outperforming larger models.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with complex reasoning, and applying Reinforcement Learning (RL) to MLLMs often leads to performance drops on general tasks and overthinking.

Method: The paper proposes Asymmetric Policy Optimization (APO), which includes Difficulty-Adaptive Divergence Shaping (DADS) to dynamically adjust the KL divergence weight based on sample difficulty and Suboptimal Trajectory Complexity Regularization (STCR) to penalize overly long responses.

Result: View-R1-3B shows an average 7% gain over the base model and outperforms larger MLLMs (7-11B) on various reasoning benchmarks while maintaining consistent improvement on general tasks.

Conclusion: The paper introduces View-R1-3B, an MLLM based on Qwen2.5-VL-3B, which significantly enhances reasoning capabilities without sacrificing performance on general tasks. This is achieved through Asymmetric Policy Optimization (APO) using Difficulty-Adaptive Divergence Shaping (DADS) for positive samples and Suboptimal Trajectory Complexity Regularization (STCR) for negative samples.

Abstract: Multimodal Large Language Models (MLLMs) are powerful at integrating diverse
data, but they often struggle with complex reasoning. While Reinforcement
learning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky.
Common issues include a drop in performance on general tasks and the generation
of overly detailed or "overthinking" reasoning. Our work investigates how the
KL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric
Policy Optimization (APO) to address these issues, which divides the sampled
responses into positive and negative groups. For positive samples,
Difficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically
adjust the KL divergence weight based on their difficulty. This method prevents
policy entropy from dropping sharply, improves training stability, utilizes
samples better, and preserves the model's existing knowledge. For negative
samples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to
penalize overly long responses. This helps mitigate overthinking and encourages
more concise reasoning while preserving the model's explorative capacity. We
apply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B
significantly enhances reasoning capabilities, showing an average 7\% gain over
the base model and outperforming larger MLLMs (7-11B) on various reasoning
benchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade
on general tasks, View-R1-3B maintains consistent improvement, demonstrating
superior generalization. These results highlight the effectiveness and broad
applicability of our DADS and STCR techniques for advancing complex multimodal
reasoning in MLLMs. The code will be made available at
https://github.com/Indolent-Kawhi/View-R1.

</details>


### [152] [Risk-Averse Total-Reward Reinforcement Learning](https://arxiv.org/abs/2506.21683)
*Xihong Su,Jia Lin Hau,Gersi Doko,Kishan Panaganti,Marek Petrik*

Main category: cs.LG

TL;DR: 提出了一种Q学习算法来解决风险规避的总奖励马尔可夫决策过程问题，该算法在数值实验中表现出快速收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于模型的风险度量算法（如熵风险度量（ERM）和熵值风险（EVaR））在小问题中有效，但需要完全访问转移概率。

Method: 提出了一个Q学习算法

Result: 在表格域的数值结果中展示了该算法快速可靠的收敛性。

Conclusion: 该论文提出了一个Q学习算法，用于计算总奖励ERM和EVaR目标的最优平稳策略，并在表格域的数值结果中展示了该算法快速可靠的收敛性。

Abstract: Risk-averse total-reward Markov Decision Processes (MDPs) offer a promising
framework for modeling and solving undiscounted infinite-horizon objectives.
Existing model-based algorithms for risk measures like the entropic risk
measure (ERM) and entropic value-at-risk (EVaR) are effective in small
problems, but require full access to transition probabilities. We propose a
Q-learning algorithm to compute the optimal stationary policy for total-reward
ERM and EVaR objectives with strong convergence and performance guarantees. The
algorithm and its optimality are made possible by ERM's dynamic consistency and
elicitability. Our numerical results on tabular domains demonstrate quick and
reliable convergence of the proposed Q-learning algorithm to the optimal
risk-averse value function.

</details>


### [153] [Unimodal Strategies in Density-Based Clustering](https://arxiv.org/abs/2506.21695)
*Oron Nir,Jay Tenenbaum,Ariel Shamir*

Main category: cs.LG

TL;DR: This paper introduces a new approach to efficiently determine the neighborhood radius in density-based clustering by exploiting the unimodal relationship between the number of clusters and the radius. The approach is validated on large-scale, high-dimensional data across NLP, Audio, and Computer Vision tasks.


<details>
  <summary>Details</summary>
Motivation: Density-based clustering methods often surpass centroid-based counterparts, when addressing data with noise or arbitrary data distributions common in real-world problems. In this study, we reveal a key property intrinsic to density-based clustering methods regarding the relation between the number of clusters and the neighborhood radius of core points - we empirically show that it is nearly unimodal, and support this claim theoretically in a specific setting.

Method: We leverage the unimodal property to devise new strategies for finding appropriate values for the radius more efficiently based on the Ternary Search algorithm.

Result: We validate our methodology through extensive applications across a range of high-dimensional, large-scale NLP, Audio, and Computer Vision tasks, demonstrating its practical effectiveness and robustness.

Conclusion: This work offers a significant advancement in parameter control for density-based clustering and also broadens the understanding regarding the relations between their guiding parameters.

Abstract: Density-based clustering methods often surpass centroid-based counterparts,
when addressing data with noise or arbitrary data distributions common in
real-world problems. In this study, we reveal a key property intrinsic to
density-based clustering methods regarding the relation between the number of
clusters and the neighborhood radius of core points - we empirically show that
it is nearly unimodal, and support this claim theoretically in a specific
setting. We leverage this property to devise new strategies for finding
appropriate values for the radius more efficiently based on the Ternary Search
algorithm. This is especially important for large scale data that is
high-dimensional, where parameter tuning is computationally intensive. We
validate our methodology through extensive applications across a range of
high-dimensional, large-scale NLP, Audio, and Computer Vision tasks,
demonstrating its practical effectiveness and robustness. This work not only
offers a significant advancement in parameter control for density-based
clustering but also broadens the understanding regarding the relations between
their guiding parameters. Our code is available at
https://github.com/oronnir/UnimodalStrategies.

</details>


### [154] [$\textrm{ODE}_t \left(\textrm{ODE}_l \right)$: Shortcutting the Time and Length in Diffusion and Flow Models for Faster Sampling](https://arxiv.org/abs/2506.21714)
*Denis Gudovskiy,Wenzhao Zheng,Tomoyuki Okuno,Yohei Nakata,Kurt Keutzer*

Main category: cs.LG

TL;DR: This paper introduces a new approach, ODE_t(ODE_l), for improving the efficiency of CNFs and DMs by dynamically controlling the quality-complexity tradeoff, reducing latency and memory usage in image generation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing CNFs and DMs require multiple iterations to solve an ODE with high computational complexity, and most methods focus on reducing time steps. This work explores dynamically controlling the quality-complexity tradeoff in terms of time steps and network length.

Method: The authors rewire blocks in a transformer-based architecture to solve an inner discretized ODE with respect to its length and employ time- and length-wise consistency terms during flow matching training.

Result: Experiments on CelebA-HQ and ImageNet show up to 3x latency reduction and a 3.5 FID score improvement compared to previous state-of-the-art methods.

Conclusion: The proposed ODE_t(ODE_l) approach reduces latency and memory usage, achieving up to 3x latency reduction and 3.5 FID score improvement compared to previous state-of-the-art methods on CelebA-HQ and ImageNet.

Abstract: Recently, continuous normalizing flows (CNFs) and diffusion models (DMs) have
been studied using the unified theoretical framework. Although such models can
generate high-quality data points from a noise distribution, the sampling
demands multiple iterations to solve an ordinary differential equation (ODE)
with high computational complexity. Most existing methods focus on reducing the
number of time steps during the sampling process to improve efficiency. In this
work, we explore a complementary direction in which the quality-complexity
tradeoff can be dynamically controlled in terms of time steps and in the length
of the neural network. We achieve this by rewiring the blocks in the
transformer-based architecture to solve an inner discretized ODE w.r.t. its
length. Then, we employ time- and length-wise consistency terms during flow
matching training, and as a result, the sampling can be performed with an
arbitrary number of time steps and transformer blocks. Unlike others, our
$\textrm{ODE}_t \left(\textrm{ODE}_l \right)$ approach is solver-agnostic in
time dimension and decreases both latency and memory usage. Compared to the
previous state of the art, image generation experiments on CelebA-HQ and
ImageNet show a latency reduction of up to $3\times$ in the most efficient
sampling mode, and a FID score improvement of up to $3.5$ points for
high-quality sampling. We release our code and model weights with fully
reproducible experiments.

</details>


### [155] [REDELEX: A Framework for Relational Deep Learning Exploration](https://arxiv.org/abs/2506.22199)
*Jakub Peleška,Gustav Šír*

Main category: cs.LG

TL;DR: 本文提出了REDELEX框架，用于评估RDL模型在各种RDB上的性能，结果表明RDL通常优于经典方法，并且模型复杂性、数据库大小和结构属性是影响性能的关键因素。


<details>
  <summary>Details</summary>
Motivation: 关系数据库（RDB）被广泛认为是存储结构化信息的黄金标准。因此，利用这种数据格式的预测任务具有重要的应用前景。最近，关系深度学习（RDL）作为一种新的范例出现，其中RDB被概念化为图结构，从而能够应用各种图神经架构来有效解决这些任务。然而，鉴于其新颖性，缺乏对各种RDL模型的性能与底层RDB的特性之间关系的分析。

Method: 我们提出了REDELEX——一个综合探索框架，用于评估超过70个RDB上不同复杂度的RDL模型。

Result: 通过与经典方法的关键代表进行基准测试，我们确认了RDL通常更优越的性能，同时提供了对影响性能的主要因素的见解，包括模型复杂性、数据库大小及其结构属性。

Conclusion: RDL表现通常优于传统方法，并且模型复杂性、数据库大小及其结构属性是影响性能的主要因素。

Abstract: Relational databases (RDBs) are widely regarded as the gold standard for
storing structured information. Consequently, predictive tasks leveraging this
data format hold significant application promise. Recently, Relational Deep
Learning (RDL) has emerged as a novel paradigm wherein RDBs are conceptualized
as graph structures, enabling the application of various graph neural
architectures to effectively address these tasks. However, given its novelty,
there is a lack of analysis into the relationships between the performance of
various RDL models and the characteristics of the underlying RDBs.
  In this study, we present REDELEX$-$a comprehensive exploration framework for
evaluating RDL models of varying complexity on the most diverse collection of
over 70 RDBs, which we make available to the community. Benchmarked alongside
key representatives of classic methods, we confirm the generally superior
performance of RDL while providing insights into the main factors shaping
performance, including model complexity, database sizes and their structural
properties.

</details>


### [156] [Performance Prediction for Large Systems via Text-to-Text Regression](https://arxiv.org/abs/2506.21718)
*Yash Akhauri,Bryan Lewandowski,Cheng-Hsi Lin,Adrian N. Reyes,Grant C. Forbes,Arissa Wongpanich,Bangding Yang,Mohamed S. Abdelfattah,Sagi Perel,Xingyou Song*

Main category: cs.LG

TL;DR: 提出了一种通用的、可扩展的文本到文本回归方法，用于预测大规模系统的指标结果，并在Google的Borg集群上取得了显著的成果。


<details>
  <summary>Details</summary>
Motivation: 传统表格回归方法在处理复杂的系统数据（如配置文件或系统日志）时存在困难，特征工程通常不可行。

Method: 使用了一个60M参数的encoder-decoder模型，从随机初始化开始训练。

Result: 该模型在整个集群中实现了高达0.99（平均0.9）的秩相关性，并且MSE比表格方法低100倍。该模型还可以轻松适应新的任务，仅需少量示例即可捕获复杂结果分布的密度。

Conclusion: text-to-text regression模型在预测Borg的资源效率方面表现出色，优于传统表格方法，并具有良好的泛化能力。

Abstract: In many industries, predicting metric outcomes of large systems is a
fundamental problem, driven largely by traditional tabular regression. However,
such methods struggle on complex systems data in the wild such as configuration
files or system logs, where feature engineering is often infeasible. We propose
text-to-text regression as a general, scalable alternative. For predicting
resource efficiency on Borg, Google's massive compute cluster scheduling
system, a 60M parameter encoder-decoder, trained from random initialization,
achieves up to a near perfect 0.99 (0.9 average) rank correlation across the
entire fleet, and 100x lower MSE than tabular approaches. The model also easily
adapts to new tasks in only 500 few-shot examples and captures the densities of
complex outcome distributions. Ablation studies highlight the importance of
using encoders, increasing sequence length, and the model's inherent
uncertainty quantification. These findings pave the way for universal
simulators of real-world outcomes.

</details>


### [157] [Federated Item Response Theory Models](https://arxiv.org/abs/2506.21744)
*Biying Zhou,Nanyu Luo,Feng Ji*

Main category: cs.LG

TL;DR: 提出了一种新的框架，联邦项目反应理论 (IRT)，以实现对传统 IRT 模型的估计，同时具有额外的隐私性，允许以分布式方式进行估计，而不会损失估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的 IRT 估计需要将所有个人原始反应数据集中在一个地方，从而可能导致隐私问题。联邦学习是计算机科学和机器学习中一个新兴领域，具有隐私保护和分布式计算的附加功能。

Method: 联邦项目反应理论 (IRT)

Result: 数值实验证实，FedIRT 实现了与使用流行的 R 包的标准 IRT 估计相似的统计精度，同时提供了关键优势：隐私保护和降低的通信成本。我们还通过真实世界的考试数据集验证了 FedIRT 的实用性，证明了其在实际教育环境中的有效性。

Conclusion: FedIRT在不牺牲精度或安全性的前提下，将 IRT 的适用性扩展到分布式环境，例如多学校评估。

Abstract: Item Response Theory (IRT) models have been widely used to estimate
respondents' latent abilities and calibrate items' difficulty. Traditional IRT
estimation requires all individual raw response data to be centralized in one
place, thus potentially causing privacy issues. Federated learning is an
emerging field in computer science and machine learning with added features of
privacy protection and distributed computing. To integrate the advances from
federated learning with modern psychometrics, we propose a novel framework,
Federated Item Response Theory (IRT), to enable estimating traditional IRT
models with additional privacy, allowing estimation in a distributed manner
without losing estimation accuracy.
  Our numerical experiments confirm that FedIRT achieves statistical accuracy
similar to standard IRT estimation using popular R packages, while offering
critical advantages: privacy protection and reduced communication costs. We
also validate FedIRT's utility through a real-world exam dataset, demonstrating
its effectiveness in realistic educational contexts. This new framework extends
IRT's applicability to distributed settings, such as multi-school assessments,
without sacrificing accuracy or security. To support practical adoption, we
provide an open-ource R package, FedIRT, implementing the framework for the
two-parameter logistic (2PL) and partial credit models (PCM).

</details>


### [158] [Gradient-Based Neuroplastic Adaptation for Concurrent Optimization of Neuro-Fuzzy Networks](https://arxiv.org/abs/2506.21771)
*John Wesley Hostetter,Min Chi*

Main category: cs.LG

TL;DR: We propose a novel application-independent approach called gradient-based neuroplastic adaptation for the concurrent optimization of NFNs' parameters and structure.


<details>
  <summary>Details</summary>
Motivation: systematic design process of NFNs remains a challenge. Existing work will often sequentially build NFNs by inefficiently isolating parametric and structural identification, leading to a premature commitment to brittle and subpar architecture

Method: gradient-based neuroplastic adaptation for the concurrent optimization of NFNs' parameters and structure

Result: settings previously unapproachable for NFNs are now accessible, such as the online reinforcement learning of NFNs for vision-based tasks

Conclusion: concurrently optimizing NFNs is effective and can be trained by online reinforcement learning to play challenging scenarios in DOOM

Abstract: Neuro-fuzzy networks (NFNs) are transparent, symbolic, and universal function
approximations that perform as well as conventional neural architectures, but
their knowledge is expressed as linguistic IF-THEN rules. Despite these
advantages, their systematic design process remains a challenge. Existing work
will often sequentially build NFNs by inefficiently isolating parametric and
structural identification, leading to a premature commitment to brittle and
subpar architecture. We propose a novel application-independent approach called
gradient-based neuroplastic adaptation for the concurrent optimization of NFNs'
parameters and structure. By recognizing that NFNs' parameters and structure
should be optimized simultaneously as they are deeply conjoined, settings
previously unapproachable for NFNs are now accessible, such as the online
reinforcement learning of NFNs for vision-based tasks. The effectiveness of
concurrently optimizing NFNs is empirically shown as it is trained by online
reinforcement learning to proficiently play challenging scenarios from a
vision-based video game called DOOM.

</details>


### [159] [M3PO: Massively Multi-Task Model-Based Policy Optimization](https://arxiv.org/abs/2506.21782)
*Aditya Narendra,Dmitry Makarov,Aleksandr Panov*

Main category: cs.LG

TL;DR: M3PO是一种可扩展的基于模型的强化学习框架，它集成了隐式世界模型和混合探索策略，以实现高效和稳健的策略优化，并在多个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于模型的方法（如DreamerV3）依赖于忽略控制中心表示的像素级生成模型，而无模型方法（如PPO）则存在样本复杂性高和探索能力弱的问题。M3PO旨在解决单任务设置中的样本效率低下以及多任务领域中的泛化能力差的问题。

Method: M3PO集成了隐式世界模型（训练用于预测任务结果而不进行观察重建）与混合探索策略（结合了基于模型的规划和无模型的，不确定性驱动的奖励）。

Result: M3PO通过使用基于模型和无模型的价值估计之间的差异来指导探索，同时通过信任区域优化器保持稳定的策略更新，从而消除了先前方法中的偏差-方差权衡。

Conclusion: M3PO提供了一种高效且稳健的替代现有基于模型的策略优化方法，并在多个基准测试中实现了最先进的性能。

Abstract: We introduce Massively Multi-Task Model-Based Policy Optimization (M3PO), a
scalable model-based reinforcement learning (MBRL) framework designed to
address sample inefficiency in single-task settings and poor generalization in
multi-task domains. Existing model-based approaches like DreamerV3 rely on
pixel-level generative models that neglect control-centric representations,
while model-free methods such as PPO suffer from high sample complexity and
weak exploration. M3PO integrates an implicit world model, trained to predict
task outcomes without observation reconstruction, with a hybrid exploration
strategy that combines model-based planning and model-free uncertainty-driven
bonuses. This eliminates the bias-variance trade-off in prior methods by using
discrepancies between model-based and model-free value estimates to guide
exploration, while maintaining stable policy updates through a trust-region
optimizer. M3PO provides an efficient and robust alternative to existing
model-based policy optimization approaches and achieves state-of-the-art
performance across multiple benchmarks.

</details>


### [160] [Multi-task parallelism for robust pre-training of graph foundation models on multi-source, multi-fidelity atomistic modeling data](https://arxiv.org/abs/2506.21788)
*Massimiliano Lupo Pasini,Jong Youl Choi,Pei Zhang,Kshitij Mehta,Rylie Weaver,Ashwin M. Aji,Karl W. Schulz,Jorda Polo,Prasanna Balaprakash*

Main category: cs.LG

TL;DR: 提出了一种多任务并行方法，使用HydraGNN架构在多个超级计算机上进行了训练和测试，实现了高效扩展。


<details>
  <summary>Details</summary>
Motivation: 为了解决预训练期间处理多源、多保真度数据的挑战，最近的研究采用了多任务学习。

Method: 提出了一种多任务并行方法，该方法利用GPU加速将每个头分布在计算资源中。

Result: 在来自五个数据集的超过 2400 万个结构上进行了训练，并在 Perlmutter、Aurora 和 Frontier 超级计算机上进行了测试，

Conclusion: 该方法在三种不同的超级计算机架构上实现了高效扩展，证明了其有效性。

Abstract: Graph foundation models using graph neural networks promise sustainable,
efficient atomistic modeling. To tackle challenges of processing multi-source,
multi-fidelity data during pre-training, recent studies employ multi-task
learning, in which shared message passing layers initially process input
atomistic structures regardless of source, then route them to multiple decoding
heads that predict data-specific outputs. This approach stabilizes pre-training
and enhances a model's transferability to unexplored chemical regions.
Preliminary results on approximately four million structures are encouraging,
yet questions remain about generalizability to larger, more diverse datasets
and scalability on supercomputers. We propose a multi-task parallelism method
that distributes each head across computing resources with GPU acceleration.
Implemented in the open-source HydraGNN architecture, our method was trained on
over 24 million structures from five datasets and tested on the Perlmutter,
Aurora, and Frontier supercomputers, demonstrating efficient scaling on all
three highly heterogeneous super-computing architectures.

</details>


### [161] [Why Neural Network Can Discover Symbolic Structures with Gradient-based Training: An Algebraic and Geometric Foundation for Neurosymbolic Reasoning](https://arxiv.org/abs/2506.21797)
*Peihao Wang,Zhangyang Wang*

Main category: cs.LG

TL;DR: developed a theoretical framework that explains how discrete symbolic structures can emerge naturally from continuous neural network training dynamics


<details>
  <summary>Details</summary>
Motivation: We develop a theoretical framework that explains how discrete symbolic structures can emerge naturally from continuous neural network training dynamics.

Method: lifting neural parameters to a measure space and modeling training as Wasserstein gradient flow

Result: the parameter measure $\mu_t$ undergoes two concurrent phenomena: (1) a decoupling of the gradient flow into independent optimization trajectories over some potential functions, and (2) a progressive contraction on the degree of freedom. These potentials encode algebraic constraints relevant to the task and act as ring homomorphisms under a commutative semi-ring structure on the measure space. As training progresses, the network transitions from a high-dimensional exploration to compositional representations that comply with algebraic operations and exhibit a lower degree of freedom. We further establish data scaling laws for realizing symbolic tasks, linking representational capacity to the group invariance that facilitates symbolic solutions.

Conclusion: This framework charts a principled foundation for understanding and designing neurosymbolic systems that integrate continuous learning with discrete algebraic reasoning.

Abstract: We develop a theoretical framework that explains how discrete symbolic
structures can emerge naturally from continuous neural network training
dynamics. By lifting neural parameters to a measure space and modeling training
as Wasserstein gradient flow, we show that under geometric constraints, such as
group invariance, the parameter measure $\mu_t$ undergoes two concurrent
phenomena: (1) a decoupling of the gradient flow into independent optimization
trajectories over some potential functions, and (2) a progressive contraction
on the degree of freedom. These potentials encode algebraic constraints
relevant to the task and act as ring homomorphisms under a commutative
semi-ring structure on the measure space. As training progresses, the network
transitions from a high-dimensional exploration to compositional
representations that comply with algebraic operations and exhibit a lower
degree of freedom. We further establish data scaling laws for realizing
symbolic tasks, linking representational capacity to the group invariance that
facilitates symbolic solutions. This framework charts a principled foundation
for understanding and designing neurosymbolic systems that integrate continuous
learning with discrete algebraic reasoning.

</details>


### [162] [The Cost of Avoiding Backpropagation](https://arxiv.org/abs/2506.21833)
*Kunjal Panchal,Sunav Choudhary,Yuriy Brun,Hui Guan*

Main category: cs.LG

TL;DR: 在大型语言和视觉语言模型上的经验实验表明，使用检查点的BP优于FmAD和ZO变体，在相当的内存使用情况下，准确率提高了31.1%，收敛速度提高了34.8%，计算量减少了3.8倍。


<details>
  <summary>Details</summary>
Motivation: 正向模式自动微分（FmAD）和零阶（ZO）优化已被提议作为梯度计算中反向传播（BP）的内存效率替代方案，尤其是在低资源环境中。然而，由于两个关键差距，它们的实际好处仍不清楚：缺乏与内存效率的BP变体（如激活检查点）的比较，以及缺乏统一的理论分析。

Method: 对反向传播、前向模式自动微分和零阶方法进行了全面的理论和实证比较。

Result: 理论分析表明，虽然FmAD和ZO可以减少内存使用，但与使用检查点的BP相比，它们在准确性、收敛速度和计算方面会产生显著的成本。随着模型变大或约束扰动预算，这些缺点会变得更糟。

Conclusion: 反向传播与检查点仍然是在内存受限设置下进行模型训练的最有效策略。

Abstract: Forward-mode automatic differentiation (FmAD) and zero-order (ZO)
optimization have been proposed as memory-efficient alternatives to
backpropagation (BP) for gradient computation, especially in low-resource
settings. However, their practical benefits remain unclear due to two key gaps:
a lack of comparison against memory-efficient BP variants, such as activation
checkpointing, and a lack of a unified theoretical analysis. This work presents
a comprehensive theoretical and empirical comparison of BP, FmAD, and ZO
methods. Our theoretical analysis shows that while FmAD, and ZO can reduce
memory usage, they incur significant costs in accuracy, convergence speed, and
computation compared to BP with checkpointing. These drawbacks worsen with
larger models or constrained perturbation budgets. Empirical experiments on
large language and vision-language models show that BP with checkpointing
outperforms FmAD and ZO variants, including those enhanced with variance
reduction, achieving up to 31.1% higher accuracy, 34.8% faster convergence, and
3.8x fewer computations at comparable memory usage. Our results highlight
fundamental limitations of FmAD and ZO, and reaffirm BP with checkpointing as
the most effective strategy for model training under memory-constrained
settings. Our code is available at
https://github.com/Astuary/The_Cost_of_Avoiding_Backpropagation.

</details>


### [163] [Koopman operator-based discussion on partial observation in stochastic systems](https://arxiv.org/abs/2506.21844)
*Jun Ohkubo*

Main category: cs.LG

TL;DR: discusses the effects of partial observation in stochastic systems using the Koopman operator theory


<details>
  <summary>Details</summary>
Motivation: It is sometimes difficult to achieve a complete observation for a full set of observables, and partial observations are necessary. For deterministic systems, the Mori-Zwanzig formalism provides a theoretical framework for handling partial observations. Recently, data-driven algorithms based on the Koopman operator theory have made significant progress, and there is a discussion to connect the Mori-Zwanzig formalism with the Koopman operator theory.

Method: the Koopman operator theory

Result: The discussion clarifies the importance of distinguishing the state space and the function space in stochastic systems.

Conclusion: Even in stochastic systems, the delay embedding technique is beneficial for partial observation, and several numerical experiments showed a power-law behavior of the accuracy for the amplitude of the additive noise. We also discuss the relation between the exponent of the power-law behavior and the effects of partial observation.

Abstract: It is sometimes difficult to achieve a complete observation for a full set of
observables, and partial observations are necessary. For deterministic systems,
the Mori-Zwanzig formalism provides a theoretical framework for handling
partial observations. Recently, data-driven algorithms based on the Koopman
operator theory have made significant progress, and there is a discussion to
connect the Mori-Zwanzig formalism with the Koopman operator theory. In this
work, we discuss the effects of partial observation in stochastic systems using
the Koopman operator theory. The discussion clarifies the importance of
distinguishing the state space and the function space in stochastic systems.
Even in stochastic systems, the delay embedding technique is beneficial for
partial observation, and several numerical experiments showed a power-law
behavior of the accuracy for the amplitude of the additive noise. We also
discuss the relation between the exponent of the power-law behavior and the
effects of partial observation.

</details>


### [164] [A Survey of Continual Reinforcement Learning](https://arxiv.org/abs/2506.21872)
*Chaofan Pan,Xin Yang,Yanhua Li,Wei Wei,Tianrui Li,Bo An,Jiye Liang*

Main category: cs.LG

TL;DR: 本文对持续强化学习 (CRL) 进行了全面的检查，重点关注其核心概念、挑战和方法。


<details>
  <summary>Details</summary>
Motivation: 强化学习 (RL) 是一种重要的机器学习范例，用于解决序贯决策问题。近年来，由于深度神经网络的快速发展，该领域取得了显著进展。然而，RL 的成功目前依赖于大量的训练数据和计算资源。此外，RL 在任务中泛化的能力有限，限制了其在动态和现实环境中的适用性。随着持续学习 (CL) 的出现，持续强化学习 (CRL) 已成为一个有希望的研究方向，通过使代理能够持续学习、适应新任务和保留先前获得的知识来解决这些限制。

Method: 对现有工作进行了详细的回顾，组织和分析了它们的指标、任务、基准和场景设置。提出了一个新的 CRL 方法分类法，从知识存储和/或转移的角度将其分为四种类型。

Result: 对 CRL 进行了全面的检查，重点关注其核心概念、挑战和方法。

Conclusion: 分析强调了 CRL 的独特挑战，并为未来的方向提供了实用的见解。

Abstract: Reinforcement Learning (RL) is an important machine learning paradigm for
solving sequential decision-making problems. Recent years have witnessed
remarkable progress in this field due to the rapid development of deep neural
networks. However, the success of RL currently relies on extensive training
data and computational resources. In addition, RL's limited ability to
generalize across tasks restricts its applicability in dynamic and real-world
environments. With the arisen of Continual Learning (CL), Continual
Reinforcement Learning (CRL) has emerged as a promising research direction to
address these limitations by enabling agents to learn continuously, adapt to
new tasks, and retain previously acquired knowledge. In this survey, we provide
a comprehensive examination of CRL, focusing on its core concepts, challenges,
and methodologies. Firstly, we conduct a detailed review of existing works,
organizing and analyzing their metrics, tasks, benchmarks, and scenario
settings. Secondly, we propose a new taxonomy of CRL methods, categorizing them
into four types from the perspective of knowledge storage and/or transfer.
Finally, our analysis highlights the unique challenges of CRL and provides
practical insights into future directions.

</details>


### [165] [Advancements and Challenges in Continual Reinforcement Learning: A Comprehensive Review](https://arxiv.org/abs/2506.21899)
*Amara Zuffer,Michael Burke,Mehrtash Harandi*

Main category: cs.LG

TL;DR: This paper reviews continual reinforcement learning, focusing on how it enables RL agents to learn continuously. It covers concepts, challenges, methodologies, advancements in robotics, and future directions.


<details>
  <summary>Details</summary>
Motivation: The diversity of tasks and dynamic nature of reinforcement learning (RL) require RL agents to be able to learn sequentially and continuously, a learning paradigm known as continuous reinforcement learning. This enables RL agents to acquire and retain useful and reusable knowledge seamlessly.

Method: This survey reviews how continual learning transforms RL agents into dynamic continual learners.

Result: Special emphasis is placed on recent advancements in continual reinforcement learning within robotics, along with a succinct overview of evaluation environments utilized in prominent research, facilitating accessibility for newcomers to the field.

Conclusion: This review concludes with a discussion on limitations and promising future directions, providing valuable insights for researchers and practitioners alike.

Abstract: The diversity of tasks and dynamic nature of reinforcement learning (RL)
require RL agents to be able to learn sequentially and continuously, a learning
paradigm known as continuous reinforcement learning. This survey reviews how
continual learning transforms RL agents into dynamic continual learners. This
enables RL agents to acquire and retain useful and reusable knowledge
seamlessly. The paper delves into fundamental aspects of continual
reinforcement learning, exploring key concepts, significant challenges, and
novel methodologies. Special emphasis is placed on recent advancements in
continual reinforcement learning within robotics, along with a succinct
overview of evaluation environments utilized in prominent research,
facilitating accessibility for newcomers to the field. The review concludes
with a discussion on limitations and promising future directions, providing
valuable insights for researchers and practitioners alike.

</details>


### [166] [TOAST: Task-Oriented Adaptive Semantic Transmission over Dynamic Wireless Environments](https://arxiv.org/abs/2506.21900)
*Sheng Yun,Jianhua Pei,Ping Wang*

Main category: cs.LG

TL;DR: TOAST是一种用于多任务优化的统一框架，它通过自适应任务平衡、低秩自适应和扩散模型来实现高性能。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要从以比特为中心的传输转变为强调任务相关信息的语义感知通信。

Method: 提出了一种统一的框架TOAST，该框架通过三个互补组件来解决动态无线环境中的多任务优化这一核心挑战。首先，我们将自适应任务平衡公式化为一个马尔可夫决策过程，采用深度强化学习来动态调整图像重建保真度和语义分类精度之间的权衡。其次，我们在基于Swin Transformer的联合源信道编码架构中集成了特定模块的低秩自适应(LoRA)机制，实现了参数高效的微调。第三，我们集成了一个在潜在空间中运行的Elucidating扩散模型，以恢复被信道噪声破坏的特征。

Result: TOAST在多个数据集上进行了广泛的实验，结果表明，与基线方法相比，TOAST取得了优异的性能。

Conclusion: TOAST在低信噪比条件下，在分类精度和重建质量方面均优于基线方法，并在所有测试场景中保持了稳健的性能。

Abstract: The evolution toward 6G networks demands a fundamental shift from bit-centric
transmission to semantic-aware communication that emphasizes task-relevant
information. This work introduces TOAST (Task-Oriented Adaptive Semantic
Transmission), a unified framework designed to address the core challenge of
multi-task optimization in dynamic wireless environments through three
complementary components. First, we formulate adaptive task balancing as a
Markov decision process, employing deep reinforcement learning to dynamically
adjust the trade-off between image reconstruction fidelity and semantic
classification accuracy based on real-time channel conditions. Second, we
integrate module-specific Low-Rank Adaptation (LoRA) mechanisms throughout our
Swin Transformer-based joint source-channel coding architecture, enabling
parameter-efficient fine-tuning that dramatically reduces adaptation overhead
while maintaining full performance across diverse channel impairments including
Additive White Gaussian Noise (AWGN), fading, phase noise, and impulse
interference. Third, we incorporate an Elucidating diffusion model that
operates in the latent space to restore features corrupted by channel noises,
providing substantial quality improvements compared to baseline approaches.
Extensive experiments across multiple datasets demonstrate that TOAST achieves
superior performance compared to baseline approaches, with significant
improvements in both classification accuracy and reconstruction quality at low
Signal-to-Noise Ratio (SNR) conditions while maintaining robust performance
across all tested scenarios.

</details>


### [167] [HQCM-EBTC: A Hybrid Quantum-Classical Model for Explainable Brain Tumor Classification](https://arxiv.org/abs/2506.21937)
*Marwan Ait Haddou,Mohamed Bennai*

Main category: cs.LG

TL;DR: HQCM-EBTC, a hybrid quantum-classical model, achieves 96.48% accuracy in brain tumor classification, outperforming classical methods.


<details>
  <summary>Details</summary>
Motivation: Automated brain tumor classification using MRI images.

Method: A hybrid quantum-classical model (HQCM-EBTC) integrating a 5-qubit, depth-2 quantum layer with 5 parallel circuits, optimized via AdamW and a composite loss blending cross-entropy and attention consistency.

Result: HQCM-EBTC achieves 96.48% accuracy, outperforming the classical baseline (86.72%), with higher precision and F1-scores, especially for glioma detection. Enhanced feature separability and accurate tumor localization are also observed.

Conclusion: Quantum-enhanced models show promise in medical imaging, improving diagnostic accuracy and interpretability for brain tumor assessment.

Abstract: We propose HQCM-EBTC, a hybrid quantum-classical model for automated brain
tumor classification using MRI images. Trained on a dataset of 7,576 scans
covering normal, meningioma, glioma, and pituitary classes, HQCM-EBTC
integrates a 5-qubit, depth-2 quantum layer with 5 parallel circuits, optimized
via AdamW and a composite loss blending cross-entropy and attention
consistency.
  HQCM-EBTC achieves 96.48% accuracy, substantially outperforming the classical
baseline (86.72%). It delivers higher precision and F1-scores, especially for
glioma detection. t-SNE projections reveal enhanced feature separability in
quantum space, and confusion matrices show lower misclassification. Attention
map analysis (Jaccard Index) confirms more accurate and focused tumor
localization at high-confidence thresholds.
  These results highlight the promise of quantum-enhanced models in medical
imaging, advancing both diagnostic accuracy and interpretability for clinical
brain tumor assessment.

</details>


### [168] [GuiderNet: A Meta-Learning Framework for Optimizing Quantum Circuit Geometry and Mitigating Barren Plateaus](https://arxiv.org/abs/2506.21940)
*Marwan Ait Haddou,Mohamed Bennai*

Main category: cs.LG

TL;DR: GuiderNet 是一种元学习框架，可指导 PQC 参数进入几何上有利的区域，并嵌入到混合量子-经典管道中，以在训练期间引导初始化和自适应调制，从而减轻贫瘠高原和病态条件。


<details>
  <summary>Details</summary>
Motivation: 变分量子算法 (VQA) 具有近期量子优势的潜力，但面临着梯度消失的贫瘠高原和条件不良的优化环境带来的挑战。

Method: GuiderNet，一种元学习框架，它使用数据相关的参数移位来调节参数化量子电路 (PQC)，旨在最大限度地减少 Fubini-Study 度量张量的对数条件数。

Result: 应用于 Kaggle 糖尿病分类任务，GuiderNet 使累积训练损失减少了 5 倍以上，将测试精度从 75.3% 提高到 98.6%，并将少数类 F1 分数从 0.67 提高到 0.95。它还可以抑制梯度爆炸并稳定参数更新，从而实现更平滑、更稳健的优化。

Conclusion: 几何元条件反射可以缓解贫瘠高原和病态条件，为增强量子机器学习的可训练性和泛化能力提供了一种可扩展的方法。

Abstract: Variational Quantum Algorithms (VQAs) offer potential for near-term quantum
advantage but face challenges from barren plateaus, where gradients vanish, and
poorly conditioned optimization landscapes. We introduce GuiderNet, a
meta-learning framework that conditions Parameterized Quantum Circuits (PQCs)
using data-dependent parameter shifts aimed at minimizing the log condition
number of the Fubini-Study metric tensor. Implemented as a classical neural
network, GuiderNet is meta-trained to guide PQC parameters into geometrically
favorable regions and is embedded within hybrid quantum-classical pipelines to
steer both initialization and adaptive modulation during training.
  Applied to the Kaggle Diabetes classification task, GuiderNet reduces
cumulative training loss by over 5x, improves test accuracy from 75.3% to
98.6%, and increases the minority-class F1 score from 0.67 to 0.95. It also
suppresses gradient explosion and stabilizes parameter updates, enabling
smoother and more robust optimization. These results demonstrate that geometric
meta-conditioning can mitigate barren plateaus and ill-conditioning, providing
a scalable approach to enhance trainability and generalization in quantum
machine learning.

</details>


### [169] [Physics-informed network paradigm with data generation and background noise removal for diverse distributed acoustic sensing applications](https://arxiv.org/abs/2506.21952)
*Yangyang Wan,Haotian Wang,Xuhui Yu,Jiageng Chen,Xinyu Fan,Zuyuan He*

Main category: cs.LG

TL;DR: A physics-informed DAS neural network paradigm is proposed, which does not need real-world events data for training and demonstrates generalization in same application on different sites.


<details>
  <summary>Details</summary>
Motivation: Existing AI models require real-world data (RWD) for training, which is contradictory to the fact of limited available event data in real-world scenarios.

Method: a physics-informed DAS neural network paradigm which does not need real-world events data for training. Physical functions are derived to train a generative network for generation of DAS events data. DAS debackground net is trained by using the generated DAS events data to eliminate background noise in DAS data.

Result: achieved comparable or better performance than data-driven networks trained with RWD. A fault diagnosis accuracy of 91.8% is achieved in belt conveyor field with networks which transferred from simulation test site without any fault events data of test site and field for training.

Conclusion: The proposed physics-informed DAS neural network paradigm is a prospective solution to address significant obstacles of data acquisition and intense noise in practical DAS applications and explore more potential fields for DAS.

Abstract: Distributed acoustic sensing (DAS) has attracted considerable attention
across various fields and artificial intelligence (AI) technology plays an
important role in DAS applications to realize event recognition and denoising.
Existing AI models require real-world data (RWD), whether labeled or not, for
training, which is contradictory to the fact of limited available event data in
real-world scenarios. Here, a physics-informed DAS neural network paradigm is
proposed, which does not need real-world events data for training. By
physically modeling target events and the constraints of real world and DAS
system, physical functions are derived to train a generative network for
generation of DAS events data. DAS debackground net is trained by using the
generated DAS events data to eliminate background noise in DAS data. The
effectiveness of the proposed paradigm is verified in event identification
application based on a public dataset of DAS spatiotemporal data and in belt
conveyor fault monitoring application based on DAS time-frequency data, and
achieved comparable or better performance than data-driven networks trained
with RWD. Owing to the introduction of physical information and capability of
background noise removal, the paradigm demonstrates generalization in same
application on different sites. A fault diagnosis accuracy of 91.8% is achieved
in belt conveyor field with networks which transferred from simulation test
site without any fault events data of test site and field for training. The
proposed paradigm is a prospective solution to address significant obstacles of
data acquisition and intense noise in practical DAS applications and explore
more potential fields for DAS.

</details>


### [170] [Optimal Return-to-Go Guided Decision Transformer for Auto-Bidding in Advertisement](https://arxiv.org/abs/2506.21956)
*Hao Jiang,Yongxiang Tang,Yanxiang Zeng,Pengjia Yuan,Yanhua Cheng,Teng Sha,Xialong Liu,Peng Jiang*

Main category: cs.LG

TL;DR: 我们提出了一种名为 R* DT 的新方法，用于自动化投标，该方法通过增强训练数据集来逐步将次优策略引向最优。


<details>
  <summary>Details</summary>
Motivation: 为了改进自动投标系统的自动化，我们采用生成模型，即决策转换器 (DT)，来解决自动投标中固有的困难。

Method: R* Decision Transformer (R* DT)

Result: 在公开的投标数据集上的综合测试验证了 R* DT 的有效性，并突出了其在处理混合质量轨迹时的优越性。

Conclusion: R* DT在处理混合质量轨迹时表现出优越性。

Abstract: In the realm of online advertising, advertisers partake in ad auctions to
obtain advertising slots, frequently taking advantage of auto-bidding tools
provided by demand-side platforms. To improve the automation of these bidding
systems, we adopt generative models, namely the Decision Transformer (DT), to
tackle the difficulties inherent in automated bidding. Applying the Decision
Transformer to the auto-bidding task enables a unified approach to sequential
modeling, which efficiently overcomes short-sightedness by capturing long-term
dependencies between past bidding actions and user behavior. Nevertheless,
conventional DT has certain drawbacks: (1) DT necessitates a preset
return-to-go (RTG) value before generating actions, which is not inherently
produced; (2) The policy learned by DT is restricted by its training data,
which is consists of mixed-quality trajectories. To address these challenges,
we introduce the R* Decision Transformer (R* DT), developed in a three-step
process: (1) R DT: Similar to traditional DT, R DT stores actions based on
state and RTG value, as well as memorizing the RTG for a given state using the
training set; (2) R^ DT: We forecast the highest value (within the training
set) of RTG for a given state, deriving a suboptimal policy based on the
current state and the forecasted supreme RTG value; (3) R* DT: Based on R^ DT,
we generate trajectories and select those with high rewards (using a simulator)
to augment our training dataset. This data enhancement has been shown to
improve the RTG of trajectories in the training data and gradually leads the
suboptimal policy towards optimality. Comprehensive tests on a publicly
available bidding dataset validate the R* DT's efficacy and highlight its
superiority when dealing with mixed-quality trajectories.

</details>


### [171] [SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model](https://arxiv.org/abs/2506.21976)
*Shuhan Tan,John Lambert,Hong Jeon,Sakshum Kulshrestha,Yijing Bai,Jing Luo,Dragomir Anguelov,Mingxing Tan,Chiyu Max Jiang*

Main category: cs.LG

TL;DR: SceneDiffuser++ is proposed as the first end-to-end generative world model for city-scale traffic simulation, showing improved realism in long simulations.


<details>
  <summary>Details</summary>
Motivation: The goal of traffic simulation is to augment a potentially limited amount of manually-driven miles for testing and validation with a much larger amount of simulated synthetic miles.

Method: SceneDiffuser++, the first end-to-end generative world model trained on a single loss function.

Result: Demonstrates city-scale traffic simulation capability and superior realism under long simulation conditions on an augmented version of the Waymo Open Motion Dataset (WOMD).

Conclusion: SceneDiffuser++ is capable of city-scale traffic simulation and demonstrates superior realism under long simulation conditions.

Abstract: The goal of traffic simulation is to augment a potentially limited amount of
manually-driven miles that is available for testing and validation, with a much
larger amount of simulated synthetic miles. The culmination of this vision
would be a generative simulated city, where given a map of the city and an
autonomous vehicle (AV) software stack, the simulator can seamlessly simulate
the trip from point A to point B by populating the city around the AV and
controlling all aspects of the scene, from animating the dynamic agents (e.g.,
vehicles, pedestrians) to controlling the traffic light states. We refer to
this vision as CitySim, which requires an agglomeration of simulation
technologies: scene generation to populate the initial scene, agent behavior
modeling to animate the scene, occlusion reasoning, dynamic scene generation to
seamlessly spawn and remove agents, and environment simulation for factors such
as traffic lights. While some key technologies have been separately studied in
various works, others such as dynamic scene generation and environment
simulation have received less attention in the research community. We propose
SceneDiffuser++, the first end-to-end generative world model trained on a
single loss function capable of point A-to-B simulation on a city scale
integrating all the requirements above. We demonstrate the city-scale traffic
simulation capability of SceneDiffuser++ and study its superior realism under
long simulation conditions. We evaluate the simulation quality on an augmented
version of the Waymo Open Motion Dataset (WOMD) with larger map regions to
support trip-level simulation.

</details>


### [172] [Binned semiparametric Bayesian networks](https://arxiv.org/abs/2506.21997)
*Rafael Sojo,Javier Díaz-Rozo,Concha Bielza,Pedro Larrañaga*

Main category: cs.LG

TL;DR: 提出了一种新的分箱半参数模型，该模型利用数据分箱来降低计算成本，并在结构学习和对数似然估计方面实现了更高的效率。


<details>
  <summary>Details</summary>
Motivation: 利用数据分箱来降低非参数分布中核密度估计的计算成本。

Method: 开发了两种新的条件概率分布，稀疏分箱核密度估计和傅里叶核密度估计。

Result: 分箱半参数贝叶斯网络在结构学习和对数似然估计方面与半参数贝叶斯网络相比没有统计学上的显著差异，但速度更快。

Conclusion: 新的分箱半参数贝叶斯网络在结构学习和对数似然估计方面与半参数贝叶斯网络相比没有统计学上的显著差异，但速度更快，证明了它是一种可靠且更有效的替代方案。

Abstract: This paper introduces a new type of probabilistic semiparametric model that
takes advantage of data binning to reduce the computational cost of kernel
density estimation in nonparametric distributions. Two new conditional
probability distributions are developed for the new binned semiparametric
Bayesian networks, the sparse binned kernel density estimation and the Fourier
kernel density estimation. These two probability distributions address the
curse of dimensionality, which typically impacts binned models, by using sparse
tensors and restricting the number of parent nodes in conditional probability
calculations. To evaluate the proposal, we perform a complexity analysis and
conduct several comparative experiments using synthetic data and datasets from
the UCI Machine Learning repository. The experiments include different binning
rules, parent restrictions, grid sizes, and number of instances to get a
holistic view of the model's behavior. As a result, our binned semiparametric
Bayesian networks achieve structural learning and log-likelihood estimations
with no statistically significant differences compared to the semiparametric
Bayesian networks, but at a much higher speed. Thus, the new binned
semiparametric Bayesian networks prove to be a reliable and more efficient
alternative to their non-binned counterparts.

</details>


### [173] [On the Necessity of Output Distribution Reweighting for Effective Class Unlearning](https://arxiv.org/abs/2506.20893)
*Yian Wang,Ali Ebrahimpour-Boroojeny,Hari Sundaram*

Main category: cs.LG

TL;DR: RWFT, a lightweight unlearning technique, erases an entire class from a trained classifier without full retraining, matching the results of full retraining and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Enforcing user deletion rights and mitigating harmful or biased predictions.

Method: output-reweighting

Result: RWFT gains 2.79% in previously used metrics and 111.45% in TV-based metric.

Conclusion: RWFT matches the results of full retraining and outperforms state-of-the-art unlearning methods.

Abstract: In this work, we introduce an output-reweighting unlearning method, RWFT, a
lightweight technique that erases an entire class from a trained classifier
without full retraining. Forgetting specific classes from trained models is
essential for enforcing user deletion rights and mitigating harmful or biased
predictions. The full retraining is costly and existing unlearning methods fail
to replicate the behavior of the retrained models when predicting samples from
the unlearned class. We prove this failure by designing a variant of membership
inference attacks, MIA-NN that successfully reveals the unlearned class for any
of these methods. We propose a simple redistribution of the probability mass
for the prediction on the samples in the forgotten class which is robust to
MIA-NN. We also introduce a new metric based on the total variation (TV)
distance of the prediction probabilities to quantify residual leakage to
prevent future methods from susceptibility to the new attack. Through extensive
experiments with state of the art baselines in machine unlearning, we show that
our approach matches the results of full retraining in both metrics used for
evaluation by prior work and the new metric we propose in this work. Compare to
state-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%
in our new TV-based metric over the best existing method.

</details>


### [174] [GKNet: Graph Kalman Filtering and Model Inference via Model-based Deep Learning](https://arxiv.org/abs/2506.22004)
*Mohammad Sabbaqi,Riccardo Taormina,Elvin Isufi*

Main category: cs.LG

TL;DR: This paper proposes a graph-aware state space model for graph time series, using both maximum likelihood and a deep learning approach for inference.


<details>
  <summary>Details</summary>
Motivation: Inference tasks with time series over graphs are important in applications such as urban water networks, economics, and networked neuroscience. These tasks typically rely on identifying a computationally affordable model that jointly captures the graph-temporal patterns of the data.

Method: The paper proposes a graph-aware state space model where both the latent state and the observation equation are parametric graph-induced models. It uses a stochastic partial differential equation for the state equation and a sampled and graph-filtered version of the state for the observation model.

Result: The model is inferred through a maximum likelihood approach and a deep learning architecture.

Conclusion: This paper introduces a graph-aware state space model for graph time series and infers the model using both maximum likelihood and a deep learning architecture.

Abstract: Inference tasks with time series over graphs are of importance in
applications such as urban water networks, economics, and networked
neuroscience. Addressing these tasks typically relies on identifying a
computationally affordable model that jointly captures the graph-temporal
patterns of the data. In this work, we propose a graph-aware state space model
for graph time series, where both the latent state and the observation equation
are parametric graph-induced models with a limited number of parameters that
need to be learned. More specifically, we consider the state equation to follow
a stochastic partial differential equation driven by noise over the graphs
edges accounting not only for potential edge uncertainties but also for
increasing the degrees of freedom in the latter in a tractable manner. The
graph structure conditioning of the noise dispersion allows the state variable
to deviate from the stochastic process in certain neighborhoods. The
observation model is a sampled and graph-filtered version of the state
capturing multi-hop neighboring influence. The goal is to learn the parameters
in both state and observation models from the partially observed data for
downstream tasks such as prediction and imputation. The model is inferred first
through a maximum likelihood approach that provides theoretical tractability
but is limited in expressivity and scalability. To improve on the latter, we
use the state-space formulation to build a principled deep learning
architecture that jointly learns the parameters and tracks the state in an
end-to-end manner in the spirit of Kalman neural networks.

</details>


### [175] [TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning](https://arxiv.org/abs/2506.22008)
*Alessandro Sestini,Joakim Bergdahl,Konrad Tollmar,Andrew D. Bagdanov,Linus Gisslén*

Main category: cs.LG

TL;DR: TROFI是一种新颖的离线逆强化学习方法，它无需预定义的奖励函数即可有效地学习策略。


<details>
  <summary>Details</summary>
Motivation: 在离线强化学习中，智能体仅使用源策略导出的一组固定的存储转换进行训练。但是，这要求数据集由奖励函数标记。在视频游戏开发等应用环境中，奖励函数的可用性并不总是得到保证。

Method: TROFI首先从人类偏好中学习奖励函数，然后使用它来标记原始数据集，使其可用于训练策略。

Result: TROFI的实验结果表明，该方法在3D游戏环境中有效，并且与使用真实奖励学习策略相比，性能相当。研究还强调了良好设计的奖励函数对于确保价值函数与实际未来折扣奖励对齐至关重要。

Conclusion: TROFI在D4RL基准测试中始终优于基线，并且与使用地面实况奖励来学习策略的性能相当。奖励模型的研究强调了奖励函数的重要性。

Abstract: In offline reinforcement learning, agents are trained using only a fixed set
of stored transitions derived from a source policy. However, this requires that
the dataset be labeled by a reward function. In applied settings such as video
game development, the availability of the reward function is not always
guaranteed. This paper proposes Trajectory-Ranked OFfline Inverse reinforcement
learning (TROFI), a novel approach to effectively learn a policy offline
without a pre-defined reward function. TROFI first learns a reward function
from human preferences, which it then uses to label the original dataset making
it usable for training the policy. In contrast to other approaches, our method
does not require optimal trajectories. Through experiments on the D4RL
benchmark we demonstrate that TROFI consistently outperforms baselines and
performs comparably to using the ground truth reward to learn policies.
Additionally, we validate the efficacy of our method in a 3D game environment.
Our studies of the reward model highlight the importance of the reward function
in this setting: we show that to ensure the alignment of a value function to
the actual future discounted reward, it is fundamental to have a
well-engineered and easy-to-learn reward function.

</details>


### [176] [Hyper-modal Imputation Diffusion Embedding with Dual-Distillation for Federated Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2506.22036)
*Ying Zhang,Yu Zhao,Xuhui Sui,Baohang Zhou,Xiangrui Cai,Li Shen,Xiaojie Yuan,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文提出了联邦多模态知识图补全 (FedMKGC) 任务，并提出了一个名为 MMFeD3-HidE 的框架来解决该任务中的多模态不确定不可用性和多模态客户端异构性挑战。


<details>
  <summary>Details</summary>
Motivation: 随着多模态知识私有化需求的增加，不同机构中的多模态知识图谱通常是分散的，缺乏有效的协作系统，难以保证更强的推理能力和传输安全。

Method: 提出一个名为 MMFeD3-HidE 的框架，该框架包含 Hyper-modal Imputation Diffusion Embedding model (HidE) 和 Multimodal FeDerated Dual Distillation (MMFeD3)。

Result: 提出了一个 FedMKGC 基准，用于综合评估，包括一个名为 MMFedE 的通用 FedMKGC 主干、具有异构多模态信息的数据集和三组构建的基线。

Conclusion: MMFeD3-HidE在基准测试中表现出有效性、语义一致性和收敛鲁棒性。

Abstract: With the increasing multimodal knowledge privatization requirements,
multimodal knowledge graphs in different institutes are usually decentralized,
lacking of effective collaboration system with both stronger reasoning ability
and transmission safety guarantees. In this paper, we propose the Federated
Multimodal Knowledge Graph Completion (FedMKGC) task, aiming at training over
federated MKGs for better predicting the missing links in clients without
sharing sensitive knowledge. We propose a framework named MMFeD3-HidE for
addressing multimodal uncertain unavailability and multimodal client
heterogeneity challenges of FedMKGC. (1) Inside the clients, our proposed
Hyper-modal Imputation Diffusion Embedding model (HidE) recovers the complete
multimodal distributions from incomplete entity embeddings constrained by
available modalities. (2) Among clients, our proposed Multimodal FeDerated Dual
Distillation (MMFeD3) transfers knowledge mutually between clients and the
server with logit and feature distillation to improve both global convergence
and semantic consistency. We propose a FedMKGC benchmark for a comprehensive
evaluation, consisting of a general FedMKGC backbone named MMFedE, datasets
with heterogeneous multimodal information, and three groups of constructed
baselines. Experiments conducted on our benchmark validate the effectiveness,
semantic consistency, and convergence robustness of MMFeD3-HidE.

</details>


### [177] [UniCA: Adapting Time Series Foundation Model to General Covariate-Aware Forecasting](https://arxiv.org/abs/2506.22039)
*Lu Han,Yu Liu,Qiwen Deng,Jian Jiang,Yinbo Sun,Zhe Yu,Binfeng Wang,Xingyu Lu,Lintao Ma,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.LG

TL;DR: UniCA弥合了TSFM与通用协变量感知预测之间的差距，在多个预测基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型（TSFM）通过大规模预训练取得了显著的成功。然而，它们的设计主要针对实值序列，限制了它们处理涉及各种且通常是异构协变量（例如，分类变量和多模态数据（例如，图像、文本））的通用预测任务的能力，这些协变量通常是特定于任务的，并且在预训练期间难以利用。

Method: UniCA首先执行协变量同质化，将异构协变量转换为高级同质序列表示，然后通过统一的基于注意力的融合机制融合它们。

Result: UniCA与同构和异构协变量的自适应兼容且通用，在保留TSFM的泛化能力的同时，结合了额外的协变量信息。

Conclusion: UniCA在多个单模和多模协变量感知预测基准上进行了广泛的实验，证明了其优越性，突出了协变量感知TSFM在实际预测场景中的应用前景。

Abstract: Time Series Foundation Models (TSFMs) have achieved remarkable success
through large-scale pretraining. However, their design primarily targets
real-valued series, limiting their ability to handle general forecasting tasks
involving diverse and often heterogeneous covariates--such as categorical
variables and multimodal data (e.g., images, text)--which are typically
task-specific and difficult to leverage during pretraining. To address this
gap, we propose Unified Covariate Adaptation (UniCA), a framework to bridge
TSFMs with general covariate-aware forecasting. UniCA first performs covariate
homogenization to transform heterogeneous covariates into high-level
homogeneous series representations and then fuses them via a unified
attention-based fusion mechanism. UniCA is compatible and universal for
adaptation with both homogeneous and heterogeneous covariates, incorporating
extra covariate information while preserving the generalization ability of
TSFMs.Extensive experiments on multiple unimodal and multimodal covariate-aware
forecasting benchmarks demonstrate the superiority of UniCA, highlighting the
promise of covariate-aware TSFM adaptation in real-world forecasting scenarios.
Codes are released on https://github.com/hanlu-nju/UniCA.

</details>


### [178] [GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling](https://arxiv.org/abs/2506.22049)
*Tianhao Chen,Xin Xu,Zijing Liu,Pengxiang Li,Xinyuan Song,Ajay Kumar Jaiswal,Fan Zhang,Jishan Hu,Yang Wang,Hao Chen,Shizhe Diao,Shiwei Liu,Yu Li,Yin Lu,Can Yang*

Main category: cs.LG

TL;DR: GPAS, a gradient-preserving activation scaling technique, mitigates activation variance issues in Pre-LN Transformers, leading to performance gains and showing promise in other architectures.


<details>
  <summary>Details</summary>
Motivation: Pre-LN suffers from an exponential growth in activation variance across layers, causing the residual path to dominate over sub-layer outputs and limiting the learning capacity of deeper layers.

Method: Gradient-Preserving Activation Scaling (GPAS), a technique that scales down intermediate activations while keeping their gradients unchanged.

Result: GPAS achieves consistent performance gains across various model sizes from 71M to 1B. GPAS also shows promise in improving alternative architectures such as Sandwich-LN and DeepNorm.

Conclusion: GPAS achieves consistent performance gains across various model sizes and shows promise in improving alternative architectures.

Abstract: Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,
predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While
being stable during pretraining and scalable to large model sizes, Pre-LN
suffers from an exponential growth in activation variance across layers,
causing the residual path to dominate over sub-layer outputs and limiting the
learning capacity of deeper layers. To mitigate this issue, we propose
Gradient-Preserving Activation Scaling (GPAS), a simple technique that can be
used in combination with existing approaches. GPAS works by scaling down the
intermediate activations while keeping their gradients unchanged. This leaves
information in the activations intact, and avoids the gradient vanishing
problem associated with gradient downscaling. Extensive experiments across
various model sizes from 71M to 1B show that GPAS achieves consistent
performance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows
promise in improving alternative architectures such as Sandwich-LN and
DeepNorm, demonstrating its versatility and potential for improving training
dynamics in a wide range of settings.

</details>


### [179] [crypto price prediction using lstm+xgboost](https://arxiv.org/abs/2506.22055)
*Mehul Gautam*

Main category: cs.LG

TL;DR: 本研究提出了一种 LSTM+XGBoost 混合模型，用于加密货币价格预测，实验结果表明该模型优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场波动性和复杂动态对准确价格预测提出了独特的挑战。

Method: 结合长短期记忆网络 (LSTM) 和极端梯度提升 (XGBoost) 的混合深度学习和机器学习模型。

Result: LSTM+XGBoost 混合模型在比特币、以太坊、狗狗币和莱特币的历史数据集上，始终优于独立模型和传统预测方法。

Conclusion: 混合深度学习模型在加密货币价格预测中优于传统方法，展示了其在金融预测中的潜力。

Abstract: The volatility and complex dynamics of cryptocurrency markets present unique
challenges for accurate price forecasting. This research proposes a hybrid deep
learning and machine learning model that integrates Long Short-Term Memory
(LSTM) networks and Extreme Gradient Boosting (XGBoost) for cryptocurrency
price prediction. The LSTM component captures temporal dependencies in
historical price data, while XGBoost enhances prediction by modeling nonlinear
relationships with auxiliary features such as sentiment scores and
macroeconomic indicators. The model is evaluated on historical datasets of
Bitcoin, Ethereum, Dogecoin, and Litecoin, incorporating both global and
localized exchange data. Comparative analysis using Mean Absolute Percentage
Error (MAPE) and Min-Max Normalized Root Mean Square Error (MinMax RMSE)
demonstrates that the LSTM+XGBoost hybrid consistently outperforms standalone
models and traditional forecasting methods. This study underscores the
potential of hybrid architectures in financial forecasting and provides
insights into model adaptability across different cryptocurrencies and market
contexts.

</details>


### [180] [Transformers are Graph Neural Networks](https://arxiv.org/abs/2506.22084)
*Chaitanya K. Joshi*

Main category: cs.LG

TL;DR: Transformers are GNNs that are more efficient on modern hardware.


<details>
  <summary>Details</summary>
Motivation: Establish connections between the Transformer architecture and Graph Neural Networks (GNNs) for representation learning on graphs.

Method: Transformers can be viewed as message passing GNNs operating on fully connected graphs of tokens, where the self-attention mechanism capture the relative importance of all tokens w.r.t. each-other, and positional encodings provide hints about sequential ordering or structure.

Result: Transformers are expressive set processing networks that learn relationships among input elements without being constrained by apriori graphs. Transformers are implemented via dense matrix operations that are significantly more efficient on modern hardware than sparse message passing.

Conclusion: Transformers are GNNs currently winning the hardware lottery.

Abstract: We establish connections between the Transformer architecture, originally
introduced for natural language processing, and Graph Neural Networks (GNNs)
for representation learning on graphs. We show how Transformers can be viewed
as message passing GNNs operating on fully connected graphs of tokens, where
the self-attention mechanism capture the relative importance of all tokens
w.r.t. each-other, and positional encodings provide hints about sequential
ordering or structure. Thus, Transformers are expressive set processing
networks that learn relationships among input elements without being
constrained by apriori graphs. Despite this mathematical connection to GNNs,
Transformers are implemented via dense matrix operations that are significantly
more efficient on modern hardware than sparse message passing. This leads to
the perspective that Transformers are GNNs currently winning the hardware
lottery.

</details>


### [181] [Learning to Solve Multi-Objective Routing Problems on Multigraphs](https://arxiv.org/abs/2506.22095)
*Filip Rydin,Attila Lischka,Jiaming Wu,Morteza Haghir Chehreghani,Balázs Kulcsár*

Main category: cs.LG

TL;DR: 本文提出两种神经方法解决多重图上的多目标路由问题，并在TSP和CVRP等问题上表现出强大的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管多重图设置具有很高的实际相关性，但多重图设置在很大程度上被忽视了，在目的地之间存在具有不同属性的多条路径。

Method: 第一种方法直接在多重图上工作，通过自回归选择边，直到完成一次旅行。第二种模型首先将多重图修剪成一个简单的图，然后构建路线。

Result: 验证了这两种模型，发现它们在各种问题上都表现出强大的性能，包括旅行商问题（TSP）和有容量车辆路径问题（CVRP）。

Conclusion: 该论文提出了两种神经方法来解决多重图上的多目标路由问题，并在TSP和CVRP等问题上表现出强大的性能。

Abstract: Learning-based methods for routing have gained significant attention in
recent years, both in single-objective and multi-objective contexts. However,
the multigraph setting, where multiple paths with distinct attributes can exist
between destinations, has largely been overlooked, despite its high practical
relevancy. In this paper, we introduce two neural approaches to address
multi-objective routing on multigraphs. Our first approach works directly on
the multigraph, by autoregressively selecting edges until a tour is completed.
On the other hand, our second model first prunes the multigraph into a simple
graph and then builds routes. We validate both models experimentally and find
that they demonstrate strong performance across a variety of problems,
including the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP).

</details>


### [182] [Transfer Learning for Assessing Heavy Metal Pollution in Seaports Sediments](https://arxiv.org/abs/2506.22096)
*Tin Lai,Farnaz Farid,Yueyang Kuan,Xintian Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的模型，该模型利用迁移学习来准确预测 PLI，从而简化了重金属评估过程，并且结果优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 检测土壤和海港中的重金属污染对于区域环境监测至关重要。传统的 PLI 评估涉及繁琐的程序和沉积物样品数据分析。水-沉积物领域的数据稀缺，传统上一直受到数据收集方面的挑战和各国标准差异的困扰。

Method: 我们提出了一个基于深度学习的模型，该模型简化了重金属评估过程。通过利用迁移学习，我们开发了一种准确的定量评估方法来预测 PLI。

Result: 结果表明，与其他模型相比，平均绝对误差 (MAE) 和平均绝对百分比误差 (MAPE) 显着降低，分别约为 0.5 和 0.03。我们的模型性能比其他基线模型高出 2 个数量级。

Conclusion: 该模型为预测水质提供了一种创新、可访问且经济高效的方法，有益于海洋生物保护、水产养殖和工业污染监测。

Abstract: Detecting heavy metal pollution in soils and seaports is vital for regional
environmental monitoring. The Pollution Load Index (PLI), an international
standard, is commonly used to assess heavy metal containment. However, the
conventional PLI assessment involves laborious procedures and data analysis of
sediment samples. To address this challenge, we propose a deep-learning-based
model that simplifies the heavy metal assessment process. Our model tackles the
issue of data scarcity in the water-sediment domain, which is traditionally
plagued by challenges in data collection and varying standards across nations.
By leveraging transfer learning, we develop an accurate quantitative assessment
method for predicting PLI. Our approach allows the transfer of learned features
across domains with different sets of features. We evaluate our model using
data from six major ports in New South Wales, Australia: Port Yamba, Port
Newcastle, Port Jackson, Port Botany, Port Kembla, and Port Eden. The results
demonstrate significantly lower Mean Absolute Error (MAE) and Mean Absolute
Percentage Error (MAPE) of approximately 0.5 and 0.03, respectively, compared
to other models. Our model performance is up to 2 orders of magnitude than
other baseline models. Our proposed model offers an innovative, accessible, and
cost-effective approach to predicting water quality, benefiting marine life
conservation, aquaculture, and industrial pollution monitoring.

</details>


### [183] [Earthquake Damage Grades Prediction using An Ensemble Approach Integrating Advanced Machine and Deep Learning Models](https://arxiv.org/abs/2506.22129)
*Anurag Panda,Gaurav Kumar Yadav*

Main category: cs.LG

TL;DR: Study uses ML/DL and SMOTE to predict earthquake structural damage grades, focusing on addressing class imbalance for better disaster response.


<details>
  <summary>Details</summary>
Motivation: Evaluating structural damage after earthquakes is crucial for efficient disaster response, rescue operations, and resource allocation. Class imbalance in damage data can lead to skewed models.

Method: Employs multi-class classification machine learning, deep learning models, and ensembling methods, along with SMOTE to handle class imbalance.

Result: Identifies key factors influencing seismic vulnerability and assesses model performance using confusion matrix techniques.

Conclusion: This study explores various machine learning, deep learning, and ensembling methods to predict structural damage grades, addressing class imbalance using SMOTE.

Abstract: In the aftermath of major earthquakes, evaluating structural and
infrastructural damage is vital for coordinating post-disaster response
efforts. This includes assessing damage's extent and spatial distribution to
prioritize rescue operations and resource allocation. Accurately estimating
damage grades to buildings post-earthquake is paramount for effective response
and recovery, given the significant impact on lives and properties,
underscoring the urgency of streamlining relief fund allocation processes.
Previous studies have shown the effectiveness of multi-class classification,
especially XGBoost, along with other machine learning models and ensembling
methods, incorporating regularization to address class imbalance. One
consequence of class imbalance is that it may give rise to skewed models that
undervalue minority classes and give preference to the majority class. This
research deals with the problem of class imbalance with the help of the
synthetic minority oversampling technique (SMOTE). We delve into multiple
multi-class classification machine learning, deep learning models, and
ensembling methods to forecast structural damage grades. The study elucidates
performance determinants through comprehensive feature manipulation experiments
and diverse training approaches. It identifies key factors contributing to
seismic vulnerability while evaluating model performance using techniques like
the confusion matrix further to enhance understanding of the effectiveness of
earthquake damage prediction.

</details>


### [184] [Thompson Sampling-Based Learning and Control for Unknown Dynamic Systems](https://arxiv.org/abs/2506.22186)
*Kaikai Zheng,Dawei Shi,Yang Shi,Long Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于 Thompson 采样的主动学习控制方法，用于解决传统 Thompson 采样在控制系统设计中的局限性。


<details>
  <summary>Details</summary>
Motivation: Thompson 采样依赖于有限参数表示，限制了其在控制系统设计中的应用。为了解决这个问题，

Method: 该论文采用再生核希尔伯特空间对控制律进行参数化，并将控制律视为函数空间中的一个元素。

Result: 理论分析表明，该方法以指数速度学习控制律和闭环性能指标之间的关系，并推导了控制遗憾的上界。

Conclusion: 该论文提出了一种基于 Thompson 采样的主动学习控制方法，并提供了学习过程的收敛性保证。数值实验验证了该方法的有效性。

Abstract: Thompson sampling (TS) is an effective method to explore parametric
uncertainties and can therefore be used for active learning-based controller
design. However, TS relies on finite parametric representations, which limits
its applicability to more general spaces, which are more commonly encountered
in control system design. To address this issue, this work pro poses a
parameterization method for control law learning using reproducing kernel
Hilbert spaces and designs a data-driven active learning control approach.
Specifically, the proposed method treats the control law as an element in a
function space, allowing the design of control laws without imposing
restrictions on the system structure or the form of the controller. A TS
framework is proposed in this work to explore potential optimal control laws,
and the convergence guarantees are further provided for the learning process.
Theoretical analysis shows that the proposed method learns the relationship
between control laws and closed-loop performance metrics at an exponential
rate, and the upper bound of control regret is also derived. Numerical
experiments on controlling unknown nonlinear systems validate the effectiveness
of the proposed method.

</details>


### [185] [Exploring Modularity of Agentic Systems for Drug Discovery](https://arxiv.org/abs/2506.22189)
*Laura van Weesep,Samuel Genheden,Ola Engkvist,Jens Sjölund*

Main category: cs.LG

TL;DR: 本研究 критически 检查了基于LLM的代理系统在药物发现中的模块化，发现不同LLM和代理类型表现各异，提示工程至关重要。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）和代理系统为加速药物发现和设计提供了令人兴奋的机会。本研究 критически 检查了基于LLM的代理系统在药物发现中的模块化，即代理系统的各个部分（如LLM）是否可互换，而这一主题在药物发现应用中受到的关注有限。

Method: 比较不同的大型语言模型，以及工具调用代理与代码生成代理的有效性，使用LLM-as-a-judge评分来比较在编排化学和药物发现工具方面的性能。

Result: Claude-3.5-Sonnet、Claude-3.7-Sonnet和GPT-4o优于其他语言模型，如Llama-3.1-8B、Llama-3.1-70B、GPT-3.5-Turbo和Nova-Micro。代码生成代理平均优于工具调用代理，但这高度依赖于问题和模型。替换系统提示的影响取决于所问的具体问题和使用的模型。

Conclusion: 即使在这个特定领域，也不能简单地替换语言模型而不考虑提示重新设计。需要进一步研究代理系统的模块化，以实现稳定和可扩展的现实问题解决方案。

Abstract: Large-language models (LLMs) and agentic systems present exciting
opportunities to accelerate drug discovery and design. In this study, we
critically examine the modularity of LLM-based agentic systems for drug
discovery, i.e., whether parts of the agentic system such as the LLM are
interchangeable, a topic that has received limited attention in drug discovery
applications. We compare the performance of different large language models
(LLMs) and the effectiveness of tool-calling agents versus code-generating
agents in this domain. Our case study, comparing performance in orchestrating
tools for chemistry and drug discovery using an LLM-as-a-judge score, shows
that Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative
language models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and
Nova-Micro. Although we confirm that code-generating agents outperform the
tool-calling ones on average, we show that this is highly question and model
dependent. Furthermore, the impact of replacing system prompts is dependent on
the specific question asked and the model used, underscoring that -- even in
this particular domain -- one cannot just replace language models without
considering prompt re-engineering. Our study highlights the necessity of
further research into the modularity of agentic systems to enable the
development of stable and scalable solutions for real-world problems.

</details>


### [186] [dreaMLearning: Data Compression Assisted Machine Learning](https://arxiv.org/abs/2506.22190)
*Xiaobo Zhao,Aaron Hurst,Panagiotis Karras,Daniel E. Lucani*

Main category: cs.LG

TL;DR: dreaMLearning 是一种新的框架，它支持从压缩数据中学习而无需解压缩，从而加快训练速度，减少内存使用量和存储空间，并尽量减少对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 机器学习，特别是深度学习，受到以下因素的阻碍：需要大量标记数据来学习有意义的模式而不会过度拟合，以及对计算和存储的巨大需求，这促使人们研究能够以更少的资源实现良好性能的架构。

Method: 本文介绍了一种新颖的框架 dreaMLearning，该框架支持从压缩数据中学习而无需解压缩，该框架建立在基于熵的广义重复数据删除 (EntroGeDe) 之上，EntroGeDe 是一种熵驱动的无损压缩方法，可将信息整合到一组紧凑的代表性样本中。

Result: 对表格和图像数据的回归和分类任务进行的大量实验表明，dreaMLearning 将训练速度提高了 8.8 倍，减少了 10 倍的内存使用量，并将存储空间减少了 42%，而对模型性能的影响很小。

Conclusion: dreaMLearning通过减少内存使用和存储空间，同时尽量减少对模型性能的影响，从而增强了包括分布式和联邦学习以及资源受限边缘设备上的 tinyML 在内的各种 ML 应用，从而为高效且可扩展的学习解锁了新的可能性。

Abstract: Despite rapid advancements, machine learning, particularly deep learning, is
hindered by the need for large amounts of labeled data to learn meaningful
patterns without overfitting and immense demands for computation and storage,
which motivate research into architectures that can achieve good performance
with fewer resources. This paper introduces dreaMLearning, a novel framework
that enables learning from compressed data without decompression, built upon
Entropy-based Generalized Deduplication (EntroGeDe), an entropy-driven lossless
compression method that consolidates information into a compact set of
representative samples. DreaMLearning accommodates a wide range of data types,
tasks, and model architectures. Extensive experiments on regression and
classification tasks with tabular and image data demonstrate that dreaMLearning
accelerates training by up to 8.8x, reduces memory usage by 10x, and cuts
storage by 42%, with a minimal impact on model performance. These advancements
enhance diverse ML applications, including distributed and federated learning,
and tinyML on resource-constrained edge devices, unlocking new possibilities
for efficient and scalable learning.

</details>


### [187] [EFRame: Deeper Reasoning via Exploration-Filtering-Replay Reinforcement Learning Framework](https://arxiv.org/abs/2506.22200)
*Chen Wang,Lai Wei,Yanzhi Zhang,Chenyang Shao,Zedong Dan,Weiran Huang,Yue Wang,Yuzhi Zhang*

Main category: cs.LG

TL;DR: EFRame improves GRPO's performance on complex reasoning tasks by addressing its limitations in exploration, sample efficiency, and stability.


<details>
  <summary>Details</summary>
Motivation: GRPO faces limited exploration, low sample efficiency, and instability, constraining its performance on complex reasoning tasks.

Method: EFRame, an Exploration-Filtering-Replay framework that augments GRPO with additional rollouts, online filtering, and experience replay.

Result: EFRame improves the robustness and efficiency of training and enables access to deeper reasoning capabilities. EFRame enables a more fine-grained categorization of training samples.

Conclusion: EFRame improves training robustness and efficiency, enabling access to deeper reasoning capabilities compared to vanilla GRPO. It also allows for a more fine-grained categorization of training samples.

Abstract: Recent advances in reinforcement learning (RL) have significantly enhanced
the reasoning capabilities of large language models (LLMs). Group Relative
Policy Optimization (GRPO), an efficient variant of PPO that lowers RL's
computational cost, still faces limited exploration, low sample efficiency and
instability, constraining its performance on complex reasoning tasks. To
address these limitations, we introduce EFRame, an Exploration-Filtering-Replay
framework that systematically augments GRPO along three critical dimensions.
EFRame performs additional rollouts to explore high-quality trajectories,
applies online filtering to eliminate low-quality samples that introduce noise
and variance, and leverages experience replay to repeatedly exploit rare but
informative samples. EFRame establishes a complete and stable learning cycle,
guiding the model through a structured transition from exploration to
convergence. Our experiments across a variety of reasoning benchmarks
demonstrate that EFRame not only improves the robustness and efficiency of
training, but also enables access to deeper reasoning capabilities that remain
unattainable under vanilla GRPO. Furthermore, EFRame enables a more
fine-grained categorization of training samples, allowing for a deeper analysis
of how different types of samples contribute to the learning process in RL. Our
code is available at https://github.com/597358816/EFRame.

</details>


### [188] [Risk-Averse Best Arm Set Identification with Fixed Budget and Fixed Confidence](https://arxiv.org/abs/2506.22253)
*Shunta Nonaga,Koji Tabata,Yuta Mizuno,Tamiki Komatsuzaki*

Main category: cs.LG

TL;DR: 提出了一种新的随机bandit优化问题设置，该设置共同解决了决策的两个关键方面：最大化预期回报和最小化相关不确定性。


<details>
  <summary>Details</summary>
Motivation: 在不确定环境下，在最大化预期回报的同时最小化风险的决策是许多学科中普遍存在的问题之一。我们引入了一种新的随机bandit优化问题设置，该设置共同解决了决策的两个关键方面：最大化预期回报和最小化相关不确定性，并通过均值-方差（MV）标准进行量化。

Method: 我们提出了一个统一的元算法框架，该框架能够在固定置信度和固定预算制度下运行，这是通过使用相同的样本探索策略为每种情况量身定制的自适应置信区间设计来实现的。

Result: 我们为两种环境中的返回解决方案的正确性提供了理论保证。为了补充这一理论分析，我们跨合成基准进行了广泛的实证评估，

Conclusion: 该方法在准确性和样本效率方面优于现有方法，突出了其在不确定环境中风险感知决策任务中的广泛适用性。

Abstract: Decision making under uncertain environments in the maximization of expected
reward while minimizing its risk is one of the ubiquitous problems in many
subjects. Here, we introduce a novel problem setting in stochastic bandit
optimization that jointly addresses two critical aspects of decision-making:
maximizing expected reward and minimizing associated uncertainty, quantified
via the mean-variance(MV) criterion. Unlike traditional bandit formulations
that focus solely on expected returns, our objective is to efficiently and
accurately identify the Pareto-optimal set of arms that strikes the best
trade-off between expected performance and risk. We propose a unified
meta-algorithmic framework capable of operating under both fixed-confidence and
fixed-budget regimes, achieved through adaptive design of confidence intervals
tailored to each scenario using the same sample exploration strategy. We
provide theoretical guarantees on the correctness of the returned solutions in
both settings. To complement this theoretical analysis, we conduct extensive
empirical evaluations across synthetic benchmarks, demonstrating that our
approach outperforms existing methods in terms of both accuracy and sample
efficiency, highlighting its broad applicability to risk-aware decision-making
tasks in uncertain environments.

</details>


### [189] [Projected Compression: Trainable Projection for Efficient Transformer Compression](https://arxiv.org/abs/2506.22255)
*Maciej Stefaniak,Michał Krutul,Jan Małaśnicki,Maciej Pióro,Jakub Krajewski,Sebastian Jaszczur,Marek Cygan,Kamil Adamczewski,Jan Ludziejewski*

Main category: cs.LG

TL;DR: Projected Compression, a novel model compression technique, reduces model size using projection modules without additional computational overhead, outperforming hard pruning and retraining.


<details>
  <summary>Details</summary>
Motivation: Large language models' increasing size leads to greater inference time and computational demands, increasing interest in model size reduction methods.

Method: Projected Compression: uses projection modules to reduce model weights by merging trainable projections into a lower-dimensional product matrix.

Result: Projected Compression matches the base model's per-token computation step in FLOPs and outperforms comparable hard pruning and retraining.

Conclusion: Projected Compression outperforms hard pruning and retraining, especially with more tokens.

Abstract: Large language models have steadily increased in size to achieve improved
performance; however, this growth has also led to greater inference time and
computational demands. Consequently, there is rising interest in model size
reduction methods. To address this issue, we propose Projected Compression, a
novel model compression technique, that reduces model weights by utilizing
projection modules. Specifically, we first train additional trainable
projections weights and preserve access to all the original model parameters.
Subsequently, these projections are merged into a lower-dimensional product
matrix, resulting in a reduced-size standard Transformer-based model. Unlike
alternative approaches that require additional computational overhead, our
method matches the base model's per-token computation step in FLOPs.
Experimental results show that Projected Compression outperforms the comparable
hard pruning and retraining approach on higher quality models. Moreover, the
performance margin scales well with the number of tokens.

</details>


### [190] [Score-Based Model for Low-Rank Tensor Recovery](https://arxiv.org/abs/2506.22295)
*Zhengyun Cheng,Changhao Wang,Guanwen Zhang,Yi Xu,Wei Zhou,Xiangyang Ji*

Main category: cs.LG

TL;DR: 提出了一种基于分数的张量分解模型，无需预定义假设，通过学习张量和共享因子之间的兼容性，实现了张量补全和去噪，并在实验中表现出优越的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的张量分解（TD）方法依赖于预定义的结构假设，例如CP或Tucker分解。从概率的角度来看，这些可以被视为使用狄拉克δ分布来建模共享因子和低秩张量之间的关系。然而，这种先验知识在实际场景中很少可用，尤其是在最佳秩结构和收缩规则方面。

Method: 设计了一个神经网络来学习能量函数，该函数通过分数匹配进行优化，以捕获张量条目和共享因子的联合对数概率的梯度。

Result: 实验结果表明，在各种张量类型（包括稀疏和连续时间张量）以及视觉数据中，性能得到显着提高。

Conclusion: 提出了一种基于分数的模型，该模型无需预定义的结构或分布假设，从而能够学习张量和共享因子之间的兼容性。结合块坐标下降（BCD）算法与所提出的平滑正则化，使模型能够执行张量补全和去噪。

Abstract: Low-rank tensor decompositions (TDs) provide an effective framework for
multiway data analysis. Traditional TD methods rely on predefined structural
assumptions, such as CP or Tucker decompositions. From a probabilistic
perspective, these can be viewed as using Dirac delta distributions to model
the relationships between shared factors and the low-rank tensor. However, such
prior knowledge is rarely available in practical scenarios, particularly
regarding the optimal rank structure and contraction rules. The optimization
procedures based on fixed contraction rules are complex, and approximations
made during these processes often lead to accuracy loss. To address this issue,
we propose a score-based model that eliminates the need for predefined
structural or distributional assumptions, enabling the learning of
compatibility between tensors and shared factors. Specifically, a neural
network is designed to learn the energy function, which is optimized via score
matching to capture the gradient of the joint log-probability of tensor entries
and shared factors. Our method allows for modeling structures and distributions
beyond the Dirac delta assumption. Moreover, integrating the block coordinate
descent (BCD) algorithm with the proposed smooth regularization enables the
model to perform both tensor completion and denoising. Experimental results
demonstrate significant performance improvements across various tensor types,
including sparse and continuous-time tensors, as well as visual data.

</details>


### [191] [CoATA: Effective Co-Augmentation of Topology and Attribute for Graph Neural Networks](https://arxiv.org/abs/2506.22299)
*Tao Liu,Longlong Lin,Yunfeng Yu,Xi Ou,Youan Zhang,Zhiqiu Ye,Tao Jia*

Main category: cs.LG

TL;DR: CoATA是一种用于图神经网络的双通道框架，通过协同增强拓扑和属性来提高性能，并在多个数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现实世界的图通常表现出大量的噪声和不完整性，这严重降低了GNN的性能。现有的方法通常通过单维增强来解决这个问题，要么侧重于改进拓扑结构，要么扰动节点属性，从而忽略了两者之间更深层次的相互作用。

Method: 提出了一种双通道GNN框架CoATA，专为拓扑和属性的协同增强而设计。CoATA首先传播结构信号以丰富和去噪节点属性。然后，它将增强的属性空间投影到节点-属性二部图中，以进一步细化或重建底层结构。随后，CoATA引入了对比学习，利用原型对齐和一致性约束，以促进增强图和原始图之间的相互校正。

Result: 提出的CoATA优于11个最先进的基线方法。

Conclusion: CoATA在七个基准数据集上优于11个最先进的基线方法，展示了其在捕获拓扑和属性之间的协同关系方面的有效性。

Abstract: Graph Neural Networks (GNNs) have garnered substantial attention due to their
remarkable capability in learning graph representations. However, real-world
graphs often exhibit substantial noise and incompleteness, which severely
degrades the performance of GNNs. Existing methods typically address this issue
through single-dimensional augmentation, focusing either on refining topology
structures or perturbing node attributes, thereby overlooking the deeper
interplays between the two. To bridge this gap, this paper presents CoATA, a
dual-channel GNN framework specifically designed for the Co-Augmentation of
Topology and Attribute. Specifically, CoATA first propagates structural signals
to enrich and denoise node attributes. Then, it projects the enhanced attribute
space into a node-attribute bipartite graph for further refinement or
reconstruction of the underlying structure. Subsequently, CoATA introduces
contrastive learning, leveraging prototype alignment and consistency
constraints, to facilitate mutual corrections between the augmented and
original graphs. Finally, extensive experiments on seven benchmark datasets
demonstrate that the proposed CoATA outperforms eleven state-of-the-art
baseline methods, showcasing its effectiveness in capturing the synergistic
relationship between topology and attributes.

</details>


### [192] [Weakly-Supervised Domain Adaptation with Proportion-Constrained Pseudo-Labeling](https://arxiv.org/abs/2506.22301)
*Takumi Okuo,Shinnosuke Matsuo,Shota Harada,Kiyohito Tanaka,Ryoma Bise*

Main category: cs.LG

TL;DR: 该论文提出了一种新的弱监督域适应方法，该方法利用目标域的类比例信息来解决医学图像分析中的域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 在机器学习中，域偏移是一个重大挑战，尤其是在医疗应用中，由于数据收集实践、设备和程序的变化，不同机构的数据分布存在差异。当在源域数据上训练的模型应用于目标域时，这会降低性能。当源域和目标域之间的类比例不同时，大多数域适应方法都存在问题。

Method: 该论文提出了一种弱监督域适应方法，该方法基于类比例信息为未标记的目标数据分配伪标签（称为比例约束伪标签）。

Result: 在两个内窥镜数据集上的实验表明，该方法优于半监督域适应技术，即使在标记了 5% 的目标域时也是如此。此外，带有噪声比例标签的实验结果突出了该方法的鲁棒性。

Conclusion: 该论文提出了一种弱监督域适应方法，该方法利用来自目标域的类比例信息，通过基于类比例的伪标签来提高性能，无需额外的注释。在两个内窥镜数据集上的实验表明，即使在标记了 5% 的目标域时，该方法也优于半监督域适应技术。

Abstract: Domain shift is a significant challenge in machine learning, particularly in
medical applications where data distributions differ across institutions due to
variations in data collection practices, equipment, and procedures. This can
degrade performance when models trained on source domain data are applied to
the target domain. Domain adaptation methods have been widely studied to
address this issue, but most struggle when class proportions between the source
and target domains differ. In this paper, we propose a weakly-supervised domain
adaptation method that leverages class proportion information from the target
domain, which is often accessible in medical datasets through prior knowledge
or statistical reports. Our method assigns pseudo-labels to the unlabeled
target data based on class proportion (called proportion-constrained
pseudo-labeling), improving performance without the need for additional
annotations. Experiments on two endoscopic datasets demonstrate that our method
outperforms semi-supervised domain adaptation techniques, even when 5% of the
target domain is labeled. Additionally, the experimental results with noisy
proportion labels highlight the robustness of our method, further demonstrating
its effectiveness in real-world application scenarios.

</details>


### [193] [Unfolding Generative Flows with Koopman Operators: Fast and Interpretable Sampling](https://arxiv.org/abs/2506.22304)
*Erkan Turan,Aristotelis Siozopoulos,Maks Ovsjanikov*

Main category: cs.LG

TL;DR: The paper proposes Koopman-CFM, a method that accelerates Conditional Flow Matching (CFM) by integrating Koopman operator theory, enabling faster sampling and providing an interpretable representation of generative dynamics.


<details>
  <summary>Details</summary>
Motivation: Sampling from CFM relies on numerically solving non-linear ODEs which can be computationally expensive and difficult to interpret. Recent alternatives address sampling speed via trajectory straightening, mini-batch coupling or distillation but do not shed light on the underlying structure of the generative process.

Method: Integrating Koopman operator theory to model non-linear flows as linear evolution in a learned space of observables and introducing a decoder-free Koopman-CFM architecture that learns an embedding where the generative dynamics become linear, enabling closed-form, one-step sampling via matrix exponentiation.

Result: Significant speedups over traditional CFM as demonstrated on controlled 2D datasets and real-world benchmarks, MNIST, Fashion-MNIST (F-MNIST), and the Toronto Face Dataset (TFD). The approach leads to a well-structured Koopman generator, whose spectral properties, eigenvalues, and eigenfunctions offer principled tools for analyzing generative behavior such as temporal scaling, mode stability, and decomposition in Koopman latent space.

Conclusion: Koopman-enhanced flow matching combines sampling efficiency with analytical structure, offering a potential step toward fast and interpretable generative modeling.

Abstract: Conditional Flow Matching (CFM) offers a simulation-free framework for
training continuous-time generative models, bridging diffusion and flow-based
approaches. However, sampling from CFM still relies on numerically solving
non-linear ODEs which can be computationally expensive and difficult to
interpret. Recent alternatives address sampling speed via trajectory
straightening, mini-batch coupling or distillation. However, these methods
typically do not shed light on the underlying \textit{structure} of the
generative process. In this work, we propose to accelerate CFM and introduce an
interpretable representation of its dynamics by integrating Koopman operator
theory, which models non-linear flows as linear evolution in a learned space of
observables. We introduce a decoder-free Koopman-CFM architecture that learns
an embedding where the generative dynamics become linear, enabling closed-form,
one-step sampling via matrix exponentiation. This results in significant
speedups over traditional CFM as demonstrated on controlled 2D datasets and
real-world benchmarks, MNIST, Fashion-MNIST (F-MNIST), and the Toronto Face
Dataset (TFD). Unlike previous methods, our approach leads to a well-structured
Koopman generator, whose spectral properties, eigenvalues, and eigenfunctions
offer principled tools for analyzing generative behavior such as temporal
scaling, mode stability, and decomposition in Koopman latent space. By
combining sampling efficiency with analytical structure, Koopman-enhanced flow
matching offers a potential step toward fast and interpretable generative
modeling.

</details>


### [194] [Less Greedy Equivalence Search](https://arxiv.org/abs/2506.22331)
*Adiba Ejaz,Elias Bareinboim*

Main category: cs.LG

TL;DR: This paper introduces Less Greedy Equivalence Search (LGES), a faster and more accurate variant of GES for causal discovery. LGES addresses the limitations of GES by modifying the greedy step and exploiting interventional data.


<details>
  <summary>Details</summary>
Motivation: GES faces two challenges in practice: computational cost and finite-sample accuracy.

Method: A variant of GES that avoids edge insertions between variables for which the score implies some conditional independence.

Result: LGES yields up to a 10-fold speed-up and a substantial reduction in structural error relative to GES. LGES can guide the search using prior assumptions, while correcting these assumptions when contradicted by the data. LGES can exploit interventional data to refine the learned observational equivalence class.

Conclusion: LGES outperforms GES and other baselines in speed, accuracy, and robustness to misspecified assumptions. It recovers the true equivalence class in the sample limit from observational and interventional data, even with misspecified prior assumptions.

Abstract: Greedy Equivalence Search (GES) is a classic score-based algorithm for causal
discovery from observational data. In the sample limit, it recovers the Markov
equivalence class of graphs that describe the data. Still, it faces two
challenges in practice: computational cost and finite-sample accuracy. In this
paper, we develop Less Greedy Equivalence Search (LGES), a variant of GES that
retains its theoretical guarantees while partially addressing these
limitations. LGES modifies the greedy step: rather than always applying the
highest-scoring insertion, it avoids edge insertions between variables for
which the score implies some conditional independence. This more targeted
search yields up to a \(10\)-fold speed-up and a substantial reduction in
structural error relative to GES. Moreover, LGES can guide the search using
prior assumptions, while correcting these assumptions when contradicted by the
data. Finally, LGES can exploit interventional data to refine the learned
observational equivalence class. We prove that LGES recovers the true
equivalence class in the sample limit from observational and interventional
data, even with misspecified prior assumptions. Experiments demonstrate that
LGES outperforms GES and other baselines in speed, accuracy, and robustness to
misspecified assumptions. Our code is available at
https://github.com/CausalAILab/lges.

</details>


### [195] [A Framework for Multi-source Privacy Preserving Epidemic Analysis](https://arxiv.org/abs/2506.22342)
*Zihan Guan,Zhiyuan Zhao,Fengwei Tian,Dung Nguyen,Payel Bhattacharjee,Ravi Tandon,B. Aditya Prakash,Anil Vullikanti*

Main category: cs.LG

TL;DR: 本文提出了一个结合深度学习和差分隐私的流行病预测框架，并通过合成金融数据验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 多样化的数据集在关键的流行病学和公共卫生分析中提供了很大的价值，例如预测和现在预测、流行病模型的开发、干预措施的评估和设计以及资源分配。其中一些数据集通常是敏感的，需要足够的隐私保护。

Method: 开发了一个整合深度学习和流行病模型的框架，以同时进行流行病预测和学习流行病传播的机制模型，同时整合多个数据集进行分析，包括一些具有DP保证的数据集。

Result: 使用具有DP的现实但合成的金融数据集演示了该框架； 这样的数据集尚未用于此类流行病分析。 结果表明，即使在具有DP保证的情况下，该数据集也为预测和学习流行病模型提供了重要价值。

Conclusion: 该论文证明了即使在具有差分隐私（DP）保证的情况下，合成金融数据集也能为疫情预测和学习流行病模型提供重要价值。

Abstract: It is now well understood that diverse datasets provide a lot of value in key
epidemiology and public health analyses, such as forecasting and nowcasting,
development of epidemic models, evaluation and design of interventions and
resource allocation. Some of these datasets are often sensitive, and need
adequate privacy protections. There are many models of privacy, but
Differential Privacy (DP) has become a de facto standard because of its strong
guarantees, without making models about adversaries. In this paper, we develop
a framework the integrates deep learning and epidemic models to simultaneously
perform epidemic forecasting and learning a mechanistic model of epidemic
spread, while incorporating multiple datasets for these analyses, including
some with DP guarantees. We demonstrate our framework using a realistic but
synthetic financial dataset with DP; such a dataset has not been used in such
epidemic analyses. We show that this dataset provides significant value in
forecasting and learning an epidemic model, even when used with DP guarantees.

</details>


### [196] [Reinforcement Learning with Physics-Informed Symbolic Program Priors for Zero-Shot Wireless Indoor Navigation](https://arxiv.org/abs/2506.22365)
*Tao Li,Haozhe Lei,Mingsheng Yin,Yaqi Hu*

Main category: cs.LG

TL;DR: This paper introduces PiPRL, a physics-informed program-guided RL framework for indoor navigation, which outperforms purely symbolic or neural policies and reduces training time.


<details>
  <summary>Details</summary>
Motivation: When using reinforcement learning (RL) to tackle physical control tasks, inductive biases that encode physics priors can help improve sample efficiency during training and enhance generalization in testing. However, the current practice of incorporating these helpful physics-informed inductive biases inevitably runs into significant manual labor and domain expertise, making them prohibitive for general users.

Method: a physics-informed program-guided RL (PiPRL) framework with applications to indoor navigation. PiPRL adopts a hierarchical and modularized neuro-symbolic integration, where a meta symbolic program receives semantically meaningful features from a neural perception module, which form the bases for symbolic programming that encodes physics priors and guides the RL process of a low-level neural controller.

Result: PiPRL consistently outperforms purely symbolic or neural policies and reduces training time by over 26% with the help of the program-based inductive biases.

Conclusion: PiPRL consistently outperforms purely symbolic or neural policies and reduces training time by over 26% with the help of the program-based inductive biases.

Abstract: When using reinforcement learning (RL) to tackle physical control tasks,
inductive biases that encode physics priors can help improve sample efficiency
during training and enhance generalization in testing. However, the current
practice of incorporating these helpful physics-informed inductive biases
inevitably runs into significant manual labor and domain expertise, making them
prohibitive for general users. This work explores a symbolic approach to
distill physics-informed inductive biases into RL agents, where the physics
priors are expressed in a domain-specific language (DSL) that is human-readable
and naturally explainable. Yet, the DSL priors do not translate directly into
an implementable policy due to partial and noisy observations and additional
physical constraints in navigation tasks. To address this gap, we develop a
physics-informed program-guided RL (PiPRL) framework with applications to
indoor navigation. PiPRL adopts a hierarchical and modularized neuro-symbolic
integration, where a meta symbolic program receives semantically meaningful
features from a neural perception module, which form the bases for symbolic
programming that encodes physics priors and guides the RL process of a
low-level neural controller. Extensive experiments demonstrate that PiPRL
consistently outperforms purely symbolic or neural policies and reduces
training time by over 26% with the help of the program-based inductive biases.

</details>


### [197] [Sheaf-Based Decentralized Multimodal Learning for Next-Generation Wireless Communication Systems](https://arxiv.org/abs/2506.22374)
*Abdulmomen Ghalkha,Zhuojun Tian,Chaouki Ben Issaid,Mehdi Bennis*

Main category: cs.LG

TL;DR: Sheaf-DMFL, a decentralized multimodal learning framework using sheaf theory, enhances collaboration among devices with diverse modalities, outperforming conventional federated learning in heterogeneous wireless communication systems.


<details>
  <summary>Details</summary>
Motivation: Conventional federated learning (FL) algorithms typically consider unimodal datasets, require identical model architectures, and fail to leverage the rich information embedded in multimodal data, limiting their applicability to real-world scenarios with diverse modalities and varying client capabilities. The increasing complexity of large-scale communication systems necessitates more intelligent collaboration among edge devices collecting various multimodal sensory data.

Method: A novel decentralized multimodal learning framework leveraging sheaf theory to enhance collaboration among devices with diverse modalities. Specifically, each client has a set of local feature encoders for its different modalities, whose outputs are concatenated before passing through a task-specific layer. While encoders for the same modality are trained collaboratively across clients, the intrinsic correlations among clients' task-specific layers are captured using a sheaf-based structure. An enhanced algorithm named Sheaf-DMFL-Att tailors the attention mechanism within each client to capture correlations among different modalities.

Result: The proposed algorithms outperform existing methods in real-world link blockage prediction and mmWave beamforming scenarios.

Conclusion: The proposed Sheaf-DMFL and Sheaf-DMFL-Att algorithms demonstrate superior performance in heterogeneous wireless communication systems through simulations on real-world link blockage prediction and mmWave beamforming scenarios.

Abstract: In large-scale communication systems, increasingly complex scenarios require
more intelligent collaboration among edge devices collecting various multimodal
sensory data to achieve a more comprehensive understanding of the environment
and improve decision-making accuracy. However, conventional federated learning
(FL) algorithms typically consider unimodal datasets, require identical model
architectures, and fail to leverage the rich information embedded in multimodal
data, limiting their applicability to real-world scenarios with diverse
modalities and varying client capabilities. To address this issue, we propose
Sheaf-DMFL, a novel decentralized multimodal learning framework leveraging
sheaf theory to enhance collaboration among devices with diverse modalities.
Specifically, each client has a set of local feature encoders for its different
modalities, whose outputs are concatenated before passing through a
task-specific layer. While encoders for the same modality are trained
collaboratively across clients, we capture the intrinsic correlations among
clients' task-specific layers using a sheaf-based structure. To further enhance
learning capability, we propose an enhanced algorithm named Sheaf-DMFL-Att,
which tailors the attention mechanism within each client to capture
correlations among different modalities. A rigorous convergence analysis of
Sheaf-DMFL-Att is provided, establishing its theoretical guarantees. Extensive
simulations are conducted on real-world link blockage prediction and mmWave
beamforming scenarios, demonstrate the superiority of the proposed algorithms
in such heterogeneous wireless communication systems.

</details>


### [198] [Probabilistic Optimality for Inference-time Scaling](https://arxiv.org/abs/2506.22376)
*Youkang Wang,Jian Wang,Rubing Chen,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

TL;DR: 提出了一种概率框架和OptScale算法，用于在推理时有效扩展LLM，并在数学推理任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常依赖于并行采样的启发式策略，缺乏原则基础，为了解决这个问题。

Method: 提出了一个概率框架，该框架形式化了推理时缩放的最优性，并开发了一种名为OptScale的算法，该算法可以动态确定最佳采样响应数。

Result: 在数学推理基准测试中，OptScale在显著降低采样开销的同时，保持或优于最先进的推理性能。

Conclusion: 该研究提供了一个理论基础和实用的解决方案，用于有原则的推理时扩展，解决了LLM在复杂推理中有效部署的关键差距。

Abstract: Inference-time scaling has emerged as a powerful technique for enhancing the
reasoning performance of Large Language Models (LLMs). However, existing
approaches often rely on heuristic strategies for parallel sampling, lacking a
principled foundation. To address this gap, we propose a probabilistic
framework that formalizes the optimality of inference-time scaling under the
assumption that parallel samples are independently and identically distributed
(i.i.d.), and where the Best-of-N selection strategy follows a probability
distribution that can be estimated. Within this framework, we derive a
theoretical lower bound on the required number of samples to achieve a target
performance level, providing the first principled guidance for
compute-efficient scaling. Leveraging this insight, we develop
\textsc{OptScale}, a practical algorithm that dynamically determines the
optimal number of sampled responses. \textsc{OptScale} employs a language
model-based predictor to estimate probabilistic prior parameters, enabling the
decision of the minimal number of samples needed that satisfy predefined
performance thresholds and confidence levels. Extensive experiments on
mathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)
demonstrate that \textsc{OptScale} significantly reduces sampling overhead
while remaining better or on par with state-of-the-art reasoning performance.
Our work offers both a theoretical foundation and a practical solution for
principled inference-time scaling, addressing a critical gap in the efficient
deployment of LLMs for complex reasoning.

</details>


### [199] [Towards Distributed Neural Architectures](https://arxiv.org/abs/2506.22389)
*Aditya Cowsik,Tianyu He,Andrey Gromov*

Main category: cs.LG

TL;DR: DNAs, a generalization of sparse methods, learn efficient computation and communication patterns, achieving competitive performance with dense models and demonstrating emergent specialization.


<details>
  <summary>Details</summary>
Motivation: DNAs generalize sparse methods like Mixture-of-Experts and address limitations in computation and communication patterns.

Method: Introduce and train distributed neural architectures (DNA) with a proto-architecture of modules and routers, where tokens can traverse any module series in any order. Computation and communication patterns are learned end-to-end.

Result: Trained DNAs are competitive with dense baselines, paths are power-law distributed, modules show emergent specialization, and compute/parameter allocation is interpretable.

Conclusion: Trained DNAs demonstrate competitive performance with dense baselines, learn compute efficiency/parameter sharing, exhibit power-law distributed paths with emergent specialization, and allocate compute/parameters interpretably.

Abstract: We introduce and train distributed neural architectures (DNA) in vision and
language domains. DNAs are initialized with a proto-architecture that consists
of (transformer, MLP, attention, etc.) modules and routers. Any token (or
patch) can traverse any series of modules in any order. DNAs are a natural
generalization of the sparse methods such as Mixture-of-Experts,
Mixture-of-Depths, parameter sharing, etc. Computation and communication
patterns of DNA modules are learnt end-to-end during training and depend on the
content and context of each token (or patch). These patterns can be shaped by
further requirements added to the optimization objective such as compute/memory
efficiency or load balancing. We empirically show that (i) trained DNAs are
competitive with the dense baselines in both domains and (ii) compute
efficiency/parameter sharing can be learnt from data. Next, we analyze the
emergent connectivity and computation patterns in the trained DNAs. We find
that the paths that tokens take through the models are themselves distributed
according to a power-law. We show that some paths (or, equivalently, groups of
modules) show emergent specialization. Finally, we demonstrate that models
learn to allocate compute and active parameters in an interpretable way.

</details>


### [200] [Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis](https://arxiv.org/abs/2506.22393)
*YongKyung Oh,Alex Bui*

Main category: cs.LG

TL;DR: 提出了一种新的框架，利用多视角对比学习来整合时间模式、基于导数的动态和频域特征，从而推进了机器学习模型的鲁棒性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 由于复杂的时间依赖性和动态分布变化，使机器学习模型适应不同领域的医疗时间序列仍然是一个挑战。目前的方法通常侧重于孤立的特征表示，限制了它们充分捕捉鲁棒域适应所需的复杂时间动态的能力。

Method: 利用多视角对比学习来整合时间模式、基于导数的动态和频域特征。

Result: 在包括脑电图 (EEG)、心电图 (ECG) 和肌电图 (EMG) 在内的不同医疗数据集上进行的大量实验表明，我们的方法在迁移学习任务中显着优于最先进的方法。

Conclusion: 该框架通过提高机器学习模型的鲁棒性和泛化性，为在不同的医疗保健环境中部署可靠的AI系统提供了一条可行的途径。

Abstract: Adapting machine learning models to medical time series across different
domains remains a challenge due to complex temporal dependencies and dynamic
distribution shifts. Current approaches often focus on isolated feature
representations, limiting their ability to fully capture the intricate temporal
dynamics necessary for robust domain adaptation. In this work, we propose a
novel framework leveraging multi-view contrastive learning to integrate
temporal patterns, derivative-based dynamics, and frequency-domain features.
Our method employs independent encoders and a hierarchical fusion mechanism to
learn feature-invariant representations that are transferable across domains
while preserving temporal coherence. Extensive experiments on diverse medical
datasets, including electroencephalogram (EEG), electrocardiogram (ECG), and
electromyography (EMG) demonstrate that our approach significantly outperforms
state-of-the-art methods in transfer learning tasks. By advancing the
robustness and generalizability of machine learning models, our framework
offers a practical pathway for deploying reliable AI systems in diverse
healthcare settings.

</details>
