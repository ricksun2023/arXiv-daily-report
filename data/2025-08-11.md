<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 42]
- [cs.CV](#cs.CV) [Total: 42]
- [cs.AI](#cs.AI) [Total: 28]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.IR](#cs.IR) [Total: 31]
- [cs.LG](#cs.LG) [Total: 44]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PEACH: A sentence-aligned Parallel English-Arabic Corpus for Healthcare](https://arxiv.org/abs/2508.05722)
*Rania Al-Sabbagh*

Main category: cs.CL

TL;DR: PEACH: A new, publicly available English-Arabic healthcare corpus for NLP and translation research.


<details>
  <summary>Details</summary>
Motivation: To aid researchers in contrastive linguistics, translation studies, and natural language processing.

Method: Manually aligned corpus

Result: The corpus contains 51,671 parallel sentences, totaling approximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths vary between 9.52 and 11.83 words on average.

Conclusion: PEACH is a publicly accessible, gold-standard English-Arabic parallel corpus of healthcare texts.

Abstract: This paper introduces PEACH, a sentence-aligned parallel English-Arabic
corpus of healthcare texts encompassing patient information leaflets and
educational materials. The corpus contains 51,671 parallel sentences, totaling
approximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths
vary between 9.52 and 11.83 words on average. As a manually aligned corpus,
PEACH is a gold-standard corpus, aiding researchers in contrastive linguistics,
translation studies, and natural language processing. It can be used to derive
bilingual lexicons, adapt large language models for domain-specific machine
translation, evaluate user perceptions of machine translation in healthcare,
assess patient information leaflets and educational materials' readability and
lay-friendliness, and as an educational resource in translation studies. PEACH
is publicly accessible.

</details>


### [2] [Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation](https://arxiv.org/abs/2508.05775)
*Chi Zhang,Changjia Zhu,Junjie Xiong,Xiaoran Xu,Lingyao Li,Yao Liu,Zhuo Lu*

Main category: cs.CL

TL;DR: 这篇综述讨论了LLM的风险和防御，包括毒性内容、越狱攻击和内容审核，并提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）彻底改变了跨数字平台的内容创作，在自然语言生成和理解方面提供了前所未有的能力。这些模型实现了有益的应用程序，如内容生成、问答（Q&A）、编程和代码推理。同时，它们也带来了严重的风险，可能无意或有意地产生有毒、冒犯性或有偏见的内容。LLM的双重角色，既是解决现实世界问题的强大工具，又是潜在的有害语言来源，提出了紧迫的社会技术挑战。

Method: 系统地回顾了最近关于无意毒性、对抗性越狱攻击和内容审核技术的研究。提出了一个统一的LLM相关危害和防御分类法，分析了新兴的多模态和LLM辅助越狱策略，并评估了缓解措施，包括基于人类反馈的强化学习（RLHF）、提示工程和安全对齐。

Result: 亮点包括LLM安全性的发展态势，当前评估方法的局限性，以及未来研究方向。

Conclusion: 这篇综述总结了LLM安全领域的最新研究，强调了当前评估方法的局限性，并概述了未来的研究方向，以指导开发稳健且符合伦理的语言技术。

Abstract: Large Language Models (LLMs) have revolutionized content creation across
digital platforms, offering unprecedented capabilities in natural language
generation and understanding. These models enable beneficial applications such
as content generation, question and answering (Q&A), programming, and code
reasoning. Meanwhile, they also pose serious risks by inadvertently or
intentionally producing toxic, offensive, or biased content. This dual role of
LLMs, both as powerful tools for solving real-world problems and as potential
sources of harmful language, presents a pressing sociotechnical challenge. In
this survey, we systematically review recent studies spanning unintentional
toxicity, adversarial jailbreaking attacks, and content moderation techniques.
We propose a unified taxonomy of LLM-related harms and defenses, analyze
emerging multimodal and LLM-assisted jailbreak strategies, and assess
mitigation efforts, including reinforcement learning with human feedback
(RLHF), prompt engineering, and safety alignment. Our synthesis highlights the
evolving landscape of LLM safety, identifies limitations in current evaluation
methodologies, and outlines future research directions to guide the development
of robust and ethically aligned language technologies.

</details>


### [3] [FineDialFact: A benchmark for Fine-grained Dialogue Fact Verification](https://arxiv.org/abs/2508.05782)
*Xiangyan Chen,Yufeng Li,Yujian Gan,Arkaitz Zubiaga,Matthew Purver*

Main category: cs.CL

TL;DR: This paper introduces FineDialFact, a benchmark for fine-grained dialogue fact verification to address the limitations of current hallucination detection methods in dialogue systems. The benchmark remains challenging for future research.


<details>
  <summary>Details</summary>
Motivation: LLMs produce hallucinations, which poses significant challenges for many NLP applications. Current approaches to hallucination detection in dialogue systems primarily focus on verifying the factual consistency of generated responses, making one factual label overly simplistic and coarse-grained.

Method: introduce a benchmark, FineDialFact, for fine-grained dialogue fact verification, which involves verifying atomic facts extracted from dialogue responses. construct a dataset based on publicly available dialogue datasets and evaluate it using various baseline methods. methods incorporating Chain-of-Thought (CoT) reasoning can enhance performance

Result: methods incorporating Chain-of-Thought (CoT) reasoning can enhance performance in dialogue fact verification. the best F1-score achieved on the HybriDialogue is only 0.75

Conclusion: The best F1-score achieved on the HybriDialogue is only 0.75, indicating that the benchmark remains a challenging task for future research.

Abstract: Large Language Models (LLMs) are known to produce hallucinations - factually
incorrect or fabricated information - which poses significant challenges for
many Natural Language Processing (NLP) applications, such as dialogue systems.
As a result, detecting hallucinations has become a critical area of research.
Current approaches to hallucination detection in dialogue systems primarily
focus on verifying the factual consistency of generated responses. However,
these responses often contain a mix of accurate, inaccurate or unverifiable
facts, making one factual label overly simplistic and coarse-grained. In this
paper, we introduce a benchmark, FineDialFact, for fine-grained dialogue fact
verification, which involves verifying atomic facts extracted from dialogue
responses. To support this, we construct a dataset based on publicly available
dialogue datasets and evaluate it using various baseline methods. Experimental
results demonstrate that methods incorporating Chain-of-Thought (CoT) reasoning
can enhance performance in dialogue fact verification. Despite this, the best
F1-score achieved on the HybriDialogue, an open-domain dialogue dataset, is
only 0.75, indicating that the benchmark remains a challenging task for future
research. Our dataset and code will be public on GitHub.

</details>


### [4] [Human-like fleeting memory improves language learning but impairs reading time prediction in transformer language models](https://arxiv.org/abs/2508.05803)
*Abishek Thamma,Micha Heilbron*

Main category: cs.CL

TL;DR: fleeting memory helps transformers learn language but hurts reading time prediction


<details>
  <summary>Details</summary>
Motivation: the rise of Transformers challenges the idea that memory limitations help in learning language

Method: training transformers with and without fleeting memory on a developmentally realistic training set

Result: fleeting memory consistently improves language learning but impairs surprisal-based prediction of human reading times

Conclusion: memory limitations improve neural network language learning but not predicting behavior

Abstract: Human memory is fleeting. As words are processed, the exact wordforms that
make up incoming sentences are rapidly lost. Cognitive scientists have long
believed that this limitation of memory may, paradoxically, help in learning
language - an idea supported by classic connectionist modelling work. The rise
of Transformers appears to challenge this idea, as these models can learn
language effectively, despite lacking memory limitations or other architectural
recency biases. Here, we investigate the hypothesized benefit of fleeting
memory for language learning in tightly controlled experiments on transformer
language models. Training transformers with and without fleeting memory on a
developmentally realistic training set, we find that fleeting memory
consistently improves language learning (as quantified by both overall language
modelling performance and targeted syntactic evaluation) but, unexpectedly,
impairs surprisal-based prediction of human reading times. Interestingly,
follow up analyses revealed that this discrepancy - better language modeling,
yet worse reading time prediction - could not be accounted for by prior
explanations of why better language models sometimes fit human reading time
worse. Together, these results support a benefit of memory limitations on
neural network language learning - but not on predicting behavior.

</details>


### [5] ["Mirror" Language AI Models of Depression are Criterion-Contaminated](https://arxiv.org/abs/2508.05830)
*Tong Li,Rasiq Hussain,Mehak Gupta,Joshua R. Oltmanns*

Main category: cs.CL

TL;DR: Mirror models for depression prediction have inflated effect sizes and less generalizability due to criterion contamination. Non-Mirror models may offer more interpretable and generalizable features for real-world assessment.


<details>
  <summary>Details</summary>
Motivation: A growing number of studies show near-perfect LLM language-based prediction of depression assessment scores, but many develop these models directly from language responses to depression assessments, leading to criterion contamination and reduced model generalizability.

Method: Compared the performance of Mirror models (developed from language responses to depression assessments) versus Non-Mirror models (developed from language that does not mirror the assessment). N = 110 research participants completed two different interviews: structured diagnostic and life history interviews. GPT-4, GPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic interview depression scores from the two transcripts separately.

Result: Mirror models showed very large effect sizes (e.g., R2 = .80), while Non-Mirror models demonstrated smaller but relatively large effect sizes (e.g., R2 = .27). Mirror and NonMirror performed the same when correlated with self-reported depression symptoms (e.g., r = ~.54), indicating bias in Mirror models. Topic modeling identified clusters across Mirror and Non-Mirror models.

Conclusion: Mirror language AI models of depression showed artificially inflated effect sizes and less generalizability. Incorporating Non-Mirror models may identify interpretable, and generalizable semantic features that have unique utility in real-world psychological assessment.

Abstract: A growing number of studies show near-perfect LLM language-based prediction
of depression assessment scores (up to R2 of .70). However, many develop these
models directly from language responses to depression assessments. These
"Mirror models" suffer from "criterion contamination", which arises when a
predicted score depends in part on the predictors themselves. This causes
artificial effect size inflation which reduces model generalizability. The
present study compares the performance of Mirror models versus "Non-Mirror
models", which are developed from language that does not mirror the assessment
they are developed to predict. N = 110 research participants completed two
different interviews: structured diagnostic and life history interviews. GPT-4,
GPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic
interview depression scores from the two transcripts separately. Mirror models
(using structured diagnostic data) showed very large effect sizes (e.g., R2 =
.80). As expected, NonMirror models (using life history data) demonstrated
smaller effect sizes, but were relatively large (e.g., R2 = .27). When Mirror
and Non-Mirror model-predicted structured interview depression scores were
correlated with self-reported depression symptoms, Mirror and NonMirror
performed the same (e.g., r = ~.54), indicating that Mirror models contain bias
perhaps due to criterion contamination. Topic modeling identified clusters
across Mirror and Non-Mirror models, as well as between true-positive and
false-positive predictions. In this head-to-head comparison study, Mirror
language AI models of depression showed artificially inflated effect sizes and
less generalizability. As language AI models for depression continue to evolve,
incorporating Non-Mirror models may identify interpretable, and generalizable
semantic features that have unique utility in real-world psychological
assessment.

</details>


### [6] [Discovering Properties of Inflectional Morphology in Neural Emergent Communication](https://arxiv.org/abs/2508.05843)
*Miles Gilberti,Shane Storks,Huteng Dai*

Main category: cs.CL

TL;DR: The paper explores emergent communication in deep neural networks, finding that phonological constraints encourage concatenative morphology, and emergent languages tend to fuse grammatical attributes, similar to natural languages.


<details>
  <summary>Details</summary>
Motivation: Emergent communication (EmCom) with deep neural network-based agents promises to yield insights into the nature of human language, but remains focused primarily on a few subfield-specific goals and metrics that prioritize communication schemes which represent attributes with unique characters one-to-one and compose them syntactically.

Method: Reinterpret a common EmCom setting, the attribute-value reconstruction game, by imposing a small-vocabulary constraint to simulate double articulation, and formulating a novel setting analogous to naturalistic inflectional morphology.

Result: Simulated phonological constraints encourage concatenative morphology

Conclusion: Emergent languages replicate the tendency of natural languages to fuse grammatical attributes.

Abstract: Emergent communication (EmCom) with deep neural network-based agents promises
to yield insights into the nature of human language, but remains focused
primarily on a few subfield-specific goals and metrics that prioritize
communication schemes which represent attributes with unique characters
one-to-one and compose them syntactically. We thus reinterpret a common EmCom
setting, the attribute-value reconstruction game, by imposing a
small-vocabulary constraint to simulate double articulation, and formulating a
novel setting analogous to naturalistic inflectional morphology (enabling
meaningful comparison to natural language communication schemes). We develop
new metrics and explore variations of this game motivated by real properties of
inflectional morphology: concatenativity and fusionality. Through our
experiments, we discover that simulated phonological constraints encourage
concatenative morphology, and emergent languages replicate the tendency of
natural languages to fuse grammatical attributes.

</details>


### [7] [Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models](https://arxiv.org/abs/2508.05880)
*Sree Bhattacharyya,Lucas Craig,Tharun Dilliraj,Jia Li,James Z. Wang*

Main category: cs.CL

TL;DR: 本文介绍了一个新的基准来评估LLM的情感推理能力，发现不同的LLM具有不同的推理模式。


<details>
  <summary>Details</summary>
Motivation: 过去的工作主要以监督方式处理情感相关任务，并且评估研究通常仅限于标准和表面情感相关任务。本文旨在研究LLM如何通过认知维度来推理情感，超越表面层次的情感任务。

Method: 引入了一个名为CoRE的大规模认知情感推理基准来评估LLM在情感推理中使用的内部认知结构。

Result: 结果和分析揭示了不同LLM中不同的推理模式。

Conclusion: LLMs表现出不同的推理模式。

Abstract: Affective Computing has been established as a crucial field of inquiry to
advance the holistic development of Artificial Intelligence (AI) systems.
Foundation models -- especially Large Language Models (LLMs) -- have been
evaluated, trained, or instruction-tuned in several past works, to become
better predictors or generators of emotion. Most of these studies, however,
approach emotion-related tasks in a supervised manner, assessing or training
the capabilities of LLMs using discrete emotion labels associated with stimuli
(e.g., text, images, video, audio). Evaluation studies, in particular, have
often been limited to standard and superficial emotion-related tasks, such as
the recognition of evoked or expressed emotions. In this paper, we move beyond
surface-level emotion tasks to investigate how LLMs reason about emotions
through cognitive dimensions. Drawing from cognitive appraisal theory, we
examine whether LLMs produce coherent and plausible cognitive reasoning when
reasoning about emotionally charged stimuli. We introduce a large-scale
benchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal
cognitive structures implicitly used by LLMs for emotional reasoning. Through a
plethora of evaluation experiments and analysis, we seek to answer: (a) Are
models more likely to implicitly rely on specific cognitive appraisal
dimensions?, (b) What cognitive dimensions are important for characterizing
specific emotions?, and, (c) Can the internal representations of different
emotion categories in LLMs be interpreted through cognitive appraisal
dimensions? Our results and analyses reveal diverse reasoning patterns across
different LLMs. Our benchmark and code will be made publicly available.

</details>


### [8] [Spectrum Projection Score: Aligning Retrieved Summaries with Reader Models in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05909)
*Zhanghao Hu,Qinglin Zhu,Siya Qi,Yulan He,Hanqi Yan,Lin Gui*

Main category: cs.CL

TL;DR: 这篇论文介绍了一种新的度量方法（SPS）和一个新的框架（xCompress），用于评估和改进 RAG 系统，实验表明它们可以提高性能。


<details>
  <summary>Details</summary>
Motivation: 先前的工作通常全面地评估 RAG，共同评估检索器和阅读器，因此难以分离检索的真正贡献，特别是考虑到用作阅读器的 LLM 的提示敏感性。

Method: 我们引入了 Spectrum Projection Score (SPS)，这是一种轻量级的、无监督的指标，允许读者通过比较摘要生成的 token 形成的区域与读者子空间中的主方向来评估检索到的摘要与其隐藏表示的语义对齐，并衡量相关性。在此基础上，我们提出了 xCompress，这是一个推理时间控制器框架，可以动态地对检索摘要候选进行采样、排序和压缩。

Result: 在四个开源 LLM 的五个 QA 基准上进行的大量实验表明 SPS 不仅提高了各种任务的性能，而且为检索和生成之间的交互提供了一个原则性的视角。

Conclusion: SPS 不仅提高了各种任务的性能，而且为检索和生成之间的交互提供了一个原则性的视角。

Abstract: Large Language Models (LLMs) have shown improved generation performance
through retrieval-augmented generation (RAG) following the retriever-reader
paradigm, which supplements model inputs with externally retrieved knowledge.
However, prior work often evaluates RAG holistically, assessing the retriever
and reader jointly, making it difficult to isolate the true contribution of
retrieval, particularly given the prompt sensitivity of LLMs used as readers.
We introduce Spectrum Projection Score (SPS), a lightweight, supervision-free
metric that allows the reader to gauge the semantic alignment of a retrieved
summary with its hidden representation by comparing the area formed by
generated tokens from the summary, and the principal directions of subspace in
the reader and to measure the relevance. Building on SPS we present xCompress,
an inference time controller framework that dynamically samples, ranks, and
compresses retrieval summary candidates. Extensive experiments on five QA
benchmarks with four open source LLMs show that SPS not only enhances
performance across a range of tasks but also provides a principled perspective
on the interaction between retrieval and generation.

</details>


### [9] [Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale](https://arxiv.org/abs/2508.05938)
*Rafal Kocielnik,Min Kim,Penphob,Boonyarungsrit,Fereshteh Soltani,Deshawn Sambrano,Animashree Anandkumar,R. Michael Alvarez*

Main category: cs.CL

TL;DR: A three-stage pipeline is presented for detecting prosociality in text, using human-AI interaction and a two-stage inference system to achieve high precision and reduce costs.


<details>
  <summary>Details</summary>
Motivation: Detecting prosociality in text is a novel and increasingly important challenge for trust and safety systems. Unlike toxic content detection, prosociality lacks well-established definitions and labeled data, requiring new approaches to both annotation and deployment.

Method: The method involves a three-stage pipeline: (1) identifying the best LLM-based labeling strategy, (2) introducing a human-AI refinement loop for clarifying the task definition, and (3) synthesizing high-quality labels using GPT-4 to train a two-stage inference system.

Result: The pipeline achieves high precision (~0.90) and reduces inference costs by ~70%.

Conclusion: This paper presents a practical three-stage pipeline for scalable, high-precision prosocial content classification, which minimizes human labeling effort and inference costs. The pipeline uses a human-AI refinement loop to improve label quality and definition alignment. A two-stage inference system is trained to reduce inference costs while achieving high precision.

Abstract: Detecting prosociality in text--communication intended to affirm, support, or
improve others' behavior--is a novel and increasingly important challenge for
trust and safety systems. Unlike toxic content detection, prosociality lacks
well-established definitions and labeled data, requiring new approaches to both
annotation and deployment. We present a practical, three-stage pipeline that
enables scalable, high-precision prosocial content classification while
minimizing human labeling effort and inference costs. First, we identify the
best LLM-based labeling strategy using a small seed set of human-labeled
examples. We then introduce a human-AI refinement loop, where annotators review
high-disagreement cases between GPT-4 and humans to iteratively clarify and
expand the task definition-a critical step for emerging annotation tasks like
prosociality. This process results in improved label quality and definition
alignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train
a two-stage inference system: a lightweight classifier handles high-confidence
predictions, while only $\sim$35\% of ambiguous instances are escalated to
GPT-4o. This architecture reduces inference costs by $\sim$70% while achieving
high precision ($\sim$0.90). Our pipeline demonstrates how targeted human-AI
interaction, careful task formulation, and deployment-aware architecture design
can unlock scalable solutions for novel responsible AI tasks.

</details>


### [10] [Adversarial Topic-aware Prompt-tuning for Cross-topic Automated Essay Scoring](https://arxiv.org/abs/2508.05987)
*Chunyun Zhang,Hongyan Zhao,Chaoran Cui,Qilong Song,Zhiqing Lu,Shuai Gong,Kailin Liu*

Main category: cs.CL

TL;DR: 提出了一种对抗性主题感知提示调整 (ATOP)，用于联合学习主题共享和主题特定特征，以改进跨主题 AES。


<details>
  <summary>Details</summary>
Motivation: 跨主题自动文章评分 (AES) 旨在开发一种能够有效评估目标主题文章的可迁移模型。该领域的一个重大挑战源于主题之间固有的差异。虽然现有的方法主要侧重于通过源主题和目标主题的分布对齐来提取主题共享特征，但它们经常忽略主题特定特征，从而限制了它们评估诸如主题遵守等关键特征的能力。

Method: 提出了一种新的对抗性主题感知提示调整 (ATOP) 方法，该方法联合学习主题共享和主题特定特征，以改进跨主题 AES。

Result: 在公开的 ASAP++ 数据集上进行的大量实验表明，

Conclusion: ATOP在整体和多特征文章评分方面显著优于现有的最先进方法。

Abstract: Cross-topic automated essay scoring (AES) aims to develop a transferable
model capable of effectively evaluating essays on a target topic. A significant
challenge in this domain arises from the inherent discrepancies between topics.
While existing methods predominantly focus on extracting topic-shared features
through distribution alignment of source and target topics, they often neglect
topic-specific features, limiting their ability to assess critical traits such
as topic adherence. To address this limitation, we propose an Adversarial
TOpic-aware Prompt-tuning (ATOP), a novel method that jointly learns
topic-shared and topic-specific features to improve cross-topic AES. ATOP
achieves this by optimizing a learnable topic-aware prompt--comprising both
shared and specific components--to elicit relevant knowledge from pre-trained
language models (PLMs). To enhance the robustness of topic-shared prompt
learning and mitigate feature scale sensitivity introduced by topic alignment,
we incorporate adversarial training within a unified regression and
classification framework. In addition, we employ a neighbor-based classifier to
model the local structure of essay representations and generate pseudo-labels
for target-topic essays. These pseudo-labels are then used to guide the
supervised learning of topic-specific prompts tailored to the target topic.
Extensive experiments on the publicly available ASAP++ dataset demonstrate that
ATOP significantly outperforms existing state-of-the-art methods in both
holistic and multi-trait essay scoring. The implementation of our method is
publicly available at: https://anonymous.4open.science/r/ATOP-A271.

</details>


### [11] [Crisp Attention: Regularizing Transformers via Structured Sparsity](https://arxiv.org/abs/2508.06016)
*Sagar Gandhi,Vishal Gandhi*

Main category: cs.CL

TL;DR: Attention sparsity improves model accuracy and generalization, and also computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The quadratic computational cost of the self-attention mechanism is a primary challenge in scaling Transformer models. Attention sparsity is widely studied as a technique to improve computational efficiency, but it is almost universally assumed to come at the cost of model accuracy.

Method: Introducing structured, post-hoc sparsity to the attention mechanism of a DistilBERT model during fine-tuning on the SST-2 sentiment analysis task.

Result: Model with 80% attention sparsity achieves a validation accuracy of 91.59%, a 0.97% absolute improvement over the dense baseline.

Conclusion: Attention sparsity can improve the generalization and performance of Transformer models.

Abstract: The quadratic computational cost of the self-attention mechanism is a primary
challenge in scaling Transformer models. While attention sparsity is widely
studied as a technique to improve computational efficiency, it is almost
universally assumed to come at the cost of model accuracy. In this paper, we
report a surprising counter-example to this common wisdom. By introducing
structured, post-hoc sparsity to the attention mechanism of a DistilBERT model
during fine-tuning on the SST-2 sentiment analysis task, we find that model
accuracy improves significantly. Our model with 80\% attention sparsity
achieves a validation accuracy of 91.59\%, a 0.97\% absolute improvement over
the dense baseline. We hypothesize that this phenomenon is due to sparsity
acting as a powerful implicit regularizer, preventing the model from
overfitting by forcing it to make predictions with a more constrained and
robust set of features. Our work recasts attention sparsity not just as a tool
for computational efficiency, but as a potential method for improving the
generalization and performance of Transformer models.

</details>


### [12] [Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future](https://arxiv.org/abs/2508.06026)
*Yidong Wang,Xin Wang,Cunxiang Wang,Junfeng Fang,Qiufeng Wang,Jianing Chu,Xuran Meng,Shuxun Yang,Libo Qin,Yue Zhang,Wei Ye,Shikun Zhang*

Main category: cs.CL

TL;DR: This paper proposes Temporal Self-Rewarding Language Models to address the limitations of standard Self-Rewarding methods by coordinating past, present, and future model generations, leading to improved performance and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing Self-Rewarding paradigms suffer from a limitation where the synchronized improvement of chosen and rejected responses narrows the representational difference, undermining effective preference learning.

Method: The paper introduces Temporal Self-Rewarding Language Models with Anchored Rejection (fixing rejected responses using past model outputs) and Future-Guided Chosen (dynamically curating chosen samples using next-generation model predictions).

Result: Llama3.1-8B achieves a 29.44 win rate on AlpacaEval 2.0 with the proposed method, outperforming the Self-Rewarding baseline (19.69) by 9.75. The method also demonstrates superior out-of-distribution generalization across mathematical reasoning, knowledge-based QA, and code generation tasks.

Conclusion: Temporal Self-Rewarding Language Models significantly improve performance and generalization compared to standard Self-Rewarding methods across various tasks and model sizes.

Abstract: Self-Rewarding Language Models propose an architecture in which the Large
Language Models(LLMs) both generates responses and evaluates its own outputs
via LLM-as-a-Judge prompting, dynamically improving its generative capabilities
through iterative Direct Preference Optimization (DPO). However, our analysis
reveals a critical limitation in existing Self-Rewarding paradigms: the
synchronized improvement of chosen and rejected responses progressively narrows
the representational difference between contrasting samples, undermining
effective preference learning. We propose \textbf{Temporal Self-Rewarding
Language Models} that strategically coordinate past, present, and future model
generations to sustain learning signals. Our dual-phase framework introduces:
(1) \textit{Anchored Rejection} - fixing rejected responses using the past
initial model's outputs and (2) \textit{Future-Guided Chosen} - dynamically
curating chosen samples using next-generation model predictions. Extensive
experiments across three model families (Llama, Qwen, Mistral) and different
model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained
with our method compared to Self-Rewarding using same computation resources.
For example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our
method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our
method also demonstrates superior out-of-distribution generalization across
mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code
generation (HumanEval) tasks, even though we do not specifically collect such
training data.

</details>


### [13] [Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings](https://arxiv.org/abs/2508.06030)
*Kartik Sharma,Yiqiao Jin,Rakshit Trivedi,Srijan Kumar*

Main category: cs.CL

TL;DR: PEEK: Use proxy embeddings to efficiently estimate LLM knowledge, achieving 90% accuracy in predicting held-out facts.


<details>
  <summary>Details</summary>
Motivation: It is difficult to predict what LLMs have acquired. Prior methods require making forward passes through the underlying model to probe the LLM's knowledge about a specific fact, making them computationally expensive and time-consuming.

Method: Propose PEEK, Proxy Embeddings to Estimate Knowledge of LLMs, by leveraging the pre-trained embedding models that effectively encode factual knowledge as text or graphs as proxies for LLMs. First, identify a training set of facts known by LLMs through various probing strategies and then adapt embedding models to predict the LLM outputs with a linear decoder layer.

Result: Embeddings can predict LLM knowledge on a held-out set with up to 90% accuracy. Sentence embedding models are more suitable than graph embeddings to predict LLM knowledge.

Conclusion: Knowledge-adapted embeddings can be used to identify knowledge gaps in LLMs at scale and can provide deeper insights into LLMs' internal inductive bias.

Abstract: Large language models (LLMs) acquire knowledge across diverse domains such as
science, history, and geography encountered during generative pre-training.
However, due to their stochasticity, it is difficult to predict what LLMs have
acquired. Prior work has developed different ways to probe this knowledge by
investigating the hidden representations, crafting specific task prompts,
curating representative samples, and estimating their uncertainty. However,
these methods require making forward passes through the underlying model to
probe the LLM's knowledge about a specific fact, making them computationally
expensive and time-consuming. To bridge this gap, we propose $\textbf{PEEK}$ or
$\textbf{P}$roxy $\textbf{E}$mbeddings to $\textbf{E}$stimate
$\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models
that effectively encode factual knowledge as text or graphs as proxies for
LLMs. First, we identify a training set of facts known by LLMs through various
probing strategies and then adapt embedding models to predict the LLM outputs
with a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived
datasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict
LLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find
that sentence embedding models are more suitable than graph embeddings to
predict LLM knowledge, shedding light on the underlying representation of the
factual landscape. Thus, we believe that knowledge-adapted embeddings can be
used to identify knowledge gaps in LLMs at scale and can provide deeper
insights into LLMs' internal inductive bias. The code and data are made
available at https://github.com/claws-lab/peek.

</details>


### [14] [EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation](https://arxiv.org/abs/2508.06046)
*Xinda Wang,Zhengxu Hou,Yangshijie Zhang,Bingren Yan,Zhibo Yang,Xingsheng Zhang,Luxi Xing,Qiang Zhou,Chen Zhang*

Main category: cs.CL

TL;DR: The paper introduces Self-Evolving Pairwise Reasoning (EvolvR) framework to improve story evaluation by addressing the limitations of current methods. EvolvR uses self-synthesized and self-filtered Chain-of-Thought data for training, achieving state-of-the-art performance and enhancing story generation quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods face a dilemma: prompt engineering for closed-source models suffers from poor adaptability, while fine-tuning approaches for open-source models lack the rigorous reasoning capabilities essential for story evaluation.

Method: The framework first self-synthesizes score-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To ensure data quality, these raw CoTs undergo a self-filtering process, utilizing multi-agents to guarantee their logical rigor and robustness. Finally, the evaluator trained on the refined data is deployed as a reward model to guide the story generation task.

Result: achieves state-of-the-art (SOTA) performance on three evaluation benchmarks including StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward model, it significantly enhances the quality of generated stories

Conclusion: The Self-Evolving Pairwise Reasoning (EvolvR) framework achieves state-of-the-art (SOTA) performance on three evaluation benchmarks and significantly enhances the quality of generated stories.

Abstract: Although the effectiveness of Large Language Models (LLMs) as judges
(LLM-as-a-judge) has been validated, their performance remains limited in
open-ended tasks, particularly in story evaluation. Accurate story evaluation
is crucial not only for assisting human quality judgment but also for providing
key signals to guide story generation. However, existing methods face a
dilemma: prompt engineering for closed-source models suffers from poor
adaptability, while fine-tuning approaches for open-source models lack the
rigorous reasoning capabilities essential for story evaluation. To address
this, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework.
Grounded in pairwise comparison, the framework first self-synthesizes
score-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To
ensure data quality, these raw CoTs undergo a self-filtering process, utilizing
multi-agents to guarantee their logical rigor and robustness. Finally, the
evaluator trained on the refined data is deployed as a reward model to guide
the story generation task. Experimental results demonstrate that our framework
achieves state-of-the-art (SOTA) performance on three evaluation benchmarks
including StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward
model, it significantly enhances the quality of generated stories, thereby
fully validating the superiority of our self-evolving approach.

</details>


### [15] [ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline](https://arxiv.org/abs/2508.06094)
*Morris Alper,Moran Yanuka,Raja Giryes,Gašper Beguš*

Main category: cs.CL

TL;DR: Leverage modern LLMs as computational creativity aids for end-to-end conlang creation using ConlangCrafter, a multi-hop pipeline that decomposes language design into modular stages.


<details>
  <summary>Details</summary>
Motivation: Constructed languages (conlangs) such as Esperanto and Quenya have played diverse roles in art, philosophy, and international communication. Meanwhile, large-scale foundation models have revolutionized creative generation in text, images, and beyond.

Method: ConlangCrafter, a multi-hop pipeline that decomposes language design into modular stages -- phonology, morphology, syntax, lexicon generation, and translation. At each stage, our method leverages LLMs' meta-linguistic reasoning capabilities, injecting randomness to encourage diversity and leveraging self-refinement feedback to encourage consistency in the emerging language description.

Result: Evaluated on metrics measuring coherence and typological diversity, demonstrating its ability to produce coherent and varied conlangs.

Conclusion: ConlangCrafter can produce coherent and varied conlangs without human linguistic expertise.

Abstract: Constructed languages (conlangs) such as Esperanto and Quenya have played
diverse roles in art, philosophy, and international communication. Meanwhile,
large-scale foundation models have revolutionized creative generation in text,
images, and beyond. In this work, we leverage modern LLMs as computational
creativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a
multi-hop pipeline that decomposes language design into modular stages --
phonology, morphology, syntax, lexicon generation, and translation. At each
stage, our method leverages LLMs' meta-linguistic reasoning capabilities,
injecting randomness to encourage diversity and leveraging self-refinement
feedback to encourage consistency in the emerging language description. We
evaluate ConlangCrafter on metrics measuring coherence and typological
diversity, demonstrating its ability to produce coherent and varied conlangs
without human linguistic expertise.

</details>


### [16] [Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs](https://arxiv.org/abs/2508.06103)
*Mohamed Basem,Islam Oshallah,Ali Hamdi,Ammar Mohammed*

Main category: cs.CL

TL;DR: This paper uses prompt-based instruction tuning with large language models for Quran QA, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: address challenges related to complex language, unique terminology, and deep meaning in the text for Extractive Question Answering (QA) on the Quran

Method: few-shot prompting with instruction-tuned large language models and a specialized Arabic prompt framework

Result: large language models with Arabic instructions outperform traditional fine-tuned models, achieving a pAP10 score of 0.637

Conclusion: prompt-based instruction tuning is effective for low-resource, semantically rich QA tasks

Abstract: This paper presents two effective approaches for Extractive Question
Answering (QA) on the Quran. It addresses challenges related to complex
language, unique terminology, and deep meaning in the text. The second uses
few-shot prompting with instruction-tuned large language models such as Gemini
and DeepSeek. A specialized Arabic prompt framework is developed for span
extraction. A strong post-processing system integrates subword alignment,
overlap suppression, and semantic filtering. This improves precision and
reduces hallucinations. Evaluations show that large language models with Arabic
instructions outperform traditional fine-tuned models. The best configuration
achieves a pAP10 score of 0.637. The results confirm that prompt-based
instruction tuning is effective for low-resource, semantically rich QA tasks.

</details>


### [17] [You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures](https://arxiv.org/abs/2508.06105)
*Shengyuan Chen,Chuang Zhou,Zheng Yuan,Qinggang Zhang,Zeyang Cui,Hao Chen,Yilin Xiao,Jiannong Cao,Xiao Huang*

Main category: cs.CL

TL;DR: LogicRAG dynamically extracts reasoning structures at inference time to guide adaptive retrieval without any pre-built graph to address the limitations of existing Graph-based RAG methods .


<details>
  <summary>Details</summary>
Motivation: LLMs often suffer from hallucination, generating factually incorrect statements when handling questions beyond their knowledge and perception. Retrieval-augmented generation (RAG) addresses this by retrieving query-relevant contexts from knowledge bases to support LLM reasoning. Recent advances leverage pre-constructed graphs to capture the relational connections among distributed documents, showing remarkable performance in complex tasks. However, existing Graph-based RAG (GraphRAG) methods rely on a costly process to transform the corpus into a graph, introducing overwhelming token cost and update latency. Moreover, real-world queries vary in type and complexity, requiring different logic structures for accurate reasoning. The pre-built graph may not align with these required structures, resulting in ineffective knowledge retrieval.

Method: propose a Logic-aware Retrieval-Augmented Generation framework (LogicRAG) that dynamically extracts reasoning structures at inference time to guide adaptive retrieval without any pre-built graph. LogicRAG begins by decomposing the input query into a set of subproblems and constructing a directed acyclic graph (DAG) to model the logical dependencies among them. To support coherent multi-step reasoning, LogicRAG then linearizes the graph using topological sort, so that subproblems can be addressed in a logically consistent order. Besides, LogicRAG applies graph pruning to reduce redundant retrieval and uses context pruning to filter irrelevant context

Result: LogicRAG achieves both superior performance and efficiency compared to state-of-the-art baselines.

Conclusion: LogicRAG achieves both superior performance and efficiency compared to state-of-the-art baselines.

Abstract: Large language models (LLMs) often suffer from hallucination, generating
factually incorrect statements when handling questions beyond their knowledge
and perception. Retrieval-augmented generation (RAG) addresses this by
retrieving query-relevant contexts from knowledge bases to support LLM
reasoning. Recent advances leverage pre-constructed graphs to capture the
relational connections among distributed documents, showing remarkable
performance in complex tasks. However, existing Graph-based RAG (GraphRAG)
methods rely on a costly process to transform the corpus into a graph,
introducing overwhelming token cost and update latency. Moreover, real-world
queries vary in type and complexity, requiring different logic structures for
accurate reasoning. The pre-built graph may not align with these required
structures, resulting in ineffective knowledge retrieval. To this end, we
propose a \textbf{\underline{Logic}}-aware
\textbf{\underline{R}}etrieval-\textbf{\underline{A}}ugmented
\textbf{\underline{G}}eneration framework (\textbf{LogicRAG}) that dynamically
extracts reasoning structures at inference time to guide adaptive retrieval
without any pre-built graph. LogicRAG begins by decomposing the input query
into a set of subproblems and constructing a directed acyclic graph (DAG) to
model the logical dependencies among them. To support coherent multi-step
reasoning, LogicRAG then linearizes the graph using topological sort, so that
subproblems can be addressed in a logically consistent order. Besides, LogicRAG
applies graph pruning to reduce redundant retrieval and uses context pruning to
filter irrelevant context, significantly reducing the overall token cost.
Extensive experiments demonstrate that LogicRAG achieves both superior
performance and efficiency compared to state-of-the-art baselines.

</details>


### [18] [AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models](https://arxiv.org/abs/2508.06124)
*Sayantan Adak,Pratyush Chatterjee,Somnath Banerjee,Rima Hazra,Somak Aditya,Animesh Mukherjee*

Main category: cs.CL

TL;DR: AURA, a multi-layered framework with Process Reward Models (PRMs), improves LLMs' safety and logical integrity by step-level evaluations and adaptive decoding.


<details>
  <summary>Details</summary>
Motivation: Present day LLMs face the challenge of managing affordance-based safety risks-situations where outputs inadvertently facilitate harmful actions due to overlooked logical implications. Traditional safety solutions, such as scalar outcome-based reward models, parameter tuning, or heuristic decoding strategies, lack the granularity and proactive nature needed to reliably detect and intervene during subtle yet crucial reasoning steps. Addressing this fundamental gap

Method: We introduce AURA, an innovative, multi-layered framework centered around Process Reward Models (PRMs), providing comprehensive, step level evaluations across logical coherence and safety-awareness. Our framework seamlessly combines introspective self-critique, fine-grained PRM assessments, and adaptive safety-aware decoding to dynamically and proactively guide models toward safer reasoning trajectories.

Result: Empirical evidence clearly demonstrates that this approach significantly surpasses existing methods, significantly improving the logical integrity and affordance-sensitive safety of model outputs.

Conclusion: This research represents a pivotal step toward safer, more responsible, and contextually aware AI, setting a new benchmark for alignment-sensitive applications.

Abstract: Present day LLMs face the challenge of managing affordance-based safety
risks-situations where outputs inadvertently facilitate harmful actions due to
overlooked logical implications. Traditional safety solutions, such as scalar
outcome-based reward models, parameter tuning, or heuristic decoding
strategies, lack the granularity and proactive nature needed to reliably detect
and intervene during subtle yet crucial reasoning steps. Addressing this
fundamental gap, we introduce AURA, an innovative, multi-layered framework
centered around Process Reward Models (PRMs), providing comprehensive, step
level evaluations across logical coherence and safety-awareness. Our framework
seamlessly combines introspective self-critique, fine-grained PRM assessments,
and adaptive safety-aware decoding to dynamically and proactively guide models
toward safer reasoning trajectories. Empirical evidence clearly demonstrates
that this approach significantly surpasses existing methods, significantly
improving the logical integrity and affordance-sensitive safety of model
outputs. This research represents a pivotal step toward safer, more
responsible, and contextually aware AI, setting a new benchmark for
alignment-sensitive applications.

</details>


### [19] [Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models](https://arxiv.org/abs/2508.06135)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.CL

TL;DR: 提出了一种名为SRD的数据管理框架，通过提炼高质量、学生兼容的训练数据，提高知识蒸馏的效果和效率，并在实验中取得了显著的性能提升和训练时间缩短。


<details>
  <summary>Details</summary>
Motivation: 现有的白盒KD方法主要侧重于平衡ground truth和学生生成的响应，而忽略了两个关键因素：训练数据质量和学生模型兼容性。

Method: 选择性反射蒸馏（SRD），一种新颖的数据管理框架，它利用来自学生模型的反思来系统地优化训练数据。SRD通过比较ground truth数据与学生模型输出，动态地评估和选择prompt-response对，通过基于难度的自动排序，选择性地管理高质量、学生兼容的训练实例。此外，在选择训练数据后，采用课程调度策略，以固定的时间间隔将这些精选的子集增量地引入到蒸馏过程中。

Result: SRD在各种语言模型基准测试中，在不同的KD方法和模型族下，持续提高蒸馏模型的性能，并将训练运行时间减少高达39%。

Conclusion: 数据质量和兼容性对于LLM的有效和高效蒸馏至关重要，SRD提供了一个实现这两者的原则性框架。这项工作提高了对KD中以数据为中心的因素的理解，并为提高压缩LLM的能力和效率提供了实践见解。

Abstract: Knowledge Distillation (KD) is a fundamental technique for compressing large
language models (LLMs) into compact, efficient student models. However,
existing white-box KD methods mainly focus on balancing ground truth and
student-generated responses while overlooking two critical factors: training
data quality and student-model compatibility. To address these limitations, we
propose Selective Reflection Distillation (SRD), a novel data curation
framework that leverages reflections from student models to systematically
refine training data. SRD dynamically evaluates and selects prompt-response
pairs by comparing ground truth data with student model outputs, selectively
curating high-quality, student-compatible training instances through automated
ranking based on difficulty. Furthermore, after selecting the training data, a
curriculum scheduling strategy is employed to incrementally introduce these
curated subsets into the distillation process at fixed intervals. As a
plug-and-play enhancement, SRD consistently improves distillation outcomes
across diverse white-box KD approaches and model architectures, as well as
decreases computational cost significantly during KD training. Experiments on a
range of language model benchmarks demonstrate SRD's consistent improvements in
distilled model performance, as well as a reduction in training runtime by up
to 39%, under diverse KD methods and model families. Notably, SRD operates as a
plug-and-play module, enhancing sample efficiency without modifying underlying
KD algorithms. Our findings highlight that data quality and compatibility are
pivotal to effective and efficient distillation of LLMs, and SRD provides a
principled framework to achieve both. This work advances the understanding of
data-centric factors in KD and offers practical insights for enhancing the
capability and efficiency of compressed LLMs.

</details>


### [20] [Scaling Personality Control in LLMs with Big Five Scaler Prompts](https://arxiv.org/abs/2508.06149)
*Gunhee Cho,Yun-Gyung Cheong*

Main category: cs.CL

TL;DR: Big5-Scaler 是一种基于提示的框架，它允许大型语言模型在没有人格的情况下控制人格特质，从而实现更有效的人格化对话。


<details>
  <summary>Details</summary>
Motivation: 论文提出了 Big5-Scaler，旨在通过提示工程使大型语言模型具备可控的人格特质。

Method: Big5-Scaler 是一个基于提示的框架，用于使用可控的 Big Five 人格特质来调节大型语言模型 (LLM)。通过将数字特征值嵌入到自然语言提示中，该方法无需额外训练即可实现细粒度的人格控制。

Result: 结果表明，Big5-Scaler 可以在不同模型中诱导出一致且可区分的人格特质，其性能随提示类型和规模而变化。

Conclusion: Big5-Scaler 通过使用简洁的提示和较低的特质强度，为构建具有人格感知能力的对话代理提供了一种有效的方法。

Abstract: We present Big5-Scaler, a prompt-based framework for conditioning large
language models (LLMs) with controllable Big Five personality traits. By
embedding numeric trait values into natural language prompts, our method
enables fine-grained personality control without additional training. We
evaluate Big5-Scaler across trait expression, dialogue generation, and human
trait imitation tasks. Results show that it induces consistent and
distinguishable personality traits across models, with performance varying by
prompt type and scale. Our analysis highlights the effectiveness of concise
prompts and lower trait intensities, providing a efficient approach for
building personality-aware dialogue agents.

</details>


### [21] [Semantic and Structural Analysis of Implicit Biases in Large Language Models: An Interpretable Approach](https://arxiv.org/abs/2508.06155)
*Renhan Zhang,Lian Lian,Zhen Qi,Guiran Liu*

Main category: cs.CL

TL;DR: 该论文提出了一种新的偏见检测方法，可以有效识别大型语言模型中的隐性社会偏见。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型生成过程中可能出现的隐性刻板印象问题，尤其是一些难以通过显式语言特征捕捉的语义倾向。

Method: 该方法结合了嵌套语义表示和上下文对比机制，从模型输出的向量空间结构中提取潜在的偏见特征，并通过注意力权重扰动分析模型对特定社会属性词的敏感性。

Result: 实验结果表明，该方法在性别、职业、宗教和种族等多个刻板印象维度上实现了强大的检测性能，能够准确识别语义相似文本之间的偏见差异，同时保持较高的语义一致性和输出稳定性。

Conclusion: 该论文提出了一种可解释的偏见检测方法，能够准确识别大型语言模型生成文本中隐藏的社会偏见，并在多个维度上取得了良好的检测效果，为提高生成内容的可靠性提供了技术基础。

Abstract: This paper addresses the issue of implicit stereotypes that may arise during
the generation process of large language models. It proposes an interpretable
bias detection method aimed at identifying hidden social biases in model
outputs, especially those semantic tendencies that are not easily captured
through explicit linguistic features. The method combines nested semantic
representation with a contextual contrast mechanism. It extracts latent bias
features from the vector space structure of model outputs. Using attention
weight perturbation, it analyzes the model's sensitivity to specific social
attribute terms, thereby revealing the semantic pathways through which bias is
formed. To validate the effectiveness of the method, this study uses the
StereoSet dataset, which covers multiple stereotype dimensions including
gender, profession, religion, and race. The evaluation focuses on several key
metrics, such as bias detection accuracy, semantic consistency, and contextual
sensitivity. Experimental results show that the proposed method achieves strong
detection performance across various dimensions. It can accurately identify
bias differences between semantically similar texts while maintaining high
semantic alignment and output stability. The method also demonstrates high
interpretability in its structural design. It helps uncover the internal bias
association mechanisms within language models. This provides a more transparent
and reliable technical foundation for bias detection. The approach is suitable
for real-world applications where high trustworthiness of generated content is
required.

</details>


### [22] [One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging](https://arxiv.org/abs/2508.06163)
*Yingfeng Luo,Dingyang Lin,Junxin Wang,Ziqiang Xu,Kaiyan Chang,Tong Zheng,Bei Li,Anxiang Ma,Tong Xiao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: TADrop is an adaptive sparsification strategy that improves model merging by tailoring the sparsity level of each parameter tensor.


<details>
  <summary>Details</summary>
Motivation: Prevailing approaches employ a "one-size-fits-all" strategy, applying a uniform sparsity ratio that overlooks the inherent structural and statistical heterogeneity of model parameters. This often leads to a suboptimal trade-off, where critical parameters are inadvertently pruned while less useful ones are retained.

Method: introduce TADrop (Tensor-wise Adaptive Drop), an adaptive sparsification strategy that respects heterogeneity by assigning a tailored sparsity level to each parameter tensor based on its distributional properties

Result: TADrop consistently and significantly boosts performance across diverse tasks (vision, language, and multimodal) and models (ViT, BEiT). For instance, when enhancing a leading merging method, it achieves an average performance gain of 2.0% across 8 ViT-B/32 tasks.

Conclusion: TADrop provides a more effective way to mitigate parameter interference by tailoring sparsification to the model's structure, offering a new baseline for high-performance model merging.

Abstract: Model merging has emerged as a compelling data-free paradigm for multi-task
learning, enabling the fusion of multiple fine-tuned models into a single,
powerful entity. A key technique in merging methods is sparsification, which
prunes redundant parameters from task vectors to mitigate interference.
However, prevailing approaches employ a ``one-size-fits-all'' strategy,
applying a uniform sparsity ratio that overlooks the inherent structural and
statistical heterogeneity of model parameters. This often leads to a suboptimal
trade-off, where critical parameters are inadvertently pruned while less useful
ones are retained. To address this limitation, we introduce \textbf{TADrop}
(\textbf{T}ensor-wise \textbf{A}daptive \textbf{Drop}), an adaptive
sparsification strategy that respects this heterogeneity. Instead of a global
ratio, TADrop assigns a tailored sparsity level to each parameter tensor based
on its distributional properties. The core intuition is that tensors with
denser, more redundant distributions can be pruned aggressively, while sparser,
more critical ones are preserved. As a simple and plug-and-play module, we
validate TADrop by integrating it with foundational, classic, and SOTA merging
methods. Extensive experiments across diverse tasks (vision, language, and
multimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and
significantly boosts their performance. For instance, when enhancing a leading
merging method, it achieves an average performance gain of 2.0\% across 8
ViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter
interference by tailoring sparsification to the model's structure, offering a
new baseline for high-performance model merging.

</details>


### [23] [UR$^2$: Unify RAG and Reasoning through Reinforcement Learning](https://arxiv.org/abs/2508.06165)
*Weitao Li,Boran Xiang,Xiaolong Wang,Zhinan Gou,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: UR2 unifies retrieval and reasoning through reinforcement learning to improve adaptability across a diverse range of tasks.


<details>
  <summary>Details</summary>
Motivation: existing efforts to unify RAG and RL remain narrow in scope-typically limited to open-domain QA with fixed retrieval settings and task-specific assumptions. This lack of integration constrains generalization and limits the applicability of RAG-RL methods to broader domains.

Method: a difficulty-aware curriculum training that selectively invokes retrieval only for challenging problems, and a hybrid knowledge access strategy combining domain-specific offline corpora with LLM-generated summaries

Result: UR2 (built on Qwen2.5-3/7B and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods, achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several benchmarks.

Conclusion: UR2 significantly outperforms existing RAG and RL methods, achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several benchmarks.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities through two
complementary paradigms: Retrieval-Augmented Generation (RAG), which enhances
knowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),
which optimizes complex reasoning abilities. However, these two capabilities
are often developed in isolation, and existing efforts to unify them remain
narrow in scope-typically limited to open-domain QA with fixed retrieval
settings and task-specific assumptions. This lack of integration constrains
generalization and limits the applicability of RAG-RL methods to broader
domains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a
general framework that unifies retrieval and reasoning through reinforcement
learning. UR2 introduces two key contributions: a difficulty-aware curriculum
training that selectively invokes retrieval only for challenging problems, and
a hybrid knowledge access strategy combining domain-specific offline corpora
with LLM-generated summaries. These components are designed to enable dynamic
coordination between retrieval and reasoning, improving adaptability across a
diverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,
and mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B
and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods,
achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several
benchmarks. We have released all code, models, and data at
https://github.com/Tsinghua-dhy/UR2.

</details>


### [24] [Pragmatics beyond humans: meaning, communication, and LLMs](https://arxiv.org/abs/2508.06167)
*Vít Gvoždiak*

Main category: cs.CL

TL;DR: This paper argues that pragmatic theory needs to adapt to account for communication involving generative AI, focusing on the limitations of traditional approaches and introducing the concept of context frustration.


<details>
  <summary>Details</summary>
Motivation: The emergence of large language models (LLMs) in communicative contexts necessitates a refinement and methodological reconsideration of pragmatics.

Method: Challenges the traditional semiotic trichotomy, examines the tension between human-centred pragmatic theories and the machine-centred nature of LLMs, and addresses the issue of substitutionalism.

Result: The paper introduces the concept of context frustration to describe the paradox of increased contextual input paired with a collapse in contextual understanding.

Conclusion: Pragmatic theory may need to be adjusted or expanded to better account for communication involving generative AI.

Abstract: The paper reconceptualizes pragmatics not as a subordinate, third dimension
of meaning, but as a dynamic interface through which language operates as a
socially embedded tool for action. With the emergence of large language models
(LLMs) in communicative contexts, this understanding needs to be further
refined and methodologically reconsidered. The first section challenges the
traditional semiotic trichotomy, arguing that connectionist LLM architectures
destabilize established hierarchies of meaning, and proposes the Human-Machine
Communication (HMC) framework as a more suitable alternative. The second
section examines the tension between human-centred pragmatic theories and the
machine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics
continue to dominate, it relies on human-specific assumptions ill-suited to
predictive systems like LLMs. Probabilistic pragmatics, particularly the
Rational Speech Act framework, offers a more compatible teleology by focusing
on optimization rather than truth-evaluation. The third section addresses the
issue of substitutionalism in three forms - generalizing, linguistic, and
communicative - highlighting the anthropomorphic biases that distort LLM
evaluation and obscure the role of human communicative subjects. Finally, the
paper introduces the concept of context frustration to describe the paradox of
increased contextual input paired with a collapse in contextual understanding,
emphasizing how users are compelled to co-construct pragmatic conditions both
for the model and themselves. These arguments suggest that pragmatic theory may
need to be adjusted or expanded to better account for communication involving
generative AI.

</details>


### [25] [Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime](https://arxiv.org/abs/2508.06178)
*Hugo Abonizio,Thales Almeida,Roberto Lotufo,Rodrigo Nogueira*

Main category: cs.CL

TL;DR: 本文研究了在少量数据下向 LLM 中注入非结构化信息的问题，并探讨了其与灾难性遗忘现象的关系。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 通常需要大量的文本才能有效地获取新知识。虽然在大型语料库上继续进行预训练或采用检索增强生成 (RAG) 已被证明是成功的，但仅用几千或几百万个 token 更新 LLM 仍然具有挑战性。

Method: 我们使用一个最新的新闻数据集（确保与模型的预训练数据没有重叠）来评估知识获取，通过使用与学习信息相关的问答对来探测模型。从持续的预训练基线开始，我们探索了不同的增强算法来生成合成数据，以提高知识获取能力。

Result: 简单地在有限的数据上继续进行预训练只能产生适度的改进，而让模型接触到多样化的文本变体可以显著提高新事实的学习效果——特别是通过多样化的提示诱导更大变异性的方法。与参数方法相比，基于 RAG 的方法通常会导致控制数据集上更大的性能下降。模型可以自己生成有效的合成训练数据，这为自我改进的模型更新提供了一条途径。

Conclusion: 通过实验，我们发现，在少量数据下，持续预训练效果有限；而使用多样化的文本变体可以显著提高新知识的学习效果，特别是通过多样化的prompt诱导更大变异性的方法。我们还阐明了小数据 regime 中的遗忘现象，说明了学习新内容和保留现有能力之间的微妙平衡。我们还证实了基于 RAG 的知识注入方法的敏感性，与参数方法相比，这种方法通常会导致控制数据集上更大的性能下降。最后，我们证明了模型可以自己生成有效的合成训练数据，这为自我改进的模型更新提供了一条途径。

Abstract: Large language models (LLMs) often require vast amounts of text to
effectively acquire new knowledge. While continuing pre-training on large
corpora or employing retrieval-augmented generation (RAG) has proven
successful, updating an LLM with only a few thousand or million tokens remains
challenging. In this work, we investigate the task of injecting small,
unstructured information into LLMs and its relation to the catastrophic
forgetting phenomenon. We use a dataset of recent news -- ensuring no overlap
with the model's pre-training data -- to evaluate the knowledge acquisition by
probing the model with question-answer pairs related the learned information.
Starting from a continued pre-training baseline, we explored different
augmentation algorithms to generate synthetic data to improve the knowledge
acquisition capabilities. Our experiments show that simply continuing
pre-training on limited data yields modest improvements, whereas exposing the
model to diverse textual variations significantly improves the learning of new
facts -- particularly with methods that induce greater variability through
diverse prompting. Furthermore, we shed light on the forgetting phenomenon in
small-data regimes, illustrating the delicate balance between learning new
content and retaining existing capabilities. We also confirm the sensitivity of
RAG-based approaches for knowledge injection, which often lead to greater
degradation on control datasets compared to parametric methods. Finally, we
demonstrate that models can generate effective synthetic training data
themselves, suggesting a pathway toward self-improving model updates. All code
and generated data used in our experiments are publicly available, providing a
resource for studying efficient knowledge injection in LLMs with limited data
at https://github.com/hugoabonizio/knowledge-injection-methods.

</details>


### [26] [DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration](https://arxiv.org/abs/2508.06186)
*Ali Sarabadani,Maryam Abdollahi Shamami,Hamidreza Sadeghsalehi,Borhan Asadi,Saba Hesaraki*

Main category: cs.CL

TL;DR: DKG-LLM framework introduces a groundbreaking approach to medical diagnosis and personalized treatment recommendations by integrating a dynamic knowledge graph (DKG) with the Grok 3 large language model.


<details>
  <summary>Details</summary>
Motivation: development of large language models is a transformative force in enhancing natural language understanding and has taken a significant step towards artificial general intelligence (AGI).

Method: integrating a dynamic knowledge graph (DKG) with the Grok 3 large language model, Using the Adaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data (including clinical reports and PubMed articles) and patient records dynamically generate a knowledge graph

Result: DKG-LLM achieves a diagnostic accuracy of 84.19%. The model also has a treatment recommendation accuracy of 89.63% and a semantic coverage of 93.48%.

Conclusion: DKG-LLM is a reliable and transformative tool that handles noisy data and complex multi-symptom diseases, along with feedback-based learning from physician input.

Abstract: Large Language Models (LLMs) have grown exponentially since the release of
ChatGPT. These models have gained attention due to their robust performance on
various tasks, including language processing tasks. These models achieve
understanding and comprehension of tasks by training billions of parameters.
The development of these models is a transformative force in enhancing natural
language understanding and has taken a significant step towards artificial
general intelligence (AGI). In this study, we aim to present the DKG-LLM
framework. The DKG-LLM framework introduces a groundbreaking approach to
medical diagnosis and personalized treatment recommendations by integrating a
dynamic knowledge graph (DKG) with the Grok 3 large language model. Using the
Adaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data
(including clinical reports and PubMed articles) and patient records
dynamically generate a knowledge graph consisting of 15,964 nodes in 13
distinct types (e.g., diseases, symptoms, treatments, patient profiles) and
127,392 edges in 26 relationship types (e.g., causal, therapeutic,
association). ASFA utilizes advanced probabilistic models, Bayesian inference,
and graph optimization to extract semantic information, dynamically updating
the graph with approximately 150 new nodes and edges in each data category
while maintaining scalability with up to 987,654 edges. Real-world datasets,
including MIMIC-III and PubMed, were utilized to evaluate the proposed
architecture. The evaluation results show that DKG-LLM achieves a diagnostic
accuracy of 84.19%. The model also has a treatment recommendation accuracy of
89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and
transformative tool that handles noisy data and complex multi-symptom diseases,
along with feedback-based learning from physician input.

</details>


### [27] [Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation](https://arxiv.org/abs/2508.06194)
*Lai Jiang,Yuekang Li,Xiaohan Zhang,Youtao Ding,Li Pan*

Main category: cs.CL

TL;DR: This paper introduces SceneJailEval, a scenario-adaptive multi-dimensional framework and dataset for jailbreak evaluation, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current jailbreak evaluation approaches lack harm intensity quantification and suffer from scenario-specific mismatches due to uniform evaluation criteria.

Method: The paper introduces SceneJailEval, a scenario-adaptive multi-dimensional framework for jailbreak evaluation, and a comprehensive 14-scenario dataset.

Result: SceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on their full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over prior SOTA).

Conclusion: SceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on our full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over prior SOTA), surpassing accuracy limits of existing evaluation methods in heterogeneous scenarios and confirming its advantage.

Abstract: Precise jailbreak evaluation is vital for LLM red teaming and jailbreak
research. Current approaches employ binary classification ( e.g., string
matching, toxic text classifiers, LLM-driven methods), yielding only "yes/no"
labels without quantifying harm intensity. Existing multi-dimensional
frameworks ( e.g., Security Violation, Relative Truthfulness, Informativeness)
apply uniform evaluation criteria across scenarios, resulting in
scenario-specific mismatches--for instance, "Relative Truthfulness" is
irrelevant to "hate speech"--which compromise evaluation precision. To tackle
these limitations, we introduce SceneJailEval, with key contributions: (1) A
groundbreaking scenario-adaptive multi-dimensional framework for jailbreak
evaluation, overcoming the critical "one-size-fits-all" constraint of existing
multi-dimensional methods, and featuring strong extensibility to flexibly adapt
to customized or emerging scenarios. (2) A comprehensive 14-scenario dataset
with diverse jailbreak variants and regional cases, filling the long-standing
gap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3)
SceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on
our full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over
prior SOTA), surpassing accuracy limits of existing evaluation methods in
heterogeneous scenarios and confirming its advantage.

</details>


### [28] [EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations](https://arxiv.org/abs/2508.06196)
*Nizi Nazar,Ehsaneddin Asgari*

Main category: cs.CL

TL;DR: 本文提出了一个用于评估大型语言模型 (LLM) 情商 (EI) 能力的框架和基准，发现现有模型在情感推理方面存在局限性，需要更有针对性的方法。


<details>
  <summary>Details</summary>
Motivation: 情商 (EI) 是人类对齐的 LLM 开发中的一个关键但未被充分探索的维度。

Method: 提出了一个统一的、心理学基础的四层 EI 分类法，并构建了一个名为 EICAP-Bench 的新型 MCQ 风格多轮基准来评估开源 LLM 中的 EI 能力。

Result: 在 EmoCap-Bench 上评估了六个 LLM，发现 Qwen2.5-Instruct 是最强的基线。对 Qwen2.5-Base 和 Qwen2.5-Instruct 进行了微调，发现只有 Appraisal 层通过 UC 获得了显著改善。

Conclusion: 现有的预训练和指令调整范式在使 LLM 具备更深层次的情感推理方面存在局限性，因此需要有针对性的数据和建模策略来实现全面的 EI 对齐。

Abstract: Emotional Intelligence (EI) is a critical yet underexplored dimension in the
development of human-aligned LLMs. To address this gap, we introduce a unified,
psychologically grounded four-layer taxonomy of EI tailored for large language
models (LLMs), encompassing emotional tracking, cause inference, appraisal, and
emotionally appropriate response generation. Building on this framework, we
present EICAP-Bench, a novel MCQ style multi-turn benchmark designed to
evaluate EI capabilities in open-source LLMs across diverse linguistic and
cultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma
(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,
identifying Qwen2.5-Instruct as the strongest baseline. To assess the potential
for enhancing EI capabilities, we fine-tune both Qwen2.5-Base and
Qwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,
instruction-tuned dialogue dataset, in both English and Arabic. Our statistical
analysis reveals that among the five EI layers, only the Appraisal layer shows
significant improvement through UC-based fine-tuning. These findings highlight
the limitations of existing pretraining and instruction-tuning paradigms in
equipping LLMs with deeper emotional reasoning and underscore the need for
targeted data and modeling strategies for comprehensive EI alignment.

</details>


### [29] [Classification is a RAG problem: A case study on hate speech detection](https://arxiv.org/abs/2508.06204)
*Richard Willats,Josh Pennington,Aravind Mohan,Bertie Vidgen*

Main category: cs.CL

TL;DR: 我们提出了使用检索增强生成(RAG)进行分类，它将传统的分类任务从确定符合预训练参数的正确类别转变为评估与推理时检索到的上下文知识相关的内容。


<details>
  <summary>Details</summary>
Motivation: 稳健的内容审核需要能够快速适应不断变化的策略而无需昂贵的再训练的分类系统。

Method: 使用检索增强生成(RAG)进行分类

Result: 该系统能够通过正确调整对特定身份群体的保护来应用细粒度的策略控制，而无需重新训练或损害整体性能。通过三个实验，证明了强大的基线性能，

Conclusion: RAG可以将分类转化为更灵活、透明和适应性更强的内容审核和更广泛的分类流程。

Abstract: Robust content moderation requires classification systems that can quickly
adapt to evolving policies without costly retraining. We present classification
using Retrieval-Augmented Generation (RAG), which shifts traditional
classification tasks from determining the correct category in accordance with
pre-trained parameters to evaluating content in relation to contextual
knowledge retrieved at inference. In hate speech detection, this transforms the
task from "is this hate speech?" to "does this violate the hate speech policy?"
  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates
this approach and offers three key advantages: (1) robust classification
accuracy comparable to leading commercial systems, (2) inherent explainability
via retrieved policy segments, and (3) dynamic policy updates without model
retraining. Through three experiments, we demonstrate strong baseline
performance and show that the system can apply fine-grained policy control by
correctly adjusting protection for specific identity groups without requiring
retraining or compromising overall performance. These findings establish that
RAG can transform classification into a more flexible, transparent, and
adaptable process for content moderation and wider classification problems.

</details>


### [30] [InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?](https://arxiv.org/abs/2508.06220)
*Keummin Ka,Junhyeong Park,Jahyun Jeon,Youngjae Yu*

Main category: cs.CL

TL;DR: InfoCausalQA benchmark is introduced to evaluate causal reasoning in VLMs using infographics. Current VLMs show limited capability in computational and semantic causal reasoning.


<details>
  <summary>Details</summary>
Motivation: the ability to perform causal inference -- a core aspect of human cognition -- remains underexplored, particularly in multimodal settings

Method: introduce InfoCausalQA, a novel benchmark designed to evaluate causal reasoning grounded in infographics that combine structured visual data with textual context. The benchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning based on inferred numerical trends, while Task 2 targets semantic causal reasoning involving five types of causal relations: cause, effect, intervention, counterfactual, and temporal. We manually collected 494 infographic-text pairs from four public sources and used GPT-4o to generate 1,482 high-quality multiple-choice QA pairs. These questions were then carefully revised by humans

Result: current VLMs exhibit limited capability in computational reasoning and even more pronounced limitations in semantic causal reasoning

Conclusion: current VLMs exhibit limited capability in computational reasoning and even more pronounced limitations in semantic causal reasoning. Their significantly lower performance compared to humans indicates a substantial gap in leveraging infographic-based information for causal inference. Through InfoCausalQA, we highlight the need for advancing the causal reasoning abilities of multimodal AI systems.

Abstract: Recent advances in Vision-Language Models (VLMs) have demonstrated impressive
capabilities in perception and reasoning. However, the ability to perform
causal inference -- a core aspect of human cognition -- remains underexplored,
particularly in multimodal settings. In this study, we introduce InfoCausalQA,
a novel benchmark designed to evaluate causal reasoning grounded in
infographics that combine structured visual data with textual context. The
benchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning
based on inferred numerical trends, while Task 2 targets semantic causal
reasoning involving five types of causal relations: cause, effect,
intervention, counterfactual, and temporal. We manually collected 494
infographic-text pairs from four public sources and used GPT-4o to generate
1,482 high-quality multiple-choice QA pairs. These questions were then
carefully revised by humans to ensure they cannot be answered based on
surface-level cues alone but instead require genuine visual grounding. Our
experimental results reveal that current VLMs exhibit limited capability in
computational reasoning and even more pronounced limitations in semantic causal
reasoning. Their significantly lower performance compared to humans indicates a
substantial gap in leveraging infographic-based information for causal
inference. Through InfoCausalQA, we highlight the need for advancing the causal
reasoning abilities of multimodal AI systems.

</details>


### [31] [Large Language Model Data Generation for Enhanced Intent Recognition in German Speech](https://arxiv.org/abs/2508.06277)
*Theresa Pekarek Rosin,Burak Can Kaplan,Stefan Wermter*

Main category: cs.CL

TL;DR: This paper introduces a novel approach for intent recognition from speech by elderly German speakers, using synthetic data generated by LLMs to improve performance and robustness. The approach combines an adapted Whisper ASR model with Transformer-based language models. LeoLM outperforms ChatGPT in generating data for this task.


<details>
  <summary>Details</summary>
Motivation: most existing approaches are limited to short commands and are predominantly developed for English. This paper addresses these limitations by focusing on IR from speech by elderly German speakers.

Method: combines an adapted Whisper ASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based language models trained on synthetic text datasets generated by three well-known large language models (LLMs): LeoLM, Llama3, and ChatGPT

Result: synthetic LLM-generated data significantly boosts classification performance and robustness to different speaking styles and unseen vocabulary. Notably, LeoLM surpasses ChatGPT in dataset quality for German intent recognition.

Conclusion: Generative AI can effectively bridge data gaps in low-resource domains.

Abstract: Intent recognition (IR) for speech commands is essential for artificial
intelligence (AI) assistant systems; however, most existing approaches are
limited to short commands and are predominantly developed for English. This
paper addresses these limitations by focusing on IR from speech by elderly
German speakers. We propose a novel approach that combines an adapted Whisper
ASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based
language models trained on synthetic text datasets generated by three
well-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To
evaluate the robustness of our approach, we generate synthetic speech with a
text-to-speech model and conduct extensive cross-dataset testing. Our results
show that synthetic LLM-generated data significantly boosts classification
performance and robustness to different speaking styles and unseen vocabulary.
Notably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the
much larger ChatGPT (175B) in dataset quality for German intent recognition.
Our approach demonstrates that generative AI can effectively bridge data gaps
in low-resource domains. We provide detailed documentation of our data
generation and training process to ensure transparency and reproducibility.

</details>


### [32] [Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC](https://arxiv.org/abs/2508.06309)
*Ruichong Zhang*

Main category: cs.CL

TL;DR: This paper introduces MDIR, a novel method for detecting LLM plagiarism that overcomes the limitations of existing methods by using matrix analysis and Large Deviation Theory. MDIR is accurate, efficient, and accessible, even after extensive transformations.


<details>
  <summary>Details</summary>
Motivation: Concerns about intellectual property (IP) in large language models (LLMs) have grown significantly. Existing methods for detecting LLM plagiarism fall short in key areas. They fail to accurately reconstruct weight correspondences, lack the ability to compute statistical significance measures such as $p$-values, and may mistakenly flag models trained on similar data as being related.

Method: Matrix-Driven Instant Review (MDIR), a novel method that leverages matrix analysis and Large Deviation Theory.

Result: MDIR achieves accurate reconstruction of weight relationships, provides rigorous $p$-value estimation, and focuses exclusively on weight similarity without requiring full model inference.

Conclusion: MDIR reliably detects plagiarism even after extensive transformations, such as random permutations and continual pretraining with trillions of tokens. Moreover, all detections can be performed on a single PC within an hour, making MDIR both efficient and accessible.

Abstract: In recent years, concerns about intellectual property (IP) in large language
models (LLMs) have grown significantly. Plagiarizing other LLMs (through direct
weight copying, upcycling, pruning, or continual pretraining) and claiming
authorship without properly attributing to the original license, is a serious
misconduct that can lead to significant financial and reputational harm to the
original developers. However, existing methods for detecting LLM plagiarism
fall short in key areas. They fail to accurately reconstruct weight
correspondences, lack the ability to compute statistical significance measures
such as $p$-values, and may mistakenly flag models trained on similar data as
being related. To address these limitations, we propose Matrix-Driven Instant
Review (MDIR), a novel method that leverages matrix analysis and Large
Deviation Theory. MDIR achieves accurate reconstruction of weight
relationships, provides rigorous $p$-value estimation, and focuses exclusively
on weight similarity without requiring full model inference. Experimental
results demonstrate that MDIR reliably detects plagiarism even after extensive
transformations, such as random permutations and continual pretraining with
trillions of tokens. Moreover, all detections can be performed on a single PC
within an hour, making MDIR both efficient and accessible.

</details>


### [33] [Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering](https://arxiv.org/abs/2508.06345)
*Yanbin Wei,Jiangyue Yan,Chun Kang,Yang Chen,Hua Liu,James T. Kwok,Yu Zhang*

Main category: cs.CL

TL;DR: DynamicTRF通过动态选择最佳图表示来提高大型多模态模型在图QA中的准确性和简洁性。


<details>
  <summary>Details</summary>
Motivation: 当前大多数方法只使用单一类型的图表示，即拓扑表示形式(TRF)，如提示统一文本描述或样式固定的视觉样式。这些“一刀切”的方法没有考虑到不同模型或任务的特定偏好，通常会导致不正确或过长的响应。

Method: DynamicTRF框架，它首先创建一个TRF偏好(TRFP)数据集，该数据集根据GRE分数对trf进行排序，以探测特定问题的TRF偏好。然后，它在TRFP数据集上训练一个TRF路由器，以便在推理过程中自适应地为每个问题分配来自$F_{ZS}$的最佳TRF。

Result: 在7个域内算法图QA任务和2个域外下游任务上的大量实验表明

Conclusion: DynamicTRF显著提高了LMM在零样本图QA方面的准确性。

Abstract: Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities
in diverse domain question-answering (QA) tasks, including graph QA that
involves complex graph topologies. However, most current approaches use only a
single type of graph representation, namely Topology Representation Form (TRF),
such as prompt-unified text descriptions or style-fixed visual styles. Those
"one-size-fits-all" approaches fail to consider the specific preferences of
different models or tasks, often leading to incorrect or overly long responses.
To address this, we first analyze the characteristics and weaknesses of
existing TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to
zero-shot graph QA. We then introduce a new metric, Graph Response Efficiency
(GRE), which measures the balance between the performance and the brevity in
graph QA. Built on these, we develop the DynamicTRF framework, which aims to
improve both the accuracy and conciseness of graph QA. To be specific,
DynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based
on their GRE scores, to probe the question-specific TRF preferences. Then it
trains a TRF router on the TRFP dataset, to adaptively assign the best TRF from
$F_{ZS}$ for each question during the inference. Extensive experiments across 7
in-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show
that DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms
of accuracy

</details>


### [34] [Cyberbullying Detection via Aggression-Enhanced Prompting](https://arxiv.org/abs/2508.06360)
*Aisha Saeid,Anu Sabu,Girish A. Koushik,Ferrante Neri,Diptesh Kanojia*

Main category: cs.CL

TL;DR: This study uses aggression detection as an auxiliary task to improve the performance of LLMs in cyberbullying detection. The enriched prompt pipeline approach consistently outperforms standard LoRA fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Detecting cyberbullying on social media remains a critical challenge due to its subtle and varied expressions. This study investigates whether integrating aggression detection as an auxiliary task within a unified training framework can enhance the generalisation and performance of large language models (LLMs) in cyberbullying detection.

Method: Experiments are conducted on five aggression datasets and one cyberbullying dataset using instruction-tuned LLMs. We evaluated multiple strategies: zero-shot, few-shot, independent LoRA fine-tuning, and multi-task learning (MTL). Given the inconsistent results of MTL, we propose an enriched prompt pipeline approach in which aggression predictions are embedded into cyberbullying detection prompts to provide contextual augmentation.

Result: Preliminary results show that the enriched prompt pipeline consistently outperforms standard LoRA fine-tuning, indicating that aggression-informed context significantly boosts cyberbullying detection.

Conclusion: This study highlights the potential of auxiliary tasks, such as aggression detection, to improve the generalisation of LLMs for safety-critical applications on social networks.

Abstract: Detecting cyberbullying on social media remains a critical challenge due to
its subtle and varied expressions. This study investigates whether integrating
aggression detection as an auxiliary task within a unified training framework
can enhance the generalisation and performance of large language models (LLMs)
in cyberbullying detection. Experiments are conducted on five aggression
datasets and one cyberbullying dataset using instruction-tuned LLMs. We
evaluated multiple strategies: zero-shot, few-shot, independent LoRA
fine-tuning, and multi-task learning (MTL). Given the inconsistent results of
MTL, we propose an enriched prompt pipeline approach in which aggression
predictions are embedded into cyberbullying detection prompts to provide
contextual augmentation. Preliminary results show that the enriched prompt
pipeline consistently outperforms standard LoRA fine-tuning, indicating that
aggression-informed context significantly boosts cyberbullying detection. This
study highlights the potential of auxiliary tasks, such as aggression
detection, to improve the generalisation of LLMs for safety-critical
applications on social networks.

</details>


### [35] [Evaluating Style-Personalized Text Generation: Challenges and Directions](https://arxiv.org/abs/2508.06374)
*Anubhav Jangra,Bahareh Sarrafzadeh,Adrian de Wynter,Silviu Cucerzan,Sujay Kumar Jauhar*

Main category: cs.CL

TL;DR: 这项研究调查了低资源风格个性化文本生成中的评估问题，发现像 BLEU 和 ROUGE 这样的常用指标效果不佳。研究建议使用更多样化的指标组合来进行评估。


<details>
  <summary>Details</summary>
Motivation: 先前关于风格个性化文本生成的研究已经构建了工具和基准，但在低资源作者风格个性化文本生成领域的评估探索有限。这项工作质疑了广泛采用的评估指标（如 BLEU 和 ROUGE）的有效性，并探索了其他评估范式，如风格嵌入和 LLM-as-judge，以全面评估风格个性化文本生成任务。

Method: 通过风格判别基准评估各种指标及其集成，该基准涵盖八个写作任务，并在三个设置中进行评估：领域判别、作者归属和 LLM 个性化与非个性化判别。

Result: 我们提供了确凿的证据，表明应该采用多样化评估指标的集成来有效评估风格个性化文本生成。

Conclusion: 应该采用多样化评估指标的集成来有效评估风格个性化文本生成。

Abstract: While prior research has built tools and benchmarks towards style
personalized text generation, there has been limited exploration of evaluation
in low-resource author style personalized text generation space. Through this
work, we question the effectiveness of the widely adopted evaluation metrics
like BLEU and ROUGE, and explore other evaluation paradigms such as style
embeddings and LLM-as-judge to holistically evaluate the style personalized
text generation task. We evaluate these metrics and their ensembles using our
style discrimination benchmark, that spans eight writing tasks, and evaluates
across three settings, domain discrimination, authorship attribution, and LLM
personalized vs non-personalized discrimination. We provide conclusive evidence
to adopt ensemble of diverse evaluation metrics to effectively evaluate style
personalized text generation.

</details>


### [36] [LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing](https://arxiv.org/abs/2508.06388)
*Lanlan Qiu,Xiao Pu,Yeqi Feng,Tianxing He*

Main category: cs.CL

TL;DR: This paper introduces ChatAnime, a dataset for Emotionally Supportive Role-Playing (ESRP) with anime characters, and finds that LLMs can surpass humans in role-playing and emotional support.


<details>
  <summary>Details</summary>
Motivation: there remains a significant research gap in combining role-playing conversations and providing emotional support to enable emotionally supportive interactions with virtual characters

Method: introducing ChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset,collecting dialogue data from 10 LLMs and 40 Chinese anime enthusiasts,designing a user experience-oriented evaluation system featuring 9 fine-grained metrics across three dimensions

Result: the dataset comprises 2,400 human-written and 24,000 LLM-generated answers, supported by over 132,000 human annotations

Conclusion: top-performing LLMs surpass human fans in role-playing and emotional support, while humans still lead in response diversity

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
role-playing conversations and providing emotional support as separate research
directions. However, there remains a significant research gap in combining
these capabilities to enable emotionally supportive interactions with virtual
characters. To address this research gap, we focus on anime characters as a
case study because of their well-defined personalities and large fan bases.
This choice enables us to effectively evaluate how well LLMs can provide
emotional support while maintaining specific character traits. We introduce
ChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We
first thoughtfully select 20 top-tier characters from popular anime communities
and design 60 emotion-centric real-world scenario questions. Then, we execute a
nationwide selection process to identify 40 Chinese anime enthusiasts with
profound knowledge of specific characters and extensive experience in
role-playing. Next, we systematically collect two rounds of dialogue data from
10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP
performance of LLMs, we design a user experience-oriented evaluation system
featuring 9 fine-grained metrics across three dimensions: basic dialogue,
role-playing and emotional support, along with an overall metric for response
diversity. In total, the dataset comprises 2,400 human-written and 24,000
LLM-generated answers, supported by over 132,000 human annotations.
Experimental results show that top-performing LLMs surpass human fans in
role-playing and emotional support, while humans still lead in response
diversity. We hope this work can provide valuable resources and insights for
future research on optimizing LLMs in ESRP. Our datasets are available at
https://github.com/LanlanQiu/ChatAnime.

</details>


### [37] [Quantifying Conversation Drift in MCP via Latent Polytope](https://arxiv.org/abs/2508.06418)
*Haoran Shi,Hongwei Yao,Shuo Shao,Shaopeng Jiao,Ziqi Peng,Zhan Qin,Cong Wang*

Main category: cs.CL

TL;DR: MCP enhances LLMs but has security risks. SecMCP uses a latent polytope-based method to detect conversation drift and prevent attacks, achieving high detection accuracy.


<details>
  <summary>Details</summary>
Motivation: The Model Context Protocol (MCP) enhances LLMs but introduces security and privacy risks due to its non-isolated execution context. Existing defenses are inadequate.

Method: A novel latent polytope-based methodology is used to quantify conversation drift by modeling LLM activation vectors within a latent polytope space.

Result: SecMCP achieves robust detection with AUROC scores exceeding 0.915 on three state-of-the-art LLMs (Llama3, Vicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA).

Conclusion: SecMCP, a secure framework, effectively detects conversation drift by modeling LLM activation vectors within a latent polytope space, enabling proactive detection of hijacking, misleading, and data exfiltration. It demonstrates robust detection with high AUROC scores while maintaining system usability.

Abstract: The Model Context Protocol (MCP) enhances large language models (LLMs) by
integrating external tools, enabling dynamic aggregation of real-time data to
improve task execution. However, its non-isolated execution context introduces
critical security and privacy risks. In particular, adversarially crafted
content can induce tool poisoning or indirect prompt injection, leading to
conversation hijacking, misinformation propagation, or data exfiltration.
Existing defenses, such as rule-based filters or LLM-driven detection, remain
inadequate due to their reliance on static signatures, computational
inefficiency, and inability to quantify conversational hijacking. To address
these limitations, we propose SecMCP, a secure framework that detects and
quantifies conversation drift, deviations in latent space trajectories induced
by adversarial external knowledge. By modeling LLM activation vectors within a
latent polytope space, SecMCP identifies anomalous shifts in conversational
dynamics, enabling proactive detection of hijacking, misleading, and data
exfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3,
Vicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA),
demonstrating robust detection with AUROC scores exceeding 0.915 while
maintaining system usability. Our contributions include a systematic
categorization of MCP security threats, a novel latent polytope-based
methodology for quantifying conversation drift, and empirical validation of
SecMCP's efficacy.

</details>


### [38] [Memp: Exploring Agent Procedural Memory](https://arxiv.org/abs/2508.06433)
*Runnan Fang,Yuan Liang,Xiaobin Wang,Jialong Wu,Shuofei Qiao,Pengjun Xie,Fei Huang,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: This paper introduces Memp, a learnable and updatable procedural memory for LLM agents that improves performance and efficiency on tasks and can be migrated to weaker models.


<details>
  <summary>Details</summary>
Motivation: LLMs based agents excel at diverse tasks, yet they suffer from brittle procedural memory that is manually engineered or entangled in static parameters.  This work investigates strategies to endow agents with a learnable, updatable, and lifelong procedural memory.

Method: Memp distills past agent trajectories into both fine-grained, step-by-step instructions and higher-level, script-like abstractions, and explore the impact of different strategies for Build, Retrieval, and Update of procedural memory.  Coupled with a dynamic regimen that continuously updates, corrects, and deprecates its contents, this repository evolves in lockstep with new experience.

Result: Empirical evaluation on TravelPlanner and ALFWorld shows that as the memory repository is refined, agents achieve steadily higher success rates and greater efficiency on analogous tasks. Moreover, procedural memory built from a stronger model retains its value: migrating the procedural memory to a weaker model yields substantial performance gains.

Conclusion: Agents achieve steadily higher success rates and greater efficiency on analogous tasks as the memory repository is refined. Migrating the procedural memory to a weaker model yields substantial performance gains.

Abstract: Large Language Models (LLMs) based agents excel at diverse tasks, yet they
suffer from brittle procedural memory that is manually engineered or entangled
in static parameters. In this work, we investigate strategies to endow agents
with a learnable, updatable, and lifelong procedural memory. We propose Memp
that distills past agent trajectories into both fine-grained, step-by-step
instructions and higher-level, script-like abstractions, and explore the impact
of different strategies for Build, Retrieval, and Update of procedural memory.
Coupled with a dynamic regimen that continuously updates, corrects, and
deprecates its contents, this repository evolves in lockstep with new
experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as
the memory repository is refined, agents achieve steadily higher success rates
and greater efficiency on analogous tasks. Moreover, procedural memory built
from a stronger model retains its value: migrating the procedural memory to a
weaker model yields substantial performance gains.

</details>


### [39] [Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages](https://arxiv.org/abs/2508.06435)
*Andrea Nasuto,Stefano Maria Iacus,Francisco Rowe,Devika Jain*

Main category: cs.CL

TL;DR: LLMs can generalize to new languages with limited fine-tuning, offering a cost-effective alternative to proprietary models for cross-lingual topic detection.


<details>
  <summary>Details</summary>
Motivation: Examine whether knowledge acquired through fine-tuning in a few languages can transfer to unseen languages that only appeared during pre-training.

Method: Fine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or multilingual data sets to classify immigration-related tweets from X/Twitter across 13 languages.

Result: LLMs fine-tuned in one or two languages can reliably classify immigration-related content in unseen languages. Identifying whether a tweet expresses a pro- or anti-immigration stance benefits from multilingual fine-tuning. Pre-training bias favours dominant languages, but even minimal exposure to under-represented languages during fine-tuning yields significant gains.The released models deliver 35 times faster inference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model.

Conclusion: LLMs fine-tuned in one or two languages can reliably classify immigration-related content in unseen languages, and structural biases can be corrected with lightweight interventions. Limited language coverage suffices for topic-level generalisation.

Abstract: Large language models (LLMs) are transforming social-science research by
enabling scalable, precise analysis. Their adaptability raises the question of
whether knowledge acquired through fine-tuning in a few languages can transfer
to unseen languages that only appeared during pre-training. To examine this, we
fine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or
multilingual data sets to classify immigration-related tweets from X/Twitter
across 13 languages, a domain characterised by polarised, culturally specific
discourse. We evaluate whether minimal language-specific fine-tuning enables
cross-lingual topic detection and whether adding targeted languages corrects
pre-training biases. Results show that LLMs fine-tuned in one or two languages
can reliably classify immigration-related content in unseen languages. However,
identifying whether a tweet expresses a pro- or anti-immigration stance
benefits from multilingual fine-tuning. Pre-training bias favours dominant
languages, but even minimal exposure to under-represented languages during
fine-tuning (as little as $9.62\times10^{-11}$ of the original pre-training
token volume) yields significant gains. These findings challenge the assumption
that cross-lingual mastery requires extensive multilingual training: limited
language coverage suffices for topic-level generalisation, and structural
biases can be corrected with lightweight interventions. By releasing
4-bit-quantised, LoRA fine-tuned models, we provide an open-source,
reproducible alternative to proprietary LLMs that delivers 35 times faster
inference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,
enabling scalable, inclusive research.

</details>


### [40] [Echoes of Automation: The Increasing Use of LLMs in Newsmaking](https://arxiv.org/abs/2508.06445)
*Abolfazl Ansari,Delvin Ce Zhang,Nafis Irtiza Tripto,Dongwon Lee*

Main category: cs.CL

TL;DR: 本研究发现GenAI在新闻中的使用显著增加，尤其是在地方和大学新闻中，它提高了可读性但降低了正式性。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能（GenAI）的迅速崛起，特别是大型语言模型（LLM），对新闻的完整性和作者身份构成威胁。

Method: 使用三种先进的AI文本检测器（例如，Binoculars、Fast-Detect GPT和GPTZero）分析来自主要、地方和大学新闻媒体的超过40,000篇新闻文章中AI生成的内容。

Result: 近年来，尤其是在地方和大学新闻中，GenAI的使用大幅增加。句子层面的分析显示，LLM通常用于新闻的引言部分，而结论通常是手动撰写的。

Conclusion: GenAI的使用提高了新闻报道的词汇丰富度和可读性，但降低了正式性，导致写作风格更加统一，尤其是在地方媒体中。

Abstract: The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns
for journalistic integrity and authorship. This study examines AI-generated
content across over 40,000 news articles from major, local, and college news
media, in various media formats. Using three advanced AI-text detectors (e.g.,
Binoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of
GenAI use in recent years, especially in local and college news. Sentence-level
analysis reveals LLMs are often used in the introduction of news, while
conclusions usually written manually. Linguistic analysis shows GenAI boosts
word richness and readability but lowers formality, leading to more uniform
writing styles, particularly in local media.

</details>


### [41] [SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning](https://arxiv.org/abs/2508.06447)
*Lingkun Long,Rubing Yang,Yushi Huang,Desheng Hui,Ao Zhou,Jianlei Yang*

Main category: cs.CL

TL;DR: SlimInfer accelerates LLM inference by pruning less critical prompt tokens during the forward pass, achieving significant speedups without performance loss.


<details>
  <summary>Details</summary>
Motivation: Long-context inference for Large Language Models (LLMs) is heavily limited by high computational demands. While several existing methods optimize attention computation, they still process the full set of hidden states at each layer, limiting overall efficiency.

Method: SlimInfer introduces a dynamic fine-grained pruning mechanism that accurately removes redundant tokens of hidden state at intermediate layers. This layer-wise pruning naturally enables an asynchronous KV cache manager that prefetches required token blocks without complex predictors, reducing both memory usage and I/O costs.

Result: SlimInfer can achieve up to 2.53x time-to-first-token (TTFT) speedup and 1.88x end-to-end latency reduction for LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on LongBench.

Conclusion: SlimInfer can achieve up to 2.53x time-to-first-token (TTFT) speedup and 1.88x end-to-end latency reduction for LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on LongBench.

Abstract: Long-context inference for Large Language Models (LLMs) is heavily limited by
high computational demands. While several existing methods optimize attention
computation, they still process the full set of hidden states at each layer,
limiting overall efficiency. In this work, we propose SlimInfer, an innovative
framework that aims to accelerate inference by directly pruning less critical
prompt tokens during the forward pass. Our key insight is an information
diffusion phenomenon: As information from critical tokens propagates through
layers, it becomes distributed across the entire sequence. This diffusion
process suggests that LLMs can maintain their semantic integrity when excessive
tokens, even including these critical ones, are pruned in hidden states.
Motivated by this, SlimInfer introduces a dynamic fine-grained pruning
mechanism that accurately removes redundant tokens of hidden state at
intermediate layers. This layer-wise pruning naturally enables an asynchronous
KV cache manager that prefetches required token blocks without complex
predictors, reducing both memory usage and I/O costs. Extensive experiments
show that SlimInfer can achieve up to $\mathbf{2.53\times}$ time-to-first-token
(TTFT) speedup and $\mathbf{1.88\times}$ end-to-end latency reduction for
LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on
LongBench. Our code will be released upon acceptance.

</details>


### [42] [GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models](https://arxiv.org/abs/2508.06471)
*GLM-4. 5 Team,:,Aohan Zeng,Xin Lv,Qinkai Zheng,Zhenyu Hou,Bin Chen,Chengxing Xie,Cunxiang Wang,Da Yin,Hao Zeng,Jiajie Zhang,Kedong Wang,Lucen Zhong,Mingdao Liu,Rui Lu,Shulin Cao,Xiaohan Zhang,Xuancheng Huang,Yao Wei,Yean Cheng,Yifan An,Yilin Niu,Yuanhao Wen,Yushi Bai,Zhengxiao Du,Zihan Wang,Zilin Zhu,Bohan Zhang,Bosi Wen,Bowen Wu,Bowen Xu,Can Huang,Casey Zhao,Changpeng Cai,Chao Yu,Chen Li,Chendi Ge,Chenghua Huang,Chenhui Zhang,Chenxi Xu,Chenzheng Zhu,Chuang Li,Congfeng Yin,Daoyan Lin,Dayong Yang,Dazhi Jiang,Ding Ai,Erle Zhu,Fei Wang,Gengzheng Pan,Guo Wang,Hailong Sun,Haitao Li,Haiyang Li,Haiyi Hu,Hanyu Zhang,Hao Peng,Hao Tai,Haoke Zhang,Haoran Wang,Haoyu Yang,He Liu,He Zhao,Hongwei Liu,Hongxi Yan,Huan Liu,Huilong Chen,Ji Li,Jiajing Zhao,Jiamin Ren,Jian Jiao,Jiani Zhao,Jianyang Yan,Jiaqi Wang,Jiayi Gui,Jiayue Zhao,Jie Liu,Jijie Li,Jing Li,Jing Lu,Jingsen Wang,Jingwei Yuan,Jingxuan Li,Jingzhao Du,Jinhua Du,Jinxin Liu,Junkai Zhi,Junli Gao,Ke Wang,Lekang Yang,Liang Xu,Lin Fan,Lindong Wu,Lintao Ding,Lu Wang,Man Zhang,Minghao Li,Minghuan Xu,Mingming Zhao,Mingshu Zhai,Pengfan Du,Qian Dong,Shangde Lei,Shangqing Tu,Shangtong Yang,Shaoyou Lu,Shijie Li,Shuang Li,Shuang-Li,Shuxun Yang,Sibo Yi,Tianshu Yu,Wei Tian,Weihan Wang,Wenbo Yu,Weng Lam Tam,Wenjie Liang,Wentao Liu,Xiao Wang,Xiaohan Jia,Xiaotao Gu,Xiaoying Ling,Xin Wang,Xing Fan,Xingru Pan,Xinyuan Zhang,Xinze Zhang,Xiuqing Fu,Xunkai Zhang,Yabo Xu,Yandong Wu,Yida Lu,Yidong Wang,Yilin Zhou,Yiming Pan,Ying Zhang,Yingli Wang,Yingru Li,Yinpei Su,Yipeng Geng,Yitong Zhu,Yongkun Yang,Yuhang Li,Yuhao Wu,Yujiang Li,Yunan Liu,Yunqing Wang,Yuntao Li,Yuxuan Zhang,Zezhen Liu,Zhen Yang,Zhengda Zhou,Zhongpei Qiao,Zhuoer Feng,Zhuorui Liu,Zichen Zhang,Zihan Wang,Zijun Yao,Zikang Wang,Ziqiang Liu,Ziwei Chai,Zixuan Li,Zuodong Zhao,Wenguang Chen,Jidong Zhai,Bin Xu,Minlie Huang,Hongning Wang,Juanzi Li,Yuxiao Dong,Jie Tang*

Main category: cs.CL

TL;DR: GLM-4.5 is an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters. It achieves strong performance across agentic, reasoning, and coding (ARC) tasks.


<details>
  <summary>Details</summary>
Motivation: To advance research in reasoning and agentic AI systems.

Method: a hybrid reasoning method that supports both thinking and direct response modes. Multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning

Result: GLM-4.5 scores 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified.

Conclusion: GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, ranking 3rd overall and 2nd on agentic benchmarks with fewer parameters than competitors. The models are released to advance research.

Abstract: We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language
model with 355B total parameters and 32B activated parameters, featuring a
hybrid reasoning method that supports both thinking and direct response modes.
Through multi-stage training on 23T tokens and comprehensive post-training with
expert model iteration and reinforcement learning, GLM-4.5 achieves strong
performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on
TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer
parameters than several competitors, GLM-4.5 ranks 3rd overall among all
evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B
parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance
research in reasoning and agentic AI systems. Code, models, and more
information are available at https://github.com/zai-org/GLM-4.5.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [43] [Boosting Adversarial Transferability via Residual Perturbation Attack](https://arxiv.org/abs/2508.05689)
*Jinjia Peng,Zeze Tao,Huibing Wang,Meng Wang,Yang Wang*

Main category: cs.CV

TL;DR: This paper proposes ResPA, a novel transfer-based attack method that uses the residual gradient as the perturbation direction to improve the transferability of adversarial examples in black-box attacks.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks are susceptible to adversarial examples, and transfer-based attacks create adversarial examples for surrogate models and transfer these examples to target models under black-box scenarios. Recent studies reveal that adversarial examples in flat loss landscapes exhibit superior transferability to alleviate overfitting on surrogate models. However, the prior arts overlook the influence of perturbation directions, resulting in limited transferability.

Method: The paper proposes a novel attack method, named Residual Perturbation Attack (ResPA), relying on the residual gradient as the perturbation direction to guide the adversarial examples toward the flat regions of the loss function. ResPA conducts an exponential moving average on the input gradients to obtain the first moment as the reference gradient, which encompasses the direction of historical gradients. ResPA further considers the residual between the current gradient and the reference gradient to capture the changes in the global perturbation direction.

Result: Experimental results demonstrate the better transferability of ResPA than the existing typical transfer-based attack methods, while the transferability can be further improved by combining ResPA with the current input transformation methods.

Conclusion: ResPA achieves better transferability than existing transfer-based attack methods, and can be further improved by combining it with current input transformation methods.

Abstract: Deep neural networks are susceptible to adversarial examples while suffering
from incorrect predictions via imperceptible perturbations. Transfer-based
attacks create adversarial examples for surrogate models and transfer these
examples to target models under black-box scenarios. Recent studies reveal that
adversarial examples in flat loss landscapes exhibit superior transferability
to alleviate overfitting on surrogate models. However, the prior arts overlook
the influence of perturbation directions, resulting in limited transferability.
In this paper, we propose a novel attack method, named Residual Perturbation
Attack (ResPA), relying on the residual gradient as the perturbation direction
to guide the adversarial examples toward the flat regions of the loss function.
Specifically, ResPA conducts an exponential moving average on the input
gradients to obtain the first moment as the reference gradient, which
encompasses the direction of historical gradients. Instead of heavily relying
on the local flatness that stems from the current gradients as the perturbation
direction, ResPA further considers the residual between the current gradient
and the reference gradient to capture the changes in the global perturbation
direction. The experimental results demonstrate the better transferability of
ResPA than the existing typical transfer-based attack methods, while the
transferability can be further improved by combining ResPA with the current
input transformation methods. The code is available at
https://github.com/ZezeTao/ResPA.

</details>


### [44] [Generalized Few-Shot Out-of-Distribution Detection](https://arxiv.org/abs/2508.05732)
*Pinxuan Li,Bing Cao,Changqing Zhang,Qinghua Hu*

Main category: cs.CV

TL;DR: Proposes a Generalized Few-shot OOD Detection (GOOD) framework to improve generalization by incorporating a General Knowledge Model (GKM) and a Knowledge Dynamic Embedding (KDE) mechanism.


<details>
  <summary>Details</summary>
Motivation: Existing few-shot OOD detection methods have insufficient generalization capability and tend to overfit to limited training data.

Method: A Generalized Few-shot OOD Detection (GOOD) framework with a Knowledge Dynamic Embedding (KDE) mechanism.

Result: Experiments on real-world OOD benchmarks demonstrate the superiority of the proposed approach.

Conclusion: The proposed GOOD framework with KDE mechanism improves the generalization ability of few-shot OOD detection, achieving superior performance on real-world benchmarks.

Abstract: Few-shot Out-of-Distribution (OOD) detection has emerged as a critical
research direction in machine learning for practical deployment. Most existing
Few-shot OOD detection methods suffer from insufficient generalization
capability for the open world. Due to the few-shot learning paradigm, the OOD
detection ability is often overfit to the limited training data itself, thus
degrading the performance on generalized data and performing inconsistently
across different scenarios. To address this challenge, we proposed a
Generalized Few-shot OOD Detection (GOOD) framework, which empowers the general
knowledge of the OOD detection model with an auxiliary General Knowledge Model
(GKM), instead of directly learning from few-shot data. We proceed to reveal
the few-shot OOD detection from a generalization perspective and theoretically
derive the Generality-Specificity balance (GS-balance) for OOD detection, which
provably reduces the upper bound of generalization error with a general
knowledge model. Accordingly, we propose a Knowledge Dynamic Embedding (KDE)
mechanism to adaptively modulate the guidance of general knowledge. KDE
dynamically aligns the output distributions of the OOD detection model to the
general knowledge model based on the Generalized Belief (G-Belief) of GKM,
thereby boosting the GS-balance. Experiments on real-world OOD benchmarks
demonstrate our superiority. Codes will be available.

</details>


### [45] [UnGuide: Learning to Forget with LoRA-Guided Diffusion Models](https://arxiv.org/abs/2508.05755)
*Agnieszka Polowczyk,Alicja Polowczyk,Dawid Malarz,Artur Kasymov,Marcin Mazur,Jacek Tabor,Przemysław Spurek*

Main category: cs.CV

TL;DR: UnGuide是一种新的unlearning方法，它结合了UnGuidance来精确控制unlearning过程，实现了受控概念移除，并保留了扩散模型的表达能力。


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型的最新进展加剧了人们对其潜在误用的担忧，尤其是在生成有害或误导性内容方面。这突显了对有效的机器unlearning的迫切需求，即在不影响整体性能的情况下，从预训练模型中删除特定的知识或概念。LoRA通常会无意中更改不相关的内容，从而导致图像保真度和真实感降低。

Method: 引入UnGuide，一种新颖的方法，它结合了UnGuidance，一种动态推理机制，该机制利用无分类器指导（CFG）来精确控制unlearning过程。UnGuide基于去噪过程的几个第一步的稳定性来调节指导尺度，从而通过LoRA适配器实现选择性unlearning。

Result: 经验结果表明，UnGuide实现了受控概念移除，并保留了扩散模型的表达能力，在对象擦除和显式内容移除任务中优于现有的基于LoRA的方法。

Conclusion: UnGuide在受控概念移除方面表现出色，并保留了扩散模型的表达能力，在对象擦除和显式内容移除任务中优于现有的基于LoRA的方法。

Abstract: Recent advances in large-scale text-to-image diffusion models have heightened
concerns about their potential misuse, especially in generating harmful or
misleading content. This underscores the urgent need for effective machine
unlearning, i.e., removing specific knowledge or concepts from pretrained
models without compromising overall performance. One possible approach is
Low-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models
for targeted unlearning. However, LoRA often inadvertently alters unrelated
content, leading to diminished image fidelity and realism. To address this
limitation, we introduce UnGuide -- a novel approach which incorporates
UnGuidance, a dynamic inference mechanism that leverages Classifier-Free
Guidance (CFG) to exert precise control over the unlearning process. UnGuide
modulates the guidance scale based on the stability of a few first steps of
denoising processes, enabling selective unlearning by LoRA adapter. For prompts
containing the erased concept, the LoRA module predominates and is
counterbalanced by the base model; for unrelated prompts, the base model
governs generation, preserving content fidelity. Empirical results demonstrate
that UnGuide achieves controlled concept removal and retains the expressive
power of diffusion models, outperforming existing LoRA-based methods in both
object erasure and explicit content removal tasks.

</details>


### [46] [Improving Masked Style Transfer using Blended Partial Convolution](https://arxiv.org/abs/2508.05769)
*Seyed Hadi Seyed,Ayberk Cansever,David Hart*

Main category: cs.CV

TL;DR: Proposes a partial-convolution-based style transfer network for applying style transfer to specific regions of interest, improving stylization accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods apply artistic style transfer to the whole image, but individual users may only need to apply a style transfer to a specific region in the image, and simply masking the image after stylization improperly captures the style features in the region of interest.

Method: partial-convolution-based style transfer network with network-internal blending techniques

Result: Visually and quantitatively improved stylization using examples from the SA-1B dataset.

Conclusion: This work proposes a partial-convolution-based style transfer network that accurately applies the style features exclusively to the region of interest and present network-internal blending techniques that account for imperfections in the region selection. This approach visually and quantitatively improves stylization.

Abstract: Artistic style transfer has long been possible with the advancements of
convolution- and transformer-based neural networks. Most algorithms apply the
artistic style transfer to the whole image, but individual users may only need
to apply a style transfer to a specific region in the image. The standard
practice is to simply mask the image after the stylization. This work shows
that this approach tends to improperly capture the style features in the region
of interest. We propose a partial-convolution-based style transfer network that
accurately applies the style features exclusively to the region of interest.
Additionally, we present network-internal blending techniques that account for
imperfections in the region selection. We show that this visually and
quantitatively improves stylization using examples from the SA-1B dataset. Code
is publicly available at https://github.com/davidmhart/StyleTransferMasked.

</details>


### [47] [MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with Rectified Flow and Region-specific Contrastive Loss](https://arxiv.org/abs/2508.05772)
*Can Zhao,Pengfei Guo,Dong Yang,Yucheng Tang,Yufan He,Benjamin Simon,Mason Belue,Stephanie Harmon,Baris Turkbey,Daguang Xu*

Main category: cs.CV

TL;DR: MAISI-v2 是一种快速、高质量的 3D 医学图像合成框架，它通过修正流加速并使用特定区域对比损失来提高图像质量和条件保真度，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的医学图像合成方法存在泛化性有限、推理速度慢以及与输入条件对齐较弱的问题。MAISI 解决了泛化性问题，但仍然存在推理速度慢和条件一致性有限的问题。

Method: MAISI-v2，一种结合了修正流的加速 3D 医学图像合成框架，并引入了一种新的特定区域对比损失。

Result: MAISI-v2 实现了 SOTA 图像质量，速度是潜在扩散模型的 33 倍。合成图像可用于数据增强。

Conclusion: MAISI-v2 通过整合修正流实现了快速高质量的生成，并通过引入新的特定区域对比损失来增强条件保真度，从而在医学图像合成方面达到了 SOTA 的图像质量，速度是潜在扩散模型的 33 倍。合成图像可用于数据增强。

Abstract: Medical image synthesis is an important topic for both clinical and research
applications. Recently, diffusion models have become a leading approach in this
area. Despite their strengths, many existing methods struggle with (1) limited
generalizability that only work for specific body regions or voxel spacings,
(2) slow inference, which is a common issue for diffusion models, and (3) weak
alignment with input conditions, which is a critical issue for medical imaging.
MAISI, a previously proposed framework, addresses generalizability issues but
still suffers from slow inference and limited condition consistency. In this
work, we present MAISI-v2, the first accelerated 3D medical image synthesis
framework that integrates rectified flow to enable fast and high quality
generation. To further enhance condition fidelity, we introduce a novel
region-specific contrastive loss to enhance the sensitivity to region of
interest. Our experiments show that MAISI-v2 can achieve SOTA image quality
with $33 \times$ acceleration for latent diffusion model. We also conducted a
downstream segmentation experiment to show that the synthetic images can be
used for data augmentation. We release our code, training details, model
weights, and a GUI demo to facilitate reproducibility and promote further
development within the community.

</details>


### [48] [Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging Tasks](https://arxiv.org/abs/2508.05783)
*Mengyu Li,Guoyao Shen,Chad W. Farris,Xin Zhang*

Main category: cs.CV

TL;DR: 本文提出了一个用于在不同的脑部成像任务中对预训练的MRI transformers进行小样本部署的实用框架。


<details>
  <summary>Details</summary>
Motivation: 基于transformers的机器学习在医学成像中显示出巨大的潜力，但由于带注释的数据稀缺，其实际应用仍然受到限制。因此本文提出了一个实用的框架，用于在不同的脑部成像任务中对预训练的MRI transformers进行小样本部署。

Method: 利用大规模多队列脑部MRI数据集上的Masked Autoencoder (MAE) 预训练策略，获得了高度可转移的潜在表征，该表征可以在任务和数据集之间很好地泛化。对于分类等高级任务，冻结的MAE编码器与轻量级线性头相结合，以最小的监督实现了MRI序列识别中的最先进的精度。对于分割等低级任务，提出了MAE-FUnet，这是一种混合架构，它将多尺度CNN特征与预训练的MAE嵌入融合在一起。该模型在数据有限的条件下，在颅骨剥离和多类解剖分割方面始终优于其他强大的基线。

Result: 对于分类等高级任务，冻结的MAE编码器与轻量级线性头相结合，以最小的监督实现了MRI序列识别中的最先进的精度。对于分割等低级任务，MAE-FUnet在数据有限的条件下，在颅骨剥离和多类解剖分割方面始终优于其他强大的基线。

Conclusion: 该框架展示了效率、稳定性和可扩展性，表明其适用于资源匮乏的临床环境和更广泛的神经影像应用。

Abstract: Machine learning using transformers has shown great potential in medical
imaging, but its real-world applicability remains limited due to the scarcity
of annotated data. In this study, we propose a practical framework for the
few-shot deployment of pretrained MRI transformers in diverse brain imaging
tasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a
large-scale, multi-cohort brain MRI dataset comprising over 31 million slices,
we obtain highly transferable latent representations that generalize well
across tasks and datasets. For high-level tasks such as classification, a
frozen MAE encoder combined with a lightweight linear head achieves
state-of-the-art accuracy in MRI sequence identification with minimal
supervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a
hybrid architecture that fuses multiscale CNN features with pretrained MAE
embeddings. This model consistently outperforms other strong baselines in both
skull stripping and multi-class anatomical segmentation under data-limited
conditions. With extensive quantitative and qualitative evaluations, our
framework demonstrates efficiency, stability, and scalability, suggesting its
suitability for low-resource clinical environments and broader neuroimaging
applications.

</details>


### [49] [Optimization-Free Style Transfer for 3D Gaussian Splats](https://arxiv.org/abs/2508.05813)
*Raphael Du Sablon,David Hart*

Main category: cs.CV

TL;DR: A fast, reconstruction- and optimization-free method for stylizing 3D Gaussian splats is proposed, using a graph structure and surface-based stylization.


<details>
  <summary>Details</summary>
Motivation: Existing style transfer methods for 3D Gaussian splats require reconstruction or fine-tuning, or optimizing a feature extraction network.

Method: A reconstruction- and optimization-free approach is proposed, generating a graph structure across the implicit surface of the splat representation. A feed-forward, surface-based stylization method is then used and interpolated back to the individual splats.

Result: Fast stylization of splats is achieved, with speeds under 2 minutes even on consumer-grade hardware. Quality results are demonstrated and compared to other methods.

Conclusion: The proposed approach achieves quality results and fast stylization of splats, with speeds under 2 minutes on consumer-grade hardware, demonstrating improvements over other 3D Gaussian splat style transfer methods.

Abstract: The task of style transfer for 3D Gaussian splats has been explored in many
previous works, but these require reconstructing or fine-tuning the splat while
incorporating style information or optimizing a feature extraction network on
the splat representation. We propose a reconstruction- and optimization-free
approach to stylizing 3D Gaussian splats. This is done by generating a graph
structure across the implicit surface of the splat representation. A
feed-forward, surface-based stylization method is then used and interpolated
back to the individual splats in the scene. This allows for any style image and
3D Gaussian splat to be used without any additional training or optimization.
This also allows for fast stylization of splats, achieving speeds under 2
minutes even on consumer-grade hardware. We demonstrate the quality results
this approach achieves and compare to other 3D Gaussian splat style transfer
methods. Code is publicly available at
https://github.com/davidmhart/FastSplatStyler.

</details>


### [50] [MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown Camera Poses](https://arxiv.org/abs/2508.05819)
*Jong-Ik Park,Carlee Joe-Wong,Gary K. Fedder*

Main category: cs.CV

TL;DR: MZEN is the first NeRF framework that natively handles multi-zoom image sets, extending NeRF to real-world factory settings.


<details>
  <summary>Details</summary>
Motivation: NeRF methods miss the fine-detailed structures that matter in industrial inspection, e.g., detecting sub-micron defects on a production line or analyzing chips with Scanning Electron Microscopy (SEM). The only way to expose fine structure is to add zoom-in images; yet, this breaks the multi-view consistency that pose-free NeRF training relies on.

Method: MZEN (i) augments the pin-hole camera model with an explicit, learnable zoom scalar that scales the focal length, and (ii) introduces a novel pose strategy: wide-field images are solved first to establish a global metric frame, and zoom-in images are then pose-primed to the nearest wide-field counterpart via a zoom-consistent crop-and-match procedure before joint refinement.

Result: MZEN consistently outperforms pose-free baselines and even high-resolution variants, boosting PSNR by up to $28 \%, SSIM by $10 \%, and reducing LPIPS by up to $222 \%.

Conclusion: MZEN extends NeRF to real-world factory settings, preserving global accuracy while capturing the micron-level details essential for industrial inspection.

Abstract: Neural Radiance Fields (NeRF) methods excel at 3D reconstruction from
multiple 2D images, even those taken with unknown camera poses. However, they
still miss the fine-detailed structures that matter in industrial inspection,
e.g., detecting sub-micron defects on a production line or analyzing chips with
Scanning Electron Microscopy (SEM). In these scenarios, the sensor resolution
is fixed and compute budgets are tight, so the only way to expose fine
structure is to add zoom-in images; yet, this breaks the multi-view consistency
that pose-free NeRF training relies on. We propose Multi-Zoom Enhanced NeRF
(MZEN), the first NeRF framework that natively handles multi-zoom image sets.
MZEN (i) augments the pin-hole camera model with an explicit, learnable zoom
scalar that scales the focal length, and (ii) introduces a novel pose strategy:
wide-field images are solved first to establish a global metric frame, and
zoom-in images are then pose-primed to the nearest wide-field counterpart via a
zoom-consistent crop-and-match procedure before joint refinement. Across eight
forward-facing scenes$\unicode{x2013}$synthetic TCAD models, real SEM of
micro-structures, and BLEFF objects$\unicode{x2013}$MZEN consistently
outperforms pose-free baselines and even high-resolution variants, boosting
PSNR by up to $28 \%$, SSIM by $10 \%$, and reducing LPIPS by up to $222 \%$.
MZEN, therefore, extends NeRF to real-world factory settings, preserving global
accuracy while capturing the micron-level details essential for industrial
inspection.

</details>


### [51] [TSMS-SAM2: Multi-scale Temporal Sampling Augmentation and Memory-Splitting Pruning for Promptable Video Object Segmentation and Tracking in Surgical Scenarios](https://arxiv.org/abs/2508.05829)
*Guoping Xu,Hua-Chieh Shao,You Zhang*

Main category: cs.CV

TL;DR: TSMS-SAM2 improves surgical video segmentation by addressing motion dynamics and memory redundancy in SAM2 with multi-temporal sampling and memory management.


<details>
  <summary>Details</summary>
Motivation: The application of foundation models like SAM2 in surgical video analysis is challenging due to complex motion dynamics and memory redundancy.

Method: The paper proposes TSMS-SAM2, a framework enhancing promptable VOST in surgical videos using multi-temporal-scale video sampling augmentation and a memory splitting and pruning mechanism.

Result: TSMS-SAM2 achieved the highest mean Dice scores of 95.24 and 86.73 on EndoVis2017 and EndoVis2018 datasets, respectively.

Conclusion: TSMS-SAM2 achieves state-of-the-art segmentation performance on EndoVis2017 and EndoVis2018 datasets, demonstrating its potential for robust and efficient segmentation in complex surgical scenarios.

Abstract: Promptable video object segmentation and tracking (VOST) has seen significant
advances with the emergence of foundation models like Segment Anything Model 2
(SAM2); however, their application in surgical video analysis remains
challenging due to complex motion dynamics and the redundancy of memory that
impedes effective learning. In this work, we propose TSMS-SAM2, a novel
framework that enhances promptable VOST in surgical videos by addressing
challenges of rapid object motion and memory redundancy in SAM2. TSMS-SAM2
introduces two key strategies: multi-temporal-scale video sampling augmentation
to improve robustness against motion variability, and a memory splitting and
pruning mechanism that organizes and filters past frame features for more
efficient and accurate segmentation. Evaluated on EndoVis2017 and EndoVis2018
datasets, TSMS-SAM2 achieved the highest mean Dice scores of 95.24 and 86.73,
respectively, outperforming prior SAM-based and task-specific methods.
Extensive ablation studies confirm the effectiveness of multiscale temporal
augmentation and memory splitting, highlighting the framework's potential for
robust, efficient segmentation in complex surgical scenarios. Our source code
will be available at https://github.com/apple1986/TSMS-SAM2.

</details>


### [52] [Temporal Cluster Assignment for Efficient Real-Time Video Segmentation](https://arxiv.org/abs/2508.05851)
*Ka-Wai Yung,Felix J. S. Bragman,Jialang Xu,Imanol Luengo,Danail Stoyanov,Evangelos B. Mazomenos*

Main category: cs.CV

TL;DR: 提出了一种名为时间聚类分配（TCA）的轻量级免微调策略，该策略通过利用时间连贯性来增强token聚类，从而提高视频分割的速度-精度权衡。


<details>
  <summary>Details</summary>
Motivation: Swin Transformer在视频分割中计算成本高，尤其是在视频密集预测中使用的大型变体中。虽然已经提出了token减少方法来缓解这个问题，但Swin的基于窗口的注意力机制需要每个窗口固定数量的token，限制了传统剪枝技术的适用性。同时，免训练token聚类方法在图像分割中显示出希望，但未能利用时间冗余。

Method: 引入时间聚类分配（TCA），一种轻量级且有效的免微调策略，通过利用跨帧的时间一致性来增强token聚类。

Result: 在YouTube-VIS 2019、YouTube-VIS 2021、OVIS和一个私有外科视频数据集上的大量评估表明，TCA始终如一地提高了现有基于聚类方法的速度-精度权衡。我们的结果表明，TCA在自然视频和特定领域视频中都能胜任。

Conclusion: TCA在多个视频分割数据集上提升了现有聚类方法的速度-精度权衡。

Abstract: Vision Transformers have substantially advanced the capabilities of
segmentation models across both image and video domains. Among them, the Swin
Transformer stands out for its ability to capture hierarchical, multi-scale
representations, making it a popular backbone for segmentation in videos.
However, despite its window-attention scheme, it still incurs a high
computational cost, especially in larger variants commonly used for dense
prediction in videos. This remains a major bottleneck for real-time,
resource-constrained applications. Whilst token reduction methods have been
proposed to alleviate this, the window-based attention mechanism of Swin
requires a fixed number of tokens per window, limiting the applicability of
conventional pruning techniques. Meanwhile, training-free token clustering
approaches have shown promise in image segmentation while maintaining window
consistency. Nevertheless, they fail to exploit temporal redundancy, missing a
key opportunity to further optimize video segmentation performance. We
introduce Temporal Cluster Assignment (TCA), a lightweight and effective,
fine-tuning-free strategy that enhances token clustering by leveraging temporal
coherence across frames. Instead of indiscriminately dropping redundant tokens,
TCA refines token clusters using temporal correlations, thereby retaining
fine-grained details while significantly reducing computation. Extensive
evaluations on YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and a private surgical
video dataset show that TCA consistently boosts the accuracy-speed trade-off of
existing clustering-based methods. Our results demonstrate that TCA generalizes
competently across both natural and domain-specific videos.

</details>


### [53] [VISTA: Vision-Language Imitation of Situational Thinking and Attention for Human-Like Driver Focus in Dynamic Environments](https://arxiv.org/abs/2508.05852)
*Kaiser Hamid,Khandakar Ashrafi Akbar,Nade Liang*

Main category: cs.CV

TL;DR: A new vision-language framework predicts driver attention shifts using natural language, improving performance and interpretability for explainable AI in autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Predicting driver visual attention is critical for autonomous driving and HCI research, but most prior studies focus on single moments in time using static RGB images.

Method: The authors fine-tuned LLaVA using high-quality captions from the BDD-A dataset, incorporating both low-level cues and top-down context.

Result: The fine-tuned model outperforms general-purpose VLMs in attention shift detection and interpretability. The approach offers a foundation for downstream tasks such as behavior forecasting, human-AI teaming, and multi-agent coordination.

Conclusion: This paper introduces a novel vision-language framework for predicting driver visual attention allocation and shifting in natural language, achieving superior performance in attention shift detection and interpretability compared to general-purpose VLMs.

Abstract: Driver visual attention prediction is a critical task in autonomous driving
and human-computer interaction (HCI) research. Most prior studies focus on
estimating attention allocation at a single moment in time, typically using
static RGB images such as driving scene pictures. In this work, we propose a
vision-language framework that models the changing landscape of drivers' gaze
through natural language, using few-shot and zero-shot learning on single RGB
images. We curate and refine high-quality captions from the BDD-A dataset using
human-in-the-loop feedback, then fine-tune LLaVA to align visual perception
with attention-centric scene understanding. Our approach integrates both
low-level cues and top-down context (e.g., route semantics, risk anticipation),
enabling language-based descriptions of gaze behavior. We evaluate performance
across training regimes (few shot, and one-shot) and introduce domain-specific
metrics for semantic alignment and response diversity. Results show that our
fine-tuned model outperforms general-purpose VLMs in attention shift detection
and interpretability. To our knowledge, this is among the first attempts to
generate driver visual attention allocation and shifting predictions in natural
language, offering a new direction for explainable AI in autonomous driving.
Our approach provides a foundation for downstream tasks such as behavior
forecasting, human-AI teaming, and multi-agent coordination.

</details>


### [54] [Multi-view Gaze Target Estimation](https://arxiv.org/abs/2508.05857)
*Qiaomu Miao,Vivek Raju Golani,Jingyi Xu,Progga Paromita Dutta,Minh Hoai,Dimitris Samaras*

Main category: cs.CV

TL;DR: 本文提出了一种利用多相机视图进行注视目标估计（GTE）的方法，该方法优于单视角方法，并引入了一个多视角数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的单视角方法面临面部遮挡、目标模糊和超出视野目标等挑战。

Method: 该方法采用多个相机视图作为输入，结合头部信息聚合（HIA）模块、基于不确定性的注视选择（UGS）模块和基于对极的场景注意力（ESA）模块。

Result: 该方法提高了准确性并扩大了适用性。

Conclusion: 该方法显著优于单视角基线，尤其是在第二个相机提供清晰的面部视图时。此外，我们的方法可以使用第二张视图中的人像来估计第一张视图中的注视目标，这是单视角 GTE 方法不具备的功能。

Abstract: This paper presents a method that utilizes multiple camera views for the gaze
target estimation (GTE) task. The approach integrates information from
different camera views to improve accuracy and expand applicability, addressing
limitations in existing single-view methods that face challenges such as face
occlusion, target ambiguity, and out-of-view targets. Our method processes a
pair of camera views as input, incorporating a Head Information Aggregation
(HIA) module for leveraging head information from both views for more accurate
gaze estimation, an Uncertainty-based Gaze Selection (UGS) for identifying the
most reliable gaze output, and an Epipolar-based Scene Attention (ESA) module
for cross-view background information sharing. This approach significantly
outperforms single-view baselines, especially when the second camera provides a
clear view of the person's face. Additionally, our method can estimate the gaze
target in the first view using the image of the person in the second view only,
a capability not possessed by single-view GTE methods. Furthermore, the paper
introduces a multi-view dataset for developing and evaluating multi-view GTE
methods. Data and code are available at
https://www3.cs.stonybrook.edu/~cvl/multiview_gte.html

</details>


### [55] [ETTA: Efficient Test-Time Adaptation for Vision-Language Models through Dynamic Embedding Updates](https://arxiv.org/abs/2508.05898)
*Hamidreza Dastmalchi,Aijun An,Ali cheraghian*

Main category: cs.CV

TL;DR: ETTA is proposed to improve the performance of VLMs under distribution shifts. It uses a Recursive Updating module and an Adaptive Ensemble module to refine the decision boundary and reduce prompt dependency, respectively. Experiments show that ETTA outperforms existing TTA models in both computational complexity and accuracy.


<details>
  <summary>Details</summary>
Motivation: Pretrained vision-language models (VLMs) like CLIP show strong zero-shot performance but struggle with generalization under distribution shifts. Current cache-based TTA models store only a limited set of high-confidence samples, restricting the decision boundary to these samples and ignoring the influence of other incoming test data.

Method: We propose Efficient Test-Time Adaptation (ETTA), introducing a Recursive Updating module that integrates all incoming test samples, progressively refining the decision boundary. ETTA also includes an Adaptive Ensemble module to reduce prompt dependency in image-to-text scores by dynamically selecting optimal prompts for each class. Furthermore, ETTA adaptively combines scores from both modules based on confidence levels, leveraging their complementary strengths.

Result: Extensive experiments on two benchmarks confirm that ETTA surpasses the state-of-the-art TTA models in computational complexity and accuracy.

Conclusion: ETTA surpasses the state-of-the-art TTA models in computational complexity and accuracy, setting a new standard for effective, efficient test-time adaptation.

Abstract: Pretrained vision-language models (VLMs) like CLIP show strong zero-shot
performance but struggle with generalization under distribution shifts.
Test-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test
data in new domains. While some TTA methods rely on prompt-tuning,
training-free cache-based approaches are preferred for efficiency. However,
current cache-based TTA models store only a limited set of high-confidence
samples, restricting the decision boundary to these samples and ignoring the
influence of other incoming test data. To address this, we propose Efficient
Test-Time Adaptation (ETTA), introducing a Recursive Updating module that
integrates all incoming test samples, progressively refining the decision
boundary. This strategy mimics an unbounded cache, dynamically updating
contextual embeddings for improved accuracy with minimal memory and
computational overhead. ETTA also includes an Adaptive Ensemble module to
reduce prompt dependency in image-to-text scores by dynamically selecting
optimal prompts for each class. Furthermore, ETTA adaptively combines scores
from both modules based on confidence levels, leveraging their complementary
strengths. Extensive experiments on two benchmarks confirm that ETTA surpasses
the state-of-the-art TTA models in computational complexity and accuracy,
setting a new standard for effective, efficient test-time adaptation. The code
has been released at https://github.com/hamidreza-dastmalchi/ETTA.

</details>


### [56] [HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing](https://arxiv.org/abs/2508.05899)
*Zixuan Bian,Ruohan Ren,Yue Yang,Chris Callison-Burch*

Main category: cs.CV

TL;DR: HOLODECK 2.0是一个先进的视觉-语言引导框架，用于 3D 世界生成，支持基于人类反馈的交互式场景编辑。


<details>
  <summary>Details</summary>
Motivation: 当前的 3D 场景设计仍然严重依赖于创作者的大量人工工作，并且现有的自动化方法难以生成开放域场景或支持灵活的编辑。因此，直接从文本生成 3D 世界越来越受到关注。

Method: HOLODECK 2.0利用视觉-语言模型 (VLM) 来识别和解析场景中需要的对象，并通过最先进的 3D 生成模型生成相应的高质量资产。然后，它迭代地应用从 VLM 导出的空间约束，以实现语义连贯且物理上合理的布局。

Result: HOLODECK 2.0可以生成多样化且风格丰富的 3D 场景（例如，逼真、卡通、动漫和赛博朋克风格），这些场景对细粒度的输入描述表现出高度的语义保真度，适用于室内和开放领域环境。人类评估和基于 CLIP 的评估表明，HOLODECK 2.0 有效地生成了与详细文本描述高度一致的高质量场景，并且在室内和开放领域场景中始终优于基线。

Conclusion: HOLODECK 2.0有效地生成了与详细文本描述高度一致的高质量场景，并且在室内和开放领域场景中始终优于基线。此外，它提供的编辑功能可以灵活地适应人类反馈，支持布局改进和风格一致的对象编辑。最后，它在程序化游戏建模中具有实际应用，可以生成视觉上丰富且身临其境的环境，从而可能提高效率。

Abstract: 3D scene generation plays a crucial role in gaming, artistic creation,
virtual reality and many other domains. However, current 3D scene design still
relies heavily on extensive manual effort from creators, and existing automated
methods struggle to generate open-domain scenes or support flexible editing. As
a result, generating 3D worlds directly from text has garnered increasing
attention. In this paper, we introduce HOLODECK 2.0, an advanced
vision-language-guided framework for 3D world generation with support for
interactive scene editing based on human feedback. HOLODECK 2.0 can generate
diverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and
cyberpunk styles) that exhibit high semantic fidelity to fine-grained input
descriptions, suitable for both indoor and open-domain environments. HOLODECK
2.0 leverages vision-language models (VLMs) to identify and parse the objects
required in a scene and generates corresponding high-quality assets via
state-of-the-art 3D generative models. It then iteratively applies spatial
constraints derived from the VLMs to achieve semantically coherent and
physically plausible layouts. Human evaluations and CLIP-based assessments
demonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely
aligned with detailed textual descriptions, consistently outperforming
baselines across indoor and open-domain scenarios. Additionally, we provide
editing capabilities that flexibly adapt to human feedback, supporting layout
refinement and style-consistent object edits. Finally, we present a practical
application of HOLODECK 2.0 in procedural game modeling, generating visually
rich and immersive environments, potentially boosting efficiency.

</details>


### [57] [Robust Image Stitching with Optimal Plane](https://arxiv.org/abs/2508.05903)
*Lang Nie,Yuan Mei,Kang Liao,Yunqiu Xu,Chunyu Lin,Bin Xiao*

Main category: cs.CV

TL;DR: RopStitch is an unsupervised deep image stitching framework that improves robustness and naturalness by using a dual-branch architecture and virtual optimal planes.


<details>
  <summary>Details</summary>
Motivation: ensure the robustness and naturalness of image stitching

Method: an unsupervised deep image stitching framework with a dual-branch architecture, a concept of virtual optimal planes, an iterative coefficient predictor and minimal semantic distortion constraint

Result: achieve highly generalizable performance across diverse unseen real-world scenes

Conclusion: RopStitch significantly outperforms existing methods in scene robustness and content naturalness.

Abstract: We present \textit{RopStitch}, an unsupervised deep image stitching framework
with both robustness and naturalness. To ensure the robustness of
\textit{RopStitch}, we propose to incorporate the universal prior of content
perception into the image stitching model by a dual-branch architecture. It
separately captures coarse and fine features and integrates them to achieve
highly generalizable performance across diverse unseen real-world scenes.
Concretely, the dual-branch model consists of a pretrained branch to capture
semantically invariant representations and a learnable branch to extract
fine-grained discriminative features, which are then merged into a whole by a
controllable factor at the correlation level. Besides, considering that content
alignment and structural preservation are often contradictory to each other, we
propose a concept of virtual optimal planes to relieve this conflict. To this
end, we model this problem as a process of estimating homography decomposition
coefficients, and design an iterative coefficient predictor and minimal
semantic distortion constraint to identify the optimal plane. This scheme is
finally incorporated into \textit{RopStitch} by warping both views onto the
optimal plane bidirectionally. Extensive experiments across various datasets
demonstrate that \textit{RopStitch} significantly outperforms existing methods,
particularly in scene robustness and content naturalness. The code is available
at {\color{red}https://github.com/MmelodYy/RopStitch}.

</details>


### [58] [Neural Field Representations of Mobile Computational Photography](https://arxiv.org/abs/2508.05907)
*Ilya Chugunov*

Main category: cs.CV

TL;DR: This paper introduces neural fields to represent complex geometry and lighting effects from mobile photography data. It enables applications such as depth estimation, layer separation, and image stitching. The methods outperform state-of-the-art approaches without complex pre-processing or labeled data.


<details>
  <summary>Details</summary>
Motivation: mobile imaging has experienced a profound transformation, with cell phones rapidly eclipsing all other forms of digital photography in popularity. Today's cell phones are equipped with a diverse range of imaging technologies and  on-board integrated chips for image and signal processing, makes the cell phone a versatile pocket-sized computational imaging platform. Parallel to this, we have seen in recent years how neural fields enable the reconstruction of complex scenes without explicit data representations such as pixel arrays or point clouds.

Method: carefully designed neural field models

Result: depth estimation, layer separation, and image stitching directly from collected in-the-wild mobile photography data. These methods outperform state-of-the-art approaches without relying on complex pre-processing steps, labeled ground truth data, or machine learning priors.

Conclusion:  carefully designed neural field models can compactly represent complex geometry and lighting effects. Enabling applications such as depth estimation, layer separation, and image stitching directly from collected in-the-wild mobile photography data. These methods outperform state-of-the-art approaches without relying on complex pre-processing steps, labeled ground truth data, or machine learning priors. Instead, they leverage well-constructed, self-regularized models that tackle challenging inverse problems through stochastic gradient descent, fitting directly to raw measurements from a smartphone.

Abstract: Over the past two decades, mobile imaging has experienced a profound
transformation, with cell phones rapidly eclipsing all other forms of digital
photography in popularity. Today's cell phones are equipped with a diverse
range of imaging technologies - laser depth ranging, multi-focal camera arrays,
and split-pixel sensors - alongside non-visual sensors such as gyroscopes,
accelerometers, and magnetometers. This, combined with on-board integrated
chips for image and signal processing, makes the cell phone a versatile
pocket-sized computational imaging platform. Parallel to this, we have seen in
recent years how neural fields - small neural networks trained to map
continuous spatial input coordinates to output signals - enable the
reconstruction of complex scenes without explicit data representations such as
pixel arrays or point clouds. In this thesis, I demonstrate how carefully
designed neural field models can compactly represent complex geometry and
lighting effects. Enabling applications such as depth estimation, layer
separation, and image stitching directly from collected in-the-wild mobile
photography data. These methods outperform state-of-the-art approaches without
relying on complex pre-processing steps, labeled ground truth data, or machine
learning priors. Instead, they leverage well-constructed, self-regularized
models that tackle challenging inverse problems through stochastic gradient
descent, fitting directly to raw measurements from a smartphone.

</details>


### [59] [Enhancing Construction Site Analysis and Understanding with 3D Segmentation](https://arxiv.org/abs/2508.05922)
*Sri Ramana Saketh Vasanthawada,Pengkun Liu,Pingbo Tang*

Main category: cs.CV

TL;DR: This paper evaluates SAM and Mask3D for construction progress monitoring, revealing the need for tailored segmentation workflows.


<details>
  <summary>Details</summary>
Motivation: Monitoring construction progress is crucial yet resource-intensive. Traditional data acquisition methods falter in construction site's complex conditions.

Method: application of two advanced 3D segmentation methods, Segment Anything Model (SAM) and Mask3D

Result: both models' adaptability and performance are assessed in real-world construction settings, highlighting the gap in current segmentation approaches due to the absence of benchmarks for outdoor scenarios.

Conclusion: This study showcases the relative effectiveness of SAM and Mask3D and addresses the critical need for tailored segmentation workflows.

Abstract: Monitoring construction progress is crucial yet resource-intensive, prompting
the exploration of computer-vision-based methodologies for enhanced efficiency
and scalability. Traditional data acquisition methods, primarily focusing on
indoor environments, falter in construction site's complex, cluttered, and
dynamically changing conditions. This paper critically evaluates the
application of two advanced 3D segmentation methods, Segment Anything Model
(SAM) and Mask3D, in challenging outdoor and indoor conditions. Trained
initially on indoor datasets, both models' adaptability and performance are
assessed in real-world construction settings, highlighting the gap in current
segmentation approaches due to the absence of benchmarks for outdoor scenarios.
Through a comparative analysis, this study not only showcases the relative
effectiveness of SAM and Mask3D but also addresses the critical need for
tailored segmentation workflows capable of extracting actionable insights from
construction site data, thereby advancing the field towards more automated and
precise monitoring techniques.

</details>


### [60] [A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image](https://arxiv.org/abs/2508.05950)
*Yanxing Liang,Yinghui Wang,Jinlong Yang,Wei Li*

Main category: cs.CV

TL;DR: 提出了一种新的自监督框架，用于从单张图像进行法线估计，通过结合光照交互建模和可微渲染，解决了多视角几何不一致性和数据依赖性问题，并在 Google Scanned Objects 数据集上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 缺乏空间维度信息仍然是单张图像法线估计的一个挑战。最近基于扩散的方法在 2D 到 3D 隐式映射中显示出巨大的潜力，但它们依赖于数据驱动的统计先验，并忽略了光-表面交互的显式建模，从而导致多视角法线方向冲突。此外，扩散模型的离散采样机制导致可微渲染重建模块中的梯度不连续，从而阻止了 3D 几何误差反向传播到法线生成网络，从而迫使现有方法依赖于密集的法线注释。

Method: 该论文提出了一种名为 SINGAD 的新框架，该框架通过 3D 高斯溅射引导扩散，从单张图像进行自监督法线估计。该框架集成了物理驱动的光交互建模和基于可微渲染的重投影策略。

Result: 通过构建光交互驱动的 3DGS 重新参数化模型以生成与光传输原理一致的多尺度几何特征，确保多视角法线一致性。在条件扩散模型中设计了一个跨域特征融合模块，嵌入了几何先验来约束法线生成，同时保持精确的几何误差传播。此外，还引入了一种可微的 3D 重投影损失策略，用于自监督优化，从而最大限度地减少重建图像和输入图像之间的几何误差，从而消除了对带注释的法线数据集的依赖。

Conclusion: 该方法在 Google Scanned Objects 数据集上的定量评估表明，其性能优于现有技术。

Abstract: The lack of spatial dimensional information remains a challenge in normal
estimation from a single image. Recent diffusion-based methods have
demonstrated significant potential in 2D-to-3D implicit mapping, they rely on
data-driven statistical priors and miss the explicit modeling of light-surface
interaction, leading to multi-view normal direction conflicts. Moreover, the
discrete sampling mechanism of diffusion models causes gradient discontinuity
in differentiable rendering reconstruction modules, preventing 3D geometric
errors from being backpropagated to the normal generation network, thereby
forcing existing methods to depend on dense normal annotations. This paper
proposes SINGAD, a novel Self-supervised framework from a single Image for
Normal estimation via 3D GAussian splatting guided Diffusion. By integrating
physics-driven light-interaction modeling and a differentiable rendering-based
reprojection strategy, our framework directly converts 3D geometric errors into
normal optimization signals, solving the challenges of multi-view geometric
inconsistency and data dependency. Specifically, the framework constructs a
light-interaction-driven 3DGS reparameterization model to generate multi-scale
geometric features consistent with light transport principles, ensuring
multi-view normal consistency. A cross-domain feature fusion module is designed
within a conditional diffusion model, embedding geometric priors to constrain
normal generation while maintaining accurate geometric error propagation.
Furthermore, a differentiable 3D reprojection loss strategy is introduced for
self-supervised optimization that minimizes geometric error between the
reconstructed and input image, eliminating dependence on annotated normal
datasets. Quantitative evaluations on the Google Scanned Objects dataset
demonstrate that our method outperforms state-of-the-art approaches across
multiple metrics.

</details>


### [61] [Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents](https://arxiv.org/abs/2508.05954)
*Han Lin,Jaemin Cho,Amir Zadeh,Chuan Li,Mohit Bansal*

Main category: cs.CV

TL;DR: Bifrost-1使用patch-level CLIP图像嵌入连接预训练的MLLM和扩散模型，实现了高效的高保真图像生成。


<details>
  <summary>Details</summary>
Motivation: 将高保真视觉合成能力整合到大型语言模型（LLM）中，而不影响其强大的推理能力，这引起了越来越多的兴趣。

Method: 提出了一种统一的框架Bifrost-1，该框架使用patch-level CLIP图像嵌入作为潜在变量，连接了预训练的多模态LLM（MLLM）和扩散模型。

Result: Bifrost-1实现了高保真可控图像生成，并具有显著的训练效率。

Conclusion: Bifrost-1在视觉保真度和多模态理解方面取得了与先前方法相当或更好的性能，同时显著降低了训练期间的计算量。

Abstract: There is growing interest in integrating high-fidelity visual synthesis
capabilities into large language models (LLMs) without compromising their
strong reasoning capabilities. Existing methods that directly train LLMs or
bridge LLMs and diffusion models usually suffer from costly training since the
backbone LLMs have not seen image representations during pretraining. We
present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs
(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent
variables, which are natively aligned with the MLLM's CLIP visual encoder.
These patch-level image embeddings are integrated into the diffusion model with
a lightweight adaptation of its ControlNet. To retain the original multimodal
reasoning capabilities of MLLMs, we equip the MLLM with a visual generation
branch initialized from the original MLLM parameters when predicting the
patch-level image embeddings. By seamlessly integrating pretrained MLLMs and
diffusion models with patch-level CLIP latents, our framework enables
high-fidelity controllable image generation with significant training
efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or
better performance than previous methods in terms of visual fidelity and
multimodal understanding, with substantially lower compute during training. We
also provide comprehensive ablation studies showing the effectiveness of our
design choices.

</details>


### [62] [PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation](https://arxiv.org/abs/2508.05976)
*Zhihao Zhu,Yifan Zheng,Siyu Pan,Yaohui Jin,Yao Mu*

Main category: cs.CV

TL;DR: PASG 弥合了几何基元和任务语义之间的差距，提高了机器人操作的性能。


<details>
  <summary>Details</summary>
Motivation: 高级任务语义和低级几何特征之间的碎片化仍然是机器人操作中一个持续存在的挑战。虽然视觉语言模型 (VLM) 在生成可供性感知视觉表示方面显示出希望，但规范空间中缺乏语义基础以及依赖于手动注释严重限制了它们捕获动态语义-可供性关系的能力。

Method: 提出了一种基元感知语义接地 (PASG) 的闭环框架，该框架引入：(1) 通过几何特征聚合自动提取基元，实现关键点和轴的跨类别检测；(2) VLM 驱动的语义锚定，动态地将几何基元与功能可供性和任务相关描述相结合；(3) 空间语义推理基准和微调的 VLM (Qwen2.5VL-PA)。

Result: 在各种实际机器人操作任务中证明了 PASG 的有效性，实现了与手动注释相当的性能。

Conclusion: PASG 通过对物体进行更细粒度的语义-可供性理解，为桥接机器人操作中的几何基元与任务语义建立了一个统一的范例。

Abstract: The fragmentation between high-level task semantics and low-level geometric
features remains a persistent challenge in robotic manipulation. While
vision-language models (VLMs) have shown promise in generating affordance-aware
visual representations, the lack of semantic grounding in canonical spaces and
reliance on manual annotations severely limit their ability to capture dynamic
semantic-affordance relationships. To address these, we propose Primitive-Aware
Semantic Grounding (PASG), a closed-loop framework that introduces: (1)
Automatic primitive extraction through geometric feature aggregation, enabling
cross-category detection of keypoints and axes; (2) VLM-driven semantic
anchoring that dynamically couples geometric primitives with functional
affordances and task-relevant description; (3) A spatial-semantic reasoning
benchmark and a fine-tuned VLM (Qwen2.5VL-PA). We demonstrate PASG's
effectiveness in practical robotic manipulation tasks across diverse scenarios,
achieving performance comparable to manual annotations. PASG achieves a
finer-grained semantic-affordance understanding of objects, establishing a
unified paradigm for bridging geometric primitives with task semantics in
robotic manipulation.

</details>


### [63] [AnimateScene: Camera-controllable Animation in Any Scene](https://arxiv.org/abs/2508.05982)
*Qingyang Liu,Bingjie Gao,Weiheng Huang,Jun Zhang,Zhongqian Sun,Yang Wei,Zelin Peng,Qianli Ma,Shuai Yang,Zhaohe Liao,Haonan Zhao,Li Niu*

Main category: cs.CV

TL;DR: AnimateScene解决了将4D人体动画无缝集成到3D场景中的问题，通过精确放置、风格对齐和联合后重建来实现逼真的动态场景视频。


<details>
  <summary>Details</summary>
Motivation: 无缝地将重建的场景与4D人体动画集成以产生视觉上吸引人的结果仍然具有挑战性。一个关键的困难在于将人放置在场景中正确的位置和比例，同时避免不真实的相互渗透。另一个挑战是，人和背景可能表现出不同的光照和风格，导致不真实的合成。

Method: 设计了一个精确的放置模块，自动确定人体合理的3D位置，并在运动过程中防止场景内的任何相互渗透。提出了一个免训练的风格对齐方法，使4D人体表示适应背景的照明和风格，实现连贯的视觉集成。为4D人体和3D场景设计了一种联合的后重建方法，允许插入相机轨迹，使最终渲染的视频具有视觉吸引力的相机运动。

Result: AnimateScene解决了上述问题。

Conclusion: AnimateScene生成具有高几何细节和跨各种相机和动作组合的时空连贯性的动态场景视频。

Abstract: 3D scene reconstruction and 4D human animation have seen rapid progress and
broad adoption in recent years. However, seamlessly integrating reconstructed
scenes with 4D human animation to produce visually engaging results remains
challenging. One key difficulty lies in placing the human at the correct
location and scale within the scene while avoiding unrealistic
interpenetration. Another challenge is that the human and the background may
exhibit different lighting and style, leading to unrealistic composites. In
addition, appealing character motion videos are often accompanied by camera
movements, which means that the viewpoints need to be reconstructed along a
specified trajectory. We present AnimateScene, which addresses the above issues
in a unified framework. First, we design an accurate placement module that
automatically determines a plausible 3D position for the human and prevents any
interpenetration within the scene during motion. Second, we propose a
training-free style alignment method that adapts the 4D human representation to
match the background's lighting and style, achieving coherent visual
integration. Finally, we design a joint post-reconstruction method for both the
4D human and the 3D scene that allows camera trajectories to be inserted,
enabling the final rendered video to feature visually appealing camera
movements. Extensive experiments show that AnimateScene generates dynamic scene
videos with high geometric detail and spatiotemporal coherence across various
camera and action combinations.

</details>


### [64] [ETA: Energy-based Test-time Adaptation for Depth Completion](https://arxiv.org/abs/2508.05989)
*Younjoon Chung,Hyoungseob Park,Patrick Rim,Xiaoran Zhang,Jihe He,Ziyao Zeng,Safa Cicek,Byung-Woo Hong,James S. Duncan,Alex Wong*

Main category: cs.CV

TL;DR: ETA adapts pretrained depth completion models to new data by using an energy model trained with adversarial perturbations, improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Depth completion models often perform poorly when transferred to target data due to covariate shift, and access to target data is limited prior to deployment.

Method: The method quantifies the likelihood of depth predictions belonging to the source data distribution using adversarial perturbations to train an energy model.

Result: ETA improves depth completion accuracy by 6.94% outdoors and 10.23% indoors compared to previous state-of-the-art methods.

Conclusion: Energy-based Test-time Adaptation (ETA) improves depth completion accuracy on target data by minimizing energy and aligning predictions to the source distribution, outperforming previous methods by 6.94% outdoors and 10.23% indoors.

Abstract: We propose a method for test-time adaptation of pretrained depth completion
models. Depth completion models, trained on some ``source'' data, often predict
erroneous outputs when transferred to ``target'' data captured in novel
environmental conditions due to a covariate shift. The crux of our method lies
in quantifying the likelihood of depth predictions belonging to the source data
distribution. The challenge is in the lack of access to out-of-distribution
(target) data prior to deployment. Hence, rather than making assumptions
regarding the target distribution, we utilize adversarial perturbations as a
mechanism to explore the data space. This enables us to train an energy model
that scores local regions of depth predictions as in- or out-of-distribution.
We update the parameters of pretrained depth completion models at test time to
minimize energy, effectively aligning test-time predictions to those of the
source distribution. We call our method ``Energy-based Test-time Adaptation'',
or ETA for short. We evaluate our method across three indoor and three outdoor
datasets, where ETA improve over the previous state-of-the-art method by an
average of 6.94% for outdoors and 10.23% for indoors. Project Page:
https://fuzzythecat.github.io/eta.

</details>


### [65] [Fast Motion Estimation and Context-Aware Refinement for Efficient Bayer-Domain Video Vision](https://arxiv.org/abs/2508.05990)
*Haichao Wang,Xinyue Xi,Jiangtao Wen,Yuxing Han*

Main category: cs.CV

TL;DR: 提出了一种高效的视频计算机视觉系统，通过去除图像信号处理器、使用快速块匹配运动估计和上下文感知的块细化网络来减少时间冗余和前端计算开销。


<details>
  <summary>Details</summary>
Motivation: 由于视频内部的时间冗余度高，视频计算机视觉系统的效率仍然是一项具有挑战性的任务。现有的工作已经被提出用于高效的视觉计算机视觉，但是，它们没有完全减少时间冗余度，忽略了前端计算开销。

Method: 提出了一种基于快速块匹配的运动估计算法，并引入了上下文感知的块细化网络来细化误差较大的区域。采用帧选择策略来进一步平衡准确性和效率。

Result: 该方法在多个视频计算机视觉任务上实现了显着的加速，性能损失很小。

Conclusion: 该方法在多个视频计算机视觉任务上实现了显着的加速，性能损失很小。

Abstract: The efficiency of video computer vision system remains a challenging task due
to the high temporal redundancy inside a video. Existing works have been
proposed for efficient vision computer vision. However, they do not fully
reduce the temporal redundancy and neglect the front end computation overhead.
In this paper, we propose an efficient video computer vision system. First,
image signal processor is removed and Bayer-format data is directly fed into
video computer vision models, thus saving the front end computation. Second,
instead of optical flow models and video codecs, a fast block matching-based
motion estimation algorithm is proposed specifically for efficient video
computer vision, with a MV refinement module. To correct the error,
context-aware block refinement network is introduced to refine regions with
large error. To further balance the accuracy and efficiency, a frame selection
strategy is employed. Experiments on multiple video computer vision tasks
demonstrate that our method achieves significant acceleration with slight
performance loss.

</details>


### [66] [ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge](https://arxiv.org/abs/2508.05991)
*Juewen Hu,Yexin Li,Jiulin Li,Shuo Chen,Pring Wong*

Main category: cs.CV

TL;DR: 提出了一种新颖的多模态情感识别框架，在MER2025-SEMI挑战赛中表现出色，通过利用预训练模型、设计特定模态的编码器和融合策略，以及优化训练数据，显著提高了情感识别的性能。


<details>
  <summary>Details</summary>
Motivation: 情感识别在增强人机交互方面起着至关重要的作用。为了解决数据稀缺的问题，我们利用大规模预训练模型从视觉、音频和文本模态中提取信息丰富的特征。

Method: 设计了一个双分支视觉编码器，捕捉全局帧级别特征和局部面部表示。引入了一种上下文丰富的文本处理方法，利用大型语言模型来丰富输入文本中的情感线索。提出了一种融合策略，包含用于动态模态加权的自注意力机制和用于保留原始表示的残差连接。采用多源标签策略来细化训练集中的噪声标签。

Result: 该方法在MER2025-SEMI数据集上取得了显著的性能提升，加权F-score为87.49%，而官方基线为78.63%。

Conclusion: 该方法在MER2025-SEMI数据集上取得了显著的性能提升，加权F-score为87.49%，而官方基线为78.63%，验证了所提出的框架的有效性。

Abstract: Emotion recognition plays a vital role in enhancing human-computer
interaction. In this study, we tackle the MER-SEMI challenge of the MER2025
competition by proposing a novel multimodal emotion recognition framework. To
address the issue of data scarcity, we leverage large-scale pre-trained models
to extract informative features from visual, audio, and textual modalities.
Specifically, for the visual modality, we design a dual-branch visual encoder
that captures both global frame-level features and localized facial
representations. For the textual modality, we introduce a context-enriched
method that employs large language models to enrich emotional cues within the
input text. To effectively integrate these multimodal features, we propose a
fusion strategy comprising two key components, i.e., self-attention mechanisms
for dynamic modality weighting, and residual connections to preserve original
representations. Beyond architectural design, we further refine noisy labels in
the training set by a multi-source labeling strategy. Our approach achieves a
substantial performance improvement over the official baseline on the
MER2025-SEMI dataset, attaining a weighted F-score of 87.49% compared to
78.63%, thereby validating the effectiveness of the proposed framework.

</details>


### [67] [EvoMakeup: High-Fidelity and Controllable Makeup Editing with MakeupQuad](https://arxiv.org/abs/2508.05994)
*Huadong Wu,Yi Fu,Yunhao Li,Yuan Gao,Kang Du*

Main category: cs.CV

TL;DR: EvoMakeup是一个用于面部化妆编辑的统一框架，它使用MakeupQuad数据集进行训练，并在化妆保真度和身份保持方面实现了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的面部化妆编辑方法通常会产生低质量的结果，化妆细节粗糙，并且难以同时保持身份和化妆的逼真度，这主要是由于缺乏结构化的配对数据。

Method: EvoMakeup，一个统一的训练框架，可以减轻多阶段蒸馏过程中的图像退化，从而能够迭代改进数据和模型质量。

Result: EvoMakeup在真实世界的基准测试中表现良好，并且优于先前的方法。它支持高保真、可控、多任务的化妆编辑。

Conclusion: EvoMakeup在化妆保真度和身份保持方面表现出色，有效地平衡了这两个方面。

Abstract: Facial makeup editing aims to realistically transfer makeup from a reference
to a target face. Existing methods often produce low-quality results with
coarse makeup details and struggle to preserve both identity and makeup
fidelity, mainly due to the lack of structured paired data -- where source and
result share identity, and reference and result share identical makeup. To
address this, we introduce MakeupQuad, a large-scale, high-quality dataset with
non-makeup faces, references, edited results, and textual makeup descriptions.
Building on this, we propose EvoMakeup, a unified training framework that
mitigates image degradation during multi-stage distillation, enabling iterative
improvement of both data and model quality. Although trained solely on
synthetic data, EvoMakeup generalizes well and outperforms prior methods on
real-world benchmarks. It supports high-fidelity, controllable, multi-task
makeup editing -- including full-face and partial reference-based editing, as
well as text-driven makeup editing -- within a single model. Experimental
results demonstrate that our method achieves superior makeup fidelity and
identity preservation, effectively balancing both aspects. Code and dataset
will be released upon acceptance.

</details>


### [68] [MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2508.06009)
*Jun Feng,Zixin Wang,Zhentao Zhang,Yue Guo,Zhihan Zhou,Xiuyi Chen,Zhenyang Li,Dawei Yin*

Main category: cs.CV

TL;DR: introduce MathReal dataset to evaluate MLLMs' mathematical reasoning abilities in real-world K-12 scenarios, finding challenges for existing models.


<details>
  <summary>Details</summary>
Motivation: existing benchmarks are predominantly based on clean or processed multimodal inputs, without incorporating the images provided by real-world Kindergarten through 12th grade (K-12) educational users

Method: introduce MathReal, a meticulously curated dataset comprising 2,000 mathematical questions with images captured by handheld mobile devices in authentic scenarios and design six experimental settings

Result: systematic analysis of MLLMs' performance and error patterns, providing insights into their recognition, comprehension, and reasoning capabilities

Conclusion: existing MLLMs are significantly challenged in realistic educational contexts

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in visual mathematical reasoning across various existing
benchmarks. However, these benchmarks are predominantly based on clean or
processed multimodal inputs, without incorporating the images provided by
real-world Kindergarten through 12th grade (K-12) educational users. To address
this gap, we introduce MathReal, a meticulously curated dataset comprising
2,000 mathematical questions with images captured by handheld mobile devices in
authentic scenarios. Each question is an image, containing the question text
and visual element. We systematically classify the real images into three
primary categories: image quality degradation, perspective variation, and
irrelevant content interference, which are further delineated into 14
subcategories. Additionally, MathReal spans five core knowledge and ability
categories, which encompass three question types and are divided into three
difficulty levels. To comprehensively evaluate the multimodal mathematical
reasoning abilities of state-of-the-art MLLMs in real-world scenarios, we
design six experimental settings that enable a systematic analysis of their
performance. Through extensive experimentation, we find that the
problem-solving abilities of existing MLLMs are significantly challenged in
realistic educational contexts. Based on this, we conduct a thorough analysis
of their performance and error patterns, providing insights into their
recognition, comprehension, and reasoning capabilities, and outlining
directions for future improvements. Data and code:
https://github.com/junfeng0288/MathReal.

</details>


### [69] [ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors](https://arxiv.org/abs/2508.06014)
*Minsu Kim,Subin Jeon,In Cho,Mijin Yoo,Seon Joo Kim*

Main category: cs.CV

TL;DR: enhance 3DGS reconstruction quality by generating additional training views and refining rendered results with video diffusion priors


<details>
  <summary>Details</summary>
Motivation: existing methods struggle with artifacts and missing regions when rendering from viewpoints that deviate from the training trajectory, limiting seamless scene exploration

Method: a 3DGS-based pipeline that generates additional training views to enhance reconstruction. introducing an information-gain-driven virtual camera placement strategy to maximize scene coverage, followed by video diffusion priors to refine rendered results. Fine-tuning 3D Gaussians with these enhanced views

Result: outperforms existing 3DGS-based methods

Conclusion: significantly improves reconstruction quality, enabling high-quality, artifact-free rendering from arbitrary viewpoints

Abstract: Recent advances in novel view synthesis (NVS) have enabled real-time
rendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle
with artifacts and missing regions when rendering from viewpoints that deviate
from the training trajectory, limiting seamless scene exploration. To address
this, we propose a 3DGS-based pipeline that generates additional training views
to enhance reconstruction. We introduce an information-gain-driven virtual
camera placement strategy to maximize scene coverage, followed by video
diffusion priors to refine rendered results. Fine-tuning 3D Gaussians with
these enhanced views significantly improves reconstruction quality. To evaluate
our method, we present Wild-Explore, a benchmark designed for challenging scene
exploration. Experiments demonstrate that our approach outperforms existing
3DGS-based methods, enabling high-quality, artifact-free rendering from
arbitrary viewpoints.
  https://exploregs.github.io

</details>


### [70] [Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis](https://arxiv.org/abs/2508.06021)
*Utku Ozbulak,Michaela Cohrs,Hristo L. Svilenov,Joris Vankerschaver,Wesley De Neve*

Main category: cs.CV

TL;DR: 本研究利用扩散模型生成高保真图像来扩充训练数据集，解决了亚可见颗粒分析中数据不平衡的问题，经验证可以有效提升分类性能，并公开了相关模型和代码。


<details>
  <summary>Details</summary>
Motivation: 在使用流式图像显微镜结合深度学习进行亚可见颗粒分析时，数据稀缺和颗粒类型之间的严重不平衡是 существенным 障碍，特别是对于那些出现量较少的颗粒类型，例如硅油和气泡。

Method: 开发了一个state-of-the-art扩散模型来解决数据不平衡问题，通过生成高保真图像来扩充训练数据集。

Result: 验证了生成样本在视觉质量和结构上与真实颗粒图像非常相似。在包含500,000个蛋白质颗粒图像的验证数据集上进行了大规模实验，证明了该方法提高了分类性能。

Conclusion: 使用扩散模型生成图像可以有效提升多分deep neural networks的分类性能，且没有明显的缺点。该团队发布了扩散模型和训练好的多分deep neural network classifiers，以及一个简单的接口。

Abstract: Sub-visible particle analysis using flow imaging microscopy combined with
deep learning has proven effective in identifying particle types, enabling the
distinction of harmless components such as silicone oil from protein particles.
However, the scarcity of available data and severe imbalance between particle
types within datasets remain substantial hurdles when applying multi-class
classifiers to such problems, often forcing researchers to rely on less
effective methods. The aforementioned issue is particularly challenging for
particle types that appear unintentionally and in lower numbers, such as
silicone oil and air bubbles, as opposed to protein particles, where obtaining
large numbers of images through controlled settings is comparatively
straightforward. In this work, we develop a state-of-the-art diffusion model to
address data imbalance by generating high-fidelity images that can augment
training datasets, enabling the effective training of multi-class deep neural
networks. We validate this approach by demonstrating that the generated samples
closely resemble real particle images in terms of visual quality and structure.
To assess the effectiveness of using diffusion-generated images in training
datasets, we conduct large-scale experiments on a validation dataset comprising
500,000 protein particle images and demonstrate that this approach improves
classification performance with no negligible downside. Finally, to promote
open research and reproducibility, we publicly release both our diffusion
models and the trained multi-class deep neural network classifiers, along with
a straightforward interface for easy integration into future studies, at
https://github.com/utkuozbulak/svp-generative-ai.

</details>


### [71] [Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts](https://arxiv.org/abs/2508.06032)
*Kiran Chhatre,Christopher Peters,Srikrishna Karanam*

Main category: cs.CV

TL;DR: Spectrum：一种用于人体部位解析的统一网络，它利用图像到纹理的扩散模型，在基于提示的分割任务中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的将人体解析为身体部位和服装的方法通常使用固定的mask类别，这些类别具有广泛的标签，模糊了细粒度的服装类型。最近的开放词汇分割方法利用预训练的文本到图像(T2I)扩散模型特征来实现强大的zero-shot迁移，但通常将整个人类分组到单个Person类别中，无法区分不同的服装或详细的身体部位。

Method: 提出了一种统一的网络Spectrum，用于零件级像素解析（身体部位和服装）和实例级分组。通过对3D人体纹理贴图进行微调，将图像到纹理(I2Tx)扩散模型重新用于改进与身体部位和服装的对齐。

Result: Spectrum可以为每个可见的身体部位和服装类别生成语义分割图，忽略独立的服装或不相关的对象，适用于场景中任意数量的人。

Conclusion: Spectrum在基于提示的分割任务中始终优于基线方法。

Abstract: Existing methods for human parsing into body parts and clothing often use
fixed mask categories with broad labels that obscure fine-grained clothing
types. Recent open-vocabulary segmentation approaches leverage pretrained
text-to-image (T2I) diffusion model features for strong zero-shot transfer, but
typically group entire humans into a single person category, failing to
distinguish diverse clothing or detailed body parts. To address this, we
propose Spectrum, a unified network for part-level pixel parsing (body parts
and clothing) and instance-level grouping. While diffusion-based
open-vocabulary models generalize well across tasks, their internal
representations are not specialized for detailed human parsing. We observe
that, unlike diffusion models with broad representations, image-driven 3D
texture generators maintain faithful correspondence to input images, enabling
stronger representations for parsing diverse clothing and body parts. Spectrum
introduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model --
obtained by fine-tuning a T2I model on 3D human texture maps -- for improved
alignment with body parts and clothing. From an input image, we extract
human-part internal features via the I2Tx diffusion model and generate
semantically valid masks aligned to diverse clothing categories through
prompt-guided grounding. Once trained, Spectrum produces semantic segmentation
maps for every visible body part and clothing category, ignoring standalone
garments or irrelevant objects, for any number of humans in the scene. We
conduct extensive cross-dataset experiments -- separately assessing body parts,
clothing parts, unseen clothing categories, and full-body masks -- and
demonstrate that Spectrum consistently outperforms baseline methods in
prompt-based segmentation.

</details>


### [72] [InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow](https://arxiv.org/abs/2508.06033)
*Yiming Gong,Zhen Zhu,Minjia Zhang*

Main category: cs.CV

TL;DR: InstantEdit是一种快速的文本引导图像编辑方法，它在速度和质量上都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提出了一种快速的文本引导图像编辑方法InstantEdit，它被构建为一个few-step编辑过程，可以在遵循文本指令的同时保留关键内容。

Method: 基于RectifiedFlow框架，引入了PerRFI反演策略，提出了反演潜在注入的再生方法，并整合了Canny-conditioned ControlNet。

Result: InstantEdit速度快，并且实现了比state-of-the-art的few-step编辑方法更好的定性和定量结果。

Conclusion: InstantEdit在PIE图像编辑数据集上，相较于目前最优的few-step编辑方法，不仅速度更快，而且取得了更好的定性和定量结果。

Abstract: We propose a fast text-guided image editing method called InstantEdit based
on the RectifiedFlow framework, which is structured as a few-step editing
process that preserves critical content while following closely to textual
instructions. Our approach leverages the straight sampling trajectories of
RectifiedFlow by introducing a specialized inversion strategy called PerRFI. To
maintain consistent while editable results for RectifiedFlow model, we further
propose a novel regeneration method, Inversion Latent Injection, which
effectively reuses latent information obtained during inversion to facilitate
more coherent and detailed regeneration. Additionally, we propose a
Disentangled Prompt Guidance technique to balance editability with detail
preservation, and integrate a Canny-conditioned ControlNet to incorporate
structural cues and suppress artifacts. Evaluation on the PIE image editing
dataset demonstrates that InstantEdit is not only fast but also achieves better
qualitative and quantitative results compared to state-of-the-art few-step
editing methods.

</details>


### [73] [More Is Better: A MoE-Based Emotion Recognition Framework with Human Preference Alignment](https://arxiv.org/abs/2508.06036)
*Jun Xie,Yingjian Zhu,Feng Chen,Zhenghao Zhang,Xiaohui Fan,Hongzhu Yi,Xinming Wang,Chen Yu,Yue Bi,Zhaoran Zhao,Xiongjun Guan,Zhepeng Wang*

Main category: cs.CV

TL;DR: Proposes a Mixture of Experts (MoE) emotion recognition system using multi-modal inputs, consensus-based pseudo-labeling, and ensemble methods, achieving 2nd place in the MER2025-SEMI challenge with an F1-score of 0.8772.


<details>
  <summary>Details</summary>
Motivation: To construct a robust Mixture of Experts (MoE) emotion recognition system.

Method: A comprehensive framework, grounded in the principle that "more is better," to construct a robust Mixture of Experts (MoE) emotion recognition system. Integrates a diverse range of input modalities as independent experts, including novel signals such as knowledge from large Vision-Language Models (VLMs) and temporal Action Unit (AU) information. Introduce a consensus-based pseudo-labeling strategy, generating high-quality labels from the agreement between a baseline model and Gemini, which are then used in a two-stage training paradigm. Employ a multi-expert voting ensemble combined with a rule-based re-ranking process.

Result: Achieves an F1-score of 0.8772 on the test set, ranking 2nd in the track.

Conclusion: Achieved an F1-score of 0.8772 on the MER2025-SEMI challenge dataset, ranking 2nd in the track.

Abstract: In this paper, we present our solution for the semi-supervised learning track
(MER-SEMI) in MER2025. We propose a comprehensive framework, grounded in the
principle that "more is better," to construct a robust Mixture of Experts (MoE)
emotion recognition system. Our approach integrates a diverse range of input
modalities as independent experts, including novel signals such as knowledge
from large Vision-Language Models (VLMs) and temporal Action Unit (AU)
information. To effectively utilize unlabeled data, we introduce a
consensus-based pseudo-labeling strategy, generating high-quality labels from
the agreement between a baseline model and Gemini, which are then used in a
two-stage training paradigm. Finally, we employ a multi-expert voting ensemble
combined with a rule-based re-ranking process to correct prediction bias and
better align the outputs with human preferences. Evaluated on the MER2025-SEMI
challenge dataset, our method achieves an F1-score of 0.8772 on the test set,
ranking 2nd in the track. Our code is available at
https://github.com/zhuyjan/MER2025-MRAC25.

</details>


### [74] [Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models](https://arxiv.org/abs/2508.06038)
*Huanyu Wang,Jushi Kai,Haoli Bai,Lu Hou,Bo Jiang,Ziwei He,Zhouhan Lin*

Main category: cs.CV

TL;DR: Fourier-VLM compresses visual representations in the frequency domain using DCT, achieving significant efficiency gains with competitive performance.


<details>
  <summary>Details</summary>
Motivation: the observation that vision features output from the vision encoder exhibit concentrated energy in low-frequency components

Method: applies a low-pass filter to the vision features using a two-dimentional Discrete Cosine Transform (DCT)

Result: reduce inference FLOPs by up to 83.8% and boots generation speed by 31.2% compared to LLaVA-v1.5

Conclusion: Fourier-VLM achieves competitive performance with strong generalizability across both LLaVA and Qwen-VL architectures. Crucially, it reduce inference FLOPs by up to 83.8% and boots generation speed by 31.2% compared to LLaVA-v1.5, highlighting the superior efficiency and practicality.

Abstract: Vision-Language Models (VLMs) typically replace the predefined image
placeholder token (<image>) in textual instructions with visual features from
an image encoder, forming the input to a backbone Large Language Model (LLM).
However, the large number of vision tokens significantly increases the context
length, leading to high computational overhead and inference latency. While
previous efforts mitigate this by selecting only important visual features or
leveraging learnable queries to reduce token count, they often compromise
performance or introduce substantial extra costs. In response, we propose
Fourier-VLM, a simple yet efficient method that compresses visual
representations in the frequency domain. Our approach is motivated by the
observation that vision features output from the vision encoder exhibit
concentrated energy in low-frequency components. Leveraging this, we apply a
low-pass filter to the vision features using a two-dimentional Discrete Cosine
Transform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier
Transform (FFT) operator with a time complexity of $\mathcal{O}(n\log n)$,
minimizing the extra computational cost while introducing no additional
parameters. Extensive experiments across various image-based benchmarks
demonstrate that Fourier-VLM achieves competitive performance with strong
generalizability across both LLaVA and Qwen-VL architectures. Crucially, it
reduce inference FLOPs by up to 83.8% and boots generation speed by 31.2%
compared to LLaVA-v1.5, highlighting the superior efficiency and practicality.

</details>


### [75] [NEP: Autoregressive Image Editing via Next Editing Token Prediction](https://arxiv.org/abs/2508.06044)
*Huimin Wu,Xiaojian Ma,Haozhe Zhao,Yanpeng Zhao,Qing Li*

Main category: cs.CV

TL;DR: propose to formulate image editing as Next Editing-token Prediction (NEP) based on autoregressive image generation, where only regions that need to be edited are regenerated, thus avoiding unintended modification to the non-editing areas


<details>
  <summary>Details</summary>
Motivation: existing approaches generate the entire target image rather than selectively regenerate only the intended editing areas. This results in (1) unnecessary computational costs and (2) a bias toward reconstructing non-editing regions, which compromises the quality of the intended edits

Method: formulate image editing as Next Editing-token Prediction (NEP) based on autoregressive image generation

Result: only regions that need to be edited are regenerated, thus avoiding unintended modification to the non-editing areas. To enable any-region editing, we propose to pre-train an any-order autoregressive text-to-image (T2I) model. Once trained, it is capable of zero-shot image editing and can be easily adapted to NEP for image editing, which achieves a new state-of-the-art on widely used image editing benchmarks. Moreover, our model naturally supports test-time scaling (TTS) through iteratively refining its generation in a zero-shot manner.

Conclusion: achieves a new state-of-the-art on widely used image editing benchmarks

Abstract: Text-guided image editing involves modifying a source image based on a
language instruction and, typically, requires changes to only small local
regions. However, existing approaches generate the entire target image rather
than selectively regenerate only the intended editing areas. This results in
(1) unnecessary computational costs and (2) a bias toward reconstructing
non-editing regions, which compromises the quality of the intended edits. To
resolve these limitations, we propose to formulate image editing as Next
Editing-token Prediction (NEP) based on autoregressive image generation, where
only regions that need to be edited are regenerated, thus avoiding unintended
modification to the non-editing areas. To enable any-region editing, we propose
to pre-train an any-order autoregressive text-to-image (T2I) model. Once
trained, it is capable of zero-shot image editing and can be easily adapted to
NEP for image editing, which achieves a new state-of-the-art on widely used
image editing benchmarks. Moreover, our model naturally supports test-time
scaling (TTS) through iteratively refining its generation in a zero-shot
manner. The project page is: https://nep-bigai.github.io/

</details>


### [76] [VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning](https://arxiv.org/abs/2508.06051)
*Linhan Cao,Wei Sun,Weixia Zhang,Xiangyang Zhu,Jun Jia,Kaiwei Zhang,Dandan Zhu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: VQAThinker, a reasoning-based VQA framework using LMMs and reinforcement learning, addresses the limitations of existing VQA models by improving generalization and explainability. It achieves state-of-the-art performance on in-domain and OOD VQA benchmarks.


<details>
  <summary>Details</summary>
Motivation: existing VQA models still suffer from two critical limitations: poor generalization to out-of-distribution (OOD) videos and limited explainability, which restrict their applicability in real-world scenarios.

Method: a reasoning-based VQA framework that leverages large multimodal models (LMMs) with reinforcement learning to jointly model video quality understanding and scoring, emulating human perceptual decision-making. Specifically, we adopt group relative policy optimization (GRPO), a rule-guided reinforcement learning algorithm that enables reasoning over video quality under score-level supervision, and introduce three VQA-specific rewards: (1) a bell-shaped regression reward (2) a pairwise ranking reward (3) a temporal consistency reward

Result: VQAThinker achieves state-of-the-art performance on both in-domain and OOD VQA benchmarks, showing strong generalization for video quality scoring. Furthermore, evaluations on video quality understanding tasks validate its superiority in distortion attribution and quality description compared to existing explainable VQA models and LMMs.

Conclusion: VQAThinker achieves state-of-the-art performance on both in-domain and OOD VQA benchmarks, showing strong generalization for video quality scoring. Furthermore, evaluations on video quality understanding tasks validate its superiority in distortion attribution and quality description compared to existing explainable VQA models and LMMs. These findings demonstrate that reinforcement learning offers an effective pathway toward building generalizable and explainable VQA models solely with score-level supervision.

Abstract: Video quality assessment (VQA) aims to objectively quantify perceptual
quality degradation in alignment with human visual perception. Despite recent
advances, existing VQA models still suffer from two critical limitations:
\textit{poor generalization to out-of-distribution (OOD) videos} and
\textit{limited explainability}, which restrict their applicability in
real-world scenarios. To address these challenges, we propose
\textbf{VQAThinker}, a reasoning-based VQA framework that leverages large
multimodal models (LMMs) with reinforcement learning to jointly model video
quality understanding and scoring, emulating human perceptual decision-making.
Specifically, we adopt group relative policy optimization (GRPO), a rule-guided
reinforcement learning algorithm that enables reasoning over video quality
under score-level supervision, and introduce three VQA-specific rewards: (1) a
\textbf{bell-shaped regression reward} that increases rapidly as the prediction
error decreases and becomes progressively less sensitive near the ground truth;
(2) a \textbf{pairwise ranking reward} that guides the model to correctly
determine the relative quality between video pairs; and (3) a \textbf{temporal
consistency reward} that encourages the model to prefer temporally coherent
videos over their perturbed counterparts. Extensive experiments demonstrate
that VQAThinker achieves state-of-the-art performance on both in-domain and OOD
VQA benchmarks, showing strong generalization for video quality scoring.
Furthermore, evaluations on video quality understanding tasks validate its
superiority in distortion attribution and quality description compared to
existing explainable VQA models and LMMs. These findings demonstrate that
reinforcement learning offers an effective pathway toward building
generalizable and explainable VQA models solely with score-level supervision.

</details>


### [77] [LV-Net: Anatomy-aware lateral ventricle shape modeling with a case study on Alzheimer's disease, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing](https://arxiv.org/abs/2508.06055)
*Wonjung Park,Suhyun Ahn,Jinah Park*

Main category: cs.CV

TL;DR: LV-Net是一种用于从脑部MRI生成个体化3D脑室网格的新框架，它提高了重建精度和形状描述符的可靠性，并可以识别与阿尔茨海默病相关的脑室亚区。


<details>
  <summary>Details</summary>
Motivation: 脑室形状分析有望成为神经疾病的生物标志物；然而，由于个体间的形状差异很大，以及MRI分辨率有限导致分割困难，仍然存在挑战。

Method: LV-Net通过变形一个解剖学感知的联合脑室-海马模板网格，从脑部MRI生成个体化的3D脑室网格。

Result: LV-Net实现了卓越的重建精度，即使在存在分割缺陷的情况下，并在不同的数据集中提供了更可靠的形状描述符。

Conclusion: LV-Net可以识别与阿尔茨海默病显著相关的脑室亚区。

Abstract: Lateral ventricle (LV) shape analysis holds promise as a biomarker for
neurological diseases; however, challenges remain due to substantial shape
variability across individuals and segmentation difficulties arising from
limited MRI resolution. We introduce LV-Net, a novel framework for producing
individualized 3D LV meshes from brain MRI by deforming an anatomy-aware joint
LV-hippocampus template mesh. By incorporating anatomical relationships
embedded within the joint template, LV-Net reduces boundary segmentation
artifacts and improves reconstruction robustness. In addition, by classifying
the vertices of the template mesh based on their anatomical adjacency, our
method enhances point correspondence across subjects, leading to more accurate
LV shape statistics. We demonstrate that LV-Net achieves superior
reconstruction accuracy, even in the presence of segmentation imperfections,
and delivers more reliable shape descriptors across diverse datasets. Finally,
we apply LV-Net to Alzheimer's disease analysis, identifying LV subregions that
show significantly associations with the disease relative to cognitively normal
controls. The codes for LV shape modeling are available at
https://github.com/PWonjung/LV_Shape_Modeling.

</details>


### [78] [AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?](https://arxiv.org/abs/2508.06057)
*Mojtaba Valipour,Kelly Zheng,James Lowman,Spencer Szabados,Mike Gartner,Bobby Braswell*

Main category: cs.CV

TL;DR: This paper proposes a comprehensive benchmark to evaluate earth observation models.


<details>
  <summary>Details</summary>
Motivation: Earth Observation data is useful for an intelligent model

Method: review existing benchmarks and highlight their limitations

Result: propose a comprehensive set of tasks that a benchmark should encompass to effectively assess a model's ability to understand and interact with Earth observation data.

Conclusion: This paper emphasizes the need for a more comprehensive benchmark to evaluate earth observation models.

Abstract: Artificial General Intelligence (AGI) is closer than ever to becoming a
reality, sparking widespread enthusiasm in the research community to collect
and work with various modalities, including text, image, video, and audio.
Despite recent efforts, satellite spectral imagery, as an additional modality,
has yet to receive the attention it deserves. This area presents unique
challenges, but also holds great promise in advancing the capabilities of AGI
in understanding the natural world. In this paper, we argue why Earth
Observation data is useful for an intelligent model, and then we review
existing benchmarks and highlight their limitations in evaluating the
generalization ability of foundation models in this domain. This paper
emphasizes the need for a more comprehensive benchmark to evaluate earth
observation models. To facilitate this, we propose a comprehensive set of tasks
that a benchmark should encompass to effectively assess a model's ability to
understand and interact with Earth observation data.

</details>


### [79] [Lightweight Quad Bayer HybridEVS Demosaicing via State Space Augmented Cross-Attention](https://arxiv.org/abs/2508.06058)
*Shiyang Zhou,Haijin Zeng,Yunfan Lu,Yongyong Chen,Jie Liu,Jingyong Su*

Main category: cs.CV

TL;DR: TSANet is a lightweight network for demosaicing HybridEVS data that outperforms existing methods with lower computational cost, making it suitable for mobile devices.


<details>
  <summary>Details</summary>
Motivation: Combining a Quad Bayer Color Filter Array (CFA) sensor with event pixels lacking color information in HybridEVS cameras leads to aliasing and artifacts during demosaicing, which current methods struggle to address, especially on mobile devices.

Method: A lightweight two-stage network (TSANet) with a Cross-Swin State Block that utilizes positional prior and enhances global dependencies through a state space model.

Result: TSANet outperforms DemosaicFormer on seven datasets in PSNR and SSIM, while reducing parameter and computation costs by 1.86x and 3.29x, respectively.

Conclusion: TSANet achieves better demosaicing performance than DemosaicFormer on HybridEVS data with lower parameter and computation costs, opening new possibilities for efficient demosaicing on mobile devices.

Abstract: Event cameras like the Hybrid Event-based Vision Sensor (HybridEVS) camera
capture brightness changes as asynchronous "events" instead of frames, offering
advanced application on mobile photography. However, challenges arise from
combining a Quad Bayer Color Filter Array (CFA) sensor with event pixels
lacking color information, resulting in aliasing and artifacts on the
demosaicing process before downstream application. Current methods struggle to
address these issues, especially on resource-limited mobile devices. In
response, we introduce \textbf{TSANet}, a lightweight \textbf{T}wo-stage
network via \textbf{S}tate space augmented cross-\textbf{A}ttention, which can
handle event pixels inpainting and demosaicing separately, leveraging the
benefits of dividing complex tasks into manageable subtasks. Furthermore, we
introduce a lightweight Cross-Swin State Block that uniquely utilizes
positional prior for demosaicing and enhances global dependencies through the
state space model with linear complexity. In summary, TSANet demonstrates
excellent demosaicing performance on both simulated and real data of HybridEVS
while maintaining a lightweight model, averaging better results than the
previous state-of-the-art method DemosaicFormer across seven diverse datasets
in both PSNR and SSIM, while respectively reducing parameter and computation
costs by $1.86\times$ and $3.29\times$. Our approach presents new possibilities
for efficient image demosaicing on mobile devices. Code is available in the
supplementary materials.

</details>


### [80] [Distribution-Specific Learning for Joint Salient and Camouflaged Object Detection](https://arxiv.org/abs/2508.06063)
*Chao Hao,Zitong Yu,Xin Liu,Yuhao Wang,Weicheng Xie,Jingang Shi,Huanjing Yue,Jingyu Yang*

Main category: cs.CV

TL;DR: This paper introduces SCJoint, a joint learning scheme for Salient object detection (SOD) and camouflaged object detection (COD) tasks. The network can simultaneously possess the capability to find both salient and camouflaged objects, allowing both tasks to benefit from joint learning. Furthermore, the paper proposes a saliency-based sampling strategy to sample the training set of the SOD task to balance the training set sizes of the two tasks. The experiments demonstrate the competitive performance and effectiveness of the proposed method.


<details>
  <summary>Details</summary>
Motivation: Previous works have mostly believed that joint learning of these two tasks would confuse the network, reducing its performance on both tasks. However, this paper presents an opposite perspective: with the correct approach to learning, the network can simultaneously possess the capability to find both salient and camouflaged objects, allowing both tasks to benefit from joint learning.

Method: The key to the method is to learn the respective means and variances of the decoding processes for both tasks by inserting a minimal amount of task-specific learnable parameters within a fully shared network structure, thereby decoupling the contradictory attributes of the two tasks at a minimal cost. Furthermore, a saliency-based sampling strategy (SBSS) is proposed to sample the training set of the SOD task to balance the training set sizes of the two tasks. Based on the proposed SCJoint and SBSS, a powerful generalist network, named JoNet, is trained.

Result: A joint learning scheme for SOD and COD tasks, assuming that the decoding processes of SOD and COD have different distribution characteristics. Propose SCJoint, a joint learning scheme for SOD and COD tasks.

Conclusion: The experiments demonstrate the competitive performance and effectiveness of the proposed method.

Abstract: Salient object detection (SOD) and camouflaged object detection (COD) are two
closely related but distinct computer vision tasks. Although both are
class-agnostic segmentation tasks that map from RGB space to binary space, the
former aims to identify the most salient objects in the image, while the latter
focuses on detecting perfectly camouflaged objects that blend into the
background in the image. These two tasks exhibit strong contradictory
attributes. Previous works have mostly believed that joint learning of these
two tasks would confuse the network, reducing its performance on both tasks.
However, here we present an opposite perspective: with the correct approach to
learning, the network can simultaneously possess the capability to find both
salient and camouflaged objects, allowing both tasks to benefit from joint
learning. We propose SCJoint, a joint learning scheme for SOD and COD tasks,
assuming that the decoding processes of SOD and COD have different distribution
characteristics. The key to our method is to learn the respective means and
variances of the decoding processes for both tasks by inserting a minimal
amount of task-specific learnable parameters within a fully shared network
structure, thereby decoupling the contradictory attributes of the two tasks at
a minimal cost. Furthermore, we propose a saliency-based sampling strategy
(SBSS) to sample the training set of the SOD task to balance the training set
sizes of the two tasks. In addition, SBSS improves the training set quality and
shortens the training time. Based on the proposed SCJoint and SBSS, we train a
powerful generalist network, named JoNet, which has the ability to
simultaneously capture both ``salient" and ``camouflaged". Extensive
experiments demonstrate the competitive performance and effectiveness of our
proposed method. The code is available at https://github.com/linuxsino/JoNet.

</details>


### [81] [Can Large Models Fool the Eye? A New Turing Test for Biological Animation](https://arxiv.org/abs/2508.06072)
*Zijian Chen,Lirong Deng,Zhengyu Chen,Kaiwei Zhang,Qi Jia,Yuan Tian,Yucheng Zhu,Guangtao Zhai*

Main category: cs.CV

TL;DR: BioMotion Arena：通过视觉动画评估 LLM 和 MLLM 的新框架，揭示了现有模型在生成生物运动方面的不足。


<details>
  <summary>Details</summary>
Motivation: 评估大型模型的能力并揭示其差距具有挑战性。当前的基准测试采用基于静态数据集的基于 ground-truth 的评分形式评估或不明确的文本聊天机器人式人类偏好收集，这可能无法为用户提供关于性能差异的即时、直观和可感知的反馈。

Method: 通过视觉动画评估大型语言模型 (LLM) 和多模态大型语言模型 (MLLM)。

Result: 众包的人工投票与专家评分者非常一致，证明了我们的 BioMotion Arena 在提供区分性反馈方面的优越性。

Conclusion: 超过 90% 的评估模型（包括 InternVL3 和 Claude-4 系列等先进的开源模型和专有模型）无法生成基本的人形点光源组，更不用说平滑且生物学上合理的运动。这使得 BioMotion Arena 能够成为一个具有挑战性的性能可视化基准和一个灵活的评估框架，不受 ground-truth 的限制。

Abstract: Evaluating the abilities of large models and manifesting their gaps are
challenging. Current benchmarks adopt either ground-truth-based score-form
evaluation on static datasets or indistinct textual chatbot-style human
preferences collection, which may not provide users with immediate, intuitive,
and perceptible feedback on performance differences. In this paper, we
introduce BioMotion Arena, a novel framework for evaluating large language
models (LLMs) and multimodal large language models (MLLMs) via visual
animation. Our methodology draws inspiration from the inherent visual
perception of motion patterns characteristic of living organisms that utilizes
point-light source imaging to amplify the performance discrepancies between
models. Specifically, we employ a pairwise comparison evaluation and collect
more than 45k votes for 53 mainstream LLMs and MLLMs on 90 biological motion
variants. Data analyses show that the crowd-sourced human votes are in good
agreement with those of expert raters, demonstrating the superiority of our
BioMotion Arena in offering discriminative feedback. We also find that over
90\% of evaluated models, including the cutting-edge open-source InternVL3 and
proprietary Claude-4 series, fail to produce fundamental humanoid point-light
groups, much less smooth and biologically plausible motions. This enables
BioMotion Arena to serve as a challenging benchmark for performance
visualization and a flexible evaluation framework without restrictions on
ground-truth.

</details>


### [82] [Towards MR-Based Trochleoplasty Planning](https://arxiv.org/abs/2508.06076)
*Michael Wehrli,Alicia Durrer,Paul Friedrich,Sidaty El Hadramy,Edwin Li,Luana Brahaj,Carol C. Hasler,Philippe C. Cattin*

Main category: cs.CV

TL;DR: This paper presents a pipeline that generates high-resolution 3D models from MR scans to guide Trochlear Dysplasia (TD) surgery, improving outcomes and reducing radiation exposure compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Current approaches to treat Trochlear Dysplasia (TD) rely on low-resolution clinical Magnetic Resonance (MR) scans and surgical intuition, leading to inconsistent outcomes with limited adoption of minimally invasive techniques. Existing work produces pseudo-healthy low-resolution 3D MR images or requires CT scans, increasing radiation.

Method: The pipeline generates super-resolved, patient-specific 3D pseudo-healthy target morphologies from conventional clinical MR scans. It computes an isotropic super-resolved MR volume using an Implicit Neural Representation (INR), segments femur, tibia, patella, and fibula with a multi-label custom-trained network, and trains a Wavelet Diffusion Model (WDM) to generate pseudo-healthy target morphologies of the trochlear region.

Result: The approach enables the generation of sub-millimeter resolved 3D shapes. Experiments on 25 TD patients show that the target morphologies significantly improve the sulcus angle (SA) and trochlear groove depth (TGD). The pipeline does not require CT scans, reducing radiation.

Conclusion: The proposed pipeline generates sub-millimeter resolved 3D shapes compatible for pre- and intraoperative use, serving as preoperative blueprints for reshaping the femoral groove while preserving native patella articulation. Evaluated on 25 TD patients, the target morphologies significantly improve the sulcus angle (SA) and trochlear groove depth (TGD).

Abstract: To treat Trochlear Dysplasia (TD), current approaches rely mainly on
low-resolution clinical Magnetic Resonance (MR) scans and surgical intuition.
The surgeries are planned based on surgeons experience, have limited adoption
of minimally invasive techniques, and lead to inconsistent outcomes. We propose
a pipeline that generates super-resolved, patient-specific 3D pseudo-healthy
target morphologies from conventional clinical MR scans. First, we compute an
isotropic super-resolved MR volume using an Implicit Neural Representation
(INR). Next, we segment femur, tibia, patella, and fibula with a multi-label
custom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to
generate pseudo-healthy target morphologies of the trochlear region. In
contrast to prior work producing pseudo-healthy low-resolution 3D MR images,
our approach enables the generation of sub-millimeter resolved 3D shapes
compatible for pre- and intraoperative use. These can serve as preoperative
blueprints for reshaping the femoral groove while preserving the native patella
articulation. Furthermore, and in contrast to other work, we do not require a
CT for our pipeline - reducing the amount of radiation. We evaluated our
approach on 25 TD patients and could show that our target morphologies
significantly improve the sulcus angle (SA) and trochlear groove depth (TGD).
The code and interactive visualization are available at
https://wehrlimi.github.io/sr-3d-planning/.

</details>


### [83] [DreamVE: Unified Instruction-based Image and Video Editing](https://arxiv.org/abs/2508.06080)
*Bin Xia,Jiyang Liu,Yuechen Zhang,Bohao Peng,Ruihang Chu,Yitong Wang,Xinglong Wu,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: DreamVE is a unified model for instruction-based image and video editing, trained in two stages with synthesized data and an efficient editing framework.


<details>
  <summary>Details</summary>
Motivation: Instruction-based video editing is constrained by limited training data, hindering its practical application.

Method: A two-stage training strategy (image then video) is used. Collage-based and generative model-based data synthesis pipelines are employed. An efficient editing framework is built on a SOTA T2V model using token concatenation with early drop.

Result: DreamVE achieves strong performance in key editing types and enhances generalization and transfer capabilities. Collage-based data excels in object manipulation, background changes, and text modifications but lacks attribute editing cases. Generative model-based data handles attribute editing cases.

Conclusion: DreamVE, a unified model for instruction-based image and video editing, is introduced with a two-stage training strategy (image then video) and comprehensive training data synthesis pipelines (collage-based and generative model-based).

Abstract: Instruction-based editing holds vast potential due to its simple and
efficient interactive editing format. However, instruction-based editing,
particularly for video, has been constrained by limited training data,
hindering its practical application. To this end, we introduce DreamVE, a
unified model for instruction-based image and video editing. Specifically, We
propose a two-stage training strategy: first image editing, then video editing.
This offers two main benefits: (1) Image data scales more easily, and models
are more efficient to train, providing useful priors for faster and better
video editing training. (2) Unifying image and video generation is natural and
aligns with current trends. Moreover, we present comprehensive training data
synthesis pipelines, including collage-based and generative model-based data
synthesis. The collage-based data synthesis combines foreground objects and
backgrounds to generate diverse editing data, such as object manipulation,
background changes, and text modifications. It can easily generate billions of
accurate, consistent, realistic, and diverse editing pairs. We pretrain DreamVE
on extensive collage-based data to achieve strong performance in key editing
types and enhance generalization and transfer capabilities. However,
collage-based data lacks some attribute editing cases, leading to a relative
drop in performance. In contrast, the generative model-based pipeline, despite
being hard to scale up, offers flexibility in handling attribute editing cases.
Therefore, we use generative model-based data to further fine-tune DreamVE.
Besides, we design an efficient and powerful editing framework for DreamVE. We
build on the SOTA T2V model and use a token concatenation with early drop
approach to inject source image guidance, ensuring strong consistency and
editability. The codes and models will be released.

</details>


### [84] [SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment](https://arxiv.org/abs/2508.06082)
*Yanxiao Sun,Jiafu Wu,Yun Cao,Chengming Xu,Yabiao Wang,Weijian Cao,Donghao Luo,Chengjie Wang,Yanwei Fu*

Main category: cs.CV

TL;DR: 提出了一种新的视频生成模型蒸馏框架SwiftVideo，该框架结合了轨迹保持和分布匹配策略，可以在减少推理步骤的同时保持高质量的视频生成。


<details>
  <summary>Details</summary>
Motivation: 基于扩散或基于流的模型在视频合成方面取得了显著进展，但需要多个迭代采样步骤，这会产生大量的计算开销。虽然已经开发了许多仅基于轨迹保持或分布匹配的蒸馏方法来加速视频生成模型，但这些方法在少量步骤的设置下通常会遇到性能下降或伪影增加的问题。

Method: 提出了一种统一且稳定的蒸馏框架SwiftVideo，该框架结合了轨迹保持和分布匹配策略的优点，引入了连续时间一致性蒸馏以确保精确保持ODE轨迹，并提出了包括合成数据和真实数据之间的分布对齐以及跨不同推理步骤的轨迹对齐的双重视角对齐。

Result: 在OpenVid-1M基准测试上的定量评估表明，该方法在少量步骤的视频生成中显著优于现有方法。

Conclusion: 该方法在少量步骤的视频生成中显著优于现有方法。

Abstract: Diffusion-based or flow-based models have achieved significant progress in
video synthesis but require multiple iterative sampling steps, which incurs
substantial computational overhead. While many distillation methods that are
solely based on trajectory-preserving or distribution-matching have been
developed to accelerate video generation models, these approaches often suffer
from performance breakdown or increased artifacts under few-step settings. To
address these limitations, we propose \textbf{\emph{SwiftVideo}}, a unified and
stable distillation framework that combines the advantages of
trajectory-preserving and distribution-matching strategies. Our approach
introduces continuous-time consistency distillation to ensure precise
preservation of ODE trajectories. Subsequently, we propose a dual-perspective
alignment that includes distribution alignment between synthetic and real data
along with trajectory alignment across different inference steps. Our method
maintains high-quality video generation while substantially reducing the number
of inference steps. Quantitative evaluations on the OpenVid-1M benchmark
demonstrate that our method significantly outperforms existing approaches in
few-step video generation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [85] [InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization](https://arxiv.org/abs/2508.05731)
*Yuhang Liu,Zeyu Liu,Shuanghe Zhu,Pengxiang Li,Congkai Xie,Jiasheng Wang,Xueyu Hu,Xiaotian Han,Jianbo Yuan,Xinyao Wang,Shengyu Zhang,Hongxia Yang,Fei Wu*

Main category: cs.AI

TL;DR: 提出了一种新的策略优化框架AEPO，解决了MLLM在GUI上进行自然语言指令理解时，由于探索不足导致的语义对齐问题。


<details>
  <summary>Details</summary>
Motivation: 多模式大型语言模型（MLLM）的出现推动了使用纯视觉输入在图形用户界面（GUI）上运行的自主代理的开发。一个根本的挑战是稳健地建立自然语言指令的基础。这需要精确的空间对齐，该对齐可以准确定位每个元素的坐标，更关键的是，正确的语义对齐，该对齐将指令与功能上合适的UI元素匹配。

Method: 提出了一种新的策略优化框架自适应探索策略优化（AEPO）。AEPO采用多答案生成策略来强制更广泛的探索，然后由从效率η=U/C的第一原理导出的理论自适应探索奖励（AER）函数来指导。

Result: AEPO训练的模型, InfiGUI-G1-3B 和 InfiGUI-G1-7B, 建立了新的state-of-the-art结果。

Conclusion: AEPO训练的模型InfiGUI-G1-3B和InfiGUI-G1-7B在多个具有挑战性的GUI基准测试中建立了新的最先进的结果，与旨在测试泛化和语义理解的基准测试中的naive RLVR基线相比，实现了高达9.0％的显着相对改进。

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has propelled the
development of autonomous agents that operate on Graphical User Interfaces
(GUIs) using pure visual input. A fundamental challenge is robustly grounding
natural language instructions. This requires a precise spatial alignment, which
accurately locates the coordinates of each element, and, more critically, a
correct semantic alignment, which matches the instructions to the functionally
appropriate UI element. Although Reinforcement Learning with Verifiable Rewards
(RLVR) has proven to be effective at improving spatial alignment for these
MLLMs, we find that inefficient exploration bottlenecks semantic alignment,
which prevent models from learning difficult semantic associations. To address
this exploration problem, we present Adaptive Exploration Policy Optimization
(AEPO), a new policy optimization framework. AEPO employs a multi-answer
generation strategy to enforce broader exploration, which is then guided by a
theoretically grounded Adaptive Exploration Reward (AER) function derived from
first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B
and InfiGUI-G1-7B, establish new state-of-the-art results across multiple
challenging GUI grounding benchmarks, achieving significant relative
improvements of up to 9.0% against the naive RLVR baseline on benchmarks
designed to test generalization and semantic understanding. Resources are
available at https://github.com/InfiXAI/InfiGUI-G1.

</details>


### [86] [A Framework for Inherently Safer AGI through Language-Mediated Active Inference](https://arxiv.org/abs/2508.05766)
*Bo Wen*

Main category: cs.AI

TL;DR: This paper proposes a novel framework for developing safe Artificial General Intelligence (AGI) by combining Active Inference principles with Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: We argue that traditional approaches to AI safety, focused on post-hoc interpretability and reward engineering, have fundamental limitations.

Method: The architecture implements a multi-agent system where agents self-organize according to Active Inference principles, with preferences and safety constraints flowing through hierarchical Markov blankets.

Result: The architecture implements a multi-agent system where agents self-organize according to Active Inference principles, with preferences and safety constraints flowing through hierarchical Markov blankets. We outline specific mechanisms for ensuring safety, including: (1) explicit separation of beliefs and preferences in natural language, (2) bounded rationality through resource-aware free energy minimization, and (3) compositional safety through modular agent structures.

Conclusion: This paper concludes with a research agenda centered on the Abstraction and Reasoning Corpus (ARC) benchmark, proposing experiments to validate our framework's safety properties. Our approach offers a path toward AGI development that is inherently safer, rather than retrofitted with safety measures.

Abstract: This paper proposes a novel framework for developing safe Artificial General
Intelligence (AGI) by combining Active Inference principles with Large Language
Models (LLMs). We argue that traditional approaches to AI safety, focused on
post-hoc interpretability and reward engineering, have fundamental limitations.
We present an architecture where safety guarantees are integrated into the
system's core design through transparent belief representations and
hierarchical value alignment. Our framework leverages natural language as a
medium for representing and manipulating beliefs, enabling direct human
oversight while maintaining computational tractability. The architecture
implements a multi-agent system where agents self-organize according to Active
Inference principles, with preferences and safety constraints flowing through
hierarchical Markov blankets. We outline specific mechanisms for ensuring
safety, including: (1) explicit separation of beliefs and preferences in
natural language, (2) bounded rationality through resource-aware free energy
minimization, and (3) compositional safety through modular agent structures.
The paper concludes with a research agenda centered on the Abstraction and
Reasoning Corpus (ARC) benchmark, proposing experiments to validate our
framework's safety properties. Our approach offers a path toward AGI
development that is inherently safer, rather than retrofitted with safety
measures.

</details>


### [87] [Whither symbols in the era of advanced neural networks?](https://arxiv.org/abs/2508.05776)
*Thomas L. Griffiths,Brenden M. Lake,R. Thomas McCoy,Ellie Pavlick,Taylor W. Webb*

Main category: cs.AI

TL;DR: 现代神经网络与人类思维类似，削弱了人类思维基于符号的观点，并提出了新的研究方向。


<details>
  <summary>Details</summary>
Motivation: 人类思维以符号系统的方式进行思考，体现在它们组合思想、产生新颖性和快速学习的方式上。

Method: 分析和论证

Result: 现代神经网络展现出与人类思维相似的能力。

Conclusion: 现代神经网络展现出与人类思维相似的能力，这削弱了认知过程和表征是符号的观点。虽然这些神经网络通常在符号系统生成的数据上进行训练，但这表明这些系统在表征人类思维必须解决的抽象问题中起着重要作用。因此，我们为人类思维的符号基础研究提出了一个新的议程。

Abstract: Some of the strongest evidence that human minds should be thought about in
terms of symbolic systems has been the way they combine ideas, produce novelty,
and learn quickly. We argue that modern neural networks -- and the artificial
intelligence systems built upon them -- exhibit similar abilities. This
undermines the argument that the cognitive processes and representations used
by human minds are symbolic, although the fact that these neural networks are
typically trained on data generated by symbolic systems illustrates that such
systems play an important role in characterizing the abstract problems that
human minds have to solve. This argument leads us to offer a new agenda for
research on the symbolic basis of human thought.

</details>


### [88] [Holistic Explainable AI (H-XAI): Extending Transparency Beyond Developers in AI-Driven Decision Making](https://arxiv.org/abs/2508.05792)
*Kausik Lakkaraju,Siva Likitha Valluru,Biplav Srivastava*

Main category: cs.AI

TL;DR: H-XAI是一个统一的框架，它结合了因果评级方法与传统的XAI方法，以支持解释作为一个交互的、多方法的过程，以回答利益相关者在个体决策层面和整体模型层面的特定问题。


<details>
  <summary>Details</summary>
Motivation: 当前的 eXplainable AI (XAI) 方法主要为开发者服务，通常侧重于证明模型输出的合理性，而不是支持不同的利益相关者的需求。最近向评估性 AI 的转变将解释重新定义为假设检验的工具，但仍然主要关注运营组织。

Method: H-XAI，一个统一的框架，它将因果评级方法与传统的XAI方法相结合，以支持解释作为一个交互的、多方法的过程。

Result: H-XAI允许利益相关者提出一系列问题，测试假设，并将模型行为与自动构建的随机和有偏见的基线进行比较。它结合了实例级别和全局解释，适应每个利益相关者的目标，无论是理解个人决策、评估组级别偏差，还是评估扰动下的鲁棒性。通过两个跨越六个场景的案例研究证明了我们方法的普遍性：二元信用风险分类和金融时间序列预测。

Conclusion: H-XAI填补了现有XAI方法的关键空白，它结合了因果评级和事后解释，以回答利益相关者在个体决策层面和整体模型层面的特定问题。

Abstract: Current eXplainable AI (XAI) methods largely serve developers, often focusing
on justifying model outputs rather than supporting diverse stakeholder needs. A
recent shift toward Evaluative AI reframes explanation as a tool for hypothesis
testing, but still focuses primarily on operational organizations. We introduce
Holistic-XAI (H-XAI), a unified framework that integrates causal rating methods
with traditional XAI methods to support explanation as an interactive,
multi-method process. H-XAI allows stakeholders to ask a series of questions,
test hypotheses, and compare model behavior against automatically constructed
random and biased baselines. It combines instance-level and global
explanations, adapting to each stakeholder's goals, whether understanding
individual decisions, assessing group-level bias, or evaluating robustness
under perturbations. We demonstrate the generality of our approach through two
case studies spanning six scenarios: binary credit risk classification and
financial time-series forecasting. H-XAI fills critical gaps left by existing
XAI methods by combining causal ratings and post-hoc explanations to answer
stakeholder-specific questions at both the individual decision level and the
overall model level.

</details>


### [89] [Safety of Embodied Navigation: A Survey](https://arxiv.org/abs/2508.05855)
*Zixia Wang,Jia Hu,Ronghui Mu*

Main category: cs.AI

TL;DR: a comprehensive analysis of safety in embodied navigation


<details>
  <summary>Details</summary>
Motivation: ensuring the safety of such systems is critical. This survey provides a comprehensive analysis of safety in embodied navigation from multiple perspectives

Method: a comprehensive analysis of safety in embodied navigation from multiple perspectives, encompassing attack strategies, defense mechanisms, and evaluation methodologies

Result: conducting a comprehensive examination of existing safety challenges, mitigation technologies, and various datasets and metrics that assess effectiveness and robustness, we explore unresolved issues and future research directions in embodied navigation safety

Conclusion: This survey aims to provide valuable insights that can guide future research toward the development of safer and more reliable embodied navigation systems. Furthermore, the findings of this study have broader implications for enhancing societal safety and increasing industrial efficiency.

Abstract: As large language models (LLMs) continue to advance and gain influence, the
development of embodied AI has accelerated, drawing significant attention,
particularly in navigation scenarios. Embodied navigation requires an agent to
perceive, interact with, and adapt to its environment while moving toward a
specified target in unfamiliar settings. However, the integration of embodied
navigation into critical applications raises substantial safety concerns. Given
their deployment in dynamic, real-world environments, ensuring the safety of
such systems is critical. This survey provides a comprehensive analysis of
safety in embodied navigation from multiple perspectives, encompassing attack
strategies, defense mechanisms, and evaluation methodologies. Beyond conducting
a comprehensive examination of existing safety challenges, mitigation
technologies, and various datasets and metrics that assess effectiveness and
robustness, we explore unresolved issues and future research directions in
embodied navigation safety. These include potential attack methods, mitigation
strategies, more reliable evaluation techniques, and the implementation of
verification frameworks. By addressing these critical gaps, this survey aims to
provide valuable insights that can guide future research toward the development
of safer and more reliable embodied navigation systems. Furthermore, the
findings of this study have broader implications for enhancing societal safety
and increasing industrial efficiency.

</details>


### [90] [Planning Agents on an Ego-Trip: Leveraging Hybrid Ego-Graph Ensembles for Improved Tool Retrieval in Enterprise Task Planning](https://arxiv.org/abs/2508.05888)
*Sahil Bansal,Sai Shruthi Sistla,Aarti Arikatala,Sebastian Schreiber*

Main category: cs.AI

TL;DR: This paper introduces a KG-based tool retrieval framework that captures the semantic relationships between tools and their functional dependencies to address the limitations of traditional approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches rely primarily on similarities between user queries and tool descriptions, which significantly limits retrieval accuracy, specifically when handling multi-step user requests.

Method: KG-based tool retrieval framework that captures the semantic relationships between tools and their functional dependencies. Our retrieval algorithm leverages ensembles of 1-hop ego tool graphs to model direct and indirect connections between tools

Result: tool graph-based method achieves 91.85% tool coverage on the micro-average Complete Recall metric, compared to 89.26% for re-ranked semantic-lexical hybrid retrieval

Conclusion: KG provides complementary signals to pure similarity matching, particularly for queries requiring sequential tool composition.

Abstract: Effective tool retrieval is essential for AI agents to select from a vast
array of tools when identifying and planning actions in the context of complex
user queries. Despite its central role in planning, this aspect remains
underexplored in the literature. Traditional approaches rely primarily on
similarities between user queries and tool descriptions, which significantly
limits retrieval accuracy, specifically when handling multi-step user requests.
To address these limitations, we propose a Knowledge Graph (KG)-based tool
retrieval framework that captures the semantic relationships between tools and
their functional dependencies. Our retrieval algorithm leverages ensembles of
1-hop ego tool graphs to model direct and indirect connections between tools,
enabling more comprehensive and contextual tool selection for multi-step tasks.
We evaluate our approach on a synthetically generated internal dataset across
six defined user classes, extending previous work on coherent dialogue
synthesis and too retrieval benchmarks. Results demonstrate that our tool
graph-based method achieves 91.85% tool coverage on the micro-average Complete
Recall metric, compared to 89.26% for re-ranked semantic-lexical hybrid
retrieval, the strongest non-KG baseline in our experiments. These findings
support our hypothesis that the structural information in the KG provides
complementary signals to pure similarity matching, particularly for queries
requiring sequential tool composition.

</details>


### [91] [Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making](https://arxiv.org/abs/2508.05996)
*Kaitao Chen,Mianxin Liu,Daoming Zong,Chaoyue Ding,Shaohao Rui,Yankai Jiang,Mu Zhou,Xiaosong Wang*

Main category: cs.AI

TL;DR: MedOrch, a mediator-guided multi-agent framework, improves medical multimodal decision-making by enabling VLMs to collaborate, outperforming individual agents without training.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent researches primarily focus on language-only tasks, with challenges in extending to multimodal scenarios. Blind combination of VLMs can amplify erroneous interpretations, and VLMs are less capable in instruction following and self-reflection compared to LLMs.

Method: MedOrch: LLM-based mediator agent enables multiple VLM-based expert agents to exchange and reflect on outputs.

Result: Collaboration within distinct VLM-based agents surpasses individual agent capabilities on five medical vision question answering benchmarks without model training.

Conclusion: Mediator-guided multi-agent collaboration advances medical multimodal intelligence, surpassing individual agent capabilities without training, validated on five medical vision question answering benchmarks.

Abstract: Complex medical decision-making involves cooperative workflows operated by
different clinicians. Designing AI multi-agent systems can expedite and augment
human-level clinical decision-making. Existing multi-agent researches primarily
focus on language-only tasks, yet their extension to multimodal scenarios
remains challenging. A blind combination of diverse vision-language models
(VLMs) can amplify an erroneous outcome interpretation. VLMs in general are
less capable in instruction following and importantly self-reflection, compared
to large language models (LLMs) of comparable sizes. This disparity largely
constrains VLMs' ability in cooperative workflows. In this study, we propose
MedOrch, a mediator-guided multi-agent collaboration framework for medical
multimodal decision-making. MedOrch employs an LLM-based mediator agent that
enables multiple VLM-based expert agents to exchange and reflect on their
outputs towards collaboration. We utilize multiple open-source general-purpose
and domain-specific VLMs instead of costly GPT-series models, revealing the
strength of heterogeneous models. We show that the collaboration within
distinct VLM-based agents can surpass the capabilities of any individual agent.
We validate our approach on five medical vision question answering benchmarks,
demonstrating superior collaboration performance without model training. Our
findings underscore the value of mediator-guided multi-agent collaboration in
advancing medical multimodal intelligence. Our code will be made publicly
available.

</details>


### [92] [Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning](https://arxiv.org/abs/2508.06042)
*Daechul Ahn,San Kim,Jonghyun Choi*

Main category: cs.AI

TL;DR: HIMA 是一种用于星际争霸 II 的分层多智能体框架，它优于现有技术，因为它结合了专业模仿模块与元级别编排。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 最近展示了令人印象深刻的动作序列预测能力，但常常难以应对动态的、长期的任务，例如实时战略游戏。在星际争霸 II (SC2) 等游戏中，智能体需要管理资源约束并适应部分可观察环境中不断变化的战场情况。这通常会使现有的基于 LLM 的方法不堪重负。

Method: 一种分层多智能体框架，该框架采用元控制器下的专业模仿学习智能体，称为战略规划器 (SP)。

Result: HIMA 在战略清晰度、适应性和计算效率方面优于现有技术

Conclusion: HIMA在战略清晰度、适应性和计算效率方面优于现有技术，强调了将专业模仿模块与元级别编排相结合以开发更强大的通用人工智能代理的潜力。

Abstract: Large Language Models (LLMs) have recently demonstrated impressive action
sequence prediction capabilities but often struggle with dynamic, long-horizon
tasks such as real-time strategic games. In a game such as StarCraftII (SC2),
agents need to manage resource constraints and adapt to evolving battlefield
situations in a partially observable environment. This often overwhelms
exisiting LLM-based approaches. To address these challenges, we propose a
hierarchical multi-agent framework that employs specialized imitation learning
agents under a meta-controller called Strategic Planner (SP). By expert
demonstrations, each specialized agent learns a distinctive strategy, such as
aerial support or defensive maneuvers, and produces coherent, structured
multistep action sequences. The SP then orchestrates these proposals into a
single, environmentally adaptive plan that ensures local decisions aligning
with long-term strategies. We call this HIMA (Hierarchical Imitation
Multi-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that
encompasses all race match combinations in SC2. Our empirical results show that
HIMA outperforms state of the arts in strategic clarity, adaptability, and
computational efficiency, underscoring the potential of combining specialized
imitation modules with meta-level orchestration to develop more robust,
general-purpose AI agents.

</details>


### [93] [LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences](https://arxiv.org/abs/2508.06060)
*Sankarshan Damle,Boi Faltings*

Main category: cs.AI

TL;DR: 本文提出了一个双重框架，利用参与式预算（PB）作为 LLM 资源分配的实践环境和评估其推理能力的自适应基准。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）越来越被期望处理复杂的决策任务，但它们执行结构化资源分配的能力仍未被充分探索。由于数据污染和现有基准的静态性质，评估它们的推理能力也很困难。

Method: 通过三种 prompting 策略（贪婪选择、直接优化和爬山启发式优化）来测试 LLMs 在可行性约束下选择项目子集的能力，并与效用最大化的 oracle 进行对比。

Result: 结果强调了提示设计的重要性，并表明 LLMs 在处理非结构化输入的情况下，对于机制设计很有希望。

Conclusion: LLMs 在处理非结构化输入的情况下，对于机制设计很有希望。

Abstract: Large Language Models (LLMs) are increasingly expected to handle complex
decision-making tasks, yet their ability to perform structured resource
allocation remains underexplored. Evaluating their reasoning is also difficult
due to data contamination and the static nature of existing benchmarks. We
present a dual-purpose framework leveraging Participatory Budgeting (PB) both
as (i) a practical setting for LLM-based resource allocation and (ii) an
adaptive benchmark for evaluating their reasoning capabilities. We task LLMs
with selecting project subsets under feasibility (e.g., budget) constraints via
three prompting strategies: greedy selection, direct optimization, and a
hill-climbing-inspired refinement. We benchmark LLMs' allocations against a
utility-maximizing oracle. Interestingly, we also test whether LLMs can infer
structured preferences from natural-language voter input or metadata, without
explicit votes. By comparing allocations based on inferred preferences to those
from ground-truth votes, we evaluate LLMs' ability to extract preferences from
open-ended input. Our results underscore the role of prompt design and show
that LLMs hold promise for mechanism design with unstructured inputs.

</details>


### [94] [Don't Forget Imagination!](https://arxiv.org/abs/2508.06062)
*Evgenii E. Vityaev,Andrei Mantsivoda*

Main category: cs.AI

TL;DR: 认知想象力在人工智能中被低估了。论文提出语义模型来模拟认知想象力。


<details>
  <summary>Details</summary>
Motivation: 论文认为认知想象力在人类思维中起着关键作用，但其在人工智能中的作用被大大低估，这导致了许多问题并削弱了人工智能的能力。

Method: 论文提出语义模型，一种新的数学模型方法，它像神经网络一样可以学习，并且基于概率因果关系。

Result: 语义模型可以模拟认知想象力，因为它们确保了想象情境的一致性，并实现了一种玻璃盒方法，允许将情境作为一个整体和连贯的相互关联的事实系统进行操作，这些事实通过因果关系粘合在一起。

Conclusion: 这篇论文呼吁更多关注认知想象力，认为它是人工智能领域下一个有希望的突破。

Abstract: Cognitive imagination is a type of imagination that plays a key role in human
thinking. It is not a ``picture-in-the-head'' imagination. It is a faculty to
mentally visualize coherent and holistic systems of concepts and causal links
that serve as semantic contexts for reasoning, decision making and prediction.
Our position is that the role of cognitive imagination is still greatly
underestimated, and this creates numerous problems and diminishes the current
capabilities of AI. For instance, when reasoning, humans rely on imaginary
contexts to retrieve background info. They also constantly return to the
context for semantic verification that their reasoning is still reasonable.
Thus, reasoning without imagination is blind. This paper is a call for greater
attention to cognitive imagination as the next promising breakthrough in
artificial intelligence. As an instrument for simulating cognitive imagination,
we propose semantic models -- a new approach to mathematical models that can
learn, like neural networks, and are based on probabilistic causal
relationships. Semantic models can simulate cognitive imagination because they
ensure the consistency of imaginary contexts and implement a glass-box approach
that allows the context to be manipulated as a holistic and coherent system of
interrelated facts glued together with causal relations.

</details>


### [95] [A Generic Complete Anytime Beam Search for Optimal Decision Tree](https://arxiv.org/abs/2508.06064)
*Harold Silvère Kiossou,Siegfried Nijssen,Pierre Schaus*

Main category: cs.AI

TL;DR: Proposed CA-DL8.5, a beam search algorithm for finding optimal decision trees with better anytime performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: Exact algorithms for finding optimal decision trees suffer from poor anytime behavior due to unbalanced search space exploration. Existing anytime extensions have not been systematically compared.

Method: CA-DL8.5, a generic, complete, and anytime beam search algorithm that extends the DL8.5 framework

Result: Introduced a new generic framework for exact and anytime decision tree learning and CA-DL8.5 using LDS provides the best anytime performance.

Conclusion: CA-DL8.5 with LDS consistently provides the best anytime performance, outperforming both other CA-DL8.5 variants and the Blossom algorithm while maintaining completeness and optimality guarantees.

Abstract: Finding an optimal decision tree that minimizes classification error is known
to be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic
programming guarantee optimality, they often suffer from poor anytime behavior
-- meaning they struggle to find high-quality decision trees quickly when the
search is stopped before completion -- due to unbalanced search space
exploration. To address this, several anytime extensions of exact methods have
been proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not
been systematically compared, making it difficult to assess their relative
effectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and
anytime beam search algorithm that extends the DL8.5 framework and unifies some
existing anytime strategies. In particular, CA-DL8.5 generalizes previous
approaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various
heuristics and relaxation mechanisms through a modular design. The algorithm
reuses DL8.5's efficient branch-and-bound pruning and trie-based caching,
combined with a restart-based beam search that gradually relaxes pruning
criteria to improve solution quality over time. Our contributions are twofold:
(1) We introduce this new generic framework for exact and anytime decision tree
learning, enabling the incorporation of diverse heuristics and search
strategies; (2) We conduct a rigorous empirical comparison of several
instantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k
heuristics -- using an anytime evaluation metric called the primal gap
integral. Experimental results on standard classification benchmarks show that
CA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime
performance, outperforming both other CA-DL8.5 variants and the Blossom
algorithm while maintaining completeness and optimality guarantees.

</details>


### [96] [ME$^3$-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception](https://arxiv.org/abs/2508.06074)
*Siyi Lu,Run Liu,Dongsheng Yang,Lei He*

Main category: cs.AI

TL;DR: This paper presents ME3-BEV, a DRL framework for autonomous driving that integrates bird's-eye view perception with the Mamba framework, achieving superior performance in CARLA simulator.


<details>
  <summary>Details</summary>
Motivation: Traditional modular approaches suffer from error propagation and coordination issues, whereas end-to-end learning systems face computational bottlenecks in autonomous driving.

Method: The paper introduces the Mamba-BEV model, an efficient spatio-temporal feature extraction network that combines BEV-based perception with the Mamba framework for temporal feature modeling. Building on this, the ME3-BEV framework utilizes the Mamba-BEV model as a feature input for end-to-end DRL.

Result: The ME3-BEV framework achieves superior performance in dynamic urban driving scenarios. The interpretability of the model is enhanced by visualizing high-dimensional features through semantic segmentation.

Conclusion: The ME3-BEV framework outperforms existing models in CARLA simulator across multiple metrics, offering a promising solution for real-time autonomous driving.

Abstract: Autonomous driving systems face significant challenges in perceiving complex
environments and making real-time decisions. Traditional modular approaches,
while offering interpretability, suffer from error propagation and coordination
issues, whereas end-to-end learning systems can simplify the design but face
computational bottlenecks. This paper presents a novel approach to autonomous
driving using deep reinforcement learning (DRL) that integrates bird's-eye view
(BEV) perception for enhanced real-time decision-making. We introduce the
\texttt{Mamba-BEV} model, an efficient spatio-temporal feature extraction
network that combines BEV-based perception with the Mamba framework for
temporal feature modeling. This integration allows the system to encode vehicle
surroundings and road features in a unified coordinate system and accurately
model long-range dependencies. Building on this, we propose the
\texttt{ME$^3$-BEV} framework, which utilizes the \texttt{Mamba-BEV} model as a
feature input for end-to-end DRL, achieving superior performance in dynamic
urban driving scenarios. We further enhance the interpretability of the model
by visualizing high-dimensional features through semantic segmentation,
providing insight into the learned representations. Extensive experiments on
the CARLA simulator demonstrate that \texttt{ME$^3$-BEV} outperforms existing
models across multiple metrics, including collision rate and trajectory
accuracy, offering a promising solution for real-time autonomous driving.

</details>


### [97] [Aggregate-Combine-Readout GNNs Are More Expressive Than Logic C2](https://arxiv.org/abs/2508.06091)
*Stan P Hauke,Przemysław Andrzej Wałęga*

Main category: cs.AI

TL;DR: The paper proves that aggregate-combine-readout GNNs are more expressive than C2, solving an open problem and providing logical insights.


<details>
  <summary>Details</summary>
Motivation: Understanding the expressive power of graph neural networks (GNNs) by relating them to logical languages, and a challenging open problem whether full C2 characterises the logical expressiveness of aggregate-combine-readout GNNs.

Method: Proving that the logical expressiveness of aggregate-combine-readout GNNs strictly exceeds that of C2.

Result: The logical expressiveness of aggregate-combine-readout GNNs strictly exceeds that of C2. This result holds over both undirected and directed graphs. Beyond its implications for GNNs, this work also leads to purely logical insights on the expressive power of infinitary logics.

Conclusion: The logical expressiveness of aggregate-combine-readout GNNs strictly exceeds that of C2.

Abstract: In recent years, there has been growing interest in understanding the
expressive power of graph neural networks (GNNs) by relating them to logical
languages. This research has been been initialised by an influential result of
Barcel\'o et al. (2020), who showed that the graded modal logic (or a guarded
fragment of the logic C2), characterises the logical expressiveness of
aggregate-combine GNNs. As a ``challenging open problem'' they left the
question whether full C2 characterises the logical expressiveness of
aggregate-combine-readout GNNs. This question has remained unresolved despite
several attempts. In this paper, we solve the above open problem by proving
that the logical expressiveness of aggregate-combine-readout GNNs strictly
exceeds that of C2. This result holds over both undirected and directed graphs.
Beyond its implications for GNNs, our work also leads to purely logical
insights on the expressive power of infinitary logics.

</details>


### [98] [PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion](https://arxiv.org/abs/2508.06110)
*Yiran Rex Ma*

Main category: cs.AI

TL;DR: PanelTR uses LLM agent scientists in a structured scientific approach for robust table reasoning, outperforming vanilla LLMs without training data.


<details>
  <summary>Details</summary>
Motivation: Table reasoning, including tabular QA and fact verification, often depends on annotated data or complex data augmentation, limiting flexibility and generalization. LLMs, despite their versatility, often underperform compared to simple supervised models.

Method: a framework utilizing LLM agent scientists for robust table reasoning through a structured scientific approach

Result: PanelTR outperforms vanilla LLMs and rivals fully supervised models, all while remaining independent of training data

Conclusion: structured scientific methodology can effectively handle complex tasks beyond table reasoning with flexible semantic understanding in a zero-shot context

Abstract: Table reasoning, including tabular QA and fact verification, often depends on
annotated data or complex data augmentation, limiting flexibility and
generalization. LLMs, despite their versatility, often underperform compared to
simple supervised models. To approach these issues, we introduce PanelTR, a
framework utilizing LLM agent scientists for robust table reasoning through a
structured scientific approach. PanelTR's workflow involves agent scientists
conducting individual investigations, engaging in self-review, and
participating in collaborative peer-review discussions. This process, driven by
five scientist personas, enables semantic-level transfer without relying on
data augmentation or parametric optimization. Experiments across four
benchmarks show that PanelTR outperforms vanilla LLMs and rivals fully
supervised models, all while remaining independent of training data. Our
findings indicate that structured scientific methodology can effectively handle
complex tasks beyond table reasoning with flexible semantic understanding in a
zero-shot context.

</details>


### [99] [SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges](https://arxiv.org/abs/2508.06111)
*Dewi S. W. Gould,Bruno Mlodozeniec,Samuel F. Brown*

Main category: cs.AI

TL;DR: SKATE is a novel evaluation framework in which large language models (LLMs) compete by generating and solving verifiable tasks for one another. It is fully automated, data-free, and scalable, requiring no human input or domain expertise. 


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods demand extensive domain expertise, hindering their scalability as these models rapidly evolve.

Method: LLMs compete by generating and solving verifiable tasks for one another.

Result: Weaker models can reliably differentiate and score stronger ones, LLM-based systems are capable of self-preferencing behavior.

Conclusion: SKATE automatically surfaces fine-grained capability differences between models and offers a scalable evaluation framework.

Abstract: Evaluating the capabilities and risks of foundation models is paramount, yet
current methods demand extensive domain expertise, hindering their scalability
as these models rapidly evolve. We introduce SKATE: a novel evaluation
framework in which large language models (LLMs) compete by generating and
solving verifiable tasks for one another. Our core insight is to treat
evaluation as a game: models act as both task-setters and solvers, incentivized
to create questions which highlight their own strengths while exposing others'
weaknesses. SKATE offers several key advantages, balancing scalability,
open-endedness, and objectivity. It is fully automated, data-free, and
scalable, requiring no human input or domain expertise. By using verifiable
tasks rather than LLM judges, scoring is objective. Unlike domain-limited
programmatically-generated benchmarks (e.g. chess-playing or spatial
reasoning), having LLMs creatively pose challenges enables open-ended and
scalable evaluation. As a proof of concept, we introduce LLM-set
code-output-prediction (COP) challenges as a verifiable and extensible
framework in which to test our approach. Using a TrueSkill-based ranking
system, we evaluate six frontier LLMs and find that: (1) weaker models can
reliably differentiate and score stronger ones, (2) LLM-based systems are
capable of self-preferencing behavior, generating questions that align with
their own capabilities, and (3) SKATE automatically surfaces fine-grained
capability differences between models. Our findings are an important step
towards general, scalable evaluation frameworks which can keep pace with LLM
progress.

</details>


### [100] [Study of Robust Features in Formulating Guidance for Heuristic Algorithms for Solving the Vehicle Routing Problem](https://arxiv.org/abs/2508.06129)
*Bachtiar Herdianto,Romain Billot,Flavien Lucas,Marc Sevaux*

Main category: cs.AI

TL;DR: This study uses machine learning and explainable AI to analyze VRP solutions, finding key features for better metaheuristic algorithm design.


<details>
  <summary>Details</summary>
Motivation: machine learning methods can be used the structural characteristics of solutions in combinatorial optimization, thereby aiding in designing more efficient algorithms, particularly for solving VRP

Method: sensitivity analysis using multiple classifier models that are capable of predicting the quality of VRP solutions, leveraging explainable AI

Result: feature importance varies, certain features consistently emerge as strong predictors; a unified framework able of ranking feature impact across different scenarios

Conclusion: feature importance analysis as a foundation for developing a guidance mechanism of metaheuristic algorithms for solving the VRP

Abstract: The Vehicle Routing Problem (VRP) is a complex optimization problem with
numerous real-world applications, mostly solved using metaheuristic algorithms
due to its $\mathcal{NP}$-Hard nature. Traditionally, these metaheuristics rely
on human-crafted designs developed through empirical studies. However, recent
research shows that machine learning methods can be used the structural
characteristics of solutions in combinatorial optimization, thereby aiding in
designing more efficient algorithms, particularly for solving VRP. Building on
this advancement, this study extends the previous research by conducting a
sensitivity analysis using multiple classifier models that are capable of
predicting the quality of VRP solutions. Hence, by leveraging explainable AI,
this research is able to extend the understanding of how these models make
decisions. Finally, our findings indicate that while feature importance varies,
certain features consistently emerge as strong predictors. Furthermore, we
propose a unified framework able of ranking feature impact across different
scenarios to illustrate this finding. These insights highlight the potential of
feature importance analysis as a foundation for developing a guidance mechanism
of metaheuristic algorithms for solving the VRP.

</details>


### [101] [Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications](https://arxiv.org/abs/2508.06145)
*Byeonghun Bang,Jongsuk Yoon,Dong-Jin Chang,Seho Park,Yong Oh Lee*

Main category: cs.AI

TL;DR: 本研究通过实施RAG管道来增强LLM有效处理药物禁忌症的能力，结果表明，该方法可以显著提高模型准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的多功能性已在各个领域得到探索，但它们在医疗保健领域的应用带来了挑战，尤其是在需要准确和可靠信息的药物禁忌症领域。

Method: 实施了一个检索增强生成（RAG）管道，利用OpenAI的GPT-4o-mini作为基础模型，text-embedding-3-small模型用于嵌入，并整合了Langchain来协调一个带有重新排序的混合检索系统。该系统利用来自公共数据库的药物利用审查（DUR）数据，重点关注特定年龄组、妊娠和伴随药物使用的禁忌症。

Result: RAG管道集成后，模型准确率显著提高，年龄组、妊娠和伴随药物使用的禁忌症准确率分别达到0.94、0.87和0.89。基线模型的准确率范围为0.49至0.57。

Conclusion: RAG框架增强LLM可以显著减少处方和药物摄入决策中的不确定性，因为它提供了更精确和可靠的药物禁忌信息。

Abstract: The versatility of large language models (LLMs) has been explored across
various sectors, but their application in healthcare poses challenges,
particularly in the domain of pharmaceutical contraindications where accurate
and reliable information is required. This study enhances the capability of
LLMs to address contraindications effectively by implementing a Retrieval
Augmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base
model, and the text-embedding-3-small model for embeddings, our approach
integrates Langchain to orchestrate a hybrid retrieval system with re-ranking.
This system leverages Drug Utilization Review (DUR) data from public databases,
focusing on contraindications for specific age groups, pregnancy, and
concomitant drug use. The dataset includes 300 question-answer pairs across
three categories, with baseline model accuracy ranging from 0.49 to 0.57.
Post-integration of the RAG pipeline, we observed a significant improvement in
model accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications
related to age groups, pregnancy, and concomitant drug use, respectively. The
results indicate that augmenting LLMs with a RAG framework can substantially
reduce uncertainty in prescription and drug intake decisions by providing more
precise and reliable drug contraindication information.

</details>


### [102] [Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven Solution](https://arxiv.org/abs/2508.06225)
*Zailong Tian,Zhuoheng Han,Yanzhe Chen,Haozhe Xu,Xi Yang,richeng xuan,Hongfeng Wang,Lizi Liao*

Main category: cs.AI

TL;DR: This paper advocates for confidence-driven, risk-aware LLM-as-a-Judge systems, identifies the overconfidence phenomenon, and introduces LLM-as-a-Fuser to improve calibration and reliability.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-as-a-Judge systems focus on accuracy but overlook the necessity of well-calibrated confidence for trustworthy and adaptive evaluation.

Method: The paper proposes LLM-as-a-Fuser and TH-Score to address the overconfidence issue.

Result: The proposed approach substantially improves calibration and enables adaptive, confidence-driven evaluation pipelines, achieving superior reliability and accuracy compared to existing baselines.

Conclusion: The paper introduces LLM-as-a-Fuser, an ensemble framework that improves calibration and enables adaptive, confidence-driven evaluation pipelines, achieving superior reliability and accuracy compared to existing baselines.

Abstract: Large Language Models (LLMs) are widely used as automated judges, where
practical value depends on both accuracy and trustworthy, risk-aware judgments.
Existing approaches predominantly focus on accuracy, overlooking the necessity
of well-calibrated confidence, which is vital for adaptive and reliable
evaluation pipelines. In this work, we advocate a shift from accuracy-centric
evaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing
the necessity of well-calibrated confidence for trustworthy and adaptive
evaluation. We systematically identify the **Overconfidence Phenomenon** in
current LLM-as-a-Judges, where predicted confidence significantly overstates
actual correctness, undermining reliability in practical deployment. To
quantify this phenomenon, we introduce **TH-Score**, a novel metric measuring
confidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an
ensemble framework that transforms LLMs into reliable, risk-aware evaluators.
Extensive experiments demonstrate that our approach substantially improves
calibration and enables adaptive, confidence-driven evaluation pipelines,
achieving superior reliability and accuracy compared to existing baselines.

</details>


### [103] [GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines](https://arxiv.org/abs/2508.06226)
*Yumeng Fu,Jiayin Zhu,Lingling Zhang,Bo Zhao,Shaoxuan Ma,Yushun Zhang,Yanrui Wu,Wenjun Wu*

Main category: cs.AI

TL;DR: GeoLaux 是一个用于评估 MLLM 几何推理能力的新基准，特别是对于长步骤推理和辅助线构造。实验表明，目前的 MLLM 在这些方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 现有的 MLLM 几何技能评估基准忽略了辅助线构造，缺乏细粒度的过程评估，不足以评估 MLLM 的长步骤推理能力。

Method: 提出了 GeoLaux 基准，包含 2,186 个几何问题，考察计算和证明题，平均 6.51 个推理步骤，41.8% 需要辅助线。

Result: 对 13 个 MLLM 的大量实验表明：模型在扩展推理步骤中性能显著下降（超过 50%），在解决证明问题时倾向于走捷径，并且缺乏辅助线意识；提高辅助线能力对提升整体几何推理能力非常有益。

Conclusion: GeoLaux 证明了 MLLM 在几何问题解决中，尤其是在长推理步骤和辅助线使用方面存在不足，并为未来的能力提升提供了方向。

Abstract: Geometry problem solving (GPS) requires models to master diagram
comprehension, logical reasoning, knowledge application, numerical computation,
and auxiliary line construction. This presents a significant challenge for
Multimodal Large Language Models (MLLMs). However, existing benchmarks for
evaluating MLLM geometry skills overlook auxiliary line construction and lack
fine-grained process evaluation, making them insufficient for assessing MLLMs'
long-step reasoning abilities. To bridge these gaps, we present the GeoLaux
benchmark, comprising 2,186 geometry problems, incorporating both calculation
and proving questions. Notably, the problems require an average of 6.51
reasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliary
line construction. Building on the dataset, we design a novel five-dimensional
evaluation strategy assessing answer correctness, process correctness, process
quality, auxiliary line impact, and error causes. Extensive experiments on 13
leading MLLMs (including thinking models and non-thinking models) yield three
pivotal findings: First, models exhibit substantial performance degradation in
extended reasoning steps (nine models demonstrate over 50% performance drop).
Second, compared to calculation problems, MLLMs tend to take shortcuts when
solving proving problems. Third, models lack auxiliary line awareness, and
enhancing this capability proves particularly beneficial for overall geometry
reasoning improvement. These findings establish GeoLaux as both a benchmark for
evaluating MLLMs' long-step geometric reasoning with auxiliary lines and a
guide for capability advancement. Our dataset and code are included in
supplementary materials and will be released.

</details>


### [104] [Learning Logical Rules using Minimum Message Length](https://arxiv.org/abs/2508.06230)
*Ruben Sharma,Sebastijan Dumančić,Ross D. King,Andrew Cropper*

Main category: cs.AI

TL;DR: 提出了一种贝叶斯归纳逻辑编程方法，可以从噪声数据中学习，并在多个领域优于现有方法，同时具有数据效率。


<details>
  <summary>Details</summary>
Motivation: 统一概率学习和逻辑学习是人工智能中的一个关键挑战。

Method: 贝叶斯归纳逻辑编程方法，该方法从噪声数据中学习最小消息长度程序。

Result: 该方法明显优于以前的方法，并且具有数据效率，对示例平衡不敏感，包括从完全正面的示例中学习的能力。

Conclusion: 该方法在多个领域（包括游戏和药物设计）的实验表明，该方法明显优于以前的方法，特别是那些学习最小描述长度程序的方法。结果还表明，该方法具有数据效率，对示例平衡不敏感，包括从完全正面的示例中学习的能力。

Abstract: Unifying probabilistic and logical learning is a key challenge in AI. We
introduce a Bayesian inductive logic programming approach that learns minimum
message length programs from noisy data. Our approach balances hypothesis
complexity and data fit through priors, which explicitly favour more general
programs, and a likelihood that favours accurate programs. Our experiments on
several domains, including game playing and drug design, show that our method
significantly outperforms previous methods, notably those that learn minimum
description length programs. Our results also show that our approach is
data-efficient and insensitive to example balance, including the ability to
learn from exclusively positive examples.

</details>


### [105] [Symmetry breaking for inductive logic programming](https://arxiv.org/abs/2508.06263)
*Andrew Cropper,David M. Cerna,Matti Järvisalo*

Main category: cs.AI

TL;DR: Introduces a method to break symmetries in the hypothesis space to address the challenge of searching vast hypothesis spaces. Implemented in answer set programming. Experiments show that the approach can reduce solving times.


<details>
  <summary>Details</summary>
Motivation: The challenge is searching vast hypothesis spaces, which is exacerbated because many logically equivalent hypotheses exist.

Method: A method to break symmetries in the hypothesis space is introduced. The idea is implemented in answer set programming.

Result: Experiments on multiple domains, including visual reasoning and game playing, show that the approach can reduce solving times from over an hour to just 17 seconds.

Conclusion: The approach can reduce solving times.

Abstract: The goal of inductive logic programming is to search for a hypothesis that
generalises training data and background knowledge. The challenge is searching
vast hypothesis spaces, which is exacerbated because many logically equivalent
hypotheses exist. To address this challenge, we introduce a method to break
symmetries in the hypothesis space. We implement our idea in answer set
programming. Our experiments on multiple domains, including visual reasoning
and game playing, show that our approach can reduce solving times from over an
hour to just 17 seconds.

</details>


### [106] [LLM Robustness Leaderboard v1 --Technical report](https://arxiv.org/abs/2508.06296)
*Pierre Peigné - Lefebvre,Quentin Feuillade-Montixi,Tom David,Nicolas Miailhe*

Main category: cs.AI

TL;DR: PRISM Eval introduces a tool (BET) that achieves 100% ASR against most LLMs, identifies vulnerabilities, and proposes distributed robustness assessment.


<details>
  <summary>Details</summary>
Motivation: The motivation is to evaluate LLM robustness and identify vulnerabilities against adversarial attacks.

Method: The paper uses PRISM Eval Behavior Elicitation Tool (BET) with Dynamic Adversarial Optimization to perform automated red-teaming.

Result: The tool achieves 100% Attack Success Rate (ASR) against 37 of 41 state-of-the-art LLMs, reveals a 300-fold variance in attack difficulty, and identifies effective jailbreaking techniques.

Conclusion: This paper introduces methods for evaluating and improving the robustness of LLMs against adversarial attacks, finding vulnerabilities and proposing solutions for distributed robustness assessment.

Abstract: This technical report accompanies the LLM robustness leaderboard published by
PRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior
Elicitation Tool (BET), an AI system performing automated red-teaming through
Dynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)
against 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we
propose a fine-grained robustness metric estimating the average number of
attempts required to elicit harmful behaviors, revealing that attack difficulty
varies by over 300-fold across models despite universal vulnerability. We
introduce primitive-level vulnerability analysis to identify which jailbreaking
techniques are most effective for specific hazard categories. Our collaborative
evaluation with trusted third parties from the AI Safety Network demonstrates
practical pathways for distributed robustness assessment across the community.

</details>


### [107] [A "good regulator theorem" for embodied agents](https://arxiv.org/abs/2508.06326)
*Nathaniel Virgo,Martin Biehl,Manuel Baltieri,Matteo Capucci*

Main category: cs.AI

TL;DR: whenever an agent is able to perform a regulation task, it is possible for an observer to interpret it as having "beliefs" about its environment, which it "updates" in response to sensory input


<details>
  <summary>Details</summary>
Motivation: Artificial Life has produced many examples of systems that perform tasks with apparently no model in sight; these suggest Conant and Ashby's theorem doesn't easily generalise beyond its restricted setup

Method: a similar intuition can be fleshed out in a different way: whenever an agent is able to perform a regulation task, it is possible for an observer to interpret it as having "beliefs" about its environment, which it "updates" in response to sensory input. This notion of belief updating provides a notion of model that is more sophisticated than Conant and Ashby's, as well as a theorem that is more broadly applicable

Result: The model might be trivial, however, and this is how the apparent counterexamples are resolved.

Conclusion: whenever an agent is able to perform a regulation task, it is possible for an observer to interpret it as having "beliefs" about its environment, which it "updates" in response to sensory input

Abstract: In a classic paper, Conant and Ashby claimed that "every good regulator of a
system must be a model of that system." Artificial Life has produced many
examples of systems that perform tasks with apparently no model in sight; these
suggest Conant and Ashby's theorem doesn't easily generalise beyond its
restricted setup. Nevertheless, here we show that a similar intuition can be
fleshed out in a different way: whenever an agent is able to perform a
regulation task, it is possible for an observer to interpret it as having
"beliefs" about its environment, which it "updates" in response to sensory
input. This notion of belief updating provides a notion of model that is more
sophisticated than Conant and Ashby's, as well as a theorem that is more
broadly applicable. However, it necessitates a change in perspective, in that
the observer plays an essential role in the theory: models are not a mere
property of the system but are imposed on it from outside. Our theorem holds
regardless of whether the system is regulating its environment in a classic
control theory setup, or whether it's regulating its own internal state; the
model is of its environment either way. The model might be trivial, however,
and this is how the apparent counterexamples are resolved.

</details>


### [108] [AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games](https://arxiv.org/abs/2508.06348)
*Mille Mei Zhen Loo,Gert Luzkov,Paolo Burelli*

Main category: cs.AI

TL;DR: Developed AntiCheatPT_256, a transformer-based model for detecting cheating in Counter-Strike 2, achieving 89.17% accuracy and 93.36% AUC using a new dataset, CS2CD.


<details>
  <summary>Details</summary>
Motivation: Cheating in online video games compromises the integrity of gaming experiences. Anti-cheat systems face significant challenges in keeping pace with evolving cheating methods without imposing invasive measures on users' systems.

Method: a transformer-based machine learning model

Result: achieved an accuracy of 89.17% and an AUC of 93.36% on an unaugmented test set

Conclusion: The transformer model achieved an accuracy of 89.17% and an AUC of 93.36% on an unaugmented test set, offering a robust baseline for future research in data-driven cheat detection.

Abstract: Cheating in online video games compromises the integrity of gaming
experiences. Anti-cheat systems, such as VAC (Valve Anti-Cheat), face
significant challenges in keeping pace with evolving cheating methods without
imposing invasive measures on users' systems. This paper presents
AntiCheatPT\_256, a transformer-based machine learning model designed to detect
cheating behaviour in Counter-Strike 2 using gameplay data. To support this, we
introduce and publicly release CS2CD: A labelled dataset of 795 matches. Using
this dataset, 90,707 context windows were created and subsequently augmented to
address class imbalance. The transformer model, trained on these windows,
achieved an accuracy of 89.17\% and an AUC of 93.36\% on an unaugmented test
set. This approach emphasizes reproducibility and real-world applicability,
offering a robust baseline for future research in data-driven cheat detection.

</details>


### [109] [From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI](https://arxiv.org/abs/2508.06352)
*Christian Meske,Justin Brenne,Erdi Uenal,Sabahat Oelcer,Ayseguel Doganguen*

Main category: cs.AI

TL;DR: 提出了“解释性人工智能”作为一种互补范例，它利用生成式人工智能的能力来服务于人类理解的解释性伙伴，而不是算法透明度的提供者。


<details>
  <summary>Details</summary>
Motivation: 当前的可解释人工智能 (XAI) 方法优先考虑算法透明度，并以抽象的、非自适应的格式呈现解释，这些格式通常无法支持有意义的最终用户理解。

Method: 快速情境设计方法

Result: 用户更喜欢情境敏感的、多模态的解释，而不是技术透明性。

Conclusion: 用户更喜欢情境敏感的、多模态的解释，而不是技术透明性。研究结果揭示了为人类理解而设计的人工智能系统的实际紧迫性，并为在不同领域和文化背景下推进以用户为中心的人工智能解释方法建立了一个全面的研究议程。

Abstract: Current explainable AI (XAI) approaches prioritize algorithmic transparency
and present explanations in abstract, non-adaptive formats that often fail to
support meaningful end-user understanding. This paper introduces "Explanatory
AI" as a complementary paradigm that leverages generative AI capabilities to
serve as explanatory partners for human understanding rather than providers of
algorithmic transparency. While XAI reveals algorithmic decision processes for
model validation, Explanatory AI addresses contextual reasoning to support
human decision-making in sociotechnical contexts. We develop a definition and
systematic eight-dimensional conceptual model distinguishing Explanatory AI
through narrative communication, adaptive personalization, and progressive
disclosure principles. Empirical validation through Rapid Contextual Design
methodology with healthcare professionals demonstrates that users consistently
prefer context-sensitive, multimodal explanations over technical transparency.
Our findings reveal the practical urgency for AI systems designed for human
comprehension rather than algorithmic introspection, establishing a
comprehensive research agenda for advancing user-centered AI explanation
approaches across diverse domains and cultural contexts.

</details>


### [110] [Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned](https://arxiv.org/abs/2508.06368)
*Claudia dAmato,Giuseppe Rubini,Francesco Didio,Donato Francioso,Fatima Zahra Amara,Nicola Fanizzi*

Main category: cs.AI

TL;DR: Developed a legal KG for violence against women cases using two automated construction approaches, validated it, and found it impactful for improving legal information accessibility.


<details>
  <summary>Details</summary>
Motivation: Legal Knowledge Graphs (KGs) would be a valuable tool to facilitate access to legal information and enable advanced reasoning, but few KGs can be found in the legal domain.

Method: The paper introduces two complementary approaches for automated legal KG construction: a systematic bottom-up approach and a new solution leveraging Large Language Models.

Result: Developed a legal KG targeting legal cases of violence against women, integrating structured data extraction, ontology development, and semantic enrichment.

Conclusion: The developed KGs are validated via suitable competency questions and may be impactful for multiple purposes, such as improving accessibility to legal information and enabling complex queries.

Abstract: Legal decision-making process requires the availability of comprehensive and
detailed legislative background knowledge and up-to-date information on legal
cases and related sentences/decisions. Legal Knowledge Graphs (KGs) would be a
valuable tool to facilitate access to legal information, to be queried and
exploited for the purpose, and to enable advanced reasoning and machine
learning applications. Indeed, legal KGs may act as knowledge intensive
component to be used by pre-dictive machine learning solutions supporting the
decision process of the legal expert. Nevertheless, a few KGs can be found in
the legal domain. To fill this gap, we developed a legal KG targeting legal
cases of violence against women, along with clear adopted methodologies.
Specifically, the paper introduces two complementary approaches for automated
legal KG construction; a systematic bottom-up approach, customized for the
legal domain, and a new solution leveraging Large Language Models. Starting
from legal sentences publicly available from the European Court of Justice, the
solutions integrate structured data extraction, ontology development, and
semantic enrichment to produce KGs tailored for legal cases involving violence
against women. After analyzing and comparing the results of the two approaches,
the developed KGs are validated via suitable competency questions. The obtained
KG may be impactful for multiple purposes: can improve the accessibility to
legal information both to humans and machine, can enable complex queries and
may constitute an important knowledge component to be possibly exploited by
machine learning tools tailored for predictive justice.

</details>


### [111] [The Fair Game: Auditing & Debiasing AI Algorithms Over Time](https://arxiv.org/abs/2508.06443)
*Debabrota Basu,Udvas Das*

Main category: cs.AI

TL;DR: Fair Game, a dynamic mechanism using Reinforcement Learning, is proposed to assure fairness in ML algorithm predictions and adapt to societal interaction over time, addressing the limitations of existing observational bias definitions.


<details>
  <summary>Details</summary>
Motivation: There is a gap between what we want Fair ML to achieve and what it does in a dynamic social environment. Observational definitions of bias are often conflicting and can only be deployed if the ground truth is known or only in retrospect after deploying the algorithm.

Method: Fair Game puts together an Auditor and a Debiasing algorithm in a loop around an ML algorithm by leveraging Reinforcement Learning (RL).

Result: Fair Game aims to simulate the evolution of ethical and legal frameworks in the society by creating an auditor which sends feedback to a debiasing algorithm deployed around an ML system. This allows us to develop a flexible and adaptive-over-time framework to build Fair ML systems pre- and post-deployment.

Conclusion: Fair Game provides a unique framework where the fairness goals can be adapted over time by only modifying the auditor and the different biases it quantifies.

Abstract: An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify
different types of bias (also known as unfairness) exhibited in the predictions
of ML algorithms, and to design new algorithms to mitigate them. Often, the
definitions of bias used in the literature are observational, i.e. they use the
input and output of a pre-trained algorithm to quantify a bias under concern.
In reality,these definitions are often conflicting in nature and can only be
deployed if either the ground truth is known or only in retrospect after
deploying the algorithm. Thus,there is a gap between what we want Fair ML to
achieve and what it does in a dynamic social environment. Hence, we propose an
alternative dynamic mechanism,"Fair Game",to assure fairness in the predictions
of an ML algorithm and to adapt its predictions as the society interacts with
the algorithm over time. "Fair Game" puts together an Auditor and a Debiasing
algorithm in a loop around an ML algorithm. The "Fair Game" puts these two
components in a loop by leveraging Reinforcement Learning (RL). RL algorithms
interact with an environment to take decisions, which yields new observations
(also known as data/feedback) from the environment and in turn, adapts future
decisions. RL is already used in algorithms with pre-fixed long-term fairness
goals. "Fair Game" provides a unique framework where the fairness goals can be
adapted over time by only modifying the auditor and the different biases it
quantifies. Thus,"Fair Game" aims to simulate the evolution of ethical and
legal frameworks in the society by creating an auditor which sends feedback to
a debiasing algorithm deployed around an ML system. This allows us to develop a
flexible and adaptive-over-time framework to build Fair ML systems pre- and
post-deployment.

</details>


### [112] [What Voting Rules Actually Do: A Data-Driven Analysis of Multi-Winner Voting](https://arxiv.org/abs/2508.06454)
*Joshua Caiata,Ben Armstrong,Kate Larson*

Main category: cs.AI

TL;DR: We propose a data-driven framework to evaluate how frequently voting rules violate axioms. We then show that neural networks can outperform traditional rules in minimizing axiom violations.


<details>
  <summary>Details</summary>
Motivation: Identifying which properties are satisfied by different multi-winner voting rules.

Method: A data-driven framework to evaluate how frequently voting rules violate axioms across diverse preference distributions.

Result: Neural networks, acting as voting rules, can outperform traditional rules in minimizing axiom violations.

Conclusion: Data-driven approaches to social choice can inform the design of new voting systems and support the continuation of data-driven research in social choice.

Abstract: Committee-selection problems arise in many contexts and applications, and
there has been increasing interest within the social choice research community
on identifying which properties are satisfied by different multi-winner voting
rules. In this work, we propose a data-driven framework to evaluate how
frequently voting rules violate axioms across diverse preference distributions
in practice, shifting away from the binary perspective of axiom satisfaction
given by worst-case analysis. Using this framework, we analyze the relationship
between multi-winner voting rules and their axiomatic performance under several
preference distributions. We then show that neural networks, acting as voting
rules, can outperform traditional rules in minimizing axiom violations. Our
results suggest that data-driven approaches to social choice can inform the
design of new voting systems and support the continuation of data-driven
research in social choice.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [113] [A Cross-Perspective Annotated Dataset for Dynamic Object-Level Attention Modeling in Cloud Gaming](https://arxiv.org/abs/2508.06077)
*Hongqin Lei,Haowei Tang,Zhe Zhang*

Main category: cs.DB

TL;DR: A new game dataset from GTA V is presented, focusing on semantic relationships of objects. Player's in-game speed, object's size, and object's speed are the main factors impacting player's interest.


<details>
  <summary>Details</summary>
Motivation: Existing datasets usually focus on the positions of objects while ignoring semantic relationships with other objects and their unique features.

Method: Collecting gameplay clips from Grand Theft Auto (GTA) V, and annotating the player's interested objects during the gameplay.

Result: We present a game dataset by collecting gameplay clips from Grand Theft Auto (GTA) V, and annotating the player's interested objects during the gameplay.

Conclusion: The player's in-game speed, object's size, and object's speed are the main factors impacting player's interest.

Abstract: Cloud gaming has gained popularity as it provides high-quality gaming
experiences on thin hardware, such as phones and tablets. Transmitting gameplay
frames at high resolutions and ultra-low latency is the key to guaranteeing
players' quality of experience (QoE). Numerous studies have explored deep
learning (DL) techniques to address this challenge. The efficiency of these
DL-based approaches is highly affected by the dataset. However, existing
datasets usually focus on the positions of objects while ignoring semantic
relationships with other objects and their unique features. In this paper, we
present a game dataset by collecting gameplay clips from Grand Theft Auto (GTA)
V, and annotating the player's interested objects during the gameplay. Based on
the collected data, we analyze several factors that have an impact on player's
interest and identify that the player's in-game speed, object's size, and
object's speed are the main factors. The dataset is available at
https://drive.google.com/drive/folders/1idH251a2K-hGGd3pKjX-3Gx5o_rUqLC4?usp=sharing

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [114] [Request-Only Optimization for Recommendation Systems](https://arxiv.org/abs/2508.05640)
*Liang Guo,Wei Li,Lucy Liao,Huihui Cheng,Rui Zhang,Yu Shi,Yueming Wang,Yanzun Huang,Keke Zhai,Pengchao Wang,Timothy Shi,Xuan Cao,Shengzhi Wang,Renqin Cai,Zhaojie Gong,Omkar Vichare,Rui Jian,Leon Gao,Shiyan Deng,Xingyu Liu,Xiong Zhang,Fu Li,Wenlei Xie,Bin Wen,Rui Li,Xing Liu,Jiaqi Zhai*

Main category: cs.IR

TL;DR: The paper introduces ROO, a new training paradigm for DLRMs that improves storage, training efficiency, and model quality by treating a user request as the unit of training data.


<details>
  <summary>Details</summary>
Motivation: Industry-scale DLRMs require new storage and training algorithms to efficiently improve the quality of complex recommendation systems due to their scale and the huge amount of training data.

Method: The paper co-designs data (request-only data), infrastructure (request-only based data processing pipeline), and model architecture (request-only neural architectures).

Result: The ROO training and modeling paradigm achieves native feature deduplication in data logging, saving data storage. It also enables highly scaled-up neural network architectures to better capture user interest signals.

Conclusion: The paper presents a Request-Only Optimizations (ROO) training and modeling paradigm that improves storage, training efficiency, and model quality.

Abstract: Deep Learning Recommendation Models (DLRMs) represent one of the largest
machine learning applications on the planet. Industry-scale DLRMs are trained
with petabytes of recommendation data to serve billions of users every day. To
utilize the rich user signals in the long user history, DLRMs have been scaled
up to unprecedented complexity, up to trillions of floating-point operations
(TFLOPs) per example. This scale, coupled with the huge amount of training
data, necessitates new storage and training algorithms to efficiently improve
the quality of these complex recommendation systems. In this paper, we present
a Request-Only Optimizations (ROO) training and modeling paradigm. ROO
simultaneously improves the storage and training efficiency as well as the
model quality of recommendation systems. We holistically approach this
challenge through co-designing data (i.e., request-only data), infrastructure
(i.e., request-only based data processing pipeline), and model architecture
(i.e., request-only neural architectures). Our ROO training and modeling
paradigm treats a user request as a unit of the training data. Compared with
the established practice of treating a user impression as a unit, our new
design achieves native feature deduplication in data logging, consequently
saving data storage. Second, by de-duplicating computations and communications
across multiple impressions in a request, this new paradigm enables highly
scaled-up neural network architectures to better capture user interest signals,
such as Generative Recommenders (GRs) and other request-only friendly
architectures.

</details>


### [115] [Query-Aware Graph Neural Networks for Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05647)
*Vibhor Agrawal,Fay Wang,Rishi Puri*

Main category: cs.IR

TL;DR: A new GNN-based RAG model improves retrieval accuracy on complex questions by using query-aware attention and knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: Traditional dense retrieval methods treat documents as independent entities, which limits retrieval accuracy on complex, multi-hop questions.

Method: A novel graph neural network (GNN) architecture for RAG is introduced, featuring query-aware attention mechanisms and learned scoring heads. It constructs per-episode knowledge graphs to capture sequential and semantic relationships between text chunks and uses an Enhanced Graph Attention Network with query-guided pooling.

Result: The proposed approach significantly outperforms standard dense retrievers on complex question answering tasks.

Conclusion: The proposed GNN-based RAG model significantly outperforms standard dense retrievers on complex question answering tasks, especially those requiring multi-document reasoning. The implementation leverages PyTorch Geometric for efficient processing and scalable deployment.

Abstract: We present a novel graph neural network (GNN) architecture for
retrieval-augmented generation (RAG) that leverages query-aware attention
mechanisms and learned scoring heads to improve retrieval accuracy on complex,
multi-hop questions. Unlike traditional dense retrieval methods that treat
documents as independent entities, our approach constructs per-episode
knowledge graphs that capture both sequential and semantic relationships
between text chunks. We introduce an Enhanced Graph Attention Network with
query-guided pooling that dynamically focuses on relevant parts of the graph
based on user queries. Experimental results demonstrate that our approach
significantly outperforms standard dense retrievers on complex question
answering tasks, particularly for questions requiring multi-document reasoning.
Our implementation leverages PyTorch Geometric for efficient processing of
graph-structured data, enabling scalable deployment in production retrieval
systems

</details>


### [116] [AquiLLM: a RAG Tool for Capturing Tacit Knowledge in Research Groups](https://arxiv.org/abs/2508.05648)
*Chandler Campbell,Bernie Boscoe,Tuan Do*

Main category: cs.IR

TL;DR: The paper introduces AquiLLM, a RAG system for research groups that addresses privacy concerns and supports various document types to improve access to both formal and informal knowledge.


<details>
  <summary>Details</summary>
Motivation: Research groups face persistent challenges in capturing, storing, and retrieving knowledge that is distributed across team members, especially tacit knowledge. Current RAG-LLM systems overlook the privacy concerns of internal research materials.

Method: The paper introduces AquiLLM, a lightweight, modular RAG system.

Result: AquiLLM supports varied document types and configurable privacy settings, enabling more effective access to both formal and informal knowledge within scholarly groups.

Conclusion: AquiLLM, a lightweight, modular RAG system, is introduced to meet the needs of research groups by supporting varied document types and configurable privacy settings, enabling more effective access to both formal and informal knowledge within scholarly groups.

Abstract: Research groups face persistent challenges in capturing, storing, and
retrieving knowledge that is distributed across team members. Although
structured data intended for analysis and publication is often well managed,
much of a group's collective knowledge remains informal, fragmented, or
undocumented--often passed down orally through meetings, mentoring, and
day-to-day collaboration. This includes private resources such as emails,
meeting notes, training materials, and ad hoc documentation. Together, these
reflect the group's tacit knowledge--the informal, experience-based expertise
that underlies much of their work. Accessing this knowledge can be difficult,
requiring significant time and insider understanding. Retrieval-augmented
generation (RAG) systems offer promising solutions by enabling users to query
and generate responses grounded in relevant source material. However, most
current RAG-LLM systems are oriented toward public documents and overlook the
privacy concerns of internal research materials. We introduce AquiLLM
(pronounced ah-quill-em), a lightweight, modular RAG system designed to meet
the needs of research groups. AquiLLM supports varied document types and
configurable privacy settings, enabling more effective access to both formal
and informal knowledge within scholarly groups.

</details>


### [117] [AI Guided Accelerator For Search Experience](https://arxiv.org/abs/2508.05649)
*Jayanth Yetukuri,Mehran Elyasi,Samarth Agrawal,Aritra Mandal,Rui Kong,Harish Vempati,Ishita Khan*

Main category: cs.IR

TL;DR: 提出了一种新的查询重构框架，该框架通过对用户会话中的过渡查询进行建模，并利用大型语言模型生成语义多样且保留意图的替代查询，从而提升电商环境中的搜索效果。


<details>
  <summary>Details</summary>
Motivation: 有效的查询重构对于缩小用户的探索性搜索行为与电子商务环境中相关产品的识别之间的差距至关重要。传统的方案主要将查询重写建模为孤立的对，但它们通常无法捕捉到实际用户行为中固有的顺序和过渡动态。

Method: 提出了一个新颖的框架，该框架明确地模拟了过渡查询——在用户实现最终购买意图的过程中发生的中间调整。

Result: 对过渡查询进行了正式的识别和建模；为意图流理解引入了结构化查询序列挖掘管道；应用 LLM 实现了可扩展的、具有意图意识的查询扩展。

Conclusion: 通过在实际电商环境中与现有相关搜索模块相比，转化率和参与度指标实现了可衡量的提升，从而验证了该方法在实际电商环境中的有效性。

Abstract: Effective query reformulation is pivotal in narrowing the gap between a
user's exploratory search behavior and the identification of relevant products
in e-commerce environments. While traditional approaches predominantly model
query rewrites as isolated pairs, they often fail to capture the sequential and
transitional dynamics inherent in real-world user behavior. In this work, we
propose a novel framework that explicitly models transitional
queries--intermediate reformulations occurring during the user's journey toward
their final purchase intent. By mining structured query trajectories from
eBay's large-scale user interaction logs, we reconstruct query sequences that
reflect shifts in intent while preserving semantic coherence. This approach
allows us to model a user's shopping funnel, where mid-journey transitions
reflect exploratory behavior and intent refinement. Furthermore, we incorporate
generative Large Language Models (LLMs) to produce semantically diverse and
intent-preserving alternative queries, extending beyond what can be derived
through collaborative filtering alone. These reformulations can be leveraged to
populate Related Searches or to power intent-clustered carousels on the search
results page, enhancing both discovery and engagement. Our contributions
include (i) the formal identification and modeling of transitional queries,
(ii) the introduction of a structured query sequence mining pipeline for intent
flow understanding, and (iii) the application of LLMs for scalable,
intent-aware query expansion. Empirical evaluation demonstrates measurable
gains in conversion and engagement metrics compared to the existing Related
Searches module, validating the effectiveness of our approach in real-world
e-commerce settings.

</details>


### [118] [OmniBench-RAG: A Multi-Domain Evaluation Platform for Retrieval-Augmented Generation Tools](https://arxiv.org/abs/2508.05650)
*Jiaxuan Liang,Shide Zhou,Kailong Wang*

Main category: cs.IR

TL;DR: OmniBench RAG是一个用于多领域评估RAG系统的平台，揭示了RAG在不同领域效果的差异性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法缺乏领域覆盖、采用粗糙指标且无法捕捉计算权衡，并且缺乏比较不同模型和领域RAG有效性的标准化框架。

Method: 引入了OmniBench RAG，一个用于RAG系统多领域评估的自动化平台，并提出了两个标准化指标：改进（准确率提升）和转换（RAG前后模型的效率差异）。

Result: 评估结果显示，RAG在文化领域有显著提升，但在数学领域有所下降。

Conclusion: RAG效果在不同领域表现出显著差异，需要进行系统性的、领域感知的评估。

Abstract: While Retrieval Augmented Generation (RAG) is now widely adopted to enhance
LLMs, evaluating its true performance benefits in a reproducible and
interpretable way remains a major hurdle. Existing methods often fall short:
they lack domain coverage, employ coarse metrics that miss sub document
precision, and fail to capture computational trade offs. Most critically, they
provide no standardized framework for comparing RAG effectiveness across
different models and domains.
  We introduce OmniBench RAG, a novel automated platform for multi domain
evaluation of RAG systems. The platform quantifies performance gains across
accuracy and efficiency dimensions, spanning nine knowledge fields including
culture, geography, and health. We introduce two standardized metrics:
Improvements (accuracy gains) and Transformation (efficiency differences
between pre RAG and post RAG models), enabling reproducible comparisons across
models and tasks. The platform features dynamic test generation, modular
evaluation pipelines, and automated knowledge base construction. Our evaluation
reveals striking variability in RAG effectiveness, from significant gains in
culture to declines in mathematics, highlighting the critical importance of
systematic, domain aware assessment. A demonstration video is available at:
https://www.youtube.com/watch?v=BZx83QFcTCI. Code and datasets:
https://github.com/Garnett-Liang/Omnibench-RAG.

</details>


### [119] [Lessons from A Large Language Model-based Outdoor Trail Recommendation Chatbot with Retrieval Augmented Generation](https://arxiv.org/abs/2508.05652)
*Julia Ann Mathew,Suining He*

Main category: cs.IR

TL;DR: This paper discusses the preliminary and practical lessons learned from developing Judy, an outdoor trail recommendation chatbot based on the large language model (LLM) with retrieval augmented generation (RAG).


<details>
  <summary>Details</summary>
Motivation: The increasing popularity of outdoor recreational activities has boosted the demand for a conversational AI system to provide informative and personalized suggestion on outdoor trails. Challenges arise in response to how to provide accurate outdoor trail information via conversational AI and how to enable usable and efficient recommendation services.

Method: developing Judy, an outdoor trail recommendation chatbot based on the large language model (LLM) with retrieval augmented generation (RAG).

Result: accuracy, effectiveness, and usability of Judy in recommending outdoor trails based on the LLM with RAG.

Conclusion: The experimental results have demonstrated the accuracy, effectiveness, and usability of Judy in recommending outdoor trails based on the LLM with RAG.

Abstract: The increasing popularity of outdoor recreational activities (such as hiking
and biking) has boosted the demand for a conversational AI system to provide
informative and personalized suggestion on outdoor trails. Challenges arise in
response to (1) how to provide accurate outdoor trail information via
conversational AI; and (2) how to enable usable and efficient recommendation
services. To address above, this paper discusses the preliminary and practical
lessons learned from developing Judy, an outdoor trail recommendation chatbot
based on the large language model (LLM) with retrieval augmented generation
(RAG). To gain concrete system insights, we have performed case studies with
the outdoor trails in Connecticut (CT), US. We have conducted web-based data
collection, outdoor trail data management, and LLM model performance studies on
the RAG-based recommendation. Our experimental results have demonstrated the
accuracy, effectiveness, and usability of Judy in recommending outdoor trails
based on the LLM with RAG.

</details>


### [120] [Comparison of Information Retrieval Techniques Applied to IT Support Tickets](https://arxiv.org/abs/2508.05654)
*Leonardo Santiago Benitez Pereira,Robinson Pizzio,Samir Bonho*

Main category: cs.IR

TL;DR: This paper compares Information Retrieval techniques for IT support tickets, finding Sentence-BERT to be the most effective. It also provides a prototype system and a new evaluation metric.


<details>
  <summary>Details</summary>
Motivation: IT help desk systems are crucial for institutions dependent on IT services, and Machine Learning models can improve their performance, but each model performs differently on different datasets.

Method: Compared eleven Information Retrieval techniques on an IT support ticket dataset.

Result: Sentence-BERT achieved the best results (78.7% relevant recommendations), followed by TF-IDF (69.0%), Word2vec (68.7%), and LDA (66.3%). Datasets and code are open source. A prototype was implemented.

Conclusion: Sentence-BERT is the best technique for IT support ticket retrieval, achieving 78.7% relevant recommendations. A prototype system was implemented and a novel metric for evaluating retrieval quality was proposed.

Abstract: Institutions dependent on IT services and resources acknowledge the crucial
significance of an IT help desk system, that act as a centralized hub
connecting IT staff and users for service requests. Employing various Machine
Learning models, these IT help desk systems allow access to corrective actions
used in the past, but each model has different performance when applied to
different datasets. This work compares eleven Information Retrieval techniques
in a dataset of IT support tickets, with the goal of implementing a software
that facilitates the work of Information Technology support analysts. The best
results were obtained with the Sentence-BERT technique, in its multi-language
variation distilluse-base-multilingual-cased-v1, where 78.7% of the
recommendations made by the model were considered relevant. TF-IDF (69.0%),
Word2vec (68.7%) and LDA (66.3%) techniques also had consistent results.
Furthermore, the used datasets and essential parts of coding have been
published and made open source. It also demonstrated the practicality of a
support ticket recovery system by implementing a minimal viable prototype, and
described in detail the implementation of the system. Finally, this work
proposed a novel metric for comparing the techniques, whose aim is to closely
reflect the perception of the IT analysts about the retrieval quality.

</details>


### [121] [Beyond Single Labels: Improving Conversational Recommendation through LLM-Powered Data Augmentation](https://arxiv.org/abs/2508.05657)
*Haozhe Xu,Xiaohua Wang,Changze Lv,Xiaoqing Zheng*

Main category: cs.IR

TL;DR: This paper introduces a data augmentation framework to improve conversational recommender systems by addressing the false negative issue, leading to better performance.


<details>
  <summary>Details</summary>
Motivation: CRSs face the false negative issue, where items a user might like are incorrectly labeled as negative, leading to suboptimal recommendations. Expanding the label set through data augmentation is challenging due to the need to balance semantic relevance and collaborative information.

Method: A novel data augmentation framework with LLM-based semantic retriever and relevance scorer, combined with a two-stage training strategy.

Result: Significant and consistent performance improvements on two benchmark datasets and user simulators.

Conclusion: The proposed data augmentation framework improves CRS performance by addressing the false negative issue. Experiments show significant and consistent performance improvements.

Abstract: Conversational recommender systems (CRSs) enhance recommendation quality by
engaging users in multi-turn dialogues, capturing nuanced preferences through
natural language interactions. However, these systems often face the false
negative issue, where items that a user might like are incorrectly labeled as
negative during training, leading to suboptimal recommendations.Expanding the
label set through data augmentation presents an intuitive solution but faces
the challenge of balancing two key aspects: ensuring semantic relevance and
preserving the collaborative information inherent in CRS datasets. To address
these issues, we propose a novel data augmentation framework that first
leverages an LLM-based semantic retriever to identify diverse and semantically
relevant items, which are then filtered by a relevance scorer to remove noisy
candidates. Building on this, we introduce a two-stage training strategy
balancing semantic relevance and collaborative information. Extensive
experiments on two benchmark datasets and user simulators demonstrate
significant and consistent performance improvements across various
recommenders, highlighting the effectiveness of our approach in advancing CRS
performance.

</details>


### [122] [Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review](https://arxiv.org/abs/2508.05660)
*Aditya Nagori,Ricardo Accorsi Casonatto,Ayush Gautam,Abhinav Manikantha Sai Cheruvu,Rishikesan Kamaleswaran*

Main category: cs.IR

TL;DR: This paper presents an agentic hybrid RAG system for scientific literature analysis that dynamically selects retrieval methods, adapts generation, and quantifies uncertainty, outperforming baselines on synthetic benchmarks.


<details>
  <summary>Details</summary>
Motivation: The surge in scientific publications challenges traditional review methods, demanding tools that integrate structured metadata with full-text analysis. Hybrid Retrieval Augmented Generation (RAG) systems offer promise but are typically static, rely on proprietary tools, and lack uncertainty estimates.

Method: An agentic approach that encapsulates the hybrid RAG pipeline within an autonomous agent capable of dynamically selecting between GraphRAG and VectorRAG, adapting instruction-tuned generation in real time, and quantifying uncertainty during inference. The pipeline ingests bibliometric open-access data, builds a Neo4j citation-based knowledge graph, and embeds full-text PDFs into a FAISS vector store. A Llama-3.3-70B agent selects GraphRAG or VectorRAG. Instruction tuning refines domain-specific generation, and bootstrapped evaluation yields standard deviation for evaluation metrics.

Result: The Instruction-Tuned Agent with DPO outperforms the baseline, achieving a gain of 0.63 in VS Context Recall and a 0.56 gain in overall Context Precision. Additional gains include 0.24 in VS Faithfulness, 0.12 in both VS Precision and KG Answer Relevance, 0.11 in overall Faithfulness score, 0.05 in KG Context Recall, and 0.04 in both VS Answer Relevance and overall Precision.

Conclusion: The Instruction-Tuned Agent with DPO outperforms the baseline, achieving gains in VS Context Recall, overall Context Precision, VS Faithfulness, VS Precision, KG Answer Relevance, overall Faithfulness score, KG Context Recall, VS Answer Relevance, and overall Precision. These results highlight the system's improved reasoning over heterogeneous sources and establish a scalable framework for autonomous, agentic scientific discovery.

Abstract: The surge in scientific publications challenges traditional review methods,
demanding tools that integrate structured metadata with full-text analysis.
Hybrid Retrieval Augmented Generation (RAG) systems, combining graph queries
with vector search offer promise but are typically static, rely on proprietary
tools, and lack uncertainty estimates. We present an agentic approach that
encapsulates the hybrid RAG pipeline within an autonomous agent capable of (1)
dynamically selecting between GraphRAG and VectorRAG for each query, (2)
adapting instruction-tuned generation in real time to researcher needs, and (3)
quantifying uncertainty during inference. This dynamic orchestration improves
relevance, reduces hallucinations, and promotes reproducibility.
  Our pipeline ingests bibliometric open-access data from PubMed, arXiv, and
Google Scholar APIs, builds a Neo4j citation-based knowledge graph (KG), and
embeds full-text PDFs into a FAISS vector store (VS) using the all-MiniLM-L6-v2
model. A Llama-3.3-70B agent selects GraphRAG (translating queries to Cypher
for KG) or VectorRAG (combining sparse and dense retrieval with re-ranking).
Instruction tuning refines domain-specific generation, and bootstrapped
evaluation yields standard deviation for evaluation metrics.
  On synthetic benchmarks mimicking real-world queries, the Instruction-Tuned
Agent with Direct Preference Optimization (DPO) outperforms the baseline,
achieving a gain of 0.63 in VS Context Recall and a 0.56 gain in overall
Context Precision. Additional gains include 0.24 in VS Faithfulness, 0.12 in
both VS Precision and KG Answer Relevance, 0.11 in overall Faithfulness score,
0.05 in KG Context Recall, and 0.04 in both VS Answer Relevance and overall
Precision. These results highlight the system's improved reasoning over
heterogeneous sources and establish a scalable framework for autonomous,
agentic scientific discovery.

</details>


### [123] [Zero-Shot Retrieval for Scalable Visual Search in a Two-Sided Marketplace](https://arxiv.org/abs/2508.05661)
*Andre Rusli,Shoma Ishimoto,Sho Akiyama,Aman Kumar Singh*

Main category: cs.IR

TL;DR: 在 Mercari 的 C2C 市场中部署了一个可扩展的视觉搜索系统，该系统利用最新的 zero-shot 模型，在参与度和转化率方面取得了显着提升。


<details>
  <summary>Details</summary>
Motivation: 视觉搜索为客户提供了一种直观的方式来探索多样化的产品目录，特别是在消费者对消费者 (C2C) 市场中，这些市场中的商品列表通常是非结构化的并且是视觉驱动的。

Method: 评估用于零样本图像检索的最新视觉语言模型，并将它们的性能与现有的微调基线进行比较。该系统集成了实时推理和后台索引工作流程，并由通过降维优化的统一嵌入管道提供支持。

Result: 使用用户交互日志的离线评估表明，多语言 SigLIP 模型在多个检索指标上优于其他模型，与基线相比，nDCG@5 提高了 13.3%。在生产环境中进行为期一周的在线 A/B 测试进一步证实了实际影响，实验组在参与度和转化率方面均实现了显着增长，通过图像搜索实现的交易率提高了 40.9%。

Conclusion: 最近的zero-shot模型可以作为生产使用的强大而实用的基线，这使团队能够以最小的开销部署有效的视觉搜索系统，同时保留基于未来数据或领域特定需求进行微调的灵活性。

Abstract: Visual search offers an intuitive way for customers to explore diverse
product catalogs, particularly in consumer-to-consumer (C2C) marketplaces where
listings are often unstructured and visually driven. This paper presents a
scalable visual search system deployed in Mercari's C2C marketplace, where
end-users act as buyers and sellers. We evaluate recent vision-language models
for zero-shot image retrieval and compare their performance with an existing
fine-tuned baseline. The system integrates real-time inference and background
indexing workflows, supported by a unified embedding pipeline optimized through
dimensionality reduction. Offline evaluation using user interaction logs shows
that the multilingual SigLIP model outperforms other models across multiple
retrieval metrics, achieving a 13.3% increase in nDCG@5 over the baseline. A
one-week online A/B test in production further confirms real-world impact, with
the treatment group showing substantial gains in engagement and conversion, up
to a 40.9% increase in transaction rate via image search. Our findings
highlight that recent zero-shot models can serve as a strong and practical
baseline for production use, which enables teams to deploy effective visual
search systems with minimal overhead, while retaining the flexibility to
fine-tune based on future data or domain-specific needs.

</details>


### [124] [From Static to Dynamic: A Streaming RAG Approach to Real-time Knowledge Base](https://arxiv.org/abs/2508.05662)
*Yuzhou Zhu*

Main category: cs.IR

TL;DR: Streaming RAG combines multi-vector cosine screening, mini-batch clustering, and a counter-based heavy-hitter filter to maintain a compact prototype set, achieving better performance in real-time streams.


<details>
  <summary>Details</summary>
Motivation: Dynamic streams from news feeds, social media, sensor networks, and financial markets challenge static RAG frameworks. Full-scale indices incur high memory costs; periodic rebuilds introduce latency that undermines data freshness; naive sampling sacrifices semantic coverage.

Method: a unified pipeline that combines multi-vector cosine screening, mini-batch clustering, and a counter-based heavy-hitter filter to maintain a compact prototype set. An incremental index upsert mechanism refreshes prototypes without interrupting queries.

Result: statistically significant gains in Recall@10 (up to 3 points, p < 0.01), end-to-end latency below 15 ms, and throughput above 900 documents per second under a 150 MB budget. 3.2-point gain in Exact Match and 2.8-point gain in F1 on SQuAD; abstractive summarization yields ROUGE-L improvements.

Conclusion: Streaming RAG establishes a new Pareto frontier for retrieval augmentation.

Abstract: Dynamic streams from news feeds, social media, sensor networks, and financial
markets challenge static RAG frameworks. Full-scale indices incur high memory
costs; periodic rebuilds introduce latency that undermines data freshness;
naive sampling sacrifices semantic coverage. We present Streaming RAG, a
unified pipeline that combines multi-vector cosine screening, mini-batch
clustering, and a counter-based heavy-hitter filter to maintain a compact
prototype set. We further prove an approximation bound \$E\[R(K\_t)] \ge R^\* -
L \Delta\$ linking retrieval quality to clustering variance. An incremental
index upsert mechanism refreshes prototypes without interrupting queries.
Experiments on eight real-time streams show statistically significant gains in
Recall\@10 (up to 3 points, p < 0.01), end-to-end latency below 15 ms, and
throughput above 900 documents per second under a 150 MB budget. Hyperparameter
sensitivity analysis over cluster count, admission probability, relevance
threshold, and counter capacity validates default settings. In open-domain
question answering with GPT-3.5 Turbo, we record 3.2-point gain in Exact Match
and 2.8-point gain in F1 on SQuAD; abstractive summarization yields ROUGE-L
improvements. Streaming RAG establishes a new Pareto frontier for retrieval
augmentation.

</details>


### [125] [Enhancing Retrieval-Augmented Generation for Electric Power Industry Customer Support](https://arxiv.org/abs/2508.05664)
*Hei Yu Chan,Kuok Tou Ho,Chenglong Ma,Yujing Si,Hok Lai Lin,Sa Lei Lam*

Main category: cs.IR

TL;DR: 本研究评估了多种技术，构建了一个强大的电力领域客户支持系统。


<details>
  <summary>Details</summary>
Motivation: 现有的AI客户服务系统在处理模糊、多意图或细节特定的查询时存在不足。

Method: 对比了向量存储和基于图的RAG框架，最终选择了基于图的RAG。

Result: 查询重写改善了检索，RAG Fusion提升了模糊或多方面查询的性能，重排序减少了幻觉，意图识别提高了相关性和效率，关键词扩充 негативно 影响了结果。

Conclusion: 集成了意图识别、RAG Fusion和重排序的系统，在GPT-4生成的数据集和真实电力供应商FAQ数据集上分别实现了97.9%和89.6%的准确率，显著优于基线RAG模型。

Abstract: Many AI customer service systems use standard NLP pipelines or finetuned
language models, which often fall short on ambiguous, multi-intent, or
detail-specific queries. This case study evaluates recent techniques: query
rewriting, RAG Fusion, keyword augmentation, intent recognition, and context
reranking, for building a robust customer support system in the electric power
domain. We compare vector-store and graph-based RAG frameworks, ultimately
selecting the graph-based RAG for its superior performance in handling complex
queries. We find that query rewriting improves retrieval for queries using
non-standard terminology or requiring precise detail. RAG Fusion boosts
performance on vague or multifaceted queries by merging multiple retrievals.
Reranking reduces hallucinations by filtering irrelevant contexts. Intent
recognition supports the decomposition of complex questions into more targeted
sub-queries, increasing both relevance and efficiency. In contrast, keyword
augmentation negatively impacts results due to biased keyword selection. Our
final system combines intent recognition, RAG Fusion, and reranking to handle
disambiguation and multi-source queries. Evaluated on both a GPT-4-generated
dataset and a real-world electricity provider FAQ dataset, it achieves 97.9%
and 89.6% accuracy respectively, substantially outperforming baseline RAG
models.

</details>


### [126] [HySemRAG: A Hybrid Semantic Retrieval-Augmented Generation Framework for Automated Literature Synthesis and Methodological Gap Analysis](https://arxiv.org/abs/2508.05666)
*Alejandro Godinez*

Main category: cs.IR

TL;DR: HySemRAG combines ETL pipelines with RAG to automate literature synthesis and identify methodological research gaps, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing RAG architectures have limitations.

Method: The system processes scholarly literature through eight integrated stages: multi-source metadata acquisition, asynchronous PDF retrieval, custom document layout analysis, bibliographic management, LLM-based field extraction, topic modeling, semantic unification, and knowledge graph construction. It uses hybrid retrieval, agentic self-correction, and post-hoc citation verification.

Result: Structured field extraction achieves 35.1% higher semantic similarity scores (0.655) compared to PDF chunking approaches (0.485). The agentic quality assurance mechanism achieves 68.3% single-pass success rates with 99.0% citation accuracy.

Conclusion: The HySemRAG system identifies methodological trends and research gaps in geospatial epidemiology literature, demonstrating broad applicability across scientific domains for accelerating evidence synthesis and discovery.

Abstract: We present HySemRAG, a framework that combines Extract, Transform, Load (ETL)
pipelines with Retrieval-Augmented Generation (RAG) to automate large-scale
literature synthesis and identify methodological research gaps. The system
addresses limitations in existing RAG architectures through a multi-layered
approach: hybrid retrieval combining semantic search, keyword filtering, and
knowledge graph traversal; an agentic self-correction framework with iterative
quality assurance; and post-hoc citation verification ensuring complete
traceability. Our implementation processes scholarly literature through eight
integrated stages: multi-source metadata acquisition, asynchronous PDF
retrieval, custom document layout analysis using modified Docling architecture,
bibliographic management, LLM-based field extraction, topic modeling, semantic
unification, and knowledge graph construction. The system creates dual data
products - a Neo4j knowledge graph enabling complex relationship queries and
Qdrant vector collections supporting semantic search - serving as foundational
infrastructure for verifiable information synthesis. Evaluation across 643
observations from 60 testing sessions demonstrates structured field extraction
achieving 35.1% higher semantic similarity scores (0.655 $\pm$ 0.178) compared
to PDF chunking approaches (0.485 $\pm$ 0.204, p < 0.000001). The agentic
quality assurance mechanism achieves 68.3% single-pass success rates with 99.0%
citation accuracy in validated responses. Applied to geospatial epidemiology
literature on ozone exposure and cardiovascular disease, the system identifies
methodological trends and research gaps, demonstrating broad applicability
across scientific domains for accelerating evidence synthesis and discovery.

</details>


### [127] [ITDR: An Instruction Tuning Dataset for Enhancing Large Language Models in Recommendations](https://arxiv.org/abs/2508.05667)
*Zekun Liu,Xiaowen Huang,Jitao Sang*

Main category: cs.IR

TL;DR: This paper introduces ITDR, a new instruction tuning dataset for recommendation systems, which improves the performance of open-source LLMs on these tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle to effectively model the associations between user preferences and items. Although prompt-based methods can generate recommendation results, their inadequate understanding of recommendation tasks leads to constrained performance.

Method: construct a sufficient instruction tuning dataset, ITDR, which encompasses 7 subtasks across two core root tasks--user-item interaction and user-item understanding. The dataset integrates data from 13 public recommendation datasets and is built using manually crafted standardized templates, comprising approximately 200,000 instances.

Result: analyze the correlations between tasks and explore the impact of task descriptions and data scale on instruction tuning effectiveness. Finally, we perform comparative experiments against closed-source LLMs with substantial parameters.

Conclusion: ITDR significantly enhances the performance of mainstream open-source LLMs such as GLM-4, Qwen2.5, Qwen2.5-Instruct and LLaMA-3.2 on recommendation tasks.

Abstract: Large language models (LLMs) have demonstrated outstanding performance in
natural language processing tasks. However, in the field of recommendation
systems, due to the structural differences between user behavior data and
natural language, LLMs struggle to effectively model the associations between
user preferences and items. Although prompt-based methods can generate
recommendation results, their inadequate understanding of recommendation tasks
leads to constrained performance. To address this gap, in this work, we
construct a sufficient instruction tuning dataset, ITDR, which encompasses 7
subtasks across two core root tasks--user-item interaction and user-item
understanding. The dataset integrates data from 13 public recommendation
datasets and is built using manually crafted standardized templates, comprising
approximately 200,000 instances. Experimental results demonstrate that ITDR
significantly enhances the performance of mainstream open-source LLMs such as
GLM-4, Qwen2.5, Qwen2.5-Instruct and LLaMA-3.2 on recommendation tasks.
Furthermore, we analyze the correlations between tasks and explore the impact
of task descriptions and data scale on instruction tuning effectiveness.
Finally, we perform comparative experiments against closed-source LLMs with
substantial parameters. Our tuning dataset ITDR and the fine-tuned large
recommendation models can be accessed at https://github.com/hellolzk/ITDR.

</details>


### [128] [A Survey of LLM-based Deep Search Agents: Paradigm, Optimization, Evaluation, and Challenges](https://arxiv.org/abs/2508.05668)
*Yunjia Xi,Jianghao Lin,Yongzhao Xiao,Zheli Zhou,Rong Shan,Te Gao,Jiachen Zhu,Weiwen Liu,Yong Yu,Weinan Zhang*

Main category: cs.IR

TL;DR: 本文对基于LLM的搜索代理进行了首次系统分析，总结了架构、优化、应用和评估，并提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 的出现显着地改变了网络搜索。基于 LLM 的搜索代理的出现标志着向更深入、动态、自主的信息寻求的关键转变。

Method: 系统分析

Result: 对搜索代理进行了首次系统分析。确定了关键的开放挑战，并概述了未来有希望的研究方向。

Conclusion: 这篇调查全面分析并分类了现有研究，从架构、优化、应用和评估的角度出发，最终确定了关键的开放挑战，并概述了快速发展的领域中未来有希望的研究方向。

Abstract: The advent of Large Language Models (LLMs) has significantly revolutionized
web search. The emergence of LLM-based Search Agents marks a pivotal shift
towards deeper, dynamic, autonomous information seeking. These agents can
comprehend user intentions and environmental context and execute multi-turn
retrieval with dynamic planning, extending search capabilities far beyond the
web. Leading examples like OpenAI's Deep Research highlight their potential for
deep information mining and real-world applications. This survey provides the
first systematic analysis of search agents. We comprehensively analyze and
categorize existing works from the perspectives of architecture, optimization,
application, and evaluation, ultimately identifying critical open challenges
and outlining promising future research directions in this rapidly evolving
field. Our repository is available on
https://github.com/YunjiaXi/Awesome-Search-Agent-Papers.

</details>


### [129] [Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports](https://arxiv.org/abs/2508.05669)
*Jin Khye Tan,En Jun Choong,Ethan Jeremiah Chitty,Yan Pheng Choo,John Hsin Yang Wong,Chern Eu Cheah*

Main category: cs.IR

TL;DR: Fine-tuned VLM accurately extracts financial tables, outperforming larger models with less computational cost.


<details>
  <summary>Details</summary>
Motivation: Extracting tabular data from financial documents is challenging but critical for regulatory and analytical use cases.

Method: Fine-tuned vision-language model (VLM) based on Qwen2.5-VL-7B with LoRA, trained on a curated dataset of 2,152 image-text pairs.

Result: Achieved 92.20% accuracy on criteria-based assessment and 96.53% Markdown TEDS score, surpassing base model, larger VLMs, and proprietary models like GPT-4o and Gemini 2.5 Flash, with reduced inference time.

Conclusion: Domain-specific fine-tuning bridges the gap between unstructured financial documents and downstream automation, rivalling larger, general models in accuracy and efficiency.

Abstract: Accurately extracting and representing the structure of tabular data from
financial documents remains a critical challenge in document understanding,
particularly for regulatory and analytical use cases. This study addresses the
complexity of converting financial tables from Malaysian audited financial
reports into Markdown format, a task complicated by rotated layouts,
multi-level headers, and implicit structural cues. We propose a fine-tuned
vision-language model (VLM), based on Qwen2.5-VL-7B, optimized for
high-fidelity Markdown generation from document images. Our approach includes a
curated dataset of 2,152 image-text pairs with augmentations and a supervised
fine-tuning strategy using LoRA. To assess performance, we evaluated our model
on 100 out-of-sample tables using a dual framework: a criteria-based
LLM-as-a-judge for fine-grained accuracy and our novel Markdown
Tree-Edit-Distance-based Similarity (TEDS) metric for holistic structural
fidelity. Our model achieves a 92.20% overall accuracy on the criteria-based
assessment and a 96.53% Markdown TEDS score. This performance significantly
surpasses its Qwen2.5-VL-7B base model, larger-scale VLMs, and specialized
reasoning-enabled models. Compared to these self-hosted alternatives, it also
significantly reduces inference time. Furthermore, its accuracy exceeds that of
widely used proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash.
These results demonstrate that domain-specific fine-tuning provides an
effective and efficient method to bridge the gap between unstructured financial
documents and downstream automation, rivalling much larger and more general
models without their computational overhead.

</details>


### [130] [LMAR: Language Model Augmented Retriever for Domain-specific Knowledge Indexing](https://arxiv.org/abs/2508.05672)
*Yao Zhao,Yantian Ding,Zhiyue Zhang,Dapeng Yao,Yanxun Xu*

Main category: cs.IR

TL;DR: LMAR通过结合LLM引导的数据合成与对比嵌入适配来解决RAG系统在领域知识方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于预训练嵌入的性能下降以及基于大型语言模型 (LLM) 的检索器的高昂计算成本，检索增强生成 (RAG) 系统通常难以处理特定领域的知识。

Method: 结合了LLM引导的数据合成、对比嵌入适配和高效文本聚类。

Result: 在多个领域特定的基准数据集上的实验结果表明，LMAR 优于多个基线模型，同时保持适中的硬件要求和低延迟。

Conclusion: LMAR是一种实用且经济高效的领域特定可扩展适配解决方案。

Abstract: Retrieval Augmented Generation (RAG) systems often struggle with
domain-specific knowledge due to performance deterioration of pre-trained
embeddings and prohibitive computational costs of large language model
(LLM)-based retrievers. While fine-tuning data augmentation embedding models
offers a promising direction, its effectiveness is limited by the need for
high-quality training data and reliable chunking strategies that preserve
contextual integrity. We propose LMAR (Language Model Augmented Retriever), a
model-agnostic framework that addresses these challenges by combining
LLM-guided data synthesis with contrastive embedding adaptation and efficient
text clustering. LMAR consists of a two-stage pipeline: (1) Triplet sampling
and synthetic data augmentation, where LLMs act as both labeler and validator
to ensure high-fidelity supervision throughout the pipeline. Experimental
results across multiple domain-specific benchmark datasets demonstrate that
LMAR outperforms multiple baseline models, while maintaining moderate hardware
requirements and low latency. Its model-agnostic nature further enables
seamless integration with emerging RAG architectures and text embedding models,
ensuring continual improvements without redesigning the pipeline. These results
highlight LMAR as a practical and cost-effective solution for scalable
domain-specific adaptation.

</details>


### [131] [Breaking the Top-$K$ Barrier: Advancing Top-$K$ Ranking Metrics Optimization in Recommender Systems](https://arxiv.org/abs/2508.05673)
*Weiqin Yang,Jiawei Chen,Shengjia Zhang,Peng Wu,Yuegang Sun,Yan Feng,Chun Chen,Can Wang*

Main category: cs.IR

TL;DR: 提出了一种新的推荐损失函数 SL@$K$，用于优化 NDCG@$K$，它优于现有的损失函数，并且具有理论保证、易于实现等优点。


<details>
  <summary>Details</summary>
Motivation: 在推荐系统 (RS) 领域，NDCG@$K$ 等 Top-$K$ 排名指标是评估推荐性能的黄金标准。然而，在推荐模型训练过程中，由于 NDCG@$K$ 固有的不连续性和复杂的 Top-$K$ 截断，优化 NDCG@$K$ 带来了重大挑战。最近优化 NDCG@$K$ 的努力要么忽略了 Top-$K$ 截断，要么遭受了高计算成本和训练不稳定性的困扰。

Method: 我们提出了 SoftmaxLoss@$K$ (SL@$K$)，这是一种专为 NDCG@$K$ 优化量身定制的新型推荐损失。

Result: 集成了分位数技术来处理 Top-$K$ 截断，并推导出用于优化 NDCG@$K$ 的平滑上限以解决不连续性。由此产生的 SL@$K$ 损失具有几个理想的属性，包括理论保证、易于实现、计算效率、梯度稳定性和噪声鲁棒性。

Conclusion: SL@$K$在四个真实世界数据集和三个推荐骨干网络上的大量实验表明，SL@$K$优于现有损失，平均提高了 6.03%。

Abstract: In the realm of recommender systems (RS), Top-$K$ ranking metrics such as
NDCG@$K$ are the gold standard for evaluating recommendation performance.
However, during the training of recommendation models, optimizing NDCG@$K$
poses significant challenges due to its inherent discontinuous nature and the
intricate Top-$K$ truncation. Recent efforts to optimize NDCG@$K$ have either
overlooked the Top-$K$ truncation or suffered from high computational costs and
training instability. To overcome these limitations, we propose SoftmaxLoss@$K$
(SL@$K$), a novel recommendation loss tailored for NDCG@$K$ optimization.
Specifically, we integrate the quantile technique to handle Top-$K$ truncation
and derive a smooth upper bound for optimizing NDCG@$K$ to address
discontinuity. The resulting SL@$K$ loss has several desirable properties,
including theoretical guarantees, ease of implementation, computational
efficiency, gradient stability, and noise robustness. Extensive experiments on
four real-world datasets and three recommendation backbones demonstrate that
SL@$K$ outperforms existing losses with a notable average improvement of 6.03%.
The code is available at https://github.com/Tiny-Snow/IR-Benchmark.

</details>


### [132] [Domain-Specific Fine-Tuning and Prompt-Based Learning: A Comparative Study for developing Natural Language-Based BIM Information Retrieval Systems](https://arxiv.org/abs/2508.05676)
*Han Gao,Timo Hartmann,Botao Zhong,Kai Lia,Hanbin Luo*

Main category: cs.IR

TL;DR: This paper compares fine-tuning and prompt-based learning for NLI-based BIM information retrieval, finding a hybrid approach most effective.


<details>
  <summary>Details</summary>
Motivation: Accurately extracting BIM-related data through natural language queries remains a persistent challenge due to the complexity use queries and specificity of domain knowledge.

Method: This study presents a comparative analysis of two prominent approaches for developing NLI-based BIM information retrieval systems: domain-specific fine-tuning and prompt-based learning using large language models (LLMs). A two-stage framework consisting of intent recognition and table-based question answering is implemented to evaluate the effectiveness of both approaches.

Result: Domain-specific fine-tuning delivers superior performance in intent recognition tasks, while prompt-based learning, particularly with GPT-4o, shows strength in table-based question answering.

Conclusion: This study identifies a hybrid configuration that combines fine-tuning for intent recognition with prompt-based learning for question answering, achieving more balanced and robust performance across tasks. The findings offer insights for researchers and practitioners in designing intelligent, language-driven BIM systems.

Abstract: Building Information Modeling (BIM) is essential for managing building data
across the entire lifecycle, supporting tasks from design to maintenance.
Natural Language Interface (NLI) systems are increasingly explored as
user-friendly tools for information retrieval in Building Information Modeling
(BIM) environments. Despite their potential, accurately extracting BIM-related
data through natural language queries remains a persistent challenge due to the
complexity use queries and specificity of domain knowledge. This study presents
a comparative analysis of two prominent approaches for developing NLI-based BIM
information retrieval systems: domain-specific fine-tuning and prompt-based
learning using large language models (LLMs). A two-stage framework consisting
of intent recognition and table-based question answering is implemented to
evaluate the effectiveness of both approaches. To support this evaluation, a
BIM-specific dataset of 1,740 annotated queries of varying types across 69
models is constructed. Experimental results show that domain-specific
fine-tuning delivers superior performance in intent recognition tasks, while
prompt-based learning, particularly with GPT-4o, shows strength in table-based
question answering. Based on these findings, this study identify a hybrid
configuration that combines fine-tuning for intent recognition with
prompt-based learning for question answering, achieving more balanced and
robust performance across tasks. This integrated approach is further tested
through case studies involving BIM models of varying complexity. This study
provides a systematic analysis of the strengths and limitations of each
approach and discusses the applicability of the NLI to real-world BIM
scenarios. The findings offer insights for researchers and practitioners in
designing intelligent, language-driven BIM systems.

</details>


### [133] [Are All Genders Equal in the Eyes of Algorithms? -- Analysing Search and Retrieval Algorithms for Algorithmic Gender Fairness](https://arxiv.org/abs/2508.05680)
*Stefanie Urchs,Veronika Thurner,Matthias Aßenmacher,Ludwig Bothmann,Christian Heumann,Stephanie Thiemichen*

Main category: cs.IR

TL;DR: 算法系统可能无意中引入性别偏见，这项研究分析了学术界数字可见性中的性别差异，发现男性教授的可见性通常高于女性教授。


<details>
  <summary>Details</summary>
Motivation: 搜索引擎和信息检索平台等算法系统会显著影响学术可见性和知识传播。尽管假设是中立的，但这些系统可能会重现或加强社会偏见，包括与性别相关的偏见。

Method: 使用来自德国大学和应用科学大学的学术概况的异构数据集，分析元数据完整性、学术数据库中的出版物检索以及谷歌搜索结果中的性别差异。

Result: 虽然我们没有观察到公开的算法歧视，但我们的研究结果揭示了微妙但一致的不平衡：男性教授与更多的搜索结果和更一致的出版记录相关联，而女性教授在数字可见性方面表现出更高的可变性。

Conclusion: 研究强调需要在数字系统中进行公平性评估，既要考虑技术性能，也要考虑代表性平等。

Abstract: Algorithmic systems such as search engines and information retrieval
platforms significantly influence academic visibility and the dissemination of
knowledge. Despite assumptions of neutrality, these systems can reproduce or
reinforce societal biases, including those related to gender. This paper
introduces and applies a bias-preserving definition of algorithmic gender
fairness, which assesses whether algorithmic outputs reflect real-world gender
distributions without introducing or amplifying disparities. Using a
heterogeneous dataset of academic profiles from German universities and
universities of applied sciences, we analyse gender differences in metadata
completeness, publication retrieval in academic databases, and visibility in
Google search results. While we observe no overt algorithmic discrimination,
our findings reveal subtle but consistent imbalances: male professors are
associated with a greater number of search results and more aligned publication
records, while female professors display higher variability in digital
visibility. These patterns reflect the interplay between platform algorithms,
institutional curation, and individual self-presentation. Our study highlights
the need for fairness evaluations that account for both technical performance
and representational equality in digital systems.

</details>


### [134] [LLM4ES: Learning User Embeddings from Event Sequences via Large Language Models](https://arxiv.org/abs/2508.05688)
*Aleksei Shestov,Omar Zoloev,Maksim Makarenko,Mikhail Orlov,Egor Fadeev,Ivan Kireev,Andrey Savchenko*

Main category: cs.IR

TL;DR: LLM4ES：一种利用LLM从事件序列中提取高质量用户嵌入的新框架，并在用户分类任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 利用大型预训练语言模型（LLM）从事件序列中提取用户嵌入。

Method: 利用大型预训练语言模型（LLM），通过next-token prediction微调LLM，从事件序列中提取用户嵌入。

Result: LLM4ES在金融等领域的用户分类任务中取得了state-of-the-art的表现。

Conclusion: LLM4ES在用户分类任务中表现出色，优于现有方法，可广泛应用于金融用户分群和医疗患者结果预测等领域。

Abstract: This paper presents LLM4ES, a novel framework that exploits large pre-trained
language models (LLMs) to derive user embeddings from event sequences. Event
sequences are transformed into a textual representation, which is subsequently
used to fine-tune an LLM through next-token prediction to generate high-quality
embeddings. We introduce a text enrichment technique that enhances LLM
adaptation to event sequence data, improving representation quality for
low-variability domains. Experimental results demonstrate that LLM4ES achieves
state-of-the-art performance in user classification tasks in financial and
other domains, outperforming existing embedding methods. The resulting user
embeddings can be incorporated into a wide range of applications, from user
segmentation in finance to patient outcome prediction in healthcare.

</details>


### [135] [Multi-Faceted Large Embedding Tables for Pinterest Ads Ranking](https://arxiv.org/abs/2508.05700)
*Runze Su,Jiayin Jin,Jiacheng Li,Sihan Wang,Guangtong Bai,Zelun Wang,Li Tang,Yixiong Meng,Huasen Wu,Zhimeng Pan,Kungang Li,Han Sun,Zhifang Liu,Haoyang Li,Siping Ji,Ling Leng,Prathibha Deshikachar*

Main category: cs.IR

TL;DR: This paper introduces a multi-faceted pretraining scheme and a CPU-GPU hybrid serving infrastructure to improve the performance and scalability of large embedding tables in Pinterest's ads ranking models.


<details>
  <summary>Details</summary>
Motivation: explore integrating large embedding tables into Pinterest's ads ranking models

Method: a novel multi-faceted pretraining scheme that incorporates multiple pretraining algorithms and a CPU-GPU hybrid serving infrastructure

Result: achieved 1.34% online CPC reduction and 2.60% CTR increase with neutral end-to-end latency change

Conclusion: multi-faceted large embedding tables bring great performance gain on both the Click-Through Rate (CTR) and Conversion Rate (CVR) domains

Abstract: Large embedding tables are indispensable in modern recommendation systems,
thanks to their ability to effectively capture and memorize intricate details
of interactions among diverse entities. As we explore integrating large
embedding tables into Pinterest's ads ranking models, we encountered not only
common challenges such as sparsity and scalability, but also several obstacles
unique to our context. Notably, our initial attempts to train large embedding
tables from scratch resulted in neutral metrics. To tackle this, we introduced
a novel multi-faceted pretraining scheme that incorporates multiple pretraining
algorithms. This approach greatly enriched the embedding tables and resulted in
significant performance improvements. As a result, the multi-faceted large
embedding tables bring great performance gain on both the Click-Through Rate
(CTR) and Conversion Rate (CVR) domains. Moreover, we designed a CPU-GPU hybrid
serving infrastructure to overcome GPU memory limits and elevate the
scalability. This framework has been deployed in the Pinterest Ads system and
achieved 1.34% online CPC reduction and 2.60% CTR increase with neutral
end-to-end latency change.

</details>


### [136] [G-UBS: Towards Robust Understanding of Implicit Feedback via Group-Aware User Behavior Simulation](https://arxiv.org/abs/2508.05709)
*Boyu Chen,Siran Chen,Zhengrong Yue,Kainan Yan,Chenyun Yu,Beibei Kong,Cheng Lei,Chengxiang Zhuo,Zang Li,Yali Wang*

Main category: cs.IR

TL;DR: G-UBS uses group-aware user behavior simulation to interpret noisy implicit feedback, improving video recommendation performance.


<details>
  <summary>Details</summary>
Motivation: Explicit feedback is scarce, and implicit feedback is often noisy, which can easily misjudge user interests and undermine recommendation performance.

Method: A novel Group-aware User Behavior Simulation (G-UBS) paradigm, which leverages contextual guidance from relevant user groups, enabling robust and in-depth interpretation of implicit feedback for individual users. It operates via two key agents: User Group Manager (UGM) and User Feedback Modeler (UFM).

Result: G-UBS achieves a 4.0% higher proportion of videos achieving a play rate > 30% and 14.9% higher reasoning accuracy on IF-VR.

Conclusion: G-UBS significantly outperforms mainstream LLMs and MLLMs on IF-VR, with a 4.0% higher proportion of videos achieving a play rate > 30% and 14.9% higher reasoning accuracy.

Abstract: User feedback is critical for refining recommendation systems, yet explicit
feedback (e.g., likes or dislikes) remains scarce in practice. As a more
feasible alternative, inferring user preferences from massive implicit feedback
has shown great potential (e.g., a user quickly skipping a recommended video
usually indicates disinterest). Unfortunately, implicit feedback is often
noisy: a user might skip a video due to accidental clicks or other reasons,
rather than disliking it. Such noise can easily misjudge user interests,
thereby undermining recommendation performance. To address this issue, we
propose a novel Group-aware User Behavior Simulation (G-UBS) paradigm, which
leverages contextual guidance from relevant user groups, enabling robust and
in-depth interpretation of implicit feedback for individual users.
Specifically, G-UBS operates via two key agents. First, the User Group Manager
(UGM) effectively clusters users to generate group profiles utilizing a
``summarize-cluster-reflect" workflow based on LLMs. Second, the User Feedback
Modeler (UFM) employs an innovative group-aware reinforcement learning
approach, where each user is guided by the associated group profiles during the
reinforcement learning process, allowing UFM to robustly and deeply examine the
reasons behind implicit feedback. To assess our G-UBS paradigm, we have
constructed a Video Recommendation benchmark with Implicit Feedback (IF-VR). To
the best of our knowledge, this is the first multi-modal benchmark for implicit
feedback evaluation in video recommendation, encompassing 15k users, 25k
videos, and 933k interaction records with implicit feedback. Extensive
experiments on IF-VR demonstrate that G-UBS significantly outperforms
mainstream LLMs and MLLMs, with a 4.0% higher proportion of videos achieving a
play rate > 30% and 14.9% higher reasoning accuracy on IF-VR.

</details>


### [137] [WebWatcher: Breaking New Frontiers of Vision-Language Deep Research Agent](https://arxiv.org/abs/2508.05748)
*Xinyu Geng,Peng Xia,Zhen Zhang,Xinyu Wang,Qiuchen Wang,Ruixue Ding,Chenxi Wang,Jialong Wu,Yida Zhao,Kuan Li,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.IR

TL;DR: WebWatcher是一种多模态Agent，它优于其他VQA模型，并且能够解决复杂的多模态信息检索任务。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要以文本为中心，忽略了现实世界中的视觉信息。这使得多模态深度研究非常具有挑战性，因为与基于文本的agent相比，此类agent需要在感知、逻辑、知识和更复杂工具的使用方面具有更强的推理能力。

Method: WebWatcher，一种多模态深度研究Agent，配备了增强的视觉语言推理能力。它利用高质量的合成多模态轨迹进行有效的冷启动训练，利用各种工具进行深度推理，并通过强化学习进一步增强泛化能力。

Result: WebWatcher在四个具有挑战性的VQA基准测试中显著优于专有基线、RAG工作流程和开源agent。

Conclusion: WebWatcher显著优于VQA基准模型，为解决复杂多模态信息检索任务铺平了道路。

Abstract: Web agents such as Deep Research have demonstrated superhuman cognitive
abilities, capable of solving highly challenging information-seeking problems.
However, most research remains primarily text-centric, overlooking visual
information in the real world. This makes multimodal Deep Research highly
challenging, as such agents require much stronger reasoning abilities in
perception, logic, knowledge, and the use of more sophisticated tools compared
to text-based agents. To address this limitation, we introduce WebWatcher, a
multi-modal Agent for Deep Research equipped with enhanced visual-language
reasoning capabilities. It leverages high-quality synthetic multimodal
trajectories for efficient cold start training, utilizes various tools for deep
reasoning, and further enhances generalization through reinforcement learning.
To better evaluate the capabilities of multimodal agents, we propose
BrowseComp-VL, a benchmark with BrowseComp-style that requires complex
information retrieval involving both visual and textual information.
Experimental results show that WebWatcher significantly outperforms proprietary
baseline, RAG workflow and open-source agents in four challenging VQA
benchmarks, which paves the way for solving complex multimodal
information-seeking tasks.

</details>


### [138] [Dual prototype attentive graph network for cross-market recommendation](https://arxiv.org/abs/2508.05969)
*Li Fan,Menglin Kong,Yang Xiang,Chong Zhang,Chengtao Ji*

Main category: cs.IR

TL;DR: 提出了一种新的跨市场推荐方法，该方法利用图表示学习中的原型来捕获市场特定和市场共享的见解，从而提高泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的跨市场推荐系统 (CMRS) 方法通常忽略了不同市场用户之间共享偏好的潜力，主要侧重于对每个市场内的特定偏好进行建模。本文认为，结合市场特定和市场共享的见解可以提高 CMRS 的泛化性和鲁棒性。

Method: 提出了一种名为双原型注意图网络用于跨市场推荐 (DGRE) 的新方法。

Result: 在真实世界的跨市场数据集上进行了大量实验来验证 DGRE 的有效性，结果表明，在建模中同时考虑市场特定和市场共享方面可以提高 CMRS 的泛化性和鲁棒性。

Conclusion: 考虑在建模中同时考虑市场特定和市场共享方面可以提高跨市场推荐系统的泛化性和鲁棒性。

Abstract: Cross-market recommender systems (CMRS) aim to utilize historical data from
mature markets to promote multinational products in emerging markets. However,
existing CMRS approaches often overlook the potential for shared preferences
among users in different markets, focusing primarily on modeling specific
preferences within each market. In this paper, we argue that incorporating both
market-specific and market-shared insights can enhance the generalizability and
robustness of CMRS. We propose a novel approach called Dual Prototype Attentive
Graph Network for Cross-Market Recommendation (DGRE) to address this. DGRE
leverages prototypes based on graph representation learning from both items and
users to capture market-specific and market-shared insights. Specifically, DGRE
incorporates market-shared prototypes by clustering users from various markets
to identify behavioural similarities and create market-shared user profiles.
Additionally, it constructs item-side prototypes by aggregating item features
within each market, providing valuable market-specific insights. We conduct
extensive experiments to validate the effectiveness of DGRE on a real-world
cross-market dataset, and the results show that considering both
market-specific and market-sharing aspects in modelling can improve the
generalization and robustness of CMRS.

</details>


### [139] [Efficient Multimodal Streaming Recommendation via Expandable Side Mixture-of-Experts](https://arxiv.org/abs/2508.05993)
*Yunke Qu,Liang Qu,Tong Chen,Quoc Viet Hung Nguyen,Hongzhi Yin*

Main category: cs.IR

TL;DR: XSMoE 是一种用于多模态流推荐的内存高效框架，它可以有效地捕获冷启动和多模态特征中的转移偏好。


<details>
  <summary>Details</summary>
Motivation: 有效捕获用户最新的偏好具有挑战性，因为反映近期兴趣的互动受到限制，并且新项目通常缺乏足够的反馈。微调大型多模态编码器的高成本，以及由于持续的模型更新而忘记长期用户偏好的风险。

Method: 我们提出了可扩展的侧面混合专家 (XSMoE)，这是一种用于多模态流推荐的内存高效框架。XSMoE 将由可扩展专家网络组成的轻量级侧面调整模块连接到冻结的预训练编码器，并根据不断发展的用户反馈逐步扩展它们。门控路由器动态地组合专家和骨干输出，而基于利用率的修剪策略保持模型的紧凑性。通过通过可扩展的专家学习新模式而不覆盖先前获得的知识，XSMoE 有效地捕获了多模态特征中的冷启动和转移偏好。

Result: 在三个真实世界数据集上的实验表明，XSMoE 在推荐质量和计算效率方面优于最先进的基线。

Conclusion: XSMoE在推荐质量和计算效率方面优于最先进的基线。

Abstract: Streaming recommender systems (SRSs) are widely deployed in real-world
applications, where user interests shift and new items arrive over time. As a
result, effectively capturing users' latest preferences is challenging, as
interactions reflecting recent interests are limited and new items often lack
sufficient feedback. A common solution is to enrich item representations using
multimodal encoders (e.g., BERT or ViT) to extract visual and textual features.
However, these encoders are pretrained on general-purpose tasks: they are not
tailored to user preference modeling, and they overlook the fact that user
tastes toward modality-specific features such as visual styles and textual
tones can also drift over time. This presents two key challenges in streaming
scenarios: the high cost of fine-tuning large multimodal encoders, and the risk
of forgetting long-term user preferences due to continuous model updates.
  To tackle these challenges, we propose Expandable Side Mixture-of-Experts
(XSMoE), a memory-efficient framework for multimodal streaming recommendation.
XSMoE attaches lightweight side-tuning modules consisting of expandable expert
networks to frozen pretrained encoders and incrementally expands them in
response to evolving user feedback. A gating router dynamically combines expert
and backbone outputs, while a utilization-based pruning strategy maintains
model compactness. By learning new patterns through expandable experts without
overwriting previously acquired knowledge, XSMoE effectively captures both cold
start and shifting preferences in multimodal features. Experiments on three
real-world datasets demonstrate that XSMoE outperforms state-of-the-art
baselines in both recommendation quality and computational efficiency.

</details>


### [140] [Semantic Item Graph Enhancement for Multimodal Recommendation](https://arxiv.org/abs/2508.06154)
*Xiaoxiong Zhang,Xin Zhou,Zhiwei Zeng,Dusit Niyato,Zhiqi Shen*

Main category: cs.IR

TL;DR: This paper addresses the semantic deficiencies in multimodal recommendation systems by enhancing semantic modeling and reducing the effect of structural noise in semantic graphs.


<details>
  <summary>Details</summary>
Motivation: Prior methods often build modality-specific item-item semantic graphs from raw modality features and use them as supplementary structures alongside the user-item interaction graph to enhance user preference learning. However, these semantic graphs suffer from semantic deficiencies, including (1) insufficient modeling of collaborative signals among items and (2) structural distortions introduced by noise in raw modality features, ultimately compromising performance.

Method: we first extract collaborative signals from the interaction graph and infuse them into each modality-specific item semantic graph to enhance semantic modeling. Then, we design a modulus-based personalized embedding perturbation mechanism that injects perturbations with modulus-guided personalized intensity into embeddings to generate contrastive views. Besides, we propose a dual representation alignment mechanism that first aligns multiple semantic representations via a designed Anchor-based InfoNCE loss using behavior representations as anchors, and then aligns behavior representations with the fused semantics by standard InfoNCE, to ensure representation consistency.

Result: the model learn noise-robust representations through contrastive learning, thereby reducing the effect of structural noise in semantic graphs

Conclusion: Extensive experiments on four benchmark datasets validate the effectiveness of our framework.

Abstract: Multimodal recommendation systems have attracted increasing attention for
their improved performance by leveraging items' multimodal information. Prior
methods often build modality-specific item-item semantic graphs from raw
modality features and use them as supplementary structures alongside the
user-item interaction graph to enhance user preference learning. However, these
semantic graphs suffer from semantic deficiencies, including (1) insufficient
modeling of collaborative signals among items and (2) structural distortions
introduced by noise in raw modality features, ultimately compromising
performance. To address these issues, we first extract collaborative signals
from the interaction graph and infuse them into each modality-specific item
semantic graph to enhance semantic modeling. Then, we design a modulus-based
personalized embedding perturbation mechanism that injects perturbations with
modulus-guided personalized intensity into embeddings to generate contrastive
views. This enables the model to learn noise-robust representations through
contrastive learning, thereby reducing the effect of structural noise in
semantic graphs. Besides, we propose a dual representation alignment mechanism
that first aligns multiple semantic representations via a designed Anchor-based
InfoNCE loss using behavior representations as anchors, and then aligns
behavior representations with the fused semantics by standard InfoNCE, to
ensure representation consistency. Extensive experiments on four benchmark
datasets validate the effectiveness of our framework.

</details>


### [141] [Improving Table Retrieval with Question Generation from Partial Tables](https://arxiv.org/abs/2508.06168)
*Hsing-Ping Liang,Che-Wei Chang,Yao-Chung Fan*

Main category: cs.IR

TL;DR: QGpT uses an LLM to generate synthetic questions based on small portions of a table, enhancing semantic alignment with user queries and improving retrieval performance.


<details>
  <summary>Details</summary>
Motivation: Little attention has been given to enhancing how tables themselves are represented in embedding space to better align with questions.

Method: Uses an LLM to generate synthetic questions based on small portions of a table.

Result: Significantly improves retrieval performance across multiple benchmarks for both dense and late-interaction retrievers.

Conclusion: QGpT improves retrieval performance across multiple benchmarks for both dense and late-interaction retrievers.

Abstract: Recent advances in open-domain question answering over tables have widely
adopted large language models (LLMs) under the Retriever-Reader architecture.
Prior works have effectively leveraged LLMs to tackle the complex reasoning
demands of the Reader component, such as text-to-text, text-to-SQL, and multi
hop reasoning. In contrast, the Retriever component has primarily focused on
optimizing the query representation-training retrievers to retrieve relevant
tables based on questions, or to select keywords from questions for matching
table segments. However, little attention has been given to enhancing how
tables themselves are represented in embedding space to better align with
questions. To address this, we propose QGpT (Question Generation from Partial
Tables), a simple yet effective method that uses an LLM to generate synthetic
questions based on small portions of a table. These questions are generated to
simulate how a user might query the content of the table currently under
consideration. The generated questions are then jointly embedded with the
partial table segments used for generation, enhancing semantic alignment with
user queries. Without the need to embed entire tables, our method significantly
improves retrieval performance across multiple benchmarks for both dense and
late-interaction retrievers.

</details>


### [142] [M2IO-R1: An Efficient RL-Enhanced Reasoning Framework for Multimodal Retrieval Augmented Multimodal Generation](https://arxiv.org/abs/2508.06328)
*Zhiyou Xiao,Qinhan Yu,Binghui Li,Geng Chen,Chong Chen,Wentao Zhang*

Main category: cs.IR

TL;DR: This paper introduces M2IO-R1, a novel framework for Multimodal Retrieval-Augmented Multimodal Generation (MRAMG) that supports both multimodal inputs and outputs. The framework uses an RL-based inserter to guide image selection and placement in a controllable and semantically aligned manner.


<details>
  <summary>Details</summary>
Motivation: Current research on Multimodal Retrieval-Augmented Generation (MRAG) enables diverse multimodal inputs but remains limited to single-modality outputs, restricting expressive capacity and practical utility. In contrast, real-world applications often demand both multimodal inputs and multimodal outputs for effective communication and grounded reasoning. Motivated by the recent success of Reinforcement Learning (RL) in complex reasoning tasks for Large Language Models (LLMs), we adopt RL as a principled and effective paradigm to address the multi-step, outcome-driven challenges inherent in multimodal output generation.

Method: an RL-based inserter, Inserter-R1-3B, trained with Group Relative Policy Optimization

Result: outperforming baselines in both quality and efficiency

Conclusion: a lightweight 3B inserter achieves strong reasoning capabilities with significantly reduced latency, outperforming baselines in both quality and efficiency.

Abstract: Current research on Multimodal Retrieval-Augmented Generation (MRAG) enables
diverse multimodal inputs but remains limited to single-modality outputs,
restricting expressive capacity and practical utility. In contrast, real-world
applications often demand both multimodal inputs and multimodal outputs for
effective communication and grounded reasoning. Motivated by the recent success
of Reinforcement Learning (RL) in complex reasoning tasks for Large Language
Models (LLMs), we adopt RL as a principled and effective paradigm to address
the multi-step, outcome-driven challenges inherent in multimodal output
generation. Here, we introduce M2IO-R1, a novel framework for Multimodal
Retrieval-Augmented Multimodal Generation (MRAMG) that supports both multimodal
inputs and outputs. Central to our framework is an RL-based inserter,
Inserter-R1-3B, trained with Group Relative Policy Optimization to guide image
selection and placement in a controllable and semantically aligned manner.
Empirical results show that our lightweight 3B inserter achieves strong
reasoning capabilities with significantly reduced latency, outperforming
baselines in both quality and efficiency.

</details>


### [143] [eSASRec: Enhancing Transformer-based Recommendations in a Modular Fashion](https://arxiv.org/abs/2508.06450)
*Daria Tikhonovich,Nikita Zelinskiy,Aleksandr V. Petrov,Mayya Spirina,Andrei Semenov,Andrey V. Savchenko,Sergei Kuliev*

Main category: cs.IR

TL;DR: eSASRec, a strong and simple baseline model, outperforms existing models and is easily integrated into recommendation pipelines.


<details>
  <summary>Details</summary>
Motivation: The additivity of modular improvements of Transformer-based models has not been systematically benchmarked

Method: Uses SASRec's training objective, LiGR Transformer layers, and Sampled Softmax Loss

Result: eSASRec is 23% more effective compared to the most recent state-of-the-art models in academic benchmarks, and resides on the Pareto frontier in terms of the accuracy-coverage tradeoff in production-like benchmark

Conclusion: eSASRec can be easily integrated into existing recommendation pipelines and can can serve as a strong yet very simple baseline for emerging complicated algorithms

Abstract: Since their introduction, Transformer-based models, such as SASRec and
BERT4Rec, have become common baselines for sequential recommendations,
surpassing earlier neural and non-neural methods. A number of following
publications have shown that the effectiveness of these models can be improved
by, for example, slightly updating the architecture of the Transformer layers,
using better training objectives, and employing improved loss functions.
However, the additivity of these modular improvements has not been
systematically benchmarked - this is the gap we aim to close in this paper.
Through our experiments, we identify a very strong model that uses SASRec's
training objective, LiGR Transformer layers, and Sampled Softmax Loss. We call
this combination eSASRec (Enhanced SASRec). While we primarily focus on
realistic, production-like evaluation, in our preliminarily study we find that
common academic benchmarks show eSASRec to be 23% more effective compared to
the most recent state-of-the-art models, such as ActionPiece. In our main
production-like benchmark, eSASRec resides on the Pareto frontier in terms of
the accuracy-coverage tradeoff (alongside the recent industrial models HSTU and
FuXi. As the modifications compared to the original SASRec are relatively
straightforward and no extra features are needed (such as timestamps in HSTU),
we believe that eSASRec can be easily integrated into existing recommendation
pipelines and can can serve as a strong yet very simple baseline for emerging
complicated algorithms. To facilitate this, we provide the open-source
implementations for our models and benchmarks in repository
https://github.com/blondered/transformer_benchmark

</details>


### [144] [Maximum Impact with Fewer Features: Efficient Feature Selection for Cold-Start Recommenders through Collaborative Importance Weighting](https://arxiv.org/abs/2508.06455)
*Nikita Sukhorukov,Danil Gusak,Evgeny Frolov*

Main category: cs.IR

TL;DR: 提出了一种特征选择策略，该策略优先考虑用户行为信息，通过结合协作行为数据的相关性来增强特征表示，并使用基于最大体积算法的机制对特征进行排序，从而在推荐准确性和计算效率之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中的冷启动挑战需要利用用户-项目交互之外的辅助特征。然而，不相关或嘈杂特征的存在会降低预测性能，而过多的特征会增加计算需求，导致更高的内存消耗和更长的训练时间。

Method: 结合了来自协作行为数据的相关性，使用混合矩阵分解技术增强特征表示，然后使用基于最大体积算法的机制对特征进行排序。

Result: 在各种数据集和混合推荐模型上进行了广泛的评估，证明了该方法在冷启动场景中表现出色，选择了最少但非常有效的特征子集。即使在严格的特征减少下，该方法也优于现有的特征选择技术，同时保持了卓越的效率。

Conclusion: 该方法在冷启动场景中表现出色，选择了最少但非常有效的特征子集。即使在严格的特征减少下，该方法也优于现有的特征选择技术，同时保持了卓越的效率。

Abstract: Cold-start challenges in recommender systems necessitate leveraging auxiliary
features beyond user-item interactions. However, the presence of irrelevant or
noisy features can degrade predictive performance, whereas an excessive number
of features increases computational demands, leading to higher memory
consumption and prolonged training times.
  To address this, we propose a feature selection strategy that prioritizes the
user behavioral information. Our method enhances the feature representation by
incorporating correlations from collaborative behavior data using a hybrid
matrix factorization technique and then ranks features using a mechanism based
on the maximum volume algorithm. This approach identifies the most influential
features, striking a balance between recommendation accuracy and computational
efficiency. We conduct an extensive evaluation across various datasets and
hybrid recommendation models, demonstrating that our method excels in
cold-start scenarios by selecting minimal yet highly effective feature subsets.
Even under strict feature reduction, our approach surpasses existing feature
selection techniques while maintaining superior efficiency.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [145] [Diagrams-to-Dynamics (D2D): Exploring Causal Loop Diagram Leverage Points under Uncertainty](https://arxiv.org/abs/2508.05659)
*Jeroen F. Uleman,Loes Crielaard,Leonie K. Elsenburg,Guido A. Veldhuis,Karien Stronks,Naja Hulvej Rod,Rick Quax,Vítor V. Vasconcelos*

Main category: cs.LG

TL;DR: Diagrams-to-Dynamics (D2D) is proposed to convert CLDs into exploratory system dynamics models (SDMs) in the absence of empirical data. It helps distinguish between high- and low-ranked leverage points and showed greater consistency with the data-driven model than network centrality analysis.


<details>
  <summary>Details</summary>
Motivation: CLDs are limited in their ability to support dynamic analysis and inform intervention strategies. Additionally, quantitative CLD analysis methods like network centrality analysis often lead to false inference.

Method: converting CLDs into exploratory system dynamics models (SDMs) in the absence of empirical data, leveraging the structural information already encoded in CLDs, namely, link existence and polarity, to simulate hypothetical interventions and explore potential leverage points under uncertainty.

Result: D2D helps distinguish between high- and low-ranked leverage points. D2D showed greater consistency with the data-driven model than network centrality analysis, while providing uncertainty estimates and guidance for future data collection.

Conclusion: D2D helps distinguish between high- and low-ranked leverage points and showed greater consistency with the data-driven model than network centrality analysis, while providing uncertainty estimates and guidance for future data collection.

Abstract: Causal loop diagrams (CLDs) are widely used in health and environmental
research to represent hypothesized causal structures underlying complex
problems. However, as qualitative and static representations, CLDs are limited
in their ability to support dynamic analysis and inform intervention
strategies. Additionally, quantitative CLD analysis methods like network
centrality analysis often lead to false inference. We propose
Diagrams-to-Dynamics (D2D), a method for converting CLDs into exploratory
system dynamics models (SDMs) in the absence of empirical data. With minimal
user input - following a protocol to label variables as stocks,
flows/auxiliaries, or constants - D2D leverages the structural information
already encoded in CLDs, namely, link existence and polarity, to simulate
hypothetical interventions and explore potential leverage points under
uncertainty. Results suggest that D2D helps distinguish between high- and
low-ranked leverage points. We compare D2D to a data-driven SDM constructed
from the same CLD and variable labeling. D2D showed greater consistency with
the data-driven model than network centrality analysis, while providing
uncertainty estimates and guidance for future data collection. The method is
implemented in an open-source Python package and a web-based application to
support further testing and lower the barrier to dynamic modeling for
researchers working with CLDs. We expect additional validation will further
establish the approach's utility across a broad range of cases and domains.

</details>


### [146] [A Graph Neural Network Approach for Mapping the Conceptual Structure and Inter-Branch Connectivity of Physics](https://arxiv.org/abs/2508.05724)
*Massimiliano Romiti*

Main category: cs.LG

TL;DR: A novel framework using a weighted knowledge graph and GAT to represent and analyze physical laws, achieving high accuracy in link prediction and rediscovering known physics structures.


<details>
  <summary>Details</summary>
Motivation: introducing a novel framework for representing and analyzing physical laws as a weighted knowledge graph

Method: enhanced graph representation and a Graph Attention Network (GAT)

Result: GAT achieved a test AUC of 0.9742 +/- 0.0018, outperforming classical heuristics and GNN architectures

Conclusion: The model autonomously rediscovers the known macroscopic structure of physics, identifies central hub equations, and generates stable, computationally-derived hypotheses for cross-domain relationships.

Abstract: This work introduces a novel framework for representing and analyzing
physical laws as a weighted knowledge graph. We constructed a database of 659
distinct physical equations, subjected to rigorous semantic cleaning to resolve
notational ambiguities, resulting in a corpus of 400 advanced physics
equations. We developed an enhanced graph representation where both physical
concepts and equations are nodes, connected by weighted inter-equation bridges.
These weights are objectively defined using normalized metrics for variable
overlap, physics-informed importance scores, and bibliometric data. A Graph
Attention Network (GAT) was trained for link prediction, achieving a test AUC
of 0.9742 +/- 0.0018 across five independent runs, significantly outperforming
both classical heuristics (best baseline AUC: 0.9487) and established GNN
architectures like GraphSAGE (AUC: 0.9504, p = 0.029). Statistical testing
confirmed significance of all comparisons (p < 0.05), with 2.7% improvement
over the best baseline. Our analysis reveals three key findings: (i) The model
autonomously rediscovers the known macroscopic structure of physics,
identifying strong conceptual axes between Electromagnetism and Statistical
Mechanics. (ii) It identifies central hub equations that serve as critical
bridges between multiple physical domains. (iii) The model generates stable,
computationally-derived hypotheses for cross-domain relationships, identifying
both known principles and suggesting novel mathematical analogies for further
theoretical investigation. The framework can generate hundreds of such
hypotheses, enabling the creation of specialized datasets for targeted analysis
of specific physics subfields. Code and data available at
https://github.com/kingelanci/graphysics

</details>


### [147] [Machine Learning-Based Nonlinear Nudging for Chaotic Dynamical Systems](https://arxiv.org/abs/2508.05778)
*Jaemin Oh,Jinsil Lee,Youngjoon Hong*

Main category: cs.LG

TL;DR: This paper introduces neural network nudging for learning nudging terms in nonlinear systems and validates it on chaotic systems.


<details>
  <summary>Details</summary>
Motivation: Designing effective nudging terms becomes significantly more challenging in the nonlinear setting.

Method: This paper proposes neural network nudging, a data-driven method for learning nudging terms in nonlinear state space models. It also establishes a theoretical existence result based on the Kazantzis--Kravaris--Luenberger observer theory.

Result: The proposed approach is evaluated on three benchmark problems that exhibit chaotic behavior: the Lorenz 96 model, the Kuramoto--Sivashinsky equation, and the Kolmogorov flow.

Conclusion: This paper proposes and evaluates neural network nudging, a data-driven method for learning nudging terms in nonlinear state space models, on three benchmark problems that exhibit chaotic behavior: the Lorenz 96 model, the Kuramoto--Sivashinsky equation, and the Kolmogorov flow.

Abstract: Nudging is an empirical data assimilation technique that incorporates an
observation-driven control term into the model dynamics. The trajectory of the
nudged system approaches the true system trajectory over time, even when the
initial conditions differ. For linear state space models, such control terms
can be derived under mild assumptions. However, designing effective nudging
terms becomes significantly more challenging in the nonlinear setting. In this
work, we propose neural network nudging, a data-driven method for learning
nudging terms in nonlinear state space models. We establish a theoretical
existence result based on the Kazantzis--Kravaris--Luenberger observer theory.
The proposed approach is evaluated on three benchmark problems that exhibit
chaotic behavior: the Lorenz 96 model, the Kuramoto--Sivashinsky equation, and
the Kolmogorov flow.

</details>


### [148] [From Imperfect Signals to Trustworthy Structure: Confidence-Aware Inference from Heterogeneous and Reliability-Varying Utility Data](https://arxiv.org/abs/2508.05791)
*Haoran Li,Lihao Mai,Muhao Guo,Jiaqi Wu,Yang Weng,Yannan Sun,Ce Jimmy Liu*

Main category: cs.LG

TL;DR: Proposes a framework for reconstructing grid topology by integrating heterogeneous data, using confidence-aware inference and physical constraints, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Accurate distribution grid topology is essential for reliable modern grid operations. However, real-world utility data originates from multiple sources with varying characteristics and levels of quality.

Method: a scalable framework that reconstructs a trustworthy grid topology by systematically integrating heterogeneous data. jointly leverage the spatial layout of physical infrastructure and the dynamic behavior of the system in the signal domain. introduce a confidence-aware inference mechanism that preserves structurally informative yet imperfect inputs, while quantifying the reliability of each inferred connection for operator interpretation. embed operational constraints, such as transformer capacity limits and radial topology requirements, directly into the learning process.

Result: over 95% accuracy in topology reconstruction and substantial improvements in confidence calibration and computational efficiency relative to baseline methods.

Conclusion: The proposed framework is validated using data from over 8000 meters across 3 feeders in Oncor's service territory, demonstrating over 95% accuracy in topology reconstruction and substantial improvements in confidence calibration and computational efficiency relative to baseline methods.

Abstract: Accurate distribution grid topology is essential for reliable modern grid
operations. However, real-world utility data originates from multiple sources
with varying characteristics and levels of quality. In this work, developed in
collaboration with Oncor Electric Delivery, we propose a scalable framework
that reconstructs a trustworthy grid topology by systematically integrating
heterogeneous data. We observe that distribution topology is fundamentally
governed by two complementary dimensions: the spatial layout of physical
infrastructure (e.g., GIS and asset metadata) and the dynamic behavior of the
system in the signal domain (e.g., voltage time series). When jointly
leveraged, these dimensions support a complete and physically coherent
reconstruction of network connectivity. To address the challenge of uneven data
quality without compromising observability, we introduce a confidence-aware
inference mechanism that preserves structurally informative yet imperfect
inputs, while quantifying the reliability of each inferred connection for
operator interpretation. This soft handling of uncertainty is tightly coupled
with hard enforcement of physical feasibility: we embed operational
constraints, such as transformer capacity limits and radial topology
requirements, directly into the learning process. Together, these components
ensure that inference is both uncertainty-aware and structurally valid,
enabling rapid convergence to actionable, trustworthy topologies under
real-world deployment conditions. The proposed framework is validated using
data from over 8000 meters across 3 feeders in Oncor's service territory,
demonstrating over 95% accuracy in topology reconstruction and substantial
improvements in confidence calibration and computational efficiency relative to
baseline methods.

</details>


### [149] [Optimal Linear Baseline Models for Scientific Machine Learning](https://arxiv.org/abs/2508.05831)
*Alexander DeLise,Kyle Loh,Krish Patel,Meredith Teague,Andrea Arnold,Matthias Chung*

Main category: cs.LG

TL;DR: 本文研究了线性神经网络在科学机器学习问题中的应用，提出了一个统一的理论框架，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 一个基本的挑战是表征和计算从底层物理过程到观测信号和测量的映射。虽然非线性神经网络已经取得了相当大的成功，但它们在理论上仍然是不透明的，这阻碍了在可解释性至关重要的环境中的采用。相比之下，线性神经网络为深入了解这些复杂关系提供了一个简单而有效的基础。

Method: 我们开发了一个统一的理论框架，通过贝叶斯风险最小化的视角来分析线性编码器-解码器架构，以解决数据驱动的科学机器学习问题。我们推导了用于前向建模和逆恢复任务的闭式、秩约束线性及仿射线性最优映射。

Result: 我们的结果通过适应数据、前向算子和测量过程中的秩缺陷来推广现有的公式。我们通过对来自简单生物医学成像、金融因素分析以及涉及通过浅水方程的非线性流体动力学模拟的数据集进行数值实验来验证我们的理论结果。

Conclusion: 这项工作为理解和评估用于科学机器学习问题的学习神经网络模型提供了一个强大的基线。

Abstract: Across scientific domains, a fundamental challenge is to characterize and
compute the mappings from underlying physical processes to observed signals and
measurements. While nonlinear neural networks have achieved considerable
success, they remain theoretically opaque, which hinders adoption in contexts
where interpretability is paramount. In contrast, linear neural networks serve
as a simple yet effective foundation for gaining insight into these complex
relationships. In this work, we develop a unified theoretical framework for
analyzing linear encoder-decoder architectures through the lens of Bayes risk
minimization for solving data-driven scientific machine learning problems. We
derive closed-form, rank-constrained linear and affine linear optimal mappings
for forward modeling and inverse recovery tasks. Our results generalize
existing formulations by accommodating rank-deficiencies in data, forward
operators, and measurement processes. We validate our theoretical results by
conducting numerical experiments on datasets from simple biomedical imaging,
financial factor analysis, and simulations involving nonlinear fluid dynamics
via the shallow water equations. This work provides a robust baseline for
understanding and benchmarking learned neural network models for scientific
machine learning problems.

</details>


### [150] [An Effective Approach for Node Classification in Textual Graphs](https://arxiv.org/abs/2508.05836)
*Rituparna Datta,Nibir Chandra Mandal*

Main category: cs.LG

TL;DR: 提出了一种新颖的框架，该框架集成了TAPE（文本属性图表示增强）与Graphormer，用于文本属性图中的节点分类。


<details>
  <summary>Details</summary>
Motivation: 由于难以将文本中的丰富语义与结构图信息集成，因此有效的节点分类仍然具有挑战性。现有的方法通常难以捕获细微的领域特定术语，建模远程依赖关系，适应时间演变以及扩展到海量数据集。

Method: 集成了TAPE（文本属性图表示增强）与Graphormer

Result: 在ogbn-arxiv数据集上实现了最先进的性能，分类精度为0.772，显著超过了最佳GCN基线0.713。该方法在精确率（0.671），召回率（0.577）和F1-score（0.610）方面也产生了强大的结果。

Conclusion: 该框架为动态TAG中的节点分类提供了一个可扩展且稳健的解决方案，为知识系统和科学发现的未来研究提供了一个有希望的方向。

Abstract: Textual Attribute Graphs (TAGs) are critical for modeling complex networks
like citation networks, but effective node classification remains challenging
due to difficulties in integrating rich semantics from text with structural
graph information. Existing methods often struggle with capturing nuanced
domain-specific terminology, modeling long-range dependencies, adapting to
temporal evolution, and scaling to massive datasets. To address these issues,
we propose a novel framework that integrates TAPE (Text-Attributed Graph
Representation Enhancement) with Graphormer. Our approach leverages a large
language model (LLM), specifically ChatGPT, within the TAPE framework to
generate semantically rich explanations from paper content, which are then
fused into enhanced node representations. These embeddings are combined with
structural features using a novel integration layer with learned attention
weights. Graphormer's path-aware position encoding and multi-head attention
mechanisms are employed to effectively capture long-range dependencies across
the citation network. We demonstrate the efficacy of our framework on the
challenging ogbn-arxiv dataset, achieving state-of-the-art performance with a
classification accuracy of 0.772, significantly surpassing the best GCN
baseline of 0.713. Our method also yields strong results in precision (0.671),
recall (0.577), and F1-score (0.610). We validate our approach through
comprehensive ablation studies that quantify the contribution of each
component, demonstrating the synergy between semantic and structural
information. Our framework provides a scalable and robust solution for node
classification in dynamic TAGs, offering a promising direction for future
research in knowledge systems and scientific discovery.

</details>


### [151] [A Markov Decision Process Framework for Early Maneuver Decisions in Satellite Collision Avoidance](https://arxiv.org/abs/2508.05876)
*Francesca Ferrara,Lander W. Schillinger Arana,Florian Dörfler,Sarah H. Q. Li*

Main category: cs.LG

TL;DR: 使用 MDP 框架和强化学习训练自主制导策略，以减少碰撞风险和燃料消耗。


<details>
  <summary>Details</summary>
Motivation: 通过尽早做出机动决策，最大限度地减少 CAM 的平均燃料消耗。

Method: 马尔可夫决策过程 (MDP) 框架和强化学习策略梯度 (RL-PG) 算法

Result: 在合成的合事件中，该策略显著降低了每次 CAM 的整体和平均燃料消耗，而在历史合事件中，该策略消耗了更多的燃料，但降低了每次 CAM 的平均燃料消耗。对于历史和合成的合事件，该策略实现了相等甚至更高的总体碰撞风险保证。

Conclusion: 该策略在保证或提高总体碰撞风险的前提下，显著降低了每次 CAM 的整体和平均燃料消耗。

Abstract: This work presents a Markov decision process (MDP) framework to model
decision-making for collision avoidance maneuver (CAM) and a reinforcement
learning policy gradient (RL-PG) algorithm to train an autonomous guidance
policy using historic CAM data. In addition to maintaining acceptable collision
risks, this approach seeks to minimize the average fuel consumption of CAMs by
making early maneuver decisions. We model CAM as a continuous state, discrete
action and finite horizon MDP, where the critical decision is determining when
to initiate the maneuver. The MDP model also incorporates analytical models for
conjunction risk, propellant consumption, and transit orbit geometry. The
Markov policy effectively trades-off maneuver delay-which improves the
reliability of conjunction risk indicators-with propellant consumption-which
increases with decreasing maneuver time. Using historical data of tracked
conjunction events, we verify this framework and conduct an extensive ablation
study on the hyper-parameters used within the MDP. On synthetic conjunction
events, the trained policy significantly minimizes both the overall and average
propellant consumption per CAM when compared to a conventional cut-off policy
that initiates maneuvers 24 hours before the time of closest approach (TCA). On
historical conjunction events, the trained policy consumes more propellant
overall but reduces the average propellant consumption per CAM. For both
historical and synthetic conjunction events, the trained policy achieves equal
if not higher overall collision risk guarantees.

</details>


### [152] [The Fourth State: Signed-Zero Ternary for Stable LLM Quantization (and More)](https://arxiv.org/abs/2508.05905)
*Jeffrey Uhlmann*

Main category: cs.LG

TL;DR: This paper introduces a 2-bit quantization method called Signed-Zero Ternary (SZT) that improves information density and provides gradient information with no forward-path penalty.


<details>
  <summary>Details</summary>
Motivation: Quantization is usually regarded as a means to trade quality of performance for reduced compute requirements, i.e., as a suboptimal approximation. However, if examined in terms of a fixed overall resource budget, a very different perspective arises.

Method: introducing Signed-Zero Ternary (SZT), a 2-bit quantization

Result: SZT deterministically provides gradient information with no forward-path penalty.

Conclusion: Signed-Zero Ternary (SZT), a 2-bit quantization, deterministically provides gradient information with no forward-path penalty and may improve information density compared to non-quantized alternatives.

Abstract: Quantization is usually regarded as a means to trade quality of performance
for reduced compute requirements, i.e., as a suboptimal approximation. However,
if examined in terms of a fixed overall resource budget, a very different
perspective arises. We introduce Signed-Zero Ternary (SZT), a 2-bit
quantization that deterministically provides gradient information with no
forward-path penalty. Our analysis provides evidence that it may improve
information density compared to non-quantized alternatives.

</details>


### [153] [Dual Signal Decomposition of Stochastic Time Series](https://arxiv.org/abs/2508.05915)
*Alex Glushkovsky*

Main category: cs.LG

TL;DR: This paper presents a machine learning approach to decompose stochastic time series into mean, dispersion, and noise, which can be used for smoothing, denoising, and further analysis.


<details>
  <summary>Details</summary>
Motivation: Decompose a stochastic time series into mean, dispersion, and isolated noise.

Method: Machine learning is used to fit a dual signal, minimizing a loss function that balances fitting the original time series and penalizing irregularities using first and second-order derivatives, with regularization components weighted by Statistical Process Control.

Result: A decomposition method is proposed that can smooth or denoise time series, with two learning approaches (sequential and joint) and hyperparameter tuning focusing on stationary noise without autocorrelation.

Conclusion: The decomposed dual signal (mean and dispersion) can be represented in 2D space for learning structures, forecasting, and analyzing cross-effects in multiple time series.

Abstract: The research paper addresses decomposition of a stochastic time series into
three time series representing a dual signal i.e., the mean and the dispersion,
with noise isolated. Decomposition is done by applying machine learning to fit
a dual signal. Machine learning minimizes the loss function which compromises
between fitting the original time series and penalizing irregularities of the
dual signal. The latter includes terms based on the first and second order
derivatives along time. To preserve special patterns, weighting of the
regularization components of the loss function has been introduced based on
Statistical Process Control methodology. The proposed decomposition can be
applied as a smoothing algorithm against the mean and dispersion of the time
series. By isolating noise, the proposed decomposition can be seen as a
denoising algorithm. Two approaches of the learning process have been
considered: sequential and jointly. The former approach learns the mean signal
first and then dispersion. The latter approach fits the dual signal jointly.
Jointly learning can uncover complex relationships for the time series with
heteroskedasticity. Learning has been set by solving the direct non-linear
unconstrained optimization problem or by applying neural networks that have
sequential or twin output architectures. Tuning of the loss function
hyperparameters focuses on the isolated noise to be a stationary stochastic
process without autocorrelation properties. Depending on the applications, the
hyperparameters of the learning can be tuned towards either the discrete states
by stepped signal or smoothed series. The decomposed dual signal can be
represented on the 2D space and used to learn inherent structures, to forecast
both mean and dispersion, or to analyze cross effects in case of multiple time
series.

</details>


### [154] [Fast, Convex and Conditioned Network for Multi-Fidelity Vectors and Stiff Univariate Differential Equations](https://arxiv.org/abs/2508.05921)
*Siddharth Rout*

Main category: cs.LG

TL;DR: Ill-conditioning limits convergence in neural PDE solvers. Shifted Gaussian Encoding improves conditioning and performance.


<details>
  <summary>Details</summary>
Motivation: Accuracy in neural PDE solvers often breaks down not because of limited expressivity, but due to poor optimisation caused by ill-conditioning, especially in multi-fidelity and stiff problems.

Method: Shifted Gaussian Encoding, a simple yet effective activation filtering step that increases matrix rank and expressivity while preserving convexity.

Result: Extends the solvable range of Peclet numbers in steady advection-diffusion equations by over two orders of magnitude, achieves up to six orders lower error on multi-frequency function learning, and fits high-fidelity image vectors more accurately and faster than deep networks with over a million parameters.

Conclusion: Conditioning, not depth, is often the bottleneck in scientific neural solvers, and simple architectural changes can unlock substantial gains.

Abstract: Accuracy in neural PDE solvers often breaks down not because of limited
expressivity, but due to poor optimisation caused by ill-conditioning,
especially in multi-fidelity and stiff problems. We study this issue in
Physics-Informed Extreme Learning Machines (PIELMs), a convex variant of neural
PDE solvers, and show that asymptotic components in governing equations can
produce highly ill-conditioned activation matrices, severely limiting
convergence. We introduce Shifted Gaussian Encoding, a simple yet effective
activation filtering step that increases matrix rank and expressivity while
preserving convexity. Our method extends the solvable range of Peclet numbers
in steady advection-diffusion equations by over two orders of magnitude,
achieves up to six orders lower error on multi-frequency function learning, and
fits high-fidelity image vectors more accurately and faster than deep networks
with over a million parameters. This work highlights that conditioning, not
depth, is often the bottleneck in scientific neural solvers and that simple
architectural changes can unlock substantial gains.

</details>


### [155] [Mitigating Think-Answer Mismatch in LLM Reasoning Through Noise-Aware Advantage Reweighting](https://arxiv.org/abs/2508.05928)
*Si Shen,Peijun Shen,Wenhua Zhao,Danhao Zhu*

Main category: cs.LG

TL;DR: S-GRPO是一种改进的GRPO方法，通过优化权重来解决噪声问题，从而更稳定有效地训练大型推理模型，并在数学推理任务上取得了更好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的Group-Relative Policy Optimization (GRPO) 技术在训练大型推理模型时存在“思考-回答不匹配”的脆弱性，尤其是在不平衡的响应组中，噪声奖励信号会破坏学习过程。

Method: 提出了一种名为Stable Group-Relative Policy Optimization (S-GRPO) 的改进方法，通过推导最优的、噪声感知的优势权重来稳定训练。

Result: S-GRPO在Qwen-Math-7B-Base、Llama-3.2-3B-Base和Qwen-Math-1.5B-Instruct等模型上，性能分别提升了+2.5%、+2.2%和+2.4%。即使在20%的合成奖励噪声下，S-GRPO仍能保持稳定的学习。

Conclusion: S-GRPO在存在噪声的情况下，能够保持稳定的学习过程，并且在数学推理基准测试中表现出有效性和鲁棒性，优于DR. GRPO。

Abstract: Group-Relative Policy Optimization (GRPO) is a key technique for training
large reasoning models, yet it suffers from a critical vulnerability: the
\emph{Think-Answer Mismatch}, where noisy reward signals corrupt the learning
process. This problem is most severe in unbalanced response groups,
paradoxically degrading the signal precisely when it should be most
informative. To address this challenge, we propose Stable Group-Relative Policy
Optimization (S-GRPO), a principled enhancement that derives optimal,
noise-aware advantage weights to stabilize training. Our comprehensive
experiments on mathematical reasoning benchmarks demonstrate S-GRPO's
effectiveness and robustness. On various models, S-GRPO significantly
outperforms DR. GRPO, achieving performance gains of +2.5% on
Qwen-Math-7B-Base, +2.2% on Llama-3.2-3B-Base, and +2.4% on
Qwen-Math-1.5B-Instruct. Most critically, while standard GRPO fails to learn
under 20% synthetic reward noise, S-GRPO maintains stable learning progress.
These results highlight S-GRPO's potential for more robust and effective
training of large-scale reasoning models. \footnote{Code and data are available
at: https://github.com/shenpeijun0212/S-GRPO

</details>


### [156] [Multi-Armed Bandits-Based Optimization of Decision Trees](https://arxiv.org/abs/2508.05957)
*Hasibul Karim Shanto,Umme Ayman Koana,Shadikur Rahman*

Main category: cs.LG

TL;DR: 提出了一种基于MAB的决策树剪枝方法，该方法在泛化性能方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的剪枝技术大多基于贪婪方法，侧重于在剪枝决策树节点时立即获得性能提升。然而，从长远来看，这可能会导致较低的泛化能力，从而在引入到未见过的数据样本时，降低树模型的鲁棒性能力，特别是在使用小型和复杂的数据集进行训练时。

Method: 提出了一种基于Multi-Armed Bandits (MAB) 的剪枝方法，该方法将剪枝过程视为一个探索-利用问题，利用MAB算法找到最佳分支节点，根据每个剪枝动作的反馈进行剪枝。

Result: 在多个基准数据集上的实验评估表明，我们提出的方法比传统方法具有更好的预测性能。

Conclusion: 提出了一种基于Multi-Armed Bandits (MAB) 的剪枝方法，实验结果表明，与传统方法相比，该方法具有更好的预测性能，表明了利用MAB进行动态和概率决策树剪枝的潜力。

Abstract: Decision trees, without appropriate constraints, can easily become overly
complex and prone to overfit, capturing noise rather than generalizable
patterns. To resolve this problem,pruning operation is a crucial part in
optimizing decision trees, as it not only reduces the complexity of trees but
also decreases the probability of generating overfit models. The conventional
pruning techniques like Cost-Complexity Pruning (CCP) and Reduced Error Pruning
(REP) are mostly based on greedy approaches that focus on immediate gains in
performance while pruning nodes of the decision tree. However, this might
result in a lower generalization in the long run, compromising the robust
ability of the tree model when introduced to unseen data samples, particularly
when trained with small and complex datasets. To address this challenge, we are
proposing a Multi-Armed Bandits (MAB)-based pruning approach, a reinforcement
learning (RL)-based technique, that will dynamically prune the tree to generate
an optimal decision tree with better generalization. Our proposed approach
assumes the pruning process as an exploration-exploitation problem, where we
are utilizing the MAB algorithms to find optimal branch nodes to prune based on
feedback from each pruning actions. Experimental evaluation on several
benchmark datasets, demonstrated that our proposed approach results in better
predictive performance compared to the traditional ones. This suggests the
potential of utilizing MAB for a dynamic and probabilistic way of decision tree
pruning, in turn optimizing the decision tree-based model.

</details>


### [157] [Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning](https://arxiv.org/abs/2508.05960)
*Haohui Chen,Zhiyong Chen*

Main category: cs.LG

TL;DR: 提出了MCRQ算法，该算法在离线强化学习中实现了优越的性能，通过平衡保守性和性能来解决分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习(RL)试图从静态数据集中学习最优策略，而无需进一步的环境交互。一个关键的挑战是学习策略和行为策略之间的分布转移，导致分布外(OOD)行动和高估。为了防止严重的高估，价值函数必须保持保守;然而，过度保守可能会阻碍性能的提高。为了解决这个问题

Method: 结合时间差分(TD)误差与Bellman备份中的行为克隆项，提出了适度保守的正则化评估(MCRE)框架，在此基础上，开发了适度保守的正则化Q学习(MCRQ)算法，该算法将MCRE集成到off-policy actor-critic框架中。

Result: MCRQ算法在benchmark数据集上优于强大的基线和最先进的离线RL算法。

Conclusion: MCRQ算法在benchmark数据集上优于强大的基线和最先进的离线RL算法。

Abstract: Offline reinforcement learning (RL) seeks to learn optimal policies from
static datasets without further environment interaction. A key challenge is the
distribution shift between the learned and behavior policies, leading to
out-of-distribution (OOD) actions and overestimation. To prevent gross
overestimation, the value function must remain conservative; however, excessive
conservatism may hinder performance improvement. To address this, we propose
the mildly conservative regularized evaluation (MCRE) framework, which balances
conservatism and performance by combining temporal difference (TD) error with a
behavior cloning term in the Bellman backup. Building on this, we develop the
mildly conservative regularized Q-learning (MCRQ) algorithm, which integrates
MCRE into an off-policy actor-critic framework. Experiments show that MCRQ
outperforms strong baselines and state-of-the-art offline RL algorithms on
benchmark datasets.

</details>


### [158] [LinguaFluid: Language Guided Fluid Control via Semantic Rewards in Reinforcement Learning](https://arxiv.org/abs/2508.05977)
*Aoming Liang,Chi Cheng,Dashuai Chen,Boai Sun,Dixia Fan*

Main category: cs.LG

TL;DR: 提出了一种语义对齐的强化学习方法，该方法使用 SBERT 通过将当前状态与目标语义指令对齐来计算奖励，无需手动定义的奖励函数。


<details>
  <summary>Details</summary>
Motivation: 在强化学习 (RL) 中，设计有效的奖励函数仍然是一个挑战，尤其是在任务目标难以用数字指定的环境中。现有工作中的奖励函数主要基于启发式、手动工程或特定于任务的调整。

Method: 使用 Sentence-Bidirectional Encoder Representations from Transformers (SBERT) 通过将当前状态与目标语义指令对齐来计算奖励。

Result: 语义奖励能够引导学习以实现有竞争力的控制行为，即使在没有手工设计的奖励函数的情况下。在多个环境中评估了该方法，并表明语义奖励可以指导学习以实现有竞争力的控制行为，即使在没有手工设计的奖励函数的情况下。研究表明语言嵌入空间和传统的欧几里得空间之间存在相关性。

Conclusion: 语义奖励能够引导学习以实现有竞争力的控制行为，即使在没有手工设计的奖励函数的情况下。语言嵌入空间和传统的欧几里得空间之间存在相关性。该框架为将代理行为与自然语言目标对齐开辟了新的视野，并为更大语言模型 (LLM) 和流体控制应用的更无缝集成奠定了基础。

Abstract: In the domain of scientific machine learning, designing effective reward
functions remains a challenge in reinforcement learning (RL), particularly in
environments where task goals are difficult to specify numerically. Reward
functions in existing work are predominantly based on heuristics, manual
engineering, or task-specific tuning. In this work, we introduce a semantically
aligned reinforcement learning method where rewards are computed by aligning
the current state with a target semantic instruction using a
Sentence-Bidirectional Encoder Representations from Transformers (SBERT).
Instead of relying on manually defined reward functions, the policy receives
feedback based on the reward, which is a cosine similarity between the goal
textual description and the statement description in the episode. We evaluated
our approach in several environments and showed that semantic reward can guide
learning to achieve competitive control behavior, even in the absence of
hand-crafted reward functions. Our study demonstrates a correlation between the
language embedding space and the conventional Euclidean space. This framework
opens new horizons for aligning agent behavior with natural language goals and
lays the groundwork for a more seamless integration of larger language models
(LLMs) and fluid control applications.

</details>


### [159] [Parameter-free Optimal Rates for Nonlinear Semi-Norm Contractions with Applications to $Q$-Learning](https://arxiv.org/abs/2508.05984)
*Ankur Naskar,Gugan Thoppe,Vijay Gupta*

Main category: cs.LG

TL;DR: 本文为Q学习实现了首个无参数最优速率，解决了非线性不动点方程中的半范数收缩问题。


<details>
  <summary>Details</summary>
Motivation: 为解决非线性不动点方程（如平均奖励Q学习和TD学习）的算法通常涉及半范数收缩，但通过Polyak-Ruppert平均为这些方法实现无参数最优收敛速度仍然难以实现，这主要是由于这种半范数的非单调性。

Method: 通过(i.)将平均误差重铸为涉及非线性扰动的线性递归，以及(ii.)通过将半范数的收缩与适当诱导范数的单调性相结合来驯服非线性。

Result: 为平均奖励和指数折扣设置下的Q学习实现了首个无参数的$\\\tilde{O}(1/\\sqrt{t})$最优速率。

Conclusion: 实现了平均奖励和指数折扣设置下Q学习的首个无参数的$\\\tilde{O}(1/\\sqrt{t})$最优速率。

Abstract: Algorithms for solving \textit{nonlinear} fixed-point equations -- such as
average-reward \textit{$Q$-learning} and \textit{TD-learning} -- often involve
semi-norm contractions. Achieving parameter-free optimal convergence rates for
these methods via Polyak--Ruppert averaging has remained elusive, largely due
to the non-monotonicity of such semi-norms. We close this gap by (i.) recasting
the averaged error as a linear recursion involving a nonlinear perturbation,
and (ii.) taming the nonlinearity by coupling the semi-norm's contraction with
the monotonicity of a suitably induced norm. Our main result yields the first
parameter-free $\tilde{O}(1/\sqrt{t})$ optimal rates for $Q$-learning in both
average-reward and exponentially discounted settings, where $t$ denotes the
iteration index. The result applies within a broad framework that accommodates
synchronous and asynchronous updates, single-agent and distributed deployments,
and data streams obtained either from simulators or along Markovian
trajectories.

</details>


### [160] [Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal](https://arxiv.org/abs/2508.05988)
*Wenhao Zeng,Yaoning Wang,Chao Hu,Yuling Shi,Chengcheng Wan,Hongyu Zhang,Xiaodong Gu*

Main category: cs.LG

TL;DR: ASAP是一种用于压缩CoT的新框架，它通过anchor-guided pruning和逻辑感知的pruning来减少token生成和推理延迟，并在代码生成任务中实现了最先进的精度。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRM）最近通过扩大思维链（CoT）的长度，在代码推理方面表现出了卓越的能力。然而，过长的推理trace在训练成本、推理延迟和部署可行性方面带来了巨大的挑战。虽然已经出现了各种CoT压缩方法来应对这一挑战，但它们面临着固有的权衡：token级别的方法通常会破坏句法和逻辑的一致性，而基于困惑度的step级别方法无法可靠地捕获逻辑上关键的推理步骤。

Method: 我们提出了ASAP（Anchor-guided，Surprisal-based Pruning），这是一种新颖的用于CoT压缩的由粗到精的框架。ASAP首先执行anchor-guided pruning以保留核心推理结构，从而有效地减少了后续处理的搜索空间。然后，它通过基于新颖的first-token surprisal指标选择逻辑上必不可少的推理步骤来实现逻辑感知的pruning。

Result: ASAP在多个代码生成基准测试中实现了最先进的精度，同时大大降低了训练和推理成本。在具有挑战性的LiveCodeBench v4_v5基准测试中，与最强的基线相比，我们的方法减少了 23.5% 的token生成和 43.5% 的推理延迟，同时在 Pass@1 中实现了 36.19% 的竞争精度。

Conclusion: ASAP在多个代码生成基准测试中实现了最先进的精度，同时大大降低了训练和推理成本。在具有挑战性的LiveCodeBench v4_v5基准测试中，与最强的基线相比，我们的方法减少了 23.5% 的token生成和 43.5% 的推理延迟，同时在 Pass@1 中实现了 36.19% 的竞争精度。

Abstract: Recently, Large Reasoning Models (LRMs) have demonstrated remarkable
capabilities in code reasoning by scaling up the length of Chain-of-Thought
(CoT). However, excessively long reasoning traces introduce substantial
challenges in terms of training cost, inference latency, and deployment
feasibility. While various CoT compression approaches have emerged to address
this challenge, they face inherent trade-offs: token-level methods often
disrupt syntactic and logical coherence, while step-level methods based on
perplexity fail to reliably capture the logically critical reasoning steps. In
this paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel
coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided
pruning to preserve the core reasoning structure, which efficiently reduces the
search space for subsequent processing. It then enables a logic-aware pruning
by selecting logically essential reasoning steps based on a novel first-token
surprisal metric. Finally, ASAP teaches models to autonomously generate and
leverage these concise CoTs at inference time, enabling efficient reasoning in
coding tasks. Experiments show that ASAP achieves state-of-the-art accuracy
across multiple code generation benchmarks while substantially reducing
training and inference costs. On the challenging LiveCodeBench v4_v5 benchmark,
our approach reduces token generation by 23.5% and inference latency by 43.5%
compared to the strongest baseline, while achieving a competitive accuracy of
36.19% in Pass@1. Our results highlight a promising direction for building
powerful and efficient LRMs.

</details>


### [161] [Optimizing Prompt Sequences using Monte Carlo Tree Search for LLM-Based Optimization](https://arxiv.org/abs/2508.05995)
*Fei Xu Yu,Gina Adam,Nathaniel D. Bastian,Tian Lan*

Main category: cs.LG

TL;DR: 提出了一种新颖的神经符号框架MCTS-OPS，用于改进代码生成质量和增强LLM在一般优化中的问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在代码生成和结构化推理方面表现出了卓越的能力；然而，它们在需要一致的多步骤规划的复杂任务上的性能通常会下降。

Method: MCTS-OPS，一种新颖的神经符号框架，它将prompt选择表述为由MCTS指导的顺序决策过程。

Result: 在网络优化方面的实验表明，与基线相比，在执行生成的代码的成功率以及具有指定目标和约束的优化结果方面都有显着改进（奖励高出2∼4倍，标准偏差降低3倍）。此外，与难题中的基线方法相比，它将达到最优解的机会提高了约10%。

Conclusion: 结合符号规划与LLM，可以在复杂领域实现稳健、高质量的代码生成。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code generation and structured reasoning; however, their performance often
degrades on complex tasks that require consistent multi-step planning. Recent
work has explored combining LLMs with Monte Carlo Tree Search (MCTS), yet
existing approaches primarily focus on generating heuristic-based code for
optimization or target simpler tasks where correctness alone is sufficient. In
this work, we propose MCTS-OPS, a novel neural-symbolic framework that
formulates prompt selection as a sequential decision process guided by MCTS.
Our method explores and refines multi-step prompt sequences for the goal of
improving code generation quality and enhancing the problem-solving
capabilities of LLMs in general optimization. Experiments on network
optimization show significant improvement over the baselines, both in the
success rate of executing the generated code and in the optimization results
with the specified objective and constraints (2$\sim$4$\times$ higher reward
and 3$\times$ lower standard deviation). Moreover, it improves the chance of
attaining the optimal solution by about 10\% of cases, compared to baseline
methods in hard problems. These results highlight the promise of combining
symbolic planning with LLMs for robust, high-quality code generation in complex
domains.

</details>


### [162] [Stepwise Fine and Gray: Subject-Specific Variable Selection Shows When Hemodynamic Data Improves Prognostication of Comatose Post-Cardiac Arrest Patients](https://arxiv.org/abs/2508.06023)
*Xiaobin Shen,Jonathan Elmer,George H. Chen*

Main category: cs.LG

TL;DR: A new model improves neurological outcome prediction for comatose post-cardiac arrest patients by using time-invariant and time-varying features in a dynamic competing risks framework.


<details>
  <summary>Details</summary>
Motivation: Prognostication for comatose post-cardiac arrest patients is a critical challenge. Clinical information is collected serially in two phases: time-invariant baseline features and time-varying hemodynamic data.

Method: A novel stepwise dynamic competing risks model is proposed, extending the Fine and Gray model and incorporating neural networks.

Result: The model improves the prediction of neurological outcomes by automatically determining when to take advantage of time-invariant and time-varying features. It was evaluated on a retrospective cohort of 2,278 patients and showed robust discriminative performance.

Conclusion: The proposed stepwise dynamic competing risks model demonstrates robust discriminative performance for predicting neurological outcomes in comatose post-cardiac arrest patients, and it can be generalized to more than two phases.

Abstract: Prognostication for comatose post-cardiac arrest patients is a critical
challenge that directly impacts clinical decision-making in the ICU. Clinical
information that informs prognostication is collected serially over time.
Shortly after cardiac arrest, various time-invariant baseline features are
collected (e.g., demographics, cardiac arrest characteristics). After ICU
admission, additional features are gathered, including time-varying hemodynamic
data (e.g., blood pressure, doses of vasopressor medications). We view these as
two phases in which we collect new features. In this study, we propose a novel
stepwise dynamic competing risks model that improves the prediction of
neurological outcomes by automatically determining when to take advantage of
time-invariant features (first phase) and time-varying features (second phase).
Notably, our model finds patients for whom this second phase (time-varying
hemodynamic) information is beneficial for prognostication and also when this
information is beneficial (as we collect more hemodynamic data for a patient
over time, how important these data are for prognostication varies). Our
approach extends the standard Fine and Gray model to explicitly model the two
phases and to incorporate neural networks to flexibly capture complex nonlinear
feature relationships. Evaluated on a retrospective cohort of 2,278 comatose
post-arrest patients, our model demonstrates robust discriminative performance
for the competing outcomes of awakening, withdrawal of life-sustaining therapy,
and death despite maximal support. Our approach generalizes to more than two
phases in which new features are collected and could be used in other dynamic
prediction tasks, where it may be helpful to know when and for whom newly
collected features significantly improve prediction.

</details>


### [163] [Adaptive Heterogeneous Graph Neural Networks: Bridging Heterophily and Heterogeneity](https://arxiv.org/abs/2508.06034)
*Qin Chen,Guojie Song*

Main category: cs.LG

TL;DR: AHGNN解决了异嗜性异构图建模中的挑战，通过异嗜性感知卷积和粗到细的注意力机制，在异嗜性环境中表现出卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多孤立地关注异构性或异嗜性，忽略了异嗜性HG在实际应用中的普遍性。这种忽略导致它们的性能下降。

Method: AHGNN采用了一种异嗜性感知卷积，该卷积考虑了特定于跳和元路径的异嗜性分布。然后，它使用从粗到细的注意力机制整合来自不同语义空间的消息，从而滤除噪声并强调信息信号。

Result: AHGNN在七个真实世界图和二十个基线上的实验表明，AHGNN具有卓越的性能，尤其是在高异嗜性情况下。

Conclusion: AHGNN在七个真实世界图和二十个基线上的实验表明，AHGNN具有卓越的性能，尤其是在高异嗜性情况下。

Abstract: Heterogeneous graphs (HGs) are common in real-world scenarios and often
exhibit heterophily. However, most existing studies focus on either
heterogeneity or heterophily in isolation, overlooking the prevalence of
heterophilic HGs in practical applications. Such ignorance leads to their
performance degradation. In this work, we first identify two main challenges in
modeling heterophily HGs: (1) varying heterophily distributions across hops and
meta-paths; (2) the intricate and often heterophily-driven diversity of
semantic information across different meta-paths. Then, we propose the Adaptive
Heterogeneous Graph Neural Network (AHGNN) to tackle these challenges. AHGNN
employs a heterophily-aware convolution that accounts for heterophily
distributions specific to both hops and meta-paths. It then integrates messages
from diverse semantic spaces using a coarse-to-fine attention mechanism, which
filters out noise and emphasizes informative signals. Experiments on seven
real-world graphs and twenty baselines demonstrate the superior performance of
AHGNN, particularly in high-heterophily situations.

</details>


### [164] [DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment](https://arxiv.org/abs/2508.06041)
*Sangwoo Kwon,Seong Hoon Seo,Jae W. Lee,Yeonhong Park*

Main category: cs.LG

TL;DR: DP-LLM dynamically adjusts layer precision in LLMs based on input, improving performance-latency trade-off.


<details>
  <summary>Details</summary>
Motivation: Effectively handle queries for on-device large language models (LLMs) with varying runtime constraints, such as latency and accuracy.

Method: DP-LLM dynamically assigns precision to each layer based on input values using a precision selector with a lightweight error estimator and learned threshold values.

Result: Demonstrates that DP-LLM achieves a superior performance-latency trade-off, outperforming prior approaches.

Conclusion: DP-LLM achieves a superior performance-latency trade-off, outperforming prior approaches.

Abstract: How can we effectively handle queries for on-device large language models
(LLMs) with varying runtime constraints, such as latency and accuracy?
Multi-scale quantization addresses this challenge by enabling memory-efficient
runtime model adaptation of LLMs through the overlaying of multiple model
variants quantized to different bitwidths. Meanwhile, an important question
still remains open-ended: how can models be properly configured to match a
target precision or latency? While mixed-precision offers a promising solution,
we take this further by leveraging the key observation that the sensitivity of
each layer dynamically changes across decoding iterations. Building on this
insight, we introduce DP-LLM, a novel mechanism that dynamically assigns
precision to each layer based on input values. DP-LLM augments each linear
layer in an LLM with a precision selector that determines the bitwidth at
runtime using a lightweight error estimator and threshold values learned
through fine-tuning. Experimental results across multiple models and benchmarks
demonstrate that DP-LLM achieves a superior performance-latency trade-off,
outperforming prior approaches.

</details>


### [165] [Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology](https://arxiv.org/abs/2508.06066)
*Barak Gahtan,Alex M. Bronstein*

Main category: cs.LG

TL;DR: The paper studies the generalization ability of deep temporal models, introduces generalization bounds and a fair comparison methodology. It finds temporal dependence can enhance learning but gaps exist between theory and practice.


<details>
  <summary>Details</summary>
Motivation: Theoretical understanding of the generalization of deep temporal architectures such as Temporal Convolutional Networks (TCNs) remains limited.

Method: Providing both the first non-vacuous, architecture-aware generalization bounds for deep temporal models and a principled evaluation methodology; delayed-feedback blocking mechanism transforms dependent samples into effectively independent ones

Result: Bounds scaling as  O(R*sqrt((D*p*n*log N)/N)), where D is network depth, p kernel size, n input dimension, and R weight norm; under N_eff=2,000, strongly dependent sequences exhibit approximately 76% smaller generalization gaps than weakly dependent ones, weak dependencies follow N_eff**(-1.21) scaling and strong dependencies follow N_eff**(-0.89).

Conclusion: Temporal dependence can enhance learning under fixed information budgets, while highlighting gaps between theory and practice that motivate future research.

Abstract: Deep temporal architectures such as Temporal Convolutional Networks (TCNs)
achieve strong predictive performance on sequential data, yet theoretical
understanding of their generalization remains limited. We address this gap by
providing both the first non-vacuous, architecture-aware generalization bounds
for deep temporal models and a principled evaluation methodology.
  For exponentially $\beta$-mixing sequences, we derive bounds scaling as $
O\!\Bigl(R\,\sqrt{\tfrac{D\,p\,n\,\log N}{N}}\Bigr), $ where $D$ is network
depth, $p$ kernel size, $n$ input dimension, and $R$ weight norm. Our
delayed-feedback blocking mechanism transforms dependent samples into
effectively independent ones while discarding only $O(1/\log N)$ of the data,
yielding $\sqrt{D}$ scaling instead of exponential, implying that doubling
depth requires approximately quadrupling the training data.
  We also introduce a fair-comparison methodology that fixes the effective
sample size to isolate the effect of temporal structure from information
content. Under $N_{\text{eff}}=2{,}000$, strongly dependent sequences
($\rho=0.8$) exhibit $\approx76\%$ smaller generalization gaps than weakly
dependent ones ($\rho=0.2$), challenging the intuition that dependence is
purely detrimental. Yet convergence rates diverge from theory: weak
dependencies follow $N_{\text{eff}}^{-1.21}$ scaling and strong dependencies
follow $N_{\text{eff}}^{-0.89}$, both steeper than the predicted $N^{-0.5}$.
These findings reveal that temporal dependence can enhance learning under fixed
information budgets, while highlighting gaps between theory and practice that
motivate future research.

</details>


### [166] [Recurrent Deep Differentiable Logic Gate Networks](https://arxiv.org/abs/2508.06097)
*Simon Bührer,Andreas Plesner,Till Aczel,Roger Wattenhofer*

Main category: cs.LG

TL;DR: This paper presents the first implementation of Recurrent Deep Differentiable Logic Gate Networks (RDDLGN), combining Boolean operations with recurrent architectures for sequence-to-sequence learning.


<details>
  <summary>Details</summary>
Motivation: While differentiable logic gates have shown promise in feedforward networks, their application to sequential modeling remains unexplored.

Method: The first implementation of Recurrent Deep Differentiable Logic Gate Networks (RDDLGN), combining Boolean operations with recurrent architectures for sequence-to-sequence learning.

Result: Evaluated on WMT'14 English-German translation, RDDLGN achieves 5.00 BLEU and 30.9% accuracy during training, approaching GRU performance (5.41 BLEU) and graceful degradation (4.39 BLEU) during inference.

Conclusion: Recurrent logic-based neural computation is viable, opening research directions for FPGA acceleration in sequential modeling and other recursive network architectures.

Abstract: While differentiable logic gates have shown promise in feedforward networks,
their application to sequential modeling remains unexplored. This paper
presents the first implementation of Recurrent Deep Differentiable Logic Gate
Networks (RDDLGN), combining Boolean operations with recurrent architectures
for sequence-to-sequence learning.
  Evaluated on WMT'14 English-German translation, RDDLGN achieves 5.00 BLEU and
30.9\% accuracy during training, approaching GRU performance (5.41 BLEU) and
graceful degradation (4.39 BLEU) during inference. This work establishes
recurrent logic-based neural computation as viable, opening research directions
for FPGA acceleration in sequential modeling and other recursive network
architectures.

</details>


### [167] [GCHR : Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning](https://arxiv.org/abs/2508.06108)
*Xing Lei,Wenyan Yang,Kaiqiang Ke,Shentao Yang,Xuetao Zhang,Joni Pajarinen,Donglin Wang*

Main category: cs.LG

TL;DR: propose Hindsight Goal-conditioned Regularization (HGR) to maximize experience utilization and achieve better performances


<details>
  <summary>Details</summary>
Motivation: Goal-conditioned reinforcement learning (GCRL) with sparse rewards remains a fundamental challenge in reinforcement learning. While hindsight experience replay (HER) has shown promise by relabeling collected trajectories with achieved goals, we argue that trajectory relabeling alone does not fully exploit the available experiences in off-policy GCRL methods, resulting in limited sample efficiency.

Method: Hindsight Goal-conditioned Regularization (HGR), a technique that generates action regularization priors based on hindsight goals. When combined with hindsight self-imitation regularization (HSR)

Result: achieve substantially more efficient sample reuse and the best performances, which we empirically demonstrate on a suite of navigation and manipulation tasks.

Conclusion: hindsight regularizations achieve substantially more efficient sample reuse and the best performances

Abstract: Goal-conditioned reinforcement learning (GCRL) with sparse rewards remains a
fundamental challenge in reinforcement learning. While hindsight experience
replay (HER) has shown promise by relabeling collected trajectories with
achieved goals, we argue that trajectory relabeling alone does not fully
exploit the available experiences in off-policy GCRL methods, resulting in
limited sample efficiency. In this paper, we propose Hindsight Goal-conditioned
Regularization (HGR), a technique that generates action regularization priors
based on hindsight goals. When combined with hindsight self-imitation
regularization (HSR), our approach enables off-policy RL algorithms to maximize
experience utilization. Compared to existing GCRL methods that employ HER and
self-imitation techniques, our hindsight regularizations achieve substantially
more efficient sample reuse and the best performances, which we empirically
demonstrate on a suite of navigation and manipulation tasks.

</details>


### [168] [Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models](https://arxiv.org/abs/2508.06151)
*Yong Oh Lee,JeeEun Kim,Jung Woo Lee*

Main category: cs.LG

TL;DR: 本研究提出了一种新方法，通过使用具有微调扩散模型的修复技术合成逼真的口腔癌病变来提高诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 在口腔癌诊断中，带注释的数据集的有限可用性经常限制诊断模型的性能，特别是因为训练数据的可变性和不足。

Method: 使用具有微调扩散模型的修复技术合成逼真的口腔癌病变

Result: 我们的分类模型在区分癌性和非癌性组织方面达到了 0.97 的诊断准确率，而我们的检测模型以 0.85 的准确率准确地识别了病变位置。

Conclusion: 该方法验证了合成图像生成在医学诊断中的潜力，并为进一步研究将这些方法扩展到其他类型的癌症诊断铺平了道路。

Abstract: In oral cancer diagnostics, the limited availability of annotated datasets
frequently constrains the performance of diagnostic models, particularly due to
the variability and insufficiency of training data. To address these
challenges, this study proposed a novel approach to enhance diagnostic accuracy
by synthesizing realistic oral cancer lesions using an inpainting technique
with a fine-tuned diffusion model. We compiled a comprehensive dataset from
multiple sources, featuring a variety of oral cancer images. Our method
generated synthetic lesions that exhibit a high degree of visual fidelity to
actual lesions, thereby significantly enhancing the performance of diagnostic
algorithms. The results show that our classification model achieved a
diagnostic accuracy of 0.97 in differentiating between cancerous and
non-cancerous tissues, while our detection model accurately identified lesion
locations with 0.85 accuracy. This method validates the potential for synthetic
image generation in medical diagnostics and paves the way for further research
into extending these methods to other types of cancer diagnostics.

</details>


### [169] [Differentially Private Federated Clustering with Random Rebalancing](https://arxiv.org/abs/2508.06183)
*Xiyuan Yang,Shengyuan Hu,Soyeon Kim,Tian Li*

Main category: cs.LG

TL;DR: RR-Cluster reduces privacy noise in federated clustering by randomly rebalancing cluster assignments, guaranteeing a minimum number of clients assigned to each cluster, resulting in improved privacy/utility tradeoffs.


<details>
  <summary>Details</summary>
Motivation: Federated clustering can be more vulnerable to privacy leakage. Directly applying client-level differentially private (DP) mechanisms to federated clustering could degrade the utilities significantly because the number of clients assigned to the same clusters is uncontrolled, making it difficult to average privacy noise within each cluster.

Method: RR-Cluster: a light-weight add-on to many federated clustering algorithms. Achieves reduced privacy noise via randomly rebalancing cluster assignments, guaranteeing a minimum number of clients assigned to each cluster.

Result: RR-Cluster achieves reduced privacy noise. Provides convergence bounds for RR-Clsuter. Significantly improved privacy/utility tradeoffs across both synthetic and real-world datasets.

Conclusion: RR-Cluster plugged into strong federated clustering algorithms results in significantly improved privacy/utility tradeoffs across both synthetic and real-world datasets.

Abstract: Federated clustering aims to group similar clients into clusters and produce
one model for each cluster. Such a personalization approach typically improves
model performance compared with training a single model to serve all clients,
but can be more vulnerable to privacy leakage. Directly applying client-level
differentially private (DP) mechanisms to federated clustering could degrade
the utilities significantly. We identify that such deficiencies are mainly due
to the difficulties of averaging privacy noise within each cluster (following
standard privacy mechanisms), as the number of clients assigned to the same
clusters is uncontrolled. To this end, we propose a simple and effective
technique, named RR-Cluster, that can be viewed as a light-weight add-on to
many federated clustering algorithms. RR-Cluster achieves reduced privacy noise
via randomly rebalancing cluster assignments, guaranteeing a minimum number of
clients assigned to each cluster. We analyze the tradeoffs between decreased
privacy noise variance and potentially increased bias from incorrect
assignments and provide convergence bounds for RR-Clsuter. Empirically, we
demonstrate the RR-Cluster plugged into strong federated clustering algorithms
results in significantly improved privacy/utility tradeoffs across both
synthetic and real-world datasets.

</details>


### [170] [Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning](https://arxiv.org/abs/2508.06199)
*Mateusz Praski,Jakub Adamczyk,Wojciech Czech*

Main category: cs.LG

TL;DR: This study presents the most extensive comparison of pretrained neural network models, evaluating 25 models across 25 datasets. We assess models spanning various modalities, architectures, and pretraining strategies. We arrive at a surprising result: nearly all neural models show negligible or no improvement over the baseline ECFP molecular fingerprint. Only the CLAMP model performs statistically significantly better than the alternatives.


<details>
  <summary>Details</summary>
Motivation: Embeddings from pretrained neural networks are widely used for molecular property prediction, virtual screening, and small data learning in molecular chemistry.

Method: evaluating 25 models across 25 datasets under a fair comparison framework, using a dedicated hierarchical Bayesian statistical testing model

Result: the CLAMP model performs statistically significantly better than the alternatives

Conclusion: nearly all neural models show negligible or no improvement over the baseline ECFP molecular fingerprint. Only the CLAMP model performs statistically significantly better than the alternatives. These findings raise concerns about the evaluation rigor in existing studies.

Abstract: Pretrained neural networks have attracted significant interest in chemistry
and small molecule drug design. Embeddings from these models are widely used
for molecular property prediction, virtual screening, and small data learning
in molecular chemistry. This study presents the most extensive comparison of
such models to date, evaluating 25 models across 25 datasets. Under a fair
comparison framework, we assess models spanning various modalities,
architectures, and pretraining strategies. Using a dedicated hierarchical
Bayesian statistical testing model, we arrive at a surprising result: nearly
all neural models show negligible or no improvement over the baseline ECFP
molecular fingerprint. Only the CLAMP model, which is also based on molecular
fingerprints, performs statistically significantly better than the
alternatives. These findings raise concerns about the evaluation rigor in
existing studies. We discuss potential causes, propose solutions, and offer
practical recommendations.

</details>


### [171] [Graph Federated Learning for Personalized Privacy Recommendation](https://arxiv.org/abs/2508.06208)
*Ce Na,Kai Yang,Dengzhao Fang,Yu Li,Jingtong Gao,Chengcheng Zhu,Jiale Zhang,Xiaobing Sun,Yi Chang*

Main category: cs.LG

TL;DR: GFed-PP是一种新的图联邦学习框架，用于个性化隐私推荐，它可以适应不同的隐私需求，同时提高推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦推荐系统(FedRecs)假设所有用户对隐私保护都有相同的要求，即他们不上传任何数据到服务器。这些方法忽略了通过利用公开可用的用户数据来增强推荐服务的潜力。在实际应用中，用户可以选择私有或公开。

Method: 提出了一种新颖的用于个性化隐私推荐的图联邦学习(GFed-PP)，它适应不同的隐私需求，同时提高推荐性能。GFed-PP结合了公共用户的交互数据来构建用户-物品交互图，然后用于形成用户关系图。采用轻量级图卷积网络(GCN)来学习每个用户的用户特定个性化物品嵌入。

Result: 实验结果表明，GFed-PP在五个数据集上显著优于现有方法。

Conclusion: GFed-PP显著优于现有方法，在不损害隐私的情况下提供卓越的推荐准确性，并为联邦推荐系统中适应不同的隐私偏好提供了一个实用的解决方案。

Abstract: Federated recommendation systems (FedRecs) have gained significant attention
for providing privacy-preserving recommendation services. However, existing
FedRecs assume that all users have the same requirements for privacy
protection, i.e., they do not upload any data to the server. The approaches
overlook the potential to enhance the recommendation service by utilizing
publicly available user data. In real-world applications, users can choose to
be private or public. Private users' interaction data is not shared, while
public users' interaction data can be shared. Inspired by the issue, this paper
proposes a novel Graph Federated Learning for Personalized Privacy
Recommendation (GFed-PP) that adapts to different privacy requirements while
improving recommendation performance. GFed-PP incorporates the interaction data
of public users to build a user-item interaction graph, which is then used to
form a user relationship graph. A lightweight graph convolutional network (GCN)
is employed to learn each user's user-specific personalized item embedding. To
protect user privacy, each client learns the user embedding and the scoring
function locally. Additionally, GFed-PP achieves optimization of the federated
recommendation framework through the initialization of item embedding on
clients and the aggregation of the user relationship graph on the server.
Experimental results demonstrate that GFed-PP significantly outperforms
existing methods for five datasets, offering superior recommendation accuracy
without compromising privacy. This framework provides a practical solution for
accommodating varying privacy preferences in federated recommendation systems.

</details>


### [172] [Reparameterization Proximal Policy Optimization](https://arxiv.org/abs/2508.06214)
*Hai Zhong,Xun Wang,Zhuoran Li,Longbo Huang*

Main category: cs.LG

TL;DR: 提出了一种新的策略优化算法RPO，它通过结合PPO的优点来稳定RPG的训练，从而提高了样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 重参数化策略梯度(RPG)有望通过利用可微动力学来提高样本效率。然而，一个关键的障碍是其训练不稳定，其中高方差梯度会破坏学习过程。

Method: 提出了一种基于RPG的稳定且具有样本效率的方法，即Reparameterization Proximal Policy Optimization (RPO)。通过优化为RPG量身定制的裁剪替代目标，同时通过Kullback-Leibler (KL)散度正则化进一步稳定，并与现有的方差减少方法完全兼容，RPO实现了多个epochs的稳定样本重用。

Result: RPO实现了卓越的样本效率和强大的性能。

Conclusion: RPO在具有挑战性的运动和操作任务中表现出卓越的样本效率和强大的性能。

Abstract: Reparameterization policy gradient (RPG) is promising for improving sample
efficiency by leveraging differentiable dynamics. However, a critical barrier
is its training instability, where high-variance gradients can destabilize the
learning process. To address this, we draw inspiration from Proximal Policy
Optimization (PPO), which uses a surrogate objective to enable stable sample
reuse in the model-free setting. We first establish a connection between this
surrogate objective and RPG, which has been largely unexplored and is
non-trivial. Then, we bridge this gap by demonstrating that the
reparameterization gradient of a PPO-like surrogate objective can be computed
efficiently using backpropagation through time. Based on this key insight, we
propose Reparameterization Proximal Policy Optimization (RPO), a stable and
sample-efficient RPG-based method. RPO enables multiple epochs of stable sample
reuse by optimizing a clipped surrogate objective tailored for RPG, while being
further stabilized by Kullback-Leibler (KL) divergence regularization and
remaining fully compatible with existing variance reduction methods. We
evaluate RPO on a suite of challenging locomotion and manipulation tasks, where
experiments demonstrate that our method achieves superior sample efficiency and
strong performance.

</details>


### [173] [Epidemic Control on a Large-Scale-Agent-Based Epidemiology Model using Deep Deterministic Policy Gradient](https://arxiv.org/abs/2304.04475)
*Gaurav Deshkar,Jayanta Kshirsagar,Harshal Hayatnagarkar,Janani Venugopalan*

Main category: cs.LG

TL;DR: Using DDPG on a large-scale simulation, this paper finds that no lockdown and targeted vaccination can balance economic and health objectives.


<details>
  <summary>Details</summary>
Motivation: Current research to model and determine an optimal intervention automatically through round-tripping is limited by the simulation objectives, scale, model types that are not suited for intervention studies, and the number of intervention strategies they can explore.

Method: Deep Deterministic Policy Gradient (DDPG) based policy optimization framework on a large-scale (100,000 individual) epidemiological agent-based simulation where we perform multi-objective optimization.

Result: Determined the optimal policy for lockdown and vaccination in a minimalist age-stratified multi-vaccine scenario with a basic simulation for economic activity.

Conclusion: Optimal economy with balanced health objectives (infection, and hospitalization) can be achieved with no lockdown and vaccination (mid-age and elderly).

Abstract: To mitigate the impact of the pandemic, several measures include lockdowns,
rapid vaccination programs, school closures, and economic stimulus. These
interventions can have positive or unintended negative consequences. Current
research to model and determine an optimal intervention automatically through
round-tripping is limited by the simulation objectives, scale (a few thousand
individuals), model types that are not suited for intervention studies, and the
number of intervention strategies they can explore (discrete vs continuous). We
address these challenges using a Deep Deterministic Policy Gradient (DDPG)
based policy optimization framework on a large-scale (100,000 individual)
epidemiological agent-based simulation where we perform multi-objective
optimization. We determine the optimal policy for lockdown and vaccination in a
minimalist age-stratified multi-vaccine scenario with a basic simulation for
economic activity. With no lockdown and vaccination (mid-age and elderly),
results show optimal economy (individuals below the poverty line) with balanced
health objectives (infection, and hospitalization). An in-depth simulation is
needed to further validate our results and open-source our framework.

</details>


### [174] [SCAR: State-Space Compression for AI-Driven Resource Management in 6G-Enabled Vehicular Infotainment Systems](https://arxiv.org/abs/2508.06243)
*Ioan-Sorin Comsa,Purav Shah,Karthik Vaidhyanathan,Deepak Gangadharan,Christof Imhof,Per Bergamin,Aryan Kaushik,Gabriel-Miro Muntean,Ramona Trestian*

Main category: cs.LG

TL;DR: SCAR, an Edge AI-assisted framework, optimizes scheduling and fairness in vehicular infotainment by employing ML-based compression techniques to reduce CQI data size and training 6G-enabled Reinforcement Learning policies.


<details>
  <summary>Details</summary>
Motivation: Traditional Radio Resource Management (RRM) techniques struggle with the increasing volume and complexity of data such as Channel Quality Indicators (CQI) from autonomous vehicles.

Method: SCAR employs ML-based compression techniques (e.g., clustering and RBF networks) to reduce CQI data size while preserving essential features. These compressed states are used to train 6G-enabled Reinforcement Learning policies.

Result: SCAR increases time in feasible scheduling regions by 14% and reduces unfair scheduling time by 15% compared to RL baselines without CQI compression. SAST-based clustering reduces CQI clustering distortion by 10%.

Conclusion: SCAR increases time in feasible scheduling regions and reduces unfair scheduling time compared to RL baselines without CQI compression. SAST-based clustering reduces CQI clustering distortion, confirming its efficiency. SCAR's scalability and fairness benefits for dynamic vehicular networks are demonstrated.

Abstract: The advent of 6G networks opens new possibilities for connected infotainment
services in vehicular environments. However, traditional Radio Resource
Management (RRM) techniques struggle with the increasing volume and complexity
of data such as Channel Quality Indicators (CQI) from autonomous vehicles. To
address this, we propose SCAR (State-Space Compression for AI-Driven Resource
Management), an Edge AI-assisted framework that optimizes scheduling and
fairness in vehicular infotainment. SCAR employs ML-based compression
techniques (e.g., clustering and RBF networks) to reduce CQI data size while
preserving essential features. These compressed states are used to train
6G-enabled Reinforcement Learning policies that maximize throughput while
meeting fairness objectives defined by the NGMN. Simulations show that SCAR
increases time in feasible scheduling regions by 14\% and reduces unfair
scheduling time by 15\% compared to RL baselines without CQI compression.
Furthermore, Simulated Annealing with Stochastic Tunneling (SAST)-based
clustering reduces CQI clustering distortion by 10\%, confirming its
efficiency. These results demonstrate SCAR's scalability and fairness benefits
for dynamic vehicular networks.

</details>


### [175] [AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models](https://arxiv.org/abs/2508.04748)
*Xuan Lin,Long Chen,Yile Wang*

Main category: cs.LG

TL;DR: AttriLens-Mol, an attribute-guided reinforcement learning framework, improves molecular property prediction with LLMs by eliciting relevant molecular attributes, enhancing performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) in molecular property prediction tasks often rely on human-crafted prompts and lack relevance in reasoning.

Method: Attribute-guided reinforcement learning framework (AttriLens-Mol) using format, count, and rationality rewards to steer the model's reasoning.

Result: Training 7B-size models with AttriLens-Mol significantly boosts performance, achieving comparable or better results than supervised fine-tuning and advanced models on both in-distribution and out-of-distribution datasets.

Conclusion: AttriLens-Mol effectively elicits more relevant and predictive molecular attributes, leading to enhanced interpretability and performance for property prediction. Experiments show significant performance boost compared to supervised fine-tuning and advanced models. Extracted attributes yield superior performance in interpretable decision tree models.

Abstract: Large Language Models (LLMs) have shown promise in assisting molecular
property prediction tasks but often rely on human-crafted prompts and
chain-of-thought templates. While recent advanced large reasoning models like
DeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process,
their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol,
an attribute-guided reinforcement learning framework for molecular property
prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1)
a format reward encouraging attribute-based structured output, (2) a count
reward to avoid enumerating irrelevant attributes, and (3) a rationality reward
using advanced LLMs and RDKit to verify the relatedness of the generated
attributes. This approach implicitly elicits the model's inherent knowledge of
relevant molecular attributes during reasoning, enables making predictions for
the molecular property more effectively. Experiments on both in-distribution
and out-of-distribution datasets show that, training both 7B-size
R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our
proposed AttriLens-Mol method significantly boosts the performance, getting
comparable or better results than supervised fine-tuning models
(Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o,
DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the
target property, when used as features for an interpretable decision tree
model, yield superior performance compared to attributes generated by prompting
LLMs. This shows that AttriLens-Mol effectively elicits more relevant and
predictive molecular attributes, leading to enhanced interpretability and
performance for property prediction. We release the code in
https://github.com/szu-tera/AttriLens-Mol.

</details>


### [176] [Membership Inference Attack with Partial Features](https://arxiv.org/abs/2508.06244)
*Xurun Wang,Guangrui Liu,Xinjie Li,Haoyu He,Lin Yao,Weizhe Zhang*

Main category: cs.LG

TL;DR: The paper studies membership inference attacks with partial feature information and proposes a two-stage attack framework called MRAD, which reconstructs missing features and uses anomaly detection to infer membership.


<details>
  <summary>Details</summary>
Motivation: Existing membership inference methods commonly assume that the adversary has full access to the features of the target sample. This assumption, however, does not hold in many real-world scenarios where only partial features information is available, thereby limiting the applicability of these methods. In this work, we study an inference scenario where the adversary observes only partial features of each sample and aims to infer whether this observed subset was present in the training set of the target model. We define this problem as Partial Feature Membership Inference (PFMI).

Method: MRAD (Memory-guided Reconstruction and Anomaly Detection), a two-stage attack framework. In the first stage, MRAD optimizes the unknown feature values to minimize the loss of the sample. In the second stage, it measures the deviation between the reconstructed sample and the training distribution using anomaly detection.

Result: MRAD is effective across a range of datasets, and maintains compatibility with various off-the-shelf anomaly detection techniques. For example, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of the missing features.

Conclusion: MRAD is effective across a range of datasets, and maintains compatibility with various off-the-shelf anomaly detection techniques. For example, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of the missing features.

Abstract: Machine learning models have been shown to be susceptible to membership
inference attack, which can be used to determine whether a given sample appears
in the training data. Existing membership inference methods commonly assume
that the adversary has full access to the features of the target sample. This
assumption, however, does not hold in many real-world scenarios where only
partial features information is available, thereby limiting the applicability
of these methods. In this work, we study an inference scenario where the
adversary observes only partial features of each sample and aims to infer
whether this observed subset was present in the training set of the target
model. We define this problem as Partial Feature Membership Inference (PFMI).
To address this problem, we propose MRAD (Memory-guided Reconstruction and
Anomaly Detection), a two-stage attack framework. In the first stage, MRAD
optimizes the unknown feature values to minimize the loss of the sample. In the
second stage, it measures the deviation between the reconstructed sample and
the training distribution using anomaly detection. Empirical results
demonstrate that MRAD is effective across a range of datasets, and maintains
compatibility with various off-the-shelf anomaly detection techniques. For
example, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of
the missing features.

</details>


### [177] [Near-Optimal Regret for Efficient Stochastic Combinatorial Semi-Bandits](https://arxiv.org/abs/2508.06247)
*Zichun Ye,Runqi Wang,Xutong Liu,Shuai Li*

Main category: cs.LG

TL;DR: 提出了一种新的CMAB算法CMOSS，该算法在计算效率方面优于现有算法，并在后悔值方面达到了最优速率。


<details>
  <summary>Details</summary>
Motivation: UCB类算法存在额外的对数T的遗憾因子，而对抗类算法如EXP3.M和HYBRID则带来显著的计算开销。为了解决这一trade-off。

Method: 提出了一种随机设置下的组合极小极大最优策略（CMOSS）。

Result: CMOSS算法实现了与$\Omega\[ \big( \sqrt{kmT}\big)$下界相匹配的$O\big( (\log k)^2\sqrt{kmT}\big )$的实例独立遗憾值。

Conclusion: CMOSS算法在合成和真实世界数据集上的实验验证表明，在后悔值和运行效率方面始终优于基准算法。

Abstract: The combinatorial multi-armed bandit (CMAB) is a cornerstone of sequential
decision-making framework, dominated by two algorithmic families: UCB-based and
adversarial methods such as follow the regularized leader (FTRL) and online
mirror descent (OMD). However, prominent UCB-based approaches like CUCB suffer
from additional regret factor $\log T$ that is detrimental over long horizons,
while adversarial methods such as EXP3.M and HYBRID impose significant
computational overhead. To resolve this trade-off, we introduce the
Combinatorial Minimax Optimal Strategy in the Stochastic setting (CMOSS). CMOSS
is a computationally efficient algorithm that achieves an instance-independent
regret of $O\big( (\log k)^2\sqrt{kmT}\big )$ under semi-bandit feedback, where
$m$ is the number of arms and $k$ is the maximum cardinality of a feasible
action. Crucially, this result eliminates the dependency on $\log T$ and
matches the established $\Omega\big( \sqrt{kmT}\big)$ lower bound up to
$O\big((\log k)^2\big)$. We then extend our analysis to show that CMOSS is also
applicable to cascading feedback. Experiments on synthetic and real-world
datasets validate that CMOSS consistently outperforms benchmark algorithms in
both regret and runtime efficiency.

</details>


### [178] [In-Training Defenses against Emergent Misalignment in Language Models](https://arxiv.org/abs/2508.06249)
*David Kaczér,Magnus Jørgenvåg,Clemens Vetter,Lucie Flek,Florian Mai*

Main category: cs.LG

TL;DR: 研究了针对 EMA 的训练中保护措施，这些措施对于通过 API 公开微调的提供商来说是实用的。


<details>
  <summary>Details</summary>
Motivation: 即使是小的、特定领域的微调也会导致目标领域之外的有害行为。即使模型权重隐藏在微调 API 之后，这也会使攻击者无意中访问广泛错位的模型，而这很难仅从微调数据中检测到。

Method: 研究了四种训练正则化干预措施：(i) KL 散度正则化到安全参考模型，(ii) 特征空间中的 $\ell_2$ 距离，(iii) 投影到安全子空间 (SafeLoRA)，以及 (iv) 穿插来自通用指令调整数据集的少量安全训练示例。

Result: 评估了这些方法在四个恶意、诱导 EMA 的任务中的紧急错位效应，并评估了这些方法对良性任务的影响。

Conclusion: 讨论了紧急错位研究中的开放性问题。

Abstract: Fine-tuning lets practitioners repurpose aligned large language models (LLMs)
for new domains, yet recent work reveals emergent misalignment (EMA): Even a
small, domain-specific fine-tune can induce harmful behaviors far outside the
target domain. Even in the case where model weights are hidden behind a
fine-tuning API, this gives attackers inadvertent access to a broadly
misaligned model in a way that can be hard to detect from the fine-tuning data
alone. We present the first systematic study of in-training safeguards against
EMA that are practical for providers who expose fine-tuning via an API. We
investigate four training regularization interventions: (i) KL-divergence
regularization toward a safe reference model, (ii) $\ell_2$ distance in feature
space, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving
of a small amount of safe training examples from a general instruct-tuning
dataset. We first evaluate the methods' emergent misalignment effect across
four malicious, EMA-inducing tasks. Second, we assess the methods' impacts on
benign tasks. We conclude with a discussion of open questions in emergent
misalignment research.

</details>


### [179] [Synthetic Data Generation and Differential Privacy using Tensor Networks' Matrix Product States (MPS)](https://arxiv.org/abs/2508.06251)
*Alejandro Moreno R.,Desale Fentaw,Samuel Palmer,Raúl Salles de Padua,Ninad Dixit,Samuel Mugel,Roman Orús,Manuel Radons,Josef Menter,Ali Abedi*

Main category: cs.LG

TL;DR: This paper introduces a method for generating privacy-preserving high-quality synthetic tabular data using Tensor Networks. The results show that MPS outperforms classical models, particularly under strict privacy constraints.


<details>
  <summary>Details</summary>
Motivation: addressing data scarcity, privacy constraints, and the need for diverse datasets in training robust models.

Method: generating privacy-preserving high-quality synthetic tabular data using Tensor Networks, specifically Matrix Product States (MPS).

Result: MPS outperforms classical models, particularly under strict privacy constraints.

Conclusion: MPS is a promising tool for privacy-aware synthetic data generation, offering an interpretable and scalable alternative for secure data sharing.

Abstract: Synthetic data generation is a key technique in modern artificial
intelligence, addressing data scarcity, privacy constraints, and the need for
diverse datasets in training robust models. In this work, we propose a method
for generating privacy-preserving high-quality synthetic tabular data using
Tensor Networks, specifically Matrix Product States (MPS). We benchmark the
MPS-based generative model against state-of-the-art models such as CTGAN, VAE,
and PrivBayes, focusing on both fidelity and privacy-preserving capabilities.
To ensure differential privacy (DP), we integrate noise injection and gradient
clipping during training, enabling privacy guarantees via R\'enyi Differential
Privacy accounting. Across multiple metrics analyzing data fidelity and
downstream machine learning task performance, our results show that MPS
outperforms classical models, particularly under strict privacy constraints.
This work highlights MPS as a promising tool for privacy-aware synthetic data
generation. By combining the expressive power of tensor network representations
with formal privacy mechanisms, the proposed approach offers an interpretable
and scalable alternative for secure data sharing. Its structured design
facilitates integration into sensitive domains where both data quality and
confidentiality are critical.

</details>


### [180] [Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph Smoothness Priors](https://arxiv.org/abs/2508.06257)
*Jielong Lu,Zhihao Wu,Jiajun Yu,Jiajun Bu,Haishuai Wang*

Main category: cs.LG

TL;DR: GTMancer使用图转换器进行多组学癌症亚型分类，利用对比学习嵌入多组学数据，并通过双重注意力机制捕获组学内部和之间的结构图先验，优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常忽略了异构组学之间复杂的耦合，限制了它们解决对精确肿瘤学至关重要的细微癌症亚型异质性的能力。

Method: 该方法利用对比学习将多组学数据嵌入到统一的语义空间中。我们在这个统一的空间中展开多路图优化问题，并引入双重注意系数集，以捕获多组学数据内部和之间结构图先验。

Result: GTMancer优于现有的最先进算法。

Conclusion: GTMancer在七个真实世界的癌症数据集上的实验结果表明，其性能优于现有的最先进算法。

Abstract: Integrating multi-omics datasets through data-driven analysis offers a
comprehensive understanding of the complex biological processes underlying
various diseases, particularly cancer. Graph Neural Networks (GNNs) have
recently demonstrated remarkable ability to exploit relational structures in
biological data, enabling advances in multi-omics integration for cancer
subtype classification. Existing approaches often neglect the intricate
coupling between heterogeneous omics, limiting their capacity to resolve subtle
cancer subtype heterogeneity critical for precision oncology. To address these
limitations, we propose a framework named Graph Transformer for Multi-omics
Cancer Subtype Classification (GTMancer). This framework builds upon the GNN
optimization problem and extends its application to complex multi-omics data.
Specifically, our method leverages contrastive learning to embed multi-omics
data into a unified semantic space. We unroll the multiplex graph optimization
problem in that unified space and introduce dual sets of attention coefficients
to capture structural graph priors both within and among multi-omics data. This
approach enables global omics information to guide the refining of the
representations of individual omics. Empirical experiments on seven real-world
cancer datasets demonstrate that GTMancer outperforms existing state-of-the-art
algorithms.

</details>


### [181] [OM2P: Offline Multi-Agent Mean-Flow Policy](https://arxiv.org/abs/2508.06269)
*Zhuoran Li,Xun Wang,Hai Zhong,Longbo Huang*

Main category: cs.LG

TL;DR: OM2P是一种新的离线MARL算法，它通过单步动作采样、奖励感知优化和减少内存开销来提高效率和稳定性，并在基准测试中表现出卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型和基于流的模型在离线多智能体强化学习中很有前景。然而，将强大的生成模型集成到这个框架中带来了独特的挑战。特别是，由于其迭代生成过程，扩散和基于流的策略的采样效率较低，这使得它们在时间敏感或资源受限的环境中不切实际。

Method: 提出了一种新的离线MARL算法OM2P，以实现高效的单步动作采样。为了解决生成目标和奖励最大化之间的不一致，我们引入了一种奖励感知优化方案，该方案将精心设计的平均流匹配损失与Q函数监督相结合。此外，我们设计了一种广义的时间步长分布和一种无导数估计策略，以减少内存开销并提高训练稳定性。

Result: 在多智能体粒子和MuJoCo基准测试中的经验评估表明，OM2P实现了卓越的性能，GPU内存使用量减少了3.8倍，训练时间缩短了10.8倍。

Conclusion: OM2P成功地将平均流模型集成到离线MARL中，为合作多智能体环境中实用且可扩展的生成策略铺平了道路。

Abstract: Generative models, especially diffusion and flow-based models, have been
promising in offline multi-agent reinforcement learning. However, integrating
powerful generative models into this framework poses unique challenges. In
particular, diffusion and flow-based policies suffer from low sampling
efficiency due to their iterative generation processes, making them impractical
in time-sensitive or resource-constrained settings. To tackle these
difficulties, we propose OM2P (Offline Multi-Agent Mean-Flow Policy), a novel
offline MARL algorithm to achieve efficient one-step action sampling. To
address the misalignment between generative objectives and reward maximization,
we introduce a reward-aware optimization scheme that integrates a
carefully-designed mean-flow matching loss with Q-function supervision.
Additionally, we design a generalized timestep distribution and a
derivative-free estimation strategy to reduce memory overhead and improve
training stability. Empirical evaluations on Multi-Agent Particle and MuJoCo
benchmarks demonstrate that OM2P achieves superior performance, with up to a
3.8x reduction in GPU memory usage and up to a 10.8x speed-up in training time.
Our approach represents the first to successfully integrate mean-flow model
into offline MARL, paving the way for practical and scalable generative
policies in cooperative multi-agent settings.

</details>


### [182] [A Study on Regularization-Based Continual Learning Methods for Indic ASR](https://arxiv.org/abs/2508.06280)
*Gokul Adethya T,S. Jaya Nirmala*

Main category: cs.LG

TL;DR: This paper investigates Continual Learning (CL) for ASR on Indian languages using a subset of the IndicSUPERB benchmark. The results demonstrate CL's effectiveness in mitigating forgetting, making it a promising approach for scalable ASR in diverse Indian languages under realistic constraints.


<details>
  <summary>Details</summary>
Motivation: India’s linguistic diversity poses significant challenges for developing inclusive Automatic Speech Recognition (ASR) systems. Traditional multilingual models, which require simultaneous access to all language data, are impractical due to the sequential arrival of data and privacy constraints. Continual Learning (CL) offers a solution by enabling models to learn new languages sequentially without catastrophically forgetting previously learned knowledge.

Method: Conformer-based hybrid RNN-T/CTC model, initially pretrained on Hindi, which is then incrementally trained on eight additional Indian languages, for a total sequence of nine languages. We evaluate three prominent regularization- and distillation-based CL strategies: Elastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning without Forgetting (LwF)

Result: Performance is analyzed using Word Error Rate (WER) for both RNN-T and CTC paths on clean and noisy data, as well as knowledge retention via Backward Transfer. We also explore the impact of varying the number of training epochs (1, 2, 5, and 10) per task. Results, compared against naive fine-tuning, demonstrate CL’s effectiveness in mitigating forgetting

Conclusion: CL is effective in mitigating forgetting, making it a promising approach for scalable ASR in diverse Indian languages under realistic constraints.

Abstract: Indias linguistic diversity poses significant challenges for developing
inclusive Automatic Speech Recognition (ASR) systems. Traditional multilingual
models, which require simultaneous access to all language data, are impractical
due to the sequential arrival of data and privacy constraints. Continual
Learning (CL) offers a solution by enabling models to learn new languages
sequentially without catastrophically forgetting previously learned knowledge.
This paper investigates CL for ASR on Indian languages using a subset of the
IndicSUPERB benchmark. We employ a Conformer-based hybrid RNN-T/CTC model,
initially pretrained on Hindi, which is then incrementally trained on eight
additional Indian languages, for a total sequence of nine languages. We
evaluate three prominent regularization- and distillation-based CL strategies:
Elastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning
without Forgetting (LwF), selected for their suitability in no-replay,
privacy-conscious scenarios. Performance is analyzed using Word Error Rate
(WER) for both RNN-T and CTC paths on clean and noisy data, as well as
knowledge retention via Backward Transfer. We also explore the impact of
varying the number of training epochs (1, 2, 5, and 10) per task. Results,
compared against naive fine-tuning, demonstrate CLs effectiveness in mitigating
forgetting, making it a promising approach for scalable ASR in diverse Indian
languages under realistic constraints. The code is available at:
https://github.com/FrozenWolf-Cyber/Indic-CL-ASR

</details>


### [183] [Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback](https://arxiv.org/abs/2508.06292)
*Sanja Karilanova,Subhrakanti Dey,Ayça Özçelikkale*

Main category: cs.LG

TL;DR: This paper proposes a novel spiking neuron model that combines a linear SSM state transition with a non-linear feedback mechanism through reset, achieving comparable performance to existing benchmarks in SNNs.


<details>
  <summary>Details</summary>
Motivation: To bridge the gains offered by SNNs and the recent deep SSM models.

Method: We propose a novel multiple-output spiking neuron model that combines a linear, general SSM state transition with a non-linear feedback mechanism through reset.

Result: The experimental results on various tasks, i.e., a keyword spotting task, an event-based vision task and a sequential pattern recognition task, show that our proposed model achieves performance comparable to existing benchmarks in the SNN literature.

Conclusion: The proposed model achieves performance comparable to existing benchmarks in the SNN literature and can overcome instability and enable learning even when the linear part of neuron dynamics is unstable.

Abstract: Neuromorphic computing is an emerging technology enabling low-latency and
energy-efficient signal processing. A key algorithmic tool in neuromorphic
computing is spiking neural networks (SNNs). SNNs are biologically inspired
neural networks which utilize stateful neurons, and provide low-bit data
processing by encoding and decoding information using spikes. Similar to SNNs,
deep state-space models (SSMs) utilize stateful building blocks. However, deep
SSMs, which recently achieved competitive performance in various temporal
modeling tasks, are typically designed with high-precision activation functions
and no reset mechanisms. To bridge the gains offered by SNNs and the recent
deep SSM models, we propose a novel multiple-output spiking neuron model that
combines a linear, general SSM state transition with a non-linear feedback
mechanism through reset. Compared to the existing neuron models for SNNs, our
proposed model clearly conceptualizes the differences between the spiking
function, the reset condition and the reset action. The experimental results on
various tasks, i.e., a keyword spotting task, an event-based vision task and a
sequential pattern recognition task, show that our proposed model achieves
performance comparable to existing benchmarks in the SNN literature. Our
results illustrate how the proposed reset mechanism can overcome instability
and enable learning even when the linear part of neuron dynamics is unstable,
allowing us to go beyond the strictly enforced stability of linear dynamics in
recent deep SSM models.

</details>


### [184] [FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields](https://arxiv.org/abs/2508.06301)
*Junhyeog Yun,Minui Hong,Gunhee Kim*

Main category: cs.LG

TL;DR: Introduces FedMeNF, a federated meta-learning approach for neural fields that preserves privacy and achieves fast optimization with limited data.


<details>
  <summary>Details</summary>
Motivation: learning to map neural fields often requires large amounts of training data and computations, which can be limited to resource-constrained edge devices. One approach to tackle this limitation is to leverage Federated Meta-Learning (FML), but traditional FML approaches suffer from privacy leakage

Method: a novel FML approach called FedMeNF utilizes a new privacy-preserving loss function that regulates privacy leakage in the local meta-optimization

Result: achieves fast optimization speed and robust reconstruction performance, even with few-shot or non-IID data across diverse data modalities, while preserving client data privacy

Conclusion: FedMeNF achieves fast optimization speed and robust reconstruction performance, even with few-shot or non-IID data across diverse data modalities, while preserving client data privacy.

Abstract: Neural fields provide a memory-efficient representation of data, which can
effectively handle diverse modalities and large-scale data. However, learning
to map neural fields often requires large amounts of training data and
computations, which can be limited to resource-constrained edge devices. One
approach to tackle this limitation is to leverage Federated Meta-Learning
(FML), but traditional FML approaches suffer from privacy leakage. To address
these issues, we introduce a novel FML approach called FedMeNF. FedMeNF
utilizes a new privacy-preserving loss function that regulates privacy leakage
in the local meta-optimization. This enables the local meta-learner to optimize
quickly and efficiently without retaining the client's private data. Our
experiments demonstrate that FedMeNF achieves fast optimization speed and
robust reconstruction performance, even with few-shot or non-IID data across
diverse data modalities, while preserving client data privacy.

</details>


### [185] [Unsupervised Partner Design Enables Robust Ad-hoc Teamwork](https://arxiv.org/abs/2508.06336)
*Constantin Ruhdorfer,Matteo Bortoletto,Victor Oei,Anna Penzkofer,Andreas Bulling*

Main category: cs.LG

TL;DR: UPD是一种用于鲁棒的ad-hoc teamwork的无监督多智能体强化学习框架，它自适应地生成训练伙伴，无需预训练伙伴或手动参数调整。


<details>
  <summary>Details</summary>
Motivation: 解决ad-hoc teamwork中的鲁棒性问题，自适应地生成训练伙伴，无需预训练伙伴或手动参数调整。

Method: 通过随机混合自我代理的策略与有偏差的随机行为来构建多样化的伙伴，并使用基于方差的可学习性指标对它们进行评分，该指标优先考虑靠近自我代理当前学习前沿的伙伴。

Result: UPD持续优于基于人口和无人口的基线以及消融实验。在用户研究中，UPD比所有基线都获得了更高的回报，并且被认为更具适应性、更像人类、更好的合作者并且不那么令人沮丧。

Conclusion: UPD在多个基线上表现更好，并且在用户研究中被认为更具适应性、更像人类、更好的合作者并且不那么令人沮丧。

Abstract: We introduce Unsupervised Partner Design (UPD) - a population-free,
multi-agent reinforcement learning framework for robust ad-hoc teamwork that
adaptively generates training partners without requiring pretrained partners or
manual parameter tuning. UPD constructs diverse partners by stochastically
mixing an ego agent's policy with biased random behaviours and scores them
using a variance-based learnability metric that prioritises partners near the
ego agent's current learning frontier. We show that UPD can be integrated with
unsupervised environment design, resulting in the first method enabling fully
unsupervised curricula over both level and partner distributions in a
cooperative setting. Through extensive evaluations on Overcooked-AI and the
Overcooked Generalisation Challenge, we demonstrate that this dynamic partner
curriculum is highly effective: UPD consistently outperforms both
population-based and population-free baselines as well as ablations. In a user
study, we further show that UPD achieves higher returns than all baselines and
was perceived as significantly more adaptive, more human-like, a better
collaborator, and less frustrating.

</details>


### [186] [Introducing Fractional Classification Loss for Robust Learning with Noisy Labels](https://arxiv.org/abs/2508.06346)
*Mert Can Kurucu,Tufan Kumbasar,İbrahim Eksin,Müjde Güzelkaya*

Main category: cs.LG

TL;DR: FCL是一种自适应鲁棒损失，它自动校准其对标签噪声的鲁棒性，在基准数据集上实现了最先进的结果，无需手动调整超参数。


<details>
  <summary>Details</summary>
Motivation: 现有的方法需要大量的、特定于数据集的超参数调整。在本文中，我们介绍了一种自适应鲁棒损失，即分数分类损失(FCL)，它可以在训练过程中自动校准其对标签噪声的鲁棒性。

Method: FCL采用交叉熵损失的分数阶导数作为其主动分量，平均绝对误差作为其被动损失分量。

Result: 分数阶导数阶数μ跨越了一系列损失函数，这些函数在类似mae的鲁棒性和类似ce的快速收敛之间进行插值。FCL可以动态地重塑其损失环境，以在标签噪声下实现有效的分类性能。

Conclusion: FCL在基准数据集上实现了最先进的结果，无需手动调整超参数。

Abstract: Robust loss functions are crucial for training deep neural networks in the
presence of label noise, yet existing approaches require extensive,
dataset-specific hyperparameter tuning. In this work, we introduce Fractional
Classification Loss (FCL), an adaptive robust loss that automatically
calibrates its robustness to label noise during training. Built within the
active-passive loss framework, FCL employs the fractional derivative of the
Cross-Entropy (CE) loss as its active component and the Mean Absolute Error
(MAE) as its passive loss component. With this formulation, we demonstrate that
the fractional derivative order $\mu$ spans a family of loss functions that
interpolate between MAE-like robustness and CE-like fast convergence.
Furthermore, we integrate $\mu$ into the gradient-based optimization as a
learnable parameter and automatically adjust it to optimize the trade-off
between robustness and convergence speed. We reveal that FCL's unique property
establishes a critical trade-off that enables the stable learning of $\mu$:
lower log penalties on difficult or mislabeled examples improve robustness but
impose higher penalties on easy or clean data, reducing model confidence in
them. Consequently, FCL can dynamically reshape its loss landscape to achieve
effective classification performance under label noise. Extensive experiments
on benchmark datasets show that FCL achieves state-of-the-art results without
the need for manual hyperparameter tuning.

</details>


### [187] [Structural Equation-VAE: Disentangled Latent Representations for Tabular Data](https://arxiv.org/abs/2508.06347)
*Ruiyu Zhang,Ce Zhao,Xin Zhao,Lin Nie,Wai-Fung Lam*

Main category: cs.LG

TL;DR: SE-VAE是一种新型的变分自动编码器架构，它通过将测量结构直接嵌入到设计中，从而学习表格数据的可解释潜在表示。


<details>
  <summary>Details</summary>
Motivation: 在深度生成建模中，从表格数据中学习可解释的潜在表示仍然是一个挑战。

Method: SE-VAE (结构方程-变分自动编码器)，一种新颖的架构，它将测量结构直接嵌入到变分自动编码器的设计中。

Result: 在模拟表格数据集上评估了SE-VAE，并使用标准解缠结指标将其性能与一系列领先的基线进行比较。

Conclusion: SE-VAE在因子恢复、可解释性和对有害变异的鲁棒性方面始终优于其他方法。消融结果表明，架构结构而非正则化强度是性能的关键驱动因素。SE-VAE为科学和社会领域中的白盒生成建模提供了一个原则性框架，在这些领域中，潜在结构是理论驱动的，测量有效性至关重要。

Abstract: Learning interpretable latent representations from tabular data remains a
challenge in deep generative modeling. We introduce SE-VAE (Structural
Equation-Variational Autoencoder), a novel architecture that embeds measurement
structure directly into the design of a variational autoencoder. Inspired by
structural equation modeling, SE-VAE aligns latent subspaces with known
indicator groupings and introduces a global nuisance latent to isolate
construct-specific confounding variation. This modular architecture enables
disentanglement through design rather than through statistical regularizers
alone. We evaluate SE-VAE on a suite of simulated tabular datasets and
benchmark its performance against a series of leading baselines using standard
disentanglement metrics. SE-VAE consistently outperforms alternatives in factor
recovery, interpretability, and robustness to nuisance variation. Ablation
results reveal that architectural structure, rather than regularization
strength, is the key driver of performance. SE-VAE offers a principled
framework for white-box generative modeling in scientific and social domains
where latent constructs are theory-driven and measurement validity is
essential.

</details>


### [188] [Geometric-k-means: A Bound Free Approach to Fast and Eco-Friendly k-means](https://arxiv.org/abs/2508.06353)
*Parichit Sharma,Marcin Stanislaw,Hasan Kurban,Oguzhan Kulekci,Mehmet Dalkilic*

Main category: cs.LG

TL;DR: Gk-means: A novel k-means algorithm using geometric principles to improve efficiency and energy economy.


<details>
  <summary>Details</summary>
Motivation: enhance the efficiency and energy economy of the widely utilized k-means algorithm

Method: active utilization of geometric principles, specifically scalar projection, to significantly accelerate the algorithm without sacrificing solution quality. focusing on high expressive data and bypassing low expressive data.

Result: considerable reductions in computational overhead

Conclusion: Gk-means significantly better than traditional and state of the art k-means variants in runtime and distance computations, exhibits better resource efficiency, as evidenced by its reduced energy footprint, placing it as more sustainable alternative.

Abstract: This paper introduces Geometric-k-means (or Gk-means for short), a novel
approach that significantly enhances the efficiency and energy economy of the
widely utilized k-means algorithm, which, despite its inception over five
decades ago, remains a cornerstone in machine learning applications. The
essence of Gk-means lies in its active utilization of geometric principles,
specifically scalar projection, to significantly accelerate the algorithm
without sacrificing solution quality. This geometric strategy enables a more
discerning focus on data points that are most likely to influence cluster
updates, which we call as high expressive data (HE). In contrast, low
expressive data (LE), does not impact clustering outcome, is effectively
bypassed, leading to considerable reductions in computational overhead.
Experiments spanning synthetic, real-world and high-dimensional datasets,
demonstrate Gk-means is significantly better than traditional and state of the
art (SOTA) k-means variants in runtime and distance computations (DC).
Moreover, Gk-means exhibits better resource efficiency, as evidenced by its
reduced energy footprint, placing it as more sustainable alternative.

</details>
