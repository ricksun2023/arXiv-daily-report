<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 1]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Preliminary Study of RAG for Taiwanese Historical Archives](https://arxiv.org/abs/2511.07445)
*Claire Lin,Bo-Han Feng,Xuanjun Chen,Te-Lun Yang,Hung-yi Lee,Jyh-Shing Roger Jang*

Main category: cs.CL

TL;DR: 本文研究了检索增强生成（RAG）在台湾历史档案中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究动机是现有研究较少关注RAG在台湾历史档案中的应用。

Method: 该研究将RAG流程应用于两个繁体中文历史数据集，并系统地研究了查询特征和元数据集成策略对检索质量、答案生成和系统整体性能的影响。

Result: 结果表明，早期元数据集成提高了检索和答案的准确性。

Conclusion: RAG系统仍然存在挑战，包括生成过程中的幻觉以及处理时间或多跳历史查询的困难。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach for knowledge-intensive tasks. However, few studies have examined RAG for Taiwanese Historical Archives. In this paper, we present an initial study of a RAG pipeline applied to two historical Traditional Chinese datasets, Fort Zeelandia and the Taiwan Provincial Council Gazette, along with their corresponding open-ended query sets. We systematically investigate the effects of query characteristics and metadata integration strategies on retrieval quality, answer generation, and the performance of the overall system. The results show that early-stage metadata integration enhances both retrieval and answer accuracy while also revealing persistent challenges for RAG systems, including hallucinations during generation and difficulties in handling temporal or multi-hop historical queries.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [2] [Analysing Environmental Efficiency in AI for X-Ray Diagnosis](https://arxiv.org/abs/2511.07436)
*Liam Kearns*

Main category: cs.AI

TL;DR: 本研究探讨了大型语言模型（LLM）和小型判别模型在检测 Covid-19 胸部 X 光片中的应用，并比较了它们的准确性和环境影响。


<details>
  <summary>Details</summary>
Motivation: 在医学应用中集成人工智能工具旨在提高诊断效率，而大型语言模型（LLM）的出现进一步扩展了这种集成。由于 LLM 的多功能性和通过 API 易于使用，这些更大的模型经常被使用，即使可以使用更小的定制模型。

Method: 将 LLM 和小型判别模型集成到 Mendix 应用程序中，以检测胸部 X 光片中的 Covid-19。这些判别模型还用于为 LLM 提供知识库，以提高准确性。对 14 种不同的模型配置进行基准研究，以比较准确性和环境影响。

Result: 研究结果表明，虽然较小的模型降低了应用程序的碳足迹，但输出结果偏向于阳性诊断，并且输出概率缺乏信心。同时，限制 LLM 仅提供概率输出会导致准确性和碳足迹方面的性能不佳，这表明了使用 LLM 作为通用 AI 解决方案的风险。虽然使用较小的 LLM GPT-4.1-Nano 将碳足迹比更大的模型减少了 94.2%，但这与判别模型相比仍然不成比例；最有效的解决方案是 Covid-Net 模型。

Conclusion: 通过比较 Covid-19 检测中的生成模型和判别模型，以及强调使用生成工具进行分类任务的环境风险，为知识做出了贡献。

Abstract: The integration of AI tools into medical applications has aimed to improve the efficiency of diagnosis. The emergence of large language models (LLMs), such as ChatGPT and Claude, has expanded this integration even further. Because of LLM versatility and ease of use through APIs, these larger models are often utilised even though smaller, custom models can be used instead. In this paper, LLMs and small discriminative models are integrated into a Mendix application to detect Covid-19 in chest X-rays. These discriminative models are also used to provide knowledge bases for LLMs to improve accuracy. This provides a benchmark study of 14 different model configurations for comparison of accuracy and environmental impact. The findings indicated that while smaller models reduced the carbon footprint of the application, the output was biased towards a positive diagnosis and the output probabilities were lacking confidence. Meanwhile, restricting LLMs to only give probabilistic output caused poor performance in both accuracy and carbon footprint, demonstrating the risk of using LLMs as a universal AI solution. While using the smaller LLM GPT-4.1-Nano reduced the carbon footprint by 94.2% compared to the larger models, this was still disproportionate to the discriminative models; the most efficient solution was the Covid-Net model. Although it had a larger carbon footprint than other small models, its carbon footprint was 99.9% less than when using GPT-4.5-Preview, whilst achieving an accuracy of 95.5%, the highest of all models examined. This paper contributes to knowledge by comparing generative and discriminative models in Covid-19 detection as well as highlighting the environmental risk of using generative tools for classification tasks.

</details>


### [3] [Agentic Educational Content Generation for African Languages on Edge Devices](https://arxiv.org/abs/2511.07437)
*Ravi Gupta,Guneet Bhatia*

Main category: cs.AI

TL;DR: 本文提出了一种自主代理编排框架，用于在边缘设备上生成去中心化的、文化自适应的教育内容，旨在解决撒哈拉以南非洲的教育不公平问题。


<details>
  <summary>Details</summary>
Motivation: 解决撒哈拉以南非洲的教育不公平问题，为资源有限的环境提供可访问、本地化和可持续的AI驱动教育。

Method: 利用四个专门的代理协同工作，生成符合上下文的教育内容，并在Raspberry Pi 4B和NVIDIA Jetson Nano等平台上进行实验验证。

Result: 在Jetson Nano上，InkubaLM实现了129毫秒的首个令牌时间（TTFT）、33毫秒的平均令牌间延迟和每秒45.2个令牌的吞吐量，功耗为8.4W。在Raspberry Pi 4B上，InkubaLM也表现出色，TTFT为326毫秒，每秒15.9个令牌，功耗为5.8W。该框架在多种非洲语言中始终保持高质量，平均BLEU分数为0.688，文化相关性为4.4/5，流畅性为4.2/5。

Conclusion: 该研究通过与非洲青年和社区组织（AYCO）和佛罗里达非洲基金会等社区组织合作，旨在为在资源受限环境中实现可访问、本地化和可持续的AI驱动教育奠定实践基础，并为联合国可持续发展目标4、9和10做出贡献。

Abstract: Addressing educational inequity in Sub-Saharan Africa, this research presents an autonomous agent-orchestrated framework for decentralized, culturally adaptive educational content generation on edge devices. The system leverages four specialized agents that work together to generate contextually appropriate educational content. Experimental validation on platforms including Raspberry Pi 4B and NVIDIA Jetson Nano demonstrates significant performance achievements. InkubaLM on Jetson Nano achieved a Time-To-First-Token (TTFT) of 129 ms, an average inter-token latency of 33 ms, and a throughput of 45.2 tokens per second while consuming 8.4 W. On Raspberry Pi 4B, InkubaLM also led with 326 ms TTFT and 15.9 tokens per second at 5.8 W power consumption. The framework consistently delivered high multilingual quality, averaging a BLEU score of 0.688, cultural relevance of 4.4/5, and fluency of 4.2/5 across tested African languages. Through potential partnerships with active community organizations including African Youth & Community Organization (AYCO) and Florida Africa Foundation, this research aims to establish a practical foundation for accessible, localized, and sustainable AI-driven education in resource-constrained environments. Keeping focus on long-term viability and cultural appropriateness, it contributes to United Nations SDGs 4, 9, and 10. Index Terms - Multi-Agent Systems, Edge AI Computing, Educational Technology, African Languages, Rural Education, Sustainable Development, UN SDG.

</details>


### [4] [Beyond Correctness: Confidence-Aware Reward Modeling for Enhancing Large Language Model Reasoning](https://arxiv.org/abs/2511.07483)
*Qianxi He,Qingyu Ren,Shanzhe Lei,Xuhong Wang,Yingchun Wang*

Main category: cs.AI

TL;DR: 提出了一种基于置信度的奖励模型，以增强 STEM 推理能力，通过惩罚不正确的答案和低置信度的正确答案，从而促进更强大和逻辑上一致的推理。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型后训练范式，即基于规则的强化学习，通常会导致低质量的推理链或推理过程与最终答案之间的不一致，尤其是在基础模型规模较小的情况下。由于缺乏知识，模型可能会采用低质量的推理链，偶尔会随机产生正确的答案，并根据已建立的基于规则的判断获得奖励。这限制了资源有限的组织对较小规模的模型进行直接强化学习训练的潜力。

Method: 提出一种新颖的基于置信度的奖励模型，该模型不仅惩罚不正确的答案，还惩罚低置信度的正确响应，从而促进更强大和逻辑上一致的推理。

Result: 通过静态评估、Best-of-N 推理测试和基于 PPO 的 RL 训练验证了该方法的有效性。该方法在各种 STEM 基准测试中优于几种最先进的开源奖励模型。

Conclusion: 该研究提出了一种基于置信度的奖励模型，可以有效提高小规模语言模型在 STEM 领域的推理能力。

Abstract: Recent advancements in large language models (LLMs) have shifted the post-training paradigm from traditional instruction tuning and human preference alignment toward reinforcement learning (RL) focused on reasoning capabilities. However, numerous technical reports indicate that purely rule-based reward RL frequently results in poor-quality reasoning chains or inconsistencies between reasoning processes and final answers, particularly when the base model is of smaller scale. During the RL exploration process, models might employ low-quality reasoning chains due to the lack of knowledge, occasionally producing correct answers randomly and receiving rewards based on established rule-based judges. This constrains the potential for resource-limited organizations to conduct direct reinforcement learning training on smaller-scale models. We propose a novel confidence-based reward model tailored for enhancing STEM reasoning capabilities. Unlike conventional approaches, our model penalizes not only incorrect answers but also low-confidence correct responses, thereby promoting more robust and logically consistent reasoning. We validate the effectiveness of our approach through static evaluations, Best-of-N inference tests, and PPO-based RL training. Our method outperforms several state-of-the-art open-source reward models across diverse STEM benchmarks. We release our codes and model in https://github.com/qianxiHe147/C2RM.

</details>


### [5] [Procedural Knowledge Improves Agentic LLM Workflows](https://arxiv.org/abs/2511.07568)
*Vincent Hsiao,Mark Roberts,Leslie Smith*

Main category: cs.AI

TL;DR: 该研究探讨了程序性知识（以分层任务网络HTN的形式）如何提高大型语言模型（LLM）在需要隐式规划的代理任务中的性能。实验表明，手工编码的HTN可以显著提高LLM的性能，并且LLM创建的HTN也能提高整体性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在执行代理任务时，如果没有大量的工具支持、提示工程或微调，通常会遇到困难。

Method: 该研究形式化、实施并评估了一种利用程序性知识（以分层任务网络HTN的形式）的代理LLM工作流程。

Result: 实验结果表明，手工编码的HTN可以显著提高LLM在代理任务中的性能，并且使用HTN可以使20b或70b参数的LLM胜过更大的120b参数的LLM基线。此外，LLM创建的HTN可以提高整体性能，尽管效果稍差。

Conclusion: 研究结果表明，利用来自人类、文档或LLM的专业知识来管理程序性知识将成为提高LLM工作流程的另一个重要工具。

Abstract: Large language models (LLMs) often struggle when performing agentic tasks without substantial tool support, prom-pt engineering, or fine tuning. Despite research showing that domain-dependent, procedural knowledge can dramatically increase planning efficiency, little work evaluates its potential for improving LLM performance on agentic tasks that may require implicit planning. We formalize, implement, and evaluate an agentic LLM workflow that leverages procedural knowledge in the form of a hierarchical task network (HTN). Empirical results of our implementation show that hand-coded HTNs can dramatically improve LLM performance on agentic tasks, and using HTNs can boost a 20b or 70b parameter LLM to outperform a much larger 120b parameter LLM baseline. Furthermore, LLM-created HTNs improve overall performance, though less so. The results suggest that leveraging expertise--from humans, documents, or LLMs--to curate procedural knowledge will become another important tool for improving LLM workflows.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [6] [Cortex AISQL: A Production SQL Engine for Unstructured Data](https://arxiv.org/abs/2511.07663)
*Paritosh Aggarwal,Bowei Chen,Anupam Datta,Benjamin Han,Boxin Jiang,Nitish Jindal,Zihan Li,Aaron Lin,Pawel Liskowski,Jay Tayade,Dimitris Tsirogiannis,Nathan Wiegand,Weicheng Zhao*

Main category: cs.DB

TL;DR: Snowflake的Cortex AISQL将语义操作集成到SQL中，允许用户查询结构化和非结构化数据。它通过AI感知查询优化、自适应模型级联和语义连接查询重写等技术，优化AI推理成本，提高查询效率和预测质量。


<details>
  <summary>Details</summary>
Motivation: 现有的查询引擎没有针对语义操作进行优化，而语义操作比传统SQL操作更昂贵，延迟和吞吐量特性不同，并且在查询编译期间成本和选择性未知。

Method: 该引擎采用三种新技术：AI感知查询优化（将AI推理成本视为首要优化目标）、自适应模型级联（通过快速代理模型路由大多数行，并将不确定情况升级到强大的oracle模型）和语义连接查询重写（将连接操作的二次时间复杂度降低到线性）。

Result: AI感知查询优化实现了2-8倍的加速，自适应模型级联在保持90-95%的oracle模型质量的同时实现了2-6倍的加速，语义连接查询重写实现了15-70倍的加速，并且通常提高了预测质量。

Conclusion: AISQL已在Snowflake中部署，为分析、搜索和内容理解等各种客户工作负载提供支持。

Abstract: Snowflake's Cortex AISQL is a production SQL engine that integrates native semantic operations directly into SQL. This integration allows users to write declarative queries that combine relational operations with semantic reasoning, enabling them to query both structured and unstructured data effortlessly. However, making semantic operations efficient at production scale poses fundamental challenges. Semantic operations are more expensive than traditional SQL operations, possess distinct latency and throughput characteristics, and their cost and selectivity are unknown during query compilation. Furthermore, existing query engines are not designed to optimize semantic operations. The AISQL query execution engine addresses these challenges through three novel techniques informed by production deployment data from Snowflake customers. First, AI-aware query optimization treats AI inference cost as a first-class optimization objective, reasoning about large language model (LLM) cost directly during query planning to achieve 2-8$\times$ speedups. Second, adaptive model cascades reduce inference costs by routing most rows through a fast proxy model while escalating uncertain cases to a powerful oracle model, achieving 2-6$\times$ speedups while maintaining 90-95% of oracle model quality. Third, semantic join query rewriting lowers the quadratic time complexity of join operations to linear through reformulation as multi-label classification tasks, achieving 15-70$\times$ speedups with often improved prediction quality. AISQL is deployed in production at Snowflake, where it powers diverse customer workloads across analytics, search, and content understanding.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [7] [A Hybrid Multimodal Deep Learning Framework for Intelligent Fashion Recommendation](https://arxiv.org/abs/2511.07573)
*Kamand Kalashi,Babak Teimourpour*

Main category: cs.IR

TL;DR: 提出了一种混合多模态深度学习框架，用于时尚推荐，可以同时处理服装搭配预测和互补商品检索。


<details>
  <summary>Details</summary>
Motivation: 在线时尚平台的快速扩张，越来越需要能够理解视觉和文本线索的智能推荐系统。

Method: 利用CLIP架构的视觉和文本编码器来获得时尚商品的联合潜在表示，然后将其整合到一个统一的特征向量中，并通过Transformer编码器进行处理。引入“服装token”来建模商品之间的整体关系，并使用代表所需商品描述的“目标商品token”来检索兼容商品。

Result: 在Polyvore数据集上，搭配预测的AUC达到0.95。在填空（FITB）指标下，互补商品检索的准确率达到69.24%。

Conclusion: 该方法在两项任务中都表现出强大的性能，突出了多模态学习对时尚推荐的有效性。

Abstract: The rapid expansion of online fashion platforms has created an increasing demand for intelligent recommender systems capable of understanding both visual and textual cues. This paper proposes a hybrid multimodal deep learning framework for fashion recommendation that jointly addresses two key tasks: outfit compatibility prediction and complementary item retrieval. The model leverages the visual and textual encoders of the CLIP architecture to obtain joint latent representations of fashion items, which are then integrated into a unified feature vector and processed by a transformer encoder. For compatibility prediction, an "outfit token" is introduced to model the holistic relationships among items, achieving an AUC of 0.95 on the Polyvore dataset. For complementary item retrieval, a "target item token" representing the desired item description is used to retrieve compatible items, reaching an accuracy of 69.24% under the Fill-in-the-Blank (FITB) metric. The proposed approach demonstrates strong performance across both tasks, highlighting the effectiveness of multimodal learning for fashion recommendation.

</details>
