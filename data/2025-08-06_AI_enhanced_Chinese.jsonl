{"id": "2508.03471", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2508.03471", "abs": "https://arxiv.org/abs/2508.03471", "authors": ["Suvam Kumar Das", "Suprio Ray"], "title": "Learned Adaptive Indexing", "comment": null, "summary": "Indexes can significantly improve search performance in relational databases.\nHowever, if the query workload changes frequently or new data updates occur\ncontinuously, it may not be worthwhile to build a conventional index upfront\nfor query processing. Adaptive indexing is a technique in which an index gets\nbuilt on the fly as a byproduct of query processing. In recent years, research\nin database indexing has taken a new direction where machine learning models\nare employed for the purpose of indexing. These indexes, known as learned\nindexes, can be more efficient compared to traditional indexes such as B+-tree\nin terms of memory footprints and query performance. However, a learned index\nhas to be constructed upfront and requires training the model in advance, which\nbecomes a challenge in dynamic situations when workload changes frequently. To\nthe best of our knowledge, no learned indexes exist yet for adaptive indexing.\nWe propose a novel learned approach for adaptive indexing. It is built on the\nfly as queries are submitted and utilizes learned models for indexing data. To\nenhance query performance, we employ a query workload prediction technique that\nmakes future workload projection based on past workload data. We have evaluated\nour learned adaptive indexing approach against existing adaptive indexes for\nvarious query workloads. Our results show that our approach performs better\nthan others in most cases, offering 1.2x - 5.6x improvement in query\nperformance.", "AI": {"tldr": "This paper introduces a novel learned adaptive indexing approach that builds indexes on the fly using machine learning models and query workload prediction, achieving significant performance improvements compared to existing adaptive indexes.", "motivation": "Conventional indexes may not be worthwhile in dynamic situations when workload changes frequently. Existing learned indexes have to be constructed upfront and require training the model in advance, posing a challenge in dynamic situations. No learned indexes exist yet for adaptive indexing.", "method": "A novel learned approach for adaptive indexing is built on the fly as queries are submitted and utilizes learned models for indexing data. A query workload prediction technique is employed to make future workload projection based on past workload data.", "result": "The proposed approach performs better than others in most cases, offering 1.2x - 5.6x improvement in query performance.", "conclusion": "The proposed learned adaptive indexing approach outperforms existing adaptive indexes in most cases, offering 1.2x - 5.6x improvement in query performance."}}
{"id": "2508.03565", "categories": ["cs.DB", "H.2.0"], "pdf": "https://arxiv.org/pdf/2508.03565", "abs": "https://arxiv.org/abs/2508.03565", "authors": ["Junfeng Liu", "Haoxuan Xie", "Siqiang Luo"], "title": "[Technical Report] ArceKV: Towards Workload-driven LSM-compactions for Key-Value Store Under Dynamic Workloads", "comment": "17 pages, 11 figures", "summary": "Key-value stores underpin a wide range of applications due to their\nsimplicity and efficiency. Log-Structured Merge Trees (LSM-trees) dominate as\ntheir underlying structure, excelling at handling rapidly growing data. Recent\nresearch has focused on optimizing LSM-tree performance under static workloads\nwith fixed read-write ratios. However, real-world workloads are highly dynamic,\nand existing workload-aware approaches often struggle to sustain optimal\nperformance or incur substantial transition overhead when workload patterns\nshift. To address this, we propose ElasticLSM, which removes traditional\nLSM-tree structural constraints to allow more flexible management actions\n(i.e., compactions and write stalls) creating greater opportunities for\ncontinuous performance optimization. We further design Arce, a lightweight\ncompaction decision engine that guides ElasticLSM in selecting the optimal\naction from its expanded action space. Building on these components, we\nimplement ArceKV, a full-fledged key-value store atop RocksDB. Extensive\nevaluations demonstrate that ArceKV outperforms state-of-the-art compaction\nstrategies across diverse workloads, delivering around 3x faster performance in\ndynamic scenarios.", "AI": {"tldr": "ElasticLSM \u79fb\u9664\u4f20\u7edf LSM \u6811\u7ed3\u6784\u7ea6\u675f\uff0cArce \u6307\u5bfc ElasticLSM \u4ece\u5176\u6269\u5c55\u7684\u884c\u52a8\u7a7a\u95f4\u4e2d\u9009\u62e9\u6700\u4f73\u884c\u52a8\u3002ArceKV \u5728\u52a8\u6001\u573a\u666f\u4e2d\u6027\u80fd\u63d0\u5347\u4e86 3 \u500d\u3002", "motivation": "\u7531\u4e8e\u5176\u7b80\u5355\u6027\u548c\u6548\u7387\uff0c\u952e\u503c\u5b58\u50a8\u662f\u5404\u79cd\u5e94\u7528\u7a0b\u5e8f\u7684\u57fa\u7840\u3002\u65e5\u5fd7\u7ed3\u6784\u5408\u5e76\u6811 (LSM \u6811) \u4f5c\u4e3a\u5176\u5e95\u5c42\u7ed3\u6784\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\uff0c\u64c5\u957f\u5904\u7406\u5feb\u901f\u589e\u957f\u7684\u6570\u636e\u3002\u6700\u8fd1\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u4f18\u5316\u5177\u6709\u56fa\u5b9a\u8bfb\u5199\u6bd4\u7387\u7684\u9759\u6001\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684 LSM \u6811\u6027\u80fd\u3002\u7136\u800c\uff0c\u73b0\u5b9e\u4e16\u754c\u7684\u5de5\u4f5c\u8d1f\u8f7d\u662f\u9ad8\u5ea6\u52a8\u6001\u7684\uff0c\u73b0\u6709\u7684\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u65b9\u6cd5\u901a\u5e38\u96be\u4ee5\u7ef4\u6301\u6700\u4f73\u6027\u80fd\uff0c\u6216\u8005\u5728\u5de5\u4f5c\u8d1f\u8f7d\u6a21\u5f0f\u53d1\u751f\u53d8\u5316\u65f6\u4f1a\u4ea7\u751f\u5927\u91cf\u7684\u8f6c\u6362\u5f00\u9500\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86 ElasticLSM\uff0c\u5b83\u6d88\u9664\u4e86\u4f20\u7edf\u7684 LSM \u6811\u7ed3\u6784\u7ea6\u675f\uff0c\u4ee5\u5141\u8bb8\u66f4\u7075\u6d3b\u7684\u7ba1\u7406\u64cd\u4f5c\uff08\u5373\uff0c\u538b\u7f29\u548c\u5199\u6682\u505c\uff09\uff0c\u4ece\u800c\u4e3a\u6301\u7eed\u7684\u6027\u80fd\u4f18\u5316\u521b\u9020\u4e86\u66f4\u591a\u673a\u4f1a\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u8bbe\u8ba1\u4e86 Arce\uff0c\u8fd9\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u538b\u7f29\u51b3\u7b56\u5f15\u64ce\uff0c\u5b83\u6307\u5bfc ElasticLSM \u4ece\u5176\u6269\u5c55\u7684\u884c\u52a8\u7a7a\u95f4\u4e2d\u9009\u62e9\u6700\u4f73\u884c\u52a8\u3002", "result": "ArceKV \u5728\u52a8\u6001\u573a\u666f\u4e2d\u63d0\u4f9b\u4e86\u5927\u7ea6\u5feb 3 \u500d\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u538b\u7f29\u7b56\u7565\u3002", "conclusion": "ArceKV\u5728\u5404\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u538b\u7f29\u7b56\u7565\uff0c\u5728\u52a8\u6001\u573a\u666f\u4e2d\u63d0\u4f9b\u5927\u7ea6\u5feb 3 \u500d\u7684\u6027\u80fd\u3002"}}
{"id": "2508.02929", "categories": ["cs.IR", "cs.AI", "cs.LG", "68T05, 68T07, 68T30", "H.3.3; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.02929", "abs": "https://arxiv.org/abs/2508.02929", "authors": ["Dai Li", "Kevin Course", "Wei Li", "Hongwei Li", "Jie Hua", "Yiqi Chen", "Zhao Zhu", "Rui Jian", "Xuan Cao", "Bi Xue", "Yu Shi", "Jing Qian", "Kai Ren", "Matt Ma", "Qunshu Zhang", "Rui Li"], "title": "Realizing Scaling Laws in Recommender Systems: A Foundation-Expert Paradigm for Hyperscale Model Deployment", "comment": null, "summary": "While scaling laws promise significant performance gains for recommender\nsystems, efficiently deploying hyperscale models remains a major unsolved\nchallenge. In contrast to fields where FMs are already widely adopted such as\nnatural language processing and computer vision, progress in recommender\nsystems is hindered by unique challenges including the need to learn from\nonline streaming data under shifting data distributions, the need to adapt to\ndifferent recommendation surfaces with a wide diversity in their downstream\ntasks and their input distributions, and stringent latency and computational\nconstraints. To bridge this gap, we propose to leverage the Foundation-Expert\nParadigm: a framework designed for the development and deployment of hyperscale\nrecommendation FMs. In our approach, a central FM is trained on lifelong,\ncross-surface, multi-modal user data to learn generalizable knowledge. This\nknowledge is then efficiently transferred to various lightweight,\nsurface-specific ``expert\" models via target-aware embeddings, allowing them to\nadapt to local data distributions and optimization goals with minimal overhead.\nTo meet our training, inference and development needs, we built HyperCast, a\nproduction-grade infrastructure system that re-engineers training, serving,\nlogging and iteration to power this decoupled paradigm. Our approach is now\ndeployed at Meta serving tens of billions of user requests daily, demonstrating\nonline metric improvements over our previous one-stage production system while\nimproving developer velocity and maintaining infrastructure efficiency. To the\nbest of our knowledge, this work represents the first successful deployment of\na Foundation-Expert paradigm at this scale, offering a proven,\ncompute-efficient, and developer-friendly blueprint to realize the promise of\nscaling laws in recommender systems.", "AI": {"tldr": "Propose and deploy the Foundation-Expert Paradigm to address the challenges of deploying hyperscale models in recommender systems. Achieved online metric improvements and improved developer velocity at Meta.", "motivation": "Efficiently deploying hyperscale models remains a major unsolved challenge. Progress in recommender systems is hindered by unique challenges including the need to learn from online streaming data under shifting data distributions, the need to adapt to different recommendation surfaces with a wide diversity in their downstream tasks and their input distributions, and stringent latency and computational constraints.", "method": "Leverage the Foundation-Expert Paradigm: a framework designed for the development and deployment of hyperscale recommendation FMs. In our approach, a central FM is trained on lifelong, cross-surface, multi-modal user data to learn generalizable knowledge. This knowledge is then efficiently transferred to various lightweight, surface-specific ``expert\" models via target-aware embeddings, allowing them to adapt to local data distributions and optimization goals with minimal overhead.", "result": "Demonstrating online metric improvements over our previous one-stage production system while improving developer velocity and maintaining infrastructure efficiency. The approach is now deployed at Meta serving tens of billions of user requests daily.", "conclusion": "This work represents the first successful deployment of a Foundation-Expert paradigm at this scale, offering a proven, compute-efficient, and developer-friendly blueprint to realize the promise of scaling laws in recommender systems."}}
{"id": "2508.02808", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02808", "abs": "https://arxiv.org/abs/2508.02808", "authors": ["Radhika Dua", "Young Joon", "Kwon", "Siddhant Dogra", "Daniel Freedman", "Diana Ruan", "Motaz Nashawaty", "Danielle Rigau", "Daniel Alexander Alber", "Kang Zhang", "Kyunghyun Cho", "Eric Karl Oermann"], "title": "Clinically Grounded Agent-based Report Evaluation: An Interpretable Metric for Radiology Report Generation", "comment": null, "summary": "Radiological imaging is central to diagnosis, treatment planning, and\nclinical decision-making. Vision-language foundation models have spurred\ninterest in automated radiology report generation (RRG), but safe deployment\nrequires reliable clinical evaluation of generated reports. Existing metrics\noften rely on surface-level similarity or behave as black boxes, lacking\ninterpretability. We introduce ICARE (Interpretable and Clinically-grounded\nAgent-based Report Evaluation), an interpretable evaluation framework\nleveraging large language model agents and dynamic multiple-choice question\nanswering (MCQA). Two agents, each with either the ground-truth or generated\nreport, generate clinically meaningful questions and quiz each other. Agreement\non answers captures preservation and consistency of findings, serving as\ninterpretable proxies for clinical precision and recall. By linking scores to\nquestion-answer pairs, ICARE enables transparent, and interpretable assessment.\nClinician studies show ICARE aligns significantly more with expert judgment\nthan prior metrics. Perturbation analyses confirm sensitivity to clinical\ncontent and reproducibility, while model comparisons reveal interpretable error\npatterns.", "AI": {"tldr": "ICARE \u662f\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u548c\u52a8\u6001\u591a\u9879\u9009\u62e9\u95ee\u9898\u56de\u7b54 (MCQA) \u6765\u8bc4\u4f30\u653e\u5c04\u5b66\u62a5\u544a\u3002", "motivation": "\u73b0\u6709\u7684\u6307\u6807\u901a\u5e38\u4f9d\u8d56\u4e8e\u8868\u9762\u76f8\u4f3c\u6027\u6216\u8868\u73b0\u4e3a\u9ed1\u76d2\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u56e0\u6b64\uff0c\u5b89\u5168\u90e8\u7f72\u9700\u8981\u5bf9\u751f\u6210\u7684\u62a5\u544a\u8fdb\u884c\u53ef\u9760\u7684\u4e34\u5e8a\u8bc4\u4f30\u3002", "method": "ICARE (Interpretable and Clinically-grounded Agent-based Report Evaluation)\uff0c\u5b83\u662f\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u548c\u52a8\u6001\u591a\u9879\u9009\u62e9\u95ee\u9898\u56de\u7b54 (MCQA)\u3002", "result": "\u4e34\u5e8a\u533b\u751f\u7814\u7a76\u8868\u660e\uff0cICARE \u6bd4\u4ee5\u524d\u7684\u6307\u6807\u66f4\u7b26\u5408\u4e13\u5bb6\u5224\u65ad\u3002\u6270\u52a8\u5206\u6790\u8bc1\u5b9e\u4e86\u5bf9\u4e34\u5e8a\u5185\u5bb9\u548c\u53ef\u91cd\u590d\u6027\u7684\u654f\u611f\u6027\uff0c\u800c\u6a21\u578b\u6bd4\u8f83\u63ed\u793a\u4e86\u53ef\u89e3\u91ca\u7684\u9519\u8bef\u6a21\u5f0f\u3002", "conclusion": "ICARE\u901a\u8fc7\u5c06\u5206\u6570\u4e0e\u95ee\u7b54\u5bf9\u8054\u7cfb\u8d77\u6765\uff0c\u5b9e\u73b0\u4e86\u900f\u660e\u4e14\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u3002"}}
{"id": "2508.02711", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02711", "abs": "https://arxiv.org/abs/2508.02711", "authors": ["Yidong Chai", "Yang Liu", "Yonghang Zhou", "Jiaheng Xie", "Daniel Dajun Zeng"], "title": "A Bayesian Hybrid Parameter-Efficient Fine-Tuning Method for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated transformative potential in\nreshaping the world. As these models are pretrained on general corpora, they\noften require domain-specific fine-tuning to optimize performance in\nspecialized business applications. Due to their massive scale,\nparameter-efficient fine-tuning (PEFT) methods are widely used to reduce\ntraining costs. Among them, hybrid PEFT methods that combine multiple PEFT\ntechniques have achieved the best performance. However, existing hybrid PEFT\nmethods face two main challenges when fine-tuning LLMs for specialized\napplications: (1) relying on point estimates, lacking the ability to quantify\nuncertainty for reliable decision-making, and (2) struggling to dynamically\nadapt to emerging data, lacking the ability to suit real-world situations. We\npropose Bayesian Hybrid Parameter-Efficient Fine-Tuning (BH-PEFT), a novel\nmethod that integrates Bayesian learning into hybrid PEFT. BH-PEFT combines\nAdapter, LoRA, and prefix-tuning to fine-tune feedforward and attention layers\nof the Transformer. By modeling learnable parameters as distributions, BH-PEFT\nenables uncertainty quantification. We further propose a Bayesian dynamic\nfine-tuning approach where the last posterior serves as the prior for the next\nround, enabling effective adaptation to new data. We evaluated BH-PEFT on\nbusiness tasks such as sentiment analysis, news categorization, and commonsense\nreasoning. Results show that our method outperforms existing PEFT baselines,\nenables uncertainty quantification for more reliable decisions, and improves\nadaptability in dynamic scenarios. This work contributes to business analytics\nand data science by proposing a novel BH-PEFT method and dynamic fine-tuning\napproach that support uncertainty-aware and adaptive decision-making in\nreal-world situations.", "AI": {"tldr": "This paper introduces BH-PEFT, a Bayesian hybrid PEFT method for LLMs that enables uncertainty quantification and dynamic adaptation, outperforming existing methods in business tasks.", "motivation": "Existing hybrid PEFT methods lack uncertainty quantification and struggle to dynamically adapt to emerging data when fine-tuning LLMs for specialized applications.", "method": "The paper proposes BH-PEFT, which integrates Bayesian learning into hybrid PEFT, combining Adapter, LoRA, and prefix-tuning. It also proposes a Bayesian dynamic fine-tuning approach.", "result": "BH-PEFT outperforms existing PEFT baselines, enables uncertainty quantification, and improves adaptability in dynamic scenarios on business tasks like sentiment analysis, news categorization, and commonsense reasoning.", "conclusion": "The paper introduces Bayesian Hybrid Parameter-Efficient Fine-Tuning (BH-PEFT) and a dynamic fine-tuning approach. Experiments show it outperforms existing PEFT methods, enables uncertainty quantification, and improves adaptability in dynamic scenarios."}}
{"id": "2508.02694", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.02694", "abs": "https://arxiv.org/abs/2508.02694", "authors": ["Ningning Wang", "Xavier Hu", "Pai Liu", "He Zhu", "Yue Hou", "Heyuan Huang", "Shengyu Zhang", "Jian Yang", "Jiaheng Liu", "Ge Zhang", "Changwang Zhang", "Jun Wang", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "title": "Efficient Agents: Building Effective Agents While Reducing Cost", "comment": "Work in progress. For GitHub repository, see\n  https://github.com/OPPO-PersonalAI/OAgents", "summary": "The remarkable capabilities of Large Language Model (LLM)-driven agents have\nenabled sophisticated systems to tackle complex, multi-step tasks, but their\nescalating costs threaten scalability and accessibility. This work presents the\nfirst systematic study of the efficiency-effectiveness trade-off in modern\nagent systems, addressing the critical need for cost-effective designs without\nsacrificing performance. We investigate three key questions: (1) How much\ncomplexity do agentic tasks inherently require? (2) When do additional modules\nyield diminishing returns? (3) How much efficiency can be gained through the\ndesign of efficient agent frameworks? Through an empirical analysis on the GAIA\nbenchmark, we evaluate the impact of LLM backbone selection, agent framework\ndesigns, and test-time scaling strategies. Using the cost-of-pass metric, we\nquantify the efficiency-performance trade-off across these dimensions. Our\nfindings inform the development of Efficient Agents , a novel agent framework\nthat has an optimal complexity to task requirements. Efficient Agents retains\n96.7% of the performance of OWL, one leading open-source agent framework, while\nreducing operational costs from $0.398 to $0.228, resulting in a 28.4%\nimprovement in cost-of-pass. Our work provides actionable insights for\ndesigning efficient, high-performing agent systems, advancing the accessibility\nand sustainability of AI-driven solutions.", "AI": {"tldr": "LLM agents are expensive. This paper optimizes agent framework design to reduce costs by 28.4% while maintaining 96.7% performance.", "motivation": "Escalating costs of Large Language Model (LLM)-driven agents threaten scalability and accessibility, necessitating cost-effective designs without sacrificing performance.", "method": "Empirical analysis on the GAIA benchmark to evaluate the impact of LLM backbone selection, agent framework designs, and test-time scaling strategies, using the cost-of-pass metric.", "result": "Efficient Agents retains 96.7% of the performance of OWL while reducing operational costs from $0.398 to $0.228, resulting in a 28.4% improvement in cost-of-pass.", "conclusion": "This work introduces Efficient Agents, a novel agent framework that balances complexity and task requirements, achieving 96.7% of OWL's performance with a 28.4% cost-of-pass improvement (reducing costs from $0.398 to $0.228)."}}
{"id": "2508.02806", "categories": ["cs.CV", "cs.LG", "I.2.10; I.4.8; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.02806", "abs": "https://arxiv.org/abs/2508.02806", "authors": ["Zongyou Yang", "Jonathan Loo"], "title": "PyCAT4: A Hierarchical Vision Transformer-based Framework for 3D Human Pose Estimation", "comment": "10 pages, 20 figures", "summary": "Recently, a significant improvement in the accuracy of 3D human pose\nestimation has been achieved by combining convolutional neural networks (CNNs)\nwith pyramid grid alignment feedback loops. Additionally, innovative\nbreakthroughs have been made in the field of computer vision through the\nadoption of Transformer-based temporal analysis architectures. Given these\nadvancements, this study aims to deeply optimize and improve the existing Pymaf\nnetwork architecture. The main innovations of this paper include: (1)\nIntroducing a Transformer feature extraction network layer based on\nself-attention mechanisms to enhance the capture of low-level features; (2)\nEnhancing the understanding and capture of temporal signals in video sequences\nthrough feature temporal fusion techniques; (3) Implementing spatial pyramid\nstructures to achieve multi-scale feature fusion, effectively balancing feature\nrepresentations differences across different scales. The new PyCAT4 model\nobtained in this study is validated through experiments on the COCO and 3DPW\ndatasets. The results demonstrate that the proposed improvement strategies\nsignificantly enhance the network's detection capability in human pose\nestimation, further advancing the development of human pose estimation\ntechnology.", "AI": {"tldr": "This paper optimizes the Pymaf network using Transformers and multi-scale feature fusion, leading to improved human pose estimation accuracy on COCO and 3DPW datasets.", "motivation": "Significant accuracy improvements in 3D human pose estimation via CNNs with pyramid grid alignment feedback loops and breakthroughs in computer vision using Transformer-based temporal analysis architectures.", "method": "Optimizing the Pymaf network architecture by introducing a Transformer feature extraction network layer, enhancing temporal signal understanding through feature temporal fusion, and implementing spatial pyramid structures for multi-scale feature fusion.", "result": "Experiments on COCO and 3DPW datasets validate that the proposed improvement strategies significantly enhance the network's detection capability in human pose estimation.", "conclusion": "The improved PyCAT4 model significantly enhances network detection capability in human pose estimation."}}
{"id": "2508.02945", "categories": ["cs.IR", "cs.AI", "cs.LG", "stat.AP", "stat.CO", "68P20, 68T50, 68T05, 62P20, 91G80", "H.3.3; I.2.6; I.2.7; J.1"], "pdf": "https://arxiv.org/pdf/2508.02945", "abs": "https://arxiv.org/abs/2508.02945", "authors": ["Ilias Aarab"], "title": "LLM-based IR-system for Bank Supervisors", "comment": null, "summary": "Bank supervisors face the complex task of ensuring that new measures are\nconsistently aligned with historical precedents. To address this challenge, we\nintroduce a novel Information Retrieval (IR) System tailored to assist\nsupervisors in drafting both consistent and effective measures. This system\ningests findings from on-site investigations. It then retrieves the most\nrelevant historical findings and their associated measures from a comprehensive\ndatabase, providing a solid basis for supervisors to write well-informed\nmeasures for new findings. Utilizing a blend of lexical, semantic, and Capital\nRequirements Regulation (CRR) fuzzy set matching techniques, the IR system\nensures the retrieval of findings that closely align with current cases. The\nperformance of this system, particularly in scenarios with partially labeled\ndata, is validated through a Monte Carlo methodology, showcasing its robustness\nand accuracy. Enhanced by a Transformer-based Denoising AutoEncoder for\nfine-tuning, the final model achieves a Mean Average Precision (MAP@100) of\n0.83 and a Mean Reciprocal Rank (MRR@100) of 0.92. These scores surpass those\nof both standalone lexical models such as BM25 and semantic BERT-like models.", "AI": {"tldr": "An Information Retrieval (IR) System was developed to help bank supervisors draft consistent and effective measures by retrieving relevant historical findings. It achieves high accuracy, outperforming other models.", "motivation": "Bank supervisors need to ensure that new measures are consistently aligned with historical precedents.", "method": "The system utilizes a blend of lexical, semantic, and Capital Requirements Regulation (CRR) fuzzy set matching techniques, enhanced by a Transformer-based Denoising AutoEncoder for fine-tuning.", "result": "The IR system retrieves relevant historical findings and their associated measures, providing a basis for supervisors to write well-informed measures. The system's performance is validated through a Monte Carlo methodology.", "conclusion": "The system achieves a Mean Average Precision (MAP@100) of 0.83 and a Mean Reciprocal Rank (MRR@100) of 0.92, surpassing standalone lexical and semantic models."}}
{"id": "2508.02853", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02853", "abs": "https://arxiv.org/abs/2508.02853", "authors": ["Yinuo Xu", "Veronica Derricks", "Allison Earl", "David Jurgens"], "title": "Modeling Annotator Disagreement with Demographic-Aware Experts and Synthetic Perspectives", "comment": "28 pages, 17 figures", "summary": "We present an approach to modeling annotator disagreement in subjective NLP\ntasks through both architectural and data-centric innovations. Our model,\nDEM-MoE (Demographic-Aware Mixture of Experts), routes inputs to expert\nsubnetworks based on annotator demographics, enabling it to better represent\nstructured, group-level variation compared to prior models. DEM-MoE\nconsistently performs competitively across demographic groups, and shows\nespecially strong results on datasets with high annotator disagreement. To\naddress sparse demographic coverage, we test whether LLM-generated synthetic\nannotations via zero-shot persona prompting can be used for data imputation. We\nshow these synthetic judgments align moderately well with human annotations on\nour data and offer a scalable way to potentially enrich training data. We then\npropose and evaluate approaches for blending real and synthetic data using\nstrategies tailored to dataset structure. We find that the optimal strategies\ndepend on dataset structure. Together, these contributions improve the\nrepresentation of diverse perspectives.", "AI": {"tldr": "The paper introduces DEM-MoE to model annotator disagreement using demographic information and explores synthetic data to improve performance.", "motivation": "The motivation is to model annotator disagreement in subjective NLP tasks.", "method": "The paper proposes DEM-MoE (Demographic-Aware Mixture of Experts), a model that routes inputs to expert subnetworks based on annotator demographics.", "result": "DEM-MoE performs competitively across demographic groups and shows strong results on datasets with high annotator disagreement. Synthetic annotations align moderately well with human annotations and offer a scalable way to enrich training data. The optimal strategies for blending real and synthetic data depend on dataset structure.", "conclusion": "This paper improves the representation of diverse perspectives in subjective NLP tasks."}}
{"id": "2508.02719", "categories": ["cs.LG", "cs.AI", "68T07, 65K10, 68Q32", "I.2.6; G.1.6; I.5.1"], "pdf": "https://arxiv.org/pdf/2508.02719", "abs": "https://arxiv.org/abs/2508.02719", "authors": ["Samiksha BC"], "title": "ZetA: A Riemann Zeta-Scaled Extension of Adam for Deep Learning", "comment": "6 pages, 1 figure, 4 references. This paper introduces a hybrid\n  optimizer combining Adam with Riemann zeta-based scaling", "summary": "This work introduces ZetA, a novel deep learning optimizer that extends Adam\nby incorporating dynamic scaling based on the Riemann zeta function. To the\nbest of our knowledge, ZetA is the first optimizer to apply zeta-based gradient\nscaling within deep learning optimization. The method improves generalization\nand robustness through a hybrid update mechanism that integrates adaptive\ndamping, cosine similarity-based momentum boosting, entropy-regularized loss,\nand Sharpness-Aware Minimization (SAM)-style perturbations. Empirical\nevaluations on SVHN, CIFAR10, CIFAR100, STL10, and noisy CIFAR10 consistently\nshow test accuracy improvements over Adam. All experiments employ a lightweight\nfully connected network trained for five epochs under mixed-precision settings.\nThe results demonstrate that ZetA is a computationally efficient and robust\nalternative to Adam, particularly effective in noisy or high-granularity\nclassification tasks.", "AI": {"tldr": "ZetA: a novel deep learning optimizer that extends Adam and improves generalization and robustness.", "motivation": "improve generalization and robustness", "method": "a novel deep learning optimizer that extends Adam by incorporating dynamic scaling based on the Riemann zeta function. The method improves generalization and robustness through a hybrid update mechanism that integrates adaptive damping, cosine similarity-based momentum boosting, entropy-regularized loss, and Sharpness-Aware Minimization (SAM)-style perturbations.", "result": "test accuracy improvements over Adam on SVHN, CIFAR10, CIFAR100, STL10, and noisy CIFAR10", "conclusion": "ZetA is a computationally efficient and robust alternative to Adam, particularly effective in noisy or high-granularity classification tasks."}}
{"id": "2508.02697", "categories": ["cs.AI", "cs.CE", "03B35 (Primary) 03A99, 03B10, 03B25, 68V15, 03C07 (Secondary)", "I.2.3; I.2.4; F.4.1; F.2.2; H.3.3"], "pdf": "https://arxiv.org/pdf/2508.02697", "abs": "https://arxiv.org/abs/2508.02697", "authors": ["Mikhail Soutchanski", "Yongmei Liu"], "title": "Planning with Dynamically Changing Domains", "comment": "A revised version of the paper accepted to the 1st International\n  Workshop on Trends in Knowledge Representation and Reasoning organized as a\n  IJCAI 2025 workshop that takes place in August 2025 in Montreal, Canada. See\n  the details at https://tkr2025.krportal.org/programme.html", "summary": "In classical planning and conformant planning, it is assumed that there are\nfinitely many named objects given in advance, and only they can participate in\nactions and in fluents. This is the Domain Closure Assumption (DCA). However,\nthere are practical planning problems where the set of objects changes\ndynamically as actions are performed; e.g., new objects can be created, old\nobjects can be destroyed. We formulate the planning problem in first-order\nlogic, assume an initial theory is a finite consistent set of fluent literals,\ndiscuss when this guarantees that in every situation there are only finitely\nmany possible actions, impose a finite integer bound on the length of the plan,\nand propose to organize search over sequences of actions that are grounded at\nplanning time. We show the soundness and completeness of our approach. It can\nbe used to solve the bounded planning problems without DCA that belong to the\nintersection of sequential generalized planning (without sensing actions) and\nconformant planning, restricted to the case without the disjunction over fluent\nliterals. We discuss a proof-of-the-concept implementation of our planner.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u5177\u6709\u52a8\u6001\u5bf9\u8c61\u521b\u5efa\u548c\u9500\u6bc1\u7684\u89c4\u5212\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u57df\u5c01\u95ed\u5047\u8bbe (DCA)\u3002", "motivation": "\u5728\u7ecf\u5178\u89c4\u5212\u548c\u4e00\u81f4\u6027\u89c4\u5212\u4e2d\uff0c\u5047\u8bbe\u9884\u5148\u7ed9\u51fa\u4e86\u6709\u9650\u591a\u4e2a\u547d\u540d\u5bf9\u8c61\uff0c\u5e76\u4e14\u53ea\u6709\u5b83\u4eec\u53ef\u4ee5\u53c2\u4e0e\u52a8\u4f5c\u548c\u6d41\u7545\u3002\u8fd9\u662f\u57df\u5c01\u95ed\u5047\u8bbe\uff08DCA\uff09\u3002\u7136\u800c\uff0c\u5b58\u5728\u5b9e\u9645\u7684\u89c4\u5212\u95ee\u9898\uff0c\u5176\u4e2d\u5bf9\u8c61\u7684\u96c6\u5408\u968f\u7740\u52a8\u4f5c\u7684\u6267\u884c\u800c\u52a8\u6001\u53d8\u5316\uff1b\u4f8b\u5982\uff0c\u53ef\u4ee5\u521b\u5efa\u65b0\u5bf9\u8c61\uff0c\u9500\u6bc1\u65e7\u5bf9\u8c61\u3002", "method": "\u6211\u4eec\u5728\u547d\u9898\u903b\u8f91\u4e2d\u5236\u5b9a\u4e86\u89c4\u5212\u95ee\u9898\uff0c\u5047\u8bbe\u521d\u59cb\u7406\u8bba\u662f\u6d41\u7545\u6587\u5b57\u7684\u6709\u9650\u4e00\u81f4\u96c6\u5408\uff0c\u8ba8\u8bba\u4e86\u4f55\u65f6\u8fd9\u4fdd\u8bc1\u5728\u6bcf\u79cd\u60c5\u51b5\u4e0b\u53ea\u6709\u6709\u9650\u591a\u4e2a\u53ef\u80fd\u7684\u52a8\u4f5c\uff0c\u5bf9\u8ba1\u5212\u7684\u957f\u5ea6\u65bd\u52a0\u6709\u9650\u6574\u6570\u754c\u9650\uff0c\u5e76\u63d0\u51fa\u7ec4\u7ec7\u5728\u89c4\u5212\u65f6\u63a5\u5730\u7684\u52a8\u4f5c\u5e8f\u5217\u4e0a\u7684\u641c\u7d22\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u89e3\u51b3\u6ca1\u6709 DCA \u7684\u6709\u754c\u89c4\u5212\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u5c5e\u4e8e\u987a\u5e8f\u6cdb\u5316\u89c4\u5212\uff08\u6ca1\u6709\u611f\u77e5\u52a8\u4f5c\uff09\u548c\u4e00\u81f4\u6027\u89c4\u5212\u7684\u4ea4\u96c6\uff0c\u4f46\u9650\u5236\u5728\u6ca1\u6709\u6d41\u7545\u6587\u5b57\u7684\u5206\u79bb\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u6211\u4eec\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6b63\u786e\u6027\u548c\u5b8c\u6574\u6027\u3002\u5b83\u53ef\u4ee5\u7528\u6765\u89e3\u51b3\u5c5e\u4e8e\u5e8f\u5217\u6cdb\u5316\u89c4\u5212\uff08\u6ca1\u6709\u611f\u77e5\u52a8\u4f5c\uff09\u548c\u4e00\u81f4\u6027\u89c4\u5212\u7684\u4ea4\u96c6\u7684\u6709\u754c\u89c4\u5212\u95ee\u9898\uff0c\u9650\u5236\u5728\u6ca1\u6709\u6d41\u7545\u6587\u5b57\u7684\u6790\u53d6\u7684\u60c5\u51b5\u4e0b\u3002\u6211\u4eec\u8ba8\u8bba\u4e86\u6211\u4eec\u7684\u89c4\u5212\u5668\u7684\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u73b0\u3002"}}
{"id": "2508.02807", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02807", "abs": "https://arxiv.org/abs/2508.02807", "authors": ["Tongchun Zuo", "Zaiyu Huang", "Shuliang Ning", "Ente Lin", "Chao Liang", "Zerong Zheng", "Jianwen Jiang", "Yuan Zhang", "Mingyuan Gao", "Xin Dong"], "title": "DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework", "comment": "18 pages, 12 figures", "summary": "Video virtual try-on (VVT) technology has garnered considerable academic\ninterest owing to its promising applications in e-commerce advertising and\nentertainment. However, most existing end-to-end methods rely heavily on scarce\npaired garment-centric datasets and fail to effectively leverage priors of\nadvanced visual models and test-time inputs, making it challenging to\naccurately preserve fine-grained garment details and maintain temporal\nconsistency in unconstrained scenarios. To address these challenges, we propose\nDreamVVT, a carefully designed two-stage framework built upon Diffusion\nTransformers (DiTs), which is inherently capable of leveraging diverse unpaired\nhuman-centric data to enhance adaptability in real-world scenarios. To further\nleverage prior knowledge from pretrained models and test-time inputs, in the\nfirst stage, we sample representative frames from the input video and utilize a\nmulti-frame try-on model integrated with a vision-language model (VLM), to\nsynthesize high-fidelity and semantically consistent keyframe try-on images.\nThese images serve as complementary appearance guidance for subsequent video\ngeneration. \\textbf{In the second stage}, skeleton maps together with\nfine-grained motion and appearance descriptions are extracted from the input\ncontent, and these along with the keyframe try-on images are then fed into a\npretrained video generation model enhanced with LoRA adapters. This ensures\nlong-term temporal coherence for unseen regions and enables highly plausible\ndynamic motions. Extensive quantitative and qualitative experiments demonstrate\nthat DreamVVT surpasses existing methods in preserving detailed garment content\nand temporal stability in real-world scenarios. Our project page\nhttps://virtu-lab.github.io/", "AI": {"tldr": "DreamVVT\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u6269\u6563\u8f6c\u6362\u5668\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6765\u63d0\u9ad8\u89c6\u9891\u865a\u62df\u8bd5\u7a7f\u7684\u51c6\u786e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u7aef\u5230\u7aef\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u7a00\u7f3a\u7684\u4ee5\u670d\u88c5\u4e3a\u4e2d\u5fc3\u7684\u6570\u636e\u96c6\uff0c\u5e76\u4e14\u672a\u80fd\u6709\u6548\u5229\u7528\u5148\u8fdb\u89c6\u89c9\u6a21\u578b\u548c\u6d4b\u8bd5\u65f6\u8f93\u5165\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u56e0\u6b64\u96be\u4ee5\u51c6\u786e\u5730\u4fdd\u7559\u7ec6\u7c92\u5ea6\u7684\u670d\u88c5\u7ec6\u8282\uff0c\u5e76\u4fdd\u6301\u975e\u7ea6\u675f\u573a\u666f\u4e2d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "method": "\u4e00\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5efa\u7acb\u5728\u6269\u6563\u8f6c\u6362\u5668\uff08DiTs\uff09\u4e4b\u4e0a\uff0c\u5229\u7528\u591a\u5e27\u8bd5\u7a7f\u6a21\u578b\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u96c6\u6210\uff0c\u5408\u6210\u9ad8\u4fdd\u771f\u548c\u8bed\u4e49\u4e00\u81f4\u7684\u5173\u952e\u5e27\u8bd5\u7a7f\u56fe\u50cf\u3002\u7136\u540e\uff0c\u5c06\u9aa8\u9abc\u56fe\u4e0e\u7ec6\u7c92\u5ea6\u7684\u8fd0\u52a8\u548c\u5916\u89c2\u63cf\u8ff0\u4e00\u8d77\u63d0\u53d6\uff0c\u5e76\u4e0e\u5173\u952e\u5e27\u8bd5\u7a7f\u56fe\u50cf\u4e00\u8d77\u8f93\u5165\u5230\u9884\u8bad\u7ec3\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\uff0c\u8be5\u6a21\u578b\u901a\u8fc7LoRA\u9002\u914d\u5668\u589e\u5f3a\u3002", "result": "DreamVVT\u5728\u4fdd\u7559\u670d\u88c5\u7ec6\u8282\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u65f6\u95f4\u7a33\u5b9a\u6027\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DreamVVT\u5728\u4fdd\u7559\u670d\u88c5\u7ec6\u8282\u548c\u65f6\u95f4\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.03000", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.03000", "abs": "https://arxiv.org/abs/2508.03000", "authors": ["Mohammed Ali", "Abdelrahman Abdallah", "Adam Jatowt"], "title": "SustainableQA: A Comprehensive Question Answering Dataset for Corporate Sustainability and EU Taxonomy Reporting", "comment": null, "summary": "The growing demand for corporate sustainability transparency, particularly\nunder new regulations like the EU Taxonomy, necessitates precise data\nextraction from large, unstructured corporate reports. Large Language Models\n(LLMs) and Retrieval-Augmented Generation (RAG) systems, requires high-quality,\ndomain-specific question-answering (QA) datasets to excel at particular\ndomains. To address this, we introduce SustainableQA, a novel dataset and a\nscalable pipeline for generating a comprehensive QA datasets from corporate\nsustainability reports and annual reports. Our approach integrates semantic\nchunk classification, a hybrid span extraction pipeline combining fine-tuned\nNamed Entity Recognition (NER), rule-based methods, and LLM-driven refinement,\nalongside a specialized table-to-paragraph transformation. With over 195,000\ndiverse factoid and non-factoid QA pairs, SustainableQA is an effective\nresource for developing and benchmarking advanced knowledge assistants capable\nof navigating complex sustainability compliance", "AI": {"tldr": "\u63d0\u51fa\u4e86SustainableQA\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u4ece\u4f01\u4e1a\u53ef\u6301\u7eed\u53d1\u5c55\u62a5\u544a\u548c\u5e74\u5ea6\u62a5\u544a\u4e2d\u751f\u6210\u5168\u9762\u7684QA\u6570\u636e\u96c6\u3002", "motivation": "\u5bf9\u4f01\u4e1a\u53ef\u6301\u7eed\u53d1\u5c55\u900f\u660e\u5ea6\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u957f\uff0c\u5c24\u5176\u662f\u5728\u6b27\u76df\u5206\u7c7b\u6cd5\u7b49\u65b0\u6cd5\u89c4\u4e0b\uff0c\u9700\u8981\u4ece\u5927\u578b\u975e\u7ed3\u6784\u5316\u4f01\u4e1a\u62a5\u544a\u4e2d\u7cbe\u786e\u63d0\u53d6\u6570\u636e\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u9700\u8981\u9ad8\u8d28\u91cf\u3001\u7279\u5b9a\u9886\u57df\u7684\u95ee\u7b54\uff08QA\uff09\u6570\u636e\u96c6\u624d\u80fd\u5728\u7279\u5b9a\u9886\u57df\u8868\u73b0\u51fa\u8272\u3002", "method": "\u8be5\u65b9\u6cd5\u6574\u5408\u4e86\u8bed\u4e49\u5757\u5206\u7c7b\u3001\u6df7\u5408\u8de8\u5ea6\u63d0\u53d6\u7ba1\u9053\uff08\u7ed3\u5408\u4e86\u5fae\u8c03\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u3001\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u548cLLM\u9a71\u52a8\u7684\u6539\u8fdb\uff09\u4ee5\u53ca\u4e13\u95e8\u7684\u8868\u5230\u6bb5\u843d\u8f6c\u6362\u3002", "result": "\u62e5\u6709\u8d85\u8fc7195,000\u4e2a\u4e0d\u540c\u7684\u4e8b\u5b9e\u578b\u548c\u975e\u4e8b\u5b9e\u578b\u95ee\u7b54\u5bf9\u3002", "conclusion": "SustainableQA\u662f\u4e00\u4e2a\u6709\u6548\u8d44\u6e90\uff0c\u53ef\u4ee5\u5f00\u53d1\u548c\u8bc4\u4f30\u80fd\u591f\u9a7e\u9a6d\u590d\u6742\u53ef\u6301\u7eed\u6027\u5408\u89c4\u7684\u9ad8\u7ea7\u77e5\u8bc6\u52a9\u624b\u3002"}}
{"id": "2508.02872", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02872", "abs": "https://arxiv.org/abs/2508.02872", "authors": ["Giovanni Cherubin", "Andrew Paverd"], "title": "Highlight & Summarize: RAG without the jailbreaks", "comment": null, "summary": "Preventing jailbreaking and model hijacking of Large Language Models (LLMs)\nis an important yet challenging task. For example, when interacting with a\nchatbot, malicious users can input specially crafted prompts to cause the LLM\nto generate undesirable content or perform a completely different task from its\nintended purpose. Existing mitigations for such attacks typically rely on\nhardening the LLM's system prompt or using a content classifier trained to\ndetect undesirable content or off-topic conversations. However, these\nprobabilistic approaches are relatively easy to bypass due to the very large\nspace of possible inputs and undesirable outputs. In this paper, we present and\nevaluate Highlight & Summarize (H&S), a new design pattern for\nretrieval-augmented generation (RAG) systems that prevents these attacks by\ndesign. The core idea is to perform the same task as a standard RAG pipeline\n(i.e., to provide natural language answers to questions, based on relevant\nsources) without ever revealing the user's question to the generative LLM. This\nis achieved by splitting the pipeline into two components: a highlighter, which\ntakes the user's question and extracts relevant passages (\"highlights\") from\nthe retrieved documents, and a summarizer, which takes the highlighted passages\nand summarizes them into a cohesive answer. We describe several possible\ninstantiations of H&S and evaluate their generated responses in terms of\ncorrectness, relevance, and response quality. Surprisingly, when using an\nLLM-based highlighter, the majority of H&S responses are judged to be better\nthan those of a standard RAG pipeline.", "AI": {"tldr": "Highlight & Summarize (H&S), a new design pattern for RAG, prevents attacks by not revealing the user's question to the generative LLM, and it performs better than standard RAG.", "motivation": "Preventing jailbreaking and model hijacking of Large Language Models (LLMs) is an important yet challenging task.", "method": "a new design pattern for retrieval-augmented generation (RAG) systems", "result": "the majority of H&S responses are judged to be better than those of a standard RAG pipeline.", "conclusion": "Highlight & Summarize (H&S) can generate better responses than a standard RAG pipeline."}}
{"id": "2508.02720", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02720", "abs": "https://arxiv.org/abs/2508.02720", "authors": ["Yongfan Lai", "Bo Liu", "Xinyan Guan", "Qinghao Zhao", "Hongyan Li", "Shenda Hong"], "title": "ECGTwin: Personalized ECG Generation Using Controllable Diffusion Model", "comment": null, "summary": "Personalized electrocardiogram (ECG) generation is to simulate a patient's\nECG digital twins tailored to specific conditions. It has the potential to\ntransform traditional healthcare into a more accurate individualized paradigm,\nwhile preserving the key benefits of conventional population-level ECG\nsynthesis. However, this promising task presents two fundamental challenges:\nextracting individual features without ground truth and injecting various types\nof conditions without confusing generative model. In this paper, we present\nECGTwin, a two-stage framework designed to address these challenges. In the\nfirst stage, an Individual Base Extractor trained via contrastive learning\nrobustly captures personal features from a reference ECG. In the second stage,\nthe extracted individual features, along with a target cardiac condition, are\nintegrated into the diffusion-based generation process through our novel AdaX\nCondition Injector, which injects these signals via two dedicated and\nspecialized pathways. Both qualitative and quantitative experiments have\ndemonstrated that our model can not only generate ECG signals of high fidelity\nand diversity by offering a fine-grained generation controllability, but also\npreserving individual-specific features. Furthermore, ECGTwin shows the\npotential to enhance ECG auto-diagnosis in downstream application, confirming\nthe possibility of precise personalized healthcare solutions.", "AI": {"tldr": "ECGTwin\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u53ef\u4ee5\u751f\u6210\u4e2a\u6027\u5316\u7684ECG\u4fe1\u53f7\uff0c\u5177\u6709\u9ad8\u4fdd\u771f\u5ea6\u3001\u591a\u6837\u6027\u548c\u4e2a\u4f53\u7279\u5f02\u6027\uff0c\u5e76\u6709\u53ef\u80fd\u6539\u5584ECG\u81ea\u52a8\u8bca\u65ad\u3002", "motivation": "\u4e2a\u6027\u5316\u5fc3\u7535\u56fe\uff08ECG\uff09\u751f\u6210\u80fd\u591f\u6a21\u62df\u60a3\u8005\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u7684ECG\u6570\u5b57\u5b6a\u751f\uff0c\u4ece\u800c\u5c06\u4f20\u7edf\u533b\u7597\u4fdd\u5065\u8f6c\u53d8\u4e3a\u66f4\u51c6\u786e\u7684\u4e2a\u4f53\u5316\u6a21\u5f0f\uff0c\u540c\u65f6\u4fdd\u7559\u4f20\u7edf\u4eba\u7fa4\u6c34\u5e73ECG\u5408\u6210\u7684\u5173\u952e\u4f18\u52bf\u3002\u7136\u800c\uff0c\u8fd9\u9879\u6709\u524d\u666f\u7684\u4efb\u52a1\u63d0\u51fa\u4e86\u4e24\u4e2a\u6839\u672c\u6311\u6218\uff1a\u5728\u6ca1\u6709ground truth\u7684\u60c5\u51b5\u4e0b\u63d0\u53d6\u4e2a\u4f53\u7279\u5f81\uff0c\u4ee5\u53ca\u5728\u4e0d\u6df7\u6dc6\u751f\u6210\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u6ce8\u5165\u5404\u79cd\u7c7b\u578b\u7684\u6761\u4ef6\u3002", "method": "\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u7684\u4e2a\u4f53\u57fa\u7840\u63d0\u53d6\u5668\u548c\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u8fc7\u7a0b\uff0c\u901a\u8fc7AdaX\u6761\u4ef6\u6ce8\u5165\u5668\u6574\u5408\u4e2a\u4f53\u7279\u5f81\u548c\u76ee\u6807\u5fc3\u810f\u72b6\u51b5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cECGTwin\u6a21\u578b\u4e0d\u4ec5\u53ef\u4ee5\u901a\u8fc7\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u751f\u6210\u53ef\u63a7\u6027\u6765\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u548c\u591a\u6837\u6027\u7684ECG\u4fe1\u53f7\uff0c\u800c\u4e14\u8fd8\u4fdd\u7559\u4e86\u4e2a\u4f53\u7279\u5f02\u6027\u7279\u5f81\u3002\u6b64\u5916\uff0cECGTwin\u663e\u793a\u51fa\u5728\u4e0b\u6e38\u5e94\u7528\u4e2d\u589e\u5f3aECG\u81ea\u52a8\u8bca\u65ad\u7684\u6f5c\u529b\u3002", "conclusion": "ECGTwin\u6a21\u578b\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u548c\u591a\u6837\u6027\u7684ECG\u4fe1\u53f7\uff0c\u540c\u65f6\u4fdd\u7559\u4e2a\u4f53\u7279\u5f02\u6027\u7279\u5f81\uff0c\u5e76\u6709\u6f5c\u529b\u589e\u5f3aECG\u81ea\u52a8\u8bca\u65ad\u3002"}}
{"id": "2508.02734", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2508.02734", "abs": "https://arxiv.org/abs/2508.02734", "authors": ["Weiyu Luo", "Chenfeng Xiong"], "title": "Recovering Individual-Level Activity Sequences from Location-Based Service Data Using a Novel Transformer-Based Model", "comment": "20 pages, 5 figures", "summary": "Location-Based Service (LBS) data provides critical insights into human\nmobility, yet its sparsity often yields incomplete trip and activity sequences,\nmaking accurate inferences about trips and activities difficult. We raise a\nresearch problem: Can we use activity sequences derived from high-quality LBS\ndata to recover incomplete activity sequences at the individual level? This\nstudy proposes a new solution, the Variable Selection Network-fused Insertion\nTransformer (VSNIT), integrating the Insertion Transformer's flexible sequence\nconstruction with the Variable Selection Network's dynamic covariate handling\ncapability, to recover missing segments in incomplete activity sequences while\npreserving existing data. The findings show that VSNIT inserts more diverse,\nrealistic activity patterns, more closely matching real-world variability, and\nrestores disrupted activity transitions more effectively aligning with the\ntarget. It also performs significantly better than the baseline model across\nall metrics. These results highlight VSNIT's superior accuracy and diversity in\nactivity sequence recovery tasks, demonstrating its potential to enhance LBS\ndata utility for mobility analysis. This approach offers a promising framework\nfor future location-based research and applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5373\u53d8\u91cf\u9009\u62e9\u7f51\u7edc\u878d\u5408\u63d2\u5165Transformer (VSNIT)\uff0c\u7528\u4e8e\u6062\u590d\u4e0d\u5b8c\u6574\u6d3b\u52a8\u5e8f\u5217\u4e2d\u7f3a\u5931\u7684\u7247\u6bb5\uff0c\u540c\u65f6\u4fdd\u7559\u73b0\u6709\u6570\u636e\u3002", "motivation": "\u57fa\u4e8e\u4f4d\u7f6e\u7684\u670d\u52a1(LBS)\u6570\u636e\u63d0\u4f9b\u4e86\u5bf9\u4eba\u7c7b\u79fb\u52a8\u6027\u7684\u91cd\u8981\u89c1\u89e3\uff0c\u4f46\u5176\u7a00\u758f\u6027\u901a\u5e38\u4f1a\u4ea7\u751f\u4e0d\u5b8c\u6574\u7684\u884c\u7a0b\u548c\u6d3b\u52a8\u5e8f\u5217\uff0c\u4ece\u800c\u96be\u4ee5\u5bf9\u884c\u7a0b\u548c\u6d3b\u52a8\u8fdb\u884c\u51c6\u786e\u7684\u63a8\u65ad\u3002", "method": "Variable Selection Network-fused Insertion Transformer (VSNIT)", "result": "VSNIT\u63d2\u5165\u4e86\u66f4\u591a\u6837\u5316\u3001\u66f4\u771f\u5b9e\u7684\u6d3b\u52a8\u6a21\u5f0f\uff0c\u66f4\u7d27\u5bc6\u5730\u5339\u914d\u4e86\u771f\u5b9e\u4e16\u754c\u7684\u53d8\u5f02\u6027\uff0c\u5e76\u66f4\u6709\u6548\u5730\u6062\u590d\u4e86\u4e2d\u65ad\u7684\u6d3b\u52a8\u8f6c\u6362\uff0c\u4e0e\u76ee\u6807\u66f4\u52a0\u4e00\u81f4\u3002\u5b83\u5728\u6240\u6709\u6307\u6807\u4e0a\u4e5f\u660e\u663e\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "VSNIT\u5728\u6d3b\u52a8\u5e8f\u5217\u6062\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u589e\u5f3aLBS\u6570\u636e\u5728\u79fb\u52a8\u6027\u5206\u6790\u4e2d\u7684\u6548\u7528\u7684\u6f5c\u529b\u3002\u8be5\u65b9\u6cd5\u4e3a\u672a\u6765\u57fa\u4e8e\u4f4d\u7f6e\u7684\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u6846\u67b6\u3002"}}
{"id": "2508.02829", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02829", "abs": "https://arxiv.org/abs/2508.02829", "authors": ["Adam Colton"], "title": "Elucidating the Role of Feature Normalization in IJEPA", "comment": null, "summary": "In the standard image joint embedding predictive architecture (IJEPA),\nfeatures at the output of the teacher encoder are layer normalized (LN) before\nserving as a distillation target for the student encoder and predictor. We\npropose that this feature normalization disrupts the natural energy hierarchy\nof visual tokens, where high-energy tokens (those with larger L2 norms) encode\nsemantically important image regions. LN forces all features to have identical\nL2 norms, effectively equalizing their energies and preventing the model from\nprioritizing semantically rich regions. We find that IJEPA models trained with\nfeature LN exhibit loss maps with significant checkerboard-like artifacts. We\npropose that feature LN be replaced with a DynTanh activation as the latter\nbetter preserves token energies and allows high-energy tokens to greater\ncontribute to the prediction loss. We show that IJEPA trained with feature\nDynTanh exhibits a longer-tailed loss distribution and fixes the checkerboard\nartifacts in the loss map. Our empirical results show that our simple\nmodification improves ImageNet linear probe accuracy from 38% to 42.7% for\nViT-Small and reduces RMSE by 0.08 on NYU Depth V2 monocular depth estimation.\nThese results suggest that preserving natural token energies is crucial for\neffective self-supervised visual representation learning.", "AI": {"tldr": "feature normalization disrupts the natural energy hierarchy of visual tokens, the paper replace feature LN with a DynTanh activation", "motivation": "feature normalization disrupts the natural energy hierarchy of visual tokens", "method": "feature LN be replaced with a DynTanh activation", "result": "improves ImageNet linear probe accuracy from 38% to 42.7% for ViT-Small and reduces RMSE by 0.08 on NYU Depth V2 monocular depth estimation", "conclusion": "preserving natural token energies is crucial for effective self-supervised visual representation learning"}}
{"id": "2508.03016", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.03016", "abs": "https://arxiv.org/abs/2508.03016", "authors": ["Kaihao MA", "Meiling Wang", "Senkevich Oleg", "Zijian LI", "Daihao Xue", "Dmitriy Malyshev", "Yangming Lv", "Shihai Xiao", "Xiao Yan", "Radionov Alexander", "Weidi Zeng", "Yuanzhan Gao", "Zhiyu Zou", "Yao xin", "Liu Lin", "Junhao Wu", "Yiding Liu", "Yaoyao Fu", "Gongyi Wang", "Gong Zhang", "Fei Yi", "Yingfan Liu"], "title": "KBest: Efficient Vector Search on Kunpeng CPU", "comment": null, "summary": "Vector search, which returns the vectors most similar to a given query vector\nfrom a large vector dataset, underlies many important applications such as\nsearch, recommendation, and LLMs. To be economic, vector search needs to be\nefficient to reduce the resources required by a given query workload. However,\nexisting vector search libraries (e.g., Faiss and DiskANN) are optimized for\nx86 CPU architectures (i.e., Intel and AMD CPUs) while Huawei Kunpeng CPUs are\nbased on the ARM architecture and competitive in compute power. In this paper,\nwe present KBest as a vector search library tailored for the latest Kunpeng 920\nCPUs. To be efficient, KBest incorporates extensive hardware-aware and\nalgorithmic optimizations, which include single-instruction-multiple-data\n(SIMD) accelerated distance computation, data prefetch, index refinement, early\ntermination, and vector quantization. Experiment results show that KBest\noutperforms SOTA vector search libraries running on x86 CPUs, and our\noptimizations can improve the query throughput by over 2x. Currently, KBest\nserves applications from both our internal business and external enterprise\nclients with tens of millions of queries on a daily basis.", "AI": {"tldr": "KBest is a vector search library tailored for Kunpeng 920 CPUs, outperforming SOTA libraries with hardware-aware and algorithmic optimizations.", "motivation": "Existing vector search libraries are optimized for x86 CPUs, while Huawei Kunpeng CPUs are based on the ARM architecture and competitive in compute power.", "method": "hardware-aware and algorithmic optimizations, including SIMD accelerated distance computation, data prefetch, index refinement, early termination, and vector quantization", "result": "KBest outperforms SOTA vector search libraries running on x86 CPUs, and our optimizations can improve the query throughput by over 2x.", "conclusion": "KBest outperforms SOTA vector search libraries on x86 CPUs, improving query throughput by over 2x. It serves applications with tens of millions of daily queries."}}
{"id": "2508.02885", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02885", "abs": "https://arxiv.org/abs/2508.02885", "authors": ["Elliot Murphy", "Rohan Venkatesh", "Edward Khokhlovich", "Andrey Vyshedskiy"], "title": "Merge-based syntax is mediated by distinct neurocognitive mechanisms: A clustering analysis of comprehension abilities in 84,000 individuals with language deficits across nine languages", "comment": null, "summary": "In the modern language sciences, the core computational operation of syntax,\n'Merge', is defined as an operation that combines two linguistic units (e.g.,\n'brown', 'cat') to form a categorized structure ('brown cat', a Noun Phrase).\nThis can then be further combined with additional linguistic units based on\nthis categorial information, respecting non-associativity such that abstract\ngrouping is respected. Some linguists have embraced the view that Merge is an\nelementary, indivisible operation that emerged in a single evolutionary step.\nFrom a neurocognitive standpoint, different mental objects constructed by Merge\nmay be supported by distinct mechanisms: (1) simple command constructions\n(e.g., \"eat apples\"); (2) the merging of adjectives and nouns (\"red boat\"); and\n(3) the merging of nouns with spatial prepositions (\"laptop behind the sofa\").\nHere, we systematically investigate participants' comprehension of sentences\nwith increasing levels of syntactic complexity. Clustering analyses revealed\nbehavioral evidence for three distinct structural types, which we discuss as\npotentially emerging at different developmental stages and subject to selective\nimpairment. While a Merge-based syntax may still have emerged suddenly in\nevolutionary time, responsible for the structured symbolic turn our species\ntook, different cognitive mechanisms seem to underwrite the processing of\nvarious types of Merge-based objects.", "AI": {"tldr": "\u5408\u5e76\u53e5\u6cd5\u53ef\u80fd\u5728\u8fdb\u5316\u65f6\u95f4\u4e2d\u7a81\u7136\u51fa\u73b0\uff0c\u4f46\u4e0d\u540c\u7684\u8ba4\u77e5\u673a\u5236\u4f3c\u4e4e\u652f\u6301\u4e0d\u540c\u7c7b\u578b\u7684\u57fa\u4e8e\u5408\u5e76\u7684\u5bf9\u8c61\u7684\u5904\u7406\u3002", "motivation": "\u5728\u73b0\u4ee3\u8bed\u8a00\u79d1\u5b66\u4e2d\uff0c\u53e5\u6cd5\u7684\u6838\u5fc3\u8ba1\u7b97\u64cd\u4f5c\u201c\u5408\u5e76\u201d\u88ab\u5b9a\u4e49\u4e3a\u4e00\u79cd\u7ec4\u5408\u4e24\u4e2a\u8bed\u8a00\u5355\u4f4d\uff08\u4f8b\u5982\uff0c\u201c\u68d5\u8272\u201d\u3001\u201c\u732b\u201d\uff09\u4ee5\u5f62\u6210\u5206\u7c7b\u7ed3\u6784\uff08\u201c\u68d5\u8272\u732b\u201d\uff0c\u4e00\u4e2a\u540d\u8bcd\u77ed\u8bed\uff09\u7684\u64cd\u4f5c\u3002\u4e00\u4e9b\u8bed\u8a00\u5b66\u5bb6\u8ba4\u4e3a\u5408\u5e76\u662f\u4e00\u79cd\u57fa\u672c\u7684\u3001\u4e0d\u53ef\u5206\u5272\u7684\u64cd\u4f5c\uff0c\u5b83\u662f\u5728\u4e00\u4e2a\u8fdb\u5316\u6b65\u9aa4\u4e2d\u51fa\u73b0\u7684\u3002", "method": "\u7cfb\u7edf\u5730\u8c03\u67e5\u53c2\u4e0e\u8005\u5bf9\u5177\u6709\u9012\u589e\u53e5\u6cd5\u590d\u6742\u5ea6\u7684\u53e5\u5b50\u7684\u7406\u89e3\u3002", "result": "\u805a\u7c7b\u5206\u6790\u63ed\u793a\u4e86\u4e09\u79cd\u4e0d\u540c\u7ed3\u6784\u7c7b\u578b\u7684\u884c\u4e3a\u8bc1\u636e\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u7ed3\u6784\u7c7b\u578b\u53ef\u80fd\u5728\u4e0d\u540c\u7684\u53d1\u5c55\u9636\u6bb5\u51fa\u73b0\uff0c\u5e76\u53d7\u5230\u9009\u62e9\u6027\u635f\u4f24\u3002", "conclusion": "\u4e0d\u540c\u7684\u8ba4\u77e5\u673a\u5236\u4f3c\u4e4e\u652f\u6301\u4e0d\u540c\u7c7b\u578b\u7684\u57fa\u4e8e\u5408\u5e76\u7684\u5bf9\u8c61\u7684\u5904\u7406\u3002"}}
{"id": "2508.02723", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02723", "abs": "https://arxiv.org/abs/2508.02723", "authors": ["Haitz S\u00e1ez de Oc\u00e1riz Borde", "Michael Bronstein"], "title": "Mathematical Foundations of Geometric Deep Learning", "comment": "78 pages", "summary": "We review the key mathematical concepts necessary for studying Geometric Deep\nLearning.", "AI": {"tldr": "review of mathematical concepts for Geometric Deep Learning", "motivation": "studying Geometric Deep Learning", "method": "reviewing key mathematical concepts", "result": "N/A", "conclusion": "N/A"}}
{"id": "2508.02744", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02744", "abs": "https://arxiv.org/abs/2508.02744", "authors": ["Peiran Wang", "Yaoning Yu", "Ke Chen", "Xianyang Zhan", "Haohan Wang"], "title": "Large Language Model-based Data Science Agent: A Survey", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has driven novel\napplications across diverse domains, with LLM-based agents emerging as a\ncrucial area of exploration. This survey presents a comprehensive analysis of\nLLM-based agents designed for data science tasks, summarizing insights from\nrecent studies. From the agent perspective, we discuss the key design\nprinciples, covering agent roles, execution, knowledge, and reflection methods.\nFrom the data science perspective, we identify key processes for LLM-based\nagents, including data preprocessing, model development, evaluation,\nvisualization, etc. Our work offers two key contributions: (1) a comprehensive\nreview of recent developments in applying LLMbased agents to data science\ntasks; (2) a dual-perspective framework that connects general agent design\nprinciples with the practical workflows in data science.", "AI": {"tldr": "This survey reviews LLM-based agents for data science, covering design principles and practical workflows.", "motivation": "The rapid advancement of Large Language Models (LLMs) has driven novel applications across diverse domains, with LLM-based agents emerging as a crucial area of exploration.", "method": "The survey analyzes LLM-based agents designed for data science tasks, summarizing insights from recent studies.", "result": "The survey discusses key design principles from the agent perspective, covering agent roles, execution, knowledge, and reflection methods. It also identifies key processes for LLM-based agents from the data science perspective, including data preprocessing, model development, evaluation, visualization, etc.", "conclusion": "This survey offers a comprehensive review of recent developments in applying LLM-based agents to data science tasks and provides a dual-perspective framework that connects general agent design principles with practical workflows in data science."}}
{"id": "2508.02831", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02831", "abs": "https://arxiv.org/abs/2508.02831", "authors": ["Miko\u0142aj Zieli\u0144ski", "Krzysztof Byrski", "Tomasz Szczepanik", "Przemys\u0142aw Spurek"], "title": "GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing", "comment": null, "summary": "Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have recently\ntransformed 3D scene representation and rendering. NeRF achieves high-fidelity\nnovel view synthesis by learning volumetric representations through neural\nnetworks, but its implicit encoding makes editing and physical interaction\nchallenging. In contrast, GS represents scenes as explicit collections of\nGaussian primitives, enabling real-time rendering, faster training, and more\nintuitive manipulation. This explicit structure has made GS particularly\nwell-suited for interactive editing and integration with physics-based\nsimulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural\nRadiance Fields Interactive Editing), a hybrid model that combines the\nphotorealistic rendering quality of NeRF with the editable and structured\nrepresentation of GS. Instead of using spherical harmonics for appearance\nmodeling, we assign each Gaussian a trainable feature embedding. These\nembeddings are used to condition a NeRF network based on the k nearest\nGaussians to each query point. To make this conditioning efficient, we\nintroduce Ray-Traced Gaussian Proximity Search (RT-GPS), a fast nearest\nGaussian search based on a modified ray-tracing pipeline. We also integrate a\nmulti-resolution hash grid to initialize and update Gaussian features.\nTogether, these components enable real-time, locality-aware editing: as\nGaussian primitives are repositioned or modified, their interpolated influence\nis immediately reflected in the rendered output. By combining the strengths of\nimplicit and explicit representations, GENIE supports intuitive scene\nmanipulation, dynamic interaction, and compatibility with physical simulation,\nbridging the gap between geometry-based editing and neural rendering. The code\ncan be found under (https://github.com/MikolajZielinski/genie)", "AI": {"tldr": "GENIE\u7ed3\u5408\u4e86NeRF\u548cGS\u7684\u4f18\u70b9\uff0c\u5b9e\u73b0\u4e86\u53ef\u4ea4\u4e92\u7f16\u8f91\u7684\u795e\u7ecf\u6e32\u67d3\u3002", "motivation": "NeRF\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u4f53\u79ef\u8868\u793a\u6765\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u7684\u65b0\u89c6\u89d2\u5408\u6210\uff0c\u4f46\u5176\u9690\u5f0f\u7f16\u7801\u4f7f\u5f97\u7f16\u8f91\u548c\u7269\u7406\u4ea4\u4e92\u5177\u6709\u6311\u6218\u6027\u3002GS\u5c06\u573a\u666f\u8868\u793a\u4e3a\u9ad8\u65af\u57fa\u5143\u7684\u663e\u5f0f\u96c6\u5408\uff0c\u4ece\u800c\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3\u3001\u66f4\u5feb\u7684\u8bad\u7ec3\u548c\u66f4\u76f4\u89c2\u7684\u64cd\u4f5c\u3002\u8fd9\u79cd\u663e\u5f0f\u7ed3\u6784\u4f7fGS\u7279\u522b\u9002\u5408\u4e8e\u4ea4\u4e92\u5f0f\u7f16\u8f91\u548c\u4e0e\u57fa\u4e8e\u7269\u7406\u7684\u4eff\u771f\u96c6\u6210\u3002", "method": "GENIE\uff1a\u4e00\u79cd\u6df7\u5408\u6a21\u578b\uff0c\u5b83\u7ed3\u5408\u4e86NeRF\u7684\u5149\u771f\u5b9e\u611f\u6e32\u67d3\u8d28\u91cf\u548cGS\u7684\u53ef\u7f16\u8f91\u548c\u7ed3\u6784\u5316\u8868\u793a\u3002\u4e3a\u6bcf\u4e2a\u9ad8\u65af\u5206\u914d\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u7279\u5f81\u5d4c\u5165\uff0c\u7528\u4e8e\u57fa\u4e8e\u6bcf\u4e2a\u67e5\u8be2\u70b9\u7684k\u4e2a\u6700\u8fd1\u9ad8\u65af\u6765\u8c03\u8282NeRF\u7f51\u7edc\u3002\u5f15\u5165\u4e86\u5149\u7ebf\u8ffd\u8e2a\u9ad8\u65af\u90bb\u8fd1\u641c\u7d22\uff08RT-GPS\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u4fee\u6539\u540e\u7684\u5149\u7ebf\u8ffd\u8e2a\u7ba1\u9053\u7684\u5feb\u901f\u6700\u8fd1\u9ad8\u65af\u641c\u7d22\u3002\u8fd8\u96c6\u6210\u4e86\u4e00\u4e2a\u591a\u5206\u8fa8\u7387\u54c8\u5e0c\u7f51\u683c\u6765\u521d\u59cb\u5316\u548c\u66f4\u65b0\u9ad8\u65af\u7279\u5f81\u3002", "result": "GENIE\u5b9e\u73b0\u4e86\u5b9e\u65f6\u3001\u5c40\u90e8\u611f\u77e5\u7f16\u8f91\uff1a\u5f53\u9ad8\u65af\u57fa\u5143\u88ab\u91cd\u65b0\u5b9a\u4f4d\u6216\u4fee\u6539\u65f6\uff0c\u5b83\u4eec\u7684\u63d2\u503c\u5f71\u54cd\u4f1a\u7acb\u5373\u53cd\u6620\u5728\u6e32\u67d3\u8f93\u51fa\u4e2d\u3002", "conclusion": "GENIE\u7ed3\u5408\u4e86NeRF\u548cGS\u7684\u4f18\u52bf\uff0c\u652f\u6301\u76f4\u89c2\u7684\u573a\u666f\u64cd\u4f5c\u3001\u52a8\u6001\u4ea4\u4e92\u4ee5\u53ca\u4e0e\u7269\u7406\u4eff\u771f\u7684\u517c\u5bb9\u6027\uff0c\u5f25\u5408\u4e86\u51e0\u4f55\u7f16\u8f91\u548c\u795e\u7ecf\u6e32\u67d3\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2508.03088", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.03088", "abs": "https://arxiv.org/abs/2508.03088", "authors": ["Kai Zhang", "Zekai Zhang", "Xihe Sun", "Jingmeng Nie", "Qinghui Chen", "Han Hao", "Jianyuan Guo", "Jinglin Zhang"], "title": "ADSeeker: A Knowledge-Infused Framework for Anomaly Detection and Reasoning", "comment": null, "summary": "Automatic vision inspection holds significant importance in industry\ninspection. While multimodal large language models (MLLMs) exhibit strong\nlanguage understanding capabilities and hold promise for this task, their\nperformance remains significantly inferior to that of human experts. In this\ncontext, we identify two key challenges: (i) insufficient integration of\nanomaly detection (AD) knowledge during pre-training, and (ii) the lack of\ntechnically precise and conte-aware language generation for anomaly reasoning.\nTo address these issues, we propose ADSeeker, an anomaly task assistant\ndesigned to enhance inspection performance through knowledge-grounded\nreasoning. ADSeeker leverages a curated visual document knowledge base,\nSEEK-MVTec&VisA (SEEK-M&V), which we construct to address the limitations of\nexisting resources that rely solely on unstructured text. SEEK-M&V includes\nsemantic-rich descriptions and image-document pairs, enabling more\ncomprehensive anomaly understanding. To effectively retrieve and utilize this\nknowledge, we introduce the Query Image-Knowledge Retrieval-Augmented\nGeneration (Q2K RAG) framework. To further enhance the performance in zero-shot\nanomaly detection (ZSAD), ADSeeker leverages the Hierarchical Sparse Prompt\nmechanism and type-level features to efficiently extract anomaly patterns.\nFurthermore, to tackle the challenge of limited in industry anomaly detection\n(IAD) data, we introduce the largest-scale AD dataset, Multi-type Anomaly\n(MulA), encompassing 72 multi-scale defect types across 26 Categories.\nExtensive experiments show that our plug-and-play framework, ADSeeker, achieves\nstate-of-the-art zero-shot performance on several benchmark datasets.", "AI": {"tldr": "The paper introduces ADSeeker, a knowledge-grounded reasoning framework for anomaly detection that enhances inspection performance. It uses a new visual document knowledge base, a novel RAG framework, and a hierarchical sparse prompt mechanism. A new large-scale anomaly detection dataset is also introduced. ADSeeker achieves state-of-the-art zero-shot performance.", "motivation": "Multimodal large language models (MLLMs) perform worse than human experts in automatic vision inspection due to insufficient integration of anomaly detection (AD) knowledge and the lack of technically precise language generation.", "method": "ADSeeker leverages a curated visual document knowledge base, SEEK-MVTec&VisA (SEEK-M&V), and introduces the Query Image-Knowledge Retrieval-Augmented Generation (Q2K RAG) framework. It also leverages the Hierarchical Sparse Prompt mechanism and type-level features. The paper introduces a new large-scale AD dataset, Multi-type Anomaly (MulA).", "result": "ADSeeker achieves state-of-the-art zero-shot performance on several benchmark datasets.", "conclusion": "ADSeeker achieves state-of-the-art zero-shot performance on several benchmark datasets."}}
{"id": "2508.02886", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02886", "abs": "https://arxiv.org/abs/2508.02886", "authors": ["Wenjie Luo", "Ruocheng Li", "Shanshan Zhu", "Julian Perry"], "title": "Coherent Multimodal Reasoning with Iterative Self-Evaluation for Vision-Language Models", "comment": null, "summary": "Despite significant advancements, current large language models (LLMs) and\nvision-language models (LVLMs) continue to struggle with complex, multi-step,\ncross-modal common sense reasoning tasks, often exhibiting a lack of\n\"deliberative thinking.\" They tend to rely on superficial associations rather\nthan deep, chained inference, particularly when integrating visual information\nwith abstract concepts. To address this, we propose the Coherent Multimodal\nReasoning Framework (CMRF), a novel approach that enhances LVLMs' common sense\nreasoning capabilities through an iterative, self-evaluating inference\nmechanism. CMRF mimics human problem-solving by decomposing complex queries,\ngenerating step-by-step inferences, and self-correcting errors. Our framework\nintegrates three key modules: a Reasoning Decomposition Unit (RDU) for breaking\ndown problems into sub-questions, a Contextual Inference Engine (CIE) for\ncontextual inference, and a Coherence Assessment Module (CAM) for evaluating\nlogical consistency and confidence. Coupled with an Adaptive Iterative\nRefinement strategy, CMRF systematically refines its reasoning paths. Built\nupon LLaVA-1.6-34B and trained on a novel Multimodal Daily Activity Reasoning\n(MDAR) dataset, CMRF achieves state-of-the-art performance among open-source\nLVLMs on challenging benchmarks like VCR, A-OKVQA, and DailyLife-MRC. It\nattains an average accuracy of 69.4%, surpassing the best open-source baseline\nby +2.4 percentage points, with particular strength in complex reasoning\nscenarios. Extensive ablation studies and human evaluations confirm the\ncritical contributions of each module and the effectiveness of iterative\nrefinement in fostering more coherent and accurate reasoning.", "AI": {"tldr": "CMRF improves LVLMs' common sense reasoning using iterative, self-evaluating inference, achieving state-of-the-art performance on challenging benchmarks.", "motivation": "Current large language models (LLMs) and vision-language models (LVLMs) struggle with complex, multi-step, cross-modal common sense reasoning tasks, often lacking \"deliberative thinking\" and relying on superficial associations.", "method": "The Coherent Multimodal Reasoning Framework (CMRF) enhances LVLMs' common sense reasoning through an iterative, self-evaluating inference mechanism, integrating a Reasoning Decomposition Unit (RDU), a Contextual Inference Engine (CIE), and a Coherence Assessment Module (CAM).", "result": "CMRF attains an average accuracy of 69.4%, surpassing the best open-source baseline by +2.4 percentage points.", "conclusion": "CMRF achieves state-of-the-art performance among open-source LVLMs on challenging benchmarks and demonstrates strength in complex reasoning."}}
{"id": "2508.02725", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02725", "abs": "https://arxiv.org/abs/2508.02725", "authors": ["Md Imtiaz Habib"], "title": "Forecasting NCAA Basketball Outcomes with Deep Learning: A Comparative Study of LSTM and Transformer Models", "comment": "20 page scientific report", "summary": "In this research, I explore advanced deep learning methodologies to forecast\nthe outcomes of the 2025 NCAA Division 1 Men's and Women's Basketball\ntournaments. Leveraging historical NCAA game data, I implement two\nsophisticated sequence-based models: Long Short-Term Memory (LSTM) and\nTransformer architectures. The predictive power of these models is augmented\nthrough comprehensive feature engineering, including team quality metrics\nderived from Generalized Linear Models (GLM), Elo ratings, seed differences,\nand aggregated box-score statistics. To evaluate the robustness and reliability\nof predictions, I train each model variant using both Binary Cross-Entropy\n(BCE) and Brier loss functions, providing insights into classification\nperformance and probability calibration. My comparative analysis reveals that\nwhile the Transformer architecture optimized with BCE yields superior\ndiscriminative power (highest AUC of 0.8473), the LSTM model trained with Brier\nloss demonstrates superior probabilistic calibration (lowest Brier score of\n0.1589). These findings underscore the importance of selecting appropriate\nmodel architectures and loss functions based on the specific requirements of\nforecasting tasks. The detailed analytical pipeline presented here serves as a\nreproducible framework for future predictive modeling tasks in sports analytics\nand beyond.", "AI": {"tldr": "Use LSTM and Transformer to predict the outcome of 2025 NCAA Division 1 Men's and Women's Basketball tournaments", "motivation": "I explore advanced deep learning methodologies to forecast the outcomes of the 2025 NCAA Division 1 Men's and Women's Basketball tournaments. Leveraging historical NCAA game data", "method": "I implement two sophisticated sequence-based models: Long Short-Term Memory (LSTM) and Transformer architectures. The predictive power of these models is augmented through comprehensive feature engineering, including team quality metrics derived from Generalized Linear Models (GLM), Elo ratings, seed differences, and aggregated box-score statistics. To evaluate the robustness and reliability of predictions, I train each model variant using both Binary Cross-Entropy (BCE) and Brier loss functions", "result": "the Transformer architecture optimized with BCE yields superior discriminative power (highest AUC of 0.8473), the LSTM model trained with Brier loss demonstrates superior probabilistic calibration (lowest Brier score of 0.1589)", "conclusion": "The Transformer architecture optimized with BCE yields superior discriminative power, while the LSTM model trained with Brier loss demonstrates superior probabilistic calibration. The importance of selecting appropriate model architectures and loss functions based on the specific requirements of forecasting tasks is underscored."}}
{"id": "2508.02789", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02789", "abs": "https://arxiv.org/abs/2508.02789", "authors": ["Newman Cheng", "Gordon Broadbent", "William Chappell"], "title": "Cognitive Loop via In-Situ Optimization: Self-Adaptive Reasoning for Science", "comment": null, "summary": "The capacity for artificial intelligence (AI) to formulate, evolve, and test\naltered thought patterns under dynamic conditions indicates advanced cognition\nthat is crucial for scientific discovery. The existing AI development landscape\nfalls into two categories: 1) frameworks over non-reasoning models that\nnatively incorporate opinions on how humans think, and 2) reasoning models that\nabstract precise control of the reasoning intuition away from end users. While\npowerful, for scientists to maximize utility of AI in scientific discovery,\nthey not only require accuracy and transparency in reasoning, but also\nsteerability. Hence, we introduce an alternative approach that enables deep and\nprecise control over the reasoning process called: a cognitive loop via in-situ\noptimization (CLIO). CLIO enables large language models (LLMs) to\nself-formulate ways of approaching a problem, adapt behavior when\nself-confidence is low, and ultimately provide scientists with a final belief\nor answer. Through CLIO's open design, scientists can observe uncertainty\nlevels, understand how final belief states are formulated using graph\nstructures, and interject corrections. Without any further post-training,\nOpenAI's GPT-4.1 with CLIO yields an accuracy of 22.37\\% in text-based biology\nand medicine questions on Humanity's Last Exam (HLE). This yields a 13.82\\% net\nor 161.64\\% relative increase when compared to the base GPT-4.1 model and\nsurpasses OpenAI's o3 performance in high and low reasoning effort modes. We\nfurther discovered that oscillations within internal uncertainty measures are\nkey in determining the accuracy of CLIO's results, revealing how its open\ndesign and internal mechanisms can provide insight and control into scientific\ndecision-making processes.", "AI": {"tldr": "The paper introduces CLIO, a novel approach for AI-driven scientific discovery that offers deep control over the reasoning process, leading to improved accuracy and insights in biology and medicine questions.", "motivation": "Scientists require accuracy, transparency, and steerability in AI-driven scientific discovery, which current AI development landscapes do not fully provide.", "method": "The study introduces a cognitive loop via in-situ optimization (CLIO) that enables large language models (LLMs) to self-formulate ways of approaching a problem, adapt behavior when self-confidence is low, and ultimately provide scientists with a final belief or answer.", "result": "GPT-4.1 with CLIO yields an accuracy of 22.37% in text-based biology and medicine questions on Humanity's Last Exam (HLE). This yields a 13.82% net or 161.64% relative increase when compared to the base GPT-4.1 model and surpasses OpenAI's o3 performance in high and low reasoning effort modes.", "conclusion": "The study found that oscillations within internal uncertainty measures are key in determining the accuracy of CLIO's results, revealing how its open design and internal mechanisms can provide insight and control into scientific decision-making processes."}}
{"id": "2508.02844", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02844", "abs": "https://arxiv.org/abs/2508.02844", "authors": ["Anghong Du", "Nay Aung", "Theodoros N. Arvanitis", "Stefan K. Piechnik", "Joao A C Lima", "Steffen E. Petersen", "Le Zhang"], "title": "RefineSeg: Dual Coarse-to-Fine Learning for Medical Image Segmentation", "comment": null, "summary": "High-quality pixel-level annotations of medical images are essential for\nsupervised segmentation tasks, but obtaining such annotations is costly and\nrequires medical expertise. To address this challenge, we propose a novel\ncoarse-to-fine segmentation framework that relies entirely on coarse-level\nannotations, encompassing both target and complementary drawings, despite their\ninherent noise. The framework works by introducing transition matrices in order\nto model the inaccurate and incomplete regions in the coarse annotations. By\njointly training on multiple sets of coarse annotations, it progressively\nrefines the network's outputs and infers the true segmentation distribution,\nachieving a robust approximation of precise labels through matrix-based\nmodeling. To validate the flexibility and effectiveness of the proposed method,\nwe demonstrate the results on two public cardiac imaging datasets, ACDC and\nMSCMRseg, and further evaluate its performance on the UK Biobank dataset.\nExperimental results indicate that our approach surpasses the state-of-the-art\nweakly supervised methods and closely matches the fully supervised approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ece\u7c97\u5230\u7cbe\u7684\u5206\u5272\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4ec5\u4f9d\u8d56\u4e8e\u7c97\u7cd9\u7684\u6ce8\u91ca\uff0c\u901a\u8fc7\u5f15\u5165\u8f6c\u79fb\u77e9\u9635\u6765\u9010\u6b65\u7ec6\u5316\u7f51\u7edc\uff0c\u5e76\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u9ad8\u8d28\u91cf\u7684\u533b\u5b66\u56fe\u50cf\u50cf\u7d20\u7ea7\u6ce8\u91ca\u5bf9\u4e8e\u6709\u76d1\u7763\u7684\u5206\u5272\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u662f\u83b7\u5f97\u8fd9\u79cd\u6ce8\u91ca\u7684\u6210\u672c\u5f88\u9ad8\uff0c\u5e76\u4e14\u9700\u8981\u533b\u5b66\u4e13\u4e1a\u77e5\u8bc6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4ece\u7c97\u5230\u7cbe\u7684\u5206\u5272\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5b8c\u5168\u4f9d\u8d56\u4e8e\u7c97\u7cd9\u7ea7\u522b\u7684\u6ce8\u91ca\uff0c\u5305\u62ec\u76ee\u6807\u56fe\u548c\u8865\u5145\u56fe\uff0c\u5c3d\u7ba1\u5b83\u4eec\u5177\u6709\u56fa\u6709\u7684\u566a\u58f0\u3002", "method": "\u8be5\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u8f6c\u79fb\u77e9\u9635\u6765\u5bf9\u7c97\u7cd9\u6ce8\u91ca\u4e2d\u4e0d\u51c6\u786e\u548c\u4e0d\u5b8c\u6574\u7684\u533a\u57df\u8fdb\u884c\u5efa\u6a21\u3002\u901a\u8fc7\u5728\u591a\u7ec4\u7c97\u7cd9\u6ce8\u91ca\u4e0a\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\uff0c\u5b83\u53ef\u4ee5\u9010\u6b65\u7ec6\u5316\u7f51\u7edc\u7684\u8f93\u51fa\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u77e9\u9635\u7684\u5efa\u6a21\u63a8\u65ad\u51fa\u771f\u5b9e\u7684\u5206\u5272\u5206\u5e03\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u7cbe\u786e\u6807\u7b7e\u7684\u7a33\u5065\u8fd1\u4f3c\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u76ee\u524d\u6700\u597d\u7684\u5f31\u76d1\u7763\u65b9\u6cd5\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0e\u5b8c\u5168\u76d1\u7763\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728ACDC\u3001MSCMRseg\u548cUK Biobank\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u76ee\u524d\u6700\u597d\u7684\u5f31\u76d1\u7763\u65b9\u6cd5\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0e\u5b8c\u5168\u76d1\u7763\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002"}}
{"id": "2508.03172", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.03172", "abs": "https://arxiv.org/abs/2508.03172", "authors": ["Haoran Zhang", "Jingtong Liu", "Jiangzhou Deng", "Junpeng Guo"], "title": "Dual-disentangle Framework for Diversified Sequential Recommendation", "comment": null, "summary": "Sequential recommendation predicts user preferences over time and has\nachieved remarkable success. However, the growing length of user interaction\nsequences and the complex entanglement of evolving user interests and\nintentions introduce significant challenges to diversity. To address these, we\npropose a model-agnostic Dual-disentangle framework for Diversified Sequential\nRecommendation (DDSRec). The framework refines user interest and intention\nmodeling by adopting disentangling perspectives in interaction modeling and\nrepresentation learning, thereby balancing accuracy and diversity in sequential\nrecommendations. Extensive experiments on multiple public datasets demonstrate\nthe effectiveness and superiority of DDSRec in terms of accuracy and diversity\nfor sequential recommendations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faDDSRec\uff0c\u4e00\u4e2a\u7528\u4e8e\u591a\u6837\u5316\u5e8f\u5217\u63a8\u8350\u7684\u53cc\u91cd\u89e3\u8026\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u7528\u6237\u5174\u8da3\u548c\u610f\u56fe\u6765\u63d0\u9ad8\u63a8\u8350\u7684\u591a\u6837\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u7528\u6237\u4ea4\u4e92\u5e8f\u5217\u7684\u957f\u5ea6\u4e0d\u65ad\u589e\u52a0\uff0c\u4ee5\u53ca\u4e0d\u65ad\u53d8\u5316\u7684\u7528\u6237\u5174\u8da3\u548c\u610f\u56fe\u7684\u590d\u6742\u7ea0\u7f20\uff0c\u7ed9\u591a\u6837\u6027\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u53cc\u91cd\u89e3\u8026\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6837\u5316\u5e8f\u5217\u63a8\u8350 (DDSRec)\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDDSRec \u5728\u5e8f\u5217\u63a8\u8350\u7684\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u65b9\u9762\u5177\u6709\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u5728\u4ea4\u4e92\u5efa\u6a21\u548c\u8868\u5f81\u5b66\u4e60\u4e2d\u91c7\u7528\u89e3\u8026\u89c6\u89d2\u6765\u6539\u8fdb\u7528\u6237\u5174\u8da3\u548c\u610f\u56fe\u5efa\u6a21\uff0c\u4ece\u800c\u5e73\u8861\u5e8f\u5217\u63a8\u8350\u7684\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u3002"}}
{"id": "2508.02901", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02901", "abs": "https://arxiv.org/abs/2508.02901", "authors": ["Osama Khalid", "Sanvesh Srivastava", "Padmini Srinivasan"], "title": "SLIM-LLMs: Modeling of Style-Sensory Language RelationshipsThrough Low-Dimensional Representations", "comment": null, "summary": "Sensorial language -- the language connected to our senses including vision,\nsound, touch, taste, smell, and interoception, plays a fundamental role in how\nwe communicate experiences and perceptions. We explore the relationship between\nsensorial language and traditional stylistic features, like those measured by\nLIWC, using a novel Reduced-Rank Ridge Regression (R4) approach. We demonstrate\nthat low-dimensional latent representations of LIWC features r = 24 effectively\ncapture stylistic information for sensorial language prediction compared to the\nfull feature set (r = 74). We introduce Stylometrically Lean Interpretable\nModels (SLIM-LLMs), which model non-linear relationships between these style\ndimensions. Evaluated across five genres, SLIM-LLMs with low-rank LIWC features\nmatch the performance of full-scale language models while reducing parameters\nby up to 80%.", "AI": {"tldr": "This paper explores the relationship between sensorial language and traditional stylistic features, and introduces SLIM-LLMs, which match the performance of full-scale language models while reducing parameters by up to 80%.", "motivation": "Sensorial language plays a fundamental role in how we communicate experiences and perceptions.", "method": "Reduced-Rank Ridge Regression (R4) approach and Stylometrically Lean Interpretable Models (SLIM-LLMs)", "result": "low-dimensional latent representations of LIWC features r = 24 effectively capture stylistic information for sensorial language prediction compared to the full feature set (r = 74)", "conclusion": "SLIM-LLMs with low-rank LIWC features match the performance of full-scale language models while reducing parameters by up to 80%."}}
{"id": "2508.02737", "categories": ["cs.LG", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.02737", "abs": "https://arxiv.org/abs/2508.02737", "authors": ["Tasnia Nobi Afee", "Jack Hutchins", "Md Mazharul Islam", "Thomas Kampfe", "Ahmedullah Aziz"], "title": "Embedding-Enhanced Probabilistic Modeling of Ferroelectric Field Effect Transistors (FeFETs)", "comment": "15 pages, 6 figures, manuscript yet not submitted anywhere", "summary": "FeFETs hold strong potential for advancing memory and logic technologies, but\ntheir inherent randomness arising from both operational cycling and fabrication\nvariability poses significant challenges for accurate and reliable modeling.\nCapturing this variability is critical, as it enables designers to predict\nbehavior, optimize performance, and ensure reliability and robustness against\nvariations in manufacturing and operating conditions. Existing deterministic\nand machine learning-based compact models often fail to capture the full extent\nof this variability or lack the mathematical smoothness required for stable\ncircuit-level integration. In this work, we present an enhanced probabilistic\nmodeling framework for FeFETs that addresses these limitations. Building upon a\nMixture Density Network (MDN) foundation, our approach integrates C-infinity\ncontinuous activation functions for smooth, stable learning and a\ndevice-specific embedding layer to capture intrinsic physical variability\nacross devices. Sampling from the learned embedding distribution enables the\ngeneration of synthetic device instances for variability-aware simulation. With\nan R2 of 0.92, the model demonstrates high accuracy in capturing the\nvariability of FeFET current behavior. Altogether, this framework provides a\nscalable, data-driven solution for modeling the full stochastic behavior of\nFeFETs and offers a strong foundation for future compact model development and\ncircuit simulation integration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u589e\u5f3a\u7684 FeFET \u6982\u7387\u5efa\u6a21\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u5bf9 FeFET \u968f\u673a\u884c\u4e3a\u7684\u7cbe\u786e\u5efa\u6a21\uff0c\u4e3a\u672a\u6765\u7684\u7535\u8def\u4eff\u771f\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "FeFET \u5728\u63a8\u8fdb\u5b58\u50a8\u5668\u548c\u903b\u8f91\u6280\u672f\u65b9\u9762\u5177\u6709\u5f3a\u5927\u7684\u6f5c\u529b\uff0c\u4f46\u5176\u56fa\u6709\u7684\u968f\u673a\u6027\uff08\u7531\u64cd\u4f5c\u5faa\u73af\u548c\u5236\u9020\u5dee\u5f02\u5f15\u8d77\uff09\u7ed9\u51c6\u786e\u548c\u53ef\u9760\u7684\u5efa\u6a21\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u6355\u6349\u8fd9\u79cd\u53ef\u53d8\u6027\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u4f7f\u8bbe\u8ba1\u4eba\u5458\u80fd\u591f\u9884\u6d4b\u884c\u4e3a\u3001\u4f18\u5316\u6027\u80fd\u5e76\u786e\u4fdd\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4ee5\u62b5\u6297\u5236\u9020\u548c\u64cd\u4f5c\u6761\u4ef6\u7684\u53d8\u5316\u3002\u73b0\u6709\u7684\u786e\u5b9a\u6027\u548c\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u7d27\u51d1\u6a21\u578b\u901a\u5e38\u65e0\u6cd5\u6355\u6349\u5230\u8fd9\u79cd\u53ef\u53d8\u6027\u7684\u5168\u90e8\u8303\u56f4\uff0c\u6216\u8005\u7f3a\u4e4f\u7a33\u5b9a\u7535\u8def\u7ea7\u96c6\u6210\u6240\u9700\u7684\u6570\u5b66\u5e73\u6ed1\u6027\u3002", "method": "\u5728\u6df7\u5408\u5bc6\u5ea6\u7f51\u7edc (MDN) \u57fa\u7840\u4e0a\uff0c\u8be5\u65b9\u6cd5\u96c6\u6210\u4e86 C-infinity \u8fde\u7eed\u6fc0\u6d3b\u51fd\u6570\uff0c\u7528\u4e8e\u5e73\u6ed1\u3001\u7a33\u5b9a\u7684\u5b66\u4e60\uff0c\u5e76\u96c6\u6210\u4e86\u4e00\u4e2a\u7279\u5b9a\u4e8e\u8bbe\u5907\u7684\u5d4c\u5165\u5c42\uff0c\u4ee5\u6355\u83b7\u5668\u4ef6\u4e4b\u95f4\u7684\u5185\u5728\u7269\u7406\u53ef\u53d8\u6027\u3002", "result": "\u8be5\u6a21\u578b\u5728\u6355\u6349 FeFET \u7535\u6d41\u884c\u4e3a\u7684\u53ef\u53d8\u6027\u65b9\u9762\u8868\u73b0\u51fa\u5f88\u9ad8\u7684\u7cbe\u5ea6\uff0cR2 \u4e3a 0.92\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a FeFET \u7684\u5b8c\u6574\u968f\u673a\u884c\u4e3a\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u3001\u6570\u636e\u9a71\u52a8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7d27\u51d1\u6a21\u578b\u5f00\u53d1\u548c\u7535\u8def\u4eff\u771f\u96c6\u6210\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u57fa\u7840\u3002"}}
{"id": "2508.02841", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.02841", "abs": "https://arxiv.org/abs/2508.02841", "authors": ["Ziruo Yi", "Jinyu Liu", "Ting Xiao", "Mark V. Albert"], "title": "A Multi-Agent System for Complex Reasoning in Radiology Visual Question Answering", "comment": null, "summary": "Radiology visual question answering (RVQA) provides precise answers to\nquestions about chest X-ray images, alleviating radiologists' workload. While\nrecent methods based on multimodal large language models (MLLMs) and\nretrieval-augmented generation (RAG) have shown promising progress in RVQA,\nthey still face challenges in factual accuracy, hallucinations, and cross-modal\nmisalignment. We introduce a multi-agent system (MAS) designed to support\ncomplex reasoning in RVQA, with specialized agents for context understanding,\nmultimodal reasoning, and answer validation. We evaluate our system on a\nchallenging RVQA set curated via model disagreement filtering, comprising\nconsistently hard cases across multiple MLLMs. Extensive experiments\ndemonstrate the superiority and effectiveness of our system over strong MLLM\nbaselines, with a case study illustrating its reliability and interpretability.\nThis work highlights the potential of multi-agent approaches to support\nexplainable and trustworthy clinical AI applications that require complex\nreasoning.", "AI": {"tldr": "\u5f15\u5165\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\uff0c\u65e8\u5728\u652f\u6301RVQA\u4e2d\u7684\u590d\u6742\u63a8\u7406\uff0c\u4e0e\u5f3a\u5927\u7684MLLM\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u7cfb\u7edf\u5177\u6709\u4f18\u8d8a\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u653e\u5c04\u5b66\u89c6\u89c9\u95ee\u9898\u56de\u7b54\uff08RVQA\uff09\u4e3a\u5173\u4e8e\u80f8\u90e8X\u5c04\u7ebf\u56fe\u50cf\u7684\u95ee\u9898\u63d0\u4f9b\u7cbe\u786e\u7684\u7b54\u6848\uff0c\u4ece\u800c\u51cf\u8f7b\u4e86\u653e\u5c04\u79d1\u533b\u751f\u7684\u5de5\u4f5c\u91cf\u3002\u867d\u7136\u6700\u8fd1\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u65b9\u6cd5\u5728RVQA\u4e2d\u663e\u793a\u51fa\u53ef\u559c\u7684\u8fdb\u5c55\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u9762\u4e34\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u5e7b\u89c9\u548c\u8de8\u6a21\u6001\u9519\u4f4d\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\uff0c\u8be5\u7cfb\u7edf\u65e8\u5728\u652f\u6301RVQA\u4e2d\u7684\u590d\u6742\u63a8\u7406\uff0c\u5177\u6709\u7528\u4e8e\u4e0a\u4e0b\u6587\u7406\u89e3\u3001\u591a\u6a21\u6001\u63a8\u7406\u548c\u7b54\u6848\u9a8c\u8bc1\u7684\u4e13\u7528\u4ee3\u7406\u3002", "result": "\u901a\u8fc7\u6a21\u578b\u4e0d\u4e00\u81f4\u8fc7\u6ee4\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684RVQA\u96c6\u4e0a\u8bc4\u4f30\u4e86\u8be5\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5305\u542b\u8de8\u591a\u4e2aMLLM\u7684\u4e00\u81f4\u56f0\u96be\u6848\u4f8b\u3002 \u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u7cfb\u7edf\u76f8\u5bf9\u4e8e\u5f3a\u5927\u7684MLLM\u57fa\u7ebf\u7684\u4f18\u8d8a\u6027\u548c\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u8bf4\u660e\u4e86\u5176\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u9700\u8981\u590d\u6742\u63a8\u7406\u7684\u53ef\u89e3\u91ca\u548c\u503c\u5f97\u4fe1\u8d56\u7684\u4e34\u5e8a\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2508.02858", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02858", "abs": "https://arxiv.org/abs/2508.02858", "authors": ["Tianheng Zhu", "Yiheng Feng"], "title": "MIDAR: Mimicking LiDAR Detection for Traffic Applications with a Lightweight Plug-and-Play Model", "comment": "18 pages, 9 figures", "summary": "As autonomous driving (AD) technology advances, increasing research has\nfocused on leveraging cooperative perception (CP) data collected from multiple\nAVs to enhance traffic applications. Due to the impracticality of large-scale\nreal-world AV deployments, simulation has become the primary approach in most\nstudies. While game-engine-based simulators like CARLA generate high-fidelity\nraw sensor data (e.g., LiDAR point clouds) which can be used to produce\nrealistic detection outputs, they face scalability challenges in multi-AV\nscenarios. In contrast, microscopic traffic simulators such as SUMO scale\nefficiently but lack perception modeling capabilities. To bridge this gap, we\npropose MIDAR, a LiDAR detection mimicking model that approximates realistic\nLiDAR detections using vehicle-level features readily available from\nmicroscopic traffic simulators. Specifically, MIDAR predicts true positives\n(TPs) and false negatives (FNs) from ideal LiDAR detection results based on the\nspatial layouts and dimensions of surrounding vehicles. A Refined Multi-hop\nLine-of-Sight (RM-LoS) graph is constructed to encode the occlusion\nrelationships among vehicles, upon which MIDAR employs a GRU-enhanced APPNP\narchitecture to propagate features from the ego AV and occluding vehicles to\nthe prediction target. MIDAR achieves an AUC of 0.909 in approximating the\ndetection results generated by CenterPoint, a mainstream 3D LiDAR detection\nmodel, on the nuScenes AD dataset. Two CP-based traffic applications further\nvalidate the necessity of such realistic detection modeling, particularly for\ntasks requiring accurate individual vehicle observations (e.g., position,\nspeed, lane index). As demonstrated in the applications, MIDAR can be\nseamlessly integrated into traffic simulators and trajectory datasets and will\nbe open-sourced upon publication.", "AI": {"tldr": "MIDAR is proposed to approximate realistic LiDAR detections using vehicle-level features from microscopic traffic simulators, bridging the gap between high-fidelity but unscalable game-engine-based simulators and efficient but perception-modeling-lacking microscopic traffic simulators.", "motivation": "Game-engine-based simulators like CARLA generate high-fidelity raw sensor data but face scalability challenges in multi-AV scenarios. Microscopic traffic simulators such as SUMO scale efficiently but lack perception modeling capabilities. This paper aims to bridge this gap.", "method": "MIDAR, a LiDAR detection mimicking model that approximates realistic LiDAR detections using vehicle-level features readily available from microscopic traffic simulators. A Refined Multi-hop Line-of-Sight (RM-LoS) graph is constructed to encode the occlusion relationships among vehicles, upon which MIDAR employs a GRU-enhanced APPNP architecture to propagate features from the ego AV and occluding vehicles to the prediction target.", "result": "MIDAR achieves an AUC of 0.909 in approximating the detection results generated by CenterPoint. Two CP-based traffic applications further validate the necessity of such realistic detection modeling.", "conclusion": "MIDAR achieves an AUC of 0.909 in approximating the detection results generated by CenterPoint on the nuScenes AD dataset. Two CP-based traffic applications further validate the necessity of such realistic detection modeling, particularly for tasks requiring accurate individual vehicle observations. MIDAR can be seamlessly integrated into traffic simulators and trajectory datasets and will be open-sourced upon publication."}}
{"id": "2508.03306", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03306", "abs": "https://arxiv.org/abs/2508.03306", "authors": ["Kisu Yang", "Yoonna Jang", "Hwanseok Jang", "Kenneth Choi", "Isabelle Augenstein", "Heuiseok Lim"], "title": "Reliable Evaluation Protocol for Low-Precision Retrieval", "comment": "11 pages, 5 figures, submitted to ARR", "summary": "Lowering the numerical precision of model parameters and computations is\nwidely adopted to improve the efficiency of retrieval systems. However, when\ncomputing relevance scores between the query and documents in low-precision, we\nobserve spurious ties due to the reduced granularity. This introduces high\nvariability in the results based on tie resolution, making the evaluation less\nreliable. To address this, we propose a more robust retrieval evaluation\nprotocol designed to reduce score variation. It consists of: (1) High-Precision\nScoring (HPS), which upcasts the final scoring step to higher precision to\nresolve tied candidates with minimal computational cost; and (2) Tie-aware\nRetrieval Metrics (TRM), which report expected scores, range, and bias to\nquantify order uncertainty of tied candidates. Our experiments test multiple\nmodels with three scoring functions on two retrieval datasets to demonstrate\nthat HPS dramatically reduces tie-induced instability, and TRM accurately\nrecovers expected metric values. This combination enables a more consistent and\nreliable evaluation system for lower-precision retrievals.", "AI": {"tldr": "Proposes High-Precision Scoring (HPS) and Tie-aware Retrieval Metrics (TRM) to address the instability in low-precision retrieval evaluation caused by spurious ties.", "motivation": "Lowering the numerical precision of model parameters and computations is widely adopted to improve the efficiency of retrieval systems. However, when computing relevance scores between the query and documents in low-precision, we observe spurious ties due to the reduced granularity. This introduces high variability in the results based on tie resolution, making the evaluation less reliable.", "method": "High-Precision Scoring (HPS), which upcasts the final scoring step to higher precision to resolve tied candidates with minimal computational cost; and Tie-aware Retrieval Metrics (TRM), which report expected scores, range, and bias to quantify order uncertainty of tied candidates.", "result": "HPS dramatically reduces tie-induced instability, and TRM accurately recovers expected metric values.", "conclusion": "This combination enables a more consistent and reliable evaluation system for lower-precision retrievals."}}
{"id": "2508.02931", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02931", "abs": "https://arxiv.org/abs/2508.02931", "authors": ["Shengqi Li", "Amarnath Gupta"], "title": "Can LLMs Generate High-Quality Task-Specific Conversations?", "comment": null, "summary": "This paper introduces a parameterization framework for controlling\nconversation quality in large language models. We explore nine key parameters\nacross six dimensions that enable precise specification of dialogue properties.\nThrough experiments with state-of-the-art LLMs, we demonstrate that\nparameter-based control produces statistically significant differences in\ngenerated conversation properties. Our approach addresses challenges in\nconversation generation, including topic coherence, knowledge progression,\ncharacter consistency, and control granularity. The framework provides a\nstandardized method for conversation quality control with applications in\neducation, therapy, customer service, and entertainment. Future work will focus\non implementing additional parameters through architectural modifications and\ndeveloping benchmark datasets for evaluation.", "AI": {"tldr": "This paper introduces a parameterization framework for controlling conversation quality in large language models.", "motivation": "address challenges in conversation generation, including topic coherence, knowledge progression, character consistency, and control granularity", "method": "a parameterization framework for controlling conversation quality in large language models with nine key parameters across six dimensions", "result": "enable precise specification of dialogue properties", "conclusion": "parameter-based control produces statistically significant differences in generated conversation properties and the framework provides a standardized method for conversation quality control with applications in education, therapy, customer service, and entertainment"}}
{"id": "2508.02741", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.02741", "abs": "https://arxiv.org/abs/2508.02741", "authors": ["Zhixiang Lu", "Yulong Li", "Feilong Tang", "Zhengyong Jiang", "Chong Li", "Mian Zhou", "Tenglong Li", "Jionglong Su"], "title": "DeepGB-TB: A Risk-Balanced Cross-Attention Gradient-Boosted Convolutional Network for Rapid, Interpretable Tuberculosis Screening", "comment": null, "summary": "Large-scale tuberculosis (TB) screening is limited by the high cost and\noperational complexity of traditional diagnostics, creating a need for\nartificial-intelligence solutions. We propose DeepGB-TB, a non-invasive system\nthat instantly assigns TB risk scores using only cough audio and basic\ndemographic data. The model couples a lightweight one-dimensional convolutional\nneural network for audio processing with a gradient-boosted decision tree for\ntabular features. Its principal innovation is a Cross-Modal Bidirectional\nCross-Attention module (CM-BCA) that iteratively exchanges salient cues between\nmodalities, emulating the way clinicians integrate symptoms and risk factors.\nTo meet the clinical priority of minimizing missed cases, we design a\nTuberculosis Risk-Balanced Loss (TRBL) that places stronger penalties on\nfalse-negative predictions, thereby reducing high-risk misclassifications.\nDeepGB-TB is evaluated on a diverse dataset of 1,105 patients collected across\nseven countries, achieving an AUROC of 0.903 and an F1-score of 0.851,\nrepresenting a new state of the art. Its computational efficiency enables\nreal-time, offline inference directly on common mobile devices, making it ideal\nfor low-resource settings. Importantly, the system produces clinically\nvalidated explanations that promote trust and adoption by frontline health\nworkers. By coupling AI innovation with public-health requirements for speed,\naffordability, and reliability, DeepGB-TB offers a tool for advancing global TB\ncontrol.", "AI": {"tldr": "DeepGB-TB\u662f\u4e00\u79cd\u4f7f\u7528\u54b3\u55fd\u97f3\u9891\u548c\u57fa\u672c\u4eba\u53e3\u7edf\u8ba1\u6570\u636e\uff0c\u901a\u8fc7AI\u5373\u65f6\u5206\u914d\u7ed3\u6838\u75c5\u98ce\u9669\u8bc4\u5206\u7684\u975e\u4fb5\u5165\u5f0f\u7cfb\u7edf\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u9002\u5408\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u3002", "motivation": "\u4f20\u7edf\u8bca\u65ad\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u64cd\u4f5c\u590d\u6742\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u7ed3\u6838\u75c5\u7b5b\u67e5\uff0c\u56e0\u6b64\u9700\u8981\u4eba\u5de5\u667a\u80fd\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u4e00\u4e2a\u7528\u4e8e\u97f3\u9891\u5904\u7406\u7684\u8f7b\u91cf\u7ea7\u4e00\u7ef4\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u4e00\u4e2a\u7528\u4e8e\u8868\u683c\u7279\u5f81\u7684\u68af\u5ea6\u63d0\u5347\u51b3\u7b56\u6811\u3002\u5176\u4e3b\u8981\u521b\u65b0\u662f\u4e00\u4e2a\u8de8\u6a21\u6001\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u6a21\u5757\uff08CM-BCA\uff09\uff0c\u8be5\u6a21\u5757\u8fed\u4ee3\u5730\u5728\u6a21\u6001\u4e4b\u95f4\u4ea4\u6362\u663e\u8457\u7ebf\u7d22\uff0c\u6a21\u62df\u4e34\u5e8a\u533b\u751f\u6574\u5408\u75c7\u72b6\u548c\u98ce\u9669\u56e0\u7d20\u7684\u65b9\u5f0f\u3002", "result": "DeepGB-TB\u5728\u4e03\u4e2a\u56fd\u5bb6\u6536\u96c6\u7684\u5305\u542b1105\u540d\u60a3\u8005\u7684\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5b9e\u73b0\u4e860.903\u7684AUROC\u548c0.851\u7684F1\u5206\u6570\uff0c\u4ee3\u8868\u4e86\u65b0\u7684\u6280\u672f\u6c34\u5e73\u3002\u5176\u8ba1\u7b97\u6548\u7387\u4f7f\u5f97\u80fd\u591f\u5728\u5e38\u89c1\u79fb\u52a8\u8bbe\u5907\u4e0a\u76f4\u63a5\u8fdb\u884c\u5b9e\u65f6\u79bb\u7ebf\u63a8\u7406\uff0c\u4f7f\u5176\u6210\u4e3a\u8d44\u6e90\u532e\u4e4f\u73af\u5883\u7684\u7406\u60f3\u9009\u62e9\u3002", "conclusion": "DeepGB-TB\u901a\u8fc7\u7ed3\u5408AI\u521b\u65b0\u4e0e\u516c\u5171\u536b\u751f\u9700\u6c42\uff0c\u4e3a\u5168\u7403\u7ed3\u6838\u75c5\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u5de5\u5177\u3002"}}
{"id": "2508.02900", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02900", "abs": "https://arxiv.org/abs/2508.02900", "authors": ["Michael Katz", "Harsha Kokel", "Sarath Sreedharan"], "title": "Seemingly Simple Planning Problems are Computationally Challenging: The Countdown Game", "comment": null, "summary": "There is a broad consensus that the inability to form long-term plans is one\nof the key limitations of current foundational models and agents. However, the\nexisting planning benchmarks remain woefully inadequate to truly measure their\nplanning capabilities. Most existing benchmarks either focus on loosely defined\ntasks like travel planning or end up leveraging existing domains and problems\nfrom international planning competitions. While the former tasks are hard to\nformalize and verify, the latter were specifically designed to test and\nchallenge the weaknesses of existing automated planners. To address these\nshortcomings, we propose a procedure for creating a planning benchmark centered\naround the game called Countdown, where a player is expected to form a target\nnumber from a list of input numbers through arithmetic operations. We discuss\nhow this problem meets many of the desiderata associated with an ideal\nbenchmark for planning capabilities evaluation. Specifically, the domain allows\nfor an intuitive, natural language description for each problem instance, it is\ncomputationally challenging (NP-complete), and the instance space is rich\nenough that we do not have to worry about memorization. We perform an extensive\ntheoretical analysis, establishing the computational complexity result and\ndemonstrate the advantage of our instance generation procedure over public\nbenchmarks. We evaluate a variety of existing LLM-assisted planning methods on\ninstances generated using our procedure. Our results show that, unlike other\ndomains like 24 Game (a special case of Countdown), our proposed dynamic\nbenchmark remains extremely challenging for existing LLM-based approaches.", "AI": {"tldr": "A new NP-complete planning benchmark, Countdown, is proposed to address the shortcomings of existing benchmarks and better evaluate the planning capabilities of LLMs. LLMs struggle with it.", "motivation": "Existing planning benchmarks are inadequate for measuring the planning capabilities of current foundational models and agents.", "method": "A procedure for creating a planning benchmark centered around the game Countdown.", "result": "The proposed Countdown benchmark is computationally challenging (NP-complete) and remains extremely challenging for existing LLM-based approaches.", "conclusion": "Existing LLM-based approaches struggle with the proposed Countdown benchmark, unlike simpler domains like 24 Game."}}
{"id": "2508.02871", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02871", "abs": "https://arxiv.org/abs/2508.02871", "authors": ["J. Alex Hurt", "Trevor M. Bajkowski", "Grant J. Scott", "Curt H. Davis"], "title": "Evaluation and Analysis of Deep Neural Transformers and Convolutional Neural Networks on Modern Remote Sensing Datasets", "comment": null, "summary": "In 2012, AlexNet established deep convolutional neural networks (DCNNs) as\nthe state-of-the-art in CV, as these networks soon led in visual tasks for many\ndomains, including remote sensing. With the publication of Visual Transformers,\nwe are witnessing the second modern leap in computational vision, and as such,\nit is imperative to understand how various transformer-based neural networks\nperform on satellite imagery. While transformers have shown high levels of\nperformance in natural language processing and CV applications, they have yet\nto be compared on a large scale to modern remote sensing data. In this paper,\nwe explore the use of transformer-based neural networks for object detection in\nhigh-resolution electro-optical satellite imagery, demonstrating\nstate-of-the-art performance on a variety of publicly available benchmark data\nsets. We compare eleven distinct bounding-box detection and localization\nalgorithms in this study, of which seven were published since 2020, and all\neleven since 2015. The performance of five transformer-based architectures is\ncompared with six convolutional networks on three state-of-the-art opensource\nhigh-resolution remote sensing imagery datasets ranging in size and complexity.\nFollowing the training and evaluation of thirty-three deep neural models, we\nthen discuss and analyze model performance across various feature extraction\nmethodologies and detection algorithms.", "AI": {"tldr": "This paper explores transformer-based neural networks for object detection in satellite imagery, achieving state-of-the-art performance and comparing various architectures and algorithms.", "motivation": "Understanding the performance of transformer-based neural networks on satellite imagery is imperative due to their success in NLP and CV and their potential in remote sensing.", "method": "Compared eleven distinct bounding-box detection and localization algorithms, including five transformer-based architectures and six convolutional networks, on three state-of-the-art open-source high-resolution remote sensing imagery datasets.", "result": "Demonstrates state-of-the-art performance of transformer-based neural networks for object detection in high-resolution electro-optical satellite imagery.", "conclusion": "Transformer-based architectures achieve state-of-the-art performance in object detection on high-resolution remote sensing imagery datasets."}}
{"id": "2508.03518", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03518", "abs": "https://arxiv.org/abs/2508.03518", "authors": ["Marta Moscati", "Shah Nawaz", "Markus Schedl"], "title": "Parameter-Efficient Single Collaborative Branch for Recommendation", "comment": "5 pages", "summary": "Recommender Systems (RS) often rely on representations of users and items in\na joint embedding space and on a similarity metric to compute relevance scores.\nIn modern RS, the modules to obtain user and item representations consist of\ntwo distinct and separate neural networks (NN). In multimodal representation\nlearning, weight sharing has been proven effective in reducing the distance\nbetween multiple modalities of a same item. Inspired by these approaches, we\npropose a novel RS that leverages weight sharing between the user and item NN\nmodules used to obtain the latent representations in the shared embedding\nspace. The proposed framework consists of a single Collaborative Branch for\nRecommendation (CoBraR). We evaluate CoBraR by means of quantitative\nexperiments on e-commerce and movie recommendation. Our experiments show that\nby reducing the number of parameters and improving beyond-accuracy aspects\nwithout compromising accuracy, CoBraR has the potential to be applied and\nextended for real-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6743\u91cd\u5171\u4eab\u7684\u63a8\u8350\u7cfb\u7edfCoBraR\uff0c\u5b9e\u9a8c\u8868\u660eCoBraR\u5177\u6709\u5e94\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u4ee3\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u83b7\u5f97\u7528\u6237\u548c\u9879\u76ee\u8868\u793a\u7684\u6a21\u5757\u7531\u4e24\u4e2a\u4e0d\u540c\u7684\u795e\u7ecf\u7f51\u7edc\u7ec4\u6210\u3002\u5728\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u4e2d\uff0c\u6743\u91cd\u5171\u4eab\u5df2\u88ab\u8bc1\u660e\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u540c\u4e00\u9879\u76ee\u7684\u591a\u4e2a\u6a21\u6001\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u53d7\u5230\u8fd9\u4e9b\u65b9\u6cd5\u7684\u542f\u53d1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u63a8\u8350\u7cfb\u7edfCoBraR\uff0c\u5b83\u5229\u7528\u7528\u6237\u548c\u9879\u76ee\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u4e4b\u95f4\u7684\u6743\u91cd\u5171\u4eab\uff0c\u4ee5\u83b7\u5f97\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u6f5c\u5728\u8868\u793a\u3002", "result": "\u901a\u8fc7\u5728\u7535\u5b50\u5546\u52a1\u548c\u7535\u5f71\u63a8\u8350\u65b9\u9762\u7684\u5b9a\u91cf\u5b9e\u9a8c\u8bc4\u4f30CoBraR\u3002\u5b9e\u9a8c\u8868\u660e\uff0c", "conclusion": "CoBraR\u5728\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u51cf\u5c11\u4e86\u53c2\u6570\u6570\u91cf\u5e76\u6539\u5584\u4e86\u8d85\u8d8a\u51c6\u786e\u6027\u7684\u65b9\u9762\uff0c\u56e0\u6b64\u5177\u6709\u5e94\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.02997", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02997", "abs": "https://arxiv.org/abs/2508.02997", "authors": ["Sri Durga Sai Sowmya Kadali", "Evangelos E. Papalexakis"], "title": "CoCoTen: Detecting Adversarial Inputs to Large Language Models through Latent Space Features of Contextual Co-occurrence Tensors", "comment": null, "summary": "The widespread use of Large Language Models (LLMs) in many applications marks\na significant advance in research and practice. However, their complexity and\nhard-to-understand nature make them vulnerable to attacks, especially\njailbreaks designed to produce harmful responses. To counter these threats,\ndeveloping strong detection methods is essential for the safe and reliable use\nof LLMs. This paper studies this detection problem using the Contextual\nCo-occurrence Matrix, a structure recognized for its efficacy in data-scarce\nenvironments. We propose a novel method leveraging the latent space\ncharacteristics of Contextual Co-occurrence Matrices and Tensors for the\neffective identification of adversarial and jailbreak prompts. Our evaluations\nshow that this approach achieves a notable F1 score of 0.83 using only 0.5% of\nlabeled prompts, which is a 96.6% improvement over baselines. This result\nhighlights the strength of our learned patterns, especially when labeled data\nis scarce. Our method is also significantly faster, speedup ranging from 2.3 to\n128.4 times compared to the baseline models. To support future research and\nreproducibility, we have made our implementation publicly available.", "AI": {"tldr": "This paper proposes a novel method leveraging the latent space characteristics of Contextual Co-occurrence Matrices and Tensors for the effective identification of adversarial and jailbreak prompts. The method achieves a notable F1 score of 0.83 using only 0.5% of labeled prompts, which is a 96.6% improvement over baselines. The implementation is publicly available.", "motivation": "Large Language Models (LLMs) are vulnerable to attacks, especially jailbreaks designed to produce harmful responses. Developing strong detection methods is essential for the safe and reliable use of LLMs.", "method": "A novel method leveraging the latent space characteristics of Contextual Co-occurrence Matrices and Tensors.", "result": "The proposed method achieves a notable F1 score of 0.83 using only 0.5% of labeled prompts, which is a 96.6% improvement over baselines. The method is also significantly faster, with speedups ranging from 2.3 to 128.4 times compared to the baseline models.", "conclusion": "The proposed method achieves a notable F1 score of 0.83 using only 0.5% of labeled prompts, which is a 96.6% improvement over baselines. The method is also significantly faster, with speedups ranging from 2.3 to 128.4 times compared to the baseline models. The implementation is publicly available."}}
{"id": "2508.02749", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02749", "abs": "https://arxiv.org/abs/2508.02749", "authors": ["Lu Gao", "Ke Yu", "Pan Lu"], "title": "Considering Spatial Structure of the Road Network in Pavement Deterioration Modeling", "comment": null, "summary": "Pavement deterioration modeling is important in providing information\nregarding the future state of the road network and in determining the needs of\npreventive maintenance or rehabilitation treatments. This research incorporated\nspatial dependence of road network into pavement deterioration modeling through\na graph neural network (GNN). The key motivation of using a GNN for pavement\nperformance modeling is the ability to easily and directly exploit the rich\nstructural information in the network. This paper explored if considering\nspatial structure of the road network will improve the prediction performance\nof the deterioration models. The data used in this research comprises a large\npavement condition data set with more than a half million observations taken\nfrom the Pavement Management Information System (PMIS) maintained by the Texas\nDepartment of Transportation. The promising comparison results indicates that\npavement deterioration prediction models perform better when spatial\nrelationship is considered.", "AI": {"tldr": "This research incorporated spatial dependence of road network into pavement deterioration modeling through a graph neural network (GNN).", "motivation": "The key motivation of using a GNN for pavement performance modeling is the ability to easily and directly exploit the rich structural information in the network", "method": "incorporated spatial dependence of road network into pavement deterioration modeling through a graph neural network (GNN)", "result": "promising comparison results indicates that pavement deterioration prediction models perform better when spatial relationship is considered", "conclusion": "pavement deterioration prediction models perform better when spatial relationship is considered"}}
{"id": "2508.02913", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02913", "abs": "https://arxiv.org/abs/2508.02913", "authors": ["Carolina Minami Oguchi", "Leo Wei", "Koyo Kobayashi", "Hsin-Tai Wu", "Dipak Ghosal"], "title": "Enhancing Japanese Large Language Models with Reasoning Vectors", "comment": null, "summary": "Post-training methods have improved the performance and enhanced the\nreasoning capability for mainstream large language models (LLMs), but the same\nis challenging for Japanese LLMs to achieve due to the amount of resources\nrequired. Inspired by task vectors that extract the change of weights before\nand after training, specifically for a certain task, we obtain reasoning\nvectors from reasoning LLMs and apply them to Japanese LLMs to boost their\nperformance. While the resources available present a challenge to improve\nJapanese LLMs, we present a simple and effective way to obtain high improvement\nand hope to inspire for other languages.", "AI": {"tldr": "\u6211\u4eec\u53d7\u5230\u4efb\u52a1\u5411\u91cf\u7684\u542f\u53d1\uff0c\u8be5\u4efb\u52a1\u5411\u91cf\u63d0\u53d6\u8bad\u7ec3\u524d\u540e\u6743\u91cd\u7684\u53d8\u5316\uff0c\u4e13\u95e8\u9488\u5bf9\u67d0\u4e2a\u4efb\u52a1\uff0c\u6211\u4eec\u4ece\u63a8\u7406 LLM \u4e2d\u83b7\u5f97\u63a8\u7406\u5411\u91cf\uff0c\u5e76\u5c06\u5b83\u4eec\u5e94\u7528\u4e8e\u65e5\u8bed LLM \u4ee5\u63d0\u9ad8\u5b83\u4eec\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u6240\u9700\u7684\u8d44\u6e90\u91cf\uff0c\u8bad\u7ec3\u540e\u65b9\u6cd5\u6539\u8fdb\u4e3b\u6d41\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u6027\u80fd\u548c\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u5177\u6709\u6311\u6218\u6027\uff0c\u4f46\u5bf9\u4e8e\u65e5\u8bed LLM \u6765\u8bf4\uff0c\u5b9e\u73b0\u540c\u6837\u7684\u76ee\u6807\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u6211\u4eec\u4ece\u63a8\u7406 LLM \u4e2d\u83b7\u5f97\u63a8\u7406\u5411\u91cf\uff0c\u5e76\u5c06\u5b83\u4eec\u5e94\u7528\u4e8e\u65e5\u8bed LLM \u4ee5\u63d0\u9ad8\u5b83\u4eec\u7684\u6027\u80fd\u3002", "result": "\u5c06\u63a8\u7406\u5411\u91cf\u5e94\u7528\u4e8e\u65e5\u8bed LLM \u4ee5\u63d0\u9ad8\u5b83\u4eec\u7684\u6027\u80fd\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\u6765\u83b7\u5f97\u9ad8\u6027\u80fd\uff0c\u5e76\u5e0c\u671b\u4e3a\u5176\u4ed6\u8bed\u8a00\u63d0\u4f9b\u7075\u611f\u3002"}}
{"id": "2508.02890", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02890", "abs": "https://arxiv.org/abs/2508.02890", "authors": ["Rongxin Jiang", "Robert Long", "Chenghao Gu", "Mingrui Yan"], "title": "VisuCraft: Enhancing Large Vision-Language Models for Complex Visual-Guided Creative Content Generation via Structured Information Extraction", "comment": null, "summary": "This paper introduces VisuCraft, a novel framework designed to significantly\nenhance the capabilities of Large Vision-Language Models (LVLMs) in complex\nvisual-guided creative content generation. Existing LVLMs often exhibit\nlimitations in maintaining high visual fidelity, genuine creativity, and\nprecise adherence to nuanced user instructions when generating long-form texts.\nVisuCraft addresses these challenges by integrating a multimodal structured\ninformation extractor (E) and a dynamic prompt generation module (G). The\nextractor distills fine-grained visual attributes from input images into a\nrich, structured representation, which the dynamic prompt module then combines\nwith user instructions to create highly optimized prompts for underlying LVLMs\n(e.g., LLaVA, InstructBLIP). Evaluated on the self-constructed\nImageStoryGen-500K dataset using VisuGen Metrics (Visual Grounding, Creativity,\nand Instruction Adherence), VisuCraft consistently outperforms baseline LVLMs\nacross tasks like story generation and poetry composition. Our results\ndemonstrate remarkable improvements, particularly in creativity and instruction\nadherence, validating VisuCraft's effectiveness in producing imaginative,\nvisually grounded, and user-aligned long-form creative text. This work unlocks\nnew potential for LVLMs in sophisticated creative AI applications.", "AI": {"tldr": "VisuCraft enhances LVLMs for creative content generation by integrating a multimodal structured information extractor and a dynamic prompt generation module, leading to improvements in creativity and instruction adherence.", "motivation": "Existing LVLMs often exhibit limitations in maintaining high visual fidelity, genuine creativity, and precise adherence to nuanced user instructions when generating long-form texts.", "method": "integrating a multimodal structured information extractor (E) and a dynamic prompt generation module (G). The extractor distills fine-grained visual attributes from input images into a rich, structured representation, which the dynamic prompt module then combines with user instructions to create highly optimized prompts for underlying LVLMs (e.g., LLaVA, InstructBLIP).", "result": "Evaluated on the self-constructed ImageStoryGen-500K dataset using VisuGen Metrics (Visual Grounding, Creativity, and Instruction Adherence), VisuCraft consistently outperforms baseline LVLMs across tasks like story generation and poetry composition.", "conclusion": "VisuCraft demonstrates remarkable improvements, particularly in creativity and instruction adherence, validating VisuCraft's effectiveness in producing imaginative, visually grounded, and user-aligned long-form creative text. This work unlocks new potential for LVLMs in sophisticated creative AI applications."}}
{"id": "2508.03553", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03553", "abs": "https://arxiv.org/abs/2508.03553", "authors": ["Wenlong Wu", "Haofen Wang", "Bohan Li", "Peixuan Huang", "Xinzhe Zhao", "Lei Liang"], "title": "MultiRAG: A Knowledge-guided Framework for Mitigating Hallucination in Multi-source Retrieval Augmented Generation", "comment": "Accepted by ICDE 2025 Research Paper", "summary": "Retrieval Augmented Generation (RAG) has emerged as a promising solution to\naddress hallucination issues in Large Language Models (LLMs). However, the\nintegration of multiple retrieval sources, while potentially more informative,\nintroduces new challenges that can paradoxically exacerbate hallucination\nproblems. These challenges manifest primarily in two aspects: the sparse\ndistribution of multi-source data that hinders the capture of logical\nrelationships and the inherent inconsistencies among different sources that\nlead to information conflicts. To address these challenges, we propose\nMultiRAG, a novel framework designed to mitigate hallucination in multi-source\nretrieval-augmented generation through knowledge-guided approaches. Our\nframework introduces two key innovations: (1) a knowledge construction module\nthat employs multi-source line graphs to efficiently aggregate logical\nrelationships across different knowledge sources, effectively addressing the\nsparse data distribution issue; and (2) a sophisticated retrieval module that\nimplements a multi-level confidence calculation mechanism, performing both\ngraph-level and node-level assessments to identify and eliminate unreliable\ninformation nodes, thereby reducing hallucinations caused by inter-source\ninconsistencies. Extensive experiments on four multi-domain query datasets and\ntwo multi-hop QA datasets demonstrate that MultiRAG significantly enhances the\nreliability and efficiency of knowledge retrieval in complex multi-source\nscenarios. \\textcolor{blue}{Our code is available in\nhttps://github.com/wuwenlong123/MultiRAG.", "AI": {"tldr": "MultiRAG\u901a\u8fc7\u77e5\u8bc6\u5f15\u5bfc\u65b9\u6cd5\u7f13\u89e3\u591a\u6e90\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u591a\u6e90\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u53ef\u4ee5\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4f46\u591a\u6e90\u96c6\u6210\u4f1a\u5f15\u5165\u65b0\u7684\u6311\u6218\uff0c\u4ece\u800c\u52a0\u5267\u5e7b\u89c9\u95ee\u9898\u3002", "method": "MultiRAG\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u6784\u5efa\u6a21\u5757\u548c\u68c0\u7d22\u6a21\u5757\u7f13\u89e3\u5e7b\u89c9\u95ee\u9898\u3002", "result": "\u5728\u56db\u4e2a\u591a\u9886\u57df\u67e5\u8be2\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u591a\u8df3QA\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c", "conclusion": "MultiRAG\u663e\u8457\u63d0\u9ad8\u4e86\u590d\u6742\u591a\u6e90\u573a\u666f\u4e2d\u77e5\u8bc6\u68c0\u7d22\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2508.03037", "categories": ["cs.CL", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.03037", "abs": "https://arxiv.org/abs/2508.03037", "authors": ["Ariya Mukherjee-Gandhi", "Oliver Muellerklein"], "title": "When Algorithms Meet Artists: Topic Modeling the AI-Art Debate, 2013-2025", "comment": "18 pages, 5 figures, 5 tables", "summary": "As generative AI continues to reshape artistic production and alternate modes\nof human expression, artists whose livelihoods are most directly affected have\nraised urgent concerns about consent, transparency, and the future of creative\nlabor. However, the voices of artists are often marginalized in dominant public\nand scholarly discourse. This study presents a twelve-year analysis, from 2013\nto 2025, of English-language discourse surrounding AI-generated art. It draws\nfrom 439 curated 500-word excerpts sampled from opinion articles, news reports,\nblogs, legal filings, and spoken-word transcripts. Through a reproducible\nmethodology, we identify five stable thematic clusters and uncover a\nmisalignment between artists' perceptions and prevailing media narratives. Our\nfindings highlight how the use of technical jargon can function as a subtle\nform of gatekeeping, often sidelining the very issues artists deem most urgent.\nOur work provides a BERTopic-based methodology and a multimodal baseline for\nfuture research, alongside a clear call for deeper, transparency-driven\nengagement with artist perspectives in the evolving AI-creative landscape.", "AI": {"tldr": "This paper analyzes AI art discourse, revealing a disconnect between artists' concerns and media narratives, and calls for greater consideration of artists' perspectives.", "motivation": "Artists are raising concerns about consent, transparency, and the future of creative labor due to generative AI, but their voices are often marginalized.", "method": "A twelve-year analysis (2013-2025) of 439 excerpts from various English-language sources using a reproducible methodology and BERTopic.", "result": "Identified five stable thematic clusters and uncovered a misalignment between artists' perceptions and media narratives. The study also provides a BERTopic-based methodology and a multimodal baseline for future research.", "conclusion": "The study highlights a misalignment between artists' perceptions and prevailing media narratives regarding AI-generated art, revealing how technical jargon can marginalize artists' concerns. It advocates for deeper engagement with artist perspectives."}}
{"id": "2508.02750", "categories": ["cs.LG", "cs.AI", "nucl-ex", "physics.app-ph", "physics.atom-ph"], "pdf": "https://arxiv.org/pdf/2508.02750", "abs": "https://arxiv.org/abs/2508.02750", "authors": ["Haoran Liu", "Yihan Zhan", "Mingzhe Liu", "Yanhua Liu", "Peng Li", "Zhuo Zuo", "Bingqi Liu", "Runxi Liu"], "title": "Pulse Shape Discrimination Algorithms: Survey and Benchmark", "comment": null, "summary": "This review presents a comprehensive survey and benchmark of pulse shape\ndiscrimination (PSD) algorithms for radiation detection, classifying nearly\nsixty methods into statistical (time-domain, frequency-domain, neural\nnetwork-based) and prior-knowledge (machine learning, deep learning) paradigms.\nWe implement and evaluate all algorithms on two standardized datasets: an\nunlabeled set from a 241Am-9Be source and a time-of-flight labeled set from a\n238Pu-9Be source, using metrics including Figure of Merit (FOM), F1-score,\nROC-AUC, and inter-method correlations. Our analysis reveals that deep learning\nmodels, particularly Multi-Layer Perceptrons (MLPs) and hybrid approaches\ncombining statistical features with neural regression, often outperform\ntraditional methods. We discuss architectural suitabilities, the limitations of\nFOM, alternative evaluation metrics, and performance across energy thresholds.\nAccompanying this work, we release an open-source toolbox in Python and MATLAB,\nalong with the datasets, to promote reproducibility and advance PSD research.", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf9\u8109\u51b2\u5f62\u72b6\u5224\u522b (PSD) \u7b97\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u8c03\u67e5\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u901a\u5e38\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u672c\u7efc\u8ff0\u5168\u9762\u8c03\u67e5\u548c\u8bc4\u4f30\u4e86\u7528\u4e8e\u8f90\u5c04\u68c0\u6d4b\u7684\u8109\u51b2\u5f62\u72b6\u5224\u522b (PSD) \u7b97\u6cd5\u3002", "method": "\u5b9e\u73b0\u4e86\u8fd1\u516d\u5341\u79cd\u65b9\u6cd5\u7684\u5206\u7c7b\uff0c\u5206\u4e3a\u7edf\u8ba1\uff08\u65f6\u57df\u3001\u9891\u57df\u3001\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\uff09\u548c\u5148\u9a8c\u77e5\u8bc6\uff08\u673a\u5668\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60\uff09\u8303\u4f8b\uff0c\u5e76\u5728\u4e24\u4e2a\u6807\u51c6\u5316\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6240\u6709\u7b97\u6cd5\uff1a\u6765\u81ea 241Am-9Be \u6e90\u7684\u672a\u6807\u8bb0\u96c6\u548c\u6765\u81ea 238Pu-9Be \u6e90\u7684\u98de\u884c\u65f6\u95f4\u6807\u8bb0\u96c6\uff0c\u4f7f\u7528\u5305\u62ec\u54c1\u8d28\u56e0\u6570 (FOM)\u3001F1 \u5206\u6570\u3001ROC-AUC \u548c\u65b9\u6cd5\u95f4\u76f8\u5173\u6027\u7b49\u6307\u6807\u3002", "result": "\u6211\u4eec\u7684\u5206\u6790\u8868\u660e\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7279\u522b\u662f\u591a\u5c42\u611f\u77e5\u5668 (MLP) \u548c\u5c06\u7edf\u8ba1\u7279\u5f81\u4e0e\u795e\u7ecf\u56de\u5f52\u76f8\u7ed3\u5408\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u901a\u5e38\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7279\u522b\u662f\u591a\u5c42\u611f\u77e5\u5668 (MLP) \u548c\u5c06\u7edf\u8ba1\u7279\u5f81\u4e0e\u795e\u7ecf\u56de\u5f52\u76f8\u7ed3\u5408\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u901a\u5e38\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002\u6211\u4eec\u8ba8\u8bba\u4e86\u67b6\u6784\u9002\u7528\u6027\u3001FOM \u7684\u5c40\u9650\u6027\u3001\u66ff\u4ee3\u8bc4\u4f30\u6307\u6807\u4ee5\u53ca\u8de8\u80fd\u91cf\u9608\u503c\u7684\u6027\u80fd\u3002"}}
{"id": "2508.02921", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02921", "abs": "https://arxiv.org/abs/2508.02921", "authors": ["Shane Caldwell", "Max Harley", "Michael Kouremetis", "Vincent Abruzzo", "Will Pearce"], "title": "PentestJudge: Judging Agent Behavior Against Operational Requirements", "comment": "18 pages, 5 figures, 3 tables", "summary": "We introduce PentestJudge, a system for evaluating the operations of\npenetration testing agents. PentestJudge is a large language model\n(LLM)-as-judge with access to tools that allow it to consume arbitrary\ntrajectories of agent states and tool call history to determine whether a\nsecurity agent's actions meet certain operating criteria that would be\nimpractical to evaluate programmatically. We develop rubrics that use a tree\nstructure to hierarchically collapse the penetration testing task for a\nparticular environment into smaller, simpler, and more manageable sub-tasks and\ncriteria until each leaf node represents simple yes-or-no criteria for\nPentestJudge to evaluate. Task nodes are broken down into different categories\nrelated to operational objectives, operational security, and tradecraft.\nLLM-as-judge scores are compared to human domain experts as a ground-truth\nreference, allowing us to compare their relative performance with standard\nbinary classification metrics, such as F1 scores. We evaluate several frontier\nand open-source models acting as judge agents, with the best model reaching an\nF1 score of 0.83. We find models that are better at tool-use perform more\nclosely to human experts. By stratifying the F1 scores by requirement type, we\nfind even models with similar overall scores struggle with different types of\nquestions, suggesting certain models may be better judges of particular\noperating criteria. We find that weaker and cheaper models can judge the\ntrajectories of pentests performed by stronger and more expensive models,\nsuggesting verification may be easier than generation for the penetration\ntesting task. We share this methodology to facilitate future research in\nunderstanding the ability of judges to holistically and scalably evaluate the\nprocess quality of AI-based information security agents so that they may be\nconfidently used in sensitive production environments.", "AI": {"tldr": "PentestJudge, a LLM-as-judge system, evaluates penetration testing agents, achieving an F1 score of 0.83, and finds weaker models can judge stronger models' pentests.", "motivation": "Evaluating the operations of penetration testing agents is hard, current methods are impractical to evaluate programmatically.", "method": "We develop rubrics that use a tree structure to hierarchically collapse the penetration testing task for a particular environment into smaller, simpler, and more manageable sub-tasks and criteria until each leaf node represents simple yes-or-no criteria for PentestJudge to evaluate.", "result": "The best model reached an F1 score of 0.83. We find models that are better at tool-use perform more closely to human experts. We find that weaker and cheaper models can judge the trajectories of pentests performed by stronger and more expensive models, suggesting verification may be easier than generation for the penetration testing task.", "conclusion": "We share this methodology to facilitate future research in understanding the ability of judges to holistically and scalably evaluate the process quality of AI-based information security agents so that they may be confidently used in sensitive production environments."}}
{"id": "2508.02903", "categories": ["cs.CV", "68T07", "I.4.9; I.2.10"], "pdf": "https://arxiv.org/pdf/2508.02903", "abs": "https://arxiv.org/abs/2508.02903", "authors": ["Mehrdad Moradi", "Kamran Paynabar"], "title": "RDDPM: Robust Denoising Diffusion Probabilistic Model for Unsupervised Anomaly Segmentation", "comment": "10 pages, 5 figures. Accepted to the ICCV 2025 Workshop on\n  Vision-based Industrial InspectiON (VISION)", "summary": "Recent advancements in diffusion models have demonstrated significant success\nin unsupervised anomaly segmentation. For anomaly segmentation, these models\nare first trained on normal data; then, an anomalous image is noised to an\nintermediate step, and the normal image is reconstructed through backward\ndiffusion. Unlike traditional statistical methods, diffusion models do not rely\non specific assumptions about the data or target anomalies, making them\nversatile for use across different domains. However, diffusion models typically\nassume access to normal data for training, limiting their applicability in\nrealistic settings. In this paper, we propose novel robust denoising diffusion\nmodels for scenarios where only contaminated (i.e., a mix of normal and\nanomalous) unlabeled data is available. By casting maximum likelihood\nestimation of the data as a nonlinear regression problem, we reinterpret the\ndenoising diffusion probabilistic model through a regression lens. Using robust\nregression, we derive a robust version of denoising diffusion probabilistic\nmodels. Our novel framework offers flexibility in constructing various robust\ndiffusion models. Our experiments show that our approach outperforms current\nstate of the art diffusion models, for unsupervised anomaly segmentation when\nonly contaminated data is available. Our method outperforms existing\ndiffusion-based approaches, achieving up to 8.08\\% higher AUROC and 10.37\\%\nhigher AUPRC on MVTec datasets. The implementation code is available at:\nhttps://github.com/mehrdadmoradi124/RDDPM", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7a33\u5065\u53bb\u566a\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u53ea\u6709\u53d7\u6c61\u67d3\u7684\u672a\u6807\u8bb0\u6570\u636e\u53ef\u7528\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u65e0\u76d1\u7763\u5f02\u5e38\u5206\u5272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u901a\u5e38\u5047\u8bbe\u53ef\u4ee5\u8bbf\u95ee\u6b63\u5e38\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u53ea\u6709\u53d7\u6c61\u67d3\u7684\uff08\u5373\u6b63\u5e38\u548c\u5f02\u5e38\u7684\u6df7\u5408\uff09\u672a\u6807\u8bb0\u6570\u636e\u53ef\u7528\u7684\u60c5\u51b5\u3002", "method": "\u901a\u8fc7\u5c06\u6570\u636e\u7684\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u8f6c\u5316\u4e3a\u975e\u7ebf\u6027\u56de\u5f52\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u7a33\u5065\u56de\u5f52\u63a8\u5bfc\u51fa\u7a33\u5065\u7248\u672c\u7684\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u4ec5\u6709\u53d7\u6c61\u67d3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u65e0\u76d1\u7763\u5f02\u5e38\u5206\u5272\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4ec5\u6709\u53d7\u6c61\u67d3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u5728 MVTec \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe 8.08% \u7684 AUROC \u548c 10.37% \u7684 AUPRC\u3002"}}
{"id": "2508.03555", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03555", "abs": "https://arxiv.org/abs/2508.03555", "authors": ["Antoine Chaffin", "Rapha\u00ebl Sourty"], "title": "PyLate: Flexible Training and Retrieval for Late Interaction Models", "comment": "5 pages", "summary": "Neural ranking has become a cornerstone of modern information retrieval.\nWhile single vector search remains the dominant paradigm, it suffers from the\nshortcoming of compressing all the information into a single vector. This\ncompression leads to notable performance degradation in out-of-domain,\nlong-context, and reasoning-intensive retrieval tasks. Multi-vector approaches\npioneered by ColBERT aim to address these limitations by preserving individual\ntoken embeddings and computing similarity via the MaxSim operator. This\narchitecture has demonstrated superior empirical advantages, including enhanced\nout-of-domain generalization, long-context handling, and performance in complex\nretrieval scenarios. Despite these compelling empirical results and clear\ntheoretical advantages, the practical adoption and public availability of late\ninteraction models remain low compared to their single-vector counterparts,\nprimarily due to a lack of accessible and modular tools for training and\nexperimenting with such models. To bridge this gap, we introduce PyLate, a\nstreamlined library built on top of Sentence Transformers to support\nmulti-vector architectures natively, inheriting its efficient training,\nadvanced logging, and automated model card generation while requiring minimal\ncode changes to code templates users are already familiar with. By offering\nmulti-vector-specific features such as efficient indexes, PyLate aims to\naccelerate research and real-world application of late interaction models,\nthereby unlocking their full potential in modern IR systems. Finally, PyLate\nhas already enabled the development of state-of-the-art models, including\nGTE-ModernColBERT and Reason-ModernColBERT, demonstrating its practical utility\nfor both research and production environments.", "AI": {"tldr": "PyLate\u662f\u4e00\u4e2a\u65e8\u5728\u7b80\u5316\u591a\u5411\u91cf\u6a21\u578b\u7684\u4f7f\u7528\u548c\u5f00\u53d1\u7684\u5e93\uff0c\u5b83\u5efa\u7acb\u5728Sentence Transformers\u4e4b\u4e0a\uff0c\u5e76\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u7d22\u5f15\u7b49\u529f\u80fd\uff0c\u4ee5\u52a0\u901f\u5ef6\u8fdf\u4ea4\u4e92\u6a21\u578b\u5728\u73b0\u4ee3\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u5355\u5411\u91cf\u641c\u7d22\u5c06\u6240\u6709\u4fe1\u606f\u538b\u7f29\u6210\u4e00\u4e2a\u5411\u91cf\uff0c\u5bfc\u81f4\u5728\u9886\u57df\u5916\u3001\u957f\u4e0a\u4e0b\u6587\u548c\u63a8\u7406\u5bc6\u96c6\u578b\u68c0\u7d22\u4efb\u52a1\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u5c3d\u7ba1\u5ef6\u8fdf\u4ea4\u4e92\u6a21\u578b\u5177\u6709\u660e\u663e\u7684\u4f18\u52bf\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u53ef\u8bbf\u95ee\u548c\u6a21\u5757\u5316\u7684\u5de5\u5177\uff0c\u5b83\u4eec\u7684\u5b9e\u9645\u91c7\u7528\u7387\u548c\u516c\u5f00\u53ef\u7528\u6027\u4ecd\u7136\u8f83\u4f4e\u3002", "method": "\u4ecb\u7ecdPyLate\uff0c\u4e00\u4e2a\u5efa\u7acb\u5728Sentence Transformers\u4e4b\u4e0a\u7684\u7b80\u5316\u5e93\uff0c\u4ee5\u539f\u751f\u652f\u6301\u591a\u5411\u91cf\u67b6\u6784\uff0c\u7ee7\u627f\u4e86\u5176\u9ad8\u6548\u7684\u8bad\u7ec3\u3001\u9ad8\u7ea7\u65e5\u5fd7\u8bb0\u5f55\u548c\u81ea\u52a8\u5316\u6a21\u578b\u5361\u751f\u6210\uff0c\u540c\u65f6\u53ea\u9700\u8981\u5bf9\u7528\u6237\u5df2\u7ecf\u719f\u6089\u7684\u4ee3\u7801\u6a21\u677f\u8fdb\u884c\u6700\u5c11\u7684\u4ee3\u7801\u66f4\u6539\u3002", "result": "PyLate\u5df2\u7ecf\u652f\u6301\u4e86\u6700\u5148\u8fdb\u7684\u6a21\u578b\u7684\u5f00\u53d1\uff0c\u5305\u62ecGTE-ModernColBERT\u548cReason-ModernColBERT\uff0c\u5c55\u793a\u4e86\u5b83\u5728\u7814\u7a76\u548c\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u6548\u7528\u3002", "conclusion": "PyLate\u901a\u8fc7\u63d0\u4f9b\u9ad8\u6548\u7d22\u5f15\u7b49\u591a\u5411\u91cf\u7279\u5b9a\u529f\u80fd\uff0c\u65e8\u5728\u52a0\u901f\u5ef6\u8fdf\u4ea4\u4e92\u6a21\u578b\u7684\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\uff0c\u4ece\u800c\u91ca\u653e\u5b83\u4eec\u5728\u73b0\u4ee3IR\u7cfb\u7edf\u4e2d\u7684\u5168\u90e8\u6f5c\u529b\u3002PyLate\u5df2\u7ecf\u652f\u6301\u4e86\u6700\u5148\u8fdb\u7684\u6a21\u578b\u7684\u5f00\u53d1\uff0c\u5305\u62ecGTE-ModernColBERT\u548cReason-ModernColBERT\uff0c\u5c55\u793a\u4e86\u5b83\u5728\u7814\u7a76\u548c\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u6548\u7528\u3002"}}
{"id": "2508.03098", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03098", "abs": "https://arxiv.org/abs/2508.03098", "authors": ["Haoran Wang", "Xiongxiao Xu", "Baixiang Huang", "Kai Shu"], "title": "Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language Models in Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances the factual accuracy of large\nlanguage models (LLMs) by conditioning outputs on external knowledge sources.\nHowever, when retrieval involves private or sensitive data, RAG systems are\nsusceptible to extraction attacks that can leak confidential information\nthrough generated responses. We propose Privacy-Aware Decoding (PAD), a\nlightweight, inference-time defense that adaptively injects calibrated Gaussian\nnoise into token logits during generation. PAD integrates confidence-based\nscreening to selectively protect high-risk tokens, efficient sensitivity\nestimation to minimize unnecessary noise, and context-aware noise calibration\nto balance privacy with generation quality. A \\renyi Differential Privacy (RDP)\naccountant rigorously tracks cumulative privacy loss, enabling explicit\nper-response $(\\varepsilon, \\delta)$-DP guarantees for sensitive outputs.\nUnlike prior approaches requiring retraining or corpus-level filtering, PAD is\nmodel-agnostic and operates entirely at decoding time with minimal\ncomputational overhead. Experiments on three real-world datasets demonstrate\nthat PAD substantially reduces private information leakage while preserving\nresponse utility, outperforming existing retrieval- and post-processing-based\ndefenses. Our work takes an important step toward mitigating privacy risks in\nRAG via decoding strategies, paving the way for universal and scalable privacy\nsolutions in sensitive domains. Our code is available:\nhttps://github.com/wang2226/PAD.", "AI": {"tldr": "This paper introduces Privacy-Aware Decoding (PAD), a novel defense mechanism against privacy leaks in Retrieval-Augmented Generation (RAG) systems. PAD injects noise during decoding to protect sensitive information and offers provable privacy guarantees, outperforming existing methods with minimal overhead.", "motivation": "Retrieval-Augmented Generation (RAG) enhances the factual accuracy of large language models (LLMs) by conditioning outputs on external knowledge sources. However, when retrieval involves private or sensitive data, RAG systems are susceptible to extraction attacks that can leak confidential information through generated responses.", "method": "Privacy-Aware Decoding (PAD), a lightweight, inference-time defense that adaptively injects calibrated Gaussian noise into token logits during generation. PAD integrates confidence-based screening to selectively protect high-risk tokens, efficient sensitivity estimation to minimize unnecessary noise, and context-aware noise calibration to balance privacy with generation quality. A Renyi Differential Privacy (RDP) accountant rigorously tracks cumulative privacy loss, enabling explicit per-response (epsilon, delta)-DP guarantees for sensitive outputs.", "result": "Experiments on three real-world datasets demonstrate that PAD substantially reduces private information leakage while preserving response utility, outperforming existing retrieval- and post-processing-based defenses.", "conclusion": "PAD substantially reduces private information leakage while preserving response utility, outperforming existing retrieval- and post-processing-based defenses. The work takes an important step toward mitigating privacy risks in RAG via decoding strategies, paving the way for universal and scalable privacy solutions in sensitive domains."}}
{"id": "2508.02751", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02751", "abs": "https://arxiv.org/abs/2508.02751", "authors": ["Yi Zhao", "Yajuan Peng", "Cam-Tu Nguyen", "Zuchao Li", "Xiaoliang Wang", "Hai Zhao", "Xiaoming Fu"], "title": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for Efficient LLM Inference", "comment": null, "summary": "KV cache eviction has emerged as an effective solution to alleviate resource\nconstraints faced by LLMs in long-context scenarios. However, existing\ntoken-level eviction methods often overlook two critical aspects: (1) their\nirreversible eviction strategy fails to adapt to dynamic attention patterns\nduring decoding (the saliency shift problem), and (2) they treat both\nmarginally important tokens and truly unimportant tokens equally, despite the\ncollective significance of marginal tokens to model performance (the marginal\ninformation over-compression problem). To address these issues, we design two\ncompensation mechanisms based on the high similarity of attention matrices\nbetween LLMs of different scales. We propose SmallKV, a small model assisted\ncompensation method for KV cache compression. SmallKV can maintain attention\nmatching between different-scale LLMs to: 1) assist the larger model in\nperceiving globally important information of attention; and 2) use the smaller\nmodel's attention scores to approximate those of marginal tokens in the larger\nmodel. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and\nLongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency\nevaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than\nbaseline methods, highlighting its potential for efficient and performant LLM\ninference in resource constrained environments.", "AI": {"tldr": "SmallKV\u901a\u8fc7\u5c0f\u6a21\u578b\u8f85\u52a9\u8865\u507f\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u957f\u6587\u672c\u573a\u666f\u4e0bLLM\u7684KV\u7f13\u5b58\u9a71\u9010\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709\u7684token\u7ea7\u522b\u9a71\u9010\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u4e86\u4e24\u4e2a\u5173\u952e\u65b9\u9762\uff1a(1)\u5b83\u4eec\u4e0d\u53ef\u9006\u8f6c\u7684\u9a71\u9010\u7b56\u7565\u65e0\u6cd5\u9002\u5e94\u89e3\u7801\u8fc7\u7a0b\u4e2d\u52a8\u6001\u7684\u6ce8\u610f\u6a21\u5f0f(\u663e\u8457\u6027\u8f6c\u79fb\u95ee\u9898)\uff1b(2)\u5b83\u4eec\u5e73\u7b49\u5730\u5bf9\u5f85\u8fb9\u7f18\u91cd\u8981token\u548c\u771f\u6b63\u4e0d\u91cd\u8981\u7684token\uff0c\u5c3d\u7ba1\u8fb9\u7f18token\u5bf9\u6a21\u578b\u6027\u80fd\u5177\u6709\u96c6\u4f53\u91cd\u8981\u6027(\u8fb9\u7f18\u4fe1\u606f\u8fc7\u5ea6\u538b\u7f29\u95ee\u9898)\u3002", "method": "\u57fa\u4e8e\u4e0d\u540c\u89c4\u6a21LLM\u4e4b\u95f4\u6ce8\u610f\u529b\u77e9\u9635\u7684\u9ad8\u76f8\u4f3c\u6027\uff0c\u8bbe\u8ba1\u4e86\u4e24\u79cd\u8865\u507f\u673a\u5236\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u5c0f\u578b\u6a21\u578b\u8f85\u52a9\u7684KV\u7f13\u5b58\u538b\u7f29\u65b9\u6cd5SmallKV\u3002", "result": "\u5728GSM8K\u3001BBH\u3001MT-Bench\u548cLongBench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86SmallKV\u7684\u6709\u6548\u6027\u3002\u6548\u7387\u8bc4\u4f30\u8868\u660e\uff0cSmallKV\u7684\u541e\u5410\u91cf\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u9ad81.75 - 2.56\u500d\u3002", "conclusion": "SmallKV\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684LLM\u63a8\u7406\u3002"}}
{"id": "2508.02936", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02936", "abs": "https://arxiv.org/abs/2508.02936", "authors": ["Songkun Yan", "Zhi Li", "Siyu Zhu", "Yixin Wen", "Mofan Zhang", "Mengye Chen", "Jie Cao", "Yang Hong"], "title": "AQUAH: Automatic Quantification and Unified Agent in Hydrology", "comment": "8 pages, 5 figures, 2025 ICCV SEA workshop paper", "summary": "We introduce AQUAH, the first end-to-end language-based agent designed\nspecifically for hydrologic modeling. Starting from a simple natural-language\nprompt (e.g., 'simulate floods for the Little Bighorn basin from 2020 to\n2022'), AQUAH autonomously retrieves the required terrain, forcing, and gauge\ndata; configures a hydrologic model; runs the simulation; and generates a\nself-contained PDF report. The workflow is driven by vision-enabled large\nlanguage models, which interpret maps and rasters on the fly and steer key\ndecisions such as outlet selection, parameter initialization, and uncertainty\ncommentary. Initial experiments across a range of U.S. basins show that AQUAH\ncan complete cold-start simulations and produce analyst-ready documentation\nwithout manual intervention. The results are judged by hydrologists as clear,\ntransparent, and physically plausible. While further calibration and validation\nare still needed for operational deployment, these early outcomes highlight the\npromise of LLM-centered, vision-grounded agents to streamline complex\nenvironmental modeling and lower the barrier between Earth observation data,\nphysics-based tools, and decision makers.", "AI": {"tldr": "AQUAH is the first end-to-end language-based agent for hydrologic modeling. It uses vision-enabled large language models to automate the process of retrieving data, configuring a hydrologic model, running the simulation, and generating a report.", "motivation": "To introduce AQUAH, the first end-to-end language-based agent designed specifically for hydrologic modeling.", "method": "vision-enabled large language models, which interpret maps and rasters on the fly and steer key decisions such as outlet selection, parameter initialization, and uncertainty commentary.", "result": "AQUAH autonomously retrieves the required terrain, forcing, and gauge data; configures a hydrologic model; runs the simulation; and generates a self-contained PDF report. Initial experiments across a range of U.S. basins show that AQUAH can complete cold-start simulations and produce analyst-ready documentation without manual intervention. The results are judged by hydrologists as clear, transparent, and physically plausible.", "conclusion": "AQUAH can complete cold-start simulations and produce analyst-ready documentation without manual intervention. The results are judged by hydrologists as clear, transparent, and physically plausible. While further calibration and validation are still needed for operational deployment, these early outcomes highlight the promise of LLM-centered, vision-grounded agents to streamline complex environmental modeling and lower the barrier between Earth observation data, physics-based tools, and decision makers."}}
{"id": "2508.02905", "categories": ["cs.CV", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.02905", "abs": "https://arxiv.org/abs/2508.02905", "authors": ["Mahnoor Fatima Saad", "Ziad Al-Halah"], "title": "How Would It Sound? Material-Controlled Multimodal Acoustic Profile Generation for Indoor Scenes", "comment": "Accepted to ICCV 2025. Project Page:\n  https://mahnoor-fatima-saad.github.io/m-capa.html", "summary": "How would the sound in a studio change with a carpeted floor and acoustic\ntiles on the walls? We introduce the task of material-controlled acoustic\nprofile generation, where, given an indoor scene with specific audio-visual\ncharacteristics, the goal is to generate a target acoustic profile based on a\nuser-defined material configuration at inference time. We address this task\nwith a novel encoder-decoder approach that encodes the scene's key properties\nfrom an audio-visual observation and generates the target Room Impulse Response\n(RIR) conditioned on the material specifications provided by the user. Our\nmodel enables the generation of diverse RIRs based on various material\nconfigurations defined dynamically at inference time. To support this task, we\ncreate a new benchmark, the Acoustic Wonderland Dataset, designed for\ndeveloping and evaluating material-aware RIR prediction methods under diverse\nand challenging settings. Our results demonstrate that the proposed model\neffectively encodes material information and generates high-fidelity RIRs,\noutperforming several baselines and state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6750\u6599\u63a7\u5236\u7684\u58f0\u5b66\u5256\u9762\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5f00\u53d1\u548c\u8bc4\u4f30\u6750\u6599\u611f\u77e5\u7684 RIR \u9884\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u5728\u5177\u6709\u7279\u5b9a\u89c6\u542c\u7279\u5f81\u7684\u5ba4\u5185\u573a\u666f\u4e2d\uff0c\u5730\u6bef\u5730\u9762\u548c\u5899\u58c1\u4e0a\u7684\u5438\u97f3\u74f7\u7816\u5982\u4f55\u6539\u53d8\u5de5\u4f5c\u5ba4\u4e2d\u7684\u58f0\u97f3\u3002", "method": "\u4e00\u79cd\u65b0\u9896\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ece\u89c6\u542c\u89c2\u5bdf\u4e2d\u7f16\u7801\u573a\u666f\u7684\u5173\u952e\u5c5e\u6027\uff0c\u5e76\u6839\u636e\u7528\u6237\u63d0\u4f9b\u7684\u6750\u6599\u89c4\u683c\u751f\u6210\u76ee\u6807\u623f\u95f4\u8109\u51b2\u54cd\u5e94\uff08RIR\uff09\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u7f16\u7801\u6750\u6599\u4fe1\u606f\u5e76\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684 RIR\uff0c\u4f18\u4e8e\u51e0\u79cd\u57fa\u7ebf\u65b9\u6cd5\u548c\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u7f16\u7801\u6750\u6599\u4fe1\u606f\u5e76\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u623f\u95f4\u8109\u51b2\u54cd\u5e94\uff08RIR\uff09\uff0c\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u548c\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.03606", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2508.03606", "abs": "https://arxiv.org/abs/2508.03606", "authors": ["Domiziano Scarcelli", "Filippo Betello", "Giuseppe Perelli", "Fabrizio Silvestri", "Gabriele Tolomei"], "title": "Demystifying Sequential Recommendations: Counterfactual Explanations via Genetic Algorithms", "comment": null, "summary": "Sequential Recommender Systems (SRSs) have demonstrated remarkable\neffectiveness in capturing users' evolving preferences. However, their inherent\ncomplexity as \"black box\" models poses significant challenges for\nexplainability. This work presents the first counterfactual explanation\ntechnique specifically developed for SRSs, introducing a novel approach in this\nspace, addressing the key question: What minimal changes in a user's\ninteraction history would lead to different recommendations? To achieve this,\nwe introduce a specialized genetic algorithm tailored for discrete sequences\nand show that generating counterfactual explanations for sequential data is an\nNP-Complete problem. We evaluate these approaches across four experimental\nsettings, varying between targeted-untargeted and categorized-uncategorized\nscenarios, to comprehensively assess their capability in generating meaningful\nexplanations. Using three different datasets and three models, we are able to\ndemonstrate that our methods successfully generate interpretable counterfactual\nexplanation while maintaining model fidelity close to one. Our findings\ncontribute to the growing field of Explainable AI by providing a framework for\nunderstanding sequential recommendation decisions through the lens of \"what-if\"\nscenarios, ultimately enhancing user trust and system transparency.", "AI": {"tldr": "This paper presents a counterfactual explanation technique for SRSs using a genetic algorithm, showing how minimal changes in user history can alter recommendations, thereby improving explainability and trust.", "motivation": "The inherent complexity of SRSs as black box models poses challenges for explainability.", "method": "A specialized genetic algorithm is tailored for discrete sequences.", "result": "The method successfully generates interpretable counterfactual explanations while maintaining model fidelity close to one across three datasets and three models.", "conclusion": "The paper introduces a counterfactual explanation technique for Sequential Recommender Systems (SRSs), enhancing user trust and system transparency."}}
{"id": "2508.03110", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03110", "abs": "https://arxiv.org/abs/2508.03110", "authors": ["Zizhong Li", "Haopeng Zhang", "Jiawei Zhang"], "title": "Token-Level Precise Attack on RAG: Searching for the Best Alternatives to Mislead Generation", "comment": null, "summary": "While large language models (LLMs) have achieved remarkable success in\nproviding trustworthy responses for knowledge-intensive tasks, they still face\ncritical limitations such as hallucinations and outdated knowledge. To address\nthese issues, the retrieval-augmented generation (RAG) framework enhances LLMs\nwith access to external knowledge via a retriever, enabling more accurate and\nreal-time outputs about the latest events. However, this integration brings new\nsecurity vulnerabilities: the risk that malicious content in the external\ndatabase can be retrieved and used to manipulate model outputs. Although prior\nwork has explored attacks on RAG systems, existing approaches either rely\nheavily on access to the retriever or fail to jointly consider both retrieval\nand generation stages, limiting their effectiveness, particularly in black-box\nscenarios. To overcome these limitations, we propose Token-level Precise Attack\non the RAG (TPARAG), a novel framework that targets both white-box and\nblack-box RAG systems. TPARAG leverages a lightweight white-box LLM as an\nattacker to generate and iteratively optimize malicious passages at the token\nlevel, ensuring both retrievability and high attack success in generation.\nExtensive experiments on open-domain QA datasets demonstrate that TPARAG\nconsistently outperforms previous approaches in retrieval-stage and end-to-end\nattack effectiveness. These results further reveal critical vulnerabilities in\nRAG pipelines and offer new insights into improving their robustness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TPARAG\uff0c\u4e00\u79cd\u9488\u5bf9\u767d\u76d2\u548c\u9ed1\u76d2RAG\u7cfb\u7edf\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5728token\u7ea7\u522b\u751f\u6210\u548c\u4f18\u5316\u6076\u610f\u6bb5\u843d\uff0c\u63d0\u9ad8\u4e86\u653b\u51fbRAG\u7cfb\u7edf\u7684\u6210\u529f\u7387\u3002\u5b9e\u9a8c\u8868\u660e\uff0cTPARAG\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86RAG\u7ba1\u9053\u4e2d\u7684\u5173\u952e\u6f0f\u6d1e\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4e3a\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u63d0\u4f9b\u503c\u5f97\u4fe1\u8d56\u7684\u54cd\u5e94\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u9762\u4e34\u8bf8\u5982\u5e7b\u89c9\u548c\u8fc7\u65f6\u77e5\u8bc6\u7b49\u5173\u952e\u9650\u5236\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6\u901a\u8fc7\u68c0\u7d22\u5668\u589e\u5f3aLLM\u5bf9\u5916\u90e8\u77e5\u8bc6\u7684\u8bbf\u95ee\uff0c\u4ece\u800c\u80fd\u591f\u5b9e\u73b0\u5173\u4e8e\u6700\u65b0\u4e8b\u4ef6\u7684\u66f4\u51c6\u786e\u548c\u5b9e\u65f6\u7684\u8f93\u51fa\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u96c6\u6210\u5e26\u6765\u4e86\u65b0\u7684\u5b89\u5168\u6f0f\u6d1e\uff1a\u5916\u90e8\u6570\u636e\u5e93\u4e2d\u7684\u6076\u610f\u5185\u5bb9\u53ef\u80fd\u88ab\u68c0\u7d22\u5e76\u7528\u4e8e\u64cd\u7eb5\u6a21\u578b\u8f93\u51fa\u7684\u98ce\u9669\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4ee4\u724c\u7ea7\u7cbe\u786e\u653b\u51fbRAG\uff08TPARAG\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u65e8\u5728\u7528\u4e8e\u767d\u76d2\u548c\u9ed1\u76d2RAG\u7cfb\u7edf\u3002TPARAG\u5229\u7528\u8f7b\u91cf\u7ea7\u767d\u76d2LLM\u4f5c\u4e3a\u653b\u51fb\u8005\uff0c\u4ee5\u5728\u4ee4\u724c\u7ea7\u522b\u751f\u6210\u548c\u8fed\u4ee3\u4f18\u5316\u6076\u610f\u6bb5\u843d\uff0c\u4ece\u800c\u786e\u4fdd\u53ef\u68c0\u7d22\u6027\u548c\u751f\u6210\u4e2d\u7684\u9ad8\u653b\u51fb\u6210\u529f\u7387\u3002", "result": "\u5728\u5f00\u653e\u57dfQA\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cTPARAG\u5728\u68c0\u7d22\u9636\u6bb5\u548c\u7aef\u5230\u7aef\u653b\u51fb\u6709\u6548\u6027\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u4ee5\u524d\u7684\u65b9\u6cd5\u3002", "conclusion": "TPARAG\u5728\u68c0\u7d22\u9636\u6bb5\u548c\u7aef\u5230\u7aef\u653b\u51fb\u6709\u6548\u6027\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u4ee5\u524d\u7684\u65b9\u6cd5\u3002\u8fd9\u4e9b\u7ed3\u679c\u8fdb\u4e00\u6b65\u63ed\u793a\u4e86RAG\u7ba1\u9053\u4e2d\u7684\u5173\u952e\u6f0f\u6d1e\uff0c\u5e76\u4e3a\u63d0\u9ad8\u5176\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2508.02753", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02753", "abs": "https://arxiv.org/abs/2508.02753", "authors": ["Haonan Yang", "Jianchao Tang", "Zhuo Li", "Long Lan"], "title": "DMSC: Dynamic Multi-Scale Coordination Framework for Time Series Forecasting", "comment": null, "summary": "Time Series Forecasting (TSF) faces persistent challenges in modeling\nintricate temporal dependencies across different scales. Despite recent\nadvances leveraging different decomposition operations and novel architectures\nbased on CNN, MLP or Transformer, existing methods still struggle with static\ndecomposition strategies, fragmented dependency modeling, and inflexible fusion\nmechanisms, limiting their ability to model intricate temporal dependencies. To\nexplicitly solve the mentioned three problems respectively, we propose a novel\nDynamic Multi-Scale Coordination Framework (DMSC) with Multi-Scale Patch\nDecomposition block (EMPD), Triad Interaction Block (TIB) and Adaptive Scale\nRouting MoE block (ASR-MoE). Specifically, EMPD is designed as a built-in\ncomponent to dynamically segment sequences into hierarchical patches with\nexponentially scaled granularities, eliminating predefined scale constraints\nthrough input-adaptive patch adjustment. TIB then jointly models intra-patch,\ninter-patch, and cross-variable dependencies within each layer's decomposed\nrepresentations. EMPD and TIB are jointly integrated into layers forming a\nmulti-layer progressive cascade architecture, where coarse-grained\nrepresentations from earlier layers adaptively guide fine-grained feature\nextraction in subsequent layers via gated pathways. And ASR-MoE dynamically\nfuses multi-scale predictions by leveraging specialized global and local\nexperts with temporal-aware weighting. Comprehensive experiments on thirteen\nreal-world benchmarks demonstrate that DMSC consistently maintains\nstate-of-the-art (SOTA) performance and superior computational efficiency for\nTSF tasks. Code is available at https://github.com/1327679995/DMSC.", "AI": {"tldr": "DMSC \u662f\u4e00\u79cd\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u52a8\u6001\u591a\u5c3a\u5ea6\u534f\u8c03\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4fdd\u6301\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5728\u9759\u6001\u5206\u89e3\u7b56\u7565\u3001\u788e\u7247\u5316\u7684\u4f9d\u8d56\u6027\u5efa\u6a21\u548c\u4e0d\u7075\u6d3b\u7684\u878d\u5408\u673a\u5236\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5bf9\u590d\u6742\u65f6\u95f4\u4f9d\u8d56\u6027\u8fdb\u884c\u5efa\u6a21\u7684\u80fd\u529b\u3002", "method": "\u52a8\u6001\u591a\u5c3a\u5ea6\u534f\u8c03\u6846\u67b6 (DMSC)\uff0c\u5177\u6709\u591a\u5c3a\u5ea6\u8865\u4e01\u5206\u89e3\u5757 (EMPD)\u3001\u4e09\u5143\u4ea4\u4e92\u5757 (TIB) \u548c\u81ea\u9002\u5e94\u5c3a\u5ea6\u8def\u7531 MoE \u5757 (ASR-MoE)\u3002", "result": "\u5728 13 \u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cDMSC \u59cb\u7ec8\u4fdd\u6301\u6700\u5148\u8fdb (SOTA) \u7684\u6027\u80fd\u548c\u5353\u8d8a\u7684\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "DMSC\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4fdd\u6301\u6700\u5148\u8fdb (SOTA) \u7684\u6027\u80fd\u548c\u5353\u8d8a\u7684\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2508.02951", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02951", "abs": "https://arxiv.org/abs/2508.02951", "authors": ["Mahtab Bigverdi", "Wisdom Ikezogwo", "Kevin Zhang", "Hyewon Jeong", "Mingyu Lu", "Sungjae Cho", "Linda Shapiro", "Ranjay Krishna"], "title": "MedBLINK: Probing Basic Perception in Multimodal Language Models for Medicine", "comment": null, "summary": "Multimodal language models (MLMs) show promise for clinical decision support\nand diagnostic reasoning, raising the prospect of end-to-end automated medical\nimage interpretation. However, clinicians are highly selective in adopting AI\ntools; a model that makes errors on seemingly simple perception tasks such as\ndetermining image orientation or identifying whether a CT scan is\ncontrast-enhance are unlikely to be adopted for clinical tasks. We introduce\nMedblink, a benchmark designed to probe these models for such perceptual\nabilities. Medblink spans eight clinically meaningful tasks across multiple\nimaging modalities and anatomical regions, totaling 1,429 multiple-choice\nquestions over 1,605 images. We evaluate 19 state-of-the-art MLMs, including\ngeneral purpose (GPT4o, Claude 3.5 Sonnet) and domain specific (Med Flamingo,\nLLaVA Med, RadFM) models. While human annotators achieve 96.4% accuracy, the\nbest-performing model reaches only 65%. These results show that current MLMs\nfrequently fail at routine perceptual checks, suggesting the need to strengthen\ntheir visual grounding to support clinical adoption. Data is available on our\nproject page.", "AI": {"tldr": "Medblink, a benchmark, reveals that current MLMs frequently fail at routine perceptual checks", "motivation": "MLMs show promise for clinical decision support and diagnostic reasoning, raising the prospect of end-to-end automated medical image interpretation. However, clinicians are highly selective in adopting AI tools", "method": "a benchmark designed to probe these models for such perceptual abilities", "result": "While human annotators achieve 96.4% accuracy, the best-performing model reaches only 65%", "conclusion": "current MLMs frequently fail at routine perceptual checks, suggesting the need to strengthen their visual grounding to support clinical adoption"}}
{"id": "2508.02917", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.02917", "abs": "https://arxiv.org/abs/2508.02917", "authors": ["Vebj\u00f8rn Haug K\u00e5sene", "Pierre Lison"], "title": "Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces", "comment": "This paper has been accepted to ICNSLP 2025", "summary": "Vision-and-Language Navigation (VLN) refers to the task of enabling\nautonomous robots to navigate unfamiliar environments by following natural\nlanguage instructions. While recent Large Vision-Language Models (LVLMs) have\nshown promise in this task, most current VLM systems rely on models\nspecifically designed and optimized for navigation, leaving the potential of\noff-the-shelf LVLMs underexplored. Furthermore, while older VLN approaches used\nlow-level action spaces with egocentric views and atomic actions (such as \"turn\nleft\" or \"move forward\"), newer models tend to favor panoramic action spaces\nwith discrete navigable viewpoints. This paper investigates (1) whether\noff-the-shelf LVLMs (fine-tuned without architectural modifications or\nsimulator-based training) can effectively support VLN tasks and (2) whether\nsuch models can support both low-level and panoramic action paradigms. To this\nend, we fine-tune the open-source model Qwen2.5-VL-3B-Instruct on the\nRoom-to-Room (R2R) dataset and evaluate its empirical performance across both\nlow-level and panoramic action spaces. The best resulting model achieves a 41%\nsuccess rate on the R2R test set, demonstrating that while off-the-shelf LVLMs\ncan learn to perform Vision-and-Language Navigation, they still lag behind\nmodels specifically designed for this task.", "AI": {"tldr": "Investigates the effectiveness of off-the-shelf LVLMs for Vision-and-Language Navigation (VLN) across different action spaces, finding they can perform VLN but underperform compared to specialized models.", "motivation": "Explore the potential of off-the-shelf LVLMs in VLN tasks, which are currently underexplored, and investigate whether such models can support both low-level and panoramic action paradigms.", "method": "Fine-tune the open-source model Qwen2.5-VL-3B-Instruct on the Room-to-Room (R2R) dataset and evaluate its empirical performance across both low-level and panoramic action spaces.", "result": "Achieves a 41% success rate on the R2R test set.", "conclusion": "Off-the-shelf LVLMs can learn to perform Vision-and-Language Navigation, but they still lag behind models specifically designed for this task."}}
{"id": "2508.03628", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03628", "abs": "https://arxiv.org/abs/2508.03628", "authors": ["Soumik Dey", "Benjamin Braun", "Naveen Ravipati", "Hansi Wu", "Binbin Li"], "title": "LLMDistill4Ads: Using Cross-Encoders to Distill from LLM Signals for Advertiser Keyphrase Recommendations at eBay", "comment": null, "summary": "Sellers at eBay are recommended keyphrases to bid on to enhance the\nperformance of their advertising campaigns. The relevance of these keyphrases\nis crucial in avoiding the overcrowding of search systems with irrelevant items\nand maintaining a positive seller perception. It is essential that keyphrase\nrecommendations align with both seller and Search judgments regarding auctions.\nDue to the difficulty in procuring negative human judgment at scale, employing\nLLM-as-a-judge to mimic seller judgment has been established as the norm in\nseveral studies. This study introduces a novel two-step LLM distillation\nprocess from a LLM-judge used to debias our Embedding Based Retrieval (EBR)\nmodel from the various biases that exist in click-data. We distill from an LLM\nteacher via a cross-encoder assistant into a bi-encoder student using a\nmulti-task training approach, ultimately employing the student bi-encoder to\nretrieve relevant advertiser keyphrases. We show that integrating a knowledge\ndistillation process from LLMs in a multi-task training setup enhances\nbi-encoder performance in retrieving relevant advertiser keyphrases at eBay.", "AI": {"tldr": "This study introduces a two-step LLM distillation process to debias an Embedding Based Retrieval model, enhancing its performance in retrieving relevant advertiser keyphrases at eBay.", "motivation": "The relevance of keyphrases is crucial in avoiding the overcrowding of search systems with irrelevant items and maintaining a positive seller perception. It is essential that keyphrase recommendations align with both seller and Search judgments regarding auctions. Due to the difficulty in procuring negative human judgment at scale, employing LLM-as-a-judge to mimic seller judgment has been established as the norm in several studies.", "method": "a novel two-step LLM distillation process from a LLM-judge used to debias our Embedding Based Retrieval (EBR) model from the various biases that exist in click-data. We distill from an LLM teacher via a cross-encoder assistant into a bi-encoder student using a multi-task training approach, ultimately employing the student bi-encoder to retrieve relevant advertiser keyphrases.", "result": "integrating a knowledge distillation process from LLMs in a multi-task training setup enhances bi-encoder performance in retrieving relevant advertiser keyphrases at eBay.", "conclusion": "Integrating a knowledge distillation process from LLMs in a multi-task training setup enhances bi-encoder performance in retrieving relevant advertiser keyphrases at eBay."}}
{"id": "2508.03112", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.03112", "abs": "https://arxiv.org/abs/2508.03112", "authors": ["Motaz Saad", "David Langlois", "Kamel Smaili"], "title": "Cross-lingual Opinions and Emotions Mining in Comparable Documents", "comment": "16 pages, 5 figures", "summary": "Comparable texts are topic-aligned documents in multiple languages that are\nnot direct translations. They are valuable for understanding how a topic is\ndiscussed across languages. This research studies differences in sentiments and\nemotions across English-Arabic comparable documents. First, texts are annotated\nwith sentiment and emotion labels. We apply a cross-lingual method to label\ndocuments with opinion classes (subjective/objective), avoiding reliance on\nmachine translation. To annotate with emotions (anger, disgust, fear, joy,\nsadness, surprise), we manually translate the English WordNet-Affect (WNA)\nlexicon into Arabic, creating bilingual emotion lexicons used to label the\ncomparable corpora. We then apply a statistical measure to assess the agreement\nof sentiments and emotions in each source-target document pair. This comparison\nis especially relevant when the documents originate from different sources. To\nour knowledge, this aspect has not been explored in prior literature. Our study\nincludes English-Arabic document pairs from Euronews, BBC, and Al-Jazeera\n(JSC). Results show that sentiment and emotion annotations align when articles\ncome from the same news agency and diverge when they come from different ones.\nThe proposed method is language-independent and generalizable to other language\npairs.", "AI": {"tldr": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u82f1\u8bed-\u963f\u62c9\u4f2f\u8bed\u53ef\u6bd4\u8f83\u6587\u6863\u4e2d\u60c5\u611f\u548c\u60c5\u7eea\u7684\u5dee\u5f02\u3002\u7ed3\u679c\u8868\u660e\uff0c\u60c5\u611f\u548c\u60c5\u7eea\u6ce8\u91ca\u5728\u6587\u7ae0\u6765\u81ea\u540c\u4e00\u65b0\u95fb\u673a\u6784\u65f6\u4e00\u81f4\uff0c\u800c\u5f53\u6587\u7ae0\u6765\u81ea\u4e0d\u540c\u7684\u65b0\u95fb\u673a\u6784\u65f6\u5219\u4e0d\u4e00\u81f4\u3002", "motivation": "\u53ef\u6bd4\u8f83\u6587\u672c\u662f\u591a\u79cd\u8bed\u8a00\u4e2d\u4e3b\u9898\u5bf9\u9f50\u4f46\u975e\u76f4\u63a5\u7ffb\u8bd1\u7684\u6587\u6863\u3002\u5b83\u4eec\u5bf9\u4e8e\u7406\u89e3\u8de8\u8bed\u8a00\u5982\u4f55\u8ba8\u8bba\u4e00\u4e2a\u4e3b\u9898\u5f88\u6709\u4ef7\u503c\u3002\u8fd9\u9879\u7814\u7a76\u8c03\u67e5\u4e86\u82f1\u8bed-\u963f\u62c9\u4f2f\u8bed\u53ef\u6bd4\u8f83\u6587\u6863\u4e2d\u60c5\u611f\u548c\u60c5\u7eea\u7684\u5dee\u5f02\u3002", "method": "\u6211\u4eec\u5e94\u7528\u4e86\u4e00\u79cd\u8de8\u8bed\u8a00\u65b9\u6cd5\u6765\u6807\u8bb0\u5177\u6709\u89c2\u70b9\u7c7b\u522b\uff08\u4e3b\u89c2/\u5ba2\u89c2\uff09\u7684\u6587\u6863\uff0c\u907f\u514d\u4f9d\u8d56\u673a\u5668\u7ffb\u8bd1\u3002\u4e3a\u4e86\u7528\u60c5\u7eea\uff08\u6124\u6012\u3001\u538c\u6076\u3001\u6050\u60e7\u3001\u5feb\u4e50\u3001\u60b2\u4f24\u3001\u60ca\u8bb6\uff09\u8fdb\u884c\u6ce8\u91ca\uff0c\u6211\u4eec\u624b\u52a8\u5c06\u82f1\u8bed WordNet-Affect (WNA) \u8bcd\u5178\u7ffb\u8bd1\u6210\u963f\u62c9\u4f2f\u8bed\uff0c\u521b\u5efa\u7528\u4e8e\u6807\u8bb0\u53ef\u6bd4\u8f83\u8bed\u6599\u5e93\u7684\u53cc\u8bed\u60c5\u611f\u8bcd\u5178\u3002\u7136\u540e\uff0c\u6211\u4eec\u5e94\u7528\u7edf\u8ba1\u6d4b\u91cf\u6765\u8bc4\u4f30\u6bcf\u4e2a\u6e90-\u76ee\u6807\u6587\u6863\u5bf9\u4e2d\u60c5\u611f\u548c\u60c5\u7eea\u7684\u4e00\u81f4\u6027\u3002", "result": "\u60c5\u611f\u548c\u60c5\u7eea\u6ce8\u91ca\u5728\u6587\u7ae0\u6765\u81ea\u540c\u4e00\u65b0\u95fb\u673a\u6784\u65f6\u4e00\u81f4\uff0c\u800c\u5f53\u6587\u7ae0\u6765\u81ea\u4e0d\u540c\u7684\u65b0\u95fb\u673a\u6784\u65f6\u5219\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u60c5\u611f\u548c\u60c5\u7eea\u6ce8\u91ca\u5728\u6587\u7ae0\u6765\u81ea\u540c\u4e00\u65b0\u95fb\u673a\u6784\u65f6\u4e00\u81f4\uff0c\u800c\u5f53\u6587\u7ae0\u6765\u81ea\u4e0d\u540c\u7684\u65b0\u95fb\u673a\u6784\u65f6\u5219\u4e0d\u4e00\u81f4\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u662f\u8bed\u8a00\u72ec\u7acb\u7684\uff0c\u5e76\u4e14\u53ef\u4ee5\u63a8\u5e7f\u5230\u5176\u4ed6\u8bed\u8a00\u5bf9\u3002"}}
{"id": "2508.02762", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02762", "abs": "https://arxiv.org/abs/2508.02762", "authors": ["Dahun Kim", "Anelia Angelova"], "title": "Context-Adaptive Multi-Prompt LLM Embedding for Vision-Language Alignment", "comment": null, "summary": "We propose Context-Adaptive Multi-Prompt Embedding, a novel approach to\nenrich semantic representations in vision-language contrastive learning. Unlike\nstandard CLIP-style models that rely on a single text embedding, our method\nintroduces multiple structured prompts, each containing a distinct adaptive\ntoken that captures diverse semantic aspects of the input text. We process all\nprompts jointly in a single forward pass. The resulting prompt embeddings are\ncombined into a unified text representation, enabling semantically richer\nalignment with visual features. To further promote semantic diversity and\nrepresentation quality, we incorporate a diversity regularization loss and a\nnegation-aware loss, encouraging specialization across prompts and improving\ncontrastive discrimination. Our method achieves consistent improvements on both\nimage-text and video-text retrieval benchmarks.", "AI": {"tldr": "enrich semantic representations in vision-language contrastive learning by Context-Adaptive Multi-Prompt Embedding", "motivation": "enrich semantic representations in vision-language contrastive learning. Unlike standard CLIP-style models that rely on a single text embedding", "method": "Context-Adaptive Multi-Prompt Embedding, introduces multiple structured prompts, each containing a distinct adaptive token that captures diverse semantic aspects of the input text. We process all prompts jointly in a single forward pass. The resulting prompt embeddings are combined into a unified text representation", "result": "enabling semantically richer alignment with visual features. To further promote semantic diversity and representation quality, we incorporate a diversity regularization loss and a negation-aware loss, encouraging specialization across prompts and improving contrastive discrimination", "conclusion": "The proposed method achieves consistent improvements on both image-text and video-text retrieval benchmarks."}}
{"id": "2508.02959", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02959", "abs": "https://arxiv.org/abs/2508.02959", "authors": ["Chia-Tung Ho", "Jing Gong", "Xufeng Yao", "Yunsheng Bai", "Abhishek B Akkur", "Haoxing Ren"], "title": "Polymath: A Self-Optimizing Agent with Dynamic Hierarchical Workflow", "comment": "18 pages, 12 figures, under review for AAAI2026", "summary": "Large language models (LLMs) excel at solving complex tasks by executing\nagentic workflows composed of detailed instructions and structured operations.\nYet, building general-purpose agents by manually embedding foundation models\ninto agentic systems such as Chain-of-Thought, Self-Reflection, and ReACT\nthrough text interfaces limits scalability and efficiency. Recently, many\nresearchers have sought to automate the generation and optimization of these\nworkflows through code-based representations. However, existing methods often\nrely on labeled datasets to train and optimize workflows, making them\nineffective and inflexible for solving real-world, dynamic problems where\nlabeled data is unavailable. To address this challenge, we introduce Polymath,\na self-optimizing agent with dynamic hierarchical workflow that leverages the\nflexibility of task flow graphs and the expressiveness of code-represented\nworkflows to solve a wide range of real-world, dynamic problems. The proposed\noptimization methodology integrates multi-grid-inspired graph optimization with\na self-reflection-guided evolutionary algorithm to refine workflows without\nlabeled data. Experimental results on six benchmark datasets across coding,\nmath, and multi-turn QA tasks show that Polymath achieves 8.1% average\nimprovement over state-of-the-art baselines.", "AI": {"tldr": "Polymath \u662f\u4e00\u79cd\u5177\u6709\u52a8\u6001\u5206\u5c42\u5de5\u4f5c\u6d41\u7a0b\u7684\u81ea\u4f18\u5316\u4ee3\u7406\uff0c\u5b83\u5229\u7528\u4efb\u52a1\u6d41\u7a0b\u56fe\u7684\u7075\u6d3b\u6027\u548c\u4ee3\u7801\u8868\u793a\u7684\u5de5\u4f5c\u6d41\u7a0b\u7684\u8868\u8fbe\u80fd\u529b\u6765\u89e3\u51b3\u5404\u79cd\u73b0\u5b9e\u4e16\u754c\u7684\u52a8\u6001\u95ee\u9898\uff0c\u65e0\u9700\u6807\u8bb0\u6570\u636e\u5373\u53ef\u4f18\u5316\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u901a\u8fc7\u6587\u672c\u754c\u9762\u624b\u52a8\u5c06\u57fa\u7840\u6a21\u578b\u5d4c\u5165\u5230\u8bf8\u5982 Chain-of-Thought\u3001Self-Reflection \u548c ReACT \u7b49\u4ee3\u7406\u7cfb\u7edf\u4e2d\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u6807\u8bb0\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u548c\u4f18\u5316\u5de5\u4f5c\u6d41\u7a0b\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u5bf9\u4e8e\u89e3\u51b3\u65e0\u6cd5\u83b7\u5f97\u6807\u8bb0\u6570\u636e\u7684\u73b0\u5b9e\u4e16\u754c\u52a8\u6001\u95ee\u9898\u65e0\u6548\u4e14\u4e0d\u7075\u6d3b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u591a\u7f51\u683c\u542f\u53d1\u5f0f\u56fe\u4f18\u5316\u4e0e\u81ea\u6211\u53cd\u601d\u5f15\u5bfc\u7684\u8fdb\u5316\u7b97\u6cd5\u76f8\u7ed3\u5408\uff0c\u4ee5\u5728\u6ca1\u6709\u6807\u7b7e\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u6539\u8fdb\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "Polymath \u5b9e\u73b0\u4e86\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u5e73\u5747 8.1% \u7684\u6539\u8fdb\u3002", "conclusion": "Polymath\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08\u5305\u62ec\u7f16\u7801\u3001\u6570\u5b66\u548c\u591a\u8f6e QA \u4efb\u52a1\uff09\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0cPolymath \u5e73\u5747\u63d0\u9ad8\u4e86 8.1%\u3002"}}
{"id": "2508.02923", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02923", "abs": "https://arxiv.org/abs/2508.02923", "authors": ["Minh-Hai Nguyen", "Edouard Pauwels", "Pierre Weiss"], "title": "How Diffusion Prior Landscapes Shape the Posterior in Blind Deconvolution", "comment": null, "summary": "The Maximum A Posteriori (MAP) estimation is a widely used framework in blind\ndeconvolution to recover sharp images from blurred observations. The estimated\nimage and blur filter are defined as the maximizer of the posterior\ndistribution. However, when paired with sparsity-promoting image priors, MAP\nestimation has been shown to favors blurry solutions, limiting its\neffectiveness. In this paper, we revisit this result using diffusion-based\npriors, a class of models that capture realistic image distributions. Through\nan empirical examination of the prior's likelihood landscape, we uncover two\nkey properties: first, blurry images tend to have higher likelihoods; second,\nthe landscape contains numerous local minimizers that correspond to natural\nimages. Building on these insights, we provide a theoretical analysis of the\nblind deblurring posterior. This reveals that the MAP estimator tends to\nproduce sharp filters (close to the Dirac delta function) and blurry solutions.\nHowever local minimizers of the posterior, which can be obtained with gradient\ndescent, correspond to realistic, natural images, effectively solving the blind\ndeconvolution problem. Our findings suggest that overcoming MAP's limitations\nrequires good local initialization to local minima in the posterior landscape.\nWe validate our analysis with numerical experiments, demonstrating the\npractical implications of our insights for designing improved priors and\noptimization techniques.", "AI": {"tldr": "MAP \u4f30\u8ba1\u901a\u5e38\u4f1a\u5bfc\u81f4\u6a21\u7cca\u56fe\u50cf\uff0c\u4f46\u540e\u9a8c\u7684\u5c40\u90e8\u6700\u5c0f\u503c\u53ef\u4ee5\u63d0\u4f9b\u6e05\u6670\u7684\u56fe\u50cf\u3002", "motivation": "\u6700\u5927\u540e\u9a8c (MAP) \u4f30\u8ba1\u662f\u76f2\u53cd\u5377\u79ef\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6a21\u7cca\u7684\u89c2\u5bdf\u4e2d\u6062\u590d\u6e05\u6670\u7684\u56fe\u50cf\u3002\u7136\u800c\uff0c\u5f53\u4e0e\u7a00\u758f\u6027\u4fc3\u8fdb\u56fe\u50cf\u5148\u9a8c\u914d\u5bf9\u65f6\uff0cMAP \u4f30\u8ba1\u5df2\u88ab\u8bc1\u660e\u6709\u5229\u4e8e\u6a21\u7cca\u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5176\u6709\u6548\u6027\u3002", "method": "\u4f7f\u7528\u6269\u6563\u5148\u9a8c", "result": "MAP \u4f30\u8ba1\u5668\u503e\u5411\u4e8e\u4ea7\u751f\u9510\u5229\u7684\u6ee4\u6ce2\u5668\uff08\u63a5\u8fd1\u72c4\u62c9\u514b delta \u51fd\u6570\uff09\u548c\u6a21\u7cca\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u53ef\u4ee5\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u83b7\u5f97\u7684\u540e\u9a8c\u7684\u5c40\u90e8\u6700\u5c0f\u503c\u5bf9\u5e94\u4e8e\u771f\u5b9e\u7684\u81ea\u7136\u56fe\u50cf\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u76f2\u53cd\u5377\u79ef\u95ee\u9898\u3002", "conclusion": "\u514b\u670d MAP \u7684\u5c40\u9650\u6027\u9700\u8981\u5728\u540e\u9a8c\u666f\u89c2\u4e2d\u826f\u597d\u5730\u5c40\u90e8\u521d\u59cb\u5316\u5230\u5c40\u90e8\u6700\u5c0f\u503c\u3002"}}
{"id": "2508.03670", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03670", "abs": "https://arxiv.org/abs/2508.03670", "authors": ["Fernando F. Granado", "Davi A. Bezerra", "Iuri Queiroz", "Nathan Oliveira", "Pedro Fernandes", "Bruno Schock"], "title": "Personalized Recommendation of Dish and Restaurant Collections on iFood", "comment": "Workshop on Two-sided Marketplace Optimization: Search, Discovery,\n  Matching, Pricing & Growth in conjunction with KDD Conference (KDD 2025) in\n  Toronto, Canada", "summary": "Food delivery platforms face the challenge of helping users navigate vast\ncatalogs of restaurants and dishes to find meals they truly enjoy. This paper\npresents RED, an automated recommendation system designed for iFood, Latin\nAmerica's largest on-demand food delivery platform, to personalize the\nselection of curated food collections displayed to millions of users. Our\napproach employs a LightGBM classifier that scores collections based on three\nfeature groups: collection characteristics, user-collection similarity, and\ncontextual information. To address the cold-start problem of recommending newly\ncreated collections, we develop content-based representations using item\nembeddings and implement monotonicity constraints to improve generalization. We\ntackle data scarcity by bootstrapping from category carousel interactions and\naddress visibility bias through unbiased sampling of impressions and purchases\nin production. The system demonstrates significant real-world impact through\nextensive A/B testing with 5-10% of iFood's user base. Online results of our\nA/B tests add up to 97% improvement in Card Conversion Rate and 1.4% increase\nin overall App Conversion Rate compared to popularity-based baselines. Notably,\nour offline accuracy metrics strongly correlate with online performance,\nenabling reliable impact prediction before deployment. To our knowledge, this\nis the first work to detail large-scale recommendation of curated food\ncollections in a dynamic commercial environment.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86RED\uff0c\u4e00\u4e2a\u4e3aiFood\u8bbe\u8ba1\u7684\u81ea\u52a8\u5316\u63a8\u8350\u7cfb\u7edf\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u63a8\u8350\u7cbe\u9009\u98df\u54c1\u96c6\u5408\u3002", "motivation": "\u98df\u54c1\u914d\u9001\u5e73\u53f0\u9762\u4e34\u7740\u5e2e\u52a9\u7528\u6237\u6d4f\u89c8\u5927\u91cf\u9910\u5385\u548c\u83dc\u80b4\u76ee\u5f55\uff0c\u627e\u5230\u4ed6\u4eec\u771f\u6b63\u559c\u6b22\u7684\u98df\u7269\u7684\u6311\u6218\u3002\u672c\u6587\u4ecb\u7ecd RED\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e3a\u62c9\u4e01\u7f8e\u6d32\u6700\u5927\u7684\u6309\u9700\u98df\u54c1\u914d\u9001\u5e73\u53f0 iFood \u8bbe\u8ba1\u7684\u81ea\u52a8\u5316\u63a8\u8350\u7cfb\u7edf\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u5411\u6570\u767e\u4e07\u7528\u6237\u5c55\u793a\u7684\u7cbe\u9009\u98df\u54c1\u96c6\u5408\u3002", "method": "\u91c7\u7528 LightGBM \u5206\u7c7b\u5668\uff0c\u6839\u636e\u96c6\u5408\u7279\u5f81\u3001\u7528\u6237-\u96c6\u5408\u76f8\u4f3c\u5ea6\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9\u96c6\u5408\u8fdb\u884c\u8bc4\u5206\u3002\u4e3a\u4e86\u89e3\u51b3\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8e\u5185\u5bb9\u7684\u8868\u793a\uff0c\u4f7f\u7528\u9879\u76ee\u5d4c\u5165\u5e76\u5b9e\u65bd\u5355\u8c03\u6027\u7ea6\u675f\u4ee5\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002\u901a\u8fc7\u4ece\u7c7b\u522b\u8f6e\u64ad\u4ea4\u4e92\u4e2d\u5f15\u5bfc\u4ee5\u53ca\u901a\u8fc7\u5bf9\u751f\u4ea7\u4e2d\u7684\u5c55\u793a\u548c\u8d2d\u4e70\u8fdb\u884c\u65e0\u504f\u91c7\u6837\u6765\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u548c\u53ef\u89c1\u6027\u504f\u5dee\u3002", "result": "\u901a\u8fc7\u4e0e iFood 5-10% \u7528\u6237\u7fa4\u8fdb\u884c\u7684\u5927\u91cf A/B \u6d4b\u8bd5\uff0c\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u663e\u8457\u7684\u5b9e\u9645\u5f71\u54cd\u3002\u4e0e\u57fa\u4e8e\u53d7\u6b22\u8fce\u7a0b\u5ea6\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0cA/B \u6d4b\u8bd5\u7684\u5728\u7ebf\u7ed3\u679c\u4f7f\u5361\u7247\u8f6c\u5316\u7387\u63d0\u9ad8\u4e86 97%\uff0c\u6574\u4f53\u5e94\u7528\u7a0b\u5e8f\u8f6c\u5316\u7387\u63d0\u9ad8\u4e86 1.4%\u3002", "conclusion": "\u8be5\u8bba\u6587\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u5728\u52a8\u6001\u5546\u4e1a\u73af\u5883\u4e2d\u5927\u89c4\u6a21\u63a8\u8350\u7cbe\u9009\u98df\u54c1\u96c6\u5408\u7684\u9996\u4e2a\u6848\u4f8b\u3002"}}
{"id": "2508.03137", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03137", "abs": "https://arxiv.org/abs/2508.03137", "authors": ["Ge Shi", "Kaiyu Huang", "Guochen Feng"], "title": "Long Story Generation via Knowledge Graph and Literary Theory", "comment": null, "summary": "The generation of a long story consisting of several thousand words is a\nsub-task in the field of long text generation~(LTG). Previous research has\naddressed this challenge through outline-based generation, which employs a\nmulti-stage method for generating outlines into stories. However, this approach\nsuffers from two common issues: almost inevitable theme drift caused by the\nloss of memory of previous outlines, and tedious plots with incoherent logic\nthat are less appealing to human readers.\n  In this paper, we propose the multi-agent Story Generator structure to\nimprove the multi-stage method, using large language models~(LLMs) as the core\ncomponents of agents. To avoid theme drift, we introduce a memory storage model\ncomprising two components: a long-term memory storage that identifies the most\nimportant memories, thereby preventing theme drift; and a short-term memory\nstorage that retains the latest outlines from each generation round. To\nincorporate engaging elements into the story, we design a story theme obstacle\nframework based on literary narratology theory that introduces uncertain\nfactors and evaluation criteria to generate outline. This framework calculates\nthe similarity of the former storyline and enhances the appeal of the story by\nbuilding a knowledge graph and integrating new node content. Additionally, we\nestablish a multi-agent interaction stage to simulate writer-reader interaction\nthrough dialogue and revise the story text according to feedback, to ensure it\nremains consistent and logical. Evaluations against previous methods\ndemonstrate that our approach can generate higher-quality long stories.", "AI": {"tldr": "Introduces a multi-agent story generator with memory and a theme obstacle framework to improve long story generation quality.", "motivation": "Theme drift and tedious plots with incoherent logic in previous outline-based long story generation methods.", "method": "A multi-agent Story Generator structure with memory storage (long-term and short-term) and a story theme obstacle framework based on literary narratology theory.", "result": "The proposed approach generates higher-quality long stories compared to previous methods.", "conclusion": "The proposed multi-agent Story Generator structure improves the multi-stage method by using LLMs as agents, incorporating memory storage models and a story theme obstacle framework, and establishing a multi-agent interaction stage to simulate writer-reader interaction, resulting in higher-quality long stories."}}
{"id": "2508.02771", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02771", "abs": "https://arxiv.org/abs/2508.02771", "authors": ["Oc\u00e9ane Doremus", "Ariel Guerra-Adames", "Marta Avalos-Fernandez", "Vianney Jouhet", "C\u00e9dric Gil-Jardin\u00e9", "Emmanuel Lagarde"], "title": "Synthetic medical data generation: state of the art and application to trauma mechanism classification", "comment": "Accepted to CIBB 2025 as a short paper", "summary": "Faced with the challenges of patient confidentiality and scientific\nreproducibility, research on machine learning for health is turning towards the\nconception of synthetic medical databases. This article presents a brief\noverview of state-of-the-art machine learning methods for generating synthetic\ntabular and textual data, focusing their application to the automatic\nclassification of trauma mechanisms, followed by our proposed methodology for\ngenerating high-quality, synthetic medical records combining tabular and\nunstructured text data.", "AI": {"tldr": "overview of machine learning methods for generating synthetic tabular and textual data in medical databases", "motivation": "Faced with the challenges of patient confidentiality and scientific reproducibility, research on machine learning for health is turning towards the conception of synthetic medical databases.", "method": "machine learning methods for generating synthetic tabular and textual data", "result": "generating high-quality, synthetic medical records combining tabular and unstructured text data.", "conclusion": "This article presents a brief overview of state-of-the-art machine learning methods for generating synthetic tabular and textual data, focusing their application to the automatic classification of trauma mechanisms, followed by our proposed methodology for generating high-quality, synthetic medical records combining tabular and unstructured text data."}}
{"id": "2508.02961", "categories": ["cs.AI", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02961", "abs": "https://arxiv.org/abs/2508.02961", "authors": ["Boshi Huang", "Fabio Nonato de Paula"], "title": "Defend LLMs Through Self-Consciousness", "comment": "Presented at KDD Workshop on Ethical Artificial Intelligence: Methods\n  and Applications (EAI) 2025", "summary": "This paper introduces a novel self-consciousness defense mechanism for Large\nLanguage Models (LLMs) to combat prompt injection attacks. Unlike traditional\napproaches that rely on external classifiers, our method leverages the LLM's\ninherent reasoning capabilities to perform self-protection. We propose a\nframework that incorporates Meta-Cognitive and Arbitration Modules, enabling\nLLMs to evaluate and regulate their own outputs autonomously. Our approach is\nevaluated on seven state-of-the-art LLMs using two datasets: AdvBench and\nPrompt-Injection-Mixed-Techniques-2024. Experiment results demonstrate\nsignificant improvements in defense success rates across models and datasets,\nwith some achieving perfect and near-perfect defense in Enhanced Mode. We also\nanalyze the trade-off between defense success rate improvement and\ncomputational overhead. This self-consciousness method offers a lightweight,\ncost-effective solution for enhancing LLM ethics, particularly beneficial for\nGenAI use cases across various platforms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u610f\u8bc6\u7684\u8f7b\u91cf\u7ea7\u3001\u4f4e\u6210\u672c\u7684 LLM \u9632\u5fa1\u673a\u5236\uff0c\u53ef\u6709\u6548\u9632\u5fa1\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u5e76\u63d0\u9ad8\u4e86 LLM \u7684\u4f26\u7406\u5b89\u5168\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u7684\u5a01\u80c1\uff0c\u9700\u8981\u6709\u6548\u7684\u9632\u5fa1\u673a\u5236\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u5143\u8ba4\u77e5\u548c\u4ef2\u88c1\u6a21\u5757\u7684\u6846\u67b6\uff0c\u4f7f LLM \u80fd\u591f\u81ea\u4e3b\u8bc4\u4f30\u548c\u8c03\u8282\u81ea\u8eab\u7684\u8f93\u51fa\u3002", "result": "\u5728\u4e03\u4e2a\u6700\u5148\u8fdb\u7684 LLM \u4e0a\uff0c\u4f7f\u7528 AdvBench \u548c Prompt-Injection-Mixed-Techniques-2024 \u4e24\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u9632\u5fa1\u6210\u529f\u7387\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u610f\u8bc6\u9632\u5fa1\u673a\u5236\uff0c\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4ee5\u5bf9\u6297\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u3002\u8be5\u65b9\u6cd5\u5229\u7528 LLM \u56fa\u6709\u7684\u63a8\u7406\u80fd\u529b\u6765\u8fdb\u884c\u81ea\u6211\u4fdd\u62a4\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u5206\u7c7b\u5668\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6a21\u578b\u548c\u6570\u636e\u96c6\u7684\u9632\u5fa1\u6210\u529f\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u7740\u6539\u8fdb\uff0c\u5728\u589e\u5f3a\u6a21\u5f0f\u4e0b\uff0c\u90e8\u5206\u6a21\u578b\u5b9e\u73b0\u4e86\u5b8c\u7f8e\u548c\u63a5\u8fd1\u5b8c\u7f8e\u7684\u9632\u5fa1\u3002"}}
{"id": "2508.02927", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02927", "abs": "https://arxiv.org/abs/2508.02927", "authors": ["Srikanth Muralidharan", "Heitor R. Medeiros", "Masih Aminbeidokhti", "Eric Granger", "Marco Pedersoli"], "title": "Infrared Object Detection with Ultra Small ConvNets: Is ImageNet Pretraining Still Useful?", "comment": null, "summary": "Many real-world applications require recognition models that are robust to\ndifferent operational conditions and modalities, but at the same time run on\nsmall embedded devices, with limited hardware. While for normal size models,\npre-training is known to be very beneficial in accuracy and robustness, for\nsmall models, that can be employed for embedded and edge devices, its effect is\nnot clear. In this work, we investigate the effect of ImageNet pretraining on\nincreasingly small backbone architectures (ultra-small models, with $<$1M\nparameters) with respect to robustness in downstream object detection tasks in\nthe infrared visual modality. Using scaling laws derived from standard object\nrecognition architectures, we construct two ultra-small backbone families and\nsystematically study their performance. Our experiments on three different\ndatasets reveal that while ImageNet pre-training is still useful, beyond a\ncertain capacity threshold, it offers diminishing returns in terms of\nout-of-distribution detection robustness. Therefore, we advise practitioners to\nstill use pre-training and, when possible avoid too small models as while they\nmight work well for in-domain problems, they are brittle when working\nconditions are different.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86 ImageNet \u9884\u8bad\u7ec3\u5bf9\u8d85\u5c0f\u578b\u9aa8\u5e72\u67b6\u6784\u5728\u7ea2\u5916\u89c6\u89c9\u6a21\u6001\u7684\u4e0b\u6e38\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u9c81\u68d2\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u9700\u8981\u5bf9\u4e0d\u540c\u64cd\u4f5c\u6761\u4ef6\u548c\u6a21\u5f0f\u5177\u6709\u9c81\u68d2\u6027\u7684\u8bc6\u522b\u6a21\u578b\uff0c\u4f46\u540c\u65f6\u8fd0\u884c\u5728\u5177\u6709\u6709\u9650\u786c\u4ef6\u7684\u5c0f\u578b\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u3002\u867d\u7136\u5bf9\u4e8e\u6b63\u5e38\u5927\u5c0f\u7684\u6a21\u578b\uff0c\u9884\u8bad\u7ec3\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5df2\u77e5\u662f\u975e\u5e38\u6709\u76ca\u7684\uff0c\u4f46\u5bf9\u4e8e\u53ef\u4ee5\u7528\u4e8e\u5d4c\u5165\u5f0f\u548c\u8fb9\u7f18\u8bbe\u5907\u7684\u5c0f\u578b\u6a21\u578b\uff0c\u5176\u6548\u679c\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u4f7f\u7528\u4ece\u6807\u51c6\u5bf9\u8c61\u8bc6\u522b\u67b6\u6784\u5bfc\u51fa\u7684\u7f29\u653e\u5b9a\u5f8b\uff0c\u6784\u5efa\u4e86\u4e24\u4e2a\u8d85\u5c0f\u578b\u9aa8\u5e72\u7cfb\u5217\uff0c\u5e76\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u5b83\u4eec\u7684\u6027\u80fd\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u867d\u7136 ImageNet \u9884\u8bad\u7ec3\u4ecd\u7136\u6709\u7528\uff0c\u4f46\u8d85\u8fc7\u67d0\u4e2a\u5bb9\u91cf\u9608\u503c\u540e\uff0c\u5b83\u5728\u57df\u5916\u68c0\u6d4b\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6536\u76ca\u4f1a\u9012\u51cf\u3002", "conclusion": "ImageNet \u9884\u8bad\u7ec3\u5bf9\u4e8e\u5c0f\u578b\u6a21\u578b\u4ecd\u7136\u6709\u7528\uff0c\u4f46\u8d85\u8fc7\u4e00\u5b9a\u5bb9\u91cf\u9608\u503c\u540e\uff0c\u5176\u5728\u57df\u5916\u68c0\u6d4b\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6536\u76ca\u4f1a\u9012\u51cf\u3002\u5efa\u8bae\u4ece\u4e1a\u8005\u4ecd\u7136\u4f7f\u7528\u9884\u8bad\u7ec3\uff0c\u5e76\u5728\u53ef\u80fd\u7684\u60c5\u51b5\u4e0b\u907f\u514d\u4f7f\u7528\u592a\u5c0f\u7684\u6a21\u578b\uff0c\u56e0\u4e3a\u5b83\u4eec\u5728\u57df\u5185\u95ee\u9898\u4e0a\u53ef\u80fd\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5de5\u4f5c\u6761\u4ef6\u4e0d\u540c\u65f6\u4f1a\u53d8\u5f97\u8106\u5f31\u3002"}}
{"id": "2508.02835", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.02835", "abs": "https://arxiv.org/abs/2508.02835", "authors": ["Kennedy Edemacu", "Vinay M. Shashidhar", "Micheal Tuape", "Dan Abudu", "Beakcheol Jang", "Jong Wook Kim"], "title": "Defending Against Knowledge Poisoning Attacks During Retrieval-Augmented Generation", "comment": "Preprint for Submission", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to\nboost the capabilities of large language models (LLMs) by incorporating\nexternal, up-to-date knowledge sources. However, this introduces a potential\nvulnerability to knowledge poisoning attacks, where attackers can compromise\nthe knowledge source to mislead the generation model. One such attack is the\nPoisonedRAG in which the injected adversarial texts steer the model to generate\nan attacker-chosen response to a target question. In this work, we propose\nnovel defense methods, FilterRAG and ML-FilterRAG, to mitigate the PoisonedRAG\nattack. First, we propose a new property to uncover distinct properties to\ndifferentiate between adversarial and clean texts in the knowledge data source.\nNext, we employ this property to filter out adversarial texts from clean ones\nin the design of our proposed approaches. Evaluation of these methods using\nbenchmark datasets demonstrate their effectiveness, with performances close to\nthose of the original RAG systems.", "AI": {"tldr": "This paper proposes methods to defend against knowledge poisoning attacks in Retrieval-Augmented Generation (RAG) systems.", "motivation": "Retrieval-Augmented Generation (RAG) is vulnerable to knowledge poisoning attacks, such as PoisonedRAG, where attackers compromise the knowledge source to mislead the generation model.", "method": "The paper introduces a new property to distinguish between adversarial and clean texts in the knowledge data source. This property is then used to filter out adversarial texts.", "result": "The proposed defense methods, FilterRAG and ML-FilterRAG, demonstrate effectiveness against PoisonedRAG attacks, achieving performance close to original RAG systems on benchmark datasets.", "conclusion": "The paper proposes FilterRAG and ML-FilterRAG to defend against PoisonedRAG attacks. These methods filter adversarial texts from clean ones using a novel property to differentiate them. Evaluation shows their effectiveness with performance close to original RAG systems."}}
{"id": "2508.03140", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03140", "abs": "https://arxiv.org/abs/2508.03140", "authors": ["Junyao Yang", "Jianwei Wang", "Huiping Zhuang", "Cen Chen", "Ziqian Zeng"], "title": "RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning Capability as Prior", "comment": "15 pages, 7 figures", "summary": "Large Language Models (LLMs) with long chain-of-thought (CoT) capability,\ntermed Reasoning Models, demonstrate superior intricate problem-solving\nabilities through multi-step long CoT reasoning. To create a dual-capability\nmodel with long CoT capability and domain-specific knowledge without\nsubstantial computational and data costs, model merging emerges as a highly\nresource-efficient method. However, significant challenges lie in merging\ndomain-specific LLMs with long CoT ones since nowadays merging methods suffer\nfrom reasoning capability degradation, even gibberish output and output\ncollapse. To overcome this, we introduce RCP-Merging: Merging Long\nChain-of-Thought Models with Domain-Specific Models by Considering Reasoning\nCapability as Prior, a novel merging framework designed to integrate\ndomain-specific LLMs with long CoT capability, meanwhile maintaining model\nperformance in the original domain. Treating reasoning model weights as\nfoundational prior, our method utilizes a reasoning capability indicator to\npreserve core long CoT capability model weights while selectively merging\nessential domain-specific weights. We conducted extensive experiments on\nQwen2.5-7B, Llama3.1-8B, and Qwen2.5-1.5B models in BioMedicine and Finance\ndomains. Our results show that RCP-Merging successfully merges a reasoning\nmodel with domain-specific ones, improving domain task performance by 9.5% and\n9.2% over state-of-the-art methods, without significantly harming the original\nlong CoT reasoning capability.", "AI": {"tldr": "The paper introduces RCP-Merging, a novel merging framework that integrates domain-specific LLMs with long CoT capability while preserving reasoning abilities and domain performance, achieving significant improvements over existing methods in BioMedicine and Finance domains.", "motivation": "To create a dual-capability model with long CoT capability and domain-specific knowledge without substantial computational and data costs, model merging emerges as a highly resource-efficient method. However, significant challenges lie in merging domain-specific LLMs with long CoT ones since nowadays merging methods suffer from reasoning capability degradation, even gibberish output and output collapse.", "method": "RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning Capability as Prior, a novel merging framework designed to integrate domain-specific LLMs with long CoT capability, meanwhile maintaining model performance in the original domain. Treating reasoning model weights as foundational prior, our method utilizes a reasoning capability indicator to preserve core long CoT capability model weights while selectively merging essential domain-specific weights.", "result": "RCP-Merging successfully merges a reasoning model with domain-specific ones, improving domain task performance by 9.5% and 9.2% over state-of-the-art methods, without significantly harming the original long CoT reasoning capability.", "conclusion": "RCP-Merging successfully merges a reasoning model with domain-specific ones, improving domain task performance by 9.5% and 9.2% over state-of-the-art methods, without significantly harming the original long CoT reasoning capability."}}
{"id": "2508.02812", "categories": ["cs.LG", "I.2.4; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.02812", "abs": "https://arxiv.org/abs/2508.02812", "authors": ["Katherine Avery", "Chinmay Pendse", "David Jensen"], "title": "Uncertainty Sets for Distributionally Robust Bandits Using Structural Equation Models", "comment": "10 pages main text, 28 pages total", "summary": "Distributionally robust evaluation estimates the worst-case expected return\nover an uncertainty set of possible covariate and reward distributions, and\ndistributionally robust learning finds a policy that maximizes that worst-case\nreturn across that uncertainty set. Unfortunately, current methods for\ndistributionally robust evaluation and learning create overly conservative\nevaluations and policies. In this work, we propose a practical bandit\nevaluation and learning algorithm that tailors the uncertainty set to specific\nproblems using mathematical programs constrained by structural equation models.\nFurther, we show how conditional independence testing can be used to detect\nshifted variables for modeling. We find that the structural equation model\n(SEM) approach gives more accurate evaluations and learns lower-variance\npolicies than traditional approaches, particularly for large shifts. Further,\nthe SEM approach learns an optimal policy, assuming the model is sufficiently\nwell-specified.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u65b9\u7a0b\u6a21\u578b\u7684\u5b9e\u7528 bandit \u8bc4\u4f30\u548c\u5b66\u4e60\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u53ef\u4ee5\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u548c\u5b66\u4e60\u4f4e\u65b9\u5dee\u7b56\u7565\u3002", "motivation": "\u5f53\u524d\u7684\u5206\u5e03\u9c81\u68d2\u8bc4\u4f30\u548c\u5b66\u4e60\u65b9\u6cd5\u4f1a\u4ea7\u751f\u8fc7\u4e8e\u4fdd\u5b88\u7684\u8bc4\u4f30\u548c\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u7528\u7684 bandit \u8bc4\u4f30\u548c\u5b66\u4e60\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4f7f\u7528\u53d7\u7ed3\u6784\u65b9\u7a0b\u6a21\u578b\u7ea6\u675f\u7684\u6570\u5b66\u7a0b\u5e8f\u4e3a\u7279\u5b9a\u95ee\u9898\u5b9a\u5236\u4e0d\u786e\u5b9a\u6027\u96c6\u3002", "result": "\u7ed3\u6784\u65b9\u7a0b\u6a21\u578b (SEM) \u65b9\u6cd5\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\uff0c\u5e76\u5b66\u4e60\u66f4\u4f4e\u65b9\u5dee\u7684\u7b56\u7565\uff0c\u5c24\u5176\u662f\u5728\u5927\u578b\u8f6c\u79fb\u4e2d\u3002\u6b64\u5916\uff0c\u5047\u8bbe\u6a21\u578b\u5145\u5206\u660e\u786e\uff0cSEM \u65b9\u6cd5\u53ef\u4ee5\u5b66\u4e60\u6700\u4f18\u7b56\u7565\u3002", "conclusion": "SEM \u65b9\u6cd5\u5728\u5927\u578b\u8f6c\u79fb\u4e2d\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\uff0c\u5e76\u5b66\u4e60\u66f4\u4f4e\u65b9\u5dee\u7684\u7b56\u7565\u3002\u5047\u8bbe\u6a21\u578b\u5145\u5206\u660e\u786e\uff0cSEM \u65b9\u6cd5\u53ef\u4ee5\u5b66\u4e60\u6700\u4f18\u7b56\u7565\u3002"}}
{"id": "2508.02979", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02979", "abs": "https://arxiv.org/abs/2508.02979", "authors": ["Peng Ding", "Rick Stevens"], "title": "Unified Tool Integration for LLMs: A Protocol-Agnostic Approach to Function Calling", "comment": "arXiv admin note: substantial text overlap with arXiv:2507.10593", "summary": "The proliferation of tool-augmented Large Language Models (LLMs) has created\na fragmented ecosystem where developers must navigate multiple protocols,\nmanual schema definitions, and complex execution workflows. We address this\nchallenge by proposing a unified approach to tool integration that abstracts\nprotocol differences while optimizing execution performance. Our solution\ndemonstrates how protocol-agnostic design principles can significantly reduce\ndevelopment overhead through automated schema generation, dual-mode concurrent\nexecution, and seamless multi-source tool management. Experimental results show\n60-80% code reduction across integration scenarios, performance improvements up\nto 3.1x through optimized concurrency, and full compatibility with existing\nfunction calling standards. This work contributes both theoretical insights\ninto tool integration architecture and practical solutions for real-world LLM\napplication development.", "AI": {"tldr": "Unified approach to tool integration that abstracts protocol differences while optimizing execution performance.", "motivation": "The proliferation of tool-augmented Large Language Models (LLMs) has created a fragmented ecosystem where developers must navigate multiple protocols, manual schema definitions, and complex execution workflows.", "method": "a unified approach to tool integration that abstracts protocol differences while optimizing execution performance", "result": "Experimental results show 60-80% code reduction across integration scenarios, performance improvements up to 3.1x through optimized concurrency, and full compatibility with existing function calling standards.", "conclusion": "This work contributes both theoretical insights into tool integration architecture and practical solutions for real-world LLM application development."}}
{"id": "2508.02944", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02944", "abs": "https://arxiv.org/abs/2508.02944", "authors": ["Chenxu Zhang", "Zenan Li", "Hongyi Xu", "You Xie", "Xiaochen Zhao", "Tianpei Gu", "Guoxian Song", "Xin Chen", "Chao Liang", "Jianwen Jiang", "Linjie Luo"], "title": "X-Actor: Emotional and Expressive Long-Range Portrait Acting from Audio", "comment": "Project Page at https://byteaigc.github.io/X-Actor/", "summary": "We present X-Actor, a novel audio-driven portrait animation framework that\ngenerates lifelike, emotionally expressive talking head videos from a single\nreference image and an input audio clip. Unlike prior methods that emphasize\nlip synchronization and short-range visual fidelity in constrained speaking\nscenarios, X-Actor enables actor-quality, long-form portrait performance\ncapturing nuanced, dynamically evolving emotions that flow coherently with the\nrhythm and content of speech. Central to our approach is a two-stage decoupled\ngeneration pipeline: an audio-conditioned autoregressive diffusion model that\npredicts expressive yet identity-agnostic facial motion latent tokens within a\nlong temporal context window, followed by a diffusion-based video synthesis\nmodule that translates these motions into high-fidelity video animations. By\noperating in a compact facial motion latent space decoupled from visual and\nidentity cues, our autoregressive diffusion model effectively captures\nlong-range correlations between audio and facial dynamics through a\ndiffusion-forcing training paradigm, enabling infinite-length emotionally-rich\nmotion prediction without error accumulation. Extensive experiments demonstrate\nthat X-Actor produces compelling, cinematic-style performances that go beyond\nstandard talking head animations and achieves state-of-the-art results in\nlong-range, audio-driven emotional portrait acting.", "AI": {"tldr": "X-Actor \u662f\u4e00\u79cd\u65b0\u7684\u97f3\u9891\u9a71\u52a8\u8096\u50cf\u52a8\u753b\u6846\u67b6\uff0c\u5b83\u53ef\u4ee5\u4ece\u5355\u4e2a\u53c2\u8003\u56fe\u50cf\u548c\u8f93\u5165\u97f3\u9891\u526a\u8f91\u751f\u6210\u903c\u771f\u3001\u60c5\u611f\u4e30\u5bcc\u7684\u8bf4\u8bdd\u5934\u50cf\u89c6\u9891\u3002", "motivation": "\u4e0e\u4ee5\u5f80\u5f3a\u8c03\u5507\u90e8\u540c\u6b65\u548c\u53d7\u9650\u8bf4\u8bdd\u573a\u666f\u4e2d\u7684\u77ed\u7a0b\u89c6\u89c9\u4fdd\u771f\u5ea6\u7684\u65b9\u6cd5\u4e0d\u540c\uff0cX-Actor \u80fd\u591f\u5b9e\u73b0\u6f14\u5458\u7ea7\u522b\u7684\u957f\u7bc7\u8096\u50cf\u8868\u6f14\uff0c\u6355\u6349\u4e0e\u8bed\u97f3\u8282\u594f\u548c\u5185\u5bb9\u8fde\u8d2f\u6d41\u52a8\u7684\u7ec6\u5fae\u3001\u52a8\u6001\u53d1\u5c55\u7684\u60c5\u611f\u3002", "method": "\u4e00\u4e2a\u4e24\u9636\u6bb5\u89e3\u8026\u751f\u6210\u7ba1\u9053\uff1a\u4e00\u4e2a\u97f3\u9891\u6761\u4ef6\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u8868\u60c5\u4e30\u5bcc\u4f46\u4e0e\u8eab\u4efd\u65e0\u5173\u7684\u9762\u90e8\u8fd0\u52a8\u6f5c\u5728\u6807\u8bb0\uff1b\u7136\u540e\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u9891\u5408\u6210\u6a21\u5757\uff0c\u5c06\u8fd9\u4e9b\u8fd0\u52a8\u8f6c\u5316\u4e3a\u9ad8\u4fdd\u771f\u89c6\u9891\u52a8\u753b\u3002", "result": "X-Actor \u4ea7\u751f\u5f15\u4eba\u6ce8\u76ee\u7684\u7535\u5f71\u98ce\u683c\u8868\u6f14\uff0c\u8d85\u8d8a\u4e86\u6807\u51c6\u7684\u8bf4\u8bdd\u5934\u50cf\u52a8\u753b\uff0c\u5e76\u5728\u957f\u7a0b\u97f3\u9891\u9a71\u52a8\u7684\u60c5\u611f\u8096\u50cf\u8868\u6f14\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "X-Actor \u5728\u957f\u7a0b\u97f3\u9891\u9a71\u52a8\u7684\u60c5\u611f\u8096\u50cf\u8868\u6f14\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u80fd\u591f\u751f\u6210\u5f15\u4eba\u6ce8\u76ee\u7684\u7535\u5f71\u98ce\u683c\u8868\u6f14\u3002"}}
{"id": "2508.03178", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03178", "abs": "https://arxiv.org/abs/2508.03178", "authors": ["Chenyang Wang", "Liang Wen", "Shousheng Jia", "Xiangzheng Zhang", "Liang Xu"], "title": "Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following", "comment": "12 pages, 10 figures, 7 tables", "summary": "While advancements in the reasoning abilities of LLMs have significantly\nenhanced their performance in solving mathematical problems, coding tasks, and\ngeneral puzzles, their effectiveness in accurately adhering to instructions\nremains inconsistent, particularly with more complex directives. Our\ninvestigation identifies lazy reasoning during the thinking stage as the\nprimary factor contributing to poor instruction adherence. To mitigate this\nissue, we propose a comprehensive framework designed to enable rigorous\nreasoning processes involving preview and self-checking, essential for\nsatisfying strict instruction constraints. Specifically, we first generate\ninstructions with complex constraints and apply a filtering process to obtain\nvalid prompts, resulting in three distinct prompt datasets categorized as hard,\neasy, and pass. Then, we employ rejection sampling on the pass prompts to\ncurate a small yet high-quality dataset, enabling a cold-start initialization\nof the model and facilitating its adaptation to effective reasoning patterns.\nSubsequently, we employ an entropy-preserving supervised fine-tuning\n(Entropy-SFT) strategy coupled with token-wise entropy-adaptive (TEA-RL)\nreinforcement learning guided by rule-based dense rewards. This approach\nencourages the model to transform its reasoning mechanism, ultimately fostering\ngeneralizable reasoning abilities that encompass preview and self-checking.\nExtensive experiments conducted on instruction-following benchmarks demonstrate\nremarkable performance improvements across various model scales. Notably, our\nLight-IF-32B model surpasses both larger open-source models such as DeepSeek-R1\nand closed-source models like Doubao-1.6.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u89c8\u548c\u81ea\u6211\u68c0\u67e5\u6765\u63d0\u9ad8LLM\u5728\u590d\u6742\u6307\u4ee4\u4e0b\u7684\u9075\u5faa\u80fd\u529b\uff0c\u5e76\u5728\u6307\u4ee4\u9075\u5faa\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u89e3\u51b3\u6570\u5b66\u95ee\u9898\u3001\u7f16\u7801\u4efb\u52a1\u548c\u4e00\u822c\u8c1c\u9898\u65b9\u9762\u7684\u80fd\u529b\u663e\u8457\u589e\u5f3a\uff0c\u4f46\u5728\u51c6\u786e\u9075\u5faa\u6307\u4ee4\u65b9\u9762\u7684\u6709\u6548\u6027\u4ecd\u7136\u4e0d\u7a33\u5b9a\uff0c\u5c24\u5176\u662f\u5728\u66f4\u590d\u6742\u7684\u6307\u4ee4\u4e0b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u601d\u8003\u9636\u6bb5\u7684\u60f0\u6027\u63a8\u7406\u662f\u5bfc\u81f4\u6307\u4ee4\u9075\u5faa\u4e0d\u4f73\u7684\u4e3b\u8981\u56e0\u7d20\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u9884\u89c8\u548c\u81ea\u6211\u68c0\u67e5\u5b9e\u73b0\u4e25\u683c\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u7ed3\u5408\u4e86Entropy-SFT\u7b56\u7565\u548ctoken-wise entropy-adaptive (TEA-RL)\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728\u6307\u4ee4\u9075\u5faa\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0cLight-IF-32B\u6a21\u578b\u8d85\u8d8a\u4e86DeepSeek-R1\u548cDoubao-1.6\u7b49\u6a21\u578b\u3002", "conclusion": "Light-IF-32B\u6a21\u578b\u5728\u6307\u4ee4\u9075\u5faa\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86DeepSeek-R1\u548cDoubao-1.6\u7b49\u5927\u578b\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u3002"}}
{"id": "2508.02833", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02833", "abs": "https://arxiv.org/abs/2508.02833", "authors": ["Lei Pang", "Ruinan Jin"], "title": "On the Theory and Practice of GRPO: A Trajectory-Corrected Approach with Fast Convergence", "comment": "12 pages", "summary": "Group Relative Policy Optimization (GRPO), recently proposed by DeepSeek, is\na critic-free reinforcement learning algorithm for fine tuning large language\nmodels. It replaces the value function in Proximal Policy Optimization (PPO)\nwith group normalized rewards, while retaining PPO style token level importance\nsampling based on an old policy. We show that GRPO update rule in fact\nestimates the policy gradient at the old policy rather than the current one.\nHowever, since the old policy is refreshed every few steps, the discrepancy\nbetween the two remains small limiting the impact of this bias in practice. We\nvalidate this through an ablation study in which importance sampling is\nentirely removed, and updates are instead performed using the gradient\nestimated at a fixed old policy across multiple optimization steps. Remarkably,\nthis simplification results in performance comparable to standard GRPO.\n  Motivated by these findings, we propose a new algorithm: Trajectory level\nImportance Corrected GRPO (TIC GRPO). TIC GRPO replaces token level importance\nratios with a single trajectory level probability ratio, yielding an unbiased\nestimate of the current policy gradient while preserving the critic free\nstructure. Furthermore, we present the first theoretical convergence analysis\nfor GRPO style methods, covering both the original GRPO and our proposed\nvariant.", "AI": {"tldr": "GRPO\u66f4\u65b0\u89c4\u5219\u5b9e\u9645\u4e0a\u4f30\u8ba1\u7684\u662f\u65e7\u7b56\u7565\u800c\u4e0d\u662f\u5f53\u524d\u7b56\u7565\u7684\u7b56\u7565\u68af\u5ea6\u3002", "motivation": "DeepSeek\u6700\u8fd1\u63d0\u51fa\u7684Group Relative Policy Optimization (GRPO)\u662f\u4e00\u79cd\u7528\u4e8e\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65e0\u6279\u8bc4\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002\u5b83\u7528\u7ec4\u5f52\u4e00\u5316\u5956\u52b1\u4ee3\u66ff\u8fd1\u7aef\u7b56\u7565\u4f18\u5316(PPO)\u4e2d\u7684\u4ef7\u503c\u51fd\u6570\uff0c\u540c\u65f6\u4fdd\u7559\u57fa\u4e8e\u65e7\u7b56\u7565\u7684PPO\u98ce\u683c\u7684\u4ee4\u724c\u7ea7\u91cd\u8981\u6027\u62bd\u6837\u3002\u6211\u4eec\u8868\u660e\uff0cGRPO\u66f4\u65b0\u89c4\u5219\u5b9e\u9645\u4e0a\u4f30\u8ba1\u7684\u662f\u65e7\u7b56\u7565\u800c\u4e0d\u662f\u5f53\u524d\u7b56\u7565\u7684\u7b56\u7565\u68af\u5ea6\u3002\u7136\u800c\uff0c\u7531\u4e8e\u65e7\u7b56\u7565\u6bcf\u9694\u51e0\u4e2a\u6b65\u9aa4\u5c31\u4f1a\u5237\u65b0\u4e00\u6b21\uff0c\u56e0\u6b64\u4e24\u8005\u4e4b\u95f4\u7684\u5dee\u5f02\u4ecd\u7136\u5f88\u5c0f\uff0c\u4ece\u800c\u9650\u5236\u4e86\u8fd9\u79cd\u504f\u5dee\u5728\u5b9e\u8df5\u4e2d\u7684\u5f71\u54cd\u3002\u6211\u4eec\u901a\u8fc7\u4e00\u9879\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u70b9\uff0c\u5728\u8be5\u7814\u7a76\u4e2d\uff0c\u91cd\u8981\u6027\u62bd\u6837\u88ab\u5b8c\u5168\u79fb\u9664\uff0c\u800c\u662f\u4f7f\u7528\u5728\u591a\u4e2a\u4f18\u5316\u6b65\u9aa4\u4e2d\u4ee5\u56fa\u5b9a\u7684\u65e7\u7b56\u7565\u4f30\u8ba1\u7684\u68af\u5ea6\u6267\u884c\u66f4\u65b0\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u79cd\u7b80\u5316\u5bfc\u81f4\u4e86\u4e0e\u6807\u51c6GRPO\u76f8\u5f53\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5:\u8f68\u8ff9\u6c34\u5e73\u91cd\u8981\u6027\u6821\u6b63GRPO(TIC GRPO)", "result": "\u8fd9\u79cd\u7b80\u5316\u5bfc\u81f4\u4e86\u4e0e\u6807\u51c6GRPO\u76f8\u5f53\u7684\u6027\u80fd", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u7b97\u6cd5:\u8f68\u8ff9\u6c34\u5e73\u91cd\u8981\u6027\u6821\u6b63GRPO(TIC GRPO)\u3002TIC GRPO\u7528\u5355\u4e2a\u8f68\u8ff9\u6c34\u5e73\u6982\u7387\u6bd4\u4ee3\u66ff\u4e86\u4ee4\u724c\u6c34\u5e73\u91cd\u8981\u6027\u6bd4\u7387\uff0c\u4ece\u800c\u5728\u4fdd\u7559\u65e0\u6279\u8bc4\u8005\u7ed3\u6784\u7684\u540c\u65f6\uff0c\u4ea7\u751f\u4e86\u5f53\u524d\u7b56\u7565\u68af\u5ea6\u7684\u65e0\u504f\u4f30\u8ba1\u3002\u6b64\u5916\uff0c\u6211\u4eec\u9996\u6b21\u5bf9GRPO\u98ce\u683c\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u7406\u8bba\u6536\u655b\u6027\u5206\u6790\uff0c\u5305\u62ec\u539f\u59cbGRPO\u548c\u6211\u4eec\u63d0\u51fa\u7684\u53d8\u4f53\u3002"}}
{"id": "2508.02994", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02994", "abs": "https://arxiv.org/abs/2508.02994", "authors": ["Fangyi Yu"], "title": "When AIs Judge AIs: The Rise of Agent-as-a-Judge Evaluation for LLMs", "comment": null, "summary": "As large language models (LLMs) grow in capability and autonomy, evaluating\ntheir outputs-especially in open-ended and complex tasks-has become a critical\nbottleneck. A new paradigm is emerging: using AI agents as the evaluators\nthemselves. This \"agent-as-a-judge\" approach leverages the reasoning and\nperspective-taking abilities of LLMs to assess the quality and safety of other\nmodels, promising calable and nuanced alternatives to human evaluation. In this\nreview, we define the agent-as-a-judge concept, trace its evolution from\nsingle-model judges to dynamic multi-agent debate frameworks, and critically\nexamine their strengths and shortcomings. We compare these approaches across\nreliability, cost, and human alignment, and survey real-world deployments in\ndomains such as medicine, law, finance, and education. Finally, we highlight\npressing challenges-including bias, robustness, and meta evaluation-and outline\nfuture research directions. By bringing together these strands, our review\ndemonstrates how agent-based judging can complement (but not replace) human\noversight, marking a step toward trustworthy, scalable evaluation for\nnext-generation LLMs.", "AI": {"tldr": "AI agents are used as evaluators to assess the quality and safety of other models, which can complement but not replace human oversight.", "motivation": "evaluating their outputs-especially in open-ended and complex tasks-has become a critical bottleneck. A new paradigm is emerging: using AI agents as the evaluators themselves.", "method": "define the agent-as-a-judge concept, trace its evolution from single-model judges to dynamic multi-agent debate frameworks, and critically examine their strengths and shortcomings. We compare these approaches across reliability, cost, and human alignment, and survey real-world deployments in domains such as medicine, law, finance, and education.", "result": "assessing the quality and safety of other models, promising calable and nuanced alternatives to human evaluation.", "conclusion": "agent-based judging can complement (but not replace) human oversight, marking a step toward trustworthy, scalable evaluation for next-generation LLMs."}}
{"id": "2508.02967", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02967", "abs": "https://arxiv.org/abs/2508.02967", "authors": ["Dawei Zhang", "Xiaojie Guo"], "title": "Towards Robust Image Denoising with Scale Equivariance", "comment": null, "summary": "Despite notable advances in image denoising, existing models often struggle\nto generalize beyond in-distribution noise patterns, particularly when\nconfronted with out-of-distribution (OOD) conditions characterized by spatially\nvariant noise. This generalization gap remains a fundamental yet underexplored\nchallenge. In this work, we investigate \\emph{scale equivariance} as a core\ninductive bias for improving OOD robustness. We argue that incorporating\nscale-equivariant structures enables models to better adapt from training on\nspatially uniform noise to inference on spatially non-uniform degradations.\nBuilding on this insight, we propose a robust blind denoising framework\nequipped with two key components: a Heterogeneous Normalization Module (HNM)\nand an Interactive Gating Module (IGM). HNM stabilizes feature distributions\nand dynamically corrects features under varying noise intensities, while IGM\nfacilitates effective information modulation via gated interactions between\nsignal and feature paths. Extensive evaluations demonstrate that our model\nconsistently outperforms state-of-the-art methods on both synthetic and\nreal-world benchmarks, especially under spatially heterogeneous noise. Code\nwill be made publicly available.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5c3a\u5ea6\u7b49\u53d8\u6027\u4f5c\u4e3a\u63d0\u9ad8 OOD \u9c81\u68d2\u6027\u7684\u6838\u5fc3\u5f52\u7eb3\u504f\u5dee\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7a33\u5065\u7684\u76f2\u53bb\u566a\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u7a7a\u95f4\u5f02\u6784\u566a\u58f0\u4e0b\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u53bb\u566a\u6a21\u578b\u901a\u5e38\u96be\u4ee5\u63a8\u5e7f\u5230\u5206\u5e03\u5185\u566a\u58f0\u6a21\u5f0f\u4e4b\u5916\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u4ee5\u7a7a\u95f4\u53d8\u5316\u566a\u58f0\u4e3a\u7279\u5f81\u7684\u5206\u5e03\u5916\uff08OOD\uff09\u6761\u4ef6\u65f6\uff0c\u8fd9\u79cd\u6cdb\u5316\u5dee\u8ddd\u4ecd\u7136\u662f\u4e00\u4e2a\u57fa\u672c\u7684\u4f46\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7a33\u5065\u7684\u76f2\u53bb\u566a\u6846\u67b6\uff0c\u914d\u5907\u4e86\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u5f02\u6784\u5f52\u4e00\u5316\u6a21\u5757\uff08HNM\uff09\u548c\u4ea4\u4e92\u95e8\u63a7\u6a21\u5757\uff08IGM\uff09\u3002", "result": "\u8be5\u6a21\u578b\u5728\u7a7a\u95f4\u5f02\u6784\u566a\u58f0\u4e0b\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u7a7a\u95f4\u5f02\u6784\u566a\u58f0\u4e0b\u3002"}}
{"id": "2508.03358", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.03358", "abs": "https://arxiv.org/abs/2508.03358", "authors": ["Tiago G Can\u00e1rio", "Catarina Duarte", "Fl\u00e1vio L. Pinheiro", "Jo\u00e3o L. M. Pereira"], "title": "Taggus: An Automated Pipeline for the Extraction of Characters' Social Networks from Portuguese Fiction Literature", "comment": "24 pages, 5 Figures, 4 Tables", "summary": "Automatically identifying characters and their interactions from fiction\nbooks is, arguably, a complex task that requires pipelines that leverage\nmultiple Natural Language Processing (NLP) methods, such as Named Entity\nRecognition (NER) and Part-of-speech (POS) tagging. However, these methods are\nnot optimized for the task that leads to the construction of Social Networks of\nCharacters. Indeed, the currently available methods tend to underperform,\nespecially in less-represented languages, due to a lack of manually annotated\ndata for training. Here, we propose a pipeline, which we call Taggus, to\nextract social networks from literary fiction works in Portuguese. Our results\nshow that compared to readily available State-of-the-Art tools -- off-the-shelf\nNER tools and Large Language Models (ChatGPT) -- the resulting pipeline, which\nuses POS tagging and a combination of heuristics, achieves satisfying results\nwith an average F1-Score of $94.1\\%$ in the task of identifying characters and\nsolving for co-reference and $75.9\\%$ in interaction detection. These\nrepresent, respectively, an increase of $50.7\\%$ and $22.3\\%$ on results\nachieved by the readily available State-of-the-Art tools. Further steps to\nimprove results are outlined, such as solutions for detecting relationships\nbetween characters. Limitations on the size and scope of our testing samples\nare acknowledged. The Taggus pipeline is publicly available to encourage\ndevelopment in this field for the Portuguese language.2", "AI": {"tldr": "The paper proposes a pipeline, Taggus, to extract social networks from literary fiction works in Portuguese, achieving better results than state-of-the-art tools.", "motivation": "Currently available methods tend to underperform, especially in less-represented languages, due to a lack of manually annotated data for training.", "method": "The pipeline uses POS tagging and a combination of heuristics.", "result": "The Taggus pipeline achieves satisfying results with an average F1-Score of $94.1\\%$ in the task of identifying characters and solving for co-reference and $75.9\\%$ in interaction detection. These represent, respectively, an increase of $50.7\\%$ and $22.3\\%$ on results achieved by the readily available State-of-the-Art tools.", "conclusion": "The Taggus pipeline achieves satisfying results with an average F1-Score of $94.1\\%$ in the task of identifying characters and solving for co-reference and $75.9\\%$ in interaction detection, which represents an increase of $50.7\\%$ and $22.3\\%$ on results achieved by the readily available State-of-the-Art tools. The Taggus pipeline is publicly available to encourage development in this field for the Portuguese language."}}
{"id": "2508.03181", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03181", "abs": "https://arxiv.org/abs/2508.03181", "authors": ["Lukas P\u00e4tz", "Moritz Beyer", "Jannik Sp\u00e4th", "Lasse Bohlen", "Patrick Zschech", "Mathias Kraus", "Julian Rosenberger"], "title": "Analyzing German Parliamentary Speeches: A Machine Learning Approach for Topic and Sentiment Classification", "comment": "Accepted at 20th International Conference on Wirtschaftsinformatik\n  (WI25); September 2025, M\\\"unster, Germany", "summary": "This study investigates political discourse in the German parliament, the\nBundestag, by analyzing approximately 28,000 parliamentary speeches from the\nlast five years. Two machine learning models for topic and sentiment\nclassification were developed and trained on a manually labeled dataset. The\nmodels showed strong classification performance, achieving an area under the\nreceiver operating characteristic curve (AUROC) of 0.94 for topic\nclassification (average across topics) and 0.89 for sentiment classification.\nBoth models were applied to assess topic trends and sentiment distributions\nacross political parties and over time. The analysis reveals remarkable\nrelationships between parties and their role in parliament. In particular, a\nchange in style can be observed for parties moving from government to\nopposition. While ideological positions matter, governing responsibilities also\nshape discourse. The analysis directly addresses key questions about the\nevolution of topics, sentiment dynamics, and party-specific discourse\nstrategies in the Bundestag.", "AI": {"tldr": "ML analysis of Bundestag speeches reveals how party roles (government vs. opposition) shape discourse, with high accuracy in topic and sentiment classification.", "motivation": "investigating political discourse in the German parliament (Bundestag) to understand topic trends, sentiment dynamics, and party-specific discourse strategies.", "method": "machine learning models for topic and sentiment classification trained on a manually labeled dataset of 28,000 parliamentary speeches.", "result": "models achieved AUROCs of 0.94 for topic classification and 0.89 for sentiment classification; analysis reveals relationships between parties and their parliamentary roles.", "conclusion": "governing responsibilities shape discourse in the Bundestag, with changes observed as parties transition between government and opposition."}}
{"id": "2508.02834", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02834", "abs": "https://arxiv.org/abs/2508.02834", "authors": ["Hanqi Feng", "Peng Qiu", "Mengchun Zhang", "Yiran Tao", "You Fan", "Jingtao Xu", "Barnabas Poczos"], "title": "Learning from B Cell Evolution: Adaptive Multi-Expert Diffusion for Antibody Design via Online Optimization", "comment": null, "summary": "Recent advances in diffusion models have shown remarkable potential for\nantibody design, yet existing approaches apply uniform generation strategies\nthat cannot adapt to each antigen's unique requirements. Inspired by B cell\naffinity maturation, where antibodies evolve through multi-objective\noptimization balancing affinity, stability, and self-avoidance, we propose the\nfirst biologically-motivated framework that leverages physics-based domain\nknowledge within an online meta-learning system. Our method employs multiple\nspecialized experts (van der Waals, molecular recognition, energy balance, and\ninterface geometry) whose parameters evolve during generation based on\niterative feedback, mimicking natural antibody refinement cycles. Instead of\nfixed protocols, this adaptive guidance discovers personalized optimization\nstrategies for each target. Our experiments demonstrate that this approach: (1)\ndiscovers optimal SE(3)-equivariant guidance strategies for different antigen\nclasses without pre-training, preserving molecular symmetries throughout\noptimization; (2) significantly enhances hotspot coverage and interface quality\nthrough target-specific adaptation, achieving balanced multi-objective\noptimization characteristic of therapeutic antibodies; (3) establishes a\nparadigm for iterative refinement where each antibody-antigen system learns its\nunique optimization profile through online evaluation; (4) generalizes\neffectively across diverse design challenges, from small epitopes to large\nprotein interfaces, enabling precision-focused campaigns for individual\ntargets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7 B \u7ec6\u80de\u4eb2\u548c\u529b\u6210\u719f\u542f\u53d1\u7684\u6297\u4f53\u8bbe\u8ba1\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u7269\u7406\u5b66\u77e5\u8bc6\u548c\u5728\u7ebf\u5143\u5b66\u4e60\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u6765\u53d1\u73b0\u9488\u5bf9\u6bcf\u4e2a\u6297\u539f\u7684\u4e2a\u6027\u5316\u7b56\u7565\uff0c\u4ece\u800c\u63d0\u9ad8\u6297\u4f53\u8d28\u91cf\u548c\u8bbe\u8ba1\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5e94\u7528\u7edf\u4e00\u7684\u751f\u6210\u7b56\u7565\uff0c\u65e0\u6cd5\u9002\u5e94\u6bcf\u79cd\u6297\u539f\u7684\u72ec\u7279\u9700\u6c42\u3002\u53d7 B \u7ec6\u80de\u4eb2\u548c\u529b\u6210\u719f\u7684\u542f\u53d1\uff0c\u6297\u4f53\u901a\u8fc7\u5e73\u8861\u4eb2\u548c\u529b\u3001\u7a33\u5b9a\u6027\u548c\u81ea\u6211\u907f\u514d\u7684\u591a\u76ee\u6807\u4f18\u5316\u800c\u8fdb\u5316\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u5728\u7ebf\u5143\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u57fa\u4e8e\u7269\u7406\u7684\u9886\u57df\u77e5\u8bc6\uff0c\u91c7\u7528\u591a\u4e2a\u4e13\u4e1a\u4e13\u5bb6\uff08\u8303\u5fb7\u534e\u529b\u3001\u5206\u5b50\u8bc6\u522b\u3001\u80fd\u91cf\u5e73\u8861\u548c\u754c\u9762\u51e0\u4f55\uff09\uff0c\u8fd9\u4e9b\u4e13\u5bb6\u7684\u53c2\u6570\u5728\u57fa\u4e8e\u8fed\u4ee3\u53cd\u9988\u7684\u751f\u6210\u8fc7\u7a0b\u4e2d\u6f14\u53d8\uff0c\u6a21\u4eff\u81ea\u7136\u6297\u4f53\u7cbe\u70bc\u5468\u671f\u3002", "result": "\u8be5\u65b9\u6cd5\uff1a\uff081\uff09\u4e3a\u4e0d\u540c\u7684\u6297\u539f\u7c7b\u522b\u53d1\u73b0\u6700\u4f73\u7684 SE(3)-\u7b49\u53d8\u5f15\u5bfc\u7b56\u7565\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\uff0c\u5728\u6574\u4e2a\u4f18\u5316\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u5206\u5b50\u5bf9\u79f0\u6027\uff1b\uff082\uff09\u901a\u8fc7\u9776\u6807\u7279\u5f02\u6027\u9002\u5e94\uff0c\u663e\u8457\u63d0\u9ad8\u70ed\u70b9\u8986\u76d6\u7387\u548c\u754c\u9762\u8d28\u91cf\uff0c\u5b9e\u73b0\u6cbb\u7597\u6027\u6297\u4f53\u7684\u5e73\u8861\u591a\u76ee\u6807\u4f18\u5316\uff1b\uff083\uff09\u4e3a\u8fed\u4ee3\u6539\u8fdb\u5efa\u7acb\u4e86\u4e00\u4e2a\u8303\u4f8b\uff0c\u5176\u4e2d\u6bcf\u4e2a\u6297\u4f53-\u6297\u539f\u7cfb\u7edf\u901a\u8fc7\u5728\u7ebf\u8bc4\u4f30\u5b66\u4e60\u5176\u72ec\u7279\u7684\u4f18\u5316\u914d\u7f6e\u6587\u4ef6\uff1b\uff084\uff09\u5728\u4e0d\u540c\u7684\u8bbe\u8ba1\u6311\u6218\u4e2d\u6709\u6548\u5730\u63a8\u5e7f\uff0c\u4ece\u5c0f\u7684\u6297\u539f\u8868\u4f4d\u5230\u5927\u7684\u86cb\u767d\u8d28\u754c\u9762\uff0c\u4e3a\u4e2a\u4f53\u76ee\u6807\u5b9e\u73b0\u7cbe\u786e\u805a\u7126\u7684\u6d3b\u52a8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u7ebf\u8bc4\u4f30\u4e3a\u6bcf\u4e2a\u6297\u4f53-\u6297\u539f\u7cfb\u7edf\u5b66\u4e60\u5176\u72ec\u7279\u7684\u4f18\u5316\u914d\u7f6e\u6587\u4ef6\uff0c\u5e76\u5728\u4e0d\u540c\u7684\u8bbe\u8ba1\u6311\u6218\u4e2d\u6709\u6548\u5730\u63a8\u5e7f\uff0c\u4ece\u5c0f\u7684\u6297\u539f\u8868\u4f4d\u5230\u5927\u7684\u86cb\u767d\u8d28\u754c\u9762\uff0c\u4e3a\u4e2a\u4f53\u76ee\u6807\u5b9e\u73b0\u7cbe\u786e\u805a\u7126\u7684\u6d3b\u52a8\u3002"}}
{"id": "2508.02999", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.02999", "abs": "https://arxiv.org/abs/2508.02999", "authors": ["Xinjie Zhao", "Moritz Blum", "Fan Gao", "Yingjian Chen", "Boming Yang", "Luis Marquez-Carpintero", "M\u00f3nica Pina-Navarro", "Yanran Fu", "So Morikawa", "Yusuke Iwasawa", "Yutaka Matsuo", "Chanjun Park", "Irene Li"], "title": "AGENTiGraph: A Multi-Agent Knowledge Graph Framework for Interactive, Domain-Specific LLM Chatbots", "comment": "CIKM 2025, Demo Track", "summary": "AGENTiGraph is a user-friendly, agent-driven system that enables intuitive\ninteraction and management of domain-specific data through the manipulation of\nknowledge graphs in natural language. It gives non-technical users a complete,\nvisual solution to incrementally build and refine their knowledge bases,\nallowing multi-round dialogues and dynamic updates without specialized query\nlanguages. The flexible design of AGENTiGraph, including intent classification,\ntask planning, and automatic knowledge integration, ensures seamless reasoning\nbetween diverse tasks. Evaluated on a 3,500-query benchmark within an\neducational scenario, the system outperforms strong zero-shot baselines\n(achieving 95.12% classification accuracy, 90.45% execution success),\nindicating potential scalability to compliance-critical or multi-step queries\nin legal and medical domains, e.g., incorporating new statutes or research on\nthe fly. Our open-source demo offers a powerful new paradigm for multi-turn\nenterprise knowledge management that bridges LLMs and structured graphs.", "AI": {"tldr": "AGENTiGraph\u662f\u4e00\u4e2a\u7528\u6237\u53cb\u597d\u7684\u3001\u4ee3\u7406\u9a71\u52a8\u7684\u7cfb\u7edf\uff0c\u5b83\u901a\u8fc7\u77e5\u8bc6\u56fe\u64cd\u4f5c\u6765\u5b9e\u73b0\u9886\u57df\u7279\u5b9a\u6570\u636e\u7684\u76f4\u89c2\u4ea4\u4e92\u548c\u7ba1\u7406\u3002", "motivation": "AGENTiGraph\u4f7f\u7528\u6237\u80fd\u591f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u64cd\u4f5c\u77e5\u8bc6\u56fe\u6765\u76f4\u89c2\u5730\u4ea4\u4e92\u548c\u7ba1\u7406\u9886\u57df\u7279\u5b9a\u6570\u636e\u3002", "method": "AGENTiGraph\u91c7\u7528\u610f\u56fe\u5206\u7c7b\u3001\u4efb\u52a1\u89c4\u5212\u548c\u81ea\u52a8\u77e5\u8bc6\u96c6\u6210", "result": "AGENTiGraph\u4e3a\u975e\u6280\u672f\u7528\u6237\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u53ef\u89c6\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u589e\u91cf\u65b9\u5f0f\u6784\u5efa\u548c\u5b8c\u5584\u4ed6\u4eec\u7684\u77e5\u8bc6\u5e93\uff0c\u4ece\u800c\u5141\u8bb8\u8fdb\u884c\u591a\u8f6e\u5bf9\u8bdd\u548c\u52a8\u6001\u66f4\u65b0\uff0c\u800c\u65e0\u9700\u4f7f\u7528\u4e13\u95e8\u7684\u67e5\u8be2\u8bed\u8a00\u3002", "conclusion": "AGENTiGraph\u5728\u6559\u80b2\u573a\u666f\u76843,500\u4e2a\u67e5\u8be2\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5f3a\u5927\u7684zero-shot\u57fa\u7ebf\uff08\u8fbe\u523095.12\uff05\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c90.45\uff05\u7684\u6267\u884c\u6210\u529f\u7387\uff09\uff0c\u8868\u660e\u5728\u6cd5\u5f8b\u548c\u533b\u5b66\u9886\u57df\u5177\u6709\u6f5c\u5728\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.02973", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02973", "abs": "https://arxiv.org/abs/2508.02973", "authors": ["Alakh Desai", "Nuno Vasconcelos"], "title": "Diffusion Models with Adaptive Negative Sampling Without External Resources", "comment": null, "summary": "Diffusion models (DMs) have demonstrated an unparalleled ability to create\ndiverse and high-fidelity images from text prompts. However, they are also\nwell-known to vary substantially regarding both prompt adherence and quality.\nNegative prompting was introduced to improve prompt compliance by specifying\nwhat an image must not contain. Previous works have shown the existence of an\nideal negative prompt that can maximize the odds of the positive prompt. In\nthis work, we explore relations between negative prompting and classifier-free\nguidance (CFG) to develop a sampling procedure, {\\it Adaptive Negative Sampling\nWithout External Resources} (ANSWER), that accounts for both positive and\nnegative conditions from a single prompt. This leverages the internal\nunderstanding of negation by the diffusion model to increase the odds of\ngenerating images faithful to the prompt. ANSWER is a training-free technique,\napplicable to any model that supports CFG, and allows for negative grounding of\nimage concepts without an explicit negative prompts, which are lossy and\nincomplete. Experiments show that adding ANSWER to existing DMs outperforms the\nbaselines on multiple benchmarks and is preferred by humans 2x more over the\nother methods.", "AI": {"tldr": "This paper introduces ANSWER, a training-free technique that improves prompt adherence and image quality in diffusion models by adaptively sampling negative prompts without external resources. It outperforms baselines and is preferred by humans.", "motivation": "Diffusion models (DMs) have demonstrated an unparalleled ability to create diverse and high-fidelity images from text prompts. However, they are also well-known to vary substantially regarding both prompt adherence and quality. Negative prompting was introduced to improve prompt compliance by specifying what an image must not contain. Previous works have shown the existence of an ideal negative prompt that can maximize the odds of the positive prompt.", "method": "develop a sampling procedure, Adaptive Negative Sampling Without External Resources (ANSWER), that accounts for both positive and negative conditions from a single prompt. This leverages the internal understanding of negation by the diffusion model to increase the odds of generating images faithful to the prompt. ANSWER is a training-free technique, applicable to any model that supports CFG, and allows for negative grounding of image concepts without an explicit negative prompts, which are lossy and incomplete.", "result": "adding ANSWER to existing DMs outperforms the baselines on multiple benchmarks and is preferred by humans 2x more over the other methods.", "conclusion": "ANSWER outperforms the baselines on multiple benchmarks and is preferred by humans 2x more over the other methods."}}
{"id": "2508.03475", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.03475", "abs": "https://arxiv.org/abs/2508.03475", "authors": ["Pranshu Rastogi"], "title": "fact check AI at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-checked Claim Retrieval", "comment": "7 pages, 6 tables. Code available at\n  https://github.com/pranshurastogi29/SemEval-2025-ACL-Multi-and-Crosslingual-Retrieval-using-Bi-encoders", "summary": "SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim\nRetrieval is approached as a Learning-to-Rank task using a bi-encoder model\nfine-tuned from a pre-trained transformer optimized for sentence similarity.\nTraining used both the source languages and their English translations for\nmultilingual retrieval and only English translations for cross-lingual\nretrieval. Using lightweight models with fewer than 500M parameters and\ntraining on Kaggle T4 GPUs, the method achieved 92% Success@10 in multilingual\nand 80% Success@10 in 5th in crosslingual and 10th in multilingual tracks.", "AI": {"tldr": "\u4f7f\u7528 bi-encoder \u6a21\u578b\u8fdb\u884c\u591a\u8bed\u8a00\u548c\u8de8\u8bed\u8a00\u4e8b\u5b9e\u68c0\u67e5\u58f0\u660e\u68c0\u7d22\uff0c\u5728 Kaggle T4 GPU \u4e0a\u8bad\u7ec3\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u591a\u8bed\u8a00\u4e2d\u5b9e\u73b0\u4e86 92% \u7684 Success@10\uff0c\u5728\u8de8\u8bed\u8a00\u4e2d\u5b9e\u73b0\u4e86 80% \u7684 Success@10\u3002", "motivation": "SemEval-2025 Task 7\uff1a\u591a\u8bed\u8a00\u548c\u8de8\u8bed\u8a00\u4e8b\u5b9e\u68c0\u67e5\u58f0\u660e\u68c0\u7d22\u3002", "method": "\u4f7f\u7528 bi-encoder \u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4ece\u9488\u5bf9\u53e5\u5b50\u76f8\u4f3c\u6027\u4f18\u5316\u7684\u9884\u8bad\u7ec3 Transformer \u4e2d\u8fdb\u884c\u5fae\u8c03\uff0c\u5c06 SemEval-2025 Task 7\uff1a\u591a\u8bed\u8a00\u548c\u8de8\u8bed\u8a00\u4e8b\u5b9e\u68c0\u67e5\u58f0\u660e\u68c0\u7d22\u4f5c\u4e3a Learning-to-Rank \u4efb\u52a1\u5904\u7406\u3002", "result": "\u4f7f\u7528\u5c11\u4e8e 5 \u4ebf\u4e2a\u53c2\u6570\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u5e76\u5728 Kaggle T4 GPU \u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u4e2d\u5b9e\u73b0\u4e86 92% \u7684 Success@10\uff0c\u5728\u8de8\u8bed\u8a00\u4e2d\u6392\u540d\u7b2c 5\uff0c\u5728\u591a\u8bed\u8a00\u4e2d\u6392\u540d\u7b2c 10\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u4e2d\u5b9e\u73b0\u4e86 92% \u7684 Success@10\uff0c\u5728\u8de8\u8bed\u8a00\u4e2d\u5b9e\u73b0\u4e86 80% \u7684 Success@10\u3002"}}
{"id": "2508.03199", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03199", "abs": "https://arxiv.org/abs/2508.03199", "authors": ["Muhammed Saeed", "Shaina Raza", "Ashmal Vayani", "Muhammad Abdul-Mageed", "Ali Emami", "Shady Shehata"], "title": "Beyond Content: How Grammatical Gender Shapes Visual Representation in Text-to-Image Models", "comment": null, "summary": "Research on bias in Text-to-Image (T2I) models has primarily focused on\ndemographic representation and stereotypical attributes, overlooking a\nfundamental question: how does grammatical gender influence visual\nrepresentation across languages? We introduce a cross-linguistic benchmark\nexamining words where grammatical gender contradicts stereotypical gender\nassociations (e.g., ``une sentinelle'' - grammatically feminine in French but\nreferring to the stereotypically masculine concept ``guard''). Our dataset\nspans five gendered languages (French, Spanish, German, Italian, Russian) and\ntwo gender-neutral control languages (English, Chinese), comprising 800 unique\nprompts that generated 28,800 images across three state-of-the-art T2I models.\nOur analysis reveals that grammatical gender dramatically influences image\ngeneration: masculine grammatical markers increase male representation to 73\\%\non average (compared to 22\\% with gender-neutral English), while feminine\ngrammatical markers increase female representation to 38\\% (compared to 28\\% in\nEnglish). These effects vary systematically by language resource availability\nand model architecture, with high-resource languages showing stronger effects.\nOur findings establish that language structure itself, not just content, shapes\nAI-generated visual outputs, introducing a new dimension for understanding bias\nand fairness in multilingual, multimodal systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u8bed\u6cd5\u6027\u522b\u4f1a\u5f71\u54cd\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e2d\u56fe\u50cf\u7684\u751f\u6210\uff0c\u9633\u6027\u8bed\u6cd5\u6807\u8bb0\u4f1a\u589e\u52a0\u7537\u6027\u8868\u5f81\uff0c\u800c\u9634\u6027\u8bed\u6cd5\u6807\u8bb0\u4f1a\u589e\u52a0\u5973\u6027\u8868\u5f81\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf (T2I) \u6a21\u578b\u4e2d\u7684\u504f\u89c1\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u4eba\u53e3\u7edf\u8ba1\u5b66\u8868\u5f81\u548c\u523b\u677f\u5370\u8c61\u5c5e\u6027\u4e0a\uff0c\u5ffd\u7565\u4e86\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\uff1a\u8bed\u6cd5\u6027\u522b\u5982\u4f55\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u5f71\u54cd\u89c6\u89c9\u8868\u5f81\uff1f", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u8de8\u8bed\u8a00\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u68c0\u67e5\u4e86\u8bed\u6cd5\u6027\u522b\u4e0e\u523b\u677f\u6027\u522b\u5173\u8054\u76f8\u77db\u76fe\u7684\u8bcd\u8bed\u3002\u8be5\u6570\u636e\u96c6\u8de8\u8d8a\u4e94\u79cd\u6709\u6027\u522b\u7684\u8bed\u8a00\uff08\u6cd5\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u5fb7\u8bed\u3001\u610f\u5927\u5229\u8bed\u3001\u4fc4\u8bed\uff09\u548c\u4e24\u79cd\u6027\u522b\u4e2d\u7acb\u7684\u63a7\u5236\u8bed\u8a00\uff08\u82f1\u8bed\u3001\u4e2d\u6587\uff09\uff0c\u5305\u542b 800 \u4e2a\u72ec\u7279\u7684\u63d0\u793a\uff0c\u8fd9\u4e9b\u63d0\u793a\u5728\u4e09\u79cd\u6700\u5148\u8fdb\u7684 T2I \u6a21\u578b\u4e2d\u751f\u6210\u4e86 28,800 \u5f20\u56fe\u50cf\u3002", "result": "\u8bed\u6cd5\u6027\u522b\u4f1a\u663e\u7740\u5f71\u54cd\u56fe\u50cf\u751f\u6210\uff1a\u9633\u6027\u8bed\u6cd5\u6807\u8bb0\u5e73\u5747\u5c06\u7537\u6027\u8868\u5f81\u63d0\u9ad8\u5230 73%\uff08\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u6027\u522b\u4e2d\u7acb\u7684\u82f1\u8bed\u4e3a 22%\uff09\uff0c\u800c\u9634\u6027\u8bed\u6cd5\u6807\u8bb0\u5c06\u5973\u6027\u8868\u5f81\u63d0\u9ad8\u5230 38%\uff08\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u82f1\u8bed\u4e3a 28%\uff09\u3002\u8fd9\u4e9b\u5f71\u54cd\u56e0\u8bed\u8a00\u8d44\u6e90\u53ef\u7528\u6027\u548c\u6a21\u578b\u67b6\u6784\u800c\u5f02\uff0c\u9ad8\u8d44\u6e90\u8bed\u8a00\u663e\u793a\u51fa\u66f4\u5f3a\u7684\u5f71\u54cd\u3002", "conclusion": "\u8bed\u8a00\u7ed3\u6784\u672c\u8eab\uff08\u800c\u4e0d\u4ec5\u4ec5\u662f\u5185\u5bb9\uff09\u4f1a\u5f71\u54cd AI \u751f\u6210\u7684\u89c6\u89c9\u8f93\u51fa\uff0c\u4ece\u800c\u4e3a\u7406\u89e3\u591a\u8bed\u8a00\u3001\u591a\u6a21\u6001\u7cfb\u7edf\u4e2d\u7684\u504f\u89c1\u548c\u516c\u5e73\u6027\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u7ef4\u5ea6\u3002"}}
{"id": "2508.03018", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03018", "abs": "https://arxiv.org/abs/2508.03018", "authors": ["Yutong Wang", "Pengliang Ji", "Kaixin Li", "Baolong Bi", "Tao Feng", "Guillaume Sartoretti"], "title": "Beyond Policy Optimization: A Data Curation Flywheel for Sparse-Reward Long-Horizon Planning", "comment": null, "summary": "Large Language Reasoning Models have demonstrated remarkable success on\nstatic tasks, yet their application to multi-round agentic planning in\ninteractive environments faces two fundamental challenges. First, the\nintractable credit assignment problem renders conventional reinforcement\nlearning ineffective in sparse-reward settings. Second, the computational\noverhead of verbose, step-by-step reasoning histories is prohibitive. To\naddress these challenges, we propose BPO, a three-stage framework\n(bootstrapping, extrapolation, and refinement) that establishes a\nself-improving data flywheel to develop robust reasoning models for\nlong-horizon, sparse-reward environments. Our framework first bootstraps\nefficient reasoning using the proposed planning quaternions with long-short\nchain-of-thought fusion. It then extrapolates to out-of-distribution tasks\nthrough complexity-stratified curriculum learning. Finally, the model\niteratively refines itself by learning exclusively on experiences selected via\nreward-gated rejection sampling. Experiments on ALFWorld, ScienceWorld, and\nWebShop demonstrate that our approach achieves state-of-the-art with\nsignificant token efficiency, providing a new recipe for reasoning models in\nagentic planning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u6211\u6539\u8fdb\u7684\u6570\u636e\u98de\u8f6e\u6765\u5f00\u53d1\u9c81\u68d2\u7684\u63a8\u7406\u6a21\u578b\uff0c\u7528\u4e8e\u957f\u65f6\u7a0b\u3001\u7a00\u758f\u5956\u52b1\u73af\u5883\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u63a8\u7406\u6a21\u578b\u5728\u9759\u6001\u4efb\u52a1\u4e0a\u5df2\u7ecf \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u0443\u0435\u0442 \u51fa\u8272\u7684\u6210\u529f\uff0c\u4f46\u5b83\u4eec\u5728\u4ea4\u4e92\u73af\u5883\u4e2d\u5bf9\u591a\u8f6eAgent\u89c4\u5212\u7684\u5e94\u7528\u9762\u4e34\u7740\u4e24\u4e2a\u6839\u672c\u6027\u7684\u6311\u6218\u3002\u9996\u5148\uff0c\u96be\u5904\u7406\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\u4f7f\u5f97\u4f20\u7edf\u7684\u5f3a\u5316\u5b66\u4e60\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u65e0\u6548\u3002\u5176\u6b21\uff0c\u5197\u957f\u7684\u3001\u9010\u6b65\u7684\u63a8\u7406\u5386\u53f2\u7684\u8ba1\u7b97\u5f00\u9500\u8fc7\u9ad8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u6846\u67b6\uff08\u5f15\u5bfc\u3001\u5916\u63a8\u548c\u7ec6\u5316\uff09\uff0c\u8be5\u6846\u67b6\u5efa\u7acb\u4e86\u4e00\u4e2a\u81ea\u6211\u6539\u8fdb\u7684\u6570\u636e\u98de\u8f6e\uff0c\u4ee5\u5f00\u53d1\u7528\u4e8e\u957f\u65f6\u7a0b\u3001\u7a00\u758f\u5956\u52b1\u73af\u5883\u7684\u9c81\u68d2\u63a8\u7406\u6a21\u578b\u3002\u8be5\u6846\u67b6\u9996\u5148\u4f7f\u7528\u63d0\u51fa\u7684\u5e26\u6709\u957f\u77ed\u671f\u601d\u7ef4\u94fe\u878d\u5408\u7684\u89c4\u5212\u56db\u5143\u6570\u6765\u5f15\u5bfc\u6709\u6548\u7684\u63a8\u7406\u3002\u7136\u540e\uff0c\u901a\u8fc7\u590d\u6742\u6027\u5206\u5c42\u8bfe\u7a0b\u5b66\u4e60\u5916\u63a8\u5230\u5206\u5e03\u5916\u4efb\u52a1\u3002\u6700\u540e\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u4e13\u95e8\u5b66\u4e60\u901a\u8fc7\u5956\u52b1\u95e8\u63a7\u62d2\u7edd\u62bd\u6837\u9009\u62e9\u7684\u7ecf\u9a8c\u6765\u8fed\u4ee3\u5730\u6539\u8fdb\u81ea\u8eab\u3002", "result": "\u8be5\u65b9\u6cd5\u5728ALFWorld\u3001ScienceWorld\u548cWebShop\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5177\u6709\u663e\u8457\u7684token\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728ALFWorld\u3001ScienceWorld\u548cWebShop\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5177\u6709\u663e\u8457\u7684token\u6548\u7387\uff0c\u4e3aAgent\u89c4\u5212\u4e2d\u7684\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.02978", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02978", "abs": "https://arxiv.org/abs/2508.02978", "authors": ["Yusaku Takama", "Ning Ding", "Tatsuya Yokota", "Toru Tamaki"], "title": "Separating Shared and Domain-Specific LoRAs for Multi-Domain Learning", "comment": "9 pages", "summary": "Existing architectures of multi-domain learning have two types of adapters:\nshared LoRA for all domains and domain-specific LoRA for each particular\ndomain. However, it remains unclear whether this structure effectively captures\ndomain-specific information. In this paper, we propose a method that ensures\nthat shared and domain-specific LoRAs exist in different subspaces;\nspecifically, the column and left null subspaces of the pre-trained weights. We\napply the proposed method to action recognition with three datasets (UCF101,\nKinetics400, and HMDB51) and demonstrate its effectiveness in some cases along\nwith the analysis of the dimensions of LoRA weights.", "AI": {"tldr": "proposes a method that ensures that shared and domain-specific LoRAs exist in different subspaces", "motivation": "it remains unclear whether this structure effectively captures domain-specific information", "method": "propose a method that ensures that shared and domain-specific LoRAs exist in different subspaces; specifically, the column and left null subspaces of the pre-trained weights", "result": "apply the proposed method to action recognition with three datasets (UCF101, Kinetics400, and HMDB51) and demonstrate its effectiveness in some cases", "conclusion": "demonstrates its effectiveness in some cases along with the analysis of the dimensions of LoRA weights"}}
{"id": "2508.03644", "categories": ["cs.CL", "cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.03644", "abs": "https://arxiv.org/abs/2508.03644", "authors": ["Wenxuan Shen", "Mingjia Wang", "Yaochen Wang", "Dongping Chen", "Junjie Yang", "Yao Wan", "Weiwei Lin"], "title": "Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?", "comment": "In submission. Project website: https://double-bench.github.io/", "summary": "Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language\nModels (MLLMs) show great promise for complex document understanding, yet their\ndevelopment is critically hampered by inadequate evaluation. Current benchmarks\noften focus on specific part of document RAG system and use synthetic data with\nincomplete ground truth and evidence labels, therefore failing to reflect\nreal-world bottlenecks and challenges. To overcome these limitations, we\nintroduce Double-Bench: a new large-scale, multilingual, and multimodal\nevaluation system that is able to produce fine-grained assessment to each\ncomponent within document RAG systems. It comprises 3,276 documents (72,880\npages) and 5,168 single- and multi-hop queries across 6 languages and 4\ndocument types with streamlined dynamic update support for potential data\ncontamination issues. Queries are grounded in exhaustively scanned evidence\npages and verified by human experts to ensure maximum quality and completeness.\nOur comprehensive experiments across 9 state-of-the-art embedding models, 4\nMLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text\nand visual embedding models is narrowing, highlighting the need in building\nstronger document retrieval models. Our findings also reveal the\nover-confidence dilemma within current document RAG frameworks that tend to\nprovide answer even without evidence support. We hope our fully open-source\nDouble-Bench provide a rigorous foundation for future research in advanced\ndocument RAG systems. We plan to retrieve timely corpus and release new\nbenchmarks on an annual basis.", "AI": {"tldr": "\u63d0\u51fa\u4e86Double-Bench\uff0c\u4e00\u4e2a\u65b0\u7684\u5927\u578b\u3001\u591a\u8bed\u8a00\u548c\u591a\u6a21\u6001\u8bc4\u4f30\u7cfb\u7edf\uff0c\u7528\u4e8e\u8bc4\u4f30\u6587\u6863RAG\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u901a\u5e38\u4fa7\u91cd\u4e8e\u6587\u6863RAG\u7cfb\u7edf\u7684\u7279\u5b9a\u90e8\u5206\uff0c\u5e76\u4f7f\u7528\u5177\u6709\u4e0d\u5b8c\u6574\u7684ground truth\u548c\u8bc1\u636e\u6807\u7b7e\u7684\u5408\u6210\u6570\u636e\uff0c\u56e0\u6b64\u65e0\u6cd5\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u7684\u74f6\u9888\u548c\u6311\u6218\u3002", "method": "\u5f15\u5165Double-Bench\uff1a\u4e00\u79cd\u65b0\u7684\u5927\u578b\u3001\u591a\u8bed\u8a00\u548c\u591a\u6a21\u6001\u8bc4\u4f30\u7cfb\u7edf\uff0c\u80fd\u591f\u5bf9\u6587\u6863RAG\u7cfb\u7edf\u4e2d\u6bcf\u4e2a\u7ec4\u4ef6\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u3002\u5b83\u5305\u542b3,276\u4efd\u6587\u6863\uff0872,880\u9875\uff09\u548c5,168\u4e2a\u8de86\u79cd\u8bed\u8a00\u548c4\u79cd\u6587\u6863\u7c7b\u578b\u7684\u5355\u8df3\u548c\u591a\u8df3\u67e5\u8be2\uff0c\u5e76\u4e3a\u6f5c\u5728\u7684\u6570\u636e\u6c61\u67d3\u95ee\u9898\u63d0\u4f9b\u7b80\u5316\u7684\u52a8\u6001\u66f4\u65b0\u652f\u6301\u3002\u67e5\u8be2\u4ee5\u8be6\u5c3d\u626b\u63cf\u7684\u8bc1\u636e\u9875\u9762\u4e3a\u57fa\u7840\uff0c\u5e76\u7531\u4eba\u5de5\u4e13\u5bb6\u9a8c\u8bc1\uff0c\u4ee5\u786e\u4fdd\u6700\u9ad8\u7684\u8d28\u91cf\u548c\u5b8c\u6574\u6027\u3002", "result": "\u8de89\u4e2a\u6700\u5148\u8fdb\u7684\u5d4c\u5165\u6a21\u578b\u30014\u4e2aMLLM\u548c4\u4e2a\u7aef\u5230\u7aef\u6587\u6863RAG\u6846\u67b6\u7684\u5168\u9762\u5b9e\u9a8c\u8868\u660e\uff0c\u6587\u672c\u548c\u89c6\u89c9\u5d4c\u5165\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\u6b63\u5728\u7f29\u5c0f\uff0c\u7a81\u51fa\u4e86\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u6587\u6863\u68c0\u7d22\u6a21\u578b\u7684\u9700\u6c42\u3002", "conclusion": "Double-Bench\u7684\u5168\u9762\u5b9e\u9a8c\u63ed\u793a\u4e86\u6587\u672c\u548c\u89c6\u89c9\u5d4c\u5165\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\u6b63\u5728\u7f29\u5c0f\uff0c\u7a81\u51fa\u4e86\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u6587\u6863\u68c0\u7d22\u6a21\u578b\u7684\u9700\u6c42\u3002\u7814\u7a76\u7ed3\u679c\u8fd8\u63ed\u793a\u4e86\u5f53\u524d\u6587\u6863RAG\u6846\u67b6\u4e2d\u7684\u8fc7\u5ea6\u81ea\u4fe1\u56f0\u5883\uff0c\u5373\u4f7f\u6ca1\u6709\u8bc1\u636e\u652f\u6301\uff0c\u4e5f\u503e\u5411\u4e8e\u63d0\u4f9b\u7b54\u6848\u3002\u5e0c\u671b\u5b8c\u5168\u5f00\u6e90\u7684Double-Bench\u4e3a\u672a\u6765\u9ad8\u7ea7\u6587\u6863RAG\u7cfb\u7edf\u7684\u7814\u7a76\u63d0\u4f9b\u4e25\u8c28\u7684\u57fa\u7840\u3002\u8ba1\u5212\u68c0\u7d22\u53ca\u65f6\u7684\u8bed\u6599\u5e93\uff0c\u5e76\u6bcf\u5e74\u53d1\u5e03\u65b0\u7684\u57fa\u51c6\u3002"}}
{"id": "2508.03204", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03204", "abs": "https://arxiv.org/abs/2508.03204", "authors": ["Abhirup Sinha", "Pritilata Saha", "Tithi Saha"], "title": "Current State in Privacy-Preserving Text Preprocessing for Domain-Agnostic NLP", "comment": "To be published in the Proceedings of Die Studierendenkonferenz\n  Informatik (SKILL) 2024", "summary": "Privacy is a fundamental human right. Data privacy is protected by different\nregulations, such as GDPR. However, modern large language models require a huge\namount of data to learn linguistic variations, and the data often contains\nprivate information. Research has shown that it is possible to extract private\ninformation from such language models. Thus, anonymizing such private and\nsensitive information is of utmost importance. While complete anonymization may\nnot be possible, a number of different pre-processing approaches exist for\nmasking or pseudonymizing private information in textual data. This report\nfocuses on a few of such approaches for domain-agnostic NLP tasks.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5305\u542b\u79c1\u4eba\u4fe1\u606f\uff0c\u53ef\u4ee5\u63d0\u53d6\u3002\u672c\u62a5\u544a\u91cd\u70b9\u4ecb\u7ecd\u4e86\u7528\u4e8e\u9886\u57df\u65e0\u5173 NLP \u4efb\u52a1\u7684\u51e0\u79cd\u533f\u540d\u5316\u65b9\u6cd5\u3002", "motivation": "\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9700\u8981\u5927\u91cf\u6570\u636e\u6765\u5b66\u4e60\u8bed\u8a00\u53d8\u5f02\uff0c\u800c\u8fd9\u4e9b\u6570\u636e\u901a\u5e38\u5305\u542b\u79c1\u4eba\u4fe1\u606f\u3002\u7814\u7a76\u8868\u660e\uff0c\u53ef\u4ee5\u4ece\u6b64\u7c7b\u8bed\u8a00\u6a21\u578b\u4e2d\u63d0\u53d6\u79c1\u4eba\u4fe1\u606f\u3002\u56e0\u6b64\uff0c\u533f\u540d\u5316\u6b64\u7c7b\u79c1\u4eba\u548c\u654f\u611f\u4fe1\u606f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c4f\u853d\u6216\u5047\u540d\u5316\u6587\u672c\u6570\u636e\u4e2d\u7684\u79c1\u6709\u4fe1\u606f\u3002", "result": "\u7814\u7a76\u8868\u660e\u53ef\u4ee5\u4ece\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u63d0\u53d6\u79c1\u4eba\u4fe1\u606f\u3002", "conclusion": "\u672c\u62a5\u544a\u91cd\u70b9\u4ecb\u7ecd\u4e86\u7528\u4e8e\u9886\u57df\u65e0\u5173 NLP \u4efb\u52a1\u7684\u51e0\u79cd\u6b64\u7c7b\u65b9\u6cd5\u3002"}}
{"id": "2508.02840", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.02840", "abs": "https://arxiv.org/abs/2508.02840", "authors": ["Chaoyang Gao", "Xiang Chen", "Jiyu Wang", "Jibin Wang", "Guang Yang"], "title": "Resource-Efficient Automatic Software Vulnerability Assessment via Knowledge Distillation and Particle Swarm Optimization", "comment": "Accepted by Engineering Applications of Artificial Intelligence", "summary": "The increasing complexity of software systems has led to a surge in\ncybersecurity vulnerabilities, necessitating efficient and scalable solutions\nfor vulnerability assessment. However, the deployment of large pre-trained\nmodels in real-world scenarios is hindered by their substantial computational\nand storage demands. To address this challenge, we propose a novel\nresource-efficient framework that integrates knowledge distillation and\nparticle swarm optimization to enable automated vulnerability assessment. Our\nframework employs a two-stage approach: First, particle swarm optimization is\nutilized to optimize the architecture of a compact student model, balancing\ncomputational efficiency and model capacity. Second, knowledge distillation is\napplied to transfer critical vulnerability assessment knowledge from a large\nteacher model to the optimized student model. This process significantly\nreduces the model size while maintaining high performance. Experimental results\non an enhanced MegaVul dataset, comprising 12,071 CVSS (Common Vulnerability\nScoring System) v3 annotated vulnerabilities, demonstrate the effectiveness of\nour approach. Our approach achieves a 99.4% reduction in model size while\nretaining 89.3% of the original model's accuracy. Furthermore, it outperforms\nstate-of-the-art baselines by 1.7% in accuracy with 60% fewer parameters. The\nframework also reduces training time by 72.1% and architecture search time by\n34.88% compared to traditional genetic algorithms.", "AI": {"tldr": "A resource-efficient framework that integrates knowledge distillation and particle swarm optimization to enable automated vulnerability assessment.", "motivation": "The increasing complexity of software systems has led to a surge in cybersecurity vulnerabilities, necessitating efficient and scalable solutions for vulnerability assessment. However, the deployment of large pre-trained models in real-world scenarios is hindered by their substantial computational and storage demands.", "method": "integrates knowledge distillation and particle swarm optimization to enable automated vulnerability assessment. Our framework employs a two-stage approach: First, particle swarm optimization is utilized to optimize the architecture of a compact student model, balancing computational efficiency and model capacity. Second, knowledge distillation is applied to transfer critical vulnerability assessment knowledge from a large teacher model to the optimized student model.", "result": "achieves a 99.4% reduction in model size while retaining 89.3% of the original model's accuracy. Furthermore, it outperforms state-of-the-art baselines by 1.7% in accuracy with 60% fewer parameters. The framework also reduces training time by 72.1% and architecture search time by 34.88% compared to traditional genetic algorithms.", "conclusion": "The proposed framework achieves a 99.4% reduction in model size while retaining 89.3% of the original model's accuracy. Furthermore, it outperforms state-of-the-art baselines by 1.7% in accuracy with 60% fewer parameters. The framework also reduces training time by 72.1% and architecture search time by 34.88% compared to traditional genetic algorithms."}}
{"id": "2508.03030", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03030", "abs": "https://arxiv.org/abs/2508.03030", "authors": ["Siyuan Li", "Yifan Yu", "Yanchen Deng", "Zhihao Zhang", "Mengjing Chen", "Fangzhou Zhu", "Tao Zhong", "Jianye Hao", "Peng Liu", "Bo An"], "title": "Collab-Solver: Collaborative Solving Policy Learning for Mixed-Integer Linear Programming", "comment": null, "summary": "Mixed-integer linear programming (MILP) has been a fundamental problem in\ncombinatorial optimization. Previous works have designed a plethora of\nhard-coded heuristics to accomplish challenging MILP solving with domain\nknowledge. Driven by the high capability of neural networks, recent research is\ndevoted to replacing manually designed heuristics with learned policies.\nAlthough learning-based MILP methods have shown great promise, existing\nworksindependentlytreatthepolicylearningineachmoduleofMILPsolvers without\nconsidering their interdependence, severely hurting the solving speed and\nquality. To address this issue, we propose a novel multi-agent-based policy\nlearning framework for MILP (Collab-Solver), which can collaboratively optimize\nthe policies for multiple modules. Specifically, we formulate the collaboration\nof cut selection and branching in MILP solving as a Stackelberg game. Under\nthis formulation, we develop a two-phase learning paradigm to stabilize the\ncollaborative policy learning, where the first phase achieves the\ndata-communicated policy pretraining and the second phase further orchestrates\nthe policy learning for various modules. The jointly learned policy\nsignificantly improves the solving performance on both synthetic and\nlarge-scale real-world MILP datasets. Moreover, the policies learned by\nCollab-Solver have also demonstrated excellent generalization abilities across\ndifferent instance sets.", "AI": {"tldr": "This paper introduces Collab-Solver, a multi-agent-based policy learning framework for MILP that collaboratively optimizes policies for multiple modules, improving solving performance and generalization abilities.", "motivation": "Existing works independently treat the policy learning in each module of MILP solvers without considering their interdependence, severely hurting the solving speed and quality.", "method": "We propose a novel multi-agent-based policy learning framework for MILP (Collab-Solver), which can collaboratively optimize the policies for multiple modules. Specifically, we formulate the collaboration of cut selection and branching in MILP solving as a Stackelberg game. Under this formulation, we develop a two-phase learning paradigm to stabilize the collaborative policy learning, where the first phase achieves the data-communicated policy pretraining and the second phase further orchestrates the policy learning for various modules.", "result": "The jointly learned policy significantly improves the solving performance on both synthetic and large-scale real-world MILP datasets. Moreover, the policies learned by Collab-Solver have also demonstrated excellent generalization abilities across different instance sets.", "conclusion": "The jointly learned policy significantly improves the solving performance on both synthetic and large-scale real-world MILP datasets. Moreover, the policies learned by Collab-Solver have also demonstrated excellent generalization abilities across different instance sets."}}
{"id": "2508.02981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02981", "abs": "https://arxiv.org/abs/2508.02981", "authors": ["Takuya Sugimoto", "Ning Ding", "Toru Tamaki"], "title": "MoExDA: Domain Adaptation for Edge-based Action Recognition", "comment": "7 pages", "summary": "Modern action recognition models suffer from static bias, leading to reduced\ngeneralization performance. In this paper, we propose MoExDA, a lightweight\ndomain adaptation between RGB and edge information using edge frames in\naddition to RGB frames to counter the static bias issue. Experiments\ndemonstrate that the proposed method effectively suppresses static bias with a\nlower computational cost, allowing for more robust action recognition than\nprevious approaches.", "AI": {"tldr": "\u63d0\u51fa MoExDA \u6765\u89e3\u51b3\u52a8\u4f5c\u8bc6\u522b\u6a21\u578b\u4e2d\u7684\u9759\u6001\u504f\u5dee\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u66f4\u5f3a\u5927\u7684\u52a8\u4f5c\u8bc6\u522b\u3002", "motivation": "\u73b0\u4ee3\u52a8\u4f5c\u8bc6\u522b\u6a21\u578b\u53d7\u5230\u9759\u6001\u504f\u5dee\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u6cdb\u5316\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd MoExDA\uff0c\u8fd9\u662f\u4e00\u79cd\u4f7f\u7528\u8fb9\u7f18\u5e27\u4ee5\u53ca RGB \u5e27\u5728 RGB \u548c\u8fb9\u7f18\u4fe1\u606f\u4e4b\u95f4\u8fdb\u884c\u8f7b\u91cf\u7ea7\u57df\u9002\u5e94\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u9759\u6001\u504f\u5dee\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4ee5\u8f83\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u6709\u6548\u5730\u6291\u5236\u4e86\u9759\u6001\u504f\u5dee\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u6bd4\u4ee5\u524d\u7684\u65b9\u6cd5\u66f4\u5f3a\u5927\u7684\u52a8\u4f5c\u8bc6\u522b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ee5\u8f83\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u6709\u6548\u5730\u6291\u5236\u4e86\u9759\u6001\u504f\u5dee\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u6bd4\u4ee5\u524d\u7684\u65b9\u6cd5\u66f4\u5f3a\u5927\u7684\u52a8\u4f5c\u8bc6\u522b\u3002"}}
{"id": "2508.03211", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03211", "abs": "https://arxiv.org/abs/2508.03211", "authors": ["Pablo J. Diego-Sim\u00f3n", "Emmanuel Chemla", "Jean-R\u00e9mi King", "Yair Lakretz"], "title": "Probing Syntax in Large Language Models: Successes and Remaining Challenges", "comment": null, "summary": "The syntactic structures of sentences can be readily read-out from the\nactivations of large language models (LLMs). However, the ``structural probes''\nthat have been developed to reveal this phenomenon are typically evaluated on\nan indiscriminate set of sentences. Consequently, it remains unclear whether\nstructural and/or statistical factors systematically affect these syntactic\nrepresentations. To address this issue, we conduct an in-depth analysis of\nstructural probes on three controlled benchmarks. Our results are three-fold.\nFirst, structural probes are biased by a superficial property: the closer two\nwords are in a sentence, the more likely structural probes will consider them\nas syntactically linked. Second, structural probes are challenged by linguistic\nproperties: they poorly represent deep syntactic structures, and get interfered\nby interacting nouns or ungrammatical verb forms. Third, structural probes do\nnot appear to be affected by the predictability of individual words. Overall,\nthis work sheds light on the current challenges faced by structural probes.\nProviding a benchmark made of controlled stimuli to better evaluate their\nperformance.", "AI": {"tldr": "structural probes in LLMs are biased and struggle with deep syntax, but are unaffected by word predictability.", "motivation": "it remains unclear whether structural and/or statistical factors systematically affect the syntactic representations in LLMs.", "method": "in-depth analysis of structural probes on three controlled benchmarks", "result": "structural probes are biased by word proximity, challenged by linguistic properties, and not affected by word predictability.", "conclusion": "structural probes face challenges in representing deep syntactic structures and are biased by superficial properties like word proximity, but are not affected by word predictability."}}
{"id": "2508.02860", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02860", "abs": "https://arxiv.org/abs/2508.02860", "authors": ["Enrique Luna Villag\u00f3mez", "Vladimir Mahalec"], "title": "Comparative Evaluation of Kolmogorov-Arnold Autoencoders and Orthogonal Autoencoders for Fault Detection with Varying Training Set Sizes", "comment": null, "summary": "Kolmogorov-Arnold Networks (KANs) have recently emerged as a flexible and\nparameter-efficient alternative to conventional neural networks. Unlike\nstandard architectures that use fixed node-based activations, KANs place\nlearnable functions on edges, parameterized by different function families.\nWhile they have shown promise in supervised settings, their utility in\nunsupervised fault detection remains largely unexplored. This study presents a\ncomparative evaluation of KAN-based autoencoders (KAN-AEs) for unsupervised\nfault detection in chemical processes. We investigate four KAN-AE variants,\neach based on a different KAN implementation (EfficientKAN, FastKAN,\nFourierKAN, and WavKAN), and benchmark them against an Orthogonal Autoencoder\n(OAE) on the Tennessee Eastman Process. Models are trained on normal operating\ndata across 13 training set sizes and evaluated on 21 fault types, using Fault\nDetection Rate (FDR) as the performance metric. WavKAN-AE achieves the highest\noverall FDR ($\\geq$92\\%) using just 4,000 training samples and remains the top\nperformer, even as other variants are trained on larger datasets.\nEfficientKAN-AE reaches $\\geq$90\\% FDR with only 500 samples, demonstrating\nrobustness in low-data settings. FastKAN-AE becomes competitive at larger\nscales ($\\geq$50,000 samples), while FourierKAN-AE consistently underperforms.\nThe OAE baseline improves gradually but requires substantially more data to\nmatch top KAN-AE performance. These results highlight the ability of KAN-AEs to\ncombine data efficiency with strong fault detection performance. Their use of\nstructured basis functions suggests potential for improved model transparency,\nmaking them promising candidates for deployment in data-constrained industrial\nsettings.", "AI": {"tldr": "KAN-AEs are data-efficient and effective for unsupervised fault detection in chemical processes, with WavKAN and EfficientKAN variants performing particularly well.", "motivation": "Explore the utility of KANs in unsupervised fault detection, an area largely unexplored despite their promise in supervised settings.", "method": "Comparative evaluation of four KAN-AE variants (EfficientKAN, FastKAN, FourierKAN, and WavKAN) against an Orthogonal Autoencoder (OAE) on the Tennessee Eastman Process.", "result": "WavKAN-AE achieves the highest overall FDR (\u226592%) with 4,000 training samples. EfficientKAN-AE reaches \u226590% FDR with 500 samples. FastKAN-AE becomes competitive at larger scales (\u226550,000 samples), while FourierKAN-AE underperforms. OAE requires substantially more data to match top KAN-AE performance.", "conclusion": "KAN-AEs combine data efficiency with strong fault detection performance and potential for improved model transparency, making them promising for data-constrained industrial settings."}}
{"id": "2508.03031", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03031", "abs": "https://arxiv.org/abs/2508.03031", "authors": ["Ziyang Ma", "Baojian Zhou", "Deqing Yang", "Yanghua Xiao"], "title": "From Text to Trajectories: GPT-2 as an ODE Solver via In-Context", "comment": null, "summary": "In-Context Learning (ICL) has emerged as a new paradigm in large language\nmodels (LLMs), enabling them to perform novel tasks by conditioning on a few\nexamples embedded in the prompt. Yet, the highly nonlinear behavior of ICL for\nNLP tasks remains poorly understood. To shed light on its underlying\nmechanisms, this paper investigates whether LLMs can solve ordinary\ndifferential equations (ODEs) under the ICL setting. We formulate standard ODE\nproblems and their solutions as sequential prompts and evaluate GPT-2 models on\nthese tasks. Experiments on two types of ODEs show that GPT-2 can effectively\nlearn a meta-ODE algorithm, with convergence behavior comparable to, or better\nthan, the Euler method, and achieve exponential accuracy gains with increasing\nnumbers of demonstrations. Moreover, the model generalizes to\nout-of-distribution (OOD) problems, demonstrating robust extrapolation\ncapabilities. These empirical findings provide new insights into the mechanisms\nof ICL in NLP and its potential for solving nonlinear numerical problems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u4e2d\u89e3\u51b3\u5e38\u5fae\u5206\u65b9\u7a0b\uff08ODE\uff09\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0cGPT-2\u53ef\u4ee5\u6709\u6548\u5730\u5b66\u4e60\u5143ODE\u7b97\u6cd5\uff0c\u5e76\u4e14\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u63ed\u793aICL\u7684\u6f5c\u5728\u673a\u5236\uff0c\u672c\u6587\u7814\u7a76\u4e86LLM\u662f\u5426\u53ef\u4ee5\u5728ICL\u8bbe\u7f6e\u4e0b\u6c42\u89e3\u5e38\u5fae\u5206\u65b9\u7a0b\uff08ODE\uff09\u3002", "method": "\u5c06\u6807\u51c6ODE\u95ee\u9898\u53ca\u5176\u89e3\u51b3\u65b9\u6848\u5236\u5b9a\u4e3a\u987a\u5e8f\u63d0\u793a\uff0c\u5e76\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u8bc4\u4f30GPT-2\u6a21\u578b\u3002", "result": "GPT-2\u53ef\u4ee5\u6709\u6548\u5730\u5b66\u4e60\u5143ODE\u7b97\u6cd5\uff0c\u5176\u6536\u655b\u884c\u4e3a\u4e0e\u6b27\u62c9\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u5e76\u4e14\u968f\u7740\u6f14\u793a\u6570\u91cf\u7684\u589e\u52a0\uff0c\u5b9e\u73b0\u4e86\u6307\u6570\u7ea7\u7684\u7cbe\u5ea6\u63d0\u5347\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u63a8\u5e7f\u5230\u5206\u5e03\u5916\uff08OOD\uff09\u95ee\u9898\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u5916\u63a8\u80fd\u529b\u3002", "conclusion": "GPT-2\u53ef\u4ee5\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u6709\u6548\u5730\u5b66\u4e60\u5143ODE\u7b97\u6cd5\uff0c\u5176\u6536\u655b\u884c\u4e3a\u4e0e\u6b27\u62c9\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u5e76\u4e14\u968f\u7740\u6f14\u793a\u6570\u91cf\u7684\u589e\u52a0\uff0c\u5b9e\u73b0\u4e86\u6307\u6570\u7ea7\u7684\u7cbe\u5ea6\u63d0\u5347\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u63a8\u5e7f\u5230\u5206\u5e03\u5916\u95ee\u9898\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u5916\u63a8\u80fd\u529b\u3002"}}
{"id": "2508.02987", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.02987", "abs": "https://arxiv.org/abs/2508.02987", "authors": ["Zachary Yahn", "Selim Furkan Tekin", "Fatih Ilhan", "Sihao Hu", "Tiansheng Huang", "Yichang Xu", "Margaret Loper", "Ling Liu"], "title": "Adversarial Attention Perturbations for Large Object Detection Transformers", "comment": "ICCV 2025", "summary": "Adversarial perturbations are useful tools for exposing vulnerabilities in\nneural networks. Existing adversarial perturbation methods for object detection\nare either limited to attacking CNN-based detectors or weak against\ntransformer-based detectors. This paper presents an Attention-Focused Offensive\nGradient (AFOG) attack against object detection transformers. By design, AFOG\nis neural-architecture agnostic and effective for attacking both large\ntransformer-based object detectors and conventional CNN-based detectors with a\nunified adversarial attention framework. This paper makes three original\ncontributions. First, AFOG utilizes a learnable attention mechanism that\nfocuses perturbations on vulnerable image regions in multi-box detection tasks,\nincreasing performance over non-attention baselines by up to 30.6%. Second,\nAFOG's attack loss is formulated by integrating two types of feature loss\nthrough learnable attention updates with iterative injection of adversarial\nperturbations. Finally, AFOG is an efficient and stealthy adversarial\nperturbation method. It probes the weak spots of detection transformers by\nadding strategically generated and visually imperceptible perturbations which\ncan cause well-trained object detection models to fail. Extensive experiments\nconducted with twelve large detection transformers on COCO demonstrate the\nefficacy of AFOG. Our empirical results also show that AFOG outperforms\nexisting attacks on transformer-based and CNN-based object detectors by up to\n83% with superior speed and imperceptibility. Code is available at\nhttps://github.com/zacharyyahn/AFOG.", "AI": {"tldr": "AFOG\u662f\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u5730\u653b\u51fb\u57fa\u4e8etransformer\u548cCNN\u7684\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u901f\u5ea6\u66f4\u5feb\uff0c\u9690\u853d\u6027\u66f4\u597d\u3002", "motivation": "\u73b0\u6709\u7684\u76ee\u6807\u68c0\u6d4b\u5bf9\u6297\u6270\u52a8\u65b9\u6cd5\u8981\u4e48\u4ec5\u9650\u4e8e\u653b\u51fb\u57fa\u4e8eCNN\u7684\u68c0\u6d4b\u5668\uff0c\u8981\u4e48\u5bf9\u57fa\u4e8etransformer\u7684\u68c0\u6d4b\u5668\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5173\u6ce8\u6ce8\u610f\u529b\u7684\u653b\u51fb\u68af\u5ea6\uff08AFOG\uff09\u653b\u51fb\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u6ce8\u610f\u529b\u673a\u5236\u5c06\u6270\u52a8\u96c6\u4e2d\u5728\u6613\u53d7\u653b\u51fb\u7684\u56fe\u50cf\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u6ce8\u5165\u5bf9\u6297\u6027\u6270\u52a8\u6765\u6574\u5408\u4e24\u79cd\u7c7b\u578b\u7684\u7279\u5f81\u635f\u5931\u3002", "result": "AFOG\u5728\u591a\u6846\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u6bd4\u975e\u6ce8\u610f\u529b\u57fa\u7ebf\u63d0\u9ad8\u4e8630.6%\u3002\u5728COCO\u4e0a\u5bf912\u4e2a\u5927\u578b\u68c0\u6d4btransformer\u8fdb\u884c\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86AFOG\u7684\u6709\u6548\u6027\u3002", "conclusion": "AFOG\u5728\u653b\u51fb\u57fa\u4e8etransformer\u548cCNN\u7684\u76ee\u6807\u68c0\u6d4b\u5668\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u653b\u51fb\uff0c\u9ad8\u8fbe83%\uff0c\u5e76\u4e14\u5177\u6709\u66f4\u5feb\u7684\u901f\u5ea6\u548c\u66f4\u597d\u7684\u9690\u853d\u6027\u3002"}}
{"id": "2508.03240", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03240", "abs": "https://arxiv.org/abs/2508.03240", "authors": ["Mutaz Ayesh", "Nicol\u00e1s Guti\u00e9rrez-Rol\u00f3n", "Fernando Alva-Manchego"], "title": "CardiffNLP at CLEARS-2025: Prompting Large Language Models for Plain Language and Easy-to-Read Text Rewriting", "comment": null, "summary": "This paper details the CardiffNLP team's contribution to the CLEARS shared\ntask on Spanish text adaptation, hosted by IberLEF 2025. The shared task\ncontained two subtasks and the team submitted to both. Our team took an\nLLM-prompting approach with different prompt variations. While we initially\nexperimented with LLaMA-3.2, we adopted Gemma-3 for our final submission, and\nlanded third place in Subtask 1 and second place in Subtask 2. We detail our\nnumerous prompt variations, examples, and experimental results.", "AI": {"tldr": "CardiffNLP \u56e2\u961f\u4f7f\u7528 LLM \u63d0\u793a\u65b9\u6cd5\u53c2\u52a0\u4e86 IberLEF 2025 CLEARS \u897f\u73ed\u7259\u8bed\u6587\u672c\u6539\u7f16\u5171\u4eab\u4efb\u52a1\uff0c\u5e76\u5728\u4e24\u4e2a\u5b50\u4efb\u52a1\u4e2d\u5206\u522b\u83b7\u5f97\u7b2c\u4e09\u540d\u548c\u7b2c\u4e8c\u540d\u3002", "motivation": "\u672c\u6587\u4ecb\u7ecd\u4e86 CardiffNLP \u56e2\u961f\u5bf9 IberLEF 2025 CLEARS \u897f\u73ed\u7259\u8bed\u6587\u672c\u6539\u7f16\u5171\u4eab\u4efb\u52a1\u7684\u8d21\u732e\u3002", "method": "\u8be5\u56e2\u961f\u91c7\u7528\u4e86\u57fa\u4e8e LLM \u63d0\u793a\u7684\u65b9\u6cd5\uff0c\u5e76\u5c1d\u8bd5\u4e86\u4e0d\u540c\u7684\u63d0\u793a\u53d8\u4f53\u3002\u6700\u521d\u5c1d\u8bd5\u4e86 LLaMA-3.2\uff0c\u6700\u7ec8\u4f7f\u7528 Gemma-3 \u63d0\u4ea4\u3002", "result": "\u8be5\u56e2\u961f\u5728\u5b50\u4efb\u52a1 1 \u4e2d\u83b7\u5f97\u7b2c\u4e09\u540d\uff0c\u5728\u5b50\u4efb\u52a1 2 \u4e2d\u83b7\u5f97\u7b2c\u4e8c\u540d\u3002", "conclusion": "CardiffNLP \u56e2\u961f\u5728 IberLEF 2025 CLEARS \u897f\u73ed\u7259\u8bed\u6587\u672c\u6539\u7f16\u5171\u4eab\u4efb\u52a1\u4e2d\uff0c\u5728\u4e24\u4e2a\u5b50\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4e86\u4e0d\u9519\u7684\u6210\u7ee9\uff08\u5b50\u4efb\u52a1 1 \u7b2c\u4e09\u540d\uff0c\u5b50\u4efb\u52a1 2 \u7b2c\u4e8c\u540d\uff09\u3002"}}
{"id": "2508.02874", "categories": ["cs.LG", "cs.AI", "stat.ML", "68T30, 65D10, 62J02, 68T07, 62F35, 62J02", "I.2.6; G.1.2; G.3"], "pdf": "https://arxiv.org/pdf/2508.02874", "abs": "https://arxiv.org/abs/2508.02874", "authors": ["Roman Gutierrez", "Tony Kai Tang", "Isabel Gutierrez"], "title": "Beyond Least Squares: Robust Regression Transformer (R2T)", "comment": "10 pages, 4 figures, 1 table", "summary": "Robust regression techniques rely on least-squares optimization, which works\nwell for Gaussian noise but fails in the presence of asymmetric structured\nnoise. We propose a hybrid neural-symbolic architecture where a transformer\nencoder processes numerical sequences, a compression NN predicts symbolic\nparameters, and a fixed symbolic equation reconstructs the original sequence.\nUsing synthetic data, the training objective is to recover the original\nsequence after adding asymmetric structured noise, effectively learning a\nsymbolic fit guided by neural parameter estimation. Our model achieves a median\nregression MSE of 6e-6 to 3.5e-5 on synthetic wearable data, which is a 10-300\ntimes improvement when compared with ordinary least squares fit and robust\nregression techniques such as Huber loss or SoftL1.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\uff0c\u7528\u4e8e\u5728\u975e\u5bf9\u79f0\u7ed3\u6784\u5316\u566a\u58f0\u4e0b\u8fdb\u884c\u9c81\u68d2\u56de\u5f52\uff0c\u5e76\u5728\u5408\u6210\u53ef\u7a7f\u6234\u8bbe\u5907\u6570\u636e\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "motivation": "\u7a33\u5065\u56de\u5f52\u6280\u672f\u4f9d\u8d56\u4e8e\u6700\u5c0f\u4e8c\u4e58\u4f18\u5316\uff0c\u8be5\u4f18\u5316\u5728\u5b58\u5728\u9ad8\u65af\u566a\u58f0\u65f6\u6548\u679c\u826f\u597d\uff0c\u4f46\u5728\u5b58\u5728\u4e0d\u5bf9\u79f0\u7ed3\u6784\u5316\u566a\u58f0\u65f6\u4f1a\u5931\u6548\u3002", "method": "\u4e00\u79cd\u6df7\u5408\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\uff0c\u5176\u4e2d Transformer \u7f16\u7801\u5668\u5904\u7406\u6570\u503c\u5e8f\u5217\uff0c\u538b\u7f29 NN \u9884\u6d4b\u7b26\u53f7\u53c2\u6570\uff0c\u56fa\u5b9a\u7b26\u53f7\u65b9\u7a0b\u91cd\u5efa\u539f\u59cb\u5e8f\u5217\u3002", "result": "\u8be5\u6a21\u578b\u5728\u5408\u6210\u6570\u636e\u4e0a\uff0c\u7ecf\u8fc7\u6dfb\u52a0\u975e\u5bf9\u79f0\u7ed3\u6784\u566a\u58f0\u540e\uff0c\u8bad\u7ec3\u76ee\u6807\u662f\u6062\u590d\u539f\u59cb\u5e8f\u5217\uff0c\u4ece\u800c\u6709\u6548\u5730\u5b66\u4e60\u7531\u795e\u7ecf\u53c2\u6570\u4f30\u8ba1\u5f15\u5bfc\u7684\u7b26\u53f7\u62df\u5408\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u5408\u6210\u53ef\u7a7f\u6234\u8bbe\u5907\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86 6e-6 \u5230 3.5e-5 \u7684\u4e2d\u503c\u56de\u5f52 MSE\uff0c\u4e0e\u666e\u901a\u6700\u5c0f\u4e8c\u4e58\u6cd5\u548c Huber \u635f\u5931\u6216 SoftL1 \u7b49\u7a33\u5065\u56de\u5f52\u6280\u672f\u76f8\u6bd4\uff0c\u63d0\u9ad8\u4e86 10-300 \u500d\u3002"}}
{"id": "2508.03038", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03038", "abs": "https://arxiv.org/abs/2508.03038", "authors": ["Qi Peng", "Jialin Cui", "Jiayuan Xie", "Yi Cai", "Qing Li"], "title": "Tree-of-Reasoning: Towards Complex Medical Diagnosis via Multi-Agent Reasoning with Evidence Tree", "comment": "Accepted by ACM MM 2025", "summary": "Large language models (LLMs) have shown great potential in the medical\ndomain. However, existing models still fall short when faced with complex\nmedical diagnosis task in the real world. This is mainly because they lack\nsufficient reasoning depth, which leads to information loss or logical jumps\nwhen processing a large amount of specialized medical data, leading to\ndiagnostic errors. To address these challenges, we propose Tree-of-Reasoning\n(ToR), a novel multi-agent framework designed to handle complex scenarios.\nSpecifically, ToR introduces a tree structure that can clearly record the\nreasoning path of LLMs and the corresponding clinical evidence. At the same\ntime, we propose a cross-validation mechanism to ensure the consistency of\nmulti-agent decision-making, thereby improving the clinical reasoning ability\nof multi-agents in complex medical scenarios. Experimental results on\nreal-world medical data show that our framework can achieve better performance\nthan existing baseline methods.", "AI": {"tldr": "The paper introduces Tree-of-Reasoning (ToR), a multi-agent framework that improves the clinical reasoning ability of LLMs in complex medical scenarios by using a tree structure to record reasoning paths and a cross-validation mechanism for consistency.", "motivation": "Existing large language models (LLMs) lack sufficient reasoning depth when faced with complex medical diagnosis tasks, leading to information loss or logical jumps and diagnostic errors.", "method": "A novel multi-agent framework called Tree-of-Reasoning (ToR) is proposed, which introduces a tree structure to record the reasoning path and uses a cross-validation mechanism to ensure the consistency of multi-agent decision-making.", "result": "The proposed framework achieves better performance than existing baseline methods on real-world medical data.", "conclusion": "The Tree-of-Reasoning (ToR) framework achieves better performance than existing baseline methods on real-world medical data."}}
{"id": "2508.03006", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03006", "abs": "https://arxiv.org/abs/2508.03006", "authors": ["Fan Yang", "Yihao Huang", "Jiayi Zhu", "Ling Shi", "Geguang Pu", "Jin Song Dong", "Kailong Wang"], "title": "Seeing It Before It Happens: In-Generation NSFW Detection for Diffusion-Based Text-to-Image Models", "comment": "8 pages", "summary": "Diffusion-based text-to-image (T2I) models enable high-quality image\ngeneration but also pose significant risks of misuse, particularly in producing\nnot-safe-for-work (NSFW) content. While prior detection methods have focused on\nfiltering prompts before generation or moderating images afterward, the\nin-generation phase of diffusion models remains largely unexplored for NSFW\ndetection. In this paper, we introduce In-Generation Detection (IGD), a simple\nyet effective approach that leverages the predicted noise during the diffusion\nprocess as an internal signal to identify NSFW content. This approach is\nmotivated by preliminary findings suggesting that the predicted noise may\ncapture semantic cues that differentiate NSFW from benign prompts, even when\nthe prompts are adversarially crafted. Experiments conducted on seven NSFW\ncategories show that IGD achieves an average detection accuracy of 91.32% over\nnaive and adversarial NSFW prompts, outperforming seven baseline methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u4e2d\u68c0\u6d4b NSFW \u5185\u5bb9\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u9884\u6d4b\u566a\u58f0\u4f5c\u4e3a\u4fe1\u53f7\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u751f\u6210\u524d\u7684\u63d0\u793a\u8fc7\u6ee4\u6216\u751f\u6210\u540e\u7684\u56fe\u50cf\u5ba1\u6838\uff0c\u800c\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u9636\u6bb5\u5728 NSFW \u68c0\u6d4b\u65b9\u9762\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u521d\u6b65\u7814\u7a76\u8868\u660e\uff0c\u9884\u6d4b\u7684\u566a\u58f0\u53ef\u80fd\u6355\u83b7\u8bed\u4e49\u7ebf\u7d22\uff0c\u533a\u5206 NSFW \u548c\u826f\u6027\u63d0\u793a\u3002", "method": "\u5229\u7528\u6269\u6563\u8fc7\u7a0b\u4e2d\u9884\u6d4b\u7684\u566a\u58f0\u4f5c\u4e3a\u5185\u90e8\u4fe1\u53f7\u6765\u8bc6\u522b NSFW \u5185\u5bb9\u3002", "result": "\u5728\u4e03\u4e2a NSFW \u7c7b\u522b\u4e0a\u5b9e\u73b0\u4e86 91.32% \u7684\u5e73\u5747\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u4e03\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIn-Generation Detection (IGD) \u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u6269\u6563\u8fc7\u7a0b\u4e2d\u9884\u6d4b\u7684\u566a\u58f0\u4f5c\u4e3a\u5185\u90e8\u4fe1\u53f7\u6765\u8bc6\u522b\u4e0d\u9002\u5b9c\u5de5\u4f5c\u573a\u6240 (NSFW) \u5185\u5bb9\uff0c\u5e76\u5728\u4e03\u4e2a NSFW \u7c7b\u522b\u4e0a\u5b9e\u73b0\u4e86 91.32% \u7684\u5e73\u5747\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u4e03\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2508.03247", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.03247", "abs": "https://arxiv.org/abs/2508.03247", "authors": ["Shintaro Sakai", "Jisun An", "Migyeong Kang", "Haewoon Kwak"], "title": "Somatic in the East, Psychological in the West?: Investigating Clinically-Grounded Cross-Cultural Depression Symptom Expression in LLMs", "comment": null, "summary": "Prior clinical psychology research shows that Western individuals with\ndepression tend to report psychological symptoms, while Eastern individuals\nreport somatic ones. We test whether Large Language Models (LLMs), which are\nincreasingly used in mental health, reproduce these cultural patterns by\nprompting them with Western or Eastern personas. Results show that LLMs largely\nfail to replicate the patterns when prompted in English, though prompting in\nmajor Eastern languages (i.e., Chinese, Japanese, and Hindi) improves alignment\nin several configurations. Our analysis pinpoints two key reasons for this\nfailure: the models' low sensitivity to cultural personas and a strong,\nculturally invariant symptom hierarchy that overrides cultural cues. These\nfindings reveal that while prompt language is important, current\ngeneral-purpose LLMs lack the robust, culture-aware capabilities essential for\nsafe and effective mental health applications.", "AI": {"tldr": "LLM\u5728\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u4e2d\u7f3a\u4e4f\u6587\u5316\u611f\u77e5\u80fd\u529b\uff0c\u65e0\u6cd5\u533a\u5206\u4e1c\u897f\u65b9\u6291\u90c1\u75c7\u60a3\u8005\u7684\u75c7\u72b6\u5dee\u5f02\u3002", "motivation": "\u5148\u524d\u7684\u4e34\u5e8a\u5fc3\u7406\u5b66\u7814\u7a76\u8868\u660e\uff0c\u897f\u65b9\u6291\u90c1\u75c7\u60a3\u8005\u503e\u5411\u4e8e\u62a5\u544a\u5fc3\u7406\u75c7\u72b6\uff0c\u800c\u4e1c\u65b9\u6291\u90c1\u75c7\u60a3\u8005\u62a5\u544a\u8eaf\u4f53\u75c7\u72b6\u3002\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u3002", "method": "\u901a\u8fc7\u63d0\u793aLLM\u626e\u6f14\u897f\u65b9\u6216\u4e1c\u65b9\u89d2\u8272\u6765\u6d4b\u8bd5LLM\u662f\u5426\u80fd\u91cd\u73b0\u8fd9\u4e9b\u6587\u5316\u6a21\u5f0f\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u4f7f\u7528\u4e3b\u8981\u4e1c\u65b9\u8bed\u8a00\uff08\u5373\u4e2d\u6587\u3001\u65e5\u6587\u548c\u5370\u5730\u8bed\uff09\u8fdb\u884c\u63d0\u793a\u53ef\u4ee5\u5728\u67d0\u4e9b\u914d\u7f6e\u4e2d\u6539\u5584\u4e00\u81f4\u6027\uff0c\u4f46\u7528\u82f1\u8bed\u63d0\u793a\u65f6\uff0cLLM\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u672a\u80fd\u91cd\u73b0\u8fd9\u4e9b\u6a21\u5f0f\u3002\u5206\u6790\u8868\u660e\uff0c\u6a21\u578b\u5bf9\u6587\u5316\u89d2\u8272\u4e0d\u654f\u611f\uff0c\u5e76\u4e14\u5b58\u5728\u5f3a\u5927\u7684\u3001\u6587\u5316\u4e0d\u53d8\u7684\u75c7\u72b6\u7b49\u7ea7\u7ed3\u6784\uff0c\u4ece\u800c\u8986\u76d6\u4e86\u6587\u5316\u7ebf\u7d22\u3002", "conclusion": "\u5f53\u524d\u901a\u7528LLM\u7f3a\u4e4f\u5b89\u5168\u6709\u6548\u7684\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u6240\u9700\u5f3a\u5927\u7684\u6587\u5316\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2508.02879", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.02879", "abs": "https://arxiv.org/abs/2508.02879", "authors": ["Shifeng Xie", "Vasilii Feofanov", "Marius Alonso", "Ambroise Odonnat", "Jianfeng Zhang", "Themis Palpanas", "Ievgen Redko"], "title": "CauKer: classification time series foundation models can be pretrained on synthetic data only", "comment": null, "summary": "Time series foundation models (TSFMs) have recently gained significant\nattention due to their strong zero-shot capabilities and widespread real-world\napplications. Such models typically require a computationally costly\npretraining on large-scale, carefully curated collections of real-world\nsequences. To allow for a sample-efficient pretraining of TSFMs, we propose\nCauKer, a novel algorithm designed to generate diverse, causally coherent\nsynthetic time series with realistic trends, seasonality, and nonlinear\ninteractions. CauKer combines Gaussian Process (GP) kernel composition with\nStructural Causal Models (SCM) to produce data for sample-efficient pretraining\nof state-of-the-art classification TSFMs having different architectures and\nfollowing different pretraining approaches. Additionally, our experiments\nreveal that CauKer-generated datasets exhibit clear scaling laws for both\ndataset size (10K to 10M samples) and model capacity (1M to 783M parameters),\nunlike real-world datasets, which display irregular scaling behavior.", "AI": {"tldr": "CauKer: a novel algorithm designed to generate diverse, causally coherent synthetic time series with realistic trends, seasonality, and nonlinear interactions for sample-efficient pretraining of TSFMs.", "motivation": "To allow for a sample-efficient pretraining of TSFMs.", "method": "CauKer combines Gaussian Process (GP) kernel composition with Structural Causal Models (SCM) to produce data for sample-efficient pretraining of state-of-the-art classification TSFMs having different architectures and following different pretraining approaches.", "result": "CauKer enables sample-efficient pretraining of state-of-the-art classification TSFMs.", "conclusion": "CauKer-generated datasets exhibit clear scaling laws for both dataset size (10K to 10M samples) and model capacity (1M to 783M parameters), unlike real-world datasets, which display irregular scaling behavior."}}
{"id": "2508.03054", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03054", "abs": "https://arxiv.org/abs/2508.03054", "authors": ["Rui Pu", "Chaozhuo Li", "Rui Ha", "Litian Zhang", "Lirong Qiu", "Xi Zhang"], "title": "Beyond Surface-Level Detection: Towards Cognitive-Driven Defense Against Jailbreak Attacks via Meta-Operations Reasoning", "comment": null, "summary": "Defending large language models (LLMs) against jailbreak attacks is essential\nfor their safe and reliable deployment. Existing defenses often rely on shallow\npattern matching, which struggles to generalize to novel and unseen attack\nstrategies. To address this challenge, we propose the Cognitive-Driven Defense\n(CDD) framework, which targets the underlying structure of jailbreak prompts by\napplying meta-operations, defined as basic manipulations that conceal harmful\nintent.CDD emulates human cognitive reasoning through a structured reasoning\nchain. It begins with a global perception of the prompt and follows with a\nlocalized analysis to uncover hidden manipulations. By applying supervised\nfine-tuning on this structured chain, the model learns to identify and reason\nabout known manipulation patterns. To enhance generalization to unseen threats,\nan entropy-guided reinforcement learning algorithm (EG-GRPO) is introduced to\nencourage exploration of new types and variants of meta-operations. Experiments\ndemonstrate that CDD can achieve state-of-the-art defense performance and\nexhibit strong generalization to unseen jailbreak attacks.", "AI": {"tldr": "CDD \u662f\u4e00\u79cd\u65b0\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u8ba4\u77e5\u63a8\u7406\u6765\u9632\u5fa1 LLM \u7684\u8d8a\u72f1\u653b\u51fb\uff0c\u5e76\u4e14\u5728\u672a\u77e5\u7684\u653b\u51fb\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4fdd\u62a4\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u514d\u53d7\u8d8a\u72f1\u653b\u51fb\u5bf9\u4e8e\u5176\u5b89\u5168\u53ef\u9760\u7684\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u9632\u5fa1\u901a\u5e38\u4f9d\u8d56\u4e8e\u6d45\u5c42\u6a21\u5f0f\u5339\u914d\uff0c\u8fd9\u79cd\u5339\u914d\u96be\u4ee5\u63a8\u5e7f\u5230\u65b0\u9896\u548c\u672a\u77e5\u7684\u653b\u51fb\u7b56\u7565\u3002", "method": "\u8ba4\u77e5\u9a71\u52a8\u9632\u5fa1 (CDD) \u6846\u67b6\uff0c\u901a\u8fc7\u5e94\u7528\u5143\u64cd\u4f5c\u6765\u9488\u5bf9\u8d8a\u72f1\u63d0\u793a\u7684\u5e95\u5c42\u7ed3\u6784\uff0c\u5143\u64cd\u4f5c\u88ab\u5b9a\u4e49\u4e3a\u9690\u85cf\u6709\u5bb3\u610f\u56fe\u7684\u57fa\u672c\u64cd\u4f5c\u3002CDD \u901a\u8fc7\u7ed3\u6784\u5316\u7684\u63a8\u7406\u94fe\u6765\u6a21\u62df\u4eba\u7c7b\u7684\u8ba4\u77e5\u63a8\u7406\u3002\u5b83\u9996\u5148\u5bf9\u63d0\u793a\u8fdb\u884c\u5168\u5c40\u611f\u77e5\uff0c\u7136\u540e\u8fdb\u884c\u5c40\u90e8\u5206\u6790\uff0c\u4ee5\u53d1\u73b0\u9690\u85cf\u7684\u64cd\u4f5c\u3002\u901a\u8fc7\u5bf9\u8fd9\u4e2a\u7ed3\u6784\u5316\u7684\u94fe\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u8be5\u6a21\u578b\u5b66\u4f1a\u8bc6\u522b\u548c\u63a8\u7406\u5df2\u77e5\u7684\u64cd\u4f5c\u6a21\u5f0f\u3002\u4e3a\u4e86\u52a0\u5f3a\u5bf9\u672a\u77e5\u5a01\u80c1\u7684\u6cdb\u5316\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u71b5\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5 (EG-GRPO)\uff0c\u4ee5\u9f13\u52b1\u63a2\u7d22\u65b0\u578b\u548c\u53d8\u578b\u7684\u5143\u64cd\u4f5c\u3002", "result": "CDD \u53ef\u4ee5\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u9632\u5fa1\u6027\u80fd\uff0c\u5e76\u5bf9\u672a\u77e5\u7684\u8d8a\u72f1\u653b\u51fb\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CDD \u53ef\u4ee5\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u9632\u5fa1\u6027\u80fd\uff0c\u5e76\u5bf9\u672a\u77e5\u7684\u8d8a\u72f1\u653b\u51fb\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.03007", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03007", "abs": "https://arxiv.org/abs/2508.03007", "authors": ["Xinhui Li", "Xiaojie Guo"], "title": "Multi-Granularity Feature Calibration via VFM for Domain Generalized Semantic Segmentation", "comment": null, "summary": "Domain Generalized Semantic Segmentation (DGSS) aims to improve the\ngeneralization ability of models across unseen domains without access to target\ndata during training. Recent advances in DGSS have increasingly exploited\nvision foundation models (VFMs) via parameter-efficient fine-tuning strategies.\nHowever, most existing approaches concentrate on global feature fine-tuning,\nwhile overlooking hierarchical adaptation across feature levels, which is\ncrucial for precise dense prediction. In this paper, we propose\nMulti-Granularity Feature Calibration (MGFC), a novel framework that performs\ncoarse-to-fine alignment of VFM features to enhance robustness under domain\nshifts. Specifically, MGFC first calibrates coarse-grained features to capture\nglobal contextual semantics and scene-level structure. Then, it refines\nmedium-grained features by promoting category-level feature discriminability.\nFinally, fine-grained features are calibrated through high-frequency spatial\ndetail enhancement. By performing hierarchical and granularity-aware\ncalibration, MGFC effectively transfers the generalization strengths of VFMs to\nthe domain-specific task of DGSS. Extensive experiments on benchmark datasets\ndemonstrate that our method outperforms state-of-the-art DGSS approaches,\nhighlighting the effectiveness of multi-granularity adaptation for the semantic\nsegmentation task of domain generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u7c92\u5ea6\u7279\u5f81\u6821\u51c6\u6846\u67b6\uff0c\u7528\u4e8e\u9886\u57df\u6cdb\u5316\u8bed\u4e49\u5206\u5272\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u7279\u5f81\u5bf9\u9f50\u6765\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u7ed3\u679c\u3002", "motivation": "\u9886\u57df\u6cdb\u5316\u8bed\u4e49\u5206\u5272(DGSS)\u65e8\u5728\u63d0\u9ad8\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u9886\u57df\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u65e0\u9700\u5728\u8bad\u7ec3\u671f\u95f4\u8bbf\u95ee\u76ee\u6807\u6570\u636e\u3002\u6700\u8fd1DGSS\u7684\u8fdb\u5c55\u8d8a\u6765\u8d8a\u591a\u5730\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u7b56\u7565\u6765\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b(VFM)\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u90fd\u96c6\u4e2d\u4e8e\u5168\u5c40\u7279\u5f81\u5fae\u8c03\uff0c\u800c\u5ffd\u7565\u4e86\u8de8\u7279\u5f81\u5c42\u6b21\u7684\u5c42\u6b21\u81ea\u9002\u5e94\uff0c\u8fd9\u5bf9\u4e8e\u7cbe\u786e\u7684\u5bc6\u96c6\u9884\u6d4b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7c92\u5ea6\u7279\u5f81\u6821\u51c6(MGFC)\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6267\u884cVFM\u7279\u5f81\u7684\u7c97\u5230\u7ec6\u5bf9\u9f50\uff0c\u4ee5\u589e\u5f3a\u57df\u79fb\u4f4d\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u901a\u8fc7\u6267\u884c\u5206\u5c42\u548c\u7c92\u5ea6\u611f\u77e5\u6821\u51c6\uff0cMGFC\u6709\u6548\u5730\u5c06VFMs\u7684\u6cdb\u5316\u4f18\u52bf\u8f6c\u79fb\u5230DGSS\u7684\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u3002\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684DGSS\u65b9\u6cd5\uff0c\u7a81\u51fa\u4e86\u591a\u7c92\u5ea6\u81ea\u9002\u5e94\u5bf9\u4e8e\u9886\u57df\u6cdb\u5316\u7684\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u7684\u6709\u6548\u6027\u3002", "conclusion": "MGFC\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684DGSS\u65b9\u6cd5\uff0c\u7a81\u51fa\u4e86\u591a\u7c92\u5ea6\u81ea\u9002\u5e94\u5bf9\u4e8e\u9886\u57df\u6cdb\u5316\u7684\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.03250", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03250", "abs": "https://arxiv.org/abs/2508.03250", "authors": ["Deborah Dore", "Elena Cabrio", "Serena Villata"], "title": "RooseBERT: A New Deal For Political Language Modelling", "comment": null, "summary": "The increasing amount of political debates and politics-related discussions\ncalls for the definition of novel computational methods to automatically\nanalyse such content with the final goal of lightening up political\ndeliberation to citizens. However, the specificity of the political language\nand the argumentative form of these debates (employing hidden communication\nstrategies and leveraging implicit arguments) make this task very challenging,\neven for current general-purpose pre-trained Language Models. To address this\nissue, we introduce a novel pre-trained Language Model for political discourse\nlanguage called RooseBERT. Pre-training a language model on a specialised\ndomain presents different technical and linguistic challenges, requiring\nextensive computational resources and large-scale data. RooseBERT has been\ntrained on large political debate and speech corpora (8K debates, each composed\nof several sub-debates on different topics) in English. To evaluate its\nperformances, we fine-tuned it on four downstream tasks related to political\ndebate analysis, i.e., named entity recognition, sentiment analysis, argument\ncomponent detection and classification, and argument relation prediction and\nclassification. Our results demonstrate significant improvements over\ngeneral-purpose Language Models on these four tasks, highlighting how\ndomain-specific pre-training enhances performance in political debate analysis.\nWe release the RooseBERT language model for the research community.", "AI": {"tldr": "RooseBERT\u662f\u4e00\u79cd\u7528\u4e8e\u653f\u6cbb\u8bed\u7bc7\u8bed\u8a00\u7684\u65b0\u578b\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u5b83\u5728\u653f\u6cbb\u8fa9\u8bba\u5206\u6790\u4e2d\u4f18\u4e8e\u901a\u7528\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u8d8a\u6765\u8d8a\u591a\u7684\u653f\u6cbb\u8fa9\u8bba\u548c\u4e0e\u653f\u6cbb\u76f8\u5173\u7684\u8ba8\u8bba\u9700\u8981\u5b9a\u4e49\u65b0\u7684\u8ba1\u7b97\u65b9\u6cd5\u6765\u81ea\u52a8\u5206\u6790\u8fd9\u4e9b\u5185\u5bb9\uff0c\u6700\u7ec8\u76ee\u6807\u662f\u542f\u53d1\u516c\u6c11\u8fdb\u884c\u653f\u6cbb\u5ba1\u8bae\u3002\u7136\u800c\uff0c\u653f\u6cbb\u8bed\u8a00\u7684\u7279\u6b8a\u6027\u548c\u8fd9\u4e9b\u8fa9\u8bba\u7684\u8bba\u8bc1\u5f62\u5f0f\uff08\u91c7\u7528\u9690\u85cf\u7684\u6c9f\u901a\u7b56\u7565\u548c\u5229\u7528\u9690\u542b\u7684\u8bba\u70b9\uff09\u4f7f\u5f97\u8fd9\u9879\u4efb\u52a1\u975e\u5e38\u5177\u6709\u6311\u6218\u6027\uff0c\u5373\u4f7f\u5bf9\u4e8e\u5f53\u524d\u901a\u7528\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e5f\u662f\u5982\u6b64\u3002", "method": "\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aRooseBERT\u7684\u65b0\u7684\u653f\u6cbb\u8bed\u7bc7\u8bed\u8a00\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u3002RooseBERT\u5df2\u7ecf\u5728\u5927\u578b\u653f\u6cbb\u8fa9\u8bba\u548c\u6f14\u8bb2\u8bed\u6599\u5e93\uff088K\u8fa9\u8bba\uff0c\u6bcf\u4e2a\u8fa9\u8bba\u7531\u5173\u4e8e\u4e0d\u540c\u4e3b\u9898\u7684\u51e0\u4e2a\u5b50\u8fa9\u8bba\u7ec4\u6210\uff09\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u3002", "result": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e0e\u653f\u6cbb\u8fa9\u8bba\u5206\u6790\u76f8\u5173\u7684\u56db\u4e2a\u4e0b\u6e38\u4efb\u52a1\uff08\u5373\uff0c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff0c\u60c5\u611f\u5206\u6790\uff0c\u8bba\u8bc1\u6210\u5206\u68c0\u6d4b\u548c\u5206\u7c7b\u4ee5\u53ca\u8bba\u8bc1\u5173\u7cfb\u9884\u6d4b\u548c\u5206\u7c7b\uff09\u4e2d\uff0cRooseBERT\u7684\u6027\u80fd\u4f18\u4e8e\u901a\u7528\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "RooseBERT\u5728\u56db\u4e2a\u653f\u6cbb\u8fa9\u8bba\u5206\u6790\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u8868\u660e\u7279\u5b9a\u9886\u57df\u7684\u9884\u8bad\u7ec3\u53ef\u4ee5\u63d0\u9ad8\u653f\u6cbb\u8fa9\u8bba\u5206\u6790\u7684\u6027\u80fd\u3002\u6211\u4eec\u4e3a\u7814\u7a76\u56e2\u4f53\u53d1\u5e03\u4e86RooseBERT\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2508.02882", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2508.02882", "abs": "https://arxiv.org/abs/2508.02882", "authors": ["Alex Massucco", "Davide Murari", "Carola-Bibiane Sch\u00f6nlieb"], "title": "Neural Networks with Orthogonal Jacobian", "comment": null, "summary": "Very deep neural networks achieve state-of-the-art performance by extracting\nrich, hierarchical features. Yet, training them via backpropagation is often\nhindered by vanishing or exploding gradients. Existing remedies, such as\northogonal or variance-preserving initialisation and residual architectures,\nallow for a more stable gradient propagation and the training of deeper models.\nIn this work, we introduce a unified mathematical framework that describes a\nbroad class of nonlinear feedforward and residual networks, whose\ninput-to-output Jacobian matrices are exactly orthogonal almost everywhere.\nSuch a constraint forces the resulting networks to achieve perfect dynamical\nisometry and train efficiently despite being very deep. Our formulation not\nonly recovers standard architectures as particular cases but also yields new\ndesigns that match the trainability of residual networks without relying on\nconventional skip connections. We provide experimental evidence that perfect\nJacobian orthogonality at initialisation is sufficient to stabilise training\nand achieve competitive performance. We compare this strategy to networks\nregularised to maintain the Jacobian orthogonality and obtain comparable\nresults. We further extend our analysis to a class of networks\nwell-approximated by those with orthogonal Jacobians and introduce networks\nwith Jacobians representing partial isometries. These generalized models are\nthen showed to maintain the favourable trainability properties.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u5177\u6709\u5b8c\u7f8e\u52a8\u6001\u7b49\u8ddd\u548c\u9ad8\u6548\u8bad\u7ec3\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u3002", "motivation": "\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u8bad\u7ec3\u975e\u5e38\u6df1\u7684\u795e\u7ecf\u7f51\u7edc\u901a\u5e38\u4f1a\u53d7\u5230\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8\u7684\u963b\u788d\u3002\u73b0\u6709\u7684\u8865\u6551\u63aa\u65bd\uff0c\u5982\u6b63\u4ea4\u6216\u65b9\u5dee\u4fdd\u6301\u521d\u59cb\u5316\u548c\u6b8b\u5dee\u67b6\u6784\uff0c\u5141\u8bb8\u66f4\u7a33\u5b9a\u7684\u68af\u5ea6\u4f20\u64ad\u548c\u66f4\u6df1\u6a21\u578b\u7684\u8bad\u7ec3\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6570\u5b66\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u63cf\u8ff0\u4e86\u5e7f\u6cdb\u7684\u975e\u7ebf\u6027\u524d\u9988\u548c\u6b8b\u5dee\u7f51\u7edc\uff0c\u8fd9\u4e9b\u7f51\u7edc\u7684\u8f93\u5165\u5230\u8f93\u51fa\u96c5\u53ef\u6bd4\u77e9\u9635\u51e0\u4e4e\u5728\u6240\u6709\u5730\u65b9\u90fd\u662f\u5b8c\u5168\u6b63\u4ea4\u7684\u3002", "result": "\u5b8c\u7f8e\u96c5\u53ef\u6bd4\u6b63\u4ea4\u6027\u5728\u521d\u59cb\u5316\u65f6\u8db3\u4ee5\u7a33\u5b9a\u8bad\u7ec3\u5e76\u83b7\u5f97\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u5c06\u5176\u4e0e\u6b63\u5219\u5316\u4e3a\u4fdd\u6301\u96c5\u53ef\u6bd4\u6b63\u4ea4\u6027\u7684\u7f51\u7edc\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u83b7\u5f97\u76f8\u5f53\u7684\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u5c06\u5206\u6790\u6269\u5c55\u5230\u4e00\u7c7b\u53ef\u4ee5\u7528\u6b63\u4ea4\u96c5\u53ef\u6bd4\u77e9\u9635\u5f88\u597d\u5730\u8fd1\u4f3c\u7684\u7f51\u7edc\uff0c\u5e76\u5f15\u5165\u5177\u6709\u8868\u793a\u90e8\u5206\u7b49\u8ddd\u7684\u96c5\u53ef\u6bd4\u77e9\u9635\u7684\u7f51\u7edc\u3002\u7136\u540e\u8bc1\u660e\u8fd9\u4e9b\u5e7f\u4e49\u6a21\u578b\u53ef\u4ee5\u4fdd\u6301\u826f\u597d\u7684\u53ef\u8bad\u7ec3\u6027\u3002", "conclusion": "\u96c5\u53ef\u6bd4\u6b63\u4ea4\u6027\u8db3\u4ee5\u7a33\u5b9a\u8bad\u7ec3\u5e76\u83b7\u5f97\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u5c06\u8fd9\u79cd\u7b56\u7565\u4e0e\u6b63\u5219\u5316\u7f51\u7edc\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ee5\u4fdd\u6301\u96c5\u53ef\u6bd4\u6b63\u4ea4\u6027\u5e76\u83b7\u5f97\u76f8\u5f53\u7684\u7ed3\u679c\u3002\u5c06\u5206\u6790\u6269\u5c55\u5230\u4e00\u7c7b\u53ef\u4ee5\u7528\u6b63\u4ea4\u96c5\u53ef\u6bd4\u77e9\u9635\u7684\u7f51\u7edc\u5f88\u597d\u5730\u8fd1\u4f3c\u7684\u7f51\u7edc\uff0c\u5e76\u5f15\u5165\u5177\u6709\u8868\u793a\u90e8\u5206\u7b49\u8ddd\u7684\u96c5\u53ef\u6bd4\u77e9\u9635\u7684\u7f51\u7edc\u3002\u8fd9\u4e9b\u5e7f\u4e49\u6a21\u578b\u968f\u540e\u88ab\u8bc1\u660e\u53ef\u4ee5\u4fdd\u6301\u826f\u597d\u7684\u53ef\u8bad\u7ec3\u6027\u3002"}}
{"id": "2508.03080", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03080", "abs": "https://arxiv.org/abs/2508.03080", "authors": ["Shuang Liu", "Zelong Li", "Ruoyun Ma", "Haiyan Zhao", "Mengnan Du"], "title": "ContractEval: Benchmarking LLMs for Clause-Level Legal Risk Identification in Commercial Contracts", "comment": null, "summary": "The potential of large language models (LLMs) in specialized domains such as\nlegal risk analysis remains underexplored. In response to growing interest in\nlocally deploying open-source LLMs for legal tasks while preserving data\nconfidentiality, this paper introduces ContractEval, the first benchmark to\nthoroughly evaluate whether open-source LLMs could match proprietary LLMs in\nidentifying clause-level legal risks in commercial contracts. Using the\nContract Understanding Atticus Dataset (CUAD), we assess 4 proprietary and 15\nopen-source LLMs. Our results highlight five key findings: (1) Proprietary\nmodels outperform open-source models in both correctness and output\neffectiveness, though some open-source models are competitive in certain\nspecific dimensions. (2) Larger open-source models generally perform better,\nthough the improvement slows down as models get bigger. (3) Reasoning\n(\"thinking\") mode improves output effectiveness but reduces correctness, likely\ndue to over-complicating simpler tasks. (4) Open-source models generate \"no\nrelated clause\" responses more frequently even when relevant clauses are\npresent. This suggests \"laziness\" in thinking or low confidence in extracting\nrelevant content. (5) Model quantization speeds up inference but at the cost of\nperformance drop, showing the tradeoff between efficiency and accuracy. These\nfindings suggest that while most LLMs perform at a level comparable to junior\nlegal assistants, open-source models require targeted fine-tuning to ensure\ncorrectness and effectiveness in high-stakes legal settings. ContractEval\noffers a solid benchmark to guide future development of legal-domain LLMs.", "AI": {"tldr": "ContractEval benchmark shows open-source LLMs lag proprietary models in legal risk analysis, needing fine-tuning for high-stakes legal tasks.", "motivation": "The potential of large language models (LLMs) in specialized domains such as legal risk analysis remains underexplored. Growing interest in locally deploying open-source LLMs for legal tasks while preserving data confidentiality.", "method": "Introduce ContractEval, the first benchmark to thoroughly evaluate whether open-source LLMs could match proprietary LLMs in identifying clause-level legal risks in commercial contracts. Assess 4 proprietary and 15 open-source LLMs using the Contract Understanding Atticus Dataset (CUAD).", "result": "Proprietary models outperform open-source models. Larger open-source models generally perform better, though the improvement slows down as models get bigger. Reasoning mode improves output effectiveness but reduces correctness. Open-source models generate 'no related clause' responses more frequently. Model quantization speeds up inference but at the cost of performance drop.", "conclusion": "Open-source LLMs require targeted fine-tuning to ensure correctness and effectiveness in high-stakes legal settings. ContractEval offers a solid benchmark to guide future development of legal-domain LLMs."}}
{"id": "2508.03009", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03009", "abs": "https://arxiv.org/abs/2508.03009", "authors": ["Xuyi Yang", "Wenhao Zhang", "Hongbo Jin", "Lin Liu", "Hongbo Xu", "Yongwei Nie", "Fei Yu", "Fei Ma"], "title": "Enhancing Long Video Question Answering with Scene-Localized Frame Grouping", "comment": null, "summary": "Current Multimodal Large Language Models (MLLMs) often perform poorly in long\nvideo understanding, primarily due to resource limitations that prevent them\nfrom processing all video frames and their associated information. Efficiently\nextracting relevant information becomes a challenging task. Existing frameworks\nand evaluation tasks focus on identifying specific frames containing core\nobjects from a large number of irrelevant frames, which does not align with the\npractical needs of real-world applications. To address this issue, we propose a\nnew scenario under the video question-answering task, SceneQA, which emphasizes\nscene-based detail perception and reasoning abilities. And we develop the LVSQA\ndataset to support the SceneQA task, which is built upon carefully selected\nvideos from LVBench and contains a new collection of question-answer pairs to\npromote a more fair evaluation of MLLMs' scene perception abilities in long\nvideos. Inspired by human cognition, we introduce a novel method called SLFG.\nThe core idea of SLFG is to combine individual frames into semantically\ncoherent scene frames. By leveraging scene localization methods and dynamic\nframe reassembly mechanisms, SLFG significantly enhances the understanding\ncapabilities of existing MLLMs in long videos. SLFG requires no modification to\nthe original model architecture and boasts excellent plug-and-play usability.\nExperimental results show that this method performs exceptionally well in\nseveral long video benchmark tests. Code and dataset will be released at\nhttp://www.slfg.pkuzwh.cn.", "AI": {"tldr": "The paper introduces SceneQA, a new scenario under the video question-answering task, and develops the LVSQA dataset. It proposes SLFG, a novel method that combines frames into semantically coherent scene frames to enhance MLLMs' understanding in long videos.", "motivation": "Current Multimodal Large Language Models (MLLMs) often perform poorly in long video understanding due to resource limitations. Existing frameworks do not align with the practical needs of real-world applications.", "method": "A novel method called SLFG is introduced, which combines individual frames into semantically coherent scene frames by leveraging scene localization methods and dynamic frame reassembly mechanisms.", "result": "Experimental results show that SLFG performs exceptionally well in several long video benchmark tests.", "conclusion": "The SLFG method significantly enhances the understanding capabilities of existing MLLMs in long videos, performs exceptionally well in several long video benchmark tests, and requires no modification to the original model architecture."}}
{"id": "2508.03259", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03259", "abs": "https://arxiv.org/abs/2508.03259", "authors": ["Duzhen Zhang", "Chenxing Li", "Jiahua Dong", "Qi Liu", "Dong Yu"], "title": "Exploring Stability-Plasticity Trade-offs for Continual Named Entity Recognition", "comment": "Accepted by IEEE/ACM Transactions on Audio, Speech and Language\n  Processing", "summary": "Continual Named Entity Recognition (CNER) is an evolving field that focuses\non sequentially updating an existing model to incorporate new entity types.\nPrevious CNER methods primarily utilize Knowledge Distillation (KD) to preserve\nprior knowledge and overcome catastrophic forgetting, strictly ensuring that\nthe representations of old and new models remain consistent. Consequently, they\noften impart the model with excessive stability (i.e., retention of old\nknowledge) but limited plasticity (i.e., acquisition of new knowledge). To\naddress this issue, we propose a Stability-Plasticity Trade-off (SPT) method\nfor CNER that balances these aspects from both representation and weight\nperspectives. From the representation perspective, we introduce a pooling\noperation into the original KD, permitting a level of plasticity by\nconsolidating representation dimensions. From the weight perspective, we\ndynamically merge the weights of old and new models, strengthening old\nknowledge while maintaining new knowledge. During this fusion, we implement a\nweight-guided selective mechanism to prioritize significant weights. Moreover,\nwe develop a confidence-based pseudo-labeling approach for the current\nnon-entity type, which predicts entity types using the old model to handle the\nsemantic shift of the non-entity type, a challenge specific to CNER that has\nlargely been ignored by previous methods. Extensive experiments across ten CNER\nsettings on three benchmark datasets demonstrate that our SPT method surpasses\nprevious CNER approaches, highlighting its effectiveness in achieving a\nsuitable stability-plasticity trade-off.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eCNER\u7684\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u6743\u8861\uff08SPT\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5408\u5e76\u8868\u793a\u7ef4\u5ea6\u5e76\u52a8\u6001\u5408\u5e76\u65b0\u65e7\u6a21\u578b\u7684\u6743\u91cd\u6765\u5e73\u8861\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\u3002", "motivation": "\u4ee5\u524d\u7684CNER\u65b9\u6cd5\u4e3b\u8981\u5229\u7528\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u6765\u4fdd\u5b58\u5148\u9a8c\u77e5\u8bc6\u5e76\u514b\u670d\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4e25\u683c\u786e\u4fdd\u65e7\u6a21\u578b\u548c\u65b0\u6a21\u578b\u7684\u8868\u793a\u4fdd\u6301\u4e00\u81f4\u3002\u56e0\u6b64\uff0c\u5b83\u4eec\u901a\u5e38\u8d4b\u4e88\u6a21\u578b\u8fc7\u5ea6\u7684\u7a33\u5b9a\u6027\uff08\u5373\u4fdd\u7559\u65e7\u77e5\u8bc6\uff09\uff0c\u4f46\u53ef\u5851\u6027\u6709\u9650\uff08\u5373\u83b7\u53d6\u65b0\u77e5\u8bc6\uff09\u3002", "method": "\u4ece\u8868\u793a\u548c\u6743\u91cd\u4e24\u4e2a\u89d2\u5ea6\u5e73\u8861\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\u3002\u4ece\u8868\u793a\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u6211\u4eec\u5c06\u6c60\u5316\u64cd\u4f5c\u5f15\u5165\u5230\u539f\u59cbKD\u4e2d\uff0c\u5141\u8bb8\u901a\u8fc7\u5408\u5e76\u8868\u793a\u7ef4\u5ea6\u6765\u5b9e\u73b0\u4e00\u5b9a\u7a0b\u5ea6\u7684\u53ef\u5851\u6027\u3002\u4ece\u6743\u91cd\u7684\u89d2\u5ea6\u6765\u770b\uff0c\u6211\u4eec\u52a8\u6001\u5730\u5408\u5e76\u65b0\u65e7\u6a21\u578b\u7684\u6743\u91cd\uff0c\u52a0\u5f3a\u65e7\u77e5\u8bc6\uff0c\u540c\u65f6\u4fdd\u6301\u65b0\u77e5\u8bc6\u3002\u5728\u6b64\u878d\u5408\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u79cd\u6743\u91cd\u5f15\u5bfc\u7684\u9009\u62e9\u673a\u5236\uff0c\u4ee5\u4f18\u5148\u8003\u8651\u91cd\u8981\u6743\u91cd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4e3a\u5f53\u524d\u7684\u975e\u5b9e\u4f53\u7c7b\u578b\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u4f2a\u6807\u8bb0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u65e7\u6a21\u578b\u9884\u6d4b\u5b9e\u4f53\u7c7b\u578b\uff0c\u4ee5\u5904\u7406\u975e\u5b9e\u4f53\u7c7b\u578b\u7684\u8bed\u4e49\u8f6c\u6362\uff0c\u8fd9\u662fCNER\u7279\u6709\u7684\u6311\u6218\uff0c\u800c\u4ee5\u524d\u7684\u65b9\u6cd5\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u5ffd\u7565\u4e86\u8fd9\u4e00\u6311\u6218\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5341\u4e2aCNER\u8bbe\u7f6e\u4e2d\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684SPT\u65b9\u6cd5\u8d85\u8fc7\u4e86\u4ee5\u524d\u7684CNER\u65b9\u6cd5\uff0c\u7a81\u51fa\u4e86\u5176\u5728\u5b9e\u73b0\u9002\u5f53\u7684\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u6743\u8861\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "SPT\u65b9\u6cd5\u5728\u6301\u7eed\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08CNER\uff09\u4e2d\u4f18\u4e8e\u4ee5\u524d\u7684\u65b9\u6cd5\uff0c\u56e0\u4e3a\u5b83\u5728\u5b9e\u73b0\u9002\u5f53\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u5851\u6027\u6743\u8861\u65b9\u9762\u662f\u6709\u6548\u7684\u3002"}}
{"id": "2508.02887", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.02887", "abs": "https://arxiv.org/abs/2508.02887", "authors": ["Jialin Zheng", "Haoyu Wang", "Yangbin Zeng", "Di Mou", "Xin Zhang", "Hong Li", "Sergio Vazquez", "Leopoldo G. Franquelo"], "title": "Physics-Embedded Neural ODEs for Sim2Real Edge Digital Twins of Hybrid Power Electronics Systems", "comment": null, "summary": "Edge Digital Twins (EDTs) are crucial for monitoring and control of Power\nElectronics Systems (PES). However, existing modeling approaches struggle to\nconsistently capture continuously evolving hybrid dynamics that are inherent in\nPES, degrading Sim-to-Real generalization on resource-constrained edge devices.\nTo address these challenges, this paper proposes a Physics-Embedded Neural ODEs\n(PENODE) that (i) embeds the hybrid operating mechanism as an event automaton\nto explicitly govern discrete switching and (ii) injects known governing ODE\ncomponents directly into the neural parameterization of unmodeled dynamics.\nThis unified design yields a differentiable end-to-end trainable architecture\nthat preserves physical interpretability while reducing redundancy, and it\nsupports a cloud-to-edge toolchain for efficient FPGA deployment. Experimental\nresults demonstrate that PENODE achieves significantly higher accuracy in\nbenchmarks in white-box, gray-box, and black-box scenarios, with a 75%\nreduction in neuron count, validating that the proposed PENODE maintains\nphysical interpretability, efficient edge deployment, and real-time control\nenhancement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u5d4c\u5165\u795e\u7ecf ODE (PENODE)\uff0c\u7528\u4e8e\u63d0\u9ad8\u7535\u529b\u7535\u5b50\u7cfb\u7edf\u8fb9\u7f18\u6570\u5b57\u5b6a\u751f\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u8fb9\u7f18\u6570\u5b57\u5b6a\u751f (EDT) \u5bf9\u4e8e\u7535\u529b\u7535\u5b50\u7cfb\u7edf (PES) \u7684\u76d1\u63a7\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5efa\u6a21\u65b9\u6cd5\u96be\u4ee5\u6301\u7eed\u6355\u6349 PES \u4e2d\u56fa\u6709\u7684\u4e0d\u65ad\u6f14\u53d8\u7684\u6df7\u5408\u52a8\u529b\u5b66\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684 Sim-to-Real \u6cdb\u5316\u80fd\u529b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u5d4c\u5165\u795e\u7ecf ODE (PENODE)\uff0c\u5b83 (i) \u5d4c\u5165\u6df7\u5408\u64cd\u4f5c\u673a\u5236\u4f5c\u4e3a\u4e8b\u4ef6\u81ea\u52a8\u673a\uff0c\u4ee5\u663e\u5f0f\u5730\u63a7\u5236\u79bb\u6563\u5207\u6362\uff0c\u5e76\u4e14 (ii) \u5c06\u5df2\u77e5\u7684\u63a7\u5236 ODE \u5206\u91cf\u76f4\u63a5\u6ce8\u5165\u5230\u672a\u5efa\u6a21\u52a8\u529b\u5b66\u7684\u795e\u7ecf\u53c2\u6570\u5316\u4e2d\u3002", "result": "PENODE \u5728\u767d\u76d2\u3001\u7070\u76d2\u548c\u9ed1\u76d2\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u660e\u663e\u66f4\u9ad8\u7684\u7cbe\u5ea6\uff0c\u795e\u7ecf\u5143\u6570\u91cf\u51cf\u5c11\u4e86 75%\u3002", "conclusion": "PENODE\u5728\u767d\u76d2\u3001\u7070\u76d2\u548c\u9ed1\u76d2\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u7cbe\u5ea6\uff0c\u795e\u7ecf\u5143\u6570\u91cf\u51cf\u5c11\u4e86 75%\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684 PENODE \u4fdd\u6301\u4e86\u7269\u7406\u53ef\u89e3\u91ca\u6027\u3001\u9ad8\u6548\u7684\u8fb9\u7f18\u90e8\u7f72\u548c\u5b9e\u65f6\u63a7\u5236\u589e\u5f3a\u3002"}}
{"id": "2508.03082", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03082", "abs": "https://arxiv.org/abs/2508.03082", "authors": ["Fei Liu", "Yilu Liu", "Qingfu Zhang", "Xialiang Tong", "Mingxuan Yuan"], "title": "EoH-S: Evolution of Heuristic Set using LLMs for Automated Heuristic Design", "comment": null, "summary": "Automated Heuristic Design (AHD) using Large Language Models (LLMs) has\nachieved notable success in recent years. Despite the effectiveness of existing\napproaches, they only design a single heuristic to serve all problem instances,\noften inducing poor generalization across different distributions or settings.\nTo address this issue, we propose Automated Heuristic Set Design (AHSD), a new\nformulation for LLM-driven AHD. The aim of AHSD is to automatically generate a\nsmall-sized complementary heuristic set to serve diverse problem instances,\nsuch that each problem instance could be optimized by at least one heuristic in\nthis set. We show that the objective function of AHSD is monotone and\nsupermodular. Then, we propose Evolution of Heuristic Set (EoH-S) to apply the\nAHSD formulation for LLM-driven AHD. With two novel mechanisms of complementary\npopulation management and complementary-aware memetic search, EoH-S could\neffectively generate a set of high-quality and complementary heuristics.\nComprehensive experimental results on three AHD tasks with diverse instances\nspanning various sizes and distributions demonstrate that EoH-S consistently\noutperforms existing state-of-the-art AHD methods and achieves up to 60\\%\nperformance improvements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u81ea\u52a8\u542f\u53d1\u5f0f\u96c6\u5408\u8bbe\u8ba1\uff08AHSD\uff09\uff0c\u4e00\u79cd\u7528\u4e8eLLM\u9a71\u52a8\u7684AHD\u7684\u65b0\u516c\u5f0f\uff0c\u65e8\u5728\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u5c0f\u578b\u4e92\u8865\u542f\u53d1\u5f0f\u96c6\u5408\u6765\u670d\u52a1\u5404\u79cd\u95ee\u9898\u5b9e\u4f8b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u8bbe\u8ba1\u4e00\u79cd\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u670d\u52a1\u4e8e\u6240\u6709\u95ee\u9898\u5b9e\u4f8b\uff0c\u901a\u5e38\u5bfc\u81f4\u8de8\u4e0d\u540c\u5206\u5e03\u6216\u8bbe\u7f6e\u7684\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u81ea\u52a8\u542f\u53d1\u5f0f\u96c6\u5408\u8bbe\u8ba1\uff08AHSD\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8eLLM\u9a71\u52a8\u7684AHD\u7684\u65b0\u516c\u5f0f\u3002AHSD\u7684\u76ee\u7684\u662f\u81ea\u52a8\u751f\u6210\u4e00\u4e2a\u5c0f\u578b\u4e92\u8865\u542f\u53d1\u5f0f\u96c6\u5408\uff0c\u4ee5\u670d\u52a1\u4e8e\u5404\u79cd\u95ee\u9898\u5b9e\u4f8b\uff0c\u4ee5\u4fbf\u6bcf\u4e2a\u95ee\u9898\u5b9e\u4f8b\u90fd\u53ef\u4ee5\u901a\u8fc7\u6b64\u96c6\u5408\u4e2d\u7684\u81f3\u5c11\u4e00\u4e2a\u542f\u53d1\u5f0f\u65b9\u6cd5\u8fdb\u884c\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u542f\u53d1\u5f0f\u96c6\u5408\u6f14\u5316\uff08EoH-S\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5e94\u7528AHSD\u516c\u5f0f\u8fdb\u884cLLM\u9a71\u52a8\u7684AHD\u3002\u901a\u8fc7\u4e92\u8865\u79cd\u7fa4\u7ba1\u7406\u548c\u4e92\u8865\u611f\u77e5\u6a21\u56e0\u641c\u7d22\u4e24\u79cd\u65b0\u673a\u5236\uff0cEoH-S\u53ef\u4ee5\u6709\u6548\u5730\u751f\u6210\u4e00\u7ec4\u9ad8\u8d28\u91cf\u548c\u4e92\u8865\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "result": "\u8868\u660eAHSD\u7684\u76ee\u6807\u51fd\u6570\u662f\u5355\u8c03\u7684\u548c\u8d85\u6a21\u7684\u3002", "conclusion": "EoH-S\u5728\u5404\u79cd\u89c4\u6a21\u548c\u5206\u5e03\u7684\u4e09\u4e2aAHD\u4efb\u52a1\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684AHD\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u8fbe60\uff05\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2508.03017", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03017", "abs": "https://arxiv.org/abs/2508.03017", "authors": ["Liheng Zhang", "Weihao Yu", "Zubo Lu", "Haozhi Gu", "Jin Huang"], "title": "SA-3DGS: A Self-Adaptive Compression Method for 3D Gaussian Splatting", "comment": "9 pages, 7 figures. Under review at AAAI 2026", "summary": "Recent advancements in 3D Gaussian Splatting have enhanced efficient and\nhigh-quality novel view synthesis. However, representing scenes requires a\nlarge number of Gaussian points, leading to high storage demands and limiting\npractical deployment. The latest methods facilitate the compression of Gaussian\nmodels but struggle to identify truly insignificant Gaussian points in the\nscene, leading to a decline in subsequent Gaussian pruning, compression\nquality, and rendering performance. To address this issue, we propose SA-3DGS,\na method that significantly reduces storage costs while maintaining rendering\nquality. SA-3DGS learns an importance score to automatically identify the least\nsignificant Gaussians in scene reconstruction, thereby enabling effective\npruning and redundancy reduction. Next, the importance-aware clustering module\ncompresses Gaussians attributes more accurately into the codebook, improving\nthe codebook's expressive capability while reducing model size. Finally, the\ncodebook repair module leverages contextual scene information to repair the\ncodebook, thereby recovering the original Gaussian point attributes and\nmitigating the degradation in rendering quality caused by information loss.\nExperimental results on several benchmark datasets show that our method\nachieves up to 66x compression while maintaining or even improving rendering\nquality. The proposed Gaussian pruning approach is not only adaptable to but\nalso improves other pruning-based methods (e.g., LightGaussian), showcasing\nexcellent performance and strong generalization ability.", "AI": {"tldr": "SA-3DGS reduces storage costs while maintaining rendering quality via importance score learning, importance-aware clustering and codebook repair.", "motivation": "Representing scenes requires a large number of Gaussian points, leading to high storage demands and limiting practical deployment. The latest methods facilitate the compression of Gaussian models but struggle to identify truly insignificant Gaussian points in the scene, leading to a decline in subsequent Gaussian pruning, compression quality, and rendering performance.", "method": "SA-3DGS, which learns an importance score to automatically identify the least significant Gaussians in scene reconstruction, importance-aware clustering module compresses Gaussians attributes more accurately into the codebook, and codebook repair module leverages contextual scene information to repair the codebook.", "result": "achieves up to 66x compression while maintaining or even improving rendering quality. The proposed Gaussian pruning approach is not only adaptable to but also improves other pruning-based methods.", "conclusion": "The method achieves up to 66x compression while maintaining or even improving rendering quality and improves other pruning-based methods."}}
{"id": "2508.03262", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03262", "abs": "https://arxiv.org/abs/2508.03262", "authors": ["Junhyuk Choi", "Hyeonchu Park", "Haemin Lee", "Hyebeen Shin", "Hyun Joung Jin", "Bugeun Kim"], "title": "Pay What LLM Wants: Can LLM Simulate Economics Experiment with 522 Real-human Persona?", "comment": "Preprint", "summary": "Recent advances in Large Language Models (LLMs) have generated significant\ninterest in their capacity to simulate human-like behaviors, yet most studies\nrely on fictional personas rather than actual human data. We address this\nlimitation by evaluating LLMs' ability to predict individual economic\ndecision-making using Pay-What-You-Want (PWYW) pricing experiments with real\n522 human personas. Our study systematically compares three state-of-the-art\nmultimodal LLMs using detailed persona information from 522 Korean participants\nin cultural consumption scenarios. We investigate whether LLMs can accurately\nreplicate individual human choices and how persona injection methods affect\nprediction performance. Results reveal that while LLMs struggle with precise\nindividual-level predictions, they demonstrate reasonable group-level\nbehavioral tendencies. Also, we found that commonly adopted prompting\ntechniques are not much better than naive prompting methods; reconstruction of\npersonal narrative nor retrieval augmented generation have no significant gain\nagainst simple prompting method. We believe that these findings can provide the\nfirst comprehensive evaluation of LLMs' capabilities on simulating economic\nbehavior using real human data, offering empirical guidance for persona-based\nsimulation in computational social science.", "AI": {"tldr": "LLMs\u5728\u6a21\u62df\u4e2a\u4f53\u7ecf\u6d4e\u884c\u4e3a\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f46\u53ef\u4ee5\u6a21\u62df\u7fa4\u4f53\u884c\u4e3a\u3002", "motivation": "\u5927\u591a\u6570\u7814\u7a76\u4f9d\u8d56\u4e8e\u865a\u6784\u7684\u4eba\u7269\u89d2\u8272\u800c\u4e0d\u662f\u5b9e\u9645\u7684\u4eba\u7c7b\u6570\u636e\u3002\u6211\u4eec\u901a\u8fc7\u8bc4\u4f30LLM\u4f7f\u7528\u771f\u5b9e522\u4e2a\u4eba\u7269\u89d2\u8272\u7684Pay-What-You-Want (PWYW)\u5b9a\u4ef7\u5b9e\u9a8c\u6765\u9884\u6d4b\u4e2a\u4f53\u7ecf\u6d4e\u51b3\u7b56\u7684\u80fd\u529b\uff0c\u4ece\u800c\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u6765\u81ea522\u540d\u97e9\u56fd\u53c2\u4e0e\u8005\u5728\u6587\u5316\u6d88\u8d39\u573a\u666f\u4e2d\u7684\u8be6\u7ec6\u4eba\u7269\u4fe1\u606f\uff0c\u7cfb\u7edf\u5730\u6bd4\u8f83\u4e86\u4e09\u79cd\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001LLM\u3002", "result": "LLMs\u5728\u7cbe\u786e\u7684\u4e2a\u4f53\u5c42\u9762\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5728\u5408\u7406\u7684\u7fa4\u4f53\u5c42\u9762\u884c\u4e3a\u503e\u5411\u65b9\u9762\u8868\u73b0\u51fa\u4e00\u5b9a\u7684\u80fd\u529b\u3002\u5e38\u7528\u7684prompting\u6280\u672f\u5e76\u4e0d\u6bd4\u7b80\u5355\u7684prompting\u65b9\u6cd5\u597d\uff1b\u4e2a\u4eba\u53d9\u4e8b\u7684\u91cd\u5efa\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5bf9\u7b80\u5355\u7684prompting\u65b9\u6cd5\u6ca1\u6709\u660e\u663e\u7684\u63d0\u5347\u3002", "conclusion": "LLMs\u5728\u7cbe\u786e\u7684\u4e2a\u4f53\u5c42\u9762\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5728\u5408\u7406\u7684\u7fa4\u4f53\u5c42\u9762\u884c\u4e3a\u503e\u5411\u65b9\u9762\u8868\u73b0\u51fa\u4e00\u5b9a\u7684\u80fd\u529b\u3002\u5e38\u7528\u7684prompting\u6280\u672f\u5e76\u4e0d\u6bd4\u7b80\u5355\u7684prompting\u65b9\u6cd5\u597d\uff1b\u4e2a\u4eba\u53d9\u4e8b\u7684\u91cd\u5efa\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5bf9\u7b80\u5355\u7684prompting\u65b9\u6cd5\u6ca1\u6709\u660e\u663e\u7684\u63d0\u5347\u3002"}}
{"id": "2508.02909", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02909", "abs": "https://arxiv.org/abs/2508.02909", "authors": ["Aakash Gore", "Prasanna Chaporkar"], "title": "Clus-UCB: A Near-Optimal Algorithm for Clustered Bandits", "comment": null, "summary": "We study a stochastic multi-armed bandit setting where arms are partitioned\ninto known clusters, such that the mean rewards of arms within a cluster differ\nby at most a known threshold. While the clustering structure is known a priori,\nthe arm means are unknown. This framework models scenarios where outcomes\ndepend on multiple factors -- some with significant and others with minor\ninfluence -- such as online advertising, clinical trials, and wireless\ncommunication. We derive asymptotic lower bounds on the regret that improve\nupon the classical bound of Lai & Robbins (1985). We then propose Clus-UCB, an\nefficient algorithm that closely matches this lower bound asymptotically.\nClus-UCB is designed to exploit the clustering structure and introduces a new\nindex to evaluate an arm, which depends on other arms within the cluster. In\nthis way, arms share information among each other. We present simulation\nresults of our algorithm and compare its performance against KL-UCB and other\nwell-known algorithms for bandits with dependent arms. Finally, we address some\nlimitations of this work and conclude by mentioning possible future research.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u4e2a\u5177\u6709\u96c6\u7fa4\u7ed3\u6784\u7684\u968f\u673a multi-armed bandit setting\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Clus-UCB \u7684\u9ad8\u6548\u7b97\u6cd5\uff0c\u5e76\u5bfc\u51fa\u4e86\u5173\u4e8e\u9057\u61be\u7684\u6e10\u8fd1\u4e0b\u754c\u3002", "motivation": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u4e2a\u968f\u673a multi-armed bandit setting\uff0c\u5176\u4e2d arms \u88ab\u5212\u5206\u4e3a\u5df2\u77e5\u7684\u96c6\u7fa4\uff0c\u56e0\u6b64\u4e00\u4e2a\u96c6\u7fa4\u5185\u7684 arm \u7684\u5e73\u5747\u5956\u52b1\u6700\u591a\u76f8\u5dee\u4e00\u4e2a\u5df2\u77e5\u7684\u9608\u503c\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Clus-UCB \u7684\u9ad8\u6548\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u65e8\u5728\u5229\u7528\u805a\u7c7b\u7ed3\u6784\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u6307\u6807\u6765\u8bc4\u4f30\u4e00\u4e2a arm\uff0c\u8be5\u6307\u6807\u53d6\u51b3\u4e8e\u96c6\u7fa4\u4e2d\u7684\u5176\u4ed6 arm\u3002", "result": "\u63a8\u5bfc\u4e86\u5173\u4e8e\u9057\u61be\u7684\u6e10\u8fd1\u4e0b\u754c\uff0c\u8be5\u4e0b\u754c\u6539\u8fdb\u4e86 Lai & Robbins (1985) \u7684\u7ecf\u5178\u754c\u9650\u3002Clus-UCB \u7b97\u6cd5\u5728\u6e10\u8fd1\u7ebf\u4e0a\u4e0e\u8fd9\u4e2a\u4e0b\u754c\u975e\u5e38\u543b\u5408\u3002\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u7684\u6027\u80fd\u4f18\u4e8e KL-UCB \u548c\u5176\u4ed6\u4f17\u6240\u5468\u77e5\u7684 dependent arms bandits \u7b97\u6cd5\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u603b\u7ed3\u4e86\u7814\u7a76\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u53ef\u80fd\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.03083", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03083", "abs": "https://arxiv.org/abs/2508.03083", "authors": ["Youran Zhou", "Mohamed Reda Bouadjenek", "Sunil Aryal"], "title": "MissDDIM: Deterministic and Efficient Conditional Diffusion for Tabular Data Imputation", "comment": null, "summary": "Diffusion models have recently emerged as powerful tools for missing data\nimputation by modeling the joint distribution of observed and unobserved\nvariables. However, existing methods, typically based on stochastic denoising\ndiffusion probabilistic models (DDPMs), suffer from high inference latency and\nvariable outputs, limiting their applicability in real-world tabular settings.\nTo address these deficiencies, we present in this paper MissDDIM, a conditional\ndiffusion framework that adapts Denoising Diffusion Implicit Models (DDIM) for\ntabular imputation. While stochastic sampling enables diverse completions, it\nalso introduces output variability that complicates downstream processing.", "AI": {"tldr": "MissDDIM \u4f7f\u7528\u6761\u4ef6\u6269\u6563\u6846\u67b6\u548c DDIM \u6765\u8fdb\u884c\u8868\u683c\u63d2\u8865\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u9ad8\u5ef6\u8fdf\u548c\u53ef\u53d8\u8f93\u51fa\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u968f\u673a\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b (DDPM)\uff0c\u5b58\u5728\u9ad8\u63a8\u7406\u5ef6\u8fdf\u548c\u53ef\u53d8\u8f93\u51fa\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u771f\u5b9e\u8868\u683c\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u8c03\u6574\u4e86\u7528\u4e8e\u8868\u683c\u63d2\u8865\u7684\u53bb\u566a\u6269\u6563\u9690\u5f0f\u6a21\u578b (DDIM)", "result": "\u6761\u4ef6\u6269\u6563\u6846\u67b6", "conclusion": "MissDDIM\uff0c\u4e00\u4e2a\u6761\u4ef6\u6269\u6563\u6846\u67b6\uff0c\u5b83\u8c03\u6574\u4e86\u7528\u4e8e\u8868\u683c\u63d2\u8865\u7684\u53bb\u566a\u6269\u6563\u9690\u5f0f\u6a21\u578b (DDIM)\u3002"}}
{"id": "2508.03034", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03034", "abs": "https://arxiv.org/abs/2508.03034", "authors": ["Qi Xie", "Yongjia Ma", "Donglin Di", "Xuehao Gao", "Xun Yang"], "title": "MoCA: Identity-Preserving Text-to-Video Generation via Mixture of Cross Attention", "comment": null, "summary": "Achieving ID-preserving text-to-video (T2V) generation remains challenging\ndespite recent advances in diffusion-based models. Existing approaches often\nfail to capture fine-grained facial dynamics or maintain temporal identity\ncoherence. To address these limitations, we propose MoCA, a novel Video\nDiffusion Model built on a Diffusion Transformer (DiT) backbone, incorporating\na Mixture of Cross-Attention mechanism inspired by the Mixture-of-Experts\nparadigm. Our framework improves inter-frame identity consistency by embedding\nMoCA layers into each DiT block, where Hierarchical Temporal Pooling captures\nidentity features over varying timescales, and Temporal-Aware Cross-Attention\nExperts dynamically model spatiotemporal relationships. We further incorporate\na Latent Video Perceptual Loss to enhance identity coherence and fine-grained\ndetails across video frames. To train this model, we collect CelebIPVid, a\ndataset of 10,000 high-resolution videos from 1,000 diverse individuals,\npromoting cross-ethnicity generalization. Extensive experiments on CelebIPVid\nshow that MoCA outperforms existing T2V methods by over 5% across Face\nsimilarity.", "AI": {"tldr": "MoCA, a novel Video Diffusion Model, outperforms existing T2V methods in maintaining identity and capturing fine-grained details, as demonstrated on the CelebIPVid dataset.", "motivation": "Achieving ID-preserving text-to-video (T2V) generation remains challenging despite recent advances in diffusion-based models. Existing approaches often fail to capture fine-grained facial dynamics or maintain temporal identity coherence. To address these limitations", "method": "a novel Video Diffusion Model built on a Diffusion Transformer (DiT) backbone, incorporating a Mixture of Cross-Attention mechanism inspired by the Mixture-of-Experts paradigm. Our framework improves inter-frame identity consistency by embedding MoCA layers into each DiT block, where Hierarchical Temporal Pooling captures identity features over varying timescales, and Temporal-Aware Cross-Attention Experts dynamically model spatiotemporal relationships. We further incorporate a Latent Video Perceptual Loss to enhance identity coherence and fine-grained details across video frames.", "result": "Extensive experiments on CelebIPVid show that MoCA outperforms existing T2V methods by over 5% across Face similarity.", "conclusion": "MoCA outperforms existing T2V methods by over 5% across Face similarity."}}
{"id": "2508.03275", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03275", "abs": "https://arxiv.org/abs/2508.03275", "authors": ["Jiahao Zhao"], "title": "LECTOR: LLM-Enhanced Concept-based Test-Oriented Repetition for Adaptive Spaced Learning", "comment": "15 pages, 4 figures, 1 table", "summary": "Spaced repetition systems are fundamental to efficient learning and memory\nretention, but existing algorithms often struggle with semantic interference\nand personalized adaptation. We present LECTOR (\\textbf{L}LM-\\textbf{E}nhanced\n\\textbf{C}oncept-based \\textbf{T}est-\\textbf{O}riented \\textbf{R}epetition), a\nnovel adaptive scheduling algorithm specifically designed for test-oriented\nlearning scenarios, particularly language examinations where success rate is\nparamount. LECTOR leverages large language models for semantic analysis while\nincorporating personalized learning profiles, addressing the critical challenge\nof semantic confusion in vocabulary learning by utilizing LLM-powered semantic\nsimilarity assessment and integrating it with established spaced repetition\nprinciples. Our comprehensive evaluation against six baseline algorithms\n(SSP-MMC, SM2, HLR, FSRS, ANKI, THRESHOLD) across 100 simulated learners over\n100 days demonstrates significant improvements: LECTOR achieves a 90.2\\%\nsuccess rate compared to 88.4\\% for the best baseline (SSP-MMC), representing a\n2.0\\% relative improvement. The algorithm shows particular strength in handling\nsemantically similar concepts, reducing confusion-induced errors while\nmaintaining computational efficiency. Our results establish LECTOR as a\npromising direction for intelligent tutoring systems and adaptive learning\nplatforms.", "AI": {"tldr": "LECTOR is a novel adaptive scheduling algorithm that uses LLMs to improve learning and memory retention.", "motivation": "Existing spaced repetition algorithms often struggle with semantic interference and personalized adaptation.", "method": "LECTOR leverages large language models for semantic analysis while incorporating personalized learning profiles, addressing the critical challenge of semantic confusion in vocabulary learning by utilizing LLM-powered semantic similarity assessment and integrating it with established spaced repetition principles.", "result": "LECTOR achieves a 90.2% success rate compared to 88.4% for the best baseline (SSP-MMC), representing a 2.0% relative improvement. The algorithm shows particular strength in handling semantically similar concepts, reducing confusion-induced errors while maintaining computational efficiency.", "conclusion": "The results establish LECTOR as a promising direction for intelligent tutoring systems and adaptive learning platforms."}}
{"id": "2508.02911", "categories": ["cs.LG", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.02911", "abs": "https://arxiv.org/abs/2508.02911", "authors": ["Zhong Zhang", "Francesco Topputo"], "title": "Neural Approximators for Low-Thrust Trajectory Transfer Cost and Reachability", "comment": null, "summary": "In trajectory design, fuel consumption and trajectory reachability are two\nkey performance indicators for low-thrust missions. This paper proposes\ngeneral-purpose pretrained neural networks to predict these metrics. The\ncontributions of this paper are as follows: Firstly, based on the confirmation\nof the Scaling Law applicable to low-thrust trajectory approximation, the\nlargest dataset is constructed using the proposed homotopy ray method, which\naligns with mission-design-oriented data requirements. Secondly, the data are\ntransformed into a self-similar space, enabling the neural network to adapt to\narbitrary semi-major axes, inclinations, and central bodies. This extends the\napplicability beyond existing studies and can generalize across diverse mission\nscenarios without retraining. Thirdly, to the best of our knowledge, this work\npresents the current most general and accurate low-thrust trajectory\napproximator, with implementations available in C++, Python, and MATLAB. The\nresulting neural network achieves a relative error of 0.78% in predicting\nvelocity increments and 0.63% in minimum transfer time estimation. The models\nhave also been validated on a third-party dataset, multi-flyby mission design\nproblem, and mission analysis scenario, demonstrating their generalization\ncapability, predictive accuracy, and computational efficiency.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u9884\u6d4b\u4f4e\u63a8\u529b\u4efb\u52a1\u7684\u71c3\u6599\u6d88\u8017\u548c\u8f68\u8ff9\u53ef\u8fbe\u6027\uff0c\u5e76\u5728\u5404\u79cd\u4efb\u52a1\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3001\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u5728\u8f68\u9053\u8bbe\u8ba1\u4e2d\uff0c\u71c3\u6599\u6d88\u8017\u548c\u8f68\u8ff9\u53ef\u8fbe\u6027\u662f\u4f4e\u63a8\u529b\u4efb\u52a1\u7684\u4e24\u4e2a\u5173\u952e\u6027\u80fd\u6307\u6807\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6765\u9884\u6d4b\u4f4e\u63a8\u529b\u4efb\u52a1\u7684\u71c3\u6599\u6d88\u8017\u548c\u8f68\u8ff9\u53ef\u8fbe\u6027\u6307\u6807\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u9002\u7528\u4e8e\u4f4e\u63a8\u529b\u8f68\u8ff9\u903c\u8fd1\u7684\u6807\u5ea6\u5f8b\uff0c\u5e76\u4f7f\u7528\u540c\u4f26\u5c04\u7ebf\u6cd5\u6784\u5efa\u4e86\u6700\u5927\u7684\u6570\u636e\u96c6\u3002\u6570\u636e\u88ab\u8f6c\u6362\u6210\u81ea\u76f8\u4f3c\u7a7a\u95f4\uff0c\u4f7f\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u9002\u5e94\u4efb\u610f\u7684\u534a\u957f\u8f74\u3001\u503e\u89d2\u548c\u4e2d\u5fc3\u4f53\u3002", "result": "\u8be5\u795e\u7ecf\u7f51\u7edc\u5728\u9884\u6d4b\u901f\u5ea6\u589e\u91cf\u65b9\u9762\u7684\u76f8\u5bf9\u8bef\u5dee\u4e3a0.78%\uff0c\u5728\u6700\u5c0f\u4f20\u8f93\u65f6\u95f4\u4f30\u8ba1\u65b9\u9762\u7684\u76f8\u5bf9\u8bef\u5dee\u4e3a0.63%\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u76ee\u524d\u6700\u901a\u7528\u3001\u6700\u51c6\u786e\u7684\u4f4e\u63a8\u529b\u8f68\u9053\u903c\u8fd1\u5668\uff0c\u5176\u9884\u6d4b\u901f\u5ea6\u589e\u91cf\u7684\u76f8\u5bf9\u8bef\u5dee\u4e3a0.78%\uff0c\u6700\u5c0f\u4f20\u8f93\u65f6\u95f4\u4f30\u8ba1\u7684\u76f8\u5bf9\u8bef\u5dee\u4e3a0.63%\u3002\u8be5\u6a21\u578b\u5df2\u5728\u7b2c\u4e09\u65b9\u6570\u636e\u96c6\u3001\u591a\u98de\u884c\u4efb\u52a1\u8bbe\u8ba1\u95ee\u9898\u548c\u4efb\u52a1\u5206\u6790\u573a\u666f\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2508.03091", "categories": ["cs.AI", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03091", "abs": "https://arxiv.org/abs/2508.03091", "authors": ["Xingjun Ma", "Hanxun Huang", "Tianwei Song", "Ye Sun", "Yifeng Gao", "Yu-Gang Jiang"], "title": "T2UE: Generating Unlearnable Examples from Text Descriptions", "comment": "To appear in ACM MM 2025", "summary": "Large-scale pre-training frameworks like CLIP have revolutionized multimodal\nlearning, but their reliance on web-scraped datasets, frequently containing\nprivate user data, raises serious concerns about misuse. Unlearnable Examples\n(UEs) have emerged as a promising countermeasure against unauthorized model\ntraining, employing carefully crafted unlearnable noise to disrupt the learning\nof meaningful representations from protected data. Current approaches typically\ngenerate UEs by jointly optimizing unlearnable noise for both images and their\nassociated text descriptions (or labels). However, this optimization process is\noften computationally prohibitive for on-device execution, forcing reliance on\nexternal third-party services. This creates a fundamental privacy paradox:\nusers must initially expose their data to these very services to achieve\nprotection, thereby compromising privacy in the process. Such a contradiction\nhas severely hindered the development of practical, scalable data protection\nsolutions. To resolve this paradox, we introduce \\textbf{Text-to-Unlearnable\nExample (T2UE)}, a novel framework that enables users to generate UEs using\nonly text descriptions. T2UE circumvents the need for original image data by\nemploying a text-to-image (T2I) model to map text descriptions into the image\n(noise) space, combined with an error-minimization framework to produce\neffective unlearnable noise. Extensive experiments show that T2UE-protected\ndata substantially degrades performance in downstream tasks (e.g., cross-modal\nretrieval) for state-of-the-art models. Notably, the protective effect\ngeneralizes across diverse architectures and even to supervised learning\nsettings. Our work demonstrates the feasibility of \"zero-contact data\nprotection\", where personal data can be safeguarded based solely on their\ntextual descriptions, eliminating the need for direct data exposure.", "AI": {"tldr": "The paper introduces T2UE, a framework for generating unlearnable examples from text descriptions to protect data privacy without direct data exposure, addressing the privacy paradox in existing methods.", "motivation": "Large-scale pre-training frameworks like CLIP rely on web-scraped datasets, frequently containing private user data, which raises concerns about misuse. Current Unlearnable Examples (UEs) generation is computationally prohibitive for on-device execution, forcing reliance on external third-party services, creating a privacy paradox.", "method": "The paper introduces Text-to-Unlearnable Example (T2UE), a novel framework that enables users to generate UEs using only text descriptions. It employs a text-to-image (T2I) model to map text descriptions into the image (noise) space, combined with an error-minimization framework to produce effective unlearnable noise.", "result": "T2UE-protected data substantially degrades performance in downstream tasks for state-of-the-art models, and the protective effect generalizes across diverse architectures and even to supervised learning settings.", "conclusion": "The paper demonstrates the feasibility of zero-contact data protection, where personal data can be safeguarded based solely on their textual descriptions, eliminating the need for direct data exposure."}}
{"id": "2508.03039", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.03039", "abs": "https://arxiv.org/abs/2508.03039", "authors": ["Yiran Meng", "Junhong Ye", "Wei Zhou", "Guanghui Yue", "Xudong Mao", "Ruomei Wang", "Baoquan Zhao"], "title": "VideoForest: Person-Anchored Hierarchical Reasoning for Cross-Video Question Answering", "comment": null, "summary": "Cross-video question answering presents significant challenges beyond\ntraditional single-video understanding, particularly in establishing meaningful\nconnections across video streams and managing the complexity of multi-source\ninformation retrieval. We introduce VideoForest, a novel framework that\naddresses these challenges through person-anchored hierarchical reasoning. Our\napproach leverages person-level features as natural bridge points between\nvideos, enabling effective cross-video understanding without requiring\nend-to-end training. VideoForest integrates three key innovations: 1) a\nhuman-anchored feature extraction mechanism that employs ReID and tracking\nalgorithms to establish robust spatiotemporal relationships across multiple\nvideo sources; 2) a multi-granularity spanning tree structure that\nhierarchically organizes visual content around person-level trajectories; and\n3) a multi-agent reasoning framework that efficiently traverses this\nhierarchical structure to answer complex cross-video queries. To evaluate our\napproach, we develop CrossVideoQA, a comprehensive benchmark dataset\nspecifically designed for person-centric cross-video analysis. Experimental\nresults demonstrate VideoForest's superior performance in cross-video reasoning\ntasks, achieving 71.93% accuracy in person recognition, 83.75% in behavior\nanalysis, and 51.67% in summarization and reasoning, significantly\noutperforming existing methods. Our work establishes a new paradigm for\ncross-video understanding by unifying multiple video streams through\nperson-level features, enabling sophisticated reasoning across distributed\nvisual information while maintaining computational efficiency.", "AI": {"tldr": "VideoForest \u901a\u8fc7\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u5c42\u7ea7\u63a8\u7406\uff0c\u89e3\u51b3\u4e86\u8de8\u89c6\u9891\u95ee\u7b54\u4e2d\u7684\u6311\u6218\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8de8\u89c6\u9891\u95ee\u7b54\u63d0\u51fa\u4e86\u8d85\u8d8a\u4f20\u7edf\u5355\u89c6\u9891\u7406\u89e3\u7684\u91cd\u5927\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5efa\u7acb\u8de8\u89c6\u9891\u6d41\u7684\u6709\u610f\u4e49\u8fde\u63a5\u548c\u7ba1\u7406\u591a\u6e90\u4fe1\u606f\u68c0\u7d22\u7684\u590d\u6742\u6027\u65b9\u9762\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528 ReID \u548c\u8ddf\u8e2a\u7b97\u6cd5\u5efa\u7acb\u9c81\u68d2\u7684\u65f6\u7a7a\u5173\u7cfb\uff0c\u5e76\u91c7\u7528\u591a\u7c92\u5ea6\u751f\u6210\u6811\u7ed3\u6784\u548c\u591a\u4ee3\u7406\u63a8\u7406\u6846\u67b6\u3002", "result": "VideoForest \u5728\u8de8\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5728\u4eba\u7269\u8bc6\u522b\u65b9\u9762\u8fbe\u5230 71.93% \u7684\u51c6\u786e\u7387\uff0c\u5728\u884c\u4e3a\u5206\u6790\u65b9\u9762\u8fbe\u5230 83.75% \u7684\u51c6\u786e\u7387\uff0c\u5728\u603b\u7ed3\u548c\u63a8\u7406\u65b9\u9762\u8fbe\u5230 51.67% \u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "VideoForest\u5728\u8de8\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u901a\u8fc7\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u7279\u5f81\u7edf\u4e00\u591a\u4e2a\u89c6\u9891\u6d41\uff0c\u5b9e\u73b0\u4e86\u5728\u5206\u5e03\u5f0f\u89c6\u89c9\u4fe1\u606f\u4e2d\u7684\u590d\u6742\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u8de8\u89c6\u9891\u7406\u89e3\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u8303\u4f8b\u3002"}}
{"id": "2508.03276", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03276", "abs": "https://arxiv.org/abs/2508.03276", "authors": ["Terra Blevins", "Susanne Schmalwieser", "Benjamin Roth"], "title": "Do language models accommodate their users? A study of linguistic convergence", "comment": null, "summary": "While large language models (LLMs) are generally considered proficient in\ngenerating language, how similar their language usage is to that of humans\nremains understudied. In this paper, we test whether models exhibit linguistic\nconvergence, a core pragmatic element of human language communication, asking:\ndo models adapt, or converge, to the linguistic patterns of their user? To\nanswer this, we systematically compare model completions of exisiting dialogues\nto the original human responses across sixteen language models, three dialogue\ncorpora, and a variety of stylometric features. We find that models strongly\nconverge to the conversation's style, often significantly overfitting relative\nto the human baseline. While convergence patterns are often feature-specific,\nwe observe consistent shifts in convergence across modeling settings, with\ninstruction-tuned and larger models converging less than their pretrained\ncounterparts. Given the differences between human and model convergence\npatterns, we hypothesize that the underlying mechanisms for these behaviors are\nvery different.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u4f1a\u50cf\u4eba\u7c7b\u4e00\u6837\u8d8b\u540c\u4e8e\u7528\u6237\u7684\u8bed\u8a00\u6a21\u5f0f\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6a21\u578b\u4f1a\u5f3a\u70c8\u8d8b\u540c\u4e8e\u5bf9\u8bdd\u98ce\u683c\uff0c\u4f46\u4e0e\u4eba\u7c7b\u7684\u8d8b\u540c\u65b9\u5f0f\u4e0d\u540c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u901a\u5e38\u88ab\u8ba4\u4e3a\u662f\u7cbe\u901a\u751f\u6210\u8bed\u8a00\u7684\uff0c\u4f46\u5b83\u4eec\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u4e0e\u4eba\u7c7b\u7684\u8bed\u8a00\u4f7f\u7528\u76f8\u4f3c\uff0c\u8fd9\u4e00\u70b9\u4ecd\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u6d4b\u8bd5\u6a21\u578b\u662f\u5426\u8868\u73b0\u51fa\u8bed\u8a00\u8d8b\u540c\uff0c\u8fd9\u662f\u4eba\u7c7b\u8bed\u8a00\u4ea4\u6d41\u7684\u6838\u5fc3\u8bed\u7528\u8981\u7d20\uff0c\u8be2\u95ee\uff1a\u6a21\u578b\u662f\u5426\u9002\u5e94\u6216\u8d8b\u540c\u4e8e\u5176\u7528\u6237\u7684\u8bed\u8a00\u6a21\u5f0f\uff1f", "method": "\u7cfb\u7edf\u5730\u6bd4\u8f83\u4e86\u73b0\u6709\u5bf9\u8bdd\u7684\u6a21\u578b\u8865\u5168\u4e0e\u539f\u59cb\u4eba\u7c7b\u54cd\u5e94\u572816\u4e2a\u8bed\u8a00\u6a21\u578b\u30013\u4e2a\u5bf9\u8bdd\u8bed\u6599\u5e93\u548c\u5404\u79cd\u6587\u4f53\u7279\u5f81\u4e0a\u7684\u5dee\u5f02\u3002", "result": "\u6a21\u578b\u5f3a\u70c8\u8d8b\u540c\u4e8e\u5bf9\u8bdd\u7684\u98ce\u683c\uff0c\u76f8\u5bf9\u4e8e\u4eba\u7c7b\u57fa\u7ebf\u901a\u5e38\u8fc7\u5ea6\u62df\u5408\u3002\u867d\u7136\u6536\u655b\u6a21\u5f0f\u901a\u5e38\u662f\u7279\u5b9a\u4e8e\u7279\u5f81\u7684\uff0c\u4f46\u6211\u4eec\u89c2\u5bdf\u5230\u8de8\u5efa\u6a21\u8bbe\u7f6e\u7684\u6536\u655b\u7684\u6301\u7eed\u53d8\u5316\uff0c\u6307\u4ee4\u8c03\u6574\u548c\u66f4\u5927\u7684\u6a21\u578b\u6bd4\u5b83\u4eec\u9884\u8bad\u7ec3\u7684\u5bf9\u5e94\u6a21\u578b\u6536\u655b\u5f97\u66f4\u5c11\u3002", "conclusion": "\u6a21\u578b\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u8d8b\u540c\u4e8e\u5bf9\u8bdd\u7684\u98ce\u683c\uff0c\u76f8\u5bf9\u4e8e\u4eba\u7c7b\u57fa\u7ebf\u901a\u5e38\u8fc7\u5ea6\u62df\u5408\u3002\u6307\u4ee4\u8c03\u6574\u548c\u66f4\u5927\u7684\u6a21\u578b\u6bd4\u5b83\u4eec\u9884\u8bad\u7ec3\u7684\u5bf9\u5e94\u6a21\u578b\u6536\u655b\u5f97\u66f4\u5c11\u3002\u4eba\u7c7b\u548c\u6a21\u578b\u6536\u655b\u6a21\u5f0f\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u6211\u4eec\u5047\u8bbe\u8fd9\u4e9b\u884c\u4e3a\u7684\u6f5c\u5728\u673a\u5236\u975e\u5e38\u4e0d\u540c\u3002"}}
{"id": "2508.02924", "categories": ["cs.LG", "stat.ML", "68T07, 68Q32", "I.2.6; I.5.1; F.1.1"], "pdf": "https://arxiv.org/pdf/2508.02924", "abs": "https://arxiv.org/abs/2508.02924", "authors": ["Biyi Fang", "Jean Utke", "Truong Vo", "Diego Klabjan"], "title": "BoostTransformer: Enhancing Transformer Models with Subgrid Selection and Importance Sampling", "comment": "10 pages, 5 figures, submitted for review at a major machine learning\n  conference. arXiv admin note: substantial text overlap with arXiv:2203.00761,\n  arXiv:2507.22842", "summary": "Transformer architectures dominate modern NLP but often demand heavy\ncomputational resources and intricate hyperparameter tuning. To mitigate these\nchallenges, we propose a novel framework, BoostTransformer, that augments\ntransformers with boosting principles through subgrid token selection and\nimportance-weighted sampling. Our method incorporates a least square boosting\nobjective directly into the transformer pipeline, enabling more efficient\ntraining and improved performance. Across multiple fine-grained text\nclassification benchmarks, BoostTransformer demonstrates both faster\nconvergence and higher accuracy, surpassing standard transformers while\nminimizing architectural search overhead.", "AI": {"tldr": "BoostTransformer\u901a\u8fc7boosting\u539f\u5219\u589e\u5f3a\u4e86transformers\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u548c\u66f4\u9ad8\u7684\u6027\u80fd\u3002", "motivation": "Transformer\u67b6\u6784\u5728\u73b0\u4ee3NLP\u4e2d\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f46\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u590d\u6742\u7684\u8d85\u53c2\u6570\u8c03\u6574\u3002\u4e3a\u4e86\u7f13\u89e3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5b50\u7f51\u683c\u4ee4\u724c\u9009\u62e9\u548c\u91cd\u8981\u6027\u52a0\u6743\u91c7\u6837\uff0c\u4f7f\u7528boosting\u539f\u5219\u6765\u589e\u5f3atransformers\u3002", "result": "BoostTransformer\u5c55\u793a\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u3002", "conclusion": "BoostTransformer\u5728\u591a\u4e2a\u7ec6\u7c92\u5ea6\u6587\u672c\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u51c6\u786e\u7387\uff0c\u8d85\u8fc7\u4e86\u6807\u51c6transformers\uff0c\u540c\u65f6\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4e86\u67b6\u6784\u641c\u7d22\u5f00\u9500\u3002"}}
{"id": "2508.03092", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03092", "abs": "https://arxiv.org/abs/2508.03092", "authors": ["Zikun Cui", "Tianyi Huang", "Chia-En Chiang", "Cuiqianhe Du"], "title": "Toward Verifiable Misinformation Detection: A Multi-Tool LLM Agent Framework", "comment": null, "summary": "With the proliferation of Large Language Models (LLMs), the detection of\nmisinformation has become increasingly important and complex. This research\nproposes an innovative verifiable misinformation detection LLM agent that goes\nbeyond traditional true/false binary judgments. The agent actively verifies\nclaims through dynamic interaction with diverse web sources, assesses\ninformation source credibility, synthesizes evidence, and provides a complete\nverifiable reasoning process. Our designed agent architecture includes three\ncore tools: precise web search tool, source credibility assessment tool and\nnumerical claim verification tool. These tools enable the agent to execute\nmulti-step verification strategies, maintain evidence logs, and form\ncomprehensive assessment conclusions. We evaluate using standard misinformation\ndatasets such as FakeNewsNet, comparing with traditional machine learning\nmodels and LLMs. Evaluation metrics include standard classification metrics,\nquality assessment of reasoning processes, and robustness testing against\nrewritten content. Experimental results show that our agent outperforms\nbaseline methods in misinformation detection accuracy, reasoning transparency,\nand resistance to information rewriting, providing a new paradigm for\ntrustworthy AI-assisted fact-checking.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u9a8c\u8bc1\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4bLLM Agent\uff0c\u5b83\u901a\u8fc7\u4e0eWeb\u6765\u6e90\u4ea4\u4e92\u3001\u8bc4\u4f30\u4fe1\u606f\u6765\u6e90\u53ef\u4fe1\u5ea6\u5e76\u7efc\u5408\u8bc1\u636e\u6765\u68c0\u6d4b\u865a\u5047\u4fe1\u606f\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5Agent\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u666e\u53ca\uff0c\u865a\u5047\u4fe1\u606f\u7684\u68c0\u6d4b\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u548c\u590d\u6742\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u53ef\u9a8c\u8bc1\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4bLLM Agent\uff0c\u8be5Agent\u901a\u8fc7\u4e0e\u5404\u79cdWeb\u6765\u6e90\u7684\u52a8\u6001\u4ea4\u4e92\u6765\u4e3b\u52a8\u9a8c\u8bc1\u58f0\u660e\uff0c\u8bc4\u4f30\u4fe1\u606f\u6765\u6e90\u7684\u53ef\u4fe1\u5ea6\uff0c\u7efc\u5408\u8bc1\u636e\uff0c\u5e76\u63d0\u4f9b\u5b8c\u6574\u7684\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u8fc7\u7a0b\u3002\u8be5Agent\u67b6\u6784\u5305\u62ec\u4e09\u4e2a\u6838\u5fc3\u5de5\u5177\uff1a\u7cbe\u786e\u7684Web\u641c\u7d22\u5de5\u5177\u3001\u6765\u6e90\u53ef\u4fe1\u5ea6\u8bc4\u4f30\u5de5\u5177\u548c\u6570\u503c\u58f0\u660e\u9a8c\u8bc1\u5de5\u5177\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5Agent\u5728\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u51c6\u786e\u7387\u3001\u63a8\u7406\u900f\u660e\u5ea6\u548c\u6297\u4fe1\u606f\u6539\u5199\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684LLM Agent\u5728\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u51c6\u786e\u7387\u3001\u63a8\u7406\u900f\u660e\u5ea6\u548c\u6297\u4fe1\u606f\u6539\u5199\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e3a\u53ef\u4fe1\u7684AI\u8f85\u52a9\u4e8b\u5b9e\u6838\u67e5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u8303\u4f8b\u3002"}}
{"id": "2508.03050", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03050", "abs": "https://arxiv.org/abs/2508.03050", "authors": ["Zeyu Zhu", "Weijia Wu", "Mike Zheng Shou"], "title": "Multi-human Interactive Talking Dataset", "comment": "9 pages, 4 figures, 4 tables", "summary": "Existing studies on talking video generation have predominantly focused on\nsingle-person monologues or isolated facial animations, limiting their\napplicability to realistic multi-human interactions. To bridge this gap, we\nintroduce MIT, a large-scale dataset specifically designed for multi-human\ntalking video generation. To this end, we develop an automatic pipeline that\ncollects and annotates multi-person conversational videos. The resulting\ndataset comprises 12 hours of high-resolution footage, each featuring two to\nfour speakers, with fine-grained annotations of body poses and speech\ninteractions. It captures natural conversational dynamics in multi-speaker\nscenario, offering a rich resource for studying interactive visual behaviors.\nTo demonstrate the potential of MIT, we furthur propose CovOG, a baseline model\nfor this novel task. It integrates a Multi-Human Pose Encoder (MPE) to handle\nvarying numbers of speakers by aggregating individual pose embeddings, and an\nInteractive Audio Driver (IAD) to modulate head dynamics based on\nspeaker-specific audio features. Together, these components showcase the\nfeasibility and challenges of generating realistic multi-human talking videos,\nestablishing MIT as a valuable benchmark for future research. The code is\navalibale at: https://github.com/showlab/Multi-human-Talking-Video-Dataset.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86MIT\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u591a\u4eba\u5bf9\u8bdd\u89c6\u9891\u751f\u6210\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCovOG\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u5173\u4e8e\u8bf4\u8bdd\u89c6\u9891\u751f\u6210\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5355\u4eba\u72ec\u767d\u6216\u5b64\u7acb\u7684\u9762\u90e8\u52a8\u753b\u4e0a\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u771f\u5b9e\u7684\u591a\u4eba\u4e92\u52a8\u4e2d\u7684\u9002\u7528\u6027\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86MIT\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3a\u591a\u4eba\u8bf4\u8bdd\u89c6\u9891\u751f\u6210\u800c\u8bbe\u8ba1\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "method": "\u8be5\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u6d41\u7a0b\uff0c\u7528\u4e8e\u6536\u96c6\u548c\u6ce8\u91ca\u591a\u4eba\u5bf9\u8bdd\u89c6\u9891\u3002\u63d0\u51fa\u7684CovOG\u6a21\u578b\u96c6\u6210\u4e86\u591a\u4eba\u59ff\u52bf\u7f16\u7801\u5668\uff08MPE\uff09\u548c\u4ea4\u4e92\u5f0f\u97f3\u9891\u9a71\u52a8\u5668\uff08IAD\uff09\u3002", "result": "\u8be5\u8bba\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b12\u5c0f\u65f6\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u7684\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u89c6\u9891\u5305\u542b2\u52304\u4e2a\u8bf4\u8bdd\u8005\uff0c\u5e76\u5bf9\u8eab\u4f53\u59ff\u52bf\u548c\u8bed\u97f3\u4e92\u52a8\u8fdb\u884c\u4e86\u7ec6\u7c92\u5ea6\u7684\u6ce8\u91ca\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCovOG\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u903c\u771f\u7684\u591a\u4eba\u5bf9\u8bdd\u89c6\u9891\uff0c\u5e76\u5c55\u793a\u4e86MIT\u6570\u636e\u96c6\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u7684\u5b9d\u8d35\u57fa\u51c6\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.03292", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03292", "abs": "https://arxiv.org/abs/2508.03292", "authors": ["Shahed Masoudian", "Gustavo Escobedo", "Hannah Strauss", "Markus Schedl"], "title": "Investigating Gender Bias in LLM-Generated Stories via Psychological Stereotypes", "comment": "Under Review", "summary": "As Large Language Models (LLMs) are increasingly used across different\napplications, concerns about their potential to amplify gender biases in\nvarious tasks are rising. Prior research has often probed gender bias using\nexplicit gender cues as counterfactual, or studied them in sentence completion\nand short question answering tasks. These formats might overlook more implicit\nforms of bias embedded in generative behavior of longer content. In this work,\nwe investigate gender bias in LLMs using gender stereotypes studied in\npsychology (e.g., aggressiveness or gossiping) in an open-ended task of\nnarrative generation. We introduce a novel dataset called StereoBias-Stories\ncontaining short stories either unconditioned or conditioned on (one, two, or\nsix) random attributes from 25 psychological stereotypes and three task-related\nstory endings. We analyze how the gender contribution in the overall story\nchanges in response to these attributes and present three key findings: (1)\nWhile models, on average, are highly biased towards male in unconditioned\nprompts, conditioning on attributes independent from gender stereotypes\nmitigates this bias. (2) Combining multiple attributes associated with the same\ngender stereotype intensifies model behavior, with male ones amplifying bias\nand female ones alleviating it. (3) Model biases align with psychological\nground-truth used for categorization, and alignment strength increases with\nmodel size. Together, these insights highlight the importance of\npsychology-grounded evaluation of LLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5fc3\u7406\u5b66\u89d2\u5ea6\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u53d1\u73b0\u6a21\u578b\u5b58\u5728\u6027\u522b\u504f\u89c1\uff0c\u4e14\u4e0e\u5fc3\u7406\u5b66\u523b\u677f\u5370\u8c61\u4e00\u81f4\uff0c\u6a21\u578b\u8d8a\u5927\u504f\u89c1\u8d8a\u660e\u663e\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8d8a\u6765\u8d8a\u591a\u5730\u5e94\u7528\u4e8e\u4e0d\u540c\u7684\u5e94\u7528\uff0c\u4eba\u4eec\u8d8a\u6765\u8d8a\u5173\u6ce8\u5b83\u4eec\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u653e\u5927\u6027\u522b\u504f\u89c1\u7684\u53ef\u80fd\u6027\u3002\u4ee5\u5f80\u7684\u7814\u7a76\u901a\u5e38\u4f7f\u7528\u660e\u786e\u7684\u6027\u522b\u7ebf\u7d22\u4f5c\u4e3a\u53cd\u4e8b\u5b9e\u6765\u63a2\u6d4b\u6027\u522b\u504f\u89c1\uff0c\u6216\u8005\u5728\u53e5\u5b50\u8865\u5168\u548c\u7b80\u77ed\u7684\u95ee\u7b54\u4efb\u52a1\u4e2d\u7814\u7a76\u5b83\u4eec\u3002\u8fd9\u4e9b\u5f62\u5f0f\u53ef\u80fd\u4f1a\u5ffd\u7565\u5d4c\u5165\u5728\u8f83\u957f\u5185\u5bb9\u7684\u751f\u6210\u884c\u4e3a\u4e2d\u7684\u66f4\u9690\u6027\u7684\u504f\u89c1\u5f62\u5f0f\u3002", "method": "\u4f7f\u7528\u5305\u542b\u77ed\u7bc7\u6545\u4e8b\u7684\u65b0\u6570\u636e\u96c6StereoBias-Stories\uff0c\u6545\u4e8b\u5305\u542b\u975e\u6761\u4ef6\u63d0\u793a\u6216\u57fa\u4e8e25\u79cd\u5fc3\u7406\u523b\u677f\u5370\u8c61\u7684\u968f\u673a\u5c5e\u6027\u7684\u6761\u4ef6\u63d0\u793a\uff0c\u4ee5\u53ca\u4e09\u4e2a\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u6545\u4e8b\u7ed3\u5c3e\u6765\u7814\u7a76LLM\u4e2d\u7684\u6027\u522b\u504f\u89c1\u3002", "result": "(1) \u6a21\u578b\u5728\u975e\u6761\u4ef6\u63d0\u793a\u4e0b\u5e73\u5747\u800c\u8a00\u9ad8\u5ea6\u504f\u5411\u7537\u6027\uff0c\u4f46\u4ee5\u72ec\u7acb\u4e8e\u6027\u522b\u523b\u677f\u5370\u8c61\u7684\u5c5e\u6027\u4e3a\u6761\u4ef6\u53ef\u4ee5\u51cf\u8f7b\u8fd9\u79cd\u504f\u89c1\u3002(2) \u7ec4\u5408\u4e0e\u540c\u4e00\u6027\u522b\u523b\u677f\u5370\u8c61\u76f8\u5173\u7684\u591a\u4e2a\u5c5e\u6027\u4f1a\u52a0\u5267\u6a21\u578b\u884c\u4e3a\uff0c\u5176\u4e2d\u7537\u6027\u5c5e\u6027\u4f1a\u653e\u5927\u504f\u89c1\uff0c\u800c\u5973\u6027\u5c5e\u6027\u4f1a\u51cf\u8f7b\u504f\u89c1\u3002(3) \u6a21\u578b\u504f\u5dee\u4e0e\u7528\u4e8e\u5206\u7c7b\u7684\u5fc3\u7406\u5b66\u57fa\u672c\u4e8b\u5b9e\u76f8\u4e00\u81f4\uff0c\u5e76\u4e14\u4e00\u81f4\u6027\u5f3a\u5ea6\u968f\u6a21\u578b\u5927\u5c0f\u800c\u589e\u52a0\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u90fd\u5b58\u5728\u6027\u522b\u504f\u89c1\uff0c\u5e76\u4e14\u8fd9\u79cd\u504f\u89c1\u4e0e\u5fc3\u7406\u5b66\u7814\u7a76\u4e2d\u7684\u6027\u522b\u523b\u677f\u5370\u8c61\u76f8\u7b26\uff0c\u6a21\u578b\u8d8a\u5927\uff0c\u8fd9\u79cd\u4e00\u81f4\u6027\u8d8a\u5f3a\u3002"}}
{"id": "2508.02926", "categories": ["cs.LG", "cs.AI", "cs.HC", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.02926", "abs": "https://arxiv.org/abs/2508.02926", "authors": ["Arthur Cho"], "title": "GrandJury: A Collaborative Machine Learning Model Evaluation Protocol for Dynamic Quality Rubrics", "comment": "26 pages, 1 table. Open-source implementation available on PyPI\n  (grandjury package) and GitHub. Dataset available on Hugging Face under\n  CC-BY-4.0 license", "summary": "Generative Machine Learning models have become central to modern systems,\npowering applications in creative writing, summarization, multi-hop reasoning,\nand context-aware dialogue. These models underpin large-scale AI assistants,\nworkflow automation, and autonomous decision-making. In such domains,\nacceptable response is rarely absolute or static, but plural and highly\ncontext-dependent. Yet standard evaluation regimes still rely on static,\nbenchmark-style tests, incentivizing optimization toward leaderboard scores\nrather than alignment with dynamic user needs or evolving realities. GrandJury\nintroduces a formal evaluation protocol combining time-decayed aggregation,\ncomplete traceability, with the support of dynamic, transparent task rubric\nattribution, and multi-rater human judgment. Together, these elements enable\npluralistic, accountable evaluation that captures evolving consensus and\nsurfaces disagreement. We provide an open-source implementation (grandjury PyPI\npackage) and a public collection of Large Language Model (LLM) inference\noutputs to illustrate the need and method. GrandJury provides a new paradigm\nfor AI practitioners when evaluating machine learning outputs without absolute\nground truth.", "AI": {"tldr": "GrandJury introduces a new evaluation protocol for machine learning outputs, addressing the limitations of static benchmarks by incorporating dynamic user needs and evolving realities.", "motivation": "standard evaluation regimes still rely on static, benchmark-style tests, incentivizing optimization toward leaderboard scores rather than alignment with dynamic user needs or evolving realities", "method": "a formal evaluation protocol combining time-decayed aggregation, complete traceability, with the support of dynamic, transparent task rubric attribution, and multi-rater human judgment", "result": "enable pluralistic, accountable evaluation that captures evolving consensus and surfaces disagreement. We provide an open-source implementation (grandjury PyPI package) and a public collection of Large Language Model (LLM) inference outputs to illustrate the need and method.", "conclusion": "GrandJury provides a new paradigm for AI practitioners when evaluating machine learning outputs without absolute ground truth."}}
{"id": "2508.03109", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03109", "abs": "https://arxiv.org/abs/2508.03109", "authors": ["Wen-Xi Yang", "Tian-Fang Zhao"], "title": "AgentSME for Simulating Diverse Communication Modes in Smart Education", "comment": null, "summary": "Generative agent models specifically tailored for smart education are\ncritical, yet remain relatively underdeveloped. A key challenge stems from the\ninherent complexity of educational contexts: learners are human beings with\nvarious cognitive behaviors, and pedagogy is fundamentally centered on\npersonalized human-to-human communication. To address this issue, this paper\nproposes AgentSME, a unified generative agent framework powered by LLM. Three\ndirectional communication modes are considered in the models, namely Solo,\nMono, and Echo, reflecting different types of agency autonomy and communicative\nreciprocity. Accuracy is adopted as the primary evaluation metric, complemented\nby three diversity indices designed to assess the diversity of reasoning\ncontents. Six widely used LLMs are tested to validate the robustness of\ncommunication modes across different model tiers, which are equally divided\ninto base-capacity and high-capacity configurations. The results show that\ngenerative agents that employ the Echo communication mode achieve the highest\naccuracy scores, while DeepSeek exhibits the greatest diversity. This study\nprovides valuable information to improve agent learning capabilities and\ninspire smart education models.", "AI": {"tldr": "This paper proposes AgentSME, a unified generative agent framework powered by LLM, explores three communication modes (Solo, Mono, Echo), and finds Echo mode achieves the highest accuracy.", "motivation": "Generative agent models specifically tailored for smart education are critical, yet remain relatively underdeveloped. A key challenge stems from the inherent complexity of educational contexts: learners are human beings with various cognitive behaviors, and pedagogy is fundamentally centered on personalized human-to-human communication.", "method": "a unified generative agent framework powered by LLM. Three directional communication modes are considered in the models, namely Solo, Mono, and Echo", "result": "Six widely used LLMs are tested to validate the robustness of communication modes across different model tiers, which are equally divided into base-capacity and high-capacity configurations.", "conclusion": "generative agents that employ the Echo communication mode achieve the highest accuracy scores, while DeepSeek exhibits the greatest diversity. This study provides valuable information to improve agent learning capabilities and inspire smart education models."}}
{"id": "2508.03055", "categories": ["cs.CV", "cs.AI", "I.4.8"], "pdf": "https://arxiv.org/pdf/2508.03055", "abs": "https://arxiv.org/abs/2508.03055", "authors": ["Hyebin Cho", "Jaehyup Lee"], "title": "Uncertainty-Guided Face Matting for Occlusion-Aware Face Transformation", "comment": "Accepted to ACM MM 2025. 9 pages, 8 figures, 6 tables", "summary": "Face filters have become a key element of short-form video content, enabling\na wide array of visual effects such as stylization and face swapping. However,\ntheir performance often degrades in the presence of occlusions, where objects\nlike hands, hair, or accessories obscure the face. To address this limitation,\nwe introduce the novel task of face matting, which estimates fine-grained alpha\nmattes to separate occluding elements from facial regions. We further present\nFaceMat, a trimap-free, uncertainty-aware framework that predicts high-quality\nalpha mattes under complex occlusions. Our approach leverages a two-stage\ntraining pipeline: a teacher model is trained to jointly estimate alpha mattes\nand per-pixel uncertainty using a negative log-likelihood (NLL) loss, and this\nuncertainty is then used to guide the student model through spatially adaptive\nknowledge distillation. This formulation enables the student to focus on\nambiguous or occluded regions, improving generalization and preserving semantic\nconsistency. Unlike previous approaches that rely on trimaps or segmentation\nmasks, our framework requires no auxiliary inputs making it well-suited for\nreal-time applications. In addition, we reformulate the matting objective by\nexplicitly treating skin as foreground and occlusions as background, enabling\nclearer compositing strategies. To support this task, we newly constructed\nCelebAMat, a large-scale synthetic dataset specifically designed for\nocclusion-aware face matting. Extensive experiments show that FaceMat\noutperforms state-of-the-art methods across multiple benchmarks, enhancing the\nvisual quality and robustness of face filters in real-world, unconstrained\nvideo scenarios. The source code and CelebAMat dataset are available at\nhttps://github.com/hyebin-c/FaceMat.git", "AI": {"tldr": "FaceMat \u662f\u4e00\u79cd\u7528\u4e8e\u9762\u90e8\u62a0\u56fe\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u5728\u590d\u6742\u906e\u6321\u4e0b\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u989d\u5916\u7684\u8f93\u5165\u3002", "motivation": "\u9762\u90e8\u6ee4\u955c\u5df2\u6210\u4e3a\u77ed\u89c6\u9891\u5185\u5bb9\u7684\u5173\u952e\u8981\u7d20\uff0c\u53ef\u5b9e\u73b0\u5404\u79cd\u89c6\u89c9\u6548\u679c\uff0c\u4f8b\u5982\u98ce\u683c\u5316\u548c\u9762\u90e8\u4ea4\u6362\u3002\u7136\u800c\uff0c\u5f53\u624b\u3001\u5934\u53d1\u6216\u914d\u9970\u7b49\u7269\u4f53\u906e\u6321\u9762\u90e8\u65f6\uff0c\u5b83\u4eec\u7684\u6027\u80fd\u901a\u5e38\u4f1a\u4e0b\u964d\u3002", "method": "FaceMat\uff0c\u4e00\u4e2a\u65e0 trimap \u7684\u3001\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u9884\u6d4b\u590d\u6742\u906e\u6321\u4e0b\u7684\u9ad8\u8d28\u91cf alpha mattes\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5229\u7528\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u8bad\u7ec3\u7ba1\u9053\uff1a\u4e00\u4e2a\u6559\u5e08\u6a21\u578b\u88ab\u8bad\u7ec3\u6765\u8054\u5408\u4f30\u8ba1 alpha mattes \u548c\u4f7f\u7528\u8d1f\u5bf9\u6570\u4f3c\u7136 (NLL) \u635f\u5931\u7684\u6bcf\u50cf\u7d20\u4e0d\u786e\u5b9a\u6027\uff0c\u7136\u540e\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u88ab\u7528\u6765\u901a\u8fc7\u7a7a\u95f4\u81ea\u9002\u5e94\u77e5\u8bc6\u84b8\u998f\u6765\u6307\u5bfc\u5b66\u751f\u6a21\u578b\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFaceMat \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u4e0d\u53d7\u7ea6\u675f\u7684\u89c6\u9891\u573a\u666f\u4e2d\u9762\u90e8\u6ee4\u955c\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "FaceMat \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u4e0d\u53d7\u7ea6\u675f\u7684\u89c6\u9891\u573a\u666f\u4e2d\u9762\u90e8\u6ee4\u955c\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.03294", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03294", "abs": "https://arxiv.org/abs/2508.03294", "authors": ["Leonidas Zotos", "Ivo Pascal de Jong", "Matias Valdenegro-Toro", "Andreea Ioana Sburlea", "Malvina Nissim", "Hedderik van Rijn"], "title": "NLP Methods May Actually Be Better Than Professors at Estimating Question Difficulty", "comment": "10 pages, 2 figures, accepted at the 2nd International Workshop on AI\n  in Society, Education and Educational Research (AISEER)", "summary": "Estimating the difficulty of exam questions is essential for developing good\nexams, but professors are not always good at this task. We compare various\nLarge Language Model-based methods with three professors in their ability to\nestimate what percentage of students will give correct answers on True/False\nexam questions in the areas of Neural Networks and Machine Learning. Our\nresults show that the professors have limited ability to distinguish between\neasy and difficult questions and that they are outperformed by directly asking\nGemini 2.5 to solve this task. Yet, we obtained even better results using\nuncertainties of the LLMs solving the questions in a supervised learning\nsetting, using only 42 training samples. We conclude that supervised learning\nusing LLM uncertainty can help professors better estimate the difficulty of\nexam questions, improving the quality of assessment.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9884\u6d4b\u8003\u9898\u96be\u5ea6\u65b9\u9762\u4f18\u4e8e\u6559\u6388\uff0c\u6709\u76d1\u7763\u5b66\u4e60\u4f7f\u7528LLM\u4e0d\u786e\u5b9a\u6027\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u8bc4\u4f30\u8003\u9898\u7684\u96be\u5ea6\u5bf9\u4e8e\u5f00\u53d1\u597d\u7684\u8003\u8bd5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6559\u6388\u5e76\u4e0d\u603b\u662f\u64c5\u957f\u8fd9\u9879\u4efb\u52a1\u3002", "method": "\u6bd4\u8f83\u5404\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u4e0e\u4e09\u4f4d\u6559\u6388\u5728\u4f30\u8ba1\u795e\u7ecf\u7f51\u7edc\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\u4e2d\u5b66\u751f\u6b63\u786e\u56de\u7b54\u662f\u975e\u9898\u7684\u767e\u5206\u6bd4\u7684\u80fd\u529b\u3002", "result": "\u6559\u6388\u533a\u5206\u7b80\u5355\u548c\u56f0\u96be\u95ee\u9898\u7684\u80fd\u529b\u6709\u9650\uff0c\u5e76\u4e14\u4ed6\u4eec\u7684\u8868\u73b0\u4e0d\u5982\u76f4\u63a5\u8981\u6c42Gemini 2.5\u89e3\u51b3\u6b64\u4efb\u52a1\u3002\u7136\u800c\uff0c\u901a\u8fc7\u5728\u76d1\u7763\u5b66\u4e60\u73af\u5883\u4e2d\u4f7f\u7528LLM\u89e3\u51b3\u95ee\u9898\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u6211\u4eec\u83b7\u5f97\u4e86\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u4ec5\u4f7f\u7528\u4e8642\u4e2a\u8bad\u7ec3\u6837\u672c\u3002", "conclusion": "\u6709\u76d1\u7763\u5b66\u4e60\u4f7f\u7528LLM\u4e0d\u786e\u5b9a\u6027\u53ef\u4ee5\u5e2e\u52a9\u6559\u6388\u66f4\u597d\u5730\u4f30\u8ba1\u8003\u9898\u7684\u96be\u5ea6\uff0c\u4ece\u800c\u63d0\u9ad8\u8bc4\u4f30\u8d28\u91cf\u3002"}}
{"id": "2508.02932", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02932", "abs": "https://arxiv.org/abs/2508.02932", "authors": ["Minghao Yan", "Zhuang Wang", "Zhen Jia", "Shivaram Venkataraman", "Yida Wang"], "title": "PLoRA: Efficient LoRA Hyperparameter Tuning for Large Models", "comment": null, "summary": "Low-rank Adaptation (LoRA) has gained popularity as a fine-tuning approach\nfor Large Language Models (LLMs) due to its low resource requirements and good\nperformance. While a plethora of work has investigated improving LoRA serving\nefficiency by serving multiple LoRAs concurrently, existing methods assume that\na wide range of LoRA adapters are available for serving. In our work, we\nconduct extensive empirical studies to identify that current training paradigms\ndo not utilize hardware resources efficiently and require high overhead to\nobtain a performant LoRA. Leveraging these insights, we propose PLoRA, which\nautomatically orchestrates concurrent LoRA fine-tuning jobs under given\nhardware and model constraints and develops performant kernels to improve\ntraining efficiency. Our experimental studies show that PLoRA reduces the\nmakespan of LoRA fine-tuning over a given hyperparameter search space by up to\n7.52x and improves training throughput by up to 12.8x across a range of\nstate-of-the-art LLMs.", "AI": {"tldr": "PLoRA improves LoRA fine-tuning efficiency by orchestrating concurrent jobs and developing performant kernels.", "motivation": "Current training paradigms do not utilize hardware resources efficiently and require high overhead to obtain a performant LoRA.", "method": "PLoRA automatically orchestrates concurrent LoRA fine-tuning jobs and develops performant kernels.", "result": "PLoRA reduces the makespan of LoRA fine-tuning over a given hyperparameter search space by up to 7.52x and improves training throughput by up to 12.8x.", "conclusion": "PLoRA reduces the makespan of LoRA fine-tuning and improves training throughput."}}
{"id": "2508.03117", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03117", "abs": "https://arxiv.org/abs/2508.03117", "authors": ["Vinicius Lima", "Dzung T. Phan", "Jayant Kalagnanam", "Dhaval Patel", "Nianjun Zhou"], "title": "Toward a Trustworthy Optimization Modeling Agent via Verifiable Synthetic Data Generation", "comment": "25 pages", "summary": "We present a framework for training trustworthy large language model (LLM)\nagents for optimization modeling via a verifiable synthetic data generation\npipeline. Focusing on linear and mixed-integer linear programming, our approach\nbegins with structured symbolic representations and systematically produces\nnatural language descriptions, mathematical formulations, and solver-executable\ncode. By programmatically constructing each instance with known optimal\nsolutions, the pipeline ensures full verifiability and enables automatic\nfiltering of low-quality demonstrations generated by teacher models. Each\ndataset instance includes a structured representation of the optimization\nproblem, a corresponding natural language description, the verified optimal\nsolution, and step-by-step demonstrations - generated by a teacher model - that\nshow how to model and solve the problem across multiple optimization modeling\nlanguages. This enables supervised fine-tuning of open-source LLMs specifically\ntailored to optimization tasks. To operationalize this pipeline, we introduce\nOptiTrust, a modular LLM agent that performs multi-stage translation from\nnatural language to solver-ready code, leveraging stepwise demonstrations,\nmulti-language inference, and majority-vote cross-validation. Our agent\nachieves state-of-the-art performance on standard benchmarks. Out of 7\ndatasets, it achieves the highest accuracy on six and outperforms the next-best\nalgorithm by at least 8 percentage on three of them. Our approach provides a\nscalable, verifiable, and principled path toward building reliable LLM agents\nfor real-world optimization applications.", "AI": {"tldr": "This paper introduces a framework (OptiTrust) for creating reliable LLM agents for optimization by generating verifiable synthetic data. OptiTrust achieves state-of-the-art results on standard benchmarks.", "motivation": "The paper aims to develop trustworthy large language model (LLM) agents for optimization modeling.", "method": "The paper introduces a framework for training trustworthy LLM agents for optimization modeling via a verifiable synthetic data generation pipeline. It uses structured symbolic representations to produce natural language descriptions, mathematical formulations, and solver-executable code. The OptiTrust agent, a modular LLM agent, performs multi-stage translation from natural language to solver-ready code, leveraging stepwise demonstrations, multi-language inference, and majority-vote cross-validation.", "result": "The OptiTrust agent achieves state-of-the-art performance on standard benchmarks, achieving the highest accuracy on six out of seven datasets and outperforming the next-best algorithm by at least 8 percentage on three of them.", "conclusion": "The paper presents a scalable, verifiable, and principled path toward building reliable LLM agents for real-world optimization applications, with the OptiTrust agent achieving state-of-the-art performance on standard benchmarks."}}
{"id": "2508.03060", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03060", "abs": "https://arxiv.org/abs/2508.03060", "authors": ["Lekang Wen", "Jing Xiao", "Liang Liao", "Jiajun Chen", "Mi Wang"], "title": "CHARM: Collaborative Harmonization across Arbitrary Modalities for Modality-agnostic Semantic Segmentation", "comment": null, "summary": "Modality-agnostic Semantic Segmentation (MaSS) aims to achieve robust scene\nunderstanding across arbitrary combinations of input modality. Existing methods\ntypically rely on explicit feature alignment to achieve modal homogenization,\nwhich dilutes the distinctive strengths of each modality and destroys their\ninherent complementarity. To achieve cooperative harmonization rather than\nhomogenization, we propose CHARM, a novel complementary learning framework\ndesigned to implicitly align content while preserving modality-specific\nadvantages through two components: (1) Mutual Perception Unit (MPU), enabling\nimplicit alignment through window-based cross-modal interaction, where\nmodalities serve as both queries and contexts for each other to discover\nmodality-interactive correspondences; (2) A dual-path optimization strategy\nthat decouples training into Collaborative Learning Strategy (CoL) for\ncomplementary fusion learning and Individual Enhancement Strategy (InE) for\nprotected modality-specific optimization. Experiments across multiple datasets\nand backbones indicate that CHARM consistently outperform the baselines, with\nsignificant increment on the fragile modalities. This work shifts the focus\nfrom model homogenization to harmonization, enabling cross-modal\ncomplementarity for true harmony in diversity.", "AI": {"tldr": "CHARM\uff1a\u4e00\u79cd\u7528\u4e8e\u6a21\u6001\u4e0d\u53ef\u77e5\u8bed\u4e49\u5206\u5272\u7684\u4e92\u8865\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e92\u611f\u77e5\u5355\u5143\u548c\u53cc\u8def\u5f84\u4f18\u5316\u7b56\u7565\u5b9e\u73b0\u8de8\u6a21\u6001\u4e92\u8865\uff0c\u4ece\u800c\u5728\u591a\u6837\u6027\u4e2d\u5b9e\u73b0\u771f\u6b63\u7684\u548c\u8c10\u3002", "motivation": "\u6a21\u6001\u4e0d\u53ef\u77e5\u8bed\u4e49\u5206\u5272(MaSS)\u65e8\u5728\u5b9e\u73b0\u8de8\u4efb\u610f\u7ec4\u5408\u7684\u8f93\u5165\u6a21\u6001\u7684\u9c81\u68d2\u573a\u666f\u7406\u89e3\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u663e\u5f0f\u7279\u5f81\u5bf9\u9f50\u6765\u5b9e\u73b0\u6a21\u6001\u540c\u8d28\u5316\uff0c\u8fd9\u524a\u5f31\u4e86\u6bcf\u79cd\u6a21\u6001\u7684\u72ec\u7279\u4f18\u52bf\uff0c\u5e76\u7834\u574f\u4e86\u5b83\u4eec\u56fa\u6709\u7684\u4e92\u8865\u6027\u3002\u4e3a\u4e86\u5b9e\u73b0\u534f\u540c\u548c\u8c10\u800c\u4e0d\u662f\u540c\u8d28\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e92\u8865\u5b66\u4e60\u6846\u67b6CHARM\uff0c\u65e8\u5728\u901a\u8fc7\u4e24\u4e2a\u7ec4\u4ef6\u9690\u5f0f\u5bf9\u9f50\u5185\u5bb9\uff0c\u540c\u65f6\u4fdd\u7559\u6a21\u6001\u7279\u5b9a\u7684\u4f18\u52bf\uff1a(1)\u4e92\u611f\u77e5\u5355\u5143(MPU)\uff0c\u901a\u8fc7\u57fa\u4e8e\u7a97\u53e3\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u5b9e\u73b0\u9690\u5f0f\u5bf9\u9f50\uff0c\u5176\u4e2d\u6a21\u6001\u76f8\u4e92\u5145\u5f53\u67e5\u8be2\u548c\u4e0a\u4e0b\u6587\uff0c\u4ee5\u53d1\u73b0\u6a21\u6001\u4ea4\u4e92\u5bf9\u5e94\u5173\u7cfb\uff1b(2)\u53cc\u8def\u5f84\u4f18\u5316\u7b56\u7565\uff0c\u5c06\u8bad\u7ec3\u89e3\u8026\u4e3a\u7528\u4e8e\u4e92\u8865\u878d\u5408\u5b66\u4e60\u7684\u534f\u540c\u5b66\u4e60\u7b56\u7565(CoL)\u548c\u7528\u4e8e\u53d7\u4fdd\u62a4\u7684\u6a21\u6001\u7279\u5b9a\u4f18\u5316\u7684\u4e2a\u4f53\u589e\u5f3a\u7b56\u7565(InE)\u3002", "result": "CHARM\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u9aa8\u5e72\u7f51\u7edc\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5bf9\u8106\u5f31\u6a21\u6001\u6709\u663e\u7740\u63d0\u5347\u3002", "conclusion": "CHARM\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u9aa8\u5e72\u7f51\u7edc\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5bf9\u8106\u5f31\u6a21\u6001\u6709\u663e\u7740\u63d0\u5347\u3002\u8fd9\u9879\u5de5\u4f5c\u5c06\u91cd\u70b9\u4ece\u6a21\u578b\u540c\u8d28\u5316\u8f6c\u79fb\u5230\u548c\u8c10\u5316\uff0c\u4ece\u800c\u5b9e\u73b0\u8de8\u6a21\u6001\u4e92\u8865\uff0c\u4ece\u800c\u5728\u591a\u6837\u6027\u4e2d\u5b9e\u73b0\u771f\u6b63\u7684\u548c\u8c10\u3002"}}
{"id": "2508.03296", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03296", "abs": "https://arxiv.org/abs/2508.03296", "authors": ["Anqi Li", "Wenwei Jin", "Jintao Tong", "Pengda Qin", "Weijia Li", "Guo Lu"], "title": "Towards Trustworthy Multimodal Moderation via Policy-Aligned Reasoning and Hierarchical Labeling", "comment": null, "summary": "Social platforms have revolutionized information sharing, but also\naccelerated the dissemination of harmful and policy-violating content. To\nensure safety and compliance at scale, moderation systems must go beyond\nefficiency and offer accuracy and interpretability. However, current approaches\nlargely rely on noisy, label-driven learning, lacking alignment with moderation\nrules and producing opaque decisions that hinder human review. Therefore, we\npropose Hierarchical Guard (Hi-Guard), a multimodal moderation framework that\nintroduces a new policy-aligned decision paradigm. The term \"Hierarchical\"\nreflects two key aspects of our system design: (1) a hierarchical moderation\npipeline, where a lightweight binary model first filters safe content and a\nstronger model handles fine-grained risk classification; and (2) a hierarchical\ntaxonomy in the second stage, where the model performs path-based\nclassification over a hierarchical taxonomy ranging from coarse to fine-grained\nlevels. To ensure alignment with evolving moderation policies, Hi-Guard\ndirectly incorporates rule definitions into the model prompt. To further\nenhance structured prediction and reasoning, we introduce a multi-level\nsoft-margin reward and optimize with Group Relative Policy Optimization (GRPO),\npenalizing semantically adjacent misclassifications and improving explanation\nquality. Extensive experiments and real-world deployment demonstrate that\nHi-Guard achieves superior classification accuracy, generalization, and\ninterpretability, paving the way toward scalable, transparent, and trustworthy\ncontent safety systems. Code is available at:\nhttps://github.com/lianqi1008/Hi-Guard.", "AI": {"tldr": "Hi-Guard is a multimodal moderation framework with a hierarchical structure and policy-aligned decision paradigm that improves classification accuracy, generalization, and interpretability.", "motivation": "Current moderation approaches largely rely on noisy, label-driven learning, lacking alignment with moderation rules and producing opaque decisions that hinder human review.", "method": "Hi-Guard, a multimodal moderation framework that introduces a new policy-aligned decision paradigm with a hierarchical moderation pipeline and a hierarchical taxonomy, incorporating rule definitions into the model prompt and optimizing with Group Relative Policy Optimization (GRPO).", "result": "Hi-Guard achieves superior classification accuracy, generalization, and interpretability in experiments and real-world deployment.", "conclusion": "Hi-Guard achieves superior classification accuracy, generalization, and interpretability, paving the way toward scalable, transparent, and trustworthy content safety systems."}}
{"id": "2508.02948", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.02948", "abs": "https://arxiv.org/abs/2508.02948", "authors": ["Zain Ulabedeen Farhat", "Debamita Ghosh", "George K. Atia", "Yue Wang"], "title": "Online Robust Multi-Agent Reinforcement Learning under Model Uncertainties", "comment": null, "summary": "Well-trained multi-agent systems can fail when deployed in real-world\nenvironments due to model mismatches between the training and deployment\nenvironments, caused by environment uncertainties including noise or\nadversarial attacks. Distributionally Robust Markov Games (DRMGs) enhance\nsystem resilience by optimizing for worst-case performance over a defined set\nof environmental uncertainties. However, current methods are limited by their\ndependence on simulators or large offline datasets, which are often\nunavailable. This paper pioneers the study of online learning in DRMGs, where\nagents learn directly from environmental interactions without prior data. We\nintroduce the {\\it Robust Optimistic Nash Value Iteration (RONAVI)} algorithm\nand provide the first provable guarantees for this setting. Our theoretical\nanalysis demonstrates that the algorithm achieves low regret and efficiently\nfinds the optimal robust policy for uncertainty sets measured by Total\nVariation divergence and Kullback-Leibler divergence. These results establish a\nnew, practical path toward developing truly robust multi-agent systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u7ebfDRMGs\u5b66\u4e60\uff0c\u63d0\u51fa\u4e86RONAVI\u7b97\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u9996\u4e2a\u7406\u8bba\u4fdd\u8bc1\uff0c\u4e3a\u5f00\u53d1\u771f\u6b63\u9c81\u68d2\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5efa\u7acb\u4e86\u4e00\u6761\u65b0\u7684\u5b9e\u7528\u8def\u5f84\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u4f1a\u56e0\u8bad\u7ec3\u548c\u90e8\u7f72\u73af\u5883\u7684\u6a21\u578b\u5931\u914d\u800c\u5931\u6548\uff0c\u8fd9\u79cd\u5931\u914d\u662f\u7531\u566a\u58f0\u6216\u5bf9\u6297\u653b\u51fb\u7b49\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u5f15\u8d77\u7684\u3002Distributionally Robust Markov Games (DRMGs) \u901a\u8fc7\u4f18\u5316\u5b9a\u4e49\u7684\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u96c6\u5408\u4e0a\u7684\u6700\u574f\u60c5\u51b5\u6027\u80fd\u6765\u589e\u5f3a\u7cfb\u7edf\u5f39\u6027\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6a21\u62df\u5668\u6216\u5927\u578b\u79bb\u7ebf\u6570\u636e\u96c6\uff0c\u800c\u8fd9\u4e9b\u901a\u5e38\u662f\u4e0d\u53ef\u7528\u7684\u3002", "method": "\u63d0\u51faRobust Optimistic Nash Value Iteration (RONAVI)\u7b97\u6cd5\u3002", "result": "\u8be5\u7b97\u6cd5\u5b9e\u73b0\u4e86\u4f4e\u9057\u61be\u503c\uff0c\u5e76\u6709\u6548\u5730\u627e\u5230\u4e86\u4e0d\u786e\u5b9a\u6027\u96c6\u5408\uff08\u901a\u8fc7Total Variation divergence\u548cKullback-Leibler divergence\u6d4b\u91cf\uff09\u7684\u6700\u4f18\u9c81\u68d2\u7b56\u7565\u3002", "conclusion": "\u8be5\u8bba\u6587\u4e3a\u5728\u7ebfDRMGs\u5b66\u4e60\u63d0\u4f9b\u9996\u4e2a\u7406\u8bba\u4fdd\u8bc1\uff0c\u8bc1\u660e\u4e86RONAVI\u7b97\u6cd5\u5728Total Variation divergence\u548cKullback-Leibler divergence\u4e0b\u80fd\u5b9e\u73b0\u4f4e\u9057\u61be\u503c\u5e76\u9ad8\u6548\u627e\u5230\u6700\u4f18\u9c81\u68d2\u7b56\u7565\u3002"}}
{"id": "2508.03149", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03149", "abs": "https://arxiv.org/abs/2508.03149", "authors": ["Linda Smail", "David Santandreu Calonge", "Firuz Kamalov", "Nur H. Orak"], "title": "Can Large Language Models Bridge the Gap in Environmental Knowledge?", "comment": "20 pages, 3 figures, 7 tables. No external funding", "summary": "This research investigates the potential of Artificial Intelligence (AI)\nmodels to bridge the knowledge gap in environmental education among university\nstudents. By focusing on prominent large language models (LLMs) such as\nGPT-3.5, GPT-4, GPT-4o, Gemini, Claude Sonnet, and Llama 2, the study assesses\ntheir effectiveness in conveying environmental concepts and, consequently,\nfacilitating environmental education. The investigation employs a standardized\ntool, the Environmental Knowledge Test (EKT-19), supplemented by targeted\nquestions, to evaluate the environmental knowledge of university students in\ncomparison to the responses generated by the AI models. The results of this\nstudy suggest that while AI models possess a vast, readily accessible, and\nvalid knowledge base with the potential to empower both students and academic\nstaff, a human discipline specialist in environmental sciences may still be\nnecessary to validate the accuracy of the information provided.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86 AI \u6a21\u578b\u5728\u73af\u5883\u6559\u80b2\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u5b83\u4eec\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u8981\u4eba\u5de5\u4e13\u5bb6\u9a8c\u8bc1\u51c6\u786e\u6027\u3002", "motivation": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u4eba\u5de5\u667a\u80fd (AI) \u6a21\u578b\u5728\u5f25\u5408\u5927\u5b66\u751f\u73af\u5883\u6559\u80b2\u77e5\u8bc6\u5dee\u8ddd\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u5229\u7528\u73af\u5883\u77e5\u8bc6\u6d4b\u8bd5 (EKT-19) \u7ed3\u5408\u6709\u9488\u5bf9\u6027\u7684\u95ee\u9898\uff0c\u8bc4\u4f30\u5927\u5b66\u751f\u7684\u73af\u5883\u77e5\u8bc6\uff0c\u5e76\u5c06\u5176\u4e0e AI \u6a21\u578b\u751f\u6210\u7684\u56de\u7b54\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "AI \u6a21\u578b\u62e5\u6709\u5e9e\u5927\u3001\u6613\u4e8e\u8bbf\u95ee\u4e14\u6709\u6548\u7684\u77e5\u8bc6\u5e93\uff0c\u6709\u6f5c\u529b\u589e\u5f3a\u5b66\u751f\u548c\u5b66\u672f\u4eba\u5458\u7684\u80fd\u529b\u3002", "conclusion": "AI\u6a21\u578b\u62e5\u6709\u5e9e\u5927\u3001\u6613\u4e8e\u8bbf\u95ee\u4e14\u6709\u6548\u7684\u77e5\u8bc6\u5e93\uff0c\u6709\u6f5c\u529b\u589e\u5f3a\u5b66\u751f\u548c\u5b66\u672f\u4eba\u5458\u7684\u80fd\u529b\uff0c\u4f46\u53ef\u80fd\u4ecd\u9700\u8981\u73af\u5883\u79d1\u5b66\u9886\u57df\u7684\u4eba\u5de5\u5b66\u79d1\u4e13\u5bb6\u6765\u9a8c\u8bc1\u6240\u63d0\u4f9b\u4fe1\u606f\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.03064", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03064", "abs": "https://arxiv.org/abs/2508.03064", "authors": ["Trinh Quoc Nguyen", "Oky Dicky Ardiansyah Prima", "Katsuyoshi Hotta"], "title": "CORE-ReID: Comprehensive Optimization and Refinement through Ensemble fusion in Domain Adaptation for person re-identification", "comment": null, "summary": "This study introduces a novel framework, \"Comprehensive Optimization and\nRefinement through Ensemble Fusion in Domain Adaptation for Person\nRe-identification (CORE-ReID)\", to address an Unsupervised Domain Adaptation\n(UDA) for Person Re-identification (ReID). The framework utilizes CycleGAN to\ngenerate diverse data that harmonizes differences in image characteristics from\ndifferent camera sources in the pre-training stage. In the fine-tuning stage,\nbased on a pair of teacher-student networks, the framework integrates\nmulti-view features for multi-level clustering to derive diverse pseudo labels.\nA learnable Ensemble Fusion component that focuses on fine-grained local\ninformation within global features is introduced to enhance learning\ncomprehensiveness and avoid ambiguity associated with multiple pseudo-labels.\nExperimental results on three common UDAs in Person ReID demonstrate\nsignificant performance gains over state-of-the-art approaches. Additional\nenhancements, such as Efficient Channel Attention Block and Bidirectional Mean\nFeature Normalization mitigate deviation effects and adaptive fusion of global\nand local features using the ResNet-based model, further strengthening the\nframework. The proposed framework ensures clarity in fusion features, avoids\nambiguity, and achieves high ac-curacy in terms of Mean Average Precision,\nTop-1, Top-5, and Top-10, positioning it as an advanced and effective solution\nfor the UDA in Person ReID. Our codes and models are available at\nhttps://github.com/TrinhQuocNguyen/CORE-ReID.", "AI": {"tldr": "CORE-ReID is a new framework for UDA in Person ReID that uses CycleGAN, multi-view features, and ensemble fusion to achieve high accuracy.", "motivation": "To address Unsupervised Domain Adaptation (UDA) for Person Re-identification (ReID).", "method": "A novel framework, CORE-ReID, using CycleGAN for data generation, multi-view feature integration for multi-level clustering, and a learnable Ensemble Fusion component.", "result": "Significant performance gains over state-of-the-art approaches on three common UDAs in Person ReID.", "conclusion": "The proposed CORE-ReID framework achieves high accuracy in UDA for Person ReID, outperforming state-of-the-art approaches."}}
{"id": "2508.03333", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03333", "abs": "https://arxiv.org/abs/2508.03333", "authors": ["Zhende Song", "Shengji Tang", "Peng Ye", "Jiayuan Fan", "Tao Chen"], "title": "CTTS: Collective Test-Time Scaling", "comment": null, "summary": "Test-time scaling (TTS) has emerged as a promising research field for\nenhancing the effectiveness of large language models (LLMs) without extra\ntraining. However, most existing approaches, e.g., Best-of-N and\nSelf-Consistency rely on a single agent interacting with a reward model\n(SA-SR), constrained by limited capabilities of a single test-time scaling\n(STTS) paradigm. On the other hand, recent works demonstrate that\ncollective-agent methods can break through the upper bound of single-agent\nsystems by orchestrating diverse models. Thus, in this paper, we take a first\nstep towards exploring Collective Test-Time Scaling (CTTS). Consider the\ndifferent interaction types of single and multiple models, we design three\nprimary paradigms to investigate the optimal paradigm of CTTS: (1) single agent\nto multiple reward models (SA-MR); (2) multiple agents to single reward model\n(MA-SR); and (3) multiple agents to multiple reward models (MA-MR). Extensive\nexperiments demonstrate that MA-MR consistently achieves the best performance.\nBased on this, we propose a novel framework named CTTS-MM that effectively\nleverages both multi-agent and multi-reward-model collaboration for enhanced\ninference. Specifically, for multi-agent collaboration, we propose an Agent\nCollaboration Search (ACS), which searches for the most effective combination\nof LLM agents from a large candidate pool; for multi-reward-model\ncollaboration, we propose Mixture of Reword Models (MoR), which consists of a\ncurated question pool and a Prior Reward model Ensemble Selection (PRES) to\nselect the optimal combinations of reward models via Pair-wise Reward Ranking\n(PRR) metric. Experiments across seven mainstream benchmarks demonstrate that\nthe proposed CTTS-MM consistently obtains superior performance. Code will be\nreleased at https://github.com/magent4aci/CTTS-MM.", "AI": {"tldr": "This paper explores Collective Test-Time Scaling (CTTS) and proposes CTTS-MM, a novel framework that leverages multi-agent and multi-reward-model collaboration for enhanced inference, achieving superior performance.", "motivation": "Existing test-time scaling approaches rely on a single agent interacting with a reward model, constrained by limited capabilities. Collective-agent methods can break through the upper bound of single-agent systems.", "method": "The paper designs three primary paradigms to investigate the optimal paradigm of CTTS: (1) single agent to multiple reward models (SA-MR); (2) multiple agents to single reward model (MA-SR); and (3) multiple agents to multiple reward models (MA-MR). It proposes Agent Collaboration Search (ACS) for multi-agent collaboration and Mixture of Reword Models (MoR) for multi-reward-model collaboration.", "result": "MA-MR consistently achieves the best performance. The proposed CTTS-MM consistently obtains superior performance across seven mainstream benchmarks.", "conclusion": "The proposed CTTS-MM framework, leveraging both multi-agent and multi-reward-model collaboration, achieves superior performance across seven mainstream benchmarks."}}
{"id": "2508.02964", "categories": ["cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2508.02964", "abs": "https://arxiv.org/abs/2508.02964", "authors": ["Jonathan Patsenker", "Henry Li", "Myeongseob Ko", "Ruoxi Jia", "Yuval Kluger"], "title": "Injecting Measurement Information Yields a Fast and Noise-Robust Diffusion-Based Inverse Problem Solver", "comment": null, "summary": "Diffusion models have been firmly established as principled zero-shot solvers\nfor linear and nonlinear inverse problems, owing to their powerful image prior\nand iterative sampling algorithm. These approaches often rely on Tweedie's\nformula, which relates the diffusion variate $\\mathbf{x}_t$ to the posterior\nmean $\\mathbb{E} [\\mathbf{x}_0 | \\mathbf{x}_t]$, in order to guide the\ndiffusion trajectory with an estimate of the final denoised sample\n$\\mathbf{x}_0$. However, this does not consider information from the\nmeasurement $\\mathbf{y}$, which must then be integrated downstream. In this\nwork, we propose to estimate the conditional posterior mean $\\mathbb{E}\n[\\mathbf{x}_0 | \\mathbf{x}_t, \\mathbf{y}]$, which can be formulated as the\nsolution to a lightweight, single-parameter maximum likelihood estimation\nproblem. The resulting prediction can be integrated into any standard sampler,\nresulting in a fast and memory-efficient inverse solver. Our optimizer is\namenable to a noise-aware likelihood-based stopping criteria that is robust to\nmeasurement noise in $\\mathbf{y}$. We demonstrate comparable or improved\nperformance against a wide selection of contemporary inverse solvers across\nmultiple datasets and tasks.", "AI": {"tldr": "\u63d0\u51fa\u4f30\u8ba1\u6761\u4ef6\u540e\u9a8c\u5747\u503c E [x_0 | x_t, y]\uff0c\u5b83\u53ef\u4ee5\u88ab\u8868\u8ff0\u4e3a\u8f7b\u91cf\u7ea7\u7684\u5355\u53c2\u6570\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u95ee\u9898\u7684\u89e3\uff0c\u4ece\u800c\u4ea7\u751f\u5feb\u901f\u4e14\u5185\u5b58\u9ad8\u6548\u7684\u9006\u6c42\u89e3\u5668\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5df2\u7262\u56fa\u5730\u786e\u7acb\u4e3a\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u9006\u95ee\u9898\u7684\u6709\u539f\u5219\u7684\u96f6\u6837\u672c\u6c42\u89e3\u5668\uff0c\u8fd9\u5f52\u529f\u4e8e\u5176\u5f3a\u5927\u7684\u56fe\u50cf\u5148\u9a8c\u548c\u8fed\u4ee3\u91c7\u6837\u7b97\u6cd5\u3002\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e Tweedie \u516c\u5f0f\uff0c\u8be5\u516c\u5f0f\u5c06\u6269\u6563\u53d8\u91cf x_t \u4e0e\u540e\u9a8c\u5747\u503c E [x_0 | x_t] \u76f8\u5173\u8054\uff0c\u4ee5\u4fbf\u901a\u8fc7\u6700\u7ec8\u53bb\u566a\u6837\u672c x_0 \u7684\u4f30\u8ba1\u6765\u6307\u5bfc\u6269\u6563\u8f68\u8ff9\u3002\u4f46\u662f\uff0c\u8fd9\u6ca1\u6709\u8003\u8651\u6765\u81ea\u6d4b\u91cf y \u7684\u4fe1\u606f\uff0c\u7136\u540e\u5fc5\u987b\u5c06\u5176\u4e0b\u6e38\u96c6\u6210\u3002", "method": "\u6211\u4eec\u5efa\u8bae\u4f30\u8ba1\u6761\u4ef6\u540e\u9a8c\u5747\u503c E [x_0 | x_t, y]\uff0c\u5b83\u53ef\u4ee5\u88ab\u8868\u8ff0\u4e3a\u8f7b\u91cf\u7ea7\u7684\u5355\u53c2\u6570\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u95ee\u9898\u7684\u89e3\u3002", "result": "\u7531\u6b64\u4ea7\u751f\u7684\u9884\u6d4b\u53ef\u4ee5\u96c6\u6210\u5230\u4efb\u4f55\u6807\u51c6\u91c7\u6837\u5668\u4e2d\uff0c\u4ece\u800c\u4ea7\u751f\u5feb\u901f\u4e14\u5185\u5b58\u9ad8\u6548\u7684\u9006\u6c42\u89e3\u5668\u3002", "conclusion": "\u8be5\u4f18\u5316\u5668\u9002\u7528\u4e8e\u5bf9 y \u4e2d\u6d4b\u91cf\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\u7684\u3001\u57fa\u4e8e\u566a\u58f0\u611f\u77e5\u4f3c\u7136\u7684\u505c\u6b62\u6807\u51c6\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e2d\uff0c\u9488\u5bf9\u5404\u79cd\u5f53\u4ee3\u9006\u6c42\u89e3\u5668\uff0c\u5177\u6709\u53ef\u6bd4\u6216\u6539\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.03167", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03167", "abs": "https://arxiv.org/abs/2508.03167", "authors": ["Charles Tapley Hoyt", "Craig Bakker", "Richard J. Callahan", "Joseph Cottam", "August George", "Benjamin M. Gyori", "Haley M. Hummel", "Nathaniel Merrill", "Sara Mohammad Taheri", "Pruthvi Prakash Navada", "Marc-Antoine Parent", "Adam Rupe", "Olga Vitek", "Jeremy Zucker"], "title": "Causal identification with $Y_0$", "comment": null, "summary": "We present the $Y_0$ Python package, which implements causal identification\nalgorithms that apply interventional, counterfactual, and transportability\nqueries to data from (randomized) controlled trials, observational studies, or\nmixtures thereof. $Y_0$ focuses on the qualitative investigation of causation,\nhelping researchers determine whether a causal relationship can be estimated\nfrom available data before attempting to estimate how strong that relationship\nis. Furthermore, $Y_0$ provides guidance on how to transform the causal query\ninto a symbolic estimand that can be non-parametrically estimated from the\navailable data. $Y_0$ provides a domain-specific language for representing\ncausal queries and estimands as symbolic probabilistic expressions, tools for\nrepresenting causal graphical models with unobserved confounders, such as\nacyclic directed mixed graphs (ADMGs), and implementations of numerous\nidentification algorithms from the recent causal inference literature. The\n$Y_0$ source code can be found under the MIT License at\nhttps://github.com/y0-causal-inference/y0 and it can be installed with pip\ninstall y0.", "AI": {"tldr": "Y0 is a Python package for causal identification, helping researchers determine if causal relationships can be estimated from data and providing tools for doing so.", "motivation": "To help researchers determine whether a causal relationship can be estimated from available data and to transform the causal query into a symbolic estimand that can be non-parametrically estimated from the available data.", "method": "The package implements causal identification algorithms that apply interventional, counterfactual, and transportability queries to data from various sources. It uses a domain-specific language and ADMGs.", "result": "The Y0 package, available under the MIT License and installable via pip, offers a domain-specific language, tools for representing causal graphical models, and implementations of identification algorithms.", "conclusion": "The Y0 Python package provides tools for causal identification, allowing researchers to determine if a causal relationship can be estimated from data before quantifying its strength. It transforms causal queries into symbolic estimands."}}
{"id": "2508.03069", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03069", "abs": "https://arxiv.org/abs/2508.03069", "authors": ["Bo Zhang", "Yifan Zhang", "Shuo Yan", "Yu Bai", "Zheng Zhang", "Wu Liu", "Xiuzhuang Zhou", "Wendong Wang"], "title": "SSFMamba: Symmetry-driven Spatial-Frequency Feature Fusion for 3D Medical Image Segmentation", "comment": null, "summary": "In light of the spatial domain's limited capacity for modeling global context\nin 3D medical image segmentation, emerging approaches have begun to incorporate\nfrequency domain representations. However, straightforward feature extraction\nstrategies often overlook the unique properties of frequency domain\ninformation, such as conjugate symmetry. They also fail to account for the\nfundamental differences in data distribution between the spatial and frequency\ndomains, which can ultimately dilute or obscure the complementary strengths\nthat frequency-based representations offer. In this paper, we propose SSFMamba,\na Mamba based Symmetry-driven Spatial-Frequency feature fusion network for 3D\nmedical image segmentation. SSFMamba employs a complementary dual-branch\narchitecture that extracts features from both the spatial and frequency\ndomains, and leverages a Mamba block to fuse these heterogeneous features to\npreserve global context while reinforcing local details. In the frequency\ndomain branch, we harness Mamba's exceptional capability to extract global\ncontextual information in conjunction with the synergistic effect of frequency\ndomain features to further enhance global modeling. Moreover, we design a 3D\nmulti-directional scanning mechanism to strengthen the fusion of local and\nglobal cues. Extensive experiments on the BraTS2020 and BraTS2023 datasets\ndemonstrate that our approach consistently outperforms state-of-the-art methods\nacross various evaluation metrics.", "AI": {"tldr": "SSFMamba, a Mamba-based network, fuses spatial and frequency domain features for improved 3D medical image segmentation, outperforming existing methods.", "motivation": "Existing methods have limitations in modeling global context in 3D medical image segmentation and overlook the unique properties of frequency domain information. They also fail to account for the fundamental differences in data distribution between the spatial and frequency domains.", "method": "The paper proposes SSFMamba, a Mamba based Symmetry-driven Spatial-Frequency feature fusion network for 3D medical image segmentation. It employs a complementary dual-branch architecture that extracts features from both spatial and frequency domains, leveraging a Mamba block to fuse these features. A 3D multi-directional scanning mechanism is designed to strengthen the fusion of local and global cues.", "result": "SSFMamba outperforms state-of-the-art methods across various evaluation metrics on the BraTS2020 and BraTS2023 datasets.", "conclusion": "The proposed SSFMamba consistently outperforms state-of-the-art methods on the BraTS2020 and BraTS2023 datasets."}}
{"id": "2508.02989", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02989", "abs": "https://arxiv.org/abs/2508.02989", "authors": ["Ninh Pham", "Yingtao Zheng", "Hugo Phibbs"], "title": "Scalable Varied-Density Clustering via Graph Propagation", "comment": null, "summary": "We propose a novel perspective on varied-density clustering for\nhigh-dimensional data by framing it as a label propagation process in\nneighborhood graphs that adapt to local density variations. Our method formally\nconnects density-based clustering with graph connectivity, enabling the use of\nefficient graph propagation techniques developed in network science. To ensure\nscalability, we introduce a density-aware neighborhood propagation algorithm\nand leverage advanced random projection methods to construct approximate\nneighborhood graphs. Our approach significantly reduces computational cost\nwhile preserving clustering quality. Empirically, it scales to datasets with\nmillions of points in minutes and achieves competitive accuracy compared to\nexisting baselines.", "AI": {"tldr": "Density-based clustering via efficient graph propagation techniques.", "motivation": "varied-density clustering for high-dimensional data", "method": "a label propagation process in neighborhood graphs that adapt to local density variations", "result": "significantly reduces computational cost while preserving clustering quality", "conclusion": "The proposed method scales to datasets with millions of points in minutes and achieves competitive accuracy compared to existing baselines."}}
{"id": "2508.03173", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03173", "abs": "https://arxiv.org/abs/2508.03173", "authors": ["Jingxuan Wei", "Caijun Jia", "Qi Chen", "Honghao He", "Linzhuang Sun", "Conghui He", "Lijun Wu", "Bihui Yu", "Cheng Tan"], "title": "Geoint-R1: Formalizing Multimodal Geometric Reasoning with Dynamic Auxiliary Constructions", "comment": null, "summary": "Mathematical geometric reasoning is essential for scientific discovery and\neducational development, requiring precise logic and rigorous formal\nverification. While recent advances in Multimodal Large Language Models (MLLMs)\nhave improved reasoning tasks, existing models typically struggle with formal\ngeometric reasoning, particularly when dynamically constructing and verifying\nauxiliary geometric elements. To address these challenges, we introduce\nGeoint-R1, a multimodal reasoning framework designed to generate formally\nverifiable geometric solutions from textual descriptions and visual diagrams.\nGeoint-R1 uniquely integrates auxiliary elements construction, formal reasoning\nrepresented via Lean4, and interactive visualization. To systematically\nevaluate and advance formal geometric reasoning, we propose the Geoint\nbenchmark, comprising 1,885 rigorously annotated geometry problems across\ndiverse topics such as plane, spatial, and solid geometry. Each problem\nincludes structured textual annotations, precise Lean4 code for auxiliary\nconstructions, and detailed solution steps verified by experts. Extensive\nexperiments demonstrate that Geoint-R1 significantly surpasses existing\nmultimodal and math-specific reasoning models, particularly on challenging\nproblems requiring explicit auxiliary element constructions.", "AI": {"tldr": "Geoint-R1: A multimodal reasoning framework designed to generate formally verifiable geometric solutions. It introduces the Geoint benchmark, and surpasses existing models.", "motivation": "existing models typically struggle with formal geometric reasoning, particularly when dynamically constructing and verifying auxiliary geometric elements", "method": "a multimodal reasoning framework designed to generate formally verifiable geometric solutions from textual descriptions and visual diagrams. Geoint-R1 uniquely integrates auxiliary elements construction, formal reasoning represented via Lean4, and interactive visualization.", "result": "introduce Geoint-R1, a multimodal reasoning framework and propose the Geoint benchmark, comprising 1,885 rigorously annotated geometry problems", "conclusion": "Geoint-R1 significantly surpasses existing multimodal and math-specific reasoning models, particularly on challenging problems requiring explicit auxiliary element constructions."}}
{"id": "2508.03077", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03077", "abs": "https://arxiv.org/abs/2508.03077", "authors": ["Anran Wu", "Long Peng", "Xin Di", "Xueyuan Dai", "Chen Wu", "Yang Wang", "Xueyang Fu", "Yang Cao", "Zheng-Jun Zha"], "title": "RobustGS: Unified Boosting of Feedforward 3D Gaussian Splatting under Low-Quality Conditions", "comment": null, "summary": "Feedforward 3D Gaussian Splatting (3DGS) overcomes the limitations of\noptimization-based 3DGS by enabling fast and high-quality reconstruction\nwithout the need for per-scene optimization. However, existing feedforward\napproaches typically assume that input multi-view images are clean and\nhigh-quality. In real-world scenarios, images are often captured under\nchallenging conditions such as noise, low light, or rain, resulting in\ninaccurate geometry and degraded 3D reconstruction. To address these\nchallenges, we propose a general and efficient multi-view feature enhancement\nmodule, RobustGS, which substantially improves the robustness of feedforward\n3DGS methods under various adverse imaging conditions, enabling high-quality 3D\nreconstruction. The RobustGS module can be seamlessly integrated into existing\npretrained pipelines in a plug-and-play manner to enhance reconstruction\nrobustness. Specifically, we introduce a novel component, Generalized\nDegradation Learner, designed to extract generic representations and\ndistributions of multiple degradations from multi-view inputs, thereby\nenhancing degradation-awareness and improving the overall quality of 3D\nreconstruction. In addition, we propose a novel semantic-aware state-space\nmodel. It first leverages the extracted degradation representations to enhance\ncorrupted inputs in the feature space. Then, it employs a semantic-aware\nstrategy to aggregate semantically similar information across different views,\nenabling the extraction of fine-grained cross-view correspondences and further\nimproving the quality of 3D representations. Extensive experiments demonstrate\nthat our approach, when integrated into existing methods in a plug-and-play\nmanner, consistently achieves state-of-the-art reconstruction quality across\nvarious types of degradations.", "AI": {"tldr": "RobustGS enhances feedforward 3DGS robustness to image degradations using a multi-view feature enhancement module.", "motivation": "Existing feedforward 3D Gaussian Splatting (3DGS) methods are vulnerable to real-world image degradations, leading to inaccurate geometry and degraded 3D reconstruction.", "method": "A multi-view feature enhancement module, RobustGS, with a Generalized Degradation Learner and a semantic-aware state-space model.", "result": "RobustGS substantially improves the robustness of feedforward 3DGS methods under various adverse imaging conditions, enabling high-quality 3D reconstruction.", "conclusion": "The proposed RobustGS module, integrated into existing methods, achieves state-of-the-art reconstruction quality across various degradations."}}
{"id": "2508.03363", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03363", "abs": "https://arxiv.org/abs/2508.03363", "authors": ["Haotian Wu", "Bo Xu", "Yao Shu", "Menglin Yang", "Chengwei Qin"], "title": "Thinking with Nothinking Calibration: A New In-Context Learning Paradigm in Reasoning Large Language Models", "comment": null, "summary": "Reasoning large language models (RLLMs) have recently demonstrated remarkable\ncapabilities through structured and multi-step reasoning. While prior research\nhas primarily focused on improving their training and inference strategies,\ntheir potential for in-context learning (ICL) remains largely underexplored. To\nfill this gap, we propose Thinking with Nothinking Calibration (JointThinking),\na new ICL paradigm that leverages the structured difference between two\nreasoning modes, i.e., Thinking and Nothinking, to improve reasoning accuracy.\nSpecifically, our method prompts the model to generate two answers in parallel:\none in Thinking mode and the other in Nothinking mode. A second round of\nThinking is triggered only when the two initial responses are inconsistent,\nusing a single prompt that incorporates the original question and both\ncandidate answers. Since such disagreement occurs infrequently (e.g., only 6\\%\nin GSM8K), our method performs just one round of reasoning in most cases,\nresulting in minimal latency overhead. Extensive experiments across multiple\nreasoning benchmarks demonstrate that JointThinking significantly outperforms\nfew-shot chain-of-thought (CoT) and majority voting with improved answer\nrobustness. Moreover, It achieves comparable in-distribution performance to\ntraining-based SOTA method, while substantially outperforming on\nout-of-distribution tasks. We further conduct a systematic analysis of the\ncalibration mechanism, showing that leveraging different reasoning modes\nconsistently lowers the error rate and highlights the value of structural\nthinking diversity. Additionally, we observe that the performance gap between\nactual and ideal reasoning narrows as model size increases in the second round\nof thinking, indicating the strong scalability of our approach. Finally, we\ndiscuss current limitations and outline promising directions for future ICL\nresearch in RLLMs.", "AI": {"tldr": "This paper introduces JointThinking, a new ICL paradigm that leverages the structured difference between two reasoning modes, i.e., Thinking and Nothinking, to improve reasoning accuracy. It achieves comparable in-distribution performance to training-based SOTA method, while substantially outperforming on out-of-distribution tasks.", "motivation": "While prior research has primarily focused on improving their training and inference strategies, their potential for in-context learning (ICL) remains largely underexplored. To fill this gap, we propose Thinking with Nothinking Calibration (JointThinking)", "method": "Thinking with Nothinking Calibration (JointThinking), a new ICL paradigm that leverages the structured difference between two reasoning modes, i.e., Thinking and Nothinking, to improve reasoning accuracy.  prompts the model to generate two answers in parallel: one in Thinking mode and the other in Nothinking mode. A second round of Thinking is triggered only when the two initial responses are inconsistent, using a single prompt that incorporates the original question and both candidate answers.", "result": "leveraging different reasoning modes consistently lowers the error rate and highlights the value of structural thinking diversity.", "conclusion": "JointThinking significantly outperforms few-shot chain-of-thought (CoT) and majority voting with improved answer robustness. Moreover, It achieves comparable in-distribution performance to training-based SOTA method, while substantially outperforming on out-of-distribution tasks. the performance gap between actual and ideal reasoning narrows as model size increases in the second round of thinking, indicating the strong scalability of our approach."}}
{"id": "2508.02993", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.02993", "abs": "https://arxiv.org/abs/2508.02993", "authors": ["Jiahui Bai", "Hai Dong", "A. K. Qin"], "title": "On the Fast Adaptation of Delayed Clients in Decentralized Federated Learning: A Centroid-Aligned Distillation Approach", "comment": "This paper is currently under peer review", "summary": "Decentralized Federated Learning (DFL) struggles with the slow adaptation of\nlate-joining delayed clients and high communication costs in asynchronous\nenvironments. These limitations significantly hinder overall performance. To\naddress this, we propose DFedCAD, a novel framework for rapid adaptation via\nCentroid-Aligned Distillation. DFedCAD first employs Weighted Cluster Pruning\n(WCP) to compress models into representative centroids, drastically reducing\ncommunication overhead. It then enables delayed clients to intelligently weigh\nand align with peer knowledge using a novel structural distance metric and a\ndifferentiable k-means distillation module, facilitating efficient end-to-end\nknowledge transfer. Extensive experiments on CIFAR-10, CIFAR-100, and\nTiny-ImageNet show that DFedCAD consistently achieves state-of-the-art\nperformance, attaining the highest accuracy across all evaluated settings while\nreducing communication overhead by over 86%. Our framework provides a scalable\nand practical solution for efficient decentralized learning in dynamic,\nreal-world scenarios.", "AI": {"tldr": "DFedCAD\u901a\u8fc7\u4e2d\u5fc3\u5bf9\u9f50\u84b8\u998f\u5b9e\u73b0\u5feb\u901f\u9002\u5e94\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u5206\u6563\u8054\u90a6\u5b66\u4e60\u4e2d\u5ef6\u8fdf\u52a0\u5165\u7684\u5ba2\u6237\u7aef\u548c\u9ad8\u901a\u4fe1\u6210\u672c\u95ee\u9898\u3002", "motivation": "\u5206\u6563\u8054\u90a6\u5b66\u4e60(DFL)\u5728\u5f02\u6b65\u73af\u5883\u4e2d\uff0c\u96be\u4ee5\u9002\u5e94\u5ef6\u8fdf\u52a0\u5165\u7684\u5ba2\u6237\u7aef\u548c\u9ad8\u901a\u4fe1\u6210\u672c\u3002\u8fd9\u4e9b\u9650\u5236\u4e25\u91cd\u963b\u788d\u4e86\u6574\u4f53\u6027\u80fd\u3002", "method": "DFedCAD\u9996\u5148\u91c7\u7528\u52a0\u6743\u805a\u7c7b\u526a\u679d(WCP)\u5c06\u6a21\u578b\u538b\u7f29\u6210\u5177\u6709\u4ee3\u8868\u6027\u7684\u4e2d\u5fc3\u70b9\uff0c\u4ece\u800c\u5927\u5927\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\u3002\u7136\u540e\uff0c\u5b83\u4f7f\u7528\u4e00\u79cd\u65b0\u7684\u7ed3\u6784\u8ddd\u79bb\u5ea6\u91cf\u548c\u4e00\u4e2a\u53ef\u5fae\u7684k-means\u84b8\u998f\u6a21\u5757\uff0c\u4f7f\u5ef6\u8fdf\u52a0\u5165\u7684\u5ba2\u6237\u7aef\u80fd\u591f\u667a\u80fd\u5730\u6743\u8861\u548c\u5bf9\u9f50\u5bf9\u7b49\u77e5\u8bc6\uff0c\u4ece\u800c\u4fc3\u8fdb\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u77e5\u8bc6\u8f6c\u79fb\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u548cTiny-ImageNet\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDFedCAD\u59cb\u7ec8\u5982\u4e00\u5730\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u6240\u6709\u8bc4\u4f30\u7684\u8bbe\u7f6e\u4e2d\u90fd\u83b7\u5f97\u4e86\u6700\u9ad8\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u5c06\u901a\u4fe1\u5f00\u9500\u964d\u4f4e\u4e8686%\u4ee5\u4e0a\u3002", "conclusion": "DFedCAD\u5728\u52a8\u6001\u3001\u771f\u5b9e\u4e16\u754c\u7684\u573a\u666f\u4e2d\uff0c\u4e3a\u9ad8\u6548\u7684\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u3001\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.03174", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03174", "abs": "https://arxiv.org/abs/2508.03174", "authors": ["Tian-Fang Zhao", "Wen-Xi Yang"], "title": "InqEduAgent: Adaptive AI Learning Partners with Gaussian Process Augmentation", "comment": null, "summary": "Collaborative partnership matters in inquiry-oriented education. However,\nmost study partners are selected either rely on experience-based assignments\nwith little scientific planning or build on rule-based machine assistants,\nencountering difficulties in knowledge expansion and inadequate flexibility.\nThis paper proposes an LLM-empowered agent model for simulating and selecting\nlearning partners tailored to inquiry-oriented learning, named InqEduAgent.\nGenerative agents are designed to capture cognitive and evaluative features of\nlearners in real-world scenarios. Then, an adaptive matching algorithm with\nGaussian process augmentation is formulated to identify patterns within prior\nknowledge. Optimal learning-partner matches are provided for learners facing\ndifferent exercises. The experimental results show the optimal performance of\nInqEduAgent in most knowledge-learning scenarios and LLM environment with\ndifferent levels of capabilities. This study promotes the intelligent\nallocation of human-based learning partners and the formulation of AI-based\nlearning partners. The code, data, and appendix are publicly available at\nhttps://github.com/InqEduAgent/InqEduAgent.", "AI": {"tldr": "Proposes InqEduAgent, an LLM-empowered agent model, for simulating and selecting learning partners in inquiry-oriented learning. It uses generative agents and an adaptive matching algorithm to identify optimal learning-partner matches. Experiments show its optimal performance in knowledge-learning scenarios.", "motivation": "Collaborative partnership matters in inquiry-oriented education. However, most study partners are selected either rely on experience-based assignments with little scientific planning or build on rule-based machine assistants, encountering difficulties in knowledge expansion and inadequate flexibility.", "method": "This paper proposes an LLM-empowered agent model for simulating and selecting learning partners tailored to inquiry-oriented learning, named InqEduAgent. Generative agents are designed to capture cognitive and evaluative features of learners in real-world scenarios. Then, an adaptive matching algorithm with Gaussian process augmentation is formulated to identify patterns within prior knowledge.", "result": "The experimental results show the optimal performance of InqEduAgent in most knowledge-learning scenarios and LLM environment with different levels of capabilities.", "conclusion": "This study promotes the intelligent allocation of human-based learning partners and the formulation of AI-based learning partners."}}
{"id": "2508.03079", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03079", "abs": "https://arxiv.org/abs/2508.03079", "authors": ["Zaiying Zhao", "Toshihiko Yamasaki"], "title": "Exploring Fairness across Fine-Grained Attributes in Large Vision-Language Models", "comment": "Accepted to the Responsible Generative AI (ReGenAI) Workshop, CVPR\n  2025", "summary": "The rapid expansion of applications using Large Vision-Language Models\n(LVLMs), such as GPT-4o, has raised significant concerns about their fairness.\nWhile existing studies primarily focus on demographic attributes such as race\nand gender, fairness across a broader range of attributes remains largely\nunexplored. In this study, we construct an open-set knowledge base of bias\nattributes leveraging Large Language Models (LLMs) and evaluate the fairness of\nLVLMs across finer-grained attributes. Our experimental results reveal that\nLVLMs exhibit biased outputs across a diverse set of attributes and further\ndemonstrate that cultural, environmental, and behavioral factors have a more\npronounced impact on LVLM decision-making than traditional demographic\nattributes.", "AI": {"tldr": "This study explores the fairness of LVLMs across a broader range of attributes, revealing biases beyond race and gender, especially related to cultural, environmental, and behavioral factors.", "motivation": "The rapid expansion of applications using Large Vision-Language Models (LVLMs) has raised significant concerns about their fairness. While existing studies primarily focus on demographic attributes such as race and gender, fairness across a broader range of attributes remains largely unexplored.", "method": "constructing an open-set knowledge base of bias attributes leveraging Large Language Models (LLMs) and evaluate the fairness of LVLMs across finer-grained attributes", "result": "LVLMs exhibit biased outputs across a diverse set of attributes and further demonstrate that cultural, environmental, and behavioral factors have a more pronounced impact on LVLM decision-making than traditional demographic attributes.", "conclusion": "LVLMs exhibit biased outputs across a diverse set of attributes, with cultural, environmental, and behavioral factors having a more pronounced impact than traditional demographic attributes."}}
{"id": "2508.03399", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03399", "abs": "https://arxiv.org/abs/2508.03399", "authors": ["Eliseo Bao", "Anxo P\u00e9rez", "Javier Parapar"], "title": "ReDSM5: A Reddit Dataset for DSM-5 Depression Detection", "comment": "Accepted as a resource paper at CIKM 2025", "summary": "Depression is a pervasive mental health condition that affects hundreds of\nmillions of individuals worldwide, yet many cases remain undiagnosed due to\nbarriers in traditional clinical access and pervasive stigma. Social media\nplatforms, and Reddit in particular, offer rich, user-generated narratives that\ncan reveal early signs of depressive symptomatology. However, existing\ncomputational approaches often label entire posts simply as depressed or not\ndepressed, without linking language to specific criteria from the DSM-5, the\nstandard clinical framework for diagnosing depression. This limits both\nclinical relevance and interpretability. To address this gap, we introduce\nReDSM5, a novel Reddit corpus comprising 1484 long-form posts, each\nexhaustively annotated at the sentence level by a licensed psychologist for the\nnine DSM-5 depression symptoms. For each label, the annotator also provides a\nconcise clinical rationale grounded in DSM-5 methodology. We conduct an\nexploratory analysis of the collection, examining lexical, syntactic, and\nemotional patterns that characterize symptom expression in social media\nnarratives. Compared to prior resources, ReDSM5 uniquely combines\nsymptom-specific supervision with expert explanations, facilitating the\ndevelopment of models that not only detect depression but also generate\nhuman-interpretable reasoning. We establish baseline benchmarks for both\nmulti-label symptom classification and explanation generation, providing\nreference results for future research on detection and interpretability.", "AI": {"tldr": "This paper introduces ReDSM5, a new Reddit corpus annotated for DSM-5 depression symptoms, and establishes baseline benchmarks for symptom classification and explanation generation.", "motivation": "Existing computational approaches often label entire posts simply as depressed or not depressed, without linking language to specific criteria from the DSM-5, the standard clinical framework for diagnosing depression. This limits both clinical relevance and interpretability. Depression is a pervasive mental health condition that affects hundreds of millions of individuals worldwide, yet many cases remain undiagnosed due to barriers in traditional clinical access and pervasive stigma. Social media platforms, and Reddit in particular, offer rich, user-generated narratives that can reveal early signs of depressive symptomatology.", "method": "We introduce ReDSM5, a novel Reddit corpus comprising 1484 long-form posts, each exhaustively annotated at the sentence level by a licensed psychologist for the nine DSM-5 depression symptoms. For each label, the annotator also provides a concise clinical rationale grounded in DSM-5 methodology. We conduct an exploratory analysis of the collection, examining lexical, syntactic, and emotional patterns that characterize symptom expression in social media narratives.", "result": "Compared to prior resources, ReDSM5 uniquely combines symptom-specific supervision with expert explanations, facilitating the development of models that not only detect depression but also generate human-interpretable reasoning.", "conclusion": "We establish baseline benchmarks for both multi-label symptom classification and explanation generation, providing reference results for future research on detection and interpretability."}}
{"id": "2508.03002", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03002", "abs": "https://arxiv.org/abs/2508.03002", "authors": ["Haidong Kang", "Lianbo Ma", "Guo Yu", "Shangce Gao"], "title": "Where and How to Enhance: Discovering Bit-Width Contribution for Mixed Precision Quantization", "comment": null, "summary": "Mixed precision quantization (MPQ) is an effective quantization approach to\nachieve accuracy-complexity trade-off of neural network, through assigning\ndifferent bit-widths to network activations and weights in each layer. The\ntypical way of existing MPQ methods is to optimize quantization policies (i.e.,\nbit-width allocation) in a gradient descent manner, termed as Differentiable\n(DMPQ). At the end of the search, the bit-width associated to the quantization\nparameters which has the largest value will be selected to form the final mixed\nprecision quantization policy, with the implicit assumption that the values of\nquantization parameters reflect the operation contribution to the accuracy\nimprovement. While much has been discussed about the MPQ improvement, the\nbit-width selection process has received little attention. We study this\nproblem and argue that the magnitude of quantization parameters does not\nnecessarily reflect the actual contribution of the bit-width to the task\nperformance. Then, we propose a Shapley-based MPQ (SMPQ) method, which measures\nthe bit-width operation direct contribution on the MPQ task. To reduce\ncomputation cost, a Monte Carlo sampling-based approximation strategy is\nproposed for Shapley computation. Extensive experiments on mainstream\nbenchmarks demonstrate that our SMPQ consistently achieves state-of-the-art\nperformance than gradient-based competitors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e Shapley \u7684 MPQ (SMPQ) \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8861\u91cf\u4e86\u4f4d\u5bbd\u64cd\u4f5c\u5bf9 MPQ \u4efb\u52a1\u7684\u76f4\u63a5\u8d21\u732e\u3002", "motivation": "\u73b0\u6709 MPQ \u65b9\u6cd5\u7684\u5178\u578b\u65b9\u6cd5\u662f\u4ee5\u68af\u5ea6\u4e0b\u964d\u7684\u65b9\u5f0f\u4f18\u5316\u91cf\u5316\u7b56\u7565\uff08\u5373\uff0c\u4f4d\u5bbd\u5206\u914d\uff09\uff0c\u79f0\u4e3a\u53ef\u5fae\uff08DMPQ\uff09\u3002\u5728\u641c\u7d22\u7ed3\u675f\u65f6\uff0c\u4e0e\u5177\u6709\u6700\u5927\u503c\u7684\u91cf\u5316\u53c2\u6570\u76f8\u5173\u8054\u7684\u4f4d\u5bbd\u5c06\u88ab\u9009\u62e9\u4ee5\u5f62\u6210\u6700\u7ec8\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u7b56\u7565\uff0c\u5176\u9690\u542b\u7684\u5047\u8bbe\u662f\u91cf\u5316\u53c2\u6570\u7684\u503c\u53cd\u6620\u4e86\u64cd\u4f5c\u5bf9\u7cbe\u5ea6\u63d0\u9ad8\u7684\u8d21\u732e\u3002\u867d\u7136\u5173\u4e8e MPQ \u6539\u8fdb\u5df2\u7ecf\u8ba8\u8bba\u4e86\u5f88\u591a\uff0c\u4f46\u4f4d\u5bbd\u9009\u62e9\u8fc7\u7a0b\u53d7\u5230\u7684\u5173\u6ce8\u5f88\u5c11\u3002\u6211\u4eec\u7814\u7a76\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e76\u8ba4\u4e3a\u91cf\u5316\u53c2\u6570\u7684\u5927\u5c0f\u4e0d\u4e00\u5b9a\u53cd\u6620\u4f4d\u5bbd\u5bf9\u4efb\u52a1\u6027\u80fd\u7684\u5b9e\u9645\u8d21\u732e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e Shapley \u7684 MPQ (SMPQ) \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8861\u91cf\u4e86\u4f4d\u5bbd\u64cd\u4f5c\u5bf9 MPQ \u4efb\u52a1\u7684\u76f4\u63a5\u8d21\u732e\u3002\u4e3a\u4e86\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a Shapley \u8ba1\u7b97\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8499\u7279\u5361\u7f57\u91c7\u6837\u7684\u8fd1\u4f3c\u7b56\u7565\u3002", "result": "\u6211\u4eec\u7684 SMPQ \u5728\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u57fa\u4e8e\u68af\u5ea6\u7684\u7ade\u4e89\u5bf9\u624b\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "SMPQ\u5728\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u57fa\u4e8e\u68af\u5ea6\u7684\u7ade\u4e89\u5bf9\u624b\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.03251", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03251", "abs": "https://arxiv.org/abs/2508.03251", "authors": ["Osama Mohammed", "Jiaxin Pan", "Mojtaba Nayyeri", "Daniel Hern\u00e1ndez", "Steffen Staab"], "title": "Full-History Graphs with Edge-Type Decoupled Networks for Temporal Reasoning", "comment": "European Conference of Artificial Intelligence 2025", "summary": "Modeling evolving interactions among entities is critical in many real-world\ntasks. For example, predicting driver maneuvers in traffic requires tracking\nhow neighboring vehicles accelerate, brake, and change lanes relative to one\nanother over consecutive frames. Likewise, detecting financial fraud hinges on\nfollowing the flow of funds through successive transactions as they propagate\nthrough the network. Unlike classic time-series forecasting, these settings\ndemand reasoning over who interacts with whom and when, calling for a\ntemporal-graph representation that makes both the relations and their evolution\nexplicit. Existing temporal-graph methods typically use snapshot graphs to\nencode temporal evolution. We introduce a full-history graph that instantiates\none node for every entity at every time step and separates two edge sets: (i)\nintra-time-step edges that capture relations within a single frame and (ii)\ninter-time-step edges that connect an entity to itself at consecutive steps. To\nlearn on this graph we design an Edge-Type Decoupled Network (ETDNet) with\nparallel modules: a graph-attention module aggregates information along\nintra-time-step edges, a multi-head temporal-attention module attends over an\nentity's inter-time-step history, and a fusion module combines the two messages\nafter every layer. Evaluated on driver-intention prediction (Waymo) and Bitcoin\nfraud detection (Elliptic++), ETDNet consistently surpasses strong baselines,\nlifting Waymo joint accuracy to 75.6\\% (vs. 74.1\\%) and raising Elliptic++\nillicit-class F1 to 88.1\\% (vs. 60.4\\%). These gains demonstrate the benefit of\nrepresenting structural and temporal relations as distinct edges in a single\ngraph.", "AI": {"tldr": "ETDNet\u901a\u8fc7\u5728\u65f6\u95f4\u56fe\u4e2d\u660e\u786e\u533a\u5206\u7ed3\u6784\u548c\u65f6\u95f4\u5173\u7cfb\uff0c\u5728\u9a7e\u9a76\u5458\u610f\u56fe\u9884\u6d4b\u548c\u6b3a\u8bc8\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5728\u8bb8\u591a\u5b9e\u9645\u4efb\u52a1\u4e2d\uff0c\u5bf9\u5b9e\u4f53\u4e4b\u95f4\u4e0d\u65ad\u53d1\u5c55\u7684\u4ea4\u4e92\u8fdb\u884c\u5efa\u6a21\u81f3\u5173\u91cd\u8981\u3002\u4f8b\u5982\uff0c\u9884\u6d4b\u4ea4\u901a\u4e2d\u7684\u9a7e\u9a76\u5458\u64cd\u4f5c\u9700\u8981\u8ddf\u8e2a\u76f8\u90bb\u8f66\u8f86\u5982\u4f55\u76f8\u5bf9\u4e8e\u5f7c\u6b64\u8fde\u7eed\u52a0\u901f\u3001\u5236\u52a8\u548c\u53d8\u6362\u8f66\u9053\u3002\u540c\u6837\uff0c\u68c0\u6d4b\u91d1\u878d\u6b3a\u8bc8\u53d6\u51b3\u4e8e\u8ddf\u8e2a\u8d44\u91d1\u901a\u8fc7\u8fde\u7eed\u4ea4\u6613\u5728\u7f51\u7edc\u4e2d\u4f20\u64ad\u7684\u6d41\u7a0b\u3002\u4e0e\u7ecf\u5178\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e0d\u540c\uff0c\u8fd9\u4e9b\u8bbe\u7f6e\u9700\u8981\u63a8\u7406\u8c01\u5728\u4f55\u65f6\u4e0e\u8c01\u4ea4\u4e92\uff0c\u8fd9\u9700\u8981\u4e00\u4e2a\u65f6\u95f4\u56fe\u8868\u793a\uff0c\u660e\u786e\u5173\u7cfb\u53ca\u5176\u6f14\u53d8\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u5386\u53f2\u56fe\uff0c\u8be5\u56fe\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u957f\u4e3a\u6bcf\u4e2a\u5b9e\u4f53\u5b9e\u4f8b\u5316\u4e00\u4e2a\u8282\u70b9\uff0c\u5e76\u5206\u79bb\u4e24\u4e2a\u8fb9\u96c6\uff1a\uff08i\uff09\u6355\u83b7\u5355\u4e2a\u5e27\u5185\u5173\u7cfb\u7684\u5e27\u5185\u65f6\u95f4\u6b65\u957f\u8fb9\uff0c\u4ee5\u53ca\uff08ii\uff09\u5c06\u5b9e\u4f53\u8fde\u63a5\u5230\u8fde\u7eed\u6b65\u9aa4\u7684\u5e27\u95f4\u65f6\u95f4\u6b65\u957f\u8fb9\u3002\u4e3a\u4e86\u5b66\u4e60\u8fd9\u4e2a\u56fe\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5177\u6709\u5e76\u884c\u6a21\u5757\u7684\u8fb9\u7c7b\u578b\u89e3\u8026\u7f51\u7edc\uff08ETDNet\uff09\uff1a\u4e00\u4e2a\u56fe\u6ce8\u610f\u529b\u6a21\u5757\u805a\u5408\u6cbf\u5e27\u5185\u65f6\u95f4\u6b65\u957f\u8fb9\u7684\u4fe1\u606f\uff0c\u4e00\u4e2a\u591a\u5934\u65f6\u95f4\u6ce8\u610f\u529b\u6a21\u5757\u5173\u6ce8\u5b9e\u4f53\u95f4\u7684\u5e27\u95f4\u65f6\u95f4\u6b65\u957f\u5386\u53f2\uff0c\u4e00\u4e2a\u878d\u5408\u6a21\u5757\u5728\u6bcf\u5c42\u4e4b\u540e\u7ec4\u5408\u8fd9\u4e24\u4e2a\u6d88\u606f\u3002", "result": "\u5728\u9a7e\u9a76\u5458\u610f\u56fe\u9884\u6d4b\uff08Waymo\uff09\u548c\u6bd4\u7279\u5e01\u6b3a\u8bc8\u68c0\u6d4b\uff08Elliptic++\uff09\u4e0a\u8bc4\u4f30\uff0cETDNet\u59cb\u7ec8\u8d85\u8d8a\u5f3a\u5927\u7684\u57fa\u7ebf\uff0c\u5c06Waymo\u8054\u5408\u7cbe\u5ea6\u63d0\u9ad8\u523075.6%\uff08\u5bf9\u6bd474.1%\uff09\uff0c\u5e76\u5c06Elliptic++\u975e\u6cd5\u7c7bF1\u63d0\u9ad8\u523088.1%\uff08\u5bf9\u6bd460.4%\uff09\u3002", "conclusion": "ETDNet\u5728\u9a7e\u9a76\u5458\u610f\u56fe\u9884\u6d4b\u548c\u6bd4\u7279\u5e01\u6b3a\u8bc8\u68c0\u6d4b\u65b9\u9762\u8d85\u8d8a\u4e86\u5f3a\u5927\u7684\u57fa\u7ebf\uff0c\u8bc1\u660e\u4e86\u5c06\u7ed3\u6784\u548c\u65f6\u95f4\u5173\u7cfb\u8868\u793a\u4e3a\u5355\u4e2a\u56fe\u4e2d\u4e0d\u540c\u8fb9\u7684\u4f18\u52bf\u3002"}}
{"id": "2508.03081", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03081", "abs": "https://arxiv.org/abs/2508.03081", "authors": ["Bo Zhang", "Xu Xinan", "Shuo Yan", "Yu Bai", "Zheng Zhang", "Wufan Wang", "Wendong Wang"], "title": "Contrastive Cross-Bag Augmentation for Multiple Instance Learning-based Whole Slide Image Classification", "comment": null, "summary": "Recent pseudo-bag augmentation methods for Multiple Instance Learning\n(MIL)-based Whole Slide Image (WSI) classification sample instances from a\nlimited number of bags, resulting in constrained diversity. To address this\nissue, we propose Contrastive Cross-Bag Augmentation ($C^2Aug$) to sample\ninstances from all bags with the same class to increase the diversity of\npseudo-bags. However, introducing new instances into the pseudo-bag increases\nthe number of critical instances (e.g., tumor instances). This increase results\nin a reduced occurrence of pseudo-bags containing few critical instances,\nthereby limiting model performance, particularly on test slides with small\ntumor areas. To address this, we introduce a bag-level and group-level\ncontrastive learning framework to enhance the discrimination of features with\ndistinct semantic meanings, thereby improving model performance. Experimental\nresults demonstrate that $C^2Aug$ consistently outperforms state-of-the-art\napproaches across multiple evaluation metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5bf9\u6bd4\u8de8\u5305\u589e\u5f3a (C^2Aug) \u4ee5\u63d0\u9ad8\u591a\u793a\u4f8b\u5b66\u4e60\u4e2d\u5168\u5207\u7247\u56fe\u50cf\u5206\u7c7b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u591a\u793a\u4f8b\u5b66\u4e60 (MIL) \u7684\u5168\u5207\u7247\u56fe\u50cf (WSI) \u5206\u7c7b\u7684\u4f2a\u5305\u589e\u5f3a\u65b9\u6cd5\u4ece\u6709\u9650\u6570\u91cf\u7684\u5305\u4e2d\u91c7\u6837\u5b9e\u4f8b\uff0c\u5bfc\u81f4\u591a\u6837\u6027\u53d7\u9650\u3002\u5f15\u5165\u65b0\u5b9e\u4f8b\u5230\u4f2a\u5305\u4e2d\u4f1a\u589e\u52a0\u5173\u952e\u5b9e\u4f8b\uff08\u4f8b\u5982\uff0c\u80bf\u7624\u5b9e\u4f8b\uff09\u7684\u6570\u91cf\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u5305\u542b\u5c11\u91cf\u5173\u952e\u5b9e\u4f8b\u7684\u4f2a\u5305\u7684\u51fa\u73b0\uff0c\u4ece\u800c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u80bf\u7624\u9762\u79ef\u5c0f\u7684\u6d4b\u8bd5\u5207\u7247\u4e0a\u3002", "method": "\u5bf9\u6bd4\u8de8\u5305\u589e\u5f3a (C^2Aug)\uff0c\u4ece\u6240\u6709\u5177\u6709\u76f8\u540c\u7c7b\u522b\u7684\u5305\u4e2d\u91c7\u6837\u5b9e\u4f8b\uff0c\u4ee5\u589e\u52a0\u4f2a\u5305\u7684\u591a\u6837\u6027\u3002\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u7ea7\u522b\u548c\u7ec4\u7ea7\u522b\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u5177\u6709\u4e0d\u540c\u8bed\u4e49\u542b\u4e49\u7684\u7279\u5f81\u7684\u533a\u5206\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "result": "C^2Aug \u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "Contrastive Cross-Bag Augmentation (C^2Aug) \u59cb\u7ec8\u4f18\u4e8e\u591a\u79cd\u8bc4\u4f30\u6307\u6807\u4e0b\u7684\u6700\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2508.03420", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.03420", "abs": "https://arxiv.org/abs/2508.03420", "authors": ["Bing Wang", "Ximing Li", "Yiming Wang", "Changchun Li", "Jiaxu Cui", "Renchu Guan", "Bo Yang"], "title": "Variety Is the Spice of Life: Detecting Misinformation with Dynamic Environmental Representations", "comment": "Accepted by CIKM 2025. 11 pages, 4 figures. Code:\n  https://github.com/wangbing1416/MISDER", "summary": "The proliferation of misinformation across diverse social media platforms has\ndrawn significant attention from both academic and industrial communities due\nto its detrimental effects. Accordingly, automatically distinguishing\nmisinformation, dubbed as Misinformation Detection (MD), has become an\nincreasingly active research topic. The mainstream methods formulate MD as a\nstatic learning paradigm, which learns the mapping between the content, links,\nand propagation of news articles and the corresponding manual veracity labels.\nHowever, the static assumption is often violated, since in real-world\nscenarios, the veracity of news articles may vacillate within the dynamically\nevolving social environment. To tackle this problem, we propose a novel\nframework, namely Misinformation detection with Dynamic Environmental\nRepresentations (MISDER). The basic idea of MISDER lies in learning a social\nenvironmental representation for each period and employing a temporal model to\npredict the representation for future periods. In this work, we specify the\ntemporal model as the LSTM model, continuous dynamics equation, and pre-trained\ndynamics system, suggesting three variants of MISDER, namely MISDER-LSTM,\nMISDER-ODE, and MISDER-PT, respectively. To evaluate the performance of MISDER,\nwe compare it to various MD baselines across 2 prevalent datasets, and the\nexperimental results can indicate the effectiveness of our proposed model.", "AI": {"tldr": "MISDER is proposed to address the problem of dynamically changing veracity of news articles in misinformation detection. It learns social environmental representations and uses a temporal model to predict future representations, outperforming existing methods.", "motivation": "The static assumption of mainstream misinformation detection (MD) methods is often violated because the veracity of news articles may vacillate within the dynamically evolving social environment.", "method": "A novel framework, MISDER, is proposed to learn social environmental representations for each period and employ a temporal model (LSTM, continuous dynamics equation, and pre-trained dynamics system) to predict the representation for future periods.", "result": "MISDER outperforms various MD baselines across 2 prevalent datasets.", "conclusion": "The experimental results demonstrate the effectiveness of the proposed MISDER model for misinformation detection."}}
{"id": "2508.03042", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03042", "abs": "https://arxiv.org/abs/2508.03042", "authors": ["Ruixing Zhang", "Bo Wang", "Tongyu Zhu", "Leilei Sun", "Weifeng Lv"], "title": "Urban In-Context Learning: Bridging Pretraining and Inference through Masked Diffusion for Urban Profiling", "comment": null, "summary": "Urban profiling aims to predict urban profiles in unknown regions and plays a\ncritical role in economic and social censuses. Existing approaches typically\nfollow a two-stage paradigm: first, learning representations of urban areas;\nsecond, performing downstream prediction via linear probing, which originates\nfrom the BERT era. Inspired by the development of GPT style models, recent\nstudies have shown that novel self-supervised pretraining schemes can endow\nmodels with direct applicability to downstream tasks, thereby eliminating the\nneed for task-specific fine-tuning. This is largely because GPT unifies the\nform of pretraining and inference through next-token prediction. However, urban\ndata exhibit structural characteristics that differ fundamentally from\nlanguage, making it challenging to design a one-stage model that unifies both\npretraining and inference. In this work, we propose Urban In-Context Learning,\na framework that unifies pretraining and inference via a masked autoencoding\nprocess over urban regions. To capture the distribution of urban profiles, we\nintroduce the Urban Masked Diffusion Transformer, which enables each region' s\nprediction to be represented as a distribution rather than a deterministic\nvalue. Furthermore, to stabilize diffusion training, we propose the Urban\nRepresentation Alignment Mechanism, which regularizes the model's intermediate\nfeatures by aligning them with those from classical urban profiling methods.\nExtensive experiments on three indicators across two cities demonstrate that\nour one-stage method consistently outperforms state-of-the-art two-stage\napproaches. Ablation studies and case studies further validate the\neffectiveness of each proposed module, particularly the use of diffusion\nmodeling.", "AI": {"tldr": "\u63d0\u51fa Urban In-Context Learning \u6846\u67b6\uff0c\u7528\u5355\u9636\u6bb5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u4e24\u9636\u6bb5\u57ce\u5e02\u5256\u9762\u5206\u6790\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57ce\u5e02\u5256\u9762\u5206\u6790\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u4e24\u9636\u6bb5\u8303\u5f0f\uff0c\u4f9d\u8d56\u4e8e BERT \u65f6\u4ee3\u7684\u7ebf\u6027\u63a2\u6d4b\uff0c\u4e14\u57ce\u5e02\u6570\u636e\u7ed3\u6784\u4e0e\u8bed\u8a00\u5b58\u5728\u6839\u672c\u5dee\u5f02\uff0c\u96be\u4ee5\u8bbe\u8ba1\u7edf\u4e00\u9884\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u5355\u9636\u6bb5\u6a21\u578b\u3002", "method": "\u63d0\u51fa Urban In-Context Learning \u6846\u67b6\uff0c\u901a\u8fc7\u57ce\u5e02\u63a9\u7801\u6269\u6563 Transformer \u5b9e\u73b0\u533a\u57df\u9884\u6d4b\u7684\u5206\u5e03\u8868\u793a\uff0c\u5e76\u63d0\u51fa Urban Representation Alignment Mechanism \u4ee5\u7a33\u5b9a\u6269\u6563\u8bad\u7ec3\u3002", "result": "\u5728\u4e24\u4e2a\u57ce\u5e02\u7684\u4e09\u4e2a\u6307\u6807\u4e0a\uff0c\u8be5\u5355\u9636\u6bb5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u548c\u6848\u4f8b\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6bcf\u4e2a\u6a21\u5757\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u6269\u6563\u5efa\u6a21\u7684\u5e94\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Urban In-Context Learning \u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u57ce\u5e02\u533a\u57df\u7684\u63a9\u7801\u81ea\u52a8\u7f16\u7801\u8fc7\u7a0b\u7edf\u4e00\u9884\u8bad\u7ec3\u548c\u63a8\u7406\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u57ce\u5e02\u7684\u4e09\u4e2a\u6307\u6807\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u3002"}}
{"id": "2508.03284", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03284", "abs": "https://arxiv.org/abs/2508.03284", "authors": ["Shaofeng Yin", "Ting Lei", "Yang Liu"], "title": "ToolVQA: A Dataset for Multi-step Reasoning VQA with External Tools", "comment": null, "summary": "Integrating external tools into Large Foundation Models (LFMs) has emerged as\na promising approach to enhance their problem-solving capabilities. While\nexisting studies have demonstrated strong performance in tool-augmented Visual\nQuestion Answering (VQA), recent benchmarks reveal significant gaps in\nreal-world tool-use proficiency, particularly in functionally diverse\nmultimodal settings requiring multi-step reasoning. In this work, we introduce\nToolVQA, a large-scale multimodal dataset comprising 23K instances, designed to\nbridge this gap. Unlike previous datasets that rely on synthetic scenarios and\nsimplified queries, ToolVQA features real-world visual contexts and challenging\nimplicit multi-step reasoning tasks, better aligning with real user\ninteractions. To construct this dataset, we propose ToolEngine, a novel data\ngeneration pipeline that employs Depth-First Search (DFS) with a dynamic\nin-context example matching mechanism to simulate human-like tool-use\nreasoning. ToolVQA encompasses 10 multimodal tools across 7 diverse task\ndomains, with an average inference length of 2.78 reasoning steps per instance.\nThe fine-tuned 7B LFMs on ToolVQA not only achieve impressive performance on\nour test set but also surpass the large close-sourced model GPT-3.5-turbo on\nvarious out-of-distribution (OOD) datasets, demonstrating strong\ngeneralizability to real-world tool-use scenarios.", "AI": {"tldr": "ToolVQA \u662f\u4e00\u4e2a\u5927\u578b\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b 23K \u4e2a\u5b9e\u4f8b\uff0c\u65e8\u5728\u5f25\u5408\u5b9e\u9645\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u65b9\u9762\u7684\u5dee\u8ddd\u3002", "motivation": "\u5c06\u5916\u90e8\u5de5\u5177\u96c6\u6210\u5230\u5927\u578b\u57fa\u7840\u6a21\u578b (LFM) \u4e2d\u5df2\u6210\u4e3a\u4e00\u79cd\u5f88\u6709\u524d\u9014\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u589e\u5f3a\u5176\u89e3\u51b3\u95ee\u9898\u7684\u80fd\u529b\u3002\u867d\u7136\u73b0\u6709\u7684\u7814\u7a76\u5df2\u7ecf\u8bc1\u660e\u4e86\u5728\u5de5\u5177\u589e\u5f3a\u578b\u89c6\u89c9\u95ee\u7b54 (VQA) \u65b9\u9762\u7684\u5f3a\u5927\u6027\u80fd\uff0c\u4f46\u6700\u8fd1\u7684\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u5728\u5b9e\u9645\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u65b9\u9762\u7684\u91cd\u5927\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u591a\u6b65\u9aa4\u63a8\u7406\u7684\u529f\u80fd\u591a\u6837\u7684\u591a\u6a21\u5f0f\u8bbe\u7f6e\u4e2d\u3002", "method": "\u6211\u4eec\u63d0\u51fa ToolEngine\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u5b83\u91c7\u7528\u6df1\u5ea6\u4f18\u5148\u641c\u7d22 (DFS) \u548c\u52a8\u6001\u4e0a\u4e0b\u6587\u793a\u4f8b\u5339\u914d\u673a\u5236\u6765\u6a21\u62df\u7c7b\u4eba\u5de5\u5177\u4f7f\u7528\u63a8\u7406\u3002", "result": "\u5728 ToolVQA \u4e0a\u5fae\u8c03\u7684 7B LFM \u4e0d\u4ec5\u5728\u6211\u4eec\u7684\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\uff0c\u800c\u4e14\u5728\u5404\u79cd\u5206\u5e03\u5916 (OOD) \u6570\u636e\u96c6\u4e0a\u8d85\u8fc7\u4e86\u5927\u578b\u5c01\u95ed\u6e90\u6a21\u578b GPT-3.5-turbo\uff0c\u5c55\u793a\u4e86\u5bf9\u771f\u5b9e\u4e16\u754c\u5de5\u5177\u4f7f\u7528\u573a\u666f\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5fae\u8c03\u540e\u76847B LFM \u5728 ToolVQA \u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u5404\u79cd\u5206\u5e03\u5916 (OOD) \u6570\u636e\u96c6\u4e0a\u8d85\u8fc7\u4e86\u5927\u578b\u5c01\u95ed\u6e90\u6a21\u578b GPT-3.5-turbo\uff0c\u5c55\u793a\u4e86\u5bf9\u771f\u5b9e\u4e16\u754c\u5de5\u5177\u4f7f\u7528\u573a\u666f\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.03094", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03094", "abs": "https://arxiv.org/abs/2508.03094", "authors": ["Jiantao Tan", "Peixian Ma", "Kanghao Chen", "Zhiming Dai", "Ruixuan Wang"], "title": "Augmenting Continual Learning of Diseases with LLM-Generated Visual Concepts", "comment": null, "summary": "Continual learning is essential for medical image classification systems to\nadapt to dynamically evolving clinical environments. The integration of\nmultimodal information can significantly enhance continual learning of image\nclasses. However, while existing approaches do utilize textual modality\ninformation, they solely rely on simplistic templates with a class name,\nthereby neglecting richer semantic information. To address these limitations,\nwe propose a novel framework that harnesses visual concepts generated by large\nlanguage models (LLMs) as discriminative semantic guidance. Our method\ndynamically constructs a visual concept pool with a similarity-based filtering\nmechanism to prevent redundancy. Then, to integrate the concepts into the\ncontinual learning process, we employ a cross-modal image-concept attention\nmodule, coupled with an attention loss. Through attention, the module can\nleverage the semantic knowledge from relevant visual concepts and produce\nclass-representative fused features for classification. Experiments on medical\nand natural image datasets show our method achieves state-of-the-art\nperformance, demonstrating the effectiveness and superiority of our method. We\nwill release the code publicly.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u89c6\u89c9\u6982\u5ff5\u4f5c\u4e3a\u5224\u522b\u8bed\u4e49\u6307\u5bfc\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u5e76\u5728\u533b\u7597\u548c\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u7cfb\u7edf\u9700\u8981\u6301\u7eed\u5b66\u4e60\u624d\u80fd\u9002\u5e94\u52a8\u6001\u53d1\u5c55\u7684\u4e34\u5e8a\u73af\u5883\u3002\u591a\u6a21\u6001\u4fe1\u606f\u7684\u6574\u5408\u53ef\u4ee5\u663e\u8457\u589e\u5f3a\u56fe\u50cf\u7c7b\u522b\u7684\u6301\u7eed\u5b66\u4e60\u3002\u7136\u800c\uff0c\u867d\u7136\u73b0\u6709\u65b9\u6cd5\u786e\u5b9e\u5229\u7528\u4e86\u6587\u672c\u6a21\u6001\u4fe1\u606f\uff0c\u4f46\u5b83\u4eec\u4ec5\u4f9d\u8d56\u4e8e\u5e26\u6709\u7c7b\u540d\u79f0\u7684\u7b80\u5355\u6a21\u677f\uff0c\u4ece\u800c\u5ffd\u7565\u4e86\u66f4\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u751f\u6210\u7684\u89c6\u89c9\u6982\u5ff5\u4f5c\u4e3a\u5224\u522b\u8bed\u4e49\u6307\u5bfc\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u52a8\u6001\u6784\u5efa\u4e86\u4e00\u4e2a\u5177\u6709\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u8fc7\u6ee4\u673a\u5236\u7684\u89c6\u89c9\u6982\u5ff5\u6c60\uff0c\u4ee5\u9632\u6b62\u5197\u4f59\u3002\u7136\u540e\uff0c\u4e3a\u4e86\u5c06\u6982\u5ff5\u6574\u5408\u5230\u6301\u7eed\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u91c7\u7528\u4e86\u4e00\u79cd\u8de8\u6a21\u6001\u56fe\u50cf-\u6982\u5ff5\u6ce8\u610f\u6a21\u5757\uff0c\u5e76\u7ed3\u5408\u4e86\u6ce8\u610f\u635f\u5931\u3002\u901a\u8fc7\u6ce8\u610f\uff0c\u8be5\u6a21\u5757\u53ef\u4ee5\u5229\u7528\u6765\u81ea\u76f8\u5173\u89c6\u89c9\u6982\u5ff5\u7684\u8bed\u4e49\u77e5\u8bc6\uff0c\u5e76\u4ea7\u751f\u7528\u4e8e\u5206\u7c7b\u7684\u5177\u6709\u7c7b\u4ee3\u8868\u6027\u7684\u878d\u5408\u7279\u5f81\u3002", "result": "\u5728\u533b\u7597\u548c\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u533b\u7597\u548c\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.03440", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03440", "abs": "https://arxiv.org/abs/2508.03440", "authors": ["Junhong Wu", "Jinliang Lu", "Zixuan Ren", "Ganqiang Hu", "Zhi Wu", "Dai Dai", "Hua Wu"], "title": "LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models", "comment": "10 pages, 7 figures, working in progress", "summary": "Human cognition naturally engages with abstract and fluid concepts, whereas\nexisting reasoning models often rely on generating discrete tokens, potentially\nconstraining their expressive capabilities. Recent advancements aim to address\nthis limitation by enabling large language models (LLMs) to generate soft,\nabstract tokens, thus facilitating reasoning within a continuous concept space.\nThis paper explores the `Soft Thinking' capabilities of various LLMs by\nexamining the models' internal behavior using a suite of probing techniques.\nContrary to the common belief that Soft Thinking enables the simultaneous\nexploration of diverse reasoning paths, our findings reveal that LLMs\npredominantly rely on the most influential component of the soft inputs during\nsubsequent decoding steps. This reliance hinders the exploration of different\nreasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding,\nobscuring the advantage of transmitting more information through Soft Tokens.\nTo tackle this issue, we explore sampling strategies to introduce\n\\emph{randomness}, employing methods such as Dirichlet resampling and the\nGumbel-Softmax trick. Our experiments demonstrate that incorporating randomness\ncan alleviate the limitations of vanilla approaches and unleash the potential\nof Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate\nrandomness with controlled smoothness, resulting in superior performance across\neight reasoning benchmarks.", "AI": {"tldr": "LLMs \u867d\u7136\u5177\u5907\u8f6f\u601d\u8003\u80fd\u529b\uff0c\u4f46\u503e\u5411\u4e8e\u4f9d\u8d56\u6700\u4e3b\u8981\u7684\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u63a8\u7406\u8def\u5f84\u7684\u591a\u6837\u6027\u3002\u901a\u8fc7\u5f15\u5165\u968f\u673a\u6027\uff0c\u7279\u522b\u662f Gumbel-Softmax \u6280\u5de7\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u8f6f\u601d\u8003\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u7406\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u4e8e\u751f\u6210\u79bb\u6563 tokens\uff0c\u8fd9\u53ef\u80fd\u4f1a\u9650\u5236\u5176\u8868\u8fbe\u80fd\u529b\u3002\u6700\u8fd1\u7684\u8fdb\u5c55\u65e8\u5728\u901a\u8fc7\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u80fd\u591f\u751f\u6210\u8f6f\u7684\u3001\u62bd\u8c61\u7684 tokens \u6765\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u4ece\u800c\u4fc3\u8fdb\u5728\u8fde\u7eed\u6982\u5ff5\u7a7a\u95f4\u5185\u7684\u63a8\u7406\u3002", "method": "\u901a\u8fc7\u4f7f\u7528\u4e00\u5957\u63a2\u6d4b\u6280\u672f\u68c0\u67e5\u6a21\u578b\u7684\u5185\u90e8\u884c\u4e3a\u6765\u63a2\u7d22\u5404\u79cd LLM \u7684\u201c\u8f6f\u601d\u8003\u201d\u80fd\u529b\u3002", "result": "LLMs \u4e3b\u8981\u4f9d\u8d56\u4e8e\u8f6f\u8f93\u5165\u4e2d\u6700\u6709\u5f71\u54cd\u529b\u7684\u90e8\u5206\uff0c\u8fd9\u963b\u788d\u4e86\u4e0d\u540c\u63a8\u7406\u8def\u5f84\u7684\u63a2\u7d22\uff0c\u5e76\u5c06 vanilla \u8f6f\u601d\u8003\u7b80\u5316\u4e3a\u4e00\u79cd\u8d2a\u5a6a\u89e3\u7801\u7684\u5f62\u5f0f\u3002\u5f15\u5165\u968f\u673a\u6027\u53ef\u4ee5\u7f13\u89e3 vanilla \u65b9\u6cd5\u7684\u5c40\u9650\u6027\u5e76\u91ca\u653e\u8f6f\u601d\u8003\u7684\u6f5c\u529b\u3002Gumbel-Softmax \u6280\u5de7\u63d0\u4f9b\u4e86\u8db3\u591f\u7684\u968f\u673a\u6027\u4e0e\u53d7\u63a7\u7684\u5e73\u6ed1\u6027\uff0c\u4ece\u800c\u5728\u516b\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "LLMs\u5728\u540e\u7eed\u89e3\u7801\u6b65\u9aa4\u4e2d\u4e3b\u8981\u4f9d\u8d56\u4e8e\u8f6f\u8f93\u5165\u4e2d\u6700\u6709\u5f71\u54cd\u529b\u7684\u90e8\u5206\uff0c\u8fd9\u963b\u788d\u4e86\u4e0d\u540c\u63a8\u7406\u8def\u5f84\u7684\u63a2\u7d22\u3002\u5f15\u5165\u968f\u673a\u6027\u7684\u62bd\u6837\u7b56\u7565\u53ef\u4ee5\u7f13\u89e3 vanilla \u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u91ca\u653e\u8f6f\u601d\u8003\u7684\u6f5c\u529b\u3002Gumbel-Softmax \u6280\u5de7\u63d0\u4f9b\u4e86\u8db3\u591f\u7684\u968f\u673a\u6027\u4e0e\u53d7\u63a7\u7684\u5e73\u6ed1\u6027\uff0c\u4ece\u800c\u5728\u516b\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2508.03046", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03046", "abs": "https://arxiv.org/abs/2508.03046", "authors": ["Tatwadarshi P Nagarhalli", "Sanket Patil", "Vishal Pande", "Uday Aswalekar", "Prafulla Patil"], "title": "A Novel Multimodal Framework for Early Detection of Alzheimers Disease Using Deep Learning", "comment": "Journal paper, 14 pages", "summary": "Alzheimers Disease (AD) is a progressive neurodegenerative disorder that\nposes significant challenges in its early diagnosis, often leading to delayed\ntreatment and poorer outcomes for patients. Traditional diagnostic methods,\ntypically reliant on single data modalities, fall short of capturing the\nmultifaceted nature of the disease. In this paper, we propose a novel\nmultimodal framework for the early detection of AD that integrates data from\nthree primary sources: MRI imaging, cognitive assessments, and biomarkers. This\nframework employs Convolutional Neural Networks (CNN) for analyzing MRI images\nand Long Short-Term Memory (LSTM) networks for processing cognitive and\nbiomarker data. The system enhances diagnostic accuracy and reliability by\naggregating results from these distinct modalities using advanced techniques\nlike weighted averaging, even in incomplete data. The multimodal approach not\nonly improves the robustness of the detection process but also enables the\nidentification of AD at its earliest stages, offering a significant advantage\nover conventional methods. The integration of biomarkers and cognitive tests is\nparticularly crucial, as these can detect Alzheimer's long before the onset of\nclinical symptoms, thereby facilitating earlier intervention and potentially\naltering the course of the disease. This research demonstrates that the\nproposed framework has the potential to revolutionize the early detection of\nAD, paving the way for more timely and effective treatments", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8eAD\u65e9\u671f\u68c0\u6d4b\u7684\u65b0\u578b\u591a\u6a21\u6001\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u6765\u81ea\u4e09\u4e2a\u4e3b\u8981\u6765\u6e90\u7684\u6570\u636e\uff1aMRI\u6210\u50cf\u3001\u8ba4\u77e5\u8bc4\u4f30\u548c\u751f\u7269\u6807\u5fd7\u7269\u3002", "motivation": "\u4f20\u7edf\u8bca\u65ad\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u5355\u4e00\u6570\u636e\u6a21\u6001\uff0c\u65e0\u6cd5\u6355\u6349\u75be\u75c5\u7684\u591a\u65b9\u9762\u6027\u8d28\uff0c\u5bf9\u65e9\u671f\u8bca\u65ad\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u7ecf\u5e38\u5bfc\u81f4\u5ef6\u8bef\u6cbb\u7597\u548c\u60a3\u8005\u9884\u540e\u8f83\u5dee\u3002", "method": "\u5229\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5206\u6790MRI\u56fe\u50cf\uff0c\u5229\u7528\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u7f51\u7edc\u5904\u7406\u8ba4\u77e5\u548c\u751f\u7269\u6807\u5fd7\u7269\u6570\u636e\u3002\u901a\u8fc7\u4f7f\u7528\u52a0\u6743\u5e73\u5747\u7b49\u5148\u8fdb\u6280\u672f\u6c47\u603b\u6765\u81ea\u8fd9\u4e9b\u4e0d\u540c\u6a21\u6001\u7684\u7ed3\u679c\uff0c\u5373\u4f7f\u5728\u4e0d\u5b8c\u6574\u7684\u6570\u636e\u4e2d\uff0c\u8be5\u7cfb\u7edf\u4e5f\u80fd\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "result": "\u591a\u6a21\u6001\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u68c0\u6d4b\u8fc7\u7a0b\u7684\u7a33\u5065\u6027\uff0c\u800c\u4e14\u80fd\u591f\u5728\u6700\u65e9\u9636\u6bb5\u8bc6\u522bAD\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u663e\u7740\u4f18\u52bf\u3002\u751f\u7269\u6807\u5fd7\u7269\u548c\u8ba4\u77e5\u6d4b\u8bd5\u7684\u6574\u5408\u5c24\u4e3a\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u4eec\u53ef\u4ee5\u5728\u4e34\u5e8a\u75c7\u72b6\u51fa\u73b0\u4e4b\u524d\u5f88\u4e45\u5c31\u68c0\u6d4b\u5230\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff0c\u4ece\u800c\u4fc3\u8fdb\u66f4\u65e9\u7684\u5e72\u9884\u5e76\u53ef\u80fd\u6539\u53d8\u75be\u75c5\u7684\u8fdb\u7a0b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u5f7b\u5e95\u6539\u53d8AD\u65e9\u671f\u68c0\u6d4b\u7684\u6f5c\u529b\uff0c\u4e3a\u66f4\u53ca\u65f6\u6709\u6548\u7684\u6cbb\u7597\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2508.03341", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03341", "abs": "https://arxiv.org/abs/2508.03341", "authors": ["Jiayan Nan", "Wenquan Ma", "Wenlong Wu", "Yize Chen"], "title": "Nemori: Self-Organizing Agent Memory Inspired by Cognitive Science", "comment": null, "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet their\ninability to maintain persistent memory in long contexts limits their\neffectiveness as autonomous agents in long-term interactions. While existing\nmemory systems have made progress, their reliance on arbitrary granularity for\ndefining the basic memory unit and passive, rule-based mechanisms for knowledge\nextraction limits their capacity for genuine learning and evolution. To address\nthese foundational limitations, we present Nemori, a novel self-organizing\nmemory architecture inspired by human cognitive principles. Nemori's core\ninnovation is twofold: First, its Two-Step Alignment Principle, inspired by\nEvent Segmentation Theory, provides a principled, top-down method for\nautonomously organizing the raw conversational stream into semantically\ncoherent episodes, solving the critical issue of memory granularity. Second,\nits Predict-Calibrate Principle, inspired by the Free-energy Principle, enables\nthe agent to proactively learn from prediction gaps, moving beyond pre-defined\nheuristics to achieve adaptive knowledge evolution. This offers a viable path\ntoward handling the long-term, dynamic workflows of autonomous agents.\nExtensive experiments on the LoCoMo and LongMemEval benchmarks demonstrate that\nNemori significantly outperforms prior state-of-the-art systems, with its\nadvantage being particularly pronounced in longer contexts.", "AI": {"tldr": "Nemori\u662f\u4e00\u79cd\u65b0\u578b\u8bb0\u5fc6\u67b6\u6784\uff0c\u7075\u611f\u6765\u81ea\u4eba\u7c7b\u8ba4\u77e5\u539f\u5219\uff0c\u901a\u8fc7\u81ea\u7ec4\u7ec7\u548c\u4e3b\u52a8\u5b66\u4e60\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u65e0\u6cd5\u7ef4\u6301\u6301\u4e45\u8bb0\u5fc6\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u4f5c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u7684\u6548\u80fd\u3002\u73b0\u6709\u7684\u8bb0\u5fc6\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u4efb\u610f\u7c92\u5ea6\u6765\u5b9a\u4e49\u57fa\u672c\u8bb0\u5fc6\u5355\u5143\uff0c\u5e76\u4e14\u91c7\u7528\u88ab\u52a8\u7684\u3001\u57fa\u4e8e\u89c4\u5219\u7684\u77e5\u8bc6\u63d0\u53d6\u673a\u5236\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u8fdb\u884c\u771f\u6b63\u5b66\u4e60\u548c\u8fdb\u5316\u7684\u80fd\u529b\u3002", "method": "Nemori\uff0c\u4e00\u79cd\u53d7\u4eba\u7c7b\u8ba4\u77e5\u539f\u5219\u542f\u53d1\u7684\u65b0\u578b\u81ea\u7ec4\u7ec7\u8bb0\u5fc6\u67b6\u6784\uff0c\u5177\u6709\u53cc\u91cd\u521b\u65b0\uff1a\u4e24\u6b65\u5bf9\u9f50\u539f\u5219\u548c\u9884\u6d4b-\u6821\u51c6\u539f\u5219\u3002", "result": "Nemori\u663e\u8457\u4f18\u4e8e\u5148\u524d\u7684\u6700\u5148\u8fdb\u7cfb\u7edf\uff0c\u5c24\u5176\u662f\u5728\u8f83\u957f\u7684\u4e0a\u4e0b\u6587\u4e2d\u4f18\u52bf\u66f4\u52a0\u660e\u663e\u3002", "conclusion": "Nemori\u5728LoCoMo\u548cLongMemEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u5148\u524d\u7684\u6700\u5148\u8fdb\u7cfb\u7edf\uff0c\u5c24\u5176\u662f\u5728\u8f83\u957f\u7684\u4e0a\u4e0b\u6587\u4e2d\u4f18\u52bf\u66f4\u52a0\u660e\u663e\u3002"}}
{"id": "2508.03100", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03100", "abs": "https://arxiv.org/abs/2508.03100", "authors": ["Yogesh Kulkarni", "Pooyan Fazli"], "title": "AVATAR: Reinforcement Learning to See, Hear, and Reason Over Video", "comment": null, "summary": "Multimodal reasoning over long-horizon video is challenging due to the need\nfor precise spatiotemporal fusion and alignment across modalities. While recent\nmethods such as Group Relative Policy Optimization (GRPO) have shown promise in\nthis domain, they suffer from three key limitations: (1) data inefficiency from\ntheir on-policy design, (2) a vanishing advantage problem, where identical or\nnear-identical rewards within a group eliminate the learning signal by\nproducing zero-valued advantages, and (3) uniform credit assignment that fails\nto emphasize critical reasoning steps. We introduce AVATAR (Audio-Video Agent\nfor Alignment and Reasoning), a framework that addresses these limitations\nthrough two core components: (1) an off-policy training architecture that\nimproves sample efficiency and resolves vanishing advantages by reusing past\nexperiences with greater reward diversity, and (2) Temporal Advantage Shaping\n(TAS), a novel credit assignment strategy that upweights key reasoning phases\nduring learning. AVATAR achieves strong performance across various benchmarks,\noutperforming the Qwen2.5-Omni baseline by +5.4on MMVU, +4.9 on OmniBench, and\n+4.5 on Video-Holmes, while demonstrating over 35% higher sample efficiency.", "AI": {"tldr": "AVATAR \u89e3\u51b3\u4e86\u591a\u6a21\u6001\u957f\u7a0b\u89c6\u9891\u63a8\u7406\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u975e\u7b56\u7565\u8bad\u7ec3\u548c\u65f6\u95f4\u4f18\u52bf\u5851\u9020\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u9700\u8981\u5728\u8de8\u6a21\u5f0f\u4e2d\u8fdb\u884c\u7cbe\u786e\u7684\u65f6\u7a7a\u878d\u5408\u548c\u5bf9\u9f50\uff0c\u56e0\u6b64\u5bf9\u957f\u7a0b\u89c6\u9891\u8fdb\u884c\u591a\u6a21\u5f0f\u63a8\u7406\u5177\u6709\u6311\u6218\u6027\u3002\u867d\u7136\u6700\u8fd1\u7684\u65b9\u6cd5\uff08\u4f8b\u5982 Group Relative Policy Optimization (GRPO)\uff09\u5728\u8be5\u9886\u57df\u663e\u793a\u51fa\u5e0c\u671b\uff0c\u4f46\u5b83\u4eec\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u5c40\u9650\u6027\uff1a(1) \u6765\u81ea\u5176\u5728\u7b56\u7565\u8bbe\u8ba1\u4e2d\u7684\u6570\u636e\u6548\u7387\u4f4e\u4e0b\uff0c(2) \u4f18\u52bf\u6d88\u5931\u95ee\u9898\uff0c\u5176\u4e2d\u7ec4\u5185\u76f8\u540c\u6216\u51e0\u4e4e\u76f8\u540c\u7684\u5956\u52b1\u901a\u8fc7\u4ea7\u751f\u96f6\u503c\u4f18\u52bf\u6765\u6d88\u9664\u5b66\u4e60\u4fe1\u53f7\uff0c\u4ee5\u53ca (3) \u672a\u80fd\u5f3a\u8c03\u5173\u952e\u63a8\u7406\u6b65\u9aa4\u7684\u7edf\u4e00\u4fe1\u7528\u5206\u914d\u3002", "method": "AVATAR \u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1) \u4e00\u79cd\u975e\u7b56\u7565\u8bad\u7ec3\u67b6\u6784\uff0c\u901a\u8fc7\u91cd\u7528\u5177\u6709\u66f4\u9ad8\u5956\u52b1\u591a\u6837\u6027\u7684\u8fc7\u53bb\u7ecf\u9a8c\u6765\u63d0\u9ad8\u6837\u672c\u6548\u7387\u5e76\u89e3\u51b3\u4f18\u52bf\u6d88\u5931\u95ee\u9898\uff0c\u4ee5\u53ca (2) \u65f6\u95f4\u4f18\u52bf\u5851\u9020 (TAS)\uff0c\u4e00\u79cd\u65b0\u7684\u4fe1\u7528\u5206\u914d\u7b56\u7565\uff0c\u53ef\u5728\u5b66\u4e60\u671f\u95f4\u589e\u52a0\u5173\u952e\u63a8\u7406\u9636\u6bb5\u7684\u6743\u91cd\u3002", "result": "AVATAR \u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u51fa\u8272\u7684\u6027\u80fd\uff0c\u5728 MMVU \u4e0a\u8d85\u8fc7 Qwen2.5-Omni \u57fa\u7ebf +5.4\uff0c\u5728 OmniBench \u4e0a\u8d85\u8fc7 +4.9\uff0c\u5728 Video-Holmes \u4e0a\u8d85\u8fc7 +4.5\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u8d85\u8fc7 35% \u7684\u66f4\u9ad8\u6837\u672c\u6548\u7387\u3002", "conclusion": "AVATAR\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728 MMVU \u4e0a\u8d85\u8fc7 Qwen2.5-Omni \u57fa\u7ebf +5.4\uff0c\u5728 OmniBench \u4e0a\u8d85\u8fc7 +4.9\uff0c\u5728 Video-Holmes \u4e0a\u8d85\u8fc7 +4.5\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u8d85\u8fc7 35% \u7684\u66f4\u9ad8\u6837\u672c\u6548\u7387\u3002"}}
{"id": "2508.03453", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03453", "abs": "https://arxiv.org/abs/2508.03453", "authors": ["Rita Gonz\u00e1lez-M\u00e1rquez", "Philipp Berens", "Dmitry Kobak"], "title": "Cropping outperforms dropout as an augmentation strategy for training self-supervised text embeddings", "comment": null, "summary": "Text embeddings, i.e. vector representations of entire texts, play an\nimportant role in many NLP applications, such as retrieval-augmented\ngeneration, sentiment analysis, clustering, or visualizing collections of texts\nfor data exploration. Currently, top-performing embedding models are derived\nfrom pre-trained language models via extensive supervised fine-tuning using\ncurated text pairs. This contrasts with computer vision, where self-supervised\ntraining based on data augmentations has demonstrated remarkable success. Here\nwe systematically compare the two most well-known augmentation strategies for\npositive pair generation in contrastive learning of text embeddings. We assess\nembedding quality on MTEB and additional in-domain evaluations and show that\ncropping augmentation strongly outperforms the dropout-based approach. We find\nthat on out-of-domain data, the quality of resulting embeddings is below the\nsupervised SOTA models, but for in-domain data, self-supervised fine-tuning\nproduces high-quality text embeddings after very short fine-tuning, sometimes\nonly marginally below the supervised SOTA. Finally, we show that representation\nquality increases towards the last transformer layers, which undergo the\nlargest change during fine-tuning; and that fine-tuning only those last layers\nis sufficient to reach similar embedding quality.", "AI": {"tldr": "\u5bf9\u6bd4\u4e86\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u6b63\u5bf9\u751f\u6210\u5bf9\u7684\u4e24\u79cd\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u53d1\u73b0\u88c1\u526a\u589e\u5f3a\u4f18\u4e8e\u57fa\u4e8e dropout \u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u81ea\u76d1\u7763\u5fae\u8c03\u53ef\u4ee5\u5728\u77ed\u65f6\u95f4\u5185\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u5d4c\u5165\u3002", "motivation": "\u6587\u672c\u5d4c\u5165\u5728\u8bb8\u591a NLP \u5e94\u7528\u4e2d\u8d77\u7740\u91cd\u8981\u4f5c\u7528\u3002\u76ee\u524d\uff0c\u6027\u80fd\u6700\u4f73\u7684\u5d4c\u5165\u6a21\u578b\u662f\u901a\u8fc7\u4f7f\u7528\u7cbe\u9009\u7684\u6587\u672c\u5bf9\u8fdb\u884c\u5e7f\u6cdb\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u4ece\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u4e2d\u83b7\u5f97\u7684\u3002\u8fd9\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\u5f62\u6210\u5bf9\u6bd4\uff0c\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\uff0c\u57fa\u4e8e\u6570\u636e\u589e\u5f3a\u7684\u81ea\u76d1\u7763\u8bad\u7ec3\u5df2\u663e\u793a\u51fa\u663e\u8457\u7684\u6210\u529f\u3002", "method": "\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u6b63\u5bf9\u751f\u6210\u5bf9\u4f7f\u7528\u4e24\u79cd\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u5373\u88c1\u526a\u589e\u5f3a\u548c\u57fa\u4e8e dropout \u7684\u65b9\u6cd5\u3002", "result": "\u88c1\u526a\u589e\u5f3a\u4f18\u4e8e\u57fa\u4e8e dropout \u7684\u65b9\u6cd5\u3002\u5728\u9886\u57df\u5916\u6570\u636e\u4e0a\uff0c\u751f\u6210\u7684\u5d4c\u5165\u8d28\u91cf\u4f4e\u4e8e\u6709\u76d1\u7763\u7684 SOTA \u6a21\u578b\uff0c\u4f46\u5bf9\u4e8e\u9886\u57df\u5185\u6570\u636e\uff0c\u81ea\u76d1\u7763\u5fae\u8c03\u53ef\u4ee5\u5728\u975e\u5e38\u77ed\u7684\u5fae\u8c03\u540e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u5d4c\u5165\u3002", "conclusion": "\u81ea\u76d1\u7763\u5fae\u8c03\u53ef\u4ee5\u5728\u77ed\u65f6\u95f4\u5185\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u5d4c\u5165\uff0c\u6709\u65f6\u4ec5\u7565\u4f4e\u4e8e\u6709\u76d1\u7763\u7684 SOTA\u3002\u8868\u793a\u8d28\u91cf\u5728\u6700\u540e\u4e00\u4e2a transformer \u5c42\u4e2d\u6709\u6240\u63d0\u9ad8\uff0c\u5e76\u4e14\u4ec5\u5fae\u8c03\u6700\u540e\u51e0\u5c42\u5c31\u8db3\u4ee5\u8fbe\u5230\u76f8\u4f3c\u7684\u5d4c\u5165\u8d28\u91cf\u3002"}}
{"id": "2508.03058", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03058", "abs": "https://arxiv.org/abs/2508.03058", "authors": ["Dingwei Zhu", "Shihan Dou", "Zhiheng Xi", "Senjie Jin", "Guoqiang Zhang", "Jiazheng Zhang", "Junjie Ye", "Mingxu Chai", "Enyu Zhou", "Ming Zhang", "Caishuang Huang", "Yunke Zhang", "Yuran Wang", "Tao Gui"], "title": "VRPO: Rethinking Value Modeling for Robust RL Training under Noisy Supervision", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) often suffers from noisy or\nimperfect reward supervision in real-world settings, which undermines policy\nstability and generalization. Such noise may cause models to lose attention on\nkey words during advantage estimation. While prior work focuses on reward\ndenoising or filtering poor data, it often overlooks the critical role of the\nvalue model in policy optimization. In this work, we show that a strong value\nmodel is essential for mitigating noise by absorbing unstable signals and\nenabling more reliable advantage estimation. We propose VRPO, a value-centric\nframework for robust PPO training under noisy supervision. VRPO combines two\ncore designs: (1) an auxiliary loss guided by entropy and perplexity from a\nfrozen language model, and (2) a variational information bottleneck. These\nmechanisms enhance the value model's ability to filter out noise and capture\nkey words from the context during advantage estimation, transforming it from a\npassive predictor into an active regulator of noise. Experiments on math\nreasoning, science QA, and multi-turn dialogue, under both rule-based and\nmodel-based noisy rewards, show that VRPO consistently outperforms PPO and GRPO\nbaselines. Our findings underscore the often-overlooked importance of the value\nmodel in RLHF and offer a principled and practical approach to robust policy\noptimization in noisy real-world environments.", "AI": {"tldr": "VRPO \u662f\u4e00\u79cd\u4ee5\u4ef7\u503c\u4e3a\u4e2d\u5fc3\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5608\u6742\u7684\u76d1\u7763\u4e0b\u8fdb\u884c\u7a33\u5065\u7684 PPO \u8bad\u7ec3\uff0c\u901a\u8fc7\u589e\u5f3a\u4ef7\u503c\u6a21\u578b\u8fc7\u6ee4\u566a\u58f0\u7684\u80fd\u529b\u6765\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u6765\u81ea\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60 (RLHF) \u7ecf\u5e38\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u53d7\u5230\u5608\u6742\u6216\u4e0d\u5b8c\u5584\u7684\u5956\u52b1\u76d1\u7763\u7684\u5f71\u54cd\uff0c\u8fd9\u4f1a\u635f\u5bb3\u7b56\u7565\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u6027\u3002\u8fd9\u79cd\u566a\u58f0\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u5728\u4f18\u52bf\u4f30\u8ba1\u671f\u95f4\u5931\u53bb\u5bf9\u5173\u952e\u8bcd\u7684\u6ce8\u610f\u3002", "method": "VRPO \u7ed3\u5408\u4e86\u4e24\u4e2a\u6838\u5fc3\u8bbe\u8ba1\uff1a(1) \u7531\u51bb\u7ed3\u8bed\u8a00\u6a21\u578b\u7684\u71b5\u548c\u56f0\u60d1\u5ea6\u5f15\u5bfc\u7684\u8f85\u52a9\u635f\u5931\uff0c\u4ee5\u53ca (2) \u53d8\u5206\u4fe1\u606f\u74f6\u9888\u3002", "result": "VRPO \u589e\u5f3a\u4e86\u4ef7\u503c\u6a21\u578b\u5728\u4f18\u52bf\u4f30\u8ba1\u671f\u95f4\u8fc7\u6ee4\u566a\u58f0\u548c\u6355\u83b7\u4e0a\u4e0b\u6587\u4e2d\u5173\u952e\u8bcd\u7684\u80fd\u529b\uff0c\u5c06\u5176\u4ece\u88ab\u52a8\u9884\u6d4b\u5668\u8f6c\u53d8\u4e3a\u566a\u58f0\u7684\u4e3b\u52a8\u8c03\u8282\u5668\u3002", "conclusion": "VRPO\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u6570\u5b66\u63a8\u7406\u3001\u79d1\u5b66\u95ee\u7b54\u548c\u591a\u8f6e\u5bf9\u8bdd\u5b9e\u9a8c\u4e2d\u59cb\u7ec8\u4f18\u4e8e PPO \u548c GRPO \u57fa\u7ebf\u3002"}}
{"id": "2508.03345", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03345", "abs": "https://arxiv.org/abs/2508.03345", "authors": ["Xingdan Wang", "Jiayi He", "Zhiqing Tang", "Jianxiong Guo", "Jiong Lou", "Liping Qian", "Tian Wang", "Weijia Jia"], "title": "Adaptive AI Agent Placement and Migration in Edge Intelligence Systems", "comment": null, "summary": "The rise of LLMs such as ChatGPT and Claude fuels the need for AI agents\ncapable of real-time task handling. However, migrating data-intensive,\nmulti-modal edge workloads to cloud data centers, traditionally used for agent\ndeployment, introduces significant latency. Deploying AI agents at the edge\nimproves efficiency and reduces latency. However, edge environments present\nchallenges due to limited and heterogeneous resources. Maintaining QoS for\nmobile users necessitates agent migration, which is complicated by the\ncomplexity of AI agents coordinating LLMs, task planning, memory, and external\ntools. This paper presents the first systematic deployment and management\nsolution for LLM-based AI agents in dynamic edge environments. We propose a\nnovel adaptive framework for AI agent placement and migration in edge\nintelligence systems. Our approach models resource constraints and\nlatency/cost, leveraging ant colony algorithms and LLM-based optimization for\nefficient decision-making. It autonomously places agents to optimize resource\nutilization and QoS and enables lightweight agent migration by transferring\nonly essential state. Implemented on a distributed system using AgentScope and\nvalidated across globally distributed edge servers, our solution significantly\nreduces deployment latency and migration costs.", "AI": {"tldr": "This paper introduces a system for deploying and managing LLM-based AI agents at the edge, using ant colony algorithms and LLM-based optimization to reduce latency and costs.", "motivation": "The rise of LLMs fuels the need for AI agents capable of real-time task handling. Deploying AI agents at the edge improves efficiency and reduces latency, but edge environments present challenges due to limited and heterogeneous resources. Maintaining QoS for mobile users necessitates agent migration, which is complicated by the complexity of AI agents coordinating LLMs, task planning, memory, and external tools.", "method": "The approach models resource constraints and latency/cost, leveraging ant colony algorithms and LLM-based optimization for efficient decision-making. It autonomously places agents to optimize resource utilization and QoS and enables lightweight agent migration by transferring only essential state.", "result": "Implemented on a distributed system using AgentScope and validated across globally distributed edge servers, our solution significantly reduces deployment latency and migration costs.", "conclusion": "The paper presents a systematic deployment and management solution for LLM-based AI agents in dynamic edge environments, validated on a distributed system, that significantly reduces deployment latency and migration costs."}}
{"id": "2508.03102", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03102", "abs": "https://arxiv.org/abs/2508.03102", "authors": ["Tianjiao Jiang", "Zhen Zhang", "Yuhang Liu", "Javen Qinfeng Shi"], "title": "Causal Disentanglement and Cross-Modal Alignment for Enhanced Few-Shot Learning", "comment": null, "summary": "Few-shot learning (FSL) often requires effective adaptation of models using\nlimited labeled data. However, most existing FSL methods rely on entangled\nrepresentations, requiring the model to implicitly recover the unmixing process\nto obtain disentangled representations using only limited supervision, which\nhinders effective adaptation. Recent theoretical studies show that multimodal\ncontrastive learning methods, such as CLIP, can disentangle latent\nrepresentations up to linear transformations. In light of this, we propose the\nCausal CLIP Adapter (CCA), a novel framework that explicitly disentangles\nvisual features extracted from CLIP using unsupervised Independent Component\nAnalysis (ICA). This removes the need to learn the unmixing process from the\nlabeled data, thereby reducing the number of trainable parameters and\nmitigating overfitting. Taking a step further, while ICA can obtain visual\ndisentangled representations, it may also disrupt CLIP's intra- and inter-modal\nalignment. To counteract this, CCA further leverages CLIP's inherent\ncross-modal alignment by enhancing it in two ways: unidirectionally, through\nfine-tuning a CLIP-based text classifier, and bidirectionally, via a\ncross-attention mechanism that enriches visual and textual representations\nthrough mutual interaction. Both unimodal and cross-modal classification\noutputs can be effectively combined linearly to improve classification\naccuracy. Extensive experiments on 11 benchmark datasets demonstrate that our\nmethod consistently outperforms state-of-the-art approaches in terms of\nfew-shot performance and robustness to distributional shifts, while maintaining\ncomputational efficiency. Code will be available at\nhttps://github.com/tianjiao-j/CCA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u56e0\u679cCLIP\u9002\u914d\u5668(CCA)\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5b83\u5229\u7528\u65e0\u76d1\u7763\u72ec\u7acb\u6210\u5206\u5206\u6790(ICA)\u663e\u5f0f\u5730\u89e3\u5f00\u4eceCLIP\u4e2d\u63d0\u53d6\u7684\u89c6\u89c9\u7279\u5f81\u3002", "motivation": "\u73b0\u6709\u7684FSL\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7ea0\u7f20\u7684\u8868\u5f81\uff0c\u8981\u6c42\u6a21\u578b\u9690\u5f0f\u5730\u6062\u590d\u89e3\u6df7\u8fc7\u7a0b\uff0c\u4ee5\u83b7\u5f97\u4ec5\u4f7f\u7528\u6709\u9650\u76d1\u7763\u7684\u89e3\u7f20\u8868\u5f81\uff0c\u8fd9\u963b\u788d\u4e86\u6709\u6548\u7684\u9002\u5e94\u3002", "method": "\u5229\u7528\u65e0\u76d1\u7763\u72ec\u7acb\u6210\u5206\u5206\u6790(ICA)\u663e\u5f0f\u5730\u89e3\u5f00\u4eceCLIP\u4e2d\u63d0\u53d6\u7684\u89c6\u89c9\u7279\u5f81\u3002\u901a\u8fc7\u5fae\u8c03\u57fa\u4e8eclip\u7684\u6587\u672c\u5206\u7c7b\u5668\u5355\u5411\u589e\u5f3a\uff0c\u5e76\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u673a\u5236\u53cc\u5411\u589e\u5f3a\uff0c\u901a\u8fc7\u76f8\u4e92\u4f5c\u7528\u4e30\u5bcc\u89c6\u89c9\u548c\u6587\u672c\u8868\u793a\u3002", "result": "\u5728\u5c11\u6837\u672c\u6027\u80fd\u548c\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\u65b9\u9762\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "CCA\u572811\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5c11\u6837\u672c\u6027\u80fd\u548c\u5bf9\u5206\u5e03\u504f\u79fb\u7684\u9c81\u68d2\u6027\u65b9\u9762\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2508.03072", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.03072", "abs": "https://arxiv.org/abs/2508.03072", "authors": ["Sukruta Prakash Midigeshi", "Tanmay Goyal", "Gaurav Sinha"], "title": "Achieving Limited Adaptivity for Multinomial Logistic Bandits", "comment": "Accepted to RLC 2025", "summary": "Multinomial Logistic Bandits have recently attracted much attention due to\ntheir ability to model problems with multiple outcomes. In this setting, each\ndecision is associated with many possible outcomes, modeled using a multinomial\nlogit function. Several recent works on multinomial logistic bandits have\nsimultaneously achieved optimal regret and computational efficiency. However,\nmotivated by real-world challenges and practicality, there is a need to develop\nalgorithms with limited adaptivity, wherein we are allowed only $M$ policy\nupdates. To address these challenges, we present two algorithms, B-MNL-CB and\nRS-MNL, that operate in the batched and rarely-switching paradigms,\nrespectively. The batched setting involves choosing the $M$ policy update\nrounds at the start of the algorithm, while the rarely-switching setting can\nchoose these $M$ policy update rounds in an adaptive fashion. Our first\nalgorithm, B-MNL-CB extends the notion of distributional optimal designs to the\nmultinomial setting and achieves $\\tilde{O}(\\sqrt{T})$ regret assuming the\ncontexts are generated stochastically when presented with $\\Omega(\\log \\log T)$\nupdate rounds. Our second algorithm, RS-MNL works with adversarially generated\ncontexts and can achieve $\\tilde{O}(\\sqrt{T})$ regret with $\\tilde{O}(\\log T)$\npolicy updates. Further, we conducted experiments that demonstrate that our\nalgorithms (with a fixed number of policy updates) are extremely competitive\n(and often better) than several state-of-the-art baselines (which update their\npolicy every round), showcasing the applicability of our algorithms in various\npractical scenarios.", "AI": {"tldr": "This paper introduces two algorithms for multinomial logistic bandits with limited adaptivity, achieving optimal regret with a small number of policy updates, and demonstrates their practical applicability.", "motivation": "There is a need to develop algorithms with limited adaptivity, allowing only M policy updates, due to real-world challenges and practicality in multinomial logistic bandits.", "method": "The paper presents two algorithms: B-MNL-CB, which extends distributional optimal designs to the multinomial setting, and RS-MNL, which works with adversarially generated contexts.", "result": "B-MNL-CB achieves $\\tilde{O}(\\sqrt{T})$ regret with $\\Omega(\\\\log \\\\log T)$ update rounds assuming stochastic contexts. RS-MNL achieves $\\tilde{O}(\\sqrt{T})$ regret with $\\tilde{O}(\\log T)$ policy updates with adversarial contexts.", "conclusion": "The paper's algorithms, B-MNL-CB and RS-MNL, are competitive and often better than state-of-the-art baselines with a fixed number of policy updates, demonstrating their applicability in various practical scenarios."}}
{"id": "2508.03346", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03346", "abs": "https://arxiv.org/abs/2508.03346", "authors": ["Zeju Li", "Jianyuan Zhong", "Ziyang Zheng", "Xiangyu Wen", "Zhijian Xu", "Yingying Cheng", "Fan Zhang", "Qiang Xu"], "title": "Compressing Chain-of-Thought in LLMs via Step Entropy", "comment": null, "summary": "Large Language Models (LLMs) using Chain-of-Thought (CoT) prompting excel at\ncomplex reasoning but generate verbose thought processes with considerable\nredundancy, leading to increased inference costs and reduced efficiency. We\nintroduce a novel CoT compression framework based on step entropy, a metric\nthat quantifies the informational contribution of individual reasoning steps to\nidentify redundancy. Through theoretical analysis and extensive empirical\nvalidation on mathematical reasoning benchmarks, we demonstrate that steps with\nlow entropy are indeed highly redundant. Our experiments reveal that an\nastonishing 80\\% of low-entropy intermediate steps can be pruned with minor\ndegradation in the final answer accuracy across DeepSeek-R1-7B, 14B and\nQwen3-8B. This finding sharply contrasts with random or high-entropy pruning,\nwhich severely impairs reasoning performance. Building on this, we propose a\nnovel two-stage training strategy combining Supervised Fine-Tuning (SFT) and\nGroup Relative Policy Optimization (GRPO) reinforcement learning. This approach\nenables LLMs to autonomously learn to generate compressed COTs during inference\nby strategically incorporating [SKIP] tokens. Our method significantly enhances\nLLM inference efficiency while rigorously preserving accuracy, offering\nprofound implications for practical LLM deployment and a deeper understanding\nof reasoning structures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684CoT\u538b\u7f29\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u6b65\u9aa4\u71b5\u6765\u91cf\u5316\u5404\u4e2a\u63a8\u7406\u6b65\u9aa4\u7684\u4fe1\u606f\u8d21\u732e\uff0c\u4ee5\u8bc6\u522b\u5197\u4f59\u3002", "motivation": "\u601d\u7ef4\u94fe\uff08CoT\uff09\u63d0\u793a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u64c5\u957f\u590d\u6742\u63a8\u7406\uff0c\u4f46\u4f1a\u751f\u6210\u5197\u957f\u7684\u601d\u8003\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u63a8\u7406\u6210\u672c\u589e\u52a0\u548c\u6548\u7387\u964d\u4f4e\u3002", "method": "\u57fa\u4e8e\u6b65\u9aa4\u71b5\u7684CoT\u538b\u7f29\u6846\u67b6\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u5f3a\u5316\u5b66\u4e60\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u53ef\u4ee5\u4fee\u526a80%\u7684\u4f4e\u71b5\u4e2d\u95f4\u6b65\u9aa4\uff0c\u800c\u6700\u7ec8\u7b54\u6848\u7684\u51c6\u786e\u6027\u7565\u6709\u4e0b\u964d\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u968f\u673a\u6216\u9ad8\u71b5\u4fee\u526a\u4f1a\u4e25\u91cd\u635f\u5bb3\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86LLM\u7684\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4e25\u683c\u4fdd\u8bc1\u4e86\u51c6\u786e\u6027\uff0c\u4e3a\u5b9e\u9645\u7684LLM\u90e8\u7f72\u548c\u5bf9\u63a8\u7406\u7ed3\u6784\u7684\u66f4\u6df1\u5165\u7406\u89e3\u63d0\u4f9b\u4e86\u6df1\u523b\u7684\u610f\u4e49\u3002"}}
{"id": "2508.03118", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03118", "abs": "https://arxiv.org/abs/2508.03118", "authors": ["Heng Jia", "Linchao Zhu", "Na Zhao"], "title": "H3R: Hybrid Multi-view Correspondence for Generalizable 3D Reconstruction", "comment": "ICCV 2025", "summary": "Despite recent advances in feed-forward 3D Gaussian Splatting, generalizable\n3D reconstruction remains challenging, particularly in multi-view\ncorrespondence modeling. Existing approaches face a fundamental trade-off:\nexplicit methods achieve geometric precision but struggle with ambiguous\nregions, while implicit methods provide robustness but suffer from slow\nconvergence. We present H3R, a hybrid framework that addresses this limitation\nby integrating volumetric latent fusion with attention-based feature\naggregation. Our framework consists of two complementary components: an\nefficient latent volume that enforces geometric consistency through epipolar\nconstraints, and a camera-aware Transformer that leverages Pl\\\"ucker\ncoordinates for adaptive correspondence refinement. By integrating both\nparadigms, our approach enhances generalization while converging 2$\\times$\nfaster than existing methods. Furthermore, we show that spatial-aligned\nfoundation models (e.g., SD-VAE) substantially outperform semantic-aligned\nmodels (e.g., DINOv2), resolving the mismatch between semantic representations\nand spatial reconstruction requirements. Our method supports variable-number\nand high-resolution input views while demonstrating robust cross-dataset\ngeneralization. Extensive experiments show that our method achieves\nstate-of-the-art performance across multiple benchmarks, with significant PSNR\nimprovements of 0.59 dB, 1.06 dB, and 0.22 dB on the RealEstate10K, ACID, and\nDTU datasets, respectively. Code is available at\nhttps://github.com/JiaHeng-DLUT/H3R.", "AI": {"tldr": "H3R is a hybrid 3D reconstruction framework that combines volumetric latent fusion and attention-based feature aggregation to achieve state-of-the-art performance with faster convergence and better generalization.", "motivation": "Generalizable 3D reconstruction remains challenging, particularly in multi-view correspondence modeling. Existing approaches face a trade-off between geometric precision and robustness.", "method": "The paper presents H3R, a hybrid framework that integrates volumetric latent fusion with attention-based feature aggregation, consisting of an efficient latent volume that enforces geometric consistency through epipolar constraints, and a camera-aware Transformer that leverages Pl\"ucker coordinates for adaptive correspondence refinement.", "result": "H3R enhances generalization while converging 2$\\times$ faster than existing methods. Spatial-aligned foundation models (e.g., SD-VAE) substantially outperform semantic-aligned models (e.g., DINOv2). The method supports variable-number and high-resolution input views while demonstrating robust cross-dataset generalization.", "conclusion": "The proposed H3R method achieves state-of-the-art performance across multiple benchmarks, with significant PSNR improvements of 0.59 dB, 1.06 dB, and 0.22 dB on the RealEstate10K, ACID, and DTU datasets, respectively."}}
{"id": "2508.03489", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03489", "abs": "https://arxiv.org/abs/2508.03489", "authors": ["Kaiwen Zhao", "Bharathan Balaji", "Stephen Lee"], "title": "CF-RAG: A Dataset and Method for Carbon Footprint QA Using Retrieval-Augmented Generation", "comment": null, "summary": "Product sustainability reports provide valuable insights into the\nenvironmental impacts of a product and are often distributed in PDF format.\nThese reports often include a combination of tables and text, which complicates\ntheir analysis. The lack of standardization and the variability in reporting\nformats further exacerbate the difficulty of extracting and interpreting\nrelevant information from large volumes of documents. In this paper, we tackle\nthe challenge of answering questions related to carbon footprints within\nsustainability reports available in PDF format. Unlike previous approaches, our\nfocus is on addressing the difficulties posed by the unstructured and\ninconsistent nature of text extracted from PDF parsing. To facilitate this\nanalysis, we introduce CarbonPDF-QA, an open-source dataset containing\nquestion-answer pairs for 1735 product report documents, along with\nhuman-annotated answers. Our analysis shows that GPT-4o struggles to answer\nquestions with data inconsistencies. To address this limitation, we propose\nCarbonPDF, an LLM-based technique specifically designed to answer carbon\nfootprint questions on such datasets. We develop CarbonPDF by fine-tuning Llama\n3 with our training data. Our results show that our technique outperforms\ncurrent state-of-the-art techniques, including question-answering (QA) systems\nfinetuned on table and text data.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86 CarbonPDF-QA\uff0c\u4e00\u4e2a\u5305\u542b 1735 \u4efd\u4ea7\u54c1\u62a5\u544a\u6587\u6863\u7684\u95ee\u7b54\u5bf9\u7684\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u4eba\u5de5\u6ce8\u91ca\u7684\u7b54\u6848\u3002", "motivation": "\u4ea7\u54c1\u53ef\u6301\u7eed\u6027\u62a5\u544a\u63d0\u4f9b\u4e86\u5bf9\u4ea7\u54c1\u73af\u5883\u5f71\u54cd\u7684\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u901a\u5e38\u4ee5 PDF \u683c\u5f0f\u5206\u53d1\u3002\u8fd9\u4e9b\u62a5\u544a\u901a\u5e38\u5305\u62ec\u8868\u683c\u548c\u6587\u672c\u7684\u7ec4\u5408\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u7684\u5206\u6790\u53d8\u5f97\u590d\u6742\u3002\u7f3a\u4e4f\u6807\u51c6\u5316\u548c\u62a5\u544a\u683c\u5f0f\u7684\u53ef\u53d8\u6027\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u4ece\u5927\u91cf\u6587\u6863\u4e2d\u63d0\u53d6\u548c\u89e3\u91ca\u76f8\u5173\u4fe1\u606f\u7684\u96be\u5ea6\u3002", "method": "\u672c\u6587\u63d0\u51fa CarbonPDF\uff0c\u4e00\u79cd\u57fa\u4e8e LLM \u7684\u6280\u672f\uff0c\u901a\u8fc7\u4f7f\u7528\u8bad\u7ec3\u6570\u636e\u5bf9 Llama 3 \u8fdb\u884c\u5fae\u8c03\u6765\u5f00\u53d1\uff0c\u4e13\u95e8\u7528\u4e8e\u56de\u7b54\u78b3\u8db3\u8ff9\u95ee\u9898\u3002", "result": "GPT-4o \u96be\u4ee5\u56de\u7b54\u6570\u636e\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u7684 CarbonPDF \u6280\u672f\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6280\u672f\uff0c\u5305\u62ec\u5728\u8868\u683c\u548c\u6587\u672c\u6570\u636e\u4e0a\u5fae\u8c03\u7684\u95ee\u7b54 (QA) \u7cfb\u7edf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a CarbonPDF \u7684\u57fa\u4e8e LLM \u7684\u6280\u672f\uff0c\u4e13\u95e8\u7528\u4e8e\u56de\u7b54\u6709\u5173\u6b64\u7c7b\u6570\u636e\u96c6\u4e0a\u7684\u78b3\u8db3\u8ff9\u95ee\u9898\u3002\u901a\u8fc7\u4f7f\u7528\u6211\u4eec\u7684\u8bad\u7ec3\u6570\u636e\u5bf9 Llama 3 \u8fdb\u884c\u5fae\u8c03\u6765\u5f00\u53d1 CarbonPDF\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6280\u672f\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6280\u672f\uff0c\u5305\u62ec\u5728\u8868\u683c\u548c\u6587\u672c\u6570\u636e\u4e0a\u5fae\u8c03\u7684\u95ee\u7b54 (QA) \u7cfb\u7edf\u3002"}}
{"id": "2508.03104", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03104", "abs": "https://arxiv.org/abs/2508.03104", "authors": ["Mengting Pan", "Fan Li", "Xiaoyang Wang", "Wenjie Zhang", "Xuemin Lin"], "title": "HiTeC: Hierarchical Contrastive Learning on Text-Attributed Hypergraph with Semantic-Aware Augmentation", "comment": "12 pages, 18 figures", "summary": "Contrastive learning (CL) has become a dominant paradigm for self-supervised\nhypergraph learning, enabling effective training without costly labels.\nHowever, node entities in real-world hypergraphs are often associated with rich\ntextual information, which is overlooked in prior works. Directly applying\nexisting CL-based methods to such text-attributed hypergraphs (TAHGs) leads to\nthree key limitations: (1) The common use of graph-agnostic text encoders\noverlooks the correlations between textual content and hypergraph topology,\nresulting in suboptimal representations. (2) Their reliance on random data\naugmentations introduces noise and weakens the contrastive objective. (3) The\nprimary focus on node- and hyperedge-level contrastive signals limits the\nability to capture long-range dependencies, which is essential for expressive\nrepresentation learning. Although HyperBERT pioneers CL on TAHGs, its\nco-training paradigm suffers from poor scalability. To fill the research gap,\nwe introduce HiTeC, a two-stage hierarchical contrastive learning framework\nwith semantic-aware augmentation for scalable and effective self-supervised\nlearning on TAHGs. In the first stage, we pre-train the text encoder with a\nstructure-aware contrastive objective to overcome the graph-agnostic nature of\nconventional methods. In the second stage, we introduce two semantic-aware\naugmentation strategies, including prompt-enhanced text augmentation and\nsemantic-aware hyperedge drop, to facilitate informative view generation.\nFurthermore, we propose a multi-scale contrastive loss that extends existing\nobjectives with an $s$-walk-based subgraph-level contrast to better capture\nlong-range dependencies. By decoupling text encoder pretraining from hypergraph\ncontrastive learning, this two-stage design enhances scalability without\ncompromising representation quality. Extensive experiments confirm the\neffectiveness of HiTeC.", "AI": {"tldr": "HiTeC is introduced to address the limitations of applying CL-based methods to text-attributed hypergraphs (TAHGs).", "motivation": "node entities in real-world hypergraphs are often associated with rich textual information, which is overlooked in prior works. Directly applying existing CL-based methods to such text-attributed hypergraphs (TAHGs) leads to three key limitations: (1) The common use of graph-agnostic text encoders overlooks the correlations between textual content and hypergraph topology, resulting in suboptimal representations. (2) Their reliance on random data augmentations introduces noise and weakens the contrastive objective. (3) The primary focus on node- and hyperedge-level contrastive signals limits the ability to capture long-range dependencies, which is essential for expressive representation learning.", "method": "a two-stage hierarchical contrastive learning framework with semantic-aware augmentation", "result": "HiTeC, a two-stage hierarchical contrastive learning framework with semantic-aware augmentation for scalable and effective self-supervised learning on TAHGs", "conclusion": "Extensive experiments confirm the effectiveness of HiTeC."}}
{"id": "2508.03360", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03360", "abs": "https://arxiv.org/abs/2508.03360", "authors": ["Feng Rui", "Zhiyao Luo", "Wei Wang", "Yuting Song", "Yong Liu", "Tingting Zhu", "Jianqing Li", "Xingyao Wang"], "title": "CogBench: A Large Language Model Benchmark for Multilingual Speech-Based Cognitive Impairment Assessment", "comment": "19 pages, 9 figures, 12 tables", "summary": "Automatic assessment of cognitive impairment from spontaneous speech offers a\npromising, non-invasive avenue for early cognitive screening. However, current\napproaches often lack generalizability when deployed across different languages\nand clinical settings, limiting their practical utility. In this study, we\npropose CogBench, the first benchmark designed to evaluate the cross-lingual\nand cross-site generalizability of large language models (LLMs) for\nspeech-based cognitive impairment assessment. Using a unified multimodal\npipeline, we evaluate model performance on three speech datasets spanning\nEnglish and Mandarin: ADReSSo, NCMMSC2021-AD, and a newly collected test set,\nCIR-E. Our results show that conventional deep learning models degrade\nsubstantially when transferred across domains. In contrast, LLMs equipped with\nchain-of-thought prompting demonstrate better adaptability, though their\nperformance remains sensitive to prompt design. Furthermore, we explore\nlightweight fine-tuning of LLMs via Low-Rank Adaptation (LoRA), which\nsignificantly improves generalization in target domains. These findings offer a\ncritical step toward building clinically useful and linguistically robust\nspeech-based cognitive assessment tools.", "AI": {"tldr": "\u63d0\u51fa\u4e86 CogBench \u57fa\u51c6\u6765\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u97f3\u8ba4\u77e5\u969c\u788d\u8bc4\u4f30\u4e2d\u7684\u8de8\u8bed\u8a00\u548c\u8de8\u7ad9\u70b9\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7684\u65b9\u6cd5\u5728\u8de8\u4e0d\u540c\u8bed\u8a00\u548c\u4e34\u5e8a\u73af\u5883\u90e8\u7f72\u65f6\u901a\u5e38\u7f3a\u4e4f\u901a\u7528\u6027\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5b9e\u9645\u6548\u7528\u3002\u56e0\u6b64\uff0c\u63d0\u51fa\u4e86 CogBench\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u65e8\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u57fa\u4e8e\u8bed\u97f3\u7684\u8ba4\u77e5\u969c\u788d\u8bc4\u4f30\u4e2d\u7684\u8de8\u8bed\u8a00\u548c\u8de8\u7ad9\u70b9\u6cdb\u5316\u80fd\u529b\u7684\u57fa\u51c6\u3002", "method": "\u4f7f\u7528\u7edf\u4e00\u7684\u591a\u6a21\u6001\u7ba1\u9053\uff0c\u5728\u8de8\u8d8a\u82f1\u8bed\u548c\u666e\u901a\u8bdd\u7684\u4e09\u4e2a\u8bed\u97f3\u6570\u636e\u96c6\uff08ADReSSo\u3001NCMMSC2021-AD \u548c\u65b0\u6536\u96c6\u7684\u6d4b\u8bd5\u96c6 CIR-E\uff09\u4e0a\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8de8\u9886\u57df\u8f6c\u79fb\u65f6\u6027\u80fd\u5927\u5e45\u4e0b\u964d\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u914d\u5907\u4e86\u601d\u7ef4\u94fe\u63d0\u793a\u7684 LLM \u8868\u73b0\u51fa\u66f4\u597d\u7684\u9002\u5e94\u6027\uff0c\u4f46\u5176\u6027\u80fd\u4ecd\u7136\u5bf9\u63d0\u793a\u8bbe\u8ba1\u654f\u611f\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u663e\u8457\u63d0\u9ad8\u4e86\u76ee\u6807\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u6784\u5efa\u4e34\u5e8a\u5b9e\u7528\u4e14\u8bed\u8a00\u7a33\u5065\u7684\u57fa\u4e8e\u8bed\u97f3\u7684\u8ba4\u77e5\u8bc4\u4f30\u5de5\u5177\u63d0\u4f9b\u4e86\u5173\u952e\u4e00\u6b65\u3002"}}
{"id": "2508.03127", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03127", "abs": "https://arxiv.org/abs/2508.03127", "authors": ["Sai Ma", "Zhuang Li", "John A Taylor"], "title": "Landsat30-AU: A Vision-Language Dataset for Australian Landsat Imagery", "comment": null, "summary": "Vision language models (VLMs) that enable natural language interaction with\nsatellite imagery can democratize Earth observation by accelerating expert\nworkflows, making data accessible to non-specialists, and enabling planet-scale\nautomation. However, existing datasets focus mainly on short-term,\nhigh-resolution imagery from a limited number of satellites, overlooking\nlow-resolution, multi-satellite, long-term archives, such as Landsat, that are\nessential for affordable and bias-robust global monitoring. We address this gap\nwith Landsat30-AU, a large-scale vision-language dataset built from 30-meter\nresolution imagery collected by four Landsat satellites (5, 7, 8, and 9) over\nAustralia, spanning more than 36 years. The dataset includes two components:\nLandsat30-AU-Cap, containing 196,262 image-caption pairs, and Landsat30-AU-VQA,\ncomprising 17,725 human-verified visual question answering (VQA) samples across\neight remote sensing domains. Both datasets are curated through a bootstrapped\npipeline that leverages generic VLMs with iterative refinement and human\nverification to ensure quality. Our evaluation of eight VLMs on our benchmark\nreveals that off-the-shelf models struggle to understand satellite imagery. The\nopen-source remote-sensing VLM EarthDial achieves only 0.07 SPIDEr in\ncaptioning and a VQA accuracy of 0.48, highlighting the limitations of current\napproaches. Encouragingly, lightweight fine-tuning of Qwen2.5-VL-7B on\nLandsat30-AU improves captioning performance from 0.11 to 0.31 SPIDEr and\nboosts VQA accuracy from \\textbf{0.74} to 0.87. Code and data are available at\nhttps://github.com/papersubmit1/landsat30-au.", "AI": {"tldr": "\u63d0\u51fa\u4e86 Landsat30-AU\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u536b\u661f\u56fe\u50cf\u89c6\u89c9\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u5e76\u8868\u660e\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u7406\u89e3\u536b\u661f\u56fe\u50cf\uff0c\u4f46\u53ef\u4ee5\u901a\u8fc7\u5bf9 Qwen2.5-VL-7B \u8fdb\u884c\u5fae\u8c03\u6765\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u6765\u81ea\u6709\u9650\u6570\u91cf\u536b\u661f\u7684\u77ed\u671f\u3001\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u5ffd\u7565\u4e86\u4f4e\u5206\u8fa8\u7387\u3001\u591a\u536b\u661f\u3001\u957f\u671f\u6863\u6848\uff08\u5982 Landsat\uff09\uff0c\u8fd9\u5bf9\u4e8e\u7ecf\u6d4e\u5b9e\u60e0\u4e14\u5177\u6709\u504f\u5dee\u9c81\u68d2\u6027\u7684\u5168\u7403\u76d1\u6d4b\u81f3\u5173\u91cd\u8981\u3002\u6211\u4eec\u901a\u8fc7 Landsat30-AU \u89e3\u51b3\u4e86\u8fd9\u4e2a\u5dee\u8ddd\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6570\u636e\u96c6 Landsat30-AU\uff0c\u8be5\u6570\u636e\u96c6\u7531\u56db\u4e2a Landsat \u536b\u661f\uff085\u30017\u30018 \u548c 9\uff09\u5728\u6fb3\u5927\u5229\u4e9a\u6536\u96c6\u7684 30 \u7c73\u5206\u8fa8\u7387\u56fe\u50cf\u7ec4\u6210\uff0c\u8de8\u8d8a 36 \u5e74\u4ee5\u4e0a\u3002\u8be5\u6570\u636e\u96c6\u5305\u62ec\u4e24\u4e2a\u7ec4\u6210\u90e8\u5206\uff1aLandsat30-AU-Cap\uff0c\u5305\u542b 196,262 \u4e2a\u56fe\u50cf-\u5b57\u5e55\u5bf9\uff1b\u4ee5\u53ca Landsat30-AU-VQA\uff0c\u5305\u542b 17,725 \u4e2a\u8de8\u516b\u4e2a\u9065\u611f\u9886\u57df\u7684\u4eba\u5de5\u9a8c\u8bc1\u89c6\u89c9\u95ee\u9898\u89e3\u7b54 (VQA) \u6837\u672c\u3002\u4e24\u4e2a\u6570\u636e\u96c6\u90fd\u901a\u8fc7\u4e00\u4e2a\u81ea\u4e3epipeline\u8fdb\u884c\u7ba1\u7406\uff0c\u8be5pipeline\u5229\u7528\u901a\u7528 VLM \u8fdb\u884c\u8fed\u4ee3\u7ec6\u5316\u548c\u4eba\u5de5\u9a8c\u8bc1\u4ee5\u786e\u4fdd\u8d28\u91cf\u3002", "result": "\u5728\u6211\u4eec\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5bf9\u516b\u4e2a VLM \u7684\u8bc4\u4f30\u8868\u660e\uff0c\u73b0\u6210\u7684\u6a21\u578b\u5f88\u96be\u7406\u89e3\u536b\u661f\u56fe\u50cf\u3002\u5f00\u6e90\u9065\u611f VLM EarthDial \u5728 captioning \u4e2d\u4ec5\u8fbe\u5230 0.07 SPIDEr\uff0cVQA \u51c6\u786e\u7387\u4e3a 0.48\uff0c\u7a81\u663e\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u5fae\u8c03 Qwen2.5-VL-7B \u5728 Landsat30-AU \u4e0a\u5c06 captioning \u6027\u80fd\u4ece 0.11 \u63d0\u9ad8\u5230 0.31 SPIDEr\uff0c\u5e76\u5c06 VQA \u51c6\u786e\u7387\u4ece 0.74 \u63d0\u9ad8\u5230 0.87\u3002"}}
{"id": "2508.03520", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03520", "abs": "https://arxiv.org/abs/2508.03520", "authors": ["Md Rakibul Hasan", "Md Zakir Hossain", "Aneesh Krishna", "Shafin Rahman", "Tom Gedeon"], "title": "UPLME: Uncertainty-Aware Probabilistic Language Modelling for Robust Empathy Regression", "comment": "Code available at https://github.com/hasan-rakibul/UPLME", "summary": "Supervised learning for empathy regression is challenged by noisy\nself-reported empathy scores. While many algorithms have been proposed for\nlearning with noisy labels in textual classification problems, the regression\ncounterpart is relatively under-explored. We propose UPLME, an\nuncertainty-aware probabilistic language modelling framework to capture label\nnoise in the regression setting of empathy detection. UPLME includes a\nprobabilistic language model that predicts both empathy score and\nheteroscedastic uncertainty and is trained using Bayesian concepts with\nvariational model ensembling. We further introduce two novel loss components:\none penalises degenerate Uncertainty Quantification (UQ), and another enforces\nthe similarity between the input pairs on which we predict empathy. UPLME\nprovides state-of-the-art performance (Pearson Correlation Coefficient:\n$0.558\\rightarrow0.580$ and $0.629\\rightarrow0.634$) in terms of the\nperformance reported in the literature in two public benchmarks, having label\nnoise. Through synthetic label noise injection, we show that UPLME is effective\nin separating noisy and clean samples based on the predicted uncertainty. UPLME\nfurther outperform (Calibration error: $0.571\\rightarrow0.376$) a recent\nvariational model ensembling-based UQ method designed for regression problems.", "AI": {"tldr": "UPLME, a new uncertainty-aware probabilistic language model, improves empathy detection in noisy data.", "motivation": "Supervised learning for empathy regression is challenged by noisy self-reported empathy scores, and the regression counterpart is relatively under-explored.", "method": "The paper proposes UPLME, an uncertainty-aware probabilistic language modelling framework with variational model ensembling, and introduces two novel loss components: one penalising degenerate Uncertainty Quantification (UQ), and another enforcing the similarity between the input pairs.", "result": "UPLME provides state-of-the-art performance (Pearson Correlation Coefficient: 0.558->0.580 and 0.629->0.634) and outperforms (Calibration error: 0.571->0.376) a recent variational model ensembling-based UQ method.", "conclusion": "UPLME achieves state-of-the-art performance on two public benchmarks with noisy labels and outperforms a recent variational model ensembling-based UQ method."}}
{"id": "2508.03105", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.03105", "abs": "https://arxiv.org/abs/2508.03105", "authors": ["Yuichi Kondo", "Hideaki Iiduka"], "title": "Accelerating SGDM via Learning Rate and Batch Size Schedules: A Lyapunov-Based Analysis", "comment": null, "summary": "We analyze the convergence behavior of stochastic gradient descent with\nmomentum (SGDM) under dynamic learning rate and batch size schedules by\nintroducing a novel Lyapunov function. This Lyapunov function has a simpler\nstructure compared with existing ones, facilitating the challenging convergence\nanalysis of SGDM and a unified analysis across various dynamic schedules.\nSpecifically, we extend the theoretical framework to cover three practical\nscheduling strategies commonly used in deep learning: (i) constant batch size\nwith a decaying learning rate, (ii) increasing batch size with a decaying\nlearning rate, and (iii) increasing batch size with an increasing learning\nrate. Our theoretical results reveal a clear hierarchy in convergence behavior:\nwhile (i) does not guarantee convergence of the expected gradient norm, both\n(ii) and (iii) do. Moreover, (iii) achieves a provably faster decay rate than\n(i) and (ii), demonstrating theoretical acceleration even in the presence of\nmomentum. Empirical results validate our theory, showing that dynamically\nscheduled SGDM significantly outperforms fixed-hyperparameter baselines in\nconvergence speed. We also evaluated a warm-up schedule in experiments, which\nempirically outperformed all other strategies in convergence behavior. These\nfindings provide a unified theoretical foundation and practical guidance for\ndesigning efficient and stable training procedures in modern deep learning.", "AI": {"tldr": "Analyzed SGDM convergence with dynamic schedules, found increasing batch size with increasing learning rate converges fastest, and warm-up schedule performs best empirically.", "motivation": "challenging convergence analysis of SGDM and a unified analysis across various dynamic schedules", "method": "introducing a novel Lyapunov function to analyze the convergence behavior of SGDM under dynamic learning rate and batch size schedules", "result": "reveal a clear hierarchy in convergence behavior: (i) does not guarantee convergence of the expected gradient norm, both (ii) and (iii) do. Moreover, (iii) achieves a provably faster decay rate than (i) and (ii)", "conclusion": "Dynamically scheduled SGDM significantly outperforms fixed-hyperparameter baselines and a warm-up schedule achieves best convergence."}}
{"id": "2508.03366", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SC"], "pdf": "https://arxiv.org/pdf/2508.03366", "abs": "https://arxiv.org/abs/2508.03366", "authors": ["Michael K. Chen"], "title": "A Comparative Study of Neurosymbolic AI Approaches to Interpretable Logical Reasoning", "comment": "Accepted to NeSy 2025", "summary": "General logical reasoning, defined as the ability to reason deductively on\ndomain-agnostic tasks, continues to be a challenge for large language models\n(LLMs). Current LLMs fail to reason deterministically and are not\ninterpretable. As such, there has been a recent surge in interest in\nneurosymbolic AI, which attempts to incorporate logic into neural networks. We\nfirst identify two main neurosymbolic approaches to improving logical\nreasoning: (i) the integrative approach comprising models where symbolic\nreasoning is contained within the neural network, and (ii) the hybrid approach\ncomprising models where a symbolic solver, separate from the neural network,\nperforms symbolic reasoning. Both contain AI systems with promising results on\ndomain-specific logical reasoning benchmarks. However, their performance on\ndomain-agnostic benchmarks is understudied. To the best of our knowledge, there\nhas not been a comparison of the contrasting approaches that answers the\nfollowing question: Which approach is more promising for developing general\nlogical reasoning? To analyze their potential, the following best-in-class\ndomain-agnostic models are introduced: Logic Neural Network (LNN), which uses\nthe integrative approach, and LLM-Symbolic Solver (LLM-SS), which uses the\nhybrid approach. Using both models as case studies and representatives of each\napproach, our analysis demonstrates that the hybrid approach is more promising\nfor developing general logical reasoning because (i) its reasoning chain is\nmore interpretable, and (ii) it retains the capabilities and advantages of\nexisting LLMs. To support future works using the hybrid approach, we propose a\ngeneralizable framework based on LLM-SS that is modular by design,\nmodel-agnostic, domain-agnostic, and requires little to no human input.", "AI": {"tldr": "This paper compares integrative and hybrid neurosymbolic approaches to improve logical reasoning for LLMs, and the analysis demonstrates that the hybrid approach is more promising for developing general logical reasoning.", "motivation": "Current LLMs fail to reason deterministically and are not interpretable. As such, there has been a recent surge in interest in neurosymbolic AI, which attempts to incorporate logic into neural networks. However, their performance on domain-agnostic benchmarks is understudied. To the best of our knowledge, there has not been a comparison of the contrasting approaches that answers the following question: Which approach is more promising for developing general logical reasoning?", "method": "introduce Logic Neural Network (LNN), which uses the integrative approach, and LLM-Symbolic Solver (LLM-SS), which uses the hybrid approach, Using both models as case studies and representatives of each approach, our analysis demonstrates that the hybrid approach is more promising for developing general logical reasoning", "result": "the hybrid approach is more promising for developing general logical reasoning because (i) its reasoning chain is more interpretable, and (ii) it retains the capabilities and advantages of existing LLMs. To support future works using the hybrid approach, we propose a generalizable framework based on LLM-SS that is modular by design, model-agnostic, domain-agnostic, and requires little to no human input.", "conclusion": "the hybrid approach is more promising for developing general logical reasoning because (i) its reasoning chain is more interpretable, and (ii) it retains the capabilities and advantages of existing LLMs"}}
{"id": "2508.03132", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03132", "abs": "https://arxiv.org/abs/2508.03132", "authors": ["Arion Zimmermann", "Soon-Jo Chung", "Fred Hadaegh"], "title": "COFFEE: A Shadow-Resilient Real-Time Pose Estimator for Unknown Tumbling Asteroids using Sparse Neural Networks", "comment": "in Proc. 75th Int. Astronautical Congress (IAC-24), Milan, Italy,\n  Oct. 2024", "summary": "The accurate state estimation of unknown bodies in space is a critical\nchallenge with applications ranging from the tracking of space debris to the\nshape estimation of small bodies. A necessary enabler to this capability is to\nfind and track features on a continuous stream of images. Existing methods,\nsuch as SIFT, ORB and AKAZE, achieve real-time but inaccurate pose estimates,\nwhereas modern deep learning methods yield higher quality features at the cost\nof more demanding computational resources which might not be available on\nspace-qualified hardware. Additionally, both classical and data-driven methods\nare not robust to the highly opaque self-cast shadows on the object of\ninterest. We show that, as the target body rotates, these shadows may lead to\nlarge biases in the resulting pose estimates. For these objects, a bias in the\nreal-time pose estimation algorithm may mislead the spacecraft's state\nestimator and cause a mission failure, especially if the body undergoes a\nchaotic tumbling motion. We present COFFEE, the Celestial Occlusion Fast\nFEature Extractor, a real-time pose estimation framework for asteroids designed\nto leverage prior information on the sun phase angle given by sun-tracking\nsensors commonly available onboard spacecraft. By associating salient contours\nto their projected shadows, a sparse set of features are detected, invariant to\nthe motion of the shadows. A Sparse Neural Network followed by an\nattention-based Graph Neural Network feature matching model are then jointly\ntrained to provide a set of correspondences between successive frames. The\nresulting pose estimation pipeline is found to be bias-free, more accurate than\nclassical pose estimation pipelines and an order of magnitude faster than other\nstate-of-the-art deep learning pipelines on synthetic data as well as on\nrenderings of the tumbling asteroid Apophis.", "AI": {"tldr": "This paper introduces COFFEE, a real-time pose estimation framework for asteroids that leverages sun phase angle information and is bias-free, more accurate, and faster than existing methods.", "motivation": "The accurate state estimation of unknown bodies in space is a critical challenge with applications ranging from the tracking of space debris to the shape estimation of small bodies. Existing methods, such as SIFT, ORB and AKAZE, achieve real-time but inaccurate pose estimates, whereas modern deep learning methods yield higher quality features at the cost of more demanding computational resources which might not be available on space-qualified hardware. Additionally, both classical and data-driven methods are not robust to the highly opaque self-cast shadows on the object of interest. We show that, as the target body rotates, these shadows may lead to large biases in the resulting pose estimates. For these objects, a bias in the real-time pose estimation algorithm may mislead the spacecraft's state estimator and cause a mission failure, especially if the body undergoes a chaotic tumbling motion.", "method": "We present COFFEE, the Celestial Occlusion Fast FEature Extractor, a real-time pose estimation framework for asteroids designed to leverage prior information on the sun phase angle given by sun-tracking sensors commonly available onboard spacecraft. By associating salient contours to their projected shadows, a sparse set of features are detected, invariant to the motion of the shadows. A Sparse Neural Network followed by an attention-based Graph Neural Network feature matching model are then jointly trained to provide a set of correspondences between successive frames.", "result": "The resulting pose estimation pipeline is found to be bias-free, more accurate than classical pose estimation pipelines and an order of magnitude faster than other state-of-the-art deep learning pipelines on synthetic data as well as on renderings of the tumbling asteroid Apophis.", "conclusion": "The resulting pose estimation pipeline is found to be bias-free, more accurate than classical pose estimation pipelines and an order of magnitude faster than other state-of-the-art deep learning pipelines on synthetic data as well as on renderings of the tumbling asteroid Apophis."}}
{"id": "2508.03523", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03523", "abs": "https://arxiv.org/abs/2508.03523", "authors": ["Lester James V. Miranda", "Elyanah Aco", "Conner Manuel", "Jan Christian Blaise Cruz", "Joseph Marvin Imperial"], "title": "FilBench: Can LLMs Understand and Generate Filipino?", "comment": null, "summary": "Despite the impressive performance of LLMs on English-based tasks, little is\nknown about their capabilities in specific languages such as Filipino. In this\nwork, we address this gap by introducing FilBench, a Filipino-centric benchmark\ndesigned to evaluate LLMs across a diverse set of tasks and capabilities in\nFilipino, Tagalog, and Cebuano. We carefully curate the tasks in FilBench to\nreflect the priorities and trends of NLP research in the Philippines such as\nCultural Knowledge, Classical NLP, Reading Comprehension, and Generation. By\nevaluating 27 state-of-the-art LLMs on FilBench, we find that several LLMs\nsuffer from reading comprehension and translation capabilities. Our results\nindicate that FilBench is challenging, with the best model, GPT-4o, achieving\nonly a score of 72.23%. Moreover, we also find that models trained specifically\nfor Southeast Asian languages tend to underperform on FilBench, with the\nhighest-performing model, SEA-LION v3 70B, achieving only a score of 61.07%.\nOur work demonstrates the value of curating language-specific LLM benchmarks to\naid in driving progress on Filipino NLP and increasing the inclusion of\nPhilippine languages in LLM development.", "AI": {"tldr": "FilBench, a new benchmark for evaluating LLMs on Filipino languages, reveals challenges in reading comprehension and translation for many models, even those trained for Southeast Asian languages.", "motivation": "little is known about their capabilities in specific languages such as Filipino", "method": "introducing FilBench, a Filipino-centric benchmark designed to evaluate LLMs across a diverse set of tasks and capabilities in Filipino, Tagalog, and Cebuano", "result": "several LLMs suffer from reading comprehension and translation capabilities", "conclusion": "FilBench is challenging, with the best model, GPT-4o, achieving only a score of 72.23%. Models trained specifically for Southeast Asian languages tend to underperform on FilBench."}}
{"id": "2508.03108", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03108", "abs": "https://arxiv.org/abs/2508.03108", "authors": ["Tarhib Al Azad", "Faizul Rakib Sayem", "Shahana Ibrahim"], "title": "Pseudo-label Induced Subspace Representation Learning for Robust Out-of-Distribution Detection", "comment": null, "summary": "Out-of-distribution (OOD) detection lies at the heart of robust artificial\nintelligence (AI), aiming to identify samples from novel distributions beyond\nthe training set. Recent approaches have exploited feature representations as\ndistinguishing signatures for OOD detection. However, most existing methods\nrely on restrictive assumptions on the feature space that limit the\nseparability between in-distribution (ID) and OOD samples. In this work, we\npropose a novel OOD detection framework based on a pseudo-label-induced\nsubspace representation, that works under more relaxed and natural assumptions\ncompared to existing feature-based techniques. In addition, we introduce a\nsimple yet effective learning criterion that integrates a cross-entropy-based\nID classification loss with a subspace distance-based regularization loss to\nenhance ID-OOD separability. Extensive experiments validate the effectiveness\nof our framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684OOD\u68c0\u6d4b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u66f4\u5bbd\u677e\u7684\u5047\u8bbe\u4e0b\u5de5\u4f5c\uff0c\u4e0e\u73b0\u6709\u7684\u57fa\u4e8e\u7279\u5f81\u7684\u6280\u672f\u76f8\u6bd4\uff0c\u5177\u6709\u66f4\u597d\u7684ID-OOD\u53ef\u5206\u79bb\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5bf9\u7279\u5f81\u7a7a\u95f4\u7684\u9650\u5236\u6027\u5047\u8bbe\uff0c\u9650\u5236\u4e86ID\u548cOOD\u6837\u672c\u4e4b\u95f4\u7684\u53ef\u5206\u79bb\u6027\u3002", "method": "\u57fa\u4e8e\u4f2a\u6807\u7b7e\u8bf1\u5bfc\u5b50\u7a7a\u95f4\u8868\u793a\u7684OOD\u68c0\u6d4b\u6846\u67b6", "result": "\u5927\u91cf\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6211\u4eec\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u57fa\u4e8e\u4f2a\u6807\u7b7e\u8bf1\u5bfc\u5b50\u7a7a\u95f4\u8868\u793a\u7684OOD\u68c0\u6d4b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u66f4\u5bbd\u677e\u548c\u81ea\u7136\u7684\u5047\u8bbe\u4e0b\u5de5\u4f5c\uff0c\u4e0e\u73b0\u6709\u7684\u57fa\u4e8e\u7279\u5f81\u7684\u6280\u672f\u76f8\u6bd4\uff0c\u5177\u6709\u66f4\u597d\u7684ID-OOD\u53ef\u5206\u79bb\u6027\u3002"}}
{"id": "2508.03368", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.03368", "abs": "https://arxiv.org/abs/2508.03368", "authors": ["Lucia Cipolina-Kun", "Marianna Nezhurina", "Jenia Jitsev"], "title": "Board Game Arena: A Framework and Benchmark for Assessing Large Language Models via Strategic Play", "comment": null, "summary": "The Board Game Arena library provides a framework for evaluating the decision\nmaking abilities of large language models (LLMs) through strategic board games\nimplemented in Google OpenSpiel library. The framework enables systematic\ncomparisons between LLM based agents and other agents (random, human,\nreinforcement learning agents, etc.) in various game scenarios by wrapping\nmultiple board and matrix games and supporting different agent types. It\nintegrates API access to models via LiteLLM, local model deployment via vLLM,\nand offers distributed execution through Ray. Additionally it provides\nextensive analysis tools for the LLM reasoning traces. This paper summarizes\nthe structure, key characteristics, and motivation of the repository,\nhighlighting how it contributes to the empirical evaluation of the reasoning of\nLLM and game-theoretic behavior", "AI": {"tldr": "The Board Game Arena library provides a framework for evaluating the decision making abilities of LLMs through strategic board games.The framework enables systematic comparisons between LLM based agents and other agents in various game scenarios by wrapping multiple board and matrix games and supporting different agent types.This paper summarizes the structure, key characteristics, and motivation of the repository, highlighting how it contributes to the empirical evaluation of the reasoning of LLM and game-theoretic behavior", "motivation": "Evaluating the decision making abilities of large language models (LLMs) through strategic board games.", "method": "The Board Game Arena library uses strategic board games implemented in Google OpenSpiel to evaluate LLMs. It integrates API access to models via LiteLLM, local model deployment via vLLM, and offers distributed execution through Ray. Additionally it provides extensive analysis tools for the LLM reasoning traces.", "result": "The framework enables systematic comparisons between LLM based agents and other agents (random, human, reinforcement learning agents, etc.) in various game scenarios by wrapping multiple board and matrix games and supporting different agent types.", "conclusion": "This paper summarizes the structure, key characteristics, and motivation of the repository, highlighting how it contributes to the empirical evaluation of the reasoning of LLM and game-theoretic behavior"}}
{"id": "2508.03139", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.03139", "abs": "https://arxiv.org/abs/2508.03139", "authors": ["Haozhou Zhai", "Yanzhe Gao", "Tianjiang Hu"], "title": "Uint: Building Uint Detection Dataset", "comment": null, "summary": "Fire scene datasets are crucial for training robust computer vision models,\nparticularly in tasks such as fire early warning and emergency rescue\noperations. However, among the currently available fire-related data, there is\na significant shortage of annotated data specifically targeting building\nunits.To tackle this issue, we introduce an annotated dataset of building units\ncaptured by drones, which incorporates multiple enhancement techniques. We\nconstruct backgrounds using real multi-story scenes, combine motion blur and\nbrightness adjustment to enhance the authenticity of the captured images,\nsimulate drone shooting conditions under various circumstances, and employ\nlarge models to generate fire effects at different locations.The synthetic\ndataset generated by this method encompasses a wide range of building\nscenarios, with a total of 1,978 images. This dataset can effectively improve\nthe generalization ability of fire unit detection, providing multi-scenario and\nscalable data while reducing the risks and costs associated with collecting\nreal fire data. The dataset is available at\nhttps://github.com/boilermakerr/FireUnitData.", "AI": {"tldr": "\u521b\u5efa\u4e86\u4e00\u4e2a\u65e0\u4eba\u673a\u62cd\u6444\u7684\u5efa\u7b51\u7269\u706b\u707e\u5408\u6210\u6570\u636e\u96c6\uff0c\u4ee5\u89e3\u51b3\u5e26\u6ce8\u91ca\u7684\u706b\u707e\u6570\u636e\u7684\u77ed\u7f3a\u95ee\u9898\u3002", "motivation": "\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u5efa\u7b51\u5355\u5143\u7684\u5e26\u6ce8\u91ca\u7684\u6570\u636e\uff0c\u8fd9\u5bf9\u4e8e\u8bad\u7ec3\u7a33\u5065\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u706b\u707e\u65e9\u671f\u9884\u8b66\u548c\u7d27\u6025\u6551\u63f4\u884c\u52a8\u7b49\u4efb\u52a1\u4e2d\u3002", "method": "\u4f7f\u7528\u771f\u5b9e\u7684\u591a\u5c42\u573a\u666f\u6784\u5efa\u80cc\u666f\uff0c\u7ed3\u5408\u8fd0\u52a8\u6a21\u7cca\u548c\u4eae\u5ea6\u8c03\u6574\u4ee5\u589e\u5f3a\u6355\u83b7\u56fe\u50cf\u7684\u771f\u5b9e\u6027\uff0c\u6a21\u62df\u5404\u79cd\u60c5\u51b5\u4e0b\u7684\u65e0\u4eba\u673a\u62cd\u6444\u6761\u4ef6\uff0c\u5e76\u91c7\u7528\u5927\u578b\u6a21\u578b\u5728\u4e0d\u540c\u4f4d\u7f6e\u751f\u6210\u706b\u707e\u6548\u679c\u3002", "result": "\u5f15\u5165\u4e86\u4e00\u4e2a\u7531\u65e0\u4eba\u673a\u6355\u83b7\u7684\u5e26\u6ce8\u91ca\u7684\u5efa\u7b51\u7269\u5355\u5143\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u7ed3\u5408\u4e86\u591a\u79cd\u589e\u5f3a\u6280\u672f\u3002\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u96c6\u5305\u542b\u5404\u79cd\u5efa\u7b51\u573a\u666f\uff0c\u5171\u6709 1,978 \u5f20\u56fe\u50cf\u3002", "conclusion": "\u8be5\u5408\u6210\u6570\u636e\u96c6\u5305\u542b\u5404\u79cd\u5efa\u7b51\u573a\u666f\uff0c\u5171\u6709 1,978 \u5f20\u56fe\u50cf\u3002\u8be5\u6570\u636e\u96c6\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u706b\u707e\u5355\u5143\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u4f9b\u591a\u573a\u666f\u548c\u53ef\u6269\u5c55\u7684\u6570\u636e\uff0c\u540c\u65f6\u964d\u4f4e\u4e0e\u6536\u96c6\u771f\u5b9e\u706b\u707e\u6570\u636e\u76f8\u5173\u7684\u98ce\u9669\u548c\u6210\u672c\u3002"}}
{"id": "2508.03529", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.03529", "abs": "https://arxiv.org/abs/2508.03529", "authors": ["Vukosi Marivate", "Isheanesu Dzingirai", "Fiskani Banda", "Richard Lastrucci", "Thapelo Sindane", "Keabetswe Madumo", "Kayode Olaleye", "Abiodun Modupe", "Unarine Netshifhefhe", "Herkulaas Combrink", "Mohlatlego Nakeng", "Matome Ledwaba"], "title": "Marito: Structuring and Building Open Multilingual Terminologies for South African NLP", "comment": "Under Review", "summary": "The critical lack of structured terminological data for South Africa's\nofficial languages hampers progress in multilingual NLP, despite the existence\nof numerous government and academic terminology lists. These valuable assets\nremain fragmented and locked in non-machine-readable formats, rendering them\nunusable for computational research and development. \\emph{Marito} addresses\nthis challenge by systematically aggregating, cleaning, and standardising these\nscattered resources into open, interoperable datasets. We introduce the\nfoundational \\emph{Marito} dataset, released under the equitable,\nAfrica-centered NOODL framework. To demonstrate its immediate utility, we\nintegrate the terminology into a Retrieval-Augmented Generation (RAG) pipeline.\nExperiments show substantial improvements in the accuracy and domain-specific\nconsistency of English-to-Tshivenda machine translation for large language\nmodels. \\emph{Marito} provides a scalable foundation for developing robust and\nequitable NLP technologies, ensuring South Africa's rich linguistic diversity\nis represented in the digital age.", "AI": {"tldr": "Marito addresses the challenge of fragmented terminology lists for South Africa's official languages by creating an open dataset and improving machine translation accuracy.", "motivation": "critical lack of structured terminological data for South Africa's official languages hampers progress in multilingual NLP", "method": "systematically aggregating, cleaning, and standardising scattered resources into open, interoperable datasets", "result": "substantial improvements in the accuracy and domain-specific consistency of English-to-Tshiven\u1e13a machine translation for large language models", "conclusion": "Marito provides a scalable foundation for developing robust and equitable NLP technologies, ensuring South Africa's rich linguistic diversity is represented in the digital age."}}
{"id": "2508.03111", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03111", "abs": "https://arxiv.org/abs/2508.03111", "authors": ["Francesco Leonardi", "Markus Orsi", "Jean-Louis Reymond", "Kaspar Riesen"], "title": "GEDAN: Learning the Edit Costs for Graph Edit Distance", "comment": null, "summary": "Graph Edit Distance (GED) is defined as the minimum cost transformation of\none graph into another and is a widely adopted metric for measuring the\ndissimilarity between graphs. The major problem of GED is that its computation\nis NP-hard, which has in turn led to the development of various approximation\nmethods, including approaches based on neural networks (NN). Most of these\nNN-based models simplify the problem of GED by assuming unit-cost edit\noperations, a rather unrealistic constraint in real-world applications. In this\nwork, we present a novel Graph Neural Network framework that approximates GED\nusing both supervised and unsupervised training. In the unsupervised setting,\nit employs a gradient-only self-organizing mechanism that enables optimization\nwithout ground-truth distances. Moreover, a core component of our architecture\nis the integration of a Generalized Additive Model, which allows the flexible\nand interpretable learning of context-aware edit costs. Experimental results\nshow that the proposed method achieves similar results as state-of-the-art\nreference methods, yet significantly improves both adaptability and\ninterpretability. That is, the learned cost function offers insights into\ncomplex graph structures, making it particularly valuable in domains such as\nmolecular analysis and structural pattern discovery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u76d1\u7763\u548c\u975e\u76d1\u7763\u8bad\u7ec3\u6765\u903c\u8fd1GED\uff0c\u5e76\u4e14\u5141\u8bb8\u7075\u6d3b\u548c\u53ef\u89e3\u91ca\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u7f16\u8f91\u6210\u672c\u5b66\u4e60\u3002", "motivation": "\u56fe\u7f16\u8f91\u8ddd\u79bb\uff08GED\uff09\u88ab\u5b9a\u4e49\u4e3a\u5c06\u4e00\u4e2a\u56fe\u8f6c\u6362\u4e3a\u53e6\u4e00\u4e2a\u56fe\u7684\u6700\u5c0f\u6210\u672c\u8f6c\u6362\uff0c\u662f\u4e00\u79cd\u5e7f\u6cdb\u91c7\u7528\u7684\u6d4b\u91cf\u56fe\u4e4b\u95f4\u5dee\u5f02\u7684\u6307\u6807\u3002GED\u7684\u4e3b\u8981\u95ee\u9898\u662f\u5b83\u7684\u8ba1\u7b97\u662fNP-hard\uff0c\u8fd9\u53cd\u8fc7\u6765\u5bfc\u81f4\u4e86\u5404\u79cd\u8fd1\u4f3c\u65b9\u6cd5\u7684\u53d1\u5c55\uff0c\u5305\u62ec\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\uff08NN\uff09\u7684\u65b9\u6cd5\u3002\u5927\u591a\u6570\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6a21\u578b\u901a\u8fc7\u5047\u8bbe\u5355\u4f4d\u6210\u672c\u7f16\u8f91\u64cd\u4f5c\u6765\u7b80\u5316GED\u95ee\u9898\uff0c\u8fd9\u662f\u5b9e\u9645\u5e94\u7528\u4e2d\u4e00\u4e2a\u76f8\u5f53\u4e0d\u5207\u5b9e\u9645\u7684\u7ea6\u675f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u76d1\u7763\u548c\u975e\u76d1\u7763\u8bad\u7ec3\u6765\u903c\u8fd1GED\u3002\u5728\u65e0\u76d1\u7763\u73af\u5883\u4e0b\uff0c\u5b83\u91c7\u7528\u4e86\u4e00\u79cd\u4ec5\u68af\u5ea6\u7684\u81ea\u7ec4\u7ec7\u673a\u5236\uff0c\u53ef\u4ee5\u5728\u6ca1\u6709\u5730\u9762\u771f\u5b9e\u8ddd\u79bb\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u4f18\u5316\u3002\u6b64\u5916\uff0c\u6211\u4eec\u67b6\u6784\u7684\u4e00\u4e2a\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\u662f\u96c6\u6210\u4e86\u4e00\u4e2a\u5e7f\u4e49\u52a0\u6027\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5141\u8bb8\u7075\u6d3b\u548c\u53ef\u89e3\u91ca\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u7f16\u8f91\u6210\u672c\u5b66\u4e60\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0e\u6700\u5148\u8fdb\u7684\u53c2\u8003\u65b9\u6cd5\u53d6\u5f97\u4e86\u76f8\u4f3c\u7684\u7ed3\u679c\uff0c\u4f46\u5728\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u90fd\u6709\u4e86\u663e\u8457\u7684\u63d0\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0e\u6700\u5148\u8fdb\u7684\u53c2\u8003\u65b9\u6cd5\u53d6\u5f97\u4e86\u76f8\u4f3c\u7684\u7ed3\u679c\uff0c\u4f46\u5728\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u90fd\u6709\u4e86\u663e\u8457\u7684\u63d0\u9ad8\u3002\u5b66\u4e60\u5230\u7684\u6210\u672c\u51fd\u6570\u53ef\u4ee5\u6df1\u5165\u4e86\u89e3\u590d\u6742\u7684\u56fe\u5f62\u7ed3\u6784\uff0c\u4f7f\u5176\u5728\u5206\u5b50\u5206\u6790\u548c\u7ed3\u6784\u6a21\u5f0f\u53d1\u73b0\u7b49\u9886\u57df\u7279\u522b\u6709\u4ef7\u503c\u3002"}}
{"id": "2508.03379", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03379", "abs": "https://arxiv.org/abs/2508.03379", "authors": ["Wenxin Mao", "Zhitao Wang Long Wang", "Sirong Chen", "Cuiyun Gao", "Luyang Cao", "Ziming Liu", "Qiming Zhang", "Jun Zhou", "Zhi Jin"], "title": "Data Dependency Inference for Industrial Code Generation Based on UML Sequence Diagrams", "comment": null, "summary": "Large language models (LLMs) excel at generating code from natural language\n(NL) descriptions. However, the plain textual descriptions are inherently\nambiguous and often fail to capture complex requirements like intricate system\nbehaviors, conditional logic, and architectural constraints; implicit data\ndependencies in service-oriented architectures are difficult to infer and\nhandle correctly. To bridge this gap, we propose a novel step-by-step code\ngeneration framework named UML2Dep by leveraging unambiguous formal\nspecifications of complex requirements. First, we introduce an enhanced Unified\nModeling Language (UML) sequence diagram tailored for service-oriented\narchitectures. This diagram extends traditional visual syntax by integrating\ndecision tables and API specifications, explicitly formalizing structural\nrelationships and business logic flows in service interactions to rigorously\neliminate linguistic ambiguity. Second, recognizing the critical role of data\nflow, we introduce a dedicated data dependency inference (DDI) task. DDI\nsystematically constructs an explicit data dependency graph prior to actual\ncode synthesis. To ensure reliability, we formalize DDI as a constrained\nmathematical reasoning task through novel prompting strategies, aligning with\nLLMs' excellent mathematical strengths. Additional static parsing and\ndependency pruning further reduce context complexity and cognitive load\nassociated with intricate specifications, thereby enhancing reasoning accuracy\nand efficiency.", "AI": {"tldr": "UML2Dep framework uses formal UML diagrams and data dependency graphs to generate code from complex requirements, overcoming the ambiguity of natural language descriptions.", "motivation": "The motivation is to address the ambiguity of natural language descriptions in code generation, which often fails to capture complex requirements in service-oriented architectures.", "method": "The method involves enhancing UML sequence diagrams with decision tables and API specifications, introducing a data dependency inference (DDI) task, and formalizing DDI as a constrained mathematical reasoning task.", "result": "The paper focuses on the framework and its components, with the aim of enhancing reasoning accuracy and efficiency.", "conclusion": "This paper introduces UML2Dep, a novel code generation framework that uses enhanced UML sequence diagrams and explicit data dependency graphs to address the ambiguity of natural language descriptions in complex system requirements."}}
