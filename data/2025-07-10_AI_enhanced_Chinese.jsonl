{"id": "2507.06373", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.06373", "abs": "https://arxiv.org/abs/2507.06373", "authors": ["Jeremy Fischer", "Ram Krishnamoorthy", "Vishal Kumar", "Mahdi Al-Husseini"], "title": "Digital Wargames to Enhance Military Medical Evacuation Decision-Making", "comment": null, "summary": "Medical evacuation is one of the United States Army's most storied and\ncritical mission sets, responsible for efficiently and expediently evacuating\nthe battlefield ill and injured. Medical evacuation planning involves designing\na robust network of medical platforms and facilities capable of moving and\ntreating large numbers of casualties. Until now, there has not been a medium to\nsimulate these networks in a classroom setting and evaluate both offline\nplanning and online decision-making performance. This work describes the\nMedical Evacuation Wargaming Initiative (MEWI), a three-dimensional multiplayer\nsimulation developed in Unity that replicates battlefield constraints and\nuncertainties. MEWI accurately models patient interactions at casualty\ncollection points, ambulance exchange points, medical treatment facilities, and\nevacuation platforms. Two operational scenarios are introduced: an amphibious\nisland assault in the Pacific and a Eurasian conflict across a sprawling road\nand river network. These scenarios pit students against the clock to save as\nmany casualties as possible while adhering to doctrinal lessons learned during\ndidactic training. We visualize performance data collected from two iterations\nof the MEWI Pacific scenario executed in the United States Army's Medical\nEvacuation Doctrine Course. We consider post-wargame Likert survey data from\nstudent participants and external observer notes to identify key planning\ndecision points, document medical evacuation lessons learned, and quantify\ngeneral utility. Results indicate that MEWI participation substantially\nimproves uptake of medical evacuation lessons learned and co-operative\ndecision-making. MEWI is a substantial step forward in the field of\nhigh-fidelity training tools for medical education, and our study findings\noffer critical insights into improving medical evacuation education and\noperations across the joint force.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e09\u7ef4\u591a\u4eba\u6a21\u62df\u7cfb\u7edf MEWI\uff0c\u7528\u4e8e\u6a21\u62df\u6218\u573a\u533b\u7597\u540e\u9001\u7f51\u7edc\uff0c\u7ed3\u679c\u8868\u660e MEWI \u663e\u8457\u63d0\u9ad8\u4e86\u533b\u7597\u540e\u9001\u8bfe\u7a0b\u7684\u7406\u89e3\u548c\u534f\u540c\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u76ee\u524d\u8fd8\u6ca1\u6709\u4e00\u79cd\u5a92\u4ecb\u53ef\u4ee5\u5728\u8bfe\u5802\u73af\u5883\u4e2d\u6a21\u62df\u8fd9\u4e9b\u7f51\u7edc\uff0c\u5e76\u8bc4\u4f30\u7ebf\u4e0b\u8ba1\u5212\u548c\u7ebf\u4e0a\u51b3\u7b56\u7684\u6027\u80fd\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3a\u533b\u7597\u540e\u9001\u5175\u68cb\u63a8\u6f14\u5021\u8bae (MEWI) \u7684\u4e09\u7ef4\u591a\u4eba\u6a21\u62df\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5728 Unity \u4e2d\u5f00\u53d1\uff0c\u590d\u5236\u4e86\u6218\u573a\u7ea6\u675f\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u51c6\u786e\u5730\u6a21\u62df\u4e86\u60a3\u8005\u5728\u4f24\u5458\u6536\u96c6\u70b9\u3001\u6551\u62a4\u8f66\u4ea4\u6362\u70b9\u3001\u533b\u7597\u8bbe\u65bd\u548c\u540e\u9001\u5e73\u53f0\u4e0a\u7684\u4e92\u52a8\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cMEWI \u53c2\u4e0e\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u533b\u7597\u540e\u9001\u8bfe\u7a0b\u7684\u5438\u6536\u548c\u534f\u540c\u51b3\u7b56\u80fd\u529b\u3002", "conclusion": "MEWI \u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u533b\u7597\u540e\u9001\u8bfe\u7a0b\u7684\u7406\u89e3\u548c\u534f\u540c\u51b3\u7b56\u80fd\u529b\uff0c\u662f\u533b\u7597\u6559\u80b2\u9ad8\u4fdd\u771f\u8bad\u7ec3\u5de5\u5177\u9886\u57df\u7684\u4e00\u5927\u8fdb\u6b65\uff0c\u4e3a\u6539\u8fdb\u8054\u5408\u90e8\u961f\u7684\u533b\u7597\u540e\u9001\u6559\u80b2\u548c\u884c\u52a8\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.06396", "categories": ["cs.AI", "cs.LG", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.06396", "abs": "https://arxiv.org/abs/2507.06396", "authors": ["Mandana Vaziri", "Louis Mandel", "Yuji Watanabe", "Hirokuni Kitahara", "Martin Hirzel", "Anca Sailer"], "title": "Representing Prompting Patterns with PDL: Compliance Agent Case Study", "comment": "ICML 2025 Workshop on Programmatic Representations for Agent Learning", "summary": "Prompt engineering for LLMs remains complex, with existing frameworks either\nhiding complexity behind restrictive APIs or providing inflexible canned\npatterns that resist customization -- making sophisticated agentic programming\nchallenging. We present the Prompt Declaration Language (PDL), a novel approach\nto prompt representation that tackles this fundamental complexity by bringing\nprompts to the forefront, enabling manual and automatic prompt tuning while\ncapturing the composition of LLM calls together with rule-based code and\nexternal tools. By abstracting away the plumbing for such compositions, PDL\naims at improving programmer productivity while providing a declarative\nrepresentation that is amenable to optimization. This paper demonstrates PDL's\nutility through a real-world case study of a compliance agent. Tuning the\nprompting pattern of this agent yielded up to 4x performance improvement\ncompared to using a canned agent and prompt pattern.", "AI": {"tldr": "The paper introduces Prompt Declaration Language (PDL) to simplify prompt engineering for LLMs, enabling better performance and customization compared to existing methods. A case study shows up to 4x performance improvement with PDL.", "motivation": "Prompt engineering for LLMs remains complex, with existing frameworks either hiding complexity behind restrictive APIs or providing inflexible canned patterns that resist customization -- making sophisticated agentic programming challenging.", "method": "We present the Prompt Declaration Language (PDL), a novel approach to prompt representation that tackles this fundamental complexity by bringing prompts to the forefront, enabling manual and automatic prompt tuning while capturing the composition of LLM calls together with rule-based code and external tools. By abstracting away the plumbing for such compositions, PDL aims at improving programmer productivity while providing a declarative representation that is amenable to optimization.", "result": "This paper demonstrates PDL's utility through a real-world case study of a compliance agent.", "conclusion": "Tuning the prompting pattern of this agent yielded up to 4x performance improvement compared to using a canned agent and prompt pattern."}}
{"id": "2507.06398", "categories": ["cs.AI", "cs.CY", "68T01, 91B26, 93C15"], "pdf": "https://arxiv.org/pdf/2507.06398", "abs": "https://arxiv.org/abs/2507.06398", "authors": ["David Orban"], "title": "Jolting Technologies: Superexponential Acceleration in AI Capabilities and Implications for AGI", "comment": "13 pages, 2 figures. Revised following peer review", "summary": "This paper investigates the Jolting Technologies Hypothesis, which posits\nsuperexponential growth (increasing acceleration, or a positive third\nderivative) in the development of AI capabilities. We develop a theoretical\nframework and validate detection methodologies through Monte Carlo simulations,\nwhile acknowledging that empirical validation awaits suitable longitudinal\ndata. Our analysis focuses on creating robust tools for future empirical\nstudies and exploring the potential implications should the hypothesis prove\nvalid. The study examines how factors such as shrinking idea-to-action\nintervals and compounding iterative AI improvements drive this jolting pattern.\nBy formalizing jolt dynamics and validating detection methods through\nsimulation, this work provides the mathematical foundation necessary for\nunderstanding potential AI trajectories and their consequences for AGI\nemergence, offering insights for research and policy.", "AI": {"tldr": "Formalizes jolt dynamics and validates detection methods through simulation to understand AI trajectories and their consequences for AGI emergence.", "motivation": "This paper investigates the Jolting Technologies Hypothesis, which posits superexponential growth in the development of AI capabilities.", "method": "We develop a theoretical framework and validate detection methodologies through Monte Carlo simulations", "result": "analysis focuses on creating robust tools for future empirical studies and exploring the potential implications should the hypothesis prove valid. The study examines how factors such as shrinking idea-to-action intervals and compounding iterative AI improvements drive this jolting pattern.", "conclusion": "This work provides the mathematical foundation necessary for understanding potential AI trajectories and their consequences for AGI emergence, offering insights for research and policy."}}
{"id": "2507.06503", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.06503", "abs": "https://arxiv.org/abs/2507.06503", "authors": ["Jiaqi Zheng", "Cheng Guo", "Yi Cao", "Chaoqun Hou", "Tong Liu", "Bo Zheng"], "title": "USD: A User-Intent-Driven Sampling and Dual-Debiasing Framework for Large-Scale Homepage Recommendations", "comment": null, "summary": "Large-scale homepage recommendations face critical challenges from\npseudo-negative samples caused by exposure bias, where non-clicks may indicate\ninattention rather than disinterest. Existing work lacks thorough analysis of\ninvalid exposures and typically addresses isolated aspects (e.g., sampling\nstrategies), overlooking the critical impact of pseudo-positive samples - such\nas homepage clicks merely to visit marketing portals. We propose a unified\nframework for large-scale homepage recommendation sampling and debiasing. Our\nframework consists of two key components: (1) a user intent-aware negative\nsampling module to filter invalid exposure samples, and (2) an intent-driven\ndual-debiasing module that jointly corrects exposure bias and click bias.\nExtensive online experiments on Taobao demonstrate the efficacy of our\nframework, achieving significant improvements in user click-through rates\n(UCTR) by 35.4\\% and 14.5\\% in two variants of the marketing block on the\nTaobao homepage, Baiyibutie and Taobaomiaosha.", "AI": {"tldr": "Unified framework for homepage recommendation sampling and debiasing, improving UCTR by addressing exposure and click biases.", "motivation": "Large-scale homepage recommendations suffer from pseudo-negative samples due to exposure bias and overlook pseudo-positive samples like marketing portal visits.", "method": "A unified framework with user intent-aware negative sampling and intent-driven dual-debiasing.", "result": "Achieved UCTR improvements of 35.4% and 14.5% in two Taobao homepage marketing block variants.", "conclusion": "The proposed framework significantly improves user click-through rates (UCTR) on Taobao homepage marketing blocks."}}
{"id": "2507.06798", "categories": ["cs.AI", "math.LO"], "pdf": "https://arxiv.org/pdf/2507.06798", "abs": "https://arxiv.org/abs/2507.06798", "authors": ["Uri Andrews", "Luca San Mauro"], "title": "Comparing Dialectical Systems: Contradiction and Counterexample in Belief Change (Extended Version)", "comment": "25 pages, accepted at JELIA 2025", "summary": "Dialectical systems are a mathematical formalism for modeling an agent\nupdating a knowledge base seeking consistency. Introduced in the 1970s by\nRoberto Magari, they were originally conceived to capture how a working\nmathematician or a research community refines beliefs in the pursuit of truth.\nDialectical systems also serve as natural models for the belief change of an\nautomated agent, offering a unifying, computable framework for dynamic belief\nmanagement.\n  The literature distinguishes three main models of dialectical systems:\n(d-)dialectical systems based on revising beliefs when they are seen to be\ninconsistent, p-dialectical systems based on revising beliefs based on finding\na counterexample, and q-dialectical systems which can do both. We answer an\nopen problem in the literature by proving that q-dialectical systems are\nstrictly more powerful than p-dialectical systems, which are themselves known\nto be strictly stronger than (d-)dialectical systems. This result highlights\nthe complementary roles of counterexample and contradiction in automated belief\nrevision, and thus also in the reasoning processes of mathematicians and\nresearch communities.", "AI": {"tldr": "q-dialectical systems are strictly more powerful than p-dialectical systems, which are themselves known to be strictly stronger than (d-)dialectical systems.", "motivation": "modeling an agent updating a knowledge base seeking consistency", "method": "proving that q-dialectical systems are strictly more powerful than p-dialectical systems", "result": "q-dialectical systems are strictly more powerful than p-dialectical systems", "conclusion": "q-dialectical systems are strictly more powerful than p-dialectical systems, which are themselves known to be strictly stronger than (d-)dialectical systems. This result highlights the complementary roles of counterexample and contradiction in automated belief revision, and thus also in the reasoning processes of mathematicians and research communities."}}
{"id": "2507.06507", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06507", "abs": "https://arxiv.org/abs/2507.06507", "authors": ["Zhen Yang", "Haitao Lin", "Jiawei xue", "Ziji Zhang"], "title": "GR-LLMs: Recent Advances in Generative Recommendation Based on Large Language Models", "comment": "8 pages, 3 figures", "summary": "In the past year, Generative Recommendations (GRs) have undergone substantial\nadvancements, especially in leveraging the powerful sequence modeling and\nreasoning capabilities of Large Language Models (LLMs) to enhance overall\nrecommendation performance. LLM-based GRs are forming a new paradigm that is\ndistinctly different from discriminative recommendations, showing strong\npotential to replace traditional recommendation systems heavily dependent on\ncomplex hand-crafted features. In this paper, we provide a comprehensive survey\naimed at facilitating further research of LLM-based GRs. Initially, we outline\nthe general preliminaries and application cases of LLM-based GRs. Subsequently,\nwe introduce the main considerations when LLM-based GRs are applied in real\nindustrial scenarios. Finally, we explore promising directions for LLM-based\nGRs. We hope that this survey contributes to the ongoing advancement of the GR\ndomain.", "AI": {"tldr": "This paper surveys the advancements and potential of Large Language Model-based Generative Recommendations (LLM-based GRs), highlighting their differences from traditional methods and exploring future directions.", "motivation": "LLM-based GRs are forming a new paradigm that is distinctly different from discriminative recommendations, showing strong potential to replace traditional recommendation systems heavily dependent on complex hand-crafted features.", "method": "a comprehensive survey aimed at facilitating further research of LLM-based GRs. It outlines the general preliminaries and application cases of LLM-based GRs, introduces the main considerations when LLM-based GRs are applied in real industrial scenarios, and explores promising directions for LLM-based GRs.", "result": "Generative Recommendations (GRs) have undergone substantial advancements, especially in leveraging the powerful sequence modeling and reasoning capabilities of Large Language Models (LLMs) to enhance overall recommendation performance.", "conclusion": "This survey contributes to the ongoing advancement of the GR domain."}}
{"id": "2507.06467", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.06467", "abs": "https://arxiv.org/abs/2507.06467", "authors": ["Luyu Qiu", "Jianing Li", "Chi Su", "Lei Chen"], "title": "Interactive Text-to-SQL via Expected Information Gain for Disambiguation", "comment": "13 pages, 5 figure", "summary": "Relational databases are foundational to numerous domains, including business\nintelligence, scientific research, and enterprise systems. However, accessing\nand analyzing structured data often requires proficiency in SQL, which is a\nskill that many end users lack. With the development of Natural Language\nProcessing (NLP) technology, the Text-to-SQL systems attempt to bridge this gap\nby translating natural language questions into executable SQL queries via an\nautomated algorithm. Yet, when operating on complex real-world databases, the\nText-to-SQL systems often suffer from ambiguity due to natural ambiguity in\nnatural language queries. These ambiguities pose a significant challenge for\nexisting Text-to-SQL translation systems, which tend to commit early to a\npotentially incorrect interpretation. To address this, we propose an\ninteractive Text-to-SQL framework that models SQL generation as a probabilistic\nreasoning process over multiple candidate queries. Rather than producing a\nsingle deterministic output, our system maintains a distribution over possible\nSQL outputs and seeks to resolve uncertainty through user interaction. At each\ninteraction step, the system selects a branching decision and formulates a\nclarification question aimed at disambiguating that aspect of the query.\nCrucially, we adopt a principled decision criterion based on Expected\nInformation Gain to identify the clarification that will, in expectation, most\nreduce the uncertainty in the SQL distribution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u7684Text-to-SQL\u6846\u67b6\uff0c\u901a\u8fc7\u7528\u6237\u4ea4\u4e92\u6765\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u4e2d\u7684\u6b67\u4e49\u6027\u3002", "motivation": "\u73b0\u6709\u7684Text-to-SQL\u7cfb\u7edf\u503e\u5411\u4e8e\u8fc7\u65e9\u5730\u81f4\u529b\u4e8e\u6f5c\u5728\u7684\u4e0d\u6b63\u786e\u7684\u89e3\u91ca\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u8be5\u7cfb\u7edf\u7ef4\u62a4\u53ef\u80fd\u7684SQL\u8f93\u51fa\u4e0a\u7684\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u4ea4\u4e92\u6765\u89e3\u51b3\u4e0d\u786e\u5b9a\u6027\u3002\u5728\u6bcf\u4e2a\u4ea4\u4e92\u6b65\u9aa4\u4e2d\uff0c\u7cfb\u7edf\u9009\u62e9\u4e00\u4e2a\u5206\u652f\u51b3\u7b56\u5e76\u5236\u5b9a\u4e00\u4e2a\u65e8\u5728\u6d88\u9664\u67e5\u8be2\u7684\u6b67\u4e49\u7684\u6f84\u6e05\u95ee\u9898\u3002\u6211\u4eec\u91c7\u7528\u57fa\u4e8e\u9884\u671f\u4fe1\u606f\u589e\u76ca\u7684\u539f\u5219\u6027\u51b3\u7b56\u6807\u51c6\u6765\u8bc6\u522b\u5728\u671f\u671b\u4e2d\u5c06\u6700\u5927\u7a0b\u5ea6\u5730\u51cf\u5c11SQL\u5206\u5e03\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u7684\u6f84\u6e05\u3002", "result": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u7528\u6237\u4ea4\u4e92\u6765\u89e3\u51b3\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u6bcf\u4e2a\u4ea4\u4e92\u6b65\u9aa4\u4e2d\uff0c\u7cfb\u7edf\u9009\u62e9\u4e00\u4e2a\u5206\u652f\u51b3\u7b56\u5e76\u5236\u5b9a\u4e00\u4e2a\u65e8\u5728\u6d88\u9664\u67e5\u8be2\u7684\u6b67\u4e49\u7684\u6f84\u6e05\u95ee\u9898\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u7684Text-to-SQL\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06SQL\u751f\u6210\u5efa\u6a21\u4e3a\u591a\u4e2a\u5019\u9009\u67e5\u8be2\u4e0a\u7684\u6982\u7387\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u4ea4\u4e92\u6765\u89e3\u51b3\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2507.06261", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06261", "abs": "https://arxiv.org/abs/2507.06261", "authors": ["Gheorghe Comanici", "Eric Bieber", "Mike Schaekermann", "Ice Pasupat", "Noveen Sachdeva", "Inderjit Dhillon", "Marcel Blistein", "Ori Ram", "Dan Zhang", "Evan Rosen", "Luke Marris", "Sam Petulla", "Colin Gaffney", "Asaf Aharoni", "Nathan Lintz", "Tiago Cardal Pais", "Henrik Jacobsson", "Idan Szpektor", "Nan-Jiang Jiang", "Krishna Haridasan", "Ahmed Omran", "Nikunj Saunshi", "Dara Bahri", "Gaurav Mishra", "Eric Chu", "Toby Boyd", "Brad Hekman", "Aaron Parisi", "Chaoyi Zhang", "Kornraphop Kawintiranon", "Tania Bedrax-Weiss", "Oliver Wang", "Ya Xu", "Ollie Purkiss", "Uri Mendlovic", "Ila\u00ef Deutel", "Nam Nguyen", "Adam Langley", "Flip Korn", "Lucia Rossazza", "Alexandre Ram\u00e9", "Sagar Waghmare", "Helen Miller", "Vaishakh Keshava", "Ying Jian", "Xiaofan Zhang", "Raluca Ada Popa", "Kedar Dhamdhere", "Bla\u017e Bratani\u010d", "Kyuyeun Kim", "Terry Koo", "Ferran Alet", "Yi-ting Chen", "Arsha Nagrani", "Hannah Muckenhirn", "Zhiyuan Zhang", "Corbin Quick", "Filip Paveti\u0107", "Duc Dung Nguyen", "Joao Carreira", "Michael Elabd", "Haroon Qureshi", "Fabian Mentzer", "Yao-Yuan Yang", "Danielle Eisenbud", "Anmol Gulati", "Ellie Talius", "Eric Ni", "Sahra Ghalebikesabi", "Edouard Yvinec", "Alaa Saade", "Thatcher Ulrich", "Lorenzo Blanco", "Dan A. Calian", "Muhuan Huang", "A\u00e4ron van den Oord", "Naman Goyal", "Terry Chen", "Praynaa Rawlani", "Christian Schallhart", "Swachhand Lokhande", "Xianghong Luo", "Jyn Shan", "Ceslee Montgomery", "Victoria Krakovna", "Federico Piccinini", "Omer Barak", "Jingyu Cui", "Yiling Jia", "Mikhail Dektiarev", "Alexey Kolganov", "Shiyu Huang", "Zhe Chen", "Xingyu Wang", "Jessica Austin", "Peter de Boursac", "Evgeny Sluzhaev", "Frank Ding", "Huijian Li", "Surya Bhupatiraju", "Mohit Agarwal", "S\u0142awek Kwasiborski", "Paramjit Sandhu", "Patrick Siegler", "Ahmet Iscen", "Eyal Ben-David", "Shiraz Butt", "Miltos Allamanis", "Seth Benjamin", "Robert Busa-Fekete", "Felix Hernandez-Campos", "Sasha Goldshtein", "Matt Dibb", "Weiyang Zhang", "Annie Marsden", "Carey Radebaugh", "Stephen Roller", "Abhishek Nayyar", "Jacob Austin", "Tayfun Terzi", "Bhargav Kanagal Shamanna", "Pete Shaw", "Aayush Singh", "Florian Luisier", "Artur Mendon\u00e7a", "Vaibhav Aggarwal", "Larisa Markeeva", "Claudio Fantacci", "Sergey Brin", "HyunJeong Choe", "Guanyu Wang", "Hartwig Adam", "Avigail Dabush", "Tatsuya Kiyono", "Eyal Marcus", "Jeremy Cole", "Theophane Weber", "Hongrae Lee", "Ronny Huang", "Alex Muzio", "Leandro Kieliger", "Maigo Le", "Courtney Biles", "Long Le", "Archit Sharma", "Chengrun Yang", "Avery Lamp", "Dave Dopson", "Nate Hurley", "Katrina", "Xu", "Zhihao Shan", "Shuang Song", "Jiewen Tan", "Alexandre Senges", "George Zhang", "Chong You", "Yennie Jun", "David Raposo", "Susanna Ricco", "Xuan Yang", "Weijie Chen", "Prakhar Gupta", "Arthur Szlam", "Kevin Villela", "Chun-Sung Ferng", "Daniel Kasenberg", "Chen Liang", "Rui Zhu", "Arunachalam Narayanaswamy", "Florence Perot", "Paul Pucciarelli", "Anna Shekhawat", "Alexey Stern", "Rishikesh Ingale", "Stefani Karp", "Sanaz Bahargam", "Adrian Goedeckemeyer", "Jie Han", "Sicheng Li", "Andrea Tacchetti", "Dian Yu", "Abhishek Chakladar", "Zhiying Zhang", "Mona El Mahdy", "Xu Gao", "Dale Johnson", "Samrat Phatale", "AJ Piergiovanni", "Hyeontaek Lim", "Clement Farabet", "Carl Lebsack", "Theo Guidroz", "John Blitzer", "Nico Duduta", "David Madras", "Steve Li", "Daniel von Dincklage", "Xin Li", "Mahdis Mahdieh", "George Tucker", "Ganesh Jawahar", "Owen Xiao", "Danny Tarlow", "Robert Geirhos", "Noam Velan", "Daniel Vlasic", "Kalesha Bullard", "SK Park", "Nishesh Gupta", "Kellie Webster", "Ayal Hitron", "Jieming Mao", "Julian Eisenschlos", "Laurel Prince", "Nina D'Souza", "Kelvin Zheng", "Sara Nasso", "Gabriela Botea", "Carl Doersch", "Caglar Unlu", "Chris Alberti", "Alexey Svyatkovskiy", "Ankita Goel", "Krzysztof Choromanski", "Pan-Pan Jiang", "Richard Nguyen", "Four Flynn", "Daria \u0106urko", "Peter Chen", "Nicholas Roth", "Kieran Milan", "Caleb Habtegebriel", "Shashi Narayan", "Michael Moffitt", "Jake Marcus", "Thomas Anthony", "Brendan McMahan", "Gowoon Cheon", "Ruibo Liu", "Megan Barnes", "Lukasz Lew", "Rebeca Santamaria-Fernandez", "Mayank Upadhyay", "Arjun Akula", "Arnar Mar Hrafnkelsson", "Alvaro Caceres", "Andrew Bunner", "Michal Sokolik", "Subha Puttagunta", "Lawrence Moore", "Berivan Isik", "Weilun Chen", "Jay Hartford", "Lawrence Chan", "Pradeep Shenoy", "Dan Holtmann-Rice", "Jane Park", "Fabio Viola", "Alex Salcianu", "Sujeevan Rajayogam", "Ian Stewart-Binks", "Zelin Wu", "Richard Everett", "Xi Xiong", "Pierre-Antoine Manzagol", "Gary Leung", "Carl Saroufim", "Bo Pang", "Dawid Wegner", "George Papamakarios", "Jennimaria Palomaki", "Helena Pankov", "Guangda Lai", "Guilherme Tubone", "Shubin Zhao", "Theofilos Strinopoulos", "Seth Neel", "Mingqiu Wang", "Joe Kelley", "Li Li", "Pingmei Xu", "Anitha Vijayakumar", "Andrea D'olimpio", "Omer Levy", "Massimo Nicosia", "Grigory Rozhdestvenskiy", "Ni Lao", "Sirui Xie", "Yash Katariya", "Jon Simon", "Sanjiv Kumar", "Florian Hartmann", "Michael Kilgore", "Jinhyuk Lee", "Aroma Mahendru", "Roman Ring", "Tom Hennigan", "Fiona Lang", "Colin Cherry", "David Steiner", "Dawsen Hwang", "Ray Smith", "Pidong Wang", "Jeremy Chen", "Ming-Hsuan Yang", "Sam Kwei", "Philippe Schlattner", "Donnie Kim", "Ganesh Poomal Girirajan", "Nikola Momchev", "Ayushi Agarwal", "Xingyi Zhou", "Ilkin Safarli", "Zachary Garrett", "AJ Pierigiovanni", "Sarthak Jauhari", "Alif Raditya Rochman", "Shikhar Vashishth", "Quan Yuan", "Christof Angermueller", "Jon Blanton", "Xinying Song", "Nitesh Bharadwaj Gundavarapu", "Thi Avrahami", "Maxine Deines", "Subhrajit Roy", "Manish Gupta", "Christopher Semturs", "Shobha Vasudevan", "Aditya Srikanth Veerubhotla", "Shriya Sharma", "Josh Jacob", "Zhen Yang", "Andreas Terzis", "Dan Karliner", "Auriel Wright", "Tania Rojas-Esponda", "Ashley Brown", "Abhijit Guha Roy", "Pawan Dogra", "Andrei Kapishnikov", "Peter Young", "Wendy Kan", "Vinodh Kumar Rajendran", "Maria Ivanova", "Salil Deshmukh", "Chia-Hua Ho", "Mike Kwong", "Stav Ginzburg", "Annie Louis", "KP Sawhney", "Slav Petrov", "Jing Xie", "Yunfei Bai", "Georgi Stoyanov", "Alex Fabrikant", "Rajesh Jayaram", "Yuqi Li", "Joe Heyward", "Justin Gilmer", "Yaqing Wang", "Radu Soricut", "Luyang Liu", "Qingnan Duan", "Jamie Hayes", "Maura O'Brien", "Gaurav Singh Tomar", "Sivan Eiger", "Bahar Fatemi", "Jeffrey Hui", "Catarina Barros", "Adaeze Chukwuka", "Alena Butryna", "Saksham Thakur", "Austin Huang", "Zhufeng Pan", "Haotian Tang", "Serkan Cabi", "Tulsee Doshi", "Michiel Bakker", "Sumit Bagri", "Ruy Ley-Wild", "Adam Lelkes", "Jennie Lees", "Patrick Kane", "David Greene", "Shimu Wu", "J\u00f6rg Bornschein", "Gabriela Surita", "Sarah Hodkinson", "Fangtao Li", "Chris Hidey", "S\u00e9bastien Pereira", "Sean Ammirati", "Phillip Lippe", "Adam Kraft", "Pu Han", "Sebastian Gerlach", "Zifeng Wang", "Liviu Panait", "Feng Han", "Brian Farris", "Yingying Bi", "Hannah DeBalsi", "Miaosen Wang", "Gladys Tyen", "James Cohan", "Susan Zhang", "Jarred Barber", "Da-Woon Chung", "Jaeyoun Kim", "Markus Kunesch", "Steven Pecht", "Nami Akazawa", "Abe Friesen", "James Lyon", "Ali Eslami", "Junru Wu", "Jie Tan", "Yue Song", "Ravi Kumar", "Chris Welty", "Ilia Akolzin", "Gena Gibson", "Sean Augenstein", "Arjun Pillai", "Nancy Yuen", "Du Phan", "Xin Wang", "Iain Barr", "Heiga Zen", "Nan Hua", "Casper Liu", "Jilei", "Wang", "Tanuj Bhatia", "Hao Xu", "Oded Elyada", "Pushmeet Kohli", "Mirek Ol\u0161\u00e1k", "Ke Chen", "Azalia Mirhoseini", "Noam Shazeer", "Shoshana Jakobovits", "Maggie Tran", "Nolan Ramsden", "Tarun Bharti", "Fred Alcober", "Yunjie Li", "Shilpa Shetty", "Jing Chen", "Dmitry Kalashnikov", "Megha Nawhal", "Sercan Arik", "Hanwen Chen", "Michiel Blokzijl", "Shubham Gupta", "James Rubin", "Rigel Swavely", "Sophie Bridgers", "Ian Gemp", "Chen Su", "Arun Suggala", "Juliette Pluto", "Mary Cassin", "Alain Vaucher", "Kaiyang Ji", "Jiahao Cai", "Andrew Audibert", "Animesh Sinha", "David Tian", "Efrat Farkash", "Amy Hua", "Jilin Chen", "Duc-Hieu Tran", "Edward Loper", "Nicole Brichtova", "Lara McConnaughey", "Ballie Sandhu", "Robert Leland", "Doug DeCarlo", "Andrew Over", "James Huang", "Xing Wu", "Connie Fan", "Eric Li", "Yun Lei", "Deepak Sharma", "Cosmin Paduraru", "Luo Yu", "Matko Bo\u0161njak", "Phuong Dao", "Min Choi", "Sneha Kudugunta", "Jakub Adamek", "Carlos Gu\u00eda", "Ali Khodaei", "Jie Feng", "Wenjun Zeng", "David Welling", "Sandeep Tata", "Christina Butterfield", "Andrey Vlasov", "Seliem El-Sayed", "Swaroop Mishra", "Tara Sainath", "Shentao Yang", "RJ Skerry-Ryan", "Jeremy Shar", "Robert Berry", "Arunkumar Rajendran", "Arun Kandoor", "Andrea Burns", "Deepali Jain", "Tom Stone", "Wonpyo Park", "Shibo Wang", "Albin Cassirer", "Guohui Wang", "Hayato Kobayashi", "Sergey Rogulenko", "Vineetha Govindaraj", "Miko\u0142aj Rybi\u0144ski", "Nadav Olmert", "Colin Evans", "Po-Sen Huang", "Kelvin Xu", "Premal Shah", "Terry Thurk", "Caitlin Sikora", "Mu Cai", "Jin Xie", "Elahe Dabir", "Saloni Shah", "Norbert Kalb", "Carrie Zhang", "Shruthi Prabhakara", "Amit Sabne", "Artiom Myaskovsky", "Vikas Raunak", "Blanca Huergo", "Behnam Neyshabur", "Jon Clark", "Ye Zhang", "Shankar Krishnan", "Eden Cohen", "Dinesh Tewari", "James Lottes", "Yumeya Yamamori", "Hui", "Li", "Mohamed Elhawaty", "Ada Maksutaj Oflazer", "Adri\u00e0 Recasens", "Sheryl Luo", "Duy Nguyen", "Taylor Bos", "Kalyan Andra", "Ana Salazar", "Ed Chi", "Jeongwoo Ko", "Matt Ginsberg", "Anders Andreassen", "Anian Ruoss", "Todor Davchev", "Elnaz Davoodi", "Chenxi Liu", "Min Kim", "Santiago Ontanon", "Chi Ming To", "Dawei Jia", "Rosemary Ke", "Jing Wang", "Anna Korsun", "Moran Ambar", "Ilya Kornakov", "Irene Giannoumis", "Toni Creswell", "Denny Zhou", "Yi Su", "Ishaan Watts", "Aleksandr Zaks", "Evgenii Eltyshev", "Ziqiang Feng", "Sidharth Mudgal", "Alex Kaskasoli", "Juliette Love", "Kingshuk Dasgupta", "Sam Shleifer", "Richard Green", "Sungyong Seo", "Chansoo Lee", "Dale Webster", "Prakash Shroff", "Ganna Raboshchuk", "Isabel Leal", "James Manyika", "Sofia Erell", "Daniel Murphy", "Zhisheng Xiao", "Anton Bulyenov", "Julian Walker", "Mark Collier", "Matej Kastelic", "Nelson George", "Sushant Prakash", "Sailesh Sidhwani", "Alexey Frolov", "Steven Hansen", "Petko Georgiev", "Tiberiu Sosea", "Chris Apps", "Aishwarya Kamath", "David Reid", "Emma Cooney", "Charlotte Magister", "Oriana Riva", "Alec Go", "Pu-Chin Chen", "Sebastian Krause", "Nir Levine", "Marco Fornoni", "Ilya Figotin", "Nick Roy", "Parsa Mahmoudieh", "Vladimir Magay", "Mukundan Madhavan", "Jin Miao", "Jianmo Ni", "Yasuhisa Fujii", "Ian Chou", "George Scrivener", "Zak Tsai", "Siobhan Mcloughlin", "Jeremy Selier", "Sandra Lefdal", "Jeffrey Zhao", "Abhijit Karmarkar", "Kushal Chauhan", "Shivanker Goel", "Zhaoyi Zhang", "Vihan Jain", "Parisa Haghani", "Mostafa Dehghani", "Jacob Scott", "Erin Farnese", "Anastasija Ili\u0107", "Steven Baker", "Julia Pawar", "Li Zhong", "Josh Camp", "Yoel Zeldes", "Shravya Shetty", "Anand Iyer", "V\u00edt List\u00edk", "Jiaxian Guo", "Luming Tang", "Mark Geller", "Simon Bucher", "Yifan Ding", "Hongzhi Shi", "Carrie Muir", "Dominik Grewe", "Ramy Eskander", "Octavio Ponce", "Boqing Gong", "Derek Gasaway", "Samira Khan", "Umang Gupta", "Angelos Filos", "Weicheng Kuo", "Klemen Kloboves", "Jennifer Beattie", "Christian Wright", "Leon Li", "Alicia Jin", "Sandeep Mariserla", "Miteyan Patel", "Jens Heitkaemper", "Dilip Krishnan", "Vivek Sharma", "David Bieber", "Christian Frank", "John Lambert", "Paul Caron", "Martin Polacek", "Mai Gim\u00e9nez", "Himadri Choudhury", "Xing Yu", "Sasan Tavakkol", "Arun Ahuja", "Franz Och", "Rodolphe Jenatton", "Wojtek Skut", "Bryan Richter", "David Gaddy", "Andy Ly", "Misha Bilenko", "Megh Umekar", "Ethan Liang", "Martin Sevenich", "Mandar Joshi", "Hassan Mansoor", "Rebecca Lin", "Sumit Sanghai", "Abhimanyu Singh", "Xiaowei Li", "Sudheendra Vijayanarasimhan", "Zaheer Abbas", "Yonatan Bitton", "Hansa Srinivasan", "Manish Reddy Vuyyuru", "Alexander Fr\u00f6mmgen", "Yanhua Sun", "Ralph Leith", "Alfonso Casta\u00f1o", "DJ Strouse", "Le Yan", "Austin Kyker", "Satish Kambala", "Mary Jasarevic", "Thibault Sellam", "Chao Jia", "Alexander Pritzel", "Raghavender R", "Huizhong Chen", "Natalie Clay", "Sudeep Gandhe", "Sean Kirmani", "Sayna Ebrahimi", "Hannah Kirkwood", "Jonathan Mallinson", "Chao Wang", "Adnan Ozturel", "Kuo Lin", "Shyam Upadhyay", "Vincent Cohen-Addad", "Sean Purser-haskell", "Yichong Xu", "Ebrahim Songhori", "Babi Seal", "Alberto Magni", "Almog Gueta", "Tingting Zou", "Guru Guruganesh", "Thais Kagohara", "Hung Nguyen", "Khalid Salama", "Alejandro Cruzado Ruiz", "Justin Frye", "Zhenkai Zhu", "Matthias Lochbrunner", "Simon Osindero", "Wentao Yuan", "Lisa Lee", "Aman Prasad", "Lam Nguyen Thiet", "Daniele Calandriello", "Victor Stone", "Qixuan Feng", "Han Ke", "Maria Voitovich", "Geta Sampemane", "Lewis Chiang", "Ling Wu", "Alexander Bykovsky", "Matt Young", "Luke Vilnis", "Ishita Dasgupta", "Aditya Chawla", "Qin Cao", "Bowen Liang", "Daniel Toyama", "Szabolcs Payrits", "Anca Stefanoiu", "Dimitrios Vytiniotis", "Ankesh Anand", "Tianxiao Shen", "Blagoj Mitrevski", "Michael Tschannen", "Sreenivas Gollapudi", "Aishwarya P S", "Jos\u00e9 Leal", "Zhe Shen", "Han Fu", "Wei Wang", "Arvind Kannan", "Doron Kukliansky", "Sergey Yaroshenko", "Svetlana Grant", "Umesh Telang", "David Wood", "Alexandra Chronopoulou", "Alexandru \u0162ifrea", "Tao Zhou", "Tony", "Nguy\\~\u00ean", "Muge Ersoy", "Anima Singh", "Meiyan Xie", "Emanuel Taropa", "Woohyun Han", "Eirikur Agustsson", "Andrei Sozanschi", "Hui Peng", "Alex Chen", "Yoel Drori", "Efren Robles", "Yang Gao", "Xerxes Dotiwalla", "Ying Chen", "Anudhyan Boral", "Alexei Bendebury", "John Nham", "Chris Tar", "Luis Castro", "Jiepu Jiang", "Canoee Liu", "Felix Halim", "Jinoo Baek", "Andy Wan", "Jeremiah Liu", "Yuan Cao", "Shengyang Dai", "Trilok Acharya", "Ruoxi Sun", "Fuzhao Xue", "Saket Joshi", "Morgane Lustman", "Yongqin Xian", "Rishabh Joshi", "Deep Karkhanis", "Nora Kassner", "Jamie Hall", "Xiangzhuo Ding", "Gan Song", "Gang Li", "Chen Zhu", "Yana Kulizhskaya", "Bin Ni", "Alexey Vlaskin", "Solomon Demmessie", "Lucio Dery", "Salah Zaiem", "Yanping Huang", "Cindy Fan", "Felix Gimeno", "Ananth Balashankar", "Koji Kojima", "Hagai Taitelbaum", "Maya Meng", "Dero Gharibian", "Sahil Singla", "Wei Chen", "Ambrose Slone", "Guanjie Chen", "Sujee Rajayogam", "Max Schumacher", "Suyog Kotecha", "Rory Blevins", "Qifei Wang", "Mor Hazan Taege", "Alex Morris", "Xin Liu", "Fayaz Jamil", "Richard Zhang", "Pratik Joshi", "Ben Ingram", "Tyler Liechty", "Ahmed Eleryan", "Scott Baird", "Alex Grills", "Gagan Bansal", "Shan Han", "Kiran Yalasangi", "Shawn Xu", "Majd Al Merey", "Isabel Gao", "Felix Weissenberger", "Igor Karpov", "Robert Riachi", "Ankit Anand", "Gautam Prasad", "Kay Lamerigts", "Reid Hayes", "Jamie Rogers", "Mandy Guo", "Ashish Shenoy", "Qiong", "Hu", "Kyle He", "Yuchen Liu", "Polina Zablotskaia", "Sagar Gubbi", "Yifan Chang", "Jay Pavagadhi", "Kristian Kjems", "Archita Vadali", "Diego Machado", "Yeqing Li", "Renshen Wang", "Dipankar Ghosh", "Aahil Mehta", "Dana Alon", "George Polovets", "Alessio Tonioni", "Nate Kushman", "Joel D'sa", "Lin Zhuo", "Allen Wu", "Rohin Shah", "John Youssef", "Jiayu Ye", "Justin Snyder", "Karel Lenc", "Senaka Buthpitiya", "Matthew Tung", "Jichuan Chang", "Tao Chen", "David Saxton", "Jenny Lee", "Lydia Lihui Zhang", "James Qin", "Prabakar Radhakrishnan", "Maxwell Chen", "Piotr Ambroszczyk", "Metin Toksoz-Exley", "Yan Zhong", "Nitzan Katz", "Brendan O'Donoghue", "Tamara von Glehn", "Adi Gerzi Rosenthal", "Aga \u015awietlik", "Xiaokai Zhao", "Nick Fernando", "Jinliang Wei", "Jieru Mei", "Sergei Vassilvitskii", "Diego Cedillo", "Pranjal Awasthi", "Hui Zheng", "Koray Kavukcuoglu", "Itay Laish", "Joseph Pagadora", "Marc Brockschmidt", "Christopher A. Choquette-Choo", "Arunkumar Byravan", "Yifeng Lu", "Xu Chen", "Mia Chen", "Kenton Lee", "Rama Pasumarthi", "Sijal Bhatnagar", "Aditya Shah", "Qiyin Wu", "Zhuoyuan Chen", "Zack Nado", "Bartek Perz", "Zixuan Jiang", "David Kao", "Ganesh Mallya", "Nino Vieillard", "Lantao Mei", "Sertan Girgin", "Mandy Jordan", "Yeongil Ko", "Alekh Agarwal", "Yaxin Liu", "Yasemin Altun", "Raoul de Liedekerke", "Anastasios Kementsietsidis", "Daiyi Peng", "Dangyi Liu", "Utku Evci", "Peter Humphreys", "Austin Tarango", "Xiang Deng", "Yoad Lewenberg", "Kevin Aydin", "Chengda Wu", "Bhavishya Mittal", "Tsendsuren Munkhdalai", "Kleopatra Chatziprimou", "Rodrigo Benenson", "Uri First", "Xiao Ma", "Jinning Li", "Armand Joulin", "Hamish Tomlinson", "Tingnan Zhang", "Milad Nasr", "Zhi Hong", "Micha\u00ebl Sander", "Lisa Anne Hendricks", "Anuj Sharma", "Andrew Bolt", "Eszter V\u00e9rtes", "Jiri Simsa", "Tomer Levinboim", "Olcan Sercinoglu", "Divyansh Shukla", "Austin Wu", "Craig Swanson", "Danny Vainstein", "Fan Bu", "Bo Wang", "Ryan Julian", "Charles Yoon", "Sergei Lebedev", "Antonious Girgis", "Bernd Bandemer", "David Du", "Todd Wang", "Xi Chen", "Ying Xiao", "Peggy Lu", "Natalie Ha", "Vlad Ionescu", "Simon Rowe", "Josip Matak", "Federico Lebron", "Andreas Steiner", "Lalit Jain", "Manaal Faruqui", "Nicolas Lacasse", "Georgie Evans", "Neesha Subramaniam", "Dean Reich", "Giulia Vezzani", "Aditya Pandey", "Joe Stanton", "Tianhao Zhou", "Liam McCafferty", "Henry Griffiths", "Verena Rieser", "Soheil Hassas Yeganeh", "Eleftheria Briakou", "Lu Huang", "Zichuan Wei", "Liangchen Luo", "Erik Jue", "Gabby Wang", "Victor Cotruta", "Myriam Khan", "Jongbin Park", "Qiuchen Guo", "Peiran Li", "Rong Rong", "Diego Antognini", "Anastasia Petrushkina", "Chetan Tekur", "Eli Collins", "Parul Bhatia", "Chester Kwak", "Wenhu Chen", "Arvind Neelakantan", "Immanuel Odisho", "Sheng Peng", "Vincent Nallatamby", "Vaibhav Tulsyan", "Fabian Pedregosa", "Peng Xu", "Raymond Lin", "Yulong Wang", "Emma Wang", "Sholto Douglas", "Reut Tsarfaty", "Elena Gribovskaya", "Renga Aravamudhan", "Manu Agarwal", "Mara Finkelstein", "Qiao Zhang", "Elizabeth Cole", "Phil Crone", "Sarmishta Velury", "Anil Das", "Chris Sauer", "Luyao Xu", "Danfeng Qin", "Chenjie Gu", "Dror Marcus", "CJ Zheng", "Wouter Van Gansbeke", "Sobhan Miryoosefi", "Haitian Sun", "YaGuang Li", "Charlie Chen", "Jae Yoo", "Pavel Dubov", "Alex Tomala", "Adams Yu", "Pawe\u0142 Weso\u0142owski", "Alok Gunjan", "Eddie Cao", "Jiaming Luo", "Nikhil Sethi", "Arkadiusz Socala", "Laura Graesser", "Tomas Kocisky", "Arturo BC", "Minmin Chen", "Edward Lee", "Sophie Wang", "Weize Kong", "Qiantong Xu", "Nilesh Tripuraneni", "Yiming Li", "Xinxin Yu", "Allen Porter", "Paul Voigtlaender", "Biao Zhang", "Arpi Vezer", "Sarah York", "Qing Wei", "Geoffrey Cideron", "Mark Kurzeja", "Seungyeon Kim", "Benny Li", "Ang\u00e9line Pouget", "Hyo Lee", "Kaspar Daugaard", "Yang Li", "Dave Uthus", "Aditya Siddhant", "Paul Cavallaro", "Sriram Ganapathy", "Maulik Shah", "Rolf Jagerman", "Jeff Stanway", "Piermaria Mendolicchio", "Li Xiao", "Kayi Lee", "Tara Thompson", "Shubham Milind Phal", "Jason Chase", "Sun Jae Lee", "Adrian N Reyes", "Disha Shrivastava", "Zhen Qin", "Roykrong Sukkerd", "Seth Odoom", "Lior Madmoni", "John Aslanides", "Jonathan Herzig", "Elena Pochernina", "Sheng Zhang", "Parker Barnes", "Daisuke Ikeda", "Qiujia Li", "Shuo-yiin Chang", "Shakir Mohamed", "Jim Sproch", "Richard Powell", "Bidisha Samanta", "Domagoj \u0106evid", "Anton Kovsharov", "Shrestha Basu Mallick", "Srinivas Tadepalli", "Anne Zheng", "Kareem Ayoub", "Andreas Noever", "Christian Reisswig", "Zhuo Xu", "Junhyuk Oh", "Martin Matysiak", "Tim Blyth", "Shereen Ashraf", "Julien Amelot", "Boone Severson", "Michele Bevilacqua", "Motoki Sano", "Ethan Dyer", "Ofir Roval", "Anu Sinha", "Yin Zhong", "Sagi Perel", "Tea Saboli\u0107", "Johannes Mauerer", "Willi Gierke", "Mauro Verzetti", "Rodrigo Cabrera", "Alvin Abdagic", "Steven Hemingray", "Austin Stone", "Jong Lee", "Farooq Ahmad", "Karthik Raman", "Lior Shani", "Jonathan Lai", "Orhan Firat", "Nathan Waters", "Eric Ge", "Mo Shomrat", "Himanshu Gupta", "Rajeev Aggarwal", "Tom Hudson", "Bill Jia", "Simon Baumgartner", "Palak Jain", "Joe Kovac", "Junehyuk Jung", "Ante \u017du\u017eul", "Will Truong", "Morteza Zadimoghaddam", "Songyou Peng", "Marco Liang", "Rachel Sterneck", "Balaji Lakshminarayanan", "Machel Reid", "Oliver Woodman", "Tong Zhou", "Jianling Wang", "Vincent Coriou", "Arjun Narayanan", "Jay Hoover", "Yenai Ma", "Apoorv Jindal", "Clayton Sanford", "Doug Reid", "Swaroop Ramaswamy", "Alex Kurakin", "Roland Zimmermann", "Yana Lunts", "Dragos Dena", "Zal\u00e1n Borsos", "Vered Cohen", "Shujian Zhang", "Will Grathwohl", "Robert Dadashi", "Morgan Redshaw", "Joshua Kessinger", "Julian Odell", "Silvano Bonacina", "Zihang Dai", "Grace Chen", "Ayush Dubey", "Pablo Sprechmann", "Mantas Pajarskas", "Wenxuan Zhou", "Niharika Ahuja", "Tara Thomas", "Martin Nikoltchev", "Matija Kecman", "Bharath Mankalale", "Andrey Ryabtsev", "Jennifer She", "Christian Walder", "Jiaming Shen", "Lu Li", "Carolina Parada", "Sheena Panthaplackel", "Okwan Kwon", "Matt Lawlor", "Utsav Prabhu", "Yannick Schroecker", "Marc'aurelio Ranzato", "Pete Blois", "Iurii Kemaev", "Ting Yu", "Dmitry", "Lepikhin", "Hao Xiong", "Sahand Sharifzadeh", "Oleaser Johnson", "Jeremiah Willcock", "Rui Yao", "Greg Farquhar", "Sujoy Basu", "Hidetoshi Shimokawa", "Nina Anderson", "Haiguang Li", "Khiem Pham", "Yizhong Liang", "Sebastian Borgeaud", "Alexandre Moufarek", "Hideto Kazawa", "Blair Kutzman", "Marcin Sieniek", "Sara Smoot", "Ruth Wang", "Natalie Axelsson", "Nova Fallen", "Prasha Sundaram", "Yuexiang Zhai", "Varun Godbole", "Petros Maniatis", "Alek Wang", "Ilia Shumailov", "Santhosh Thangaraj", "Remi Crocker", "Nikita Gupta", "Gang Wu", "Phil Chen", "Gell\u00e9rt Weisz", "Celine Smith", "Mojtaba Seyedhosseini", "Boya Fang", "Xiyang Luo", "Roey Yogev", "Zeynep Cankara", "Andrew Hard", "Helen Ran", "Rahul Sukthankar", "George Necula", "Ga\u00ebl Liu", "Honglong Cai", "Praseem Banzal", "Daniel Keysers", "Sanjay Ghemawat", "Connie Tao", "Emma Dunleavy", "Aditi Chaudhary", "Wei Li", "Maciej Miku\u0142a", "Chen-Yu Lee", "Tiziana Refice", "Krishna Somandepalli", "Alexandre Fr\u00e9chette", "Dan Bahir", "John Karro", "Keith Rush", "Sarah Perrin", "Bill Rosgen", "Xiaomeng Yang", "Clara Huiyi Hu", "Mahmoud Alnahlawi", "Justin Mao-Jones", "Roopal Garg", "Hoang Nguyen", "Bat-Orgil Batsaikhan", "I\u00f1aki Iturrate", "Anselm Levskaya", "Avi Singh", "Ashyana Kachra", "Tony Lu", "Denis Petek", "Zheng Xu", "Mark Graham", "Lukas Zilka", "Yael Karov", "Marija Kostelac", "Fangyu Liu", "Yaohui Guo", "Weiyue Wang", "Bernd Bohnet", "Emily Pitler", "Tony Bruguier", "Keisuke Kinoshita", "Chrysovalantis Anastasiou", "Nilpa Jha", "Ting Liu", "Jerome Connor", "Phil Wallis", "Philip Pham", "Eric Bailey", "Shixin Li", "Heng-Tze Cheng", "Sally Ma", "Haiqiong Li", "Akanksha Maurya", "Kate Olszewska", "Manfred Warmuth", "Christy Koh", "Dominik Paulus", "Siddhartha Reddy Jonnalagadda", "Enrique Piqueras", "Ali Elqursh", "Geoff Brown", "Hadar Shemtov", "Loren Maggiore", "Fei Xia", "Ryan Foley", "Beka Westberg", "George van den Driessche", "Livio Baldini Soares", "Arjun Kar", "Michael Quinn", "Siqi Zuo", "Jialin Wu", "Kyle Kastner", "Anna Bortsova", "Aijun Bai", "Ales Mikhalap", "Luowei Zhou", "Jennifer Brennan", "Vinay Ramasesh", "Honglei Zhuang", "John Maggs", "Johan Schalkwyk", "Yuntao Xu", "Hui Huang", "Andrew Howard", "Sasha Brown", "Linting Xue", "Gloria Shen", "Brian Albert", "Neha Jha", "Daniel Zheng", "Varvara Krayvanova", "Spurthi Amba Hombaiah", "Olivier Lacombe", "Gautam Vasudevan", "Dan Graur", "Tian Xie", "Meet Gandhi", "Bangju Wang", "Dustin Zelle", "Harman Singh", "Dahun Kim", "S\u00e9bastien Cevey", "Victor Ungureanu", "Natasha Noy", "Fei Liu", "Annie Xie", "Fangxiaoyu Feng", "Katerina Tsihlas", "Daniel Formoso", "Neera Vats", "Quentin Wellens", "Yinan Wang", "Niket Kumar Bhumihar", "Samrat Ghosh", "Matt Hoffman", "Tom Lieber", "Oran Lang", "Kush Bhatia", "Tom Paine", "Aroonalok Pyne", "Ronny Votel", "Madeleine Clare Elish", "Benoit Schillings", "Alex Panagopoulos", "Haichuan Yang", "Adam Raveret", "Zohar Yahav", "Shuang Liu", "Warren Chen", "Dalia El Badawy", "Nishant Agrawal", "Mohammed Badawi", "Mahdi Mirzazadeh", "Carla Bromberg", "Fan Ye", "Chang Liu", "Tatiana Sholokhova", "George-Cristian Muraru", "Gargi Balasubramaniam", "Jonathan Malmaud", "Alen Carin", "Danilo Martins", "Irina Jurenka", "Pankil Botadra", "Dave Lacey", "Richa Singh", "Mariano Schain", "Dan Zheng", "Isabelle Guyon", "Victor Lavrenko", "Seungji Lee", "Xiang Zhou", "Demis Hassabis", "Jeshwanth Challagundla", "Derek Cheng", "Nikhil Mehta", "Matthew Mauger", "Michela Paganini", "Pushkar Mishra", "Kate Lee", "Zhang Li", "Lexi Baugher", "Ondrej Skopek", "Max Chang", "Amir Zait", "Gaurav Menghani", "Lizzetth Bellot", "Guangxing Han", "Jean-Michel Sarr", "Sharat Chikkerur", "Himanshu Sahni", "Rohan Anil", "Arun Narayanan", "Chandu Thekkath", "Daniele Pighin", "Hana Strej\u010dek", "Marko Velic", "Fred Bertsch", "Manuel Tragut", "Keran Rong", "Alicia Parrish", "Kai Bailey", "Jiho Park", "Isabela Albuquerque", "Abhishek Bapna", "Rajesh Venkataraman", "Alec Kosik", "Johannes Griesser", "Zhiwei Deng", "Alek Andreev", "Qingyun Dou", "Kevin Hui", "Fanny Wei", "Xiaobin Yu", "Lei Shu", "Avia Aharon", "David Barker", "Badih Ghazi", "Sebastian Flennerhag", "Chris Breaux", "Yuchuan Liu", "Matthew Bilotti", "Josh Woodward", "Uri Alon", "Stephanie Winkler", "Tzu-Kuo Huang", "Kostas Andriopoulos", "Jo\u00e3o Gabriel Oliveira", "Penporn Koanantakool", "Berkin Akin", "Michael Wunder", "Cicero Nogueira dos Santos", "Mohammad Hossein Bateni", "Lin Yang", "Dan Horgan", "Beer Changpinyo", "Keyvan Amiri", "Min Ma", "Dayeong Lee", "Lihao Liang", "Anirudh Baddepudi", "Tejasi Latkar", "Raia Hadsell", "Jun Xu", "Hairong Mu", "Michael Han", "Aedan Pope", "Snchit Grover", "Frank Kim", "Ankit Bhagatwala", "Guan Sun", "Yamini Bansal", "Amir Globerson", "Alireza Nazari", "Samira Daruki", "Hagen Soltau", "Jane Labanowski", "Laurent El Shafey", "Matt Harvey", "Yanif Ahmad", "Elan Rosenfeld", "William Kong", "Etienne Pot", "Yi-Xuan Tan", "Aurora Wei", "Victoria Langston", "Marcel Prasetya", "Petar Veli\u010dkovi\u0107", "Richard Killam", "Robin Strudel", "Darren Ni", "Zhenhai Zhu", "Aaron Archer", "Kavya Kopparapu", "Lynn Nguyen", "Emilio Parisotto", "Hussain Masoom", "Sravanti Addepalli", "Jordan Grimstad", "Hexiang Hu", "Joss Moore", "Avinatan Hassidim", "Le Hou", "Mukund Raghavachari", "Jared Lichtarge", "Adam R. Brown", "Hilal Dib", "Natalia Ponomareva", "Justin Fu", "Yujing Zhang", "Altaf Rahman", "Joana Iljazi", "Edouard Leurent", "Gabriel Dulac-Arnold", "Cosmo Du", "Chulayuth Asawaroengchai", "Larry Jin", "Ela Gruzewska", "Ziwei Ji", "Benigno Uria", "Daniel De Freitas", "Paul Barham", "Lauren Beltrone", "V\u00edctor Campos", "Jun Yan", "Neel Kovelamudi", "Arthur Nguyen", "Elinor Davies", "Zhichun Wu", "Zoltan Egyed", "Kristina Toutanova", "Nithya Attaluri", "Hongliang Fei", "Peter Stys", "Siddhartha Brahma", "Martin Izzard", "Siva Velusamy", "Scott Lundberg", "Vincent Zhuang", "Kevin Sequeira", "Adam Santoro", "Ehsan Amid", "Ophir Aharoni", "Shuai Ye", "Mukund Sundararajan", "Lijun Yu", "Yu-Cheng Ling", "Stephen Spencer", "Hugo Song", "Josip Djolonga", "Christo Kirov", "Sonal Gupta", "Alessandro Bissacco", "Clemens Meyer", "Mukul Bhutani", "Andrew Dai", "Weiyi Wang", "Siqi Liu", "Ashwin Sreevatsa", "Qijun Tan", "Maria Wang", "Lucy Kim", "Yicheng Wang", "Alex Irpan", "Yang Xiao", "Stanislav Fort", "Yifan He", "Alex Gurney", "Bryan Gale", "Yue Ma", "Monica Roy", "Viorica Patraucean", "Taylan Bilal", "Golnaz Ghiasi", "Anahita Hosseini", "Melvin Johnson", "Zhuowan Li", "Yi Tay", "Benjamin Beyret", "Katie Millican", "Josef Broder", "Mayank Lunayach", "Danny Swisher", "Eugen Vu\u0161ak", "David Parkinson", "MH Tessler", "Adi Mayrav Gilady", "Richard Song", "Allan Dafoe", "Yves Raimond", "Masa Yamaguchi", "Itay Karo", "Elizabeth Nielsen", "Kevin Kilgour", "Mike Dusenberry", "Rajiv Mathews", "Jiho Choi", "Siyuan Qiao", "Harsh Mehta", "Sahitya Potluri", "Chris Knutsen", "Jialu Liu", "Tat Tan", "Kuntal Sengupta", "Keerthana Gopalakrishnan", "Abodunrinwa Toki", "Mencher Chiang", "Mike Burrows", "Grace Vesom", "Zafarali Ahmed", "Ilia Labzovsky", "Siddharth Vashishtha", "Preeti Singh", "Ankur Sharma", "Ada Ma", "Jinyu Xie", "Pranav Talluri", "Hannah Forbes-Pollard", "Aarush Selvan", "Joel Wee", "Loic Matthey", "Tom Funkhouser", "Parthasarathy Gopavarapu", "Lev Proleev", "Cheng Li", "Matt Thomas", "Kashyap Kolipaka", "Zhipeng Jia", "Ashwin Kakarla", "Srinivas Sunkara", "Joan Puigcerver", "Suraj Satishkumar Sheth", "Emily Graves", "Chen Wang", "Sadh MNM Khan", "Kai Kang", "Shyamal Buch", "Fred Zhang", "Omkar Savant", "David Soergel", "Kevin Lee", "Linda Friso", "Xuanyi Dong", "Rahul Arya", "Shreyas Chandrakaladharan", "Connor Schenck", "Greg Billock", "Tejas Iyer", "Anton Bakalov", "Leslie Baker", "Alex Ruiz", "Angad Chandorkar", "Trieu Trinh", "Matt Miecnikowski", "Yanqi Zhou", "Yangsibo Huang", "Jiazhong Nie", "Ali Shah", "Ashish Thapliyal", "Sam Haves", "Lun Wang", "Uri Shaham", "Patrick Morris-Suzuki", "Soroush Radpour", "Leonard Berrada", "Thomas Strohmann", "Chaochao Yan", "Jingwei Shen", "Sonam Goenka", "Tris Warkentin", "Petar Devi\u0107", "Dan Belov", "Albert Webson", "Madhavi Yenugula", "Puranjay Datta", "Jerry Chang", "Nimesh Ghelani", "Aviral Kumar", "Vincent Perot", "Jessica Lo", "Yang Song", "Herman Schmit", "Jianmin Chen", "Vasilisa Bashlovkina", "Xiaoyue Pan", "Diana Mincu", "Paul Roit", "Isabel Edkins", "Andy Davis", "Yujia Li", "Ben Horn", "Xinjian Li", "Pradeep Kumar S", "Eric Doi", "Wanzheng Zhu", "Sri Gayatri Sundara Padmanabhan", "Siddharth Verma", "Jasmine Liu", "Heng Chen", "Mihajlo Velimirovi\u0107", "Malcolm Reynolds", "Priyanka Agrawal", "Nick Sukhanov", "Abhinit Modi", "Siddharth Goyal", "John Palowitch", "Nima Khajehnouri", "Wing Lowe", "David Klinghoffer", "Sharon Silver", "Vinh Tran", "Candice Schumann", "Francesco Piccinno", "Xi Liu", "Mario Lu\u010di\u0107", "Xiaochen Yang", "Sandeep Kumar", "Ajay Kannan", "Ragha Kotikalapudi", "Mudit Bansal", "Fabian Fuchs", "Javad Hosseini", "Abdelrahman Abdelhamed", "Dawn Bloxwich", "Tianhe Yu", "Ruoxin Sang", "Gregory Thornton", "Karan Gill", "Yuchi Liu", "Virat Shejwalkar", "Jason Lin", "Zhipeng Yan", "Kehang Han", "Thomas Buschmann", "Michael Pliskin", "Zhi Xing", "Susheel Tatineni", "Junlin Zhang", "Sissie Hsiao", "Gavin Buttimore", "Marcus Wu", "Zefei Li", "Geza Kovacs", "Legg Yeung", "Tao Huang", "Aaron Cohen", "Bethanie Brownfield", "Averi Nowak", "Mikel Rodriguez", "Tianze Shi", "Hado van Hasselt", "Kevin Cen", "Deepanway Ghoshal", "Kushal Majmundar", "Weiren Yu", "Warren", "Chen", "Danila Sinopalnikov", "Hao Zhang", "Vlado Gali\u0107", "Di Lu", "Zeyu Zheng", "Maggie Song", "Gary Wang", "Gui Citovsky", "Swapnil Gawde", "Isaac Galatzer-Levy", "David Silver", "Ivana Balazevic", "Dipanjan Das", "Kingshuk Majumder", "Yale Cong", "Praneet Dutta", "Dustin Tran", "Hui Wan", "Junwei Yuan", "Daniel Eppens", "Alanna Walton", "Been Kim", "Harry Ragan", "James Cobon-Kerr", "Lu Liu", "Weijun Wang", "Bryce Petrini", "Jack Rae", "Rakesh Shivanna", "Yan Xiong", "Chace Lee", "Pauline Coquinot", "Yiming Gu", "Lisa Patel", "Blake Hechtman", "Aviel Boag", "Orion Jankowski", "Alex Wertheim", "Alex Lee", "Paul Covington", "Hila Noga", "Sam Sobell", "Shanthal Vasanth", "William Bono", "Chirag Nagpal", "Wei Fan", "Xavier Garcia", "Kedar Soparkar", "Aybuke Turker", "Nathan Howard", "Sachit Menon", "Yuankai Chen", "Vikas Verma", "Vladimir Pchelin", "Harish Rajamani", "Valentin Dalibard", "Ana Ramalho", "Yang Guo", "Kartikeya Badola", "Seojin Bang", "Nathalie Rauschmayr", "Julia Proskurnia", "Sudeep Dasari", "Xinyun Chen", "Mikhail Sushkov", "Anja Hauth", "Pauline Sho", "Abhinav Singh", "Bilva Chandra", "Allie Culp", "Max Dylla", "Olivier Bachem", "James Besley", "Heri Zhao", "Timothy Lillicrap", "Wei Wei", "Wael Al Jishi", "Ning Niu", "Alban Rrustemi", "Rapha\u00ebl Lopez Kaufman", "Ryan Poplin", "Jewel Zhao", "Minh Truong", "Shikhar Bharadwaj", "Ester Hlavnova", "Eli Stickgold", "Cordelia Schmid", "Georgi Stephanov", "Zhaoqi Leng", "Frederick Liu", "L\u00e9onard Hussenot", "Shenil Dodhia", "Juliana Vicente Franco", "Lesley Katzen", "Abhanshu Sharma", "Sarah Cogan", "Zuguang Yang", "Aniket Ray", "Sergi Caelles", "Shen Yan", "Ravin Kumar", "Daniel Gillick", "Renee Wong", "Joshua Ainslie", "Jonathan Hoech", "S\u00e9b Arnold", "Dan Abolafia", "Anca Dragan", "Ben Hora", "Grace Hu", "Alexey Guseynov", "Yang Lu", "Chas Leichner", "Jinmeng Rao", "Abhimanyu Goyal", "Nagabhushan Baddi", "Daniel Hernandez Diaz", "Tim McConnell", "Max Bain", "Jake Abernethy", "Qiqi Yan", "Rylan Schaeffer", "Paul Vicol", "Will Thompson", "Montse Gonzalez Arenas", "Mathias Bellaiche", "Pablo Barrio", "Stefan Zinke", "Riccardo Patana", "Pulkit Mehta", "JK Kearns", "Avraham Ruderman", "Scott Pollom", "David D'Ambrosio", "Cath Hope", "Yang Yu", "Andrea Gesmundo", "Kuang-Huei Lee", "Aviv Rosenberg", "Yiqian Zhou", "Yaoyiran Li", "Drew Garmon", "Yonghui Wu", "Safeen Huda", "Gil Fidel", "Martin Baeuml", "Jian Li", "Phoebe Kirk", "Rhys May", "Tao Tu", "Sara Mc Carthy", "Toshiyuki Fukuzawa", "Miranda Aperghis", "Chih-Kuan Yeh", "Toshihiro Yoshino", "Bo Li", "Austin Myers", "Kaisheng Yao", "Ben Limonchik", "Changwan Ryu", "Rohun Saxena", "Alex Goldin", "Ruizhe Zhao", "Rocky Rhodes", "Tao Zhu", "Divya Tyam", "Heidi Howard", "Nathan Byrd", "Hongxu Ma", "Yan Wu", "Ryan Mullins", "Qingze Wang", "Aida Amini", "Sebastien Baur", "Yiran Mao", "Subhashini Venugopalan", "Will Song", "Wen Ding", "Paul Collins", "Sashank Reddi", "Megan Shum", "Andrei Rusu", "Luisa Zintgraf", "Kelvin Chan", "Sheela Goenka", "Mathieu Blondel", "Michael Collins", "Renke Pan", "Marissa Giustina", "Nikolai Chinaev", "Christian Schuler", "Ce Zheng", "Jonas Valfridsson", "Alyssa Loo", "Alex Yakubovich", "Jamie Smith", "Tao Jiang", "Rich Munoz", "Gabriel Barcik", "Rishabh Bansal", "Mingyao Yang", "Yilun Du", "Pablo Duque", "Mary Phuong", "Alexandra Belias", "Kunal Lad", "Zeyu Liu", "Tal Schuster", "Karthik Duddu", "Jieru Hu", "Paige Kunkle", "Matthew Watson", "Jackson Tolins", "Josh Smith", "Denis Teplyashin", "Garrett Bingham", "Marvin Ritter", "Marco Andreetto", "Divya Pitta", "Mohak Patel", "Shashank Viswanadha", "Trevor Strohman", "Catalin Ionescu", "Jincheng Luo", "Yogesh Kalley", "Jeremy Wiesner", "Dan Deutsch", "Derek Lockhart", "Peter Choy", "Rumen Dangovski", "Chawin Sitawarin", "Cat Graves", "Tanya Lando", "Joost van Amersfoort", "Ndidi Elue", "Zhouyuan Huo", "Pooya Moradi", "Jean Tarbouriech", "Henryk Michalewski", "Wenting Ye", "Eunyoung Kim", "Alex Druinsky", "Florent Altch\u00e9", "Xinyi Chen", "Artur Dwornik", "Da-Cheng Juan", "Rivka Moroshko", "Horia Toma", "Jarrod Kahn", "Hai Qian", "Maximilian Sieb", "Irene Cai", "Roman Goldenberg", "Praneeth Netrapalli", "Sindhu Raghuram", "Yuan Gong", "Lijie Fan", "Evan Palmer", "Yossi Matias", "Valentin Gabeur", "Shreya Pathak", "Tom Ouyang", "Don Metzler", "Geoff Bacon", "Srinivasan Venkatachary", "Sridhar Thiagarajan", "Alex Cullum", "Eran Ofek", "Vytenis Sakenas", "Mohamed Hammad", "Cesar Magalhaes", "Mayank Daswani", "Oscar Chang", "Ashok Popat", "Ruichao Li", "Komal Jalan", "Yanhan Hou", "Josh Lipschultz", "Antoine He", "Wenhao Jia", "Pier Giuseppe Sessa", "Prateek Kolhar", "William Wong", "Sumeet Singh", "Lukas Haas", "Jay Whang", "Hanna Klimczak-Pluci\u0144ska", "Georges Rotival", "Grace Chung", "Yiqing Hua", "Anfal Siddiqui", "Nicolas Serrano", "Dongkai Chen", "Billy Porter", "Libin Bai", "Keshav Shivam", "Sho Arora", "Partha Talukdar", "Tom Cobley", "Sangnie Bhardwaj", "Evgeny Gladchenko", "Simon Green", "Kelvin Guu", "Felix Fischer", "Xiao Wu", "Eric Wang", "Achintya Singhal", "Tatiana Matejovicova", "James Martens", "Hongji Li", "Roma Patel", "Elizabeth Kemp", "Jiaqi Pan", "Lily Wang", "Blake JianHang Chen", "Jean-Baptiste Alayrac", "Navneet Potti", "Erika Gemzer", "Eugene Ie", "Kay McKinney", "Takaaki Saeki", "Edward Chou", "Pascal Lamblin", "SQ Mah", "Zach Fisher", "Martin Chadwick", "Jon Stritar", "Obaid Sarvana", "Andrew Hogue", "Artem Shtefan", "Hadi Hashemi", "Yang Xu", "Jindong Gu", "Sharad Vikram", "Chung-Ching Chang", "Sabela Ramos", "Logan Kilpatrick", "Weijuan Xi", "Jenny Brennan", "Yinghao Sun", "Abhishek Jindal", "Ionel Gog", "Dawn Chen", "Felix Wu", "Jason Lee", "Sudhindra Kopalle", "Srinadh Bhojanapalli", "Oriol Vinyals", "Natan Potikha", "Burcu Karagol Ayan", "Yuan Yuan", "Michael Riley", "Piotr Stanczyk", "Sergey Kishchenko", "Bing Wang", "Dan Garrette", "Antoine Yang", "Vlad Feinberg", "CJ Carey", "Javad Azizi", "Viral Shah", "Erica Moreira", "Chongyang Shi", "Josh Feldman", "Elizabeth Salesky", "Thomas Lampe", "Aneesh Pappu", "Duhyeon Kim", "Jonas Adler", "Avi Caciularu", "Brian Walker", "Yunhan Xu", "Yochai Blau", "Dylan Scandinaro", "Terry Huang", "Sam El-Husseini", "Abhishek Sinha", "Lijie Ren", "Taylor Tobin", "Patrik Sundberg", "Tim Sohn", "Vikas Yadav", "Mimi Ly", "Emily Xue", "Jing Xiong", "Afzal Shama Soudagar", "Sneha Mondal", "Nikhil Khadke", "Qingchun Ren", "Ben Vargas", "Stan Bileschi", "Sarah Chakera", "Cindy Wang", "Boyu Wang", "Yoni Halpern", "Joe Jiang", "Vikas Sindhwani", "Petre Petrov", "Pranavaraj Ponnuramu", "Sanket Vaibhav Mehta", "Yu Watanabe", "Betty Chan", "Matheus Wisniewski", "Trang Pham", "Jingwei Zhang", "Conglong Li", "Dario de Cesare", "Art Khurshudov", "Alex Vasiloff", "Melissa Tan", "Zoe Ashwood", "Bobak Shahriari", "Maryam Majzoubi", "Garrett Tanzer", "Olga Kozlova", "Robin Alazard", "James Lee-Thorp", "Nguyet Minh Phu", "Isaac Tian", "Junwhan Ahn", "Andy Crawford", "Lauren Lax", "Yuan", "Shangguan", "Iftekhar Naim", "David Ross", "Oleksandr Ferludin", "Tongfei Guo", "Andrea Banino", "Hubert Soyer", "Xiaoen Ju", "Dominika Rogozi\u0144ska", "Ishaan Malhi", "Marcella Valentine", "Daniel Balle", "Apoorv Kulshreshtha", "Maciej Kula", "Yiwen Song", "Sophia Austin", "John Schultz", "Roy Hirsch", "Arthur Douillard", "Apoorv Reddy", "Michael Fink", "Summer Yue", "Khyatti Gupta", "Adam Zhang", "Norman Rink", "Daniel McDuff", "Lei Meng", "Andr\u00e1s Gy\u00f6rgy", "Yasaman Razeghi", "Ricky Liang", "Kazuki Osawa", "Aviel Atias", "Matan Eyal", "Tyrone Hill", "Nikolai Grigorev", "Zhengdong Wang", "Nitish Kulkarni", "Rachel Soh", "Ivan Lobov", "Zachary Charles", "Sid Lall", "Kazuma Hashimoto", "Ido Kessler", "Victor Gomes", "Zelda Mariet", "Danny Driess", "Alessandro Agostini", "Canfer Akbulut", "Jingcao Hu", "Marissa Ikonomidis", "Emily Caveness", "Kartik Audhkhasi", "Saurabh Agrawal", "Ioana Bica", "Evan Senter", "Jayaram Mudigonda", "Kelly Chen", "Jingchen Ye", "Xuanhui Wang", "James Svensson", "Philipp Fr\u00e4nken", "Josh Newlan", "Li Lao", "Eva Schnider", "Sami Alabed", "Joseph Kready", "Jesse Emond", "Afief Halumi", "Tim Zaman", "Chengxi Ye", "Naina Raisinghani", "Vilobh Meshram", "Bo Chang", "Ankit Singh Rawat", "Axel Stjerngren", "Sergey Levi", "Rui Wang", "Xiangzhu Long", "Mitchelle Rasquinha", "Steven Hand", "Aditi Mavalankar", "Lauren Agubuzu", "Sudeshna Roy", "Junquan Chen", "Jarek Wilkiewicz", "Hao Zhou", "Michal Jastrzebski", "Qiong Hu", "Agustin Dal Lago", "Ramya Sree Boppana", "Wei-Jen Ko", "Jennifer Prendki", "Yao Su", "Zhi Li", "Eliza Rutherford", "Girish Ramchandra Rao", "Ramona Comanescu", "Adri\u00e0 Puigdom\u00e8nech", "Qihang Chen", "Dessie Petrova", "Christine Chan", "Vedrana Milutinovic", "Felipe Tiengo Ferreira", "Chin-Yi Cheng", "Ming Zhang", "Tapomay Dey", "Sherry Yang", "Ramesh Sampath", "Quoc Le", "Howard Zhou", "Chu-Cheng Lin", "Hoi Lam", "Christine Kaeser-Chen", "Kai Hui", "Dean Hirsch", "Tom Eccles", "Basil Mustafa", "Shruti Rijhwani", "Morgane Rivi\u00e8re", "Yuanzhong Xu", "Junjie Wang", "Xinyang Geng", "Xiance Si", "Arjun Khare", "Cheolmin Kim", "Vahab Mirrokni", "Kamyu Lee", "Khuslen Baatarsukh", "Nathaniel Braun", "Lisa Wang", "Pallavi LV", "Richard Tanburn", "Yuvein", "Zhu", "Fangda Li", "Setareh Ariafar", "Dan Goldberg", "Ken Burke", "Daniil Mirylenka", "Meiqi Guo", "Olaf Ronneberger", "Hadas Natalie Vogel", "Liqun Cheng", "Nishita Shetty", "Johnson Jia", "Thomas Jimma", "Corey Fry", "Ted Xiao", "Martin Sundermeyer", "Ryan Burnell", "Yannis Assael", "Mario Pinto", "JD Chen", "Rohit Sathyanarayana", "Donghyun Cho", "Jing Lu", "Rishabh Agarwal", "Sugato Basu", "Lucas Gonzalez", "Dhruv Shah", "Meng Wei", "Dre Mahaarachchi", "Rohan Agrawal", "Tero Rissa", "Yani Donchev", "Ramiro Leal-Cavazos", "Adrian Hutter", "Markus Mircea", "Alon Jacovi", "Faruk Ahmed", "Jiageng Zhang", "Shuguang Hu", "Bo-Juen Chen", "Jonni Kanerva", "Guillaume Desjardins", "Andrew Lee", "Nikos Parotsidis", "Asier Mujika", "Tobias Weyand", "Jasper Snoek", "Jo Chick", "Kai Chen", "Paul Chang", "Ethan Mahintorabi", "Zi Wang", "Tolly Powell", "Orgad Keller", "Abhirut Gupta", "Claire Sha", "Kanav Garg", "Nicolas Heess", "\u00c1goston Weisz", "Cassidy Hardin", "Bartek Wydrowski", "Ben Coleman", "Karina Zainullina", "Pankaj Joshi", "Alessandro Epasto", "Terry Spitz", "Binbin Xiong", "Kai Zhao", "Arseniy Klimovskiy", "Ivy Zheng", "Johan Ferret", "Itay Yona", "Waleed Khawaja", "Jean-Baptiste Lespiau", "Maxim Krikun", "Siamak Shakeri", "Timothee Cour", "Bonnie Li", "Igor Krivokon", "Dan Suh", "Alex Hofer", "Jad Al Abdallah", "Nikita Putikhin", "Oscar Akerlund", "Silvio Lattanzi", "Anurag Kumar", "Shane Settle", "Himanshu Srivastava", "Folawiyo Campbell-Ajala", "Edouard Rosseel", "Mihai Dorin Istin", "Nishanth Dikkala", "Anand Rao", "Nick Young", "Kate Lin", "Dhruva Bhaswar", "Yiming Wang", "Jaume Sanchez Elias", "Kritika Muralidharan", "James Keeling", "Dayou Du", "Siddharth Gopal", "Gregory Dibb", "Charles Blundell", "Manolis Delakis", "Jacky Liang", "Marco Tulio Ribeiro", "Georgi Karadzhov", "Guillermo Garrido", "Ankur Bapna", "Jiawei Cao", "Adam Sadovsky", "Pouya Tafti", "Arthur Guez", "Coline Devin", "Yixian Di", "Jinwei Xing", "Chuqiao", "Xu", "Hanzhao Lin", "Chun-Te Chu", "Sameera Ponda", "Wesley Helmholz", "Fan Yang", "Yue Gao", "Sara Javanmardi", "Wael Farhan", "Alex Ramirez", "Ricardo Figueira", "Khe Chai Sim", "Yuval Bahat", "Ashwin Vaswani", "Liangzhe Yuan", "Gufeng Zhang", "Leland Rechis", "Hanjun Dai", "Tayo Oguntebi", "Alexandra Cordell", "Eug\u00e9nie Rives", "Kaan Tekelioglu", "Naveen Kumar", "Bing Zhang", "Aurick Zhou", "Nikolay Savinov", "Andrew Leach", "Alex Tudor", "Sanjay Ganapathy", "Yanyan Zheng", "Mirko Rossini", "Vera Axelrod", "Arnaud Autef", "Yukun Zhu", "Zheng Zheng", "Mingda Zhang", "Baochen Sun", "Jie Ren", "Nenad Tomasev", "Nithish Kannan", "Amer Sinha", "Charles Chen", "Louis O'Bryan", "Alex Pak", "Aditya Kusupati", "Weel Yang", "Deepak Ramachandran", "Patrick Griffin", "Seokhwan Kim", "Philipp Neubeck", "Craig Schiff", "Tammo Spalink", "Mingyang Ling", "Arun Nair", "Ga-Young Joung", "Linda Deng", "Avishkar Bhoopchand", "Lora Aroyo", "Tom Duerig", "Jordan Griffith", "Gabe Barth-Maron", "Jake Ades", "Alex Haig", "Ankur Taly", "Yunting Song", "Paul Michel", "Dave Orr", "Dean Weesner", "Corentin Tallec", "Carrie Grimes Bostock", "Paul Niemczyk", "Andy Twigg", "Mudit Verma", "Rohith Vallu", "Henry Wang", "Marco Gelmi", "Kiranbir Sodhia", "Aleksandr Chuklin", "Omer Goldman", "Jasmine George", "Liang Bai", "Kelvin Zhang", "Petar Sirkovic", "Efrat Nehoran", "Golan Pundak", "Jiaqi Mu", "Alice Chen", "Alex Greve", "Paulo Zacchello", "David Amos", "Heming Ge", "Eric Noland", "Colton Bishop", "Jeffrey Dudek", "Youhei Namiki", "Elena Buchatskaya", "Jing Li", "Dorsa Sadigh", "Masha Samsikova", "Dan Malkin", "Damien Vincent", "Robert David", "Rob Willoughby", "Phoenix Meadowlark", "Shawn Gao", "Yan Li", "Raj Apte", "Amit Jhindal", "Stein Xudong Lin", "Alex Polozov", "Zhicheng Wang", "Tomas Mery", "Anirudh GP", "Varun Yerram", "Sage Stevens", "Tianqi Liu", "Noah Fiedel", "Charles Sutton", "Matthew Johnson", "Xiaodan Song", "Kate Baumli", "Nir Shabat", "Muqthar Mohammad", "Hao Liu", "Marco Selvi", "Yichao Zhou", "Mehdi Hafezi Manshadi", "Chu-ling Ko", "Anthony Chen", "Michael Bendersky", "Jorge Gonzalez Mendez", "Nisarg Kothari", "Amir Zandieh", "Yiling Huang", "Daniel Andor", "Ellie Pavlick", "Idan Brusilovsky", "Jitendra Harlalka", "Sally Goldman", "Andrew Lampinen", "Guowang Li", "Asahi Ushio", "Somit Gupta", "Lei Zhang", "Chuyuan Kelly Fu", "Madhavi Sewak", "Timo Denk", "Jed Borovik", "Brendan Jou", "Avital Zipori", "Prateek Jain", "Junwen Bai", "Thang Luong", "Jonathan Tompson", "Alice Li", "Li Liu", "George Powell", "Jiajun Shen", "Alex Feng", "Grishma Chole", "Da Yu", "Yinlam Chow", "Tongxin Yin", "Eric Malmi", "Kefan Xiao", "Yash Pande", "Shachi Paul", "Niccol\u00f2 Dal Santo", "Adil Dostmohamed", "Sergio Guadarrama", "Aaron Phillips", "Thanumalayan Sankaranarayana Pillai", "Gal Yona", "Amin Ghafouri", "Preethi Lahoti", "Benjamin Lee", "Dhruv Madeka", "Eren Sezener", "Simon Tokumine", "Adrian Collister", "Nicola De Cao", "Richard Shin", "Uday Kalra", "Parker Beak", "Emily Nottage", "Ryo Nakashima", "Ivan Jurin", "Vikash Sehwag", "Meenu Gaba", "Junhao Zeng", "Kevin R. McKee", "Fernando Pereira", "Tamar Yakar", "Amayika Panda", "Arka Dhar", "Peilin Zhong", "Daniel Sohn", "Mark Brand", "Lars Lowe Sjoesund", "Viral Carpenter", "Sharon Lin", "Shantanu Thakoor", "Marcus Wainwright", "Ashwin Chaugule", "Pranesh Srinivasan", "Muye Zhu", "Bernett Orlando", "Jack Weber", "Ayzaan Wahid", "Gilles Baechler", "Apurv Suman", "Jovana Mitrovi\u0107", "Gabe Taubman", "Honglin Yu", "Helen King", "Josh Dillon", "Cathy Yip", "Dhriti Varma", "Tomas Izo", "Levent Bolelli", "Borja De Balle Pigem", "Julia Di Trapani", "Fotis Iliopoulos", "Adam Paszke", "Nishant Ranka", "Joe Zou", "Francesco Pongetti", "Jed McGiffin", "Alex Siegman", "Rich Galt", "Ross Hemsley", "Goran \u017du\u017ei\u0107", "Victor Carbune", "Tao Li", "Myle Ott", "F\u00e9lix de Chaumont Quitry", "David Vilar Torres", "Yuri Chervonyi", "Tomy Tsai", "Prem Eruvbetine", "Samuel Yang", "Matthew Denton", "Jake Walker", "Slavica Anda\u010di\u0107", "Idan Heimlich Shtacher", "Vittal Premachandran", "Harshal Tushar Lehri", "Cip Baetu", "Damion Yates", "Lampros Lamprou", "Mariko Iinuma", "Ioana Mihailescu", "Ben Albrecht", "Shachi Dave", "Susie Sargsyan", "Bryan Perozzi", "Lucas Manning", "Chiyuan Zhang", "Denis Vnukov", "Igor Mordatch", "Raia Hadsell Wolfgang Macherey", "Ryan Kappedal", "Jim Stephan", "Aditya Tripathi", "Klaus Macherey", "Jun Qian", "Abhishek Bhowmick", "Shekoofeh Azizi", "R\u00e9mi Leblond", "Shiva Mohan Reddy Garlapati", "Timothy Knight", "Matthew Wiethoff", "Wei-Chih Hung", "Anelia Angelova", "Georgios Evangelopoulos", "Pawel Janus", "Dimitris Paparas", "Matthew Rahtz", "Ken Caluwaerts", "Vivek Sampathkumar", "Daniel Jarrett", "Shadi Noghabi", "Antoine Miech", "Chak Yeung", "Geoff Clark", "Henry Prior", "Fei Zheng", "Jean Pouget-Abadie", "Indro Bhattacharya", "Kalpesh Krishna", "Will Bishop", "Zhe Yuan", "Yunxiao Deng", "Ashutosh Sathe", "Kacper Krasowiak", "Ciprian Chelba", "Cho-Jui Hsieh", "Kiran Vodrahalli", "Buhuang Liu", "Thomas K\u00f6ppe", "Amr Khalifa", "Lubo Litchev", "Pichi Charoenpanit", "Reed Roberts", "Sachin Yadav", "Yasumasa Onoe", "Desi Ivanov", "Megha Mohabey", "Vighnesh Birodkar", "Nemanja Raki\u0107evi\u0107", "Pierre Sermanet", "Vaibhav Mehta", "Krishan Subudhi", "Travis Choma", "Will Ng", "Luheng He", "Kathie Wang", "Tasos Kementsietsidis", "Shane Gu", "Mansi Gupta", "Andrew Nystrom", "Mehran Kazemi", "Timothy Chung", "Nacho Cano", "Nikhil Dhawan", "Yufei Wang", "Jiawei Xia", "Trevor Yacovone", "Eric Jia", "Mingqing Chen", "Simeon Ivanov", "Ashrith Sheshan", "Sid Dalmia", "Pawe\u0142 Stradomski", "Pengcheng Yin", "Salem Haykal", "Congchao Wang", "Dennis Duan", "Neslihan Bulut", "Greg Kochanski", "Liam MacDermed", "Namrata Godbole", "Shitao Weng", "Jingjing Chen", "Rachana Fellinger", "Ramin Mehran", "Daniel Suo", "Hisham Husain", "Tong He", "Kaushal Patel", "Joshua Howland", "Randall Parker", "Kelvin Nguyen", "Sharath Maddineni", "Chris Rawles", "Mina Khan", "Shlomi Cohen-Ganor", "Amol Mandhane", "Xinyi Wu", "Chenkai Kuang", "Iulia Com\u015fa", "Ramya Ganeshan", "Hanie Sedghi", "Adam Bloniarz", "Nuo Wang Pierse", "Anton Briukhov", "Petr Mitrichev", "Anita Gergely", "Serena Zhan", "Allan Zhou", "Nikita Saxena", "Eva Lu", "Josef Dean", "Ashish Gupta", "Nicolas Perez-Nieves", "Renjie Wu", "Cory McLean", "Wei Liang", "Disha Jindal", "Anton Tsitsulin", "Wenhao Yu", "Kaiz Alarakyia", "Tom Schaul", "Piyush Patil", "Peter Sung", "Elijah Peake", "Hongkun Yu", "Feryal Behbahani", "JD Co-Reyes", "Alan Ansell", "Sean Sun", "Clara Barbu", "Jonathan Lee", "Seb Noury", "James Allingham", "Bilal Piot", "Mohit Sharma", "Christopher Yew", "Ivan Korotkov", "Bibo Xu", "Demetra Brady", "Goran Petrovic", "Shibl Mourad", "Claire Cui", "Aditya Gupta", "Parker Schuh", "Saarthak Khanna", "Anna Goldie", "Abhinav Arora", "Vadim Zubov", "Amy Stuart", "Mark Epstein", "Yun Zhu", "Jianqiao Liu", "Yury Stuken", "Ziyue Wang", "Karolis Misiunas", "Dee Guo", "Ashleah Gill", "Ale Hartman", "Zaid Nabulsi", "Aurko Roy", "Aleksandra Faust", "Jason Riesa", "Ben Withbroe", "Mengchao Wang", "Marco Tagliasacchi", "Andreea Marzoca", "James Noraky", "Serge Toropov", "Malika Mehrotra", "Bahram Raad", "Sanja Deur", "Steve Xu", "Marianne Monteiro", "Zhongru Wu", "Yi Luan", "Sam Ritter", "Nick Li", "H\u00e5vard Garnes", "Yanzhang He", "Martin Zlocha", "Jifan Zhu", "Matteo Hessel", "Will Wu", "Spandana Raj Babbula", "Chizu Kawamoto", "Yuanzhen Li", "Mehadi Hassen", "Yan Wang", "Brian Wieder", "James Freedman", "Yin Zhang", "Xinyi Bai", "Tianli Yu", "David Reitter", "XiangHai Sheng", "Mateo Wirth", "Aditya Kini", "Dima Damen", "Mingcen Gao", "Rachel Hornung", "Michael Voznesensky", "Brian Roark", "Adhi Kuncoro", "Yuxiang Zhou", "Rushin Shah", "Anthony Brohan", "Kuangyuan Chen", "James Wendt", "David Rim", "Paul Kishan Rubenstein", "Jonathan Halcrow", "Michelle Liu", "Ty Geri", "Yunhsuan Sung", "Jane Shapiro", "Shaan Bijwadia", "Chris Duvarney", "Christina Sorokin", "Paul Natsev", "Reeve Ingle", "Pramod Gupta", "Young Maeng", "Ndaba Ndebele", "Kexin Zhu", "Valentin Anklin", "Katherine Lee", "Yuan Liu", "Yaroslav Akulov", "Shaleen Gupta", "Guolong Su", "Flavien Prost", "Tianlin Liu", "Vitaly Kovalev", "Pol Moreno", "Martin Scholz", "Sam Redmond", "Zongwei Zhou", "Alex Castro-Ros", "Andr\u00e9 Susano Pinto", "Dia Kharrat", "Michal Yarom", "Rachel Saputro", "Jannis Bulian", "Ben Caine", "Ji Liu", "Abbas Abdolmaleki", "Shariq Iqbal", "Tautvydas Misiunas", "Mikhail Sirotenko", "Shefali Garg", "Guy Bensky", "Huan Gui", "Xuezhi Wang", "Raphael Koster", "Mike Bernico", "Da Huang", "Romal Thoppilan", "Trevor Cohn", "Ben Golan", "Wenlei Zhou", "Andrew Rosenberg", "Markus Freitag", "Tynan Gangwani", "Vincent Tsang", "Anand Shukla", "Xiaoqi Ren", "Minh Giang", "Chi Zou", "Andre Elisseeff", "Charline Le Lan", "Dheeru Dua", "Shuba Lall", "Pranav Shyam", "Frankie Garcia", "Sarah Nguyen", "Michael Guzman", "AJ Maschinot", "Marcello Maggioni", "Ming-Wei Chang", "Karol Gregor", "Lotte Weerts", "Kumaran Venkatesan", "Bogdan Damoc", "Leon Liu", "Jan Wassenberg", "Lewis Ho", "Becca Roelofs", "Majid Hadian", "Fran\u00e7ois-Xavier Aubet", "Yu Liang", "Sami Lachgar", "Danny Karmon", "Yong Cheng", "Amelio V\u00e1zquez-Reina", "Angie Chen", "Zhuyun Dai", "Andy Brock", "Shubham Agrawal", "Chenxi Pang", "Peter Garst", "Mariella Sanchez-Vargas", "Ivor Rendulic", "Aditya Ayyar", "Andrija Ra\u017enatovi\u0107", "Olivia Ma", "Roopali Vij", "Neha Sharma", "Ashwin Balakrishna", "Bingyuan Liu", "Ian Mackinnon", "Sorin Baltateanu", "Petra Poklukar", "Gabriel Ibagon", "Colin Ji", "Hongyang Jiao", "Isaac Noble", "Wojciech Stokowiec", "Zhihao Li", "Jeff Dean", "David Lindner", "Mark Omernick", "Kristen Chiafullo", "Mason Dimarco", "Vitor Rodrigues", "Vittorio Selo", "Garrett Honke", "Xintian", "Wu", "Wei He", "Adam Hillier", "Anhad Mohananey", "Vihari Piratla", "Chang Ye", "Chase Malik", "Sebastian Riedel", "Samuel Albanie", "Zi Yang", "Kenny Vassigh", "Maria Bauza", "Sheng Li", "Yiqing Tao", "Nevan Wichers", "Andrii Maksai", "Abe Ittycheriah", "Ross Mcilroy", "Bryan Seybold", "Noah Goodman", "Romina Datta", "Steven M. Hernandez", "Tian Shi", "Yony Kochinski", "Anna Bulanova", "Ken Franko", "Mikita Sazanovich", "Nicholas FitzGerald", "Praneeth Kacham", "Shubha Srinivas Raghvendra", "Vincent Hellendoorn", "Alexander Grushetsky", "Julian Salazar", "Angeliki Lazaridou", "Jason Chang", "Jan-Thorsten Peter", "Sushant Kafle", "Yann Dauphin", "Abhishek Rao", "Filippo Graziano", "Izhak Shafran", "Yuguo Liao", "Tianli Ding", "Geng Yan", "Grace Chu", "Zhao Fu", "Vincent Roulet", "Gabriel Rasskin", "Duncan Williams", "Shahar Drath", "Alex Mossin", "Raphael Hoffmann", "Jordi Orbay", "Francesco Bertolini", "Hila Sheftel", "Justin Chiu", "Siyang Xue", "Yuheng Kuang", "Ferjad Naeem", "Swaroop Nath", "Nana Nti", "Phil Culliton", "Kashyap Krishnakumar", "Michael Isard", "Pei Sun", "Ayan Chakrabarti", "Nathan Clement", "Regev Cohen", "Arissa Wongpanich", "GS Oh", "Ashwin Murthy", "Hao Zheng", "Jessica Hamrick", "Oskar Bunyan", "Suhas Ganesh", "Nitish Gupta", "Roy Frostig", "John Wieting", "Yury Malkov", "Pierre Marcenac", "Zhixin", "Lai", "Xiaodan Tang", "Mohammad Saleh", "Fedir Zubach", "Chinmay Kulkarni", "Huanjie Zhou", "Vicky Zayats", "Nan Ding", "Anshuman Tripathi", "Arijit Pramanik", "Patrik Zochbauer", "Harish Ganapathy", "Vedant Misra", "Zach Behrman", "Hugo Vallet", "Mingyang Zhang", "Mukund Sridhar", "Ye Jin", "Mohammad Babaeizadeh", "Siim P\u00f5der", "Megha Goel", "Divya Jain", "Tajwar Nasir", "Shubham Mittal", "Tim Dozat", "Diego Ardila", "Aliaksei Severyn", "Fabio Pardo", "Sammy Jerome", "Siyang Qin", "Louis Rouillard", "Amir Yazdanbakhsh", "Zizhao Zhang", "Shivani Agrawal", "Kaushik Shivakumar", "Caden Lu", "Praveen Kallakuri", "Rachita Chhaparia", "Kanishka Rao", "Charles Kwong", "Asya Fadeeva", "Shitij Nigam", "Yan Virin", "Yuan Zhang", "Balaji Venkatraman", "Beliz Gunel", "Marc Wilson", "Huiyu Wang", "Abhinav Gupta", "Xiaowei Xu", "Adrien Ali Ta\u00efga", "Kareem Mohamed", "Doug Fritz", "Daniel Rodriguez", "Zoubin Ghahramani", "Harry Askham", "Lior Belenki", "James Zhao", "Rahul Gupta", "Krzysztof Jastrz\u0119bski", "Takahiro Kosakai", "Kaan Katircioglu", "Jon Schneider", "Rina Panigrahy", "Konstantinos Bousmalis", "Peter Grabowski", "Prajit Ramachandran", "Chaitra Hegde", "Mihaela Rosca", "Angelo Scorza Scarpati", "Kyriakos Axiotis", "Ying Xu", "Zach Gleicher", "Assaf Hurwitz Michaely", "Mandar Sharma", "Sanil Jain", "Christoph Hirnschall", "Tal Marian", "Xuhui Jia", "Kevin Mather", "Kilol Gupta", "Linhai Qiu", "Nigamaa Nayakanti", "Lucian Ionita", "Steven Zheng", "Lucia Loher", "Kurt Shuster", "Igor Petrovski", "Roshan Sharma", "Rahma Chaabouni", "Angel Yeh", "James An", "Arushi Gupta", "Steven Schwarcz", "Seher Ellis", "Sam Conway-Rahman", "Javier Snaider", "Alex Zhai", "James Atwood", "Daniel Golovin", "Liqian Peng", "Te I", "Vivian Xia", "Salvatore Scellato", "Mahan Malihi", "Arthur Bra\u017einskas", "Vlad-Doru Ion", "Younghoon Jun", "James Swirhun", "Soroosh Mariooryad", "Jiao Sun", "Steve Chien", "Rey Coaguila", "Ariel Brand", "Yi Gao", "Tom Kwiatkowski", "Roee Aharoni", "Cheng-Chun Lee", "Mislav \u017dani\u0107", "Yichi Zhang", "Dan Ethier", "Vitaly Nikolaev", "Pranav Nair", "Yoav Ben Shalom", "Hen Fitoussi", "Jai Gupta", "Hongbin Liu", "Dee Cattle", "Tolga Bolukbasi", "Ben Murdoch", "Fantine Huot", "Yin Li", "Chris Hahn"], "title": "Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities", "comment": "72 pages, 17 figures", "summary": "In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and\nGemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite\nmodels. Gemini 2.5 Pro is our most capable model yet, achieving SoTA\nperformance on frontier coding and reasoning benchmarks. In addition to its\nincredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that\nexcels at multimodal understanding and it is now able to process up to 3 hours\nof video content. Its unique combination of long context, multimodal and\nreasoning capabilities can be combined to unlock new agentic workflows. Gemini\n2.5 Flash provides excellent reasoning abilities at a fraction of the compute\nand latency requirements and Gemini 2.0 Flash and Flash-Lite provide high\nperformance at low latency and cost. Taken together, the Gemini 2.X model\ngeneration spans the full Pareto frontier of model capability vs cost, allowing\nusers to explore the boundaries of what is possible with complex agentic\nproblem solving.", "AI": {"tldr": "The Gemini 2.X model family, including 2.5 Pro and Flash, achieves state-of-the-art performance in coding and reasoning, with varying trade-offs between capability and cost, suitable for agentic problem solving.", "motivation": "achieving SoTA performance on frontier coding and reasoning benchmarks and unlock new agentic workflows", "method": "introducing the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models", "result": "Gemini 2.5 Pro is our most capable model yet, Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost", "conclusion": "The Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving."}}
{"id": "2507.06234", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06234", "abs": "https://arxiv.org/abs/2507.06234", "authors": ["Jiangzhong Cao", "Zekai Zeng", "Xu Zhang", "Huan Zhang", "Chunling Fan", "Gangyi Jiang", "Weisi Lin"], "title": "Unveiling the Underwater World: CLIP Perception Model-Guided Underwater Image Enhancement", "comment": "10 pages, 7 figures;Accepted to PR 2025;The source code is available\n  at https://github.com/Ave001025/UIE_CLIP", "summary": "High-quality underwater images are essential for both machine vision tasks\nand viewers with their aesthetic appeal.However, the quality of underwater\nimages is severely affected by light absorption and scattering. Deep\nlearning-based methods for Underwater Image Enhancement (UIE) have achieved\ngood performance. However, these methods often overlook considering human\nperception and lack sufficient constraints within the solution space.\nConsequently, the enhanced images often suffer from diminished perceptual\nquality or poor content restoration.To address these issues, we propose a UIE\nmethod with a Contrastive Language-Image Pre-Training (CLIP) perception loss\nmodule and curriculum contrastive regularization. Above all, to develop a\nperception model for underwater images that more aligns with human visual\nperception, the visual semantic feature extraction capability of the CLIP model\nis leveraged to learn an appropriate prompt pair to map and evaluate the\nquality of underwater images. This CLIP perception model is then incorporated\nas a perception loss module into the enhancement network to improve the\nperceptual quality of enhanced images. Furthermore, the CLIP perception model\nis integrated with the curriculum contrastive regularization to enhance the\nconstraints imposed on the enhanced images within the CLIP perceptual space,\nmitigating the risk of both under-enhancement and over-enhancement.\nSpecifically, the CLIP perception model is employed to assess and categorize\nthe learning difficulty level of negatives in the regularization process,\nensuring comprehensive and nuanced utilization of distorted images and\nnegatives with varied quality levels. Extensive experiments demonstrate that\nour method outperforms state-of-the-art methods in terms of visual quality and\ngeneralization ability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528 CLIP \u6a21\u578b\u6765\u63d0\u9ad8\u611f\u77e5\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6c34\u4e0b\u56fe\u50cf\u7684\u8d28\u91cf\u53d7\u5230\u5149\u5438\u6536\u548c\u6563\u5c04\u7684\u4e25\u91cd\u5f71\u54cd\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a (UIE) \u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u8003\u8651\u4eba\u7c7b\u611f\u77e5\uff0c\u5e76\u4e14\u5728\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u5185\u7f3a\u4e4f\u8db3\u591f\u7684\u7ea6\u675f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u5bf9\u6bd4\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3 (CLIP) \u611f\u77e5\u635f\u5931\u6a21\u5757\u548c\u8bfe\u7a0b\u5bf9\u6bd4\u6b63\u5219\u5316\u7684 UIE \u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5229\u7528 CLIP \u6a21\u578b\u7684\u89c6\u89c9\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u5b66\u4e60\u9002\u5f53\u7684\u63d0\u793a\u5bf9\u6765\u6620\u5c04\u548c\u8bc4\u4f30\u6c34\u4e0b\u56fe\u50cf\u7684\u8d28\u91cf\uff0c\u4ece\u800c\u5f00\u53d1\u51fa\u66f4\u7b26\u5408\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u7684\u6c34\u4e0b\u56fe\u50cf\u611f\u77e5\u6a21\u578b\u3002\u8be5\u6a21\u578b\u88ab\u6574\u5408\u5230\u589e\u5f3a\u7f51\u7edc\u4e2d\u4f5c\u4e3a\u611f\u77e5\u635f\u5931\u6a21\u5757\uff0c\u4ee5\u63d0\u9ad8\u589e\u5f3a\u56fe\u50cf\u7684\u611f\u77e5\u8d28\u91cf\u3002\u6b64\u5916\uff0cCLIP \u611f\u77e5\u6a21\u578b\u4e0e\u8bfe\u7a0b\u5bf9\u6bd4\u6b63\u5219\u5316\u76f8\u7ed3\u5408\uff0c\u4ee5\u589e\u5f3a\u5bf9 CLIP \u611f\u77e5\u7a7a\u95f4\u5185\u589e\u5f3a\u56fe\u50cf\u7684\u7ea6\u675f\uff0c\u4ece\u800c\u51cf\u8f7b\u6b20\u589e\u5f3a\u548c\u8fc7\u589e\u5f3a\u7684\u98ce\u9669\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u3002"}}
{"id": "2507.06267", "categories": ["cs.LG", "34C60, 92B05, 68T07, 93C15, 65K10"], "pdf": "https://arxiv.org/pdf/2507.06267", "abs": "https://arxiv.org/abs/2507.06267", "authors": ["Hyeontae Jo", "Kre\u0161imir Josi\u0107", "Jae Kyoung Kim"], "title": "Neural Network-Based Parameter Estimation for Non-Autonomous Differential Equations with Discontinuous Signals", "comment": null, "summary": "Non-autonomous differential equations are crucial for modeling systems\ninfluenced by external signals, yet fitting these models to data becomes\nparticularly challenging when the signals change abruptly. To address this\nproblem, we propose a novel parameter estimation method utilizing functional\napproximations with artificial neural networks. Our approach, termed Harmonic\nApproximation of Discontinuous External Signals using Neural Networks\n(HADES-NN), operates in two iterated stages. In the first stage, the algorithm\nemploys a neural network to approximate the discontinuous signal with a smooth\nfunction. In the second stage, it uses this smooth approximate signal to\nestimate model parameters. HADES-NN gives highly accurate and precise parameter\nestimates across various applications, including circadian clock systems\nregulated by external light inputs measured via wearable devices and the mating\nresponse of yeast to external pheromone signals. HADES-NN greatly extends the\nrange of model systems that can be fit to real-world measurements.", "AI": {"tldr": "This paper introduces HADES-NN, a novel method using neural networks to estimate parameters for non-autonomous differential equations with discontinuous external signals. It demonstrates high accuracy across various applications.", "motivation": "fitting non-autonomous differential equations to data becomes particularly challenging when the signals change abruptly.", "method": "a novel parameter estimation method utilizing functional approximations with artificial neural networks. It operates in two iterated stages: employing a neural network to approximate the discontinuous signal with a smooth function and using this smooth approximate signal to estimate model parameters.", "result": "HADES-NN gives highly accurate and precise parameter estimates across various applications, including circadian clock systems regulated by external light inputs measured via wearable devices and the mating response of yeast to external pheromone signals.", "conclusion": "HADES-NN greatly extends the range of model systems that can be fit to real-world measurements."}}
{"id": "2507.06852", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06852", "abs": "https://arxiv.org/abs/2507.06852", "authors": ["Uri Andrews", "Luca San Mauro"], "title": "SCC-recursiveness in infinite argumentation (extended version)", "comment": "26 pages, accepted at JELIA 2025", "summary": "Argumentation frameworks (AFs) are a foundational tool in artificial\nintelligence for modeling structured reasoning and conflict. SCC-recursiveness\nis a well-known design principle in which the evaluation of arguments is\ndecomposed according to the strongly connected components (SCCs) of the attack\ngraph, proceeding recursively from \"higher\" to \"lower\" components. While\nSCC-recursive semantics such as \\cft and \\stgt have proven effective for finite\nAFs, Baumann and Spanring showed the failure of SCC-recursive semantics to\ngeneralize reliably to infinite AFs due to issues with well-foundedness.\n  We propose two approaches to extending SCC-recursiveness to the infinite\nsetting. We systematically evaluate these semantics using Baroni and Giacomin's\nestablished criteria, showing in particular that directionality fails in\ngeneral. We then examine these semantics' behavior in finitary frameworks,\nwhere we find some of our semantics satisfy directionality. These results\nadvance the theory of infinite argumentation and lay the groundwork for\nreasoning systems capable of handling unbounded or evolving domains.", "AI": {"tldr": "This paper extends SCC-recursiveness to infinite argumentation frameworks.", "motivation": "SCC-recursive semantics such as \\", "method": "The authors propose two approaches to extending SCC-recursiveness to the infinite setting and systematically evaluate these semantics using Baroni and Giacomin's established criteria.", "result": "The authors show that directionality fails in general, but some of their semantics satisfy directionality in finitary frameworks.", "conclusion": "This paper advances the theory of infinite argumentation and lays the groundwork for reasoning systems capable of handling unbounded or evolving domains."}}
{"id": "2507.06554", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.06554", "abs": "https://arxiv.org/abs/2507.06554", "authors": ["Zou Yuheng", "Wang Yiran", "Tian Yuzhu", "Zhu Min", "Huang Yanhua"], "title": "SPEAR: Subset-sampled Performance Evaluation via Automated Ground Truth Generation for RAG", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is a core approach for enhancing Large\nLanguage Models (LLMs), where the effectiveness of the retriever largely\ndetermines the overall response quality of RAG systems. Retrievers encompass a\nmultitude of hyperparameters that significantly impact performance outcomes and\ndemonstrate sensitivity to specific applications. Nevertheless, hyperparameter\noptimization entails prohibitively high computational expenses. Existing\nevaluation methods suffer from either prohibitive costs or disconnection from\ndomain-specific scenarios. This paper proposes SEARA (Subset sampling\nEvaluation for Automatic Retriever Assessment), which addresses evaluation data\nchallenges through subset sampling techniques and achieves robust automated\nretriever evaluation by minimal retrieval facts extraction and comprehensive\nretrieval metrics. Based on real user queries, this method enables fully\nautomated retriever evaluation at low cost, thereby obtaining optimal retriever\nfor specific business scenarios. We validate our method across classic RAG\napplications in rednote, including knowledge-based Q&A system and\nretrieval-based travel assistant, successfully obtaining scenario-specific\noptimal retrievers.", "AI": {"tldr": "This paper introduces SEARA, a cost-effective subset sampling method for automated retriever evaluation in RAG systems, enabling optimization for specific applications.", "motivation": "The effectiveness of the retriever largely determines the overall response quality of RAG systems. Nevertheless, hyperparameter optimization entails prohibitively high computational expenses. Existing evaluation methods suffer from either prohibitive costs or disconnection from domain-specific scenarios.", "method": "The paper proposes SEARA (Subset sampling Evaluation for Automatic Retriever Assessment), which addresses evaluation data challenges through subset sampling techniques and achieves robust automated retriever evaluation by minimal retrieval facts extraction and comprehensive retrieval metrics.", "result": "Based on real user queries, this method enables fully automated retriever evaluation at low cost, thereby obtaining optimal retriever for specific business scenarios.", "conclusion": "The paper validates SEARA across classic RAG applications, successfully obtaining scenario-specific optimal retrievers."}}
{"id": "2507.06515", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.06515", "abs": "https://arxiv.org/abs/2507.06515", "authors": ["Zhaoze Sun", "Qiyan Deng", "Chengliang Chai", "Kaisen Jin", "Xinyu Guo", "Han Han", "Ye Yuan", "Guoren Wang", "Lei Cao"], "title": "QUEST: Query Optimization in Unstructured Document Analysis", "comment": null, "summary": "Most recently, researchers have started building large language models (LLMs)\npowered data systems that allow users to analyze unstructured text documents\nlike working with a database because LLMs are very effective in extracting\nattributes from documents. In such systems, LLM-based extraction operations\nconstitute the performance bottleneck of query execution due to the high\nmonetary cost and slow LLM inference. Existing systems typically borrow the\nquery optimization principles popular in relational databases to produce query\nexecution plans, which unfortunately are ineffective in minimizing LLM cost. To\nfill this gap, we propose QUEST, which features a bunch of novel optimization\nstrategies for unstructured document analysis. First, we introduce an\nindex-based strategy to minimize the cost of each extraction operation. With\nthis index, QUEST quickly retrieves the text segments relevant to the target\nattributes and only feeds them to LLMs. Furthermore, we design an\nevidence-augmented retrieval strategy to reduce the possibility of missing\nrelevant segments. Moreover, we develop an instance-optimized query execution\nstrategy: because the attribute extraction cost could vary significantly\ndocument by document, QUEST produces different plans for different documents.\nFor each document, QUEST produces a plan to minimize the frequency of attribute\nextraction. The innovations include LLM cost-aware operator ordering strategies\nand an optimized join execution approach that transforms joins into filters.\nExtensive experiments on 3 real-world datasets demonstrate the superiority of\nQUEST, achieving 30%-6x cost savings while improving the F1 score by 10% -27%\ncompared with state-of-the-art baselines.", "AI": {"tldr": "QUEST\u901a\u8fc7\u65b0\u9896\u7684\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u975e\u7ed3\u6784\u5316\u6587\u6863\u5206\u6790\u4e2dLLM\u7684\u4f7f\u7528\u6210\u672c\uff0c\u5e76\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u901a\u5e38\u501f\u7528\u5173\u7cfb\u6570\u636e\u5e93\u4e2d\u6d41\u884c\u7684\u67e5\u8be2\u4f18\u5316\u539f\u5219\u6765\u751f\u6210\u67e5\u8be2\u6267\u884c\u8ba1\u5212\uff0c\u4f46\u4e0d\u5e78\u7684\u662f\uff0c\u8fd9\u4e9b\u8ba1\u5212\u5728\u6700\u5c0f\u5316LLM\u6210\u672c\u65b9\u9762\u65e0\u6548\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86QUEST\u3002", "method": "QUEST\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7d22\u5f15\u7684\u7b56\u7565\u6765\u6700\u5c0f\u5316\u6bcf\u4e2a\u63d0\u53d6\u64cd\u4f5c\u7684\u6210\u672c\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8bc1\u636e\u589e\u5f3a\u7684\u68c0\u7d22\u7b56\u7565\uff0c\u4ee5\u51cf\u5c11\u9057\u6f0f\u76f8\u5173\u7247\u6bb5\u7684\u53ef\u80fd\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u5b9e\u4f8b\u4f18\u5316\u7684\u67e5\u8be2\u6267\u884c\u7b56\u7565\u3002", "result": "QUEST\u901a\u8fc7\u91c7\u7528\u7d22\u5f15\u3001\u8bc1\u636e\u589e\u5f3a\u68c0\u7d22\u548c\u5b9e\u4f8b\u4f18\u5316\u67e5\u8be2\u6267\u884c\u7b49\u521b\u65b0\u7b56\u7565\uff0c\u5728\u975e\u7ed3\u6784\u5316\u6587\u6863\u5206\u6790\u65b9\u9762\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6210\u672c\u8282\u7ea6\u548c\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "QUEST\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eQUEST\u5177\u6709\u4f18\u8d8a\u6027\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6210\u672c\u8282\u7701\u4e8630%-6\u500d\uff0c\u540c\u65f6F1\u8bc4\u5206\u63d0\u9ad8\u4e8610%-27%\u3002"}}
{"id": "2507.06306", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.06306", "abs": "https://arxiv.org/abs/2507.06306", "authors": ["Neil Rathi", "Dan Jurafsky", "Kaitlyn Zhou"], "title": "Humans overrely on overconfident language models, across languages", "comment": "10 pages main text, to appear at COLM 2025", "summary": "As large language models (LLMs) are deployed globally, it is crucial that\ntheir responses are calibrated across languages to accurately convey\nuncertainty and limitations. Previous work has shown that LLMs are\nlinguistically overconfident in English, leading users to overrely on confident\ngenerations. However, the usage and interpretation of epistemic markers (e.g.,\n'It's definitely,' 'I think') can differ sharply across languages. Here, we\nstudy the risks of multilingual linguistic (mis)calibration, overconfidence,\nand overreliance across five languages to evaluate the safety of LLMs in a\nglobal context.\n  We find that overreliance risks are high across all languages. We first\nanalyze the distribution of LLM-generated epistemic markers, and observe that\nwhile LLMs are cross-linguistically overconfident, they are also sensitive to\ndocumented linguistic variation. For example, models generate the most markers\nof uncertainty in Japanese and the most markers of certainty in German and\nMandarin. We then measure human reliance rates across languages, finding that\nwhile users strongly rely on confident LLM generations in all languages,\nreliance behaviors differ cross-linguistically: for example, users rely\nsignificantly more on expressions of uncertainty in Japanese than in English.\nTaken together, these results indicate high risk of reliance on overconfident\nmodel generations across languages. Our findings highlight the challenges of\nmultilingual linguistic calibration and stress the importance of culturally and\nlinguistically contextualized model safety evaluations.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5168\u7403\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u53d1\u73b0\u7528\u6237\u8fc7\u5ea6\u4f9d\u8d56\u4e8e\u81ea\u4fe1\u7684\u6a21\u578b\u751f\u6210\u5185\u5bb9\uff0c\u5e76\u4e14\u8fd9\u79cd\u4f9d\u8d56\u884c\u4e3a\u5728\u4e0d\u540c\u8bed\u8a00\u4e4b\u95f4\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5168\u7403\u90e8\u7f72\uff0c\u56e0\u6b64\u6821\u51c6\u5b83\u4eec\u5728\u5404\u79cd\u8bed\u8a00\u4e2d\u7684\u54cd\u5e94\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u51c6\u786e\u4f20\u8fbe\u4e0d\u786e\u5b9a\u6027\u548c\u5c40\u9650\u6027\u3002\u5148\u524d\u7684\u7814\u7a76\u8868\u660e\uff0cLLM\u5728\u82f1\u8bed\u4e2d\u5177\u6709\u8bed\u8a00\u4e0a\u7684\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u5bfc\u81f4\u7528\u6237\u8fc7\u5ea6\u4f9d\u8d56\u81ea\u4fe1\u7684\u751f\u6210\u5185\u5bb9\u3002\u7136\u800c\uff0c\u8ba4\u77e5\u6807\u8bb0\uff08\u4f8b\u5982\uff0c\u201cIt's definitely\u201d\uff0c\u201cI think\u201d\uff09\u7684\u4f7f\u7528\u548c\u89e3\u91ca\u53ef\u80fd\u5728\u4e0d\u540c\u8bed\u8a00\u4e4b\u95f4\u5b58\u5728\u5f88\u5927\u5dee\u5f02\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LLM\u5728\u5168\u7403\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u3002", "method": "\u5206\u6790\u4e86LLM\u751f\u6210\u7684\u8ba4\u77e5\u6807\u8bb0\u7684\u5206\u5e03\uff0c\u5e76\u6d4b\u91cf\u4e86\u4e0d\u540c\u8bed\u8a00\u7684\u4eba\u7c7b\u4f9d\u8d56\u7387\u3002", "result": "LLM\u5728\u8de8\u8bed\u8a00\u4e0a\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u4f46\u5bf9\u5df2\u8bb0\u5f55\u7684\u8bed\u8a00\u53d8\u5f02\u4e5f\u5f88\u654f\u611f\u3002\u4f8b\u5982\uff0c\u6a21\u578b\u5728\u65e5\u8bed\u4e2d\u751f\u6210\u6700\u591a\u7684\u4e0d\u786e\u5b9a\u6027\u6807\u8bb0\uff0c\u5728\u5fb7\u8bed\u548c\u666e\u901a\u8bdd\u4e2d\u751f\u6210\u6700\u591a\u7684\u786e\u5b9a\u6027\u6807\u8bb0\u3002\u7528\u6237\u5728\u6240\u6709\u8bed\u8a00\u4e2d\u90fd\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u81ea\u4fe1\u7684LLM\u751f\u6210\u5185\u5bb9\uff0c\u4f46\u4f9d\u8d56\u884c\u4e3a\u5728\u8de8\u8bed\u8a00\u4e0a\u6709\u6240\u4e0d\u540c\uff1a\u4f8b\u5982\uff0c\u7528\u6237\u5bf9\u65e5\u8bed\u4e2d\u4e0d\u786e\u5b9a\u6027\u8868\u8fbe\u7684\u4f9d\u8d56\u7a0b\u5ea6\u660e\u663e\u9ad8\u4e8e\u82f1\u8bed\u3002", "conclusion": "\u8de8\u8bed\u8a00\u5730\uff0c\u7528\u6237\u8fc7\u5ea6\u4f9d\u8d56\u4e8e\u81ea\u4fe1\u7684\u6a21\u578b\u751f\u6210\u5185\u5bb9\uff0c\u8fd9\u5e26\u6765\u4e86\u9ad8\u98ce\u9669\u3002"}}
{"id": "2507.06265", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06265", "abs": "https://arxiv.org/abs/2507.06265", "authors": ["Ali Nasiri-Sarvi", "Hassan Rivaz", "Mahdi S. Hosseini"], "title": "SPARC: Concept-Aligned Sparse Autoencoders for Cross-Model and Cross-Modal Interpretability", "comment": null, "summary": "Understanding how different AI models encode the same high-level concepts,\nsuch as objects or attributes, remains challenging because each model typically\nproduces its own isolated representation. Existing interpretability methods\nlike Sparse Autoencoders (SAEs) produce latent concepts individually for each\nmodel, resulting in incompatible concept spaces and limiting cross-model\ninterpretability. To address this, we introduce SPARC (Sparse Autoencoders for\nAligned Representation of Concepts), a new framework that learns a single,\nunified latent space shared across diverse architectures and modalities (e.g.,\nvision models like DINO, and multimodal models like CLIP). SPARC's alignment is\nenforced through two key innovations: (1) a Global TopK sparsity mechanism,\nensuring all input streams activate identical latent dimensions for a given\nconcept; and (2) a Cross-Reconstruction Loss, which explicitly encourages\nsemantic consistency between models. On Open Images, SPARC dramatically\nimproves concept alignment, achieving a Jaccard similarity of 0.80, more than\ntripling the alignment compared to previous methods. SPARC creates a shared\nsparse latent space where individual dimensions often correspond to similar\nhigh-level concepts across models and modalities, enabling direct comparison of\nhow different architectures represent identical concepts without requiring\nmanual alignment or model-specific analysis. As a consequence of this aligned\nrepresentation, SPARC also enables practical applications such as text-guided\nspatial localization in vision-only models and cross-model/cross-modal\nretrieval. Code and models are available at\nhttps://github.com/AtlasAnalyticsLab/SPARC.", "AI": {"tldr": "SPARC: Sparse Autoencoders for Aligned Representation of Concepts, a new framework that learns a single, unified latent space shared across diverse architectures and modalities", "motivation": "Understanding how different AI models encode the same high-level concepts, such as objects or attributes, remains challenging because each model typically produces its own isolated representation. Existing interpretability methods like Sparse Autoencoders (SAEs) produce latent concepts individually for each model, resulting in incompatible concept spaces and limiting cross-model interpretability.", "method": "a new framework that learns a single, unified latent space shared across diverse architectures and modalities (e.g., vision models like DINO, and multimodal models like CLIP). SPARC's alignment is enforced through two key innovations: (1) a Global TopK sparsity mechanism, ensuring all input streams activate identical latent dimensions for a given concept; and (2) a Cross-Reconstruction Loss, which explicitly encourages semantic consistency between models.", "result": "On Open Images, SPARC dramatically improves concept alignment, achieving a Jaccard similarity of 0.80, more than tripling the alignment compared to previous methods.", "conclusion": "SPARC creates a shared sparse latent space where individual dimensions often correspond to similar high-level concepts across models and modalities, enabling direct comparison of how different architectures represent identical concepts without requiring manual alignment or model-specific analysis. As a consequence of this aligned representation, SPARC also enables practical applications such as text-guided spatial localization in vision-only models and cross-model/cross-modal retrieval."}}
{"id": "2507.06326", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.06326", "abs": "https://arxiv.org/abs/2507.06326", "authors": ["Harsh Ravivarapu", "Gaurav Bagwe", "Xiaoyong Yuan", "Chunxiu Yu", "Lan Zhang"], "title": "Sample-Efficient Reinforcement Learning Controller for Deep Brain Stimulation in Parkinson's Disease", "comment": "Accepted by IEEE IMC 2025", "summary": "Deep brain stimulation (DBS) is an established intervention for Parkinson's\ndisease (PD), but conventional open-loop systems lack adaptability, are\nenergy-inefficient due to continuous stimulation, and provide limited\npersonalization to individual neural dynamics. Adaptive DBS (aDBS) offers a\nclosed-loop alternative, using biomarkers such as beta-band oscillations to\ndynamically modulate stimulation. While reinforcement learning (RL) holds\npromise for personalized aDBS control, existing methods suffer from high sample\ncomplexity, unstable exploration in binary action spaces, and limited\ndeployability on resource-constrained hardware.\n  We propose SEA-DBS, a sample-efficient actor-critic framework that addresses\nthe core challenges of RL-based adaptive neurostimulation. SEA-DBS integrates a\npredictive reward model to reduce reliance on real-time feedback and employs\nGumbel Softmax-based exploration for stable, differentiable policy updates in\nbinary action spaces. Together, these components improve sample efficiency,\nexploration robustness, and compatibility with resource-constrained\nneuromodulatory hardware. We evaluate SEA-DBS on a biologically realistic\nsimulation of Parkinsonian basal ganglia activity, demonstrating faster\nconvergence, stronger suppression of pathological beta-band power, and\nresilience to post-training FP16 quantization. Our results show that SEA-DBS\noffers a practical and effective RL-based aDBS framework for real-time,\nresource-constrained neuromodulation.", "AI": {"tldr": "SEA-DBS\u662f\u4e00\u79cd\u7528\u4e8e\u5e15\u91d1\u68ee\u75c5\u7684\u65b0\u578b\u81ea\u9002\u5e94\u8111\u6df1\u90e8\u523a\u6fc0\uff08aDBS\uff09\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6765\u4f18\u5316\u523a\u6fc0\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u6837\u672c\u6548\u7387\u548c\u786c\u4ef6\u9650\u5236\u7b49\u5173\u952e\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u7684\u5f00\u73af\u7cfb\u7edf\u7f3a\u4e4f\u9002\u5e94\u6027\uff0c\u7531\u4e8e\u8fde\u7eed\u523a\u6fc0\u800c\u5bfc\u81f4\u80fd\u6e90\u6548\u7387\u4f4e\u4e0b\uff0c\u5e76\u4e14\u5bf9\u4e2a\u4f53\u795e\u7ecf\u52a8\u529b\u5b66\u7684\u4e2a\u6027\u5316\u6709\u9650\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60 (RL) \u7684\u4e2a\u6027\u5316 aDBS \u63a7\u5236\u65b9\u6cd5\u5b58\u5728\u6837\u672c\u590d\u6742\u5ea6\u9ad8\u3001\u4e8c\u5143\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u63a2\u7d22\u4e0d\u7a33\u5b9a\u4ee5\u53ca\u5728\u8d44\u6e90\u53d7\u9650\u7684\u786c\u4ef6\u4e0a\u90e8\u7f72\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "SEA-DBS\uff0c\u4e00\u4e2asample-efficient actor-critic\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u9884\u6d4b\u5956\u52b1\u6a21\u578b\u4ee5\u51cf\u5c11\u5bf9\u5b9e\u65f6\u53cd\u9988\u7684\u4f9d\u8d56\uff0c\u5e76\u91c7\u7528\u57fa\u4e8eGumbel Softmax\u7684\u63a2\u7d22\uff0c\u4ee5\u5b9e\u73b0\u4e8c\u5143\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u7a33\u5b9a\u3001\u53ef\u5fae\u7684\u7b56\u7565\u66f4\u65b0\u3002", "result": "SEA-DBS\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u3001\u63a2\u7d22\u9c81\u68d2\u6027\u4ee5\u53ca\u4e0e\u8d44\u6e90\u53d7\u9650\u7684\u795e\u7ecf\u8c03\u8282\u786c\u4ef6\u7684\u517c\u5bb9\u6027\u3002", "conclusion": "SEA-DBS\u5728\u751f\u7269\u5b66\u4e0a\u771f\u5b9e\u7684\u5e15\u91d1\u68ee\u75c5\u57fa\u5e95\u795e\u7ecf\u8282\u6d3b\u52a8\u6a21\u62df\u4e2d\u8868\u73b0\u51fa\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3001\u5bf9\u75c5\u7406\u6027\u03b2\u6ce2\u6bb5\u529f\u7387\u7684\u66f4\u5f3a\u6291\u5236\u4ee5\u53ca\u5bf9\u8bad\u7ec3\u540eFP16\u91cf\u5316\u7684\u5f39\u6027\uff0c\u4e3a\u5b9e\u65f6\u3001\u8d44\u6e90\u53d7\u9650\u7684\u795e\u7ecf\u8c03\u8282\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u6709\u6548\u7684\u57fa\u4e8eRL\u7684aDBS\u6846\u67b6\u3002"}}
{"id": "2507.06968", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06968", "abs": "https://arxiv.org/abs/2507.06968", "authors": ["Li Du", "Hanyu Zhao", "Yiming Ju", "Tengfei Pan"], "title": "Scaling Towards the Information Boundary of Instruction Set: InfinityInstruct-Subject Technical Report", "comment": null, "summary": "Instruction tuning has become a foundation for unlocking the capabilities of\nlarge-scale pretrained models and improving their performance on complex tasks.\nThus, the construction of high-quality instruction datasets is crucial for\nenhancing model performance and generalizability. Although current instruction\ndatasets have reached tens of millions of samples, models finetuned on them may\nstill struggle with complex instruction following and tasks in rare domains.\nThis is primarily due to limited expansion in both ``coverage'' (coverage of\ntask types and knowledge areas) and ``depth'' (instruction complexity) of the\ninstruction set. To address this issue, we propose a systematic instruction\ndata construction framework, which integrates a hierarchical labeling system,\nan informative seed selection algorithm, an evolutionary data synthesis\nprocess, and a model deficiency diagnosis with targeted data generation. These\ncomponents form an iterative closed-loop to continuously enhance the coverage\nand depth of instruction data. Based on this framework, we construct\nInfinityInstruct-Subject, a high-quality dataset containing ~1.5 million\ninstructions. Experiments on multiple foundation models and benchmark tasks\ndemonstrate its effectiveness in improving instruction-following capabilities.\nFurther analyses suggest that InfinityInstruct-Subject shows enlarged coverage\nand depth compared to comparable synthesized instruction datasets. Our work\nlays a theoretical and practical foundation for the efficient, continuous\nevolution of instruction datasets, moving from data quantity expansion to\nqualitative improvement.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6307\u4ee4\u6570\u636e\u6784\u5efa\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u6307\u4ee4\u6570\u636e\u96c6InfinityInstruct-Subject\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u5728\u590d\u6742\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6307\u4ee4\u6570\u636e\u96c6\u5728\u4efb\u52a1\u7c7b\u578b\u3001\u77e5\u8bc6\u9886\u57df\u548c\u6307\u4ee4\u590d\u6742\u5ea6\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u590d\u6742\u6307\u4ee4\u8ddf\u968f\u548c\u7f55\u89c1\u9886\u57df\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u7684\u6307\u4ee4\u6570\u636e\u6784\u5efa\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u5206\u5c42\u6807\u7b7e\u7cfb\u7edf\u3001\u4fe1\u606f\u4e30\u5bcc\u7684\u79cd\u5b50\u9009\u62e9\u7b97\u6cd5\u3001\u8fdb\u5316\u6570\u636e\u5408\u6210\u8fc7\u7a0b\u4ee5\u53ca\u9488\u5bf9\u6027\u6570\u636e\u751f\u6210\u7684\u6a21\u578b\u7f3a\u9677\u8bca\u65ad\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6InfinityInstruct-Subject\uff0c\u5176\u4e2d\u5305\u542b\u7ea6150\u4e07\u6761\u6307\u4ee4\u3002\u5728\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u548c\u57fa\u51c6\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6570\u636e\u96c6\u5728\u63d0\u9ad8\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u65b9\u9762\u662f\u6709\u6548\u7684\u3002", "conclusion": "InfinityInstruct-Subject\u6570\u636e\u96c6\u63d0\u5347\u4e86\u6a21\u578b\u5728\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5c55\u793a\u51fa\u6bd4\u540c\u7c7b\u5408\u6210\u6570\u636e\u96c6\u66f4\u5927\u7684\u8986\u76d6\u8303\u56f4\u548c\u6df1\u5ea6\u3002"}}
{"id": "2507.06563", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06563", "abs": "https://arxiv.org/abs/2507.06563", "authors": ["Jeanette Schofield", "Shuyu Tian", "Hoang Thanh Thanh Truong", "Maximilian Heil"], "title": "DS@GT at CheckThat! 2025: Exploring Retrieval and Reranking Pipelines for Scientific Claim Source Retrieval on Social Media Discourse", "comment": null, "summary": "Social media users often make scientific claims without citing where these\nclaims come from, generating a need to verify these claims. This paper details\nwork done by the DS@GT team for CLEF 2025 CheckThat! Lab Task 4b Scientific\nClaim Source Retrieval which seeks to find relevant scientific papers based on\nimplicit references in tweets. Our team explored 6 different data augmentation\ntechniques, 7 different retrieval and reranking pipelines, and finetuned a\nbi-encoder. Achieving an MRR@5 of 0.58, our team ranked 16th out of 30 teams\nfor the CLEF 2025 CheckThat! Lab Task 4b, and improvement of 0.15 over the BM25\nbaseline of 0.43. Our code is available on Github at\nhttps://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4b.", "AI": {"tldr": "The DS@GT team participated in the CLEF 2025 CheckThat! Lab Task 4b, improving scientific claim source retrieval using data augmentation and bi-encoders.", "motivation": "Social media users often make scientific claims without citing sources, creating a need for verification.", "method": "The team explored 6 data augmentation techniques, 7 retrieval and reranking pipelines, and finetuned a bi-encoder.", "result": "The team's approach improved the MRR@5 score by 0.15 over the baseline.", "conclusion": "The DS@GT team achieved an MRR@5 of 0.58, ranking 16th out of 30 teams in the CLEF 2025 CheckThat! Lab Task 4b, a 0.15 improvement over the BM25 baseline."}}
{"id": "2507.06313", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06313", "abs": "https://arxiv.org/abs/2507.06313", "authors": ["Kiarash Zahirnia", "Zahra Golpayegani", "Walid Ahmad", "Yang Liu"], "title": "ETT: Expanding the Long Context Understanding Capability of LLMs at Test-Time", "comment": null, "summary": "Transformer-based Language Models' computation and memory overhead increase\nquadratically as a function of sequence length. The quadratic cost poses\nchallenges when employing LLMs for processing long sequences. In this work, we\nintroduce \\ourmodelacronym~(Extend at Test-Time), method for extending the\ncontext length of short context Transformer-based LLMs, with constant memory\nrequirement and linear computation overhead. ETT enable the extension of the\ncontext length at test-time by efficient fine-tuning the model's parameters on\nthe input context, chunked into overlapping small subsequences. We evaluate ETT\non LongBench by extending the context length of GPT-Large and Phi-2 up to 32\ntimes, increasing from 1k to 32k tokens. This results in up to a 30 percent\nimprovement in the model's accuracy. We also study how context can be stored in\nLLM's weights effectively and efficiently. Through a detailed ablation study,\nwe examine which Transformer modules are most beneficial to fine-tune at\ntest-time. Interestingly, we find that fine-tuning the second layer of the FFNs\nis more effective than full fine-tuning, leading to a further improvement in\nthe models' accuracy.", "AI": {"tldr": "Introduces Extend at Test-Time (ETT) to extend the context length of short context Transformer-based LLMs with constant memory and linear computation overhead, achieving up to 30% accuracy improvement on LongBench.", "motivation": "Transformer-based Language Models' computation and memory overhead increase quadratically as a function of sequence length. The quadratic cost poses challenges when employing LLMs for processing long sequences.", "method": "Extend at Test-Time (ETT): efficiently fine-tuning the model's parameters on the input context, chunked into overlapping small subsequences.", "result": "Extending the context length of GPT-Large and Phi-2 up to 32 times (increasing from 1k to 32k tokens) on LongBench results in up to a 30 percent improvement in the model's accuracy.", "conclusion": "Fine-tuning the second layer of the FFNs is more effective than full fine-tuning, leading to a further improvement in the models' accuracy."}}
{"id": "2507.06269", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06269", "abs": "https://arxiv.org/abs/2507.06269", "authors": ["Rushil Desai", "Frederik Warburg", "Trevor Darrell", "Marissa Ramirez de Chanlatte"], "title": "A Probabilistic Approach to Uncertainty Quantification Leveraging 3D Geometry", "comment": "ICCV 2025 Workshops (8 Pages, 6 Figures, 2 Tables)", "summary": "Quantifying uncertainty in neural implicit 3D representations, particularly\nthose utilizing Signed Distance Functions (SDFs), remains a substantial\nchallenge due to computational inefficiencies, scalability issues, and\ngeometric inconsistencies. Existing methods typically neglect direct geometric\nintegration, leading to poorly calibrated uncertainty maps. We introduce\nBayesSDF, a novel probabilistic framework for uncertainty quantification in\nneural implicit SDF models, motivated by scientific simulation applications\nwith 3D environments (e.g., forests) such as modeling fluid flow through\nforests, where precise surface geometry and awareness of fidelity surface\ngeometric uncertainty are essential. Unlike radiance-based models such as NeRF\nor 3D Gaussian splatting, which lack explicit surface formulations, SDFs define\ncontinuous and differentiable geometry, making them better suited for physical\nmodeling and analysis. BayesSDF leverages a Laplace approximation to quantify\nlocal surface instability via Hessian-based metrics, enabling computationally\nefficient, surface-aware uncertainty estimation. Our method shows that\nuncertainty predictions correspond closely with poorly reconstructed geometry,\nproviding actionable confidence measures for downstream use. Extensive\nevaluations on synthetic and real-world datasets demonstrate that BayesSDF\noutperforms existing methods in both calibration and geometric consistency,\nestablishing a strong foundation for uncertainty-aware 3D scene reconstruction,\nsimulation, and robotic decision-making.", "AI": {"tldr": "BayesSDF \u662f\u4e00\u79cd\u7528\u4e8e\u91cf\u5316\u795e\u7ecf\u9690\u5f0f SDF \u6a21\u578b\u4e2d\u4e0d\u786e\u5b9a\u6027\u7684\u6982\u7387\u6846\u67b6\uff0c\u5b83\u4f18\u4e8e\u73b0\u6709\u7684\u65b9\u6cd5\u3002", "motivation": "\u5bf9\u795e\u7ecf\u9690\u5f0f 3D \u8868\u793a\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u8fdb\u884c\u91cf\u5316\u4ecd\u7136\u662f\u4e00\u4e2a\u5de8\u5927\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u90a3\u4e9b\u5229\u7528\u6709\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570 (SDF) \u7684\u60c5\u51b5\u4e0b\uff0c\u7531\u4e8e\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3001\u53ef\u6269\u5c55\u6027\u95ee\u9898\u548c\u51e0\u4f55\u4e0d\u4e00\u81f4\u6027\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u76f4\u63a5\u51e0\u4f55\u79ef\u5206\uff0c\u5bfc\u81f4\u6821\u51c6\u4e0d\u826f\u7684\u4e0d\u786e\u5b9a\u6027\u56fe\u3002\u9700\u8981\u7cbe\u786e\u7684\u8868\u9762\u51e0\u4f55\u5f62\u72b6\u548c\u5bf9\u4fdd\u771f\u8868\u9762\u51e0\u4f55\u4e0d\u786e\u5b9a\u6027\u7684\u8ba4\u8bc6\u3002", "method": "BayesSDF \u5229\u7528\u62c9\u666e\u62c9\u65af\u8fd1\u4f3c\uff0c\u901a\u8fc7\u57fa\u4e8e Hessian \u7684\u6307\u6807\u91cf\u5316\u5c40\u90e8\u8868\u9762\u4e0d\u7a33\u5b9a\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u8ba1\u7b97\u9ad8\u6548\u7684\u3001\u8868\u9762\u611f\u77e5\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "result": "\u4e0d\u786e\u5b9a\u6027\u9884\u6d4b\u4e0e\u91cd\u5efa\u4e0d\u826f\u7684\u51e0\u4f55\u5f62\u72b6\u5bc6\u5207\u76f8\u5173\uff0c\u4e3a\u4e0b\u6e38\u4f7f\u7528\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u7f6e\u4fe1\u5ea6\u5ea6\u91cf\u3002\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u8bc4\u4f30\u8868\u660e\uff0cBayesSDF \u5728\u6821\u51c6\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "BayesSDF\u5728\u6821\u51c6\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5 3D \u573a\u666f\u91cd\u5efa\u3001\u6a21\u62df\u548c\u673a\u5668\u4eba\u51b3\u7b56\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2507.06342", "categories": ["cs.LG", "cs.AI", "math.DS", "math.SG"], "pdf": "https://arxiv.org/pdf/2507.06342", "abs": "https://arxiv.org/abs/2507.06342", "authors": ["M. A. Evangelista-Alvarado", "P. Su\u00e1rez-Serrato"], "title": "SymFlux: deep symbolic regression of Hamiltonian vector fields", "comment": "26 pages, 7 figures", "summary": "We present SymFlux, a novel deep learning framework that performs symbolic\nregression to identify Hamiltonian functions from their corresponding vector\nfields on the standard symplectic plane. SymFlux models utilize hybrid CNN-LSTM\narchitectures to learn and output the symbolic mathematical expression of the\nunderlying Hamiltonian. Training and validation are conducted on newly\ndeveloped datasets of Hamiltonian vector fields, a key contribution of this\nwork. Our results demonstrate the model's effectiveness in accurately\nrecovering these symbolic expressions, advancing automated discovery in\nHamiltonian mechanics.", "AI": {"tldr": "A novel deep learning framework that performs symbolic regression to identify Hamiltonian functions.", "motivation": "Identify Hamiltonian functions from their corresponding vector fields on the standard symplectic plane.", "method": "Utilize hybrid CNN-LSTM architectures to learn and output the symbolic mathematical expression of the underlying Hamiltonian.", "result": "Demonstrate the model's effectiveness in accurately recovering these symbolic expressions.", "conclusion": "The model's effectiveness in accurately recovering symbolic expressions, advancing automated discovery in Hamiltonian mechanics."}}
{"id": "2507.06993", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06993", "abs": "https://arxiv.org/abs/2507.06993", "authors": ["Jieren Deng", "Aleksandar Cvetkovic", "Pak Kiu Chung", "Dragomir Yankov", "Chiqun Zhang"], "title": "The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced Planning, Navigation, and Dynamic Adaptation", "comment": null, "summary": "Traditional travel-planning systems are often static and fragmented, leaving\nthem ill-equipped to handle real-world complexities such as evolving\nenvironmental conditions and unexpected itinerary disruptions. In this paper,\nwe identify three gaps between existing service providers causing frustrating\nuser experience: intelligent trip planning, precision \"last-100-meter\"\nnavigation, and dynamic itinerary adaptation. We propose three cooperative\nagents: a Travel Planning Agent that employs grid-based spatial grounding and\nmap analysis to help resolve complex multi-modal user queries; a Destination\nAssistant Agent that provides fine-grained guidance for the final navigation\nleg of each journey; and a Local Discovery Agent that leverages image\nembeddings and Retrieval-Augmented Generation (RAG) to detect and respond to\ntrip plan disruptions. With evaluations and experiments, our system\ndemonstrates substantial improvements in query interpretation, navigation\naccuracy, and disruption resilience, underscoring its promise for applications\nfrom urban exploration to emergency response.", "AI": {"tldr": "This paper introduces a travel-planning system with three cooperative agents that improve query interpretation, navigation accuracy, and disruption resilience.", "motivation": "Traditional travel-planning systems are often static and fragmented, leaving them ill-equipped to handle real-world complexities such as evolving environmental conditions and unexpected itinerary disruptions. We identify three gaps between existing service providers causing frustrating user experience: intelligent trip planning, precision 'last-100-meter' navigation, and dynamic itinerary adaptation.", "method": "We propose three cooperative agents: a Travel Planning Agent that employs grid-based spatial grounding and map analysis; a Destination Assistant Agent that provides fine-grained guidance; and a Local Discovery Agent that leverages image embeddings and Retrieval-Augmented Generation (RAG).", "result": "demonstrates substantial improvements in query interpretation, navigation accuracy, and disruption resilience", "conclusion": "The system demonstrates substantial improvements in query interpretation, navigation accuracy, and disruption resilience, underscoring its promise for applications from urban exploration to emergency response."}}
{"id": "2507.06596", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.06596", "abs": "https://arxiv.org/abs/2507.06596", "authors": ["Robin Ungruh", "Alejandro Bellog\u00edn", "Dominik Kowald", "Maria Soledad Pera"], "title": "Impacts of Mainstream-Driven Algorithms on Recommendations for Children Across Domains: A Reproducibility Study", "comment": "Preprint of accepted RecSys 2025 contribution", "summary": "Children are often exposed to items curated by recommendation algorithms.\nYet, research seldom considers children as a user group, and when it does, it\nis anchored on datasets where children are underrepresented, risking\noverlooking their interests, favoring those of the majority, i.e., mainstream\nusers. Recently, Ungruh et al. demonstrated that children's consumption\npatterns and preferences differ from those of mainstream users, resulting in\ninconsistent recommendation algorithm performance and behavior for this user\ngroup. These findings, however, are based on two datasets with a limited child\nuser sample. We reproduce and replicate this study on a wider range of datasets\nin the movie, music, and book domains, uncovering interaction patterns and\naspects of child-recommender interactions consistent across domains, as well as\nthose specific to some user samples in the data. We also extend insights from\nthe original study with popularity bias metrics, given the interpretation of\nresults from the original study. With this reproduction and extension, we\nuncover consumption patterns and differences between age groups stemming from\nintrinsic differences between children and others, and those unique to specific\ndatasets or domains.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u901a\u8fc7\u5728\u66f4\u5e7f\u6cdb\u7684\u6570\u636e\u96c6\u4e0a\u590d\u5236\u548c\u6269\u5c55\u5148\u524d\u7684\u7814\u7a76\uff0c\u5206\u6790\u4e86\u513f\u7ae5\u7684\u63a8\u8350\u7b97\u6cd5\u6d88\u8d39\u6a21\u5f0f\u3002", "motivation": "\u513f\u7ae5\u7ecf\u5e38\u63a5\u89e6\u7531\u63a8\u8350\u7b97\u6cd5\u7b56\u5212\u7684\u9879\u76ee\u3002\u7136\u800c\uff0c\u7814\u7a76\u5f88\u5c11\u5c06\u513f\u7ae5\u4f5c\u4e3a\u4e00\u4e2a\u7528\u6237\u7fa4\u4f53\u6765\u8003\u8651\uff0c\u800c\u4e14\u5f53\u5b83\u8fd9\u6837\u505a\u65f6\uff0c\u5b83\u951a\u5b9a\u5728\u513f\u7ae5\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u6709\u53ef\u80fd\u5ffd\u89c6\u4ed6\u4eec\u7684\u5174\u8da3\uff0c\u800c\u504f\u8892\u5927\u591a\u6570\u4eba\uff0c\u5373\u4e3b\u6d41\u7528\u6237\u3002", "method": "\u5728\u7535\u5f71\u3001\u97f3\u4e50\u548c\u4e66\u7c4d\u9886\u57df\u66f4\u5e7f\u6cdb\u7684\u6570\u636e\u96c6\u4e0a\u590d\u5236\u548c\u91cd\u590d\u5148\u524d\u7684\u7814\u7a76\uff0c\u5e76\u4f7f\u7528\u53d7\u6b22\u8fce\u7a0b\u5ea6\u504f\u5dee\u6307\u6807\u6269\u5c55\u4e86\u539f\u59cb\u7814\u7a76\u7684\u89c1\u89e3\u3002", "result": "\u63ed\u793a\u4e86\u8de8\u9886\u57df\u4e00\u81f4\u7684\u513f\u7ae5-\u63a8\u8350\u8005\u4e92\u52a8\u6a21\u5f0f\u548c\u65b9\u9762\uff0c\u4ee5\u53ca\u6570\u636e\u4e2d\u67d0\u4e9b\u7528\u6237\u6837\u672c\u7279\u6709\u7684\u6a21\u5f0f\u548c\u65b9\u9762\u3002", "conclusion": "\u63ed\u793a\u4e86\u513f\u7ae5\u4e0e\u5176\u4ed6\u5e74\u9f84\u7ec4\u4e4b\u95f4\u7531\u4e8e\u5185\u5728\u5dee\u5f02\u4ee5\u53ca\u7279\u5b9a\u4e8e\u7279\u5b9a\u6570\u636e\u96c6\u6216\u9886\u57df\u7684\u5dee\u5f02\u800c\u4ea7\u751f\u7684\u6d88\u8d39\u6a21\u5f0f\u548c\u5dee\u5f02\u3002"}}
{"id": "2507.06335", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06335", "abs": "https://arxiv.org/abs/2507.06335", "authors": ["Casey Kennington", "David Schlangen"], "title": "Could the Road to Grounded, Neuro-symbolic AI be Paved with Words-as-Classifiers?", "comment": "9 pages", "summary": "Formal, Distributional, and Grounded theories of computational semantics each\nhave their uses and their drawbacks. There has been a shift to ground models of\nlanguage by adding visual knowledge, and there has been a call to enrich models\nof language with symbolic methods to gain the benefits from formal,\ndistributional, and grounded theories. In this paper, we attempt to make the\ncase that one potential path forward in unifying all three semantic fields is\npaved with the words-as-classifier model, a model of word-level grounded\nsemantics that has been incorporated into formalisms and distributional\nlanguage models in the literature, and it has been well-tested within\ninteractive dialogue settings. We review that literature, motivate the\nwords-as-classifiers model with an appeal to recent work in cognitive science,\nand describe a small experiment. Finally, we sketch a model of semantics\nunified through words-as-classifiers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528 words-as-classifiers \u6a21\u578b\u7edf\u4e00\u5f62\u5f0f\u3001\u5206\u5e03\u548c\u5177\u8eab\u8ba1\u7b97\u8bed\u4e49\u3002", "motivation": "\u65e8\u5728\u7edf\u4e00\u5f62\u5f0f\u3001\u5206\u5e03\u548c\u5177\u8eab\u8ba1\u7b97\u8bed\u4e49\u8fd9\u4e09\u4e2a\u9886\u57df\uff0c\u5e76\u7ed3\u5408\u8ba4\u77e5\u79d1\u5b66\u7684\u6700\u65b0\u7814\u7a76\u3002", "method": "\u56de\u987e\u4e86 words-as-classifiers \u6a21\u578b\u7684\u6587\u732e\uff0c\u5e76\u8fdb\u884c\u4e86\u4e00\u4e2a\u5c0f\u578b\u5b9e\u9a8c\u3002", "result": "words-as-classifiers \u6a21\u578b\u5df2\u88ab\u7eb3\u5165\u6587\u732e\u4e2d\u7684\u5f62\u5f0f\u4f53\u7cfb\u548c\u5206\u5e03\u5f0f\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5728\u4ea4\u4e92\u5f0f\u5bf9\u8bdd\u8bbe\u7f6e\u4e2d\u5f97\u5230\u4e86\u5145\u5206\u7684\u6d4b\u8bd5\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u8fc7 words-as-classifiers \u7edf\u4e00\u8bed\u4e49\u7684\u6a21\u578b\u8349\u56fe\u3002"}}
{"id": "2507.06272", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06272", "abs": "https://arxiv.org/abs/2507.06272", "authors": ["Zhang Li", "Biao Yang", "Qiang Liu", "Shuo Zhang", "Zhiyin Ma", "Shuo Zhang", "Liang Yin", "Linger Deng", "Yabo Sun", "Yuliang Liu", "Xiang Bai"], "title": "LIRA: Inferring Segmentation in Large Multi-modal Models with Local Interleaved Region Assistance", "comment": null, "summary": "While large multi-modal models (LMMs) demonstrate promising capabilities in\nsegmentation and comprehension, they still struggle with two limitations:\ninaccurate segmentation and hallucinated comprehension. These challenges stem\nprimarily from constraints in weak visual comprehension and a lack of\nfine-grained perception. To alleviate these limitations, we propose LIRA, a\nframework that capitalizes on the complementary relationship between visual\ncomprehension and segmentation via two key components: (1) Semantic-Enhanced\nFeature Extractor (SEFE) improves object attribute inference by fusing semantic\nand pixel-level features, leading to more accurate segmentation; (2)\nInterleaved Local Visual Coupling (ILVC) autoregressively generates local\ndescriptions after extracting local features based on segmentation masks,\noffering fine-grained supervision to mitigate hallucinations. Furthermore, we\nfind that the precision of object segmentation is positively correlated with\nthe latent related semantics of the <seg> token. To quantify this relationship\nand the model's potential semantic inferring ability, we introduce the\nAttributes Evaluation (AttrEval) dataset. Our experiments show that LIRA\nachieves state-of-the-art performance in both segmentation and comprehension\ntasks. Code will be available at https://github.com/echo840/LIRA.", "AI": {"tldr": "LIRA improves segmentation and comprehension in large multi-modal models by fusing semantic and pixel-level features and generating local descriptions based on segmentation masks.", "motivation": "Large multi-modal models (LMMs) still struggle with inaccurate segmentation and hallucinated comprehension. These challenges stem primarily from constraints in weak visual comprehension and a lack of fine-grained perception.", "method": "LIRA, a framework that capitalizes on the complementary relationship between visual comprehension and segmentation via two key components: (1) Semantic-Enhanced Feature Extractor (SEFE); (2) Interleaved Local Visual Coupling (ILVC).", "result": "The precision of object segmentation is positively correlated with the latent related semantics of the <seg> token. LIRA achieves state-of-the-art performance in both segmentation and comprehension tasks.", "conclusion": "LIRA achieves state-of-the-art performance in both segmentation and comprehension tasks."}}
{"id": "2507.06366", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2507.06366", "abs": "https://arxiv.org/abs/2507.06366", "authors": ["Yupu Zhang", "Zelin Xu", "Tingsong Xiao", "Gustavo Seabra", "Yanjun Li", "Chenglong Li", "Zhe Jiang"], "title": "DecoyDB: A Dataset for Graph Contrastive Learning in Protein-Ligand Binding Affinity Prediction", "comment": null, "summary": "Predicting the binding affinity of protein-ligand complexes plays a vital\nrole in drug discovery. Unfortunately, progress has been hindered by the lack\nof large-scale and high-quality binding affinity labels. The widely used\nPDBbind dataset has fewer than 20K labeled complexes. Self-supervised learning,\nespecially graph contrastive learning (GCL), provides a unique opportunity to\nbreak the barrier by pre-training graph neural network models based on vast\nunlabeled complexes and fine-tuning the models on much fewer labeled complexes.\nHowever, the problem faces unique challenges, including a lack of a\ncomprehensive unlabeled dataset with well-defined positive/negative complex\npairs and the need to design GCL algorithms that incorporate the unique\ncharacteristics of such data. To fill the gap, we propose DecoyDB, a\nlarge-scale, structure-aware dataset specifically designed for self-supervised\nGCL on protein-ligand complexes. DecoyDB consists of high-resolution ground\ntruth complexes (less than 2.5 Angstrom) and diverse decoy structures with\ncomputationally generated binding poses that range from realistic to suboptimal\n(negative pairs). Each decoy is annotated with a Root Mean Squared Deviation\n(RMSD) from the native pose. We further design a customized GCL framework to\npre-train graph neural networks based on DecoyDB and fine-tune the models with\nlabels from PDBbind. Extensive experiments confirm that models pre-trained with\nDecoyDB achieve superior accuracy, label efficiency, and generalizability.", "AI": {"tldr": "This paper introduces DecoyDB, a large-scale dataset for self-supervised graph contrastive learning on protein-ligand complexes. A customized GCL framework is designed to pre-train graph neural networks based on DecoyDB and fine-tune the models with labels from PDBbind. Models pre-trained with DecoyDB achieve superior accuracy, label efficiency, and generalizability.", "motivation": "Predicting the binding affinity of protein-ligand complexes plays a vital role in drug discovery. Unfortunately, progress has been hindered by the lack of large-scale and high-quality binding affinity labels. Self-supervised learning, especially graph contrastive learning (GCL), provides a unique opportunity to break the barrier by pre-training graph neural network models based on vast unlabeled complexes and fine-tuning the models on much fewer labeled complexes. However, the problem faces unique challenges, including a lack of a comprehensive unlabeled dataset with well-defined positive/negative complex pairs and the need to design GCL algorithms that incorporate the unique characteristics of such data.", "method": "We propose DecoyDB, a large-scale, structure-aware dataset specifically designed for self-supervised GCL on protein-ligand complexes. We further design a customized GCL framework to pre-train graph neural networks based on DecoyDB and fine-tune the models with labels from PDBbind.", "result": "DecoyDB consists of high-resolution ground truth complexes (less than 2.5 Angstrom) and diverse decoy structures with computationally generated binding poses that range from realistic to suboptimal (negative pairs). Each decoy is annotated with a Root Mean Squared Deviation (RMSD) from the native pose.", "conclusion": "Models pre-trained with DecoyDB achieve superior accuracy, label efficiency, and generalizability."}}
{"id": "2507.07017", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07017", "abs": "https://arxiv.org/abs/2507.07017", "authors": ["Tianyu Zheng", "Tianshun Xing", "Qingshui Gu", "Taoran Liang", "Xingwei Qu", "Xin Zhou", "Yizhi Li", "Zhoufutu Wen", "Chenghua Lin", "Wenhao Huang", "Qian Liu", "Ge Zhang", "Zejun Ma"], "title": "First Return, Entropy-Eliciting Explore", "comment": null, "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning\nabilities of Large Language Models (LLMs) but it struggles with unstable\nexploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a\nstructured exploration framework that identifies high-uncertainty decision\npoints in reasoning trajectories and performs targeted rollouts to construct\nsemantically grounded intermediate feedback. Our method provides targeted\nguidance without relying on dense supervision. Empirical results on\nmathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable\ntraining, produces longer and more coherent responses, and increases the\nproportion of fully correct trajectories. These results highlight the\nframework's effectiveness in improving LLM reasoning through more robust and\nstructured exploration.", "AI": {"tldr": "FR3E\uff1a\u4e00\u79cd\u7ed3\u6784\u5316\u7684\u63a2\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684rollout\u548c\u8bed\u4e49\u4e2d\u95f4\u53cd\u9988\u6765\u63d0\u9ad8LLM\u63a8\u7406\u80fd\u529b\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u548c\u66f4\u51c6\u786e\u7684\u63a8\u7406\u3002", "motivation": "\u9a8c\u8bc1\u6027\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60(RLVR)\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5b83\u5728\u4e0d\u7a33\u5b9a\u7684\u63a2\u7d22\u4e2d\u6323\u624e\u3002", "method": "FR3E\uff08\u9996\u6b21\u56de\u62a5\uff0c\u71b5\u5f15\u53d1\u63a2\u7d22\uff09\uff0c\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u63a2\u7d22\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u9ad8\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u70b9\uff0c\u5e76\u6267\u884c\u6709\u9488\u5bf9\u6027\u7684rollout\u4ee5\u6784\u5efa\u8bed\u4e49\u4e0a\u6709\u6839\u636e\u7684\u4e2d\u95f4\u53cd\u9988\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6(AIME24)\u4e0a\u7684\u7ecf\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFR3E\u4fc3\u8fdb\u4e86\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\uff0c\u4ea7\u751f\u66f4\u957f\u548c\u66f4\u8fde\u8d2f\u7684\u54cd\u5e94\uff0c\u5e76\u589e\u52a0\u4e86\u5b8c\u5168\u6b63\u786e\u7684\u8f68\u8ff9\u7684\u6bd4\u4f8b\u3002", "conclusion": "FR3E\u901a\u8fc7\u66f4\u7a33\u5b9a\u548c\u7ed3\u6784\u5316\u7684\u63a2\u7d22\uff0c\u6709\u6548\u5730\u63d0\u9ad8\u4e86LLM\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2507.06782", "categories": ["cs.IR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06782", "abs": "https://arxiv.org/abs/2507.06782", "authors": ["SeungYoon Han", "Taeho Hwang", "Sukmin Cho", "Soyeong Jeong", "Hoyun Song", "Huije Lee", "Jong C. Park"], "title": "Temporal Information Retrieval via Time-Specifier Model Merging", "comment": null, "summary": "The rapid expansion of digital information and knowledge across structured\nand unstructured sources has heightened the importance of Information Retrieval\n(IR). While dense retrieval methods have substantially improved semantic\nmatching for general queries, they consistently underperform on queries with\nexplicit temporal constraints--often those containing numerical expressions and\ntime specifiers such as ``in 2015.'' Existing approaches to Temporal\nInformation Retrieval (TIR) improve temporal reasoning but often suffer from\ncatastrophic forgetting, leading to reduced performance on non-temporal\nqueries. To address this, we propose Time-Specifier Model Merging (TSM), a\nnovel method that enhances temporal retrieval while preserving accuracy on\nnon-temporal queries. TSM trains specialized retrievers for individual time\nspecifiers and merges them in to a unified model, enabling precise handling of\ntemporal constraints without compromising non-temporal retrieval. Extensive\nexperiments on both temporal and non-temporal datasets demonstrate that TSM\nsignificantly improves performance on temporally constrained queries while\nmaintaining strong results on non-temporal queries, consistently outperforming\nother baseline methods. Our code is available at\nhttps://github.com/seungyoonee/TSM .", "AI": {"tldr": "TSM enhances temporal retrieval while preserving accuracy on non-temporal queries by training specialized retrievers for individual time specifiers and merging them into a unified model.", "motivation": "dense retrieval methods consistently underperform on queries with explicit temporal constraints, Existing approaches to Temporal Information Retrieval (TIR) improve temporal reasoning but often suffer from catastrophic forgetting, leading to reduced performance on non-temporal queries.", "method": "Time-Specifier Model Merging (TSM), a novel method that enhances temporal retrieval while preserving accuracy on non-temporal queries. TSM trains specialized retrievers for individual time specifiers and merges them in to a unified model", "result": "TSM significantly improves performance on temporally constrained queries while maintaining strong results on non-temporal queries, consistently outperforming other baseline methods.", "conclusion": "TSM significantly improves performance on temporally constrained queries while maintaining strong results on non-temporal queries, consistently outperforming other baseline methods."}}
{"id": "2507.06378", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06378", "abs": "https://arxiv.org/abs/2507.06378", "authors": ["Catherine Arnett", "Marisa Hudspeth", "Brendan O'Connor"], "title": "Evaluating Morphological Alignment of Tokenizers in 70 Languages", "comment": "6 pages, 3 figures. Accepted to the Tokenization Workshop at ICML\n  2025", "summary": "While tokenization is a key step in language modeling, with effects on model\ntraining and performance, it remains unclear how to effectively evaluate\ntokenizer quality. One proposed dimension of tokenizer quality is the extent to\nwhich tokenizers preserve linguistically meaningful subwords, aligning token\nboundaries with morphological boundaries within a word. We expand MorphScore\n(Arnett & Bergen, 2025), which previously covered 22 languages, to support a\ntotal of 70 languages. The updated MorphScore offers more flexibility in\nevaluation and addresses some of the limitations of the original version. We\nthen correlate our alignment scores with downstream task performance for five\npre-trained languages models on seven tasks, with at least one task in each of\nthe languages in our sample. We find that morphological alignment does not\nexplain very much variance in model performance, suggesting that morphological\nalignment alone does not measure dimensions of tokenization quality relevant to\nmodel performance.", "AI": {"tldr": "evaluate tokenizer quality by morphological alignment", "motivation": "it remains unclear how to effectively evaluate tokenizer quality", "method": "expand MorphScore to support a total of 70 languages and correlate alignment scores with downstream task performance for five pre-trained languages models on seven tasks", "result": "morphological alignment does not explain very much variance in model performance", "conclusion": "morphological alignment alone does not measure dimensions of tokenization quality relevant to model performance"}}
{"id": "2507.06275", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06275", "abs": "https://arxiv.org/abs/2507.06275", "authors": ["Yassin Hussein Rassul", "Aram M. Ahmed", "Polla Fattah", "Bryar A. Hassan", "Arwaa W. Abdulkareem", "Tarik A. Rashid", "Joan Lu"], "title": "Advancing Offline Handwritten Text Recognition: A Systematic Review of Data Augmentation and Generation Techniques", "comment": null, "summary": "Offline Handwritten Text Recognition (HTR) systems play a crucial role in\napplications such as historical document digitization, automatic form\nprocessing, and biometric authentication. However, their performance is often\nhindered by the limited availability of annotated training data, particularly\nfor low-resource languages and complex scripts. This paper presents a\ncomprehensive survey of offline handwritten data augmentation and generation\ntechniques designed to improve the accuracy and robustness of HTR systems. We\nsystematically examine traditional augmentation methods alongside recent\nadvances in deep learning, including Generative Adversarial Networks (GANs),\ndiffusion models, and transformer-based approaches. Furthermore, we explore the\nchallenges associated with generating diverse and realistic handwriting\nsamples, particularly in preserving script authenticity and addressing data\nscarcity. This survey follows the PRISMA methodology, ensuring a structured and\nrigorous selection process. Our analysis began with 1,302 primary studies,\nwhich were filtered down to 848 after removing duplicates, drawing from key\nacademic sources such as IEEE Digital Library, Springer Link, Science Direct,\nand ACM Digital Library. By evaluating existing datasets, assessment metrics,\nand state-of-the-art methodologies, this survey identifies key research gaps\nand proposes future directions to advance the field of handwritten text\ngeneration across diverse linguistic and stylistic landscapes.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u79bb\u7ebf\u624b\u5199\u6570\u636e\u589e\u5f3a\u548c\u751f\u6210\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8 HTR \u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7740\u91cd\u4e8e GAN\u3001\u6269\u6563\u6a21\u578b\u548c Transformer \u7b49\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u8131\u673a\u624b\u5199\u6587\u672c\u8bc6\u522b (HTR) \u7cfb\u7edf\u5728\u5386\u53f2\u6587\u6863\u6570\u5b57\u5316\u3001\u81ea\u52a8\u8868\u5355\u5904\u7406\u548c\u751f\u7269\u7279\u5f81\u8ba4\u8bc1\u7b49\u5e94\u7528\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u7136\u800c\uff0c\u5b83\u4eec\u7684\u6027\u80fd\u901a\u5e38\u53d7\u5230\u5e26\u6ce8\u91ca\u7684\u8bad\u7ec3\u6570\u636e\u53ef\u7528\u6027\u6709\u9650\u7684\u963b\u788d\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u590d\u6742\u811a\u672c\u3002", "method": "\u8be5\u8c03\u67e5\u9075\u5faa PRISMA \u65b9\u6cd5\uff0c\u786e\u4fdd\u7ed3\u6784\u5316\u548c\u4e25\u683c\u7684\u9009\u62e9\u8fc7\u7a0b\u3002\u6211\u4eec\u7684\u5206\u6790\u59cb\u4e8e 1,302 \u9879\u521d\u6b65\u7814\u7a76\uff0c\u5728\u5220\u9664\u91cd\u590d\u9879\u540e\u51cf\u5c11\u5230 848 \u9879\uff0c\u8fd9\u4e9b\u7814\u7a76\u6765\u81ea IEEE \u6570\u5b57\u56fe\u4e66\u9986\u3001Springer Link\u3001Science Direct \u548c ACM \u6570\u5b57\u56fe\u4e66\u9986\u7b49\u4e3b\u8981\u5b66\u672f\u6765\u6e90\u3002", "result": "\u672c\u6587\u5168\u9762\u8c03\u67e5\u4e86\u79bb\u7ebf\u624b\u5199\u6570\u636e\u589e\u5f3a\u548c\u751f\u6210\u6280\u672f\uff0c\u65e8\u5728\u63d0\u9ad8 HTR \u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002\u6211\u4eec\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u4f20\u7edf\u589e\u5f3a\u65b9\u6cd5\u4ee5\u53ca\u6df1\u5ea6\u5b66\u4e60\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5305\u62ec\u751f\u6210\u5bf9\u6297\u7f51\u7edc (GAN)\u3001\u6269\u6563\u6a21\u578b\u548c\u57fa\u4e8e Transformer \u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8c03\u67e5\u901a\u8fc7\u8bc4\u4f30\u73b0\u6709\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u548c\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u786e\u5b9a\u4e86\u5173\u952e\u7684\u7814\u7a76\u5dee\u8ddd\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u4ee5\u4fc3\u8fdb\u8de8\u4e0d\u540c\u8bed\u8a00\u548c\u98ce\u683c\u7684\u624b\u5199\u6587\u672c\u751f\u6210\u9886\u57df\u3002"}}
{"id": "2507.06367", "categories": ["cs.LG", "math.AG"], "pdf": "https://arxiv.org/pdf/2507.06367", "abs": "https://arxiv.org/abs/2507.06367", "authors": ["El Mehdi Achour", "Kathl\u00e9n Kohn", "Holger Rauhut"], "title": "The Riemannian Geometry associated to Gradient Flows of Linear Convolutional Networks", "comment": null, "summary": "We study geometric properties of the gradient flow for learning deep linear\nconvolutional networks. For linear fully connected networks, it has been shown\nrecently that the corresponding gradient flow on parameter space can be written\nas a Riemannian gradient flow on function space (i.e., on the product of weight\nmatrices) if the initialization satisfies a so-called balancedness condition.\nWe establish that the gradient flow on parameter space for learning linear\nconvolutional networks can be written as a Riemannian gradient flow on function\nspace regardless of the initialization. This result holds for $D$-dimensional\nconvolutions with $D \\geq 2$, and for $D =1$ it holds if all so-called strides\nof the convolutions are greater than one. The corresponding Riemannian metric\ndepends on the initialization.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6df1\u5ea6\u7ebf\u6027\u5377\u79ef\u7f51\u7edc\u68af\u5ea6\u6d41\u7684\u51e0\u4f55\u6027\u8d28\uff0c\u53d1\u73b0\u5728\u4e00\u5b9a\u6761\u4ef6\u4e0b\uff0c\u5b83\u53ef\u4ee5\u5199\u6210\u51fd\u6570\u7a7a\u95f4\u4e0a\u7684\u9ece\u66fc\u68af\u5ea6\u6d41\u3002", "motivation": "\u7814\u7a76\u7ebf\u6027\u5168\u8fde\u63a5\u7f51\u7edc\u7684\u53c2\u6570\u7a7a\u95f4\u4e0a\u7684\u68af\u5ea6\u6d41\u53ef\u4ee5\u5199\u6210\u51fd\u6570\u7a7a\u95f4\u4e0a\u7684\u9ece\u66fc\u68af\u5ea6\u6d41\uff08\u5373\uff0c\u5728\u6743\u91cd\u77e9\u9635\u7684\u4e58\u79ef\u4e0a\uff09\uff0c\u5982\u679c\u521d\u59cb\u5316\u6ee1\u8db3\u6240\u8c13\u7684\u5e73\u8861\u6761\u4ef6\u3002", "method": "\u7814\u7a76\u4e86\u7528\u4e8e\u5b66\u4e60\u6df1\u5ea6\u7ebf\u6027\u5377\u79ef\u7f51\u7edc\u7684\u68af\u5ea6\u6d41\u7684\u51e0\u4f55\u6027\u8d28\u3002", "result": "\u7ebf\u6027\u5377\u79ef\u7f51\u7edc\u7684\u53c2\u6570\u7a7a\u95f4\u4e0a\u7684\u68af\u5ea6\u6d41\u53ef\u4ee5\u5199\u6210\u51fd\u6570\u7a7a\u95f4\u4e0a\u7684\u9ece\u66fc\u68af\u5ea6\u6d41\uff0c\u800c\u4e0e\u521d\u59cb\u5316\u65e0\u5173\u3002\u8fd9\u4e00\u7ed3\u679c\u9002\u7528\u4e8e$D \\geq 2$\u7684$D$\u7ef4\u5377\u79ef\uff0c\u5bf9\u4e8e$D =1$\uff0c\u5982\u679c\u5377\u79ef\u7684\u6240\u6709\u6b65\u957f\u90fd\u5927\u4e8e1\uff0c\u5219\u8be5\u7ed3\u679c\u6210\u7acb\u3002", "conclusion": "\u68af\u5ea6\u6d41\u5728\u51fd\u6570\u7a7a\u95f4\u4e0a\u53ef\u4ee5\u5199\u6210\u9ece\u66fc\u68af\u5ea6\u6d41\uff0c\u800c\u9ece\u66fc\u5ea6\u91cf\u53d6\u51b3\u4e8e\u521d\u59cb\u5316\u3002"}}
{"id": "2507.05116", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.05116", "abs": "https://arxiv.org/abs/2507.05116", "authors": ["Juyi Lin", "Amir Taherin", "Arash Akbari", "Arman Akbari", "Lei Lu", "Guangyu Chen", "Taskin Padir", "Xiaomeng Yang", "Weiwei Chen", "Yiqian Li", "Xue Lin", "David Kaeli", "Pu Zhao", "Yanzhi Wang"], "title": "VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting", "comment": null, "summary": "Recent large-scale Vision Language Action (VLA) models have shown superior\nperformance in robotic manipulation tasks guided by natural language. However,\ntheir generalization remains limited when applied to novel objects or\nunfamiliar environments that lie outside the training distribution. To address\nthis, many existing approaches integrate additional components such as depth\nestimation, segmentation, or even diffusion to improve generalization, at the\ncost of adding significant computation overhead, resulting in low efficiency.\nThis motivates the exploration of efficient action prediction methods, which\nare independent of additional high-level visual representations or diffusion\ntechniques. In this work, we propose VOTE, an efficient and general framework\nfor the optimization and acceleration of VLA models. In details, we propose a\nnovel tokenizer-free fine-tuning approach for parallel accurate action\nprediction, which reduces computational overhead and accelerates inference\nspeed. Additionally, we adopt an ensemble voting strategy for the action\nsampling, which significantly improves model performance and enhances\ngeneralization. Experimental results show that our method achieves\nstate-of-the-art performance with 35$\\times$ faster inference and 145 Hz\nthroughput. All the details and codes will be open-sourced.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVOTE\u7684\u9ad8\u6548\u901a\u7528\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u548c\u52a0\u901fVLA\u6a21\u578b\uff0c\u901a\u8fc7\u65e0\u5206\u8bcd\u5668\u7684\u5fae\u8c03\u548c\u96c6\u6210\u6295\u7968\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u901f\u5ea6\u548c\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u5f15\u5bfc\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5f53\u5e94\u7528\u4e8e\u8bad\u7ec3\u5206\u5e03\u4e4b\u5916\u7684\u65b0\u7269\u4f53\u6216\u4e0d\u719f\u6089\u7684\u73af\u5883\u65f6\uff0c\u5b83\u4eec\u7684\u6cdb\u5316\u80fd\u529b\u4ecd\u7136\u6709\u9650\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u8bb8\u591a\u73b0\u6709\u7684\u65b9\u6cd5\u96c6\u6210\u4e86\u989d\u5916\u7684\u7ec4\u4ef6\uff0c\u5982\u6df1\u5ea6\u4f30\u8ba1\u3001\u5206\u5272\uff0c\u751a\u81f3\u6269\u6563\uff0c\u4ee5\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u4ee3\u4ef7\u662f\u589e\u52a0\u4e86\u5927\u91cf\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002\u8fd9\u4fc3\u4f7f\u4eba\u4eec\u63a2\u7d22\u6709\u6548\u7684\u884c\u52a8\u9884\u6d4b\u65b9\u6cd5\uff0c\u8fd9\u79cd\u65b9\u6cd5\u72ec\u7acb\u4e8e\u989d\u5916\u7684\u9ad8\u7ea7\u89c6\u89c9\u8868\u5f81\u6216\u6269\u6563\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u5206\u8bcd\u5668\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e\u5e76\u884c\u7cbe\u786e\u7684\u52a8\u4f5c\u9884\u6d4b\uff0c\u5e76\u91c7\u7528\u4e86\u4e00\u79cd\u96c6\u6210\u6295\u7968\u7b56\u7565\u8fdb\u884c\u52a8\u4f5c\u91c7\u6837\u3002", "result": "VOTE\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8\u4e8635\u500d\uff0c\u541e\u5410\u91cf\u8fbe\u5230\u4e86145 Hz\u3002", "conclusion": "VOTE\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8\u4e8635\u500d\uff0c\u541e\u5410\u91cf\u8fbe\u5230\u4e86145 Hz\u3002"}}
{"id": "2507.06877", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.06877", "abs": "https://arxiv.org/abs/2507.06877", "authors": ["Huishi Luo", "Yiqing Wu", "Yiwen Chen", "Fuzhen Zhuang", "Deqing Wang"], "title": "CDC: Causal Domain Clustering for Multi-Domain Recommendation", "comment": "Accepted at SIGIR 2025", "summary": "Multi-domain recommendation leverages domain-general knowledge to improve\nrecommendations across several domains. However, as platforms expand to dozens\nor hundreds of scenarios, training all domains in a unified model leads to\nperformance degradation due to significant inter-domain differences. Existing\ndomain grouping methods, based on business logic or data similarities, often\nfail to capture the true transfer relationships required for optimal grouping.\nTo effectively cluster domains, we propose Causal Domain Clustering (CDC). CDC\nmodels domain transfer patterns within a large number of domains using two\ndistinct effects: the Isolated Domain Affinity Matrix for modeling\nnon-interactive domain transfers, and the Hybrid Domain Affinity Matrix for\nconsidering dynamic domain synergy or interference under joint training. To\nintegrate these two transfer effects, we introduce causal discovery to\ncalculate a cohesion-based coefficient that adaptively balances their\ncontributions. A Co-Optimized Dynamic Clustering algorithm iteratively\noptimizes target domain clustering and source domain selection for training.\nCDC significantly enhances performance across over 50 domains on public\ndatasets and in industrial settings, achieving a 4.9% increase in online eCPM.\nCode is available at\nhttps://github.com/Chrissie-Law/Causal-Domain-Clustering-for-Multi-Domain-Recommendation", "AI": {"tldr": "\u63d0\u51fa\u4e86\u56e0\u679c\u57df\u805a\u7c7b\uff08CDC\uff09\u4ee5\u63d0\u9ad8\u8de8\u591a\u4e2a\u9886\u57df\u7684\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u663e\u7740\u7684\u57df\u95f4\u5dee\u5f02\uff0c\u5728\u7edf\u4e00\u6a21\u578b\u4e2d\u8bad\u7ec3\u6240\u6709\u57df\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u4e1a\u52a1\u903b\u8f91\u6216\u6570\u636e\u76f8\u4f3c\u6027\u7684\u57df\u5206\u7ec4\u65b9\u6cd5\uff0c\u901a\u5e38\u65e0\u6cd5\u6355\u6349\u5230\u6700\u4f73\u5206\u7ec4\u6240\u9700\u7684\u771f\u5b9e\u8f6c\u79fb\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86\u56e0\u679c\u57df\u805a\u7c7b\uff08CDC\uff09\u3002CDC\u4f7f\u7528\u4e24\u79cd\u4e0d\u540c\u7684\u6548\u5e94\u6765\u6a21\u62df\u5927\u91cf\u57df\u4e2d\u7684\u57df\u8f6c\u79fb\u6a21\u5f0f\uff1a\u7528\u4e8e\u6a21\u62df\u975e\u4ea4\u4e92\u57df\u8f6c\u79fb\u7684\u9694\u79bb\u57df\u4eb2\u548c\u529b\u77e9\u9635\uff0c\u4ee5\u53ca\u7528\u4e8e\u8003\u8651\u8054\u5408\u8bad\u7ec3\u4e0b\u52a8\u6001\u57df\u534f\u540c\u6216\u5e72\u6270\u7684\u6df7\u5408\u57df\u4eb2\u548c\u529b\u77e9\u9635\u3002\u4e3a\u4e86\u6574\u5408\u8fd9\u4e24\u79cd\u8f6c\u79fb\u6548\u5e94\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u56e0\u679c\u53d1\u73b0\u6765\u8ba1\u7b97\u4e00\u4e2a\u57fa\u4e8e\u5185\u805a\u529b\u7684\u7cfb\u6570\uff0c\u8be5\u7cfb\u6570\u81ea\u9002\u5e94\u5730\u5e73\u8861\u5b83\u4eec\u7684\u8d21\u732e\u3002\u4e00\u79cd\u534f\u540c\u4f18\u5316\u52a8\u6001\u805a\u7c7b\u7b97\u6cd5\u8fed\u4ee3\u5730\u4f18\u5316\u76ee\u6807\u57df\u805a\u7c7b\u548c\u6e90\u57df\u9009\u62e9\u4ee5\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "CDC\u663e\u8457\u63d0\u9ad8\u4e86\u516c\u5171\u6570\u636e\u96c6\u548c\u5de5\u4e1a\u73af\u5883\u4e2d50\u591a\u4e2a\u9886\u57df\u7684\u6027\u80fd\uff0c\u5728\u7ebfeCPM\u63d0\u9ad8\u4e864.9%\u3002", "conclusion": "CDC\u663e\u8457\u63d0\u9ad8\u4e86\u516c\u5171\u6570\u636e\u96c6\u548c\u5de5\u4e1a\u73af\u5883\u4e2d50\u591a\u4e2a\u9886\u57df\u7684\u6027\u80fd\uff0c\u5728\u7ebfeCPM\u63d0\u9ad8\u4e864.9%\u3002"}}
{"id": "2507.06393", "categories": ["cs.CL", "math.QA", "math.RA", "91F20, 18M60, 18M80, 16T05, 68Q70"], "pdf": "https://arxiv.org/pdf/2507.06393", "abs": "https://arxiv.org/abs/2507.06393", "authors": ["Matilde Marcolli", "Riny Huijbregts", "Richard K. Larson"], "title": "Hypermagmas and Colored Operads: Heads, Phases, and Theta Roles", "comment": "LaTeX, 48 pages", "summary": "We show that head functions on syntactic objects extend the magma structure\nto a hypermagma, with the c-command relation compatible with the magma\noperation and the m-command relation with the hypermagma. We then show that the\nstructure of head and complement and specifier, additional modifier positions,\nand the structure of phases in the Extended Projection can be formulated as a\nbud generating system of a colored operad, in a form similar to the structure\nof theta roles. We also show that, due to the special form of the colored\noperad generators, the filtering of freely generated syntactic objects by these\ncoloring rules can be equivalently formulated as a filtering in the course of\nstructure formation via a colored Merge, which can in turn be related to the\nhypermagma structure. The rules on movement by Internal Merge with respect to\nphases, the Extended Projection Principle, Empty Category Principle, and Phase\nImpenetrability Condition are all subsumed into the form of the colored operad\ngenerators. Movement compatibilities between the phase structure and the theta\nroles assignments can then be formulated in terms of the respective colored\noperads and a transduction of colored operads.", "AI": {"tldr": "This paper shows how syntactic structures can be formulated using colored operads and hypermagmas, unifying various linguistic principles and movement rules.", "motivation": "We show that head functions on syntactic objects extend the magma structure to a hypermagma, with the c-command relation compatible with the magma operation and the m-command relation with the hypermagma.", "method": "the structure of head and complement and specifier, additional modifier positions, and the structure of phases in the Extended Projection can be formulated as a bud generating system of a colored operad, in a form similar to the structure of theta roles.", "result": "the filtering of freely generated syntactic objects by these coloring rules can be equivalently formulated as a filtering in the course of structure formation via a colored Merge, which can in turn be related to the hypermagma structure. The rules on movement by Internal Merge with respect to phases, the Extended Projection Principle, Empty Category Principle, and Phase Impenetrability Condition are all subsumed into the form of the colored operad generators.", "conclusion": "Movement compatibilities between the phase structure and the theta roles assignments can then be formulated in terms of the respective colored operads and a transduction of colored operads."}}
{"id": "2507.06321", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06321", "abs": "https://arxiv.org/abs/2507.06321", "authors": ["Joon Tai Kim", "Tianle Chen", "Ziyu Dong", "Nishanth Kunchala", "Alexander Guller", "Daniel Ospina Acero", "Roger Williams", "Mrinal Kumar"], "title": "Centralized Copy-Paste: Enhanced Data Augmentation Strategy for Wildland Fire Semantic Segmentation", "comment": "21 pages, 5 figures, and under review for AIAA SciTech 2026", "summary": "Collecting and annotating images for the purpose of training segmentation\nmodels is often cost prohibitive. In the domain of wildland fire science, this\nchallenge is further compounded by the scarcity of reliable public datasets\nwith labeled ground truth. This paper presents the Centralized Copy-Paste Data\nAugmentation (CCPDA) method, for the purpose of assisting with the training of\ndeep-learning multiclass segmentation models, with special focus on improving\nsegmentation outcomes for the fire-class. CCPDA has three main steps: (i)\nidentify fire clusters in the source image, (ii) apply a centralization\ntechnique to focus on the core of the fire area, and (iii) paste the refined\nfire clusters onto a target image. This method increases dataset diversity\nwhile preserving the essential characteristics of the fire class. The\neffectiveness of this augmentation technique is demonstrated via numerical\nanalysis and comparison against various other augmentation methods using a\nweighted sum-based multi-objective optimization approach. This approach helps\nelevate segmentation performance metrics specific to the fire class, which\ncarries significantly more operational significance than other classes (fuel,\nash, or background). Numerical performance assessment validates the efficacy of\nthe presented CCPDA method in alleviating the difficulties associated with\nsmall, manually labeled training datasets. It also illustrates that CCPDA\noutperforms other augmentation strategies in the application scenario\nconsidered, particularly in improving fire-class segmentation performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u4e2d\u590d\u5236\u7c98\u8d34\u6570\u636e\u589e\u5f3a\uff08CCPDA\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u6df1\u5ea6\u5b66\u4e60\u591a\u7c7b\u5206\u5272\u6a21\u578b\u5728\u706b\u707e\u7c7b\u522b\u4e0a\u7684\u5206\u5272\u6548\u679c\uff0c\u901a\u8fc7\u5728\u6e90\u56fe\u50cf\u4e2d\u8bc6\u522b\u706b\u707e\u7c07\uff0c\u96c6\u4e2d\u5904\u7406\u540e\u7c98\u8d34\u5230\u76ee\u6807\u56fe\u50cf\u4e0a\uff0c\u7ecf\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u6709\u6548\u3002", "motivation": "\u6536\u96c6\u548c\u6807\u6ce8\u56fe\u50cf\u4ee5\u8bad\u7ec3\u5206\u5272\u6a21\u578b\u901a\u5e38\u6210\u672c\u8fc7\u9ad8\u3002\u5728\u8352\u5730\u706b\u707e\u79d1\u5b66\u9886\u57df\uff0c\u7f3a\u4e4f\u5e26\u6709\u6807\u8bb0\u7684\u53ef\u9760\u516c\u5171\u6570\u636e\u96c6\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u96c6\u4e2d\u590d\u5236\u7c98\u8d34\u6570\u636e\u589e\u5f3a\uff08CCPDA\uff09\u65b9\u6cd5\uff0c\u5305\u62ec\u8bc6\u522b\u706b\u707e\u7c07\uff0c\u5e94\u7528\u96c6\u4e2d\u6280\u672f\uff0c\u5e76\u5c06\u7cbe\u70bc\u7684\u706b\u707e\u7c07\u7c98\u8d34\u5230\u76ee\u6807\u56fe\u50cf\u4e0a\u3002", "result": "\u6570\u503c\u6027\u80fd\u8bc4\u4f30\u9a8c\u8bc1\u4e86CCPDA\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u5728\u63d0\u9ad8\u706b\u707e\u7c7b\u522b\u5206\u5272\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u589e\u5f3a\u7b56\u7565\u3002", "conclusion": "CCPDA\u65b9\u6cd5\u901a\u8fc7\u6570\u503c\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u5728\u7f13\u89e3\u5c0f\u89c4\u6a21\u624b\u52a8\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\u96c6\u76f8\u5173\u56f0\u96be\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e14\u5728\u6539\u8fdb\u706b\u707e\u7c7b\u522b\u5206\u5272\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u589e\u5f3a\u7b56\u7565\u3002"}}
{"id": "2507.06380", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06380", "abs": "https://arxiv.org/abs/2507.06380", "authors": ["Habibur Rahaman", "Atri Chatterjee", "Swarup Bhunia"], "title": "Secure and Storage-Efficient Deep Learning Models for Edge AI Using Automatic Weight Generation", "comment": "7 pages, 7 figures", "summary": "Complex neural networks require substantial memory to store a large number of\nsynaptic weights. This work introduces WINGs (Automatic Weight Generator for\nSecure and Storage-Efficient Deep Learning Models), a novel framework that\ndynamically generates layer weights in a fully connected neural network (FC)\nand compresses the weights in convolutional neural networks (CNNs) during\ninference, significantly reducing memory requirements without sacrificing\naccuracy. WINGs framework uses principal component analysis (PCA) for\ndimensionality reduction and lightweight support vector regression (SVR) models\nto predict layer weights in the FC networks, removing the need for storing\nfull-weight matrices and achieving substantial memory savings. It also\npreferentially compresses the weights in low-sensitivity layers of CNNs using\nPCA and SVR with sensitivity analysis. The sensitivity-aware design also offers\nan added level of security, as any bit-flip attack with weights in compressed\nlayers has an amplified and readily detectable effect on accuracy. WINGs\nachieves 53x compression for the FC layers and 28x for AlexNet with MNIST\ndataset, and 18x for Alexnet with CIFAR-10 dataset with 1-2% accuracy loss.\nThis significant reduction in memory results in higher throughput and lower\nenergy for DNN inference, making it attractive for resource-constrained edge\napplications.", "AI": {"tldr": "WINGs: a framework that dynamically generates layer weights in a fully connected neural network (FC) and compresses the weights in convolutional neural networks (CNNs) during inference, significantly reducing memory requirements without sacrificing accuracy.", "motivation": "Complex neural networks require substantial memory to store a large number of synaptic weights.", "method": "WINGs framework uses principal component analysis (PCA) for dimensionality reduction and lightweight support vector regression (SVR) models to predict layer weights in the FC networks, removing the need for storing full-weight matrices and achieving substantial memory savings. It also preferentially compresses the weights in low-sensitivity layers of CNNs using PCA and SVR with sensitivity analysis.", "result": "WINGs achieves 53x compression for the FC layers and 28x for AlexNet with MNIST dataset, and 18x for Alexnet with CIFAR-10 dataset with 1-2% accuracy loss.", "conclusion": "WINGs achieves 53x compression for the FC layers and 28x for AlexNet with MNIST dataset, and 18x for Alexnet with CIFAR-10 dataset with 1-2% accuracy loss. This significant reduction in memory results in higher throughput and lower energy for DNN inference, making it attractive for resource-constrained edge applications."}}
{"id": "2507.07064", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.07064", "abs": "https://arxiv.org/abs/2507.07064", "authors": ["Shanle Zheng", "Keqin Bao", "Jizhi Zhang", "Yang Zhang", "Fuli Feng", "Xiangnan He"], "title": "Boosting Parameter Efficiency in LLM-Based Recommendation through Sophisticated Pruning", "comment": null, "summary": "LLM-based recommender systems have made significant progress; however, the\ndeployment cost associated with the large parameter volume of LLMs still\nhinders their real-world applications. This work explores parameter pruning to\nimprove parameter efficiency while maintaining recommendation quality, thereby\nenabling easier deployment. Unlike existing approaches that focus primarily on\ninter-layer redundancy, we uncover intra-layer redundancy within components\nsuch as self-attention and MLP modules. Building on this analysis, we propose a\nmore fine-grained pruning approach that integrates both intra-layer and\nlayer-wise pruning. Specifically, we introduce a three-stage pruning strategy\nthat progressively prunes parameters at different levels and parts of the\nmodel, moving from intra-layer to layer-wise pruning, or from width to depth.\nEach stage also includes a performance restoration step using distillation\ntechniques, helping to strike a balance between performance and parameter\nefficiency. Empirical results demonstrate the effectiveness of our approach:\nacross three datasets, our models achieve an average of 88% of the original\nmodel's performance while pruning more than 95% of the non-embedding\nparameters. This underscores the potential of our method to significantly\nreduce resource requirements without greatly compromising recommendation\nquality. Our code will be available at: https://github.com/zheng-sl/PruneRec", "AI": {"tldr": "Proposes a fine-grained pruning approach that integrates both intra-layer and layer-wise pruning to improve parameter efficiency of LLM-based recommender systems.", "motivation": "Deployment cost associated with the large parameter volume of LLMs still hinders their real-world applications.", "method": "A three-stage pruning strategy that progressively prunes parameters at different levels and parts of the model, moving from intra-layer to layer-wise pruning, or from width to depth. Each stage also includes a performance restoration step using distillation techniques", "result": "Achieve an average of 88% of the original model's performance while pruning more than 95% of the non-embedding parameters across three datasets.", "conclusion": "The proposed pruning method significantly reduces resource requirements without greatly compromising recommendation quality, achieving an average of 88% of the original model's performance while pruning more than 95% of the non-embedding parameters."}}
{"id": "2507.06415", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06415", "abs": "https://arxiv.org/abs/2507.06415", "authors": ["Zeming Chen", "Angelika Romanou", "Gail Weiss", "Antoine Bosselut"], "title": "PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning", "comment": "10 pages, 7 figures", "summary": "Long-context reasoning requires accurately identifying relevant information\nin extensive, noisy input contexts. Previous research shows that using\ntest-time learning to encode context directly into model parameters can\neffectively enable reasoning over noisy information. However, meta-learning\nmethods for enabling test-time learning are prohibitively memory-intensive,\npreventing their application to long context settings. In this work, we propose\nPERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for\nlearning to encode long input contexts using gradient updates to a lightweight\nmodel adapter at test time. Specifically, PERK employs two nested optimization\nloops in a meta-training phase. The inner loop rapidly encodes contexts into a\nlow-rank adapter (LoRA) that serves as a parameter-efficient memory module for\nthe base model. Concurrently, the outer loop learns to use the updated adapter\nto accurately recall and reason over relevant information from the encoded long\ncontext. Our evaluations on several long-context reasoning tasks show that PERK\nsignificantly outperforms the standard prompt-based long-context baseline,\nachieving average absolute performance gains of up to 90% for smaller models\n(GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In\ngeneral, PERK is more robust to reasoning complexity, length extrapolation, and\nthe locations of relevant information in contexts. Finally, we show that while\nPERK is memory-intensive during training, it scales more efficiently at\ninference time than prompt-based long-context inference.", "AI": {"tldr": "PERK \u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e60\u4f7f\u7528\u68af\u5ea6\u66f4\u65b0\u5728\u6d4b\u8bd5\u65f6\u5c06\u957f\u8f93\u5165\u4e0a\u4e0b\u6587\u7f16\u7801\u5230\u8f7b\u91cf\u7ea7\u6a21\u578b\u9002\u914d\u5668\u3002", "motivation": "\u5148\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u4f7f\u7528\u6d4b\u8bd5\u65f6\u5b66\u4e60\u5c06\u4e0a\u4e0b\u6587\u76f4\u63a5\u7f16\u7801\u5230\u6a21\u578b\u53c2\u6570\u4e2d\u53ef\u4ee5\u6709\u6548\u5730\u5b9e\u73b0\u5bf9\u566a\u58f0\u4fe1\u606f\u7684\u63a8\u7406\u3002\u7136\u800c\uff0c\u7528\u4e8e\u542f\u7528\u6d4b\u8bd5\u65f6\u5b66\u4e60\u7684\u5143\u5b66\u4e60\u65b9\u6cd5\u5bf9\u5185\u5b58\u7684\u8981\u6c42\u8fc7\u9ad8\uff0c\u963b\u6b62\u4e86\u5b83\u4eec\u5728\u957f\u6587\u672c\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "PERK \u91c7\u7528\u5728\u5143\u8bad\u7ec3\u9636\u6bb5\u7684\u4e24\u4e2a\u5d4c\u5957\u4f18\u5316\u5faa\u73af\u3002\u5185\u90e8\u5faa\u73af\u5c06\u4e0a\u4e0b\u6587\u5feb\u901f\u7f16\u7801\u5230\u4f4e\u79e9\u9002\u914d\u5668 (LoRA) \u4e2d\uff0c\u8be5\u9002\u914d\u5668\u5145\u5f53\u57fa\u672c\u6a21\u578b\u7684\u53c2\u6570\u9ad8\u6548\u5185\u5b58\u6a21\u5757\u3002\u540c\u65f6\uff0c\u5916\u90e8\u5faa\u73af\u5b66\u4e60\u4f7f\u7528\u66f4\u65b0\u540e\u7684\u9002\u914d\u5668\u6765\u51c6\u786e\u5730\u56de\u5fc6\u548c\u63a8\u7406\u6765\u81ea\u7f16\u7801\u957f\u6587\u672c\u4e0a\u4e0b\u6587\u7684\u76f8\u5173\u4fe1\u606f\u3002", "result": "PERK \u5728\u591a\u4e2a\u957f\u6587\u672c\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u7684\u57fa\u4e8e prompt \u7684\u957f\u6587\u672c\u57fa\u7ebf\uff0c\u5bf9\u4e8e\u8f83\u5c0f\u7684\u6a21\u578b (GPT-2) \u5b9e\u73b0\u4e86\u9ad8\u8fbe 90% \u7684\u5e73\u5747\u7edd\u5bf9\u6027\u80fd\u63d0\u5347\uff0c\u5bf9\u4e8e\u6211\u4eec\u8bc4\u4f30\u7684\u6700\u5927\u7684\u6a21\u578b Qwen-2.5-0.5B \u5b9e\u73b0\u4e86\u9ad8\u8fbe 27% \u7684\u63d0\u5347\u3002PERK \u901a\u5e38\u5bf9\u63a8\u7406\u590d\u6742\u6027\u3001\u957f\u5ea6\u5916\u63a8\u548c\u4e0a\u4e0b\u6587\u4e2d\u76f8\u5173\u4fe1\u606f\u7684\u4f4d\u7f6e\u66f4\u5177\u9c81\u68d2\u6027\u3002\u867d\u7136 PERK \u5728\u8bad\u7ec3\u671f\u95f4\u9700\u8981\u5927\u91cf\u5185\u5b58\uff0c\u4f46\u5b83\u5728\u63a8\u7406\u65f6\u6bd4\u57fa\u4e8e prompt \u7684\u957f\u6587\u672c\u63a8\u7406\u66f4\u6709\u6548\u5730\u6269\u5c55\u3002", "conclusion": "PERK\u5728\u591a\u4e2a\u957f\u6587\u672c\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u7684\u57fa\u4e8e prompt \u7684\u957f\u6587\u672c\u57fa\u7ebf\uff0c\u5bf9\u4e8e\u8f83\u5c0f\u7684\u6a21\u578b (GPT-2) \u5b9e\u73b0\u4e86\u9ad8\u8fbe 90% \u7684\u5e73\u5747\u7edd\u5bf9\u6027\u80fd\u63d0\u5347\uff0c\u5bf9\u4e8e\u6211\u4eec\u8bc4\u4f30\u7684\u6700\u5927\u7684\u6a21\u578b Qwen-2.5-0.5B \u5b9e\u73b0\u4e86\u9ad8\u8fbe 27% \u7684\u63d0\u5347\u3002PERK \u901a\u5e38\u5bf9\u63a8\u7406\u590d\u6742\u6027\u3001\u957f\u5ea6\u5916\u63a8\u548c\u4e0a\u4e0b\u6587\u4e2d\u76f8\u5173\u4fe1\u606f\u7684\u4f4d\u7f6e\u66f4\u5177\u9c81\u68d2\u6027\u3002\u867d\u7136 PERK \u5728\u8bad\u7ec3\u671f\u95f4\u9700\u8981\u5927\u91cf\u5185\u5b58\uff0c\u4f46\u5b83\u5728\u63a8\u7406\u65f6\u6bd4\u57fa\u4e8e prompt \u7684\u957f\u6587\u672c\u63a8\u7406\u66f4\u6709\u6548\u5730\u6269\u5c55\u3002"}}
{"id": "2507.06332", "categories": ["cs.CV", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.06332", "abs": "https://arxiv.org/abs/2507.06332", "authors": ["Fuyuan Zhang", "Qichen Wang", "Jianjun Zhao"], "title": "AR2: Attention-Guided Repair for the Robustness of CNNs Against Common Corruptions", "comment": null, "summary": "Deep neural networks suffer from significant performance degradation when\nexposed to common corruptions such as noise, blur, weather, and digital\ndistortions, limiting their reliability in real-world applications. In this\npaper, we propose AR2 (Attention-Guided Repair for Robustness), a simple yet\neffective method to enhance the corruption robustness of pretrained CNNs. AR2\noperates by explicitly aligning the class activation maps (CAMs) between clean\nand corrupted images, encouraging the model to maintain consistent attention\neven under input perturbations. Our approach follows an iterative repair\nstrategy that alternates between CAM-guided refinement and standard\nfine-tuning, without requiring architectural changes. Extensive experiments\nshow that AR2 consistently outperforms existing state-of-the-art methods in\nrestoring robustness on standard corruption benchmarks (CIFAR-10-C, CIFAR-100-C\nand ImageNet-C), achieving a favorable balance between accuracy on clean data\nand corruption robustness. These results demonstrate that AR2 provides a robust\nand scalable solution for enhancing model reliability in real-world\nenvironments with diverse corruptions.", "AI": {"tldr": "AR2, a method to enhance the corruption robustness of pretrained CNNs, aligns class activation maps (CAMs) between clean and corrupted images.", "motivation": "Deep neural networks suffer from significant performance degradation when exposed to common corruptions such as noise, blur, weather, and digital distortions, limiting their reliability in real-world applications.", "method": "AR2 (Attention-Guided Repair for Robustness), operates by explicitly aligning the class activation maps (CAMs) between clean and corrupted images, encourages the model to maintain consistent attention even under input perturbations.  It follows an iterative repair strategy that alternates between CAM-guided refinement and standard fine-tuning, without requiring architectural changes.", "result": "AR2 consistently outperforms existing state-of-the-art methods in restoring robustness on standard corruption benchmarks (CIFAR-10-C, CIFAR-100-C and ImageNet-C), achieving a favorable balance between accuracy on clean data and corruption robustness.", "conclusion": "AR2 provides a robust and scalable solution for enhancing model reliability in real-world environments with diverse corruptions."}}
{"id": "2507.06381", "categories": ["cs.LG", "cs.AI", "math.DS", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.06381", "abs": "https://arxiv.org/abs/2507.06381", "authors": ["James Hazelden", "Laura Driscoll", "Eli Shlizerman", "Eric Shea-Brown"], "title": "KPFlow: An Operator Perspective on Dynamic Collapse Under Gradient Descent Training of Recurrent Networks", "comment": null, "summary": "Gradient Descent (GD) and its variants are the primary tool for enabling\nefficient training of recurrent dynamical systems such as Recurrent Neural\nNetworks (RNNs), Neural ODEs and Gated Recurrent units (GRUs). The dynamics\nthat are formed in these models exhibit features such as neural collapse and\nemergence of latent representations that may support the remarkable\ngeneralization properties of networks. In neuroscience, qualitative features of\nthese representations are used to compare learning in biological and artificial\nsystems. Despite recent progress, there remains a need for theoretical tools to\nrigorously understand the mechanisms shaping learned representations,\nespecially in finite, non-linear models. Here, we show that the gradient flow,\nwhich describes how the model's dynamics evolve over GD, can be decomposed into\na product that involves two operators: a Parameter Operator, K, and a\nLinearized Flow Propagator, P. K mirrors the Neural Tangent Kernel in\nfeed-forward neural networks, while P appears in Lyapunov stability and optimal\ncontrol theory. We demonstrate two applications of our decomposition. First, we\nshow how their interplay gives rise to low-dimensional latent dynamics under\nGD, and, specifically, how the collapse is a result of the network structure,\nover and above the nature of the underlying task. Second, for multi-task\ntraining, we show that the operators can be used to measure how objectives\nrelevant to individual sub-tasks align. We experimentally and theoretically\nvalidate these findings, providing an efficient Pytorch package, \\emph{KPFlow},\nimplementing robust analysis tools for general recurrent architectures. Taken\ntogether, our work moves towards building a next stage of understanding of GD\nlearning in non-linear recurrent models.", "AI": {"tldr": "The paper decomposes the gradient flow in recurrent models into two operators, K and P, and demonstrates their applications in understanding low-dimensional latent dynamics and multi-task training.", "motivation": "There remains a need for theoretical tools to rigorously understand the mechanisms shaping learned representations, especially in finite, non-linear models.", "method": "The gradient flow is decomposed into a Parameter Operator, K, and a Linearized Flow Propagator, P.", "result": "The interplay of K and P gives rise to low-dimensional latent dynamics under GD, and the operators can be used to measure how objectives relevant to individual sub-tasks align for multi-task training.", "conclusion": "This work moves towards building a next stage of understanding of GD learning in non-linear recurrent models."}}
{"id": "2507.06654", "categories": ["cs.CV", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.06654", "abs": "https://arxiv.org/abs/2507.06654", "authors": ["Naoya Sogi", "Takashi Shibata", "Makoto Terao", "Masanori Suganuma", "Takayuki Okatani"], "title": "MS-DPPs: Multi-Source Determinantal Point Processes for Contextual Diversity Refinement of Composite Attributes in Text to Image Retrieval", "comment": "IJCAI 2025. Code: https://github.com/NEC-N-SOGI/msdpp", "summary": "Result diversification (RD) is a crucial technique in Text-to-Image Retrieval\nfor enhancing the efficiency of a practical application. Conventional methods\nfocus solely on increasing the diversity metric of image appearances. However,\nthe diversity metric and its desired value vary depending on the application,\nwhich limits the applications of RD. This paper proposes a novel task called\nCDR-CA (Contextual Diversity Refinement of Composite Attributes). CDR-CA aims\nto refine the diversities of multiple attributes, according to the\napplication's context. To address this task, we propose Multi-Source DPPs, a\nsimple yet strong baseline that extends the Determinantal Point Process (DPP)\nto multi-sources. We model MS-DPP as a single DPP model with a unified\nsimilarity matrix based on a manifold representation. We also introduce Tangent\nNormalization to reflect contexts. Extensive experiments demonstrate the\neffectiveness of the proposed method. Our code is publicly available at\nhttps://github.com/NEC-N-SOGI/msdpp.", "AI": {"tldr": "This paper introduces CDR-CA, a novel task for refining diversities of multiple attributes in Text-to-Image Retrieval, and proposes Multi-Source DPPs with Tangent Normalization to address it.", "motivation": "Conventional result diversification methods in Text-to-Image Retrieval are limited because the diversity metric and its desired value vary depending on the application.", "method": "The paper proposes Multi-Source DPPs, extending the Determinantal Point Process (DPP) to multi-sources, modeled as a single DPP with a unified similarity matrix based on manifold representation. Tangent Normalization is introduced to reflect contexts.", "result": "Extensive experiments demonstrate the effectiveness of the proposed method.", "conclusion": "The proposed Multi-Source DPPs method with Tangent Normalization effectively addresses the CDR-CA task, as demonstrated by extensive experiments."}}
{"id": "2507.06419", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06419", "abs": "https://arxiv.org/abs/2507.06419", "authors": ["Pankayaraj Pathmanathan", "Furong Huang"], "title": "Reward Models Can Improve Themselves: Reward-Guided Adversarial Failure Mode Discovery for Robust Reward Modeling", "comment": null, "summary": "Reward modeling (RM), which captures human preferences to align large\nlanguage models (LLMs), is increasingly employed in tasks such as model\nfinetuning, response filtering, and ranking. However, due to the inherent\ncomplexity of human preferences and the limited coverage of available datasets,\nreward models often fail under distributional shifts or adversarial\nperturbations. Existing approaches for identifying such failure modes typically\nrely on prior knowledge about preference distributions or failure attributes,\nlimiting their practicality in real-world settings where such information is\nunavailable. In this work, we propose a tractable, preference-distribution\nagnostic method for discovering reward model failure modes via reward guided\ncontrolled decoding. Building on this, we introduce REFORM, a self-improving\nreward modeling framework that enhances robustness by using the reward model\nitself to guide the generation of falsely scored responses. These adversarial\nexamples are then used to augment the training data and patch the reward\nmodel's misaligned behavior. We evaluate REFORM on two widely used preference\ndatasets Anthropic Helpful Harmless (HH) and PKU Beavertails and demonstrate\nthat it significantly improves robustness without sacrificing reward quality.\nNotably, REFORM preserves performance both in direct evaluation and in\ndownstream policy training, and further improves alignment quality by removing\nspurious correlations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5956\u52b1\u6a21\u578bREFORM\uff0c\u901a\u8fc7\u81ea\u6211\u6539\u8fdb\u6765\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5956\u52b1\u5efa\u6a21\uff08RM\uff09\u88ab\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u6a21\u578b\u5fae\u8c03\u3001\u54cd\u5e94\u8fc7\u6ee4\u548c\u6392\u5e8f\u7b49\u4efb\u52a1\u4e2d\uff0c\u4ee5\u6355\u6349\u4eba\u7c7b\u504f\u597d\u6765\u5bf9\u9f50\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4eba\u7c7b\u504f\u597d\u7684\u5185\u5728\u590d\u6742\u6027\u548c\u53ef\u7528\u6570\u636e\u96c6\u7684\u6709\u9650\u8986\u76d6\uff0c\u5956\u52b1\u6a21\u578b\u7ecf\u5e38\u5728\u5206\u5e03\u504f\u79fb\u6216\u5bf9\u6297\u6027\u6270\u52a8\u4e0b\u5931\u8d25\u3002\u73b0\u6709\u7684\u8bc6\u522b\u8fd9\u79cd\u5931\u8d25\u6a21\u5f0f\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u5173\u4e8e\u504f\u597d\u5206\u5e03\u6216\u5931\u8d25\u5c5e\u6027\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u65e0\u6cd5\u83b7\u5f97\u6b64\u7c7b\u4fe1\u606f\u7684\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5904\u7406\u7684\u3001\u504f\u597d\u5206\u5e03\u4e0d\u53ef\u77e5\u7684\u3001\u901a\u8fc7\u5956\u52b1\u5f15\u5bfc\u7684\u63a7\u5236\u89e3\u7801\u6765\u53d1\u73b0\u5956\u52b1\u6a21\u578b\u5931\u8d25\u6a21\u5f0f\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5f15\u5165\u4e86REFORM\uff0c\u8fd9\u662f\u4e00\u4e2a\u81ea\u6211\u6539\u8fdb\u7684\u5956\u52b1\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u5956\u52b1\u6a21\u578b\u672c\u8eab\u6765\u6307\u5bfc\u9519\u8bef\u8bc4\u5206\u7684\u54cd\u5e94\u751f\u6210\uff0c\u4ece\u800c\u589e\u5f3a\u9c81\u68d2\u6027\u3002\u8fd9\u4e9b\u5bf9\u6297\u6027\u793a\u4f8b\u7136\u540e\u7528\u4e8e\u6269\u5145\u8bad\u7ec3\u6570\u636e\u5e76\u4fee\u8865\u5956\u52b1\u6a21\u578b\u7684\u9519\u4f4d\u884c\u4e3a\u3002", "result": "\u5728\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u504f\u597d\u6570\u636e\u96c6Anthropic Helpful Harmless (HH)\u548cPKU Beavertails\u4e0a\u8bc4\u4f30\u4e86REFORM\uff0c\u5e76\u8bc1\u660e\u4e86\u5b83\u5728\u4e0d\u727a\u7272\u5956\u52b1\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u3002", "conclusion": "REFORM\u663e\u8457\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u6ca1\u6709\u727a\u7272\u5956\u52b1\u8d28\u91cf\uff0c\u5728\u76f4\u63a5\u8bc4\u4f30\u548c\u4e0b\u6e38\u7b56\u7565\u8bad\u7ec3\u4e2d\u4fdd\u6301\u4e86\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u6d88\u9664\u865a\u5047\u76f8\u5173\u6027\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u5bf9\u9f50\u8d28\u91cf\u3002"}}
{"id": "2507.06400", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06400", "abs": "https://arxiv.org/abs/2507.06400", "authors": ["Weiran Li", "Yeqiang Liu", "Qiannan Guo", "Yijie Wei", "Hwa Liang Leo", "Zhenbo Li"], "title": "When Trackers Date Fish: A Benchmark and Framework for Underwater Multiple Fish Tracking", "comment": null, "summary": "Multiple object tracking (MOT) technology has made significant progress in\nterrestrial applications, but underwater tracking scenarios remain\nunderexplored despite their importance to marine ecology and aquaculture. We\npresent Multiple Fish Tracking Dataset 2025 (MFT25), the first comprehensive\ndataset specifically designed for underwater multiple fish tracking, featuring\n15 diverse video sequences with 408,578 meticulously annotated bounding boxes\nacross 48,066 frames. Our dataset captures various underwater environments,\nfish species, and challenging conditions including occlusions, similar\nappearances, and erratic motion patterns. Additionally, we introduce\nScale-aware and Unscented Tracker (SU-T), a specialized tracking framework\nfeaturing an Unscented Kalman Filter (UKF) optimized for non-linear fish\nswimming patterns and a novel Fish-Intersection-over-Union (FishIoU) matching\nthat accounts for the unique morphological characteristics of aquatic species.\nExtensive experiments demonstrate that our SU-T baseline achieves\nstate-of-the-art performance on MFT25, with 34.1 HOTA and 44.6 IDF1, while\nrevealing fundamental differences between fish tracking and terrestrial object\ntracking scenarios. MFT25 establishes a robust foundation for advancing\nresearch in underwater tracking systems with important applications in marine\nbiology, aquaculture monitoring, and ecological conservation. The dataset and\ncodes are released at https://vranlee.github.io/SU-T/.", "AI": {"tldr": "Introduce MFT25, a dataset for underwater multiple fish tracking, and SU-T, a specialized tracking framework, which achieves state-of-the-art performance on MFT25.", "motivation": "underwater tracking scenarios remain underexplored despite their importance to marine ecology and aquaculture", "method": "Scale-aware and Unscented Tracker (SU-T), a specialized tracking framework featuring an Unscented Kalman Filter (UKF) optimized for non-linear fish swimming patterns and a novel Fish-Intersection-over-Union (FishIoU) matching", "result": "SU-T baseline achieves state-of-the-art performance on MFT25, with 34.1 HOTA and 44.6 IDF1", "conclusion": "MFT25 establishes a robust foundation for advancing research in underwater tracking systems with important applications in marine biology, aquaculture monitoring, and ecological conservation."}}
{"id": "2507.06402", "categories": ["cs.LG", "cs.CR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.06402", "abs": "https://arxiv.org/abs/2507.06402", "authors": ["Siddhant Deshpande", "Yalemzerf Getnet", "Waltenegus Dargie"], "title": "Detection of Intelligent Tampering in Wireless Electrocardiogram Signals Using Hybrid Machine Learning", "comment": null, "summary": "With the proliferation of wireless electrocardiogram (ECG) systems for health\nmonitoring and authentication, protecting signal integrity against tampering is\nbecoming increasingly important. This paper analyzes the performance of CNN,\nResNet, and hybrid Transformer-CNN models for tamper detection. It also\nevaluates the performance of a Siamese network for ECG based identity\nverification. Six tampering strategies, including structured segment\nsubstitutions and random insertions, are emulated to mimic real world attacks.\nThe one-dimensional ECG signals are transformed into a two dimensional\nrepresentation in the time frequency domain using the continuous wavelet\ntransform (CWT). The models are trained and evaluated using ECG data from 54\nsubjects recorded in four sessions 2019 to 2025 outside of clinical settings\nwhile the subjects performed seven different daily activities. Experimental\nresults show that in highly fragmented manipulation scenarios, CNN,\nFeatCNN-TranCNN, FeatCNN-Tran and ResNet models achieved an accuracy exceeding\n99.5 percent . Similarly, for subtle manipulations (for example, 50 percent\nfrom A and 50 percent from B and, 75 percent from A and 25 percent from B\nsubstitutions) our FeatCNN-TranCNN model demonstrated consistently reliable\nperformance, achieving an average accuracy of 98 percent . For identity\nverification, the pure Transformer-Siamese network achieved an average accuracy\nof 98.30 percent . In contrast, the hybrid CNN-Transformer Siamese model\ndelivered perfect verification performance with 100 percent accuracy.", "AI": {"tldr": "This paper explores deep learning models for detecting tampering and verifying identity using ECG signals, achieving high accuracy with CNN, ResNet, and Transformer-based models.", "motivation": "Protecting signal integrity against tampering is becoming increasingly important with the proliferation of wireless electrocardiogram (ECG) systems.", "method": "Uses CNN, ResNet, hybrid Transformer-CNN models for tamper detection and Siamese network for ECG based identity verification. Emulates six tampering strategies and transforms 1D ECG signals into 2D representation using continuous wavelet transform (CWT).", "result": "CNN, FeatCNN-TranCNN, FeatCNN-Tran and ResNet models achieved an accuracy exceeding 99.5 percent in highly fragmented manipulation scenarios. FeatCNN-TranCNN model achieved an average accuracy of 98 percent for subtle manipulations. The hybrid CNN-Transformer Siamese model delivered perfect verification performance with 100 percent accuracy.", "conclusion": "CNN, ResNet, and hybrid Transformer-CNN models achieve high accuracy in tamper detection, while hybrid CNN-Transformer Siamese network achieves perfect accuracy in identity verification."}}
{"id": "2507.06715", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.06715", "abs": "https://arxiv.org/abs/2507.06715", "authors": ["Garapati Keerthana", "Manik Gupta"], "title": "CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and Context Aware Text Generation with LLMs", "comment": "12 pages, 4 figures", "summary": "Large language models (LLMs), including zero-shot and few-shot paradigms,\nhave shown promising capabilities in clinical text generation. However,\nreal-world applications face two key challenges: (1) patient data is highly\nunstructured, heterogeneous, and scattered across multiple note types and (2)\nclinical notes are often long and semantically dense, making naive prompting\ninfeasible due to context length constraints and the risk of omitting\nclinically relevant information.\n  We introduce CLI-RAG (Clinically Informed Retrieval-Augmented Generation), a\ndomain-specific framework for structured and clinically grounded text\ngeneration using LLMs. It incorporates a novel hierarchical chunking strategy\nthat respects clinical document structure and introduces a task-specific\ndual-stage retrieval mechanism. The global stage identifies relevant note types\nusing evidence-based queries, while the local stage extracts high-value content\nwithin those notes creating relevance at both document and section levels.\n  We apply the system to generate structured progress notes for individual\nhospital visits using 15 clinical note types from the MIMIC-III dataset.\nExperiments show that it preserves temporal and semantic alignment across\nvisits, achieving an average alignment score of 87.7%, surpassing the 80.7%\nbaseline from real clinician-authored notes. The generated outputs also\ndemonstrate high consistency across LLMs, reinforcing deterministic behavior\nessential for reproducibility, reliability, and clinical trust.", "AI": {"tldr": "CLI-RAG, a new framework for clinical text generation, addresses challenges of unstructured data and long notes by using hierarchical chunking and dual-stage retrieval, achieving high accuracy and consistency in generating structured progress notes.", "motivation": "Real-world applications of LLMs in clinical text generation face challenges due to unstructured, heterogeneous, and scattered patient data, as well as long and semantically dense clinical notes.", "method": "The study introduces CLI-RAG (Clinically Informed Retrieval-Augmented Generation), a domain-specific framework that incorporates a novel hierarchical chunking strategy and a task-specific dual-stage retrieval mechanism.", "result": "CLI-RAG achieves an 87.7% alignment score, surpassing the 80.7% baseline from real clinician-authored notes, and demonstrates high consistency across LLMs.", "conclusion": "The study shows that CLI-RAG preserves temporal and semantic alignment across visits, achieving an average alignment score of 87.7%, surpassing the 80.7% baseline from real clinician-authored notes. The generated outputs also demonstrate high consistency across LLMs."}}
{"id": "2507.06427", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06427", "abs": "https://arxiv.org/abs/2507.06427", "authors": ["Shun Wang", "Tyler Loakman", "Youbo Lei", "Yi Liu", "Bohao Yang", "Yuting Zhao", "Dong Yang", "Chenghua Lin"], "title": "Exploring Task Performance with Interpretable Models via Sparse Auto-Encoders", "comment": null, "summary": "Large Language Models (LLMs) are traditionally viewed as black-box\nalgorithms, therefore reducing trustworthiness and obscuring potential\napproaches to increasing performance on downstream tasks. In this work, we\napply an effective LLM decomposition method using a dictionary-learning\napproach with sparse autoencoders. This helps extract monosemantic features\nfrom polysemantic LLM neurons. Remarkably, our work identifies model-internal\nmisunderstanding, allowing the automatic reformulation of the prompts with\nadditional annotations to improve the interpretation by LLMs. Moreover, this\napproach demonstrates a significant performance improvement in downstream\ntasks, such as mathematical reasoning and metaphor detection.", "AI": {"tldr": "Apply an effective LLM decomposition method using a dictionary-learning approach with sparse autoencoders to improve the interpretation by LLMs and identifies model-internal misunderstanding", "motivation": "LLMs are traditionally viewed as black-box algorithms, therefore reducing trustworthiness and obscuring potential approaches to increasing performance on downstream tasks.", "method": "an effective LLM decomposition method using a dictionary-learning approach with sparse autoencoders", "result": "identifies model-internal misunderstanding, allowing the automatic reformulation of the prompts with additional annotations to improve the interpretation by LLMs", "conclusion": "This approach demonstrates a significant performance improvement in downstream tasks, such as mathematical reasoning and metaphor detection."}}
{"id": "2507.06405", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06405", "abs": "https://arxiv.org/abs/2507.06405", "authors": ["Lala Shakti Swarup Ray", "Mengxi Liu", "Deepika Gurung", "Bo Zhou", "Sungho Suh", "Paul Lukowicz"], "title": "SImpHAR: Advancing impedance-based human activity recognition using 3D simulation and text-to-motion models", "comment": null, "summary": "Human Activity Recognition (HAR) with wearable sensors is essential for\napplications in healthcare, fitness, and human-computer interaction.\nBio-impedance sensing offers unique advantages for fine-grained motion capture\nbut remains underutilized due to the scarcity of labeled data. We introduce\nSImpHAR, a novel framework addressing this limitation through two core\ncontributions. First, we propose a simulation pipeline that generates realistic\nbio-impedance signals from 3D human meshes using shortest-path estimation,\nsoft-body physics, and text-to-motion generation serving as a digital twin for\ndata augmentation. Second, we design a two-stage training strategy with\ndecoupled approach that enables broader activity coverage without requiring\nlabel-aligned synthetic data. We evaluate SImpHAR on our collected ImpAct\ndataset and two public benchmarks, showing consistent improvements over\nstate-of-the-art methods, with gains of up to 22.3% and 21.8%, in terms of\naccuracy and macro F1 score, respectively. Our results highlight the promise of\nsimulation-driven augmentation and modular training for impedance-based HAR.", "AI": {"tldr": "SImpHAR, a novel framework that addresses the limitation of scarcity of labeled data in HAR with wearable sensors, showing consistent improvements over state-of-the-art methods.", "motivation": "Bio-impedance sensing offers unique advantages for fine-grained motion capture but remains underutilized due to the scarcity of labeled data", "method": "a simulation pipeline that generates realistic bio-impedance signals from 3D human meshes using shortest-path estimation, soft-body physics, and text-to-motion generation and a two-stage training strategy with decoupled approach", "result": "improvements over state-of-the-art methods, with gains of up to 22.3% and 21.8%, in terms of accuracy and macro F1 score, respectively", "conclusion": "simulation-driven augmentation and modular training hold promise for impedance-based HAR"}}
{"id": "2507.06432", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06432", "abs": "https://arxiv.org/abs/2507.06432", "authors": ["Mingcheng Zhu", "Yu Liu", "Zhiyao Luo", "Tingting Zhu"], "title": "Bridging Data Gaps of Rare Conditions in ICU: A Multi-Disease Adaptation Approach for Clinical Prediction", "comment": null, "summary": "Artificial Intelligence has revolutionised critical care for common\nconditions. Yet, rare conditions in the intensive care unit (ICU), including\nrecognised rare diseases and low-prevalence conditions in the ICU, remain\nunderserved due to data scarcity and intra-condition heterogeneity. To bridge\nsuch gaps, we developed KnowRare, a domain adaptation-based deep learning\nframework for predicting clinical outcomes for rare conditions in the ICU.\nKnowRare mitigates data scarcity by initially learning condition-agnostic\nrepresentations from diverse electronic health records through self-supervised\npre-training. It addresses intra-condition heterogeneity by selectively\nadapting knowledge from clinically similar conditions with a developed\ncondition knowledge graph. Evaluated on two ICU datasets across five clinical\nprediction tasks (90-day mortality, 30-day readmission, ICU mortality,\nremaining length of stay, and phenotyping), KnowRare consistently outperformed\nexisting state-of-the-art models. Additionally, KnowRare demonstrated superior\npredictive performance compared to established ICU scoring systems, including\nAPACHE IV and IV-a. Case studies further demonstrated KnowRare's flexibility in\nadapting its parameters to accommodate dataset-specific and task-specific\ncharacteristics, its generalisation to common conditions under limited data\nscenarios, and its rationality in selecting source conditions. These findings\nhighlight KnowRare's potential as a robust and practical solution for\nsupporting clinical decision-making and improving care for rare conditions in\nthe ICU.", "AI": {"tldr": "KnowRare\u662f\u4e00\u79cd\u57fa\u4e8e\u9886\u57df\u81ea\u9002\u5e94\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4bICU\u4e2d\u7f55\u89c1\u75be\u75c5\u7684\u4e34\u5e8a\u7ed3\u679c\u3002", "motivation": "\u7531\u4e8e\u6570\u636e\u7a00\u7f3a\u548c\u6761\u4ef6\u5185\u90e8\u5f02\u8d28\u6027\uff0cICU\u4e2d\u5305\u62ec\u516c\u8ba4\u7684\u7f55\u89c1\u75be\u75c5\u548c\u4f4e\u60a3\u75c5\u7387\u75be\u75c5\u5728\u5185\u7684\u7f55\u89c1\u75be\u75c5\u4ecd\u7136\u670d\u52a1\u4e0d\u8db3\u3002", "method": "\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u9886\u57df\u81ea\u9002\u5e94\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6KnowRare\uff0c\u7528\u4e8e\u9884\u6d4bICU\u4e2d\u7f55\u89c1\u75be\u75c5\u7684\u4e34\u5e8a\u7ed3\u679c\u3002", "result": "\u5728\u4e24\u4e2aICU\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u4e94\u4e2a\u4e34\u5e8a\u9884\u6d4b\u4efb\u52a1\uff0cKnowRare\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u6a21\u578b\u3002\u4e0e\u5305\u62ecAPACHE IV\u548cIV-a\u5728\u5185\u7684\u5df2\u5efa\u7acb\u7684ICU\u8bc4\u5206\u7cfb\u7edf\u76f8\u6bd4\uff0cKnowRare\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "KnowRare\u662f\u4e00\u79cd\u5f3a\u5927\u800c\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6f5c\u529b\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u5e76\u6539\u5584ICU\u4e2d\u7f55\u89c1\u75be\u75c5\u7684\u62a4\u7406\u3002"}}
{"id": "2507.06838", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.06838", "abs": "https://arxiv.org/abs/2507.06838", "authors": ["Dahyun Lee", "Yongrae Jo", "Haeju Park", "Moontae Lee"], "title": "Shifting from Ranking to Set Selection for Retrieval Augmented Generation", "comment": "Accepted to ACL 2025 Oral", "summary": "Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved\npassages are not only individually relevant but also collectively form a\ncomprehensive set. Existing approaches primarily rerank top-k passages based on\ntheir individual relevance, often failing to meet the information needs of\ncomplex queries in multi-hop question answering. In this work, we propose a\nset-wise passage selection approach and introduce SETR, which explicitly\nidentifies the information requirements of a query through Chain-of-Thought\nreasoning and selects an optimal set of passages that collectively satisfy\nthose requirements. Experiments on multi-hop RAG benchmarks show that SETR\noutperforms both proprietary LLM-based rerankers and open-source baselines in\nterms of answer correctness and retrieval quality, providing an effective and\nefficient alternative to traditional rerankers in RAG systems. The code is\navailable at https://github.com/LGAI-Research/SetR", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a SETR \u7684\u96c6\u5408\u5f0f\u6bb5\u843d\u9009\u62e9\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528 Chain-of-Thought \u63a8\u7406\u6765\u8bc6\u522b\u67e5\u8be2\u7684\u4fe1\u606f\u9700\u6c42\uff0c\u5e76\u9009\u62e9\u6700\u4f73\u7684\u6bb5\u843d\u96c6\u5408\u3002", "motivation": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG) \u4e2d\u7684\u68c0\u7d22\u5fc5\u987b\u786e\u4fdd\u68c0\u7d22\u5230\u7684\u6bb5\u843d\u4e0d\u4ec5\u5728\u4e2a\u4f53\u4e0a\u76f8\u5173\uff0c\u800c\u4e14\u5728\u603b\u4f53\u4e0a\u5f62\u6210\u4e00\u4e2a\u5168\u9762\u7684\u96c6\u5408\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u6839\u636e top-k \u6bb5\u843d\u5728\u4e2a\u4f53\u4e0a\u7684\u76f8\u5173\u6027\u8fdb\u884c\u91cd\u6392\u5e8f\uff0c\u901a\u5e38\u65e0\u6cd5\u6ee1\u8db3\u591a\u8df3\u95ee\u7b54\u4e2d\u590d\u6742\u67e5\u8be2\u7684\u4fe1\u606f\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u5408\u5f0f\u6bb5\u843d\u9009\u62e9\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86 SETR\uff0c\u5b83\u901a\u8fc7 Chain-of-Thought \u63a8\u7406\u663e\u5f0f\u5730\u8bc6\u522b\u67e5\u8be2\u7684\u4fe1\u606f\u9700\u6c42\uff0c\u5e76\u9009\u62e9\u4e00\u4e2a\u80fd\u591f\u5171\u540c\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u7684\u6700\u4f73\u6bb5\u843d\u96c6\u5408\u3002", "result": "\u5728\u591a\u8df3 RAG \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSETR \u5728\u7b54\u6848\u6b63\u786e\u6027\u548c\u68c0\u7d22\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u4e13\u6709\u7684\u57fa\u4e8e LLM \u7684\u91cd\u6392\u5e8f\u5668\u548c\u5f00\u6e90\u57fa\u7ebf\u3002", "conclusion": "SETR \u5728\u591a\u8df3 RAG \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u7b54\u6848\u6b63\u786e\u6027\u548c\u68c0\u7d22\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u4e13\u6709\u7684\u57fa\u4e8e LLM \u7684\u91cd\u6392\u5e8f\u5668\u548c\u5f00\u6e90\u57fa\u7ebf\uff0c\u4e3a RAG \u7cfb\u7edf\u4e2d\u4f20\u7edf\u7684\u91cd\u6392\u5e8f\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.06435", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06435", "abs": "https://arxiv.org/abs/2507.06435", "authors": ["Rafiu Adekoya Badekale", "Adewale Akinfaderin"], "title": "Temporal Analysis of Climate Policy Discourse: Insights from Dynamic Embedded Topic Modeling", "comment": "10 pages, 7 figures. Code and data available at\n  https://github.com/AdeTheBade/TACPD.git", "summary": "Understanding how policy language evolves over time is critical for assessing\nglobal responses to complex challenges such as climate change. Temporal\nanalysis helps stakeholders, including policymakers and researchers, to\nevaluate past priorities, identify emerging themes, design governance\nstrategies, and develop mitigation measures. Traditional approaches, such as\nmanual thematic coding, are time-consuming and limited in capturing the\ncomplex, interconnected nature of global policy discourse. With the increasing\nrelevance of unsupervised machine learning, these limitations can be addressed,\nparticularly under high-volume, complex, and high-dimensional data conditions.\nIn this work, we explore a novel approach that applies the dynamic embedded\ntopic model (DETM) to analyze the evolution of global climate policy discourse.\nA probabilistic model designed to capture the temporal dynamics of topics over\ntime. We collected a corpus of United Nations Framework Convention on Climate\nChange (UNFCCC) policy decisions from 1995 to 2023, excluding 2020 due to the\npostponement of COP26 as a result of the COVID-19 pandemic. The model reveals\nshifts from early emphases on greenhouse gases and international conventions to\nrecent focuses on implementation, technical collaboration, capacity building,\nfinance, and global agreements. Section 3 presents the modeling pipeline,\nincluding preprocessing, model training, and visualization of temporal word\ndistributions. Our results show that DETM is a scalable and effective tool for\nanalyzing the evolution of global policy discourse. Section 4 discusses the\nimplications of these findings and we concluded with future directions and\nrefinements to extend this approach to other policy domains.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u52a8\u6001\u5d4c\u5165\u4e3b\u9898\u6a21\u578b (DETM) \u5206\u6790\u4e86 1995 \u5e74\u81f3 2023 \u5e74\u7684\u8054\u5408\u56fd\u6c14\u5019\u53d8\u5316\u6846\u67b6\u516c\u7ea6 (UNFCCC) \u653f\u7b56\u51b3\u7b56\uff0c\u63ed\u793a\u4e86\u5168\u7403\u6c14\u5019\u653f\u7b56\u8ba8\u8bba\u968f\u65f6\u95f4\u7684\u6f14\u53d8\u3002", "motivation": "\u4e86\u89e3\u653f\u7b56\u8bed\u8a00\u968f\u65f6\u95f4\u7684\u53d8\u5316\u5bf9\u4e8e\u8bc4\u4f30\u5168\u7403\u5e94\u5bf9\u6c14\u5019\u53d8\u5316\u7b49\u590d\u6742\u6311\u6218\u81f3\u5173\u91cd\u8981\u3002\u65f6\u95f4\u5206\u6790\u6709\u52a9\u4e8e\u5229\u76ca\u76f8\u5173\u8005\u8bc4\u4f30\u8fc7\u53bb\u7684\u4f18\u5148\u4e8b\u9879\u3001\u8bc6\u522b\u65b0\u5174\u4e3b\u9898\u3001\u8bbe\u8ba1\u6cbb\u7406\u6218\u7565\u4ee5\u53ca\u5236\u5b9a\u7f13\u89e3\u63aa\u65bd\u3002", "method": "\u5e94\u7528\u52a8\u6001\u5d4c\u5165\u4e3b\u9898\u6a21\u578b (DETM) \u6765\u5206\u6790\u5168\u7403\u6c14\u5019\u653f\u7b56\u8ba8\u8bba\u7684\u6f14\u53d8\u3002", "result": "\u8be5\u6a21\u578b\u63ed\u793a\u4e86\u4ece\u65e9\u671f\u5f3a\u8c03\u6e29\u5ba4\u6c14\u4f53\u548c\u56fd\u9645\u516c\u7ea6\u5230\u6700\u8fd1\u5173\u6ce8\u5b9e\u65bd\u3001\u6280\u672f\u5408\u4f5c\u3001\u80fd\u529b\u5efa\u8bbe\u3001\u91d1\u878d\u548c\u5168\u7403\u534f\u8bae\u7684\u8f6c\u53d8\u3002", "conclusion": "DETM \u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u5206\u6790\u5168\u7403\u653f\u7b56\u8ba8\u8bba\u7684\u6f14\u53d8\u3002"}}
{"id": "2507.06411", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06411", "abs": "https://arxiv.org/abs/2507.06411", "authors": ["Hayat Ullah", "Arslan Munir", "Oliver Nina"], "title": "Hierarchical Multi-Stage Transformer Architecture for Context-Aware Temporal Action Localization", "comment": "17 pages, 6 figures,", "summary": "Inspired by the recent success of transformers and multi-stage architectures\nin video recognition and object detection domains. We thoroughly explore the\nrich spatio-temporal properties of transformers within a multi-stage\narchitecture paradigm for the temporal action localization (TAL) task. This\nexploration led to the development of a hierarchical multi-stage transformer\narchitecture called PCL-Former, where each subtask is handled by a dedicated\ntransformer module with a specialized loss function. Specifically, the\nProposal-Former identifies candidate segments in an untrimmed video that may\ncontain actions, the Classification-Former classifies the action categories\nwithin those segments, and the Localization-Former precisely predicts the\ntemporal boundaries (i.e., start and end) of the action instances. To evaluate\nthe performance of our method, we have conducted extensive experiments on three\nchallenging benchmark datasets: THUMOS-14, ActivityNet-1.3, and HACS Segments.\nWe also conducted detailed ablation experiments to assess the impact of each\nindividual module of our PCL-Former. The obtained quantitative results validate\nthe effectiveness of the proposed PCL-Former, outperforming state-of-the-art\nTAL approaches by 2.8%, 1.2%, and 4.8% on THUMOS14, ActivityNet-1.3, and HACS\ndatasets, respectively.", "AI": {"tldr": "PCL-Former\u662f\u4e00\u79cd\u7528\u4e8e\u65f6\u95f4\u52a8\u4f5c\u5b9a\u4f4d\u7684\u5206\u5c42\u591a\u9636\u6bb5transformer\u67b6\u6784\uff0c\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u3002", "motivation": "\u53d7transformers\u548c\u591a\u9636\u6bb5\u67b6\u6784\u5728\u89c6\u9891\u8bc6\u522b\u548c\u76ee\u6807\u68c0\u6d4b\u9886\u57df\u7684\u6210\u529f\u542f\u53d1\uff0c\u6df1\u5165\u63a2\u7d22\u4e86transformer\u5728\u591a\u9636\u6bb5\u67b6\u6784\u8303\u4f8b\u4e2d\u7528\u4e8e\u65f6\u95f4\u52a8\u4f5c\u5b9a\u4f4d (TAL) \u4efb\u52a1\u7684\u4e30\u5bcc\u65f6\u7a7a\u5c5e\u6027\u3002", "method": " hierarchical multi-stage transformer architecture called PCL-Former", "result": "PCL-Former\u5728THUMOS14\u3001ActivityNet-1.3\u548cHACS\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8d85\u51fa\u5f53\u524d\u6700\u4f18\u7684TAL\u65b9\u6cd52.8%\u30011.2%\u548c4.8%\u3002", "conclusion": "PCL-Former\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u7684TAL\u65b9\u6cd5\uff0c\u5206\u522b\u8d85\u51fa2.8%\u30011.2%\u548c4.8%\u3002"}}
{"id": "2507.06433", "categories": ["cs.LG", "eess.SP", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.06433", "abs": "https://arxiv.org/abs/2507.06433", "authors": ["Niloy Sikder", "Paul Zerr", "Mahdad Jafarzadeh Esfahani", "Martin Dresler", "Matthias Krauledat"], "title": "eegFloss: A Python package for refining sleep EEG recordings using machine learning models", "comment": "The eegFloss package is available under the MIT License at\n  https://github.com/Niloy333/eegFloss", "summary": "Electroencephalography (EEG) allows monitoring of brain activity, providing\ninsights into the functional dynamics of various brain regions and their roles\nin cognitive processes. EEG is a cornerstone in sleep research, serving as the\nprimary modality of polysomnography, the gold standard in the field. However,\nEEG signals are prone to artifacts caused by both internal (device-specific)\nfactors and external (environmental) interferences. As sleep studies are\nbecoming larger, most rely on automatic sleep staging, a process highly\nsusceptible to artifacts, leading to erroneous sleep scores. This paper\naddresses this challenge by introducing eegFloss, an open-source Python package\nto utilize eegUsability, a novel machine learning (ML) model designed to detect\nsegments with artifacts in sleep EEG recordings. eegUsability has been trained\nand evaluated on manually artifact-labeled EEG data collected from 15\nparticipants over 127 nights using the Zmax headband. It demonstrates solid\noverall classification performance (F1-score is approximately 0.85, Cohens\nkappa is 0.78), achieving a high recall rate of approximately 94% in\nidentifying channel-wise usable EEG data, and extends beyond Zmax.\nAdditionally, eegFloss offers features such as automatic time-in-bed detection\nusing another ML model named eegMobility, filtering out certain artifacts, and\ngenerating hypnograms and sleep statistics. By addressing a fundamental\nchallenge faced by most sleep studies, eegFloss can enhance the precision and\nrigor of their analysis as well as the accuracy and reliability of their\noutcomes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aeegFloss\u7684\u5f00\u6e90Python\u5305\uff0c\u5b83\u53ef\u4ee5\u63d0\u9ad8\u7761\u7720\u7814\u7a76\u7684\u5206\u6790\u7cbe\u5ea6\u548c\u4e25\u8c28\u6027\u4ee5\u53ca\u7ed3\u679c\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u8111\u7535\u56fe(EEG)\u53ef\u4ee5\u76d1\u6d4b\u5927\u8111\u6d3b\u52a8\uff0c\u4e3a\u4e86\u89e3\u51b3\u8111\u7535\u4fe1\u53f7\u5bb9\u6613\u53d7\u5230\u4f2a\u5f71\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u9519\u8bef\u7684\u7761\u7720\u8bc4\u5206\u7684\u95ee\u9898\u3002", "method": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aeegFloss\u7684\u5f00\u6e90Python\u5305\uff0c\u5b83\u5229\u7528\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u5b66\u4e60(ML)\u6a21\u578beegUsability\uff0c\u8be5\u6a21\u578b\u65e8\u5728\u68c0\u6d4b\u7761\u7720\u8111\u7535\u56fe\u8bb0\u5f55\u4e2d\u5305\u542b\u4f2a\u5f71\u7684\u7247\u6bb5\u3002\u6b64\u5916\uff0ceegFloss\u8fd8\u63d0\u4f9b\u81ea\u52a8time-in-bed\u68c0\u6d4b\u7b49\u529f\u80fd\uff0c\u8fc7\u6ee4\u6389\u67d0\u4e9b\u4f2a\u5f71\uff0c\u5e76\u751f\u6210\u7761\u7720\u56fe\u548c\u7761\u7720\u7edf\u8ba1\u6570\u636e\u3002", "result": "eegUsability\u5728\u4ece15\u540d\u53c2\u4e0e\u8005\u8d85\u8fc7127\u4e2a\u591c\u665a\u6536\u96c6\u7684\u624b\u5de5\u6807\u8bb0\u4f2a\u5f71\u7684\u8111\u7535\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u5b83\u5c55\u793a\u4e86\u53ef\u9760\u7684\u6574\u4f53\u5206\u7c7b\u6027\u80fd(F1-score\u7ea6\u4e3a0.85, Cohens kappa\u4e3a0.78)\uff0c\u5728\u8bc6\u522b\u901a\u9053\u65b9\u9762\u5b9e\u73b0\u4e86\u7ea694%\u7684\u9ad8\u53ec\u56de\u7387\u53ef\u7528\u7684\u8111\u7535\u6570\u636e\uff0c\u5e76\u6269\u5c55\u5230Zmax\u4e4b\u5916\u3002", "conclusion": "eegFloss\u901a\u8fc7\u89e3\u51b3\u7761\u7720\u7814\u7a76\u9762\u4e34\u7684\u6839\u672c\u6311\u6218\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5176\u5206\u6790\u7684\u7cbe\u786e\u6027\u548c\u4e25\u8c28\u6027\uff0c\u4ee5\u53ca\u7ed3\u679c\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.06895", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06895", "abs": "https://arxiv.org/abs/2507.06895", "authors": ["Luca Mariotti", "Veronica Guidetti", "Federica Mandreoli"], "title": "SCoRE: Streamlined Corpus-based Relation Extraction using Multi-Label Contrastive Learning and Bayesian kNN", "comment": null, "summary": "The growing demand for efficient knowledge graph (KG) enrichment leveraging\nexternal corpora has intensified interest in relation extraction (RE),\nparticularly under low-supervision settings. To address the need for adaptable\nand noise-resilient RE solutions that integrate seamlessly with pre-trained\nlarge language models (PLMs), we introduce SCoRE, a modular and cost-effective\nsentence-level RE system. SCoRE enables easy PLM switching, requires no\nfinetuning, and adapts smoothly to diverse corpora and KGs. By combining\nsupervised contrastive learning with a Bayesian k-Nearest Neighbors (kNN)\nclassifier for multi-label classification, it delivers robust performance\ndespite the noisy annotations of distantly supervised corpora. To improve RE\nevaluation, we propose two novel metrics: Correlation Structure Distance (CSD),\nmeasuring the alignment between learned relational patterns and KG structures,\nand Precision at R (P@R), assessing utility as a recommender system. We also\nrelease Wiki20d, a benchmark dataset replicating real-world RE conditions where\nonly KG-derived annotations are available. Experiments on five benchmarks show\nthat SCoRE matches or surpasses state-of-the-art methods while significantly\nreducing energy consumption. Further analyses reveal that increasing model\ncomplexity, as seen in prior work, degrades performance, highlighting the\nadvantages of SCoRE's minimal design. Combining efficiency, modularity, and\nscalability, SCoRE stands as an optimal choice for real-world RE applications.", "AI": {"tldr": "SCoRE: A modular, efficient, and scalable sentence-level RE system that doesn't require fine-tuning and adapts well to noisy data. It uses contrastive learning and kNN, and outperforms existing methods with less energy. Includes new evaluation metrics and a benchmark dataset.", "motivation": "Growing demand for efficient knowledge graph (KG) enrichment leveraging external corpora, and the need for adaptable and noise-resilient RE solutions that integrate seamlessly with pre-trained large language models (PLMs).", "method": "Supervised contrastive learning with a Bayesian k-Nearest Neighbors (kNN) classifier.", "result": "SCoRE matches or surpasses state-of-the-art methods while significantly reducing energy consumption. Introduces Wiki20d, a benchmark dataset. Proposes two novel metrics: Correlation Structure Distance (CSD) and Precision at R (P@R).", "conclusion": "SCoRE is an efficient, modular, and scalable RE system suitable for real-world applications, outperforming existing methods with lower energy consumption."}}
{"id": "2507.06448", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06448", "abs": "https://arxiv.org/abs/2507.06448", "authors": ["Zhenhailong Wang", "Xuehang Guo", "Sofia Stoica", "Haiyang Xu", "Hongru Wang", "Hyeonjeong Ha", "Xiusi Chen", "Yangyi Chen", "Ming Yan", "Fei Huang", "Heng Ji"], "title": "Perception-Aware Policy Optimization for Multimodal Reasoning", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a\nhighly effective strategy for endowing Large Language Models (LLMs) with robust\nmulti-step reasoning abilities. However, its design and optimizations remain\ntailored to purely textual domains, resulting in suboptimal performance when\napplied to multimodal reasoning tasks. In particular, we observe that a major\nsource of error in current multimodal reasoning lies in the perception of\nvisual inputs. To address this bottleneck, we propose Perception-Aware Policy\nOptimization (PAPO), a simple yet effective extension of GRPO that encourages\nthe model to learn to perceive while learning to reason, entirely from internal\nsupervision signals. Notably, PAPO does not rely on additional data curation,\nexternal reward models, or proprietary models. Specifically, we introduce the\nImplicit Perception Loss in the form of a KL divergence term to the GRPO\nobjective, which, despite its simplicity, yields significant overall\nimprovements (4.4%) on diverse multimodal benchmarks. The improvements are more\npronounced, approaching 8.0%, on tasks with high vision dependency. We also\nobserve a substantial reduction (30.5%) in perception errors, indicating\nimproved perceptual capabilities with PAPO. We conduct comprehensive analysis\nof PAPO and identify a unique loss hacking issue, which we rigorously analyze\nand mitigate through a Double Entropy Loss. Overall, our work introduces a\ndeeper integration of perception-aware supervision into RLVR learning\nobjectives and lays the groundwork for a new RL framework that encourages\nvisually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO.", "AI": {"tldr": "They propose Perception-Aware Policy Optimization (PAPO) to improve the perception of visual inputs and overall performance on multimodal benchmarks.", "motivation": "Current multimodal reasoning has a major source of error in the perception of visual inputs. RLVR's design and optimizations remain tailored to purely textual domains, resulting in suboptimal performance when applied to multimodal reasoning tasks.", "method": "They propose Perception-Aware Policy Optimization (PAPO), a simple yet effective extension of GRPO that encourages the model to learn to perceive while learning to reason, entirely from internal supervision signals. They introduce the Implicit Perception Loss in the form of a KL divergence term to the GRPO objective.", "result": "PAPO yields significant overall improvements (4.4%) on diverse multimodal benchmarks. The improvements are more pronounced, approaching 8.0%, on tasks with high vision dependency. They also observe a substantial reduction (30.5%) in perception errors, indicating improved perceptual capabilities with PAPO.", "conclusion": "This work introduces a deeper integration of perception-aware supervision into RLVR learning objectives and lays the groundwork for a new RL framework that encourages visually grounded reasoning. They also identify a unique loss hacking issue, which they rigorously analyze and mitigate through a Double Entropy Loss."}}
{"id": "2507.06442", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06442", "abs": "https://arxiv.org/abs/2507.06442", "authors": ["Soroush Shahi", "Farzad Shahabi", "Rama Nabulsi", "Glenn Fernandes", "Aggelos Katsaggelos", "Nabil Alshurafa"], "title": "THOR: Thermal-guided Hand-Object Reasoning via Adaptive Vision Sampling", "comment": null, "summary": "Wearable cameras are increasingly used as an observational and interventional\ntool for human behaviors by providing detailed visual data of hand-related\nactivities. This data can be leveraged to facilitate memory recall for logging\nof behavior or timely interventions aimed at improving health. However,\ncontinuous processing of RGB images from these cameras consumes significant\npower impacting battery lifetime, generates a large volume of unnecessary video\ndata for post-processing, raises privacy concerns, and requires substantial\ncomputational resources for real-time analysis. We introduce THOR, a real-time\nadaptive spatio-temporal RGB frame sampling method that leverages thermal\nsensing to capture hand-object patches and classify them in real-time. We use\nlow-resolution thermal camera data to identify moments when a person switches\nfrom one hand-related activity to another, and adjust the RGB frame sampling\nrate by increasing it during activity transitions and reducing it during\nperiods of sustained activity. Additionally, we use the thermal cues from the\nhand to localize the region of interest (i.e., the hand-object interaction) in\neach RGB frame, allowing the system to crop and process only the necessary part\nof the image for activity recognition. We develop a wearable device to validate\nour method through an in-the-wild study with 14 participants and over 30\nactivities, and further evaluate it on Ego4D (923 participants across 9\ncountries, totaling 3,670 hours of video). Our results show that using only 3%\nof the original RGB video data, our method captures all the activity segments,\nand achieves hand-related activity recognition F1-score (95%) comparable to\nusing the entire RGB video (94%). Our work provides a more practical path for\nthe longitudinal use of wearable cameras to monitor hand-related activities and\nhealth-risk behaviors in real time.", "AI": {"tldr": "THOR uses thermal sensing and adaptive RGB frame sampling to efficiently capture and classify hand-object interactions with minimal data processing and power consumption.", "motivation": "Continuous processing of RGB images from wearable cameras consumes significant power, generates a large volume of unnecessary video data, raises privacy concerns, and requires substantial computational resources.", "method": "A real-time adaptive spatio-temporal RGB frame sampling method that leverages thermal sensing to capture hand-object patches and classify them in real-time. It uses low-resolution thermal camera data to identify moments when a person switches from one hand-related activity to another, and adjusts the RGB frame sampling rate.", "result": "Using only 3% of the original RGB video data, the method captures all the activity segments, and achieves hand-related activity recognition F1-score (95%) comparable to using the entire RGB video (94%).", "conclusion": "The method captures all activity segments using only 3% of the original RGB video data, and achieves hand-related activity recognition F1-score (95%) comparable to using the entire RGB video (94%). This provides a more practical path for the longitudinal use of wearable cameras to monitor hand-related activities and health-risk behaviors in real time."}}
{"id": "2507.06445", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06445", "abs": "https://arxiv.org/abs/2507.06445", "authors": ["Victoria R. Li", "Jenny Kaufmann", "Martin Wattenberg", "David Alvarez-Melis", "Naomi Saphra"], "title": "Can Interpretation Predict Behavior on Unseen Data?", "comment": null, "summary": "Interpretability research often aims to predict how a model will respond to\ntargeted interventions on specific mechanisms. However, it rarely predicts how\na model will respond to unseen input data. This paper explores the promises and\nchallenges of interpretability as a tool for predicting out-of-distribution\n(OOD) model behavior. Specifically, we investigate the correspondence between\nattention patterns and OOD generalization in hundreds of Transformer models\nindependently trained on a synthetic classification task. These models exhibit\nseveral distinct systematic generalization rules OOD, forming a diverse\npopulation for correlational analysis. In this setting, we find that simple\nobservational tools from interpretability can predict OOD performance. In\nparticular, when in-distribution attention exhibits hierarchical patterns, the\nmodel is likely to generalize hierarchically on OOD data -- even when the\nrule's implementation does not rely on these hierarchical patterns, according\nto ablation tests. Our findings offer a proof-of-concept to motivate further\ninterpretability work on predicting unseen model behavior.", "AI": {"tldr": "\u53ef\u89e3\u91ca\u6027\u53ef\u4ee5\u9884\u6d4bOOD\u6a21\u578b\u884c\u4e3a\u3002", "motivation": "\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u901a\u5e38\u65e8\u5728\u9884\u6d4b\u6a21\u578b\u5c06\u5982\u4f55\u54cd\u5e94\u9488\u5bf9\u7279\u5b9a\u673a\u5236\u7684\u5b9a\u5411\u5e72\u9884\u3002\u7136\u800c\uff0c\u5b83\u5f88\u5c11\u9884\u6d4b\u6a21\u578b\u5c06\u5982\u4f55\u54cd\u5e94\u770b\u4e0d\u89c1\u7684\u8f93\u5165\u6570\u636e\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u53ef\u89e3\u91ca\u6027\u4f5c\u4e3a\u9884\u6d4b\u8d85\u51fa\u5206\u5e03 (OOD) \u6a21\u578b\u884c\u4e3a\u7684\u5de5\u5177\u7684\u524d\u666f\u548c\u6311\u6218\u3002", "method": "\u5bf9\u6570\u767e\u4e2a\u72ec\u7acb\u8bad\u7ec3\u7684Transformer\u6a21\u578b\u8fdb\u884c\u76f8\u5173\u5206\u6790\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u5408\u6210\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5f53\u5206\u5e03\u5185\u7684\u6ce8\u610f\u529b\u5448\u73b0\u51fa\u5206\u5c42\u6a21\u5f0f\u65f6\uff0c\u8be5\u6a21\u578b\u5f88\u53ef\u80fd\u5728OOD\u6570\u636e\u4e0a\u8fdb\u884c\u5206\u5c42\u63a8\u5e7f\u2014\u2014\u5373\u4f7f\u6839\u636e\u6d88\u878d\u6d4b\u8bd5\uff0c\u89c4\u5219\u7684\u5b9e\u73b0\u5e76\u4e0d\u4f9d\u8d56\u4e8e\u8fd9\u4e9b\u5206\u5c42\u6a21\u5f0f\u3002", "conclusion": "\u7b80\u5355\u7684\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u53ef\u4ee5\u9884\u6d4bOOD\u6027\u80fd\u3002\u5f53\u5206\u5e03\u5185\u7684\u6ce8\u610f\u529b\u5448\u73b0\u51fa\u5206\u5c42\u6a21\u5f0f\u65f6\uff0c\u8be5\u6a21\u578b\u5f88\u53ef\u80fd\u5728OOD\u6570\u636e\u4e0a\u8fdb\u884c\u5206\u5c42\u63a8\u5e7f\u2014\u2014\u5373\u4f7f\u6839\u636e\u6d88\u878d\u6d4b\u8bd5\uff0c\u89c4\u5219\u7684\u5b9e\u73b0\u5e76\u4e0d\u4f9d\u8d56\u4e8e\u8fd9\u4e9b\u5206\u5c42\u6a21\u5f0f\u3002"}}
{"id": "2507.06450", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06450", "abs": "https://arxiv.org/abs/2507.06450", "authors": ["Xin Su", "Sungduk Yu", "Phillip Howard", "Steven Bethard"], "title": "A Semantic Parsing Framework for End-to-End Time Normalization", "comment": null, "summary": "Time normalization is the task of converting natural language temporal\nexpressions into machine-readable representations. It underpins many downstream\napplications in information retrieval, question answering, and clinical\ndecision-making. Traditional systems based on the ISO-TimeML schema limit\nexpressivity and struggle with complex constructs such as compositional,\nevent-relative, and multi-span time expressions. In this work, we introduce a\nnovel formulation of time normalization as a code generation task grounded in\nthe SCATE framework, which defines temporal semantics through symbolic and\ncompositional operators. We implement a fully executable SCATE Python library\nand demonstrate that large language models (LLMs) can generate executable SCATE\ncode. Leveraging this capability, we develop an automatic data augmentation\npipeline using LLMs to synthesize large-scale annotated data with code-level\nvalidation. Our experiments show that small, locally deployable models trained\non this augmented data can achieve strong performance, outperforming even their\nLLM parents and enabling practical, accurate, and interpretable time\nnormalization.", "AI": {"tldr": "Time normalization is framed as code generation using SCATE and LLMs, enabling smaller models to outperform larger ones after data augmentation.", "motivation": "Traditional time normalization systems have limited expressivity and struggle with complex temporal expressions.", "method": "Formulation of time normalization as code generation with SCATE framework and LLM-based data augmentation.", "result": "Small models achieve strong performance in time normalization, outperforming LLMs.", "conclusion": "Small models trained on augmented data outperform LLMs in time normalization."}}
{"id": "2507.06459", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06459", "abs": "https://arxiv.org/abs/2507.06459", "authors": ["Riadul Islam", "Joey Mul\u00e9", "Dhandeep Challagundla", "Shahmir Rizvi", "Sean Carson"], "title": "EA: An Event Autoencoder for High-Speed Vision Sensing", "comment": null, "summary": "High-speed vision sensing is essential for real-time perception in\napplications such as robotics, autonomous vehicles, and industrial automation.\nTraditional frame-based vision systems suffer from motion blur, high latency,\nand redundant data processing, limiting their performance in dynamic\nenvironments. Event cameras, which capture asynchronous brightness changes at\nthe pixel level, offer a promising alternative but pose challenges in object\ndetection due to sparse and noisy event streams. To address this, we propose an\nevent autoencoder architecture that efficiently compresses and reconstructs\nevent data while preserving critical spatial and temporal features. The\nproposed model employs convolutional encoding and incorporates adaptive\nthreshold selection and a lightweight classifier to enhance recognition\naccuracy while reducing computational complexity. Experimental results on the\nexisting Smart Event Face Dataset (SEFD) demonstrate that our approach achieves\ncomparable accuracy to the YOLO-v4 model while utilizing up to $35.5\\times$\nfewer parameters. Implementations on embedded platforms, including Raspberry Pi\n4B and NVIDIA Jetson Nano, show high frame rates ranging from 8 FPS up to 44.8\nFPS. The proposed classifier exhibits up to 87.84x better FPS than the\nstate-of-the-art and significantly improves event-based vision performance,\nmaking it ideal for low-power, high-speed applications in real-time edge\ncomputing.", "AI": {"tldr": "This paper introduces an event autoencoder for efficient event data compression and reconstruction, achieving comparable accuracy to YOLO-v4 with fewer parameters and higher speed, ideal for real-time edge computing.", "motivation": "Traditional frame-based vision systems are limited by motion blur, high latency, and redundant data processing in dynamic environments. Event cameras offer a promising alternative but pose challenges in object detection due to sparse and noisy event streams.", "method": "The paper proposes an event autoencoder architecture with convolutional encoding, adaptive threshold selection, and a lightweight classifier.", "result": "The proposed approach achieves comparable accuracy to YOLO-v4 on the SEFD dataset while utilizing up to 35.5x fewer parameters. Implementations on Raspberry Pi 4B and NVIDIA Jetson Nano show high frame rates, and the classifier exhibits up to 87.84x better FPS than the state-of-the-art.", "conclusion": "The proposed event autoencoder architecture achieves comparable accuracy to YOLO-v4 with significantly fewer parameters and higher FPS on embedded platforms, making it suitable for low-power, high-speed real-time edge computing applications."}}
{"id": "2507.06449", "categories": ["cs.LG", "cs.AI", "cs.DC", "68T05, 68T07, 68Q85, 94A08", "I.2.6; I.2.11; C.2.4"], "pdf": "https://arxiv.org/pdf/2507.06449", "abs": "https://arxiv.org/abs/2507.06449", "authors": ["Qianyu Long", "Qiyuan Wang", "Christos Anagnostopoulos", "Daning Bi"], "title": "FedPhD: Federated Pruning with Hierarchical Learning of Diffusion Models", "comment": "12 pages, 8 figures, 5 tables. This paper introduces FedPhD, a novel\n  hierarchical federated learning framework for training diffusion models that\n  addresses data heterogeneity and communication costs through\n  homogeneity-aware aggregation and structured pruning. Submitted to IEEE\n  Transactions on Cybernetics and is under review", "summary": "Federated Learning (FL), as a distributed learning paradigm, trains models\nover distributed clients' data. FL is particularly beneficial for distributed\ntraining of Diffusion Models (DMs), which are high-quality image generators\nthat require diverse data. However, challenges such as high communication costs\nand data heterogeneity persist in training DMs similar to training Transformers\nand Convolutional Neural Networks. Limited research has addressed these issues\nin FL environments. To address this gap and challenges, we introduce a novel\napproach, FedPhD, designed to efficiently train DMs in FL environments. FedPhD\nleverages Hierarchical FL with homogeneity-aware model aggregation and\nselection policy to tackle data heterogeneity while reducing communication\ncosts. The distributed structured pruning of FedPhD enhances computational\nefficiency and reduces model storage requirements in clients. Our experiments\nacross multiple datasets demonstrate that FedPhD achieves high model\nperformance regarding Fr\\'echet Inception Distance (FID) scores while reducing\ncommunication costs by up to $88\\%$. FedPhD outperforms baseline methods\nachieving at least a $34\\%$ improvement in FID, while utilizing only $56\\%$ of\nthe total computation and communication resources.", "AI": {"tldr": "FedPhD\u662f\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u9ad8\u6548\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5c42\u8054\u90a6\u5b66\u4e60\u548c\u5206\u5e03\u5f0f\u7ed3\u6784\u526a\u679d\u6765\u964d\u4f4e\u901a\u4fe1\u6210\u672c\u548c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u65f6\u9762\u4e34\u9ad8\u901a\u4fe1\u6210\u672c\u548c\u6570\u636e\u5f02\u6784\u6027\u7b49\u6311\u6218\uff0c\u800c\u73b0\u6709\u7814\u7a76\u5bf9\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e0b\u7684\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u95ee\u9898\u5173\u6ce8\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5FedPhD\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u5206\u5c42\u8054\u90a6\u5b66\u4e60\u3001\u540c\u8d28\u6027\u611f\u77e5\u6a21\u578b\u805a\u5408\u548c\u9009\u62e9\u7b56\u7565\u4ee5\u53ca\u5206\u5e03\u5f0f\u7ed3\u6784\u526a\u679d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFedPhD\u5728FID\u8bc4\u5206\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u901a\u4fe1\u6210\u672c\u964d\u4f4e\u9ad8\u8fbe88%\uff0c\u5e76\u4e14\u5728\u4ec5\u4f7f\u752856%\u7684\u603b\u8ba1\u7b97\u548c\u901a\u4fe1\u8d44\u6e90\u7684\u60c5\u51b5\u4e0b\uff0cFID\u63d0\u9ad8\u4e86\u81f3\u5c1134%\u3002", "conclusion": "FedPhD\u5728\u4fdd\u8bc1\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u901a\u4fe1\u6210\u672c\u548c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2507.06457", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06457", "abs": "https://arxiv.org/abs/2507.06457", "authors": ["Dustin Wang", "Rui-Jie Zhu", "Steven Abreu", "Yong Shan", "Taylor Kergan", "Yuqi Pan", "Yuhong Chou", "Zheng Li", "Ge Zhang", "Wenhao Huang", "Jason Eshraghian"], "title": "A Systematic Analysis of Hybrid Linear Attention", "comment": null, "summary": "Transformers face quadratic complexity and memory issues with long sequences,\nprompting the adoption of linear attention mechanisms using fixed-size hidden\nstates. However, linear models often suffer from limited recall performance,\nleading to hybrid architectures that combine linear and full attention layers.\nDespite extensive hybrid architecture research, the choice of linear attention\ncomponent has not been deeply explored. We systematically evaluate various\nlinear attention models across generations - vector recurrences to advanced\ngating mechanisms - both standalone and hybridized. To enable this\ncomprehensive analysis, we trained and open-sourced 72 models: 36 at 340M\nparameters (20B tokens) and 36 at 1.3B parameters (100B tokens), covering six\nlinear attention variants across five hybridization ratios. Benchmarking on\nstandard language modeling and recall tasks reveals that superior standalone\nlinear models do not necessarily excel in hybrids. While language modeling\nremains stable across linear-to-full attention ratios, recall significantly\nimproves with increased full attention layers, particularly below a 3:1 ratio.\nOur study highlights selective gating, hierarchical recurrence, and controlled\nforgetting as critical for effective hybrid models. We recommend architectures\nsuch as HGRN-2 or GatedDeltaNet with a linear-to-full ratio between 3:1 and 6:1\nto achieve Transformer-level recall efficiently. Our models are open-sourced at\nhttps://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\u4e0e\u5b8c\u6574\u6ce8\u610f\u529b\u6a21\u578b\u6df7\u5408\u67b6\u6784\uff0c\u53d1\u73b0\u4f18\u79c0\u7684\u72ec\u7acb\u7ebf\u6027\u6a21\u578b\u4e0d\u4e00\u5b9a\u5728\u6df7\u5408\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u63a8\u8350\u4f7f\u7528HGRN-2\u6216GatedDeltaNet\u7b49\u67b6\u6784\uff0c\u7ebf\u6027\u4e0e\u5b8c\u6574\u6bd4\u4f8b\u57283:1\u548c6:1\u4e4b\u95f4\u4ee5\u5b9e\u73b0Transformer\u7ea7\u522b\u7684\u6548\u7387\u3002", "motivation": "Transformer\u5728\u5904\u7406\u957f\u5e8f\u5217\u65f6\u9762\u4e34\u4e8c\u6b21\u590d\u6742\u5ea6\u548c\u5185\u5b58\u95ee\u9898\uff0c\u4fc3\u4f7f\u91c7\u7528\u4f7f\u7528\u56fa\u5b9a\u5927\u5c0f\u9690\u85cf\u72b6\u6001\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u3002\u7136\u800c\uff0c\u7ebf\u6027\u6a21\u578b\u901a\u5e38\u53d7\u5230\u6709\u9650\u7684\u53ec\u56de\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u6df7\u5408\u67b6\u6784\u7ed3\u5408\u7ebf\u6027\u548c\u5b8c\u6574\u6ce8\u610f\u529b\u5c42\u3002\u5c3d\u7ba1\u5bf9\u6df7\u5408\u67b6\u6784\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u7814\u7a76\uff0c\u4f46\u7ebf\u6027\u6ce8\u610f\u529b\u7ec4\u4ef6\u7684\u9009\u62e9\u5c1a\u672a\u5f97\u5230\u6df1\u5165\u63a2\u8ba8\u3002", "method": "\u7cfb\u7edf\u5730\u8bc4\u4f30\u5404\u79cd\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\u5728\u751f\u6210\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u5411\u91cf\u9012\u5f52\u5230\u9ad8\u7ea7\u95e8\u63a7\u673a\u5236\uff0c\u65e0\u8bba\u662f\u72ec\u7acb\u4f7f\u7528\u8fd8\u662f\u6df7\u5408\u4f7f\u7528\u3002\u8bad\u7ec3\u5e76\u5f00\u6e90\u4e8672\u4e2a\u6a21\u578b\uff1a36\u4e2a3.4\u4ebf\u53c2\u6570\u548c36\u4e2a13\u4ebf\u53c2\u6570\uff0c\u6db5\u76d6\u4e86\u516d\u79cd\u7ebf\u6027\u6ce8\u610f\u529b\u53d8\u4f53\u548c\u4e94\u79cd\u6df7\u5408\u6bd4\u4f8b\u3002", "result": "\u4f18\u79c0\u7684\u72ec\u7acb\u7ebf\u6027\u6a21\u578b\u4e0d\u4e00\u5b9a\u5728\u6df7\u5408\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u867d\u7136\u8bed\u8a00\u5efa\u6a21\u5728\u4ece\u7ebf\u6027\u5230\u5b8c\u5168\u6ce8\u610f\u529b\u7684\u6bd4\u7387\u4e2d\u4fdd\u6301\u7a33\u5b9a\uff0c\u4f46\u53ec\u56de\u7387\u968f\u7740\u5b8c\u5168\u6ce8\u610f\u529b\u5c42\u7684\u589e\u52a0\u800c\u663e\u7740\u63d0\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u4e8e3\uff1a1\u7684\u6bd4\u7387\u4e0b\u3002\u9009\u62e9\u6027\u95e8\u63a7\u3001\u5206\u5c42\u9012\u5f52\u548c\u53d7\u63a7\u9057\u5fd8\u5bf9\u4e8e\u6709\u6548\u7684\u6df7\u5408\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u7ed3\u5408\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\u548c\u5b8c\u6574\u6ce8\u610f\u529b\u5c42\u7684\u6df7\u5408\u67b6\u6784\uff0c\u4e3a\u4e86\u8fbe\u5230Transformer\u7ea7\u522b\u7684\u6548\u7387\uff0c\u63a8\u8350\u4f7f\u7528HGRN-2\u6216GatedDeltaNet\u7b49\u67b6\u6784\uff0c\u7ebf\u6027\u4e0e\u5b8c\u6574\u6bd4\u4f8b\u57283:1\u548c6:1\u4e4b\u95f4\u3002"}}
{"id": "2507.06485", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06485", "abs": "https://arxiv.org/abs/2507.06485", "authors": ["Ziyang Wang", "Jaehong Yoon", "Shoubin Yu", "Md Mohaiminul Islam", "Gedas Bertasius", "Mohit Bansal"], "title": "Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning", "comment": "The first two authors contributed equally. Project page:\n  https://sites.google.com/cs.unc.edu/videorts2025/", "summary": "Despite advances in reinforcement learning (RL)-based video reasoning with\nlarge language models (LLMs), data collection and finetuning remain significant\nchallenges. These methods often rely on large-scale supervised fine-tuning\n(SFT) with extensive video data and long Chain-of-Thought (CoT) annotations,\nmaking them costly and hard to scale. To address this, we present Video-RTS, a\nnew approach to improve video reasoning capability with drastically improved\ndata efficiency by combining data-efficient RL with a video-adaptive test-time\nscaling (TTS) strategy. Based on observations about the data scaling of RL\nsamples, we skip the resource-intensive SFT step and employ efficient pure-RL\ntraining with output-based rewards, requiring no additional annotations or\nextensive fine-tuning. Furthermore, to utilize computational resources more\nefficiently, we introduce a sparse-to-dense video TTS strategy that improves\ninference by iteratively adding frames based on output consistency. We validate\nour approach on multiple video reasoning benchmarks, showing that Video-RTS\nsurpasses existing video reasoning models by an average of 2.4% in accuracy\nusing only 3.6% training samples. For example, Video-RTS achieves a 4.2%\nimprovement on Video-Holmes, a recent and challenging video reasoning\nbenchmark, and a 2.6% improvement on MMVU. Notably, our pure RL training and\nadaptive video TTS offer complementary strengths, enabling Video-RTS's strong\nreasoning performance.", "AI": {"tldr": "Video-RTS improves video reasoning with data-efficient RL and adaptive video TTS, achieving better accuracy with significantly fewer training samples, avoiding costly supervised fine-tuning.", "motivation": "Existing RL-based video reasoning methods with LLMs rely on large-scale supervised fine-tuning (SFT) with extensive video data and long Chain-of-Thought (CoT) annotations, making them costly and hard to scale.", "method": "The method combines data-efficient RL with a sparse-to-dense video TTS strategy that improves inference by iteratively adding frames based on output consistency. It employs efficient pure-RL training with output-based rewards, requiring no additional annotations or extensive fine-tuning, skipping the resource-intensive SFT step.", "result": "Video-RTS surpasses existing video reasoning models by an average of 2.4% in accuracy using only 3.6% training samples. It achieves a 4.2% improvement on Video-Holmes and a 2.6% improvement on MMVU.", "conclusion": "The paper introduces Video-RTS, which combines data-efficient RL with a video-adaptive test-time scaling (TTS) strategy to improve video reasoning capability. The approach surpasses existing models by an average of 2.4% in accuracy using only 3.6% training samples."}}
{"id": "2507.06458", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2507.06458", "abs": "https://arxiv.org/abs/2507.06458", "authors": ["Arjun Banerjee", "David Martinez", "Camille Dang", "Ethan Tam"], "title": "Automated Neuron Labelling Enables Generative Steering and Interpretability in Protein Language Models", "comment": "15 pages, 13 figures. Accepted to Proceedings of the Workshop on\n  Generative AI for Biology at the 42nd International Conference on Machine\n  Learning (Spotlight)", "summary": "Protein language models (PLMs) encode rich biological information, yet their\ninternal neuron representations are poorly understood. We introduce the first\nautomated framework for labeling every neuron in a PLM with biologically\ngrounded natural language descriptions. Unlike prior approaches relying on\nsparse autoencoders or manual annotation, our method scales to hundreds of\nthousands of neurons, revealing individual neurons are selectively sensitive to\ndiverse biochemical and structural properties. We then develop a novel neuron\nactivation-guided steering method to generate proteins with desired traits,\nenabling convergence to target biochemical properties like molecular weight and\ninstability index as well as secondary and tertiary structural motifs,\nincluding alpha helices and canonical Zinc Fingers. We finally show that\nanalysis of labeled neurons in different model sizes reveals PLM scaling laws\nand a structured neuron space distribution.", "AI": {"tldr": "Introduce an automated framework for labeling every neuron in a PLM with biologically grounded natural language descriptions, revealing individual neurons are selectively sensitive to diverse biochemical and structural properties. ", "motivation": "Protein language models (PLMs) encode rich biological information, yet their internal neuron representations are poorly understood.", "method": "Automated framework for labeling every neuron in a PLM with biologically grounded natural language descriptions and neuron activation-guided steering method.", "result": "Individual neurons are selectively sensitive to diverse biochemical and structural properties. Generation of proteins with desired traits, enabling convergence to target biochemical properties and structural motifs.", "conclusion": "Analysis of labeled neurons in different model sizes reveals PLM scaling laws and a structured neuron space distribution."}}
{"id": "2507.06489", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06489", "abs": "https://arxiv.org/abs/2507.06489", "authors": ["Stephen Obadinma", "Xiaodan Zhu"], "title": "On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks", "comment": null, "summary": "Robust verbal confidence generated by large language models (LLMs) is crucial\nfor the deployment of LLMs to ensure transparency, trust, and safety in\nhuman-AI interactions across many high-stakes applications. In this paper, we\npresent the first comprehensive study on the robustness of verbal confidence\nunder adversarial attacks. We introduce a novel framework for attacking verbal\nconfidence scores through both perturbation and jailbreak-based methods, and\nshow that these attacks can significantly jeopardize verbal confidence\nestimates and lead to frequent answer changes. We examine a variety of\nprompting strategies, model sizes, and application domains, revealing that\ncurrent confidence elicitation methods are vulnerable and that commonly used\ndefence techniques are largely ineffective or counterproductive. Our findings\nunderscore the urgent need to design more robust mechanisms for confidence\nexpression in LLMs, as even subtle semantic-preserving modifications can lead\nto misleading confidence in responses.", "AI": {"tldr": "Verbal confidence scores in LLMs are vulnerable to adversarial attacks, necessitating the design of more robust mechanisms for confidence expression.", "motivation": "Robust verbal confidence generated by large language models (LLMs) is crucial for the deployment of LLMs to ensure transparency, trust, and safety in human-AI interactions across many high-stakes applications.", "method": "A novel framework for attacking verbal confidence scores through both perturbation and jailbreak-based methods", "result": "Attacks can significantly jeopardize verbal confidence estimates and lead to frequent answer changes. Examination of a variety of prompting strategies, model sizes, and application domains, revealing that current confidence elicitation methods are vulnerable and that commonly used defence techniques are largely ineffective or counterproductive.", "conclusion": "Current confidence elicitation methods are vulnerable, and commonly used defence techniques are largely ineffective or counterproductive. Urgent need to design more robust mechanisms for confidence expression in LLMs, as even subtle semantic-preserving modifications can lead to misleading confidence in responses."}}
{"id": "2507.06486", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06486", "abs": "https://arxiv.org/abs/2507.06486", "authors": ["Yuechen Xie", "Haobo Jiang", "Jin Xie"], "title": "Mask6D: Masked Pose Priors For 6D Object Pose Estimation", "comment": "Accepted at ICASSP 2024. 4 figures, 3 tables", "summary": "Robust 6D object pose estimation in cluttered or occluded conditions using\nmonocular RGB images remains a challenging task. One reason is that current\npose estimation networks struggle to extract discriminative, pose-aware\nfeatures using 2D feature backbones, especially when the available RGB\ninformation is limited due to target occlusion in cluttered scenes. To mitigate\nthis, we propose a novel pose estimation-specific pre-training strategy named\nMask6D. Our approach incorporates pose-aware 2D-3D correspondence maps and\nvisible mask maps as additional modal information, which is combined with RGB\nimages for the reconstruction-based model pre-training. Essentially, this 2D-3D\ncorrespondence maps a transformed 3D object model to 2D pixels, reflecting the\npose information of the target in camera coordinate system. Meanwhile, the\nintegrated visible mask map can effectively guide our model to disregard\ncluttered background information. In addition, an object-focused pre-training\nloss function is designed to further facilitate our network to remove the\nbackground interference. Finally, we fine-tune our pre-trained pose prior-aware\nnetwork via conventional pose training strategy to realize the reliable pose\nprediction. Extensive experiments verify that our method outperforms previous\nend-to-end pose estimation methods.", "AI": {"tldr": "\u63d0\u51faMask6D\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u54082D-3D\u5bf9\u5e94\u56fe\u548c\u53ef\u89c1\u63a9\u7801\u56fe\uff0c\u4ee5\u63d0\u9ad8\u5728\u6742\u4e71\u548c\u906e\u6321\u6761\u4ef6\u4e0b\u4f7f\u7528\u5355\u76eeRGB\u56fe\u50cf\u8fdb\u884c6D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4f7f\u7528\u5355\u76eeRGB\u56fe\u50cf\u5728\u6742\u4e71\u6216\u906e\u6321\u6761\u4ef6\u4e0b\u8fdb\u884c\u9c81\u68d2\u76846D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u4ecd\u7136\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u4e00\u4e2a\u539f\u56e0\u662f\u5f53\u524d\u7684\u59ff\u6001\u4f30\u8ba1\u7f51\u7edc\u96be\u4ee5\u4f7f\u75282D\u7279\u5f81\u9aa8\u5e72\u63d0\u53d6\u6709\u533a\u5206\u6027\u7684\u3001\u59ff\u6001\u611f\u77e5\u7684\u7279\u5f81\uff0c\u7279\u522b\u662f\u5f53\u7531\u4e8e\u6742\u4e71\u573a\u666f\u4e2d\u7684\u76ee\u6807\u906e\u6321\u5bfc\u81f4\u53ef\u7528\u7684RGB\u4fe1\u606f\u6709\u9650\u65f6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMask6D\u7684\u59ff\u6001\u4f30\u8ba1\u7279\u5b9a\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u7ed3\u5408\u4e86\u59ff\u6001\u611f\u77e5\u76842D-3D\u5bf9\u5e94\u56fe\u548c\u53ef\u89c1\u63a9\u7801\u56fe\u4f5c\u4e3a\u989d\u5916\u7684\u6a21\u6001\u4fe1\u606f\uff0c\u5e76\u4e0eRGB\u56fe\u50cf\u7ed3\u5408\u7528\u4e8e\u57fa\u4e8e\u91cd\u5efa\u7684\u6a21\u578b\u9884\u8bad\u7ec3\u3002", "result": "\u7ecf\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5148\u524d\u7684\u7aef\u5230\u7aef\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5148\u524d\u7684\u7aef\u5230\u7aef\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u3002"}}
{"id": "2507.06461", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.06461", "abs": "https://arxiv.org/abs/2507.06461", "authors": ["Risi Jaiswal", "Supriyo Datta", "Joseph G. Makin"], "title": "Energy-Efficient Supervised Learning with a Binary Stochastic Forward-Forward Algorithm", "comment": "24 pages, 5 figures, 4 tables. Under review", "summary": "Reducing energy consumption has become a pressing need for modern machine\nlearning, which has achieved many of its most impressive results by scaling to\nlarger and more energy-consumptive neural networks. Unfortunately, the main\nalgorithm for training such networks, backpropagation, poses significant\nchallenges for custom hardware accelerators, due to both its serial\ndependencies and the memory footprint needed to store forward activations for\nthe backward pass. Alternatives to backprop, although less effective, do exist;\nhere the main computational bottleneck becomes matrix multiplication. In this\nstudy, we derive forward-forward algorithms for binary, stochastic units.\nBinarization of the activations transforms matrix multiplications into indexing\noperations, which can be executed efficiently in hardware. Stochasticity,\ncombined with tied weights across units with different biases, bypasses the\ninformation bottleneck imposed by binary units. Furthermore, although slow and\nexpensive in traditional hardware, binary sampling that is very fast can be\nimplemented cheaply with p-bits (probabilistic bits), novel devices made up of\nunstable magnets. We evaluate our proposed algorithms on the MNIST,\nFashion-MNIST, and CIFAR-10 datasets, showing that its performance is close to\nreal-valued forward-forward, but with an estimated energy savings of about one\norder of magnitude.", "AI": {"tldr": "This paper introduces binary stochastic forward-forward algorithms that reduce energy consumption in machine learning with minimal performance loss compared to real-valued alternatives.", "motivation": "Reducing energy consumption in machine learning is a pressing need, but backpropagation poses challenges for custom hardware accelerators. Alternatives like forward-forward algorithms exist, but their main bottleneck is matrix multiplication.", "method": "Derivation of forward-forward algorithms for binary, stochastic units, using binarization to transform matrix multiplications into indexing operations and tied weights to bypass the information bottleneck.", "result": "Evaluation on MNIST, Fashion-MNIST, and CIFAR-10 datasets shows performance close to real-valued forward-forward with an estimated energy savings of about one order of magnitude.", "conclusion": "The proposed binary stochastic forward-forward algorithms achieve performance close to real-valued forward-forward while saving about one order of magnitude in energy."}}
