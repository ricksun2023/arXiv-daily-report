<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 15]
- [cs.CV](#cs.CV) [Total: 10]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 11]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs](https://arxiv.org/abs/2511.03738)
*Pranav Bhandari,Nicolas Fay,Sanjeevan Selvaganapathy,Amitava Datta,Usman Naseem,Mehwish Nasim*

Main category: cs.CL

TL;DR: 大型语言模型在生成过程中表现出隐含的个性，但可靠地控制或调整这些特征以满足特定需求仍然是一个未解决的挑战。


<details>
  <summary>Details</summary>
Motivation: 缺乏在生成过程中有效操纵模型行为的机制。个性化的大型语言模型为此提供了一个有希望的方向。

Method: 我们提出了一个新颖的流程，该流程使用五大人格特质从 Transformer 层中提取隐藏状态激活，应用低秩子空间发现方法，并识别跨不同模型架构的特定于特征的最佳层以进行稳健的注入。

Result: 研究结果表明，人格特质占据一个低秩共享子空间，并且这些潜在结构可以转化为可操作的机制，通过仔细的扰动进行有效的引导，而不会影响流畅性、方差和一般能力。

Conclusion: 该研究弥合了心理学理论和实际模型对齐之间的差距。

Abstract: Large Language Models exhibit implicit personalities in their generation, but
reliably controlling or aligning these traits to meet specific needs remains an
open challenge. The need for effective mechanisms for behavioural manipulation
of the model during generation is a critical gap in the literature that needs
to be fulfilled. Personality-aware LLMs hold a promising direction towards this
objective. However, the relationship between these psychological constructs and
their representations within LLMs remains underexplored and requires further
investigation. Moreover, it is intriguing to understand and study the use of
these representations to steer the models' behaviour. We propose a novel
pipeline that extracts hidden state activations from transformer layers using
the Big Five Personality Traits (Openness, Conscientiousness, Extraversion,
Agreeableness and Neuroticism), which is a comprehensive and empirically
validated framework to model human personality applies low-rank subspace
discovery methods, and identifies trait-specific optimal layers across
different model architectures for robust injection. The resulting
personality-aligned directions are then operationalised through a flexible
steering framework with dynamic layer selection, enabling precise control of
trait expression in LLM outputs. Our findings reveal that personality traits
occupy a low-rank shared subspace, and that these latent structures can be
transformed into actionable mechanisms for effective steering through careful
perturbations without impacting the fluency, variance and general capabilities,
helping to bridge the gap between psychological theory and practical model
alignment.

</details>


### [2] [TextualVerifier: Verify TextGrad Step-by-Step](https://arxiv.org/abs/2511.03739)
*Eugenius Mario Situmorang,Adila Alfa Krisnadhi,Ari Wibisono*

Main category: cs.CL

TL;DR: TextGrad introduces a novel approach to text-based automatic differentiation that enables composite AI systems to perform optimization without explicit numerical equations. However, it currently lacks self-verification mechanisms that ensure reasoning validity in text-based decision making.


<details>
  <summary>Details</summary>
Motivation: This research introduces TextualVerifier, a verification framework that leverages chain-of-thought reasoning and majority voting with large language models to address this verification gap.

Method: TextualVerifier implements a four-stage workflow: chain-of-thought decomposition, variant generation, majority voting, and consensus aggregation. It integrates non-invasively with TextGrad at both the loss function and optimization result verification stages.

Result: In phase one, TextualVerifier improves the validity of reasoning steps by 29 percent. In phase two, integration into TextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4 percent with a moderate overhead of 5.9 LLM calls on average. Further evaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92 percentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively.

Conclusion: TextualVerifier thus presents the first self-verification framework for TextGrad through LLM-based techniques without requiring numerical gradients, enabling more reliable reasoning and opening new directions for verification in text-based optimization.

Abstract: TextGrad is a novel approach to text-based automatic differentiation that
enables composite AI systems to perform optimization without explicit numerical
equations. However, it currently lacks self-verification mechanisms that ensure
reasoning validity in text-based decision making. This research introduces
TextualVerifier, a verification framework that leverages chain-of-thought
reasoning and majority voting with large language models to address this
verification gap. TextualVerifier implements a four-stage workflow:
chain-of-thought decomposition, variant generation, majority voting, and
consensus aggregation. It integrates non-invasively with TextGrad at both the
loss function and optimization result verification stages. Experimental
evaluation using the Gemini 1.5 Pro model is conducted in two phases: (1)
standalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad
on GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically
significant improvements (p < 0.001). In phase one, TextualVerifier improves
the validity of reasoning steps by 29 percent. In phase two, integration into
TextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4
percent with a moderate overhead of 5.9 LLM calls on average. Further
evaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92
percentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively.
TextualVerifier thus presents the first self-verification framework for
TextGrad through LLM-based techniques without requiring numerical gradients,
enabling more reliable reasoning and opening new directions for verification in
text-based optimization.

</details>


### [3] [GRDD+: An Extended Greek Dialectal Dataset with Cross-Architecture Fine-tuning Evaluation](https://arxiv.org/abs/2511.03772)
*Stergios Chatzikyriakidis,Dimitris Papadakis,Sevasti-Ioanna Papaioannou,Erofili Psaltaki*

Main category: cs.CL

TL;DR: 本文介绍了一个扩展的希腊方言数据集 GRDD+，它在现有 GRDD 数据集的基础上增加了来自克里特岛、塞浦路斯、本都和希腊北部的数据，同时增加了六个新的变种。


<details>
  <summary>Details</summary>
Motivation: 创建包含多种方言的大型数据集，以研究高质量方言数据对大型语言模型的影响。

Method: 使用 GRDD+ 数据集对三种模型架构（Llama-3-8B、Llama-3.1-8B、Krikri-8B）进行微调，并将结果与前沿模型（Claude-3.7-Sonnet、Gemini-2.5、ChatGPT-5）进行比较。

Result: GRDD+ 是第一个具有这种变异和大小的数据集，总大小为 6,374,939 个单词和 10 个变种。

Conclusion: 通过对 LLM 进行微调实验，来验证该数据集的有效性

Abstract: We present an extended Greek Dialectal Dataset (GRDD+) 1that complements the
existing GRDD dataset with more data from Cretan, Cypriot, Pontic and Northern
Greek, while we add six new varieties: Greco-Corsican, Griko (Southern Italian
Greek), Maniot, Heptanesian, Tsakonian, and Katharevusa Greek. The result is a
dataset with total size 6,374,939 words and 10 varieties. This is the first
dataset with such variation and size to date. We conduct a number of
fine-tuning experiments to see the effect of good quality dialectal data on a
number of LLMs. We fine-tune three model architectures (Llama-3-8B,
Llama-3.1-8B, Krikri-8B) and compare the results to frontier models
(Claude-3.7-Sonnet, Gemini-2.5, ChatGPT-5).

</details>


### [4] [PLLuM: A Family of Polish Large Language Models](https://arxiv.org/abs/2511.03823)
*Jan Kocoń,Maciej Piasecki,Arkadiusz Janz,Teddy Ferdinan,Łukasz Radliński,Bartłomiej Koptyra,Marcin Oleksy,Stanisław Woźniak,Paweł Walkowiak,Konrad Wojtasik,Julia Moska,Tomasz Naskręt,Bartosz Walkowiak,Mateusz Gniewkowski,Kamil Szyc,Dawid Motyka,Dawid Banach,Jonatan Dalasiński,Ewa Rudnicka,Bartłomiej Alberski,Tomasz Walkowiak,Aleksander Szczęsny,Maciej Markiewicz,Tomasz Bernaś,Hubert Mazur,Kamil Żyta,Mateusz Tykierko,Grzegorz Chodak,Tomasz Kajdanowicz,Przemysław Kazienko,Agnieszka Karlińska,Karolina Seweryn,Anna Kołos,Maciej Chrabąszcz,Katarzyna Lorenc,Aleksandra Krasnodębska,Artur Wilczek,Katarzyna Dziewulska,Paula Betscher,Zofia Cieślińska,Katarzyna Kowol,Daria Mikoś,Maciej Trzciński,Dawid Krutul,Marek Kozłowski,Sławomir Dadas,Rafał Poświata,Michał Perełkiewicz,Małgorzata Grębowiec,Maciej Kazuła,Marcin Białas,Roman Roszko,Danuta Roszko,Jurgita Vaičenonienė,Andrius Utka,Paweł Levchuk,Paweł Kowalski,Irena Prawdzic-Jankowska,Maciej Ogrodniczuk,Monika Borys,Anna Bulińska,Wiktoria Gumienna,Witold Kieraś,Dorota Komosińska,Katarzyna Krasnowska-Kieraś,Łukasz Kobyliński,Martyna Lewandowska,Marek Łaziński,Mikołaj Łątkowski,Dawid Mastalerz,Beata Milewicz,Agnieszka Anna Mykowiecka,Angelika Peljak-Łapińska,Sandra Penno,Zuzanna Przybysz,Michał Rudolf,Piotr Rybak,Karolina Saputa,Aleksandra Tomaszewska,Aleksander Wawer,Marcin Woliński,Joanna Wołoszyn,Alina Wróblewska,Bartosz Żuk,Filip Żarnecki,Konrad Kaczyński,Anna Cichosz,Zuzanna Deckert,Monika Garnys,Izabela Grabarczyk,Wojciech Janowski,Sylwia Karasińska,Aleksandra Kujawiak,Piotr Misztela,Maria Szymańska,Karolina Walkusz,Igor Siek,Jakub Kwiatkowski,Piotr Pęzik*

Main category: cs.CL

TL;DR: PLLuM：为波兰语定制的大型开源语言模型，旨在促进开放研究和加强波兰的自主人工智能技术。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型主要集中于英语，对其他语言的支持有限。PLLuM的出现是为了满足对高质量、透明且与文化相关的语言模型的需求，以弥补以英语为中心的商业环境的不足。

Method: 构建了一个新的1400亿token的波兰语文本语料库用于预训练，一个7.7万条自定义指令数据集和一个10万条偏好优化数据集。采用了负责任的AI框架，该框架结合了严格的数据治理和一个用于输出校正和安全过滤的混合模块。

Result: 发布了基础模型和指令调整变体，并证明了它们在公共管理下游任务中的效用。

Conclusion: PLLuM旨在通过公开发布这些模型，从而促进开放研究并加强波兰的自主人工智能技术。

Abstract: Large Language Models (LLMs) play a central role in modern artificial
intelligence, yet their development has been primarily focused on English,
resulting in limited support for other languages. We present PLLuM (Polish
Large Language Model), the largest open-source family of foundation models
tailored specifically for the Polish language. Developed by a consortium of
major Polish research institutions, PLLuM addresses the need for high-quality,
transparent, and culturally relevant language models beyond the English-centric
commercial landscape. We describe the development process, including the
construction of a new 140-billion-token Polish text corpus for pre-training, a
77k custom instructions dataset, and a 100k preference optimization dataset. A
key component is a Responsible AI framework that incorporates strict data
governance and a hybrid module for output correction and safety filtering. We
detail the models' architecture, training procedures, and alignment techniques
for both base and instruction-tuned variants, and demonstrate their utility in
a downstream task within public administration. By releasing these models
publicly, PLLuM aims to foster open research and strengthen sovereign AI
technologies in Poland.

</details>


### [5] [STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models](https://arxiv.org/abs/2511.03827)
*Mohammad Atif Quamar,Mohammad Areeb,Mikhail Kuznetsov,Muslum Ozgur Ozmen,Z. Berkay Celik*

Main category: cs.CL

TL;DR: 提出了一种新的解码算法STARS，通过在分段级别上对token进行对齐和拒绝采样，从而在推理时引导模型生成，提高计算效率和对齐质量。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型对齐方法（如微调）计算成本高且效果欠佳，而Best-of-N采样等推理时方法需要大量计算才能实现最佳对齐。

Method: 提出STARS算法，该算法通过迭代地采样、评分和拒绝/接受短的、固定大小的token段来引导模型生成。允许提前纠正生成路径。

Result: 在六个LLM上，STARS的胜率比Supervised Fine-Tuning (SFT) 高出14.9个百分点，比Direct Preference Optimization (DPO) 高出4.3个百分点，同时与强大的Best-of-N基线相比仍具有很强的竞争力。

Conclusion: STARS算法是一种通用的、稳健的、高效的替代方案，可以替代传统的微调和全序列排序方法，用于对齐LLM。

Abstract: Aligning large language models with human values is crucial for their safe
deployment; however, existing methods, such as fine-tuning, are computationally
expensive and suboptimal. In contrast, inference-time approaches like Best-of-N
sampling require practically infeasible computation to achieve optimal
alignment. We propose STARS: Segment-level Token Alignment with Rejection
Sampling, a decoding-time algorithm that steers model generation by iteratively
sampling, scoring, and rejecting/accepting short, fixed-size token segments.
This allows for early correction of the generation path, significantly
improving computational efficiency and boosting alignment quality. Across a
suite of six LLMs, we show that STARS outperforms Supervised Fine-Tuning (SFT)
by up to 14.9 percentage points and Direct Preference Optimization (DPO) by up
to 4.3 percentage points on win-rates, while remaining highly competitive with
strong Best-of-N baselines. Our work establishes granular, reward-guided
sampling as a generalizable, robust, and efficient alternative to traditional
fine-tuning and full-sequence ranking methods for aligning LLMs.

</details>


### [6] [Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification](https://arxiv.org/abs/2511.03830)
*Mikołaj Langner,Jan Eliasz,Ewa Rudnicka,Jan Kocoń*

Main category: cs.CL

TL;DR: 提出了一种高效的多标签文本分类方法，该方法将分类任务重构为一系列二分（是/否）决策。


<details>
  <summary>Details</summary>
Motivation: 为了提高大型语言模型（LLM）在多标签文本分类中的效率，特别是在情感文本分析等任务中。

Method: 通过将每个目标维度独立查询，并结合前缀缓存机制，来实现效率提升。使用LLM-to-SLM知识蒸馏，利用DeepSeek-V3模型提供多个注释，聚合后微调较小的模型。

Result: 微调后的模型在训练期间看到的维度上，相对于零样本基线显示出显著改进。表明将多标签分类分解为二分查询，结合蒸馏和缓存感知推理，为基于LLM的分类提供了一个可扩展且有效的框架。

Conclusion: 该方法具有通用性，适用于各个领域，为LLM在多标签文本分类中的应用提供了一种高效且准确的解决方案。

Abstract: We introduce a method for efficient multi-label text classification with
large language models (LLMs), built on reformulating classification tasks as
sequences of dichotomic (yes/no) decisions. Instead of generating all labels in
a single structured response, each target dimension is queried independently,
which, combined with a prefix caching mechanism, yields substantial efficiency
gains for short-text inference without loss of accuracy. To demonstrate the
approach, we focus on affective text analysis, covering 24 dimensions including
emotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator
model (DeepSeek-V3) provides multiple annotations per text, which are
aggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,
Gemma3-1B). The fine-tuned models show significant improvements over zero-shot
baselines, particularly on the dimensions seen during training. Our findings
suggest that decomposing multi-label classification into dichotomic queries,
combined with distillation and cache-aware inference, offers a scalable and
effective framework for LLM-based classification. While we validate the method
on affective states, the approach is general and applicable across domains.

</details>


### [7] [Evaluating Machine Translation Datasets for Low-Web Data Languages: A Gendered Lens](https://arxiv.org/abs/2511.03880)
*Hellina Hailu Nigatu,Bethelhem Yemane Mamo,Bontu Fufa Balcha,Debora Taye Tesfaye,Elbethel Daniel Zewdie,Ikram Behiru Nesiru,Jitu Ewnetu Hailu,Senait Mengesha Yayo*

Main category: cs.CL

TL;DR: 本研究调查了三种低资源语言（阿凡奥罗莫语、阿姆哈拉语和提格里尼亚语）的机器翻译 (MT) 数据集的质量，重点关注数据集中的性别表征。


<details>
  <summary>Details</summary>
Motivation: 随着低资源语言越来越多地被纳入自然语言处理研究，人们越来越重视收集大规模数据集。但是，在优先考虑数量而不是质量时，我们可能会构建出对这些语言表现不佳的语言技术，并产生有害的内容，从而 perpetuates 社会偏见。

Method: 我们调查了三种低资源语言（阿凡奥罗莫语、阿姆哈拉语和提格里尼亚语）的机器翻译 (MT) 数据集的质量，重点关注数据集中的性别表征。

Result: 研究结果表明，虽然训练数据中政治和宗教领域文本的代表性很强，但基准数据集侧重于新闻、健康和体育。我们还发现男性性别存在很大的偏差——在人名、动词的语法性别以及数据集中刻板的描述中。此外，我们还发现针对女性的有害和有毒描述，对于数据量最大的语言来说，这些描述更为突出，这 подчеркивает 数量并不能保证质量。

Conclusion: 我们希望我们的工作能够激发人们对为低资源语言收集的数据集进行进一步的探究，并促使人们尽早缓解有害内容。

Abstract: As low-resourced languages are increasingly incorporated into NLP research,
there is an emphasis on collecting large-scale datasets. But in prioritizing
quantity over quality, we risk 1) building language technologies that perform
poorly for these languages and 2) producing harmful content that perpetuates
societal biases. In this paper, we investigate the quality of Machine
Translation (MT) datasets for three low-resourced languages--Afan Oromo,
Amharic, and Tigrinya, with a focus on the gender representation in the
datasets. Our findings demonstrate that while training data has a large
representation of political and religious domain text, benchmark datasets are
focused on news, health, and sports. We also found a large skew towards the
male gender--in names of persons, the grammatical gender of verbs, and in
stereotypical depictions in the datasets. Further, we found harmful and toxic
depictions against women, which were more prominent for the language with the
largest amount of data, underscoring that quantity does not guarantee quality.
We hope that our work inspires further inquiry into the datasets collected for
low-resourced languages and prompts early mitigation of harmful content.
WARNING: This paper contains discussion of NSFW content that some may find
disturbing.

</details>


### [8] [GRAD: Graph-Retrieved Adaptive Decoding for Hallucination Mitigation](https://arxiv.org/abs/2511.03900)
*Manh Nguyen,Sunil Gupta,Dai Do,Hung Le*

Main category: cs.CL

TL;DR: 提出了一种新的解码方法，名为图检索自适应解码 (GRAD)，以减轻大型语言模型 (LLM) 中的幻觉问题，该方法在解码时将生成扎根于语料库衍生的证据中，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有的幻觉缓解方法通常依赖于外部知识来源，例如结构化数据库或知识图，但基于提示的 grounding 方法脆弱且对领域敏感，而符号知识集成会产生繁重的检索和格式化成本。

Method: GRAD 通过在单个前向传递中积累少量检索到的语料库中的下一个 token logits 来构建稀疏 token 转换图。在解码过程中，图检索的 logits 经过最大归一化，并与模型 logits 自适应融合，以支持高证据延续，同时保持流畅性。

Result: 在三个模型和一系列问题回答基准测试中，涵盖内在、外在幻觉和事实性任务，GRAD 始终优于基线，与贪婪解码相比，内在准确率提高了 9.7$\%$，幻觉率降低了 8.6$\%$，正确性提高了 6.9$\%$，同时在所有方法中获得了最高的 truth--informativeness product score。

Conclusion: GRAD 提供了一种轻量级的即插即用替代方案，可以替代对比解码和知识图增强，表明来自语料库级别 token 转换的统计证据可以有效地引导生成朝着更真实和可验证的输出发展。

Abstract: Hallucination mitigation remains a persistent challenge for large language
models (LLMs), even as model scales grow. Existing approaches often rely on
external knowledge sources, such as structured databases or knowledge graphs,
accessed through prompting or retrieval. However, prompt-based grounding is
fragile and domain-sensitive, while symbolic knowledge integration incurs heavy
retrieval and formatting costs. Motivated by knowledge graphs, we introduce
Graph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds
generation in corpus-derived evidence without retraining. GRAD constructs a
sparse token transition graph by accumulating next-token logits across a small
retrieved corpus in a single forward pass. During decoding, graph-retrieved
logits are max-normalized and adaptively fused with model logits to favor
high-evidence continuations while preserving fluency. Across three models and a
range of question-answering benchmarks spanning intrinsic, extrinsic
hallucination, and factuality tasks, GRAD consistently surpasses baselines,
achieving up to 9.7$\%$ higher intrinsic accuracy, 8.6$\%$ lower hallucination
rates, and 6.9$\%$ greater correctness compared to greedy decoding, while
attaining the highest truth--informativeness product score among all methods.
GRAD offers a lightweight, plug-and-play alternative to contrastive decoding
and knowledge graph augmentation, demonstrating that statistical evidence from
corpus-level token transitions can effectively steer generation toward more
truthful and verifiable outputs.

</details>


### [9] [BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation](https://arxiv.org/abs/2511.04153)
*Fahim Ahmed,Md Mubtasim Ahasan,Jahir Sadik Monon,Muntasir Wahed,M Ashraful Amin,A K M Mahbubur Rahman,Amin Ahsan Ali*

Main category: cs.CL

TL;DR: 这篇论文探讨了如何使用多代理LLM流水线来改进Text-to-SQL系统中SQL的生成，特别是针对小型高效模型。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM在处理大型schema和复杂推理时，难以从自然语言指令生成SQL，并且对小型模型的研究不足。

Method: 论文探索了三种多代理LLM流水线：多代理讨论流水线、Planner-Coder流水线和Coder-Aggregator流水线，并对不同大小的开源模型进行了系统性能基准测试。

Result: 实验表明，多代理讨论可以提高小型模型的性能，例如Qwen2.5-7b-Instruct在三轮讨论后，执行准确率提高了10.6%。LLM Reasoner-Coder流水线效果最好，DeepSeek-R1-32B和QwQ-32B planners将Gemma 3 27B IT的准确率从52.4%提高到56.4%。

Conclusion: 多代理LLM流水线可以有效提高小型模型在Text-to-SQL任务中的性能，特别是LLM Reasoner-Coder流水线。

Abstract: Text-to-SQL systems provide a natural language interface that can enable even
laymen to access information stored in databases. However, existing Large
Language Models (LLM) struggle with SQL generation from natural instructions
due to large schema sizes and complex reasoning. Prior work often focuses on
complex, somewhat impractical pipelines using flagship models, while smaller,
efficient models remain overlooked. In this work, we explore three multi-agent
LLM pipelines, with systematic performance benchmarking across a range of small
to large open-source models: (1) Multi-agent discussion pipeline, where agents
iteratively critique and refine SQL queries, and a judge synthesizes the final
answer; (2) Planner-Coder pipeline, where a thinking model planner generates
stepwise SQL generation plans and a coder synthesizes queries; and (3)
Coder-Aggregator pipeline, where multiple coders independently generate SQL
queries, and a reasoning agent selects the best query. Experiments on the
Bird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small
model performance, with up to 10.6% increase in Execution Accuracy for
Qwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines,
the LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B
and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest
score of 56.4%. Codes are available at
https://github.com/treeDweller98/bappa-sql.

</details>


### [10] [Context informs pragmatic interpretation in vision-language models](https://arxiv.org/abs/2511.03908)
*Alvin Wei Ming Tan,Ben Prystawski,Veronica Boyce,Michael C. Frank*

Main category: cs.CL

TL;DR: 本文研究了迭代指称游戏中，智能体在多轮语言环境中执行上下文敏感的语用推理能力。


<details>
  <summary>Details</summary>
Motivation: 探究智能体在不同上下文条件下，利用语言选择新颖指称对象的能力。

Method: 在迭代指称游戏中测试人类和视觉-语言模型，并改变上下文的数量、顺序和相关性。

Result: 在没有相关上下文的情况下，模型表现优于随机水平但远逊于人类；在有相关上下文的情况下，模型性能随试验次数显著提高。

Conclusion: 对于机器学习模型来说，使用抽象指称对象的Few-shot指称游戏仍然是一项艰巨的任务。

Abstract: Iterated reference games - in which players repeatedly pick out novel
referents using language - present a test case for agents' ability to perform
context-sensitive pragmatic reasoning in multi-turn linguistic environments. We
tested humans and vision-language models on trials from iterated reference
games, varying the given context in terms of amount, order, and relevance.
Without relevant context, models were above chance but substantially worse than
humans. However, with relevant context, model performance increased
dramatically over trials. Few-shot reference games with abstract referents
remain a difficult task for machine learning models.

</details>


### [11] [The Human Flourishing Geographic Index: A County-Level Dataset for the United States, 2013--2023](https://arxiv.org/abs/2511.03915)
*Stefano M. Iacus,Devika Jain,Andrea Nasuto,Giuseppe Porro,Marcello Carammia,Andrea Vezzulli*

Main category: cs.CL

TL;DR: 本文提出了人类繁荣地理指数（HFGI），通过分析美国地理位置的推文来衡量幸福感、健康、目标、美德、人际关系和财务稳定性等多维度指标，以补充经济指标之外的社会福祉理解。


<details>
  <summary>Details</summary>
Motivation: 现有的幸福感衡量标准缺乏精细的时空分辨率，无法充分理解社会福祉。

Method: 利用微调的大型语言模型分析2013-2023年间大约26亿条美国地理位置的推文，对与哈佛全球繁荣研究框架相关的48个指标以及对移民的态度和腐败的看法进行分类，从而得出HFGI。

Result: 该数据集提供月度和年度的县和州级别指标，验证表明这些指标准确地代表了潜在的结构，并显示出与已建立指标的预期相关性。

Conclusion: HFGI能够以空前的分辨率对福祉、不平等和社会变革进行多学科分析，从而深入了解过去十年中美国社交媒体 дискурс 中反映的人类繁荣动态。

Abstract: Quantifying human flourishing, a multidimensional construct including
happiness, health, purpose, virtue, relationships, and financial stability, is
critical for understanding societal well-being beyond economic indicators.
Existing measures often lack fine spatial and temporal resolution. Here we
introduce the Human Flourishing Geographic Index (HFGI), derived from analyzing
approximately 2.6 billion geolocated U.S. tweets (2013-2023) using fine-tuned
large language models to classify expressions across 48 indicators aligned with
Harvard's Global Flourishing Study framework plus attitudes towards migration
and perception of corruption. The dataset offers monthly and yearly county- and
state-level indicators of flourishing-related discourse, validated to confirm
that the measures accurately represent the underlying constructs and show
expected correlations with established indicators. This resource enables
multidisciplinary analyses of well-being, inequality, and social change at
unprecedented resolution, offering insights into the dynamics of human
flourishing as reflected in social media discourse across the United States
over the past decade.

</details>


### [12] [RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables](https://arxiv.org/abs/2511.04491)
*Nikhil Abhyankar,Purvi Chaurasia,Sanchit Kabra,Ananya Srivastava,Vivek Gupta,Chandan K. Reddy*

Main category: cs.CL

TL;DR: RUST-BENCH是一个新的表格推理基准，它使用来自现实世界的更复杂的数据来评估大型语言模型（LLM）。


<details>
  <summary>Details</summary>
Motivation: 现有的表格推理基准不能充分代表真实世界数据的复杂性，因此需要一个新的基准来更全面地评估LLM的推理能力。

Method: 引入RUST-BENCH，一个包含来自两个领域的2031个真实世界表格的7966个问题的基准：RB-Science（NSF资助记录）和RB-Sports（NBA统计数据）。

Result: 实验表明，LLM在异构模式和复杂的多跳推理方面存在困难。

Conclusion: RUST-BENCH 为推进表格推理研究建立了一个具有挑战性的新测试平台。

Abstract: Existing tabular reasoning benchmarks mostly test models on small, uniform
tables, underrepresenting the complexity of real-world data and giving an
incomplete view of Large Language Models' (LLMs) reasoning abilities. Real
tables are long, heterogeneous, and domain-specific, mixing structured fields
with free text and requiring multi-hop reasoning across thousands of tokens. To
address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from
2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)
and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates
LLMs jointly across scale, heterogeneity, domain specificity, and reasoning
complexity. Experiments with open-source and proprietary models show that LLMs
struggle with heterogeneous schemas and complex multi-hop inference, revealing
persistent weaknesses in current architectures and prompting strategies.
RUST-BENCH establishes a challenging new testbed for advancing tabular
reasoning research.

</details>


### [13] [Direct Semantic Communication Between Large Language Models via Vector Translation](https://arxiv.org/abs/2511.03945)
*Fu-Chun Yang,Jason Eshraghian*

Main category: cs.CL

TL;DR: 大型语言模型在多智能体环境中传递消息时丢弃了大部分潜在语义，限制了信息传递并增加了计算开销。本文提出了一种通过向量转换形成潜在桥梁的方法，实现了表征空间之间的直接语义交换。


<details>
  <summary>Details</summary>
Motivation: 在多智能体环境中，大型语言模型之间的信息传递受限于纯文本令牌，缺乏深层语义信息的有效共享。

Method: 通过学习到的映射，使用向量转换在Llama-2-7B和Mistral-7B-Instruct之间建立潜在桥梁，实现直接的语义交换。训练了一个双编码器翻译器，并在目标模型中注入翻译后的向量。

Result: 双向评估显示出2.01:1的转移不对称性，表明通用模型比指令调整变体产生更可转移的表示。保守注入在保持计算稳定性的同时，证明了跨模型潜在通信的可行性。

Conclusion: 本文验证了跨模型潜在通信是可行的，从而使协作AI系统能够共享意义而不是令牌。

Abstract: In multi-agent settings, such as debate, reflection, or tool-calling, large
language models (LLMs) pass messages as plain tokens, discarding most latent
semantics. This constrains information transfer and adds unnecessary
computational overhead. We form a latent bridge via vector translations, which
use learned mappings that enable direct semantic exchange between
representation spaces. A dual-encoder translator trained between Llama-2-7B and
Mistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the
translated vectors at 30 percent blending strength steers the target model's
generation without destabilizing logits. Bidirectional evaluation shows a
2.01:1 transfer asymmetry, indicating that general-purpose models yield more
transferable representations than instruction-tuned variants. This conservative
injection preserves computational stability while demonstrating that
cross-model latent communication is feasible, enabling collaborative AI systems
that share meaning rather than tokens.

</details>


### [14] [Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises](https://arxiv.org/abs/2511.04020)
*Shiyin Lin*

Main category: cs.CL

TL;DR: 这篇论文提出了一种通过溯因推理增强检索增强语言模型（RAG）的方法，以解决检索到的证据不完整时RAG管道失效的问题。


<details>
  <summary>Details</summary>
Motivation: RAG在知识密集型任务中表现出色，但当检索到的证据不完整时会失败，留下推理过程中的空白。

Method: 该方法通过检测证据不足，生成候选缺失前提，并通过一致性和合理性检查来验证它们，从而将溯因推理集成到RAG中。

Result: 在溯因推理和多跳QA基准测试中的实验结果表明，该方法提高了答案的准确性和推理的真实性。

Conclusion: 该研究强调了溯因推理是提高RAG系统鲁棒性和可解释性的一个有希望的方向。

Abstract: Large Language Models (LLMs) enhanced with retrieval -- commonly referred to
as Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance
in knowledge-intensive tasks. However, RAG pipelines often fail when retrieved
evidence is incomplete, leaving gaps in the reasoning process. In such cases,
\emph{abductive inference} -- the process of generating plausible missing
premises to explain observations -- offers a principled approach to bridge
these gaps. In this paper, we propose a framework that integrates abductive
inference into retrieval-augmented LLMs. Our method detects insufficient
evidence, generates candidate missing premises, and validates them through
consistency and plausibility checks. Experimental results on abductive
reasoning and multi-hop QA benchmarks show that our approach improves both
answer accuracy and reasoning faithfulness. This work highlights abductive
inference as a promising direction for enhancing the robustness and
explainability of RAG systems.

</details>


### [15] [WST: Weakly Supervised Transducer for Automatic Speech Recognition](https://arxiv.org/abs/2511.04035)
*Dongji Gao,Chenda Liao,Changliang Liu,Matthew Wiesner,Leibny Paola Garcia,Daniel Povey,Sanjeev Khudanpur,Jian Wu*

Main category: cs.CL

TL;DR: 提出了一种弱监督Transducer（WST），以减少对大规模高质量标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有的语音识别模型依赖于大量高质量标注数据，获取成本高昂。

Method: 设计了一个灵活的训练图，可以鲁棒地处理转录中的错误，无需额外的置信度估计或辅助预训练模型。

Result: 在合成和工业数据集上的评估表明，即使转录错误率高达70%，WST 仍能有效保持性能，并且始终优于现有的基于 CTC 的弱监督方法。

Conclusion: WST 在实际 ASR 设置中具有实用性和鲁棒性。

Abstract: The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in
end-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily
on large-scale, high-quality annotated data, which are often costly and
difficult to obtain. To mitigate this reliance, we propose a Weakly Supervised
Transducer (WST), which integrates a flexible training graph designed to
robustly handle errors in the transcripts without requiring additional
confidence estimation or auxiliary pre-trained models. Empirical evaluations on
synthetic and industrial datasets reveal that WST effectively maintains
performance even with transcription error rates of up to 70%, consistently
outperforming existing Connectionist Temporal Classification (CTC)-based weakly
supervised approaches, such as Bypass Temporal Classification (BTC) and
Omni-Temporal Classification (OTC). These results demonstrate the practical
utility and robustness of WST in realistic ASR settings. The implementation
will be publicly available.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [16] [LoRA-Edge: Tensor-Train-Assisted LoRA for Practical CNN Fine-Tuning on Edge Devices](https://arxiv.org/abs/2511.03765)
*Hyunseok Kwak,Kyeongwon Lee,Jae-Jin Lee,Woojoo Lee*

Main category: cs.CV

TL;DR: LoRA-Edge: A parameter-efficient fine-tuning method for CNNs on edge devices, achieving comparable accuracy to full fine-tuning with significantly fewer trainable parameters.


<details>
  <summary>Details</summary>
Motivation: Full fine-tuning of CNNs on edge devices is infeasible due to resource constraints, but on-device fine-tuning is necessary to adapt to domain shift.

Method: LoRA-Edge applies Tensor-Train SVD to pre-trained convolutional layers, selectively updates only the output-side core with zero-initialization, and fuses the update back into dense kernels.

Result: LoRA-Edge achieves accuracy within 4.7% of full fine-tuning while updating at most 1.49% of parameters, outperforming prior parameter-efficient baselines. It also yields 1.4-3.8x faster convergence.

Conclusion: LoRA-Edge enables practical structure-aligned, parameter-efficient on-device CNN adaptation for edge platforms.

Abstract: On-device fine-tuning of CNNs is essential to withstand domain shift in edge
applications such as Human Activity Recognition (HAR), yet full fine-tuning is
infeasible under strict memory, compute, and energy budgets. We present
LoRA-Edge, a parameter-efficient fine-tuning (PEFT) method that builds on
Low-Rank Adaptation (LoRA) with tensor-train assistance. LoRA-Edge (i) applies
Tensor-Train Singular Value Decomposition (TT-SVD) to pre-trained convolutional
layers, (ii) selectively updates only the output-side core with
zero-initialization to keep the auxiliary path inactive at the start, and (iii)
fuses the update back into dense kernels, leaving inference cost unchanged.
This design preserves convolutional structure and reduces the number of
trainable parameters by up to two orders of magnitude compared to full
fine-tuning. Across diverse HAR datasets and CNN backbones, LoRA-Edge achieves
accuracy within 4.7% of full fine-tuning while updating at most 1.49% of
parameters, consistently outperforming prior parameter-efficient baselines
under similar budgets. On a Jetson Orin Nano, TT-SVD initialization and
selective-core training yield 1.4-3.8x faster convergence to target F1.
LoRA-Edge thus makes structure-aligned, parameter-efficient on-device CNN
adaptation practical for edge platforms.

</details>


### [17] [SILVI: Simple Interface for Labeling Video Interactions](https://arxiv.org/abs/2511.03819)
*Ozan Kanbertay,Richard Vogg,Elif Karakoc,Peter M. Kappeler,Claudia Fichtel,Alexander S. Ecker*

Main category: cs.CV

TL;DR: SILVI是一个开源标注软件，集成了行为标注和个体定位功能，旨在促进动物行为的自动分析。


<details>
  <summary>Details</summary>
Motivation: 现有的开源标注工具要么支持行为标注但缺乏个体定位，要么支持个体定位但无法捕捉互动，这阻碍了对动物社会和个体行为的理解。

Method: 提出了SILVI，一个集成了行为标注和个体定位功能的开源标注软件。

Result: SILVI能够直接在视频数据中注释行为和交互，生成结构化输出，适用于训练和验证计算机视觉模型。通过连接行为生态学和计算机视觉，SILVI有助于开发用于细粒度行为分析的自动化方法。

Conclusion: SILVI主要为动物行为研究开发，但也可用于注释需要提取动态场景图的人类互动视频。

Abstract: Computer vision methods are increasingly used for the automated analysis of
large volumes of video data collected through camera traps, drones, or direct
observations of animals in the wild. While recent advances have focused
primarily on detecting individual actions, much less work has addressed the
detection and annotation of interactions -- a crucial aspect for understanding
social and individualized animal behavior. Existing open-source annotation
tools support either behavioral labeling without localization of individuals,
or localization without the capacity to capture interactions. To bridge this
gap, we present SILVI, an open-source labeling software that integrates both
functionalities. SILVI enables researchers to annotate behaviors and
interactions directly within video data, generating structured outputs suitable
for training and validating computer vision models. By linking behavioral
ecology with computer vision, SILVI facilitates the development of automated
approaches for fine-grained behavioral analyses. Although developed primarily
in the context of animal behavior, SILVI could be useful more broadly to
annotate human interactions in other videos that require extracting dynamic
scene graphs. The software, along with documentation and download instructions,
is available at: https://gitlab.gwdg.de/kanbertay/interaction-labelling-app.

</details>


### [18] [Improving Diagnostic Performance on Small and Imbalanced Datasets Using Class-Based Input Image Composition](https://arxiv.org/abs/2511.03891)
*Hlali Azzeddine,Majid Ben Yakhlef,Soulaiman El Hazzat*

Main category: cs.CV

TL;DR: 提出了一种名为基于类别的图像合成(Class-Based Image Composition)的方法，通过融合同一类别的多个图像来重新构建训练输入，从而提高模型区分细微疾病模式的能力。


<details>
  <summary>Details</summary>
Motivation: 小型的、不平衡的数据集和较差的输入图像质量会导致深度学习模型出现较高的错误预测率。

Method: 将同一类别的多个图像融合成组合的视觉合成图像，称为合成输入图像(CoImg)。在OCTDL数据集上，构建了一个完美类别平衡的版本Co-OCTDL，其中每个扫描都表示为一个3x1布局的合成图像。使用VGG16模型在原始数据集和Co-OCTDL数据集上进行了比较分析。

Result: Co-OCTDL数据集实现了接近完美的准确率(99.6%)，F1-score为0.995，AUC为0.9996，与在原始数据集上训练的基线模型相比，错误预测率也显著降低。

Conclusion: 该方法即使对于受类别不平衡或小样本量影响的弱数据集，也能产生高质量的预测。

Abstract: Small, imbalanced datasets and poor input image quality can lead to high
false predictions rates with deep learning models. This paper introduces
Class-Based Image Composition, an approach that allows us to reformulate
training inputs through a fusion of multiple images of the same class into
combined visual composites, named Composite Input Images (CoImg). That enhances
the intra-class variance and improves the valuable information density per
training sample and increases the ability of the model to distinguish between
subtle disease patterns. Our method was evaluated on the Optical Coherence
Tomography Dataset for Image-Based Deep Learning Methods (OCTDL) (Kulyabin et
al., 2024), which contains 2,064 high-resolution optical coherence tomography
(OCT) scans of the human retina, representing seven distinct diseases with a
significant class imbalance. We constructed a perfectly class-balanced version
of this dataset, named Co-OCTDL, where each scan is resented as a 3x1 layout
composite image. To assess the effectiveness of this new representation, we
conducted a comparative analysis between the original dataset and its variant
using a VGG16 model. A fair comparison was ensured by utilizing the identical
model architecture and hyperparameters for all experiments. The proposed
approach markedly improved diagnostic results.The enhanced Dataset achieved
near-perfect accuracy (99.6%) with F1-score (0.995) and AUC (0.9996), compared
to a baseline model trained on raw dataset. The false prediction rate was also
significantly lower, this demonstrates that the method can producehigh-quality
predictions even for weak datasets affected by class imbalance or small sample
size.

</details>


### [19] [Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets](https://arxiv.org/abs/2511.03855)
*Duong Mai,Lawrence Hall*

Main category: cs.CV

TL;DR: 深度学习模型在图像识别方面泛化能力差，尤其是在COVID-19胸部X光检测中。模型学习利用源特定伪像，而不是合理的生物标志物。本研究调查了在训练期间使用噪声注入技术，以提高模型对分布偏移的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 模型学习利用源特定伪像，而不是合理的生物标志物，导致无法泛化到新的临床来源数据。

Method: 在训练期间使用高斯、散斑、泊松和椒盐噪声注入技术。

Result: 该技术可显著缩小ID和OOD评估之间的性能差距，从0.10-0.20降至0.01-0.06。

Conclusion: 噪声注入技术可以提高模型对分布偏移的鲁棒性。

Abstract: Deep learned (DL) models for image recognition have been shown to fail to
generalize to data from different devices, populations, etc. COVID-19 detection
from Chest X-rays (CXRs), in particular, has been shown to fail to generalize
to out-of-distribution (OOD) data from new clinical sources not covered in the
training set. This occurs because models learn to exploit shortcuts -
source-specific artifacts that do not translate to new distributions - rather
than reasonable biomarkers to maximize performance on in-distribution (ID)
data. Rendering the models more robust to distribution shifts, our study
investigates the use of fundamental noise injection techniques (Gaussian,
Speckle, Poisson, and Salt and Pepper) during training. Our empirical results
demonstrate that this technique can significantly reduce the performance gap
between ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results
averaged over ten random seeds across key metrics such as AUC, F1, accuracy,
recall and specificity. Our source code is publicly available at
https://github.com/Duongmai127/Noisy-ood

</details>


### [20] [Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures](https://arxiv.org/abs/2511.03882)
*Florence Klitzner,Blanca Inigo,Benjamin D. Killeen,Lalithkumar Seenivasan,Michelle Song,Axel Krieger,Mathias Unberath*

Main category: cs.CV

TL;DR: 本研究探讨了基于模仿学习的机器人控制策略在X射线引导下的脊柱器械植入手术中的应用。通过开发一个模拟环境，生成数据集并训练模仿学习策略，实现了基于视觉信息的导管对齐。结果表明该策略在一定程度上是成功的，但也存在局限性，尤其是在入口点精度方面。


<details>
  <summary>Details</summary>
Motivation: 现有基于模仿学习的机器人控制策略在视频机器人领域备受关注，但其在X射线引导手术中的应用尚不明确，因为多角度X射线的解读非常复杂。本研究旨在探索其在双平面引导下的套管插入中的机会和挑战。

Method: 1. 开发一个用于模拟X射线引导脊柱手术的计算机环境。
2. 整理一个包含正确轨迹和相应的双平面X射线序列的数据集，模拟医生的逐步对齐过程。
3. 训练模仿学习策略，用于规划和开环控制，仅基于视觉信息迭代对齐套管。

Result: 该策略在68.5%的案例中首次尝试成功，并在不同的椎体水平上保持安全的椎弓根内轨迹。该策略推广到复杂的解剖结构，包括骨折，并且对不同的初始化保持鲁棒性。在真实双平面X射线上的实验表明，该模型可以产生合理的轨迹，尽管仅在模拟环境中进行训练。

Conclusion: 初步结果表明，基于模仿学习的策略在X射线引导脊柱手术中具有潜力，但也存在入口点精度等局限性。未来的研究需要考虑如何提供足够频繁的反馈，并结合更强的先验知识和领域知识，以实现更稳健的闭环控制，为未来的轻量级和无CT的机器人术中脊柱导航奠定基础。

Abstract: Imitation learning-based robot control policies are enjoying renewed interest
in video-based robotics. However, it remains unclear whether this approach
applies to X-ray-guided procedures, such as spine instrumentation. This is
because interpretation of multi-view X-rays is complex. We examine
opportunities and challenges for imitation policy learning in bi-plane-guided
cannula insertion. We develop an in silico sandbox for scalable, automated
simulation of X-ray-guided spine procedures with a high degree of realism. We
curate a dataset of correct trajectories and corresponding bi-planar X-ray
sequences that emulate the stepwise alignment of providers. We then train
imitation learning policies for planning and open-loop control that iteratively
align a cannula solely based on visual information. This precisely controlled
setup offers insights into limitations and capabilities of this method. Our
policy succeeded on the first attempt in 68.5% of cases, maintaining safe
intra-pedicular trajectories across diverse vertebral levels. The policy
generalized to complex anatomy, including fractures, and remained robust to
varied initializations. Rollouts on real bi-planar X-rays further suggest that
the model can produce plausible trajectories, despite training exclusively in
simulation. While these preliminary results are promising, we also identify
limitations, especially in entry point precision. Full closed-look control will
require additional considerations around how to provide sufficiently frequent
feedback. With more robust priors and domain knowledge, such models may provide
a foundation for future efforts toward lightweight and CT-free robotic
intra-operative spinal navigation.

</details>


### [21] [Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model](https://arxiv.org/abs/2511.03888)
*Abdulmumin Sa'ad,Sulaimon Oyeniyi Adebayo,Abdul Jabbar Siddiqui*

Main category: cs.CV

TL;DR: 提出了一种用于沙漠环境的实时垃圾检测框架，该框架基于YOLOv12的轻量级版本，并结合了自对抗训练（SAT）和专门的数据增强策略。


<details>
  <summary>Details</summary>
Motivation: 传统的垃圾收集方法在偏远或恶劣的环境中劳动密集、效率低下且通常具有危险性。大多数研究侧重于城市环境和可回收材料，忽略了有机和有害废物以及未充分开发的 terrain，例如沙漠。

Method: 基于YOLOv12的修剪轻量级版本，并结合了自对抗训练（SAT）和专门的数据增强策略，提出了一个增强的实时对象检测框架。

Result: 在 DroneTrashNet 数据集上，展示了在精度、召回率和平均精度均值 (mAP) 方面的显着改进，同时实现了低延迟和紧凑的模型尺寸，适合部署在资源受限的空中无人机上。将我们的模型与最先进的轻量级 YOLO 变体进行基准测试进一步凸显了其在准确性和效率方面的最佳平衡。

Conclusion: 我们的结果验证了结合以数据为中心和以模型为中心的增强功能对于在沙漠环境中进行鲁棒的实时废物检测的有效性。

Abstract: The global waste crisis is escalating, with solid waste generation expected
to increase by 70% by 2050. Traditional waste collection methods, particularly
in remote or harsh environments like deserts, are labor-intensive, inefficient,
and often hazardous. Recent advances in computer vision and deep learning have
opened the door to automated waste detection systems, yet most research focuses
on urban environments and recyclable materials, overlooking organic and
hazardous waste and underexplored terrains such as deserts. In this work, we
propose an enhanced real-time object detection framework based on a pruned,
lightweight version of YOLOv12 integrated with Self-Adversarial Training (SAT)
and specialized data augmentation strategies. Using the DroneTrashNet dataset,
we demonstrate significant improvements in precision, recall, and mean average
precision (mAP), while achieving low latency and compact model size suitable
for deployment on resource-constrained aerial drones. Benchmarking our model
against state-of-the-art lightweight YOLO variants further highlights its
optimal balance of accuracy and efficiency. Our results validate the
effectiveness of combining data-centric and model-centric enhancements for
robust, real-time waste detection in desert environments.

</details>


### [22] [I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging](https://arxiv.org/abs/2511.03912)
*Nand Kumar Yadav,Rodrigue Rizk,William CW Chen,KC Santosh*

Main category: cs.CV

TL;DR: 提出了一种无监督的异常检测框架，通过增量扩展正常样本集来提高检测性能，无需异常标签。


<details>
  <summary>Details</summary>
Motivation: 医学影像中带标签的异常数据稀缺，专家监督成本高昂，因此需要一种无需标签的异常检测方法。

Method: 该方法从一个小的正常图像种子集开始，交替进行轻量级适配器更新和不确定性门控样本准入。利用预训练的视觉骨干网络和卷积适配器进行快速领域自适应，并通过双重概率门控机制保证增量扩展的安全性。

Result: 在COVID-CXR、Pneumonia CXR和Brain MRI ND-5数据集上，ROC-AUC和PR-AUC指标均得到显著提升。

Conclusion: 该框架在标签稀缺的医学影像应用中有效且高效。

Abstract: Unknown anomaly detection in medical imaging remains a fundamental challenge
due to the scarcity of labeled anomalies and the high cost of expert
supervision. We introduce an unsupervised, oracle-free framework that
incrementally expands a trusted set of normal samples without any anomaly
labels. Starting from a small, verified seed of normal images, our method
alternates between lightweight adapter updates and uncertainty-gated sample
admission. A frozen pretrained vision backbone is augmented with tiny
convolutional adapters, ensuring rapid domain adaptation with negligible
computational overhead. Extracted embeddings are stored in a compact coreset
enabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during
incremental expansion is enforced by dual probabilistic gates, a sample is
admitted into the normal memory only if its distance to the existing coreset
lies within a calibrated z-score threshold, and its SWAG-based epistemic
uncertainty remains below a seed-calibrated bound. This mechanism prevents
drift and false inclusions without relying on generative reconstruction or
replay buffers. Empirically, our system steadily refines the notion of
normality as unlabeled data arrive, producing substantial gains over baselines.
On COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on
Pneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5,
ROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These
results highlight the effectiveness and efficiency of the proposed framework
for real-world, label-scarce medical imaging applications.

</details>


### [23] [Adaptive Temporal Refinement: Continuous Depth Allocation and Distance Regression for Efficient Action Localization](https://arxiv.org/abs/2511.03943)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.CV

TL;DR: 提出了一种新的时间动作定位方法，该方法通过边界距离回归（BDR）和自适应时间细化（ATR）来提高边界检测的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 当前的时间动作定位方法在处理不同难度的边界时采用统一的计算方式，效率低下。

Method: 1. 边界距离回归（BDR）：通过有符号距离回归实现信息论最优定位，提高边界峰值的锐度。2. 自适应时间细化（ATR）：通过连续深度选择分配计算资源，实现端到端可微优化。

Result: 1. BDR 在各种架构上实现了 1.8% 到 3.1% 的 mAP@0.7 提升。2. ATR 在 THUMOS14 上以更少的计算量实现了更高的 mAP@0.7，并在短动作上表现出更大的改进。

Conclusion: 提出的 BDR 和 ATR 方法能够有效提高时间动作定位的精度和效率，并在多个基准测试中得到验证。

Abstract: Temporal action localization requires precise boundary detection; however,
current methods apply uniform computation despite significant variations in
difficulty across boundaries. We present two complementary contributions.
First, Boundary Distance Regression (BDR) provides information-theoretically
optimal localization through signed-distance regression rather than
classification, achieving 43\% sharper boundary peaks. BDR retrofits to
existing methods with approximately 50 lines of code, yielding consistent 1.8
to 3.1\% mAP@0.7 improvements across diverse architectures. Second, Adaptive
Temporal Refinement (ATR) allocates computation via continuous depth selection
$\tau \in [0,1]$, enabling end-to-end differentiable optimization without
reinforcement learning. On THUMOS14, ATR achieves 56.5\% mAP@0.7 at 162G FLOPs,
compared to 53.6\% at 198G for uniform processing, providing a 2.9\%
improvement with 18\% less compute. Gains scale with boundary heterogeneity,
showing 4.2\% improvement on short actions. Training cost is mitigated via
knowledge distillation, with lightweight students retaining 99\% performance at
baseline cost. Results are validated across four benchmarks with rigorous
statistical testing.

</details>


### [24] [Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization](https://arxiv.org/abs/2511.03950)
*Zhejia Cai,Puhua Jiang,Shiwei Mao,Hongkun Cao,Ruqi Huang*

Main category: cs.CV

TL;DR: 提出了一种用于从多视图图像重建真实世界物体的框架，该框架同时优化网格几何体和顶点颜色，以实现无缝的高斯-网格联合优化。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常优先考虑几何精度或照片般真实的渲染，经常将几何和外观优化分离，这阻碍了下游编辑任务。

Method: 通过高斯引导的网格可微渲染，利用来自输入图像的光度一致性和来自法线和深度图的几何正则化，同时优化网格几何体（顶点位置和面）和顶点颜色。

Result: 获得高质量的 3D 重建，可进一步用于下游编辑任务，例如重新照明和形状变形。

Conclusion: 倡导对几何和外观优化进行统一处理，以实现无缝的高斯-网格联合优化。

Abstract: Reconstructing real-world objects from multi-view images is essential for
applications in 3D editing, AR/VR, and digital content creation. Existing
methods typically prioritize either geometric accuracy (Multi-View Stereo) or
photorealistic rendering (Novel View Synthesis), often decoupling geometry and
appearance optimization, which hinders downstream editing tasks. This paper
advocates an unified treatment on geometry and appearance optimization for
seamless Gaussian-mesh joint optimization. More specifically, we propose a
novel framework that simultaneously optimizes mesh geometry (vertex positions
and faces) and vertex colors via Gaussian-guided mesh differentiable rendering,
leveraging photometric consistency from input images and geometric
regularization from normal and depth maps. The obtained high-quality 3D
reconstruction can be further exploit in down-stream editing tasks, such as
relighting and shape deformation. The code will be publicly available upon
acceptance.

</details>


### [25] [A Linear Fractional Transformation Model and Calibration Method for Light Field Camera](https://arxiv.org/abs/2511.03962)
*Zhong Chen,Changfeng Chen*

Main category: cs.CV

TL;DR: 提出了一种新的光场相机标定方法，通过解耦主镜头和微透镜阵列来提高标定精度和效率，并提供了代码。


<details>
  <summary>Details</summary>
Motivation: 精确标定内参数是光场相机进行3D重建的关键前提，但具有挑战性。

Method: 提出线性分式变换(LFT)参数α来解耦主镜头和微透镜阵列，包括基于最小二乘的解析解和非线性细化，并介绍了从原始图像中检测特征的方法。

Result: 在物理和模拟数据上的实验结果验证了所提出方法的性能。基于该模型，原始光场图像的模拟速度更快。

Conclusion: 该方法提高了光场相机标定的精度和效率，并且加速了光场图像的模拟，对数据驱动的深度学习方法至关重要。

Abstract: Accurate calibration of internal parameters is a crucial yet challenging
prerequisite for 3D reconstruction using light field cameras. In this paper, we
propose a linear fractional transformation(LFT) parameter $\alpha$ to decoupled
the main lens and micro lens array (MLA). The proposed method includes an
analytical solution based on least squares, followed by nonlinear refinement.
The method for detecting features from the raw images is also introduced.
Experimental results on both physical and simulated data have verified the
performance of proposed method. Based on proposed model, the simulation of raw
light field images becomes faster, which is crucial for data-driven deep
learning methods. The corresponding code can be obtained from the author's
website.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [26] [Scaling Agent Learning via Experience Synthesis](https://arxiv.org/abs/2511.03773)
*Zhaorun Chen,Zhuokai Zhao,Kai Zhang,Bo Liu,Qi Qi,Yifan Wu,Tarun Kalluri,Sara Cao,Yuanhao Xiong,Haibo Tong,Huaxiu Yao,Hengduo Li,Jiacheng Zhu,Xian Li,Dawn Song,Bo Li,Jason Weston,Dat Huynh*

Main category: cs.AI

TL;DR: DreamGym是一个统一的框架，旨在合成多样化的经验，并通过基于推理的经验模型，实现自主代理的有效在线强化学习（RL）训练。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）虽然可以通过交互实现自我改进，从而增强大型语言模型（LLM）代理的能力，但由于代价高昂的rollout、有限的任务多样性、不可靠的奖励信号以及基础设施的复杂性，其实际应用仍然具有挑战性，所有这些都阻碍了可扩展的经验数据的收集。

Method: DreamGym将环境动态提炼成一个基于推理的经验模型，该模型通过逐步推理得出一致的状态转换和反馈信号，从而实现用于RL的可扩展代理rollout收集。为了提高转换的稳定性和质量，DreamGym利用一个用离线真实世界数据初始化的经验回放缓冲区，并不断用新的交互来丰富该缓冲区，以积极支持代理训练。为了提高知识获取，DreamGym自适应地生成新的任务，以挑战当前的代理策略，从而实现更有效的在线课程学习。

Result: 在不同的环境和代理骨干上的实验表明，DreamGym大大改进了RL训练，无论是在完全合成的设置中还是在sim-to-real的转移场景中。在像WebArena这样非RL-ready的任务中，DreamGym的性能超过了所有基线30%以上。在RL-ready但成本高昂的设置中，它仅使用合成交互就能与GRPO和PPO的性能相匹配。当将完全在合成经验上训练的策略转移到真实环境RL时，DreamGym在需要更少的真实世界交互的同时，产生了显著的额外性能提升，为通用RL提供了一种可扩展的warm-start策略。

Conclusion: DreamGym通过合成经验和基于推理的建模，显著提升了强化学习的效率和效果，尤其是在计算成本高昂和任务多样性有限的场景中，为通用RL提供了一种可扩展的解决方案。

Abstract: While reinforcement learning (RL) can empower large language model (LLM)
agents by enabling self-improvement through interaction, its practical adoption
remains challenging due to costly rollouts, limited task diversity, unreliable
reward signals, and infrastructure complexity, all of which obstruct the
collection of scalable experience data. To address these challenges, we
introduce DreamGym, the first unified framework designed to synthesize diverse
experiences with scalability in mind to enable effective online RL training for
autonomous agents. Rather than relying on expensive real-environment rollouts,
DreamGym distills environment dynamics into a reasoning-based experience model
that derives consistent state transitions and feedback signals through
step-by-step reasoning, enabling scalable agent rollout collection for RL. To
improve the stability and quality of transitions, DreamGym leverages an
experience replay buffer initialized with offline real-world data and
continuously enriched with fresh interactions to actively support agent
training. To improve knowledge acquisition, DreamGym adaptively generates new
tasks that challenge the current agent policy, enabling more effective online
curriculum learning. Experiments across diverse environments and agent
backbones demonstrate that DreamGym substantially improves RL training, both in
fully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready
tasks like WebArena, DreamGym outperforms all baselines by over 30%. And in
RL-ready but costly settings, it matches GRPO and PPO performance using only
synthetic interactions. When transferring a policy trained purely on synthetic
experiences to real-environment RL, DreamGym yields significant additional
performance gains while requiring far fewer real-world interactions, providing
a scalable warm-start strategy for general-purpose RL.

</details>


### [27] [How Different Tokenization Algorithms Impact LLMs and Transformer Models for Binary Code Analysis](https://arxiv.org/abs/2511.03825)
*Ahmed Mostafa,Raisul Arefin Nahid,Samuel Mulder*

Main category: cs.AI

TL;DR: 本研究探讨了汇编代码分析中tokenization的重要性，并评估了NLP tokenization模型和参数选择对汇编代码的影响。


<details>
  <summary>Details</summary>
Motivation: 汇编代码的tokenization是一个未被充分研究的领域，但它对词汇量大小、语义覆盖和下游任务的性能有重要影响。本研究旨在填补这一空白。

Method: 本研究对各种tokenization模型进行了全面的研究，系统地分析了它们在编码汇编指令和捕捉语义细微之处方面的效率。通过内在评估，我们比较了基于tokenization效率、词汇压缩和汇编代码表示保真度的tokenizer。使用最先进的预训练模型，如decoder-only的LLM Llama 3.2、encoder-only的transformer BERT和encoder-decoder模型BART，我们评估了这些tokenizer在多个性能指标上的有效性。

Result: 初步研究结果表明，tokenizer的选择显著影响下游性能，内在指标提供了外在评估结果的部分但不完整的可预测性。

Conclusion: 本研究为优化低级代码分析的tokenization模型提供了有价值的见解，有助于提高基于自然语言模型的二进制分析工作流程的鲁棒性和可扩展性。

Abstract: Tokenization is fundamental in assembly code analysis, impacting intrinsic
characteristics like vocabulary size, semantic coverage, and extrinsic
performance in downstream tasks. Despite its significance, tokenization in the
context of assembly code remains an underexplored area. This study aims to
address this gap by evaluating the intrinsic properties of Natural Language
Processing (NLP) tokenization models and parameter choices, such as vocabulary
size. We explore preprocessing customization options and pre-tokenization rules
tailored to the unique characteristics of assembly code. Additionally, we
assess their impact on downstream tasks like function signature prediction -- a
critical problem in binary code analysis.
  To this end, we conduct a thorough study on various tokenization models,
systematically analyzing their efficiency in encoding assembly instructions and
capturing semantic nuances. Through intrinsic evaluations, we compare
tokenizers based on tokenization efficiency, vocabulary compression, and
representational fidelity for assembly code. Using state-of-the-art pre-trained
models such as the decoder-only Large Language Model (LLM) Llama 3.2, the
encoder-only transformer BERT, and the encoder-decoder model BART, we evaluate
the effectiveness of these tokenizers across multiple performance metrics.
Preliminary findings indicate that tokenizer choice significantly influences
downstream performance, with intrinsic metrics providing partial but incomplete
predictability of extrinsic evaluation outcomes. These results reveal complex
trade-offs between intrinsic tokenizer properties and their utility in
practical assembly code tasks. Ultimately, this study provides valuable
insights into optimizing tokenization models for low-level code analysis,
contributing to the robustness and scalability of Natural Language Model
(NLM)-based binary analysis workflows.

</details>


### [28] [To See or To Read: User Behavior Reasoning in Multimodal LLMs](https://arxiv.org/abs/2511.03845)
*Tianning Dong,Luyi Ma,Varun Vasudevan,Jason Cho,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: 本文研究了多模态大语言模型(MLLM)在用户行为数据上的表现，发现图像表示比文本表示更有效。


<details>
  <summary>Details</summary>
Motivation: 探讨文本或图像表示的用户行为数据，哪种方式能更有效地提升MLLM的性能。

Method: 提出了一个名为BehaviorLens的系统性基准测试框架，用于评估六个MLLM在用户行为推理中的模态权衡，将交易数据表示为文本段落、散点图和流程图。

Result: 发现当数据表示为图像时，MLLM的下一次购买预测准确率比同等的文本表示提高了87.5%，且没有增加额外的计算成本。

Conclusion: 图像表示比文本表示更有效，可以显著提高MLLM在用户行为预测方面的准确性。

Abstract: Multimodal Large Language Models (MLLMs) are reshaping how modern agentic
systems reason over sequential user-behavior data. However, whether textual or
image representations of user behavior data are more effective for maximizing
MLLM performance remains underexplored. We present \texttt{BehaviorLens}, a
systematic benchmarking framework for assessing modality trade-offs in
user-behavior reasoning across six MLLMs by representing transaction data as
(1) a text paragraph, (2) a scatter plot, and (3) a flowchart. Using a
real-world purchase-sequence dataset, we find that when data is represented as
images, MLLMs next-purchase prediction accuracy is improved by 87.5% compared
with an equivalent textual representation without any additional computational
cost.

</details>


### [29] [KnowThyself: An Agentic Assistant for LLM Interpretability](https://arxiv.org/abs/2511.03878)
*Suraj Prasai,Mengnan Du,Ying Zhang,Fan Yang*

Main category: cs.AI

TL;DR: KnowThyself is a chat-based agentic assistant that consolidates LLM interpretability tools into an accessible interface.


<details>
  <summary>Details</summary>
Motivation: Existing LLM interpretability tools are fragmented and code-intensive.

Method: An orchestrator LLM reformulates queries, an agent router directs them to specialized modules, and outputs are contextualized into explanations.

Result: KnowThyself lowers technical barriers and provides an extensible platform for LLM inspection.

Conclusion: KnowThyself offers a robust foundation for accessible LLM interpretability by embedding the process in a conversational workflow.

Abstract: We develop KnowThyself, an agentic assistant that advances large language
model (LLM) interpretability. Existing tools provide useful insights but remain
fragmented and code-intensive. KnowThyself consolidates these capabilities into
a chat-based interface, where users can upload models, pose natural language
questions, and obtain interactive visualizations with guided explanations. At
its core, an orchestrator LLM first reformulates user queries, an agent router
further directs them to specialized modules, and the outputs are finally
contextualized into coherent explanations. This design lowers technical
barriers and provides an extensible platform for LLM inspection. By embedding
the whole process into a conversational workflow, KnowThyself offers a robust
foundation for accessible LLM interpretability.

</details>


### [30] [Extracting Causal Relations in Deep Knowledge Tracing](https://arxiv.org/abs/2511.03948)
*Kevin Hong,Kia Karbasi,Gregory Pottie*

Main category: cs.AI

TL;DR: 本文挑战了关于深度知识追踪（DKT）模型有效性的主流解释。先前认为 DKT 擅长建模知识成分（KC）之间的双向关系，但本文提出 DKT 的优势在于其能够建模先决关系作为因果结构。


<details>
  <summary>Details</summary>
Motivation: 研究表明，深度知识追踪（DKT）模型优于传统知识追踪方法，因为它能够建模课程中不同知识成分（KC）之间的双向关系，从而能够从学生在其他 KC 上的表现推断他们对一个 KC 的理解。本文旨在挑战这种解释。

Method: 通过将练习关系图修剪为有向无环图（DAG），并在 Assistments 数据集的因果子集上训练 DKT，我们展示了 DKT 的预测能力与这些因果结构高度一致。此外，我们提出了一种替代方法，用于使用 DKT 学习的表示来提取练习关系 DAG，并提供实证证据来支持我们的主张。

Result: 研究结果表明，DKT 的有效性主要源于其近似 KC 之间因果依赖关系的能力，而不是简单的关系映射。

Conclusion: DKT 的优势在于其能够建模先决关系作为因果结构，而不是知识成分（KC）之间的双向关系。

Abstract: A longstanding goal in computational educational research is to develop
explainable knowledge tracing (KT) models. Deep Knowledge Tracing (DKT), which
leverages a Recurrent Neural Network (RNN) to predict student knowledge and
performance on exercises, has been proposed as a major advancement over
traditional KT methods. Several studies suggest that its performance gains stem
from its ability to model bidirectional relationships between different
knowledge components (KCs) within a course, enabling the inference of a
student's understanding of one KC from their performance on others. In this
paper, we challenge this prevailing explanation and demonstrate that DKT's
strength lies in its implicit ability to model prerequisite relationships as a
causal structure, rather than bidirectional relationships. By pruning exercise
relation graphs into Directed Acyclic Graphs (DAGs) and training DKT on causal
subsets of the Assistments dataset, we show that DKT's predictive capabilities
align strongly with these causal structures. Furthermore, we propose an
alternative method for extracting exercise relation DAGs using DKT's learned
representations and provide empirical evidence supporting our claim. Our
findings suggest that DKT's effectiveness is largely driven by its capacity to
approximate causal dependencies between KCs rather than simple relational
mappings.

</details>


### [31] [LLMs and Cultural Values: the Impact of Prompt Language and Explicit Cultural Framing](https://arxiv.org/abs/2511.03980)
*Bram Bulté,Ayla Rigouts Terryn*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）在全球范围内被广泛采用，但由于训练数据和优化目标的不平衡，LLM是否能代表其广泛用户群体的文化多样性令人怀疑。本研究探讨了LLM和文化价值观，以及prompt语言和文化框架如何影响模型响应，以及它们与不同国家人类价值观的对齐。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型是否能够代表其广泛用户群体的文化多样性，特别是考虑到训练数据和优化目标的不平衡。

Method: 使用 Hofstede 价值观调查模块和世界价值观调查中的 63 个项目，翻译成 11 种语言，并构建为带有和不带有不同显式文化视角的 prompt，来探测 10 个 LLM。

Result: Prompt 语言和文化视角都会导致 LLM 输出的变化。有针对性的 prompt 可以在一定程度上引导 LLM 响应朝着相应国家的主导价值观方向发展，但无法克服模型对数据集中一组特定国家（荷兰、德国、美国和日本）相关价值观的系统性偏差。所有测试模型都表现出非常相似的模式：它们在大多数主题上产生相当中立的响应，并在社会宽容等问题上采取选择性的进步立场。与有针对性的 prompt 语言相比，显式的文化视角更能改善与人类受访者文化价值观的对齐。令人意外的是，结合这两种方法并不比使用英语 prompt 进行文化框架更有效。

Conclusion: LLM 处于一个令人不安的中间地带：它们对 prompt 的变化足够敏感，可以产生变化，但又过于牢固地锚定在特定的文化默认值上，无法充分代表文化多样性。

Abstract: Large Language Models (LLMs) are rapidly being adopted by users across the
globe, who interact with them in a diverse range of languages. At the same
time, there are well-documented imbalances in the training data and
optimisation objectives of this technology, raising doubts as to whether LLMs
can represent the cultural diversity of their broad user base. In this study,
we look at LLMs and cultural values and examine how prompt language and
cultural framing influence model responses and their alignment with human
values in different countries. We probe 10 LLMs with 63 items from the Hofstede
Values Survey Module and World Values Survey, translated into 11 languages, and
formulated as prompts with and without different explicit cultural
perspectives. Our study confirms that both prompt language and cultural
perspective produce variation in LLM outputs, but with an important caveat:
While targeted prompting can, to a certain extent, steer LLM responses in the
direction of the predominant values of the corresponding countries, it does not
overcome the models' systematic bias toward the values associated with a
restricted set of countries in our dataset: the Netherlands, Germany, the US,
and Japan. All tested models, regardless of their origin, exhibit remarkably
similar patterns: They produce fairly neutral responses on most topics, with
selective progressive stances on issues such as social tolerance. Alignment
with cultural values of human respondents is improved more with an explicit
cultural perspective than with a targeted prompt language. Unexpectedly,
combining both approaches is no more effective than cultural framing with an
English prompt. These findings reveal that LLMs occupy an uncomfortable middle
ground: They are responsive enough to changes in prompts to produce variation,
but too firmly anchored to specific cultural defaults to adequately represent
cultural diversity.

</details>


### [32] [ArchPilot: A Proxy-Guided Multi-Agent Approach for Machine Learning Engineering](https://arxiv.org/abs/2511.03985)
*Zhuowen Yuan,Tao Liu,Yang Yang,Yang Wang,Feng Qi,Kaushik Rangadurai,Bo Li,Shuang Yang*

Main category: cs.AI

TL;DR: ArchPilot is a multi-agent system that integrates architecture generation, proxy-based evaluation, and adaptive search into a unified framework to address the challenges of repeated full training runs in LLM-based agents.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based agents rely on repeated full training runs to evaluate candidate solutions, resulting in significant computational overhead, limited scalability, and slow iteration cycles.

Method: ArchPilot uses a multi-agent system with three specialized agents: an orchestration agent, a generation agent, and an evaluation agent. The orchestration agent coordinates the search process using a Monte Carlo Tree Search (MCTS)-inspired algorithm. The generation agent generates, improves, and debugs candidate architectures. The evaluation agent executes proxy training runs and optimizes proxy functions.

Result: ArchPilot outperforms SOTA baselines such as AIDE and ML-Master on MLE-Bench.

Conclusion: The experiments validate the effectiveness of the multi-agent system in ArchPilot.

Abstract: Recent LLM-based agents have demonstrated strong capabilities in automated ML
engineering. However, they heavily rely on repeated full training runs to
evaluate candidate solutions, resulting in significant computational overhead,
limited scalability to large search spaces, and slow iteration cycles. To
address these challenges, we introduce ArchPilot, a multi-agent system that
integrates architecture generation, proxy-based evaluation, and adaptive search
into a unified framework. ArchPilot consists of three specialized agents: an
orchestration agent that coordinates the search process using a Monte Carlo
Tree Search (MCTS)-inspired novel algorithm with a restart mechanism and
manages memory of previous candidates; a generation agent that iteratively
generates, improves, and debugs candidate architectures; and an evaluation
agent that executes proxy training runs, generates and optimizes proxy
functions, and aggregates the proxy scores into a fidelity-aware performance
metric. This multi-agent collaboration allows ArchPilot to prioritize
high-potential candidates with minimal reliance on expensive full training
runs, facilitating efficient ML engineering under limited budgets. Experiments
on MLE-Bench demonstrate that ArchPilot outperforms SOTA baselines such as AIDE
and ML-Master, validating the effectiveness of our multi-agent system.

</details>


### [33] [Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis](https://arxiv.org/abs/2511.04584)
*Daniel Gomm,Cornelius Wolff,Madelon Hulsebos*

Main category: cs.AI

TL;DR: 论文提出了一种新的视角来看待表格数据自然语言接口中的歧义性，将其视为用户与系统之间协作交互的特征。


<details>
  <summary>Details</summary>
Motivation: 目前表格数据自然语言接口在处理查询时，未能有效区分可解析和不可解析的查询，从而影响系统评估。

Method: 论文开发了一个原则性框架，用于区分协作查询和非协作查询，并应用于15个流行数据集。

Result: 通过对15个数据集的分析，发现查询类型混合，无法有效评估系统的执行准确性和解释能力。

Conclusion: 该框架将重点从解决歧义性转变为在解决查询时加强协作，从而为表格数据的自然语言接口的设计和评估提供更明智的方向。

Abstract: Natural language interfaces to tabular data must handle ambiguities inherent
to queries. Instead of treating ambiguity as a deficiency, we reframe it as a
feature of cooperative interaction, where the responsibility of query
specification is shared among the user and the system. We develop a principled
framework distinguishing cooperative queries, i.e., queries that yield a
resolvable interpretation, from uncooperative queries that cannot be resolved.
Applying the framework to evaluations for tabular question answering and
analysis, we analyze the queries in 15 popular datasets, and observe an
uncontrolled mixing of query types neither adequate for evaluating a system's
execution accuracy nor for evaluating interpretation capabilities. Our
framework and analysis of queries shifts the perspective from fixing ambiguity
to embracing cooperation in resolving queries. This reflection enables more
informed design and evaluation for natural language interfaces for tabular
data, for which we outline implications and directions for future research.

</details>


### [34] [Detecting Silent Failures in Multi-Agentic AI Trajectories](https://arxiv.org/abs/2511.04032)
*Divya Pathak,Harshit Kumar,Anuska Roy,Felix George,Mudit Verma,Pratibha Moogi*

Main category: cs.AI

TL;DR: 这篇论文研究了多智能体人工智能系统中的异常检测问题，这些系统具有不确定性，容易出现难以检测的故障。


<details>
  <summary>Details</summary>
Motivation: 旨在识别多智能体人工智能系统中的漂移、循环和输出细节缺失等隐性故障。

Method: 提出了一个数据集管理流程，用于捕获用户行为、智能体不确定性和LLM变异，并使用XGBoost和SVDD等异常检测方法进行基准测试。

Result: 构建了包含4,275和894个轨迹的基准数据集，实验表明监督学习（XGBoost）和半监督学习（SVDD）方法表现相当，准确率分别高达98%和96%。

Conclusion: 本研究首次系统地研究了多智能体人工智能系统中的异常检测问题，提供了数据集、基准和见解，以指导未来的研究。

Abstract: Multi-Agentic AI systems, powered by large language models (LLMs), are
inherently non-deterministic and prone to silent failures such as drift,
cycles, and missing details in outputs, which are difficult to detect. We
introduce the task of anomaly detection in agentic trajectories to identify
these failures and present a dataset curation pipeline that captures user
behavior, agent non-determinism, and LLM variation. Using this pipeline, we
curate and label two benchmark datasets comprising \textbf{4,275 and 894}
trajectories from Multi-Agentic AI systems. Benchmarking anomaly detection
methods on these datasets, we show that supervised (XGBoost) and
semi-supervised (SVDD) approaches perform comparably, achieving accuracies up
to 98% and 96%, respectively. This work provides the first systematic study of
anomaly detection in Multi-Agentic AI systems, offering datasets, benchmarks,
and insights to guide future research.

</details>


### [35] [Interpreting Multi-Attribute Confounding through Numerical Attributes in Large Language Models](https://arxiv.org/abs/2511.04053)
*Hirohane Takagi,Gouki Minegishi,Shota Kizawa,Issey Sukeda,Hitomi Yanaka*

Main category: cs.AI

TL;DR: 大型语言模型(LLM)在数值推理方面存在错误，但其潜在的表征机制尚不清楚。本文研究了LLM如何整合单个实体的多个数值属性，以及不相关的数值上下文如何扰乱这些表征及其下游输出。 


<details>
  <summary>Details</summary>
Motivation: 研究LLM内部整合多个数值属性的方式，以及不相关的数值上下文如何影响这些表征。

Method: 结合线性探测、偏相关分析和基于提示的脆弱性测试，针对不同规模的模型进行研究。

Result: LLM编码了真实世界的数值相关性，但倾向于系统地放大它们。不相关的上下文会导致幅度表示的持续变化，下游效应随模型大小而变化。

Conclusion: LLM决策中存在漏洞，为多属性纠缠下更公平、具有表征意识的控制奠定了基础。

Abstract: Although behavioral studies have documented numerical reasoning errors in
large language models (LLMs), the underlying representational mechanisms remain
unclear. We hypothesize that numerical attributes occupy shared latent
subspaces and investigate two questions:(1) How do LLMs internally integrate
multiple numerical attributes of a single entity? (2)How does irrelevant
numerical context perturb these representations and their downstream outputs?
To address these questions, we combine linear probing with partial correlation
analysis and prompt-based vulnerability tests across models of varying sizes.
Our results show that LLMs encode real-world numerical correlations but tend to
systematically amplify them. Moreover, irrelevant context induces consistent
shifts in magnitude representations, with downstream effects that vary by model
size. These findings reveal a vulnerability in LLM decision-making and lay the
groundwork for fairer, representation-aware control under multi-attribute
entanglement.

</details>


### [36] [Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large Language Model Agents](https://arxiv.org/abs/2511.04076)
*Hao Li,Haotian Chen,Ruoyuan Gong,Juanjuan Wang,Hao Jiang*

Main category: cs.AI

TL;DR: 提出 Agentmandering 框架，通过模拟两方政治势力的博弈来减少选区划分中的党派偏见和不公正。


<details>
  <summary>Details</summary>
Motivation: 现有的计算方法主要旨在生成大量合法的选区划分方案，但常常忽略选择过程中的策略动态，这为党派行动者提供了机会来挑选在政治上有利的地图。

Method: Agentmandering 框架将选区划分重新构想为代表对立政治利益的两个 Agent 之间的回合制谈判，Agent 轮流从候选地图中选择和冻结选区。

Result: Agentmandering 显著减少了党派偏见和不公正，同时实现了比标准基线低 2 到 3 个数量级的方差。

Conclusion: Agentmandering 框架在摇摆州情景中表现出公平性和稳定性。

Abstract: Redistricting plays a central role in shaping how votes are translated into
political power. While existing computational methods primarily aim to generate
large ensembles of legally valid districting plans, they often neglect the
strategic dynamics involved in the selection process. This oversight creates
opportunities for partisan actors to cherry-pick maps that, while technically
compliant, are politically advantageous. Simply satisfying formal constraints
does not ensure fairness when the selection process itself can be manipulated.
We propose \textbf{Agentmandering}, a framework that reimagines redistricting
as a turn-based negotiation between two agents representing opposing political
interests. Drawing inspiration from game-theoretic ideas, particularly the
\textit{Choose-and-Freeze} protocol, our method embeds strategic interaction
into the redistricting process via large language model (LLM) agents. Agents
alternate between selecting and freezing districts from a small set of
candidate maps, gradually partitioning the state through constrained and
interpretable choices. Evaluation on post-2020 U.S. Census data across all
states shows that Agentmandering significantly reduces partisan bias and
unfairness, while achieving 2 to 3 orders of magnitude lower variance than
standard baselines. These results demonstrate both fairness and stability,
especially in swing-state scenarios. Our code is available at
https://github.com/Lihaogx/AgentMandering.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [37] [GPU-Based Floating-point Adaptive Lossless Compression](https://arxiv.org/abs/2511.04140)
*Zheng Li,Weiyan Wang,Ruiyuan Li,Chao Chen,Xianlei Long,Linjiang Zheng,Quanqing Xu,Chuanhui Yang*

Main category: cs.DB

TL;DR: Falcon是一个基于GPU的浮点自适应无损压缩框架，旨在提高物联网和高性能计算等领域中浮点时间序列数据的压缩吞吐量。


<details>
  <summary>Details</summary>
Motivation: 在物联网和高性能计算等领域，浮点时间序列数据大量涌现，对其进行压缩并保持绝对精度至关重要；利用现代GPU的大规模并行性可以实现前所未有的吞吐量。然而，设计这种高性能的基于GPU的无损压缩器面临三个关键挑战：异构数据移动瓶颈、精度保持转换复杂性和异常引起的稀疏性降低。

Method: Falcon首先引入了一个轻量级异步流水线，它隐藏了CPU和GPU之间数据传输过程中的I/O延迟。然后，我们提出了一种具有理论保证的精确快速的浮点到整数转换方法，消除了浮点运算引起的误差。此外，我们设计了一种自适应稀疏位平面无损编码策略，减少了由异常值引起的稀疏性。

Result: 在12个不同的数据集上进行的大量实验表明，我们的压缩率比最先进的基于CPU的方法提高了9.1%，压缩吞吐量分别比最快的基于GPU的竞争对手高2.43倍，解压缩吞吐量高2.4倍。

Conclusion: Falcon框架有效地解决了异构数据移动瓶颈、精度保持转换复杂性和异常引起的稀疏性降低这三个关键挑战，实现了更高的压缩率和吞吐量。

Abstract: Domains such as IoT (Internet of Things) and HPC (High Performance Computing)
generate a torrential influx of floating-point time-series data. Compressing
these data while preserving their absolute fidelity is critical, and leveraging
the massive parallelism of modern GPUs offers a path to unprecedented
throughput. Nevertheless, designing such a high-performance GPU-based lossless
compressor faces three key challenges: 1) heterogeneous data movement
bottlenecks, 2) precision-preserving conversion complexity, and 3)
anomaly-induced sparsity degradation. To address these challenges, this paper
proposes Falcon, a GPU-based Floating-point Adaptive Lossless COmpressioN
framework. Specifically, Falcon first introduces a lightweight asynchronous
pipeline, which hides the I/O latency during the data transmission between the
CPU and GPU. Then, we propose an accurate and fast float-to-integer
transformation method with theoretical guarantees, which eliminates the errors
caused by floating-point arithmetic. Moreover, we devise an adaptive sparse
bit-plane lossless encoding strategy, which reduces the sparsity caused by
outliers. Extensive experiments on 12 diverse datasets show that our
compression ratio improves by 9.1% over the most advanced CPU-based method,
with compression throughput 2.43X higher and decompression throughput 2.4X
higher than the fastest GPU-based competitors, respectively.

</details>


### [38] [EntroGD: Efficient Compression and Accurate Direct Analytics on Compressed Data](https://arxiv.org/abs/2511.04148)
*Xiaobo Zhao,Daniel E. Lucani*

Main category: cs.DB

TL;DR: EntroGD: An entropy-guided GD framework that reduces complexity of the bit-selection algorithm to O(nd).


<details>
  <summary>Details</summary>
Motivation: GD algorithms face scalability challenges for high-dimensional data.

Method: It generates condensed samples to preserve analytic fidelity; it applies entropy-guided bit selection to maximize compression efficiency.

Result: achieves compression performance comparable to GD-based and universal compressors, while reducing configuration time by up to 53.5× over GreedyGD and accelerating clustering by up to 31.6× over the original data with negligible accuracy loss.

Conclusion: EntroGD provides an efficient and scalable solution to performing analytics directly on compressed data.

Abstract: Generalized Deduplication (GD) enables lossless compression with direct
analytics on compressed data by dividing data into \emph{bases} and
\emph{deviations} and performing dictionary encoding on the former. However, GD
algorithms face scalability challenges for high-dimensional data. For example,
the GreedyGD algorithm relies on an iterative bit-selection process across
$d$-dimensional data resulting in $O(nd^2)$ complexity for $n$ data rows to
select bits to be used as bases and deviations. Although the $n$ data rows can
be reduced during training at the expense of performance, highly dimensional
data still experiences a marked loss in performance. This paper introduces
EntroGD, an entropy-guided GD framework that reduces complexity of the
bit-selection algorithm to $O(nd)$. EntroGD operates considers a two-step
process. First, it generates condensed samples to preserve analytic fidelity.
Second, it applies entropy-guided bit selection to maximize compression
efficiency. Across 18 datasets of varying types and dimensionalities, EntroGD
achieves compression performance comparable to GD-based and universal
compressors, while reducing configuration time by up to 53.5$\times$ over
GreedyGD and accelerating clustering by up to 31.6$\times$ over the original
data with negligible accuracy loss by performing analytics on the condensed
samples, which are much fewer than original samples. Thus, EntroGD provides an
efficient and scalable solution to performing analytics directly on compressed
data.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [39] [Caption Injection for Optimization in Generative Search Engine](https://arxiv.org/abs/2511.04080)
*Xiaolu Chen,Yong Liao*

Main category: cs.IR

TL;DR: 提出了Caption Injection，一种多模态G-SEO方法，通过提取图像标题并将其注入到文本内容中，整合视觉语义以增强生成搜索场景中内容的主观可见性。


<details>
  <summary>Details</summary>
Motivation: 现有的G-SEO方法仅限于基于文本的优化，未能充分利用多模态数据。

Method: 通过提取图像标题并将其注入到文本内容中。

Result: Caption Injection在MRAMG上显著优于仅文本的G-SEO基线。

Conclusion: 多模态集成在G-SEO中对于提高用户感知的内容可见性是必要和有效的。

Abstract: Generative Search Engines (GSEs) leverage Retrieval-Augmented Generation
(RAG) techniques and Large Language Models (LLMs) to integrate multi-source
information and provide users with accurate and comprehensive responses. Unlike
traditional search engines that present results in ranked lists, GSEs shift
users' attention from sequential browsing to content-driven subjective
perception, driving a paradigm shift in information retrieval. In this context,
enhancing the subjective visibility of content through Generative Search Engine
Optimization (G-SEO) methods has emerged as a new research focus. With the
rapid advancement of Multimodal Retrieval-Augmented Generation (MRAG)
techniques, GSEs can now efficiently integrate text, images, audio, and video,
producing richer responses that better satisfy complex information needs.
Existing G-SEO methods, however, remain limited to text-based optimization and
fail to fully exploit multimodal data. To address this gap, we propose Caption
Injection, the first multimodal G-SEO approach, which extracts captions from
images and injects them into textual content, integrating visual semantics to
enhance the subjective visibility of content in generative search scenarios. We
systematically evaluate Caption Injection on MRAMG, a benchmark for MRAG, under
both unimodal and multimodal settings. Experimental results show that Caption
Injection significantly outperforms text-only G-SEO baselines under the G-Eval
metric, demonstrating the necessity and effectiveness of multimodal integration
in G-SEO to improve user-perceived content visibility.

</details>


### [40] [E-CARE: An Efficient LLM-based Commonsense-Augmented Framework for E-Commerce](https://arxiv.org/abs/2511.04087)
*Ge Zhang,Rohan Deepak Ajwani,Tony Zheng,Hongjian Gu,Yaochen Hu,Wei Guo,Mark Coates,Yingxue Zhang*

Main category: cs.IR

TL;DR: 提出了一种名为 E-CARE 的高效常识增强推荐方法，旨在降低电商推荐系统中利用大型语言模型 (LLM) 进行常识推理的成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于密集的实时 LLM 推理，成本高昂，并且需要人工标注和潜在的监督微调 (SFT)。

Method: 利用常识推理因子图编码来自强大 LLM 的大部分推理模式，从而在推理过程中，模型只需对每个查询进行一次 LLM 前向传递即可访问常识推理。

Result: 在两个下游任务上的实验表明，精确率@5 提高了高达 12.1%。

Conclusion: E-CARE 能够在提升效率的同时，利用 LLM 的常识推理能力来增强各种电商任务。

Abstract: Finding relevant products given a user query plays a pivotal role in an
e-commerce platform, as it can spark shopping behaviors and result in revenue
gains. The challenge lies in accurately predicting the correlation between
queries and products. Recently, mining the cross-features between queries and
products based on the commonsense reasoning capacity of Large Language Models
(LLMs) has shown promising performance. However, such methods suffer from high
costs due to intensive real-time LLM inference during serving, as well as human
annotations and potential Supervised Fine Tuning (SFT). To boost efficiency
while leveraging the commonsense reasoning capacity of LLMs for various
e-commerce tasks, we propose the Efficient Commonsense-Augmented Recommendation
Enhancer (E-CARE). During inference, models augmented with E-CARE can access
commonsense reasoning with only a single LLM forward pass per query by
utilizing a commonsense reasoning factor graph that encodes most of the
reasoning schema from powerful LLMs. The experiments on 2 downstream tasks show
an improvement of up to 12.1% on precision@5.

</details>


### [41] [Transforming Mentorship: An AI Powered Chatbot Approach to University Guidance](https://arxiv.org/abs/2511.04172)
*Mashrur Rahman,Mantaqa abedin,Monowar Zamil Abir,Faizul Islam Ansari,Adib Reza,Farig Yousuf Sadeque,Niloy Farhan*

Main category: cs.IR

TL;DR: 本研究提出了一种AI驱动的聊天机器人，旨在为BRAC大学的学生提供个性化的指导。


<details>
  <summary>Details</summary>
Motivation: 本科生在大学生活中面临巨大的挑战，缺乏个性化的指导。现有的数字化工具无法为新生提供定制化的指导。

Method: 该聊天机器人结合了BM25词汇排序和ChromaDB语义检索的混合方法来检索信息，并使用大型语言模型LLaMA-3.3-70B来生成对话回复。数据管道能够高效地处理和更新来自CSV文件和大学网页等不同来源的信息。

Result: 生成的文本在语义上具有高度相关性，BERTScore为0.831，METEOR评分为0.809。数据管道的更新速度非常快，更新数据仅需106.82秒，而新数据则需要368.62秒。

Conclusion: 该聊天机器人能够通过回答学生的问题，帮助他们更好地了解大学生活，并协助他们规划更好的学期课程。

Abstract: University students face immense challenges during their undergraduate lives,
often being deprived of personalized on-demand guidance that mentors fail to
provide at scale. Digital tools exist, but there is a serious lack of
customized coaching for newcomers. This paper presents an AI-powered chatbot
that will serve as a mentor for the students of BRAC University. The main
component is a data ingestion pipeline that efficiently processes and updates
information from diverse sources, such as CSV files and university webpages.
The chatbot retrieves information through a hybrid approach, combining BM25
lexical ranking with ChromaDB semantic retrieval, and uses a Large Language
Model, LLaMA-3.3-70B, to generate conversational responses. The generated text
was found to be semantically highly relevant, with a BERTScore of 0.831 and a
METEOR score of 0.809. The data pipeline was also very efficient, taking 106.82
seconds for updates, compared to 368.62 seconds for new data. This chatbot will
be able to help students by responding to their queries, helping them to get a
better understanding of university life, and assisting them to plan better
routines for their semester in the open-credit university.

</details>


### [42] [Coordination-Free Lane Partitioning for Convergent ANN Search](https://arxiv.org/abs/2511.04221)
*Carl Kugblenu,Petri Vuorimaa*

Main category: cs.IR

TL;DR: 提出了一种无协调的通道分配器，通过确定性地将候选池划分为不相交的切片，将冗余计算转化为互补覆盖，从而提高向量搜索系统的效率。


<details>
  <summary>Details</summary>
Motivation: 在向量搜索系统中，并行通道会重复发现相同的候选对象，导致计算资源的浪费。

Method: 该方法包括：(1) 构建一个确定性的候选池，其大小与总的 top-k 预算相匹配；(2) 应用一个基于查询的伪随机排列；(3) 为每个通道分配一个不相交的位置切片。

Result: 在 SIFT1M 数据集上，recall@10 从 0.249 提升到 0.999，通道重叠从近 100% 降到 0%。在 MS MARCO 数据集上，hit@10 从 0.200 提升到 0.601，MRR@10 从 0.133 提升到 0.330。

Conclusion: 通过将每个查询池的大小调整到总预算，并确定性地在通道上分配位置，可以将冗余的 fan-out 转换为互补的覆盖，而无需更改预算或截止时间。

Abstract: Production vector search systems often fan out each query across parallel
lanes (threads, replicas, or shards) to meet latency service-level objectives
(SLOs). In practice, these lanes rediscover the same candidates, so extra
compute does not increase coverage. We present a coordination-free lane
partitioner that turns duplication into complementary work at the same cost and
deadline. For each query we (1) build a deterministic candidate pool sized to
the total top-k budget, (2) apply a per-query pseudorandom permutation, and (3)
assign each lane a disjoint slice of positions. Lanes then return different
results by construction, with no runtime coordination.
  At equal cost with four lanes (total candidate budget 64), on SIFT1M (1M SIFT
feature vectors) with Hierarchical Navigable Small World graphs (HNSW)
recall@10 rises from 0.249 to 0.999 while lane overlap falls from nearly 100%
to 0%. On MS MARCO (8.8M passages) with HNSW, hit@10 improves from 0.200 to
0.601 and Mean Reciprocal Rank at 10 (MRR@10) from 0.133 to 0.330. For inverted
file (IVF) indexes we see smaller but consistent gains (for example, +11% on MS
MARCO) by de-duplicating list routing. A microbenchmark shows planner overhead
of ~37 microseconds per query (mean at the main setting) with linear growth in
the number of merged candidates.
  These results yield a simple operational guideline: size the per-query pool
to the total budget, deterministically partition positions across lanes, and
turn redundant fan-out into complementary coverage without changing budget or
deadline.

</details>


### [43] [Denoised Recommendation Model with Collaborative Signal Decoupling](https://arxiv.org/abs/2511.04237)
*Zefeng Li,Ning Yang*

Main category: cs.IR

TL;DR: 这篇论文提出了一种新的基于图神经网络（GNN）的协同过滤（CF）模型DRCSD，用于去除不稳定的交互噪声，以提高推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的去噪方法在单个图上进行，可能导致协作信号衰减。

Method: DRCSD模型包含两个核心模块：协作信号解耦模块（按结构特征将信号分解为不同的阶数）和按阶去噪模块（对每个阶数执行有针对性的去噪）。此外，修改了传统GNN-CF模型的信息聚合机制，以避免跨阶信号干扰。

Result: 在三个公共真实数据集上的大量实验表明，DRCSD对不稳定的交互具有优越的鲁棒性，并且与最先进的基线模型相比，在推荐准确性指标方面实现了具有统计意义的性能改进。

Conclusion: DRCSD模型能够有效地去除不稳定的交互噪声，提高推荐系统的性能。

Abstract: Although the collaborative filtering (CF) algorithm has achieved remarkable
performance in recommendation systems, it suffers from suboptimal
recommendation performance due to noise in the user-item interaction matrix.
Numerous noise-removal studies have improved recommendation models, but most
existing approaches conduct denoising on a single graph. This may cause
attenuation of collaborative signals: removing edges between two nodes can
interrupt paths between other nodes, weakening path-dependent collaborative
information. To address these limitations, this study proposes a novel
GNN-based CF model called DRCSD for denoising unstable interactions. DRCSD
includes two core modules: a collaborative signal decoupling module (decomposes
signals into distinct orders by structural characteristics) and an order-wise
denoising module (performs targeted denoising on each order). Additionally, the
information aggregation mechanism of traditional GNN-based CF models is
modified to avoid cross-order signal interference until the final pooling
operation. Extensive experiments on three public real-world datasets show that
DRCSD has superior robustness against unstable interactions and achieves
statistically significant performance improvements in recommendation accuracy
metrics compared to state-of-the-art baseline models.

</details>


### [44] [LLM-as-a-Judge: Toward World Models for Slate Recommendation Systems](https://arxiv.org/abs/2511.04541)
*Baptiste Bonin,Maxime Heuillet,Audrey Durand*

Main category: cs.IR

TL;DR: 利用大型语言模型(LLM)通过成对推理预测用户在跨领域的排序推荐中的偏好。


<details>
  <summary>Details</summary>
Motivation: 在排序推荐中，跨领域建模用户偏好仍然是一个关键挑战。

Method: 使用大型语言模型(LLM)作为用户偏好的世界模型，通过成对推理来对排序推荐进行建模。

Result: 在三个数据集上对几个LLM进行了实证研究，揭示了任务表现和LLM捕获的偏好函数属性之间的关系。

Conclusion: 结果表明LLM有潜力作为推荐系统中的世界模型，并指出了需要改进的领域。

Abstract: Modeling user preferences across domains remains a key challenge in slate
recommendation (i.e. recommending an ordered sequence of items) research. We
investigate how Large Language Models (LLM) can effectively act as world models
of user preferences through pairwise reasoning over slates. We conduct an
empirical study involving several LLMs on three tasks spanning different
datasets. Our results reveal relationships between task performance and
properties of the preference function captured by LLMs, hinting towards areas
for improvement and highlighting the potential of LLMs as world models in
recommender systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland](https://arxiv.org/abs/2511.03749)
*Oluwadurotimi Onibonoje,Vuong M. Ngo,Andrew McCarre,Elodie Ruelle,Bernadette O-Briend,Mark Roantree*

Main category: cs.LG

TL;DR: 爱尔兰的草地是重要的碳汇，但目前的牧草生长预测依赖于不切实际的机械模型。本文提出了一种基于深度学习的单变量数据集模型，用于预测多年生黑麦草的生长，并在科克地区取得了较好的效果（RMSE为2.74，MAE为3.46）。


<details>
  <summary>Details</summary>
Motivation: 爱尔兰乳制品业面临盈利能力和可持续性的挑战，现有的牧草生长预测模型不实用。

Method: 利用深度学习模型，特别是时间卷积网络，处理单变量数据集，预测多年生黑麦草的生长。

Result: 在科克地区，时间卷积网络在预测多年生黑麦草生长方面表现出色，RMSE为2.74，MAE为3.46。通过对34年内1757周的数据进行验证，获得了最佳模型配置。

Conclusion: 该研究提高了对模型行为的理解，从而提高了牧草生长预测的可靠性，并有助于推进可持续奶牛养殖实践。

Abstract: Grasslands, constituting the world's second-largest terrestrial carbon sink,
play a crucial role in biodiversity and the regulation of the carbon cycle.
Currently, the Irish dairy sector, a significant economic contributor, grapples
with challenges related to profitability and sustainability. Presently, grass
growth forecasting relies on impractical mechanistic models. In response, we
propose deep learning models tailored for univariate datasets, presenting
cost-effective alternatives. Notably, a temporal convolutional network designed
for forecasting Perennial Ryegrass growth in Cork exhibits high performance,
leveraging historical grass height data with RMSE of 2.74 and MAE of 3.46.
Validation across a comprehensive dataset spanning 1,757 weeks over 34 years
provides insights into optimal model configurations. This study enhances our
understanding of model behavior, thereby improving reliability in grass growth
forecasting and contributing to the advancement of sustainable dairy farming
practices.

</details>


### [46] [Federated Learning with Gramian Angular Fields for Privacy-Preserving ECG Classification on Heterogeneous IoT Devices](https://arxiv.org/abs/2511.03753)
*Youssef Elmir,Yassine Himeur,Abbes Amira*

Main category: cs.LG

TL;DR: 本文提出了一种用于物联网 (IoT) 医疗环境中保护隐私的心电图 (ECG) 分类的联邦学习 (FL) 框架。


<details>
  <summary>Details</summary>
Motivation: 在物联网医疗环境中，保护心电图数据的隐私。

Method: 通过将 1D 心电图信号转换为 2D 格拉姆角场 (GAF) 图像，利用卷积神经网络 (CNN) 进行特征提取。

Result: FL-GAF 模型在多客户端设置中实现了 95.18% 的高分类精度，在准确性和训练时间方面均显着优于单客户端基线。

Conclusion: 该框架突出了轻量级、保护隐私的 AI 在基于物联网的医疗监控中的潜力，支持智能健康系统中可扩展且安全的边缘部署。

Abstract: This study presents a federated learning (FL) framework for
privacy-preserving electrocardiogram (ECG) classification in Internet of Things
(IoT) healthcare environments. By transforming 1D ECG signals into 2D Gramian
Angular Field (GAF) images, the proposed approach enables efficient feature
extraction through Convolutional Neural Networks (CNNs) while ensuring that
sensitive medical data remain local to each device. This work is among the
first to experimentally validate GAF-based federated ECG classification across
heterogeneous IoT devices, quantifying both performance and communication
efficiency. To evaluate feasibility in realistic IoT settings, we deployed the
framework across a server, a laptop, and a resource-constrained Raspberry Pi 4,
reflecting edge-cloud integration in IoT ecosystems. Experimental results
demonstrate that the FL-GAF model achieves a high classification accuracy of
95.18% in a multi-client setup, significantly outperforming a single-client
baseline in both accuracy and training time. Despite the added computational
complexity of GAF transformations, the framework maintains efficient resource
utilization and communication overhead. These findings highlight the potential
of lightweight, privacy-preserving AI for IoT-based healthcare monitoring,
supporting scalable and secure edge deployments in smart health systems.

</details>


### [47] [Laugh, Relate, Engage: Stylized Comment Generation for Short Videos](https://arxiv.org/abs/2511.03757)
*Xuan Ouyang,Senan Wang,Bouzhou Wang,Siyuan Xiahou,Jinrong Zhou,Yuekang Li*

Main category: cs.LG

TL;DR: LOLGORITHM是一个用于生成可控短视频评论的模块化多代理系统，它集成了视频分割、上下文和情感分析以及风格感知提示构建，支持六种不同的评论风格。


<details>
  <summary>Details</summary>
Motivation: 在短视频平台中，评论在促进社区参与和内容再创作方面起着至关重要的作用。然而，生成既符合平台准则又具有文体多样性和上下文意识的评论仍然是一个重大挑战。

Method: 该系统集成了视频分割、上下文和情感分析以及风格感知提示构建。它支持六种不同的评论风格：双关语（同音词）、押韵、表情包应用、讽刺（反语）、纯粹的幽默和内容提取。由多模态大型语言模型 (MLLM) 提供支持，LOLGORITHM 直接处理视频输入，并通过显式提示标记和少量示例实现细粒度的风格控制。

Result: LOLGORITHM 的性能明显优于基线模型，在抖音上的偏好率超过 90%，在 YouTube 上的偏好率达到 87.55%。

Conclusion: 这项工作提出了一个可扩展且具有文化适应性的短视频平台风格化评论生成框架，为增强用户参与度和创造性互动提供了一条有希望的途径。

Abstract: Short-video platforms have become a central medium in the modern Internet
landscape, where efficient information delivery and strong interactivity are
reshaping user engagement and cultural dissemination. Among the various forms
of user interaction, comments play a vital role in fostering community
participation and enabling content re-creation. However, generating comments
that are both compliant with platform guidelines and capable of exhibiting
stylistic diversity and contextual awareness remains a significant challenge.
We introduce LOLGORITHM, a modular multi-agent system (MAS) designed for
controllable short-video comment generation. The system integrates video
segmentation, contextual and affective analysis, and style-aware prompt
construction. It supports six distinct comment styles: puns (homophones),
rhyming, meme application, sarcasm (irony), plain humor, and content
extraction. Powered by a multimodal large language model (MLLM), LOLGORITHM
directly processes video inputs and achieves fine-grained style control through
explicit prompt markers and few-shot examples. To support development and
evaluation, we construct a bilingual dataset using official APIs from Douyin
(Chinese) and YouTube (English), covering five popular video genres: comedy
skits, daily life jokes, funny animal clips, humorous commentary, and talk
shows. Evaluation combines automated metrics originality, relevance, and style
conformity with a large-scale human preference study involving 40 videos and
105 participants. Results show that LOLGORITHM significantly outperforms
baseline models, achieving preference rates of over 90% on Douyin and 87.55% on
YouTube. This work presents a scalable and culturally adaptive framework for
stylized comment generation on short-video platforms, offering a promising path
to enhance user engagement and creative interaction.

</details>


### [48] [Learning Filter-Aware Distance Metrics for Nearest Neighbor Search with Multiple Filters](https://arxiv.org/abs/2511.04073)
*Ananya Sutradhar,Suryansh Gupta,Ravishankar Krishnaswamy,Haiyang Xu,Aseem Rastogi,Gopal Srinivasa*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的过滤近似最近邻搜索（ANN）方法，该方法通过学习数据中的向量距离和过滤匹配之间的最佳权衡来提高搜索准确性，避免了传统方法中使用固定惩罚的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图的方法在过滤近似最近邻搜索中通常使用固定的惩罚或优先级来考虑过滤条件，但这些方法无法推广到具有不同标签和向量分布的数据集。

Method: 该论文将问题建模为一个约束线性优化问题，从而学习更有效地反映底层过滤分布的权重。这些学习到的权重指导搜索过程和索引构建。

Result: 实验结果表明，与固定惩罚方法相比，该方法在准确性方面提高了 5-10%。

Conclusion: 该论文提出了一种更灵活和通用的过滤 ANN 搜索框架，通过自适应距离函数显著提高了搜索准确性。

Abstract: Filtered Approximate Nearest Neighbor (ANN) search retrieves the closest
vectors for a query vector from a dataset. It enforces that a specified set of
discrete labels $S$ for the query must be included in the labels of each
retrieved vector. Existing graph-based methods typically incorporate filter
awareness by assigning fixed penalties or prioritizing nodes based on filter
satisfaction. However, since these methods use fixed, data in- dependent
penalties, they often fail to generalize across datasets with diverse label and
vector distributions. In this work, we propose a principled alternative that
learns the optimal trade-off between vector distance and filter match directly
from the data, rather than relying on fixed penalties. We formulate this as a
constrained linear optimization problem, deriving weights that better reflect
the underlying filter distribution and more effectively address the filtered
ANN search problem. These learned weights guide both the search process and
index construction, leading to graph structures that more effectively capture
the underlying filter distribution and filter semantics. Our experiments
demonstrate that adapting the distance function to the data significantly im-
proves accuracy by 5-10% over fixed-penalty methods, providing a more flexible
and generalizable framework for the filtered ANN search problem.

</details>


### [49] [What's in Common? Multimodal Models Hallucinate When Reasoning Across Scenes](https://arxiv.org/abs/2511.03768)
*Candace Ross,Florian Bordes,Adina Williams,Polina Kirichenko,Mark Ibrahim*

Main category: cs.LG

TL;DR: 现有的多模态模型在实际场景推理中存在幻觉问题，无法真正理解真实世界。


<details>
  <summary>Details</summary>
Motivation: 为了弥补模型在感知基准测试中表现看似强大但实际推理能力不足的差距。

Method: 构建了一个名为Common-O的新基准，包含10.5k个真实场景的例子，并通过提问“有什么共同点？”来考察跨场景的推理能力。

Result: 即使是最先进的模型在Common-O上的表现也很差，在复杂场景下甚至只有1%的准确率。模型在相似对象出现时更容易产生幻觉。

Conclusion: 大规模多图像训练可能有所帮助。该基准已公开，旨在促进对跨场景推理中幻觉挑战的研究。

Abstract: Multimodal language models possess a remarkable ability to handle an
open-vocabulary's worth of objects. Yet the best models still suffer from
hallucinations when reasoning about scenes in the real world, revealing a gap
between their seemingly strong performance on existing perception benchmarks
that are saturating and their reasoning in the real world. To address this gap,
we build a novel benchmark of in-the-wild scenes that we call Common-O. With
more than 10.5k examples using exclusively new images not found in web training
data to avoid contamination, Common-O goes beyond just perception, inspired by
cognitive tests for humans, to probe reasoning across scenes by asking "what's
in common?". We evaluate leading multimodal language models, including models
specifically trained to perform chain-of-thought reasoning. We find that
perceiving objects in single images is tractable for most models, yet reasoning
across scenes is very challenging even for the best models, including reasoning
models. Despite saturating many leaderboards focusing on perception, the best
performing model only achieves 35% on Common-O -- and on Common-O Complex,
consisting of more complex scenes, the best model achieves only 1%. Curiously,
we find models are more prone to hallucinate when similar objects are present
in the scene, suggesting models may be relying on object co-occurrence seen
during training. Among the models we evaluated, we found scale can provide
modest improvements while models explicitly trained with multi-image inputs
show bigger improvements, suggesting scaled multi-image training may offer
promise. We make our benchmark publicly available to spur research into the
challenge of hallucination when reasoning across scenes.

</details>


### [50] [Contamination Detection for VLMs using Multi-Modal Semantic Perturbation](https://arxiv.org/abs/2511.03774)
*Jaden Park,Mu Cai,Feng Yao,Jingbo Shang,Soochahn Lee,Yong Jae Lee*

Main category: cs.LG

TL;DR: 这篇论文提出了一种检测被污染的视觉语言模型（VLM）的方法，这些模型在训练前可能已经接触过测试集数据。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型由于使用了大规模的专有预训练语料库，存在测试集泄露的问题，导致性能虚高。虽然之前的工作提出了数据清理和基准测试重新设计等缓解策略，但对于被污染的视觉语言模型的检测方法的研究还不够充分。

Method: 该论文提出了一种基于多模态语义扰动的新颖且有效的检测方法。通过对模型施加受控扰动，观察模型是否还能泛化，以此来判断模型是否被污染。

Result: 实验结果表明，现有的检测方法要么完全失败，要么表现不稳定。而该论文提出的方法在多种实际污染策略下都表现出鲁棒性和有效性。

Conclusion: 该论文提出了一种有效的检测视觉语言模型是否被污染的方法，并通过实验验证了其有效性。

Abstract: Recent advances in Vision-Language Models (VLMs) have achieved
state-of-the-art performance on numerous benchmark tasks. However, the use of
internet-scale, often proprietary, pretraining corpora raises a critical
concern for both practitioners and users: inflated performance due to test-set
leakage. While prior works have proposed mitigation strategies such as
decontamination of pretraining data and benchmark redesign for LLMs, the
complementary direction of developing detection methods for contaminated VLMs
remains underexplored. To address this gap, we deliberately contaminate
open-source VLMs on popular benchmarks and show that existing detection
approaches either fail outright or exhibit inconsistent behavior. We then
propose a novel simple yet effective detection method based on multi-modal
semantic perturbation, demonstrating that contaminated models fail to
generalize under controlled perturbations. Finally, we validate our approach
across multiple realistic contamination strategies, confirming its robustness
and effectiveness. The code and perturbed dataset will be released publicly.

</details>


### [51] [FusionDP: Foundation Model-Assisted Differentially Private Learning for Partially Sensitive Features](https://arxiv.org/abs/2511.03806)
*Linghui Zeng,Ruixuan Liu,Atiquer Rahman Sarkar,Xiaoqian Jiang,Joyce C. Ho,Li Xiong*

Main category: cs.LG

TL;DR: FusionDP 利用大型基础模型来填补敏感特征，并结合改进的 DP-SGD 算法，在保护特征级别隐私的同时，显著提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 在实践中，隐私保护可能只需要针对部分特征。传统 DP-SGD 对所有特征强制隐私保护，导致过度噪声注入和效用降低。

Method: FusionDP 框架首先利用大型基础模型根据非敏感特征推算敏感特征，然后使用改进的 DP-SGD 算法在原始和推算特征上训练模型。

Result: 在 PhysioNet 的 sepsis 预测任务和 MIMIC-III 的临床笔记分类任务上的评估表明，FusionDP 显著提高了模型性能，同时保持了严格的特征级别隐私。

Conclusion: FusionDP 证明了基础模型驱动的插补在增强各种模态的隐私-效用权衡方面的潜力。

Abstract: Ensuring the privacy of sensitive training data is crucial in
privacy-preserving machine learning. However, in practical scenarios, privacy
protection may be required for only a subset of features. For instance, in ICU
data, demographic attributes like age and gender pose higher privacy risks due
to their re-identification potential, whereas raw lab results are generally
less sensitive. Traditional DP-SGD enforces privacy protection on all features
in one sample, leading to excessive noise injection and significant utility
degradation. We propose FusionDP, a two-step framework that enhances model
utility under feature-level differential privacy. First, FusionDP leverages
large foundation models to impute sensitive features given non-sensitive
features, treating them as external priors that provide high-quality estimates
of sensitive attributes without accessing the true values during model
training. Second, we introduce a modified DP-SGD algorithm that trains models
on both original and imputed features while formally preserving the privacy of
the original sensitive features. We evaluate FusionDP on two modalities: a
sepsis prediction task on tabular data from PhysioNet and a clinical note
classification task from MIMIC-III. By comparing against privacy-preserving
baselines, our results show that FusionDP significantly improves model
performance while maintaining rigorous feature-level privacy, demonstrating the
potential of foundation model-driven imputation to enhance the privacy-utility
trade-off for various modalities.

</details>


### [52] [Fair and Explainable Credit-Scoring under Concept Drift: Adaptive Explanation Frameworks for Evolving Populations](https://arxiv.org/abs/2511.03807)
*Shivogo John*

Main category: cs.LG

TL;DR: 传统SHAP等可解释性技术在概念漂移下不稳定且可能不公平。本研究开发自适应解释框架，重新校准动态演进信用模型中的可解释性和公平性。


<details>
  <summary>Details</summary>
Motivation: 在现代信用评分系统中，借款人行为、经济状况和监管环境不断变化，重塑了数据分布。传统的可解释性技术假设静态数据和固定的背景分布，导致解释不稳定和潜在的不公平。

Method: 将XGBoost预测模型与三种自适应SHAP变体集成：(A)调整特征分布变化的切片解释重加权，(B)使用滑动窗口背景样本的漂移感知SHAP重定基线，(C)使用增量岭回归的在线代理校准。

Result: 自适应方法，特别是重定基线和基于代理的解释，在不降低预测准确性的前提下，显著提高了时间稳定性，并减少了不同人群之间的差异性影响。

Conclusion: 自适应可解释性是一种实用的机制，可在数据驱动的信用系统以及决策模型随人口变化而演变的任何领域中，维持透明度、问责制和道德可靠性。

Abstract: Evolving borrower behaviors, shifting economic conditions, and changing
regulatory landscapes continuously reshape the data distributions underlying
modern credit-scoring systems. Conventional explainability techniques, such as
SHAP, assume static data and fixed background distributions, making their
explanations unstable and potentially unfair when concept drift occurs. This
study addresses that challenge by developing adaptive explanation frameworks
that recalibrate interpretability and fairness in dynamically evolving credit
models. Using a multi-year credit dataset, we integrate predictive modeling via
XGBoost with three adaptive SHAP variants: (A) per-slice explanation
reweighting that adjusts for feature distribution shifts, (B) drift-aware SHAP
rebaselining with sliding-window background samples, and (C) online surrogate
calibration using incremental Ridge regression. Each method is benchmarked
against static SHAP explanations using metrics of predictive performance (AUC,
F1), directional and rank stability (cosine, Kendall tau), and fairness
(demographic parity and recalibration). Results show that adaptive methods,
particularly rebaselined and surrogate-based explanations, substantially
improve temporal stability and reduce disparate impact across demographic
groups without degrading predictive accuracy. Robustness tests, including
counterfactual perturbations, background sensitivity analysis, and
proxy-variable detection, confirm the resilience of adaptive explanations under
real-world drift conditions. These findings establish adaptive explainability
as a practical mechanism for sustaining transparency, accountability, and
ethical reliability in data-driven credit systems, and more broadly, in any
domain where decision models evolve with population change.

</details>


### [53] [Optimizing Reasoning Efficiency through Prompt Difficulty Prediction](https://arxiv.org/abs/2511.03808)
*Bo Zhao,Berkcan Kapusuzoglu,Kartik Balasubramaniam,Sambit Sahu,Supriyo Chakraborty,Genta Indra Winata*

Main category: cs.LG

TL;DR: 提出了一种路由方法，将每个问题分配给最有可能解决它的最小模型，从而在不牺牲准确性的前提下减少计算。


<details>
  <summary>Details</summary>
Motivation: 由于规模和长推理轨迹，推理语言模型在复杂任务上表现良好，但部署成本高昂。

Method: 使用来自 s1.1-32B 的中间表示，我们训练问题难度或模型正确性的轻量级预测器，以指导跨推理模型池的路由。

Result: 在不同的数学基准测试中，路由提高了效率，超过了随机分配，并且在使用明显更少的计算资源的同时，与 s1.1-32B 的性能相匹配。

Conclusion: 我们的结果表明，难度感知路由对于推理模型的经济高效部署是有效的。

Abstract: Reasoning language models perform well on complex tasks but are costly to
deploy due to their size and long reasoning traces. We propose a routing
approach that assigns each problem to the smallest model likely to solve it,
reducing compute without sacrificing accuracy. Using intermediate
representations from s1.1-32B, we train lightweight predictors of problem
difficulty or model correctness to guide routing across a pool of reasoning
models. On diverse math benchmarks, routing improves efficiency over random
assignment and matches s1.1-32B's performance while using significantly less
compute. Our results demonstrate that difficulty-aware routing is effective for
cost-efficient deployment of reasoning models.

</details>


### [54] [Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs](https://arxiv.org/abs/2511.04473)
*Alberto Cattaneo,Carlo Luschi,Daniel Justus*

Main category: cs.LG

TL;DR: 本文介绍了一个名为SynthKGQA的框架，用于生成高质量的合成知识图谱问答数据集，并提出了GTSQA数据集来测试知识图谱检索器的zero-shot泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的知识图谱问答数据集缺乏挑战性，难以比较不同方法的优劣。

Method: 提出了SynthKGQA框架，可以从任何知识图谱生成高质量的合成问答数据集，并提供了完整的ground-truth。

Result: 使用SynthKGQA生成的数据可以更有效地评估知识图谱检索器，并训练出更好的模型。提出了GTSQA数据集，用于测试知识图谱检索器的zero-shot泛化能力。

Conclusion: 本文提出了SynthKGQA框架和GTSQA数据集，为知识图谱问答领域的研究提供了新的工具和基准。

Abstract: Retrieval of information from graph-structured knowledge bases represents a
promising direction for improving the factuality of LLMs. While various
solutions have been proposed, a comparison of methods is difficult due to the
lack of challenging QA datasets with ground-truth targets for graph retrieval.
We present SynthKGQA, a framework for generating high-quality synthetic
Knowledge Graph Question Answering datasets from any Knowledge Graph, providing
the full set of ground-truth facts in the KG to reason over each question. We
show how, in addition to enabling more informative benchmarking of KG
retrievers, the data produced with SynthKGQA also allows us to train better
models. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset
designed to test zero-shot generalization abilities of KG retrievers with
respect to unseen graph structures and relation types, and benchmark popular
solutions for KG-augmented LLMs on it.

</details>


### [55] [One Size Does Not Fit All: Architecture-Aware Adaptive Batch Scheduling with DEBA](https://arxiv.org/abs/2511.03809)
*François Belias,Naser Ezzati-Jivan,Foutse Khomh*

Main category: cs.LG

TL;DR: DEBA是一种自适应批量大小调整方法，它通过监控梯度方差、梯度范数变化和损失变化来指导批量大小调整。实验表明，不同架构从自适应批量大小调整中获益程度不同。


<details>
  <summary>Details</summary>
Motivation: 现有的自适应批量大小方法假设一个尺寸适合所有架构，但本文发现架构会根本上决定自适应效果。

Method: DEBA：一种动态高效批量自适应方法，它监控梯度方差、梯度范数变化和损失变化来指导批量大小调整。

Result: 轻量级和中等深度架构（MobileNet-V3、DenseNet-121、EfficientNet-B0）实现了 45-62% 的训练加速，同时精度提高了 1-7%；浅层残差网络（ResNet-18）在精度上显示出 +2.4 - 4.0% 的持续收益，速度提高了 36 - 43%，而深层残差网络（ResNet-50）表现出高方差和偶尔的性能下降；已经稳定的架构（ViT-B16）显示出最小的加速（6%），尽管保持了精度。

Conclusion: 自适应方法不能跨架构推广，批量大小调整需要一种架构感知的设置。

Abstract: Adaptive batch size methods aim to accelerate neural network training, but
existing approaches apply identical adaptation strategies across all
architectures, assuming a one-size-fits-all solution. We introduce DEBA
(Dynamic Efficient Batch Adaptation), an adaptive batch scheduler that monitors
gradient variance, gradient norm variation and loss variation to guide batch
size adaptations. Through systematic evaluation across six architectures
(ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V3, ViT-B16) on
CIFAR-10 and CIFAR-100, with five random seeds per configuration, we
demonstrate that the architecture fundamentally determines adaptation efficacy.
Our findings reveal that: (1) lightweight and medium-depth architectures
(MobileNet-V3, DenseNet-121, EfficientNet-B0) achieve a 45-62% training speedup
with simultaneous accuracy improvements of 1-7%; (2) shallow residual networks
(ResNet-18) show consistent gains of +2.4 - 4.0% in accuracy, 36 - 43% in
speedup, while deep residual networks (ResNet-50) exhibit high variance and
occasional degradation; (3) already-stable architectures (ViT-B16) show minimal
speedup (6%) despite maintaining accuracy, indicating that adaptation benefits
vary with baseline optimization characteristics. We introduce a baseline
characterization framework using gradient stability metrics (stability score,
gradient norm variation) that predicts which architectures will benefit from
adaptive scheduling. Our ablation studies reveal critical design choices often
overlooked in prior work: sliding window statistics (vs. full history) and
sufficient cooldown periods (5+ epochs) between adaptations are essential for
success. This work challenges the prevailing assumption that adaptive methods
generalize across architectures and provides the first systematic evidence that
batch size adaptation requires an architecture-aware design.

</details>
