{"id": "2507.22951", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22951", "abs": "https://arxiv.org/abs/2507.22951", "authors": ["Alessandro Lonardi", "Samy Badreddine", "Tarek R. Besold", "Pablo Sanchez Martin"], "title": "Unifying Post-hoc Explanations of Knowledge Graph Completions", "comment": null, "summary": "Post-hoc explainability for Knowledge Graph Completion (KGC) lacks\nformalization and consistent evaluations, hindering reproducibility and\ncross-study comparisons. This paper argues for a unified approach to post-hoc\nexplainability in KGC. First, we propose a general framework to characterize\npost-hoc explanations via multi-objective optimization, balancing their\neffectiveness and conciseness. This unifies existing post-hoc explainability\nalgorithms in KGC and the explanations they produce. Next, we suggest and\nempirically support improved evaluation protocols using popular metrics like\nMean Reciprocal Rank and Hits@$k$. Finally, we stress the importance of\ninterpretability as the ability of explanations to address queries meaningful\nto end-users. By unifying methods and refining evaluation standards, this work\naims to make research in KGC explainability more reproducible and impactful.", "AI": {"tldr": "This paper introduces a unified framework for post-hoc explainability in KGC, proposes improved evaluation protocols, and emphasizes interpretability to enhance reproducibility and impact.", "motivation": "Post-hoc explainability for Knowledge Graph Completion (KGC) lacks formalization and consistent evaluations, hindering reproducibility and cross-study comparisons. This paper argues for a unified approach to post-hoc explainability in KGC.", "method": "We propose a general framework to characterize post-hoc explanations via multi-objective optimization, balancing their effectiveness and conciseness. This unifies existing post-hoc explainability algorithms in KGC and the explanations they produce. Next, we suggest and empirically support improved evaluation protocols using popular metrics like Mean Reciprocal Rank and Hits@$k$.", "result": "The framework balances effectiveness and conciseness. Improved evaluation protocols using popular metrics like Mean Reciprocal Rank and Hits@$k$ are suggested and empirically supported.", "conclusion": "This work aims to make research in KGC explainability more reproducible and impactful by unifying methods and refining evaluation standards."}}
{"id": "2507.23018", "categories": ["cs.AI", "cs.CE", "cs.DC", "cs.LG", "I.2.6"], "pdf": "https://arxiv.org/pdf/2507.23018", "abs": "https://arxiv.org/abs/2507.23018", "authors": ["Wesley Brewer", "Patrick Widener", "Valentine Anantharaj", "Feiyi Wang", "Tom Beck", "Arjun Shankar", "Sarp Oral"], "title": "Data Readiness for Scientific AI at Scale", "comment": "10 pages, 1 figure, 2 tables", "summary": "This paper examines how Data Readiness for AI (DRAI) principles apply to\nleadership-scale scientific datasets used to train foundation models. We\nanalyze archetypal workflows across four representative domains - climate,\nnuclear fusion, bio/health, and materials - to identify common preprocessing\npatterns and domain-specific constraints. We introduce a two-dimensional\nreadiness framework composed of Data Readiness Levels (raw to AI-ready) and\nData Processing Stages (ingest to shard), both tailored to high performance\ncomputing (HPC) environments. This framework outlines key challenges in\ntransforming scientific data for scalable AI training, emphasizing\ntransformer-based generative models. Together, these dimensions form a\nconceptual maturity matrix that characterizes scientific data readiness and\nguides infrastructure development toward standardized, cross-domain support for\nscalable and reproducible AI for science.", "AI": {"tldr": "This paper introduces a two-dimensional framework to characterize scientific data readiness for AI, guiding infrastructure development for scalable and reproducible AI in science.", "motivation": "The motivation is to examine how Data Readiness for AI (DRAI) principles apply to leadership-scale scientific datasets used to train foundation models.", "method": "The paper analyzes archetypal workflows across four domains: climate, nuclear fusion, bio/health, and materials, to identify common preprocessing patterns and domain-specific constraints.", "result": "The paper outlines key challenges in transforming scientific data for scalable AI training, emphasizing transformer-based generative models. The framework guides infrastructure development toward standardized, cross-domain support for scalable and reproducible AI for science.", "conclusion": "This paper introduces a readiness framework for leadership-scale scientific datasets, composed of Data Readiness Levels and Data Processing Stages, to address the challenges in transforming scientific data for scalable AI training."}}
{"id": "2507.23067", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23067", "abs": "https://arxiv.org/abs/2507.23067", "authors": ["Zhenyu Pan", "Yutong Zhang", "Jianshu Zhang", "Haoran Lu", "Haozheng Luo", "Yuwei Han", "Philip S. Yu", "Manling Li", "Han Liu"], "title": "FairReason: Balancing Reasoning and Social Bias in MLLMs", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) already achieve state-of-the-art\nresults across a wide range of tasks and modalities. To push their reasoning\nability further, recent studies explore advanced prompting schemes and\npost-training fine-tuning. Although these techniques improve logical accuracy,\nthey frequently leave the models' outputs burdened with pronounced social\nbiases. Clarifying how reasoning gains interact with bias mitigation-and\nwhether the two objectives inherently trade off-therefore remains an open and\npressing research problem. Our study begins by benchmarking three\nbias-mitigation strategies-supervised fine-uning (SFT), knowledge distillation\n(KD), and rule-based reinforcement learning (RL)-under identical conditions,\nestablishing their baseline strengths and weaknesses. Building on these\nresults, we vary the proportion of debias-focused and reasoning-centric samples\nwithin each paradigm to chart the reasoning-versus-bias trade-off. Our sweeps\nreveal a consistent sweet spot: a roughly 1:4 mix trained with reinforcement\nlearning cuts stereotype scores by 10% while retaining 88% of the model's\noriginal reasoning accuracy, offering concrete guidance for balancing fairness\nand capability in MLLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u4e2d\u63a8\u7406\u4e0e\u504f\u89c1\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u53d1\u73b0\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u7279\u5b9a\u6bd4\u4f8b\u7684\u6df7\u5408\u6837\u672c\u53ef\u4ee5\u5728\u4fdd\u6301\u63a8\u7406\u51c6\u786e\u7387\u7684\u540c\u65f6\u6709\u6548\u51cf\u5c11\u504f\u89c1\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u5df2\u7ecf\u5728\u5e7f\u6cdb\u7684\u4efb\u52a1\u548c\u6a21\u6001\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4ed6\u4eec\u7684\u63a8\u7406\u80fd\u529b\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u63a2\u7d22\u4e86\u5148\u8fdb\u7684\u63d0\u793a\u65b9\u6848\u548c\u540e\u8bad\u7ec3\u5fae\u8c03\u3002\u867d\u7136\u8fd9\u4e9b\u6280\u672f\u63d0\u9ad8\u4e86\u903b\u8f91\u51c6\u786e\u6027\uff0c\u4f46\u5b83\u4eec\u7ecf\u5e38\u4f7f\u6a21\u578b\u7684\u8f93\u51fa\u5e26\u6709\u660e\u663e\u7684\u793e\u4f1a\u504f\u89c1\u3002\u56e0\u6b64\uff0c\u9610\u660e\u63a8\u7406\u6536\u76ca\u5982\u4f55\u4e0e\u504f\u89c1\u7f13\u89e3\u76f8\u4e92\u4f5c\u7528\uff0c\u4ee5\u53ca\u8fd9\u4e24\u4e2a\u76ee\u6807\u662f\u5426\u56fa\u6709\u5730\u76f8\u4e92\u6743\u8861\uff0c\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u4e14\u7d27\u8feb\u7684\u7814\u7a76\u95ee\u9898\u3002", "method": "\u5bf9\u4e09\u79cd\u504f\u89c1\u7f13\u89e3\u7b56\u7565\uff08\u76d1\u7763\u5fae\u8c03 (SFT)\u3001\u77e5\u8bc6\u84b8\u998f (KD) \u548c\u57fa\u4e8e\u89c4\u5219\u7684\u5f3a\u5316\u5b66\u4e60 (RL)\uff09\u5728\u76f8\u540c\u6761\u4ef6\u4e0b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5efa\u7acb\u5b83\u4eec\u7684\u57fa\u672c\u4f18\u52bf\u548c\u52a3\u52bf\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u6539\u53d8\u6bcf\u4e2a\u8303\u4f8b\u4e2d\u4ee5\u6d88\u9664\u504f\u89c1\u4e3a\u91cd\u70b9\u548c\u4ee5\u63a8\u7406\u4e3a\u4e2d\u5fc3\u7684\u6837\u672c\u7684\u6bd4\u4f8b\uff0c\u4ee5\u7ed8\u5236\u63a8\u7406\u4e0e\u504f\u89c1\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "\u63ed\u793a\u4e86\u4e00\u4e2a\u4e00\u81f4\u7684\u6700\u4f73\u70b9\uff1a\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u7ea6 1:4 \u7684\u6df7\u5408\u6bd4\u4f8b\u53ef\u4ee5\u5728\u4fdd\u6301\u6a21\u578b\u539f\u59cb\u63a8\u7406\u51c6\u786e\u7387\u7684 88% \u7684\u540c\u65f6\uff0c\u5c06\u523b\u677f\u5370\u8c61\u5206\u6570\u964d\u4f4e 10%\uff0c\u4e3a\u5e73\u8861 MLLM \u4e2d\u7684\u516c\u5e73\u6027\u548c\u80fd\u529b\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u6307\u5bfc\u3002", "conclusion": "\u53d1\u73b0\u4e86\u4e00\u4e2a\u5e73\u8861\u70b9\uff1a\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u7ea6 1:4 \u7684\u6df7\u5408\u6bd4\u4f8b\u53ef\u4ee5\u5728\u4fdd\u6301\u6a21\u578b\u539f\u59cb\u63a8\u7406\u51c6\u786e\u7387\u7684 88% \u7684\u540c\u65f6\uff0c\u5c06\u523b\u677f\u5370\u8c61\u5206\u6570\u964d\u4f4e 10%\uff0c\u4e3a\u5e73\u8861 MLLM \u4e2d\u7684\u516c\u5e73\u6027\u548c\u80fd\u529b\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u6307\u5bfc\u3002"}}
{"id": "2507.23084", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23084", "abs": "https://arxiv.org/abs/2507.23084", "authors": ["Taiyi Wang", "Eiko Yoneki"], "title": "AutoIndexer: A Reinforcement Learning-Enhanced Index Advisor Towards Scaling Workloads", "comment": "14 pages", "summary": "Efficiently selecting indexes is fundamental to database performance\noptimization, particularly for systems handling large-scale analytical\nworkloads. While deep reinforcement learning (DRL) has shown promise in\nautomating index selection through its ability to learn from experience, few\nworks address how these RL-based index advisors can adapt to scaling workloads\ndue to exponentially growing action spaces and heavy trial and error. To\naddress these challenges, we introduce AutoIndexer, a framework that combines\nworkload compression, query optimization, and specialized RL models to scale\nindex selection effectively. By operating on compressed workloads, AutoIndexer\nsubstantially lowers search complexity without sacrificing much index quality.\nExtensive evaluations show that it reduces end-to-end query execution time by\nup to 95% versus non-indexed baselines. On average, it outperforms\nstate-of-the-art RL-based index advisors by approximately 20% in workload cost\nsavings while cutting tuning time by over 50%. These results affirm\nAutoIndexer's practicality for large and diverse workloads.", "AI": {"tldr": "AutoIndexer\u901a\u8fc7\u7ed3\u5408\u5de5\u4f5c\u8d1f\u8f7d\u538b\u7f29\u3001\u67e5\u8be2\u4f18\u5316\u548c\u4e13\u7528RL\u6a21\u578b\uff0c\u6709\u6548\u6269\u5c55\u7d22\u5f15\u9009\u62e9\uff0c\u4ece\u800c\u663e\u8457\u964d\u4f4e\u4e86\u641c\u7d22\u590d\u6742\u6027\uff0c\u540c\u65f6\u6ca1\u6709\u727a\u7272\u592a\u591a\u7684\u7d22\u5f15\u8d28\u91cf\u3002", "motivation": "\u5bf9\u4e8e\u5904\u7406\u5927\u89c4\u6a21\u5206\u6790\u5de5\u4f5c\u8d1f\u8f7d\u7684\u7cfb\u7edf\uff0c\u9ad8\u6548\u9009\u62e9\u7d22\u5f15\u5bf9\u4e8e\u6570\u636e\u5e93\u6027\u80fd\u4f18\u5316\u81f3\u5173\u91cd\u8981\u3002\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60(DRL)\u901a\u8fc7\u5176\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\uff0c\u5728\u81ea\u52a8\u5316\u7d22\u5f15\u9009\u62e9\u65b9\u9762\u663e\u793a\u51fa\u4e86\u5e0c\u671b\uff0c\u4f46\u7531\u4e8e\u6307\u6570\u589e\u957f\u7684\u884c\u52a8\u7a7a\u95f4\u548c\u5927\u91cf\u7684\u8bd5\u9a8c\u548c\u9519\u8bef\uff0c\u5f88\u5c11\u6709\u5de5\u4f5c\u6d89\u53ca\u8fd9\u4e9b\u57fa\u4e8erl\u7684\u7d22\u5f15\u987e\u95ee\u5982\u4f55\u9002\u5e94\u6269\u5c55\u7684\u5de5\u4f5c\u8d1f\u8f7d\u3002", "method": "AutoIndexer\u7ed3\u5408\u4e86\u5de5\u4f5c\u8d1f\u8f7d\u538b\u7f29\u3001\u67e5\u8be2\u4f18\u5316\u548c\u4e13\u7528RL\u6a21\u578b\u6765\u6709\u6548\u6269\u5c55\u7d22\u5f15\u9009\u62e9\u3002", "result": "\u4e0e\u975e\u7d22\u5f15\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5b83\u53ef\u4ee5\u51cf\u5c11\u9ad8\u8fbe95%\u7684\u7aef\u5230\u7aef\u67e5\u8be2\u6267\u884c\u65f6\u95f4\u3002\u5e73\u5747\u800c\u8a00\uff0c\u5728\u5de5\u4f5c\u8d1f\u8f7d\u6210\u672c\u8282\u7ea6\u65b9\u9762\uff0c\u5b83\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u4e8erl\u7684\u7d22\u5f15\u987e\u95ee\u9ad8\u51fa\u7ea620%\uff0c\u540c\u65f6\u5c06\u8c03\u4f18\u65f6\u95f4\u7f29\u77ed\u4e8650%\u4ee5\u4e0a\u3002", "conclusion": "AutoIndexer\u5728\u5927\u578b\u548c\u591a\u6837\u5316\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u5177\u6709\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.23091", "categories": ["cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.23091", "abs": "https://arxiv.org/abs/2507.23091", "authors": ["David Noever", "Forrest McKee"], "title": "Moravec's Paradox: Towards an Auditory Turing Test", "comment": null, "summary": "This research work demonstrates that current AI systems fail catastrophically\non auditory tasks that humans perform effortlessly. Drawing inspiration from\nMoravec's paradox (i.e., tasks simple for humans often prove difficult for\nmachines, and vice versa), we introduce an auditory Turing test comprising 917\nchallenges across seven categories: overlapping speech, speech in noise,\ntemporal distortion, spatial audio, coffee-shop noise, phone distortion, and\nperceptual illusions. Our evaluation of state-of-the-art audio models including\nGPT-4's audio capabilities and OpenAI's Whisper reveals a striking failure rate\nexceeding 93%, with even the best-performing model achieving only 6.9% accuracy\non tasks that humans solved at 7.5 times higher success (52%). These results\nexpose focusing failures in how AI systems process complex auditory scenes,\nparticularly in selective attention, noise robustness, and contextual\nadaptation. Our benchmark not only quantifies the human-machine auditory gap\nbut also provides insights into why these failures occur, suggesting that\ncurrent architectures lack fundamental mechanisms for human-like auditory scene\nanalysis. The traditional design of audio CAPTCHAs highlights common filters\nthat humans evolved but machines fail to select in multimodal language models.\nThis work establishes a diagnostic framework for measuring progress toward\nhuman-level machine listening and highlights the need for novel approaches\nintegrating selective attention, physics-based audio understanding, and\ncontext-aware perception into multimodal AI systems.", "AI": {"tldr": "\u5f53\u524d\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5728\u542c\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u6539\u8fdb\u3002", "motivation": "\u5f53\u524d\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5728\u4eba\u7c7b\u8f7b\u677e\u6267\u884c\u7684\u542c\u89c9\u4efb\u52a1\u4e0a \u043a\u0430\u0442\u0430\u0441\u0442\u0440\u043e\u0444\u0438\u0447\u0435\u0441\u043a\u0438 \u5931\u8d25\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b\u4e03\u4e2a\u7c7b\u522b\uff08\u91cd\u53e0\u8bed\u97f3\u3001\u566a\u58f0\u4e2d\u7684\u8bed\u97f3\u3001\u65f6\u95f4\u626d\u66f2\u3001\u7a7a\u95f4\u97f3\u9891\u3001\u5496\u5561\u5e97\u566a\u58f0\u3001\u7535\u8bdd\u5931\u771f\u548c\u611f\u77e5\u9519\u89c9\uff09\u7684 917 \u9879\u6311\u6218\u7684\u542c\u89c9\u56fe\u7075\u6d4b\u8bd5\u3002", "result": "\u5bf9\u5305\u62ec GPT-4 \u7684\u97f3\u9891\u529f\u80fd\u548c OpenAI \u7684 Whisper \u5728\u5185\u7684\u6700\u5148\u8fdb\u97f3\u9891\u6a21\u578b\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5931\u8d25\u7387\u8d85\u8fc7 93%\uff0c\u5373\u4f7f\u662f\u6027\u80fd\u6700\u597d\u7684\u6a21\u578b\u5728\u4eba\u7c7b\u4ee5 7.5 \u500d\u7684\u6210\u529f\u7387\uff0852%\uff09\u89e3\u51b3\u7684\u4efb\u52a1\u4e2d\u4e5f\u53ea\u8fbe\u5230 6.9% \u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u5f53\u524d\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5728\u542c\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u66b4\u9732\u51fa\u5728\u9009\u62e9\u6027\u6ce8\u610f\u3001\u566a\u58f0\u9c81\u68d2\u6027\u548c\u60c5\u5883\u9002\u5e94\u65b9\u9762\u7684\u7f3a\u9677\u3002\u9700\u8981\u5c06\u9009\u62e9\u6027\u6ce8\u610f\u3001\u57fa\u4e8e\u7269\u7406\u7684\u97f3\u9891\u7406\u89e3\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u6574\u5408\u5230\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u4e2d\u3002"}}
{"id": "2507.23499", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.23499", "abs": "https://arxiv.org/abs/2507.23499", "authors": ["Piotr Sowinski", "Kacper Grzymkowski", "Anastasiya Danilenka"], "title": "Jelly-Patch: a Fast Format for Recording Changes in RDF Datasets", "comment": null, "summary": "Recording data changes in RDF systems is a crucial capability, needed to\nsupport auditing, incremental backups, database replication, and event-driven\nworkflows. In large-scale and low-latency RDF applications, the high volume and\nfrequency of updates can cause performance bottlenecks in the serialization and\ntransmission of changes. To alleviate this, we propose Jelly-Patch -- a\nhigh-performance, compressed binary serialization format for changes in RDF\ndatasets. To evaluate its performance, we benchmark Jelly-Patch against\nexisting RDF Patch formats, using two datasets representing different use cases\n(change data capture and IoT streams). Jelly-Patch is shown to achieve\n3.5--8.9x better compression, and up to 2.5x and 4.6x higher throughput in\nserialization and parsing, respectively. These significant advancements in\nthroughput and compression are expected to improve the performance of\nlarge-scale and low-latency RDF systems.", "AI": {"tldr": "Jelly-Patch\u662f\u4e00\u79cd\u7528\u4e8eRDF\u6570\u636e\u96c6\u66f4\u6539\u7684\u9ad8\u6027\u80fd\u538b\u7f29\u4e8c\u8fdb\u5236\u5e8f\u5217\u5316\u683c\u5f0f\uff0c\u53ef\u63d0\u9ad8\u541e\u5410\u91cf\u548c\u538b\u7f29\u7387\u3002", "motivation": "\u5728\u5927\u578b\u548c\u4f4e\u5ef6\u8fdfRDF\u5e94\u7528\u4e2d\uff0c\u5927\u91cf\u548c\u9891\u7e41\u7684\u66f4\u65b0\u4f1a\u5bfc\u81f4\u66f4\u6539\u7684\u5e8f\u5217\u5316\u548c\u4f20\u8f93\u4e2d\u7684\u6027\u80fd\u74f6\u9888\u3002\u8bb0\u5f55RDF\u7cfb\u7edf\u4e2d\u7684\u6570\u636e\u66f4\u6539\u662f\u4e00\u9879\u5173\u952e\u529f\u80fd\uff0c\u9700\u8981\u652f\u6301\u5ba1\u8ba1\u3001\u589e\u91cf\u5907\u4efd\u3001\u6570\u636e\u5e93\u590d\u5236\u548c\u4e8b\u4ef6\u9a71\u52a8\u7684\u5de5\u4f5c\u6d41\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86Jelly-Patch\u2014\u2014\u4e00\u79cd\u7528\u4e8eRDF\u6570\u636e\u96c6\u66f4\u6539\u7684\u9ad8\u6027\u80fd\u538b\u7f29\u4e8c\u8fdb\u5236\u5e8f\u5217\u5316\u683c\u5f0f\u3002", "result": "Jelly-Patch\u7684\u538b\u7f29\u7387\u63d0\u9ad8\u4e863.5--8.9\u500d\uff0c\u5e8f\u5217\u5316\u548c\u89e3\u6790\u7684\u541e\u5410\u91cf\u5206\u522b\u63d0\u9ad8\u4e862.5\u500d\u548c4.6\u500d\u3002", "conclusion": "Jelly-Patch\u663e\u8457\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\u548c\u538b\u7f29\u7387\uff0c\u6709\u671b\u6539\u5584\u5927\u89c4\u6a21\u548c\u4f4e\u5ef6\u8fdfRDF\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2507.23208", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23208", "abs": "https://arxiv.org/abs/2507.23208", "authors": ["Jiayu Li", "Ziyi Ye", "Guohao Jian", "Zhiqiang Guo", "Weizhi Ma", "Qingyao Ai", "Min Zhang"], "title": "Are Recommenders Self-Aware? Label-Free Recommendation Performance Estimation via Model Uncertainty", "comment": null, "summary": "Can a recommendation model be self-aware? This paper investigates the\nrecommender's self-awareness by quantifying its uncertainty, which provides a\nlabel-free estimation of its performance. Such self-assessment can enable more\ninformed understanding and decision-making before the recommender engages with\nany users. To this end, we propose an intuitive and effective method,\nprobability-based List Distribution uncertainty (LiDu). LiDu measures\nuncertainty by determining the probability that a recommender will generate a\ncertain ranking list based on the prediction distributions of individual items.\nWe validate LiDu's ability to represent model self-awareness in two settings:\n(1) with a matrix factorization model on a synthetic dataset, and (2) with\npopular recommendation algorithms on real-world datasets. Experimental results\nshow that LiDu is more correlated with recommendation performance than a series\nof label-free performance estimators. Additionally, LiDu provides valuable\ninsights into the dynamic inner states of models throughout training and\ninference. This work establishes an empirical connection between recommendation\nuncertainty and performance, framing it as a step towards more transparent and\nself-evaluating recommender systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLiDu\u7684\u65b9\u6cd5\u6765\u91cf\u5316\u63a8\u8350\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u81ea\u6211\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u8868\u660eLiDu\u4e0e\u63a8\u8350\u6027\u80fd\u9ad8\u5ea6\u76f8\u5173\u3002", "motivation": "\u672c\u6587\u901a\u8fc7\u91cf\u5316\u63a8\u8350\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u7814\u7a76\u63a8\u8350\u6a21\u578b\u7684\u81ea\u6211\u611f\u77e5\u80fd\u529b\uff0c\u8fd9\u63d0\u4f9b\u4e86\u5bf9\u5176\u6027\u80fd\u7684\u65e0\u6807\u7b7e\u4f30\u8ba1\u3002\u8fd9\u79cd\u81ea\u6211\u8bc4\u4f30\u53ef\u4ee5\u5728\u63a8\u8350\u5668\u4e0e\u4efb\u4f55\u7528\u6237\u4ea4\u4e92\u4e4b\u524d\u5b9e\u73b0\u66f4\u660e\u667a\u7684\u7406\u89e3\u548c\u51b3\u7b56\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u89c2\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5373\u57fa\u4e8e\u6982\u7387\u7684\u5217\u8868\u5206\u5e03\u4e0d\u786e\u5b9a\u6027\uff08LiDu\uff09\u3002LiDu\u901a\u8fc7\u786e\u5b9a\u63a8\u8350\u5668\u57fa\u4e8e\u5404\u4e2a\u9879\u76ee\u7684\u9884\u6d4b\u5206\u5e03\u751f\u6210\u7279\u5b9a\u6392\u540d\u5217\u8868\u7684\u6982\u7387\u6765\u8861\u91cf\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4e00\u7cfb\u5217\u65e0\u6807\u7b7e\u6027\u80fd\u4f30\u8ba1\u5668\u76f8\u6bd4\uff0cLiDu\u4e0e\u63a8\u8350\u6027\u80fd\u7684\u76f8\u5173\u6027\u66f4\u9ad8\u3002\u6b64\u5916\uff0cLiDu\u8fd8\u63d0\u4f9b\u4e86\u5bf9\u6a21\u578b\u5728\u6574\u4e2a\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u5185\u90e8\u72b6\u6001\u7684\u5b9d\u8d35\u89c1\u89e3\u3002", "conclusion": "\u5efa\u7acb\u4e86\u63a8\u8350\u4e0d\u786e\u5b9a\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u7684\u7ecf\u9a8c\u8054\u7cfb\uff0c\u5e76\u5c06\u5176\u6784\u5efa\u4e3a\u8fc8\u5411\u66f4\u900f\u660e\u548c\u81ea\u6211\u8bc4\u4f30\u7684\u63a8\u8350\u7cfb\u7edf\u7684\u4e00\u6b65\u3002"}}
{"id": "2507.22910", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22910", "abs": "https://arxiv.org/abs/2507.22910", "authors": ["Sergio Di Meglio", "Aniello Somma", "Luigi Libero Lucio Starace", "Fabio Scippacercola", "Giancarlo Sperl\u00ec", "Sergio Di Martino"], "title": "Large Language Models in the Travel Domain: An Industrial Experience", "comment": "Manuscript accepted to the International Conference on Software\n  Engineering and Knowledge Engineering (SEKE) 2025", "summary": "Online property booking platforms are widely used and rely heavily on\nconsistent, up-to-date information about accommodation facilities, often\nsourced from third-party providers. However, these external data sources are\nfrequently affected by incomplete or inconsistent details, which can frustrate\nusers and result in a loss of market. In response to these challenges, we\npresent an industrial case study involving the integration of Large Language\nModels (LLMs) into CALEIDOHOTELS, a property reservation platform developed by\nFERVENTO. We evaluate two well-known LLMs in this context: Mistral 7B,\nfine-tuned with QLoRA, and Mixtral 8x7B, utilized with a refined system prompt.\nBoth models were assessed based on their ability to generate consistent and\nhomogeneous descriptions while minimizing hallucinations. Mixtral 8x7B\noutperformed Mistral 7B in terms of completeness (99.6% vs. 93%), precision\n(98.8% vs. 96%), and hallucination rate (1.2% vs. 4%), producing shorter yet\nmore concise content (249 vs. 277 words on average). However, this came at a\nsignificantly higher computational cost: 50GB VRAM and $1.61/hour versus 5GB\nand $0.16/hour for Mistral 7B. Our findings provide practical insights into the\ntrade-offs between model quality and resource efficiency, offering guidance for\ndeploying LLMs in production environments and demonstrating their effectiveness\nin enhancing the consistency and reliability of accommodation data.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u96c6\u6210\u5230\u623f\u4ea7\u9884\u8ba2\u5e73\u53f0\u4e2d\uff0c\u53d1\u73b0 Mixtral 8x7B \u5728\u6570\u636e\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e Mistral 7B\uff0c\u4f46\u6210\u672c\u66f4\u9ad8\u3002", "motivation": "\u5728\u7ebf\u623f\u4ea7\u9884\u8ba2\u5e73\u53f0\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u5e76\u4e14\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u5173\u4e8e\u4f4f\u5bbf\u8bbe\u65bd\u7684\u6700\u65b0\u4fe1\u606f\uff0c\u8fd9\u4e9b\u4fe1\u606f\u901a\u5e38\u6765\u81ea\u7b2c\u4e09\u65b9\u63d0\u4f9b\u5546\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u5916\u90e8\u6570\u636e\u6e90\u7ecf\u5e38\u53d7\u5230\u4e0d\u5b8c\u6574\u6216\u4e0d\u4e00\u81f4\u7684\u7ec6\u8282\u7684\u5f71\u54cd\uff0c\u8fd9\u53ef\u80fd\u4f1a\u8ba9\u7528\u6237\u611f\u5230\u6cae\u4e27\u5e76\u5bfc\u81f4\u5e02\u573a\u635f\u5931\u3002", "method": "\u5bf9\u4e24\u4e2a\u8457\u540d\u7684\u6cd5\u5b66\u7855\u58eb\u8fdb\u884c\u4e86\u8bc4\u4f30\uff1a\u4f7f\u7528 QLoRA \u5fae\u8c03\u7684 Mistral 7B \u548c\u4f7f\u7528\u6539\u8fdb\u7684\u7cfb\u7edf\u63d0\u793a\u4f7f\u7528\u7684 Mixtral 8x7B\u3002", "result": "Mixtral 8x7B \u5728\u5b8c\u6574\u6027\uff0899.6% \u5bf9 93%\uff09\u3001\u7cbe\u786e\u6027\uff0898.8% \u5bf9 96%\uff09\u548c\u5e7b\u89c9\u7387\uff081.2% \u5bf9 4%\uff09\u65b9\u9762\u4f18\u4e8e Mistral 7B\uff0c\u4ea7\u751f\u7684\u5185\u5bb9\u66f4\u77ed\u4f46\u66f4\u7b80\u6d01\uff08\u5e73\u5747 249 \u4e2a\u5355\u8bcd\u5bf9 277 \u4e2a\u5355\u8bcd\uff09\u3002", "conclusion": "Mixtral 8x7B \u5728\u5b8c\u6574\u6027\u3001\u7cbe\u786e\u6027\u548c\u5e7b\u89c9\u7387\u65b9\u9762\u4f18\u4e8e Mistral 7B\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u66f4\u9ad8\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72\u6cd5\u5b66\u7855\u58eb\u63d0\u4f9b\u4e86\u6a21\u578b\u8d28\u91cf\u548c\u8d44\u6e90\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u7684\u5b9e\u8df5\u89c1\u89e3\uff0c\u5e76\u8bc1\u660e\u4e86\u5b83\u4eec\u5728\u63d0\u9ad8\u4f4f\u5bbf\u6570\u636e\u7684\u4e00\u81f4\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.22958", "categories": ["cs.CV", "cs.AI", "cs.LG", "68T07, 97D50", "I.2.7; I.4; K.3.1"], "pdf": "https://arxiv.org/pdf/2507.22958", "abs": "https://arxiv.org/abs/2507.22958", "authors": ["Ruslan Khrulev"], "title": "CHECK-MAT: Checking Hand-Written Mathematical Answers for the Russian Unified State Exam", "comment": "15 pages, 3 figures, 10 tables. Code is available at:\n  https://github.com/Karifannaa/Auto-check-EGE-math", "summary": "This paper introduces a novel benchmark, EGE-Math Solutions Assessment\nBenchmark, for evaluating Vision-Language Models (VLMs) on their ability to\nassess hand-written mathematical solutions. Unlike existing benchmarks that\nfocus on problem solving, our approach centres on understanding student\nsolutions, identifying mistakes, and assigning grades according to fixed\ncriteria. We compile 122 scanned solutions from the Russian Unified State Exam\n(EGE) together with official expert grades, and evaluate seven modern VLMs from\nGoogle, OpenAI, Arcee AI, and Alibaba Cloud in three inference modes. The\nresults reveal current limitations in mathematical reasoning and human-rubric\nalignment, opening new research avenues in AI-assisted assessment. You can find\ncode in https://github.com/Karifannaa/Auto-check-EGE-math", "AI": {"tldr": "\u63d0\u51fa\u4e86EGE-Math Solutions Assessment Benchmark\uff0c\u7528\u4e8e\u8bc4\u4f30VLMs\u8bc4\u4f30\u624b\u5199\u6570\u5b66\u89e3\u9898\u8fc7\u7a0b\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u4e3b\u8981\u96c6\u4e2d\u5728\u89e3\u51b3\u95ee\u9898\u4e0a\uff0c\u800c\u672c\u6587\u4fa7\u91cd\u4e8e\u7406\u89e3\u5b66\u751f\u7684\u89e3\u9898\u8fc7\u7a0b\uff0c\u8bc6\u522b\u9519\u8bef\uff0c\u5e76\u6839\u636e\u56fa\u5b9a\u6807\u51c6\u8fdb\u884c\u8bc4\u5206\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6EGE-Math Solutions Assessment Benchmark\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u8bc4\u4f30\u624b\u5199\u6570\u5b66\u89e3\u9898\u8fc7\u7a0b\u7684\u80fd\u529b\u3002", "result": "\u5bf9\u6765\u81eaGoogle\u3001OpenAI\u3001Arcee AI\u548cAlibaba Cloud\u7684\u4e03\u4e2a\u73b0\u4ee3VLMs\u5728\u4e09\u79cd\u63a8\u7406\u6a21\u5f0f\u4e0b\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e86\u6765\u81ea\u4fc4\u7f57\u65afUnified State Exam (EGE)\u7684122\u4e2a\u626b\u63cf\u89e3\u51b3\u65b9\u6848\u4ee5\u53ca\u5b98\u65b9\u4e13\u5bb6\u8bc4\u5206\u3002", "conclusion": "VLMs\u8868\u73b0\u51fa\u6570\u5b66\u63a8\u7406\u548c\u4eba\u5de5\u8bc4\u5206\u5bf9\u9f50\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3aAI\u8f85\u52a9\u8bc4\u4f30\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.22954", "categories": ["cs.LG", "eess.IV", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.22954", "abs": "https://arxiv.org/abs/2507.22954", "authors": ["Ridvan Yesiloglu", "Wei Peng", "Md Tauhidul Islam", "Ehsan Adeli"], "title": "Neural Autoregressive Modeling of Brain Aging", "comment": "Accepted at Deep Generative Models Workshop @ MICCAI 2025", "summary": "Brain aging synthesis is a critical task with broad applications in clinical\nand computational neuroscience. The ability to predict the future structural\nevolution of a subject's brain from an earlier MRI scan provides valuable\ninsights into aging trajectories. Yet, the high-dimensionality of data, subtle\nchanges of structure across ages, and subject-specific patterns constitute\nchallenges in the synthesis of the aging brain. To overcome these challenges,\nwe propose NeuroAR, a novel brain aging simulation model based on generative\nautoregressive transformers. NeuroAR synthesizes the aging brain by\nautoregressively estimating the discrete token maps of a future scan from a\nconvenient space of concatenated token embeddings of a previous and future\nscan. To guide the generation, it concatenates into each scale the subject's\nprevious scan, and uses its acquisition age and the target age at each block\nvia cross-attention. We evaluate our approach on both the elderly population\nand adolescent subjects, demonstrating superior performance over\nstate-of-the-art generative models, including latent diffusion models (LDM) and\ngenerative adversarial networks, in terms of image fidelity. Furthermore, we\nemploy a pre-trained age predictor to further validate the consistency and\nrealism of the synthesized images with respect to expected aging patterns.\nNeuroAR significantly outperforms key models, including LDM, demonstrating its\nability to model subject-specific brain aging trajectories with high fidelity.", "AI": {"tldr": "NeuroAR \u662f\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u81ea\u56de\u5f52\u53d8\u6362\u5668\u7684\u65b0\u578b\u5927\u8111\u8870\u8001\u6a21\u62df\u6a21\u578b\uff0c\u5b83\u4ee5\u9ad8\u4fdd\u771f\u5ea6\u5bf9\u7279\u5b9a\u4e8e\u53d7\u8bd5\u8005\u7684\u5927\u8111\u8870\u8001\u8f68\u8ff9\u8fdb\u884c\u5efa\u6a21\uff0c\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "motivation": "\u5927\u8111\u8870\u8001\u5408\u6210\u662f\u4e00\u9879\u5173\u952e\u4efb\u52a1\uff0c\u5728\u4e34\u5e8a\u548c\u8ba1\u7b97\u795e\u7ecf\u79d1\u5b66\u4e2d\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u3002\u4ece\u65e9\u671f MRI \u626b\u63cf\u9884\u6d4b\u53d7\u8bd5\u8005\u5927\u8111\u7684\u672a\u6765\u7ed3\u6784\u6f14\u53d8\u7684\u80fd\u529b\uff0c\u4e3a\u8870\u8001\u8f68\u8ff9\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002\u7136\u800c\uff0c\u6570\u636e\u7684\u9ad8\u7ef4\u5ea6\u3001\u8de8\u5e74\u9f84\u7684\u7ed3\u6784\u7ec6\u5fae\u53d8\u5316\u4ee5\u53ca\u53d7\u8bd5\u8005\u7279\u5f02\u6027\u6a21\u5f0f\u6784\u6210\u4e86\u8870\u8001\u5927\u8111\u5408\u6210\u7684\u6311\u6218\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u81ea\u56de\u5f52\u53d8\u6362\u5668\u7684\u65b0\u578b\u5927\u8111\u8870\u8001\u6a21\u62df\u6a21\u578b NeuroAR\u3002NeuroAR \u901a\u8fc7\u81ea\u56de\u5f52\u65b9\u5f0f\u4f30\u8ba1\u5148\u524d\u626b\u63cf\u548c\u672a\u6765\u626b\u63cf\u7684\u4e32\u8054\u4ee4\u724c\u5d4c\u5165\u7684\u4fbf\u5229\u7a7a\u95f4\u4e2d\u7684\u672a\u6765\u626b\u63cf\u7684\u79bb\u6563\u4ee4\u724c\u56fe\u6765\u5408\u6210\u8870\u8001\u7684\u5927\u8111\u3002", "result": "\u6211\u4eec\u5728\u8001\u5e74\u4eba\u7fa4\u548c\u9752\u5c11\u5e74\u53d7\u8bd5\u8005\u4e2d\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5728\u56fe\u50cf\u4fdd\u771f\u5ea6\u65b9\u9762\uff0c\u8bc1\u660e\u4e86\u5176\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b\uff0c\u5305\u62ec\u6f5c\u5728\u6269\u6563\u6a21\u578b (LDM) \u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u3002\u6b64\u5916\uff0c\u6211\u4eec\u91c7\u7528\u9884\u5148\u8bad\u7ec3\u7684\u5e74\u9f84\u9884\u6d4b\u5668\u6765\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u5408\u6210\u56fe\u50cf\u5728\u9884\u671f\u8870\u8001\u6a21\u5f0f\u65b9\u9762\u7684\u4e00\u81f4\u6027\u548c\u771f\u5b9e\u6027\u3002", "conclusion": "NeuroAR\u663e\u8457\u4f18\u4e8e\u5173\u952e\u6a21\u578b\uff0c\u5305\u62ecLDM\uff0c\u8bc1\u660e\u4e86\u5176\u80fd\u591f\u4ee5\u9ad8\u4fdd\u771f\u5ea6\u5bf9\u7279\u5b9a\u4e8e\u53d7\u8bd5\u8005\u7684\u5927\u8111\u8870\u8001\u8f68\u8ff9\u8fdb\u884c\u5efa\u6a21\u3002"}}
{"id": "2507.23163", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.23163", "abs": "https://arxiv.org/abs/2507.23163", "authors": ["Deniz Gorur", "Antonio Rago", "Francesca Toni"], "title": "Argumentatively Coherent Judgmental Forecasting", "comment": "17 pages, 18 figures, ECAI 2025", "summary": "Judgmental forecasting employs human opinions to make predictions about\nfuture events, rather than exclusively historical data as in quantitative\nforecasting. When these opinions form an argumentative structure around\nforecasts, it is useful to study the properties of the forecasts from an\nargumentative perspective. In this paper, we advocate and formally define a\nproperty of argumentative coherence, which, in essence, requires that a\nforecaster's reasoning is coherent with their forecast. We then conduct three\nevaluations with our notion of coherence. First, we assess the impact of\nenforcing coherence on human forecasters as well as on Large Language Model\n(LLM)-based forecasters, given that they have recently shown to be competitive\nwith human forecasters. In both cases, we show that filtering out incoherent\npredictions improves forecasting accuracy consistently, supporting the\npractical value of coherence in both human and LLM-based forecasting. Then, via\ncrowd-sourced user experiments, we show that, despite its apparent\nintuitiveness and usefulness, users do not generally align with this coherence\nproperty. This points to the need to integrate, within argumentation-based\njudgmental forecasting, mechanisms to filter out incoherent opinions before\nobtaining group forecasting predictions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bba\u8bc1\u8fde\u8d2f\u6027\u7684\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8fc7\u6ee4\u6389\u4e0d\u8fde\u8d2f\u7684\u9884\u6d4b\u53ef\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u7814\u7a76\u6765\u81ea\u8bba\u8bc1\u89d2\u5ea6\u7684\u9884\u6d4b\u5c5e\u6027\u3002\u5f53\u8fd9\u4e9b\u610f\u89c1\u56f4\u7ed5\u9884\u6d4b\u5f62\u6210\u8bba\u8bc1\u7ed3\u6784\u65f6\uff0c\u7814\u7a76\u6765\u81ea\u8bba\u8bc1\u89d2\u5ea6\u7684\u9884\u6d4b\u5c5e\u6027\u662f\u6709\u7528\u7684\u3002", "method": "\u5f62\u5f0f\u5316\u5730\u5b9a\u4e49\u4e86\u4e00\u79cd\u8bba\u8bc1\u8fde\u8d2f\u6027\u7684\u5c5e\u6027\uff0c\u5e76\u8fdb\u884c\u4e86\u4e09\u6b21\u8bc4\u4f30\u3002", "result": "\u5f3a\u5236\u6267\u884c\u8fde\u8d2f\u6027\u5bf9\u4eba\u7c7b\u9884\u6d4b\u8005\u4ee5\u53ca\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u9884\u6d4b\u8005\u7684\u5f71\u54cd\u3002\u5728\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0c\u8fc7\u6ee4\u6389\u4e0d\u8fde\u8d2f\u7684\u9884\u6d4b\u90fd\u53ef\u4ee5\u6301\u7eed\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u7387\u3002\u7528\u6237\u901a\u5e38\u4e0d\u7b26\u5408\u8fd9\u79cd\u8fde\u8d2f\u6027\u5c5e\u6027\u3002", "conclusion": "\u8fc7\u6ee4\u6389\u4e0d\u8fde\u8d2f\u7684\u9884\u6d4b\u53ef\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u8fd9\u652f\u6301\u4e86\u8fde\u8d2f\u6027\u5728\u4eba\u7c7b\u548c\u57fa\u4e8e LLM \u7684\u9884\u6d4b\u4e2d\u7684\u5b9e\u9645\u4ef7\u503c\u3002\u7528\u6237\u901a\u5e38\u4e0d\u7b26\u5408\u8fd9\u79cd\u8fde\u8d2f\u6027\u5c5e\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5728\u57fa\u4e8e\u8bba\u8bc1\u7684\u5224\u65ad\u9884\u6d4b\u4e2d\u6574\u5408\u673a\u5236\uff0c\u4ee5\u5728\u83b7\u5f97\u7fa4\u4f53\u9884\u6d4b\u4e4b\u524d\u8fc7\u6ee4\u6389\u4e0d\u8fde\u8d2f\u7684\u610f\u89c1\u3002"}}
{"id": "2507.23515", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2507.23515", "abs": "https://arxiv.org/abs/2507.23515", "authors": ["Ana\u00efs Ollagnier", "Aline Menin"], "title": "DataLens: Enhancing Dataset Discovery via Network Topologies", "comment": null, "summary": "The rapid growth of publicly available textual resources, such as lexicons\nand domain-specific corpora, presents challenges in efficiently identifying\nrelevant resources. While repositories are emerging, they often lack advanced\nsearch and exploration features. Most search methods rely on keyword queries\nand metadata filtering, which require prior knowledge and fail to reveal\nconnections between resources. To address this, we present DataLens, a\nweb-based platform that combines faceted search with advanced visualization\ntechniques to enhance resource discovery. DataLens offers network-based\nvisualizations, where the network structure can be adapted to suit the specific\nanalysis task. It also supports a chained views approach, enabling users to\nexplore data from multiple perspectives. A formative user study involving six\ndata practitioners revealed that users highly value visualization\ntools-especially network-based exploration-and offered insights to help refine\nour approach to better support dataset search.", "AI": {"tldr": "DataLens is a web platform combining faceted search and visualizations for better resource discovery. User study validates the value of network-based exploration.", "motivation": "Emerging repositories lack advanced search and exploration features. Most search methods rely on keyword queries and metadata filtering, which require prior knowledge and fail to reveal connections between resources.", "method": "a web-based platform that combines faceted search with advanced visualization techniques", "result": "DataLens enhances resource discovery through network-based visualizations and a chained views approach.", "conclusion": "A formative user study with six data practitioners showed that users highly value visualization tools, especially network-based exploration, and provided insights for refining our approach to better support dataset search."}}
{"id": "2507.23209", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23209", "abs": "https://arxiv.org/abs/2507.23209", "authors": ["Wei-Wei Du", "Takuma Udagawa", "Kei Tateno"], "title": "Not Just What, But When: Integrating Irregular Intervals to LLM for Sequential Recommendation", "comment": "Accepted by RecSys 2025 short paper track", "summary": "Time intervals between purchasing items are a crucial factor in sequential\nrecommendation tasks, whereas existing approaches focus on item sequences and\noften overlook by assuming the intervals between items are static. However,\ndynamic intervals serve as a dimension that describes user profiling on not\nonly the history within a user but also different users with the same item\nhistory. In this work, we propose IntervalLLM, a novel framework that\nintegrates interval information into LLM and incorporates the novel\ninterval-infused attention to jointly consider information of items and\nintervals. Furthermore, unlike prior studies that address the cold-start\nscenario only from the perspectives of users and items, we introduce a new\nviewpoint: the interval perspective to serve as an additional metric for\nevaluating recommendation methods on the warm and cold scenarios. Extensive\nexperiments on 3 benchmarks with both traditional- and LLM-based baselines\ndemonstrate that our IntervalLLM achieves not only 4.4% improvements in average\nbut also the best-performing warm and cold scenarios across all users, items,\nand the proposed interval perspectives. In addition, we observe that the cold\nscenario from the interval perspective experiences the most significant\nperformance drop among all recommendation methods. This finding underscores the\nnecessity of further research on interval-based cold challenges and our\nintegration of interval information in the realm of sequential recommendation\ntasks. Our code is available here:\nhttps://github.com/sony/ds-research-code/tree/master/recsys25-IntervalLLM.", "AI": {"tldr": "IntervalLLM integrates interval information into LLM for sequential recommendation, achieving improvements in warm and cold scenarios. Interval-based cold challenges need further research.", "motivation": "Existing approaches overlook dynamic intervals between items, which serve as a dimension that describes user profiling.", "method": "Integrates interval information into LLM and incorporates the novel interval-infused attention.", "result": "IntervalLLM achieves 4.4% improvements in average and the best-performing warm and cold scenarios. The cold scenario from the interval perspective experiences the most significant performance drop.", "conclusion": "IntervalLLM achieves 4.4% improvements and best-performing warm and cold scenarios. Interval perspective experiences the most significant performance drop."}}
{"id": "2507.22911", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22911", "abs": "https://arxiv.org/abs/2507.22911", "authors": ["Jinzhi Wang", "Qingke Peng", "Haozhou Li", "Zeyuan Zeng", "Qinfeng Song", "Kaixuan Yang", "Jiangbo Zhang", "Yaoying Wang", "Ruimeng Li", "Biyi Zhou"], "title": "ElectriQ: A Benchmark for Assessing the Response Capability of Large Language Models in Power Marketing", "comment": null, "summary": "Electric power marketing customer service plays a critical role in addressing\ninquiries, complaints, and service requests. However, current systems, such as\nChina's 95598 hotline, often struggle with slow response times, inflexible\nprocedures, and limited accuracy in domain-specific tasks. While large language\nmodels (LLMs) like GPT-4o and Claude 3 demonstrate strong general capabilities,\nthey lack the domain expertise and empathy required in this field. To bridge\nthis gap, we introduce ElectriQ, the first benchmark designed to evaluate and\nenhance LLMs in electric power marketing scenarios. ElectriQ consists of a\ndialogue dataset covering six key service categories and introduces four\nevaluation metrics: professionalism, popularity, readability, and\nuser-friendliness. We further incorporate a domain-specific knowledge base and\npropose a knowledge augmentation method to boost model performance. Experiments\non 13 LLMs reveal that smaller models such as LLama3-8B, when fine-tuned and\naugmented, can surpass GPT-4o in terms of professionalism and\nuser-friendliness. ElectriQ establishes a comprehensive foundation for\ndeveloping LLMs tailored to the needs of power marketing services.", "AI": {"tldr": "ElectriQ\u662f\u4e00\u4e2a\u7535\u529b\u8425\u9500LLM\u57fa\u51c6\uff0c\u901a\u8fc7\u9886\u57df\u77e5\u8bc6\u589e\u5f3a\u548c\u5fae\u8c03\uff0c\u5c0f\u578bLLM\u53ef\u4ee5\u8d85\u8d8a\u5927\u578bLLM\u3002", "motivation": "\u5f53\u524d\u7684\u7535\u529b\u8425\u9500\u5ba2\u6237\u670d\u52a1\u7cfb\u7edf\u5b58\u5728\u54cd\u5e94\u65f6\u95f4\u6162\u3001\u7a0b\u5e8f\u4e0d\u7075\u6d3b\u4ee5\u53ca\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u51c6\u786e\u6027\u6709\u9650\u7b49\u95ee\u9898\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u867d\u7136\u5177\u6709\u5f3a\u5927\u7684\u901a\u7528\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u8be5\u9886\u57df\u7684\u4e13\u4e1a\u77e5\u8bc6\u548c\u540c\u7406\u5fc3\u3002", "method": "\u5f15\u5165ElectriQ\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u65e8\u5728\u8bc4\u4f30\u548c\u589e\u5f3a\u7535\u529b\u8425\u9500\u573a\u666f\u4e2dLLM\u7684\u57fa\u51c6\u3002", "result": "\u572813\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7ecf\u8fc7\u5fae\u8c03\u548c\u589e\u5f3a\u7684\u5c0f\u578b\u6a21\u578b\uff08\u5982LLama3-8B\uff09\u5728\u4e13\u4e1a\u6027\u548c\u7528\u6237\u53cb\u597d\u6027\u65b9\u9762\u53ef\u4ee5\u8d85\u8fc7GPT-4o\u3002", "conclusion": "ElectriQ\u5efa\u7acb\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u7840\uff0c\u7528\u4e8e\u5f00\u53d1\u4e3a\u7535\u529b\u8425\u9500\u670d\u52a1\u5b9a\u5236\u7684LLM\u3002"}}
{"id": "2507.23006", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23006", "abs": "https://arxiv.org/abs/2507.23006", "authors": ["Zhensheng Yuan", "Haozhi Huang", "Zhen Xiong", "Di Wang", "Guanghua Yang"], "title": "Robust and Efficient 3D Gaussian Splatting for Urban Scene Reconstruction", "comment": null, "summary": "We present a framework that enables fast reconstruction and real-time\nrendering of urban-scale scenes while maintaining robustness against appearance\nvariations across multi-view captures. Our approach begins with scene\npartitioning for parallel training, employing a visibility-based image\nselection strategy to optimize training efficiency. A controllable\nlevel-of-detail (LOD) strategy explicitly regulates Gaussian density under a\nuser-defined budget, enabling efficient training and rendering while\nmaintaining high visual fidelity. The appearance transformation module\nmitigates the negative effects of appearance inconsistencies across images\nwhile enabling flexible adjustments. Additionally, we utilize enhancement\nmodules, such as depth regularization, scale regularization, and antialiasing,\nto improve reconstruction fidelity. Experimental results demonstrate that our\nmethod effectively reconstructs urban-scale scenes and outperforms previous\napproaches in both efficiency and quality. The source code is available at:\nhttps://yzslab.github.io/REUrbanGS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5feb\u901f\u91cd\u5efa\u548c\u5b9e\u65f6\u6e32\u67d3\u57ce\u5e02\u89c4\u6a21\u7684\u573a\u666f\uff0c\u8be5\u6846\u67b6\u5728\u6548\u7387\u548c\u8d28\u91cf\u4e0a\u4f18\u4e8e\u4ee5\u524d\u7684\u65b9\u6cd5\u3002", "motivation": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u57ce\u5e02\u89c4\u6a21\u573a\u666f\u7684\u5feb\u901f\u91cd\u5efa\u548c\u5b9e\u65f6\u6e32\u67d3\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u591a\u89c6\u56fe\u6355\u83b7\u4e2d\u5916\u89c2\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u8fdb\u884c\u573a\u666f\u5206\u5272\u4ee5\u8fdb\u884c\u5e76\u884c\u8bad\u7ec3\uff0c\u91c7\u7528\u57fa\u4e8e\u53ef\u89c1\u6027\u7684\u56fe\u50cf\u9009\u62e9\u7b56\u7565\u6765\u4f18\u5316\u8bad\u7ec3\u6548\u7387\u3002\u53ef\u63a7\u7684\u7ec6\u8282\u5c42\u6b21 (LOD) \u7b56\u7565\u5728\u7528\u6237\u5b9a\u4e49\u7684\u9884\u7b97\u4e0b\u663e\u5f0f\u5730\u8c03\u8282\u9ad8\u65af\u5bc6\u5ea6\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u7684\u8bad\u7ec3\u548c\u6e32\u67d3\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002\u5916\u89c2\u53d8\u6362\u6a21\u5757\u51cf\u8f7b\u4e86\u56fe\u50cf\u4e4b\u95f4\u5916\u89c2\u4e0d\u4e00\u81f4\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u7075\u6d3b\u7684\u8c03\u6574\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5229\u7528\u589e\u5f3a\u6a21\u5757\uff0c\u4f8b\u5982\u6df1\u5ea6\u6b63\u5219\u5316\u3001\u5c3a\u5ea6\u6b63\u5219\u5316\u548c\u6297\u952f\u9f7f\uff0c\u4ee5\u63d0\u9ad8\u91cd\u5efa\u4fdd\u771f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u91cd\u5efa\u57ce\u5e02\u89c4\u6a21\u7684\u573a\u666f\uff0c\u5e76\u4e14\u5728\u6548\u7387\u548c\u8d28\u91cf\u4e0a\u4f18\u4e8e\u4ee5\u524d\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u91cd\u5efa\u57ce\u5e02\u89c4\u6a21\u7684\u573a\u666f\uff0c\u5e76\u4e14\u5728\u6548\u7387\u548c\u8d28\u91cf\u4e0a\u4f18\u4e8e\u4ee5\u524d\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.22956", "categories": ["cs.LG", "cs.HC", "K.3.1"], "pdf": "https://arxiv.org/pdf/2507.22956", "abs": "https://arxiv.org/abs/2507.22956", "authors": ["Dong Hyun Roh", "Rajesh Kumar", "An Ngo"], "title": "LLM-Assisted Cheating Detection in Korean Language via Keystrokes", "comment": "This paper has 11 pages, 6 figures, 2 tables, and has been accepted\n  for publication at IEEE-IJCB 2025", "summary": "This paper presents a keystroke-based framework for detecting LLM-assisted\ncheating in Korean, addressing key gaps in prior research regarding language\ncoverage, cognitive context, and the granularity of LLM involvement. Our\nproposed dataset includes 69 participants who completed writing tasks under\nthree conditions: Bona fide writing, paraphrasing ChatGPT responses, and\ntranscribing ChatGPT responses. Each task spans six cognitive processes defined\nin Bloom's Taxonomy (remember, understand, apply, analyze, evaluate, and\ncreate). We extract interpretable temporal and rhythmic features and evaluate\nmultiple classifiers under both Cognition-Aware and Cognition-Unaware settings.\nTemporal features perform well under Cognition-Aware evaluation scenarios,\nwhile rhythmic features generalize better under cross-cognition scenarios.\nMoreover, detecting bona fide and transcribed responses was easier than\nparaphrased ones for both the proposed models and human evaluators, with the\nmodels significantly outperforming the humans. Our findings affirm that\nkeystroke dynamics facilitate reliable detection of LLM-assisted writing across\nvarying cognitive demands and writing strategies, including paraphrasing and\ntranscribing LLM-generated responses.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u51fb\u952e\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u97e9\u8bed\u4e2d LLM \u8f85\u52a9\u7684\u4f5c\u5f0a\u884c\u4e3a\u3002", "motivation": "\u5148\u524d\u7684\u7814\u7a76\u5728\u8bed\u8a00\u8986\u76d6\u8303\u56f4\u3001\u8ba4\u77e5\u80cc\u666f\u548c LLM \u53c2\u4e0e\u7684\u7c92\u5ea6\u65b9\u9762\u5b58\u5728\u5173\u952e\u5dee\u8ddd\u3002", "method": "\u57fa\u4e8e\u51fb\u952e\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u97e9\u8bed\u4e2d LLM \u8f85\u52a9\u7684\u4f5c\u5f0a\u884c\u4e3a\u3002\u8be5\u6570\u636e\u96c6\u5305\u62ec 69 \u540d\u53c2\u4e0e\u8005\uff0c\u4ed6\u4eec\u5728\u4e09\u79cd\u6761\u4ef6\u4e0b\u5b8c\u6210\u5199\u4f5c\u4efb\u52a1\uff1a\u771f\u5b9e\u5199\u4f5c\u3001\u91ca\u4e49 ChatGPT \u56de\u590d\u548c\u8f6c\u5f55 ChatGPT \u56de\u590d\u3002\u6bcf\u4e2a\u4efb\u52a1\u8de8\u8d8a Bloom \u5206\u7c7b\u6cd5\u4e2d\u5b9a\u4e49\u7684\u516d\u4e2a\u8ba4\u77e5\u8fc7\u7a0b\u3002", "result": "\u65f6\u95f4\u7279\u5f81\u5728\u8ba4\u77e5\u611f\u77e5\u8bc4\u4f30\u573a\u666f\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u800c\u8282\u594f\u7279\u5f81\u5728\u8de8\u8ba4\u77e5\u573a\u666f\u4e0b\u6cdb\u5316\u5f97\u66f4\u597d\u3002\u68c0\u6d4b\u771f\u5b9e\u56de\u590d\u548c\u8f6c\u5f55\u56de\u590d\u6bd4\u91ca\u4e49\u56de\u590d\u66f4\u5bb9\u6613\uff0c\u5e76\u4e14\u6a21\u578b\u660e\u663e\u4f18\u4e8e\u4eba\u7c7b\u8bc4\u4f30\u8005\u3002", "conclusion": "\u952e\u76d8\u8f93\u5165\u52a8\u6001\u5b66\u6709\u52a9\u4e8e\u53ef\u9760\u5730\u68c0\u6d4b\u5728\u4e0d\u540c\u8ba4\u77e5\u9700\u6c42\u548c\u5199\u4f5c\u7b56\u7565\uff08\u5305\u62ec\u91ca\u4e49\u548c\u8f6c\u5f55 LLM \u751f\u6210\u7684\u56de\u590d\uff09\u4e0b LLM \u8f85\u52a9\u7684\u5199\u4f5c\u3002"}}
{"id": "2507.23191", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23191", "abs": "https://arxiv.org/abs/2507.23191", "authors": ["Meghyn Bienvenu", "Diego Figueira", "Pierre Lafourcade"], "title": "Tractable Responsibility Measures for Ontology-Mediated Query Answering", "comment": "Long version of a paper to appear at KR 2025, which contains further\n  proof details in the appendix", "summary": "Recent work on quantitative approaches to explaining query answers employs\nresponsibility measures to assign scores to facts in order to quantify their\nrespective contributions to obtaining a given answer. In this paper, we study\nthe complexity of computing such responsibility scores in the setting of\nontology-mediated query answering, focusing on a very recently introduced\nfamily of Shapley-value-based responsibility measures defined in terms of\nweighted sums of minimal supports (WSMS). By exploiting results from the\ndatabase setting, we can show that such measures enjoy polynomial data\ncomplexity for classes of ontology-mediated queries that are\nfirst-order-rewritable, whereas the problem becomes \"shP\"-hard when the\nontology language can encode reachability queries (via axioms like $\\exists R.\nA \\sqsubseteq A$). To better understand the tractability frontier, we next\nexplore the combined complexity of WSMS computation. We prove that\nintractability applies already to atomic queries if the ontology language\nsupports conjunction, as well as to unions of `well-behaved' conjunctive\nqueries, even in the absence of an ontology. By contrast, our study yields\npositive results for common DL-Lite dialects: by means of careful analysis, we\nidentify classes of structurally restricted conjunctive queries (which\nintuitively disallow undesirable interactions between query atoms) that admit\ntractable WSMS computation.", "AI": {"tldr": "\u7814\u7a76\u4e86\u672c\u4f53\u4ecb\u5bfc\u7684\u67e5\u8be2\u56de\u7b54\u4e2d\u8d23\u4efb\u5ea6\u91cf\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u5e76\u786e\u5b9a\u4e86\u5728\u54ea\u4e9b\u6761\u4ef6\u4e0b\u53ef\u4ee5\u6709\u6548\u5730\u8ba1\u7b97\u8fd9\u4e9b\u5ea6\u91cf\u3002", "motivation": "\u91cf\u5316\u67e5\u8be2\u7b54\u6848\u89e3\u91ca\u7684\u5b9a\u91cf\u65b9\u6cd5\u4f7f\u7528\u8d23\u4efb\u5ea6\u91cf\u6765\u4e3a\u4e8b\u5b9e\u5206\u914d\u5206\u6570\uff0c\u4ee5\u91cf\u5316\u5b83\u4eec\u5bf9\u83b7\u5f97\u7ed9\u5b9a\u7b54\u6848\u7684\u5404\u81ea\u8d21\u732e\u3002\u672c\u6587\u7814\u7a76\u4e86\u5728\u672c\u4f53\u4ecb\u5bfc\u7684\u67e5\u8be2\u56de\u7b54\u4e2d\u8ba1\u7b97\u8fd9\u79cd\u8d23\u4efb\u5206\u6570\u7684\u590d\u6742\u6027\u3002", "method": "\u7814\u7a76\u4e86\u57fa\u4e8e Shapley \u503c\u7684\u8d23\u4efb\u5ea6\u91cf\uff0c\u5e76\u5229\u7528\u6570\u636e\u5e93\u8bbe\u7f6e\u4e2d\u7684\u7ed3\u679c\u6765\u5206\u6790\u5176\u590d\u6742\u6027\u3002\u901a\u8fc7\u5bf9\u7ed3\u6784\u53d7\u9650\u7684\u5408\u53d6\u67e5\u8be2\u8fdb\u884c\u7ec6\u81f4\u5206\u6790\uff0c\u8bc6\u522b\u51fa\u53ef\u6709\u6548\u8ba1\u7b97 WSMS \u7684\u7c7b\u522b\u3002", "result": "\u5bf9\u4e8e\u4e00\u9636\u53ef\u91cd\u5199\u7684\u672c\u4f53\u4ecb\u5bfc\u7684\u67e5\u8be2\u7c7b\u522b\uff0c\u8fd9\u4e9b\u5ea6\u91cf\u5177\u6709\u591a\u9879\u5f0f\u6570\u636e\u590d\u6742\u6027\uff1b\u5f53\u672c\u4f53\u8bed\u8a00\u53ef\u4ee5\u7f16\u7801\u53ef\u8fbe\u6027\u67e5\u8be2\u65f6\uff0c\u95ee\u9898\u53d8\u6210\u201cshP\u201d-hard\u3002\u5982\u679c\u672c\u4f53\u8bed\u8a00\u652f\u6301\u5408\u53d6\uff0c\u5219\u5373\u4f7f\u5728\u6ca1\u6709\u672c\u4f53\u7684\u60c5\u51b5\u4e0b\uff0c\u539f\u5b50\u67e5\u8be2\u548c\u201c\u8868\u73b0\u826f\u597d\u201d\u7684\u5408\u53d6\u67e5\u8be2\u7684\u5e76\u96c6\u4e5f\u5b58\u5728\u96be\u5904\u7406\u6027\u3002\u5bf9\u4e8e\u5e38\u89c1\u7684 DL-Lite \u65b9\u8a00\uff0c\u8bc6\u522b\u51fa\u7ed3\u6784\u53d7\u9650\u7684\u5408\u53d6\u67e5\u8be2\u7c7b\u522b\uff0c\u5141\u8bb8\u8fdb\u884c\u6613\u4e8e\u5904\u7406\u7684 WSMS \u8ba1\u7b97\u3002", "conclusion": "\u5bf9\u4e8e\u672c\u4f53\u4ecb\u5bfc\u7684\u67e5\u8be2\u56de\u7b54\uff0c\u8ba1\u7b97\u57fa\u4e8e Shapley \u503c\u7684\u8d23\u4efb\u5ea6\u91cf\u7684\u590d\u6742\u6027\u95ee\u9898\uff0c\u5e76\u786e\u5b9a\u4e86\u5728\u54ea\u4e9b\u6761\u4ef6\u4e0b\u53ef\u4ee5\u6709\u6548\u5730\u8ba1\u7b97\u8fd9\u4e9b\u5ea6\u91cf\u3002"}}
{"id": "2507.23358", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.23358", "abs": "https://arxiv.org/abs/2507.23358", "authors": ["Renato Vukovic", "Carel van Niekerk", "Michael Heck", "Benjamin Ruppik", "Hsien-Chin Lin", "Shutong Feng", "Nurul Lubis", "Milica Gasic"], "title": "Text-to-SQL Task-oriented Dialogue Ontology Construction", "comment": null, "summary": "Large language models (LLMs) are widely used as general-purpose knowledge\nsources, but they rely on parametric knowledge, limiting explainability and\ntrustworthiness. In task-oriented dialogue (TOD) systems, this separation is\nexplicit, using an external database structured by an explicit ontology to\nensure explainability and controllability. However, building such ontologies\nrequires manual labels or supervised training. We introduce TeQoDO: a\nText-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM\nautonomously builds a TOD ontology from scratch without supervision using its\ninherent SQL programming capabilities combined with dialogue theory provided in\nthe prompt. We show that TeQoDO outperforms transfer learning approaches, and\nits constructed ontology is competitive on a downstream dialogue state tracking\ntask. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also\nscales to allow construction of much larger ontologies, which we investigate on\na Wikipedia and ArXiv dataset. We view this as a step towards broader\napplication of ontologies to increase LLM explainability.", "AI": {"tldr": "TeQoDO\u662f\u4e00\u79cd\u65e0\u9700\u76d1\u7763\u7684\u672c\u4f53\u6784\u5efa\u65b9\u6cd5\uff0c\u5b83\u5229\u7528LLM\u7684SQL\u7f16\u7a0b\u80fd\u529b\u548c\u5bf9\u8bdd\u7406\u8bba\u6765\u6784\u5efa\u4efb\u52a1\u578b\u5bf9\u8bdd\u672c\u4f53\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u63d0\u9ad8\u4e86LLM\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u88ab\u5e7f\u6cdb\u7528\u4f5c\u901a\u7528\u77e5\u8bc6\u6765\u6e90\uff0c\u4f46\u5b83\u4eec\u4f9d\u8d56\u4e8e\u53c2\u6570\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u3002\u5728\u9762\u5411\u4efb\u52a1\u7684\u5bf9\u8bdd\uff08TOD\uff09\u7cfb\u7edf\u4e2d\uff0c\u8fd9\u79cd\u5206\u79bb\u662f\u660e\u786e\u7684\uff0c\u4f7f\u7528\u7531\u663e\u5f0f\u672c\u4f53\u6784\u5efa\u7684\u5916\u90e8\u6570\u636e\u5e93\uff0c\u4ee5\u786e\u4fdd\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u63a7\u6027\u3002\u7136\u800c\uff0c\u6784\u5efa\u8fd9\u6837\u7684\u672c\u4f53\u9700\u8981\u624b\u52a8\u6807\u7b7e\u6216\u76d1\u7763\u8bad\u7ec3\u3002", "method": "TeQoDO\uff1a\u4e00\u79cd\u6587\u672c\u5230SQL\u7684\u4efb\u52a1\u578b\u5bf9\u8bdd\u672c\u4f53\u6784\u5efa\u65b9\u6cd5\uff0c\u5229\u7528LLM\u56fa\u6709\u7684SQL\u7f16\u7a0b\u80fd\u529b\uff0c\u7ed3\u5408prompt\u4e2d\u63d0\u4f9b\u7684\u5bf9\u8bdd\u7406\u8bba\uff0c\u81ea\u4e3b\u5730\u4ece\u5934\u5f00\u59cb\u6784\u5efaTOD\u672c\u4f53\uff0c\u65e0\u9700\u76d1\u7763\u3002", "result": "TeQoDO\u4f18\u4e8e\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "TeQoDO\u6784\u5efa\u7684\u672c\u4f53\u5728\u4e0b\u6e38\u5bf9\u8bdd\u72b6\u6001\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5e76\u4e14\u53ef\u4ee5\u6269\u5c55\u4ee5\u6784\u5efa\u66f4\u5927\u7684\u672c\u4f53\uff0c\u4ece\u800c\u63d0\u9ad8LLM\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.23267", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.23267", "abs": "https://arxiv.org/abs/2507.23267", "authors": ["D. T. Braithwaite", "Misael Cavalcanti", "R. Austin McEver", "Hiroto Udagawa", "Daniel Silva", "Rohan Ramanath", "Felipe Meneses", "Arissa Yoshida", "Evan Wingert", "Matheus Ramos", "Brian Zanfelice", "Aman Gupta"], "title": "Your Spending Needs Attention: Modeling Financial Habits with Transformers", "comment": null, "summary": "Predictive models play a crucial role in the financial industry, enabling\nrisk prediction, fraud detection, and personalized recommendations, where\nslight changes in core model performance can result in billions of dollars in\nrevenue or losses. While financial institutions have access to enormous amounts\nof user data (e.g., bank transactions, in-app events, and customer support\nlogs), leveraging this data effectively remains challenging due to its\ncomplexity and scale. Thus, in many financial institutions, most production\nmodels follow traditional machine learning (ML) approaches by converting\nunstructured data into manually engineered tabular features. Conversely, other\ndomains (e.g., natural language processing) have effectively utilized\nself-supervised learning (SSL) to learn rich representations from raw data,\nremoving the need for manual feature extraction. In this paper, we investigate\nusing transformer-based representation learning models for transaction data,\nhypothesizing that these models, trained on massive data, can provide a novel\nand powerful approach to understanding customer behavior. We propose a new\nmethod enabling the use of SSL with transaction data by adapting\ntransformer-based models to handle both textual and structured attributes. Our\napproach, denoted nuFormer, includes an end-to-end fine-tuning method that\nintegrates user embeddings with existing tabular features. Our experiments\ndemonstrate improvements for large-scale recommendation problems at Nubank.\nNotably, these gains are achieved solely through enhanced representation\nlearning rather than incorporating new data sources.", "AI": {"tldr": "This paper introduces nuFormer, a transformer-based representation learning model for transaction data that improves large-scale recommendation problems by enhancing representation learning.", "motivation": "Leveraging user data effectively remains challenging due to its complexity and scale. Thus, in many financial institutions, most production models follow traditional machine learning (ML) approaches by converting unstructured data into manually engineered tabular features. Conversely, other domains (e.g., natural language processing) have effectively utilized self-supervised learning (SSL) to learn rich representations from raw data, removing the need for manual feature extraction.", "method": "A new method enabling the use of SSL with transaction data by adapting transformer-based models to handle both textual and structured attributes. The approach, denoted nuFormer, includes an end-to-end fine-tuning method that integrates user embeddings with existing tabular features.", "result": "Transformer-based representation learning models for transaction data can provide a novel and powerful approach to understanding customer behavior.", "conclusion": "The experiments demonstrate improvements for large-scale recommendation problems at Nubank. Notably, these gains are achieved solely through enhanced representation learning rather than incorporating new data sources."}}
{"id": "2507.22912", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07, 68T50"], "pdf": "https://arxiv.org/pdf/2507.22912", "abs": "https://arxiv.org/abs/2507.22912", "authors": ["Navid Yazdanjue", "Morteza Rakhshaninejad", "Hossein Yazdanjouei", "Mohammad Sadegh Khorshidi", "Mikko S. Niemela", "Fang Chen", "Amir H. Gandomi"], "title": "A Language Model-Driven Semi-Supervised Ensemble Framework for Illicit Market Detection Across Deep/Dark Web and Social Platforms", "comment": "16 pages, 5 figures, 9 tables", "summary": "Illegal marketplaces have increasingly shifted to concealed parts of the\ninternet, including the deep and dark web, as well as platforms such as\nTelegram, Reddit, and Pastebin. These channels enable the anonymous trade of\nillicit goods including drugs, weapons, and stolen credentials. Detecting and\ncategorizing such content remains challenging due to limited labeled data, the\nevolving nature of illicit language, and the structural heterogeneity of online\nsources. This paper presents a hierarchical classification framework that\ncombines fine-tuned language models with a semi-supervised ensemble learning\nstrategy to detect and classify illicit marketplace content across diverse\nplatforms. We extract semantic representations using ModernBERT, a transformer\nmodel for long documents, finetuned on domain-specific data from deep and dark\nweb pages, Telegram channels, Subreddits, and Pastebin pastes to capture\nspecialized jargon and ambiguous linguistic patterns. In addition, we\nincorporate manually engineered features such as document structure, embedded\npatterns including Bitcoin addresses, emails, and IPs, and metadata, which\ncomplement language model embeddings. The classification pipeline operates in\ntwo stages. The first stage uses a semi-supervised ensemble of XGBoost, Random\nForest, and SVM with entropy-based weighted voting to detect sales-related\ndocuments. The second stage further classifies these into drug, weapon, or\ncredential sales. Experiments on three datasets, including our multi-source\ncorpus, DUTA, and CoDA, show that our model outperforms several baselines,\nincluding BERT, ModernBERT, DarkBERT, ALBERT, Longformer, and BigBird. The\nmodel achieves an accuracy of 0.96489, an F1-score of 0.93467, and a TMCC of\n0.95388, demonstrating strong generalization, robustness under limited\nsupervision, and effectiveness in real-world illicit content detection.", "AI": {"tldr": "detect and classify illicit marketplace content across diverse platforms using a hierarchical classification framework", "motivation": "Detecting and categorizing such content remains challenging due to limited labeled data, the evolving nature of illicit language, and the structural heterogeneity of online sources", "method": "a hierarchical classification framework that combines fine-tuned language models with a semi-supervised ensemble learning strategy", "result": "outperforms several baselines, including BERT, ModernBERT, DarkBERT, ALBERT, Longformer, and BigBird", "conclusion": "The model achieves an accuracy of 0.96489, an F1-score of 0.93467, and a TMCC of 0.95388, demonstrating strong generalization, robustness under limited supervision, and effectiveness in real-world illicit content detection."}}
{"id": "2507.23021", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23021", "abs": "https://arxiv.org/abs/2507.23021", "authors": ["Giuseppe Cartella", "Vittorio Cuculo", "Alessandro D'Amelio", "Marcella Cornia", "Giuseppe Boccignone", "Rita Cucchiara"], "title": "Modeling Human Gaze Behavior with Diffusion Models for Unified Scanpath Prediction", "comment": "Proceedings of the IEEE/CVF International Conference on Computer\n  Vision (ICCV), 2025", "summary": "Predicting human gaze scanpaths is crucial for understanding visual\nattention, with applications in human-computer interaction, autonomous systems,\nand cognitive robotics. While deep learning models have advanced scanpath\nprediction, most existing approaches generate averaged behaviors, failing to\ncapture the variability of human visual exploration. In this work, we present\nScanDiff, a novel architecture that combines diffusion models with Vision\nTransformers to generate diverse and realistic scanpaths. Our method explicitly\nmodels scanpath variability by leveraging the stochastic nature of diffusion\nmodels, producing a wide range of plausible gaze trajectories. Additionally, we\nintroduce textual conditioning to enable task-driven scanpath generation,\nallowing the model to adapt to different visual search objectives. Experiments\non benchmark datasets show that ScanDiff surpasses state-of-the-art methods in\nboth free-viewing and task-driven scenarios, producing more diverse and\naccurate scanpaths. These results highlight its ability to better capture the\ncomplexity of human visual behavior, pushing forward gaze prediction research.\nSource code and models are publicly available at\nhttps://aimagelab.github.io/ScanDiff.", "AI": {"tldr": "ScanDiff \u662f\u4e00\u79cd\u65b0\u7684\u67b6\u6784\uff0c\u5b83\u7ed3\u5408\u4e86\u6269\u6563\u6a21\u578b\u548c Vision Transformers \u6765\u751f\u6210\u591a\u6837\u4e14\u903c\u771f\u7684\u4eba\u7c7b gaze scanpath\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u9884\u6d4b\u4eba\u7c7b gaze scanpath \u5bf9\u4e8e\u7406\u89e3\u89c6\u89c9\u6ce8\u610f\u529b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u89c6\u89c9\u63a2\u7d22\u7684\u53ef\u53d8\u6027\u3002", "method": "\u7ed3\u5408\u4e86\u6269\u6563\u6a21\u578b\u548c Vision Transformers \u7684 ScanDiff \u67b6\u6784", "result": "ScanDiff \u80fd\u591f\u751f\u6210\u591a\u6837\u4e14\u903c\u771f\u7684 scanpath\uff0c\u5e76\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\u6c34\u5e73\u3002", "conclusion": "ScanDiff \u5728\u81ea\u7531\u89c2\u770b\u548c\u4efb\u52a1\u9a71\u52a8\u573a\u666f\u4e2d\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u751f\u6210\u4e86\u66f4\u591a\u6837\u5316\u548c\u51c6\u786e\u7684 scanpath\uff0c\u66f4\u597d\u5730\u6355\u6349\u4e86\u4eba\u7c7b\u89c6\u89c9\u884c\u4e3a\u7684\u590d\u6742\u6027\uff0c\u63a8\u52a8\u4e86 gaze prediction \u7814\u7a76\u3002"}}
{"id": "2507.22959", "categories": ["cs.LG", "cs.CE", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2507.22959", "abs": "https://arxiv.org/abs/2507.22959", "authors": ["Salah A. Faroughi", "Farinaz Mostajeran", "Amin Hamed Mashhadzadeh", "Shirko Faroughi"], "title": "Scientific Machine Learning with Kolmogorov-Arnold Networks", "comment": null, "summary": "The field of scientific machine learning, which originally utilized\nmultilayer perceptrons (MLPs), is increasingly adopting Kolmogorov-Arnold\nNetworks (KANs) for data encoding. This shift is driven by the limitations of\nMLPs, including poor interpretability, fixed activation functions, and\ndifficulty capturing localized or high-frequency features. KANs address these\nissues with enhanced interpretability and flexibility, enabling more efficient\nmodeling of complex nonlinear interactions and effectively overcoming the\nconstraints associated with conventional MLP architectures. This review\ncategorizes recent progress in KAN-based models across three distinct\nperspectives: (i) data-driven learning, (ii) physics-informed modeling, and\n(iii) deep operator learning. Each perspective is examined through the lens of\narchitectural design, training strategies, application efficacy, and\ncomparative evaluation against MLP-based counterparts. By benchmarking KANs\nagainst MLPs, we highlight consistent improvements in accuracy, convergence,\nand spectral representation, clarifying KANs' advantages in capturing complex\ndynamics while learning more effectively. Finally, this review identifies\ncritical challenges and open research questions in KAN development,\nparticularly regarding computational efficiency, theoretical guarantees,\nhyperparameter tuning, and algorithm complexity. We also outline future\nresearch directions aimed at improving the robustness, scalability, and\nphysical consistency of KAN-based frameworks.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86Kolmogorov-Arnold Networks (KANs)\u5728\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u8fdb\u5c55\uff0c\u7a81\u51fa\u4e86\u5176\u76f8\u5bf9\u4e8e\u591a\u5c42\u611f\u77e5\u5668 (MLP) \u7684\u4f18\u52bf\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u591a\u5c42\u611f\u77e5\u5668 (MLP) \u7684\u5c40\u9650\u6027\uff0c\u5305\u62ec\u8f83\u5dee\u7684\u53ef\u89e3\u91ca\u6027\u3001\u56fa\u5b9a\u7684\u6fc0\u6d3b\u51fd\u6570\u4ee5\u53ca\u96be\u4ee5\u6355\u83b7\u5c40\u90e8\u6216\u9ad8\u9891\u7279\u5f81\uff0c\u63a8\u52a8\u4e86\u4eceMLP\u5230Kolmogorov-Arnold Networks (KANs) \u7684\u8f6c\u53d8\u3002", "method": "\u901a\u8fc7\u67b6\u6784\u8bbe\u8ba1\u3001\u8bad\u7ec3\u7b56\u7565\u3001\u5e94\u7528\u6548\u679c\u4ee5\u53ca\u4e0e\u57fa\u4e8eMLP\u7684\u5bf9\u5e94\u7269\u7684\u6bd4\u8f83\u8bc4\u4f30\uff0c\u5bf9\u6bcf\u4e2a\u89d2\u5ea6\u8fdb\u884c\u68c0\u67e5\u3002", "result": "\u901a\u8fc7\u9488\u5bf9MLP\u5bf9KAN\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6211\u4eec\u5f3a\u8c03\u4e86\u5728\u51c6\u786e\u6027\u3001\u6536\u655b\u6027\u548c\u9891\u8c31\u8868\u793a\u65b9\u9762\u7684\u4e00\u81f4\u6539\u8fdb\uff0c\u9610\u660e\u4e86KAN\u5728\u6355\u83b7\u590d\u6742\u52a8\u529b\u5b66\u7684\u540c\u65f6\u66f4\u6709\u6548\u5730\u5b66\u4e60\u7684\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u57fa\u4e8eKAN\u7684\u6a21\u578b\u5728\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u3001\u7269\u7406\u4fe1\u606f\u5efa\u6a21\u548c\u6df1\u5ea6\u7b97\u5b50\u5b66\u4e60\u4e09\u4e2a\u4e0d\u540c\u89d2\u5ea6\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u786e\u5b9a\u4e86KAN\u5f00\u53d1\u4e2d\u7684\u5173\u952e\u6311\u6218\u548c\u5f00\u653e\u7684\u7814\u7a76\u95ee\u9898\u3002"}}
{"id": "2507.23197", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23197", "abs": "https://arxiv.org/abs/2507.23197", "authors": ["Yuke Liao", "Blaise Genest", "Kuldeep Meel", "Shaan Aryaman"], "title": "Solution-aware vs global ReLU selection: partial MILP strikes back for DNN verification", "comment": null, "summary": "To handle complex instances, we revisit a divide-and-conquer approach to\nbreak down the complexity: instead of few complex BaB calls, we rely on many\nsmall {\\em partial} MILP calls. The crucial step is to select very few but very\nimportant ReLUs to treat using (costly) binary variables. The previous attempts\nwere suboptimal in that respect. To select these important ReLU variables, we\npropose a novel {\\em solution-aware} ReLU scoring ({\\sf SAS}), as well as adapt\nthe BaB-SR and BaB-FSB branching functions as {\\em global} ReLU scoring ({\\sf\nGS}) functions. We compare them theoretically as well as experimentally, and\n{\\sf SAS} is more efficient at selecting a set of variables to open using\nbinary variables. Compared with previous attempts, SAS reduces the number of\nbinary variables by around 6 times, while maintaining the same level of\naccuracy. Implemented in {\\em Hybrid MILP}, calling first $\\alpha,\\beta$-CROWN\nwith a short time-out to solve easier instances, and then partial MILP,\nproduces a very accurate yet efficient verifier, reducing by up to $40\\%$ the\nnumber of undecided instances to low levels ($8-15\\%$), while keeping a\nreasonable runtime ($46s-417s$ on average per instance), even for fairly large\nCNNs with 2 million parameters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684ReLU\u8bc4\u5206\u65b9\u6cd5\uff08SAS\uff09\uff0c\u7ed3\u5408Hybrid MILP\uff0c\u63d0\u9ad8\u4e86\u9a8c\u8bc1\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u672a\u786e\u5b9a\u5b9e\u4f8b\u7684\u6570\u91cf\u3002", "motivation": "\u4e3a\u4e86\u5904\u7406\u590d\u6742\u5b9e\u4f8b\uff0c\u91cd\u65b0\u5ba1\u89c6\u5206\u800c\u6cbb\u4e4b\u7684\u65b9\u6cd5\u6765\u5206\u89e3\u590d\u6742\u6027\uff1a\u4e0d\u4f9d\u8d56\u4e8e\u5c11\u6570\u590d\u6742\u7684BaB\u8c03\u7528\uff0c\u800c\u662f\u4f9d\u8d56\u4e8e\u8bb8\u591a\u5c0f\u7684partial MILP\u8c03\u7528\u3002\u5173\u952e\u6b65\u9aa4\u662f\u9009\u62e9\u975e\u5e38\u5c11\u4f46\u975e\u5e38\u91cd\u8981\u7684ReLU\uff0c\u4f7f\u7528\uff08\u6602\u8d35\u7684\uff09\u4e8c\u5143\u53d8\u91cf\u6765\u5904\u7406\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u611f\u77e5ReLU\u8bc4\u5206\uff08SAS\uff09\uff0c\u5e76\u8c03\u6574BaB-SR\u548cBaB-FSB\u5206\u652f\u51fd\u6570\u4f5c\u4e3a\u5168\u5c40ReLU\u8bc4\u5206\uff08GS\uff09\u51fd\u6570\u3002", "result": "SAS\u5728\u9009\u62e9\u4e00\u7ec4\u4f7f\u7528\u4e8c\u5143\u53d8\u91cf\u6253\u5f00\u7684\u53d8\u91cf\u65f6\u6548\u7387\u66f4\u9ad8\u3002\u4e0e\u4e4b\u524d\u7684\u5c1d\u8bd5\u76f8\u6bd4\uff0cSAS\u5c06\u4e8c\u5143\u53d8\u91cf\u7684\u6570\u91cf\u51cf\u5c11\u4e86\u7ea66\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u540c\u7684\u7cbe\u5ea6\u3002\u4e0e\u4e4b\u524d\u7684\u5c1d\u8bd5\u76f8\u6bd4\uff0c\u5728\u4fdd\u6301\u76f8\u540c\u6c34\u5e73\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\uff0cSAS\u5c06\u4e8c\u5143\u53d8\u91cf\u7684\u6570\u91cf\u51cf\u5c11\u4e86\u5927\u7ea6 6 \u500d\u3002\u672a\u786e\u5b9a\u5b9e\u4f8b\u7684\u6570\u91cf\u51cf\u5c11\u4e86\u9ad8\u8fbe 40%\uff0c\u964d\u81f3\u8f83\u4f4e\u6c34\u5e73\uff088-15%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5408\u7406\u7684\u8fd0\u884c\u65f6\u95f4\uff08\u6bcf\u4e2a\u5b9e\u4f8b\u5e73\u5747 46 \u79d2-417 \u79d2\uff09\u3002", "conclusion": "Hybrid MILP\u7ed3\u5408$\\alpha,\\beta$-CROWN\u548cpartial MILP\uff0c\u80fd\u6709\u6548\u51cf\u5c11\u672a\u786e\u5b9a\u5b9e\u4f8b\u7684\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u5408\u7406\u7684\u8fd0\u884c\u65f6\u95f4\u3002"}}
{"id": "2507.23429", "categories": ["cs.AI", "cs.DB", "cs.ET", "cs.HC", "cs.MA", "68T50, 68P20", "I.2.7; H.2.5; H.2.8; H.5.m"], "pdf": "https://arxiv.org/pdf/2507.23429", "abs": "https://arxiv.org/abs/2507.23429", "authors": ["Jorge Ruiz G\u00f3mez", "Lidia Andr\u00e9s Susinos", "Jorge Alamo Oliv\u00e9", "Sonia Rey Osorno", "Manuel Luis Gonzalez Hern\u00e1ndez"], "title": "Chatting with your ERP: A Recipe", "comment": "11 pages, includes 3 tables summarizing schema and model performance.\n  Submitted on July 31, 2025. Targets integration of LLM agents with ERP\n  systems using open-weight models and Ollama deployment", "summary": "This paper presents the design, implementation, and evaluation behind a Large\nLanguage Model (LLM) agent that chats with an industrial production-grade ERP\nsystem. The agent is capable of interpreting natural language queries and\ntranslating them into executable SQL statements, leveraging open-weight LLMs. A\nnovel dual-agent architecture combining reasoning and critique stages was\nproposed to improve query generation reliability.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cdLLM\u4ee3\u7406\uff0c\u5b83\u53ef\u4ee5\u5c06\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u6362\u4e3aSQL\u67e5\u8be2\uff0c\u4e0eERP\u7cfb\u7edf\u8fdb\u884c\u4ea4\u4e92\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u4ee3\u7406\u53ef\u4ee5\u89e3\u91ca\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\uff0c\u5e76\u5c06\u5176\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684SQL\u8bed\u53e5\u3002", "method": "\u4e00\u79cd\u7ed3\u5408\u63a8\u7406\u548c\u6279\u5224\u9636\u6bb5\u7684\u65b0\u578b\u53cc\u4ee3\u7406\u67b6\u6784\u88ab\u63d0\u51fa\uff0c\u4ee5\u63d0\u9ad8\u67e5\u8be2\u751f\u6210\u53ef\u9760\u6027\u3002", "result": "\u5229\u7528\u5f00\u653e\u6743\u91cd\u7684llm\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e0e\u5de5\u4e1a\u751f\u4ea7\u7ea7ERP\u7cfb\u7edf\u804a\u5929\u7684LLM\u4ee3\u7406\u7684\u8bbe\u8ba1\u3001\u5b9e\u73b0\u548c\u8bc4\u4f30\u3002"}}
{"id": "2507.23364", "categories": ["cs.IR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23364", "abs": "https://arxiv.org/abs/2507.23364", "authors": ["Thomas Compton"], "title": "Holistic Evaluations of Topic Models", "comment": "10 pages, 6 tables", "summary": "Topic models are gaining increasing commercial and academic interest for\ntheir ability to summarize large volumes of unstructured text. As unsupervised\nmachine learning methods, they enable researchers to explore data and help\ngeneral users understand key themes in large text collections. However, they\nrisk becoming a 'black box', where users input data and accept the output as an\naccurate summary without scrutiny. This article evaluates topic models from a\ndatabase perspective, drawing insights from 1140 BERTopic model runs. The goal\nis to identify trade-offs in optimizing model parameters and to reflect on what\nthese findings mean for the interpretation and responsible use of topic models", "AI": {"tldr": "evaluate topic models from a database perspective", "motivation": "summarize large volumes of unstructured text", "method": "evaluate topic models from a database perspective, drawing insights from 1140 BERTopic model runs", "result": "identify trade-offs in optimizing model parameters", "conclusion": "optimize model parameters and responsible use of topic models"}}
{"id": "2507.22913", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22913", "abs": "https://arxiv.org/abs/2507.22913", "authors": ["Jinyu Liu", "Xiaoying Song", "Diana Zhang", "Jason Thomale", "Daqing He", "Lingzi Hong"], "title": "A Hybrid Framework for Subject Analysis: Integrating Embedding-Based Regression Models with Large Language Models", "comment": "13 pages, 2 figures, accepted by ASIST 2025", "summary": "Providing subject access to information resources is an essential function of\nany library management system. Large language models (LLMs) have been widely\nused in classification and summarization tasks, but their capability to perform\nsubject analysis is underexplored. Multi-label classification with traditional\nmachine learning (ML) models has been used for subject analysis but struggles\nwith unseen cases. LLMs offer an alternative but often over-generate and\nhallucinate. Therefore, we propose a hybrid framework that integrates\nembedding-based ML models with LLMs. This approach uses ML models to (1)\npredict the optimal number of LCSH labels to guide LLM predictions and (2)\npost-edit the predicted terms with actual LCSH terms to mitigate\nhallucinations. We experimented with LLMs and the hybrid framework to predict\nthe subject terms of books using the Library of Congress Subject Headings\n(LCSH). Experiment results show that providing initial predictions to guide LLM\ngenerations and imposing post-edits result in more controlled and\nvocabulary-aligned outputs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u57fa\u4e8e\u5d4c\u5165\u7684 ML \u6a21\u578b\u4e0e LLM\uff0c\u4ee5\u9884\u6d4b\u56fe\u4e66\u7684\u4e3b\u9898\u8bcd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5df2\u5e7f\u6cdb\u7528\u4e8e\u5206\u7c7b\u548c\u6458\u8981\u4efb\u52a1\uff0c\u4f46\u5b83\u4eec\u6267\u884c\u4e3b\u9898\u5206\u6790\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u4f7f\u7528\u4f20\u7edf\u673a\u5668\u5b66\u4e60 (ML) \u6a21\u578b\u7684\u591a\u6807\u7b7e\u5206\u7c7b\u5df2\u7528\u4e8e\u4e3b\u9898\u5206\u6790\uff0c\u4f46\u5728\u672a\u89c1\u8fc7\u7684\u60c5\u51b5\u4e0b\u4f1a\u9047\u5230\u56f0\u96be\u3002LLM \u63d0\u4f9b\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u901a\u5e38\u4f1a\u8fc7\u5ea6\u751f\u6210\u548c\u4ea7\u751f\u5e7b\u89c9\u3002", "method": "\u96c6\u6210\u57fa\u4e8e\u5d4c\u5165\u7684 ML \u6a21\u578b\u4e0e LLM \u7684\u6df7\u5408\u6846\u67b6\uff0c\u4f7f\u7528 ML \u6a21\u578b\u6765 (1) \u9884\u6d4b\u6700\u4f73 LCSH \u6807\u7b7e\u6570\u91cf\u4ee5\u6307\u5bfc LLM \u9884\u6d4b\uff0c\u4ee5\u53ca (2) \u4f7f\u7528\u5b9e\u9645 LCSH \u672f\u8bed\u5bf9\u9884\u6d4b\u7684\u672f\u8bed\u8fdb\u884c\u540e\u671f\u7f16\u8f91\uff0c\u4ee5\u51cf\u8f7b\u5e7b\u89c9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u4f9b\u521d\u59cb\u9884\u6d4b\u4ee5\u6307\u5bfc LLM \u751f\u6210\u5e76\u5f3a\u5236\u8fdb\u884c\u540e\u671f\u7f16\u8f91\u53ef\u4ee5\u4ea7\u751f\u66f4\u53d7\u63a7\u4e14\u4e0e\u8bcd\u6c47\u5bf9\u9f50\u7684\u8f93\u51fa\u3002", "conclusion": "\u5f15\u5bfc LLM \u751f\u6210\u5e76\u8fdb\u884c\u540e\u671f\u7f16\u8f91\u53ef\u4ee5\u4ea7\u751f\u66f4\u53d7\u63a7\u4e14\u4e0e\u8bcd\u6c47\u5bf9\u9f50\u7684\u8f93\u51fa\u3002"}}
{"id": "2507.23027", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23027", "abs": "https://arxiv.org/abs/2507.23027", "authors": ["Krishan Agyakari Raja Babu", "Om Prabhu", "Annu", "Mohanasankar Sivaprakasam"], "title": "Recovering Diagnostic Value: Super-Resolution-Aided Echocardiographic Classification in Resource-Constrained Imaging", "comment": "Accepted at the MICCAI Workshop on \"Medical Image Computing in\n  Resource Constrained Settings & Knowledge Interchange (MIRASOL)\" 2025", "summary": "Automated cardiac interpretation in resource-constrained settings (RCS) is\noften hindered by poor-quality echocardiographic imaging, limiting the\neffectiveness of downstream diagnostic models. While super-resolution (SR)\ntechniques have shown promise in enhancing magnetic resonance imaging (MRI) and\ncomputed tomography (CT) scans, their application to echocardiography-a widely\naccessible but noise-prone modality-remains underexplored. In this work, we\ninvestigate the potential of deep learning-based SR to improve classification\naccuracy on low-quality 2D echocardiograms. Using the publicly available CAMUS\ndataset, we stratify samples by image quality and evaluate two clinically\nrelevant tasks of varying complexity: a relatively simple Two-Chamber vs.\nFour-Chamber (2CH vs. 4CH) view classification and a more complex End-Diastole\nvs. End-Systole (ED vs. ES) phase classification. We apply two widely used SR\nmodels-Super-Resolution Generative Adversarial Network (SRGAN) and\nSuper-Resolution Residual Network (SRResNet), to enhance poor-quality images\nand observe significant gains in performance metric-particularly with SRResNet,\nwhich also offers computational efficiency. Our findings demonstrate that SR\ncan effectively recover diagnostic value in degraded echo scans, making it a\nviable tool for AI-assisted care in RCS, achieving more with less.", "AI": {"tldr": "This paper investigates the potential of deep learning-based SR to improve classification accuracy on low-quality 2D echocardiograms.", "motivation": "Automated cardiac interpretation in resource-constrained settings (RCS) is often hindered by poor-quality echocardiographic imaging, limiting the effectiveness of downstream diagnostic models. While super-resolution (SR) techniques have shown promise in enhancing magnetic resonance imaging (MRI) and computed tomography (CT) scans, their application to echocardiography-a widely accessible but noise-prone modality-remains underexplored.", "method": "deep learning-based SR to improve classification accuracy on low-quality 2D echocardiograms. Using the publicly available CAMUS dataset, we stratify samples by image quality and evaluate two clinically relevant tasks of varying complexity: a relatively simple Two-Chamber vs. Four-Chamber (2CH vs. 4CH) view classification and a more complex End-Diastole vs. End-Systole (ED vs. ES) phase classification. We apply two widely used SR models-Super-Resolution Generative Adversarial Network (SRGAN) and Super-Resolution Residual Network (SRResNet), to enhance poor-quality images", "result": "observe significant gains in performance metric-particularly with SRResNet, which also offers computational efficiency.", "conclusion": "SR can effectively recover diagnostic value in degraded echo scans, making it a viable tool for AI-assisted care in RCS, achieving more with less."}}
{"id": "2507.22962", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22962", "abs": "https://arxiv.org/abs/2507.22962", "authors": ["Boyuan Zheng", "Victor W. Chu"], "title": "Multi-Hazard Early Warning Systems for Agriculture with Featural-Temporal Explanations", "comment": "Pre-print v0.8 2025-07-30", "summary": "Climate extremes present escalating risks to agriculture intensifying the\nneed for reliable multi-hazard early warning systems (EWS). The situation is\nevolving due to climate change and hence such systems should have the\nintelligent to continue to learn from recent climate behaviours. However,\ntraditional single-hazard forecasting methods fall short in capturing complex\ninteractions among concurrent climatic events. To address this deficiency, in\nthis paper, we combine sequential deep learning models and advanced Explainable\nArtificial Intelligence (XAI) techniques to introduce a multi-hazard\nforecasting framework for agriculture. In our experiments, we utilize\nmeteorological data from four prominent agricultural regions in the United\nStates (between 2010 and 2023) to validate the predictive accuracy of our\nframework on multiple severe event types, which are extreme cold, floods,\nfrost, hail, heatwaves, and heavy rainfall, with tailored models for each area.\nThe framework uniquely integrates attention mechanisms with TimeSHAP (a\nrecurrent XAI explainer for time series) to provide comprehensive temporal\nexplanations revealing not only which climatic features are influential but\nprecisely when their impacts occur. Our results demonstrate strong predictive\naccuracy, particularly with the BiLSTM architecture, and highlight the system's\ncapacity to inform nuanced, proactive risk management strategies. This research\nsignificantly advances the explainability and applicability of multi-hazard\nEWS, fostering interdisciplinary trust and effective decision-making process\nfor climate risk management in the agricultural industry.", "AI": {"tldr": "The paper introduces a multi-hazard forecasting framework for agriculture using deep learning and XAI, achieving strong predictive accuracy and detailed temporal explanations of climate impacts.", "motivation": "Climate extremes pose increasing risks to agriculture, necessitating reliable multi-hazard early warning systems that can learn from recent climate behaviors. Traditional methods fail to capture complex interactions among concurrent climatic events.", "method": "The paper combines sequential deep learning models and Explainable Artificial Intelligence (XAI) techniques to introduce a multi-hazard forecasting framework.", "result": "The framework demonstrates strong predictive accuracy, particularly with the BiLSTM architecture, and provides comprehensive temporal explanations of influential climatic features and their impacts.", "conclusion": "This research advances the explainability and applicability of multi-hazard early warning systems, fostering trust and effective decision-making for climate risk management in agriculture."}}
{"id": "2507.23276", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23276", "abs": "https://arxiv.org/abs/2507.23276", "authors": ["Qiujie Xie", "Yixuan Weng", "Minjun Zhu", "Fuchen Shen", "Shulin Huang", "Zhen Lin", "Jiahui Zhou", "Zilan Mao", "Zijie Yang", "Linyi Yang", "Jian Wu", "Yue Zhang"], "title": "How Far Are AI Scientists from Changing the World?", "comment": null, "summary": "The emergence of large language models (LLMs) is propelling automated\nscientific discovery to the next level, with LLM-based Artificial Intelligence\n(AI) Scientist systems now taking the lead in scientific research. Several\ninfluential works have already appeared in the field of AI Scientist systems,\nwith AI-generated research papers having been accepted at the ICLR 2025\nworkshop, suggesting that a human-level AI Scientist capable of uncovering\nphenomena previously unknown to humans, may soon become a reality. In this\nsurvey, we focus on the central question: How far are AI scientists from\nchanging the world and reshaping the scientific research paradigm? To answer\nthis question, we provide a prospect-driven review that comprehensively\nanalyzes the current achievements of AI Scientist systems, identifying key\nbottlenecks and the critical components required for the emergence of a\nscientific agent capable of producing ground-breaking discoveries that solve\ngrand challenges. We hope this survey will contribute to a clearer\nunderstanding of limitations of current AI Scientist systems, showing where we\nare, what is missing, and what the ultimate goals for scientific AI should be.", "AI": {"tldr": "This survey reviews AI Scientist systems, analyzes their achievements, identifies bottlenecks, and discusses the components needed for groundbreaking discoveries.", "motivation": "The emergence of large language models (LLMs) is propelling automated scientific discovery to the next level", "method": "prospect-driven review that comprehensively analyzes the current achievements of AI Scientist systems", "result": "identifying key bottlenecks and the critical components required for the emergence of a scientific agent capable of producing ground-breaking discoveries that solve grand challenges", "conclusion": "This survey contributes to a clearer understanding of limitations of current AI Scientist systems, showing where we are, what is missing, and what the ultimate goals for scientific AI should be."}}
{"id": "2507.23410", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2507.23410", "abs": "https://arxiv.org/abs/2507.23410", "authors": ["Alexander Felfernig", "Damian Garber", "Viet-Man Le", "Sebastian Lubos", "Thi Ngoc Trang Tran"], "title": "Towards LLM-Enhanced Product Line Scoping", "comment": null, "summary": "The idea of product line scoping is to identify the set of features and\nconfigurations that a product line should include, i.e., offer for\nconfiguration purposes. In this context, a major scoping task is to find a\nbalance between commercial relevance and technical feasibility. Traditional\nproduct line scoping approaches rely on formal feature models and require a\nmanual analysis which can be quite time-consuming. In this paper, we sketch how\nLarge Language Models (LLMs) can be applied to support product line scoping\ntasks with a natural language interaction based scoping process. Using a\nworking example from the smarthome domain, we sketch how LLMs can be applied to\nevaluate different feature model alternatives. We discuss open research\nchallenges regarding the integration of LLMs with product line scoping.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5982\u4f55\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u6539\u8fdb\u4ea7\u54c1\u7ebf\u8303\u56f4\u754c\u5b9a\uff0c\u4ece\u800c\u5728\u5546\u4e1a\u76f8\u5173\u6027\u548c\u6280\u672f\u53ef\u884c\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u4ea7\u54c1\u7ebf\u8303\u56f4\u754c\u5b9a\u7684\u76ee\u7684\u662f\u786e\u5b9a\u4ea7\u54c1\u7ebf\u5e94\u5305\u62ec\u7684\u529f\u80fd\u548c\u914d\u7f6e\u96c6\u3002\u4f20\u7edf\u7684\u4ea7\u54c1\u7ebf\u8303\u56f4\u754c\u5b9a\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6b63\u5f0f\u7684\u7279\u5f81\u6a21\u578b\uff0c\u5e76\u4e14\u9700\u8981\u624b\u52a8\u5206\u6790\uff0c\u8fd9\u975e\u5e38\u8017\u65f6\u3002", "method": "\u4f7f\u7528\u6765\u81ea\u667a\u80fd\u5bb6\u5c45\u9886\u57df\u7684\u5de5\u4f5c\u793a\u4f8b\uff0c\u6982\u8ff0\u4e86\u5982\u4f55\u5e94\u7528LLM\u6765\u8bc4\u4f30\u4e0d\u540c\u7684\u7279\u5f81\u6a21\u578b\u66ff\u4ee3\u65b9\u6848\u3002", "result": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u53ef\u7528\u4e8e\u652f\u6301\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u7684\u8303\u56f4\u754c\u5b9a\u8fc7\u7a0b\u7684\u4ea7\u54c1\u7ebf\u8303\u56f4\u754c\u5b9a\u4efb\u52a1\u3002", "conclusion": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5e94\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u652f\u6301\u4ea7\u54c1\u7ebf\u8303\u56f4\u754c\u5b9a\u4efb\u52a1\uff0c\u5e76\u8ba8\u8bba\u4e86\u5c06LLM\u4e0e\u4ea7\u54c1\u7ebf\u8303\u56f4\u754c\u5b9a\u76f8\u7ed3\u5408\u7684\u5f00\u653e\u7814\u7a76\u6311\u6218\u3002"}}
{"id": "2507.22914", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22914", "abs": "https://arxiv.org/abs/2507.22914", "authors": ["Victor Eiti Yamamoto", "Hideaki Takeda"], "title": "Full Triple Matcher: Integrating all triple elements between heterogeneous Knowledge Graphs", "comment": null, "summary": "Knowledge graphs (KGs) are powerful tools for representing and reasoning over\nstructured information. Their main components include schema, identity, and\ncontext. While schema and identity matching are well-established in ontology\nand entity matching research, context matching remains largely unexplored. This\nis particularly important because real-world KGs often vary significantly in\nsource, size, and information density - factors not typically represented in\nthe datasets on which current entity matching methods are evaluated. As a\nresult, existing approaches may fall short in scenarios where diverse and\ncomplex contexts need to be integrated.\n  To address this gap, we propose a novel KG integration method consisting of\nlabel matching and triple matching. We use string manipulation, fuzzy matching,\nand vector similarity techniques to align entity and predicate labels. Next, we\nidentify mappings between triples that convey comparable information, using\nthese mappings to improve entity-matching accuracy. Our approach demonstrates\ncompetitive performance compared to leading systems in the OAEI competition and\nagainst supervised methods, achieving high accuracy across diverse test cases.\nAdditionally, we introduce a new dataset derived from the benchmark dataset to\nevaluate the triple-matching step more comprehensively.", "AI": {"tldr": "This paper introduces a new knowledge graph integration method that uses label and triple matching to improve entity-matching accuracy, especially in diverse and complex contexts. It outperforms existing systems and includes a new dataset for evaluating triple matching.", "motivation": "Context matching in knowledge graphs (KGs) remains largely unexplored, which is important because real-world KGs often vary significantly in source, size, and information density. Existing approaches may fall short in scenarios where diverse and complex contexts need to be integrated.", "method": "A novel KG integration method consisting of label matching and triple matching. String manipulation, fuzzy matching, and vector similarity techniques are used to align entity and predicate labels. Mappings between triples that convey comparable information are identified to improve entity-matching accuracy.", "result": "The approach demonstrates competitive performance compared to leading systems and against supervised methods, achieving high accuracy across diverse test cases.", "conclusion": "The proposed KG integration method demonstrates competitive performance compared to leading systems and achieves high accuracy across diverse test cases. A new dataset is introduced to evaluate the triple-matching step more comprehensively."}}
{"id": "2507.23033", "categories": ["cs.CV", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.23033", "abs": "https://arxiv.org/abs/2507.23033", "authors": ["Ranxi Lin", "Canming Yao", "Jiayi Li", "Weihang Liu", "Xin Lou", "Pingqiang Zhou"], "title": "Adaptive Time-step Training for Enhancing Spike-Based Neural Radiance Fields", "comment": null, "summary": "Neural Radiance Fields (NeRF)-based models have achieved remarkable success\nin 3D reconstruction and rendering tasks. However, during both training and\ninference, these models rely heavily on dense point sampling along rays from\nmultiple viewpoints, resulting in a surge in floating-point operations and\nseverely limiting their use in resource-constrained scenarios like edge\ncomputing. Spiking Neural Networks (SNNs), which communicate via binary spikes\nover discrete time steps, offer a promising alternative due to their\nenergy-efficient nature. Given the inherent variability in scene scale and\ntexture complexity in neural rendering and the prevailing practice of training\nseparate models per scene, we propose a spike-based NeRF framework with a\ndynamic time step training strategy, termed Pretrain-Adaptive Time-step\nAdjustment (PATA). This approach automatically explores the trade-off between\nrendering quality and time step length during training. Consequently, it\nenables scene-adaptive inference with variable time steps and reduces the\nadditional consumption of computational resources in the inference process.\nAnchoring to the established Instant-NGP architecture, we evaluate our method\nacross diverse datasets. The experimental results show that PATA can preserve\nrendering fidelity while reducing inference time steps by 64\\% and running\npower by 61.55\\%.", "AI": {"tldr": "This paper proposes PATA, a spike-based NeRF framework with a dynamic time step training strategy, to reduce the computational cost of NeRF models. PATA achieves significant reductions in inference time steps and running power while preserving rendering fidelity.", "motivation": "NeRF-based models rely heavily on dense point sampling along rays from multiple viewpoints, resulting in a surge in floating-point operations and severely limiting their use in resource-constrained scenarios like edge computing. Spiking Neural Networks (SNNs), which communicate via binary spikes over discrete time steps, offer a promising alternative due to their energy-efficient nature. Given the inherent variability in scene scale and texture complexity in neural rendering and the prevailing practice of training separate models per scene", "method": "a spike-based NeRF framework with a dynamic time step training strategy, termed Pretrain-Adaptive Time-step Adjustment (PATA)", "result": "PATA can preserve rendering fidelity while reducing inference time steps by 64% and running power by 61.55%.", "conclusion": "PATA can preserve rendering fidelity while reducing inference time steps by 64% and running power by 61.55%."}}
{"id": "2507.22963", "categories": ["cs.LG", "q-bio.OT"], "pdf": "https://arxiv.org/pdf/2507.22963", "abs": "https://arxiv.org/abs/2507.22963", "authors": ["Abdelrhman Gaber", "Hassan Abd-Eltawab", "John Elgallab", "Youssif Abuzied", "Dineo Mpanya", "Turgay Celik", "Swarun Kumar", "Tamer ElBatt"], "title": "FedCVD++: Communication-Efficient Federated Learning for Cardiovascular Risk Prediction with Parametric and Non-Parametric Model Optimization", "comment": null, "summary": "Cardiovascular diseases (CVD) cause over 17 million deaths annually\nworldwide, highlighting the urgent need for privacy-preserving predictive\nsystems. We introduce FedCVD++, an enhanced federated learning (FL) framework\nthat integrates both parametric models (logistic regression, SVM, neural\nnetworks) and non-parametric models (Random Forest, XGBoost) for coronary heart\ndisease risk prediction. To address key FL challenges, we propose: (1)\ntree-subset sampling that reduces Random Forest communication overhead by 70%,\n(2) XGBoost-based feature extraction enabling lightweight federated ensembles,\nand (3) federated SMOTE synchronization for resolving cross-institutional class\nimbalance.\n  Evaluated on the Framingham dataset (4,238 records), FedCVD++ achieves\nstate-of-the-art results: federated XGBoost (F1 = 0.80) surpasses its\ncentralized counterpart (F1 = 0.78), and federated Random Forest (F1 = 0.81)\nmatches non-federated performance. Additionally, our communication-efficient\nstrategies reduce bandwidth consumption by 3.2X while preserving 95% accuracy.\n  Compared to existing FL frameworks, FedCVD++ delivers up to 15% higher\nF1-scores and superior scalability for multi-institutional deployment. This\nwork represents the first practical integration of non-parametric models into\nfederated healthcare systems, providing a privacy-preserving solution validated\nunder real-world clinical constraints.", "AI": {"tldr": "FedCVD++\u662f\u4e00\u79cd\u7528\u4e8e\u51a0\u5fc3\u75c5\u98ce\u9669\u9884\u6d4b\u7684\u589e\u5f3a\u578b\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u96c6\u6210\u4e86\u53c2\u6570\u548c\u975e\u53c2\u6570\u6a21\u578b\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u964d\u4f4e\u4e86\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u5168\u7403\u6bcf\u5e74\u6709\u8d85\u8fc7 1700 \u4e07\u4eba\u6b7b\u4e8e\u5fc3\u8840\u7ba1\u75be\u75c5 (CVD)\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u4fdd\u62a4\u9690\u79c1\u7684\u9884\u6d4b\u7cfb\u7edf\u3002", "method": "\u96c6\u6210\u4e86\u53c2\u6570\u6a21\u578b\uff08\u903b\u8f91\u56de\u5f52\u3001SVM\u3001\u795e\u7ecf\u7f51\u7edc\uff09\u548c\u975e\u53c2\u6570\u6a21\u578b\uff08\u968f\u673a\u68ee\u6797\u3001XGBoost\uff09\u7684\u8054\u90a6\u5b66\u4e60(FL)\u6846\u67b6FedCVD++\u3002", "result": "FedCVD++ \u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff1a\u8054\u90a6 XGBoost (F1 = 0.80) \u8d85\u8fc7\u4e86\u5176\u4e2d\u5fc3\u5316\u5bf9\u5e94\u6a21\u578b (F1 = 0.78)\uff0c\u8054\u90a6\u968f\u673a\u68ee\u6797 (F1 = 0.81) \u4e0e\u975e\u8054\u90a6\u6027\u80fd\u76f8\u5339\u914d\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u901a\u4fe1\u6548\u7387\u7b56\u7565\u5c06\u5e26\u5bbd\u6d88\u8017\u964d\u4f4e\u4e86 3.2 \u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86 95% \u7684\u51c6\u786e\u7387\u3002", "conclusion": "FedCVD++\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51a0\u5fc3\u75c5\u98ce\u9669\u9884\u6d4b\uff0c\u5e76\u5728\u591a\u673a\u6784\u90e8\u7f72\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.23330", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23330", "abs": "https://arxiv.org/abs/2507.23330", "authors": ["Tosin Adewumi", "Lama Alkhaled", "Florent Imbert", "Hui Han", "Nudrat Habib", "Karl L\u00f6wenmark"], "title": "AI Must not be Fully Autonomous", "comment": "11 pages, 1 figure", "summary": "Autonomous Artificial Intelligence (AI) has many benefits. It also has many\nrisks. In this work, we identify the 3 levels of autonomous AI. We are of the\nposition that AI must not be fully autonomous because of the many risks,\nespecially as artificial superintelligence (ASI) is speculated to be just\ndecades away. Fully autonomous AI, which can develop its own objectives, is at\nlevel 3 and without responsible human oversight. However, responsible human\noversight is crucial for mitigating the risks. To ague for our position, we\ndiscuss theories of autonomy, AI and agents. Then, we offer 12 distinct\narguments and 6 counterarguments with rebuttals to the counterarguments. We\nalso present 15 pieces of recent evidence of AI misaligned values and other\nrisks in the appendix.", "AI": {"tldr": "The paper argues against fully autonomous AI due to potential risks, advocating for responsible human oversight. It presents arguments, counterarguments, and evidence of AI misalignment.", "motivation": "Autonomous Artificial Intelligence (AI) has many benefits. It also has many risks. In this work, we identify the 3 levels of autonomous AI. We are of the position that AI must not be fully autonomous because of the many risks, especially as artificial superintelligence (ASI) is speculated to be just decades away.", "method": "discuss theories of autonomy, AI and agents. Then, offer 12 distinct arguments and 6 counterarguments with rebuttals to the counterarguments. We also present 15 pieces of recent evidence of AI misaligned values and other risks in the appendix.", "result": "N/A", "conclusion": "AI must not be fully autonomous because of the many risks, especially as artificial superintelligence (ASI) is speculated to be just decades away. Fully autonomous AI, which can develop its own objectives, is at level 3 and without responsible human oversight. However, responsible human oversight is crucial for mitigating the risks."}}
{"id": "2507.23459", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23459", "abs": "https://arxiv.org/abs/2507.23459", "authors": ["Fan Li", "Chang Meng", "Jiaqi Fu", "Shuchang Liu", "Jiashuo Zhang", "Tianke Zhang", "Xueliang Wang", "Xiaoqiang Feng"], "title": "KLAN: Kuaishou Landing-page Adaptive Navigator", "comment": "We propose PLPM, a new task for selecting optimal landing pages upon\n  user entry. Our solution, KLAN, models static and dynamic user interests and\n  is successfully deployed on Kuaishou, improving DAU and user lifetime", "summary": "Modern online platforms configure multiple pages to accommodate diverse user\nneeds. This multi-page architecture inherently establishes a two-stage\ninteraction paradigm between the user and the platform: (1) Stage I: page\nnavigation, navigating users to a specific page and (2) Stage II: in-page\ninteraction, where users engage with customized content within the specific\npage. While the majority of research has been focusing on the sequential\nrecommendation task that improves users' feedback in Stage II, there has been\nlittle investigation on how to achieve better page navigation in Stage I. To\nfill this gap, we formally define the task of Personalized Landing Page\nModeling (PLPM) into the field of recommender systems: Given a user upon app\nentry, the goal of PLPM is to proactively select the most suitable landing page\nfrom a set of candidates (e.g., functional tabs, content channels, or\naggregation pages) to optimize the short-term PDR metric and the long-term user\nengagement and satisfaction metrics, while adhering to industrial constraints.\nAdditionally, we propose KLAN (Kuaishou Landing-page Adaptive Navigator), a\nhierarchical solution framework designed to provide personalized landing pages\nunder the formulation of PLPM. KLAN comprises three key components: (1)\nKLAN-ISP captures inter-day static page preference; (2) KLAN-IIT captures\nintra-day dynamic interest transitions and (3) KLAN-AM adaptively integrates\nboth components for optimal navigation decisions. Extensive online experiments\nconducted on the Kuaishou platform demonstrate the effectiveness of KLAN,\nobtaining +0.205% and +0.192% improvements on in Daily Active Users (DAU) and\nuser Lifetime (LT). Our KLAN is ultimately deployed on the online platform at\nfull traffic, serving hundreds of millions of users. To promote further\nresearch in this important area, we will release our dataset and code upon\npaper acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e2a\u6027\u5316\u7740\u9646\u9875\u5efa\u6a21 (PLPM) \u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a KLAN \u7684\u5206\u5c42\u89e3\u51b3\u65b9\u6848\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u5feb\u624b\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u5728\u7ebf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e KLAN \u5728 DAU \u548c\u7528\u6237\u751f\u547d\u5468\u671f\u65b9\u9762\u6709\u6240\u6539\u8fdb\u3002", "motivation": "\u5927\u591a\u6570\u7814\u7a76\u90fd\u96c6\u4e2d\u5728\u6539\u8fdb\u7b2c\u4e8c\u9636\u6bb5\u7528\u6237\u53cd\u9988\u7684\u987a\u5e8f\u63a8\u8350\u4efb\u52a1\u4e0a\uff0c\u800c\u5bf9\u5982\u4f55\u5728\u7b2c\u4e00\u9636\u6bb5\u5b9e\u73b0\u66f4\u597d\u7684\u9875\u9762\u5bfc\u822a\u7684\u7814\u7a76\u5f88\u5c11\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u5c06\u4e2a\u6027\u5316\u7740\u9646\u9875\u5efa\u6a21 (PLPM) \u7684\u4efb\u52a1\u6b63\u5f0f\u5b9a\u4e49\u5230\u63a8\u8350\u7cfb\u7edf\u9886\u57df\u4e2d\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86 KLAN\uff08\u5feb\u624b\u7740\u9646\u9875\u81ea\u9002\u5e94\u5bfc\u822a\u5668\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u5206\u5c42\u89e3\u51b3\u65b9\u6848\u6846\u67b6\uff0c\u65e8\u5728\u5728 PLPM \u7684\u516c\u5f0f\u4e0b\u63d0\u4f9b\u4e2a\u6027\u5316\u7684\u7740\u9646\u9875\u3002KLAN \u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a(1) KLAN-ISP \u6355\u83b7\u65e5\u95f4\u9759\u6001\u9875\u9762\u504f\u597d\uff1b(2) KLAN-IIT \u6355\u83b7\u65e5\u5185\u52a8\u6001\u5174\u8da3\u8f6c\u79fb\uff1b(3) KLAN-AM \u81ea\u9002\u5e94\u5730\u96c6\u6210\u8fd9\u4e24\u4e2a\u7ec4\u4ef6\u4ee5\u5b9e\u73b0\u6700\u4f73\u5bfc\u822a\u51b3\u7b56\u3002", "result": "KLAN\u5728\u6bcf\u65e5\u6d3b\u8dc3\u7528\u6237 (DAU) \u548c\u7528\u6237\u751f\u547d\u5468\u671f (LT) \u65b9\u9762\u5206\u522b\u83b7\u5f97\u4e86 +0.205% \u548c +0.192% \u7684\u6539\u8fdb\u3002", "conclusion": "KLAN\u5728\u5feb\u624b\u5e73\u53f0\u4e0a\u7684\u5927\u91cf\u5728\u7ebf\u5b9e\u9a8c\u8868\u660e\u4e86\u5176\u6709\u6548\u6027\uff0c\u5728\u6bcf\u65e5\u6d3b\u8dc3\u7528\u6237 (DAU) \u548c\u7528\u6237\u751f\u547d\u5468\u671f (LT) \u65b9\u9762\u5206\u522b\u83b7\u5f97\u4e86 +0.205% \u548c +0.192% \u7684\u6539\u8fdb\u3002\u6211\u4eec\u7684 KLAN \u6700\u7ec8\u4ee5\u5168\u6d41\u91cf\u90e8\u7f72\u5728\u5728\u7ebf\u5e73\u53f0\u4e0a\uff0c\u4e3a\u6570\u4ebf\u7528\u6237\u63d0\u4f9b\u670d\u52a1\u3002\u4e3a\u4e86\u4fc3\u8fdb\u5bf9\u8fd9\u4e00\u91cd\u8981\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u6211\u4eec\u5c06\u5728\u8bba\u6587\u88ab\u63a5\u53d7\u540e\u53d1\u5e03\u6211\u4eec\u7684\u6570\u636e\u96c6\u548c\u4ee3\u7801\u3002"}}
{"id": "2507.22915", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22915", "abs": "https://arxiv.org/abs/2507.22915", "authors": ["Esmail Gumaan"], "title": "Theoretical Foundations and Mitigation of Hallucination in Large Language Models", "comment": "12 pages", "summary": "Hallucination in Large Language Models (LLMs) refers to the generation of\ncontent that is not faithful to the input or the real-world facts. This paper\nprovides a rigorous treatment of hallucination in LLMs, including formal\ndefinitions and theoretical analyses. We distinguish between intrinsic and\nextrinsic hallucinations, and define a \\textit{hallucination risk} for models.\nWe derive bounds on this risk using learning-theoretic frameworks (PAC-Bayes\nand Rademacher complexity). We then survey detection strategies for\nhallucinations, such as token-level uncertainty estimation, confidence\ncalibration, and attention alignment checks. On the mitigation side, we discuss\napproaches including retrieval-augmented generation, hallucination-aware\nfine-tuning, logit calibration, and the incorporation of fact-verification\nmodules. We propose a unified detection and mitigation workflow, illustrated\nwith a diagram, to integrate these strategies. Finally, we outline evaluation\nprotocols for hallucination, recommending datasets, metrics, and experimental\nsetups to quantify and reduce hallucinations. Our work lays a theoretical\nfoundation and practical guidelines for addressing the crucial challenge of\nhallucination in LLMs.", "AI": {"tldr": "\u672c\u6587\u6df1\u5165\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u5b9a\u4e49\u3001\u7406\u8bba\u5206\u6790\u3001\u68c0\u6d4b\u4e0e\u7f13\u89e3\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u4e86\u8bc4\u4f30\u65b9\u6848\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u5e7b\u89c9\u6307\u7684\u662f\u751f\u6210\u7684\u5185\u5bb9\u4e0d\u5fe0\u5b9e\u4e8e\u8f93\u5165\u6216\u771f\u5b9e\u4e16\u754c\u7684\u4e8b\u5b9e\u3002", "method": "\u8bba\u6587\u8fd0\u7528\u5b66\u4e60\u7406\u8bba\u6846\u67b6\uff08PAC-Bayes\u548cRademacher\u590d\u6742\u5ea6\uff09\u63a8\u5bfc\u4e86\u5e7b\u89c9\u98ce\u9669\u7684\u754c\u9650\uff0c\u5e76\u8c03\u7814\u4e86\u5e7b\u89c9\u68c0\u6d4b\u7b56\u7565\uff0c\u5982token-level\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3001\u7f6e\u4fe1\u5ea6\u6821\u51c6\u548c\u6ce8\u610f\u529b\u5bf9\u9f50\u68c0\u67e5\u3002\u5728\u7f13\u89e3\u65b9\u9762\uff0c\u8ba8\u8bba\u4e86\u5305\u62ec\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u5e7b\u89c9\u611f\u77e5\u5fae\u8c03\u3001logit\u6821\u51c6\u548c\u6574\u5408\u4e8b\u5b9e\u9a8c\u8bc1\u6a21\u5757\u7b49\u65b9\u6cd5\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u68c0\u6d4b\u548c\u7f13\u89e3\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5e76\u6982\u8ff0\u4e86\u5e7b\u89c9\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u8bba\u6587\u533a\u5206\u4e86\u5185\u5728\u548c\u5916\u5728\u5e7b\u89c9\uff0c\u5e76\u4e3a\u6a21\u578b\u5b9a\u4e49\u4e86\u5e7b\u89c9\u98ce\u9669\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u4e3a\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2507.23042", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.RO", "I.2.6; I.2.9; I.2.10; C.3.3"], "pdf": "https://arxiv.org/pdf/2507.23042", "abs": "https://arxiv.org/abs/2507.23042", "authors": ["Santosh Patapati", "Trisanth Srinivasan"], "title": "Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language Driving", "comment": "6 pages", "summary": "Autonomous vehicles must react in milliseconds while reasoning about road\ngeometry and traffic intent to navigate complex situations. We introduce\nNovaDrive, a single-branch vision-language architecture that processes\nfront-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a\nsingle branch. A lightweight, two-stage cross-attention block first aligns\nwaypoint tokens with the HD map, then refines attention over fine-grained image\nand depth patches. Coupled with a novel smoothness loss that discourages abrupt\nsteering and speed changes, this design eliminates the need for recurrent\nmemory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language\nbackbone, enabling real-time inference. On the nuScenes / Waymo subset of the\nMD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts\npath-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from\n2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations\nconfirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention\nfusion each contribute the most to these gains. Beyond safety, NovaDrive's\nshorter routes (resulting from the novel smoothness loss) translate to lower\nfuel or battery usage, pointing toward leaner, more easily updated driving\nstacks. NovaDrive can be extended to other embodied-AI domains as well.", "AI": {"tldr": "NovaDrive, a vision-language architecture, improves autonomous vehicle navigation with higher success rates, path efficiency, and lower collision frequency. It uses a novel cross-attention mechanism and smoothness loss, fine-tuning a LLaMA-3.2 vision-language backbone for real-time inference.", "motivation": "Autonomous vehicles must react in milliseconds while reasoning about road geometry and traffic intent to navigate complex situations.", "method": "a single-branch vision-language architecture that processes front-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a single branch. A lightweight, two-stage cross-attention block first aligns waypoint tokens with the HD map, then refines attention over fine-grained image and depth patches. Coupled with a novel smoothness loss that discourages abrupt steering and speed changes, this design eliminates the need for recurrent memory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language backbone, enabling real-time inference.", "result": "On the nuScenes / Waymo subset of the MD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts path-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from 2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art.", "conclusion": "NovaDrive raises success rate and boosts path-efficiency, and reduces collision frequency relative to the previous state-of-the-art. Beyond safety, NovaDrive's shorter routes translate to lower fuel or battery usage, pointing toward leaner, more easily updated driving stacks. NovaDrive can be extended to other embodied-AI domains as well."}}
{"id": "2507.23000", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23000", "abs": "https://arxiv.org/abs/2507.23000", "authors": ["Shengao Yi", "Xiaojiang Li", "Wei Tu", "Tianhong Zhao"], "title": "Planning for Cooler Cities: A Multimodal AI Framework for Predicting and Mitigating Urban Heat Stress through Urban Landscape Transformation", "comment": null, "summary": "As extreme heat events intensify due to climate change and urbanization,\ncities face increasing challenges in mitigating outdoor heat stress. While\ntraditional physical models such as SOLWEIG and ENVI-met provide detailed\nassessments of human-perceived heat exposure, their computational demands limit\nscalability for city-wide planning. In this study, we propose GSM-UTCI, a\nmultimodal deep learning framework designed to predict daytime average\nUniversal Thermal Climate Index (UTCI) at 1-meter hyperlocal resolution. The\nmodel fuses surface morphology (nDSM), high-resolution land cover data, and\nhourly meteorological conditions using a feature-wise linear modulation (FiLM)\narchitecture that dynamically conditions spatial features on atmospheric\ncontext. Trained on SOLWEIG-derived UTCI maps, GSM-UTCI achieves near-physical\naccuracy, with an R2 of 0.9151 and a mean absolute error (MAE) of 0.41{\\deg}C,\nwhile reducing inference time from hours to under five minutes for an entire\ncity. To demonstrate its planning relevance, we apply GSM-UTCI to simulate\nsystematic landscape transformation scenarios in Philadelphia, replacing bare\nearth, grass, and impervious surfaces with tree canopy. Results show spatially\nheterogeneous but consistently strong cooling effects, with impervious-to-tree\nconversion producing the highest aggregated benefit (-4.18{\\deg}C average\nchange in UTCI across 270.7 km2). Tract-level bivariate analysis further\nreveals strong alignment between thermal reduction potential and land cover\nproportions. These findings underscore the utility of GSM-UTCI as a scalable,\nfine-grained decision support tool for urban climate adaptation, enabling\nscenario-based evaluation of greening strategies across diverse urban\nenvironments.", "AI": {"tldr": "GSM-UTCI \u662f\u4e00\u79cd\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u53ef\u9884\u6d4b\u57ce\u5e02\u70ed\u5e94\u529b\uff0c\u901f\u5ea6\u66f4\u5feb\u3001\u7cbe\u5ea6\u66f4\u9ad8\u3002", "motivation": "\u7531\u4e8e\u6c14\u5019\u53d8\u5316\u548c\u57ce\u5e02\u5316\uff0c\u6781\u7aef\u9ad8\u6e29\u4e8b\u4ef6\u65e5\u76ca\u52a0\u5267\uff0c\u57ce\u5e02\u5728\u7f13\u89e3\u5ba4\u5916\u70ed\u5e94\u529b\u65b9\u9762\u9762\u4e34\u8d8a\u6765\u8d8a\u591a\u7684\u6311\u6218\u3002\u867d\u7136\u4f20\u7edf\u7684\u7269\u7406\u6a21\u578b\uff08\u5982 SOLWEIG \u548c ENVI-met\uff09\u53ef\u4ee5\u5bf9\u4eba\u4f53\u611f\u77e5\u5230\u7684\u70ed\u66b4\u9732\u8fdb\u884c\u8be6\u7ec6\u8bc4\u4f30\uff0c\u4f46\u5b83\u4eec\u7684\u8ba1\u7b97\u9700\u6c42\u9650\u5236\u4e86\u57ce\u5e02\u8303\u56f4\u89c4\u5212\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86 GSM-UTCI\uff0c\u8fd9\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u4ee5 1 \u7c73\u7684\u8d85\u5c40\u90e8\u5206\u8fa8\u7387\u9884\u6d4b\u767d\u5929\u5e73\u5747\u901a\u7528\u70ed\u6c14\u5019\u6307\u6570 (UTCI)\u3002", "result": "GSM-UTCI \u5b9e\u73b0\u4e86\u63a5\u8fd1\u7269\u7406\u7684\u7cbe\u5ea6\uff0cR2 \u4e3a 0.9151\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee (MAE) \u4e3a 0.41{\\deg}C\uff0c\u540c\u65f6\u5c06\u6574\u4e2a\u57ce\u5e02\u7684\u63a8\u7406\u65f6\u95f4\u4ece\u6570\u5c0f\u65f6\u7f29\u77ed\u5230\u4e94\u5206\u949f\u4ee5\u5185\u3002\u4e0d\u900f\u6c34\u8868\u9762\u5230\u6811\u6728\u7684\u8f6c\u6362\u4ea7\u751f\u4e86\u6700\u9ad8\u7684\u7efc\u5408\u6548\u76ca\uff08270.7 \u5e73\u65b9\u516c\u91cc\u8303\u56f4\u5185 UTCI \u7684\u5e73\u5747\u53d8\u5316\u4e3a -4.18{\\deg}C\uff09\u3002", "conclusion": "GSM-UTCI \u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u3001\u7ec6\u7c92\u5ea6\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u57ce\u5e02\u6c14\u5019\u9002\u5e94\uff0c\u80fd\u591f\u5bf9\u4e0d\u540c\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u7eff\u5316\u7b56\u7565\u8fdb\u884c\u57fa\u4e8e\u60c5\u666f\u7684\u8bc4\u4f30\u3002"}}
{"id": "2507.23336", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.23336", "abs": "https://arxiv.org/abs/2507.23336", "authors": ["Ram Mohan Rao Kadiyala", "Siddhant Gupta", "Jebish Purbey", "Giulio Martini", "Suman Debnath", "Hamza Farooq"], "title": "DSBC : Data Science task Benchmarking with Context engineering", "comment": "32 pages", "summary": "Recent advances in large language models (LLMs) have significantly impacted\ndata science workflows, giving rise to specialized data science agents designed\nto automate analytical tasks. Despite rapid adoption, systematic benchmarks\nevaluating the efficacy and limitations of these agents remain scarce. In this\npaper, we introduce a comprehensive benchmark specifically crafted to reflect\nreal-world user interactions with data science agents by observing usage of our\ncommercial applications. We evaluate three LLMs: Claude-4.0-Sonnet,\nGemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with\ncontext engineering, multi-step with context engineering, and with SmolAgent.\nOur benchmark assesses performance across a diverse set of eight data science\ntask categories, additionally exploring the sensitivity of models to common\nprompting issues, such as data leakage and slightly ambiguous instructions. We\nfurther investigate the influence of temperature parameters on overall and\ntask-specific outcomes for each model and approach. Our findings reveal\ndistinct performance disparities among the evaluated models and methodologies,\nhighlighting critical factors that affect practical deployment. The benchmark\ndataset and evaluation framework introduced herein aim to provide a foundation\nfor future research of more robust and effective data science agents.", "AI": {"tldr": "This paper introduces a new benchmark for evaluating data science agents, compares the performance of three LLMs across different approaches, and analyzes the impact of prompting issues and temperature parameters.", "motivation": "Despite rapid adoption, systematic benchmarks evaluating the efficacy and limitations of these agents remain scarce. In this paper, we introduce a comprehensive benchmark specifically crafted to reflect real-world user interactions with data science agents by observing usage of our commercial applications.", "method": "We evaluate three LLMs: Claude-4.0-Sonnet, Gemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with context engineering, multi-step with context engineering, and with SmolAgent. Our benchmark assesses performance across a diverse set of eight data science task categories, additionally exploring the sensitivity of models to common prompting issues, such as data leakage and slightly ambiguous instructions. We further investigate the influence of temperature parameters on overall and task-specific outcomes for each model and approach.", "result": "Our findings reveal distinct performance disparities among the evaluated models and methodologies, highlighting critical factors that affect practical deployment.", "conclusion": "The benchmark dataset and evaluation framework introduced herein aim to provide a foundation for future research of more robust and effective data science agents."}}
{"id": "2507.22917", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.22917", "abs": "https://arxiv.org/abs/2507.22917", "authors": ["Kwun Hang Lau", "Ruiyuan Zhang", "Weijie Shi", "Xiaofang Zhou", "Xiaojun Cheng"], "title": "Reading Between the Timelines: RAG for Answering Diachronic Questions", "comment": null, "summary": "While Retrieval-Augmented Generation (RAG) excels at injecting static,\nfactual knowledge into Large Language Models (LLMs), it exhibits a critical\ndeficit in handling longitudinal queries that require tracking entities and\nphenomena across time. This blind spot arises because conventional,\nsemantically-driven retrieval methods are not equipped to gather evidence that\nis both topically relevant and temporally coherent for a specified duration. We\naddress this challenge by proposing a new framework that fundamentally\nredesigns the RAG pipeline to infuse temporal logic. Our methodology begins by\ndisentangling a user's query into its core subject and its temporal window. It\nthen employs a specialized retriever that calibrates semantic matching against\ntemporal relevance, ensuring the collection of a contiguous evidence set that\nspans the entire queried period. To enable rigorous evaluation of this\ncapability, we also introduce the Analytical Diachronic Question Answering\nBenchmark (ADQAB), a challenging evaluation suite grounded in a hybrid corpus\nof real and synthetic financial news. Empirical results on ADQAB show that our\napproach yields substantial gains in answer accuracy, surpassing standard RAG\nimplementations by 13% to 27%. This work provides a validated pathway toward\nRAG systems capable of performing the nuanced, evolutionary analysis required\nfor complex, real-world questions. The dataset and code for this study are\npublicly available at https://github.com/kwunhang/TA-RAG.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65f6\u95f4\u611f\u77e5RAG\u6846\u67b6\uff0c\u5e76\u5728ADQAB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u89e3\u51b3\u4e86RAG\u5728\u5904\u7406\u7eb5\u5411\u67e5\u8be2\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u7684\u3001\u8bed\u4e49\u9a71\u52a8\u7684\u68c0\u7d22\u65b9\u6cd5\u6ca1\u6709\u80fd\u529b\u4e3a\u6307\u5b9a\u7684\u65f6\u95f4\u6bb5\u6536\u96c6\u5728\u4e3b\u9898\u76f8\u5173\u548c\u65f6\u95f4\u4e0a\u8fde\u8d2f\u7684\u8bc1\u636e\uff0c\u5bfc\u81f4RAG\u5728\u5904\u7406\u7eb5\u5411\u67e5\u8be2\u65f6\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u5c06\u7528\u6237\u7684\u67e5\u8be2\u5206\u89e3\u4e3a\u6838\u5fc3\u4e3b\u9898\u548c\u65f6\u95f4\u7a97\u53e3\uff0c\u7136\u540e\u4f7f\u7528\u4e13\u95e8\u7684\u68c0\u7d22\u5668\u6765\u6821\u51c6\u8bed\u4e49\u5339\u914d\u4e0e\u65f6\u95f4\u76f8\u5173\u6027\uff0c\u786e\u4fdd\u6536\u96c6\u5230\u8de8\u8d8a\u6574\u4e2a\u67e5\u8be2\u671f\u95f4\u7684\u8fde\u7eed\u8bc1\u636e\u96c6\u3002", "result": "\u5728ADQAB\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7b54\u6848\u51c6\u786e\u6027\u65b9\u9762\u6bd4\u6807\u51c6RAG\u63d0\u9ad8\u4e8613%\u523027%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684RAG\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u5165\u65f6\u95f4\u903b\u8f91\u6765\u89e3\u51b3RAG\u5728\u5904\u7406\u9700\u8981\u8de8\u65f6\u95f4\u8ddf\u8e2a\u5b9e\u4f53\u548c\u73b0\u8c61\u7684\u7eb5\u5411\u67e5\u8be2\u65b9\u9762\u7684\u7f3a\u9677\u3002\u5728ADQAB\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7b54\u6848\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u6807\u51c6RAG\u5b9e\u73b0\u3002"}}
{"id": "2507.23058", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23058", "abs": "https://arxiv.org/abs/2507.23058", "authors": ["Alexandru Buburuzan"], "title": "Reference-Guided Diffusion Inpainting For Multimodal Counterfactual Generation", "comment": "A dissertation submitted to The University of Manchester for the\n  degree of Bachelor of Science in Artificial Intelligence", "summary": "Safety-critical applications, such as autonomous driving and medical image\nanalysis, require extensive multimodal data for rigorous testing. Synthetic\ndata methods are gaining prominence due to the cost and complexity of gathering\nreal-world data, but they demand a high degree of realism and controllability\nto be useful. This work introduces two novel methods for synthetic data\ngeneration in autonomous driving and medical image analysis, namely MObI and\nAnydoorMed, respectively. MObI is a first-of-its-kind framework for Multimodal\nObject Inpainting that leverages a diffusion model to produce realistic and\ncontrollable object inpaintings across perceptual modalities, demonstrated\nsimultaneously for camera and lidar. Given a single reference RGB image, MObI\nenables seamless object insertion into existing multimodal scenes at a\nspecified 3D location, guided by a bounding box, while maintaining semantic\nconsistency and multimodal coherence. Unlike traditional inpainting methods\nthat rely solely on edit masks, this approach uses 3D bounding box conditioning\nto ensure accurate spatial positioning and realistic scaling. AnydoorMed\nextends this paradigm to the medical imaging domain, focusing on\nreference-guided inpainting for mammography scans. It leverages a\ndiffusion-based model to inpaint anomalies with impressive detail preservation,\nmaintaining the reference anomaly's structural integrity while semantically\nblending it with the surrounding tissue. Together, these methods demonstrate\nthat foundation models for reference-guided inpainting in natural images can be\nreadily adapted to diverse perceptual modalities, paving the way for the next\ngeneration of systems capable of constructing highly realistic, controllable\nand multimodal counterfactual scenarios.", "AI": {"tldr": "Introduces MObI and AnydoorMed, two novel methods for synthetic data generation in autonomous driving and medical image analysis, respectively, using diffusion models for realistic and controllable inpainting.", "motivation": "Safety-critical applications require extensive multimodal data for rigorous testing, but real-world data is costly and complex to gather. Synthetic data methods are promising but need high realism and controllability.", "method": "two novel methods for synthetic data generation: MObI (Multimodal Object Inpainting) and AnydoorMed (reference-guided inpainting for mammography scans). MObI uses a diffusion model for realistic object insertion in autonomous driving scenes, guided by 3D bounding boxes. AnydoorMed extends this to medical imaging, using a diffusion-based model to inpaint anomalies in mammography scans with detail preservation.", "result": "MObI enables seamless object insertion into existing multimodal scenes at a specified 3D location, guided by a bounding box, while maintaining semantic consistency and multimodal coherence. AnydoorMed demonstrates impressive detail preservation, maintaining the reference anomaly's structural integrity while semantically blending it with the surrounding tissue.", "conclusion": "foundation models for reference-guided inpainting in natural images can be readily adapted to diverse perceptual modalities, paving the way for the next generation of systems capable of constructing highly realistic, controllable and multimodal counterfactual scenarios"}}
{"id": "2507.23009", "categories": ["cs.LG", "cs.AI", "91E45", "I.2"], "pdf": "https://arxiv.org/pdf/2507.23009", "abs": "https://arxiv.org/abs/2507.23009", "authors": ["Tom S\u00fchr", "Florian E. Dorner", "Olawale Salaudeen", "Augustin Kelava", "Samira Samadi"], "title": "Stop Evaluating AI with Human Tests, Develop Principled, AI-specific Tests instead", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable results on a range of\nstandardized tests originally designed to assess human cognitive and\npsychological traits, such as intelligence and personality. While these results\nare often interpreted as strong evidence of human-like characteristics in LLMs,\nthis paper argues that such interpretations constitute an ontological error.\nHuman psychological and educational tests are theory-driven measurement\ninstruments, calibrated to a specific human population. Applying these tests to\nnon-human subjects without empirical validation, risks mischaracterizing what\nis being measured. Furthermore, a growing trend frames AI performance on\nbenchmarks as measurements of traits such as ``intelligence'', despite known\nissues with validity, data contamination, cultural bias and sensitivity to\nsuperficial prompt changes. We argue that interpreting benchmark performance as\nmeasurements of human-like traits, lacks sufficient theoretical and empirical\njustification. This leads to our position: Stop Evaluating AI with Human Tests,\nDevelop Principled, AI-specific Tests instead. We call for the development of\nprincipled, AI-specific evaluation frameworks tailored to AI systems. Such\nframeworks might build on existing frameworks for constructing and validating\npsychometrics tests, or could be created entirely from scratch to fit the\nunique context of AI.", "AI": {"tldr": "\u76ee\u524d\u4f7f\u7528\u4eba\u7c7b\u6d4b\u8bd5\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\u5b58\u5728\u672c\u4f53\u8bba\u9519\u8bef\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bc4\u4f30\u4eba\u7c7b\u8ba4\u77e5\u548c\u5fc3\u7406\u7279\u5f81\u7684\u6807\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u4f46\u8fd9\u4e9b\u7ed3\u679c\u901a\u5e38\u88ab\u8bef\u89e3\u4e3a\u4eba\u5de5\u667a\u80fd\u5177\u6709\u7c7b\u4f3c\u4eba\u7c7b\u7684\u7279\u5f81\u3002", "method": "\u901a\u8fc7\u8bba\u8bc1\u548c\u5206\u6790\uff0c\u5f3a\u8c03\u4e86\u5f53\u524d\u4f7f\u7528\u4eba\u7c7b\u5fc3\u7406\u548c\u6559\u80b2\u6d4b\u8bd5\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\u7684\u5c40\u9650\u6027\u3002", "result": "\u8bba\u8bc1\u4e86\u5c06\u57fa\u51c6\u6d4b\u8bd5\u6027\u80fd\u89e3\u91ca\u4e3a\u8861\u91cf\u7c7b\u4f3c\u4eba\u7c7b\u7684\u7279\u5f81\u7f3a\u4e4f\u8db3\u591f\u7684\u7406\u8bba\u548c\u5b9e\u8bc1\u4f9d\u636e\u3002", "conclusion": "\u505c\u6b62\u4f7f\u7528\u4eba\u7c7b\u6d4b\u8bd5\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\uff0c\u800c\u662f\u5f00\u53d1\u6709\u539f\u5219\u7684\u3001\u7279\u5b9a\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u6d4b\u8bd5\u3002"}}
{"id": "2507.23377", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23377", "abs": "https://arxiv.org/abs/2507.23377", "authors": ["Zhuo Li", "Xianghuai Deng", "Chiwei Feng", "Hanmeng Li", "Shenjie Wang", "Haichao Zhang", "Teng Jia", "Conlin Chen", "Louis Linchun Wu", "Jia Wang"], "title": "LLM4Rail: An LLM-Augmented Railway Service Consulting Platform", "comment": null, "summary": "Large language models (LLMs) have significantly reshaped different walks of\nbusiness. To meet the increasing demands for individualized railway service, we\ndevelop LLM4Rail - a novel LLM-augmented railway service consulting platform.\nEmpowered by LLM, LLM4Rail can provide custom modules for ticketing, railway\nfood & drink recommendations, weather information, and chitchat. In LLM4Rail,\nwe propose the iterative \"Question-Thought-Action-Observation (QTAO)\" prompting\nframework. It meticulously integrates verbal reasoning with task-oriented\nactions, that is, reasoning to guide action selection, to effectively retrieve\nexternal observations relevant to railway operation and service to generate\naccurate responses. To provide personalized onboard dining services, we first\nconstruct the Chinese Railway Food and Drink (CRFD-25) - a publicly accessible\ntakeout dataset tailored for railway services. CRFD-25 covers a wide range of\nsignature dishes categorized by cities, cuisines, age groups, and spiciness\nlevels. We further introduce an LLM-based zero-shot conversational recommender\nfor railway catering. To address the unconstrained nature of open\nrecommendations, the feature similarity-based post-processing step is\nintroduced to ensure all the recommended items are aligned with CRFD-25\ndataset.", "AI": {"tldr": "LLM4Rail \u662f\u4e00\u4e2a\u65b0\u578b\u7684 LLM \u589e\u5f3a\u578b\u94c1\u8def\u670d\u52a1\u54a8\u8be2\u5e73\u53f0\uff0c\u5b83\u53ef\u4ee5\u63d0\u4f9b\u5b9a\u5236\u7684\u94c1\u8def\u670d\u52a1\uff0c\u4f8b\u5982\u7968\u52a1\u548c\u9910\u996e\u63a8\u8350\u3002", "motivation": "\u4e3a\u4e86\u6ee1\u8db3\u5bf9\u4e2a\u6027\u5316\u94c1\u8def\u670d\u52a1\u4e0d\u65ad\u589e\u957f\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u8fed\u4ee3\u7684\u201c\u95ee\u9898-\u601d\u8003-\u884c\u52a8-\u89c2\u5bdf (QTAO)\u201d\u63d0\u793a\u6846\u67b6\uff0c\u5e76\u5c06\u53e3\u5934\u63a8\u7406\u4e0e\u9762\u5411\u4efb\u52a1\u7684\u884c\u52a8\u76f8\u7ed3\u5408\uff0c\u4ee5\u6709\u6548\u5730\u68c0\u7d22\u4e0e\u94c1\u8def\u8fd0\u8425\u548c\u670d\u52a1\u76f8\u5173\u7684\u5916\u90e8\u89c2\u5bdf\u7ed3\u679c\uff0c\u4ece\u800c\u751f\u6210\u51c6\u786e\u7684\u54cd\u5e94\u3002\u8fd8\u6784\u5efa\u4e86\u4e2d\u56fd\u94c1\u8def\u98df\u54c1\u996e\u6599 (CRFD-25) \u6570\u636e\u96c6\uff0c\u5e76\u5f15\u5165\u4e86\u57fa\u4e8e LLM \u7684\u96f6\u6837\u672c\u4f1a\u8bdd\u63a8\u8350\u5668\u3002", "result": "LLM4Rail \u5e73\u53f0\u80fd\u591f\u63d0\u4f9b\u4e2a\u6027\u5316\u7684\u94c1\u8def\u9910\u996e\u670d\u52a1\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u7279\u5f81\u76f8\u4f3c\u6027\u7684\u540e\u5904\u7406\u6b65\u9aa4\uff0c\u786e\u4fdd\u6240\u6709\u63a8\u8350\u9879\u76ee\u4e0e CRFD-25 \u6570\u636e\u96c6\u5bf9\u9f50\u3002", "conclusion": "LLM4Rail \u53ef\u4ee5\u4e3a\u94c1\u8def\u670d\u52a1\u63d0\u4f9b\u5b9a\u5236\u6a21\u5757\uff0c\u4f8b\u5982\u7968\u52a1\u3001\u9910\u996e\u63a8\u8350\u3001\u5929\u6c14\u4fe1\u606f\u548c\u95f2\u804a\u3002"}}
{"id": "2507.22921", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.22921", "abs": "https://arxiv.org/abs/2507.22921", "authors": ["Lee Harris"], "title": "Fast and Accurate Contextual Knowledge Extraction Using Cascading Language Model Chains and Candidate Answers", "comment": null, "summary": "Language models can capture complex relationships in given text, but these\nare notorious for being costly and for producing information that does not\nexist (i.e., hallucinations). Furthermore, the resources invested into\nproducing this information would be wasted if it were incorrect. We address\nthese issues by proposing, implementing, and applying the Language Model Chain\n(LMC) algorithm. In this, a language model's response to a given prompt about\ngiven text is only correct if it exists in the collection of possible (i.e.,\ncandidate) answers, and text corresponding to incorrect responses is fed into a\nmore predictive (but slower) language model. This process is repeated for a\ncollection of language models, or until all predictions about the text are\ncorrect. We used the LMC algorithm to extract patient dates of birth from\nmedical documents, and combining a collection of language models in a\nmulti-stage cascade significantly increased prediction speed and accuracy over\nindividual language models, while greatly reducing the number of corresponding\nhallucinations. We believe that the novel LMC algorithm significantly\ncontributes to the knowledge extraction field, and that this should be explored\nmuch further in the future.", "AI": {"tldr": "Proposing, implementing, and applying the Language Model Chain (LMC) algorithm to extract patient dates of birth from medical documents.", "motivation": "Language models are costly and produce hallucinations.", "method": "Language Model Chain (LMC) algorithm", "result": "combining a collection of language models in a multi-stage cascade significantly increased prediction speed and accuracy over individual language models, while greatly reducing the number of corresponding hallucinations.", "conclusion": "The novel LMC algorithm significantly contributes to the knowledge extraction field, and that this should be explored much further in the future."}}
{"id": "2507.22918", "categories": ["cs.CL", "cs.LG", "68T50", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.22918", "abs": "https://arxiv.org/abs/2507.22918", "authors": ["Daniel Son", "Sanjana Rathore", "Andrew Rufail", "Adrian Simon", "Daniel Zhang", "Soham Dave", "Cole Blondin", "Kevin Zhu", "Sean O'Brien"], "title": "Semantic Convergence: Investigating Shared Representations Across Scaled LLMs", "comment": "Submitted to ACL 2025 Student Research Workshop (poster)", "summary": "We investigate feature universality in Gemma-2 language models (Gemma-2-2B\nand Gemma-2-9B), asking whether models with a four-fold difference in scale\nstill converge on comparable internal concepts. Using the Sparse Autoencoder\n(SAE) dictionary-learning pipeline, we utilize SAEs on each model's\nresidual-stream activations, align the resulting monosemantic features via\nactivation correlation, and compare the matched feature spaces with SVCCA and\nRSA. Middle layers yield the strongest overlap, while early and late layers\nshow far less similarity. Preliminary experiments extend the analysis from\nsingle tokens to multi-token subspaces, showing that semantically similar\nsubspaces interact similarly with language models. These results strengthen the\ncase that large language models carve the world into broadly similar,\ninterpretable features despite size differences, reinforcing universality as a\nfoundation for cross-model interpretability.", "AI": {"tldr": "Investigating feature universality in Gemma-2 language models.", "motivation": "We investigate feature universality in Gemma-2 language models (Gemma-2-2B and Gemma-2-9B), asking whether models with a four-fold difference in scale still converge on comparable internal concepts.", "method": "Using the Sparse Autoencoder (SAE) dictionary-learning pipeline, we utilize SAEs on each model's residual-stream activations, align the resulting monosemantic features via activation correlation, and compare the matched feature spaces with SVCCA and RSA.", "result": "Middle layers yield the strongest overlap, while early and late layers show far less similarity. Preliminary experiments extend the analysis from single tokens to multi-token subspaces, showing that semantically similar subspaces interact similarly with language models.", "conclusion": "These results strengthen the case that large language models carve the world into broadly similar, interpretable features despite size differences, reinforcing universality as a foundation for cross-model interpretability."}}
{"id": "2507.23064", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO", "I.4.8; I.2.10; I.2.6; C.3.3; I.4.9"], "pdf": "https://arxiv.org/pdf/2507.23064", "abs": "https://arxiv.org/abs/2507.23064", "authors": ["Santosh Patapati", "Trisanth Srinivasan", "Murari Ambati"], "title": "Vision-Language Fusion for Real-Time Autonomous Driving: Goal-Centered Cross-Attention of Camera, HD-Map, & Waypoints", "comment": "5 pages", "summary": "Autonomous cars need geometric accuracy and semantic understanding to\nnavigate complex environments, yet most stacks handle them separately. We\npresent XYZ-Drive, a single vision-language model that reads a front-camera\nframe, a 25m $\\times$ 25m overhead map, and the next waypoint, then outputs\nsteering and speed. A lightweight goal-centered cross-attention layer lets\nwaypoint tokens highlight relevant image and map patches, supporting both\naction and textual explanations, before the fused tokens enter a partially\nfine-tuned LLaMA-3.2 11B model.\n  On the MD-NEX Outdoor-Driving benchmark XYZ-Drive attains 95% success and\n0.80 Success weighted by Path Length (SPL), surpassing PhysNav-DG by 15%. and\nhalving collisions, all while significantly improving efficiency by using only\na single branch. Sixteen ablations explain the gains. Removing any modality\n(vision, waypoint, map) drops success by up to 11%, confirming their\ncomplementary roles and rich connections. Replacing goal-centered attention\nwith simple concatenation cuts 3% in performance, showing query-based fusion\ninjects map knowledge more effectively. Keeping the transformer frozen loses\n5%, showing the importance of fine-tuning when applying VLMs for specific tasks\nsuch as autonomous driving. Coarsening map resolution from 10 cm to 40 cm blurs\nlane edges and raises crash rate.\n  Overall, these results demonstrate that early, token-level fusion of intent\nand map layout enables accurate, transparent, real-time driving.", "AI": {"tldr": "XYZ-Drive is a vision-language model for autonomous driving that uses a single branch to achieve high accuracy and efficiency by fusing intent and map layout at the token level.", "motivation": "Autonomous cars need geometric accuracy and semantic understanding to navigate complex environments, yet most stacks handle them separately", "method": "a single vision-language model that reads a front-camera frame, a 25m $\\times$ 25m overhead map, and the next waypoint, then outputs steering and speed. A lightweight goal-centered cross-attention layer lets waypoint tokens highlight relevant image and map patches, supporting both action and textual explanations, before the fused tokens enter a partially fine-tuned LLaMA-3.2 11B model", "result": "XYZ-Drive attains 95% success and 0.80 Success weighted by Path Length (SPL), surpassing PhysNav-DG by 15%. and halving collisions", "conclusion": "token-level fusion of intent and map layout enables accurate, transparent, real-time driving"}}
{"id": "2507.23010", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.23010", "abs": "https://arxiv.org/abs/2507.23010", "authors": ["Siwoo Park"], "title": "Investigating the Invertibility of Multimodal Latent Spaces: Limitations of Optimization-Based Methods", "comment": null, "summary": "This paper investigates the inverse capabilities and broader utility of\nmultimodal latent spaces within task-specific AI (Artificial Intelligence)\nmodels. While these models excel at their designed forward tasks (e.g.,\ntext-to-image generation, audio-to-text transcription), their potential for\ninverse mappings remains largely unexplored. We propose an optimization-based\nframework to infer input characteristics from desired outputs, applying it\nbidirectionally across Text-Image (BLIP, Flux.1-dev) and Text-Audio\n(Whisper-Large-V3, Chatterbox-TTS) modalities.\n  Our central hypothesis posits that while optimization can guide models\ntowards inverse tasks, their multimodal latent spaces will not consistently\nsupport semantically meaningful and perceptually coherent inverse mappings.\nExperimental results consistently validate this hypothesis. We demonstrate that\nwhile optimization can force models to produce outputs that align textually\nwith targets (e.g., a text-to-image model generating an image that an image\ncaptioning model describes correctly, or an ASR model transcribing optimized\naudio accurately), the perceptual quality of these inversions is chaotic and\nincoherent. Furthermore, when attempting to infer the original semantic input\nfrom generative models, the reconstructed latent space embeddings frequently\nlack semantic interpretability, aligning with nonsensical vocabulary tokens.\n  These findings highlight a critical limitation. multimodal latent spaces,\nprimarily optimized for specific forward tasks, do not inherently possess the\nstructure required for robust and interpretable inverse mappings. Our work\nunderscores the need for further research into developing truly semantically\nrich and invertible multimodal latent spaces.", "AI": {"tldr": "\u591a\u6a21\u6001\u6f5c\u5728\u7a7a\u95f4\u867d\u7136\u64c5\u957f\u524d\u5411\u4efb\u52a1\uff0c\u4f46\u5728\u53cd\u5411\u6620\u5c04\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "motivation": "\u7814\u7a76\u4efb\u52a1\u7279\u5b9a AI \u6a21\u578b\u4e2d\u591a\u6a21\u6001\u6f5c\u5728\u7a7a\u95f4\u7684\u53cd\u5411\u80fd\u529b\u548c\u66f4\u5e7f\u6cdb\u7684\u6548\u7528\u3002\u867d\u7136\u8fd9\u4e9b\u6a21\u578b\u64c5\u957f\u5176\u8bbe\u8ba1\u7684\u524d\u5411\u4efb\u52a1\uff08\u4f8b\u5982\uff0c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0c\u97f3\u9891\u5230\u6587\u672c\u8f6c\u5f55\uff09\uff0c\u4f46\u5b83\u4eec\u5728\u9006\u5411\u6620\u5c04\u65b9\u9762\u7684\u6f5c\u529b\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u671f\u671b\u7684\u8f93\u51fa\u4e2d\u63a8\u65ad\u8f93\u5165\u7279\u5f81\uff0c\u5e76\u5c06\u5176\u53cc\u5411\u5e94\u7528\u4e8e\u6587\u672c-\u56fe\u50cf\u548c\u6587\u672c-\u97f3\u9891\u6a21\u6001\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u4f18\u5316\u53ef\u4ee5\u8feb\u4f7f\u6a21\u578b\u4ea7\u751f\u5728\u6587\u672c\u4e0a\u4e0e\u76ee\u6807\u5bf9\u9f50\u7684\u8f93\u51fa\uff0c\u4f46\u8fd9\u4e9b\u53cd\u6f14\u7684\u611f\u77e5\u8d28\u91cf\u662f\u6df7\u4e71\u548c\u4e0d\u8fde\u8d2f\u7684\u3002\u6b64\u5916\uff0c\u5f53\u5c1d\u8bd5\u4ece\u751f\u6210\u6a21\u578b\u63a8\u65ad\u539f\u59cb\u8bed\u4e49\u8f93\u5165\u65f6\uff0c\u91cd\u6784\u7684\u6f5c\u5728\u7a7a\u95f4\u5d4c\u5165\u901a\u5e38\u7f3a\u4e4f\u8bed\u4e49\u53ef\u89e3\u91ca\u6027\uff0c\u4e0e\u65e0\u610f\u4e49\u7684\u8bcd\u6c47\u6807\u8bb0\u5bf9\u9f50\u3002", "conclusion": "\u591a\u6a21\u6001\u6f5c\u5728\u7a7a\u95f4\u4e3b\u8981\u9488\u5bf9\u7279\u5b9a\u524d\u5411\u4efb\u52a1\u8fdb\u884c\u4f18\u5316\uff0c\u4e0d\u5177\u5907\u7a33\u5065\u548c\u53ef\u89e3\u91ca\u9006\u6620\u5c04\u6240\u9700\u7684\u7ed3\u6784\u3002\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5f00\u53d1\u771f\u6b63\u5177\u6709\u8bed\u4e49\u4e30\u5bcc\u6027\u548c\u53ef\u9006\u6027\u7684\u591a\u6a21\u6001\u6f5c\u5728\u7a7a\u95f4\u3002"}}
{"id": "2507.22926", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.22926", "abs": "https://arxiv.org/abs/2507.22926", "authors": ["Nilesh", "Atul Gupta", "Avinash C Panday"], "title": "Multi-Relation Extraction in Entity Pairs using Global Context", "comment": "11 pages, 9 figures", "summary": "In document-level relation extraction, entities may appear multiple times in\na document, and their relationships can shift from one context to another.\nAccurate prediction of the relationship between two entities across an entire\ndocument requires building a global context spanning all relevant sentences.\nPrevious approaches have focused only on the sentences where entities are\nmentioned, which fails to capture the complete document context necessary for\naccurate relation extraction. Therefore, this paper introduces a novel input\nembedding approach to capture the positions of mentioned entities throughout\nthe document rather than focusing solely on the span where they appear. The\nproposed input encoding approach leverages global relationships and\nmulti-sentence reasoning by representing entities as standalone segments,\nindependent of their positions within the document. The performance of the\nproposed method has been tested on three benchmark relation extraction\ndatasets, namely DocRED, Re-DocRED, and REBEL. The experimental results\ndemonstrated that the proposed method accurately predicts relationships between\nentities in a document-level setting. The proposed research also has\ntheoretical and practical implications. Theoretically, it advances global\ncontext modeling and multi-sentence reasoning in document-level relation\nextraction. Practically, it enhances relationship detection, enabling improved\nperformance in real-world NLP applications requiring comprehensive entity-level\ninsights and interpretability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8f93\u5165\u5d4c\u5165\u65b9\u6cd5\uff0c\u4ee5\u6355\u6349\u6574\u4e2a\u6587\u6863\u4e2d\u63d0\u5230\u7684\u5b9e\u4f53\u7684\u4f4d\u7f6e\uff0c\u800c\u4e0d\u662f\u53ea\u5173\u6ce8\u5b83\u4eec\u51fa\u73b0\u7684\u8303\u56f4\u3002", "motivation": "\u4ee5\u524d\u7684\u65b9\u6cd5\u53ea\u5173\u6ce8\u63d0\u5230\u5b9e\u4f53\u7684\u53e5\u5b50\uff0c\u8fd9\u4e0d\u80fd\u6355\u6349\u5230\u51c6\u786e\u5173\u7cfb\u63d0\u53d6\u6240\u9700\u7684\u5b8c\u6574\u6587\u6863\u4e0a\u4e0b\u6587\u3002", "method": "\u901a\u8fc7\u5c06\u5b9e\u4f53\u8868\u793a\u4e3a\u72ec\u7acb\u7684\u6bb5\uff0c\u72ec\u7acb\u4e8e\u5b83\u4eec\u5728\u6587\u6863\u4e2d\u7684\u4f4d\u7f6e\uff0c\u4ece\u800c\u5229\u7528\u5168\u5c40\u5173\u7cfb\u548c\u591a\u53e5\u63a8\u7406\u3002", "result": "\u8be5\u65b9\u6cd5\u5728 DocRED\u3001Re-DocRED \u548c REBEL \u4e09\u4e2a\u57fa\u51c6\u5173\u7cfb\u62bd\u53d6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u6587\u6863\u7ea7\u522b\u8bbe\u7f6e\u4e2d\u5b9e\u4f53\u95f4\u7684\u5173\u7cfb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u51c6\u786e\u5730\u9884\u6d4b\u4e86\u6587\u6863\u7ea7\u522b\u8bbe\u7f6e\u4e2d\u5b9e\u4f53\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u5728\u5173\u7cfb\u62bd\u53d6\u65b9\u9762\u63a8\u8fdb\u4e86\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u591a\u53e5\u63a8\u7406\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u5173\u7cfb\u68c0\u6d4b\uff0c\u4ece\u800c\u5728\u9700\u8981\u7efc\u5408\u5b9e\u4f53\u7ea7\u522b\u89c1\u89e3\u548c\u53ef\u89e3\u91ca\u6027\u7684\u5b9e\u9645 NLP \u5e94\u7528\u4e2d\u5b9e\u73b0\u6539\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2507.22919", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22919", "abs": "https://arxiv.org/abs/2507.22919", "authors": ["Qixuan Hu", "Xumou Zhang", "Jinman Kim", "Florence Bourgeois", "Adam G. Dunn"], "title": "A novel language model for predicting serious adverse event results in clinical trials from their prospective registrations", "comment": null, "summary": "Objectives: With accurate estimates of expected safety results, clinical\ntrials could be designed to avoid terminations and limit exposing participants\nto unnecessary risks. We evaluated methods for predicting serious adverse event\n(SAE) results in clinical trials using information only from their\nregistrations prior to the trial. Material and Methods: We analysed 22,107\ntwo-arm parallel interventional clinical trials from ClinicalTrials.gov with\nstructured summary results. Two prediction models were developed: a classifier\npredicting will experimental arm have higher SAE rates (area under the receiver\noperating characteristic curve; AUC) than control arm, and a regression model\nto predict the proportion of SAEs in control arms (root mean squared error;\nRMSE). A transfer learning approach using pretrained language models (e.g.,\nClinicalT5, BioBERT) was used for feature extraction, combined with downstream\nmodel for prediction. To maintain semantic representation in long trial texts\nexceeding localised language model input limits, a sliding window method was\ndeveloped for embedding extraction. Results: The best model\n(ClinicalT5+Transformer+MLP) had 77.6% AUC predicting which trial arm has a\nhigher proportion of patients with SAEs. When predicting proportion of\nparticipants experiencing SAE in the control arm, the same model achieved RMSE\nof 18.6%. The sliding window approach consistently outperformed methods without\nit. Across 12 classifiers, the average absolute AUC increase was 2.00%; across\n12 regressors, the average absolute RMSE reduction was 1.58%. Discussion:\nSummary results data available at ClinicalTrials.gov remains underutilised. The\npotential to estimate results of trials before they start is an opportunity to\nimprove trial design and flag discrepancies between expected and reported\nsafety results.", "AI": {"tldr": "The study evaluated methods for predicting serious adverse event (SAE) results in clinical trials using information only from their registrations prior to the trial. They found that the best model (ClinicalT5+Transformer+MLP) had 77.6% AUC predicting which trial arm has a higher proportion of patients with SAEs.", "motivation": "With accurate estimates of expected safety results, clinical trials could be designed to avoid terminations and limit exposing participants to unnecessary risks.", "method": "Two prediction models were developed: a classifier predicting will experimental arm have higher SAE rates (area under the receiver operating characteristic curve; AUC) than control arm, and a regression model to predict the proportion of SAEs in control arms (root mean squared error; RMSE). A transfer learning approach using pretrained language models (e.g., ClinicalT5, BioBERT) was used for feature extraction, combined with downstream model for prediction. To maintain semantic representation in long trial texts exceeding localised language model input limits, a sliding window method was developed for embedding extraction.", "result": "The best model (ClinicalT5+Transformer+MLP) had 77.6% AUC predicting which trial arm has a higher proportion of patients with SAEs. When predicting proportion of participants experiencing SAE in the control arm, the same model achieved RMSE of 18.6%. The sliding window approach consistently outperformed methods without it. Across 12 classifiers, the average absolute AUC increase was 2.00%; across 12 regressors, the average absolute RMSE reduction was 1.58%.", "conclusion": "Summary results data available at ClinicalTrials.gov remains underutilised. The potential to estimate results of trials before they start is an opportunity to improve trial design and flag discrepancies between expected and reported safety results."}}
{"id": "2507.23070", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23070", "abs": "https://arxiv.org/abs/2507.23070", "authors": ["Dmitry Demidov", "Zaigham Zaheer", "Omkar Thawakar", "Salman Khan", "Fahad Shahbaz Khan"], "title": "Vocabulary-free Fine-grained Visual Recognition via Enriched Contextually Grounded Vision-Language Model", "comment": "Accepted to ICCV 2025", "summary": "Fine-grained image classification, the task of distinguishing between\nvisually similar subcategories within a broader category (e.g., bird species,\ncar models, flower types), is a challenging computer vision problem.\nTraditional approaches rely heavily on fixed vocabularies and closed-set\nclassification paradigms, limiting their scalability and adaptability in\nreal-world settings where novel classes frequently emerge. Recent research has\ndemonstrated that combining large language models (LLMs) with vision-language\nmodels (VLMs) makes open-set recognition possible without the need for\npredefined class labels. However, the existing methods are often limited in\nharnessing the power of LLMs at the classification phase, and also rely heavily\non the guessed class names provided by an LLM without thorough analysis and\nrefinement. To address these bottlenecks, we propose our training-free method,\nEnriched-FineR (or E-FineR for short), which demonstrates state-of-the-art\nresults in fine-grained visual recognition while also offering greater\ninterpretability, highlighting its strong potential in real-world scenarios and\nnew domains where expert annotations are difficult to obtain. Additionally, we\ndemonstrate the application of our proposed approach to zero-shot and few-shot\nclassification, where it demonstrated performance on par with the existing SOTA\nwhile being training-free and not requiring human interventions. Overall, our\nvocabulary-free framework supports the shift in image classification from rigid\nlabel prediction to flexible, language-driven understanding, enabling scalable\nand generalizable systems for real-world applications. Well-documented code is\navailable on https://github.com/demidovd98/e-finer.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aE-FineR\u7684\u514d\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u66f4\u5177\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u652f\u6301\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5206\u7c7b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u56fa\u5b9a\u8bcd\u6c47\u548c\u95ed\u96c6\u5206\u7c7b\u8303\u5f0f\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u7ecf\u5e38\u51fa\u73b0\u65b0\u7c7b\u522b\u7684\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u7c7b\u9636\u6bb5\u5229\u7528LLM\u7684\u80fd\u529b\u6709\u9650\uff0c\u5e76\u4e14\u4e25\u91cd\u4f9d\u8d56LLM\u63d0\u4f9b\u7684\u731c\u6d4b\u7c7b\u540d\uff0c\u7f3a\u4e4f\u900f\u5f7b\u7684\u5206\u6790\u548c\u6539\u8fdb\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEnriched-FineR (E-FineR) \u7684\u514d\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4e0e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b (VLM)\u3002", "result": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684E-FineR\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u8be5\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u4e0e\u73b0\u6709\u7684SOTA\u76f8\u5f53\uff0c\u540c\u65f6\u662f\u514d\u8bad\u7ec3\u7684\uff0c\u4e0d\u9700\u8981\u4eba\u5de5\u5e72\u9884\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aE-FineR\u7684\u514d\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u5927\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u73b0\u5b9e\u573a\u666f\u548c\u96be\u4ee5\u83b7\u5f97\u4e13\u5bb6\u6ce8\u91ca\u7684\u65b0\u9886\u57df\u7684\u5f3a\u5927\u6f5c\u529b\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u4e0e\u73b0\u6709\u7684SOTA\u76f8\u5f53\uff0c\u540c\u65f6\u662f\u514d\u8bad\u7ec3\u7684\uff0c\u4e0d\u9700\u8981\u4eba\u5de5\u5e72\u9884\u3002\u603b\u7684\u6765\u8bf4\uff0c\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u65e0\u8bcd\u6c47\u6846\u67b6\u652f\u6301\u56fe\u50cf\u5206\u7c7b\u4ece\u521a\u6027\u6807\u7b7e\u9884\u6d4b\u5230\u7075\u6d3b\u7684\u3001\u8bed\u8a00\u9a71\u52a8\u7684\u7406\u89e3\u7684\u8f6c\u53d8\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u548c\u901a\u7528\u7684\u73b0\u5b9e\u5e94\u7528\u7cfb\u7edf\u3002"}}
{"id": "2507.23035", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2507.23035", "abs": "https://arxiv.org/abs/2507.23035", "authors": ["Xueying Wu", "Baijun Zhou", "Zhihui Gao", "Yuzhe Fu", "Qilin Zheng", "Yintao He", "Hai Li"], "title": "KLLM: Fast LLM Inference with K-Means Quantization", "comment": null, "summary": "Large language model (LLM) inference poses significant challenges due to its\nintensive memory and computation demands. Weight and activation quantization\n(WAQ) offers a promising solution by reducing both memory footprint and\narithmetic complexity. However, two key challenges remain in the existing WAQ\ndesigns. (1) Traditional WAQ designs rely on uniform integer-based quantization\nfor hardware efficiency, but this often results in significant accuracy\ndegradation at low precision. K-Means-based quantization, a non-uniform\nquantization technique, achieves higher accuracy by matching the Gaussian-like\ndistributions of weights and activations in LLMs. However, its non-uniform\nnature prevents direct execution on low-precision compute units, requiring\ndequantization and floating-point matrix multiplications (MatMuls) during\ninference. (2) Activation outliers further hinder effective low-precision WAQ.\nOffline thresholding methods for outlier detection can lead to significant\nmodel performance degradation, while existing online detection techniques\nintroduce substantial runtime overhead.\n  To address the aforementioned challenges and fully unleash the potential of\nWAQ with K-Means quantization for LLM inference, in this paper, we propose\nKLLM, a hardware-software co-design framework. KLLM features an index-based\ncomputation scheme for efficient execution of MatMuls and nonlinear operations\non K-Means-quantized data, which avoids most of the dequantization and\nfull-precision computations. Moreover, KLLM incorporates a novel outlier\ndetection engine, Orizuru, that efficiently identifies the top-$k$ largest and\nsmallest elements in the activation data stream during online inference.\n  Extensive experiments show that, on average, KLLM achieves speedups of 9.67x,\n7.03x and energy efficiency improvements of 229.50x, 150.21x compared to the\nA100 GPU and Atom, respectively.", "AI": {"tldr": "KLLM, a hardware-software co-design framework, addresses challenges in weight and activation quantization for LLM inference, achieving significant speedups and energy efficiency improvements.", "motivation": "Existing weight and activation quantization (WAQ) designs for large language model (LLM) inference face challenges due to accuracy degradation at low precision with uniform quantization and runtime overhead with online outlier detection.", "method": "proposes KLLM, a hardware-software co-design framework featuring an index-based computation scheme and a novel outlier detection engine called Orizuru.", "result": "KLLM achieves significant speedups and energy efficiency improvements compared to A100 GPU and Atom.", "conclusion": "KLLM achieves speedups of 9.67x and 7.03x and energy efficiency improvements of 229.50x and 150.21x compared to the A100 GPU and Atom, respectively."}}
{"id": "2507.23440", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23440", "abs": "https://arxiv.org/abs/2507.23440", "authors": ["Mingzhe Li", "Xin Lu", "Yanyan Zhao"], "title": "Self-Foveate: Enhancing Diversity and Difficulty of Synthesized Instructions from Unsupervised Text via Multi-Level Foveation", "comment": "Accepted by Findings of ACL 2025", "summary": "Large language models (LLMs) with instruction following capabilities have\ndemonstrated impressive problem-solving abilities. While synthesizing\ninstructional data from unsupervised text has become a common approach for\ntraining such models, conventional methods rely heavily on human effort for\ndata annotation. Although existing automated synthesis paradigms have\nalleviated this constraint, they still exhibit significant limitations in\nensuring adequate diversity and difficulty of synthesized instructions. To\naddress these challenges, we propose Self-Foveate, an innovative LLM-driven\nmethod for instruction synthesis. This approach introduces a\n\"Micro-Scatter-Macro\" multi-level foveation methodology that effectively guides\nthe LLM to deeply excavate fine-grained information embedded in unsupervised\ntext, thereby enhancing both the diversity and difficulty of synthesized\ninstructions. Comprehensive experiments across multiple unsupervised corpora\nand diverse model architectures validate the effectiveness and superiority of\nour proposed method. We publicly release our data and codes:\nhttps://github.com/Mubuky/Self-Foveate", "AI": {"tldr": "The paper presents Self-Foveate, an LLM-driven method for instruction synthesis that improves the diversity and difficulty of instructions by using a multi-level foveation approach.", "motivation": "Conventional methods for training LLMs rely heavily on human effort for data annotation, while existing automated synthesis paradigms have limitations in ensuring adequate diversity and difficulty of synthesized instructions.", "method": "The paper proposes a Micro-Scatter-Macro multi-level foveation methodology to guide the LLM in excavating fine-grained information from unsupervised text.", "result": "Comprehensive experiments across multiple unsupervised corpora and diverse model architectures validate the effectiveness and superiority of the proposed method.", "conclusion": "The paper introduces Self-Foveate, a novel LLM-driven method for instruction synthesis that enhances the diversity and difficulty of synthesized instructions."}}
{"id": "2507.23334", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23334", "abs": "https://arxiv.org/abs/2507.23334", "authors": ["Daeyong Kwon", "SeungHeon Doh", "Juhan Nam"], "title": "MUST-RAG: MUSical Text Question Answering with Retrieval Augmented Generation", "comment": "8 pages, 2 figures", "summary": "Recent advancements in Large language models (LLMs) have demonstrated\nremarkable capabilities across diverse domains. While they exhibit strong\nzero-shot performance on various tasks, LLMs' effectiveness in music-related\napplications remains limited due to the relatively small proportion of\nmusic-specific knowledge in their training data. To address this limitation, we\npropose MusT-RAG, a comprehensive framework based on Retrieval Augmented\nGeneration (RAG) to adapt general-purpose LLMs for text-only music question\nanswering (MQA) tasks. RAG is a technique that provides external knowledge to\nLLMs by retrieving relevant context information when generating answers to\nquestions. To optimize RAG for the music domain, we (1) propose MusWikiDB, a\nmusic-specialized vector database for the retrieval stage, and (2) utilizes\ncontext information during both inference and fine-tuning processes to\neffectively transform general-purpose LLMs into music-specific models. Our\nexperiment demonstrates that MusT-RAG significantly outperforms traditional\nfine-tuning approaches in enhancing LLMs' music domain adaptation capabilities,\nshowing consistent improvements across both in-domain and out-of-domain MQA\nbenchmarks. Additionally, our MusWikiDB proves substantially more effective\nthan general Wikipedia corpora, delivering superior performance and\ncomputational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86MusT-RAG\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u97f3\u4e50\u77e5\u8bc6\u5e93\u548c\u4f18\u5316\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff0c\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u97f3\u4e50\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u5fae\u8c03\uff0c\u4e14\u66f4\u6709\u6548\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5404\u4e2a\u9886\u57df\u90fd\u8868\u73b0\u51fa\u4e86\u5353\u8d8a\u7684\u80fd\u529b\u3002\u867d\u7136\u5b83\u4eec\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u4f46\u7531\u4e8e\u5176\u8bad\u7ec3\u6570\u636e\u4e2d\u7279\u5b9a\u4e8e\u97f3\u4e50\u7684\u77e5\u8bc6\u6bd4\u4f8b\u76f8\u5bf9\u8f83\u5c0f\uff0cLLM\u5728\u97f3\u4e50\u76f8\u5173\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u4ecd\u7136\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86MusT-RAG\uff0c\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u7efc\u5408\u6846\u67b6\uff0c\u4ee5\u4f7f\u901a\u7528LLM\u9002\u5e94\u4ec5\u6587\u672c\u97f3\u4e50\u95ee\u7b54\uff08MQA\uff09\u4efb\u52a1\u3002\u4e3a\u4e86\u4f18\u5316RAG\u5728\u97f3\u4e50\u9886\u57df\uff0c\u6211\u4eec\uff081\uff09\u63d0\u51fa\u4e86MusWikiDB\uff0c\u4e00\u4e2a\u7528\u4e8e\u68c0\u7d22\u9636\u6bb5\u7684\u97f3\u4e50\u4e13\u7528\u5411\u91cf\u6570\u636e\u5e93\uff0c\u4ee5\u53ca\uff082\uff09\u5728\u63a8\u7406\u548c\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5229\u7528\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ee5\u6709\u6548\u5730\u5c06\u901a\u7528LLM\u8f6c\u6362\u4e3a\u97f3\u4e50\u4e13\u7528\u6a21\u578b\u3002", "result": "MusT-RAG\u5728\u589e\u5f3aLLM\u7684\u97f3\u4e50\u9886\u57df\u9002\u5e94\u80fd\u529b\u65b9\u9762\u660e\u663e\u4f18\u4e8e\u4f20\u7edf\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u5728\u9886\u57df\u5185\u548c\u9886\u57df\u5916MQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8868\u73b0\u51fa\u6301\u7eed\u7684\u6539\u8fdb\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684MusWikiDB\u88ab\u8bc1\u660e\u6bd4\u4e00\u822c\u7684\u7ef4\u57fa\u767e\u79d1\u8bed\u6599\u5e93\u66f4\u6709\u6548\uff0c\u63d0\u4f9b\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "MusT-RAG\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86LLM\u7684\u97f3\u4e50\u9886\u57df\u9002\u5e94\u80fd\u529b\uff0c\u5e76\u5728\u9886\u57df\u5185\u548c\u9886\u57df\u5916MQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u6301\u7eed\u7684\u6539\u8fdb\u3002MusWikiDB\u88ab\u8bc1\u660e\u6bd4\u4e00\u822c\u7684\u7ef4\u57fa\u767e\u79d1\u8bed\u6599\u5e93\u66f4\u6709\u6548\uff0c\u63d0\u4f9b\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2507.22920", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22920", "abs": "https://arxiv.org/abs/2507.22920", "authors": ["Jindong Li", "Yali Fu", "Jiahong Liu", "Linxiao Cao", "Wei Ji", "Menglin Yang", "Irwin King", "Ming-Hsuan Yang"], "title": "Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has intensified the\nneed for effective mechanisms to transform continuous multimodal data into\ndiscrete representations suitable for language-based processing. Discrete\ntokenization, with vector quantization (VQ) as a central approach, offers both\ncomputational efficiency and compatibility with LLM architectures. Despite its\ngrowing importance, there is a lack of a comprehensive survey that\nsystematically examines VQ techniques in the context of LLM-based systems. This\nwork fills this gap by presenting the first structured taxonomy and analysis of\ndiscrete tokenization methods designed for LLMs. We categorize 8 representative\nVQ variants that span classical and modern paradigms and analyze their\nalgorithmic principles, training dynamics, and integration challenges with LLM\npipelines. Beyond algorithm-level investigation, we discuss existing research\nin terms of classical applications without LLMs, LLM-based single-modality\nsystems, and LLM-based multimodal systems, highlighting how quantization\nstrategies influence alignment, reasoning, and generation performance. In\naddition, we identify key challenges including codebook collapse, unstable\ngradient estimation, and modality-specific encoding constraints. Finally, we\ndiscuss emerging research directions such as dynamic and task-adaptive\nquantization, unified tokenization frameworks, and biologically inspired\ncodebook learning. This survey bridges the gap between traditional vector\nquantization and modern LLM applications, serving as a foundational reference\nfor the development of efficient and generalizable multimodal systems. A\ncontinuously updated version is available at:\nhttps://github.com/jindongli-Ai/LLM-Discrete-Tokenization-Survey.", "AI": {"tldr": "This survey provides a structured taxonomy and analysis of discrete tokenization methods designed for LLMs, focusing on vector quantization (VQ) techniques. It identifies challenges and emerging research directions, bridging the gap between traditional VQ and modern LLM applications.", "motivation": "The rapid advancement of large language models (LLMs) has intensified the need for effective mechanisms to transform continuous multimodal data into discrete representations suitable for language-based processing. Despite its growing importance, there is a lack of a comprehensive survey that systematically examines VQ techniques in the context of LLM-based systems.", "method": "We categorize 8 representative VQ variants that span classical and modern paradigms and analyze their algorithmic principles, training dynamics, and integration challenges with LLM pipelines.", "result": "We discuss existing research in terms of classical applications without LLMs, LLM-based single-modality systems, and LLM-based multimodal systems, highlighting how quantization strategies influence alignment, reasoning, and generation performance. In addition, we identify key challenges including codebook collapse, unstable gradient estimation, and modality-specific encoding constraints.", "conclusion": "This survey bridges the gap between traditional vector quantization and modern LLM applications, serving as a foundational reference for the development of efficient and generalizable multimodal systems."}}
{"id": "2507.23110", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23110", "abs": "https://arxiv.org/abs/2507.23110", "authors": ["Zheyuan Zhang", "Linkai Peng", "Wanying Dou", "Cuiling Sun", "Halil Ertugrul Aktas", "Andrea M. Bejar", "Elif Keles", "Gorkem Durak", "Ulas Bagci"], "title": "Rethink Domain Generalization in Heterogeneous Sequence MRI Segmentation", "comment": null, "summary": "Clinical magnetic-resonance (MR) protocols generate many T1 and T2 sequences\nwhose appearance differs more than the acquisition sites that produce them.\nExisting domain-generalization benchmarks focus almost on cross-center shifts\nand overlook this dominant source of variability. Pancreas segmentation remains\na major challenge in abdominal imaging: the gland is small, irregularly,\nsurrounded by organs and fat, and often suffers from low T1 contrast.\nState-of-the-art deep networks that already achieve >90% Dice on the liver or\nkidneys still miss 20-30% of the pancreas. The organ is also systematically\nunder-represented in public cross-domain benchmarks, despite its clinical\nimportance in early cancer detection, surgery, and diabetes research. To close\nthis gap, we present PancreasDG, a large-scale multi-center 3D MRI pancreas\nsegmentation dataset for investigating domain generalization in medical\nimaging. The dataset comprises 563 MRI scans from six institutions, spanning\nboth venous phase and out-of-phase sequences, enabling study of both\ncross-center and cross-sequence variations with pixel-accurate pancreas masks\ncreated by a double-blind, two-pass protocol. Through comprehensive analysis,\nwe reveal three insights: (i) limited sampling introduces significant variance\nthat may be mistaken for distribution shifts, (ii) cross-center performance\ncorrelates with source domain performance for identical sequences, and (iii)\ncross-sequence shifts require specialized solutions. We also propose a\nsemi-supervised approach that leverages anatomical invariances, significantly\noutperforming state-of-the-art domain generalization techniques with 61.63%\nDice score improvements and 87.00% on two test centers for cross-sequence\nsegmentation. PancreasDG sets a new benchmark for domain generalization in\nmedical imaging. Dataset, code, and models will be available at\nhttps://pancreasdg.netlify.app.", "AI": {"tldr": "PancreasDG, a large-scale multi-center 3D MRI pancreas segmentation dataset, is presented to address domain generalization challenges in medical imaging. A semi-supervised approach improves cross-sequence segmentation.", "motivation": "Existing benchmarks overlook the dominant source of variability in clinical MR protocols, and pancreas segmentation remains a challenge with under-representation in public cross-domain benchmarks.", "method": "A semi-supervised approach leveraging anatomical invariances is proposed.", "result": "The semi-supervised approach significantly outperforms state-of-the-art domain generalization techniques, achieving 61.63% Dice score improvements and 87.00% on two test centers for cross-sequence segmentation. The analysis reveals insights into limited sampling variance, cross-center performance correlation, and the need for specialized solutions for cross-sequence shifts.", "conclusion": "PancreasDG establishes a new benchmark for domain generalization in medical imaging, with a semi-supervised approach demonstrating significant improvements in cross-sequence pancreas segmentation."}}
{"id": "2507.23037", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23037", "abs": "https://arxiv.org/abs/2507.23037", "authors": ["Aur\u00e9lie Leribaux", "Rafael Oyamada", "Johannes De Smedt", "Zahra Dasht Bozorgi", "Artem Polyvyanyy", "Jochen De Weerdt"], "title": "Linking Actor Behavior to Process Performance Over Time", "comment": "Accepted for presentation at the 5th Workshop on Change, Drift, and\n  Dynamics of Organizational Processes (ProDy), BPM 2025", "summary": "Understanding how actor behavior influences process outcomes is a critical\naspect of process mining. Traditional approaches often use aggregate and static\nprocess data, overlooking the temporal and causal dynamics that arise from\nindividual actor behavior. This limits the ability to accurately capture the\ncomplexity of real-world processes, where individual actor behavior and\ninteractions between actors significantly shape performance. In this work, we\naddress this gap by integrating actor behavior analysis with Granger causality\nto identify correlating links in time series data. We apply this approach to\nrealworld event logs, constructing time series for actor interactions, i.e.\ncontinuation, interruption, and handovers, and process outcomes. Using Group\nLasso for lag selection, we identify a small but consistently influential set\nof lags that capture the majority of causal influence, revealing that actor\nbehavior has direct and measurable impacts on process performance, particularly\nthroughput time. These findings demonstrate the potential of actor-centric,\ntime series-based methods for uncovering the temporal dependencies that drive\nprocess outcomes, offering a more nuanced understanding of how individual\nbehaviors impact overall process efficiency.", "AI": {"tldr": "This paper integrates actor behavior analysis with Granger causality to identify correlating links in time series data. It reveals that actor behavior has direct and measurable impacts on process performance, particularly throughput time.", "motivation": "Traditional approaches often use aggregate and static process data, overlooking the temporal and causal dynamics that arise from individual actor behavior. This limits the ability to accurately capture the complexity of real-world processes, where individual actor behavior and interactions between actors significantly shape performance.", "method": "integrating actor behavior analysis with Granger causality to identify correlating links in time series data.Constructing time series for actor interactions, i.e. continuation, interruption, and handovers, and process outcomes. Using Group Lasso for lag selection, we identify a small but consistently influential set of lags that capture the majority of causal influence", "result": "identifying a small but consistently influential set of lags that capture the majority of causal influence, revealing that actor behavior has direct and measurable impacts on process performance, particularly throughput time.", "conclusion": "Actor behavior has direct and measurable impacts on process performance, particularly throughput time."}}
{"id": "2507.23488", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23488", "abs": "https://arxiv.org/abs/2507.23488", "authors": ["Kacper Kadziolka", "Saber Salehkaleybar"], "title": "Causal Reasoning in Pieces: Modular In-Context Learning for Causal Discovery", "comment": null, "summary": "Causal inference remains a fundamental challenge for large language models.\nRecent advances in internal reasoning with large language models have sparked\ninterest in whether state-of-the-art reasoning models can robustly perform\ncausal discovery-a task where conventional models often suffer from severe\noverfitting and near-random performance under data perturbations. We study\ncausal discovery on the Corr2Cause benchmark using the emergent OpenAI's\no-series and DeepSeek-R model families and find that these reasoning-first\narchitectures achieve significantly greater native gains than prior approaches.\nTo capitalize on these strengths, we introduce a modular in-context pipeline\ninspired by the Tree-of-Thoughts and Chain-of-Thoughts methodologies, yielding\nnearly three-fold improvements over conventional baselines. We further probe\nthe pipeline's impact by analyzing reasoning chain length, complexity, and\nconducting qualitative and quantitative comparisons between conventional and\nreasoning models. Our findings suggest that while advanced reasoning models\nrepresent a substantial leap forward, carefully structured in-context\nframeworks are essential to maximize their capabilities and offer a\ngeneralizable blueprint for causal discovery across diverse domains.", "AI": {"tldr": "Reasoning-first architectures achieve greater gains in causal discovery, and a modular in-context pipeline yields nearly three-fold improvements over conventional baselines.", "motivation": "Causal inference remains a fundamental challenge for large language models. Recent advances in internal reasoning with large language models have sparked interest in whether state-of-the-art reasoning models can robustly perform causal discovery-a task where conventional models often suffer from severe overfitting and near-random performance under data perturbations.", "method": "a modular in-context pipeline inspired by the Tree-of-Thoughts and Chain-of-Thoughts methodologies", "result": "reasoning-first architectures achieve significantly greater native gains than prior approaches, yielding nearly three-fold improvements over conventional baselines", "conclusion": "Advanced reasoning models represent a substantial leap forward, but carefully structured in-context frameworks are essential to maximize their capabilities and offer a generalizable blueprint for causal discovery across diverse domains."}}
{"id": "2507.23134", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23134", "abs": "https://arxiv.org/abs/2507.23134", "authors": ["Sanghun Jung", "Jingjing Zheng", "Ke Zhang", "Nan Qiao", "Albert Y. C. Chen", "Lu Xia", "Chi Liu", "Yuyin Sun", "Xiao Zeng", "Hsiang-Wei Huang", "Byron Boots", "Min Sun", "Cheng-Hao Kuo"], "title": "Details Matter for Indoor Open-vocabulary 3D Instance Segmentation", "comment": "ICCV 2025", "summary": "Unlike closed-vocabulary 3D instance segmentation that is often trained\nend-to-end, open-vocabulary 3D instance segmentation (OV-3DIS) often leverages\nvision-language models (VLMs) to generate 3D instance proposals and classify\nthem. While various concepts have been proposed from existing research, we\nobserve that these individual concepts are not mutually exclusive but\ncomplementary. In this paper, we propose a new state-of-the-art solution for\nOV-3DIS by carefully designing a recipe to combine the concepts together and\nrefining them to address key challenges. Our solution follows the two-stage\nscheme: 3D proposal generation and instance classification. We employ robust 3D\ntracking-based proposal aggregation to generate 3D proposals and remove\noverlapped or partial proposals by iterative merging/removal. For the\nclassification stage, we replace the standard CLIP model with Alpha-CLIP, which\nincorporates object masks as an alpha channel to reduce background noise and\nobtain object-centric representation. Additionally, we introduce the\nstandardized maximum similarity (SMS) score to normalize text-to-proposal\nsimilarity, effectively filtering out false positives and boosting precision.\nOur framework achieves state-of-the-art performance on ScanNet200 and S3DIS\nacross all AP and AR metrics, even surpassing an end-to-end closed-vocabulary\nmethod.", "AI": {"tldr": "Proposes a new state-of-the-art solution for open-vocabulary 3D instance segmentation by combining existing concepts and refining them. Uses 3D tracking-based proposal aggregation and Alpha-CLIP with standardized maximum similarity (SMS) score.", "motivation": "Open-vocabulary 3D instance segmentation (OV-3DIS) often leverages vision-language models (VLMs) to generate 3D instance proposals and classify them. Existing individual concepts are not mutually exclusive but complementary.", "method": "The solution follows the two-stage scheme: 3D proposal generation and instance classification. It employs robust 3D tracking-based proposal aggregation to generate 3D proposals and remove overlapped or partial proposals by iterative merging/removal. For the classification stage, the standard CLIP model is replaced with Alpha-CLIP, which incorporates object masks as an alpha channel to reduce background noise and obtain object-centric representation. Additionally, the standardized maximum similarity (SMS) score is introduced to normalize text-to-proposal similarity.", "result": "Achieves state-of-the-art performance on ScanNet200 and S3DIS across all AP and AR metrics.", "conclusion": "The proposed framework achieves state-of-the-art performance on ScanNet200 and S3DIS across all AP and AR metrics, even surpassing an end-to-end closed-vocabulary method."}}
{"id": "2507.23043", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23043", "abs": "https://arxiv.org/abs/2507.23043", "authors": ["Junyi Fan", "Li Sun", "Shuheng Chen", "Yong Si", "Minoo Ahmadi", "Greg Placencia", "Elham Pishgar", "Kamiar Alaei", "Maryam Pishgar"], "title": "Prediction of Significant Creatinine Elevation in First ICU Stays with Vancomycin Use: A retrospective study through Catboost", "comment": null, "summary": "Background: Vancomycin, a key antibiotic for severe Gram-positive infections\nin ICUs, poses a high nephrotoxicity risk. Early prediction of kidney injury in\ncritically ill patients is challenging. This study aimed to develop a machine\nlearning model to predict vancomycin-related creatinine elevation using routine\nICU data.\n  Methods: We analyzed 10,288 ICU patients (aged 18-80) from the MIMIC-IV\ndatabase who received vancomycin. Kidney injury was defined by KDIGO criteria\n(creatinine rise >=0.3 mg/dL within 48h or >=50% within 7d). Features were\nselected via SelectKBest (top 30) and Random Forest ranking (final 15). Six\nalgorithms were tested with 5-fold cross-validation. Interpretability was\nevaluated using SHAP, Accumulated Local Effects (ALE), and Bayesian posterior\nsampling.\n  Results: Of 10,288 patients, 2,903 (28.2%) developed creatinine elevation.\nCatBoost performed best (AUROC 0.818 [95% CI: 0.801-0.834], sensitivity 0.800,\nspecificity 0.681, negative predictive value 0.900). Key predictors were\nphosphate, total bilirubin, magnesium, Charlson index, and APSIII. SHAP\nconfirmed phosphate as a major risk factor. ALE showed dose-response patterns.\nBayesian analysis estimated mean risk 60.5% (95% credible interval: 16.8-89.4%)\nin high-risk cases.\n  Conclusions: This machine learning model predicts vancomycin-associated\ncreatinine elevation from routine ICU data with strong accuracy and\ninterpretability, enabling early risk detection and supporting timely\ninterventions in critical care.", "AI": {"tldr": "Developed a machine learning model using ICU data to predict vancomycin-related kidney injury, achieving high accuracy with CatBoost. Phosphate, bilirubin, and magnesium levels are key predictors.", "motivation": "Vancomycin, a key antibiotic for severe Gram-positive infections in ICUs, poses a high nephrotoxicity risk. Early prediction of kidney injury in critically ill patients is challenging.", "method": "We analyzed 10,288 ICU patients (aged 18-80) from the MIMIC-IV database who received vancomycin. Kidney injury was defined by KDIGO criteria (creatinine rise >=0.3 mg/dL within 48h or >=50% within 7d). Features were selected via SelectKBest (top 30) and Random Forest ranking (final 15). Six algorithms were tested with 5-fold cross-validation. Interpretability was evaluated using SHAP, Accumulated Local Effects (ALE), and Bayesian posterior sampling.", "result": "Of 10,288 patients, 2,903 (28.2%) developed creatinine elevation. CatBoost performed best (AUROC 0.818 [95% CI: 0.801-0.834], sensitivity 0.800, specificity 0.681, negative predictive value 0.900). Key predictors were phosphate, total bilirubin, magnesium, Charlson index, and APSIII.", "conclusion": "This machine learning model predicts vancomycin-associated creatinine elevation from routine ICU data with strong accuracy and interpretability, enabling early risk detection and supporting timely interventions in critical care."}}
{"id": "2507.23497", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23497", "abs": "https://arxiv.org/abs/2507.23497", "authors": ["David A Kelly", "Hana Chockler"], "title": "Causal Identification of Sufficient, Contrastive and Complete Feature Sets in Image Classification", "comment": "13 pages, 13 figures, appendix included", "summary": "Existing algorithms for explaining the outputs of image classifiers are based\non a variety of approaches and produce explanations that lack formal rigor. On\nthe other hand, logic-based explanations are formally and rigorously defined\nbut their computability relies on strict assumptions about the model that do\nnot hold on image classifiers.\n  In this paper, we show that causal explanations, in addition to being\nformally and rigorously defined, enjoy the same formal properties as\nlogic-based ones, while still lending themselves to black-box algorithms and\nbeing a natural fit for image classifiers. We prove formal properties of causal\nexplanations and introduce contrastive causal explanations for image\nclassifiers. Moreover, we augment the definition of explanation with confidence\nawareness and introduce complete causal explanations: explanations that are\nclassified with exactly the same confidence as the original image.\n  We implement our definitions, and our experimental results demonstrate that\ndifferent models have different patterns of sufficiency, contrastiveness, and\ncompleteness. Our algorithms are efficiently computable, taking on average 6s\nper image on a ResNet50 model to compute all types of explanations, and are\ntotally black-box, needing no knowledge of the model, no access to model\ninternals, no access to gradient, nor requiring any properties, such as\nmonotonicity, of the model.", "AI": {"tldr": "\u56e0\u679c\u89e3\u91ca\u65e2\u5177\u6709\u5f62\u5f0f\u4e25\u8c28\u6027\uff0c\u53c8\u5177\u6709\u4e0e\u57fa\u4e8e\u903b\u8f91\u7684\u89e3\u91ca\u76f8\u540c\u7684\u5f62\u5f0f\u5c5e\u6027\uff0c\u540c\u65f6\u4ecd\u7136\u9002\u7528\u4e8e\u9ed1\u76d2\u7b97\u6cd5\uff0c\u5e76\u4e14\u81ea\u7136\u9002\u5408\u56fe\u50cf\u5206\u7c7b\u5668\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u5206\u7c7b\u5668\u8f93\u51fa\u89e3\u91ca\u7b97\u6cd5\u57fa\u4e8e\u591a\u79cd\u65b9\u6cd5\uff0c\u4ea7\u751f\u7684\u89e3\u91ca\u7f3a\u4e4f\u5f62\u5f0f\u4e25\u8c28\u6027\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u57fa\u4e8e\u903b\u8f91\u7684\u89e3\u91ca\u5728\u5f62\u5f0f\u4e0a\u662f\u4e25\u683c\u5b9a\u4e49\u7684\uff0c\u4f46\u5b83\u4eec\u7684\u53ef\u8ba1\u7b97\u6027\u4f9d\u8d56\u4e8e\u5bf9\u6a21\u578b\u7684\u4e25\u683c\u5047\u8bbe\uff0c\u800c\u8fd9\u4e9b\u5047\u8bbe\u5e76\u4e0d\u9002\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u5668\u3002", "method": "\u56e0\u679c\u89e3\u91ca", "result": "\u8bc1\u660e\u4e86\u56e0\u679c\u89e3\u91ca\u7684\u5f62\u5f0f\u5c5e\u6027\uff0c\u5e76\u4ecb\u7ecd\u4e86\u56fe\u50cf\u5206\u7c7b\u5668\u7684\u5bf9\u6bd4\u56e0\u679c\u89e3\u91ca\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4f7f\u7528\u7f6e\u4fe1\u5ea6\u611f\u77e5\u6765\u589e\u5f3a\u89e3\u91ca\u7684\u5b9a\u4e49\uff0c\u5e76\u5f15\u5165\u5b8c\u6574\u7684\u56e0\u679c\u89e3\u91ca\uff1a\u4ee5\u4e0e\u539f\u59cb\u56fe\u50cf\u5b8c\u5168\u76f8\u540c\u7684\u7f6e\u4fe1\u5ea6\u8fdb\u884c\u5206\u7c7b\u7684\u89e3\u91ca\u3002", "conclusion": "\u4e0d\u540c\u6a21\u578b\u5728\u5145\u5206\u6027\u3001\u5bf9\u6bd4\u6027\u548c\u5b8c\u6574\u6027\u65b9\u9762\u5177\u6709\u4e0d\u540c\u7684\u6a21\u5f0f\u3002\u8fd9\u4e9b\u7b97\u6cd5\u662f\u9ad8\u6548\u53ef\u8ba1\u7b97\u7684\uff0c\u5728 ResNet50 \u6a21\u578b\u4e0a\u5e73\u5747\u6bcf\u5f20\u56fe\u50cf\u82b1\u8d39 6 \u79d2\u6765\u8ba1\u7b97\u6240\u6709\u7c7b\u578b\u7684\u89e3\u91ca\uff0c\u5e76\u4e14\u662f\u5b8c\u5168\u9ed1\u76d2\u7684\uff0c\u4e0d\u9700\u8981\u4e86\u89e3\u6a21\u578b\uff0c\u4e0d\u9700\u8981\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\uff0c\u4e0d\u9700\u8981\u8bbf\u95ee\u68af\u5ea6\uff0c\u4e5f\u4e0d\u9700\u8981\u6a21\u578b\u7684\u4efb\u4f55\u5c5e\u6027\uff0c\u4f8b\u5982\u5355\u8c03\u6027\u3002"}}
{"id": "2507.23400", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.23400", "abs": "https://arxiv.org/abs/2507.23400", "authors": ["Yongbing Zhang", "Fang Nan", "Shengxiang Gao", "Yuxin Huang", "Kaiwen Tan", "Zhengtao Yu"], "title": "MRGSEM-Sum: An Unsupervised Multi-document Summarization Framework based on Multi-Relational Graphs and Structural Entropy Minimization", "comment": null, "summary": "The core challenge faced by multi-document summarization is the complexity of\nrelationships among documents and the presence of information redundancy. Graph\nclustering is an effective paradigm for addressing this issue, as it models the\ncomplex relationships among documents using graph structures and reduces\ninformation redundancy through clustering, achieving significant research\nprogress. However, existing methods often only consider single-relational\ngraphs and require a predefined number of clusters, which hinders their ability\nto fully represent rich relational information and adaptively partition\nsentence groups to reduce redundancy. To overcome these limitations, we propose\nMRGSEM-Sum, an unsupervised multi-document summarization framework based on\nmulti-relational graphs and structural entropy minimization. Specifically, we\nconstruct a multi-relational graph that integrates semantic and discourse\nrelations between sentences, comprehensively modeling the intricate and dynamic\nconnections among sentences across documents. We then apply a two-dimensional\nstructural entropy minimization algorithm for clustering, automatically\ndetermining the optimal number of clusters and effectively organizing sentences\ninto coherent groups. Finally, we introduce a position-aware compression\nmechanism to distill each cluster, generating concise and informative\nsummaries. Extensive experiments on four benchmark datasets (Multi-News,\nDUC-2004, PubMed, and WikiSum) demonstrate that our approach consistently\noutperforms previous unsupervised methods and, in several cases, achieves\nperformance comparable to supervised models and large language models. Human\nevaluation demonstrates that the summaries generated by MRGSEM-Sum exhibit high\nconsistency and coverage, approaching human-level quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u76d1\u7763\u591a\u6587\u6863\u6458\u8981\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u591a\u5173\u7cfb\u56fe\u548c\u7ed3\u6784\u71b5\u6700\u5c0f\u5316\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u8003\u8651\u5355\u5173\u7cfb\u56fe\uff0c\u9700\u8981\u9884\u5b9a\u4e49\u6570\u91cf\u7684\u805a\u7c7b\uff0c\u8fd9\u963b\u788d\u4e86\u5b83\u4eec\u5145\u5206\u8868\u793a\u4e30\u5bcc\u7684\u5173\u7cfb\u4fe1\u606f\u548c\u81ea\u9002\u5e94\u5730\u5212\u5206\u53e5\u5b50\u7ec4\u4ee5\u51cf\u5c11\u5197\u4f59\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5173\u7cfb\u56fe\u548c\u7ed3\u6784\u71b5\u6700\u5c0f\u5316\u7684\u65e0\u76d1\u7763\u591a\u6587\u6863\u6458\u8981\u6846\u67b6 MRGSEM-Sum\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u5173\u7cfb\u56fe\uff0c\u8be5\u56fe\u96c6\u6210\u4e86\u53e5\u5b50\u4e4b\u95f4\u7684\u8bed\u4e49\u548c\u8bed\u7bc7\u5173\u7cfb\uff0c\u5168\u9762\u5efa\u6a21\u4e86\u6587\u6863\u4e2d\u53e5\u5b50\u4e4b\u95f4\u590d\u6742\u800c\u52a8\u6001\u7684\u8fde\u63a5\u3002\u7136\u540e\u5e94\u7528\u4e8c\u7ef4\u7ed3\u6784\u71b5\u6700\u5c0f\u5316\u7b97\u6cd5\u8fdb\u884c\u805a\u7c7b\uff0c\u81ea\u52a8\u786e\u5b9a\u6700\u4f73\u805a\u7c7b\u6570\u91cf\uff0c\u6709\u6548\u5730\u5c06\u53e5\u5b50\u7ec4\u7ec7\u6210\u8fde\u8d2f\u7684\u7ec4\u3002\u5f15\u5165\u4e86\u4e00\u79cd\u4f4d\u7f6e\u611f\u77e5\u538b\u7f29\u673a\u5236\u6765\u63d0\u70bc\u6bcf\u4e2a\u96c6\u7fa4\uff0c\u751f\u6210\u7b80\u6d01\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u6458\u8981\u3002", "conclusion": "MRGSEM-Sum \u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u4ee5\u524d\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u5176\u6027\u80fd\u4e0e\u76d1\u7763\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f8\u5f53\u3002\u4eba\u5de5\u8bc4\u4f30\u8868\u660e\uff0cMRGSEM-Sum \u751f\u6210\u7684\u6458\u8981\u663e\u793a\u51fa\u9ad8\u5ea6\u7684\u4e00\u81f4\u6027\u548c\u8986\u76d6\u7387\uff0c\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u7684\u8d28\u91cf\u3002"}}
{"id": "2507.22922", "categories": ["cs.CL", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.22922", "abs": "https://arxiv.org/abs/2507.22922", "authors": ["Mateusz Kmak", "Kamil Chmurzy\u0144ski", "Kamil Matejuk", "Pawe\u0142 Kotzbach", "Jan Koco\u0144"], "title": "Predicting stock prices with ChatGPT-annotated Reddit sentiment", "comment": "International Conference on Computational Science 2025", "summary": "The surge of retail investor activity on social media, exemplified by the\n2021 GameStop short squeeze, raised questions about the influence of online\nsentiment on stock prices. This paper explores whether sentiment derived from\nsocial media discussions can meaningfully predict stock market movements. We\nfocus on Reddit's r/wallstreetbets and analyze sentiment related to two\ncompanies: GameStop (GME) and AMC Entertainment (AMC). To assess sentiment's\nrole, we employ two existing text-based sentiment analysis methods and\nintroduce a third, a ChatGPT-annotated and fine-tuned RoBERTa-based model\ndesigned to better interpret the informal language and emojis prevalent in\nsocial media discussions. We use correlation and causality metrics to determine\nthese models' predictive power. Surprisingly, our findings suggest that social\nmedia sentiment has only a weak correlation with stock prices. At the same\ntime, simpler metrics, such as the volume of comments and Google search trends,\nexhibit stronger predictive signals. These results highlight the complexity of\nretail investor behavior and suggest that traditional sentiment analysis may\nnot fully capture the nuances of market-moving online discussions.", "AI": {"tldr": "Social media sentiment has only a weak correlation with stock prices, simpler metrics have stronger predictive signals.", "motivation": "Explore whether sentiment derived from social media discussions can meaningfully predict stock market movements.", "method": "Employ two existing text-based sentiment analysis methods and introduce a third, a ChatGPT-annotated and fine-tuned RoBERTa-based model.", "result": "Social media sentiment has only a weak correlation with stock prices.", "conclusion": "Social media sentiment has only a weak correlation with stock prices, while simpler metrics, such as the volume of comments and Google search trends, exhibit stronger predictive signals."}}
{"id": "2507.23143", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23143", "abs": "https://arxiv.org/abs/2507.23143", "authors": ["Xiaochen Zhao", "Hongyi Xu", "Guoxian Song", "You Xie", "Chenxu Zhang", "Xiu Li", "Linjie Luo", "Jinli Suo", "Yebin Liu"], "title": "X-NeMo: Expressive Neural Motion Reenactment via Disentangled Latent Attention", "comment": "ICLR 2025, code is available at\n  https://github.com/bytedance/x-nemo-inference", "summary": "We propose X-NeMo, a novel zero-shot diffusion-based portrait animation\npipeline that animates a static portrait using facial movements from a driving\nvideo of a different individual. Our work first identifies the root causes of\nthe key issues in prior approaches, such as identity leakage and difficulty in\ncapturing subtle and extreme expressions. To address these challenges, we\nintroduce a fully end-to-end training framework that distills a 1D\nidentity-agnostic latent motion descriptor from driving image, effectively\ncontrolling motion through cross-attention during image generation. Our\nimplicit motion descriptor captures expressive facial motion in fine detail,\nlearned end-to-end from a diverse video dataset without reliance on pretrained\nmotion detectors. We further enhance expressiveness and disentangle motion\nlatents from identity cues by supervising their learning with a dual GAN\ndecoder, alongside spatial and color augmentations. By embedding the driving\nmotion into a 1D latent vector and controlling motion via cross-attention\nrather than additive spatial guidance, our design eliminates the transmission\nof spatial-aligned structural clues from the driving condition to the diffusion\nbackbone, substantially mitigating identity leakage. Extensive experiments\ndemonstrate that X-NeMo surpasses state-of-the-art baselines, producing highly\nexpressive animations with superior identity resemblance. Our code and models\nare available for research.", "AI": {"tldr": "X-NeMo is a new portrait animation pipeline that solves identity leakage and expression capture issues by using a 1D identity-agnostic latent motion descriptor and cross-attention.", "motivation": "identifies the root causes of the key issues in prior approaches, such as identity leakage and difficulty in capturing subtle and extreme expressions. To address these challenges", "method": "a novel zero-shot diffusion-based portrait animation pipeline that animates a static portrait using facial movements from a driving video of a different individual. introduce a fully end-to-end training framework that distills a 1D identity-agnostic latent motion descriptor from driving image, effectively controlling motion through cross-attention during image generation. embedding the driving motion into a 1D latent vector and controlling motion via cross-attention rather than additive spatial guidance", "result": "implicit motion descriptor captures expressive facial motion in fine detail, learned end-to-end from a diverse video dataset without reliance on pretrained motion detectors. enhance expressiveness and disentangle motion latents from identity cues by supervising their learning with a dual GAN decoder, alongside spatial and color augmentations.", "conclusion": "X-NeMo surpasses state-of-the-art baselines, producing highly expressive animations with superior identity resemblance."}}
{"id": "2507.23073", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23073", "abs": "https://arxiv.org/abs/2507.23073", "authors": ["Annalisa Barbara", "Joseph Lazzaro", "Ciara Pike-Burke"], "title": "Locally Differentially Private Thresholding Bandits", "comment": "18th European Workshop on Reinforcement Learning (EWRL 2025)", "summary": "This work investigates the impact of ensuring local differential privacy in\nthe thresholding bandit problem. We consider both the fixed budget and fixed\nconfidence settings. We propose methods that utilize private responses,\nobtained through a Bernoulli-based differentially private mechanism, to\nidentify arms with expected rewards exceeding a predefined threshold. We show\nthat this procedure provides strong privacy guarantees and derive theoretical\nperformance bounds on the proposed algorithms. Additionally, we present general\nlower bounds that characterize the additional loss incurred by any\ndifferentially private mechanism, and show that the presented algorithms match\nthese lower bounds up to poly-logarithmic factors. Our results provide valuable\ninsights into privacy-preserving decision-making frameworks in bandit problems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8c03\u67e5\u4e86\u5dee\u5206\u9690\u79c1\u5bf9\u9608\u503cbandit\u95ee\u9898\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u5229\u7528\u79c1\u6709\u54cd\u5e94\u8bc6\u522b\u9ad8\u56de\u62a5\u624b\u81c2\u7684\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u6027\u80fd\u754c\u9650\u548c\u4e0b\u754c\u3002", "motivation": "\u8fd9\u9879\u5de5\u4f5c\u8c03\u67e5\u4e86\u5728\u9608\u503cbandit\u95ee\u9898\u4e2d\u786e\u4fdd\u5c40\u90e8\u5dee\u5206\u9690\u79c1\u7684\u5f71\u54cd\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u5229\u7528\u57fa\u4e8e\u4f2f\u52aa\u5229\u7684\u5dee\u5206\u9690\u79c1\u673a\u5236\u83b7\u5f97\u7684\u79c1\u6709\u54cd\u5e94\u6765\u8bc6\u522b\u9884\u671f\u56de\u62a5\u8d85\u8fc7\u9884\u5b9a\u4e49\u9608\u503c\u7684\u624b\u81c2\u7684\u65b9\u6cd5\u3002", "result": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u8be5\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u9690\u79c1\u4fdd\u8bc1\uff0c\u5e76\u63a8\u5bfc\u4e86\u6240\u63d0\u51fa\u7b97\u6cd5\u7684\u7406\u8bba\u6027\u80fd\u754c\u9650\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u8868\u5f81\u4efb\u4f55\u5dee\u5206\u9690\u79c1\u673a\u5236\u6240\u4ea7\u751f\u7684\u989d\u5916\u635f\u5931\u7684\u901a\u7528\u4e0b\u754c\uff0c\u5e76\u8868\u660e\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u4e0e\u8fd9\u4e9b\u4e0b\u754c\u76f8\u5339\u914d\uff0c\u76f4\u81f3poly-logarithmic\u56e0\u5b50\u3002", "conclusion": "\u8be5\u7814\u7a76\u7684\u7ed3\u679c\u4e3abandit\u95ee\u9898\u4e2d\u9690\u79c1\u4fdd\u62a4\u51b3\u7b56\u6846\u67b6\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.23554", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23554", "abs": "https://arxiv.org/abs/2507.23554", "authors": ["Ruoyu Wang", "Junda Wu", "Yu Xia", "Tong Yu", "Ryan A. Rossi", "Julian McAuley", "Lina Yao"], "title": "DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient Knowledge Transfer", "comment": null, "summary": "Large language model-based agents, empowered by in-context learning (ICL),\nhave demonstrated strong capabilities in complex reasoning and tool-use tasks.\nHowever, existing works have shown that the effectiveness of ICL is highly\nsensitive to the choice of demonstrations, with suboptimal examples often\nleading to unstable or degraded performance. While prior work has explored\nexample selection, including in some agentic or multi-step settings, existing\napproaches typically rely on heuristics or task-specific designs and lack a\ngeneral, theoretically grounded criterion for what constitutes an effective\ndemonstration across reasoning steps. Therefore, it is non-trivial to develop a\nprincipled, general-purpose method for selecting demonstrations that\nconsistently benefit agent performance. In this paper, we address this\nchallenge with DICE, Dynamic In-Context Example Selection for LLM Agents, a\ntheoretically grounded ICL framework for agentic tasks that selects the most\nrelevant demonstrations at each step of reasoning. Our approach decomposes\ndemonstration knowledge into transferable and non-transferable components\nthrough a causal lens, showing how the latter can introduce spurious\ndependencies that impair generalization. We further propose a stepwise\nselection criterion with a formal guarantee of improved agent performance.\nImportantly, DICE is a general, framework-agnostic solution that can be\nintegrated as a plug-in module into existing agentic frameworks without any\nadditional training cost. Extensive experiments across diverse domains\ndemonstrate our method's effectiveness and generality, highlighting the\nimportance of principled, context-aware demo selection for robust and efficient\nLLM agents.", "AI": {"tldr": "DICE selects relevant demonstrations at each reasoning step for LLM agents, improving performance without extra training.", "motivation": "The effectiveness of ICL is highly sensitive to the choice of demonstrations, with suboptimal examples often leading to unstable or degraded performance. Existing approaches typically rely on heuristics or task-specific designs and lack a general, theoretically grounded criterion for what constitutes an effective demonstration across reasoning steps.", "method": "DICE, Dynamic In-Context Example Selection for LLM Agents, a theoretically grounded ICL framework for agentic tasks that selects the most relevant demonstrations at each step of reasoning. Decomposes demonstration knowledge into transferable and non-transferable components through a causal lens, and proposes a stepwise selection criterion with a formal guarantee of improved agent performance.", "result": "DICE is a general, framework-agnostic solution that can be integrated as a plug-in module into existing agentic frameworks without any additional training cost. Extensive experiments across diverse domains demonstrate our method's effectiveness and generality.", "conclusion": "DICE is effective and general, highlighting the importance of principled, context-aware demo selection for robust and efficient LLM agents."}}
{"id": "2507.23664", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.23664", "abs": "https://arxiv.org/abs/2507.23664", "authors": ["Haipeng Liu", "Yuxuan Liu", "Ting Long"], "title": "Personalized Education with Ranking Alignment Recommendation", "comment": null, "summary": "Personalized question recommendation aims to guide individual students\nthrough questions to enhance their mastery of learning targets. Most previous\nmethods model this task as a Markov Decision Process and use reinforcement\nlearning to solve, but they struggle with efficient exploration, failing to\nidentify the best questions for each student during training. To address this,\nwe propose Ranking Alignment Recommendation (RAR), which incorporates\ncollaborative ideas into the exploration mechanism, enabling more efficient\nexploration within limited training episodes. Experiments show that RAR\neffectively improves recommendation performance, and our framework can be\napplied to any RL-based question recommender. Our code is available in\nhttps://github.com/wuming29/RAR.git.", "AI": {"tldr": "Personalized question recommendation struggle with efficient exploration, failing to identify the best questions. We propose Ranking Alignment Recommendation (RAR) to solve this.", "motivation": "Most previous methods model personalized question recommendation as a Markov Decision Process and use reinforcement learning to solve, but they struggle with efficient exploration, failing to identify the best questions for each student during training.", "method": "Ranking Alignment Recommendation (RAR), which incorporates collaborative ideas into the exploration mechanism", "result": "Experiments show that RAR effectively improves recommendation performance", "conclusion": "RAR can be applied to any RL-based question recommender and  effectively improves recommendation performance."}}
{"id": "2507.22923", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22923", "abs": "https://arxiv.org/abs/2507.22923", "authors": ["Aman Gupta", "Yingying Zhuang", "Zhou Yu", "Ziji Zhang", "Anurag Beniwal"], "title": "How and Where to Translate? The Impact of Translation Strategies in Cross-lingual LLM Prompting", "comment": "Accepted at Prompt Optimization KDD '25", "summary": "Despite advances in the multilingual capabilities of Large Language Models\n(LLMs), their performance varies substantially across different languages and\ntasks. In multilingual retrieval-augmented generation (RAG)-based systems,\nknowledge bases (KB) are often shared from high-resource languages (such as\nEnglish) to low-resource ones, resulting in retrieved information from the KB\nbeing in a different language than the rest of the context. In such scenarios,\ntwo common practices are pre-translation to create a mono-lingual prompt and\ncross-lingual prompting for direct inference. However, the impact of these\nchoices remains unclear. In this paper, we systematically evaluate the impact\nof different prompt translation strategies for classification tasks with\nRAG-enhanced LLMs in multilingual systems. Experimental results show that an\noptimized prompting strategy can significantly improve knowledge sharing across\nlanguages, therefore improve the performance on the downstream classification\ntask. The findings advocate for a broader utilization of multilingual resource\nsharing and cross-lingual prompt optimization for non-English languages,\nespecially the low-resource ones.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u8bed\u8a00RAG\u7cfb\u7edf\u4e2dprompt\u7ffb\u8bd1\u7b56\u7565\u5bf9\u5206\u7c7b\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4f18\u5316\u7684prompt\u7b56\u7565\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u77e5\u8bc6\u5171\u4eab\u548c\u4efb\u52a1\u6027\u80fd\uff0c\u5c24\u5176\u5bf9\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u7684\u591a\u8bed\u8a00\u80fd\u529b\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5b83\u4eec\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\u5f88\u5927\u3002\u5728\u591a\u8bed\u8a00\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7cfb\u7edf\u4e2d\uff0c\u77e5\u8bc6\u5e93(KB)\u901a\u5e38\u4ece\u9ad8\u8d44\u6e90\u8bed\u8a00(\u5982\u82f1\u8bed)\u5171\u4eab\u5230\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u5bfc\u81f4\u4eceKB\u68c0\u7d22\u5230\u7684\u4fe1\u606f\u4e0e\u4e0a\u4e0b\u6587\u7684\u5176\u4f59\u90e8\u5206\u4f7f\u7528\u4e0d\u540c\u7684\u8bed\u8a00\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u4e24\u79cd\u5e38\u89c1\u7684\u505a\u6cd5\u662f\u9884\u7ffb\u8bd1\u4ee5\u521b\u5efa\u5355\u8bedprompt\u548c\u8de8\u8bed\u8a00prompt\u4ee5\u8fdb\u884c\u76f4\u63a5\u63a8\u65ad\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u9009\u62e9\u7684\u5f71\u54cd\u4ecd\u4e0d\u6e05\u695a\u3002", "method": "\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u4e0d\u540cprompt\u7ffb\u8bd1\u7b56\u7565\u5bf9\u591a\u8bed\u8a00\u7cfb\u7edf\u4e2dRAG\u589e\u5f3a\u7684LLM\u5206\u7c7b\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f18\u5316\u7684prompt\u7b56\u7565\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u8de8\u8bed\u8a00\u7684\u77e5\u8bc6\u5171\u4eab\uff0c\u4ece\u800c\u63d0\u9ad8\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "\u4f18\u5316\u7684prompt\u7b56\u7565\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u8de8\u8bed\u8a00\u7684\u77e5\u8bc6\u5171\u4eab\uff0c\u4ece\u800c\u63d0\u9ad8\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\u3002\u7814\u7a76\u7ed3\u679c\u63d0\u5021\u66f4\u5e7f\u6cdb\u5730\u5229\u7528\u591a\u8bed\u8a00\u8d44\u6e90\u5171\u4eab\u548c\u975e\u82f1\u8bed\u8bed\u8a00\u7684\u8de8\u8bed\u8a00prompt\u4f18\u5316\uff0c\u7279\u522b\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00\u3002"}}
{"id": "2507.23150", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23150", "abs": "https://arxiv.org/abs/2507.23150", "authors": ["Philip Wootaek Shin", "Vishal Gaur", "Rahul Ramachandran", "Manil Maskey", "Jack Sampson", "Vijaykrishnan Narayanan", "Sujit Roy"], "title": "Towards High-Resolution Alignment and Super-Resolution of Multi-Sensor Satellite Imagery", "comment": null, "summary": "High-resolution satellite imagery is essential for geospatial analysis, yet\ndifferences in spatial resolution across satellite sensors present challenges\nfor data fusion and downstream applications. Super-resolution techniques can\nhelp bridge this gap, but existing methods rely on artificially downscaled\nimages rather than real sensor data and are not well suited for heterogeneous\nsatellite sensors with differing spectral, temporal characteristics. In this\nwork, we develop a preliminary framework to align and Harmonized Landsat\nSentinel 30m(HLS 30) imagery using Harmonized Landsat Sentinel 10m(HLS10) as a\nreference from the HLS dataset. Our approach aims to bridge the resolution gap\nbetween these sensors and improve the quality of super-resolved Landsat\nimagery. Quantitative and qualitative evaluations demonstrate the effectiveness\nof our method, showing its potential for enhancing satellite-based sensing\napplications. This study provides insights into the feasibility of\nheterogeneous satellite image super-resolution and highlights key\nconsiderations for future advancements in the field.", "AI": {"tldr": "align and Harmonized Landsat Sentinel 30m(HLS 30) imagery using Harmonized Landsat Sentinel 10m(HLS10) as a reference to improve the quality of super-resolved Landsat imagery.", "motivation": "differences in spatial resolution across satellite sensors present challenges for data fusion and downstream applications. Super-resolution techniques can help bridge this gap, but existing methods rely on artificially downscaled images rather than real sensor data and are not well suited for heterogeneous satellite sensors with differing spectral, temporal characteristics.", "method": "develop a preliminary framework to align and Harmonized Landsat Sentinel 30m(HLS 30) imagery using Harmonized Landsat Sentinel 10m(HLS10) as a reference from the HLS dataset", "result": "Quantitative and qualitative evaluations demonstrate the effectiveness of our method, showing its potential for enhancing satellite-based sensing applications.", "conclusion": "This study provides insights into the feasibility of heterogeneous satellite image super-resolution and highlights key considerations for future advancements in the field."}}
{"id": "2507.23077", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2507.23077", "abs": "https://arxiv.org/abs/2507.23077", "authors": ["Agnese Marcato", "Aleksandra Pachalieva", "Ryley G. Hill", "Kai Gao", "Xiaoyu Wang", "Esteban Rougier", "Zhou Lei", "Vinamra Agrawal", "Janel Chua", "Qinjun Kang", "Jeffrey D. Hyman", "Abigail Hunter", "Nathan DeBardeleben", "Earl Lawrence", "Hari Viswanathan", "Daniel O'Malley", "Javier E. Santos"], "title": "A Foundation Model for Material Fracture Prediction", "comment": null, "summary": "Accurately predicting when and how materials fail is critical to designing\nsafe, reliable structures, mechanical systems, and engineered components that\noperate under stress. Yet, fracture behavior remains difficult to model across\nthe diversity of materials, geometries, and loading conditions in real-world\napplications. While machine learning (ML) methods show promise, most models are\ntrained on narrow datasets, lack robustness, and struggle to generalize.\nMeanwhile, physics-based simulators offer high-fidelity predictions but are\nfragmented across specialized methods and require substantial high-performance\ncomputing resources to explore the input space. To address these limitations,\nwe present a data-driven foundation model for fracture prediction, a\ntransformer-based architecture that operates across simulators, a wide range of\nmaterials (including plastic-bonded explosives, steel, aluminum, shale, and\ntungsten), and diverse loading conditions. The model supports both structured\nand unstructured meshes, combining them with large language model embeddings of\ntextual input decks specifying material properties, boundary conditions, and\nsolver settings. This multimodal input design enables flexible adaptation\nacross simulation scenarios without changes to the model architecture. The\ntrained model can be fine-tuned with minimal data on diverse downstream tasks,\nincluding time-to-failure estimation, modeling fracture evolution, and adapting\nto combined finite-discrete element method simulations. It also generalizes to\nunseen materials such as titanium and concrete, requiring as few as a single\nsample, dramatically reducing data needs compared to standard ML. Our results\nshow that fracture prediction can be unified under a single model architecture,\noffering a scalable, extensible alternative to simulator-specific workflows.", "AI": {"tldr": "This paper introduces a data-driven foundation model for fracture prediction that generalizes across materials and simulation scenarios, offering a scalable alternative to simulator-specific workflows.", "motivation": "Accurately predicting when and how materials fail is critical to designing safe, reliable structures, mechanical systems, and engineered components that operate under stress. Yet, fracture behavior remains difficult to model across the diversity of materials, geometries, and loading conditions in real-world applications. While machine learning (ML) methods show promise, most models are trained on narrow datasets, lack robustness, and struggle to generalize. Meanwhile, physics-based simulators offer high-fidelity predictions but are fragmented across specialized methods and require substantial high-performance computing resources to explore the input space", "method": "a data-driven foundation model for fracture prediction, a transformer-based architecture that operates across simulators, a wide range of materials and diverse loading conditions. The model supports both structured and unstructured meshes, combining them with large language model embeddings of textual input decks specifying material properties, boundary conditions, and solver settings", "result": "The trained model can be fine-tuned with minimal data on diverse downstream tasks, including time-to-failure estimation, modeling fracture evolution, and adapting to combined finite-discrete element method simulations. It also generalizes to unseen materials such as titanium and concrete, requiring as few as a single sample, dramatically reducing data needs compared to standard ML", "conclusion": "fracture prediction can be unified under a single model architecture, offering a scalable, extensible alternative to simulator-specific workflows"}}
{"id": "2507.23565", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23565", "abs": "https://arxiv.org/abs/2507.23565", "authors": ["Botao Zhu", "Xianbin Wang", "Dusit Niyato"], "title": "Semantic Chain-of-Trust: Autonomous Trust Orchestration for Collaborator Selection via Hypergraph-Aided Agentic AI", "comment": null, "summary": "In collaborative systems, the effective completion of tasks hinges on\ntask-specific trust evaluations of potential devices for distributed\ncollaboration. However, the complexity of tasks, the spatiotemporal dynamism of\ndistributed device resources, and the inevitable assessment overhead\ndramatically increase the complexity and resource consumption of the trust\nevaluation process. As a result, ill-timed or overly frequent trust evaluations\ncan reduce utilization rate of constrained resources, negatively affecting\ncollaborative task execution. To address this challenge, this paper proposes an\nautonomous trust orchestration method based on a new concept of semantic\nchain-of-trust. Our technique employs agentic AI and hypergraph to establish\nand maintain trust relationships among devices. By leveraging its strengths in\nautonomous perception, task decomposition, and semantic reasoning, we propose\nagentic AI to perceive device states and autonomously perform trust evaluations\nof collaborators based on historical performance data only during device idle\nperiods, thereby enabling efficient utilization of distributed resources. In\naddition, agentic AI performs task-specific trust evaluations on collaborator\nresources by analyzing the alignment between resource capabilities and task\nrequirements. Moreover, by maintaining a trust hypergraph embedded with trust\nsemantics for each device, agentic AI enables hierarchical management of\ncollaborators and identifies collaborators requiring trust evaluation based on\ntrust semantics, thereby achieving a balance between overhead and trust\naccuracy. Furthermore, local trust hypergraphs from multiple devices can be\nchained together to support multi-hop collaboration, enabling efficient\ncoordination in large-scale systems. Experimental results demonstrate that the\nproposed method achieves resource-efficient trust evaluation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406AI\u548c\u8d85\u56fe\u7684\u81ea\u4e3b\u4fe1\u4efb\u7f16\u6392\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u8d44\u6e90\u9ad8\u6548\u7684\u4fe1\u4efb\u8bc4\u4f30\uff0c\u4ece\u800c\u89e3\u51b3\u534f\u4f5c\u7cfb\u7edf\u4e2d\u4fe1\u4efb\u8bc4\u4f30\u7684\u590d\u6742\u6027\u548c\u8d44\u6e90\u6d88\u8017\u95ee\u9898\u3002", "motivation": "\u5728\u534f\u4f5c\u7cfb\u7edf\u4e2d\uff0c\u4efb\u52a1\u7684\u6709\u6548\u5b8c\u6210\u53d6\u51b3\u4e8e\u5bf9\u6f5c\u5728\u8bbe\u5907\u8fdb\u884c\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u4fe1\u4efb\u8bc4\u4f30\u4ee5\u8fdb\u884c\u5206\u5e03\u5f0f\u534f\u4f5c\u3002\u7136\u800c\uff0c\u4efb\u52a1\u7684\u590d\u6742\u6027\u3001\u5206\u5e03\u5f0f\u8bbe\u5907\u8d44\u6e90\u7684\u65f6\u7a7a\u52a8\u6001\u6027\u4ee5\u53ca\u4e0d\u53ef\u907f\u514d\u7684\u8bc4\u4f30\u5f00\u9500\uff0c\u90fd\u6781\u5927\u5730\u589e\u52a0\u4e86\u4fe1\u4efb\u8bc4\u4f30\u8fc7\u7a0b\u7684\u590d\u6742\u6027\u548c\u8d44\u6e90\u6d88\u8017\u3002\u56e0\u6b64\uff0c\u4e0d\u5408\u65f6\u5b9c\u6216\u8fc7\u4e8e\u9891\u7e41\u7684\u4fe1\u4efb\u8bc4\u4f30\u4f1a\u964d\u4f4e\u53d7\u9650\u8d44\u6e90\u7684\u5229\u7528\u7387\uff0c\u4ece\u800c\u5bf9\u534f\u4f5c\u4efb\u52a1\u7684\u6267\u884c\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u4fe1\u4efb\u94fe\u7684\u65b0\u6982\u5ff5\u7684\u81ea\u4e3b\u4fe1\u4efb\u7f16\u6392\u65b9\u6cd5\u3002\u8be5\u6280\u672f\u91c7\u7528\u4ee3\u7406AI\u548c\u8d85\u56fe\u6765\u5efa\u7acb\u548c\u7ef4\u62a4\u8bbe\u5907\u4e4b\u95f4\u7684\u4fe1\u4efb\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8d44\u6e90\u9ad8\u6548\u7684\u4fe1\u4efb\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8d44\u6e90\u9ad8\u6548\u7684\u4fe1\u4efb\u8bc4\u4f30\u3002"}}
{"id": "2507.22924", "categories": ["cs.CL", "I.2.7; K.3.1"], "pdf": "https://arxiv.org/pdf/2507.22924", "abs": "https://arxiv.org/abs/2507.22924", "authors": ["Brittney Exline", "Melanie Duffin", "Brittany Harbison", "Chrissa da Gomez", "David Joyner"], "title": "Using Sentiment Analysis to Investigate Peer Feedback by Native and Non-Native English Speakers", "comment": null, "summary": "Graduate-level CS programs in the U.S. increasingly enroll international\nstudents, with 60.2 percent of master's degrees in 2023 awarded to non-U.S.\nstudents. Many of these students take online courses, where peer feedback is\nused to engage students and improve pedagogy in a scalable manner. Since these\ncourses are conducted in English, many students study in a language other than\ntheir first. This paper examines how native versus non-native English speaker\nstatus affects three metrics of peer feedback experience in online U.S.-based\ncomputing courses. Using the Twitter-roBERTa-based model, we analyze the\nsentiment of peer reviews written by and to a random sample of 500 students. We\nthen relate sentiment scores and peer feedback ratings to students' language\nbackground. Results show that native English speakers rate feedback less\nfavorably, while non-native speakers write more positively but receive less\npositive sentiment in return. When controlling for sex and age, significant\ninteractions emerge, suggesting that language background plays a modest but\ncomplex role in shaping peer feedback experiences.", "AI": {"tldr": "This paper analyzes how native vs. non-native English speaker status affects peer feedback in online U.S. computing courses. Results show that native English speakers rate feedback less favorably, while non-native speakers write more positively but receive less positive sentiment in return.", "motivation": "Many international students in U.S. graduate-level CS programs take online courses where peer feedback is used. Since these courses are conducted in English, many students study in a language other than their first. This paper examines how native versus non-native English speaker status affects peer feedback experience in online U.S.-based computing courses.", "method": "Using the Twitter-roBERTa-based model, we analyze the sentiment of peer reviews written by and to a random sample of 500 students. We then relate sentiment scores and peer feedback ratings to students' language background.", "result": "native English speakers rate feedback less favorably, while non-native speakers write more positively but receive less positive sentiment in return.", "conclusion": "native English speakers rate feedback less favorably, while non-native speakers write more positively but receive less positive sentiment in return. When controlling for sex and age, significant interactions emerge, suggesting that language background plays a modest but complex role in shaping peer feedback experiences."}}
{"id": "2507.23162", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23162", "abs": "https://arxiv.org/abs/2507.23162", "authors": ["Xu Cao", "Takafumi Taketomi"], "title": "Neural Multi-View Self-Calibrated Photometric Stereo without Photometric Stereo Cues", "comment": "Accepted to ICCV 2025", "summary": "We propose a neural inverse rendering approach that jointly reconstructs\ngeometry, spatially varying reflectance, and lighting conditions from\nmulti-view images captured under varying directional lighting. Unlike prior\nmulti-view photometric stereo methods that require light calibration or\nintermediate cues such as per-view normal maps, our method jointly optimizes\nall scene parameters from raw images in a single stage. We represent both\ngeometry and reflectance as neural implicit fields and apply shadow-aware\nvolume rendering. A spatial network first predicts the signed distance and a\nreflectance latent code for each scene point. A reflectance network then\nestimates reflectance values conditioned on the latent code and angularly\nencoded surface normal, view, and light directions. The proposed method\noutperforms state-of-the-art normal-guided approaches in shape and lighting\nestimation accuracy, generalizes to view-unaligned multi-light images, and\nhandles objects with challenging geometry and reflectance.", "AI": {"tldr": "propose a neural inverse rendering approach that jointly reconstructs geometry, spatially varying reflectance, and lighting conditions from multi-view images captured under varying directional lighting", "motivation": "Unlike prior multi-view photometric stereo methods that require light calibration or intermediate cues such as per-view normal maps, our method jointly optimizes all scene parameters from raw images in a single stage.", "method": "A spatial network first predicts the signed distance and a reflectance latent code for each scene point. A reflectance network then estimates reflectance values conditioned on the latent code and angularly encoded surface normal, view, and light directions.", "result": "jointly reconstructs geometry, spatially varying reflectance, and lighting conditions from multi-view images captured under varying directional lighting", "conclusion": "The proposed method outperforms state-of-the-art normal-guided approaches in shape and lighting estimation accuracy, generalizes to view-unaligned multi-light images, and handles objects with challenging geometry and reflectance."}}
{"id": "2507.23093", "categories": ["cs.LG", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.23093", "abs": "https://arxiv.org/abs/2507.23093", "authors": ["Ghazal Sobhani", "Md. Monzurul Amin Ifath", "Tushar Sharma", "Israat Haque"], "title": "On the Sustainability of AI Inferences in the Edge", "comment": "14 pages, 8 figures, 6 tables, in preparation for journal submission", "summary": "The proliferation of the Internet of Things (IoT) and its cutting-edge\nAI-enabled applications (e.g., autonomous vehicles and smart industries)\ncombine two paradigms: data-driven systems and their deployment on the edge.\nUsually, edge devices perform inferences to support latency-critical\napplications. In addition to the performance of these resource-constrained edge\ndevices, their energy usage is a critical factor in adopting and deploying edge\napplications. Examples of such devices include Raspberry Pi (RPi), Intel Neural\nCompute Stick (INCS), NVIDIA Jetson nano (NJn), and Google Coral USB (GCU).\nDespite their adoption in edge deployment for AI inferences, there is no study\non their performance and energy usage for informed decision-making on the\ndevice and model selection to meet the demands of applications. This study\nfills the gap by rigorously characterizing the performance of traditional,\nneural networks, and large language models on the above-edge devices.\nSpecifically, we analyze trade-offs among model F1 score, inference time,\ninference power, and memory usage. Hardware and framework optimization, along\nwith external parameter tuning of AI models, can balance between model\nperformance and resource usage to realize practical edge AI deployments.", "AI": {"tldr": "study on the performance and energy usage of edge devices", "motivation": "no study on their performance and energy usage for informed decision-making on the device and model selection to meet the demands of applications", "method": "rigorously characterizing the performance of traditional, neural networks, and large language models on the above-edge devices", "result": "analyze trade-offs among model F1 score, inference time, inference power, and memory usage", "conclusion": "Hardware and framework optimization, along with external parameter tuning of AI models, can balance between model performance and resource usage to realize practical edge AI deployments."}}
{"id": "2507.23633", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23633", "abs": "https://arxiv.org/abs/2507.23633", "authors": ["Qian Zhao", "Zhuo Sun", "Bin Guo", "Zhiwen Yu"], "title": "MemoCue: Empowering LLM-Based Agents for Human Memory Recall via Strategy-Guided Querying", "comment": null, "summary": "Agent-assisted memory recall is one critical research problem in the field of\nhuman-computer interaction. In conventional methods, the agent can retrieve\ninformation from its equipped memory module to help the person recall\nincomplete or vague memories. The limited size of memory module hinders the\nacquisition of complete memories and impacts the memory recall performance in\npractice. Memory theories suggest that the person's relevant memory can be\nproactively activated through some effective cues. Inspired by this, we propose\na novel strategy-guided agent-assisted memory recall method, allowing the agent\nto transform an original query into a cue-rich one via the judiciously designed\nstrategy to help the person recall memories. To this end, there are two key\nchallenges. (1) How to choose the appropriate recall strategy for diverse\nforgetting scenarios with distinct memory-recall characteristics? (2) How to\nobtain the high-quality responses leveraging recall strategies, given only\nabstract and sparsely annotated strategy patterns? To address the challenges,\nwe propose a Recall Router framework. Specifically, we design a 5W Recall Map\nto classify memory queries into five typical scenarios and define fifteen\nrecall strategy patterns across the corresponding scenarios. We then propose a\nhierarchical recall tree combined with the Monte Carlo Tree Search algorithm to\noptimize the selection of strategy and the generation of strategy responses. We\nconstruct an instruction tuning dataset and fine-tune multiple open-source\nlarge language models (LLMs) to develop MemoCue, an agent that excels in\nproviding memory-inspired responses. Experiments on three representative\ndatasets show that MemoCue surpasses LLM-based methods by 17.74% in recall\ninspiration. Further human evaluation highlights its advantages in\nmemory-recall applications.", "AI": {"tldr": "This paper introduces MemoCue, a strategy-guided agent that enhances memory recall by transforming queries into cue-rich prompts, outperforming existing LLM-based methods.", "motivation": "The limited size of memory modules in conventional agent-assisted memory recall methods hinders the acquisition of complete memories. Memory theories suggest that proactive activation of relevant memory through effective cues can improve recall.", "method": "a Recall Router framework with a 5W Recall Map to classify memory queries and a hierarchical recall tree with Monte Carlo Tree Search to optimize strategy selection and response generation. Instruction tuning is used to fine-tune LLMs to develop MemoCue.", "result": "MemoCue surpasses LLM-based methods by 17.74% in recall inspiration.", "conclusion": "MemoCue surpasses LLM-based methods by 17.74% in recall inspiration. Further human evaluation highlights its advantages in memory-recall applications."}}
{"id": "2507.22925", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22925", "abs": "https://arxiv.org/abs/2507.22925", "authors": ["Haoran Sun", "Shaoning Zeng"], "title": "Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents", "comment": null, "summary": "Long-term memory is one of the key factors influencing the reasoning\ncapabilities of Large Language Model Agents (LLM Agents). Incorporating a\nmemory mechanism that effectively integrates past interactions can\nsignificantly enhance decision-making and contextual coherence of LLM Agents.\nWhile recent works have made progress in memory storage and retrieval, such as\nencoding memory into dense vectors for similarity-based search or organizing\nknowledge in the form of graph, these approaches often fall short in structured\nmemory organization and efficient retrieval. To address these limitations, we\npropose a Hierarchical Memory (H-MEM) architecture for LLM Agents that\norganizes and updates memory in a multi-level fashion based on the degree of\nsemantic abstraction. Each memory vector is embedded with a positional index\nencoding pointing to its semantically related sub-memories in the next layer.\nDuring the reasoning phase, an index-based routing mechanism enables efficient,\nlayer-by-layer retrieval without performing exhaustive similarity computations.\nWe evaluate our method on five task settings from the LoCoMo dataset.\nExperimental results show that our approach consistently outperforms five\nbaseline methods, demonstrating its effectiveness in long-term dialogue\nscenarios.", "AI": {"tldr": "This paper introduces a hierarchical memory architecture (H-MEM) for LLM Agents that improves long-term dialogue performance by organizing and efficiently retrieving memories in a multi-level fashion.", "motivation": "Long-term memory is one of the key factors influencing the reasoning capabilities of Large Language Model Agents (LLM Agents). Recent works often fall short in structured memory organization and efficient retrieval.", "method": "We propose a Hierarchical Memory (H-MEM) architecture for LLM Agents that organizes and updates memory in a multi-level fashion based on the degree of semantic abstraction. Each memory vector is embedded with a positional index encoding pointing to its semantically related sub-memories in the next layer. During the reasoning phase, an index-based routing mechanism enables efficient, layer-by-layer retrieval without performing exhaustive similarity computations.", "result": "Our approach consistently outperforms five baseline methods, demonstrating its effectiveness in long-term dialogue scenarios.", "conclusion": "The proposed Hierarchical Memory (H-MEM) architecture outperforms five baseline methods in long-term dialogue scenarios."}}
{"id": "2507.23174", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23174", "abs": "https://arxiv.org/abs/2507.23174", "authors": ["Beatriz D\u00edaz Pe\u00f3n", "Jorge Torres G\u00f3mez", "Ariel Fajardo M\u00e1rquez"], "title": "CNN-based solution for mango classification in agricultural environments", "comment": null, "summary": "This article exemplifies the design of a fruit detection and classification\nsystem using Convolutional\n  Neural Networks (CNN). The goal is to develop a system that automatically\nassesses fruit quality for\n  farm inventory management. Specifically, a method for mango fruit\nclassification was developed using\n  image processing, ensuring both accuracy and efficiency. Resnet-18 was\nselected as the preliminary\n  architecture for classification, while a cascade detector was used for\ndetection, balancing execution speed\n  and computational resource consumption. Detection and classification results\nwere displayed through a\n  graphical interface developed in MatLab App Designer, streamlining system\ninteraction. The integration\n  of convolutional neural networks and cascade detectors proffers a reliable\nsolution for fruit classification\n  and detection, with potential applications in agricultural quality control.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u548c\u7ea7\u8054\u68c0\u6d4b\u5668\u8fdb\u884c\u6c34\u679c\u68c0\u6d4b\u548c\u5206\u7c7b\u7684\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u53ef\u81ea\u52a8\u8bc4\u4f30\u6c34\u679c\u8d28\u91cf\uff0c\u7528\u4e8e\u519c\u573a\u5e93\u5b58\u7ba1\u7406\u3002", "motivation": "\u672c\u6587\u5c55\u793a\u4e86\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc (CNN) \u7684\u6c34\u679c\u68c0\u6d4b\u548c\u5206\u7c7b\u7cfb\u7edf\u7684\u8bbe\u8ba1\u3002\u76ee\u6807\u662f\u5f00\u53d1\u4e00\u79cd\u81ea\u52a8\u8bc4\u4f30\u6c34\u679c\u8d28\u91cf\u4ee5\u8fdb\u884c\u519c\u573a\u5e93\u5b58\u7ba1\u7406\u7684\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528\u56fe\u50cf\u5904\u7406\u5f00\u53d1\u4e86\u4e00\u79cd\u8292\u679c\u6c34\u679c\u5206\u7c7b\u65b9\u6cd5\u3002Resnet-18\u88ab\u9009\u4e3a\u5206\u7c7b\u7684\u521d\u6b65\u67b6\u6784\uff0c\u800c\u7ea7\u8054\u68c0\u6d4b\u5668\u7528\u4e8e\u68c0\u6d4b\uff0c\u5e73\u8861\u4e86\u6267\u884c\u901f\u5ea6\u548c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u3002", "result": "\u68c0\u6d4b\u548c\u5206\u7c7b\u7ed3\u679c\u901a\u8fc7\u5728MatLab App Designer\u4e2d\u5f00\u53d1\u7684\u56fe\u5f62\u754c\u9762\u663e\u793a\uff0c\u4ece\u800c\u7b80\u5316\u4e86\u7cfb\u7edf\u4ea4\u4e92\u3002", "conclusion": "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u7ea7\u8054\u68c0\u6d4b\u5668\u7684\u96c6\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u7684\u6c34\u679c\u5206\u7c7b\u548c\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u519c\u4e1a\u8d28\u91cf\u63a7\u5236\u65b9\u9762\u5177\u6709\u6f5c\u5728\u7684\u5e94\u7528\u3002"}}
{"id": "2507.23111", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23111", "abs": "https://arxiv.org/abs/2507.23111", "authors": ["Richard Williams", "Eric Nalisnick", "Andrew Holbrook"], "title": "Scalable Generative Modeling of Weighted Graphs", "comment": "25 pages, 5 figures, included appendix. code at\n  https://github.com/rlwilliams34/BiGG-E", "summary": "Weighted graphs are ubiquitous throughout biology, chemistry, and the social\nsciences, motivating the development of generative models for abstract weighted\ngraph data using deep neural networks. However, most current deep generative\nmodels are either designed for unweighted graphs and are not easily extended to\nweighted topologies or incorporate edge weights without consideration of a\njoint distribution with topology. Furthermore, learning a distribution over\nweighted graphs must account for complex nonlocal dependencies between both the\nedges of the graph and corresponding weights of each edge. We develop an\nautoregressive model BiGG-E, a nontrivial extension of the BiGG model, that\nlearns a joint distribution over weighted graphs while still exploiting\nsparsity to generate a weighted graph with $n$ nodes and $m$ edges in $O((n +\nm)\\log n)$ time. Simulation studies and experiments on a variety of benchmark\ndatasets demonstrate that BiGG-E best captures distributions over weighted\ngraphs while remaining scalable and computationally efficient.", "AI": {"tldr": "BiGG-E, an autoregressive model, learns a joint distribution over weighted graphs, captures distributions, is scalable, and is computationally efficient.", "motivation": "motivating the development of generative models for abstract weighted graph data using deep neural networks. However, most current deep generative models are either designed for unweighted graphs and are not easily extended to weighted topologies or incorporate edge weights without consideration of a joint distribution with topology. Furthermore, learning a distribution over weighted graphs must account for complex nonlocal dependencies between both the edges of the graph and corresponding weights of each edge.", "method": "an autoregressive model BiGG-E, a nontrivial extension of the BiGG model, that learns a joint distribution over weighted graphs while still exploiting sparsity", "result": "Simulation studies and experiments on a variety of benchmark datasets demonstrate that BiGG-E best captures distributions over weighted graphs while remaining scalable and computationally efficient.", "conclusion": "BiGG-E captures distributions over weighted graphs while remaining scalable and computationally efficient."}}
{"id": "2507.23185", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.23185", "abs": "https://arxiv.org/abs/2507.23185", "authors": ["Jongwook Si", "Sungyoung Kim"], "title": "Single Image Rain Streak Removal Using Harris Corner Loss and R-CBAM Network", "comment": "21 pages", "summary": "The problem of single-image rain streak removal goes beyond simple noise\nsuppression, requiring the simultaneous preservation of fine structural details\nand overall visual quality. In this study, we propose a novel image restoration\nnetwork that effectively constrains the restoration process by introducing a\nCorner Loss, which prevents the loss of object boundaries and detailed texture\ninformation during restoration. Furthermore, we propose a Residual\nConvolutional Block Attention Module (R-CBAM) Block into the encoder and\ndecoder to dynamically adjust the importance of features in both spatial and\nchannel dimensions, enabling the network to focus more effectively on regions\nheavily affected by rain streaks. Quantitative evaluations conducted on the\nRain100L and Rain100H datasets demonstrate that the proposed method\nsignificantly outperforms previous approaches, achieving a PSNR of 33.29 dB on\nRain100L and 26.16 dB on Rain100H.", "AI": {"tldr": "This paper introduces a new image restoration network with Corner Loss and R-CBAM to remove rain streaks while preserving details, outperforming previous methods on benchmark datasets.", "motivation": "The problem of single-image rain streak removal requires the simultaneous preservation of fine structural details and overall visual quality.", "method": "a novel image restoration network with a Corner Loss and a Residual Convolutional Block Attention Module (R-CBAM) Block", "result": "achieving a PSNR of 33.29 dB on Rain100L and 26.16 dB on Rain100H", "conclusion": "The proposed method significantly outperforms previous approaches on Rain100L and Rain100H datasets."}}
{"id": "2507.23115", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23115", "abs": "https://arxiv.org/abs/2507.23115", "authors": ["David J Goetze", "Dahlia J Felten", "Jeannie R Albrecht", "Rohit Bhattacharya"], "title": "FLOSS: Federated Learning with Opt-Out and Straggler Support", "comment": "5 pages", "summary": "Previous work on data privacy in federated learning systems focuses on\nprivacy-preserving operations for data from users who have agreed to share\ntheir data for training. However, modern data privacy agreements also empower\nusers to use the system while opting out of sharing their data as desired. When\ncombined with stragglers that arise from heterogeneous device capabilities, the\nresult is missing data from a variety of sources that introduces bias and\ndegrades model performance. In this paper, we present FLOSS, a system that\nmitigates the impacts of such missing data on federated learning in the\npresence of stragglers and user opt-out, and empirically demonstrate its\nperformance in simulations.", "AI": {"tldr": "FLOSS mitigates the impacts of missing data on federated learning in the presence of stragglers and user opt-out", "motivation": "missing data from a variety of sources that introduces bias and degrades model performance.", "method": "a system", "result": "empirically demonstrate its performance in simulations.", "conclusion": "FLOSS mitigates the impacts of missing data on federated learning."}}
{"id": "2507.23701", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23701", "abs": "https://arxiv.org/abs/2507.23701", "authors": ["Long Phan", "Mantas Mazeika", "Andy Zou", "Dan Hendrycks"], "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "comment": null, "summary": "Evaluating AI agents within complex, interactive environments that mirror\nreal-world challenges is critical for understanding their practical\ncapabilities. While existing agent benchmarks effectively assess skills like\ntool use or performance on structured tasks, they often do not fully capture an\nagent's ability to operate autonomously in exploratory environments that demand\nsustained, self-directed reasoning over a long and growing context. To spur the\ndevelopment of agents capable of more robust intrinsic reasoning over long\nhorizons, we introduce TextQuests, a benchmark based on the Infocom suite of\ninteractive fiction games. These text-based adventures, which can take human\nplayers over 30 hours and require hundreds of precise actions to solve, serve\nas an effective proxy for evaluating AI agents on focused, stateful tasks. The\nbenchmark is specifically designed to assess an LLM agent's capacity for\nself-contained problem-solving by precluding the use of external tools, thereby\nfocusing on intrinsic long-context reasoning capabilities in an exploratory\nenvironment characterized by the need for trial-and-error learning and\nsustained problem-solving within a single interactive session. We release\nTextQuests at https://textquests.ai.", "AI": {"tldr": "introduce TextQuests, a benchmark based on the Infocom suite of interactive fiction games to assess an LLM agent's capacity for self-contained problem-solving by precluding the use of external tools, thereby focusing on intrinsic long-context reasoning capabilities in an exploratory environment characterized by the need for trial-and-error learning and sustained problem-solving within a single interactive session.", "motivation": "Evaluating AI agents within complex, interactive environments that mirror real-world challenges is critical for understanding their practical capabilities. While existing agent benchmarks effectively assess skills like tool use or performance on structured tasks, they often do not fully capture an agent's ability to operate autonomously in exploratory environments that demand sustained, self-directed reasoning over a long and growing context.", "method": "a benchmark based on the Infocom suite of interactive fiction games", "result": "These text-based adventures, which can take human players over 30 hours and require hundreds of precise actions to solve, serve as an effective proxy for evaluating AI agents on focused, stateful tasks.", "conclusion": "We release TextQuests at https://textquests.ai."}}
{"id": "2507.22927", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22927", "abs": "https://arxiv.org/abs/2507.22927", "authors": ["Zhehao Tan", "Yihan Jiao", "Dan Yang", "Lei Liu", "Jie Feng", "Duolin Sun", "Yue Shen", "Jian Wang", "Peng Wei", "Jinjie Gu"], "title": "PRGB Benchmark: A Robust Placeholder-Assisted Algorithm for Benchmarking Retrieval-Augmented Generation", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating external knowledge, where the LLM's ability to generate responses\nbased on the combination of a given query and retrieved documents is crucial.\nHowever, most benchmarks focus on overall RAG system performance, rarely\nassessing LLM-specific capabilities. Current benchmarks emphasize broad aspects\nsuch as noise robustness, but lack a systematic and granular evaluation\nframework on document utilization. To this end, we introduce\n\\textit{Placeholder-RAG-Benchmark}, a multi-level fine-grained benchmark,\nemphasizing the following progressive dimensions: (1) multi-level filtering\nabilities, (2) combination abilities, and (3) reference reasoning. To provide a\nmore nuanced understanding of LLMs' roles in RAG systems, we formulate an\ninnovative placeholder-based approach to decouple the contributions of the\nLLM's parametric knowledge and the external knowledge. Experiments demonstrate\nthe limitations of representative LLMs in the RAG system's generation\ncapabilities, particularly in error resilience and context faithfulness. Our\nbenchmark provides a reproducible framework for developing more reliable and\nefficient RAG systems. Our code is available in\nhttps://github.com/Alipay-Med/PRGB.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u591a\u5c42\u6b21\u7ec6\u7c92\u5ea6\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5f3a\u8c03\u591a\u5c42\u6b21\u8fc7\u6ee4\u80fd\u529b\u3001\u7ec4\u5408\u80fd\u529b\u548c\u53c2\u8003\u63a8\u7406\uff0c\u4ee5\u66f4\u7ec6\u81f4\u5730\u7406\u89e3LLM\u5728RAG\u7cfb\u7edf\u4e2d\u7684\u4f5c\u7528\u3002", "motivation": "\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u4fa7\u91cd\u4e8e\u566a\u58f0\u9c81\u68d2\u6027\u7b49\u5e7f\u6cdb\u65b9\u9762\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u6587\u6863\u5229\u7528\u7684\u7cfb\u7edf\u548c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5360\u4f4d\u7b26\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u4ee5\u89e3\u8026LLM\u7684\u53c2\u6570\u77e5\u8bc6\u548c\u5916\u90e8\u77e5\u8bc6\u7684\u8d21\u732e\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u4ee3\u8868\u6027LLM\u5728RAG\u7cfb\u7edf\u7684\u751f\u6210\u80fd\u529b\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u8bef\u5dee\u5f39\u6027\u548c\u4e0a\u4e0b\u6587\u5fe0\u5b9e\u6027\u65b9\u9762\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728RAG\u7cfb\u7edf\u7684\u751f\u6210\u80fd\u529b\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u8bef\u5dee\u5f39\u6027\u548c\u4e0a\u4e0b\u6587\u5fe0\u5b9e\u6027\u65b9\u9762\u3002"}}
{"id": "2507.23188", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23188", "abs": "https://arxiv.org/abs/2507.23188", "authors": ["Shiyao Yu", "Zi-An Wang", "Kangning Yin", "Zheng Tian", "Mingyuan Zhang", "Weixin Si", "Shihao Zou"], "title": "Multi-Modal Motion Retrieval by Learning a Fine-Grained Joint Embedding Space", "comment": "Accepted by IEEE TMM 2025", "summary": "Motion retrieval is crucial for motion acquisition, offering superior\nprecision, realism, controllability, and editability compared to motion\ngeneration. Existing approaches leverage contrastive learning to construct a\nunified embedding space for motion retrieval from text or visual modality.\nHowever, these methods lack a more intuitive and user-friendly interaction mode\nand often overlook the sequential representation of most modalities for\nimproved retrieval performance. To address these limitations, we propose a\nframework that aligns four modalities -- text, audio, video, and motion --\nwithin a fine-grained joint embedding space, incorporating audio for the first\ntime in motion retrieval to enhance user immersion and convenience. This\nfine-grained space is achieved through a sequence-level contrastive learning\napproach, which captures critical details across modalities for better\nalignment. To evaluate our framework, we augment existing text-motion datasets\nwith synthetic but diverse audio recordings, creating two multi-modal motion\nretrieval datasets. Experimental results demonstrate superior performance over\nstate-of-the-art methods across multiple sub-tasks, including an 10.16%\nimprovement in R@10 for text-to-motion retrieval and a 25.43% improvement in\nR@1 for video-to-motion retrieval on the HumanML3D dataset. Furthermore, our\nresults show that our 4-modal framework significantly outperforms its 3-modal\ncounterpart, underscoring the potential of multi-modal motion retrieval for\nadvancing motion acquisition.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56db\u6a21\u6001\u8fd0\u52a8\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u5e8f\u5217\u7ea7\u5bf9\u6bd4\u5b66\u4e60\u5728\u7ec6\u7c92\u5ea6\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u6587\u672c\u3001\u97f3\u9891\u3001\u89c6\u9891\u548c\u8fd0\u52a8\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u7684\u8fd0\u52a8\u68c0\u7d22\u65b9\u6cd5\u7f3a\u4e4f\u76f4\u89c2\u548c\u7528\u6237\u53cb\u597d\u7684\u4ea4\u4e92\u6a21\u5f0f\uff0c\u5e76\u4e14\u901a\u5e38\u5ffd\u7565\u4e86\u5927\u591a\u6570\u6a21\u6001\u7684\u987a\u5e8f\u8868\u793a\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7ec6\u7c92\u5ea6\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u6587\u672c\u3001\u97f3\u9891\u3001\u89c6\u9891\u548c\u8fd0\u52a8\u56db\u79cd\u6a21\u6001\u7684\u6846\u67b6\uff0c\u5e76\u91c7\u7528\u5e8f\u5217\u7ea7\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u5b50\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4f8b\u5982\u5728HumanML3D\u6570\u636e\u96c6\u4e0a\uff0c\u6587\u672c\u5230\u8fd0\u52a8\u68c0\u7d22\u7684R@10\u63d0\u9ad8\u4e8610.16%\uff0c\u89c6\u9891\u5230\u8fd0\u52a8\u68c0\u7d22\u7684R@1\u63d0\u9ad8\u4e8625.43%\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u56db\u6a21\u6001\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u4e09\u6a21\u6001\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u8fd0\u52a8\u68c0\u7d22\u5728\u63a8\u8fdb\u8fd0\u52a8\u83b7\u53d6\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.23128", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23128", "abs": "https://arxiv.org/abs/2507.23128", "authors": ["Ana\u00efs Baranger", "Lucas Maison"], "title": "Evaluating and Improving the Robustness of Speech Command Recognition Models to Noise and Distribution Shifts", "comment": "Submitted to ICASSP 2026", "summary": "Although prior work in computer vision has shown strong correlations between\nin-distribution (ID) and out-of-distribution (OOD) accuracies, such\nrelationships remain underexplored in audio-based models. In this study, we\ninvestigate how training conditions and input features affect the robustness\nand generalization abilities of spoken keyword classifiers under OOD\nconditions. We benchmark several neural architectures across a variety of\nevaluation sets. To quantify the impact of noise on generalization, we make use\nof two metrics: Fairness (F), which measures overall accuracy gains compared to\na baseline model, and Robustness (R), which assesses the convergence between ID\nand OOD performance. Our results suggest that noise-aware training improves\nrobustness in some configurations. These findings shed new light on the\nbenefits and limitations of noise-based augmentation for generalization in\nspeech models.", "AI": {"tldr": "This study investigates the impact of noise on the robustness and generalization of spoken keyword classifiers, finding that noise-aware training improves robustness in some configurations.", "motivation": "Prior work has shown correlations between in-distribution (ID) and out-of-distribution (OOD) accuracies in computer vision, but such relationships remain underexplored in audio-based models. The study investigates how training conditions and input features affect the robustness and generalization abilities of spoken keyword classifiers under OOD conditions.", "method": "Benchmark several neural architectures across a variety of evaluation sets and quantify the impact of noise on generalization using Fairness (F) and Robustness (R) metrics.", "result": "Noise-aware training improves robustness in some configurations.", "conclusion": "Noise-aware training improves robustness in some configurations, which sheds new light on the benefits and limitations of noise-based augmentation for generalization in speech models."}}
{"id": "2507.23726", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23726", "abs": "https://arxiv.org/abs/2507.23726", "authors": ["Luoxin Chen", "Jinming Gu", "Liankai Huang", "Wenhao Huang", "Zhicheng Jiang", "Allan Jie", "Xiaoran Jin", "Xing Jin", "Chenggang Li", "Kaijing Ma", "Cheng Ren", "Jiawei Shen", "Wenlei Shi", "Tong Sun", "He Sun", "Jiahui Wang", "Siran Wang", "Zhihong Wang", "Chenrui Wei", "Shufa Wei", "Yonghui Wu", "Yuchen Wu", "Yihang Xia", "Huajian Xin", "Fan Yang", "Huaiyuan Ying", "Hongyi Yuan", "Zheng Yuan", "Tianyang Zhan", "Chi Zhang", "Yue Zhang", "Ge Zhang", "Tianyun Zhao", "Jianqiu Zhao", "Yichi Zhou", "Thomas Hanwen Zhu"], "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "comment": null, "summary": "LLMs have demonstrated strong mathematical reasoning abilities by leveraging\nreinforcement learning with long chain-of-thought, yet they continue to\nstruggle with theorem proving due to the lack of clear supervision signals when\nsolely using natural language. Dedicated domain-specific languages like Lean\nprovide clear supervision via formal verification of proofs, enabling effective\ntraining through reinforcement learning. In this work, we propose\n\\textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover\ncan iteratively refine its proof based on Lean feedback, proved lemmas, and\nself-summarization. To solve IMO-level contest problems, we design three\ntest-time inference strategies that enable both deep and broad reasoning.\nSeed-Prover proves $78.1\\%$ of formalized past IMO problems, saturates MiniF2F,\nand achieves over 50\\% on PutnamBench, outperforming the previous\nstate-of-the-art by a large margin. To address the lack of geometry support in\nLean, we introduce a geometry reasoning engine \\textbf{Seed-Geometry}, which\noutperforms previous formal geometry engines. We use these two systems to\nparticipate in IMO 2025 and fully prove 5 out of 6 problems. This work\nrepresents a significant advancement in automated mathematical reasoning,\ndemonstrating the effectiveness of formal verification with long\nchain-of-thought reasoning.", "AI": {"tldr": "Proposes Seed-Prover, a lemma-style whole-proof reasoning model, and Seed-Geometry, a geometry reasoning engine, achieving state-of-the-art results in mathematical reasoning and proving 5/6 IMO 2025 problems.", "motivation": "LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought, yet they continue to struggle with theorem proving due to the lack of clear supervision signals when solely using natural language. Dedicated domain-specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning.", "method": "a lemma-style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design three test-time inference strategies that enable both deep and broad reasoning.", "result": "Seed-Prover proves $78.1\\%$ of formalized past IMO problems, saturates MiniF2F, and achieves over 50\\% on PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the lack of geometry support in Lean, we introduce a geometry reasoning engine Seed-Geometry, which outperforms previous formal geometry engines. We use these two systems to participate in IMO 2025 and fully prove 5 out of 6 problems.", "conclusion": "This work represents a significant advancement in automated mathematical reasoning, demonstrating the effectiveness of formal verification with long chain-of-thought reasoning."}}
{"id": "2507.22928", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22928", "abs": "https://arxiv.org/abs/2507.22928", "authors": ["Xi Chen", "Aske Plaat", "Niki van Stein"], "title": "How does Chain of Thought Think? Mechanistic Interpretability of Chain-of-Thought Reasoning with Sparse Autoencoding", "comment": null, "summary": "Chain-of-thought (CoT) prompting boosts Large Language Models accuracy on\nmulti-step tasks, yet whether the generated \"thoughts\" reflect the true\ninternal reasoning process is unresolved. We present the first feature-level\ncausal study of CoT faithfulness. Combining sparse autoencoders with activation\npatching, we extract monosemantic features from Pythia-70M and Pythia-2.8B\nwhile they tackle GSM8K math problems under CoT and plain (noCoT) prompting.\nSwapping a small set of CoT-reasoning features into a noCoT run raises answer\nlog-probabilities significantly in the 2.8B model, but has no reliable effect\nin 70M, revealing a clear scale threshold. CoT also leads to significantly\nhigher activation sparsity and feature interpretability scores in the larger\nmodel, signalling more modular internal computation. For example, the model's\nconfidence in generating correct answers improves from 1.2 to 4.3. We introduce\npatch-curves and random-feature patching baselines, showing that useful CoT\ninformation is not only present in the top-K patches but widely distributed.\nOverall, our results indicate that CoT can induce more interpretable internal\nstructures in high-capacity LLMs, validating its role as a structured prompting\nmethod.", "AI": {"tldr": "CoT\u63d0\u793a\u53ef\u4ee5\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6b65\u9aa4\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027", "motivation": "\u5c3d\u7ba1Chain-of-thought (CoT) \u63d0\u793a\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6b65\u9aa4\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u4f46\u751f\u6210\u7684\u201c\u60f3\u6cd5\u201d\u662f\u5426\u53cd\u6620\u4e86\u771f\u5b9e\u7684\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u7ed3\u5408\u7a00\u758f\u81ea\u52a8\u7f16\u7801\u5668\u4e0e\u6fc0\u6d3b\u8865\u4e01\uff0c\u4ecePythia-70M\u548cPythia-2.8B\u4e2d\u63d0\u53d6\u5355\u4e49\u7279\u5f81\uff0c\u540c\u65f6\u5b83\u4eec\u5728CoT\u548cplain (noCoT) \u63d0\u793a\u4e0b\u5904\u7406GSM8K\u6570\u5b66\u95ee\u9898\u3002", "result": "\u5c06\u4e00\u5c0f\u90e8\u5206CoT\u63a8\u7406\u7279\u5f81\u4ea4\u6362\u5230noCoT\u8fd0\u884c\u4e2d\uff0c\u663e\u8457\u63d0\u9ad8\u4e862.8B\u6a21\u578b\u4e2d\u7684\u7b54\u6848\u5bf9\u6570\u6982\u7387\uff0c\u4f46\u572870M\u4e2d\u6ca1\u6709\u53ef\u9760\u7684\u6548\u679c\uff0c\u63ed\u793a\u4e86\u6e05\u6670\u7684\u89c4\u6a21\u9608\u503c\u3002CoT\u8fd8\u5728\u66f4\u5927\u7684\u6a21\u578b\u4e2d\u5bfc\u81f4\u663e\u8457\u66f4\u9ad8\u7684\u6fc0\u6d3b\u7a00\u758f\u6027\u548c\u7279\u5f81\u53ef\u89e3\u91ca\u6027\u5206\u6570\uff0c\u8868\u660e\u66f4\u6a21\u5757\u5316\u7684\u5185\u90e8\u8ba1\u7b97\u3002\u4f8b\u5982\uff0c\u6a21\u578b\u751f\u6210\u6b63\u786e\u7b54\u6848\u7684\u7f6e\u4fe1\u5ea6\u4ece1.2\u63d0\u9ad8\u52304.3\u3002", "conclusion": "CoT\u53ef\u4ee5\u8bf1\u5bfc\u9ad8\u5bb9\u91cfLLM\u4e2d\u66f4\u6613\u4e8e\u89e3\u91ca\u7684\u5185\u90e8\u7ed3\u6784\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u7ed3\u6784\u5316\u63d0\u793a\u65b9\u6cd5\u7684\u89d2\u8272\u3002"}}
{"id": "2507.23193", "categories": ["cs.CV", "I.4.6; I.2.10; I.5.4"], "pdf": "https://arxiv.org/pdf/2507.23193", "abs": "https://arxiv.org/abs/2507.23193", "authors": ["Youngsun Jang", "Dongyoun Kim", "Chulwoo Pack", "Kwanghee Won"], "title": "A Novel Dataset for Flood Detection Robust to Seasonal Changes in Satellite Imagery", "comment": "8 pages, 2 figures. Presented at ACM RACS 2024 (Pompei, Italy, Nov\n  5-8, 2024)", "summary": "This study introduces a novel dataset for segmenting flooded areas in\nsatellite images. After reviewing 77 existing benchmarks utilizing satellite\nimagery, we identified a shortage of suitable datasets for this specific task.\nTo fill this gap, we collected satellite imagery of the 2019 Midwestern USA\nfloods from Planet Explorer by Planet Labs (Image \\c{opyright} 2024 Planet Labs\nPBC). The dataset consists of 10 satellite images per location, each containing\nboth flooded and non-flooded areas. We selected ten locations from each of the\nfive states: Iowa, Kansas, Montana, Nebraska, and South Dakota. The dataset\nensures uniform resolution and resizing during data processing. For evaluating\nsemantic segmentation performance, we tested state-of-the-art models in\ncomputer vision and remote sensing on our dataset. Additionally, we conducted\nan ablation study varying window sizes to capture temporal characteristics.\nOverall, the models demonstrated modest results, suggesting a requirement for\nfuture multimodal and temporal learning strategies. The dataset will be\npublicly available on\n<https://github.com/youngsunjang/SDSU_MidWest_Flood_2019>.", "AI": {"tldr": "\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u6d2a\u6c34\u533a\u57df\u536b\u661f\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u6548\u679c\u6709\u9650\uff0c\u672a\u6765\u9700\u8981\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u7684\u536b\u661f\u56fe\u50cf\u6570\u636e\u96c6\u4e2d\uff0c\u7f3a\u4e4f\u7528\u4e8e\u5206\u5272\u6d2a\u6c34\u533a\u57df\u7684\u5408\u9002\u6570\u636e\u96c6\u3002", "method": "\u6536\u96c6\u4e862019\u5e74\u7f8e\u56fd\u4e2d\u897f\u90e8\u6d2a\u6c34\u5730\u533a\u7684\u536b\u661f\u56fe\u50cf\uff0c\u5e76\u6d4b\u8bd5\u4e86\u6700\u5148\u8fdb\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u9065\u611f\u6a21\u578b\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u5305\u542b\u4e94\u4e2a\u5dde\uff08\u7231\u8377\u534e\u5dde\u3001\u582a\u8428\u65af\u5dde\u3001\u8499\u5927\u62ff\u5dde\u3001\u5185\u5e03\u62c9\u65af\u52a0\u5dde\u548c\u5357\u8fbe\u79d1\u4ed6\u5dde\uff09\u517150\u4e2a\u5730\u70b9\u7684\u6d2a\u6c34\u533a\u57df\u536b\u661f\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u73b0\u6709\u6a21\u578b\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "conclusion": "\u6a21\u578b\u8868\u73b0\u4e00\u822c\uff0c\u672a\u6765\u9700\u8981\u591a\u6a21\u6001\u548c\u65f6\u95f4\u5b66\u4e60\u7b56\u7565\u3002"}}
{"id": "2507.23136", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23136", "abs": "https://arxiv.org/abs/2507.23136", "authors": ["Erin George", "Deanna Needell", "Berk Ustun"], "title": "Observational Multiplicity", "comment": null, "summary": "Many prediction tasks can admit multiple models that can perform almost\nequally well. This phenomenon can can undermine interpretability and safety\nwhen competing models assign conflicting predictions to individuals. In this\nwork, we study how arbitrariness can arise in probabilistic classification\ntasks as a result of an effect that we call \\emph{observational multiplicity}.\nWe discuss how this effect arises in a broad class of practical applications\nwhere we learn a classifier to predict probabilities $p_i \\in [0,1]$ but are\ngiven a dataset of observations $y_i \\in \\{0,1\\}$. We propose to evaluate the\narbitrariness of individual probability predictions through the lens of\n\\emph{regret}. We introduce a measure of regret for probabilistic\nclassification tasks, which measures how the predictions of a model could\nchange as a result of different training labels change. We present a\ngeneral-purpose method to estimate the regret in a probabilistic classification\ntask. We use our measure to show that regret is higher for certain groups in\nthe dataset and discuss potential applications of regret. We demonstrate how\nestimating regret promote safety in real-world applications by abstention and\ndata collection.", "AI": {"tldr": "This paper studies the arbitrariness in probabilistic classification tasks and introduces a measure of regret to evaluate the predictions of a model could change as a result of different training labels change. Estimating regret can promote safety in real-world applications by abstention and data collection.", "motivation": "This paper studies how arbitrariness can arise in probabilistic classification tasks as a result of an effect that we call observational multiplicity, which can undermine interpretability and safety when competing models assign conflicting predictions to individuals.", "method": "The paper proposes to evaluate the arbitrariness of individual probability predictions through the lens of regret. A general-purpose method to estimate the regret in a probabilistic classification task is presented.", "result": "The paper uses the measure to show that regret is higher for certain groups in the dataset and discuss potential applications of regret.", "conclusion": "This paper introduces a measure of regret for probabilistic classification tasks, which measures how the predictions of a model could change as a result of different training labels change. The paper demonstrates how estimating regret promote safety in real-world applications by abstention and data collection."}}
{"id": "2507.23751", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23751", "abs": "https://arxiv.org/abs/2507.23751", "authors": ["Ping Yu", "Jack Lanchantin", "Tianlu Wang", "Weizhe Yuan", "Olga Golovneva", "Ilia Kulikov", "Sainbayar Sukhbaatar", "Jason Weston", "Jing Xu"], "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "comment": null, "summary": "We propose CoT-Self-Instruct, a synthetic data generation method that\ninstructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the\ngiven seed tasks, and then to generate a new synthetic prompt of similar\nquality and complexity for use in LLM training, followed by filtering for\nhigh-quality data with automatic metrics. In verifiable reasoning, our\nsynthetic data significantly outperforms existing training datasets, such as\ns1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For\nnon-verifiable instruction-following tasks, our method surpasses the\nperformance of human or standard self-instruct prompts on both AlpacaEval 2.0\nand Arena-Hard.", "AI": {"tldr": "CoT-Self-Instruct\u662f\u4e00\u79cd\u65b0\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u63d0\u5347LLM\u5728\u63a8\u7406\u548c\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u751f\u6210\u9ad8\u8d28\u91cf\u7684LLM\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u4f7f\u7528Chain-of-Thought (CoT) \u8ba9LLM\u5148\u8fdb\u884c\u63a8\u7406\u548c\u89c4\u5212\uff0c\u7136\u540e\u751f\u6210\u65b0\u7684\u5408\u6210\u63d0\u793a\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u6307\u6807\u8fc7\u6ee4\u9ad8\u8d28\u91cf\u6570\u636e\u3002", "result": "\u5728MATH500, AMC23, AIME24\u548cGPQA-Diamond\u7b49\u53ef\u9a8c\u8bc1\u63a8\u7406\u4efb\u52a1\u4ee5\u53caAlpacaEval 2.0\u548cArena-Hard\u7b49\u975e\u9a8c\u8bc1\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e0a\uff0cCoT-Self-Instruct\u65b9\u6cd5\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "CoT-Self-Instruct\u65b9\u6cd5\u5728\u53ef\u9a8c\u8bc1\u63a8\u7406\u548c\u975e\u9a8c\u8bc1\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e0a\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8bad\u7ec3\u6570\u636e\u96c6\u6216\u4eba\u5de5\u63d0\u793a\u3002"}}
{"id": "2507.22929", "categories": ["cs.CL", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.22929", "abs": "https://arxiv.org/abs/2507.22929", "authors": ["Xiaoyu Pan", "Yang Bai", "Ke Zou", "Yang Zhou", "Jun Zhou", "Huazhu Fu", "Yih-Chung Tham", "Yong Liu"], "title": "EH-Benchmark Ophthalmic Hallucination Benchmark and Agent-Driven Top-Down Traceable Reasoning Workflow", "comment": "9 figures, 5 tables. submit/6621751", "summary": "Medical Large Language Models (MLLMs) play a crucial role in ophthalmic\ndiagnosis, holding significant potential to address vision-threatening\ndiseases. However, their accuracy is constrained by hallucinations stemming\nfrom limited ophthalmic knowledge, insufficient visual localization and\nreasoning capabilities, and a scarcity of multimodal ophthalmic data, which\ncollectively impede precise lesion detection and disease diagnosis.\nFurthermore, existing medical benchmarks fail to effectively evaluate various\ntypes of hallucinations or provide actionable solutions to mitigate them. To\naddress the above challenges, we introduce EH-Benchmark, a novel ophthalmology\nbenchmark designed to evaluate hallucinations in MLLMs. We categorize MLLMs'\nhallucinations based on specific tasks and error types into two primary\nclasses: Visual Understanding and Logical Composition, each comprising multiple\nsubclasses. Given that MLLMs predominantly rely on language-based reasoning\nrather than visual processing, we propose an agent-centric, three-phase\nframework, including the Knowledge-Level Retrieval stage, the Task-Level Case\nStudies stage, and the Result-Level Validation stage. Experimental results show\nthat our multi-agent framework significantly mitigates both types of\nhallucinations, enhancing accuracy, interpretability, and reliability. Our\nproject is available at https://github.com/ppxy1/EH-Benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u773c\u79d1\u57fa\u51c6\u6765\u8bc4\u4f30\u533b\u5b66\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6765\u51cf\u8f7b\u8fd9\u4e9b\u5e7b\u89c9\u3002", "motivation": "\u533b\u5b66\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u773c\u79d1\u8bca\u65ad\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u4f46\u5b83\u4eec\u7684\u51c6\u786e\u6027\u53d7\u5230\u5e7b\u89c9\u7684\u9650\u5236\uff0c\u8fd9\u4e9b\u5e7b\u89c9\u6e90\u4e8e\u6709\u9650\u7684\u773c\u79d1\u77e5\u8bc6\u3001\u4e0d\u8db3\u7684\u89c6\u89c9\u5b9a\u4f4d\u548c\u63a8\u7406\u80fd\u529b\u4ee5\u53ca\u591a\u6a21\u6001\u773c\u79d1\u6570\u636e\u7684\u7a00\u7f3a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ee5\u667a\u80fd\u4f53\u4e3a\u4e2d\u5fc3\u7684\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u5305\u62ec\u77e5\u8bc6\u6c34\u5e73\u68c0\u7d22\u9636\u6bb5\u3001\u4efb\u52a1\u6c34\u5e73\u6848\u4f8b\u7814\u7a76\u9636\u6bb5\u548c\u7ed3\u679c\u6c34\u5e73\u9a8c\u8bc1\u9636\u6bb5\u3002", "result": "\u591a\u667a\u80fd\u4f53\u6846\u67b6\u663e\u8457\u51cf\u8f7b\u4e86\u4e24\u79cd\u7c7b\u578b\u7684\u5e7b\u89c9\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u6846\u67b6\u663e\u8457\u51cf\u8f7b\u4e86\u5e7b\u89c9\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.23202", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23202", "abs": "https://arxiv.org/abs/2507.23202", "authors": ["Chengwei Xia", "Fan Ma", "Ruijie Quan", "Kun Zhan", "Yi Yang"], "title": "Adversarial-Guided Diffusion for Multimodal LLM Attacks", "comment": null, "summary": "This paper addresses the challenge of generating adversarial image using a\ndiffusion model to deceive multimodal large language models (MLLMs) into\ngenerating the targeted responses, while avoiding significant distortion of the\nclean image. To address the above challenges, we propose an adversarial-guided\ndiffusion (AGD) approach for adversarial attack MLLMs. We introduce\nadversarial-guided noise to ensure attack efficacy. A key observation in our\ndesign is that, unlike most traditional adversarial attacks which embed\nhigh-frequency perturbations directly into the clean image, AGD injects target\nsemantics into the noise component of the reverse diffusion. Since the added\nnoise in a diffusion model spans the entire frequency spectrum, the adversarial\nsignal embedded within it also inherits this full-spectrum property.\nImportantly, during reverse diffusion, the adversarial image is formed as a\nlinear combination of the clean image and the noise. Thus, when applying\ndefenses such as a simple low-pass filtering, which act independently on each\ncomponent, the adversarial image within the noise component is less likely to\nbe suppressed, as it is not confined to the high-frequency band. This makes AGD\ninherently robust to variety defenses. Extensive experiments demonstrate that\nour AGD outperforms state-of-the-art methods in attack performance as well as\nin model robustness to some defenses.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6297\u5f15\u5bfc\u6269\u6563\uff08AGD\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5bf9\u6297\u653b\u51fb\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u8be5\u65b9\u6cd5\u5728\u653b\u51fb\u6027\u80fd\u548c\u6a21\u578b\u5bf9\u67d0\u4e9b\u9632\u5fa1\u7684\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u89e3\u51b3\u4ee5\u4e0b\u6311\u6218\uff1a\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u5bf9\u6297\u56fe\u50cf\uff0c\u4ee5\u6b3a\u9a97\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u751f\u6210\u76ee\u6807\u54cd\u5e94\uff0c\u540c\u65f6\u907f\u514d\u5bf9\u6e05\u6670\u56fe\u50cf\u4ea7\u751f\u663e\u7740\u5931\u771f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6297\u5f15\u5bfc\u6269\u6563\uff08AGD\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5bf9\u6297\u653b\u51fb\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "result": "AGD\u5728\u653b\u51fb\u6027\u80fd\u548c\u6a21\u578b\u5bf9\u67d0\u4e9b\u9632\u5fa1\u7684\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "AGD\u5728\u653b\u51fb\u6027\u80fd\u548c\u6a21\u578b\u5bf9\u67d0\u4e9b\u9632\u5fa1\u7684\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.23141", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.23141", "abs": "https://arxiv.org/abs/2507.23141", "authors": ["Xiangshu Gong", "Zhiqiang Xie", "Xiaowei Jin", "Chen Wang", "Yanling Qu", "Wangmeng Zuo", "Hui Li"], "title": "AI paradigm for solving differential equations: first-principles data generation and scale-dilation operator AI solver", "comment": null, "summary": "Many problems are governed by differential equations (DEs). Artificial\nintelligence (AI) is a new path for solving DEs. However, data is very scarce\nand existing AI solvers struggle with approximation of high frequency\ncomponents (AHFC). We propose an AI paradigm for solving diverse DEs, including\nDE-ruled first-principles data generation methodology and scale-dilation\noperator (SDO) AI solver. Using either prior knowledge or random fields, we\ngenerate solutions and then substitute them into the DEs to derive the sources\nand initial/boundary conditions through balancing DEs, thus producing\narbitrarily vast amount of, first-principles-consistent training datasets at\nextremely low computational cost. We introduce a reversible SDO that leverages\nthe Fourier transform of the multiscale solutions to fix AHFC, and design a\nspatiotemporally coupled, attention-based Transformer AI solver of DEs with\nSDO. An upper bound on the Hessian condition number of the loss function is\nproven to be proportional to the squared 2-norm of the solution gradient,\nrevealing that SDO yields a smoother loss landscape, consequently fixing AHFC\nwith efficient training. Extensive tests on diverse DEs demonstrate that our AI\nparadigm achieves consistently superior accuracy over state-of-the-art methods.\nThis work makes AI solver of DEs to be truly usable in broad nature and\nengineering fields.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684AI\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5fae\u5206\u65b9\u7a0b\uff0c\u8be5\u6846\u67b6\u5229\u7528\u7b2c\u4e00\u6027\u539f\u7406\u6570\u636e\u751f\u6210\u65b9\u6cd5\u548c\u5c3a\u5ea6\u6269\u5f20\u7b97\u5b50\uff08SDO\uff09AI\u6c42\u89e3\u5668\u6765\u63d0\u9ad8\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u8bb8\u591a\u95ee\u9898\u90fd\u53d7\u5fae\u5206\u65b9\u7a0b\uff08DE\uff09\u652f\u914d\u3002\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u662f\u89e3\u51b3DE\u95ee\u9898\u7684\u4e00\u6761\u65b0\u9014\u5f84\u3002\u7136\u800c\uff0c\u6570\u636e\u975e\u5e38\u7a00\u7f3a\uff0c\u73b0\u6709\u7684AI\u6c42\u89e3\u5668\u5728\u903c\u8fd1\u9ad8\u9891\u5206\u91cf\uff08AHFC\uff09\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u62ecDE\u7edf\u6cbb\u7684\u7b2c\u4e00\u6027\u539f\u7406\u6570\u636e\u751f\u6210\u65b9\u6cd5\u548c\u5c3a\u5ea6\u6269\u5f20\u7b97\u5b50\uff08SDO\uff09AI\u6c42\u89e3\u5668\u3002\u5229\u7528\u5148\u9a8c\u77e5\u8bc6\u6216\u968f\u673a\u573a\uff0c\u6211\u4eec\u751f\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u7136\u540e\u5c06\u5b83\u4eec\u4ee3\u5165DE\u4e2d\uff0c\u901a\u8fc7\u5e73\u8861DE\u6765\u63a8\u5bfc\u6e90\u548c\u521d\u59cb/\u8fb9\u754c\u6761\u4ef6\uff0c\u4ece\u800c\u4ee5\u6781\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u4ea7\u751f\u5927\u91cf\u7684\u3001\u4e0e\u7b2c\u4e00\u6027\u539f\u7406\u76f8\u4e00\u81f4\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u53ef\u9006\u7684SDO\uff0c\u5b83\u5229\u7528\u591a\u5c3a\u5ea6\u89e3\u7684\u5085\u91cc\u53f6\u53d8\u6362\u6765\u4fee\u6b63AHFC\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5177\u6709SDO\u7684\u65f6\u7a7a\u8026\u5408\u7684\u3001\u57fa\u4e8e\u6ce8\u610f\u529b\u7684Transformer AI DE\u6c42\u89e3\u5668\u3002", "result": "\u5728\u635f\u5931\u51fd\u6570\u7684Hessian\u6761\u4ef6\u6570\u7684\u4e0a\u9650\u88ab\u8bc1\u660e\u4e0e\u89e3\u68af\u5ea6\u5e73\u65b92-\u8303\u6570\u6210\u6b63\u6bd4\uff0c\u8868\u660eSDO\u4ea7\u751f\u4e86\u4e00\u4e2a\u66f4\u5e73\u6ed1\u7684\u635f\u5931\u9762\uff0c\u56e0\u6b64\u901a\u8fc7\u6709\u6548\u7684\u8bad\u7ec3\u4fee\u6b63\u4e86AHFC\u3002\u5728\u4e0d\u540c\u7684DE\u4e0a\u7684\u5e7f\u6cdb\u6d4b\u8bd5\u8868\u660e\uff0c\u6211\u4eec\u7684AI\u8303\u4f8b\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u59cb\u7ec8\u4f18\u8d8a\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684AI\u8303\u4f8b\u6765\u89e3\u51b3\u5404\u79cd\u5fae\u5206\u65b9\u7a0b\uff0c\u5e76\u5728\u4e0d\u540c\u7684\u5fae\u5206\u65b9\u7a0b\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u76ee\u524d\u6700\u597d\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5AI\u8303\u4f8b\u53d6\u5f97\u4e86\u4e00\u81f4\u7684\u5353\u8d8a\u7cbe\u5ea6\u3002\u8fd9\u9879\u5de5\u4f5c\u4f7f\u5fae\u5206\u65b9\u7a0b\u7684AI\u6c42\u89e3\u5668\u5728\u5e7f\u6cdb\u7684\u81ea\u7136\u548c\u5de5\u7a0b\u9886\u57df\u4e2d\u771f\u6b63\u53ef\u7528\u3002"}}
{"id": "2507.23773", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23773", "abs": "https://arxiv.org/abs/2507.23773", "authors": ["Mingkai Deng", "Jinyu Hou", "Yilin Shen", "Hongxia Jin", "Graham Neubig", "Zhiting Hu", "Eric Xing"], "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "comment": null, "summary": "AI agents built on large language models (LLMs) hold enormous promise, but\ncurrent practice focuses on a one-task-one-agent approach, which not only falls\nshort of scalability and generality, but also suffers from the fundamental\nlimitations of autoregressive LLMs. On the other hand, humans are general\nagents who reason by mentally simulating the outcomes of their actions and\nplans. Moving towards a more general and powerful AI agent, we introduce\nSimuRA, a goal-oriented architecture for generalized agentic reasoning. Based\non a principled formulation of optimal agent in any environment, \\modelname\novercomes the limitations of autoregressive reasoning by introducing a world\nmodel for planning via simulation. The generalized world model is implemented\nusing LLM, which can flexibly plan in a wide range of environments using the\nconcept-rich latent space of natural language. Experiments on difficult web\nbrowsing tasks show that \\modelname improves the success of flight search from\n0\\% to 32.2\\%. World-model-based planning, in particular, shows consistent\nadvantage of up to 124\\% over autoregressive planning, demonstrating the\nadvantage of world model simulation as a reasoning paradigm. We are excited\nabout the possibility for training a single, general agent model based on LLMs\nthat can act superintelligently in all environments. To start, we make SimuRA,\na web-browsing agent built on \\modelname with pretrained LLMs, available as a\nresearch demo for public testing.", "AI": {"tldr": "SimuRA\uff1a\u4e00\u79cd\u7528\u4e8e\u901a\u7528\u667a\u80fd\u4f53\u63a8\u7406\u7684\u9762\u5411\u76ee\u6807\u7684\u67b6\u6784\uff0c\u5b83\u4f7f\u7528LLM\u8fdb\u884c\u6a21\u62df\u89c4\u5212\uff0c\u5e76\u5728\u7f51\u9875\u6d4f\u89c8\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6539\u8fdb\u3002", "motivation": "\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6784\u5efa\u7684AI\u667a\u80fd\u4f53\u5177\u6709\u5de8\u5927\u7684\u524d\u666f\uff0c\u4f46\u76ee\u524d\u7684\u5b9e\u8df5\u4fa7\u91cd\u4e8e\u4e00\u4e2a\u4efb\u52a1\u4e00\u4e2a\u667a\u80fd\u4f53\u7684\u65b9\u6cd5\uff0c\u8fd9\u4e0d\u4ec5\u65e0\u6cd5\u6269\u5c55\u548c\u901a\u7528\uff0c\u800c\u4e14\u8fd8\u53d7\u5230\u81ea\u56de\u5f52LLM\u7684\u6839\u672c\u9650\u5236\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u4eba\u7c7b\u662f\u901a\u7528\u7684\u667a\u80fd\u4f53\uff0c\u4ed6\u4eec\u901a\u8fc7\u5728\u7cbe\u795e\u4e0a\u6a21\u62df\u4ed6\u4eec\u7684\u884c\u4e3a\u548c\u8ba1\u5212\u7684\u7ed3\u679c\u6765\u8fdb\u884c\u63a8\u7406\u3002\u4e3a\u4e86\u6784\u5efa\u4e00\u4e2a\u66f4\u901a\u7528\u548c\u5f3a\u5927\u7684AI\u667a\u80fd\u4f53\uff0c\u6211\u4eec\u5f15\u5165\u4e86SimuRA\uff0c\u4e00\u4e2a\u9762\u5411\u76ee\u6807\u7684\u901a\u7528\u667a\u80fd\u4f53\u63a8\u7406\u67b6\u6784\u3002", "method": "\u57fa\u4e8e\u5bf9\u4efb\u4f55\u73af\u5883\u4e2d\u6700\u4f73\u667a\u80fd\u4f53\u7684\u539f\u5219\u6027\u516c\u5f0f\uff0c\u901a\u8fc7\u5f15\u5165\u7528\u4e8e\u6a21\u62df\u89c4\u5212\u7684\u4e16\u754c\u6a21\u578b\uff0c\u514b\u670d\u4e86\u81ea\u56de\u5f52\u63a8\u7406\u7684\u5c40\u9650\u6027\u3002\u901a\u7528\u4e16\u754c\u6a21\u578b\u4f7f\u7528LLM\u5b9e\u73b0\uff0cLLM\u53ef\u4ee5\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u7684\u6982\u5ff5\u4e30\u5bcc\u7684\u6f5c\u5728\u7a7a\u95f4\u5728\u5e7f\u6cdb\u7684\u73af\u5883\u4e2d\u7075\u6d3b\u5730\u8fdb\u884c\u89c4\u5212\u3002", "result": "\u5728\u56f0\u96be\u7684\u7f51\u9875\u6d4f\u89c8\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\\[\u6a21\u578b\u540d\u79f0]\u5c06\u822a\u73ed\u641c\u7d22\u7684\u6210\u529f\u7387\u4ece0\uff05\u63d0\u9ad8\u523032.2\uff05\u3002\u7279\u522b\u662f\uff0c\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u89c4\u5212\u663e\u793a\u51fa\u6bd4\u81ea\u56de\u5f52\u89c4\u5212\u9ad8\u8fbe124\uff05\u7684\u4e00\u81f4\u4f18\u52bf\uff0c\u8bc1\u660e\u4e86\u4e16\u754c\u6a21\u578b\u6a21\u62df\u4f5c\u4e3a\u4e00\u79cd\u63a8\u7406\u8303\u4f8b\u7684\u4f18\u52bf\u3002", "conclusion": "\u6211\u4eec\u5bf9\u57fa\u4e8eLLM\u7684\u5355\u4e00\u901a\u7528\u667a\u80fd\u4f53\u6a21\u578b\u5728\u6240\u6709\u73af\u5883\u4e2d\u90fd\u80fd\u8d85\u667a\u80fd\u5730\u884c\u52a8\u7684\u53ef\u80fd\u6027\u611f\u5230\u5174\u594b\u3002\u6211\u4eec\u63d0\u4f9bSimuRA\uff0c\u4e00\u4e2a\u57fa\u4e8e\\[\u6a21\u578b\u540d\u79f0]\u548c\u9884\u8bad\u7ec3LLM\u7684\u7f51\u9875\u6d4f\u89c8\u667a\u80fd\u4f53\uff0c\u4f5c\u4e3a\u4e00\u4e2a\u7814\u7a76\u6f14\u793a\u4f9b\u516c\u4f17\u6d4b\u8bd5\u3002"}}
{"id": "2507.22930", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.22930", "abs": "https://arxiv.org/abs/2507.22930", "authors": ["Shalini Jangra", "Suparna De", "Nishanth Sastry", "Saeed Fadaei"], "title": "Protecting Vulnerable Voices: Synthetic Dataset Generation for Self-Disclosure Detection", "comment": "15 pages, 4 Figures, Accepted in \"The 17th International Conference\n  on Advances in Social Networks Analysis and Mining -ASONAM-2025\"", "summary": "Social platforms such as Reddit have a network of communities of shared\ninterests, with a prevalence of posts and comments from which one can infer\nusers' Personal Information Identifiers (PIIs). While such self-disclosures can\nlead to rewarding social interactions, they pose privacy risks and the threat\nof online harms. Research into the identification and retrieval of such risky\nself-disclosures of PIIs is hampered by the lack of open-source labeled\ndatasets. To foster reproducible research into PII-revealing text detection, we\ndevelop a novel methodology to create synthetic equivalents of PII-revealing\ndata that can be safely shared. Our contributions include creating a taxonomy\nof 19 PII-revealing categories for vulnerable populations and the creation and\nrelease of a synthetic PII-labeled multi-text span dataset generated from 3\ntext generation Large Language Models (LLMs), Llama2-7B, Llama3-8B, and\nzephyr-7b-beta, with sequential instruction prompting to resemble the original\nReddit posts. The utility of our methodology to generate this synthetic dataset\nis evaluated with three metrics: First, we require reproducibility equivalence,\ni.e., results from training a model on the synthetic data should be comparable\nto those obtained by training the same models on the original posts. Second, we\nrequire that the synthetic data be unlinkable to the original users, through\ncommon mechanisms such as Google Search. Third, we wish to ensure that the\nsynthetic data be indistinguishable from the original, i.e., trained humans\nshould not be able to tell them apart. We release our dataset and code at\nhttps://netsys.surrey.ac.uk/datasets/synthetic-self-disclosure/ to foster\nreproducible research into PII privacy risks in online social media.", "AI": {"tldr": "To foster reproducible research into PII-revealing text detection, we develop a novel methodology to create synthetic equivalents of PII-revealing data that can be safely shared.", "motivation": "Research into the identification and retrieval of such risky self-disclosures of PIIs is hampered by the lack of open-source labeled datasets.", "method": "develop a novel methodology to create synthetic equivalents of PII-revealing data that can be safely shared", "result": "creating a taxonomy of 19 PII-revealing categories for vulnerable populations and the creation and release of a synthetic PII-labeled multi-text span dataset generated from 3 text generation Large Language Models (LLMs)", "conclusion": "We release our dataset and code to foster reproducible research into PII privacy risks in online social media."}}
{"id": "2507.23206", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23206", "abs": "https://arxiv.org/abs/2507.23206", "authors": ["Xiaoyu Ji", "Ali Shakouri", "Fengqing Zhu"], "title": "Confidence-aware agglomeration classification and segmentation of 2D microscopic food crystal images", "comment": null, "summary": "Food crystal agglomeration is a phenomenon occurs during crystallization\nwhich traps water between crystals and affects food product quality. Manual\nannotation of agglomeration in 2D microscopic images is particularly difficult\ndue to the transparency of water bonding and the limited perspective focusing\non a single slide of the imaged sample. To address this challenge, we first\npropose a supervised baseline model to generate segmentation pseudo-labels for\nthe coarsely labeled classification dataset. Next, an instance classification\nmodel that simultaneously performs pixel-wise segmentation is trained. Both\nmodels are used in the inference stage to combine their respective strengths in\nclassification and segmentation. To preserve crystal properties, a post\nprocessing module is designed and included to both steps. Our method improves\ntrue positive agglomeration classification accuracy and size distribution\npredictions compared to other existing methods. Given the variability in\nconfidence levels of manual annotations, our proposed method is evaluated under\ntwo confidence levels and successfully classifies potential agglomerated\ninstances.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u98df\u54c1\u6676\u4f53\u56e2\u805a\u5206\u7c7b\u548c\u5206\u5272\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u76d1\u7763\u5b66\u4e60\u3001\u5b9e\u4f8b\u5206\u7c7b\u548c\u540e\u5904\u7406\uff0c\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u4fdd\u6301\u6676\u4f53\u7279\u6027\u3002", "motivation": "\u98df\u7269\u6676\u4f53\u56e2\u805a\u662f\u4e00\u79cd\u5728\u7ed3\u6676\u8fc7\u7a0b\u4e2d\u53d1\u751f\u7684\u73b0\u8c61\uff0c\u5b83\u4f1a\u5728\u6676\u4f53\u4e4b\u95f4\u622a\u7559\u6c34\u5206\uff0c\u4ece\u800c\u5f71\u54cd\u98df\u54c1\u7684\u8d28\u91cf\u3002\u7531\u4e8e\u6c34\u7ed3\u5408\u7684\u900f\u660e\u6027\u548c\u96c6\u4e2d\u4e8e\u6210\u50cf\u6837\u54c1\u5355\u5f20\u5207\u7247\u7684\u6709\u9650\u89c6\u89d2\uff0c\u5728\u4e8c\u7ef4\u663e\u5fae\u56fe\u50cf\u4e2d\u624b\u52a8\u6ce8\u91ca\u56e2\u805a\u7279\u522b\u56f0\u96be\u3002", "method": "\u9996\u5148\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u76d1\u7763\u57fa\u7ebf\u6a21\u578b\uff0c\u7528\u4e8e\u4e3a\u7c97\u7565\u6807\u8bb0\u7684\u5206\u7c7b\u6570\u636e\u96c6\u751f\u6210\u5206\u5272\u4f2a\u6807\u7b7e\u3002\u63a5\u4e0b\u6765\uff0c\u8bad\u7ec3\u4e00\u4e2a\u540c\u65f6\u6267\u884c\u50cf\u7d20\u7ea7\u5206\u5272\u7684\u5b9e\u4f8b\u5206\u7c7b\u6a21\u578b\u3002\u5728\u63a8\u7406\u9636\u6bb5\uff0c\u4e24\u4e2a\u6a21\u578b\u7ed3\u5408\u5404\u81ea\u5728\u5206\u7c7b\u548c\u5206\u5272\u65b9\u9762\u7684\u4f18\u52bf\u3002\u4e3a\u4e86\u4fdd\u6301\u6676\u4f53\u7279\u6027\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540e\u5904\u7406\u6a21\u5757\uff0c\u5e76\u5c06\u5176\u5305\u542b\u5230\u4e24\u4e2a\u6b65\u9aa4\u4e2d\u3002", "result": "\u4e0e\u5176\u4ed6\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u771f\u9633\u6027\u56e2\u805a\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u5c3a\u5bf8\u5206\u5e03\u9884\u6d4b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e24\u79cd\u7f6e\u4fe1\u5ea6\u6c34\u5e73\u4e0b\u8bc4\u4f30\uff0c\u5e76\u6210\u529f\u5206\u7c7b\u4e86\u6f5c\u5728\u7684\u56e2\u805a\u5b9e\u4f8b\u3002"}}
{"id": "2507.23154", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23154", "abs": "https://arxiv.org/abs/2507.23154", "authors": ["Sofiane Bouaziz", "Adel Hafiane", "Raphael Canals", "Rachid Nedjai"], "title": "FuseTen: A Generative Model for Daily 10 m Land Surface Temperature Estimation from Spatio-Temporal Satellite Observations", "comment": "Accepted in the 2025 International Conference on Machine Intelligence\n  for GeoAnalytics and Remote Sensing (MIGARS)", "summary": "Urban heatwaves, droughts, and land degradation are pressing and growing\nchallenges in the context of climate change. A valuable approach to studying\nthem requires accurate spatio-temporal information on land surface conditions.\nOne of the most important variables for assessing and understanding these\nphenomena is Land Surface Temperature (LST), which is derived from satellites\nand provides essential information about the thermal state of the Earth's\nsurface. However, satellite platforms inherently face a trade-off between\nspatial and temporal resolutions. To bridge this gap, we propose FuseTen, a\nnovel generative framework that produces daily LST observations at a fine 10 m\nspatial resolution by fusing spatio-temporal observations derived from\nSentinel-2, Landsat 8, and Terra MODIS. FuseTen employs a generative\narchitecture trained using an averaging-based supervision strategy grounded in\nphysical principles. It incorporates attention and normalization modules within\nthe fusion process and uses a PatchGAN discriminator to enforce realism.\nExperiments across multiple dates show that FuseTen outperforms linear\nbaselines, with an average 32.06% improvement in quantitative metrics and\n31.42% in visual fidelity. To the best of our knowledge, this is the first\nnon-linear method to generate daily LST estimates at such fine spatial\nresolution.", "AI": {"tldr": "FuseTen, a novel generative framework, generates daily Land Surface Temperature (LST) observations at a fine 10 m spatial resolution by fusing spatio-temporal observations derived from Sentinel-2, Landsat 8, and Terra MODIS. It outperforms linear baselines.", "motivation": "Urban heatwaves, droughts, and land degradation are pressing and growing challenges in the context of climate change. A valuable approach to studying them requires accurate spatio-temporal information on land surface conditions. One of the most important variables for assessing and understanding these phenomena is Land Surface Temperature (LST).", "method": "FuseTen, a novel generative framework that produces daily LST observations at a fine 10 m spatial resolution by fusing spatio-temporal observations derived from Sentinel-2, Landsat 8, and Terra MODIS. It employs a generative architecture trained using an averaging-based supervision strategy grounded in physical principles. It incorporates attention and normalization modules within the fusion process and uses a PatchGAN discriminator to enforce realism.", "result": "FuseTen produces daily LST observations at a fine 10 m spatial resolution.", "conclusion": "FuseTen outperforms linear baselines, with an average 32.06% improvement in quantitative metrics and 31.42% in visual fidelity. This is the first non-linear method to generate daily LST estimates at such fine spatial resolution."}}
{"id": "2503.21813", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2503.21813", "abs": "https://arxiv.org/abs/2503.21813", "authors": ["Zhangcheng Qiang", "Kerry Taylor", "Weiqing Wang", "Jing Jiang"], "title": "OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching", "comment": "14 pages, 4 figures, 4 tables, 2 prompt templates", "summary": "Hallucinations are often inevitable in downstream tasks using large language\nmodels (LLMs). To tackle the substantial challenge of addressing hallucinations\nfor LLM-based ontology matching (OM) systems, we introduce a new benchmark\ndataset OAEI-LLM-T. The dataset evolves from seven TBox datasets in the\nOntology Alignment Evaluation Initiative (OAEI), capturing hallucinations of\nten different LLMs performing OM tasks. These OM-specific hallucinations are\norganised into two primary categories and six sub-categories. We showcase the\nusefulness of the dataset in constructing an LLM leaderboard for OM tasks and\nfor fine-tuning LLMs used in OM tasks.", "AI": {"tldr": "\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\u6765\u6355\u83b7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u672c\u4f53\u5339\u914d\u4efb\u52a1\u4e2d\u4ea7\u751f\u7684\u5e7b\u89c9\uff0c\u5e76\u5c55\u793a\u4e86\u8be5\u6570\u636e\u96c6\u5728\u6784\u5efa LLM \u6392\u884c\u699c\u548c\u5fae\u8c03 LLM \u65b9\u9762\u7684\u7528\u9014\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0c\u5e7b\u89c9\u901a\u5e38\u662f\u4e0d\u53ef\u907f\u514d\u7684\u3002\u4e3a\u4e86\u5e94\u5bf9\u57fa\u4e8e LLM \u7684\u672c\u4f53\u5339\u914d (OM) \u7cfb\u7edf\u4e2d\u89e3\u51b3\u5e7b\u89c9\u8fd9\u4e00\u91cd\u5927\u6311\u6218", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6 OAEI-LLM-T\uff0c\u8be5\u6570\u636e\u96c6\u4ece\u672c\u4f53\u5bf9\u9f50\u8bc4\u4f30\u8ba1\u5212 (OAEI) \u4e2d\u7684\u4e03\u4e2a TBox \u6570\u636e\u96c6\u6f14\u53d8\u800c\u6765\uff0c\u6355\u83b7\u4e86\u6267\u884c OM \u4efb\u52a1\u7684\u5341\u4e2a\u4e0d\u540c LLM \u7684\u5e7b\u89c9\u3002", "result": "\u8fd9\u4e9b OM \u7279\u5b9a\u7684\u5e7b\u89c9\u88ab\u7ec4\u7ec7\u6210\u4e24\u4e2a\u4e3b\u8981\u7c7b\u522b\u548c\u516d\u4e2a\u5b50\u7c7b\u522b\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u53ef\u7528\u4e8e\u6784\u5efa\u7528\u4e8e\u672c\u4f53\u5339\u914d\u4efb\u52a1\u7684 LLM \u6392\u884c\u699c\uff0c\u4ee5\u53ca\u7528\u4e8e\u5fae\u8c03\u7528\u4e8e\u672c\u4f53\u5339\u914d\u4efb\u52a1\u7684 LLM\u3002"}}
{"id": "2507.22931", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22931", "abs": "https://arxiv.org/abs/2507.22931", "authors": ["Shuyu Guo", "Zhaochun Ren"], "title": "Enhancing RAG Efficiency with Adaptive Context Compression", "comment": null, "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith external knowledge but incurs significant inference costs due to lengthy\nretrieved contexts. While context compression mitigates this issue, existing\nmethods apply fixed compression rates, over-compressing simple queries or\nunder-compressing complex ones. We propose Adaptive Context Compression for RAG\n(ACC-RAG), a framework that dynamically adjusts compression rates based on\ninput complexity, optimizing inference efficiency without sacrificing accuracy.\nACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with\na context selector to retain minimal sufficient information, akin to human\nskimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms\nfixed-rate methods and matches/unlocks over 4 times faster inference versus\nstandard RAG while maintaining or improving accuracy.", "AI": {"tldr": "ACC-RAG dynamically adjusts compression rates based on input complexity, optimizing inference efficiency without sacrificing accuracy.", "motivation": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but incurs significant inference costs due to lengthy retrieved contexts. While context compression mitigates this issue, existing methods apply fixed compression rates, over-compressing simple queries or under-compressing complex ones.", "method": "Adaptive Context Compression for RAG (ACC-RAG), a framework that dynamically adjusts compression rates based on input complexity, optimizing inference efficiency without sacrificing accuracy. ACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with a context selector to retain minimal sufficient information", "result": "ACC-RAG outperforms fixed-rate methods and matches/unlocks over 4 times faster inference versus standard RAG while maintaining or improving accuracy.", "conclusion": "ACC-RAG outperforms fixed-rate methods and matches/unlocks over 4 times faster inference versus standard RAG while maintaining or improving accuracy."}}
{"id": "2507.23225", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23225", "abs": "https://arxiv.org/abs/2507.23225", "authors": ["Zicheng Lin", "Weichao Pan"], "title": "YOLO-ROC: A High-Precision and Ultra-Lightweight Model for Real-Time Road Damage Detection", "comment": null, "summary": "Road damage detection is a critical task for ensuring traffic safety and\nmaintaining infrastructure integrity. While deep learning-based detection\nmethods are now widely adopted, they still face two core challenges: first, the\ninadequate multi-scale feature extraction capabilities of existing networks for\ndiverse targets like cracks and potholes, leading to high miss rates for\nsmall-scale damage; and second, the substantial parameter counts and\ncomputational demands of mainstream models, which hinder their deployment for\nefficient, real-time detection in practical applications. To address these\nissues, this paper proposes a high-precision and lightweight model, YOLO - Road\nOrthogonal Compact (YOLO-ROC). We designed a Bidirectional Multi-scale Spatial\nPyramid Pooling Fast (BMS-SPPF) module to enhance multi-scale feature\nextraction and implemented a hierarchical channel compression strategy to\nreduce computational complexity. The BMS-SPPF module leverages a bidirectional\nspatial-channel attention mechanism to improve the detection of small targets.\nConcurrently, the channel compression strategy reduces the parameter count from\n3.01M to 0.89M and GFLOPs from 8.1 to 2.6. Experiments on the\nRDD2022_China_Drone dataset demonstrate that YOLO-ROC achieves a mAP50 of\n67.6%, surpassing the baseline YOLOv8n by 2.11%. Notably, the mAP50 for the\nsmall-target D40 category improved by 16.8%, and the final model size is only\n2.0 MB. Furthermore, the model exhibits excellent generalization performance on\nthe RDD2022_China_Motorbike dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u7cbe\u5ea6\u548c\u8f7b\u91cf\u7ea7\u6a21\u578bYOLO-ROC\uff0c\u4ee5\u89e3\u51b3\u9053\u8def\u635f\u4f24\u68c0\u6d4b\u4e2d\u73b0\u6709\u7f51\u7edc\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7f51\u7edc\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u4e0d\u8db3\uff0c\u5bfc\u81f4\u5c0f\u5c3a\u5ea6\u635f\u4f24\u7684\u6f0f\u68c0\u7387\u9ad8\uff1b\u4e3b\u6d41\u6a21\u578b\u7684\u53c2\u6570\u6570\u91cf\u548c\u8ba1\u7b97\u9700\u6c42\u5927\uff0c\u963b\u788d\u4e86\u5b83\u4eec\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8fdb\u884c\u9ad8\u6548\u3001\u5b9e\u65f6\u68c0\u6d4b\u7684\u90e8\u7f72\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53cc\u5411\u591a\u5c3a\u5ea6\u7a7a\u95f4\u91d1\u5b57\u5854\u6c60\u5316\u5feb\u901f(BMS-SPPF)\u6a21\u5757\uff0c\u4ee5\u589e\u5f3a\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd\u5206\u5c42\u901a\u9053\u538b\u7f29\u7b56\u7565\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u6027\u3002", "result": "\u53c2\u6570\u6570\u91cf\u4ece3.01M\u51cf\u5c11\u52300.89M\uff0cGFLOPs\u4ece8.1\u51cf\u5c11\u52302.6\u3002\u5728RDD2022_China_Drone\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cYOLO-ROC\u5b9e\u73b0\u4e8667.6%\u7684mAP50\uff0c\u8d85\u8fc7\u4e86\u57fa\u7ebfYOLOv8n 2.11%\u3002\u5c0f\u76ee\u6807D40\u7c7b\u522b\u7684mAP50\u63d0\u9ad8\u4e8616.8%\uff0c\u6700\u7ec8\u6a21\u578b\u5927\u5c0f\u4ec5\u4e3a2.0 MB\u3002\u8be5\u6a21\u578b\u5728RDD2022_China_Motorbike\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "YOLO-ROC\u5728RDD2022_China_Drone\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8667.6%\u7684mAP50\uff0c\u8d85\u8fc7\u4e86\u57fa\u7ebfYOLOv8n 2.11%\u3002\u5bf9\u4e8e\u5c0f\u76ee\u6807D40\u7c7b\u522b\uff0cmAP50\u63d0\u9ad8\u4e8616.8%\uff0c\u6700\u7ec8\u6a21\u578b\u5927\u5c0f\u4ec5\u4e3a2.0 MB\u3002\u8be5\u6a21\u578b\u5728RDD2022_China_Motorbike\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2507.23170", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23170", "abs": "https://arxiv.org/abs/2507.23170", "authors": ["Jinan Zhou", "Rajat Ghosh", "Vaishnavi Bhargava", "Debojyoti Dutta", "Aryan Singhal"], "title": "BAR Conjecture: the Feasibility of Inference Budget-Constrained LLM Services with Authenticity and Reasoning", "comment": null, "summary": "When designing LLM services, practitioners care about three key properties:\ninference-time budget, factual authenticity, and reasoning capacity. However,\nour analysis shows that no model can simultaneously optimize for all three. We\nformally prove this trade-off and propose a principled framework named The BAR\nTheorem for LLM-application design.", "AI": {"tldr": "LLM\u5e94\u7528\u8bbe\u8ba1\u9700\u8981\u5728\u63a8\u7406\u65f6\u95f4\u3001\u4e8b\u5b9e\u6027\u548c\u63a8\u7406\u80fd\u529b\u4e4b\u95f4\u6743\u8861\u3002", "motivation": "\u5728\u8bbe\u8ba1LLM\u670d\u52a1\u65f6\uff0c\u4ece\u4e1a\u8005\u5173\u5fc3\u4e09\u4e2a\u5173\u952e\u5c5e\u6027\uff1a\u63a8\u7406\u65f6\u95f4\u9884\u7b97\u3001\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5f62\u5f0f\u5316\u8bc1\u660e", "result": "\u6ca1\u6709\u6a21\u578b\u53ef\u4ee5\u540c\u65f6\u4f18\u5316\u6240\u6709\u8fd9\u4e09\u4e2a\u5c5e\u6027\u3002", "conclusion": "\u6a21\u578b\u65e0\u6cd5\u540c\u65f6\u4f18\u5316\u63a8\u7406\u65f6\u95f4\u9884\u7b97\u3001\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u63a8\u7406\u80fd\u529b\u8fd9\u4e09\u4e2a\u5173\u952e\u5c5e\u6027\u3002"}}
{"id": "2507.22932", "categories": ["cs.CL", "q-fin.GN"], "pdf": "https://arxiv.org/pdf/2507.22932", "abs": "https://arxiv.org/abs/2507.22932", "authors": ["Baptiste Lefort", "Eric Benhamou", "Beatrice Guez", "Jean-Jacques Ohana", "Ethan Setrouk", "Alban Etienne"], "title": "FinMarBa: A Market-Informed Dataset for Financial Sentiment Classification", "comment": "8 pages", "summary": "This paper presents a novel hierarchical framework for portfolio\noptimization, integrating lightweight Large Language Models (LLMs) with Deep\nReinforcement Learning (DRL) to combine sentiment signals from financial news\nwith traditional market indicators. Our three-tier architecture employs base RL\nagents to process hybrid data, meta-agents to aggregate their decisions, and a\nsuper-agent to merge decisions based on market data and sentiment analysis.\nEvaluated on data from 2018 to 2024, after training on 2000-2017, the framework\nachieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming\nequal-weighted and S&P 500 benchmarks. Key contributions include scalable\ncross-modal integration, a hierarchical RL structure for enhanced stability,\nand open-source reproducibility.", "AI": {"tldr": "A novel hierarchical framework for portfolio optimization, integrating lightweight Large Language Models (LLMs) with Deep Reinforcement Learning (DRL) to combine sentiment signals from financial news with traditional market indicators.", "motivation": "This paper presents a novel hierarchical framework for portfolio optimization", "method": "integrating lightweight Large Language Models (LLMs) with Deep Reinforcement Learning (DRL) to combine sentiment signals from financial news with traditional market indicators. Our three-tier architecture employs base RL agents to process hybrid data, meta-agents to aggregate their decisions, and a super-agent to merge decisions based on market data and sentiment analysis.", "result": "Key contributions include scalable cross-modal integration, a hierarchical RL structure for enhanced stability, and open-source reproducibility.", "conclusion": "The framework achieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming equal-weighted and S&P 500 benchmarks."}}
{"id": "2507.23226", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23226", "abs": "https://arxiv.org/abs/2507.23226", "authors": ["Yanming Xiu"], "title": "Toward Safe, Trustworthy and Realistic Augmented Reality User Experience", "comment": "2 pages, 4 figures", "summary": "As augmented reality (AR) becomes increasingly integrated into everyday life,\nensuring the safety and trustworthiness of its virtual content is critical. Our\nresearch addresses the risks of task-detrimental AR content, particularly that\nwhich obstructs critical information or subtly manipulates user perception. We\ndeveloped two systems, ViDDAR and VIM-Sense, to detect such attacks using\nvision-language models (VLMs) and multimodal reasoning modules. Building on\nthis foundation, we propose three future directions: automated, perceptually\naligned quality assessment of virtual content; detection of multimodal attacks;\nand adaptation of VLMs for efficient and user-centered deployment on AR\ndevices. Overall, our work aims to establish a scalable, human-aligned\nframework for safeguarding AR experiences and seeks feedback on perceptual\nmodeling, multimodal AR content implementation, and lightweight model\nadaptation.", "AI": {"tldr": "Addresses the risks of task-detrimental AR content by developing two systems, ViDDAR and VIM-Sense, to detect such attacks using vision-language models (VLMs) and multimodal reasoning modules. Proposes three future directions: automated, perceptually aligned quality assessment; detection of multimodal attacks; and adaptation of VLMs for efficient and user-centered deployment on AR devices.", "motivation": "Ensuring the safety and trustworthiness of virtual content is critical as augmented reality (AR) becomes increasingly integrated into everyday life. This research addresses the risks of task-detrimental AR content, particularly that which obstructs critical information or subtly manipulates user perception.", "method": "The authors developed two systems, ViDDAR and VIM-Sense, to detect such attacks using vision-language models (VLMs) and multimodal reasoning modules.", "result": "The authors developed two systems, ViDDAR and VIM-Sense, to detect such attacks using vision-language models (VLMs) and multimodal reasoning modules.", "conclusion": "This work aims to establish a scalable, human-aligned framework for safeguarding AR experiences and seeks feedback on perceptual modeling, multimodal AR content implementation, and lightweight model adaptation."}}
{"id": "2507.23186", "categories": ["cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.23186", "abs": "https://arxiv.org/abs/2507.23186", "authors": ["Peter Sharpe"], "title": "NaN-Propagation: A Novel Method for Sparsity Detection in Black-Box Computational Functions", "comment": null, "summary": "Sparsity detection in black-box functions enables significant computational\nspeedups in gradient-based optimization through Jacobian compression, but\nexisting finite-difference methods suffer from false negatives due to\ncoincidental zero gradients. These false negatives can silently corrupt\ngradient calculations, leading to difficult-to-diagnose errors. We introduce\nNaN-propagation, which exploits the universal contamination property of IEEE\n754 Not-a-Number floating-point values to trace input-output dependencies\nthrough floating-point numerical computations. By systematically contaminating\ninputs with NaN and observing which outputs become NaN, the method reconstructs\nconservative sparsity patterns that eliminate false negatives. We demonstrate\nthe approach on an aerospace wing weight model, achieving a 1.52x speedup while\ndetecting dozens of dependencies missed by conventional methods -- a\nsignificant improvement since gradient computation is the bottleneck in many\noptimization workflows. The technique leverages IEEE 754 compliance to work\nacross programming languages and math libraries without modifying existing\nblack-box codes. Advanced strategies including NaN payload encoding enable\nfaster-than-linear time complexity, improving upon existing black-box sparsity\ndetection methods. Practical algorithms are also proposed to mitigate\nchallenges from branching code execution common in engineering applications.", "AI": {"tldr": "\u901a\u8fc7\u5229\u7528 NaN \u4f20\u64ad\u6765\u68c0\u6d4b\u9ed1\u76d2\u51fd\u6570\u4e2d\u7684\u7a00\u758f\u6027\uff0c\u4ece\u800c\u5728\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u4e2d\u5b9e\u73b0\u8ba1\u7b97\u52a0\u901f\u3002", "motivation": "\u9ed1\u76d2\u51fd\u6570\u4e2d\u7684\u7a00\u758f\u6027\u68c0\u6d4b\u53ef\u4ee5\u901a\u8fc7\u96c5\u53ef\u6bd4\u77e9\u9635\u538b\u7f29\u663e\u7740\u52a0\u5feb\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u4e2d\u7684\u8ba1\u7b97\u901f\u5ea6\uff0c\u4f46\u73b0\u6709\u7684\u6709\u9650\u5dee\u5206\u65b9\u6cd5\u7531\u4e8e\u5076\u7136\u7684\u96f6\u68af\u5ea6\u800c\u5b58\u5728\u8bef\u62a5\u3002\u8fd9\u4e9b\u8bef\u62a5\u4f1a\u6084\u65e0\u58f0\u606f\u5730\u7834\u574f\u68af\u5ea6\u8ba1\u7b97\uff0c\u4ece\u800c\u5bfc\u81f4\u96be\u4ee5\u8bca\u65ad\u7684\u9519\u8bef\u3002", "method": "\u5f15\u5165 NaN \u4f20\u64ad\uff0c\u5b83\u5229\u7528 IEEE 754 Not-a-Number \u6d6e\u70b9\u503c\u7684\u901a\u7528\u6c61\u67d3\u7279\u6027\uff0c\u901a\u8fc7\u6d6e\u70b9\u6570\u503c\u8ba1\u7b97\u6765\u8ddf\u8e2a\u8f93\u5165-\u8f93\u51fa\u4f9d\u8d56\u5173\u7cfb\u3002\u901a\u8fc7\u7cfb\u7edf\u5730\u7528 NaN \u6c61\u67d3\u8f93\u5165\u5e76\u89c2\u5bdf\u54ea\u4e9b\u8f93\u51fa\u53d8\u4e3a NaN\uff0c\u8be5\u65b9\u6cd5\u91cd\u5efa\u4e86\u6d88\u9664\u8bef\u62a5\u7684\u4fdd\u5b88\u7a00\u758f\u6a21\u5f0f\u3002", "result": "\u5728\u822a\u7a7a\u822a\u5929\u673a\u7ffc\u91cd\u91cf\u6a21\u578b\u4e0a\u6f14\u793a\u4e86\u8be5\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86 1.52 \u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u68c0\u6d4b\u5230\u4f20\u7edf\u65b9\u6cd5\u9057\u6f0f\u7684\u6570\u5341\u4e2a\u4f9d\u8d56\u5173\u7cfb\u2014\u2014\u8fd9\u662f\u4e00\u9879\u663e\u7740\u6539\u8fdb\uff0c\u56e0\u4e3a\u68af\u5ea6\u8ba1\u7b97\u662f\u8bb8\u591a\u4f18\u5316\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u74f6\u9888\u3002", "conclusion": "\u8be5\u6280\u672f\u5229\u7528 IEEE 754 \u5408\u89c4\u6027\uff0c\u53ef\u5728\u4e0d\u4fee\u6539\u73b0\u6709\u9ed1\u76d2\u4ee3\u7801\u7684\u60c5\u51b5\u4e0b\u8de8\u7f16\u7a0b\u8bed\u8a00\u548c\u6570\u5b66\u5e93\u5de5\u4f5c\u3002\u5305\u62ec NaN \u6709\u6548\u8d1f\u8f7d\u7f16\u7801\u5728\u5185\u7684\u9ad8\u7ea7\u7b56\u7565\u5b9e\u73b0\u4e86\u5feb\u4e8e\u7ebf\u6027\u7684\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u9ed1\u76d2\u7a00\u758f\u6027\u68c0\u6d4b\u65b9\u6cd5\u3002\u8fd8\u63d0\u51fa\u4e86\u5b9e\u7528\u7684\u7b97\u6cd5\u6765\u7f13\u89e3\u5de5\u7a0b\u5e94\u7528\u4e2d\u5e38\u89c1\u7684\u6765\u81ea\u5206\u652f\u4ee3\u7801\u6267\u884c\u7684\u6311\u6218\u3002"}}
{"id": "2507.22933", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22933", "abs": "https://arxiv.org/abs/2507.22933", "authors": ["Anthony C Davis", "Burhan Sadiq", "Tianmin Shu", "Chien-Ming Huang"], "title": "Augmented Vision-Language Models: A Systematic Review", "comment": null, "summary": "Recent advances in visual-language machine learning models have demonstrated\nexceptional ability to use natural language and understand visual scenes by\ntraining on large, unstructured datasets. However, this training paradigm\ncannot produce interpretable explanations for its outputs, requires retraining\nto integrate new information, is highly resource-intensive, and struggles with\ncertain forms of logical reasoning. One promising solution involves integrating\nneural networks with external symbolic information systems, forming neural\nsymbolic systems that can enhance reasoning and memory abilities. These neural\nsymbolic systems provide more interpretable explanations to their outputs and\nthe capacity to assimilate new information without extensive retraining.\nUtilizing powerful pre-trained Vision-Language Models (VLMs) as the core neural\ncomponent, augmented by external systems, offers a pragmatic approach to\nrealizing the benefits of neural-symbolic integration. This systematic\nliterature review aims to categorize techniques through which visual-language\nunderstanding can be improved by interacting with external symbolic information\nsystems.", "AI": {"tldr": "\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5927\u578b\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3001\u9700\u8981\u5927\u91cf\u8d44\u6e90\u4e14\u903b\u8f91\u63a8\u7406\u80fd\u529b\u5dee\u3002\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\u662f\u5c06\u795e\u7ecf\u7f51\u7edc\u4e0e\u5916\u90e8\u7b26\u53f7\u4fe1\u606f\u7cfb\u7edf\u96c6\u6210\u3002\u672c\u6587\u65e8\u5728\u5bf9\u901a\u8fc7\u4e0e\u5916\u90e8\u7b26\u53f7\u4fe1\u606f\u7cfb\u7edf\u4ea4\u4e92\u6765\u6539\u8fdb\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\u7684\u6280\u672f\u8fdb\u884c\u5206\u7c7b\u3002", "motivation": "Recent advances in visual-language machine learning models have demonstrated exceptional ability to use natural language and understand visual scenes by training on large, unstructured datasets. However, this training paradigm cannot produce interpretable explanations for its outputs, requires retraining to integrate new information, is highly resource-intensive, and struggles with certain forms of logical reasoning. One promising solution involves integrating neural networks with external symbolic information systems, forming neural symbolic systems that can enhance reasoning and memory abilities. These neural symbolic systems provide more interpretable explanations to their outputs and the capacity to assimilate new information without extensive retraining.", "method": "Utilizing powerful pre-trained Vision-Language Models (VLMs) as the core neural component, augmented by external systems, offers a pragmatic approach to realizing the benefits of neural-symbolic integration.", "result": "\u65e0", "conclusion": "This systematic literature review aims to categorize techniques through which visual-language understanding can be improved by interacting with external symbolic information systems."}}
{"id": "2507.23237", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23237", "abs": "https://arxiv.org/abs/2507.23237", "authors": ["Fan Lyu", "Linglan Zhao", "Chengyan Liu", "Yinying Mei", "Zhang Zhang", "Jian Zhang", "Fuyuan Hu", "Liang Wang"], "title": "Ambiguity-Guided Learnable Distribution Calibration for Semi-Supervised Few-Shot Class-Incremental Learning", "comment": "6 pages, 5 figures", "summary": "Few-Shot Class-Incremental Learning (FSCIL) focuses on models learning new\nconcepts from limited data while retaining knowledge of previous classes.\nRecently, many studies have started to leverage unlabeled samples to assist\nmodels in learning from few-shot samples, giving rise to the field of\nSemi-supervised Few-shot Class-Incremental Learning (Semi-FSCIL). However,\nthese studies often assume that the source of unlabeled data is only confined\nto novel classes of the current session, which presents a narrow perspective\nand cannot align well with practical scenarios. To better reflect real-world\nscenarios, we redefine Semi-FSCIL as Generalized Semi-FSCIL (GSemi-FSCIL) by\nincorporating both base and all the ever-seen novel classes in the unlabeled\nset. This change in the composition of unlabeled samples poses a new challenge\nfor existing methods, as they struggle to distinguish between unlabeled samples\nfrom base and novel classes. To address this issue, we propose an\nAmbiguity-guided Learnable Distribution Calibration (ALDC) strategy. ALDC\ndynamically uses abundant base samples to correct biased feature distributions\nfor few-shot novel classes. Experiments on three benchmark datasets show that\nour method outperforms existing works, setting new state-of-the-art results.", "AI": {"tldr": "This paper introduces Generalized Semi-FSCIL (GSemi-FSCIL) and an Ambiguity-guided Learnable Distribution Calibration (ALDC) strategy to address the challenge of distinguishing between unlabeled samples from base and novel classes. The proposed method achieves state-of-the-art results.", "motivation": "Existing Semi-FSCIL studies have a narrow perspective by assuming unlabeled data is only from novel classes.  GSemi-FSCIL is introduced to incorporate both base and all ever-seen novel classes in the unlabeled set, better reflecting real-world scenarios.", "method": "Ambiguity-guided Learnable Distribution Calibration (ALDC) strategy", "result": "ALDC dynamically uses abundant base samples to correct biased feature distributions for few-shot novel classes.", "conclusion": "The proposed ALDC method outperforms existing works on three benchmark datasets, setting new state-of-the-art results."}}
{"id": "2507.23217", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23217", "abs": "https://arxiv.org/abs/2507.23217", "authors": ["Hyeon Seong Jeong", "Sangwoo Jo", "Byeong Hyun Yoon", "Yoonseok Heo", "Haedong Jeong", "Taehoon Kim"], "title": "Zero-Shot Document Understanding using Pseudo Table of Contents-Guided Retrieval-Augmented Generation", "comment": null, "summary": "Understanding complex multimodal documents remains challenging due to their\nstructural inconsistencies and limited training data availability. We introduce\n\\textit{DocsRay}, a training-free document understanding system that integrates\npseudo Table of Contents (TOC) generation with hierarchical Retrieval-Augmented\nGeneration (RAG). Our approach leverages multimodal Large Language Models'\n(LLMs) native capabilities to seamlessly process documents containing diverse\nelements such as text, images, charts, and tables without requiring specialized\nmodels or additional training. DocsRay's framework synergistically combines\nthree key techniques: (1) a semantic structuring module using prompt-based LLM\ninteractions to generate a hierarchical pseudo-TOC, (2) zero-shot multimodal\nanalysis that converts diverse document elements into unified, text-centric\nrepresentations using the inherent capabilities of multimodal LLMs, and (3) an\nefficient two-stage hierarchical retrieval system that reduces retrieval\ncomplexity from $O(N)$ to $O(S + k_1 \\cdot N_s)$. Evaluated on documents\naveraging 49.4 pages and 20,971 textual tokens, DocsRay reduced query latency\nfrom 3.89 to 2.12 seconds, achieving a 45% efficiency improvement. On the\nMMLongBench-Doc benchmark, DocsRay-Pro attains an accuracy of 64.7%,\nsubstantially surpassing previous state-of-the-art results.", "AI": {"tldr": "DocsRay \u662f\u4e00\u4e2a\u514d\u8bad\u7ec3\u7684\u6587\u6863\u7406\u89e3\u7cfb\u7edf\uff0c\u5b83\u96c6\u6210\u4e86\u4f2a\u76ee\u5f55\u751f\u6210\u548c\u5206\u5c42 RAG\uff0c\u5728 MMLongBench-Doc \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5927\u5927\u8d85\u8fc7\u4e86\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u7531\u4e8e\u7ed3\u6784\u4e0d\u4e00\u81f4\u548c\u6709\u9650\u7684\u8bad\u7ec3\u6570\u636e\u53ef\u7528\u6027\uff0c\u7406\u89e3\u590d\u6742\u7684\u591a\u6a21\u6001\u6587\u6863\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u96c6\u6210\u4e86\u4f2a\u76ee\u5f55 (TOC) \u751f\u6210\u4e0e\u5206\u5c42\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG)\u3002", "result": "DocsRay \u5c06\u67e5\u8be2\u5ef6\u8fdf\u4ece 3.89 \u79d2\u964d\u4f4e\u5230 2.12 \u79d2\uff0c\u6548\u7387\u63d0\u9ad8\u4e86 45%\u3002", "conclusion": "DocsRay-Pro \u5728 MMLongBench-Doc \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86 64.7% \u7684\u51c6\u786e\u7387\uff0c\u5927\u5927\u8d85\u8fc7\u4e86\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2507.22934", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22934", "abs": "https://arxiv.org/abs/2507.22934", "authors": ["Jingwei Zhao", "Yuhua Wen", "Qifei Li", "Minchi Hu", "Yingying Zhou", "Jingyao Xue", "Junyang Wu", "Yingming Gao", "Zhengqi Wen", "Jianhua Tao", "Ya Li"], "title": "Deep Learning Approaches for Multimodal Intent Recognition: A Survey", "comment": "Submitted to ACM Computing Surveys", "summary": "Intent recognition aims to identify users' underlying intentions,\ntraditionally focusing on text in natural language processing. With growing\ndemands for natural human-computer interaction, the field has evolved through\ndeep learning and multimodal approaches, incorporating data from audio, vision,\nand physiological signals. Recently, the introduction of Transformer-based\nmodels has led to notable breakthroughs in this domain. This article surveys\ndeep learning methods for intent recognition, covering the shift from unimodal\nto multimodal techniques, relevant datasets, methodologies, applications, and\ncurrent challenges. It provides researchers with insights into the latest\ndevelopments in multimodal intent recognition (MIR) and directions for future\nresearch.", "AI": {"tldr": "\u672c\u6587\u6982\u8ff0\u4e86\u7528\u4e8e\u610f\u56fe\u8bc6\u522b\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u6db5\u76d6\u4e86\u4ece\u5355\u6a21\u6001\u5230\u591a\u6a21\u6001\u6280\u672f\u7684\u8f6c\u53d8\u3001\u76f8\u5173\u6570\u636e\u96c6\u3001\u65b9\u6cd5\u3001\u5e94\u7528\u548c\u5f53\u524d\u6311\u6218\u3002", "motivation": "\u968f\u7740\u5bf9\u81ea\u7136\u4eba\u673a\u4ea4\u4e92\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u957f\uff0c\u8be5\u9886\u57df\u5df2\u7ecf\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u548c\u591a\u6a21\u6001\u65b9\u6cd5\u53d1\u5c55\u800c\u6765\uff0c\u6574\u5408\u4e86\u6765\u81ea\u97f3\u9891\u3001\u89c6\u89c9\u548c\u751f\u7406\u4fe1\u53f7\u7684\u6570\u636e\u3002", "method": "\u8c03\u67e5\u4e86\u7528\u4e8e\u610f\u56fe\u8bc6\u522b\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u6db5\u76d6\u4e86\u4ece\u5355\u6a21\u6001\u5230\u591a\u6a21\u6001\u6280\u672f\u7684\u8f6c\u53d8\u3002", "result": "Transformer\u6a21\u578b\u7684\u5f15\u5165\u5bfc\u81f4\u4e86\u8be5\u9886\u57df\u7684\u663e\u8457\u7a81\u7834\u3002", "conclusion": "\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u5bf9\u591a\u6a21\u6001\u610f\u56fe\u8bc6\u522b (MIR) \u6700\u65b0\u8fdb\u5c55\u7684\u89c1\u89e3\u4ee5\u53ca\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\u3002"}}
{"id": "2507.23242", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23242", "abs": "https://arxiv.org/abs/2507.23242", "authors": ["Sungguk Cha", "DongWook Kim", "Taeseung Hahn", "Mintae Kim", "Youngsub Han", "Byoung-Ki Jeon"], "title": "Generalized Reinforcement Learning for Retriever-Specific Query Rewriter with Unstructured Real-World Documents", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems rely heavily on effective query\nformulation to unlock external knowledge, yet optimizing queries for diverse,\nunstructured real-world documents remains a challenge. We introduce\n\\textbf{RL-QR}, a reinforcement learning framework for retriever-specific query\nrewriting that eliminates the need for human-annotated datasets and extends\napplicability to both text-only and multi-modal databases. By synthesizing\nscenario-question pairs and leveraging Generalized Reward Policy Optimization\n(GRPO), RL-QR trains query rewriters tailored to specific retrievers, enhancing\nretrieval performance across varied domains. Experiments on industrial in-house\ndata demonstrate significant improvements, with\n$\\text{RL-QR}_{\\text{multi-modal}}$ achieving an 11\\% relative gain in NDCG@3\nfor multi-modal RAG and $\\text{RL-QR}_{\\text{lexical}}$ yielding a 9\\% gain for\nlexical retrievers. However, challenges persist with semantic and hybrid\nretrievers, where rewriters failed to improve performance, likely due to\ntraining misalignments. Our findings highlight RL-QR's potential to\nrevolutionize query optimization for RAG systems, offering a scalable,\nannotation-free solution for real-world retrieval tasks, while identifying\navenues for further refinement in semantic retrieval contexts.", "AI": {"tldr": "RL-QR\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u67e5\u8be2\u91cd\u5199\u6846\u67b6\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u53ef\u6269\u5c55\u5230\u6587\u672c\u548c\u591a\u6a21\u6001\u6570\u636e\u5e93\uff0c\u5e76\u5728\u591a\u6a21\u6001\u548c\u8bcd\u6cd5\u68c0\u7d22\u5668\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u4e25\u91cd\u4f9d\u8d56\u6709\u6548\u7684\u67e5\u8be2\u5236\u5b9a\u6765\u89e3\u9501\u5916\u90e8\u77e5\u8bc6\uff0c\u4f46\u4e3a\u4e0d\u540c\u7684\u3001\u975e\u7ed3\u6784\u5316\u7684\u771f\u5b9e\u4e16\u754c\u6587\u6863\u4f18\u5316\u67e5\u8be2\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u67e5\u8be2\u91cd\u5199\u6846\u67b6RL-QR\uff0c\u8be5\u6846\u67b6\u5229\u7528GRPO\u8bad\u7ec3\u7279\u5b9a\u4e8e\u68c0\u7d22\u5668\u7684\u67e5\u8be2\u91cd\u5199\u5668\u3002", "result": "\u5728\u5de5\u4e1a\u5185\u90e8\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u591a\u6a21\u6001RAG\u7684NDCG@3\u76f8\u5bf9\u589e\u76ca\u4e3a11%\uff0c\u8bcd\u6cd5\u68c0\u7d22\u5668\u7684\u589e\u76ca\u4e3a9%\u3002", "conclusion": "RL-QR\u5728\u591a\u6a21\u6001\u548c\u8bcd\u6cd5\u68c0\u7d22\u5668\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u7684\u6539\u8fdb\uff0c\u4f46\u5728\u8bed\u4e49\u548c\u6df7\u5408\u68c0\u7d22\u5668\u4e0a\u6548\u679c\u4e0d\u4f73\uff0c\u672a\u6765\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2507.23221", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23221", "abs": "https://arxiv.org/abs/2507.23221", "authors": ["Charles O'Neill", "Slava Chalnev", "Chi Chi Zhao", "Max Kirkby", "Mudith Jayasekara"], "title": "A Single Direction of Truth: An Observer Model's Linear Residual Probe Exposes and Steers Contextual Hallucinations", "comment": null, "summary": "Contextual hallucinations -- statements unsupported by given context --\nremain a significant challenge in AI. We demonstrate a practical\ninterpretability insight: a generator-agnostic observer model detects\nhallucinations via a single forward pass and a linear probe on its residual\nstream. This probe isolates a single, transferable linear direction separating\nhallucinated from faithful text, outperforming baselines by 5-27 points and\nshowing robust mid-layer performance across Gemma-2 models (2B to 27B).\nGradient-times-activation localises this signal to sparse, late-layer MLP\nactivity. Critically, manipulating this direction causally steers generator\nhallucination rates, proving its actionability. Our results offer novel\nevidence of internal, low-dimensional hallucination tracking linked to specific\nMLP sub-circuits, exploitable for detection and mitigation. We release the\n2000-example ContraTales benchmark for realistic assessment of such solutions.", "AI": {"tldr": "\u901a\u8fc7\u7ebf\u6027\u63a2\u6d4b\u6b8b\u5dee\u6d41\uff0c\u89c2\u5bdf\u8005\u6a21\u578b\u53ef\u4ee5\u68c0\u6d4b\u5230\u5e7b\u89c9\uff0c\u5e76\u4e14\u64cd\u7eb5\u8fd9\u4e2a\u65b9\u5411\u53ef\u4ee5\u63a7\u5236\u751f\u6210\u5668\u7684\u5e7b\u89c9\u7387\u3002", "motivation": "\u4e0a\u4e0b\u6587\u5316\u7684\u5e7b\u89c9\u4ecd\u7136\u662f\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5355\u4e2a\u524d\u5411\u4f20\u9012\u548c\u5bf9\u5176\u6b8b\u5dee\u6d41\u7684\u7ebf\u6027\u63a2\u6d4b\uff0c\u89c2\u5bdf\u8005\u6a21\u578b\u53ef\u4ee5\u68c0\u6d4b\u5230\u5e7b\u89c9\u3002", "result": "\u8be5\u63a2\u6d4b\u5668\u5206\u79bb\u51fa\u4e00\u4e2a\u5355\u4e00\u7684\u3001\u53ef\u8f6c\u79fb\u7684\u7ebf\u6027\u65b9\u5411\uff0c\u5c06\u5e7b\u89c9\u6587\u672c\u4e0e\u5fe0\u5b9e\u6587\u672c\u533a\u5206\u5f00\u6765\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf5-27\u4e2a\u70b9\uff0c\u5e76\u5728Gemma-2\u6a21\u578b\uff082B\u523027B\uff09\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u7684\u4e2d\u95f4\u5c42\u6027\u80fd\u3002", "conclusion": "\u5185\u90e8\u7684\u3001\u4f4e\u7ef4\u7684\u5e7b\u89c9\u8ddf\u8e2a\u4e0e\u7279\u5b9a\u7684MLP\u5b50\u7535\u8def\u76f8\u5173\u8054\uff0c\u53ef\u7528\u4e8e\u68c0\u6d4b\u548c\u7f13\u89e3\u3002"}}
{"id": "2507.22935", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22935", "abs": "https://arxiv.org/abs/2507.22935", "authors": ["Kathleen Mealey", "Jonathan A. Karr Jr.", "Priscila Saboia Moreira", "Paul R. Brenner", "Charles F. Vardeman II"], "title": "Trusted Knowledge Extraction for Operations and Maintenance Intelligence", "comment": null, "summary": "Deriving operational intelligence from organizational data repositories is a\nkey challenge due to the dichotomy of data confidentiality vs data integration\nobjectives, as well as the limitations of Natural Language Processing (NLP)\ntools relative to the specific knowledge structure of domains such as\noperations and maintenance. In this work, we discuss Knowledge Graph\nconstruction and break down the Knowledge Extraction process into its Named\nEntity Recognition, Coreference Resolution, Named Entity Linking, and Relation\nExtraction functional components. We then evaluate sixteen NLP tools in concert\nwith or in comparison to the rapidly advancing capabilities of Large Language\nModels (LLMs). We focus on the operational and maintenance intelligence use\ncase for trusted applications in the aircraft industry. A baseline dataset is\nderived from a rich public domain US Federal Aviation Administration dataset\nfocused on equipment failures or maintenance requirements. We assess the\nzero-shot performance of NLP and LLM tools that can be operated within a\ncontrolled, confidential environment (no data is sent to third parties). Based\non our observation of significant performance limitations, we discuss the\nchallenges related to trusted NLP and LLM tools as well as their Technical\nReadiness Level for wider use in mission-critical industries such as aviation.\nWe conclude with recommendations to enhance trust and provide our open-source\ncurated dataset to support further baseline testing and evaluation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u822a\u7a7a\u4e1a\u7b49\u5173\u952e\u9886\u57df\u4e2d\uff0c\u5982\u4f55\u5229\u7528\u53ef\u4fe1\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5de5\u5177\uff0c\u4ece\u7ec4\u7ec7\u6570\u636e\u4e2d\u63d0\u53d6\u8fd0\u8425\u60c5\u62a5\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5173\u5efa\u8bae\u3002", "motivation": "\u4ece\u7ec4\u7ec7\u6570\u636e\u5b58\u50a8\u5e93\u4e2d\u83b7\u53d6\u8fd0\u8425\u60c5\u62a5\u662f\u4e00\u9879\u5173\u952e\u6311\u6218\uff0c\u539f\u56e0\u5728\u4e8e\u6570\u636e\u673a\u5bc6\u6027\u4e0e\u6570\u636e\u96c6\u6210\u76ee\u6807\u4e4b\u95f4\u7684\u5bf9\u7acb\uff0c\u4ee5\u53ca\u81ea\u7136\u8bed\u8a00\u5904\u7406 (NLP) \u5de5\u5177\u76f8\u5bf9\u4e8e\u8fd0\u8425\u548c\u7ef4\u62a4\u7b49\u9886\u57df\u7684\u7279\u5b9a\u77e5\u8bc6\u7ed3\u6784\u7684\u5c40\u9650\u6027\u3002", "method": "\u8be5\u8bba\u6587\u5c06\u77e5\u8bc6\u63d0\u53d6\u8fc7\u7a0b\u5206\u89e3\u4e3a\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u5171\u6307\u6d88\u89e3\u3001\u547d\u540d\u5b9e\u4f53\u94fe\u63a5\u548c\u5173\u7cfb\u63d0\u53d6\u7b49\u529f\u80fd\u7ec4\u4ef6\uff0c\u5e76\u7ed3\u5408\u6216\u6bd4\u8f83\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5feb\u901f\u53d1\u5c55\u80fd\u529b\uff0c\u8bc4\u4f30\u4e86 16 \u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u3002", "result": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e86\u53ef\u5728\u53d7\u63a7\u3001\u4fdd\u5bc6\u73af\u5883\u4e2d\u8fd0\u884c\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5de5\u5177\u7684\u96f6\u6837\u672c\u6027\u80fd\uff08\u4e0d\u5411\u7b2c\u4e09\u65b9\u53d1\u9001\u4efb\u4f55\u6570\u636e\uff09\u3002", "conclusion": "\u8be5\u8bba\u6587\u603b\u7ed3\u4e86\u53ef\u4fe1\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5de5\u5177\u7684\u6311\u6218\uff0c\u5e76\u9488\u5bf9\u822a\u7a7a\u7b49\u5173\u952e\u4efb\u52a1\u884c\u4e1a\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u63d0\u51fa\u4e86\u6280\u672f\u51c6\u5907\u7b49\u7ea7\u5efa\u8bae\u3002\u6700\u540e\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u589e\u5f3a\u4fe1\u4efb\u7684\u5efa\u8bae\uff0c\u5e76\u63d0\u4f9b\u5176\u5f00\u6e90\u7684\u7cbe\u9009\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7684\u57fa\u7ebf\u6d4b\u8bd5\u548c\u8bc4\u4f30\u3002"}}
{"id": "2507.23245", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23245", "abs": "https://arxiv.org/abs/2507.23245", "authors": ["Lei Xie", "Jiahao Huang", "Jiawei Zhang", "Jianzhong He", "Yiang Pan", "Guoqiang Xie", "Mengjun Li", "Qingrun Zeng", "Mingchu Li", "Yuanjing Feng"], "title": "Automated Mapping the Pathways of Cranial Nerve II, III, V, and VII/VIII: A Multi-Parametric Multi-Stage Diffusion Tractography Atlas", "comment": null, "summary": "Cranial nerves (CNs) play a crucial role in various essential functions of\nthe human brain, and mapping their pathways from diffusion MRI (dMRI) provides\nvaluable preoperative insights into the spatial relationships between\nindividual CNs and key tissues. However, mapping a comprehensive and detailed\nCN atlas is challenging because of the unique anatomical structures of each CN\npair and the complexity of the skull base environment.In this work, we present\nwhat we believe to be the first study to develop a comprehensive diffusion\ntractography atlas for automated mapping of CN pathways in the human brain. The\nCN atlas is generated by fiber clustering by using the streamlines generated by\nmulti-parametric fiber tractography for each pair of CNs. Instead of disposable\nclustering, we explore a new strategy of multi-stage fiber clustering for\nmultiple analysis of approximately 1,000,000 streamlines generated from the 50\nsubjects from the Human Connectome Project (HCP). Quantitative and visual\nexperiments demonstrate that our CN atlas achieves high spatial correspondence\nwith expert manual annotations on multiple acquisition sites, including the HCP\ndataset, the Multi-shell Diffusion MRI (MDM) dataset and two clinical cases of\npituitary adenoma patients. The proposed CN atlas can automatically identify 8\nfiber bundles associated with 5 pairs of CNs, including the optic nerve CN II,\noculomotor nerve CN III, trigeminal nerve CN V and facial-vestibulocochlear\nnerve CN VII/VIII, and its robustness is demonstrated experimentally. This work\ncontributes to the field of diffusion imaging by facilitating more efficient\nand automated mapping the pathways of multiple pairs of CNs, thereby enhancing\nthe analysis and understanding of complex brain structures through\nvisualization of their spatial relationships with nearby anatomy.", "AI": {"tldr": "Developed a comprehensive diffusion tractography atlas for automated mapping of CN pathways, which achieves high spatial correspondence with expert manual annotations on multiple acquisition sites.", "motivation": "Mapping a comprehensive and detailed CN atlas is challenging because of the unique anatomical structures of each CN pair and the complexity of the skull base environment.In this work, we present what we believe to be the first study to develop a comprehensive diffusion tractography atlas for automated mapping of CN pathways in the human brain.", "method": "The CN atlas is generated by fiber clustering by using the streamlines generated by multi-parametric fiber tractography for each pair of CNs. Instead of disposable clustering, we explore a new strategy of multi-stage fiber clustering for multiple analysis of approximately 1,000,000 streamlines generated from the 50 subjects from the Human Connectome Project (HCP).", "result": "Quantitative and visual experiments demonstrate that our CN atlas achieves high spatial correspondence with expert manual annotations on multiple acquisition sites, including the HCP dataset, the Multi-shell Diffusion MRI (MDM) dataset and two clinical cases of pituitary adenoma patients.", "conclusion": "The proposed CN atlas can automatically identify 8 fiber bundles associated with 5 pairs of CNs, including the optic nerve CN II, oculomotor nerve CN III, trigeminal nerve CN V and facial-vestibulocochlear nerve CN VII/VIII, and its robustness is demonstrated experimentally. This work contributes to the field of diffusion imaging by facilitating more efficient and automated mapping the pathways of multiple pairs of CNs, thereby enhancing the analysis and understanding of complex brain structures through visualization of their spatial relationships with nearby anatomy."}}
{"id": "2507.23257", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23257", "abs": "https://arxiv.org/abs/2507.23257", "authors": ["Jiawei Liu", "Chenwang Wu", "Defu Lian", "Enhong Chen"], "title": "Efficient Machine Unlearning via Influence Approximation", "comment": "12 pages, 4 figures", "summary": "Due to growing privacy concerns, machine unlearning, which aims at enabling\nmachine learning models to ``forget\" specific training data, has received\nincreasing attention. Among existing methods, influence-based unlearning has\nemerged as a prominent approach due to its ability to estimate the impact of\nindividual training samples on model parameters without retraining. However,\nthis approach suffers from prohibitive computational overhead arising from the\nnecessity to compute the Hessian matrix and its inverse across all training\nsamples and parameters, rendering it impractical for large-scale models and\nscenarios involving frequent data deletion requests. This highlights the\ndifficulty of forgetting. Inspired by cognitive science, which suggests that\nmemorizing is easier than forgetting, this paper establishes a theoretical link\nbetween memorizing (incremental learning) and forgetting (unlearning). This\nconnection allows machine unlearning to be addressed from the perspective of\nincremental learning. Unlike the time-consuming Hessian computations in\nunlearning (forgetting), incremental learning (memorizing) typically relies on\nmore efficient gradient optimization, which supports the aforementioned\ncognitive theory. Based on this connection, we introduce the Influence\nApproximation Unlearning (IAU) algorithm for efficient machine unlearning from\nthe incremental perspective. Extensive empirical evaluations demonstrate that\nIAU achieves a superior balance among removal guarantee, unlearning efficiency,\nand comparable model utility, while outperforming state-of-the-art methods\nacross diverse datasets and model architectures. Our code is available at\nhttps://github.com/Lolo1222/IAU.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u673a\u5668\u5378\u8f7d\u7b97\u6cd5IAU\uff0c\u8be5\u7b97\u6cd5\u57fa\u4e8e\u589e\u91cf\u5b66\u4e60\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5f71\u54cd\u7684\u5378\u8f7d\u65b9\u6cd5\u9700\u8981\u8ba1\u7b97Hessian\u77e9\u9635\u53ca\u5176\u5728\u6240\u6709\u8bad\u7ec3\u6837\u672c\u548c\u53c2\u6570\u4e0a\u7684\u9006\u77e9\u9635\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u8fc7\u5927\uff0c\u5bf9\u4e8e\u5927\u578b\u6a21\u578b\u548c\u6d89\u53ca\u9891\u7e41\u6570\u636e\u5220\u9664\u8bf7\u6c42\u7684\u573a\u666f\u6765\u8bf4\u662f\u4e0d\u5207\u5b9e\u9645\u7684\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u589e\u91cf\u5b66\u4e60\u7684Influence Approximation Unlearning (IAU)\u7b97\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u7684\u673a\u5668\u5378\u8f7d\u3002", "result": "IAU\u7b97\u6cd5\u5728\u5404\u79cd\u6570\u636e\u96c6\u548c\u6a21\u578b\u67b6\u6784\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u5220\u9664\u4fdd\u8bc1\u3001\u5378\u8f7d\u6548\u7387\u548c\u6a21\u578b\u6548\u7528\u4e4b\u95f4\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u5e73\u8861\u3002", "conclusion": "IAU\u7b97\u6cd5\u5728\u5220\u9664\u4fdd\u8bc1\u3001\u5378\u8f7d\u6548\u7387\u548c\u6a21\u578b\u6548\u7528\u4e4b\u95f4\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u5e73\u8861\uff0c\u540c\u65f6\u5728\u5404\u79cd\u6570\u636e\u96c6\u548c\u6a21\u578b\u67b6\u6784\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.22936", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.HC", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2507.22936", "abs": "https://arxiv.org/abs/2507.22936", "authors": ["Md Talha Mohsin"], "title": "Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis", "comment": "22 Pages, 6 Tables, 7 Figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide variety of Financial Natural Language Processing (FinNLP) tasks.\nHowever, systematic comparisons among widely used LLMs remain underexplored.\nGiven the rapid advancement and growing influence of LLMs in financial\nanalysis, this study conducts a thorough comparative evaluation of five leading\nLLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the\n'Magnificent Seven' technology companies. We create a set of domain-specific\nprompts and then use three methodologies to evaluate model performance: human\nannotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity,\nJaccard), and model behavior diagnostics (prompt-level variance and\nacross-model similarity). The results show that GPT gives the most coherent,\nsemantically aligned, and contextually relevant answers; followed by Claude and\nPerplexity. Gemini and DeepSeek, on the other hand, have more variability and\nless agreement. Also, the similarity and stability of outputs change from\ncompany to company and over time, showing that they are sensitive to how\nprompts are written and what source material is used.", "AI": {"tldr": "This study conducts a thorough comparative evaluation of five leading LLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the 'Magnificent Seven' technology companies.", "motivation": "Systematic comparisons among widely used LLMs remain underexplored. Given the rapid advancement and growing influence of LLMs in financial analysis", "method": "We create a set of domain-specific prompts and then use three methodologies to evaluate model performance: human annotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity, Jaccard), and model behavior diagnostics (prompt-level variance and across-model similarity).", "result": "GPT gives the most coherent, semantically aligned, and contextually relevant answers; followed by Claude and Perplexity. Gemini and DeepSeek, on the other hand, have more variability and less agreement.", "conclusion": "GPT gives the most coherent, semantically aligned, and contextually relevant answers; followed by Claude and Perplexity. Gemini and DeepSeek, on the other hand, have more variability and less agreement. Also, the similarity and stability of outputs change from company to company and over time, showing that they are sensitive to how prompts are written and what source material is used."}}
{"id": "2507.23251", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23251", "abs": "https://arxiv.org/abs/2507.23251", "authors": ["Fereshteh Aghaee Meibodi", "Shadi Alijani", "Homayoun Najjaran"], "title": "A Deep Dive into Generic Object Tracking: A Survey", "comment": "55 pages, 29 figures, 9 tables", "summary": "Generic object tracking remains an important yet challenging task in computer\nvision due to complex spatio-temporal dynamics, especially in the presence of\nocclusions, similar distractors, and appearance variations. Over the past two\ndecades, a wide range of tracking paradigms, including Siamese-based trackers,\ndiscriminative trackers, and, more recently, prominent transformer-based\napproaches, have been introduced to address these challenges. While a few\nexisting survey papers in this field have either concentrated on a single\ncategory or widely covered multiple ones to capture progress, our paper\npresents a comprehensive review of all three categories, with particular\nemphasis on the rapidly evolving transformer-based methods. We analyze the core\ndesign principles, innovations, and limitations of each approach through both\nqualitative and quantitative comparisons. Our study introduces a novel\ncategorization and offers a unified visual and tabular comparison of\nrepresentative methods. Additionally, we organize existing trackers from\nmultiple perspectives and summarize the major evaluation benchmarks,\nhighlighting the fast-paced advancements in transformer-based tracking driven\nby their robust spatio-temporal modeling capabilities.", "AI": {"tldr": "\u672c\u6587\u5bf9\u901a\u7528\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u7684\u56de\u987e\uff0c\u91cd\u70b9\u5173\u6ce8\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u5176\u8bbe\u8ba1\u539f\u5219\u3001\u521b\u65b0\u548c\u5c40\u9650\u6027\u3002", "motivation": "\u7531\u4e8e\u590d\u6742\u7684\u65f6\u7a7a\u52a8\u6001\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u906e\u6321\u3001\u76f8\u4f3c\u5e72\u6270\u7269\u548c\u5916\u89c2\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u7528\u76ee\u6807\u8ddf\u8e2a\u4ecd\u7136\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u4e00\u9879\u91cd\u8981\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002", "method": "\u5bf9\u6240\u6709\u4e09\u4e2a\u7c7b\u522b\u8fdb\u884c\u5168\u9762\u7684\u56de\u987e\uff0c\u7279\u522b\u5f3a\u8c03\u5feb\u901f\u53d1\u5c55\u7684\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u5b9a\u6027\u548c\u5b9a\u91cf\u6bd4\u8f83\uff0c\u5206\u6790\u6bcf\u79cd\u65b9\u6cd5\u7684\u6838\u5fc3\u8bbe\u8ba1\u539f\u5219\u3001\u521b\u65b0\u548c\u5c40\u9650\u6027\u3002\u8bba\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u4ee3\u8868\u6027\u65b9\u6cd5\u7684\u7edf\u4e00\u89c6\u89c9\u548c\u8868\u683c\u6bd4\u8f83\u3002", "result": "\u8bba\u6587\u5bf9\u4ee3\u8868\u6027\u65b9\u6cd5\u8fdb\u884c\u4e86\u7edf\u4e00\u7684\u89c6\u89c9\u548c\u8868\u683c\u6bd4\u8f83\u3002\u603b\u7ed3\u4e86\u4e3b\u8981\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u5f3a\u8c03\u4e86\u57fa\u4e8eTransformer\u7684\u8ddf\u8e2a\u6280\u672f\u5728\u5f3a\u5927\u7684\u65f6\u7a7a\u5efa\u6a21\u80fd\u529b\u7684\u9a71\u52a8\u4e0b\u7684\u5feb\u901f\u53d1\u5c55\u3002", "conclusion": "\u672c\u6587\u5168\u9762\u56de\u987e\u4e86\u57fa\u4e8eSiamese\u7684\u8ddf\u8e2a\u5668\u3001\u5224\u522b\u5f0f\u8ddf\u8e2a\u5668\u4ee5\u53ca\u6700\u8fd1\u51fa\u73b0\u7684\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u8fd9\u4e09\u79cd\u7c7b\u522b\uff0c\u7279\u522b\u5f3a\u8c03\u4e86\u5feb\u901f\u53d1\u5c55\u7684\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u5b9a\u6027\u548c\u5b9a\u91cf\u6bd4\u8f83\uff0c\u5206\u6790\u4e86\u6bcf\u79cd\u65b9\u6cd5\u7684\u6838\u5fc3\u8bbe\u8ba1\u539f\u5219\u3001\u521b\u65b0\u548c\u5c40\u9650\u6027\u3002\u603b\u7ed3\u4e86\u4e3b\u8981\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u5f3a\u8c03\u4e86\u57fa\u4e8eTransformer\u7684\u8ddf\u8e2a\u6280\u672f\u5728\u5f3a\u5927\u7684\u65f6\u7a7a\u5efa\u6a21\u80fd\u529b\u7684\u9a71\u52a8\u4e0b\u7684\u5feb\u901f\u53d1\u5c55\u3002"}}
{"id": "2507.23261", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.23261", "abs": "https://arxiv.org/abs/2507.23261", "authors": ["Hui Yi Leong", "Yuqing Wu"], "title": "DynaSwarm: Dynamically Graph Structure Selection for LLM-based Multi-agent System", "comment": null, "summary": "Current multi-agent systems (MAS) frameworks often rely on manually designed\nand static collaboration graph structures, limiting adaptability and\nperformance. To address these limitations, we propose DynaSwarm, a dynamic\nframework that enhances LLM-based MAS through two key innovations: (1) an\nactor-critic reinforcement learning (A2C) mechanism to optimize graph\nstructures with improved stability over prior RL methods, and (2) a dynamic\ngraph selector that adaptively chooses the optimal graph structure for each\ninput sample via parameter-efficient LLM fine-tuning. DynaSwarm eliminates the\nneed for rigid, one-fits-all graph architectures, instead leveraging\nsample-specific idiosyncrasies to dynamically route queries through specialized\nagent networks. (c) We propose to fine-tune the demonstration retriever to\nfully exploit the power of in-context learning (ICL). Extensive experiments on\nquestion answering, mathematical reasoning, and coding tasks demonstrate that\nDynaSwarm consistently outperforms state-of-the-art single-agent and MAS\nbaselines across multiple LLM backbones. Our findings highlight the importance\nof sample-aware structural flexibility in LLM MAS designs.", "AI": {"tldr": "DynaSwarm is a dynamic framework that enhances LLM-based MAS through an actor-critic reinforcement learning mechanism to optimize graph structures and a dynamic graph selector, which outperforms state-of-the-art baselines.", "motivation": "Current multi-agent systems (MAS) frameworks often rely on manually designed and static collaboration graph structures, limiting adaptability and performance.", "method": "actor-critic reinforcement learning (A2C) mechanism to optimize graph structures and a dynamic graph selector that adaptively chooses the optimal graph structure for each input sample via parameter-efficient LLM fine-tuning", "result": "DynaSwarm eliminates the need for rigid, one-fits-all graph architectures, instead leveraging sample-specific idiosyncrasies to dynamically route queries through specialized agent networks. DynaSwarm consistently outperforms state-of-the-art single-agent and MAS baselines across multiple LLM backbones.", "conclusion": "DynaSwarm consistently outperforms state-of-the-art single-agent and MAS baselines across multiple LLM backbones, highlighting the importance of sample-aware structural flexibility in LLM MAS designs."}}
{"id": "2507.22937", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22937", "abs": "https://arxiv.org/abs/2507.22937", "authors": ["Jinkun Zhao", "Yuanshuai Wang", "Xingjian Zhang", "Ruibo Chen", "Xingchuang Liao", "Junle Wang", "Lei Huang", "Kui Zhang", "Wenjun Wu"], "title": "CoE-Ops: Collaboration of LLM-based Experts for AIOps Question-Answering", "comment": null, "summary": "With the rapid evolution of artificial intelligence, AIOps has emerged as a\nprominent paradigm in DevOps. Lots of work has been proposed to improve the\nperformance of different AIOps phases. However, constrained by domain-specific\nknowledge, a single model can only handle the operation requirement of a\nspecific task,such as log parser,root cause analysis. Meanwhile, combining\nmultiple models can achieve more efficient results, which have been proved in\nboth previous ensemble learning and the recent LLM training domain. Inspired by\nthese works,to address the similar challenges in AIOPS, this paper first\nproposes a collaboration-of-expert framework(CoE-Ops) incorporating a\ngeneral-purpose large language model task classifier. A retrieval-augmented\ngeneration mechanism is introduced to improve the framework's capability in\nhandling both Question-Answering tasks with high-level(Code,build,Test,etc.)\nand low-level(fault analysis,anomaly detection,etc.). Finally, the proposed\nmethod is implemented in the AIOps domain, and extensive experiments are\nconducted on the DevOps-EVAL dataset. Experimental results demonstrate that\nCoE-Ops achieves a 72% improvement in routing accuracy for high-level AIOps\ntasks compared to existing CoE methods, delivers up to 8% accuracy enhancement\nover single AIOps models in DevOps problem resolution, and outperforms\nlarger-scale Mixture-of-Experts (MoE) models by up to 14% in accuracy.", "AI": {"tldr": "This paper proposes CoE-Ops, a collaboration-of-expert framework that improves AIOps performance by incorporating a large language model task classifier and a retrieval-augmented generation mechanism.", "motivation": "constrained by domain-specific knowledge, a single model can only handle the operation requirement of a specific task. combining multiple models can achieve more efficient results", "method": "proposes a collaboration-of-expert framework(CoE-Ops) incorporating a general-purpose large language model task classifier. A retrieval-augmented generation mechanism is introduced", "result": "CoE-Ops achieves a 72% improvement in routing accuracy for high-level AIOps tasks compared to existing CoE methods, delivers up to 8% accuracy enhancement over single AIOps models in DevOps problem resolution, and outperforms larger-scale Mixture-of-Experts (MoE) models by up to 14% in accuracy.", "conclusion": "CoE-Ops achieves a 72% improvement in routing accuracy for high-level AIOps tasks compared to existing CoE methods, delivers up to 8% accuracy enhancement over single AIOps models in DevOps problem resolution, and outperforms larger-scale Mixture-of-Experts (MoE) models by up to 14% in accuracy."}}
{"id": "2507.23253", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23253", "abs": "https://arxiv.org/abs/2507.23253", "authors": ["Mingyang Yu", "Xiahui Guo", "Peng chen", "Zhenkai Li", "Yang Shu"], "title": "Towards Measuring and Modeling Geometric Structures in Time Series Forecasting via Image Modality", "comment": null, "summary": "Time Series forecasting is critical in diverse domains such as weather\nforecasting, financial investment, and traffic management. While traditional\nnumerical metrics like mean squared error (MSE) can quantify point-wise\naccuracy, they fail to evaluate the geometric structure of time series data,\nwhich is essential to understand temporal dynamics. To address this issue, we\npropose the time series Geometric Structure Index (TGSI), a novel evaluation\nmetric that transforms time series into images to leverage their inherent\ntwo-dimensional geometric representations. However, since the image\ntransformation process is non-differentiable, TGSI cannot be directly\nintegrated as a training loss. We further introduce the Shape-Aware Temporal\nLoss (SATL), a multi-component loss function operating in the time series\nmodality to bridge this gap and enhance structure modeling during training.\nSATL combines three components: a first-order difference loss that measures\nstructural consistency through the MSE between first-order differences, a\nfrequency domain loss that captures essential periodic patterns using the Fast\nFourier Transform while minimizing noise, and a perceptual feature loss that\nmeasures geometric structure difference in time-series by aligning temporal\nfeatures with geometric structure features through a pre-trained temporal\nfeature extractor and time-series image autoencoder. Experiments across\nmultiple datasets demonstrate that models trained with SATL achieve superior\nperformance in both MSE and the proposed TGSI metrics compared to baseline\nmethods, without additional computational cost during inference.", "AI": {"tldr": "This paper introduces TGSI, a novel evaluation metric for time series data, and SATL, a multi-component loss function to enhance structure modeling during training. Models trained with SATL achieve superior performance in both MSE and the proposed TGSI metrics compared to baseline methods, without additional computational cost during inference.", "motivation": "Traditional numerical metrics like mean squared error (MSE) fail to evaluate the geometric structure of time series data, which is essential to understand temporal dynamics.", "method": "We propose the time series Geometric Structure Index (TGSI), a novel evaluation metric that transforms time series into images to leverage their inherent two-dimensional geometric representations. We further introduce the Shape-Aware Temporal Loss (SATL), a multi-component loss function operating in the time series modality to bridge this gap and enhance structure modeling during training. SATL combines three components: a first-order difference loss, a frequency domain loss, and a perceptual feature loss.", "result": "Models trained with SATL achieve superior performance in both MSE and the proposed TGSI metrics compared to baseline methods, without additional computational cost during inference.", "conclusion": "Models trained with SATL achieve superior performance in both MSE and the proposed TGSI metrics compared to baseline methods, without additional computational cost during inference."}}
{"id": "2507.23291", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23291", "abs": "https://arxiv.org/abs/2507.23291", "authors": ["Yuetian Chen", "Zhiqi Wang", "Nathalie Baracaldo", "Swanand Ravindra Kadhe", "Lei Yu"], "title": "Evaluating the Dynamics of Membership Privacy in Deep Learning", "comment": null, "summary": "Membership inference attacks (MIAs) pose a critical threat to the privacy of\ntraining data in deep learning. Despite significant progress in attack\nmethodologies, our understanding of when and how models encode membership\ninformation during training remains limited. This paper presents a dynamic\nanalytical framework for dissecting and quantifying privacy leakage dynamics at\nthe individual sample level. By tracking per-sample vulnerabilities on an\nFPR-TPR plane throughout training, our framework systematically measures how\nfactors such as dataset complexity, model architecture, and optimizer choice\ninfluence the rate and severity at which samples become vulnerable. Crucially,\nwe discover a robust correlation between a sample's intrinsic learning\ndifficulty, and find that the privacy risk of samples highly vulnerable in the\nfinal trained model is largely determined early during training. Our results\nthus provide a deeper understanding of how privacy risks dynamically emerge\nduring training, laying the groundwork for proactive, privacy-aware model\ntraining strategies.", "AI": {"tldr": "This paper presents a dynamic analytical framework for dissecting and quantifying privacy leakage dynamics at the individual sample level and discovers a robust correlation between a sample's intrinsic learning difficulty, and find that the privacy risk of samples highly vulnerable in the final trained model is largely determined early during training.", "motivation": "understanding of when and how models encode membership information during training remains limited. Membership inference attacks (MIAs) pose a critical threat to the privacy of training data in deep learning. Despite significant progress in attack methodologies", "method": "a dynamic analytical framework for dissecting and quantifying privacy leakage dynamics at the individual sample level. By tracking per-sample vulnerabilities on an FPR-TPR plane throughout training, our framework systematically measures how factors such as dataset complexity, model architecture, and optimizer choice influence the rate and severity at which samples become vulnerable.", "result": "a robust correlation between a sample's intrinsic learning difficulty", "conclusion": "The privacy risk of samples highly vulnerable in the final trained model is largely determined early during training."}}
{"id": "2507.22938", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.22938", "abs": "https://arxiv.org/abs/2507.22938", "authors": ["Sumit Soman", "H. G. Ranjani", "Sujoy Roychowdhury", "Venkata Dharma Surya Narayana Sastry", "Akshat Jain", "Pranav Gangrade", "Ayaaz Khan"], "title": "A Graph-based Approach for Multi-Modal Question Answering from Flowcharts in Telecom Documents", "comment": "Accepted for publication at the KDD 2025 Workshop on Structured\n  Knowledge for Large Language Models", "summary": "Question-Answering (QA) from technical documents often involves questions\nwhose answers are present in figures, such as flowcharts or flow diagrams.\nText-based Retrieval Augmented Generation (RAG) systems may fail to answer such\nquestions. We leverage graph representations of flowcharts obtained from Visual\nlarge Language Models (VLMs) and incorporate them in a text-based RAG system to\nshow that this approach can enable image retrieval for QA in the telecom\ndomain. We present the end-to-end approach from processing technical documents,\nclassifying image types, building graph representations, and incorporating them\nwith the text embedding pipeline for efficient retrieval. We benchmark the same\non a QA dataset created based on proprietary telecom product information\ndocuments. Results show that the graph representations obtained using a\nfine-tuned VLM model have lower edit distance with respect to the ground truth,\nwhich illustrate the robustness of these representations for flowchart images.\nFurther, the approach for QA using these representations gives good retrieval\nperformance using text-based embedding models, including a telecom-domain\nadapted one. Our approach also alleviates the need for a VLM in inference,\nwhich is an important cost benefit for deployed QA systems.", "AI": {"tldr": "\u6211\u4eec\u5229\u7528\u6d41\u7a0b\u56fe\u7684\u56fe\u8868\u793a\uff0c\u5e76\u5c06\u5176\u7eb3\u5165\u57fa\u4e8e\u6587\u672c\u7684 RAG \u7cfb\u7edf\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u56fe\u50cf\u68c0\u7d22\u3002", "motivation": "\u4ece\u6280\u672f\u6587\u6863\u4e2d\u8fdb\u884c\u95ee\u7b54 (QA) \u901a\u5e38\u6d89\u53ca\u7b54\u6848\u5b58\u5728\u4e8e\u56fe\u5f62\uff08\u4f8b\u5982\u6d41\u7a0b\u56fe\u6216\u6d41\u7a0b\u56fe\uff09\u4e2d\u7684\u95ee\u9898\u3002\u57fa\u4e8e\u6587\u672c\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG) \u7cfb\u7edf\u53ef\u80fd\u65e0\u6cd5\u56de\u7b54\u6b64\u7c7b\u95ee\u9898\u3002", "method": "\u5229\u7528\u4ece\u89c6\u89c9\u5927\u578b\u8bed\u8a00\u6a21\u578b (VLM) \u83b7\u5f97\u7684\u6d41\u7a0b\u56fe\u7684\u56fe\u8868\u793a\uff0c\u5e76\u5c06\u5176\u7eb3\u5165\u57fa\u4e8e\u6587\u672c\u7684 RAG \u7cfb\u7edf\u4e2d\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u5fae\u8c03\u7684 VLM \u6a21\u578b\u83b7\u5f97\u7684\u56fe\u8868\u793a\u76f8\u5bf9\u4e8eground truth\u5177\u6709\u8f83\u4f4e\u7684\u7f16\u8f91\u8ddd\u79bb\uff0c\u8fd9\u8bf4\u660e\u4e86\u8fd9\u4e9b\u8868\u793a\u5bf9\u4e8e\u6d41\u7a0b\u56fe\u56fe\u50cf\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u57fa\u4e8e\u6587\u672c\u7684\u5d4c\u5165\u6a21\u578b\uff0c\u5305\u62ec\u7535\u4fe1\u9886\u57df\u8c03\u6574\u540e\u7684\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u68c0\u7d22\u6027\u80fd\uff0c\u5e76\u4e14\u51cf\u8f7b\u4e86\u63a8\u7406\u4e2d\u5bf9 VLM \u7684\u9700\u6c42\uff0c\u8fd9\u5bf9\u4e8e\u90e8\u7f72\u7684 QA \u7cfb\u7edf\u6765\u8bf4\u662f\u4e00\u9879\u91cd\u8981\u7684\u6210\u672c\u4f18\u52bf\u3002"}}
{"id": "2507.23263", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23263", "abs": "https://arxiv.org/abs/2507.23263", "authors": ["Haoxian Ruan", "Zhihua Xu", "Zhijing Yang", "Guang Ma", "Jieming Xie", "Changxiang Fan", "Tianshui Chen"], "title": "Learning Semantic-Aware Threshold for Multi-Label Image Recognition with Partial Labels", "comment": "15 pages, 13 figures, publish to ESWA (Expert Systems With\n  Applications)", "summary": "Multi-label image recognition with partial labels (MLR-PL) is designed to\ntrain models using a mix of known and unknown labels. Traditional methods rely\non semantic or feature correlations to create pseudo-labels for unidentified\nlabels using pre-set thresholds. This approach often overlooks the varying\nscore distributions across categories, resulting in inaccurate and incomplete\npseudo-labels, thereby affecting performance. In our study, we introduce the\nSemantic-Aware Threshold Learning (SATL) algorithm. This innovative approach\ncalculates the score distribution for both positive and negative samples within\neach category and determines category-specific thresholds based on these\ndistributions. These distributions and thresholds are dynamically updated\nthroughout the learning process. Additionally, we implement a differential\nranking loss to establish a significant gap between the score distributions of\npositive and negative samples, enhancing the discrimination of the thresholds.\nComprehensive experiments and analysis on large-scale multi-label datasets,\nsuch as Microsoft COCO and VG-200, demonstrate that our method significantly\nimproves performance in scenarios with limited labels.", "AI": {"tldr": "This paper introduces the Semantic-Aware Threshold Learning (SATL) algorithm for multi-label image recognition with partial labels, which dynamically learns category-specific thresholds and improves performance, especially when labels are limited.", "motivation": "Traditional methods for multi-label image recognition with partial labels (MLR-PL) often overlook the varying score distributions across categories, resulting in inaccurate and incomplete pseudo-labels, thereby affecting performance.", "method": "The paper introduces a Semantic-Aware Threshold Learning (SATL) algorithm that calculates score distributions for positive and negative samples within each category and determines category-specific thresholds based on these distributions. A differential ranking loss is implemented to enhance the discrimination of the thresholds. The distributions and thresholds are dynamically updated throughout the learning process.", "result": "The proposed method significantly improves performance in scenarios with limited labels on large-scale multi-label datasets, such as Microsoft COCO and VG-200.", "conclusion": "The proposed Semantic-Aware Threshold Learning (SATL) algorithm significantly improves performance in multi-label image recognition with partial labels, especially in scenarios with limited labels, as demonstrated on large-scale datasets like Microsoft COCO and VG-200."}}
{"id": "2507.23292", "categories": ["cs.LG", "cs.CL", "cs.PL", "cs.SE", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.23292", "abs": "https://arxiv.org/abs/2507.23292", "authors": ["RJ Skerry-Ryan", "Julian Salazar", "Soroosh Mariooryad", "David Kao", "Daisy Stanton", "Eric Battenberg", "Matt Shannon", "Ron J. Weiss", "Robin Scheibler", "Jonas Rothfuss", "Tom Bagby"], "title": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made Easy", "comment": null, "summary": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.", "AI": {"tldr": "Introduces SequenceLayers, a neural network layer API and library for sequence modeling, designed for easy creation of sequence models that can be executed both layer-by-layer and step-by-step.", "motivation": "The paper aims to provide an easy creation of sequence models that can be executed both layer-by-layer and step-by-step.", "method": "The paper introduces a neural network layer API and library for sequence modeling where layers define an explicit representation of their state over time and a step method that evolves that state.", "result": "The SequenceLayers implementations (JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.", "conclusion": "SequenceLayers enables complex models to be immediately streamable, mitigates a wide range of common bugs, and can be implemented in any deep learning library, streamlining the construction of production-scale models from simple streamable components while preserving strong correctness guarantees."}}
{"id": "2507.22939", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22939", "abs": "https://arxiv.org/abs/2507.22939", "authors": ["Bastien Le Guellec", "Kokou Adambounou", "Lisa C Adams", "Thibault Agripnidis", "Sung Soo Ahn", "Radhia Ait Chalal", "Tugba Akinci D Antonoli", "Philippe Amouyel", "Henrik Andersson", "Raphael Bentegeac", "Claudio Benzoni", "Antonino Andrea Blandino", "Felix Busch", "Elif Can", "Riccardo Cau", "Armando Ugo Cavallo", "Christelle Chavihot", "Erwin Chiquete", "Renato Cuocolo", "Eugen Divjak", "Gordana Ivanac", "Barbara Dziadkowiec Macek", "Armel Elogne", "Salvatore Claudio Fanni", "Carlos Ferrarotti", "Claudia Fossataro", "Federica Fossataro", "Katarzyna Fulek", "Michal Fulek", "Pawel Gac", "Martyna Gachowska", "Ignacio Garcia Juarez", "Marco Gatti", "Natalia Gorelik", "Alexia Maria Goulianou", "Aghiles Hamroun", "Nicolas Herinirina", "Krzysztof Kraik", "Dominik Krupka", "Quentin Holay", "Felipe Kitamura", "Michail E Klontzas", "Anna Kompanowska", "Rafal Kompanowski", "Alexandre Lefevre", "Tristan Lemke", "Maximilian Lindholz", "Lukas Muller", "Piotr Macek", "Marcus Makowski", "Luigi Mannacio", "Aymen Meddeb", "Antonio Natale", "Beatrice Nguema Edzang", "Adriana Ojeda", "Yae Won Park", "Federica Piccione", "Andrea Ponsiglione", "Malgorzata Poreba", "Rafal Poreba", "Philipp Prucker", "Jean Pierre Pruvo", "Rosa Alba Pugliesi", "Feno Hasina Rabemanorintsoa", "Vasileios Rafailidis", "Katarzyna Resler", "Jan Rotkegel", "Luca Saba", "Ezann Siebert", "Arnaldo Stanzione", "Ali Fuat Tekin", "Liz Toapanta Yanchapaxi", "Matthaios Triantafyllou", "Ekaterini Tsaoulia", "Evangelia Vassalou", "Federica Vernuccio", "Johan Wasselius", "Weilang Wang", "Szymon Urban", "Adrian Wlodarczak", "Szymon Wlodarczak", "Andrzej Wysocki", "Lina Xu", "Tomasz Zatonski", "Shuhang Zhang", "Sebastian Ziegelmayer", "Gregory Kuchcinski", "Keno K Bressem"], "title": "PARROT: An Open Multilingual Radiology Reports Dataset", "comment": null, "summary": "Rationale and Objectives: To develop and validate PARROT (Polyglottal\nAnnotated Radiology Reports for Open Testing), a large, multicentric,\nopen-access dataset of fictional radiology reports spanning multiple languages\nfor testing natural language processing applications in radiology. Materials\nand Methods: From May to September 2024, radiologists were invited to\ncontribute fictional radiology reports following their standard reporting\npractices. Contributors provided at least 20 reports with associated metadata\nincluding anatomical region, imaging modality, clinical context, and for\nnon-English reports, English translations. All reports were assigned ICD-10\ncodes. A human vs. AI report differentiation study was conducted with 154\nparticipants (radiologists, healthcare professionals, and non-healthcare\nprofessionals) assessing whether reports were human-authored or AI-generated.\nResults: The dataset comprises 2,658 radiology reports from 76 authors across\n21 countries and 13 languages. Reports cover multiple imaging modalities (CT:\n36.1%, MRI: 22.8%, radiography: 19.0%, ultrasound: 16.8%) and anatomical\nregions, with chest (19.9%), abdomen (18.6%), head (17.3%), and pelvis (14.1%)\nbeing most prevalent. In the differentiation study, participants achieved 53.9%\naccuracy (95% CI: 50.7%-57.1%) in distinguishing between human and AI-generated\nreports, with radiologists performing significantly better (56.9%, 95% CI:\n53.3%-60.6%, p<0.05) than other groups. Conclusion: PARROT represents the\nlargest open multilingual radiology report dataset, enabling development and\nvalidation of natural language processing applications across linguistic,\ngeographic, and clinical boundaries without privacy constraints.", "AI": {"tldr": "PARROT: A large, open, multilingual radiology report dataset for NLP application testing.", "motivation": "To develop and validate PARROT (Polyglottal Annotated Radiology Reports for Open Testing), a large, multicentric, open-access dataset of fictional radiology reports spanning multiple languages for testing natural language processing applications in radiology.", "method": "radiologists were invited to contribute fictional radiology reports following their standard reporting practices. Contributors provided at least 20 reports with associated metadata including anatomical region, imaging modality, clinical context, and for non-English reports, English translations. All reports were assigned ICD-10 codes. A human vs. AI report differentiation study was conducted with 154 participants (radiologists, healthcare professionals, and non-healthcare professionals) assessing whether reports were human-authored or AI-generated.", "result": "The dataset comprises 2,658 radiology reports from 76 authors across 21 countries and 13 languages. Reports cover multiple imaging modalities (CT: 36.1%, MRI: 22.8%, radiography: 19.0%, ultrasound: 16.8%) and anatomical regions, with chest (19.9%), abdomen (18.6%), head (17.3%), and pelvis (14.1%) being most prevalent. In the differentiation study, participants achieved 53.9% accuracy (95% CI: 50.7%-57.1%) in distinguishing between human and AI-generated reports, with radiologists performing significantly better (56.9%, 95% CI: 53.3%-60.6%, p<0.05) than other groups.", "conclusion": "PARROT represents the largest open multilingual radiology report dataset, enabling development and validation of natural language processing applications across linguistic, geographic, and clinical boundaries without privacy constraints."}}
{"id": "2507.23268", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23268", "abs": "https://arxiv.org/abs/2507.23268", "authors": ["Shuai Wang", "Ziteng Gao", "Chenhui Zhu", "Weilin Huang", "Limin Wang"], "title": "PixNerd: Pixel Neural Field Diffusion", "comment": "a single-scale, single-stage, efficient, end-to-end pixel space\n  diffusion model", "summary": "The current success of diffusion transformers heavily depends on the\ncompressed latent space shaped by the pre-trained variational autoencoder(VAE).\nHowever, this two-stage training paradigm inevitably introduces accumulated\nerrors and decoding artifacts. To address the aforementioned problems,\nresearchers return to pixel space at the cost of complicated cascade pipelines\nand increased token complexity. In contrast to their efforts, we propose to\nmodel the patch-wise decoding with neural field and present a single-scale,\nsingle-stage, efficient, end-to-end solution, coined as pixel neural field\ndiffusion~(PixelNerd). Thanks to the efficient neural field representation in\nPixNerd, we directly achieved 2.15 FID on ImageNet $256\\times256$ and 2.84 FID\non ImageNet $512\\times512$ without any complex cascade pipeline or VAE. We also\nextend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16\nachieved a competitive 0.73 overall score on the GenEval benchmark and 80.9\noverall score on the DPG benchmark.", "AI": {"tldr": "This paper proposes PixelNerd, a single-stage diffusion model using neural fields, which avoids the limitations of VAE-based approaches and achieves strong image generation results.", "motivation": "The current success of diffusion transformers heavily depends on the compressed latent space shaped by the pre-trained variational autoencoder(VAE). However, this two-stage training paradigm inevitably introduces accumulated errors and decoding artifacts. To address the aforementioned problems, researchers return to pixel space at the cost of complicated cascade pipelines and increased token complexity.", "method": "The paper models patch-wise decoding with neural fields.", "result": "PixelNerd achieved 2.15 FID on ImageNet $256\\times256$ and 2.84 FID on ImageNet $512\\times512$ without any complex cascade pipeline or VAE. PixNerd-XXL/16 achieved a competitive 0.73 overall score on the GenEval benchmark and 80.9 overall score on the DPG benchmark.", "conclusion": "The paper introduces PixelNerd, a single-stage, efficient, end-to-end diffusion model using neural fields for patch-wise decoding, achieving competitive results on image generation and text-to-image tasks."}}
{"id": "2507.23303", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23303", "abs": "https://arxiv.org/abs/2507.23303", "authors": ["Luca Corbucci", "Javier Alejandro Borges Legrottaglie", "Francesco Spinnato", "Anna Monreale", "Riccardo Guidotti"], "title": "An Interpretable Data-Driven Unsupervised Approach for the Prevention of Forgotten Items", "comment": null, "summary": "Accurately identifying items forgotten during a supermarket visit and\nproviding clear, interpretable explanations for recommending them remains an\nunderexplored problem within the Next Basket Prediction (NBP) domain. Existing\nNBP approaches typically only focus on forecasting future purchases, without\nexplicitly addressing the detection of unintentionally omitted items. This gap\nis partly due to the scarcity of real-world datasets that allow for the\nreliable estimation of forgotten items. Furthermore, most current NBP methods\nrely on black-box models, which lack transparency and limit the ability to\njustify recommendations to end users. In this paper, we formally introduce the\nforgotten item prediction task and propose two novel interpretable-by-design\nalgorithms. These methods are tailored to identify forgotten items while\noffering intuitive, human-understandable explanations. Experiments on a\nreal-world retail dataset show our algorithms outperform state-of-the-art NBP\nbaselines by 10-15% across multiple evaluation metrics.", "AI": {"tldr": "This paper introduces the forgotten item prediction task and proposes interpretable algorithms that outperform existing methods in identifying forgotten items during supermarket visits.", "motivation": "Accurately identifying forgotten items and explaining recommendations is underexplored in Next Basket Prediction (NBP). Existing NBP approaches don't detect unintentionally omitted items due to data scarcity and reliance on black-box models.", "method": "Two novel interpretable-by-design algorithms are proposed to identify forgotten items.", "result": "Algorithms outperform state-of-the-art NBP baselines by 10-15%.", "conclusion": "The proposed algorithms outperform state-of-the-art NBP baselines by 10-15% on a real-world retail dataset."}}
{"id": "2507.22940", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22940", "abs": "https://arxiv.org/abs/2507.22940", "authors": ["Rui Jiao", "Yue Zhang", "Jinku Li"], "title": "Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM Intermediate Thought Processes", "comment": null, "summary": "We present RELIANCE (Reasoning Evaluation with Logical Integrity and Accuracy\nfor Confidence Enhancement), a novel framework addressing a critical\nvulnerability in Large Language Models (LLMs): the prevalence of factual\ninaccuracies within intermediate reasoning steps despite correct final answers.\nThis phenomenon poses substantial risks in high-stakes domains including\nhealthcare, legal analysis, and scientific research, where erroneous yet\nconfidently presented reasoning can mislead users into dangerous decisions. Our\nframework integrates three core components: (1) a specialized fact-checking\nclassifier trained on counterfactually augmented data to detect subtle factual\ninconsistencies within reasoning chains; (2) a Group Relative Policy\nOptimization (GRPO) reinforcement learning approach that balances factuality,\ncoherence, and structural correctness through multi-dimensional rewards; and\n(3) a mechanistic interpretability module examining how factuality improvements\nmanifest in model activations during reasoning processes. Extensive evaluation\nacross ten state-of-the-art models reveals concerning patterns: even leading\nmodels like Claude-3.7 and GPT-o1 demonstrate reasoning factual accuracy of\nonly 81.93% and 82.57% respectively. RELIANCE significantly enhances factual\nrobustness (up to 49.90% improvement) while maintaining or improving\nperformance on challenging benchmarks including Math-500, AIME-2024, and GPQA.\nFurthermore, our activation-level analysis provides actionable insights into\nhow factual enhancements reshape reasoning trajectories within model\narchitectures, establishing foundations for future training methodologies that\nexplicitly target factual robustness through activation-guided optimization.", "AI": {"tldr": "RELIANCE\u6846\u67b6\u901a\u8fc7\u4e8b\u5b9e\u68c0\u67e5\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u53ef\u89e3\u91ca\u6027\u5206\u6790\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u6216\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u4e2d\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u7684\u6f0f\u6d1e\uff1a\u5c3d\u7ba1\u6700\u7ec8\u7b54\u6848\u6b63\u786e\uff0c\u4f46\u5728\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u4e2d\u666e\u904d\u5b58\u5728\u4e8b\u5b9e\u4e0d\u51c6\u786e\u7684\u60c5\u51b5\u3002\u8fd9\u79cd\u73b0\u8c61\u5728\u9ad8\u98ce\u9669\u9886\u57df(\u5305\u62ec\u533b\u7597\u4fdd\u5065\u3001\u6cd5\u5f8b\u5206\u6790\u548c\u79d1\u5b66\u7814\u7a76)\u4e2d\u6784\u6210\u4e86\u91cd\u5927\u98ce\u9669\uff0c\u5728\u8fd9\u4e9b\u9886\u57df\u4e2d\uff0c\u9519\u8bef\u4f46\u81ea\u4fe1\u5730\u5448\u73b0\u7684\u63a8\u7406\u53ef\u80fd\u4f1a\u8bef\u5bfc\u7528\u6237\u505a\u51fa\u5371\u9669\u7684\u51b3\u5b9a\u3002", "method": "RELIANCE\u6846\u67b6\u6574\u5408\u4e86\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1)\u4e00\u4e2a\u4e13\u95e8\u7684\u4e8b\u5b9e\u68c0\u67e5\u5206\u7c7b\u5668\uff0c\u8be5\u5206\u7c7b\u5668\u7ecf\u8fc7\u53cd\u4e8b\u5b9e\u589e\u5f3a\u6570\u636e\u7684\u8bad\u7ec3\uff0c\u4ee5\u68c0\u6d4b\u63a8\u7406\u94fe\u4e2d\u7ec6\u5fae\u7684\u4e8b\u5b9e\u4e0d\u4e00\u81f4\uff1b(2)\u4e00\u79cd\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316(GRPO)\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u591a\u7ef4\u5956\u52b1\u6765\u5e73\u8861\u4e8b\u5b9e\u6027\u3001\u8fde\u8d2f\u6027\u548c\u7ed3\u6784\u6b63\u786e\u6027\uff1b(3)\u4e00\u4e2a\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u6a21\u5757\uff0c\u7528\u4e8e\u68c0\u67e5\u4e8b\u5b9e\u6027\u6539\u8fdb\u5982\u4f55\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u6a21\u578b\u6fc0\u6d3b\u4e2d\u4f53\u73b0\u51fa\u6765\u3002", "result": "\u5728\u5341\u4e2a\u6700\u5148\u8fdb\u7684\u6a21\u578b\u4e2d\u8fdb\u884c\u7684\u5e7f\u6cdb\u8bc4\u4f30\u63ed\u793a\u4e86\u4ee4\u4eba\u62c5\u5fe7\u7684\u6a21\u5f0f\uff1a\u5373\u4f7f\u662f\u50cfClaude-3.7\u548cGPT-o1\u8fd9\u6837\u7684\u9886\u5148\u6a21\u578b\uff0c\u5176\u63a8\u7406\u4e8b\u5b9e\u51c6\u786e\u7387\u4e5f\u5206\u522b\u53ea\u670981.93%\u548c82.57%\u3002", "conclusion": "RELIANCE\u663e\u8457\u63d0\u9ad8\u4e86\u4e8b\u5b9e\u7684\u7a33\u5065\u6027\uff08\u63d0\u9ad8\u4e86\u9ad8\u8fbe49.90%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u4e86\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ecMath-500\u3001AIME-2024\u548cGPQA\uff09\u4e0a\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6fc0\u6d3b\u6c34\u5e73\u5206\u6790\u63d0\u4f9b\u4e86\u5173\u4e8e\u4e8b\u5b9e\u589e\u5f3a\u5982\u4f55\u91cd\u5851\u6a21\u578b\u67b6\u6784\u4e2d\u7684\u63a8\u7406\u8f68\u8ff9\u7684\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u4e3a\u672a\u6765\u901a\u8fc7\u6fc0\u6d3b\u5f15\u5bfc\u4f18\u5316\u660e\u786e\u9488\u5bf9\u4e8b\u5b9e\u7a33\u5065\u6027\u7684\u8bad\u7ec3\u65b9\u6cd5\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.23272", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23272", "abs": "https://arxiv.org/abs/2507.23272", "authors": ["Solha Kang", "Eugene Kim", "Joris Vankerschaver", "Utku Ozbulak"], "title": "Towards Affordable Tumor Segmentation and Visualization for 3D Breast MRI Using SAM2", "comment": "Accepted for publication in the 28th International Conference on\n  Medical Image Computing and Computer Assisted Intervention (MICCAI), 2nd Deep\n  Breast Workshop on AI and Imaging for Diagnostic and Treatment Challenges in\n  Breast Care (DeepBreath), 2025", "summary": "Breast MRI provides high-resolution volumetric imaging critical for tumor\nassessment and treatment planning, yet manual interpretation of 3D scans\nremains labor-intensive and subjective. While AI-powered tools hold promise for\naccelerating medical image analysis, adoption of commercial medical AI products\nremains limited in low- and middle-income countries due to high license costs,\nproprietary software, and infrastructure demands. In this work, we investigate\nwhether the Segment Anything Model 2 (SAM2) can be adapted for low-cost,\nminimal-input 3D tumor segmentation in breast MRI. Using a single bounding box\nannotation on one slice, we propagate segmentation predictions across the 3D\nvolume using three different slice-wise tracking strategies: top-to-bottom,\nbottom-to-top, and center-outward. We evaluate these strategies across a large\ncohort of patients and find that center-outward propagation yields the most\nconsistent and accurate segmentations. Despite being a zero-shot model not\ntrained for volumetric medical data, SAM2 achieves strong segmentation\nperformance under minimal supervision. We further analyze how segmentation\nperformance relates to tumor size, location, and shape, identifying key failure\nmodes. Our results suggest that general-purpose foundation models such as SAM2\ncan support 3D medical image analysis with minimal supervision, offering an\naccessible and affordable alternative for resource-constrained settings.", "AI": {"tldr": "\u7814\u7a76\u4e86 Segment Anything Model 2 (SAM2) \u662f\u5426\u53ef\u4ee5\u9002\u5e94\u4e73\u623f MRI \u4e2d\u4f4e\u6210\u672c\u3001\u6700\u5c0f\u8f93\u5165 3D \u80bf\u7624\u5206\u5272\u3002", "motivation": "\u4e73\u623f MRI \u63d0\u4f9b\u9ad8\u5206\u8fa8\u7387\u4f53\u79ef\u6210\u50cf\uff0c\u8fd9\u5bf9\u4e8e\u80bf\u7624\u8bc4\u4f30\u548c\u6cbb\u7597\u8ba1\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46 3D \u626b\u63cf\u7684\u624b\u52a8\u89e3\u8bfb\u4ecd\u7136 labor-intensive \u548c\u4e3b\u89c2\u7684\u3002\u867d\u7136\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u5de5\u5177\u5728\u52a0\u901f\u533b\u5b66\u56fe\u50cf\u5206\u6790\u65b9\u9762\u5177\u6709\u5e0c\u671b\uff0c\u4f46\u7531\u4e8e\u8bb8\u53ef\u6210\u672c\u9ad8\u3001\u4e13\u6709\u8f6f\u4ef6\u548c\u57fa\u7840\u8bbe\u65bd\u9700\u6c42\uff0c\u5546\u4e1a\u533b\u5b66\u4eba\u5de5\u667a\u80fd\u4ea7\u54c1\u5728\u4f4e\u6536\u5165\u548c\u4e2d\u7b49\u6536\u5165\u56fd\u5bb6\u7684\u91c7\u7528\u4ecd\u7136\u6709\u9650\u3002", "method": "\u4f7f\u7528\u4e00\u4e2a\u5207\u7247\u4e0a\u7684\u5355\u4e2a\u8fb9\u754c\u6846\u6ce8\u91ca\uff0c\u6211\u4eec\u4f7f\u7528\u4e09\u79cd\u4e0d\u540c\u7684\u5206\u7247\u8ddf\u8e2a\u7b56\u7565\u5728 3D \u4f53\u79ef\u4e2d\u4f20\u64ad\u5206\u5272\u9884\u6d4b\uff1a\u81ea\u4e0a\u800c\u4e0b\u3001\u81ea\u4e0b\u800c\u4e0a\u548c\u7531\u4e2d\u5fc3\u5411\u5916\u3002", "result": "\u5c3d\u7ba1\u662f\u4e00\u4e2a\u6ca1\u6709\u7ecf\u8fc7\u4f53\u79ef\u533b\u5b66\u6570\u636e\u8bad\u7ec3\u7684 zero-shot \u6a21\u578b\uff0c\u4f46 SAM2 \u5728\u6700\u5c11\u7684\u76d1\u7763\u4e0b\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u5206\u5272\u6027\u80fd\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5206\u6790\u4e86\u5206\u5272\u6027\u80fd\u4e0e\u80bf\u7624\u5927\u5c0f\u3001\u4f4d\u7f6e\u548c\u5f62\u72b6\u7684\u5173\u7cfb\uff0c\u786e\u5b9a\u4e86\u5173\u952e\u7684\u5931\u8d25\u6a21\u5f0f\u3002\u6211\u4eec\u53d1\u73b0\u7531\u4e2d\u5fc3\u5411\u5916\u4f20\u64ad\u4ea7\u751f\u6700\u4e00\u81f4\u548c\u51c6\u786e\u7684\u5206\u5272\u3002", "conclusion": "SAM2 \u8fd9\u6837\u7684\u901a\u7528\u57fa\u7840\u6a21\u578b\u53ef\u4ee5\u5728\u6700\u5c11\u7684\u76d1\u7763\u4e0b\u652f\u6301 3D \u533b\u5b66\u56fe\u50cf\u5206\u6790\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u5b9e\u60e0\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.23317", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23317", "abs": "https://arxiv.org/abs/2507.23317", "authors": ["Tao He", "Rongchuan Mu", "Lizi Liao", "Yixin Cao", "Ming Liu", "Bing Qin"], "title": "Good Learners Think Their Thinking: Generative PRM Makes Large Reasoning Model More Efficient Math Learner", "comment": "33 pages, 3 figures, 19 tables", "summary": "Large reasoning models (LRMs) have recently shown promise in solving complex\nmath problems when optimized with Reinforcement Learning (RL). But conventional\napproaches rely on outcome-only rewards that provide sparse feedback, resulting\nin inefficient optimization process. In this work, we investigate the function\nof process reward models (PRMs) to accelerate the RL training for LRMs. We\npropose a novel intrinsic signal-driven generative process evaluation mechanism\noperating at the thought level to address major bottlenecks in RL-based\ntraining. Specifically, instead of requiring PRMs to know how to solve\nproblems, our method uses intrinsic signals in solutions to judge stepwise\ncorrectness and aggregate contiguous correct/incorrect steps into coherent\n'thought' units. This structured, thought-level rewards enable more reliable\ncredit assignment by reducing ambiguity in step segmentation and alleviating\nreward hacking. We further introduce a capability-adaptive reward mechanism\nthat dynamically balances exploration and exploitation based on the LRM's\ncurrent proficiency, guiding learning without stifling creative\ntrial-and-error. These innovations are integrated into a new off-policy RL\nalgorithm, TP-GRPO, which extends grouped proximal optimization with\nprocess-based rewards and improves training efficiency. Experiments on 1.5B and\n7B parameter LRMs demonstrate that our method achieves higher problem-solving\naccuracy with significantly fewer training samples than outcome-only reward\nbaselines. The results validate that well-structured process rewards can\nsubstantially accelerate LRM optimization in math reasoning tasks. Code is\navailable at https://github.com/cs-holder/tp_grpo.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5728\u601d\u60f3\u5c42\u9762\u64cd\u4f5c\u7684\u5185\u5728\u4fe1\u53f7\u6765\u5224\u65ad\u9010\u6b65\u6b63\u786e\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u5c06\u8fde\u7eed\u7684\u6b63\u786e/\u4e0d\u6b63\u786e\u7684\u6b65\u9aa4\u805a\u5408\u4e3a\u8fde\u8d2f\u7684\u201c\u601d\u60f3\u201d\u5355\u5143\uff0c\u4ece\u800c\u52a0\u901fLRM\u7684RL\u8bad\u7ec3\u3002", "motivation": "\u4f20\u7edf\u7684reinforcement learning (RL)\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4ec5\u63d0\u4f9b\u7a00\u758f\u53cd\u9988\u7684\u7ed3\u679c\u5956\u52b1\uff0c\u4ece\u800c\u5bfc\u81f4\u4f18\u5316\u8fc7\u7a0b\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5185\u5728\u4fe1\u53f7\u9a71\u52a8\u7684\u751f\u6210\u8fc7\u7a0b\u8bc4\u4f30\u673a\u5236\uff0c\u8be5\u673a\u5236\u5728\u601d\u60f3\u5c42\u9762\u8fd0\u4f5c\uff0c\u4ee5\u89e3\u51b3\u57fa\u4e8eRL\u7684\u8bad\u7ec3\u4e2d\u7684\u4e3b\u8981\u74f6\u9888\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u6bd4\u4ec5\u7ed3\u679c\u5956\u52b1\u7684\u57fa\u7ebf\u65b9\u6cd5\u4ee5\u66f4\u5c11\u7684\u8bad\u7ec3\u6837\u672c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u89e3\u51b3\u95ee\u9898\u51c6\u786e\u7387\u3002", "conclusion": "\u4f7f\u7528\u7ed3\u6784\u826f\u597d\u7684\u8fc7\u7a0b\u5956\u52b1\u53ef\u4ee5\u663e\u8457\u52a0\u901f\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2dLRM\u7684\u4f18\u5316\u3002"}}
{"id": "2507.22941", "categories": ["cs.CL", "cs.CY", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.22941", "abs": "https://arxiv.org/abs/2507.22941", "authors": ["Paul Minchella", "Lo\u00efc Verlingue", "St\u00e9phane Chr\u00e9tien", "R\u00e9mi Vaucher", "Guillaume Metzler"], "title": "SigBERT: Combining Narrative Medical Reports and Rough Path Signature Theory for Survival Risk Estimation in Oncology", "comment": "12 pages, 2 figures, accepted for ECML PKDD 2025", "summary": "Electronic medical reports (EHR) contain a vast amount of information that\ncan be leveraged for machine learning applications in healthcare. However,\nexisting survival analysis methods often struggle to effectively handle the\ncomplexity of textual data, particularly in its sequential form. Here, we\npropose SigBERT, an innovative temporal survival analysis framework designed to\nefficiently process a large number of clinical reports per patient. SigBERT\nprocesses timestamped medical reports by extracting and averaging word\nembeddings into sentence embeddings. To capture temporal dynamics from the time\nseries of sentence embedding coordinates, we apply signature extraction from\nrough path theory to derive geometric features for each patient, which\nsignificantly enhance survival model performance by capturing complex temporal\ndynamics. These features are then integrated into a LASSO-penalized Cox model\nto estimate patient-specific risk scores. The model was trained and evaluated\non a real-world oncology dataset from the L\\'eon B\\'erard Center corpus, with a\nC-index score of 0.75 (sd 0.014) on the independent test cohort. SigBERT\nintegrates sequential medical data to enhance risk estimation, advancing\nnarrative-based survival analysis.", "AI": {"tldr": "SigBERT is a temporal survival analysis framework that processes clinical reports to estimate patient-specific risk scores, achieving a C-index of 0.75 on an oncology dataset.", "motivation": "Existing survival analysis methods often struggle to effectively handle the complexity of textual data, particularly in its sequential form.", "method": "SigBERT processes timestamped medical reports by extracting and averaging word embeddings into sentence embeddings. To capture temporal dynamics from the time series of sentence embedding coordinates, we apply signature extraction from rough path theory to derive geometric features for each patient, which significantly enhance survival model performance by capturing complex temporal dynamics. These features are then integrated into a LASSO-penalized Cox model to estimate patient-specific risk scores.", "result": "The model was trained and evaluated on a real-world oncology dataset from the L'\u00e9on B'erard Center corpus, with a C-index score of 0.75 (sd 0.014) on the independent test cohort.", "conclusion": "SigBERT integrates sequential medical data to enhance risk estimation, advancing narrative-based survival analysis."}}
{"id": "2507.23277", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23277", "abs": "https://arxiv.org/abs/2507.23277", "authors": ["Gyeongjin Kang", "Seungtae Nam", "Xiangyu Sun", "Sameh Khamis", "Abdelrahman Mohamed", "Eunbyung Park"], "title": "iLRM: An Iterative Large 3D Reconstruction Model", "comment": "Project page: https://gynjn.github.io/iLRM/", "summary": "Feed-forward 3D modeling has emerged as a promising approach for rapid and\nhigh-quality 3D reconstruction. In particular, directly generating explicit 3D\nrepresentations, such as 3D Gaussian splatting, has attracted significant\nattention due to its fast and high-quality rendering, as well as numerous\napplications. However, many state-of-the-art methods, primarily based on\ntransformer architectures, suffer from severe scalability issues because they\nrely on full attention across image tokens from multiple input views, resulting\nin prohibitive computational costs as the number of views or image resolution\nincreases. Toward a scalable and efficient feed-forward 3D reconstruction, we\nintroduce an iterative Large 3D Reconstruction Model (iLRM) that generates 3D\nGaussian representations through an iterative refinement mechanism, guided by\nthree core principles: (1) decoupling the scene representation from input-view\nimages to enable compact 3D representations; (2) decomposing fully-attentional\nmulti-view interactions into a two-stage attention scheme to reduce\ncomputational costs; and (3) injecting high-resolution information at every\nlayer to achieve high-fidelity reconstruction. Experimental results on widely\nused datasets, such as RE10K and DL3DV, demonstrate that iLRM outperforms\nexisting methods in both reconstruction quality and speed. Notably, iLRM\nexhibits superior scalability, delivering significantly higher reconstruction\nquality under comparable computational cost by efficiently leveraging a larger\nnumber of input views.", "AI": {"tldr": "This paper introduces iLRM, a scalable and efficient feed-forward 3D reconstruction model that generates 3D Gaussian representations through an iterative refinement mechanism. iLRM outperforms existing methods in both reconstruction quality and speed on RE10K and DL3DV datasets.", "motivation": "many state-of-the-art methods, primarily based on transformer architectures, suffer from severe scalability issues because they rely on full attention across image tokens from multiple input views, resulting in prohibitive computational costs as the number of views or image resolution increases.", "method": "introduce an iterative Large 3D Reconstruction Model (iLRM) that generates 3D Gaussian representations through an iterative refinement mechanism, guided by three core principles: (1) decoupling the scene representation from input-view images to enable compact 3D representations; (2) decomposing fully-attentional multi-view interactions into a two-stage attention scheme to reduce computational costs; and (3) injecting high-resolution information at every layer to achieve high-fidelity reconstruction.", "result": "iLRM outperforms existing methods in both reconstruction quality and speed.", "conclusion": "iLRM outperforms existing methods in both reconstruction quality and speed. Notably, iLRM exhibits superior scalability, delivering significantly higher reconstruction quality under comparable computational cost by efficiently leveraging a larger number of input views."}}
{"id": "2507.23335", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.23335", "abs": "https://arxiv.org/abs/2507.23335", "authors": ["Qilin Zhou", "Haipeng Wang", "Zhengyuan Wei", "W. K. Chan"], "title": "Scalable and Precise Patch Robustness Certification for Deep Learning Models with Top-k Predictions", "comment": "accepted by QRS 2025", "summary": "Patch robustness certification is an emerging verification approach for\ndefending against adversarial patch attacks with provable guarantees for deep\nlearning systems. Certified recovery techniques guarantee the prediction of the\nsole true label of a certified sample. However, existing techniques, if\napplicable to top-k predictions, commonly conduct pairwise comparisons on those\nvotes between labels, failing to certify the sole true label within the top k\nprediction labels precisely due to the inflation on the number of votes\ncontrolled by the attacker (i.e., attack budget); yet enumerating all\ncombinations of vote allocation suffers from the combinatorial explosion\nproblem. We propose CostCert, a novel, scalable, and precise voting-based\ncertified recovery defender. CostCert verifies the true label of a sample\nwithin the top k predictions without pairwise comparisons and combinatorial\nexplosion through a novel design: whether the attack budget on the sample is\ninfeasible to cover the smallest total additional votes on top of the votes\nuncontrollable by the attacker to exclude the true labels from the top k\nprediction labels. Experiments show that CostCert significantly outperforms the\ncurrent state-of-the-art defender PatchGuard, such as retaining up to 57.3% in\ncertified accuracy when the patch size is 96, whereas PatchGuard has already\ndropped to zero.", "AI": {"tldr": "CostCert\u662f\u4e00\u79cd\u65b0\u9896\u3001\u53ef\u6269\u5c55\u4e14\u7cbe\u786e\u7684\u57fa\u4e8e\u6295\u7968\u7684\u8ba4\u8bc1\u6062\u590d\u9632\u5fa1\u5668\uff0c\u5b83\u901a\u8fc7\u4e00\u79cd\u65b0\u9896\u7684\u8bbe\u8ba1\u6765\u9a8c\u8bc1\u524dk\u4e2a\u9884\u6d4b\u4e2d\u7684\u771f\u5b9e\u6807\u7b7e\uff0c\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u9632\u5fa1\u5668PatchGuard\u3002", "motivation": "\u73b0\u6709\u7684\u8ba4\u8bc1\u6062\u590d\u6280\u672f\u901a\u5e38\u5bf9\u6807\u7b7e\u4e4b\u95f4\u7684\u6295\u7968\u8fdb\u884c\u6210\u5bf9\u6bd4\u8f83\uff0c\u4f46\u7531\u4e8e\u653b\u51fb\u8005\u63a7\u5236\u7684\u6295\u7968\u6570\u91cf\u81a8\u80c0\uff08\u5373\u653b\u51fb\u9884\u7b97\uff09\uff0c\u672a\u80fd\u7cbe\u786e\u5730\u8ba4\u8bc1\u524dk\u4e2a\u9884\u6d4b\u6807\u7b7e\u4e2d\u7684\u552f\u4e00\u771f\u5b9e\u6807\u7b7e\uff1b\u7136\u800c\uff0c\u679a\u4e3e\u6295\u7968\u5206\u914d\u7684\u6240\u6709\u7ec4\u5408\u4f1a\u53d7\u5230\u7ec4\u5408\u7206\u70b8\u95ee\u9898\u7684\u56f0\u6270\u3002", "method": "CostCert\u901a\u8fc7\u4e00\u79cd\u65b0\u9896\u7684\u8bbe\u8ba1\u6765\u9a8c\u8bc1\u6837\u672c\u5728\u524dk\u4e2a\u9884\u6d4b\u4e2d\u7684\u771f\u5b9e\u6807\u7b7e\uff0c\u800c\u65e0\u9700\u6210\u5bf9\u6bd4\u8f83\u548c\u7ec4\u5408\u7206\u70b8\uff1a\u5373\u653b\u51fb\u6837\u672c\u7684\u9884\u7b97\u662f\u5426\u4e0d\u8db3\u4ee5\u8986\u76d6\u6700\u5c0f\u7684\u603b\u989d\u5916\u6295\u7968\u6570\uff0c\u4ece\u800c\u5c06\u771f\u5b9e\u6807\u7b7e\u4ece\u524dk\u4e2a\u9884\u6d4b\u6807\u7b7e\u4e2d\u6392\u9664\u3002", "result": "CostCert\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u9632\u5fa1\u5668PatchGuard\uff0c\u4f8b\u5982\uff0c\u5f53\u8865\u4e01\u5927\u5c0f\u4e3a96\u65f6\uff0cCostCert\u4fdd\u6301\u9ad8\u8fbe57.3%\u7684\u8ba4\u8bc1\u51c6\u786e\u7387\uff0c\u800cPatchGuard\u5df2\u7ecf\u964d\u81f3\u96f6\u3002", "conclusion": "CostCert\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u9632\u5fa1\u5668PatchGuard\uff0c\u4f8b\u5982\uff0c\u5f53\u8865\u4e01\u5927\u5c0f\u4e3a96\u65f6\uff0cCostCert\u4fdd\u6301\u9ad8\u8fbe57.3%\u7684\u8ba4\u8bc1\u51c6\u786e\u7387\uff0c\u800cPatchGuard\u5df2\u7ecf\u964d\u81f3\u96f6\u3002"}}
{"id": "2507.22943", "categories": ["cs.CL", "stat.ME"], "pdf": "https://arxiv.org/pdf/2507.22943", "abs": "https://arxiv.org/abs/2507.22943", "authors": ["Shirley V Wang", "Georg Hahn", "Sushama Kattinakere Sreedhara", "Mufaddal Mahesri", "Haritha S. Pillai", "Rajendra Aldis", "Joyce Lii", "Sarah K. Dutcher", "Rhoda Eniafe", "Jamal T. Jones", "Keewan Kim", "Jiwei He", "Hana Lee", "Sengwee Toh", "Rishi J Desai", "Jie Yang"], "title": "A chart review process aided by natural language processing and multi-wave adaptive sampling to expedite validation of code-based algorithms for large database studies", "comment": null, "summary": "Background: One of the ways to enhance analyses conducted with large claims\ndatabases is by validating the measurement characteristics of code-based\nalgorithms used to identify health outcomes or other key study parameters of\ninterest. These metrics can be used in quantitative bias analyses to assess the\nrobustness of results for an inferential study given potential bias from\noutcome misclassification. However, extensive time and resource allocation are\ntypically re-quired to create reference-standard labels through manual chart\nreview of free-text notes from linked electronic health records. Methods: We\ndescribe an expedited process that introduces efficiency in a validation study\nus-ing two distinct mechanisms: 1) use of natural language processing (NLP) to\nreduce time spent by human reviewers to review each chart, and 2) a multi-wave\nadaptive sampling approach with pre-defined criteria to stop the validation\nstudy once performance characteristics are identified with sufficient\nprecision. We illustrate this process in a case study that validates the\nperformance of a claims-based outcome algorithm for intentional self-harm in\npatients with obesity. Results: We empirically demonstrate that the\nNLP-assisted annotation process reduced the time spent on review per chart by\n40% and use of the pre-defined stopping rule with multi-wave samples would have\nprevented review of 77% of patient charts with limited compromise to precision\nin derived measurement characteristics. Conclusion: This approach could\nfacilitate more routine validation of code-based algorithms used to define key\nstudy parameters, ultimately enhancing understanding of the reliability of\nfind-ings derived from database studies.", "AI": {"tldr": "This study introduces an efficient validation process using NLP and adaptive sampling, reducing review time and resources while maintaining precision.", "motivation": "enhance analyses conducted with large claims databases is by validating the measurement characteristics of code-based algorithms used to identify health outcomes", "method": "use of natural language processing (NLP) to reduce time spent by human reviewers to review each chart, and 2) a multi-wave adaptive sampling approach with pre-defined criteria to stop the validation study once performance characteristics are identified with sufficient precision", "result": "NLP-assisted annotation process reduced the time spent on review per chart by 40% and use of the pre-defined stopping rule with multi-wave samples would have prevented review of 77% of patient charts with limited compromise to precision in derived measurement characteristics.", "conclusion": "This approach could facilitate more routine validation of code-based algorithms used to define key study parameters, ultimately enhancing understanding of the reliability of find-ings derived from database studies."}}
{"id": "2507.23278", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23278", "abs": "https://arxiv.org/abs/2507.23278", "authors": ["Hao Tang", "Chenwei Xie", "Xiaoyi Bao", "Tingyu Weng", "Pandeng Li", "Yun Zheng", "Liwei Wang"], "title": "UniLiP: Adapting CLIP for Unified Multimodal Understanding, Generation and Editing", "comment": null, "summary": "In this paper, we propose UniLIP, which extends CLIP to reconstruction,\ngeneration and editing, thereby building a unified tokenizer upon its\nexceptional comprehension capabilities. Previous CLIP-based unified methods\noften require additional diffusion decoders or quantization to support\nreconstruction and generation tasks, leading to inconsistent reconstruction or\ndegradation of original comprehension performance.In contrast, we introduce a\ntwo-stage training scheme and a self-distillation strategy that progressively\nintegrates reconstruction capabilities into CLIP, allowing it to maintain\noriginal comprehension performance while achieving effective image\nreconstruction. Furthermore, we propose a dual-condition architecture to\nconnect the MLLM and diffusion transformer, using both learnable queries and\nthe last layer multimodal hidden states as joint conditions. This method not\nonly enables the utilization of the MLLM's strong reasoning capabilities in\ngeneration tasks, but also maximizes the exploitation of the rich information\nin UniLIP features during editing tasks. In text-to-image generation tasks,\nUniLIP obtains scores of 0.87 and 0.53 on GenEval and WISE benchmark\nrespectively, surpassing all previous unified models of similar scale. In image\nediting, UniLIP also achieves a score of 3.62 on the ImgEdit Benchmark,\nsurpassing recent state-of-the-art models such as BAGEL and UniWorld-V1. UniLIP\neffectively expand the application scope of CLIP, enabling continuous CLIP\nfeatures to not only serve as the optimal choice for understanding tasks but\nalso achieve highly competitive performance in generation and editing tasks.", "AI": {"tldr": "UniLIP\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u548c\u81ea\u84b8\u998f\u7b56\u7565\u6269\u5c55\u4e86CLIP\u7684\u529f\u80fd\uff0c\u4f7f\u5176\u5728\u56fe\u50cf\u91cd\u5efa\u3001\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u4ee5\u5f80\u7684\u7edf\u4e00\u6a21\u578b\u3002", "motivation": "\u4ee5\u524d\u57fa\u4e8eCLIP\u7684\u7edf\u4e00\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u989d\u5916\u7684\u6269\u6563\u89e3\u7801\u5668\u6216\u91cf\u5316\u6765\u652f\u6301\u91cd\u5efa\u548c\u751f\u6210\u4efb\u52a1\uff0c\u5bfc\u81f4\u4e0d\u4e00\u81f4\u7684\u91cd\u5efa\u6216\u539f\u59cb\u7406\u89e3\u6027\u80fd\u7684\u4e0b\u964d\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\u548c\u81ea\u84b8\u998f\u7b56\u7565\uff0c\u9010\u6b65\u5c06\u91cd\u5efa\u80fd\u529b\u96c6\u6210\u5230CLIP\u4e2d\uff0c\u4f7f\u5176\u5728\u4fdd\u6301\u539f\u6709\u7406\u89e3\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u6709\u6548\u7684\u56fe\u50cf\u91cd\u5efa\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6761\u4ef6\u67b6\u6784\u6765\u8fde\u63a5MLLM\u548c\u6269\u6563transformer\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u67e5\u8be2\u548c\u6700\u540e\u4e00\u5c42\u591a\u6a21\u6001\u9690\u85cf\u72b6\u6001\u4f5c\u4e3a\u8054\u5408\u6761\u4ef6\u3002", "result": "\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0cUniLIP\u5728GenEval\u548cWISE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u83b7\u5f970.87\u548c0.53\u7684\u5206\u6570\uff0c\u8d85\u8fc7\u4e86\u6240\u6709\u4ee5\u524d\u7c7b\u4f3c\u89c4\u6a21\u7684\u7edf\u4e00\u6a21\u578b\u3002\u5728\u56fe\u50cf\u7f16\u8f91\u65b9\u9762\uff0cUniLIP\u5728ImgEdit\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u83b7\u5f97\u4e863.62\u7684\u5206\u6570\uff0c\u8d85\u8fc7\u4e86\u6700\u8fd1\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u5982BAGEL\u548cUniWorld-V1\u3002", "conclusion": "UniLIP\u6709\u6548\u5730\u6269\u5c55\u4e86CLIP\u7684\u5e94\u7528\u8303\u56f4\uff0c\u4f7f\u5176\u8fde\u7eed\u7684CLIP\u7279\u5f81\u4e0d\u4ec5\u53ef\u4ee5\u4f5c\u4e3a\u7406\u89e3\u4efb\u52a1\u7684\u6700\u4f73\u9009\u62e9\uff0c\u800c\u4e14\u5728\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u4e2d\u4e5f\u80fd\u5b9e\u73b0\u6781\u5177\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.23344", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.23344", "abs": "https://arxiv.org/abs/2507.23344", "authors": ["Tatsuya Mitomi", "Fumiyasu Makinoshima", "Fumiya Makihara", "Eigo Segawa"], "title": "Designing Dynamic Pricing for Bike-sharing Systems via Differentiable Agent-based Simulation", "comment": null, "summary": "Bike-sharing systems are emerging in various cities as a new ecofriendly\ntransportation system. In these systems, spatiotemporally varying user demands\nlead to imbalanced inventory at bicycle stations, resulting in additional\nrelocation costs. Therefore, it is essential to manage user demand through\noptimal dynamic pricing for the system. However, optimal pricing design for\nsuch a system is challenging because the system involves users with diverse\nbackgrounds and their probabilistic choices. To address this problem, we\ndevelop a differentiable agent-based simulation to rapidly design dynamic\npricing in bike-sharing systems, achieving balanced bicycle inventory despite\nspatiotemporally heterogeneous trips and probabilistic user decisions. We first\nvalidate our approach against conventional methods through numerical\nexperiments involving 25 bicycle stations and five time slots, yielding 100\nparameters. Compared to the conventional methods, our approach obtains a more\naccurate solution with a 73% to 78% reduction in loss while achieving more than\na 100-fold increase in convergence speed. We further validate our approach on a\nlarge-scale urban bike-sharing system scenario involving 289 bicycle stations,\nresulting in a total of 1156 parameters. Through simulations using the obtained\npricing policies, we confirm that these policies can naturally induce balanced\ninventory without any manual relocation. Additionally, we find that the cost of\ndiscounts to induce the balanced inventory can be minimized by setting\nappropriate initial conditions.", "AI": {"tldr": "This paper introduces a differentiable agent-based simulation for dynamic pricing in bike-sharing systems to balance bicycle inventory, outperforming conventional methods in accuracy and speed.", "motivation": "User demands in bike-sharing systems lead to imbalanced inventory, resulting in additional relocation costs. Optimal pricing design is challenging due to diverse users and their probabilistic choices.", "method": "The authors develop a differentiable agent-based simulation to design dynamic pricing in bike-sharing systems.", "result": "The approach obtains a more accurate solution with a 73% to 78% reduction in loss and achieves more than a 100-fold increase in convergence speed compared to conventional methods. Simulations confirm that the pricing policies can naturally induce balanced inventory.", "conclusion": "The study confirms that the designed pricing policies can induce balanced inventory without manual relocation and that the cost of discounts can be minimized by setting appropriate initial conditions."}}
{"id": "2507.22944", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.22944", "abs": "https://arxiv.org/abs/2507.22944", "authors": ["Naomi Omeonga wa Kayembe"], "title": "Opacity as Authority: Arbitrariness and the Preclusion of Contestation", "comment": null, "summary": "This article redefines arbitrariness not as a normative flaw or a symptom of\ndomination, but as a foundational functional mechanism structuring human\nsystems and interactions. Diverging from critical traditions that conflate\narbitrariness with injustice, it posits arbitrariness as a semiotic trait: a\nproperty enabling systems - linguistic, legal, or social - to operate\neffectively while withholding their internal rationale. Building on Ferdinand\nde Saussure's concept of l'arbitraire du signe, the analysis extends this\nprinciple beyond language to demonstrate its cross-domain applicability,\nparticularly in law and social dynamics. The paper introduces the \"Motivation\n-> Constatability -> Contestability\" chain, arguing that motivation functions\nas a crucial interface rendering an act's logic vulnerable to intersubjective\ncontestation. When this chain is broken through mechanisms like\n\"immotivization\" or \"Conflict Lateralization\" (exemplified by \"the blur of the\nwolf drowned in the fish\"), acts produce binding effects without exposing their\nrationale, thus precluding justiciability. This structural opacity, while\nappearing illogical, is a deliberate design protecting authority from\naccountability. Drawing on Shannon's entropy model, the paper formalizes\narbitrariness as A = H(L|M) (conditional entropy). It thereby proposes a modern\ntheory of arbitrariness as a neutral operator central to control as well as\ncare, an overlooked dimension of interpersonal relations. While primarily\ndeveloped through human social systems, this framework also illuminates a new\npathway for analyzing explainability in advanced artificial intelligence\nsystems.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5b9a\u4e49\u4e86\u4efb\u610f\u6027\uff0c\u8ba4\u4e3a\u5b83\u662f\u4e00\u79cd\u4f7f\u7cfb\u7edf\u6709\u6548\u8fd0\u4f5c\u4e14\u9690\u85cf\u5185\u90e8\u539f\u7406\u7684\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u73b0\u4ee3\u4efb\u610f\u6027\u7406\u8bba\uff0c\u53ef\u7528\u4e8e\u5206\u6790\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u91cd\u65b0\u5b9a\u4e49\u4efb\u610f\u6027\uff0c\u4e0d\u518d\u5c06\u5176\u89c6\u4e3a\u89c4\u8303\u7f3a\u9677\u6216\u652f\u914d\u7684\u5f81\u5146\uff0c\u800c\u662f\u5c06\u5176\u89c6\u4e3a\u6784\u5efa\u4eba\u7c7b\u7cfb\u7edf\u548c\u4e92\u52a8\u7684\u57fa\u7840\u529f\u80fd\u673a\u5236\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u7d22\u7eea\u5c14\u7684\u7b26\u53f7\u4efb\u610f\u6027\u6982\u5ff5\uff0c\u5e76\u5f15\u5165\u201c\u52a8\u673a -> \u53ef\u8bc1\u5b9e\u6027 -> \u53ef\u4e89\u8bae\u6027\u201d\u94fe\u6761\uff0c\u4ee5\u53ca\u9999\u519c\u7684\u71b5\u6a21\u578b\u8fdb\u884c\u5f62\u5f0f\u5316\u3002", "result": "\u8bba\u8bc1\u4e86\u4efb\u610f\u6027\u4f5c\u4e3a\u4e00\u79cd\u7b26\u53f7\u7279\u5f81\uff0c\u4f7f\u7cfb\u7edf\u5728\u4e0d\u66b4\u9732\u5176\u5185\u90e8\u539f\u7406\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u8fd0\u4f5c\uff0c\u5e76\u901a\u8fc7\u201c\u53bb\u52a8\u673a\u5316\u201d\u6216\u201c\u51b2\u7a81\u6a2a\u5411\u5316\u201d\u7b49\u673a\u5236\uff0c\u4f7f\u884c\u4e3a\u4ea7\u751f\u7ea6\u675f\u6548\u679c\u4e14\u65e0\u9700\u95ee\u8d23\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u73b0\u4ee3\u7684\u4efb\u610f\u6027\u7406\u8bba\uff0c\u5c06\u5176\u89c6\u4e3a\u63a7\u5236\u548c\u5173\u6000\u7684\u6838\u5fc3\u4e2d\u7acb\u7b97\u5b50\uff0c\u5e76\u63a2\u7d22\u4e86\u5176\u5728\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.23284", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23284", "abs": "https://arxiv.org/abs/2507.23284", "authors": ["Dohwan Ko", "Ji Soo Lee", "Minhyuk Choi", "Zihang Meng", "Hyunwoo J. Kim"], "title": "Bidirectional Likelihood Estimation with Multi-Modal Large Language Models for Text-Video Retrieval", "comment": "ICCV 2025 Highlight", "summary": "Text-Video Retrieval aims to find the most relevant text (or video) candidate\ngiven a video (or text) query from large-scale online databases. Recent work\nleverages multi-modal large language models (MLLMs) to improve retrieval,\nespecially for long or complex query-candidate pairs. However, we observe that\nthe naive application of MLLMs, i.e., retrieval based on candidate likelihood,\nintroduces candidate prior bias, favoring candidates with inherently higher\npriors over those more relevant to the query. To this end, we propose a novel\nretrieval framework, Bidirectional Likelihood Estimation with MLLM (BLiM),\nwhich leverages both query and candidate likelihoods by training the model to\ngenerate text from a given video as well as video features from a given text.\nFurthermore, we introduce Candidate Prior Normalization (CPN), a simple yet\neffective training-free score calibration module designed to mitigate candidate\nprior bias in candidate likelihood. On four Text-Video Retrieval benchmarks,\nour BLiM equipped with CPN outperforms previous state-of-the-art models by 6.4\nR@1 on average, effectively alleviating candidate prior bias and emphasizing\nquery-candidate relevance. Our in-depth analysis across various multi-modal\ntasks beyond retrieval highlights the broad applicability of CPN which enhances\nvisual understanding by reducing reliance on textual priors. Code is available\nat https://github.com/mlvlab/BLiM.", "AI": {"tldr": "Proposes BLiM, a retrieval framework using bidirectional likelihood estimation with MLLMs, and CPN, a training-free score calibration module, to mitigate candidate prior bias in Text-Video Retrieval.", "motivation": "The naive application of MLLMs introduces candidate prior bias, favoring candidates with inherently higher priors over those more relevant to the query.", "method": "Bidirectional Likelihood Estimation with MLLM (BLiM), which leverages both query and candidate likelihoods by training the model to generate text from a given video as well as video features from a given text. Candidate Prior Normalization (CPN), a simple yet effective training-free score calibration module designed to mitigate candidate prior bias in candidate likelihood.", "result": "BLiM equipped with CPN outperforms previous state-of-the-art models by 6.4 R@1 on average.", "conclusion": "BLiM equipped with CPN outperforms previous state-of-the-art models by 6.4 R@1 on average, effectively alleviating candidate prior bias and emphasizing query-candidate relevance. CPN enhances visual understanding by reducing reliance on textual priors."}}
{"id": "2507.23389", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23389", "abs": "https://arxiv.org/abs/2507.23389", "authors": ["David Komnick", "Kathrin Lammers", "Barbara Hammer", "Valerie Vaquet", "Fabian Hinder"], "title": "Causal Explanation of Concept Drift -- A Truly Actionable Approach", "comment": "This manuscript is accepted to be presented at the TempXAI workshop\n  at the European Conference on Machine Learning and Principles and Practice of\n  Knowledge Discovery in Databases (ECMLPKDD 2025)", "summary": "In a world that constantly changes, it is crucial to understand how those\nchanges impact different systems, such as industrial manufacturing or critical\ninfrastructure. Explaining critical changes, referred to as concept drift in\nthe field of machine learning, is the first step towards enabling targeted\ninterventions to avoid or correct model failures, as well as malfunctions and\nerrors in the physical world. Therefore, in this work, we extend model-based\ndrift explanations towards causal explanations, which increases the\nactionability of the provided explanations. We evaluate our explanation\nstrategy on a number of use cases, demonstrating the practical usefulness of\nour framework, which isolates the causally relevant features impacted by\nconcept drift and, thus, allows for targeted intervention.", "AI": {"tldr": "The paper introduces causal explanations for concept drift to enable targeted interventions in changing systems.", "motivation": "Understanding how changes impact different systems is crucial, and explaining concept drift enables targeted interventions.", "method": "The paper proposes an explanation strategy that isolates causally relevant features impacted by concept drift.", "result": "The explanation strategy is evaluated on use cases, demonstrating the practical usefulness of the framework for targeted intervention.", "conclusion": "This work extends model-based drift explanations towards causal explanations to increase the actionability of explanations."}}
{"id": "2507.22968", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22968", "abs": "https://arxiv.org/abs/2507.22968", "authors": ["Chengqian Ma", "Wei Tao", "Yiwen Guo"], "title": "C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations", "comment": null, "summary": "Spoken Dialogue Models (SDMs) have recently attracted significant attention\nfor their ability to generate voice responses directly to users' spoken\nqueries. Despite their increasing popularity, there exists a gap in research\nfocused on comprehensively understanding their practical effectiveness in\ncomprehending and emulating human conversations. This is especially true\ncompared to text-based Large Language Models (LLMs), which benefit from\nextensive benchmarking. Human voice interactions are inherently more complex\nthan text due to characteristics unique to spoken dialogue. Ambiguity poses one\nchallenge, stemming from semantic factors like polysemy, as well as\nphonological aspects such as heterograph, heteronyms, and stress patterns.\nAdditionally, context-dependency, like omission, coreference, and multi-turn\ninteraction, adds further complexity to human conversational dynamics. To\nilluminate the current state of SDM development and to address these\nchallenges, we present a benchmark dataset in this paper, which comprises 1,079\ninstances in English and Chinese. Accompanied by an LLM-based evaluation method\nthat closely aligns with human judgment, this dataset facilitates a\ncomprehensive exploration of the performance of SDMs in tackling these\npractical challenges.", "AI": {"tldr": "This paper introduces a new benchmark dataset for spoken dialogue models (SDMs) in English and Chinese, along with an LLM-based evaluation method, to address the challenges of understanding and emulating human conversations compared to text-based LLMs.", "motivation": "there exists a gap in research focused on comprehensively understanding their practical effectiveness in comprehending and emulating human conversations. This is especially true compared to text-based Large Language Models (LLMs), which benefit from extensive benchmarking. Human voice interactions are inherently more complex than text due to characteristics unique to spoken dialogue. Ambiguity poses one challenge, stemming from semantic factors like polysemy, as well as phonological aspects such as heterograph, heteronyms, and stress patterns. Additionally, context-dependency, like omission, coreference, and multi-turn interaction, adds further complexity to human conversational dynamics.", "method": "a benchmark dataset in this paper, which comprises 1,079 instances in English and Chinese. Accompanied by an LLM-based evaluation method that closely aligns with human judgment", "result": "this dataset facilitates a comprehensive exploration of the performance of SDMs in tackling these practical challenges.", "conclusion": "We present a benchmark dataset in this paper, which comprises 1,079 instances in English and Chinese. Accompanied by an LLM-based evaluation method that closely aligns with human judgment, this dataset facilitates a comprehensive exploration of the performance of SDMs in tackling these practical challenges."}}
{"id": "2507.23295", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23295", "abs": "https://arxiv.org/abs/2507.23295", "authors": ["Inbum Heo", "Taewook Hwang", "Jeesu Jung", "Sangkeun Jung"], "title": "LED Benchmark: Diagnosing Structural Layout Errors for Document Layout Analysis", "comment": null, "summary": "Recent advancements in Document Layout Analysis through Large Language Models\nand Multimodal Models have significantly improved layout detection. However,\ndespite these improvements, challenges remain in addressing critical structural\nerrors, such as region merging, splitting, and missing content. Conventional\nevaluation metrics like IoU and mAP, which focus primarily on spatial overlap,\nare insufficient for detecting these errors. To address this limitation, we\npropose Layout Error Detection (LED), a novel benchmark designed to evaluate\nthe structural robustness of document layout predictions. LED defines eight\nstandardized error types, and formulates three complementary tasks: error\nexistence detection, error type classification, and element-wise error type\nclassification. Furthermore, we construct LED-Dataset, a synthetic dataset\ngenerated by injecting realistic structural errors based on empirical\ndistributions from DLA models. Experimental results across a range of LMMs\nreveal that LED effectively differentiates structural understanding\ncapabilities, exposing modality biases and performance trade-offs not visible\nthrough traditional metrics.", "AI": {"tldr": "This paper introduces LED, a new benchmark to evaluate structural errors in document layout analysis, revealing limitations of current models and metrics.", "motivation": "Conventional evaluation metrics like IoU and mAP, which focus primarily on spatial overlap, are insufficient for detecting these errors.", "method": "We propose Layout Error Detection (LED), a novel benchmark designed to evaluate the structural robustness of document layout predictions. LED defines eight standardized error types, and formulates three complementary tasks: error existence detection, error type classification, and element-wise error type classification. Furthermore, we construct LED-Dataset, a synthetic dataset generated by injecting realistic structural errors based on empirical distributions from DLA models.", "result": "Experimental results across a range of LMMs reveal that LED effectively differentiates structural understanding capabilities, exposing modality biases and performance trade-offs not visible through traditional metrics.", "conclusion": "LED effectively differentiates structural understanding capabilities, exposing modality biases and performance trade-offs not visible through traditional metrics."}}
{"id": "2507.23391", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23391", "abs": "https://arxiv.org/abs/2507.23391", "authors": ["Tung M. Luu", "Donghoon Lee", "Younghwan Lee", "Chang D. Yoo"], "title": "Policy Learning from Large Vision-Language Model Feedback without Reward Modeling", "comment": "Accepted to IROS 2025", "summary": "Offline reinforcement learning (RL) provides a powerful framework for\ntraining robotic agents using pre-collected, suboptimal datasets, eliminating\nthe need for costly, time-consuming, and potentially hazardous online\ninteractions. This is particularly useful in safety-critical real-world\napplications, where online data collection is expensive and impractical.\nHowever, existing offline RL algorithms typically require reward labeled data,\nwhich introduces an additional bottleneck: reward function design is itself\ncostly, labor-intensive, and requires significant domain expertise. In this\npaper, we introduce PLARE, a novel approach that leverages large\nvision-language models (VLMs) to provide guidance signals for agent training.\nInstead of relying on manually designed reward functions, PLARE queries a VLM\nfor preference labels on pairs of visual trajectory segments based on a\nlanguage task description. The policy is then trained directly from these\npreference labels using a supervised contrastive preference learning objective,\nbypassing the need to learn explicit reward models. Through extensive\nexperiments on robotic manipulation tasks from the MetaWorld, PLARE achieves\nperformance on par with or surpassing existing state-of-the-art VLM-based\nreward generation methods. Furthermore, we demonstrate the effectiveness of\nPLARE in real-world manipulation tasks with a physical robot, further\nvalidating its practical applicability.", "AI": {"tldr": "PLARE\u662f\u4e00\u79cd\u65b0\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b83\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6765\u6307\u5bfcagent\u8bad\u7ec3\uff0c\u907f\u514d\u4e86\u624b\u52a8\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\u7684\u9700\u8981\uff0c\u5e76\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u79bb\u7ebfRL\u7b97\u6cd5\u901a\u5e38\u9700\u8981\u5956\u52b1\u6807\u8bb0\u6570\u636e\uff0c\u8fd9\u5f15\u5165\u4e86\u989d\u5916\u7684\u74f6\u9888\uff1a\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u672c\u8eab\u6210\u672c\u9ad8\u6602\u3001\u52b3\u52a8\u5bc6\u96c6\u4e14\u9700\u8981\u5927\u91cf\u7684\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u4e0d\u4f9d\u8d56\u4e8e\u624b\u52a8\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u7684\u65b9\u6cd5\u3002", "method": "PLARE\u5229\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e3a\u4ee3\u7406\u8bad\u7ec3\u63d0\u4f9b\u6307\u5bfc\u4fe1\u53f7\uff0c\u901a\u8fc7\u67e5\u8be2VLM\uff0c\u57fa\u4e8e\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u5bf9\u89c6\u89c9\u8f68\u8ff9\u7247\u6bb5\u5bf9\u8fdb\u884c\u504f\u597d\u6807\u8bb0\uff0c\u5e76\u4f7f\u7528\u76d1\u7763\u5bf9\u6bd4\u504f\u597d\u5b66\u4e60\u76ee\u6807\u76f4\u63a5\u4ece\u8fd9\u4e9b\u504f\u597d\u6807\u7b7e\u4e2d\u8bad\u7ec3\u7b56\u7565\uff0c\u65e0\u9700\u5b66\u4e60\u663e\u5f0f\u5956\u52b1\u6a21\u578b\u3002", "result": "PLARE\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4e0e\u73b0\u6709VLM\u5956\u52b1\u751f\u6210\u65b9\u6cd5\u76f8\u5f53\u6216\u8d85\u8fc7\u7684\u6027\u80fd\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "PLARE\u5728MetaWorld\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u4e0e\u73b0\u6709VLM\u5956\u52b1\u751f\u6210\u65b9\u6cd5\u76f8\u5f53\u6216\u8d85\u8fc7\u7684\u6027\u80fd\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.23063", "categories": ["cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.23063", "abs": "https://arxiv.org/abs/2507.23063", "authors": ["Valeria de Paiva", "Qiyue Gao", "Hai Hu", "Pavel Kovalev", "Yikang Liu", "Lawrence S. Moss", "Zhiheng Qian"], "title": "Math Natural Language Inference: this should be easy!", "comment": "9 pages plus appendices", "summary": "We ask whether contemporary LLMs are able to perform natural language\ninference (NLI) tasks on mathematical texts. We call this the Math NLI problem.\nWe construct a corpus of Math NLI pairs whose premises are from extant\nmathematical text and whose hypotheses and gold labels were provided by people\nwith experience in both research-level mathematics and also in the NLI field.\nWe also investigate the quality of corpora using the same premises but whose\nhypotheses are provided by LLMs themselves. We not only investigate the\nperformance but also the inter-group consistency of the diverse group of LLMs.\nWe have both positive and negative findings. Among our positive findings: in\nsome settings, using a majority vote of LLMs is approximately equivalent to\nusing human-labeled data in the Math NLI area. On the negative side: LLMs still\nstruggle with mathematical language. They occasionally fail at even basic\ninferences. Current models are not as prone to hypothesis-only \"inference\" in\nour data the way the previous generation had been. In addition to our findings,\nwe also provide our corpora as data to support future work on Math NLI.", "AI": {"tldr": "LLM \u5728\u6570\u5b66 NLI \u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4e00\u4e9b\u8fdb\u5c55\uff0c\u4f46\u4ecd\u5b58\u5728\u6311\u6218\u3002", "motivation": "\u7814\u7a76\u5f53\u4ee3 LLM \u662f\u5426\u80fd\u591f\u6267\u884c\u6570\u5b66\u6587\u672c\u4e0a\u7684\u81ea\u7136\u8bed\u8a00\u63a8\u7406 (NLI) \u4efb\u52a1\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u6570\u5b66 NLI \u5bf9\u7684\u8bed\u6599\u5e93\uff0c\u5e76\u4f7f\u7528\u4eba\u5de5\u6807\u6ce8\u548c LLM \u751f\u6210\u7684\u5047\u8bbe\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528 LLM \u7684\u591a\u6570\u6295\u7968\u4e0e\u4f7f\u7528\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u5728\u6570\u5b66 NLI \u9886\u57df\u4e2d\u5927\u81f4\u76f8\u5f53\u3002LLM \u4ecd\u7136\u96be\u4ee5\u7406\u89e3\u6570\u5b66\u8bed\u8a00\uff0c\u5e76\u4e14\u5076\u5c14\u4f1a\u5728\u57fa\u672c\u63a8\u7406\u4e0a\u5931\u8d25\u3002", "conclusion": "LLMs \u5728\u6570\u5b66 NLI \u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e00\u5b9a\u7684\u80fd\u529b\uff0c\u4f46\u5728\u7406\u89e3\u6570\u5b66\u8bed\u8a00\u548c\u8fdb\u884c\u57fa\u672c\u63a8\u7406\u65b9\u9762\u4ecd\u6709\u56f0\u96be\u3002"}}
{"id": "2507.23300", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23300", "abs": "https://arxiv.org/abs/2507.23300", "authors": ["Hanshen Zhu", "Zhen Zhu", "Kaile Zhang", "Yiming Gong", "Yuliang Liu", "Xiang Bai"], "title": "Training-free Geometric Image Editing on Diffusion Models", "comment": "24 pages, 22 figures, ICCV", "summary": "We tackle the task of geometric image editing, where an object within an\nimage is repositioned, reoriented, or reshaped while preserving overall scene\ncoherence. Previous diffusion-based editing methods often attempt to handle all\nrelevant subtasks in a single step, proving difficult when transformations\nbecome large or structurally complex. We address this by proposing a decoupled\npipeline that separates object transformation, source region inpainting, and\ntarget region refinement. Both inpainting and refinement are implemented using\na training-free diffusion approach, FreeFine. In experiments on our new\nGeoBench benchmark, which contains both 2D and 3D editing scenarios, FreeFine\noutperforms state-of-the-art alternatives in image fidelity, and edit\nprecision, especially under demanding transformations. Code and benchmark are\navailable at: https://github.com/CIawevy/FreeFine", "AI": {"tldr": "FreeFine: a decoupled pipeline that separates object transformation, source region inpainting, and target region refinement, outperforms state-of-the-art alternatives in image fidelity, and edit precision.", "motivation": "Previous diffusion-based editing methods often attempt to handle all relevant subtasks in a single step, proving difficult when transformations become large or structurally complex.", "method": "a decoupled pipeline that separates object transformation, source region inpainting, and target region refinement. Both inpainting and refinement are implemented using a training-free diffusion approach, FreeFine.", "result": "FreeFine outperforms state-of-the-art alternatives in image fidelity, and edit precision, especially under demanding transformations on GeoBench benchmark.", "conclusion": "FreeFine outperforms state-of-the-art alternatives in image fidelity and edit precision, especially under demanding transformations."}}
{"id": "2507.23412", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23412", "abs": "https://arxiv.org/abs/2507.23412", "authors": ["Mokhtar A. Al-Awadhi", "Ratnadeep R. Deshmukh"], "title": "A Machine Learning Approach for Honey Adulteration Detection using Mineral Element Profiles", "comment": null, "summary": "This paper aims to develop a Machine Learning (ML)-based system for detecting\nhoney adulteration utilizing honey mineral element profiles. The proposed\nsystem comprises two phases: preprocessing and classification. The\npreprocessing phase involves the treatment of missing-value attributes and\nnormalization. In the classifica-tion phase, we use three supervised ML models:\nlogistic regression, decision tree, and random forest, to dis-criminate between\nauthentic and adulterated honey. To evaluate the performance of the ML models,\nwe use a public dataset comprising measurements of mineral element content of\nauthentic honey, sugar syrups, and adul-terated honey. Experimental findings\nshow that mineral element content in honey provides robust discriminative\ninformation for detecting honey adulteration. Results also demonstrate that the\nrandom forest-based classifier outperforms other classifiers on this dataset,\nachieving the highest cross-validation accuracy of 98.37%.", "AI": {"tldr": "This paper uses machine learning to detect honey adulteration using mineral element profiles, with random forest achieving 98.37% accuracy.", "motivation": "To develop a Machine Learning (ML)-based system for detecting honey adulteration utilizing honey mineral element profiles.", "method": "logistic regression, decision tree, and random forest", "result": "random forest-based classifier outperforms other classifiers on this dataset, achieving the highest cross-validation accuracy of 98.37%.", "conclusion": "Mineral element content in honey provides robust discriminative information for detecting honey adulteration, and random forest outperforms other classifiers, achieving the highest cross-validation accuracy of 98.37%."}}
{"id": "2507.23082", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23082", "abs": "https://arxiv.org/abs/2507.23082", "authors": ["Diego Garat", "Guillermo Moncecchi", "Dina Wonsever"], "title": "Exploring In-Context Learning for Frame-Semantic Parsing", "comment": null, "summary": "Frame Semantic Parsing (FSP) entails identifying predicates and labeling\ntheir arguments according to Frame Semantics. This paper investigates the use\nof In-Context Learning (ICL) with Large Language Models (LLMs) to perform FSP\nwithout model fine-tuning. We propose a method that automatically generates\ntask-specific prompts for the Frame Identification (FI) and Frame Semantic Role\nLabeling (FSRL) subtasks, relying solely on the FrameNet database. These\nprompts, constructed from frame definitions and annotated examples, are used to\nguide six different LLMs. Experiments are conducted on a subset of frames\nrelated to violent events. The method achieves competitive results, with F1\nscores of 94.3% for FI and 77.4% for FSRL. The findings suggest that ICL offers\na practical and effective alternative to traditional fine-tuning for\ndomain-specific FSP tasks.", "AI": {"tldr": "This paper explores using In-Context Learning (ICL) with LLMs for Frame Semantic Parsing (FSP) without fine-tuning, achieving competitive results by automatically generating task-specific prompts.", "motivation": "investigates the use of In-Context Learning (ICL) with Large Language Models (LLMs) to perform FSP without model fine-tuning", "method": "a method that automatically generates task-specific prompts for the Frame Identification (FI) and Frame Semantic Role Labeling (FSRL) subtasks, relying solely on the FrameNet database", "result": "achieves competitive results, with F1 scores of 94.3% for FI and 77.4% for FSRL", "conclusion": "ICL offers a practical and effective alternative to traditional fine-tuning for domain-specific FSP tasks."}}
{"id": "2507.23307", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23307", "abs": "https://arxiv.org/abs/2507.23307", "authors": ["Xihang Hu", "Fuming Sun", "Jiazhe Liu", "Feilong Xu", "Xiaoli Zhang"], "title": "ST-SAM: SAM-Driven Self-Training Framework for Semi-Supervised Camouflaged Object Detection", "comment": "10 pages, 6 figures, ACM MM 2025", "summary": "Semi-supervised Camouflaged Object Detection (SSCOD) aims to reduce reliance\non costly pixel-level annotations by leveraging limited annotated data and\nabundant unlabeled data. However, existing SSCOD methods based on\nTeacher-Student frameworks suffer from severe prediction bias and error\npropagation under scarce supervision, while their multi-network architectures\nincur high computational overhead and limited scalability. To overcome these\nlimitations, we propose ST-SAM, a highly annotation-efficient yet concise\nframework that breaks away from conventional SSCOD constraints. Specifically,\nST-SAM employs Self-Training strategy that dynamically filters and expands\nhigh-confidence pseudo-labels to enhance a single-model architecture, thereby\nfundamentally circumventing inter-model prediction bias. Furthermore, by\ntransforming pseudo-labels into hybrid prompts containing domain-specific\nknowledge, ST-SAM effectively harnesses the Segment Anything Model's potential\nfor specialized tasks to mitigate error accumulation in self-training.\nExperiments on COD benchmark datasets demonstrate that ST-SAM achieves\nstate-of-the-art performance with only 1\\% labeled data, outperforming existing\nSSCOD methods and even matching fully supervised methods. Remarkably, ST-SAM\nrequires training only a single network, without relying on specific models or\nloss functions. This work establishes a new paradigm for annotation-efficient\nSSCOD. Codes will be available at https://github.com/hu-xh/ST-SAM.", "AI": {"tldr": "ST-SAM is a highly annotation-efficient and concise framework for Semi-supervised Camouflaged Object Detection (SSCOD) that overcomes the limitations of existing methods by using a self-training strategy and hybrid prompts to enhance a single-model architecture.", "motivation": "Existing SSCOD methods based on Teacher-Student frameworks suffer from severe prediction bias and error propagation under scarce supervision, while their multi-network architectures incur high computational overhead and limited scalability.", "method": "ST-SAM employs Self-Training strategy that dynamically filters and expands high-confidence pseudo-labels to enhance a single-model architecture, thereby fundamentally circumventing inter-model prediction bias. Furthermore, by transforming pseudo-labels into hybrid prompts containing domain-specific knowledge, ST-SAM effectively harnesses the Segment Anything Model's potential for specialized tasks to mitigate error accumulation in self-training.", "result": "ST-SAM achieves state-of-the-art performance with only 1% labeled data, outperforming existing SSCOD methods and even matching fully supervised methods.", "conclusion": "ST-SAM achieves state-of-the-art performance with only 1% labeled data, outperforming existing SSCOD methods and even matching fully supervised methods. Remarkably, ST-SAM requires training only a single network, without relying on specific models or loss functions. This work establishes a new paradigm for annotation-efficient SSCOD."}}
{"id": "2507.23418", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23418", "abs": "https://arxiv.org/abs/2507.23418", "authors": ["Mokhtar A. Al-Awadhi", "Ratnadeep R. Deshmukh"], "title": "Detection of Adulteration in Coconut Milk using Infrared Spectroscopy and Machine Learning", "comment": null, "summary": "In this paper, we propose a system for detecting adulteration in coconut\nmilk, utilizing infrared spectroscopy. The machine learning-based proposed\nsystem comprises three phases: preprocessing, feature extraction, and\nclassification. The first phase involves removing irrelevant data from coconut\nmilk spectral signals. In the second phase, we employ the Linear Discriminant\nAnalysis (LDA) algorithm for extracting the most discriminating features. In\nthe third phase, we use the K-Nearest Neighbor (KNN) model to classify coconut\nmilk samples into authentic or adulterated. We evaluate the performance of the\nproposed system using a public dataset comprising Fourier Transform Infrared\n(FTIR) spectral information of pure and contaminated coconut milk samples.\nFindings show that the proposed method successfully detects adulteration with a\ncross-validation accuracy of 93.33%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ea2\u5916\u5149\u8c31\u548c\u673a\u5668\u5b66\u4e60\u7684\u6930\u5976\u63ba\u5047\u68c0\u6d4b\u7cfb\u7edf\uff0c\u4ea4\u53c9\u9a8c\u8bc1\u51c6\u786e\u7387\u8fbe 93.33%\u3002", "motivation": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u7ea2\u5916\u5149\u8c31\u68c0\u6d4b\u6930\u5976\u4e2d\u63ba\u5047\u7269\u7684\u7cfb\u7edf\u3002", "method": "\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u7cfb\u7edf\uff0c\u5305\u62ec\u9884\u5904\u7406\u3001\u7279\u5f81\u63d0\u53d6\u548c\u5206\u7c7b\u4e09\u4e2a\u9636\u6bb5\u3002\u91c7\u7528\u7ebf\u6027\u5224\u522b\u5206\u6790 (LDA) \u7b97\u6cd5\u63d0\u53d6\u6700\u5177\u533a\u5206\u6027\u7684\u7279\u5f81\u3002\u4f7f\u7528 K \u6700\u8fd1\u90bb (KNN) \u6a21\u578b\u5c06\u6930\u5976\u6837\u54c1\u5206\u7c7b\u4e3a\u771f\u54c1\u6216\u63ba\u5047\u54c1\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u68c0\u6d4b\u5230\u63ba\u5047\u7269\uff0c\u4ea4\u53c9\u9a8c\u8bc1\u51c6\u786e\u7387\u4e3a 93.33%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6210\u529f\u68c0\u6d4b\u63ba\u5047\uff0c\u4ea4\u53c9\u9a8c\u8bc1\u51c6\u786e\u7387\u4e3a 93.33%\u3002"}}
{"id": "2507.23083", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23083", "abs": "https://arxiv.org/abs/2507.23083", "authors": ["Ali Veisi", "Delaram Fartoot", "Hamidreza Amirzadeh"], "title": "Context-aware Rotary Position Embedding", "comment": "4 pages, 1 table", "summary": "Positional encoding is a vital component of Transformer architectures,\nenabling models to incorporate sequence order into self-attention mechanisms.\nRotary Positional Embeddings (RoPE) have become a widely adopted solution due\nto their compatibility with relative position encoding and computational\nefficiency. However, RoPE relies on static, input-independent sinusoidal\nfrequency patterns, limiting its ability to model context-sensitive\nrelationships. In this work, we propose CARoPE (Context-Aware Rotary Positional\nEmbedding), a novel generalization of RoPE that dynamically generates\nhead-specific frequency patterns conditioned on token embeddings. This design\nintroduces token- and context-sensitive positional representations while\npreserving RoPE efficiency and architectural simplicity. CARoPE computes\ninput-dependent phase shifts using a bounded transformation of token embeddings\nand integrates them into the rotary mechanism across attention heads. We\nevaluate CARoPE on the FineWeb-Edu-10B dataset using GPT-2 variants trained on\nnext-token prediction tasks. Experimental results show that CARoPE consistently\noutperforms RoPE and other common positional encoding baselines, achieving\nsignificantly lower perplexity, even at longer context lengths. Additionally,\nCARoPE enables faster training throughput without sacrificing model stability.\nThese findings demonstrate that CARoPE offers a scalable, expressive, and\nefficient upgrade to existing positional encoding strategies in Transformer\nmodels.", "AI": {"tldr": "CARoPE\u662f\u4e00\u79cdRoPE\u7684\u6cdb\u5316\uff0c\u5b83\u52a8\u6001\u751f\u6210\u4f9d\u8d56\u4e8etoken\u5d4c\u5165\u7684\u7279\u5b9a\u4e8ehead\u7684\u9891\u7387\u6a21\u5f0f\uff0c\u5728\u6027\u80fd\u4e0a\u4f18\u4e8eRoPE\u3002", "motivation": "RoPE\u4f9d\u8d56\u4e8e\u9759\u6001\u7684\u3001\u72ec\u7acb\u4e8e\u8f93\u5165\u7684\u6b63\u5f26\u9891\u7387\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u4e0a\u4e0b\u6587\u654f\u611f\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdCARoPE\uff08\u4e0a\u4e0b\u6587\u611f\u77e5\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff09\uff0c\u5b83\u662fRoPE\u7684\u4e00\u79cd\u65b0\u7684\u6cdb\u5316\uff0c\u53ef\u4ee5\u52a8\u6001\u751f\u6210\u4f9d\u8d56\u4e8etoken\u5d4c\u5165\u7684\u7279\u5b9a\u4e8ehead\u7684\u9891\u7387\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u66f4\u957f\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\uff0cCARoPE\u59cb\u7ec8\u4f18\u4e8eRoPE\u548c\u5176\u4ed6\u5e38\u89c1\u7684\u4f4d\u7f6e\u7f16\u7801\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u964d\u4f4e\u7684\u56f0\u60d1\u5ea6\u3002\u6b64\u5916\uff0cCARoPE\u80fd\u591f\u5b9e\u73b0\u66f4\u5feb\u7684\u8bad\u7ec3\u541e\u5410\u91cf\uff0c\u800c\u4e0d\u4f1a\u727a\u7272\u6a21\u578b\u7a33\u5b9a\u6027\u3002", "conclusion": "CARoPE\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u5bcc\u6709\u8868\u73b0\u529b\u4e14\u9ad8\u6548\u7684Transformer\u6a21\u578b\u4e2d\u73b0\u6709\u4f4d\u7f6e\u7f16\u7801\u7b56\u7565\u7684\u5347\u7ea7\u65b9\u6848\u3002"}}
{"id": "2507.23309", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23309", "abs": "https://arxiv.org/abs/2507.23309", "authors": ["Xuewei Tang", "Mengmeng Yang", "Tuopu Wen", "Peijin Jia", "Le Cui", "Mingshang Luo", "Kehua Sheng", "Bo Zhang", "Diange Yang", "Kun Jiang"], "title": "PriorFusion: Unified Integration of Priors for Robust Road Perception in Autonomous Driving", "comment": null, "summary": "With the growing interest in autonomous driving, there is an increasing\ndemand for accurate and reliable road perception technologies. In complex\nenvironments without high-definition map support, autonomous vehicles must\nindependently interpret their surroundings to ensure safe and robust\ndecision-making. However, these scenarios pose significant challenges due to\nthe large number, complex geometries, and frequent occlusions of road elements.\nA key limitation of existing approaches lies in their insufficient exploitation\nof the structured priors inherently present in road elements, resulting in\nirregular, inaccurate predictions. To address this, we propose PriorFusion, a\nunified framework that effectively integrates semantic, geometric, and\ngenerative priors to enhance road element perception. We introduce an\ninstance-aware attention mechanism guided by shape-prior features, then\nconstruct a data-driven shape template space that encodes low-dimensional\nrepresentations of road elements, enabling clustering to generate anchor points\nas reference priors. We design a diffusion-based framework that leverages these\nprior anchors to generate accurate and complete predictions. Experiments on\nlarge-scale autonomous driving datasets demonstrate that our method\nsignificantly improves perception accuracy, particularly under challenging\nconditions. Visualization results further confirm that our approach produces\nmore accurate, regular, and coherent predictions of road elements.", "AI": {"tldr": "PriorFusion, a unified framework, integrates semantic, geometric, and generative priors to enhance road element perception using instance-aware attention, shape template space, and a diffusion-based framework.", "motivation": "The increasing demand for accurate and reliable road perception technologies in complex environments without high-definition map support. Existing approaches insufficiently exploit the structured priors inherently present in road elements, resulting in irregular, inaccurate predictions.", "method": "The paper proposes PriorFusion, a unified framework that effectively integrates semantic, geometric, and generative priors to enhance road element perception. It introduces an instance-aware attention mechanism guided by shape-prior features, and constructs a data-driven shape template space that encodes low-dimensional representations of road elements, enabling clustering to generate anchor points as reference priors. A diffusion-based framework leverages these prior anchors to generate accurate and complete predictions.", "result": "Experiments on large-scale autonomous driving datasets demonstrate that the method significantly improves perception accuracy, particularly under challenging conditions.", "conclusion": "The method significantly improves perception accuracy, particularly under challenging conditions, and produces more accurate, regular, and coherent predictions of road elements."}}
{"id": "2507.23428", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23428", "abs": "https://arxiv.org/abs/2507.23428", "authors": ["Nodens F. Koren", "Samuel Lanthaler"], "title": "Merging Memory and Space: A Spatiotemporal State Space Neural Operator", "comment": null, "summary": "We propose the Spatiotemporal State Space Neural Operator (ST-SSM), a compact\narchitecture for learning solution operators of time-dependent partial\ndifferential equations (PDEs). ST-SSM introduces a novel factorization of the\nspatial and temporal dimensions, using structured state-space models to\nindependently model temporal evolution and spatial interactions. This design\nenables parameter efficiency and flexible modeling of long-range spatiotemporal\ndynamics. A theoretical connection is established between SSMs and neural\noperators, and a unified universality theorem is proved for the resulting class\nof architectures. Empirically, we demonstrate that our factorized formulation\noutperforms alternative schemes such as zigzag scanning and parallel\nindependent processing on several PDE benchmarks, including 1D Burgers'\nequation, 1D Kuramoto-Sivashinsky equation, and 2D Navier-Stokes equations\nunder varying physical conditions. Our model performs competitively with\nexisting baselines while using significantly fewer parameters. In addition, our\nresults reinforce previous findings on the benefits of temporal memory by\nshowing improved performance under partial observability. Our results highlight\nthe advantages of dimensionally factorized operator learning for efficient and\ngeneralizable PDE modeling, and put this approach on a firm theoretical\nfooting.", "AI": {"tldr": "ST-SSM\u662f\u4e00\u79cd\u7528\u4e8e\u5b66\u4e60\u504f\u5fae\u5206\u65b9\u7a0b\u89e3\u7b97\u5b50\u7684\u7d27\u51d1\u67b6\u6784\uff0c\u5b83\u901a\u8fc7\u5206\u89e3\u7a7a\u95f4\u548c\u65f6\u95f4\u7ef4\u5ea6\u6765\u5b9e\u73b0\u53c2\u6570\u6548\u7387\u548c\u7075\u6d3b\u5efa\u6a21\u3002", "motivation": "\u5b66\u4e60\u65f6\u53d8\u504f\u5fae\u5206\u65b9\u7a0b(PDEs)\u7684\u89e3\u7b97\u5b50", "method": "\u65f6\u7a7a\u72b6\u6001\u7a7a\u95f4\u795e\u7ecf\u7b97\u5b50(ST-SSM)", "result": "\u5728\u591a\u4e2aPDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u5206\u89e3\u516c\u5f0f\u4f18\u4e8e\u5176\u4ed6\u65b9\u6848\uff0c\u540c\u65f6\u4f7f\u7528\u660e\u663e\u66f4\u5c11\u7684\u53c2\u6570\u3002\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u4e0b\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "dimensionally factorized operator learning\u5177\u6709\u9ad8\u6548\u548c\u6cdb\u5316\u7684PDE\u5efa\u6a21\u4f18\u52bf\uff0c\u5e76\u4e3a\u8fd9\u79cd\u65b9\u6cd5\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2507.23095", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23095", "abs": "https://arxiv.org/abs/2507.23095", "authors": ["Ishani Mondal", "Meera Bharadwaj", "Ayush Roy", "Aparna Garimella", "Jordan Lee Boyd-Graber"], "title": "SMART-Editor: A Multi-Agent Framework for Human-Like Design Editing with Structural Integrity", "comment": "Under Submission", "summary": "We present SMART-Editor, a framework for compositional layout and content\nediting across structured (posters, websites) and unstructured (natural images)\ndomains. Unlike prior models that perform local edits, SMART-Editor preserves\nglobal coherence through two strategies: Reward-Refine, an inference-time\nrewardguided refinement method, and RewardDPO, a training-time preference\noptimization approach using reward-aligned layout pairs. To evaluate model\nperformance, we introduce SMARTEdit-Bench, a benchmark covering multi-domain,\ncascading edit scenarios. SMART-Editor outperforms strong baselines like\nInstructPix2Pix and HIVE, with RewardDPO achieving up to 15% gains in\nstructured settings and Reward-Refine showing advantages on natural images.\nAutomatic and human evaluations confirm the value of reward-guided planning in\nproducing semantically consistent and visually aligned edits.", "AI": {"tldr": "SMART-Editor \u662f\u4e00\u79cd\u7528\u4e8e\u7ec4\u5408\u5e03\u5c40\u548c\u5185\u5bb9\u7f16\u8f91\u7684\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u5956\u52b1\u5f15\u5bfc\u7684\u89c4\u5212\u6765\u4fdd\u6301\u5168\u5c40\u8fde\u8d2f\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u6a21\u578b\u6267\u884c\u5c40\u90e8\u7f16\u8f91\uff0cSMART-Editor \u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u4fdd\u6301\u5168\u5c40\u8fde\u8d2f\u6027\u6765\u5b9e\u73b0\u7ec4\u5408\u5e03\u5c40\u548c\u5185\u5bb9\u7f16\u8f91\u3002", "method": "SMART-Editor \u91c7\u7528 Reward-Refine\uff08\u4e00\u79cd\u63a8\u7406\u65f6\u5956\u52b1\u5f15\u5bfc\u7684\u7ec6\u5316\u65b9\u6cd5\uff09\u548c RewardDPO\uff08\u4e00\u79cd\u4f7f\u7528\u5956\u52b1\u5bf9\u9f50\u5e03\u5c40\u5bf9\u7684\u8bad\u7ec3\u65f6\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff09\u4e24\u79cd\u7b56\u7565\u3002", "result": "SMART-Editor \u5728 SMARTEdit-Bench \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e InstructPix2Pix \u548c HIVE \u7b49\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\uff0cRewardDPO \u5728\u7ed3\u6784\u5316\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe 15% \u7684\u6536\u76ca\uff0cReward-Refine \u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u663e\u793a\u51fa\u4f18\u52bf\u3002", "conclusion": "SMART-Editor \u901a\u8fc7\u5956\u52b1\u5f15\u5bfc\u7684\u89c4\u5212\uff0c\u5728\u751f\u6210\u8bed\u4e49\u4e00\u81f4\u548c\u89c6\u89c9\u5bf9\u9f50\u7684\u7f16\u8f91\u65b9\u9762\u5177\u6709\u4ef7\u503c\u3002"}}
{"id": "2507.23311", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23311", "abs": "https://arxiv.org/abs/2507.23311", "authors": ["Timm Hess", "Gido M van de Ven", "Tinne Tuytelaars"], "title": "Forgetting of task-specific knowledge in model merging-based continual learning", "comment": null, "summary": "This paper investigates the linear merging of models in the context of\ncontinual learning (CL). Using controlled visual cues in computer vision\nexperiments, we demonstrate that merging largely preserves or enhances shared\nknowledge, while unshared task-specific knowledge rapidly degrades. We further\nfind that merging models from an incremental training process consistently\noutperforms merging models trained in parallel.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u6a21\u578b\u7ebf\u6027\u5408\u5e76\u3002", "motivation": "\u7814\u7a76\u5728\u7ebf\u6027\u5408\u5e76\u6a21\u578b\u4e2d\u6301\u7eed\u5b66\u4e60(CL)\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u5b9e\u9a8c\u4e2d\u4f7f\u7528\u53d7\u63a7\u89c6\u89c9\u7ebf\u7d22\u3002", "result": "\u5408\u5e76\u4e3b\u8981\u4fdd\u7559\u6216\u589e\u5f3a\u4e86\u5171\u4eab\u77e5\u8bc6\uff0c\u800c\u672a\u5171\u4eab\u7684\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u77e5\u8bc6\u8fc5\u901f\u9000\u5316\u3002\u6765\u81ea\u589e\u91cf\u8bad\u7ec3\u8fc7\u7a0b\u7684\u5408\u5e76\u6a21\u578b\u59cb\u7ec8\u4f18\u4e8e\u5e76\u884c\u8bad\u7ec3\u7684\u5408\u5e76\u6a21\u578b\u3002", "conclusion": "\u5408\u5e76\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u4fdd\u6301\u6216\u589e\u5f3a\u5171\u4eab\u77e5\u8bc6\u3002"}}
{"id": "2507.23437", "categories": ["cs.LG", "I.2.6; C.1.3; C.3"], "pdf": "https://arxiv.org/pdf/2507.23437", "abs": "https://arxiv.org/abs/2507.23437", "authors": ["Yinhui Ma", "Tomomasa Yamasaki", "Zhehui Wang", "Tao Luo", "Bo Wang"], "title": "Coflex: Enhancing HW-NAS with Sparse Gaussian Processes for Efficient and Scalable DNN Accelerator Design", "comment": "Accepted to ICCAD 2025 (camera-ready); 9 pages, 5 figures", "summary": "Hardware-Aware Neural Architecture Search (HW-NAS) is an efficient approach\nto automatically co-optimizing neural network performance and hardware energy\nefficiency, making it particularly useful for the development of Deep Neural\nNetwork accelerators on the edge. However, the extensive search space and high\ncomputational cost pose significant challenges to its practical adoption. To\naddress these limitations, we propose Coflex, a novel HW-NAS framework that\nintegrates the Sparse Gaussian Process (SGP) with multi-objective Bayesian\noptimization. By leveraging sparse inducing points, Coflex reduces the GP\nkernel complexity from cubic to near-linear with respect to the number of\ntraining samples, without compromising optimization performance. This enables\nscalable approximation of large-scale search space, substantially decreasing\ncomputational overhead while preserving high predictive accuracy. We evaluate\nthe efficacy of Coflex across various benchmarks, focusing on\naccelerator-specific architecture. Our experi- mental results show that Coflex\noutperforms state-of-the-art methods in terms of network accuracy and\nEnergy-Delay-Product, while achieving a computational speed-up ranging from\n1.9x to 9.5x.", "AI": {"tldr": "Coflex \u662f\u4e00\u79cd\u65b0\u7684 HW-NAS \u6846\u67b6\uff0c\u5b83\u96c6\u6210\u4e86\u7a00\u758f\u9ad8\u65af\u8fc7\u7a0b (SGP) \u548c\u591a\u76ee\u6807\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u53ef\u63d0\u9ad8\u8ba1\u7b97\u901f\u5ea6\u548c\u7f51\u7edc\u7cbe\u5ea6\u3002", "motivation": "\u786c\u4ef6\u611f\u77e5\u795e\u7ecf\u67b6\u6784\u641c\u7d22 (HW-NAS) \u662f\u4e00\u79cd\u81ea\u52a8\u534f\u540c\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\u548c\u786c\u4ef6\u80fd\u6548\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f7f\u5176\u7279\u522b\u9002\u7528\u4e8e\u8fb9\u7f18\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\u7684\u5f00\u53d1\u3002\u7136\u800c\uff0c\u5e7f\u6cdb\u7684\u641c\u7d22\u7a7a\u95f4\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u7ed9\u5b83\u7684\u5b9e\u9645\u5e94\u7528\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u4e00\u79cd\u5c06\u7a00\u758f\u9ad8\u65af\u8fc7\u7a0b (SGP) \u4e0e\u591a\u76ee\u6807\u8d1d\u53f6\u65af\u4f18\u5316\u76f8\u7ed3\u5408\u7684\u65b0\u578b HW-NAS \u6846\u67b6", "result": "Coflex \u901a\u8fc7\u5229\u7528\u7a00\u758f\u8bf1\u5bfc\u70b9\uff0c\u5c06 GP \u5185\u6838\u590d\u6742\u5ea6\u4ece\u4e09\u6b21\u65b9\u964d\u4f4e\u5230\u63a5\u8fd1\u7ebf\u6027\uff0c\u800c\u4e0d\u4f1a\u5f71\u54cd\u4f18\u5316\u6027\u80fd\u3002\u8fd9\u4f7f\u5f97\u80fd\u591f\u5bf9\u5927\u89c4\u6a21\u641c\u7d22\u7a7a\u95f4\u8fdb\u884c\u53ef\u6269\u5c55\u7684\u903c\u8fd1\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5927\u5927\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "Coflex\u5728\u7f51\u7edc\u7cbe\u5ea6\u548c\u80fd\u91cf\u5ef6\u8fdf\u4e58\u79ef\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86 1.9 \u500d\u5230 9.5 \u500d\u7684\u8ba1\u7b97\u52a0\u901f\u3002"}}
{"id": "2507.23104", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23104", "abs": "https://arxiv.org/abs/2507.23104", "authors": ["Jeffrey Eben", "Aitzaz Ahmad", "Stephen Lau"], "title": "RASL: Retrieval Augmented Schema Linking for Massive Database Text-to-SQL", "comment": null, "summary": "Despite advances in large language model (LLM)-based natural language\ninterfaces for databases, scaling to enterprise-level data catalogs remains an\nunder-explored challenge. Prior works addressing this challenge rely on\ndomain-specific fine-tuning - complicating deployment - and fail to leverage\nimportant semantic context contained within database metadata. To address these\nlimitations, we introduce a component-based retrieval architecture that\ndecomposes database schemas and metadata into discrete semantic units, each\nseparately indexed for targeted retrieval. Our approach prioritizes effective\ntable identification while leveraging column-level information, ensuring the\ntotal number of retrieved tables remains within a manageable context budget.\nExperiments demonstrate that our method maintains high recall and accuracy,\nwith our system outperforming baselines over massive databases with varying\nstructure and available metadata. Our solution enables practical text-to-SQL\nsystems deployable across diverse enterprise settings without specialized\nfine-tuning, addressing a critical scalability gap in natural language database\ninterfaces.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6587\u672c\u5230 SQL \u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5229\u7528\u7ec4\u4ef6\u5316\u7684\u68c0\u7d22\u67b6\u6784\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5728\u5404\u79cd\u4f01\u4e1a\u73af\u5883\u4e2d\u4f7f\u7528\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u6570\u636e\u5e93\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u6269\u5c55\u5230\u4f01\u4e1a\u7ea7\u6570\u636e\u76ee\u5f55\u4ecd\u7136\u662f\u4e00\u4e2a\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u6311\u6218\u3002\u5148\u524d\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u7684\u8457\u4f5c\u4f9d\u8d56\u4e8e\u7279\u5b9a\u9886\u57df\u7684\u5fae\u8c03\u2014\u2014\u4f7f\u90e8\u7f72\u590d\u6742\u5316\u2014\u2014\u5e76\u4e14\u672a\u80fd\u5229\u7528\u6570\u636e\u5e93\u5143\u6570\u636e\u4e2d\u5305\u542b\u7684\u91cd\u8981\u8bed\u4e49\u4e0a\u4e0b\u6587\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ec4\u4ef6\u7684\u68c0\u7d22\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5c06\u6570\u636e\u5e93\u6a21\u5f0f\u548c\u5143\u6570\u636e\u5206\u89e3\u4e3a\u79bb\u6563\u7684\u8bed\u4e49\u5355\u5143\uff0c\u6bcf\u4e2a\u5355\u5143\u90fd\u5355\u72ec\u7d22\u5f15\u4ee5\u8fdb\u884c\u6709\u9488\u5bf9\u6027\u7684\u68c0\u7d22\u3002\u8be5\u65b9\u6cd5\u4f18\u5148\u8003\u8651\u6709\u6548\u7684\u8868\u8bc6\u522b\uff0c\u540c\u65f6\u5229\u7528\u5217\u7ea7\u4fe1\u606f\uff0c\u786e\u4fdd\u68c0\u7d22\u5230\u7684\u8868\u603b\u6570\u4fdd\u6301\u5728\u53ef\u7ba1\u7406\u7684\u4e0a\u4e0b\u6587\u9884\u7b97\u5185\u3002", "result": "\u8be5\u65b9\u6cd5\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u53ec\u56de\u7387\u548c\u51c6\u786e\u7387\uff0c\u5e76\u4e14\u8be5\u7cfb\u7edf\u5728\u5177\u6709\u4e0d\u540c\u7ed3\u6784\u548c\u53ef\u7528\u5143\u7684\u6d77\u91cf\u6570\u636e\u5e93\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u89e3\u51b3\u65b9\u6848\u65e0\u9700\u4e13\u95e8\u7684\u5fae\u8c03\u5373\u53ef\u5728\u5404\u79cd\u4f01\u4e1a\u73af\u5883\u4e2d\u90e8\u7f72\u5b9e\u7528\u7684\u6587\u672c\u5230 SQL \u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u81ea\u7136\u8bed\u8a00\u6570\u636e\u5e93\u63a5\u53e3\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u53ef\u6269\u5c55\u6027\u7f3a\u53e3\u3002"}}
{"id": "2507.23313", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23313", "abs": "https://arxiv.org/abs/2507.23313", "authors": ["Alfio Ferrara", "Sergio Picascia", "Elisabetta Rocchetti"], "title": "The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in Text-to-Image Models", "comment": "to be published in: Applications of AI in the Analysis of Cultural\n  and Artistic Heritage, organized within the 35th IEEE International Workshop\n  on Machine Learning for Signal Processing (MLSP) 2025", "summary": "Text-to-image diffusion models have demonstrated remarkable capabilities in\ngenerating artistic content by learning from billions of images, including\npopular artworks. However, the fundamental question of how these models\ninternally represent concepts, such as content and style in paintings, remains\nunexplored. Traditional computer vision assumes content and style are\northogonal, but diffusion models receive no explicit guidance about this\ndistinction during training. In this work, we investigate how transformer-based\ntext-to-image diffusion models encode content and style concepts when\ngenerating artworks. We leverage cross-attention heatmaps to attribute pixels\nin generated images to specific prompt tokens, enabling us to isolate image\nregions influenced by content-describing versus style-describing tokens. Our\nfindings reveal that diffusion models demonstrate varying degrees of\ncontent-style separation depending on the specific artistic prompt and style\nrequested. In many cases, content tokens primarily influence object-related\nregions while style tokens affect background and texture areas, suggesting an\nemergent understanding of the content-style distinction. These insights\ncontribute to our understanding of how large-scale generative models internally\nrepresent complex artistic concepts without explicit supervision. We share the\ncode and dataset, together with an exploratory tool for visualizing attention\nmaps at https://github.com/umilISLab/artistic-prompt-interpretation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8eTransformer\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u827a\u672f\u54c1\u65f6\u5982\u4f55\u7f16\u7801\u5185\u5bb9\u548c\u98ce\u683c\u6982\u5ff5\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u901a\u8fc7\u5b66\u4e60\u6570\u5341\u4ebf\u5f20\u56fe\u50cf\uff08\u5305\u62ec\u6d41\u884c\u7684\u827a\u672f\u54c1\uff09\u6765\u751f\u6210\u827a\u672f\u5185\u5bb9\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u975e\u51e1\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u5982\u4f55\u5728\u5185\u90e8\u8868\u793a\u6982\u5ff5\uff08\u4f8b\u5982\u7ed8\u753b\u4e2d\u7684\u5185\u5bb9\u548c\u98ce\u683c\uff09\u8fd9\u4e00\u6839\u672c\u95ee\u9898\u4ecd\u672a\u5f97\u5230\u63a2\u7d22\u3002\u4f20\u7edf\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u5047\u8bbe\u5185\u5bb9\u548c\u98ce\u683c\u662f\u6b63\u4ea4\u7684\uff0c\u4f46\u6269\u6563\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6ca1\u6709\u6536\u5230\u5173\u4e8e\u8fd9\u79cd\u533a\u5206\u7684\u660e\u786e\u6307\u5bfc\u3002", "method": "\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u70ed\u56fe\u5c06\u751f\u6210\u56fe\u50cf\u4e2d\u7684\u50cf\u7d20\u5f52\u56e0\u4e8e\u7279\u5b9a\u7684\u63d0\u793atoken\uff0c\u4ece\u800c\u80fd\u591f\u5206\u79bb\u53d7\u5185\u5bb9\u63cf\u8ff0token\u548c\u98ce\u683c\u63cf\u8ff0token\u5f71\u54cd\u7684\u56fe\u50cf\u533a\u57df\u3002", "result": "\u6269\u6563\u6a21\u578b\u8868\u73b0\u51fa\u4e0d\u540c\u7a0b\u5ea6\u7684\u5185\u5bb9-\u98ce\u683c\u5206\u79bb\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u7279\u5b9a\u7684\u827a\u672f\u63d0\u793a\u548c\u6240\u8981\u6c42\u7684\u98ce\u683c\u3002\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\uff0c\u5185\u5bb9token\u4e3b\u8981\u5f71\u54cd\u4e0e\u5bf9\u8c61\u76f8\u5173\u7684\u533a\u57df\uff0c\u800c\u98ce\u683ctoken\u5f71\u54cd\u80cc\u666f\u548c\u7eb9\u7406\u533a\u57df\uff0c\u8fd9\u8868\u660e\u5bf9\u5185\u5bb9-\u98ce\u683c\u533a\u5206\u7684\u521d\u6b65\u7406\u89e3\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u8868\u73b0\u51fa\u5185\u5bb9\u548c\u98ce\u683c\u7684\u5206\u79bb\uff0c\u5185\u5bb9token\u4e3b\u8981\u5f71\u54cd\u4e0e\u5bf9\u8c61\u76f8\u5173\u7684\u533a\u57df\uff0c\u800c\u98ce\u683ctoken\u5f71\u54cd\u80cc\u666f\u548c\u7eb9\u7406\u533a\u57df\uff0c\u8fd9\u8868\u660e\u5bf9\u5185\u5bb9-\u98ce\u683c\u5dee\u5f02\u7684\u521d\u6b65\u7406\u89e3\u3002"}}
{"id": "2507.23449", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23449", "abs": "https://arxiv.org/abs/2507.23449", "authors": ["Shervin Rahimzadeh Arashloo"], "title": "Manifold-regularised Signature Kernel Large-Margin $\\ell_p$-SVDD for Multidimensional Time Series Anomaly Detection", "comment": null, "summary": "We generalise the recently introduced large-margin $\\ell_p$-SVDD approach to\nexploit the geometry of data distribution via manifold regularising and a\nsignature kernel representation for time series anomaly detection.\nSpecifically, we formulate a manifold-regularised variant of the $\\ell_p$-SVDD\nmethod to encourage label smoothness on the underlying manifold to capture\nstructural information for improved detection performance. Drawing on an\nexisting Representer theorem, we then provide an effective optimisation\ntechnique for the proposed method and show that it can benefit from the\nsignature kernel to capture time series complexities for anomaly detection.\n  We theoretically study the proposed approach using Rademacher complexities to\nanalyse its generalisation performance and also provide an experimental\nassessment of the proposed method across various data sets to compare its\nperformance against other methods.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7684\u6d41\u5f62\u6b63\u5219\u5316 \\ell_p$-SVDD \u65b9\u6cd5\uff0c\u5e76\u8868\u660e\u5b83\u53ef\u4ee5\u4ece\u7b7e\u540d\u5185\u6838\u4e2d\u53d7\u76ca\u3002", "motivation": "\u6211\u4eec\u63a8\u5e7f\u4e86\u6700\u8fd1\u5f15\u5165\u7684\u5927\u8fb9\u8ddd \\ell_p$-SVDD \u65b9\u6cd5\uff0c\u4ee5\u901a\u8fc7\u6d41\u5f62\u6b63\u5219\u5316\u548c\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7684\u7b7e\u540d\u5185\u6838\u8868\u793a\u6765\u5229\u7528\u6570\u636e\u5206\u5e03\u7684\u51e0\u4f55\u5f62\u72b6\u3002", "method": "\u6211\u4eec\u5236\u5b9a\u4e86\u6d41\u5f62\u6b63\u5219\u5316\u7684 \\ell_p$-SVDD \u65b9\u6cd5\u7684\u53d8\u4f53\uff0c\u4ee5\u9f13\u52b1\u5e95\u5c42\u6d41\u5f62\u4e0a\u7684\u6807\u7b7e\u5e73\u6ed1\u6027\uff0c\u4ece\u800c\u6355\u83b7\u7ed3\u6784\u4fe1\u606f\u4ee5\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002\u5229\u7528\u73b0\u6709\u7684\u8868\u793a\u5b9a\u7406\uff0c\u6211\u4eec\u4e3a\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4f18\u5316\u6280\u672f\uff0c\u5e76\u8868\u660e\u5b83\u53ef\u4ee5\u4ece\u7b7e\u540d\u5185\u6838\u4e2d\u53d7\u76ca\uff0c\u4ee5\u6355\u83b7\u65f6\u95f4\u5e8f\u5217\u7684\u590d\u6742\u6027\u4ee5\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u4e0e\u5176\u4ed6\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u6211\u4eec\u4ece\u7406\u8bba\u4e0a\u7814\u7a76\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528 Rademacher \u590d\u6742\u5ea6\u6765\u5206\u6790\u5176\u6cdb\u5316\u6027\u80fd\uff0c\u5e76\u4e14\u8fd8\u901a\u8fc7\u5404\u79cd\u6570\u636e\u96c6\u5bf9\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u4ee5\u5c06\u5176\u6027\u80fd\u4e0e\u5176\u4ed6\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002"}}
{"id": "2507.23121", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23121", "abs": "https://arxiv.org/abs/2507.23121", "authors": ["Xinwei Wu", "Haojie Li", "Hongyu Liu", "Xinyu Ji", "Ruohan Li", "Yule Chen", "Yigeng Zhang"], "title": "Uncovering the Fragility of Trustworthy LLMs through Chinese Textual Ambiguity", "comment": "Accepted at KDD workshop on Evaluation and Trustworthiness of Agentic\n  and Generative AI Models (Agentic & GenAI Evaluation Workshop KDD '25)", "summary": "In this work, we study a critical research problem regarding the\ntrustworthiness of large language models (LLMs): how LLMs behave when\nencountering ambiguous narrative text, with a particular focus on Chinese\ntextual ambiguity. We created a benchmark dataset by collecting and generating\nambiguous sentences with context and their corresponding disambiguated pairs,\nrepresenting multiple possible interpretations. These annotated examples are\nsystematically categorized into 3 main categories and 9 subcategories. Through\nexperiments, we discovered significant fragility in LLMs when handling\nambiguity, revealing behavior that differs substantially from humans.\nSpecifically, LLMs cannot reliably distinguish ambiguous text from unambiguous\ntext, show overconfidence in interpreting ambiguous text as having a single\nmeaning rather than multiple meanings, and exhibit overthinking when attempting\nto understand the various possible meanings. Our findings highlight a\nfundamental limitation in current LLMs that has significant implications for\ntheir deployment in real-world applications where linguistic ambiguity is\ncommon, calling for improved approaches to handle uncertainty in language\nunderstanding. The dataset and code are publicly available at this GitHub\nrepository: https://github.com/ictup/LLM-Chinese-Textual-Disambiguation.", "AI": {"tldr": "LLM\u5728\u5904\u7406\u4e2d\u6587\u6587\u672c\u6b67\u4e49\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u6539\u8fdb\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b(llm)\u7684\u53ef\u4fe1\u5ea6\u65b9\u9762\u7684\u4e00\u4e2a\u5173\u952e\u7814\u7a76\u95ee\u9898:\u5f53llm\u9047\u5230\u6a21\u7cca\u7684\u53d9\u8ff0\u6587\u672c\u65f6\uff0c\u5b83\u4eec\u4f1a\u5982\u4f55\u8868\u73b0\uff0c\u7279\u522b\u5173\u6ce8\u4e2d\u6587\u6587\u672c\u7684\u6a21\u7cca\u6027\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6536\u96c6\u548c\u751f\u6210\u5e26\u6709\u4e0a\u4e0b\u6587\u7684\u6b67\u4e49\u53e5\u53ca\u5176\u76f8\u5e94\u7684\u6d88\u9664\u6b67\u4e49\u7684\u914d\u5bf9\uff0c\u4ee3\u8868\u591a\u79cd\u53ef\u80fd\u7684\u89e3\u91ca\u3002\u8fd9\u4e9b\u5e26\u6ce8\u91ca\u7684\u4f8b\u5b50\u88ab\u7cfb\u7edf\u5730\u5206\u4e3a3\u4e2a\u4e3b\u8981\u7c7b\u522b\u548c9\u4e2a\u5b50\u7c7b\u522b\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\uff0c\u6211\u4eec\u53d1\u73b0\u5728\u5904\u7406\u6b67\u4e49\u65f6\uff0cllm\u5b58\u5728\u663e\u8457\u7684\u8106\u5f31\u6027\uff0c\u63ed\u793a\u4e86\u4e0e\u4eba\u7c7b\u884c\u4e3a\u6709\u5f88\u5927\u4e0d\u540c\u7684\u884c\u4e3a\u3002\u5177\u4f53\u6765\u8bf4\uff0cllm\u4e0d\u80fd\u53ef\u9760\u5730\u533a\u5206\u6b67\u4e49\u6587\u672c\u548c\u975e\u6b67\u4e49\u6587\u672c\uff0c\u5728\u5c06\u6b67\u4e49\u6587\u672c\u89e3\u91ca\u4e3a\u5177\u6709\u5355\u4e00\u542b\u4e49\u800c\u4e0d\u662f\u591a\u79cd\u542b\u4e49\u65f6\u8868\u73b0\u51fa\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u5e76\u4e14\u5728\u8bd5\u56fe\u7406\u89e3\u5404\u79cd\u53ef\u80fd\u7684\u542b\u4e49\u65f6\u8868\u73b0\u51fa\u8fc7\u5ea6\u601d\u8003\u3002", "conclusion": "LLMs\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u4e0d\u540c\u7684\u884c\u4e3a\uff0c\u5e76\u4e14\u5728\u5904\u7406\u6b67\u4e49\u65f6\u5b58\u5728\u660e\u663e\u7684\u8106\u5f31\u6027\u3002\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5f53\u524dLLM\u7684\u4e00\u4e2a\u57fa\u672c\u5c40\u9650\u6027\uff0c\u8fd9\u5bf9\u5b83\u4eec\u5728\u8bed\u8a00\u6b67\u4e49\u666e\u904d\u5b58\u5728\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u9700\u8981\u6539\u8fdb\u5904\u7406\u8bed\u8a00\u7406\u89e3\u4e2d\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.23315", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23315", "abs": "https://arxiv.org/abs/2507.23315", "authors": ["Vineet Kumar Rakesh", "Soumya Mazumdar", "Tapas Samanta", "Sarbajit Pal", "Amitabha Das"], "title": "Impact of Hyperparameter Optimization on the Accuracy of Lightweight Deep Learning Models for Real-Time Image Classification", "comment": "13 pages, 4 figures, 4 tables. Includes ablation study and evaluation\n  on 7 lightweight deep learning models. Code and logs available at\n  https://github.com/VineetKumarRakesh/lcnn-opt", "summary": "Lightweight convolutional and transformer-based models have become vital for\nreal-time image classification in resource-constrained applications, such as\nembedded systems and edge devices. This work analyzes the influence of\nhyperparameter adjustment on the accuracy and convergence behavior of seven\nefficient deep learning architectures: EfficientNetV2-S, ConvNeXt-T, MobileViT\nv2 (XXS/XS/S), MobileNetV3-L, TinyViT-21M, and RepVGG-A2. All models are\ntrained on the ImageNet-1K dataset under consistent training settings, with an\nemphasis on real-time practicality. An comprehensive ablation study is\nundertaken to separate the effect of critical hyperparameters, including\nlearning rate schedules, batch sizes, input resolution, data augmentation,\nregularization approaches, and optimizer choice. To assess appropriateness for\nreal-time applications, each model is assessed not only in terms of Top-1 and\nTop-5 classification accuracy, but also in terms of inference time, parameter\ncount, model size, and frames-per-second (FPS) on a GPU-accelerated edge\ndeployment simulation. Results demonstrate that cosine learning rate decay and\nadjustable batch size may greatly boost both accuracy and convergence speed,\nwhile keeping low latency and memory cost. Notably, RepVGG-A2 achieves over 80%\nTop-1 accuracy with efficient inference performance, offering a compelling\nbalance between accuracy and deployment cost for VGG-style models. The results\ngive practical guidance for constructing resource-efficient deep learning\nmodels appropriate for real-time image processing pipelines. All code and\ntraining logs are publicly accessible at\nhttps://github.com/VineetKumarRakesh/lcnn-opt.", "AI": {"tldr": "This paper analyzes the influence of hyperparameter adjustment on the accuracy and convergence behavior of seven efficient deep learning architectures and find that cosine learning rate decay and adjustable batch size may greatly boost both accuracy and convergence speed, while keeping low latency and memory cost.", "motivation": "Lightweight convolutional and transformer-based models have become vital for real-time image classification in resource-constrained applications, such as embedded systems and edge devices.", "method": "An comprehensive ablation study is undertaken to separate the effect of critical hyperparameters, including learning rate schedules, batch sizes, input resolution, data augmentation, regularization approaches, and optimizer choice. All models are trained on the ImageNet-1K dataset under consistent training settings, with an emphasis on real-time practicality.", "result": "cosine learning rate decay and adjustable batch size may greatly boost both accuracy and convergence speed, while keeping low latency and memory cost.", "conclusion": "RepVGG-A2 achieves over 80% Top-1 accuracy with efficient inference performance, offering a compelling balance between accuracy and deployment cost for VGG-style models. The results give practical guidance for constructing resource-efficient deep learning models appropriate for real-time image processing pipelines."}}
{"id": "2507.23491", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23491", "abs": "https://arxiv.org/abs/2507.23491", "authors": ["Olga Vershinina", "Jacopo Sabbatinelli", "Anna Rita Bonfigli", "Dalila Colombaretti", "Angelica Giuliani", "Mikhail Krivonosov", "Arseniy Trukhanov", "Claudio Franceschi", "Mikhail Ivanchenko", "Fabiola Olivieri"], "title": "Explainable artificial intelligence model predicting the risk of all-cause mortality in patients with type 2 diabetes mellitus", "comment": null, "summary": "Objective. Type 2 diabetes mellitus (T2DM) is a highly prevalent\nnon-communicable chronic disease that substantially reduces life expectancy.\nAccurate estimation of all-cause mortality risk in T2DM patients is crucial for\npersonalizing and optimizing treatment strategies. Research Design and Methods.\nThis study analyzed a cohort of 554 patients (aged 40-87 years) with diagnosed\nT2DM over a maximum follow-up period of 16.8 years, during which 202 patients\n(36%) died. Key survival-associated features were identified, and multiple\nmachine learning (ML) models were trained and validated to predict all-cause\nmortality risk. To improve model interpretability, Shapley additive\nexplanations (SHAP) was applied to the best-performing model. Results. The\nextra survival trees (EST) model, incorporating ten key features, demonstrated\nthe best predictive performance. The model achieved a C-statistic of 0.776,\nwith the area under the receiver operating characteristic curve (AUC) values of\n0.86, 0.80, 0.841, and 0.826 for 5-, 10-, 15-, and 16.8-year all-cause\nmortality predictions, respectively. The SHAP approach was employed to\ninterpret the model's individual decision-making processes. Conclusions. The\ndeveloped model exhibited strong predictive performance for mortality risk\nassessment. Its clinically interpretable outputs enable potential bedside\napplication, improving the identification of high-risk patients and supporting\ntimely treatment optimization.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b 2 \u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u7684\u5168\u56e0\u6b7b\u4ea1\u98ce\u9669\uff0c\u8be5\u6a21\u578b\u5177\u6709\u826f\u597d\u7684\u9884\u6d4b\u6027\u80fd\u548c\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "2 \u578b\u7cd6\u5c3f\u75c5 (T2DM) \u662f\u4e00\u79cd\u9ad8\u5ea6\u6d41\u884c\u7684\u975e\u4f20\u67d3\u6027\u6162\u6027\u75be\u75c5\uff0c\u4f1a\u5927\u5927\u964d\u4f4e\u9884\u671f\u5bff\u547d\u3002\u51c6\u786e\u4f30\u8ba1 T2DM \u60a3\u8005\u7684\u5168\u56e0\u6b7b\u4ea1\u7387\u98ce\u9669\u5bf9\u4e8e\u4e2a\u6027\u5316\u548c\u4f18\u5316\u6cbb\u7597\u7b56\u7565\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86\u4e00\u4e2a\u7531 554 \u540d 40-87 \u5c81 T2DM \u60a3\u8005\u7ec4\u6210\u7684\u961f\u5217\uff0c\u6700\u957f\u968f\u8bbf\u671f\u4e3a 16.8 \u5e74\uff0c\u5728\u6b64\u671f\u95f4 202 \u540d\u60a3\u8005 (36%) \u6b7b\u4ea1\u3002\u786e\u5b9a\u4e86\u5173\u952e\u7684\u751f\u5b58\u76f8\u5173\u7279\u5f81\uff0c\u5e76\u8bad\u7ec3\u548c\u9a8c\u8bc1\u4e86\u591a\u4e2a\u673a\u5668\u5b66\u4e60 (ML) \u6a21\u578b\u6765\u9884\u6d4b\u5168\u56e0\u6b7b\u4ea1\u98ce\u9669\u3002\u4e3a\u4e86\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5c06 Shapley additive explanations (SHAP) \u5e94\u7528\u4e8e\u6027\u80fd\u6700\u4f73\u7684\u6a21\u578b\u3002", "result": "\u5305\u542b\u5341\u4e2a\u5173\u952e\u7279\u5f81\u7684 extra survival trees (EST) \u6a21\u578b\u8868\u73b0\u51fa\u6700\u4f73\u7684\u9884\u6d4b\u6027\u80fd\u3002\u8be5\u6a21\u578b\u5b9e\u73b0\u7684 C \u7edf\u8ba1\u91cf\u4e3a 0.776\uff0c5 \u5e74\u300110 \u5e74\u300115 \u5e74\u548c 16.8 \u5e74\u5168\u56e0\u6b7b\u4ea1\u7387\u9884\u6d4b\u7684\u53d7\u8bd5\u8005\u5de5\u4f5c\u7279\u5f81\u66f2\u7ebf\u4e0b\u9762\u79ef (AUC) \u503c\u5206\u522b\u4e3a 0.86\u30010.80\u30010.841 \u548c 0.826\u3002\u91c7\u7528 SHAP \u65b9\u6cd5\u6765\u89e3\u91ca\u6a21\u578b\u7684\u4e2a\u4f53\u51b3\u7b56\u8fc7\u7a0b\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u6b7b\u4ea1\u98ce\u9669\u8bc4\u4f30\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5176\u4e34\u5e8a\u53ef\u89e3\u91ca\u7684\u8f93\u51fa\u4f7f\u5176\u80fd\u591f\u6f5c\u5728\u5730\u5e94\u7528\u4e8e\u5e8a\u65c1\uff0c\u4ece\u800c\u6539\u8fdb\u9ad8\u5371\u60a3\u8005\u7684\u8bc6\u522b\u5e76\u652f\u6301\u53ca\u65f6\u7684\u6cbb\u7597\u4f18\u5316\u3002"}}
{"id": "2507.23135", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23135", "abs": "https://arxiv.org/abs/2507.23135", "authors": ["Ananya Sadana", "Yash Kumar Lal", "Jiawei Zhou"], "title": "ISO-Bench: Benchmarking Multimodal Causal Reasoning in Visual-Language Models through Procedural Plans", "comment": null, "summary": "Understanding causal relationships across modalities is a core challenge for\nmultimodal models operating in real-world environments. We introduce ISO-Bench,\na benchmark for evaluating whether models can infer causal dependencies between\nvisual observations and procedural text. Each example presents an image of a\ntask step and a text snippet from a plan, with the goal of deciding whether the\nvisual step occurs before or after the referenced text step. Evaluation results\non ten frontier vision-language models show underwhelming performance: the best\nzero-shot F1 is only 0.57, and chain-of-thought reasoning yields only modest\ngains (up to 0.62 F1), largely behind humans (0.98 F1). Our analysis further\nhighlights concrete directions for improving causal understanding in multimodal\nmodels.", "AI": {"tldr": "introduce ISO-Bench, a benchmark for evaluating whether models can infer causal dependencies between visual observations and procedural text", "motivation": "Understanding causal relationships across modalities is a core challenge for multimodal models operating in real-world environments", "method": "introducing ISO-Bench, a benchmark for evaluating whether models can infer causal dependencies between visual observations and procedural text", "result": "the best zero-shot F1 is only 0.57, and chain-of-thought reasoning yields only modest gains (up to 0.62 F1), largely behind humans (0.98 F1)", "conclusion": "Evaluation results on ten frontier vision-language models show underwhelming performance"}}
{"id": "2507.23318", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23318", "abs": "https://arxiv.org/abs/2507.23318", "authors": ["Jiajun Cao", "Qizhe Zhang", "Peidong Jia", "Xuhui Zhao", "Bo Lan", "Xiaoan Zhang", "Xiaobao Wei", "Sixiang Chen", "Zhuo Li", "Yang Wang", "Liyun Li", "Xianming Liu", "Ming Lu", "Shanghang Zhang"], "title": "FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning", "comment": "9 pages, 5 figures", "summary": "Vision-Language-Action (VLA) models have demonstrated significant potential\nin complex scene understanding and action reasoning, leading to their\nincreasing adoption in end-to-end autonomous driving systems. However, the long\nvisual tokens of VLA models greatly increase computational costs. Current\nvisual token pruning methods in Vision-Language Models (VLM) rely on either\nvisual token similarity or visual-text attention, but both have shown poor\nperformance in autonomous driving scenarios. Given that human drivers\nconcentrate on relevant foreground areas while driving, we assert that\nretaining visual tokens containing this foreground information is essential for\neffective decision-making. Inspired by this, we propose FastDriveVLA, a novel\nreconstruction-based vision token pruning framework designed specifically for\nautonomous driving. FastDriveVLA includes a plug-and-play visual token pruner\ncalled ReconPruner, which prioritizes foreground information through MAE-style\npixel reconstruction. A novel adversarial foreground-background reconstruction\nstrategy is designed to train ReconPruner for the visual encoder of VLA models.\nOnce trained, ReconPruner can be seamlessly applied to different VLA models\nwith the same visual encoder without retraining. To train ReconPruner, we also\nintroduce a large-scale dataset called nuScenes-FG, consisting of 241K\nimage-mask pairs with annotated foreground regions. Our approach achieves\nstate-of-the-art results on the nuScenes closed-loop planning benchmark across\ndifferent pruning ratios.", "AI": {"tldr": "FastDriveVLA, a novel vision token pruning framework for autonomous driving, reduces computational costs by prioritizing foreground information. It outperforms existing methods on the nuScenes benchmark.", "motivation": "Long visual tokens in VLA models increase computational costs, and current visual token pruning methods perform poorly in autonomous driving. Human drivers focus on foreground areas, so retaining visual tokens containing foreground information is essential.", "method": "The paper proposes FastDriveVLA, a reconstruction-based vision token pruning framework with a plug-and-play visual token pruner called ReconPruner. It uses MAE-style pixel reconstruction and an adversarial foreground-background reconstruction strategy. A new dataset, nuScenes-FG, is also introduced.", "result": "FastDriveVLA achieves state-of-the-art results on the nuScenes closed-loop planning benchmark.", "conclusion": "The proposed FastDriveVLA achieves state-of-the-art results on the nuScenes closed-loop planning benchmark across different pruning ratios."}}
{"id": "2507.23495", "categories": ["cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.23495", "abs": "https://arxiv.org/abs/2507.23495", "authors": ["Maurits Kaptein"], "title": "Incorporating structural uncertainty in causal decision making", "comment": "This work is under review at the Journal of Causal Inference", "summary": "Practitioners making decisions based on causal effects typically ignore\nstructural uncertainty. We analyze when this uncertainty is consequential\nenough to warrant methodological solutions (Bayesian model averaging over\ncompeting causal structures). Focusing on bivariate relationships ($X\n\\rightarrow Y$ vs. $X \\leftarrow Y$), we establish that model averaging is\nbeneficial when: (1) structural uncertainty is moderate to high, (2) causal\neffects differ substantially between structures, and (3) loss functions are\nsufficiently sensitive to the size of the causal effect. We prove optimality\nresults of our suggested methodological solution under regularity conditions\nand demonstrate through simulations that modern causal discovery methods can\nprovide, within limits, the necessary quantification. Our framework complements\nexisting robust causal inference approaches by addressing a distinct source of\nuncertainty typically overlooked in practice.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u56e0\u679c\u63a8\u65ad\u4e2d\u7ed3\u6784\u4e0d\u786e\u5b9a\u6027\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4f7f\u7528\u8d1d\u53f6\u65af\u6a21\u578b\u5e73\u5747\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u51b3\u7b56\u8005\u5728\u8fdb\u884c\u56e0\u679c\u6548\u5e94\u51b3\u7b56\u65f6\u901a\u5e38\u5ffd\u7565\u7ed3\u6784\u4e0d\u786e\u5b9a\u6027\u3002\u672c\u6587\u65e8\u5728\u5206\u6790\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u5728\u4f55\u65f6\u4f1a\u4ea7\u751f\u8db3\u591f\u91cd\u8981\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u9700\u8981\u91c7\u53d6\u65b9\u6cd5\u8bba\u4e0a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u8d1d\u53f6\u65af\u6a21\u578b\u5e73\u5747\u6765\u5904\u7406\u7ade\u4e89\u6027\u56e0\u679c\u7ed3\u6784\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5f53\u7ed3\u6784\u4e0d\u786e\u5b9a\u6027\u4e3a\u4e2d\u7b49\u5230\u9ad8\uff0c\u56e0\u679c\u6548\u5e94\u5728\u4e0d\u540c\u7ed3\u6784\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4e14\u635f\u5931\u51fd\u6570\u5bf9\u56e0\u679c\u6548\u5e94\u7684\u5927\u5c0f\u8db3\u591f\u654f\u611f\u65f6\uff0c\u6a21\u578b\u5e73\u5747\u662f\u6709\u76ca\u7684\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u89e3\u51b3\u7ed3\u6784\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5728\u4e00\u5b9a\u6761\u4ef6\u4e0b\u8be5\u65b9\u6cd5\u7684\u6700\u4f18\u6027\u3002\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u73b0\u4ee3\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u53ef\u4ee5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u63d0\u4f9b\u5fc5\u8981\u7684\u91cf\u5316\u3002"}}
{"id": "2507.23158", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23158", "abs": "https://arxiv.org/abs/2507.23158", "authors": ["Yuhan Liu", "Michael J. Q. Zhang", "Eunsol Choi"], "title": "User Feedback in Human-LLM Dialogues: A Lens to Understand Users But Noisy as a Learning Signal", "comment": "Earlier version of this paper was presented at 2nd Workshop on Models\n  of Human Feedback for AI Alignment (MoFA), ICML 2025", "summary": "Once language models (LMs) are deployed, they can interact with users\nlong-term, ideally evolving continuously based on their feedback. Asking for\ndirect user feedback can be disruptive; thus, we study harvesting user feedback\nfrom user-LM interaction logs. We study implicit user feedback in two user-LM\ninteraction datasets (WildChat and LMSYS). First, we analyze user feedback in\nthe user-LLM conversation trajectory, providing insights into when and why such\nfeedback occurs. Second, we study harvesting learning signals from such\nimplicit user feedback. We find that the contents of user feedback (e.g., user\nwanted clarification), not just the polarity (e.g., users were unhappy with the\nprevious model response), can improve model performance in short human-designed\nquestions (MTBench) but not on longer and more complex questions (WildBench).\nWe also find that the usefulness of user feedback is largely tied to the\nquality of the user's initial prompt. Together, we provide an in-depth study of\nimplicit user feedback, showing its potential and limitations.", "AI": {"tldr": "\u6211\u4eec\u7814\u7a76\u4e86\u4ece\u7528\u6237-LM\u4ea4\u4e92\u65e5\u5fd7\u4e2d\u83b7\u53d6\u7528\u6237\u53cd\u9988\uff0c\u53d1\u73b0\u7528\u6237\u53cd\u9988\u7684\u5185\u5bb9\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u5728\u7b80\u77ed\u95ee\u9898\u4e0a\u7684\u6027\u80fd\uff0c\u4f46\u4e0d\u80fd\u63d0\u9ad8\u5728\u8f83\u957f\u548c\u66f4\u590d\u6742\u7684\u95ee\u9898\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u76f4\u63a5\u7684\u7528\u6237\u53cd\u9988\u53ef\u80fd\u4f1a\u9020\u6210\u5e72\u6270\uff1b\u56e0\u6b64\uff0c\u6211\u4eec\u7814\u7a76\u4ece\u7528\u6237-LM\u4ea4\u4e92\u65e5\u5fd7\u4e2d\u83b7\u53d6\u7528\u6237\u53cd\u9988\u3002", "method": "\u7814\u7a76\u4e86\u4e24\u4e2a\u7528\u6237-LM\u4ea4\u4e92\u6570\u636e\u96c6\uff08WildChat\u548cLMSYS\uff09\u4e2d\u7684\u9690\u5f0f\u7528\u6237\u53cd\u9988", "result": "\u7528\u6237\u53cd\u9988\u7684\u5185\u5bb9\uff08\u4f8b\u5982\uff0c\u7528\u6237\u60f3\u8981\u6f84\u6e05\uff09\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u5728\u7b80\u77ed\u7684\u4eba\u5de5\u8bbe\u8ba1\u95ee\u9898\uff08MTBench\uff09\u4e0a\u7684\u6027\u80fd\uff0c\u4f46\u4e0d\u80fd\u63d0\u9ad8\u5728\u8f83\u957f\u548c\u66f4\u590d\u6742\u7684\u95ee\u9898\uff08WildBench\uff09\u4e0a\u7684\u6027\u80fd\u3002\u6211\u4eec\u8fd8\u53d1\u73b0\uff0c\u7528\u6237\u53cd\u9988\u7684\u6709\u7528\u6027\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4e0e\u7528\u6237\u521d\u59cb\u63d0\u793a\u7684\u8d28\u91cf\u6709\u5173\u3002", "conclusion": "\u5bf9\u9690\u5f0f\u7528\u6237\u53cd\u9988\u8fdb\u884c\u4e86\u6df1\u5165\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u5b83\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2507.23325", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23325", "abs": "https://arxiv.org/abs/2507.23325", "authors": ["Yiming Yang", "Hongbin Lin", "Yueru Luo", "Suzhong Fu", "Chao Zheng", "Xinrui Yan", "Shuqi Mei", "Kun Tang", "Shuguang Cui", "Zhen Li"], "title": "FASTopoWM: Fast-Slow Lane Segment Topology Reasoning with Latent World Models", "comment": null, "summary": "Lane segment topology reasoning provides comprehensive bird's-eye view (BEV)\nroad scene understanding, which can serve as a key perception module in\nplanning-oriented end-to-end autonomous driving systems. Existing lane topology\nreasoning methods often fall short in effectively leveraging temporal\ninformation to enhance detection and reasoning performance. Recently,\nstream-based temporal propagation method has demonstrated promising results by\nincorporating temporal cues at both the query and BEV levels. However, it\nremains limited by over-reliance on historical queries, vulnerability to pose\nestimation failures, and insufficient temporal propagation. To overcome these\nlimitations, we propose FASTopoWM, a novel fast-slow lane segment topology\nreasoning framework augmented with latent world models. To reduce the impact of\npose estimation failures, this unified framework enables parallel supervision\nof both historical and newly initialized queries, facilitating mutual\nreinforcement between the fast and slow systems. Furthermore, we introduce\nlatent query and BEV world models conditioned on the action latent to propagate\nthe state representations from past observations to the current timestep. This\ndesign substantially improves the performance of temporal perception within the\nslow pipeline. Extensive experiments on the OpenLane-V2 benchmark demonstrate\nthat FASTopoWM outperforms state-of-the-art methods in both lane segment\ndetection (37.4% v.s. 33.6% on mAP) and centerline perception (46.3% v.s. 41.5%\non OLS).", "AI": {"tldr": "FASTopoWM\u662f\u4e00\u79cd\u65b0\u7684\u8f66\u9053\u7ebf\u5206\u5272\u62d3\u6251\u63a8\u7406\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u6f5c\u5728\u4e16\u754c\u6a21\u578b\u589e\u5f3a\uff0c\u4ece\u800c\u5728\u8f66\u9053\u7ebf\u5206\u5272\u68c0\u6d4b\u548c\u4e2d\u5fc3\u7ebf\u611f\u77e5\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u8f66\u9053\u7ebf\u62d3\u6251\u63a8\u7406\u65b9\u6cd5\u901a\u5e38\u65e0\u6cd5\u6709\u6548\u5730\u5229\u7528\u65f6\u95f4\u4fe1\u606f\u6765\u589e\u5f3a\u68c0\u6d4b\u548c\u63a8\u7406\u6027\u80fd\uff1b\u5bf9\u5386\u53f2\u67e5\u8be2\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u5bb9\u6613\u53d7\u5230\u59ff\u6001\u4f30\u8ba1\u5931\u8d25\u7684\u5f71\u54cd\uff0c\u5e76\u4e14\u65f6\u95f4\u4f20\u64ad\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5feb-\u6162\u8f66\u9053\u7ebf\u5206\u5272\u62d3\u6251\u63a8\u7406\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u6f5c\u5728\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u589e\u5f3a\u3002", "result": "FASTopoWM\u5728OpenLane-V2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "FASTopoWM\u5728\u8f66\u9053\u7ebf\u5206\u5272\u68c0\u6d4b(mAP 37.4% vs 33.6%)\u548c\u4e2d\u5fc3\u7ebf\u611f\u77e5(OLS 46.3% vs 41.5%)\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.23501", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.23501", "abs": "https://arxiv.org/abs/2507.23501", "authors": ["Nicklas Werge", "Yi-Shan Wu", "Bahareh Tasdighi", "Melih Kandemir"], "title": "Directional Ensemble Aggregation for Actor-Critics", "comment": null, "summary": "Off-policy reinforcement learning in continuous control tasks depends\ncritically on accurate $Q$-value estimates. Conservative aggregation over\nensembles, such as taking the minimum, is commonly used to mitigate\noverestimation bias. However, these static rules are coarse, discard valuable\ninformation from the ensemble, and cannot adapt to task-specific needs or\ndifferent learning regimes. We propose Directional Ensemble Aggregation (DEA),\nan aggregation method that adaptively combines $Q$-value estimates in\nactor-critic frameworks. DEA introduces two fully learnable directional\nparameters: one that modulates critic-side conservatism and another that guides\nactor-side policy exploration. Both parameters are learned using ensemble\ndisagreement-weighted Bellman errors, which weight each sample solely by the\ndirection of its Bellman error. This directional learning mechanism allows DEA\nto adjust conservatism and exploration in a data-driven way, adapting\naggregation to both uncertainty levels and the phase of training. We evaluate\nDEA across continuous control benchmarks and learning regimes - from\ninteractive to sample-efficient - and demonstrate its effectiveness over static\nensemble strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u65b9\u5411\u96c6\u6210\u805a\u5408\uff08DEA\uff09\u7684\u81ea\u9002\u5e94Q\u503c\u4f30\u8ba1\u7ec4\u5408\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u65b9\u5411\u53c2\u6570\u6765\u8c03\u8282\u4fdd\u5b88\u6027\u548c\u63a2\u7d22\uff0c\u5e76\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u9759\u6001\u96c6\u6210\u7b56\u7565\u7684\u6027\u80fd\u3002", "motivation": "\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u79bb\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u5173\u952e\u53d6\u51b3\u4e8e\u51c6\u786e\u7684Q\u503c\u4f30\u8ba1\u3002\u5bf9\u96c6\u6210\u8fdb\u884c\u4fdd\u5b88\u805a\u5408\uff08\u4f8b\u5982\u53d6\u6700\u5c0f\u503c\uff09\u901a\u5e38\u7528\u4e8e\u51cf\u8f7b\u8fc7\u5ea6\u4f30\u8ba1\u504f\u5dee\u3002\u4f46\u662f\uff0c\u8fd9\u4e9b\u9759\u6001\u89c4\u5219\u662f\u7c97\u7cd9\u7684\uff0c\u4e22\u5f03\u4e86\u6765\u81ea\u96c6\u6210\u7684\u6709\u4ef7\u503c\u4fe1\u606f\uff0c\u5e76\u4e14\u65e0\u6cd5\u9002\u5e94\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u9700\u6c42\u6216\u4e0d\u540c\u7684\u5b66\u4e60\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u5411\u96c6\u6210\u805a\u5408\uff08DEA\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u7ec4\u5408actor-critic\u6846\u67b6\u4e2d\u7684Q\u503c\u4f30\u8ba1\u7684\u805a\u5408\u65b9\u6cd5\u3002DEA\u5f15\u5165\u4e86\u4e24\u4e2a\u5b8c\u5168\u53ef\u5b66\u4e60\u7684\u65b9\u5411\u53c2\u6570\uff1a\u4e00\u4e2a\u8c03\u8282\u8bc4\u8bba\u5bb6\u4e00\u65b9\u7684\u4fdd\u5b88\u6027\uff0c\u53e6\u4e00\u4e2a\u6307\u5bfc\u6f14\u5458\u4e00\u65b9\u7684\u7b56\u7565\u63a2\u7d22\u3002\u8fd9\u4e24\u4e2a\u53c2\u6570\u90fd\u662f\u4f7f\u7528\u96c6\u6210\u4e0d\u4e00\u81f4\u52a0\u6743\u8d1d\u5c14\u66fc\u8bef\u5dee\u5b66\u4e60\u7684\uff0c\u8be5\u8bef\u5dee\u4ec5\u6309\u5176\u8d1d\u5c14\u66fc\u8bef\u5dee\u7684\u65b9\u5411\u5bf9\u6bcf\u4e2a\u6837\u672c\u8fdb\u884c\u52a0\u6743\u3002", "result": "\u65b9\u5411\u5b66\u4e60\u673a\u5236\u5141\u8bb8DEA\u4ee5\u6570\u636e\u9a71\u52a8\u7684\u65b9\u5f0f\u8c03\u6574\u4fdd\u5b88\u6027\u548c\u63a2\u7d22\uff0c\u4ece\u800c\u4f7f\u805a\u5408\u9002\u5e94\u4e0d\u786e\u5b9a\u6027\u6c34\u5e73\u548c\u8bad\u7ec3\u9636\u6bb5\u3002", "conclusion": "DEA\u5728\u5404\u79cd\u8fde\u7eed\u63a7\u5236\u57fa\u51c6\u548c\u5b66\u4e60\u65b9\u6848\uff08\u4ece\u4ea4\u4e92\u5f0f\u5230\u6837\u672c\u9ad8\u6548\uff09\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u76f8\u5bf9\u4e8e\u9759\u6001\u96c6\u6210\u7b56\u7565\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.23167", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.23167", "abs": "https://arxiv.org/abs/2507.23167", "authors": ["Jizhou Guo"], "title": "LENS: Learning Ensemble Confidence from Neural States for Multi-LLM Answer Integration", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, with different models excelling in distinct domains and specific\nabilities. Effectively combining the predictions of multiple LLMs is crucial\nfor enhancing system robustness and performance. However, existing ensemble\nmethods often rely on simple techniques like voting or logits ensembling, which\noverlook the varying confidence and reliability of models in different\ncontexts. In this work, we propose LENS (Learning ENsemble confidence from\nNeural States), a novel approach that learns to estimate model confidence by\nanalyzing internal representations. For each LLM, we train a lightweight linear\nconfidence predictor that leverages layer-wise hidden states and normalized\nprobabilities as inputs. This allows for more nuanced weighting of model\npredictions based on their context-dependent reliability. Our method does not\nrequire modifying the model parameters and requires negligible additional\ncomputation. Experimental results on multiple-choice and boolean\nquestion-answering tasks demonstrate that LENS outperforms traditional ensemble\nmethods by a substantial margin. Our findings suggest that internal\nrepresentations provide valuable signals for determining model confidence and\ncan be effectively leveraged for ensemble learning.", "AI": {"tldr": "LENS\u901a\u8fc7\u5206\u6790\u5185\u90e8\u8868\u5f81\u6765\u5b66\u4e60\u4f30\u8ba1\u6a21\u578b\u7f6e\u4fe1\u5ea6\uff0c\u4ece\u800c\u4f18\u4e8e\u4f20\u7edf\u7684\u96c6\u6210\u65b9\u6cd5\u3002", "motivation": "\u6709\u6548\u5730\u7ed3\u5408\u591a\u4e2aLLM\u7684\u9884\u6d4b\u5bf9\u4e8e\u589e\u5f3a\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u96c6\u6210\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u7b80\u5355\u7684\u6280\u672f\uff0c\u5982\u6295\u7968\u6216logits\u96c6\u6210\uff0c\u8fd9\u5ffd\u7565\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u4e2d\u4e0d\u540c\u7684\u7f6e\u4fe1\u5ea6\u548c\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faLENS (Learning ENsemble confidence from Neural States)\uff0c\u8fd9\u662f\u4e00\u79cd\u901a\u8fc7\u5206\u6790\u5185\u90e8\u8868\u5f81\u6765\u5b66\u4e60\u4f30\u8ba1\u6a21\u578b\u7f6e\u4fe1\u5ea6\u7684\u65b0\u65b9\u6cd5\u3002\u5bf9\u4e8e\u6bcf\u4e2aLLM\uff0c\u6211\u4eec\u8bad\u7ec3\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u7ebf\u6027\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u5668\uff0c\u8be5\u9884\u6d4b\u5668\u5229\u7528\u9010\u5c42\u9690\u85cf\u72b6\u6001\u548c\u6807\u51c6\u5316\u6982\u7387\u4f5c\u4e3a\u8f93\u5165\u3002", "result": "\u5728\u591a\u9879\u9009\u62e9\u548c\u5e03\u5c14\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLENS\u7684\u6027\u80fd\u660e\u663e\u4f18\u4e8e\u4f20\u7edf\u7684\u96c6\u6210\u65b9\u6cd5\u3002", "conclusion": "\u5185\u90e8\u8868\u5f81\u4e3a\u786e\u5b9a\u6a21\u578b\u7f6e\u4fe1\u5ea6\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u4fe1\u53f7\uff0c\u5e76\u4e14\u53ef\u4ee5\u6709\u6548\u5730\u7528\u4e8e\u96c6\u6210\u5b66\u4e60\u3002"}}
{"id": "2507.23326", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23326", "abs": "https://arxiv.org/abs/2507.23326", "authors": ["Yingkai Wang", "Yaoyao Zhu", "Xiuding Cai", "Yuhao Xiao", "Haotian Wu", "Yu Yao"], "title": "Learning Semantic Directions for Feature Augmentation in Domain-Generalized Medical Segmentation", "comment": null, "summary": "Medical image segmentation plays a crucial role in clinical workflows, but\ndomain shift often leads to performance degradation when models are applied to\nunseen clinical domains. This challenge arises due to variations in imaging\nconditions, scanner types, and acquisition protocols, limiting the practical\ndeployment of segmentation models. Unlike natural images, medical images\ntypically exhibit consistent anatomical structures across patients, with\ndomain-specific variations mainly caused by imaging conditions. This unique\ncharacteristic makes medical image segmentation particularly challenging.\n  To address this challenge, we propose a domain generalization framework\ntailored for medical image segmentation. Our approach improves robustness to\ndomain-specific variations by introducing implicit feature perturbations guided\nby domain statistics. Specifically, we employ a learnable semantic direction\nselector and a covariance-based semantic intensity sampler to modulate\ndomain-variant features while preserving task-relevant anatomical consistency.\nFurthermore, we design an adaptive consistency constraint that is selectively\napplied only when feature adjustment leads to degraded segmentation\nperformance. This constraint encourages the adjusted features to align with the\noriginal predictions, thereby stabilizing feature selection and improving the\nreliability of the segmentation.\n  Extensive experiments on two public multi-center benchmarks show that our\nframework consistently outperforms existing domain generalization approaches,\nachieving robust and generalizable segmentation performance across diverse\nclinical domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9886\u57df\u6cdb\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u7531\u9886\u57df\u7edf\u8ba1\u6570\u636e\u5f15\u5bfc\u7684\u9690\u5f0f\u7279\u5f81\u6270\u52a8\u6765\u63d0\u9ad8\u5bf9\u9886\u57df\u7279\u5b9a\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u4e0d\u540c\u7684\u4e34\u5e8a\u9886\u57df\u5b9e\u73b0\u4e86\u7a33\u5065\u548c\u53ef\u6cdb\u5316\u7684\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u5728\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u4f46\u662f\uff0c\u5f53\u5c06\u6a21\u578b\u5e94\u7528\u4e8e\u770b\u4e0d\u89c1\u7684\u4e34\u5e8a\u9886\u57df\u65f6\uff0c\u9886\u57df\u8f6c\u79fb\u901a\u5e38\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u7531\u4e8e\u6210\u50cf\u6761\u4ef6\u3001\u626b\u63cf\u4eea\u7c7b\u578b\u548c\u91c7\u96c6\u534f\u8bae\u7684\u53d8\u5316\uff0c\u5bfc\u81f4\u4e86\u8fd9\u4e00\u6311\u6218\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5206\u5272\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u3002\u4e0e\u81ea\u7136\u56fe\u50cf\u4e0d\u540c\uff0c\u533b\u5b66\u56fe\u50cf\u901a\u5e38\u5728\u60a3\u8005\u4e4b\u95f4\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u89e3\u5256\u7ed3\u6784\uff0c\u800c\u9886\u57df\u7279\u5f02\u6027\u53d8\u5316\u4e3b\u8981\u7531\u6210\u50cf\u6761\u4ef6\u5f15\u8d77\u3002\u8fd9\u79cd\u72ec\u7279\u7684\u7279\u6027\u4f7f\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7279\u522b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u91cf\u8eab\u5b9a\u5236\u7684\u9886\u57df\u6cdb\u5316\u6846\u67b6\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u7531\u9886\u57df\u7edf\u8ba1\u6570\u636e\u5f15\u5bfc\u7684\u9690\u5f0f\u7279\u5f81\u6270\u52a8\u6765\u63d0\u9ad8\u5bf9\u9886\u57df\u7279\u5b9a\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u91c7\u7528\u53ef\u5b66\u4e60\u7684\u8bed\u4e49\u65b9\u5411\u9009\u62e9\u5668\u548c\u57fa\u4e8e\u534f\u65b9\u5dee\u7684\u8bed\u4e49\u5f3a\u5ea6\u91c7\u6837\u5668\u6765\u8c03\u8282\u9886\u57df\u53d8\u5f02\u7279\u5f81\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u76f8\u5173\u7684\u89e3\u5256\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u8be5\u7ea6\u675f\u4ec5\u5728\u7279\u5f81\u8c03\u6574\u5bfc\u81f4\u5206\u5272\u6027\u80fd\u4e0b\u964d\u65f6\u624d\u9009\u62e9\u6027\u5730\u5e94\u7528\u3002\u6b64\u7ea6\u675f\u9f13\u52b1\u8c03\u6574\u540e\u7684\u7279\u5f81\u4e0e\u539f\u59cb\u9884\u6d4b\u5bf9\u9f50\uff0c\u4ece\u800c\u7a33\u5b9a\u7279\u5f81\u9009\u62e9\u5e76\u63d0\u9ad8\u5206\u5272\u7684\u53ef\u9760\u6027\u3002", "result": "\u6211\u4eec\u7684\u6846\u67b6\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u7684\u4e34\u5e8a\u9886\u57df\u5b9e\u73b0\u4e86\u7a33\u5065\u548c\u53ef\u6cdb\u5316\u7684\u5206\u5272\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4e24\u4e2a\u516c\u5171\u591a\u4e2d\u5fc3\u57fa\u51c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u7684\u4e34\u5e8a\u9886\u57df\u5b9e\u73b0\u4e86\u7a33\u5065\u548c\u53ef\u6cdb\u5316\u7684\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2507.23504", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23504", "abs": "https://arxiv.org/abs/2507.23504", "authors": ["Maurits Kaptein"], "title": "A Verifier Hierarchy", "comment": "This paper is primarily relevant to cs.CC, but submitted under cs.ML\n  due to lack of endorsement. The paper is under review at \"Information and\n  Communication\"", "summary": "We investigate the trade-off between certificate length and verifier runtime.\nWe prove a Verifier Trade-off Theorem showing that reducing the inherent\nverification time of a language from \\(f(n)\\) to \\(g(n)\\), where \\(f(n) \\ge\ng(n)\\), requires certificates of length at least \\(\\Omega(\\log(f(n) / g(n)))\\).\nThis theorem induces a natural hierarchy based on certificate complexity. We\ndemonstrate its applicability to analyzing conjectured separations between\ncomplexity classes (e.g., \\(\\np\\) and \\(\\exptime\\)) and to studying natural\nproblems such as string periodicity and rotation detection. Additionally, we\nprovide perspectives on the \\(\\p\\) vs. \\(\\np\\) problem by relating it to the\nexistence of sub-linear certificates.", "AI": {"tldr": "This paper studies the trade-off between certificate length and verifier runtime, and relates it to the P vs. NP problem.", "motivation": "The paper investigates the trade-off between certificate length and verifier runtime.", "method": "The paper proves a Verifier Trade-off Theorem.", "result": "Reducing the inherent verification time of a language from f(n) to g(n), where f(n) >= g(n), requires certificates of length at least Omega(log(f(n) / g(n))).", "conclusion": "This paper relates the P vs. NP problem to the existence of sub-linear certificates."}}
{"id": "2507.23194", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23194", "abs": "https://arxiv.org/abs/2507.23194", "authors": ["Jianghui Wang", "Vinay Joshi", "Saptarshi Majumder", "Xu Chao", "Bin Ding", "Ziqiong Liu", "Pratik Prabhanjan Brahma", "Dong Li", "Zicheng Liu", "Emad Barsoum"], "title": "Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks", "comment": null, "summary": "The demand for AI-generated GPU kernels is rapidly growing, influenced by the\nneed for scalable, hardware-optimized solutions in both industry and academia.\nAs deep learning workloads grow in complexity and diversity, it is imperative\nto automate low-level kernel development to meet performance and productivity\ndemands. Major cloud providers, semiconductor companies, and research\ninstitutions are now investing heavily in AI-driven code generation for GPUs,\naiming to reduce manual optimization efforts while achieving near-expert\nperformance on hardware like AMD MI300X. The Triton language, a Python-based\nDSL for GPU programming, has emerged as a popular target for such AI-generated\nkernels due to its balance of performance and ease-of-coding. In this work, we\npresent an evaluation suite for Triton-based GPU kernels and GEAK (Generating\nEfficient AI-centric GPU Kernels)-a framework that leverages cutting-edge LLMs\nto generate performant Triton code specifically for AMD GPUs, including the AMD\nMI300X and MI250. GEAK leverages inference-time compute scaling to produce\nTriton-based GPU kernels using a reasoning loop adapted from Reflexion-style\nfeedback mechanisms. On two evaluation benchmarks, GEAK significantly\noutperformed the baselines of directly prompting frontier LLMs as well as\nReflexion-based generation pipelines by achieving correctness up to $63$% and\nexecution speed up of up to $2.59$X. These results highlight the promise of\nGEAK-like agentic code generation for accelerating the adoption of diverse\nhardware platforms and democratizing access to expert-level kernel performance.", "AI": {"tldr": "GEAK \u662f\u4e00\u79cd\u5229\u7528 LLM \u4e3a AMD GPU \u751f\u6210\u9ad8\u6027\u80fd Triton \u4ee3\u7801\u7684\u6846\u67b6\uff0c\u4f18\u4e8e\u5176\u4ed6 LLM \u65b9\u6cd5\u3002", "motivation": "\u5bf9 AI \u751f\u6210\u7684 GPU \u5185\u6838\u7684\u9700\u6c42\u6b63\u5728\u5feb\u901f\u589e\u957f\uff0c\u8fd9\u53d7\u5230\u884c\u4e1a\u548c\u5b66\u672f\u754c\u5bf9\u53ef\u6269\u5c55\u7684\u786c\u4ef6\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u7684\u9700\u6c42\u7684\u5f71\u54cd\u3002\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u5728\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u65b9\u9762\u4e0d\u65ad\u589e\u957f\uff0c\u5fc5\u987b\u81ea\u52a8\u5316\u5e95\u5c42\u5185\u6838\u5f00\u53d1\uff0c\u4ee5\u6ee1\u8db3\u6027\u80fd\u548c\u751f\u4ea7\u529b\u9700\u6c42\u3002", "method": "GEAK \u6846\u67b6\u5229\u7528\u6700\u5148\u8fdb\u7684 LLM \u751f\u6210\u9002\u7528\u4e8e AMD GPU\uff08\u5305\u62ec AMD MI300X \u548c MI250\uff09\u7684\u9ad8\u6027\u80fd Triton \u4ee3\u7801\uff0c\u5e76\u5229\u7528\u63a8\u7406\u65f6\u8ba1\u7b97\u6269\u5c55\u6765\u4f7f\u7528\u6539\u7f16\u81ea Reflexion \u98ce\u683c\u53cd\u9988\u673a\u5236\u7684\u63a8\u7406\u5faa\u73af\u6765\u751f\u6210\u57fa\u4e8e Triton \u7684 GPU \u5185\u6838\u3002", "result": "GEAK \u5728\u4e24\u4e2a\u8bc4\u4f30\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u76f4\u63a5\u63d0\u793a\u524d\u6cbf LLM \u4ee5\u53ca\u57fa\u4e8e Reflexion \u7684\u751f\u6210\u7ba1\u9053\uff0c\u6b63\u786e\u7387\u9ad8\u8fbe $63$%\uff0c\u6267\u884c\u901f\u5ea6\u63d0\u9ad8\u4e86\u9ad8\u8fbe $2.59$ \u500d\u3002", "conclusion": "GEAK \u4ee3\u7406\u4ee3\u7801\u751f\u6210\u5728\u52a0\u901f\u5404\u79cd\u786c\u4ef6\u5e73\u53f0\u7684\u91c7\u7528\u548c\u666e\u53ca\u4e13\u5bb6\u7ea7\u5185\u6838\u6027\u80fd\u65b9\u9762\u5f88\u6709\u524d\u666f\uff0c\u5728\u4e24\u4e2a\u8bc4\u4f30\u57fa\u51c6\u4e0a\uff0cGEAK \u663e\u8457\u4f18\u4e8e\u76f4\u63a5\u63d0\u793a\u524d\u6cbf LLM \u4ee5\u53ca\u57fa\u4e8e Reflexion \u7684\u751f\u6210\u7ba1\u9053\uff0c\u6b63\u786e\u7387\u9ad8\u8fbe $63$%\uff0c\u6267\u884c\u901f\u5ea6\u63d0\u9ad8\u4e86\u9ad8\u8fbe $2.59$ \u500d\u3002"}}
{"id": "2507.23331", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23331", "abs": "https://arxiv.org/abs/2507.23331", "authors": ["Qiang Lu", "Waikit Xiu", "Xiying Li", "Shenyu Hu", "Shengbo Sun"], "title": "Contrastive Learning-Driven Traffic Sign Perception: Multi-Modal Fusion of Text and Vision", "comment": "11pages, 5 figures", "summary": "Traffic sign recognition, as a core component of autonomous driving\nperception systems, directly influences vehicle environmental awareness and\ndriving safety. Current technologies face two significant challenges: first,\nthe traffic sign dataset exhibits a pronounced long-tail distribution,\nresulting in a substantial decline in recognition performance of traditional\nconvolutional networks when processing low-frequency and out-of-distribution\nclasses; second, traffic signs in real-world scenarios are predominantly small\ntargets with significant scale variations, making it difficult to extract\nmulti-scale features.To overcome these issues, we propose a novel two-stage\nframework combining open-vocabulary detection and cross-modal learning. For\ntraffic sign detection, our NanoVerse YOLO model integrates a reparameterizable\nvision-language path aggregation network (RepVL-PAN) and an SPD-Conv module to\nspecifically enhance feature extraction for small, multi-scale targets. For\ntraffic sign classification, we designed a Traffic Sign Recognition Multimodal\nContrastive Learning model (TSR-MCL). By contrasting visual features from a\nVision Transformer with semantic features from a rule-based BERT, TSR-MCL\nlearns robust, frequency-independent representations, effectively mitigating\nclass confusion caused by data imbalance. On the TT100K dataset, our method\nachieves a state-of-the-art 78.4% mAP in the long-tail detection task for\nall-class recognition. The model also obtains 91.8% accuracy and 88.9% recall,\nsignificantly outperforming mainstream algorithms and demonstrating superior\naccuracy and generalization in complex, open-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\u4e2d\u7684\u957f\u5c3e\u5206\u5e03\u548c\u5c0f\u76ee\u6807\u95ee\u9898\uff0c\u5e76\u5728TT100K\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86SOTA\u7ed3\u679c\u3002", "motivation": "\u5f53\u524d\u6280\u672f\u5728\u5904\u7406\u4f4e\u9891\u548c\u5206\u5e03\u5916\u7c7b\u522b\u65f6\uff0c\u8bc6\u522b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff1b\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u4ea4\u901a\u6807\u5fd7\u4ee5\u5c0f\u76ee\u6807\u4e3a\u4e3b\uff0c\u4e14\u5c3a\u5ea6\u53d8\u5316\u5927\uff0c\u96be\u4ee5\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\u3002", "method": "\u7ed3\u5408\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u548c\u8de8\u6a21\u6001\u5b66\u4e60\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5305\u62ecNanoVerse YOLO\u6a21\u578b\u548cTSR-MCL\u6a21\u578b\u3002", "result": "NanoVerse YOLO\u6a21\u578b\u96c6\u6210\u4e86RepVL-PAN\u548cSPD-Conv\u6a21\u5757\uff0cTSR-MCL\u6a21\u578b\u901a\u8fc7\u5bf9\u6bd4\u89c6\u89c9\u7279\u5f81\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u5b66\u4e60\u9c81\u68d2\u7684\u3001\u9891\u7387\u72ec\u7acb\u7684\u8868\u793a\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u6570\u636e\u4e0d\u5e73\u8861\u5bfc\u81f4\u7684\u7c7b\u522b\u6df7\u6dc6\u3002\u6a21\u578b\u5728TT100K\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8678.4% mAP\uff0c\u540c\u65f6\u83b7\u5f97\u4e8691.8%\u7684\u51c6\u786e\u7387\u548c88.9%\u7684\u53ec\u56de\u7387\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728TT100K\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u768478.4% mAP\uff0c\u5e76\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.23512", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.23512", "abs": "https://arxiv.org/abs/2507.23512", "authors": ["Saleh Vatan Khah", "Savelii Chezhegov", "Shahrokh Farahmand", "Samuel Horv\u00e1th", "Eduard Gorbunov"], "title": "Differentially Private Clipped-SGD: High-Probability Convergence with Arbitrary Clipping Level", "comment": "60 pages", "summary": "Gradient clipping is a fundamental tool in Deep Learning, improving the\nhigh-probability convergence of stochastic first-order methods like SGD,\nAdaGrad, and Adam under heavy-tailed noise, which is common in training large\nlanguage models. It is also a crucial component of Differential Privacy (DP)\nmechanisms. However, existing high-probability convergence analyses typically\nrequire the clipping threshold to increase with the number of optimization\nsteps, which is incompatible with standard DP mechanisms like the Gaussian\nmechanism. In this work, we close this gap by providing the first\nhigh-probability convergence analysis for DP-Clipped-SGD with a fixed clipping\nlevel, applicable to both convex and non-convex smooth optimization under\nheavy-tailed noise, characterized by a bounded central $\\alpha$-th moment\nassumption, $\\alpha \\in (1,2]$. Our results show that, with a fixed clipping\nlevel, the method converges to a neighborhood of the optimal solution with a\nfaster rate than the existing ones. The neighborhood can be balanced against\nthe noise introduced by DP, providing a refined trade-off between convergence\nspeed and privacy guarantees.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DP-Clipped-SGD\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5728\u56fa\u5b9a\u88c1\u526a\u6c34\u5e73\u4e0b\u7684\u9ad8\u6982\u7387\u6536\u655b\u5206\u6790\uff0c\u9002\u7528\u4e8e\u91cd\u5c3e\u566a\u58f0\u4e0b\u7684\u51f8\u548c\u975e\u51f8\u4f18\u5316\uff0c\u5e76\u5728\u6536\u655b\u901f\u5ea6\u548c\u9690\u79c1\u4fdd\u8bc1\u4e4b\u95f4\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u7684\u9ad8\u6982\u7387\u6536\u655b\u5206\u6790\u901a\u5e38\u8981\u6c42\u88c1\u526a\u9608\u503c\u968f\u7740\u4f18\u5316\u6b65\u9aa4\u7684\u6570\u91cf\u800c\u589e\u52a0\uff0c\u8fd9\u4e0e\u6807\u51c6DP\u673a\u5236\uff08\u5982\u9ad8\u65af\u673a\u5236\uff09\u4e0d\u517c\u5bb9\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "DP-Clipped-SGD", "result": "\u5728\u91cd\u5c3e\u566a\u58f0\u4e0b\uff0c\u5f53\u4e2d\u5fc3\u03b1\u9636\u77e9\u6709\u754c\u65f6\uff0cDP-Clipped-SGD\u65b9\u6cd5\u5728\u56fa\u5b9a\u88c1\u526a\u6c34\u5e73\u4e0b\uff0c\u4ee5\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5feb\u7684\u901f\u5ea6\u6536\u655b\u5230\u6700\u4f18\u89e3\u7684\u90bb\u57df\u3002", "conclusion": "DP-Clipped-SGD\u65b9\u6cd5\u5728\u56fa\u5b9a\u88c1\u526a\u6c34\u5e73\u4e0b\uff0c\u5bf9\u4e8e\u51f8\u548c\u975e\u51f8\u5e73\u6ed1\u4f18\u5316\u95ee\u9898\uff0c\u4ee5\u66f4\u5feb\u7684\u901f\u5ea6\u6536\u655b\u5230\u6700\u4f18\u89e3\u7684\u90bb\u57df\uff0c\u5e76\u5728\u6536\u655b\u901f\u5ea6\u548c\u9690\u79c1\u4fdd\u8bc1\u4e4b\u95f4\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2507.23211", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23211", "abs": "https://arxiv.org/abs/2507.23211", "authors": ["Yunhao Liang", "Ruixuan Ying", "Takuya Taniguchi", "Zhe Cui"], "title": "Failures Are the Stepping Stones to Success: Enhancing Few-Shot In-Context Learning by Leveraging Negative Samples", "comment": null, "summary": "Large Language Models exhibit powerful few-shot in-context learning (ICL)\ncapabilities, but the performance is highly sensitive to provided examples.\n  Recent research has focused on retrieving corresponding examples for each\ninput query, not only enhancing the efficiency and scalability of the learning\nprocess but also mitigating inherent biases in manual example selection.\n  However, these studies have primarily emphasized leveraging Positive samples\nwhile overlooking the additional information within Negative samples for\ncontextual learning.\n  We propose a novel method that utilizes Negative samples to better select\nPositive sample examples, thereby enhancing the performance of few-shot ICL.\nInitially, we construct Positive and Negative sample corpora based on\nZero-Shot-Cot. Then, during inference, we employ a semantic similarity-based\napproach to select the most similar examples from both the Positive and\nNegative corpora for a given query. Subsequently, we further retrieve Positive\nexamples from the Positive sample corpus based on semantic similarity to the\nNegative examples, then concatenating them with the previously selected\nPositive examples to serve as ICL demonstrations. Experimental results\ndemonstrate that our approach surpasses methods solely relying on the most\nsimilar positive examples for context, validating that the additional\ninformation in negative samples aids in enhancing ICL performance through\nimproved Positive sample selection.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u8d1f\u6837\u672c\u6765\u66f4\u597d\u5730\u9009\u62e9\u6b63\u6837\u672c\u793a\u4f8b\uff0c\u4ece\u800c\u63d0\u9ad8\u5c11\u6837\u672c ICL \u7684\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u5c11\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60 (ICL) \u80fd\u529b\uff0c\u4f46\u6027\u80fd\u5bf9\u63d0\u4f9b\u7684\u793a\u4f8b\u9ad8\u5ea6\u654f\u611f\u3002\u6700\u8fd1\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u68c0\u7d22\u6bcf\u4e2a\u8f93\u5165\u67e5\u8be2\u7684\u76f8\u5e94\u793a\u4f8b\uff0c\u8fd9\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5b66\u4e60\u8fc7\u7a0b\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u800c\u4e14\u51cf\u8f7b\u4e86\u624b\u52a8\u793a\u4f8b\u9009\u62e9\u4e2d\u56fa\u6709\u7684\u504f\u5dee\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u7814\u7a76\u4e3b\u8981\u5f3a\u8c03\u5229\u7528\u6b63\u6837\u672c\uff0c\u800c\u5ffd\u7565\u4e86\u8d1f\u6837\u672c\u4e2d\u7528\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u989d\u5916\u4fe1\u606f\u3002", "method": "\u5229\u7528\u8d1f\u6837\u672c\u6765\u66f4\u597d\u5730\u9009\u62e9\u6b63\u6837\u672c\u4f8b\u5b50\uff0c\u4ece\u800c\u63d0\u9ad8\u5c11\u6837\u672c ICL \u7684\u6027\u80fd\u3002\u9996\u5148\uff0c\u6211\u4eec\u57fa\u4e8e Zero-Shot-Cot \u6784\u5efa\u6b63\u6837\u672c\u548c\u8d1f\u6837\u672c\u8bed\u6599\u5e93\u3002\u7136\u540e\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u91c7\u7528\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u7684\u65b9\u6cd5\uff0c\u4ece\u6b63\u6837\u672c\u548c\u8d1f\u6837\u672c\u8bed\u6599\u5e93\u4e2d\u9009\u62e9\u4e0e\u7ed9\u5b9a\u67e5\u8be2\u6700\u76f8\u4f3c\u7684\u4f8b\u5b50\u3002\u968f\u540e\uff0c\u6211\u4eec\u57fa\u4e8e\u4e0e\u8d1f\u6837\u672c\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\uff0c\u4ece\u6b63\u6837\u672c\u8bed\u6599\u5e93\u4e2d\u8fdb\u4e00\u6b65\u68c0\u7d22\u6b63\u6837\u672c\uff0c\u7136\u540e\u5c06\u5b83\u4eec\u4e0e\u5148\u524d\u9009\u62e9\u7684\u6b63\u6837\u672c\u8fde\u63a5\u8d77\u6765\uff0c\u4f5c\u4e3a ICL \u6f14\u793a\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u8d85\u8fc7\u4e86\u4ec5\u4f9d\u8d56\u4e8e\u6700\u76f8\u4f3c\u7684\u6b63\u6837\u672c\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u8d1f\u6837\u672c\u4e2d\u7684\u989d\u5916\u4fe1\u606f\u6709\u52a9\u4e8e\u901a\u8fc7\u6539\u8fdb\u6b63\u6837\u672c\u9009\u62e9\u6765\u63d0\u9ad8 ICL \u6027\u80fd\u3002", "conclusion": "\u5229\u7528\u8d1f\u6837\u672c\u6765\u66f4\u597d\u5730\u9009\u62e9\u6b63\u6837\u672c\u4f8b\u5b50\uff0c\u4ece\u800c\u63d0\u9ad8\u5c11\u6837\u672c ICL \u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u8d85\u8fc7\u4e86\u4ec5\u4f9d\u8d56\u4e8e\u6700\u76f8\u4f3c\u7684\u6b63\u6837\u672c\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u8d1f\u6837\u672c\u4e2d\u7684\u989d\u5916\u4fe1\u606f\u6709\u52a9\u4e8e\u901a\u8fc7\u6539\u8fdb\u6b63\u6837\u672c\u9009\u62e9\u6765\u63d0\u9ad8 ICL \u6027\u80fd\u3002"}}
{"id": "2507.23340", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23340", "abs": "https://arxiv.org/abs/2507.23340", "authors": ["Xingyue Peng", "Yuandong Lyu", "Lang Zhang", "Jian Zhu", "Songtao Wang", "Jiaxin Deng", "Songxin Lu", "Weiliang Ma", "Dangen She", "Peng Jia", "XianPeng Lang"], "title": "MagicRoad: Semantic-Aware 3D Road Surface Reconstruction via Obstacle Inpainting", "comment": null, "summary": "Road surface reconstruction is essential for autonomous driving, supporting\ncentimeter-accurate lane perception and high-definition mapping in complex\nurban environments.While recent methods based on mesh rendering or 3D Gaussian\nsplatting (3DGS) achieve promising results under clean and static conditions,\nthey remain vulnerable to occlusions from dynamic agents, visual clutter from\nstatic obstacles, and appearance degradation caused by lighting and weather\nchanges. We present a robust reconstruction framework that integrates\nocclusion-aware 2D Gaussian surfels with semantic-guided color enhancement to\nrecover clean, consistent road surfaces. Our method leverages a planar-adapted\nGaussian representation for efficient large-scale modeling, employs\nsegmentation-guided video inpainting to remove both dynamic and static\nforeground objects, and enhances color coherence via semantic-aware correction\nin HSV space. Extensive experiments on urban-scale datasets demonstrate that\nour framework produces visually coherent and geometrically faithful\nreconstructions, significantly outperforming prior methods under real-world\nconditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7a33\u5065\u7684\u91cd\u5efa\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u906e\u6321\u611f\u77e5\u7684 2D \u9ad8\u65af surfel \u4e0e\u8bed\u4e49\u5f15\u5bfc\u7684\u989c\u8272\u589e\u5f3a\uff0c\u4ee5\u6062\u590d\u5e72\u51c0\u3001\u4e00\u81f4\u7684\u9053\u8def\u8868\u9762\u3002", "motivation": "\u9053\u8def\u8868\u9762\u91cd\u5efa\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u81f3\u5173\u91cd\u8981\uff0c\u53ef\u5728\u590d\u6742\u7684\u57ce\u5e02\u73af\u5883\u4e2d\u652f\u6301\u5398\u7c73\u7ea7\u7cbe\u786e\u7684 lane \u611f\u77e5\u548c\u9ad8\u6e05\u6670\u5ea6\u5730\u56fe\u7ed8\u5236\u3002\u867d\u7136\u6700\u8fd1\u57fa\u4e8e\u7f51\u683c\u6e32\u67d3\u6216 3D \u9ad8\u65af\u6e85\u5c04 (3DGS) \u7684\u65b9\u6cd5\u5728\u5e72\u51c0\u548c\u9759\u6001\u7684\u6761\u4ef6\u4e0b\u53d6\u5f97\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u52a8\u6001\u4ee3\u7406\u7684\u906e\u6321\u3001\u9759\u6001\u969c\u788d\u7269\u7684\u89c6\u89c9\u6df7\u4e71\u4ee5\u53ca\u5149\u7167\u548c\u5929\u6c14\u53d8\u5316\u5f15\u8d77\u7684\u5916\u89c2\u9000\u5316\u7684\u5f71\u54cd\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u5e73\u9762\u9002\u5e94\u9ad8\u65af\u8868\u793a\u8fdb\u884c\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u5efa\u6a21\uff0c\u91c7\u7528\u5206\u5272\u5f15\u5bfc\u7684\u89c6\u9891\u4fee\u590d\u6765\u79fb\u9664\u52a8\u6001\u548c\u9759\u6001\u524d\u666f\u5bf9\u8c61\uff0c\u5e76\u901a\u8fc7 HSV \u7a7a\u95f4\u4e2d\u8bed\u4e49\u611f\u77e5\u7684\u6821\u6b63\u6765\u589e\u5f3a\u989c\u8272\u4e00\u81f4\u6027\u3002", "result": "\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u906e\u6321\u611f\u77e5\u7684 2D \u9ad8\u65af surfel \u4e0e\u8bed\u4e49\u5f15\u5bfc\u7684\u989c\u8272\u589e\u5f3a\uff0c\u4ee5\u6062\u590d\u5e72\u51c0\u3001\u4e00\u81f4\u7684\u9053\u8def\u8868\u9762\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u57ce\u5e02\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u4ea7\u751f\u4e86\u89c6\u89c9\u8fde\u8d2f\u4e14\u51e0\u4f55\u4e0a\u5fe0\u5b9e\u7684\u91cd\u5efa\uff0c\u660e\u663e\u4f18\u4e8e\u73b0\u5b9e\u6761\u4ef6\u4e0b\u7684\u5148\u524d\u65b9\u6cd5\u3002"}}
{"id": "2507.23534", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23534", "abs": "https://arxiv.org/abs/2507.23534", "authors": ["Chih-Fan Hsu", "Ming-Ching Chang", "Wei-Chao Chen"], "title": "Continual Learning with Synthetic Boundary Experience Blending", "comment": null, "summary": "Continual learning (CL) aims to address catastrophic forgetting in models\ntrained sequentially on multiple tasks. While experience replay has shown\npromise, its effectiveness is often limited by the sparse distribution of\nstored key samples, leading to overly simplified decision boundaries. We\nhypothesize that introducing synthetic data near the decision boundary\n(Synthetic Boundary Data, or SBD) during training serves as an implicit\nregularizer, improving boundary stability and mitigating forgetting. To\nvalidate this hypothesis, we propose a novel training framework, {\\bf\nExperience Blending}, which integrates knowledge from both stored key samples\nand synthetic, boundary-adjacent data. Experience blending consists of two core\ncomponents: (1) a multivariate Differential Privacy (DP) noise mechanism that\ninjects batch-wise noise into low-dimensional feature representations,\ngenerating SBD; and (2) an end-to-end training strategy that jointly leverages\nboth stored key samples and SBD. Extensive experiments on CIFAR-10, CIFAR-100,\nand Tiny ImageNet demonstrate that our method outperforms nine CL baselines,\nachieving accuracy improvements of 10%, 6%, and 13%, respectively.", "AI": {"tldr": "This paper introduces Experience Blending, a continual learning framework that uses synthetic boundary data and stored key samples to improve accuracy and mitigate forgetting, outperforming existing methods on image classification tasks.", "motivation": "The effectiveness of experience replay in continual learning is often limited by the sparse distribution of stored key samples, leading to overly simplified decision boundaries. Introducing synthetic data near the decision boundary during training serves as an implicit regularizer, improving boundary stability and mitigating forgetting.", "method": "The paper proposes a novel training framework, Experience Blending, which integrates knowledge from both stored key samples and synthetic, boundary-adjacent data. It consists of a multivariate Differential Privacy (DP) noise mechanism that injects batch-wise noise into low-dimensional feature representations, generating SBD, and an end-to-end training strategy that jointly leverages both stored key samples and SBD.", "result": "The Experience Blending method achieves accuracy improvements of 10%, 6%, and 13% on CIFAR-10, CIFAR-100, and Tiny ImageNet, respectively, compared to nine CL baselines.", "conclusion": "The proposed Experience Blending method outperforms nine CL baselines on CIFAR-10, CIFAR-100, and Tiny ImageNet, achieving accuracy improvements of 10%, 6%, and 13%, respectively."}}
{"id": "2507.23220", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23220", "abs": "https://arxiv.org/abs/2507.23220", "authors": ["Carolina Zheng", "Nicolas Beltran-Velez", "Sweta Karlekar", "Claudia Shi", "Achille Nazaret", "Asif Mallik", "Amir Feder", "David M. Blei"], "title": "Model Directions, Not Words: Mechanistic Topic Models Using Sparse Autoencoders", "comment": null, "summary": "Traditional topic models are effective at uncovering latent themes in large\ntext collections. However, due to their reliance on bag-of-words\nrepresentations, they struggle to capture semantically abstract features. While\nsome neural variants use richer representations, they are similarly constrained\nby expressing topics as word lists, which limits their ability to articulate\ncomplex topics. We introduce Mechanistic Topic Models (MTMs), a class of topic\nmodels that operate on interpretable features learned by sparse autoencoders\n(SAEs). By defining topics over this semantically rich space, MTMs can reveal\ndeeper conceptual themes with expressive feature descriptions. Moreover,\nuniquely among topic models, MTMs enable controllable text generation using\ntopic-based steering vectors. To properly evaluate MTM topics against\nword-list-based approaches, we propose \\textit{topic judge}, an LLM-based\npairwise comparison evaluation framework. Across five datasets, MTMs match or\nexceed traditional and neural baselines on coherence metrics, are consistently\npreferred by topic judge, and enable effective steering of LLM outputs.", "AI": {"tldr": "MTMs use interpretable features learned by sparse autoencoders to reveal deeper conceptual themes and enable controllable text generation.", "motivation": "Traditional topic models struggle to capture semantically abstract features. Neural variants are similarly constrained by expressing topics as word lists, which limits their ability to articulate complex topics.", "method": "Mechanistic Topic Models (MTMs), a class of topic models that operate on interpretable features learned by sparse autoencoders (SAEs).", "result": "MTMs match or exceed traditional and neural baselines on coherence metrics, are consistently preferred by topic judge, and enable effective steering of LLM outputs.", "conclusion": "Mechanistic Topic Models (MTMs) can reveal deeper conceptual themes with expressive feature descriptions. MTMs match or exceed traditional and neural baselines on coherence metrics, are consistently preferred by topic judge, and enable effective steering of LLM outputs."}}
{"id": "2507.23341", "categories": ["cs.CV", "68T45, 68T07", "I.4.8; I.4.9; I.5.4"], "pdf": "https://arxiv.org/pdf/2507.23341", "abs": "https://arxiv.org/abs/2507.23341", "authors": ["Ahmet Can \u00d6merciko\u011flu", "Mustafa Mansur Y\u00f6n\u00fcg\u00fcl", "Pakize Erdo\u011fmu\u015f"], "title": "The Impact of Image Resolution on Face Detection: A Comparative Analysis of MTCNN, YOLOv XI and YOLOv XII models", "comment": "6 pages, 5 figures, 4 tables", "summary": "Face detection is a crucial component in many AI-driven applications such as\nsurveillance, biometric authentication, and human-computer interaction.\nHowever, real-world conditions like low-resolution imagery present significant\nchallenges that degrade detection performance. In this study, we systematically\ninvestigate the impact of input resolution on the accuracy and robustness of\nthree prominent deep learning-based face detectors: YOLOv11, YOLOv12, and\nMTCNN. Using the WIDER FACE dataset, we conduct extensive evaluations across\nmultiple image resolutions (160x160, 320x320, and 640x640) and assess each\nmodel's performance using metrics such as precision, recall, mAP50, mAP50-95,\nand inference time. Results indicate that YOLOv11 outperforms YOLOv12 and MTCNN\nin terms of detection accuracy, especially at higher resolutions, while YOLOv12\nexhibits slightly better recall. MTCNN, although competitive in landmark\nlocalization, lags in real-time inference speed. Our findings provide\nactionable insights for selecting resolution-aware face detection models\nsuitable for varying operational constraints.", "AI": {"tldr": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u8f93\u5165\u5206\u8fa8\u7387\u5bf9\u4e09\u6b3e\u4e3b\u6d41\u6df1\u5ea6\u5b66\u4e60\u4eba\u8138\u68c0\u6d4b\u5668\uff08YOLOv11\u3001YOLOv12\u548cMTCNN\uff09\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u4eba\u8138\u68c0\u6d4b\u662f\u8bb8\u591a\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u5e94\u7528\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u5982\u76d1\u63a7\u3001\u751f\u7269\u7279\u5f81\u8ba4\u8bc1\u548c\u4eba\u673a\u4ea4\u4e92\u3002\u7136\u800c\uff0c\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u7b49\u73b0\u5b9e\u6761\u4ef6\u5e26\u6765\u4e86\u663e\u8457\u7684\u6311\u6218\uff0c\u964d\u4f4e\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u4f7f\u7528WIDER FACE\u6570\u636e\u96c6\uff0c\u6211\u4eec\u5bf9\u591a\u4e2a\u56fe\u50cf\u5206\u8fa8\u7387\uff08160x160\u3001320x320\u548c640x640\uff09\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u8bc4\u4f30\uff0c\u5e76\u4f7f\u7528\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001mAP50\u3001mAP50-95\u548c\u63a8\u7406\u65f6\u95f4\u7b49\u6307\u6807\u8bc4\u4f30\u4e86\u6bcf\u4e2a\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cYOLOv11\u5728\u68c0\u6d4b\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8eYOLOv12\u548cMTCNN\uff0c\u5c24\u5176\u662f\u5728\u8f83\u9ad8\u5206\u8fa8\u7387\u4e0b\uff0c\u800cYOLOv12\u7684\u53ec\u56de\u7387\u7565\u597d\u3002MTCNN\u867d\u7136\u5728landmark localization\u65b9\u9762\u5177\u6709\u7ade\u4e89\u529b\uff0c\u4f46\u5728\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u6ede\u540e\u3002", "conclusion": "YOLOv11\u5728\u68c0\u6d4b\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8eYOLOv12\u548cMTCNN\uff0c\u5c24\u5176\u662f\u5728\u8f83\u9ad8\u5206\u8fa8\u7387\u4e0b\uff0c\u800cYOLOv12\u7684\u53ec\u56de\u7387\u7565\u597d\u3002MTCNN\u867d\u7136\u5728landmark localization\u65b9\u9762\u5177\u6709\u7ade\u4e89\u529b\uff0c\u4f46\u5728\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u6ede\u540e\u3002\u6211\u4eec\u7684\u53d1\u73b0\u4e3a\u9009\u62e9\u9002\u7528\u4e8e\u4e0d\u540c\u64cd\u4f5c\u7ea6\u675f\u7684\u3001\u5177\u6709\u5206\u8fa8\u7387\u611f\u77e5\u7684\u4eba\u8138\u68c0\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.23535", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.23535", "abs": "https://arxiv.org/abs/2507.23535", "authors": ["Dhanesh Ramachandram", "Himanshu Joshi", "Judy Zhu", "Dhari Gandhi", "Lucas Hartman", "Ananya Raval"], "title": "Transparent AI: The Case for Interpretability and Explainability", "comment": null, "summary": "As artificial intelligence systems increasingly inform high-stakes decisions\nacross sectors, transparency has become foundational to responsible and\ntrustworthy AI implementation. Leveraging our role as a leading institute in\nadvancing AI research and enabling industry adoption, we present key insights\nand lessons learned from practical interpretability applications across diverse\ndomains. This paper offers actionable strategies and implementation guidance\ntailored to organizations at varying stages of AI maturity, emphasizing the\nintegration of interpretability as a core design principle rather than a\nretrospective add-on.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5728\u5404\u9886\u57df\u5e94\u7528\u53ef\u89e3\u91ca\u6027AI\u7684\u5b9e\u8df5\u7ecf\u9a8c\uff0c\u4e3a\u4e0d\u540c\u9636\u6bb5\u7684\u7ec4\u7ec7\u63d0\u4f9b\u5b9e\u65bd\u6307\u5bfc\uff0c\u5e76\u5f3a\u8c03\u53ef\u89e3\u91ca\u6027\u5e94\u4f5c\u4e3a\u6838\u5fc3\u8bbe\u8ba1\u539f\u5219\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u5f71\u54cd\u9ad8\u98ce\u9669\u51b3\u7b56\uff0c\u900f\u660e\u5ea6\u5df2\u6210\u4e3a\u8d1f\u8d23\u4efb\u548c\u503c\u5f97\u4fe1\u8d56\u7684AI\u5b9e\u65bd\u7684\u57fa\u7840\u3002", "method": "\u4ece\u5404\u4e2a\u9886\u57df\u7684\u53ef\u89e3\u91ca\u6027\u5e94\u7528\u4e2d\u603b\u7ed3\u7ecf\u9a8c\u3002", "result": "\u4e3a\u5904\u4e8e\u4e0d\u540cAI\u6210\u719f\u9636\u6bb5\u7684\u7ec4\u7ec7\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u7b56\u7565\u548c\u5b9e\u65bd\u6307\u5bfc\u3002", "conclusion": "\u5f3a\u8c03\u5c06\u53ef\u89e3\u91ca\u6027\u6574\u5408\u4e3a\u6838\u5fc3\u8bbe\u8ba1\u539f\u5219\uff0c\u800c\u975e\u8ffd\u6eaf\u6027\u7684\u9644\u52a0\u7ec4\u4ef6\u3002"}}
{"id": "2507.23227", "categories": ["cs.CL", "cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.23227", "abs": "https://arxiv.org/abs/2507.23227", "authors": ["Sophie Kearney", "Shu Yang", "Zixuan Wen", "Bojian Hou", "Duy Duong-Tran", "Tianlong Chen", "Jason Moore", "Marylyn Ritchie", "Li Shen"], "title": "Enabling Few-Shot Alzheimer's Disease Diagnosis on Tabular Biomarker Data with LLMs", "comment": null, "summary": "Early and accurate diagnosis of Alzheimer's disease (AD), a complex\nneurodegenerative disorder, requires analysis of heterogeneous biomarkers\n(e.g., neuroimaging, genetic risk factors, cognitive tests, and cerebrospinal\nfluid proteins) typically represented in a tabular format. With flexible\nfew-shot reasoning, multimodal integration, and natural-language-based\ninterpretability, large language models (LLMs) offer unprecedented\nopportunities for prediction with structured biomedical data. We propose a\nnovel framework called TAP-GPT, Tabular Alzheimer's Prediction GPT, that adapts\nTableGPT2, a multimodal tabular-specialized LLM originally developed for\nbusiness intelligence tasks, for AD diagnosis using structured biomarker data\nwith small sample sizes. Our approach constructs few-shot tabular prompts using\nin-context learning examples from structured biomedical data and finetunes\nTableGPT2 using the parameter-efficient qLoRA adaption for a clinical binary\nclassification task of AD or cognitively normal (CN). The TAP-GPT framework\nharnesses the powerful tabular understanding ability of TableGPT2 and the\nencoded prior knowledge of LLMs to outperform more advanced general-purpose\nLLMs and a tabular foundation model (TFM) developed for prediction tasks. To\nour knowledge, this is the first application of LLMs to the prediction task\nusing tabular biomarker data, paving the way for future LLM-driven multi-agent\nframeworks in biomedical informatics.", "AI": {"tldr": "TAP-GPT, a modified TableGPT2, predicts Alzheimer's disease from tabular biomarker data better than other LLMs.", "motivation": "Early and accurate diagnosis of Alzheimer's disease (AD) requires analysis of heterogeneous biomarkers typically represented in a tabular format. Large language models (LLMs) offer unprecedented opportunities for prediction with structured biomedical data.", "method": "The authors construct few-shot tabular prompts using in-context learning examples and finetune TableGPT2 using the parameter-efficient qLoRA adaption for a clinical binary classification task of AD or cognitively normal (CN).", "result": "TAP-GPT outperforms more advanced general-purpose LLMs and a tabular foundation model (TFM) developed for prediction tasks.", "conclusion": "This paper introduces TAP-GPT, a novel framework that adapts TableGPT2 for Alzheimer's disease diagnosis using structured biomarker data. It demonstrates that TAP-GPT outperforms other LLMs and a tabular foundation model in predicting AD from tabular data."}}
{"id": "2507.23343", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.23343", "abs": "https://arxiv.org/abs/2507.23343", "authors": ["Yingjie Zhou", "Jiezhang Cao", "Zicheng Zhang", "Farong Wen", "Yanwei Jiang", "Jun Jia", "Xiaohong Liu", "Xiongkuo Min", "Guangtao Zhai"], "title": "Who is a Better Talker: Subjective and Objective Quality Assessment for AI-Generated Talking Heads", "comment": null, "summary": "Speech-driven methods for portraits are figuratively known as \"Talkers\"\nbecause of their capability to synthesize speaking mouth shapes and facial\nmovements. Especially with the rapid development of the Text-to-Image (T2I)\nmodels, AI-Generated Talking Heads (AGTHs) have gradually become an emerging\ndigital human media. However, challenges persist regarding the quality of these\ntalkers and AGTHs they generate, and comprehensive studies addressing these\nissues remain limited. To address this gap, this paper presents the largest\nAGTH quality assessment dataset THQA-10K to date, which selects 12 prominent\nT2I models and 14 advanced talkers to generate AGTHs for 14 prompts. After\nexcluding instances where AGTH generation is unsuccessful, the THQA-10K dataset\ncontains 10,457 AGTHs. Then, volunteers are recruited to subjectively rate the\nAGTHs and give the corresponding distortion categories. In our analysis for\nsubjective experimental results, we evaluate the performance of talkers in\nterms of generalizability and quality, and also expose the distortions of\nexisting AGTHs. Finally, an objective quality assessment method based on the\nfirst frame, Y-T slice and tone-lip consistency is proposed. Experimental\nresults show that this method can achieve state-of-the-art (SOTA) performance\nin AGTH quality assessment. The work is released at\nhttps://github.com/zyj-2000/Talker.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u578bAGTH\u8d28\u91cf\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5ba2\u89c2\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728AGTH\u8d28\u91cf\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u8bf4\u8bdd\u8005\u548c\u4ed6\u4eec\u751f\u6210\u7684AGTH\u7684\u8d28\u91cf\u4ecd\u7136\u5b58\u5728\u6311\u6218\uff0c\u5e76\u4e14\u9488\u5bf9\u8fd9\u4e9b\u95ee\u9898\u7684\u7efc\u5408\u7814\u7a76\u4ecd\u7136\u6709\u9650\u3002", "method": "\u901a\u8fc7\u9009\u62e912\u4e2a\u7a81\u51fa\u7684T2I\u6a21\u578b\u548c14\u4e2a\u5148\u8fdb\u7684talkers\u6765\u751f\u6210AGTHs\uff0c\u5e76\u62db\u52df\u5fd7\u613f\u8005\u5bf9AGTHs\u8fdb\u884c\u4e3b\u89c2\u8bc4\u7ea7\uff0c\u7ed9\u51fa\u76f8\u5e94\u7684\u5931\u771f\u7c7b\u522b\u3002", "result": "\u6784\u5efa\u4e86\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684AGTH\u8d28\u91cf\u8bc4\u4f30\u6570\u636e\u96c6THQA-10K\uff0c\u5176\u4e2d\u5305\u542b10,457\u4e2aAGTHs\u3002\u8bc4\u4f30\u4e86talkers\u5728\u6cdb\u5316\u6027\u548c\u8d28\u91cf\u65b9\u9762\u7684\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709AGTH\u7684\u5931\u771f\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9996\u5e27\u3001Y-T\u5207\u7247\u548c\u97f3\u8c03-\u5634\u5507\u4e00\u81f4\u6027\u7684\u5ba2\u89c2\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728AGTH\u8d28\u91cf\u8bc4\u4f30\u4e2d\u53ef\u4ee5\u8fbe\u5230\u6700\u5148\u8fdb\u7684(SOTA)\u6027\u80fd\u3002"}}
{"id": "2507.23536", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23536", "abs": "https://arxiv.org/abs/2507.23536", "authors": ["Georg Slamanig", "Francesco Corti", "Olga Saukh"], "title": "From LLMs to Edge: Parameter-Efficient Fine-Tuning on Edge Devices", "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) methods reduce the computational costs\nof updating deep learning models by minimizing the number of additional\nparameters used to adapt a model to a down- stream task. While extensively\nresearched in large language models (LLMs), their application to smaller models\nused on edge devices, such as convolutional neural networks, remains\nunderexplored. This paper benchmarks and analyzes popular PEFT methods on\nconvolutional architectures typically deployed in resource-constrained edge\nenvironments. We evaluate LoRA, DoRA, and GaLore for updating standard and\ndepthwise convolutional architectures to handle distribution shifts and\naccommodate unseen classes. We utilize recently proposed PyTorch profilers to\ncompare the updated model performance and computational costs of these PEFT\nmethods with traditional fine-tuning approaches. With resource efficiency in\nmind, we investigate their update behavior across different rank dimensions. We\nfind that the evaluated PEFT methods are only half as memory-efficient when\napplied to depthwise-separable convolution architectures, compared to their\nefficiency with LLMs. Conversely, when targeting convolu- tional architectures\noptimized for edge deployment, adapter-based PEFT methods can reduce floating\npoint operations (FLOPs) during model updates by up to 95%. These insights\noffer valuable guidance for selecting PEFT methods based on hardware\nconstraints, performance requirements, and application needs. Our code is\nonline.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5e94\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03 (PEFT) \u65b9\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u548c\u5206\u6790\u3002", "motivation": "\u53c2\u6570\u9ad8\u6548\u5fae\u8c03 (PEFT) \u65b9\u6cd5\u901a\u8fc7\u6700\u5c0f\u5316\u7528\u4e8e\u5c06\u6a21\u578b\u9002\u5e94\u4e0b\u6e38\u4efb\u52a1\u7684\u989d\u5916\u53c2\u6570\u6570\u91cf\u6765\u964d\u4f4e\u66f4\u65b0\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c\u3002\u867d\u7136\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5b83\u4eec\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u4f7f\u7528\u7684\u5c0f\u578b\u6a21\u578b\uff08\u5982\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff09\u4e2d\u7684\u5e94\u7528\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5bf9\u6807\u51c6\u548c\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u67b6\u6784\u5e94\u7528 LoRA\u3001DoRA \u548c GaLore\uff0c\u4ee5\u5904\u7406\u5206\u5e03\u504f\u79fb\u548c\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u7c7b\u522b\u3002\u5229\u7528\u6700\u8fd1\u63d0\u51fa\u7684 PyTorch \u5206\u6790\u5668\u6765\u6bd4\u8f83\u66f4\u65b0\u6a21\u578b\u7684\u6027\u80fd\u548c\u8fd9\u4e9b PEFT \u65b9\u6cd5\u4e0e\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u7684\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5f53\u5e94\u7528\u4e8e\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u67b6\u6784\u65f6\uff0c\u6240\u8bc4\u4f30\u7684 PEFT \u65b9\u6cd5\u7684\u5185\u5b58\u6548\u7387\u4ec5\u4e3a LLM \u7684\u4e00\u534a\u3002\u5f53\u4ee5\u9488\u5bf9\u8fb9\u7f18\u90e8\u7f72\u4f18\u5316\u7684\u5377\u79ef\u67b6\u6784\u4e3a\u76ee\u6807\u65f6\uff0c\u57fa\u4e8e\u9002\u914d\u5668\u7684 PEFT \u65b9\u6cd5\u53ef\u4ee5\u5c06\u6a21\u578b\u66f4\u65b0\u671f\u95f4\u7684\u6d6e\u70b9\u8fd0\u7b97 (FLOP) \u51cf\u5c11\u9ad8\u8fbe 95%\u3002", "conclusion": "\u5728\u9762\u5411\u8fb9\u7f18\u90e8\u7f72\u4f18\u5316\u7684\u5377\u79ef\u67b6\u6784\u4e0a\uff0c\u57fa\u4e8e\u9002\u914d\u5668\u7684 PEFT \u65b9\u6cd5\u53ef\u4ee5\u5c06\u6a21\u578b\u66f4\u65b0\u671f\u95f4\u7684\u6d6e\u70b9\u8fd0\u7b97 (FLOP) \u51cf\u5c11\u9ad8\u8fbe 95%\u3002"}}
{"id": "2507.23247", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23247", "abs": "https://arxiv.org/abs/2507.23247", "authors": ["Sneha Oram", "Pushpak Bhattacharyya"], "title": "P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication", "comment": null, "summary": "There has been an increase in recent advancements in the explainability and\ndevelopment of personalized chatbots for mental health. However, the reasoning\naspects for explainability and dialogue discourse have not been explored\npreviously for mental health. Hence, we are investigating the pragmatic\nreasoning capability of large language models (LLMs) in this domain. We\nintroduce P-ReMe dataset, and propose a modified definition for the pragmatic\nphenomena of implicature (implied meaning) and presupposition (implicit\nassumption) in mental health. Following the definition, we formulate two tasks\nin implicature and one task in presupposition. To benchmark the dataset and the\npresented tasks, we consider four models - Llama3.1, Mistral, MentaLLaMa, and\nQwen. The results of the experiments suggest that Mistral and Qwen show\nsubstantial reasoning capabilities in the domain. In addition, we also propose\nStiPRompts to study the stigma around mental health with the state-of-the-art\nLLMs, GPT-4o mini, Deepseek-chat, and Claude-3.5-haiku. Our evaluated findings\nshow that Claude-3.5-haiku deals with the stigma more responsibly compared to\nthe other two LLMs.", "AI": {"tldr": "This paper investigates the pragmatic reasoning capability of large language models (LLMs) in mental health, introduces P-ReMe dataset, and proposes StiPRompts to study the stigma around mental health.", "motivation": "Reasoning aspects for explainability and dialogue discourse have not been explored previously for mental health.", "method": "introducing P-ReMe dataset, proposing a modified definition for the pragmatic phenomena of implicature and presupposition in mental health, formulating two tasks in implicature and one task in presupposition, and proposing StiPRompts", "result": "Mistral and Qwen show substantial reasoning capabilities in the domain. Claude-3.5-haiku deals with the stigma more responsibly compared to the other two LLMs.", "conclusion": "Mistral and Qwen show substantial reasoning capabilities in the mental health domain. Claude-3.5-haiku deals with the stigma more responsibly compared to GPT-4o mini, and Deepseek-chat."}}
{"id": "2507.23357", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23357", "abs": "https://arxiv.org/abs/2507.23357", "authors": ["Radu-Andrei Bourceanu", "Neil De La Fuente", "Jan Grimm", "Andrei Jardan", "Andriy Manucharyan", "Cornelius Weiss", "Roman Pflugfelder"], "title": "IN45023 Neural Network Design Patterns in Computer Vision Seminar Report, Summer 2025", "comment": null, "summary": "This report analyzes the evolution of key design patterns in computer vision\nby examining six influential papers. The analy- sis begins with foundational\narchitectures for image recognition. We review ResNet, which introduced\nresidual connections to overcome the vanishing gradient problem and enable\neffective training of significantly deeper convolutional networks.\nSubsequently, we examine the Vision Transformer (ViT), which established a new\nparadigm by applying the Transformer ar- chitecture to sequences of image\npatches, demonstrating the efficacy of attention-based models for large-scale\nimage recogni- tion. Building on these visual representation backbones, we\ninvestigate generative models. Generative Adversarial Networks (GANs) are\nanalyzed for their novel adversarial training process, which challenges a\ngenerator against a discriminator to learn complex data distributions. Then,\nLatent Diffusion Models (LDMs) are covered, which improve upon prior generative\nmethods by performing a sequential denoising process in a perceptually\ncompressed latent space. LDMs achieve high-fidelity synthesis with greater\ncomputational efficiency, representing the current state-of-the-art for image\ngeneration. Finally, we explore self-supervised learning techniques that reduce\ndependency on labeled data. DINO is a self-distillation framework in which a\nstudent network learns to match the output of a momentum-updated teacher,\nyielding features with strong k-NN classification performance. We conclude with\nMasked Autoencoders (MAE), which utilize an asymmetric encoder-decoder design\nto reconstruct heavily masked inputs, providing a highly scalable and effective\nmethod for pre-training large-scale vision models.", "AI": {"tldr": "This paper analyzes the evolution of computer vision design patterns through six influential papers, covering advancements in image recognition, generation, and self-supervised learning.", "motivation": "The paper aims to analyze the evolution of key design patterns in computer vision.", "method": "The paper analyzes key design patterns in computer vision by examining six influential papers, starting with foundational architectures and progressing to generative and self-supervised learning techniques.", "result": "The paper reviews ResNet, Vision Transformer (ViT), Generative Adversarial Networks (GANs), Latent Diffusion Models (LDMs), DINO, and Masked Autoencoders (MAE), highlighting their contributions and advancements in image recognition, generation, and self-supervised learning.", "conclusion": "This paper concludes by exploring self-supervised learning techniques like DINO and MAE to reduce dependency on labeled data, enhancing scalability and effectiveness in pre-training large-scale vision models."}}
{"id": "2507.23539", "categories": ["cs.LG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2507.23539", "abs": "https://arxiv.org/abs/2507.23539", "authors": ["Piotr Indyk", "Michael Kapralov", "Kshiteej Sheth", "Tal Wagner"], "title": "Improved Algorithms for Kernel Matrix-Vector Multiplication Under Sparsity Assumptions", "comment": "Published in ICLR 2025", "summary": "Motivated by the problem of fast processing of attention matrices, we study\nfast algorithms for computing matrix-vector products for asymmetric Gaussian\nKernel matrices $K\\in \\mathbb{R}^{n\\times n}$. $K$'s columns are indexed by a\nset of $n$ keys $k_1,k_2\\ldots, k_n\\in \\mathbb{R}^d$, rows by a set of $n$\nqueries $q_1,q_2,\\ldots,q_n\\in \\mathbb{R}^d $, and its $i,j$ entry is $K_{ij} =\ne^{-\\|q_i-k_j\\|_2^2/2\\sigma^2}$ for some bandwidth parameter $\\sigma>0$. Given\na vector $x\\in \\mathbb{R}^n$ and error parameter $\\epsilon>0$, our task is to\noutput a $y\\in \\mathbb{R}^n$ such that $\\|Kx-y\\|_2\\leq \\epsilon \\|x\\|_2$ in\ntime subquadratic in $n$ and linear in $d$. Our algorithms rely on the\nfollowing modelling assumption about the matrices $K$: the sum of the entries\nof $K$ scales linearly in $n$, as opposed to worst case quadratic growth. We\nvalidate this assumption experimentally, for Gaussian kernel matrices\nencountered in various settings such as fast attention computation in LLMs. We\nobtain the first subquadratic-time algorithm that works under this assumption,\nfor unrestricted vectors.", "AI": {"tldr": "fast algorithm for computing matrix-vector products for asymmetric Gaussian Kernel matrices", "motivation": "Motivated by the problem of fast processing of attention matrices", "method": "fast algorithms for computing matrix-vector products for asymmetric Gaussian Kernel matrices", "result": "output a $y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "conclusion": "We obtain the first subquadratic-time algorithm that works under this assumption, for unrestricted vectors."}}
{"id": "2507.23248", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23248", "abs": "https://arxiv.org/abs/2507.23248", "authors": ["Shimanto Bhowmik", "Tawsif Tashwar Dipto", "Md Sazzad Islam", "Sheryl Hsu", "Tahsin Reasat"], "title": "Evaluating LLMs' Multilingual Capabilities for Bengali: Benchmark Creation and Performance Analysis", "comment": null, "summary": "Bengali is an underrepresented language in NLP research. However, it remains\na challenge due to its unique linguistic structure and computational\nconstraints. In this work, we systematically investigate the challenges that\nhinder Bengali NLP performance by focusing on the absence of standardized\nevaluation benchmarks. We then evaluated 10 recent open source Large Language\nModels (LLMs) in 8 of the translated datasets and performed a comprehensive\nerror analysis to pinpoint their primary failure modes. Our findings reveal\nconsistent performance gaps for Bengali compared to English, particularly for\nsmaller models and specific model families like Mistral. We also identified\npromising robustness in certain architectures, such as DeepSeek, that maintain\nmore stable performance across languages. Our analysis reveals an inverse\nrelationship between tokenization efficiency and LLM accuracy where models tend\nto perform worse when inputs are excessively tokenized, whereas more efficient\n\\& concise tokenization results in improved performance. These findings\nhighlight critical areas where current models fall short and underscore the\nneed for improved dataset quality and evaluation methodologies tailored to\nmultilingual contexts. This work will catalyze further research on NLP for\nunderrepresented languages, helping to democratize access to advanced language\ntechnologies worldwide. The code and dataset used in this research is publicly\navailable at https://github.com/BengaliAI/bn-llm-benchmark.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b5f\u52a0\u62c9\u8bed\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6027\u80fd\u5dee\u8ddd\u548c tokenization \u6548\u7387\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5f3a\u8c03\u9700\u8981\u6539\u8fdb\u6570\u636e\u96c6\u8d28\u91cf\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u662f NLP \u7814\u7a76\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u8bed\u8a00\u3002\u7531\u4e8e\u5176\u72ec\u7279\u7684\u8bed\u8a00\u7ed3\u6784\u548c\u8ba1\u7b97\u9650\u5236\uff0c\u5b83\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u672c\u6587\u7cfb\u7edf\u5730\u8c03\u67e5\u4e86\u963b\u788d\u5b5f\u52a0\u62c9\u8bed NLP \u6027\u80fd\u7684\u6311\u6218\uff0c\u91cd\u70b9\u5173\u6ce8\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u5bf9 8 \u4e2a\u7ffb\u8bd1\u6570\u636e\u96c6\u4e2d 10 \u4e2a\u6700\u65b0\u7684\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u8fdb\u884c\u4e86\u5168\u9762\u7684\u9519\u8bef\u5206\u6790\u3002", "result": "\u53d1\u73b0\u5b5f\u52a0\u62c9\u8bed\u7684\u8868\u73b0\u4e0e\u82f1\u8bed\u76f8\u6bd4\u5b58\u5728\u6301\u7eed\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5c24\u5176\u662f\u5728\u8f83\u5c0f\u7684\u6a21\u578b\u548c\u7279\u5b9a\u7684\u6a21\u578b\u7cfb\u5217\uff08\u5982 Mistral\uff09\u4e2d\u3002Tokenization \u6548\u7387\u4e0e LLM \u51c6\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u53cd\u6bd4\u5173\u7cfb\u3002", "conclusion": "\u5f53\u524d\u6a21\u578b\u5728\u5b5f\u52a0\u62c9\u8bed\u4e0a\u7684\u8868\u73b0\u4e0d\u5982\u82f1\u8bed\uff0c\u5c24\u5176\u662f\u5728\u8f83\u5c0f\u6a21\u578b\u548c\u7279\u5b9a\u6a21\u578b\u7cfb\u5217\uff08\u5982 Mistral\uff09\u4e2d\u3002\u67d0\u4e9b\u67b6\u6784\uff08\u5982 DeepSeek\uff09\u8868\u73b0\u51fa\u826f\u597d\u7684\u9c81\u68d2\u6027\u3002Tokenization \u6548\u7387\u4e0e LLM \u51c6\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u53cd\u6bd4\u5173\u7cfb\u3002"}}
{"id": "2507.23362", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23362", "abs": "https://arxiv.org/abs/2507.23362", "authors": ["Ji Ma", "Wei Suo", "Peng Wang", "Yanning Zhang"], "title": "Short-LVLM: Compressing and Accelerating Large Vision-Language Models by Pruning Redundant Layers", "comment": "Accepted By ACM MM 25", "summary": "Although large vision-language models (LVLMs) have demonstrated impressive\ncapabilities in multi-modal understanding and reasoning, their practical\napplications are still limited by massive model parameters and high\ncomputational costs. Recent efforts from natural language processing (NLP) have\nshown the effectiveness of layer pruning, offering a plausible training-free\ncompression solution. However, due to the modality divergence between vision\nand language, it is unclear whether these NLP techniques are still effective in\nLVLMs. In this paper, we empirically prove that directly applying these layer\npruning methods to LVLMs is ineffective. Through extensive experiments, we find\nthat non-essential vision-language (VL) tokens and inter-layer feature gaps\npose critical challenges to pruning layers in LVLMs. Based on these insights,\nwe propose a novel framework Short-LVLM (SVL) that can utilize important VL\ntokens and mitigate the layer-wise feature gaps. Notably, Short-LVLM not only\nachieves a superior trade-off between performance and efficiency but also\nexhibits several potential advantages, i.e., training-free, model-agnostic, and\nhighly compatible. The code for this work is publicly available at\nhttps://github.com/ASGO-MM/Short-LVLM.", "AI": {"tldr": "LVLMs are too large. NLP pruning methods don't work well on them. Short-LVLM solves this by utilizing important VL tokens and mitigating the layer-wise feature gaps, achieving a better performance/efficiency trade-off.", "motivation": "Practical applications of LVLMs are limited by massive model parameters and high computational costs. Directly applying NLP layer pruning methods to LVLMs is ineffective due to modality divergence between vision and language.", "method": "A novel framework Short-LVLM (SVL) that can utilize important VL tokens and mitigate the layer-wise feature gaps.", "result": "Directly applying NLP layer pruning methods to LVLMs is ineffective. Non-essential vision-language (VL) tokens and inter-layer feature gaps pose critical challenges to pruning layers in LVLMs.", "conclusion": "Short-LVLM achieves a superior trade-off between performance and efficiency. It is training-free, model-agnostic, and highly compatible."}}
{"id": "2507.23562", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2507.23562", "abs": "https://arxiv.org/abs/2507.23562", "authors": ["Sirine Arfa", "Bernhard Vogginger", "Christian Mayr"], "title": "Hardware-Aware Fine-Tuning of Spiking Q-Networks on the SpiNNaker2 Neuromorphic Platform", "comment": "8 pages, 5 figures, 3 tables", "summary": "Spiking Neural Networks (SNNs) promise orders-of-magnitude lower power\nconsumption and low-latency inference on neuromorphic hardware for a wide range\nof robotic tasks. In this work, we present an energy-efficient implementation\nof a reinforcement learning (RL) algorithm using quantized SNNs to solve two\nclassical control tasks. The network is trained using the Q-learning algorithm,\nthen fine-tuned and quantized to low-bit (8-bit) precision for embedded\ndeployment on the SpiNNaker2 neuromorphic chip. To evaluate the comparative\nadvantage of SpiNNaker2 over conventional computing platforms, we analyze\ninference latency, dynamic power consumption, and energy cost per inference for\nour SNN models, comparing performance against a GTX 1650 GPU baseline. Our\nresults demonstrate SpiNNaker2's strong potential for scalable, low-energy\nneuromorphic computing, achieving up to 32x reduction in energy consumption.\nInference latency remains on par with GPU-based execution, with improvements\nobserved in certain task settings, reinforcing SpiNNaker2's viability for\nreal-time neuromorphic control and making the neuromorphic approach a\ncompelling direction for efficient deep Q-learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u91cf\u5316SNN\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u8282\u80fd\u5b9e\u73b0\uff0c\u4ee5\u89e3\u51b3\u4e24\u4e2a\u7ecf\u5178\u63a7\u5236\u4efb\u52a1\u3002\u7ed3\u679c\u8868\u660eSpiNNaker2\u5728\u80fd\u8017\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "motivation": "Spiking\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u6709\u671b\u5728\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u4e0a\u5b9e\u73b0\u66f4\u4f4e\u6570\u91cf\u7ea7\u7684\u529f\u8017\u548c\u4f4e\u5ef6\u8fdf\u63a8\u7406\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u673a\u5668\u4eba\u4efb\u52a1\u3002", "method": "\u4f7f\u7528\u91cf\u5316\u7684SNN\u548cQ\u5b66\u4e60\u7b97\u6cd5\u8bad\u7ec3\u7f51\u7edc\uff0c\u7136\u540e\u5728SpiNNaker2\u795e\u7ecf\u5f62\u6001\u82af\u7247\u4e0a\u8fdb\u884c\u5fae\u8c03\u548c\u91cf\u5316\u5230\u4f4e\u6bd4\u7279\uff088\u4f4d\uff09\u7cbe\u5ea6\u3002", "result": "SpiNNaker2\u7684\u63a8\u7406\u5ef6\u8fdf\u4e0e\u57fa\u4e8eGPU\u7684\u6267\u884c\u4fdd\u6301\u4e00\u81f4\uff0c\u5e76\u5728\u67d0\u4e9b\u4efb\u52a1\u8bbe\u7f6e\u4e2d\u89c2\u5bdf\u5230\u6539\u8fdb\u3002", "conclusion": "SpiNNaker2\u5728\u53ef\u6269\u5c55\u7684\u4f4e\u529f\u8017\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u65b9\u9762\u5177\u6709\u5f3a\u5927\u7684\u6f5c\u529b\uff0c\u4e0e\u57fa\u4e8eGPU\u7684\u6267\u884c\u76f8\u6bd4\uff0c\u80fd\u8017\u964d\u4f4e\u9ad8\u8fbe32\u500d\u3002"}}
{"id": "2507.23279", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23279", "abs": "https://arxiv.org/abs/2507.23279", "authors": ["Zunhai Su", "Qingyuan Li", "Hao Zhang", "YuLei Qian", "Yuchen Xie", "Kehong Yuan"], "title": "Unveiling Super Experts in Mixture-of-Experts Large Language Models", "comment": null, "summary": "Sparsely activated Mixture-of-Experts (MoE) models have shown promise in\nenhancing the learning capacity of large language models (LLMs). Leveraging the\nintrinsic importance differences among experts, recent research has explored\nexpert-level compression techniques to improve the efficiency of MoE LLMs.\nHowever, existing approaches often rely on empirical criteria to identify\ncritical experts, lacking a deeper exploration and understanding of the\nheterogeneous importance of experts. In this study, we present the first\ndiscovery and investigation of a distinct subset of experts that play a crucial\nrole in the underlying mechanisms during the model's forward inference. These\nexperts are prevalent in open-source MoE LLMs, and despite their limited\nnumber, pruning them leads to a significant decline in model performance (e.g.,\npruning three causes Qwen3-30B-A3B to produce repetitive and uninformative\noutputs). We refer to these experts as Super Experts (SEs). Our comprehensive\nanalysis provides progressively deeper insights into SEs. (i) SEs are\ncharacterized by rare but extreme activation outliers in the output of the\ndown_proj, which give rise to massive activations in the hidden states between\ndecoder layers. Moreover, the distribution of SEs remains model-specific and is\nunaffected by post-training processes. (ii) By pruning SEs, we assess their\nsignificance across a variety of tasks, revealing their considerable impact on\nthe model's overall performance, particularly in mathematical reasoning. (iii)\nWe further enhance our understanding of the influence of SEs compression. Our\nfindings confirm that MoE LLMs rely on SEs to induce attention sinks, which are\ncrucial for the distribution of attention scores but are significantly\ndisrupted by SE pruning. The code is available at\nhttps://github.com/ZunhaiSu/Super-Experts-Profilling.", "AI": {"tldr": "Identified and analyzed Super Experts (SEs) in MoE LLMs, showing their critical role in model performance, especially in mathematical reasoning, and their influence on attention sinks.", "motivation": "existing approaches often rely on empirical criteria to identify critical experts, lacking a deeper exploration and understanding of the heterogeneous importance of experts", "method": "discovery and investigation of a distinct subset of experts that play a crucial role in the underlying mechanisms during the model's forward inference", "result": "pruning them leads to a significant decline in model performance", "conclusion": "MoE LLMs rely on SEs to induce attention sinks, which are crucial for the distribution of attention scores but are significantly disrupted by SE pruning."}}
{"id": "2507.23371", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23371", "abs": "https://arxiv.org/abs/2507.23371", "authors": ["Ali Youssef"], "title": "VMatcher: State-Space Semi-Dense Local Feature Matching", "comment": null, "summary": "This paper introduces VMatcher, a hybrid Mamba-Transformer network for\nsemi-dense feature matching between image pairs. Learning-based feature\nmatching methods, whether detector-based or detector-free, achieve\nstate-of-the-art performance but depend heavily on the Transformer's attention\nmechanism, which, while effective, incurs high computational costs due to its\nquadratic complexity. In contrast, Mamba introduces a Selective State-Space\nModel (SSM) that achieves comparable or superior performance with linear\ncomplexity, offering significant efficiency gains. VMatcher leverages a hybrid\napproach, integrating Mamba's highly efficient long-sequence processing with\nthe Transformer's attention mechanism. Multiple VMatcher configurations are\nproposed, including hierarchical architectures, demonstrating their\neffectiveness in setting new benchmarks efficiently while ensuring robustness\nand practicality for real-time applications where rapid inference is crucial.\nSource Code is available at: https://github.com/ayoussf/VMatcher", "AI": {"tldr": "VMatcher is a hybrid Mamba-Transformer network for efficient semi-dense feature matching, reducing computational costs while maintaining performance.", "motivation": "Learning-based feature matching methods depend heavily on the Transformer's attention mechanism, which incurs high computational costs due to its quadratic complexity. Mamba introduces a Selective State-Space Model (SSM) that achieves comparable or superior performance with linear complexity, offering significant efficiency gains.", "method": "VMatcher leverages a hybrid approach, integrating Mamba's highly efficient long-sequence processing with the Transformer's attention mechanism. Multiple VMatcher configurations are proposed, including hierarchical architectures", "result": "VMatcher, a hybrid Mamba-Transformer network for semi-dense feature matching between image pairs.", "conclusion": "VMatcher demonstrates effectiveness in setting new benchmarks efficiently while ensuring robustness and practicality for real-time applications where rapid inference is crucial."}}
{"id": "2507.23568", "categories": ["cs.LG", "cond-mat.stat-mech", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.23568", "abs": "https://arxiv.org/abs/2507.23568", "authors": ["Fernando Mart\u00ednez-Garc\u00eda", "\u00c1lvaro Rubio-Garc\u00eda", "Samuel Fern\u00e1ndez-Lorenzo", "Juan Jos\u00e9 Garc\u00eda-Ripoll", "Diego Porras"], "title": "Optimised Feature Subset Selection via Simulated Annealing", "comment": "12 pages, 2 figures", "summary": "We introduce SA-FDR, a novel algorithm for $\\ell_0$-norm feature selection\nthat considers this task as a combinatorial optimisation problem and solves it\nby using simulated annealing to perform a global search over the space of\nfeature subsets. The optimisation is guided by the Fisher discriminant ratio,\nwhich we use as a computationally efficient proxy for model quality in\nclassification tasks. Our experiments, conducted on datasets with up to\nhundreds of thousands of samples and hundreds of features, demonstrate that\nSA-FDR consistently selects more compact feature subsets while achieving a high\npredictive accuracy. This ability to recover informative yet minimal sets of\nfeatures stems from its capacity to capture inter-feature dependencies often\nmissed by greedy optimisation approaches. As a result, SA-FDR provides a\nflexible and effective solution for designing interpretable models in\nhigh-dimensional settings, particularly when model sparsity, interpretability,\nand performance are crucial.", "AI": {"tldr": "SA-FDR, a novel algorithm,consistently selects more compact feature subsets while achieving a high predictive accuracy.This ability stems from its capacity to capture inter-feature dependencies often missed by greedy optimisation approaches.", "motivation": "designing interpretable models in high-dimensional settings, particularly when model sparsity, interpretability, and performance are crucial", "method": "a novel algorithm for $\\ell_0$-norm feature selection that considers this task as a combinatorial optimisation problem and solves it by using simulated annealing to perform a global search over the space of feature subsets. The optimisation is guided by the Fisher discriminant ratio", "result": "SA-FDR consistently selects more compact feature subsets while achieving a high predictive accuracy", "conclusion": "SA-FDR provides a flexible and effective solution for designing interpretable models in high-dimensional settings, particularly when model sparsity, interpretability, and performance are crucial."}}
{"id": "2507.23319", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23319", "abs": "https://arxiv.org/abs/2507.23319", "authors": ["Alfio Ferrara", "Sergio Picascia", "Laura Pinnavaia", "Vojimir Ranitovic", "Elisabetta Rocchetti", "Alice Tuveri"], "title": "What's Taboo for You? - An Empirical Evaluation of LLMs Behavior Toward Sensitive Content", "comment": null, "summary": "Proprietary Large Language Models (LLMs) have shown tendencies toward\npoliteness, formality, and implicit content moderation. While previous research\nhas primarily focused on explicitly training models to moderate and detoxify\nsensitive content, there has been limited exploration of whether LLMs\nimplicitly sanitize language without explicit instructions. This study\nempirically analyzes the implicit moderation behavior of GPT-4o-mini when\nparaphrasing sensitive content and evaluates the extent of sensitivity shifts.\nOur experiments indicate that GPT-4o-mini systematically moderates content\ntoward less sensitive classes, with substantial reductions in derogatory and\ntaboo language. Also, we evaluate the zero-shot capabilities of LLMs in\nclassifying sentence sensitivity, comparing their performances against\ntraditional methods.", "AI": {"tldr": "GPT-4o-mini implicitly moderates sensitive content, reducing derogatory and taboo language. The study also evaluates LLMs' zero-shot capabilities in classifying sentence sensitivity.", "motivation": "Proprietary Large Language Models (LLMs) have shown tendencies toward politeness, formality, and implicit content moderation. While previous research has primarily focused on explicitly training models to moderate and detoxify sensitive content, there has been limited exploration of whether LLMs implicitly sanitize language without explicit instructions.", "method": "This study empirically analyzes the implicit moderation behavior of GPT-4o-mini when paraphrasing sensitive content and evaluates the extent of sensitivity shifts.", "result": "GPT-4o-mini systematically moderates content toward less sensitive classes, with substantial reductions in derogatory and taboo language.", "conclusion": "GPT-4o-mini systematically moderates content toward less sensitive classes, with substantial reductions in derogatory and taboo language. Also, we evaluate the zero-shot capabilities of LLMs in classifying sentence sensitivity, comparing their performances against traditional methods."}}
{"id": "2507.23372", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23372", "abs": "https://arxiv.org/abs/2507.23372", "authors": ["Yijie Zhu", "Lingsen Zhang", "Zitong Yu", "Rui Shao", "Tao Tan", "Liqiang Nie"], "title": "UniEmo: Unifying Emotional Understanding and Generation with Learnable Expert Queries", "comment": null, "summary": "Emotional understanding and generation are often treated as separate tasks,\nyet they are inherently complementary and can mutually enhance each other. In\nthis paper, we propose the UniEmo, a unified framework that seamlessly\nintegrates these two tasks. The key challenge lies in the abstract nature of\nemotions, necessitating the extraction of visual representations beneficial for\nboth tasks. To address this, we propose a hierarchical emotional understanding\nchain with learnable expert queries that progressively extracts multi-scale\nemotional features, thereby serving as a foundational step for unification.\nSimultaneously, we fuse these expert queries and emotional representations to\nguide the diffusion model in generating emotion-evoking images. To enhance the\ndiversity and fidelity of the generated emotional images, we further introduce\nthe emotional correlation coefficient and emotional condition loss into the\nfusion process. This step facilitates fusion and alignment for emotional\ngeneration guided by the understanding. In turn, we demonstrate that joint\ntraining allows the generation component to provide implicit feedback to the\nunderstanding part. Furthermore, we propose a novel data filtering algorithm to\nselect high-quality and diverse emotional images generated by the well-trained\nmodel, which explicitly feedback into the understanding part. Together, these\ngeneration-driven dual feedback processes enhance the model's understanding\ncapacity. Extensive experiments show that UniEmo significantly outperforms\nstate-of-the-art methods in both emotional understanding and generation tasks.\nThe code for the proposed method is available at\nhttps://github.com/JiuTian-VL/UniEmo.", "AI": {"tldr": "UniEmo\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u60c5\u611f\u7406\u89e3\u548c\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u548c\u53cc\u91cd\u53cd\u9988\u8fc7\u7a0b\uff0c\u5728\u4e24\u4e2a\u4efb\u52a1\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u60c5\u611f\u7406\u89e3\u548c\u751f\u6210\u901a\u5e38\u88ab\u89c6\u4e3a\u72ec\u7acb\u7684\u4efb\u52a1\uff0c\u4f46\u5b83\u4eec\u672c\u8d28\u4e0a\u662f\u4e92\u8865\u7684\uff0c\u5e76\u4e14\u53ef\u4ee5\u76f8\u4e92\u4fc3\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6UniEmo\uff0c\u96c6\u6210\u4e86\u60c5\u611f\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u3002\u8be5\u6846\u67b6\u5229\u7528\u5206\u5c42\u60c5\u611f\u7406\u89e3\u94fe\u63d0\u53d6\u591a\u5c3a\u5ea6\u60c5\u611f\u7279\u5f81\uff0c\u5e76\u878d\u5408\u4e13\u5bb6\u67e5\u8be2\u548c\u60c5\u611f\u8868\u5f81\u6765\u6307\u5bfc\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\u3002\u5f15\u5165\u60c5\u611f\u76f8\u5173\u7cfb\u6570\u548c\u60c5\u611f\u6761\u4ef6\u635f\u5931\u6765\u63d0\u9ad8\u751f\u6210\u56fe\u50cf\u7684\u591a\u6837\u6027\u548c\u4fdd\u771f\u5ea6\u3002\u63d0\u51fa\u4e86\u6570\u636e\u8fc7\u6ee4\u7b97\u6cd5\u6765\u9009\u62e9\u9ad8\u8d28\u91cf\u548c\u591a\u6837\u5316\u7684\u60c5\u611f\u56fe\u50cf\uff0c\u5e76\u53cd\u9988\u5230\u7406\u89e3\u90e8\u5206\u3002", "result": "UniEmo\u5728\u60c5\u611f\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "UniEmo\u5728\u60c5\u611f\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.23581", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23581", "abs": "https://arxiv.org/abs/2507.23581", "authors": ["Chuanyue Yu", "Kuo Zhao", "Yuhan Li", "Heng Chang", "Mingjian Feng", "Xiangzhe Jiang", "Yufei Sun", "Jia Li", "Yuzhi Zhang", "Jianxin Li", "Ziwei Zhang"], "title": "GraphRAG-R1: Graph Retrieval-Augmented Generation with Process-Constrained Reinforcement Learning", "comment": null, "summary": "Graph Retrieval-Augmented Generation (GraphRAG) has shown great effectiveness\nin enhancing the reasoning abilities of LLMs by leveraging graph structures for\nknowledge representation and modeling complex real-world relationships.\nHowever, existing GraphRAG methods still face significant bottlenecks when\nhandling complex problems that require multi-hop reasoning, as their query and\nretrieval phases are largely based on pre-defined heuristics and do not fully\nutilize the reasoning potentials of LLMs. To address this problem, we propose\nGraphRAG-R1, an adaptive GraphRAG framework by training LLMs with\nprocess-constrained outcome-based reinforcement learning (RL) to enhance the\nmulti-hop reasoning ability. Our method can decompose complex problems,\nautonomously invoke retrieval tools to acquire necessary information, and\nperform effective reasoning. Specifically, we utilize a modified version of\nGroup Relative Policy Optimization (GRPO) that supports rollout-with-thinking\ncapability. Next, we design two process-constrained reward functions. To handle\nthe shallow retrieval problem, we design a Progressive Retrieval Attenuation\n(PRA) reward to encourage essential retrievals. Then, to handle the\nover-thinking problem, we design Cost-Aware F1 (CAF) reward to balance the\nmodel performance with computational costs. We further design a phase-dependent\ntraining strategy, containing three training stages corresponding to cold start\nand these two rewards. Lastly, our method adopts a hybrid graph-textual\nretrieval to improve the reasoning capacity. Extensive experimental results\ndemonstrate that GraphRAG-R1 boosts LLM capabilities in solving complex\nreasoning problems compared to state-of-the-art GraphRAG methods on both\nin-domain and out-of-domain datasets. Furthermore, our framework can be\nflexibly integrated with various existing retrieval methods, consistently\ndelivering performance improvements.", "AI": {"tldr": "GraphRAG-R1 is proposed to enhance multi-hop reasoning in GraphRAG by training LLMs with reinforcement learning and hybrid graph-textual retrieval.", "motivation": "Existing GraphRAG methods face significant bottlenecks when handling complex problems that require multi-hop reasoning, as their query and retrieval phases are largely based on pre-defined heuristics and do not fully utilize the reasoning potentials of LLMs.", "method": "training LLMs with process-constrained outcome-based reinforcement learning (RL) to enhance the multi-hop reasoning ability. It utilizes a modified version of Group Relative Policy Optimization (GRPO) that supports rollout-with-thinking capability and designs two process-constrained reward functions: Progressive Retrieval Attenuation (PRA) and Cost-Aware F1 (CAF). A phase-dependent training strategy, containing three training stages corresponding to cold start and these two rewards is used. Lastly, a hybrid graph-textual retrieval is adopted.", "result": "GraphRAG-R1 boosts LLM capabilities in solving complex reasoning problems compared to state-of-the-art GraphRAG methods on both in-domain and out-of-domain datasets. Furthermore, the framework can be flexibly integrated with various existing retrieval methods, consistently delivering performance improvements.", "conclusion": "GraphRAG-R1 boosts LLM capabilities in solving complex reasoning problems compared to state-of-the-art GraphRAG methods on both in-domain and out-of-domain datasets and can be flexibly integrated with various existing retrieval methods, consistently delivering performance improvements."}}
{"id": "2507.23373", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23373", "abs": "https://arxiv.org/abs/2507.23373", "authors": ["Haoran Chen", "Zexiao Wang", "Haidong Cao", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "Multi-Prompt Progressive Alignment for Multi-Source Unsupervised Domain Adaptation", "comment": null, "summary": "Large Vision-Language Models like CLIP have become a powerful foundation for\nUnsupervised Domain Adaptation due to their strong zero-shot generalization.\nState-of-the-art methods typically leverage CLIP to generate pseudo-labels for\nthe target domain, then fine-tune the model to learn domain-invariant features.\nHowever, these methods attempt to align source and target domains using all\npseudo-labeled data simultaneously. This one-shot alignment struggles with\nnoisy, hard-to-classify samples, leading to error propagation and suboptimal\nfeature learning. The problem is even more amplified in the multi-source\nscenario, where diverse domain gaps and varying noise levels across multiple\nsource domains further destabilize the alignment process. To address this\nissue, in this work, we propose a progressive alignment strategy for adapting\nCLIP to unlabeled downstream task. Our method begins by training the model on a\nhigh-confidence subset of target samples, allowing it to first learn a\nwell-aligned representation from the most reliable data. As training\nprogresses, it gradually incorporates more challenging samples, guiding the\nmodel to refine its understanding without being overwhelmed by initial label\nnoise. This progressive approach effectively mitigates confirmation bias and\npromotes a more robust convergence, allowing for the learning of genuinely\ndomain-invariant features. We name our approach MP^2A and test it on three\npopular UDA benchmarks, namely ImageCLEF, Office-Home, and the most challenging\nDomainNet. Experiments showcase that MP^2A achieves state-of-the-art\nperformance when compared with most recent CLIP-based MS-UDA approaches,\ndemonstrating the effectiveness of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6e10\u8fdb\u5f0f\u5bf9\u9f50\u7b56\u7565MP^2A\uff0c\u4ee5\u63d0\u9ad8CLIP\u5728\u65e0\u76d1\u7763\u57df\u9002\u5e94\u4e2d\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u5728\u53ef\u9760\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u9010\u6b65\u52a0\u5165\u66f4\u5177\u6311\u6218\u6027\u7684\u6837\u672c\uff0c\u4ece\u800c\u51cf\u8f7b\u786e\u8ba4\u504f\u5dee\u5e76\u4fc3\u8fdb\u66f4\u7a33\u5065\u7684\u6536\u655b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8bd5\u56fe\u540c\u65f6\u4f7f\u7528\u6240\u6709\u4f2a\u6807\u8bb0\u6570\u636e\u6765\u5bf9\u9f50\u6e90\u57df\u548c\u76ee\u6807\u57df\uff0c\u8fd9\u79cd\u4e00\u6b21\u6027\u5bf9\u9f50\u96be\u4ee5\u5904\u7406\u566a\u58f0\u6837\u672c\uff0c\u5bfc\u81f4\u8bef\u5dee\u4f20\u64ad\u548c\u6b21\u4f18\u7279\u5f81\u5b66\u4e60\u3002\u591a\u6e90\u573a\u666f\u4e2d\uff0c\u4e0d\u540c\u7684\u57df\u95f4\u5dee\u8ddd\u548c\u4e0d\u540c\u566a\u58f0\u6c34\u5e73\u4f1a\u8fdb\u4e00\u6b65\u7834\u574f\u5bf9\u9f50\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6e10\u8fdb\u5f0f\u5bf9\u9f50\u7b56\u7565\uff0c\u7528\u4e8e\u5c06CLIP\u9002\u5e94\u5230\u65e0\u6807\u7b7e\u7684\u4e0b\u6e38\u4efb\u52a1\u3002", "result": "MP^2A\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MP^2A\u5728\u4e09\u4e2a\u6d41\u884c\u7684UDA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.23600", "categories": ["cs.LG", "cs.CE", "G.1.6; G.3; G.4; I.6.5"], "pdf": "https://arxiv.org/pdf/2507.23600", "abs": "https://arxiv.org/abs/2507.23600", "authors": ["Yu-Tang Chang", "Shih-Fang Chen"], "title": "EB-gMCR: Energy-Based Generative Modeling for Signal Unmixing and Multivariate Curve Resolution", "comment": null, "summary": "Signal unmixing analysis decomposes data into basic patterns and is widely\napplied in chemical and biological research. Multivariate curve resolution\n(MCR), a branch of signal unmixing, separates mixed chemical signals into base\npatterns (components) and their concentrations, playing a key role in\nunderstanding composition. Classical MCR is typically framed as matrix\nfactorization (MF) and requires a user-specified component count, usually\nunknown in real data. As dataset size or component count increases, the\nscalability and reliability of MF-based MCR face significant challenges. This\nstudy reformulates MCR as a generative process (gMCR), and introduces an\nenergy-based deep learning solver, EB-gMCR, that automatically discovers the\nsmallest component set able to reconstruct the data faithfully. EB-gMCR starts\nfrom a large candidate pool (e.g., 1024 spectra) and employs a differentiable\ngating network to retain only active components while estimating their\nconcentrations. On noisy synthetic datasets containing up to 256 latent\nsources, EB-gMCR maintained R^2 >= 0.98 and recovered the component count\nwithin 5% of the ground truth; at lower noise it achieved R^2 >= 0.99 with near\nexact component estimation. Additional chemical priors, such as non-negativity\nor nonlinear mixing, enter as simple plug-in functions, enabling adaptation to\nother instruments or domains without altering the core learning process. By\nuniting high-capacity generative modeling and hard component selection, EB-gMCR\noffers a practical route to large-scale signal unmixing analysis, including\nchemical library-driven scenarios. The source code is available at\nhttps://github.com/b05611038/ebgmcr_solver.", "AI": {"tldr": "This paper introduces EB-gMCR, a deep learning approach to signal unmixing that overcomes limitations of traditional methods by automatically discovering the optimal number of components and scaling to large datasets.", "motivation": "Classical MCR is typically framed as matrix factorization (MF) and requires a user-specified component count, usually unknown in real data. As dataset size or component count increases, the scalability and reliability of MF-based MCR face significant challenges.", "method": "reformulates MCR as a generative process (gMCR), and introduces an energy-based deep learning solver, EB-gMCR, that automatically discovers the smallest component set able to reconstruct the data faithfully. EB-gMCR starts from a large candidate pool (e.g., 1024 spectra) and employs a differentiable gating network to retain only active components while estimating their concentrations.", "result": "On noisy synthetic datasets containing up to 256 latent sources, EB-gMCR maintained R^2 >= 0.98 and recovered the component count within 5% of the ground truth; at lower noise it achieved R^2 >= 0.99 with near exact component estimation.", "conclusion": "EB-gMCR offers a practical route to large-scale signal unmixing analysis, including chemical library-driven scenarios."}}
{"id": "2507.23374", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23374", "abs": "https://arxiv.org/abs/2507.23374", "authors": ["Shuangkang Fang", "I-Chao Shen", "Takeo Igarashi", "Yufeng Wang", "ZeSheng Wang", "Yi Yang", "Wenrui Ding", "Shuchang Zhou"], "title": "NeRF Is a Valuable Assistant for 3D Gaussian Splatting", "comment": "Accepted by ICCV", "summary": "We introduce NeRF-GS, a novel framework that jointly optimizes Neural\nRadiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). This framework\nleverages the inherent continuous spatial representation of NeRF to mitigate\nseveral limitations of 3DGS, including sensitivity to Gaussian initialization,\nlimited spatial awareness, and weak inter-Gaussian correlations, thereby\nenhancing its performance. In NeRF-GS, we revisit the design of 3DGS and\nprogressively align its spatial features with NeRF, enabling both\nrepresentations to be optimized within the same scene through shared 3D spatial\ninformation. We further address the formal distinctions between the two\napproaches by optimizing residual vectors for both implicit features and\nGaussian positions to enhance the personalized capabilities of 3DGS.\nExperimental results on benchmark datasets show that NeRF-GS surpasses existing\nmethods and achieves state-of-the-art performance. This outcome confirms that\nNeRF and 3DGS are complementary rather than competing, offering new insights\ninto hybrid approaches that combine 3DGS and NeRF for efficient 3D scene\nrepresentation.", "AI": {"tldr": "NeRF-GS \u7ed3\u5408\u4e86 NeRF \u548c 3DGS \u7684\u4f18\u70b9\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5229\u7528 NeRF \u56fa\u6709\u7684\u8fde\u7eed\u7a7a\u95f4\u8868\u793a\u6765\u7f13\u89e3 3DGS \u7684\u4e00\u4e9b\u5c40\u9650\u6027\uff0c\u5305\u62ec\u5bf9\u9ad8\u65af\u521d\u59cb\u5316\u654f\u611f\u3001\u7a7a\u95f4\u611f\u77e5\u6709\u9650\u4ee5\u53ca\u9ad8\u65af\u95f4\u76f8\u5173\u6027\u8f83\u5f31\uff0c\u4ece\u800c\u63d0\u9ad8\u5176\u6027\u80fd\u3002", "method": "\u5171\u540c\u4f18\u5316\u795e\u7ecf\u8f90\u5c04\u573a (NeRF) \u548c 3D \u9ad8\u65af Splatting (3DGS)\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cNeRF-GS \u8d85\u8fc7\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "NeRF \u548c 3DGS \u662f\u4e92\u8865\u7684\uff0c\u800c\u4e0d\u662f\u7ade\u4e89\u5173\u7cfb\uff0c\u4e3a\u7ed3\u5408 3DGS \u548c NeRF \u4ee5\u5b9e\u73b0\u9ad8\u6548 3D \u573a\u666f\u8868\u793a\u7684\u6df7\u5408\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.23604", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23604", "abs": "https://arxiv.org/abs/2507.23604", "authors": ["Tommaso Marzi", "Cesare Alippi", "Andrea Cini"], "title": "Hierarchical Message-Passing Policies for Multi-Agent Reinforcement Learning", "comment": null, "summary": "Decentralized Multi-Agent Reinforcement Learning (MARL) methods allow for\nlearning scalable multi-agent policies, but suffer from partial observability\nand induced non-stationarity. These challenges can be addressed by introducing\nmechanisms that facilitate coordination and high-level planning. Specifically,\ncoordination and temporal abstraction can be achieved through communication\n(e.g., message passing) and Hierarchical Reinforcement Learning (HRL)\napproaches to decision-making. However, optimization issues limit the\napplicability of hierarchical policies to multi-agent systems. As such, the\ncombination of these approaches has not been fully explored. To fill this void,\nwe propose a novel and effective methodology for learning multi-agent\nhierarchies of message-passing policies. We adopt the feudal HRL framework and\nrely on a hierarchical graph structure for planning and coordination among\nagents. Agents at lower levels in the hierarchy receive goals from the upper\nlevels and exchange messages with neighboring agents at the same level. To\nlearn hierarchical multi-agent policies, we design a novel reward-assignment\nmethod based on training the lower-level policies to maximize the advantage\nfunction associated with the upper levels. Results on relevant benchmarks show\nthat our method performs favorably compared to the state of the art.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u667a\u80fd\u4f53\u5206\u5c42\u6d88\u606f\u4f20\u9012\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5206\u6563\u5f0f\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u65b9\u6cd5\u5141\u8bb8\u5b66\u4e60\u53ef\u6269\u5c55\u7684\u591a\u667a\u80fd\u4f53\u7b56\u7565\uff0c\u4f46\u5b58\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u6027\u548c\u8bf1\u5bfc\u7684\u975e\u5e73\u7a33\u6027\u3002\u8fd9\u4e9b\u6311\u6218\u53ef\u4ee5\u901a\u8fc7\u5f15\u5165\u4fc3\u8fdb\u534f\u8c03\u548c\u9ad8\u7ea7\u89c4\u5212\u7684\u673a\u5236\u6765\u89e3\u51b3\u3002\u5177\u4f53\u800c\u8a00\uff0c\u534f\u8c03\u548c\u65f6\u95f4\u62bd\u8c61\u53ef\u4ee5\u901a\u8fc7\u901a\u4fe1\uff08\u4f8b\u5982\uff0c\u6d88\u606f\u4f20\u9012\uff09\u548c\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\uff08HRL\uff09\u65b9\u6cd5\u6765\u5b9e\u73b0\u51b3\u7b56\u3002\u4f46\u662f\uff0c\u4f18\u5316\u95ee\u9898\u9650\u5236\u4e86\u5206\u5c42\u7b56\u7565\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u9002\u7528\u6027\u3002\u56e0\u6b64\uff0c\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u8fd9\u4e9b\u65b9\u6cd5\u7684\u7ec4\u5408\u3002", "method": "\u6211\u4eec\u91c7\u7528\u4e86\u5c01\u5efaHRL\u6846\u67b6\uff0c\u5e76\u4f9d\u8d56\u4e8e\u5206\u5c42\u56fe\u7ed3\u6784\u6765\u5b9e\u73b0\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u89c4\u5212\u548c\u534f\u8c03\u3002\u5c42\u7ea7\u7ed3\u6784\u4e2d\u8f83\u4f4e\u7ea7\u522b\u7684\u667a\u80fd\u4f53\u63a5\u6536\u6765\u81ea\u4e0a\u7ea7\u522b\u7684\u76ee\u6807\uff0c\u5e76\u4e0e\u540c\u4e00\u7ea7\u522b\u7684\u76f8\u90bb\u667a\u80fd\u4f53\u4ea4\u6362\u6d88\u606f\u3002\u4e3a\u4e86\u5b66\u4e60\u5206\u5c42\u591a\u667a\u80fd\u4f53\u7b56\u7565\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u5956\u52b1\u5206\u914d\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u8bad\u7ec3\u8f83\u4f4e\u7ea7\u522b\u7684\u7b56\u7565\uff0c\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u4e0e\u4e0a\u7ea7\u522b\u76f8\u5173\u7684\u4f18\u52bf\u51fd\u6570\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u76f8\u5173\u57fa\u51c6\u4e0a\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6709\u6548\u7684\u5b66\u4e60\u6d88\u606f\u4f20\u9012\u7b56\u7565\u7684\u591a\u667a\u80fd\u4f53\u5c42\u7ea7\u7ed3\u6784\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u5728\u76f8\u5173\u57fa\u51c6\u4e0a\u7684\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2507.23382", "categories": ["cs.CL", "cs.AI", "cs.CV", "I.2.8; I.2.10"], "pdf": "https://arxiv.org/pdf/2507.23382", "abs": "https://arxiv.org/abs/2507.23382", "authors": ["Yiyan Ji", "Haoran Chen", "Qiguang Chen", "Chengyue Wu", "Libo Qin", "Wanxiang Che"], "title": "MPCC: A Novel Benchmark for Multimodal Planning with Complex Constraints in Multimodal Large Language Models", "comment": "Accepted to ACM Multimedia 2025", "summary": "Multimodal planning capabilities refer to the ability to predict, reason, and\ndesign steps for task execution with multimodal context, which is essential for\ncomplex reasoning and decision-making across multiple steps. However, current\nbenchmarks face two key challenges: (1) they cannot directly assess multimodal\nreal-world planning capabilities, and (2) they lack constraints or implicit\nconstraints across modalities. To address these issues, we introduce Multimodal\nPlanning with Complex Constraints (MPCC), the first benchmark to systematically\nevaluate MLLMs' ability to handle multimodal constraints in planning. To\naddress the first challenge, MPCC focuses on three real-world tasks: Flight\nPlanning, Calendar Planning, and Meeting Planning. To solve the second\nchallenge, we introduce complex constraints (e.g. budget, temporal, and\nspatial) in these tasks, with graded difficulty levels (EASY, MEDIUM, HARD) to\nseparate constraint complexity from search space expansion. Experiments on 13\nadvanced MLLMs reveal significant challenges: closed-source models achieve only\n21.3% feasible plans, while open-source models average below 11%. Additionally,\nwe observe that MLLMs are highly sensitive to constraint complexity and that\ntraditional multimodal prompting strategies fail in multi-constraint scenarios.\nOur work formalizes multimodal constraints in planning, provides a rigorous\nevaluation framework, and highlights the need for advancements in\nconstraint-aware reasoning for real-world MLLM applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001\u89c4\u5212\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u7ea6\u675f\u65f6\u7684\u4e0d\u8db3\uff0c\u5f3a\u8c03\u4e86\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u63d0\u5347\u7ea6\u675f\u611f\u77e5\u63a8\u7406\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u76f4\u63a5\u8bc4\u4f30\u591a\u6a21\u6001\u73b0\u5b9e\u4e16\u754c\u89c4\u5212\u80fd\u529b\uff0c\u5e76\u4e14\u7f3a\u4e4f\u8de8\u6a21\u6001\u7684\u7ea6\u675f\u6216\u9690\u5f0f\u7ea6\u675f\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5MPCC\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89c4\u5212\u4e2d\u5904\u7406\u591a\u6a21\u6001\u7ea6\u675f\u7684\u80fd\u529b\u3002MPCC\u5305\u542b\u4e09\u4e2a\u73b0\u5b9e\u4e16\u754c\u7684\u4efb\u52a1\uff1a\u822a\u73ed\u89c4\u5212\u3001\u65e5\u5386\u89c4\u5212\u548c\u4f1a\u8bae\u89c4\u5212\uff0c\u5e76\u5f15\u5165\u4e86\u9884\u7b97\u3001\u65f6\u95f4\u3001\u7a7a\u95f4\u7b49\u590d\u6742\u7ea6\u675f\uff0c\u4ee5\u53ca\u96be\u5ea6\u7b49\u7ea7\u5212\u5206\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u95ed\u6e90\u6a21\u578b\u7684\u53ef\u884c\u8ba1\u5212\u6210\u529f\u7387\u4ec5\u4e3a21.3%\uff0c\u5f00\u6e90\u6a21\u578b\u5e73\u5747\u4f4e\u4e8e11%\u3002\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u7ea6\u675f\u590d\u6742\u6027\u9ad8\u5ea6\u654f\u611f\uff0c\u4f20\u7edf\u7684\u63d0\u793a\u7b56\u7565\u5728\u591a\u91cd\u7ea6\u675f\u573a\u666f\u4e0b\u5931\u6548\u3002", "conclusion": "\u8fd9\u7bc7\u8bba\u6587\u8868\u660e\uff0c\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5177\u6709\u590d\u6742\u7ea6\u675f\u7684\u73b0\u5b9e\u4e16\u754c\u89c4\u5212\u4efb\u52a1\u65f6\u9762\u4e34\u663e\u8457\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u7ea6\u675f\u611f\u77e5\u63a8\u7406\u65b9\u9762\u9700\u8981\u8fdb\u4e00\u6b65\u63d0\u5347\u3002"}}
{"id": "2507.23402", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23402", "abs": "https://arxiv.org/abs/2507.23402", "authors": ["Wei Li", "Xun Gong", "Jiao Li", "Xiaobin Sun"], "title": "AGA: An adaptive group alignment framework for structured medical cross-modal representation learning", "comment": null, "summary": "Learning medical visual representations from paired images and reports is a\npromising direction in representation learning. However, current\nvision-language pretraining methods in the medical domain often simplify\nclinical reports into single entities or fragmented tokens, ignoring their\ninherent structure. In addition, contrastive learning frameworks typically\ndepend on large quantities of hard negative samples, which is impractical for\nsmall-scale medical datasets. To tackle these challenges, we propose Adaptive\nGrouped Alignment (AGA), a new framework that captures structured semantics\nfrom paired medical images and reports. AGA introduces a bidirectional grouping\nmechanism based on a sparse similarity matrix. For each image-report pair, we\ncompute fine-grained similarities between text tokens and image patches. Each\ntoken selects its top-matching patches to form a visual group, and each patch\nselects its most related tokens to form a language group. To enable adaptive\ngrouping, we design two threshold gating modules, called Language Grouped\nThreshold Gate and Vision Grouped Threshold Gate, which learn grouping\nthresholds dynamically. Group representations are computed as weighted averages\nbased on similarity scores. To align each token with its group representation,\nwe introduce an Instance Aware Group Alignment loss that operates within each\nimage-text pair, removing the need for external negatives. Finally, a\nBidirectional Cross-modal Grouped Alignment module is applied to enhance\nfine-grained alignment between visual and linguistic group representations.\nExtensive experiments on public and private datasets show that our method\nachieves strong performance on image-text retrieval and classification tasks\nunder both fine-tuning and zero-shot settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u5206\u7ec4\u5bf9\u9f50\u6846\u67b6\uff08AGA\uff09\uff0c\u7528\u4e8e\u4ece\u914d\u5bf9\u7684\u533b\u5b66\u56fe\u50cf\u548c\u62a5\u544a\u4e2d\u6355\u83b7\u7ed3\u6784\u5316\u8bed\u4e49\uff0c\u5e76\u5728\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u5f3a\u5927\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u5b66\u9886\u57df\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u65b9\u6cd5\u901a\u5e38\u5c06\u4e34\u5e8a\u62a5\u544a\u7b80\u5316\u4e3a\u5355\u4e2a\u5b9e\u4f53\u6216\u788e\u7247\u5316\u7684token\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u56fa\u6709\u7684\u7ed3\u6784\u3002\u6b64\u5916\uff0c\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u901a\u5e38\u4f9d\u8d56\u4e8e\u5927\u91cf\u7684\u96be\u8d1f\u6837\u672c\uff0c\u8fd9\u5bf9\u4e8e\u5c0f\u89c4\u6a21\u533b\u5b66\u6570\u636e\u96c6\u662f\u4e0d\u5207\u5b9e\u9645\u7684\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u5373\u81ea\u9002\u5e94\u5206\u7ec4\u5bf9\u9f50\uff08AGA\uff09\uff0c\u8be5\u6846\u67b6\u4ece\u914d\u5bf9\u7684\u533b\u5b66\u56fe\u50cf\u548c\u62a5\u544a\u4e2d\u6355\u83b7\u7ed3\u6784\u5316\u8bed\u4e49\u3002AGA\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u76f8\u4f3c\u5ea6\u77e9\u9635\u7684\u53cc\u5411\u5206\u7ec4\u673a\u5236\u3002\u4e3a\u5b9e\u73b0\u81ea\u9002\u5e94\u5206\u7ec4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u9608\u503c\u95e8\u63a7\u6a21\u5757\uff0c\u5206\u522b\u79f0\u4e3a\u8bed\u8a00\u5206\u7ec4\u9608\u503c\u95e8\u548c\u89c6\u89c9\u5206\u7ec4\u9608\u503c\u95e8\uff0c\u5b83\u4eec\u53ef\u4ee5\u52a8\u6001\u5b66\u4e60\u5206\u7ec4\u9608\u503c\u3002\u7ec4\u8868\u793a\u88ab\u8ba1\u7b97\u4e3a\u57fa\u4e8e\u76f8\u4f3c\u5ea6\u5206\u6570\u7684\u52a0\u6743\u5e73\u5747\u503c\u3002\u4e3a\u4e86\u4f7f\u6bcf\u4e2atoken\u4e0e\u5176\u7ec4\u8868\u793a\u5bf9\u9f50\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u5b9e\u4f8b\u611f\u77e5\u7ec4\u5bf9\u9f50\u635f\u5931\uff0c\u8be5\u635f\u5931\u5728\u6bcf\u4e2a\u56fe\u50cf-\u6587\u672c\u5bf9\u4e2d\u8fd0\u884c\uff0c\u4ece\u800c\u65e0\u9700\u5916\u90e8\u8d1f\u4f8b\u3002\u6700\u540e\uff0c\u5e94\u7528\u53cc\u5411\u8de8\u6a21\u6001\u5206\u7ec4\u5bf9\u9f50\u6a21\u5757\u6765\u589e\u5f3a\u89c6\u89c9\u548c\u8bed\u8a00\u7ec4\u8868\u793a\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u3002", "result": "\u5728\u516c\u5171\u548c\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5fae\u8c03\u548c\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u90fd\u53d6\u5f97\u4e86\u5f3a\u5927\u7684\u6027\u80fd\u3002", "conclusion": "AGA\u5728\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u65e0\u8bba\u662f\u5728\u5fae\u8c03\u8fd8\u662f\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u3002"}}
{"id": "2507.23607", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23607", "abs": "https://arxiv.org/abs/2507.23607", "authors": ["Tien Huu Do", "Antoine Masquelier", "Nae Eoun Lee", "Jonathan Crowther"], "title": "Deep Learning-based Prediction of Clinical Trial Enrollment with Uncertainty Estimates", "comment": null, "summary": "Clinical trials are a systematic endeavor to assess the safety and efficacy\nof new drugs or treatments. Conducting such trials typically demands\nsignificant financial investment and meticulous planning, highlighting the need\nfor accurate predictions of trial outcomes. Accurately predicting patient\nenrollment, a key factor in trial success, is one of the primary challenges\nduring the planning phase. In this work, we propose a novel deep learning-based\nmethod to address this critical challenge. Our method, implemented as a neural\nnetwork model, leverages pre-trained language models (PLMs) to capture the\ncomplexities and nuances of clinical documents, transforming them into\nexpressive representations. These representations are then combined with\nencoded tabular features via an attention mechanism. To account for\nuncertainties in enrollment prediction, we enhance the model with a\nprobabilistic layer based on the Gamma distribution, which enables range\nestimation. We apply the proposed model to predict clinical trial duration,\nassuming site-level enrollment follows a Poisson-Gamma process. We carry out\nextensive experiments on real-world clinical trial data, and show that the\nproposed method can effectively predict the number of patients enrolled at a\nnumber of sites for a given clinical trial, outperforming established baseline\nmodels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u4e34\u5e8a\u8bd5\u9a8c\u7684\u60a3\u8005\u5165\u7ec4\u60c5\u51b5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u60a3\u8005\u5165\u7ec4\u662f\u8bd5\u9a8c\u6210\u529f\u7684\u5173\u952e\u56e0\u7d20\uff0c\u662f\u89c4\u5212\u9636\u6bb5\u7684\u4e3b\u8981\u6311\u6218\u4e4b\u4e00\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u6355\u83b7\u4e34\u5e8a\u6587\u6863\u7684\u590d\u6742\u6027\u548c\u7ec6\u5fae\u5dee\u522b\uff0c\u5e76\u5c06\u5b83\u4eec\u8f6c\u6362\u4e3a\u5bcc\u6709\u8868\u73b0\u529b\u7684\u8868\u793a\u3002\u7136\u540e\uff0c\u8fd9\u4e9b\u8868\u793a\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u7f16\u7801\u7684\u8868\u683c\u7279\u5f81\u76f8\u7ed3\u5408\u3002\u4e3a\u4e86\u89e3\u51b3\u6ce8\u518c\u9884\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u6211\u4eec\u4f7f\u7528\u57fa\u4e8e\u4f3d\u9a6c\u5206\u5e03\u7684\u6982\u7387\u5c42\u589e\u5f3a\u4e86\u6a21\u578b\uff0c\u4ece\u800c\u53ef\u4ee5\u8fdb\u884c\u8303\u56f4\u4f30\u8ba1\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u4e34\u5e8a\u8bd5\u9a8c\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u9884\u6d4b\u7ed9\u5b9a\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u591a\u4e2a\u5730\u70b9\u7684\u60a3\u8005\u5165\u7ec4\u6570\u91cf\uff0c\u4f18\u4e8e\u5df2\u5efa\u7acb\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u9884\u6d4b\u7ed9\u5b9a\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u591a\u4e2a\u5730\u70b9\u7684\u60a3\u8005\u5165\u7ec4\u6570\u91cf\uff0c\u4f18\u4e8e\u5df2\u5efa\u7acb\u7684\u57fa\u7ebf\u6a21\u578b\u3002"}}
{"id": "2507.23386", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23386", "abs": "https://arxiv.org/abs/2507.23386", "authors": ["Ailiang Lin", "Zhuoyun Li", "Kotaro Funakoshi"], "title": "Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models", "comment": null, "summary": "Decoder-only large language models (LLMs) are increasingly used to build\nembedding models that effectively encode the semantic information of natural\nlanguage texts into dense vector representations for various embedding tasks.\nHowever, many existing methods primarily focus on removing the causal attention\nmask in LLMs to enable bidirectional attention, potentially undermining the\nmodel's ability to extract semantic information acquired during pretraining.\nAdditionally, leading unidirectional approaches often rely on extra input text\nto overcome the inherent limitations of causal attention, inevitably increasing\ncomputational costs. In this work, we propose Causal2Vec, a general-purpose\nembedding model tailored to enhance the performance of decoder-only LLMs\nwithout altering their original architectures or introducing significant\ncomputational overhead. Specifically, we first employ a lightweight BERT-style\nmodel to pre-encode the input text into a single Contextual token, which is\nthen prepended to the LLM's input sequence, allowing each token to capture\ncontextualized information even without attending to future tokens.\nFurthermore, to mitigate the recency bias introduced by last-token pooling and\nhelp LLMs better leverage the semantic information encoded in the Contextual\ntoken, we concatenate the last hidden states of Contextual and EOS tokens as\nthe final text embedding. In practice, Causal2Vec achieves state-of-the-art\nperformance on the Massive Text Embeddings Benchmark (MTEB) among models\ntrained solely on publicly available retrieval datasets, while reducing the\nrequired sequence length by up to 85% and inference time by up to 82% compared\nto best-performing methods.", "AI": {"tldr": "Causal2Vec\u662f\u4e00\u79cd\u901a\u7528\u5d4c\u5165\u6a21\u578b\uff0c\u65e8\u5728\u589e\u5f3adecoder-only LLM\u7684\u6027\u80fd\uff0c\u800c\u65e0\u9700\u6539\u53d8\u5176\u539f\u59cb\u67b6\u6784\u6216\u5f15\u5165\u663e\u8457\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684\u8bb8\u591a\u65b9\u6cd5\u4e3b\u8981\u4fa7\u91cd\u4e8e\u79fb\u9664LLM\u4e2d\u7684\u56e0\u679c\u6ce8\u610f\u529b\u63a9\u7801\uff0c\u4ee5\u5b9e\u73b0\u53cc\u5411\u6ce8\u610f\u529b\uff0c\u8fd9\u53ef\u80fd\u4f1a\u524a\u5f31\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u83b7\u53d6\u8bed\u4e49\u4fe1\u606f\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u4e3b\u8981\u7684\u5355\u5411\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u989d\u5916\u7684\u8f93\u5165\u6587\u672c\u6765\u514b\u670d\u56e0\u679c\u6ce8\u610f\u529b\u7684\u56fa\u6709\u5c40\u9650\u6027\uff0c\u8fd9\u4e0d\u53ef\u907f\u514d\u5730\u589e\u52a0\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u9996\u5148\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7684BERT\u98ce\u683c\u6a21\u578b\u5c06\u8f93\u5165\u6587\u672c\u9884\u7f16\u7801\u4e3a\u5355\u4e2a\u4e0a\u4e0b\u6587token\uff0c\u7136\u540e\u5c06\u5176\u6dfb\u52a0\u5230LLM\u7684\u8f93\u5165\u5e8f\u5217\u4e2d\uff0c\u5141\u8bb8\u6bcf\u4e2atoken\u6355\u83b7\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5373\u4f7f\u4e0d\u5173\u6ce8\u672a\u6765\u7684token\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u51cf\u8f7b\u7531last-token pooling\u5f15\u5165\u7684\u8fd1\u56e0\u504f\u5dee\uff0c\u5e76\u5e2e\u52a9llm\u66f4\u597d\u5730\u5229\u7528\u4e0a\u4e0b\u6587token\u4e2d\u7f16\u7801\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u6211\u4eec\u5c06\u4e0a\u4e0b\u6587token\u548cEOS token\u7684\u6700\u540e\u9690\u85cf\u72b6\u6001\u8fde\u63a5\u8d77\u6765\u4f5c\u4e3a\u6700\u7ec8\u7684\u6587\u672c\u5d4c\u5165\u3002", "result": "Causal2Vec\u5728MTEB\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5c06\u6240\u9700\u7684\u5e8f\u5217\u957f\u5ea6\u51cf\u5c11\u4e86\u9ad8\u8fbe85%\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u4e86\u9ad8\u8fbe82%\u3002", "conclusion": "Causal2Vec\u5728\u516c\u5f00\u68c0\u7d22\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u4e2d\uff0c\u5728MTEB\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4e0e\u6027\u80fd\u6700\u4f73\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u9700\u5e8f\u5217\u957f\u5ea6\u51cf\u5c11\u4e86\u9ad8\u8fbe85%\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u4e86\u9ad8\u8fbe82%\u3002"}}
{"id": "2507.23411", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23411", "abs": "https://arxiv.org/abs/2507.23411", "authors": ["Lemar Abdi", "Francisco Caetano", "Amaan Valiuddin", "Christiaan Viviers", "Hamdi Joudeh", "Fons van der Sommen"], "title": "Out-of-Distribution Detection in Medical Imaging via Diffusion Trajectories", "comment": "Accepted at Uncertainty for Safe Utilization of Machine Learning in\n  Medical Imaging, MICCAI 2025", "summary": "In medical imaging, unsupervised out-of-distribution (OOD) detection offers\nan attractive approach for identifying pathological cases with extremely low\nincidence rates. In contrast to supervised methods, OOD-based approaches\nfunction without labels and are inherently robust to data imbalances. Current\ngenerative approaches often rely on likelihood estimation or reconstruction\nerror, but these methods can be computationally expensive, unreliable, and\nrequire retraining if the inlier data changes. These limitations hinder their\nability to distinguish nominal from anomalous inputs efficiently, consistently,\nand robustly. We propose a reconstruction-free OOD detection method that\nleverages the forward diffusion trajectories of a Stein score-based denoising\ndiffusion model (SBDDM). By capturing trajectory curvature via the estimated\nStein score, our approach enables accurate anomaly scoring with only five\ndiffusion steps. A single SBDDM pre-trained on a large, semantically aligned\nmedical dataset generalizes effectively across multiple Near-OOD and Far-OOD\nbenchmarks, achieving state-of-the-art performance while drastically reducing\ncomputational cost during inference. Compared to existing methods, SBDDM\nachieves a relative improvement of up to 10.43% and 18.10% for Near-OOD and\nFar-OOD detection, making it a practical building block for real-time, reliable\ncomputer-aided diagnosis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u91cd\u5efa\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u57fa\u4e8eStein score\u7684\u53bb\u566a\u6269\u6563\u6a21\u578b(SBDDM)\u7684\u6b63\u5411\u6269\u6563\u8f68\u8ff9\uff0c\u5b9e\u73b0\u4e86state-of-the-art\u7684\u6027\u80fd\uff0c\u5e76\u5927\u5927\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u4f3c\u7136\u4f30\u8ba1\u6216\u91cd\u5efa\u8bef\u5dee\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3001\u4e0d\u53ef\u9760\uff0c\u5e76\u4e14\u5982\u679c\u5185\u90e8\u6570\u636e\u53d1\u751f\u53d8\u5316\uff0c\u5219\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u3002\u8fd9\u4e9b\u9650\u5236\u963b\u788d\u4e86\u5b83\u4eec\u6709\u6548\u3001\u4e00\u81f4\u548c\u7a33\u5065\u5730\u533a\u5206\u6807\u79f0\u8f93\u5165\u548c\u5f02\u5e38\u8f93\u5165\u7684\u80fd\u529b\u3002", "method": "\u5229\u7528\u57fa\u4e8eStein score\u7684\u53bb\u566a\u6269\u6563\u6a21\u578b(SBDDM)\u7684\u6b63\u5411\u6269\u6563\u8f68\u8ff9\u6765\u6355\u83b7\u8f68\u8ff9\u66f2\u7387\uff0c\u4ece\u800c\u5b9e\u73b0\u7cbe\u786e\u7684\u5f02\u5e38\u8bc4\u5206\uff0c\u4ec5\u9700\u4e94\u4e2a\u6269\u6563\u6b65\u9aa4\u3002", "result": "SBDDM\u5728\u8fd1OOD\u548c\u8fdcOOD\u68c0\u6d4b\u65b9\u9762\u5206\u522b\u5b9e\u73b0\u4e86\u9ad8\u8fbe10.43%\u548c18.10%\u7684\u76f8\u5bf9\u6539\u8fdb\u3002", "conclusion": "\u57fa\u4e8eStein score\u7684\u53bb\u566a\u6269\u6563\u6a21\u578b(SBDDM)\u5728\u8fd1OOD\u548c\u8fdcOOD\u68c0\u6d4b\u4e2d\u53d6\u5f97\u4e86state-of-the-art\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5927\u5927\u964d\u4f4e\u4e86\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u5b9e\u65f6\u53ef\u9760\u7684\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u57fa\u7840\u6a21\u5757\u3002"}}
{"id": "2507.23615", "categories": ["cs.LG", "cs.AI", "68T01", "I.5.1; G.3; H.2.8; I.2.1"], "pdf": "https://arxiv.org/pdf/2507.23615", "abs": "https://arxiv.org/abs/2507.23615", "authors": ["Luis Roque", "Carlos Soares", "Vitor Cerqueira", "Luis Torgo"], "title": "L-GTA: Latent Generative Modeling for Time Series Augmentation", "comment": null, "summary": "Data augmentation is gaining importance across various aspects of time series\nanalysis, from forecasting to classification and anomaly detection tasks. We\nintroduce the Latent Generative Transformer Augmentation (L-GTA) model, a\ngenerative approach using a transformer-based variational recurrent\nautoencoder. This model uses controlled transformations within the latent space\nof the model to generate new time series that preserve the intrinsic properties\nof the original dataset. L-GTA enables the application of diverse\ntransformations, ranging from simple jittering to magnitude warping, and\ncombining these basic transformations to generate more complex synthetic time\nseries datasets. Our evaluation of several real-world datasets demonstrates the\nability of L-GTA to produce more reliable, consistent, and controllable\naugmented data. This translates into significant improvements in predictive\naccuracy and similarity measures compared to direct transformation methods.", "AI": {"tldr": "L-GTA \u662f\u4e00\u79cd\u4f7f\u7528 transformer \u7684\u751f\u6210\u65b9\u6cd5\uff0c\u53ef\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\u751f\u6210\u589e\u5f3a\u6570\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u548c\u76f8\u4f3c\u6027\u5ea6\u91cf\u3002", "motivation": "\u6570\u636e\u589e\u5f3a\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u7684\u5404\u4e2a\u65b9\u9762\uff08\u4ece\u9884\u6d4b\u5230\u5206\u7c7b\u548c\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\uff09\u4e2d\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002", "method": "\u4e00\u79cd\u4f7f\u7528\u57fa\u4e8etransformer\u7684\u53d8\u5206\u5faa\u73af\u81ea\u52a8\u7f16\u7801\u5668\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u8be5\u6a21\u578b\u4f7f\u7528\u6a21\u578b\u6f5c\u5728\u7a7a\u95f4\u5185\u7684\u53d7\u63a7\u8f6c\u6362\u6765\u751f\u6210\u65b0\u7684\u65f6\u95f4\u5e8f\u5217\uff0c\u8fd9\u4e9b\u65f6\u95f4\u5e8f\u5217\u4fdd\u7559\u4e86\u539f\u59cb\u6570\u636e\u96c6\u7684\u5185\u5728\u5c5e\u6027\u3002", "result": "\u5bf9\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u8868\u660e\uff0cL-GTA \u80fd\u591f\u751f\u6210\u66f4\u53ef\u9760\u3001\u4e00\u81f4\u548c\u53ef\u63a7\u7684\u589e\u5f3a\u6570\u636e\u3002", "conclusion": "L-GTA \u80fd\u591f\u751f\u6210\u66f4\u53ef\u9760\u3001\u4e00\u81f4\u548c\u53ef\u63a7\u7684\u589e\u5f3a\u6570\u636e\uff0c\u4e0e\u76f4\u63a5\u8f6c\u6362\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8fd9\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u548c\u76f8\u4f3c\u6027\u5ea6\u91cf\u3002"}}
{"id": "2507.23399", "categories": ["cs.CL", "cs.CY", "I.2.7; K.4.3"], "pdf": "https://arxiv.org/pdf/2507.23399", "abs": "https://arxiv.org/abs/2507.23399", "authors": ["Peter Sandrini"], "title": "Beyond the Cloud: Assessing the Benefits and Drawbacks of Local LLM Deployment for Translators", "comment": null, "summary": "The rapid proliferation of Large Language Models presents both opportunities\nand challenges for the translation field. While commercial, cloud-based AI\nchatbots have garnered significant attention in translation studies, concerns\nregarding data privacy, security, and equitable access necessitate exploration\nof alternative deployment models. This paper investigates the feasibility and\nperformance of locally deployable, free language models as a viable alternative\nto proprietary, cloud-based AI solutions. This study evaluates three\nopen-source models installed on CPU-based platforms and compared against\ncommercially available online chat-bots. The evaluation focuses on functional\nperformance rather than a comparative analysis of human-machine translation\nquality, an area already subject to extensive research. The platforms assessed\nwere chosen for their accessibility and ease of use across various operating\nsystems. While local deployment introduces its own challenges, the benefits of\nenhanced data control, improved privacy, and reduced dependency on cloud\nservices are compelling. The findings of this study contribute to a growing\nbody of knowledge concerning the democratization of AI technology and inform\nfuture research and development efforts aimed at making LLMs more accessible\nand practical for a wider range of users, specifically focusing on the needs of\nindividual translators and small businesses.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u672c\u5730\u90e8\u7f72\u7684\u514d\u8d39\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4e13\u6709\u4e91\u4eba\u5de5\u667a\u80fd\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u884c\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8fc5\u901f\u666e\u53ca\u4e3a\u7ffb\u8bd1\u9886\u57df\u5e26\u6765\u4e86\u673a\u9047\u548c\u6311\u6218\u3002\u7531\u4e8e\u5bf9\u6570\u636e\u9690\u79c1\u3001\u5b89\u5168\u548c\u516c\u5e73\u8bbf\u95ee\u7684\u62c5\u5fe7\uff0c\u6709\u5fc5\u8981\u63a2\u7d22\u66ff\u4ee3\u90e8\u7f72\u6a21\u578b\u3002", "method": "\u8bc4\u4f30\u4e86\u5b89\u88c5\u5728\u57fa\u4e8eCPU\u7684\u5e73\u53f0\u4e0a\u7684\u4e09\u4e2a\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u4e0e\u5546\u4e1a\u4e0a\u53ef\u7528\u7684\u5728\u7ebf\u804a\u5929\u673a\u5668\u4eba\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u8bc4\u4f30\u4fa7\u91cd\u4e8e\u529f\u80fd\u6027\u80fd\uff0c\u800c\u4e0d\u662f\u4eba\u673a\u7ffb\u8bd1\u8d28\u91cf\u7684\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u672c\u5730\u90e8\u7f72\u5728\u589e\u5f3a\u6570\u636e\u63a7\u5236\u3001\u6539\u5584\u9690\u79c1\u548c\u51cf\u5c11\u5bf9\u4e91\u670d\u52a1\u4f9d\u8d56\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u672c\u5730\u90e8\u7f72\u5f15\u5165\u4e86\u6311\u6218\uff0c\u4f46\u589e\u5f3a\u6570\u636e\u63a7\u5236\u3001\u6539\u5584\u9690\u79c1\u548c\u51cf\u5c11\u5bf9\u4e91\u670d\u52a1\u4f9d\u8d56\u7684\u597d\u5904\u662f\u663e\u8457\u7684\u3002\u7814\u7a76\u7ed3\u679c\u6709\u52a9\u4e8e\u6269\u5c55\u5173\u4e8e\u4eba\u5de5\u667a\u80fd\u6280\u672f\u6c11\u4e3b\u5316\u7684\u77e5\u8bc6\u4f53\u7cfb\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u548c\u5f00\u53d1\u5de5\u4f5c\u63d0\u4f9b\u4fe1\u606f\uff0c\u65e8\u5728\u4f7fLLM\u5bf9\u4e8e\u66f4\u5e7f\u6cdb\u7684\u7528\u6237\u6765\u8bf4\u66f4\u5bb9\u6613\u8bbf\u95ee\u548c\u5b9e\u7528\uff0c\u7279\u522b\u662f\u5173\u6ce8\u4e2a\u4f53\u7ffb\u8bd1\u548c\u5c0f\u578b\u4f01\u4e1a\u7684\u9700\u6c42\u3002"}}
{"id": "2507.23416", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23416", "abs": "https://arxiv.org/abs/2507.23416", "authors": ["Mokhtar A. Al-Awadhi", "Ratnadeep R. Deshmukh"], "title": "Honey Adulteration Detection using Hyperspectral Imaging and Machine Learning", "comment": null, "summary": "This paper aims to develop a machine learning-based system for automatically\ndetecting honey adulteration with sugar syrup, based on honey hyperspectral\nimaging data. First, the floral source of a honey sample is classified by a\nbotanical origin identification subsystem. Then, the sugar syrup adulteration\nis identified, and its concentration is quantified by an adulteration detection\nsubsystem. Both subsystems consist of two steps. The first step involves\nextracting relevant features from the honey sample using Linear Discriminant\nAnalysis (LDA). In the second step, we utilize the K-Nearest Neighbors (KNN)\nmodel to classify the honey botanical origin in the first subsystem and\nidentify the adulteration level in the second subsystem. We assess the proposed\nsystem performance on a public honey hyperspectral image dataset. The result\nindicates that the proposed system can detect adulteration in honey with an\noverall cross-validation accuracy of 96.39%, making it an appropriate\nalternative to the current chemical-based detection methods.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u8702\u871c\u63ba\u5047\u81ea\u52a8\u68c0\u6d4b\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5229\u7528\u8702\u871c\u9ad8\u5149\u8c31\u6210\u50cf\u6570\u636e\uff0c\u5b9e\u73b0\u4e86 96.39% \u7684\u4ea4\u53c9\u9a8c\u8bc1\u51c6\u786e\u7387\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u8702\u871c\u4e2d\u63ba\u5165\u7cd6\u6d46\u7684\u884c\u4e3a\uff0c\u8be5\u7cfb\u7edf\u57fa\u4e8e\u8702\u871c\u9ad8\u5149\u8c31\u6210\u50cf\u6570\u636e\u3002", "method": "\u5229\u7528\u7ebf\u6027\u5224\u522b\u5206\u6790 (LDA) \u4ece\u8702\u871c\u6837\u672c\u4e2d\u63d0\u53d6\u76f8\u5173\u7279\u5f81\u3002\u5728\u7b2c\u4e8c\u6b65\u4e2d\uff0c\u6211\u4eec\u5229\u7528 K \u6700\u8fd1\u90bb (KNN) \u6a21\u578b\u6765\u5bf9\u7b2c\u4e00\u5b50\u7cfb\u7edf\u4e2d\u7684\u8702\u871c\u690d\u7269\u6765\u6e90\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u8bc6\u522b\u7b2c\u4e8c\u5b50\u7cfb\u7edf\u4e2d\u7684\u63ba\u5047\u6c34\u5e73\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u4ee5 96.39% \u7684\u4ea4\u53c9\u9a8c\u8bc1\u51c6\u786e\u7387\u68c0\u6d4b\u8702\u871c\u4e2d\u7684\u63ba\u5047\u884c\u4e3a", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u591f\u4ee5 96.39% \u7684\u4ea4\u53c9\u9a8c\u8bc1\u51c6\u786e\u7387\u68c0\u6d4b\u8702\u871c\u4e2d\u7684\u63ba\u5047\u884c\u4e3a\uff0c\u4f7f\u5176\u6210\u4e3a\u5f53\u524d\u5316\u5b66\u68c0\u6d4b\u65b9\u6cd5\u7684\u5408\u9002\u66ff\u4ee3\u54c1\u3002"}}
